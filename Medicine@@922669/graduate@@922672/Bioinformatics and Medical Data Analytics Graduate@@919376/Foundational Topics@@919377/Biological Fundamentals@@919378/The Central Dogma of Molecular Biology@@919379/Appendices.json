{"hands_on_practices": [{"introduction": "The Central Dogma's principle of information preservation begins with the high-fidelity replication of DNA. This exercise challenges you to move beyond a qualitative appreciation of this accuracy and build a quantitative model from first principles. By combining the error rates of DNA polymerase, proofreading, and mismatch repair, you will calculate the expected number of mutations per cell division, providing a concrete understanding of how multiple layers of quality control achieve the remarkable stability of the genome [@problem_id:2965554].", "problem": "A eukaryotic cell replicates a linear genome comprising $3.2 \\times 10^{9}$ nucleotide positions during each cell division. At each position, the deoxyribonucleic acid (DNA) polymerase misincorporates a nucleotide with marginal probability $\\epsilon_{\\text{pol}} = 10^{-5}$. Conditional on a misincorporation event having occurred at the 3' terminus, the exonuclease proofreading mechanism attempts to remove the mismatch; the conditional probability that proofreading fails to correct the mismatch is $\\epsilon_{\\text{proof}} = 10^{-2}$. Conditional on a post-replicative mismatch persisting, the mismatch repair (MMR) system attempts to correct it; the conditional probability that mismatch repair fails is $\\epsilon_{\\text{MMR}} = 10^{-1}$. Assume independence of error events across positions and conditional independence of correction failures given the presence of a mismatch. A base substitution becomes a fixed mutation only if a misincorporation occurs and both correction systems fail before the next replication cycle.\n\nWithin the framework of the Central Dogma of Molecular Biology, where high-fidelity replication preserves information flow from DNA to DNA, use only the definitions above and first principles of probability and expectation to derive an expression for the expected number of replication-induced base substitutions per cell division across the $3.2 \\times 10^{9}$ positions. Then evaluate this expectation numerically using the given parameter values. Report the final number of substitutions per division as an exact number (no rounding, no units).\n\nFinally, based on your result and established qualitative knowledge of replication fidelity in normal human somatic cells with intact proofreading and mismatch repair, briefly assess in words whether the magnitude you computed is biologically plausible, and justify your reasoning qualitatively (your qualitative assessment will not affect the numerical answer requirement).", "solution": "The problem requires the derivation and calculation of the expected number of base substitutions that become fixed in a genome after one cycle of replication. This quantity is determined by the interplay of error introduction by DNA polymerase and subsequent correction by two major repair systems: proofreading and mismatch repair. The foundation of the solution lies in probability theory, specifically the calculation of the probability of a sequence of dependent events, and the application of the linearity of expectation.\n\nLet $N$ be the total number of nucleotide positions in the genome, which is given as $N = 3.2 \\times 10^{9}$.\nA base substitution becomes a fixed mutation at a single position if and only if a sequence of three events occurs: an initial misincorporation, a failure of proofreading, and a subsequent failure of mismatch repair. We must calculate the probability of this compound event.\n\nLet us define the events for a single nucleotide position:\n- $M$: The event that DNA polymerase misincorporates a nucleotide. The probability is given as $P(M) = \\epsilon_{\\text{pol}} = 10^{-5}$.\n- $F_P$: The event that the exonuclease proofreading mechanism fails to correct the mismatch. This is a conditional event, dependent on the occurrence of a misincorporation. The problem provides the conditional probability $P(F_P | M) = \\epsilon_{\\text{proof}} = 10^{-2}$.\n- $F_{MMR}$: The event that the mismatch repair (MMR) system fails to correct the mismatch. This event is conditional on a mismatch having persisted through both replication and proofreading, i.e., conditional on the event $M \\cap F_P$. The problem provides the conditional probability $P(F_{MMR} | M \\cap F_P) = \\epsilon_{\\text{MMR}} = 10^{-1}$.\n\nThe probability of a single fixed substitution, which we denote as $\\mu$, is the probability of the intersection of these three events: $\\mu = P(M \\cap F_P \\cap F_{MMR})$. Using the chain rule for probabilities, we can express $\\mu$ as follows:\n$$ \\mu = P(M) \\times P(F_P | M) \\times P(F_{MMR} | M \\cap F_P) $$\nThe problem statement provides all the necessary probabilities on the right-hand side. Substituting the given symbolic parameters:\n$$ \\mu = \\epsilon_{\\text{pol}} \\cdot \\epsilon_{\\text{proof}} \\cdot \\epsilon_{\\text{MMR}} $$\nThis expression represents the probability of a fixed mutation arising at any single nucleotide position during one cell division.\n\nNow, we must find the expected total number of such mutations across the entire genome of $N$ positions. Let $X$ be a random variable representing the total number of substitutions per cell division. We can define $N$ indicator random variables, $X_1, X_2, \\dots, X_N$, where $X_i = 1$ if a fixed substitution occurs at position $i$, and $X_i = 0$ otherwise. The probability that $X_i = 1$ is precisely $\\mu$, as calculated above.\nThe total number of substitutions is the sum of these indicator variables: $X = \\sum_{i=1}^{N} X_i$.\n\nBy the linearity of expectation, the expected value of a sum of random variables is the sum of their individual expected values:\n$$ E[X] = E\\left[\\sum_{i=1}^{N} X_i\\right] = \\sum_{i=1}^{N} E[X_i] $$\nThe expectation of an indicator variable $X_i$ is equal to the probability of the event it indicates: $E[X_i] = 1 \\cdot P(X_i=1) + 0 \\cdot P(X_i=0) = P(X_i=1) = \\mu$.\nSince the probability of mutation $\\mu$ is assumed to be identical for all positions, the sum simplifies to:\n$$ E[X] = \\sum_{i=1}^{N} \\mu = N \\cdot \\mu $$\nThus, the final expression for the expected number of substitutions is:\n$$ E[X] = N \\cdot \\epsilon_{\\text{pol}} \\cdot \\epsilon_{\\text{proof}} \\cdot \\epsilon_{\\text{MMR}} $$\nWe now proceed to the numerical evaluation using the given parameter values:\n$N = 3.2 \\times 10^{9}$\n$\\epsilon_{\\text{pol}} = 10^{-5}$\n$\\epsilon_{\\text{proof}} = 10^{-2}$\n$\\epsilon_{\\text{MMR}} = 10^{-1}$\n\nFirst, we calculate the single-site mutation probability, $\\mu$:\n$$ \\mu = (10^{-5}) \\times (10^{-2}) \\times (10^{-1}) = 10^{-5-2-1} = 10^{-8} $$\nThis is the per-site, per-replication mutation rate.\n\nNext, we calculate the expected total number of substitutions, $E[X]$:\n$$ E[X] = (3.2 \\times 10^{9}) \\times (10^{-8}) = 3.2 \\times 10^{9-8} = 3.2 \\times 10^{1} = 32 $$\nThe expected number of replication-induced base substitutions per cell division is therefore $32$.\n\nFinally, we assess the biological plausibility of this result. The calculated overall mutation rate of $\\mu = 10^{-8}$ mutations per base pair per replication cycle is a textbook value, highly consistent with measurements of the germline mutation rate in humans and other mammals. The resulting expectation of $32$ new mutations for a genome of size $3.2 \\times 10^{9}$ base pairs is also well within the range of established biological estimates. Empirical studies of *de novo* mutations in humans, which accumulate over cell divisions, report tens of new mutations per generation. An expectation of $32$ mutations arising from a single round of replication of the genome is therefore a scientifically sound and plausible magnitude. It correctly quantifies the outcome of a process where an immense number of nucleotides are copied with extraordinarily high, yet imperfect, fidelity. The Central Dogma's tenet of information preservation from DNA to DNA is maintained with high fidelity, but this calculation demonstrates that it is not absolute.", "answer": "$$\n\\boxed{32}\n$$", "id": "2965554"}, {"introduction": "While DNA replication preserves the entire genomic sequence, the first step of gene expression involves transcribing only specific portionsâ€”the genes. This practice moves from the sequence to its functional annotation by asking you to build a Hidden Markov Model (HMM) to distinguish exons from introns and intergenic DNA. Implementing the Viterbi algorithm will give you hands-on experience with a foundational bioinformatics tool for decoding the structural grammar of the genome based on local sequence composition [@problem_id:2434915].", "problem": "You are asked to formalize and implement a probabilistic sequence model to classify genomic positions into exon, intron, or intergenic states using a Hidden Markov Model (HMM) informed by $k$-mer frequencies. The biological foundation is the central dogma of molecular biology, where genomic DNA is transcribed and processed, creating characteristic composition patterns in exons, introns, and intergenic regions. You must design the model and algorithm from first principles, adhering to probabilistic definitions and using only the following foundational bases: the Markov property for latent states, independence of emissions conditioned on the current state, and the observation that exons, introns, and intergenic regions have different $k$-mer composition biases.\n\nDefine a Hidden Markov Model with latent states $\\mathcal{S} = \\{\\mathrm{E}, \\mathrm{I}, \\mathrm{G}\\}$ corresponding to exon, intron, and intergenic. The observable alphabet is the set $\\mathcal{K}$ of all DNA $k$-mers over $\\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$, so $\\lvert \\mathcal{K} \\rvert = 4^{k}$. Let the initial distribution be $\\boldsymbol{\\pi} = (\\pi_{s})_{s \\in \\mathcal{S}}$, the state transition matrix be $\\mathbf{A} = (a_{s \\to s^{\\prime}})_{s,s^{\\prime} \\in \\mathcal{S}}$, and the emission matrix be $\\mathbf{B} = (b_{s}(o))_{s \\in \\mathcal{S}, o \\in \\mathcal{K}}$, where $b_{s}(o) = \\mathbb{P}(O_{t} = o \\mid S_{t} = s)$. Assume the standard HMM generative assumptions: $S_{t}$ is first-order Markov with $\\mathbb{P}(S_{t} \\mid S_{1:t-1}) = \\mathbb{P}(S_{t} \\mid S_{t-1})$, and $O_{t}$ is conditionally independent of other variables given $S_{t}$.\n\nTraining data are provided as follows.\n- Emission training sequences: for each state $s \\in \\{\\mathrm{E},\\mathrm{I},\\mathrm{G}\\}$, you are given labeled DNA sequences that are known to be generated from $s$. For a chosen $k$, extract overlapping $k$-mers from each sequence and accumulate counts $c_{s}(o)$ for $o \\in \\mathcal{K}$. Use Laplace smoothing with hyperparameter $\\alpha > 0$ to construct emission probabilities:\n$$\nb_{s}(o) = \\frac{c_{s}(o) + \\alpha}{\\sum_{o^{\\prime} \\in \\mathcal{K}} c_{s}(o^{\\prime}) + \\alpha \\lvert \\mathcal{K} \\rvert}.\n$$\n- Transition training state paths: you are given labeled state paths over $\\{\\mathrm{E},\\mathrm{I},\\mathrm{G}\\}$ that represent example traversals through genomic regions. Let $n_{s \\to s^{\\prime}}$ denote the count of transitions from $s$ to $s^{\\prime}$ across all provided paths. Estimate transition probabilities with Laplace smoothing parameter $\\beta > 0$:\n$$\na_{s \\to s^{\\prime}} = \\frac{n_{s \\to s^{\\prime}} + \\beta}{\\sum_{u \\in \\mathcal{S}} n_{s \\to u} + \\beta \\lvert \\mathcal{S} \\rvert}.\n$$\nAlso estimate the initial distribution with Laplace smoothing parameter $\\gamma > 0$ from the starting states of the training paths. Let $m_{s}$ be the number of paths that start in $s$, and let $M$ be the number of paths. Then\n$$\n\\pi_{s} = \\frac{m_{s} + \\gamma}{M + \\gamma \\lvert \\mathcal{S} \\rvert}.\n$$\nFor each test case below, take $\\beta = \\gamma = \\alpha$.\n\nInference requirement:\n- Given a test DNA sequence and a fixed $k$, transform the sequence into a length-$T$ observation sequence $O_{1:T}$ by taking overlapping $k$-mers starting at each position, so $T = L - k + 1$ for a DNA string of length $L$.\n- Decode the most likely state path $\\hat{S}_{1:T}$ using the Viterbi algorithm that maximizes\n$$\n\\hat{S}_{1:T} = \\arg\\max_{s_{1:T} \\in \\mathcal{S}^{T}} \\left[ \\log \\pi_{s_{1}} + \\sum_{t=2}^{T} \\log a_{s_{t-1} \\to s_{t}} + \\sum_{t=1}^{T} \\log b_{s_{t}}(O_{t}) \\right].\n$$\n\nLabeling convention for test evaluation:\n- Each test sequence is provided as a concatenation of labeled segments $(\\text{state}, \\text{DNA segment})$. For a given $k$, the ground-truth label for observation index $t \\in \\{1,\\dots,T\\}$ is taken to be the state of the segment in which the $k$-mer starting at position $t$ begins. That is, if the starting nucleotide index (zero-based) of the $k$-mer lies in the range of a segment labeled $s$, then the ground-truth state at index $t$ is $s$, regardless of whether the $k$-mer crosses a segment boundary.\n- Compute the accuracy for a test case as the fraction in $[0,1]$ of indices $t \\in \\{1,\\dots,T\\}$ where $\\hat{S}_{t}$ equals the ground-truth state.\n\nYour program must implement the above model estimation and decoding and then evaluate the accuracy for multiple test cases using the following fixed training data and test suite. All sequences contain only $\\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$.\n\nCommon training data (used by every test case):\n- Emission training sequences by state:\n  - Exon ($\\mathrm{E}$): \n    - \"GCGCGCGCGCGCGCGCATGCGCGC\"\n    - \"CGCGCCGCGCGCGGCGCGC\"\n  - Intron ($\\mathrm{I}$):\n    - \"ATATATATATATATATATATTTATA\"\n    - \"AATAATATATAAATATATA\"\n  - Intergenic ($\\mathrm{G}$):\n    - \"AGCTAGCTAGCTAGCTAGCTAGCT\"\n    - \"GATCGATCGATCGATC\"\n- Transition training state paths:\n  - \"GGGGGIIIIIEEEEEEEGGGGG\"\n  - \"GGGIIIIIGGGEEE\"\n  - \"EEEEEEIIIIGGGG\"\n\nTest suite:\n- Test case $1$ (happy path): $k = 2$, $\\alpha = 0.5$. Test sequence is the concatenation of:\n  - $(\\mathrm{G}, \\text{\"AGCTAGCTAGC\"})$\n  - $(\\mathrm{I}, \\text{\"ATATATATATAT\"})$\n  - $(\\mathrm{E}, \\text{\"GCGCGCGCGC\"})$\n- Test case $2$ (boundary length): $k = 2$, $\\alpha = 0.5$. Test sequence is the single segment:\n  - $(\\mathrm{I}, \\text{\"AT\"})$\n- Test case $3$ (ambiguous composition, stronger smoothing): $k = 2$, $\\alpha = 1.0$. Test sequence is the concatenation of:\n  - $(\\mathrm{G}, \\text{\"AGCTAGC\"})$\n  - $(\\mathrm{E}, \\text{\"GCGCGC\"})$\n  - $(\\mathrm{I}, \\text{\"ATATAT\"})$\n- Test case $4$ (larger alphabet, $k$-mers of length $3$): $k = 3$, $\\alpha = 0.5$. Test sequence is the concatenation of:\n  - $(\\mathrm{G}, \\text{\"AGCTAGCT\"})$\n  - $(\\mathrm{I}, \\text{\"ATATATAT\"})$\n  - $(\\mathrm{E}, \\text{\"GCGCGCGC\"})$\n\nComputational and numerical requirements:\n- Use only floating-point arithmetic in base-$e$ logarithms when implementing the Viterbi algorithm to maintain numerical stability. No angle units are involved.\n- Round each accuracy to exactly $4$ decimal places. Express each accuracy as a decimal in $[0,1]$ without a percent sign.\n- The final program output must be a single line containing a Python-style list of the accuracy values for the test suite, in order from test case $1$ to test case $4$, formatted as a comma-separated list enclosed in square brackets. For example, the output must look like \"[$x_{1}$,$x_{2}$,$x_{3}$,$x_{4}$]\" where each $x_{i}$ has exactly $4$ digits after the decimal point.\n\nYour program must be a complete, runnable implementation that constructs the HMM from the provided training data, decodes with Viterbi for each test case, computes the accuracies as specified, rounds them to $4$ decimal places, and prints the results in the required format on a single line.", "solution": "The problem statement poses a valid and well-defined task in computational biology. It requires the construction and application of a first-order Hidden Markov Model (HMM) to classify genomic regions. All necessary data and definitions for model training and inference are provided, and the problem is scientifically grounded in the established use of HMMs for sequence annotation. The premises are internally consistent and formalizable. We proceed with the derivation and implementation.\n\nThe HMM is defined by the tuple $(\\mathcal{S}, \\mathcal{K}, \\boldsymbol{\\pi}, \\mathbf{A}, \\mathbf{B})$, where:\n- $\\mathcal{S} = \\{\\mathrm{E}, \\mathrm{I}, \\mathrm{G}\\}$ is the set of $3$ latent states (Exon, Intron, Intergenic).\n- $\\mathcal{K}$ is the observation alphabet of all possible DNA $k$-mers, with size $\\lvert \\mathcal{K} \\rvert = 4^{k}$.\n- $\\boldsymbol{\\pi}$ is the initial state probability vector.\n- $\\mathbf{A}$ is the state transition probability matrix.\n- $\\mathbf{B}$ is the emission probability matrix.\n\nThe solution is structured into three parts: model parameter estimation, sequence decoding via the Viterbi algorithm, and performance evaluation.\n\n**1. Model Parameter Estimation**\n\nThe HMM parameters $(\\boldsymbol{\\pi}, \\mathbf{A}, \\mathbf{B})$ are estimated from the provided training data using Maximum Likelihood Estimation with Laplace smoothing to prevent zero probabilities. All probabilities are handled in the logarithmic domain to ensure numerical stability.\n\n**Initial State Probabilities ($\\boldsymbol{\\pi}$)**:\nThe initial probability $\\pi_s$ for each state $s \\in \\mathcal{S}$ is estimated from the $M$ provided state paths. Let $m_s$ be the count of paths starting with state $s$, and $\\gamma$ be the smoothing hyperparameter. The smoothed probability is:\n$$\n\\pi_s = \\frac{m_s + \\gamma}{M + \\gamma \\lvert\\mathcal{S}\\rvert}\n$$\nThe corresponding log-probability is $\\log \\pi_s = \\log(m_s + \\gamma) - \\log(M + \\gamma \\lvert\\mathcal{S}\\rvert)$.\n\n**State Transition Probabilities ($\\mathbf{A}$)**:\nThe transition probability $a_{s \\to s'}$ from state $s$ to $s'$ is estimated by counting such transitions, $n_{s \\to s'}$, in the training paths. With smoothing parameter $\\beta$, the probability is:\n$$\na_{s \\to s'} = \\frac{n_{s \\to s'} + \\beta}{\\sum_{u \\in \\mathcal{S}} n_{s \\to u} + \\beta \\lvert\\mathcal{S}\\rvert}\n$$\nThe log-probability is $\\log a_{s \\to s'} = \\log(n_{s \\to s'} + \\beta) - \\log(\\sum_{u \\in \\mathcal{S}} n_{s \\to u} + \\beta \\lvert\\mathcal{S}\\rvert)$.\n\n**Emission Probabilities ($\\mathbf{B}$)**:\nThe emission probability $b_s(o)$ of observing $k$-mer $o \\in \\mathcal{K}$ from state $s \\in \\mathcal{S}$ is estimated from labeled training sequences. First, for each state $s$, we count the occurrences $c_s(o)$ of each $k$-mer $o$ by sliding a window of length $k$ over the corresponding training sequences. With smoothing parameter $\\alpha$, the probability is:\n$$\nb_s(o) = \\frac{c_s(o) + \\alpha}{\\sum_{o' \\in \\mathcal{K}} c_s(o') + \\alpha \\lvert\\mathcal{K}\\rvert}\n$$\nThe log-probability is $\\log b_s(o) = \\log(c_s(o) + \\alpha) - \\log(\\sum_{o' \\in \\mathcal{K}} c_s(o') + \\alpha \\lvert\\mathcal{K}\\rvert)$.\n\nFor all test cases, the problem specifies $\\alpha = \\beta = \\gamma$.\n\n**2. State Path Decoding via Viterbi Algorithm**\n\nGiven a test DNA sequence of length $L$, it is converted into an observation sequence $O_{1:T} = (O_1, O_2, \\dots, O_T)$ of $T = L - k + 1$ overlapping $k$-mers. The Viterbi algorithm is a dynamic programming approach to find the most likely sequence of states $\\hat{S}_{1:T}$ that generated $O_{1:T}$. It works by maximizing the joint log-probability:\n$$\n\\log \\mathbb{P}(O_{1:T}, S_{1:T}) = \\log \\pi_{S_1} + \\sum_{t=2}^{T} \\log a_{S_{t-1} \\to S_t} + \\sum_{t=1}^{T} \\log b_{S_t}(O_t)\n$$\nWe define two tables: $\\delta_t(s)$, which stores the maximum log-probability of any path ending in state $s$ at time $t$, and $\\psi_t(s)$, which stores the previous state that leads to this maximum probability.\n\n- **Initialization ($t=1$)**: For each state $s \\in \\mathcal{S}$:\n$$\n\\delta_1(s) = \\log \\pi_s + \\log b_s(O_1)\n$$\n$$\n\\psi_1(s) = 0 \\quad (\\text{or any convention for the start})\n$$\n\n- **Recursion ($t=2, \\dots, T$)**: For each state $s' \\in \\mathcal{S}$:\n$$\n\\delta_t(s') = \\left( \\max_{s \\in \\mathcal{S}} \\left\\{ \\delta_{t-1}(s) + \\log a_{s \\to s'} \\right\\} \\right) + \\log b_{s'}(O_t)\n$$\n$$\n\\psi_t(s') = \\arg\\max_{s \\in \\mathcal{S}} \\left\\{ \\delta_{t-1}(s) + \\log a_{s \\to s'} \\right\\}\n$$\n\n- **Termination**: The probability of the most likely path is $\\max_{s \\in \\mathcal{S}} \\delta_T(s)$. The final state of the path is:\n$$\n\\hat{S}_T = \\arg\\max_{s \\in \\mathcal{S}} \\delta_T(s)\n$$\n\n- **Path Backtracking**: The most likely path is reconstructed by starting from $\\hat{S}_T$ and tracing back using the $\\psi$ table:\n$$\n\\hat{S}_{t-1} = \\psi_t(\\hat{S}_t) \\quad \\text{for } t = T, T-1, \\dots, 2\n$$\n\n**3. Evaluation**\n\nThe performance of the decoder is quantified by its accuracy. For each test case, a ground-truth state path is constructed. The test sequence is formed by concatenating labeled DNA segments. The ground-truth state for observation $O_t$ (the $k$-mer starting at nucleotide index $t-1$ of the concatenated sequence) is the label of the segment containing this start position.\n\nThe accuracy is then computed as the fraction of correctly predicted states:\n$$\n\\text{Accuracy} = \\frac{1}{T} \\sum_{t=1}^{T} \\mathbb{I}(\\hat{S}_t = S^{\\text{truth}}_t)\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function, which is $1$ if its argument is true and $0$ otherwise. The final result for each test case is this accuracy value, rounded to $4$ decimal places.\n\nThe implementation will follow these principles precisely. For each test case, we first determine the parameters $k$ and $\\alpha$, train the HMM if its parameters have not been computed already, prepare the test observation sequence and corresponding ground-truth path, execute the Viterbi algorithm to obtain the predicted path, and finally calculate the accuracy.", "answer": "```python\nimport numpy as np\nfrom itertools import product\nfrom collections import Counter\n\n# Define states and bases for consistency.\nSTATES = ['E', 'I', 'G']\nSTATE_MAP = {s: i for i, s in enumerate(STATES)}\nN_STATES = len(STATES)\nBASES = ['A', 'C', 'G', 'T']\n\ndef generate_kmers(seq, k):\n    \"\"\"Generates a list of overlapping k-mers from a sequence.\"\"\"\n    if len(seq) < k:\n        return []\n    return [seq[i:i+k] for i in range(len(seq) - k + 1)]\n\ndef train_hmm(emission_data, transition_paths, k, alpha, beta, gamma):\n    \"\"\"\n    Trains HMM parameters (pi, A, B) from training data and returns them in log-space.\n    \"\"\"\n    # 1. K-mer alphabet generation\n    all_kmers = [''.join(p) for p in product(BASES, repeat=k)]\n    kmer_map = {kmer: i for i, kmer in enumerate(all_kmers)}\n    n_kmers = len(all_kmers)\n\n    # 2. Emission probabilities (B) estimation\n    emission_counts = np.zeros((N_STATES, n_kmers))\n    total_state_kmers = np.zeros(N_STATES)\n    for state_str, seqs in emission_data.items():\n        state_idx = STATE_MAP[state_str]\n        for seq in seqs:\n            kmers = generate_kmers(seq, k)\n            total_state_kmers[state_idx] += len(kmers)\n            counts = Counter(kmers)\n            for kmer, count in counts.items():\n                if kmer in kmer_map:\n                    emission_counts[state_idx, kmer_map[kmer]] += count\n    \n    # Laplace smoothing for B and conversion to log-space\n    log_B = np.log(emission_counts + alpha) - np.log(total_state_kmers.reshape(-1, 1) + alpha * n_kmers)\n\n    # 3. Initial probabilities (pi) estimation\n    initial_counts = np.zeros(N_STATES)\n    num_paths = len(transition_paths)\n    for path in transition_paths:\n        initial_counts[STATE_MAP[path[0]]] += 1\n\n    # Laplace smoothing for pi and conversion to log-space\n    log_pi = np.log(initial_counts + gamma) - np.log(num_paths + gamma * N_STATES)\n\n    # 4. Transition probabilities (A) estimation\n    transition_counts = np.zeros((N_STATES, N_STATES))\n    total_state_transitions = np.zeros(N_STATES)\n    for path in transition_paths:\n        for i in range(len(path) - 1):\n            s1_idx, s2_idx = STATE_MAP[path[i]], STATE_MAP[path[i+1]]\n            transition_counts[s1_idx, s2_idx] += 1\n            total_state_transitions[s1_idx] += 1\n\n    # Laplace smoothing for A and conversion to log-space\n    log_A = np.log(transition_counts + beta) - np.log(total_state_transitions.reshape(-1, 1) + beta * N_STATES)\n\n    return log_pi, log_A, log_B, kmer_map\n\ndef viterbi_decode(obs_seq, log_pi, log_A, log_B, kmer_map):\n    \"\"\"\n    Decodes the most likely state path for an observation sequence using the Viterbi algorithm.\n    \"\"\"\n    T = len(obs_seq)\n    \n    delta = np.zeros((T, N_STATES))\n    psi = np.zeros((T, N_STATES), dtype=int)\n\n    # Initialization step\n    first_obs_idx = kmer_map[obs_seq[0]]\n    delta[0, :] = log_pi + log_B[:, first_obs_idx]\n\n    # Recursion step\n    for t in range(1, T):\n        obs_idx = kmer_map[obs_seq[t]]\n        for j in range(N_STATES):  # current state\n            # Broadcasted calculation of max previous path probability\n            probs = delta[t-1, :] + log_A[:, j]\n            psi[t, j] = np.argmax(probs)\n            delta[t, j] = np.max(probs) + log_B[j, obs_idx]\n\n    # Termination and backtracking\n    path = np.zeros(T, dtype=int)\n    path[T-1] = np.argmax(delta[T-1, :])\n    for t in range(T-2, -1, -1):\n        path[t] = psi[t+1, path[t+1]]\n\n    # Convert state indices back to state labels\n    str_path = [STATES[i] for i in path]\n    return str_path\n\ndef solve():\n    # Common training data defined in the problem\n    emission_training_seqs = {\n        \"E\": [\"GCGCGCGCGCGCGCGCATGCGCGC\", \"CGCGCCGCGCGCGGCGCGC\"],\n        \"I\": [\"ATATATATATATATATATATTTATA\", \"AATAATATATAAATATATA\"],\n        \"G\": [\"AGCTAGCTAGCTAGCTAGCTAGCT\", \"GATCGATCGATCGATC\"]\n    }\n    transition_training_paths = [\n        \"GGGGGIIIIIEEEEEEEGGGGG\",\n        \"GGGIIIIIGGGEEE\",\n        \"EEEEEEIIIIGGGG\"\n    ]\n\n    # Test suite defined in the problem\n    test_cases = [\n        {'k': 2, 'alpha': 0.5, 'test_seq_parts': [(\"G\", \"AGCTAGCTAGC\"), (\"I\", \"ATATATATATAT\"), (\"E\", \"GCGCGCGCGC\")]},\n        {'k': 2, 'alpha': 0.5, 'test_seq_parts': [(\"I\", \"AT\")]},\n        {'k': 2, 'alpha': 1.0, 'test_seq_parts': [(\"G\", \"AGCTAGC\"), (\"E\", \"GCGCGC\"), (\"I\", \"ATATAT\")]},\n        {'k': 3, 'alpha': 0.5, 'test_seq_parts': [(\"G\", \"AGCTAGCT\"), (\"I\", \"ATATATAT\"), (\"E\", \"GCGCGCGC\")]}\n    ]\n\n    results = []\n    trained_models = {}\n\n    for case in test_cases:\n        k = case['k']\n        alpha = case['alpha']\n        beta = gamma = alpha\n        \n        # Memoize model training to avoid redundant computations\n        model_key = (k, alpha)\n        if model_key not in trained_models:\n            trained_models[model_key] = train_hmm(\n                emission_training_seqs, transition_training_paths, k, alpha, beta, gamma\n            )\n        log_pi, log_A, log_B, kmer_map = trained_models[model_key]\n\n        # Prepare test observation sequence\n        test_dna_seq = \"\".join([part[1] for part in case['test_seq_parts']])\n        obs_seq = generate_kmers(test_dna_seq, k)\n\n        # Generate ground-truth path\n        ground_truth_map = {}\n        current_pos = 0\n        for state, segment in case['test_seq_parts']:\n            for i in range(len(segment)):\n                ground_truth_map[current_pos + i] = state\n            current_pos += len(segment)\n        \n        num_obs = len(test_dna_seq) - k + 1 if len(test_dna_seq) >= k else 0\n        ground_truth = [ground_truth_map[i] for i in range(num_obs)]\n        \n        # Handle cases with no observations\n        if not obs_seq:\n            accuracy = 1.0 if not ground_truth else 0.0\n        else:\n            # Decode using Viterbi\n            predicted_path = viterbi_decode(obs_seq, log_pi, log_A, log_B, kmer_map)\n\n            # Calculate accuracy\n            correct_predictions = sum(p == g for p, g in zip(predicted_path, ground_truth))\n            accuracy = correct_predictions / len(ground_truth) if ground_truth else 1.0\n        \n        results.append(f\"{accuracy:.4f}\")\n\n    # Print a single line with the list of accuracies\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2434915"}, {"introduction": "Understanding the regulation of transcription is key to interpreting the functional output of the genome. In this exercise, you will step into the role of a data analyst to model the relationship between transcription factor binding, chromatin state, and gene expression levels derived from simulated functional genomics data. Crucially, this practice forces you to confront and quantify the statistical challenge of confounding, developing essential skills for drawing robust conclusions from complex, high-dimensional biological data [@problem_id:4613289].", "problem": "Consider the Central Dogma of Molecular Biology, which states that deoxyribonucleic acid (DNA) is transcribed into ribonucleic acid (RNA) and then translated into protein. At the level of transcription, for a gene indexed by $i$, let $r_i$ denote its transcription initiation rate (molecules per cell per unit time), $m_i$ its steady-state messenger ribonucleic acid (mRNA) abundance (molecules per cell), and $d_i$ its first-order degradation rate (per unit time). A well-tested steady-state relationship for mRNA kinetics is given by the differential equation\n$$\\frac{dm_i}{dt} = r_i - d_i m_i,$$\nwhich at steady state ($\\frac{dm_i}{dt}=0$) implies\n$$r_i = d_i m_i.$$\nAssume that properly normalized ribonucleic acid sequencing (RNA-seq) measurements $E_i$ are proportional to $m_i$. For this problem, take $E_i$ to be in the same arbitrary units as $m_i$ so that $E_i = m_i$. Assume Chromatin Immunoprecipitation followed by sequencing (ChIP-seq) provides a scalar occupancy feature $O_i$ that summarizes transcription factor binding near the gene promoter, and chromatin accessibility $A_i$ (for example from Assay for Transposase-Accessible Chromatin using sequencing, ATAC-seq) may act as a confounder that is associated with both $O_i$ and $r_i$. The latent transcription rate is modeled as\n$$\\log r_i = \\alpha + \\beta_O O_i + \\beta_A A_i + \\varepsilon_i,$$\nwhere $\\varepsilon_i$ are independent and identically distributed with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$.\n\nYou are to implement a program that, given simulated arrays $(O_i,A_i,E_i,d_i)$ for $i=1,\\dots,N$, will:\n- Use the steady-state relationship to infer $\\hat{r}_i$ from $E_i$ and $d_i$ via $\\hat{r}_i = d_i E_i$.\n- Fit two linear models by ordinary least squares (OLS):\n  1. Model $\\mathcal{M}_1$: regress $y_i = \\log \\hat{r}_i$ on $O_i$ with an intercept.\n  2. Model $\\mathcal{M}_2$: regress $y_i = \\log \\hat{r}_i$ on $O_i$ and $A_i$ with an intercept.\n- Use the first $\\lfloor 0.7N \\rfloor$ genes as the training set and the remaining genes as the test set for prediction evaluation. Angles are not used, so no angle units are required. No physical unit conversions are required in the output; all requested outputs are dimensionless.\n- Compute the following quantities for each test case:\n  1. The root mean squared error (RMSE) on the test set for $\\mathcal{M}_1$:\n     $$\\mathrm{RMSE}_1 = \\sqrt{\\frac{1}{n_{\\text{test}}} \\sum_{i \\in \\text{test}} \\left(y_i - \\hat{y}_{1,i}\\right)^2},$$\n     where $\\hat{y}_{1,i}$ are the predictions from $\\mathcal{M}_1$ and $n_{\\text{test}}$ is the number of test genes.\n  2. The RMSE on the test set for $\\mathcal{M}_2$:\n     $$\\mathrm{RMSE}_2 = \\sqrt{\\frac{1}{n_{\\text{test}}} \\sum_{i \\in \\text{test}} \\left(y_i - \\hat{y}_{2,i}\\right)^2},$$\n     where $\\hat{y}_{2,i}$ are the predictions from $\\mathcal{M}_2$.\n  3. The absolute change-in-estimate for the occupancy coefficient between the two models, computed on the training set:\n     $$\\Delta_{\\beta_O} = \\left|\\hat{\\beta}_O^{(2)} - \\hat{\\beta}_O^{(1)}\\right|,$$\n     where $\\hat{\\beta}_O^{(1)}$ is the coefficient of $O$ in $\\mathcal{M}_1$ and $\\hat{\\beta}_O^{(2)}$ is the coefficient of $O$ in $\\mathcal{M}_2$.\n  4. A boolean confounding flag defined as $\\mathrm{True}$ if $\\Delta_{\\beta_O} > \\tau$ and $\\mathrm{False}$ otherwise, where $\\tau$ is a specified threshold for that test case.\n  5. The partial coefficient of determination for $O$ given $A$ on the training set,\n     $$R^2_{O\\mid A} = \\frac{R^2_{\\text{full}} - R^2_{\\text{reduced}}}{1 - R^2_{\\text{reduced}}},$$\n     where $R^2_{\\text{full}}$ is from $\\mathcal{M}_2$ and $R^2_{\\text{reduced}}$ is from the reduced model regressing $y$ on $A$ with an intercept only. If numerical rounding yields values slightly outside $[0,1]$, clip to $[0,1]$.\n\nImplement OLS by minimizing the sum of squared residuals; do not rely on any prepackaged model-fitting utilities beyond basic linear algebra. The training/test split must be deterministic as specified. All logarithms are natural logarithms.\n\nTest suite. Your program must generate data internally for the following test cases, using independent Gaussian bases and the specified random seeds to ensure determinism. Let $Z_i \\sim \\mathcal{N}(0,1)$ and $O_i \\sim \\mathcal{N}(0,1)$ be independent, and construct $A_i = \\rho O_i + \\sqrt{1-\\rho^2} Z_i$. Let $\\log r_i = \\alpha + \\beta_O O_i + \\beta_A A_i + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$, and set $r_i = \\exp(\\log r_i)$. Draw $\\log d_i \\sim \\mathcal{N}(\\mu_d,\\sigma_d^2)$ and set $d_i = \\exp(\\log d_i)$. Finally, set $E_i = r_i / d_i$. For each case, provide $(N,\\text{seed},\\alpha,\\beta_O,\\beta_A,\\sigma,\\rho,\\mu_d,\\sigma_d,\\tau)$:\n\n- Case 1 (independent accessibility, moderate effects): (N=300, seed=13, alpha=1.0, beta_O=0.8, beta_A=0.5, sigma=0.2, rho=0.0, mu_d=-2.0, sigma_d=0.5, tau=0.1).\n- Case 2 (confounding via correlation): (N=300, seed=17, alpha=1.0, beta_O=0.8, beta_A=0.8, sigma=0.2, rho=0.8, mu_d=-2.0, sigma_d=0.5, tau=0.1).\n- Case 3 (stronger confounding, low noise): (N=200, seed=19, alpha=1.0, beta_O=0.8, beta_A=0.8, sigma=0.05, rho=0.95, mu_d=-2.0, sigma_d=0.7, tau=0.1).\n- Case 4 (no true occupancy effect, spurious association risk): (N=300, seed=23, alpha=1.0, beta_O=0.0, beta_A=0.8, sigma=0.2, rho=0.9, mu_d=-2.0, sigma_d=0.5, tau=0.1).\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case in the order above, output the quadruple $[\\mathrm{RMSE}_1,\\mathrm{RMSE}_2,\\text{flag},R^2_{O\\mid A}]$ and concatenate these quadruples into one flat list across all cases. For example, the output should look like\n$$[\\mathrm{RMSE}_{1,1},\\mathrm{RMSE}_{2,1},\\text{flag}_1,R^2_{O\\mid A,1},\\mathrm{RMSE}_{1,2},\\mathrm{RMSE}_{2,2},\\text{flag}_2,R^2_{O\\mid A,2},\\mathrm{RMSE}_{1,3},\\mathrm{RMSE}_{2,3},\\text{flag}_3,R^2_{O\\mid A,3},\\mathrm{RMSE}_{1,4},\\mathrm{RMSE}_{2,4},\\text{flag}_4,R^2_{O\\mid A,4}],$$\nwhere commas separate all entries and booleans appear as either True or False. No other text should be printed.", "solution": "The problem is scientifically and mathematically well-posed, providing a clear protocol to simulate and analyze a common statistical issue in genomics known as confounding. The problem is valid and a solution can be formulated.\n\nThe core of this problem lies in understanding and quantifying the effect of a confounding variable in a linear regression model, framed within the context of transcriptional regulation. The central dogma provides the biological foundation: deoxyribonucleic acid ($DNA$) is transcribed into messenger ribonucleic acid ($mRNA$), which is then translated into protein. The abundance of a specific $mRNA$ molecule, denoted $m_i$, is governed by its rates of synthesis (transcription initiation, $r_i$) and degradation ($d_i$). A common kinetic model is given by the ordinary differential equation $\\frac{dm_i}{dt} = r_i - d_i m_i$. At steady state, where the concentration of $mRNA$ is stable, $\\frac{dm_i}{dt} = 0$, leading to the fundamental relationship $r_i = d_i m_i$.\n\nThe problem states that we have measurements of $mRNA$ abundance ($E_i$, assumed equal to $m_i$), and the $mRNA$ degradation rate ($d_i$). From these, we can infer the transcription rate for each gene $i$ as $\\hat{r}_i = d_i E_i$. The problem posits that this transcription rate is influenced by regulatory factors. Specifically, the logarithm of the transcription rate is modeled as a linear function of transcription factor occupancy ($O_i$) and chromatin accessibility ($A_i$):\n$$ \\log r_i = \\alpha + \\beta_O O_i + \\beta_A A_i + \\varepsilon_i $$\nHere, $\\alpha$ is a basal transcription rate (as an intercept), $\\beta_O$ and $\\beta_A$ are the effects of occupancy and accessibility, respectively, and $\\varepsilon_i$ is a random noise term assumed to be normally distributed, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. Our task is to estimate these effects from simulated data.\n\nThe key statistical challenge is confounding. A confounder is a variable that is associated with both the predictor of interest (here, $O_i$) and the outcome (here, $y_i = \\log \\hat{r}_i$). In our simulation setup, chromatin accessibility $A_i$ is constructed to be correlated with occupancy $O_i$ through the parameter $\\rho$, expressed as $A_i = \\rho O_i + \\sqrt{1-\\rho^2} Z_i$, where $Z_i$ is an independent random variable. Since $A_i$ also directly influences $\\log r_i$ via the term $\\beta_A A_i$, it is a potential confounder.\n\nTo investigate this, we fit two linear models using Ordinary Least Squares (OLS). For any linear model $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$, the OLS estimate of the coefficient vector $\\boldsymbol{\\beta}$ is the one that minimizes the sum of squared residuals, given by the solution to the normal equations:\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y} $$\nwhere $\\mathbf{X}$ is the design matrix (including a column of ones for the intercept) and $\\mathbf{y}$ is the vector of outcomes.\n\nModel $\\mathcal{M}_1$ is a simple regression that omits the potential confounder:\n$$ y_i = \\log \\hat{r}_i = \\alpha^{(1)} + \\beta_O^{(1)} O_i + \\text{error}_1 $$\nIf $A_i$ is indeed a confounder (i.e., $\\rho \\neq 0$ and $\\beta_A \\neq 0$), the estimate $\\hat{\\beta}_O^{(1)}$ will be biased. It will incorrectly absorb some of the effect of the omitted variable $A_i$.\n\nModel $\\mathcal{M}_2$ is a multiple regression that includes the potential confounder:\n$$ y_i = \\log \\hat{r}_i = \\alpha^{(2)} + \\beta_O^{(2)} O_i + \\beta_A^{(2)} A_i + \\text{error}_2 $$\nBy including $A_i$ in the model, we can obtain an estimate, $\\hat{\\beta}_O^{(2)}$, that is adjusted for the effect of $A_i$. This estimate reflects the association of $O_i$ with the outcome, holding $A_i$ statistically constant.\n\nThe process for each test case is as follows:\n1.  **Data Simulation**: Generate vectors $(O_i, A_i, E_i, d_i)$ of length $N$ according to the specified stochastic process, ensuring reproducibility via a random seed.\n2.  **Data Preparation**: Calculate the outcome variable $y_i = \\log(d_i E_i)$. Split the data deterministically into a training set (the first $\\lfloor 0.7N \\rfloor$ samples) and a test set (the remainder).\n3.  **Model Fitting**: On the training set, fit models $\\mathcal{M}_1$ and $\\mathcal{M}_2$ using OLS to obtain coefficient estimates $(\\hat{\\alpha}^{(1)}, \\hat{\\beta}_O^{(1)})$ and $(\\hat{\\alpha}^{(2)}, \\hat{\\beta}_O^{(2)}, \\hat{\\beta}_A^{(2)})$.\n4.  **Performance Evaluation**:\n    - Use the fitted models to predict outcomes on the test set, yielding $\\hat{y}_{1,i}$ and $\\hat{y}_{2,i}$. Compute the Root Mean Squared Error (RMSE) for both models, $\\mathrm{RMSE}_1$ and $\\mathrm{RMSE}_2$, to assess their predictive accuracy on unseen data. A lower RMSE indicates better performance.\n5.  **Confounding Assessment**:\n    - Compute the change-in-estimate, $\\Delta_{\\beta_O} = \\left|\\hat{\\beta}_O^{(2)} - \\hat{\\beta}_O^{(1)}\\right|$. A large value suggests that $A_i$ was a significant confounder, as its inclusion in the model substantially changed the estimated effect of $O_i$. A boolean flag is set based on a threshold $\\tau$.\n    - Compute the partial coefficient of determination, $R^2_{O\\mid A}$. This metric quantifies the proportion of variance in $y_i$ that is explained by $O_i$ after the variance explained by $A_i$ has already been accounted for. It is calculated on the training data via the formula $R^2_{O\\mid A} = (R^2_{\\text{full}} - R^2_{\\text{reduced}}) / (1 - R^2_{\\text{reduced}})$, where $R^2_{\\text{full}}$ is the R-squared from model $\\mathcal{M}_2$ (regressing $y$ on $O$ and $A$) and $R^2_{\\text{reduced}}$ is from a model regressing $y$ on $A$ alone. This isolates the unique contribution of $O_i$.\n\nThis comprehensive analysis allows us to not only detect the presence of confounding but also to quantify its impact on both model coefficients and predictive power.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation and analysis for all test cases.\n    \"\"\"\n    test_cases = [\n        # (N, seed, alpha, beta_O, beta_A, sigma, rho, mu_d, sigma_d, tau)\n        (300, 13, 1.0, 0.8, 0.5, 0.2, 0.0, -2.0, 0.5, 0.1),\n        (300, 17, 1.0, 0.8, 0.8, 0.2, 0.8, -2.0, 0.5, 0.1),\n        (200, 19, 1.0, 0.8, 0.8, 0.05, 0.95, -2.0, 0.7, 0.1),\n        (300, 23, 1.0, 0.0, 0.8, 0.2, 0.9, -2.0, 0.5, 0.1),\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        N, seed, alpha, beta_O, beta_A, sigma, rho, mu_d, sigma_d, tau = case\n        \n        # 1. Data Generation\n        rng = np.random.default_rng(seed)\n        O = rng.normal(0, 1, N)\n        Z = rng.normal(0, 1, N)\n        A = rho * O + np.sqrt(1 - rho**2) * Z\n        epsilon = rng.normal(0, sigma, N)\n        \n        log_r = alpha + beta_O * O + beta_A * A + epsilon\n        r = np.exp(log_r)\n        \n        log_d = rng.normal(mu_d, sigma_d, N)\n        d = np.exp(log_d)\n        \n        E = r / d\n        \n        # 2. Data Preparation\n        r_hat = d * E\n        y = np.log(r_hat)\n        \n        n_train = int(np.floor(0.7 * N))\n        n_test = N - n_train\n        \n        # Training set\n        O_train, A_train, y_train = O[:n_train], A[:n_train], y[:n_train]\n        # Test set\n        O_test, A_test, y_test = O[n_train:], A[n_train:], y[n_train:]\n\n        def perform_ols(X, y_data):\n            \"\"\"Performs OLS regression using np.linalg.solve for stability.\"\"\"\n            # beta_hat = (X.T @ X)^-1 @ X.T @ y\n            try:\n                # More stable than inv()\n                beta_hat = np.linalg.solve(X.T @ X, X.T @ y_data)\n            except np.linalg.LinAlgError:\n                # Fallback for singular matrix\n                beta_hat = np.linalg.pinv(X.T @ X) @ X.T @ y_data\n            return beta_hat\n\n        # 3. Model Fitting on Training Data\n        \n        # Model 1: y ~ 1 + O\n        X1_train = np.c_[np.ones(n_train), O_train]\n        beta1_hat = perform_ols(X1_train, y_train)\n        \n        # Model 2: y ~ 1 + O + A (Full model)\n        X2_train = np.c_[np.ones(n_train), O_train, A_train]\n        beta2_hat = perform_ols(X2_train, y_train)\n\n        # 4. RMSE Calculation on Test Data\n        \n        # Predictions for Model 1\n        X1_test = np.c_[np.ones(n_test), O_test]\n        y1_pred = X1_test @ beta1_hat\n        rmse1 = np.sqrt(np.mean((y_test - y1_pred)**2))\n        \n        # Predictions for Model 2\n        X2_test = np.c_[np.ones(n_test), O_test, A_test]\n        y2_pred = X2_test @ beta2_hat\n        rmse2 = np.sqrt(np.mean((y_test - y2_pred)**2))\n        \n        # 5. Confounding Assessment (on training coefficients)\n        beta_O1 = beta1_hat[1]\n        beta_O2 = beta2_hat[1]\n        delta_beta_O = np.abs(beta_O2 - beta_O1)\n        confounding_flag = delta_beta_O > tau\n\n        # 6. Partial R^2 Calculation (on training data)\n        \n        # Reduced model for partial R^2: y ~ 1 + A\n        XA_train = np.c_[np.ones(n_train), A_train]\n        betaA_hat = perform_ols(XA_train, y_train)\n        \n        SST = np.sum((y_train - np.mean(y_train))**2)\n        \n        # R^2 for full model (M2)\n        y2_pred_train = X2_train @ beta2_hat\n        SSR_full = np.sum((y_train - y2_pred_train)**2)\n        R2_full = 1 - SSR_full / SST if SST > 0 else 1.0\n\n        # R^2 for reduced model\n        yA_pred_train = XA_train @ betaA_hat\n        SSR_reduced = np.sum((y_train - yA_pred_train)**2)\n        R2_reduced = 1 - SSR_reduced / SST if SST > 0 else 1.0\n        \n        # Partial R^2\n        denominator = 1 - R2_reduced\n        if np.isclose(denominator, 0):\n             # If reduced model explains all variance, O can't explain more\n            R2_partial = 0.0\n        else:\n            R2_partial = (R2_full - R2_reduced) / denominator\n        \n        R2_partial_clipped = np.clip(R2_partial, 0, 1)\n\n        all_results.extend([rmse1, rmse2, confounding_flag, R2_partial_clipped])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "4613289"}]}