{"hands_on_practices": [{"introduction": "Principal Component Analysis begins with understanding the relationships between variables. This foundational exercise guides you through the crucial first steps of preparing a clinical dataset for PCA, starting from raw measurements of different scales and types. By constructing the sample correlation matrix and computing its largest eigenvalue, you will practice the essential calculations that underpin the entire method and justify why standardization is critical for meaningful results [@problem_id:4598175].", "problem": "A research team is preparing to apply Principal Component Analysis (PCA) for clinical risk stratification in a cohort of adults. Consider a synthetic dataset of $n=6$ patients with the following variables recorded: fasting plasma glucose in $\\mathrm{mg/dL}$, systolic blood pressure in $\\mathrm{mmHg}$, and a binary diagnosis indicator of Type $2$ diabetes mellitus, where $1$ denotes diagnosed and $0$ denotes not diagnosed. The patient-level data are as follows (each tuple is $(\\text{glucose}, \\text{systolic}, \\text{diagnosis})$):\n$$(90, 110, 0),\\quad (100, 120, 0),\\quad (110, 130, 0),\\quad (120, 140, 1),\\quad (130, 150, 1),\\quad (140, 160, 1).$$\nTasks:\n- Justify the encoding of the diagnosis as $0$ and $1$ in the context of PCA on mixed clinical variables, and justify centering and scaling choices needed to construct a correlation-based PCA.\n- Using appropriate encoding and centering (and any additional preprocessing you justify as necessary to correctly form a correlation matrix), compute the sample correlation matrix for these three variables.\n- Compute the largest eigenvalue of the resulting correlation matrix, which corresponds to the variance explained by the first principal component in a correlation-based PCA.\n\nProvide the largest eigenvalue as a single closed-form analytic expression. No rounding is required and no units should be included in the final expression.", "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and self-contained. It presents a standard, albeit simplified, application of Principal Component Analysis (PCA) to a synthetic clinical dataset. The tasks involve justifying the methodology for mixed data types and performing the necessary calculations, which are mathematically sound and central to the topic.\n\nThe analysis proceeds in three stages:\n1.  Justification of the preprocessing steps for a correlation-based PCA on mixed data types.\n2.  Computation of the sample correlation matrix.\n3.  Calculation of the largest eigenvalue of the correlation matrix.\n\nLet the three variables be denoted as $X_1$ (fasting plasma glucose), $X_2$ (systolic blood pressure), and $X_3$ (diabetes diagnosis). The dataset consists of $n=6$ observations.\n\n**1. Justification of Preprocessing**\n\nPrincipal Component Analysis identifies orthogonal axes of maximum variance in a dataset. The variance of a variable, however, is dependent on its scale and units. The given variables, glucose and systolic blood pressure, are measured in different units ($\\mathrm{mg/dL}$ and $\\mathrm{mmHg}$, respectively) and have different typical ranges. If PCA were performed on the raw data (i.e., on the covariance matrix), the variable with the numerically largest variance would disproportionately dominate the first principal component, regardless of its actual importance in the data structure. To mitigate this, it is standard practice to standardize the data. This involves two steps for each variable: centering, by subtracting the sample mean, and scaling, by dividing by the sample standard deviation. Performing PCA on such standardized data is mathematically equivalent to performing PCA on the sample correlation matrix. This approach ensures that each variable has a mean of $0$ and a variance of $1$, placing them on an equal footing.\n\nThe third variable, the diagnosis indicator, is binary. Encoding it numerically as $0$ and $1$ is a common method to include categorical data in this type of analysis. When the Pearson correlation coefficient is computed between a continuous variable and a dichotomous variable coded as $0$ and $1$, the result is algebraically equivalent to the point-biserial correlation coefficient. This coefficient is a valid measure of the linear association between the continuous variable and the binary category, justifying the inclusion of the diagnosis variable in the correlation matrix.\n\n**2. Computation of the Sample Correlation Matrix**\n\nThe sample correlation matrix $R$ is a $3 \\times 3$ matrix where the entry $R_{ij}$ is the sample correlation coefficient between variables $X_i$ and $X_j$. The data vectors are:\n$$ X_1 = \\begin{pmatrix} 90 \\\\ 100 \\\\ 110 \\\\ 120 \\\\ 130 \\\\ 140 \\end{pmatrix}, \\quad X_2 = \\begin{pmatrix} 110 \\\\ 120 \\\\ 130 \\\\ 140 \\\\ 150 \\\\ 160 \\end{pmatrix}, \\quad X_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} $$\nFirst, we compute the sample means ($\\bar{x}_i$):\n$$ \\bar{x}_1 = \\frac{1}{6}(90+100+110+120+130+140) = \\frac{690}{6} = 115 $$\n$$ \\bar{x}_2 = \\frac{1}{6}(110+120+130+140+150+160) = \\frac{810}{6} = 135 $$\n$$ \\bar{x}_3 = \\frac{1}{6}(0+0+0+1+1+1) = \\frac{3}{6} = \\frac{1}{2} $$\nNext, we compute the sample variances ($s_i^2$) and standard deviations ($s_i$), using the unbiased estimator with a denominator of $n-1 = 5$:\n$$ s_1^2 = \\frac{1}{5} \\sum_{k=1}^{6} (x_{1k} - \\bar{x}_1)^2 = \\frac{1}{5}[(-25)^2 + (-15)^2 + (-5)^2 + 5^2 + 15^2 + 25^2] = \\frac{1}{5}[625+225+25+25+225+625] = \\frac{1750}{5} = 350 $$\n$$ s_1 = \\sqrt{350} $$\nThe data for $X_2$ is a simple linear transformation of $X_1$, specifically $X_2 = X_1 + 20$. Therefore, their centered vectors are identical, and their variances are equal.\n$$ s_2^2 = 350 \\implies s_2 = \\sqrt{350} $$\nFor $X_3$:\n$$ s_3^2 = \\frac{1}{5} \\sum_{k=1}^{6} (x_{3k} - \\bar{x}_3)^2 = \\frac{1}{5}[3 \\cdot (0 - \\frac{1}{2})^2 + 3 \\cdot (1 - \\frac{1}{2})^2] = \\frac{1}{5}[3 \\cdot \\frac{1}{4} + 3 \\cdot \\frac{1}{4}] = \\frac{1}{5}[\\frac{6}{4}] = \\frac{3}{10} $$\n$$ s_3 = \\sqrt{\\frac{3}{10}} $$\nThe sample correlation coefficient is $R_{ij} = \\frac{\\text{Cov}(X_i, X_j)}{s_i s_j}$, where $\\text{Cov}(X_i, X_j) = \\frac{1}{n-1}\\sum_{k=1}^{6} (x_{ik} - \\bar{x}_i)(x_{jk} - \\bar{x}_j)$. The diagonal elements $R_{ii}$ are all $1$.\nFor $R_{12}$: Since $X_2 = X_1 + 20$, the variables are perfectly linearly correlated. Thus, $R_{12} = 1$.\nFor $R_{13}$:\n$$ \\text{Cov}(X_1, X_3) = \\frac{1}{5} \\sum_{k=1}^{6} (x_{1k} - \\bar{x}_1)(x_{3k} - \\bar{x}_3) = \\frac{1}{5} [(-25)(-\\frac{1}{2}) + (-15)(-\\frac{1}{2}) + (-5)(-\\frac{1}{2}) + (5)(\\frac{1}{2}) + (15)(\\frac{1}{2}) + (25)(\\frac{1}{2})] $$\n$$ \\text{Cov}(X_1, X_3) = \\frac{1}{5} [\\frac{1}{2}(25+15+5) + \\frac{1}{2}(5+15+25)] = \\frac{1}{5} [\\frac{45}{2} + \\frac{45}{2}] = \\frac{45}{5} = 9 $$\n$$ R_{13} = \\frac{9}{s_1 s_3} = \\frac{9}{\\sqrt{350}\\sqrt{3/10}} = \\frac{9}{\\sqrt{350 \\cdot 0.3}} = \\frac{9}{\\sqrt{105}} $$\nFor $R_{23}$: Since the centered vectors for $X_1$ and $X_2$ are identical, $\\text{Cov}(X_2, X_3) = \\text{Cov}(X_1, X_3) = 9$. Also, $s_2=s_1$. Thus, $R_{23} = R_{13} = \\frac{9}{\\sqrt{105}}$.\nLet $c = \\frac{9}{\\sqrt{105}}$. The correlation matrix is:\n$$ R = \\begin{pmatrix} 1 & 1 & c \\\\ 1 & 1 & c \\\\ c & c & 1 \\end{pmatrix} $$\n\n**3. Calculation of the Largest Eigenvalue**\n\nWe need to find the eigenvalues $\\lambda$ of $R$ by solving the characteristic equation $\\det(R - \\lambda I) = 0$.\n$$ \\det \\begin{pmatrix} 1-\\lambda & 1 & c \\\\ 1 & 1-\\lambda & c \\\\ c & c & 1-\\lambda \\end{pmatrix} = 0 $$\nExpanding the determinant:\n$$ (1-\\lambda)((1-\\lambda)^2 - c^2) - 1( (1-\\lambda) - c^2 ) + c(c - c(1-\\lambda)) = 0 $$\n$$ (1-\\lambda)^3 - c^2(1-\\lambda) - (1-\\lambda) + c^2 + c^2(1 - 1 + \\lambda) = 0 $$\n$$ (1-\\lambda)^3 - (1-\\lambda) - c^2(1-\\lambda) + c^2 + \\lambda c^2 = 0 $$\n$$ (1-\\lambda)^3 - (1-\\lambda) - c^2 + \\lambda c^2 + c^2 + \\lambda c^2 = 0 $$\n$$ (1-\\lambda)^3 - (1-\\lambda) + 2\\lambda c^2 = 0 $$\nFactor out $(1-\\lambda)$:\n$$ (1-\\lambda)[(1-\\lambda)^2 - 1] + 2\\lambda c^2 = 0 $$\n$$ (1-\\lambda)[\\lambda^2 - 2\\lambda] + 2\\lambda c^2 = 0 $$\n$$ \\lambda(1-\\lambda)(\\lambda-2) + 2\\lambda c^2 = 0 $$\nOne solution is $\\lambda=0$. If $\\lambda \\neq 0$, we can divide by $\\lambda$:\n$$ (1-\\lambda)(\\lambda-2) + 2c^2 = 0 $$\n$$ -\\lambda^2 + 3\\lambda - 2 + 2c^2 = 0 $$\n$$ \\lambda^2 - 3\\lambda + (2 - 2c^2) = 0 $$\nUsing the quadratic formula to find the other two eigenvalues:\n$$ \\lambda = \\frac{3 \\pm \\sqrt{(-3)^2 - 4(1)(2-2c^2)}}{2} = \\frac{3 \\pm \\sqrt{9 - 8 + 8c^2}}{2} = \\frac{3 \\pm \\sqrt{1 + 8c^2}}{2} $$\nThe three eigenvalues are $\\lambda_1 = 0$, $\\lambda_2 = \\frac{3 - \\sqrt{1 + 8c^2}}{2}$, and $\\lambda_3 = \\frac{3 + \\sqrt{1 + 8c^2}}{2}$. The largest eigenvalue is $\\lambda_{max} = \\lambda_3$.\nNow, we substitute the value of $c^2$:\n$$ c^2 = \\left(\\frac{9}{\\sqrt{105}}\\right)^2 = \\frac{81}{105} = \\frac{27}{35} $$\nSubstituting $c^2$ into the expression for $\\lambda_{max}$:\n$$ \\lambda_{max} = \\frac{3 + \\sqrt{1 + 8\\left(\\frac{27}{35}\\right)}}{2} = \\frac{3 + \\sqrt{1 + \\frac{216}{35}}}{2} = \\frac{3 + \\sqrt{\\frac{35}{35} + \\frac{216}{35}}}{2} = \\frac{3 + \\sqrt{\\frac{251}{35}}}{2} $$\nThis is the final closed-form analytic expression for the largest eigenvalue.\nThe sum of the eigenvalues is $0 + \\frac{3 - \\sqrt{1 + 8c^2}}{2} + \\frac{3 + \\sqrt{1 + 8c^2}}{2} = \\frac{6}{2} = 3$, which correctly equals the trace of the correlation matrix, $\\text{Tr}(R) = 1+1+1=3$. The largest eigenvalue represents the variance explained by the first principal component derived from the correlation matrix.", "answer": "$$ \\boxed{\\frac{3 + \\sqrt{\\frac{251}{35}}}{2}} $$", "id": "4598175"}, {"introduction": "The assumptions of PCA are best met when data are symmetrically distributed, but clinical and biological measurements frequently exhibit strong right-skew. This practice addresses this common challenge by demonstrating the rationale and effect of a variance-stabilizing transformation. You will use first principles to justify applying a logarithmic transform and then calculate its concrete impact on the sample covariance, a key skill for robust data preprocessing [@problem_id:4598130].", "problem": "A research team is preparing to apply Principal Component Analysis (PCA) to a panel of dimensionless clinical indices that are constructed as fold-changes relative to patient-specific baselines. Many such indices are known to exhibit extreme right-skew and multiplicative error, consistent with a log-normal generative mechanism commonly observed for biochemical concentrations. The team wishes to justify a variance-stabilizing transformation before fitting PCA, and then quantify how the sample covariance between two such indices changes after transformation on a small numeric example.\n\nStarting from core definitions and well-tested facts, make the case for applying a monotone variance-stabilizing transform such as a logarithm or Box–Cox transformation to clinical indices with multiplicative noise. Then, for the following dimensionless fold-change data collected on $n=6$ patients,\n- Index $A$: $1$, $1$, $2$, $8$, $32$, $64$,\n- Index $B$: $0.5$, $1$, $2$, $8$, $24$, $48$,\n\ncompute the sample covariance between $A$ and $B$ and the sample covariance between $\\ln(A)$ and $\\ln(B)$, using the definition of sample covariance. Define the “change in sample covariance” as the difference\n$$\\Delta = \\operatorname{Cov}(\\ln(A),\\ln(B)) - \\operatorname{Cov}(A,B).$$\nRound your final numerical value of $\\Delta$ to four significant figures. The indices are dimensionless, so no physical units are required for the final answer.", "solution": "The problem requires a two-part response: first, a theoretical justification for applying a logarithmic transformation to the specified type of clinical data before using Principal Component Analysis (PCA); second, a specific calculation of the change in sample covariance after such a transformation for a given dataset.\n\n### Part 1: Justification for Logarithmic Transformation\n\nPrincipal Component Analysis is a technique that identifies the orthogonal axes (principal components) of maximum variance in a dataset. The first principal component is the linear combination of the original variables that captures the largest amount of variance in the data. Consequently, PCA is highly sensitive to the scales and variances of the input variables. A variable with a disproportionately large variance can dominate the first principal component, potentially obscuring the contributions of other variables and leading to a misinterpretation of the underlying data structure.\n\nThe problem states that the clinical indices exhibit extreme right-skew and multiplicative error, which is consistent with a log-normal generative mechanism. Let a random variable $X$ represent such an index. If $X$ is log-normally distributed, its natural logarithm, $Y = \\ln(X)$, is normally distributed. Let $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The expected value (mean) and variance of $X$ are given by:\n$$ \\mathbb{E}[X] = \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right) $$\n$$ \\operatorname{Var}(X) = \\left(\\exp(\\sigma^2) - 1\\right) \\exp\\left(2\\mu + \\sigma^2\\right) $$\nBy substituting the expression for the mean into the variance equation, we can see the relationship between them:\n$$ \\operatorname{Var}(X) = \\left(\\exp(\\sigma^2) - 1\\right) \\left(\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)\\right)^2 = \\left(\\exp(\\sigma^2) - 1\\right) (\\mathbb{E}[X])^2 $$\nThis shows that the variance of $X$ is proportional to the square of its mean, $\\operatorname{Var}(X) \\propto (\\mathbb{E}[X])^2$. The standard deviation is therefore proportional to the mean, $\\operatorname{SD}(X) \\propto \\mathbb{E}[X]$. This dependency of variance on the mean is a form of heteroscedasticity. In the context of PCA, variables with larger means will have disproportionately larger variances, causing them to unduly influence the analysis.\n\nA variance-stabilizing transformation is a function $g$ applied to the data such that the variance of the transformed variable $g(X)$ is approximately constant and independent of its mean. We can find an appropriate transformation using the Delta method, which provides a first-order approximation for the variance of a function of a random variable:\n$$ \\operatorname{Var}(g(X)) \\approx \\left(g'(\\mathbb{E}[X])\\right)^2 \\operatorname{Var}(X) $$\nwhere $g'$ is the first derivative of $g$. We seek a function $g$ such that $\\operatorname{Var}(g(X))$ is a constant, say $c^2$. Substituting the variance structure of $X$, and denoting $\\mu_X = \\mathbb{E}[X]$:\n$$ c^2 \\approx \\left(g'(\\mu_X)\\right)^2 \\left(k \\cdot \\mu_X^2\\right) $$\nwhere $k = \\exp(\\sigma^2) - 1$ is the constant of proportionality. To make the left side constant, we require:\n$$ g'(\\mu_X) \\propto \\frac{1}{\\mu_X} $$\nIntegrating this differential equation with respect to $\\mu_X$ yields:\n$$ g(\\mu_X) \\propto \\ln(\\mu_X) $$\nThus, the natural logarithm is the appropriate variance-stabilizing transformation for data with multiplicative error where the standard deviation is proportional to the mean. Applying a log transformation converts the multiplicative error structure to an additive one, stabilizes the variance, and mitigates the right-skew, making the data more symmetric and conforming better to the assumptions underlying PCA. This robustly justifies its use for the described clinical indices.\n\n### Part 2: Calculation of Change in Sample Covariance\n\nWe are given data for $n=6$ patients for two indices, $A$ and $B$:\n- Index $A$: $\\{1, 1, 2, 8, 32, 64\\}$\n- Index $B$: $\\{0.5, 1, 2, 8, 24, 48\\}$\n\nThe sample covariance between two sets of observations $\\{x_i\\}_{i=1}^n$ and $\\{y_i\\}_{i=1}^n$ is defined as:\n$$ \\operatorname{Cov}(X,Y) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) $$\nA computationally equivalent formula is $\\operatorname{Cov}(X,Y) = \\frac{1}{n-1} \\left( \\sum_{i=1}^n x_i y_i - n\\bar{x}\\bar{y} \\right)$. We will use this form for calculation.\n\n**Step 2.1: Compute $\\operatorname{Cov}(A,B)$**\n\nFirst, we calculate the sample means $\\bar{A}$ and $\\bar{B}$:\n$$ \\bar{A} = \\frac{1}{6}(1+1+2+8+32+64) = \\frac{108}{6} = 18 $$\n$$ \\bar{B} = \\frac{1}{6}(0.5+1+2+8+24+48) = \\frac{83.5}{6} $$\nNext, we calculate the sum of products $\\sum A_i B_i$:\n$$ \\sum_{i=1}^6 A_i B_i = (1)(0.5) + (1)(1) + (2)(2) + (8)(8) + (32)(24) + (64)(48) $$\n$$ \\sum_{i=1}^6 A_i B_i = 0.5 + 1 + 4 + 64 + 768 + 3072 = 3909.5 $$\nNow, we compute $\\operatorname{Cov}(A,B)$ with $n=6$:\n$$ \\operatorname{Cov}(A,B) = \\frac{1}{6-1} \\left( \\sum_{i=1}^6 A_i B_i - 6\\bar{A}\\bar{B} \\right) = \\frac{1}{5} \\left( 3909.5 - 6(18)\\left(\\frac{83.5}{6}\\right) \\right) $$\n$$ \\operatorname{Cov}(A,B) = \\frac{1}{5} (3909.5 - 18 \\cdot 83.5) = \\frac{1}{5} (3909.5 - 1503) = \\frac{2406.5}{5} = 481.3 $$\n\n**Step 2.2: Compute $\\operatorname{Cov}(\\ln(A), \\ln(B))$**\n\nLet $A' = \\ln(A)$ and $B' = \\ln(B)$. The transformed data are:\n- $A' = \\{\\ln(1), \\ln(1), \\ln(2), \\ln(8), \\ln(32), \\ln(64)\\} = \\{0, 0, \\ln(2), 3\\ln(2), 5\\ln(2), 6\\ln(2)\\}$\n- $B' = \\{\\ln(0.5), \\ln(1), \\ln(2), \\ln(8), \\ln(24), \\ln(48)\\} = \\{-\\ln(2), 0, \\ln(2), 3\\ln(2), \\ln(3)+3\\ln(2), \\ln(3)+4\\ln(2)\\}$\nThe sample means $\\bar{A'}$ and $\\bar{B'}$ are:\n$$ \\bar{A'} = \\frac{1}{6}(0+0+1+3+5+6)\\ln(2) = \\frac{15}{6}\\ln(2) = \\frac{5}{2}\\ln(2) $$\n$$ \\bar{B'} = \\frac{1}{6}(-\\ln(2) + 0 + \\ln(2) + 3\\ln(2) + \\ln(3)+3\\ln(2) + \\ln(3)+4\\ln(2)) = \\frac{1}{6}(10\\ln(2) + 2\\ln(3)) = \\frac{5}{3}\\ln(2) + \\frac{1}{3}\\ln(3) $$\nThe sum of products $\\sum A'_i B'_i$:\n$$ \\sum A'_i B'_i = (0) + (0) + (\\ln(2))(\\ln(2)) + (3\\ln(2))(3\\ln(2)) + (5\\ln(2))(\\ln(3)+3\\ln(2)) + (6\\ln(2))(\\ln(3)+4\\ln(2)) $$\n$$ \\sum A'_i B'_i = (\\ln(2))^2 + 9(\\ln(2))^2 + 5\\ln(2)\\ln(3) + 15(\\ln(2))^2 + 6\\ln(2)\\ln(3) + 24(\\ln(2))^2 $$\n$$ \\sum A'_i B'_i = (1+9+15+24)(\\ln(2))^2 + (5+6)\\ln(2)\\ln(3) = 49(\\ln(2))^2 + 11\\ln(2)\\ln(3) $$\nNow, we compute $\\operatorname{Cov}(\\ln(A), \\ln(B))$:\n$$ \\operatorname{Cov}(A', B') = \\frac{1}{5} \\left( \\sum_{i=1}^6 A'_i B'_i - 6\\bar{A'}\\bar{B'} \\right) $$\nThe term $6\\bar{A'}\\bar{B'}$ is:\n$$ 6\\bar{A'}\\bar{B'} = 6 \\left(\\frac{5}{2}\\ln(2)\\right) \\left(\\frac{5}{3}\\ln(2) + \\frac{1}{3}\\ln(3)\\right) = 15\\ln(2) \\left(\\frac{1}{3}(5\\ln(2) + \\ln(3))\\right) = 5\\ln(2)(5\\ln(2) + \\ln(3)) $$\n$$ 6\\bar{A'}\\bar{B'} = 25(\\ln(2))^2 + 5\\ln(2)\\ln(3) $$\nThe sum of cross-products of deviations is:\n$$ \\sum_{i=1}^6 (A'_i - \\bar{A'})(B'_i - \\bar{B'}) = (49(\\ln(2))^2 + 11\\ln(2)\\ln(3)) - (25(\\ln(2))^2 + 5\\ln(2)\\ln(3)) $$\n$$ = 24(\\ln(2))^2 + 6\\ln(2)\\ln(3) $$\nSo the covariance is:\n$$ \\operatorname{Cov}(\\ln(A), \\ln(B)) = \\frac{1}{5} \\left( 24(\\ln(2))^2 + 6\\ln(2)\\ln(3) \\right) $$\nUsing numerical values $\\ln(2) \\approx 0.693147$ and $\\ln(3) \\approx 1.098612$:\n$$ \\operatorname{Cov}(\\ln(A), \\ln(B)) \\approx \\frac{1}{5} (24(0.480453) + 6(0.761567)) = \\frac{1}{5}(11.530872 + 4.569402) = \\frac{16.100274}{5} \\approx 3.220055 $$\n\n**Step 2.3: Compute the Change in Sample Covariance $\\Delta$**\n\nThe change is defined as $\\Delta = \\operatorname{Cov}(\\ln(A),\\ln(B)) - \\operatorname{Cov}(A,B)$.\n$$ \\Delta \\approx 3.220055 - 481.3 = -478.079945 $$\nRounding the result to four significant figures:\n$$ \\Delta \\approx -478.1 $$", "answer": "$$\\boxed{-478.1}$$", "id": "4598130"}, {"introduction": "Building a reliable predictive model with PCA goes beyond just the algorithm; it requires rigorous and valid evaluation. This exercise confronts one of the most critical pitfalls in applied machine learning: data leakage during cross-validation. You will analyze several proposed pipelines to identify the correct, leakage-free procedure, a vital skill for producing trustworthy and generalizable results in any clinical modeling task [@problem_id:4598164].", "problem": "A clinical transcriptomics dataset consists of $n$ patients with $p$ messenger ribonucleic acid (mRNA) expression features, collected prior to therapy initiation. For each patient $i \\in \\{1,\\dots,n\\}$, a feature vector $\\mathbf{x}_i \\in \\mathbb{R}^p$ and a binary outcome label $y_i \\in \\{0,1\\}$ indicating progression within $2$ years are recorded. You intend to evaluate a Principal Component Analysis (PCA)-based classifier under $K$-fold Cross-Validation (CV), where $K \\ge 2$.\n\nFundamental definitions and facts to use:\n- Cross-Validation (CV) estimates generalization performance by partitioning data into disjoint training and validation folds and ensuring that any function fitted on the training fold is applied to the validation fold without using validation information for fitting.\n- Centering and scaling are defined by using training-only statistics: for a feature $j$, the training mean $\\hat{\\mu}_{\\mathcal{T},j}$ and standard deviation $\\hat{\\sigma}_{\\mathcal{T},j}$ yield the standardized coordinate $z_{i,j} = (x_{i,j} - \\hat{\\mu}_{\\mathcal{T},j}) / \\hat{\\sigma}_{\\mathcal{T},j}$.\n- Principal Component Analysis (PCA) computes eigenvectors of the sample covariance matrix $\\hat{\\Sigma}_{\\mathcal{T}}$ estimated on the training data to obtain a projection matrix $\\mathbf{V}_{\\mathcal{T}}$ and score vectors $\\mathbf{s}_i = \\mathbf{V}_{\\mathcal{T}}^\\top \\mathbf{z}_i$.\n- A leakage-free pipeline requires that every data-dependent transformation (e.g., imputation, centering, scaling, batch correction, PCA fitting, hyperparameter tuning) is fitted exclusively on the training fold and applied to the validation fold.\n\nConsider the following proposed pipelines and statements about data leakage. Select all options that correctly avoid data leakage and correctly characterize the leakage that arises from centering and scaling using full data.\n\nA. Compute centering and scaling parameters $\\hat{\\mu}_j$ and $\\hat{\\sigma}_j$ from all $n$ patients once, standardize all features, fit PCA on all standardized patients to obtain $\\mathbf{V}$, and then perform $K$-fold CV by training the classifier on fold-specific training PCA scores while evaluating on validation scores. This stabilizes preprocessing and avoids leakage because PCA and scaling are unsupervised.\n\nB. For each CV fold with training index set $\\mathcal{T}$ and validation index set $\\mathcal{V}$, fit missing value imputation parameters, centering $\\hat{\\mu}_{\\mathcal{T}}$, scaling $\\hat{\\sigma}_{\\mathcal{T}}$, and PCA projection $\\mathbf{V}_{\\mathcal{T}}$ using only $\\{\\mathbf{x}_i : i \\in \\mathcal{T}\\}$. If the number of components $k$ is to be tuned, perform an inner CV restricted to $\\mathcal{T}$ to select $k$. Transform $\\{\\mathbf{x}_i : i \\in \\mathcal{V}\\}$ using the training-fitted transformations and evaluate the classifier. This avoids leakage.\n\nC. Use leave-one-out CV with $K = n$. For each left-out patient $i$, compute centering and scaling parameters using all $n$ patients to stabilize estimates, fit PCA on the remaining $n-1$ patients, project the left-out patient onto the PCA components, and evaluate. Because PCA excludes the left-out patient, there is no leakage.\n\nD. To mitigate batch effects, fit a batch correction model such as empirical Bayes ComBat on all $n$ patients to remove site-specific offsets, then perform $K$-fold CV where, in each fold, centering, scaling, and PCA are fitted using only the training patients. Since batch correction is unsupervised, this preprocessing on all patients does not leak information.\n\nE. Implement a single pipeline mapping $\\phi_{\\mathcal{T}}$ that is fit only on $\\mathcal{T}$ within each CV fold and contains, in order, missing value imputation, centering, scaling, PCA, and classifier training. Choose hyperparameters (including the number of principal components $k$ and classifier regularization) by an inner CV that uses only $\\mathcal{T}$. Apply $\\phi_{\\mathcal{T}}$ to $\\mathcal{V}$ to obtain validation predictions. This avoids leakage and yields an unbiased CV estimate under independent and identically distributed sampling.\n\nYour task: Based on the definitions above and the requirement that validation data must not influence any fitted transformation, identify all options that are correct. In doing so, justify how leakage arises when centering and scaling are computed using full data, and what constitutes a proper CV pipeline for PCA-based models in clinical settings with $p \\gg n$ and potential batch effects.", "solution": "The problem statement is evaluated for validity.\n\n### Step 1: Extract Givens\n- **Dataset**: A clinical transcriptomics dataset with $n$ patients and $p$ messenger ribonucleic acid (mRNA) expression features.\n- **Feature Vectors**: $\\mathbf{x}_i \\in \\mathbb{R}^p$ for each patient $i \\in \\{1,\\dots,n\\}$.\n- **Outcome Labels**: $y_i \\in \\{0,1\\}$ for each patient, indicating a binary outcome (progression within $2$ years).\n- **Stated Task**: Evaluate a Principal Component Analysis (PCA)-based classifier under $K$-fold Cross-Validation (CV), where $K \\ge 2$.\n- **Definition of Cross-Validation (CV)**: Data is partitioned into disjoint training and validation folds. Functions are fitted on the training fold and applied to the validation fold. Information from the validation fold is not used for fitting.\n- **Definition of Centering and Scaling**: These transformations use training-only statistics. For a feature $j$, the training mean $\\hat{\\mu}_{\\mathcal{T},j}$ and standard deviation $\\hat{\\sigma}_{\\mathcal{T},j}$ are used to compute the standardized coordinate $z_{i,j} = (x_{i,j} - \\hat{\\mu}_{\\mathcal{T},j}) / \\hat{\\sigma}_{\\mathcal{T},j}$.\n- **Definition of PCA**: PCA computes eigenvectors of the sample covariance matrix $\\hat{\\Sigma}_{\\mathcal{T}}$ estimated on the training data. This yields a projection matrix $\\mathbf{V}_{\\mathcal{T}}$. The resulting score vectors are $\\mathbf{s}_i = \\mathbf{V}_{\\mathcal{T}}^\\top \\mathbf{z}_i$.\n- **Definition of a Leakage-Free Pipeline**: Every data-dependent transformation (e.g., imputation, centering, scaling, batch correction, PCA fitting, hyperparameter tuning) is fitted exclusively on the training fold ($\\mathcal{T}$) and subsequently applied to the validation fold ($\\mathcal{V}$).\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement describes a standard and critically important task in bioinformatics and machine learning: the proper evaluation of a predictive model using cross-validation.\n\n- **Scientifically Grounded**: The problem is firmly rooted in established principles of machine learning and statistical learning theory. The scenario—a high-dimensional ($p \\gg n$) transcriptomics dataset, binary clinical outcome, and the use of PCA for dimensionality reduction—is a canonical example in modern bioinformatics. The definitions provided for CV, scaling, PCA, and data leakage are standard and correct.\n- **Well-Posed**: The question is structured to have a definite answer. It asks to identify which of the proposed pipelines adhere to the provided, well-defined principles of a leakage-free CV process. The definitions are sufficient to unambiguously classify each option.\n- **Objective**: The language is technical, precise, and devoid of subjectivity. The definitions are formal and universally accepted in the field.\n\nThe problem statement does not violate any of the invalidity criteria. It is a valid, well-posed, and scientifically relevant problem that tests a crucial concept in applied machine learning methodology.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived by analyzing each option against the provided definitions.\n\n### Derivation and Option Analysis\n\nThe core principle of cross-validation is to estimate the performance of a model-building *procedure* on unseen data. This requires that the validation fold in each iteration must be treated as if it were truly unseen. Any step in the pipeline that \"learns\" or \"fits\" parameters from data—be it supervised or unsupervised—must do so using only the training data for that fold. Using any information from the validation fold during the training or preprocessing phase constitutes **data leakage**, which leads to an optimistically biased and therefore invalid estimate of generalization performance.\n\n**Characterization of Leakage from Full-Data Centering and Scaling:**\nWhen centering and scaling parameters (mean $\\hat{\\mu}_j$ and standard deviation $\\hat{\\sigma}_j$) are computed on the full dataset of $n$ patients, every data point contributes to these statistics. For any given CV fold, the validation set $\\mathcal{V}$ has contributed to the calculation of the mean and standard deviation that are used to transform the training set $\\mathcal{T}$. This violates the principle of withholding the validation set. The model is implicitly given information about the location and scale of the validation data distribution before it ever \"sees\" the validation set for evaluation. This makes the validation data less \"foreign\" to the trained model than truly new data would be, resulting in an artificially inflated performance estimate. The same logic applies to PCA, batch correction, or any other data-driven transformation. The argument that a method is \"unsupervised\" (i.e., does not use outcome labels $y_i$) is irrelevant, as the leakage pertains to the feature data $\\mathbf{x}_i$.\n\n**Option-by-Option Analysis:**\n\n**A. Compute centering and scaling parameters $\\hat{\\mu}_j$ and $\\hat{\\sigma}_j$ from all $n$ patients once, standardize all features, fit PCA on all standardized patients to obtain $\\mathbf{V}$, and then perform $K$-fold CV by training the classifier on fold-specific training PCA scores while evaluating on validation scores. This stabilizes preprocessing and avoids leakage because PCA and scaling are unsupervised.**\n\n- **Analysis**: This pipeline explicitly violates the leakage-free principle. Both centering/scaling and PCA are fitted using all $n$ patients *before* the CV process begins. This means for every fold, the training procedure (including the transformation of training data) has been influenced by the validation data. The justification that this is permissible because the methods are \"unsupervised\" is a common but profound fallacy. Data leakage occurs through the feature vectors $\\mathbf{x}_i$, not just the labels $y_i$.\n- **Verdict**: **Incorrect**.\n\n**B. For each CV fold with training index set $\\mathcal{T}$ and validation index set $\\mathcal{V}$, fit missing value imputation parameters, centering $\\hat{\\mu}_{\\mathcal{T}}$, scaling $\\hat{\\sigma}_{\\mathcal{T}}$, and PCA projection $\\mathbf{V}_{\\mathcal{T}}$ using only $\\{\\mathbf{x}_i : i \\in \\mathcal{T}\\}$. If the number of components $k$ is to be tuned, perform an inner CV restricted to $\\mathcal{T}$ to select $k$. Transform $\\{\\mathbf{x}_i : i \\in \\mathcal{V}\\}$ using the training-fitted transformations and evaluate the classifier. This avoids leakage.**\n\n- **Analysis**: This pipeline correctly describes the proper CV procedure. All data-dependent fitting steps (imputation, scaling, PCA, and hyperparameter tuning via inner CV) are strictly confined to the training set $\\mathcal{T}$ of each fold. The resulting transformations and tuned model are then applied to the held-out validation set $\\mathcal{V}$. This adheres perfectly to the definition of a leakage-free pipeline.\n- **Verdict**: **Correct**.\n\n**C. Use leave-one-out CV with $K = n$. For each left-out patient $i$, compute centering and scaling parameters using all $n$ patients to stabilize estimates, fit PCA on the remaining $n-1$ patients, project the left-out patient onto the PCA components, and evaluate. Because PCA excludes the left-out patient, there is no leakage.**\n\n- **Analysis**: This pipeline contains a critical flaw. While it correctly fits PCA on the $n-1$ training patients, it explicitly states that centering and scaling parameters are computed using all $n$ patients. This leaks information from the left-out patient into the preprocessing of the training data. The justification that leakage is avoided because the PCA step is correct is a misdirection that ignores the leakage from the scaling step. While the magnitude of the leakage from a single sample may be small, it is a violation of the principle, and the statement incorrectly claims there is no leakage.\n- **Verdict**: **Incorrect**.\n\n**D. To mitigate batch effects, fit a batch correction model such as empirical Bayes ComBat on all $n$ patients to remove site-specific offsets, then perform $K$-fold CV where, in each fold, centering, scaling, and PCA are fitted using only the training patients. Since batch correction is unsupervised, this preprocessing on all patients does not leak information.**\n\n- **Analysis**: This pipeline commits the same fundamental error as option A, but with batch correction. Batch correction models learn location and scale parameters from the data to harmonize distributions across batches. Fitting the model on all $n$ patients means that information from the validation set of any given fold is used to adjust the training set data (and vice-versa). This is a form of data leakage. The subsequent steps are described correctly, but the initial, global batch correction has already contaminated the CV procedure. The justification based on the \"unsupervised\" nature of the method is, again, fallacious.\n- **Verdict**: **Incorrect**.\n\n**E. Implement a single pipeline mapping $\\phi_{\\mathcal{T}}$ that is fit only on $\\mathcal{T}$ within each CV fold and contains, in order, missing value imputation, centering, scaling, PCA, and classifier training. Choose hyperparameters (including the number of principal components $k$ and classifier regularization) by an inner CV that uses only $\\mathcal{T}$. Apply $\\phi_{\\mathcal{T}}$ to $\\mathcal{V}$ to obtain validation predictions. This avoids leakage and yields an unbiased CV estimate under independent and identically distributed sampling.**\n\n- **Analysis**: This option provides a formal and complete description of a correct, leakage-free CV pipeline. It conceptualizes the entire model-building process as a single mapping $\\phi_{\\mathcal{T}}$ that is learned anew for each training fold $\\mathcal{T}$. This includes all preprocessing, hyperparameter tuning (via a correctly specified nested CV on $\\mathcal{T}$), and final model fitting. This composite function is then applied to the validation fold $\\mathcal{V}$. The conclusion that this process avoids leakage and yields an unbiased performance estimate (assuming i.i.d. data) is the primary goal and benefit of such a rigorous methodology. This statement is entirely correct.\n- **Verdict**: **Correct**.", "answer": "$$\\boxed{BE}$$", "id": "4598164"}]}