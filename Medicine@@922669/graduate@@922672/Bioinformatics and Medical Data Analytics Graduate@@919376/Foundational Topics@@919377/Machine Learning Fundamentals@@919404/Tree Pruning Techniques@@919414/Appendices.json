{"hands_on_practices": [{"introduction": "To build robust predictive models, we must prevent them from becoming overly complex and fitting noise in the training data. Cost-complexity pruning offers a principled way to do this by penalizing the number of leaves in a decision tree. This first exercise [@problem_id:4615665] provides direct practice with the core trade-off, requiring you to calculate the change in cost-complexity, which balances a change in impurity against a change in tree size, to determine if pruning a specific branch is advantageous for a given penalty parameter $\\alpha$.", "problem": "Consider a multiclass decision tree used to classify acute leukemia subtypes based on messenger ribonucleic acid (mRNA) expression profiles in bioinformatics and medical data analytics. At an internal node $\\tau$, a small subtree has $3$ terminal leaves. These leaves summarize training data with the following sample sizes ($N_t$) and class proportions $(p_{t,1}, p_{t,2}, p_{t,3})$ across three subtypes:\n- Leaf $\\mathcal{L}_1$: $N_{\\mathcal{L}_1} = 80$, $(0.55, 0.30, 0.15)$.\n- Leaf $\\mathcal{L}_2$: $N_{\\mathcal{L}_2} = 65$, $(0.20, 0.50, 0.30)$.\n- Leaf $\\mathcal{R}$: $N_{\\mathcal{R}} = 55$, $(0.60, 0.25, 0.15)$.\n\nAssume a Classification and Regression Trees (CART) framework with cost-complexity pruning. Let the impurity at a leaf $t$ be the Gini impurity, defined by $Q(t) = 1 - \\sum_{c=1}^{3} p_{t,c}^{2}$. The cost-complexity of a tree $T$ with terminal leaves $\\mathcal{L}(T)$ is\n$$\nR_{\\alpha}(T) = \\sum_{t \\in \\mathcal{L}(T)} N_{t} \\, Q(t) + \\alpha \\, |\\mathcal{L}(T)|,\n$$\nwhere $\\alpha$ is a nonnegative penalty parameter and $|\\mathcal{L}(T)|$ is the number of terminal leaves. Suppose we consider pruning the entire subtree rooted at $\\tau$ and replacing it with a single leaf whose class proportions equal the aggregated proportions of all samples currently in the subtree. Let the penalty parameter be $\\alpha = 5$.\n\nCompute the change in cost-complexity,\n$$\n\\Delta R_{\\alpha} = R_{\\alpha}(\\text{after pruning}) - R_{\\alpha}(\\text{before pruning}),\n$$\nfor this pruning decision at node $\\tau$. Use the aggregated class proportions computed from the given leaves to evaluate the single-leaf impurity after pruning. Round your final numerical answer to four significant figures. Express the final answer as a dimensionless real number.", "solution": "The problem is well-posed, scientifically grounded in the principles of decision tree algorithms (specifically, the CART framework), and provides all necessary information for a unique solution. The data and definitions are self-contained and consistent. Therefore, we proceed with the calculation.\n\nThe objective is to compute the change in cost-complexity, $\\Delta R_{\\alpha}$, resulting from pruning a subtree. This change is defined as:\n$$\n\\Delta R_{\\alpha} = R_{\\alpha}(\\text{after pruning}) - R_{\\alpha}(\\text{before pruning})\n$$\nThe cost-complexity $R_{\\alpha}(T)$ for a tree $T$ is given by:\n$$\nR_{\\alpha}(T) = \\sum_{t \\in \\mathcal{L}(T)} N_{t} \\, Q(t) + \\alpha \\, |\\mathcal{L}(T)|\n$$\nwhere $N_t$ is the number of samples in a terminal leaf $t$, $Q(t)$ is the Gini impurity at that leaf, $|\\mathcal{L}(T)|$ is the number of terminal leaves in the tree, and $\\alpha$ is the complexity parameter. The Gini impurity for a leaf $t$ with $C$ classes is $Q(t) = 1 - \\sum_{c=1}^{C} p_{t,c}^{2}$, where $p_{t,c}$ is the proportion of samples of class $c$ in leaf $t$. In this problem, we have $C=3$ classes.\n\nFirst, we calculate the cost-complexity of the subtree before pruning, denoted $R_{\\alpha}(\\text{before})$. The subtree has $3$ terminal leaves: $\\mathcal{L}_1$, $\\mathcal{L}_2$, and $\\mathcal{R}$. Thus, the number of leaves is $|\\mathcal{L}(\\text{before})| = 3$. The penalty parameter is given as $\\alpha = 5$.\n\nWe begin by computing the Gini impurity for each of the three leaves.\nFor leaf $\\mathcal{L}_1$, with proportions $(0.55, 0.30, 0.15)$:\n$$\nQ(\\mathcal{L}_1) = 1 - (0.55^{2} + 0.30^{2} + 0.15^{2}) = 1 - (0.3025 + 0.09 + 0.0225) = 1 - 0.415 = 0.585\n$$\nFor leaf $\\mathcal{L}_2$, with proportions $(0.20, 0.50, 0.30)$:\n$$\nQ(\\mathcal{L}_2) = 1 - (0.20^{2} + 0.50^{2} + 0.30^{2}) = 1 - (0.04 + 0.25 + 0.09) = 1 - 0.38 = 0.62\n$$\nFor leaf $\\mathcal{R}$, with proportions $(0.60, 0.25, 0.15)$:\n$$\nQ(\\mathcal{R}) = 1 - (0.60^{2} + 0.25^{2} + 0.15^{2}) = 1 - (0.36 + 0.0625 + 0.0225) = 1 - 0.445 = 0.555\n$$\nNext, we calculate the total weighted impurity for the subtree before pruning:\n$$\n\\sum_{t \\in \\{\\mathcal{L}_1, \\mathcal{L}_2, \\mathcal{R}\\}} N_t Q(t) = N_{\\mathcal{L}_1}Q(\\mathcal{L}_1) + N_{\\mathcal{L}_2}Q(\\mathcal{L}_2) + N_{\\mathcal{R}}Q(\\mathcal{R})\n$$\nUsing the given sample sizes $N_{\\mathcal{L}_1} = 80$, $N_{\\mathcal{L}_2} = 65$, and $N_{\\mathcal{R}} = 55$:\n$$\n\\sum N_t Q(t) = 80(0.585) + 65(0.62) + 55(0.555) = 46.8 + 40.3 + 30.525 = 117.625\n$$\nThe cost-complexity before pruning is:\n$$\nR_{\\alpha}(\\text{before}) = 117.625 + \\alpha |\\mathcal{L}(\\text{before})| = 117.625 + 5 \\times 3 = 117.625 + 15 = 132.625\n$$\n\nSecond, we calculate the cost-complexity after pruning, $R_{\\alpha}(\\text{after})$. After pruning, the entire subtree is replaced by a single leaf, which we will call $\\tau$. Thus, the number of leaves is $|\\mathcal{L}(\\text{after})| = 1$.\n\nThe total number of samples in this new leaf $\\tau$ is the sum of samples from the constituent leaves:\n$$\nN_{\\tau} = N_{\\mathcal{L}_1} + N_{\\mathcal{L}_2} + N_{\\mathcal{R}} = 80 + 65 + 55 = 200\n$$\nThe class proportions for leaf $\\tau$ are the aggregated proportions, calculated as a weighted average of the proportions of the original leaves, where the weights are the sample sizes. For each class $c \\in \\{1, 2, 3\\}$:\n$$\np_{\\tau,c} = \\frac{N_{\\mathcal{L}_1} p_{\\mathcal{L}_1,c} + N_{\\mathcal{L}_2} p_{\\mathcal{L}_2,c} + N_{\\mathcal{R}} p_{\\mathcal{R},c}}{N_{\\tau}}\n$$\n$$\np_{\\tau,1} = \\frac{80(0.55) + 65(0.20) + 55(0.60)}{200} = \\frac{44 + 13 + 33}{200} = \\frac{90}{200} = 0.45\n$$\n$$\np_{\\tau,2} = \\frac{80(0.30) + 65(0.50) + 55(0.25)}{200} = \\frac{24 + 32.5 + 13.75}{200} = \\frac{70.25}{200} = 0.35125\n$$\n$$\np_{\\tau,3} = \\frac{80(0.15) + 65(0.30) + 55(0.15)}{200} = \\frac{12 + 19.5 + 8.25}{200} = \\frac{39.75}{200} = 0.19875\n$$\nThe Gini impurity for the new leaf $\\tau$ is:\n$$\nQ(\\tau) = 1 - (p_{\\tau,1}^2 + p_{\\tau,2}^2 + p_{\\tau,3}^2) = 1 - (0.45^2 + 0.35125^2 + 0.19875^2)\n$$\n$$\nQ(\\tau) = 1 - (0.2025 + 0.1233765625 + 0.0395015625) = 1 - 0.365378125 = 0.634621875\n$$\nThe total weighted impurity for the pruned tree (the single leaf $\\tau$) is:\n$$\nN_{\\tau} Q(\\tau) = 200 \\times 0.634621875 = 126.924375\n$$\nThe cost-complexity after pruning is:\n$$\nR_{\\alpha}(\\text{after}) = N_{\\tau} Q(\\tau) + \\alpha |\\mathcal{L}(\\text{after})| = 126.924375 + 5 \\times 1 = 131.924375\n$$\n\nFinally, we compute the change in cost-complexity:\n$$\n\\Delta R_{\\alpha} = R_{\\alpha}(\\text{after}) - R_{\\alpha}(\\text{before}) = 131.924375 - 132.625 = -0.700625\n$$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $7, 0, 0, 6$. The subsequent digit is $2$, so we round down.\n$$\n\\Delta R_{\\alpha} \\approx -0.7006\n$$", "answer": "$$\\boxed{-0.7006}$$", "id": "4615665"}, {"introduction": "A single pruning decision depends on the penalty parameter $\\alpha$, but how do we choose the best value for $\\alpha$ to ensure our model generalizes well? This exercise [@problem_id:4615670] moves from a single calculation to a complete model selection procedure. You will apply the one-standard-error rule to cross-validation results, a standard and powerful technique for finding the simplest model whose performance is statistically comparable to the best-performing model.", "problem": "In a clinical genomics study that integrates Electronic Health Records (EHR) and whole-exome variants to predict 30-day readmission ($0$ for no readmission, $1$ for readmission), a Classification and Regression Trees (CART) classifier is trained and pruned using cost-complexity pruning. The cost-complexity pruning parameter is denoted by $\\alpha \\geq 0$, and increasing $\\alpha$ yields strictly simpler subtrees along a nested pruning path. To estimate generalization error, $K$-fold cross-validation (CV) is performed with $K = 10$, and the fold-wise misclassification error (expressed as a decimal fraction) is recorded for each $\\alpha$ on the pruning path. The following $\\alpha$ values and corresponding subtree sizes (number of terminal nodes) are considered:\n- $\\alpha$: $0$, $0.005$, $0.01$, $0.02$, $0.03$, $0.05$.\n- Subtree sizes: $22$, $16$, $10$, $8$, $6$, $4$.\n\nFor each $\\alpha$, the ten fold-wise misclassification errors are:\n- $\\alpha = 0$, subtree size $22$: $(0.150,\\;0.130,\\;0.148,\\;0.132,\\;0.146,\\;0.134,\\;0.144,\\;0.136,\\;0.142,\\;0.138)$.\n- $\\alpha = 0.005$, subtree size $16$: $(0.145,\\;0.125,\\;0.143,\\;0.127,\\;0.141,\\;0.129,\\;0.139,\\;0.131,\\;0.137,\\;0.133)$.\n- $\\alpha = 0.01$, subtree size $10$: $(0.150,\\;0.118,\\;0.146,\\;0.122,\\;0.142,\\;0.126,\\;0.138,\\;0.130,\\;0.134,\\;0.134)$.\n- $\\alpha = 0.02$, subtree size $8$: $(0.148,\\;0.126,\\;0.146,\\;0.128,\\;0.144,\\;0.130,\\;0.142,\\;0.132,\\;0.140,\\;0.134)$.\n- $\\alpha = 0.03$, subtree size $6$: $(0.153,\\;0.129,\\;0.151,\\;0.131,\\;0.149,\\;0.133,\\;0.147,\\;0.135,\\;0.145,\\;0.137)$.\n- $\\alpha = 0.05$, subtree size $4$: $(0.159,\\;0.129,\\;0.157,\\;0.131,\\;0.155,\\;0.133,\\;0.153,\\;0.135,\\;0.151,\\;0.137)$.\n\nStarting from the foundational definitions of cross-validation error estimation and cost-complexity pruning, use the one-standard-error rule to select the pruning parameter $\\alpha$ and report the corresponding subtree size. Specifically:\n1. For each $\\alpha$, compute the mean cross-validated misclassification error $\\mu(\\alpha)$ across the $10$ folds.\n2. Identify the $\\alpha^{\\star}$ that minimizes $\\mu(\\alpha)$ and compute the standard error of $\\mu(\\alpha^{\\star})$ as $SE(\\alpha^{\\star}) = s(\\alpha^{\\star}) / \\sqrt{K}$, where $s(\\alpha^{\\star})$ is the sample standard deviation of the fold-wise errors at $\\alpha^{\\star}$ and $K = 10$.\n3. Form the one-standard-error threshold $\\tau = \\mu(\\alpha^{\\star}) + SE(\\alpha^{\\star})$.\n4. Select the largest $\\alpha$ (i.e., the simplest subtree on the path) whose mean error $\\mu(\\alpha)$ is less than or equal to $\\tau$.\nReport your final answer as a row vector containing the selected $\\alpha$ and the corresponding subtree size. Express exact values; no rounding is required. No units should be included in the final answer.", "solution": "The problem requires the selection of an optimal cost-complexity pruning parameter, $\\alpha$, and its corresponding subtree size for a CART classifier using the one-standard-error rule. The process involves analyzing the $K$-fold cross-validation results provided for a set of $\\alpha$ values. Here, $K=10$.\n\nThe one-standard-error rule is a heuristic for model selection that aims to find the simplest model whose performance is statistically comparable to the best-performing model. The procedure is as follows:\n\n1.  For each value of the tuning parameter $\\alpha$, compute the mean cross-validated misclassification error, $\\mu(\\alpha)$.\n2.  Identify the parameter $\\alpha^{\\star}$ that results in the minimum mean cross-validated error, $\\mu(\\alpha^{\\star})$.\n3.  Calculate the standard error of the mean error at $\\alpha^{\\star}$, denoted as $SE(\\alpha^{\\star})$.\n4.  Find the simplest model (in this context, the one corresponding to the largest $\\alpha$) whose mean error is within one standard error of the minimum. That is, select the largest $\\alpha$ such that $\\mu(\\alpha) \\leq \\mu(\\alpha^{\\star}) + SE(\\alpha^{\\star})$.\n\nLet's execute these steps with the given data.\n\n**Step 1: Compute the mean cross-validated error $\\mu(\\alpha)$ for each $\\alpha$.**\n\nThe mean error $\\mu(\\alpha)$ is the average of the $K=10$ fold-wise misclassification errors for each $\\alpha$.\n\nFor $\\alpha = 0$:\n$\\mu(0) = \\frac{1}{10}(0.150+0.130+0.148+0.132+0.146+0.134+0.144+0.136+0.142+0.138) = \\frac{1.400}{10} = 0.140$\n\nFor $\\alpha = 0.005$:\n$\\mu(0.005) = \\frac{1}{10}(0.145+0.125+0.143+0.127+0.141+0.129+0.139+0.131+0.137+0.133) = \\frac{1.350}{10} = 0.135$\n\nFor $\\alpha = 0.01$:\n$\\mu(0.01) = \\frac{1}{10}(0.150+0.118+0.146+0.122+0.142+0.126+0.138+0.130+0.134+0.134) = \\frac{1.340}{10} = 0.134$\n\nFor $\\alpha = 0.02$:\n$\\mu(0.02) = \\frac{1}{10}(0.148+0.126+0.146+0.128+0.144+0.130+0.142+0.132+0.140+0.134) = \\frac{1.370}{10} = 0.137$\n\nFor $\\alpha = 0.03$:\n$\\mu(0.03) = \\frac{1}{10}(0.153+0.129+0.151+0.131+0.149+0.133+0.147+0.135+0.145+0.137) = \\frac{1.410}{10} = 0.141$\n\nFor $\\alpha = 0.05$:\n$\\mu(0.05) = \\frac{1}{10}(0.159+0.129+0.157+0.131+0.155+0.133+0.153+0.135+0.151+0.137) = \\frac{1.480}{10} = 0.148$\n\nSummary of mean errors $\\mu(\\alpha)$ and subtree sizes $|T_{\\alpha}|$:\n- $\\alpha=0, |T_{\\alpha}|=22: \\mu(0) = 0.140$\n- $\\alpha=0.005, |T_{\\alpha}|=16: \\mu(0.005) = 0.135$\n- $\\alpha=0.01, |T_{\\alpha}|=10: \\mu(0.01) = 0.134$\n- $\\alpha=0.02, |T_{\\alpha}|=8: \\mu(0.02) = 0.137$\n- $\\alpha=0.03, |T_{\\alpha}|=6: \\mu(0.03) = 0.141$\n- $\\alpha=0.05, |T_{\\alpha}|=4: \\mu(0.05) = 0.148$\n\n**Step 2: Identify $\\alpha^{\\star}$ and compute $SE(\\alpha^{\\star})$.**\n\nThe minimum mean error is $\\mu_{\\min} = 0.134$, which occurs at $\\alpha = 0.01$. Therefore, $\\alpha^{\\star} = 0.01$.\n\nNext, we compute the standard error of the mean for this $\\alpha^{\\star}$. The standard error is given by $SE(\\alpha^{\\star}) = \\frac{s(\\alpha^{\\star})}{\\sqrt{K}}$, where $s(\\alpha^{\\star})$ is the sample standard deviation of the fold-wise errors for $\\alpha^{\\star}$. The sample variance $s^2(\\alpha^{\\star})$ is calculated as:\n$$s^2(\\alpha^{\\star}) = \\frac{1}{K-1} \\sum_{k=1}^{K} (e_k - \\mu(\\alpha^{\\star}))^2$$\nFor $\\alpha^{\\star}=0.01$, the errors are $e_k = (0.150, 0.118, 0.146, 0.122, 0.142, 0.126, 0.138, 0.130, 0.134, 0.134)$ and the mean is $\\mu(0.01) = 0.134$. The number of folds is $K=10$.\n\nThe deviations from the mean $(e_k - \\mu)$ are:\n$(0.016, -0.016, 0.012, -0.012, 0.008, -0.008, 0.004, -0.004, 0, 0)$.\n\nThe sum of squared deviations is:\n$\\sum (e_k - \\mu)^2 = (0.016)^2 + (-0.016)^2 + (0.012)^2 + (-0.012)^2 + (0.008)^2 + (-0.008)^2 + (0.004)^2 + (-0.004)^2 + 0^2 + 0^2$\n$\\sum (e_k - \\mu)^2 = 2 \\times [ (0.016)^2 + (0.012)^2 + (0.008)^2 + (0.004)^2 ]$\n$\\sum (e_k - \\mu)^2 = 2 \\times [ 0.000256 + 0.000144 + 0.000064 + 0.000016 ]$\n$\\sum (e_k - \\mu)^2 = 2 \\times [ 0.000480 ] = 0.00096$\n\nThe sample variance is:\n$s^2(0.01) = \\frac{0.00096}{10-1} = \\frac{0.00096}{9}$\n\nThe standard error of the mean is:\n$SE(0.01) = \\sqrt{\\frac{s^2(0.01)}{K}} = \\sqrt{\\frac{0.00096/9}{10}} = \\sqrt{\\frac{0.00096}{90}}$\n\nTo express this as an exact value, we convert $0.00096$ to a fraction: $0.00096 = \\frac{96}{100000} = \\frac{6}{6250} = \\frac{3}{3125}$.\n$SE(0.01) = \\sqrt{\\frac{3/3125}{90}} = \\sqrt{\\frac{3}{3125 \\times 90}} = \\sqrt{\\frac{1}{3125 \\times 30}} = \\sqrt{\\frac{1}{93750}}$.\nTo simplify the radical, we factor the denominator: $93750 = 10 \\times 9375 = (2 \\times 5) \\times (3 \\times 5^5) = 2 \\times 3 \\times 5^6$.\n$SE(0.01) = \\sqrt{\\frac{1}{2 \\times 3 \\times 5^6}} = \\frac{1}{\\sqrt{5^6}\\sqrt{6}} = \\frac{1}{5^3\\sqrt{6}} = \\frac{1}{125\\sqrt{6}} = \\frac{\\sqrt{6}}{125 \\times 6} = \\frac{\\sqrt{6}}{750}$.\n\n**Step 3: Form the one-standard-error threshold $\\tau$.**\n\nThe threshold is $\\tau = \\mu(\\alpha^{\\star}) + SE(\\alpha^{\\star})$.\n$\\tau = 0.134 + \\frac{\\sqrt{6}}{750}$.\n\n**Step 4: Select the largest $\\alpha$ such that $\\mu(\\alpha) \\leq \\tau$.**\n\nWe compare each $\\mu(\\alpha)$ to the threshold $\\tau$.\n$\\tau \\approx 0.134 + \\frac{2.44949}{750} \\approx 0.134 + 0.003266 \\approx 0.137266$.\n\n- $\\mu(0) = 0.140$. Is $0.140 \\leq 0.134 + \\frac{\\sqrt{6}}{750}$? This is equivalent to $0.006 \\leq \\frac{\\sqrt{6}}{750}$, or $0.006 \\times 750 \\leq \\sqrt{6}$, which is $4.5 \\leq \\sqrt{6}$. Squaring both sides gives $20.25 \\leq 6$, which is false.\n\n- $\\mu(0.005) = 0.135$. Is $0.135 \\leq 0.134 + \\frac{\\sqrt{6}}{750}$? This is equivalent to $0.001 \\leq \\frac{\\sqrt{6}}{750}$, or $0.75 \\leq \\sqrt{6}$. Squaring both sides gives $0.5625 \\leq 6$, which is true.\n\n- $\\mu(0.01) = 0.134$. Is $0.134 \\leq 0.134 + \\frac{\\sqrt{6}}{750}$? This is equivalent to $0 \\leq \\frac{\\sqrt{6}}{750}$, which is true.\n\n- $\\mu(0.02) = 0.137$. Is $0.137 \\leq 0.134 + \\frac{\\sqrt{6}}{750}$? This is equivalent to $0.003 \\leq \\frac{\\sqrt{6}}{750}$, or $2.25 \\leq \\sqrt{6}$. Squaring both sides gives $5.0625 \\leq 6$, which is true.\n\n- $\\mu(0.03) = 0.141$. Is $0.141 \\leq 0.134 + \\frac{\\sqrt{6}}{750}$? This is equivalent to $0.007 \\leq \\frac{\\sqrt{6}}{750}$, or $5.25 \\leq \\sqrt{6}$. Squaring both sides gives $27.5625 \\leq 6$, which is false.\n\n- $\\mu(0.05) = 0.148$. Is $0.148 \\leq 0.134 + \\frac{\\sqrt{6}}{750}$? This is equivalent to $0.014 \\leq \\frac{\\sqrt{6}}{750}$, or $10.5 \\leq \\sqrt{6}$. Squaring both sides gives $110.25 \\leq 6$, which is false.\n\nThe values of $\\alpha$ that satisfy the condition $\\mu(\\alpha) \\leq \\tau$ are $0.005$, $0.01$, and $0.02$. The rule is to select the largest $\\alpha$ from this set, which corresponds to the simplest (most pruned) model. The largest $\\alpha$ is $0.02$.\n\nThe subtree size corresponding to $\\alpha = 0.02$ is given as $8$.\n\nThus, the selected parameter is $\\alpha=0.02$ and the corresponding subtree size is $8$. The final answer is to be reported as a row vector.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.02 & 8\n\\end{pmatrix}\n}\n$$", "id": "4615670"}, {"introduction": "Theoretical methods often meet practical challenges, and real-world data is rarely perfect. This final exercise [@problem_id:4615706] explores a critical issue frequently encountered in medical data analytics: class imbalance. By reasoning from first principles, you will diagnose why standard cross-validation can yield unstable error estimates when predicting rare events and understand why stratified sampling is an essential technique for stabilizing the pruning process.", "problem": "A clinical decision support system uses a binary decision tree to predict the presence of a disease in a cohort of $n$ patients, where the positive (diseased) class prevalence is $p$ and the negative (non-diseased) class prevalence is $1-p$. Cost-complexity pruning is applied to the fitted tree by selecting the pruning parameter $\\alpha$ that minimizes a Cross-Validation (CV) estimate of misclassification risk. The CV estimator aggregates per-fold misclassification risk at a fixed $\\alpha$ by summing class-specific misclassification rates weighted by the population prevalences, that is, the true risk is $E(\\alpha) = p \\,\\theta_{+}(\\alpha) + (1-p) \\,\\theta_{-}(\\alpha)$, where $\\theta_{+}(\\alpha)$ and $\\theta_{-}(\\alpha)$ are the class-conditional error probabilities on the positive and negative classes, respectively.\n\nAssume $p \\ll 1$, as is typical for rare diseases in bioinformatics and medical data analytics. In $k$-fold CV with non-stratified random partitioning, the number of positive examples $n_{+,i}$ in fold $i$ is random and can vary widely across folds, while the number of negative examples $n_{-,i}$ is comparatively large and stable. The per-fold risk estimator at a fixed $\\alpha$ can be written as $\\hat{E}_{i}(\\alpha) = p \\,\\hat{\\theta}_{+,i}(\\alpha) + (1-p)\\,\\hat{\\theta}_{-,i}(\\alpha)$, where $\\hat{\\theta}_{+,i}(\\alpha)$ and $\\hat{\\theta}_{-,i}(\\alpha)$ are empirical error rates computed on the held-out data in fold $i$. For a given fold $i$, conditional on the counts $n_{+,i}$ and $n_{-,i}$, the empirical class-specific errors can be treated as binomial proportions with variances that depend on $n_{+,i}$ and $n_{-,i}$.\n\nFrom first principles, starting with the definition of binomial proportion variance and the law of total variance, reason about how class imbalance ($p \\ll 1$) affects the across-fold variance of $\\hat{E}_{i}(\\alpha)$ and, consequently, the stability of the selected pruning parameter $\\alpha$ when minimizing the average CV risk. Then consider the effect of enforcing stratified $k$-fold CV with balanced folds, in which each fold has approximately $n_{+,i} \\approx n_{+}/k$ positives and $n_{-,i} \\approx n_{-}/k$ negatives, where $n_{+} = p n$ and $n_{-} = (1-p)n$.\n\nChoose the option that most accurately characterizes the effect of class imbalance on CV error variance and a scientifically sound procedure to improve the stability of pruning selection:\n\nA. With $p \\ll 1$, random $k$-fold Cross-Validation yields high across-fold variance of the estimated misclassification risk at fixed $\\alpha$ because the minority-class error estimate has variance that inflates when $n_{+,i}$ is small and random; stratified CV that fixes $n_{+,i} \\approx n_{+}/k$ in every fold reduces this variance and stabilizes pruning selection.\n\nB. Leave-One-Out Cross-Validation eliminates variance in the CV error estimate regardless of class imbalance, making pruning selection perfectly stable.\n\nC. Oversampling the majority class within each fold is sufficient to stabilize pruning selection because reducing randomness in $n_{-,i}$ addresses the dominant source of CV error variance under imbalance.\n\nD. Using stratified $k$-fold CV with per-class weighting $w_{+} = 1/p$ and $w_{-} = 1/(1-p)$ in the loss reduces both variance and bias in the CV error estimator, leading to more stable and fair pruning selection under class imbalance.\n\nE. Increasing $k$ to a large value without stratification always reduces the across-fold variance of CV error under class imbalance, ensuring stability.", "solution": "We begin from the definition of the misclassification risk under population prevalences, $E(\\alpha) = p\\,\\theta_{+}(\\alpha) + (1-p)\\,\\theta_{-}(\\alpha)$, where $\\theta_{+}(\\alpha)$ and $\\theta_{-}(\\alpha)$ are class-conditional error probabilities. In $k$-fold Cross-Validation (CV), the per-fold estimator at fixed $\\alpha$ is $\\hat{E}_{i}(\\alpha) = p \\,\\hat{\\theta}_{+,i}(\\alpha) + (1-p)\\,\\hat{\\theta}_{-,i}(\\alpha)$, where $\\hat{\\theta}_{+,i}(\\alpha)$ is the empirical error proportion on the positive class in fold $i$ and $\\hat{\\theta}_{-,i}(\\alpha)$ is the empirical error proportion on the negative class.\n\nFor a given fold $i$, conditional on $n_{+,i}$ positives and $n_{-,i}$ negatives in the held-out set, each $\\hat{\\theta}_{+,i}(\\alpha)$ is a binomial proportion with variance\n$$\n\\operatorname{Var}\\big(\\hat{\\theta}_{+,i}(\\alpha) \\mid n_{+,i}\\big) = \\frac{\\theta_{+}(\\alpha) \\big(1-\\theta_{+}(\\alpha)\\big)}{n_{+,i}},\n$$\nand similarly\n$$\n\\operatorname{Var}\\big(\\hat{\\theta}_{-,i}(\\alpha) \\mid n_{-,i}\\big) = \\frac{\\theta_{-}(\\alpha) \\big(1-\\theta_{-}(\\alpha)\\big)}{n_{-,i}}.\n$$\nAssuming the class-specific error counts are conditionally independent given the counts, the conditional variance of the per-fold risk estimator is\n$$\n\\operatorname{Var}\\big(\\hat{E}_{i}(\\alpha) \\mid n_{+,i}, n_{-,i}\\big) = p^{2} \\operatorname{Var}\\big(\\hat{\\theta}_{+,i}(\\alpha) \\mid n_{+,i}\\big) + (1-p)^{2} \\operatorname{Var}\\big(\\hat{\\theta}_{-,i}(\\alpha) \\mid n_{-,i}\\big),\n$$\nwhich yields\n$$\n\\operatorname{Var}\\big(\\hat{E}_{i}(\\alpha) \\mid n_{+,i}, n_{-,i}\\big) = p^{2} \\frac{\\theta_{+}(\\alpha)\\big(1-\\theta_{+}(\\alpha)\\big)}{n_{+,i}} + (1-p)^{2} \\frac{\\theta_{-}(\\alpha)\\big(1-\\theta_{-}(\\alpha)\\big)}{n_{-,i}}.\n$$\nBy the law of total variance,\n$$\n\\operatorname{Var}\\big(\\hat{E}_{i}(\\alpha)\\big) = \\mathbb{E}\\big[\\operatorname{Var}\\big(\\hat{E}_{i}(\\alpha)\\mid n_{+,i}, n_{-,i}\\big)\\big] + \\operatorname{Var}\\big(\\mathbb{E}\\big[\\hat{E}_{i}(\\alpha)\\mid n_{+,i}, n_{-,i}\\big]\\big).\n$$\nGiven that $\\mathbb{E}\\big[\\hat{\\theta}_{+,i}(\\alpha)\\mid n_{+,i}\\big] = \\theta_{+}(\\alpha)$ and $\\mathbb{E}\\big[\\hat{\\theta}_{-,i}(\\alpha)\\mid n_{-,i}\\big] = \\theta_{-}(\\alpha)$, the second term simplifies to zero for a fixed $\\alpha$ because $\\mathbb{E}\\big[\\hat{E}_{i}(\\alpha)\\mid n_{+,i}, n_{-,i}\\big] = p \\,\\theta_{+}(\\alpha) + (1-p)\\,\\theta_{-}(\\alpha)$ does not depend on $n_{+,i}$ or $n_{-,i}$. Therefore,\n$$\n\\operatorname{Var}\\big(\\hat{E}_{i}(\\alpha)\\big) = \\mathbb{E}\\left[p^{2} \\frac{\\theta_{+}(\\alpha)\\big(1-\\theta_{+}(\\alpha)\\big)}{n_{+,i}} + (1-p)^{2} \\frac{\\theta_{-}(\\alpha)\\big(1-\\theta_{-}(\\alpha)\\big)}{n_{-,i}}\\right].\n$$\nUnder non-stratified random partitioning, $n_{+,i}$ varies across folds. For large $n$ and moderate $k$, we can approximate $n_{+,i}$ as a binomial random variable $n_{+,i} \\sim \\mathrm{Bin}\\big(n/k, p\\big)$, so that the expectation $\\mathbb{E}\\big[1/n_{+,i}\\big]$ is dominated by the right tail near small $n_{+,i}$ when $p \\ll 1$. In practice, folds can have very few or even zero positives, and the term $p^{2} \\theta_{+}(\\alpha)\\big(1-\\theta_{+}(\\alpha)\\big)/n_{+,i}$ can become extremely large or ill-defined, inflating the across-fold variance of $\\hat{E}_{i}(\\alpha)$. Meanwhile, the negative-class term $(1-p)^{2} \\theta_{-}(\\alpha)\\big(1-\\theta_{-}(\\alpha)\\big)/n_{-,i}$ is relatively small because $n_{-,i} \\approx (1-p) n/k$ is large and more stable.\n\nThis heteroscedasticity directly impacts pruning selection. The selected $\\alpha$ minimizes the average CV risk across folds, $\\bar{E}(\\alpha) = \\frac{1}{k}\\sum_{i=1}^{k} \\hat{E}_{i}(\\alpha)$. If $\\hat{E}_{i}(\\alpha)$ exhibits high variance due to the minority-class term, the minimizer of $\\bar{E}(\\alpha)$ fluctuates substantially across repeated CV runs, and pruning selection is unstable.\n\nBy enforcing stratified $k$-fold CV with balanced folds, we fix $n_{+,i} \\approx n_{+}/k$ and $n_{-,i} \\approx n_{-}/k$ deterministically or with negligible variation. Then\n$$\n\\operatorname{Var}\\big(\\hat{E}_{i}(\\alpha)\\big) \\approx p^{2} \\frac{\\theta_{+}(\\alpha)\\big(1-\\theta_{+}(\\alpha)\\big)}{n_{+}/k} + (1-p)^{2} \\frac{\\theta_{-}(\\alpha)\\big(1-\\theta_{-}(\\alpha)\\big)}{n_{-}/k},\n$$\nwhich is minimized relative to the non-stratified case because the harmful variability in $1/n_{+,i}$ is removed, and the minority-class contribution now scales as $O\\big(k/n_{+}\\big)$. Consequently, the average CV risk $\\bar{E}(\\alpha)$ has smaller variance, and pruning selection is more stable.\n\nWe now evaluate each option:\n\nA. This option correctly identifies the mechanism: with $p \\ll 1$, the minority-class sample size $n_{+,i}$ is small and random across folds under non-stratified CV, inflating $\\operatorname{Var}\\big(\\hat{\\theta}_{+,i}(\\alpha)\\big)$ and, via the weighting by $p$, the variance of $\\hat{E}_{i}(\\alpha)$. It correctly states that stratified CV that approximately fixes $n_{+,i}$ across folds reduces this variance and stabilizes pruning selection. Verdict: Correct.\n\nB. Leave-One-Out Cross-Validation (LOOCV) does not eliminate variance; in fact, LOOCV typically has high variance in model selection settings. Under class imbalance, LOOCV yields test folds of size $1$ in which the presence of minority-class examples is very rare, exacerbating instability and producing highly variable per-fold estimates. Pruning selection is not perfectly stable. Verdict: Incorrect.\n\nC. Oversampling the majority class reduces randomness in $n_{-,i}$, which is already large and stable; it does not address the dominant source of variance arising from the small and variable $n_{+,i}$. Oversampling the majority would further skew class counts and can worsen imbalance effects. Verdict: Incorrect.\n\nD. Per-class weighting $w_{+} = 1/p$ and $w_{-} = 1/(1-p)$ changes the target loss to emphasize minority errors, which can reduce bias in the sense of aligning the estimator with a balanced objective. However, large weights on the minority class generally increase the variance contribution from $\\hat{\\theta}_{+,i}(\\alpha)$ unless minority sample sizes are stabilized and sufficiently large. Claiming that this procedure reduces both variance and bias is not generally valid; the variance may increase. Verdict: Incorrect.\n\nE. Increasing $k$ without stratification does not guarantee variance reduction. As $k$ grows, test folds shrink and the probability of very small $n_{+,i}$ increases under imbalance, often increasing the variance of per-fold estimates and destabilizing selection (extreme case: LOOCV). Verdict: Incorrect.", "answer": "$$\\boxed{A}$$", "id": "4615706"}]}