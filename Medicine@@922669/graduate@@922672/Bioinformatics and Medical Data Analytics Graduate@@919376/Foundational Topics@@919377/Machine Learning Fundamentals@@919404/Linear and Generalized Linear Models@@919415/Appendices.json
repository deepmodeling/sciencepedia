{"hands_on_practices": [{"introduction": "After fitting a generalized linear model, the immediate task is to interpret its coefficients in the context of the scientific question. For logistic regression, which models binary outcomes, the raw coefficient $\\beta$ on the log-odds scale is not directly intuitive. This exercise provides fundamental practice in translating such a coefficient into an odds ratio ($OR$), a widely used measure of effect size that quantifies the association between an exposure and an outcome. [@problem_id:4595172]", "problem": "A cohort study investigates whether a binary exposure $X$ (coded $X=1$ for exposed and $X=0$ for unexposed) is associated with the onset of a binary disease outcome $Y$ within one year, adjusting for a continuous covariate $Z$ (centered age). Researchers fit a Generalized Linear Model (GLM) with a binomial outcome and the canonical logit link, so that the conditional mean satisfies $g(\\mathbb{E}[Y \\mid X,Z]) = \\log\\!\\left(\\frac{\\Pr(Y=1 \\mid X,Z)}{1-\\Pr(Y=1 \\mid X,Z)}\\right)$, and the linear predictor is $\\eta = \\beta_{0} + \\beta_{1} X + \\beta_{2} Z$. The fitted coefficient for the exposure is $\\hat{\\beta}_{1} = 1.1$.\n\nUsing only the core definitions of odds and odds ratio (OR) under the logistic link and the structure of the model above, derive the exposure odds ratio comparing $X=1$ to $X=0$ at a fixed value of $Z$, as implied by this fitted model, and then compute its numerical value using $\\hat{\\beta}_{1} = 1.1$. Report your final numerical answer as a unitless ratio rounded to $3$ significant figures.", "solution": "The problem statement is subjected to validation before proceeding with a solution.\n\n### Step 1: Extract Givens\n-   **Model Type**: Generalized Linear Model (GLM).\n-   **Outcome**: Binary disease outcome $Y$. $Y=1$ for disease onset, $Y=0$ for no disease. The outcome follows a binomial distribution.\n-   **Exposure**: Binary exposure $X$, with $X=1$ for exposed and $X=0$ for unexposed.\n-   **Covariate**: Continuous covariate $Z$ (centered age).\n-   **Link Function**: Canonical logit link, $g(\\mu) = \\log\\!\\left(\\frac{\\Pr(Y=1 \\mid X,Z)}{1-\\Pr(Y=1 \\mid X,Z)}\\right)$.\n-   **Linear Predictor**: $\\eta = \\beta_{0} + \\beta_{1} X + \\beta_{2} Z$.\n-   **Fitted Coefficient**: The estimated coefficient for the exposure is $\\hat{\\beta}_{1} = 1.1$.\n-   **Task**: \n    1. Derive the exposure odds ratio (OR) comparing $X=1$ to $X=0$ at a fixed value of $Z$.\n    2. Compute the numerical value of this OR using the given fitted coefficient.\n    3. Report the result rounded to $3$ significant figures.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem describes a logistic regression model, which is a standard and fundamental tool in epidemiology and biostatistics for analyzing binary outcomes. The concepts of odds, odds ratios, logit link, and linear predictors are all standard and scientifically sound.\n-   **Well-Posed**: The problem is clearly defined. It provides the specific model structure and the necessary coefficient to derive and calculate the odds ratio. The question is unambiguous and has a unique, stable solution.\n-   **Objective**: The problem is stated in precise, formal, and unbiased mathematical and statistical language.\n-   **Complete and Consistent**: The information provided is sufficient and self-consistent. The model structure is fully specified for the purpose of deriving the requested odds ratio. No essential information is missing, and there are no contradictions.\n-   **Realistic**: The scenario of a cohort study examining an exposure-disease relationship while adjusting for a confounding variable like age is a classic and highly realistic application of GLMs in epidemiology. The coefficient value $\\hat{\\beta}_{1} = 1.1$ is a plausible magnitude for an effect estimate in such a study.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid as it is scientifically grounded, well-posed, objective, complete, consistent, and realistic. I will proceed with a full and reasoned solution.\n\nThe core of the problem is to derive the odds ratio (OR) for the exposure $X$ from the given logistic regression model. The model specifies the relationship between the predictors ($X$, $Z$) and the probability of the outcome ($Y=1$) via the logit link function.\n\nThe model equation states that the log-odds of the disease is a linear function of the predictors:\n$$\n\\log\\!\\left(\\frac{\\Pr(Y=1 \\mid X,Z)}{1-\\Pr(Y=1 \\mid X,Z)}\\right) = \\beta_{0} + \\beta_{1} X + \\beta_{2} Z\n$$\n\nThe odds of an event is defined as the probability of the event occurring divided by the probability of the event not occurring. In this context, the odds of disease for individuals with exposure status $X$ and age $Z$ is:\n$$\n\\text{Odds}(Y=1 \\mid X, Z) = \\frac{\\Pr(Y=1 \\mid X,Z)}{1-\\Pr(Y=1 \\mid X,Z)}\n$$\n\nBy taking the exponential of both sides of the model equation, we can express the odds directly:\n$$\n\\text{Odds}(Y=1 \\mid X, Z) = \\exp(\\beta_{0} + \\beta_{1} X + \\beta_{2} Z)\n$$\n\nThe odds ratio (OR) for the exposure compares the odds of disease in the exposed group ($X=1$) to the odds of disease in the unexposed group ($X=0$), while holding all other covariates constant. Here, we hold the covariate $Z$ at a fixed, arbitrary value.\nThe OR is defined as the ratio:\n$$\n\\text{OR} = \\frac{\\text{Odds}(Y=1 \\mid X=1, Z)}{\\text{Odds}(Y=1 \\mid X=0, Z)}\n$$\n\nWe now substitute the model-based expression for the odds into this ratio.\n\nFor the exposed group ($X=1$), the odds are:\n$$\n\\text{Odds}(Y=1 \\mid X=1, Z) = \\exp(\\beta_{0} + \\beta_{1}(1) + \\beta_{2} Z) = \\exp(\\beta_{0} + \\beta_{1} + \\beta_{2} Z)\n$$\n\nFor the unexposed group ($X=0$), the odds are:\n$$\n\\text{Odds}(Y=1 \\mid X=0, Z) = \\exp(\\beta_{0} + \\beta_{1}(0) + \\beta_{2} Z) = \\exp(\\beta_{0} + \\beta_{2} Z)\n$$\n\nNow, we form the ratio to find the odds ratio:\n$$\n\\text{OR} = \\frac{\\exp(\\beta_{0} + \\beta_{1} + \\beta_{2} Z)}{\\exp(\\beta_{0} + \\beta_{2} Z)}\n$$\n\nUsing the property of exponents, $\\frac{\\exp(a)}{\\exp(b)} = \\exp(a-b)$, we simplify the expression:\n$$\n\\text{OR} = \\exp\\left((\\beta_{0} + \\beta_{1} + \\beta_{2} Z) - (\\beta_{0} + \\beta_{2} Z)\\right)\n$$\n$$\n\\text{OR} = \\exp(\\beta_{0} + \\beta_{1} + \\beta_{2} Z - \\beta_{0} - \\beta_{2} Z)\n$$\n$$\n\\text{OR} = \\exp(\\beta_{1})\n$$\n\nThis derivation shows that in a logistic regression model with no interaction terms, the odds ratio for a unit change in a predictor variable (in this case, for $X$ changing from $0$ to $1$) is simply the exponential of the coefficient for that variable, $\\exp(\\beta_{1})$. The OR is constant across all levels of the other covariates in the model (in this case, $Z$).\n\nThe problem provides the fitted coefficient for the exposure, $\\hat{\\beta}_{1} = 1.1$. The estimated odds ratio is therefore:\n$$\n\\widehat{\\text{OR}} = \\exp(\\hat{\\beta}_{1}) = \\exp(1.1)\n$$\n\nWe are asked to compute this value and round it to $3$ significant figures.\n$$\n\\exp(1.1) \\approx 3.0041660239\n$$\nRounding this value to $3$ significant figures results in $3.00$. The first three significant figures are $3$, $0$, and $0$. The fourth digit is $4$, which is less than $5$, so we round down (i.e., keep the third significant figure as is).\n\nThus, the estimated odds ratio comparing exposed to unexposed individuals is $3.00$.", "answer": "$$\n\\boxed{3.00}\n$$", "id": "4595172"}, {"introduction": "Understanding how to interpret a model's output is crucial, but a deeper understanding comes from knowing how the model parameters are estimated. GLMs are typically fit by maximizing the log-likelihood function, a process handled by numerical optimization algorithms. This practice delves into the mathematical engine of logistic regression, guiding you through the derivation of the log-likelihood function $\\ell(\\boldsymbol{\\beta})$, its gradient (the score vector), and its second derivative (the Hessian matrix) from first principles. [@problem_id:4578844]", "problem": "In a case-control biomarker study for a binary disease outcome, suppose we observe $n$ independent individuals indexed by $i = 1, \\dots, n$. For each individual $i$, let $\\mathbf{x}_{i} \\in \\mathbb{R}^{p}$ denote a fixed $p$-dimensional covariate vector of bioinformatics features (for example, normalized expression measurements of $p$ genes), and let $y_{i} \\in \\{0,1\\}$ denote the disease status (where $y_{i} = 1$ indicates case and $y_{i} = 0$ indicates control). Assume a Generalized Linear Model (GLM) with Bernoulli responses and canonical logit link, that is, the conditional distribution of $Y_{i}$ given $\\mathbf{x}_{i}$ and parameter $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ is Bernoulli with mean $\\mu_{i} = \\Pr(Y_{i} = 1 \\mid \\mathbf{x}_{i}, \\boldsymbol{\\beta})$, and the linear predictor is $\\eta_{i} = \\mathbf{x}_{i}^{\\top}\\boldsymbol{\\beta}$ with the logistic mean function $\\mu_{i} = \\frac{1}{1 + \\exp(-\\eta_{i})}$. \n\nStarting only from the following fundamental base:\n- The Bernoulli probability mass function $p(y \\mid \\mu) = \\mu^{y}(1 - \\mu)^{1 - y}$ for $y \\in \\{0,1\\}$.\n- Independence of observations across individuals.\n- The canonical logit link $\\eta = \\ln\\left(\\frac{\\mu}{1-\\mu}\\right)$ and its inverse $\\mu = \\frac{1}{1 + \\exp(-\\eta)}$.\n\nDerive the full-sample log-likelihood function $\\ell(\\boldsymbol{\\beta})$ for this logistic regression model, expressed in terms of $\\{\\mathbf{x}_{i}, y_{i}\\}_{i=1}^{n}$ and $\\boldsymbol{\\beta}$, and then compute explicitly:\n1. The score vector (the gradient of the log-likelihood with respect to $\\boldsymbol{\\beta}$).\n2. The Hessian matrix (the matrix of second derivatives of the log-likelihood with respect to $\\boldsymbol{\\beta}$).\n\nYour derivation should proceed from the base facts above without invoking pre-formed logistic regression formulas. Express your final results using standard matrix notation by introducing the $n \\times p$ design matrix $\\mathbf{X}$ whose $i$-th row is $\\mathbf{x}_{i}^{\\top}$, the response vector $\\mathbf{y} = (y_{1}, \\dots, y_{n})^{\\top}$, the mean vector $\\boldsymbol{\\mu}(\\boldsymbol{\\beta}) = (\\mu_{1}, \\dots, \\mu_{n})^{\\top}$, and the diagonal weight matrix $\\mathbf{W}(\\boldsymbol{\\beta}) = \\mathrm{diag}\\big(\\mu_{i}(1 - \\mu_{i})\\big)$. Provide the three expressions in closed form. The final answer should be a single closed-form analytic expression. No rounding is required.", "solution": "The problem statement constitutes a standard, well-posed derivation in the theory of Generalized Linear Models (GLMs). It is scientifically sound, objective, and contains all necessary information for a unique solution. We may therefore proceed with the derivation.\n\nThe objective is to derive the log-likelihood function, its gradient (the score vector), and its Hessian matrix for a logistic regression model. We start from the provided fundamental principles.\n\n**1. Derivation of the Log-Likelihood Function $\\ell(\\boldsymbol{\\beta})$**\n\nFor a single observation $i$, the response variable $Y_i$ follows a Bernoulli distribution with parameter $\\mu_i = \\Pr(Y_i=1 \\mid \\mathbf{x}_i, \\boldsymbol{\\beta})$. The probability mass function (PMF) is given as $p(y_i \\mid \\mu_i) = \\mu_i^{y_i} (1 - \\mu_i)^{1 - y_i}$. Since the $n$ observations are independent, the total likelihood function for the parameter $\\boldsymbol{\\beta}$ is the product of the individual PMFs:\n$$ L(\\boldsymbol{\\beta}) = \\prod_{i=1}^{n} p(y_i \\mid \\mu_i(\\boldsymbol{\\beta})) = \\prod_{i=1}^{n} \\mu_i^{y_i} (1 - \\mu_i)^{1-y_i} $$\nThe log-likelihood function, $\\ell(\\boldsymbol{\\beta})$, is the natural logarithm of the likelihood function:\n$$ \\ell(\\boldsymbol{\\beta}) = \\ln(L(\\boldsymbol{\\beta})) = \\ln\\left(\\prod_{i=1}^{n} \\mu_i^{y_i} (1 - \\mu_i)^{1-y_i}\\right) = \\sum_{i=1}^{n} \\ln\\left(\\mu_i^{y_i} (1 - \\mu_i)^{1-y_i}\\right) $$\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[ y_i \\ln(\\mu_i) + (1-y_i)\\ln(1-\\mu_i) \\right] $$\nThis expression can be rearranged as:\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[ y_i \\ln(\\mu_i) - y_i \\ln(1-\\mu_i) + \\ln(1-\\mu_i) \\right] = \\sum_{i=1}^{n} \\left[ y_i \\ln\\left(\\frac{\\mu_i}{1-\\mu_i}\\right) + \\ln(1-\\mu_i) \\right] $$\nThe model uses the canonical logit link function, $\\eta_i = \\ln\\left(\\frac{\\mu_i}{1-\\mu_i}\\right)$, where the linear predictor is $\\eta_i = \\mathbf{x}_i^\\top\\boldsymbol{\\beta}$. Substituting $\\eta_i$ into the expression for $\\ell(\\boldsymbol{\\beta})$ gives:\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} [y_i \\eta_i + \\ln(1-\\mu_i)] $$\nTo express $\\ln(1-\\mu_i)$ in terms of $\\eta_i$, we use the inverse link function $\\mu_i = \\frac{1}{1 + \\exp(-\\eta_i)}$.\n$$ 1 - \\mu_i = 1 - \\frac{1}{1 + \\exp(-\\eta_i)} = \\frac{1 + \\exp(-\\eta_i) - 1}{1 + \\exp(-\\eta_i)} = \\frac{\\exp(-\\eta_i)}{1 + \\exp(-\\eta_i)} $$\nAn equivalent expression is obtained by multiplying the numerator and denominator by $\\exp(\\eta_i)$:\n$$ 1 - \\mu_i = \\frac{1}{ \\exp(\\eta_i) + 1 } $$\nTaking the natural logarithm, we get:\n$$ \\ln(1-\\mu_i) = \\ln\\left(\\frac{1}{1+\\exp(\\eta_i)}\\right) = -\\ln(1+\\exp(\\eta_i)) $$\nSubstituting this back into the expression for $\\ell(\\boldsymbol{\\beta})$ yields the final form of the log-likelihood function in terms of the linear predictor $\\eta_i = \\mathbf{x}_i^\\top\\boldsymbol{\\beta}$:\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[ y_i (\\mathbf{x}_i^\\top \\boldsymbol{\\beta}) - \\ln(1 + \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})) \\right] $$\n\n**2. Derivation of the Score Vector $\\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta})$**\n\nThe score vector is the gradient of the log-likelihood function with respect to the parameter vector $\\boldsymbol{\\beta}$. It is a $p \\times 1$ vector.\n$$ \\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) = \\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} \\left[ y_i (\\mathbf{x}_i^\\top \\boldsymbol{\\beta}) - \\ln(1 + \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})) \\right] $$\nWe use the chain rule for each term in the sum. Let $\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$. Then $\\frac{\\partial \\eta_i}{\\partial \\boldsymbol{\\beta}} = \\mathbf{x}_i$.\nThe derivative of the $i$-th term is:\n$$ \\frac{\\partial \\ell_i(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = y_i \\frac{\\partial(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}}[\\ln(1 + \\exp(\\eta_i))] $$\n$$ \\frac{\\partial \\ell_i(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = y_i \\mathbf{x}_i - \\frac{1}{1 + \\exp(\\eta_i)} \\cdot \\exp(\\eta_i) \\cdot \\frac{\\partial \\eta_i}{\\partial \\boldsymbol{\\beta}} $$\nRecognizing that $\\mu_i = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} = \\frac{1}{1 + \\exp(-\\eta_i)}$, we have:\n$$ \\frac{\\partial \\ell_i(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = y_i \\mathbf{x}_i - \\mu_i \\mathbf{x}_i = (y_i - \\mu_i) \\mathbf{x}_i $$\nSumming over all observations gives the score vector:\n$$ \\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mu_i(\\boldsymbol{\\beta})) \\mathbf{x}_i $$\nTo express this in matrix notation, let $\\mathbf{X}$ be the $n \\times p$ design matrix with rows $\\mathbf{x}_i^\\top$, $\\mathbf{y}$ be the $n \\times 1$ vector of outcomes, and $\\boldsymbol{\\mu}(\\boldsymbol{\\beta})$ be the $n \\times 1$ vector of means. The sum is equivalent to the matrix product:\n$$ \\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\mu}(\\boldsymbol{\\beta})) $$\n\n**3. Derivation of the Hessian Matrix $\\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta})$**\n\nThe Hessian matrix is the $p \\times p$ matrix of second partial derivatives of the log-likelihood function. It is obtained by differentiating the score vector with respect to $\\boldsymbol{\\beta}^\\top$.\n$$ \\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta}) = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^\\top} \\left( \\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) \\right) = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^\\top} \\left( \\sum_{i=1}^{n} (y_i - \\mu_i) \\mathbf{x}_i \\right) $$\n$$ \\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^\\top} \\left( y_i \\mathbf{x}_i - \\mu_i \\mathbf{x}_i \\right) $$\nSince $y_i$ and $\\mathbf{x}_i$ are fixed, the derivative of the first term $y_i\\mathbf{x}_i$ is zero. For the second term, we apply the product rule for vector calculus, noting that $\\mathbf{x}_i$ is not a function of $\\boldsymbol{\\beta}$:\n$$ \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^\\top}(-\\mu_i \\mathbf{x}_i) = - \\mathbf{x}_i \\frac{\\partial \\mu_i}{\\partial \\boldsymbol{\\beta}^\\top} $$\nWe need to find the derivative of $\\mu_i$ with respect to $\\boldsymbol{\\beta}^\\top$. We use the chain rule again: $\\frac{\\partial \\mu_i}{\\partial \\boldsymbol{\\beta}^\\top} = \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\boldsymbol{\\beta}^\\top}$. We have $\\frac{\\partial \\eta_i}{\\partial \\boldsymbol{\\beta}^\\top} = \\mathbf{x}_i^\\top$. The derivative of the mean function $\\mu_i$ with respect to the linear predictor $\\eta_i$ is:\n$$ \\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i}\\left(\\frac{1}{1+\\exp(-\\eta_i)}\\right) = - (1+\\exp(-\\eta_i))^{-2} \\cdot (\\exp(-\\eta_i)) \\cdot (-1) = \\frac{\\exp(-\\eta_i)}{(1+\\exp(-\\eta_i))^2} $$\nThis can be factored as:\n$$ \\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\left(\\frac{1}{1+\\exp(-\\eta_i)}\\right) \\left(\\frac{\\exp(-\\eta_i)}{1+\\exp(-\\eta_i)}\\right) = \\mu_i (1-\\mu_i) $$\nThus, the derivative of $\\mu_i$ with respect to $\\boldsymbol{\\beta}^\\top$ is:\n$$ \\frac{\\partial \\mu_i}{\\partial \\boldsymbol{\\beta}^\\top} = \\mu_i(1-\\mu_i) \\mathbf{x}_i^\\top $$\nSubstituting this into the expression for the $i$-th term of the Hessian gives:\n$$ \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^\\top}(-\\mu_i \\mathbf{x}_i) = - \\mathbf{x}_i (\\mu_i(1-\\mu_i)\\mathbf{x}_i^\\top) = - \\mu_i(1-\\mu_i) \\mathbf{x}_i \\mathbf{x}_i^\\top $$\nThis is a $p \\times p$ matrix. Summing over all observations yields the Hessian matrix:\n$$ \\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} - \\mu_i(1-\\mu_i) \\mathbf{x}_i \\mathbf{x}_i^\\top = - \\sum_{i=1}^{n} \\mu_i(1-\\mu_i) \\mathbf{x}_i \\mathbf{x}_i^\\top $$\nUsing the defined weight matrix $\\mathbf{W}(\\boldsymbol{\\beta}) = \\mathrm{diag}(\\mu_i(1-\\mu_i))$, this sum is the definition of the matrix product $-\\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}$.\n$$ \\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta}) = - \\mathbf{X}^\\top \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{X} $$\nThis matrix is central to the Newton-Raphson algorithm for fitting GLMs, where it is related to the Fisher information matrix.\n\nThe three requested expressions are:\n1.  Log-likelihood: $\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( y_i \\mathbf{x}_i^\\top \\boldsymbol{\\beta} - \\ln(1 + \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})) \\right)$\n2.  Score vector: $\\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\mu}(\\boldsymbol{\\beta}))$\n3.  Hessian matrix: $\\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta}) = - \\mathbf{X}^\\top \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{X}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\ell(\\boldsymbol{\\beta}) \\\\\n\\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) \\\\\n\\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta})\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\sum_{i=1}^{n} \\left( y_i \\mathbf{x}_i^\\top \\boldsymbol{\\beta} - \\ln(1 + \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})) \\right) \\\\\n\\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\mu}(\\boldsymbol{\\beta})) \\\\\n- \\mathbf{X}^\\top \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{X}\n\\end{pmatrix}\n}\n$$", "id": "4578844"}, {"introduction": "This final practice synthesizes theory and application in a comprehensive, real-world bioinformatics scenario involving count data, such as gene expression reads or clinical event counts. You will move beyond a single model fit to implement a complete computational workflow for a penalized Poisson regression model, which is essential for handling high-dimensional data. The exercise covers implementing the Iteratively Reweighted Least Squares (IRLS) algorithm, using cross-validation to select the optimal regularization parameter $\\lambda$, and applying the one-standard-error rule to balance the bias-variance trade-off. [@problem_id:5197921]", "problem": "You are to design, derive, and implement a complete procedure to estimate a penalized Poisson generalized linear model (GLM) by minimizing a regularized empirical risk and to evaluate this procedure using cross-validated deviance, selecting the regularization parameter with the one-standard-error rule. Your implementation must be a single, runnable program that takes no input and prints the required outputs. The setting is commonly encountered in artificial intelligence in medicine and data science, where count outcomes (for example, counts of clinical events) are modeled using Poisson regression.\n\nStarting from the fundamental base of the Poisson distribution and the generalized linear model framework:\n- The response $y_i$ for observation $i$ follows a Poisson distribution with mean parameter $\\mu_i$, and the canonical log link is used so that $\\log \\mu_i = \\eta_i$, where $\\eta_i = \\alpha + \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$ and $\\mathbf{x}_i \\in \\mathbb{R}^p$.\n- The goal is to estimate the intercept $\\alpha$ and coefficients $\\boldsymbol{\\beta}$ by minimizing a regularized empirical loss that combines the negative log-likelihood with an $\\ell_2$ (ridge) penalty on $\\boldsymbol{\\beta}$ only (the intercept is not penalized).\n\nYour tasks are as follows:\n1. From the Poisson mass function and the GLM canonical link definition, derive the negative log-likelihood for the model $y_i \\sim \\mathrm{Poisson}(\\mu_i)$ with $\\log \\mu_i = \\alpha + \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$, and from it derive the deviance as twice the difference between the saturated model log-likelihood and the fitted model log-likelihood. State the closed form of the per-observation deviance contribution for Poisson responses using only well-posed limits where needed.\n2. Derive the Iteratively Reweighted Least Squares (IRLS) updates for the unpenalized Poisson GLM with canonical link, then modify these to incorporate an $\\ell_2$ penalty $\\lambda \\lVert \\boldsymbol{\\beta} \\rVert_2^2/2$ that excludes the intercept term. Clearly articulate the linear system that must be solved at each IRLS iteration for a fixed $\\lambda$ in terms of the working response and weights.\n3. Design a $K$-fold cross-validation estimator for the expected out-of-sample deviance per observation. For a given $\\lambda$, the cross-validated deviance should be the mean of held-out deviances across folds, and the standard error should be the sample standard deviation across folds divided by $\\sqrt{K}$. Define the \"minimum-deviance\" choice $\\lambda_{\\mathrm{min}}$ and the \"one-standard-error\" choice $\\lambda_{\\mathrm{1SE}}$ as the largest $\\lambda$ whose mean deviance is within one standard error of the minimum mean deviance.\n4. Implement a complete algorithm that:\n   - For each fold, standardizes predictors using training-fold mean and standard deviation (replace zero standard deviations by $1$ to avoid division by zero). Do not standardize the response.\n   - Fits the penalized model across a grid of $\\lambda$ values using IRLS with warm starts across the path. Use an intercept that is not penalized.\n   - Computes the cross-validated deviance and its standard error for each $\\lambda$.\n   - Selects $\\lambda_{\\mathrm{min}}$ and $\\lambda_{\\mathrm{1SE}}$ and outputs them for each test case defined below.\n   - Uses numerically stable computations for the deviance, especially when $y_i = 0$, and clips linear predictors if needed to avoid overflow in the exponential map.\n5. Discuss, in concise and precise terms, the bias-variance trade-off induced by the $\\ell_2$ penalty in this model and explain why the one-standard-error rule is a variance-favoring selection with typically small increases in bias.\n\nNo physical units are involved. Do not use percentages in your computations; if proportions were needed, they must be decimal fractions, but here only real-valued floats are required.\n\nTest Suite:\nImplement your program to generate synthetic data according to the specifications below, then run your cross-validation and selection routine for each case. For each test case, the program must output the pair $[\\lambda_{\\mathrm{min}}, \\lambda_{\\mathrm{1SE}}]$.\n\n- Case A (happy path, moderate signal): \n  - $n=200$, $p=5$, seed $42$.\n  - True intercept $\\alpha = 1.0$, true coefficients $\\boldsymbol{\\beta} = [0.2, -0.5, 0.0, 0.3, 0.7]$.\n  - Predictors $\\mathbf{x}_i \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_p)$ independently, responses $y_i \\sim \\mathrm{Poisson}(\\exp(\\alpha + \\mathbf{x}_i^\\top \\boldsymbol{\\beta}))$ independently.\n  - Cross-validation: $K=5$ folds, deterministic fold assignment defined by shuffling with the given seed and then partitioning into contiguous blocks.\n  - Regularization grid: $\\lambda \\in \\{\\lambda_j\\}_{j=1}^{20}$ logarithmically spaced from $10^{-3}$ to $10^{1}$.\n\n- Case B (boundary with many zeros, smaller means, and mild sparsity):\n  - $n=120$, $p=8$, seed $123$.\n  - True intercept $\\alpha = 0.3$, true coefficients $\\boldsymbol{\\beta} = [-0.8, 0.0, 0.0, 0.5, 0.0, 0.2, 0.0, 0.0]$.\n  - Predictors and responses generated as in Case A.\n  - Cross-validation: $K=6$ folds as above.\n  - Regularization grid: $\\lambda \\in \\{\\lambda_j\\}_{j=1}^{30}$ logarithmically spaced from $10^{-4}$ to $10^{2}$.\n\n- Case C (high-dimensional, $p  n$, strong regularization needed):\n  - $n=60$, $p=80$, seed $777$.\n  - True intercept $\\alpha = 1.5$, true coefficients $\\boldsymbol{\\beta}$ are zero except for $5$ nonzero entries at fixed indices $[0, 5, 10, 20, 50]$ with values $[0.7, -0.6, 0.5, 0.4, -0.3]$.\n  - Predictors and responses generated as in Case A.\n  - Cross-validation: $K=5$ folds as above.\n  - Regularization grid: $\\lambda \\in \\{\\lambda_j\\}_{j=1}^{25}$ logarithmically spaced from $10^{-3}$ to $10^{3}$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Specifically, it must print a list of three lists, each inner list containing two floating-point values rounded to six decimal places, in the order of the test cases. The required format is:\n[[lambda_min_A,lambda_1se_A],[lambda_min_B,lambda_1se_B],[lambda_min_C,lambda_1se_C]]", "solution": "The posed problem requires the derivation, design, and implementation of a procedure for fitting a penalized Poisson generalized linear model (GLM) and selecting the regularization parameter via cross-validation. This is a standard and well-defined problem in statistical machine learning.\n\nThe response variable $y_i$ for an observation $i$ is assumed to follow a Poisson distribution, $y_i \\sim \\mathrm{Poisson}(\\mu_i)$. The mean $\\mu_i$ is related to a set of predictors $\\mathbf{x}_i \\in \\mathbb{R}^p$ through a canonical log link function, such that $\\log(\\mu_i) = \\eta_i = \\alpha + \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$. Here, $\\alpha$ is the intercept and $\\boldsymbol{\\beta}$ is a vector of $p$ coefficients.\n\nThe parameters $\\alpha$ and $\\boldsymbol{\\beta}$ are estimated by minimizing a penalized negative log-likelihood function. The penalty is an $\\ell_2$ (ridge) term on the coefficients $\\boldsymbol{\\beta}$, but not on the intercept $\\alpha$. The objective function to minimize is $J(\\alpha, \\boldsymbol{\\beta}) = -\\mathcal{L}(\\alpha, \\boldsymbol{\\beta}) + \\frac{\\lambda}{2} \\lVert \\boldsymbol{\\beta} \\rVert_2^2$, where $\\mathcal{L}$ is the log-likelihood and $\\lambda$ is the regularization parameter.\n\n**1. Negative Log-Likelihood and Deviance Derivation**\n\nThe probability mass function (PMF) for a single observation $y_i$ from a Poisson distribution with mean $\\mu_i$ is given by $P(y_i ; \\mu_i) = \\frac{e^{-\\mu_i} \\mu_i^{y_i}}{y_i!}$. The log-likelihood for this observation is $\\ell_i(\\mu_i; y_i) = \\log P(y_i ; \\mu_i) = y_i \\log \\mu_i - \\mu_i - \\log(y_i!)$. Using the link function $\\mu_i = e^{\\eta_i} = e^{\\alpha + \\mathbf{x}_i^\\top \\boldsymbol{\\beta}}$, the log-likelihood becomes $\\ell_i(\\alpha, \\boldsymbol{\\beta}; y_i, \\mathbf{x}_i) = y_i \\eta_i - e^{\\eta_i} - \\log(y_i!)$.\n\nFor a dataset of $n$ independent observations, the total log-likelihood is the sum $\\mathcal{L}(\\alpha, \\boldsymbol{\\beta}) = \\sum_{i=1}^n \\ell_i = \\sum_{i=1}^n (y_i \\eta_i - e^{\\eta_i} - \\log(y_i!))$. The negative log-likelihood (NLL) is $-\\mathcal{L}(\\alpha, \\boldsymbol{\\beta}) = \\sum_{i=1}^n (e^{\\eta_i} - y_i \\eta_i + \\log(y_i!))$. Since $\\log(y_i!)$ is constant with respect to the parameters, minimizing the NLL is equivalent to minimizing $\\sum_{i=1}^n (e^{\\eta_i} - y_i \\eta_i)$.\n\nThe deviance of a GLM is defined as $D = 2(\\mathcal{L}_{\\text{sat}} - \\mathcal{L}_{\\text{fit}})$, where $\\mathcal{L}_{\\text{sat}}$ is the log-likelihood of a saturated model and $\\mathcal{L}_{\\text{fit}}$ is that of the fitted model. In a saturated model, the mean parameter for each observation, $\\tilde{\\mu}_i$, is set to perfectly match the data, which for the Poisson distribution is $\\tilde{\\mu}_i = y_i$. The log-likelihood for the saturated model is $\\mathcal{L}_{\\text{sat}} = \\sum_{i=1}^n (y_i \\log y_i - y_i - \\log(y_i!))$. The fitted model has means $\\hat{\\mu}_i$ estimated from the GLM, with log-likelihood $\\mathcal{L}_{\\text{fit}} = \\sum_{i=1}^n (y_i \\log \\hat{\\mu}_i - \\hat{\\mu}_i - \\log(y_i!))$.\n\nThus, the total deviance is:\n$$ D = 2 \\left[ \\sum_{i=1}^n (y_i \\log y_i - y_i) - \\sum_{i=1}^n (y_i \\log \\hat{\\mu}_i - \\hat{\\mu}_i) \\right] = 2 \\sum_{i=1}^n \\left( y_i \\log\\left(\\frac{y_i}{\\hat{\\mu}_i}\\right) - (y_i - \\hat{\\mu}_i) \\right) $$\nThe per-observation deviance contribution is $d_i = 2 ( y_i \\log(y_i/\\hat{\\mu}_i) - (y_i - \\hat{\\mu}_i) )$. For cases where $y_i=0$, we use the well-posed limit $\\lim_{y \\to 0} y \\log y = 0$. The deviance contribution for an observation with $y_i=0$ simplifies to $d_i = 2\\hat{\\mu}_i$.\n\n**2. Penalized Iteratively Reweighted Least Squares (IRLS) Derivation**\n\nThe IRLS algorithm is a procedure for finding the maximum likelihood estimates of a GLM, equivalent to a Newton-Raphson method. We extend it to the penalized case. Let $\\boldsymbol{\\theta} = [\\alpha, \\boldsymbol{\\beta}^\\top]^\\top$ be the $(p+1)$-dimensional vector of parameters, and let $\\mathbf{X}^*$ be the $n \\times (p+1)$ design matrix with an initial column of ones, so $\\boldsymbol{\\eta} = \\mathbf{X}^* \\boldsymbol{\\theta}$.\n\nThe objective function to minimize is $J(\\boldsymbol{\\theta}) = \\sum_{i=1}^n (e^{\\eta_i} - y_i \\eta_i) + \\frac{\\lambda}{2} \\boldsymbol{\\beta}^\\top \\boldsymbol{\\beta}$.\nThe Newton-Raphson update for $\\boldsymbol{\\theta}$ at iteration $(t)$ is $\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - (\\mathbf{H}_J^{(t)})^{-1} \\nabla J^{(t)}$, where $\\nabla J$ is the gradient and $\\mathbf{H}_J$ is the Hessian of the objective function.\n\nThe gradient of the NLL part is $\\nabla(-\\mathcal{L}) = (\\mathbf{X}^*)^\\top(\\boldsymbol{\\mu} - \\mathbf{y})$. The Hessian of the NLL part is $\\mathbf{H}_{-\\mathcal{L}} = (\\mathbf{X}^*)^\\top \\mathbf{W} \\mathbf{X}^*$, where $\\mathbf{W}$ is a diagonal matrix with entries $W_{ii} = \\mu_i$.\nLet $\\mathbf{\\Lambda}$ be a $(p+1) \\times (p+1)$ diagonal matrix with $\\Lambda_{00}=0$ and $\\Lambda_{jj}=\\lambda$ for $j=1, \\dots, p$. The gradient of the penalty term is $\\mathbf{\\Lambda}\\boldsymbol{\\theta}$, and its Hessian is $\\mathbf{\\Lambda}$.\nThe total gradient is $\\nabla J(\\boldsymbol{\\theta}) = (\\mathbf{X}^*)^\\top(\\boldsymbol{\\mu} - \\mathbf{y}) + \\mathbf{\\Lambda}\\boldsymbol{\\theta}$.\nThe total Hessian is $\\mathbf{H}_J(\\boldsymbol{\\theta}) = (\\mathbf{X}^*)^\\top \\mathbf{W} \\mathbf{X}^* + \\mathbf{\\Lambda}$.\n\nSubstituting these into the Newton-Raphson formula and rearranging leads to the IRLS update. At each step, we solve for $\\boldsymbol{\\theta}^{(t+1)}$ in the linear system:\n$$ (\\mathbf{H}_J^{(t)}) \\boldsymbol{\\theta}^{(t+1)} = (\\mathbf{H}_J^{(t)}) \\boldsymbol{\\theta}^{(t)} - \\nabla J^{(t)} $$\n$$ \\left((\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{X}^* + \\mathbf{\\Lambda}\\right) \\boldsymbol{\\theta}^{(t+1)} = \\left((\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{X}^* + \\mathbf{\\Lambda}\\right) \\boldsymbol{\\theta}^{(t)} - \\left((\\mathbf{X}^*)^{\\top}(\\boldsymbol{\\mu}^{(t)} - \\mathbf{y}) + \\mathbf{\\Lambda}\\boldsymbol{\\theta}^{(t)}\\right) $$\n$$ \\left((\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{X}^* + \\mathbf{\\Lambda}\\right) \\boldsymbol{\\theta}^{(t+1)} = (\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{X}^* \\boldsymbol{\\theta}^{(t)} + (\\mathbf{X}^*)^{\\top}(\\mathbf{y} - \\boldsymbol{\\mu}^{(t)}) $$\nThis can be expressed as a penalized weighted least squares problem by defining a \"working response\" vector $\\mathbf{z}^{(t)}$ with components $z_i^{(t)} = \\eta_i^{(t)} + \\frac{y_i - \\mu_i^{(t)}}{\\mu_i^{(t)}}$. The right-hand side of the update equation is equivalent to $(\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{z}^{(t)}$. Thus, the linear system to solve at each iteration is:\n$$ \\left((\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{X}^* + \\mathbf{\\Lambda}\\right) \\boldsymbol{\\theta}^{(t+1)} = (\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{z}^{(t)} $$\n\n**3. Cross-Validation Design and Lambda Selection**\n\nA $K$-fold cross-validation procedure is employed to estimate the out-of-sample deviance for models fitted with different values of $\\lambda$.\n1. The data is partitioned into $K$ disjoint folds.\n2. For each fold $k=1, \\dots, K$, the model is trained on the remaining $K-1$ folds and validated on fold $k$.\n3. For each train/validation split, predictors in the training set are standardized to have zero mean and unit variance. The calculated mean and standard deviation are then used to standardize the validation set predictors.\n4. For each $\\lambda$ in a predefined grid, a model is fit on the training data, and the mean per-observation deviance is computed on the validation data. Let this be $d_k(\\lambda)$.\n5. After iterating through all folds, the cross-validated deviance for each $\\lambda$ is the mean of the fold-specific deviances: $\\mathrm{CV}(\\lambda) = \\frac{1}{K}\\sum_{k=1}^K d_k(\\lambda)$.\n6. The standard error is also computed: $\\mathrm{SE}(\\lambda) = \\sqrt{\\frac{1}{K(K-1)} \\sum_{k=1}^K (d_k(\\lambda) - \\mathrm{CV}(\\lambda))^2}$.\n7. The optimal $\\lambda$ values are chosen based on two rules:\n   - $\\lambda_{\\mathrm{min}}$: The value of $\\lambda$ that yields the minimum cross-validated deviance, $\\mathrm{CV}_{\\min} = \\mathrm{CV}(\\lambda_{\\mathrm{min}})$.\n   - $\\lambda_{\\mathrm{1SE}}$: The largest value of $\\lambda$ (most regularized) for which the cross-validated deviance is within one standard error of the minimum, i.e., $\\mathrm{CV}(\\lambda) \\le \\mathrm{CV}_{\\min} + \\mathrm{SE}(\\lambda_{\\min})$.\n\n**4. Implementation Notes**\n\nThe implementation requires careful handling of numerical details. The linear predictor $\\eta_i$ should be clipped before applying the exponential function to prevent overflow. Warm starts, where the solution for a given $\\lambda$ is used as the initial guess for the next $\\lambda$ in the grid, significantly accelerate convergence of the IRLS algorithm.\n\n**5. Bias-Variance Trade-off and the One-Standard-Error Rule**\n\nThe $\\ell_2$ penalty introduces a bias-variance trade-off. By shrinking coefficient estimates $\\boldsymbol{\\beta}$ toward zero, the penalty adds bias to the model, as the true coefficients are unlikely to be zero. However, this shrinkage reduces the variance of the estimates, making the model less sensitive to the specific sample of training data. For an optimal $\\lambda$, the reduction in variance outweighs the increase in bias, leading to improved predictive performance on unseen data. The one-standard-error rule is a heuristic that favors simpler, more regularized models. It selects the most parsimonious model ($\\lambda_{\\mathrm{1SE}} \\ge \\lambda_{\\min}$) that has a predictive performance statistically indistinguishable from the best model identified by cross-validation. This is a variance-favoring strategy, as it accepts a small, statistically insignificant increase in prediction error (bias) in return for a model that is likely more stable and generalizable (lower variance).", "answer": "```python\nimport numpy as np\nfrom scipy.special import xlogy\n\ndef make_beta_c():\n    \"\"\"Creates the specific beta vector for Test Case C.\"\"\"\n    beta = np.zeros(80)\n    indices = [0, 5, 10, 20, 50]\n    values = [0.7, -0.6, 0.5, 0.4, -0.3]\n    beta[indices] = values\n    return beta\n\ndef generate_data(spec):\n    \"\"\"Generates synthetic data for a given test case specification.\"\"\"\n    rng = np.random.default_rng(spec['seed'])\n    X = rng.normal(size=(spec['n'], spec['p']))\n    eta = spec['alpha'] + X @ spec['beta']\n    mu = np.exp(np.clip(eta, -30, 30)) # Clip for numerical stability\n    y = rng.poisson(mu)\n    return X, y\n\ndef create_folds(n, K, seed):\n    \"\"\"Creates deterministic K-fold cross-validation indices.\"\"\"\n    rng = np.random.default_rng(seed)\n    indices = np.arange(n)\n    rng.shuffle(indices)\n    fold_indices = np.array_split(indices, K)\n    folds = []\n    for k in range(K):\n        val_idx = fold_indices[k]\n        train_idx = np.concatenate([fold_indices[i] for i in range(K) if i != k])\n        folds.append((train_idx, val_idx))\n    return folds\n\ndef standardize(X_train, X_val):\n    \"\"\"Standardizes training and validation sets based on training set statistics.\"\"\"\n    means = np.mean(X_train, axis=0)\n    stds = np.std(X_train, axis=0)\n    stds[stds == 0] = 1.0  # As per problem, avoid division by zero\n    X_train_std = (X_train - means) / stds\n    X_val_std = (X_val - means) / stds\n    return X_train_std, X_val_std\n\ndef poisson_deviance(y, mu):\n    \"\"\"Computes the per-observation Poisson deviance.\"\"\"\n    mu = np.maximum(mu, 1e-15)  # Avoid log(0)\n    # Deviance = 2 * (y*log(y/mu) - (y-mu))\n    term1 = xlogy(y, y) - xlogy(y, mu)\n    term2 = mu - y\n    return 2 * (term1 + term2)\n\ndef fit_penalized_poisson(X_aug, y, lam, initial_theta, max_iter=100, tol=1e-7):\n    \"\"\"\n    Fits a penalized Poisson GLM using Iteratively Reweighted Least Squares (IRLS).\n    \"\"\"\n    p_plus_1 = X_aug.shape[1]\n    theta = np.copy(initial_theta)\n    \n    # Penalty matrix (intercept is not penalized)\n    Lambda = np.diag([0.0] + [lam] * (p_plus_1 - 1))\n    \n    for _ in range(max_iter):\n        eta = X_aug @ theta\n        eta = np.clip(eta, -30, 30) # Prevent overflow/underflow\n        mu = np.exp(eta)\n        \n        # Ensure mu is positive to avoid division by zero in working response\n        mu = np.maximum(mu, 1e-8)\n        \n        W_diag = mu  # Diagonal of the weight matrix W\n        z = eta + (y - mu) / mu  # Working response\n        \n        # Efficiently compute (X_aug.T @ W @ X_aug + Lambda) and (X_aug.T @ W @ z)\n        X_t_W = X_aug.T * W_diag  # Broadcasting for X_aug.T @ W\n        \n        A = X_t_W @ X_aug + Lambda\n        b = X_t_W @ z\n        \n        try:\n            theta_new = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Fallback for ill-conditioned matrices\n            theta_new = np.linalg.pinv(A) @ b\n            \n        # Check for convergence using relative change\n        if np.linalg.norm(theta_new - theta) / (np.linalg.norm(theta) + 1e-8)  tol:\n            theta = theta_new\n            break\n            \n        theta = theta_new\n        \n    return theta\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        {\n            'n': 200, 'p': 5, 'seed': 42, 'alpha': 1.0, \n            'beta': np.array([0.2, -0.5, 0.0, 0.3, 0.7]), \n            'K': 5, 'lambda_grid': np.logspace(-3, 1, 20)\n        },\n        {\n            'n': 120, 'p': 8, 'seed': 123, 'alpha': 0.3, \n            'beta': np.array([-0.8, 0.0, 0.0, 0.5, 0.0, 0.2, 0.0, 0.0]), \n            'K': 6, 'lambda_grid': np.logspace(-4, 2, 30)\n        },\n        {\n            'n': 60, 'p': 80, 'seed': 777, 'alpha': 1.5, \n            'beta': make_beta_c(), \n            'K': 5, 'lambda_grid': np.logspace(-3, 3, 25)\n        }\n    ]\n    \n    all_final_results = []\n\n    for case in test_cases:\n        X, y = generate_data(case)\n        folds = create_folds(case['n'], case['K'], case['seed'])\n        \n        num_lambdas = len(case['lambda_grid'])\n        fold_deviances = np.zeros((case['K'], num_lambdas))\n\n        for k in range(case['K']):\n            train_idx, val_idx = folds[k]\n            X_train, y_train = X[train_idx], y[train_idx]\n            X_val, y_val = X[val_idx], y[val_idx]\n            \n            X_train_std, X_val_std = standardize(X_train, X_val)\n            \n            X_train_aug = np.c_[np.ones(X_train_std.shape[0]), X_train_std]\n            X_val_aug = np.c_[np.ones(X_val_std.shape[0]), X_val_std]\n            \n            # Use warm starts for coefficients across the lambda path\n            theta = np.zeros(case['p'] + 1)\n            \n            for j, lam in enumerate(case['lambda_grid']):\n                theta = fit_penalized_poisson(X_train_aug, y_train, lam, initial_theta=theta)\n                \n                eta_val = X_val_aug @ theta\n                mu_val = np.exp(np.clip(eta_val, -30, 30))\n                \n                dev = poisson_deviance(y_val, mu_val)\n                fold_deviances[k, j] = np.mean(dev)\n\n        mean_cv_deviances = np.mean(fold_deviances, axis=0)\n        std_err_cv_deviances = np.std(fold_deviances, axis=0, ddof=1) / np.sqrt(case['K'])\n        \n        # Select lambda_min\n        lambda_min_idx = np.argmin(mean_cv_deviances)\n        lambda_min = case['lambda_grid'][lambda_min_idx]\n        \n        # Select lambda_1se\n        min_dev_val = mean_cv_deviances[lambda_min_idx]\n        min_dev_se = std_err_cv_deviances[lambda_min_idx]\n        threshold = min_dev_val + min_dev_se\n        \n        eligible_indices = np.where(mean_cv_deviances = threshold)[0]\n        # The 1SE rule takes the largest lambda (most regularized) from this set\n        lambda_1se_idx = np.max(eligible_indices)\n        lambda_1se = case['lambda_grid'][lambda_1se_idx]\n        \n        all_final_results.append([lambda_min, lambda_1se])\n\n    case_results_str = []\n    for res_pair in all_final_results:\n        case_results_str.append(f\"[{res_pair[0]:.6f},{res_pair[1]:.6f}]\")\n    \n    # Print in the exact required format\n    print(f\"[{','.join(case_results_str)}]\")\n\nsolve()\n```", "id": "5197921"}]}