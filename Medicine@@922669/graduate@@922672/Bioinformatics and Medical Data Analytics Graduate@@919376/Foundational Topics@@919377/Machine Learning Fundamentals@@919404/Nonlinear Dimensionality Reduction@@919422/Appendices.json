{"hands_on_practices": [{"introduction": "The t-SNE algorithm includes a crucial heuristic known as \"early exaggeration\" to guide the initial stages of optimization. This practice invites you to dissect this mechanism by analyzing how it asymmetrically scales the attractive forces in the gradient descent update [@problem_id:4590746]. By understanding this process from first principles, you will gain a deeper intuition for how t-SNE organizes data points into distinct clusters from an initially random configuration.", "problem": "A bioinformatics team is embedding single-cell RNA-sequencing profiles into a two-dimensional space using t-distributed Stochastic Neighbor Embedding (t-SNE). The algorithm minimizes the Kullback–Leibler divergence between a high-dimensional pairwise similarity distribution and a low-dimensional pairwise similarity distribution. Let the high-dimensional joint similarities be denoted by $p_{ij}$, constructed from Gaussian kernels over standardized gene-expression distances and symmetrized so that $\\sum_{i \\neq j} p_{ij} = 1$, and let the low-dimensional joint similarities be $q_{ij}$ constructed from a Student-$t$ kernel over the two-dimensional coordinates $\\{y_i\\}$ with $\\sum_{i \\neq j} q_{ij} = 1$. The loss is the Kullback–Leibler divergence $C = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}$, which is minimized by gradient descent with respect to the coordinates $\\{y_i\\}$.\n\nDuring the initial phase, the team applies an “early exaggeration” schedule: for a fixed number of iterations, they replace $p_{ij}$ by $\\tilde{p}_{ij} = \\alpha p_{ij}$ with $\\alpha > 1$ in the optimization while keeping the construction of $q_{ij}$ and its normalization unchanged. From first principles beginning with the definition of $C$ and the Student-$t$ kernel for $q_{ij}$, reason about how this modification alters the gradient of $C$ with respect to $y_i$ and, consequently, the dynamics of points belonging to the same cell type (which typically have larger $p_{ij}$ within that neighborhood). Then, determine which statement best captures the effect of increasing $\\alpha$ on the early trajectory of the embedding and the sharpening of clusters.\n\nChoose the single best option:\n\nA. Increasing $\\alpha$ scales the attractive component of the gradient associated with $p_{ij}$ by a factor of $\\alpha$ without directly scaling the repulsive component associated with $q_{ij}$, thereby causing points with large $p_{ij}$ to move together more rapidly and sharpening within-cluster cohesion early in optimization, while leaving the final optimum unchanged when $\\alpha$ is later returned to $1$.\n\nB. Increasing $\\alpha$ scales the low-dimensional similarities $q_{ij}$ by a factor of $\\alpha$, amplifying repulsive forces uniformly across all pairs and causing clusters to spread apart early, which sharpens boundaries by pushing points away.\n\nC. Increasing $\\alpha$ scales both the attractive and repulsive components of the gradient equally, so the net forces are unchanged and early exaggeration has negligible impact on how quickly clusters sharpen.\n\nD. Increasing $\\alpha$ effectively lowers the perplexity used to construct $p_{ij}$, shrinking inter-point distances in the high-dimensional space and causing a global contraction that merges nearby clusters rather than sharpening them.", "solution": "The gradient of the Kullback–Leibler (KL) divergence cost function $C$ with respect to an embedded point $y_i$ can be conceptually separated into an attractive and a repulsive component:\n$$\n\\frac{\\partial C}{\\partial y_i} = \\underbrace{4 \\sum_{j \\neq i} p_{ij} (y_i - y_j) (1 + \\|y_i - y_j\\|^2)^{-1}}_{\\text{Attractive forces}} - \\underbrace{4 \\sum_{j \\neq i} q_{ij} (y_i - y_j) (1 + \\|y_i - y_j\\|^2)^{-1}}_{\\text{Repulsive forces}}\n$$\nThe attractive term pulls points together that are similar in the high-dimensional space (large $p_{ij}$), while the repulsive term pushes all points apart to prevent crowding in the low-dimensional space.\n\nThe \"early exaggeration\" procedure modifies this dynamic by replacing the high-dimensional similarities $p_{ij}$ with $\\tilde{p}_{ij} = \\alpha p_{ij}$ (where $\\alpha > 1$) during the initial optimization steps. The modified gradient becomes:\n$$\n\\frac{\\partial C_{\\text{exaggerated}}}{\\partial y_i} = 4 \\sum_{j \\neq i} (\\alpha p_{ij} - q_{ij}) (y_i - y_j) (1 + \\|y_i - y_j\\|^2)^{-1}\n$$\nThis can be rewritten as:\n$$\n\\frac{\\partial C_{\\text{exaggerated}}}{\\partial y_i} = \\alpha \\left( 4 \\sum_{j \\neq i} p_{ij} (y_i - y_j) (1 + \\|y_i - y_j\\|^2)^{-1} \\right) - \\left( 4 \\sum_{j \\neq i} q_{ij} (y_i - y_j) (1 + \\|y_i - y_j\\|^2)^{-1} \\right)\n$$\nThis clearly shows that the attractive component of the gradient is scaled by the factor $\\alpha$, while the repulsive component remains unchanged.\n\n*   **Effect of $\\alpha > 1$**: This scaling disproportionately strengthens the attractive forces between points with high similarity ($p_{ij}$). It causes these points (e.g., cells of the same type) to rapidly form tight clumps early in the optimization process. This helps to organize the global structure of the embedding before the fine-tuning phase when $\\alpha$ is returned to $1$.\n\n*   **Analysis of Options:**\n    *   **A:** Correct. This option accurately states that the attractive component is scaled by $\\alpha$ while the repulsive component is not, leading to faster within-cluster cohesion. The final optimum is defined by the original cost function with $\\alpha=1$.\n    *   **B:** Incorrect. Early exaggeration modifies $p_{ij}$ (attractive forces), not $q_{ij}$ (repulsive forces).\n    *   **C:** Incorrect. The scaling is asymmetric; attractive and repulsive forces are not scaled equally.\n    *   **D:** Incorrect. Early exaggeration is a modification to the optimization gradient, not a change in the perplexity used to construct the initial $p_{ij}$ values. It sharpens clusters rather than merging them.", "answer": "$$\\boxed{A}$$", "id": "4590746"}, {"introduction": "Effective visualization of complex biological data requires a nuanced choice of hyperparameters, as default settings often fail to capture structures present at different scales. This exercise challenges you to devise a tuning strategy for a dataset containing both tight clusters and diffuse continua, a common scenario in single-cell genomics [@problem_id:4590753]. Mastering this skill is key to producing visualizations that accurately reflect the underlying biology rather than algorithmic artifacts.", "problem": "A single-cell ribonucleic acid sequencing (scRNA-seq) study of a heterogeneous tumor microenvironment yields a matrix of $N=8000$ cells by $p=20000$ genes, which is preprocessed by logarithmic normalization and projection onto the top $d=50$ principal components. Prior biological knowledge suggests two regimes in the latent structure: (i) tightly clustered immune cell subtypes with small within-cluster variance, and (ii) diffuse epithelial cell states spanning an epithelial-to-mesenchymal transition with broad within-cluster variance and transitional continua. You will construct a two-dimensional visualization using both t-Distributed Stochastic Neighbor Embedding (t-SNE) and Uniform Manifold Approximation and Projection (UMAP) to support downstream medical data analytics, such as overlaying clinical covariates and discovering putative subtypes. The aim is to avoid over-clumping, that is, artificially compact clusters that distort local densities and merge transitional continua into dense blobs, while still preserving meaningful local neighborhoods.\n\nAs fundamental base, recall that t-SNE defines high-dimensional conditional similarities $p_{j \\mid i} \\propto \\exp\\!\\left(-\\lVert x_i - x_j \\rVert^2 / (2 \\sigma_i^2)\\right)$ with $\\sigma_i$ chosen so that the Shannon entropy of $p_{\\cdot \\mid i}$ equals a target perplexity; the embedding minimizes a Kullback–Leibler divergence between high-dimensional $p_{ij}$ and low-dimensional $q_{ij}$, producing attractive forces weighted by $p_{ij}$ and repulsive forces weighted by $q_{ij}$. Uniform Manifold Approximation and Projection (UMAP) constructs a fuzzy simplicial set by defining neighbor radii $\\rho_i$ and smooth scales $\\sigma_i$ from $k=n_{\\text{neighbors}}$ nearest neighbors, symmetrizes memberships to form a high-dimensional fuzzy graph, and learns an embedding by minimizing cross-entropy between high- and low-dimensional fuzzy sets; the parameter $\\text{min\\_dist}$ shapes how strongly small low-dimensional distances increase membership, thereby controlling how tightly points are permitted to pack. Multi-scale constructions in each method combine neighborhoods at multiple scales to balance local and global structure.\n\nWhich combined hyperparameter strategy best mitigates over-clumping in this mixed-density dataset while justifying the use of different $n_{\\text{neighbors}}$ strata?\n\nA. Use UMAP with a single neighborhood scale $n_{\\text{neighbors}}=15$, $\\text{min\\_dist}=0.01$, and $\\text{set\\_op\\_mix\\_ratio}=0.0$. Use t-SNE with a single perplexity of $30$ and $\\text{early exaggeration}=12$.\n\nB. Use multi-scale UMAP by aggregating fuzzy graphs from $n_{\\text{neighbors}} \\in \\{15, 60, 200\\}$, set $\\text{min\\_dist}=0.3$, and $\\text{set\\_op\\_mix\\_ratio}=0.7$ to balance union and intersection. Use multi-scale t-SNE by averaging similarities from perplexities $\\{30, 120\\}$, set $\\text{early exaggeration}=8$, and learning rate $\\eta = N/12$.\n\nC. Use UMAP with $n_{\\text{neighbors}}=200$, $\\text{min\\_dist}=0.0$, and $\\text{set\\_op\\_mix\\_ratio}=1.0$. Use t-SNE with perplexity $5$ and $\\text{early exaggeration}=36$.\n\nD. Use UMAP with $n_{\\text{neighbors}}=5$, $\\text{min\\_dist}=0.5$, and $\\text{local\\_connectivity}=1$. Use t-SNE with perplexity $200$ and $\\text{early exaggeration}=4$.\n\nSelect the best option and be prepared to justify your choice from the first-principles behavior of neighborhood probability distributions and fuzzy memberships, including why multiple $n_{\\text{neighbors}}$ strata are appropriate in this scenario of tight and diffuse clusters.", "solution": "The core challenge is to visualize a dataset containing structures at multiple scales: small, dense immune cell clusters and a large, diffuse epithelial-to-mesenchymal transition (EMT) continuum. An effective strategy must capture both local (cluster) and global (continuum) structures without introducing artifacts like \"over-clumping,\" where diffuse structures are artificially compressed.\n\n**Principles for an Optimal Strategy:**\n\n1.  **Multi-Scale Neighborhoods:** A single neighborhood size (t-SNE's perplexity or UMAP's `n_neighbors`) creates a trade-off. A small neighborhood resolves tight clusters but can fragment large continua. A large neighborhood captures continua but can merge tight clusters. The justification for using multiple neighborhood strata is to overcome this limitation by integrating information from different scales, thereby resolving both structure types simultaneously.\n2.  **Control of Embedding Density:** The tendency for algorithms to create artificially dense clusters must be actively managed. In UMAP, the `min_dist` parameter directly controls the minimum distance between points in the embedding. A value greater than the default of $0.1$ (e.g., $0.3$) allows points to spread out, better representing diffuse structures. In t-SNE, a lower `early_exaggeration` factor reduces the aggressive initial clumping, helping to preserve the geometry of less dense regions.\n\n**Analysis of Options:**\n\n*   **A:** This strategy uses single, small-to-moderate neighborhood sizes for both algorithms. Crucially, the UMAP `min_dist=0.01` setting promotes maximum clumping, which is the opposite of the desired outcome.\n*   **B:** This strategy correctly implements a multi-scale approach for both algorithms. For UMAP, aggregating graphs from `n_neighbors` in $\\{15, 60, 200\\}$ captures structures at local, medium, and global scales. The `min_dist=0.3` value is well-chosen to prevent over-clumping and preserve the diffuse nature of the EMT. For t-SNE, averaging similarities from multiple perplexities $\\{30, 120\\}$ is the correct way to implement a multi-scale analysis. The reduced `early_exaggeration=8` also helps preserve global structure. This is a comprehensive and well-justified strategy.\n*   **C:** The UMAP parameters are poor choices: `n_neighbors=200` is a single, large scale that will obscure local clusters, and `min_dist=0.0` will cause extreme over-clumping. The t-SNE parameters are also contradictory, with a hyper-local perplexity of $5$ and an extremely high exaggeration factor of $36$.\n*   **D:** This option combines two opposing single-scale strategies: a hyper-local UMAP (`n_neighbors=5`) and a highly global t-SNE (`perplexity=200`). Neither can simultaneously resolve both the local and global structures present in the data.\n\nThe strategy in option B is the only one that uses a principled, multi-scale approach for both algorithms and explicitly sets parameters to mitigate the problem of over-clumping described in the problem.", "answer": "$$\\boxed{B}$$", "id": "4590753"}, {"introduction": "A common but perilous practice is to perform quantitative analysis, such as clustering, directly on the 2D output of t-SNE or UMAP. This problem explores the critical reasons why this can lead to unstable and misleading results, focusing on how these algorithms distort local density information [@problem_id:4590830]. Engaging with this challenge will help you distinguish between using embeddings for visualization versus using the original high-dimensional data for robust quantitative tasks.", "problem": "A single-cell ribonucleic acid sequencing dataset comprises $n$ cells with $p$ gene features, and the goal is to visualize and cluster discrete cell types. A practitioner uses t-Distributed Stochastic Neighbor Embedding (t-SNE) to obtain a $2$-D embedding $\\{y_i\\}_{i=1}^n$ of the high-dimensional points $\\{x_i\\}_{i=1}^n$, where t-SNE is defined by high-dimensional conditional probabilities $p_{j \\mid i} \\propto \\exp\\!\\left(-\\frac{\\|x_i - x_j\\|^2}{2 \\sigma_i^2}\\right)$ with $\\sigma_i$ chosen to match a target perplexity $\\mathcal{P}$ via $\\mathrm{Perp}\\!\\left(P_i\\right) = 2^{H(P_i)} = \\mathcal{P}$, and low-dimensional joint probabilities $q_{ij} \\propto \\left(1 + \\|y_i - y_j\\|^2\\right)^{-1}$; the embedding minimizes the Kullback–Leibler Divergence (KLD) between $\\{p_{ij}\\}$ and $\\{q_{ij}\\}$. Alternatively, the practitioner uses Uniform Manifold Approximation and Projection (UMAP), which constructs fuzzy simplicial set weights $w_{ij}$ using local connectivity $\\rho_i$ and scale $\\sigma_i$ and optimizes a cross-entropy objective to preserve these fuzzy memberships. The practitioner then clusters in the embedding space and observes that dense regions fracture into many micro-clusters across different random seeds, while sparse regions are lumped together, producing inconsistent biological annotations.\n\nFrom first principles of neighbor graphs and density-sensitive distances, explain how variable local sampling density $\\rho(x)$ drives instability in post-embedding clustering. In particular, note that for a fixed $k$-nearest neighbor graph, the local neighbor radius $r_i$ that captures $k$ neighbors satisfies $r_i \\approx \\left(\\frac{k}{\\rho(x_i)}\\right)^{1/d}$ for ambient dimension $d$, so high-density regions (large $\\rho(x_i)$) have small $r_i$ and sparse regions have large $r_i$. When clustering is performed in a $2$-D embedding with algorithms that assume uniform variance or a single global density threshold, density heterogeneity can induce over-splitting of dense regions and under-splitting of sparse regions. Propose methods that correct for density and reduce this instability without discarding meaningful boundaries between cell types.\n\nWhich of the following strategies most directly and justifiably address post-embedding clustering instability due to variable density and avoid over-splitting artifacts, while preserving biologically meaningful separations? Select all that apply.\n\nA. Construct a shared-nearest-neighbor graph in the original high-dimensional space using adaptive local scales $\\{\\sigma_i\\}$, and cluster this graph with the Leiden algorithm; use the $2$-D embedding strictly for visualization rather than for clustering.\n\nB. Use the density-preserving variant of Uniform Manifold Approximation and Projection (densMAP) to add a density-matching term to the objective so that relative densities are preserved in the $2$-D embedding, and then cluster with Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) using mutual reachability distance to accommodate variable density.\n\nC. Increase the neighborhood size in UMAP to a very large value and then apply Density-Based Spatial Clustering of Applications with Noise (DBSCAN) on the $2$-D embedding with a single global $\\varepsilon$ parameter chosen from the knee of the $k$-distance plot.\n\nD. Apply $k$-means directly in the $2$-D embedding with $k$ selected by the silhouette index, and trust that the t-SNE heavy-tailed kernel has already equalized density sufficiently to prevent over-splitting.\n\nE. Replace Euclidean distances with diffusion distances computed from the high-dimensional Markov transition matrix at timescale $t$ chosen to smooth local stochasticity, then perform spectral clustering (for example, normalized cuts) on the resulting affinity; use the embedding only for visualization.\n\nF. Normalize embedded coordinates by dividing each point’s coordinates by a kernel density estimate at that location to equalize density and then run any off-the-shelf clustering method on the transformed $2$-D space.", "solution": "The fundamental issue with clustering on the 2D output of t-SNE or UMAP is that these algorithms are designed for visualization and do not preserve the data's original density structure. They actively distort local densities to create an aesthetically pleasing embedding where clusters have similar sizes and are well-separated. Dense regions in the high-dimensional space are often expanded, while sparse regions are compressed. This distortion makes any subsequent clustering in the 2D space highly sensitive to the algorithm's hyperparameters and random seed, leading to unstable and biologically misleading results.\n\nAlgorithms like DBSCAN (with a single `ε`) or `k`-means, which make assumptions about cluster density or variance, are particularly prone to failure on these distorted embeddings. The most justifiable strategies, therefore, either perform clustering on a representation of the original high-dimensional data or use specialized algorithms designed to handle the specific artifacts of the embedding.\n\n**Analysis of Options:**\n\n*   **A:** Correct. This is a canonical best-practice approach. A shared-nearest-neighbor (SNN) graph is constructed in the original high-dimensional space, which makes it robust to variations in cluster density. Graph-based clustering algorithms like Leiden are state-of-the-art for this task. The 2D embedding is correctly used only for visualization of the resulting cluster labels.\n*   **B:** Correct. This strategy directly confronts the problem. The densMAP algorithm is a UMAP variant specifically designed to preserve relative density information in the embedding. It is then paired with HDBSCAN, a powerful clustering algorithm that does not require a global density parameter and can identify clusters of varying densities, making it an excellent choice for clustering the output of densMAP (or even standard UMAP/t-SNE).\n*   **C:** Incorrect. Applying DBSCAN with a single global `ε` to a UMAP embedding is precisely the flawed practice the problem describes. The variable densities in the embedding will cause DBSCAN to either merge distinct clusters or fragment large ones.\n*   **D:** Incorrect. Applying `k`-means to a t-SNE embedding is highly inadvisable as `k`-means assumes isotropic, convex clusters of similar variance, properties that are not guaranteed in the embedding. The premise that the t-SNE kernel equalizes density is a misunderstanding; it distorts it.\n*   **E:** Correct. This is another robust, principled approach. Diffusion distances, derived from a Markov process on the high-dimensional data, capture the manifold's intrinsic geometry and are robust to noise and density. Spectral clustering is the natural clustering method for an affinity matrix based on such distances. Like option A, this approach separates the robust clustering step (in high-D) from the visualization step (in 2D).\n*   **F:** Incorrect. This is an ad-hoc, unprincipled method. While it attempts to counteract density variations, \"normalizing\" coordinates in this way is not a standard technique and can introduce its own artifacts without guarantees of success. The methods in A, B, and E are far more theoretically grounded and empirically validated.", "answer": "$$\\boxed{ABE}$$", "id": "4590830"}]}