{"hands_on_practices": [{"introduction": "A probabilistic model's output is not a final decision; a threshold is needed to translate risk scores into actionable binary classifications. This exercise provides foundational practice in calculating essential performance metrics like sensitivity and specificity, forcing a direct confrontation with the trade-offs inherent in threshold selection. By computing how these metrics change, you will gain a more concrete understanding of how a model's utility in a clinical setting is not fixed, but depends critically on the chosen operating point [@problem_id:4585247].", "problem": "A single-stage binary disease classifier outputs predicted probabilities $p_i \\in [0,1]$ for each patient $i \\in \\{1,\\dots,100\\}$. A decision rule declares a patient predicted positive if $p_i \\ge t$ and predicted negative otherwise. You are given $100$ patients grouped by identical predicted probabilities and their realized binary labels. The grouping is as follows:\n- $30$ patients with $p_i = 0.05$, among whom $0$ are truly positive.\n- $20$ patients with $p_i = 0.15$, among whom $0$ are truly positive.\n- $20$ patients with $p_i = 0.25$, among whom $0$ are truly positive.\n- $10$ patients with $p_i = 0.35$, among whom $1$ is truly positive.\n- $10$ patients with $p_i = 0.60$, among whom $3$ are truly positive.\n- $10$ patients with $p_i = 0.80$, among whom $6$ are truly positive.\n\nThe disease prevalence in this cohort is $\\pi = 0.1$.\n\nUsing the standard definitions of sensitivity and specificity in terms of the confusion matrix, compute these two metrics at threshold $t=0.2$ and at threshold $t=0.5$. Then, form the contrast defined as the change in Youden’s $J$ statistic across these thresholds, where Youden’s $J$ statistic is the sum of sensitivity and specificity minus $1$. Report the value of $J(0.2)-J(0.5)$ as a single simplified exact number. No rounding is necessary, and no units should be included in the final answer.", "solution": "The user-provided problem has been rigorously validated and found to be self-contained, consistent, and scientifically sound. All necessary data and definitions are provided, and there are no contradictions. The problem is well-posed and objective, falling squarely within the domain of model evaluation in data analytics.\n\nThe objective is to compute the change in Youden's $J$ statistic between two decision thresholds, $t=0.2$ and $t=0.5$. Youden's $J$ statistic is defined as $J = \\text{Sensitivity} + \\text{Specificity} - 1$.\n\nFirst, we must establish the total number of condition positive and condition negative cases in the cohort of $100$ patients. The data is provided in groups based on predicted probabilities. Let's aggregate the number of truly positive patients.\n- Group with $p_i = 0.05$: $0$ positive cases.\n- Group with $p_i = 0.15$: $0$ positive cases.\n- Group with $p_i = 0.25$: $0$ positive cases.\n- Group with $p_i = 0.35$: $1$ positive case.\n- Group with $p_i = 0.60$: $3$ positive cases.\n- Group with $p_i = 0.80$: $6$ positive cases.\n\nThe total number of truly positive patients, denoted by $P$, is the sum:\n$$P = 0 + 0 + 0 + 1 + 3 + 6 = 10$$\nThe total number of patients in the cohort is given by the sum of patients in each group: $30 + 20 + 20 + 10 + 10 + 10 = 100$.\nThe total number of truly negative patients, denoted by $N$, is the total number of patients minus the total number of truly positive patients:\n$$N = 100 - P = 100 - 10 = 90$$\nThe prevalence $\\pi$ is $\\frac{P}{100} = \\frac{10}{100} = 0.1$, which matches the value provided in the problem statement, confirming data consistency.\n\nThe definitions for sensitivity and specificity are:\n$$ \\text{Sensitivity} = \\frac{\\text{True Positives (TP)}}{\\text{Total Positives (P)}} $$\n$$ \\text{Specificity} = \\frac{\\text{True Negatives (TN)}}{\\text{Total Negatives (N)}} $$\n\nWe will now compute these metrics for each threshold.\n\n**Case 1: Threshold $t = 0.2$**\n\nA patient is predicted positive if their predicted probability $p_i$ is greater than or equal to $t=0.2$. The groups meeting this criterion are those with $p_i$ of $0.25$, $0.35$, $0.60$, and $0.80$.\n\nThe number of True Positives at this threshold, $TP(0.2)$, is the sum of truly positive patients in these predicted positive groups:\n$$TP(0.2) = 0 + 1 + 3 + 6 = 10$$\nThe sensitivity at $t=0.2$ is:\n$$\\text{Sensitivity}(0.2) = \\frac{TP(0.2)}{P} = \\frac{10}{10} = 1$$\n\nA patient is predicted negative if $p_i  0.2$. The groups meeting this criterion are those with $p_i$ of $0.05$ and $0.15$.\nThe number of True Negatives, $TN(0.2)$, is the sum of truly negative patients in these predicted negative groups.\n- For the group with $p_i = 0.05$: There are $30$ patients, $0$ of whom are positive, so all $30$ are negative.\n- For the group with $p_i = 0.15$: There are $20$ patients, $0$ of whom are positive, so all $20$ are negative.\n$$TN(0.2) = 30 + 20 = 50$$\nThe specificity at $t=0.2$ is:\n$$\\text{Specificity}(0.2) = \\frac{TN(0.2)}{N} = \\frac{50}{90} = \\frac{5}{9}$$\nYouden's $J$ statistic at $t=0.2$ is:\n$$J(0.2) = \\text{Sensitivity}(0.2) + \\text{Specificity}(0.2) - 1 = 1 + \\frac{5}{9} - 1 = \\frac{5}{9}$$\n\n**Case 2: Threshold $t = 0.5$**\n\nA patient is predicted positive if $p_i \\ge 0.5$. The groups meeting this criterion are those with $p_i$ of $0.60$ and $0.80$.\nThe number of True Positives at this threshold, $TP(0.5)$, is the sum of truly positive patients in these groups:\n$$TP(0.5) = 3 + 6 = 9$$\nThe sensitivity at $t=0.5$ is:\n$$\\text{Sensitivity}(0.5) = \\frac{TP(0.5)}{P} = \\frac{9}{10}$$\n\nA patient is predicted negative if $p_i  0.5$. The groups meeting this criterion are those with $p_i$ of $0.05$, $0.15$, $0.25$, and $0.35$.\nThe number of True Negatives, $TN(0.5)$, is the sum of truly negative patients in these predicted negative groups.\n- For the group with $p_i = 0.05$: $30$ negative patients.\n- For the group with $p_i = 0.15$: $20$ negative patients.\n- For the group with $p_i = 0.25$: $20$ negative patients.\n- For the group with $p_i = 0.35$: There are $10$ patients, $1$ of whom is positive, so $10 - 1 = 9$ are negative.\n$$TN(0.5) = 30 + 20 + 20 + 9 = 79$$\nThe specificity at $t=0.5$ is:\n$$\\text{Specificity}(0.5) = \\frac{TN(0.5)}{N} = \\frac{79}{90}$$\nYouden's $J$ statistic at $t=0.5$ is:\n$$J(0.5) = \\text{Sensitivity}(0.5) + \\text{Specificity}(0.5) - 1 = \\frac{9}{10} + \\frac{79}{90} - 1$$\nTo combine these terms, we find a common denominator, which is $90$:\n$$J(0.5) = \\frac{9 \\times 9}{10 \\times 9} + \\frac{79}{90} - \\frac{90}{90} = \\frac{81}{90} + \\frac{79}{90} - \\frac{90}{90} = \\frac{81 + 79 - 90}{90} = \\frac{160 - 90}{90} = \\frac{70}{90} = \\frac{7}{9}$$\n\n**Final Calculation**\n\nThe problem requires the computation of the contrast $J(0.2) - J(0.5)$.\n$$J(0.2) - J(0.5) = \\frac{5}{9} - \\frac{7}{9} = \\frac{5-7}{9} = -\\frac{2}{9}$$\nThe result is an exact, simplified fraction.", "answer": "$$\\boxed{-\\frac{2}{9}}$$", "id": "4585247"}, {"introduction": "While the Area Under the ROC Curve (AUROC) is a powerful measure of a model's overall discrimination ability, it does not tell the whole story. This exercise uses a carefully constructed scenario to demonstrate how two models with identical AUROC can exhibit different optimal performances when evaluated with a threshold-dependent metric like the $F_1$ score. This practice [@problem_id:4585277] reveals the crucial distinction between a model's ranking quality and its practical utility at a specific decision point.", "problem": "A clinical informatics team is evaluating two probabilistic binary classifiers for early septic shock detection from electronic health records. The ground-truth labels are fixed across the cohort of $N=10$ adult intensive care unit patients, with $Y_i \\in \\{0,1\\}$ indicating septic shock ($1$) or non-shock ($0$) status for patient $i$. The labels are: $Y_1=1$, $Y_2=1$, $Y_3=0$, $Y_4=1$, $Y_5=0$, $Y_6=1$, $Y_7=0$, $Y_8=0$, $Y_9=0$, $Y_{10}=1$, so there are $m=5$ positives and $n=5$ negatives.\n\nTwo models produce risk scores interpreted as predicted probabilities:\n- Model $\\mathrm{A}$ is overconfident and outputs $s^{(\\mathrm{A})}_1=0.98$, $s^{(\\mathrm{A})}_2=0.92$, $s^{(\\mathrm{A})}_3=0.83$, $s^{(\\mathrm{A})}_4=0.79$, $s^{(\\mathrm{A})}_5=0.72$, $s^{(\\mathrm{A})}_6=0.68$, $s^{(\\mathrm{A})}_7=0.55$, $s^{(\\mathrm{A})}_8=0.51$, $s^{(\\mathrm{A})}_9=0.44$, $s^{(\\mathrm{A})}_{10}=0.37$, reflecting sharp score separation.\n- Model $\\mathrm{B}$ is conservative and better calibrated to prevalence, with compressed scores: $s^{(\\mathrm{B})}_1=0.88$, $s^{(\\mathrm{B})}_2=0.78$, $s^{(\\mathrm{B})}_3=0.81$, $s^{(\\mathrm{B})}_4=0.80$, $s^{(\\mathrm{B})}_5=0.79$, $s^{(\\mathrm{B})}_6=0.77$, $s^{(\\mathrm{B})}_7=0.76$, $s^{(\\mathrm{B})}_8=0.74$, $s^{(\\mathrm{B})}_9=0.73$, $s^{(\\mathrm{B})}_{10}=0.75$, so the descending score order is $\\{1,3,4,5,2,6,7,10,8,9\\}$.\n\nUse the following foundational definitions:\n- The Area Under the Receiver Operating Characteristic (AUROC) equals the probability that a randomly drawn positive has a strictly higher score than a randomly drawn negative, plus one-half the probability of ties. For finite samples with $m$ positives and $n$ negatives, it can be computed as $\\mathrm{AUROC} = \\frac{U}{mn}$, where $U$ is the Mann–Whitney $U$ statistic counting positive–negative concordant pairs (with one-half weight for ties).\n- The $F_1$ score for a thresholded classifier is $F_1 = \\frac{2\\,\\mathrm{TP}}{2\\,\\mathrm{TP}+\\mathrm{FP}+\\mathrm{FN}}$, where $\\mathrm{TP}$, $\\mathrm{FP}$, and $\\mathrm{FN}$ denote true positives, false positives, and false negatives. A threshold on scores yields a ranking-based selection of the top $k$ predicted positives, $k \\in \\{1,2,\\dots,N\\}$, and determines $(\\mathrm{TP},\\mathrm{FP},\\mathrm{FN})$ from the prefix of the descending score order.\n\nTask: Verify that Models $\\mathrm{A}$ and $\\mathrm{B}$ have identical AUROC using the ranking interpretation of the Mann–Whitney $U$ statistic, then determine the maximum achievable $F_1$ for each model over all thresholds, and finally compute the difference $F_{1,\\max}^{(\\mathrm{B})} - F_{1,\\max}^{(\\mathrm{A})}$. Express your final answer as a single simplified fraction. No rounding is required.", "solution": "The problem requires a three-part analysis: first, to verify that two models, $\\mathrm{A}$ and $\\mathrm{B}$, have identical Area Under the Receiver Operating Characteristic (AUROC) values; second, to determine the maximum achievable $F_1$ score for each model across all possible classification thresholds; and third, to compute the difference between these maximum $F_1$ scores.\n\nThe ground-truth data consists of $N=10$ patients. The set of true labels is $\\{Y_1=1, Y_2=1, Y_3=0, Y_4=1, Y_5=0, Y_6=1, Y_7=0, Y_8=0, Y_9=0, Y_{10}=1\\}$.\nThis gives $m=5$ positive instances (patients $\\{1, 2, 4, 6, 10\\}$) and $n=5$ negative instances (patients $\\{3, 5, 7, 8, 9\\}$).\n\n## Part 1: Verification of Identical AUROC\n\nThe AUROC is defined as $\\frac{U}{mn}$, where $U$ is the Mann–Whitney $U$ statistic. For samples without score ties, $U$ is the number of pairs of one positive and one negative instance where the positive instance has a higher score. Since AUROC depends only on the rank ordering of scores, we first establish the rank of each patient for both models, with rank $1$ being the highest score and rank $10$ being the lowest.\n\n**Model A:**\nThe scores $s_i^{(\\mathrm{A})}$ are provided in descending order corresponding to patient indices $i=1, 2, \\dots, 10$.\nThe rank-ordered list of patient indices is $\\{1, 2, 3, 4, 5, 6, 7, 8, 9, 10\\}$.\nThe corresponding sequence of true labels is $\\{Y_1, Y_2, \\dots, Y_{10}\\}$, which is $\\{1, 1, 0, 1, 0, 1, 0, 0, 0, 1\\}$.\nThe positive samples (label $1$) are patients $\\{1, 2, 4, 6, 10\\}$. Their ranks are $\\{1, 2, 4, 6, 10\\}$.\nThe negative samples (label $0$) are patients $\\{3, 5, 7, 8, 9\\}$. Their ranks are $\\{3, 5, 7, 8, 9\\}$.\n\nWe calculate $U_A$ by summing, for each positive instance, the number of negative instances with a lower score (i.e., a higher rank number).\n- Patient $1$ (rank $1$): ranked higher than all $5$ negatives (ranks $3, 5, 7, 8, 9$). Contribution: $5$.\n- Patient $2$ (rank $2$): ranked higher than all $5$ negatives. Contribution: $5$.\n- Patient $4$ (rank $4$): ranked higher than $4$ negatives (ranks $5, 7, 8, 9$). Contribution: $4$.\n- Patient $6$ (rank $6$): ranked higher than $3$ negatives (ranks $7, 8, 9$). Contribution: $3$.\n- Patient $10$ (rank $10$): ranked higher than $0$ negatives. Contribution: $0$.\nThe $U$ statistic for Model A is $U_A = 5+5+4+3+0=17$.\nThe AUROC for Model A is $\\mathrm{AUROC}_A = \\frac{U_A}{mn} = \\frac{17}{5 \\times 5} = \\frac{17}{25}$.\n\n**Model B:**\nThe problem provides the rank-ordered list of patient indices: $\\{1, 3, 4, 5, 2, 6, 7, 10, 8, 9\\}$.\nThe corresponding sequence of true labels is $\\{Y_1, Y_3, Y_4, Y_5, Y_2, Y_6, Y_7, Y_{10}, Y_8, Y_9\\}$, which is $\\{1, 0, 1, 0, 1, 1, 0, 1, 0, 0\\}$.\nThe positive samples are patients $\\{1, 4, 2, 6, 10\\}$. Their ranks are $\\{1, 3, 5, 6, 8\\}$.\nThe negative samples are patients $\\{3, 5, 7, 8, 9\\}$. Their ranks are $\\{2, 4, 7, 9, 10\\}$.\n\nWe calculate $U_B$ similarly:\n- Patient $1$ (rank $1$): ranked higher than all $5$ negatives (ranks $2, 4, 7, 9, 10$). Contribution: $5$.\n- Patient $4$ (rank $3$): ranked higher than $4$ negatives (ranks $4, 7, 9, 10$). Contribution: $4$.\n- Patient $2$ (rank $5$): ranked higher than $3$ negatives (ranks $7, 9, 10$). Contribution: $3$.\n- Patient $6$ (rank $6$): ranked higher than $3$ negatives (ranks $7, 9, 10$). Contribution: $3$.\n- Patient $10$ (rank $8$): ranked higher than $2$ negatives (ranks $9, 10$). Contribution: $2$.\nThe $U$ statistic for Model B is $U_B = 5+4+3+3+2=17$.\nThe AUROC for Model B is $\\mathrm{AUROC}_B = \\frac{U_B}{mn} = \\frac{17}{5 \\times 5} = \\frac{17}{25}$.\n\nThus, we have verified that $\\mathrm{AUROC}_A = \\mathrm{AUROC}_B = \\frac{17}{25}$.\n\n## Part 2: Maximum Achievable $F_1$ Score\n\nThe $F_1$ score is given by $F_1 = \\frac{2\\,\\mathrm{TP}}{2\\,\\mathrm{TP}+\\mathrm{FP}+\\mathrm{FN}}$. A classification threshold corresponds to selecting the top $k$ patients from the rank-ordered list, for $k \\in \\{1, \\dots, 10\\}$. For such a selection, the number of predicted positives is $k = \\mathrm{TP}+\\mathrm{FP}$. The total number of actual positives is fixed at $m = \\mathrm{TP}+\\mathrm{FN} = 5$.\nWe can rewrite the $F_1$ score as a function of $k$:\n$$F_1(k) = \\frac{2\\,\\mathrm{TP}(k)}{(\\mathrm{TP}(k)+\\mathrm{FP}(k)) + (\\mathrm{TP}(k)+\\mathrm{FN}(k))} = \\frac{2\\,\\mathrm{TP}(k)}{k + m} = \\frac{2\\,\\mathrm{TP}(k)}{k+5}$$\nwhere $\\mathrm{TP}(k)$ is the number of true positives among the top $k$ ranked patients. We now compute $F_1(k)$ for each model for $k=1, \\dots, 10$.\n\n**Model A:**\nThe rank-ordered label sequence is $\\{1, 1, 0, 1, 0, 1, 0, 0, 0, 1\\}$.\n- $k=1$: $\\mathrm{TP}(1)=1$, $F_1(1) = \\frac{2(1)}{1+5} = \\frac{2}{6} = \\frac{1}{3}$.\n- $k=2$: $\\mathrm{TP}(2)=2$, $F_1(2) = \\frac{2(2)}{2+5} = \\frac{4}{7}$.\n- $k=3$: $\\mathrm{TP}(3)=2$, $F_1(3) = \\frac{2(2)}{3+5} = \\frac{4}{8} = \\frac{1}{2}$.\n- $k=4$: $\\mathrm{TP}(4)=3$, $F_1(4) = \\frac{2(3)}{4+5} = \\frac{6}{9} = \\frac{2}{3}$.\n- $k=5$: $\\mathrm{TP}(5)=3$, $F_1(5) = \\frac{2(3)}{5+5} = \\frac{6}{10} = \\frac{3}{5}$.\n- $k=6$: $\\mathrm{TP}(6)=4$, $F_1(6) = \\frac{2(4)}{6+5} = \\frac{8}{11}$.\n- $k=7$: $\\mathrm{TP}(7)=4$, $F_1(7) = \\frac{2(4)}{7+5} = \\frac{8}{12} = \\frac{2}{3}$.\n- $k=8$: $\\mathrm{TP}(8)=4$, $F_1(8) = \\frac{2(4)}{8+5} = \\frac{8}{13}$.\n- $k=9$: $\\mathrm{TP}(9)=4$, $F_1(9) = \\frac{2(4)}{9+5} = \\frac{8}{14} = \\frac{4}{7}$.\n- $k=10$: $\\mathrm{TP}(10)=5$, $F_1(10) = \\frac{2(5)}{10+5} = \\frac{10}{15} = \\frac{2}{3}$.\nTo find the maximum, we compare the values. $\\frac{8}{11} \\approx 0.727$ is greater than $\\frac{2}{3} \\approx 0.667$ and $\\frac{8}{13} \\approx 0.615$. Thus, the maximum $F_1$ score for Model A is $F_{1,\\max}^{(\\mathrm{A})} = \\frac{8}{11}$.\n\n**Model B:**\nThe rank-ordered label sequence is $\\{1, 0, 1, 0, 1, 1, 0, 1, 0, 0\\}$.\n- $k=1$: $\\mathrm{TP}(1)=1$, $F_1(1) = \\frac{2(1)}{1+5} = \\frac{1}{3}$.\n- $k=2$: $\\mathrm{TP}(2)=1$, $F_1(2) = \\frac{2(1)}{2+5} = \\frac{2}{7}$.\n- $k=3$: $\\mathrm{TP}(3)=2$, $F_1(3) = \\frac{2(2)}{3+5} = \\frac{1}{2}$.\n- $k=4$: $\\mathrm{TP}(4)=2$, $F_1(4) = \\frac{2(2)}{4+5} = \\frac{4}{9}$.\n- $k=5$: $\\mathrm{TP}(5)=3$, $F_1(5) = \\frac{2(3)}{5+5} = \\frac{3}{5}$.\n- $k=6$: $\\mathrm{TP}(6)=4$, $F_1(6) = \\frac{2(4)}{6+5} = \\frac{8}{11}$.\n- $k=7$: $\\mathrm{TP}(7)=4$, $F_1(7) = \\frac{2(4)}{7+5} = \\frac{2}{3}$.\n- $k=8$: $\\mathrm{TP}(8)=5$, $F_1(8) = \\frac{2(5)}{8+5} = \\frac{10}{13}$.\n- $k=9$: $\\mathrm{TP}(9)=5$, $F_1(9) = \\frac{2(5)}{9+5} = \\frac{10}{14} = \\frac{5}{7}$.\n- $k=10$: $\\mathrm{TP}(10)=5$, $F_1(10) = \\frac{2(5)}{10+5} = \\frac{2}{3}$.\nTo find the maximum, we compare the larger values: $\\frac{8}{11} \\approx 0.727$, $\\frac{10}{13} \\approx 0.769$, and $\\frac{5}{7} \\approx 0.714$. The largest value is $\\frac{10}{13}$. Thus, the maximum $F_1$ score for Model B is $F_{1,\\max}^{(\\mathrm{B})} = \\frac{10}{13}$.\n\n## Part 3: Difference in Maximum $F_1$ Scores\n\nThe final task is to compute the difference $F_{1,\\max}^{(\\mathrm{B})} - F_{1,\\max}^{(\\mathrm{A})}$.\n$$F_{1,\\max}^{(\\mathrm{B})} - F_{1,\\max}^{(\\mathrm{A})} = \\frac{10}{13} - \\frac{8}{11}$$\nTo subtract the fractions, we find a common denominator, which is $13 \\times 11 = 143$.\n$$\\frac{10 \\times 11}{13 \\times 11} - \\frac{8 \\times 13}{11 \\times 13} = \\frac{110}{143} - \\frac{104}{143} = \\frac{110 - 104}{143} = \\frac{6}{143}$$\nThe fraction $\\frac{6}{143}$ is in simplest form, as $6 = 2 \\times 3$ and $143=11 \\times 13$ share no common factors.\nThis result demonstrates that even with identical discrimination power (AUROC), models can exhibit different optimal performances under threshold-based metrics like the $F_1$ score, due to differences in how they rank individual cases.", "answer": "$$\\boxed{\\frac{6}{143}}$$", "id": "4585277"}, {"introduction": "This coding exercise moves from theory to practice, guiding you to implement a standard calibration algorithm: isotonic regression via the Pool Adjacent Violators Algorithm (PAVA). In doing so, you will not only learn how to improve a model's probability estimates but also witness a critical and often counterintuitive phenomenon in model evaluation. The act of improving calibration (measured by Expected Calibration Error) can sometimes degrade discrimination (measured by ROC AUC), a trade-off that is essential to understand when refining predictive models for clinical use [@problem_id:4585289].", "problem": "You must write a complete, runnable program that implements isotonic regression for probability calibration learned on a labeled validation set and applies the fitted monotone mapping to a separate labeled test set. The goal is to compute calibration and discrimination metrics before and after calibration, and to evaluate whether isotonic calibration improves calibration while worsening discrimination on a toy example. Work within the domain of bioinformatics and medical data analytics, focusing on the evaluation and selection of probabilistic models used to estimate clinical event probabilities.\n\nUse the following foundational base and definitions:\n- Probability calibration: A probabilistic classifier outputs a score $p \\in [0,1]$ meant to estimate the conditional event probability $\\mathbb{P}(Y=1 \\mid X)$. Calibration quality can be measured by comparing predicted probabilities to empirical event frequencies across subsets of samples.\n- Expected Calibration Error (ECE): For a chosen number of bins $m \\in \\mathbb{N}$ partitioning $[0,1]$ into equal-width intervals, let $B_j$ denote the $j$-th bin, let $n_j$ be the number of samples with predictions in $B_j$, and let $N$ be the total number of samples. Define $$\\mathrm{ECE} = \\sum_{j=1}^{m} \\frac{n_j}{N} \\left| \\bar{p}_j - \\bar{y}_j \\right|,$$ where $\\bar{p}_j$ is the mean predicted probability in $B_j$ and $\\bar{y}_j$ is the empirical fraction of positives in $B_j$. Express $\\mathrm{ECE}$ as a decimal.\n- Discrimination: The Receiver Operating Characteristic (ROC) Area Under the Curve (AUC) quantifies how well predictions rank positive cases higher than negative cases. It equals the probability that a randomly chosen positive case has a higher score than a randomly chosen negative case, with the standard tie-handling definition based on the Mann–Whitney $U$ statistic.\n\nIsotonic regression problem:\n- Given validation predictions $\\{x_i\\}_{i=1}^{n}$ with corresponding binary labels $\\{y_i\\}_{i=1}^{n}$, isotonic regression computes fitted values $\\{\\hat{f}_i\\}_{i=1}^{n}$ minimizing $$\\sum_{i=1}^{n} w_i \\left( \\hat{f}_i - y_i \\right)^2$$ subject to the monotonicity constraints $$x_i \\le x_j \\implies \\hat{f}_i \\le \\hat{f}_j,$$ where $\\{w_i\\}$ are nonnegative weights, here set to $w_i = 1$ for all $i$. The fitted function is piecewise constant and non-decreasing over the sorted validation predictions. Apply this learned mapping to test predictions by constant interpolation: for a new $x^\\ast$, find the largest validation $x_i \\le x^\\ast$ and return the corresponding $\\hat{f}_i$; if $x^\\ast$ is below the minimum validation $x_i$, return the smallest fitted value, and if above the maximum, return the largest fitted value.\n\nTask requirements:\n- Implement isotonic regression via the Pool Adjacent Violators algorithm, which pools adjacent segments when monotonicity is violated and replaces them with their weighted mean, until all fitted values are non-decreasing with respect to sorted validation predictions.\n- Implement a function to apply the fitted piecewise-constant mapping to test predictions, including extrapolation outside the validation range.\n- Compute $\\mathrm{ECE}$ on the test set before and after calibration, using $m = 5$ equal-width bins over $[0,1]$, and the ROC AUC on the test set before and after calibration using the standard rank-based formula that averages ranks in the presence of ties.\n- For each test case, return a boolean indicating whether calibration improved and discrimination worsened simultaneously, i.e., whether $\\mathrm{ECE}_{\\mathrm{after}}  \\mathrm{ECE}_{\\mathrm{before}}$ and $\\mathrm{AUC}_{\\mathrm{after}}  \\mathrm{AUC}_{\\mathrm{before}}$.\n\nTest suite:\nProvide three test cases with explicit validation and test sets. All numbers are unitless probabilities in $[0,1]$.\n- Case $1$ (toy phenomenon demonstration: improved calibration and worsened discrimination):\n    - Validation predicted probabilities: $\\{0.05, 0.10, 0.20, 0.35, 0.40, 0.45, 0.50, 0.55, 0.75, 0.80, 0.90\\}$.\n    - Validation labels: $\\{0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1\\}$.\n    - Test predicted probabilities: $\\{0.58, 0.59, 0.60, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67\\}$.\n    - Test labels: $\\{0, 0, 0, 1, 1, 1, 1, 1, 1\\}$.\n    This case is constructed so isotonic regression pools the mid-range validation segment to a constant near $2/3$, and the test segment lies entirely in this flat region; calibration (ECE) improves by aligning the mean prediction with the empirical event frequency, while ROC AUC worsens due to ties collapsing a previously perfect ranking within the pooled region.\n- Case $2$ (boundary coverage: no discrimination worsening):\n    - Validation predicted probabilities: $\\{0.10, 0.20, 0.30, 0.40, 0.60, 0.70, 0.80, 0.90\\}$.\n    - Validation labels: $\\{0, 0, 0, 0, 1, 1, 1, 1\\}$.\n    - Test predicted probabilities: $\\{0.12, 0.15, 0.18, 0.22, 0.25, 0.82, 0.84, 0.86, 0.88, 0.90\\}$.\n    - Test labels: $\\{0, 0, 0, 0, 0, 1, 1, 1, 1, 1\\}$.\n    This case yields near-perfect discrimination both before and after calibration, so even if calibration improves, discrimination does not worsen.\n- Case $3$ (edge case: extrapolation outside validation range):\n    - Validation predicted probabilities: $\\{0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80\\}$.\n    - Validation labels: $\\{0, 0, 0, 1, 1, 1, 1\\}$.\n    - Test predicted probabilities: $\\{0.05, 0.10, 0.90, 0.95\\}$.\n    - Test labels: $\\{0, 0, 1, 1\\}$.\n    This case tests extrapolation. Predictions below the smallest validation value map to the smallest fitted constant; predictions above the largest map to the largest constant. Discrimination remains unchanged at a high level, while calibration typically improves.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The result for each test case must be a boolean indicating whether both conditions hold simultaneously: $\\mathrm{ECE}$ strictly decreased and $\\mathrm{AUC}$ strictly decreased on the test set after isotonic calibration. The expected output format is $[b_1,b_2,b_3]$, where each $b_i$ is either $\\mathrm{True}$ or $\\mathrm{False}$.", "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n\n- **Topic**: Model evaluation and selection in bioinformatics and medical data analytics, specifically focusing on probability calibration.\n- **Core Concepts**:\n    - **Probability Calibration**: The process of adjusting a model's predicted probabilities $p \\in [0,1]$ to better reflect the true conditional event probabilities $\\mathbb{P}(Y=1 \\mid X)$.\n    - **Expected Calibration Error (ECE)**: A metric for calibration quality, defined as $\\mathrm{ECE} = \\sum_{j=1}^{m} \\frac{n_j}{N} \\left| \\bar{p}_j - \\bar{y}_j \\right|$, where $m$ is the number of bins, $n_j$ is the number of samples in bin $j$, $N$ is the total number of samples, $\\bar{p}_j$ is the mean predicted probability in bin $j$, and $\\bar{y}_j$ is the empirical fraction of positives in bin $j$. The number of bins is specified as $m=5$.\n    - **Discrimination**: The ability of a model to assign higher scores to positive cases than to negative cases, measured by the Receiver Operating Characteristic (ROC) Area Under the Curve (AUC). The AUC is to be calculated using the Mann–Whitney $U$ statistic formulation to handle ties.\n    - **Isotonic Regression**: A non-parametric method to find a non-decreasing approximation of a function. The problem asks for minimization of $\\sum_{i=1}^{n} w_i \\left( \\hat{f}_i - y_i \\right)^2$ subject to monotonicity constraints $x_i \\le x_j \\implies \\hat{f}_i \\le \\hat{f}_j$. Weights are given as $w_i = 1$ for all $i$.\n    - **Algorithm**: Isotonic regression is to be implemented via the Pool Adjacent Violators Algorithm (PAVA). The learned mapping is a piecewise-constant, non-decreasing function.\n    - **Application to Test Set**: For a new prediction $x^\\ast$, the calibrated value is found by constant interpolation. If $x^\\ast$ is outside the range of validation predictions, extrapolation is performed by returning the smallest or largest fitted value.\n- **Task Requirements**:\n    1. Implement PAVA for isotonic regression.\n    2. Implement a function to apply the aformentioned mapping to test data.\n    3. For a test set, compute ECE (with $m=5$ bins) and ROC AUC before and after calibration.\n    4. Determine if, for each test case, calibration improves (ECE decreases) and discrimination worsens (AUC decreases) simultaneously, i.e., if $\\mathrm{ECE}_{\\mathrm{after}}  \\mathrm{ECE}_{\\mathrm{before}}$ and $\\mathrm{AUC}_{\\mathrm{after}}  \\mathrm{AUC}_{\\mathrm{before}}$.\n- **Test Data**: Three specific test cases are provided, each with a validation set (predicted probabilities and labels) and a test set (predicted probabilities and labels).\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is well-grounded in established statistical and machine learning theory. Isotonic regression, ECE, and ROC AUC are standard, widely-used methods for evaluating and calibrating probabilistic models, particularly in fields like clinical medicine and bioinformatics. The described phenomenon, where calibration can degrade discrimination, is a known and important trade-off.\n- **Well-Posed**: The problem is well-posed. All necessary data, definitions, and algorithms are specified. The inputs (validation and test sets) are explicit, and the required output is a deterministic function of these inputs. For each test case, a unique, stable, and meaningful boolean result can be computed.\n- **Objective**: The problem is stated in precise, objective, and mathematical language. There are no subjective claims or ambiguities.\n\nThe problem does not violate any of the invalidity criteria:\n1.  **Scientific/Factual Unsoundness**: None. The principles are correct.\n2.  **Non-Formalizable/Irrelevant**: The problem is formalizable and highly relevant to the specified domain.\n3.  **Incomplete/Contradictory Setup**: None. The givens are complete and consistent.\n4.  **Unrealistic/Infeasible**: None. The data and scenario are realistic representations of a model evaluation workflow.\n5.  **Ill-Posed/Poorly Structured**: None. The problem is well-structured and leads to a unique solution.\n6.  **Pseudo-Profound/Trivial**: The problem is not trivial; it requires the correct implementation of several standard but non-trivial algorithms (PAVA, AUC, ECE) and illuminates a subtle but important concept in model evaluation.\n7.  **Outside Scientific Verifiability**: The results are computationally verifiable.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution will be provided.\n\nThe solution will proceed by implementing four main components:\n1.  A function to compute the ROC AUC using the rank-based Mann-Whitney $U$ statistic formulation, which correctly handles ties. The `scipy.stats.rankdata` function will be used for this.\n2.  A function to compute the Expected Calibration Error (ECE) for a given number of bins, $m=5$.\n3.  A function to fit an isotonic regression model using the Pool Adjacent Violators Algorithm (PAVA). This function takes the validation predictions and labels and returns the learned piecewise-constant mapping.\n4.  A function to apply this learned mapping to new (test) prediction scores, handling interpolation and extrapolation as specified.\n\nFor each test case, these components will be used to calculate the ECE and AUC on the test set both before and after applying the isotonic calibration learned from the validation set. Finally, the condition ($\\mathrm{ECE}_{\\mathrm{after}}  \\mathrm{ECE}_{\\mathrm{before}}$ and $\\mathrm{AUC}_{\\mathrm{after}}  \\mathrm{AUC}_{\\mathrm{before}}$) will be evaluated and the boolean result stored. The final output will be a list of these boolean results.\n\nThe PAVA implementation will follow the standard active-set method. Given a set of observations $y_1, y_2, \\dots, y_n$ (ordered by their corresponding $x$ values), the algorithm proceeds in a single pass. It maintains a set of pooled, monotonically non-decreasing values. When a new observation $y_j$ creates a violation with the previous pool (i.e., its value is smaller), it is merged with the preceding pool. This merging continues backwards until monotonicity is restored. The value of a new pool is the weighted average of the constituent observations. Since all weights $w_i$ are $1$ in this problem, the value is the simple arithmetic mean of the $y_i$ in the pool.\n\nThe ROC AUC will be calculated from its relationship with the Mann-Whitney $U$ statistic:\n$$ \\mathrm{AUC} = \\frac{U}{n_1 n_0} $$\nwhere $n_1$ and $n_0$ are the counts of positive and negative samples, respectively, and $U$ is given by:\n$$ U = R_1 - \\frac{n_1(n_1+1)}{2} $$\nHere, $R_1$ is the sum of the ranks of the positive samples' scores, with ranks computed by assigning the average rank to tied scores. This method is robust to the ties that are introduced by the piecewise-constant nature of the isotonic calibration.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import rankdata\n\ndef calculate_auc(y_true, y_score):\n    \"\"\"\n    Computes the ROC AUC score using the Mann-Whitney U statistic formulation.\n    This handles ties correctly by using average ranks.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_score = np.asarray(y_score)\n    \n    pos_mask = (y_true == 1)\n    neg_mask = (y_true == 0)\n    \n    n_pos = np.sum(pos_mask)\n    n_neg = np.sum(neg_mask)\n    \n    if n_pos == 0 or n_neg == 0:\n        return 0.5  # Undefined, but 0.5 is a common convention\n    \n    # Ranks are calculated on all scores, with ties given the average rank\n    ranks = rankdata(y_score)\n    \n    # Sum of ranks for the positive class\n    rank_sum_pos = np.sum(ranks[pos_mask])\n    \n    # Mann-Whitney U statistic for the positive class\n    U = rank_sum_pos - (n_pos * (n_pos + 1)) / 2\n    \n    auc = U / (n_pos * n_neg)\n    return auc\n\ndef calculate_ece(y_true, y_pred, m=5):\n    \"\"\"\n    Computes the Expected Calibration Error (ECE) for m bins.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    \n    N = len(y_true)\n    if N == 0:\n        return 0.0\n\n    # Create bin edges. Bins are of the form (lower, upper] except for the first bin [0, upper]\n    bin_edges = np.linspace(0, 1, m + 1)\n    \n    ece = 0.0\n    for j in range(m):\n        bin_lower = bin_edges[j]\n        bin_upper = bin_edges[j+1]\n        \n        # Find samples in the current bin\n        in_bin_mask = (y_pred > bin_lower)  (y_pred = bin_upper)\n        if bin_lower == 0.0:\n            in_bin_mask |= (y_pred == 0.0) # Include 0 in the first bin\n            \n        n_j = np.sum(in_bin_mask)\n        \n        if n_j > 0:\n            # Mean of true labels (accuracy) in the bin\n            accuracy_in_bin = np.mean(y_true[in_bin_mask])\n            # Mean of predicted probabilities (confidence) in the bin\n            confidence_in_bin = np.mean(y_pred[in_bin_mask])\n            \n            ece += (n_j / N) * np.abs(confidence_in_bin - accuracy_in_bin)\n            \n    return ece\n\ndef fit_pava(y_val):\n    \"\"\"\n    Fits isotonic regression using the Pool Adjacent Violators Algorithm (PAVA).\n    Assumes unit weights.\n    Input y_val should be ordered by corresponding x_val.\n    \"\"\"\n    n = len(y_val)\n    if n == 0:\n        return np.array([])\n    \n    g = np.copy(y_val).astype(float)\n    w = np.ones(n, dtype=float)\n    active_set = [0]\n    \n    for j in range(1, n):\n        active_set.append(j)\n        \n        # Check for monotonicity violation and merge pools if necessary\n        while len(active_set) > 1 and g[active_set[-2]] > g[active_set[-1]]:\n            prev_idx = active_set[-2]\n            curr_idx = active_set[-1]\n            \n            # Merge the last two pools\n            w_prev = w[prev_idx]\n            w_curr = w[curr_idx]\n            \n            merged_val = (g[prev_idx] * w_prev + g[curr_idx] * w_curr) / (w_prev + w_curr)\n            \n            g[prev_idx] = merged_val\n            w[prev_idx] += w_curr\n            \n            active_set.pop()\n\n    # Expand the pooled solution to the full length\n    final_g = np.zeros(n)\n    active_set.append(n)\n    for i in range(len(active_set) - 1):\n        start_idx = active_set[i]\n        end_idx = active_set[i+1]\n        final_g[start_idx:end_idx] = g[start_idx]\n        \n    return final_g\n\ndef predict_iso(p_test, p_val, fitted_val):\n    \"\"\"\n    Applies the learned isotonic mapping to new predictions.\n    Handles interpolation and extrapolation.\n    \"\"\"\n    indices = np.searchsorted(p_val, p_test, side='right')\n    # Clip indices to handle extrapolation\n    clipped_indices = np.maximum(0, indices - 1)\n    return fitted_val[clipped_indices]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"val_p\": np.array([0.05, 0.10, 0.20, 0.35, 0.40, 0.45, 0.50, 0.55, 0.75, 0.80, 0.90]),\n            \"val_y\": np.array([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1]),\n            \"test_p\": np.array([0.58, 0.59, 0.60, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67]),\n            \"test_y\": np.array([0, 0, 0, 1, 1, 1, 1, 1, 1]),\n        },\n        {\n            \"val_p\": np.array([0.10, 0.20, 0.30, 0.40, 0.60, 0.70, 0.80, 0.90]),\n            \"val_y\": np.array([0, 0, 0, 0, 1, 1, 1, 1]),\n            \"test_p\": np.array([0.12, 0.15, 0.18, 0.22, 0.25, 0.82, 0.84, 0.86, 0.88, 0.90]),\n            \"test_y\": np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1]),\n        },\n        {\n            \"val_p\": np.array([0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80]),\n            \"val_y\": np.array([0, 0, 0, 1, 1, 1, 1]),\n            \"test_p\": np.array([0.05, 0.10, 0.90, 0.95]),\n            \"test_y\": np.array([0, 0, 1, 1]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        val_p, val_y = case[\"val_p\"], case[\"val_y\"]\n        test_p, test_y = case[\"test_p\"], case[\"test_y\"]\n        num_bins = 5\n\n        # 1. Calculate metrics before calibration\n        ece_before = calculate_ece(test_y, test_p, m=num_bins)\n        auc_before = calculate_auc(test_y, test_p)\n\n        # 2. Fit isotonic regression on validation data\n        # Note: The problem statement guarantees val_p is sorted, a prerequisite for PAVA.\n        fitted_y = fit_pava(val_y)\n        \n        # We need the unique mapping from validation predictions to fitted values\n        # Since val_p is sorted and unique in the test cases, we can use it directly.\n        iso_map_x = val_p\n        iso_map_y = fitted_y\n\n        # 3. Apply calibration to test data\n        calibrated_p = predict_iso(test_p, iso_map_x, iso_map_y)\n\n        # 4. Calculate metrics after calibration\n        ece_after = calculate_ece(test_y, calibrated_p, m=num_bins)\n        auc_after = calculate_auc(test_y, calibrated_p)\n\n        # 5. Check the condition: ECE decreased AND AUC decreased\n        condition_met = (ece_after  ece_before) and (auc_after  auc_before)\n        results.append(condition_met)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4585289"}]}