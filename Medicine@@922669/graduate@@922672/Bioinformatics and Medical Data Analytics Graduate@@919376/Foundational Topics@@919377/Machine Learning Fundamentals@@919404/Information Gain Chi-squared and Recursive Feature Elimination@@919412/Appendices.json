{"hands_on_practices": [{"introduction": "A common first step in feature selection is to rank features based on their individual predictive power using metrics like information gain or the chi-squared statistic. While intuitive, this univariate approach can fail when the most predictive signals arise from the interaction of multiple features, a phenomenon known as synergy. This exercise [@problem_id:4573615] presents a classic scenario of epistatic interaction—the XOR problem—to demonstrate how features that are useless in isolation can be powerfully predictive when combined, and why common filter and linear wrapper methods may fail to identify them.", "problem": "Consider a synthetic epistatic phenotype model in a case-control study in bioinformatics and medical data analytics. Let $X_1$ and $X_2$ denote two independent bi-allelic Single Nucleotide Polymorphism (SNP) indicators, each distributed as $\\mathrm{Bernoulli}(0.5)$ and independent of each other. Define the binary phenotype $Y \\in \\{0,1\\}$ by a noisy parity rule: let $Z = X_1 \\oplus X_2$ denote the exclusive-or of $X_1$ and $X_2$, and let $N \\sim \\mathrm{Bernoulli}(\\epsilon)$ be independent noise. Then define $Y = Z \\oplus N$, with a fixed noise rate $\\epsilon \\in (0,0.5)$; here $\\oplus$ denotes addition modulo $2$. In addition, suppose there is a third binary feature $X_3$ constructed as a noisy proxy of $Y$: let $M \\sim \\mathrm{Bernoulli}(1-q)$ be independent of $(X_1,X_2,N,Y)$, and define $X_3 = Y \\oplus M$, where $q \\in (0.5,1)$ is the agreement probability between $X_3$ and $Y$. Assume $q = 0.6$ and $\\epsilon = 0.1$ for concreteness.\n\nYou may use the following fundamental definitions and facts:\n- Shannon entropy $H$ of a discrete random variable $U$ with support $\\mathcal{U}$ and probability mass function $p(u)$ is $$H(U) = -\\sum_{u \\in \\mathcal{U}} p(u) \\log_2 p(u).$$\n- The conditional entropy is $$H(U \\mid V) = -\\sum_{v \\in \\mathcal{V}} p(v) \\sum_{u \\in \\mathcal{U}} p(u \\mid v) \\log_2 p(u \\mid v).$$\n- Mutual Information (MI) is $$I(U;V) = H(U) - H(U \\mid V) = H(V) - H(V \\mid U).$$\n- The binary entropy function is $$H_2(p) = -p \\log_2 p - (1-p) \\log_2(1-p).$$\n- For a noisy parity construction $Y = Z \\oplus N$ with $Z \\sim \\mathrm{Bernoulli}(0.5)$ independent of $N \\sim \\mathrm{Bernoulli}(\\epsilon)$, $Y$ is uniform: $H(Y) = 1$ bit.\n- Pearson’s chi-squared test of independence for two discrete variables compares observed counts to expected counts under the independence model; if the variables are independent in the population, the chi-squared statistic is $0$ in the population limit because observed proportions equal expected proportions.\n\nConsider three feature selection approaches that are commonly used in bioinformatics and medical data analytics:\n1. An information gain filter that ranks features by univariate Mutual Information (MI) $I(X_j;Y)$ and selects top-ranked individual features.\n2. A Pearson’s chi-squared filter that tests independence between each individual feature $X_j$ and the label $Y$ and selects features with large univariate chi-squared statistics.\n3. Recursive Feature Elimination (RFE), defined here as a wrapper method that fits a linear classifier (e.g., a linear Support Vector Machine (SVM) or logistic regression) and recursively removes the lowest-weight features, repeating until a desired subset size is reached. In its basic form, the linear model uses only the raw features $(X_1,X_2,X_3)$ without explicit interaction terms.\n\nAnswer the following multiple-choice question (select all that apply):\n\nWhich statements are true in this setting?\n\nA. $I(X_1;Y) = I(X_2;Y) = 0$ in the population, whereas the joint mutual information $I(X_1,X_2;Y)$ is positive and equals $1 - H_2(\\epsilon)$ bits; with $\\epsilon = 0.1$, this synergy is strictly positive.\n\nB. The univariate Pearson’s chi-squared statistics for $X_1$ versus $Y$ and for $X_2$ versus $Y$ are $0$ in the population, but a chi-squared test on the joint categorical variable $(X_1,X_2)$ versus $Y$ yields a strictly positive statistic.\n\nC. A univariate information gain filter will rank $X_3$ above $X_1$ and $X_2$ because $I(X_3;Y) > 0$ while $I(X_1;Y) = I(X_2;Y) = 0$, potentially discarding the synergistic pair $(X_1,X_2)$.\n\nD. Recursive Feature Elimination (RFE) with a linear classifier that uses only $(X_1,X_2,X_3)$ and no interaction terms tends to eliminate $X_1$ and $X_2$ before $X_3$ because a linear decision boundary cannot represent the parity rule, causing near-zero linear weights for $X_1$ and $X_2$.\n\nE. The joint mutual information $I(X_1,X_2;Y)$ equals $H(Y)$ because knowing $Y$ determines $(X_1,X_2)$ uniquely in this model.", "solution": "The problem statement is a well-posed and scientifically sound theoretical exercise in feature selection, commonly used to illustrate the limitations of univariate methods and the challenges of synergistic or epistatic interactions. All variables, distributions, and relationships are clearly defined, and the premises are consistent with the principles of probability theory, information theory, and machine learning. Therefore, the problem is valid.\n\nWe will now evaluate each statement.\n\n**Preliminary Analysis**\n\nLet us first establish the distributions of the key random variables.\n- The features $X_1$ and $X_2$ are independent and identically distributed as $\\mathrm{Bernoulli}(0.5)$. This means $P(X_1=k) = P(X_2=k) = 0.5$ for $k \\in \\{0,1\\}$.\n- $Z = X_1 \\oplus X_2$. The probability mass function for $Z$ is:\n  - $P(Z=0) = P(X_1=0, X_2=0) + P(X_1=1, X_2=1) = (0.5)(0.5) + (0.5)(0.5) = 0.5$.\n  - $P(Z=1) = P(X_1=0, X_2=1) + P(X_1=1, X_2=0) = (0.5)(0.5) + (0.5)(0.5) = 0.5$.\n  - Thus, $Z \\sim \\mathrm{Bernoulli}(0.5)$.\n- The phenotype is $Y = Z \\oplus N$, where $N \\sim \\mathrm{Bernoulli}(\\epsilon)$ is independent of $Z$. The problem states $\\epsilon=0.1$.\n  - $P(Y=0) = P(Z=0,N=0) + P(Z=1,N=1) = P(Z=0)P(N=0) + P(Z=1)P(N=1)$ (by independence).\n  - $P(Y=0) = (0.5)(1-\\epsilon) + (0.5)(\\epsilon) = 0.5$.\n  - Thus, $Y \\sim \\mathrm{Bernoulli}(0.5)$, and its Shannon entropy is $H(Y) = H_2(0.5) = 1$ bit. This confirms the fact provided in the problem.\n- The proxy feature is $X_3 = Y \\oplus M$, where $M \\sim \\mathrm{Bernoulli}(1-q)$ is independent of $Y$. The problem states $q=0.6$, so $1-q=0.4$.\n  - The probability of agreement is $P(X_3=Y) = P(M=0) = q = 0.6$.\n\n**Option A: Analysis of Mutual Information**\n\nThe statement is: $I(X_1;Y) = I(X_2;Y) = 0$ in the population, whereas the joint mutual information $I(X_1,X_2;Y)$ is positive and equals $1 - H_2(\\epsilon)$ bits; with $\\epsilon = 0.1$, this synergy is strictly positive.\n\n1.  **Univariate Mutual Information $I(X_1;Y)$ and $I(X_2;Y)$:**\n    $I(X_1;Y) = 0$ if and only if $X_1$ and $Y$ are independent. Let's check for independence by computing $P(Y=y | X_1=x_1)$.\n    If $X_1=0$, then $Z = 0 \\oplus X_2 = X_2$. Since $X_2 \\sim \\mathrm{Bernoulli}(0.5)$, $Z$ is also $\\mathrm{Bernoulli}(0.5)$ in this case.\n    $P(Y=0|X_1=0) = P(Z \\oplus N = 0 | X_1=0) = P(X_2 \\oplus N = 0)$.\n    By independence of $X_2$ and $N$, this is $P(X_2=0)P(N=0) + P(X_2=1)P(N=1) = (0.5)(1-\\epsilon) + (0.5)(\\epsilon) = 0.5$.\n    If $X_1=1$, then $Z = 1 \\oplus X_2$. This variable $Z$ is also $\\mathrm{Bernoulli}(0.5)$.\n    $P(Y=0|X_1=1) = P((1 \\oplus X_2) \\oplus N = 0)$. The distribution of $(1 \\oplus X_2) \\oplus N$ is identical to $X_2 \\oplus N$, so this probability is also $0.5$.\n    Since $P(Y=0|X_1=x_1) = 0.5 = P(Y=0)$ for all $x_1$, $Y$ is independent of $X_1$.\n    Therefore, $I(X_1;Y) = 0$. By symmetry, $Y$ is also independent of $X_2$, so $I(X_2;Y) = 0$.\n\n2.  **Joint Mutual Information $I(X_1,X_2;Y)$:**\n    The mutual information is $I(X_1,X_2;Y) = H(Y) - H(Y|X_1,X_2)$.\n    We know $H(Y)=1$ bit.\n    The conditional entropy $H(Y|X_1,X_2)$ is a weighted average of $H(Y|X_1=x_1, X_2=x_2)$ over all $(x_1,x_2)$.\n    For any fixed pair $(x_1,x_2)$, the value of $Z = x_1 \\oplus x_2$ is fixed.\n    Then $Y = Z \\oplus N$, where $Z$ is a constant. The conditional distribution of $Y$ is that of a constant XORed with $N \\sim \\mathrm{Bernoulli}(\\epsilon)$. This results in a $\\mathrm{Bernoulli}$ variable with parameter $\\epsilon$ or $1-\\epsilon$. In either case, the entropy is $H_2(\\epsilon)$.\n    For instance, if $Z=z$, then $P(Y=z) = P(N=0) = 1-\\epsilon$ and $P(Y \\ne z) = P(N=1) = \\epsilon$. The entropy of this distribution is $H_2(\\epsilon)$.\n    Thus, $H(Y|X_1=x_1, X_2=x_2) = H_2(\\epsilon)$ for all $(x_1,x_2)$.\n    The average is therefore constant: $H(Y|X_1,X_2) = H_2(\\epsilon)$.\n    So, $I(X_1,X_2;Y) = 1 - H_2(\\epsilon)$.\n\n3.  **Synergy:**\n    Synergy is present when the information from the whole is greater than the sum of the information from the parts: $I(X_1,X_2;Y) > I(X_1;Y) + I(X_2;Y)$.\n    In our case, this is $1 - H_2(\\epsilon) > 0 + 0$.\n    The noise rate is $\\epsilon = 0.1$, which is in $(0, 0.5)$. For any $\\epsilon$ in this range, $0  H_2(\\epsilon)  1$.\n    Therefore, $I(X_1,X_2;Y) = 1 - H_2(0.1) > 0$. $H_2(0.1) \\approx 0.469$ bits, so $I(X_1,X_2;Y) \\approx 0.531$ bits. This value is strictly positive.\n\nThe statement is entirely correct.\nVerdict: **Correct**.\n\n**Option B: Analysis of Pearson's Chi-Squared Statistics**\n\nThe statement is: The univariate Pearson’s chi-squared statistics for $X_1$ versus $Y$ and for $X_2$ versus $Y$ are $0$ in the population, but a chi-squared test on the joint categorical variable $(X_1,X_2)$ versus $Y$ yields a strictly positive statistic.\n\n1.  **Univariate Statistics:**\n    The Pearson's chi-squared test of independence, in the population limit (infinite data), yields a statistic of $0$ if and only if the two variables are statistically independent.\n    In the analysis for Option A, we rigorously proved that $X_1$ and $Y$ are independent, and $X_2$ and $Y$ are independent.\n    Consequently, the population chi-squared statistics for $X_1$ vs. $Y$ and for $X_2$ vs. $Y$ are both exactly $0$.\n\n2.  **Joint Statistic:**\n    This tests for independence between the joint variable $V=(X_1,X_2)$ and $Y$.\n    $V$ and $Y$ are independent if and only if $I(V;Y)=0$, which is $I(X_1,X_2;Y)=0$.\n    In the analysis for Option A, we showed that $I(X_1,X_2;Y) = 1 - H_2(\\epsilon)$, which is strictly positive for $\\epsilon=0.1$.\n    Since the mutual information is positive, the variables $(X_1,X_2)$ and $Y$ are not independent.\n    Therefore, the population chi-squared statistic for a test of independence between $(X_1,X_2)$ and $Y$ must be strictly positive.\n\nThe statement is a direct statistical consequence of the information-theoretic results in Option A.\nVerdict: **Correct**.\n\n**Option C: Analysis of an Information Gain Filter**\n\nThe statement is: A univariate information gain filter will rank $X_3$ above $X_1$ and $X_2$ because $I(X_3;Y) > 0$ while $I(X_1;Y) = I(X_2;Y) = 0$, potentially discarding the synergistic pair $(X_1,X_2)$.\n\n1.  **Information Gain of $X_1, X_2$:**\n    As established in Option A, the information gain, which is another name for mutual information, is $I(X_1;Y) = 0$ and $I(X_2;Y) = 0$.\n\n2.  **Information Gain of $X_3$:**\n    We need to compute $I(X_3;Y) = H(Y) - H(Y|X_3)$. We know $H(Y)=1$.\n    The relationship is $X_3 = Y \\oplus M$, with $Y \\sim \\mathrm{Bernoulli}(0.5)$ and $M \\sim \\mathrm{Bernoulli}(1-q)$ ($q=0.6$). This is a binary symmetric channel with crossover probability $1-q = 0.4$.\n    The conditional entropy is $H(Y|X_3) = H_2(1-q)$. This is a standard result for binary symmetric channels.\n    Thus, $I(X_3;Y) = 1 - H_2(1-q)$.\n    With $q=0.6$, we have $1-q=0.4$. Since $0.4 \\ne 0.5$, $H_2(0.4)  1$.\n    $H_2(0.4) = -0.4\\log_2(0.4) - 0.6\\log_2(0.6) \\approx 0.971$ bits.\n    So, $I(X_3;Y) = 1 - H_2(0.4) \\approx 1 - 0.971 = 0.029$ bits.\n    This is strictly positive.\n\n3.  **Ranking and Conclusion:**\n    The univariate filter ranks features by $I(X_j;Y)$. The ranking is:\n    - $X_3$: $I(X_3;Y) \\approx 0.029  0$\n    - $X_1$: $I(X_1;Y) = 0$\n    - $X_2$: $I(X_2;Y) = 0$\n    $X_3$ will be ranked highest. If the feature selection method chooses, for example, only the top-ranked feature, it will select $X_3$ and discard the causally upstream, synergistic features $X_1$ and $X_2$. This demonstrates a classic failure mode of univariate filter methods.\n\nThe statement is entirely correct.\nVerdict: **Correct**.\n\n**Option D: Analysis of Recursive Feature Elimination (RFE)**\n\nThe statement is: Recursive Feature Elimination (RFE) with a linear classifier that uses only $(X_1,X_2,X_3)$ and no interaction terms tends to eliminate $X_1$ and $X_2$ before $X_3$ because a linear decision boundary cannot represent the parity rule, causing near-zero linear weights for $X_1$ and $X_2$.\n\n1.  **Linear Model Fitness:**\n    A linear classifier models thetarget $Y$ (or a function of its probability) as a linear combination of features: $f(X_1,X_2,X_3) = w_1 X_1 + w_2 X_2 + w_3 X_3 + b$. The weights $w_j$ are determined by fitting the model to data. The magnitude of $w_j$ is often interpreted as the importance of feature $X_j$.\n    The core relationship $Y \\approx X_1 \\oplus X_2$ is the XOR problem, which is not linearly separable.\n    As we showed for Option A, $Y$ is independent of $X_1$ and $X_2$ individually. This means there is no linear correlation between $Y$ and $X_1$, or between $Y$ and $X_2$. For a linear model, the best-fitting coefficients $w_1$ and $w_2$ will be close to $0$. Any non-zero weight for $X_1$ or $X_2$ would increase prediction error on average.\n    In contrast, $X_3$ is a noisy copy of $Y$. $P(Y=1|X_3=1) = q = 0.6$ and $P(Y=1|X_3=0) = 1-q = 0.4$. Since these conditional probabilities are different, $X_3$ is predictive of $Y$, and there is a positive correlation between them. A linear classifier will fit a non-zero weight $w_3$ to capture this relationship.\n\n2.  **RFE Behavior:**\n    RFE iteratively fits a model and removes the feature with the lowest-magnitude weight.\n    Since the model will find $|w_3|  0$ and $|w_1| \\approx |w_2| \\approx 0$, RFE will rank $X_1$ and $X_2$ as the least important features.\n    In the first step, it will eliminate either $X_1$ or $X_2$. In the second step, it will eliminate the other. $X_3$ will be the last feature remaining.\n    The reasoning provided in the statement is a correct diagnosis of how a linear model Wrapper method like RFE behaves in the presence of non-linear interactions.\n\nThe statement is entirely correct.\nVerdict: **Correct**.\n\n**Option E: Analysis of Joint Mutual Information and Determinism**\n\nThe statement is: The joint mutual information $I(X_1,X_2;Y)$ equals $H(Y)$ because knowing $Y$ determines $(X_1,X_2)$ uniquely in this model.\n\n1.  **Information-Theoretic Implication:**\n    The claim $I(X_1,X_2;Y) = H(Y)$ is equivalent to $H(Y) - H(Y|X_1,X_2) = H(Y)$, which implies $H(Y|X_1,X_2) = 0$.\n    $H(Y|X_1,X_2)=0$ would mean that $Y$ is a deterministic function of $(X_1,X_2)$.\n    However, the model is $Y = (X_1 \\oplus X_2) \\oplus N$. Because of the random noise term $N \\sim \\mathrm{Bernoulli}(\\epsilon)$, $Y$ is *not* a deterministic function of $(X_1,X_2)$.\n    As calculated for Option A, $H(Y|X_1,X_2) = H_2(\\epsilon)$. Since $\\epsilon=0.1 \\in (0,1)$, $H_2(\\epsilon)  0$.\n    Therefore, $I(X_1,X_2;Y) = H(Y) - H(Y|X_1,X_2) = 1 - H_2(\\epsilon)  1 = H(Y)$. The claim is false.\n\n2.  **Causal Reasoning Implication:**\n    The reason given is \"because knowing $Y$ determines $(X_1,X_2)$ uniquely\". This means $H(X_1,X_2|Y) = 0$.\n    Let's test this. Suppose we observe $Y=0$. We know $Y=(X_1 \\oplus X_2) \\oplus N$, or $Z = Y \\oplus N$.\n    If $N=0$ (probability $1-\\epsilon$), then $Z=0$. This means $(X_1,X_2)$ could be $(0,0)$ or $(1,1)$.\n    If $N=1$ (probability $\\epsilon$), then $Z=1$. This means $(X_1,X_2)$ could be $(0,1)$ or $(1,0)$.\n    Since we don't know $N$, and even if we knew $N$ (and thus $Z$), we still cannot uniquely determine $(X_1,X_2)$, the statement is false. There is remaining uncertainty about $(X_1,X_2)$ after observing $Y$.\n    In fact, $H(X_1,X_2|Y) = H(X_1,X_2) + H(Y) - H(Y|X_1,X_2) - H(Y) = H(X_1,X_2) - I(X_1,X_2;Y) = 2 - (1 - H_2(\\epsilon)) = 1 + H_2(\\epsilon)  0$.\n\nThe statement and its justification are both incorrect.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{ABCD}$$", "id": "4573615"}, {"introduction": "Beyond synergy, another critical challenge in feature selection is managing redundancy, where multiple features provide similar information about the target variable. Selecting a group of highly correlated features can lead to overly complex and unstable models without improving predictive accuracy. This practice [@problem_id:4573646] introduces the Minimum Redundancy Maximum Relevance (mRMR) framework, guiding you through a concrete calculation that demonstrates how to quantitatively balance a feature's relevance against its redundancy with already-selected features.", "problem": "A translational oncology consortium is validating a panel of three discretized biomarker features $\\{X_1, X_2, X_3\\}$ to predict a binary disease status $Y \\in \\{0,1\\}$, with $Y=1$ indicating disease and $Y=0$ indicating healthy. All variables are binary and arise from a well-specified generative model consistent with the empirical preprocessing used in bioinformatics and medical data analytics. The following fundamental statistical facts are assumed: the distribution of $Y$ is balanced, the conditional distributions $p(x_1 \\mid y)$ and $p(x_3 \\mid y)$ are independent draws given $y$, and a redundant assay causes $X_2$ to be deterministically equal to $X_1$.\n\nThe measurement process is summarized by\n- $p(Y=1) = p(Y=0) = 1/2$,\n- $p(X_1=1 \\mid Y=1) = 9/10$ and $p(X_1=1 \\mid Y=0) = 1/10$,\n- $X_2 = X_1$ deterministically,\n- $p(X_3=1 \\mid Y=1) = 4/5$ and $p(X_3=1 \\mid Y=0) = 1/5$,\nwith conditional independence of $X_1$ and $X_3$ given $Y$.\n\nStarting from first principles in information theory and probability, compute the mutual information $I(X_j;Y)$ for $j \\in \\{1,2,3\\}$ and the pairwise redundancy $I(X_j;X_k)$ for $j \\in \\{2,3\\}$ with $k=1$. Then, perform a single selection iteration of the difference variant of Minimum Redundancy Maximum Relevance (mRMR) using the current selected set $S=\\{X_1\\}$, that is, select the feature index $j$ among $\\{2,3\\}$ that maximizes relevance minus redundancy with respect to $S$. Use natural logarithms $\\ln$ and interpret information values in nats. Report the index $j^{\\ast}$ of the feature selected in this iteration as a single integer. No rounding is required. Express the final answer as the integer without units.", "solution": "The problem statement is evaluated for validity.\n\n### Step 1: Extract Givens\n- Feature set: $\\{X_1, X_2, X_3\\}$, all binary variables.\n- Target variable: $Y \\in \\{0, 1\\}$, a binary variable representing disease status.\n- Prior probability of target: $p(Y=1) = p(Y=0) = \\frac{1}{2}$.\n- Conditional probabilities for $X_1$: $p(X_1=1 \\mid Y=1) = \\frac{9}{10}$ and $p(X_1=1 \\mid Y=0) = \\frac{1}{10}$.\n- Deterministic relationship: $X_2 = X_1$.\n- Conditional probabilities for $X_3$: $p(X_3=1 \\mid Y=1) = \\frac{4}{5}$ and $p(X_3=1 \\mid Y=0) = \\frac{1}{5}$.\n- Conditional independence: $X_1$ and $X_3$ are conditionally independent given $Y$, denoted $X_1 \\perp X_3 \\mid Y$.\n- Task: Perform a single iteration of the difference variant of Minimum Redundancy Maximum Relevance (mRMR) feature selection.\n- Initial selected set: $S = \\{X_1\\}$.\n- Candidate features: $\\{X_2, X_3\\}$.\n- Selection criterion: Maximize the score $D_j = I(X_j;Y) - R_j$ for $j \\in \\{2,3\\}$, where $I(X_j;Y)$ is the relevance and $R_j = I(X_j;X_1)$ is the redundancy with respect to the currently selected feature $X_1$.\n- Logarithm: Natural logarithm ($\\ln$), with information measured in nats.\n- Final Output: Report the selected feature index $j^{\\ast}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem uses standard, well-defined concepts from information theory (mutual information, entropy) and machine learning (mRMR feature selection). These are staple methods in bioinformatics and data analytics.\n- **Well-Posed:** All necessary probabilities and relationships between variables are provided. The objective function for feature selection is explicitly defined. The problem is self-contained and structured to admit a unique solution.\n- **Objective:** The problem is stated using formal mathematical and statistical language, free of ambiguity or subjective claims.\n- **Consistency and Completeness:** The provided probabilities are consistent (e.g., $p(X_1=0|Y=1) = 1 - p(X_1=1|Y=1) = \\frac{1}{10}$). The setup is complete for carrying out the required calculations. The deterministic relation $X_2=X_1$ models perfect redundancy, and the conditional independence $X_1 \\perp X_3 \\mid Y$ models a common scenario where biomarkers are independent measurements conditional on the disease state.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined exercise in applying information-theoretic principles to a feature selection task. A full solution will be provided.\n\nThe objective is to select the feature index $j^{\\ast}$ from the candidate set $\\{2,3\\}$ that maximizes the mRMR score:\n$$j^{\\ast} = \\arg\\max_{j \\in \\{2,3\\}} \\left( I(X_j; Y) - I(X_j; X_1) \\right)$$\nwhere $I(X_j;Y)$ is the relevance of feature $X_j$ and $I(X_j;X_1)$ is its redundancy with the already selected feature $X_1$. We will evaluate the score for $j=2$ and $j=3$.\n\nAll information-theoretic quantities are defined using the natural logarithm. The entropy of a random variable $Z$ is $H(Z) = - \\sum_z p(z)\\ln p(z)$. The mutual information between two variables $A$ and $B$ is $I(A;B) = H(A) - H(A|B)$.\n\n**Analysis for Feature $X_2$**\n\nThe score for $X_2$ is $D_2 = I(X_2; Y) - I(X_2; X_1)$.\n\n1.  **Relevance $I(X_2; Y)$:** Since $X_2 = X_1$ deterministically, $X_2$ and $X_1$ are information-theoretically equivalent with respect to any other variable. Therefore, $I(X_2; Y) = I(X_1; Y)$.\n\n2.  **Redundancy $I(X_2; X_1)$:** The mutual information between a variable and a deterministic copy of itself is the entropy of that variable.\n    $I(X_2; X_1) = H(X_1) - H(X_1|X_2)$. Since $X_1$ is perfectly determined by $X_2$, the conditional entropy $H(X_1|X_2) = 0$. Thus, $I(X_2; X_1) = H(X_1)$.\n\nTo calculate $H(X_1)$, we first need the marginal probability distribution of $X_1$.\n$$p(X_1=1) = p(X_1=1|Y=1)p(Y=1) + p(X_1=1|Y=0)p(Y=0)$$\n$$p(X_1=1) = \\left(\\frac{9}{10}\\right)\\left(\\frac{1}{2}\\right) + \\left(\\frac{1}{10}\\right)\\left(\\frac{1}{2}\\right) = \\frac{9}{20} + \\frac{1}{20} = \\frac{10}{20} = \\frac{1}{2}$$\nSince $X_1$ is binary, $p(X_1=0) = 1 - \\frac{1}{2} = \\frac{1}{2}$. The distribution is balanced. The entropy is:\n$$H(X_1) = -\\left(\\frac{1}{2}\\ln\\frac{1}{2} + \\frac{1}{2}\\ln\\frac{1}{2}\\right) = -\\ln\\frac{1}{2} = \\ln(2)$$\nSo, the redundancy is $I(X_2; X_1) = \\ln(2)$.\n\n3.  **mRMR Score $D_2$:**\n    $$D_2 = I(X_1; Y) - H(X_1)$$\n    Using the identity $I(X_1; Y) = H(X_1) - H(X_1|Y)$, we substitute into the expression for $D_2$:\n    $$D_2 = \\left(H(X_1) - H(X_1|Y)\\right) - H(X_1) = -H(X_1|Y)$$\n    Entropy is always non-negative, $H(X_1|Y) \\ge 0$. It is zero only if $X_1$ is a deterministic function of $Y$. Given the probabilities $p(X_1|Y) \\in \\{\\frac{1}{10}, \\frac{9}{10}\\}$, this is not the case. Therefore, $H(X_1|Y)  0$, which implies $D_2  0$. This negative score reflects the fact that adding a completely redundant feature is penalized.\n\n**Analysis for Feature $X_3$**\n\nThe score for $X_3$ is $D_3 = I(X_3; Y) - I(X_3; X_1)$.\n\nWe can simplify this expression using information-theoretic identities and the problem's conditional independence assumption ($X_1 \\perp X_3 \\mid Y$). The chain rule for mutual information states:\n$$I(A; B,C) = I(A; C) + I(A; B|C)$$\nLet $A=X_3$, $B=Y$, and $C=X_1$.\n$$I(X_3; Y, X_1) = I(X_3; X_1) + I(X_3; Y|X_1)$$\nRearranging for $I(X_3; Y|X_1)$:\n$$I(X_3; Y|X_1) = I(X_3; Y, X_1) - I(X_3; X_1)$$\nWe can also write the chain rule as $I(A; B,C) = I(A;B) + I(A;C|B)$. Applying this to $I(X_3; Y, X_1)$:\n$$I(X_3; Y, X_1) = I(X_3; Y) + I(X_3; X_1|Y)$$\nThe problem states $X_1 \\perp X_3 \\mid Y$, which means their conditional mutual information is zero: $I(X_3; X_1|Y)=0$.\nThus, $I(X_3; Y, X_1) = I(X_3; Y)$.\nSubstituting this back into the expression for $I(X_3; Y|X_1)$:\n$$I(X_3; Y|X_1) = I(X_3; Y) - I(X_3; X_1)$$\nThis is exactly the expression for the mRMR score $D_3$. So, $D_3 = I(X_3; Y|X_1)$.\n\nThe quantity $I(X_3; Y|X_1)$ represents the new information that $X_3$ provides about the target $Y$, given that we already know $X_1$.\n\n**Comparison of Scores**\n\nWe must compare $D_2$ and $D_3$:\n- $D_2 = -H(X_1|Y)$\n- $D_3 = I(X_3; Y|X_1)$\n\nWe have already established that $D_2  0$.\nMutual information is always non-negative, so $D_3 = I(X_3; Y|X_1) \\ge 0$.\n\nEquality $D_3=0$ would hold if and only if $X_3 \\perp Y \\mid X_1$. We can test this condition.\n$X_3 \\perp Y \\mid X_1$ if $p(Y|X_1, X_3) = p(Y|X_1)$ for all values.\nLet's check for $Y=1, X_1=1, X_3=1$.\nUsing Bayes' rule, $p(Y=1|X_1=1) = \\frac{p(X_1=1|Y=1)p(Y=1)}{p(X_1=1)} = \\frac{(\\frac{9}{10})(\\frac{1}{2})}{\\frac{1}{2}} = \\frac{9}{10}$.\nAnd $p(Y=1|X_1=1, X_3=1) = \\frac{p(X_1=1, X_3=1|Y=1)p(Y=1)}{p(X_1=1, X_3=1)}$.\n- Numerator: $p(X_1=1, X_3=1|Y=1)p(Y=1) = p(X_1=1|Y=1)p(X_3=1|Y=1)p(Y=1) = (\\frac{9}{10})(\\frac{4}{5})(\\frac{1}{2}) = \\frac{36}{100}$.\n- Denominator: $p(X_1=1, X_3=1) = \\sum_{y=0,1} p(X_1=1,X_3=1|Y=y)p(Y=y)$.\n$p(X_1=1, X_3=1) = p(X_1=1|Y=1)p(X_3=1|Y=1)p(Y=1) + p(X_1=1|Y=0)p(X_3=1|Y=0)p(Y=0)$\n$p(X_1=1, X_3=1) = (\\frac{9}{10})(\\frac{4}{5})(\\frac{1}{2}) + (\\frac{1}{10})(\\frac{1}{5})(\\frac{1}{2}) = \\frac{36}{100} + \\frac{1}{100} = \\frac{37}{100}$.\n- So, $p(Y=1|X_1=1, X_3=1) = \\frac{36/100}{37/100} = \\frac{36}{37}$.\n\nSince $\\frac{36}{37} \\neq \\frac{9}{10}$, we have $p(Y=1|X_1=1, X_3=1) \\neq p(Y=1|X_1=1)$, which proves that $Y$ and $X_3$ are not conditionally independent given $X_1$.\nTherefore, $I(X_3; Y|X_1)  0$, and $D_3  0$.\n\nWe are selecting the feature that maximizes the mRMR score. Comparing the scores:\n$$D_3  0  D_2$$\nThe maximum score is $D_3$. The feature corresponding to this score is $X_3$.\nThe index of the selected feature is $j^{\\ast}=3$.\n\nThis result is intuitive: mRMR penalizes feature $X_2$ for being perfectly redundant with the existing feature $X_1$. In contrast, it rewards feature $X_3$ because, despite some redundancy (as both $X_1$ and $X_3$ are related to $Y$), it still provides positive new information about $Y$ even after observing $X_1$.", "answer": "$$\\boxed{3}$$", "id": "4573646"}, {"introduction": "Wrapper methods offer a powerful alternative to filters by evaluating feature subsets based on the performance of a specific machine learning model. This capstone exercise [@problem_id:4573605] challenges you to implement one of the most effective wrapper methods, Recursive Feature Elimination (RFE), using a linear Support Vector Machine (SVM). By iteratively training the model, ranking features by their contribution, and eliminating the weakest ones, you will build a complete feature selection engine from the ground up, solidifying your understanding of this robust and widely-used technique.", "problem": "You are given three synthetic cohorts of gene expression profiles together with initial linear Support Vector Machine (SVM) weights. Each cohort consists of a real-valued gene expression matrix and a binary phenotype vector indicating case versus control. The matrices are to be treated as standardized: for each feature (gene), column-wise $z$-score normalization (zero mean and unit variance) must be applied before any computation. You must compute the recursive feature elimination (RFE) order using linear SVM weights with step size $k=5$, iteratively updating the weights after each elimination by retraining the linear SVM on the remaining features. When ranking features for elimination at each iteration, you must primarily use squared weight magnitudes, and you must resolve ties by using feature-label association strength measured by the Pearson chi-squared statistic and information gain (mutual information), both computed via a binary discretization of each standardized feature by its sign. Explicitly, you must rank features by ascending $w_j^2$; if $w_{j_1}^2 = w_{j_2}^2$ within exact equality, then rank by ascending chi-squared; if still tied, rank by ascending information gain; if still tied, rank by ascending original feature index.\n\nFundamental base definitions and facts:\n- A standardized feature $x_j$ is a column whose $z$-scores are computed by $x_{ij}^{\\mathrm{std}} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}$, where $\\mu_j$ is the empirical mean and $\\sigma_j$ is the empirical standard deviation of column $j$.\n- Linear Support Vector Machine (SVM) classification uses a separating hyperplane $f(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x} + b$, with primal objective minimizing the regularized empirical risk given by the hinge loss with $\\ell_2$ regularization, which is a well-tested formulation for maximum-margin linear classification.\n- Recursive Feature Elimination (RFE) removes the least important features in batches, recalculating model weights on the remaining features after each batch removal.\n- Pearson chi-squared for a $2\\times 2$ contingency table with observed counts $O_{r,c}$ and expected counts $E_{r,c}$ is $\\chi^2 = \\sum_{r=1}^{2}\\sum_{c=1}^{2} \\frac{(O_{r,c} - E_{r,c})^2}{E_{r,c}}$, with $E_{r,c} = \\frac{(\\text{row}_r)(\\text{col}_c)}{N}$ under independence.\n- Information gain (mutual information) between two discrete variables $X$ and $Y$ is $I(X;Y) = \\sum_{x}\\sum_{y} p(x,y) \\log_2\\left(\\frac{p(x,y)}{p(x)p(y)}\\right)$, with terms of the form $0\\log(\\cdot)$ defined as $0$.\n\nYour program must implement the following procedure for each test case:\n1. Standardize the input gene expression matrix column-wise to obtain $X^{\\mathrm{std}}$.\n2. Initialize the feature set to all feature indices $\\{0,1,\\dots,d-1\\}$ and set the current weight vector $\\mathbf{w}^{(0)}$ to the provided initial SVM weights.\n3. At each RFE iteration:\n   a. For the current remaining features, compute the primary importance as $s_j = (w_j)^2$.\n   b. For tie resolution of equal $s_j$, discretize each standardized feature $x_j$ by $\\tilde{x}_{ij} = 1$ if $x_{ij}^{\\mathrm{std}} \\ge 0$ and $\\tilde{x}_{ij} = 0$ otherwise, and compute the $2\\times 2$ contingency table with phenotype $y \\in \\{0,1\\}$. Compute the Pearson chi-squared statistic and the information gain (mutual information) between $\\tilde{x}_j$ and $y$.\n   c. Rank features by ascending $s_j$, then ascending chi-squared, then ascending information gain, then ascending original index. Remove the first $\\min(k, \\text{number of remaining features})$ features in this ranking and append their original indices to the global elimination order list.\n   d. Retrain the linear SVM on the remaining features by minimizing the hinge-loss objective with $\\ell_2$ regularization using subgradient descent for a fixed number of epochs and learning rate, warm-starting from the previous weights restricted to the remaining features. Let the labels be $y_i \\in \\{0,1\\}$ but converted internally to $\\{-1,+1\\}$ via $y'_i = 2y_i - 1$. The primal objective is\n   $$\\min_{\\mathbf{w}, b} \\frac{1}{2}\\|\\mathbf{w}\\|_2^2 + C\\sum_{i=1}^{n} \\max\\left(0, 1 - y'_i(\\mathbf{w}^{\\top}\\mathbf{x}_i + b)\\right),$$\n   and a subgradient step updates $\\mathbf{w}$ and $b$ using the set of margin violations $\\{i \\mid y'_i(\\mathbf{w}^{\\top}\\mathbf{x}_i + b)  1\\}$.\n4. Continue until all features have been eliminated. Return the full elimination order.\n\nTest suite:\n- Test Case A (general case with multiple iterations, no ties expected initially):\n  - Samples: $n=20$, features: $d=13$.\n  - Raw gene expression matrix $X$ defined by $X_{i,j} = \\sin(0.17 i + 0.31 j) + 0.03(i - 10) + 0.02 j$ for $i \\in \\{0,\\dots,19\\}$ and $j \\in \\{0,\\dots,12\\}$.\n  - Phenotype vector $y$ defined by $y_i = 1$ if $i \\bmod 4 \\in \\{0,1\\}$ else $y_i = 0$.\n  - Initial weights $\\mathbf{w}^{(0)}$ defined by $w^{(0)}_j = 0.2\\sin(0.5 j) + 0.05 j - 0.4$ for $j \\in \\{0,\\dots,12\\}$.\n  - SVM hyperparameters: regularization $C=1.0$, learning rate $\\eta=0.1$, epochs $T=300$.\n\n- Test Case B (tie scenario):\n  - Samples: $n=16$, features: $d=10$.\n  - Raw gene expression matrix $X$ defined by $X_{i,j} = \\cos(0.21 i - 0.27 j) + 0.04(j - 5)$ for $i \\in \\{0,\\dots,15\\}$ and $j \\in \\{0,\\dots,9\\}$.\n  - Phenotype vector $y$ defined by $y_i = 1$ for $i \\in \\{0,\\dots,7\\}$ and $y_i = 0$ for $i \\in \\{8,\\dots,15\\}$.\n  - Initial weights $\\mathbf{w}^{(0)} = [0.2, -0.2, 0.2, -0.2, 0.05, 0.05, -0.05, -0.05, 0.0, 0.0]$.\n  - SVM hyperparameters: $C=1.0$, $\\eta=0.1$, $T=300$.\n\n- Test Case C (boundary case: number of features less than two steps of $k$, includes zero initial weights):\n  - Samples: $n=12$, features: $d=7$.\n  - Raw gene expression matrix $X$ defined by $X_{i,j} = \\sin(0.5 i)\\cos(0.4 j) + 0.01(i - j)$ for $i \\in \\{0,\\dots,11\\}$ and $j \\in \\{0,\\dots,6\\}$.\n  - Phenotype vector $y$ defined by $y_i = 1$ if $i$ is even, else $y_i = 0$.\n  - Initial weights $\\mathbf{w}^{(0)} = [0.0, 0.0, 0.1, -0.1, 0.05, -0.05, 0.0]$.\n  - SVM hyperparameters: $C=1.0$, $\\eta=0.1$, $T=300$.\n\nAngle units are not applicable. Physical units are not applicable. Percentages are not used; all probability-related quantities must be computed and manipulated as decimals.\n\nYour program must produce a single line of output containing the elimination orders for the three test cases as a comma-separated list of Python-style lists, enclosed in square brackets, for example, \"[[order_case_A],[order_case_B],[order_case_C]]\". Each \"order_case_*\" must be a list of integers indicating the indices of features in the order they were eliminated, expressed with $0$-based indexing and referring to the original feature indices before any elimination.", "solution": "The user-provided problem is valid. It is scientifically sound, well-posed, and all necessary parameters and methods are explicitly defined, allowing for a deterministic and verifiable solution. The task involves implementing a Recursive Feature Elimination (RFE) algorithm using a linear Support Vector Machine (SVM) as the base model, which is a standard and recognized technique in machine learning, particularly for bioinformatics applications like gene selection.\n\nThe solution is developed by methodically implementing each component of the RFE-SVM procedure as specified. The core of the algorithm is an iterative process where features are progressively culled based on their importance, and the model is retrained on the remaining feature subset.\n\n### 1. Overall RFE-SVM Algorithm\nThe procedure begins with the full set of $d$ features. In each step, the features are ranked according to a multi-level criterion. A fixed number of the least important features, $k=5$, are removed. A linear SVM is then retrained on the remaining features to update the feature importance scores (i.e., the model weights). This process is repeated until no features are left. The final output is a list of the original feature indices, ordered by their elimination time.\n\nThe process for a given dataset $(X, y)$ and initial weights $\\mathbf{w}^{(0)}$ is as follows:\n1.  Compute the standardized data matrix $X^{\\mathrm{std}}$. This matrix is used for all subsequent computations.\n2.  Initialize the set of active features $F$ to $\\{0, 1, \\dots, d-1\\}$. Initialize the elimination order list $E$ as empty. The current weights $\\mathbf{w}$ are set to $\\mathbf{w}^{(0)}$, and the bias $b$ is initialized to $0$.\n3.  While $F$ is not empty:\n    a. Determine the number of features to remove in this step: $k' = \\min(k, |F|)$.\n    b. For each feature $j \\in F$, calculate a tuple of ranking scores: $(s_1, s_2, s_3, s_4)$, where $s_1$ is the primary score and $s_2, s_3, s_4$ are for tie-breaking.\n    c. Sort the features in $F$ in ascending order based on these tuples.\n    d. Select the first $k'$ features from the sorted list. Add their original indices to the elimination list $E$.\n    e. Remove these $k'$ features from $F$.\n    f. If $F$ is not empty, retrain the linear SVM using the data columns from $X^{\\mathrm{std}}$ corresponding to the features in the updated set $F$. The weights from the current step are used to warm-start the training for the next step. The new weights and bias become the current model parameters.\n4.  Return the complete elimination order $E$.\n\n### 2. Data Standardization\nBefore any other computation, the raw gene expression matrix $X$ is standardized column-wise (per feature). This is a standard preprocessing step in machine learning that scales features to have a common range, preventing features with large variances from dominating the model fitting process. For each feature column $j$, the $z$-score is computed for each sample $i$:\n$$\nx_{ij}^{\\mathrm{std}} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}\n$$\nwhere $\\mu_j$ is the mean and $\\sigma_j$ is the standard deviation of feature $j$ across all samples. If a feature has zero standard deviation ($\\sigma_j = 0$), its standardized values will all be $0$.\n\n### 3. Feature Ranking and Tie-Breaking\nAt each RFE step, features are ranked based on a hierarchical criterion. The goal is to eliminate features that are least important for the classification task.\n\n1.  **Primary Criterion (Squared Weight)**: The primary measure of a feature's importance is the magnitude of its corresponding weight in the linear SVM model. A small weight magnitude suggests the feature has little influence on the decision boundary. We use the squared weight $s_1 = w_j^2$. Features are ranked in ascending order of $s_1$.\n2.  **Secondary Criterion ($\\chi^2$ Statistic)**: To resolve ties in $w_j^2$, we assess the statistical association between a feature and the class label. The standardized feature column $x_j^{\\mathrm{std}}$ is binarized: $\\tilde{x}_{ij} = 1$ if $x_{ij}^{\\mathrm{std}} \\ge 0$, and $\\tilde{x}_{ij} = 0$ otherwise. A $2 \\times 2$ contingency table of observed counts $O$ is constructed for the binary feature $\\tilde{x}_j$ and the binary phenotype $y$. The Pearson's chi-squared statistic is then computed:\n    $$\n    s_2 = \\chi^2 = \\sum_{r \\in \\{0,1\\}} \\sum_{c \\in \\{0,1\\}} \\frac{(O_{rc} - E_{rc})^2}{E_{rc}}\n    $$\n    where $E_{rc}$ are the expected counts under the null hypothesis of independence. A smaller $\\chi^2$ value indicates weaker association, so features are ranked in ascending order of $s_2$.\n3.  **Tertiary Criterion (Information Gain)**: If features are still tied, we use information gain (or mutual information), which measures the reduction in uncertainty about the phenotype $y$ given the discretized feature $\\tilde{x}_j$. It is calculated from the same contingency table:\n    $$\n    s_3 = I(\\tilde{X}_j; Y) = \\sum_{\\tilde{x} \\in \\{0,1\\}} \\sum_{y \\in \\{0,1\\}} p(\\tilde{x}, y) \\log_2\\left(\\frac{p(\\tilde{x}, y)}{p(\\tilde{x})p(y)}\\right)\n    $$\n    where $p(\\cdot)$ are probabilities estimated from the sample counts. A smaller information gain signifies less predictive power, so features are ranked in ascending order of $s_3$.\n4.  **Final Criterion (Original Index)**: If a tie persists after the above three criteria, it is broken by the original feature index $s_4 = j$, in ascending order. This ensures a unique, deterministic ranking.\n\n### 4. SVM Retraining via Subgradient Descent\nAfter eliminating a batch of features, the SVM model must be retrained on the remaining feature subset to obtain updated weights for the next ranking. The training process minimizes the primal objective function for a linear SVM, which balances model complexity (regularization term) and classification error on the training data (hinge loss term). The labels $y_i \\in \\{0,1\\}$ are mapped to $y'_i \\in \\{-1,1\\}$ for the SVM formulation.\n\nObjective Function:\n$$\n\\min_{\\mathbf{w}, b} \\frac{1}{2}\\|\\mathbf{w}\\|_2^2 + C \\sum_{i=1}^{n} \\max\\left(0, 1 - y'_i(\\mathbf{w}^{\\top}\\mathbf{x}_i + b)\\right)\n$$\nThis minimization is performed using batch subgradient descent. For a fixed number of epochs $T$, the weights $\\mathbf{w}$ and bias $b$ are updated iteratively. In each epoch, the subgradients of the objective function with respect to all samples are computed and aggregated. Let $V$ be the set of indices of samples that violate the margin condition, i.e., $V = \\{i \\mid y'_i(\\mathbf{w}^{\\top}\\mathbf{x}_i + b)  1\\}$. The batch subgradients are:\n$$\n\\mathbf{g}_{\\mathbf{w}} = \\mathbf{w} - C \\sum_{i \\in V} y'_i \\mathbf{x}_i \\quad \\quad \\quad g_b = -C \\sum_{i \\in V} y'_i\n$$\nThe update rules with learning rate $\\eta$ are:\n$$\n\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\mathbf{g}_{\\mathbf{w}} \\quad \\quad \\quad b \\leftarrow b - \\eta g_b\n$$\nTo accelerate convergence, the training is warm-started: the weights $\\mathbf{w}$ and bias $b$ from the previous RFE step are used as the initial values for the current training run, with the weight vector being restricted to the dimensions of the currently active features.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef standardize(X):\n    \"\"\"Applies z-score normalization to each column of X.\"\"\"\n    mean = np.mean(X, axis=0)\n    std = np.std(X, axis=0)\n    # Avoid division by zero for columns with zero variance\n    std_safe = np.where(std == 0, 1.0, std)\n    return (X - mean) / std_safe\n\ndef calculate_tiebreakers(X_std_col, y):\n    \"\"\"\n    Calculates chi-squared and information gain for a discretized feature.\n    \"\"\"\n    n_samples = len(y)\n    X_binned = (X_std_col >= 0).astype(int)\n\n    # Build 2x2 contingency table\n    contingency_table = np.zeros((2, 2), dtype=float)\n    for i in range(n_samples):\n        contingency_table[X_binned[i], y[i]] += 1\n    \n    # 1. Chi-squared statistic\n    # correction=False for Pearson's chi-squared test\n    # If all values are in one row/col, chi2_contingency returns chi2=0, which is correct.\n    chi2, _, _, _ = chi2_contingency(contingency_table, correction=False)\n\n    # 2. Information Gain (Mutual Information)\n    p_xy = contingency_table / n_samples\n    p_x = np.sum(p_xy, axis=1)\n    p_y = np.sum(p_xy, axis=0)\n\n    ig = 0.0\n    for r in range(2):\n        for c in range(2):\n            if p_xy[r, c] > 1e-12: # Avoid log(0)\n                # p_x[r] and p_y[c] will also be > 0 if p_xy[r,c] > 0\n                ig += p_xy[r, c] * np.log2(p_xy[r, c] / (p_x[r] * p_y[c]))\n\n    return chi2, ig\n\ndef train_svm(X_train, y_train, w_initial, b_initial, C, eta, T):\n    \"\"\"\n    Trains a linear SVM using batch subgradient descent.\n    \"\"\"\n    y_svm = 2 * y_train - 1\n    w = w_initial.copy()\n    b = b_initial\n\n    for _ in range(T):\n        margins = y_svm * (X_train @ w + b)\n        \n        # Identify margin violations\n        violations_idx = np.where(margins  1)[0]\n        \n        # Calculate batch subgradients\n        # Grad w.r.t. w for samples in violation set\n        grad_w_loss = -C * (y_svm[violations_idx].reshape(-1, 1) * X_train[violations_idx, :]).sum(axis=0)\n        # Add grad of regularizer\n        grad_w = w + grad_w_loss\n\n        # Grad w.r.t. b\n        grad_b = -C * y_svm[violations_idx].sum()\n        \n        # Update weights and bias\n        w -= eta * grad_w\n        b -= eta * grad_b\n        \n    return w, b\n\ndef perform_rfe(X, y, w_initial, k, C, eta, T):\n    \"\"\"\n    Performs Recursive Feature Elimination with a linear SVM.\n    \"\"\"\n    n_samples, n_features = X.shape\n    X_std = standardize(X)\n\n    remaining_indices = list(range(n_features))\n    elimination_order = []\n    \n    current_w = w_initial.copy()\n    current_b = 0.0\n\n    while len(remaining_indices) > 0:\n        d_current = len(remaining_indices)\n        num_to_remove = min(k, d_current)\n\n        scores = []\n        for i, original_idx in enumerate(remaining_indices):\n            w_j = current_w[i]\n            s1_w2 = w_j**2\n            \n            s2_chi2, s3_ig = calculate_tiebreakers(X_std[:, original_idx], y)\n\n            s4_orig_idx = original_idx\n            \n            scores.append((s1_w2, s2_chi2, s3_ig, s4_orig_idx, original_idx))\n        \n        scores.sort()\n        \n        to_eliminate_orig_indices = [s[4] for s in scores[:num_to_remove]]\n        elimination_order.extend(to_eliminate_orig_indices)\n\n        new_remaining_indices = [idx for idx in remaining_indices if idx not in to_eliminate_orig_indices]\n\n        if not new_remaining_indices:\n            break\n\n        # Prepare for retraining\n        w_map = {old_idx: i for i, old_idx in enumerate(remaining_indices)}\n        w_warm_start = np.array([current_w[w_map[idx]] for idx in new_remaining_indices])\n        b_warm_start = current_b\n        \n        X_train_subset = X_std[:, new_remaining_indices]\n\n        current_w, current_b = train_svm(X_train_subset, y, w_warm_start, b_warm_start, C, eta, T)\n        remaining_indices = new_remaining_indices\n\n    return elimination_order\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    k = 5\n    C = 1.0\n    eta = 0.1\n    T = 300\n\n    # Test Case A\n    n_a, d_a = 20, 13\n    i_a = np.arange(n_a).reshape(-1, 1)\n    j_a = np.arange(d_a).reshape(1, -1)\n    X_a = np.sin(0.17 * i_a + 0.31 * j_a) + 0.03 * (i_a - 10) + 0.02 * j_a\n    y_a = np.zeros(n_a, dtype=int)\n    y_a[np.isin(np.arange(n_a) % 4, [0, 1])] = 1\n    w_init_a = 0.2 * np.sin(0.5 * np.arange(d_a)) + 0.05 * np.arange(d_a) - 0.4\n    \n    # Test Case B\n    n_b, d_b = 16, 10\n    i_b = np.arange(n_b).reshape(-1, 1)\n    j_b = np.arange(d_b).reshape(1, -1)\n    X_b = np.cos(0.21 * i_b - 0.27 * j_b) + 0.04 * (j_b - 5)\n    y_b = np.zeros(n_b, dtype=int)\n    y_b[:8] = 1\n    w_init_b = np.array([0.2, -0.2, 0.2, -0.2, 0.05, 0.05, -0.05, -0.05, 0.0, 0.0])\n\n    # Test Case C\n    n_c, d_c = 12, 7\n    i_c = np.arange(n_c).reshape(-1, 1)\n    j_c = np.arange(d_c).reshape(1, -1)\n    X_c = np.sin(0.5 * i_c) * np.cos(0.4 * j_c) + 0.01 * (i_c - j_c)\n    y_c = (np.arange(n_c) % 2 == 0).astype(int)\n    w_init_c = np.array([0.0, 0.0, 0.1, -0.1, 0.05, -0.05, 0.0])\n\n    test_cases = [\n        (X_a, y_a, w_init_a, k, C, eta, T),\n        (X_b, y_b, w_init_b, k, C, eta, T),\n        (X_c, y_c, w_init_c, k, C, eta, T),\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = perform_rfe(*case_params)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "4573605"}]}