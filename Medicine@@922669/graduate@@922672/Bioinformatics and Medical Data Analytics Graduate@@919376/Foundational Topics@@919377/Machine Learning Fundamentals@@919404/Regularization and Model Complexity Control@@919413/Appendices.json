{"hands_on_practices": [{"introduction": "To effectively apply regularization, we must first understand the optimization algorithms that make it computationally feasible, especially in high-dimensional settings. This practice delves into the mechanics of coordinate descent, a simple yet powerful algorithm for solving regularized regression problems. By deriving the closed-form updates for both ridge and lasso penalties from first principles, you will gain a deeper intuition for how these methods operate at a fundamental level. [@problem_id:4605272]", "problem": "In a gene expression prediction task from a cohort of patients, let the response vector be $y \\in \\mathbb{R}^{n}$ (e.g., a continuous clinical phenotype) and the design matrix be $X \\in \\mathbb{R}^{n \\times p}$, where the $j$-th column $x_{j} \\in \\mathbb{R}^{n}$ corresponds to the measured abundance of a candidate biomarker across the $n$ patients. Consider the regularized linear regression objective that combines a smooth squared-error data-fitting term with either an $\\ell_{2}$-norm penalty (ridge) or an $\\ell_{1}$-norm penalty (lasso). Define the smooth component $g(\\beta)$ and the non-smooth component $h(\\beta)$ by\n$$\ng(\\beta) \\equiv \\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\alpha}{2}\\|\\beta\\|_{2}^{2}, \\quad h(\\beta) \\equiv \\lambda \\|\\beta\\|_{1},\n$$\nwhere $\\beta \\in \\mathbb{R}^{p}$, $\\alpha \\ge 0$, and $\\lambda \\ge 0$. For the pure ridge problem, set $\\lambda = 0$ and $\\alpha > 0$. For the pure lasso problem, set $\\alpha = 0$ and $\\lambda > 0$. Let the coordinate descent algorithm update one coefficient at a time while holding the others fixed. For coordinate $j \\in \\{1,\\dots,p\\}$, define the partial residual\n$$\nr^{(j)} \\equiv y - X\\beta + x_{j}\\beta_{j},\n$$\nso that $y - X\\beta = r^{(j)} - x_{j}\\beta_{j}$. Starting from the definitions of coordinate-wise minimization for convex functions, the subgradient optimality condition for $\\ell_{1}$-regularization, and the definition of coordinate-wise Lipschitz continuity of the gradient of a smooth function, perform the following tasks:\n(i) Derive the exact closed-form coordinate descent update for $\\beta_{j}$ in the ridge case, expressed only in terms of $n$, $\\alpha$, $x_{j}$, and $r^{(j)}$.\n(ii) Derive the exact closed-form coordinate descent update for $\\beta_{j}$ in the lasso case, expressed only in terms of $n$, $\\lambda$, $x_{j}$, and $r^{(j)}$, using standard scalar functions such as the absolute value, maximum, and sign.\n(iii) Identify a valid coordinate-wise Lipschitz constant $L_{j}$ for the partial derivative of the smooth component $g(\\beta)$ with respect to $\\beta_{j}$, expressed in terms of $n$, $\\alpha$, and $x_{j}$, and state sufficient conditions for convergence of cyclic coordinate descent in terms of convexity and coordinate-wise Lipschitz continuity of the gradient of $g(\\beta)$ combined with separability of $h(\\beta)$. Provide your final answer as a row matrix containing, in order, the ridge update for $\\beta_{j}$, the lasso update for $\\beta_{j}$, and the coordinate-wise Lipschitz constant $L_{j}$. No numerical approximation or rounding is required, and no units are needed.", "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in standard optimization and statistical learning theory, well-posed with all necessary information provided, and phrased objectively. It is a formal and relevant problem in the specified field. I will now proceed with the solution.\n\nThe objective function to minimize is $F(\\beta) = g(\\beta) + h(\\beta)$, where\n$$\ng(\\beta) = \\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\alpha}{2}\\|\\beta\\|_{2}^{2}\n$$\nand\n$$\nh(\\beta) = \\lambda \\|\\beta\\|_{1}\n$$\nCoordinate descent updates a single coefficient $\\beta_j$ at a time, holding all other coefficients $\\beta_{k \\neq j}$ fixed. The objective function can be written as a function of only $\\beta_j$ by treating all other components of $\\beta$ as constants.\n\nWe use the provided definition of the partial residual, $r^{(j)} \\equiv y - X\\beta + x_{j}\\beta_{j} = y - \\sum_{k \\neq j} x_k \\beta_k$. This allows us to express the full residual as $y - X\\beta = r^{(j)} - x_j \\beta_j$.\n\nThe data-fitting term becomes:\n$$\n\\|y - X\\beta\\|_{2}^{2} = \\|r^{(j)} - x_j \\beta_j\\|_{2}^{2} = (r^{(j)} - x_j \\beta_j)^T(r^{(j)} - x_j \\beta_j) = (r^{(j)})^T r^{(j)} - 2\\beta_j x_j^T r^{(j)} + \\beta_j^2 x_j^T x_j\n$$\nNote that $x_j^T x_j = \\|x_j\\|_2^2$. The term $(r^{(j)})^T r^{(j)}$ is constant with respect to the optimization over $\\beta_j$.\n\nThe $\\ell_2$ penalty term becomes:\n$$\n\\|\\beta\\|_{2}^{2} = \\sum_{k=1}^{p} \\beta_k^2 = \\beta_j^2 + \\sum_{k \\neq j} \\beta_k^2\n$$\nThe term $\\sum_{k \\neq j} \\beta_k^2$ is constant with respect to the optimization over $\\beta_j$.\n\nThe $\\ell_1$ penalty term becomes:\n$$\n\\|\\beta\\|_{1} = \\sum_{k=1}^{p} |\\beta_k| = |\\beta_j| + \\sum_{k \\neq j} |\\beta_k|\n$$\nThe term $\\sum_{k \\neq j} |\\beta_k|$ is constant with respect to the optimization over $\\beta_j$.\n\n### (i) Ridge Regression Coordinate Update ($\\lambda = 0, \\alpha > 0$)\n\nFor pure ridge regression, the objective function is $F(\\beta) = g(\\beta)$ with $\\lambda=0$. We want to find the value of $\\beta_j$ that minimizes $F(\\beta)$ while keeping $\\beta_{k \\neq j}$ fixed. The terms in $F(\\beta)$ that depend on $\\beta_j$ are:\n$$\nF(\\beta_j) = \\frac{1}{2n} (-2\\beta_j x_j^T r^{(j)} + \\beta_j^2 \\|x_j\\|_2^2) + \\frac{\\alpha}{2} \\beta_j^2 + \\text{constants}\n$$\nThis is a smooth, convex (quadratic) function of $\\beta_j$. We find the minimum by taking the derivative with respect to $\\beta_j$ and setting it to zero.\n$$\n\\frac{dF}{d\\beta_j} = \\frac{1}{2n} (-2 x_j^T r^{(j)} + 2\\beta_j \\|x_j\\|_2^2) + \\alpha \\beta_j = 0\n$$\n$$\n-\\frac{1}{n} x_j^T r^{(j)} + \\frac{1}{n} \\beta_j \\|x_j\\|_2^2 + \\alpha \\beta_j = 0\n$$\nNow, we solve for $\\beta_j$:\n$$\n\\beta_j \\left(\\frac{1}{n} \\|x_j\\|_2^2 + \\alpha\\right) = \\frac{1}{n} x_j^T r^{(j)}\n$$\n$$\n\\beta_j \\left(\\|x_j\\|_2^2 + n\\alpha\\right) = x_j^T r^{(j)}\n$$\nThe coordinate update for $\\beta_j$ in the ridge case is:\n$$\n\\beta_j = \\frac{x_j^T r^{(j)}}{\\|x_j\\|_2^2 + n\\alpha}\n$$\n\n### (ii) Lasso Regression Coordinate Update ($\\alpha = 0, \\lambda > 0$)\n\nFor pure lasso regression, the objective function is $F(\\beta) = g(\\beta) + h(\\beta)$ with $\\alpha=0$. The task is to minimize the following function of $\\beta_j$:\n$$\nF(\\beta_j) = \\frac{1}{2n} \\|r^{(j)} - x_j \\beta_j\\|_{2}^{2} + \\lambda |\\beta_j| + \\text{constants}\n$$\n$$\nF(\\beta_j) = \\frac{1}{2n} (\\beta_j^2 \\|x_j\\|_2^2 - 2\\beta_j x_j^T r^{(j)}) + \\lambda |\\beta_j| + \\text{constants}\n$$\nThis function is convex but non-smooth due to the absolute value term. We use the subgradient optimality condition, which states that $0$ must be in the subdifferential of $F(\\beta_j)$ at the minimum. Let's denote $c_j = x_j^T r^{(j)}$ and $a_j = \\|x_j\\|_2^2$. The objective simplifies to minimizing $f(\\beta_j) = \\frac{1}{2n}(a_j \\beta_j^2 - 2c_j \\beta_j) + \\lambda|\\beta_j|$.\nThe subdifferential is $\\partial F(\\beta_j) = \\{ \\frac{1}{n}(a_j\\beta_j - c_j) + \\lambda \\cdot s \\}$, where $s = \\text{sign}(\\beta_j)$ if $\\beta_j \\neq 0$ and $s \\in [-1, 1]$ if $\\beta_j = 0$.\nSetting $0 \\in \\partial F(\\beta_j)$:\n$$\n0 = \\frac{1}{n}(a_j\\beta_j - c_j) + \\lambda s \\implies c_j - a_j\\beta_j = n\\lambda s\n$$\nCase 1: $\\beta_j > 0$. Then $s=1$.\n$c_j - a_j\\beta_j = n\\lambda \\implies \\beta_j = \\frac{c_j - n\\lambda}{a_j}$. This is valid only if $\\beta_j > 0$, which requires $c_j > n\\lambda$.\n\nCase 2: $\\beta_j < 0$. Then $s=-1$.\n$c_j - a_j\\beta_j = -n\\lambda \\implies \\beta_j = \\frac{c_j + n\\lambda}{a_j}$. This is valid only if $\\beta_j < 0$, which requires $c_j < -n\\lambda$.\n\nCase 3: $\\beta_j = 0$. Then $s \\in [-1, 1]$.\n$c_j - a_j(0) = n\\lambda s \\implies c_j = n\\lambda s$. Since $s \\in [-1, 1]$, this means $|c_j| \\le n\\lambda$.\n\nCombining these three cases and substituting back $c_j = x_j^T r^{(j)}$ and $a_j = \\|x_j\\|_2^2$ (assuming $\\|x_j\\|_2^2 > 0$):\n- If $x_j^T r^{(j)} > n\\lambda$, then $\\beta_j = \\frac{x_j^T r^{(j)} - n\\lambda}{\\|x_j\\|_2^2}$.\n- If $x_j^T r^{(j)} < -n\\lambda$, then $\\beta_j = \\frac{x_j^T r^{(j)} + n\\lambda}{\\|x_j\\|_2^2}$.\n- If $|x_j^T r^{(j)}| \\le n\\lambda$, then $\\beta_j = 0$.\n\nThis is the soft-thresholding operator. The update can be written compactly using the sign and maximum functions:\n$$\n\\beta_j = \\frac{1}{\\|x_j\\|_2^2} \\text{sign}(x_j^T r^{(j)}) \\max(|x_j^T r^{(j)}| - n\\lambda, 0)\n$$\n\n### (iii) Coordinate-wise Lipschitz Constant and Convergence Conditions\n\nA coordinate-wise Lipschitz constant $L_j$ for the gradient of the smooth part, $\\nabla_j g(\\beta)$, is an upper bound on how much this partial derivative can change as we change only $\\beta_j$. Formally, $|\\nabla_j g(\\beta + h e_j) - \\nabla_j g(\\beta)| \\le L_j |h|$ for any scalar $h$. For twice-differentiable functions, $L_j$ can be taken as the maximum absolute value of the second partial derivative with respect to $\\beta_j$.\n\nThe smooth component is $g(\\beta) = \\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\alpha}{2}\\|\\beta\\|_{2}^{2}$.\nFirst, we find the partial derivative $\\nabla_j g(\\beta) = \\frac{\\partial g}{\\partial \\beta_j}$:\n$$\n\\frac{\\partial g}{\\partial \\beta_j} = \\frac{1}{2n} \\frac{\\partial}{\\partial \\beta_j} (y - X\\beta)^T(y - X\\beta) + \\frac{\\alpha}{2} \\frac{\\partial}{\\partial \\beta_j} \\sum_k \\beta_k^2\n$$\n$$\n\\frac{\\partial g}{\\partial \\beta_j} = \\frac{1}{n} (X\\beta - y)^T (-x_j) + \\alpha \\beta_j = -\\frac{1}{n} (y - X\\beta)^T x_j + \\alpha \\beta_j = \\frac{1}{n} x_j^T(X\\beta - y) + \\alpha \\beta_j\n$$\nNow, we find the second partial derivative $\\frac{\\partial^2 g}{\\partial \\beta_j^2}$:\n$$\n\\frac{\\partial^2 g}{\\partial \\beta_j^2} = \\frac{\\partial}{\\partial \\beta_j} \\left( \\frac{1}{n} x_j^T(X\\beta - y) + \\alpha\\beta_j \\right) = \\frac{1}{n} x_j^T x_j + \\alpha = \\frac{1}{n}\\|x_j\\|_2^2 + \\alpha\n$$\nThis second derivative is constant with respect to $\\beta$. Thus, a valid coordinate-wise Lipschitz constant for $\\nabla_j g$ is precisely this value:\n$$\nL_j = \\frac{1}{n}\\|x_j\\|_2^2 + \\alpha\n$$\nFor the convergence of cyclic coordinate descent on an objective of the form $F(\\beta) = g(\\beta) + h(\\beta)$, sufficient conditions are:\n1.  $g(\\beta)$ is convex and its gradient $\\nabla g(\\beta)$ is coordinate-wise Lipschitz continuous. The function $g(\\beta)$ is convex because its Hessian, $\\nabla^2 g(\\beta) = \\frac{1}{n}X^T X + \\alpha I$, is positive semi-definite (since $X^T X$ is positive semi-definite and $\\alpha \\ge 0$). We have just shown that its gradient is coordinate-wise Lipschitz continuous with constant $L_j = \\frac{1}{n}\\|x_j\\|_2^2 + \\alpha$.\n2.  $h(\\beta)$ is convex and separable. The function $h(\\beta) = \\lambda \\|\\beta\\|_1 = \\lambda \\sum_j |\\beta_j|$ is a sum of convex functions, hence convex. It is separable because it is a sum of functions that each depend on only one component $\\beta_j$.\n\nThese conditions ensure that the sequence of iterates generated by cyclic coordinate descent converges to a global minimizer of the objective function $F(\\beta)$.", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{x_j^T r^{(j)}}{\\|x_j\\|_2^2 + n\\alpha} & \\frac{\\text{sign}(x_j^T r^{(j)}) \\max(|x_j^T r^{(j)}| - n\\lambda, 0)}{\\|x_j\\|_2^2} & \\frac{1}{n}\\|x_j\\|_2^2 + \\alpha \\end{pmatrix} } $$", "id": "4605272"}, {"introduction": "Moving from theory to application, this exercise challenges you to implement the coordinate descent algorithm for LASSO regression. You will translate the soft-thresholding update rule into functional code and use it to observe one of LASSO's most important features: the regularization path. By tracing how coefficients enter the model as the penalty $\\lambda$ decreases, you will build a practical understanding of how LASSO performs variable selection. [@problem_id:4605278]", "problem": "You are given a synthetic Single Nucleotide Polymorphism (SNP) genotype dataset and a continuous phenotype constructed from a sparse linear model. The features are coded as minor allele counts $0$, $1$, or $2$, which are then standardized to have zero mean and unit variance, and the phenotype is centered to zero mean. Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem for linear regression with coordinate descent. Your task is to implement a complete, runnable program that computes the LASSO solution along a decreasing grid of regularization strengths $\\lambda$ using coordinate descent and reports the path of coefficient entry: the order in which standardized feature coefficients become nonzero as $\\lambda$ decreases. You must strictly adhere to the final output formatting requirement stated below.\n\nFundamental base to use:\n- The linear model for a centered phenotype $y \\in \\mathbb{R}^n$ and standardized design matrix $X \\in \\mathbb{R}^{n \\times p}$, with columns centered and scaled so that $\\frac{1}{n}\\sum_{i=1}^n x_{ij}^2 = 1$ for each feature index $j$.\n- The LASSO objective, which penalizes the $\\ell_1$ norm of the coefficients: minimize\n$$\n\\frac{1}{2n}\\lVert y - X\\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1,\n$$\nwhere $\\beta \\in \\mathbb{R}^p$ are the coefficients and $\\lambda \\ge 0$ is the regularization parameter.\n- The coordinate-wise soft-threshold update for standardized features derived from subgradient/Karush–Kuhn–Tucker (KKT) stationarity:\n$$\n\\beta_j \\leftarrow \\frac{1}{\\frac{1}{n}\\lVert X_{\\cdot j} \\rVert_2^2} \\cdot S\\!\\left(\\frac{1}{n}X_{\\cdot j}^\\top \\left(y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k\\right), \\lambda \\right),\n$$\nwith the soft-threshold operator defined by $S(a,t) = \\operatorname{sign}(a)\\max(|a| - t, 0)$. Under the given standardization, $\\frac{1}{n}\\lVert X_{\\cdot j} \\rVert_2^2 = 1$ and the denominator simplifies to $1$.\n\nData construction:\n- Let $n = 60$ individuals and $p = 8$ SNPs. For reproducibility, draw each SNP column $X_{\\cdot j}$ independently as $\\operatorname{Binomial}(2, q_j)$, where the minor allele frequencies (MAF) are fixed as $(q_1,\\dots,q_8) = (0.05, 0.10, 0.20, 0.15, 0.30, 0.25, 0.40, 0.35)$ using a fixed random seed. After generation, standardize each feature to have zero mean and unit variance as above.\n- Construct the centered phenotype $y$ from a sparse linear model with fixed true coefficients $\\beta^\\star = (0.0, 1.2, 0.0, -0.8, 0.0, 0.0, 0.5, 0.0)$ and a deterministic perturbation $\\epsilon_i = 0.1\\sin(i)$ for index $i = 0, 1, \\dots, n-1$, and then center $y$:\n$$\ny \\leftarrow X\\beta^\\star + \\epsilon, \\quad \\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i, \\quad y \\leftarrow y - \\bar{y}.\n$$\n\nAlgorithmic requirements:\n- Use coordinate descent with cyclic updates over $j = 1, \\dots, p$ to solve the LASSO objective at each $\\lambda$ until convergence. Use a warm-start across the decreasing $\\lambda$ path.\n- Use the stopping criterion $\\max_j |\\Delta\\beta_j| < 10^{-6}$ or a maximum of $1000$ full coordinate sweeps, whichever occurs first.\n- Let $\\lambda_{\\max} = \\max_j \\left|\\frac{1}{n} X_{\\cdot j}^\\top y\\right|$, for which the solution is $\\beta = 0$. Build decreasing $\\lambda$ grids as specified in the test suite below from $\\lambda_{\\max}$ to smaller values.\n\nPath of coefficient entry:\n- For each feature index $j \\in \\{0,1,\\dots,p-1\\}$, define its \"entry $\\lambda$\" as the first $\\lambda$ value (in the decreasing grid) at which the converged $\\beta_j$ becomes nonzero (numerically, treat $|\\beta_j| > 10^{-8}$ as nonzero). If a coefficient never becomes nonzero over the grid, it has no entry and should not appear in the order list. In the presence of ties (multiple features entering at the same grid $\\lambda$), list features in ascending index order. The path of coefficient entry is the ordered list of feature indices according to first entry as $\\lambda$ decreases.\n\nTest suite:\nImplement the following four test cases using the same base dataset, with explicit $\\lambda$ grids. The design ensures a happy path, boundary, extreme, and collinearity edge case coverage.\n\n- Test Case 1 (happy path): Use a geometric grid of $50$ values from $\\lambda_{\\max}$ down to $0.01\\lambda_{\\max}$, i.e., $\\lambda_k = \\lambda_{\\max} \\cdot r^{k-1}$ with common ratio $r$ chosen so that $\\lambda_{50} = 0.01\\lambda_{\\max}$. Report the ordered list of feature indices that first become nonzero along this path.\n- Test Case 2 (boundary case): Use the singleton grid $\\{\\lambda_{\\max}\\}$ only. Report the ordered list of feature indices (expected to be empty).\n- Test Case 3 (extreme small $\\lambda$): Use a geometric grid of $120$ values from $\\lambda_{\\max}$ down to $10^{-6}\\lambda_{\\max}$. Report the ordered list of feature indices that first become nonzero along this path.\n- Test Case 4 (collinearity edge case): Create an augmented dataset by duplicating SNP feature index $3$ to form a ninth feature (resulting in $p=9$). Standardize the augmented matrix column-wise using the same protocol, recompute $\\lambda_{\\max}$ for the augmented dataset, and use a geometric grid of $50$ values from this augmented $\\lambda_{\\max}$ down to $0.01$ times it. Report the ordered list of feature indices that first become nonzero along this path.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element should itself be a bracketed, comma-separated list of integers representing the feature indices (zero-based) in their order of first entry for the corresponding test case. For example, an output with four test cases might look like \"[[1,3,2],[],[1,7,5,3,6],[1,3,8]]\". No other text should be printed.\n\nNo physical units or angle units are involved. All numeric answers are pure numbers. Each test-case result must be a list of integers as specified.", "solution": "The user has provided a well-defined computational problem in the domain of statistical machine learning, specifically focusing on the implementation of the LASSO regression model. The task is to compute the coefficient entry path for a synthetic single nucleotide polymorphism (SNP) dataset using a coordinate descent algorithm. The problem statement is validated to be scientifically sound, well-posed, and complete.\n\n### Step 1: Problem Validation\n\n**1.1. Givens Extraction:**\n- **Model:** Linear regression with LASSO penalty: $\\min_{\\beta} \\frac{1}{2n}\\lVert y - X\\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1$.\n- **Data Properties:** $y$ is a centered phenotype vector in $\\mathbb{R}^n$. $X$ is a standardized design matrix in $\\mathbb{R}^{n \\times p}$ with columns centered to zero mean and scaled such that $\\frac{1}{n}\\sum_{i=1}^n x_{ij}^2 = 1$.\n- **Algorithm:** Coordinate descent with cyclic updates. The update rule for a standardized feature $j$ is $\\beta_j \\leftarrow S(\\rho_j, \\lambda)$, where $\\rho_j = \\frac{1}{n}X_{\\cdot j}^\\top (y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k)$ and $S(a,t) = \\operatorname{sign}(a)\\max(|a| - t, 0)$ is the soft-thresholding operator.\n- **Convergence Criteria:** Maximum change in coefficients $\\max_j |\\Delta\\beta_j| < 10^{-6}$ or a maximum of $1000$ sweeps.\n- **Regularization Path:** The path starts from $\\lambda_{\\max} = \\max_j |\\frac{1}{n} X_{\\cdot j}^\\top y|$ and decreases along a specified grid. Warm starts are used, where the solution for one $\\lambda$ initializes the solver for the next smaller $\\lambda$.\n- **Dataset Generation:** $n=60$, $p=8$. SNP features $X_{\\cdot j}$ are drawn from a $\\operatorname{Binomial}(2, q_j)$ distribution with specified minor allele frequencies (MAFs) $(q_j) = (0.05, 0.10, \\dots, 0.35)$ using a fixed random seed. The phenotype is $y = X\\beta^\\star + \\epsilon - \\operatorname{mean}(X\\beta^\\star + \\epsilon)$, with a fixed true coefficient vector $\\beta^\\star$ and a deterministic perturbation $\\epsilon_i = 0.1\\sin(i)$.\n- **Output Definition:** The \"path of coefficient entry\" is the ordered list of feature indices that become nonzero (defined as $|\\beta_j| > 10^{-8}$) for the first time as $\\lambda$ decreases along its grid. Ties at a given $\\lambda$ are broken by ascending feature index.\n- **Test Cases:** Four specific test cases are defined, varying the $\\lambda$ grid and including a case with induced collinearity.\n\n**1.2. Validation Verdict:**\nThe problem is **valid**. It is a clear, self-contained, and scientifically grounded task. It specifies a standard algorithm (coordinate descent for LASSO) applied to a synthetically generated dataset that mimics a real-world application in bioinformatics. All required parameters, constants, and procedures are defined, making the problem reproducible and verifiable. The one minor ambiguity of \"a fixed random seed\" is resolved by selecting a standard conventional value, which aligns with the intent of reproducibility.\n\n### Step 2: Solution Implementation\n\nThe solution proceeds by first implementing the necessary components: data generation, the coordinate descent algorithm, and the logic to trace the coefficient path. These components are then orchestrated to execute the four specified test cases.\n\n**2.1. Data Generation:**\nA function is defined to generate the synthetic dataset according to the problem's specifications. It uses a fixed random seed for reproducibility. It first generates the raw SNP data (allele counts $0, 1, 2$) from binomial distributions. Then, it standardizes this data matrix $X$ by centering each column to have a mean of $0$ and scaling it to have a unit empirical variance (i.e., $\\frac{1}{n}\\sum_i (x_{ij} - \\bar{x}_j)^2 = 1$). The phenotype $y$ is constructed using the standardized $X$, the true coefficient vector $\\beta^\\star$, and the specified deterministic perturbation $\\epsilon$, after which it is centered to have zero mean.\n\n**2.2. Coordinate Descent for LASSO:**\nThe core of the solution is the coordinate descent solver. For a given regularization parameter $\\lambda$, the algorithm iteratively updates each coefficient $\\beta_j$ until convergence. The implementation employs an efficient Gauss-Seidel style update, where the most recent values of other coefficients are used. To achieve this efficiently, we maintain and update the model's residual vector, $r = y - X\\beta$, iteratively.\n\nThe update for a single coefficient $\\beta_j$ proceeds as follows:\n1.  The argument for the soft-thresholding function, $\\rho_j = \\frac{1}{n}X_{\\cdot j}^\\top (y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k)$, is calculated efficiently. Recognizing that $y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k = r + X_{\\cdot j}\\beta_j^{\\text{old}}$, the argument becomes $\\rho_j = \\frac{1}{n}X_{\\cdot j}^\\top r + \\beta_j^{\\text{old}}$ because $\\frac{1}{n}X_{\\cdot j}^\\top X_{\\cdot j} = 1$ for standardized features. Here, $r$ is the residual calculated using the most up-to-date coefficient vector.\n2.  The coefficient is updated: $\\beta_j^{\\text{new}} \\leftarrow S(\\rho_j, \\lambda)$.\n3.  The change in the coefficient, $\\Delta\\beta_j = \\beta_j^{\\text{new}} - \\beta_j^{\\text{old}}$, is used to update the global residual vector efficiently: $r \\leftarrow r - X_{\\cdot j}\\Delta\\beta_j$. This avoids repeated expensive matrix-vector multiplications.\n\nThis cyclic process over all features constitutes one sweep. The algorithm terminates when the maximum change in any coefficient over a full sweep falls below the tolerance $10^{-6}$ or when $1000$ sweeps are completed.\n\n**2.3. Coefficient Path Calculation:**\nTo determine the entry path, we solve the LASSO problem for each $\\lambda$ in a decreasing grid. The solution for a larger $\\lambda$ serves as a \"warm start\" for the next smaller $\\lambda$, which accelerates convergence.\n\nAfter convergence at each $\\lambda$, we identify the set of features with nonzero coefficients (where $|\\beta_j| > 10^{-8}$). By comparing this set to the set of nonzero features from the previous (larger) $\\lambda$, we can identify any newly entered features. These new features, sorted by their index to handle ties, are appended to the overall entry path list.\n\n**2.4. Test Case Execution:**\nThe main program executes the four specified test cases:\n1.  **Happy Path:** A standard geometric grid of $50$ $\\lambda$ values is created for the base dataset ($p=8$).\n2.  **Boundary Case:** The grid consists of a single value, $\\lambda_{\\max}$. As theory predicts, no coefficients should become nonzero.\n3.  **Extreme Small $\\lambda$:** A longer grid of $120$ values reaching a much smaller minimum $\\lambda$ is used, allowing more coefficients to enter the model.\n4.  **Collinearity Case:** The design matrix is augmented by duplicating a feature column, creating perfect collinearity. The augmented matrix is re-standardized, a new $\\lambda_{\\max}$ is computed, and the path is traced on a new grid. This tests the algorithm's behavior and the tie-breaking rule under a known challenging condition.\n\nThe results from these four cases are collected and formatted into the precise string representation required by the problem statement.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the LASSO coefficient entry path for four test cases\n    using coordinate descent on a synthetic SNP dataset.\n    \"\"\"\n\n    # --- Problem Constants and Algorithm Parameters ---\n    N_SAMPLES = 60\n    N_FEATURES_BASE = 8\n    MAF_RATES = np.array([0.05, 0.10, 0.20, 0.15, 0.30, 0.25, 0.40, 0.35])\n    BETA_STAR = np.array([0.0, 1.2, 0.0, -0.8, 0.0, 0.0, 0.5, 0.0])\n    EPSILON = 0.1 * np.sin(np.arange(N_SAMPLES))\n    RANDOM_SEED = 0\n\n    CONV_TOL = 1e-6\n    NON_ZERO_TOL = 1e-8\n    MAX_SWEEPS = 1000\n\n    # --- Helper Functions ---\n\n    def soft_threshold(a, t):\n        \"\"\"Soft-thresholding operator S(a, t).\"\"\"\n        return np.sign(a) * np.maximum(np.abs(a) - t, 0)\n\n    def generate_base_data():\n        \"\"\"\n        Generates and standardizes the base dataset (X, y) for p=8\n        and also returns the raw, unstandardized X matrix.\n        \"\"\"\n        rng = np.random.default_rng(RANDOM_SEED)\n        X_raw = np.zeros((N_SAMPLES, N_FEATURES_BASE))\n        for j in range(N_FEATURES_BASE):\n            X_raw[:, j] = rng.binomial(2, MAF_RATES[j], size=N_SAMPLES)\n\n        # Standardize X: zero mean, unit variance (with 1/n normalization)\n        mean_X = np.mean(X_raw, axis=0)\n        std_X = np.std(X_raw, axis=0)\n        std_X[std_X == 0] = 1.0  # Avoid division by zero\n        X = (X_raw - mean_X) / std_X\n\n        # Generate y and center it\n        y_raw = X @ BETA_STAR + EPSILON\n        y = y_raw - np.mean(y_raw)\n\n        return X, y, X_raw\n\n    def coordinate_descent_lasso(X, y, lambda_val, beta_init):\n        \"\"\"\n        Solves the LASSO objective for a single lambda value.\n        Uses an efficient coordinate descent with Gauss-Seidel updates.\n        \"\"\"\n        n, p = X.shape\n        beta = np.copy(beta_init)\n        \n        for _ in range(MAX_SWEEPS):\n            beta_old_sweep = np.copy(beta)\n            for j in range(p):\n                # Calculate rho_j = (1/n) * X_j^T * (y - sum_{k!=j} X_k * beta_k)\n                # This uses the most recent beta values (Gauss-Seidel style).\n                # The calculation is done efficiently.\n                r_partial = y - (X @ beta - X[:, j] * beta[j])\n                rho_j = (X[:, j] @ r_partial) / n\n                beta[j] = soft_threshold(rho_j, lambda_val)\n            \n            if np.max(np.abs(beta - beta_old_sweep)) < CONV_TOL:\n                break\n        return beta\n\n    def solve_lasso_path(X, y, lambda_grid):\n        \"\"\"\n        Computes the coefficient entry path along a decreasing lambda grid.\n        \"\"\"\n        p = X.shape[1]\n        beta = np.zeros(p)\n        entry_path = []\n        entered_indices = set()\n\n        for lambda_val in lambda_grid:\n            # Use warm starts: previous solution initializes the next run\n            beta = coordinate_descent_lasso(X, y, lambda_val, beta_init=beta)\n\n            # Check for newly entered features\n            current_nonzero_indices = set(np.where(np.abs(beta) > NON_ZERO_TOL)[0])\n            newly_entered = sorted(list(current_nonzero_indices - entered_indices))\n\n            if newly_entered:\n                entry_path.extend(newly_entered)\n                entered_indices.update(newly_entered)\n        \n        return entry_path\n\n    # --- Main Execution Logic for All Test Cases ---\n    \n    results = []\n    \n    # Generate the base dataset once\n    X_base, y, X_raw_base = generate_base_data()\n    n_base, _ = X_base.shape\n\n    # -- Test Case 1: Happy Path --\n    lambda_max_1 = np.max(np.abs(X_base.T @ y / n_base))\n    lambda_grid_1 = np.geomspace(lambda_max_1, 0.01 * lambda_max_1, 50)\n    path_1 = solve_lasso_path(X_base, y, lambda_grid_1)\n    results.append(path_1)\n\n    # -- Test Case 2: Boundary Case --\n    lambda_grid_2 = np.array([lambda_max_1])\n    path_2 = solve_lasso_path(X_base, y, lambda_grid_2)\n    results.append(path_2)\n\n    # -- Test Case 3: Extreme Small Lambda --\n    lambda_grid_3 = np.geomspace(lambda_max_1, 1e-6 * lambda_max_1, 120)\n    path_3 = solve_lasso_path(X_base, y, lambda_grid_3)\n    results.append(path_3)\n    \n    # -- Test Case 4: Collinearity Edge Case --\n    X_aug_raw = np.hstack((X_raw_base, X_raw_base[:, 3:4]))\n    \n    # Re-standardize the augmented matrix\n    mean_aug = np.mean(X_aug_raw, axis=0)\n    std_aug = np.std(X_aug_raw, axis=0)\n    std_aug[std_aug == 0] = 1.0\n    X_aug = (X_aug_raw - mean_aug) / std_aug\n    n_aug, _ = X_aug.shape\n    \n    # Recompute lambda_max and grid for the augmented dataset\n    lambda_max_4 = np.max(np.abs(X_aug.T @ y / n_aug))\n    lambda_grid_4 = np.geomspace(lambda_max_4, 0.01 * lambda_max_4, 50)\n    \n    path_4 = solve_lasso_path(X_aug, y, lambda_grid_4)\n    results.append(path_4)\n\n    # --- Format and Print Final Output ---\n    output_str = \"[\" + \",\".join(f\"[{','.join(map(str, r))}]\" for r in results) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "4605278"}, {"introduction": "In contrast to LASSO's sparsity, ridge regression offers a different approach to controlling model complexity by shrinking coefficients towards zero. This practice focuses on implementing ridge regression using a numerically stable Singular Value Decomposition (SVD) approach, which is crucial when dealing with collinear features common in genomic data. You will analyze how ridge regression distributes coefficients among correlated predictors and quantify the model's complexity using the concept of effective degrees of freedom. [@problem_id:4605283]", "problem": "You are given a linear modeling task motivated by gene expression analysis but framed purely as a numerical linear algebra problem. Consider a linear model with a design matrix $X \\in \\mathbb{R}^{n \\times p}$ and a response vector $y \\in \\mathbb{R}^{n}$. The ridge estimator is defined as the minimizer of the penalized least squares objective that adds an $\\ell_2$ penalty on the coefficients. You must analyze how the ridge estimator redistributes coefficients among collinear predictors and quantify its effective degrees of freedom by the trace of its linear smoother.\n\nYour program must implement the following steps for each test case:\n\n- Preprocessing:\n  - Standardize each column of $X$ to have zero mean and unit variance.\n  - Center $y$ to have zero mean.\n  - Do not add an intercept and do not standardize $y$ beyond centering.\n\n- Ridge estimator:\n  - Compute the ridge coefficient vector $\\hat{\\beta}_{\\lambda} \\in \\mathbb{R}^{p}$ for a specified penalty parameter $\\lambda \\in \\mathbb{R}_{\\ge 0}$ using a numerically stable method that does not explicitly invert matrices. You must use a singular value decomposition-based computation that remains valid when $X$ is rank-deficient.\n\n- Redistribution metric:\n  - For the first two predictors (the first two columns of $X$ after preprocessing), compute the ratio $r = \\hat{\\beta}_{\\lambda,1} / \\hat{\\beta}_{\\lambda,2}$ to quantify how coefficients are redistributed across collinear predictors.\n\n- Effective degrees of freedom:\n  - Compute the effective degrees of freedom as the trace of the linear operator that maps $y$ to the fitted values produced by the ridge estimator. This value must be computed in a numerically stable way that leverages singular value decomposition, without forming or inverting ill-conditioned matrices explicitly.\n\n- Rounding and output:\n  - Round both $r$ and the effective degrees of freedom to $6$ decimal places for reporting.\n\nFundamental base and constraints:\n- You must start from the definition of the ridge estimator as the minimizer of the penalized least squares objective and from the definition of the effective degrees of freedom as the trace of the linear operator that maps $y$ to its fitted values under ridge. You must derive any computational formulas you use from these foundations. Do not assume access to shortcut formulas not derived from these foundations in your implementation.\n- Your algorithm must be robust to multicollinearity, including exact collinearity.\n\nTest suite:\nImplement the above for the following three test cases. For clarity, vectors are column vectors.\n\n- Case A (high correlation, moderate regularization):\n  - Let $x_1 = [-3,-2,-1,0,1,2,3,4,5,6]^{\\top}$.\n  - Let $\\delta = [0.1, -0.2, 0.05, -0.05, 0.02, -0.01, 0.08, -0.03, 0.04, -0.02]^{\\top}$.\n  - Let $x_2 = 0.99\\,x_1 + \\delta$.\n  - Let $x_3 = [2,-1,3,-2,0,1,-3,2,-2,4]^{\\top}$.\n  - Form $X = [x_1, x_2, x_3]$ and $y = 1.5\\,x_1 + 1.5\\,x_2 + 0.2\\,x_3$.\n  - Use $\\lambda = 1.0$.\n\n- Case B (exact collinearity, near-unpenalized limit):\n  - Let $x_1' = [-4,-3,-2,-1,0,1,2,3]^{\\top}$.\n  - Let $x_2' = x_1'$.\n  - Let $x_3' = [1,-2,3,-4,4,-3,2,-1]^{\\top}$.\n  - Form $X' = [x_1', x_2', x_3']$ and $y' = 3\\,x_1' + 0.5\\,x_3'$.\n  - Use $\\lambda = 1\\times 10^{-8}$.\n\n- Case C (high correlation, heavy regularization):\n  - Use the same $X$ and $y$ from Case A.\n  - Use $\\lambda = 1\\times 10^{6}$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of results for the three cases in order A, B, C.\n- Each case result is a list $[r, \\mathrm{df}]$, where $r$ is the ratio and $\\mathrm{df}$ is the effective degrees of freedom, both rounded to $6$ decimal places.\n- The final output must be exactly a single line in the format:\n  - $[[r_A,\\mathrm{df}_A],[r_B,\\mathrm{df}_B],[r_C,\\mathrm{df}_C]]$\n- No additional text should be printed.", "solution": "The problem as stated is a well-posed numerical linear algebra exercise grounded in the principles of regularized linear models. All data, parameters, and procedures are explicitly defined, and the task is scientifically and mathematically sound. No inconsistencies, ambiguities, or factual errors are present. The problem is therefore deemed valid.\n\nThe core of the problem is to compute the ridge regression coefficient vector $\\hat{\\beta}_{\\lambda}$ and the effective degrees of freedom $\\mathrm{df}(\\lambda)$ using a numerically stable method based on Singular Value Decomposition (SVD).\n\n### Preprocessing\n\nLet the design matrix be $X \\in \\mathbb{R}^{n \\times p}$ and the response vector be $y \\in \\mathbb{R}^{n}$.\nFirst, each column $x_j$ of the matrix $X$ is standardized to have a mean of $0$ and a standard deviation of $1$. Let $\\mu_j = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij}$ and $\\sigma_j = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (X_{ij} - \\mu_j)^2}$ be the mean and population standard deviation of the $j$-th column. The standardized matrix, which we will also denote as $X$ for simplicity in the following derivations, has columns $x'_j = (x_j - \\mu_j) / \\sigma_j$. If $\\sigma_j=0$, the standardized column is a vector of zeros.\nThe response vector $y$ is centered to have a mean of $0$. Let $\\mu_y = \\frac{1}{n} \\sum_{i=1}^{n} y_i$. The centered vector, also denoted as $y$, is $y' = y - \\mu_y$.\n\n### Ridge Regression Estimator\n\nThe ridge regression estimator $\\hat{\\beta}_{\\lambda}$ is the vector $\\beta \\in \\mathbb{R}^{p}$ that minimizes the penalized least squares objective function:\n$$\nL(\\beta) = \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2\n$$\nwhere $\\lambda \\ge 0$ is the regularization parameter. To find the minimizer, we compute the gradient of $L(\\beta)$ with respect to $\\beta$ and set it to zero:\n$$\n\\nabla_{\\beta} L(\\beta) = \\nabla_{\\beta} \\left( (y - X\\beta)^T(y - X\\beta) + \\lambda\\beta^T\\beta \\right)\n$$\n$$\n\\nabla_{\\beta} L(\\beta) = \\nabla_{\\beta} \\left( y^Ty - 2y^TX\\beta + \\beta^TX^TX\\beta + \\lambda\\beta^T\\beta \\right)\n$$\n$$\n\\nabla_{\\beta} L(\\beta) = -2X^Ty + 2X^TX\\beta + 2\\lambda\\beta\n$$\nSetting the gradient to zero gives the normal equations for ridge regression:\n$$\n(X^TX + \\lambda I)\\hat{\\beta}_{\\lambda} = X^Ty\n$$\nThis leads to the formal solution $\\hat{\\beta}_{\\lambda} = (X^TX + \\lambda I)^{-1}X^Ty$. However, directly inverting the matrix $(X^TX + \\lambda I)$ can be numerically unstable, especially if $X$ is ill-conditioned (i.e., has highly correlated columns).\n\nA more stable approach utilizes the Singular Value Decomposition (SVD) of $X$. Let the SVD of the $n \\times p$ matrix $X$ be:\n$$\nX = U D V^T\n$$\nwhere $U$ is an $n \\times p$ matrix with orthonormal columns ($U^TU = I_p$), $D$ is a $p \\times p$ diagonal matrix with the singular values $d_1, d_2, \\ldots, d_p$ on its diagonal, and $V$ is a $p \\times p$ orthogonal matrix ($V^TV = VV^T = I_p$). Note that we are using the economy-size SVD as $n \\ge p$ in the test cases.\n\nSubstitute the SVD into the normal equations:\n$$\n\\hat{\\beta}_{\\lambda} = ( (VD^TU^T)(UDV^T) + \\lambda I )^{-1} (VD^TU^T)y\n$$\n$$\n\\hat{\\beta}_{\\lambda} = ( V D^T D V^T + \\lambda V I V^T )^{-1} V D^T U^T y\n$$\n$$\n\\hat{\\beta}_{\\lambda} = ( V(D^2 + \\lambda I)V^T )^{-1} V D^T U^T y\n$$\nUsing the property $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$:\n$$\n\\hat{\\beta}_{\\lambda} = (V^T)^{-1} (D^2 + \\lambda I)^{-1} V^{-1} V D^T U^T y\n$$\nSince $V$ is orthogonal, $(V^T)^{-1} = V$ and $V^{-1}=V^T$.\n$$\n\\hat{\\beta}_{\\lambda} = V (D^2 + \\lambda I)^{-1} D^T U^T y\n$$\nBecause $D$ (and thus $D^T$) is diagonal, the matrix $(D^2 + \\lambda I)^{-1}D^T$ is also diagonal. Its $j$-th diagonal element is $d_j / (d_j^2 + \\lambda)$. This formulation is numerically stable because it avoids matrix inversion and the term $d_j^2+\\lambda$ is strictly positive for $\\lambda>0$. The computation involves stable operations: SVD, matrix-vector products, and element-wise scaling.\n\nThe coefficient ratio is then computed as $r = \\hat{\\beta}_{\\lambda,1} / \\hat{\\beta}_{\\lambda,2}$.\n\n### Effective Degrees of Freedom\n\nThe fitted values from the ridge regression are given by $\\hat{y} = X\\hat{\\beta}_{\\lambda}$. We can express $\\hat{y}$ as a linear transformation of $y$:\n$$\n\\hat{y} = X \\left( (X^TX + \\lambda I)^{-1}X^T y \\right) = S_{\\lambda} y\n$$\nThe matrix $S_{\\lambda} = X(X^TX + \\lambda I)^{-1}X^T$ is known as the smoother matrix or hat matrix for ridge regression. The effective degrees of freedom, $\\mathrm{df}(\\lambda)$, is defined as the trace of this matrix:\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr}(S_{\\lambda}) = \\mathrm{tr}(X(X^TX + \\lambda I)^{-1}X^T)\n$$\nUsing the cyclic property of the trace, $\\mathrm{tr}(ABC) = \\mathrm{tr}(CAB)$, we can write:\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr}((X^TX + \\lambda I)^{-1}X^TX)\n$$\nNow, substitute the SVD of $X$ into this expression:\n$$\nX^TX = (UDV^T)^T(UDV^T) = VD^TU^TUDV^T = VD^2V^T\n$$\n$$\n(X^TX + \\lambda I)^{-1} = (VD^2V^T + \\lambda VIV^T)^{-1} = (V(D^2 + \\lambda I)V^T)^{-1} = V(D^2 + \\lambda I)^{-1}V^T\n$$\nThus,\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr} \\left( V(D^2 + \\lambda I)^{-1}V^T \\cdot VD^2V^T \\right)\n$$\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr} \\left( V(D^2 + \\lambda I)^{-1}D^2V^T \\right)\n$$\nAgain, using the cyclic property of a trace, $\\mathrm{tr}(V A V^T) = \\mathrm{tr}(A V^T V) = \\mathrm{tr}(A)$:\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr} \\left( (D^2 + \\lambda I)^{-1}D^2 \\right)\n$$\nSince $(D^2 + \\lambda I)^{-1}D^2$ is a diagonal matrix with diagonal elements $d_j^2 / (d_j^2 + \\lambda)$, its trace is the sum of these elements:\n$$\n\\mathrm{df}(\\lambda) = \\sum_{j=1}^{p} \\frac{d_j^2}{d_j^2 + \\lambda}\n$$\nThis formula provides a direct, efficient, and numerically stable way to compute the effective degrees of freedom from the singular values of $X$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the specified ridge regression analysis for three test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    x1_a = np.array([-3, -2, -1, 0, 1, 2, 3, 4, 5, 6])\n    delta_a = np.array([0.1, -0.2, 0.05, -0.05, 0.02, -0.01, 0.08, -0.03, 0.04, -0.02])\n    x2_a = 0.99 * x1_a + delta_a\n    x3_a = np.array([2, -1, 3, -2, 0, 1, -3, 2, -2, 4])\n    X_a = np.vstack([x1_a, x2_a, x3_a]).T\n    y_a = 1.5 * x1_a + 1.5 * x2_a + 0.2 * x3_a\n    \n    x1_b = np.array([-4, -3, -2, -1, 0, 1, 2, 3])\n    x2_b = x1_b.copy()\n    x3_b = np.array([1, -2, 3, -4, 4, -3, 2, -1])\n    X_b = np.vstack([x1_b, x2_b, x3_b]).T\n    y_b = 3 * x1_b + 0.5 * x3_b\n\n    test_cases = [\n        {\"X\": X_a, \"y\": y_a, \"lambda\": 1.0},\n        {\"X\": X_b, \"y\": y_b, \"lambda\": 1e-8},\n        {\"X\": X_a, \"y\": y_a, \"lambda\": 1e6},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        X, y, lambda_val = case[\"X\"], case[\"y\"], case[\"lambda\"]\n        n, p = X.shape\n\n        # Preprocessing:\n        # 1. Center y\n        y_centered = y - np.mean(y)\n        \n        # 2. Standardize X\n        X_mean = np.mean(X, axis=0)\n        # Use ddof=0 for population standard deviation, matching the derivation.\n        X_std = np.std(X, axis=0, ddof=0)\n        \n        # Standardize X, handling columns with zero standard deviation\n        X_scaled = np.zeros_like(X, dtype=float)\n        non_zero_std = X_std > 1e-12 # A small tolerance for floating point\n        X_scaled[:, non_zero_std] = (X[:, non_zero_std] - X_mean[non_zero_std]) / X_std[non_zero_std]\n\n        # Singular Value Decomposition of the standardized matrix\n        # Use economy SVD since n >= p\n        U, s, Vt = np.linalg.svd(X_scaled, full_matrices=False)\n        V = Vt.T\n\n        # Compute the ridge coefficient vector beta_hat\n        # beta_hat = V @ diag(s / (s^2 + lambda)) @ U.T @ y_centered\n        # This is implemented efficiently without forming diagonal matrices\n        tmp = U.T @ y_centered\n        d_term = s / (s**2 + lambda_val)\n        beta_hat = V @ (d_term * tmp)\n\n        # Compute redistribution metric r\n        # The problem statement guarantees this won't be a division by zero for the test cases.\n        if abs(beta_hat[1])  1e-12:\n            r = np.inf if beta_hat[0] > 0 else -np.inf if beta_hat[0]  0 else np.nan\n        else:\n            r = beta_hat[0] / beta_hat[1]\n\n        # Compute effective degrees of freedom df\n        # df = sum(s_j^2 / (s_j^2 + lambda))\n        s_squared = s**2\n        df = np.sum(s_squared / (s_squared + lambda_val))\n\n        # Rounding and appending results\n        r_rounded = round(r, 6)\n        df_rounded = round(df, 6)\n        results.append([r_rounded, df_rounded])\n\n    # Format the final output string exactly as required\n    formatted_results = \",\".join([f\"[{res[0]},{res[1]}]\" for res in results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```", "id": "4605283"}]}