{"hands_on_practices": [{"introduction": "To build a strong foundation in precision-recall analysis, we begin with the fundamental building blocks. This first practice [@problem_id:4597653] situates you in a realistic clinical setting—evaluating a novel cancer screening test—where you will compute precision and recall directly from a confusion matrix at a single decision threshold. By translating these metrics into their real-world clinical implications, you will develop a concrete understanding of what precision (Positive Predictive Value) and recall (sensitivity) truly measure and why the $F_1$-score is used to find a balance between them.", "problem": "A clinical genomics laboratory has developed a machine learning classifier that integrates cell-free deoxyribonucleic acid (cfDNA) methylation profiles with clinical covariates to screen for colorectal cancer in asymptomatic adults. In a multi-center prospective study, the classifier was evaluated at a fixed decision threshold on an independent holdout cohort. The study enrolled $N$ participants from primary care clinics with age $50$–$75$ years and no prior cancer diagnosis; colonoscopy was used as the reference standard. The aggregated confusion matrix counts from the holdout cohort at the operating threshold are:\n- True Positive (TP): $180$\n- False Positive (FP): $300$\n- False Negative (FN): $45$\n- True Negative (TN): $4475$\n\nAssume these counts are accurate and that colorectal cancer status is constant during the study window. Using fundamental definitions of event frequencies and conditional probabilities appropriate to binary classification in bioinformatics and medical data analytics, compute the precision–recall metrics at this threshold, and infer the clinical implications of each in this screening context. In your reasoning, explicitly connect precision to the probability of disease among test-positive individuals and recall to the probability that a diseased individual is correctly identified by the test, and comment on how disease prevalence influences precision.\n\nFinally, compute the harmonic mean of precision and recall (the $\\mathrm{F1}$-score) at this operating point and report this single scalar as your final numeric answer. Round your final numeric answer to four significant figures and express it as a dimensionless decimal (no percent sign).", "solution": "The problem is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\nThe problem provides the following data from a confusion matrix for a machine learning classifier used for colorectal cancer screening:\n- True Positive ($TP$): $180$\n- False Positive ($FP$): $300$\n- False Negative ($FN$): $45$\n- True Negative ($TN$): $4475$\n\nThe task is to compute precision, recall, and the F1-score. It also requires an explanation of the clinical implications of precision and recall in this context, including the influence of disease prevalence on precision. The final numeric answer must be the F1-score, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated based on the established criteria.\n- **Scientifically Grounded**: The problem is set within a standard and realistic context of medical diagnostics and machine learning evaluation. The metrics—$TP$, $FP$, $FN$, $TN$, precision, recall, and F1-score—are fundamental to binary classification performance analysis. The scenario of using cfDNA methylation for cancer screening is a current and significant area of research in bioinformatics. The provided counts are internally consistent and plausible for a screening study.\n- **Well-Posed**: The problem statement is self-contained. It provides all necessary data (the confusion matrix counts) to compute the requested metrics. The objective is unambiguous: to calculate and interpret specific performance metrics and report the F1-score. A unique and stable solution exists.\n- **Objective**: The problem is articulated using precise, quantitative, and unbiased language, free of any subjective claims.\n- **Completeness and Consistency**: The data are complete for the task. The sum of individuals with the condition ($TP + FN = 180 + 45 = 225$) and without the condition ($FP + TN = 300 + 4475 = 4775$) equals the sum of individuals who tested positive ($TP + FP = 180 + 300 = 480$) and who tested negative ($FN + TN = 45 + 4475 = 4520$), as $225 + 4775 = 5000$ and $480 + 4520 = 5000$. The data are consistent.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid** as it is scientifically grounded, well-posed, objective, and internally consistent. We may proceed with the solution.\n\n***\n\nThe solution begins by defining the core metrics based on the provided confusion matrix counts. Let $D$ represent the event that an individual has colorectal cancer (the \"positive\" condition) and $\\neg D$ represent the event that they do not. Let $T^+$ be the event that the classifier returns a positive result, and $T^-$ be the event that it returns a negative result. The provided counts are:\n- True Positives ($TP$): Number of individuals with disease who test positive. $TP = 180$.\n- False Positives ($FP$): Number of individuals without disease who test positive. $FP = 300$.\n- False Negatives ($FN$): Number of individuals with disease who test negative. $FN = 45$.\n- True Negatives ($TN$): Number of individuals without disease who test negative. $TN = 4475$.\n\nFrom these counts, we can determine the total number of individuals in several key groups:\n- Total with disease (Condition Positive): $P = TP + FN = 180 + 45 = 225$.\n- Total without disease (Condition Negative): $N_{neg} = FP + TN = 300 + 4475 = 4775$.\n- Total classified as positive (Test Outcome Positive): $T_{pos} = TP + FP = 180 + 300 = 480$.\n- Total classified as negative (Test Outcome Negative): $T_{neg} = FN + TN = 45 + 4475 = 4520$.\n- Total study population: $N_{total} = TP + FP + FN + TN = 180 + 300 + 45 + 4475 = 5000$.\n\nThe problem requires the calculation and interpretation of precision and recall.\n\n**Precision**, also known as the Positive Predictive Value ($PPV$), is the fraction of positive test results that are correct. It is defined as:\n$$\n\\text{Precision} = \\frac{TP}{TP + FP}\n$$\nIn probabilistic terms, precision is the conditional probability of having the disease given a positive test result, $P(D|T^+)$. It answers the question: \"If a patient receives a positive test result, what is the probability that they actually have cancer?\"\nFor this classifier:\n$$\n\\text{Precision} = \\frac{180}{180 + 300} = \\frac{180}{480} = \\frac{3}{8} = 0.375\n$$\nA precision of $0.375$ implies that only $37.5\\%$ of individuals who test positive in this screening setting will actually have colorectal cancer confirmed by colonoscopy. The remaining $62.5\\%$ are false positives. This has significant clinical implications: a low precision leads to a high number of unnecessary, invasive, and costly follow-up procedures (colonoscopies) for healthy individuals, causing patient anxiety and burdening the healthcare system.\n\n**Recall**, also known as Sensitivity or the True Positive Rate ($TPR$), is the fraction of all diseased individuals that are correctly identified by the test. It is defined as:\n$$\n\\text{Recall} = \\frac{TP}{TP + FN}\n$$\nIn probabilistic terms, recall is the conditional probability of testing positive given that one has the disease, $P(T^+|D)$. It answers the question: \"Of all the patients who truly have cancer, what proportion does the test correctly identify?\"\nFor this classifier:\n$$\n\\text{Recall} = \\frac{180}{180 + 45} = \\frac{180}{225} = \\frac{4}{5} = 0.8\n$$\nA recall of $0.8$ means the test successfully identifies $80\\%$ of all individuals with colorectal cancer in the study population. The remaining $20\\%$ are false negatives—diseased individuals who are missed by the test. In a cancer screening context, high recall is of paramount importance. A missed diagnosis (a false negative) can delay treatment and lead to poorer clinical outcomes. Therefore, a recall of $0.8$ indicates that while the test is effective, it still misses one out of every five cancer cases.\n\nThe problem also requires a comment on how disease **prevalence** influences precision. The prevalence of the disease in the study cohort is the proportion of individuals who actually have the disease:\n$$\n\\text{Prevalence} = \\frac{TP + FN}{N_{total}} = \\frac{225}{5000} = 0.045\n$$\nThe relationship between precision, recall, and prevalence is formalized by Bayes' theorem. Precision, $P(D|T^+)$, can be expressed as:\n$$\n\\text{Precision} = P(D|T^+) = \\frac{P(T^+|D) P(D)}{P(T^+)}\n$$\nwhere $P(D)$ is the prevalence, and $P(T^+|D)$ is the recall. The denominator, $P(T^+)$, can be expanded using the law of total probability:\n$$\nP(T^+) = P(T^+|D)P(D) + P(T^+|\\neg D)P(\\neg D)\n$$\nHere, $P(T^+|\\neg D)$ is the False Positive Rate ($FPR = \\frac{FP}{FP+TN}$), and $P(\\neg D) = 1 - \\text{prevalence}$. Substituting this yields:\n$$\n\\text{Precision} = \\frac{\\text{Recall} \\times \\text{Prevalence}}{\\text{Recall} \\times \\text{Prevalence} + \\text{FPR} \\times (1 - \\text{Prevalence})}\n$$\nThis equation demonstrates that precision is highly dependent on prevalence. Even for a test with excellent recall and a low $FPR$, if the prevalence of the disease is low (as is typical for cancer in an asymptomatic screening population), the term $\\text{FPR} \\times (1 - \\text{Prevalence})$ in the denominator can be significant relative to $\\text{Recall} \\times \\text{Prevalence}$, thus lowering the precision. In this case, with a prevalence of $4.5\\%$, the relatively large number of healthy individuals ($4775$) compared to diseased ones ($225$) provides many opportunities for false positives to occur, which in turn reduces the precision.\n\nFinally, we compute the **F1-score**, which is the harmonic mean of precision and recall. It provides a single metric that balances both concerns. It is defined as:\n$$\nF_1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n$$\nAlternatively, it can be computed directly from the confusion matrix counts:\n$$\nF_1 = \\frac{2TP}{2TP + FP + FN}\n$$\nUsing the latter formula for direct computation:\n$$\nF_1 = \\frac{2 \\times 180}{2 \\times 180 + 300 + 45} = \\frac{360}{360 + 345} = \\frac{360}{705}\n$$\nCalculating the decimal value:\n$$\nF_1 = \\frac{360}{705} \\approx 0.51063829...\n$$\nThe problem requires this value to be rounded to four significant figures.\n$$\nF_1 \\approx 0.5106\n$$\nThis F1-score of approximately $0.5106$ reflects the trade-off between the moderate precision ($0.375$) and the high recall ($0.8$) of the classifier at this operating threshold.", "answer": "$$\\boxed{0.5106}$$", "id": "4597653"}, {"introduction": "While the $F_1$-score provides an even balance between precision and recall, not all errors are created equal in bioinformatics. This exercise [@problem_id:4597657] challenges you to derive and apply the generalized $F_{\\beta}$ score, a powerful tool for tailoring your evaluation to specific clinical needs. You will explore how changing the $\\beta$ parameter allows you to place more weight on avoiding either false positives or false negatives, a crucial skill when the consequences of different types of errors are vastly different.", "problem": "A clinical gene variant caller is evaluated on a curated tumor sequencing dataset with pronounced class imbalance typical of somatic mutation detection. Let the caller’s precision $P$ and recall $R$ be defined in terms of the confusion matrix counts as $P = \\mathrm{TP}/(\\mathrm{TP}+\\mathrm{FP})$ and $R = \\mathrm{TP}/(\\mathrm{TP}+\\mathrm{FN})$, where $\\mathrm{TP}$ denotes true positives, $\\mathrm{FP}$ denotes false positives, and $\\mathrm{FN}$ denotes false negatives. An aggregation score $F_{\\beta}(P,R)$ is desired that combines $P$ and $R$ under the following requirements: it is monotone increasing in both $P$ and $R$, equals $0$ if either $P=0$ or $R=0$, equals $1$ if and only if $P=R=1$, is symmetric in $P$ and $R$ when $\\beta=1$, and for general $\\beta>0$ treats recall as $\\beta$ times more important than precision in how it penalizes imbalances between $P$ and $R$. Starting from the base definitions of $P$ and $R$ and the stated requirements on $F_{\\beta}$, derive a closed-form expression for $F_{\\beta}(P,R)$ that is consistent with these properties. Then, for a variant caller with $P=0.98$ and $R=0.85$, compute $F_{0.5}$ and $F_{2}$. Round your numerical results to four significant figures and express them as decimals. Finally, articulate which choice of $\\beta$ emphasizes avoidance of false positives versus avoidance of false negatives in the context of clinical variant calling, and briefly justify your interpretation based on the derived expression and its parameter dependence. The final reported values must be the computed $F_{0.5}$ and $F_{2}$ only.", "solution": "The problem asks for the derivation of the $F_{\\beta}$ score, its computation for specific values, and an interpretation of the parameter $\\beta$.\n\n**1. Derivation of the $F_{\\beta}(P,R)$ Expression**\n\nThe problem provides a set of requirements for an aggregation score $F_{\\beta}(P,R)$ that combines precision ($P$) and recall ($R$). These requirements are characteristic of the standard F-measure, which is a weighted harmonic mean of precision and recall. The harmonic mean is appropriate as it strongly penalizes cases where either metric is low, satisfying the requirement that $F_{\\beta}=0$ if $P=0$ or $R=0$.\n\nThe inverse of a weighted harmonic mean is a weighted arithmetic mean of the inverses of its components. We can thus write $F_{\\beta}^{-1}$ as a linear combination of $P^{-1}$ and $R^{-1}$:\n$$F_{\\beta}^{-1} = \\alpha_P P^{-1} + \\alpha_R R^{-1}$$\nwhere $\\alpha_P$ and $\\alpha_R$ are weights. To satisfy the requirement that $F_{\\beta}=1$ when $P=R=1$, the weights must be normalized such that $\\alpha_P + \\alpha_R = 1$.\n\nThe crucial requirement is that the score \"treats recall as $\\beta$ times more important than precision\". In the context of the F-measure, this is conventionally formalized by setting the ratio of the weights on the inverse of recall to the inverse of precision to be $\\beta^2$. This means:\n$$\\frac{\\alpha_R}{\\alpha_P} = \\beta^2$$\nThe use of $\\beta^2$ instead of $\\beta$ is a standard convention that originates from the theoretical foundations of the F-measure in retrieval effectiveness.\nWe now solve the system of two equations for the weights $\\alpha_P$ and $\\alpha_R$:\n$$ \\begin{cases} \\alpha_P + \\alpha_R = 1 \\\\ \\alpha_R = \\beta^2 \\alpha_P \\end{cases} $$\nSubstituting the second equation into the first gives:\n$$ \\alpha_P + \\beta^2 \\alpha_P = 1 \\implies \\alpha_P(1+\\beta^2) = 1 \\implies \\alpha_P = \\frac{1}{1+\\beta^2} $$\nThen, the weight for recall is:\n$$ \\alpha_R = 1 - \\alpha_P = 1 - \\frac{1}{1+\\beta^2} = \\frac{1+\\beta^2-1}{1+\\beta^2} = \\frac{\\beta^2}{1+\\beta^2} $$\nSubstituting these weights back into the expression for $F_{\\beta}^{-1}$:\n$$F_{\\beta}^{-1} = \\frac{1}{1+\\beta^2} \\frac{1}{P} + \\frac{\\beta^2}{1+\\beta^2} \\frac{1}{R} = \\frac{1}{1+\\beta^2} \\left( \\frac{1}{P} + \\frac{\\beta^2}{R} \\right)$$\nTaking the reciprocal to find $F_{\\beta}$:\n$$F_{\\beta} = (1+\\beta^2) \\left( \\frac{1}{\\frac{1}{P} + \\frac{\\beta^2}{R}} \\right)$$\nTo obtain a more common closed form, we simplify the term in the parenthesis:\n$$\\frac{1}{P} + \\frac{\\beta^2}{R} = \\frac{R + \\beta^2 P}{PR}$$\nSubstituting this back yields the final expression for the $F_{\\beta}$ score:\n$$F_{\\beta}(P,R) = (1+\\beta^2) \\frac{PR}{\\beta^2 P + R}$$\nThis derived formula satisfies all the properties listed in the problem statement.\n\n**2. Computation of $F_{0.5}$ and $F_{2}$**\n\nGiven the values for precision $P=0.98$ and recall $R=0.85$.\n\nFor $\\beta = 0.5$:\n$$F_{0.5} = (1 + (0.5)^2) \\frac{0.98 \\times 0.85}{(0.5)^2 \\times 0.98 + 0.85}$$\n$$F_{0.5} = (1 + 0.25) \\frac{0.833}{0.25 \\times 0.98 + 0.85}$$\n$$F_{0.5} = 1.25 \\times \\frac{0.833}{0.245 + 0.85} = 1.25 \\times \\frac{0.833}{1.095}$$\n$$F_{0.5} = \\frac{1.04125}{1.095} \\approx 0.9509132...$$\nRounding to four significant figures, we get $F_{0.5} = 0.9509$.\n\nFor $\\beta = 2$:\n$$F_2 = (1 + 2^2) \\frac{0.98 \\times 0.85}{2^2 \\times 0.98 + 0.85}$$\n$$F_2 = (1 + 4) \\frac{0.833}{4 \\times 0.98 + 0.85}$$\n$$F_2 = 5 \\times \\frac{0.833}{3.92 + 0.85} = 5 \\times \\frac{0.833}{4.77}$$\n$$F_2 = \\frac{4.165}{4.77} \\approx 0.8731656...$$\nRounding to four significant figures, we get $F_2 = 0.8732$.\n\n**3. Interpretation of $\\beta$**\n\nTo understand how $\\beta$ relates to the avoidance of false positives versus false negatives, we can express $F_{\\beta}$ in terms of the confusion matrix counts: true positives ($\\mathrm{TP}$), false positives ($\\mathrm{FP}$), and false negatives ($\\mathrm{FN}$).\nUsing $P = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}$ and $R = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}$, the $F_{\\beta}$ formula becomes:\n$$F_\\beta = (1+\\beta^2) \\frac{\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}} \\cdot \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}}{\\beta^2 \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}} + \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}} = (1+\\beta^2) \\frac{\\mathrm{TP}}{\\beta^2(\\mathrm{TP}+\\mathrm{FN}) + (\\mathrm{TP}+\\mathrm{FP})} = \\frac{(1+\\beta^2)\\mathrm{TP}}{(1+\\beta^2)\\mathrm{TP} + \\beta^2\\mathrm{FN} + \\mathrm{FP}}$$\nFrom this expression, we observe that the count of false negatives, $\\mathrm{FN}$, is weighted by a factor of $\\beta^2$, while the count of false positives, $\\mathrm{FP}$, is weighted by a factor of $1$.\n\n-   **Choice for avoiding false positives ($\\mathrm{FP}$)**: To emphasize the avoidance of false positives, we must penalize them more heavily. This corresponds to making the weight on $\\mathrm{FP}$ larger than or equal to the weight on $\\mathrm{FN}$. This implies $1 \\ge \\beta^2$, which means $0 < \\beta \\le 1$. Therefore, a choice of $\\beta < 1$ (such as $F_{0.5}$) places more emphasis on precision and, consequently, on the avoidance of false positives.\n\n-   **Choice for avoiding false negatives ($\\mathrm{FN}$)**: To emphasize the avoidance of false negatives, we must penalize them more heavily. This corresponds to making the weight on $\\mathrm{FN}$ larger than the weight on $\\mathrm{FP}$. This implies $\\beta^2 > 1$, which means $\\beta > 1$. Therefore, a choice of $\\beta > 1$ (such as $F_2$) places more emphasis on recall and, consequently, on the avoidance of false negatives.\n\nIn a clinical context, a false negative might mean missing a cancer mutation, leading to a failure to diagnose or treat a patient. A false positive might lead to unnecessary anxiety, biopsies, or toxic treatments. The choice of $\\beta$ reflects the clinical judgment of the relative costs of these two types of errors.", "answer": "$$\\boxed{\\begin{pmatrix} 0.9509 & 0.8732 \\end{pmatrix}}$$", "id": "4597657"}, {"introduction": "Real-world bioinformatics problems rarely involve a single binary choice; more often, we face multi-label scenarios with significant class imbalance. This final practice [@problem_id:4597647] delves into this complexity, tasking you with dissecting the performance of a clinical phenotyping system using micro- and macro-averaged metrics. By working through a carefully constructed example, you will uncover how a seemingly improved model can mask catastrophic failures on rare labels, underscoring the critical importance of choosing the right aggregation strategy for evaluation.", "problem": "A clinical natural language processing system performs multilabel phenotyping on intensive care unit discharge notes to predict the presence of two phenotypes: a common phenotype $A$ (e.g., Type $2$ Diabetes Mellitus) and a rare phenotype $B$ (e.g., a rare metabolic disorder). Evaluation follows standard precision–recall analysis for multilabel classification. Consider a held-out cohort where each phenotype is evaluated across $10$ note–patient pairs, yielding $10$ label–instance pairs per phenotype. The ground truth for phenotype $A$ has $4$ positives ($A_1, A_2, A_3, A_4$) and $6$ negatives ($A_5, \\dots, A_{10}$). The ground truth for phenotype $B$ has $1$ positive ($B_1$) and $9$ negatives ($B_2, \\dots, B_{10}$). Two models $M_0$ and $M_1$ output scores in $[0,1]$ interpreted as confidence. Scores for each phenotype are given below.\n\nFor phenotype $A$:\n- Under $M_0$: $A_1$ (pos) $= 0.90$, $A_2$ (pos) $= 0.70$, $A_3$ (pos) $= 0.60$, $A_4$ (pos) $= 0.40$, $A_5$ (neg) $= 0.95$, $A_6$ (neg) $= 0.80$, $A_7$ (neg) $= 0.50$, $A_8$ (neg) $= 0.45$, $A_9$ (neg) $= 0.30$, $A_{10}$ (neg) $= 0.20$.\n- Under $M_1$: $A_1$ (pos) $= 0.98$, $A_2$ (pos) $= 0.97$, $A_3$ (pos) $= 0.96$, $A_4$ (pos) $= 0.50$, $A_5$ (neg) $= 0.49$, $A_6$ (neg) $= 0.48$, $A_7$ (neg) $= 0.47$, $A_8$ (neg) $= 0.46$, $A_9$ (neg) $= 0.45$, $A_{10}$ (neg) $= 0.44$.\n\nFor phenotype $B$:\n- Under $M_0$: $B_1$ (pos) $= 0.96$, $B_2$ (neg) $= 0.85$, $B_3$ (neg) $= 0.75$, $B_4$ (neg) $= 0.65$, $B_5$ (neg) $= 0.55$, $B_6$ (neg) $= 0.44$, $B_7$ (neg) $= 0.43$, $B_8$ (neg) $= 0.35$, $B_9$ (neg) $= 0.25$, $B_{10}$ (neg) $= 0.15$.\n- Under $M_1$: $B_1$ (pos) $= 0.10$, $B_2$ (neg) $= 0.95$, $B_3$ (neg) $= 0.85$, $B_4$ (neg) $= 0.75$, $B_5$ (neg) $= 0.65$, $B_6$ (neg) $= 0.55$, $B_7$ (neg) $= 0.43$, $B_8$ (neg) $= 0.35$, $B_9$ (neg) $= 0.25$, $B_{10}$ (neg) $= 0.15$.\n\nTasks:\n- Using first principles of precision–recall analysis, compute for each model $M_0$ and $M_1$: the per-phenotype Average Precision (AP) for $A$ and $B$; the Macro-averaged Average Precision (macro AP) across $A$ and $B$; and the Micro-averaged Average Precision (micro AP) obtained by pooling all $20$ label–instance pairs and ranking them by score.\n- Use your computations to judge how micro AP and macro AP change from $M_0$ to $M_1$ and interpret the implications for rare-phenotype performance.\n\nWhich of the following statements are correct?\n\nA. From $M_0$ to $M_1$, the micro-averaged AP increases (from $\\dfrac{181}{315}$ to $\\dfrac{133}{180}$), while the macro-averaged AP decreases (from $\\dfrac{61}{80}$ to $\\dfrac{11}{20}$). This indicates that performance on the rare phenotype $B$ worsened even though the micro average improved.\n\nB. From $M_0$ to $M_1$, both micro and macro AP increase; therefore, rare phenotype performance improved alongside common phenotype performance.\n\nC. Micro-averaged AP is equal to the unweighted mean of per-phenotype APs; therefore, if macro AP decreases, micro AP must also decrease.\n\nD. The common phenotype $A$ improved at the expense of the rare phenotype $B$, illustrating that in imbalanced biomedical phenotyping, reporting only micro-averaged AP can mask harms on rare labels; macro-averaged or per-phenotype AP should also be reported.\n\nE. With only one positive for phenotype $B$, its per-phenotype AP equals its recall and thus cannot deteriorate when the positive is ranked lower.", "solution": "The problem requires a thorough precision-recall analysis of two models, $M_0$ and $M_1$, on a multilabel classification task with a common phenotype $A$ and a rare phenotype $B$. We must calculate per-phenotype Average Precision ($AP$), Macro-averaged $AP$, and Micro-averaged $AP$ to evaluate the provided statements.\n\nFirst, let us define Average Precision ($AP$). For a given class, the instances are ranked by their prediction scores in descending order. The $AP$ is the average of the precision values calculated at the rank of each positive instance. It is formally defined as:\n$$AP = \\frac{\\sum_{k=1}^{N} (P(k) \\times rel(k))}{\\text{Total number of positive instances}}$$\nwhere $N$ is the total number of instances, $k$ is the rank, $P(k)$ is the precision at rank $k$ (i.e., the fraction of positive instances among the top $k$ predictions), and $rel(k)$ is an indicator function that is $1$ if the instance at rank $k$ is a true positive and $0$ otherwise.\n\nThe total number of positive instances for phenotype $A$ is $4$. The total for phenotype $B$ is $1$.\n\n### Analysis of Model $M_0$\n\n**1. Average Precision for Phenotype A ($AP_A(M_0)$)**\n\nFirst, we rank the instances for phenotype $A$ by the scores from $M_0$:\n- Rank 1: $A_5$ (neg) - $0.95$\n- Rank 2: $A_1$ (pos) - $0.90$\n- Rank 3: $A_6$ (neg) - $0.80$\n- Rank 4: $A_2$ (pos) - $0.70$\n- Rank 5: $A_3$ (pos) - $0.60$\n- Rank 6: $A_7$ (neg) - $0.50$\n- Rank 7: $A_8$ (neg) - $0.45$\n- Rank 8: $A_4$ (pos) - $0.40$\n- Rank 9: $A_9$ (neg) - $0.30$\n- Rank 10: $A_{10}$ (neg) - $0.20$\n\nThe precision values at the ranks of the positive instances are:\n- At rank $2$ ($A_1$): $P(2) = 1/2$\n- At rank $4$ ($A_2$): $P(4) = 2/4 = 1/2$\n- At rank $5$ ($A_3$): $P(5) = 3/5$\n- At rank $8$ ($A_4$): $P(8) = 4/8 = 1/2$\n\nThe $AP$ is the average of these precision values:\n$$AP_A(M_0) = \\frac{1}{4} \\left( \\frac{1}{2} + \\frac{2}{4} + \\frac{3}{5} + \\frac{4}{8} \\right) = \\frac{1}{4} \\left( 0.5 + 0.5 + 0.6 + 0.5 \\right) = \\frac{2.1}{4} = 0.525 = \\frac{21}{40}$$\n\n**2. Average Precision for Phenotype B ($AP_B(M_0)$)**\n\nWe rank the instances for phenotype $B$:\n- Rank 1: $B_1$ (pos) - $0.96$\n- ...and so on.\n\nThere is only one positive instance, $B_1$, and it is ranked first. The precision at its rank is:\n- At rank $1$ ($B_1$): $P(1) = 1/1 = 1$\n\nThe $AP$ is:\n$$AP_B(M_0) = \\frac{1}{1} (1) = 1.0$$\n\n**3. Macro-averaged AP for $M_0$**\n\nMacro-averaging is the unweighted mean of the per-phenotype APs.\n$$MacroAP(M_0) = \\frac{AP_A(M_0) + AP_B(M_0)}{2} = \\frac{0.525 + 1.0}{2} = 0.7625 = \\frac{1525}{2000} = \\frac{61}{80}$$\n\n**4. Micro-averaged AP for $M_0$**\n\nFor micro-averaging, we pool all $20$ label-instance pairs and calculate a single $AP$. There are $4+1 = 5$ positive instances in total.\nRanked list of all $20$ instances under $M_0$:\n1. $B_1$ (pos, B) $0.96$\n2. $A_5$ (neg, A) $0.95$\n3. $A_1$ (pos, A) $0.90$\n4. $B_2$ (neg, B) $0.85$\n5. $A_6$ (neg, A) $0.80$\n6. $B_3$ (neg, B) $0.75$\n7. $A_2$ (pos, A) $0.70$\n8. $B_4$ (neg, B) $0.65$\n9. $A_3$ (pos, A) $0.60$\n10. $B_5$ (neg, B) $0.55$\n11. $A_7$ (neg, A) $0.50$\n12. $A_8$ (neg, A) $0.45$\n13. $B_6$ (neg, B) $0.44$\n14. $B_7$ (neg, B) $0.43$\n15. $A_4$ (pos, A) $0.40$\n... (remaining are negative).\n\nThe precisions at the ranks of the five positive instances:\n- At rank $1$ ($B_1$): $P(1) = 1/1$\n- At rank $3$ ($A_1$): $P(3) = 2/3$\n- At rank $7$ ($A_2$): $P(7) = 3/7$\n- At rank $9$ ($A_3$): $P(9) = 4/9$\n- At rank $15$ ($A_4$): $P(15) = 5/15 = 1/3$\n\n$$MicroAP(M_0) = \\frac{1}{5} \\left( \\frac{1}{1} + \\frac{2}{3} + \\frac{3}{7} + \\frac{4}{9} + \\frac{1}{3} \\right) = \\frac{1}{5} \\left( 1 + \\left(\\frac{2}{3}+\\frac{1}{3}\\right) + \\frac{3}{7} + \\frac{4}{9} \\right) = \\frac{1}{5} \\left( 2 + \\frac{27+28}{63} \\right) = \\frac{1}{5} \\left( \\frac{126+55}{63} \\right) = \\frac{181}{315}$$\n\n### Analysis of Model $M_1$\n\n**1. Average Precision for Phenotype A ($AP_A(M_1)$)**\n\nRanked instances for $A$ under $M_1$:\n- Rank 1: $A_1$ (pos) - $0.98$\n- Rank 2: $A_2$ (pos) - $0.97$\n- Rank 3: $A_3$ (pos) - $0.96$\n- Rank 4: $A_4$ (pos) - $0.50$\n- Rank 5: $A_5$ (neg) - $0.49$\n...and so on.\n\nAll four positive instances are ranked at the top.\n- At rank $1$ ($A_1$): $P(1) = 1/1$\n- At rank $2$ ($A_2$): $P(2) = 2/2 = 1$\n- At rank $3$ ($A_3$): $P(3) = 3/3 = 1$\n- At rank $4$ ($A_4$): $P(4) = 4/4 = 1$\n\n$$AP_A(M_1) = \\frac{1}{4} (1+1+1+1) = 1.0$$\n\n**2. Average Precision for Phenotype B ($AP_B(M_1)$)**\n\nRanked instances for $B$ under $M_1$:\n- Rank 1: $B_2$ (neg) - $0.95$\n...\n- Rank 9: $B_{10}$ (neg) - $0.15$\n- Rank 10: $B_1$ (pos) - $0.10$\n\nThe single positive instance is ranked last.\n- At rank $10$ ($B_1$): $P(10) = 1/10$\n\n$$AP_B(M_1) = \\frac{1}{1} \\left( \\frac{1}{10} \\right) = 0.1$$\n\n**3. Macro-averaged AP for $M_1$**\n\n$$MacroAP(M_1) = \\frac{AP_A(M_1) + AP_B(M_1)}{2} = \\frac{1.0 + 0.1}{2} = 0.55 = \\frac{11}{20}$$\n\n**4. Micro-averaged AP for $M_1$**\n\nWe pool all $20$ instances. There are $5$ positive instances. Ranked list:\n1. $A_1$ (pos, A) $0.98$\n2. $A_2$ (pos, A) $0.97$\n3. $A_3$ (pos, A) $0.96$\n4. $B_2$ (neg, B) $0.95$\n5. $B_3$ (neg, B) $0.85$\n6. $B_4$ (neg, B) $0.75$\n7. $B_5$ (neg, B) $0.65$\n8. $B_6$ (neg, B) $0.55$\n9. $A_4$ (pos, A) $0.50$\n...\n20. $B_1$ (pos, B) $0.10$\n\nThe precisions at the ranks of the five positive instances:\n- At rank $1$ ($A_1$): $P(1) = 1/1$\n- At rank $2$ ($A_2$): $P(2) = 2/2 = 1$\n- At rank $3$ ($A_3$): $P(3) = 3/3 = 1$\n- At rank $9$ ($A_4$): $P(9) = 4/9$\n- At rank $20$ ($B_1$): $P(20) = 5/20 = 1/4$\n\n$$MicroAP(M_1) = \\frac{1}{5} \\left( 1 + 1 + 1 + \\frac{4}{9} + \\frac{1}{4} \\right) = \\frac{1}{5} \\left( 3 + \\frac{16+9}{36} \\right) = \\frac{1}{5} \\left( 3 + \\frac{25}{36} \\right) = \\frac{1}{5} \\left( \\frac{108+25}{36} \\right) = \\frac{133}{180}$$\n\n### Summary of Results\n| Metric | Model $M_0$ | Model $M_1$ | Change ($M_0 \\to M_1$) |\n|---|---|---|---|\n| $AP_A$ | $0.525$ | $1.0$ | Increase |\n| $AP_B$ | $1.0$ | $0.1$ | Decrease |\n| Macro AP | $0.7625 = \\frac{61}{80}$ | $0.55 = \\frac{11}{20}$ | Decrease |\n| Micro AP | $\\approx 0.5746 = \\frac{181}{315}$ | $\\approx 0.7389 = \\frac{133}{180}$ | Increase |\n\n### Evaluation of Options\n\n**A. From $M_0$ to $M_1$, the micro-averaged AP increases (from $\\dfrac{181}{315}$ to $\\dfrac{133}{180}$), while the macro-averaged AP decreases (from $\\dfrac{61}{80}$ to $\\dfrac{11}{20}$). This indicates that performance on the rare phenotype $B$ worsened even though the micro average improved.**\n\nOur calculations confirm every numerical value and trend in this statement.\n- Micro AP increases: $\\frac{181}{315} \\approx 0.5746$ increases to $\\frac{133}{180} \\approx 0.7389$.\n- Macro AP decreases: $\\frac{61}{80} = 0.7625$ decreases to $\\frac{11}{20} = 0.55$.\n- The interpretation is also correct: The decrease in macro AP is driven by the collapse in performance for the rare phenotype $B$ (from $AP_B=1.0$ to $AP_B=0.1$). Micro-averaged AP, which is dominated by the more numerous instances of phenotype $A$ (which has $4$ positives vs. $1$ for $B$), improved because the performance on $A$ dramatically improved (from $AP_A=0.525$ to $AP_A=1.0$).\n**Verdict: Correct.**\n\n**B. From $M_0$ to $M_1$, both micro and macro AP increase; therefore, rare phenotype performance improved alongside common phenotype performance.**\n\nThis statement is factually incorrect. Our calculations show that macro AP decreases. Therefore, the premise is false.\n**Verdict: Incorrect.**\n\n**C. Micro-averaged AP is equal to the unweighted mean of per-phenotype APs; therefore, if macro AP decreases, micro AP must also decrease.**\n\nThe first part of the statement is a definition of **macro-averaged AP**, not micro-averaged AP. Micro-averaged AP is calculated by pooling all predictions. The problem's results serve as a direct counterexample: macro AP decreased while micro AP increased. The statement is fundamentally flawed in its definition and its logical deduction.\n**Verdict: Incorrect.**\n\n**D. The common phenotype $A$ improved at the expense of the rare phenotype $B$, illustrating that in imbalanced biomedical phenotyping, reporting only micro-averaged AP can mask harms on rare labels; macro-averaged or per-phenotype AP should also be reported.**\n\nThis statement provides an accurate interpretation of the results.\n- $AP_A$ improved from $0.525$ to $1.0$.\n- $AP_B$ worsened from $1.0$ to $0.1$.\n- The micro-averaged AP improved, which, if viewed in isolation, would falsely suggest that $M_1$ is an unequivocally better model. This masks the severe performance degradation on the rare phenotype $B$. This scenario perfectly illustrates a known pitfall of micro-averaging in imbalanced settings. The recommendation to also report macro-averaged or per-phenotype metrics is standard best practice to avoid this issue.\n**Verdict: Correct.**\n\n**E. With only one positive for phenotype $B$, its per-phenotype AP equals its recall and thus cannot deteriorate when the positive is ranked lower.**\n\nThe premise \"AP equals its recall\" is ill-defined and generally false. For a class with a single positive instance ranked at position $k$, its $AP$ is exactly the precision at that rank, $P(k) = 1/k$. Recall at rank $k$ is $1$ (since the single positive has been found). $AP$ equals recall only if $k=1$. The conclusion that AP \"cannot deteriorate when the positive is ranked lower\" is the opposite of the truth. As the rank $k$ increases (i.e., the positive is ranked lower), the $AP = 1/k$ decreases (deteriorates). Our calculation shows $AP_B$ deteriorated from $1.0$ to $0.1$ as its rank changed from $1$ to $10$.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{AD}$$", "id": "4597647"}]}