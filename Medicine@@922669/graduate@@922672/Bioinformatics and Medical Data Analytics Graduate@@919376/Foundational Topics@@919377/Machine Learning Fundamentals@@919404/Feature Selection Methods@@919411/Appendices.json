{"hands_on_practices": [{"introduction": "At the heart of filter-based feature selection lies the ability to quantify the relationship between an individual feature and the target variable. This foundational exercise grounds this concept in statistical first principles by exploring how common correlation measures are adapted for bioinformatics scenarios, such as comparing a continuous gene expression value with a binary disease status. By deriving and calculating the point-biserial correlation, you will gain a deeper understanding of the mechanics behind many univariate feature ranking methods. [@problem_id:4563593]", "problem": "In high-dimensional gene expression studies for disease prediction, univariate filter methods routinely score features by their association with a binary clinical endpoint. Two widely used association measures are the Pearson linear correlation (specialized to the point-biserial setting when the outcome is binary) and the Spearman rank correlation (which, in the binary-outcome setting, connects to the rank-biserial measure). Starting from first principles, use only the core definitions of covariance, correlation, and rank transformation to write down analytic definitions that specialize these two measures to the case of a continuous feature and a binary outcome. Then compute a sample Pearson correlation for the dataset below.\n\nYou are given a single continuous feature $X$ (e.g., a gene’s normalized expression) and a binary outcome $Y \\in \\{0,1\\}$ for $n$ patients. The sample consists of the following pairs $(x_i,y_i)$ for $i=1,\\dots,n$:\n- Patient $1$: $(x_1,y_1) = (8.1,1)$\n- Patient $2$: $(x_2,y_2) = (6.2,0)$\n- Patient $3$: $(x_3,y_3) = (7.5,1)$\n- Patient $4$: $(x_4,y_4) = (5.9,0)$\n- Patient $5$: $(x_5,y_5) = (9.2,1)$\n- Patient $6$: $(x_6,y_6) = (6.5,0)$\n- Patient $7$: $(x_7,y_7) = (8.0,1)$\n- Patient $8$: $(x_8,y_8) = (5.7,0)$\n- Patient $9$: $(x_9,y_9) = (7.7,1)$\n- Patient $10$: $(x_{10},y_{10}) = (6.0,0)$\n\nTasks:\n- Using only the foundational definitions of covariance and correlation, provide analytic definitions that express the Pearson linear correlation between a continuous $X$ and a binary $Y$ in this setting (that is, the point-biserial interpretation), and the Spearman rank correlation as the Pearson correlation of ranks (that is, the rank-biserial interpretation when $Y$ is binary).\n- Compute the sample Pearson correlation between $X$ and $Y$ for the sample above using the standard sample definitions.\n\nRound your final numeric answer to four significant figures. Do not include any units. Express angles, if any, in radians. The final answer must be a single real number.", "solution": "The problem requires a two-part response: first, to derive analytic definitions for the Pearson and Spearman correlation coefficients when one variable is continuous and the other is binary, starting from foundational principles; second, to compute the sample Pearson correlation for a given dataset.\n\n### Part 1: Analytic Definitions from First Principles\n\nLet $X$ be a continuous feature and $Y$ be a binary outcome variable where $Y \\in \\{0,1\\}$. We have a sample of $n$ pairs $(x_i, y_i)$. Let $n_1$ be the number of observations where $y_i=1$ and $n_0$ be the number of observations where $y_i=0$, such that $n = n_0 + n_1$.\n\n#### Pearson Linear Correlation (Point-Biserial Correlation)\n\nThe sample Pearson correlation coefficient, $r_{xy}$, is defined as the sample covariance of $X$ and $Y$ divided by the product of their sample standard deviations:\n$$ r_{xy} = \\frac{\\text{cov}(x,y)}{s_x s_y} $$\nThe sample covariance is defined as $\\text{cov}(x,y) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})$, and the sample standard deviations are $s_x = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}$ and $s_y = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (y_i - \\bar{y})^2}$.\n\nFirst, we specialize the standard deviation of the binary variable $Y$. The sample mean of $Y$ is $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i = \\frac{n_1}{n}$.\nThe sum of squared deviations for $Y$ is:\n$$ \\sum_{i=1}^n (y_i - \\bar{y})^2 = \\sum_{y_i=1} (1 - \\bar{y})^2 + \\sum_{y_i=0} (0 - \\bar{y})^2 $$\n$$ = n_1 \\left(1 - \\frac{n_1}{n}\\right)^2 + n_0 \\left(0 - \\frac{n_1}{n}\\right)^2 = n_1 \\left(\\frac{n_0}{n}\\right)^2 + n_0 \\left(-\\frac{n_1}{n}\\right)^2 $$\n$$ = \\frac{n_1 n_0^2 + n_0 n_1^2}{n^2} = \\frac{n_0 n_1 (n_0 + n_1)}{n^2} = \\frac{n_0 n_1 n}{n^2} = \\frac{n_0 n_1}{n} $$\nThus, the sample variance of $Y$ is $s_y^2 = \\frac{1}{n-1} \\frac{n_0 n_1}{n}$, and the standard deviation is $s_y = \\sqrt{\\frac{n_0 n_1}{n(n-1)}}$.\n\nNext, we specialize the covariance term. Let $\\bar{x}_1$ be the mean of $X$ for the group where $Y=1$, and $\\bar{x}_0$ be the mean for the group where $Y=0$.\n$$ (n-1)\\text{cov}(x,y) = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{y_i=1} (x_i - \\bar{x})(1 - \\bar{y}) + \\sum_{y_i=0} (x_i - \\bar{x})(0 - \\bar{y}) $$\n$$ = \\left(1 - \\frac{n_1}{n}\\right) \\sum_{y_i=1} (x_i - \\bar{x}) - \\frac{n_1}{n} \\sum_{y_i=0} (x_i - \\bar{x}) $$\n$$ = \\frac{n_0}{n} (n_1 \\bar{x}_1 - n_1 \\bar{x}) - \\frac{n_1}{n} (n_0 \\bar{x}_0 - n_0 \\bar{x}) $$\nUsing $n\\bar{x} = n_1\\bar{x}_1 + n_0\\bar{x}_0$:\n$$ n_1(\\bar{x}_1 - \\bar{x}) = n_1\\bar{x}_1 - n_1\\frac{n_1\\bar{x}_1+n_0\\bar{x}_0}{n} = \\frac{n_1(n-n_1)\\bar{x}_1 - n_1n_0\\bar{x}_0}{n} = \\frac{n_1n_0(\\bar{x}_1-\\bar{x}_0)}{n} $$\n$$ n_0(\\bar{x}_0 - \\bar{x}) = n_0\\bar{x}_0 - n_0\\frac{n_1\\bar{x}_1+n_0\\bar{x}_0}{n} = \\frac{n_0(n-n_0)\\bar{x}_0 - n_0n_1\\bar{x}_1}{n} = \\frac{n_0n_1(\\bar{x}_0-\\bar{x}_1)}{n} $$\nSubstituting back into the covariance expression:\n$$ (n-1)\\text{cov}(x,y) = \\frac{n_0}{n} \\left(\\frac{n_1n_0(\\bar{x}_1-\\bar{x}_0)}{n}\\right) - \\frac{n_1}{n} \\left(\\frac{n_0n_1(\\bar{x}_0-\\bar{x}_1)}{n}\\right) $$\n$$ = \\frac{n_0^2 n_1}{n^2}(\\bar{x}_1-\\bar{x}_0) + \\frac{n_1^2 n_0}{n^2}(\\bar{x}_1-\\bar{x}_0) = \\frac{n_0 n_1(n_0+n_1)}{n^2}(\\bar{x}_1-\\bar{x}_0) = \\frac{n_0 n_1}{n}(\\bar{x}_1-\\bar{x}_0) $$\nSo, $\\text{cov}(x,y) = \\frac{n_0 n_1}{n(n-1)}(\\bar{x}_1 - \\bar{x}_0)$.\n\nCombining these results gives the point-biserial correlation coefficient, $r_{\\text{pb}}$:\n$$ r_{\\text{pb}} = \\frac{\\text{cov}(x,y)}{s_x s_y} = \\frac{\\frac{n_0 n_1}{n(n-1)}(\\bar{x}_1 - \\bar{x}_0)}{s_x \\sqrt{\\frac{n_0 n_1}{n(n-1)}}} = \\frac{\\bar{x}_1 - \\bar{x}_0}{s_x} \\sqrt{\\frac{n_0 n_1}{n(n-1)}} $$\nThis expression defines the Pearson correlation for a continuous feature and a binary outcome in terms of the means of the two groups and the overall standard deviation of the feature.\n\n#### Spearman Rank Correlation (Rank-Biserial Correlation)\n\nThe Spearman rank correlation, $r_s$, is defined as the Pearson correlation coefficient calculated on the ranks of the variables. Let $u_i = \\text{rank}(x_i)$ and $v_i = \\text{rank}(y_i)$.\n$$ r_s(x,y) = r(u,v) = \\frac{\\text{cov}(u,v)}{s_u s_v} $$\nFor the binary variable $Y$, the $n_0$ values of $0$ will occupy the first $n_0$ ranks, and the $n_1$ values of $1$ will occupy the remaining $n_1$ ranks. Due to ties, all $y_i=0$ are assigned the average of the first $n_0$ ranks, which is $v_0 = \\frac{1+2+...+n_0}{n_0} = \\frac{n_0+1}{2}$. All $y_i=1$ are assigned the average of the subsequent $n_1$ ranks, which is $v_1 = \\frac{(n_0+1)+...+(n_0+n_1)}{n_1} = n_0 + \\frac{n_1+1}{2}$.\nThe rank variable $v$ is thus also a binary variable, taking values $v_0$ and $v_1$. Crucially, $v$ is a simple linear transformation of $y$: $v_i = (v_1-v_0)y_i + v_0$. Let $a = v_1-v_0$ and $b=v_0$.\nThe Pearson correlation is invariant under positive linear transformations. Since $v_1 > v_0$, the transformation factor $a$ is positive. Therefore:\n$$ r(u,v) = r(u, ay+b) = \\frac{\\text{cov}(u, ay+b)}{s_u s_{ay+b}} = \\frac{a \\cdot \\text{cov}(u,y)}{s_u \\cdot |a| \\cdot s_y} = \\frac{a}{a} \\cdot r(u,y) = r(u,y) $$\nThis demonstrates that the Spearman correlation between $X$ and binary $Y$ is equivalent to the Pearson correlation between the ranks of $X$ and the original binary variable $Y$. This specific measure is known as the rank-biserial correlation, $r_{\\text{rb}}$.\n\nWe can use the point-biserial formula derived above, simply replacing the continuous variable $X$ with its rank-transformed version $U$:\n$$ r_{\\text{rb}} = r_s(x,y) = \\frac{\\bar{u}_1 - \\bar{u}_0}{s_u} \\sqrt{\\frac{n_0 n_1}{n(n-1)}} $$\nHere, $\\bar{u}_1$ is the mean rank of $X$ for the group $Y=1$, $\\bar{u}_0$ is the mean rank of $X$ for the group $Y=0$, and $s_u$ is the standard deviation of all ranks of $X$.\n\n### Part 2: Computation of the Sample Pearson Correlation\n\nThe sample consists of $n=10$ pairs:\n$x = (8.1, 6.2, 7.5, 5.9, 9.2, 6.5, 8.0, 5.7, 7.7, 6.0)$\n$y = (1, 0, 1, 0, 1, 0, 1, 0, 1, 0)$\n\nWe will use the computational formula for the sample Pearson correlation coefficient $r_{xy}$:\n$$ r_{xy} = \\frac{n \\sum_{i=1}^n x_i y_i - (\\sum_{i=1}^n x_i)(\\sum_{i=1}^n y_i)}{\\sqrt{\\left[n \\sum_{i=1}^n x_i^2 - (\\sum_{i=1}^n x_i)^2\\right]\\left[n \\sum_{i=1}^n y_i^2 - (\\sum_{i=1}^n y_i)^2\\right]}} $$\n\nFirst, we compute the necessary sums:\n1.  $\\sum_{i=1}^{10} x_i = 8.1+6.2+7.5+5.9+9.2+6.5+8.0+5.7+7.7+6.0 = 70.8$\n2.  $\\sum_{i=1}^{10} y_i = 1+0+1+0+1+0+1+0+1+0 = 5$\n3.  $\\sum_{i=1}^{10} x_i y_i = (8.1 \\times 1) + (6.2 \\times 0) + (7.5 \\times 1) + (5.9 \\times 0) + (9.2 \\times 1) + (6.5 \\times 0) + (8.0 \\times 1) + (5.7 \\times 0) + (7.7 \\times 1) + (6.0 \\times 0) = 8.1+7.5+9.2+8.0+7.7 = 40.5$\n4.  $\\sum_{i=1}^{10} x_i^2 = 8.1^2+6.2^2+7.5^2+5.9^2+9.2^2+6.5^2+8.0^2+5.7^2+7.7^2+6.0^2 = 65.61+38.44+56.25+34.81+84.64+42.25+64.00+32.49+59.29+36.00 = 513.78$\n5.  Since $y_i \\in \\{0,1\\}$, $y_i^2 = y_i$. Therefore, $\\sum_{i=1}^{10} y_i^2 = \\sum_{i=1}^{10} y_i = 5$.\n\nNow, we substitute these sums into the formula.\nNumerator:\n$$ n \\sum x_i y_i - (\\sum x_i)(\\sum y_i) = 10 \\times 40.5 - (70.8)(5) = 405 - 354 = 51 $$\nDenominator, part 1 (for $X$):\n$$ n \\sum x_i^2 - (\\sum x_i)^2 = 10 \\times 513.78 - (70.8)^2 = 5137.8 - 5012.64 = 125.16 $$\nDenominator, part 2 (for $Y$):\n$$ n \\sum y_i^2 - (\\sum y_i)^2 = 10 \\times 5 - 5^2 = 50 - 25 = 25 $$\nCombining these parts for the final calculation:\n$$ r_{xy} = \\frac{51}{\\sqrt{125.16 \\times 25}} = \\frac{51}{\\sqrt{3129}} \\approx 0.9117329... $$\n\nRounding the result to four significant figures, we get $0.9117$.", "answer": "$$\\boxed{0.9117}$$", "id": "4563593"}, {"introduction": "While scoring a single feature is straightforward, genomic studies present a challenge of immense scale, often involving tens of thousands of features tested simultaneously. This massive multiplicity inflates the risk of false positives, making naive significance thresholds unusable. This practice delves into the critical task of multiple testing correction, asking you to derive and compare the Bonferroni method for controlling the family-wise error rate (FWER) with the Benjamini-Hochberg procedure for controlling the false discovery rate (FDR), a crucial trade-off in any large-scale feature selection pipeline. [@problem_id:4563535]", "problem": "A research team is performing a filter-based feature selection stage in a genomics pipeline prior to training a disease classifier. They test association for $m = 20{,}000$ transcript features against a binary disease phenotype, obtaining one independent $p$-value per feature. They wish to set a fixed $p$-value cutoff under two distinct error-control regimes at level $\\alpha = 0.05$:\n\n1. Family-Wise Error Rate (FWER), defined as the probability of making at least one false positive among all $m$ tests.\n\n2. False Discovery Rate (FDR), defined as the expected proportion of false positives among all rejected hypotheses.\n\nFrom first principles, using only the union bound for controlling FWER and the definition of FDR together with the known null $p$-value property that under true null hypotheses $p$-values are stochastically no smaller than a uniform distribution on $[0,1]$, derive:\n\n- The Bonferroni per-feature significance threshold that controls FWER at level $\\alpha$.\n\n- The Benjamini–Hochberg (BH) step-up critical value function $t(k)$, where $k \\in \\{1,2,\\ldots,m\\}$ indexes the rank of the sorted $p$-values in ascending order, that controls FDR at level $\\alpha$ under independence of $p$-values.\n\nCompute these quantities for $m = 20{,}000$ and $\\alpha = 0.05$. Express your final answer as a single row matrix whose first entry is the Bonferroni per-feature threshold and whose second entry is the closed-form expression for the BH critical value function $t(k)$ as a function of the rank index $k$. No rounding is required.\n\nFinally, justify in words the trade-off between controlling FWER and controlling FDR in the context of large-scale omics feature selection, and why a practitioner might prefer one over the other when the downstream objective is predictive performance rather than confirmatory inference. Your justification must proceed from the definitions above and the behavior of the corresponding thresholds without invoking unproven heuristics.", "solution": "The problem has been validated as scientifically grounded, well-posed, and objective. It is based on established principles of multiple hypothesis testing in statistics and bioinformatics. All necessary definitions and data are provided, and the problem is free of contradictions or ambiguities.\n\nLet $m = 20{,}000$ be the total number of features, which corresponds to $m$ independent hypothesis tests. Let $\\alpha = 0.05$ be the desired significance level for error control. For each hypothesis $H_i$, $i \\in \\{1, 2, \\ldots, m\\}$, we have a corresponding $p$-value, $p_i$. A hypothesis $H_i$ is rejected if its $p$-value $p_i$ is less than or equal to some threshold.\n\n### 1. Family-Wise Error Rate (FWER) and the Bonferroni Correction\n\nThe Family-Wise Error Rate (FWER) is defined as the probability of making at least one Type I error (a false positive) among all $m$ tests. Let $I_0$ be the set of indices corresponding to true null hypotheses, and let $m_0 = |I_0|$ be the number of true nulls. A Type I error for hypothesis $H_i$ occurs if $H_i$ is true ($i \\in I_0$) but is rejected. Let this event be $E_i$.\n\nThe FWER is then $P(\\cup_{i \\in I_0} E_i)$. The problem requires using the union bound (also known as Boole's inequality) to control the FWER. The union bound states that for any collection of events, the probability of their union is no greater than the sum of their individual probabilities.\n$$\n\\text{FWER} = P(\\cup_{i \\in I_0} E_i) \\le \\sum_{i \\in I_0} P(E_i)\n$$\nWe employ a single, fixed $p$-value cutoff, let's call it $t_{bonf}$. A hypothesis $H_i$ is rejected if $p_i \\le t_{bonf}$. The event $E_i$ is thus $\\{p_i \\le t_{bonf}\\}$. Under a true null hypothesis, the $p$-value $p_i$ is stochastically no smaller than a uniform random variable on $[0,1]$. For a continuous test statistic, which is standard, $p_i \\sim U(0,1)$. Therefore, for any $i \\in I_0$, the probability of a Type I error is $P(p_i \\le t_{bonf}) = t_{bonf}$.\n\nSubstituting this into the union bound inequality:\n$$\n\\text{FWER} \\le \\sum_{i \\in I_0} t_{bonf} = m_0 t_{bonf}\n$$\nSince the number of true nulls, $m_0$, is unknown, we use the most conservative case where all null hypotheses could be true, i.e., $m_0 \\le m$. This gives:\n$$\n\\text{FWER} \\le m_0 t_{bonf} \\le m t_{bonf}\n$$\nTo control the FWER at level $\\alpha$, we enforce the condition that this upper bound is no greater than $\\alpha$:\n$$\nm t_{bonf} \\le \\alpha\n$$\nSolving for the per-feature significance threshold $t_{bonf}$ gives the Bonferroni correction:\n$$\nt_{bonf} = \\frac{\\alpha}{m}\n$$\nFor the given values $m = 20{,}000$ and $\\alpha = 0.05$:\n$$\nt_{bonf} = \\frac{0.05}{20{,}000} = \\frac{5 \\times 10^{-2}}{2 \\times 10^4} = 2.5 \\times 10^{-6}\n$$\n\n### 2. False Discovery Rate (FDR) and the Benjamini-Hochberg (BH) Procedure\n\nThe False Discovery Rate (FDR) is defined as the expected proportion of false positives among all rejected hypotheses (discoveries). Let $R$ be the total number of rejected hypotheses and $V$ be the number of false positives (true null hypotheses that were rejected). The FDR is $E\\left[\\frac{V}{R}\\right]$ (with the fraction taken as $0$ if $R=0$).\n\nThe Benjamini-Hochberg (BH) procedure provides a less stringent control than FWER, which is often more powerful for exploratory analyses. We are asked to derive its critical value function, $t(k)$, from first principles.\n\nThe core idea is to find an adaptive threshold. Let us seek a threshold $t$ such that our estimate of the FDR is controlled at level $\\alpha$. For a given threshold $t$, the number of discoveries is $R(t) = \\sum_{i=1}^m \\mathbb{I}(p_i \\le t)$, where $\\mathbb{I}(\\cdot)$ is the indicator function. The expected number of false positives is $E[V(t)] = E[\\sum_{i \\in I_0} \\mathbb{I}(p_i \\le t)] = \\sum_{i \\in I_0} P(p_i \\le t) = m_0 t$.\n\nA simple, though not formally rigorous, plug-in estimator for the FDR for a given threshold $t$ is $\\widehat{\\text{FDR}}(t) = \\frac{E[V(t)]}{R(t)} = \\frac{m_0 t}{R(t)}$. Since $m_0$ is unknown, we can use the conservative upper bound $m_0 \\le m$, which yields the condition:\n$$\n\\frac{m t}{R(t)} \\le \\alpha \\implies t \\le \\frac{R(t)}{m} \\alpha\n$$\nThis inequality must hold for our rejection threshold $t$. However, the threshold $t$ itself determines the number of rejections $R(t)$. This circularity suggests a search procedure. The BH procedure provides a systematic way to perform this search.\n\nLet's order the $p$-values in ascending order: $p_{(1)} \\le p_{(2)} \\le \\ldots \\le p_{(m)}$.\nSuppose we consider the $k$-th ordered $p$-value, $p_{(k)}$, as our potential threshold. If we were to set our threshold $t = p_{(k)}$, then we would make exactly $k$ rejections. So, $R(t) = R(p_{(k)}) = k$.\nSubstituting $t=p_{(k)}$ and $R(t)=k$ into the condition above gives:\n$$\np_{(k)} \\le \\frac{k}{m} \\alpha\n$$\nThis gives us a condition for each ranked $p$-value. The BH procedure checks this condition for all $k \\in \\{1, 2, \\ldots, m\\}$. To maximize the number of discoveries (i.e., to have the highest power), we find the largest $k$ for which this condition holds. Let this be $k_{max} = \\max\\{k : p_{(k)} \\le \\frac{k\\alpha}{m}\\}$. Then, all hypotheses with $p$-values up to $p_{(k_{max})}$ are rejected.\n\nFrom this derivation, the critical value against which the $k$-th ranked $p$-value, $p_{(k)}$, is compared is:\n$$\nt(k) = \\frac{k \\alpha}{m}\n$$\nThis is the Benjamini-Hochberg step-up critical value function. For the given values, this becomes:\n$$\nt(k) = \\frac{k \\times 0.05}{20{,}000} = (2.5 \\times 10^{-6}) k\n$$\n\n### Justification of Trade-off for Predictive Performance\n\nThe choice between controlling FWER and FDR depends critically on the scientific goal. FWER is suited for confirmatory research where the cost of a single false claim is high. FDR is suited for exploratory research, such as feature selection for prediction, where the goal is to generate a promising set of candidates for a downstream task.\n\nThe Bonferroni threshold for FWER control is $t_{bonf} = \\alpha/m = 2.5 \\times 10^{-6}$. This is a single, extremely stringent threshold that does not adapt to the data. In a typical large-scale omics study where true effects may be modest, this high bar for significance often results in very few or no features being selected. This corresponds to a high rate of false negatives (truly predictive features being missed), which is detrimental to building a powerful predictive model. A model built on an impoverished feature set is likely to underfit and have poor predictive accuracy.\n\nThe BH critical values for FDR control are $t(k) = k\\alpha/m$. These values form an increasing ramp: $t(1) = \\alpha/m$, $t(2) = 2\\alpha/m$, and so on. Any selected feature under Bonferroni (where $p_{(1)} \\le \\alpha/m$) will also be selected by BH. However, BH allows for the selection of many more features, as the threshold becomes more lenient for less significant $p$-values. For example, the $100$-th ranked feature is tested against a threshold of $100\\alpha/m$, which is $100$ times larger than the Bonferroni threshold.\n\nThis more liberal approach of FDR control allows for a greater number of true positives to be included in the selected feature set. While this also increases the number of false positives, the FDR framework guarantees that the *expected proportion* of these \"noise\" features among all selected features is controlled at level $\\alpha$. For prediction, this is a highly desirable trade-off. Many modern machine learning algorithms (e.g., L1-regularized regression, random forests) are robust to the inclusion of a limited number of non-informative features; they have internal mechanisms to assign low importance or zero weight to them. The damage caused by including a few spurious features is often far less than the damage caused by excluding numerous truly predictive features.\n\nTherefore, when the objective is predictive performance rather than confirmatory inference, a practitioner would prefer FDR control over FWER control. FDR prioritizes higher power (sensitivity) to discover potentially predictive features, accepting a controlled proportion of false discoveries, which is a trade-off that aligns well with the goal of building robust and accurate predictive models. FWER's extreme conservatism (prioritizing specificity) is counterproductive in this context.", "answer": "$$\n\\boxed{\\begin{pmatrix} 2.5 \\times 10^{-6} & 2.5 \\times 10^{-6} k \\end{pmatrix}}\n$$", "id": "4563535"}, {"introduction": "After mastering the application of univariate filters, it is crucial to recognize their fundamental limitations. These methods assess each feature in isolation, rendering them blind to relationships that only emerge when features are considered together. This exercise uses the classic Exclusive OR (XOR) problem to demonstrate a scenario where features are collectively essential for prediction but exhibit zero marginal association with the outcome, a blind spot for all univariate techniques. Completing this practice will solidify your understanding of why more complex, multivariate feature selection strategies are often necessary. [@problem_id:4563606]", "problem": "You are analyzing a synthetic cohort in a genome-wide association study, where two binary Single Nucleotide Polymorphism (SNP) features $X_{1}$ and $X_{2}$ are coded as $X_{1} \\in \\{0,1\\}$ and $X_{2} \\in \\{0,1\\}$. The binary phenotype $Y \\in \\{0,1\\}$ is generated deterministically by the Exclusive OR (XOR) rule, $Y = X_{1} \\oplus X_{2}$, where $\\oplus$ denotes Exclusive OR (XOR). Assume $X_{1}$ and $X_{2}$ are independent and identically distributed Bernoulli random variables with parameter $1/2$, and that the synthetic cohort consists of $N = 4n$ individuals for some integer $n \\geq 1$, with each of the four $(X_{1}, X_{2})$ combinations occurring exactly $n$ times. In filter-based feature selection, features are ranked by univariate association with the target (for example, by Mutual Information (MI) or correlation), and selected accordingly.\n\nStarting from the foundational definitions of independence in probability theory and the definition of Mutual Information (MI),\n$$\nI(X;Y) = \\sum_{x} \\sum_{y} p(x,y) \\ln\\!\\left(\\frac{p(x,y)}{p(x)\\,p(y)}\\right),\n$$\nderive whether the marginal relationship between $X_{1}$ and $Y$ is informative for feature selection under the described data-generating mechanism. Then, using the specified synthetic cohort, compute the exact value of the Mutual Information $I(X_{1}; Y)$ implied by the cohort construction.\n\nExpress your final answer as the exact value. No rounding is required. No physical units are involved in this computation.", "solution": "The user has provided a problem to be solved. As per the instructions, the first step is to validate the problem statement.\n\n### Step 1: Extract Givens\n- **Features:** Two binary Single Nucleotide Polymorphism (SNP) features, coded as $X_{1} \\in \\{0,1\\}$ and $X_{2} \\in \\{0,1\\}$.\n- **Phenotype:** A binary phenotype, coded as $Y \\in \\{0,1\\}$.\n- **Data-Generating Rule:** $Y$ is generated deterministically by the Exclusive OR (XOR) rule, $Y = X_{1} \\oplus X_{2}$.\n- **Feature Distribution (Probabilistic):** $X_{1}$ and $X_{2}$ are independent and identically distributed (i.i.d.) Bernoulli random variables with parameter $1/2$.\n- **Cohort Construction (Deterministic):** The synthetic cohort consists of $N = 4n$ individuals for some integer $n \\geq 1$. Each of the four $(X_{1}, X_{2})$ combinations—$(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$—occurs exactly $n$ times.\n- **Methodology:** Analysis is in the context of filter-based feature selection using univariate association.\n- **Task 1:** Starting from the foundational definitions of independence and Mutual Information, derive whether the marginal relationship between $X_{1}$ and $Y$ is informative.\n- **Task 2:** Compute the exact value of the Mutual Information $I(X_{1}; Y)$ implied by the cohort construction.\n- **Formula for Mutual Information (MI):** $I(X;Y) = \\sum_{x} \\sum_{y} p(x,y) \\ln\\!\\left(\\frac{p(x,y)}{p(x)\\,p(y)}\\right)$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is firmly rooted in information theory, probability theory, and machine learning. The XOR problem is a classic example used to illustrate the limitations of linear models and univariate feature selection methods. The concepts of Bernoulli variables, Mutual Information, and filter methods are standard in bioinformatics and data analytics. The problem setup is scientifically sound.\n- **Well-Posed:** All necessary information is provided. The probabilistic description of the features and the deterministic construction of the cohort are consistent with each other, as the cohort perfectly represents the underlying probability distribution. The task is clearly defined, and a unique, meaningful solution exists.\n- **Objective:** The problem is stated in precise, mathematical language. It is free of any subjectivity or ambiguity.\n- **Flaws Checklist:**\n    1.  **Scientific/Factual Unsoundness:** None. The premises are logically consistent and grounded in established theory.\n    2.  **Non-Formalizable/Irrelevant:** None. The problem is directly relevant to feature selection methods in bioinformatics.\n    3.  **Incomplete/Contradictory Setup:** None. The two descriptions of the data (probabilistic and cohort-based) are internally consistent and complete.\n    4.  **Unrealistic/Infeasible:** None. It describes a synthetic dataset, which is a common and valid tool for theoretical analysis.\n    5.  **Ill-Posed/Poorly Structured:** None. The question is specific and leads to a unique answer.\n    6.  **Pseudo-Profound/Trivial:** None. The problem addresses a conceptually important limitation of a common class of algorithms.\n    7.  **Outside Scientific Verifiability:** None. The derivation and calculation are mathematically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. The solution process will now proceed.\n\n### Solution Derivation\n\nThe problem asks for two related results: first, to determine if the marginal relationship between $X_{1}$ and $Y$ is informative, and second, to compute the Mutual Information $I(X_{1}; Y)$.\n\n**Part 1: Informativeness of the Marginal Relationship**\n\nIn the context of feature selection, a marginal relationship is \"informative\" if the feature provides information about the target variable when considered alone. Statistically, this means the feature and the target variable are not independent. If they are statistically independent, knowing the value of the feature provides no information about the value of the target. We will test for the independence of $X_{1}$ and $Y$.\n\nTwo random variables are independent if and only if their joint probability distribution is the product of their marginal probability distributions, i.e., $p(x_{1}, y) = p(x_{1})p(y)$ for all possible values of $x_{1}$ and $y$. We will derive these distributions from the given data-generating process.\n\nFirst, we determine the marginal distributions for $X_{1}$ and $Y$.\nThe problem states that $X_{1}$ is a Bernoulli random variable with parameter $1/2$. Thus, its probability distribution is:\n$$ p(X_{1}=0) = \\frac{1}{2} $$\n$$ p(X_{1}=1) = \\frac{1}{2} $$\n\nNext, we find the distribution of $Y = X_{1} \\oplus X_{2}$. The variable $Y$ takes the value $0$ if $X_{1} = X_{2}$ and the value $1$ if $X_{1} \\neq X_{2}$.\nThe value $Y=0$ occurs for two mutually exclusive events: $(X_{1}=0, X_{2}=0)$ or $(X_{1}=1, X_{2}=1)$. Since $X_{1}$ and $X_{2}$ are independent, the probability is:\n$$ p(Y=0) = p(X_{1}=0, X_{2}=0) + p(X_{1}=1, X_{2}=1) $$\n$$ p(Y=0) = p(X_{1}=0)p(X_{2}=0) + p(X_{1}=1)p(X_{2}=1) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} $$\nThe value $Y=1$ occurs for two mutually exclusive events: $(X_{1}=0, X_{2}=1)$ or $(X_{1}=1, X_{2}=0)$. The probability is:\n$$ p(Y=1) = p(X_{1}=0, X_{2}=1) + p(X_{1}=1, X_{2}=0) $$\n$$ p(Y=1) = p(X_{1}=0)p(X_{2}=1) + p(X_{1}=1)p(X_{2}=0) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} $$\nSo, $Y$ is also a Bernoulli random variable with parameter $1/2$.\n\nNow, we determine the joint distribution $p(X_{1}, Y)$.\n-   $p(X_{1}=0, Y=0)$: This requires $X_{1}=0$ and $Y=0$. Since $Y=X_{1} \\oplus X_{2}$, we have $0 = 0 \\oplus X_{2}$, which implies $X_{2}=0$. Thus, this event corresponds to $(X_{1}=0, X_{2}=0)$.\n    $$ p(X_{1}=0, Y=0) = p(X_{1}=0, X_{2}=0) = p(X_{1}=0)p(X_{2}=0) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} $$\n-   $p(X_{1}=0, Y=1)$: This requires $X_{1}=0$ and $Y=1$. We have $1 = 0 \\oplus X_{2}$, which implies $X_{2}=1$. This event is $(X_{1}=0, X_{2}=1)$.\n    $$ p(X_{1}=0, Y=1) = p(X_{1}=0, X_{2}=1) = p(X_{1}=0)p(X_{2}=1) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} $$\n-   $p(X_{1}=1, Y=0)$: This requires $X_{1}=1$ and $Y=0$. We have $0 = 1 \\oplus X_{2}$, which implies $X_{2}=1$. This event is $(X_{1}=1, X_{2}=1)$.\n    $$ p(X_{1}=1, Y=0) = p(X_{1}=1, X_{2}=1) = p(X_{1}=1)p(X_{2}=1) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} $$\n-   $p(X_{1}=1, Y=1)$: This requires $X_{1}=1$ and $Y=1$. We have $1 = 1 \\oplus X_{2}$, which implies $X_{2}=0$. This event is $(X_{1}=1, X_{2}=0)$.\n    $$ p(X_{1}=1, Y=1) = p(X_{1}=1, X_{2}=0) = p(X_{1}=1)p(X_{2}=0) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} $$\n\nFinally, we test the independence condition $p(x_{1}, y) = p(x_{1})p(y)$:\n-   For $(x_{1}=0, y=0)$: $p(X_{1}=0, Y=0) = \\frac{1}{4}$. The product of marginals is $p(X_{1}=0)p(Y=0) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$. The condition holds.\n-   For $(x_{1}=0, y=1)$: $p(X_{1}=0, Y=1) = \\frac{1}{4}$. The product of marginals is $p(X_{1}=0)p(Y=1) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$. The condition holds.\n-   For $(x_{1}=1, y=0)$: $p(X_{1}=1, Y=0) = \\frac{1}{4}$. The product of marginals is $p(X_{1}=1)p(Y=0) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$. The condition holds.\n-   For $(x_{1}=1, y=1)$: $p(X_{1}=1, Y=1) = \\frac{1}{4}$. The product of marginals is $p(X_{1}=1)p(Y=1) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$. The condition holds.\n\nSince the independence condition holds for all possible outcomes, $X_{1}$ and $Y$ are statistically independent. Therefore, the marginal relationship between $X_{1}$ and $Y$ is uninformative for feature selection. A filter method based on univariate association would assign $X_1$ zero or near-zero importance and fail to select it, despite its critical role in the multivariate relationship that determines $Y$.\n\n**Part 2: Calculation of Mutual Information $I(X_{1}; Y)$**\n\nWe will now compute the exact value of the Mutual Information $I(X_{1}; Y)$ using the specified synthetic cohort. The cohort consists of $N = 4n$ individuals, with each of the four possible combinations of $(X_1, X_2)$ appearing exactly $n$ times. The value of $Y$ is determined by $Y=X_1 \\oplus X_2$.\n\nThe counts for each joint outcome of $(X_1, X_2, Y)$ are:\n-   $(X_1=0, X_2=0) \\implies Y=0$: count is $n$.\n-   $(X_1=0, X_2=1) \\implies Y=1$: count is $n$.\n-   $(X_1=1, X_2=0) \\implies Y=1$: count is $n$.\n-   $(X_1=1, X_2=1) \\implies Y=0$: count is $n$.\n\nFrom these counts, we can calculate the empirical probabilities required for the MI formula. The total number of individuals is $4n$.\n-   $p(X_{1}=0, Y=0)$: count for $(X_1=0, Y=0)$ comes from $(X_1=0, X_2=0)$, which is $n$. So, $p(X_{1}=0, Y=0) = \\frac{n}{4n} = \\frac{1}{4}$.\n-   $p(X_{1}=0, Y=1)$: count for $(X_1=0, Y=1)$ comes from $(X_1=0, X_2=1)$, which is $n$. So, $p(X_{1}=0, Y=1) = \\frac{n}{4n} = \\frac{1}{4}$.\n-   $p(X_{1}=1, Y=0)$: count for $(X_1=1, Y=0)$ comes from $(X_1=1, X_2=1)$, which is $n$. So, $p(X_{1}=1, Y=0) = \\frac{n}{4n} = \\frac{1}{4}$.\n-   $p(X_{1}=1, Y=1)$: count for $(X_1=1, Y=1)$ comes from $(X_1=1, X_2=0)$, which is $n$. So, $p(X_{1}=1, Y=1) = \\frac{n}{4n} = \\frac{1}{4}$.\n\nThe marginal probabilities are:\n-   $p(X_{1}=0) = p(X_{1}=0, Y=0) + p(X_{1}=0, Y=1) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$.\n-   $p(X_{1}=1) = p(X_{1}=1, Y=0) + p(X_{1}=1, Y=1) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$.\n-   $p(Y=0) = p(X_{1}=0, Y=0) + p(X_{1}=1, Y=0) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$.\n-   $p(Y=1) = p(X_{1}=0, Y=1) + p(X_{1}=1, Y=1) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$.\n\nThese empirical probabilities from the cohort are identical to the theoretical probabilities derived in Part 1.\n\nNow, we use the MI formula:\n$$ I(X_{1}; Y) = \\sum_{x_{1} \\in \\{0,1\\}} \\sum_{y \\in \\{0,1\\}} p(x_{1},y) \\ln\\!\\left(\\frac{p(x_{1},y)}{p(x_{1})\\,p(y)}\\right) $$\nFor each term in the summation, the ratio inside the logarithm is:\n$$ \\frac{p(x_{1},y)}{p(x_{1})\\,p(y)} = \\frac{1/4}{(1/2)(1/2)} = \\frac{1/4}{1/4} = 1 $$\nThis is true for all four combinations of $(x_1, y)$.\n\nTherefore, the MI calculation becomes:\n$$ I(X_1; Y) = p(0,0)\\ln(1) + p(0,1)\\ln(1) + p(1,0)\\ln(1) + p(1,1)\\ln(1) $$\n$$ I(X_1; Y) = \\frac{1}{4}\\ln(1) + \\frac{1}{4}\\ln(1) + \\frac{1}{4}\\ln(1) + \\frac{1}{4}\\ln(1) $$\nSince $\\ln(1) = 0$, every term in the sum is zero.\n$$ I(X_1; Y) = \\frac{1}{4}(0) + \\frac{1}{4}(0) + \\frac{1}{4}(0) + \\frac{1}{4}(0) = 0 $$\nThe Mutual Information between $X_{1}$ and $Y$ is exactly $0$. This confirms the conclusion from Part 1 that $X_1$ and $Y$ are statistically independent, and thus the marginal relationship is uninformative.", "answer": "$$\n\\boxed{0}\n$$", "id": "4563606"}]}