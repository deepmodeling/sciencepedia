{"hands_on_practices": [{"introduction": "This first practice exercise grounds our study in a common and essential task: normalizing RNA-sequencing data. You will calculate Transcripts Per Million (TPM) values by hand, a process that illuminates how we correct for two major sources of technical bias: gene length and sequencing depth. This fundamental skill ensures that gene expression values are comparable both within and across different samples. [@problem_id:4562784]", "problem": "You are building gene expression features for downstream prognostic modeling in RNA sequencing data within clinical bioinformatics. A central normalization used in feature engineering is Transcripts Per Million (TPM), which adjusts for gene length and library size to yield comparable abundance estimates across samples. Consider a toy dataset comprising $3$ genes, labeled $G_1$, $G_2$, and $G_3$, with lengths $L_{G_1} = 1000$ base pairs (bp), $L_{G_2} = 2000$ bp, and $L_{G_3} = 500$ bp. Two patient samples, $S_1$ and $S_2$, have observed gene-level read counts $r_{g,s}$ as follows: for $S_1$, $(r_{G_1,S_1}, r_{G_2,S_1}, r_{G_3,S_1}) = (300, 200, 200)$; for $S_2$, $(r_{G_1,S_2}, r_{G_2,S_2}, r_{G_3,S_2}) = (200, 600, 250)$. Using first principles appropriate to RNA sequencing normalization, namely that expression estimation must correct for gene length (by scaling counts by gene length measured in kilobases) and for sample-specific library size (by rescaling within-sample abundance to a fixed constant of $10^6$), derive the Transcripts Per Million (TPM) values for each gene in each sample. Explicitly verify, as part of your derivation, that within each sample the TPM values sum to $10^6$. Finally, report the TPM of gene $G_3$ in sample $S_2$. Round your final numerical answer to four significant figures and express it as a pure number (no units).", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of RNA sequencing data normalization, well-posed with all necessary information provided, and objective in its language. The problem requires the calculation of Transcripts Per Million (TPM) values for a toy dataset, a standard procedure in bioinformatics.\n\nThe calculation of TPM is a two-step normalization process. First, read counts are normalized for gene length. Second, they are normalized for sequencing depth (library size) to make them comparable across samples.\n\nLet $r_{g,s}$ be the raw read count for gene $g$ in sample $s$.\nLet $L_g$ be the length of gene $g$ in base pairs (bp).\n\nThe given data are:\nGene lengths:\n$L_{G_1} = 1000$ bp\n$L_{G_2} = 2000$ bp\n$L_{G_3} = 500$ bp\n\nRead counts for sample $S_1$:\n$(r_{G_1,S_1}, r_{G_2,S_1}, r_{G_3,S_1}) = (300, 200, 200)$\n\nRead counts for sample $S_2$:\n$(r_{G_1,S_2}, r_{G_2,S_2}, r_{G_3,S_2}) = (200, 600, 250)$\n\nThe TPM calculation proceeds as follows:\n\n**Step 1: Normalize for gene length.**\nFirst, we convert the gene lengths from base pairs (bp) to kilobases (kb), as specified. Let $L'_g$ be the length of gene $g$ in kb.\n$$L'_g = \\frac{L_g}{1000}$$\nFor our $3$ genes:\n$L'_{G_1} = \\frac{1000}{1000} = 1.0 \\text{ kb}$\n$L'_{G_2} = \\frac{2000}{1000} = 2.0 \\text{ kb}$\n$L'_{G_3} = \\frac{500}{1000} = 0.5 \\text{ kb}$\n\nNext, we calculate the rate of Reads Per Kilobase (RPK) for each gene in each sample.\n$$RPK_{g,s} = \\frac{r_{g,s}}{L'_g}$$\n\n**Calculations for Sample $S_1$:**\n$RPK_{G_1,S_1} = \\frac{r_{G_1,S_1}}{L'_{G_1}} = \\frac{300}{1.0} = 300$\n$RPK_{G_2,S_1} = \\frac{r_{G_2,S_1}}{L'_{G_2}} = \\frac{200}{2.0} = 100$\n$RPK_{G_3,S_1} = \\frac{r_{G_3,S_1}}{L'_{G_3}} = \\frac{200}{0.5} = 400$\n\n**Calculations for Sample $S_2$:**\n$RPK_{G_1,S_2} = \\frac{r_{G_1,S_2}}{L'_{G_1}} = \\frac{200}{1.0} = 200$\n$RPK_{G_2,S_2} = \\frac{r_{G_2,S_2}}{L'_{G_2}} = \\frac{600}{2.0} = 300$\n$RPK_{G_3,S_2} = \\frac{r_{G_3,S_2}}{L'_{G_3}} = \\frac{250}{0.5} = 500$\n\n**Step 2: Normalize for library size and scale to $10^6$.**\nWe compute a per-sample normalization factor, $T_s$, by summing the RPK values across all genes within that sample.\n$$T_s = \\sum_{g} RPK_{g,s}$$\nThen, the TPM value for each gene is calculated by dividing its RPK by the normalization factor and multiplying by $10^6$.\n$$TPM_{g,s} = \\left(\\frac{RPK_{g,s}}{T_s}\\right) \\times 10^6$$\n\n**Calculations for Sample $S_1$:**\nFirst, calculate the normalization factor $T_{S_1}$:\n$T_{S_1} = RPK_{G_1,S_1} + RPK_{G_2,S_1} + RPK_{G_3,S_1} = 300 + 100 + 400 = 800$\n\nNow, calculate the TPM values for each gene in $S_1$:\n$TPM_{G_1,S_1} = \\left(\\frac{300}{800}\\right) \\times 10^6 = 0.375 \\times 10^6 = 375000$\n$TPM_{G_2,S_1} = \\left(\\frac{100}{800}\\right) \\times 10^6 = 0.125 \\times 10^6 = 125000$\n$TPM_{G_3,S_1} = \\left(\\frac{400}{800}\\right) \\times 10^6 = 0.5 \\times 10^6 = 500000$\n\nVerification for $S_1$: The sum of TPMs must be $10^6$.\n$\\sum_{g} TPM_{g,S_1} = 375000 + 125000 + 500000 = 1000000 = 10^6$. The condition is met.\n\n**Calculations for Sample $S_2$:**\nFirst, calculate the normalization factor $T_{S_2}$:\n$T_{S_2} = RPK_{G_1,S_2} + RPK_{G_2,S_2} + RPK_{G_3,S_2} = 200 + 300 + 500 = 1000$\n\nNow, calculate the TPM values for each gene in $S_2$:\n$TPM_{G_1,S_2} = \\left(\\frac{200}{1000}\\right) \\times 10^6 = 0.2 \\times 10^6 = 200000$\n$TPM_{G_2,S_2} = \\left(\\frac{300}{1000}\\right) \\times 10^6 = 0.3 \\times 10^6 = 300000$\n$TPM_{G_3,S_2} = \\left(\\frac{500}{1000}\\right) \\times 10^6 = 0.5 \\times 10^6 = 500000$\n\nVerification for $S_2$: The sum of TPMs must be $10^6$.\n$\\sum_{g} TPM_{g,S_2} = 200000 + 300000 + 500000 = 1000000 = 10^6$. The condition is met.\n\nThe problem asks for the TPM of gene $G_3$ in sample $S_2$. Based on the calculation above:\n$TPM_{G_3,S_2} = 500000$\n\nThe final answer must be rounded to four significant figures. The exact value is $500000$. To represent this with four significant figures, we use scientific notation: $5.000 \\times 10^5$.", "answer": "$$\\boxed{5.000 \\times 10^{5}}$$", "id": "4562784"}, {"introduction": "Moving beyond corrections for specific known biases, our next exercise involves implementing quantile normalization, a powerful technique that forces the statistical distributions of different samples to be identical. By writing the code for this algorithm, you will gain a deep, procedural understanding of how to align data from different experiments, a crucial step for reducing batch effects. This practice demonstrates the goal of making features directly comparable by mapping them to a common reference distribution. [@problem_id:4562739]", "problem": "Consider a data matrix $X \\in \\mathbb{R}^{m \\times n}$ whose columns represent samples and whose rows represent features (for example, gene expression intensities across multiple assay runs). In many bioinformatics pipelines, the goal of normalization is to map each sample to a common marginal distribution so that inter-sample comparisons are not confounded by technical variation. One widely used method is quantile normalization, which can be formalized as follows. For each column $j \\in \\{1,\\dots,n\\}$, let $\\pi_j : \\{1,\\dots,m\\} \\to \\{1,\\dots,m\\}$ be a permutation that sorts the entries of column $j$ in non-decreasing order, and define the order statistics $x_{(1)j} \\le x_{(2)j} \\le \\dots \\le x_{(m)j}$ by $x_{(r)j} = x_{\\pi_j(r), j}$. The quantile-normalized column is constructed by computing the across-sample average at each rank,\n$$\ns_r = \\frac{1}{n} \\sum_{j=1}^n x_{(r)j} \\quad \\text{for } r \\in \\{1,\\dots,m\\},\n$$\nand then assigning these averaged values back to the original indices via $y_{\\pi_j(r), j} = s_r$. The normalized matrix is $Y \\in \\mathbb{R}^{m \\times n}$ with entries $y_{ij}$. Let $F_j(t)$ denote the empirical cumulative distribution function (empirical CDF) of column $j$,\n$$\nF_j(t) = \\frac{1}{m} \\sum_{i=1}^m \\mathbf{1}\\{ y_{ij} \\le t \\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. By construction, each column of $Y$ is a permutation of the multiset $\\{ s_1, s_2, \\dots, s_m \\}$, so each $F_j$ is the same step function when evaluated on the common support $\\{ s_1, \\dots, s_m \\}$ (up to multiplicities). To numerically verify identity of the marginal empirical CDFs after quantile normalization, define the maximum pairwise sup-norm deviation as\n$$\n\\Delta(X) = \\max_{1 \\le j_1 < j_2 \\le n} \\sup_{t \\in \\mathbb{R}} \\left| F_{j_1}(t) - F_{j_2}(t) \\right|.\n$$\nBecause the $F_{j}$ are right-continuous step functions with jumps only at the values in $\\{ s_1, \\dots, s_m \\}$, it suffices to compute the supremum over the sorted unique values of $\\{ s_1, \\dots, s_m \\}$. Implement a program that:\n- Computes the quantile-normalized matrix $Y$ from a given input matrix $X$ via the above rank-averaging procedure using a stable sort for ties.\n- Computes the empirical CDFs $F_j$ of all columns of $Y$ on the common, sorted, unique support of $\\{ s_1, \\dots, s_m \\}$.\n- Computes $\\Delta(X)$ as the maximum, over all column pairs, of the sup-norm difference between their empirical CDFs evaluated on that support.\nYour program should process the following test suite of matrices (each matrix is given by its rows, each row is a list of real numbers; ensure they are used exactly as specified):\n- Test case $1$ ($5 \\times 3$):\n  `[[5.2, 2.0, 8.1], [3.1, -1.0, 7.9], [9.0, 5.5, 0.0], [4.4, 3.3, 1.2], [7.7, 2.2, 9.9]]`.\n- Test case $2$ ($6 \\times 4$, with a constant column):\n  `[[10.0, 1.0, 3.0, 7.0], [10.0, 4.0, 3.5, -2.0], [10.0, -5.0, 10.0, 7.1], [10.0, 2.0, -8.0, 7.2], [10.0, 2.5, 0.0, -2.1], [10.0, 100.0, 5.0, 0.0]]`.\n- Test case $3$ ($4 \\times 2$, with ties):\n  `[[1.0, 1.0], [1.0, 2.0], [3.0, 2.0], [3.0, 3.0]]`.\n- Test case $4$ ($1 \\times 3$, trivial single-row):\n  `[[42.0, -1.0, 100.0]]`.\n- Test case $5$ ($3 \\times 3$, extreme dynamic range):\n  `[[1e-9, 1e9, -1e3], [2e-9, -1e9, 0.5], [3e-9, 0.0, 1.5]]`.\nFor each test case matrix $X$, compute $\\Delta(X)$ as a floating-point number. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $\\big[ \\text{result1}, \\text{result2}, \\dots \\big]$). There are no physical units involved; angles are not relevant; and fractional outputs must be provided as decimal numbers. The test suite is designed to cover: a general case, a boundary case with a constant column, explicit ties, a trivial dimensionality case, and a case with extreme magnitudes to probe numerical stability. The final outputs must be floats.", "solution": "The problem requires the implementation of the quantile normalization algorithm and the subsequent numerical verification of its defining property: that all columns (samples) of the resulting matrix share the same empirical marginal distribution. This verification is performed by computing $\\Delta(X)$, the maximum pairwise supremum-norm deviation between the empirical cumulative distribution functions (ECDFs) of the normalized columns.\n\nThe solution proceeds in three stages: first, the quantile normalization of the input matrix $X$; second, the computation of the ECDFs for each column of the normalized matrix $Y$; and third, the calculation of the maximum pairwise deviation $\\Delta(X)$.\n\n**1. Quantile Normalization Procedure**\n\nGiven an input data matrix $X \\in \\mathbb{R}^{m \\times n}$, where $m$ is the number of features and $n$ is the number of samples, the quantile-normalized matrix $Y \\in \\mathbb{R}^{m \\times n}$ is computed as follows:\n\n- **Sorting and Ranking:** For each column $j \\in \\{1, \\dots, n\\}$, we determine the permutation $\\pi_j$ of the row indices $\\{1, \\dots, m\\}$ that sorts the elements of the column in non-decreasing order. This yields the order statistics $x_{(r)j} = x_{\\pi_j(r), j}$ for each rank $r \\in \\{1, \\dots, m\\}$, such that $x_{(1)j} \\le x_{(2)j} \\le \\dots \\le x_{(m)j}$. The problem specifies the use of a stable sort to ensure deterministic handling of ties, where elements with equal value maintain their original relative order.\n\n- **Rank-wise Averaging:** A target set of quantiles is derived by averaging the values across all samples for each rank. For each rank $r$, the average value $s_r$ is computed as:\n$$\ns_r = \\frac{1}{n} \\sum_{j=1}^{n} x_{(r)j}\n$$\nThe collection of these averages $\\{s_1, s_2, \\dots, s_m\\}$ forms the common empirical distribution to which all samples will be mapped.\n\n- **Value Replacement:** The normalized matrix $Y$ is constructed by replacing the original values in $X$ with the computed rank-wise averages. Specifically, the element in column $j$ that had the $r$-th rank is replaced by $s_r$. This operation is formalized by the assignment $y_{\\pi_j(r), j} = s_r$. Algorithmically, this is achieved by associating each original value $x_{ij}$ with its rank in column $j$, and then substituting it with the corresponding value from the vector $s = (s_1, \\dots, s_m)$. An efficient way to implement this is to find the inverse of the sorting permutation for each column and use it to arrange the elements of $s$ into the new column of $Y$.\n\n**2. ECDF Calculation and the $\\Delta(X)$ Metric**\n\nBy the construction outlined above, each column of the normalized matrix $Y$ is a permutation of the same multiset of values $S = \\{s_1, s_2, \\dots, s_m\\}$. As a direct consequence, the empirical cumulative distribution function (ECDF) for each column must be identical. The ECDF for a column $j$ of $Y$ is defined as:\n$$\nF_j(t) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathbf{1}\\{y_{ij} \\le t\\}\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. Since the multiset of values $\\{y_{ij}\\}_{i=1}^m$ is the same for every column $j$, the count of values less than or equal to any given $t \\in \\mathbb{R}$ is also the same. Therefore, in exact arithmetic, $F_{j_1}(t) = F_{j_2}(t)$ for any pair of columns $(j_1, j_2)$ and for all $t$.\n\nThe problem defines a metric, $\\Delta(X)$, to numerically verify this identity:\n$$\n\\Delta(X) = \\max_{1 \\le j_1 < j_2 \\le n} \\sup_{t \\in \\mathbb{R}} \\left| F_{j_1}(t) - F_{j_2}(t) \\right|\n$$\nSince the ECDFs are step functions, the supremum (sup-norm) of the difference only needs to be evaluated at the points where jumps occur, which are the values present in the normalized columns. Thus, the evaluation can be restricted to the set of sorted, unique values from $\\{s_1, \\dots, s_m\\}$.\n\nTheoretically, $\\Delta(X)$ should be exactly $0$. The computation serves as a confirmation that the implementation correctly adheres to the algorithm's definition. Even with finite-precision floating-point arithmetic, the procedure involves creating the vector $s$ and then permuting its elements to form the columns of $Y$. This means each column of $Y$ will contain the exact same set of floating-point numbers, and thus their ECDFs will be identical, yielding a numerical result of $\\Delta(X) = 0.0$.\n\n**3. Computational Algorithm**\n\nThe implementation will strictly follow the definitions provided.\n\n1.  Given the $m \\times n$ matrix $X$, compute an $m \\times n$ matrix of sorting indices `sort_idx` where `sort_idx[:, j]` contains the row indices that sort column `X[:, j]`. A stable sorting algorithm is used.\n2.  Use `sort_idx` to create the sorted matrix $X_{sorted}$, where $X_{\\text{sorted}}[r, j] = X[\\text{sort\\_idx}[r, j], j]$.\n3.  Compute the rank-wise mean vector $s$ of length $m$ by averaging across the rows of $X_{sorted}$.\n4.  Construct the $m \\times n$ normalized matrix $Y$. For each column $j$, the values from $s$ are placed into the positions dictated by the ranks of the original elements in $X[:,j]$.\n5.  Determine the evaluation points for the ECDFs by finding the sorted unique values in $s$, denoted as the vector $t_{support}$.\n6.  For each column $j \\in \\{1, \\dots, n\\}$, calculate its ECDF evaluated at each point in $t_{support}$. This results in an ECDF value vector for each column.\n7.  Iterate through all unique pairs of columns $(j_1, j_2)$ with $j_1 < j_2$. For each pair, compute the element-wise absolute difference of their ECDF value vectors and find the maximum value (the sup-norm for the discrete support).\n8.  The final result, $\\Delta(X)$, is the maximum of these sup-norm differences over all pairs. For all test cases, this value is expected to be $0.0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not needed for this problem.\n\ndef quantile_normalize_and_compute_delta(X_list: list[list[float]]) -> float:\n    \"\"\"\n    Performs quantile normalization on a matrix and computes the maximum pairwise\n    sup-norm deviation between the empirical CDFs of the normalized columns.\n\n    Args:\n        X_list: A matrix represented as a list of lists of floats.\n\n    Returns:\n        The computed Delta(X) value as a float.\n    \"\"\"\n    X = np.array(X_list, dtype=np.float64)\n    \n    if X.ndim == 1: # Handle 1-row case\n        X = X.reshape(1, -1)\n        \n    m, n = X.shape\n\n    # If there are fewer than 2 columns, no pairs exist, so deviation is 0.\n    if n < 2:\n        return 0.0\n\n    # Step 1: Compute sorting indices for each column (stable sort)\n    # sort_idx[r, j] is the original row index of the r-th ranked element in column j.\n    sort_idx = np.argsort(X, axis=0, kind='stable')\n\n    # Step 2: Create the sorted matrix X_sorted\n    # np.take_along_axis is used to apply the sorting indices to the original matrix.\n    X_sorted = np.take_along_axis(X, sort_idx, axis=0)\n\n    # Step 3: Compute the rank-wise mean vector 's'\n    # 's' becomes the target distribution for all columns.\n    s = np.mean(X_sorted, axis=1)\n\n    # Step 4: Construct the quantile-normalized matrix Y\n    # First, find the inverse of the sorting permutations.\n    # inv_sort_idx[i, j] gives the rank of the element X[i, j].\n    inv_sort_idx = np.argsort(sort_idx, axis=0)\n    \n    # Broadcast 's' to an m x n matrix and permute its rows according to inv_sort_idx.\n    # This places the mean s[k] at the position of the element that originally had rank k.\n    s_broadcast = np.tile(s[:, np.newaxis], (1, n))\n    Y = np.take_along_axis(s_broadcast, inv_sort_idx, axis=0)\n\n    # Step 5: Compute the maximum pairwise sup-norm deviation Delta(X)\n    # The problem implies constructing Y and then computing ECDFs.\n    # By construction, every column of Y is a permutation of s.\n    # Therefore, their ECDFs must be identical. We implement the full\n    # calculation as a verification of this property.\n\n    # Common support for ECDF evaluation: sorted unique values of s.\n    t_support = np.unique(s)\n    \n    # Compute ECDFs for all columns of Y on the common support.\n    # ecdfs[j, t] stores the value of the ECDF for column j at support point t.\n    # Broadcasting Y[:, j, np.newaxis] against t_support allows for vectorized ECDF calculation.\n    num_support_points = t_support.shape[0]\n    ecdfs = np.zeros((n, num_support_points))\n    for j in range(n):\n        column_y = Y[:, j]\n        # For each support point t, count how many elements in column_y are <= t.\n        # The mean gives the ECDF value.\n        ecdfs[j, :] = np.mean(column_y[:, np.newaxis] <= t_support, axis=0)\n\n    # Calculate the maximum sup-norm deviation over all pairs of columns.\n    max_deviation = 0.0\n    for j1 in range(n):\n        for j2 in range(j1 + 1, n):\n            # Sup-norm is the max of the absolute differences.\n            sup_norm = np.max(np.abs(ecdfs[j1, :] - ecdfs[j2, :]))\n            if sup_norm > max_deviation:\n                max_deviation = sup_norm\n                \n    return max_deviation\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (5 x 3, general case)\n        [[5.2, 2.0, 8.1], [3.1, -1.0, 7.9], [9.0, 5.5, 0.0], [4.4, 3.3, 1.2], [7.7, 2.2, 9.9]],\n        # Test case 2 (6 x 4, with a constant column)\n        [[10.0, 1.0, 3.0, 7.0], [10.0, 4.0, 3.5, -2.0], [10.0, -5.0, 10.0, 7.1], [10.0, 2.0, -8.0, 7.2], [10.0, 2.5, 0.0, -2.1], [10.0, 100.0, 5.0, 0.0]],\n        # Test case 3 (4 x 2, with ties)\n        [[1.0, 1.0], [1.0, 2.0], [3.0, 2.0], [3.0, 3.0]],\n        # Test case 4 (1 x 3, trivial single-row)\n        [[42.0, -1.0, 100.0]],\n        # Test case 5 (3 x 3, extreme dynamic range)\n        [[1e-9, 1e9, -1e3], [2e-9, -1e9, 0.5], [3e-9, 0.0, 1.5]],\n    ]\n\n    results = []\n    for case in test_cases:\n        result = quantile_normalize_and_compute_delta(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4562739"}, {"introduction": "Our final practice addresses a more profound challenge in feature engineering: choosing a transformation that respects the intrinsic nature of the data. This problem explores compositional data, such as microbiome abundances, where values represent relative proportions rather than absolute quantities. You will build a synthetic dataset and quantitatively evaluate how well different pipelines—including the principled Centered Log-Ratio (CLR) transform—recover the true underlying relationships, highlighting why understanding data structure is paramount for meaningful analysis. [@problem_id:4562735]", "problem": "You are given a synthetic scenario modeling high-throughput sequencing data in bioinformatics and medical data analytics, where measured taxon counts per sample are compositional by construction. The downstream goal is to decide, by principled computation, which feature normalization and transformation pipeline yields pairwise distances that best align with the latent, data-generating process. Your program must be a complete, runnable program that constructs the data, computes pairwise distances under specified transforms, quantitatively assesses alignment to the ground truth, and outputs a final aggregated decision for a provided test suite.\n\nFundamental base and definitions:\n- Compositional data consist of positive vectors that are only informative up to scale. The closure operator maps any positive vector $\\mathbf{v} \\in \\mathbb{R}_{>0}^{K}$ to the simplex by $C(\\mathbf{v}) = \\mathbf{v} / \\left(\\sum_{k=1}^{K} v_k\\right)$.\n- The Centered Log-Ratio (CLR) transform of a composition $\\mathbf{x} \\in \\mathbb{R}_{>0}^{K}$ is defined as $\\operatorname{clr}(\\mathbf{x}) = \\log(\\mathbf{x}) - \\frac{1}{K}\\mathbf{1}\\mathbf{1}^{\\top}\\log(\\mathbf{x})$, where $\\log(\\cdot)$ is applied elementwise and $\\mathbf{1}$ is the all-ones vector in $\\mathbb{R}^{K}$.\n- The Aitchison distance between two compositions equals the Euclidean distance between their CLR transforms.\n- The Euclidean distance between two vectors $\\mathbf{u},\\mathbf{v} \\in \\mathbb{R}^{K}$ is $\\|\\mathbf{u} - \\mathbf{v}\\|_{2}$.\n- The Pearson correlation between two real vectors is defined as the covariance divided by the product of standard deviations.\n\nData-generating process:\n- There are $K$ taxa, $N$ samples, and $F$ latent factors. For sample $i \\in \\{1,\\ldots,N\\}$, the latent absolute abundance vector is\n$$\n\\log \\mathbf{A}_i = \\mathbf{b} + \\mathbf{W}\\,\\mathbf{s}_i,\n$$\nwhere $\\mathbf{b} \\in \\mathbb{R}^{K}$ is a baseline log-abundance vector, $\\mathbf{W} \\in \\mathbb{R}^{K \\times F}$ is a loadings matrix, and $\\mathbf{s}_i \\in \\mathbb{R}^{F}$ is the factor score vector for sample $i$. Absolute abundances are $\\mathbf{A}_i = \\exp(\\log \\mathbf{A}_i)$, applied elementwise.\n- Observed counts before rounding are $\\tilde{\\mathbf{c}}_i = L_i \\, C(\\mathbf{A}_i)$, where $L_i \\in \\mathbb{R}_{>0}$ is the library size (a strictly positive scalar) for sample $i$. If rounding is enabled for a test case, the observed counts become $\\mathbf{c}_i = \\operatorname{round}(\\tilde{\\mathbf{c}}_i)$, where $\\operatorname{round}(\\cdot)$ denotes rounding to the nearest integer, applied elementwise. If rounding is disabled, set $\\mathbf{c}_i = \\tilde{\\mathbf{c}}_i$.\n\nGround-truth distances:\n- The target distances induced by the data-generating process are defined between samples $i$ and $j$ as\n$$\nD_{\\mathrm{true}}(i,j) = \\left\\|\\log \\mathbf{A}_i - \\log \\mathbf{A}_j \\right\\|_{2}.\n$$\n\nCandidate pipelines and their pairwise distances:\n- Raw-count Euclidean: $D_{\\mathrm{raw}}(i,j) = \\left\\| \\mathbf{c}_i - \\mathbf{c}_j \\right\\|_{2}$.\n- Log-count Euclidean (with pseudocount): given a pseudocount $\\epsilon > 0$, define $\\mathbf{z}_i = \\log(\\mathbf{c}_i + \\epsilon)$ elementwise, and compute $D_{\\log}(i,j) = \\left\\|\\mathbf{z}_i - \\mathbf{z}_j\\right\\|_{2}$.\n- Aitchison via CLR (with pseudocount before closure): given a pseudocount $\\epsilon > 0$, define $\\mathbf{u}_i = \\mathbf{c}_i + \\epsilon$, then proportions $\\mathbf{p}_i = C(\\mathbf{u}_i)$, then $\\operatorname{clr}(\\mathbf{p}_i)$, and compute $D_{\\mathrm{clr}}(i,j) = \\left\\| \\operatorname{clr}(\\mathbf{p}_i) - \\operatorname{clr}(\\mathbf{p}_j) \\right\\|_{2}$.\n\nAlignment criterion:\n- For each candidate distance type $T \\in \\{\\mathrm{raw},\\log,\\mathrm{clr}\\}$, vectorize the upper-triangle (excluding the diagonal) of the pairwise distance matrix $D_T$ and also of $D_{\\mathrm{true}}$ into vectors $\\mathbf{d}_T$ and $\\mathbf{d}_{\\mathrm{true}}$. Compute the Pearson correlation $r_T$ between $\\mathbf{d}_T$ and $\\mathbf{d}_{\\mathrm{true}}$.\n- The best-aligned metric is the one with the largest $r_T$. In the event of a tie within a tolerance $\\gamma = 10^{-9}$, choose the metric with the smallest index according to the ordering $\\mathrm{raw} \\mapsto 0$, $\\log \\mapsto 1$, $\\mathrm{clr} \\mapsto 2$.\n\nTest suite:\nUse $K=5$ taxa, $N=4$ samples, and $F=2$ factors. Use the following shared, fixed parameters for all tests:\n- Baseline log-abundance vector $\\mathbf{b} = [\\,1.0,\\,0.5,\\,-0.2,\\,0.3,\\,-0.1\\,]$.\n- Loadings matrix with columns summing to zero across taxa to model compositional shifts unimpaired by constant offsets:\n$$\n\\mathbf{W} = \n\\begin{bmatrix}\n0.8 & -0.5 \\\\\n0.2 & 0.1 \\\\\n-0.4 & 0.2 \\\\\n-0.3 & 0.4 \\\\\n-0.3 & -0.2\n\\end{bmatrix}.\n$$\n- Sample factor scores\n$$\n\\mathbf{s}_1 = \\begin{bmatrix} -1.0 \\\\ 0.5 \\end{bmatrix},\\quad\n\\mathbf{s}_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix},\\quad\n\\mathbf{s}_3 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\end{bmatrix},\\quad\n\\mathbf{s}_4 = \\begin{bmatrix} 1.2 \\\\ 0.8 \\end{bmatrix}.\n$$\n\nDefine three test cases that differ only in library sizes, rounding, and pseudocount:\n- Test case A (happy path, equal libraries; minimal compositional distortion):\n  - Library sizes $\\mathbf{L}^{(A)} = [\\,10000,\\,10000,\\,10000,\\,10000\\,]$.\n  - Rounding enabled.\n  - Pseudocount $\\epsilon^{(A)} = 1.0$.\n- Test case B (library confounding; strong differences in total counts):\n  - Library sizes $\\mathbf{L}^{(B)} = [\\,500,\\,20000,\\,200,\\,80000\\,]$.\n  - Rounding enabled.\n  - Pseudocount $\\epsilon^{(B)} = 1.0$.\n- Test case C (sparse regime with many small counts inducing zeros; robustness to pseudocount choice):\n  - Library sizes $\\mathbf{L}^{(C)} = [\\,100,\\,120,\\,80,\\,60\\,]$.\n  - Rounding enabled.\n  - Pseudocount $\\epsilon^{(C)} = 0.5$.\n\nRequired outputs:\n- For each test case, compute $r_{\\mathrm{raw}}$, $r_{\\log}$, and $r_{\\mathrm{clr}}$ as defined, determine the best-aligned metric using the tie-breaking rule with $\\gamma = 10^{-9}$, and encode it as an integer according to $\\mathrm{raw} \\mapsto 0$, $\\log \\mapsto 1$, $\\mathrm{clr} \\mapsto 2$.\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list of integers enclosed in square brackets (e.g., \"[2,2,2]\"). No additional characters or whitespace are permitted in the output.\n\nAngle units, physical units, and percentages do not apply to this problem. Ensure all computations use the natural logarithm. The program must be self-contained, must not accept any input, and must rely only on the specified environment. All numbers must be handled and represented internally in standard floating-point arithmetic.", "solution": "The posed problem requires an evaluation of three distinct feature engineering and normalization pipelines for compositional data, which is characteristic of high-throughput sequencing outputs in bioinformatics. The goal is to determine which pipeline's resulting sample-to-sample distances best correlate with a known, ground-truth distance matrix derived from the latent data-generating process. This is a synthetic benchmark designed to quantitatively assess the efficacy of common data transformation techniques.\n\nFirst, we will construct the dataset according to the specified data-generating process. This process is founded on a log-linear model, a standard approach for simulating count-based biological data. For each of the $N=4$ samples, its latent state is defined by a factor score vector $\\mathbf{s}_i \\in \\mathbb{R}^{F}$ where $F=2$. These scores modulate a baseline log-abundance vector $\\mathbf{b} \\in \\mathbb{R}^{K}$ (with $K=5$ taxa) via a loadings matrix $\\mathbf{W} \\in \\mathbb{R}^{K \\times F}$. The latent log-absolute abundance vector for sample $i$ is given by:\n$$\n\\log \\mathbf{A}_i = \\mathbf{b} + \\mathbf{W}\\,\\mathbf{s}_i\n$$\nThe absolute abundances $\\mathbf{A}_i$ are obtained by elementwise exponentiation, $\\mathbf{A}_i = \\exp(\\log \\mathbf{A}_i)$. These represent the true, unobserved quantities of each taxon.\n\nIn sequencing experiments, the total number of reads per sample (library size, $L_i$) is a technical artifact and does not typically reflect total biomass. The measurement process captures relative, not absolute, abundances. This is modeled by applying the closure operator $C(\\mathbf{v}) = \\mathbf{v} / \\sum_{k=1}^{K} v_k$ to the absolute abundances, yielding a composition $\\mathbf{p}_i = C(\\mathbf{A}_i)$ on the simplex. The pre-rounded observed counts are then simulated by scaling this composition by the library size, $\\tilde{\\mathbf{c}}_i = L_i \\mathbf{p}_i$. Finally, these are rounded to the nearest integer to mimic discrete count data, $\\mathbf{c}_i = \\operatorname{round}(\\tilde{\\mathbf{c}}_i)$. This process creates a dataset where sample-to-sample differences are a mixture of true biological variation (from $\\mathbf{s}_i$) and technical artifacts (from $L_i$).\n\nThe ground-truth distance between any two samples, $i$ and $j$, is defined in the latent space of log-abundances:\n$$\nD_{\\mathrm{true}}(i,j) = \\left\\|\\log \\mathbf{A}_i - \\log \\mathbf{A}_j \\right\\|_{2}\n$$\nThis is the Euclidean distance between the unobserved log-abundance vectors. It represents the \"true\" dissimilarity that an ideal analysis pipeline should recover from the observed count data $\\mathbf{c}$.\n\nNext, we calculate sample-to-sample distances using three candidate pipelines applied to the observed counts $\\mathbf{c}_i$.\n\n$1$. **Raw-count Euclidean distance ($D_{\\mathrm{raw}}$)**:\n$D_{\\mathrm{raw}}(i,j) = \\left\\| \\mathbf{c}_i - \\mathbf{c}_j \\right\\|_{2}$. This is the most naive approach. It treats the counts as measurements in a standard Euclidean space. This metric is highly sensitive to differences in library sizes ($L_i$), which can dominate the distance calculation and obscure the underlying compositional differences. It is expected to perform poorly, especially in Test Case B where library sizes vary dramatically.\n\n$2$. **Log-count Euclidean distance ($D_{\\log}$)**:\nGiven a pseudocount $\\epsilon > 0$, we transform the counts to $\\mathbf{z}_i = \\log(\\mathbf{c}_i + \\epsilon)$ and compute $D_{\\log}(i,j) = \\left\\|\\mathbf{z}_i - \\mathbf{z}_j\\right\\|_{2}$. The logarithm helps to down-weight the influence of large counts and makes the data more symmetric. The pseudocount $\\epsilon$ is necessary to handle zero counts, which would otherwise be undefined under the logarithm. While often an improvement over raw counts, this method does not fully account for the unit-sum constraint of compositional data.\n\n$3$. **Aitchison distance via Centered Log-Ratio (CLR) transform ($D_{\\mathrm{clr}}$)**:\nThis is a principled approach from Compositional Data Analysis (CoDa). It correctly handles the relative nature of the data. First, a pseudocount $\\epsilon > 0$ is added to the raw counts to handle zeros: $\\mathbf{u}_i = \\mathbf{c}_i + \\epsilon$. Then, the data is closed to a composition, $\\mathbf{p}_i = C(\\mathbf{u}_i)$. The CLR transform is then applied:\n$$\n\\operatorname{clr}(\\mathbf{p}_i) = \\log(\\mathbf{p}_i) - \\frac{1}{K}\\sum_{k=1}^{K}\\log(p_{ik})\n$$\nThis transform maps the $K$-part compositional data from the simplex to a $(K-1)$-dimensional Euclidean space. The Aitchison distance is defined as the standard Euclidean distance in this CLR-space:\n$$\nD_{\\mathrm{clr}}(i,j) = \\left\\| \\operatorname{clr}(\\mathbf{p}_i) - \\operatorname{clr}(\\mathbf{p}_j) \\right\\|_{2}\n$$\nBy centering on the geometric mean of each composition's parts, the CLR transform effectively removes the influence of scale (and thus library size), focusing only on the relative changes between components. This method is theoretically best-suited for this type of data.\n\nTo evaluate which pipeline is superior, we compare the distances from each candidate pipeline to the ground-truth distances. For each distance type $T \\in \\{\\mathrm{raw}, \\log, \\mathrm{clr}\\}$, we form a vector $\\mathbf{d}_T$ containing all unique pairwise distances (the upper triangle of the distance matrix). We do the same for the true distances, creating vector $\\mathbf{d}_{\\mathrm{true}}$. The performance metric is the Pearson correlation coefficient, $r_T = \\operatorname{corr}(\\mathbf{d}_T, \\mathbf{d}_{\\mathrm{true}})$. The pipeline yielding the highest correlation is deemed the best. A tie-breaking rule, based on a tolerance of $\\gamma = 10^{-9}$ and a predefined ordering ($\\mathrm{raw} \\mapsto 0$, $\\log \\mapsto 1$, $\\mathrm{clr} \\mapsto 2$), ensures a unique winner.\n\nThe overall algorithm proceeds as follows for each of the three test cases:\n$1$. Generate the observed count matrix $\\{\\mathbf{c}_i\\}_{i=1..N}$ using the specified parameters for that test case.\n$2$. Compute the $N \\times N$ ground-truth distance matrix $D_{\\mathrm{true}}$ and vectorize its upper triangle into $\\mathbf{d}_{\\mathrm{true}}$.\n$3$. For each of the three pipelines, compute the corresponding $N \\times N$ distance matrix ($D_{\\mathrm{raw}}$, $D_{\\log}$, $D_{\\mathrm{clr}}$) and vectorize its upper triangle ($\\mathbf{d}_{\\mathrm{raw}}$, $\\mathbf{d}_{\\log}$, $\\mathbf{d}_{\\mathrm{clr}}$).\n$4$. Calculate the three Pearson correlations: $r_{\\mathrm{raw}}$, $r_{\\log}$, and $r_{\\mathrm{clr}}$.\n$5$. Identify the largest correlation value and use the tie-breaking rule to select the index of the winning pipeline.\nThis process will be repeated for all test cases, and the final list of winning indices will be reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by generating compositional data, computing distances\n    via three pipelines, and evaluating their alignment with ground-truth distances.\n    \"\"\"\n    # Shared parameters across all test cases\n    K = 5  # Number of taxa\n    N = 4  # Number of samples\n    F = 2  # Number of latent factors\n    gamma = 1e-9  # Tie-breaking tolerance\n\n    b = np.array([1.0, 0.5, -0.2, 0.3, -0.1])\n    W = np.array([\n        [0.8, -0.5],\n        [0.2, 0.1],\n        [-0.4, 0.2],\n        [-0.3, 0.4],\n        [-0.3, -0.2]\n    ])\n    s_vectors = [\n        np.array([-1.0, 0.5]),\n        np.array([0.0, 0.0]),\n        np.array([0.5, -0.5]),\n        np.array([1.2, 0.8])\n    ]\n\n    # Definition of the three test cases\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"L\": np.array([10000.0, 10000.0, 10000.0, 10000.0]),\n            \"rounding\": True,\n            \"epsilon\": 1.0\n        },\n        {\n            \"name\": \"B\",\n            \"L\": np.array([500.0, 20000.0, 200.0, 80000.0]),\n            \"rounding\": True,\n            \"epsilon\": 1.0\n        },\n        {\n            \"name\": \"C\",\n            \"L\": np.array([100.0, 120.0, 80.0, 60.0]),\n            \"rounding\": True,\n            \"epsilon\": 0.5\n        }\n    ]\n\n    # Mapping from metric name to index\n    metric_map = {\"raw\": 0, \"log\": 1, \"clr\": 2}\n    \n    final_results = []\n\n    for case in test_cases:\n        L = case[\"L\"]\n        rounding = case[\"rounding\"]\n        epsilon = case[\"epsilon\"]\n\n        # 1. Data Generation\n        log_A = []\n        for i in range(N):\n            log_A_i = b + W @ s_vectors[i]\n            log_A.append(log_A_i)\n        \n        A = [np.exp(la) for la in log_A]\n        \n        # Closure operator C(v)\n        compositions = [a / np.sum(a) for a in A]\n        \n        # Scale by library size to get counts\n        c_tilde = [L[i] * compositions[i] for i in range(N)]\n        \n        if rounding:\n            c = [np.round(ct) for ct in c_tilde]\n        else:\n            c = c_tilde\n\n        # 2. Distance Calculations and Vectorization\n        d_true = []\n        d_raw = []\n        d_log = []\n        d_clr = []\n        \n        # Helper for CLR transform\n        def clr_transform(vec):\n            # The problem applies pseudocount before closure and CLR\n            # The input vec to this function will be pre-processed (closed composition)\n            log_vec = np.log(vec)\n            return log_vec - np.mean(log_vec)\n\n        pairs = []\n        for i in range(N):\n            for j in range(i + 1, N):\n                pairs.append((i, j))\n\n        for i, j in pairs:\n            # Ground-truth distance\n            dist_true = np.linalg.norm(log_A[i] - log_A[j])\n            d_true.append(dist_true)\n            \n            # Raw-count Euclidean distance\n            dist_raw = np.linalg.norm(c[i] - c[j])\n            d_raw.append(dist_raw)\n\n            # Log-count Euclidean distance\n            z_i = np.log(c[i] + epsilon)\n            z_j = np.log(c[j] + epsilon)\n            dist_log = np.linalg.norm(z_i - z_j)\n            d_log.append(dist_log)\n            \n            # Aitchison distance via CLR\n            u_i = c[i] + epsilon\n            u_j = c[j] + epsilon\n            \n            p_i = u_i / np.sum(u_i)\n            p_j = u_j / np.sum(u_j)\n            \n            clr_i = clr_transform(p_i)\n            clr_j = clr_transform(p_j)\n            \n            dist_clr = np.linalg.norm(clr_i - clr_j)\n            d_clr.append(dist_clr)\n\n        d_true_vec = np.array(d_true)\n        all_d_vecs = {\n            \"raw\": np.array(d_raw),\n            \"log\": np.array(d_log),\n            \"clr\": np.array(d_clr)\n        }\n\n        # 3. Alignment Criterion\n        correlations = {}\n        for name, d_vec in all_d_vecs.items():\n            # Pearson correlation\n            # np.corrcoef returns a 2x2 matrix\n            corr_matrix = np.corrcoef(d_true_vec, d_vec)\n            correlations[name] = corr_matrix[0, 1]\n\n        # 4. Decision\n        corr_list = np.array([correlations[\"raw\"], correlations[\"log\"], correlations[\"clr\"]])\n        \n        # Replace NaN correlations with a very small number if a distance vector is constant\n        # This can happen if all differences are zero for a metric\n        corr_list = np.nan_to_num(corr_list, nan=-np.inf)\n\n        max_corr = np.max(corr_list)\n        \n        # Find indices of all metrics that are within gamma of the max\n        tied_indices = np.where(np.abs(corr_list - max_corr) < gamma)[0]\n        \n        # The winner is the one with the smallest index among those tied\n        best_index = np.min(tied_indices)\n        \n        final_results.append(best_index)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "4562735"}]}