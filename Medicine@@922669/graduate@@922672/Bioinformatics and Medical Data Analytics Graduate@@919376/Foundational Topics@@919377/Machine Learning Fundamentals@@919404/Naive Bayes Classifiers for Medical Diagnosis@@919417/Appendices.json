{"hands_on_practices": [{"introduction": "A key challenge in medical diagnostics is building reliable models from limited data, especially for rare diseases or symptoms. This first practice explores the critical difference between Maximum Likelihood (ML) and Maximum A Posteriori (MAP) estimation for the parameters of a Naive Bayes classifier. By working through this hypothetical scenario, you will see how MAP estimation, by incorporating prior knowledge, provides a safeguard against the overconfidence and potential errors of ML estimation when faced with sparse data, a common issue in bioinformatics [@problem_id:4588350].", "problem": "A hospital is constructing a binary naive Bayes classifier for diagnosing a rare disease, denoted by the class label $y \\in \\{0,1\\}$ with $y=1$ indicating disease presence. The classifier uses a single binary symptom indicator $x \\in \\{0,1\\}$ that equals $1$ if the patient reports a specific symptom. The disease prevalence in the target population is known from a disease registry to be $p(y=1)=\\pi$, with $\\pi=0.005$. A small labeled training set is available: among $n_{1}=4$ known diseased patients ($y=1$), $k_{1}=1$ reported the symptom ($x=1$); among $n_{0}=6$ known non-diseased controls ($y=0$), $k_{0}=0$ reported the symptom ($x=1$).\n\nLet $\\theta_{1}=p(x=1 \\mid y=1)$ and $\\theta_{0}=p(x=1 \\mid y=0)$. Two estimation strategies are considered:\n- Maximum Likelihood (ML), using only the observed counts.\n- Maximum A Posteriori (MAP), using independent Beta priors consistent with clinical knowledge: $\\theta_{1} \\sim \\mathrm{Beta}(\\alpha_{1},\\beta_{1})$ with $\\alpha_{1}=2$, $\\beta_{1}=2$, and $\\theta_{0} \\sim \\mathrm{Beta}(\\alpha_{0},\\beta_{0})$ with $\\alpha_{0}=2$, $\\beta_{0}=198$.\n\nUsing only fundamental definitions (Bayes’ theorem for posterior probability under the naive Bayes modeling assumption, and the conjugacy of the Beta prior with the Bernoulli likelihood), compute the posterior disease probability for a new patient with $x=1$ under each estimation strategy, namely $p_{\\mathrm{ML}}(y=1 \\mid x=1)$ and $p_{\\mathrm{MAP}}(y=1 \\mid x=1)$. Then quantify the downstream change in posterior by the difference\n$$\\Delta \\equiv p_{\\mathrm{MAP}}(y=1 \\mid x=1) - p_{\\mathrm{ML}}(y=1 \\mid x=1).$$\nReport the single numeric value of $\\Delta$ rounded to four significant figures. Express the answer as a pure decimal (no percentage sign).", "solution": "The problem requires the computation and comparison of the posterior probability of a disease given a symptom, $p(y=1 \\mid x=1)$, under two different parameter estimation schemes for a naive Bayes classifier: Maximum Likelihood (ML) and Maximum A Posteriori (MAP). The final goal is to calculate the difference $\\Delta$ between the posterior probabilities obtained from these two methods.\n\nFirst, let's establish the general formula for the posterior probability using Bayes' theorem. The naive Bayes model assumes that the feature $x$ is conditionally independent of other features given the class $y$. As there is only one feature, this assumption is trivially satisfied. The posterior probability of having the disease ($y=1$) given the presence of the symptom ($x=1$) is:\n$$p(y=1 \\mid x=1) = \\frac{p(x=1 \\mid y=1) p(y=1)}{p(x=1)}$$\nThe denominator, $p(x=1)$, is the marginal probability of the evidence, which can be expanded using the law of total probability:\n$$p(x=1) = p(x=1 \\mid y=1) p(y=1) + p(x=1 \\mid y=0) p(y=0)$$\nUsing the notation provided in the problem statement:\n-   The disease prevalence (prior probability of disease) is $p(y=1) = \\pi$.\n-   The probability of a non-diseased state is $p(y=0) = 1 - \\pi$.\n-   The class-conditional probability of the symptom in diseased patients is $\\theta_1 = p(x=1 \\mid y=1)$.\n-   The class-conditional probability of the symptom in non-diseased patients is $\\theta_0 = p(x=1 \\mid y=0)$.\n\nSubstituting these into the formula for the posterior probability gives:\n$$p(y=1 \\mid x=1) = \\frac{\\theta_1 \\pi}{\\theta_1 \\pi + \\theta_0 (1-\\pi)}$$\nWe are given $\\pi = 0.005$, so $1-\\pi = 0.995$. The core of the problem lies in estimating the parameters $\\theta_1$ and $\\theta_0$ using the ML and MAP methods.\n\n**1. Maximum Likelihood (ML) Estimation**\n\nThe ML estimate for the parameter of a Bernoulli distribution (which models our binary symptom $x$) is simply the sample proportion of successes.\n-   For the diseased group ($y=1$), there were $k_1=1$ occurrences of the symptom ($x=1$) in $n_1=4$ patients. The ML estimate for $\\theta_1$ is:\n$$\\hat{\\theta}_{1, \\mathrm{ML}} = \\frac{k_1}{n_1} = \\frac{1}{4} = 0.25$$\n-   For the non-diseased group ($y=0$), there were $k_0=0$ occurrences of the symptom ($x=1$) in $n_0=6$ patients. The ML estimate for $\\theta_0$ is:\n$$\\hat{\\theta}_{0, \\mathrm{ML}} = \\frac{k_0}{n_0} = \\frac{0}{6} = 0$$\n\nNow, we compute the posterior probability $p_{\\mathrm{ML}}(y=1 \\mid x=1)$ using these ML estimates:\n$$p_{\\mathrm{ML}}(y=1 \\mid x=1) = \\frac{\\hat{\\theta}_{1, \\mathrm{ML}} \\pi}{\\hat{\\theta}_{1, \\mathrm{ML}} \\pi + \\hat{\\theta}_{0, \\mathrm{ML}} (1-\\pi)} = \\frac{0.25 \\times 0.005}{0.25 \\times 0.005 + 0 \\times 0.995}$$\n$$p_{\\mathrm{ML}}(y=1 \\mid x=1) = \\frac{0.00125}{0.00125 + 0} = 1$$\nThis result indicates that, under ML estimation, observing the symptom guarantees the presence of the disease. This is a direct consequence of the zero-frequency problem: since no non-diseased individuals in the small training set had the symptom, the model incorrectly learns that it's impossible for a non-diseased person to have it.\n\n**2. Maximum A Posteriori (MAP) Estimation**\n\nMAP estimation incorporates prior beliefs about the parameters, which helps to regularize the estimates and avoid issues like the zero-frequency problem. The problem specifies Beta priors for $\\theta_1$ and $\\theta_0$. The Beta distribution is the conjugate prior for the Bernoulli likelihood, which simplifies the calculation of the posterior distribution.\n\nFor a parameter $\\theta$ with a $\\mathrm{Beta}(\\alpha, \\beta)$ prior, after observing $k$ successes in $n$ trials, the posterior distribution is $\\mathrm{Beta}(k+\\alpha, n-k+\\beta)$. The MAP estimate is the mode of this posterior distribution. The mode of a $\\mathrm{Beta}(a,b)$ distribution (for $a, b > 1$) is given by $\\frac{a-1}{a+b-2}$.\n\n-   For $\\theta_1$, the prior is $\\mathrm{Beta}(\\alpha_1, \\beta_1)$ with $\\alpha_1=2, \\beta_1=2$. The data is $k_1=1, n_1=4$. The posterior distribution for $\\theta_1$ is:\n$$\\theta_1 \\mid \\text{data} \\sim \\mathrm{Beta}(k_1+\\alpha_1, n_1-k_1+\\beta_1) = \\mathrm{Beta}(1+2, 4-1+2) = \\mathrm{Beta}(3, 5)$$\nSince both parameters of the posterior distribution ($3$ and $5$) are greater than $1$, the MAP estimate is the mode:\n$$\\hat{\\theta}_{1, \\mathrm{MAP}} = \\frac{(k_1+\\alpha_1)-1}{(k_1+\\alpha_1) + (n_1-k_1+\\beta_1)-2} = \\frac{3-1}{3+5-2} = \\frac{2}{6} = \\frac{1}{3}$$\n-   For $\\theta_0$, the prior is $\\mathrm{Beta}(\\alpha_0, \\beta_0)$ with $\\alpha_0=2, \\beta_0=198$. The data is $k_0=0, n_0=6$. The posterior distribution for $\\theta_0$ is:\n$$\\theta_0 \\mid \\text{data} \\sim \\mathrm{Beta}(k_0+\\alpha_0, n_0-k_0+\\beta_0) = \\mathrm{Beta}(0+2, 6-0+198) = \\mathrm{Beta}(2, 204)$$\nThe parameters of the posterior ($2$ and $204$) are also greater than $1$. The MAP estimate is:\n$$\\hat{\\theta}_{0, \\mathrm{MAP}} = \\frac{(k_0+\\alpha_0)-1}{(k_0+\\alpha_0) + (n_0-k_0+\\beta_0)-2} = \\frac{2-1}{2+204-2} = \\frac{1}{204}$$\n\nNow we compute the posterior probability $p_{\\mathrm{MAP}}(y=1 \\mid x=1)$ using these MAP estimates. We use the exact fractional forms of the parameters, along with $\\pi = 0.005 = \\frac{1}{200}$ and $1-\\pi = 0.995 = \\frac{199}{200}$:\n$$p_{\\mathrm{MAP}}(y=1 \\mid x=1) = \\frac{\\hat{\\theta}_{1, \\mathrm{MAP}} \\pi}{\\hat{\\theta}_{1, \\mathrm{MAP}} \\pi + \\hat{\\theta}_{0, \\mathrm{MAP}} (1-\\pi)} = \\frac{\\frac{1}{3} \\times \\frac{1}{200}}{\\frac{1}{3} \\times \\frac{1}{200} + \\frac{1}{204} \\times \\frac{199}{200}}$$\nTo simplify this complex fraction, we can cancel the $\\frac{1}{200}$ term from the numerator and both terms in the denominator's sum:\n$$p_{\\mathrm{MAP}}(y=1 \\mid x=1) = \\frac{\\frac{1}{3}}{\\frac{1}{3} + \\frac{199}{204}} = \\frac{\\frac{1}{3}}{\\frac{1 \\times 68}{3 \\times 68} + \\frac{199}{204}} = \\frac{\\frac{1}{3}}{\\frac{68}{204} + \\frac{199}{204}} = \\frac{\\frac{1}{3}}{\\frac{267}{204}}$$\n$$p_{\\mathrm{MAP}}(y=1 \\mid x=1) = \\frac{1}{3} \\times \\frac{204}{267} = \\frac{68}{267}$$\n\n**3. Quantifying the Change**\n\nThe final step is to compute the difference $\\Delta$:\n$$\\Delta = p_{\\mathrm{MAP}}(y=1 \\mid x=1) - p_{\\mathrm{ML}}(y=1 \\mid x=1)$$\n$$\\Delta = \\frac{68}{267} - 1 = \\frac{68 - 267}{267} = -\\frac{199}{267}$$\nTo provide the numerical answer, we compute the decimal value and round to four significant figures:\n$$\\Delta = -\\frac{199}{267} \\approx -0.74531835...$$\nRounding to four significant figures yields $-0.7453$. This substantial negative difference highlights the strong regularizing effect of the MAP estimation, which pulled the posterior probability away from the extreme value of $1$ produced by the overconfident ML model.", "answer": "$$\\boxed{-0.7453}$$", "id": "4588350"}, {"introduction": "The power of the Naive Bayes classifier lies in its 'naive' assumption of conditional independence among features, but this is also its greatest potential weakness. This exercise delves into the practical consequences of violating this core assumption, a common occurrence when biomarkers are part of the same biological pathway. You will quantify the multiplicative bias introduced by the model's assumption, providing a clear understanding of how and why the classifier might misestimate a patient's disease risk [@problem_id:4588292].", "problem": "In a medical diagnostic context within bioinformatics and medical data analytics, consider a binary disease indicator $D \\in \\{0,1\\}$, where $D=1$ denotes presence of disease and $D=0$ denotes absence of disease. Two binary features, $X_{1}$ and $X_{2}$, are derived from gene expression thresholds (each feature equals $1$ when the corresponding biomarker is overexpressed and equals $0$ otherwise). Assume the following scientifically plausible data:\n- Disease prevalence: $P(D=1)=0.02$ and $P(D=0)=0.98$.\n- Under disease ($D=1$), the features are positively conditionally correlated with marginals $P(X_{1}=1 \\mid D=1)=0.70$ and $P(X_{2}=1 \\mid D=1)=0.60$, and joint $P(X_{1}=1, X_{2}=1 \\mid D=1)=0.55$. These values imply $P(X_{1}=1, X_{2}=0 \\mid D=1)=0.15$, $P(X_{1}=0, X_{2}=1 \\mid D=1)=0.05$, and $P(X_{1}=0, X_{2}=0 \\mid D=1)=0.25$, which are consistent with the marginals and demonstrate positive conditional dependence because $P(X_{1}=1, X_{2}=1 \\mid D=1)=0.55 > 0.70 \\times 0.60=0.42$.\n- Under no disease ($D=0$), the features are conditionally independent with $P(X_{1}=1 \\mid D=0)=0.10$, $P(X_{2}=1 \\mid D=0)=0.15$, so that $P(X_{1}=1, X_{2}=1 \\mid D=0)=0.10 \\times 0.15=0.015$.\n\nA Naive Bayes (NB) classifier models $X_{1}$ and $X_{2}$ as conditionally independent given $D$, so that for any $d \\in \\{0,1\\}$,\n$$\nP_{\\text{NB}}(X_{1}=x_{1}, X_{2}=x_{2} \\mid D=d) = P(X_{1}=x_{1} \\mid D=d)\\, P(X_{2}=x_{2} \\mid D=d).\n$$\nA correctly specified dependent model uses the true joint distribution $P(X_{1}, X_{2} \\mid D)$ without the independence assumption.\n\nStarting only from the definition of Bayes’ theorem and posterior odds, derive the multiplicative bias in posterior odds induced by the Naive Bayes independence assumption for a patient presenting with $X_{1}=1$ and $X_{2}=1$. Define the multiplicative bias as\n$$\nB \\equiv \\frac{O_{\\text{NB}}(D=1 \\mid X_{1}=1, X_{2}=1)}{O_{\\text{true}}(D=1 \\mid X_{1}=1, X_{2}=1)},\n$$\nwhere $O(\\cdot)$ denotes posterior odds. Compute $B$ using the data above and express your final result as a dimensionless decimal number. Round your answer to four significant figures. Do not use a percentage sign.", "solution": "The problem asks for the multiplicative bias in posterior odds induced by the Naive Bayes (NB) independence assumption for a patient presenting with features $X_{1}=1$ and $X_{2}=1$. The bias $B$ is defined as the ratio of the posterior odds calculated by the NB model to the true posterior odds. Let the evidence be denoted by $X = (X_{1}=1, X_{2}=1)$.\n\nFirst, we establish the general form of posterior odds. The posterior odds of having the disease ($D=1$) versus not having the disease ($D=0$) given the evidence $X$ are defined as:\n$$\nO(D=1 \\mid X) = \\frac{P(D=1 \\mid X)}{P(D=0 \\mid X)}\n$$\nBy applying Bayes' theorem, which states $P(D=d \\mid X) = \\frac{P(X \\mid D=d)P(D=d)}{P(X)}$, to both the numerator and the denominator, we get:\n$$\nO(D=1 \\mid X) = \\frac{\\frac{P(X \\mid D=1)P(D=1)}{P(X)}}{\\frac{P(X \\mid D=0)P(D=0)}{P(X)}}\n$$\nThe marginal probability of the evidence, $P(X)$, cancels out, yielding the expression for posterior odds as the product of the likelihood ratio and the prior odds:\n$$\nO(D=1 \\mid X) = \\left(\\frac{P(X \\mid D=1)}{P(X \\mid D=0)}\\right) \\left(\\frac{P(D=1)}{P(D=0)}\\right)\n$$\n\nThe multiplicative bias $B$ is defined as:\n$$\nB \\equiv \\frac{O_{\\text{NB}}(D=1 \\mid X)}{O_{\\text{true}}(D=1 \\mid X)}\n$$\nwhere $O_{\\text{NB}}$ are the odds calculated using the Naive Bayes assumption and $O_{\\text{true}}$ are the odds calculated using the true conditional probabilities.\n\nUsing the derived formula for posterior odds, we can write the true odds and the NB odds. Let $X$ represent the specific observation $(X_{1}=1, X_{2}=1)$.\nThe true posterior odds are:\n$$\nO_{\\text{true}}(D=1 \\mid X) = \\frac{P(X_{1}=1, X_{2}=1 \\mid D=1)}{P(X_{1}=1, X_{2}=1 \\mid D=0)} \\times \\frac{P(D=1)}{P(D=0)}\n$$\nThe Naive Bayes posterior odds are:\n$$\nO_{\\text{NB}}(D=1 \\mid X) = \\frac{P_{\\text{NB}}(X_{1}=1, X_{2}=1 \\mid D=1)}{P_{\\text{NB}}(X_{1}=1, X_{2}=1 \\mid D=0)} \\times \\frac{P(D=1)}{P(D=0)}\n$$\nSubstituting these expressions into the definition of the bias $B$:\n$$\nB = \\frac{\\frac{P_{\\text{NB}}(X_{1}=1, X_{2}=1 \\mid D=1)}{P_{\\text{NB}}(X_{1}=1, X_{2}=1 \\mid D=0)} \\times \\frac{P(D=1)}{P(D=0)}}{\\frac{P(X_{1}=1, X_{2}=1 \\mid D=1)}{P(X_{1}=1, X_{2}=1 \\mid D=0)} \\times \\frac{P(D=1)}{P(D=0)}}\n$$\nThe prior odds term, $\\frac{P(D=1)}{P(D=0)}$, cancels out, leaving a ratio of likelihood ratios:\n$$\nB = \\frac{P_{\\text{NB}}(X_{1}=1, X_{2}=1 \\mid D=1)}{P(X_{1}=1, X_{2}=1 \\mid D=1)} \\times \\frac{P(X_{1}=1, X_{2}=1 \\mid D=0)}{P_{\\text{NB}}(X_{1}=1, X_{2}=1 \\mid D=0)}\n$$\nNow we must calculate the four conditional probabilities in this expression using the provided data.\n\n1.  The true conditional probability for $D=1$: The problem explicitly gives the joint probability for the dependent features:\n    $$P(X_{1}=1, X_{2}=1 \\mid D=1) = 0.55$$\n\n2.  The Naive Bayes conditional probability for $D=1$: The NB model assumes conditional independence, using the given marginal probabilities:\n    $$P_{\\text{NB}}(X_{1}=1, X_{2}=1 \\mid D=1) = P(X_{1}=1 \\mid D=1) P(X_{2}=1 \\mid D=1) = 0.70 \\times 0.60 = 0.42$$\n\n3.  The true conditional probability for $D=0$: The problem states that for $D=0$, the features are conditionally independent. Therefore, the true joint probability is the product of the marginals:\n    $$P(X_{1}=1, X_{2}=1 \\mid D=0) = P(X_{1}=1 \\mid D=0) P(X_{2}=1 \\mid D=0) = 0.10 \\times 0.15 = 0.015$$\n\n4.  The Naive Bayes conditional probability for $D=0$: The NB model also assumes conditional independence:\n    $$P_{\\text{NB}}(X_{1}=1, X_{2}=1 \\mid D=0) = P(X_{1}=1 \\mid D=0) P(X_{2}=1 \\mid D=0) = 0.10 \\times 0.15 = 0.015$$\n\nSince the features are truly conditionally independent for the $D=0$ class, the NB assumption holds, and $P(X_{1}=1, X_{2}=1 \\mid D=0) = P_{\\text{NB}}(X_{1}=1, X_{2}=1 \\mid D=0)$. The ratio of these two terms is $1$.\n\nSubstituting these values into the expression for the bias $B$:\n$$\nB = \\frac{0.42}{0.55} \\times \\frac{0.015}{0.015} = \\frac{0.42}{0.55} \\times 1\n$$\n$$\nB = \\frac{0.42}{0.55}\n$$\nPerforming the division:\n$$\nB \\approx 0.76363636...\n$$\nRounding the result to four significant figures, as requested, we obtain:\n$$\nB \\approx 0.7636\n$$\nThis bias factor, being less than $1$, indicates that the Naive Bayes classifier underestimates the posterior odds of the disease for a patient with both biomarkers overexpressed, due to its failure to account for the positive conditional dependence between the features in the diseased population.", "answer": "$$\\boxed{0.7636}$$", "id": "4588292"}, {"introduction": "Real-world clinical datasets are rarely perfect; they often contain missing values or measurements that fall below an instrument's limit of detection (LOD). This advanced practice demonstrates how the probabilistic framework of a Naive Bayes classifier can be elegantly extended to handle such data imperfections. By modeling the likelihood for censored data and correctly handling missing information, you will learn to build more robust diagnostic models that make the most of all available evidence [@problem_id:4588331].", "problem": "A clinical laboratory uses a Naive Bayes classifier to estimate the probability of an acute infectious disease, denoted by the binary variable $D \\in \\{1,0\\}$, from two serum biomarkers $X$ and $Y$. The classifier assumes conditional independence of features given $D$ and a generative measurement model for each biomarker. For biomarker $X$, the instrument has a Limit of Detection (LOD), meaning that reported results are left-censored: if the measured value $M_X$ falls below an instrument threshold $L$, the report returns the categorical event “below LOD” rather than a numeric value. The measurement process for $X$ is modeled as the sum of an unobserved true concentration $X^{\\text{true}}$ and an independent measurement error $\\varepsilon_X$. Conditional on $D=d$, the true concentration $X^{\\text{true}}$ follows a Gaussian distribution with mean $\\mu_{Xd}$ and standard deviation $\\sigma_{Xd}$, and the independent measurement error $\\varepsilon_X$ follows a Gaussian distribution with mean $0$ and standard deviation $\\tau_X$. Biomarker $Y$ is missing for the patient, and the missingness mechanism is Missing Completely At Random (MCAR), independent of $D$ and all other variables.\n\nFor the disease class $D=1$ (present) and $D=0$ (absent), the generative parameters for biomarker $X$ are:\n- $D=1$: $\\mu_{X1} = 1.25$, $\\sigma_{X1} = 0.4$,\n- $D=0$: $\\mu_{X0} = 0.6$, $\\sigma_{X0} = 0.4$,\nand the measurement error for $X$ has $\\tau_X = 0.3$ in both classes. The Limit of Detection is $L = 0.5$. The prior probabilities are $P(D=1) = 0.12$ and $P(D=0) = 0.88$.\n\nFor a particular patient, the laboratory report for $X$ is “below LOD,” meaning $M_X < L$, and the biomarker $Y$ is missing. Using the Naive Bayes model with the assumptions described, compute the posterior probability $P(D=1 \\mid M_X < L, \\text{ $Y$ missing})$. Express your final answer as a decimal between $0$ and $1$, and round your result to four significant figures. No units are required.", "solution": "The objective is to compute the posterior probability of the disease being present, $D=1$, given the evidence from two biomarkers. The evidence, denoted as $E$, consists of the observation that the measured value for biomarker $X$, $M_X$, is below the Limit of Detection, $L$, and that the measurement for biomarker $Y$ is missing. We are thus asked to calculate $P(D=1 \\mid E)$, where $E = \\{M_X < L, \\text{ $Y$ missing}\\}$.\n\nWe begin with Bayes' theorem:\n$$\nP(D=1 \\mid E) = \\frac{P(E \\mid D=1) P(D=1)}{P(E)}\n$$\nThe denominator, $P(E)$, is the marginal likelihood of the evidence, which can be computed by marginalizing over the disease status $D$ using the law of total probability:\n$$\nP(E) = P(E \\mid D=1) P(D=1) + P(E \\mid D=0) P(D=0)\n$$\nSubstituting this into the Bayes' theorem expression gives:\n$$\nP(D=1 \\mid E) = \\frac{P(E \\mid D=1) P(D=1)}{P(E \\mid D=1) P(D=1) + P(E \\mid D=0) P(D=0)}\n$$\nThe problem states that the Naive Bayes classifier assumes conditional independence of the features (biomarkers $X$ and $Y$) given the disease state $D$. Therefore, the likelihood of the combined evidence $E$ can be factored:\n$$\nP(E \\mid D=d) = P(M_X < L, \\text{ $Y$ missing} \\mid D=d) = P(M_X < L \\mid D=d) \\cdot P(\\text{$Y$ missing} \\mid D=d)\n$$\nfor $d \\in \\{0, 1\\}$.\n\nThe problem also states that the missingness mechanism for biomarker $Y$ is Missing Completely At Random (MCAR) and is independent of $D$. This implies that the probability of $Y$ being missing is the same regardless of the disease state:\n$$\nP(\\text{$Y$ missing} \\mid D=1) = P(\\text{$Y$ missing} \\mid D=0) = c\n$$\nwhere $c$ is some constant probability.\n\nLet's substitute this into the expression for the posterior probability:\n$$\nP(D=1 \\mid E) = \\frac{P(M_X < L \\mid D=1) \\cdot c \\cdot P(D=1)}{P(M_X < L \\mid D=1) \\cdot c \\cdot P(D=1) + P(M_X < L \\mid D=0) \\cdot c \\cdot P(D=0)}\n$$\nThe constant factor $c$ appears in both the numerator and all terms of the denominator, and thus can be cancelled out. This demonstrates that under the MCAR assumption, the missing biomarker $Y$ provides no information for discriminating between $D=1$ and $D=0$, and the posterior probability is determined solely by the evidence from biomarker $X$. The problem simplifies to computing:\n$$\nP(D=1 \\mid M_X < L) = \\frac{P(M_X < L \\mid D=1) P(D=1)}{P(M_X < L \\mid D=1) P(D=1) + P(M_X < L \\mid D=0) P(D=0)}\n$$\nThe next step is to determine the likelihood terms, $P(M_X < L \\mid D=d)$. The measured value $M_X$ is modeled as the sum of the true concentration $X^{\\text{true}}$ and an independent measurement error $\\varepsilon_X$: $M_X = X^{\\text{true}} + \\varepsilon_X$.\nConditional on the disease state $D=d$, we are given the distributions:\n-   $X^{\\text{true}} \\mid (D=d) \\sim \\mathcal{N}(\\mu_{Xd}, \\sigma_{Xd}^2)$\n-   $\\varepsilon_X \\sim \\mathcal{N}(0, \\tau_X^2)$\n\nSince $X^{\\text{true}}$ and $\\varepsilon_X$ are independent Gaussian random variables, their sum $M_X$ is also a Gaussian random variable.\nThe mean of $M_X$ conditional on $D=d$ is the sum of the means:\n$$\nE[M_X \\mid D=d] = E[X^{\\text{true}} \\mid D=d] + E[\\varepsilon_X] = \\mu_{Xd} + 0 = \\mu_{Xd}\n$$\nThe variance of $M_X$ conditional on $D=d$ is the sum of the variances (due to independence):\n$$\n\\text{Var}(M_X \\mid D=d) = \\text{Var}(X^{\\text{true}} \\mid D=d) + \\text{Var}(\\varepsilon_X) = \\sigma_{Xd}^2 + \\tau_X^2\n$$\nLet $\\Sigma_{Xd}^2 = \\sigma_{Xd}^2 + \\tau_X^2$ be the variance of the measured value $M_X$ conditional on $D=d$. Thus, $M_X \\mid (D=d) \\sim \\mathcal{N}(\\mu_{Xd}, \\Sigma_{Xd}^2)$.\n\nWe can now calculate the likelihoods for $d=1$ and $d=0$.\nFor $D=1$:\n-   $\\mu_{X1} = 1.25$\n-   $\\sigma_{X1} = 0.4$\n-   $\\tau_X = 0.3$\nThe variance is $\\Sigma_{X1}^2 = \\sigma_{X1}^2 + \\tau_X^2 = (0.4)^2 + (0.3)^2 = 0.16 + 0.09 = 0.25$.\nThe standard deviation is $\\Sigma_{X1} = \\sqrt{0.25} = 0.5$.\nSo, $M_X \\mid (D=1) \\sim \\mathcal{N}(1.25, 0.5^2)$.\n\nFor $D=0$:\n-   $\\mu_{X0} = 0.6$\n-   $\\sigma_{X0} = 0.4$\n-   $\\tau_X = 0.3$\nThe variance is $\\Sigma_{X0}^2 = \\sigma_{X0}^2 + \\tau_X^2 = (0.4)^2 + (0.3)^2 = 0.16 + 0.09 = 0.25$.\nThe standard deviation is $\\Sigma_{X0} = \\sqrt{0.25} = 0.5$.\nSo, $M_X \\mid (D=0) \\sim \\mathcal{N}(0.6, 0.5^2)$.\n\nThe likelihood $P(M_X < L \\mid D=d)$ is the cumulative distribution function (CDF) of the corresponding Gaussian distribution, evaluated at $L=0.5$. Let $\\Phi(z)$ denote the CDF of the standard normal distribution $\\mathcal{N}(0, 1)$. By standardizing the variable, we get:\n$$\nP(M_X < L \\mid D=d) = \\Phi\\left(\\frac{L - \\mu_{Xd}}{\\Sigma_{Xd}}\\right)\n$$\nFor $D=1$:\n$$\nP(M_X < 0.5 \\mid D=1) = \\Phi\\left(\\frac{0.5 - 1.25}{0.5}\\right) = \\Phi\\left(\\frac{-0.75}{0.5}\\right) = \\Phi(-1.5)\n$$\nFor $D=0$:\n$$\nP(M_X < 0.5 \\mid D=0) = \\Phi\\left(\\frac{0.5 - 0.6}{0.5}\\right) = \\Phi\\left(\\frac{-0.1}{0.5}\\right) = \\Phi(-0.2)\n$$\nThe prior probabilities are given as $P(D=1) = 0.12$ and $P(D=0) = 0.88$.\nWe now substitute these components into the posterior probability formula:\n$$\nP(D=1 \\mid M_X < 0.5) = \\frac{\\Phi(-1.5) \\cdot P(D=1)}{\\Phi(-1.5) \\cdot P(D=1) + \\Phi(-0.2) \\cdot P(D=0)}\n$$\n$$\nP(D=1 \\mid M_X < 0.5) = \\frac{\\Phi(-1.5) \\cdot 0.12}{\\Phi(-1.5) \\cdot 0.12 + \\Phi(-0.2) \\cdot 0.88}\n$$\nUsing standard values for the normal CDF:\n-   $\\Phi(-1.5) \\approx 0.0668072$\n-   $\\Phi(-0.2) \\approx 0.4207403$\n\nNow, we compute the numerical result:\nNumerator: $0.0668072 \\times 0.12 = 0.008016864$\nDenominator term 1: $0.008016864$\nDenominator term 2: $0.4207403 \\times 0.88 = 0.370251464$\nDenominator total: $0.008016864 + 0.370251464 = 0.378268328$\nPosterior probability:\n$$\nP(D=1 \\mid M_X < 0.5) = \\frac{0.008016864}{0.378268328} \\approx 0.02119293\n$$\nThe problem requires the answer to be rounded to four significant figures. The value is $0.02119293...$. The first four significant figures are $2, 1, 1, 9$. The fifth significant figure is $2$, which is less than $5$, so we round down.\nThe final result is $0.02119$.", "answer": "$$\\boxed{0.02119}$$", "id": "4588331"}]}