{"hands_on_practices": [{"introduction": "When comparing patient profiles, such as laboratory results, we often represent them as vectors in a high-dimensional space. The first step in many analyses, like clustering or finding similar patients, is to define a notion of 'distance'. This exercise explores how two fundamental metrics, the Manhattan ($L_1$) and Euclidean ($L_2$) distances, behave differently in the presence of outliers—a common challenge in real-world medical data. [@problem_id:4558075]", "problem": "A research team models each patient’s laboratory profile as a vector in a feature space where each coordinate is a standardized score obtained by z-scoring that laboratory measure across a large reference cohort. Consider the following six features in the order: fasting plasma glucose (FPG), hemoglobin A1c (HbA1c), systolic blood pressure (SBP), low-density lipoprotein (LDL), C-reactive protein (CRP), and body mass index (BMI). Two patients are represented by standardized vectors:\n$$\nx_A = (0.5,\\; 0.1,\\; -0.6,\\; -0.2,\\; 0.3,\\; 0.4), \\quad\nx_B = (9.5,\\; 0.2,\\; 0.8,\\; 0.8,\\; -0.1,\\; 0.4).\n$$\nThe difference between the two patients is dominated by a single extreme fasting plasma glucose measurement in patient $B$, while the other features differ modestly.\n\nStarting from the definitions of the Manhattan metric (also called the $L_1$ metric) and the Euclidean metric (also called the $L_2$ metric) on $\\mathbb{R}^d$, and treating the laboratory vectors as points in $\\mathbb{R}^6$, compute the $L_1$ and $L_2$ distances between $x_A$ and $x_B$. Then, briefly justify which metric is more influenced by the single extreme coordinate in this setting and why, using only properties that follow from the definitions.\n\nProvide your final numerical distances as exact values without rounding, and report them in the order $L_1$ distance, then $L_2$ distance, as a pair. Do not include any units in your final answer.", "solution": "The context is a finite-dimensional real vector space where each patient is a point in $\\mathbb{R}^6$ consisting of z-scored laboratory values. The Manhattan metric ($L_1$) and the Euclidean metric ($L_2$) are induced by the corresponding norms on $\\mathbb{R}^d$, defined for a vector $v = (v_1,\\dots,v_d)$ by\n$$\n\\|v\\|_1 = \\sum_{i=1}^{d} |v_i|, \\quad\n\\|v\\|_2 = \\left(\\sum_{i=1}^{d} v_i^2\\right)^{1/2}.\n$$\nGiven two points $x, y \\in \\mathbb{R}^d$, the induced metric distances are $d_1(x,y) = \\|x-y\\|_1$ and $d_2(x,y) = \\|x-y\\|_2$.\n\nWe first compute the coordinate-wise difference vector $x_B - x_A$:\n- Fasting plasma glucose (FPG): $9.5 - 0.5 = 9.0$.\n- Hemoglobin A1c (HbA1c): $0.2 - 0.1 = 0.1$.\n- Systolic blood pressure (SBP): $0.8 - (-0.6) = 1.4$.\n- Low-density lipoprotein (LDL): $0.8 - (-0.2) = 1.0$.\n- C-reactive protein (CRP): $-0.1 - 0.3 = -0.4$.\n- Body mass index (BMI): $0.4 - 0.4 = 0.0$.\n\nThus,\n$$\nx_B - x_A = (9.0,\\; 0.1,\\; 1.4,\\; 1.0,\\; -0.4,\\; 0.0).\n$$\n\nCompute the $L_1$ distance:\n$$\nd_1(x_A, x_B) \\;=\\; |9.0| + |0.1| + |1.4| + |1.0| + |{-0.4}| + |0.0|\n= 9.0 + 0.1 + 1.4 + 1.0 + 0.4 + 0\n= 11.9.\n$$\nAs an exact rational number, $11.9 = \\frac{119}{10}$.\n\nCompute the $L_2$ distance by summing squared differences:\n$$\n\\begin{aligned}\nd_2(x_A, x_B)\n= \\left( (9.0)^2 + (0.1)^2 + (1.4)^2 + (1.0)^2 + (-0.4)^2 + (0.0)^2 \\right)^{1/2} \\\\\n= \\left( 81 + 0.01 + 1.96 + 1 + 0.16 + 0 \\right)^{1/2}\n= \\left( 84.13 \\right)^{1/2}.\n\\end{aligned}\n$$\nIn exact fractional form, $84.13 = \\frac{8413}{100}$, so\n$$\nd_2(x_A, x_B) = \\frac{\\sqrt{8413}}{10}.\n$$\n\nJustification of outlier influence based on definitions: In the $L_1$ distance, each coordinate contributes linearly via $|v_i|$, so a single large coordinate contributes in proportion to its magnitude. In the $L_2$ distance, each coordinate contributes quadratically via $v_i^2$ before taking a square root, which amplifies the relative influence of larger coordinates. In this example, the squared contribution of the extreme glucose coordinate is $81$, while the sum of squared contributions from all other coordinates is $0.01 + 1.96 + 1 + 0.16 + 0 = 3.13$. The fraction of the total squared sum attributable to glucose is therefore\n$$\n\\frac{81}{81 + 3.13} \\;=\\; \\frac{81}{84.13} \\;=\\; \\frac{8100}{8413},\n$$\nwhich is substantially larger than the fraction of the total $L_1$ distance attributable to glucose,\n$$\n\\frac{9.0}{11.9} \\;=\\; \\frac{90}{119}.\n$$\nThese ratios, derived solely from the metric definitions, show that the Euclidean ($L_2$) metric is more influenced by a single extreme coordinate than the Manhattan ($L_1$) metric in this setting.\n\nTherefore, the exact distances are $d_1(x_A,x_B) = \\frac{119}{10}$ and $d_2(x_A,x_B) = \\frac{\\sqrt{8413}}{10}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{119}{10}  \\frac{\\sqrt{8413}}{10}\\end{pmatrix}}$$", "id": "4558075"}, {"introduction": "Moving from numerical vectors to biological sequences like DNA or proteins requires a different conceptual toolkit for measuring similarity. Instead of geometric distance, we often think in terms of the 'effort' required to transform one sequence into another. This practice introduces the weighted Levenshtein distance, a powerful metric that quantifies this effort by assigning biologically-motivated costs to different types of edits, and requires the use of dynamic programming to find the optimal solution. [@problem_id:4558085]", "problem": "In Deoxyribonucleic Acid (DNA) sequence analysis, distances between strings over the nucleotide alphabet $\\Sigma = \\{\\text{A}, \\text{C}, \\text{G}, \\text{T}\\}$ are often defined by counting weighted edits required to transform one string into another. Begin from the core definition of a metric and the concept of an edit script composed of insertions, deletions, and substitutions with nonnegative costs. Using that foundation, formally define a weighted Levenshtein edit distance $d:\\Sigma^{\\ast} \\times \\Sigma^{\\ast} \\to \\mathbb{R}_{\\ge 0}$ for DNA that assigns a cost $c_{\\mathrm{ins}}$ to insertions, a cost $c_{\\mathrm{del}}$ to deletions, and a substitution cost $c_{\\mathrm{sub}}(a,b)$ that depends on whether the substitution is a transition or a transversion: a transition is a substitution between purines $\\{\\text{A}, \\text{G}\\}$ or between pyrimidines $\\{\\text{C}, \\text{T}\\}$, and a transversion is any other mismatch. Then, with the specific costs $c_{\\mathrm{ins}} = 2$, $c_{\\mathrm{del}} = 2$, $c_{\\mathrm{ts}} = 1$ for transitions, $c_{\\mathrm{tv}} = 3$ for transversions, and $c_{\\mathrm{sub}}(a,b) = 0$ if $a=b$, compute the distance between the two short gene sequences\n$$\nx = \\text{ACGTCGTA}, \\quad y = \\text{AGTTTGCA}.\n$$\nYour final answer must be a single real-valued number. No rounding is required.", "solution": "The problem requires the formal definition of a weighted Levenshtein distance for DNA sequences and the computation of this distance between two specific sequences, $x = \\text{ACGTCGTA}$ and $y = \\text{AGTTTGCA}$. The solution will proceed in two parts: first, a formal definition of the distance and its properties, and second, the explicit computation using a dynamic programming algorithm.\n\nA distance function $d$ on a set of objects is a metric if it satisfies four axioms for any objects $x, y, z$:\n1.  Non-negativity: $d(x,y) \\ge 0$.\n2.  Identity of indiscernibles: $d(x,y) = 0$ if and only if $x=y$.\n3.  Symmetry: $d(x,y) = d(y,x)$.\n4.  Triangle inequality: $d(x,z) \\le d(x,y) + d(y,z)$.\n\nA weighted Levenshtein distance is defined as the minimum cost to transform one string into another using a set of weighted edit operations: insertion, deletion, and substitution. Let $\\Sigma$ be our alphabet, which is $\\Sigma = \\{\\text{A}, \\text{C}, \\text{G}, \\text{T}\\}$. For any two strings $x, y \\in \\Sigma^{\\ast}$, the distance $d(x,y)$ is the minimum total cost of an edit script (a sequence of operations) that converts $x$ to $y$. The costs for the operations are given as:\n-   Insertion cost: $c_{\\mathrm{ins}} = 2$.\n-   Deletion cost: $c_{\\mathrm{del}} = 2$.\n-   Substitution cost $c_{\\mathrm{sub}}(a,b)$ for $a,b \\in \\Sigma$.\n\nThe substitution cost depends on whether the substitution is a transition or a transversion. A transition is a substitution within the same class of nucleotide: either between purines $\\{\\text{A}, \\text{G}\\}$ or between pyrimidines $\\{\\text{C}, \\text{T}\\}$. A transversion is a substitution between different classes (a purine for a pyrimidine or vice versa). The costs are:\n-   $c_{\\mathrm{sub}}(a,a) = 0$ (match).\n-   $c_{\\mathrm{sub}}(a,b) = c_{\\mathrm{ts}} = 1$ for a transition.\n-   $c_{\\mathrm{sub}}(a,b) = c_{\\mathrm{tv}} = 3$ for a transversion.\n\nThe given costs ($c_{\\mathrm{ins}} = c_{\\mathrm{del}} = 2  0$, and substitution costs $0, 1, 3 \\ge 0$) ensure non-negativity and the identity of indiscernibles. Symmetry is satisfied because $c_{\\mathrm{ins}} = c_{\\mathrm{del}}$ and the substitution cost matrix is symmetric, i.e., $c_{\\mathrm{sub}}(a,b) = c_{\\mathrm{sub}}(b,a)$. The triangle inequality also holds for this cost structure. Thus, the defined distance is a valid metric.\n\nThe substitution cost matrix $S(a,b) = c_{\\mathrm{sub}}(a,b)$ can be explicitly written as:\n$$\n\\begin{array}{c|cccc}\nc_{\\mathrm{sub}}  \\text{A}  \\text{C}  \\text{G}  \\text{T} \\\\\n\\hline\n\\text{A}  0  3  1  3 \\\\\n\\text{C}  3  0  3  1 \\\\\n\\text{G}  1  3  0  3 \\\\\n\\text{T}  3  1  3  0 \\\\\n\\end{array}\n$$\n\nTo compute the distance $d(x,y)$ for $x = \\text{ACGTCGTA}$ and $y = \\text{AGTTTGCA}$, we use the Wagner-Fischer algorithm. This dynamic programming approach constructs a matrix $D$ of size $(|x|+1) \\times (|y|+1)$, where $|x|=8$ and $|y|=8$. The entry $D_{i,j}$ stores the minimum cost to transform the prefix $x[1..i]$ to $y[1..j]$.\n\nThe matrix is initialized as follows:\n$D_{0,0} = 0$\n$D_{i,0} = i \\times c_{\\mathrm{del}} = 2i$ for $i \\in \\{1, \\dots, 8\\}$.\n$D_{0,j} = j \\times c_{\\mathrm{ins}} = 2j$ for $j \\in \\{1, \\dots, 8\\}$.\n\nThe remaining entries are filled using the recurrence relation:\n$$\nD_{i,j} = \\min \\begin{cases} D_{i-1,j} + c_{\\mathrm{del}} \\\\ D_{i,j-1} + c_{\\mathrm{ins}} \\\\ D_{i-1,j-1} + c_{\\mathrm{sub}}(x_i, y_j) \\end{cases}\n= \\min( D_{i-1,j} + 2, D_{i,j-1} + 2, D_{i-1,j-1} + c_{\\mathrm{sub}}(x_i, y_j) )\n$$\nThe calculation proceeds row by row. For example, to compute $D_{1,1}$:\n$x_1 = \\text{A}$, $y_1 = \\text{A}$. $c_{\\mathrm{sub}}(\\text{A},\\text{A}) = 0$.\n$D_{1,1} = \\min(D_{0,1}+2, D_{1,0}+2, D_{0,0}+0) = \\min(2+2, 2+2, 0+0) = 0$.\n\nTo compute $D_{1,2}$:\n$x_1 = \\text{A}$, $y_2 = \\text{G}$. $c_{\\mathrm{sub}}(\\text{A},\\text{G}) = 1$ (transition).\n$D_{1,2} = \\min(D_{0,2}+2, D_{1,1}+2, D_{0,1}+1) = \\min(4+2, 0+2, 2+1) = \\min(6, 2, 3) = 2$.\n\nBy completing this process for the entire matrix, we obtain the following table, where rows correspond to prefixes of $x$ (indexed by $i$) and columns to prefixes of $y$ (indexed by $j$):\n\n$$\n\\begin{array}{c|c|cccccccc}\n   j  0  1  2  3  4  5  6  7  8 \\\\\ni   \\text{ }  \\text{A}  \\text{G}  \\text{T}  \\text{T}  \\text{T}  \\text{G}  \\text{C}  \\text{A} \\\\\n\\hline\n0  \\text{ }  0  2  4  6  8  10  12  14  16 \\\\\n1  \\text{A}  2  0  2  4  6  8  10  12  14 \\\\\n2  \\text{C}  4  2  3  3  5  7  9  10  12 \\\\\n3  \\text{G}  6  4  2  4  6  8  7  9  11 \\\\\n4  \\text{T}  8  6  4  2  4  6  8  8  10 \\\\\n5  \\text{C}  10  8  6  4  3  5  7  8  10 \\\\\n6  \\text{G}  12  10  8  6  5  6  5  7  9 \\\\\n7  \\text{T}  14  12  10  8  6  5  7  6  8 \\\\\n8  \\text{A}  16  14  12  10  8  7  6  8  \\mathbf{6} \\\\\n\\end{array}\n$$\nThe distance $d(x,y)$ is the value in the bottom-right cell of the matrix, $D_{|x|,|y|} = D_{8,8}$.\nLet us verify the final entry, $D_{8,8}$, which corresponds to aligning $x_8=\\text{A}$ and $y_8=\\text{A}$. The substitution cost is $c_{\\mathrm{sub}}(\\text{A},\\text{A})=0$.\n$$\nD_{8,8} = \\min(D_{7,8}+2, D_{8,7}+2, D_{7,7}+0) = \\min(8+2, 8+2, 6+0) = \\min(10, 10, 6)=6.\n$$\nThe minimum cost to transform string $x$ into string $y$ is $6$.", "answer": "$$\\boxed{6}$$", "id": "4558085"}, {"introduction": "In fields like metabolomics or transcriptomics, we often care more about the relative pattern of feature changes than their absolute magnitudes, which can be skewed by technical artifacts known as batch effects. This exercise delves into the critical distinction between cosine similarity and the Pearson correlation coefficient, demonstrating how mean-centering data fundamentally changes the similarity measurement. By working through a hypothetical scenario, you will see firsthand how choosing the right metric is crucial for obtaining biologically meaningful results. [@problem_id:4558147]", "problem": "A metabolomics laboratory quantifies intensities for $m=5$ metabolites in three plasma samples and suspects that a batch-induced mean shift affects one sample. Let the reference sample be $S_{A}$ with profile vector $x_{A} = [2, 5, 7, 4, 6]$. Consider three comparison samples:\n- $S_{B}$ with $x_{B} = [4, 11, 13, 9, 11]$, representing a near multiplicative scaling of $S_{A}$ with small zero-mean perturbations that preserve biological proportionality.\n- $S_{C}$ with $x_{C} = [7, 10, 12, 9, 11]$, representing a constant mean shift likely due to a batch effect (each metabolite increased by approximately a constant from $S_{A}$).\n- $S_{D}$ with $x_{D} = [18, 15, 13, 16, 14]$, representing a mean-shifted inversion $x_{D} \\approx -x_{A} + c$ that flips biological pattern while lifting all intensities by a constant.\n\nUsing these vectors, evaluate similarity between $S_{A}$ and each of $S_{B}$, $S_{C}$, $S_{D}$ under two metrics fundamental to bioinformatics and medical data analytics:\n1. Cosine similarity computed on the raw vectors.\n2. Pearson correlation coefficient computed on mean-centered vectors.\n\nForm two rankings of $S_{B}$, $S_{C}$, and $S_{D}$ by ordering from highest to lowest similarity to $S_{A}$, once using cosine similarity and once using Pearson correlation. Then, to quantify the impact of the mean shift on ranking, compute the Kendall tau distance between these two rankings, defined as the number of discordant pairwise orderings among $\\{S_{B}, S_{C}, S_{D}\\}$ across the two metrics.\n\nProvide your final answer as a single integer. No units are required. If intermediate numerical values are approximated, retain at least four significant figures in your working.", "solution": "The problem requires the evaluation of similarity between a reference sample vector $x_{A}$ and three other sample vectors $x_{B}$, $x_{C}$, and $x_{D}$ using two different metrics: cosine similarity and Pearson correlation coefficient. Subsequently, two rankings of the samples based on these similarity scores are to be generated. Finally, the Kendall tau distance between these two rankings must be computed.\n\nThe given data vectors are:\n$x_{A} = [2, 5, 7, 4, 6]$\n$x_{B} = [4, 11, 13, 9, 11]$\n$x_{C} = [7, 10, 12, 9, 11]$\n$x_{D} = [18, 15, 13, 16, 14]$\n\nFirst, we compute the cosine similarity for each pair of vectors $(x_{A}, x_{B})$, $(x_{A}, x_{C})$, and $(x_{A}, x_{D})$. The cosine similarity between two vectors $u$ and $v$ is defined as $\\cos(\\theta) = \\frac{u \\cdot v}{\\|u\\| \\|v\\|}$.\n\nWe calculate the dot products and vector magnitudes:\nThe squared norm of $x_{A}$ is $\\|x_{A}\\|^2 = 2^2 + 5^2 + 7^2 + 4^2 + 6^2 = 4 + 25 + 49 + 16 + 36 = 130$.\nThe norm is $\\|x_{A}\\| = \\sqrt{130}$.\n\nFor $S_{B}$:\n$x_{A} \\cdot x_{B} = (2)(4) + (5)(11) + (7)(13) + (4)(9) + (6)(11) = 8 + 55 + 91 + 36 + 66 = 256$.\n$\\|x_{B}\\|^2 = 4^2 + 11^2 + 13^2 + 9^2 + 11^2 = 16 + 121 + 169 + 81 + 121 = 508$.\n$\\|x_{B}\\| = \\sqrt{508}$.\nThe cosine similarity between $x_{A}$ and $x_{B}$ is:\n$$ \\cos(x_{A}, x_{B}) = \\frac{256}{\\sqrt{130} \\sqrt{508}} = \\frac{256}{\\sqrt{66040}} \\approx 0.9961 $$\n\nFor $S_{C}$:\n$x_{A} \\cdot x_{C} = (2)(7) + (5)(10) + (7)(12) + (4)(9) + (6)(11) = 14 + 50 + 84 + 36 + 66 = 250$.\n$\\|x_{C}\\|^2 = 7^2 + 10^2 + 12^2 + 9^2 + 11^2 = 49 + 100 + 144 + 81 + 121 = 495$.\n$\\|x_{C}\\| = \\sqrt{495}$.\nThe cosine similarity between $x_{A}$ and $x_{C}$ is:\n$$ \\cos(x_{A}, x_{C}) = \\frac{250}{\\sqrt{130} \\sqrt{495}} = \\frac{250}{\\sqrt{64350}} \\approx 0.9855 $$\n\nFor $S_{D}$:\n$x_{A} \\cdot x_{D} = (2)(18) + (5)(15) + (7)(13) + (4)(16) + (6)(14) = 36 + 75 + 91 + 64 + 84 = 350$.\n$\\|x_{D}\\|^2 = 18^2 + 15^2 + 13^2 + 16^2 + 14^2 = 324 + 225 + 169 + 256 + 196 = 1170$.\n$\\|x_{D}\\| = \\sqrt{1170}$.\nThe cosine similarity between $x_{A}$ and $x_{D}$ is:\n$$ \\cos(x_{A}, x_{D}) = \\frac{350}{\\sqrt{130} \\sqrt{1170}} = \\frac{350}{\\sqrt{152100}} = \\frac{350}{390} = \\frac{35}{39} \\approx 0.8974 $$\n\nOrdering the samples by decreasing cosine similarity to $S_{A}$: $S_{B}$ ($0.9961$)  $S_{C}$ ($0.9855$)  $S_{D}$ ($0.8974$).\nThe first ranking, $R_{1}$, is $(S_{B}, S_{C}, S_{D})$.\n\nNext, we compute the Pearson correlation coefficient, which is the cosine similarity of the mean-centered vectors. For a vector $u$ with mean $\\bar{u}$, the mean-centered vector is $u' = u - \\bar{u}$. The Pearson correlation is $\\rho(u,v) = \\frac{(u - \\bar{u}) \\cdot (v - \\bar{v})}{\\|u - \\bar{u}\\| \\|v - \\bar{v}\\|}$.\n\nFirst, we compute the means of the vectors:\n$\\bar{x}_{A} = \\frac{2+5+7+4+6}{5} = \\frac{24}{5} = 4.8$.\n$\\bar{x}_{B} = \\frac{4+11+13+9+11}{5} = \\frac{48}{5} = 9.6$.\n$\\bar{x}_{C} = \\frac{7+10+12+9+11}{5} = \\frac{49}{5} = 9.8$.\n$\\bar{x}_{D} = \\frac{18+15+13+16+14}{5} = \\frac{76}{5} = 15.2$.\n\nNow, we create the mean-centered vectors:\n$x'_{A} = [2-4.8, 5-4.8, 7-4.8, 4-4.8, 6-4.8] = [-2.8, 0.2, 2.2, -0.8, 1.2]$.\n$x'_{B} = [4-9.6, 11-9.6, 13-9.6, 9-9.6, 11-9.6] = [-5.6, 1.4, 3.4, -0.6, 1.4]$.\n$x'_{C} = [7-9.8, 10-9.8, 12-9.8, 9-9.8, 11-9.8] = [-2.8, 0.2, 2.2, -0.8, 1.2]$.\n$x'_{D} = [18-15.2, 15-15.2, 13-15.2, 16-15.2, 14-15.2] = [2.8, -0.2, -2.2, 0.8, -1.2]$.\n\nWe observe that $x'_{C} = x'_{A}$ and $x'_{D} = -x'_{A}$. This directly allows us to determine the Pearson correlations for $S_{C}$ and $S_{D}$ with $S_{A}$.\nFor $S_{C}$: Since the mean-centered vectors are identical, their correlation is perfect.\n$$ \\rho(x_{A}, x_{C}) = 1 $$\nFor $S_{D}$: Since the mean-centered vectors are perfectly anti-parallel ($x'_{D} = -1 \\cdot x'_{A}$), their correlation is perfectly negative.\n$$ \\rho(x_{A}, x_{D}) = -1 $$\n\nFor $S_{B}$, we compute the correlation with $S_{A}$:\n$x'_{A} \\cdot x'_{B} = (-2.8)(-5.6) + (0.2)(1.4) + (2.2)(3.4) + (-0.8)(-0.6) + (1.2)(1.4) = 15.68 + 0.28 + 7.48 + 0.48 + 1.68 = 25.6$.\n$\\|x'_{A}\\|^2 = (-2.8)^2 + (0.2)^2 + (2.2)^2 + (-0.8)^2 + (1.2)^2 = 7.84 + 0.04 + 4.84 + 0.64 + 1.44 = 14.8$.\n$\\|x'_{B}\\|^2 = (-5.6)^2 + (1.4)^2 + (3.4)^2 + (-0.6)^2 + (1.4)^2 = 31.36 + 1.96 + 11.56 + 0.36 + 1.96 = 47.2$.\nThe Pearson correlation between $x_{A}$ and $x_{B}$ is:\n$$ \\rho(x_{A}, x_{B}) = \\frac{x'_{A} \\cdot x'_{B}}{\\|x'_{A}\\| \\|x'_{B}\\|} = \\frac{25.6}{\\sqrt{14.8} \\sqrt{47.2}} = \\frac{25.6}{\\sqrt{698.56}} \\approx 0.9686 $$\n\nOrdering the samples by decreasing Pearson correlation to $S_{A}$: $S_{C}$ ($1.0$)  $S_{B}$ ($0.9686$)  $S_{D}$ ($-1.0$).\nThe second ranking, $R_{2}$, is $(S_{C}, S_{B}, S_{D})$.\n\nFinally, we compute the Kendall tau distance between the two rankings, $R_{1} = (S_{B}, S_{C}, S_{D})$ and $R_{2} = (S_{C}, S_{B}, S_{D})$. The Kendall tau distance is the number of pairs of items that have a different relative ordering in the two rankings. The set of items is $\\{S_{B}, S_{C}, S_{D}\\}$. There are $\\binom{3}{2} = 3$ pairs to check: $\\{S_{B}, S_{C}\\}$, $\\{S_{B}, S_{D}\\}$, and $\\{S_{C}, S_{D}\\}$.\n\n1.  Pair $\\{S_{B}, S_{C}\\}$:\n    In $R_{1}$, $S_{B}$ comes before $S_{C}$. Order: $S_{B} \\succ S_{C}$.\n    In $R_{2}$, $S_{C}$ comes before $S_{B}$. Order: $S_{C} \\succ S_{B}$.\n    The orders are different, so this is a discordant pair.\n\n2.  Pair $\\{S_{B}, S_{D}\\}$:\n    In $R_{1}$, $S_{B}$ comes before $S_{D}$. Order: $S_{B} \\succ S_{D}$.\n    In $R_{2}$, $S_{B}$ comes before $S_{D}$. Order: $S_{B} \\succ S_{D}$.\n    The orders are the same, so this is a concordant pair.\n\n3.  Pair $\\{S_{C}, S_{D}\\}$:\n    In $R_{1}$, $S_{C}$ comes before $S_{D}$. Order: $S_{C} \\succ S_{D}$.\n    In $R_{2}$, $S_{C}$ comes before $S_{D}$. Order: $S_{C} \\succ S_{D}$.\n    The orders are the same, so this is a concordant pair.\n\nThere is exactly one discordant pair. Therefore, the Kendall tau distance between the two rankings is $1$.\nThis result illustrates the differing sensitivities of cosine similarity and Pearson correlation. Cosine similarity is sensitive to differences in vector magnitudes and mean offsets, while Pearson correlation is invariant to linear transformations (scaling and shifting), making it sensitive only to the shape of the data profile. The constant additive shift in $S_{C}$ heavily penalizes its cosine similarity but results in a perfect Pearson correlation, causing the rank switch with $S_{B}$.", "answer": "$$\\boxed{1}$$", "id": "4558147"}]}