{"hands_on_practices": [{"introduction": "Understanding the performance characteristics of fundamental algorithms is a cornerstone of computational science. This exercise explores the classic Quicksort algorithm, contrasting its vulnerability to adversarial inputs with the robustness achieved through randomized pivot selection [@problem_id:4538777]. By deriving the exact comparison counts for both worst-case and average-case scenarios, you will gain a deeper appreciation for why randomization is a powerful tool in algorithm design.", "problem": "You are given a collection of $n$ distinct sequencing read identifiers $\\mathcal{R} = (r_{1}, r_{2}, \\ldots, r_{n})$, where each identifier is a unique string. Define a key function $\\kappa$ that maps each read identifier to its lexicographic rank in $\\{1, 2, \\ldots, n\\}$, so that $\\kappa$ induces a strict total order and there are no ties. Consider an in-memory quicksort implementation $Q$ that, on each recursive call, chooses as pivot the first element of the current subarray and partitions by comparing this pivot to every other element in the subarray (counting one key comparison per pairwise ordering check). The input order given to $Q$ is fully under the control of an adversary. The running time is measured by the total number of key comparisons performed during the sort.\n\nTasks:\n- Construct an explicit input order of $\\mathcal{R}$ that is adversarial for $Q$ in the sense that each partition step produces subproblems of sizes $0$ and $n-1$. Using only the definition of $Q$ and the comparison model, derive a closed-form, exact expression for the total number of comparisons $C_{\\mathrm{det}}(n)$ performed by $Q$ on that input as a function of $n$.\n- Now consider a randomized variant $Q_{\\mathrm{rand}}$ that, on each recursive call, selects its pivot uniformly at random from the current subarray, independently of input order and independently across calls. Under the same cost model and assuming all $\\kappa(r)$ are distinct, derive an exact, closed-form expression for the expected total number of comparisons $\\mathbb{E}[C_{\\mathrm{rand}}(n)]$ in terms of $n$ and the $n$-th harmonic number $H_{n} = \\sum_{j=1}^{n} \\frac{1}{j}$. Justify from first principles why this expression implies that randomized pivot selection restores expected $O(n \\log n)$ complexity.\n\nProvide your final answer as a row matrix containing two entries $\\big(C_{\\mathrm{det}}(n), \\mathbb{E}[C_{\\mathrm{rand}}(n)]\\big)$ in that order. Do not round and do not include any units.", "solution": "The problem statement has been validated and is deemed to be scientifically grounded, well-posed, and objective. It presents a standard, formal problem in the analysis of algorithms.\n\nWe will address the two tasks in order. Let the set of $n$ distinct sequencing read identifiers be $\\mathcal{R} = \\{r_1, r_2, \\ldots, r_n\\}$. The key function $\\kappa$ maps these identifiers to their lexicographic ranks in $\\{1, 2, \\ldots, n\\}$. For the purpose of analysis, we can represent the items to be sorted by their ranks, which we denote as the set of sorted elements $z_1  z_2  \\ldots  z_n$, where $z_i = \\kappa(r_j)$ for some $j$ and $z_i$ is the $i$-th smallest key.\n\n**Part 1: Adversarial Analysis of Deterministic Quicksort ($Q$)**\n\nThe deterministic quicksort algorithm $Q$ selects the first element of a subarray as its pivot. The cost is measured by the number of key comparisons. A partition of a subarray of size $k$ involves comparing the pivot to the other $k-1$ elements, resulting in $k-1$ comparisons. An adversarial input is one that forces each partition step to produce subproblems of sizes $0$ and $n-1$ (or, more generally, $0$ and $k-1$ for a subarray of size $k$).\n\nTo achieve such a split, the pivot chosen must always be the minimum or maximum element in the current subarray. Since the pivot is always the first element, the adversary must construct an input list where the first element of every subarray processed by $Q$ is an extremal element.\n\nAn explicit input order that guarantees this is an already sorted list. Let the input to $Q$ be the sequence $(z_1, z_2, \\ldots, z_n)$.\n\\begin{enumerate}\n    \\item The initial call is $Q((z_1, z_2, \\ldots, z_n))$. The pivot is $z_1$. It is compared with the other $n-1$ elements. Since $z_1$ is the minimum element, the partition results in a subproblem of size $0$ (for elements less than $z_1$) and a subproblem of size $n-1$ containing $(z_2, \\ldots, z_n)$. This step costs $n-1$ comparisons.\n    \\item The next recursive call is $Q((z_2, \\ldots, z_n))$. The pivot is $z_2$. It is compared with the other $n-2$ elements in this subarray. Since $z_2$ is the minimum element in this subarray, the partition results in subproblems of size $0$ and $n-2$. This step costs $n-2$ comparisons.\n    \\item This process continues. For a general step on a subarray of size $k \\ge 2$, the input is $(z_{n-k+1}, \\ldots, z_n)$. The pivot is $z_{n-k+1}$, which is the minimum. This requires $k-1$ comparisons and generates subproblems of size $0$ and $k-1$.\n\\end{enumerate}\nA reverse-sorted list $(z_n, z_{n-1}, \\ldots, z_1)$ is also an adversarial input, as the pivot is always the maximum element.\n\nLet $C_{\\mathrm{det}}(n)$ be the total number of comparisons on this adversarial input of size $n$. The sequence of subarray sizes processed is $n, n-1, n-2, \\ldots, 2$. A subarray of size $1$ is a base case and requires $0$ comparisons. The total number of comparisons is the sum of comparisons at each step:\n$$C_{\\mathrm{det}}(n) = (n-1) + (n-2) + \\dots + (2-1)$$\nThis is the sum of the first $n-1$ positive integers:\n$$C_{\\mathrm{det}}(n) = \\sum_{k=1}^{n-1} k = \\frac{(n-1)n}{2}$$\nThis is a quadratic function of $n$, demonstrating the $O(n^2)$ worst-case time complexity of this deterministic quicksort implementation.\n\n**Part 2: Average-Case Analysis of Randomized Quicksort ($Q_{\\mathrm{rand}}$)**\n\nThe randomized variant $Q_{\\mathrm{rand}}$ selects a pivot uniformly at random from the current subarray. To find the expected number of comparisons, $\\mathbb{E}[C_{\\mathrm{rand}}(n)]$, we use a method based on indicator random variables.\n\nLet the sorted elements be $z_1  z_2  \\ldots  z_n$. The total number of comparisons $C_{\\mathrm{rand}}(n)$ is the sum of comparisons over all pairs of distinct elements. Let $I_{ij}$ be an indicator random variable for the event that $z_i$ and $z_j$ are compared. By definition, $I_{ij}=1$ if they are compared, and $I_{ij}=0$ otherwise. We are interested in pairs where $i  j$.\nThe total number of comparisons is $C_{\\mathrm{rand}}(n) = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} I_{ij}$.\nBy linearity of expectation:\n$$\\mathbb{E}[C_{\\mathrm{rand}}(n)] = \\mathbb{E}\\left[\\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} I_{ij}\\right] = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} \\mathbb{E}[I_{ij}]$$\nThe expectation of an indicator variable is the probability of the event it indicates: $\\mathbb{E}[I_{ij}] = P(z_i \\text{ is compared with } z_j)$.\n\nConsider the set of elements $S_{ij} = \\{z_i, z_{i+1}, \\ldots, z_j\\}$. Two elements $z_i$ and $z_j$ are compared if and only if the first pivot chosen from the set $S_{ij}$ is either $z_i$ or $z_j$.\nIf any other element $z_k$ with $i  k  j$ is chosen as a pivot first, $z_i$ and $z_j$ will be separated into different partitions (one with elements $ z_k$, one with elements $ z_k$) and will never be compared. If an element outside the range $[z_i, z_j]$ is chosen as a pivot, both $z_i$ and $z_j$ will end up in the same partition, and the decision of whether they will be compared is deferred. Thus, the comparison of $z_i$ and $z_j$ depends only on the first pivot selected from within $S_{ij}$.\n\nThe size of the set $S_{ij}$ is $j-i+1$. Since the pivot is chosen uniformly at random from any subarray, any element in $S_{ij}$ is equally likely to be the first pivot chosen from this set. The probability of any specific element $z_k \\in S_{ij}$ being the first chosen is $\\frac{1}{j-i+1}$.\nThe event \"$z_i$ is compared with $z_j$\" occurs if $z_i$ is the first pivot or if $z_j$ is the first pivot from $S_{ij}$. These are mutually exclusive events.\n$$P(z_i \\text{ is compared with } z_j) = P(z_i \\text{ is first pivot from } S_{ij}) + P(z_j \\text{ is first pivot from } S_{ij}) = \\frac{1}{j-i+1} + \\frac{1}{j-i+1} = \\frac{2}{j-i+1}$$\nNow we substitute this probability back into the summation for the expected total comparisons:\n$$\\mathbb{E}[C_{\\mathrm{rand}}(n)] = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} \\frac{2}{j-i+1}$$\nTo evaluate this sum, we can re-index by setting $k = j-i$. The inner sum becomes:\n$$\\mathbb{E}[C_{\\mathrm{rand}}(n)] = \\sum_{i=1}^{n-1} \\sum_{k=1}^{n-i} \\frac{2}{k+1}$$\nThis is still somewhat complex. A more direct approach is to change the order of summation by fixing the distance between elements. Let $d = j-i+1$ be the size of the set $S_{ij}$. The minimum value for $d$ is $2$ (for adjacent elements) and the maximum is $n$ (for $z_1, z_n$). For a fixed $d \\in \\{2, \\ldots, n\\}$, we count the number of pairs $(i, j)$ such that $j-i+1 = d$, or $j-i = d-1$. The pairs are $(1, d), (2, d+1), \\ldots, (n-d+1, n)$. There are $n-d+1$ such pairs.\nSo, we can rewrite the sum over $d$:\n$$\\mathbb{E}[C_{\\mathrm{rand}}(n)] = \\sum_{d=2}^{n} (\\text{number of pairs with size } d) \\times \\frac{2}{d} = \\sum_{d=2}^{n} (n-d+1) \\frac{2}{d}$$\n$$\\mathbb{E}[C_{\\mathrm{rand}}(n)] = 2 \\sum_{d=2}^{n} \\frac{n+1-d}{d} = 2 \\sum_{d=2}^{n} \\left(\\frac{n+1}{d} - 1\\right)$$\n$$= 2 \\left( (n+1)\\sum_{d=2}^{n} \\frac{1}{d} - \\sum_{d=2}^{n} 1 \\right)$$\nUsing the definition of the $n$-th harmonic number, $H_n = \\sum_{j=1}^{n}\\frac{1}{j}$, we have $\\sum_{d=2}^{n} \\frac{1}{d} = H_n-1$. The second sum is over $n-1$ terms.\n$$\\mathbb{E}[C_{\\mathrm{rand}}(n)] = 2 \\left( (n+1)(H_n - 1) - (n-1) \\right)$$\n$$= 2 ( (n+1)H_n - (n+1) - (n-1) ) = 2 ( (n+1)H_n - 2n )$$\n$$\\mathbb{E}[C_{\\mathrm{rand}}(n)] = 2(n+1)H_n - 4n$$\n\nFinally, we justify why this expression implies an expected complexity of $O(n \\log n)$. The harmonic number $H_n$ has the well-known asymptotic behavior:\n$$H_n = \\ln(n) + \\gamma + O\\left(\\frac{1}{n}\\right)$$\nwhere $\\gamma \\approx 0.577$ is the Euler-Mascheroni constant. Thus, $H_n$ grows as $\\Theta(\\ln n)$.\nSubstituting this into our expression for $\\mathbb{E}[C_{\\mathrm{rand}}(n)]$:\n$$\\mathbb{E}[C_{\\mathrm{rand}}(n)] = 2(n+1)(\\ln(n) + \\gamma + O(n^{-1})) - 4n$$\n$$= 2n\\ln(n) + 2n\\gamma + O(1) + 2\\ln(n) + 2\\gamma + O(n^{-1}) - 4n$$\n$$= 2n\\ln(n) + n(2\\gamma - 4) + 2\\ln(n) + O(1)$$\nThe dominant term in this expression is $2n\\ln(n)$. Since $\\ln(n)$ and $\\log(n)$ differ only by a constant factor $(\\ln(n) = \\ln(2) \\log_2(n))$, the asymptotic complexity is:\n$$\\mathbb{E}[C_{\\mathrm{rand}}(n)] = \\Theta(n \\log n)$$\nRandomized pivot selection thus mitigates the possibility of a consistently bad pivot choice, reducing the expected number of comparisons from the worst-case $\\Theta(n^2)$ down to an average-case $\\Theta(n \\log n)$, which is optimal for comparison-based sorting.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{n(n-1)}{2}  2(n+1)H_{n} - 4n \\end{pmatrix}}$$", "id": "4538777"}, {"introduction": "In bioinformatics, hash tables are indispensable for tasks like k-mer counting in large-scale genomic datasets. This practice problem delves into the theoretical underpinnings of their efficiency, asking you to derive the famous $O(1)$ expected lookup time under the Simple Uniform Hashing Assumption [@problem_id:4538794]. Analyzing both the expected cost and its variance provides crucial insights into the performance and predictability of hash-based data structures at scale.", "problem": "In a large-scale metagenomic variant surveillance pipeline for a hospital network, counts for $n$ distinct $k$-mers are stored in a hash table with $m$ buckets using separate chaining (linked lists per bucket). Assume the Simple Uniform Hashing Assumption (SUHA), that is, each $k$-mer hashes independently and uniformly at random to any of the $m$ buckets. Let the load factor be $\\alpha = n/m$. The lookup cost is defined as the number of key comparisons performed against stored $k$-mers in the table during a query. Consider two natural query regimes: an unsuccessful lookup where the queried $k$-mer is not in the table, and a successful lookup where the queried key is a uniformly random one from the $n$ stored $k$-mers.\n\nStarting from the fundamental definitions of SUHA and $\\alpha$, and using well-tested facts about occupancy distributions (binomial modeling of bucket sizes and the Poisson approximation valid for large $n$ and $m$ with fixed $\\alpha$), do the following:\n\n1. Derive the expected lookup cost in terms of $\\alpha$ for each regime and explain why the expected lookup time is $O(1)$ as the problem scales (that is, as $n$ and $m$ grow while keeping $\\alpha$ bounded).\n2. Under the Poisson approximation appropriate for large-scale $k$-mer tables in bioinformatics, compute the variance of the lookup cost for the successful lookup regime as a function of $\\alpha$ only. Express your final result as a single closed-form symbolic expression in terms of $\\alpha$.\n\nYour final answer must be a single closed-form expression for the variance as a function of $\\alpha$ only, with no units.", "solution": "The problem is well-posed, scientifically grounded, objective, and contains all necessary information for a complete analytical solution. It is a standard problem in the analysis of algorithms, applied to a realistic bioinformatics context.\n\n### Part 1: Expected Lookup Costs\n\nThe problem defines a hash table with $m$ buckets and $n$ distinct keys, using separate chaining. The load factor is $\\alpha = n/m$. We assume Simple Uniform Hashing Assumption (SUHA), which states that each key is hashed to one of the $m$ buckets with equal probability $1/m$, independently of all other keys. The lookup cost is the number of key comparisons.\n\n**Unsuccessful Lookup**\n\nFor an unsuccessful lookup of a key $x$, the algorithm first computes the hash $h(x)$ to identify a bucket, say bucket $j$. It must then traverse the entire linked list at bucket $j$ to confirm that $x$ is not present. The cost is therefore equal to the number of keys in bucket $j$, which we denote by the random variable $N_j$. We need to find the expected cost, $E[N_j]$.\n\nLet $X_i$ be an indicator random variable for the event that the $i$-th key (for $i \\in \\{1, 2, \\dots, n\\}$) hashes to bucket $j$. Under SUHA, the probability of this event is $P(\\text{hash}(k_i) = j) = 1/m$. Therefore, $E[X_i] = 1 \\cdot P(X_i=1) + 0 \\cdot P(X_i=0) = 1/m$.\n\nThe total number of keys in bucket $j$ is the sum of these indicator variables over all $n$ keys: $N_j = \\sum_{i=1}^{n} X_i$.\nBy linearity of expectation, the expected length of the list is:\n$$E[N_j] = E\\left[\\sum_{i=1}^{n} X_i\\right] = \\sum_{i=1}^{n} E[X_i] = \\sum_{i=1}^{n} \\frac{1}{m} = \\frac{n}{m}$$\nGiven the definition of the load factor $\\alpha = n/m$, the expected cost for an unsuccessful lookup, $C_{unsucc}$, is:\n$$E[C_{unsucc}] = \\alpha$$\n\n**Successful Lookup**\n\nFor a successful lookup, we query a key $x$ that is guaranteed to be in the table. The problem specifies that $x$ is chosen uniformly at random from the $n$ stored keys. The cost of the lookup is $1$ (for the comparison with $x$ itself) plus the number of other keys that appear before $x$ in its linked list.\n\nLet's consider the total cost of successful lookups for all $n$ keys and then find the average. Let the keys be $k_1, k_2, \\dots, k_n$ in the order they were inserted into the table. Assume new keys are added to the end of the list in their respective buckets. The cost to find key $k_i$ is $1$ plus the number of previously inserted keys ($k_j$ with $j  i$) that hash to the same bucket as $k_i$.\nLet $I_{ij}$ be the indicator variable for the event that $\\text{hash}(k_i) = \\text{hash}(k_j)$. Under SUHA, $E[I_{ij}] = P(\\text{hash}(k_i) = \\text{hash}(k_j)) = 1/m$ for $i \\neq j$.\n\nThe expected cost to find a specific key $k_i$ is $E[1 + \\sum_{j=1}^{i-1} I_{ij}] = 1 + \\sum_{j=1}^{i-1} E[I_{ij}] = 1 + \\frac{i-1}{m}$.\nThe average cost for a successful lookup, $C_{succ}$, is the average of these costs over all $n$ keys:\n$$E[C_{succ}] = \\frac{1}{n} \\sum_{i=1}^{n} \\left(1 + \\frac{i-1}{m}\\right) = \\frac{1}{n} \\left(\\sum_{i=1}^{n} 1 + \\frac{1}{m} \\sum_{i=1}^{n} (i-1)\\right)$$\n$$E[C_{succ}] = \\frac{1}{n} \\left(n + \\frac{1}{m} \\frac{(n-1)n}{2}\\right) = 1 + \\frac{n-1}{2m}$$\nSubstituting $\\alpha = n/m$, we can write this as:\n$$E[C_{succ}] = 1 + \\frac{n}{2m} - \\frac{1}{2m} = 1 + \\frac{\\alpha}{2} - \\frac{1}{2m}$$\nIn the large-scale limit where $n, m \\to \\infty$, the term $1/(2m)$ approaches $0$, yielding the well-known result:\n$$E[C_{succ}] \\approx 1 + \\frac{\\alpha}{2}$$\n\n**Asymptotic Complexity $O(1)$**\n\nThe problem scales by letting $n$ and $m$ grow while keeping the load factor $\\alpha = n/m$ bounded by a constant. The expected lookup costs are $E[C_{unsucc}] = \\alpha$ and $E[C_{succ}] \\approx 1 + \\alpha/2$. Since $\\alpha$ is a constant, both expected costs are constant. Assuming the hash function itself can be computed in constant time, the total expected time for both successful and unsuccessful lookups is $O(1)$. This is a key advantage of hash tables.\n\n### Part 2: Variance of Successful Lookup Cost\n\nWe are asked to compute the variance of the successful lookup cost, $\\text{Var}(C_{succ})$, under the Poisson approximation. This approximation is valid for large $n$ and $m$ with a fixed ratio $\\alpha = n/m$. It states that the number of keys in any given bucket follows a Poisson distribution with mean $\\alpha$.\n\nFor a successful search, we select a key uniformly at random. Let this key be $x$. The number of *other* keys that hash to the same bucket as $x$ follows a binomial distribution $B(n-1, 1/m)$. For large $n$ and small $1/m$, this is well-approximated by a Poisson distribution with mean $\\lambda = (n-1)/m \\approx n/m = \\alpha$. Let $K$ be the random variable for the number of other keys in the bucket of our chosen key $x$. Thus, $K \\sim \\text{Pois}(\\alpha)$. The total number of keys in this bucket is $L = K+1$.\n\nThe cost $C$ of the successful lookup depends on the position of $x$ in the list of length $L$. Since $x$ is chosen uniformly at random from all $n$ keys, its position within its own list is uniformly distributed from $1$ to $L$.\nLet $C$ be the random variable for the cost. We will use the Law of Total Variance:\n$$\\text{Var}(C) = E[\\text{Var}(C|L)] + \\text{Var}(E[C|L])$$\n\nFirst, we need the conditional expectation and variance of $C$ given $L$. For a discrete uniform random variable on $\\{1, 2, \\dots, L\\}$, the expectation and variance are:\n$$E[C|L] = \\frac{L+1}{2}$$\n$$\\text{Var}(C|L) = \\frac{L^2-1}{12}$$\n\nNow, we compute the two terms of the Law of Total Variance.\n\n1.  **Term 1: $\\text{Var}(E[C|L])$**\n    $$\\text{Var}(E[C|L]) = \\text{Var}\\left(\\frac{L+1}{2}\\right) = \\left(\\frac{1}{2}\\right)^2 \\text{Var}(L) = \\frac{1}{4}\\text{Var}(L)$$\n    Since $L = K+1$ and $K \\sim \\text{Pois}(\\alpha)$, we have $\\text{Var}(L) = \\text{Var}(K+1) = \\text{Var}(K)$. For a Poisson distribution, the variance equals the mean, so $\\text{Var}(K) = \\alpha$.\n    $$\\text{Var}(E[C|L]) = \\frac{1}{4}\\alpha$$\n\n2.  **Term 2: $E[\\text{Var}(C|L)]$**\n    $$E[\\text{Var}(C|L)] = E\\left[\\frac{L^2-1}{12}\\right] = \\frac{1}{12}(E[L^2] - 1)$$\n    To find $E[L^2]$, we use $L=K+1$:\n    $$E[L^2] = E[(K+1)^2] = E[K^2 + 2K + 1] = E[K^2] + 2E[K] + 1$$\n    We know $E[K]=\\alpha$. We find $E[K^2]$ from the variance of $K$:\n    $$E[K^2] = \\text{Var}(K) + (E[K])^2 = \\alpha + \\alpha^2$$\n    Substituting this back:\n    $$E[L^2] = (\\alpha + \\alpha^2) + 2(\\alpha) + 1 = \\alpha^2 + 3\\alpha + 1$$\n    Now, we can compute the second term:\n    $$E[\\text{Var}(C|L)] = \\frac{1}{12}((\\alpha^2 + 3\\alpha + 1) - 1) = \\frac{\\alpha^2 + 3\\alpha}{12}$$\n\nFinally, we sum the two terms to find the total variance:\n$$\\text{Var}(C) = \\text{Var}(E[C|L]) + E[\\text{Var}(C|L)] = \\frac{\\alpha}{4} + \\frac{\\alpha^2 + 3\\alpha}{12}$$\nTo combine these, we find a common denominator:\n$$\\text{Var}(C) = \\frac{3\\alpha}{12} + \\frac{\\alpha^2 + 3\\alpha}{12} = \\frac{\\alpha^2 + 6\\alpha}{12}$$\n\nThis is the variance of the lookup cost for a successful lookup under the Poisson approximation.", "answer": "$$\\boxed{\\frac{\\alpha^2 + 6\\alpha}{12}}$$", "id": "4538794"}, {"introduction": "As computational power increasingly comes from parallel processors, understanding the limits of speedup is critical for designing efficient bioinformatics pipelines. This problem applies Amdahl's Law, a fundamental model for parallel speedup, to a realistic whole-genome alignment scenario [@problem_id:4538775]. By using real-world performance data to infer an application's serial fraction, you will learn to predict the maximum achievable performance and appreciate the bottlenecks that persist even with massive parallelism.", "problem": "A clinical whole-genome alignment pipeline processes independent short reads against a fixed reference using a seed-and-extend strategy on a single shared-memory node. The pipeline has distinct stages: input decompression and index loading, per-read alignment, and output collation. Assume that the per-read alignment stage can be perfectly partitioned across $P$ worker threads without inter-worker communication, while the remaining work is strictly serial and cannot be parallelized. Let the serial fraction be $s \\in (0,1)$, defined as the fraction of the total runtime spent in non-parallelizable stages when running with $P=1$. The idealized model assumes that the parallelizable portion’s time scales inversely with $P$ and the serial portion’s time is independent of $P$.\n\nOn a benchmark dataset of $10^6$ reads, profiled on the same node, the observed speedup with $P=16$ worker threads relative to $P=1$ is $10$. Treat this observation as indicative of the ideal strong-scaling behavior under the stated assumptions and ignore additional overheads such as scheduling or non-uniform memory access.\n\nUsing only the stated idealization and the definition of speedup, first infer the implied value of $s$ from the observation at $P=16$, and then compute the ideal speedup achievable when scaling to $P=256$ worker threads on the same node under the same $s$. Round your final answer to four significant figures. State only the final speedup as a pure number with no units.", "solution": "The problem describes a classic strong-scaling scenario, the behavior of which can be modeled by Amdahl's Law. Let $T_1$ be the total runtime of the pipeline on a single worker thread, which corresponds to the case $P=1$. We can normalize this time to $T_1 = 1$ without loss of generality.\n\nThe problem defines a serial fraction, $s$, where $s \\in (0,1)$, as the portion of the total runtime for $P=1$ that cannot be parallelized. Thus, the time spent on serial tasks is $s T_1 = s$. The remaining portion of the work, $1-s$, is designated as perfectly parallelizable. When using $P$ worker threads, the time to complete the parallelizable part scales inversely with $P$ and becomes $\\frac{1-s}{P}$. The time for the serial part is assumed to be independent of $P$ and remains $s$.\n\nThe total runtime on $P$ threads, denoted $T_P$, is the sum of the serial and parallel execution times:\n$$T_P = s + \\frac{1-s}{P}$$\n\nThe speedup, $S_P$, is defined as the ratio of the single-thread execution time to the multi-thread execution time:\n$$S_P = \\frac{T_1}{T_P} = \\frac{1}{s + \\frac{1-s}{P}}$$\n\nThe first step is to infer the value of the serial fraction $s$ using the provided benchmark data. We are given that for $P=16$ worker threads, the observed speedup is $S_{16} = 10$. Substituting these values into the speedup equation:\n$$10 = \\frac{1}{s + \\frac{1-s}{16}}$$\n\nWe proceed to solve this equation for $s$:\n$$10 \\left( s + \\frac{1-s}{16} \\right) = 1$$\n$$10s + \\frac{10(1-s)}{16} = 1$$\nTo clear the denominator, we can multiply the entire equation by $16$:\n$$16 \\times 10s + 16 \\times \\frac{10(1-s)}{16} = 16 \\times 1$$\n$$160s + 10(1-s) = 16$$\n$$160s + 10 - 10s = 16$$\n$$150s = 16 - 10$$\n$$150s = 6$$\n$$s = \\frac{6}{150} = \\frac{1}{25} = 0.04$$\nThis value $s=0.04$ is within the specified range $s \\in (0,1)$, confirming its validity within the model. This implies that $4\\%$ of the pipeline's workload is strictly serial.\n\nThe second step is to use this inferred serial fraction to predict the ideal speedup for $P=256$ worker threads. We use the same speedup formula with $s=0.04$ and $P=256$:\n$$S_{256} = \\frac{1}{s + \\frac{1-s}{256}} = \\frac{1}{0.04 + \\frac{1-0.04}{256}}$$\n$$S_{256} = \\frac{1}{0.04 + \\frac{0.96}{256}}$$\n\nNow, we evaluate the expression. Let us compute the fractional term in the denominator:\n$$\\frac{0.96}{256} = \\frac{96 \\times 10^{-2}}{256} = \\frac{3 \\times 32}{8 \\times 32} \\times 10^{-2} = \\frac{3}{8} \\times 10^{-2} = 0.375 \\times 10^{-2} = 0.00375$$\nSubstituting this back into the expression for $S_{256}$:\n$$S_{256} = \\frac{1}{0.04 + 0.00375} = \\frac{1}{0.04375}$$\n\nTo find the numerical value, it is more precise to work with fractions until the final step. We can convert the denominator to a fraction:\n$$0.04375 = \\frac{4375}{100000} = \\frac{7 \\times 625}{160 \\times 625} = \\frac{7}{160}$$\nSo, the speedup is:\n$$S_{256} = \\frac{1}{\\frac{7}{160}} = \\frac{160}{7}$$\n\nFinally, we compute the decimal value and round it to four significant figures as required by the problem statement:\n$$S_{256} = \\frac{160}{7} \\approx 22.8571428...$$\nThe first four significant figures are $2$, $2$, $8$, and $5$. The fifth significant digit is $7$. Since $7 \\ge 5$, we round up the fourth digit ($5$ becomes $6$).\n$$S_{256} \\approx 22.86$$\nThis is the ideal speedup achievable under the given model with $256$ worker threads.", "answer": "$$\\boxed{22.86}$$", "id": "4538775"}]}