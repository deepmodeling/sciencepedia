{"hands_on_practices": [{"introduction": "We begin our hands-on exploration with the cornerstone of information theory: Shannon entropy. This exercise [@problem_id:4573945] provides a direct application of this concept to a common bioinformatics scenario, asking you to quantify the information content, or uncertainty, within a DNA sequence with a known compositional bias. By calculating the entropy of a given nucleotide distribution, you will develop an intuition for how information theory measures predictability in biological data.", "problem": "A single-nucleotide emission model is used to summarize base composition in a short genomic region exhibiting compositional bias. Let $X$ be a discrete random variable over the Deoxyribonucleic Acid (DNA) alphabet $\\{A,C,G,T\\}$ with probability mass function $P(A)=0.4$, $P(C)=0.1$, $P(G)=0.1$, $P(T)=0.4$. Starting from the foundational definition of Shannon entropy for a discrete random variable and the change-of-base identity for logarithms, derive an expression for the entropy $H(X)$ measured in bits and evaluate it for the given distribution. Then, quantify how close this entropy is to the maximal value attainable on a four-symbol alphabet by computing the ratio $H(X)/\\log_{2}(4)$. Report this single ratio as your final result, rounded to $4$ significant figures.", "solution": "The problem requires the calculation of the Shannon entropy for a given probability distribution over the DNA alphabet and then finding the ratio of this entropy to the maximum possible entropy for an alphabet of the same size.\n\n**Step 1: Problem Validation**\n\nThe givens are:\n- A discrete random variable $X$ over the alphabet $\\mathcal{X} = \\{A, C, G, T\\}$.\n- The probability mass function (PMF) is $P(X=A)=0.4$, $P(X=C)=0.1$, $P(X=G)=0.1$, and $P(X=T)=0.4$.\n\nThe problem is validated as follows:\n- **Scientifically Grounded:** The concepts of Shannon entropy, DNA base composition, and compositional bias are standard in information theory and bioinformatics. The provided probabilities are non-negative and sum to $1$: $0.4 + 0.1 + 0.1 + 0.4 = 1.0$. The problem is scientifically and mathematically sound.\n- **Well-Posed:** The problem provides all necessary information (the alphabet and the PMF) and clearly states the objective. The question is unambiguous and has a unique, stable solution.\n- **Objective:** The problem is phrased in precise, objective language.\n\nThe problem is deemed valid.\n\n**Step 2: Derivation and Calculation of Entropy $H(X)$**\n\nThe foundational definition of Shannon entropy for a discrete random variable $X$ with outcomes $x_i$ from an alphabet $\\mathcal{X}$ and a probability mass function $P(x_i)$ is given by:\n$$\nH(X) = -\\sum_{x_i \\in \\mathcal{X}} P(x_i) \\log_b(P(x_i))\n$$\nThe base of the logarithm, $b$, determines the units of entropy. For entropy measured in bits, the base must be $b=2$.\n$$\nH(X)_{\\text{bits}} = -\\sum_{x_i \\in \\mathcal{X}} P(x_i) \\log_2(P(x_i))\n$$\nThe problem requires starting from a general base and using the change-of-base identity. Let's start with the natural logarithm (base $e$, units of nats). The change-of-base formula is $\\log_b(a) = \\frac{\\log_c(a)}{\\log_c(b)}$. To convert from a generic base $c$ (e.g., $e$) to base $b=2$, we have:\n$$\n\\log_2(z) = \\frac{\\log_c(z)}{\\log_c(2)}\n$$\nSubstituting this into the entropy formula in bits:\n$$\nH(X) = -\\sum_{x_i \\in \\mathcal{X}} P(x_i) \\frac{\\log_c(P(x_i))}{\\log_c(2)} = \\frac{1}{\\log_c(2)} \\left( -\\sum_{x_i \\in \\mathcal{X}} P(x_i) \\log_c(P(x_i)) \\right)\n$$\nThis explicitly shows the conversion. For our calculation, we will directly use the formula with base $2$.\n\nThe alphabet is $\\mathcal{X} = \\{A, C, G, T\\}$, and the probabilities are $P(A) = 0.4$, $P(C) = 0.1$, $P(G) = 0.1$, and $P(T) = 0.4$.\n\nThe entropy $H(X)$ is:\n$$\nH(X) = -[P(A)\\log_2(P(A)) + P(C)\\log_2(P(C)) + P(G)\\log_2(P(G)) + P(T)\\log_2(P(T))]\n$$\nSubstituting the given probabilities:\n$$\nH(X) = -[0.4 \\log_2(0.4) + 0.1 \\log_2(0.1) + 0.1 \\log_2(0.1) + 0.4 \\log_2(0.4)]\n$$\nDue to the symmetry in probabilities ($P(A)=P(T)$ and $P(C)=P(G)$), we can simplify the expression:\n$$\nH(X) = -[2 \\cdot (0.4 \\log_2(0.4)) + 2 \\cdot (0.1 \\log_2(0.1))]\n$$\n$$\nH(X) = -[0.8 \\log_2(0.4) + 0.2 \\log_2(0.1)]\n$$\nWe can express the logarithms using the natural logarithm and the change of base formula $\\log_2(z) = \\frac{\\ln(z)}{\\ln(2)}$:\n$$\nH(X) = - \\left[ 0.8 \\frac{\\ln(0.4)}{\\ln(2)} + 0.2 \\frac{\\ln(0.1)}{\\ln(2)} \\right] = -\\frac{1}{\\ln(2)} [0.8 \\ln(0.4) + 0.2 \\ln(0.1)]\n$$\n\n**Step 3: Determination of Maximal Entropy**\n\nThe maximum entropy for a discrete random variable with $|\\mathcal{X}| = K$ outcomes occurs when the probability distribution is uniform, i.e., $P(x_i) = 1/K$ for all $i$. In this case, the alphabet size is $K=4$. The maximum entropy, $H_{\\text{max}}$, is:\n$$\nH_{\\text{max}} = -\\sum_{i=1}^{4} P(x_i) \\log_2(P(x_i)) = -\\sum_{i=1}^{4} \\frac{1}{4} \\log_2\\left(\\frac{1}{4}\\right)\n$$\n$$\nH_{\\text{max}} = -4 \\cdot \\frac{1}{4} \\log_2\\left(\\frac{1}{4}\\right) = -\\log_2(4^{-1}) = \\log_2(4)\n$$\nSince $4=2^2$, the maximum entropy is:\n$$\nH_{\\text{max}} = \\log_2(2^2) = 2 \\text{ bits}\n$$\n\n**Step 4: Calculation of the Ratio**\n\nThe problem asks for the ratio $H(X)/\\log_2(4)$, which is the ratio of the calculated entropy to the maximum possible entropy for this alphabet size. This is also known as the normalized entropy.\n$$\n\\text{Ratio} = \\frac{H(X)}{H_{\\text{max}}} = \\frac{H(X)}{\\log_2(4)}\n$$\nSubstituting the expressions for $H(X)$ and $H_{\\text{max}}$:\n$$\n\\text{Ratio} = \\frac{-[0.8 \\log_2(0.4) + 0.2 \\log_2(0.1)]}{2}\n$$\n$$\n\\text{Ratio} = -[0.4 \\log_2(0.4) + 0.1 \\log_2(0.1)]\n$$\nNow, we evaluate this expression numerically:\n$$\n\\text{Ratio} = - \\left[ 0.4 \\left(\\frac{\\ln(0.4)}{\\ln(2)}\\right) + 0.1 \\left(\\frac{\\ln(0.1)}{\\ln(2)}\\right) \\right]\n$$\n$$\n\\text{Ratio} = - \\frac{1}{\\ln(2)} [0.4 \\ln(0.4) + 0.1 \\ln(0.1)]\n$$\nUsing the values $\\ln(0.4) \\approx -0.9162907$ and $\\ln(0.1) \\approx -2.3025851$ and $\\ln(2) \\approx 0.6931472$:\n$$\n0.4 \\ln(0.4) \\approx 0.4 \\times (-0.9162907) = -0.3665163\n$$\n$$\n0.1 \\ln(0.1) \\approx 0.1 \\times (-2.3025851) = -0.2302585\n$$\n$$\n\\text{Ratio} \\approx - \\frac{1}{0.6931472} [-0.3665163 - 0.2302585] = - \\frac{-0.5967748}{0.6931472}\n$$\n$$\n\\text{Ratio} \\approx 0.8609640\n$$\nRounding this result to $4$ significant figures, we look at the fifth significant figure. Since $6 \\geq 5$, we round up the fourth digit:\n$$\n\\text{Ratio} \\approx 0.8610\n$$\nThis value represents the entropy of the given distribution as a fraction of the maximum possible entropy, indicating a moderate level of compositional bias (a value of $1$ would indicate no bias, while a value closer to $0$ would indicate extreme bias).", "answer": "$$\\boxed{0.8610}$$", "id": "4573945"}, {"introduction": "Building on the concept of entropy, we now investigate the relationships between multiple variables—a critical task in medical data analytics. This practice [@problem_id:4573951] delves into the phenomenon of confounding by using Mutual Information ($I(X;Y)$) and Conditional Mutual Information ($I(X;Y \\mid Z)$) to analyze a hypothetical clinical dataset. You will see firsthand how a variable can appear associated with an outcome, only for that association to vanish when accounting for a hidden confounder, a classic case of Simpson's paradox.", "problem": "A medical data scientist is analyzing an observational dataset to evaluate a new therapy. Let $X \\in \\{0,1\\}$ indicate treatment assignment where $X=1$ denotes the new therapy and $X=0$ denotes the standard therapy. Let $Y \\in \\{0,1\\}$ indicate outcome where $Y=1$ denotes recovery and $Y=0$ denotes no recovery. Let $Z \\in \\{0,1\\}$ be a binary confounder indicating severity group where $Z=0$ denotes low severity and $Z=1$ denotes high severity. The investigation aims to understand the association between $X$ and $Y$ both marginally and when conditioning on $Z$, using Mutual Information (MI) and Conditional Mutual Information (CMI) computed with base-$2$ logarithms.\n\nThe dataset comprises $N=400$ patients, split evenly across the severity groups $Z=0$ and $Z=1$:\n\n- For $Z=0$ (low severity), there are $200$ patients. Among these, $40$ receive the new therapy ($X=1$) and $160$ receive the standard therapy ($X=0$). Of the $200$, $180$ recover ($Y=1$) and $20$ do not ($Y=0$). The joint counts are:\n  - $(X=1,Y=1)$: $36$,\n  - $(X=1,Y=0)$: $4$,\n  - $(X=0,Y=1)$: $144$,\n  - $(X=0,Y=0)$: $16$.\n\n- For $Z=1$ (high severity), there are $200$ patients. Among these, $160$ receive the new therapy ($X=1$) and $40$ receive the standard therapy ($X=0$). Of the $200$, $60$ recover ($Y=1$) and $140$ do not ($Y=0$). The joint counts are:\n  - $(X=1,Y=1)$: $48$,\n  - $(X=1,Y=0)$: $112$,\n  - $(X=0,Y=1)$: $12$,\n  - $(X=0,Y=0)$: $28$.\n\nAssume the definitions of Shannon entropy and mutual information apply. Using only first principles, derive the Mutual Information $I(X;Y)$ from the aggregated data (i.e., ignoring $Z$) and the Conditional Mutual Information $I(X;Y \\mid Z)$. Work in bits with base-$2$ logarithms.\n\nWhich option best characterizes this dataset and your computed quantities?\n\nA. $I(X;Y)$ is strictly positive (approximately $0.10$ bits), while $I(X;Y \\mid Z)=0$; therefore, the apparent association is explained entirely by the confounder $Z$.\n\nB. $I(X;Y)=0$, while $I(X;Y \\mid Z)$ is strictly positive; therefore, conditioning on $Z$ uncovers a hidden association.\n\nC. Both $I(X;Y)$ and $I(X;Y \\mid Z)$ are strictly positive and approximately equal.\n\nD. Both $I(X;Y)$ and $I(X;Y \\mid Z)$ are zero; therefore, $X$ and $Y$ are independent both marginally and conditionally.", "solution": "The user has provided a problem that requires the calculation and interpretation of Mutual Information ($I(X;Y)$) and Conditional Mutual Information ($I(X;Y \\mid Z)$) from a given dataset.\n\n### Step 1: Problem Validation\n\n**Givens:**\nThe problem provides the following data and definitions:\n-   A total of $N=400$ patients.\n-   Variables: Treatment $X \\in \\{0,1\\}$, Outcome $Y \\in \\{0,1\\}$, Confounder $Z \\in \\{0,1\\}$.\n-   Logarithms are base-$2$.\n-   For $Z=0$ (low severity), $N_0 = 200$ patients:\n    -   $N(X=1,Y=1,Z=0) = 36$\n    -   $N(X=1,Y=0,Z=0) = 4$\n    -   $N(X=0,Y=1,Z=0) = 144$\n    -   $N(X=0,Y=0,Z=0) = 16$\n-   For $Z=1$ (high severity), $N_1 = 200$ patients:\n    -   $N(X=1,Y=1,Z=1) = 48$\n    -   $N(X=1,Y=0,Z=1) = 112$\n    -   $N(X=0,Y=1,Z=1) = 12$\n    -   $N(X=0,Y=0,Z=1) = 28$\n\n**Validation using Extracted Givens:**\nLet's verify the internal consistency of the provided counts.\n-   For $Z=0$:\n    -   $N(X=1, Z=0) = 36+4 = 40$.\n    -   $N(X=0, Z=0) = 144+16 = 160$.\n    -   $N(Y=1, Z=0) = 36+144 = 180$.\n    -   $N(Y=0, Z=0) = 4+16 = 20$.\n    -   Total for $Z=0$: $36+4+144+16=200$. The counts are consistent with the marginal totals described in the problem statement.\n-   For $Z=1$:\n    -   $N(X=1, Z=1) = 48+112 = 160$.\n    -   $N(X=0, Z=1) = 12+28 = 40$.\n    -   $N(Y=1, Z=1) = 48+12 = 60$.\n    -   $N(Y=0, Z=1) = 112+28 = 140$.\n    -   Total for $Z=1$: $48+112+12+28=200$. The counts are consistent with the marginal totals described in the problem statement.\n\nThe problem is scientifically grounded, using standard concepts from information theory to analyze a dataset, a common task in medical data analytics. The setup is complete, with all necessary data provided. The problem is well-posed, objective, and contains no contradictions. The scenario is a classic illustration of Simpson's Paradox.\n\n**Verdict:** The problem is valid.\n\n### Step 2: Derivation of the Solution\n\nThe solution requires calculating the Mutual Information $I(X;Y)$ and the Conditional Mutual Information $I(X;Y \\mid Z)$.\n\n**1. Calculation of the Marginal Mutual Information, $I(X;Y)$**\n\nFirst, we aggregate the data across the confounder $Z$ to obtain the marginal counts for $(X,Y)$.\n-   $N(X=1, Y=1) = N(X=1, Y=1, Z=0) + N(X=1, Y=1, Z=1) = 36 + 48 = 84$\n-   $N(X=1, Y=0) = N(X=1, Y=0, Z=0) + N(X=1, Y=0, Z=1) = 4 + 112 = 116$\n-   $N(X=0, Y=1) = N(X=0, Y=1, Z=0) + N(X=0, Y=1, Z=1) = 144 + 12 = 156$\n-   $N(X=0, Y=0) = N(X=0, Y=0, Z=0) + N(X=0, Y=0, Z=1) = 16 + 28 = 44$\n\nThe total number of patients is $N = 84+116+156+44 = 400$. From these counts, we derive the joint probability distribution $P(X,Y)$:\n-   $P(X=1, Y=1) = 84/400 = 0.21$\n-   $P(X=1, Y=0) = 116/400 = 0.29$\n-   $P(X=0, Y=1) = 156/400 = 0.39$\n-   $P(X=0, Y=0) = 44/400 = 0.11$\n\nNext, we find the marginal probabilities $P(X)$ and $P(Y)$:\n-   $P(X=1) = P(X=1, Y=1) + P(X=1, Y=0) = 0.21 + 0.29 = 0.5$\n-   $P(X=0) = P(X=0, Y=1) + P(X=0, Y=0) = 0.39 + 0.11 = 0.5$\n-   $P(Y=1) = P(X=1, Y=1) + P(X=0, Y=1) = 0.21 + 0.39 = 0.6$\n-   $P(Y=0) = P(X=1, Y=0) + P(X=0, Y=0) = 0.29 + 0.11 = 0.4$\n\nThe Mutual Information is defined as $I(X;Y) = \\sum_{x,y} P(x,y) \\log_2 \\frac{P(x,y)}{P(x)P(y)}$.\nLet's compute the product of marginals $P(x)P(y)$:\n-   $P(X=1)P(Y=1) = 0.5 \\times 0.6 = 0.3$\n-   $P(X=1)P(Y=0) = 0.5 \\times 0.4 = 0.2$\n-   $P(X=0)P(Y=1) = 0.5 \\times 0.6 = 0.3$\n-   $P(X=0)P(Y=0) = 0.5 \\times 0.4 = 0.2$\n\nSince $P(X,Y) \\neq P(X)P(Y)$, the variables are not marginally independent, and thus $I(X;Y)$ must be strictly positive.\n$$ I(X;Y) = P(1,1)\\log_2\\frac{P(1,1)}{P(1)P(1)} + P(1,0)\\log_2\\frac{P(1,0)}{P(1)P(0)} + P(0,1)\\log_2\\frac{P(0,1)}{P(0)P(1)} + P(0,0)\\log_2\\frac{P(0,0)}{P(0)P(0)} $$\n$$ I(X;Y) = 0.21\\log_2\\frac{0.21}{0.3} + 0.29\\log_2\\frac{0.29}{0.2} + 0.39\\log_2\\frac{0.39}{0.3} + 0.11\\log_2\\frac{0.11}{0.2} $$\n$$ I(X;Y) = 0.21\\log_2(0.7) + 0.29\\log_2(1.45) + 0.39\\log_2(1.3) + 0.11\\log_2(0.55) $$\nUsing $\\log_2(a) \\approx 3.3219 \\ln(a)$:\n$$ I(X;Y) \\approx 0.21(-0.5146) + 0.29(0.5362) + 0.39(0.3785) + 0.11(-0.8625) $$\n$$ I(X;Y) \\approx -0.1081 + 0.1555 + 0.1476 - 0.0949 \\approx 0.1001 \\text{ bits} $$\nSo, $I(X;Y)$ is strictly positive and approximately $0.10$ bits.\n\n**2. Calculation of the Conditional Mutual Information, $I(X;Y \\mid Z)$**\n\nThe CMI is defined as $I(X;Y \\mid Z) = \\sum_{z} P(z) I(X;Y \\mid Z=z)$.\nThe dataset is split evenly, so $P(Z=0) = 200/400 = 0.5$ and $P(Z=1) = 200/400 = 0.5$.\nWe need to calculate $I(X;Y \\mid Z=z)$ for each value of $z$.\n\n**For $Z=0$ (low severity):**\nThe counts are given for $N_0=200$ patients. We calculate the conditional probabilities given $Z=0$:\n-   $P(X=1 \\mid Z=0) = (36+4)/200 = 40/200 = 0.2$\n-   $P(X=0 \\mid Z=0) = (144+16)/200 = 160/200 = 0.8$\n-   $P(Y=1 \\mid Z=0) = (36+144)/200 = 180/200 = 0.9$\n-   $P(Y=0 \\mid Z=0) = (4+16)/200 = 20/200 = 0.1$\nThe joint conditional probabilities $P(X,Y \\mid Z=0)$ are:\n-   $P(X=1, Y=1 \\mid Z=0) = 36/200 = 0.18$\n-   $P(X=1, Y=0 \\mid Z=0) = 4/200 = 0.02$\n-   $P(X=0, Y=1 \\mid Z=0) = 144/200 = 0.72$\n-   $P(X=0, Y=0 \\mid Z=0) = 16/200 = 0.08$\n\nWe check for conditional independence: $P(X,Y \\mid Z=0) \\stackrel{?}{=} P(X \\mid Z=0) P(Y \\mid Z=0)$.\n-   $P(X=1 \\mid Z=0)P(Y=1 \\mid Z=0) = 0.2 \\times 0.9 = 0.18 = P(X=1, Y=1 \\mid Z=0)$\n-   $P(X=1 \\mid Z=0)P(Y=0 \\mid Z=0) = 0.2 \\times 0.1 = 0.02 = P(X=1, Y=0 \\mid Z=0)$\n-   $P(X=0 \\mid Z=0)P(Y=1 \\mid Z=0) = 0.8 \\times 0.9 = 0.72 = P(X=0, Y=1 \\mid Z=0)$\n-   $P(X=0 \\mid Z=0)P(Y=0 \\mid Z=0) = 0.8 \\times 0.1 = 0.08 = P(X=0, Y=0 \\mid Z=0)$\nSince the equality holds for all pairs $(x,y)$, the variables $X$ and $Y$ are conditionally independent given $Z=0$. Therefore, $I(X;Y \\mid Z=0) = 0$.\n\n**For $Z=1$ (high severity):**\nThe counts are given for $N_1=200$ patients. We calculate the conditional probabilities given $Z=1$:\n-   $P(X=1 \\mid Z=1) = (48+112)/200 = 160/200 = 0.8$\n-   $P(X=0 \\mid Z=1) = (12+28)/200 = 40/200 = 0.2$\n-   $P(Y=1 \\mid Z=1) = (48+12)/200 = 60/200 = 0.3$\n-   $P(Y=0 \\mid Z=1) = (112+28)/200 = 140/200 = 0.7$\nThe joint conditional probabilities $P(X,Y \\mid Z=1)$ are:\n-   $P(X=1, Y=1 \\mid Z=1) = 48/200 = 0.24$\n-   $P(X=1, Y=0 \\mid Z=1) = 112/200 = 0.56$\n-   $P(X=0, Y=1 \\mid Z=1) = 12/200 = 0.06$\n-   $P(X=0, Y=0 \\mid Z=1) = 28/200 = 0.14$\n\nWe check for conditional independence: $P(X,Y \\mid Z=1) \\stackrel{?}{=} P(X \\mid Z=1) P(Y \\mid Z=1)$.\n-   $P(X=1 \\mid Z=1)P(Y=1 \\mid Z=1) = 0.8 \\times 0.3 = 0.24 = P(X=1, Y=1 \\mid Z=1)$\n-   $P(X=1 \\mid Z=1)P(Y=0 \\mid Z=1) = 0.8 \\times 0.7 = 0.56 = P(X=1, Y=0 \\mid Z=1)$\n-   $P(X=0 \\mid Z=1)P(Y=1 \\mid Z=1) = 0.2 \\times 0.3 = 0.06 = P(X=0, Y=1 \\mid Z=1)$\n-   $P(X=0 \\mid Z=1)P(Y=0 \\mid Z=1) = 0.2 \\times 0.7 = 0.14 = P(X=0, Y=0 \\mid Z=1)$\nSince the equality holds for all pairs $(x,y)$, the variables $X$ and $Y$ are also conditionally independent given $Z=1$. Therefore, $I(X;Y \\mid Z=1) = 0$.\n\nFinally, we compute the total CMI:\n$$ I(X;Y \\mid Z) = P(Z=0)I(X;Y \\mid Z=0) + P(Z=1)I(X;Y \\mid Z=1) $$\n$$ I(X;Y \\mid Z) = 0.5 \\times 0 + 0.5 \\times 0 = 0 $$\n\n**Summary of Results:**\n-   $I(X;Y) \\approx 0.10$ bits (strictly positive)\n-   $I(X;Y \\mid Z) = 0$ bits\n\nThis result demonstrates a classic case of confounding, often associated with Simpson's paradox. There is no association between treatment $X$ and outcome $Y$ within either the low-severity ($Z=0$) or high-severity ($Z=1$) groups. However, because the treatment assignment is heavily correlated with the severity group (standard therapy for low severity, new therapy for high severity), a spurious association appears when the data is aggregated. The mutual information $I(X;Y)  0$ captures this apparent marginal association, while $I(X;Y \\mid Z)=0$ correctly shows the absence of association once the confounder $Z$ is taken into account.\n\n### Step 3: Option-by-Option Analysis\n\n**A. $I(X;Y)$ is strictly positive (approximately $0.10$ bits), while $I(X;Y \\mid Z)=0$; therefore, the apparent association is explained entirely by the confounder $Z$.**\nThis statement accurately reflects our calculations. We found $I(X;Y) \\approx 0.10$ bits and $I(X;Y \\mid Z) = 0$. The interpretation that the confounder $Z$ explains the entire apparent association is the correct conclusion from these information-theoretic quantities.\n**Verdict: Correct**\n\n**B. $I(X;Y)=0$, while $I(X;Y \\mid Z)$ is strictly positive; therefore, conditioning on $Z$ uncovers a hidden association.**\nThis statement contradicts our findings. We calculated $I(X;Y)  0$ and $I(X;Y \\mid Z) = 0$. This option describes the reverse situation.\n**Verdict: Incorrect**\n\n**C. Both $I(X;Y)$ and $I(X;Y \\mid Z)$ are strictly positive and approximately equal.**\nThis statement is incorrect because our calculation shows $I(X;Y \\mid Z) = 0$. This scenario would imply that $Z$ is not a confounder and does not significantly mediate the relationship between $X$ and $Y$.\n**Verdict: Incorrect**\n\n**D. Both $I(X;Y)$ and $I(X;Y \\mid Z)$ are zero; therefore, $X$ and $Y$ are independent both marginally and conditionally.**\nThis statement is incorrect because our calculation shows $I(X;Y)  0$. This would imply $X$ and $Y$ have no relationship whatsoever.\n**Verdict: Incorrect**", "answer": "$$\\boxed{A}$$", "id": "4573951"}, {"introduction": "Our final practice applies information theory to the practical machine learning task of feature selection for a clinical decision tree. This problem [@problem_id:4573998] illustrates a critical pitfall of using the standard Information Gain criterion, which is inherently biased towards features with many unique values. By comparing Information Gain with the normalized Gain Ratio, you will learn how to make more robust feature selections and avoid the trap of overfitting to uninformative but high-cardinality data like patient IDs.", "problem": "Consider a binary disease outcome variable $Y \\in \\{0,1\\}$ for $N$ patients, and two candidate features $X_{\\mathrm{id}}$ (a patient identifier) and $X_{\\mathrm{bio}}$ (a thresholded biomarker) used to build the root split of a decision tree for clinical risk stratification in bioinformatics and medical data analytics. Suppose the dataset has $N=12$ patients with $6$ cases ($Y=1$) and $6$ controls ($Y=0$). The patient identifier $X_{\\mathrm{id}}$ takes $12$ unique categorical values, one per patient. The biomarker feature $X_{\\mathrm{bio}}$ is binary and partitions the $12$ patients into two groups: group $X_{\\mathrm{bio}}=0$ contains $8$ patients with $2$ cases and $6$ controls, and group $X_{\\mathrm{bio}}=1$ contains $4$ patients with $4$ cases and $0$ controls.\n\nYou will assess the behavior of the Information Gain (IG) and the Gain Ratio (GR) for these features from first principles in information theory. Use the following core definitions with base-$2$ logarithms. The Shannon entropy of a discrete random variable $Z$ with probability mass function $p(z)$ is\n$$\nH(Z) = -\\sum_{z} p(z) \\log_2 p(z).\n$$\nThe conditional entropy of $Y$ given $X$ is\n$$\nH(Y \\mid X) = \\sum_{x} p(x) H(Y \\mid X=x),\n$$\nwhere $H(Y \\mid X=x) = -\\sum_{y} p(y \\mid x) \\log_2 p(y \\mid x)$.\nThe mutual information is\n$$\nI(X;Y) = H(Y) - H(Y \\mid X) = H(X) - H(X \\mid Y).\n$$\nFor decision tree splitting, the Information Gain is $I(X;Y)$ computed under the empirical distribution of $(X,Y)$ in the dataset. The Gain Ratio is defined as\n$$\n\\mathrm{GR}(X;Y) = \\frac{\\mathrm{IG}(X;Y)}{H(X)} = \\frac{I(X;Y)}{H(X)}.\n$$\n\nAssume the data-generating mechanism is such that $X_{\\mathrm{id}}$ is statistically independent of $Y$ in the population, and $X_{\\mathrm{bio}}$ is a noisy but predictive biomarker as summarized by the empirical counts provided above. Using only the definitions stated, compute the relevant entropies and conditional entropies for the empirical dataset to derive the Information Gain and Gain Ratio for both $X_{\\mathrm{id}}$ and $X_{\\mathrm{bio}}$, and determine which feature each criterion would select at the root. Then evaluate the following statements.\n\nA. The Information Gain of the identifier feature $X_{\\mathrm{id}}$ equals $H(Y)$ even though $X_{\\mathrm{id}}$ is statistically independent of $Y$ under the data-generating mechanism, illustrating a finite-sample bias that favors high-cardinality splits.\n\nB. The Gain Ratio of $X_{\\mathrm{id}}$ equals $1$ because $H(X_{\\mathrm{id}}) = H(Y)$ when every sample has a unique identifier.\n\nC. For the biomarker feature $X_{\\mathrm{bio}}$ with the given counts, the Information Gain is approximately $0.459$ bits and the Gain Ratio is approximately $0.500$, so the Gain Ratio would select $X_{\\mathrm{bio}}$ over $X_{\\mathrm{id}}$ at the root split.\n\nD. Mutual information satisfies $I(X;Y) \\leq H(X)$ for any $(X,Y)$, therefore $\\mathrm{GR}(X;Y) \\leq 1$ always; in this dataset, both features have $\\mathrm{GR} \\leq 1$.\n\nE. With infinite data and $X_{\\mathrm{id}}$ independent of $Y$, the expected Information Gain of $X_{\\mathrm{id}}$ approaches $H(Y)$ due to the large number of categories, justifying preferring $X_{\\mathrm{id}}$ in general.\n\nSelect all correct statements.", "solution": "The problem statement is critically evaluated for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n- Total patients: $N=12$.\n- Outcome variable $Y$: $6$ cases ($Y=1$) and $6$ controls ($Y=0$).\n- Feature $X_{\\mathrm{id}}$: A patient identifier with $12$ unique categorical values. In the population (data-generating mechanism), $X_{\\mathrm{id}}$ is statistically independent of $Y$.\n- Feature $X_{\\mathrm{bio}}$: A binary biomarker.\n- Data partition by $X_{\\mathrm{bio}}$:\n    - For $X_{\\mathrm{bio}}=0$: $8$ patients ($2$ cases, $6$ controls).\n    - For $X_{\\mathrm{bio}}=1$: $4$ patients ($4$ cases, $0$ controls).\n- All calculations are to be based on the empirical distribution of the given dataset.\n- Definitions provided: Shannon entropy $H(Z)$, conditional entropy $H(Y \\mid X)$, mutual information $I(X;Y)$, Information Gain $\\mathrm{IG}(X;Y) = I(X;Y)$, and Gain Ratio $\\mathrm{GR}(X;Y) = I(X;Y)/H(X)$. Logarithms are base-$2$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It uses standard, correctly stated definitions from information theory and applies them to a classic problem in machine learning (feature selection for decision trees) and bioinformatics. The distinction between population properties (independence of $X_{\\mathrm{id}}$ and $Y$) and finite-sample estimates is a key, valid concept being tested. All numerical data is consistent: the sum of patients in the $X_{\\mathrm{bio}}$ groups is $8+4=12=N$; the sum of cases is $2+4=6$; the sum of controls is $6+0=6$. The problem is well-defined and contains sufficient information for a unique solution.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution will now be derived.\n\n### Derivations\nFirst, we compute the entropy of the target variable $Y$ at the root node. With $6$ cases and $6$ controls out of $N=12$ patients, the probabilities are $p(Y=1) = 6/12 = 1/2$ and $p(Y=0) = 6/12 = 1/2$.\n$$\nH(Y) = -\\left( p(Y=1) \\log_2 p(Y=1) + p(Y=0) \\log_2 p(Y=0) \\right)\n$$\n$$\nH(Y) = -\\left( \\frac{1}{2} \\log_2\\left(\\frac{1}{2}\\right) + \\frac{1}{2} \\log_2\\left(\\frac{1}{2}\\right) \\right) = -2 \\cdot \\frac{1}{2} \\cdot (-1) = 1 \\, \\text{bit}.\n$$\n\n#### Analysis of Feature $X_{\\mathrm{id}}$ (Patient Identifier)\n1.  **Entropy of $X_{\\mathrm{id}}$**: The feature $X_{\\mathrm{id}}$ has $12$ unique values, one for each patient. The empirical probability of each value is $p(x_i) = 1/12$.\n    $$\n    H(X_{\\mathrm{id}}) = -\\sum_{i=1}^{12} p(x_i) \\log_2 p(x_i) = -\\sum_{i=1}^{12} \\frac{1}{12} \\log_2\\left(\\frac{1}{12}\\right) = -12 \\cdot \\frac{1}{12} \\log_2\\left(\\frac{1}{12}\\right) = \\log_2(12) \\, \\text{bits}.\n    $$\n    Numerically, $H(X_{\\mathrm{id}}) = \\log_2(3 \\times 4) = \\log_2(3) + \\log_2(4) \\approx 1.585 + 2 = 3.585$ bits.\n\n2.  **Conditional Entropy $H(Y \\mid X_{\\mathrm{id}})$**: For each unique value of $X_{\\mathrm{id}}$, there is exactly one patient. Therefore, the conditional distribution $p(y \\mid X_{\\mathrm{id}}=x_i)$ is deterministic: either $p(Y=1 \\mid x_i)=1$ or $p(Y=0 \\mid x_i)=1$. The entropy of any such deterministic distribution is $0$.\n    $$\n    H(Y \\mid X_{\\mathrm{id}}=x_i) = - (1 \\log_2(1) + 0 \\log_2(0)) = 0 \\quad \\forall i.\n    $$\n    The total conditional entropy is the weighted average:\n    $$\n    H(Y \\mid X_{\\mathrm{id}}) = \\sum_{i=1}^{12} p(x_i) H(Y \\mid X_{\\mathrm{id}}=x_i) = \\sum_{i=1}^{12} \\frac{1}{12} \\cdot 0 = 0.\n    $$\n\n3.  **Information Gain $\\mathrm{IG}(X_{\\mathrm{id}}; Y)$**:\n    $$\n    \\mathrm{IG}(X_{\\mathrm{id}}; Y) = H(Y) - H(Y \\mid X_{\\mathrm{id}}) = 1 - 0 = 1 \\, \\text{bit}.\n    $$\n\n4.  **Gain Ratio $\\mathrm{GR}(X_{\\mathrm{id}}; Y)$**:\n    $$\n    \\mathrm{GR}(X_{\\mathrm{id}}; Y) = \\frac{\\mathrm{IG}(X_{\\mathrm{id}}; Y)}{H(X_{\\mathrm{id}})} = \\frac{1}{\\log_2(12)} \\approx \\frac{1}{3.585} \\approx 0.279.\n    $$\n\n#### Analysis of Feature $X_{\\mathrm{bio}}$ (Biomarker)\n1.  **Entropy of $X_{\\mathrm{bio}}$**: The feature is binary. $p(X_{\\mathrm{bio}}=0) = 8/12 = 2/3$ and $p(X_{\\mathrm{bio}}=1) = 4/12 = 1/3$.\n    $$\n    H(X_{\\mathrm{bio}}) = -\\left( \\frac{2}{3} \\log_2\\left(\\frac{2}{3}\\right) + \\frac{1}{3} \\log_2\\left(\\frac{1}{3}\\right) \\right) \\approx - (0.667 \\times (-0.585) + 0.333 \\times (-1.585)) \\approx 0.390 + 0.528 \\approx 0.918 \\, \\text{bits}.\n    $$\n\n2.  **Conditional Entropy $H(Y \\mid X_{\\mathrm{bio}})$**:\n    - For $X_{\\mathrm{bio}}=0$: $8$ patients, $2$ cases ($Y=1$), $6$ controls ($Y=0$).\n      $p(Y=1|X_{\\mathrm{bio}}=0)=2/8=1/4$, $p(Y=0|X_{\\mathrm{bio}}=0)=6/8=3/4$.\n      $$\n      H(Y \\mid X_{\\mathrm{bio}}=0) = -\\left( \\frac{1}{4} \\log_2\\left(\\frac{1}{4}\\right) + \\frac{3}{4} \\log_2\\left(\\frac{3}{4}\\right) \\right) \\approx - (0.25 \\times (-2) + 0.75 \\times (-0.415)) \\approx 0.5 + 0.311 = 0.811 \\, \\text{bits}.\n      $$\n    - For $X_{\\mathrm{bio}}=1$: $4$ patients, $4$ cases ($Y=1$), $0$ controls ($Y=0$). This node is pure.\n      $p(Y=1|X_{\\mathrm{bio}}=1)=4/4=1$, $p(Y=0|X_{\\mathrm{bio}}=1)=0/4=0$.\n      $$\n      H(Y \\mid X_{\\mathrm{bio}}=1) = - (1 \\log_2(1) + 0 \\log_2(0)) = 0 \\, \\text{bits}.\n      $$\n    - The total conditional entropy is:\n      $$\n      H(Y \\mid X_{\\mathrm{bio}}) = p(X_{\\mathrm{bio}}=0) H(Y \\mid X_{\\mathrm{bio}}=0) + p(X_{\\mathrm{bio}}=1) H(Y \\mid X_{\\mathrm{bio}}=1)\n      $$\n      $$\n      H(Y \\mid X_{\\mathrm{bio}}) = \\frac{2}{3} \\cdot H(Y \\mid X_{\\mathrm{bio}}=0) + \\frac{1}{3} \\cdot 0 \\approx \\frac{2}{3} \\cdot 0.811 \\approx 0.541 \\, \\text{bits}.\n      $$\n\n3.  **Information Gain $\\mathrm{IG}(X_{\\mathrm{bio}}; Y)$**:\n    $$\n    \\mathrm{IG}(X_{\\mathrm{bio}}; Y) = H(Y) - H(Y \\mid X_{\\mathrm{bio}}) \\approx 1 - 0.541 = 0.459 \\, \\text{bits}.\n    $$\n\n4.  **Gain Ratio $\\mathrm{GR}(X_{\\mathrm{bio}}; Y)$**:\n    $$\n    \\mathrm{GR}(X_{\\mathrm{bio}}; Y) = \\frac{\\mathrm{IG}(X_{\\mathrm{bio}}; Y)}{H(X_{\\mathrm{bio}})} \\approx \\frac{0.459}{0.918} \\approx 0.500.\n    $$\n\n#### Feature Selection\n- **Using Information Gain**:\n  $\\mathrm{IG}(X_{\\mathrm{id}}; Y) = 1$ bit.\n  $\\mathrm{IG}(X_{\\mathrm{bio}}; Y) \\approx 0.459$ bits.\n  Since $1  0.459$, IG would select $X_{\\mathrm{id}}$.\n- **Using Gain Ratio**:\n  $\\mathrm{GR}(X_{\\mathrm{id}}; Y) \\approx 0.279$.\n  $\\mathrm{GR}(X_{\\mathrm{bio}}; Y) \\approx 0.500$.\n  Since $0.500  0.279$, GR would select $X_{\\mathrm{bio}}$.\n\n### Evaluation of Statements\n\n**A. The Information Gain of the identifier feature $X_{\\mathrm{id}}$ equals $H(Y)$ even though $X_{\\mathrm{id}}$ is statistically independent of $Y$ under the data-generating mechanism, illustrating a finite-sample bias that favors high-cardinality splits.**\nOur calculation shows $\\mathrm{IG}(X_{\\mathrm{id}}; Y) = 1$ bit, which is exactly $H(Y)$. This outcome occurs because $X_{\\mathrm{id}}$ has as many unique values as there are data points ($12$), creating pure \"leaves\" and making the conditional entropy $H(Y \\mid X_{\\mathrm{id}})$ zero. This is a classic example of overfitting in a finite sample. A feature that is genuinely uninformative in the population ($I(X;Y)_{pop}=0$) appears maximally informative on the sample. This artifact is known as a bias favoring high-cardinality features. Thus, the statement is entirely correct.\n**Verdict: Correct.**\n\n**B. The Gain Ratio of $X_{\\mathrm{id}}$ equals $1$ because $H(X_{\\mathrm{id}}) = H(Y)$ when every sample has a unique identifier.**\nOur calculation shows $\\mathrm{GR}(X_{\\mathrm{id}}; Y) = 1/\\log_2(12) \\approx 0.279$, which is not $1$. The premise used to justify this claim is that $H(X_{\\mathrm{id}}) = H(Y)$. We calculated $H(X_{\\mathrm{id}}) = \\log_2(12) \\approx 3.585$ bits and $H(Y) = 1$ bit. These are not equal. Therefore, both the conclusion and the premise are false. For the Gain Ratio to be $1$, it would require $\\mathrm{IG}(X_{\\mathrm{id}}; Y) = H(X_{\\mathrm{id}})$. Here, $\\mathrm{IG}(X_{\\mathrm{id}}; Y) = H(Y)$, so it would require $H(Y) = H(X_{\\mathrm{id}})$, which is false.\n**Verdict: Incorrect.**\n\n**C. For the biomarker feature $X_{\\mathrm{bio}}$ with the given counts, the Information Gain is approximately $0.459$ bits and the Gain Ratio is approximately $0.500$, so the Gain Ratio would select $X_{\\mathrm{bio}}$ over $X_{\\mathrm{id}}$ at the root split.**\nOur calculations yield $\\mathrm{IG}(X_{\\mathrm{bio}}; Y) \\approx 0.459$ bits and $\\mathrm{GR}(X_{\\mathrm{bio}}; Y) \\approx 0.500$, which match the statement. We also determined that the Gain Ratio criterion compares $\\mathrm{GR}(X_{\\mathrm{bio}}) \\approx 0.500$ with $\\mathrm{GR}(X_{\\mathrm{id}}) \\approx 0.279$. Since $0.500  0.279$, it would indeed select $X_{\\mathrm{bio}}$. The statement is fully supported by our derivation.\n**Verdict: Correct.**\n\n**D. Mutual information satisfies $I(X;Y) \\leq H(X)$ for any $(X,Y)$, therefore $\\mathrm{GR}(X;Y) \\leq 1$ always; in this dataset, both features have $\\mathrm{GR} \\leq 1$.**\nThis statement makes three claims.\n1. $I(X;Y) \\leq H(X)$: This is a fundamental property of mutual information. From the definition $I(X;Y) = H(X) - H(X \\mid Y)$ and the non-negativity of entropy ($H(X \\mid Y) \\geq 0$), the inequality holds.\n2. $\\mathrm{GR}(X;Y) \\leq 1$: This follows directly from the definition $\\mathrm{GR}(X;Y) = I(X;Y)/H(X)$ and the property $I(X;Y) \\leq H(X)$ (assuming $H(X)  0$, which is true for any non-constant feature).\n3. Application to this dataset: We calculated $\\mathrm{GR}(X_{\\mathrm{id}}) \\approx 0.279$ and $\\mathrm{GR}(X_{\\mathrm{bio}}) \\approx 0.500$. Both values are less than or equal to $1$.\nAll parts of the statement are mathematically sound and factually correct for the given data.\n**Verdict: Correct.**\n\n**E. With infinite data and $X_{\\mathrm{id}}$ independent of $Y$, the expected Information Gain of $X_{\\mathrm{id}}$ approaches $H(Y)$ due to the large number of categories, justifying preferring $X_{\\mathrm{id}}$ in general.**\nThe statement begins with a correct premise from the problem setup: with infinite data, $X_{\\mathrm{id}}$ is independent of $Y$. The consequence of statistical independence is that the true mutual information is zero: $I(X_{\\mathrm{id}}; Y) = 0$. The expected Information Gain is this true mutual information. Therefore, with infinite data, the expected IG of $X_{\\mathrm{id}}$ approaches $0$, not $H(Y)$. The behavior described in the statement—IG approaching $H(Y)$—is a finite-sample artifact, not a population property. The justification is also flawed; this artifact is a reason to be wary of IG and prefer metrics like GR, not a reason to prefer the high-cardinality feature.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{ACD}$$", "id": "4573998"}]}