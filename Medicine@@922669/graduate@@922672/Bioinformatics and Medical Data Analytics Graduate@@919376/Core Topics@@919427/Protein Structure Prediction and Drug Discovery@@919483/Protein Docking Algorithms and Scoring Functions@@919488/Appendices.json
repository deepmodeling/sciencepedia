{"hands_on_practices": [{"introduction": "Scoring functions must translate complex biophysical interactions into a single numerical score. A major driving force for protein-ligand binding is the hydrophobic effect, which involves the thermodynamically favorable removal of nonpolar surfaces from contact with water. This exercise demonstrates how this effect is commonly estimated in scoring functions using a simple but powerful linear model based on the change in Solvent-Accessible Surface Area (SASA) upon complex formation [@problem_id:4599774].", "problem": "A docking scoring function includes a nonpolar solvation free energy term modeled as the product of a proportionality constant and the solvent-accessible surface area (SASA), plus an offset. The nonpolar solvation free energy is approximated by the expression $$G_{\\mathrm{np}} = \\gamma A + c,$$ where $G_{\\mathrm{np}}$ is the nonpolar solvation free energy, $\\gamma$ is a proportionality constant, $A$ is the solvent-accessible surface area (SASA), and $c$ is an offset constant associated with the solute. Solvent-accessible surface area (SASA) is defined as the area traced by the center of a solvent probe rolled over the van der Waals surface of the solute.\n\nConsider a ligand binding to a protein to form a complex. The SASA of the free protein is $A_{\\mathrm{protein}} = 8.200 \\times 10^{3}\\ \\mathrm{\\AA}^{2}$, the SASA of the free ligand is $A_{\\mathrm{ligand}} = 6.50 \\times 10^{2}\\ \\mathrm{\\AA}^{2}$, and the SASA of the protein-ligand complex is $A_{\\mathrm{complex}} = 7.800 \\times 10^{3}\\ \\mathrm{\\AA}^{2}$. The nonpolar parameter is $\\gamma = 5.42 \\times 10^{-3}\\ \\mathrm{kcal}\\,\\mathrm{mol}^{-1}\\,\\mathrm{\\AA}^{-2}$, and the offset constant is $c = 0.92\\ \\mathrm{kcal}\\,\\mathrm{mol}^{-1}$ per solute. Assume the offset $c$ applies once per distinct solute entity and is the same for the free protein, free ligand, and the complex.\n\nStarting from the definition of the binding free energy change as the free energy of the complex minus the sum of the free energies of the isolated components, derive an expression for the change in nonpolar solvation free energy upon binding, $$\\Delta G_{\\mathrm{np,bind}} = G_{\\mathrm{np}}(\\mathrm{complex}) - G_{\\mathrm{np}}(\\mathrm{protein}) - G_{\\mathrm{np}}(\\mathrm{ligand}),$$ and compute its numerical value. Convert the final energy to $\\mathrm{kJ}\\,\\mathrm{mol}^{-1}$ using $1\\ \\mathrm{kcal} = 4.184\\ \\mathrm{kJ}$. Express your final answer in $\\mathrm{kJ}\\,\\mathrm{mol}^{-1}$ and round to four significant figures.", "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   The model for nonpolar solvation free energy is $G_{\\mathrm{np}} = \\gamma A + c$.\n-   $G_{\\mathrm{np}}$ is the nonpolar solvation free energy.\n-   $\\gamma$ is the proportionality constant, given as $\\gamma = 5.42 \\times 10^{-3}\\ \\mathrm{kcal}\\,\\mathrm{mol}^{-1}\\,\\mathrm{\\AA}^{-2}$.\n-   $A$ is the solvent-accessible surface area (SASA).\n-   $c$ is the offset constant, given as $c = 0.92\\ \\mathrm{kcal}\\,\\mathrm{mol}^{-1}$ per solute.\n-   SASA of the free protein: $A_{\\mathrm{protein}} = 8.200 \\times 10^{3}\\ \\mathrm{\\AA}^{2}$.\n-   SASA of the free ligand: $A_{\\mathrm{ligand}} = 6.50 \\times 10^{2}\\ \\mathrm{\\AA}^{2}$.\n-   SASA of the protein-ligand complex: $A_{\\mathrm{complex}} = 7.800 \\times 10^{3}\\ \\mathrm{\\AA}^{2}$.\n-   The offset $c$ applies once per distinct solute entity (protein, ligand, complex).\n-   The change in nonpolar solvation free energy upon binding is defined as $\\Delta G_{\\mathrm{np,bind}} = G_{\\mathrm{np}}(\\mathrm{complex}) - G_{\\mathrm{np}}(\\mathrm{protein}) - G_{\\mathrm{np}}(\\mathrm{ligand})$.\n-   The conversion factor is $1\\ \\mathrm{kcal} = 4.184\\ \\mathrm{kJ}$.\n-   The final answer must be in units of $\\mathrm{kJ}\\,\\mathrm{mol}^{-1}$ and rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is grounded in the principles of biophysical chemistry and computational biology. The model $G_{\\mathrm{np}} = \\gamma A + c$ represents a standard, albeit simplified, approach for calculating the nonpolar component of solvation free energy in molecular mechanics force fields and docking scoring functions. The concept of SASA and its change upon binding (buried surface area) is fundamental to understanding the hydrophobic effect in protein-ligand interactions. The provided numerical values for SASA are realistic for a typical protein-ligand system, and the value of $\\gamma$ is within the accepted range for such models.\n-   **Well-Posed**: The problem is well-posed. It provides all necessary data, a clear definition of the quantity to be calculated, and explicit instructions for the final output format. A unique solution exists.\n-   **Objective**: The problem is stated in precise, objective, and technical language, free from any subjective or biased content.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is a standard calculation in the field of bioinformatics and medical data analytics. The solution will now be derived.\n\nThe change in nonpolar solvation free energy upon binding, $\\Delta G_{\\mathrm{np,bind}}$, is defined as the difference between the free energy of the products (the complex) and the reactants (the free protein and free ligand).\n$$\n\\Delta G_{\\mathrm{np,bind}} = G_{\\mathrm{np}}(\\mathrm{complex}) - (G_{\\mathrm{np}}(\\mathrm{protein}) + G_{\\mathrm{np}}(\\mathrm{ligand}))\n$$\n$$\n\\Delta G_{\\mathrm{np,bind}} = G_{\\mathrm{np}}(\\mathrm{complex}) - G_{\\mathrm{np}}(\\mathrm{protein}) - G_{\\mathrm{np}}(\\mathrm{ligand})\n$$\nThe problem specifies the model for the nonpolar solvation free energy as $G_{\\mathrm{np}} = \\gamma A + c$, where $\\gamma$ is the surface tension coefficient, $A$ is the solvent-accessible surface area (SASA), and $c$ is an offset constant. The problem explicitly states that $c$ applies once per distinct solute entity. We apply this model to each term in the binding equation:\n-   For the complex (one solute entity): $G_{\\mathrm{np}}(\\mathrm{complex}) = \\gamma A_{\\mathrm{complex}} + c$\n-   For the free protein (one solute entity): $G_{\\mathrm{np}}(\\mathrm{protein}) = \\gamma A_{\\mathrm{protein}} + c$\n-   For the free ligand (one solute entity): $G_{\\mathrm{np}}(\\mathrm{ligand}) = \\gamma A_{\\mathrm{ligand}} + c$\n\nSubstituting these expressions into the equation for $\\Delta G_{\\mathrm{np,bind}}$:\n$$\n\\Delta G_{\\mathrm{np,bind}} = (\\gamma A_{\\mathrm{complex}} + c) - (\\gamma A_{\\mathrm{protein}} + c) - (\\gamma A_{\\mathrm{ligand}} + c)\n$$\nDistributing the negative signs and collecting terms:\n$$\n\\Delta G_{\\mathrm{np,bind}} = \\gamma A_{\\mathrm{complex}} + c - \\gamma A_{\\mathrm{protein}} - c - \\gamma A_{\\mathrm{ligand}} - c\n$$\n$$\n\\Delta G_{\\mathrm{np,bind}} = \\gamma (A_{\\mathrm{complex}} - A_{\\mathrm{protein}} - A_{\\mathrm{ligand}}) + (c - c - c)\n$$\nThis simplifies to:\n$$\n\\Delta G_{\\mathrm{np,bind}} = \\gamma (A_{\\mathrm{complex}} - A_{\\mathrm{protein}} - A_{\\mathrm{ligand}}) - c\n$$\nThe term $(A_{\\mathrm{complex}} - A_{\\mathrm{protein}} - A_{\\mathrm{ligand}})$ represents the change in the total SASA upon binding, which we denote as $\\Delta A_{\\mathrm{bind}}$.\nFirst, we calculate $\\Delta A_{\\mathrm{bind}}$ using the provided values:\n$A_{\\mathrm{protein}} = 8.200 \\times 10^{3}\\ \\mathrm{\\AA}^{2}$\n$A_{\\mathrm{ligand}} = 6.50 \\times 10^{2}\\ \\mathrm{\\AA}^{2} = 0.650 \\times 10^{3}\\ \\mathrm{\\AA}^{2}$\n$A_{\\mathrm{complex}} = 7.800 \\times 10^{3}\\ \\mathrm{\\AA}^{2}$\n$$\n\\Delta A_{\\mathrm{bind}} = (7.800 \\times 10^{3}\\ \\mathrm{\\AA}^{2}) - (8.200 \\times 10^{3}\\ \\mathrm{\\AA}^{2}) - (0.650 \\times 10^{3}\\ \\mathrm{\\AA}^{2})\n$$\n$$\n\\Delta A_{\\mathrm{bind}} = (7.800 - 8.200 - 0.650) \\times 10^{3}\\ \\mathrm{\\AA}^{2} = -1.050 \\times 10^{3}\\ \\mathrm{\\AA}^{2}\n$$\nThis negative value indicates a reduction in solvent-exposed surface area, which is expected upon binding. This reduction is often called the buried surface area (BSA).\n\nNow, we substitute the values of $\\gamma$, $\\Delta A_{\\mathrm{bind}}$, and $c$ into the simplified equation for $\\Delta G_{\\mathrm{np,bind}}$.\n$\\gamma = 5.42 \\times 10^{-3}\\ \\mathrm{kcal}\\,\\mathrm{mol}^{-1}\\,\\mathrm{\\AA}^{-2}$\n$c = 0.92\\ \\mathrm{kcal}\\,\\mathrm{mol}^{-1}$\n$$\n\\Delta G_{\\mathrm{np,bind}} = (5.42 \\times 10^{-3}\\ \\mathrm{kcal}\\,\\mathrm{mol}^{-1}\\,\\mathrm{\\AA}^{-2}) \\times (-1.050 \\times 10^{3}\\ \\mathrm{\\AA}^{2}) - 0.92\\ \\mathrm{kcal}\\,\\mathrm{mol}^{-1}\n$$\nCalculate the first term:\n$$\n(5.42 \\times 10^{-3}) \\times (-1.050 \\times 10^{3}) = (5.42 \\times -1.050) = -5.691\\ \\mathrm{kcal}\\,\\mathrm{mol}^{-1}\n$$\nNow, subtract the offset constant $c$:\n$$\n\\Delta G_{\\mathrm{np,bind}} = -5.691\\ \\mathrm{kcal}\\,\\mathrm{mol}^{-1} - 0.92\\ \\mathrm{kcal}\\,\\mathrm{mol}^{-1} = -6.611\\ \\mathrm{kcal}\\,\\mathrm{mol}^{-1}\n$$\nThe problem requires the final answer in $\\mathrm{kJ}\\,\\mathrm{mol}^{-1}$. We use the conversion factor $1\\ \\mathrm{kcal} = 4.184\\ \\mathrm{kJ}$.\n$$\n\\Delta G_{\\mathrm{np,bind}}[\\mathrm{kJ}\\,\\mathrm{mol}^{-1}] = -6.611\\ \\mathrm{kcal}\\,\\mathrm{mol}^{-1} \\times 4.184\\ \\frac{\\mathrm{kJ}}{\\mathrm{kcal}}\n$$\n$$\n\\Delta G_{\\mathrm{np,bind}}[\\mathrm{kJ}\\,\\mathrm{mol}^{-1}] = -27.660124\\ \\mathrm{kJ}\\,\\mathrm{mol}^{-1}\n$$\nFinally, we round the result to four significant figures as instructed in the problem statement.\n$$\n\\Delta G_{\\mathrm{np,bind}} \\approx -27.66\\ \\mathrm{kJ}\\,\\mathrm{mol}^{-1}\n$$", "answer": "$$\n\\boxed{-27.66}\n$$", "id": "4599774"}, {"introduction": "While physics-based terms are crucial, many successful scoring functions are \"empirical,\" meaning their parameters are optimized to reproduce experimental binding data. This practice guides you through the fundamental statistical process of creating such a function using linear regression. By deriving the normal equations from first principles and applying them to a well-defined dataset, you will gain insight into how different interaction features are weighted to predict binding affinity [@problem_id:4599711].", "problem": "In empirical protein–ligand docking, a linear scoring function is often used to approximate the negative standard binding free energy, denoted $-\\Delta G^{\\circ}$, from a set of engineered features that summarize intermolecular interactions. Consider a linear scoring function of the form\n$$\nE(x; w) \\equiv \\sum_{k=0}^{p-1} w_k\\, f_k(x),\n$$\nwhere $f_0(x) \\equiv 1$ is an intercept feature and $f_k(x)$ for $k \\geq 1$ are precomputed features for complex $x$. Suppose that measurements follow the additive-noise model $y_i = E(x_i; w) + \\varepsilon_i$ for complexes $i \\in \\{1,\\dots,n\\}$, where the $\\varepsilon_i$ are independent, zero-mean fluctuations with finite variance. You are asked to derive, from first principles, the necessary optimality condition for the parameter vector $w$ that minimizes the sum of squared residuals in the sense of Ordinary Least Squares (OLS). Your derivation must start from the definition of the least squares objective in its scalar form and proceed to a rigorous matrix-vector formulation.\n\nThen, using the same linear model $E(x; w)$ with $p = 3$ features (intercept plus two standardized interaction features), fit $w$ to the following training set of $n = 4$ protein–ligand complexes. The features $f_1$ and $f_2$ have been standardized across the training set to have zero mean and are constructed to be uncorrelated with each other and with the intercept:\n- Complex $i=1$: $(f_0, f_1, f_2) = (1, -1, -1)$, measured $y_1 = 10$ in $\\mathrm{kJ/mol}$.\n- Complex $i=2$: $(f_0, f_1, f_2) = (1, -1, 1)$, measured $y_2 = 14$ in $\\mathrm{kJ/mol}$.\n- Complex $i=3$: $(f_0, f_1, f_2) = (1, 1, -1)$, measured $y_3 = 18$ in $\\mathrm{kJ/mol}$.\n- Complex $i=4$: $(f_0, f_1, f_2) = (1, 1, 1)$, measured $y_4 = 22$ in $\\mathrm{kJ/mol}$.\n\nAfter fitting $w$ by least squares, predict the score $E(x^{\\ast}; w)$ (interpretable as $-\\Delta G^{\\circ}$) for a new complex $x^{\\ast}$ with standardized features $(f_0, f_1, f_2) = (1, 0.3, -0.7)$. Round your final numerical prediction to four significant figures. Express the final energy in $\\mathrm{kJ/mol}$.", "solution": "The user-provided problem is evaluated against the established validation criteria.\n\n### Step 1: Extract Givens\n-   **Model**: A linear scoring function $E(x; w) \\equiv \\sum_{k=0}^{p-1} w_k\\, f_k(x)$ approximates the negative standard binding free energy, $-\\Delta G^{\\circ}$.\n-   **Intercept**: The first feature is an intercept, $f_0(x) \\equiv 1$.\n-   **Data Model**: Measurements follow an additive-noise model, $y_i = E(x_i; w) + \\varepsilon_i$, for $i \\in \\{1,\\dots,n\\}$.\n-   **Noise Properties**: The noise terms $\\varepsilon_i$ are independent, zero-mean fluctuations with finite variance.\n-   **Objective**: Minimize the sum of squared residuals (Ordinary Least Squares).\n-   **Task 1**: Derive the necessary optimality condition for the parameter vector $w$ from first principles, starting from the scalar sum of squared residuals and proceeding to a matrix-vector formulation.\n-   **Task 2**: Apply the method to a specific dataset.\n    -   Number of features, $p=3$.\n    -   Number of complexes, $n=4$.\n    -   Feature properties: $f_1$ and $f_2$ are standardized to have zero mean and are uncorrelated with each other and with the intercept.\n    -   Training data:\n        -   Complex $i=1$: feature vector $(f_0, f_1, f_2) = (1, -1, -1)$; measured $y_1 = 10 \\ \\mathrm{kJ/mol}$.\n        -   Complex $i=2$: feature vector $(f_0, f_1, f_2) = (1, -1, 1)$; measured $y_2 = 14 \\ \\mathrm{kJ/mol}$.\n        -   Complex $i=3$: feature vector $(f_0, f_1, f_2) = (1, 1, -1)$; measured $y_3 = 18 \\ \\mathrm{kJ/mol}$.\n        -   Complex $i=4$: feature vector $(f_0, f_1, f_2) = (1, 1, 1)$; measured $y_4 = 22 \\ \\mathrm{kJ/mol}$.\n    -   Prediction task: Predict the score $E(x^{\\ast}; w)$ for a new complex $x^{\\ast}$ with features $(f_0, f_1, f_2) = (1, 0.3, -0.7)$.\n    -   Rounding: The final numerical prediction must be rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem describes the use of Ordinary Least Squares (OLS) to fit a linear model, a fundamental and widely-used technique in statistics and machine learning. Its application to building empirical scoring functions for protein-ligand docking is a standard practice in bioinformatics and computational chemistry. The problem is scientifically sound.\n-   **Well-Posed**: The problem is well-posed. The first part asks for a standard derivation in linear regression theory. The second-part provides a complete dataset for a specific case. The feature matrix for the given data is of full rank, which guarantees the existence of a unique solution for the OLS problem.\n-   **Objective**: The problem is stated using precise mathematical and scientific language. It is free from ambiguity, subjectivity, and non-formalizable concepts.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically grounded, well-posed, and objective. A complete solution will be provided.\n\n### Derivation of the Optimality Condition (Normal Equations)\n\nThe goal is to find the parameter vector $w$ that minimizes the sum of squared residuals (SSR). The residual for the $i$-th observation is the difference between the measured value $y_i$ and the value predicted by the model, $E(x_i; w)$.\n\nThe SSR, denoted by $S(w)$, is defined as:\n$$\nS(w) = \\sum_{i=1}^{n} (y_i - E(x_i; w))^2 = \\sum_{i=1}^{n} \\left(y_i - \\sum_{k=0}^{p-1} w_k f_k(x_i)\\right)^2\n$$\nTo find the minimum of this function, we must find the point where its gradient with respect to $w$ is the zero vector. This is the necessary optimality condition. We compute the partial derivative of $S(w)$ with respect to each parameter $w_j$ for $j \\in \\{0, 1, \\dots, p-1\\}$.\n\nUsing the chain rule:\n$$\n\\frac{\\partial S}{\\partial w_j} = \\sum_{i=1}^{n} 2 \\left(y_i - \\sum_{k=0}^{p-1} w_k f_k(x_i)\\right) \\cdot \\frac{\\partial}{\\partial w_j} \\left(y_i - \\sum_{k=0}^{p-1} w_k f_k(x_i)\\right)\n$$\nThe derivative of the inner term is:\n$$\n\\frac{\\partial}{\\partial w_j} \\left(y_i - \\sum_{k=0}^{p-1} w_k f_k(x_i)\\right) = 0 - \\frac{\\partial}{\\partial w_j} (w_0 f_0(x_i) + \\dots + w_j f_j(x_i) + \\dots) = -f_j(x_i)\n$$\nSubstituting this back, we get:\n$$\n\\frac{\\partial S}{\\partial w_j} = \\sum_{i=1}^{n} 2 \\left(y_i - \\sum_{k=0}^{p-1} w_k f_k(x_i)\\right) (-f_j(x_i)) = -2 \\sum_{i=1}^{n} f_j(x_i) \\left(y_i - \\sum_{k=0}^{p-1} w_k f_k(x_i)\\right)\n$$\nSetting this partial derivative to zero for each $j$:\n$$\n\\sum_{i=1}^{n} f_j(x_i) \\left(y_i - \\sum_{k=0}^{p-1} w_k f_k(x_i)\\right) = 0\n$$\nRearranging the terms:\n$$\n\\sum_{i=1}^{n} f_j(x_i) y_i = \\sum_{i=1}^{n} f_j(x_i) \\left(\\sum_{k=0}^{p-1} w_k f_k(x_i)\\right)\n$$\nThis equation can be rewritten as:\n$$\n\\sum_{k=0}^{p-1} \\left(\\sum_{i=1}^{n} f_j(x_i) f_k(x_i)\\right) w_k = \\sum_{i=1}^{n} f_j(x_i) y_i\n$$\nThis represents a system of $p$ linear equations in $p$ unknowns ($w_0, \\dots, w_{p-1}$).\n\nTo express this in matrix-vector form, we define:\n-   The design matrix $X$, an $n \\times p$ matrix where the element $X_{ik} = f_k(x_i)$.\n-   The response vector $y$, an $n \\times 1$ column vector with elements $y_i$.\n-   The parameter vector $w$, a $p \\times 1$ column vector with elements $w_k$.\n$$\nX = \\begin{pmatrix}\nf_0(x_1) & \\dots & f_{p-1}(x_1) \\\\\n\\vdots & \\ddots & \\vdots \\\\\nf_0(x_n) & \\dots & f_{p-1}(x_n)\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix}, \\quad\nw = \\begin{pmatrix} w_0 \\\\ \\vdots \\\\ w_{p-1} \\end{pmatrix}\n$$\nThe vector of predicted values is $\\hat{y} = Xw$, and the vector of residuals is $r = y - \\hat{y} = y - Xw$.\nThe SSR is the squared Euclidean norm of the residual vector:\n$$\nS(w) = r^T r = (y - Xw)^T (y - Xw)\n$$\nExpanding this expression:\n$$\nS(w) = (y^T - w^T X^T)(y - Xw) = y^T y - y^T X w - w^T X^T y + w^T X^T X w\n$$\nSince $w^T X^T y$ is a scalar, it is equal to its transpose, $(w^T X^T y)^T = y^T X w$. So we can combine the middle two terms:\n$$\nS(w) = y^T y - 2 w^T X^T y + w^T X^T X w\n$$\nTaking the gradient with respect to the vector $w$ (using matrix calculus identities $\\nabla_v(v^T a) = a$ and $\\nabla_v(v^T A v) = 2Av$ for symmetric $A$, noting that $X^T X$ is symmetric):\n$$\n\\nabla_w S(w) = -2X^T y + 2X^T X w\n$$\nSetting the gradient to zero gives the necessary condition for a minimum:\n$$\n-2X^T y + 2X^T X w = 0 \\implies X^T X w = X^T y\n$$\nThis is the celebrated Normal Equation, which is the matrix-vector formulation of the optimality condition.\n\n### Application to the Dataset\n\nFor the given problem, $n=4$ and $p=3$. The design matrix $X$ and the response vector $y$ are constructed from the provided data:\n$$\nX = \\begin{pmatrix}\n1 & -1 & -1 \\\\\n1 & -1 & 1 \\\\\n1 & 1 & -1 \\\\\n1 & 1 & 1\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix} 10 \\\\ 14 \\\\ 18 \\\\ 22 \\end{pmatrix}\n$$\nThe parameter vector to be found is $w = (w_0, w_1, w_2)^T$. We now solve the normal equation $X^T X w = X^T y$.\n\nFirst, compute $X^T X$:\n$$\nX^T X = \\begin{pmatrix}\n1 & 1 & 1 & 1 \\\\\n-1 & -1 & 1 & 1 \\\\\n-1 & 1 & -1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & -1 & -1 \\\\\n1 & -1 & 1 \\\\\n1 & 1 & -1 \\\\\n1 & 1 & 1\n\\end{pmatrix} = \\begin{pmatrix}\n4 & 0 & 0 \\\\\n0 & 4 & 0 \\\\\n0 & 0 & 4\n\\end{pmatrix} = 4I_3\n$$\nThe matrix $X^T X$ is diagonal because the columns of $X$ are mutually orthogonal, which is a consequence of the features $f_1$ and $f_2$ being mean-centered and uncorrelated on this training set.\n\nNext, compute $X^T y$:\n$$\nX^T y = \\begin{pmatrix}\n1 & 1 & 1 & 1 \\\\\n-1 & -1 & 1 & 1 \\\\\n-1 & 1 & -1 & 1\n\\end{pmatrix}\n\\begin{pmatrix} 10 \\\\ 14 \\\\ 18 \\\\ 22 \\end{pmatrix} = \\begin{pmatrix}\n1(10) + 1(14) + 1(18) + 1(22) \\\\\n-1(10) - 1(14) + 1(18) + 1(22) \\\\\n-1(10) + 1(14) - 1(18) + 1(22)\n\\end{pmatrix} = \\begin{pmatrix}\n64 \\\\\n16 \\\\\n8\n\\end{pmatrix}\n$$\nThe normal equation becomes:\n$$\n\\begin{pmatrix}\n4 & 0 & 0 \\\\\n0 & 4 & 0 \\\\\n0 & 0 & 4\n\\end{pmatrix}\n\\begin{pmatrix} w_0 \\\\ w_1 \\\\ w_2 \\end{pmatrix} =\n\\begin{pmatrix} 64 \\\\ 16 \\\\ 8 \\end{pmatrix}\n$$\nThis system is easily solved:\n-   $4w_0 = 64 \\implies w_0 = 16$\n-   $4w_1 = 16 \\implies w_1 = 4$\n-   $4w_2 = 8 \\implies w_2 = 2$\n\nSo, the optimal parameter vector is $w = (16, 4, 2)^T$. The fitted scoring function is $E(x; w) = 16 + 4f_1(x) + 2f_2(x)$.\n\n### Prediction for a New Complex\n\nWe are asked to predict the score for a new complex $x^{\\ast}$ with a feature vector $f^{\\ast} = (f_0, f_1, f_2)^T = (1, 0.3, -0.7)^T$. The predicted score is $E(x^{\\ast}; w) = (f^{\\ast})^T w$.\n$$\nE(x^{\\ast}; w) = \\begin{pmatrix} 1 & 0.3 & -0.7 \\end{pmatrix} \\begin{pmatrix} 16 \\\\ 4 \\\\ 2 \\end{pmatrix}\n$$\n$$\nE(x^{\\ast}; w) = 1 \\cdot 16 + 0.3 \\cdot 4 + (-0.7) \\cdot 2 = 16 + 1.2 - 1.4 = 15.8\n$$\nThe problem specifies that the final numerical prediction must be rounded to four significant figures. The predicted energy is $15.8 \\ \\mathrm{kJ/mol}$. Expressed with four significant figures, this is $15.80 \\ \\mathrm{kJ/mol}$.", "answer": "$$\n\\boxed{15.80}\n$$", "id": "4599711"}, {"introduction": "A predictive model is only as good as its validation. Simply fitting a model to available data is insufficient; we must rigorously assess its ability to generalize to new, unseen examples. This coding practice provides direct experience with k-fold cross-validation, a cornerstone technique for robust model evaluation that helps prevent overfitting. By implementing this procedure and calculating both accuracy (RMSE) and ranking (Spearman correlation) metrics, you will learn to build a more complete picture of a scoring function's practical strengths and weaknesses [@problem_id:4599727].", "problem": "You are evaluating a simple linear protein–ligand docking scoring function that predicts binding affinity from feature vectors. The goal is to implement $k$-fold cross-validation, compute the root mean squared error (RMSE) and the Spearman rank correlation coefficient, and then detect cases where ranking quality and absolute accuracy diverge. You must use a linear model without an intercept term, trained by minimizing the sum of squared errors. Your program must be a complete, runnable program that takes no input and prints the required outputs.\n\nFundamental base and definitions:\n- Given feature vectors $\\{\\mathbf{x}_i\\}_{i=1}^n$ and affinities $\\{y_i\\}_{i=1}^n$, consider the linear scoring function $f(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x}$, with $\\mathbf{w} \\in \\mathbb{R}^d$. The model parameters $\\mathbf{w}$ are learned by minimizing the sum of squared errors $\\sum_{i=1}^n \\left(y_i - \\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^2$. In cross-validation, this minimization is performed on training folds only. The minimizer can be obtained by solving the normal equations or equivalently by using the Moore–Penrose pseudoinverse.\n- The $k$-fold cross-validation procedure partitions the indices $\\{0,1,\\dots,n-1\\}$ into $k$ disjoint test sets. For each fold, train on the complement of the test set and predict on the test set. Aggregate predictions across all folds so that each sample $i$ has exactly one cross-validated prediction $\\hat{y}_i$.\n- The root mean squared error is defined as $\\mathrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=0}^{n-1} (y_i - \\hat{y}_i)^2}$.\n- The Spearman rank correlation coefficient is the Pearson correlation coefficient between the rank-transformed vectors of $\\{y_i\\}$ and $\\{\\hat{y}_i\\}$. Ties must be handled by assigning average ranks. If either rank vector has zero variance, define the Spearman correlation to be $0$.\n\nDiscrepancy detection between ranking and absolute error metrics:\n- Define two thresholds $T_{\\text{high}}$ and $T_{\\text{low}}$.\n- Define a discrepancy indicator $d$ by\n  $$d = \\begin{cases}\n  1, & \\text{if } \\rho_s \\ge 0.9 \\text{ and } \\mathrm{RMSE} \\ge T_{\\text{high}}, \\\\\n  1, & \\text{if } \\rho_s \\le 0.1 \\text{ and } \\mathrm{RMSE} \\le T_{\\text{low}}, \\\\\n  0, & \\text{otherwise,}\n  \\end{cases}$$\n  where $\\rho_s$ is the Spearman rank correlation.\n- Use $T_{\\text{high}} = 5.0$ and $T_{\\text{low}} = 0.2$.\n\nImplementation requirements:\n- Implement $k$-fold cross-validation exactly as specified by the provided test folds for each test case.\n- Use a linear model without an intercept term.\n- For fitting on each training set, compute $\\mathbf{w}$ using the Moore–Penrose pseudoinverse to minimize the squared error on the training data.\n- Compute the aggregated cross-validated $\\mathrm{RMSE}$ and Spearman rank correlation over the entire dataset for each test case.\n- For each test case, output a list containing three items: the $\\mathrm{RMSE}$ rounded to three decimal places, the Spearman correlation rounded to three decimal places, and the discrepancy indicator as a boolean ($\\mathrm{True}$ for $d=1$, $\\mathrm{False}$ for $d=0$).\n\nTest suite:\nProvide four independent test cases. For each case, you are given the feature matrix $X \\in \\mathbb{R}^{n \\times d}$, the affinity vector $y \\in \\mathbb{R}^{n}$, and explicit test index folds. Indices are $0$-based. All models are trained without an intercept.\n\n- Test case $1$ (high $\\rho_s$, high $\\mathrm{RMSE}$ due to missing intercept):\n  - $X = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\end{bmatrix}$, $y = \\begin{bmatrix} 11 \\\\ 12 \\\\ 13 \\\\ 14 \\\\ 15 \\\\ 16 \\end{bmatrix}$.\n  - Folds: $\\big[\\,[0,1],\\,[2,3],\\,[4,5]\\,\\big]$.\n\n- Test case $2$ (low $\\rho_s$, low $\\mathrm{RMSE}$ due to sign mismatch on small-magnitude affinities):\n  - $X = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$, $y = \\begin{bmatrix} 0.04 \\\\ 0.08 \\\\ 0.12 \\\\ -0.04 \\\\ -0.08 \\\\ -0.12 \\end{bmatrix}$.\n  - Folds: $\\big[\\,[0,1,2],\\,[3,4,5]\\,\\big]$.\n\n- Test case $3$ (boundary case: zero variance in true ranks):\n  - $X = \\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{bmatrix}$, $y = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$.\n  - Folds: $\\big[\\,[0,1],\\,[2,3]\\,\\big]$.\n\n- Test case $4$ (happy path: high $\\rho_s$, low $\\mathrm{RMSE}$):\n  - $X = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$, $y = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$.\n  - Folds: $\\big[\\,[0,1],\\,[2,3]\\,\\big]$.\n\nFinal output format:\n- Your program should produce a single line of output containing a comma-separated list of the four per-case results, where each per-case result is a three-element list in the form $[\\mathrm{RMSE}, \\rho_s, d]$ with the two floating-point values rounded to three decimal places and the boolean unquoted. For example: $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3],[a_4,b_4,c_4]]$.", "solution": "The user has provided a well-defined computational problem in the domain of bioinformatics. The task is to evaluate a simple linear scoring function using $k$-fold cross-validation and to identify discrepancies between different performance metrics.\n\n### Step 1: Extract Givens\n\n1.  **Model**: A linear scoring function without an intercept term, $f(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x}$.\n2.  **Training**: The weight vector $\\mathbf{w}$ is found by minimizing the sum of squared errors on the training data, $\\sum_{i \\in \\text{train}} \\left(y_i - \\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^2$. This is to be solved using the Moore-Penrose pseudoinverse.\n3.  **Validation**: $k$-fold cross-validation using provided fold partitions. Predictions are aggregated across all folds.\n4.  **Metrics**:\n    -   Root Mean Squared Error (RMSE): $\\mathrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=0}^{n-1} (y_i - \\hat{y}_i)^2}$, where $\\hat{y}_i$ are the cross-validated predictions.\n    -   Spearman Rank Correlation Coefficient ($\\rho_s$): The Pearson correlation between the rank-transformed true affinities $\\{y_i\\}$ and predicted affinities $\\{\\hat{y}_i\\}$. Ties are handled by average ranks. If either rank vector has zero variance, $\\rho_s$ is defined as $0$.\n5.  **Discrepancy Detection**: A discrepancy indicator $d$ is defined as:\n    $$d = \\begin{cases}\n    1, & \\text{if } \\rho_s \\ge 0.9 \\text{ and } \\mathrm{RMSE} \\ge T_{\\text{high}}, \\\\\n    1, & \\text{if } \\rho_s \\le 0.1 \\text{ and } \\mathrm{RMSE} \\le T_{\\text{low}}, \\\\\n    0, & \\text{otherwise.}\n    \\end{cases}$$\n6.  **Constants**: Thresholds $T_{\\text{high}} = 5.0$ and $T_{\\text{low}} = 0.2$.\n7.  **Test Cases**: Four specific test cases are provided, each with a feature matrix $X$, an affinity vector $y$, and a list of test folds.\n    -   Case 1: $X = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\end{bmatrix}$, $y = \\begin{bmatrix} 11 \\\\ 12 \\\\ 13 \\\\ 14 \\\\ 15 \\\\ 16 \\end{bmatrix}$, Folds: $\\big[\\,[0,1],\\,[2,3],\\,[4,5]\\,\\big]$.\n    -   Case 2: $X = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$, $y = \\begin{bmatrix} 0.04 \\\\ 0.08 \\\\ 0.12 \\\\ -0.04 \\\\ -0.08 \\\\ -0.12 \\end{bmatrix}$, Folds: $\\big[\\,[0,1,2],\\,[3,4,5]\\,\\big]$.\n    -   Case 3: $X = \\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{bmatrix}$, $y = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$, Folds: $\\big[\\,[0,1],\\,[2,3]\\,\\big]$.\n    -   Case 4: $X = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$, $y = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$, Folds: $\\big[\\,[0,1],\\,[2,3]\\,\\big]$.\n8.  **Output Format**: A single-line list of lists, `[[rmse1, rho1, d1], [rmse2, rho2, d2], ...]`, with floats rounded to three decimal places and the discrepancy indicator as an unquoted boolean.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is reviewed against the specified criteria:\n-   **Scientifically Grounded**: The problem describes a standard machine learning methodology ($k$-fold cross-validation) for model evaluation, a fundamental practice in computational sciences. The linear model, RMSE, and Spearman correlation are all elementary concepts in statistics and machine learning.\n-   **Well-Posed**: The problem is well-posed. All necessary data ($X$, $y$, folds), definitions (RMSE, $\\rho_s$, $d$), and procedures (training via pseudoinverse, cross-validation) are explicitly provided. The test cases are concrete and allow for a unique solution to be computed.\n-   **Objective**: The language is precise and quantitative. All instructions are clear and unambiguous.\n-   **No Flaws**: The problem does not exhibit any of the invalidity flaws. It is self-contained, factually sound, formalizable, and computationally feasible. The test cases are designed to test specific behaviors of the metrics, which is a common practice in algorithm validation.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A solution will be developed and implemented as requested.\n\n### Solution Derivation\n\nThe solution requires implementing a computational workflow for each test case. This workflow consists of cross-validation, metric calculation, and discrepancy detection.\n\n**1. Model Training**\nThe model is a linear function without an intercept: $f(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x}$. Given a training set with feature matrix $\\mathbf{X}_{\\text{train}}$ (where each row is a feature vector $\\mathbf{x}_i^{\\top}$) and target vector $\\mathbf{y}_{\\text{train}}$, we aim to find the weight vector $\\mathbf{w}$ that minimizes the sum of squared errors (SSE), $L(\\mathbf{w}) = ||\\mathbf{y}_{\\text{train}} - \\mathbf{X}_{\\text{train}}\\mathbf{w}||_2^2$. The solution to this linear least squares problem is given by the normal equations:\n$$ \\mathbf{X}_{\\text{train}}^{\\top}\\mathbf{X}_{\\text{train}}\\mathbf{w} = \\mathbf{X}_{\\text{train}}^{\\top}\\mathbf{y}_{\\text{train}} $$\nThe weight vector $\\mathbf{w}$ can be solved for as:\n$$ \\mathbf{w} = (\\mathbf{X}_{\\text{train}}^{\\top}\\mathbf{X}_{\\text{train}})^{-1}\\mathbf{X}_{\\text{train}}^{\\top}\\mathbf{y}_{\\text{train}} $$\nThe term $(\\mathbf{X}_{\\text{train}}^{\\top}\\mathbf{X}_{\\text{train}})^{-1}\\mathbf{X}_{\\text{train}}^{\\top}$ is the Moore-Penrose pseudoinverse of $\\mathbf{X}_{\\text{train}}$, denoted $\\mathbf{X}_{\\text{train}}^{+}$. Thus, the training step simplifies to:\n$$ \\mathbf{w} = \\mathbf{X}_{\\text{train}}^{+} \\mathbf{y}_{\\text{train}} $$\nThis approach is robust even when $\\mathbf{X}_{\\text{train}}^{\\top}\\mathbf{X}_{\\text{train}}$ is singular.\n\n**2. $k$-Fold Cross-Validation**\nThe dataset is partitioned into $k$ folds. For each fold $j \\in \\{1, \\dots, k\\}$:\n-   The indices in fold $j$ constitute the test set.\n-   All other indices form the training set.\n-   The model is trained using the training set data $(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}})$ to obtain a weight vector $\\mathbf{w}_j$.\n-   Predictions are made on the test set features $\\mathbf{X}_{\\text{test}}$: $\\hat{\\mathbf{y}}_{\\text{test}} = \\mathbf{X}_{\\text{test}} \\mathbf{w}_j$.\nThis process is repeated for all $k$ folds. The predictions for each test set are collected. Since the test folds are disjoint and their union covers the entire dataset, we obtain a single prediction $\\hat{y}_i$ for every sample $i$ in the original dataset. These aggregated predictions form the vector $\\hat{\\mathbf{y}}$.\n\n**3. Performance Metrics**\nOnce the aggregated prediction vector $\\hat{\\mathbf{y}}$ is obtained, we compute the two required metrics.\n\n-   **Root Mean Squared Error (RMSE)**: A measure of the average magnitude of the prediction errors. It is calculated as:\n    $$ \\mathrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=0}^{n-1} (y_i - \\hat{y}_i)^2} $$\n    where $n$ is the total number of samples.\n\n-   **Spearman Rank Correlation Coefficient ($\\rho_s$)**: A non-parametric measure of rank correlation, assessing how well the relationship between two variables can be described using a monotonic function. It is calculated as the Pearson correlation coefficient of the rank-transformed variables.\n    Let $R(\\mathbf{y})$ be the vector of ranks of the elements in $\\mathbf{y}$, and $R(\\hat{\\mathbf{y}})$ be the vector of ranks for $\\hat{\\mathbf{y}}$. Ties are resolved by assigning the average of the ranks that would have been assigned to the tied values.\n    The Pearson correlation coefficient $r$ between two vectors $\\mathbf{u}$ and $\\mathbf{v}$ of length $n$ is:\n    $$ r(\\mathbf{u}, \\mathbf{v}) = \\frac{\\sum_{i=1}^n (u_i - \\bar{u})(v_i - \\bar{v})}{\\sqrt{\\sum_{i=1}^n (u_i - \\bar{u})^2} \\sqrt{\\sum_{i=1}^n (v_i - \\bar{v})^2}} $$\n    where $\\bar{u}$ and $\\bar{v}$ are the means of the vectors.\n    Then, $\\rho_s = r(R(\\mathbf{y}), R(\\hat{\\mathbf{y}}))$. As per the problem specification, if the variance of either $R(\\mathbf{y})$ or $R(\\hat{\\mathbf{y}})$ is zero (i.e., all values in the original vector are identical), $\\rho_s$ is set to $0$.\n\n**4. Discrepancy Indicator ($d$)**\nThe indicator $d$ is a boolean flag that is set to true ($1$) if the ranking performance and absolute error are at odds.\n-   **Case 1 (Good ranking, poor accuracy)**: A high Spearman correlation ($\\rho_s \\ge 0.9$) indicates that the model correctly orders the samples, but a high RMSE ($\\ge T_{\\text{high}}=5.0$) indicates large absolute prediction errors. This often occurs when there is a systematic error, such as a missing intercept in a model for data with a large offset from the origin.\n-   **Case 2 (Poor ranking, good accuracy)**: A low Spearman correlation ($\\rho_s \\le 0.1$) indicates the model fails to capture the correct ordering, but a low RMSE ($\\le T_{\\text{low}}=0.2$) suggests the absolute errors are small. This can happen when the affinities themselves are small, so even significant mispredictions (e.g., wrong sign) result in small absolute errors.\n\nThe final implementation will encapsulate this logic, apply it to each of the four test cases, and format the output as specified.\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import rankdata\n\ndef solve():\n    \"\"\"\n    Solves the protein docking scoring function evaluation problem.\n    \"\"\"\n    \n    # Define thresholds from the problem statement.\n    T_high = 5.0\n    T_low = 0.2\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"X\": np.array([[1], [2], [3], [4], [5], [6]]),\n            \"y\": np.array([11, 12, 13, 14, 15, 16]),\n            \"folds\": [[0, 1], [2, 3], [4, 5]]\n        },\n        {\n            \"X\": np.array([[1], [2], [3], [1], [2], [3]]),\n            \"y\": np.array([0.04, 0.08, 0.12, -0.04, -0.08, -0.12]),\n            \"folds\": [[0, 1, 2], [3, 4, 5]]\n        },\n        {\n            \"X\": np.array([[-1], [0], [1], [2]]),\n            \"y\": np.array([0.5, 0.5, 0.5, 0.5]),\n            \"folds\": [[0, 1], [2, 3]]\n        },\n        {\n            \"X\": np.array([[1], [2], [3], [4]]),\n            \"y\": np.array([1, 2, 3, 4]),\n            \"folds\": [[0, 1], [2, 3]]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case[\"X\"]\n        y = case[\"y\"]\n        folds = case[\"folds\"]\n        n_samples = X.shape[0]\n        all_indices = np.arange(n_samples)\n        \n        y_hat_all = np.zeros(n_samples)\n\n        # k-fold cross-validation\n        for test_indices in folds:\n            train_indices = np.setdiff1d(all_indices, test_indices)\n            \n            X_train, y_train = X[train_indices], y[train_indices]\n            X_test = X[test_indices]\n            \n            # Train model using Moore-Penrose pseudoinverse\n            w = np.linalg.pinv(X_train) @ y_train\n            \n            # Predict on the test set\n            y_hat_test = X_test @ w\n            \n            # Store predictions\n            y_hat_all[test_indices] = y_hat_test\n            \n        # Calculate RMSE\n        rmse = np.sqrt(np.mean((y - y_hat_all)**2))\n        \n        # Calculate Spearman rank correlation\n        rank_y = rankdata(y, method='average')\n        rank_y_hat = rankdata(y_hat_all, method='average')\n        \n        # Handle zero variance case as per problem description\n        if np.var(rank_y) == 0 or np.var(rank_y_hat) == 0:\n            spearman_rho = 0.0\n        else:\n            spearman_rho = np.corrcoef(rank_y, rank_y_hat)[0, 1]\n            \n        # Detect discrepancy\n        discrepancy = (spearman_rho >= 0.9 and rmse >= T_high) or \\\n                      (spearman_rho <= 0.1 and rmse <= T_low)\n                      \n        results.append([rmse, spearman_rho, discrepancy])\n\n    # Format the final output string\n    output_parts = []\n    for r in results:\n        # Format the numbers to 3 decimal places and the boolean as True/False\n        # Python's str() for a boolean correctly produces 'True' or 'False'\n        output_parts.append(f\"[{r[0]:.3f},{r[1]:.3f},{str(r[2])}]\")\n    \n    final_output_str = f\"[{','.join(output_parts)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output_str)\n\nsolve()\n```", "answer": "[[6.533,1.000,True],[0.173,-1.000,True],[1.017,0.000,False],[0.000,1.000,False]]", "id": "4599727"}]}