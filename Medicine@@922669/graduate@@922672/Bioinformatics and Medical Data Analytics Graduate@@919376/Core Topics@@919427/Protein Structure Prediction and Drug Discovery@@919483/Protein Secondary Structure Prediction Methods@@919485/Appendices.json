{"hands_on_practices": [{"introduction": "This first exercise explores a foundational concept in many bioinformatics algorithms: the \"sliding window\" approach. By considering the trade-offs of using a small versus a large window, you will develop an intuition for how the scale of local information impacts prediction accuracy, particularly at the boundaries of structural elements [@problem_id:2135783]. This principle of balancing signal strength against boundary resolution is a recurring theme in sequence analysis.", "problem": "In the field of bioinformatics, early methods for predicting protein secondary structure often relied on a \"sliding window\" approach. For each amino acid residue in a protein sequence, the algorithm would consider that residue as the center of a window of a fixed, odd-numbered size, $W$. The secondary structure state (e.g., alpha-helix, beta-sheet, or coil) of the central residue was then predicted based on the statistical propensities of the $W$ amino acids contained within that window.\n\nTwo research groups are developing competing prediction algorithms based on this principle. Group A argues that a small window size, such as $W=7$, is optimal. Group B contends that a much larger window size, for instance $W=17$, will yield better results. They are testing their methods on a set of proteins known to contain long, stable alpha-helical segments connected by short, flexible coil regions.\n\nWhich of the following statements most accurately describes the fundamental trade-off associated with the choice of window size and the likely performance of the two groups' methods on the test proteins?\n\nA. Group B's method with a larger window will be superior in all aspects because processing more sequence information always leads to a more statistically significant and accurate prediction for every residue.\n\nB. Group A's method with a smaller window will be more effective at correctly identifying residues deep within the core of the long alpha-helical segments, as it avoids the \"noise\" from distant residues in the connecting coil regions.\n\nC. Group B's method with a larger window will likely be more accurate at predicting the helical state for residues in the middle of the long alpha-helices, but it will perform poorly at identifying the exact starting and ending residues of those helices.\n\nD. Group A's method with a smaller window is fundamentally better because secondary structure is almost exclusively determined by interactions between adjacent residues, making the information from residues outside a small window irrelevant.\n\nE. The choice of window size is a minor parameter; the overall prediction accuracy for both Group A and Group B will be nearly identical and will depend almost entirely on the quality of the statistical scoring scheme they implement.", "solution": "The problem asks us to evaluate the trade-offs involved in selecting a window size for a simple, statistics-based protein secondary structure prediction method. The core of the problem lies in understanding how the amount of local sequence context (determined by the window size) affects the prediction quality for different parts of a secondary structure element.\n\nLet's analyze the consequences of using a small window versus a large window.\n\n**Analysis of a Small Window (e.g., $W=7$, Group A's method):**\nA small window uses information from only a few residues on either side of the central residue being predicted.\n- **Advantage:** When the window is positioned at the boundary between two different structural types (e.g., where an alpha-helix ends and a coil begins), a small window is advantageous. It can precisely capture the local sequence features that signal a termination of one structure and the beginning of another. For a residue at the exact C-terminus of a helix, a small window will contain a mix of helix-prone residues upstream and coil-prone residues downstream, providing a distinct signal for a \"helix end\" state. Therefore, smaller windows are generally better at delineating the precise termini of secondary structure elements.\n- **Disadvantage:** A small window incorporates less statistical information. The prediction is highly sensitive to the identity of just a few amino acids and can be easily thrown off by a single residue that has an atypical propensity for the surrounding structural element. More importantly, it may fail to capture slightly longer-range patterns that are crucial for the stability of secondary structures. For example, an alpha-helix is stabilized by hydrogen bonds between the backbone atoms of residue $i$ and residue $i+4$. A window of size 7 centered on residue $i$ only extends to $i+3$ and may not fully capture this essential pattern. This makes predictions for residues in the stable core of a secondary structure element less reliable.\n\n**Analysis of a Large Window (e.g., $W=17$, Group B's method):**\nA large window averages the propensities of a much larger stretch of the sequence.\n- **Advantage:** When the window is centered deep within a long, uniform secondary structure element (like the middle of a long alpha-helix), it will be filled entirely with residues that have a high propensity for that structure. This provides a very strong, clear, and statistically robust signal. The prediction is less susceptible to noise from one or two aberrant residues. Therefore, large windows are generally better at correctly identifying the state of residues in the stable core of long helices and sheets.\n- **Disadvantage:** The main drawback of a large window becomes apparent at the boundaries. Consider a central residue that is the last residue of a helix. A large window of size 17 will extend 8 residues into the helix and 8 residues into the subsequent coil region. The prediction for the central residue will be based on an \"average\" of a strong helix signal and a strong coil signal. This \"smearing\" or \"over-smoothing\" effect often leads to an incorrect prediction for the boundary residue itself and for its immediate neighbors. The algorithm will fail to detect the sharp transition and will often incorrectly extend the helix prediction into the coil region or vice versa.\n\n**Evaluating the Options:**\n\n- **A:** This statement is incorrect. \"Superior in all aspects\" and \"always leads\" are absolute claims that ignore the boundary prediction problem inherent in large windows.\n- **B:** This statement is incorrect. It reverses the roles. A small window is generally *less* effective at identifying the core of long elements because it has less statistical power and may miss longer-range stabilizing patterns. Large windows are better for core regions.\n- **C:** This statement correctly captures the trade-off. A large window (Group B) provides more statistical power, improving predictions for the core of long elements (\"middle of the long alpha-helices\"). However, this same averaging property (\"smearing\") makes it poor at resolving sharp transitions, thus it will be \"less accurate at identifying the exact starting and ending residues.\" This aligns perfectly with our analysis.\n- **D:** This statement is incorrect. While local interactions are very important, interactions are not limited to immediately adjacent residues (e.g., the $i, i+4$ pattern in helices). Claiming that information outside a small window is \"irrelevant\" is a false oversimplification.\n- **E:** This statement is incorrect. The window size is a critical hyperparameter that defines the amount and type of information the algorithm uses. Changing it from 7 to 17 will have a dramatic and predictable effect on the algorithm's performance characteristics, as described in the trade-off above. It is not a \"minor parameter.\"\n\nTherefore, statement C provides the most accurate and nuanced description of the situation.", "answer": "$$\\boxed{C}$$", "id": "2135783"}, {"introduction": "Moving from general principles to a specific quantitative model, this problem asks you to apply Bayesian inference to predict an $\\alpha$-helix start site. You will use empirically derived probabilities for local sequence features, known as capping motifs, to calculate a posterior probability [@problem_id:4601343]. This practice demonstrates how a Naive Bayes classifier, a cornerstone of early machine learning, can be used to integrate multiple sources of biological evidence into a single predictive score.", "problem": "In a protein secondary structure prediction pipeline, a binary latent variable $H$ marks whether a given position $i$ is the N-terminal boundary (start) of an $\\alpha$-helix ($H=1$) or not ($H=0$). Capping motifs and local boundary effects are modeled as observable binary features that are conditionally generated given the state $H$. Consider three biologically grounded features at and around position $i$: $E_{1}$ indicates that the residue at position $i$ is serine or threonine (an N-cap preference), $E_{2}$ indicates that the residue at position $i-1$ is glycine (a favorable N-cap capping partner), and $E_{3}$ indicates that the residue at position $i+1$ is proline (a boundary-disruptive effect that disfavors helix continuation). Assume that, given $H$, the features $E_{1}$, $E_{2}$, and $E_{3}$ are conditionally independent. Empirical estimates from a curated nonredundant structural dataset yield the following class-conditional probabilities:\n- $P(E_{1}=1 \\mid H=1) = 0.35$, $P(E_{1}=1 \\mid H=0) = 0.14$,\n- $P(E_{2}=1 \\mid H=1) = 0.24$, $P(E_{2}=1 \\mid H=0) = 0.15$,\n- $P(E_{3}=1 \\mid H=1) = 0.10$, $P(E_{3}=1 \\mid H=0) = 0.25$.\nThe prior probability that any position is the start of an $\\alpha$-helix is $P(H=1) = 0.08$. A sequence window around position $i$ is observed with $E_{1}=1$, $E_{2}=1$, and $E_{3}=1$. Using only foundational probabilistic principles and the stated modeling assumptions, determine the posterior probability $P(H=1 \\mid E_{1}=1, E_{2}=1, E_{3}=1)$, expressed as a decimal and rounded to four significant figures. No units are required.", "solution": "The objective is to compute the posterior probability of a position being the start of an $\\alpha$-helix ($H=1$), given the observation of three specific local features ($E_{1}=1$, $E_{2}=1$, and $E_{3}=1$). This calls for the application of Bayes' theorem.\n\nLet the event of observing the specific feature set be denoted by $\\mathcal{E}$, where $\\mathcal{E}$ is the conjunction of events $\\{E_{1}=1, E_{2}=1, E_{3}=1\\}$. We are tasked with finding $P(H=1 \\mid \\mathcal{E})$.\n\nBayes' theorem states:\n$$P(H=1 \\mid \\mathcal{E}) = \\frac{P(\\mathcal{E} \\mid H=1) P(H=1)}{P(\\mathcal{E})}$$\nThe term $P(H=1)$ is the prior probability of being a helix start, which is given as $P(H=1) = 0.08$.\n\nThe term $P(\\mathcal{E} \\mid H=1)$ is the likelihood of observing the features given that the position is a helix start. The problem states that the features $E_{1}$, $E_{2}$, and $E_{3}$ are conditionally independent given $H$. This assumption allows us to write the joint conditional probability as the product of the individual conditional probabilities:\n$$P(\\mathcal{E} \\mid H=1) = P(E_{1}=1, E_{2}=1, E_{3}=1 \\mid H=1)$$\n$$P(\\mathcal{E} \\mid H=1) = P(E_{1}=1 \\mid H=1) \\times P(E_{2}=1 \\mid H=1) \\times P(E_{3}=1 \\mid H=1)$$\nUsing the provided values:\n$$P(\\mathcal{E} \\mid H=1) = 0.35 \\times 0.24 \\times 0.10 = 0.0084$$\n\nThe denominator, $P(\\mathcal{E})$, is the total probability of observing the evidence, also known as the marginal likelihood. It can be computed using the law of total probability by summing over all possible states of the latent variable $H$:\n$$P(\\mathcal{E}) = P(\\mathcal{E} \\mid H=1) P(H=1) + P(\\mathcal{E} \\mid H=0) P(H=0)$$\nWe have already calculated $P(\\mathcal{E} \\mid H=1)$ and are given $P(H=1)$. We need to determine the remaining terms, $P(\\mathcal{E} \\mid H=0)$ and $P(H=0)$.\n\nThe prior probability for $H=0$ is complementary to $H=1$:\n$$P(H=0) = 1 - P(H=1) = 1 - 0.08 = 0.92$$\nUsing conditional independence again, we calculate the likelihood of the evidence given that the position is *not* a helix start ($H=0$):\n$$P(\\mathcal{E} \\mid H=0) = P(E_{1}=1 \\mid H=0) \\times P(E_{2}=1 \\mid H=0) \\times P(E_{3}=1 \\mid H=0)$$\nUsing the provided values:\n$$P(\\mathcal{E} \\mid H=0) = 0.14 \\times 0.15 \\times 0.25 = 0.00525$$\n\nNow, we can compute the total evidence $P(\\mathcal{E})$:\n$$P(\\mathcal{E}) = (0.0084 \\times 0.08) + (0.00525 \\times 0.92)$$\n$$P(\\mathcal{E}) = 0.000672 + 0.00483$$\n$$P(\\mathcal{E}) = 0.005502$$\n\nFinally, we can substitute all the calculated components back into Bayes' theorem to find the posterior probability:\n$$P(H=1 \\mid \\mathcal{E}) = \\frac{P(\\mathcal{E} \\mid H=1) P(H=1)}{P(\\mathcal{E})}$$\n$$P(H=1 \\mid \\mathcal{E}) = \\frac{0.000672}{0.005502}$$\n$$P(H=1 \\mid \\mathcal{E}) \\approx 0.12213740458$$\nThe problem requires the result to be rounded to four significant figures.\n$$P(H=1 \\mid \\mathcal{E}) \\approx 0.1221$$\nThis is the posterior probability that the position is the N-terminus of an $\\alpha$-helix, given the observed capping and boundary-disruptive signals. The posterior probability ($0.1221$) is higher than the prior probability ($0.08$), indicating that the observed combination of features, on balance, increases our belief that this position is a helix start, despite the presence of the disruptive proline.", "answer": "$$\\boxed{0.1221}$$", "id": "4601343"}, {"introduction": "Our final practice addresses a critical, real-world challenge in deploying machine learning models: dataset shift and performance evaluation under class imbalance. You will derive and apply a correction for a classifier's output when the prevalence of a structural class changes between training and testing data [@problem_id:4601358]. Calculating a class-imbalance weighted Brier score will provide hands-on experience with robust model evaluation techniques essential for modern bioinformatics.", "problem": "In protein secondary structure prediction, a common binary reduction distinguishes between helix ($H$) and non-helix ($\\neg H$). Consider a probabilistic classifier trained on a source dataset where the helix prevalence is $ \\pi_{s} = 0.65 $. The classifier outputs source-calibrated posteriors $ p_{s}(x) = \\mathbb{P}_{s}(H \\mid x) $. At deployment, the target prevalence shifts to $ \\pi_{t} = 0.25 $, and you may assume prior probability shift, namely that the class-conditional feature distributions $ p(x \\mid H) $ and $ p(x \\mid \\neg H) $ are invariant across source and target domains.\n\nYou are given four residues with their source-calibrated probabilities and ground-truth labels in the target domain:\n- Residue $1$: $p_{s}(x_{1}) = 0.95$, $y_{1} = 0$,\n- Residue $2$: $p_{s}(x_{2}) = 0.70$, $y_{2} = 1$,\n- Residue $3$: $p_{s}(x_{3}) = 0.40$, $y_{3} = 1$,\n- Residue $4$: $p_{s}(x_{4}) = 0.20$, $y_{4} = 0$,\n\nwhere $ y_{i} \\in \\{0,1\\} $ with $1$ indicating helix.\n\nUsing only foundational definitions from probability theory and the assumption of prior probability shift, first derive from first principles an expression that maps $ p_{s}(x) $ to the target posterior $ p_{t}(x) = \\mathbb{P}_{t}(H \\mid x) $ in terms of $ \\pi_{s} $ and $ \\pi_{t} $. Then, compute the class-imbalance weighted Brier score on these four residues using the adjusted probabilities $ p_{t}(x_{i}) $ with the following class-dependent weights:\n- $ w(1) = \\frac{1}{\\pi_{t}} $ for $ y_{i} = 1 $,\n- $ w(0) = \\frac{1}{1-\\pi_{t}} $ for $ y_{i} = 0 $.\n\nDefine the weighted Brier score as\n$$\n\\mathrm{Brier}_{w} \\;=\\; \\frac{\\sum_{i=1}^{4} w(y_{i}) \\, \\big(y_{i} - p_{t}(x_{i})\\big)^{2}}{\\sum_{i=1}^{4} w(y_{i})}.\n$$\n\nProvide the final value of $ \\mathrm{Brier}_{w} $ as a unitless real number rounded to four significant figures.", "solution": "The problem asks for two main tasks: first, to derive an expression for adjusting posterior probabilities under a prior probability shift, and second, to use this expression to compute a weighted Brier score for a given set of data points.\n\nThe problem is valid as it is scientifically grounded, well-posed, and objective. It presents a standard scenario in machine learning and bioinformatics concerning classifier calibration under dataset shift. All necessary data and definitions are provided, and no contradictions are present.\n\n**Part 1: Derivation of the Target Posterior Probability**\n\nLet $H$ denote the event that a residue is in a helix and $\\neg H$ be the complementary event. The source domain ($s$) and target domain ($t$) have different prior probabilities (prevalences) for the helix class, denoted by $\\pi_{s} = \\mathbb{P}_{s}(H)$ and $\\pi_{t} = \\mathbb{P}_{t}(H)$, respectively. The source-calibrated posterior is $p_{s}(x) = \\mathbb{P}_{s}(H \\mid x)$, and we seek the target posterior $p_{t}(x) = \\mathbb{P}_{t}(H \\mid x)$.\n\nThe core assumption is that of prior probability shift, which means the class-conditional likelihoods of the features $x$ are invariant across domains:\n$$p_{s}(x \\mid H) = p_{t}(x \\mid H) = p(x \\mid H)$$\n$$p_{s}(x \\mid \\neg H) = p_{t}(x \\mid \\neg H) = p(x \\mid \\neg H)$$\n\nWe can express the posterior probabilities in both domains using Bayes' theorem. For the source domain:\n$$p_{s}(x) = \\frac{p(x \\mid H) \\mathbb{P}_{s}(H)}{p(x \\mid H) \\mathbb{P}_{s}(H) + p(x \\mid \\neg H) \\mathbb{P}_{s}(\\neg H)} = \\frac{p(x \\mid H) \\pi_{s}}{p(x \\mid H) \\pi_{s} + p(x \\mid \\neg H) (1 - \\pi_{s})}$$\nIt is more convenient to work with odds. The posterior odds in the source domain are:\n$$\\frac{p_{s}(x)}{1-p_{s}(x)} = \\frac{p(x \\mid H) \\pi_{s}}{p(x \\mid \\neg H) (1 - \\pi_{s})} = \\frac{p(x \\mid H)}{p(x \\mid \\neg H)} \\frac{\\pi_{s}}{1-\\pi_{s}}$$\nThis equation relates the posterior odds to the likelihood ratio $\\frac{p(x \\mid H)}{p(x \\mid \\neg H)}$ and the prior odds $\\frac{\\pi_{s}}{1-\\pi_{s}}$. Since the likelihood ratio is invariant across domains, we can express it in terms of source domain quantities:\n$$\\frac{p(x \\mid H)}{p(x \\mid \\neg H)} = \\frac{p_{s}(x)}{1-p_{s}(x)} \\frac{1-\\pi_{s}}{\\pi_{s}}$$\nNow, we write the posterior odds for the target domain:\n$$\\frac{p_{t}(x)}{1-p_{t}(x)} = \\frac{p(x \\mid H)}{p(x \\mid \\neg H)} \\frac{\\pi_{t}}{1-\\pi_{t}}$$\nSubstituting the expression for the invariant likelihood ratio:\n$$\\frac{p_{t}(x)}{1-p_{t}(x)} = \\left(\\frac{p_{s}(x)}{1-p_{s}(x)} \\frac{1-\\pi_{s}}{\\pi_{s}}\\right) \\frac{\\pi_{t}}{1-\\pi_{t}} = \\frac{p_{s}(x)}{1-p_{s}(x)} \\left( \\frac{\\pi_{t}(1-\\pi_{s})}{\\pi_{s}(1-\\pi_{t})} \\right)$$\nLet's define a correction factor $A = \\frac{\\pi_{t}(1-\\pi_{s})}{\\pi_{s}(1-\\pi_{t})}$. The target posterior odds are $o_{t}(x) = o_{s}(x) \\cdot A$. To find the probability $p_{t}(x)$, we convert the odds back:\n$$p_{t}(x) = \\frac{\\frac{p_{t}(x)}{1-p_{t}(x)}}{1 + \\frac{p_{t}(x)}{1-p_{t}(x)}} = \\frac{\\frac{p_{s}(x)}{1-p_{s}(x)} A}{1 + \\frac{p_{s}(x)}{1-p_{s}(x)} A}$$\nMultiplying the numerator and denominator by $1-p_{s}(x)$ yields the desired expression:\n$$p_{t}(x) = \\frac{A \\cdot p_{s}(x)}{(1-p_{s}(x)) + A \\cdot p_{s}(x)}$$\n\n**Part 2: Calculation of the Weighted Brier Score**\n\nFirst, we calculate the numerical value of the correction factor $A$ using the given prevalences $\\pi_{s} = 0.65$ and $\\pi_{t} = 0.25$.\n$$A = \\frac{0.25 \\cdot (1-0.65)}{0.65 \\cdot (1-0.25)} = \\frac{0.25 \\cdot 0.35}{0.65 \\cdot 0.75} = \\frac{0.0875}{0.4875} = \\frac{875}{4875} = \\frac{7}{39}$$\nThe mapping from source to target probabilities is:\n$$p_{t}(x) = \\frac{\\frac{7}{39} p_{s}(x)}{(1-p_{s}(x)) + \\frac{7}{39} p_{s}(x)}$$\nNext, we compute the adjusted probabilities $p_{t}(x_{i})$ for the four given residues:\n- Residue $1$: $p_{s}(x_{1}) = 0.95$, $y_{1}=0$.\n  $p_{t}(x_{1}) = \\frac{\\frac{7}{39} (0.95)}{1-0.95 + \\frac{7}{39} (0.95)} = \\frac{\\frac{7}{39} \\frac{19}{20}}{\\frac{1}{20} + \\frac{7}{39} \\frac{19}{20}} = \\frac{7 \\cdot 19}{39 + 7 \\cdot 19} = \\frac{133}{39+133} = \\frac{133}{172}$.\n- Residue $2$: $p_{s}(x_{2}) = 0.70$, $y_{2}=1$.\n  $p_{t}(x_{2}) = \\frac{\\frac{7}{39} (0.7)}{1-0.7 + \\frac{7}{39} (0.7)} = \\frac{\\frac{7}{39} \\frac{7}{10}}{\\frac{3}{10} + \\frac{7}{39} \\frac{7}{10}} = \\frac{7 \\cdot 7}{39 \\cdot 3 + 7 \\cdot 7} = \\frac{49}{117+49} = \\frac{49}{166}$.\n- Residue $3$: $p_{s}(x_{3}) = 0.40$, $y_{3}=1$.\n  $p_{t}(x_{3}) = \\frac{\\frac{7}{39} (0.4)}{1-0.4 + \\frac{7}{39} (0.4)} = \\frac{\\frac{7}{39} \\frac{2}{5}}{\\frac{3}{5} + \\frac{7}{39} \\frac{2}{5}} = \\frac{7 \\cdot 2}{39 \\cdot 3 + 7 \\cdot 2} = \\frac{14}{117+14} = \\frac{14}{131}$.\n- Residue $4$: $p_{s}(x_{4}) = 0.20$, $y_{4}=0$.\n  $p_{t}(x_{4}) = \\frac{\\frac{7}{39} (0.2)}{1-0.2 + \\frac{7}{39} (0.2)} = \\frac{\\frac{7}{39} \\frac{1}{5}}{\\frac{4}{5} + \\frac{7}{39} \\frac{1}{5}} = \\frac{7}{39 \\cdot 4 + 7} = \\frac{7}{156+7} = \\frac{7}{163}$.\n\nThe class-dependent weights are:\n$w(1) = \\frac{1}{\\pi_{t}} = \\frac{1}{0.25} = 4$.\n$w(0) = \\frac{1}{1-\\pi_{t}} = \\frac{1}{1-0.25} = \\frac{1}{0.75} = \\frac{4}{3}$.\n\nThe weighted Brier score is given by $\\mathrm{Brier}_{w} = \\frac{N}{D}$, where $N = \\sum_{i=1}^{4} w(y_{i}) (y_{i} - p_{t}(x_{i}))^{2}$ and $D = \\sum_{i=1}^{4} w(y_{i})$.\nThe denominator is:\n$D = w(y_1) + w(y_2) + w(y_3) + w(y_4) = w(0) + w(1) + w(1) + w(0) = 2 w(0) + 2 w(1) = 2(\\frac{4}{3}) + 2(4) = \\frac{8}{3} + 8 = \\frac{32}{3}$.\nThe numerator consists of four terms:\n- Term $1$: $w(0) (0 - p_t(x_1))^2 = \\frac{4}{3} (\\frac{133}{172})^2 = \\frac{4}{3} \\frac{17689}{29584} = \\frac{17689}{22188}$.\n- Term $2$: $w(1) (1 - p_t(x_2))^2 = 4 (1 - \\frac{49}{166})^2 = 4 (\\frac{117}{166})^2 = 4 \\frac{13689}{27556} = \\frac{13689}{6889}$.\n- Term $3$: $w(1) (1 - p_t(x_3))^2 = 4 (1 - \\frac{14}{131})^2 = 4 (\\frac{117}{131})^2 = 4 \\frac{13689}{17161} = \\frac{54756}{17161}$.\n- Term $4$: $w(0) (0 - p_t(x_4))^2 = \\frac{4}{3} (\\frac{7}{163})^2 = \\frac{4}{3} \\frac{49}{26569} = \\frac{196}{79707}$.\n\nNow we sum these terms:\n$N \\approx 0.79723298 + 1.98708085 + 3.19072315 + 0.00245906 = 5.97749604$.\nFinally, we compute the score:\n$\\mathrm{Brier}_{w} = \\frac{N}{D} \\approx \\frac{5.97749604}{32/3} = \\frac{5.97749604 \\times 3}{32} \\approx \\frac{17.93248812}{32} \\approx 0.56039025$.\n\nRounding to four significant figures gives $0.5604$.", "answer": "$$\\boxed{0.5604}$$", "id": "4601358"}]}