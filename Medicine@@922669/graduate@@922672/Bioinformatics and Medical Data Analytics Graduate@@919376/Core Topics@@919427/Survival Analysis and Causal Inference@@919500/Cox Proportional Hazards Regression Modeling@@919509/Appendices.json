{"hands_on_practices": [{"introduction": "The output of a Cox model provides regression coefficients, such as $\\beta$, which represent log-hazard ratios. To make these results interpretable for clinicians and researchers, we must convert them into hazard ratios, $\\exp(\\beta)$, and quantify their statistical uncertainty. This exercise [@problem_id:4551012] provides essential practice in constructing a Wald confidence interval for a hazard ratio, a fundamental skill in reporting and understanding the results of a survival analysis.", "problem": "A translational oncology study analyzes overall survival for patients with metastatic colorectal cancer using the Cox proportional hazards model (CPHM). Let the covariate $X$ denote a standardized messenger ribonucleic acid (mRNA) expression score for a gene implicated in epithelial–mesenchymal transition, scaled to have mean $0$ and variance $1$ in the cohort. Under the CPHM, the hazard for a patient with covariate value $x$ is modeled as $h(t \\mid x) = h_{0}(t)\\,\\exp(x\\,\\beta_{1})$, where $h_{0}(t)$ is the baseline hazard and $\\beta_{1}$ is the regression coefficient for $X$. The maximum partial likelihood estimator (MPLE) $\\hat{\\beta}_{1}$ is used for inference, and under standard regularity conditions for the partial likelihood it is asymptotically normal with mean $\\beta_{1}$ and variance given by the inverse of the observed Fisher information.\n\nSuppose the estimated coefficient and its estimated variance from the fitted model are $\\hat{\\beta}_{1} = 0.37$ and $\\widehat{\\mathrm{Var}}(\\hat{\\beta}_{1}) = 0.0484$, respectively. Using the asymptotic normality of $\\hat{\\beta}_{1}$ and the fact that the hazard ratio for a one-unit increase in $X$ is $\\exp(\\beta_{1})$, derive the two-sided $95\\%$ Wald confidence interval for the hazard ratio by working on the log scale and then transforming appropriately. Compute the upper endpoint of this interval. Use the standard normal quantile corresponding to level $\\alpha = 0.05$ and express the final answer as a pure number. Round your answer to four significant figures.", "solution": "The problem requires the calculation of the upper endpoint of the two-sided $95\\%$ Wald confidence interval for the hazard ratio, $\\mathrm{HR}$. The hazard ratio is defined in the context of the Cox proportional hazards model, $h(t \\mid x) = h_{0}(t)\\,\\exp(x\\,\\beta_{1})$, as $\\mathrm{HR} = \\exp(\\beta_{1})$ for a one-unit increase in the covariate $X$.\n\nThe standard approach, as specified in the problem, is to first construct a confidence interval for the regression coefficient $\\beta_{1}$ and then transform its endpoints to obtain the confidence interval for the hazard ratio. The logarithm of the hazard ratio is $\\ln(\\mathrm{HR}) = \\ln(\\exp(\\beta_{1})) = \\beta_{1}$. We will therefore construct the confidence interval on this \"log scale\".\n\nThe maximum partial likelihood estimator (MPLE) $\\hat{\\beta}_{1}$ is asymptotically normally distributed. A two-sided $(1-\\alpha) \\times 100\\%$ Wald confidence interval for $\\beta_{1}$ is given by the formula:\n$$ \\hat{\\beta}_{1} \\pm z_{\\alpha/2} \\cdot \\widehat{\\mathrm{SE}}(\\hat{\\beta}_{1}) $$\nwhere $\\hat{\\beta}_{1}$ is the point estimate of the coefficient, $\\widehat{\\mathrm{SE}}(\\hat{\\beta}_{1})$ is the estimated standard error of the estimator, and $z_{\\alpha/2}$ is the critical value from the standard normal distribution that corresponds to an upper tail probability of $\\alpha/2$.\n\nFrom the problem statement, we are given:\n- The point estimate of the coefficient: $\\hat{\\beta}_{1} = 0.37$.\n- The estimated variance of the coefficient: $\\widehat{\\mathrm{Var}}(\\hat{\\beta}_{1}) = 0.0484$.\n\nFirst, we calculate the estimated standard error, which is the square root of the estimated variance:\n$$ \\widehat{\\mathrm{SE}}(\\hat{\\beta}_{1}) = \\sqrt{\\widehat{\\mathrm{Var}}(\\hat{\\beta}_{1})} = \\sqrt{0.0484} = 0.22 $$\nFor a $95\\%$ confidence interval, the significance level is $\\alpha = 0.05$. Therefore, we need the critical value $z_{\\alpha/2} = z_{0.05/2} = z_{0.025}$. This is the standard normal quantile for which the cumulative probability is $1 - 0.025 = 0.975$. The value is widely known to be approximately $1.96$.\n$$ z_{0.025} \\approx 1.95996 \\dots $$\nWe will use the conventional value $z_{0.025} = 1.96$.\n\nNow we can construct the $95\\%$ confidence interval for $\\beta_{1}$:\n$$ \\mathrm{CI}_{95\\%}(\\beta_{1}) = \\hat{\\beta}_{1} \\pm z_{0.025} \\cdot \\widehat{\\mathrm{SE}}(\\hat{\\beta}_{1}) $$\n$$ \\mathrm{CI}_{95\\%}(\\beta_{1}) = 0.37 \\pm 1.96 \\times 0.22 $$\nThe margin of error is $M = 1.96 \\times 0.22 = 0.4312$.\nThe interval for $\\beta_{1}$ is therefore:\n$$ [0.37 - 0.4312, 0.37 + 0.4312] = [-0.0612, 0.8012] $$\nThe hazard ratio is $\\mathrm{HR} = \\exp(\\beta_{1})$. Since the exponential function $f(y) = \\exp(y)$ is a strictly increasing monotonic function, the confidence interval for the hazard ratio can be obtained by exponentiating the endpoints of the confidence interval for $\\beta_{1}$.\nLet the lower and upper bounds for the $\\beta_{1}$ CI be $L_{\\beta}$ and $U_{\\beta}$ respectively. The CI for the HR is then $[\\exp(L_{\\beta}), \\exp(U_{\\beta})]$.\n$$ \\mathrm{CI}_{95\\%}(\\mathrm{HR}) = [\\exp(-0.0612), \\exp(0.8012)] $$\nThe problem asks specifically for the upper endpoint of this interval.\n$$ \\text{Upper Endpoint} = \\exp(0.8012) $$\nWe now compute the numerical value:\n$$ \\exp(0.8012) \\approx 2.22822409\\dots $$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $2, 2, 2, 8$. The fifth digit is $2$, so we round down.\n$$ \\text{Upper Endpoint} \\approx 2.228 $$\nThis is the upper bound of the $95\\%$ confidence interval for the hazard ratio associated with a one-unit increase in the standardized mRNA expression score.", "answer": "$$\\boxed{2.228}$$", "id": "4551012"}, {"introduction": "The validity of the Cox model hinges on the proportional hazards (PH) assumption, which posits that the effect of a covariate on the hazard is constant over time. It is crucial to test this assumption before drawing conclusions from the model. This practice [@problem_id:4550937] delves into the theory and application of Schoenfeld residuals, the primary diagnostic tool for assessing the PH assumption and identifying time-dependent covariate effects.", "problem": "A clinical genomics study in bioinformatics and medical data analytics investigates survival among patients with a continuous gene expression covariate $x_j$ and additional covariates $\\mathbf{x}_{-j}$. A Cox proportional hazards model is fit, where the hazard at time $t$ for a subject with covariate vector $\\mathbf{x}$ is $h(t \\mid \\mathbf{x}) = h_0(t)\\exp\\{\\boldsymbol{\\beta}^\\top \\mathbf{x}\\}$, with $h_0(t)$ the baseline hazard and $\\boldsymbol{\\beta}$ the regression coefficients. Let $T_i$ denote the $i$-th distinct event time among observed failures, with right censoring and possibly left truncation; the risk set at time $t$ is $R(t)$, defined as all individuals under observation and not yet failed just prior to $t$.\n\nDefine the Schoenfeld residual for covariate $j$ at event time $T_i$ by $r_{ij} = x_{ij} - \\tilde{x}_j(T_i)$, where $x_{ij}$ is the observed covariate $j$ for the subject failing at $T_i$, and $\\tilde{x}_j(t)$ is the risk set weighted mean of covariate $j$ at time $t$ under the fitted model, given by\n$$\n\\tilde{x}_j(t) = \\frac{\\sum_{k \\in R(t)} x_{kj}\\exp\\{\\hat{\\boldsymbol{\\beta}}^\\top \\mathbf{x}_k\\}}{\\sum_{k \\in R(t)} \\exp\\{\\hat{\\boldsymbol{\\beta}}^\\top \\mathbf{x}_k\\}},\n$$\nwith $\\hat{\\boldsymbol{\\beta}}$ the estimated coefficients from partial likelihood. The partial likelihood for $\\boldsymbol{\\beta}$ is\n$$\nL(\\boldsymbol{\\beta}) = \\prod_{i}\\frac{\\exp\\{\\boldsymbol{\\beta}^\\top \\mathbf{x}_{(i)}\\}}{\\sum_{k \\in R(T_i)} \\exp\\{\\boldsymbol{\\beta}^\\top \\mathbf{x}_k\\}},\n$$\nwhere $\\mathbf{x}_{(i)}$ denotes the covariate vector of the subject who failed at $T_i$.\n\nWhich statement(s) correctly explain how the Schoenfeld residuals $r_{ij}$ diagnose violations of the proportional hazards assumption for covariate $j$ in this Cox model?\n\nA. Under the proportional hazards model with a constant coefficient $\\beta_j$, the conditional expectation $\\mathbb{E}\\{r_{ij} \\mid T_i, R(T_i)\\}$ equals $0$, so plotting $r_{ij}$ (or appropriately scaled variants) against $T_i$ should show no systematic trend; a systematic trend indicates a time-varying effect for $x_j$ and thus a violation of proportional hazards.\n\nB. Because $r_{ij}$ uses $x_{ij}$ at censored times, the residuals are available for all subjects and are primarily designed to assess nonlinearity of $x_j$ via Martingale residual plots.\n\nC. The magnitude of $r_{ij}$ directly estimates the instantaneous hazard ratio for a one-unit change in $x_j$ at time $T_i$; constant magnitudes across event times confirm proportional hazards.\n\nD. Scaled Schoenfeld residuals, defined by multiplying $r_{ij}$ by an estimate of the covariance for $\\hat{\\beta}_j$ (or by appropriate components of the inverse observed information), approximate the deviation $\\beta_j(t) - \\hat{\\beta}_j$ of a local time-varying effect from the fitted constant; a nonzero slope when regressed on $t$ or $\\log t$ provides evidence against proportional hazards for covariate $j$.\n\nE. In the presence of left truncation, Schoenfeld residuals cannot be defined because the risk set is ill-posed, so proportional hazards cannot be assessed with them.", "solution": "This problem tests the understanding of Schoenfeld residuals, which are the primary diagnostic tool for assessing the proportional hazards (PH) assumption in a Cox model. The PH assumption states that the effect of a covariate, represented by its coefficient $\\beta_j$, is constant over time. A violation means the true effect $\\beta_j(t)$ is time-dependent.\n\nThe Schoenfeld residual for covariate $j$ at an event time $T_i$ is defined as the difference between the covariate value for the individual who failed, $x_{ij}$, and the expected covariate value for the failing individual, $\\tilde{x}_j(T_i)$, which is a weighted average over the risk set $R(T_i)$. If the PH assumption holds, these residuals should be centered around zero and show no systematic pattern when plotted against time. A trend indicates that the covariate's effect is changing with time, violating the assumption.\n\nLet's evaluate each statement:\n\n**A. Correct.** This statement accurately describes the fundamental principle of Schoenfeld residuals. Under the null hypothesis of proportional hazards, the expected value of the residual is zero at each event time. Therefore, a plot of the residuals (or their scaled versions) against time should resemble a random scatter around a horizontal line at zero. A systematic trend is the key indicator of a PH violation.\n\n**B. Incorrect.** This statement contains two errors. First, Schoenfeld residuals are calculated only for subjects who experience an event, at the time of their event, not for censored subjects. Second, their primary purpose is to check the PH assumption (time-constancy of effects), not to assess the functional form (e.g., linearity) of a covariate. Martingale residuals are typically used for the latter.\n\n**C. Incorrect.** A Schoenfeld residual, $r_{ij}$, has the units of the covariate $x_j$ and represents a deviation from a conditional mean. It does not estimate the hazard ratio, which is a dimensionless quantity, $\\exp(\\beta_j)$. The diagnostic is based on the trend of the residuals, not their individual magnitudes.\n\n**D. Correct.** This statement provides a more advanced and precise description of modern PH diagnostics. Based on the work of Grambsch and Therneau, scaled Schoenfeld residuals provide a formal test. A plot of these scaled residuals against time (or a function of time) approximates the deviation of a time-varying coefficient from the overall fitted constant coefficient, $\\beta_j(t) - \\hat{\\beta}_j$. Regressing the scaled residuals on time and testing if the slope is non-zero is a standard formal test for the PH assumption.\n\n**E. Incorrect.** The Cox model framework, including the definition of risk sets and the calculation of partial likelihood and residuals, is specifically designed to correctly handle complex data structures like left truncation (delayed entry). The risk set at any time $t$ is well-defined as the set of individuals under observation at that time, so Schoenfeld residuals can be computed and used for diagnostics without issue.\n\nTherefore, statements A and D correctly explain the use of Schoenfeld residuals for diagnosing violations of the proportional hazards assumption.", "answer": "$$\\boxed{AD}$$", "id": "4550937"}, {"introduction": "Beyond single-event analysis, many biomedical studies involve phenomena like recurrent hospitalizations, which require specialized methods. The Andersen-Gill counting process formulation extends the Cox model to handle such recurrent event data. This advanced programming exercise [@problem_id:4550962] challenges you to implement this model from first principles, providing a deep, mechanical understanding of how the partial likelihood is constructed and maximized for complex, multi-episode survival histories.", "problem": "A developer is tasked with building a program that estimates the regression coefficient in a Cox proportional hazards regression for recurrent events using the Andersen–Gill counting process formulation. The biomedical context is recurrent hospitalizations for heart failure patients. Each patient may experience multiple hospitalizations during follow-up, and the exposure is a binary indicator for receiving a care management program. The model should be constructed using the start–stop representation of at-risk intervals and should handle ties in event times using the Breslow approximation. The developer must implement the estimator from first principles, beginning with the definition of the hazard and risk sets, and must not rely on external survival analysis libraries.\n\nFundamental base:\n- The Cox proportional hazards regression model defines the hazard for individual $i$ at time $t$ as $h_i(t) = h_0(t) \\exp\\{\\beta x_i(t)\\}$ where $h_0(t)$ is an unspecified baseline hazard, $x_i(t)$ is the covariate value that may change with time, and $\\beta$ is the regression coefficient to be estimated.\n- In the Andersen–Gill counting process formulation (AG), each subject contributes one or more intervals $\\left[s, t\\right)$ over which they are at risk. For each interval, there is a binary event indicator associated with the right endpoint, reflecting whether an event occurred at the interval stop time. The risk set at time $t$ consists of all intervals for which the start time is strictly less than $t$ and the stop time is greater than or equal to $t$.\n- Estimation proceeds by maximizing the partial likelihood derived from these definitions, using an appropriate tie-handling method (Breslow approximation) for multiple events at the same time.\n\nYour program must:\n1. Demonstrate start–stop coding for recurrent events under the Andersen–Gill counting process formulation by transforming raw event times into interval data of the form $(\\text{start}, \\text{stop}, \\text{event}, x)$ for each patient.\n2. Implement from first principles the maximum partial likelihood estimator for a single coefficient $\\beta$ using Newton–Raphson iterations, the Breslow approximation for tied event times, and the risk set definition above. The covariate $x$ is constant within each interval and equals $1$ if the patient is in the care management program and $0$ otherwise.\n3. Apply the implementation to the following test suite (three independent datasets), each representing a plausible biomedical registry of recurrent hospitalizations. Times are in days.\n\nDataset A (happy path; recurrent events without left truncation):\n- Patient 1: entry at day $0$, events at days $10$, $40$, $90$, censored at day $120$, $x=1$.\n- Patient 2: entry at day $0$, events at days $20$, $85$, censored at day $100$, $x=0$.\n- Patient 3: entry at day $0$, no events, censored at day $100$, $x=1$.\n\nDataset B (ties in event times across subjects):\n- Patient 1: entry at day $0$, events at days $30$, $60$, censored at day $80$, $x=0$.\n- Patient 2: entry at day $0$, event at day $30$, censored at day $50$, $x=1$.\n- Patient 3: entry at day $0$, event at day $60$, censored at day $90$, $x=1$.\n- Patient 4: entry at day $0$, no events, censored at day $60$, $x=0$.\n\nDataset C (left truncation; staggered entries with recurrent events):\n- Patient 1: entry at day $20$, event at day $50$, censored at day $60$, $x=1$.\n- Patient 2: entry at day $0$, events at days $25$, $120$, censored at day $150$, $x=0$.\n- Patient 3: entry at day $0$, no events, censored at day $150$, $x=1$.\n- Patient 4: entry at day $60$, event at day $120$, censored at day $160$, $x=0$.\n\nStart–stop coding specifics:\n- For each patient with entry time $a$, hospitalization event times $t_1  t_2  \\dots  t_K$, and censoring time $C$, create intervals:\n  - $(a, t_1]$ with event indicator $1$ if $K \\ge 1$,\n  - $(t_j, t_{j+1}]$ with event indicator $1$ for $j = 1, \\dots, K-1$,\n  - $(t_K, C]$ with event indicator $0$ if $K \\ge 1$, or $(a, C]$ with event indicator $0$ if $K = 0$.\n- The risk set at calendar time $t$ includes every interval $(s, u]$ such that $s  t \\le u$.\n\nOutput specification:\n- For each dataset, estimate the regression coefficient $\\hat{\\beta}$ for the care management program using the above approach.\n- Express each estimate as a decimal rounded to six places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order [Dataset A result, Dataset B result, Dataset C result]. For example: \"[0.123456,-0.010000,0.500000]\".\n\nThe program must be fully self-contained, require no input, and be runnable in any modern environment supporting Python and the specified libraries. The final answers are floats; no physical or angle unit reporting is required. Ensure scientific realism through consistent interval construction and proper risk set formation as above.", "solution": "The solution implements the estimator for the Andersen–Gill (AG) recurrent event model from first principles. The process involves transforming the data into a counting process format, defining the partial log-likelihood function for this format, and maximizing it numerically using the Newton-Raphson algorithm.\n\n**1. Data Transformation into Start-Stop Format**\n\nThe AG model analyzes recurrent events by restructuring each patient's follow-up history into a series of disjoint time intervals $(\\text{start}, \\text{stop}]$. For each interval, the data records an event indicator ($1$ if an event occurred at the stop time, $0$ otherwise) and the corresponding covariate values. For a patient with entry time $a$, event times $t_1  t_2  \\dots  t_K$, and censoring time $C$, the intervals are constructed as specified in the problem description.\n\n**2. The Partial Log-Likelihood with Breslow Approximation**\n\nThe Cox model specifies the hazard rate for an event at time $t$ for interval $i$ as $h_i(t) = h_0(t) \\exp(\\beta x_i)$. The partial log-likelihood is constructed by considering the unique ordered event times $T_1  T_2  \\dots  T_m$.\n\nThe risk set at time $T_j$, denoted $R(T_j)$, is the set of all intervals $k$ that are active at that time (i.e., for which $\\text{start}_k  T_j \\le \\text{stop}_k$). Let $D_j$ be the set of intervals where an event occurred at time $T_j$, and let $d_j = |D_j|$ be the number of tied events.\n\nUsing the Breslow approximation for ties, the partial log-likelihood function is:\n$$ \\ell(\\beta) = \\sum_{j=1}^{m} \\left( \\sum_{k \\in D_j} \\beta x_k - d_j \\log \\left( \\sum_{l \\in R(T_j)} \\exp(\\beta x_l) \\right) \\right) $$\n\n**3. Maximum Likelihood Estimation via Newton-Raphson**\n\nThe value of $\\beta$ that maximizes $\\ell(\\beta)$ is found using the Newton-Raphson method, an iterative algorithm that finds the root of the score function, $U(\\beta) = \\partial \\ell(\\beta) / \\partial \\beta$. The update rule is:\n$$ \\beta_{k+1} = \\beta_k + \\frac{U(\\beta_k)}{\\mathcal{I}(\\beta_k)} $$\nwhere $\\mathcal{I}(\\beta) = -\\frac{\\partial^2 \\ell(\\beta)}{\\partial \\beta^2}$ is the observed information.\n\nThe required derivatives are defined using the following sums over the risk set $R(T_j)$:\n- $S^{(0)}(\\beta, T_j) = \\sum_{l \\in R(T_j)} \\exp(\\beta x_l)$\n- $S^{(1)}(\\beta, T_j) = \\sum_{l \\in R(T_j)} x_l \\exp(\\beta x_l)$\n- $S^{(2)}(\\beta, T_j) = \\sum_{l \\in R(T_j)} x_l^2 \\exp(\\beta x_l)$\n\nThe **score function** is:\n$$ U(\\beta) = \\sum_{j=1}^{m} \\left( \\sum_{k \\in D_j} x_k - d_j \\frac{S^{(1)}(\\beta, T_j)}{S^{(0)}(\\beta, T_j)} \\right) $$\n\nAnd the **observed information** is:\n$$ \\mathcal{I}(\\beta) = \\sum_{j=1}^{m} d_j \\left[ \\frac{S^{(2)}(\\beta, T_j)}{S^{(0)}(\\beta, T_j)} - \\left( \\frac{S^{(1)}(\\beta, T_j)}{S^{(0)}(\\beta, T_j)} \\right)^2 \\right] $$\nThe algorithm initializes $\\beta=0$ and iteratively applies the update rule until convergence. The provided code in the answer block implements this entire procedure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _create_ag_intervals(raw_patient_data):\n    \"\"\"\n    Transforms raw patient data into Andersen-Gill start-stop intervals.\n\n    Args:\n        raw_patient_data (list): A list of dictionaries, each representing a patient.\n            Keys: 'entry', 'events' (list of times), 'censor', 'x'.\n\n    Returns:\n        list: A list of dictionaries, each representing a risk interval.\n            Keys: 'start', 'stop', 'event', 'x'.\n    \"\"\"\n    intervals = []\n    for pt in raw_patient_data:\n        event_times = sorted(pt['events'])\n        current_start_time = pt['entry']\n        \n        # Create intervals ending with an event\n        for event_time in event_times:\n            if event_time  current_start_time:\n                intervals.append({\n                    'start': current_start_time,\n                    'stop': event_time,\n                    'event': 1,\n                    'x': pt['x']\n                })\n            current_start_time = event_time\n            \n        # Create final censoring interval\n        if pt['censor']  current_start_time:\n            intervals.append({\n                'start': current_start_time,\n                'stop': pt['censor'],\n                'event': 0,\n                'x': pt['x']\n            })\n    return intervals\n\ndef calculate_beta_ag(raw_patient_data, max_iter=20, tol=1e-7):\n    \"\"\"\n    Estimates the regression coefficient for a single covariate Cox model\n    with recurrent events using the Andersen-Gill formulation.\n\n    Args:\n        raw_patient_data (list): A list of patient data dictionaries.\n        max_iter (int): Maximum number of Newton-Raphson iterations.\n        tol (float): Tolerance for convergence.\n\n    Returns:\n        float: The estimated regression coefficient beta.\n    \"\"\"\n    intervals = _create_ag_intervals(raw_patient_data)\n    \n    unique_event_times = sorted(list(set(\n        ival['stop'] for ival in intervals if ival['event'] == 1\n    )))\n    \n    beta = 0.0\n    for _ in range(max_iter):\n        score_U = 0.0  # First derivative of log-likelihood\n        info_I = 0.0   # Negative second derivative (information)\n\n        for t_event in unique_event_times:\n            # Identify risk set and event set for the current event time\n            risk_set_covariates, event_set_covariates = [], []\n            \n            for ival in intervals:\n                # Risk set: interval is active at t_event\n                if ival['start']  t_event = ival['stop']:\n                    risk_set_covariates.append(ival['x'])\n                    # Event set: event occurs exactly at t_event\n                    if ival['stop'] == t_event and ival['event'] == 1:\n                        event_set_covariates.append(ival['x'])\n\n            risk_set_x = np.array(risk_set_covariates)\n            d_j = len(event_set_covariates)\n            sum_x_at_event = sum(event_set_covariates)\n\n            if d_j == 0:\n                continue\n\n            # Calculate S_0, S_1, S_2 sums\n            risk_weights = np.exp(beta * risk_set_x)\n            \n            s0 = np.sum(risk_weights)\n            s1 = np.sum(risk_set_x * risk_weights)\n            s2 = np.sum(risk_set_x**2 * risk_weights)\n            \n            if s0 > 0:\n                E1 = s1 / s0\n                E2 = s2 / s0\n                \n                # Update total score and information\n                score_U += sum_x_at_event - d_j * E1\n                info_I += d_j * (E2 - E1**2)\n\n        if info_I = 1e-9:\n            # Information is zero or negative, cannot update.\n            # This can happen with perfect separation or sparse data.\n            break\n\n        delta_beta = score_U / info_I\n        beta += delta_beta\n\n        if abs(delta_beta)  tol:\n            break\n            \n    return beta\n\ndef solve():\n    \"\"\"\n    Main function to define datasets, run the analysis, and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Dataset A\n        [\n            {'id': 1, 'entry': 0, 'events': [10, 40, 90], 'censor': 120, 'x': 1},\n            {'id': 2, 'entry': 0, 'events': [20, 85], 'censor': 100, 'x': 0},\n            {'id': 3, 'entry': 0, 'events': [], 'censor': 100, 'x': 1},\n        ],\n        # Dataset B\n        [\n            {'id': 1, 'entry': 0, 'events': [30, 60], 'censor': 80, 'x': 0},\n            {'id': 2, 'entry': 0, 'events': [30], 'censor': 50, 'x': 1},\n            {'id': 3, 'entry': 0, 'events': [60], 'censor': 90, 'x': 1},\n            {'id': 4, 'entry': 0, 'events': [], 'censor': 60, 'x': 0},\n        ],\n        # Dataset C\n        [\n            {'id': 1, 'entry': 20, 'events': [50], 'censor': 60, 'x': 1},\n            {'id': 2, 'entry': 0, 'events': [25, 120], 'censor': 150, 'x': 0},\n            {'id': 3, 'entry': 0, 'events': [], 'censor': 150, 'x': 1},\n            {'id': 4, 'entry': 60, 'events': [120], 'censor': 160, 'x': 0},\n        ],\n    ]\n\n    results = []\n    for case_data in test_cases:\n        beta_hat = calculate_beta_ag(case_data)\n        results.append(beta_hat)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4550962"}]}