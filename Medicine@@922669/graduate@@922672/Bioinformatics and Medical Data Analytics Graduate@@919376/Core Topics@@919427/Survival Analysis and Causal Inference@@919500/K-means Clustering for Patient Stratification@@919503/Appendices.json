{"hands_on_practices": [{"introduction": "Before applying k-means, a crucial decision is how to scale your features, as the algorithm's reliance on Euclidean distance means variables with large ranges can dominate the clustering process. This exercise guides you to analytically derive how feature scaling transforms the k-means objective function, $J$, and to quantify the precise point at which a patient's cluster assignment can flip as a result [@problem_id:4576104]. This practice illuminates the profound impact of data preprocessing on the stability and interpretation of patient subgroups.", "problem": "A biomedical cohort of $N$ patients is represented by two features per patient: a high-variance laboratory measurement $L$ and a standardized risk score $R$. Clustering is performed with $K=2$ using the $k$-means objective that minimizes the within-cluster sum of squared Euclidean distances. Starting from the definition of the $k$-means objective, $$J=\\sum_{k=1}^{K}\\sum_{i \\in C_k}\\|x_i-\\mu_k\\|_2^2,$$ where $x_i \\in \\mathbb{R}^2$ are patient feature vectors and $\\mu_k$ are cluster means, consider the following affine min-max compression of the $L$ feature only:\n- Let the empirical cohort minimum and maximum of $L$ be $L_{\\min}$ and $L_{\\max}$, with range $r=L_{\\max}-L_{\\min}$.\n- Define a compressed coordinate for $L$ by $z=\\frac{c}{r}(L-L_{\\min})$ for a chosen compression factor $c \\in (0,r]$.\n- Leave $R$ unchanged.\n\nTask A. Using only the definition of $J$ and basic linearity properties of means and variances, derive the transformed objective $J_c$ after applying the above compression to $L$ (with $R$ unchanged) and recomputing cluster means in the compressed space. Express $J_c$ in terms of the original per-feature within-cluster sums of squares.\n\nTask B. Now quantify the change in cluster assignments for a specific patient when the $L$-range is compressed. Suppose that on the original uncompressed scale, across the cohort $L_{\\min}=0$ and $L_{\\max}=100$ (so $r=100$). Consider two cluster centers estimated on the original scale, $\\mu_1=(L_1,R_1)=(35.5,0)$ and $\\mu_2=(L_2,R_2)=(39.5,1)$. Consider a patient $x=(L_x,R_x)=(50,0)$. Assume that when the compression with factor $c$ is applied to $L$ (and $R$ is left unchanged), the cluster means in the compressed space are the images of $\\mu_1$ and $\\mu_2$ under the same affine transform on $L$ (and identity on $R$). Determine the exact critical value $c^\\star \\in (0,100]$ such that for $c=c^\\star$ the squared distances from $x$ to the two cluster centers in the compressed space are equal, and for $c>c^\\star$ the assignment of $x$ matches that on the original scale while for $c<c^\\star$ the assignment flips. Report $c^\\star$ as a single number. Do not round.", "solution": "The user has provided a two-part problem concerning the effect of feature scaling on the $k$-means clustering objective function and cluster assignments. The problem is well-posed, scientifically grounded, and contains all necessary information for a complete solution.\n\n**Part A: Derivation of the Transformed Objective Function $J_c$**\n\nThe original $k$-means objective function is given by\n$$J=\\sum_{k=1}^{K}\\sum_{i \\in C_k}\\|x_i-\\mu_k\\|_2^2$$\nwhere $x_i = (L_i, R_i)$ is the feature vector for patient $i$, and $\\mu_k = (\\mu_{L,k}, \\mu_{R,k})$ is the mean vector for cluster $C_k$. The squared Euclidean norm can be expanded by component:\n$$J = \\sum_{k=1}^{K}\\sum_{i \\in C_k} \\left[ (L_i - \\mu_{L,k})^2 + (R_i - \\mu_{R,k})^2 \\right]$$\nThis can be separated into the within-cluster sum of squares for each feature:\n$J = WSS_L + WSS_R$\nwhere $WSS_L = \\sum_{k=1}^{K}\\sum_{i \\in C_k} (L_i - \\mu_{L,k})^2$ and $WSS_R = \\sum_{k=1}^{K}\\sum_{i \\in C_k} (R_i - \\mu_{R,k})^2$.\n\nThe problem introduces an affine compression on the $L$ feature only. The new coordinates for patient $i$ are $x_{i,c} = (z_i, R_i)$, where $z_i = \\frac{c}{r}(L_i - L_{\\min})$. The feature $R_i$ is unchanged. The range of the $L$ feature is $r = L_{\\max} - L_{\\min}$.\n\nThe new cluster means, $\\mu_{k,c}$, are computed in the transformed space. The mean for the $R$ feature, $\\mu_{R,k}$, is unaffected. The mean for the new $z$ feature, $\\mu_{z,k}$, is the average of the $z_i$ values for all points in cluster $C_k$:\n$$\\mu_{z,k} = \\frac{1}{|C_k|} \\sum_{i \\in C_k} z_i = \\frac{1}{|C_k|} \\sum_{i \\in C_k} \\frac{c}{r}(L_i - L_{\\min})$$\nBy the linearity of summation and expectation, we can factor out the constants:\n$$\\mu_{z,k} = \\frac{c}{r} \\left( \\left(\\frac{1}{|C_k|} \\sum_{i \\in C_k} L_i\\right) - L_{\\min} \\right) = \\frac{c}{r}(\\mu_{L,k} - L_{\\min})$$\nSo the new cluster mean vector is $\\mu_{k,c} = \\left(\\frac{c}{r}(\\mu_{L,k} - L_{\\min}), \\mu_{R,k}\\right)$.\n\nThe new objective function, $J_c$, is the sum of squared Euclidean distances in the compressed space:\n$$J_c = \\sum_{k=1}^{K}\\sum_{i \\in C_k} \\|x_{i,c} - \\mu_{k,c}\\|_2^2 = \\sum_{k=1}^{K}\\sum_{i \\in C_k} \\left[ (z_i - \\mu_{z,k})^2 + (R_i - \\mu_{R,k})^2 \\right]$$\nLet's analyze the term for the transformed feature:\n$$z_i - \\mu_{z,k} = \\frac{c}{r}(L_i - L_{\\min}) - \\frac{c}{r}(\\mu_{L,k} - L_{\\min}) = \\frac{c}{r}(L_i - L_{\\min} - \\mu_{L,k} + L_{\\min}) = \\frac{c}{r}(L_i - \\mu_{L,k})$$\nSubstituting this back into the expression for $J_c$:\n$$J_c = \\sum_{k=1}^{K}\\sum_{i \\in C_k} \\left[ \\left(\\frac{c}{r}(L_i - \\mu_{L,k})\\right)^2 + (R_i - \\mu_{R,k})^2 \\right]$$\n$$J_c = \\sum_{k=1}^{K}\\sum_{i \\in C_k} \\left[ \\left(\\frac{c}{r}\\right)^2 (L_i - \\mu_{L,k})^2 + (R_i - \\mu_{R,k})^2 \\right]$$\nWe can now separate the sums:\n$$J_c = \\left(\\frac{c}{r}\\right)^2 \\sum_{k=1}^{K}\\sum_{i \\in C_k} (L_i - \\mu_{L,k})^2 + \\sum_{k=1}^{K}\\sum_{i \\in C_k} (R_i - \\mu_{R,k})^2$$\nRecognizing the definitions of $WSS_L$ and $WSS_R$, we arrive at the final expression for the transformed objective function:\n$$J_c = \\left(\\frac{c}{r}\\right)^2 WSS_L + WSS_R$$\n\n**Part B: Calculation of the Critical Compression Factor $c^\\star$**\n\nIn this part, we are given specific values and asked to find the critical compression factor $c^\\star$ where a patient's cluster assignment becomes ambiguous.\nThe given data are:\n-   $L_{\\min} = 0$, $L_{\\max} = 100$, so $r = 100$.\n-   Original cluster means: $\\mu_1 = (L_1, R_1) = (35.5, 0)$ and $\\mu_2 = (L_2, R_2) = (39.5, 1)$.\n-   Patient data point: $x = (L_x, R_x) = (50, 0)$.\n\nThe compression transformation for $L$ simplifies to $z = \\frac{c}{100}(L - 0) = \\frac{c}{100}L$.\nThe patient's data point in the compressed space is $x_c = (z_x, R_x) = \\left(\\frac{c}{100} \\cdot 50, 0\\right)$.\nThe cluster means in the compressed space are assumed to be the images of the original means under the same transformation:\n-   $\\mu_{1,c} = \\left(\\frac{c}{100} \\cdot 35.5, 0\\right)$\n-   $\\mu_{2,c} = \\left(\\frac{c}{100} \\cdot 39.5, 1\\right)$\n\nThe critical value $c^\\star$ is defined as the value of $c$ for which the squared Euclidean distances from the patient point $x_c$ to the two cluster means $\\mu_{1,c}$ and $\\mu_{2,c}$ are equal.\nLet's calculate these two squared distances, $d_1^2$ and $d_2^2$.\n\n$d_1^2 = \\|x_c - \\mu_{1,c}\\|_2^2 = \\left(\\frac{c}{100} \\cdot 50 - \\frac{c}{100} \\cdot 35.5\\right)^2 + (0 - 0)^2$\n$d_1^2 = \\left(\\frac{c}{100}(50 - 35.5)\\right)^2 = \\left(\\frac{c}{100} \\cdot 14.5\\right)^2 = \\left(\\frac{c}{100}\\right)^2 (14.5)^2$\n\n$d_2^2 = \\|x_c - \\mu_{2,c}\\|_2^2 = \\left(\\frac{c}{100} \\cdot 50 - \\frac{c}{100} \\cdot 39.5\\right)^2 + (0 - 1)^2$\n$d_2^2 = \\left(\\frac{c}{100}(50 - 39.5)\\right)^2 + (-1)^2 = \\left(\\frac{c}{100} \\cdot 10.5\\right)^2 + 1 = \\left(\\frac{c}{100}\\right)^2 (10.5)^2 + 1$\n\nAt the critical value $c = c^\\star$, we set $d_1^2 = d_2^2$:\n$$\\left(\\frac{c^\\star}{100}\\right)^2 (14.5)^2 = \\left(\\frac{c^\\star}{100}\\right)^2 (10.5)^2 + 1$$\nRearranging the terms to solve for $c^\\star$:\n$$\\left(\\frac{c^\\star}{100}\\right)^2 \\left( (14.5)^2 - (10.5)^2 \\right) = 1$$\nWe use the difference of squares formula $a^2 - b^2 = (a-b)(a+b)$:\n$$(14.5)^2 - (10.5)^2 = (14.5 - 10.5)(14.5 + 10.5) = (4)(25) = 100$$\nSubstituting this result back into the equation:\n$$\\left(\\frac{c^\\star}{100}\\right)^2 (100) = 1$$\n$$\\frac{(c^\\star)^2}{10000} \\cdot 100 = 1$$\n$$\\frac{(c^\\star)^2}{100} = 1$$\n$$(c^\\star)^2 = 100$$\nSince the problem states $c \\in (0, r]$, which is $c \\in (0, 100]$, we take the positive root:\n$$c^\\star = 10$$\n\nTo verify the behavior, for $c > 10$, $c^2 > 100$, so $(\\frac{c}{100})^2 (100) > 1$, which implies $d_1^2 > d_2^2$. The patient is assigned to cluster $2$, which matches the original assignment on the uncompressed scale where $d(x, \\mu_1)^2 = (14.5)^2 = 210.25$ and $d(x, \\mu_2)^2 = (10.5)^2 + 1^2 = 111.25$. For $c < 10$, the inequality flips, $d_1^2 < d_2^2$, and the assignment flips to cluster $1$. The value $c^\\star=10$ is therefore correct.", "answer": "$$\\boxed{10}$$", "id": "4576104"}, {"introduction": "Generating clusters is only the first step; assessing their quality is paramount to ensure the identified patient subgroups are distinct and meaningful. The Davies-Bouldin Index (DBI) provides a powerful metric for this internal validation by evaluating cluster compactness and separation. This practice challenges you to implement the DBI from its fundamental geometric principles, connecting the concepts of within-cluster scatter and between-cluster distance to the practical task of quantifying the quality of a patient stratification [@problem_id:4576086].", "problem": "A cohort of patients is represented in a standardized feature space in which each patient is a point in a Euclidean space with zero mean and unit variance for each feature, hence all measurements are dimensionless. Consider a stratification of the cohort into clusters, where each cluster is a subset of points. The goal is to define a quantitative index of cluster quality based on fundamental definitions from Euclidean geometry and then compute it for several patient stratifications. The interpretation must connect centroid separation and within-cluster scatter to clinical distinctness.\n\nUse the following context-appropriate fundamental base:\n- A cluster centroid is the arithmetic mean in Euclidean space: for cluster $i$ with point set $C_i$, the centroid is $c_i = \\frac{1}{|C_i|}\\sum_{x \\in C_i} x$.\n- Within-cluster scatter is the average Euclidean distance of points in a cluster to its centroid: for cluster $i$, $S_i = \\frac{1}{|C_i|}\\sum_{x \\in C_i} \\lVert x - c_i \\rVert_2$.\n- Between-cluster separation is the Euclidean distance between centroids: for clusters $i$ and $j$, $M_{ij} = \\lVert c_i - c_j \\rVert_2$.\n\nTask requirements:\n1. Starting only from the definitions above, define the Davies–Bouldin index (DBI) as a function of the cluster scatters and centroid separations without giving any shortcut formula in this problem statement. The definition must be mathematically consistent in the case of identical centroids and zero scatter.\n2. Implement a program that, for each provided test case, computes the Davies–Bouldin index using the definition you derived. The computation must use the Euclidean norm. If any pair of centroids has distance $M_{ij} = 0$ and the corresponding sum of scatters $S_i + S_j$ is positive, treat the corresponding ratio as $+\\infty$ using the Institute of Electrical and Electronics Engineers (IEEE) Standard $754$ representation. If $M_{ij} = 0$ and $S_i + S_j = 0$, treat the ratio as $0$.\n3. The program must format the final output as a single line containing the results for all test cases as a comma-separated list enclosed in square brackets, with each finite Davies–Bouldin index rounded to $4$ decimal places. Represent $+\\infty$ as the string `inf`.\n\nTest suite:\n- Case $1$ (happy path, well-separated clusters in $2$ dimensions): three clinically distinct strata with tight within-cluster scatter and large centroid separation\n  - Points $X_1$: $\\{(-2.0,-2.0),\\,(-1.8,-1.9),\\,(-2.2,-2.1),\\,(2.0,2.1),\\,(2.2,1.9),\\,(1.8,2.0),\\,(-2.0,2.0),\\,(-2.2,2.2),\\,(-1.8,1.8)\\}$\n  - Labels $y_1$: $\\{0,\\,0,\\,0,\\,1,\\,1,\\,1,\\,2,\\,2,\\,2\\}$\n- Case $2$ (overlapping strata in $3$ dimensions): two strata with moderate overlap due to similar profiles\n  - Points $X_2$: $\\{(-0.1,0.0,0.1),\\,(0.1,-0.1,0.0),\\,(0.0,0.1,-0.1),\\,(0.05,0.05,0.05),\\,(0.4,0.6,0.5),\\,(0.6,0.4,0.5),\\,(0.5,0.5,0.6),\\,(0.5,0.5,0.4)\\}$\n  - Labels $y_2$: $\\{0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,1,\\,1\\}$\n- Case $3$ (boundary condition: a zero-scatter stratum in $2$ dimensions): one stratum consists of a single patient, yielding zero scatter\n  - Points $X_3$: $\\{(1.0,-1.0),\\,(-1.0,-1.0),\\,(-1.2,-0.8),\\,(1.0,1.2),\\,(0.8,0.9)\\}$\n  - Labels $y_3$: $\\{0,\\,1,\\,1,\\,2,\\,2\\}$\n- Case $4$ (edge case: identical centroids in $2$ dimensions): two strata whose centroids coincide, creating undefined separation unless handled properly\n  - Points $X_4$: $\\{(1.0,0.0),\\,(-1.0,0.0),\\,(0.0,1.0),\\,(0.0,-1.0)\\}$\n  - Labels $y_4$: $\\{0,\\,0,\\,1,\\,1\\}$\n\nAnswer specification:\n- For each test case, the program must compute a single real number (the Davies–Bouldin index) or $+\\infty$ if applicable, and output them in order as a single line formatted exactly as `[r_1,r_2,r_3,r_4]` where each `$r_i$` is either a decimal rounded to $4$ places or the literal `inf`.", "solution": "The problem statement is valid. It is scientifically grounded in the domain of cluster analysis, a subfield of unsupervised machine learning, and presents a well-posed computational task based on clear, mathematically consistent definitions from Euclidean geometry. The provided data and constraints are complete and unambiguous, allowing for a unique and verifiable solution for each test case.\n\nWe begin by formalizing the definition of the Davies–Bouldin Index (DBI) using the provided fundamental concepts. The objective of clustering is to partition a set of data points into groups, or clusters, such that points within the same cluster are highly similar, and points in different clusters are dissimilar. The quality of such a partition can be quantified by comparing the 'compactness' of clusters (within-cluster scatter) to their 'separation' (between-cluster separation).\n\nLet the set of $N$ data points be $X = \\{x_1, x_2, \\ldots, x_N\\}$, where each $x_p$ is a vector in a $d$-dimensional Euclidean space. Let a partition of $X$ into $k$ clusters be $\\{C_1, C_2, \\ldots, C_k\\}$.\n\nFor each cluster $C_i$, we are given two fundamental quantities:\n1.  The centroid, $c_i$, which is the arithmetic mean of the points in the cluster:\n    $$c_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i} x$$\n    where $|C_i|$ is the number of points in cluster $C_i$.\n\n2.  The within-cluster scatter, $S_i$, which measures the average distance of points in the cluster to their centroid:\n    $$S_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i} \\lVert x - c_i \\rVert_2$$\n    where $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm. A smaller value of $S_i$ implies a more compact or 'tighter' cluster.\n\nFor any pair of distinct clusters, $C_i$ and $C_j$, we are given the between-cluster separation, $M_{ij}$:\n$$M_{ij} = \\lVert c_i - c_j \\rVert_2$$\nA larger value of $M_{ij}$ implies that the clusters are farther apart and thus more distinct.\n\nThe Davies–Bouldin Index is constructed by synthesizing these concepts. For any pair of clusters, $C_i$ and $C_j$, a measure of their similarity, $R_{ij}$, can be defined by combining their internal scatters and their external separation. An effective ratio is:\n$$R_{ij} = \\frac{S_i + S_j}{M_{ij}}$$\nThis ratio is large when the clusters are diffuse (large $S_i, S_j$) and close to each other (small $M_{ij}$), indicating poor clustering. Conversely, the ratio is small for tight, well-separated clusters. This ratio quantifies the overlap or ambiguity between two clusters.\n\nTo evaluate the quality of a single cluster, $C_i$, we consider its worst-case scenario. This involves finding the cluster $C_j$ ($j \\neq i$) that is most 'similar' to $C_i$ according to the measure $R_{ij}$. This worst-case similarity for cluster $C_i$ is denoted as $D_i$:\n$$D_i = \\max_{j \\neq i} \\left( R_{ij} \\right) = \\max_{j \\neq i} \\left( \\frac{S_i + S_j}{M_{ij}} \\right)$$\n\nFinally, the Davies–Bouldin Index for the entire clustering partition is the average of these worst-case similarity measures over all $k$ clusters:\n$$\\text{DBI} = \\frac{1}{k} \\sum_{i=1}^{k} D_i = \\frac{1}{k} \\sum_{i=1}^{k} \\max_{j \\neq i} \\left( \\frac{S_i + S_j}{M_{ij}} \\right)$$\nA lower DBI value corresponds to a better clustering partition, indicating that, on average, clusters are compact and well-separated from their most similar neighbors.\n\nThe problem specifies the handling of cases where the denominator $M_{ij}$ is zero. This occurs if and only if two distinct clusters, $C_i$ and $C_j$, have identical centroids ($c_i = c_j$).\n- If $M_{ij} = 0$ and the sum of scatters $S_i + S_j > 0$, it implies that at least one of the clusters contains points that are not at the common centroid. This represents a pathological overlap, where two distinct groups of points are centered at the same location. The separation is infinitely poor, and thus the ratio $R_{ij}$ is treated as $+\\infty$, consistent with the limit $\\lim_{m \\to 0^+} \\frac{c}{m}$ for $c>0$.\n- If $M_{ij} = 0$ and $S_i + S_j = 0$, this implies $S_i = 0$ and $S_j = 0$. A scatter of zero means all points in the cluster are identical to the centroid. Therefore, all points in $C_i$ are $c_i$, and all points in $C_j$ are $c_j$. Since $c_i = c_j$, all points in both clusters are the same single point. In this degenerate case, the clusters are identical and perfectly compact. The problem mandates that the ratio be treated as $0$, which is a reasonable convention for this indeterminate form $\\frac{0}{0}$.\n\nThe computational algorithm proceeds as follows:\n1.  For a given set of points and their cluster labels, partition the points into their respective clusters.\n2.  For each cluster $C_i$, compute its centroid $c_i$ and its within-cluster scatter $S_i$. A cluster with a single point will have a scatter of $0$.\n3.  Store the computed arrays of centroids and scatters.\n4.  Iterate through each cluster $C_i$ to calculate its worst-case similarity $D_i$:\n    a. For each other cluster $C_j$ ($j \\neq i$), calculate the separation $M_{ij} = \\lVert c_i - c_j \\rVert_2$.\n    b. Compute the ratio $R_{ij}$ using the specified rules for handling $M_{ij}=0$.\n    c. $D_i$ is the maximum of these $R_{ij}$ values over all $j \\neq i$.\n5.  Calculate the final DBI by averaging all the $D_i$ values. If any $D_i$ is $+\\infty$, the resulting DBI is also $+\\infty$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Davies-Bouldin Index for a suite of test cases.\n    \"\"\"\n\n    def compute_dbi(X: np.ndarray, y: np.ndarray) -> float:\n        \"\"\"\n        Calculates the Davies-Bouldin Index for a given clustering.\n\n        Args:\n            X: A numpy array of shape (n_samples, n_features) representing the data points.\n            y: A numpy array of shape (n_samples,) with integer labels for each point.\n\n        Returns:\n            The Davies-Bouldin Index as a float. Returns np.inf for pathological cases.\n        \"\"\"\n        unique_labels = np.unique(y)\n        k = len(unique_labels)\n\n        if k <= 1:\n            return 0.0  # DBI is conventionally 0 for a single cluster.\n\n        # Step 1 & 2: Compute centroids and scatters for each cluster\n        centroids = []\n        scatters = []\n        for label in unique_labels:\n            cluster_points = X[y == label]\n            \n            # Compute centroid\n            centroid = np.mean(cluster_points, axis=0)\n            centroids.append(centroid)\n\n            # Compute scatter\n            if cluster_points.shape[0] > 0:\n                distances = np.linalg.norm(cluster_points - centroid, axis=1)\n                scatter = np.mean(distances)\n            else: # Should not happen with np.unique\n                scatter = 0.0\n            scatters.append(scatter)\n\n        centroids = np.array(centroids)\n        scatters = np.array(scatters)\n        \n        # Step 3 & 4: Compute the DBI\n        total_D = 0.0\n        for i in range(k):\n            max_R_i = 0.0\n            for j in range(k):\n                if i == j:\n                    continue\n\n                # Compute separation Mij\n                M_ij = np.linalg.norm(centroids[i] - centroids[j])\n                \n                # Compute ratio Rij with special handling\n                S_sum = scatters[i] + scatters[j]\n                \n                if M_ij == 0:\n                    if S_sum > 0:\n                        R_ij = np.inf\n                    else:  # S_sum == 0, implies S_i=0 and S_j=0\n                        R_ij = 0.0\n                else:\n                    R_ij = S_sum / M_ij\n                \n                if R_ij > max_R_i:\n                    max_R_i = R_ij\n            \n            # max_R_i is now D_i\n            total_D += max_R_i\n            \n            # If any D_i is infinity, the total sum will be infinity\n            if total_D == np.inf:\n                break\n\n        # Step 5: Final DBI is the average of D_i values\n        return total_D / k\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (np.array([\n            [-2.0, -2.0], [-1.8, -1.9], [-2.2, -2.1],\n            [2.0, 2.1], [2.2, 1.9], [1.8, 2.0],\n            [-2.0, 2.0], [-2.2, 2.2], [-1.8, 1.8]\n        ]), np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])),\n        # Case 2\n        (np.array([\n            [-0.1, 0.0, 0.1], [0.1, -0.1, 0.0], [0.0, 0.1, -0.1], [0.05, 0.05, 0.05],\n            [0.4, 0.6, 0.5], [0.6, 0.4, 0.5], [0.5, 0.5, 0.6], [0.5, 0.5, 0.4]\n        ]), np.array([0, 0, 0, 0, 1, 1, 1, 1])),\n        # Case 3\n        (np.array([\n            [1.0, -1.0], \n            [-1.0, -1.0], [-1.2, -0.8],\n            [1.0, 1.2], [0.8, 0.9]\n        ]), np.array([0, 1, 1, 2, 2])),\n        # Case 4\n        (np.array([\n            [1.0, 0.0], [-1.0, 0.0], \n            [0.0, 1.0], [0.0, -1.0]\n        ]), np.array([0, 0, 1, 1]))\n    ]\n\n    results = []\n    for X, y in test_cases:\n        result = compute_dbi(X, y)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    formatted_results = []\n    for res in results:\n        if res == np.inf:\n            formatted_results.append('inf')\n        else:\n            formatted_results.append(f'{res:.4f}')\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4576086"}, {"introduction": "Real-world clinical datasets are often incomplete, and the way we handle missing data can profoundly impact analysis. This advanced simulation demonstrates how a specific non-random missingness pattern (MNAR), when combined with naive mean imputation, can create artificial data structures that k-means may misinterpret as genuine patient subgroups. By building this simulation and a corresponding sensitivity analysis, you will learn to detect these dangerous artifacts and appreciate the importance of critically examining your data processing pipeline [@problem_id:4576069].", "problem": "You are asked to formalize and implement a simulation-based study of how Missing Not At Random (MNAR) missingness in a key clinical laboratory variable can induce spurious clusters under naive imputation when applying $k$-means clustering for patient stratification. Your program must be fully deterministic and must implement all algorithms from first principles using only basic numerical operations.\n\nStart from the following foundational base and definitions:\n\n1. Clustering objective. The $k$-means objective for data matrix $X \\in \\mathbb{R}^{n \\times d}$ and cluster centroids $C = \\{c_{1},\\dots,c_{k}\\}$ with labels $\\ell_{i} \\in \\{1,\\dots,k\\}$ is to minimize the within-cluster sum of squares\n$$\n\\sum_{i=1}^{n} \\left\\| x_{i} - c_{\\ell_{i}} \\right\\|_{2}^{2}.\n$$\nA standard algorithm alternates minimizing with respect to labels and centroids.\n\n2. Internal validation. The silhouette index for a point $i$ with label $\\ell_{i}$ is\n$$\ns_{i} = \\frac{b_{i} - a_{i}}{\\max\\{a_{i}, b_{i}\\}},\n$$\nwhere $a_{i}$ is the average distance from $i$ to all other points in its own cluster and $b_{i}$ is the minimal average distance from $i$ to points in any other cluster. The overall silhouette is the arithmetic mean of $s_{i}$ over $i \\in \\{1,\\dots,n\\}$. Use Euclidean distance.\n\n3. Missingness mechanisms. Let $X = (X_{1}, X_{2}, X_{3})$ denote three clinical measurements with $X_{1}$ being a key laboratory. Let $R$ denote the missingness indicator for $X_{1}$, with $R=1$ if observed and $R=0$ if missing. Missing Completely At Random (MCAR) means $\\mathbb{P}(R=1 \\mid X)$ is constant; Missing Not At Random (MNAR) means $\\mathbb{P}(R=1 \\mid X)$ depends on unobserved values, e.g., directly on $X_{1}$.\n\n4. Naive imputation. When $X_{1}$ is missing ($R=0$), a naive mean imputation replaces it by the empirical mean of observed $X_{1}$ values.\n\n5. Sensitivity to key-lab dependence. A simple sensitivity analysis is to recompute the clustering validity after excluding the key lab feature $X_{1}$, thereby testing whether the apparent clustering structure is driven by artifacts of imputation in $X_{1}$.\n\nTasks to implement, from these base definitions only:\n\nA. Synthetic patient generator. For each test case below, generate $n$ patients with a true latent stratum $Y \\in \\{0,1\\}$, equally likely, and conditionally Gaussian features $X \\in \\mathbb{R}^{3}$ with independent coordinates. Use two strata with means differing primarily in the key lab $X_{1}$, so that the disease signal is concentrated in $X_{1}$. Let the conditional distribution of $X$ given $Y=y$ be multivariate normal with mean vector $\\mu^{(y)}$ and diagonal covariance with standard deviations specified below.\n\nB. Missingness. For the key lab $X_{1}$, induce missingness according to the specified mechanism in each test case:\n- For MCAR with parameter $q \\in [0,1]$, set $\\mathbb{P}(R=1 \\mid X) = 1 - q$ independently of $X$.\n- For MNAR with parameters $\\alpha \\in \\mathbb{R}$ and $\\tau \\in \\mathbb{R}$, set\n$$\n\\mathbb{P}(R=1 \\mid X_{1}=x) = 1 - \\sigma\\left(\\alpha (x - \\tau)\\right), \\quad \\sigma(z) = \\frac{1}{1 + e^{-z}}.\n$$\nThis choice makes higher $x$ less likely to be observed when $\\alpha > 0$ and $\\tau$ is a threshold.\n\nC. Imputation. Compute the empirical mean $\\bar{x}_{1,\\text{obs}}$ of observed $X_{1}$ across all patients with $R=1$, and for all patients with $R=0$ set $X_{1} \\leftarrow \\bar{x}_{1,\\text{obs}}$ (leave $X_{2}$ and $X_{3}$ unchanged, as they have no missingness).\n\nD. Clustering and internal validation. For each dataset, compute the silhouette index for $k \\in \\{2,3\\}$ using $k$-means clustering with a fixed number of random initializations and iterations. Use Euclidean distance and choose the best run by minimal within-cluster sum of squares. Compute silhouettes for:\n- The full feature set $(X_{1}, X_{2}, X_{3})$ after imputation.\n- The reduced feature set $(X_{2}, X_{3})$ obtained by excluding $X_{1}$ entirely.\n\nE. Artifact detection rule. Declare that a “spurious cluster due to naive imputation” is detected if and only if both of the following hold:\n- The silhouette with $k=3$ exceeds that with $k=2$ when using the full features, i.e., $S_{\\text{full}}(3) > S_{\\text{full}}(2)$, suggesting a preference for three clusters.\n- After excluding $X_{1}$, the silhouette no longer prefers three clusters, i.e., $S_{\\setminus X_{1}}(3) \\le S_{\\setminus X_{1}}(2)$.\nReturn a boolean per test case indicating whether this artifact is detected.\n\nImplementation and numerical details that must be followed:\n- Use a fixed random seed $42$ for all randomness.\n- Use $k$-means with $k \\in \\{2,3\\}$, $n_{\\text{init}} = 20$ random initializations, and at most $100$ Lloyd iterations per initialization.\n- For silhouette calculation, if a point is the only member of its cluster, define its silhouette contribution as $0$.\n\nTest suite. Your program must run exactly the following three test cases and output a boolean result for each, in order:\n\n- Test case $1$ (MNAR, strong artifact expected): $n=600$ patients; dimension $d=3$ features; strata means\n$$\n\\mu^{(0)} = (0, 0, 0), \\quad \\mu^{(1)} = (5, 0, 0),\n$$\nstandard deviations $(0.7, 0.6, 0.6)$ for both strata; MNAR missingness on $X_{1}$ with $\\alpha = 1.2$, $\\tau = 4.0$.\n\n- Test case $2$ (MCAR, no artifact expected): same data-generating parameters as Test case $1$ but MCAR on $X_{1}$ with $q = 0.05$.\n\n- Test case $3$ (No missingness, baseline): same data-generating parameters as Test case $1$ but no missingness on $X_{1}$, i.e., $q = 0$.\n\nRequired final output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true_case1,true_case2,true_case3]\") using Python boolean literals with capitalized first letters, in the order of the test cases, with no extra whitespace. Concretely, print a single line in the form\n$$\n[\\text{result}_{1},\\text{result}_{2},\\text{result}_{3}],\n$$\nwhere each $\\text{result}_{j}$ is either True or False.", "solution": "The problem requires the implementation of a simulation study to demonstrate how a Missing Not At Random (MNAR) data mechanism, combined with naive mean imputation, can introduce spurious clusters when analyzed with $k$-means clustering. The validity of the problem statement is confirmed as it is scientifically grounded in established statistical principles, well-posed with specific, deterministic instructions, and objective in its formulation. All algorithms and metrics are standard in the field of data analysis and bioinformatics.\n\nThe core principle to be demonstrated is that the interaction between a specific missingness pattern and a simple imputation method can create artificial structure in the data that a clustering algorithm may incorrectly identify as a meaningful biological subgroup. The simulation is structured to test this hypothesis under controlled conditions.\n\nThe solution will be constructed by implementing a series of components as specified, proceeding from data generation to final artifact detection. All mathematical entities are rendered in LaTeX as required.\n\n**A. Synthetic Patient Data Generation**\n\nFirst, we generate a synthetic dataset of $n$ patients. Each patient belongs to one of two true latent strata, $Y \\in \\{0, 1\\}$, with equal probability, i.e., $\\mathbb{P}(Y=0) = \\mathbb{P}(Y=1) = 0.5$. The data for each patient consists of $d=3$ features, $X = (X_1, X_2, X_3)$, which are conditionally independent given the stratum $Y$. The conditional distribution of $X$ for a patient in stratum $y$ is a multivariate normal distribution, $X | (Y=y) \\sim \\mathcal{N}(\\mu^{(y)}, \\Sigma)$, where the covariance matrix $\\Sigma$ is diagonal. The mean vectors are given as $\\mu^{(0)} = (0, 0, 0)$ and $\\mu^{(1)} = (5, 0, 0)$. The standard deviations for the features are $(0.7, 0.6, 0.6)$, constant across both strata. This setup ensures that the true underlying biological signal distinguishing the two strata is concentrated entirely within the key laboratory feature $X_1$. For all simulations, a fixed random seed of $42$ is used to ensure reproducibility.\n\n**B. Missingness Induction**\n\nMissingness is introduced only in the key feature $X_1$. We model two distinct mechanisms:\n\n1.  **Missing Completely At Random (MCAR)**: The probability that a value of $X_1$ is observed is constant and independent of any data values. Let $R_i=1$ if the $i$-th patient's $X_1$ is observed and $R_i=0$ if it is missing. For MCAR with parameter $q$, we set $\\mathbb{P}(R_i=1) = 1-q$ for all $i$.\n\n2.  **Missing Not At Random (MNAR)**: The probability of missingness depends on the value of $X_1$ itself. The specified model is:\n    $$\n    \\mathbb{P}(R_i=1 \\mid X_{1,i}=x) = 1 - \\sigma\\left(\\alpha (x - \\tau)\\right)\n    $$\n    where $\\sigma(z) = (1 + e^{-z})^{-1}$ is the standard logistic (sigmoid) function. In this model, if the parameter $\\alpha > 0$, values of $x$ significantly greater than the threshold $\\tau$ will have a high probability of being missing, as $\\sigma(\\cdot)$ will approach $1$. This simulates a plausible clinical scenario where, for instance, extremely high lab values might be subject to re-testing or instrument failure, leading to them being missing in an initial data pull.\n\n**C. Naive Mean Imputation**\n\nThe missing values in $X_1$ are handled using naive mean imputation. First, the empirical mean of all *observed* values of $X_1$ is calculated:\n$$\n\\bar{x}_{1,\\text{obs}} = \\frac{\\sum_{i=1}^{n} X_{1,i} \\cdot \\mathbb{I}(R_i=1)}{\\sum_{i=1}^{n} \\mathbb{I}(R_i=1)}\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. Then, every missing entry in $X_1$ (where $R_i=0$) is replaced by this single value, $\\bar{x}_{1,\\text{obs}}$. When the missingness is MNAR and high values are preferentially missing, $\\bar{x}_{1,\\text{obs}}$ will be systematically biased downwards compared to the true mean of the stratum from which the missing values originate. This process creates an artificial concentration of data points at the value $\\bar{x}_{1,\\text{obs}}$ for the $X_1$ feature.\n\n**D. Clustering and Internal Validation**\n\nThe core of the analysis involves applying the $k$-means clustering algorithm and evaluating the results using the silhouette index.\n\nThe **$k$-means algorithm** (Lloyd's algorithm) is an iterative procedure to partition the $n$ data points into $k$ clusters. It aims to minimize the within-cluster sum of squares (WCSS):\n$$\n\\text{WCSS} = \\sum_{j=1}^{k} \\sum_{x_i \\in S_j} \\| x_i - c_j \\|_2^2\n$$\nwhere $S_j$ is the set of points in cluster $j$, and $c_j$ is the centroid (mean) of the points in $S_j$. The algorithm alternates between assigning each point to its nearest centroid and re-calculating each centroid as the mean of its assigned points. To mitigate sensitivity to initial conditions, this process is repeated $n_{\\text{init}} = 20$ times with different random initial centroids, and the partition that yields the minimum WCSS is selected. A maximum of $100$ iterations are performed for each initialization.\n\nThe **silhouette index** provides a measure of how well-separated the resulting clusters are. For each point $i$, the silhouette value $s_i$ is defined as:\n$$\ns_i = \\frac{b_i - a_i}{\\max\\{a_i, b_i\\}}\n$$\nHere, $a_i$ is the average Euclidean distance from point $i$ to all other points within the same cluster. $b_i$ is the smallest average Euclidean distance from point $i$ to all points in any *other* cluster. The overall silhouette score for a given clustering is the mean of all $s_i$. A score near $1$ indicates well-separated clusters, while a score near $0$ suggests overlapping clusters. If a cluster contains only a single point, its silhouette value is defined as $0$.\n\nThis analysis is performed for $k=2$ and $k=3$ on two datasets:\n1.  The full, imputed dataset with features $(X_1, X_2, X_3)$.\n2.  The reduced dataset with features $(X_2, X_3)$, excluding the key lab feature $X_1$ entirely.\n\n**E. Spurious Cluster Artifact Detection**\n\nThe central hypothesis is that the MNAR mechanism, coupled with mean imputation, creates a spurious third cluster composed of patients whose high $X_1$ values were replaced by the imputed mean. This leads the silhouette score to incorrectly favor a $k=3$ solution over the true $k=2$ solution. The artifact is formally detected if two conditions are met simultaneously:\n\n1.  The silhouette score for the full, imputed dataset is higher for $k=3$ than for $k=2$: $S_{\\text{full}}(3) > S_{\\text{full}}(2)$.\n2.  After removing the artifact-driving feature $X_1$, the preference for $k=3$ disappears. The remaining data $(X_2, X_3)$ has no real structure (as both true strata have a mean of $0$ for these features), so a $k=3$ solution should not be preferred: $S_{\\setminus X_{1}}(3) \\le S_{\\setminus X_{1}}(2)$.\n\nThis two-part rule constitutes a sensitivity analysis. A positive result for this rule indicates that the clustering structure is not robust and is likely an artifact of the data processing steps. The procedure is applied to each of the three test cases specified.", "answer": "```python\nimport numpy as np\nfrom scipy.special import expit as sigmoid\n\n# Use a fixed random number generator for reproducibility as per problem specification.\nRNG = np.random.default_rng(42)\n\ndef generate_data(n, mu0, mu1, stds):\n    \"\"\"\n    Generates synthetic patient data with two latent strata.\n    \"\"\"\n    true_labels = RNG.integers(0, 2, size=n)\n    X = np.zeros((n, 3))\n    \n    mask0 = (true_labels == 0)\n    n0 = np.sum(mask0)\n    X[mask0, :] = RNG.normal(loc=mu0, scale=stds, size=(n0, 3))\n    \n    mask1 = (true_labels == 1)\n    n1 = n - n0\n    X[mask1, :] = RNG.normal(loc=mu1, scale=stds, size=(n1, 3))\n    \n    return X, true_labels\n\ndef induce_missingness(X1, mechanism, params):\n    \"\"\"\n    Induces missingness in a single feature vector X1.\n    \"\"\"\n    n = len(X1)\n    X1_missing = X1.copy()\n    \n    if mechanism == 'mcar':\n        q = params['q']\n        is_missing = RNG.uniform(0, 1, size=n) < q\n        X1_missing[is_missing] = np.nan\n    elif mechanism == 'mnar':\n        alpha, tau = params['alpha'], params['tau']\n        # Probability of being MISSING is sigma(alpha * (x - tau))\n        # P(R=1) = 1 - sigma(...) => P(R=0) = sigma(...)\n        prob_missing = sigmoid(alpha * (X1 - tau))\n        is_missing = RNG.uniform(0, 1, size=n) < prob_missing\n        X1_missing[is_missing] = np.nan\n    \n    return X1_missing\n\ndef impute_mean(X_with_nan):\n    \"\"\"\n    Performs mean imputation on a feature vector with np.nan for missing values.\n    \"\"\"\n    X_imputed = X_with_nan.copy()\n    if np.any(np.isnan(X_imputed)):\n        mean_obs = np.nanmean(X_imputed)\n        X_imputed[np.isnan(X_imputed)] = mean_obs\n    return X_imputed\n\ndef kmeans(X, k, n_init, max_iter):\n    \"\"\"\n    Implements k-means clustering from first principles.\n    \"\"\"\n    best_wcss = np.inf\n    best_labels = None\n    best_centroids = None\n\n    for _ in range(n_init):\n        # Randomly initialize centroids by picking k unique points from data\n        initial_indices = RNG.choice(X.shape[0], size=k, replace=False)\n        centroids = X[initial_indices]\n        \n        for i in range(max_iter):\n            # Assignment step: compute distances and assign labels\n            dist_sq = np.sum((X[:, np.newaxis, :] - centroids[np.newaxis, :, :])**2, axis=2)\n            labels = np.argmin(dist_sq, axis=1)\n            \n            # Update step: compute new centroids\n            new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n            \n            # Handle empty clusters by not updating their centroid (rare, but possible)\n            for j in range(k):\n                if np.sum(labels == j) == 0:\n                    new_centroids[j] = centroids[j]\n            \n            # Check for convergence\n            if np.allclose(new_centroids, centroids):\n                break\n            centroids = new_centroids\n\n        # Calculate WCSS for this run\n        wcss = 0\n        for j in range(k):\n            cluster_points = X[labels == j]\n            if len(cluster_points) > 0:\n                wcss += np.sum((cluster_points - centroids[j])**2)\n        \n        if wcss < best_wcss:\n            best_wcss = wcss\n            best_labels = labels\n            best_centroids = centroids\n            \n    return best_labels, best_centroids\n\ndef silhouette_score(X, labels):\n    \"\"\"\n    Implements silhouette index calculation from first principles.\n    \"\"\"\n    n_samples = X.shape[0]\n    unique_labels = np.unique(labels)\n    n_clusters = len(unique_labels)\n\n    if n_clusters < 2:\n        return 0.0\n\n    # Pre-compute pairwise distance matrix\n    # Using squared Euclidean distance and then sqrt is often more stable/faster\n    dist_matrix = np.sqrt(np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :])**2, axis=2))\n    \n    silhouette_vals = np.zeros(n_samples)\n    \n    for i in range(n_samples):\n        label_i = labels[i]\n        \n        # a_i: mean intra-cluster distance\n        mask_i = (labels == label_i)\n        cluster_size_i = np.sum(mask_i)\n        if cluster_size_i == 1:\n            silhouette_vals[i] = 0  # As per problem definition\n            continue\n        a_i = np.sum(dist_matrix[i, mask_i]) / (cluster_size_i - 1)\n        \n        # b_i: mean nearest-cluster distance\n        min_mean_dist = np.inf\n        for j in unique_labels:\n            if j == label_i:\n                continue\n            mask_j = (labels == j)\n            mean_dist_j = np.mean(dist_matrix[i, mask_j])\n            min_mean_dist = min(min_mean_dist, mean_dist_j)\n        b_i = min_mean_dist\n        \n        # Silhouette for point i\n        if max(a_i, b_i) == 0:\n            silhouette_vals[i] = 0\n        else:\n            silhouette_vals[i] = (b_i - a_i) / max(a_i, b_i)\n            \n    return np.mean(silhouette_vals)\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases.\n    \"\"\"\n    # K-means parameters\n    N_INIT = 20\n    MAX_ITER = 100\n\n    test_cases = [\n        # Case 1: MNAR, strong artifact expected\n        {'n': 600, 'mu0': [0, 0, 0], 'mu1': [5, 0, 0], 'stds': [0.7, 0.6, 0.6], \n         'missingness': {'mechanism': 'mnar', 'params': {'alpha': 1.2, 'tau': 4.0}}},\n        # Case 2: MCAR, no artifact expected\n        {'n': 600, 'mu0': [0, 0, 0], 'mu1': [5, 0, 0], 'stds': [0.7, 0.6, 0.6], \n         'missingness': {'mechanism': 'mcar', 'params': {'q': 0.05}}},\n        # Case 3: No missingness, baseline\n        {'n': 600, 'mu0': [0, 0, 0], 'mu1': [5, 0, 0], 'stds': [0.7, 0.6, 0.6], \n         'missingness': {'mechanism': 'mcar', 'params': {'q': 0.0}}},\n    ]\n    \n    results = []\n\n    for case in test_cases:\n        # A. Generate data\n        X, _ = generate_data(\n            n=case['n'], \n            mu0=np.array(case['mu0']), \n            mu1=np.array(case['mu1']), \n            stds=np.array(case['stds'])\n        )\n\n        # B. Induce missingness\n        X1_with_nan = induce_missingness(\n            X[:, 0], \n            case['missingness']['mechanism'], \n            case['missingness']['params']\n        )\n        \n        # C. Impute\n        X1_imputed = impute_mean(X1_with_nan)\n        X_full = X.copy()\n        X_full[:, 0] = X1_imputed\n        \n        # Define reduced feature set\n        X_reduced = X[:, 1:]\n\n        # D. Clustering and validation for full features\n        labels_full_k2, _ = kmeans(X_full, k=2, n_init=N_INIT, max_iter=MAX_ITER)\n        s_full_k2 = silhouette_score(X_full, labels_full_k2)\n        \n        labels_full_k3, _ = kmeans(X_full, k=3, n_init=N_INIT, max_iter=MAX_ITER)\n        s_full_k3 = silhouette_score(X_full, labels_full_k3)\n\n        # D. Clustering and validation for reduced features\n        labels_reduced_k2, _ = kmeans(X_reduced, k=2, n_init=N_INIT, max_iter=MAX_ITER)\n        s_reduced_k2 = silhouette_score(X_reduced, labels_reduced_k2)\n\n        labels_reduced_k3, _ = kmeans(X_reduced, k=3, n_init=N_INIT, max_iter=MAX_ITER)\n        s_reduced_k3 = silhouette_score(X_reduced, labels_reduced_k3)\n        \n        # E. Artifact detection rule\n        is_artifact_detected = (s_full_k3 > s_full_k2) and (s_reduced_k3 <= s_reduced_k2)\n        results.append(is_artifact_detected)\n\n    # Final output formatting\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the simulation\nsolve()\n```", "id": "4576069"}]}