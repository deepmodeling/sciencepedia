{"hands_on_practices": [{"introduction": "Hierarchical agglomerative clustering builds a tree of relationships by iteratively merging the most similar clusters. The specific criterion for 'most similar' defines the linkage method. This first practice focuses on the mechanics of Ward's linkage, a popular method that merges clusters to achieve the minimum possible increase in total within-cluster variance. By manually performing one full iteration, you will gain a concrete understanding of how cluster centroids and the within-cluster sum of squares ($WCSS$) are used to make optimal merge decisions [@problem_id:4572309].", "problem": "A cohort of $4$ tumor biopsies is assayed on $2$ genes, and the gene expression values have been standardized to $z$-scores (unitless). Let the sample-level expression vectors be\n- Sample $1$: $\\left(-0.5,\\,0.0\\right)$,\n- Sample $2$: $\\left(0.0,\\,0.0\\right)$,\n- Sample $3$: $\\left(1.0,\\,0.0\\right)$,\n- Sample $4$: $\\left(1.0,\\,1.0\\right)$.\n\nConsider hierarchical agglomerative clustering with Ward’s linkage, initialized with each sample as its own cluster. Using only the definitions of a cluster centroid (the arithmetic mean of its member vectors) and within-cluster sum of squares (the sum over members of the squared Euclidean distance to the cluster centroid), execute one full iteration of Ward merging as follows:\n- For every possible pairwise merge among the current singleton clusters, compute the candidate merged cluster’s centroid and the within-cluster sum of squares that would result from that merge.\n- Identify the pair whose merge yields the minimal increase in the total within-cluster sum of squares and perform that merge.\n- After this merge, report the resulting total within-cluster sum of squares across the new clustering.\n\nExpress your final answer as a single real number, rounded to four significant figures. No units are required (values are unitless).", "solution": "The problem is validated as follows.\n\n### Step 1: Extract Givens\n- Number of samples: $4$\n- Number of features (genes): $2$\n- Sample vectors (unitless $z$-scores):\n  - Sample $1$ ($S_1$): $\\left(-0.5,\\,0.0\\right)$\n  - Sample $2$ ($S_2$): $\\left(0.0,\\,0.0\\right)$\n  - Sample $3$ ($S_3$): $\\left(1.0,\\,0.0\\right)$\n  - Sample $4$ ($S_4$): $\\left(1.0,\\,1.0\\right)$\n- Clustering method: Hierarchical agglomerative clustering\n- Linkage criterion: Ward’s linkage\n- Initialization: Each sample is its own cluster.\n- Definitions to be used:\n  - Cluster centroid: The arithmetic mean of its member vectors.\n  - Within-cluster sum of squares (WCSS): The sum over members of the squared Euclidean distance to the cluster centroid.\n- Task:\n  1. For every possible pairwise merge of the initial singleton clusters, compute the resulting merged cluster's centroid and WCSS.\n  2. Identify the pair whose merge yields the minimal increase in total WCSS.\n  3. Perform this merge.\n  4. Report the resulting total WCSS across the new clustering.\n  5. The final answer must be rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, using standard definitions from cluster analysis, specifically hierarchical agglomerative clustering with Ward's linkage. These are fundamental concepts in statistics and bioinformatics. The problem is well-posed, providing all necessary data (sample vectors) and clear, unambiguous definitions and instructions to arrive at a unique solution. The language is objective and precise. The problem is self-contained and computationally feasible. There are no contradictions or factual errors.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n### Solution\nThe initial state of the clustering consists of four singleton clusters, one for each sample: $C_1 = \\{S_1\\}$, $C_2 = \\{S_2\\}$, $C_3 = \\{S_3\\}$, and $C_4 = \\{S_4\\}$.\n\nThe within-cluster sum of squares (WCSS) for any singleton cluster is $0$, because the single point in the cluster is identical to the cluster's centroid. Therefore, the initial total WCSS for the system of four clusters is:\n$$\n\\text{Total WCSS}_{\\text{initial}} = \\text{WCSS}(C_1) + \\text{WCSS}(C_2) + \\text{WCSS}(C_3) + \\text{WCSS}(C_4) = 0 + 0 + 0 + 0 = 0\n$$\nWard’s linkage criterion seeks to merge the pair of clusters that results in the minimum increase in the total WCSS. The increase in total WCSS from merging two clusters, $C_i$ and $C_j$, is denoted as $\\Delta\\text{WCSS}(C_i, C_j)$. It is given by:\n$$\n\\Delta\\text{WCSS}(C_i, C_j) = \\text{WCSS}(C_i \\cup C_j) - (\\text{WCSS}(C_i) + \\text{WCSS}(C_j))\n$$\nSince the initial clusters are all singletons, their WCSS is $0$. Thus, for the first merge, the increase in total WCSS is simply the WCSS of the new two-point cluster:\n$$\n\\Delta\\text{WCSS}(C_i, C_j) = \\text{WCSS}(C_i \\cup C_j)\n$$\nA well-known formula for the increase in WCSS (also called the merging cost for Ward's method) when merging two clusters $C_i$ and $C_j$ with $n_i$ and $n_j$ points and centroids $\\mu_i$ and $\\mu_j$ respectively, is:\n$$\n\\Delta\\text{WCSS}(C_i, C_j) = \\frac{n_i n_j}{n_i + n_j} \\|\\mu_i - \\mu_j\\|^2\n$$\nwhere $\\|\\mu_i - \\mu_j\\|^2$ is the squared Euclidean distance between the centroids. For merging two singleton clusters $\\{S_i\\}$ and $\\{S_j\\}$, we have $n_i=1$, $n_j=1$, $\\mu_i=S_i$, and $\\mu_j=S_j$. The formula simplifies to:\n$$\n\\Delta\\text{WCSS}(\\{S_i\\}, \\{S_j\\}) = \\frac{1 \\times 1}{1 + 1} \\|S_i - S_j\\|^2 = \\frac{1}{2} \\|S_i - S_j\\|^2\n$$\nWe must now calculate this value for all $\\binom{4}{2} = 6$ possible pairs of samples.\n\n1.  **Pair $(S_1, S_2)$:** $S_1 = (-0.5, 0.0)$, $S_2 = (0.0, 0.0)$\n    $$\n    \\Delta\\text{WCSS}(C_1, C_2) = \\frac{1}{2} \\|S_1 - S_2\\|^2 = \\frac{1}{2} \\left[ (-0.5 - 0.0)^2 + (0.0 - 0.0)^2 \\right] = \\frac{1}{2} (0.25) = 0.125\n    $$\n\n2.  **Pair $(S_1, S_3)$:** $S_1 = (-0.5, 0.0)$, $S_3 = (1.0, 0.0)$\n    $$\n    \\Delta\\text{WCSS}(C_1, C_3) = \\frac{1}{2} \\|S_1 - S_3\\|^2 = \\frac{1}{2} \\left[ (-0.5 - 1.0)^2 + (0.0 - 0.0)^2 \\right] = \\frac{1}{2} ((-1.5)^2) = \\frac{1}{2} (2.25) = 1.125\n    $$\n\n3.  **Pair $(S_1, S_4)$:** $S_1 = (-0.5, 0.0)$, $S_4 = (1.0, 1.0)$\n    $$\n    \\Delta\\text{WCSS}(C_1, C_4) = \\frac{1}{2} \\|S_1 - S_4\\|^2 = \\frac{1}{2} \\left[ (-0.5 - 1.0)^2 + (0.0 - 1.0)^2 \\right] = \\frac{1}{2} (2.25 + 1.0) = \\frac{1}{2} (3.25) = 1.625\n    $$\n\n4.  **Pair $(S_2, S_3)$:** $S_2 = (0.0, 0.0)$, $S_3 = (1.0, 0.0)$\n    $$\n    \\Delta\\text{WCSS}(C_2, C_3) = \\frac{1}{2} \\|S_2 - S_3\\|^2 = \\frac{1}{2} \\left[ (0.0 - 1.0)^2 + (0.0 - 0.0)^2 \\right] = \\frac{1}{2} (1.0) = 0.5\n    $$\n\n5.  **Pair $(S_2, S_4)$:** $S_2 = (0.0, 0.0)$, $S_4 = (1.0, 1.0)$\n    $$\n    \\Delta\\text{WCSS}(C_2, C_4) = \\frac{1}{2} \\|S_2 - S_4\\|^2 = \\frac{1}{2} \\left[ (0.0 - 1.0)^2 + (0.0 - 1.0)^2 \\right] = \\frac{1}{2} (1.0 + 1.0) = \\frac{1}{2} (2.0) = 1.0\n    $$\n\n6.  **Pair $(S_3, S_4)$:** $S_3 = (1.0, 0.0)$, $S_4 = (1.0, 1.0)$\n    $$\n    \\Delta\\text{WCSS}(C_3, C_4) = \\frac{1}{2} \\|S_3 - S_4\\|^2 = \\frac{1}{2} \\left[ (1.0 - 1.0)^2 + (0.0 - 1.0)^2 \\right] = \\frac{1}{2} (1.0) = 0.5\n    $$\n\nComparing the increase in WCSS for all possible merges:\n- Merge $(S_1, S_2)$: $\\Delta\\text{WCSS} = 0.125$\n- Merge $(S_1, S_3)$: $\\Delta\\text{WCSS} = 1.125$\n- Merge $(S_1, S_4)$: $\\Delta\\text{WCSS} = 1.625$\n- Merge $(S_2, S_3)$: $\\Delta\\text{WCSS} = 0.5$\n- Merge $(S_2, S_4)$: $\\Delta\\text{WCSS} = 1.0$\n- Merge $(S_3, S_4)$: $\\Delta\\text{WCSS} = 0.5$\n\nThe minimal increase in WCSS is $0.125$, which corresponds to merging clusters $C_1$ and $C_2$.\n\nFollowing the identified merge, the new set of clusters is $\\{C_{12}, C_3, C_4\\}$, where $C_{12} = C_1 \\cup C_2 = \\{S_1, S_2\\}$. The problem asks for the total WCSS of this new configuration.\n\nThe total WCSS is the sum of the WCSS of the individual clusters in the new partition:\n$$\n\\text{Total WCSS}_{\\text{new}} = \\text{WCSS}(C_{12}) + \\text{WCSS}(C_3) + \\text{WCSS}(C_4)\n$$\nAs $C_3$ and $C_4$ remain singleton clusters, their WCSS is still $0$.\n$$\n\\text{WCSS}(C_3) = 0 \\quad \\text{and} \\quad \\text{WCSS}(C_4) = 0\n$$\nThe WCSS of the newly formed cluster $C_{12}$ is, by definition, the increase in total WCSS caused by its formation. We have already calculated this value:\n$$\n\\text{WCSS}(C_{12}) = \\Delta\\text{WCSS}(C_1, C_2) = 0.125\n$$\nTherefore, the new total WCSS is:\n$$\n\\text{Total WCSS}_{\\text{new}} = 0.125 + 0 + 0 = 0.125\n$$\nAlternatively, we can calculate $\\text{WCSS}(C_{12})$ from its definition. The centroid of $C_{12}$ is $\\mu_{12} = \\frac{S_1 + S_2}{2} = \\frac{(-0.5, 0.0) + (0.0, 0.0)}{2} = (-0.25, 0.0)$.\nThe WCSS is the sum of squared distances from the members to this centroid:\n$$\n\\text{WCSS}(C_{12}) = \\|S_1 - \\mu_{12}\\|^2 + \\|S_2 - \\mu_{12}\\|^2\n$$\n$$\n= \\|(-0.5, 0.0) - (-0.25, 0.0)\\|^2 + \\|(0.0, 0.0) - (-0.25, 0.0)\\|^2\n$$\n$$\n= \\|(-0.25, 0.0)\\|^2 + \\|(0.25, 0.0)\\|^2\n$$\n$$\n= ((-0.25)^2 + 0^2) + (0.25^2 + 0^2) = 0.0625 + 0.0625 = 0.125\n$$\nThis confirms the previous result. The total WCSS after one iteration is $0.125$.\n\nThe problem requires the answer to be rounded to four significant figures.\nThe number $0.125$ has three significant figures. To express it with four, we add a trailing zero: $0.1250$.", "answer": "$$\n\\boxed{0.1250}\n$$", "id": "4572309"}, {"introduction": "Beyond variance-based approaches like Ward's method, other linkage criteria reveal deep connections to different mathematical fields. This practice explores single linkage, where the distance between two clusters is defined by the closest pair of points between them. You will discover a fundamental and elegant equivalence between the sequence of merges in single linkage clustering and the construction of a Minimum Spanning Tree (MST), a cornerstone of graph theory [@problem_id:4572327]. This exercise demonstrates how clustering can be viewed through the lens of graph optimization.", "problem": "In a study of transcriptomic similarity among $6$ tumor biopsies, pairwise dissimilarities were computed as unitless values derived from correlation-based distances. Let $S=\\{s_{1},s_{2},s_{3},s_{4},s_{5},s_{6}\\}$ denote the biopsies. The dissimilarity matrix $D$ is symmetric, has zero diagonal, and is given by\n$$\nD=\\begin{pmatrix}\n0  1.0  2.8  6.0  6.9  10.5 \\\\\n1.0  0  1.8  5.0  5.9  9.5 \\\\\n2.8  1.8  0  3.2  4.1  7.7 \\\\\n6.0  5.0  3.2  0  0.9  4.5 \\\\\n6.9  5.9  4.1  0.9  0  3.6 \\\\\n10.5  9.5  7.7  4.5  3.6  0\n\\end{pmatrix}.\n$$\nModel the dataset as a complete weighted graph on $S$ with edge weights given by $D$. Starting only from the fundamental definitions of a Minimum Spanning Tree (MST) and hierarchical agglomerative clustering with single linkage, perform the following:\n\n1. Construct the MST on $S$.\n2. From first principles, extract the single linkage agglomerative merge sequence (that is, the ordered list of cluster merges) together with the corresponding merge heights, where the height of a merge between two clusters $A$ and $B$ is defined as $d(A,B)=\\min\\{D(i,j): i\\in A, j\\in B\\}$.\n3. Finally, compute the sum of the merge heights over the full agglomeration from $6$ singletons to one cluster. Express your final numeric answer as a unitless real number. No rounding is required; report the exact sum.\n\nYour reasoning and construction must be scientifically consistent with core definitions, and the MST and merge sequence must be derived without invoking any unproven shortcuts. The final answer must be a single real-valued number.", "solution": "We begin by formalizing the foundational concepts relevant to hierarchical clustering and graph optimization.\n\nLet the dataset be represented as a complete weighted graph $G=(V,E)$ with $V=S=\\{s_{1},\\dots,s_{6}\\}$ and weights $w(i,j)=D(i,j)$ for all unordered pairs $\\{i,j\\}$. A Minimum Spanning Tree (MST) is a tree subgraph $T$ of $G$ that spans all vertices in $V$ and minimizes the total weight $\\sum_{\\{i,j\\}\\in T}w(i,j)$ over all spanning trees. Two well-tested principles underpin MST construction:\n\n- The cut property: For any partition of $V$ into two nonempty sets $(A,B)$, the minimum-weight edge crossing the cut (from $A$ to $B$) belongs to some MST.\n- Kruskal’s algorithm: Sorting edges in nondecreasing order of weight and adding the next lowest-weight edge that does not form a cycle yields an MST.\n\nHierarchical agglomerative clustering with single linkage defines the dissimilarity between two clusters $A$ and $B$ as\n$$\nd(A,B)=\\min\\{D(i,j): i\\in A, j\\in B\\}.\n$$\nAt each agglomeration step, single linkage merges the pair of clusters $(A,B)$ with the smallest $d(A,B)$ and records the merge height as that minimum distance. A fundamental equivalence connects single linkage merges to MST edges: If we view each current cluster as a connected component, then the minimum inter-cluster dissimilarity corresponds to the minimum-weight edge crossing the cut between two components. By the cut property, such an edge is valid for inclusion in an MST. Proceeding from $n$ singletons, the sequence of inter-component edges selected by Kruskal’s algorithm both constructs the MST and determines the single linkage merge heights in nondecreasing order of weights. Therefore, extracting the MST suffices to obtain the single linkage merge sequence with heights, and the sum of merge heights equals the total weight of the MST.\n\nWe now apply this to the given matrix $D$. List the distinct edge weights in nondecreasing order, together with their endpoints (we write $(i,j)$ for the edge between $s_{i}$ and $s_{j}$):\n- $0.9$ for $(4,5)$.\n- $1.0$ for $(1,2)$.\n- $1.8$ for $(2,3)$.\n- $2.8$ for $(1,3)$.\n- $3.2$ for $(3,4)$.\n- $3.6$ for $(5,6)$.\n- $4.1$ for $(3,5)$.\n- $4.5$ for $(4,6)$.\n- $5.0$ for $(2,4)$.\n- $5.9$ for $(2,5)$.\n- $6.0$ for $(1,4)$.\n- $6.9$ for $(1,5)$.\n- $7.7$ for $(3,6)$.\n- $9.5$ for $(2,6)$.\n- $10.5$ for $(1,6)$.\n\nImplement Kruskal’s algorithm step by step:\n\n- Start with $6$ components: $\\{1\\},\\{2\\},\\{3\\},\\{4\\},\\{5\\},\\{6\\}$.\n- Add $(4,5)$ at weight $0.9$; components become $\\{4,5\\}$ and the remaining singletons $\\{1\\},\\{2\\},\\{3\\},\\{6\\}$.\n- Add $(1,2)$ at weight $1.0$; components become $\\{1,2\\}$, $\\{3\\}$, $\\{4,5\\}$, $\\{6\\}$.\n- Add $(2,3)$ at weight $1.8$; components become $\\{1,2,3\\}$, $\\{4,5\\}$, $\\{6\\}$.\n- Consider $(1,3)$ at weight $2.8$; this edge lies within $\\{1,2,3\\}$ and would form a cycle, so skip.\n- Add $(3,4)$ at weight $3.2$; this connects $\\{1,2,3\\}$ with $\\{4,5\\}$, yielding $\\{1,2,3,4,5\\}$ and a singleton $\\{6\\}$.\n- Add $(5,6)$ at weight $3.6$; this connects $\\{1,2,3,4,5\\}$ with $\\{6\\}$, yielding a single component $\\{1,2,3,4,5,6\\}$.\n\nStop upon having added $5$ edges, which is $n-1$ for $n=6$. The MST edges are therefore\n$$\n\\{(4,5),(1,2),(2,3),(3,4),(5,6)\\}\n$$\nwith weights\n$$\n0.9,\\quad 1.0,\\quad 1.8,\\quad 3.2,\\quad 3.6.\n$$\n\nBy the equivalence argued above, the single linkage agglomerative merge sequence and heights are:\n\n- Merge $\\{4\\}$ and $\\{5\\}$ at height $0.9$.\n- Merge $\\{1\\}$ and $\\{2\\}$ at height $1.0$.\n- Merge $\\{1,2\\}$ and $\\{3\\}$ at height $1.8$.\n- Merge $\\{1,2,3\\}$ and $\\{4,5\\}$ at height $3.2$.\n- Merge $\\{1,2,3,4,5\\}$ and $\\{6\\}$ at height $3.6$.\n\nFinally, compute the sum of the merge heights. Let the heights be $h_{1}=0.9$, $h_{2}=1.0$, $h_{3}=1.8$, $h_{4}=3.2$, $h_{5}=3.6$. Then\n$$\n\\sum_{k=1}^{5} h_{k} = 0.9 + 1.0 + 1.8 + 3.2 + 3.6 = 10.5.\n$$\nThis sum equals the total weight of the MST, consistent with the theoretical connection between single linkage merging and MST edge addition under the cut property.\n\nThus, the required unitless real number is $10.5$.", "answer": "$$\\boxed{10.5}$$", "id": "4572327"}, {"introduction": "Choosing an appropriate clustering algorithm is not just a technical choice but a scientific one, deeply tied to the assumptions about the data's structure. In fields like phylogenetics, distance matrices may exhibit properties like additivity without being ultrametric, a distinction that has major implications for method selection. This practice challenges you to apply two different methods, UPGMA and Neighbor-Joining, to such a dataset, allowing you to directly quantify the reconstruction error, or bias, that arises when a method's assumptions (like UPGMA's assumption of ultrametricity) are violated [@problem_id:4572299]. This is a crucial exercise in critically evaluating the suitability of a model for a given problem.", "problem": "A bioinformatics team is comparing hierarchical clustering strategies on a small genomic distance dataset from four tumor isolates labeled $A$, $B$, $C$, and $D$. Distances were estimated under a standard reversible substitution model from independent loci and are additive to an unrooted tree with topology $(A,B)|(C,D)$ but are not ultrametric due to heterogeneous lineage-specific rates. The pairwise genetic distances are:\n- $d_{AB} = 0.10$, $d_{CD} = 0.09$,\n- $d_{AC} = 0.11$, $d_{AD} = 0.04$,\n- $d_{BC} = 0.19$, $d_{BD} = 0.12$.\n\nDefinitions to use:\n- A distance matrix is ultrametric if, for every triple $(i,j,k)$, the two largest of $d_{ij}$, $d_{ik}$, and $d_{jk}$ are equal.\n- Unweighted Pair Group Method with Arithmetic mean (UPGMA) is an agglomerative hierarchical clustering method that assumes ultrametricity and constructs a rooted ultrametric tree by repeatedly merging the two closest clusters and assigning the new node height to be half the average intercluster distance. The implied ultrametric distance between any two leaves equals twice the height of their least common ancestor.\n- Neighbor-joining (NJ) is a distance-based method that, under additivity, reconstructs the exact unrooted tree and pairwise distances.\n\nTask:\n1. Apply UPGMA to the given distance matrix to construct the hierarchical clustering and compute the implied ultrametric pairwise distances $\\hat{d}^{\\mathrm{UPGMA}}_{ij}$ for all unordered pairs $(i,j)$.\n2. Using the additivity of the given matrix, consider the neighbor-joining reconstruction and the implied pairwise distances $\\hat{d}^{\\mathrm{NJ}}_{ij}$.\n3. Define the bias magnitude against non-ultrametric distances as the difference in total squared residuals\n$$\n\\Delta \\equiv \\sum_{ij}\\left(d_{ij}-\\hat{d}^{\\mathrm{UPGMA}}_{ij}\\right)^{2}-\\sum_{ij}\\left(d_{ij}-\\hat{d}^{\\mathrm{NJ}}_{ij}\\right)^{2}.\n$$\nCompute $\\Delta$ exactly. Give your final answer as a reduced fraction. Do not round.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of phylogenetic distance-based methods, well-posed with sufficient and consistent data, and objectively formulated. The additivity condition for the given distance matrix, which is central to the problem, can be verified for the specified topology of $(A,B)|(C,D)$. The condition states that for any four taxa, two of the sums of distances between pairs of taxa must be equal. For the given topology, the equality should be $d_{AC} + d_{BD} = d_{AD} + d_{BC}$. Using the provided values: $d_{AC} + d_{BD} = 0.11 + 0.12 = 0.23$ and $d_{AD} + d_{BC} = 0.04 + 0.19 = 0.23$. The condition holds, confirming the distances are additive for this tree. The problem is also self-contained and provides all necessary definitions.\n\nThe task is to compute the bias magnitude $\\Delta$, defined as:\n$$\n\\Delta \\equiv \\sum_{ij}\\left(d_{ij}-\\hat{d}^{\\mathrm{UPGMA}}_{ij}\\right)^{2}-\\sum_{ij}\\left(d_{ij}-\\hat{d}^{\\mathrm{NJ}}_{ij}\\right)^{2}\n$$\n\nFirst, we address the term involving Neighbor-Joining (NJ). The problem states that NJ, when applied to an additive distance matrix, reconstructs the exact unrooted tree and pairwise distances. Since the provided distance matrix is additive, the distances implied by the NJ reconstruction, $\\hat{d}^{\\mathrm{NJ}}_{ij}$, are identical to the original distances, $d_{ij}$.\n$$\n\\hat{d}^{\\mathrm{NJ}}_{ij} = d_{ij} \\quad \\text{for all pairs } (i,j)\n$$\nTherefore, the second summation term in the expression for $\\Delta$ is zero:\n$$\n\\sum_{ij}\\left(d_{ij}-\\hat{d}^{\\mathrm{NJ}}_{ij}\\right)^{2} = \\sum_{ij}\\left(d_{ij}-d_{ij}\\right)^{2} = \\sum_{ij}(0)^{2} = 0\n$$\nThe expression for $\\Delta$ simplifies to the sum of squared errors between the original distances and the distances implied by the UPGMA clustering:\n$$\n\\Delta = \\sum_{ij}\\left(d_{ij}-\\hat{d}^{\\mathrm{UPGMA}}_{ij}\\right)^{2}\n$$\nTo compute this, we must first perform UPGMA clustering. For precision, all decimal values are converted to fractions.\nThe initial pairwise distances are:\n$d_{AB} = 0.10 = \\frac{1}{10}$, $d_{CD} = 0.09 = \\frac{9}{100}$,\n$d_{AC} = 0.11 = \\frac{11}{100}$, $d_{AD} = 0.04 = \\frac{4}{100} = \\frac{1}{25}$,\n$d_{BC} = 0.19 = \\frac{19}{100}$, $d_{BD} = 0.12 = \\frac{12}{100} = \\frac{3}{25}$.\n\nThe UPGMA algorithm proceeds as follows:\n\n**Step 1:** Identify the smallest distance in the matrix. This is $d_{AD} = \\frac{1}{25} = 0.04$.\nClusters $A$ and $D$ are merged into a new cluster, which we denote as $(AD)$. The height of the node connecting $A$ and $D$ is $h_{(AD)} = \\frac{d_{AD}}{2} = \\frac{1/25}{2} = \\frac{1}{50}$.\nWe compute the distances from this new cluster to the remaining clusters, $B$ and $C$, using the unweighted average linkage formula:\n$$d_{(AD),B} = \\frac{d_{AB} + d_{DB}}{2} = \\frac{\\frac{1}{10} + \\frac{3}{25}}{2} = \\frac{\\frac{5}{50} + \\frac{6}{50}}{2} = \\frac{11/50}{2} = \\frac{11}{100}$$\n$$d_{(AD),C} = \\frac{d_{AC} + d_{DC}}{2} = \\frac{\\frac{11}{100} + \\frac{9}{100}}{2} = \\frac{20/100}{2} = \\frac{1/5}{2} = \\frac{1}{10}$$\nThe matrix of distances between clusters is now:\n$d_{(AD),B} = \\frac{11}{100}$, $d_{(AD),C} = \\frac{10}{100}$, $d_{B,C} = \\frac{19}{100}$.\n\n**Step 2:** Identify the smallest distance in the new matrix. This is $d_{(AD),C} = \\frac{1}{10} = \\frac{10}{100}$.\nClusters $(AD)$ and $C$ are merged into a new cluster, $((AD)C)$. The height of the node connecting $(AD)$ and $C$ is $h_{((AD)C)} = \\frac{d_{(AD),C}}{2} = \\frac{1/10}{2} = \\frac{1}{20}$.\nWe compute the distance from this new cluster to the only remaining cluster, $B$. We use the UPGMA formula, weighting by cluster size:\n$$d_{((AD)C),B} = \\frac{|(AD)|d_{(AD),B} + |C|d_{C,B}}{|(AD)|+|C|} = \\frac{2 \\cdot \\frac{11}{100} + 1 \\cdot \\frac{19}{100}}{2+1} = \\frac{\\frac{22}{100} + \\frac{19}{100}}{3} = \\frac{41/100}{3} = \\frac{41}{300}$$\n\n**Step 3:** The final merge is between clusters $((AD)C)$ and $B$. The height of the root node is $h_{\\text{root}} = \\frac{d_{((AD)C),B}}{2} = \\frac{41/300}{2} = \\frac{41}{600}$.\n\nThe UPGMA procedure yields a rooted tree with topology $((A,D),C),B$. The implied ultrametric distances, $\\hat{d}^{\\mathrm{UPGMA}}_{ij}$, are twice the height of the least common ancestor (LCA) of each pair $(i,j)$:\n- $\\text{LCA}(A,D)$ is the node at height $h_{(AD)}=\\frac{1}{50}$. Thus, $\\hat{d}^{\\mathrm{UPGMA}}_{AD} = 2 \\cdot \\frac{1}{50} = \\frac{1}{25}$.\n- $\\text{LCA}(A,C) = \\text{LCA}(D,C)$ is the node at height $h_{((AD)C)}=\\frac{1}{20}$. Thus, $\\hat{d}^{\\mathrm{UPGMA}}_{AC} = \\hat{d}^{\\mathrm{UPGMA}}_{DC} = 2 \\cdot \\frac{1}{20} = \\frac{1}{10}$.\n- $\\text{LCA}(A,B) = \\text{LCA}(D,B) = \\text{LCA}(C,B)$ is the root node at height $h_{\\text{root}}=\\frac{41}{600}$. Thus, $\\hat{d}^{\\mathrm{UPGMA}}_{AB} = \\hat{d}^{\\mathrm{UPGMA}}_{DB} = \\hat{d}^{\\mathrm{UPGMA}}_{CB} = 2 \\cdot \\frac{41}{600} = \\frac{41}{300}$.\n\nNow we compute the squared differences for each pair $(i,j)$:\n- $(A,B)$: $\\left(d_{AB} - \\hat{d}^{\\mathrm{UPGMA}}_{AB}\\right)^2 = \\left(\\frac{1}{10} - \\frac{41}{300}\\right)^2 = \\left(\\frac{30}{300} - \\frac{41}{300}\\right)^2 = \\left(-\\frac{11}{300}\\right)^2 = \\frac{121}{90000}$.\n- $(A,C)$: $\\left(d_{AC} - \\hat{d}^{\\mathrm{UPGMA}}_{AC}\\right)^2 = \\left(\\frac{11}{100} - \\frac{1}{10}\\right)^2 = \\left(\\frac{11}{100} - \\frac{10}{100}\\right)^2 = \\left(\\frac{1}{100}\\right)^2 = \\frac{1}{10000}$.\n- $(A,D)$: $\\left(d_{AD} - \\hat{d}^{\\mathrm{UPGMA}}_{AD}\\right)^2 = \\left(\\frac{1}{25} - \\frac{1}{25}\\right)^2 = 0$.\n- $(B,C)$: $\\left(d_{BC} - \\hat{d}^{\\mathrm{UPGMA}}_{BC}\\right)^2 = \\left(\\frac{19}{100} - \\frac{41}{300}\\right)^2 = \\left(\\frac{57}{300} - \\frac{41}{300}\\right)^2 = \\left(\\frac{16}{300}\\right)^2 = \\frac{256}{90000}$.\n- $(B,D)$: $\\left(d_{BD} - \\hat{d}^{\\mathrm{UPGMA}}_{BD}\\right)^2 = \\left(\\frac{3}{25} - \\frac{41}{300}\\right)^2 = \\left(\\frac{36}{300} - \\frac{41}{300}\\right)^2 = \\left(-\\frac{5}{300}\\right)^2 = \\left(-\\frac{1}{60}\\right)^2 = \\frac{1}{3600}$.\n- $(C,D)$: $\\left(d_{CD} - \\hat{d}^{\\mathrm{UPGMA}}_{CD}\\right)^2 = \\left(\\frac{9}{100} - \\frac{1}{10}\\right)^2 = \\left(\\frac{9}{100} - \\frac{10}{100}\\right)^2 = \\left(-\\frac{1}{100}\\right)^2 = \\frac{1}{10000}$.\n\nFinally, we sum these values to find $\\Delta$. Using a common denominator of $90000$:\n$\\frac{1}{10000} = \\frac{9}{90000}$\n$\\frac{1}{3600} = \\frac{25}{90000}$\n$$\n\\Delta = \\frac{121}{90000} + \\frac{9}{90000} + 0 + \\frac{256}{90000} + \\frac{25}{90000} + \\frac{9}{90000}\n$$\n$$\n\\Delta = \\frac{121 + 9 + 0 + 256 + 25 + 9}{90000} = \\frac{420}{90000}\n$$\nThis fraction can be reduced by dividing the numerator and denominator by their greatest common divisor.\n$$\n\\Delta = \\frac{420}{90000} = \\frac{42}{9000} = \\frac{7 \\cdot 6}{1500 \\cdot 6} = \\frac{7}{1500}\n$$\nThe bias magnitude $\\Delta$ is $\\frac{7}{1500}$.", "answer": "$$\\boxed{\\frac{7}{1500}}$$", "id": "4572299"}]}