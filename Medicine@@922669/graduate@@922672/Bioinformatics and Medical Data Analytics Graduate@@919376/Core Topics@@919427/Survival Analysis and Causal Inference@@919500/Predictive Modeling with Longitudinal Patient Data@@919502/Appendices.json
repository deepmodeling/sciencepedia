{"hands_on_practices": [{"introduction": "Understanding time-to-event data is a cornerstone of clinical predictive modeling. This first exercise guides you through the foundational Kaplan-Meier method, a non-parametric approach to estimating survival functions from data with right-censoring. By implementing the estimator from first principles, you will gain a deep, practical understanding of how risk sets, event times, and censoring are handled to produce the ubiquitous step-function survival curves seen in clinical literature [@problem_id:4597912].", "problem": "You are given three small longitudinal patient datasets consisting of follow-up times and event indicators under independent right-censoring, where an event indicates a clinical endpoint such as death or disease progression. Let $T$ denote the time-to-event random variable with survival function $S(t) = \\mathbb{P}(T > t)$, and let right-censoring indicate that for some patients the exact event time is not observed but is known to exceed a last follow-up time. Patients are assumed to be Independent and Identically Distributed (IID), and censoring is assumed independent of the event process. The task is to construct the nonparametric maximum likelihood estimator of $S(t)$ based on the observed event times and censoring times, explain the behavior of the estimator as a right-continuous step function that changes only at observed event times, and derive a variance estimator for $\\hat{S}(t)$ based on large-sample principles.\n\nStart from fundamental definitions:\n- The survival function $S(t) = \\mathbb{P}(T > t)$.\n- Right-censoring is independent of the event process, and observations contribute to risk sets up to their observed time.\n- Likelihood contributions for discrete event times are governed by risk sets and event counts at those times.\n\nUsing these bases, construct the nonparametric product-limit estimator for the survival function under independent right-censoring, and articulate the magnitude of the step change at each observed event time in terms of the risk set just prior to that time. Then, using asymptotic theory for sums of independent contributions at event times and the delta method, derive a variance estimator for $\\hat{S}(t)$ that aggregates contributions from event times up to $t$.\n\nYour program must compute, for each dataset and a specified evaluation time $t^\\star$ (in days):\n1. The estimated survival $\\hat{S}(t^\\star)$.\n2. The variance estimate $\\widehat{\\mathrm{Var}}(\\hat{S}(t^\\star))$ derived from large-sample theory of the estimator.\n3. The total magnitude of all survival drops up to $t^\\star$, defined as $1 - \\hat{S}(t^\\star)$.\n4. The largest single step magnitude up to $t^\\star$, defined as the largest drop in the survival function at an event time not exceeding $t^\\star$.\n\nAll survival probabilities and variances are dimensionless quantities. Time values are expressed in days, and no angle units are involved. Outputs must be numeric (floats).\n\nTest suite:\n- Case A (general mixed case with censoring and ties):\n  - Times (days): $[3, 5, 7, 7, 10, 12]$\n  - Event indicators (1 = event observed, 0 = censored): $[1, 0, 1, 1, 0, 1]$\n  - Evaluation time $t^\\star = 9$ days\n- Case B (boundary case: all censored):\n  - Times (days): $[2, 4, 6, 8]$\n  - Event indicators: $[0, 0, 0, 0]$\n  - Evaluation time $t^\\star = 5$ days\n- Case C (edge case: tied events at the same time):\n  - Times (days): $[1, 4, 4, 4, 5, 9]$\n  - Event indicators: $[1, 1, 1, 0, 0, 1]$\n  - Evaluation time $t^\\star = 4$ days\n\nAlgorithmic requirements:\n- Sort unique event times in ascending order.\n- For each distinct event time $t_j$ not exceeding $t^\\star$, compute the risk set size $n_j$ as the number of individuals with observed time at least $t_j$, and the event count $d_j$ as the number of observed events exactly at $t_j$.\n- Construct the estimator of $S(t^\\star)$ as the product over event times not exceeding $t^\\star$ of factors that reflect the event probabilities conditional on the risk sets.\n- Compute the step magnitude at $t_j$ as the drop in the survival function at $t_j$, and aggregate these magnitudes to obtain the total decrease up to $t^\\star$ and the largest single step.\n- Derive and implement the variance estimator for $\\hat{S}(t^\\star)$ using the aggregation of contributions at each event time up to $t^\\star$ obtained from the risk sets and event counts. If $\\hat{S}(t^\\star)$ equals zero due to complete depletion of the risk set by events, set the variance estimate to zero.\n\nFinal output format:\nYour program should produce a single line of output containing a list of per-test-case result lists. Each per-test-case result list must be ordered as $[\\hat{S}(t^\\star), \\widehat{\\mathrm{Var}}(\\hat{S}(t^\\star)), 1 - \\hat{S}(t^\\star), \\max\\text{ step magnitude up to }t^\\star]$, all as floats. The entire output must be a single comma-separated list enclosed in square brackets; for example, a valid format is $[[r_{1,1}, r_{1,2}, r_{1,3}, r_{1,4}], [r_{2,1}, r_{2,2}, r_{2,3}, r_{2,4}], [r_{3,1}, r_{3,2}, r_{3,3}, r_{3,4}]]$.", "solution": "The problem requires the construction and application of the nonparametric maximum likelihood estimator (NPMLE) for the survival function, $S(t)$, from right-censored data. This estimator is widely known as the Kaplan-Meier product-limit estimator. We will first derive the estimator, then derive its variance using large-sample theory, and finally apply these results to the provided test cases.\n\nLet the observed data for $n$ independent individuals be $\\{(t_i, \\delta_i)\\}_{i=1}^n$, where $t_i$ is the observed follow-up time (either time of event or time of censoring) and $\\delta_i$ is the event indicator, with $\\delta_i=1$ if the event occurred at $t_i$ and $\\delta_i=0$ if the observation was censored at $t_i$. We assume that the censoring mechanism is independent of the event process.\n\nThe survival function is defined as $S(t) = \\mathbb{P}(T > t)$, where $T$ is the true time-to-event. For a continuous-time model, the likelihood contribution of an individual with an observed event at $t_i$ is the probability density function $f(t_i)$, while for an individual censored at $t_i$, the contribution is the probability of survival beyond $t_i$, $S(t_i)$. The total likelihood is $L = \\prod_{i=1}^n [f(t_i)]^{\\delta_i} [S(t_i)]^{1-\\delta_i}$.\n\nTo maximize this likelihood nonparametrically, we make no assumptions about the functional form of $S(t)$. It can be shown that the NPMLE must be a step function that only decreases at the observed event times. Let the distinct, ordered event times in the sample be $\\tau_1  \\tau_2  \\dots  \\tau_k$. The survival function can be expressed as a product of conditional probabilities of surviving past each event time, given survival up to that time:\n$$\nS(t) = \\prod_{j: \\tau_j \\le t} \\mathbb{P}(T > \\tau_j | T \\ge \\tau_j)\n$$\nwhere the conditioning event is sometimes written as $T \\ge \\tau_j$ or $T > \\tau_j^-$.\n\nAt each event time $\\tau_j$, we define the risk set, $R_j$, as the set of individuals who are still under observation just before $\\tau_j$. The size of this set, $n_j$, is the number of individuals with observed times $t_i \\ge \\tau_j$. Let $d_j$ be the number of individuals who experience the event at time $\\tau_j$.\n\nThe probability of experiencing an event at $\\tau_j$ given being at risk is the hazard, $h_j$. The $d_j$ events and $n_j - d_j$ survivors (or censored) at $\\tau_j$ can be modeled with a binomial likelihood term proportional to $h_j^{d_j} (1 - h_j)^{n_j - d_j}$. The maximum likelihood estimate for the hazard is $\\hat{h}_j = d_j / n_j$.\nThe conditional probability of surviving past $\\tau_j$ given being at risk is $1 - h_j$. Its estimate is $1 - \\hat{h}_j = 1 - d_j/n_j = (n_j - d_j)/n_j$.\n\nSubstituting this into the product form gives the Kaplan-Meier estimator for the survival function:\n$$\n\\hat{S}(t) = \\prod_{j: \\tau_j \\le t} \\left( \\frac{n_j - d_j}{n_j} \\right)\n$$\nBy convention, $\\hat{S}(t)=1$ for $t  \\tau_1$. $\\hat{S}(t)$ is a right-continuous step function. It is constant between event times and drops only at the observed event times $\\tau_j$. The magnitude of the step (drop) at $\\tau_j$ is given by:\n$$\n\\text{Step}_j = \\hat{S}(\\tau_j^-) - \\hat{S}(\\tau_j) = \\hat{S}(\\tau_j^-) - \\hat{S}(\\tau_j^-) \\left( \\frac{n_j - d_j}{n_j} \\right) = \\hat{S}(\\tau_j^-) \\frac{d_j}{n_j}\n$$\nwhere $\\hat{S}(\\tau_j^-) = \\prod_{l=1}^{j-1} (n_l - d_l)/n_l$.\n\nTo estimate the variance of $\\hat{S}(t)$, we use large-sample theory and the delta method. It is more convenient to first find the variance of $\\log \\hat{S}(t)$:\n$$\n\\log \\hat{S}(t) = \\sum_{j: \\tau_j \\le t} \\log \\left(1 - \\frac{d_j}{n_j} \\right)\n$$\nAssuming the number of events $d_j$ at each $\\tau_j$ follows a binomial distribution $d_j \\sim \\text{Bin}(n_j, h_j)$, the variance of the estimated hazard $\\hat{h}_j = d_j/n_j$ is $\\text{Var}(\\hat{h}_j) = \\frac{h_j(1-h_j)}{n_j}$. Applying the delta method to the function $g(x) = \\log(1-x)$, where $g'(x) = -1/(1-x)$, we get:\n$$\n\\text{Var}(\\log(1-\\hat{h}_j)) \\approx [g'(h_j)]^2 \\text{Var}(\\hat{h}_j) = \\left(\\frac{-1}{1-h_j}\\right)^2 \\frac{h_j(1-h_j)}{n_j} = \\frac{h_j}{n_j(1-h_j)}\n$$\nSubstituting the estimates $\\hat{h}_j = d_j/n_j$ and assuming approximate independence of the terms in the sum, the variance of the log-survival is estimated as:\n$$\n\\widehat{\\text{Var}}(\\log \\hat{S}(t)) \\approx \\sum_{j: \\tau_j \\le t} \\frac{d_j/n_j}{n_j(1-d_j/n_j)} = \\sum_{j: \\tau_j \\le t} \\frac{d_j}{n_j(n_j - d_j)}\n$$\nTo find the variance of $\\hat{S}(t)$, we apply the delta method again, this time with the function $g(x) = e^x$. Since $\\hat{S}(t) = \\exp(\\log \\hat{S}(t))$ and $g'(x) = e^x$, we have:\n$$\n\\text{Var}(\\hat{S}(t)) \\approx [g'(\\mathbb{E}[\\log \\hat{S}(t)])]^2 \\text{Var}(\\log \\hat{S}(t)) \\approx [S(t)]^2 \\text{Var}(\\log \\hat{S}(t))\n$$\nPlugging in the estimates, we obtain Greenwood's formula for the variance of the Kaplan-Meier estimator:\n$$\n\\widehat{\\text{Var}}(\\hat{S}(t)) = [\\hat{S}(t)]^2 \\sum_{j: \\tau_j \\le t} \\frac{d_j}{n_j(n_j - d_j)}\n$$\nIf $\\hat{S}(t)=0$ because an event occurred when $d_j=n_j$, the variance from that point on is taken to be $0$, as stated in the problem.\n\nWe now apply this framework to the given test cases.\n\n**Case A**: Times: $[3, 5, 7, 7, 10, 12]$, Events: $[1, 0, 1, 1, 0, 1]$, $t^\\star = 9$.\nThe distinct event times are $\\tau_1=3, \\tau_2=7, \\tau_3=12$. We consider event times $\\le 9$.\n- At $\\tau_1 = 3$: Risk set size $n_1=6$. Event count $d_1=1$. $\\hat{S}(3) = \\frac{6-1}{6} = \\frac{5}{6}$.\n- At $\\tau_2 = 7$: Risk set size $n_2=4$ (individuals at times $[7, 7, 10, 12]$). Event count $d_2=2$.\nThe survival estimate after this time is $\\hat{S}(7) = \\hat{S}(3) \\times \\frac{4-2}{4} = \\frac{5}{6} \\times \\frac{2}{4} = \\frac{5}{12}$.\nFor $t^\\star = 9$, $\\hat{S}(9) = \\hat{S}(7) = \\frac{5}{12}$.\n\n1. $\\hat{S}(t^\\star)$: $\\frac{5}{12} \\approx 0.416667$.\n2. $\\widehat{\\text{Var}}(\\hat{S}(t^\\star))$: Sum term is $\\frac{d_1}{n_1(n_1-d_1)} + \\frac{d_2}{n_2(n_2-d_2)} = \\frac{1}{6(5)} + \\frac{2}{4(2)} = \\frac{1}{30} + \\frac{1}{4} = \\frac{2+15}{60} = \\frac{17}{60}$.\n $\\widehat{\\text{Var}}(\\hat{S}(9)) = (\\frac{5}{12})^2 \\times \\frac{17}{60} = \\frac{25}{144} \\times \\frac{17}{60} = \\frac{5 \\times 17}{144 \\times 12} = \\frac{85}{1728} \\approx 0.049190$.\n3. Total drop $1 - \\hat{S}(t^\\star)$: $1 - \\frac{5}{12} = \\frac{7}{12} \\approx 0.583333$.\n4. Max step magnitude: Step at $3$ is $1 - \\frac{5}{6} = \\frac{1}{6}$. Step at $7$ is $\\hat{S}(3) - \\hat{S}(7) = \\frac{5}{6} - \\frac{5}{12} = \\frac{5}{12}$. The max step is $\\frac{5}{12} \\approx 0.416667$.\n\n**Case B**: Times: $[2, 4, 6, 8]$, Events: $[0, 0, 0, 0]$, $t^\\star = 5$.\nThere are no events. The Kaplan-Meier curve remains at $1$.\n1. $\\hat{S}(t^\\star)$: $1.0$.\n2. $\\widehat{\\text{Var}}(\\hat{S}(t^\\star))$: The sum in Greenwood's formula is empty, hence $0$. Variance is $1^2 \\times 0 = 0.0$.\n3. Total drop $1 - \\hat{S}(t^\\star)$: $1 - 1 = 0.0$.\n4. Max step magnitude: There are no steps, so the max is $0.0$.\n\n**Case C**: Times: $[1, 4, 4, 4, 5, 9]$, Events: $[1, 1, 1, 0, 0, 1]$, $t^\\star = 4$.\nThe distinct event times are $\\tau_1=1, \\tau_2=4, \\tau_3=9$. We consider event times $\\le 4$.\n- At $\\tau_1 = 1$: Risk set size $n_1=6$. Event count $d_1=1$. $\\hat{S}(1) = \\frac{6-1}{6} = \\frac{5}{6}$.\n- At $\\tau_2 = 4$: Risk set size $n_2=5$ (individuals at times $[4, 4, 4, 5, 9]$). Event count $d_2=2$.\nThe survival estimate at $t^\\star=4$ is right-continuous, so we compute the value after the drop: $\\hat{S}(4) = \\hat{S}(1) \\times \\frac{5-2}{5} = \\frac{5}{6} \\times \\frac{3}{5} = \\frac{1}{2}$.\n\n1. $\\hat{S}(t^\\star)$: $\\frac{1}{2} = 0.5$.\n2. $\\widehat{\\text{Var}}(\\hat{S}(t^\\star))$: Sum term is $\\frac{d_1}{n_1(n_1-d_1)} + \\frac{d_2}{n_2(n_2-d_2)} = \\frac{1}{6(5)} + \\frac{2}{5(3)} = \\frac{1}{30} + \\frac{2}{15} = \\frac{1+4}{30} = \\frac{5}{30} = \\frac{1}{6}$.\n $\\widehat{\\text{Var}}(\\hat{S}(4)) = (\\frac{1}{2})^2 \\times \\frac{1}{6} = \\frac{1}{4} \\times \\frac{1}{6} = \\frac{1}{24} \\approx 0.041667$.\n3. Total drop $1 - \\hat{S}(t^\\star)$: $1 - \\frac{1}{2} = \\frac{1}{2} = 0.5$.\n4. Max step magnitude: Step at $1$ is $1 - \\frac{5}{6} = \\frac{1}{6}$. Step at $4$ is $\\hat{S}(1) - \\hat{S}(4) = \\frac{5}{6} - \\frac{1}{2} = \\frac{2}{6} = \\frac{1}{3}$. The max step is $\\frac{1}{3} \\approx 0.333333$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef kaplan_meier_analysis(times, events, t_star):\n    \"\"\"\n    Computes Kaplan-Meier survival estimates and related quantities.\n\n    Args:\n        times (list or np.ndarray): Observed follow-up times.\n        events (list or np.ndarray): Event indicators (1=event, 0=censored).\n        t_star (float): The evaluation time.\n\n    Returns:\n        tuple: A tuple containing:\n            - s_hat (float): Estimated survival probability at t_star.\n            - variance (float): Estimated variance of s_hat using Greenwood's formula.\n            - total_drop (float): 1 - s_hat.\n            - max_step (float): The largest single drop in the survival curve up to t_star.\n    \"\"\"\n    times = np.array(times)\n    events = np.array(events)\n\n    # Find unique event times in the data, sorted in ascending order.\n    # An event is where events array is 1.\n    event_times_all = np.unique(times[events == 1])\n    \n    # Filter for event times up to the evaluation time t_star\n    event_times = event_times_all[event_times_all = t_star]\n\n    s_hat = 1.0\n    s_hat_previous = 1.0\n    var_sum = 0.0\n    max_step = 0.0\n\n    if len(event_times) == 0:\n        return 1.0, 0.0, 0.0, 0.0\n\n    for tau_j in event_times:\n        # n_j: number of individuals at risk just before time tau_j\n        # This is the count of subjects whose observed time is >= tau_j.\n        n_j = np.sum(times >= tau_j)\n\n        # d_j: number of events at time tau_j\n        d_j = np.sum((times == tau_j)  (events == 1))\n\n        if n_j == 0:\n            # No one at risk, survival cannot be estimated further.\n            # This case shouldn't be reached if tau_j is a valid event time.\n            break\n\n        # Calculate hazard and update survival estimate\n        survival_factor = (n_j - d_j) / n_j\n        s_hat *= survival_factor\n\n        # Calculate the drop in survival at this step\n        step = s_hat_previous - s_hat\n        if step > max_step:\n            max_step = step\n        \n        # Update s_hat_previous for the next iteration's step calculation\n        s_hat_previous = s_hat\n        \n        # Update sum for Greenwood's formula\n        # If n_j == d_j, the risk set is depleted. s_hat becomes 0.\n        # The variance is 0 according to the problem rule.\n        # The term d_j / (n_j * (n_j - d_j)) would be a division by zero.\n        # We can break as s_hat will be 0 and thus variance will be 0.\n        if n_j - d_j == 0:\n            var_sum = 0 # Future terms are irrelevant. S_hat is 0.\n            break\n        else:\n            var_sum += d_j / (n_j * (n_j - d_j))\n\n    # Greenwood's formula for variance\n    variance = (s_hat ** 2) * var_sum\n    \n    # Total magnitude of survival drops\n    total_drop = 1.0 - s_hat\n    \n    return s_hat, variance, total_drop, max_step\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case A (general mixed case with censoring and ties)\n        {'times': [3, 5, 7, 7, 10, 12], 'events': [1, 0, 1, 1, 0, 1], 't_star': 9},\n        # Case B (boundary case: all censored)\n        {'times': [2, 4, 6, 8], 'events': [0, 0, 0, 0], 't_star': 5},\n        # Case C (edge case: tied events at the same time)\n        {'times': [1, 4, 4, 4, 5, 9], 'events': [1, 1, 1, 0, 0, 1], 't_star': 4}\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = kaplan_meier_analysis(case['times'], case['events'], case['t_star'])\n        # Format each individual result as a list of floats\n        all_results.append(list(result))\n\n    # Format the final output string exactly as required\n    # e.g., [[r1, r2, r3, r4], [r5, r6, r7, r8], ...]\n    result_str = \"[\" + \", \".join([f\"[{', '.join(map(str, res))}]\" for res in all_results]) + \"]\"\n\n    print(result_str)\n\nsolve()\n```", "id": "4597912"}, {"introduction": "Beyond population-level survival, predictive medicine aims to model the unique trajectory of each patient. This practice transitions to modeling the longitudinal biomarker data itself using a Bayesian framework to estimate patient-specific parameters. You will develop a model that captures individual trends and provides a principled way to quantify uncertainty in future predictions, a crucial skill for building personalized risk-assessment tools [@problem_id:4597855].", "problem": "You are given a longitudinal biomarker modeling task rooted in conjugate Bayesian linear regression for patient-specific trajectories. For each patient index $i$, observed data consist of time points $\\{t_{ij}\\}_{j=1}^{n_i}$ measured in days and corresponding biomarker values $\\{y_{ij}\\}_{j=1}^{n_i}$ measured in milligrams per deciliter (mg/dL). Assume the following generative model: for each patient $i$, there exists a patient-specific intercept $a_i$ and slope $b_i$ such that the observation model is\n$$\ny_{ij} = a_i + b_i\\, t_{ij} + \\varepsilon_{ij}, \\quad \\varepsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^2),\n$$\nwhere $\\sigma^2$ is known and shared across patients. Let the joint prior for the patient-specific parameters be Gaussian,\n$$\n\\begin{bmatrix} a_i \\\\ b_i \\end{bmatrix} \\sim \\mathcal{N}\\!\\left( \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} \\right),\n$$\nwith known prior mean vector $\\boldsymbol{\\mu} \\in \\mathbb{R}^2$ and covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{2 \\times 2}$ that are shared across patients.\n\nYour task is to use this model to compute, for each patient:\n- The posterior mean (Bayesian estimator under squared error loss) of the patient-specific slope $b_i$ in units of mg/dL per day.\n- For a given set of future time points $\\{t^\\ast_{ik}\\}_{k=1}^{m_i}$, the posterior predictive mean of the biomarker and a symmetric central credible interval at a specified coverage level $\\alpha$ for each $t^\\ast_{ik}$. The interval endpoints must be reported in mg/dL. The credible interval must be computed as the image of a symmetric central interval for a Gaussian distribution, using quantiles expressed as decimals (for example, $\\alpha = 0.95$ uses lower and upper quantiles at $0.025$ and $0.975$, respectively). No percentage signs may be used.\n\nFundamental base to use in your derivation and algorithm design:\n- The Gaussian likelihood definition for linear regression with homoscedastic noise.\n- The Normal prior conjugacy for linear models.\n- The rules for conditioning and marginalization in multivariate Gaussian distributions.\n- The fact that the predictive distribution under a Normal-Normal model is Gaussian with mean and variance obtained by integrating out the posterior over parameters.\n\nDesign your algorithm from these principles. Do not rely on any other unintroduced \"shortcut\" formulas.\n\nUse the following globally shared hyperparameters across all patients:\n- Prior mean $\\boldsymbol{\\mu} = \\begin{bmatrix} 95.0 \\\\ 0.05 \\end{bmatrix}$ (units: mg/dL for the intercept and mg/dL per day for the slope).\n- Prior covariance $\\boldsymbol{\\Sigma} = \\begin{bmatrix} 100.0  0.0 \\\\ 0.0  0.01 \\end{bmatrix}$.\n- Observation noise variance $\\sigma^2 = 4.0$ (units: $(\\text{mg/dL})^2$).\n\nTest suite. Apply your program to the following three patients, each with their own observed longitudinal data, requested future prediction times, and credible interval coverage level $\\alpha$:\n- Case $1$ (happy path):\n  - Observed times (days): $[\\,0,\\,30,\\,60,\\,90\\,]$\n  - Observed biomarkers (mg/dL): $[\\,100,\\,103,\\,107,\\,112\\,]$\n  - Future times (days): $[\\,120,\\,180\\,]$\n  - Coverage level $\\alpha = 0.95$\n- Case $2$ (boundary: single observation):\n  - Observed times (days): $[\\,0\\,]$\n  - Observed biomarkers (mg/dL): $[\\,80\\,]$\n  - Future times (days): $[\\,30,\\,60,\\,90\\,]$\n  - Coverage level $\\alpha = 0.95$\n- Case $3$ (noisy trajectory and different coverage):\n  - Observed times (days): $[\\,0,\\,10,\\,20,\\,30,\\,40\\,]$\n  - Observed biomarkers (mg/dL): $[\\,50,\\,49,\\,53,\\,47,\\,52\\,]$\n  - Future times (days): $[\\,50,\\,100\\,]$\n  - Coverage level $\\alpha = 0.90$\n\nFinal output format. Your program must produce a single line of output containing a comma-separated list enclosed in square brackets. For each case $i$ in the order given ($i=1,2,3$), append the following numbers in order:\n- The posterior mean of $b_i$ (mg/dL per day), rounded to six decimal places.\n- For each future time $t^\\ast_{ik}$ in the order specified, append the predictive mean (mg/dL), the lower endpoint (mg/dL), and the upper endpoint (mg/dL) of the symmetric central credible interval at coverage $\\alpha$, all rounded to six decimal places.\n\nThus, the final printed output is a single list of floats like $[\\,\\text{case 1 slope},\\,\\text{case 1 pred mean at first future time},\\,\\text{case 1 lower},\\,\\text{case 1 upper},\\,\\ldots,\\,\\text{case 3 upper}\\,]$, with no additional text. Angles are not used in this problem, so no angle units are required. All physical units are as specified above; report all numbers as dimensionless floats representing values in those units.", "solution": "The problem requires the application of Bayesian linear regression to model patient-specific biomarker trajectories. We are given a generative model for each patient $i$, and our goal is to compute the posterior distribution of the model parameters and subsequently the posterior predictive distribution for future biomarker values. The solution will be derived from first principles as stipulated.\n\nLet the patient-specific parameters be the intercept $a_i$ and slope $b_i$, collected in a vector $\\boldsymbol{\\theta}_i = [a_i, b_i]^T$. The observed data for patient $i$ consists of $n_i$ pairs of time points $\\{t_{ij}\\}_{j=1}^{n_i}$ and biomarker values $\\{y_{ij}\\}_{j=1}^{n_i}$.\n\nThe observation model is given by $y_{ij} = a_i + b_i t_{ij} + \\varepsilon_{ij}$, with noise $\\varepsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$. We can express this in matrix form for all observations of a patient. Let $\\mathbf{y}_i = [y_{i1}, \\dots, y_{in_i}]^T$ be the vector of biomarker observations and $\\mathbf{X}_i$ be the $n_i \\times 2$ design matrix:\n$$\n\\mathbf{X}_i = \\begin{bmatrix} 1  t_{i1} \\\\ 1  t_{i2} \\\\ \\vdots  \\vdots \\\\ 1  t_{in_i} \\end{bmatrix}\n$$\nThe model for the vector of observations $\\mathbf{y}_i$ is then $\\mathbf{y}_i = \\mathbf{X}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\varepsilon}_i$, where $\\boldsymbol{\\varepsilon}_i \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_{n_i})$. This defines the likelihood function for $\\boldsymbol{\\theta}_i$ given the data $\\mathcal{D}_i = \\{\\mathbf{y}_i, \\mathbf{X}_i\\}$:\n$$\np(\\mathbf{y}_i | \\boldsymbol{\\theta}_i, \\sigma^2) = \\mathcal{N}(\\mathbf{y}_i | \\mathbf{X}_i \\boldsymbol{\\theta}_i, \\sigma^2 \\mathbf{I}_{n_i}) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} (\\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\theta}_i)^T (\\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\theta}_i) \\right)\n$$\nThe prior distribution for the parameters is given as a multivariate Gaussian:\n$$\np(\\boldsymbol{\\theta}_i) = \\mathcal{N}(\\boldsymbol{\\theta}_i | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) \\propto \\exp\\left( -\\frac{1}{2} (\\boldsymbol{\\theta}_i - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\theta}_i - \\boldsymbol{\\mu}) \\right)\n$$\nwhere $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$ are the prior mean and covariance, respectively.\n\n**1. Posterior Distribution of Parameters**\n\nThe posterior distribution of the parameters $\\boldsymbol{\\theta}_i$ is obtained via Bayes' theorem: $p(\\boldsymbol{\\theta}_i | \\mathcal{D}_i) \\propto p(\\mathbf{y}_i | \\boldsymbol{\\theta}_i) p(\\boldsymbol{\\theta}_i)$. Since the likelihood and prior are both Gaussian, the posterior is also Gaussian, a property known as conjugacy. We denote the posterior as $p(\\boldsymbol{\\theta}_i | \\mathcal{D}_i) = \\mathcal{N}(\\boldsymbol{\\theta}_i | \\boldsymbol{\\mu}_{\\text{post}}, \\boldsymbol{\\Sigma}_{\\text{post}})$.\n\nTo find the posterior parameters $\\boldsymbol{\\mu}_{\\text{post}}$ and $\\boldsymbol{\\Sigma}_{\\text{post}}$, we analyze the exponent of the posterior distribution, which is the sum of the exponents of the likelihood and the prior (ignoring constant terms):\n$$\n-\\frac{1}{2} \\left[ (\\boldsymbol{\\theta}_i - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\theta}_i - \\boldsymbol{\\mu}) + \\frac{1}{\\sigma^2} (\\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\theta}_i)^T (\\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\theta}_i) \\right]\n$$\nBy expanding the quadratic forms and collecting terms involving $\\boldsymbol{\\theta}_i$, we can identify the posterior mean and covariance by completing the square. The terms quadratic in $\\boldsymbol{\\theta}_i$ are:\n$$\n\\boldsymbol{\\theta}_i^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\theta}_i + \\frac{1}{\\sigma^2} \\boldsymbol{\\theta}_i^T \\mathbf{X}_i^T \\mathbf{X}_i \\boldsymbol{\\theta}_i = \\boldsymbol{\\theta}_i^T \\left( \\boldsymbol{\\Sigma}^{-1} + \\frac{1}{\\sigma^2} \\mathbf{X}_i^T \\mathbf{X}_i \\right) \\boldsymbol{\\theta}_i\n$$\nFrom this, we identify the inverse of the posterior covariance matrix (the posterior precision):\n$$\n\\boldsymbol{\\Sigma}_{\\text{post}}^{-1} = \\boldsymbol{\\Sigma}^{-1} + \\frac{1}{\\sigma^2} \\mathbf{X}_i^T \\mathbf{X}_i\n$$\nThe posterior covariance matrix is therefore:\n$$\n\\boldsymbol{\\Sigma}_{\\text{post}} = \\left( \\boldsymbol{\\Sigma}^{-1} + \\frac{1}{\\sigma^2} \\mathbf{X}_i^T \\mathbf{X}_i \\right)^{-1}\n$$\nThe terms linear in $\\boldsymbol{\\theta}_i$ are:\n$$\n-2 \\boldsymbol{\\mu}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\theta}_i - \\frac{2}{\\sigma^2} \\mathbf{y}_i^T \\mathbf{X}_i \\boldsymbol{\\theta}_i = -2 \\left( \\boldsymbol{\\mu}^T \\boldsymbol{\\Sigma}^{-1} + \\frac{1}{\\sigma^2} \\mathbf{y}_i^T \\mathbf{X}_i \\right) \\boldsymbol{\\theta}_i\n$$\nComparing this to the linear term from the general Gaussian form, $-2\\boldsymbol{\\mu}_{\\text{post}}^T \\boldsymbol{\\Sigma}_{\\text{post}}^{-1} \\boldsymbol{\\theta}_i$, we find:\n$$\n\\boldsymbol{\\mu}_{\\text{post}}^T \\boldsymbol{\\Sigma}_{\\text{post}}^{-1} = \\boldsymbol{\\mu}^T \\boldsymbol{\\Sigma}^{-1} + \\frac{1}{\\sigma^2} \\mathbf{y}_i^T \\mathbf{X}_i\n$$\nTransposing and solving for $\\boldsymbol{\\mu}_{\\text{post}}$, we get the posterior mean:\n$$\n\\boldsymbol{\\mu}_{\\text{post}} = \\boldsymbol{\\Sigma}_{\\text{post}} \\left( \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu} + \\frac{1}{\\sigma^2} \\mathbf{X}_i^T \\mathbf{y}_i \\right)\n$$\nThe posterior mean of the slope, $\\mathbb{E}[b_i | \\mathcal{D}_i]$, is the second component of the vector $\\boldsymbol{\\mu}_{\\text{post}}$.\n\n**2. Posterior Predictive Distribution**\n\nFor a new time point $t^\\ast$, we want to predict the corresponding biomarker value $y^\\ast$. The model for this new point is $y^\\ast = \\mathbf{x}^{\\ast T} \\boldsymbol{\\theta}_i + \\varepsilon^\\ast$, where $\\mathbf{x}^\\ast = [1, t^\\ast]^T$ and $\\varepsilon^\\ast \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nThe posterior predictive distribution is obtained by marginalizing (integrating out) the model parameters over their posterior distribution:\n$$\np(y^\\ast | \\mathcal{D}_i) = \\int p(y^\\ast | \\boldsymbol{\\theta}_i) p(\\boldsymbol{\\theta}_i | \\mathcal{D}_i) d\\boldsymbol{\\theta}_i\n$$\nThis is an integral over the product of two Gaussians, which results in another Gaussian. We can find its mean and variance.\n\nThe posterior predictive mean is found using the law of total expectation:\n$$\n\\mu_{\\text{pred}} = \\mathbb{E}[y^\\ast | \\mathcal{D}_i] = \\mathbb{E}_{\\boldsymbol{\\theta}_i | \\mathcal{D}_i} \\left[ \\mathbb{E}[y^\\ast | \\boldsymbol{\\theta}_i, \\mathcal{D}_i] \\right] = \\mathbb{E}_{\\boldsymbol{\\theta}_i | \\mathcal{D}_i} \\left[ \\mathbf{x}^{\\ast T} \\boldsymbol{\\theta}_i \\right] = \\mathbf{x}^{\\ast T} \\mathbb{E}[\\boldsymbol{\\theta}_i | \\mathcal{D}_i] = \\mathbf{x}^{\\ast T} \\boldsymbol{\\mu}_{\\text{post}}\n$$\nThe posterior predictive variance is found using the law of total variance:\n$$\n\\sigma^2_{\\text{pred}} = \\text{Var}(y^\\ast | \\mathcal{D}_i) = \\mathbb{E}_{\\boldsymbol{\\theta}_i | \\mathcal{D}_i} \\left[ \\text{Var}(y^\\ast | \\boldsymbol{\\theta}_i, \\mathcal{D}_i) \\right] + \\text{Var}_{\\boldsymbol{\\theta}_i | \\mathcal{D}_i} \\left( \\mathbb{E}[y^\\ast | \\boldsymbol{\\theta}_i, \\mathcal{D}_i] \\right)\n$$\nThe first term is the expected observation variance: $\\mathbb{E}[\\sigma^2] = \\sigma^2$. The second term is the variance of the mean prediction due to uncertainty in the parameters $\\boldsymbol{\\theta}_i$: $\\text{Var}(\\mathbf{x}^{\\ast T} \\boldsymbol{\\theta}_i | \\mathcal{D}_i) = \\mathbf{x}^{\\ast T} \\text{Var}(\\boldsymbol{\\theta}_i | \\mathcal{D}_i) \\mathbf{x}^\\ast = \\mathbf{x}^{\\ast T} \\boldsymbol{\\Sigma}_{\\text{post}} \\mathbf{x}^\\ast$.\nCombining these, the posterior predictive variance is:\n$$\n\\sigma^2_{\\text{pred}} = \\sigma^2 + \\mathbf{x}^{\\ast T} \\boldsymbol{\\Sigma}_{\\text{post}} \\mathbf{x}^\\ast\n$$\nThus, the posterior predictive distribution is $p(y^\\ast | \\mathcal{D}_i) = \\mathcal{N}(y^\\ast | \\mu_{\\text{pred}}, \\sigma^2_{\\text{pred}})$.\n\n**3. Credible Interval**\n\nA symmetric central credible interval with coverage $\\alpha$ for $y^\\ast$ is constructed from the quantiles of its Gaussian posterior predictive distribution. Let $z_q$ be the $q$-th quantile of the standard normal distribution $\\mathcal{N}(0, 1)$. The lower and upper bounds of the interval are:\n$$\n\\text{Lower bound} = \\mu_{\\text{pred}} + z_{(1-\\alpha)/2} \\sqrt{\\sigma^2_{\\text{pred}}} = \\mu_{\\text{pred}} - z_{(1+\\alpha)/2} \\sqrt{\\sigma^2_{\\text{pred}}}\n$$\n$$\n\\text{Upper bound} = \\mu_{\\text{pred}} + z_{(1+\\alpha)/2} \\sqrt{\\sigma^2_{\\text{pred}}}\n$$\nThe quantile $z_{(1+\\alpha)/2}$ is calculated using the inverse cumulative distribution function (also known as the percent point function) of the standard normal distribution.\n\nThe algorithm proceeds by applying these formulas for each patient case using the provided data and hyperparameters.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian linear regression problem for multiple patient cases.\n    \"\"\"\n    # Define globally shared hyperparameters\n    mu_prior = np.array([95.0, 0.05])\n    Sigma_prior = np.array([[100.0, 0.0], [0.0, 0.01]])\n    sigma2 = 4.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"t_obs\": np.array([0.0, 30.0, 60.0, 90.0]),\n            \"y_obs\": np.array([100.0, 103.0, 107.0, 112.0]),\n            \"t_future\": np.array([120.0, 180.0]),\n            \"alpha\": 0.95\n        },\n        # Case 2 (boundary: single observation)\n        {\n            \"t_obs\": np.array([0.0]),\n            \"y_obs\": np.array([80.0]),\n            \"t_future\": np.array([30.0, 60.0, 90.0]),\n            \"alpha\": 0.95\n        },\n        # Case 3 (noisy trajectory and different coverage)\n        {\n            \"t_obs\": np.array([0.0, 10.0, 20.0, 30.0, 40.0]),\n            \"y_obs\": np.array([50.0, 49.0, 53.0, 47.0, 52.0]),\n            \"t_future\": np.array([50.0, 100.0]),\n            \"alpha\": 0.90\n        }\n    ]\n\n    results = []\n    \n    # Pre-compute the prior precision matrix\n    Sigma_prior_inv = np.linalg.inv(Sigma_prior)\n    \n    for case in test_cases:\n        t_obs = case[\"t_obs\"]\n        y_obs = case[\"y_obs\"]\n        t_future = case[\"t_future\"]\n        alpha = case[\"alpha\"]\n        \n        # Construct design matrix X for observed data\n        X_obs = np.vstack([np.ones(len(t_obs)), t_obs]).T\n        \n        # Calculate posterior distribution parameters\n        # Posterior precision: Sigma_post_inv = Sigma_prior_inv + (1/sigma2) * X_obs.T @ X_obs\n        Sigma_post_inv = Sigma_prior_inv + (1.0 / sigma2) * (X_obs.T @ X_obs)\n        # Posterior covariance\n        Sigma_post = np.linalg.inv(Sigma_post_inv)\n        # Posterior mean\n        mu_post = Sigma_post @ (Sigma_prior_inv @ mu_prior + (1.0 / sigma2) * (X_obs.T @ y_obs))\n        \n        # Task 1: Find the posterior mean of the slope b_i\n        posterior_b_mean = mu_post[1]\n        results.append(round(posterior_b_mean, 6))\n        \n        # Task 2: Compute posterior predictive means and credible intervals\n        # Get the z-score corresponding to the coverage level alpha\n        z_score = norm.ppf((1.0 + alpha) / 2.0)\n        \n        for t_star in t_future:\n            # Construct design vector for the future time point\n            x_star = np.array([1.0, t_star])\n            \n            # Posterior predictive mean\n            pred_mean = x_star.T @ mu_post\n            \n            # Posterior predictive variance is the sum of two components:\n            # 1. Variance from the observation noise (sigma2)\n            # 2. Variance from the uncertainty in the posterior of the parameters\n            pred_var = sigma2 + x_star.T @ Sigma_post @ x_star\n            pred_std = np.sqrt(pred_var)\n            \n            # Calculate the symmetric central credible interval\n            margin_of_error = z_score * pred_std\n            lower_bound = pred_mean - margin_of_error\n            upper_bound = pred_mean + margin_of_error\n            \n            results.append(round(pred_mean, 6))\n            results.append(round(lower_bound, 6))\n            results.append(round(upper_bound, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4597855"}, {"introduction": "The ultimate goal of many longitudinal studies is to link biomarker dynamics to clinical outcomes. This final practice integrates the concepts from the previous exercises by introducing a joint model, where a time-dependent biomarker trajectory influences the instantaneous risk of an event within a Cox Proportional Hazards framework. By estimating the association parameter $\\eta$, you will learn to quantify the predictive power of a longitudinal marker and build the core of a dynamic prediction model that updates a patient's risk in real time [@problem_id:4597903].", "problem": "You are given a scenario typical of joint modeling where a longitudinal biomarker trajectory is associated with instantaneous risk of a clinical event through a Cox Proportional Hazards (CPH) structure. The goal is to estimate the association parameter $\\eta$ that quantifies how the current level of the biomarker affects the hazard. The estimation must be performed using the partial likelihood induced by the CPH model with time-dependent covariates, handling tied event times using the Breslow approximation.\n\nFundamental base:\n- The Cox Proportional Hazards (CPH) model defines the individual hazard as\n$$\nh_i(t) \\;=\\; h_0(t)\\,\\exp\\!\\big(\\eta\\,m_i(t)\\big),\n$$\nwhere $\\,h_0(t)\\,$ is an unspecified baseline hazard, $\\,m_i(t)\\,$ is the current biomarker level for individual $\\,i\\,$ at time $\\,t\\,$, and $\\,\\eta\\,$ is the association parameter to be estimated.\n- The partial likelihood for $\\eta$ uses event times and risk sets and does not require specification of $\\,h_0(t)\\,$. Tied event times must be handled via the Breslow approximation.\n\nBiomarker trajectory representation:\n- Each subject $\\,i\\,$ has two observed biomarker measurements at times $\\,t_{i0}\\,$ and $\\,t_{i1}\\,$ with values $\\,m_{i0}\\,$ and $\\,m_{i1}\\,$. The current biomarker level at any time $\\,t\\,$ is assumed to follow a linear trajectory between the two observed points:\n$$\nm_i(t) \\;=\\; m_{i0} + (m_{i1}-m_{i0})\\;\\frac{t - t_{i0}}{t_{i1}-t_{i0}}.\n$$\n\nData elements and construction rules:\n- For each test case, subjects are defined by vectors of survival times $\\,T_i\\,$ (in days), event indicators $\\,\\delta_i \\in \\{0,1\\}\\,$, biomarker measurement times $\\,\\{t_{i0},t_{i1}\\}\\,$ (in days), and biomarker values $\\,\\{m_{i0},m_{i1}\\}\\,$.\n- Event times $\\,\\{t_k\\}\\,$ are the distinct times at which at least one event occurs. At each event time $\\,t_k\\,$, define the event set $\\,D_k \\,=\\, \\{\\,i : \\delta_i=1,\\,T_i = t_k\\,\\}\\,$ and the risk set $\\,R_k \\,=\\, \\{\\,i : T_i \\ge t_k \\,\\}\\,$.\n- At each event time $\\,t_k\\,$, evaluate $\\,m_i(t_k)\\,$ for all $\\,i\\in R_k\\,$ by linear interpolation between the two measurement times. No extrapolation outside $\\,\\min(t_{i0},t_{i1})\\,$ to $\\,\\max(t_{i0},t_{i1})\\,$ occurs in the provided cases.\n\nEstimation target:\n- Maximize the Breslow partial likelihood in $\\eta$ using a numerically stable method. You must implement a Newton–Raphson procedure with backtracking line search and a fallback bounded scalar maximization if the Hessian is near-singular or the Newton update fails to improve the objective.\n- If, at all event times $\\,t_k\\,$, the biomarker levels $\\,\\{m_i(t_k) : i \\in R_k\\}\\,$ are all equal (yielding zero variance in the risk set), the partial likelihood is flat in $\\eta$; in that case, return $\\,0.0\\,$.\n\nUnits:\n- Time must be treated in days. The association parameter $\\eta$ is dimensionless. Your program must output $\\eta$ rounded to three decimal places.\n\nTest suite:\n- Case A (positive association, happy path):\n  - Subjects: $\\,N=6\\,$.\n  - Survival times (days): $\\,T = [\\,3,\\,5,\\,7,\\,12,\\,12,\\,12\\,]\\,$.\n  - Event indicators: $\\,\\delta = [\\,1,\\,1,\\,1,\\,0,\\,0,\\,0\\,]\\,$.\n  - Biomarker measurement times (days): for all subjects, $\\,t_{i0}=0\\,$, $\\,t_{i1}=10\\,$.\n  - Biomarker values:\n    - Subject $\\,1$: $\\,m_{10}=3.0\\,$, $\\,m_{11}=5.0\\,$.\n    - Subject $\\,2$: $\\,m_{20}=2.8\\,$, $\\,m_{21}=4.3\\,$.\n    - Subject $\\,3$: $\\,m_{30}=2.5\\,$, $\\,m_{31}=3.5\\,$.\n    - Subject $\\,4$: $\\,m_{40}=1.5\\,$, $\\,m_{41}=2.0\\,$.\n    - Subject $\\,5$: $\\,m_{50}=1.0\\,$, $\\,m_{51}=2.0\\,$.\n    - Subject $\\,6$: $\\,m_{60}=0.8\\,$, $\\,m_{61}=1.3\\,$.\n- Case B (negative association):\n  - Subjects: $\\,N=6\\,$.\n  - Survival times (days): $\\,T = [\\,3,\\,5,\\,7,\\,12,\\,12,\\,12\\,]\\,$.\n  - Event indicators: $\\,\\delta = [\\,1,\\,1,\\,1,\\,0,\\,0,\\,0\\,]\\,$.\n  - Biomarker measurement times (days): for all subjects, $\\,t_{i0}=0\\,$, $\\,t_{i1}=10\\,$.\n  - Biomarker values:\n    - Subject $\\,1$: $\\,m_{10}=0.5\\,$, $\\,m_{11}=0.5\\,$.\n    - Subject $\\,2$: $\\,m_{20}=0.7\\,$, $\\,m_{21}=1.2\\,$.\n    - Subject $\\,3$: $\\,m_{30}=0.8\\,$, $\\,m_{31}=1.3\\,$.\n    - Subject $\\,4$: $\\,m_{40}=2.5\\,$, $\\,m_{41}=3.5\\,$.\n    - Subject $\\,5$: $\\,m_{50}=2.8\\,$, $\\,m_{51}=4.3\\,$.\n    - Subject $\\,6$: $\\,m_{60}=3.0\\,$, $\\,m_{61}=5.0\\,$.\n- Case C (no association; flat likelihood):\n  - Subjects: $\\,N=6\\,$.\n  - Survival times (days): $\\,T = [\\,3,\\,5,\\,7,\\,12,\\,12,\\,12\\,]\\,$.\n  - Event indicators: $\\,\\delta = [\\,1,\\,1,\\,1,\\,0,\\,0,\\,0\\,]\\,$.\n  - Biomarker measurement times (days): for all subjects, $\\,t_{i0}=0\\,$, $\\,t_{i1}=10\\,$.\n  - Biomarker values: for all subjects, $\\,m_{i0}=2.0\\,$ and $\\,m_{i1}=2.0\\,$.\n- Case D (ties; Breslow handling):\n  - Subjects: $\\,N=5\\,$.\n  - Survival times (days): $\\,T = [\\,4,\\,4,\\,6,\\,8,\\,8\\,]\\,$.\n  - Event indicators: $\\,\\delta = [\\,1,\\,1,\\,1,\\,0,\\,0\\,]\\,$.\n  - Biomarker measurement times (days): for all subjects, $\\,t_{i0}=0\\,$, $\\,t_{i1}=8\\,$.\n  - Biomarker values:\n    - Subject $\\,1$: $\\,m_{10}=2.5\\,$, $\\,m_{11}=3.5\\,$.\n    - Subject $\\,2$: $\\,m_{20}=2.3\\,$, $\\,m_{21}=3.3\\,$.\n    - Subject $\\,3$: $\\,m_{30}=1.7\\,$, $\\,m_{31}=2.1\\,$.\n    - Subject $\\,4$: $\\,m_{40}=1.5\\,$, $\\,m_{41}=1.9\\,$.\n    - Subject $\\,5$: $\\,m_{50}=1.2\\,$, $\\,m_{51}=1.6\\,$.\n\nAlgorithmic requirements:\n- Construct $\\,\\{(t_k, D_k, R_k)\\}\\,$ and evaluate $\\,m_i(t_k)\\,$ by linear interpolation.\n- Implement a Newton–Raphson optimizer for the Breslow partial log-likelihood in $\\eta$ with backtracking line search and Hessian ridge stabilization. If the Hessian magnitude is below a small threshold or the update fails to increase the objective, fall back to a bounded scalar maximization over $\\,[-5,5]\\,$.\n- Convergence criterion: terminate when the absolute update is below $\\,10^{-6}\\,$ and the gradient magnitude is below $\\,10^{-6}\\,$, or when a maximum of $\\,50\\,$ iterations is reached.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each $\\eta$ rounded to three decimal places, in the order $\\,[$Case A, Case B, Case C, Case D$]$.\n\nThe answer must be dimensionless (no physical unit) and be rounded to three decimal places. No other text should be printed.", "solution": "The problem requires the estimation of the association parameter $\\eta$ in a time-dependent Cox Proportional Hazards (CPH) model. This model links a longitudinal biomarker trajectory to the instantaneous risk of a clinical event. The estimation is accomplished by maximizing the Breslow partial likelihood using a Newton-Raphson algorithm.\n\n### 1. Model and Likelihood Formulation\n\nThe hazard function for an individual $i$ at time $t$ is given by the CPH model:\n$$\nh_i(t) = h_0(t)\\,\\exp(\\eta\\,m_i(t))\n$$\nwhere $h_0(t)$ is the unspecified baseline hazard, $m_i(t)$ is the value of the time-dependent biomarker for individual $i$ at time $t$, and $\\eta$ is the association parameter to be estimated.\n\nThe biomarker trajectory $m_i(t)$ is determined by linear interpolation between two observed measurements, $(t_{i0}, m_{i0})$ and $(t_{i1}, m_{i1})$:\n$$\nm_i(t) = m_{i0} + (m_{i1} - m_{i0}) \\frac{t - t_{i0}}{t_{i1} - t_{i0}}\n$$\n\nTo estimate $\\eta$ without specifying $h_0(t)$, we use the partial likelihood. Let the distinct ordered event times be $t_1  t_2  \\dots  t_K$. At each event time $t_k$, we define the set of individuals experiencing an event as $D_k = \\{i : \\delta_i=1, T_i = t_k\\}$, with size $d_k = |D_k|$. The set of individuals at risk is $R_k = \\{i : T_i \\ge t_k\\}$.\n\nTo handle tied event times (i.e., $d_k > 1$), the problem specifies using the Breslow approximation to the partial likelihood. The resulting log-partial likelihood, $\\ell(\\eta)$, which we aim to maximize, is:\n$$\n\\ell(\\eta) = \\sum_{k=1}^K \\left[ \\eta \\sum_{j \\in D_k} m_j(t_k) - d_k \\log\\left(\\sum_{i \\in R_k} \\exp(\\eta m_i(t_k))\\right) \\right]\n$$\nThis function is concave in $\\eta$, which simplifies maximization.\n\n### 2. Optimization via Newton-Raphson\n\nWe employ the Newton-Raphson method, an iterative-descent algorithm, to find the value of $\\eta$ that maximizes $\\ell(\\eta)$. This requires the first and second derivatives of the log-likelihood function.\n\nLet us define the following sums over the risk set $R_k$ at event time $t_k$:\n$$\nS_k^{(p)}(\\eta) = \\sum_{i \\in R_k} m_i(t_k)^p \\exp(\\eta \\, m_i(t_k)) \\quad \\text{for } p \\in \\{0, 1, 2\\}\n$$\n\nThe first derivative of $\\ell(\\eta)$, known as the score function $U(\\eta)$, is:\n$$\nU(\\eta) = \\frac{\\partial \\ell}{\\partial \\eta} = \\sum_{k=1}^K \\left[ \\sum_{j \\in D_k} m_j(t_k) - d_k \\frac{S_k^{(1)}(\\eta)}{S_k^{(0)}(\\eta)} \\right]\n$$\n\nThe second derivative, the Hessian $H(\\eta)$ (a scalar in this one-parameter case), is:\n$$\nH(\\eta) = \\frac{\\partial^2 \\ell}{\\partial \\eta^2} = \\sum_{k=1}^K -d_k \\left[ \\frac{S_k^{(2)}(\\eta)}{S_k^{(0)}(\\eta)} - \\left(\\frac{S_k^{(1)}(\\eta)}{S_k^{(0)}(\\eta)}\\right)^2 \\right]\n$$\nThe term in the square brackets is the variance of the biomarker values $\\{m_i(t_k) : i \\in R_k\\}$ weighted by $\\exp(\\eta m_i(t_k))$, which is always non-negative. Since $d_k \\ge 1$, the Hessian $H(\\eta)$ is non-positive, confirming the concavity of $\\ell(\\eta)$.\n\nThe Newton-Raphson update at each iteration is:\n$$\n\\eta_{\\text{new}} = \\eta_{\\text{old}} - \\alpha \\cdot (H(\\eta_{\\text{old}}))^{-1} U(\\eta_{\\text{old}})\n$$\nwhere $\\alpha \\in (0, 1]$ is a step size determined by a backtracking line search to ensure an increase in the log-likelihood at each step.\n\n### 3. Numerical Implementation Details\n\n**Numerical Stability**: Direct computation of $S_k^{(p)}(\\eta)$ can lead to numerical overflow due to the exponential terms. To prevent this, we use the log-sum-exp trick. For each event time $t_k$, we define a shift constant $c_k = \\max_{i \\in R_k} \\{\\eta \\cdot m_i(t_k)\\}$. The log of the sum is then computed stably as:\n$$\n\\log(S_k^{(0)}(\\eta)) = \\log\\left(\\sum_{i \\in R_k} e^{\\eta m_i(t_k)}\\right) = c_k + \\log\\left(\\sum_{i \\in R_k} e^{\\eta m_i(t_k) - c_k}\\right)\n$$\nThe ratios $S_k^{(1)}/S_k^{(0)}$ and $S_k^{(2)}/S_k^{(0)}$ can be computed using terms $e^{\\eta m_i(t_k) - c_k}$, which are bounded above by $1$ and thus avoid overflow.\n\n**Backtracking Line Search**: To ensure convergence, the step size $\\alpha$ is chosen to satisfy the Armijo condition. Starting with $\\alpha=1$, we iteratively reduce it (e.g., by a factor of $0.5$) until the following inequality holds, where $\\Delta\\eta = -U(\\eta)/H(\\eta)$ is the Newton step direction:\n$$\n\\ell(\\eta + \\alpha \\Delta\\eta) > \\ell(\\eta) + c_1 \\alpha U(\\eta) \\Delta\\eta\n$$\nA typical value for the control parameter is $c_1 = 10^{-4}$.\n\n**Special Conditions and Fallbacks**:\n- **Flat Likelihood**: If for every event time $t_k$, all biomarker values $\\{m_i(t_k) : i \\in R_k\\}$ in the risk set are identical, the variance term in the Hessian is zero for all $k$. This makes $H(\\eta) = 0$ for all $\\eta$, rendering the likelihood flat. In this scenario, $\\eta$ is unidentifiable, and the problem specifies returning a value of $0.0$.\n- **Fallback Optimization**: The Newton-Raphson method may fail if the Hessian is near-singular (i.e., its magnitude is below a small threshold, e.g., $10^{-8}$) or if the backtracking line search fails to find an adequate step size $\\alpha$. In such cases, the algorithm is specified to switch to a more robust, but potentially slower, bounded scalar maximization routine (`scipy.optimize.minimize_scalar` with `method='bounded'`) to find the maximum of $\\ell(\\eta)$ within the interval $[-5, 5]$.\n\n**Algorithm Summary**:\n1.  Initialize $\\eta = 0.0$.\n2.  For each test case, parse the data and pre-calculate the sets $\\{t_k, D_k, R_k\\}$ and the interpolated biomarker values $m_i(t_k)$ for each $i \\in R_k$.\n3.  Check for the flat likelihood condition. If met, return $0.0$.\n4.  Iterate the Newton-Raphson procedure:\n    a. Compute $U(\\eta)$ and $H(\\eta)$ using numerically stable methods.\n    b. Check for convergence (gradient magnitude $|U(\\eta)|  10^{-6}$ and update size $|\\Delta\\eta|  10^{-6}$).\n    c. Check for a near-singular Hessian. If so, fall back to the bounded optimizer.\n    d. Calculate the Newton step $\\Delta\\eta = -U(\\eta)/H(\\eta)$.\n    e. Perform backtracking line search to find a suitable step size $\\alpha$. If the line search fails, fall back.\n    f. Update $\\eta \\leftarrow \\eta + \\alpha \\Delta\\eta$.\n5.  If the maximum number of iterations ($50$) is reached without convergence, fall back to the bounded optimizer.\n6.  The final estimated $\\eta$ is rounded to three decimal places.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Solves for the association parameter eta in a time-dependent Cox model\n    for several test cases, as specified in the problem statement.\n    \"\"\"\n\n    # --- Data Definition ---\n\n    case_a_data = {\n        'T': np.array([3, 5, 7, 12, 12, 12]),\n        'delta': np.array([1, 1, 1, 0, 0, 0]),\n        'biomarker_t': np.array([[0, 10] for _ in range(6)]),\n        'biomarker_m': np.array([\n            [3.0, 5.0], [2.8, 4.3], [2.5, 3.5],\n            [1.5, 2.0], [1.0, 2.0], [0.8, 1.3]\n        ])\n    }\n\n    case_b_data = {\n        'T': np.array([3, 5, 7, 12, 12, 12]),\n        'delta': np.array([1, 1, 1, 0, 0, 0]),\n        'biomarker_t': np.array([[0, 10] for _ in range(6)]),\n        'biomarker_m': np.array([\n            [0.5, 0.5], [0.7, 1.2], [0.8, 1.3],\n            [2.5, 3.5], [2.8, 4.3], [3.0, 5.0]\n        ])\n    }\n\n    case_c_data = {\n        'T': np.array([3, 5, 7, 12, 12, 12]),\n        'delta': np.array([1, 1, 1, 0, 0, 0]),\n        'biomarker_t': np.array([[0, 10] for _ in range(6)]),\n        'biomarker_m': np.array([[2.0, 2.0] for _ in range(6)])\n    }\n\n    case_d_data = {\n        'T': np.array([4, 4, 6, 8, 8]),\n        'delta': np.array([1, 1, 1, 0, 0]),\n        'biomarker_t': np.array([[0, 8] for _ in range(5)]),\n        'biomarker_m': np.array([\n            [2.5, 3.5], [2.3, 3.3], [1.7, 2.1],\n            [1.5, 1.9], [1.2, 1.6]\n        ])\n    }\n\n    test_cases = [case_a_data, case_b_data, case_c_data, case_d_data]\n\n    # --- Solver Implementation ---\n\n    class CoxPHSolver:\n        def __init__(self, T, delta, biomarker_t, biomarker_m):\n            self.T = T\n            self.delta = delta\n            self.biomarker_t = biomarker_t\n            self.biomarker_m = biomarker_m\n            self.n_subjects = len(T)\n            self.subject_indices = np.arange(self.n_subjects)\n            \n            self.unique_event_times = sorted(np.unique(T[delta == 1]))\n            self.precomputed_data = self._precompute()\n\n        def _get_biomarker_value(self, subject_idx, t):\n            t0, t1 = self.biomarker_t[subject_idx]\n            m0, m1 = self.biomarker_m[subject_idx]\n            if t1 == t0:\n                return m0\n            return m0 + (m1 - m0) * (t - t0) / (t1 - t0)\n\n        def _precompute(self):\n            data = []\n            if not self.unique_event_times:\n                return data\n\n            for t_k in self.unique_event_times:\n                risk_set_indices = self.subject_indices[self.T >= t_k]\n                event_set_indices = self.subject_indices[(self.T == t_k)  (self.delta == 1)]\n                \n                m_values = {i: self._get_biomarker_value(i, t_k) for i in risk_set_indices}\n                \n                data.append({\n                    \"risk_set_indices\": risk_set_indices,\n                    \"event_set_indices\": event_set_indices,\n                    \"d_k\": len(event_set_indices),\n                    \"m_values\": m_values,\n                })\n            return data\n\n        def _log_likelihood_and_derivatives(self, eta):\n            logL, U, H = 0.0, 0.0, 0.0\n\n            for data_k in self.precomputed_data:\n                m_vals_risk = np.array(list(data_k[\"m_values\"].values()))\n                \n                # Numerically stable calculation using log-sum-exp trick\n                eta_m = eta * m_vals_risk\n                c_k = np.max(eta_m) if eta_m.size > 0 else 0\n                exp_terms = np.exp(eta_m - c_k)\n                \n                S0 = np.sum(exp_terms)\n                S1 = np.sum(m_vals_risk * exp_terms)\n                S2 = np.sum(m_vals_risk**2 * exp_terms)\n\n                m_vals_event = np.array([data_k[\"m_values\"][i] for i in data_k[\"event_set_indices\"]])\n                sum_m_events = np.sum(m_vals_event)\n                \n                logL += eta * sum_m_events - data_k[\"d_k\"] * (c_k + np.log(S0))\n                \n                E_m = S1 / S0\n                U += sum_m_events - data_k[\"d_k\"] * E_m\n                \n                Var_m = S2 / S0 - E_m**2\n                H -= data_k[\"d_k\"] * Var_m\n                \n            return logL, U, H\n\n        def _is_likelihood_flat(self):\n            if not self.precomputed_data:\n                return True\n            for data_k in self.precomputed_data:\n                m_vals = np.array(list(data_k[\"m_values\"].values()))\n                if m_vals.size > 1 and np.std(m_vals) > 1e-9:\n                    return False\n            return True\n\n        def _fallback_solver(self):\n            def objective(eta_val):\n                try:\n                    logL, _, _ = self._log_likelihood_and_derivatives(eta_val)\n                    if not np.isfinite(logL):\n                        return np.finfo(np.float64).max\n                    return -logL\n                except (ValueError, FloatingPointError):\n                    return np.finfo(np.float64).max\n\n            res = minimize_scalar(objective, bounds=(-5, 5), method='bounded')\n            return res.x if res.success else np.nan\n\n        def estimate_eta(self):\n            if self._is_likelihood_flat():\n                return 0.0\n\n            eta = 0.0\n            max_iter = 50\n            conv_tol_update = 1e-6\n            conv_tol_grad = 1e-6\n            hessian_min_mag = 1e-8\n            \n            for i in range(max_iter):\n                try:\n                    logL, U, H = self._log_likelihood_and_derivatives(eta)\n                except (ValueError, FloatingPointError):\n                    return self._fallback_solver()\n\n                if abs(H)  hessian_min_mag:\n                    return self._fallback_solver()\n\n                update = -U / H\n                \n                # Check for convergence before line search\n                if i > 0 and abs(last_update)  conv_tol_update and abs(U)  conv_tol_grad:\n                    return eta\n                last_update = update\n\n                # Backtracking line search\n                alpha, alpha_min, c1 = 1.0, 1e-8, 1e-4\n                dir_deriv = U * update\n                \n                line_search_failed = True\n                while alpha > alpha_min:\n                    try:\n                        next_logL, _, _ = self._log_likelihood_and_derivatives(eta + alpha * update)\n                        if np.isfinite(next_logL) and next_logL > logL + c1 * alpha * dir_deriv:\n                            line_search_failed = False\n                            break\n                    except (ValueError, FloatingPointError):\n                        pass\n                    alpha *= 0.5\n\n                if line_search_failed:\n                    return self._fallback_solver()\n                \n                eta += alpha * update\n            \n            # If max iterations reached, fall back\n            return self._fallback_solver()\n\n    # --- Main Execution Logic ---\n    results = []\n    for case_data in test_cases:\n        solver = CoxPHSolver(\n            T=case_data['T'],\n            delta=case_data['delta'],\n            biomarker_t=case_data['biomarker_t'],\n            biomarker_m=case_data['biomarker_m']\n        )\n        eta_hat = solver.estimate_eta()\n        results.append(f\"{eta_hat:.3f}\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4597903"}]}