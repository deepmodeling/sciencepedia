{"hands_on_practices": [{"introduction": "To truly understand a clustering evaluation metric, there is no substitute for building it from the ground up. This first practice challenges you to implement the Silhouette Coefficient from its fundamental definitions of intra-cluster cohesion, $a(i)$, and inter-cluster separation, $b(i)$. By applying your implementation to datasets with varying feature scales and data types, you will gain a practical understanding of why appropriate data preprocessing, such as z-score normalization or variance-stabilizing transformations, is not just a preliminary step but a critical component of meaningful cluster evaluation [@problem_id:4561589].", "problem": "You are given multiple small feature matrices representing bioinformatics and medical data analytics scenarios where clustering has been performed and cluster labels are specified. You must evaluate the clustering results by computing a standardized index for each sample that is based on intra-cluster cohesion and nearest inter-cluster separation, and then produce the overall average index across all samples. The evaluation must be done under different preprocessing pipelines, including feature scaling, normalization, and variance stabilization, and use a Euclidean metric.\n\nFundamental base definitions to use:\n- Let the dataset be a set of points $\\{x_i\\}_{i=1}^n$ in $\\mathbb{R}^d$.\n- Let a dissimilarity be defined by a metric $d(\\cdot,\\cdot)$ on $\\mathbb{R}^d$; specifically, use the Euclidean metric $d(x,y)=\\sqrt{\\sum_{j=1}^d (x_j - y_j)^2}$.\n- Let a clustering be a partition of $\\{1,\\dots,n\\}$ into $K$ disjoint non-empty index sets $\\{C_k\\}_{k=1}^K$ where each $C_k$ indexes the samples belonging to cluster $k$. The cluster label for sample $i$ is an integer $L_i \\in \\{0,1,\\dots,K-1\\}$ that maps sample $i$ to its cluster.\n- For each sample $i$, define the average within-cluster dissimilarity $a(i)$ as the average of $d(x_i, x_j)$ over all $j$ in the same cluster as $i$ with $j \\neq i$. If the cluster containing $i$ has size $1$, then by convention set the per-sample standardized index to $0$ directly.\n- For each sample $i$, define the nearest other cluster by computing, for each cluster $k \\neq L_i$, the average dissimilarity from $i$ to all samples in $C_k$, and let $b(i)$ be the minimum of these average dissimilarities.\n- From $a(i)$ and $b(i)$, derive a per-sample standardized index bounded in $\\left[-1, 1\\right]$ that increases when $x_i$ is closer (on average) to members of its own cluster than to members of the nearest other cluster; if the normalizing denominator in your derived expression is $0$, set the index to $0$ for numerical stability. The overall evaluation metric, the Silhouette Coefficient (SC), is defined as the average of this per-sample standardized index across all samples.\n\nPreprocessing pipelines to apply before computing the metric:\n- Z-score normalization: For each feature $j \\in \\{1,\\dots,d\\}$, compute the feature mean $\\mu_j$ and the feature Standard Deviation (SD) $\\sigma_j$ over all samples, and transform each $x_{ij}$ to $(x_{ij} - \\mu_j)/\\sigma_j$. If $\\sigma_j = 0$, set the transformed feature value to $0$ for all samples to avoid division by zero.\n- Min-max scaling: For each feature $j$, compute the feature minimum $m_j$ and maximum $M_j$ over all samples and transform each $x_{ij}$ to $(x_{ij} - m_j)/(M_j - m_j)$. If $M_j = m_j$, set the transformed feature value to $0$ for all samples to avoid division by zero.\n- Variance stabilization for counts: For count-like features, apply the natural logarithm transformation $\\log(1+x)$ elementwise to the data matrix, then apply z-score normalization as above.\n\nDistance computations must be performed after the specified preprocessing pipeline is applied. Use the Euclidean distance with no additional weighting.\n\nConventions and handling:\n- If there is only one cluster ($K=1$), define the overall SC to be $0$.\n- If a sample belongs to a singleton cluster (cluster size $1$), set its per-sample standardized index to $0$.\n- If the normalizing denominator in the standardized index is $0$ for a sample, set that sample’s standardized index to $0$.\n\nTest suite specification:\nCompute the overall SC for each of the following test cases. Each case specifies a data matrix, a label vector, and a preprocessing pipeline.\n\n- Test case $1$ (happy path, mixed scales with z-score normalization):\n  Data matrix $X_{\\text{mixed}} \\in \\mathbb{R}^{6 \\times 3}$ with rows:\n  $\\left[100.0, 1.2, 5000.0\\right]$,\n  $\\left[110.0, 0.8, 5100.0\\right]$,\n  $\\left[95.0, 1.0, 4900.0\\right]$,\n  $\\left[5.0, 50.0, 180.0\\right]$,\n  $\\left[7.0, 49.5, 200.0\\right]$,\n  $\\left[6.0, 48.0, 220.0\\right]$.\n  Labels $L_{\\text{mixed}}$:\n  $\\left[0, 0, 0, 1, 1, 1\\right]$.\n  Preprocessing pipeline: z-score normalization.\n\n- Test case $2$ (effect of no scaling on mixed scales):\n  Use the same $X_{\\text{mixed}}$ and $L_{\\text{mixed}}$ as in test case $1$.\n  Preprocessing pipeline: none.\n\n- Test case $3$ (boundary condition with a singleton cluster and min-max scaling):\n  Data matrix $X_{\\text{singleton}} \\in \\mathbb{R}^{5 \\times 2}$ with rows:\n  $\\left[1.0, 100.0\\right]$,\n  $\\left[1.1, 98.0\\right]$,\n  $\\left[10.0, 5.0\\right]$,\n  $\\left[9.5, 6.0\\right]$,\n  $\\left[100.0, 1000.0\\right]$.\n  Labels $L_{\\text{singleton}}$:\n  $\\left[0, 0, 1, 1, 2\\right]$.\n  Preprocessing pipeline: min-max scaling.\n\n- Test case $4$ (count data without variance stabilization):\n  Data matrix $X_{\\text{counts}} \\in \\mathbb{R}^{9 \\times 4}$ with rows:\n  $\\left[0, 10, 500, 2000\\right]$,\n  $\\left[1, 12, 520, 2100\\right]$,\n  $\\left[0, 9, 480, 1950\\right]$,\n  $\\left[50, 0, 30, 0\\right]$,\n  $\\left[45, 1, 28, 2\\right]$,\n  $\\left[55, 2, 35, 1\\right]$,\n  $\\left[0, 300, 0, 100\\right]$,\n  $\\left[1, 290, 0, 90\\right]$,\n  $\\left[2, 310, 1, 110\\right]$.\n  Labels $L_{\\text{counts}}$:\n  $\\left[0, 0, 0, 1, 1, 1, 2, 2, 2\\right]$.\n  Preprocessing pipeline: none.\n\n- Test case $5$ (count data with variance stabilization via $\\log(1+x)$ followed by z-score normalization):\n  Use the same $X_{\\text{counts}}$ and $L_{\\text{counts}}$ as in test case $4$.\n  Preprocessing pipeline: log-one-plus followed by z-score normalization.\n\n- Test case $6$ (edge case with a zero-variance feature under z-score normalization):\n  Data matrix $X_{\\text{zerovar}} \\in \\mathbb{R}^{6 \\times 3}$ with rows:\n  $\\left[1.0, 2.0, 10.0\\right]$,\n  $\\left[1.2, 1.8, 10.0\\right]$,\n  $\\left[0.8, 2.1, 10.0\\right]$,\n  $\\left[5.0, 6.0, 10.0\\right]$,\n  $\\left[5.2, 6.1, 10.0\\right]$,\n  $\\left[4.8, 5.9, 10.0\\right]$.\n  Labels $L_{\\text{zerovar}}$:\n  $\\left[0, 0, 0, 1, 1, 1\\right]$.\n  Preprocessing pipeline: z-score normalization.\n\nProgram requirements:\n- Implement the preprocessing pipelines exactly as specified.\n- Use the Euclidean metric as the dissimilarity after preprocessing.\n- Derive and implement the per-sample standardized index from the definitions of $a(i)$ and $b(i)$ as above, and compute the overall SC as the mean across samples.\n\nOutput format:\n- Your program should produce a single line of output containing the overall SC for each of the $6$ test cases, rounded to $6$ decimal places, aggregated as a comma-separated list enclosed in square brackets, for example, $\\left[\\text{sc}_1,\\text{sc}_2,\\dots,\\text{sc}_6\\right]$ printed as Python-style list syntax.\n\nThere are no physical units and no angles in this problem. All outputs are real-valued floats. The program must be self-contained, require no input, and run as-is. Ensure that the computations and conventions follow the specifications above.", "solution": "The user-provided problem statement is subjected to rigorous validation.\n\n### Step 1: Extract Givens\n- **Dataset**: A set of points $\\{x_i\\}_{i=1}^n$ in $\\mathbb{R}^d$.\n- **Dissimilarity Metric**: Euclidean distance, $d(x,y)=\\sqrt{\\sum_{j=1}^d (x_j - y_j)^2}$.\n- **Clustering**: A partition of sample indices into $K$ disjoint non-empty sets $\\{C_k\\}_{k=1}^K$. The label for sample $i$ is $L_i \\in \\{0, 1, \\dots, K-1\\}$.\n- **Intra-cluster Dissimilarity $a(i)$**: For sample $i$, $a(i)$ is the average dissimilarity to all other samples $j$ in the same cluster: $a(i) = \\frac{1}{|C_{L_i}| - 1} \\sum_{j \\in C_{L_i}, j \\neq i} d(x_i, x_j)$.\n- **Inter-cluster Dissimilarity $b(i)$**: For sample $i$, $b(i)$ is the minimum of the average dissimilarities to all samples in any other cluster $C_k$ where $k \\neq L_i$: $b(i) = \\min_{k \\neq L_i} \\left\\{ \\frac{1}{|C_k|} \\sum_{j \\in C_k} d(x_i, x_j) \\right\\}$.\n- **Per-sample Standardized Index $s(i)$**: Derived from $a(i)$ and $b(i)$, bounded in $[-1, 1]$.\n- **Overall Evaluation Metric (Silhouette Coefficient, SC)**: The average of the per-sample standardized index across all samples: $SC = \\frac{1}{n} \\sum_{i=1}^n s(i)$.\n- **Preprocessing Pipelines**:\n  1.  **Z-score normalization**: Transform each feature value $x_{ij}$ to $(x_{ij} - \\mu_j)/\\sigma_j$, where $\\mu_j$ is the feature mean and $\\sigma_j$ is the feature standard deviation. If $\\sigma_j=0$, the transformed feature is $0$.\n  2.  **Min-max scaling**: Transform each feature value $x_{ij}$ to $(x_{ij} - m_j)/(M_j - m_j)$, where $m_j$ and $M_j$ are the feature minimum and maximum. If $M_j = m_j$, the transformed feature is $0$.\n  3.  **Variance stabilization for counts**: Apply the natural logarithm transformation $\\log(1+x)$ elementwise, then apply z-score normalization.\n- **Conventions**:\n  1.  If $K=1$, the overall SC is $0$.\n  2.  If a sample belongs to a singleton cluster, its per-sample index is $0$.\n  3.  If the normalizing denominator in the standardized index is $0$, the index is set to $0$.\n- **Test Cases**: Six specific test cases are provided, each with a data matrix, a label vector, and a designated preprocessing pipeline.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is analyzed against the validation criteria:\n- **Scientifically Grounded**: The problem asks for the computation of the Silhouette Coefficient, a standard and widely used metric for evaluating the quality of clustering in machine learning and data analysis. The definitions of intra-cluster cohesion ($a(i)$) and inter-cluster separation ($b(i)$), the formula for the per-sample score, and the preprocessing methods (z-score, min-max, log transform) are all standard and mathematically sound concepts from statistics and data science. The problem is firmly grounded in established principles.\n- **Well-Posed**: The problem is well-posed. For each test case, the inputs (data, labels, preprocessing method) are explicitly defined. The calculation steps and handling of all relevant edge cases (singleton clusters, single cluster overall, division by zero) are unambiguously specified. This deterministic nature ensures that a unique and meaningful numerical solution exists for each test case.\n- **Objective**: The problem is stated in precise, objective mathematical language. There are no subjective or ambiguous terms.\n- **Completeness and Consistency**: The problem is self-contained. It provides all necessary data and definitions required for the solution. The conventions for handling edge cases are explicit and consistent with standard practice, preventing ambiguity.\n- **Realism**: The test cases, while synthetic, are designed to model realistic scenarios encountered in bioinformatics, such as features with vastly different scales and count-based data, justifying the use of different preprocessing strategies. The task itself—evaluating clustering under various data transformations—is a core activity in practical data analytics.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a well-defined, scientifically sound computational task based on established principles of data analysis. I will proceed with the solution.\n\n###\nThe problem requires the calculation of the Silhouette Coefficient (SC) for several clustering results on different datasets, each subjected to a specific preprocessing pipeline. The solution involves a systematic application of data transformation followed by the computation of the SC according to its formal definition.\n\nThe core of the problem is the Silhouette Coefficient, a metric that quantifies how well-defined the clusters are. The per-sample silhouette score, $s(i)$, is defined as:\n$$\ns(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}\n$$\nwhere:\n- $a(i)$ is the mean distance from sample $i$ to all other samples in the same cluster. This measures the cohesion of the sample with its own cluster. A small value is desirable.\n- $b(i)$ is the mean distance from sample $i$ to all samples in the nearest neighboring cluster. This measures the separation of the sample from other clusters. A large value is desirable.\n\nThe value of $s(i)$ ranges from $-1$ to $1$. A value near $1$ indicates that the sample is well-clustered, as its intra-cluster distance is much smaller than its nearest inter-cluster distance. A value near $0$ indicates that the sample lies on or very close to the decision boundary between two clusters. A value near $-1$ indicates that the sample is likely misclassified. The overall SC is the average of $s(i)$ over all samples, providing a global measure of clustering quality.\n\nThe problem specifies three preprocessing pipelines to be applied to the data before distance calculations:\n1.  **Z-score Normalization (Standardization)**: This transforms each feature to have a mean of $0$ and a standard deviation of $1$. For a feature vector $X_j = [x_{1j}, \\dots, x_{nj}]^T$, the transformation is:\n    $$ x'_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j} $$\n    where $\\mu_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij}$ and $\\sigma_j = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\mu_j)^2}$. If $\\sigma_j = 0$, the transformed feature $X'_j$ becomes a zero vector. This method is effective when features have different scales and the data is approximately normally distributed.\n\n2.  **Min-Max Scaling (Normalization)**: This scales each feature to a fixed range, typically $[0, 1]$. For a feature vector $X_j$, the transformation is:\n    $$ x'_{ij} = \\frac{x_{ij} - \\min(X_j)}{\\max(X_j) - \\min(X_j)} $$\n    If the range $\\max(X_j) - \\min(X_j) = 0$, the transformed feature $X'_j$ becomes a zero vector. This method is sensitive to outliers but preserves the shape of the original distribution.\n\n3.  **Variance Stabilization**: This pipeline is designed for count data, which often exhibits a mean-variance relationship (e.g., in Poisson-distributed data, variance equals the mean). The transformation $x \\to \\log(1+x)$ helps to stabilize the variance, making the data more amenable to methods like z-score normalization that assume homoscedasticity. It is followed by z-score normalization as described above.\n\nThe distance metric is fixed as the Euclidean distance, calculated on the preprocessed data $X'$:\n$$ d(x'_i, x'_j) = \\sqrt{\\sum_{k=1}^d (x'_{ik} - x'_{jk})^2} $$\n\nThe overall algorithm proceeds as follows for each test case:\n1.  Apply the specified preprocessing pipeline to the input data matrix $X$ to obtain the transformed matrix $X'$.\n2.  Determine the number of unique clusters, $K$. If $K \\le 1$, the SC is $0$ by definition.\n3.  Compute the pairwise Euclidean distance matrix for all samples in $X'$.\n4.  For each sample $i$:\n    a. If sample $i$ is in a singleton cluster, its score $s(i)$ is $0$.\n    b. Otherwise, calculate $a(i)$, the mean distance to other points in its cluster.\n    c. Calculate $b(i)$, the minimum of the mean distances to points in each of the other clusters.\n    d. Calculate $s(i) = (b(i) - a(i))/\\max\\{a(i), b(i)\\}$. If the denominator is $0$, $s(i)$ is $0$.\n5.  The final SC is the arithmetic mean of all per-sample scores $s(i)$.\n\nThis structured approach ensures that each test case is evaluated correctly according to the specified rules, preprocessing, and mathematical definitions.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing the Silhouette Coefficient (SC)\n    for six test cases, each with a specified preprocessing pipeline.\n    \"\"\"\n\n    def preprocess_data(X, method):\n        \"\"\"\n        Applies a specified preprocessing pipeline to the data matrix X.\n\n        Args:\n            X (np.ndarray): The input data matrix (n_samples, n_features).\n            method (str): The preprocessing method ('zscore', 'minmax', 'log_zscore', 'none').\n\n        Returns:\n            np.ndarray: The preprocessed data matrix.\n        \"\"\"\n        if method == 'none':\n            return X.copy()\n        \n        X_proc = X.copy().astype(float)\n        \n        if method == 'log_zscore':\n            X_proc = np.log1p(X_proc)\n            # Fall through to 'zscore'\n            method = 'zscore'\n\n        if method == 'zscore':\n            mean = np.mean(X_proc, axis=0)\n            std = np.std(X_proc, axis=0)\n            # Avoid division by zero for zero-variance features\n            # np.divide handles this with a 'where' argument\n            return np.divide(X_proc - mean, std, out=np.zeros_like(X_proc), where=std!=0)\n        \n        if method == 'minmax':\n            min_val = np.min(X_proc, axis=0)\n            max_val = np.max(X_proc, axis=0)\n            data_range = max_val - min_val\n            # Avoid division by zero for zero-range features\n            return np.divide(X_proc - min_val, data_range, out=np.zeros_like(X_proc), where=data_range!=0)\n            \n        return X_proc\n\n    def calculate_silhouette_score(X, labels):\n        \"\"\"\n        Computes the overall Silhouette Coefficient for a given dataset and labels.\n\n        Args:\n            X (np.ndarray): The data matrix (n_samples, n_features), potentially preprocessed.\n            labels (np.ndarray): The cluster labels for each sample.\n\n        Returns:\n            float: The mean Silhouette Coefficient for all samples.\n        \"\"\"\n        unique_labels = np.unique(labels)\n        n_clusters = len(unique_labels)\n        n_samples = X.shape[0]\n\n        if n_clusters = 1 or n_samples  2:\n            return 0.0\n\n        # Pre-compute the pairwise Euclidean distance matrix\n        dist_matrix = squareform(pdist(X, metric='euclidean'))\n\n        sample_scores = np.zeros(n_samples)\n\n        for i in range(n_samples):\n            current_label = labels[i]\n            \n            # Mask for samples in the same cluster as sample i (excluding i)\n            in_cluster_mask = (labels == current_label)\n            in_cluster_mask[i] = False\n            \n            n_in_cluster = np.sum(in_cluster_mask)\n            \n            # If singleton cluster, silhouette score is 0 by convention\n            if n_in_cluster == 0:\n                sample_scores[i] = 0.0\n                continue\n\n            # Calculate a(i): average intra-cluster distance\n            a_i = np.mean(dist_matrix[i, in_cluster_mask])\n            \n            # Calculate b(i): minimum average inter-cluster distance\n            b_i = np.inf\n            for label in unique_labels:\n                if label == current_label:\n                    continue\n                \n                # Mask for samples in the other cluster\n                out_cluster_mask = (labels == label)\n                mean_dist_to_other_cluster = np.mean(dist_matrix[i, out_cluster_mask])\n                b_i = min(b_i, mean_dist_to_other_cluster)\n            \n            # Calculate the silhouette score for sample i\n            denominator = max(a_i, b_i)\n            if denominator == 0:\n                sample_scores[i] = 0.0 # Handle case for numerical stability\n            else:\n                sample_scores[i] = (b_i - a_i) / denominator\n        \n        return np.mean(sample_scores)\n\n    # Test case definitions\n    test_cases = [\n        {\n            \"data\": np.array([\n                [100.0, 1.2, 5000.0], [110.0, 0.8, 5100.0], [95.0, 1.0, 4900.0],\n                [5.0, 50.0, 180.0], [7.0, 49.5, 200.0], [6.0, 48.0, 220.0]\n            ]),\n            \"labels\": np.array([0, 0, 0, 1, 1, 1]),\n            \"preprocess\": \"zscore\"\n        },\n        {\n            \"data\": np.array([\n                [100.0, 1.2, 5000.0], [110.0, 0.8, 5100.0], [95.0, 1.0, 4900.0],\n                [5.0, 50.0, 180.0], [7.0, 49.5, 200.0], [6.0, 48.0, 220.0]\n            ]),\n            \"labels\": np.array([0, 0, 0, 1, 1, 1]),\n            \"preprocess\": \"none\"\n        },\n        {\n            \"data\": np.array([\n                [1.0, 100.0], [1.1, 98.0], [10.0, 5.0], [9.5, 6.0], [100.0, 1000.0]\n            ]),\n            \"labels\": np.array([0, 0, 1, 1, 2]),\n            \"preprocess\": \"minmax\"\n        },\n        {\n            \"data\": np.array([\n                [0, 10, 500, 2000], [1, 12, 520, 2100], [0, 9, 480, 1950],\n                [50, 0, 30, 0], [45, 1, 28, 2], [55, 2, 35, 1],\n                [0, 300, 0, 100], [1, 290, 0, 90], [2, 310, 1, 110]\n            ]),\n            \"labels\": np.array([0, 0, 0, 1, 1, 1, 2, 2, 2]),\n            \"preprocess\": \"none\"\n        },\n        {\n            \"data\": np.array([\n                [0, 10, 500, 2000], [1, 12, 520, 2100], [0, 9, 480, 1950],\n                [50, 0, 30, 0], [45, 1, 28, 2], [55, 2, 35, 1],\n                [0, 300, 0, 100], [1, 290, 0, 90], [2, 310, 1, 110]\n            ]),\n            \"labels\": np.array([0, 0, 0, 1, 1, 1, 2, 2, 2]),\n            \"preprocess\": \"log_zscore\"\n        },\n        {\n            \"data\": np.array([\n                [1.0, 2.0, 10.0], [1.2, 1.8, 10.0], [0.8, 2.1, 10.0],\n                [5.0, 6.0, 10.0], [5.2, 6.1, 10.0], [4.8, 5.9, 10.0]\n            ]),\n            \"labels\": np.array([0, 0, 0, 1, 1, 1]),\n            \"preprocess\": \"zscore\"\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case[\"data\"]\n        labels = case[\"labels\"]\n        preprocess_method = case[\"preprocess\"]\n\n        X_processed = preprocess_data(X, preprocess_method)\n        sc = calculate_silhouette_score(X_processed, labels)\n        results.append(sc)\n\n    # Format the output as specified\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4561589"}, {"introduction": "While Euclidean distance is intuitive, it often fails to capture the underlying structure of complex biological data, which may be better described as a non-linear manifold. This exercise extends the concept of the Silhouette Coefficient to a more sophisticated distance measure: graph geodesic distance. You will construct a $k$-nearest neighbor graph, compute shortest path distances, and then re-evaluate clustering, allowing you to explore how the notion of 'distance' itself can be adapted and how sensitive the evaluation is to key graph construction parameters like $k$ [@problem_id:4561549].", "problem": "A bioinformatics team is evaluating the stability of cluster assignments for cell populations in a low-dimensional embedding derived from gene expression data. They use graph-based geodesic distances, where the graph is the symmetric $k$-nearest neighbor graph with edge weights equal to Euclidean distances between samples in the embedding. They seek to quantify how sensitive the mean silhouette statistic is to the choice of the $k$-nearest neighbor parameter used to compute graph distances.\n\nGiven a fixed dataset of $n$ two-dimensional points $x_i \\in \\mathbb{R}^2$ (representing low-dimensional embeddings of gene expression profiles), a cluster assignment vector, and a choice of $k$, construct the symmetric $k$-nearest neighbor graph $G_k$ as follows:\n- For each node $i$, compute the Euclidean distances to all other nodes $j \\neq i$.\n- Connect $i$ to its $k$ nearest neighbors; symmetrize by taking the undirected union of these directed edges.\n- Edge weights are the Euclidean distances. Let $d_{ij}$ denote the weight of the edge between $i$ and $j$ if it exists, and $+\\infty$ otherwise.\n- Define the graph geodesic distance between nodes $i$ and $j$ as the length of the shortest path in $G_k$ using the edge weights $d_{uv}$, with $d_{ii} = 0$ for all $i$.\n\nDefine the per-sample silhouette value using the canonical definition from clustering evaluation literature that compares, for each sample $i$, the average graph distance to its own cluster versus the minimum over other clusters of the average graph distance to those clusters, normalized by the larger of these two averages. The mean silhouette is the arithmetic mean of all per-sample silhouette values.\n\nConventions to ensure well-defined behavior on graphs that may be disconnected:\n- If a sample $i$ is the only member of its assigned cluster (a singleton), define its silhouette value to be $0$.\n- When computing averages within or across clusters, use only finite graph distances (i.e., nodes reachable by some path). If $i$ has no finite-distance neighbors in its own cluster, or has no finite-distance neighbors in any other cluster, define its silhouette value to be $0$.\n\nYou must implement the entire computation and apply it to the test suite below. All quantities are unitless. Your program must produce a single line of output containing a comma-separated list of the mean silhouette values for each test case, rounded to exactly six decimal places, enclosed in square brackets.\n\nDataset:\n- Points $x_i = (x_i^{(1)}, x_i^{(2)})$ for $i \\in \\{0,1,\\dots,7\\}$:\n  - $x_0 = (0, 0)$\n  - $x_1 = (1, 0)$\n  - $x_2 = (2, 0)$\n  - $x_3 = (10, 0)$\n  - $x_4 = (11, 0)$\n  - $x_5 = (12, 0)$\n  - $x_6 = (25, 0)$\n  - $x_7 = (26, 0)$\n\nTest suite of five cases. In each case, $k$ is the $k$-nearest neighbor parameter, and labels is the cluster assignment vector $c \\in \\mathbb{Z}^n$ with $c_i$ the cluster label of point $i$:\n- Case $1$: $k = 3$, labels $= [0,0,0,1,1,1,2,2]$.\n- Case $2$: $k = 1$, labels $= [0,0,0,1,1,1,2,2]$.\n- Case $3$: $k = 7$, labels $= [0,0,0,1,1,1,2,2]$.\n- Case $4$: $k = 3$, labels $= [0,0,0,1,1,1,1,1]$.\n- Case $5$: $k = 3$, labels $= [0,0,0,1,1,1,1,2]$.\n\nRequirements:\n- Compute the symmetric $k$-nearest neighbor graph $G_k$ as specified, using Euclidean edge weights.\n- Compute all-pairs graph geodesic distances via shortest paths on $G_k$.\n- Compute the mean silhouette for each test case under the conventions above.\n- Final output format: a single line containing a list of five floating-point numbers, each rounded to exactly six decimal places, in the order of cases $1$ through $5$, printed as a comma-separated list enclosed in square brackets (for example, $[a,b,c,d,e]$ with each of $a,b,c,d,e$ having six digits after the decimal point).", "solution": "The problem requires the computation of the mean silhouette statistic for several clustering scenarios on a given dataset. The core of the problem lies in using a non-standard distance metric: the graph geodesic distance derived from a symmetric $k$-nearest neighbor ($k$-NN) graph. The solution involves a multi-step, principle-based process that combines concepts from graph theory, matrix computations, and clustering evaluation.\n\nThe overall algorithmic approach is as follows:\n1.  For each test case, defined by a parameter $k$ and a cluster assignment vector, construct the corresponding symmetric $k$-NN graph.\n2.  Compute all-pairs shortest paths on this graph to determine the geodesic distance matrix.\n3.  Using this distance matrix, calculate the per-sample silhouette value for each data point according to the canonical definition, carefully adhering to the specified conventions for handling disconnected components and singleton clusters.\n4.  Finally, compute the arithmetic mean of all per-sample silhouette values to obtain the mean silhouette score for the test case.\n\nThe detailed, step-by-step implementation of this approach is founded on the following principles:\n\nFirst, we construct the weighted, undirected graph $G_k = (V, E_k)$, where the vertex set $V$ corresponds to the $n$ data samples. The edge set $E_k$ and associated weights are determined in three stages.\na. We begin by computing the all-pairs Euclidean distance matrix, let's call it $D_{Euc}$, where $D_{Euc}[i, j]$ is the Euclidean distance between points $x_i$ and $x_j$ in $\\mathbb{R}^2$. This matrix forms the basis for all subsequent distance-based operations. For any two points $x_i = (x_i^{(1)}, x_i^{(2)})$ and $x_j = (x_j^{(1)}, x_j^{(2)})$, the distance is $D_{Euc}[i,j] = \\sqrt{(x_i^{(1)} - x_j^{(1)})^2 + (x_i^{(2)} - x_j^{(2)})^2}$.\nb. Next, for each point $i$, we identify its set of $k$ nearest neighbors, $N_k(i)$, by finding the $k$ points $j \\ne i$ with the smallest values in the $i$-th row of $D_{Euc}$. This defines a directed $k$-NN graph.\nc. The graph is then symmetrized by taking the \"undirected union\" of edges. An undirected edge $(i, j)$ exists in $G_k$ if and only if $j \\in N_k(i)$ or $i \\in N_k(j)$. The weight of this edge is its Euclidean distance, $D_{Euc}[i, j]$. This process generates a weighted adjacency matrix, $W$, where $W_{ij} = D_{Euc}[i, j]$ if an edge $(i, j)$ exists, and $W_{ij} = +\\infty$ otherwise. By definition, $W_{ii} = 0$.\n\nSecond, we compute the graph geodesic distance matrix, $D_{geo}$. The value $D_{geo}[i, j]$ is the length of the shortest path between nodes $i$ and $j$ in the graph $G_k$. For a dense graph representation (the adjacency matrix $W$) and a small number of nodes ($n=8$), the Floyd-Warshall algorithm is the most suitable method for this all-pairs shortest path problem. This algorithm iteratively considers each node as a potential intermediate point in paths between all other pairs of nodes, systematically relaxing path lengths until the shortest paths are found. The output is a matrix $D_{geo}$ where an entry is $+\\infty$ if no path exists between the corresponding nodes.\n\nThird, we calculate the silhouette score for each sample $i$. This requires computing two quantities, $a(i)$ and $b(i)$, using the geodesic distances in $D_{geo}$. Let $C(i)$ denote the cluster to which sample $i$ is assigned.\n-   $a(i)$: The average intra-cluster distance. It is the mean of $D_{geo}[i,j]$ for all other samples $j$ in the same cluster, $j \\in C(i), j \\ne i$. Only finite distances are included in the average.\n-   $b(i)$: The minimum average inter-cluster distance. For each other cluster $C_m$ where $m \\ne C(i)$, we calculate the average distance from $i$ to all samples $j \\in C_m$, again using only finite distances. $b(i)$ is the minimum of these average values over all other clusters $C_m$.\n\nThe problem specifies crucial conventions for ill-defined situations:\n-   If sample $i$ is a singleton (the only member of its cluster), its silhouette value $s(i)$ is $0$. This is a base case.\n-   If sample $i$ has no finite-distance paths to any other member of its own cluster, $a(i)$ is undefined. The silhouette value $s(i)$ is then set to $0$.\n-   If sample $i$ has no finite-distance paths to any sample in any other cluster, $b(i)$ is undefined. The silhouette value $s(i)$ is then set to $0$.\n\nOnce well-defined values for $a(i)$ and $b(i)$ are obtained, the silhouette value for sample $i$ is calculated using the canonical formula:\n$$ s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}} $$\nIf $a(i)=b(i)=0$, this results in a division by zero; however, given the problem's data and structure, this case does not arise for non-zero silhouette values. The zero-value conventions handle all degenerate scenarios.\n\nFinally, the mean silhouette score for the entire dataset is simply the arithmetic mean of the individual $s(i)$ values over all $n$ samples. This procedure is repeated for each of the five test cases provided.", "answer": "```python\nimport numpy as np\nfrom scipy.sparse.csgraph import floyd_warshall\nfrom scipy.spatial.distance import cdist\n\ndef calculate_mean_silhouette(points, labels, k):\n    \"\"\"\n    Computes the mean silhouette score based on graph geodesic distances.\n\n    Args:\n        points (np.ndarray): An (n, d) array of n points in d dimensions.\n        labels (np.ndarray): An (n,) array of cluster labels.\n        k (int): The number of nearest neighbors for graph construction.\n\n    Returns:\n        float: The mean silhouette score.\n    \"\"\"\n    n = points.shape[0]\n    \n    # Step A: Construct the symmetric k-NN graph\n    # 1. Compute Euclidean distance matrix\n    euc_dist = cdist(points, points, 'euclidean')\n    \n    # 2. Build weighted adjacency matrix W for the symmetric k-NN graph\n    adj_matrix = np.full((n, n), np.inf)\n    np.fill_diagonal(adj_matrix, 0)\n    \n    # Find k-nearest neighbors for each point\n    # argsort returns indices, [:, 1:k+1] skips self (dist=0) and takes k neighbors\n    # A stable sort tie-breaking rule (e.g., lower index first) is implicitly\n    # handled by how np.argsort is implemented, which is sufficient here.\n    neighbor_indices = np.argsort(euc_dist, axis=1)[:, 1:k+1]\n    \n    for i in range(n):\n        for j_idx in neighbor_indices[i]:\n            # Symmetrize by taking the union of directed edges\n            if adj_matrix[i, j_idx] == np.inf:\n                adj_matrix[i, j_idx] = euc_dist[i, j_idx]\n            if adj_matrix[j_idx, i] == np.inf:\n                adj_matrix[j_idx, i] = euc_dist[j_idx, i]\n\n    # Step B: Compute all-pairs geodesic distances using Floyd-Warshall\n    geo_dist = floyd_warshall(csgraph=adj_matrix, directed=False)\n    \n    # Step C: Compute per-sample silhouette values\n    silhouette_values = np.zeros(n)\n    unique_labels = np.unique(labels)\n    \n    for i in range(n):\n        label_i = labels[i]\n        \n        # Identify points in the same cluster and other clusters\n        in_cluster_mask = (labels == label_i)\n        in_cluster_mask[i] = False  # Exclude the point itself\n        \n        # Handle singleton clusters\n        if not np.any(in_cluster_mask):\n            silhouette_values[i] = 0.0\n            continue\n            \n        # Calculate a(i): average intra-cluster distance\n        a_dists = geo_dist[i, in_cluster_mask]\n        a_dists_finite = a_dists[np.isfinite(a_dists)]\n        \n        # Handle disconnection within the cluster\n        if len(a_dists_finite) == 0:\n            silhouette_values[i] = 0.0\n            continue\n        \n        a_i = np.mean(a_dists_finite)\n        \n        # Calculate b(i): minimum average inter-cluster distance\n        other_cluster_means = []\n        other_labels = [ul for ul in unique_labels if ul != label_i]\n        \n        for other_label in other_labels:\n            other_cluster_mask = (labels == other_label)\n            b_dists = geo_dist[i, other_cluster_mask]\n            b_dists_finite = b_dists[np.isfinite(b_dists)]\n            \n            if len(b_dists_finite)  0:\n                other_cluster_means.append(np.mean(b_dists_finite))\n        \n        # Handle disconnection from all other clusters\n        if len(other_cluster_means) == 0:\n            silhouette_values[i] = 0.0\n            continue\n            \n        b_i = np.min(other_cluster_means)\n        \n        # Calculate silhouette value s(i)\n        numerator = b_i - a_i\n        denominator = max(a_i, b_i)\n        \n        silhouette_values[i] = numerator / denominator if denominator  0 else 0.0\n        \n    # Step D: Compute mean silhouette score\n    return np.mean(silhouette_values)\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Dataset definition\n    points = np.array([\n        [0, 0], [1, 0], [2, 0],\n        [10, 0], [11, 0], [12, 0],\n        [25, 0], [26, 0]\n    ], dtype=float)\n\n    # Test suite definition\n    test_cases = [\n        {'k': 3, 'labels': np.array([0, 0, 0, 1, 1, 1, 2, 2])},\n        {'k': 1, 'labels': np.array([0, 0, 0, 1, 1, 1, 2, 2])},\n        {'k': 7, 'labels': np.array([0, 0, 0, 1, 1, 1, 2, 2])},\n        {'k': 3, 'labels': np.array([0, 0, 0, 1, 1, 1, 1, 1])},\n        {'k': 3, 'labels': np.array([0, 0, 0, 1, 1, 1, 1, 2])}\n    ]\n\n    results = []\n    for case in test_cases:\n        mean_silhouette = calculate_mean_silhouette(points, case['labels'], case['k'])\n        results.append(mean_silhouette)\n    \n    # Format and print the final output\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4561549"}, {"introduction": "In the era of large-scale biomedical data, algorithmic efficiency is as important as theoretical correctness. The exact calculation of metrics like the Silhouette Coefficient, with its quadratic complexity in the number of samples ($O(n^2)$), is often computationally infeasible for datasets containing millions of cells or patients. This final practice directly addresses this real-world challenge by tasking you with implementing both the exact algorithm and a deterministic, sampling-based approximation. This comparison will provide concrete insights into the trade-offs between computational cost and evaluation accuracy, a crucial consideration for any practicing bioinformatician [@problem_id:4561574].", "problem": "A clinical bioinformatics team evaluates the quality of unsupervised cluster assignments on a small panel of gene expression profiles by comparing intra-cluster and inter-cluster dissimilarities. Consider a finite set of data points embedded in a real vector space of dimension $d$ with a specified metric that is the standard Euclidean distance on $\\mathbb{R}^d$. The dataset is partitioned into labeled clusters. The evaluation metric for a point should be based on the mean dissimilarity to points in its own cluster and the smallest mean dissimilarity to any other cluster, normalized by the larger of those two means, and then averaged across all points to yield a single dataset score. If a point is the only member of its cluster, the point-level score is defined to be $0$. If both the mean intra-cluster dissimilarity and the smallest mean inter-cluster dissimilarity are $0$ for a point, its point-level score is defined to be $0$.\n\nYou must implement two algorithms that compute dataset-level scores under this definition:\n\n- An exact algorithm that precomputes all pairwise distances once and then computes the dataset-level score. For the exact algorithm, define the distance evaluation count to be the number of unique unordered point pairs whose distances are evaluated. If there are $n$ points, this count must be $n(n-1)/2$.\n\n- A deterministic approximation algorithm with a sampling parameter $m \\in \\mathbb{N}$. For each point $i$, approximate the mean intra-cluster dissimilarity using up to $m$ other points from its own cluster (if fewer are available, use all available). The selected points for approximation must be the cluster members with the smallest indices, excluding $i$ itself where applicable. To approximate the smallest mean inter-cluster dissimilarity, for each other cluster compute the mean dissimilarity to up to $m$ points from that cluster chosen as the smallest-indexed members of that cluster, and take the minimum of these means over clusters. The approximation must be deterministic, with no randomness. For this approximation algorithm, define the distance evaluation count as the total number of directed point-to-point distance computations performed across all points, equal to the sum across all points of the number of selected intra-cluster partners plus the sum over all other clusters of the number of selected inter-cluster partners.\n\nYour program must accept no input and must instead compute the results for the following test suite. Each test case specifies a data matrix $X \\in \\mathbb{R}^{n \\times d}$, an integer label vector $y \\in \\mathbb{Z}^n$ indicating cluster membership, and the sampling parameter $m$:\n\nTest case $1$ (well-separated balanced clusters, $m=2$):\n$$\nX = \\begin{bmatrix}\n0  0\\\\\n0  1\\\\\n1  0\\\\\n1  1\\\\\n5  5\\\\\n5  6\\\\\n6  5\\\\\n6  6\n\\end{bmatrix},\\quad\ny = [0,0,0,0,1,1,1,1],\\quad\nm = 2.\n$$\n\nTest case $2$ (singleton cluster, $m=2$):\n$$\nX = \\begin{bmatrix}\n0  0\\\\\n0  0.1\\\\\n10  10\n\\end{bmatrix},\\quad\ny = [0,0,1],\\quad\nm = 2.\n$$\n\nTest case $3$ (imbalanced clusters, $m=1$):\n$$\nX = \\begin{bmatrix}\n0  0\\\\\n0  1\\\\\n1  0\\\\\n3  0\\\\\n3  1\\\\\n4  0\\\\\n8  0\\\\\n8  1\n\\end{bmatrix},\\quad\ny = [0,0,0,1,1,1,2,2],\\quad\nm = 1.\n$$\n\nTest case $4$ (zero intra-cluster dissimilarity pairs, $m=3$):\n$$\nX = \\begin{bmatrix}\n0  0\\\\\n0  0\\\\\n10  0\\\\\n10  0\n\\end{bmatrix},\\quad\ny = [0,0,1,1],\\quad\nm = 3.\n$$\n\nFor each test case, compute:\n\n- the exact dataset-level score using the exact algorithm described,\n- the approximate dataset-level score using the deterministic sampling algorithm with the given $m$,\n- the exact algorithm’s unique unordered pair distance evaluation count $n(n-1)/2$,\n- the approximate algorithm’s directed distance evaluation count as defined above.\n\nReturn results rounded to $6$ decimal places for the dataset-level scores. All counts must be integers. There are no physical units in this problem.\n\nYour program should produce a single line of output containing a list of results, one per test case, where each result is a list in the form $[\\text{exact\\_score}, \\text{approx\\_score}, \\text{exact\\_count}, \\text{approx\\_count}]$. The line must contain the results as a comma-separated list enclosed in square brackets with no spaces, for example:\n$[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3],[a_4,b_4,c_4,d_4]]$.\n\nConstraints and assumptions:\n\n- All distances are Euclidean on $\\mathbb{R}^d$.\n- If a point has no other points in its own cluster, its point-level score is $0$ by definition and does not contribute any intra-cluster distance computations in the approximation algorithm.\n- If the maximum of the two means used for normalization for a point equals $0$, define that point’s score to be $0$ to avoid division by zero.", "solution": "The problem requires the implementation of two algorithms to evaluate the quality of a given data clustering. The evaluation is based on a metric that, for each data point, compares its average distance to other points in the same cluster against its average distance to points in other clusters. This is a common method in unsupervised learning to assess cluster cohesion (how close points within a cluster are) and separation (how far apart different clusters are). The specified point-level score is a variant of the well-known silhouette coefficient.\n\nLet the dataset be a set of $n$ points $\\{p_0, p_1, \\dots, p_{n-1}\\}$ in a $d$-dimensional Euclidean space $\\mathbb{R}^d$. The points are partitioned into a set of clusters $C = \\{C_1, C_2, \\dots, C_K\\}$. The distance between two points $p_i$ and $p_j$ is the Euclidean distance, denoted by $d(p_i, p_j) = \\|p_i - p_j\\|_2$.\n\nFor each point $p_i$, the problem defines two key quantities:\n1.  The mean intra-cluster dissimilarity, $a(i)$, which is the average distance from $p_i$ to all other points within the same cluster.\n2.  The smallest mean inter-cluster dissimilarity, $b(i)$, which is the minimum of the average distances from $p_i$ to all points in any single other cluster.\n\nThe point-level score, $s(i)$, is then defined as:\n$$s(i) = \\begin{cases}\n    \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}  \\text{if } \\max\\{a(i), b(i)\\}  0 \\\\\n    0  \\text{if } \\max\\{a(i), b(i)\\} = 0\n\\end{cases}$$\nA special condition is given for a point $p_i$ that is the sole member of its cluster (a singleton cluster): its score $s(i)$ is defined to be $0$.\n\nThe overall dataset score is the arithmetic mean of the point-level scores for all points in the dataset:\n$$\\text{Dataset Score} = \\frac{1}{n} \\sum_{i=0}^{n-1} s(i)$$\n\nWe will now detail the two required algorithms for computing this score.\n\n### Exact Algorithm\n\nThe exact algorithm computes the score precisely according to the definitions. It follows a two-stage process: precomputation of distances and then calculation of scores.\n\n1.  **Distance Precomputation**: All unique pairwise distances between the $n$ points are computed exactly once. For $n$ points, there are $\\binom{n}{2} = \\frac{n(n-1)}{2}$ such unique unordered pairs. These distances are stored in a symmetric $n \\times n$ distance matrix $D$, where $D_{ij} = d(p_i, p_j)$. The distance evaluation count for this algorithm is fixed at this value: $\\text{exact\\_count} = \\frac{n(n-1)}{2}$.\n\n2.  **Score Calculation**: For each point $p_i$ belonging to cluster $C_k$:\n    *   If $C_k$ is a singleton cluster (i.e., $|C_k| = 1$), then $s(i) = 0$.\n    *   Otherwise, the intra-cluster dissimilarity $a(i)$ is calculated as the mean of distances to all other points in $C_k$:\n        $$a(i) = \\frac{1}{|C_k|-1} \\sum_{p_j \\in C_k, j \\neq i} D_{ij}$$\n    *   The mean dissimilarity to every other cluster $C_l$ (where $l \\neq k$) is computed:\n        $$d(p_i, C_l) = \\frac{1}{|C_l|} \\sum_{p_j \\in C_l} D_{ij}$$\n    *   The smallest mean inter-cluster dissimilarity $b(i)$ is the minimum of these values:\n        $$b(i) = \\min_{l \\neq k} \\{d(p_i, C_l)\\}$$\n    *   The point-level score $s(i)$ is then computed using the formula provided, and the final dataset score is the average of all $s(i)$.\n\n### Deterministic Approximation Algorithm\n\nThis algorithm approximates the score by using a fixed-size sample of points for distance calculations, governed by a parameter $m \\in \\mathbb{N}$. The sampling is deterministic, based on the indices of the points.\n\nFor each point $p_i$ with label $y_i$ and index $i$:\n\n1.  **Approximate Intra-Cluster Dissimilarity $\\tilde{a}(i)$**:\n    *   If $p_i$ is in a singleton cluster, its score $\\tilde{s}(i)$ is $0$, and no intra-cluster distance computations are performed for it.\n    *   Otherwise, identify the set of indices of other points in its cluster, $I = \\{j \\mid y_j = y_i, j \\neq i\\}$. From this set, select up to $m$ indices with the smallest values. Let this sampled index set be $I' \\subseteq I$, where $|I'| = \\min(m, |I|)$.\n    *   The approximate intra-cluster dissimilarity is the mean distance to these sampled points:\n        $$\\tilde{a}(i) = \\frac{1}{|I'|} \\sum_{j \\in I'} d(p_i, p_j)$$\n    *   The number of distance computations for this step is $|I'|$.\n\n2.  **Approximate Inter-Cluster Dissimilarity $\\tilde{b}(i)$**:\n    *   For each other cluster $C_l$ (where its label is not $y_i$), identify the set of indices of its members, $J_l = \\{j \\mid y_j = l\\}$. From this set, select up to $m$ indices with the smallest values. Let this sampled index set be $J'_l \\subseteq J_l$, where $|J'_l| = \\min(m, |J_l|)$.\n    *   The approximate mean distance to cluster $C_l$ is:\n        $$\\tilde{d}(p_i, C_l) = \\frac{1}{|J'_l|} \\sum_{j \\in J'_l} d(p_i, p_j)$$\n    *   The number of distance computations for this cluster is $|J'_l|$.\n    *   The approximate smallest mean inter-cluster dissimilarity $\\tilde{b}(i)$ is the minimum of these mean distances over all other clusters $l$:\n        $$\\tilde{b}(i) = \\min_{l \\neq y_i} \\{\\tilde{d}(p_i, C_l)\\}$$\n\n3.  **Score and Count Calculation**:\n    *   The approximate point-level score $\\tilde{s}(i)$ is computed using $\\tilde{a}(i)$ and $\\tilde{b}(i)$ with the same formula as the exact method.\n    *   The total distance evaluation count, `approx_count`, is the sum of all distance computations performed across all points. For a single point $p_i$, the count increases by its number of intra-cluster samples plus the sum of inter-cluster samples from all other clusters.\n\nThe final approximate dataset score is the average of all $\\tilde{s}(i)$. This method reduces computational cost by limiting the number of distance calculations, especially for large clusters.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef compute_exact(X, y):\n    \"\"\"\n    Computes the exact dataset-level score and distance evaluation count.\n    \"\"\"\n    n = X.shape[0]\n    if n = 1:\n        return 0.0, 0\n    \n    # Precompute all pairwise distances. pdist computes n(n-1)/2 distances.\n    # This corresponds exactly to the definition of exact_count.\n    exact_count = n * (n - 1) // 2\n    dist_matrix = squareform(pdist(X, 'euclidean'))\n    \n    unique_labels = np.unique(y)\n    cluster_indices = {label: np.where(y == label)[0] for label in unique_labels}\n    \n    point_scores = np.zeros(n)\n    \n    for i in range(n):\n        label_i = y[i]\n        indices_in_cluster = cluster_indices[label_i]\n        \n        # Handle singleton cluster\n        if len(indices_in_cluster) == 1:\n            point_scores[i] = 0.0\n            continue\n            \n        # Calculate a(i): mean intra-cluster distance\n        mask_a = np.ones_like(indices_in_cluster, dtype=bool)\n        mask_a[indices_in_cluster == i] = False\n        other_indices_in_cluster = indices_in_cluster[mask_a]\n        a_i = np.mean(dist_matrix[i, other_indices_in_cluster])\n        \n        # Calculate b(i): smallest mean inter-cluster distance\n        mean_inter_dists = []\n        for other_label in unique_labels:\n            if other_label == label_i:\n                continue\n            indices_other_cluster = cluster_indices[other_label]\n            mean_dist = np.mean(dist_matrix[i, indices_other_cluster])\n            mean_inter_dists.append(mean_dist)\n            \n        b_i = np.min(mean_inter_dists)\n        \n        # Calculate point score s(i)\n        denominator = max(a_i, b_i)\n        if denominator == 0:\n            point_scores[i] = 0.0\n        else:\n            point_scores[i] = (b_i - a_i) / denominator\n            \n    exact_score = np.mean(point_scores)\n    return exact_score, exact_count\n\ndef compute_approx(X, y, m):\n    \"\"\"\n    Computes the approximate dataset-level score and distance evaluation count.\n    \"\"\"\n    n = X.shape[0]\n    if n = 1:\n        return 0.0, 0\n\n    approx_count = 0\n    point_scores = np.zeros(n)\n    \n    # Pre-sort indices for each cluster label for deterministic sampling\n    unique_labels = np.unique(y)\n    cluster_indices = {label: np.sort(np.where(y == label)[0]) for label in unique_labels}\n    \n    for i in range(n):\n        label_i = y[i]\n        indices_in_cluster = cluster_indices[label_i]\n        p_i = X[i]\n\n        # Handle singleton cluster\n        if len(indices_in_cluster) == 1:\n            point_scores[i] = 0.0\n            # Still need to calculate inter-cluster distances for the count\n            for other_label in unique_labels:\n                if other_label == label_i:\n                    continue\n                indices_other_cluster = cluster_indices[other_label]\n                num_samples = min(m, len(indices_other_cluster))\n                approx_count += num_samples\n            continue\n\n        # Calculate a(i): approximate mean intra-cluster distance\n        other_indices_in_cluster = indices_in_cluster[indices_in_cluster != i]\n        num_samples_a = min(m, len(other_indices_in_cluster))\n        sample_indices_a = other_indices_in_cluster[:num_samples_a]\n        \n        dists_a = np.linalg.norm(X[sample_indices_a] - p_i, axis=1)\n        a_i_tilde = np.mean(dists_a)\n        approx_count += num_samples_a\n\n        # Calculate b(i): approximate smallest mean inter-cluster distance\n        mean_inter_dists_tilde = []\n        for other_label in unique_labels:\n            if other_label == label_i:\n                continue\n            indices_other_cluster = cluster_indices[other_label]\n            num_samples_b = min(m, len(indices_other_cluster))\n            sample_indices_b = indices_other_cluster[:num_samples_b]\n            \n            dists_b = np.linalg.norm(X[sample_indices_b] - p_i, axis=1)\n            mean_dist_tilde = np.mean(dists_b)\n            mean_inter_dists_tilde.append(mean_dist_tilde)\n            approx_count += num_samples_b\n        \n        b_i_tilde = np.min(mean_inter_dists_tilde) if mean_inter_dists_tilde else 0\n        \n        # Calculate point score s(i)\n        denominator = max(a_i_tilde, b_i_tilde)\n        if denominator == 0:\n            point_scores[i] = 0.0\n        else:\n            point_scores[i] = (b_i_tilde - a_i_tilde) / denominator\n            \n    approx_score = np.mean(point_scores)\n    return approx_score, approx_count\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (\n            np.array([[0,0], [0,1], [1,0], [1,1], [5,5], [5,6], [6,5], [6,6]], dtype=np.float64),\n            np.array([0,0,0,0,1,1,1,1]),\n            2\n        ),\n        (\n            np.array([[0,0], [0,0.1], [10,10]], dtype=np.float64),\n            np.array([0,0,1]),\n            2\n        ),\n        (\n            np.array([[0,0], [0,1], [1,0], [3,0], [3,1], [4,0], [8,0], [8,1]], dtype=np.float64),\n            np.array([0,0,0,1,1,1,2,2]),\n            1\n        ),\n        (\n            np.array([[0,0], [0,0], [10,0], [10,0]], dtype=np.float64),\n            np.array([0,0,1,1]),\n            3\n        )\n    ]\n\n    all_results = []\n    for X, y, m in test_cases:\n        exact_score, exact_count = compute_exact(X, y)\n        approx_score, approx_count = compute_approx(X, y, m)\n        \n        # Format results as specified\n        result_str = (\n            f\"[{exact_score:.6f},\"\n            f\"{approx_score:.6f},\"\n            f\"{exact_count},\"\n            f\"{approx_count}]\"\n        )\n        all_results.append(result_str)\n\n    # Print the final output in the required single-line format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "4561574"}]}