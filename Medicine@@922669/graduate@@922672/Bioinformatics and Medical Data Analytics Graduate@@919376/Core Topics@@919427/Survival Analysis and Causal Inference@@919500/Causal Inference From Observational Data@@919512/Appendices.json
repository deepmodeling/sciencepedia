{"hands_on_practices": [{"introduction": "A cornerstone of causal inference from observational data is adjusting for confounding variables to isolate the true effect of a treatment or exposure. One of the most powerful and intuitive methods for this is Inverse Probability Weighting (IPW). This technique creates a pseudo-population where the baseline covariates no longer predict treatment assignment, effectively mimicking the balance achieved in a randomized controlled trial. This foundational exercise [@problem_id:4545088] asks you to formally derive the expectation of the IPW estimator, proving that it provides an unbiased estimate of the Average Treatment Effect (ATE) under the ideal condition that the true propensity scores are known.", "problem": "Consider an observational cohort study in bioinformatics and medical data analytics evaluating a binary treatment, denoted by $A \\in \\{0,1\\}$, on a continuous biomarker outcome $Y \\in \\mathbb{R}$ measured post-treatment. Each subject has pre-treatment covariates $X \\in \\mathbb{R}^{p}$, and the treatment assignment mechanism is described by the true propensity score $e(X) = \\Pr(A=1 \\mid X)$. Let the potential outcomes be $Y(1)$ and $Y(0)$, and define the Average Treatment Effect (ATE) as $\\tau = \\mathbb{E}[Y(1) - Y(0)]$. Assume the following standard identification conditions: Stable Unit Treatment Value Assumption (SUTVA), consistency ($Y = A Y(1) + (1-A) Y(0)$), conditional ignorability ($(Y(1), Y(0)) \\perp A \\mid X$), and positivity ($0  e(X)  1$ almost surely). You observe an independent and identically distributed sample $\\{(X_i, A_i, Y_i)\\}_{i=1}^{n}$ from this data-generating process.\n\nSuppose the estimated propensity score $\\hat{e}(X)$ is correctly specified so that $\\hat{e}(X) = e(X)$ almost surely. Define the Inverse Probability Weighting (IPW) estimator for the Average Treatment Effect (ATE) as\n$$\n\\hat{\\tau} \\;=\\; \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{A_i Y_i}{\\hat{e}(X_i)} \\;-\\; \\frac{(1 - A_i) Y_i}{1 - \\hat{e}(X_i)} \\right).\n$$\nDerive the unconditional expectation $\\mathbb{E}[\\hat{\\tau}]$ under the stated conditions and correct specification of $\\hat{e}(X)$. Express your final answer as a single closed-form analytic expression in terms of the potential outcomes. No rounding is required and no physical units are involved.", "solution": "The validity of the problem statement is first assessed based on the provided information.\n\n### Step 1: Extract Givens\n- **Treatment:** A binary variable $A \\in \\{0,1\\}$.\n- **Outcome:** A continuous biomarker $Y \\in \\mathbb{R}$.\n- **Covariates:** A vector of pre-treatment covariates $X \\in \\mathbb{R}^{p}$.\n- **True Propensity Score:** $e(X) = \\Pr(A=1 \\mid X)$.\n- **Potential Outcomes:** $Y(1)$ and $Y(0)$.\n- **Average Treatment Effect (ATE):** $\\tau = \\mathbb{E}[Y(1) - Y(0)]$.\n- **Identification Assumptions:**\n    1.  Stable Unit Treatment Value Assumption (SUTVA).\n    2.  Consistency: $Y = A Y(1) + (1-A) Y(0)$.\n    3.  Conditional Ignorability: $(Y(1), Y(0)) \\perp A \\mid X$.\n    4.  Positivity: $0  e(X)  1$ almost surely.\n- **Data:** An independent and identically distributed (i.i.d.) sample $\\{(X_i, A_i, Y_i)\\}_{i=1}^{n}$.\n- **Estimator Specification:** The estimated propensity score is correctly specified, i.e., $\\hat{e}(X) = e(X)$ almost surely.\n- **Estimator Definition:** The Inverse Probability Weighting (IPW) estimator is $\\hat{\\tau} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{A_i Y_i}{\\hat{e}(X_i)} - \\frac{(1 - A_i) Y_i}{1 - \\hat{e}(X_i)} \\right)$.\n- **Objective:** Derive the unconditional expectation $\\mathbb{E}[\\hat{\\tau}]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, being a standard theoretical question in the field of causal inference, which is a core area of biostatistics and bioinformatics. It is well-posed, providing all necessary assumptions (SUTVA, consistency, ignorability, positivity) and a correctly specified model for the propensity score, which are the sufficient conditions required to derive the expectation of the IPW estimator. The problem is objective, stated in precise mathematical language. It does not violate any fundamental principles, is not incomplete or contradictory, and represents a foundational derivation in the topic area. The condition $\\hat{e}(X) = e(X)$ is a standard simplifying assumption for analyzing the bias of such estimators, representing an idealized but theoretically important scenario.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Derivation of $\\mathbb{E}[\\hat{\\tau}]$\nThe objective is to compute the expectation of the IPW estimator, $\\mathbb{E}[\\hat{\\tau}]$.\nThe estimator is given by:\n$$\n\\hat{\\tau} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{A_i Y_i}{\\hat{e}(X_i)} - \\frac{(1 - A_i) Y_i}{1 - \\hat{e}(X_i)} \\right)\n$$\nGiven the assumption of a correctly specified propensity score model, we have $\\hat{e}(X_i) = e(X_i)$. The estimator can be written as:\n$$\n\\hat{\\tau} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{A_i Y_i}{e(X_i)} - \\frac{(1 - A_i) Y_i}{1 - e(X_i)} \\right)\n$$\nBy linearity of expectation, we have:\n$$\n\\mathbb{E}[\\hat{\\tau}] = \\mathbb{E}\\left[ \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{A_i Y_i}{e(X_i)} - \\frac{(1 - A_i) Y_i}{1 - e(X_i)} \\right) \\right] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}\\left[ \\frac{A_i Y_i}{e(X_i)} - \\frac{(1 - A_i) Y_i}{1 - e(X_i)} \\right]\n$$\nSince the observations are independent and identically distributed, the expectation is the same for all $i$. We can therefore drop the index $i$ and analyze the expectation for a single observation:\n$$\n\\mathbb{E}[\\hat{\\tau}] = \\mathbb{E}\\left[ \\frac{A Y}{e(X)} - \\frac{(1 - A) Y}{1 - e(X)} \\right] = \\mathbb{E}\\left[ \\frac{A Y}{e(X)} \\right] - \\mathbb{E}\\left[ \\frac{(1 - A) Y}{1 - e(X)} \\right]\n$$\nWe will evaluate each of the two terms separately using the law of total expectation, $\\mathbb{E}[Z] = \\mathbb{E}_X[\\mathbb{E}[Z \\mid X]]$.\n\nFor the first term, $\\mathbb{E}\\left[ \\frac{A Y}{e(X)} \\right]$:\n$$\n\\mathbb{E}\\left[ \\frac{A Y}{e(X)} \\right] = \\mathbb{E}_X\\left[ \\mathbb{E}\\left[ \\frac{A Y}{e(X)} \\mid X \\right] \\right]\n$$\nConditioned on $X$, the propensity score $e(X)$ is a fixed value. Thus,\n$$\n\\mathbb{E}\\left[ \\frac{A Y}{e(X)} \\mid X \\right] = \\frac{1}{e(X)} \\mathbb{E}[A Y \\mid X]\n$$\nUsing the consistency assumption, $Y = A Y(1) + (1-A) Y(0)$, we can write $AY = A(A Y(1) + (1-A) Y(0)) = A^2 Y(1) + A(1-A) Y(0)$. Since $A$ is binary, $A^2 = A$ and $A(1-A) = 0$. This simplifies to $AY = AY(1)$.\nSubstituting this into the conditional expectation:\n$$\n\\mathbb{E}[A Y \\mid X] = \\mathbb{E}[A Y(1) \\mid X]\n$$\nNext, we apply the conditional ignorability assumption, $(Y(1), Y(0)) \\perp A \\mid X$, which implies that given $X$, $A$ is independent of $Y(1)$. Therefore, we can factor the expectation:\n$$\n\\mathbb{E}[A Y(1) \\mid X] = \\mathbb{E}[A \\mid X] \\mathbb{E}[Y(1) \\mid X]\n$$\nBy definition, the propensity score is $e(X) = \\Pr(A=1 \\mid X) = \\mathbb{E}[A \\mid X]$. So,\n$$\n\\mathbb{E}[A Y(1) \\mid X] = e(X) \\mathbb{E}[Y(1) \\mid X]\n$$\nSubstituting this back into the expression for the first term's conditional expectation:\n$$\n\\mathbb{E}\\left[ \\frac{A Y}{e(X)} \\mid X \\right] = \\frac{1}{e(X)} \\left( e(X) \\mathbb{E}[Y(1) \\mid X] \\right) = \\mathbb{E}[Y(1) \\mid X]\n$$\nTaking the outer expectation with respect to $X$:\n$$\n\\mathbb{E}_X\\left[ \\mathbb{E}[Y(1) \\mid X] \\right] = \\mathbb{E}[Y(1)]\n$$\nThus, the first term evaluates to $\\mathbb{E}[Y(1)]$.\n\nFor the second term, $-\\mathbb{E}\\left[ \\frac{(1 - A) Y}{1 - e(X)} \\right]$, the procedure is analogous:\n$$\n\\mathbb{E}\\left[ \\frac{(1 - A) Y}{1 - e(X)} \\right] = \\mathbb{E}_X\\left[ \\mathbb{E}\\left[ \\frac{(1 - A) Y}{1 - e(X)} \\mid X \\right] \\right] = \\mathbb{E}_X\\left[ \\frac{1}{1-e(X)} \\mathbb{E}[(1-A)Y \\mid X] \\right]\n$$\nUsing consistency, $(1-A)Y = (1-A)(A Y(1) + (1-A) Y(0)) = (1-A)A Y(1) + (1-A)^2 Y(0)$. Since $A$ is binary, this simplifies to $(1-A)Y = (1-A)Y(0)$.\nThe conditional expectation becomes:\n$$\n\\mathbb{E}[(1-A)Y \\mid X] = \\mathbb{E}[(1-A)Y(0) \\mid X]\n$$\nUsing conditional ignorability, $(Y(1), Y(0)) \\perp A \\mid X$, we factor the expectation:\n$$\n\\mathbb{E}[(1-A)Y(0) \\mid X] = \\mathbb{E}[1-A \\mid X] \\mathbb{E}[Y(0) \\mid X]\n$$\nThe term $\\mathbb{E}[1-A \\mid X] = 1 - \\mathbb{E}[A \\mid X] = 1 - e(X)$. So,\n$$\n\\mathbb{E}[(1-A)Y(0) \\mid X] = (1 - e(X)) \\mathbb{E}[Y(0) \\mid X]\n$$\nSubstituting this back:\n$$\n\\frac{1}{1-e(X)} \\mathbb{E}[(1-A)Y \\mid X] = \\frac{1}{1-e(X)} \\left( (1-e(X)) \\mathbb{E}[Y(0) \\mid X] \\right) = \\mathbb{E}[Y(0) \\mid X]\n$$\nTaking the outer expectation with respect to $X$:\n$$\n\\mathbb{E}_X\\left[ \\mathbb{E}[Y(0) \\mid X] \\right] = \\mathbb{E}[Y(0)]\n$$\nThus, the second term evaluates to $\\mathbb{E}[Y(0)]$.\n\nFinally, combining the results for both terms:\n$$\n\\mathbb{E}[\\hat{\\tau}] = \\mathbb{E}[Y(1)] - \\mathbb{E}[Y(0)]\n$$\nThis expression is the definition of the Average Treatment Effect, $\\tau$. Therefore, the IPW estimator is an unbiased estimator for the ATE under the stated conditions, most notably the crucial condition that the true propensity score is known.\n\nThe final answer is the derived expression for the expectation of the estimator in terms of the potential outcomes.", "answer": "$$\n\\boxed{\\mathbb{E}[Y(1)] - \\mathbb{E}[Y(0)]}\n$$", "id": "4545088"}, {"introduction": "While adjusting for confounders is crucial, the indiscriminate inclusion of covariates in a model can be perilous, sometimes introducing bias where none existed. This phenomenon is vividly illustrated by collider bias, also known as M-bias, where conditioning on a variable that is a common effect of two other variables can induce a spurious association between them. This practice problem [@problem_id:4145189] presents a realistic scenario from fMRI data analysis to help you develop the graphical reasoning skills needed to identify colliders and understand why adjusting for them is harmful. Mastering this concept is essential for moving beyond naive adjustment strategies and toward thoughtful, theory-guided causal modeling.", "problem": "In a functional Magnetic Resonance Imaging (fMRI) study of resting-state brain activity, consider a directed acyclic graph (DAG) with variables $X$, $A$, $U$, $B$, and $Y$ arranged as $X \\to A \\leftarrow U \\to B \\to Y$. Here $X$ denotes head motion amplitude computed from framewise displacement, $U$ denotes unobserved physiological arousal, $A$ denotes a data-driven image quality metric that loads on both motion and arousal (for example, an independent component analysis artifact component), $B$ denotes a global signal regressor influenced by arousal, and $Y$ denotes the Blood Oxygen Level Dependent (BOLD) signal averaged in a cortical region of interest. Assume a structural causal model (SCM) consistent with the graph in which each node is generated by a linear structural equation with mutually independent, zero-mean Gaussian noise:\n$$\nX = \\varepsilon_X,\\quad U = \\varepsilon_U,\\quad A = \\alpha_X X + \\alpha_U U + \\varepsilon_A,\\quad B = \\beta U + \\varepsilon_B,\\quad Y = \\gamma B + \\varepsilon_Y,\n$$\nwith all noises $\\varepsilon_X,\\varepsilon_U,\\varepsilon_A,\\varepsilon_B,\\varepsilon_Y$ mutually independent, zero-mean Gaussian, and with finite, positive variances. There is no direct causal arrow from $X$ to $Y$ in the DAG. Use only foundational definitions from causal graphical models (for example, d-separation, collider paths, back-door criterion) and well-tested facts of linear Gaussian models (for example, properties of covariance and conditional covariance in multivariate normal distributions) to reason about statistical associations.\n\nWhich of the following statements are correct in this setting? Select all that apply.\n\nA. Under the special case $\\alpha_X=\\alpha_U=\\beta=\\gamma=1$ and $\\operatorname{Var}(\\varepsilon_X)=\\operatorname{Var}(\\varepsilon_U)=\\operatorname{Var}(\\varepsilon_A)=\\operatorname{Var}(\\varepsilon_B)=\\operatorname{Var}(\\varepsilon_Y)=1$, the marginal covariance between $X$ and $Y$ is $0$, but the covariance between $X$ and $Y$ conditional on $A$ is nonzero and negative.\n\nB. Conditioning on $A$ blocks any noncausal path between $X$ and $Y$, so adjusting for $A$ eliminates spurious association between $X$ and $Y$.\n\nC. Conditioning on $B$ (without conditioning on $A$) opens a noncausal path between $X$ and $Y$, thereby inducing a spurious association even if no direct effect exists.\n\nD. In fMRI artifact adjustment, including $A$ as a nuisance regressor can induce an association between motion regressors $X$ and neural outcome $Y$ that was absent marginally, via an opened path through $U$ and $B$.\n\nE. To estimate the causal effect of $X$ on $Y$ in this DAG, a valid adjustment set is the empty set; adjusting for $A$ is harmful for bias and adjusting for $B$ is unnecessary for bias reduction.", "solution": "The problem statement is internally consistent, scientifically grounded in the principles of fMRI data analysis and causal inference, and well-posed. The provided directed acyclic graph (DAG) and the corresponding linear structural causal model (SCM) are clearly defined, enabling rigorous mathematical and graphical analysis. Therefore, the problem is valid. We proceed with the derivation.\n\nThe core of the problem lies in understanding the statistical associations between head motion ($X$) and a regional BOLD signal ($Y$) as dictated by the given causal graph: $X \\to A \\leftarrow U \\to B \\to Y$. The true causal effect of $X$ on $Y$ is zero, as there is no directed path from $X$ to $Y$. We will analyze the statistical association (covariance) between $X$ and $Y$ under different conditioning scenarios.\n\nFirst, let's analyze the paths between $X$ and $Y$ using the rules of d-separation. There is only one path between $X$ and $Y$: $X \\to A \\leftarrow U \\to B \\to Y$. This path is a non-causal path. The node $A$ is a collider on this path because it has two arrows pointing into it ($X \\to A$ and $U \\to A$).\nAccording to d-separation, a path is blocked if it contains a collider that is not in the conditioning set and none of whose descendants are in the conditioning set.\n\nLet's also perform a mathematical analysis using the provided linear SCM:\n$$\nX = \\varepsilon_X,\\quad U = \\varepsilon_U,\\quad A = \\alpha_X X + \\alpha_U U + \\varepsilon_A,\\quad B = \\beta U + \\varepsilon_B,\\quad Y = \\gamma B + \\varepsilon_Y\n$$\nAll noise terms $\\varepsilon_X, \\varepsilon_U, \\varepsilon_A, \\varepsilon_B, \\varepsilon_Y$ are mutually independent, zero-mean Gaussian variables with positive, finite variances, which we denote as $\\sigma^2_V = \\operatorname{Var}(\\varepsilon_V)$ for $V \\in \\{X, U, A, B, Y\\}$.\n\nBy substitution, we can express $Y$ in terms of the exogenous variables:\n$Y = \\gamma B + \\varepsilon_Y = \\gamma(\\beta U + \\varepsilon_B) + \\varepsilon_Y = \\gamma \\beta \\varepsilon_U + \\gamma \\varepsilon_B + \\varepsilon_Y$.\nNote that $X = \\varepsilon_X$ and $U = \\varepsilon_U$.\n\n**Marginal Covariance, $\\operatorname{Cov}(X, Y)$**\nThe marginal association between $X$ and $Y$ is given by their covariance.\n$$\n\\operatorname{Cov}(X, Y) = \\operatorname{Cov}(\\varepsilon_X, \\gamma \\beta \\varepsilon_U + \\gamma \\varepsilon_B + \\varepsilon_Y)\n$$\nUsing the linearity of covariance and the mutual independence of all noise terms:\n$$\n\\operatorname{Cov}(X, Y) = \\gamma \\beta \\operatorname{Cov}(\\varepsilon_X, \\varepsilon_U) + \\gamma \\operatorname{Cov}(\\varepsilon_X, \\varepsilon_B) + \\operatorname{Cov}(\\varepsilon_X, \\varepsilon_Y) = 0 + 0 + 0 = 0\n$$\nThis is consistent with d-separation: the path $X \\to A \\leftarrow U \\to B \\to Y$ is blocked by the collider $A$ when we are not conditioning on anything. Thus, $X$ and $Y$ are marginally independent (and uncorrelated).\n\n**Conditional Covariance, $\\operatorname{Cov}(X, Y | A)$**\nConditioning on the collider $A$ opens the path between $X$ and $Y$. In a linear Gaussian model, the conditional covariance is given by $\\operatorname{Cov}(X, Y | A) = \\operatorname{Cov}(X, Y) - \\frac{\\operatorname{Cov}(X, A)\\operatorname{Cov}(Y, A)}{\\operatorname{Var}(A)}$. Since $\\operatorname{Cov}(X, Y)=0$, this simplifies to:\n$$\n\\operatorname{Cov}(X, Y | A) = - \\frac{\\operatorname{Cov}(X, A)\\operatorname{Cov}(Y, A)}{\\operatorname{Var}(A)}\n$$\nWe calculate the necessary terms:\n- $\\operatorname{Cov}(X, A) = \\operatorname{Cov}(X, \\alpha_X X + \\alpha_U U + \\varepsilon_A) = \\alpha_X \\operatorname{Var}(X) = \\alpha_X \\sigma^2_X$.\n- $\\operatorname{Cov}(Y, A) = \\operatorname{Cov}(\\gamma\\beta U + \\gamma\\varepsilon_B + \\varepsilon_Y, \\alpha_X X + \\alpha_U U + \\varepsilon_A)$. Due to independence of noise terms, this simplifies to $\\operatorname{Cov}(\\gamma\\beta U, \\alpha_U U) = \\gamma\\beta\\alpha_U \\operatorname{Var}(U) = \\gamma\\beta\\alpha_U \\sigma^2_U$.\n- $\\operatorname{Var}(A) = \\operatorname{Var}(\\alpha_X X + \\alpha_U U + \\varepsilon_A) = \\alpha_X^2 \\operatorname{Var}(X) + \\alpha_U^2 \\operatorname{Var}(U) + \\operatorname{Var}(\\varepsilon_A) = \\alpha_X^2 \\sigma^2_X + \\alpha_U^2 \\sigma^2_U + \\sigma^2_A$.\n\nSubstituting these into the formula for conditional covariance:\n$$\n\\operatorname{Cov}(X, Y | A) = - \\frac{(\\alpha_X \\sigma^2_X)(\\gamma\\beta\\alpha_U \\sigma^2_U)}{\\alpha_X^2 \\sigma^2_X + \\alpha_U^2 \\sigma^2_U + \\sigma^2_A} = - \\frac{\\alpha_X \\alpha_U \\beta \\gamma \\sigma^2_X \\sigma^2_U}{\\operatorname{Var}(A)}\n$$\nSince all variances are positive and the path coefficients $\\alpha_X, \\alpha_U, \\beta, \\gamma$ are implicitly non-zero (as arrows exist in the graph), this conditional covariance is non-zero. It has the opposite sign of the product of the path coefficients $\\alpha_X \\alpha_U \\beta \\gamma$.\n\n**Conditional Covariance, $\\operatorname{Cov}(X, Y | B)$**\nConditioning on $B$ does not open the collider path, because $B$ is not the collider $A$ and it is not a descendant of $A$. Graphically, $X$ and $Y$ are d-separated by the empty set given $B$. Let's verify this with a calculation:\n$$\n\\operatorname{Cov}(X, Y | B) = \\operatorname{Cov}(X, Y) - \\frac{\\operatorname{Cov}(X, B)\\operatorname{Cov(Y, B)}}{\\operatorname{Var}(B)}\n$$\nWe need $\\operatorname{Cov}(X, B)$:\n- $\\operatorname{Cov}(X, B) = \\operatorname{Cov}(X, \\beta U + \\varepsilon_B) = \\beta \\operatorname{Cov}(X, U) + \\operatorname{Cov}(X, \\varepsilon_B) = \\beta \\operatorname{Cov}(\\varepsilon_X, \\varepsilon_U) + \\operatorname{Cov}(\\varepsilon_X, \\varepsilon_B) = 0$.\nSince $\\operatorname{Cov}(X, B)=0$ and $\\operatorname{Cov}(X, Y)=0$, the entire expression is zero:\n$$\n\\operatorname{Cov}(X, Y | B) = 0 - \\frac{0 \\cdot \\operatorname{Cov}(Y, B)}{\\operatorname{Var}(B)} = 0\n$$\nConditioning on $B$ does not induce an association between $X$ and $Y$.\n\nNow we evaluate each statement.\n\n**A. Under the special case $\\alpha_X=\\alpha_U=\\beta=\\gamma=1$ and $\\operatorname{Var}(\\varepsilon_X)=\\operatorname{Var}(\\varepsilon_U)=\\operatorname{Var}(\\varepsilon_A)=\\operatorname{Var}(\\varepsilon_B)=\\operatorname{Var}(\\varepsilon_Y)=1$, the marginal covariance between $X$ and $Y$ is $0$, but the covariance between $X$ and $Y$ conditional on $A$ is nonzero and negative.**\n- The marginal covariance $\\operatorname{Cov}(X, Y)$ is indeed $0$, as derived generally above.\n- Let's compute the conditional covariance $\\operatorname{Cov}(X, Y | A)$ with the given parameters: $\\alpha_X=1$, $\\alpha_U=1$, $\\beta=1$, $\\gamma=1$, and all variances are $1$.\n$$\n\\operatorname{Cov}(X, Y | A) = - \\frac{(1)(1)(1)(1)(1)(1)}{(1)^2(1) + (1)^2(1) + 1} = - \\frac{1}{1 + 1 + 1} = -\\frac{1}{3}\n$$\nThis value is non-zero and negative. The statement is fully consistent with our derivations.\n**Verdict: Correct.**\n\n**B. Conditioning on $A$ blocks any noncausal path between $X$ and $Y$, so adjusting for $A$ eliminates spurious association between $X$ and $Y$.**\nThis statement misrepresents the rule for colliders. The path $X \\to A \\leftarrow U \\to B \\to Y$ is a non-causal path. Conditioning on the collider $A$ *opens* this path, it does not block it. This induces a spurious association where none existed marginally. Therefore, adjusting for $A$ *creates* a spurious association, it does not eliminate one.\n**Verdict: Incorrect.**\n\n**C. Conditioning on $B$ (without conditioning on $A$) opens a noncausal path between $X$ and $Y$, thereby inducing a spurious association even if no direct effect exists.**\nAs shown by both d-separation logic and our covariance calculation, conditioning on $B$ does not open the collider path $X \\to A \\leftarrow U \\to B \\to Y$. $B$ is not the collider $A$, nor is it a descendant of $A$. We calculated that $\\operatorname{Cov}(X, Y | B) = 0$. Therefore, conditioning on $B$ does not induce an association.\n**Verdict: Incorrect.**\n\n**D. In fMRI artifact adjustment, including $A$ as a nuisance regressor can induce an association between motion regressors $X$ and neural outcome $Y$ that was absent marginally, via an opened path through $U$ and $B$.**\nThis statement accurately describes the phenomenon of collider bias in the specified context.\n- \"Including $A$ as a nuisance regressor\" means conditioning on $A$.\n- \"induce an association... that was absent marginally\": We showed $\\operatorname{Cov}(X, Y | A) \\neq 0$ while $\\operatorname{Cov}(X, Y) = 0$. This is true.\n- \"via an opened path through $U$ and $B$\": The path that is opened by conditioning on $A$ is precisely $X \\to A \\leftarrow U \\to B \\to Y$, which connects $X$ and $Y$ via the unobserved arousal $U$ and the global signal $B$. The statement is a correct qualitative description of our findings.\n**Verdict: Correct.**\n\n**E. To estimate the causal effect of $X$ on $Y$ in this DAG, a valid adjustment set is the empty set; adjusting for $A$ is harmful for bias and adjusting for $B$ is unnecessary for bias reduction.**\nTo estimate the causal effect of $X$ on $Y$, we must block all non-causal \"back-door\" paths. A back-door path is a path connecting $X$ and $Y$ that starts with an arrow pointing into $X$. In this DAG, $X$ is an exogenous variable ($X = \\varepsilon_X$), so there are no arrows into $X$. Consequently, there are no back-door paths.\n- \"a valid adjustment set is the empty set\": The back-door criterion requires blocking all back-door paths. Since there are none, the empty set $\\emptyset$ is a valid adjustment set. Estimating the effect from the marginal association (i.e., regression of $Y$ on $X$) is unbiased. The regression coefficient would be proportional to $\\operatorname{Cov}(X,Y) = 0$, correctly identifying the zero causal effect.\n- \"adjusting for $A$ is harmful for bias\": As shown previously, adjusting for the collider $A$ induces a non-zero association $\\operatorname{Cov}(X, Y | A) \\neq 0$. This leads to a non-zero regression coefficient for $X$, which is a biased estimate of the true zero causal effect. Thus, it is harmful.\n- \"adjusting for $B$ is unnecessary for bias reduction\": Since the unadjusted estimate is already unbiased, no adjustment is necessary. Adjusting for $B$ still yields an unbiased estimate since $\\operatorname{Cov}(X, Y | B) = 0$, but it is not required for bias reduction. The statement is correct.\n**Verdict: Correct.**", "answer": "$$\\boxed{ADE}$$", "id": "4145189"}, {"introduction": "No matter how carefully we adjust for measured covariates, the specter of unmeasured confounding haunts every observational study. The crucial final step in presenting a causal claim is to assess how sensitive the conclusion is to such potential unmeasured factors. The E-value has emerged as a standard tool for this sensitivity analysis, quantifying the minimum strength of association an unmeasured confounder would need to have with both the exposure and the outcome to fully \"explain away\" the observed effect. In this exercise [@problem_id:4545130], you will derive the formula for the E-value from first principles and apply it, providing you with a practical method to gauge and communicate the robustness of your causal findings.", "problem": "An observational cohort study in medical data analytics examines whether a binary genomic exposure, denoted by $A \\in \\{0,1\\}$ indicating high baseline tumor mutational burden from whole-exome sequencing, affects the $1$-year risk of a severe immune-related adverse event, denoted by $Y \\in \\{0,1\\}$. Let $C$ denote a vector of measured baseline covariates. Suppose the causal estimand of interest is the causal risk ratio, defined in terms of potential outcomes as the ratio of the marginal risks under a hypothetical intervention on $A$, given by $\\mathrm{RR}^{\\text{true}} = \\frac{\\Pr(Y^{1}=1)}{\\Pr(Y^{0}=1)}$, where $Y^{a}$ denotes the counterfactual outcome under intervention setting $A=a$.\n\nAn investigator reports an observational, covariate-adjusted point estimate for the risk ratio as $\\mathrm{RR}^{\\text{obs}} = 1.8$ after controlling for $C$. Consider the possibility of an unmeasured baseline confounder $U \\in \\{0,1\\}$, independent of $A$ only conditional on $(C,U)$, that may be associated with both $A$ and $Y$.\n\nUsing only foundational definitions of causal effects and the established sensitivity-bias bounding framework on the risk ratio scale for a single unmeasured binary confounder, do the following:\n\n- Define precisely the quantity often termed the E-value for the risk ratio estimand as the minimal common strength, on the risk ratio scale, of the associations of an unmeasured confounder $U$ with $A$ and with $Y$ (conditional on $A$ and $C$), that would be sufficient to reduce the causal risk ratio $\\mathrm{RR}^{\\text{true}}$ to $1$ while being consistent with the observed $\\mathrm{RR}^{\\text{obs}}$.\n- From first principles of bias factor decomposition and bounding on the risk ratio scale, derive a closed-form expression for this E-value in terms of a generic observed risk ratio $\\mathrm{RR}^{\\text{obs}} \\ge 1$.\n- Compute this E-value for the given observational estimate $\\mathrm{RR}^{\\text{obs}} = 1.8$.\n\nProvide the final numerical answer for the computed E-value as a single real number without units. If any rounding is needed, round your final answer to four significant figures; if an exact value is available, report the exact value.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective, situated within the established potential outcomes framework of causal inference and the sensitivity analysis methods developed for it. It is therefore valid and permits a rigorous solution.\n\nThe problem requires a three-part response: a precise definition of the E-value, its derivation from first principles, and its computation for the given data.\n\n### 1. Definition of the E-value\n\nLet $A$ be a binary exposure, $Y$ a binary outcome, and $C$ a set of measured covariates. The observed risk ratio, adjusted for $C$, is denoted $\\mathrm{RR}^{\\text{obs}}$. The true causal risk ratio is $\\mathrm{RR}^{\\text{true}} = \\frac{\\Pr(Y^1=1)}{\\Pr(Y^0=1)}$. Let $U$ be a single unmeasured binary confounder. For the derivation, we consider a single stratum of $C$, and for notational simplicity, we omit $C$ from the expressions, with the understanding that all probabilities are conditional on $C$.\n\nThe strength of the association between the unmeasured confounder $U$ and the exposure $A$ is parameterized by the risk ratio $\\mathrm{RR}_{AU}$, defined as the maximum of $\\frac{\\Pr(A=1|U=1)}{\\Pr(A=1|U=0)}$ and $\\frac{\\Pr(A=0|U=1)}{\\Pr(A=0|U=0)}$ over all strata of the covariates $C$. Without loss of generality, let $U$ be coded such that this risk ratio is $\\ge 1$.\n\nThe strength of the association between the unmeasured confounder $U$ and the outcome $Y$, conditional on $A$, is parameterized by the risk ratio $\\mathrm{RR}_{UY}$. This is defined as $\\mathrm{RR}_{UY} = \\max_{a} \\frac{\\Pr(Y=1|A=a, U=1)}{\\Pr(Y=1|A=a, U=0)}$, where the maximum is taken over $a \\in \\{0,1\\}$ and all strata of $C$.\n\nAn unmeasured confounder $U$ can introduce bias, causing $\\mathrm{RR}^{\\text{obs}}$ to differ from $\\mathrm{RR}^{\\text{true}}$. The E-value is defined as the minimal common strength of association, $E$, that the confounder $U$ must have with both the exposure $A$ and the outcome $Y$ (i.e., $\\mathrm{RR}_{AU} \\ge E$ and $\\mathrm{RR}_{UY} \\ge E$) to be potentially capable of \"explaining away\" the observed association. \"Explaining away\" means that the observed association $\\mathrm{RR}^{\\text{obs}}$ is consistent with a true causal risk ratio $\\mathrm{RR}^{\\text{true}}$ of $1$.\n\nFormally, for an observed risk ratio $\\mathrm{RR}^{\\text{obs}} \\ge 1$, the E-value is the value $E$ such that if $\\mathrm{RR}_{AU}  E$ or $\\mathrm{RR}_{UY}  E$, then it is impossible for $U$ alone to account for the observed association (i.e., $\\mathrm{RR}^{\\text{true}}$ must be greater than $1$). It is the minimum value $E$ for which there could exist an unmeasured confounder $U$ with $\\mathrm{RR}_{AU} = E$ and $\\mathrm{RR}_{UY} = E$ that can shift the estimate to or through the null, allowing for $\\mathrm{RR}^{\\text{true}}=1$.\n\n### 2. Derivation of the E-value Formula from First Principles\n\nThe relationship between the observed risk ratio and the true causal risk ratio can be expressed as $\\mathrm{RR}^{\\text{obs}} = \\mathrm{RR}^{\\text{true}} \\times B$, where $B$ is a bias factor due to the unmeasured confounder $U$. To explain away the observed effect, i.e., to allow for the possibility that $\\mathrm{RR}^{\\text{true}}=1$, the bias factor must be at least as large as the observed risk ratio, $B \\ge \\mathrm{RR}^{\\text{obs}}$. Our goal is to find the maximum possible bias factor, $B_{\\max}$, for given values of $\\mathrm{RR}_{AU}$ and $\\mathrm{RR}_{UY}$.\n\nAssuming consistency and conditional exchangeability given $(C,U)$, we can write the observed outcome probability for exposure level $a$ by marginalizing over $U$:\n$$ \\Pr(Y=1|A=a) = \\Pr(Y=1|A=a, U=0)\\Pr(U=0|A=a) + \\Pr(Y=1|A=a, U=1)\\Pr(U=1|A=a) $$\nUsing the definition $\\mathrm{RR}_{UY} = \\frac{\\Pr(Y=1|A=a, U=1)}{\\Pr(Y=1|A=a, U=0)}$ (assuming it is constant across $a$ for simplicity in derivation), we have:\n$$ \\Pr(Y=1|A=a) = \\Pr(Y=1|A=a, U=0) \\left[ \\Pr(U=0|A=a) + \\mathrm{RR}_{UY} \\Pr(U=1|A=a) \\right] $$\nThe observed risk ratio is the ratio of these probabilities for $a=1$ and $a=0$:\n$$ \\mathrm{RR}^{\\text{obs}} = \\frac{\\Pr(Y=1|A=1)}{\\Pr(Y=1|A=0)} = \\frac{\\Pr(Y=1|A=1, U=0)}{\\Pr(Y=1|A=0, U=0)} \\times \\frac{\\Pr(U=0|A=1) + \\mathrm{RR}_{UY} \\Pr(U=1|A=1)}{\\Pr(U=0|A=0) + \\mathrm{RR}_{UY} \\Pr(U=1|A=0)} $$\nThe first term on the right-hand side is the true causal risk ratio conditional on $U=0$, which, under the assumption of no modification of the RR by $U$, is equal to the marginal causal risk ratio $\\mathrm{RR}^{\\text{true}}$. The second term is the bias factor $B$.\n$$ B = \\frac{1 - p_1 + \\mathrm{RR}_{UY} p_1}{1 - p_0 + \\mathrm{RR}_{UY} p_0} $$\nwhere $p_a = \\Pr(U=1|A=a)$. Now, we re-parameterize in terms of $\\mathrm{RR}_{AU}$. A common parameterization, which gives the sharpest bound, defines the exposure-confounder association as the prevalence ratio $\\mathrm{RR}_{AU} = \\frac{\\Pr(U=1|A=1)}{\\Pr(U=1|A=0)} = \\frac{p_1}{p_0}$. This implies $p_1 = p_0 \\mathrm{RR}_{AU}$. The value of $p_0$ is unknown. To find the maximal bias, we maximize $B$ over the possible range of $p_0$. The constraint $p_1 \\le 1$ implies $p_0 \\mathrm{RR}_{AU} \\le 1$, so $p_0 \\in [0, 1/\\mathrm{RR}_{AU}]$.\nThe derivative of $B$ with respect to $p_0$ has a numerator proportional to $(\\mathrm{RR}_{AU}-1)(\\mathrm{RR}_{UY}-1)$. Since $\\mathrm{RR}_{AU} \\ge 1$ and $\\mathrm{RR}_{UY} \\ge 1$, this derivative is non-negative, so $B$ is a non-decreasing function of $p_0$. The maximum value of $B$ is attained at the maximum possible value for $p_0$, which is $p_0 = 1/\\mathrm{RR}_{AU}$. At this point, $p_1 = 1$.\nSubstituting $p_0=1/\\mathrm{RR}_{AU}$ and $p_1=1$ into the expression for $B$ yields the maximum bias factor:\n$$ B_{\\max} = \\frac{1 - 1 + \\mathrm{RR}_{UY} (1)}{1 - 1/\\mathrm{RR}_{AU} + \\mathrm{RR}_{UY} (1/\\mathrm{RR}_{AU})} = \\frac{\\mathrm{RR}_{UY}}{( \\mathrm{RR}_{AU} - 1 + \\mathrm{RR}_{UY} ) / \\mathrm{RR}_{AU}} = \\frac{\\mathrm{RR}_{AU} \\mathrm{RR}_{UY}}{\\mathrm{RR}_{AU} + \\mathrm{RR}_{UY} - 1} $$\nFor an unmeasured confounder to explain the observed association, this maximal bias must be equal to or greater than the observed risk ratio, $B_{\\max} \\ge \\mathrm{RR}^{\\text{obs}}$. The E-value, $E$, is found by setting $\\mathrm{RR}_{AU} = \\mathrm{RR}_{UY} = E$ and solving the boundary condition $B_{\\max} = \\mathrm{RR}^{\\text{obs}}$:\n$$ \\mathrm{RR}^{\\text{obs}} = \\frac{E \\cdot E}{E + E - 1} = \\frac{E^2}{2E - 1} $$\nThis leads to a quadratic equation for $E$:\n$$ E^2 = \\mathrm{RR}^{\\text{obs}}(2E - 1) $$\n$$ E^2 - 2E(\\mathrm{RR}^{\\text{obs}}) + \\mathrm{RR}^{\\text{obs}} = 0 $$\nUsing the quadratic formula, $E = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$, with $a=1$, $b=-2\\mathrm{RR}^{\\text{obs}}$, and $c=\\mathrm{RR}^{\\text{obs}}$:\n$$ E = \\frac{2\\mathrm{RR}^{\\text{obs}} \\pm \\sqrt{(-2\\mathrm{RR}^{\\text{obs}})^2 - 4(1)(\\mathrm{RR}^{\\text{obs}})}}{2} $$\n$$ E = \\frac{2\\mathrm{RR}^{\\text{obs}} \\pm \\sqrt{4(\\mathrm{RR}^{\\text{obs}})^2 - 4\\mathrm{RR}^{\\text{obs}}}}{2} $$\n$$ E = \\mathrm{RR}^{\\text{obs}} \\pm \\sqrt{(\\mathrm{RR}^{\\text{obs}})^2 - \\mathrm{RR}^{\\text{obs}}} $$\nBy definition, a risk ratio parameter representing a positive association, such as the E-value, must be greater than or equal to $1$. For $\\mathrm{RR}^{\\text{obs}}  1$, the smaller root, $\\mathrm{RR}^{\\text{obs}} - \\sqrt{(\\mathrm{RR}^{\\text{obs}})^2 - \\mathrm{RR}^{\\text{obs}}}$, is less than $1$. Therefore, the only physically meaningful solution for the E-value is the larger root.\nThe final closed-form expression for the E-value is:\n$$ E = \\mathrm{RR}^{\\text{obs}} + \\sqrt{\\mathrm{RR}^{\\text{obs}} (\\mathrm{RR}^{\\text{obs}} - 1)} $$\n\n### 3. Computation for the Given Observational Estimate\n\nThe problem provides an observed risk ratio of $\\mathrm{RR}^{\\text{obs}} = 1.8$. We substitute this value into the derived formula:\n$$ E = 1.8 + \\sqrt{1.8 \\times (1.8 - 1)} $$\n$$ E = 1.8 + \\sqrt{1.8 \\times 0.8} $$\n$$ E = 1.8 + \\sqrt{1.44} $$\nThe square root of $1.44$ is $1.2$.\n$$ E = 1.8 + 1.2 = 3.0 $$\nThe computed E-value is exactly $3.0$. This means that an unmeasured confounder associated with both the genomic exposure and the adverse event by a risk ratio of $3.0$ each (conditional on the measured covariates $C$) could be sufficient to explain away the observed risk ratio of $1.8$. Conversely, if no unmeasured confounder has associations this strong with both the exposure and outcome, the true causal risk ratio is likely greater than $1$.", "answer": "$$\\boxed{3}$$", "id": "4545130"}]}