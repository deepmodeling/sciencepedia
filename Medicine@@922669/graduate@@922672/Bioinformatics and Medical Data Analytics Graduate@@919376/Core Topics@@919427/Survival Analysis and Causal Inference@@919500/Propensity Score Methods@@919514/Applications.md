## Applications and Interdisciplinary Connections

Having established the theoretical foundations and mechanics of [propensity score](@entry_id:635864) methods in previous chapters, we now turn to their application in diverse scientific domains. The true utility of a statistical method is revealed not in abstract derivations but in its capacity to help solve substantive problems. This chapter explores how propensity score principles are implemented, extended, and integrated to address complex research questions in fields ranging from clinical medicine and epidemiology to genomics and health economics. Our focus will be on bridging theory with practice, demonstrating how to navigate the common challenges of real-world data, and situating propensity score analysis within the broader toolkit of causal inference.

### Core Methodological Applications in Observational Research

Propensity score methods offer a flexible framework for emulating a randomized trial from observational data by balancing measured confounders between treatment groups. The three most common implementations of this principle are [inverse probability](@entry_id:196307) of treatment weighting, matching, and stratification.

#### Inverse Probability of Treatment Weighting (IPTW)

Inverse probability of treatment weighting is a powerful technique that conceptually creates a "pseudo-population" in which treatment assignment is independent of the measured baseline covariates. In this pseudo-population, a direct comparison of outcomes between treatment groups is no longer confounded by those covariates. This is achieved by weighting each individual by the inverse of the probability of receiving the treatment they actually received. For estimating the Average Treatment Effect (ATE), the weight for a treated individual ($T=1$) is $1/\hat{e}(X)$ and for an untreated individual ($T=0$) is $1/(1-\hat{e}(X))$, where $\hat{e}(X)$ is the estimated [propensity score](@entry_id:635864).

Consider a common scenario in pharmacoepidemiology: evaluating the effectiveness of a new antihypertensive medication compared to a standard therapy using electronic health records. Clinicians' decisions to prescribe the new drug may be influenced by patient characteristics like age, baseline blood pressure, kidney function, and comorbidities—the very factors that also influence the final blood pressure outcome. A naive comparison would be biased. An IPTW analysis would first model the probability of receiving the new drug given these baseline covariates, typically using [logistic regression](@entry_id:136386). After applying the inverse probability weights, the weighted distributions of age, baseline blood pressure, etc., should be nearly identical between the two treatment groups. A simple difference in the weighted mean outcomes then provides an estimate of the ATE. This approach, however, hinges on the critical, untestable assumption of conditional exchangeability (i.e., no unmeasured confounding) and requires careful diagnostic checks of covariate balance and positivity (i.e., practical overlap in propensity scores across groups) [@problem_id:4828155]. The finite-sample estimator for the ATE under IPTW is constructed by averaging the weighted outcomes across all $n$ individuals in the study [@problem_id:4599477]:
$$
\widehat{ATE}_{IPW} = \frac{1}{n} \sum_{i=1}^n \left( \frac{T_i Y_i}{\hat{e}(X_i)} - \frac{(1-T_i)Y_i}{1-\hat{e}(X_i)} \right)
$$

#### Propensity Score Matching

An alternative to weighting is matching. In [propensity score matching](@entry_id:166096), the goal is to construct a control group that is similar to the treated group with respect to the distribution of measured confounders. This is achieved by pairing each treated subject with one or more control subjects who have a similar [propensity score](@entry_id:635864). The most common approach is nearest-neighbor matching, where each treated unit is matched to the [control unit](@entry_id:165199) with the closest [propensity score](@entry_id:635864).

Because this procedure conditions on being treated, it naturally estimates the Average Treatment effect on the Treated (ATT), defined as $\mathbb{E}[Y(1) - Y(0) | T=1]$. The ATT answers a slightly different question than the ATE: what was the effect of the treatment on those who actually received it? The estimator is intuitive: it is the average of the differences in outcomes between each treated subject and their matched control counterpart. In a study evaluating a clinical decision support tool, for example, the ATT would measure the tool's effect specifically for the clinicians and patients who were exposed to it [@problem_id:5221156].

To improve the quality of matches and reduce bias, it is common practice to impose a "caliper," which is a maximum allowable distance between propensity scores for a match to be considered valid. Furthermore, research has shown that matching on the logit of the propensity score, $\ln(\hat{e}(X)/(1-\hat{e}(X)))$, often performs better than matching on the raw score. This is because the logit transformation "stretches out" the tails of the [propensity score](@entry_id:635864) distribution (i.e., values near 0 or 1), preventing matches that appear close on the probability scale but are far apart in terms of the underlying covariate profiles that drive treatment selection. A widely adopted heuristic is to use a caliper width of $0.2$ standard deviations of the logit-[propensity score](@entry_id:635864), a choice supported by simulation studies as providing a good balance between reducing bias and retaining an adequate sample size [@problem_id:5221122].

#### Propensity Score Stratification

Stratification, or subclassification, is a third foundational approach that involves dividing the sample into several (typically 5 to 10) strata based on [quantiles](@entry_id:178417) of the estimated [propensity score](@entry_id:635864). The logic is that within each stratum, subjects have similar propensity scores and thus, on average, a similar distribution of baseline covariates. This creates approximate balance within each stratum, allowing for a nearly unconfounded estimate of the treatment effect to be calculated simply by taking the difference in mean outcomes between treated and control subjects within that stratum.

The overall ATE is then estimated by taking a weighted average of these stratum-specific effects, where each stratum is weighted by its proportion of the total study population. The stratification estimator for the ATE, based on $K$ strata, is given by:
$$
\widehat{ATE}_{Strat} = \sum_{k=1}^{K} \frac{N_k}{N} (\bar{Y}_{1k} - \bar{Y}_{0k})
$$
where $N$ is the total sample size, $N_k$ is the number of individuals in stratum $k$, and $\bar{Y}_{1k}$ and $\bar{Y}_{0k}$ are the sample mean outcomes for the treated and control groups within stratum $k$, respectively. This method is intuitive, easy to implement, and provides a clear diagnostic of where overlap in the propensity score distribution may be poor (e.g., in strata with few or no subjects from one treatment group) [@problem_id:4599519].

### Extensions for Complex Data Structures

The utility of propensity scores extends far beyond the basic case of a single, binary treatment. The framework can be adapted to handle more complex [data structures](@entry_id:262134) common in modern biomedical research.

#### Multi-Category Treatments

Many clinical scenarios involve choosing among more than two treatment options, such as different oncology regimens or classes of diabetes medications. In this case, the binary propensity score is replaced by the **Generalized Propensity Score (GPS)**, which is a vector of conditional probabilities of receiving each of the $K$ possible treatments, given the covariates: $e(X) = (P(T=1|X), \dots, P(T=K|X))$. Since this vector sums to 1, it lies on a $(K-1)$-dimensional [simplex](@entry_id:270623).

To achieve covariate balance across all $K$ treatment groups, one must condition on this entire vector. Simply stratifying on the propensity score for one treatment versus all others is insufficient, as it does not guarantee balance between the other treatment groups. A valid approach is to stratify or match subjects based on the similarity of their entire GPS vector. This can be achieved through multi-dimensional binning or [clustering algorithms](@entry_id:146720) (e.g., [k-means](@entry_id:164073)) applied to the $(K-1)$-dimensional GPS. Once subjects are grouped into subclasses with similar GPS vectors, treatment effects can be estimated within each subclass and aggregated to produce population-level estimates of contrasts between any pair of treatments [@problem_id:4599467].

#### Longitudinal Data and Time-Varying Confounding

In longitudinal studies, where treatments and covariates are measured repeatedly over time, a particularly challenging form of bias known as **time-varying confounding** can arise. This occurs when a variable is both a predictor of future treatment (a confounder) and is also affected by past treatment (an intermediate outcome). For instance, in managing a chronic disease, a patient's clinical status (e.g., lab values) at one visit may influence the physician's treatment decision at that visit, while that same clinical status was itself influenced by the treatment given at the previous visit.

Standard propensity score methods fail in this setting. The solution is to use **Marginal Structural Models (MSMs)** estimated via [inverse probability](@entry_id:196307) weighting. In this framework, a weight is calculated for each subject at each time point, equal to the inverse of the conditional probability of receiving the observed treatment at that time, given their entire observed past history of covariates and treatments. The subject's overall weight is the product of these time-specific weights. This "longitudinal" weighting creates a pseudo-population in which the treatment given at each time point is independent of the time-varying confounders. This powerful technique allows for the estimation of the causal effects of entire sequences of treatments or dynamic treatment regimes, under an assumption of sequential ignorability—essentially, that there are no unmeasured confounders at any time point [@problem_id:4599479].

#### Pharmacogenomics and Effect Modification

Propensity score methods are invaluable in pharmacogenomics, a field that studies how genetic variation affects [drug response](@entry_id:182654). In an observational study comparing two antiplatelet drugs, for instance, a patient's `CYP2C19` genotype might be a known confounder, as clinicians may preferentially prescribe a newer drug to patients with a "poor metabolizer" genotype. At the same time, the genotype is also a known effect modifier, as the older drug is less effective in these patients. Propensity score methods are perfectly suited to handle the confounding aspect. By including the genotype in the [propensity score](@entry_id:635864) model, the method can create treatment groups that are balanced with respect to the genotype distribution. This allows for the estimation of an average treatment effect across the entire population. Importantly, the presence of effect modification does not invalidate the use of propensity scores. To study the effect modification itself, one can perform a stratified analysis, estimating the treatment effect separately within each genotype group, using propensity scores to control for other confounders within each stratum [@problem_id:4814015].

### Addressing Practical Challenges in Real-World Data

The analysis of real-world data, such as from Electronic Health Records (EHR), presents unique challenges including missing data and high dimensionality. Propensity score methods can be adapted to navigate these issues.

#### Missing Covariate Data

Covariates in EHR data are frequently missing, as lab tests or clinical assessments are not performed uniformly on all patients. The validity of any analysis depends on the **missingness mechanism**. If data are **Missing Completely At Random (MCAR)**—meaning missingness is unrelated to any patient characteristic—a complete-case analysis (using only patients with no missing data) is unbiased, though inefficient. More commonly, data are **Missing At Random (MAR)**, where the probability of a variable being missing depends on other *observed* data, including treatment status and the outcome. In this case, complete-case analysis is biased. Finally, if data are **Missing Not At Random (MNAR)**, where missingness depends on the unobserved value itself, standard methods fail and identification is not generally possible without strong, untestable assumptions [@problem_id:4612514].

Under the common MAR assumption, the standard and most principled approach is to use **Multiple Imputation (MI)**. In this procedure, one first creates multiple "completed" datasets by imputing the missing values from a model that predicts the missing values based on the observed data. Crucially, for this process to be valid for a subsequent causal analysis, the imputation model must be "congenial" with the analysis model. This means the imputation model for the missing covariates must include not only the other covariates but also the treatment variable ($T$) and the outcome variable ($Y$). After [imputation](@entry_id:270805), the propensity score analysis is performed independently on each of the completed datasets, and the resulting effect estimates are pooled using Rubin's rules. This ensures that the uncertainty due to the [missing data](@entry_id:271026) is properly propagated into the final confidence interval [@problem_id:5221133].

#### High-Dimensional Data

In fields like genomics or when using vast EHR datasets, the number of potential confounders ($p$) can be much larger than the number of subjects ($n$). In this high-dimensional setting, standard [logistic regression](@entry_id:136386) for the [propensity score](@entry_id:635864) is not feasible. The solution is to use regularized regression techniques, such as the **logistic LASSO (Least Absolute Shrinkage and Selection Operator)**, which performs variable selection and parameter estimation simultaneously.

However, using machine learning for nuisance model estimation (like the [propensity score](@entry_id:635864)) introduces another challenge: **[post-selection inference](@entry_id:634249)**. If the same data is used to both select the model (e.g., choose which variables LASSO retains) and estimate the final causal effect, the standard errors of the effect estimate will be invalid. The modern solution to this problem is a framework known as **Double/Debiased Machine Learning (DML)**. DML combines three key ideas: (1) using flexible machine learning to estimate nuisance functions (both the [propensity score](@entry_id:635864) and the outcome regression); (2) using an orthogonalized, doubly robust estimator (like Augmented IPTW); and (3) employing **cross-fitting** (or sample splitting), where models are trained on one part of the data and used to predict on another. This procedure breaks the [statistical dependence](@entry_id:267552) that causes post-selection bias, yielding valid, robust confidence intervals even when complex machine learning models are used for confounding adjustment [@problem_id:4599493].

### Robustness, Efficiency, and Advanced Estimators

While the core methods are powerful, a new generation of estimators has been developed to improve robustness to [model misspecification](@entry_id:170325) and increase statistical efficiency.

#### Doubly Robust Estimators

An estimator is **doubly robust** if it provides a consistent estimate of the causal effect when at least one of two nuisance models is correctly specified: the propensity score model or the outcome [regression model](@entry_id:163386) ($E[Y|A,X]$). This property provides an extra layer of protection against [model misspecification](@entry_id:170325). A simple and intuitive example combines stratification with regression adjustment. First, the data is stratified by the [propensity score](@entry_id:635864). Then, within each stratum, an outcome regression model is fitted to adjust for any *residual* covariate imbalance. The overall ATE is a weighted average of the regression-adjusted treatment effects from each stratum. This estimator is consistent if the [propensity score](@entry_id:635864) model is right (as stratification creates balance) OR if the outcome models are right (as they correctly adjust for confounding), even if the stratification is imperfect [@problem_id:4599504]. The most common doubly robust estimator is the **Augmented Inverse Probability Weighted (AIPW)** estimator, which augments the standard IPTW estimator with a regression-based term.

#### Targeting Estimands for Precision: Overlap Weights

The standard IPTW estimator for the ATE can be unstable if some subjects have propensity scores very close to 0 or 1, leading to extremely large weights. This inflates the variance of the estimator. One strategy to address this is to target a different, more stable causal estimand. **Overlap weights** are designed to do just this. In overlap weighting, treated subjects receive a weight of $1-\hat{e}(X_i)$, and control subjects receive a weight of $\hat{e}(X_i)$. This scheme down-weights subjects with extreme propensity scores and gives the most weight to subjects in the region of maximal overlap—those for whom treatment assignment was most uncertain.

This procedure no longer estimates the ATE for the full population. Instead, it estimates the **Average Treatment effect on the Overlap population (ATO)**, which is the treatment effect for the subpopulation of individuals who are reasonably likely to receive either treatment. This estimand can often be estimated with much greater statistical precision than the ATE and may be of greater clinical or policy relevance, as it focuses on the group for whom the treatment decision is most equivocal [@problem_id:4599514] [@problem_id:5211115].

#### Targeted Maximum Likelihood Estimation (TMLE)

Targeted Maximum Likelihood Estimation (TMLE) is a general and highly efficient framework for estimating causal effects. It is a semi-parametric, doubly robust method. The procedure starts with initial estimates of the [propensity score](@entry_id:635864), $g(A,W)$, and the outcome regression, $Q(A,W)$. The key step is a "targeting" fluctuation, where the initial estimate of the outcome regression, $Q^0$, is updated to a new estimate, $Q^*$, in a way that is specifically tailored to optimize the estimation of the target causal parameter (e.g., the ATE). This update is done by fitting a small, clever regression of the outcome on a special "clever covariate" $H(A,W)$ derived from the [propensity score](@entry_id:635864), with the initial predictions $Q^0(A,W)$ as an offset. This fluctuation step ensures that the final plug-in estimator for the ATE solves the [efficient influence function](@entry_id:748828) equation, a property that imparts its double robustness and [asymptotic efficiency](@entry_id:168529) under minimal conditions [@problem_id:5221171].

### Situating Propensity Scores in the Causal Inference Landscape

Propensity score methods are a cornerstone of causal inference but are not the only tool available. It is crucial to understand their relationship with other methods, particularly in scenarios where their core assumption is violated.

#### Propensity Scores versus Instrumental Variables

The primary limitation of all propensity score methods is their reliance on the assumption of conditional exchangeability—that all confounders have been measured and included in the model. In many settings, there is strong reason to suspect the presence of **unmeasured confounding**. For example, in studying the effect of a therapy, unmeasured factors like patient frailty, health literacy, or physician skill may influence both treatment choice and outcome.

In such cases, an **Instrumental Variable (IV)** analysis may be possible. An IV is a variable, $Z$, that (1) is strongly associated with the treatment $T$ (relevance), (2) affects the outcome $Y$ *only* through the treatment $T$ ([exclusion restriction](@entry_id:142409)), and (3) is independent of the unmeasured confounders that bias the $T-Y$ relationship (independence). A classic example is using geographic variation in prescribing patterns or a sudden change in health insurance formulary policy as an instrument for a medication choice. Such instruments induce "as-if-random" variation in treatment assignment that is free from confounding.

When a valid instrument is available, IV methods can provide a consistent estimate of a causal effect even with unmeasured confounding. However, IV and PS methods are not interchangeable. First, they rely on different, strong, and untestable assumptions. Second, they typically estimate different parameters. Whereas PS methods can estimate the ATE or ATT, a standard IV analysis estimates the **Local Average Treatment Effect (LATE)**—the average effect only for the subpopulation of "compliers" whose treatment choice was actually influenced by the instrument. Understanding the distinct assumptions and target estimands of both PS and IV methods is critical for choosing the appropriate analytical strategy for a given research question [@problem_id:5051592].

### Conclusion

This chapter has traversed the wide-ranging applications of [propensity score](@entry_id:635864) methods, from their foundational use in clinical epidemiology to their advanced adaptation for complex data challenges in genomics and longitudinal studies. We have seen how the core principle of balancing covariates can be implemented through weighting, matching, and stratification, and how these techniques can be extended to handle multi-category treatments, time-varying confounders, missing data, and high-dimensional covariate spaces. Furthermore, we explored modern estimators that enhance robustness and efficiency, and contextualized propensity score analysis by comparing it with the [instrumental variable](@entry_id:137851) framework.

The versatility of [propensity score](@entry_id:635864) methods makes them an indispensable tool for researchers seeking to draw causal conclusions from observational data. However, their power is matched by the gravity of their underlying assumptions. A successful application requires not just technical proficiency but also deep subject-matter knowledge, careful model specification, and rigorous diagnostic checking. When applied with care and transparency, [propensity score](@entry_id:635864) methods provide a powerful lens through which to understand the causal effects that shape our world.