{"hands_on_practices": [{"introduction": "To effectively use Directed Acyclic Graphs (DAGs) for causal inference, one must first master the language they are written in: the rules of d-separation. These rules determine whether variables are statistically associated or independent based on the graph's structure. This exercise provides foundational practice in applying these rules by asking you to trace a path between two variables and determine its status when conditioning on a third, which in this case is a \"collider\" node—a common effect of the first two variables. Correctly identifying how colliders affect associations is a critical first step toward spotting and preventing common statistical biases. [@problem_id:4557701]", "problem": "In a causal analysis for medical data, consider a Directed Acyclic Graph (DAG) representing a simplified pathway in translational bioinformatics where $X$ denotes a genetic variant, $Y$ denotes an environmental exposure, $Z$ denotes a molecular biomarker, and $W$ denotes a downstream clinical outcome. The DAG has node set $\\{X, Y, Z, W\\}$ and directed edges $X \\to Z$, $Y \\to Z$, $Z \\to W$, and no other edges. Using the formal definition of $d$-separation in Directed Acyclic Graphs, proceed as follows:\n\n- Enumerate all simple paths (paths with no repeated nodes) between $X$ and $Y$.\n- For each simple path $p$ between $X$ and $Y$, let $I(p)$ be an indicator defined by $I(p) = 1$ if the path is blocked when conditioning on $Z$ under $d$-separation, and $I(p) = 0$ otherwise.\n- Define the quantity $D$ by\n$$\nD \\;=\\; \\prod_{p \\in \\mathcal{P}(X,Y)} I(p),\n$$\nwhere $\\mathcal{P}(X,Y)$ is the set of all simple paths between $X$ and $Y$.\n\nCompute the value of $D$. The final answer must be reported as a single real number. No rounding is required and no units are to be included in the final answer.", "solution": "The problem requires the computation of a quantity $D$ related to the concept of $d$-separation in a specified Directed Acyclic Graph (DAG). We must first validate the problem statement.\n\nThe problem provides the following givens:\n- A set of nodes $V = \\{X, Y, Z, W\\}$, representing a genetic variant ($X$), an environmental exposure ($Y$), a molecular biomarker ($Z$), and a clinical outcome ($W$).\n- A set of directed edges $E = \\{(X,Z), (Y,Z), (Z,W)\\}$, which correspond to the graphical structure $X \\to Z$, $Y \\to Z$, and $Z \\to W$.\n- The constraint that no other edges exist in the graph.\n- A definition for an indicator function $I(p)$ for any simple path $p$ between $X$ and $Y$. $I(p)=1$ if path $p$ is blocked by conditioning on $Z$, and $I(p)=0$ otherwise.\n- A definition for the quantity $D = \\prod_{p \\in \\mathcal{P}(X,Y)} I(p)$, where $\\mathcal{P}(X,Y)$ is the set of all simple paths between $X$ and $Y$.\n\nThe problem is scientifically grounded, as it uses standard, well-defined concepts from the theory of graphical models and causal inference, specifically DAGs and $d$-separation. The structure and variables are typical of a simplified model in bioinformatics. The problem is well-posed, with a fully specified graph and a clear objective. It is objective, complete, and contains no contradictions. Therefore, the problem is deemed valid and we may proceed with the solution.\n\nThe solution process involves three steps:\n1.  Identify the set of all simple paths between $X$ and $Y$, denoted $\\mathcal{P}(X,Y)$.\n2.  For each path in this set, determine if it is blocked by conditioning on $Z$.\n3.  Compute the value of $D$ based on these determinations.\n\nStep 1: Enumerate simple paths between $X$ and $Y$.\nA path in a graph is a sequence of nodes connected by edges, where the direction of the edges is disregarded for the purpose of path traversal. A simple path is one that does not contain repeated nodes.\nThe graph structure is given by the edges $X \\to Z$, $Y \\to Z$, and $Z \\to W$.\nTo find a path from $X$ to $Y$, we start at $X$. The only node adjacent to $X$ is $Z$. From $Z$, the adjacent nodes are $X$, $Y$, and $W$. To reach $Y$ without repeating nodes, the only possible next step from $Z$ is to $Y$.\nThus, there is exactly one simple path between $X$ and $Y$: the path $p_1 = (X, Z, Y)$.\nThe set of all simple paths is $\\mathcal{P}(X,Y) = \\{p_1\\}$.\n\nStep 2: Apply the rules of $d$-separation to the path $p_1$.\nThe path $p_1 = (X, Z, Y)$ consists of the edges $X \\to Z$ and $Y \\to Z$. When viewed as a segment of a path, this structure is $X \\to Z \\leftarrow Y$.\nIn the terminology of $d$-separation, the node $Z$ is a **collider** on this path, because two arrowheads meet at $Z$.\n\nThe rules for a path being blocked by a conditioning set $S$ are as follows:\nA path is blocked if it contains a node $N$ such that:\n(a) $N$ is a chain node ($\\to N \\to$) or a fork node ($\\leftarrow N \\to$) on the path, and $N \\in S$.\n(b) $N$ is a collider node ($\\to N \\leftarrow$) on the path, and neither $N$ nor any of its descendants are in $S$.\n\nWe are asked to determine if the path $p_1$ is blocked when conditioning on the set $S = \\{Z\\}$.\nThe path $p_1$ contains the collider node $Z$. We apply rule (b).\nFor the path to be blocked by the collider, the condition is that neither the collider itself ($Z$) nor any of its descendants must be in the conditioning set $S$.\nIn this case, the conditioning set is $S = \\{Z\\}$. The collider node $Z$ is itself in the conditioning set.\nTherefore, the condition for the path to be blocked is not met. A path that is not blocked is said to be open or unblocked.\nConditioning on a collider (or one of its descendants) opens the path of association.\nSo, the path $p_1$ is **not blocked** when conditioning on $Z$.\n\nStep 3: Compute the value of $D$.\nThe indicator function $I(p)$ is defined as $I(p)=1$ if the path $p$ is blocked, and $I(p)=0$ otherwise.\nFor our path $p_1$, since it is not blocked by conditioning on $Z$, the value of the indicator is $I(p_1) = 0$.\n\nThe quantity $D$ is the product of these indicator values over all simple paths between $X$ and $Y$.\n$$\nD = \\prod_{p \\in \\mathcal{P}(X,Y)} I(p)\n$$\nSince $\\mathcal{P}(X,Y) = \\{p_1\\}$, this product simplifies to:\n$$\nD = I(p_1)\n$$\nSubstituting the value we found for $I(p_1)$:\n$$\nD = 0\n$$\nThe final value is $0$.", "answer": "$$\\boxed{0}$$", "id": "4557701"}, {"introduction": "Building on the rules of d-separation, this practice problem moves from abstract path-blocking to a quantitative demonstration of one of causal inference's most counter-intuitive phenomena: collider bias. While two independent causes are not associated with each other, conditioning on their common effect can induce a spurious statistical relationship. This exercise [@problem_id:4557747] asks you to derive this induced covariance from first principles within a linear system, providing a concrete mathematical understanding of why conditioning on a collider—a common practice in traditional regression analysis—can be so problematic for causal claims.", "problem": "Consider a bioinformatics and medical data analytics pipeline where gene expression $X$ and a clinical outcome severity score $Y$ are analyzed in the presence of a measured quality control variable $C$ and two unobserved biological factors $U$ and $V$. Assume the following Directed Acyclic Graph (DAG): $X \\leftarrow U \\to C \\leftarrow V \\to Y$, with no direct causal edge from $X$ to $Y$ $(X \\not\\to Y)$, and with $U$ and $V$ unobserved. The variable $U$ represents an unmeasured cell-type composition factor that influences both $X$ and $C$, while $V$ represents an unmeasured inflammatory state that influences both $C$ and $Y$. Let the system be governed by a linear-Gaussian Structural Equation Model (SEM) with independent exogenous components:\n- $U \\sim \\mathcal{N}(0,\\sigma_{U}^{2})$ and $V \\sim \\mathcal{N}(0,\\sigma_{V}^{2})$, with $U$ independent of $V$.\n- $\\varepsilon_{X} \\sim \\mathcal{N}(0,\\sigma_{X}^{2})$, $\\varepsilon_{C} \\sim \\mathcal{N}(0,\\sigma_{C}^{2})$, and $\\varepsilon_{Y} \\sim \\mathcal{N}(0,\\sigma_{Y}^{2})$, mutually independent and independent of $U$ and $V$.\n- Linear structural equations: $X = \\alpha U + \\varepsilon_{X}$, $C = \\beta U + \\gamma V + \\varepsilon_{C}$, and $Y = \\delta V + \\varepsilon_{Y}$.\n\nStarting from the core definitions of a Directed Acyclic Graph and the properties of multivariate Gaussian models, derive the unconditional covariance $\\operatorname{Cov}(X,Y)$ and then, by quantifying the effect of conditioning on the collider $C$, derive the conditional covariance $\\operatorname{Cov}(X,Y \\mid C)$ in terms of the parameters $\\alpha$, $\\beta$, $\\gamma$, $\\delta$, $\\sigma_{U}^{2}$, $\\sigma_{V}^{2}$, and $\\sigma_{C}^{2}$. Your derivation must be grounded in first principles (conditional distributions of multivariate normal variables and linear projection), without assuming any pre-stated shortcut formulas. Provide your final answer as a single closed-form analytic expression for $\\operatorname{Cov}(X,Y \\mid C)$. No numerical approximation is required, and no units are needed.", "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in the established theory of structural equation models (SEMs) and multivariate Gaussian distributions, which are standard tools in causal inference and biostatistics. The problem is well-posed, with a complete and consistent set of definitions and parameters, ensuring that a unique, analytical solution can be derived. The language is objective and formal. Therefore, we proceed with a full derivation.\n\nThe problem describes a system of random variables $(X, Y, C, U, V)$. The variables $X$, $Y$, and $C$ are defined as linear combinations of the independent, zero-mean Gaussian random variables $U$, $V$, $\\varepsilon_{X}$, $\\varepsilon_{C}$, and $\\varepsilon_{Y}$. A linear combination of Gaussian variables is itself Gaussian. Consequently, the vector of observable variables $(X, Y, C)^T$ follows a multivariate normal distribution with a mean vector of zero, since all constituent variables have zero mean. We denote this vector by $\\mathbf{Z} = (X, Y, C)^T$. The properties of this system are entirely determined by its covariance matrix, $\\Sigma_{\\mathbf{Z}}$.\n\nFirst, we derive the unconditional covariance $\\operatorname{Cov}(X,Y)$ as requested. This is an element of the covariance matrix $\\Sigma_{\\mathbf{Z}}$. The elements of this matrix are computed using the linearity of the covariance operator and the specified independence conditions.\n\nThe structural equations are:\n$X = \\alpha U + \\varepsilon_{X}$\n$Y = \\delta V + \\varepsilon_{Y}$\n$C = \\beta U + \\gamma V + \\varepsilon_{C}$\n\nThe variances and covariances of the exogenous variables are given as:\n$\\operatorname{Var}(U) = \\sigma_{U}^{2}$, $\\operatorname{Var}(V) = \\sigma_{V}^{2}$, $\\operatorname{Var}(\\varepsilon_{X}) = \\sigma_{X}^{2}$, $\\operatorname{Var}(\\varepsilon_{C}) = \\sigma_{C}^{2}$, $\\operatorname{Var}(\\varepsilon_{Y}) = \\sigma_{Y}^{2}$.\nAll exogenous variables ($U, V, \\varepsilon_{X}, \\varepsilon_{C}, \\varepsilon_{Y}$) are mutually independent.\n\nWe compute the elements of the covariance matrix $\\Sigma_{\\mathbf{Z}}$:\nThe variances (diagonal elements):\n$\\operatorname{Var}(X) = \\operatorname{Cov}(\\alpha U + \\varepsilon_{X}, \\alpha U + \\varepsilon_{X}) = \\alpha^2 \\operatorname{Var}(U) + \\operatorname{Var}(\\varepsilon_{X}) = \\alpha^2\\sigma_{U}^{2} + \\sigma_{X}^{2}$.\n$\\operatorname{Var(Y)} = \\operatorname{Cov}(\\delta V + \\varepsilon_{Y}, \\delta V + \\varepsilon_{Y}) = \\delta^2 \\operatorname{Var}(V) + \\operatorname{Var}(\\varepsilon_{Y}) = \\delta^2\\sigma_{V}^{2} + \\sigma_{Y}^{2}$.\n$\\operatorname{Var}(C) = \\operatorname{Cov}(\\beta U + \\gamma V + \\varepsilon_{C}, \\beta U + \\gamma V + \\varepsilon_{C}) = \\beta^2 \\operatorname{Var}(U) + \\gamma^2 \\operatorname{Var}(V) + \\operatorname{Var}(\\varepsilon_{C}) = \\beta^2\\sigma_{U}^{2} + \\gamma^2\\sigma_{V}^{2} + \\sigma_{C}^{2}$.\n\nThe covariances (off-diagonal elements):\n$\\operatorname{Cov}(X, Y) = \\operatorname{Cov}(\\alpha U + \\varepsilon_{X}, \\delta V + \\varepsilon_{Y}) = \\alpha\\delta\\operatorname{Cov}(U,V) + \\alpha\\operatorname{Cov}(U,\\varepsilon_{Y}) + \\delta\\operatorname{Cov}(\\varepsilon_{X},V) + \\operatorname{Cov}(\\varepsilon_{X},\\varepsilon_{Y})$.\nDue to independence of all exogenous variables, all these covariance terms are zero.\n$$ \\operatorname{Cov}(X, Y) = 0 $$\nThis result is consistent with the d-separation criterion from the theory of Directed Acyclic Graphs. The only path between $X$ and $Y$ is $X \\leftarrow U \\to C \\leftarrow V \\to Y$, which is blocked by the collider node $C$. Thus, $X$ and $Y$ are unconditionally independent.\n\n$\\operatorname{Cov}(X, C) = \\operatorname{Cov}(\\alpha U + \\varepsilon_{X}, \\beta U + \\gamma V + \\varepsilon_{C}) = \\alpha\\beta\\operatorname{Var}(U) = \\alpha\\beta\\sigma_{U}^{2}$.\n$\\operatorname{Cov}(Y, C) = \\operatorname{Cov}(\\delta V + \\varepsilon_{Y}, \\beta U + \\gamma V + \\varepsilon_{C}) = \\gamma\\delta\\operatorname{Var}(V) = \\gamma\\delta\\sigma_{V}^{2}$.\n\nThe full covariance matrix for $\\mathbf{Z} = (X, Y, C)^T$ is therefore:\n$$\n\\Sigma_{\\mathbf{Z}} =\n\\begin{pmatrix}\n\\alpha^2\\sigma_{U}^{2} + \\sigma_{X}^{2} & 0 & \\alpha\\beta\\sigma_{U}^{2} \\\\\n0 & \\delta^2\\sigma_{V}^{2} + \\sigma_{Y}^{2} & \\gamma\\delta\\sigma_{V}^{2} \\\\\n\\alpha\\beta\\sigma_{U}^{2} & \\gamma\\delta\\sigma_{V}^{2} & \\beta^2\\sigma_{U}^{2} + \\gamma^2\\sigma_{V}^{2} + \\sigma_{C}^{2}\n\\end{pmatrix}\n$$\nTo find the conditional covariance $\\operatorname{Cov}(X,Y \\mid C)$, we use the formula for the conditional covariance matrix of a partitioned multivariate normal vector. Let us partition the vector $\\mathbf{Z}$ into $\\mathbf{Z}_1 = (X, Y)^T$ and $\\mathbf{Z}_2 = (C)$. The covariance matrix $\\Sigma_{\\mathbf{Z}}$ is partitioned conformably:\n$$\n\\Sigma_{\\mathbf{Z}} = \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}\n$$\nwhere\n$\\Sigma_{11} = \\operatorname{Cov}(\\mathbf{Z}_1) = \\begin{pmatrix} \\operatorname{Var}(X) & \\operatorname{Cov}(X,Y) \\\\ \\operatorname{Cov}(Y,X) & \\operatorname{Var}(Y) \\end{pmatrix} = \\begin{pmatrix} \\alpha^2\\sigma_{U}^{2} + \\sigma_{X}^{2} & 0 \\\\ 0 & \\delta^2\\sigma_{V}^{2} + \\sigma_{Y}^{2} \\end{pmatrix}$\n\n$\\Sigma_{12} = \\operatorname{Cov}(\\mathbf{Z}_1, \\mathbf{Z}_2) = \\begin{pmatrix} \\operatorname{Cov}(X,C) \\\\ \\operatorname{Cov}(Y,C) \\end{pmatrix} = \\begin{pmatrix} \\alpha\\beta\\sigma_{U}^{2} \\\\ \\gamma\\delta\\sigma_{V}^{2} \\end{pmatrix}$\n\n$\\Sigma_{21} = \\operatorname{Cov}(\\mathbf{Z}_2, \\mathbf{Z}_1) = \\Sigma_{12}^T = \\begin{pmatrix} \\alpha\\beta\\sigma_{U}^{2} & \\gamma\\delta\\sigma_{V}^{2} \\end{pmatrix}$\n\n$\\Sigma_{22} = \\operatorname{Var}(\\mathbf{Z}_2) = \\operatorname{Var}(C) = \\beta^2\\sigma_{U}^{2} + \\gamma^2\\sigma_{V}^{2} + \\sigma_{C}^{2}$\n\nFor a multivariate normal distribution, the covariance matrix of $\\mathbf{Z}_1$ conditional on $\\mathbf{Z}_2$ is given by the Schur complement of $\\Sigma_{22}$ in $\\Sigma_{\\mathbf{Z}}$:\n$$\n\\operatorname{Cov}(\\mathbf{Z}_1 \\mid \\mathbf{Z}_2) = \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\n$$\nThe conditional covariance matrix for $(X,Y)$ given $C$ is $\\operatorname{Cov}((X,Y)^T \\mid C) = \\begin{pmatrix} \\operatorname{Var}(X \\mid C) & \\operatorname{Cov}(X,Y \\mid C) \\\\ \\operatorname{Cov}(Y,X \\mid C) & \\operatorname{Var}(Y \\mid C) \\end{pmatrix}$.\nSince $C$ is a scalar variable, $\\Sigma_{22}^{-1}$ is simply the reciprocal of the variance of $C$:\n$\\Sigma_{22}^{-1} = \\frac{1}{\\operatorname{Var}(C)} = \\frac{1}{\\beta^2\\sigma_{U}^{2} + \\gamma^2\\sigma_{V}^{2} + \\sigma_{C}^{2}}$.\n\nNow we compute the term $\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}$:\n$$\n\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21} = \\frac{1}{\\operatorname{Var}(C)} \\begin{pmatrix} \\alpha\\beta\\sigma_{U}^{2} \\\\ \\gamma\\delta\\sigma_{V}^{2} \\end{pmatrix} \\begin{pmatrix} \\alpha\\beta\\sigma_{U}^{2} & \\gamma\\delta\\sigma_{V}^{2} \\end{pmatrix}\n$$\n$$\n= \\frac{1}{\\operatorname{Var}(C)} \\begin{pmatrix} (\\alpha\\beta\\sigma_{U}^{2})^2 & (\\alpha\\beta\\sigma_{U}^{2})(\\gamma\\delta\\sigma_{V}^{2}) \\\\ (\\gamma\\delta\\sigma_{V}^{2})(\\alpha\\beta\\sigma_{U}^{2}) & (\\gamma\\delta\\sigma_{V}^{2})^2 \\end{pmatrix}\n$$\n$$\n= \\frac{1}{\\beta^2\\sigma_{U}^{2} + \\gamma^2\\sigma_{V}^{2} + \\sigma_{C}^{2}} \\begin{pmatrix} \\alpha^2\\beta^2(\\sigma_{U}^{2})^2 & \\alpha\\beta\\gamma\\delta\\sigma_{U}^{2}\\sigma_{V}^{2} \\\\ \\alpha\\beta\\gamma\\delta\\sigma_{U}^{2}\\sigma_{V}^{2} & \\gamma^2\\delta^2(\\sigma_{V}^{2})^2 \\end{pmatrix}\n$$\nThe conditional covariance matrix is then:\n$$\n\\operatorname{Cov}((X,Y)^T \\mid C) = \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\n$$\n$$\n= \\begin{pmatrix} \\alpha^2\\sigma_{U}^{2} + \\sigma_{X}^{2} & 0 \\\\ 0 & \\delta^2\\sigma_{V}^{2} + \\sigma_{Y}^{2} \\end{pmatrix} - \\frac{1}{\\beta^2\\sigma_{U}^{2} + \\gamma^2\\sigma_{V}^{2} + \\sigma_{C}^{2}} \\begin{pmatrix} \\alpha^2\\beta^2(\\sigma_{U}^{2})^2 & \\alpha\\beta\\gamma\\delta\\sigma_{U}^{2}\\sigma_{V}^{2} \\\\ \\alpha\\beta\\gamma\\delta\\sigma_{U}^{2}\\sigma_{V}^{2} & \\gamma^2\\delta^2(\\sigma_{V}^{2})^2 \\end{pmatrix}\n$$\nWe are asked to find $\\operatorname{Cov}(X,Y \\mid C)$, which is the off-diagonal element of this resulting $2 \\times 2$ matrix.\n$$\n\\operatorname{Cov}(X,Y \\mid C) = 0 - \\frac{\\alpha\\beta\\gamma\\delta\\sigma_{U}^{2}\\sigma_{V}^{2}}{\\beta^2\\sigma_{U}^{2} + \\gamma^2\\sigma_{V}^{2} + \\sigma_{C}^{2}}\n$$\nThis expression quantifies the covariance between $X$ and $Y$ induced by conditioning on the common effect (collider) $C$. While $X$ and $Y$ are unconditionally independent, they become conditionally dependent. This phenomenon is known as collider stratification bias. The derived expression is non-zero as long as all path coefficients ($\\alpha, \\beta, \\gamma, \\delta$) and the variances of the unobserved common causes ($\\sigma_U^2, \\sigma_V^2$) are non-zero.", "answer": "$$\n\\boxed{- \\frac{\\alpha \\beta \\gamma \\delta \\sigma_{U}^{2} \\sigma_{V}^{2}}{\\beta^{2} \\sigma_{U}^{2} + \\gamma^{2} \\sigma_{V}^{2} + \\sigma_{C}^{2}}}\n$$", "id": "4557747"}, {"introduction": "After learning to use DAGs to identify potential sources of bias, the next logical step is to use them to estimate causal effects correctly. This exercise demonstrates a powerful technique for doing so: the g-computation formula (also known as standardization). By identifying a valid adjustment set using the back-door criterion, we can simulate an intervention and predict its effect using only observational data. This problem [@problem_id:4557812] guides you through the process of deriving and applying the g-computation estimand, showing how to calculate what the average outcome would be if everyone in the population had received a specific treatment level, thereby isolating the causal effect from confounding.", "problem": "Consider a bioinformatics and medical data analytics cohort study of a kinase inhibitor where the treatment dosage is represented by a continuous variable $X$ (log-dosage), the outcome $Y$ is a dimensionless standardized tumor growth index, and $Z$ is a binary indicator of baseline comorbidity ($Z=1$ for present, $Z=0$ for absent). The causal relationships are described by a Directed Acyclic Graph (DAG): $Z \\rightarrow X$, $Z \\rightarrow Y$, $X \\rightarrow Y$, and $X \\rightarrow M \\rightarrow Y$, where $M$ is a mediator capturing drug-induced signaling downstream of $X$. Assume $Z$ is a valid adjustment set for the causal effect of $X$ on $Y$ according to the back-door criterion, and that the standard identification conditions of consistency, conditional exchangeability given $Z$, and positivity hold.\n\nThe following statistical summaries are plausibly obtained from observational data:\n- The observational distribution of $Z$ is $\\mathbb{P}(Z=1)=\\frac{1}{3}$ and $\\mathbb{P}(Z=0)=\\frac{2}{3}$.\n- The conditional expected outcome model (a saturated linear model in $X$, $Z$, and their interaction, compatible with the DAG and not conditioning on the mediator $M$) is\n$$\n\\mathbb{E}(Y\\mid X,Z)=\\theta_{0}+\\theta_{1}X+\\theta_{2}Z+\\theta_{3}XZ,\n$$\nwith parameter values $\\theta_{0}=10$, $\\theta_{1}=2$, $\\theta_{2}=5$, and $\\theta_{3}=1$.\n\nStarting from the core definitions of the do-operator in causal modeling and the law of total expectation, derive the g-computation estimand for the interventional mean $\\mathbb{E}\\big(Y\\mid \\mathrm{do}(X=x)\\big)$ under the intervention $X=x$. Then, using the provided observational summaries, evaluate this estimand at $x=2$. Round your final numerical answer to four significant figures. Briefly explain, from first principles, how this estimand would be estimated from observational data without access to randomized experiments, ensuring that no mediator is included in the adjustment set and that the identification conditions are met.", "solution": "We begin by recalling the core definitions and assumptions required for identification of causal effects in Directed Acyclic Graphs (DAGs). A valid adjustment set $Z$ for the causal effect of $X$ on $Y$ satisfies the back-door criterion, meaning that $Z$ blocks all back-door paths from $X$ to $Y$ and does not include any descendants of $X$. Under the identification conditions of consistency, conditional exchangeability given $Z$, and positivity, the interventional distribution under $\\mathrm{do}(X=x)$ is identified via the g-computation formula, which is grounded in the do-operator and the law of total expectation. Specifically, conditioning on $Z$ suffices to remove confounding, yielding that the interventional mean is obtained by averaging the conditional expectation of $Y$ given $X=x$ over the observational distribution of $Z$.\n\nFormally, the target estimand for the interventional mean under the intervention $X=x$ is derived as follows. By consistency, $Y^{x}$ equals the observed $Y$ when $X$ is set to $x$. By conditional exchangeability given $Z$, we have $Y^{x}\\perp\\!\\!\\!\\perp X\\mid Z$. Positivity requires that $\\mathbb{P}(X=x\\mid Z=z)>0$ whenever $\\mathbb{P}(Z=z)>0$. Applying the law of total expectation to the interventional distribution, we obtain\n$$\n\\mathbb{E}\\big(Y\\mid \\mathrm{do}(X=x)\\big)=\\int \\mathbb{E}\\big(Y\\mid X=x,Z=z\\big)\\, \\mathrm{d}F_{Z}(z),\n$$\nwhere $F_{Z}$ denotes the observational distribution of $Z$. In the discrete case for $Z$, this reduces to a finite sum across the support of $Z$.\n\nGiven the provided model,\n$$\n\\mathbb{E}(Y\\mid X,Z)=\\theta_{0}+\\theta_{1}X+\\theta_{2}Z+\\theta_{3}XZ,\n$$\nwith $\\theta_{0}=10$, $\\theta_{1}=2$, $\\theta_{2}=5$, and $\\theta_{3}=1$, we compute the conditional expectations needed for the g-computation:\n\n1. For $Z=0$ and $X=x$,\n$$\n\\mathbb{E}(Y\\mid X=x,Z=0)=\\theta_{0}+\\theta_{1}x+\\theta_{2}\\cdot 0+\\theta_{3}\\cdot x\\cdot 0=10+2x.\n$$\n\n2. For $Z=1$ and $X=x$,\n$$\n\\mathbb{E}(Y\\mid X=x,Z=1)=\\theta_{0}+\\theta_{1}x+\\theta_{2}\\cdot 1+\\theta_{3}\\cdot x\\cdot 1=10+2x+5+x=15+3x.\n$$\n\nThe observational distribution of $Z$ is $\\mathbb{P}(Z=1)=\\frac{1}{3}$ and $\\mathbb{P}(Z=0)=\\frac{2}{3}$. Therefore, the interventional mean is\n$$\n\\mathbb{E}\\big(Y\\mid \\mathrm{do}(X=x)\\big)=\\mathbb{E}\\big(Y\\mid X=x,Z=0\\big)\\,\\mathbb{P}(Z=0)+\\mathbb{E}\\big(Y\\mid X=x,Z=1\\big)\\,\\mathbb{P}(Z=1).\n$$\nEvaluating at $x=2$:\n- $\\mathbb{E}(Y\\mid X=2,Z=0)=10+2\\cdot 2=14$,\n- $\\mathbb{E}(Y\\mid X=2,Z=1)=15+3\\cdot 2=21$.\n\nHence,\n$$\n\\mathbb{E}\\big(Y\\mid \\mathrm{do}(X=2)\\big)=14\\cdot \\frac{2}{3}+21\\cdot \\frac{1}{3}=\\frac{28}{3}+\\frac{21}{3}=\\frac{49}{3}.\n$$\nTo four significant figures, this is\n$$\n\\mathbb{E}\\big(Y\\mid \\mathrm{do}(X=2)\\big)\\approx 16.33.\n$$\n\nWe now explain how this estimand is estimated from observational data. Under the DAG with $Z$ as a valid adjustment set and with the identification assumptions satisfied, the parametric g-formula (g-computation) estimator proceeds by modeling the conditional expectation of $Y$ given $(X,Z)$ from observational data and then standardizing (i.e., averaging) the model-predicted outcomes over the empirical distribution of $Z$ at the intervention $X=x$. Let $\\hat{m}(x,z)$ denote an estimate of $\\mathbb{E}(Y\\mid X=x,Z=z)$, for example obtained via linear regression using the specified model, yielding estimated parameters $(\\hat{\\theta}_{0},\\hat{\\theta}_{1},\\hat{\\theta}_{2},\\hat{\\theta}_{3})$. Given an independent and identically distributed sample $\\{(Y_{i},X_{i},Z_{i}):i=1,\\dots,n\\}$, the plug-in estimator of the interventional mean is\n$$\n\\hat{\\psi}(x)=\\frac{1}{n}\\sum_{i=1}^{n}\\hat{m}(x,Z_{i}).\n$$\nThis estimator is a sample analog of the law of total expectation applied to the observational distribution of $Z$. Crucially, we do not condition on the mediator $M$ in the adjustment set to avoid biasing the total effect by blocking part of the causal pathway. Under correct model specification for $\\mathbb{E}(Y\\mid X,Z)$, consistency, conditional exchangeability given $Z$, and positivity, $\\hat{\\psi}(x)$ is a consistent estimator of $\\mathbb{E}\\big(Y\\mid \\mathrm{do}(X=x)\\big)$. In practice, flexible nonparametric or machine learning models can be used for $\\hat{m}(x,z)$ along with techniques such as cross-fitting, provided the same identification conditions are respected and the empirical distribution of $Z$ is used for standardization. Finally, diagnostics should confirm positivity, i.e., that for observed $Z=z$ values, treatment levels near $x$ have support in the data, ensuring meaningful extrapolation.", "answer": "$$\\boxed{16.33}$$", "id": "4557812"}]}