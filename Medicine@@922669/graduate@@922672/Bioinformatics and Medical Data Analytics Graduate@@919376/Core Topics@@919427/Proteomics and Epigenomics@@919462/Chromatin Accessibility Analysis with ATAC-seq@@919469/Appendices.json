{"hands_on_practices": [{"introduction": "A successful ATAC-seq analysis begins with robust experimental design and rigorous quality control. Before diving into complex algorithms, it's crucial to understand the fundamental metrics that determine data quality, such as library complexity and sequencing depth. This exercise [@problem_id:4545808] provides a practical scenario to calculate the effective number of unique DNA fragments from raw sequencing reads, a critical first step in assessing whether an experiment has sufficient power for downstream analyses like peak calling.", "problem": "A bulk Assay for Transposase-Accessible Chromatin using sequencing (ATAC-seq) experiment is planned to achieve a target of $30 \\times 10^{6}$ paired-end read pairs that pass standard filtering and map uniquely to the reference genome. The estimated library duplication rate after duplicate marking is $d = 0.20$. In bulk ATAC-seq, each read pair corresponds to one physical DNA fragment, and duplicate-marked read pairs are considered non-informative for downstream signal quantification and peak calling. Using only fundamental definitions of duplication rate and fragment counting, derive an expression for the expected number of unique fragments retained for analysis and compute its value. Then, starting from foundational considerations about signal accumulation and detection in high-throughput sequencing (for example, modeling fragment sampling as a stochastic process over accessible loci and the need for sufficient counts to distinguish true accessibility signal from background), discuss whether this unique fragment depth is adequate for standard bulk ATAC-seq peak calling in a typical mammalian genome under widely accepted practice. Provide the computed unique fragment count as your final answer. Express the final count as an exact value; no rounding is required.", "solution": "The problem requires a two-part response: first, a calculation of the number of unique fragments based on provided sequencing metrics, and second, a critical evaluation of whether this number is sufficient for standard analysis.\n\nThe validation of the problem statement is the first procedural step.\n\n**Step 1: Extract Givens**\n- Total number of paired-end read pairs passing filtering, $N_{total} = 30 \\times 10^{6}$.\n- Estimated library duplication rate, $d = 0.20$.\n- Definition: Each read pair corresponds to one physical DNA fragment.\n- Definition: Duplicate-marked read pairs are considered non-informative.\n- Objective 1: Derive an expression for the expected number of unique fragments and compute its value.\n- Objective 2: Discuss the adequacy of this number for standard bulk ATAC-seq peak calling in a typical mammalian genome.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, as it uses standard terminology and realistic parameters from the field of bioinformatics and genomics (ATAC-seq, duplication rate, read depth). The concepts are well-defined and the question is self-contained and internally consistent. It asks for a straightforward calculation followed by a discussion based on established scientific practice. The problem is well-posed, objective, and does not violate any of the criteria for invalidity.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A complete solution will be provided.\n\n**Part 1: Derivation and Computation of Unique Fragment Count**\n\nWe begin from the fundamental definitions provided. Let $N_{total}$ represent the total number of paired-end read pairs that pass standard filtering and mapping. In this problem, we are given $N_{total} = 30 \\times 10^{6}$.\n\nThe total set of read pairs can be partitioned into two disjoint sets: those that are unique and those that are duplicates. Let $N_{unique}$ be the number of unique read pairs and $N_{dup}$ be the number of duplicate read pairs. By definition, the total number of reads is the sum of these two quantities:\n$$N_{total} = N_{unique} + N_{dup}$$\n\nThe duplication rate, $d$, is defined as the fraction of duplicate read pairs relative to the total number of read pairs.\n$$d = \\frac{N_{dup}}{N_{total}}$$\nWe are given $d = 0.20$.\n\nThe problem states that duplicate-marked read pairs are non-informative and are removed prior to downstream analysis. The remaining read pairs are the unique ones, and each corresponds to a single, unique DNA fragment that was successfully sequenced. Therefore, the number of unique fragments retained for analysis is equal to the number of unique read pairs, $N_{unique}$.\n\nOur goal is to find an expression for $N_{unique}$. We can rearrange the first equation to express $N_{unique}$ in terms of $N_{total}$ and $N_{dup}$:\n$$N_{unique} = N_{total} - N_{dup}$$\nNext, we can express $N_{dup}$ in terms of $N_{total}$ and $d$ using the definition of the duplication rate:\n$$N_{dup} = d \\times N_{total}$$\nSubstituting this expression for $N_{dup}$ into the equation for $N_{unique}$, we arrive at the general expression for the number of unique fragments:\n$$N_{unique} = N_{total} - (d \\times N_{total})$$\n$$N_{unique} = N_{total} (1 - d)$$\nThis is the derived expression for the expected number of unique fragments retained for analysis.\n\nNow, we compute the value using the givens: $N_{total} = 30 \\times 10^{6}$ and $d = 0.20$.\n$$N_{unique} = (30 \\times 10^{6}) \\times (1 - 0.20)$$\n$$N_{unique} = (30 \\times 10^{6}) \\times 0.80$$\n$$N_{unique} = 24 \\times 10^{6}$$\nThus, the expected number of unique fragments retained for analysis is $24$ million.\n\n**Part 2: Discussion of Adequacy for Peak Calling**\n\nThe adequacy of a given sequencing depth for ATAC-seq peak calling must be evaluated based on foundational principles of signal detection in high-throughput sequencing and widely accepted community standards.\n\nThe fundamental principle is that detecting a feature, such as an accessible chromatin region (a \"peak\"), requires sequencing enough fragments from that locus to distinguish its signal from background noise. The process can be conceptualized as a stochastic sampling of fragments. The total number of unique fragments, $N_{unique}$, represents the total number of successful samples. A higher $N_{unique}$ increases the probability of sampling from all true accessible sites, including those with lower intrinsic accessibility, and provides the statistical power to declare them as significant peaks.\n\nFor a \"typical mammalian genome,\" such as human or mouse (approximately $3 \\times 10^{9}$ base pairs), the number of accessible sites is estimated to be in the hundreds of thousands (e.g., $100,000$ to $500,000$ sites). The $N_{unique}$ fragments are distributed non-uniformly across these sites and the background. The adequacy of the depth determines the sensitivity and specificity of peak calling.\n\nA calculated depth of $N_{unique} = 24 \\times 10^{6}$ fragments is a substantial amount of data. It is certainly sufficient to identify the most prominent and highly accessible chromatin regions, such as active promoters and strong enhancers. An analysis with this depth would not be a failure; it would produce a valid, albeit potentially incomplete, map of the accessible genome.\n\nHowever, the question refers to \"standard\" bulk ATAC-seq under \"widely accepted practice.\" This implies a comparison to benchmarks established by major genomics consortia, such as the Encyclopedia of DNA Elements (ENCODE). The ENCODE consortium has performed extensive empirical analysis to determine the sequencing depth required for robust and reproducible characterization of the regulatory landscape.\n\nAccording to ENCODE guidelines, the target for a high-quality bulk ATAC-seq experiment in human or mouse samples is a minimum of $50 \\times 10^{6}$ unique, non-mitochondrial, non-duplicate read pairs (i.e., $N_{unique} \\ge 50 \\times 10^{6}$). This recommendation is based on saturation analyses showing that depths below this level may fail to capture a significant fraction of true accessible sites, particularly weaker or cell-type-specific enhancers. An experiment with $24 \\times 10^{6}$ unique fragments has not reached saturation, meaning that further sequencing would continue to identify a substantial number of new peaks.\n\nIn conclusion, while $24 \\times 10^{6}$ unique fragments provide enough data for a preliminary or exploratory analysis, this depth is considered suboptimal and does not meet the current rigorous standards for a comprehensive and high-quality bulk ATAC-seq analysis in a mammalian system. An experiment with this depth would have a higher false-negative rate for peak discovery compared to one adhering to the $\\ge 50 \\times 10^{6}$ unique fragment guideline. It would be considered adequate for identifying strong signals but inadequate for sensitive, genome-wide discovery of all regulatory elements.", "answer": "$$\\boxed{24 \\times 10^{6}}$$", "id": "4545808"}, {"introduction": "The primary goal of ATAC-seq is to map the landscape of open chromatin across the genome. This is achieved through a computational process called 'peak calling,' which identifies regions with a statistically significant enrichment of transposase cut sites. In this hands-on coding practice [@problem_id:4545839], you will implement a simplified peak-calling algorithm from first principles, giving you a deep appreciation for the statistical foundations—like the Poisson model and multiple testing correction—that underpin modern bioinformatics tools.", "problem": "You are given integer coverage counts across a genomic region from Assay for Transposase-Accessible Chromatin using sequencing (ATAC-seq) and a corresponding local background rate estimate at each position. Assume the following fundamental base: under the null hypothesis of no enrichment, the per-position coverage counts are independent samples from a Poisson process with rate equal to the local background, and the sum of independent Poisson variables is itself Poisson with rate equal to the sum of the component rates. To test for local enrichment, consider fixed-width windows that slide across the region with stride $1$. For each window, compute the observed summed coverage and the expected summed background rate, then compute a one-sided tail probability under the null. Finally, adjust these $p$-values to control the False Discovery Rate using the Benjamini–Hochberg (BH) procedure.\n\nDefinitions and requirements:\n- Let $c_i$ denote the observed coverage at position $i$ and let $\\lambda_i$ denote the local background rate at position $i$, for $i$ ranging from $0$ to $n-1$.\n- For a window width $w$, define the window starting at index $s$ as the set of positions $\\{s, s+1, \\ldots, s+w-1\\}$, with $s$ ranging from $0$ to $n-w$. The observed window sum is $K_s = \\sum_{i=s}^{s+w-1} c_i$ and the expected window rate is $\\Lambda_s = \\sum_{i=s}^{s+w-1} \\lambda_i$.\n- Under the null, model $K_s$ as a realization from a Poisson random variable with rate $\\Lambda_s$. Use the one-sided tail probability $p_s = \\Pr[X \\geq K_s]$ where $X$ is Poisson with rate $\\Lambda_s$ and the inequality is interpreted on the support of $X$.\n- Perform the Benjamini–Hochberg (BH) multiple testing adjustment across all windows to obtain adjusted $p$-values. Treat overlapping windows as separate hypotheses.\n- Identify \"significant peaks\" as windows whose BH-adjusted $p$-value is less than or equal to a specified level $q$.\n\nYour task is to write a complete, runnable program that, for each provided test case, computes the list of indices $s$ of significant windows and the corresponding BH-adjusted $p$-values. Report the BH-adjusted $p$-values rounded to six decimal places. All counts are unitless and there are no physical units involved.\n\nTest suite:\n- Case $1$ (happy path):\n  - Coverage vector $c$: $[0,1,0,1,2,3,1,0,1,2,6,7,8,6,5,2,1,1,0,1,0,5,6,7,5,1,1,0,0,1]$\n  - Background vector $\\lambda$: $[1.0,1.0,1.2,1.0,1.0,1.1,1.0,0.9,1.1,1.2,1.3,1.2,1.1,1.2,1.1,1.0,1.0,1.0,1.0,1.0,1.1,1.2,1.3,1.2,1.1,1.0,1.0,1.0,1.0,1.0]$\n  - Window width $w$: $5$\n  - Significance level $q$: $0.05$\n- Case $2$ (boundary window width $1$):\n  - Coverage vector $c$: $[0,0,1,0,2,3,0,5,0,1,0,4]$\n  - Background vector $\\lambda$: $[0.1,0.1,0.2,0.1,0.2,0.3,0.1,0.2,0.1,0.2,0.1,0.2]$\n  - Window width $w$: $1$\n  - Significance level $q$: $0.05$\n- Case $3$ (window spanning the entire region):\n  - Coverage vector $c$: $[1,3,2,1,2,3,1,2,2,1,3,2,2,1,2,3,2,1,1,2]$\n  - Background vector $\\lambda$: $[2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0]$\n  - Window width $w$: $20$\n  - Significance level $q$: $0.10$\n- Case $4$ (high background, no expected peaks):\n  - Coverage vector $c$: $[1,0,2,1,0,1,1,0,2,0,1,1,0,1,0,1,1,0]$\n  - Background vector $\\lambda$: $[5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0]$\n  - Window width $w$: $4$\n  - Significance level $q$: $0.05$\n\nAlgorithmic constraints:\n- Compute $K_s$ and $\\Lambda_s$ for all valid $s$.\n- Use a standard numerical routine for the Poisson one-sided tail probability.\n- Compute Benjamini–Hochberg adjusted $p$-values across all windows in the case, ensuring monotonicity of the adjusted values when mapped back to the original order.\n- Return the indices of windows with adjusted $p$-values $\\leq q$ and the corresponding adjusted $p$-values, rounded to six decimal places, sorted by the window start index $s$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list with two elements: the first is the list of significant window start indices and the second is the list of their BH-adjusted $p$-values rounded to six decimal places. For example, the output should look like $[[[s\\_1, s\\_2], [p\\_1, p\\_2]], \\ldots]$ with all numbers in standard decimal notation.", "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded, well-posed, and objective. It presents a standard bioinformatics task—peak calling in ATAC-seq data—based on established statistical principles. All data, definitions, and constraints are provided, forming a self-contained and solvable problem.\n\nThe problem requires a computational procedure to identify statistically significant regions of chromatin accessibility from ATAC-seq coverage data. This is achieved by testing for local enrichment of sequencing reads against an estimated background rate. The core of the solution involves three steps: sliding window signal aggregation, Poisson-based statistical testing for each window, and a multiple testing correction to control the False Discovery Rate (FDR).\n\nFirst, we formalize the statistical foundation. The problem posits that under the null hypothesis ($H_0$), the observed read count $c_i$ at any genomic position $i$ is a random variable drawn from a Poisson distribution with a rate parameter $\\lambda_i$, notationally $c_i \\sim \\text{Poisson}(\\lambda_i)$. The parameter $\\lambda_i$ represents the expected background count rate at that position. A key property of the Poisson distribution is its closure under addition: the sum of independent Poisson random variables is also a Poisson random variable, with a rate equal to the sum of the individual rates.\n\nSecond, we apply a sliding window approach to assess enrichment over contiguous genomic regions. A window of a fixed integer width $w$ slides across the genomic region of length $n$ with a stride of $1$. A window starting at index $s$ (where $s$ ranges from $0$ to $n-w$) encompasses the positions $\\{s, s+1, \\ldots, s+w-1\\}$. For each such window, we compute two aggregate quantities:\n1. The total observed count, $K_s = \\sum_{i=s}^{s+w-1} c_i$.\n2. The total expected background rate, $\\Lambda_s = \\sum_{i=s}^{s+w-1} \\lambda_i$.\n\nBased on the additive property of the Poisson distribution, the null hypothesis for the window starting at $s$ is that the observed sum $K_s$ is a realization of a Poisson random variable $X$ with rate $\\Lambda_s$. That is, $H_{0,s}: K_s \\sim \\text{Poisson}(\\Lambda_s)$.\n\nThird, for each of the $m = n-w+1$ windows, we perform a one-sided hypothesis test. We are interested in whether the observed count $K_s$ is unexpectedly large, which would indicate an enrichment peak. The statistical significance is quantified by a $p$-value, $p_s$, defined as the probability of observing a count sum of at least $K_s$ under the null hypothesis. This is the tail probability:\n$$p_s = \\Pr[X \\geq K_s] \\quad \\text{where} \\quad X \\sim \\text{Poisson}(\\Lambda_s)$$\nThis probability is computed as the sum of probabilities over the discrete support of the Poisson distribution:\n$$p_s = \\sum_{j=K_s}^{\\infty} \\frac{e^{-\\Lambda_s} \\Lambda_s^j}{j!}$$\nNumerically, this is more efficiently calculated using the cumulative distribution function (CDF), $F(k; \\Lambda_s) = \\Pr[X \\leq k]$, as $p_s = 1 - F(K_s - 1; \\Lambda_s)$. This is equivalent to the survival function (SF) evaluated at $K_s - 1$.\n\nFinally, since we are performing $m$ simultaneous hypothesis tests, we must correct for multiple comparisons to avoid an inflation of false positives. The problem specifies a less conservative and often more powerful approach than controlling the family-wise error rate: controlling the False Discovery Rate (FDR). The Benjamini–Hochberg (BH) procedure is employed for this purpose. The procedure is as follows:\n1. Let the $m$ computed $p$-values be $p_1, p_2, \\ldots, p_m$.\n2. Sort these $p$-values in ascending order: $p_{(1)} \\le p_{(2)} \\le \\ldots \\le p_{(m)}$.\n3. For each sorted $p$-value $p_{(i)}$, where $i$ is its rank from $1$ to $m$, calculate its corresponding BH-adjusted $p$-value, often called a $q$-value:\n$$q_{(i)} = \\min_{j=i}^{m} \\left\\{ \\min \\left(1, \\frac{m \\cdot p_{(j)}}{j} \\right) \\right\\}$$\nThis formula includes an enforcement of monotonicity, ensuring that for the sorted raw $p$-values, the corresponding adjusted $q$-values are also non-decreasing: $q_{(1)} \\le q_{(2)} \\le \\ldots \\le q_{(m)}$.\n4. The adjusted $q$-values are then mapped back to their original window indices.\n5. A window $s$ is declared to contain a \"significant peak\" if its adjusted $p$-value, $q_s$, is less than or equal to the specified FDR control level $q$.\n\nThe algorithm to be implemented will first compute the windowed sums $K_s$ and rates $\\Lambda_s$ for all possible start positions $s$. This can be optimized by pre-calculating the cumulative sums of the $c$ and $\\lambda$ vectors. Subsequently, it will compute the raw $p$-value for each window using a standard numerical library function for the Poisson survival function. The BH adjustment will then be applied to this set of $p$-values. The final step is to filter for windows whose adjusted $p$-value meets the significance threshold $q$, and to report their starting indices and adjusted $p$-values, sorted by index.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import poisson\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            [0,1,0,1,2,3,1,0,1,2,6,7,8,6,5,2,1,1,0,1,0,5,6,7,5,1,1,0,0,1],\n            [1.0,1.0,1.2,1.0,1.0,1.1,1.0,0.9,1.1,1.2,1.3,1.2,1.1,1.2,1.1,1.0,1.0,1.0,1.0,1.0,1.1,1.2,1.3,1.2,1.1,1.0,1.0,1.0,1.0,1.0],\n            5,\n            0.05\n        ),\n        (\n            [0,0,1,0,2,3,0,5,0,1,0,4],\n            [0.1,0.1,0.2,0.1,0.2,0.3,0.1,0.2,0.1,0.2,0.1,0.2],\n            1,\n            0.05\n        ),\n        (\n            [1,3,2,1,2,3,1,2,2,1,3,2,2,1,2,3,2,1,1,2],\n            [2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0],\n            20,\n            0.10\n        ),\n        (\n            [1,0,2,1,0,1,1,0,2,0,1,1,0,1,0,1,1,0],\n            [5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0],\n            4,\n            0.05\n        )\n    ]\n\n    def _calculate_peaks(c_vec, lambda_vec, w, q_level):\n        \"\"\"\n        Calculates significant peaks for a single test case.\n        \"\"\"\n        c = np.array(c_vec, dtype=np.int64)\n        lambda_bg = np.array(lambda_vec, dtype=np.float64)\n        \n        n = len(c)\n        num_windows = n - w + 1\n\n        if num_windows <= 0:\n            return [[], []]\n\n        # Efficiently compute sliding window sums using cumulative sums.\n        # This is O(n) instead of O(n*w).\n        c_cumsum = np.concatenate(([0], np.cumsum(c)))\n        lambda_cumsum = np.concatenate(([0], np.cumsum(lambda_bg)))\n        \n        indices = np.arange(num_windows)\n        k_s_array = c_cumsum[indices + w] - c_cumsum[indices]\n        lambda_s_array = lambda_cumsum[indices + w] - lambda_cumsum[indices]\n\n        # Compute one-sided p-values for each window.\n        # p_s = P(X >= K_s) where X ~ Poisson(Lambda_s).\n        # This is equivalent to the survival function P(X > K_s - 1).\n        raw_p_values = poisson.sf(k_s_array - 1, lambda_s_array)\n        \n        # Benjamini-Hochberg FDR correction.\n        m = len(raw_p_values)\n        if m == 0:\n            return [[], []]\n            \n        # Store original order, then sort p-values.\n        original_indices = np.arange(m)\n        p_sorted_indices = np.argsort(raw_p_values)\n        p_sorted = raw_p_values[p_sorted_indices]\n\n        # Calculate ranks (from 1 to m).\n        ranks = np.arange(1, m + 1)\n        \n        # Calculate BH values: (p_sorted * m) / rank.\n        bh_values = (m / ranks) * p_sorted\n        \n        # Enforce monotonicity by taking the cumulative minimum from the end of the sorted list.\n        bh_adjusted_sorted = np.minimum.accumulate(bh_values[::-1])[::-1]\n        \n        # Clip values at 1.0, as adjusted p-values cannot be > 1.\n        bh_adjusted_sorted = np.minimum(bh_adjusted_sorted, 1.0)\n        \n        # Map adjusted p-values back to their original window order.\n        bh_adjusted = np.empty_like(bh_adjusted_sorted)\n        bh_adjusted[p_sorted_indices] = bh_adjusted_sorted\n\n        # Identify significant windows and their adjusted p-values.\n        significant_mask = bh_adjusted <= q_level\n        significant_indices = original_indices[significant_mask].tolist()\n        significant_p_values = [round(p, 6) for p in bh_adjusted[significant_mask]]\n        \n        return [significant_indices, significant_p_values]\n\n    results = []\n    for case in test_cases:\n        c, l, w, q = case\n        result = _calculate_peaks(c, l, w, q)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\nsolve()\n```", "id": "4545839"}, {"introduction": "Identifying accessible chromatin regions is informative, but the ultimate goal is to understand their regulatory function. A powerful strategy is to integrate ATAC-seq data with other functional genomics data, such as RNA-seq. This practice [@problem_id:4545850] guides you through the process of linking chromatin accessibility to gene expression, a cornerstone of systems biology, by correlating peak signals with the expression of nearby genes to uncover potential regulatory elements.", "problem": "You are given a matched Assay for Transposase-Accessible Chromatin using sequencing (ATAC-seq) and RNA sequencing (RNA-seq) dataset across $6$ samples. The objective is to quantify the statistical relationship between chromatin accessibility peaks and nearby gene expression by computing correlations and identifying significant peak–gene links under false discovery rate control. Ground your approach in the following fundamental base:\n\n- Central Dogma of Molecular Biology: regulatory DNA elements influence transcription initiation, and open chromatin measured by ATAC-seq is plausibly associated with messenger ribonucleic acid (mRNA) levels measured by RNA-seq.\n- Statistical definition of correlation: for two sample vectors $x$ and $y$ of length $n$, the sample Pearson correlation is defined as $r(x,y) = \\dfrac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}$, where $\\bar{x}$ and $\\bar{y}$ denote sample means.\n- Hypothesis testing for correlation: under the null hypothesis $H_0: r = 0$, the test statistic $t = r \\sqrt{\\dfrac{n - 2}{1 - r^2}}$ follows a Student’s $t$ distribution with $n - 2$ degrees of freedom, enabling a two-sided $p$-value $p = 2 \\cdot \\Pr(T \\geq |t|)$.\n- Multiple testing control: for $m$ independent or positively dependent hypothesis tests with $p$-values $\\{p_1, \\dots, p_m\\}$, the Benjamini–Hochberg step-up procedure controls the false discovery rate at level $\\alpha$ by finding the largest index $k$ such that $p_{(k)} \\leq \\dfrac{k}{m} \\alpha$, where $p_{(k)}$ is the $k$-th smallest $p$-value. All $p_{(i)}$ with $i \\leq k$ are declared significant.\n\nData are encoded as deterministic arrays and genomic coordinates, ensuring scientific realism without external files. A peak–gene pair is considered a candidate if the chromosomal labels match and the absolute distance between the peak center and the gene transcription start site (TSS) is less than or equal to a user-specified maximum distance in base pairs.\n\nDataset specification:\n- Samples: $6$ matched biological samples indexed by $i = 1, \\dots, 6$.\n- Genes with chromosome and TSS:\n    - $G1$: chromosome $chr1$, $\\text{TSS} = 100000$.\n    - $G2$: chromosome $chr1$, $\\text{TSS} = 150000$.\n    - $G3$: chromosome $chr1$, $\\text{TSS} = 500000$.\n    - $G4$: chromosome $chr2$, $\\text{TSS} = 100000$.\n- ATAC-seq peaks with chromosome and center position:\n    - $P1$: chromosome $chr1$, position $103000$.\n    - $P2$: chromosome $chr1$, position $130000$.\n    - $P3$: chromosome $chr1$, position $148000$.\n    - $P4$: chromosome $chr1$, position $510000$.\n    - $P5$: chromosome $chr2$, position $90000$.\n- ATAC-seq accessibility vectors (arbitrary units) across $6$ samples:\n    - $P1$: $[10, 15, 20, 25, 30, 36]$.\n    - $P2$: $[13, 15, 14, 15, 14, 15]$.\n    - $P3$: $[10, 12, 14, 16, 18, 20]$.\n    - $P4$: $[5, 15, 5, 16, 10, 13]$.\n    - $P5$: $[5, 5, 5, 5, 5, 5]$.\n- RNA-seq expression vectors (arbitrary units) across $6$ samples:\n    - $G1$: $[50, 62, 69, 81, 88, 101]$.\n    - $G2$: $[200, 180, 160, 140, 120, 100]$.\n    - $G3$: $[80, 85, 80, 79, 82, 81]$.\n    - $G4$: $[30, 28, 29, 31, 32, 30]$.\n\nConstraints and definitions:\n- Candidate peak–gene pairs must have matching chromosome labels and absolute distance less than or equal to a specified maximum distance in base pairs.\n- For each candidate pair, compute the Pearson correlation coefficient and the associated two-sided $p$-value using the null $H_0: r = 0$ and the Student’s $t$ distribution with $n - 2$ degrees of freedom.\n- If either vector has zero variance (for example, $P5$), treat the correlation as undefined and exclude the pair from hypothesis testing.\n- Apply the Benjamini–Hochberg procedure at level $\\alpha$ to the set of valid $p$-values to decide significance.\n- Report the total count of significant peak–gene links for each test case as an integer.\n\nTest suite:\nYour program must evaluate the following test cases, each described by $(\\alpha, d_{\\max})$ where $\\alpha$ is the false discovery rate level and $d_{\\max}$ is the maximum allowed distance in base pairs:\n- Case $1$: $(\\alpha = 0.05, d_{\\max} = 10000)$.\n- Case $2$: $(\\alpha = 0.05, d_{\\max} = 2000)$.\n- Case $3$: $(\\alpha = 0.05, d_{\\max} = 20000)$.\n- Case $4$: $(\\alpha = 0.5, d_{\\max} = 20000)$.\n- Case $5$: $(\\alpha = 0.05, d_{\\max} = 500)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[result1,result2,result3,result4,result5]$), where each $result$ is the integer count of significant peak–gene links for the corresponding test case. No physical units are required in the output since all quantities are dimensionless counts.", "solution": "The objective is to quantify the statistical association between chromatin accessibility at specific genomic loci (ATAC-seq peaks) and the expression of nearby genes (RNA-seq). This analysis is grounded in the central dogma of molecular biology, which posits that accessible regulatory regions, such as enhancers and promoters, influence the rate of gene transcription. The problem provides a dataset comprising ATAC-seq and RNA-seq measurements for $6$ samples, along with genomic coordinates for $4$ genes and $5$ peaks. The analytical procedure involves three main steps: identifying candidate peak–gene pairs based on genomic proximity, computing a statistical measure of association for each pair, and applying a multiple testing correction to control the false discovery rate.\n\n**Step 1: Identification of Candidate Peak–Gene Pairs**\n\nA peak–gene pair is considered a candidate for association analysis if two conditions are met:\n1.  Both the peak and the gene reside on the same chromosome.\n2.  The absolute genomic distance between the peak's center and the gene's transcription start site (TSS) is less than or equal to a specified maximum distance, $d_{\\max}$. The distance is calculated as $|\\text{peak position} - \\text{gene TSS}|$.\n\nFirst, we enumerate all possible pairs and check the chromosome matching condition. There are $5$ peaks and $4$ genes, resulting in $20$ total combinations.\n-   Peaks on chromosome $chr1$: $P1, P2, P3, P4$. Genes on chromosome $chr1$: $G1, G2, G3$. This gives $4 \\times 3 = 12$ potential pairs on $chr1$.\n-   Peaks on chromosome $chr2$: $P5$. Genes on chromosome $chr2$: $G4$. This gives $1 \\times 1 = 1$ potential pair on $chr2$.\nAll other combinations involve different chromosomes and are invalid. The distances for the $13$ valid pairs are calculated and tabulated below:\n\n| Peak | Gene | Peak Position | Gene TSS | Distance (bp) |\n| :--- | :--- | :--- | :--- | :--- |\n| $P1$ | $G1$ | $103000$ | $100000$ | $3000$ |\n| $P1$ | $G2$ | $103000$ | $150000$ | $47000$ |\n| $P1$ | $G3$ | $103000$ | $500000$ | $397000$ |\n| $P2$ | $G1$ | $130000$ | $100000$ | $30000$ |\n| $P2$ | $G2$ | $130000$ | $150000$ | $20000$ |\n| $P2$ | $G3$ | $130000$ | $500000$ | $370000$ |\n| $P3$ | $G1$ | $148000$ | $100000$ | $48000$ |\n| $P3$ | $G2$ | $148000$ | $150000$ | $2000$ |\n| $P3$ | $G3$ | $148000$ | $500000$ | $352000$ |\n| $P4$ | $G1$ | $510000$ | $100000$ | $410000$ |\n| $P4$ | $G2$ | $510000$ | $150000$ | $360000$ |\n| $P4$ | $G3$ | $510000$ | $500000$ | $10000$ |\n| $P5$ | $G4$ | $90000$ | $100000$ | $10000$ |\n\nFor a given test case with a specific $d_{\\max}$, we select only the pairs from this table where Distance $\\le d_{\\max}$.\n\n**Step 2: Calculation of Statistical Significance**\n\nFor each candidate pair, we analyze the association between its ATAC-seq accessibility vector ($x$) and RNA-seq expression vector ($y$). Both vectors have a length of $n=6$.\n\nFirst, we must exclude pairs where either vector exhibits zero variance. The ATAC-seq vector for peak $P5$ is $[5, 5, 5, 5, 5, 5]$, which has a variance of $0$. Therefore, any candidate pair involving $P5$ (i.e., ($P5, G4$)) is excluded from hypothesis testing. All other provided vectors have non-zero variance.\n\nFor each remaining valid candidate pair, we compute the sample Pearson correlation coefficient:\n$$r(x,y) = \\dfrac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}$$\nUnder the null hypothesis $H_0$ that the true correlation is zero, the test statistic $t$ is calculated as:\n$$t = r \\sqrt{\\dfrac{n - 2}{1 - r^2}}$$\nThis statistic follows a Student's $t$-distribution with $n - 2 = 6 - 2 = 4$ degrees of freedom. The two-sided $p$-value is the probability of observing a test statistic at least as extreme as $|t|$, given by $p = 2 \\cdot \\Pr(T \\ge |t|)$, where $T$ is a random variable following the $t(4)$ distribution. We calculate these statistics for all $12$ potentially testable pairs.\n\n| Pair | Distance (bp) | Correlation ($r$) | $t$-statistic | $p$-value |\n| :--- | :--- | :--- | :--- | :--- |\n| ($P1$, $G1$) | $3000$ | $0.9936$ | $15.35$ | $0.00018$ |\n| ($P1$, $G2$) | $47000$ | $-0.9634$ | $-6.74$ | $0.00259$ |\n| ($P1$, $G3$) | $397000$ | $-0.0150$ | $-0.03$ | $0.97780$ |\n| ($P2$, $G1$) | $30000$ | $0.2312$ | $0.48$ | $0.66040$ |\n| ($P2$, $G2$) | $20000$ | $-0.3475$ | $-0.74$ | $0.49980$ |\n| ($P2$, $G3$) | $370000$ | $0.8143$ | $2.76$ | $0.05090$ |\n| ($P3$, $G1$) | $48000$ | $0.9856$ | $10.33$ | $0.00049$ |\n| ($P3$, $G2$) | $2000$ | $-1.0000$ | $-\\infty$ | $0.00000$ |\n| ($P3$, $G3$) | $352000$ | $0.1691$ | $0.35$ | $0.74820$ |\n| ($P4$, $G1$) | $410000$ | $-0.0435$ | $-0.09$ | $0.93480$ |\n| ($P4$, $G2$) | $360000$ | $0.1332$ | $0.27$ | $0.80060$ |\n| ($P4$, $G3$) | $10000$ | $0.5000$ | $1.15$ | $0.31250$ |\n\n**Step 3: Control of False Discovery Rate (FDR)**\n\nTo account for multiple hypothesis tests, we apply the Benjamini–Hochberg (BH) procedure. For a set of $m$ $p$-values and a desired FDR level $\\alpha$:\n1.  Sort the $p$-values in non-decreasing order: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$.\n2.  Find the largest integer $k$ such that $p_{(k)} \\le \\frac{k}{m} \\alpha$.\n3.  Reject the null hypotheses for all tests corresponding to $p_{(1)}, \\dots, p_{(k)}$. The count of significant links is $k$. If no such $k$ exists, no hypotheses are rejected, and the count is $0$.\n\n**Step 4: Application to Test Cases**\n\nWe now apply this entire workflow to each test case.\n\n**Case 1: $(\\alpha = 0.05, d_{\\max} = 10000)$**\n- Candidate pairs (distance $\\le 10000$): ($P1, G1$), ($P3, G2$), ($P4, G3$). The candidate ($P5, G4$) is excluded due to zero variance.\n- Number of tests $m=3$.\n- $p$-values: $\\{0.00018, 0.00000, 0.31250\\}$.\n- Sorted $p$-values: $p_{(1)}=0.00000$, $p_{(2)}=0.00018$, $p_{(3)}=0.31250$.\n- BH thresholds ($\\frac{k}{3} \\cdot 0.05$): $0.0167$ for $k=1$, $0.0333$ for $k=2$, $0.0500$ for $k=3$.\n- Check:\n  - $k=3: p_{(3)} = 0.31250 \\not\\le 0.0500$.\n  - $k=2: p_{(2)} = 0.00018 \\le 0.0333$. This condition holds.\n- The largest $k$ is $2$.\n- Result: $2$ significant links.\n\n**Case 2: $(\\alpha = 0.05, d_{\\max} = 2000)$**\n- Candidate pairs (distance $\\le 2000$): ($P3, G2$).\n- Number of tests $m=1$.\n- $p$-value: $\\{0.00000\\}$.\n- Sorted $p$-value: $p_{(1)}=0.00000$.\n- BH threshold ($\\frac{1}{1} \\cdot 0.05$): $0.0500$ for $k=1$.\n- Check: $p_{(1)} = 0.00000 \\le 0.0500$. The condition holds.\n- The largest $k$ is $1$.\n- Result: $1$ significant link.\n\n**Case 3: $(\\alpha = 0.05, d_{\\max} = 20000)$**\n- Candidate pairs (distance $\\le 20000$): ($P1, G1$), ($P2, G2$), ($P3, G2$), ($P4, G3$).\n- Number of tests $m=4$.\n- $p$-values: $\\{0.00018, 0.49980, 0.00000, 0.31250\\}$.\n- Sorted $p$-values: $p_{(1)}=0.00000$, $p_{(2)}=0.00018$, $p_{(3)}=0.31250$, $p_{(4)}=0.49980$.\n- BH thresholds ($\\frac{k}{4} \\cdot 0.05$): $0.0125$ ($k=1$), $0.0250$ ($k=2$), $0.0375$ ($k=3$), $0.0500$ ($k=4$).\n- Check:\n  - $k=4: p_{(4)} = 0.49980 \\not\\le 0.0500$.\n  - $k=3: p_{(3)} = 0.31250 \\not\\le 0.0375$.\n  - $k=2: p_{(2)} = 0.00018 \\le 0.0250$. This condition holds.\n- The largest $k$ is $2$.\n- Result: $2$ significant links.\n\n**Case 4: $(\\alpha = 0.5, d_{\\max} = 20000)$**\n- Candidate pairs are the same as in Case 3, $m=4$.\n- $p$-values: $\\{0.00018, 0.49980, 0.00000, 0.31250\\}$.\n- Sorted $p$-values: $p_{(1)}=0.00000, p_{(2)}=0.00018, p_{(3)}=0.31250, p_{(4)}=0.49980$.\n- BH thresholds ($\\frac{k}{4} \\cdot 0.5$): $0.125$ ($k=1$), $0.250$ ($k=2$), $0.375$ ($k=3$), $0.500$ ($k=4$).\n- Check:\n  - $k=4: p_{(4)} = 0.49980 \\le 0.500$. This condition holds.\n- The largest $k$ is $4$.\n- Result: $4$ significant links.\n\n**Case 5: $(\\alpha = 0.05, d_{\\max} = 500)$**\n- Candidate pairs (distance $\\le 500$): None.\n- Number of tests $m=0$.\n- With no tests performed, there are no significant results.\n- Result: $0$ significant links.\n\nThe final counts for the five test cases are $[2, 1, 2, 4, 0]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Solves the bioinformatics problem of finding significant peak-gene links.\n    The solution involves identifying candidate pairs by distance, calculating\n    correlation and p-values, and applying the Benjamini-Hochberg procedure.\n    \"\"\"\n    \n    # Dataset specification\n    n_samples = 6\n    \n    genes_data = {\n        'G1': {'chr': 'chr1', 'tss': 100000, 'expr': np.array([50, 62, 69, 81, 88, 101])},\n        'G2': {'chr': 'chr1', 'tss': 150000, 'expr': np.array([200, 180, 160, 140, 120, 100])},\n        'G3': {'chr': 'chr1', 'tss': 500000, 'expr': np.array([80, 85, 80, 79, 82, 81])},\n        'G4': {'chr': 'chr2', 'tss': 100000, 'expr': np.array([30, 28, 29, 31, 32, 30])},\n    }\n\n    peaks_data = {\n        'P1': {'chr': 'chr1', 'pos': 103000, 'access': np.array([10, 15, 20, 25, 30, 36])},\n        'P2': {'chr': 'chr1', 'pos': 130000, 'access': np.array([13, 15, 14, 15, 14, 15])},\n        'P3': {'chr': 'chr1', 'pos': 148000, 'access': np.array([10, 12, 14, 16, 18, 20])},\n        'P4': {'chr': 'chr1', 'pos': 510000, 'access': np.array([5, 15, 5, 16, 10, 13])},\n        'P5': {'chr': 'chr2', 'pos': 90000, 'access': np.array([5, 5, 5, 5, 5, 5])},\n    }\n    \n    # Test suite\n    test_cases = [\n        # (alpha, d_max)\n        (0.05, 10000),\n        (0.05, 2000),\n        (0.05, 20000),\n        (0.5, 20000),\n        (0.05, 500),\n    ]\n\n    def calculate_p_value(vec1, vec2, n):\n        \"\"\"Calculates the two-sided p-value for a Pearson correlation.\"\"\"\n        # Check for zero variance\n        if np.var(vec1) == 0 or np.var(vec2) == 0:\n            return None # Indicates an invalid pair for hypothesis testing\n        \n        r = np.corrcoef(vec1, vec2)[0, 1]\n        \n        # Handle perfect correlation edge case\n        if abs(r) == 1.0:\n            return 0.0\n        \n        dof = n - 2\n        t_stat = r * np.sqrt(dof / (1 - r**2))\n        p_val = 2 * t.sf(np.abs(t_stat), df=dof) # sf is survival function (1-cdf)\n        \n        return p_val\n\n    results = []\n    \n    for alpha, d_max in test_cases:\n        candidate_p_values = []\n        \n        # Step 1: Identify candidate pairs and calculate p-values\n        for g_id, g_info in genes_data.items():\n            for p_id, p_info in peaks_data.items():\n                \n                # Condition 1: Matching chromosome\n                if g_info['chr'] != p_info['chr']:\n                    continue\n                \n                # Condition 2: Distance constraint\n                distance = abs(p_info['pos'] - g_info['tss'])\n                if distance > d_max:\n                    continue\n\n                # The pair is a candidate, proceed to statistical test\n                p_val = calculate_p_value(p_info['access'], g_info['expr'], n_samples)\n                \n                # Exclude pairs where correlation is undefined (zero variance)\n                if p_val is not None:\n                    candidate_p_values.append(p_val)\n        \n        # Step 2: Apply Benjamini-Hochberg procedure\n        m = len(candidate_p_values)\n        if m == 0:\n            results.append(0)\n            continue\n            \n        sorted_p_values = sorted(candidate_p_values)\n        \n        k = 0\n        for i in range(m - 1, -1, -1):\n            rank = i + 1\n            p_i = sorted_p_values[i]\n            bh_threshold = (rank / m) * alpha\n            if p_i <= bh_threshold:\n                k = rank\n                break\n        \n        results.append(k)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4545850"}]}