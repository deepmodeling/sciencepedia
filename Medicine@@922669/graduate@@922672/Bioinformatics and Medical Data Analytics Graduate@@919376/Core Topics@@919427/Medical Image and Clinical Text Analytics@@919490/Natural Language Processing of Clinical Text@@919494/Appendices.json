{"hands_on_practices": [{"introduction": "The foundational step in any natural language processing pipeline is tokenization, the process of breaking down raw text into discrete units, or tokens. In the clinical domain, this task is complicated by the presence of abbreviations, dictated artifacts, complex punctuation, and numeric values attached to units. This exercise provides a concrete comparison of different tokenization strategies, revealing the trade-offs between simplicity, domain-aware rules, and modern subword methods, thereby building an intuition for how this initial choice impacts all downstream analyses [@problem_id:4588759].", "problem": "Consider the following clinical note snippet (verbatim, including punctuation and spacing):\nPt c/o chest pain, 2/10, onset ~3h ago; Na 142 mg/dL; uh... denies SOB.\n\nYou will analyze three tokenization schemes applied to this snippet and compute their token counts and out-of-vocabulary rates, then combine these quantities into a single scalar. Work from first principles of tokenization and out-of-vocabulary rate definitions as described below.\n\nTokenization schemes:\n1. Whitespace tokenization: Split the snippet on spaces only (that is, on space characters). No punctuation splitting is performed; punctuation remains attached to the surrounding text if present.\n2. Rule-based tokenization that preserves numeric units:\n   - First split on whitespace as above to obtain preliminary tokens.\n   - Then, for each preliminary token:\n     - If the token contains the exact substring \"...\", split it into the substring before \"...\" (if non-empty) and a separate token \"...\".\n     - Detach any single trailing character that is a comma, semicolon, or period (\",\", \";\", \".\") and emit it as its own token. Do not split inside slashes (\"/\") and do not detach punctuation when it is part of a unit (e.g., \"mg/dL\") or part of the three-character ellipsis \"...\".\n   - After punctuation handling, merge numeric-unit expressions: if a token that is exactly a numeral optionally prefixed by \"~\" (that is, \"~\" followed by digits, or digits alone) is immediately followed by a unit token from the unit list U = {\"mg/dL\", \"mmHg\", \"h\"}, merge the two into a single token by concatenation without any intervening space (e.g., \"142\" followed by \"mg/dL\" becomes \"142mg/dL\"; \"~3\" followed by \"h\" is equivalent to \"~3h\" which remains a single token).\n3. Subword Byte Pair Encoding (BPE)-style tokenization:\n   - Lowercase the snippet prior to subword segmentation.\n   - Split on whitespace to obtain words, then segment each word left-to-right using a greedy longest-match rule over the following subword vocabulary V_b:\n     {\"pt\", \"c\", \"/\", \"o\", \"chest\", \"pain\", \",\", \"2/10\", \"onset\", \"~\", \"3h\", \"ago\", \";\", \"na\", \"142\", \"mg/dl\", \"uh\", \"...\", \"denies\", \"sob\", \".\"}.\n   - If a word cannot be fully segmented using V_b, represent that entire word with the special symbol \"[UNK]\" as a single subword token. For this problem, treat only the literal token \"[UNK]\" as out-of-vocabulary at the subword level.\n\nLexicons for out-of-vocabulary (OOV) determination:\n- For whitespace tokenization, use the word-level lexicon V_w = {\"Pt\", \"c/o\", \"chest\", \"pain\", \"2/10\", \"onset\", \"~3h\", \"ago\", \"Na\", \"142\", \"mg/dL\", \"uh...\", \"denies\", \"SOB\"}.\n- For rule-based tokenization, use the token lexicon V_r = {\"Pt\", \"c/o\", \"chest\", \"pain\", \"2/10\", \"onset\", \"~3h\", \"ago\", \"Na\", \"142mg/dL\", \"uh\", \"...\", \"denies\", \"SOB\", \",\", \";\", \".\"}.\n- For BPE tokenization, use the subword vocabulary V_b above and define the OOV symbol exclusively as \"[UNK]\".\n\nDefinitions:\n- The token count under a tokenization scheme is the total number of tokens produced by that scheme on the snippet.\n- The out-of-vocabulary rate under a scheme is defined as the number of tokens not found in the scheme’s corresponding lexicon (for BPE: the number of subword tokens equal to \"[UNK]\") divided by the total number of tokens produced by that scheme.\n\nTasks:\n- Compute the number of tokens produced by whitespace tokenization, by the rule-based tokenization, and by the BPE tokenization. Denote these by $N_{\\mathrm{ws}}$, $N_{\\mathrm{rule}}$, and $N_{\\mathrm{bpe}}$, respectively.\n- Compute the out-of-vocabulary rates under each scheme. Denote these by $R_{\\mathrm{ws}}$, $R_{\\mathrm{rule}}$, and $R_{\\mathrm{bpe}}$, respectively.\n- Finally, compute the scalar\n$$S \\equiv \\left(N_{\\mathrm{bpe}} - N_{\\mathrm{rule}}\\right) + \\left(R_{\\mathrm{ws}} - R_{\\mathrm{rule}}\\right).$$\n\nProvide the final value of $S$ as a single reduced fraction. Do not round. No units are required.", "solution": "The problem posed is a well-defined exercise in computational linguistics applied to clinical text. All procedures and definitions are explicitly stated, allowing for a unique and verifiable solution. The problem is therefore valid. We shall proceed by systematically analyzing each specified tokenization scheme to compute the required quantities.\n\nThe clinical note snippet is: \"Pt c/o chest pain, 2/10, onset ~3h ago; Na 142 mg/dL; uh... denies SOB.\"\n\n**1. Whitespace Tokenization**\n\nThis scheme splits the text solely on space characters.\nApplying this rule to the snippet yields the following tokens:\n`\"Pt\"`, `\"c/o\"`, `\"chest\"`, `\"pain,\"`, `\"2/10,\"`, `\"onset\"`, `\"~3h\"`, `\"ago;\"`, `\"Na\"`, `\"142\"`, `\"mg/dL;\"`, `\"uh...\"`, `\"denies\"`, `\"SOB.\"`\n\nThe total number of tokens is the count of elements in this list.\n$$N_{\\mathrm{ws}} = 14$$\n\nNext, we calculate the out-of-vocabulary (OOV) rate. The given word-level lexicon is $V_w = \\{\"Pt\", \"c/o\", \"chest\", \"pain\", \"2/10\", \"onset\", \"~3h\", \"ago\", \"Na\", \"142\", \"mg/dL\", \"uh...\", \"denies\", \"SOB\"\\}$. A token is considered OOV if it is not present in $V_w$. We check each token:\n- `\"Pt\"`: in $V_w$\n- `\"c/o\"`: in $V_w$\n- `\"chest\"`: in $V_w$\n- `\"pain,\"`: **not** in $V_w$ (due to the comma)\n- `\"2/10,\"`: **not** in $V_w$ (due to the comma)\n- `\"onset\"`: in $V_w$\n- `\"~3h\"`: in $V_w$\n- `\"ago;\"`: **not** in $V_w$ (due to the semicolon)\n- `\"Na\"`: in $V_w$\n- `\"142\"`: in $V_w$\n- `\"mg/dL;\"`: **not** in $V_w$ (due to the semicolon)\n- `\"uh...\"`: in $V_w$\n- `\"denies\"`: in $V_w$\n- `\"SOB.\"`: **not** in $V_w$ (due to the period)\n\nThe number of OOV tokens is $5$.\nThe OOV rate, $R_{\\mathrm{ws}}$, is the ratio of OOV tokens to the total number of tokens.\n$$R_{\\mathrm{ws}} = \\frac{5}{14}$$\n\n**2. Rule-based Tokenization**\n\nThis scheme involves a multi-step process.\nStep (a): Initial split on whitespace, which gives the same $14$ preliminary tokens as in the first scheme.\nStep (b): Punctuation handling.\n- `\"pain,\"` $\\rightarrow$ `\"pain\"`, `\",\"`\n- `\"2/10,\"` $\\rightarrow$ `\"2/10\"`, `\",\"`\n- `\"ago;\"` $\\rightarrow$ `\"ago\"`, `\";\"`\n- `\"mg/dL;\"` $\\rightarrow$ `\"mg/dL\"`, `\";\"`\n- `\"uh...\"` $\\rightarrow$ `\"uh\"`, `\"...\"`\n- `\"SOB.\"` $\\rightarrow$ `\"SOB\"`, `\".\"`\nThe list of tokens after this step is:\n`\"Pt\"`, `\"c/o\"`, `\"chest\"`, `\"pain\"`, `\",\"`, `\"2/10\"`, `\",\"`, `\"onset\"`, `\"~3h\"`, `\"ago\"`, `\";\"`, `\"Na\"`, `\"142\"`, `\"mg/dL\"`, `\";\"`, `\"uh\"`, `\"...\"`, `\"denies\"`, `\"SOB\"`, `\".\"`\n\nStep (c): Merge numeric-unit expressions. The unit list is $U = \\{\"mg/dL\", \"mmHg\", \"h\"\\}$. We look for a sequence of a numeral token followed by a unit token from $U$. In the list above, the token `\"142\"` is a numeral, and it is immediately followed by `\"mg/dL\"`, which is in $U$. These two tokens are merged.\n- `\"142\"`, `\"mg/dL\"` $\\rightarrow$ `\"142mg/dL\"`\n\nThe final list of tokens for the rule-based scheme is:\n`\"Pt\"`, `\"c/o\"`, `\"chest\"`, `\"pain\"`, `\",\"`, `\"2/10\"`, `\",\"`, `\"onset\"`, `\"~3h\"`, `\"ago\"`, `\";\"`, `\"Na\"`, `\"142mg/dL\"`, `\";\"`, `\"uh\"`, `\"...\"`, `\"denies\"`, `\"SOB\"`, `\".\"`\n\nThe total count of these tokens is:\n$$N_{\\mathrm{rule}} = 19$$\n\nThe lexicon for this scheme is $V_r = \\{\"Pt\", \"c/o\", \"chest\", \"pain\", \"2/10\", \"onset\", \"~3h\", \"ago\", \"Na\", \"142mg/dL\", \"uh\", \"...\", \"denies\", \"SOB\", \",\", \";\", \".\"\\}$. By inspection, every single one of the $19$ tokens generated by our rule-based process is present in $V_r$. Therefore, the number of OOV tokens is $0$.\nThe OOV rate is:\n$$R_{\\mathrm{rule}} = \\frac{0}{19} = 0$$\n\n**3. BPE-style Tokenization**\n\nStep (a): The snippet is lowercased:\n`\"pt c/o chest pain, 2/10, onset ~3h ago; na 142 mg/dl; uh... denies sob.\"`\nStep (b): Splitting on whitespace gives the initial word list:\n`[\"pt\", \"c/o\", \"chest\", \"pain,\", \"2/10,\", \"onset\", \"~3h\", \"ago;\", \"na\", \"142\", \"mg/dl;\", \"uh...\", \"denies\", \"sob.\"]`\nStep (c): Each word is segmented using a greedy longest-match with the subword vocabulary $V_b = \\{\"pt\", \"c\", \"/\", \"o\", \"chest\", \"pain\", \",\", \"2/10\", \"onset\", \"~\", \"3h\", \"ago\", \";\", \"na\", \"142\", \"mg/dl\", \"uh\", \"...\", \"denies\", \"sob\", \".\"\\}$.\n- `\"pt\"` $\\rightarrow$ `[\"pt\"]` ($1$ token)\n- `\"c/o\"` $\\rightarrow$ `[\"c\", \"/\", \"o\"]` ($3$ tokens)\n- `\"chest\"` $\\rightarrow$ `[\"chest\"]` ($1$ token)\n- `\"pain,\"` $\\rightarrow$ `[\"pain\", \",\"]` ($2$ tokens)\n- `\"2/10,\"` $\\rightarrow$ `[\"2/10\", \",\"]` ($2$ tokens)\n- `\"onset\"` $\\rightarrow$ `[\"onset\"]` ($1$ token)\n- `\"~3h\"` $\\rightarrow$ `[\"~\", \"3h\"]` ($2$ tokens)\n- `\"ago;\"` $\\rightarrow$ `[\"ago\", \";\"]` ($2$ tokens)\n- `\"na\"` $\\rightarrow$ `[\"na\"]` ($1$ token)\n- `\"142\"` $\\rightarrow$ `[\"142\"]` ($1$ token)\n- `\"mg/dl;\"` $\\rightarrow$ `[\"mg/dl\", \";\"]` ($2$ tokens)\n- `\"uh...\"` $\\rightarrow$ `[\"uh\", \"...\"]` ($2$ tokens)\n- `\"denies\"` $\\rightarrow$ `[\"denies\"]` ($1$ token)\n- `\"sob.\"` $\\rightarrow$ `[\"sob\", \".\"]` ($2$ tokens)\n\nAll words were segmented completely. No `\"[UNK]\"` tokens were generated.\nThe total number of BPE tokens is the sum of the counts for each word:\n$$N_{\\mathrm{bpe}} = 1 + 3 + 1 + 2 + 2 + 1 + 2 + 2 + 1 + 1 + 2 + 2 + 1 + 2 = 23$$\nThe number of OOV tokens, defined as the count of `\"[UNK]\"`, is $0$.\nThe OOV rate is:\n$$R_{\\mathrm{bpe}} = \\frac{0}{23} = 0$$\n\n**Final Calculation**\n\nWe are asked to compute the scalar $S$:\n$$S \\equiv \\left(N_{\\mathrm{bpe}} - N_{\\mathrm{rule}}\\right) + \\left(R_{\\mathrm{ws}} - R_{\\mathrm{rule}}\\right)$$\nWe substitute the values calculated above:\n- $N_{\\mathrm{bpe}} = 23$\n- $N_{\\mathrm{rule}} = 19$\n- $R_{\\mathrm{ws}} = \\frac{5}{14}$\n- $R_{\\mathrm{rule}} = 0$\n\n$$S = (23 - 19) + \\left(\\frac{5}{14} - 0\\right)$$\n$$S = 4 + \\frac{5}{14}$$\nTo express this as a single fraction, we find a common denominator:\n$$S = \\frac{4 \\times 14}{14} + \\frac{5}{14} = \\frac{56}{14} + \\frac{5}{14} = \\frac{61}{14}$$\nThe number $61$ is prime, and $14 = 2 \\times 7$. Thus, the fraction $\\frac{61}{14}$ is in its simplest form.", "answer": "$$\\boxed{\\frac{61}{14}}$$", "id": "4588759"}, {"introduction": "Once text is tokenized, the next challenge is to represent the tokens numerically in a way that captures their meaning. This exercise introduces the classic Term Frequency-Inverse Document Frequency (TF-IDF) weighting scheme, which allows us to create vector representations of words based on their importance within a document and rarity across a collection of documents. By calculating TF-IDF vectors and their cosine similarity for a common clinical abbreviation and its long-form equivalent, you will gain hands-on experience in using vector space models to quantify semantic relationships from data [@problem_id:4588751].", "problem": "You are given a small corpus of clinical sentences and asked to compute the Term Frequency–Inverse Document Frequency (TF-IDF) vectors for the tokens \"htn\" and \"hypertension\" under a standard bag-of-words model from Natural Language Processing (NLP), and then calculate the cosine similarity between these two vectors to assess their synonymy in context. Work from the following fundamental base: the vector space representation of text where each term is represented by a document-indexed vector of weights; the definition of term frequency as the count of a term in a document; and the definition of inverse document frequency as the logarithm of the ratio of the total number of documents to the number of documents containing the term. Use the following tokenization and normalization rules: convert to lowercase, remove punctuation, split on whitespace, do not remove stopwords, and treat the strings \"htn\" and \"hypertension\" as distinct tokens without abbreviation expansion or lemmatization. The corpus consists of $N=5$ documents:\n\nDocument $d_{1}$: \"Patient with HTN and diabetes; HTN uncontrolled.\"\nDocument $d_{2}$: \"History of hypertension, treated with Angiotensin-Converting Enzyme (ACE) inhibitors.\"\nDocument $d_{3}$: \"Hypertension has been noted; HTN stage $2$.\"\nDocument $d_{4}$: \"No history of HTN; blood pressure normal.\"\nDocument $d_{5}$: \"Family history negative for hypertension.\"\n\nAdopt the following definitions:\n\n- Term Frequency (TF): for term $t$ in document $d_{i}$, define $\\mathrm{tf}(t,d_{i})$ as the raw count of $t$ in $d_{i}$.\n- Inverse Document Frequency (IDF): for term $t$, define $\\mathrm{idf}(t) = \\ln\\!\\left(\\frac{N}{\\mathrm{df}(t)}\\right)$, where $\\mathrm{df}(t)$ is the number of documents containing $t$ and $\\ln$ denotes the natural logarithm.\n- TF-IDF vector: for term $t$, define the document-indexed vector $v_{t}$ whose $i$-th component is $\\mathrm{tf}(t,d_{i}) \\cdot \\mathrm{idf}(t)$.\n- Cosine similarity: for two term vectors $v_{a}$ and $v_{b}$, define $\\cos\\theta = \\frac{v_{a} \\cdot v_{b}}{\\|v_{a}\\|_{2}\\,\\|v_{b}\\|_{2}}$, where $\\cdot$ denotes the Euclidean dot product and $\\|\\cdot\\|_{2}$ denotes the Euclidean norm.\n\nCompute the TF-IDF vectors $v_{\\text{htn}}$ and $v_{\\text{hypertension}}$ and then compute the cosine similarity between them. Express the final cosine similarity as a real number rounded to four significant figures. No units are required for the final answer.", "solution": "The problem statement has been analyzed and is deemed valid. It is scientifically grounded in the principles of natural language processing and vector space models, well-posed with all necessary information and definitions provided, and objective in its formulation. We may therefore proceed with a full solution.\n\nThe objective is to compute the cosine similarity between the Term Frequency-Inverse Document Frequency (TF-IDF) vectors for the tokens \"htn\" and \"hypertension\". The corpus consists of $N=5$ documents. The process involves several sequential steps: calculating term frequencies, calculating inverse document frequencies, constructing the TF-IDF vectors, and finally, computing their cosine similarity.\n\nLet the two terms of interest be $t_{1} = \\text{\"htn\"}$ and $t_{2} = \\text{\"hypertension\"}$.\n\n**Step 1: Calculate Term Frequencies (TF)**\nFirst, we apply the specified normalization rules (lowercase, remove punctuation, split on whitespace) to the documents and count the occurrences of $t_{1}$ and $t_{2}$ in each document $d_{i}$. The term frequency, $\\mathrm{tf}(t, d_{i})$, is the raw count of term $t$ in document $d_{i}$.\n\nFor $t_1 = \\text{\"htn\"}$:\n- $d_1$: \"Patient with HTN and diabetes; HTN uncontrolled.\" $\\rightarrow$ `patient with htn and diabetes htn uncontrolled`. Count of \"htn\" is $2$. So, $\\mathrm{tf}(t_1, d_1) = 2$.\n- $d_2$: \"History of hypertension, treated with...\" $\\rightarrow$ Count of \"htn\" is $0$. So, $\\mathrm{tf}(t_1, d_2) = 0$.\n- $d_3$: \"Hypertension has been noted; HTN stage 2.\" $\\rightarrow$ `hypertension has been noted htn stage 2`. Count of \"htn\" is $1$. So, $\\mathrm{tf}(t_1, d_3) = 1$.\n- $d_4$: \"No history of HTN; blood pressure normal.\" $\\rightarrow$ `no history of htn blood pressure normal`. Count of \"htn\" is $1$. So, $\\mathrm{tf}(t_1, d_4) = 1$.\n- $d_5$: \"Family history negative for hypertension.\" $\\rightarrow$ Count of \"htn\" is $0$. So, $\\mathrm{tf}(t_1, d_5) = 0$.\n\nThe term frequency vector for $t_{1}$ across the corpus is $T_{\\text{htn}} = (2, 0, 1, 1, 0)$.\n\nFor $t_2 = \\text{\"hypertension\"}$:\n- $d_1$: Count of \"hypertension\" is $0$. So, $\\mathrm{tf}(t_2, d_1) = 0$.\n- $d_2$: \"History of hypertension...\" $\\rightarrow$ `history of hypertension...`. Count of \"hypertension\" is $1$. So, $\\mathrm{tf}(t_2, d_2) = 1$.\n- $d_3$: \"Hypertension has been noted...\" $\\rightarrow$ `hypertension has been noted...`. Count of \"hypertension\" is $1$. So, $\\mathrm{tf}(t_2, d_3) = 1$.\n- $d_4$: Count of \"hypertension\" is $0$. So, $\\mathrm{tf}(t_2, d_4) = 0$.\n- $d_5$: \"Family history negative for hypertension.\" $\\rightarrow$ `... for hypertension`. Count of \"hypertension\" is $1$. So, $\\mathrm{tf}(t_2, d_5) = 1$.\n\nThe term frequency vector for $t_{2}$ is $T_{\\text{hypertension}} = (0, 1, 1, 0, 1)$.\n\n**Step 2: Calculate Inverse Document Frequencies (IDF)**\nThe IDF for a term $t$ is given by $\\mathrm{idf}(t) = \\ln(\\frac{N}{\\mathrm{df}(t)})$, where $N=5$ is the total number of documents and $\\mathrm{df}(t)$ is the number of documents containing term $t$.\n\nFor $t_1 = \\text{\"htn\"}$:\nThe term \"htn\" appears in documents $d_{1}$, $d_{3}$, and $d_{4}$. Thus, the document frequency is $\\mathrm{df}(t_1) = 3$.\nThe IDF is $\\mathrm{idf}(t_1) = \\ln(\\frac{5}{3})$.\n\nFor $t_2 = \\text{\"hypertension\"}$:\nThe term \"hypertension\" appears in documents $d_{2}$, $d_{3}$, and $d_{5}$. Thus, the document frequency is $\\mathrm{df}(t_2) = 3$.\nThe IDF is $\\mathrm{idf}(t_2) = \\ln(\\frac{5}{3})$.\n\nNotably, both terms have the same IDF value because they appear in the same number of documents, reflecting equal rarity at the corpus level.\n\n**Step 3: Construct the TF-IDF Vectors**\nThe $i$-th component of the TF-IDF vector $v_{t}$ is the product of the term frequency and the inverse document frequency: $(v_t)_i = \\mathrm{tf}(t,d_i) \\cdot \\mathrm{idf}(t)$.\n\nLet $v_{\\text{htn}}$ be the vector for $t_1$ and $v_{\\text{hypertension}}$ be the vector for $t_2$.\n$v_{\\text{htn}} = \\left( \\mathrm{tf}(t_1, d_1)\\cdot\\mathrm{idf}(t_1), \\dots, \\mathrm{tf}(t_1, d_5)\\cdot\\mathrm{idf}(t_1) \\right)$\n$v_{\\text{htn}} = \\ln(\\frac{5}{3}) \\cdot (2, 0, 1, 1, 0)$\n\n$v_{\\text{hypertension}} = \\left( \\mathrm{tf}(t_2, d_1)\\cdot\\mathrm{idf}(t_2), \\dots, \\mathrm{tf}(t_2, d_5)\\cdot\\mathrm{idf}(t_2) \\right)$\n$v_{\\text{hypertension}} = \\ln(\\frac{5}{3}) \\cdot (0, 1, 1, 0, 1)$\n\n**Step 4: Compute the Cosine Similarity**\nThe cosine similarity between two vectors $v_{a}$ and $v_{b}$ is defined as $\\cos\\theta = \\frac{v_{a} \\cdot v_{b}}{\\|v_{a}\\|_{2}\\,\\|v_{b}\\|_{2}}$.\n\nLet $I = \\ln(\\frac{5}{3})$. The vectors are $v_{\\text{htn}} = I \\cdot T_{\\text{htn}}$ and $v_{\\text{hypertension}} = I \\cdot T_{\\text{hypertension}}$.\n\nThe dot product is:\n$$v_{\\text{htn}} \\cdot v_{\\text{hypertension}} = (I \\cdot T_{\\text{htn}}) \\cdot (I \\cdot T_{\\text{hypertension}}) = I^2 (T_{\\text{htn}} \\cdot T_{\\text{hypertension}})$$\n$$T_{\\text{htn}} \\cdot T_{\\text{hypertension}} = (2)(0) + (0)(1) + (1)(1) + (1)(0) + (0)(1) = 1$$\nSo, $v_{\\text{htn}} \\cdot v_{\\text{hypertension}} = I^2 \\cdot 1 = \\left(\\ln(\\frac{5}{3})\\right)^2$.\n\nThe Euclidean norms are:\n$$\\|v_{\\text{htn}}\\|_{2} = \\|I \\cdot T_{\\text{htn}}\\|_{2} = |I| \\cdot \\|T_{\\text{htn}}\\|_{2}$$\n$$\\|T_{\\text{htn}}\\|_{2} = \\sqrt{2^2 + 0^2 + 1^2 + 1^2 + 0^2} = \\sqrt{4 + 1 + 1} = \\sqrt{6}$$\nSo, $\\|v_{\\text{htn}}\\|_{2} = \\ln(\\frac{5}{3}) \\sqrt{6}$.\n\n$$\\|v_{\\text{hypertension}}\\|_{2} = \\|I \\cdot T_{\\text{hypertension}}\\|_{2} = |I| \\cdot \\|T_{\\text{hypertension}}\\|_{2}$$\n$$\\|T_{\\text{hypertension}}\\|_{2} = \\sqrt{0^2 + 1^2 + 1^2 + 0^2 + 1^2} = \\sqrt{1 + 1 + 1} = \\sqrt{3}$$\nSo, $\\|v_{\\text{hypertension}}\\|_{2} = \\ln(\\frac{5}{3}) \\sqrt{3}$.\n\nNow, we calculate the cosine similarity:\n$$\\cos\\theta = \\frac{v_{\\text{htn}} \\cdot v_{\\text{hypertension}}}{\\|v_{\\text{htn}}\\|_{2}\\,\\|v_{\\text{hypertension}}\\|_{2}} = \\frac{\\left(\\ln(\\frac{5}{3})\\right)^2}{\\left(\\ln(\\frac{5}{3})\\sqrt{6}\\right) \\left(\\ln(\\frac{5}{3})\\sqrt{3}\\right)}$$\nThe common factor of $(\\ln(\\frac{5}{3}))^2$ cancels from the numerator and denominator, provided it is non-zero, which it is since $\\frac{5}{3} \\neq 1$.\n$$\\cos\\theta = \\frac{1}{\\sqrt{6} \\cdot \\sqrt{3}} = \\frac{1}{\\sqrt{18}} = \\frac{1}{3\\sqrt{2}}$$\nTo rationalize the denominator, we multiply the numerator and denominator by $\\sqrt{2}$:\n$$\\cos\\theta = \\frac{\\sqrt{2}}{3\\sqrt{2} \\cdot \\sqrt{2}} = \\frac{\\sqrt{2}}{3 \\cdot 2} = \\frac{\\sqrt{2}}{6}$$\n\nThis result demonstrates that when the IDF values for two terms are identical, the cosine similarity of their TF-IDF vectors simplifies to the cosine similarity of their raw term frequency vectors. The TF-IDF weighting, in this specific case, does not alter the angle between the vectors, only their magnitudes. The similarity value itself, which is positive but far from $1$, reflects the fact that the terms co-occur in one document ($d_3$) but appear in distinct document sets otherwise, indicating a relationship more complex than simple synonymy in this corpus (i.e., they are not perfectly interchangeable).\n\n**Step 5: Numerical Calculation**\nThe final step is to compute the numerical value and round it to four significant figures as required.\n$$\\cos\\theta = \\frac{\\sqrt{2}}{6} \\approx \\frac{1.41421356}{6} \\approx 0.23570226$$\nRounding to four significant figures, we get $0.2357$.", "answer": "$$\\boxed{0.2357}$$", "id": "4588751"}, {"introduction": "Building a model is only half the battle; evaluating its performance is paramount, especially in a high-stakes domain like medicine. This practice focuses on the essential task of evaluation, using a confusion matrix from a clinical relation extraction classifier to compute the core metrics of precision, recall, and the $F_1$ score. This exercise is critical for not only mastering these calculations but also for developing the crucial skill of interpreting what different types of model errors—false positives and false negatives—mean in the context of pharmacovigilance and patient safety [@problem_id:4588718].", "problem": "A clinical natural language processing system processes de-identified Electronic Health Records (EHR) to identify whether a documented drug mention is causally linked to an adverse event mention within the same clinical note. A binary relation classifier is trained on domain-expert annotated drug–event pairs. Each pair is labeled as either the positive class $C$ (a causal drug–event relation holds) or the negative class $\\neg C$ (no causal relation). On a held-out evaluation set of $1{,}500$ candidate pairs, the following confusion matrix counts are obtained for the positive class $C$: true positives $TP = 208$, false positives $FP = 104$, false negatives $FN = 52$, and true negatives $TN = 1{,}136$. Using foundational definitions of evaluation for binary classifiers in information retrieval and machine learning derived from set membership of predicted and true labels, compute the positive-class precision, the positive-class recall, and the $F_1$ score. Report the three metrics in the order: precision, recall, $F_1$. Round each metric to four significant figures. Additionally, concisely interpret, grounded in clinical pharmacovigilance reasoning, the distinct implications of false positives versus false negatives in this setting, focusing on potential impacts on downstream safety signal detection and clinical decision support (do not provide numerical values in the interpretation).", "solution": "The problem statement provides a self-contained and internally consistent set of data for evaluating a binary classifier in a well-defined context of clinical natural language processing. The given values for true positives ($TP$), false positives ($FP$), false negatives ($FN$), and true negatives ($TN$) sum to the total sample size: $208 + 104 + 52 + 1,136 = 1,500$. The task is grounded in established principles of machine learning evaluation and pharmacovigilance. Therefore, the problem is deemed valid and a solution can be formulated.\n\nThe first part of the task is to compute the precision, recall, and $F_1$ score for the positive class ($C$), which represents the presence of a causal drug-event relation. These metrics are defined as follows:\n\nPrecision ($P$) is the proportion of predicted positive instances that are actually positive. It measures the exactness of the classifier.\n$$P = \\frac{TP}{TP + FP}$$\n\nRecall ($R$), also known as sensitivity or true positive rate, is the proportion of actual positive instances that are correctly identified by the classifier. It measures the completeness of the classifier.\n$$R = \\frac{TP}{TP + FN}$$\n\nThe $F_1$ score is the harmonic mean of precision and recall, providing a single metric that balances both.\n$$F_1 = 2 \\cdot \\frac{P \\cdot R}{P + R} = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}$$\n\nUsing the provided values: $TP = 208$, $FP = 104$, and $FN = 52$.\n\nFirst, we compute the precision:\n$$P = \\frac{208}{208 + 104} = \\frac{208}{312} = \\frac{2}{3} \\approx 0.666666...$$\nRounding to four significant figures, we get $P \\approx 0.6667$.\n\nNext, we compute the recall:\n$$R = \\frac{208}{208 + 52} = \\frac{208}{260} = \\frac{4}{5} = 0.8$$\nTo express this with four significant figures, we write $R = 0.8000$.\n\nFinally, we compute the $F_1$ score using the exact fractional values of $P$ and $R$ to avoid premature rounding errors:\n$$F_1 = 2 \\cdot \\frac{\\left(\\frac{2}{3}\\right) \\cdot \\left(\\frac{4}{5}\\right)}{\\left(\\frac{2}{3}\\right) + \\left(\\frac{4}{5}\\right)} = 2 \\cdot \\frac{\\frac{8}{15}}{\\frac{10 + 12}{15}} = 2 \\cdot \\frac{\\frac{8}{15}}{\\frac{22}{15}} = 2 \\cdot \\frac{8}{22} = \\frac{16}{22} = \\frac{8}{11} \\approx 0.727272...$$\nRounding to four significant figures, we get $F_1 \\approx 0.7273$.\n\nThe second part of the task requires a concise interpretation of the implications of false positives versus false negatives in the context of clinical pharmacovigilance.\n\nA **false positive** ($FP$) occurs when the system incorrectly identifies a causal relationship between a drug and an adverse event. In the context of safety signal detection, this leads to the generation of false alarms. The primary consequence is a burden on human experts, who must expend valuable time and resources investigating these spurious signals, diverting them from legitimate safety issues. In a clinical decision support setting, a high rate of false positives can lead to \"alert fatigue,\" where clinicians become desensitized and begin to ignore all system warnings, including valid ones, thereby undermining the system's effectiveness and potentially increasing patient risk.\n\nA **false negative** ($FN$) occurs when the system fails to identify a true causal relationship between a drug and an adverse event. This represents a missed safety signal. The implications are severe and directly impact patient safety. A failure to detect a genuine adverse drug reaction means that patients may continue to be harmed by a medication, and the opportunity for early regulatory intervention (e.g., label updates, safety warnings) is lost. The delay in recognizing a true safety signal can lead to widespread, preventable harm across a patient population, representing a critical failure of the pharmacovigilance process. In this specific application, a false negative is of graver concern than a false positive due to the direct potential for patient harm.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.6667 & 0.8000 & 0.7273\n\\end{pmatrix}\n}\n$$", "id": "4588718"}]}