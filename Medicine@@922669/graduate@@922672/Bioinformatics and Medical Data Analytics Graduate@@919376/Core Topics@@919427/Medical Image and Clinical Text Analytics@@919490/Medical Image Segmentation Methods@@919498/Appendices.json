{"hands_on_practices": [{"introduction": "Before a deep learning model can learn, we must provide it with properly formatted data. This first practice addresses a fundamental challenge in 3D medical imaging: handling anisotropic voxel spacing, where dimensions are not uniform in physical space [@problem_id:4582642]. By designing an input patch with an isotropic physical field of view that also respects network-specific architectural constraints, you will develop a crucial data engineering skill essential for any segmentation pipeline.", "problem": "A Three-Dimensional Magnetic Resonance Imaging (3D MRI) dataset for abdominal organ segmentation is acquired with anisotropic voxel spacing, where the in-plane spacings are $s_{x} = 0.9\\,\\text{mm}$ and $s_{y} = 0.9\\,\\text{mm}$, and the through-plane spacing is $s_{z} = 4.5\\,\\text{mm}$. A Three-Dimensional Convolutional Neural Network (3D CNN) based on the U-Net segmentation architecture (U-Net) is to be trained with patches extracted from the volume. The encoder uses three downsampling stages, each with stride $(2, 2, 1)$ along $(x,y,z)$, and all convolutions use stride $1$ with \"same\" padding. To ensure consistent feature map dimensions at all pyramid levels, the patch dimensions along $x$ and $y$ must be divisible by $2^{3}$, while the $z$ dimension has no divisibility constraint from downsampling.\n\nDesign a patch such that the physical field of view (FOV) of the patch is isotropic across axes and meets a minimum physical extent requirement to capture sufficient anatomical context. Specifically, let the patch dimensions in voxels be $(P_{x}, P_{y}, P_{z})$, and require:\n- $P_{x} s_{x} = P_{y} s_{y} = P_{z} s_{z}$ (isotropic physical FOV),\n- $P_{x} s_{x} \\geq 72\\,\\text{mm}$ (minimum physical FOV),\n- $P_{x}$ and $P_{y}$ are divisible by $2^{3}$.\n\nUnder these constraints, determine the minimal integer triple $(P_{x}, P_{y}, P_{z})$ in voxels that satisfies all conditions. Express the final answer as a row matrix using the LaTeX $\\texttt{pmatrix}$ environment. No rounding is required, and no units should be included in the final answer box.", "solution": "The problem is to determine the minimal integer dimensions of a patch, $(P_{x}, P_{y}, P_{z})$, in voxels for training a 3D Convolutional Neural Network. The design must satisfy several constraints related to voxel spacing, network architecture, and physical field of view.\n\nFirst, we formalize the given information and constraints. The voxel spacings are $s_{x} = 0.9\\,\\text{mm}$, $s_{y} = 0.9\\,\\text{mm}$, and $s_{z} = 4.5\\,\\text{mm}$. The patch dimensions $(P_{x}, P_{y}, P_{z})$ must be integers.\n\nThe constraints are as follows:\n1.  **Architectural Constraint**: The encoder of the network has three downsampling stages with a stride of $2$ along the $x$ and $y$ axes. Therefore, the initial patch dimensions $P_{x}$ and $P_{y}$ must be divisible by $2^{3} = 8$ to ensure valid dimensions at all levels of the network. There is no such constraint on $P_z$ as the stride is $1$ along the $z$-axis.\n2.  **Isotropic FOV Constraint**: The physical field of view (FOV) of the patch must be isotropic. This means the physical size along each axis must be equal. Let this common physical size be $L$.\n    $$L = P_{x} s_{x} = P_{y} s_{y} = P_{z} s_{z}$$\n3.  **Minimum FOV Constraint**: The physical FOV must be at least $72\\,\\text{mm}$.\n    $$L \\geq 72\\,\\text{mm}$$\n\nOur objective is to find the minimal integer triple $(P_{x}, P_{y}, P_{z})$ that satisfies all these conditions.\n\nWe begin by analyzing the isotropic FOV constraint. Using the first two parts of the equality and the given spacings:\n$$P_{x} s_{x} = P_{y} s_{y}$$\n$$P_{x} (0.9) = P_{y} (0.9)$$\nThis simplifies to $P_{x} = P_{y}$. This means any constraint on $P_{x}$ also applies to $P_{y}$, which is consistent with the architectural constraint that both must be divisible by $8$.\n\nNext, we use the constraint relating the $x$ and $z$ dimensions:\n$$P_{x} s_{x} = P_{z} s_{z}$$\nSubstituting the given voxel spacings:\n$$P_{x} (0.9) = P_{z} (4.5)$$\nWe can solve for $P_{x}$ in terms of $P_{z}$:\n$$P_{x} = \\frac{4.5}{0.9} P_{z} = 5 P_{z}$$\nSince $P_{x}$ and $P_{z}$ must both be integers, this equation implies that $P_{x}$ must be an integer multiple of $5$.\n\nNow, we combine all the constraints on $P_{x}$:\na) $P_{x}$ must be divisible by $8$ (from the architectural constraint).\nb) $P_{x}$ must be divisible by $5$ (from the isotropic FOV and integer voxel constraints).\nc) The physical FOV $L = P_{x} s_{x}$ must be at least $72\\,\\text{mm}$.\n   $$P_{x} (0.9) \\geq 72$$\n   $$P_{x} \\geq \\frac{72}{0.9} = \\frac{720}{9} = 80$$\n\nTo satisfy both (a) and (b), $P_{x}$ must be a common multiple of $8$ and $5$. Since $8$ and $5$ are coprime (their greatest common divisor is $1$), $P_{x}$ must be a multiple of their least common multiple, which is $\\text{lcm}(8, 5) = 8 \\times 5 = 40$.\n\nSo, we are looking for the smallest integer $P_{x}$ that is a multiple of $40$ and is also greater than or equal to $80$. The multiples of $40$ are $40, 80, 120, \\dots$. The smallest multiple of $40$ that satisfies the condition $P_{x} \\geq 80$ is $80$.\n\nThus, the minimal value for $P_{x}$ is $80$.\n\nWith this minimal value for $P_{x}$, we can find the corresponding minimal values for $P_{y}$ and $P_{z}$:\n-   From $P_{x} = P_{y}$, we have $P_{y} = 80$.\n-   From $P_{x} = 5 P_{z}$, we have $80 = 5 P_{z}$, which gives $P_{z} = \\frac{80}{5} = 16$.\n\nThe minimal integer triple for the patch dimensions is $(P_{x}, P_{y}, P_{z}) = (80, 80, 16)$.\n\nAs a final check, we verify that this solution satisfies all initial conditions:\n-   $P_{x}=80$, $P_{y}=80$, $P_{z}=16$ are all integers.\n-   $P_{x}=80$ is divisible by $8$ ($80 = 8 \\times 10$).\n-   $P_{y}=80$ is divisible by $8$ ($80 = 8 \\times 10$).\n-   The physical FOV is $L = P_{x} s_{x} = 80 \\times 0.9\\,\\text{mm} = 72\\,\\text{mm}$. This satisfies $L \\geq 72\\,\\text{mm}$.\n-   The FOV is isotropic:\n    -   $P_{x} s_{x} = 80 \\times 0.9\\,\\text{mm} = 72\\,\\text{mm}$.\n    -   $P_{y} s_{y} = 80 \\times 0.9\\,\\text{mm} = 72\\,\\text{mm}$.\n    -   $P_{z} s_{z} = 16 \\times 4.5\\,\\text{mm} = 72\\,\\text{mm}$.\nAll conditions are met. Since we started by finding the smallest possible value for $P_x$, this triple is minimal.", "answer": "$$\\boxed{\\begin{pmatrix} 80 & 80 & 16 \\end{pmatrix}}$$", "id": "4582642"}, {"introduction": "With our data prepared, we must define the objective for learning. This is the role of the loss function, whose gradient guides the model's optimization. This exercise delves into the mathematical core of this process, asking you to derive the gradient for a powerful composite loss function built from the Tversky Index and Binary Cross-Entropy [@problem_id:4582623]. Understanding how to construct and differentiate custom loss functions is a vital skill for tackling specific challenges like class imbalance.", "problem": "You are training a binary medical image segmentation model that outputs, for each voxel index $i \\in \\{1,\\dots,N\\}$, a predicted foreground probability $p_i \\in (0,1)$ against a ground-truth label $y_i \\in \\{0,1\\}$. Consider a differentiable class-imbalance-aware loss constructed as a weighted sum of the negative logarithm of the Tversky Index (TI) and the Binary Cross-Entropy (BCE), defined from fundamental soft confusion components.\n\nStart from the standard soft counts definitions,\n$$\\mathrm{TP} = \\sum_{i=1}^{N} p_i y_i,\\quad \\mathrm{FP} = \\sum_{i=1}^{N} p_i (1 - y_i),\\quad \\mathrm{FN} = \\sum_{i=1}^{N} (1 - p_i) y_i,$$\nthe Tversky Index (TI) with nonnegative trade-off parameters $\\alpha$ and $\\beta$ and a stability constant $\\varepsilon > 0$,\n$$\\mathrm{TI}(\\alpha,\\beta) = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\alpha\\,\\mathrm{FP} + \\beta\\,\\mathrm{FN} + \\varepsilon},$$\nand Binary Cross-Entropy (BCE),\n$$\\mathrm{BCE}(\\boldsymbol{p},\\boldsymbol{y}) = -\\frac{1}{N}\\sum_{i=1}^{N}\\left( y_i \\ln p_i + (1 - y_i)\\ln(1 - p_i) \\right).$$\n\nDefine the combined loss\n$$\\mathcal{L}(\\boldsymbol{p}) = -\\ln\\!\\big(\\mathrm{TI}(\\alpha,\\beta)\\big) + \\lambda \\,\\mathrm{BCE}(\\boldsymbol{p},\\boldsymbol{y}),$$\nwhere the weight $\\lambda$ is chosen to balance the gradient magnitudes of the two constituent terms at the current point, in the sense that the Euclidean ($\\ell_2$) norm of the gradient of $-\\ln(\\mathrm{TI})$ equals that of $\\lambda\\,\\mathrm{BCE}$.\n\nUsing only these definitions and standard rules of calculus, derive the general expression for the partial derivative $\\frac{\\partial \\mathcal{L}}{\\partial p_k}$ for an arbitrary index $k$, and then evaluate it numerically at index $k = 2$ for the following data:\n- $N = 5$,\n- $\\boldsymbol{y} = [1,\\,1,\\,0,\\,0,\\,1]$,\n- $\\boldsymbol{p} = [0.7,\\,0.2,\\,0.1,\\,0.4,\\,0.6]$,\n- $\\alpha = 0.3$, $\\beta = 0.7$, and $\\varepsilon = 10^{-6}$,\n- $\\lambda$ is determined by the balancing condition $\\left\\|\\nabla_{\\boldsymbol{p}}\\big(-\\ln(\\mathrm{TI})\\big)\\right\\|_{2} = \\left\\|\\nabla_{\\boldsymbol{p}}\\big(\\lambda\\,\\mathrm{BCE}\\big)\\right\\|_{2}$ evaluated at the given $\\boldsymbol{p}$ and $\\boldsymbol{y}$.\n\nExpress the final answer for $\\frac{\\partial \\mathcal{L}}{\\partial p_2}$ as a real number rounded to four significant figures. No units are required.", "solution": "### Derivation of the Partial Derivative\nThe total loss is $\\mathcal{L} = \\mathcal{L}_{\\mathrm{TI}} + \\mathcal{L}_{\\mathrm{BCE}}$, where $\\mathcal{L}_{\\mathrm{TI}} = -\\ln(\\mathrm{TI})$ and $\\mathcal{L}_{\\mathrm{BCE}} = \\lambda\\,\\mathrm{BCE}$. The partial derivative is the sum of the partial derivatives of its components: $\\frac{\\partial \\mathcal{L}}{\\partial p_k} = \\frac{\\partial \\mathcal{L}_{\\mathrm{TI}}}{\\partial p_k} + \\frac{\\partial \\mathcal{L}_{\\mathrm{BCE}}}{\\partial p_k}$.\n\nFirst, we find the partial derivatives of the soft confusion components with respect to an arbitrary prediction $p_k$:\n$$\n\\frac{\\partial (\\mathrm{TP})}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\sum_{i=1}^{N} p_i y_i = y_k\n$$\n$$\n\\frac{\\partial (\\mathrm{FP})}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\sum_{i=1}^{N} p_i (1 - y_i) = 1 - y_k\n$$\n$$\n\\frac{\\partial (\\mathrm{FN})}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\sum_{i=1}^{N} (1 - p_i) y_i = -y_k\n$$\n\nNow, we derive the partial derivative for the Tversky Index loss component, $\\mathcal{L}_{\\mathrm{TI}}$.\n$$\n\\mathcal{L}_{\\mathrm{TI}} = -\\ln(\\mathrm{TI}) = -\\ln\\left(\\frac{\\mathrm{TP}}{\\mathrm{TP} + \\alpha\\,\\mathrm{FP} + \\beta\\,\\mathrm{FN} + \\varepsilon}\\right) = \\ln(\\mathrm{TP} + \\alpha\\,\\mathrm{FP} + \\beta\\,\\mathrm{FN} + \\varepsilon) - \\ln(\\mathrm{TP})\n$$\nUsing the chain rule:\n$$\n\\frac{\\partial \\mathcal{L}_{\\mathrm{TI}}}{\\partial p_k} = \\frac{1}{\\mathrm{TP} + \\alpha\\,\\mathrm{FP} + \\beta\\,\\mathrm{FN} + \\varepsilon} \\frac{\\partial}{\\partial p_k}(\\mathrm{TP} + \\alpha\\,\\mathrm{FP} + \\beta\\,\\mathrm{FN}) - \\frac{1}{\\mathrm{TP}}\\frac{\\partial (\\mathrm{TP})}{\\partial p_k}\n$$\nThe derivative of the denominator term is:\n$$\n\\frac{\\partial}{\\partial p_k}(\\mathrm{TP} + \\alpha\\,\\mathrm{FP} + \\beta\\,\\mathrm{FN}) = y_k + \\alpha(1-y_k) - \\beta y_k = \\alpha + y_k(1-\\alpha-\\beta)\n$$\nSubstituting this back, we get the derivative for the Tversky loss part:\n$$\n\\frac{\\partial \\mathcal{L}_{\\mathrm{TI}}}{\\partial p_k} = \\frac{\\alpha + y_k(1 - \\alpha - \\beta)}{\\mathrm{TP} + \\alpha\\,\\mathrm{FP} + \\beta\\,\\mathrm{FN} + \\varepsilon} - \\frac{y_k}{\\mathrm{TP}}\n$$\n\nNext, we derive the partial derivative for the BCE loss component, $\\mathcal{L}_{\\mathrm{BCE}}$.\n$$\n\\mathcal{L}_{\\mathrm{BCE}} = \\lambda\\,\\mathrm{BCE} = -\\frac{\\lambda}{N}\\sum_{i=1}^{N}\\left( y_i \\ln p_i + (1 - y_i)\\ln(1 - p_i) \\right)\n$$\nThe derivative with respect to $p_k$ only affects the $k$-th term of the sum:\n$$\n\\frac{\\partial \\mathcal{L}_{\\mathrm{BCE}}}{\\partial p_k} = -\\frac{\\lambda}{N} \\frac{\\partial}{\\partial p_k} \\left( y_k \\ln p_k + (1 - y_k)\\ln(1 - p_k) \\right)\n$$\n$$\n\\frac{\\partial \\mathcal{L}_{\\mathrm{BCE}}}{\\partial p_k} = -\\frac{\\lambda}{N} \\left( \\frac{y_k}{p_k} - \\frac{1-y_k}{1-p_k} \\right) = -\\frac{\\lambda}{N} \\frac{y_k(1-p_k) - p_k(1-y_k)}{p_k(1-p_k)} = -\\frac{\\lambda}{N} \\frac{y_k - p_k}{p_k(1-p_k)} = \\frac{\\lambda}{N} \\frac{p_k - y_k}{p_k(1-p_k)}\n$$\n\nCombining the two components gives the general expression for the total loss gradient:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p_k} = \\left( \\frac{\\alpha + y_k(1 - \\alpha - \\beta)}{\\mathrm{TP} + \\alpha\\,\\mathrm{FP} + \\beta\\,\\mathrm{FN} + \\varepsilon} - \\frac{y_k}{\\mathrm{TP}} \\right) + \\frac{\\lambda}{N} \\frac{p_k - y_k}{p_k(1 - p_k)}\n$$\n\n### Numerical Evaluation\nWe are given $N=5$, $\\boldsymbol{y}=[1,1,0,0,1]$, $\\boldsymbol{p}=[0.7,0.2,0.1,0.4,0.6]$, $\\alpha=0.3$, $\\beta=0.7$, $\\varepsilon=10^{-6}$.\n\nFirst, we compute the soft confusion matrix components:\n$$\n\\mathrm{TP} = \\sum p_i y_i = (0.7)(1) + (0.2)(1) + (0.1)(0) + (0.4)(0) + (0.6)(1) = 0.7 + 0.2 + 0.6 = 1.5\n$$\n$$\n\\mathrm{FP} = \\sum p_i (1-y_i) = (0.7)(0) + (0.2)(0) + (0.1)(1) + (0.4)(1) + (0.6)(0) = 0.1 + 0.4 = 0.5\n$$\n$$\n\\mathrm{FN} = \\sum (1-p_i) y_i = (0.3)(1) + (0.8)(1) + (0.9)(0) + (0.6)(0) + (0.4)(1) = 0.3 + 0.8 + 0.4 = 1.5\n$$\n\nNext, we determine $\\lambda$ from the balancing condition: $\\lambda = \\frac{\\left\\|\\nabla_{\\boldsymbol{p}}\\mathcal{L}_{\\mathrm{TI}}\\right\\|_{2}}{\\left\\|\\nabla_{\\boldsymbol{p}}\\mathrm{BCE}\\right\\|_{2}}$.\nLet $D_{\\mathrm{TI}} = \\mathrm{TP} + \\alpha\\,\\mathrm{FP} + \\beta\\,\\mathrm{FN} + \\varepsilon = 1.5 + (0.3)(0.5) + (0.7)(1.5) + 10^{-6} = 1.5 + 0.15 + 1.05 + 10^{-6} = 2.7 + 10^{-6}$. We can approximate $D_{\\mathrm{TI}} \\approx 2.7$ since $\\varepsilon$ is very small.\n\nThe gradient vector for $\\mathcal{L}_{\\mathrm{TI}} = -\\ln(\\mathrm{TI})$ has components $\\frac{\\partial \\mathcal{L}_{\\mathrm{TI}}}{\\partial p_k} = \\frac{\\alpha + y_k(1-\\alpha-\\beta)}{D_{\\mathrm{TI}}} - \\frac{y_k}{\\mathrm{TP}}$.\nSince $1-\\alpha-\\beta = 1 - 0.3 - 0.7 = 0$, the first term's numerator is simply $\\alpha$.\nFor $y_k=1$: $\\frac{\\partial \\mathcal{L}_{\\mathrm{TI}}}{\\partial p_k} = \\frac{0.3}{2.7} - \\frac{1}{1.5} = \\frac{1}{9} - \\frac{2}{3} = -\\frac{5}{9}$.\nFor $y_k=0$: $\\frac{\\partial \\mathcal{L}_{\\mathrm{TI}}}{\\partial p_k} = \\frac{0.3}{2.7} - \\frac{0}{1.5} = \\frac{1}{9}$.\nSince $\\boldsymbol{y}=[1,1,0,0,1]$, the gradient vector is $\\nabla_{\\boldsymbol{p}}\\mathcal{L}_{\\mathrm{TI}} = \\left[-\\frac{5}{9}, -\\frac{5}{9}, \\frac{1}{9}, \\frac{1}{9}, -\\frac{5}{9}\\right]$.\nIts $\\ell_2$ norm is:\n$$\n\\left\\|\\nabla_{\\boldsymbol{p}}\\mathcal{L}_{\\mathrm{TI}}\\right\\|_{2} = \\sqrt{3\\left(-\\frac{5}{9}\\right)^2 + 2\\left(\\frac{1}{9}\\right)^2} = \\sqrt{3\\frac{25}{81} + 2\\frac{1}{81}} = \\sqrt{\\frac{75+2}{81}} = \\sqrt{\\frac{77}{81}} = \\frac{\\sqrt{77}}{9}\n$$\n\nThe gradient vector for $\\mathrm{BCE}$ (note: unweighted by $\\lambda$) has components $\\frac{\\partial \\mathrm{BCE}}{\\partial p_k} = \\frac{1}{N} \\frac{p_k - y_k}{p_k(1-p_k)}$.\nFor $k=1$: $y_1=1, p_1=0.7 \\Rightarrow \\frac{1}{5} \\frac{0.7-1}{0.7(0.3)} = \\frac{1}{5}\\frac{-0.3}{0.21} = -\\frac{2}{7}$.\nFor $k=2$: $y_2=1, p_2=0.2 \\Rightarrow \\frac{1}{5} \\frac{0.2-1}{0.2(0.8)} = \\frac{1}{5}\\frac{-0.8}{0.16} = -1$.\nFor $k=3$: $y_3=0, p_3=0.1 \\Rightarrow \\frac{1}{5} \\frac{0.1-0}{0.1(0.9)} = \\frac{1}{5}\\frac{0.1}{0.09} = \\frac{2}{9}$.\nFor $k=4$: $y_4=0, p_4=0.4 \\Rightarrow \\frac{1}{5} \\frac{0.4-0}{0.4(0.6)} = \\frac{1}{5}\\frac{0.4}{0.24} = \\frac{1}{3}$.\nFor $k=5$: $y_5=1, p_5=0.6 \\Rightarrow \\frac{1}{5} \\frac{0.6-1}{0.6(0.4)} = \\frac{1}{5}\\frac{-0.4}{0.24} = -\\frac{1}{3}$.\nSo, $\\nabla_{\\boldsymbol{p}}\\mathrm{BCE} = \\left[-\\frac{2}{7}, -1, \\frac{2}{9}, \\frac{1}{3}, -\\frac{1}{3}\\right]$.\nIts $\\ell_2$ norm is:\n$$\n\\left\\|\\nabla_{\\boldsymbol{p}}\\mathrm{BCE}\\right\\|_{2} = \\sqrt{\\left(-\\frac{2}{7}\\right)^2 + (-1)^2 + \\left(\\frac{2}{9}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 + \\left(-\\frac{1}{3}\\right)^2} = \\sqrt{\\frac{4}{49} + 1 + \\frac{4}{81} + \\frac{1}{9} + \\frac{1}{9}}\n$$\n$$\n= \\sqrt{1 + \\frac{4}{49} + \\frac{22}{81}} = \\sqrt{\\frac{3969}{3969} + \\frac{4 \\times 81}{3969} + \\frac{22 \\times 49}{3969}} = \\sqrt{\\frac{3969 + 324 + 1078}{3969}} = \\sqrt{\\frac{5371}{3969}} = \\frac{\\sqrt{5371}}{63}\n$$\n\nThe balancing weight $\\lambda$ is therefore:\n$$\n\\lambda = \\frac{\\left\\|\\nabla_{\\boldsymbol{p}}\\mathcal{L}_{\\mathrm{TI}}\\right\\|_{2}}{\\left\\|\\nabla_{\\boldsymbol{p}}\\mathrm{BCE}\\right\\|_{2}} = \\frac{\\sqrt{77}/9}{\\sqrt{5371}/63} = \\frac{\\sqrt{77}}{9} \\frac{63}{\\sqrt{5371}} = \\frac{7\\sqrt{77}}{\\sqrt{5371}}\n$$\n\nFinally, we compute the value of the total loss derivative $\\frac{\\partial \\mathcal{L}}{\\partial p_k}$ at index $k=2$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p_2} = \\frac{\\partial \\mathcal{L}_{\\mathrm{TI}}}{\\partial p_2} + \\lambda \\frac{\\partial \\mathrm{BCE}}{\\partial p_2}\n$$\nWe have already calculated the components:\n$\\frac{\\partial \\mathcal{L}_{\\mathrm{TI}}}{\\partial p_2} = -\\frac{5}{9}$\n$\\frac{\\partial \\mathrm{BCE}}{\\partial p_2} = -1$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p_2} = -\\frac{5}{9} + \\left(\\frac{7\\sqrt{77}}{\\sqrt{5371}}\\right)(-1) = -\\frac{5}{9} - \\frac{7\\sqrt{77}}{\\sqrt{5371}}\n$$\nNow, we compute the numerical value:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p_2} \\approx -0.555555... - 7\\sqrt{\\frac{77}{5371}} \\approx -0.555555... - 7\\sqrt{0.01433625...}\n$$\n$$\n\\approx -0.555555... - 7(0.1197341...) \\approx -0.555555... - 0.8381387... \\approx -1.3936942...\n$$\nRounding to four significant figures, we get $-1.394$.", "answer": "$$\\boxed{-1.394}$$", "id": "4582623"}, {"introduction": "A robust training regimen involves more than a well-designed loss function; it requires dynamic strategies that adapt throughout training. This final practice explores several such techniques, including data augmentation via linear label mixing and sophisticated learning rate scheduling with cosine annealing [@problem_id:4582643]. Implementing these methods is key to improving model generalization, accelerating convergence, and pushing the boundaries of segmentation performance.", "problem": "You are given a training setup for binary medical image segmentation that uses augmentation by linear label mixing and a learning rate schedule based on a cosine profile. The objective is to derive and implement, from first principles, the mathematical quantities that govern training under this protocol, and then compute them for specified test cases. The base principles you must use are the definitions of Empirical Risk Minimization (ERM), Cross-Entropy (CE) for Bernoulli labels, and the Dice Similarity Coefficient (DSC), together with the requirement that the learning rate schedule is a smooth function that starts at a maximum and ends at a minimum over a cycle. All angles must be treated in radians.\n\nTask A (Loss under label mixing): Consider two binary label vectors $y_1 \\in \\{0,1\\}^n$ and $y_2 \\in \\{0,1\\}^n$, and predicted positive class probabilities $p \\in [0,1]^n$ for a patch of $n$ pixels. The augmentation is a linear label mixing (commonly known as \"mixup\") yielding a soft label $y' = \\lambda y_1 + (1-\\lambda) y_2$ for a mixing coefficient $\\lambda \\in [0,1]$. Starting from the definition of the Cross-Entropy (CE) for Bernoulli targets and the linearity of expectation, derive the expression for the average per-pixel CE under the soft label $y'$ and implement its computation for given $p$, $y_1$, $y_2$, and $\\lambda$.\n\nTask B (Soft Dice for probabilistic predictions): Using the standard definition of the Dice Similarity Coefficient (DSC) for sets and extending it to probabilistic predictions by interpreting sums of products as expected overlap, derive the expression for the soft Dice between $p$ and $y'$ and implement its computation for given $p$, $y'$.\n\nTask C (Cosine-annealed learning rate within a cycle): A training protocol uses Cosine Annealing with Warm Restarts (CAWR) in which the learning rate over a cycle of length $T$ steps starts at a maximum $\\eta_0$ at step $t=0$ and ends at a minimum at step $t=T$, following a single half-wave of a cosine in radians. Starting from the requirements that the schedule is smooth, attains $\\eta_0$ at the start and a minimum at the end, and is monotonically decreasing over the cycle, derive a formula for the learning rate at an integer step $t \\in \\{0,1,\\dots,T\\}$ within a cycle and implement its computation for given $\\eta_0$, $T$, and $t$.\n\nTest Suite: For each test case, you are given $p$, $y_1$, $y_2$, $\\lambda$, $\\eta_0$, $T$, and $t$. Compute three quantities: the average CE loss (Task A), the soft Dice (Task B), and the learning rate (Task C). Use the following four test cases with $n=4$:\n\n- Test Case $1$:\n  - $p = [0.9, 0.2, 0.8, 0.1]$\n  - $y_1 = [1, 0, 1, 0]$\n  - $y_2 = [0, 1, 0, 1]$\n  - $\\lambda = 0.3$\n  - $\\eta_0 = 0.01$\n  - $T = 10$\n  - $t = 3$\n\n- Test Case $2$ (boundary mixing and cycle start):\n  - $p = [0.5, 0.5, 0.5, 0.5]$\n  - $y_1 = [1, 1, 0, 0]$\n  - $y_2 = [0, 0, 1, 1]$\n  - $\\lambda = 1.0$\n  - $\\eta_0 = 0.001$\n  - $T = 20$\n  - $t = 0$\n\n- Test Case $3$ (boundary mixing and cycle end):\n  - $p = [0.99, 0.01, 0.99, 0.01]$\n  - $y_1 = [1, 0, 1, 0]$\n  - $y_2 = [1, 0, 0, 1]$\n  - $\\lambda = 0.0$\n  - $\\eta_0 = 0.02$\n  - $T = 5$\n  - $t = 5$\n\n- Test Case $4$ (symmetric mixing and cycle end):\n  - $p = [0.8, 0.8, 0.2, 0.2]$\n  - $y_1 = [1, 1, 0, 0]$\n  - $y_2 = [0, 0, 1, 1]$\n  - $\\lambda = 0.5$\n  - $\\eta_0 = 0.005$\n  - $T = 7$\n  - $t = 7$\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test caseâ€™s result should itself be a bracketed triple in the order $[\\text{loss}, \\text{dice}, \\text{learning\\ rate}]$. For example, the full output should look like $[[\\text{L}_1,\\text{D}_1,\\text{LR}_1],[\\text{L}_2,\\text{D}_2,\\text{LR}_2],\\dots]$. If any logarithm appears in your computation, use angles in radians and treat numerical stability by clipping probabilities to avoid undefined logarithms.", "solution": "### Derivations and Solution\n\n**Task A: Average Cross-Entropy Loss under Label Mixing**\n\nThe Cross-Entropy (CE) loss measures the dissimilarity between a true probability distribution and a predicted one. For a binary classification problem (e.g., pixel-wise segmentation), the ground truth for a single pixel $i$ is a Bernoulli random variable with parameter $y_i \\in \\{0,1\\}$. The predicted probability for the positive class ($1$) is $p_i$. The CE loss for this single pixel is given by:\n$$\n\\mathcal{L}_{CE,i}(p_i, y_i) = - [y_i \\log(p_i) + (1-y_i) \\log(1-p_i)]\n$$\nwhere $\\log$ denotes the natural logarithm, as is standard in information theory. In Empirical Risk Minimization (ERM), the total loss is the average loss over all $n$ pixels in the given data sample (image patch).\n\nThe problem introduces a soft label $y'$ derived from linear mixing of two hard labels, $y_1$ and $y_2$:\n$$\ny' = \\lambda y_1 + (1-\\lambda) y_2\n$$\nFor each pixel $i$, the soft label is $y'_i = \\lambda y_{1,i} + (1-\\lambda) y_{2,i}$. Since $y_{1,i}, y_{2,i} \\in \\{0,1\\}$ and $\\lambda \\in [0,1]$, we have $y'_i \\in [0,1]$. We can interpret $y'_i$ as the parameter of a Bernoulli distribution, i.e., the probability of the true label being $1$. The CE loss is a valid measure for such probabilistic targets.\n\nTo find the CE loss for the soft label $y'$, we substitute $y'_i$ for $y_i$ in the per-pixel loss formula:\n$$\n\\mathcal{L}_{CE,i}(p_i, y'_i) = - [y'_i \\log(p_i) + (1-y'_i) \\log(1-p_i)]\n$$\nThe average per-pixel CE loss over the patch of $n$ pixels is the mean of these individual losses:\n$$\n\\mathcal{L}_{CE}(p, y') = \\frac{1}{n} \\sum_{i=1}^n \\mathcal{L}_{CE,i}(p_i, y'_i) = -\\frac{1}{n} \\sum_{i=1}^n [y'_i \\log(p_i) + (1-y'_i) \\log(1-p_i)]\n$$\nThis is the final expression for the loss. As required by the problem for numerical stability, to prevent the logarithm from being undefined, the predicted probabilities $p_i$ must be clipped to a small-epsilon-bounded range, for instance $[\\epsilon, 1-\\epsilon]$ for a small $\\epsilon > 0$.\n\n**Task B: Soft Dice Similarity Coefficient**\n\nThe Dice Similarity Coefficient (DSC) is a statistic used to gauge the similarity of two sets. For sets $A$ and $B$, it is defined as:\n$$\nDSC(A,B) = \\frac{2 |A \\cap B|}{|A| + |B|}\n$$\nIn image segmentation, we can represent the ground truth mask and the predicted mask as binary vectors $y$ and $\\hat{y}$ of length $n$. The set operations translate to vector operations:\n- $|A| \\rightarrow \\sum_{i=1}^n y_i$ (the number of positive pixels in the ground truth)\n- $|B| \\rightarrow \\sum_{i=1}^n \\hat{y}_i$ (the number of positive pixels in the prediction)\n- $|A \\cap B| \\rightarrow \\sum_{i=1}^n y_i \\hat{y}_i$ (the number of correctly predicted positive pixels, i.e., true positives)\n\nThe task requires extending this to a probabilistic prediction $p$ and a soft ground truth $y'$. This is achieved by replacing the binary indicators with their probabilistic counterparts. The sums of products are interpreted as expected counts.\n- The prediction $\\hat{y}$ is replaced by the probability vector $p$.\n- The ground truth $y$ is replaced by the soft label vector $y'$.\n\nThe terms in the DSC formula are then generalized as follows:\n- Expected size of the predicted set: $\\sum_{i=1}^n p_i$\n- Expected size of the ground truth set: $\\sum_{i=1}^n y'_i$\n- Expected size of the intersection: $\\sum_{i=1}^n p_i y'_i$\n\nSubstituting these into the DSC definition yields the soft Dice coefficient:\n$$\nDSC_{soft}(p, y') = \\frac{2 \\sum_{i=1}^n p_i y'_i}{\\sum_{i=1}^n p_i + \\sum_{i=1}^n y'_i}\n$$\nThis formula computes the Dice score for probabilistic inputs. To prevent division by zero in cases where both the prediction and ground truth are all zeros, a small smoothing constant $\\epsilon$ can be added to the numerator and denominator, but we will adhere to the direct derivation as the problem does not specify this.\n\n**Task C: Cosine-Annealed Learning Rate**\n\nThe learning rate schedule $\\eta(t)$ must follow a single half-wave of a cosine over a cycle of $T$ steps, for $t \\in \\{0, 1, \\dots, T\\}$. A general cosine function is of the form $f(x) = A \\cos(x) + C$. We need to scale and shift this to meet the given criteria. Let the learning rate be given by:\n$$\n\\eta(t) = C + A \\cos(\\omega t + \\phi)\n$$\nThe requirements are:\n$1$. $\\eta(0) = \\eta_0$ (maximum value). This implies the cosine term should be at its maximum ($+1$) at $t=0$. Choosing $\\phi=0$ simplifies this. So, $\\eta(t) = C + A \\cos(\\omega t)$.\n$2$. $\\eta(T) = \\eta_{min}$ (minimum value). This implies the cosine term should be at its minimum ($-1$) at $t=T$.\n$3$. The schedule describes a single half-wave. This means the argument of the cosine must go from $0$ to $\\pi$ as $t$ goes from $0$ to $T$.\n\nFrom requirement $3$, we set the argument of the cosine to be $\\frac{t\\pi}{T}$. Our function becomes:\n$$\n\\eta(t) = C + A \\cos\\left(\\frac{t\\pi}{T}\\right)\n$$\nNow we apply the boundary conditions:\n- At $t=0$: $\\eta(0) = C + A \\cos(0) = C + A = \\eta_0$.\n- At $t=T$: $\\eta(T) = C + A \\cos(\\pi) = C - A = \\eta_{min}$.\n\nThe problem states the schedule \"ends at a minimum\", but does not specify a value for $\\eta_{min}$. Standard cosine annealing schedules often anneal to a minimum of $\\eta_{min}=0$. This is the most natural interpretation of a \"single half-wave\" that starts at a maximum. Assuming $\\eta_{min}=0$:\n- $C + A = \\eta_0$\n- $C - A = 0 \\implies C = A$\n\nSubstituting $C=A$ into the first equation gives $2A = \\eta_0$, so $A = \\frac{\\eta_0}{2}$. This also means $C = \\frac{\\eta_0}{2}$.\nThe final formula for the learning rate at step $t$ is therefore:\n$$\n\\eta(t) = \\frac{\\eta_0}{2} + \\frac{\\eta_0}{2} \\cos\\left(\\frac{t\\pi}{T}\\right) = \\frac{\\eta_0}{2}\\left(1 + \\cos\\left(\\frac{t\\pi}{T}\\right)\\right)\n$$\nThis function correctly starts at $\\eta(0)=\\eta_0$, ends at $\\eta(T)=0$, is smooth, and is monotonically decreasing over the interval $t \\in [0, T]$, thus satisfying all stated principles.", "answer": "[[1.418063073062016,0.36,0.007938926261462366],[0.6931471805599453,0.6666666666666666,0.001],[0.2325381163456064,0.6622073578595317,0.0],[0.5108256237659907,0.8,0.0]]", "id": "4582643"}]}