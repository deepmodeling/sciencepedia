{"hands_on_practices": [{"introduction": "The foundation of any successful supervised machine learning model is a high-quality, reliably annotated dataset. In clinical NER, where ambiguity is common, it is crucial to measure the consistency between human annotators before using their labels as a \"gold standard\". This exercise [@problem_id:4547588] provides hands-on practice in calculating Cohen's kappa ($\\kappa$), a key metric for Inter-Annotator Agreement (IAA) that corrects for chance, allowing you to quantitatively assess the reliability of your foundational data.", "problem": "A hospital research team is evaluating the consistency of clinical Named Entity Recognition (NER) annotations in a short de-identified physician note. Two human raters independently annotated each token into one of five mutually exclusive classes: Outside ($O$), Problem ($\\mathrm{PROB}$), Medication ($\\mathrm{MED}$), Laboratory Test ($\\mathrm{TEST}$), and Anatomy ($\\mathrm{ANAT}$). The total number of tokens is $60$. The token-level joint counts between the two raters are summarized as follows, where each bullet denotes counts for the subset of tokens to which Rater A assigned a fixed class, partitioned by Rater B’s assignments:\n\n- For tokens Rater A labeled $O$: Rater B labeled $O$ for $38$ tokens, $\\mathrm{PROB}$ for $1$ token, $\\mathrm{MED}$ for $0$ tokens, $\\mathrm{TEST}$ for $1$ token, and $\\mathrm{ANAT}$ for $0$ tokens.\n- For tokens Rater A labeled $\\mathrm{PROB}$: Rater B labeled $O$ for $2$ tokens, $\\mathrm{PROB}$ for $5$ tokens, $\\mathrm{MED}$ for $0$ tokens, $\\mathrm{TEST}$ for $1$ token, and $\\mathrm{ANAT}$ for $0$ tokens.\n- For tokens Rater A labeled $\\mathrm{MED}$: Rater B labeled $O$ for $1$ token, $\\mathrm{PROB}$ for $0$ tokens, $\\mathrm{MED}$ for $3$ tokens, $\\mathrm{TEST}$ for $1$ token, and $\\mathrm{ANAT}$ for $1$ token.\n- For tokens Rater A labeled $\\mathrm{TEST}$: Rater B labeled $O$ for $1$ token, $\\mathrm{PROB}$ for $0$ tokens, $\\mathrm{MED}$ for $1$ token, $\\mathrm{TEST}$ for $2$ tokens, and $\\mathrm{ANAT}$ for $0$ tokens.\n- For tokens Rater A labeled $\\mathrm{ANAT}$: Rater B labeled $O$ for $0$ tokens, $\\mathrm{PROB}$ for $1$ token, $\\mathrm{MED}$ for $1$ token, $\\mathrm{TEST}$ for $0$ tokens, and $\\mathrm{ANAT}$ for $0$ tokens.\n\nAssume the standard reliability-theoretic definition of observed agreement and expected agreement under independent raters based on their marginal label distributions. Compute the token-level Cohen’s kappa $\\kappa$ for these annotations, expressed as a single real number. Round your final numeric answer to four significant figures.\n\nThen, briefly explain, without computing any additional numeric metric, how span-level agreement (for example, Inter-Annotator Agreement (IAA) based on exact span matches) might differ from token-level agreement in this setting, particularly in the presence of boundary alignment issues such as one rater annotating “$\\mathrm{Type\\ 2\\ diabetes\\ mellitus}$” as a single $\\mathrm{PROB}$ span while the other annotates “$\\mathrm{diabetes}$” only. No units are required for the final numeric answer.", "solution": "The problem is validated as scientifically grounded, well-posed, objective, and self-consistent. The provided data is complete and sufficient for the calculation of Cohen's kappa, a standard metric for inter-rater reliability. The total number of tokens derived from the joint counts ($38+1+0+1+0 + 2+5+0+1+0 + 1+0+3+1+1 + 1+0+1+2+0 + 0+1+1+0+0=60$) matches the stated total of $60$. The problem is therefore valid.\n\nThe solution proceeds in two parts: first, the computation of Cohen's kappa ($\\kappa$), and second, a conceptual explanation of the difference between token-level and span-level agreement.\n\nPart 1: Computation of Cohen's Kappa\n\nCohen's kappa coefficient is defined as $\\kappa = \\frac{p_o - p_e}{1 - p_e}$, where $p_o$ is the observed proportional agreement between raters, and $p_e$ is the hypothetical probability of chance agreement. The total number of annotated tokens is $N=60$. The five annotation classes are $O$, $\\mathrm{PROB}$, $\\mathrm{MED}$, $\\mathrm{TEST}$, and $\\mathrm{ANAT}$.\n\nFirst, we construct a contingency table (or confusion matrix) from the given data, where the rows represent the labels assigned by Rater A and the columns represent the labels assigned by Rater B. Let $n_{ij}$ be the number of tokens assigned to class $i$ by Rater A and class $j$ by Rater B.\n\nThe contingency table of counts is:\n$$\nN_{obs} = \n\\begin{pmatrix}\n & \\text{O} & \\text{PROB} & \\text{MED} & \\text{TEST} & \\text{ANAT} \\\\\n\\text{O} & 38 & 1 & 0 & 1 & 0 \\\\\n\\text{PROB} & 2 & 5 & 0 & 1 & 0 \\\\\n\\text{MED} & 1 & 0 & 3 & 1 & 1 \\\\\n\\text{TEST} & 1 & 0 & 1 & 2 & 0 \\\\\n\\text{ANAT} & 0 & 1 & 1 & 0 & 0\n\\end{pmatrix}\n$$\n\nNext, we calculate the observed agreement, $p_o$. This is the proportion of tokens for which both raters agree on the label. It is the sum of the diagonal elements of the contingency table divided by the total number of tokens, $N$.\n$$\np_o = \\frac{\\sum_{i=1}^{5} n_{ii}}{N} = \\frac{38 + 5 + 3 + 2 + 0}{60} = \\frac{48}{60} = 0.8\n$$\n\nNext, we calculate the expected agreement by chance, $p_e$. This requires the marginal totals for each rater. Let $n_{i \\cdot}$ be the total number of tokens labeled as class $i$ by Rater A (row sums), and let $n_{\\cdot j}$ be the total number of tokens labeled as class $j$ by Rater B (column sums).\n\nRow sums (Rater A's marginals):\n$n_{O \\cdot} = 38 + 1 + 0 + 1 + 0 = 40$\n$n_{\\mathrm{PROB} \\cdot} = 2 + 5 + 0 + 1 + 0 = 8$\n$n_{\\mathrm{MED} \\cdot} = 1 + 0 + 3 + 1 + 1 = 6$\n$n_{\\mathrm{TEST} \\cdot} = 1 + 0 + 1 + 2 + 0 = 4$\n$n_{\\mathrm{ANAT} \\cdot} = 0 + 1 + 1 + 0 + 0 = 2$\nThe sum of row totals is $40+8+6+4+2=60=N$.\n\nColumn sums (Rater B's marginals):\n$n_{\\cdot O} = 38 + 2 + 1 + 1 + 0 = 42$\n$n_{\\cdot \\mathrm{PROB}} = 1 + 5 + 0 + 0 + 1 = 7$\n$n_{\\cdot \\mathrm{MED}} = 0 + 0 + 3 + 1 + 1 = 5$\n$n_{\\cdot \\mathrm{TEST}} = 1 + 1 + 1 + 2 + 0 = 5$\n$n_{\\cdot \\mathrm{ANAT}} = 0 + 0 + 1 + 0 + 0 = 1$\nThe sum of column totals is $42+7+5+5+1=60=N$.\n\nThe expected agreement, $p_e$, is the sum of the products of the marginal probabilities for each class, assuming the raters' judgments are independent.\n$$\np_e = \\sum_{i \\in \\{\\text{classes}\\}} P(\\text{Rater A}=i) \\times P(\\text{Rater B}=i) = \\frac{1}{N^2} \\sum_{i=1}^{5} (n_{i \\cdot} \\times n_{\\cdot i})\n$$\n$$\np_e = \\frac{1}{60^2} \\left[ (40 \\times 42) + (8 \\times 7) + (6 \\times 5) + (4 \\times 5) + (2 \\times 1) \\right]\n$$\n$$\np_e = \\frac{1}{3600} (1680 + 56 + 30 + 20 + 2) = \\frac{1788}{3600}\n$$\nSimplifying the fraction: $\\frac{1788}{3600} = \\frac{447}{900} = \\frac{149}{300}$.\n\nFinally, we compute Cohen's kappa, $\\kappa$.\n$$\n\\kappa = \\frac{p_o - p_e}{1 - p_e} = \\frac{\\frac{48}{60} - \\frac{1788}{3600}}{1 - \\frac{1788}{3600}} = \\frac{\\frac{4 \\times 60}{300} - \\frac{149}{300}}{1 - \\frac{149}{300}} = \\frac{\\frac{240 - 149}{300}}{\\frac{300-149}{300}} = \\frac{\\frac{91}{300}}{\\frac{151}{300}} = \\frac{91}{151}\n$$\nTo obtain the numerical value, we perform the division:\n$$\n\\kappa = \\frac{91}{151} \\approx 0.6026490066...\n$$\nRounding to four significant figures, we get $\\kappa \\approx 0.6026$.\n\nPart 2: Explanation of Token-level vs. Span-level Agreement\n\nToken-level agreement, as computed above, treats each token as an independent unit of annotation. It measures the fraction of tokens that are assigned the same label by both raters, corrected for chance. This metric can be misleadingly high in Named Entity Recognition (NER) tasks because it does not account for the contiguity of entities. Span-level agreement is a stricter measure that evaluates agreement on entire phrases or \"spans\" of text identified as entities. For an agreement to be counted under a common span-level metric like exact match Inter-Annotator Agreement (IAA), both raters must identify the exact same sequence of tokens (identical start and end boundaries) and assign it the same entity label.\n\nBoundary alignment issues significantly impact these two types of agreement differently. Consider the example where Rater A annotates the span “$\\mathrm{Type\\ 2\\ diabetes\\ mellitus}$” as a single $\\mathrm{PROB}$ entity, while Rater B annotates only “$\\mathrm{diabetes}$” as $\\mathrm{PROB}$. At the span level (with exact match), this is a complete disagreement; two different spans have been identified, so they do not match, resulting in zero contribution to agreement. However, at the token level, the raters agree on the label $\\mathrm{PROB}$ for the token “$\\mathrm{diabetes}$”. This single token agreement would contribute positively to the token-level kappa, while the disagreements on “$\\mathrm{Type}$”, “$2$”, and “$\\mathrm{mellitus}$” would contribute negatively. Consequently, span-level IAA is typically lower than token-level IAA in NER tasks, as it is far more sensitive to disagreements in entity boundaries, which are common.", "answer": "$$\n\\boxed{0.6026}\n$$", "id": "4547588"}, {"introduction": "Modern NLP models, particularly large language models, operate on subword tokens rather than whole words, a crucial detail for handling the rich and varied vocabulary of clinical text. This requires a precise alignment between human-annotated entity spans and the model's tokenized input. This practice [@problem_id:4547499] will guide you through the essential steps of applying WordPiece tokenization and mapping entities to subword tokens using the standard BILOU labeling scheme, a fundamental skill for implementing modern NER systems.", "problem": "A clinical Natural Language Processing task requires aligning subword pieces produced by a WordPiece tokenizer with entity labels in the Begin–Inside–Last–Outside–Unit (BILOU) scheme, in order to support downstream relation extraction between a drug name, its strength, and its dosing frequency. Consider the Electronic Health Record (EHR) snippet: “metoprolol-XL 50mg qHS”. The following setup is used.\n\nFundamental base:\n1) Definition of WordPiece tokenization: A basic tokenizer first splits on whitespace and isolates punctuation into standalone tokens. Then WordPiece applies a greedy longest-match-first rule over each basic token. At the start of a basic token, any vocabulary item without the continuation marker “##” may be chosen; for a continuation within the same basic token, only vocabulary items with the “##” prefix may be chosen. This production must exactly reconstruct the original basic token string when the “##” prefixes are removed and the subwords are concatenated in order.\n2) Definition of the BILOU scheme: For any entity span, the first token in a multi-token span is labeled Begin ($\\text{B}$), any strictly interior token is labeled Inside ($\\text{I}$), and the last token is labeled Last ($\\text{L}$). A single-token span is labeled Unit ($\\text{U}$). Tokens not part of any entity are labeled Outside ($\\text{O}$).\n\nYou are given:\n- A basic tokenizer that splits on whitespace and treats the hyphen “-” as a separate token.\n- A WordPiece vocabulary limited to the set\n$$\\{\\texttt{met},\\ \\texttt{##op},\\ \\texttt{##ro},\\ \\texttt{##lol},\\ \\texttt{-},\\ \\texttt{XL},\\ \\texttt{50},\\ \\texttt{##mg},\\ \\texttt{q},\\ \\texttt{##HS}\\}.$$\n- Gold entity mentions defined over the raw text string as follows:\n  a) Drug Name (DRUG): “metoprolol-XL”\n  b) Strength (STRENGTH): “50mg”\n  c) Frequency (FREQ): “qHS”\n\nTasks:\n1) Apply the basic tokenizer and then the WordPiece tokenizer (as defined above) to the string “metoprolol-XL 50mg qHS”, ensuring correct handling of the hyphen and the unit.\n2) Align the resulting WordPiece tokens to the gold entity spans and produce a BILOU label for each WordPiece token. Treat each WordPiece sub-token as an individual token for the purpose of BILOU assignment, so that multi-subword entities receive $\\text{B}$ on their first subword, $\\text{I}$ on interior subwords, and $\\text{L}$ on their final subword; single-subword entities receive $\\text{U}$.\n3) Map each BILOU label to an integer code using the following mapping:\n   - $\\text{B-DRUG} \\mapsto 11$, $\\text{I-DRUG} \\mapsto 12$, $\\text{L-DRUG} \\mapsto 13$, $\\text{U-DRUG} \\mapsto 14$,\n   - $\\text{B-STRENGTH} \\mapsto 21$, $\\text{I-STRENGTH} \\mapsto 22$, $\\text{L-STRENGTH} \\mapsto 23$, $\\text{U-STRENGTH} \\mapsto 24$,\n   - $\\text{B-FREQ} \\mapsto 31$, $\\text{I-FREQ} \\mapsto 32$, $\\text{L-FREQ} \\mapsto 33$, $\\text{U-FREQ} \\mapsto 34$,\n   - $\\text{O} \\mapsto 0$.\n4) Let the final sequence have length $N$. Define the checksum\n$$H \\;=\\; \\sum_{i=1}^{N} i \\cdot \\mathrm{code}_i,$$\nwhere $\\mathrm{code}_i$ is the integer code for the $i$-th WordPiece token’s BILOU label in order of appearance.\n\nCompute the value of $H$. Express the final answer as an integer with no units. No rounding is required.", "solution": "The problem statement is critically validated before proceeding to a solution.\n\n### Step 1: Extract Givens\n- **Input String**: “metoprolol-XL 50mg qHS”\n- **Basic Tokenizer Rules**: Splits on whitespace and treats the hyphen “-” as a separate token. More generally, it \"isolates punctuation into standalone tokens.\"\n- **WordPiece Tokenizer Rules**: Applies a greedy longest-match-first algorithm on each basic token. Vocabulary items prefixed with \"##\" are continuation subwords. The subwords must perfectly reconstruct the original basic token.\n- **WordPiece Vocabulary**: $\\{\\texttt{met},\\ \\texttt{##op},\\ \\texttt{##ro},\\ \\texttt{##lol},\\ \\texttt{-},\\ \\texttt{XL},\\ \\texttt{50},\\ \\texttt{##mg},\\ \\texttt{q},\\ \\texttt{##HS}\\}$\n- **Gold Entity Spans**:\n  - Drug Name (DRUG): “metoprolol-XL”\n  - Strength (STRENGTH): “50mg”\n  - Frequency (FREQ): “qHS”\n- **BILOU Scheme Definition**: First token in a multi-token span is Begin ($\\text{B}$), interior tokens are Inside ($\\text{I}$), last token is Last ($\\text{L}$). A single-token span is Unit ($\\text{U}$). Non-entity tokens are Outside ($\\text{O}$).\n- **Integer Code Mapping**: \n  - $\\text{B-DRUG} \\mapsto 11$, $\\text{I-DRUG} \\mapsto 12$, $\\text{L-DRUG} \\mapsto 13$, $\\text{U-DRUG} \\mapsto 14$\n  - $\\text{B-STRENGTH} \\mapsto 21$, $\\text{I-STRENGTH} \\mapsto 22$, $\\text{L-STRENGTH} \\mapsto 23$, $\\text{U-STRENGTH} \\mapsto 24$\n  - $\\text{B-FREQ} \\mapsto 31$, $\\text{I-FREQ} \\mapsto 32$, $\\text{L-FREQ} \\mapsto 33$, $\\text{U-FREQ} \\mapsto 34$\n  - $\\text{O} \\mapsto 0$\n- **Checksum Formula**: $H = \\sum_{i=1}^{N} i \\cdot \\mathrm{code}_i$, where $N$ is the number of WordPiece tokens and $\\mathrm{code}_i$ is the integer code for the $i$-th token.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is well-grounded in the field of Natural Language Processing, a subfield of computer science and artificial intelligence. The concepts of WordPiece tokenization and BILOU labeling for named entity recognition are standard techniques in clinical NLP.\n- **Well-Posed**: All rules, definitions, and data are provided. The tokenization algorithms are deterministic, and the labeling and calculation rules are explicit, leading to a unique, meaningful solution.\n- **Objective**: The problem is stated using precise, objective language with no subjective or opinion-based components.\n- There are no scientific or factual unsoundness, incompleteness, contradictions, or other flaws.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A complete, reasoned solution will be provided.\n\nThe solution is derived by executing the four tasks specified in the problem statement.\n\n**Task 1: Tokenization**\n\nFirst, the basic tokenizer is applied to the input string “metoprolol-XL 50mg qHS”. The tokenizer splits on whitespace and isolates punctuation.\n1.  Splitting on whitespace yields the preliminary tokens: `[“metoprolol-XL”, “50mg”, “qHS”]`.\n2.  The hyphen in `“metoprolol-XL”` is isolated, as it is punctuation. This breaks the first token into `[“metoprolol”, “-”, “XL”]`.\n3.  The final list of basic tokens is: `[“metoprolol”, “-”, “XL”, “50mg”, “qHS”]`.\n\nNext, the WordPiece tokenizer is applied to each basic token using the provided vocabulary and greedy longest-match-first rule.\n- For the basic token `“metoprolol”`:\n  - `met` is the longest prefix in the vocabulary. Remainder: `oprolol`.\n  - `##op` is the longest continuation match. Remainder: `rolol`.\n  - `##ro` is the longest continuation match. Remainder: `lol`.\n  - `##lol` is the longest continuation match. Remainder is empty.\n  - Result: `[“met”, “##op”, “##ro”, “##lol”]`\n- For the basic token `“-”`:\n  - `-` is in the vocabulary.\n  - Result: `[“-”]`\n- For the basic token `“XL”`:\n  - `XL` is in the vocabulary.\n  - Result: `[“XL”]`\n- For the basic token `“50mg”`:\n  - `50` is the longest prefix. Remainder: `mg`.\n  - `##mg` is the continuation match. Remainder is empty.\n  - Result: `[“50”, “##mg”]`\n- For the basic token `“qHS”`:\n  - `q` is the longest prefix. Remainder: `HS`.\n  - `##HS` is the continuation match. Remainder is empty.\n  - Result: `[“q”, “##HS”]`\n\nCombining the results, the final sequence of $N=10$ WordPiece tokens is:\n`[“met”, “##op”, “##ro”, “##lol”, “-”, “XL”, “50”, “##mg”, “q”, “##HS”]`\n\n**Task 2: BILOU Labeling**\n\nThe WordPiece tokens are aligned with the gold entity spans to assign a BILOU label to each.\n\n- **Entity: Drug Name (DRUG) “metoprolol-XL”**\n  - This span corresponds to the first $6$ WordPiece tokens: `“met”, “##op”, “##ro”, “##lol”, “-”, “XL”`.\n  - As a multi-token span, the labels are:\n    - `“met”`: B-DRUG (Begin)\n    - `“##op”`: I-DRUG (Inside)\n    - `“##ro”`: I-DRUG (Inside)\n    - `“##lol”`: I-DRUG (Inside)\n    - `“-”`: I-DRUG (Inside)\n    - `“XL”`: L-DRUG (Last)\n- **Entity: Strength (STRENGTH) “50mg”**\n  - This span corresponds to the next $2$ WordPiece tokens: `“50”, “##mg”`.\n  - As a $2$-token span, the labels are:\n    - `“50”`: B-STRENGTH (Begin)\n    - `“##mg”`: L-STRENGTH (Last)\n- **Entity: Frequency (FREQ) “qHS”**\n  - This span corresponds to the final $2$ WordPiece tokens: `“q”, “##HS”`.\n  - As a $2$-token span, the labels are:\n    - `“q”`: B-FREQ (Begin)\n    - `“##HS”`: L-FREQ (Last)\n\nThe ordered list of (token, label) pairs is:\n1. (`“met”`, B-DRUG)\n2. (`“##op”`, I-DRUG)\n3. (`“##ro”`, I-DRUG)\n4. (`“##lol”`, I-DRUG)\n5. (`“-”`, I-DRUG)\n6. (`“XL”`, L-DRUG)\n7. (`“50”`, B-STRENGTH)\n8. (`“##mg”`, L-STRENGTH)\n9. (`“q”`, B-FREQ)\n10. (`“##HS”`, L-FREQ)\n\n**Task 3: Integer Code Mapping**\n\nEach BILOU label is mapped to its corresponding integer code, $\\mathrm{code}_i$, using the provided mapping.\n1. B-DRUG $\\mapsto 11$\n2. I-DRUG $\\mapsto 12$\n3. I-DRUG $\\mapsto 12$\n4. I-DRUG $\\mapsto 12$\n5. I-DRUG $\\mapsto 12$\n6. L-DRUG $\\mapsto 13$\n7. B-STRENGTH $\\mapsto 21$\n8. L-STRENGTH $\\mapsto 23$\n9. B-FREQ $\\mapsto 31$\n10. L-FREQ $\\mapsto 33$\n\nThe sequence of integer codes is: $[11, 12, 12, 12, 12, 13, 21, 23, 31, 33]$.\n\n**Task 4: Checksum Calculation**\n\nThe checksum $H$ is computed using the formula $H = \\sum_{i=1}^{N} i \\cdot \\mathrm{code}_i$, with $N = 10$.\n\n$H = (1 \\cdot \\mathrm{code}_1) + (2 \\cdot \\mathrm{code}_2) + (3 \\cdot \\mathrm{code}_3) + (4 \\cdot \\mathrm{code}_4) + (5 \\cdot \\mathrm{code}_5) + (6 \\cdot \\mathrm{code}_6) + (7 \\cdot \\mathrm{code}_7) + (8 \\cdot \\mathrm{code}_8) + (9 \\cdot \\mathrm{code}_9) + (10 \\cdot \\mathrm{code}_{10})$\n\nSubstituting the integer codes:\n$H = (1 \\cdot 11) + (2 \\cdot 12) + (3 \\cdot 12) + (4 \\cdot 12) + (5 \\cdot 12) + (6 \\cdot 13) + (7 \\cdot 21) + (8 \\cdot 23) + (9 \\cdot 31) + (10 \\cdot 33)$\n\nCalculating each term:\n$H = 11 + 24 + 36 + 48 + 60 + 78 + 147 + 184 + 279 + 330$\n\nSumming the terms:\n$H = (11 + 24 + 36 + 48 + 60) + 78 + 147 + 184 + 279 + 330$\n$H = 179 + 78 + 147 + 184 + 279 + 330$\n$H = 257 + 147 + 184 + 279 + 330$\n$H = 404 + 184 + 279 + 330$\n$H = 588 + 279 + 330$\n$H = 867 + 330$\n$H = 1197$\n\nThe value of the checksum $H$ is $1197$.", "answer": "$$\\boxed{1197}$$", "id": "4547499"}, {"introduction": "Evaluating a Named Entity Recognition system requires more than just a single accuracy score, as a model's performance depends on identifying both the correct entity type and its precise boundaries. Small errors in boundary detection are common and can significantly impact downstream tasks like relation extraction. This exercise [@problem_id:4547529] demonstrates how to quantify a model's performance with more nuance by calculating and comparing F1-scores under both \"strict\" (exact match) and \"relaxed\" (overlap) evaluation criteria, helping you diagnose specific error patterns in your system.", "problem": "A clinical named entity recognition (NER) system processes a tokenized clinical note. Each entity is represented as a closed interval of token indices $[s,e]$ with an associated semantic type. The gold standard set contains the following five entities:\n- $G_{1}: [3,6]$, type “Problem”.\n- $G_{2}: [10,10]$, type “Medication”.\n- $G_{3}: [11,12]$, type “Dosage”.\n- $G_{4}: [15,17]$, type “Problem”.\n- $G_{5}: [21,22]$, type “Problem”.\n\nThe system outputs the following six predicted entities (original prediction set):\n- $P_{1}: [3,6]$, type “Problem”.\n- $P_{2}: [10,11]$, type “Medication”.\n- $P_{3}: [11,12]$, type “Dosage”.\n- $P_{4}: [16,17]$, type “Problem”.\n- $P_{5}: [20,22]$, type “Problem”.\n- $P_{6}: [25,25]$, type “Medication”.\n\nEvaluation uses two matching regimes, each with one-to-one alignment (each gold entity can be matched to at most one predicted entity, and vice versa):\n\n1. Strict matching: A predicted entity counts as a true positive (TP) if and only if its $[s,e]$ boundaries exactly match a gold entity and the type is identical. Predicted entities that do not match any gold are false positives (FP). Gold entities that are not matched by any prediction are false negatives (FN).\n\n2. Relaxed matching: A predicted entity counts as a TP if and only if it overlaps a gold entity by at least one token (that is, the intersection of intervals is non-empty) and the type is identical. Exact matches are allowed. If a predicted entity overlaps multiple gold entities of the same type, it is aligned to the unmatched gold entity with which it has the largest number of overlapping tokens; ties are broken in favor of the gold entity with the smallest start index. Predicted entities without a valid overlap are FPs. Gold entities not matched are FNs.\n\nFrom the core definitions of precision and recall,\n- Precision is $P = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$.\n- Recall is $R = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$.\nThe $F1$-score is defined as the harmonic mean of precision and recall:\n$$F1 = \\frac{2PR}{P+R}.$$\n\nTask A: Using the original prediction set, compute $F1^{\\mathrm{strict}}$ and $F1^{\\mathrm{relaxed}}$.\n\nTask B: To simulate boundary errors, perturb each predicted interval by shifting both its start and end indices one token to the right, producing a perturbed set $P'_{i}: [s+1,e+1]$ with the same types. Re-evaluate under strict and relaxed matching to compute $F1'^{\\mathrm{strict}}$ and $F1'^{\\mathrm{relaxed}}$.\n\nTask C: Quantify sensitivity to boundary errors by computing the changes $\\Delta F1^{\\mathrm{strict}} = F1'^{\\mathrm{strict}} - F1^{\\mathrm{strict}}$ and $\\Delta F1^{\\mathrm{relaxed}} = F1'^{\\mathrm{relaxed}} - F1^{\\mathrm{relaxed}}$.\n\nReport your final result as a single row vector containing four components in the following order:\n$$\\left(F1^{\\mathrm{strict}},\\ F1^{\\mathrm{relaxed}},\\ \\Delta F1^{\\mathrm{strict}},\\ \\Delta F1^{\\mathrm{relaxed}}\\right).$$\nExpress all quantities exactly; no rounding is required.", "solution": "The user-provided problem is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\nThe verbatim givens from the problem statement are as follows:\n\n**Gold Standard Entities ($G$):**\n- $G_{1}: [3,6]$, type “Problem”.\n- $G_{2}: [10,10]$, type “Medication”.\n- $G_{3}: [11,12]$, type “Dosage”.\n- $G_{4}: [15,17]$, type “Problem”.\n- $G_{5}: [21,22]$, type “Problem”.\nTotal Gold Entities: $N_G = 5$.\n\n**Original Predicted Entities ($P$):**\n- $P_{1}: [3,6]$, type “Problem”.\n- $P_{2}: [10,11]$, type “Medication”.\n- $P_{3}: [11,12]$, type “Dosage”.\n- $P_{4}: [16,17]$, type “Problem”.\n- $P_{5}: [20,22]$, type “Problem”.\n- $P_{6}: [25,25]$, type “Medication”.\nTotal Predicted Entities: $N_P = 6$.\n\n**Matching Regimes:**\n- **Strict matching:** A predicted entity is a true positive (TP) iff its interval $[s,e]$ and type are identical to a gold entity. A one-to-one alignment is enforced.\n- **Relaxed matching:** A predicted entity is a TP iff its interval overlaps with a gold entity's interval (intersection is non-empty) and the types are identical. A one-to-one alignment is enforced, with tie-breaking based on maximal overlap, then smallest start index of the gold entity.\n\n**Evaluation Metrics:**\n- Precision: $P = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$\n- Recall: $R = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$\n- F1-score: $F1 = \\frac{2PR}{P+R}$\n\n**Tasks:**\n- **Task A:** Compute $F1^{\\mathrm{strict}}$ and $F1^{\\mathrm{relaxed}}$ for the original set $P$.\n- **Task B:** Create a perturbed set $P'$ by shifting each interval in $P$ by $+1$ ($[s+1, e+1]$). Compute $F1'^{\\mathrm{strict}}$ and $F1'^{\\mathrm{relaxed}}$.\n- **Task C:** Compute the changes $\\Delta F1^{\\mathrm{strict}} = F1'^{\\mathrm{strict}} - F1^{\\mathrm{strict}}$ and $\\Delta F1^{\\mathrm{relaxed}} = F1'^{\\mathrm{relaxed}} - F1^{\\mathrm{relaxed}}$.\n- **Final Result:** Report as a row vector $(F1^{\\mathrm{strict}}, F1^{\\mathrm{relaxed}}, \\Delta F1^{\\mathrm{strict}}, \\Delta F1^{\\mathrm{relaxed}})$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n1.  **Scientifically Grounded:** The problem uses standard, well-established metrics and concepts from the field of Natural Language Processing (NLP), specifically for evaluating Named Entity Recognition (NER) systems. Precision, recall, F1-score, and the concepts of strict and relaxed matching are fundamental to this domain.\n2.  **Well-Posed:** The problem provides all necessary data (gold and predicted entities) and clear, unambiguous definitions for all procedures (matching criteria, tie-breaking rules, metric formulas). The tasks are specific computational exercises that lead to a unique, deterministic solution.\n3.  **Objective:** The language is formal and technical. The definitions are precise. There are no subjective or opinion-based elements.\n4.  **Topic Relevance:** The problem is explicitly framed within the context of *clinical named entity recognition*, which is a core task in *bioinformatics and medical data analytics*.\n\nThe problem is self-contained, consistent, and adheres to the principles of scientific evaluation in its domain.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivation\n\nThe solution proceeds by executing each task in order.\n\n**Task A: Evaluation of the Original Prediction Set ($P$)**\n\n**1. Strict Matching:**\nWe compare each predicted entity $P_i$ with the gold standard set $G$ for an exact match of interval and type.\n- $P_1:[3,6]$, \"Problem\" exactly matches $G_1:[3,6]$, \"Problem\". This is a True Positive (TP).\n- $P_2:[10,11]$, \"Medication\" does not match $G_2:[10,10]$, \"Medication\" (boundaries differ).\n- $P_3:[11,12]$, \"Dosage\" exactly matches $G_3:[11,12]$, \"Dosage\". This is a TP.\n- $P_4:[16,17]$, \"Problem\" does not match $G_4:[15,17]$, \"Problem\" (boundaries differ).\n- $P_5:[20,22]$, \"Problem\" does not match $G_5:[21,22]$, \"Problem\" (boundaries differ).\n- $P_6:[25,25]$, \"Medication\" does not match any gold entity.\n\nUnder strict matching, we have $2$ TPs: $(P_1, G_1)$ and $(P_3, G_3)$.\n- Number of True Positives: $\\text{TP}^{\\mathrm{strict}} = 2$.\n- Number of False Positives: $\\text{FP}^{\\mathrm{strict}} = N_P - \\text{TP}^{\\mathrm{strict}} = 6 - 2 = 4$. (The FPs are $P_2, P_4, P_5, P_6$).\n- Number of False Negatives: $\\text{FN}^{\\mathrm{strict}} = N_G - \\text{TP}^{\\mathrm{strict}} = 5 - 2 = 3$. (The FNs are $G_2, G_4, G_5$).\n\nNow, we compute precision, recall, and F1-score:\n- Precision: $P^{\\mathrm{strict}} = \\frac{\\text{TP}^{\\mathrm{strict}}}{\\text{TP}^{\\mathrm{strict}} + \\text{FP}^{\\mathrm{strict}}} = \\frac{2}{2+4} = \\frac{2}{6} = \\frac{1}{3}$.\n- Recall: $R^{\\mathrm{strict}} = \\frac{\\text{TP}^{\\mathrm{strict}}}{\\text{TP}^{\\mathrm{strict}} + \\text{FN}^{\\mathrm{strict}}} = \\frac{2}{2+3} = \\frac{2}{5}$.\n- F1-score: $F1^{\\mathrm{strict}} = \\frac{2 \\cdot P^{\\mathrm{strict}} \\cdot R^{\\mathrm{strict}}}{P^{\\mathrm{strict}} + R^{\\mathrm{strict}}} = \\frac{2 \\cdot \\frac{1}{3} \\cdot \\frac{2}{5}}{\\frac{1}{3} + \\frac{2}{5}} = \\frac{\\frac{4}{15}}{\\frac{5+6}{15}} = \\frac{4}{11}$.\n\n**2. Relaxed Matching:**\nWe check for any overlap between intervals of the same type.\n- $P_1:[3,6]$, \"Problem\" overlaps with $G_1:[3,6]$, \"Problem\". Match.\n- $P_2:[10,11]$, \"Medication\" overlaps with $G_2:[10,10]$, \"Medication\". Match.\n- $P_3:[11,12]$, \"Dosage\" overlaps with $G_3:[11,12]$, \"Dosage\". Match.\n- $P_4:[16,17]$, \"Problem\" overlaps with $G_4:[15,17]$, \"Problem\". Match.\n- $P_5:[20,22]$, \"Problem\" overlaps with $G_5:[21,22]$, \"Problem\". Match.\n- $P_6:[25,25]$, \"Medication\" does not overlap with any gold entity of type \"Medication\" (only $G_2$).\n\nNo predicted entity overlaps with multiple gold entities of the same type, and no gold entity is overlapped by multiple predictions of the same type, so the one-to-one alignment is straightforward.\nWe find $5$ TPs: $(P_1, G_1), (P_2, G_2), (P_3, G_3), (P_4, G_4), (P_5, G_5)$.\n- $\\text{TP}^{\\mathrm{relaxed}} = 5$.\n- $\\text{FP}^{\\mathrm{relaxed}} = N_P - \\text{TP}^{\\mathrm{relaxed}} = 6 - 5 = 1$. (The FP is $P_6$).\n- $\\text{FN}^{\\mathrm{relaxed}} = N_G - \\text{TP}^{\\mathrm{relaxed}} = 5 - 5 = 0$.\n\nNow, we compute the metrics:\n- Precision: $P^{\\mathrm{relaxed}} = \\frac{5}{5+1} = \\frac{5}{6}$.\n- Recall: $R^{\\mathrm{relaxed}} = \\frac{5}{5+0} = 1$.\n- F1-score: $F1^{\\mathrm{relaxed}} = \\frac{2 \\cdot \\frac{5}{6} \\cdot 1}{\\frac{5}{6} + 1} = \\frac{\\frac{10}{6}}{\\frac{11}{6}} = \\frac{10}{11}$.\n\n**Task B: Evaluation of the Perturbed Prediction Set ($P'$)**\n\nFirst, we define the perturbed set $P'$ by shifting each interval $[s,e]$ to $[s+1, e+1]$.\n- $P'_{1}: [4,7]$, \"Problem\"\n- $P'_{2}: [11,12]$, \"Medication\"\n- $P'_{3}: [12,13]$, \"Dosage\"\n- $P'_{4}: [17,18]$, \"Problem\"\n- $P'_{5}: [21,23]$, \"Problem\"\n- $P'_{6}: [26,26]$, \"Medication\"\n\n**1. Strict Matching for $P'$:**\nWe compare each $P'_i$ to the gold set $G$.\n- $P'_1:[4,7]$ vs $G_1:[3,6]$. No match.\n- $P'_2:[11,12]$, \"Medication\" vs $G_3:[11,12]$, \"Dosage\". Type mismatch. No match.\n- No entity in $P'$ has an interval and type that are identical to any entity in $G$.\n- $\\text{TP}'^{\\mathrm{strict}} = 0$.\n- $\\text{FP}'^{\\mathrm{strict}} = N_P - \\text{TP}'^{\\mathrm{strict}} = 6 - 0 = 6$.\n- $\\text{FN}'^{\\mathrm{strict}} = N_G - \\text{TP}'^{\\mathrm{strict}} = 5 - 0 = 5$.\n\nThe F1-score is calculated:\n- $P'^{\\mathrm{strict}} = \\frac{0}{0+6} = 0$.\n- $R'^{\\mathrm{strict}} = \\frac{0}{0+5} = 0$.\n- $F1'^{\\mathrm{strict}} = 0$, as precision and recall are both $0$.\n\n**2. Relaxed Matching for $P'$:**\nWe check for overlaps with matching types.\n- $P'_1:[4,7]$, \"Problem\" overlaps with $G_1:[3,6]$, \"Problem\". Match.\n- $P'_2:[11,12]$, \"Medication\" does not overlap with $G_2:[10,10]$, \"Medication\". No match.\n- $P'_3:[12,13]$, \"Dosage\" overlaps with $G_3:[11,12]$, \"Dosage\". Match.\n- $P'_4:[17,18]$, \"Problem\" overlaps with $G_4:[15,17]$, \"Problem\". Match.\n- $P'_5:[21,23]$, \"Problem\" overlaps with $G_5:[21,22]$, \"Problem\". Match.\n- $P'_6:[26,26]$, \"Medication\" does not overlap with $G_2:[10,10]$, \"Medication\". No match.\n\nThe alignment is again straightforward.\nWe find $4$ TPs from the pairs $(P'_1, G_1), (P'_3, G_3), (P'_4, G_4), (P'_5, G_5)$.\n- $\\text{TP}'^{\\mathrm{relaxed}} = 4$.\n- $\\text{FP}'^{\\mathrm{relaxed}} = N_P - \\text{TP}'^{\\mathrm{relaxed}} = 6 - 4 = 2$. (The FPs are $P'_2$ and $P'_6$).\n- $\\text{FN}'^{\\mathrm{relaxed}} = N_G - \\text{TP}'^{\\mathrm{relaxed}} = 5 - 4 = 1$. (The FN is $G_2$).\n\nNow, we compute the metrics:\n- $P'^{\\mathrm{relaxed}} = \\frac{4}{4+2} = \\frac{4}{6} = \\frac{2}{3}$.\n- $R'^{\\mathrm{relaxed}} = \\frac{4}{4+1} = \\frac{4}{5}$.\n- $F1'^{\\mathrm{relaxed}} = \\frac{2 \\cdot \\frac{2}{3} \\cdot \\frac{4}{5}}{\\frac{2}{3} + \\frac{4}{5}} = \\frac{\\frac{16}{15}}{\\frac{10+12}{15}} = \\frac{16}{22} = \\frac{8}{11}$.\n\n**Task C: Quantify Sensitivity to Boundary Errors**\n\nWe compute the change in F1-scores.\n- $\\Delta F1^{\\mathrm{strict}} = F1'^{\\mathrm{strict}} - F1^{\\mathrm{strict}} = 0 - \\frac{4}{11} = -\\frac{4}{11}$.\n- $\\Delta F1^{\\mathrm{relaxed}} = F1'^{\\mathrm{relaxed}} - F1^{\\mathrm{relaxed}} = \\frac{8}{11} - \\frac{10}{11} = -\\frac{2}{11}$.\n\n**Final Result Compilation**\nThe four required components for the final answer are:\n- $F1^{\\mathrm{strict}} = \\frac{4}{11}$\n- $F1^{\\mathrm{relaxed}} = \\frac{10}{11}$\n- $\\Delta F1^{\\mathrm{strict}} = -\\frac{4}{11}$\n- $\\Delta F1^{\\mathrm{relaxed}} = -\\frac{2}{11}$\n\nThese are combined into a single row vector.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{4}{11} & \\frac{10}{11} & -\\frac{4}{11} & -\\frac{2}{11}\n\\end{pmatrix}\n}\n$$", "id": "4547529"}]}