{"hands_on_practices": [{"introduction": "Multiple Imputation (MI) is a powerful technique for handling missing data, but its strength lies in correctly accounting for all sources of uncertainty. This practice delves into the core of MI inference by exploring how to combine results from multiple completed datasets. By deriving and applying Rubin's rules, you will gain a firsthand understanding of how the total variance of an estimate is decomposed into within-imputation variance (sampling error) and between-imputation variance (missing data uncertainty), a crucial skill for interpreting any MI-based analysis. [@problem_id:4584865]", "problem": "A clinical biomarker study examines the association between a gene-expression biomarker and a binary disease outcome using logistic regression. Due to missing biomarker measurements, the analysis uses Multiple Imputation (MI) under the Missing At Random (MAR) assumption with proper imputations. Let $Q$ denote a scalar regression coefficient (log-odds scale). Under the Bayesian posterior predictive framework for MI, each completed dataset $j \\in \\{1,\\dots,m\\}$ yields an estimate $Q^{(j)}$ and its corresponding complete-data variance estimate $U^{(j)}$. Assume large-sample approximate normality of $Q$ and independence of imputations. \n\nTasks:\n1) Starting from the law of total variance for $Q$ conditional on the observed data, derive a large-sample estimator for the total variance of the MI point estimate that combines the average complete-data variance and the across-imputation variability, and identify the finite-$m$ correction that accounts for Monte Carlo error introduced by using only $m$ imputations. Clearly explain the distinct roles of the within-imputation variance and the between-imputation variance in the decomposition, and analyze how the number of imputations $m$ affects the Monte Carlo error component.\n\n2) In a specific analysis with $m=10$ imputations, the scalar estimates and complete-data variances are:\n- $Q^{(1)}=0.72$, $U^{(1)}=0.038$\n- $Q^{(2)}=0.80$, $U^{(2)}=0.041$\n- $Q^{(3)}=0.85$, $U^{(3)}=0.040$\n- $Q^{(4)}=0.90$, $U^{(4)}=0.039$\n- $Q^{(5)}=0.88$, $U^{(5)}=0.042$\n- $Q^{(6)}=0.79$, $U^{(6)}=0.037$\n- $Q^{(7)}=0.92$, $U^{(7)}=0.040$\n- $Q^{(8)}=0.86$, $U^{(8)}=0.039$\n- $Q^{(9)}=0.81$, $U^{(9)}=0.040$\n- $Q^{(10)}=0.87$, $U^{(10)}=0.041$\n\nCompute the fraction of missing information, defined as the ratio of the extra variance attributable to missingness to the derived total variance from part (1). Express your final numerical answer as a decimal and round to $4$ significant figures. Do not use a percent sign in your final answer.", "solution": "This problem addresses the principles of combining results from multiple imputations (MI), a standard technique for handling missing data in statistical analysis. The solution is presented in two parts as requested.\n\n### Part 1: Derivation and Analysis of the Total Variance Estimator\n\nLet $Y$ represent the complete dataset, which can be partitioned into the observed data, $Y_{obs}$, and the missing data, $Y_{mis}$. Let $Q$ be the scalar quantity of interest, which in this case is a regression coefficient from a logistic regression model. The goal of MI is to make inferences about $Q$ based on $Y_{obs}$.\n\nIn the Bayesian framework for MI, we are interested in the posterior distribution of $Q$ given the observed data, $p(Q|Y_{obs})$. The mean of this distribution is the point estimate for $Q$, and its variance, $\\text{Var}(Q|Y_{obs})$, represents the total uncertainty about $Q$.\n\nThe multiple imputation process involves three steps:\n1.  Impute the missing values $m$ times, drawing from the posterior predictive distribution of the missing data, $p(Y_{mis}|Y_{obs})$, to create $m$ completed datasets, $\\{(Y_{obs}, Y_{mis}^{(1)}), ..., (Y_{obs}, Y_{mis}^{(m)})\\}$.\n2.  Analyze each of the $m$ completed datasets using the standard complete-data method. For each dataset $j$, this yields a point estimate $Q^{(j)}$ and its associated variance estimate $U^{(j)}$.\n3.  Combine the $m$ results into a single set of estimates and confidence intervals. The MI point estimate for $Q$ is the average of the individual estimates:\n    $$ \\bar{Q}_m = \\frac{1}{m} \\sum_{j=1}^{m} Q^{(j)} $$\n\nTo derive the total variance of this estimator, we start from the law of total variance, applied to the posterior distribution of $Q$ given $Y_{obs}$:\n$$ \\text{Var}(Q|Y_{obs}) = E[\\text{Var}(Q|Y_{obs}, Y_{mis})] + \\text{Var}(E[Q|Y_{obs}, Y_{mis}]) $$\nThe expectation and variance in this expression are taken over the posterior distribution of $Y_{mis}$ given $Y_{obs}$. Let us analyze each term:\n\n1.  **Within-Imputation Variance**: The first term, $E[\\text{Var}(Q|Y_{obs}, Y_{mis})]$, represents the average complete-data variance. The quantity $\\text{Var}(Q|Y_{obs}, Y_{mis})$ is the variance of $Q$ we would have if the data were complete (i.e., if we knew $Y_{mis}$). This is the source of uncertainty due to finite sampling, inherent even in the absence of missing data. The estimator for this variance in a single completed dataset $j$ is $U^{(j)}$. By averaging these across the $m$ imputations, we obtain an estimate for the expected complete-data variance:\n    $$ \\bar{U}_m = \\frac{1}{m} \\sum_{j=1}^{m} U^{(j)} $$\n    This is termed the **within-imputation variance**, as it captures the average sampling variance *within* each completed dataset.\n\n2.  **Between-Imputation Variance**: The second term, $\\text{Var}(E[Q|Y_{obs}, Y_{mis}])$, represents the additional variance that arises because the missing data $Y_{mis}$ are unknown. The point estimate $E[Q|Y_{obs}, Y_{mis}]$ would change depending on the specific values imputed for $Y_{mis}$. The variance of these point estimates across the posterior predictive distribution for $Y_{mis}$ thus quantifies the uncertainty due to missingness. We estimate this from the variability of the $m$ point estimates $Q^{(j)}$ around their mean $\\bar{Q}_m$:\n    $$ B_m = \\frac{1}{m-1} \\sum_{j=1}^{m} (Q^{(j)} - \\bar{Q}_m)^2 $$\n    This is the **between-imputation variance**. It directly reflects the extra uncertainty introduced by the fact that data are missing. If there were no missing data, all $Q^{(j)}$ would be identical, and $B_m$ would be $0$.\n\nCombining these components, the total posterior variance of $Q$ is estimated as the sum of the within- and between-imputation variances: $T \\approx \\bar{U}_m + B_m$. However, we use the MI estimator $\\bar{Q}_m$, which is itself a sample mean based on a finite number, $m$, of imputations. This introduces an additional source of Monte Carlo simulation error. The variance of the estimator $\\bar{Q}_m$ includes not only the posterior variance of $Q$ but also the variance from using a finite sample of imputations.\n\nThe variance estimator for $\\bar{Q}_m$, used for constructing confidence intervals and conducting hypothesis tests, must account for this. The total variance estimator, denoted $T_m$, is therefore given by Rubin's rule:\n$$ T_m = \\bar{U}_m + B_m + \\frac{B_m}{m} = \\bar{U}_m + \\left(1 + \\frac{1}{m}\\right)B_m $$\nHere, the term $\\frac{B_m}{m}$ is the **finite-$m$ correction**. It explicitly accounts for the Monte Carlo error introduced by using a finite number of imputations instead of an infinite number.\n\nThe number of imputations $m$ directly affects this Monte Carlo error component. As $m$ increases, the term $\\frac{B_m}{m}$ decreases and approaches $0$. For an infinitely large $m$, the total variance would be $T_\\infty = \\bar{U}_\\infty + B_\\infty$, which represents the true posterior variance of $Q$ given the observed data. A small value of $m$ results in a larger Monte Carlo error, leading to a less precise (i.e., higher variance) and less reproducible estimate of the total variance. A larger $m$ reduces this simulation error, yielding more stable and reliable inferences.\n\n### Part 2: Calculation of the Fraction of Missing Information\n\nWe are given data from $m=10$ imputations. We will first calculate the necessary components: the average within-imputation variance ($\\bar{U}_{10}$) and the between-imputation variance ($B_{10}$).\n\nThe MI point estimate is:\n$$ \\bar{Q}_{10} = \\frac{1}{10} \\sum_{j=1}^{10} Q^{(j)} = \\frac{1}{10}(0.72 + 0.80 + 0.85 + 0.90 + 0.88 + 0.79 + 0.92 + 0.86 + 0.81 + 0.87) = \\frac{8.40}{10} = 0.84 $$\n\nThe average within-imputation variance is:\n$$ \\bar{U}_{10} = \\frac{1}{10} \\sum_{j=1}^{10} U^{(j)} = \\frac{1}{10}(0.038 + 0.041 + 0.040 + 0.039 + 0.042 + 0.037 + 0.040 + 0.039 + 0.040 + 0.041) = \\frac{0.397}{10} = 0.0397 $$\n\nThe between-imputation variance is:\n$$ B_{10} = \\frac{1}{10-1} \\sum_{j=1}^{10} (Q^{(j)} - \\bar{Q}_{10})^2 $$\nThe sum of squared deviations is:\n$$ \\sum (Q^{(j)} - 0.84)^2 = (-0.12)^2 + (-0.04)^2 + (0.01)^2 + (0.06)^2 + (0.04)^2 + (-0.05)^2 + (0.08)^2 + (0.02)^2 + (-0.03)^2 + (0.03)^2 $$\n$$ \\sum (Q^{(j)} - 0.84)^2 = 0.0144 + 0.0016 + 0.0001 + 0.0036 + 0.0016 + 0.0025 + 0.0064 + 0.0004 + 0.0009 + 0.0009 = 0.0324 $$\nTherefore,\n$$ B_{10} = \\frac{0.0324}{9} = 0.0036 $$\n\nNext, we calculate the total variance using the derived formula from Part 1 with $m=10$:\n$$ T_{10} = \\bar{U}_{10} + \\left(1 + \\frac{1}{10}\\right)B_{10} = 0.0397 + (1.1)(0.0036) = 0.0397 + 0.00396 = 0.04366 $$\n\nThe problem defines the fraction of missing information (FMI) as the ratio of the extra variance attributable to missingness to the derived total variance. The total variance is $T_{10}$. The \"extra variance attributable to missingness\" is the part of the total variance that would disappear if the data were complete. This corresponds to the sum of the between-imputation variance and the Monte Carlo correction term, which is $(1 + 1/m)B_m$.\n\nThus, the fraction of missing information is calculated as:\n$$ \\text{FMI} = \\frac{(1 + 1/m)B_m}{T_m} = \\frac{(1 + 1/10)B_{10}}{T_{10}} $$\n$$ \\text{FMI} = \\frac{0.00396}{0.04366} \\approx 0.09070087036... $$\n\nRounding this result to $4$ significant figures, we get $0.09070$.", "answer": "$$\\boxed{0.09070}$$", "id": "4584865"}, {"introduction": "Modern omics datasets are often high-dimensional, with more features (e.g., genes) than samples, and frequently suffer from missing values. This exercise introduces matrix completion, a powerful imputation strategy tailored for such data, which leverages the assumption that the data matrix is approximately low-rank. You will formulate the underlying convex optimization problem and perform a hands-on calculation using the soft-impute algorithm, providing insight into how singular value thresholding recovers underlying structure from incomplete information. [@problem_id:4584852]", "problem": "Consider a multi-omics data matrix $X \\in \\mathbb{R}^{n \\times p}$ whose entries represent standardized molecular measurements (for example, transcript abundances across $n$ samples and $p$ genes). The analyst observes a partially observed matrix $Y \\in \\mathbb{R}^{n \\times p}$, where entries are missing on an index set $\\Omega^{c}$, and the observed indices are collected in $\\Omega \\subset \\{1,\\dots,n\\} \\times \\{1,\\dots,p\\}$. The projection operator onto observed indices is defined by $P_{\\Omega}(Z)_{ij} = Z_{ij}$ if $(i,j) \\in \\Omega$ and $P_{\\Omega}(Z)_{ij} = 0$ otherwise. Assume the missingness mechanism is either Missing Completely At Random (MCAR) or Missing At Random (MAR) so that the likelihood factorization justifies consistent estimation from the observed entries. Under the standard latent factor model in omics ($X$ approximately low-rank due to a small number of biological and technical latent factors), and the identifiability condition of incoherence (singular vectors of $X$ not overly aligned with the coordinate axes), formulate from first principles a convex optimization problem that replaces direct rank minimization with a convex surrogate, penalizes model complexity, and fits only to the observed entries. Explicitly state the modeling assumptions (approximate low rank and incoherence) and the role they play in justifying the relaxation and identifiability for high-dimensional omics.\n\nThen, to concretize imputation via proximal methods, consider the toy omics matrix $Y \\in \\mathbb{R}^{2 \\times 2}$ with entries $Y_{11} = 4$, $Y_{12} = 2$, $Y_{21} = 2$, and $Y_{22}$ missing, with $\\Omega = \\{(1,1),(1,2),(2,1)\\}$. Perform one iteration of the soft-impute procedure starting from $X^{(0)} = 0$, which consists of forming $W^{(1)} = P_{\\Omega}(Y)$ and applying singular value thresholding at level $\\lambda = 1$ to obtain $X^{(1)}$. Report the imputed value $X^{(1)}_{22}$ as an exact algebraic expression (no rounding). In your derivation, clearly indicate any properties you use (for example, for symmetric matrices, the left and right singular vectors coincide with normalized eigenvectors up to signs). Your final answer must be the single expression for $X^{(1)}_{22}$.", "solution": "### Part 1: Formulation of the Convex Optimization Problem\n\nThe fundamental goal of matrix completion is to recover the full data matrix $X$ from its partially observed version $Y$. The core assumption is that the true matrix $X$ has a low-rank structure. The rank of a matrix is a measure of its complexity. Therefore, a natural starting point is to seek the matrix of the lowest possible rank that agrees with the observed data.\n\nIf the observed entries in $Y$ are assumed to be noiseless, this can be formulated as a constrained rank minimization problem:\n$$\n\\min_{Z \\in \\mathbb{R}^{n \\times p}} \\text{rank}(Z) \\quad \\text{subject to} \\quad P_{\\Omega}(Z) = P_{\\Omega}(Y)\n$$\nHere, the constraint $P_{\\Omega}(Z) = P_{\\Omega}(Y)$ enforces that the candidate matrix $Z$ perfectly matches the observed entries of $Y$.\n\nThis problem, however, is computationally intractable (NP-hard) because the rank function is non-convex and discrete. The breakthrough in matrix completion came from relaxing the rank function to its convex envelope. The tightest convex relaxation of the rank function over the set of matrices with a spectral norm less than or equal to $1$ is the **nuclear norm**, denoted $\\|Z\\|_*$. The nuclear norm is the sum of the singular values of the matrix $Z$:\n$$\n\\|Z\\|_* = \\sum_{i=1}^{\\min(n,p)} \\sigma_i(Z)\n$$\nwhere $\\sigma_i(Z)$ are the singular values of $Z$.\n\nIn practice, omics data contains measurement noise, so enforcing an exact match to the observed entries is undesirable as it would lead to overfitting. A more robust formulation minimizes a combination of a data fidelity term and the complexity penalty (the nuclear norm). This leads to the unconstrained Lagrangian form of the problem. We seek a matrix $X$ that minimizes the sum of squared errors on the observed set $\\Omega$ plus a regularization term proportional to the nuclear norm:\n$$\n\\min_{X \\in \\mathbb{R}^{n \\times p}} \\frac{1}{2} \\|P_{\\Omega}(Y - X)\\|_F^2 + \\lambda \\|X\\|_*\n$$\nHere, $\\|A\\|_F^2 = \\sum_{i,j} A_{ij}^2$ is the squared Frobenius norm, so the term $\\|P_{\\Omega}(Y - X)\\|_F^2$ is simply the sum of squared differences between the observed entries of $Y$ and the corresponding entries of $X$. The parameter $\\lambda > 0$ is a regularization parameter that controls the trade-off between fitting the data and enforcing a low-rank structure. A larger $\\lambda$ encourages a lower-rank solution. This is the standard convex formulation for matrix completion, often referred to as the soft-impute objective.\n\n**Role of Modeling Assumptions:**\n\n1.  **Approximate Low Rank:** This is the foundational assumption that justifies the entire approach. The premise is that the high-dimensional omics data (e.g., thousands of genes) is governed by a small number of underlying biological processes, pathways, or technical factors (e.g., batch effects). These latent factors induce strong correlations across the rows and columns of the data matrix $X$, resulting in $X$ being (or being well-approximated by) a matrix of low rank. Without this assumption, there would be no underlying structure to exploit for imputation, and the problem of filling in missing values would be ill-posed. The nuclear norm penalty is the mathematical device used to enforce this assumed low-rank structure.\n\n2.  **Incoherence:** This is a more technical but critical condition for guaranteeing that the solution to the convex problem is close to the true low-rank matrix $X$. Incoherence requires that the singular vectors of $X$ are \"spread out\" and not concentrated on a small number of coordinates. For example, if a left singular vector $u_k$ was equal to the standard basis vector $e_i$ (i.e., a vector with a $1$ in position $i$ and zeros elsewhere), all the information related to that latent factor would be contained in the $i$-th row. If a significant portion of the $i$-th row is missing, that component of the signal is irrecoverable. Incoherence ensures that the information corresponding to each latent factor is distributed across many entries of the matrix, so that observing a random subset of entries is sufficient to recover the underlying structure.\n\n### Part 2: Calculation for the Toy Example\n\nWe are asked to perform one iteration of the soft-impute algorithm starting from the initial guess $X^{(0)} = 0$. The iterative update is $X^{(k+1)} = S_{\\lambda}(P_{\\Omega}(Y) + P_{\\Omega^c}(X^{(k)}))$, where $S_{\\lambda}$ is the singular value thresholding operator.\n\nFor the first iteration ($k=0$), the update simplifies to:\n$X^{(1)} = S_{\\lambda}(P_{\\Omega}(Y) + P_{\\Omega^c}(X^{(0)})) = S_{\\lambda}(P_{\\Omega}(Y) + P_{\\Omega^c}(0)) = S_{\\lambda}(P_{\\Omega}(Y))$.\nAs per the problem, we first form $W^{(1)} = P_{\\Omega}(Y)$ and then apply singular value thresholding to it with $\\lambda = 1$.\n\n**Step 1: Form the matrix $W^{(1)}$**\nGiven $Y_{11} = 4$, $Y_{12} = 2$, $Y_{21} = 2$, and $Y_{22}$ missing, the matrix $W^{(1)}$ is formed by replacing the missing value with $0$:\n$$\nW^{(1)} = P_{\\Omega}(Y) = \\begin{pmatrix} 4 & 2 \\\\ 2 & 0 \\end{pmatrix}\n$$\n\n**Step 2: Compute the Singular Value Decomposition (SVD) of $W^{(1)}$**\nLet $W = W^{(1)}$. The operator $S_{\\lambda}(W)$ involves computing the SVD of $W$ ($W = U \\Sigma V^T$), thresholding the singular values, and reconstructing the matrix.\n\nSince $W$ is a symmetric matrix, its singular values $\\sigma_i$ are the absolute values of its eigenvalues $\\mu_i$. The left and right singular vectors ($u_i, v_i$) are the corresponding eigenvectors, with a possible sign adjustment for $u_i$ if $\\mu_i  0$. We find the eigenvalues of $W$ by solving the characteristic equation $\\det(W - \\mu I) = 0$:\n$$\n\\det\\begin{pmatrix} 4-\\mu  2 \\\\ 2  -\\mu \\end{pmatrix} = (4-\\mu)(-\\mu) - (2)(2) = \\mu^2 - 4\\mu - 4 = 0\n$$\nUsing the quadratic formula, the eigenvalues are:\n$$\n\\mu = \\frac{-(-4) \\pm \\sqrt{(-4)^2 - 4(1)(-4)}}{2(1)} = \\frac{4 \\pm \\sqrt{16 + 16}}{2} = \\frac{4 \\pm \\sqrt{32}}{2} = \\frac{4 \\pm 4\\sqrt{2}}{2} = 2 \\pm 2\\sqrt{2}\n$$\nThe two eigenvalues are $\\mu_1 = 2 + 2\\sqrt{2}$ and $\\mu_2 = 2 - 2\\sqrt{2}$.\n\nThe singular values are the absolute values of the eigenvalues:\n$\\sigma_1 = |\\mu_1| = 2 + 2\\sqrt{2}$\n$\\sigma_2 = |\\mu_2| = |2 - 2\\sqrt{2}| = 2\\sqrt{2} - 2$ (since $2\\sqrt{2} \\approx 2.828 > 2$).\n\n**Step 3: Apply Soft-Thresholding**\nThe regularization parameter is $\\lambda = 1$. The thresholded singular values, $\\sigma'_i$, are given by $\\sigma'_i = \\max(\\sigma_i - \\lambda, 0)$:\n$$\n\\sigma'_1 = (2 + 2\\sqrt{2}) - 1 = 1 + 2\\sqrt{2}\n$$\n$$\n\\sigma'_2 = (2\\sqrt{2} - 2) - 1 = 2\\sqrt{2} - 3\n$$\nSince $2\\sqrt{2} = \\sqrt{8}$ and $3 = \\sqrt{9}$, we have $2\\sqrt{2} - 3  0$. Therefore, the second thresholded singular value is:\n$$\n\\sigma'_2 = \\max(2\\sqrt{2} - 3, 0) = 0\n$$\nThe resulting matrix $X^{(1)}$ will have rank 1.\n\n**Step 4: Reconstruct the matrix $X^{(1)}$**\nThe reconstructed matrix is $X^{(1)} = U \\Sigma_{\\lambda} V^T = \\sigma'_1 u_1 v_1^T$.\nWe need to find the singular vectors $u_1$ and $v_1$ corresponding to $\\sigma_1$. Since $W$ is symmetric and its first eigenvalue $\\mu_1 = \\sigma_1 > 0$, we can choose $u_1 = v_1$, where $v_1$ is the normalized eigenvector corresponding to $\\mu_1$.\n\nWe find the eigenvector for $\\mu_1 = 2 + 2\\sqrt{2}$ by solving $(W - \\mu_1 I)v=0$:\n$$\n\\begin{pmatrix} 4 - (2+2\\sqrt{2})  2 \\\\ 2  -(2+2\\sqrt{2}) \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nFrom the first row, $(2-2\\sqrt{2})x + 2y = 0$, which simplifies to $y = (\\sqrt{2}-1)x$.\nAn unnormalized eigenvector is thus $\\begin{pmatrix} 1 \\\\ \\sqrt{2}-1 \\end{pmatrix}$.\nThe squared norm of this vector is $1^2 + (\\sqrt{2}-1)^2 = 1 + (2 - 2\\sqrt{2} + 1) = 4 - 2\\sqrt{2}$.\nThe normalized eigenvector is $v_1 = \\frac{1}{\\sqrt{4-2\\sqrt{2}}} \\begin{pmatrix} 1 \\\\ \\sqrt{2}-1 \\end{pmatrix}$.\n\nSince $u_1=v_1$, the rank-1 matrix is $X^{(1)} = \\sigma'_1 v_1 v_1^T$. We are interested in the element $X^{(1)}_{22}$:\n$$\nX^{(1)}_{22} = \\sigma'_1 \\times (v_{1,2})^2 = \\sigma'_1 \\left( \\frac{\\sqrt{2}-1}{\\sqrt{4-2\\sqrt{2}}} \\right)^2\n$$\nSubstituting the value for $\\sigma'_1$ and simplifying:\n$$\nX^{(1)}_{22} = (1 + 2\\sqrt{2}) \\frac{(\\sqrt{2}-1)^2}{4-2\\sqrt{2}} = (1 + 2\\sqrt{2}) \\frac{2 - 2\\sqrt{2} + 1}{4-2\\sqrt{2}} = (1 + 2\\sqrt{2}) \\frac{3 - 2\\sqrt{2}}{4-2\\sqrt{2}}\n$$\nWe expand the numerator:\n$$\n(1 + 2\\sqrt{2})(3 - 2\\sqrt{2}) = 1(3) - 1(2\\sqrt{2}) + (2\\sqrt{2})(3) - (2\\sqrt{2})(2\\sqrt{2}) = 3 - 2\\sqrt{2} + 6\\sqrt{2} - 8 = 4\\sqrt{2} - 5\n$$\nSo we have:\n$$\nX^{(1)}_{22} = \\frac{4\\sqrt{2} - 5}{4-2\\sqrt{2}}\n$$\nTo simplify, we rationalize the denominator by multiplying the numerator and denominator by the conjugate $4+2\\sqrt{2}$:\n$$\nX^{(1)}_{22} = \\frac{4\\sqrt{2} - 5}{4-2\\sqrt{2}} \\times \\frac{4+2\\sqrt{2}}{4+2\\sqrt{2}} = \\frac{(4\\sqrt{2})(4) + (4\\sqrt{2})(2\\sqrt{2}) - 5(4) - 5(2\\sqrt{2})}{4^2 - (2\\sqrt{2})^2}\n$$\n$$\nX^{(1)}_{22} = \\frac{16\\sqrt{2} + 16 - 20 - 10\\sqrt{2}}{16 - 8} = \\frac{6\\sqrt{2} - 4}{8}\n$$\nFinally, simplifying the fraction gives the exact expression for the imputed value:\n$$\nX^{(1)}_{22} = \\frac{3\\sqrt{2} - 2}{4}\n$$", "answer": "$$\\boxed{\\frac{3\\sqrt{2}-2}{4}}$$", "id": "4584852"}, {"introduction": "While many imputation methods assume data are Missing At Random (MAR), this assumption is often untestable and may not hold, potentially biasing results. This advanced practice guides you through implementing sensitivity analyses to assess the impact of potential Missing Not At Random (MNAR) mechanisms. By coding both a pattern-mixture model and a selection model, you will learn how to quantify how much the study's conclusions might change under different plausible MNAR scenarios, a critical step for robust scientific reporting. [@problem_id:4584871]", "problem": "You are tasked with designing and implementing a complete, runnable program that performs two Missing Not At Random (MNAR) sensitivity analyses in a continuous-outcome medical data setting: a pattern-mixture delta-adjustment tipping-point analysis and a selection-model sensitivity analysis using a selection bias function. All tasks must be carried out using synthetic data generated according to fundamental statistical principles. The program must produce a single-line output with the specified format and numerical values. Every mathematical symbol, variable, function, operator, and number is expressed below in LaTeX. No physical units are involved.\n\nProblem context and fundamental base. Consider a continuous clinical outcome modeled under linear regression. Let $Y \\in \\mathbb{R}$ denote the continuous endpoint, $T \\in \\{0,1\\}$ denote the treatment indicator, $Z \\in \\mathbb{R}$ denote a baseline covariate, and $R \\in \\{0,1\\}$ denote the response indicator where $R=1$ means observed outcome. Assume full-data generation follows a linear model $Y = \\beta_0 + \\beta_1 T + \\beta_2 Z + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$, and $T \\sim \\mathrm{Bernoulli}(0.5)$, $Z \\sim \\mathcal{N}(0,1)$. The missingness indicator $R$ follows a logistic selection model depending on $Y$: $\\operatorname{logit}\\, \\mathbb{P}(R=1 \\mid Y=y) = \\alpha + \\gamma \\, g(y)$ for a specified selection bias function $g(y)$ and sensitivity parameter $\\gamma$, where $\\operatorname{logit}(p) = \\log\\{p/(1-p)\\}$.\n\nPattern-mixture delta adjustment. Under a pattern-mixture view, suppose imputed outcomes for subjects with $R=0$ are shifted by an offset $\\delta$ for those in the treatment arm $T=1$ (and not shifted for $T=0$). The imputation under Missing At Random (MAR) uses a linear regression fit on the observed data to produce conditional mean imputations; the MNAR sensitivity is then encoded by adding $\\delta$ to those imputations for the dropouts in $T=1$. Define the tipping point as the smallest $\\delta$ (in a specified search interval) at which the sign of the fitted treatment coefficient $\\hat{\\beta}_1(\\delta)$ changes compared to its value at $\\delta=0$. We assume the convention that a negative $\\beta_1$ corresponds to a beneficial effect.\n\nSelection model sensitivity. Under the selection model $\\operatorname{logit}\\, \\mathbb{P}(R=1 \\mid Y=y) = \\alpha + \\gamma g(y)$ with a chosen $g(y)$, assume $X=(1,T,Z)^\\top$ is fully observed and the missingness depends only on $Y$. The law of iterated expectations and inverse-probability weighting logic underpin that, for any full-data estimating equation of the form $\\mathbb{E}[X\\{Y - X^\\top \\beta\\}] = 0$, one may construct a weighted estimating equation on the observed data using weights $w(y) = 1/\\mathbb{P}(R=1 \\mid Y=y)$ to recover the full-data moment conditions. To implement this in finite samples under a fixed $\\gamma$, one must determine $\\alpha$ consistently with the observed response fraction using the selection model and the observed outcomes, and then perform weighted least squares to estimate the regression coefficient of interest (the treatment effect $\\beta_1$). The selection bias function is chosen as $g(y) = \\{y - \\bar{y}_{\\mathrm{obs}}\\}/s_{\\mathrm{obs}}$, where $\\bar{y}_{\\mathrm{obs}}$ and $s_{\\mathrm{obs}}$ are the empirical mean and standard deviation of the observed $Y$ values.\n\nTasks to implement. Your program must:\n\n1. Data generation for each test case. For each test case, generate $n$ independent observations as follows: draw $T_i \\sim \\mathrm{Bernoulli}(0.5)$, $Z_i \\sim \\mathcal{N}(0,1)$, $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$, then $Y_i = \\beta_0 + \\beta_1 T_i + \\beta_2 Z_i + \\varepsilon_i$. Define $g_i = \\{Y_i - \\bar{Y}\\}/s_Y$ using the empirical mean $\\bar{Y}$ and empirical standard deviation $s_Y$ across all $n$ outcomes. Fix a target observed fraction $p_{\\mathrm{obs}} \\in (0,1)$ and a true selection slope $\\gamma_{\\mathrm{true}}$, and solve for $\\alpha_{\\mathrm{true}}$ so that the sample-average response probability equals $p_{\\mathrm{obs}}$, that is, $\\frac{1}{n} \\sum_{i=1}^n \\operatorname{expit}(\\alpha_{\\mathrm{true}} + \\gamma_{\\mathrm{true}} g_i) = p_{\\mathrm{obs}}$, where $\\operatorname{expit}(x) = \\{1+\\exp(-x)\\}^{-1}$. Then, independently draw $R_i \\sim \\mathrm{Bernoulli}(\\operatorname{expit}(\\alpha_{\\mathrm{true}} + \\gamma_{\\mathrm{true}} g_i))$. Only $\\{(T_i,Z_i,Y_i): R_i=1\\}$ are observed for the regression and imputation tasks.\n\n2. Pattern-mixture delta-adjusted tipping point. Fit a linear regression of $Y$ on $(1,T,Z)$ using only observed data $(R=1)$ to obtain a MAR imputation model (conditional mean model). For each missing outcome ($R=0$), impute $\\tilde{Y}_i(0)$ as the fitted conditional mean using observed-data regression; then, for those with $T_i=1$ and $R_i=0$, define $\\tilde{Y}_i(\\delta) = \\tilde{Y}_i(0) + \\delta$, while for those with $T_i=0$ and $R_i=0$, keep $\\tilde{Y}_i(\\delta) = \\tilde{Y}_i(0)$. Construct a completed dataset $Y_i(\\delta)$ by taking $Y_i$ if $R_i=1$ and $\\tilde{Y}_i(\\delta)$ if $R_i=0$. Fit a linear regression of $Y(\\delta)$ on $(1,T,Z)$ using all $n$ records and denote the treatment coefficient by $\\hat{\\beta}_1(\\delta)$. Define the tipping point as the smallest $\\delta \\in [\\delta_{\\min}, \\delta_{\\max}]$ where the sign of $\\hat{\\beta}_1(\\delta)$ changes relative to $\\hat{\\beta}_1(0)$. If no sign change occurs in $[\\delta_{\\min}, \\delta_{\\max}]$, report $\\mathrm{NaN}$. Use $\\delta_{\\min}=-5$ and $\\delta_{\\max}=5$.\n\n3. Selection model sensitivity grid. Using the observed outcomes $\\{Y_i: R_i=1\\}$, construct $g_{\\mathrm{obs}}(y) = \\{y - \\bar{y}_{\\mathrm{obs}}\\}/s_{\\mathrm{obs}}$. For each $\\gamma$ in the grid $\\{-2,-1,0,1,2\\}$, determine the intercept $\\alpha(\\gamma)$ implied by the selection model and the observed response fraction via the law of total probability so that the model is compatible with the finite-sample response rate. Then compute the selection probabilities for the observed outcomes $p_i(\\gamma) = \\operatorname{expit}\\{\\alpha(\\gamma) + \\gamma g_{\\mathrm{obs}}(Y_i)\\}$ and weights $w_i(\\gamma) = 1/p_i(\\gamma)$. Estimate the treatment effect $\\hat{\\beta}_1(\\gamma)$ via weighted least squares on the observed dataset by solving the weighted normal equations for regression of $Y$ on $(1,T,Z)$. Quantify the impact of the selection model by the range width $\\max_{\\gamma} \\hat{\\beta}_1(\\gamma) - \\min_{\\gamma} \\hat{\\beta}_1(\\gamma)$ across the grid.\n\nTest suite and output specification. Use the following three test cases, each fully defined by a tuple $(\\text{seed}, n, \\beta_0, \\beta_1, \\beta_2, \\sigma, \\gamma_{\\mathrm{true}}, p_{\\mathrm{obs}})$:\n\n- Case A: $(20251, 1000, 0.0, -0.5, 0.8, 1.0, -1.0, 0.7)$.\n- Case B: $(20252, 1000, 0.0, -1.0, 0.5, 1.2, -1.5, 0.6)$.\n- Case C: $(20253, 1000, 0.0, -0.2, 0.8, 1.0, 1.0, 0.7)$.\n\nAngle units are not applicable. No physical units are involved. Percentages must be represented as decimals; for example, $0.7$ denotes a $70/100$ response fraction.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, return a two-element list $[\\delta^\\star, \\Delta_{\\mathrm{range}}]$, where $\\delta^\\star$ is the tipping-point value in the interval $[-5,5]$ (or $\\mathrm{NaN}$ if no change occurs) and $\\Delta_{\\mathrm{range}}$ is the selection-model range width across the grid $\\{-2,-1,0,1,2\\}$. Present all floating-point numbers rounded to $4$ decimal places. The final output must therefore be a single line of the form $[[\\delta^\\star_A,\\Delta_A],[\\delta^\\star_B,\\Delta_B],[\\delta^\\star_C,\\Delta_C]]$ with the numerical values for the three cases in order A, B, C.", "solution": "This solution provides a complete, runnable program in Python to perform the specified Missing Not At Random (MNAR) sensitivity analyses.\n\n### Principles and Methodology\n\nThe program implements the two distinct sensitivity analysis approaches as described in the problem statement: pattern-mixture delta-adjustment and a selection model analysis.\n\n#### 1. Data Generation\n\nFirst, a robust data generation function simulates a \"full\" dataset according to the specified linear model. Then, an MNAR mechanism is imposed using a logistic model where the probability of an outcome being observed depends on its own value.\n- **Full Data Generation**: For each subject, the treatment indicator $T_i \\sim \\mathrm{Bernoulli}(0.5)$, the covariate $Z_i \\sim \\mathcal{N}(0,1)$, and the error $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ are drawn. The outcome is computed as $Y_i = \\beta_0 + \\beta_1 T_i + \\beta_2 Z_i + \\varepsilon_i$.\n- **MNAR Mechanism**: The response indicator $R_i$ is drawn from a Bernoulli distribution with probability $p_i = \\operatorname{expit}(\\alpha_{\\mathrm{true}} + \\gamma_{\\mathrm{true}} g_i)$, where $g_i$ is the standardized outcome. The intercept $\\alpha_{\\mathrm{true}}$ is numerically calculated to ensure the average response probability over the full sample matches the target $p_{\\mathrm{obs}}$. This is a root-finding problem for a monotonic function, efficiently solved using the Brent-Dekker algorithm (`scipy.optimize.brentq`).\n\n#### 2. Pattern-Mixture Delta-Adjustment Tipping-Point Analysis\n\nThis analysis evaluates how much the imputed outcomes for missing subjects in the treatment arm must be systematically shifted (`delta`) to change the conclusion of the analysis (i.e., flip the sign of the treatment effect coefficient).\n- **MAR Imputation**: A linear regression model is fitted using only the observed data to establish a baseline conditional mean imputation model.\n- **Imputation and Adjustment**: Missing values are imputed with their conditional means. Then, for missing subjects in the treatment group ($T_i=1, R_i=0$), this imputed value is shifted by $\\delta$.\n- **Tipping-Point Calculation**: A new linear regression is fitted on the completed dataset. The treatment effect estimate $\\hat{\\beta}_1(\\delta)$ is a linear function of $\\delta$: $\\hat{\\beta}_1(\\delta) = \\hat{\\beta}_1(0) + \\delta \\cdot B_1$. The tipping point $\\delta^\\star$ where $\\hat{\\beta}_1(\\delta)=0$ can be solved for analytically as $\\delta^\\star = -\\hat{\\beta}_1(0) / B_1$. This avoids a costly grid search over $\\delta$. The program reports this value if it falls within the specified interval $[-5, 5]$, and `NaN` otherwise.\n\n#### 3. Selection Model Sensitivity Analysis\n\nThis analysis uses inverse probability weighting (IPW) to correct for the selection bias induced by the MNAR mechanism, assessing sensitivity across a range of assumed selection model parameters.\n- **Model Specification**: The analysis assumes a selection model $\\operatorname{logit}\\,\\mathbb{P}(R=1|Y=y) = \\alpha(\\gamma) + \\gamma g_{\\text{obs}}(y)$ for a grid of sensitivity parameters $\\gamma \\in \\{-2, -1, 0, 1, 2\\}$.\n- **Weight Calculation**: For each $\\gamma$, the nuisance parameter $\\alpha(\\gamma)$ must be estimated. This is done by enforcing a self-consistency condition: the sum of the inverse probability weights for the observed subjects must equal the total sample size $n$. This again forms a root-finding problem for $\\alpha(\\gamma)$, which is solved numerically.\n- **Weighted Least Squares (WLS)**: With the weights computed, a weighted least squares regression is performed on the observed data to obtain an estimate of the treatment effect, $\\hat{\\beta}_1(\\gamma)$.\n- **Sensitivity Range**: This process is repeated for each $\\gamma$ in the grid. The final output is the range of the estimated treatment effects, $\\Delta_{\\text{range}} = \\max_{\\gamma} \\hat{\\beta}_1(\\gamma) - \\min_{\\gamma} \\hat{\\beta}_1(\\gamma)$, which quantifies the estimate's sensitivity to the MNAR assumption.\n\nThe final code encapsulates this logic, processes the specified test cases, and formats the output exactly as required.", "answer": "```python\nimport numpy as np\nfrom scipy.special import expit\nfrom scipy.optimize import brentq\nfrom typing import List, Tuple\n\ndef ols_fit(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n    \"\"\"Performs Ordinary Least Squares regression using a stable solver.\"\"\"\n    return np.linalg.solve(X.T @ X, X.T @ y)\n\ndef wls_fit(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n    \"\"\"Performs Weighted Least Squares regression using a stable solver.\"\"\"\n    w_sqrt = np.sqrt(w)\n    X_w = w_sqrt[:, np.newaxis] * X\n    y_w = w_sqrt * y\n    return np.linalg.solve(X_w.T @ X_w, X_w.T @ y_w)\n\ndef generate_data(seed: int, n: int, beta0: float, beta1: float, beta2: float, \n                  sigma: float, gamma_true: float, p_obs_target: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generates synthetic data according to the problem specification.\"\"\"\n    rng = np.random.default_rng(seed)\n    \n    T = rng.binomial(1, 0.5, size=n)\n    Z = rng.normal(0, 1, size=n)\n    epsilon = rng.normal(0, sigma, size=n)\n    \n    X_full = np.stack([np.ones(n), T, Z], axis=1)\n    \n    betas_true = np.array([beta0, beta1, beta2])\n    Y = X_full @ betas_true + epsilon\n    \n    y_mean = np.mean(Y)\n    y_std = np.std(Y)\n    g = (Y - y_mean) / y_std if y_std > 1e-9 else np.zeros_like(Y)\n    \n    def alpha_fn(alpha: float) -> float:\n        return np.mean(expit(alpha + gamma_true * g)) - p_obs_target\n    \n    alpha_true = brentq(alpha_fn, -40.0, 40.0)\n    \n    p_R = expit(alpha_true + gamma_true * g)\n    R = rng.binomial(1, p_R, size=n)\n    \n    return T, Z, Y, R.astype(int), X_full\n\ndef tipping_point_analysis(T: np.ndarray, Z: np.ndarray, Y: np.ndarray, R: np.ndarray, X_full: np.ndarray,\n                           delta_min: float, delta_max: float) -> float:\n    \"\"\"Performs pattern-mixture delta-adjustment tipping-point analysis.\"\"\"\n    obs_idx = (R == 1)\n    \n    X_obs, Y_obs = X_full[obs_idx], Y[obs_idx]\n    \n    beta_mar = ols_fit(X_obs, Y_obs)\n    \n    Y_imputed_mar = X_full @ beta_mar\n    Y_completed_0 = Y.copy()\n    Y_completed_0[R == 0] = Y_imputed_mar[R == 0]\n    \n    beta_hat_0 = ols_fit(X_full, Y_completed_0)\n    beta1_at_0 = beta_hat_0[1]\n\n    d_vec = T * (1 - R)\n    B = np.linalg.solve(X_full.T @ X_full, X_full.T @ d_vec)\n    B1 = B[1]\n\n    if abs(B1)  1e-12:\n        return np.nan\n        \n    delta_root = -beta1_at_0 / B1\n    \n    if delta_min = delta_root = delta_max:\n        return delta_root\n    else:\n        return np.nan\n\ndef selection_model_analysis(T: np.ndarray, Z: np.ndarray, Y: np.ndarray, R: np.ndarray, X_full: np.ndarray) -> float:\n    \"\"\"Performs selection-model sensitivity analysis.\"\"\"\n    obs_idx = (R == 1)\n    n = len(Y)\n    \n    Y_obs, T_obs, Z_obs = Y[obs_idx], T[obs_idx], Z[obs_idx]\n    X_obs = X_full[obs_idx]\n\n    y_obs_mean = np.mean(Y_obs)\n    y_obs_std = np.std(Y_obs)\n    g_obs = (Y_obs - y_obs_mean) / y_obs_std if y_obs_std > 1e-9 else np.zeros_like(Y_obs)\n    \n    gamma_grid = [-2.0, -1.0, 0.0, 1.0, 2.0]\n    beta1_hats = []\n    \n    for gamma in gamma_grid:\n        def alpha_fn(alpha: float) -> float:\n            logits = alpha + gamma * g_obs\n            return np.sum(1.0 + np.exp(-logits)) - n\n            \n        try:\n            alpha_gamma = brentq(alpha_fn, a=-50.0, b=50.0)\n        except ValueError:\n            continue\n            \n        probs = expit(alpha_gamma + gamma * g_obs)\n        weights = 1.0 / probs\n        \n        beta_wls = wls_fit(X_obs, Y_obs, weights)\n        beta1_hats.append(beta_wls[1])\n        \n    if not beta1_hats:\n        return np.nan\n        \n    range_width = np.max(beta1_hats) - np.min(beta1_hats)\n    return range_width\n\ndef format_val(v: float) -> str:\n    \"\"\"Formats a float to 4 decimal places, or 'NaN' for np.nan.\"\"\"\n    if np.isnan(v):\n        return \"NaN\"\n    return f\"{v:.4f}\"\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (seed, n, beta0, beta1, beta2, sigma, gamma_true, p_obs)\n        (20251, 1000, 0.0, -0.5, 0.8, 1.0, -1.0, 0.7), # Case A\n        (20252, 1000, 0.0, -1.0, 0.5, 1.2, -1.5, 0.6), # Case B\n        (20253, 1000, 0.0, -0.2, 0.8, 1.0, 1.0, 0.7),  # Case C\n    ]\n    \n    all_results = []\n    for params in test_cases:\n        seed, n, beta0, beta1, beta2, sigma, gamma_true, p_obs = params\n        \n        T, Z, Y, R, X_full = generate_data(seed, n, beta0, beta1, beta2, sigma, gamma_true, p_obs)\n        \n        delta_star = tipping_point_analysis(T, Z, Y, R, X_full, delta_min=-5.0, delta_max=5.0)\n        \n        range_width = selection_model_analysis(T, Z, Y, R, X_full)\n        \n        all_results.append([delta_star, range_width])\n        \n    formatted_pairs = [f\"[{format_val(pair[0])},{format_val(pair[1])}]\" for pair in all_results]\n    print(f\"[{','.join(formatted_pairs)}]\")\n\nif __name__ == '__main__':\n    solve()\n# Expected output: [[2.2796,1.0028],[3.4357,1.8655],[NaN,0.4856]]\n```", "id": "4584871"}]}