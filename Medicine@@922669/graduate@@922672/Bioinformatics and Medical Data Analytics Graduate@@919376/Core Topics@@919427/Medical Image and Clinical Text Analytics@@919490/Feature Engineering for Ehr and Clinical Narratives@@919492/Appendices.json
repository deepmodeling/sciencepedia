{"hands_on_practices": [{"introduction": "Clinical narratives are a rich source of information but are often recorded as unstructured free text. To unlock their value for large-scale analytics, we must first map these local, variable expressions to standardized medical terminologies. This practice ([@problem_id:4563113]) walks you through building a hybrid mapping engine that combines token-based and character-based similarity measures to create a robust normalization pipeline. By completing this exercise, you will gain hands-on experience in handling the ambiguity and variability inherent in clinical language, a critical skill for processing clinical narratives.", "problem": "You are given the task of constructing a deterministic mapping pipeline that normalizes local Electronic Health Record (EHR) codes to standardized terminology concept identifiers using a concept dictionary and a principled strategy for handling unmapped and ambiguous codes. Your program must implement the pipeline and produce quantitative outcomes for a specified test suite, expressed in the exact output format described at the end of this problem.\n\nFundamental base and core definitions:\n- A concept dictionary is a finite set of pairs $\\{(c, L_c)\\}$ where $c$ is a standardized concept identifier (an integer) and $L_c$ is a finite set of canonical surface forms (synonymous labels) associated with concept $c$.\n- A prior $p(c)$ is a nonnegative real-valued prior weight for concept $c$, reflecting corpus-derived frequency or prevalence. The priors need not sum to $1$, and serve only for tie-breaking preferences.\n- A local EHR code is a free-text string. The pipeline maps each local code to a single concept identifier or to a designated unknown sentinel $0$ if no suitable match exists.\n\nString normalization and tokenization:\n- Let $g(s)$ be a deterministic normalization and tokenization function on strings $s$ applying the following transformations in order:\n  $1.$ Lowercase all characters.\n  $2.$ Expand abbreviations by substituting known tokens with their expanded phrases; e.g., \"hbp\" $\\to$ \"high blood pressure\", \"ckd\" $\\to$ \"chronic kidney disease\", \"akd\" $\\to$ \"acute kidney disease\", \"dz\" $\\to$ \"disease\", \"t2dm\" $\\to$ \"type 2 diabetes mellitus\", \"t1dm\" $\\to$ \"type 1 diabetes mellitus\".\n  $3.$ Convert Roman numerals up to $10$ ($\\mathrm{I},\\mathrm{II},\\mathrm{III},\\mathrm{IV},\\mathrm{V},\\mathrm{VI},\\mathrm{VII},\\mathrm{VIII},\\mathrm{IX},\\mathrm{X}$) to Arabic equivalents ($1,2,3,4,5,6,7,8,9,10$).\n  $4.$ Replace all non-alphanumeric characters with spaces and split on whitespace into tokens.\n  $5.$ Remove known stopwords such as \"nos\" and \"unspecified\".\n  The output $g(s)$ is the set of tokens (treated as a set for similarity computations) resulting from the above steps.\n\nWeighted token similarity:\n- For a concept dictionary $\\{(c,L_c)\\}$, define the document frequency $\\mathrm{df}(t)$ for token $t$ as the number of distinct concept labels in $\\bigcup_{c} L_c$ whose normalized token set (via $g$) contains $t$. Define a weight\n$$\nw(t) = \\frac{1}{1+\\mathrm{df}(t)}.\n$$\n- For two normalized token sets $X$ and $Y$, define the weighted Jaccard similarity\n$$\nJ(X,Y) = \\frac{\\sum_{t \\in X \\cap Y} w(t)}{\\sum_{t \\in X \\cup Y} w(t)},\n$$\nwith the convention $J(\\varnothing,Y)=0$ when the denominator is zero.\n\nCharacter-level similarity:\n- Let $d(a,b)$ be the Levenshtein edit distance between strings $a$ and $b$, and let $|a|$ denote the string length. Define the normalized character-level similarity\n$$\nr(a,b) = \n\\begin{cases}\n0,  \\text{if } \\max(|a|,|b|)=0,\\\\\n1 - \\frac{d(a,b)}{\\max(|a|,|b|)},  \\text{otherwise}.\n\\end{cases}\n$$\n- For character similarity, use the normalized forms of the strings after applying abbreviation expansion and Roman numeral normalization, and removing whitespace.\n\nCombined similarity and concept scoring:\n- For a local code $s$ and a concept $c$ with labels $L_c$, define the concept score\n$$\nS_c(s) = \\max_{\\ell \\in L_c} \\left[ \\alpha \\cdot J\\big(g(s), g(\\ell)\\big) + (1-\\alpha) \\cdot r\\big(\\tilde{s}, \\tilde{\\ell}\\big) \\right],\n$$\nwhere $\\alpha \\in [0,1]$, and $\\tilde{s}$ and $\\tilde{\\ell}$ are the character-normalized forms of $s$ and $\\ell$ (abbreviation-expanded, Roman-normalized, and whitespace-stripped).\n\nDecision rule, ambiguity, and unmapped strategy:\n- Given a threshold $\\tau \\in [0,1]$ and a tie tolerance $\\epsilon  0$, define the candidate set\n$$\n\\mathcal{C}(s) = \\{ c \\mid S_c(s) \\ge \\tau \\}.\n$$\n- If $\\mathcal{C}(s)=\\varnothing$, assign the unknown sentinel $0$ to $s$ (this is an unmapped code).\n- Otherwise, let $S_{\\max}(s) = \\max_{c \\in \\mathcal{C}(s)} S_c(s)$ and define the set of top-scoring candidates\n$$\n\\mathcal{T}(s) = \\{ c \\in \\mathcal{C}(s) \\mid S_{\\max}(s) - S_c(s) \\le \\epsilon \\}.\n$$\n- If $|\\mathcal{T}(s)|=1$, assign that unique concept to $s$.\n- If $|\\mathcal{T}(s)|1$ (ambiguous), select the concept in $\\mathcal{T}(s)$ with the largest prior $p(c)$. If multiple candidates remain tied on $p(c)$, select the candidate with the smallest identifier $c$ (deterministic tie-break). Count each such instance as \"ambiguous resolved.\"\n\nQuantitative outputs:\n- For a batch of local codes, define $M$ as the integer count of codes mapped to a nonzero concept identifier (i.e., not the unknown sentinel), $U$ as the integer count of codes mapped to the unknown sentinel $0$, and $A$ as the integer count of codes that were ambiguous (i.e., $|\\mathcal{T}(s)|1$) but resolved via the tie-breaking policy. For each test case, the program must output the list $[M,U,A]$.\n\nTest suite specification:\nImplement your program for the following four test cases. Each test case specifies a concept dictionary, priors, local codes, and parameters $\\alpha$ and $\\tau$.\n\n- Test case $1$:\n  - Concept identifiers and labels:\n    $1001$: {\"Type 2 diabetes mellitus\", \"T2DM\"},\n    $1002$: {\"Type 1 diabetes mellitus\", \"T1DM\"},\n    $2001$: {\"Hypertension\", \"High blood pressure\"}.\n  - Priors $p(c)$:\n    $p(1001)=0.6$, $p(1002)=0.2$, $p(2001)=0.4$.\n  - Local codes:\n    \"type II diabetes\", \"HBP\", \"diabetes type 1\", \"unknown diagnosis\".\n  - Parameters:\n    $\\alpha=0.7$, $\\tau=0.5$, unknown sentinel $0$, tie tolerance $\\epsilon=10^{-12}$.\n\n- Test case $2$:\n  - Concept identifiers and labels:\n    $3001$: {\"Chronic kidney disease\", \"CKD\", \"Chronic renal disease\"},\n    $3002$: {\"Acute kidney disease\", \"AKD\", \"Acute renal disease\"},\n    $4001$: {\"Chronic renal insufficiency\"}.\n  - Priors $p(c)$:\n    $p(3001)=0.55$, $p(3002)=0.25$, $p(4001)=0.30$.\n  - Local codes:\n    \"renal disease\", \"ckd stage\", \"acute kidney dz\".\n  - Parameters:\n    $\\alpha=0.7$, $\\tau=0.55$, unknown sentinel $0$, tie tolerance $\\epsilon=10^{-12}$.\n\n- Test case $3$:\n  - Concept identifiers and labels:\n    $5001$: {\"Fever\", \"Pyrexia\"},\n    $5002$: {\"Hypothermia\", \"Cold\"},\n    $5003$: {\"Cold\", \"Common cold\"}.\n  - Priors $p(c)$:\n    $p(5001)=0.5$, $p(5002)=0.5$, $p(5003)=0.5$.\n  - Local codes:\n    \"fever\", \"\", \"pyrexia\", \"cold\".\n  - Parameters:\n    $\\alpha=0.7$, $\\tau=0.6$, unknown sentinel $0$, tie tolerance $\\epsilon=10^{-12}$.\n\n- Test case $4$:\n  - Concept identifiers and labels:\n    $6001$: {\"Myocardial infarction\", \"Heart attack\"},\n    $6002$: {\"Angina\", \"Anginal pain\"}.\n  - Priors $p(c)$:\n    $p(6001)=0.4$, $p(6002)=0.6$.\n  - Local codes:\n    \"heart attak\", \"anginal pain\".\n  - Parameters:\n    $\\alpha=0.5$, $\\tau=0.5$, unknown sentinel $0$, tie tolerance $\\epsilon=10^{-12}$.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each result is a list $[M,U,A]$ in the order of the test cases. For example, the output must look like \"[[x1,y1,z1],[x2,y2,z2],[x3,y3,z3],[x4,y4,z4]]\" with no spaces.", "solution": "The problem requires the implementation of a deterministic pipeline for mapping local Electronic Health Record (EHR) text codes to standardized concept identifiers. The solution is constructed by systematically implementing each component of the specified pipeline, from string normalization to the final decision logic, and then applying this pipeline to a suite of test cases to produce quantitative evaluation metrics.\n\nThe overall approach is to first establish a set of pre-computation steps to prepare the necessary data structures, such as token weights, and then to process each local code through the scoring and decision pipeline.\n\n### Step 1: String Normalization and Pre-computation\n\nThe foundation of the mapping pipeline is a deterministic string normalization function, $g(s)$, and a character-normalized form, $\\tilde{s}$. These are implemented as follows:\n\n1.  **Common Pre-processing**: A shared normalization sequence is applied first for both $g(s)$ and $\\tilde{s}$. A given string $s$ is processed by:\n    a. Converting to lowercase.\n    b. Expanding known abbreviations (e.g., \"$t2dm$\" $\\to$ \"type 2 diabetes mellitus\"). This is implemented using word-boundary-aware regular expression substitution to prevent incorrect replacements within words.\n    c. Converting Roman numerals up to $10$ (e.g., \"$\\mathrm{ix}$\" $\\to$ \"$9$\"). To ensure correctness (e.g., \"$\\mathrm{ix}$\" is processed before \"$\\mathrm{i}$\"), the list of Roman numerals is sorted by length in descending order before substitution.\n\n2.  **Token Set Generation $g(s)$**: After the common pre-processing, the resulting string is tokenized by replacing all non-alphanumeric characters with spaces and splitting on whitespace. Known stopwords (\"$nos$\", \"$unspecified$\") are then removed. The final output is a set of unique tokens.\n\n3.  **Character-Normalized String $\\tilde{s}$**: After the common pre-processing, all non-alphanumeric characters are removed from the string to produce $\\tilde{s}$.\n\nBefore processing local codes, several components are pre-computed for efficiency:\n-   **Token Weights $w(t)$**: The document frequency, $\\mathrm{df}(t)$, is calculated for each token $t$. This is the count of distinct labels in the concept dictionary $\\bigcup_{c} L_c$ whose normalized form $g(\\ell)$ contains $t$. The token weight is then $w(t) = \\frac{1}{1+\\mathrm{df}(t)}$. These weights are pre-computed for all tokens present in the dictionary's labels.\n-   **Processed Concept Dictionary**: For each concept $c$, its set of labels $L_c$ is pre-processed. For each label $\\ell \\in L_c$, we compute and store its token set representation $g(\\ell)$ and its character-normalized form $\\tilde{\\ell}$. This avoids redundant computations during the scoring phase.\n\n### Step 2: Similarity and Scoring\n\nWith the pre-computed data, we can efficiently calculate the score $S_c(s)$ for a given local code $s$ and concept $c$.\n\n1.  **Levenshtein Distance $d(a,b)$**: A standard dynamic programming algorithm is implemented to compute the Levenshtein edit distance between two strings. This function is a prerequisite for the character-level similarity.\n\n2.  **Weighted Jaccard Similarity $J(X,Y)$**: For a local code's token set $X=g(s)$ and a label's token set $Y=g(\\ell)$, the similarity is computed as:\n    $$\n    J(X,Y) = \\frac{\\sum_{t \\in X \\cap Y} w(t)}{\\sum_{t \\in X \\cup Y} w(t)}\n    $$\n    The numerator is the sum of weights of tokens common to both sets, and the denominator is the sum of weights of all unique tokens across both sets. If the union is empty (i.e., both $X$ and $Y$ are empty), $J(X, Y)$ is $0$.\n\n3.  **Character-Level Similarity $r(a,b)$**: For a local code's character-normalized form $a=\\tilde{s}$ and a label's form $b=\\tilde{\\ell}$, the similarity is:\n    $$\n    r(a,b) = 1 - \\frac{d(a,b)}{\\max(|a|,|b|)}\n    $$\n    This is calculated only if $\\max(|a|,|b|)  0$; otherwise, it is $0$.\n\n4.  **Concept Score $S_c(s)$**: The final score for a concept $c$ is the maximum combined similarity score over all of its labels $\\ell \\in L_c$. The combined score for a single label is a weighted average of the two similarity metrics:\n    $$\n    S_c(s) = \\max_{\\ell \\in L_c} \\left[ \\alpha \\cdot J\\big(g(s), g(\\ell)\\big) + (1-\\alpha) \\cdot r\\big(\\tilde{s}, \\tilde{\\ell}\\big) \\right]\n    $$\n\n### Step 3: Decision Logic and Quantitative Metrics\n\nFor each local code $s$, after computing $S_c(s)$ for all concepts $c$ in the dictionary, the final mapping is determined by a multi-step decision rule. The counters for mapped ($M$), unmapped ($U$), and ambiguous ($A$) codes are updated accordingly.\n\n1.  **Candidate Selection**: The set of candidate concepts is formed as $\\mathcal{C}(s) = \\{ c \\mid S_c(s) \\ge \\tau \\}$, where $\\tau$ is a given similarity threshold.\n\n2.  **Unmapped Codes**: If $\\mathcal{C}(s)$ is empty, the code is considered unmapped. It is assigned the unknown sentinel $0$, and the counter $U$ is incremented.\n\n3.  **Top-Scoring Candidates**: If $\\mathcal{C}(s)$ is not empty, the code will be mapped. The counter $M$ is incremented. We find the maximum score $S_{\\max}(s) = \\max_{c \\in \\mathcal{C}(s)} S_c(s)$. The set of top-scoring candidates is then $\\mathcal{T}(s) = \\{ c \\in \\mathcal{C}(s) \\mid S_{\\max}(s) - S_c(s) \\le \\epsilon \\}$, where $\\epsilon$ is a small tolerance.\n\n4.  **Ambiguity Resolution**:\n    -   If $|\\mathcal{T}(s)|=1$, the mapping is unambiguous to the single concept in $\\mathcal{T}(s)$.\n    -   If $|\\mathcal{T}(s)|1$, the mapping is ambiguous. The counter $A$ is incremented. The ambiguity is resolved by selecting the concept from $\\mathcal{T}(s)$ that has the highest prior weight $p(c)$. If a tie in priors exists, the concept with the smallest numerical identifier $c$ is chosen. This two-level tie-breaking ensures a single, deterministic mapping.\n\nBy processing all local codes in each test case through this pipeline, the final counts $[M, U, A]$ are generated.", "answer": "```python\nimport re\nimport numpy as np\nfrom collections import defaultdict\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        {\n            \"concepts\": {\n                1001: {\"Type 2 diabetes mellitus\", \"T2DM\"},\n                1002: {\"Type 1 diabetes mellitus\", \"T1DM\"},\n                2001: {\"Hypertension\", \"High blood pressure\"},\n            },\n            \"priors\": {1001: 0.6, 1002: 0.2, 2001: 0.4},\n            \"codes\": [\"type II diabetes\", \"HBP\", \"diabetes type 1\", \"unknown diagnosis\"],\n            \"alpha\": 0.7, \"tau\": 0.5, \"epsilon\": 1e-12\n        },\n        {\n            \"concepts\": {\n                3001: {\"Chronic kidney disease\", \"CKD\", \"Chronic renal disease\"},\n                3002: {\"Acute kidney disease\", \"AKD\", \"Acute renal disease\"},\n                4001: {\"Chronic renal insufficiency\"},\n            },\n            \"priors\": {3001: 0.55, 3002: 0.25, 4001: 0.30},\n            \"codes\": [\"renal disease\", \"ckd stage\", \"acute kidney dz\"],\n            \"alpha\": 0.7, \"tau\": 0.55, \"epsilon\": 1e-12\n        },\n        {\n            \"concepts\": {\n                5001: {\"Fever\", \"Pyrexia\"},\n                5002: {\"Hypothermia\", \"Cold\"},\n                5003: {\"Cold\", \"Common cold\"},\n            },\n            \"priors\": {5001: 0.5, 5002: 0.5, 5003: 0.5},\n            \"codes\": [\"fever\", \"\", \"pyrexia\", \"cold\"],\n            \"alpha\": 0.7, \"tau\": 0.6, \"epsilon\": 1e-12\n        },\n        {\n            \"concepts\": {\n                6001: {\"Myocardial infarction\", \"Heart attack\"},\n                6002: {\"Angina\", \"Anginal pain\"},\n            },\n            \"priors\": {6001: 0.4, 6002: 0.6},\n            \"codes\": [\"heart attak\", \"anginal pain\"],\n            \"alpha\": 0.5, \"tau\": 0.5, \"epsilon\": 1e-12\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _solve_case(\n            case[\"concepts\"], case[\"priors\"], case[\"codes\"], \n            case[\"alpha\"], case[\"tau\"], case[\"epsilon\"]\n        )\n        results.append(result)\n\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef _solve_case(concept_dict, priors, local_codes, alpha, tau, epsilon):\n    \"\"\"\n    Solves a single test case based on the problem specification.\n    \"\"\"\n    \n    # Memoization caches for normalization functions\n    memo_g = {}\n    memo_char_norm = {}\n\n    ABBREVIATIONS = {\n        \"hbp\": \"high blood pressure\", \"ckd\": \"chronic kidney disease\",\n        \"akd\": \"acute kidney disease\", \"dz\": \"disease\",\n        \"t2dm\": \"type 2 diabetes mellitus\", \"t1dm\": \"type 1 diabetes mellitus\",\n    }\n    ROMAN_NUMERALS = {\n        \"i\": \"1\", \"ii\": \"2\", \"iii\": \"3\", \"iv\": \"4\", \"v\": \"5\",\n        \"vi\": \"6\", \"vii\": \"7\", \"viii\": \"8\", \"ix\": \"9\", \"x\": \"10\",\n    }\n    ROMAN_KEYS_SORTED = sorted(ROMAN_NUMERALS.keys(), key=len, reverse=True)\n    STOPWORDS = {\"nos\", \"unspecified\"}\n\n    def _normalize_common(s):\n        norm_s = s.lower()\n        for abbr, expansion in ABBREVIATIONS.items():\n            norm_s = re.sub(r'\\b' + re.escape(abbr) + r'\\b', expansion, norm_s)\n        for r_key in ROMAN_KEYS_SORTED:\n            norm_s = re.sub(r'\\b' + r_key + r'\\b', ROMAN_NUMERALS[r_key], norm_s)\n        return norm_s\n    \n    def g(s):\n        if s in memo_g: return memo_g[s]\n        norm_s = _normalize_common(s)\n        tokens = re.sub(r'[^a-zA-Z0-9]', ' ', norm_s).split()\n        final_tokens = {token for token in tokens if token not in STOPWORDS}\n        memo_g[s] = final_tokens\n        return final_tokens\n\n    def char_normalize(s):\n        if s in memo_char_norm: return memo_char_norm[s]\n        norm_s = _normalize_common(s)\n        result = re.sub(r'[^a-zA-Z0-9]', '', norm_s)\n        memo_char_norm[s] = result\n        return result\n\n    def levenshtein(s1, s2):\n        m, n = len(s1), len(s2)\n        dp = np.zeros((m + 1, n + 1), dtype=int)\n        for i in range(m + 1):\n            dp[i, 0] = i\n        for j in range(n + 1):\n            dp[0, j] = j\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                cost = 0 if s1[i - 1] == s2[j - 1] else 1\n                dp[i, j] = min(dp[i - 1, j] + 1,\n                               dp[i, j - 1] + 1,\n                               dp[i - 1, j - 1] + cost)\n        return dp[m, n]\n\n    # Pre-computation steps\n    all_labels = {label for labels in concept_dict.values() for label in labels}\n    \n    df = defaultdict(int)\n    for label in all_labels:\n        for token in g(label):\n            df[token] += 1\n            \n    all_dict_tokens = df.keys()\n    w = {t: 1.0 / (1.0 + df.get(t, 0)) for t in all_dict_tokens}\n\n    processed_concepts = {}\n    for cid, labels in concept_dict.items():\n        processed_labels = [\n            {\"g\": g(label), \"char_norm\": char_normalize(label)}\n            for label in labels\n        ]\n        processed_concepts[cid] = processed_labels\n\n    M, U, A = 0, 0, 0\n    for s in local_codes:\n        if not s.strip():\n            U += 1\n            continue\n        \n        s_g = g(s)\n        s_char_norm = char_normalize(s)\n        \n        scores = {}\n        for cid, p_labels in processed_concepts.items():\n            max_label_score = 0.0\n            for label_data in p_labels:\n                l_g = label_data[\"g\"]\n                l_char_norm = label_data[\"char_norm\"]\n                \n                intersect = s_g.intersection(l_g)\n                union = s_g.union(l_g)\n                sum_w_intersect = sum(w.get(t, 1.0) for t in intersect)\n                sum_w_union = sum(w.get(t, 1.0) for t in union)\n                j_sim = sum_w_intersect / sum_w_union if sum_w_union > 0 else 0.0\n                \n                lev_dist = levenshtein(s_char_norm, l_char_norm)\n                max_len = max(len(s_char_norm), len(l_char_norm))\n                r_sim = (1.0 - lev_dist / max_len) if max_len > 0 else 0.0\n\n                combined_score = alpha * j_sim + (1 - alpha) * r_sim\n                if combined_score > max_label_score:\n                    max_label_score = combined_score\n            scores[cid] = max_label_score\n\n        candidate_set = {cid for cid, score in scores.items() if score >= tau}\n        \n        if not candidate_set:\n            U += 1\n            continue\n        \n        M += 1\n        max_score = max(scores[cid] for cid in candidate_set)\n        top_candidates = {cid for cid in candidate_set if max_score - scores[cid] = epsilon}\n\n        if len(top_candidates) > 1:\n            A += 1\n            # Ambiguity resolution not needed for counting, but included for completeness:\n            # resolved_candidate = sorted(list(top_candidates), key=lambda cid: (-priors[cid], cid))[0]\n    \n    return [M, U, A]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "4563113"}, {"introduction": "Beyond text, electronic health records contain vast streams of time-stamped data, such as lab results, vital signs, and medication administrations, which are almost always sampled irregularly. Before building temporal models, this data must be aligned onto a common time grid, a process known as resampling. This exercise ([@problem_id:4563173]) demonstrates that a one-size-fits-all approach is flawed; instead, you must apply semantics-preserving rules for different data types, such as interpolation for continuous measurements and rate aggregation for discrete dosages. This practice will equip you with the fundamental techniques to transform messy, asynchronous event streams into the structured feature tensors required for sophisticated time-series modeling.", "problem": "You are given a set of irregularly sampled clinical event streams and asked to construct time-aligned feature tensors under event-type-specific semantics that preserve the meaning of the underlying clinical processes. The streams come from Electronic Health Records (EHR; Electronic Health Records) and include instantaneous laboratory measurements, discrete medication doses, binary event flags, and categorical modality states. Your task is to formalize resampling rules from first principles, implement them, and verify invariants that reflect semantic preservation.\n\nFundamental base. Consider clinical time series as realizations of underlying continuous-time signals with event semantics. Let $t \\in \\mathbb{R}$ denote time in hours, and let $G = \\{ g_j \\}_{j=0}^{J-1}$ be a uniform grid of times with step $\\Delta  0$ hours. For each feature $k$, let an event stream be $S_k = \\{ (t_i^{(k)}, v_i^{(k)}) \\}_{i=1}^{N_k}$, where $t_i^{(k)}$ is the event time in hours and $v_i^{(k)}$ is the event value. We assume the following broad semantics, aligned with well-tested facts in measurement theory and time series signal processing:\n\n- Intensive quantities (for example, laboratory measurements such as serum sodium) are instantaneous values of a state variable $x_k(t)$, where $x_k(t)$ has units independent of the observation duration. Interpolation for intensives should reflect continuity only within clinically valid horizons.\n- Extensive quantities (for example, discrete doses) are amounts aggregated over time, represented as impulse measures. Conservation of total amount under resampling must hold.\n- Binary event flags represent the occurrence of an event within a detection window and should not be interpolated as continuous values.\n- Categorical states (for example, ventilation mode) are step processes; they carry forward until changed and should not be interpolated across categories.\n\nCore definitions for resampling. Define semantics-specific resampling operators:\n\n- Intensive interpolation with horizon: Given an intensive stream $S_k$ and a validity horizon $\\tau_k  0$, define the aligned value at $g_j$ as\n$$\nx_k(g_j) = \\begin{cases}\nv_{\\text{prev}} + \\dfrac{v_{\\text{next}} - v_{\\text{prev}}}{t_{\\text{next}} - t_{\\text{prev}}} \\cdot (g_j - t_{\\text{prev}}),  \\text{if } \\exists\\, t_{\\text{prev}} \\le g_j \\le t_{\\text{next}},\\ g_j - t_{\\text{prev}} \\le \\tau_k,\\ t_{\\text{next}} - g_j \\le \\tau_k, \\\\\nv_i,  \\text{if } \\exists\\, i \\text{ s.t. } t_i = g_j \\text{ exactly}, \\\\\n\\text{missing},  \\text{otherwise}.\n\\end{cases}\n$$\nThis is linear interpolation constrained by a clinical validity horizon; it disallows carry-forward without a future anchor and disallows extrapolation beyond the horizon.\n\n- Extensive rate aggregation with conservation: Represent discrete amounts as impulses. Over window $[g_j, g_{j+1})$, define the rate\n$$\nr_k(g_j) = \\frac{1}{\\Delta} \\sum_{i: t_i^{(k)} \\in [g_j, g_{j+1})} v_i^{(k)}.\n$$\nConservation requires that\n$$\n\\sum_{j=0}^{J-2} r_k(g_j) \\cdot \\Delta = \\sum_{i=1}^{N_k} v_i^{(k)}.\n$$\nWe set $r_k(g_{J-1}) = 0$ for the terminal grid point with no subsequent window.\n\n- Binary detection window: For a binary event stream with window $\\omega  0$, define\n$$\nb_k(g_j) = \\mathbf{1}\\left( \\exists\\, i \\text{ with } t_i^{(k)} \\in (g_j - \\omega, g_j] \\right),\n$$\nthat is, $b_k(g_j)$ equals $1$ if any event occurred strictly after $g_j - \\omega$ up to and including $g_j$, else $0$.\n\n- Categorical step carry-forward: For categorical states encoded as integers, define\n$$\nc_k(g_j) = \\begin{cases}\n\\text{last known category at time } \\le g_j,  \\text{if any exists}, \\\\\n-1,  \\text{if unknown before first observation}.\n\\end{cases}\n$$\n\nConstructing the feature tensor. For a fixed grid $G$ and four features (intensive laboratory, extensive dose, binary event, categorical state), define the aligned tensor $X \\in \\mathbb{R}^{J \\times 4}$ whose columns are, respectively, the interpolated intensive values, the extensive rates, the binary flags, and the categorical codes.\n\nYour implementation must:\n\n- Sort irregular event times before resampling.\n- Apply the intensive interpolation rule with the specified horizon $\\tau$.\n- Apply extensive rate aggregation over half-open windows $[g_j, g_{j+1})$ with conservation.\n- Apply binary windowing over $(g_j - \\omega, g_j]$.\n- Apply categorical carry-forward with $-1$ representing unknown before first observation.\n\nUnits. Time is in hours. Extensive rates are in amounts per hour. All comparisons should be exact under these units.\n\nTest suite. Use the following three test cases to construct tensors and verify invariants. For all cases, use grid $G = [0, 4, 8, 12, 16, 20]$ hours (that is, $J = 6$ and $\\Delta = 4$ hours), binary window $\\omega = 4$ hours, and categorical encoding $\\{\\text{\"VC\"} \\mapsto 0, \\text{\"PS\"} \\mapsto 1\\}$.\n\n- Test case $1$ (general case):\n    - Intensive (serum sodium): $S_{\\text{Na}} = \\{(1, 140), (9, 138), (15, 142)\\}$, horizon $\\tau = 8$.\n    - Extensive (insulin doses): $S_{\\text{dose}} = \\{(2, 5), (6, 3), (11, 4), (19, 2)\\}$.\n    - Binary (fever spikes): $S_{\\text{fever}} = \\{(3, 1), (10, 1), (17, 1)\\}$.\n    - Categorical (ventilation mode): $S_{\\text{vent}} = \\{(5, \\text{\"VC\"}), (14, \\text{\"PS\"})\\}$.\n    - Expected checks:\n        - Conservation: $\\left| \\sum_{j=0}^{4} r_{\\text{dose}}(g_j)\\cdot 4 - (5 + 3 + 4 + 2) \\right| = 0$.\n        - Intensive at $g = 12$: expected $140.0$ by linear interpolation between $(9, 138)$ and $(15, 142)$ within horizon.\n        - Binary at $g = 12$: expected $1$.\n        - Categorical at $g = 16$: expected $1$.\n\n- Test case $2$ (horizon-boundary and window-boundary behavior):\n    - Intensive (serum sodium): $S_{\\text{Na}} = \\{(0, 135), (12, 145)\\}$, horizon $\\tau = 4$.\n    - Extensive (doses): $S_{\\text{dose}} = \\{(4, 1), (8, 1), (12, 1), (16, 1)\\}$.\n    - Binary (events): $S_{\\text{fever}} = \\{(4, 1), (7, 1)\\}$.\n    - Categorical: $S_{\\text{vent}} = \\{(0, \\text{\"PS\"})\\}$.\n    - Expected checks:\n        - Conservation: $\\left| \\sum_{j=0}^{4} r_{\\text{dose}}(g_j)\\cdot 4 - (1 + 1 + 1 + 1) \\right| = 0$.\n        - Intensive at $g = 4$: expected missing due to failure of horizon on the future anchor (exactly $4$ hours past the previous but $8$ hours before the next, so the forward horizon fails).\n        - Binary at $g = 4$: expected $1$.\n        - Categorical at $g = 20$: expected $1$.\n\n- Test case $3$ (multiple events in a window and late categorical start):\n    - Intensive (serum sodium): $S_{\\text{Na}} = \\{(9, 139)\\}$, horizon $\\tau = 12$.\n    - Extensive (doses): $S_{\\text{dose}} = \\{(5, 2), (6, 1), (6, 3), (7, 1), (15, 2), (15, 3)\\}$.\n    - Binary (events): $S_{\\text{fever}} = \\{(1, 1), (20, 1)\\}$.\n    - Categorical: $S_{\\text{vent}} = \\{(10, \\text{\"VC\"})\\}$.\n    - Expected checks:\n        - Conservation: $\\left| \\sum_{j=0}^{4} r_{\\text{dose}}(g_j)\\cdot 4 - (2 + 1 + 3 + 1 + 2 + 3) \\right| = 0$.\n        - Intensive at $g = 12$: expected missing (no future anchor for interpolation).\n        - Binary at $g = 20$: expected $1$.\n        - Categorical at $g = 12$: expected $0$.\n\nYour program must:\n\n- Implement the resampling operators as defined.\n- For each test case, construct the aligned tensor $X$ and compute four invariant errors:\n    - $e_{\\text{dose}} = \\left| \\sum_{j=0}^{4} r_{\\text{dose}}(g_j)\\cdot 4 - \\sum_i v_i^{(\\text{dose})} \\right|$.\n    - $e_{\\text{Na}} = |x_{\\text{Na}}(g^*) - v^*|$ if the expectation is a numeric value $v^*$ at grid time $g^*$; if the expectation is missing, set $e_{\\text{Na}} = 0$ when $x_{\\text{Na}}(g^*)$ is missing and $e_{\\text{Na}} = 1$ otherwise.\n    - $e_{\\text{bin}} = |b(g^\\dagger) - b^*|$ for the specified grid time $g^\\dagger$ and expected binary $b^*$.\n    - $e_{\\text{cat}} = |c(g^\\ddagger) - c^*|$ for the specified grid time $g^\\ddagger$ and expected categorical code $c^*$.\n- Report a single float per test case equal to the maximum of the four errors.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, that is, $\\texttt{[r_1,r_2,r_3]}$, where $r_1$, $r_2$, and $r_3$ are the maximum invariant errors (floats) for test cases $1$, $2$, and $3$, respectively.", "solution": "The user has provided a problem statement that requires the implementation of several time-series resampling algorithms specific to different types of clinical data. The problem is to construct a time-aligned feature tensor from irregularly sampled event streams and verify the correctness of the implementation against provided test cases and invariants.\n\n### Step 1: Extract Givens\n\n-   **Time Grid**: A uniform grid $G = \\{ g_j \\}_{j=0}^{J-1}$ with step $\\Delta  0$ hours.\n-   **Event Stream**: For each feature $k$, an event stream is given as $S_k = \\{ (t_i^{(k)}, v_i^{(k)}) \\}_{i=1}^{N_k}$.\n-   **Resampling Operators**: Four types of resampling operators are defined based on event semantics:\n    1.  **Intensive Interpolation**: For a stream $S_k$ and validity horizon $\\tau_k$, the value at grid point $g_j$ is:\n        $$\n        x_k(g_j) = \\begin{cases}\n        v_{\\text{prev}} + \\dfrac{v_{\\text{next}} - v_{\\text{prev}}}{t_{\\text{next}} - t_{\\text{prev}}} \\cdot (g_j - t_{\\text{prev}}),  \\text{if } \\exists\\, t_{\\text{prev}} \\le g_j \\le t_{\\text{next}},\\ g_j - t_{\\text{prev}} \\le \\tau_k,\\ t_{\\text{next}} - g_j \\le \\tau_k, \\\\\n        v_i,  \\text{if } \\exists\\, i \\text{ s.t. } t_i = g_j \\text{ exactly}, \\\\\n        \\text{missing},  \\text{otherwise}.\n        \\end{cases}\n        $$\n        Where $(t_{\\text{prev}}, v_{\\text{prev}})$ and $(t_{\\text{next}}, v_{\\text{next}})$ are the events immediately preceding and succeeding $g_j$.\n    2.  **Extensive Rate Aggregation**: For a grid step $\\Delta$, the rate over window $[g_j, g_{j+1})$ is:\n        $$\n        r_k(g_j) = \\frac{1}{\\Delta} \\sum_{i: t_i^{(k)} \\in [g_j, g_{j+1})} v_i^{(k)}.\n        $$\n        For the terminal point, $r_k(g_{J-1}) = 0$. Conservation requires $\\sum_{j=0}^{J-2} r_k(g_j) \\cdot \\Delta = \\sum_{i=1}^{N_k} v_i^{(k)}$.\n    3.  **Binary Detection Window**: For a window $\\omega  0$, the binary flag at $g_j$ is:\n        $$\n        b_k(g_j) = \\mathbf{1}\\left( \\exists\\, i \\text{ with } t_i^{(k)} \\in (g_j - \\omega, g_j] \\right).\n        $$\n    4.  **Categorical Step Carry-Forward**: The state at $g_j$ is:\n        $$\n        c_k(g_j) = \\begin{cases}\n        \\text{last known category at time } \\le g_j,  \\text{if any exists}, \\\\\n        -1,  \\text{if unknown before first observation}.\n        \\end{cases}\n        $$\n-   **Common Parameters**: Grid $G = [0, 4, 8, 12, 16, 20]$ hours ($J=6, \\Delta=4$), binary window $\\omega = 4$ hours, categorical encoding $\\{\\text{\"VC\"} \\mapsto 0, \\text{\"PS\"} \\mapsto 1\\}$.\n-   **Test Cases**: Three distinct test cases are provided with specific event streams and parameters, along with expected values for verification at certain grid points.\n-   **Error Metrics**: Four error metrics ($e_{\\text{dose}}, e_{\\text{Na}}, e_{\\text{bin}}, e_{\\text{cat}}$) are defined to quantify deviation from the expected checks.\n-   **Final Output**: The program must report a single float per test case, representing the maximum of the four calculated errors. The output format is a comma-separated list in brackets: `[r_1,r_2,r_3]`.\n\n### Step 2: Validate Using Extracted Givens\n\n1.  **Scientific Grounding**: The problem is well-grounded in the principles of time-series analysis and signal processing as applied to medical data. The differentiation between intensive and extensive quantities, and the specific handling of binary and categorical data, are standard and valid concepts in this domain. All definitions are scientifically and mathematically sound.\n2.  **Well-Posedness and Objectivity**: The problem is well-posed. The resampling rules are defined with mathematical precision, including specifications for handling boundary conditions and time intervals (e.g., $[g_j, g_{j+1})$ vs. $(g_j-\\omega, g_j]$). Given the inputs, a unique output tensor can be deterministically computed. The language is objective and free of subjective claims.\n3.  **Incompleteness or Contradiction**: The specification for intensive interpolation presents multiple cases. A potential ambiguity arises in the order of evaluating these cases. Specifically, if a measurement occurs exactly on a grid point, it could also be considered for interpolation. However, the structure implies disjoint cases, and the only logical, non-contradictory interpretation is that an exact match takes precedence over interpolation, preventing a division-by-zero in the interpolation formula. This is a standard assumption in such problems. The conservation law for extensive quantities is defined over the interval $[g_0, g_{J-1})$, and the test data correctly fall within this range, avoiding contradiction. Overall, the problem is self-contained and consistent.\n4.  **Unrealistic Conditions**: The data values and time scales are realistic for clinical scenarios. No physical or scientific impossibilities are present.\n5.  **Ill-Posed Structure**: The problem is well-structured, leading to a unique solution. The terms are clearly defined.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a well-defined computational task based on sound principles of data engineering for clinical time series. A solution will be developed and implemented.\n\n### Solution and Implementation\n\nThe solution involves implementing four separate resampling functions, one for each data type, and a main function to process the test cases. NumPy is utilized for efficient array operations and numerical calculations.\n\n1.  **Intensive Resampling (`resample_intensive`)**: This function first checks for exact time matches between events and grid points. If no exact match exists for a grid point $g_j$, it finds the immediately preceding event $(t_{\\text{prev}}, v_{\\text{prev}})$ and succeeding event $(t_{\\text{next}}, v_{\\text{next}})$. It then verifies that both events are within the specified horizon $\\tau$ from $g_j$ (i.e., $g_j - t_{\\text{prev}} \\le \\tau$ and $t_{\\text{next}} - g_j \\le \\tau$). If all conditions are met, linear interpolation is performed. Otherwise, the value is marked as missing (represented by `np.nan`). Sorted event times and `np.searchsorted` are used for efficient lookups.\n\n2.  **Extensive Resampling (`resample_extensive`)**: This function calculates the rate of an extensive quantity. It aggregates the total value of all events falling within each half-open grid interval $[g_j, g_{j+1})$. This sum is then divided by the interval duration $\\Delta$ to yield the rate $r(g_j)$. The rate for the final grid point, $r(g_{J-1})$, is set to $0$ as per the problem definition. `np.searchsorted` is used to efficiently map event times to their corresponding grid intervals, ensuring correctness for the half-open interval definition.\n\n3.  **Binary Resampling (`resample_binary`)**: For each grid point $g_j$, this function determines if any event occurred within the retrospective window $(g_j-\\omega, g_j]$. The result is $1$ if at least one event is found, and $0$ otherwise. `np.searchsorted` is employed on the sorted event times to efficiently query the number of events within each window without redundant scanning.\n\n4.  **Categorical Resampling (`resample_categorical`)**: This function implements a last-observation-carried-forward (LOCF) logic. For each grid point $g_j$, it finds the most recent event with a timestamp less than or equal to $g_j$. The value of that event is then assigned to $g_j$. If no preceding event exists, a default value of $-1$ is used. This is implemented efficiently using `np.searchsorted` with `side='right'`.\n\nFinally, a main `solve` function orchestrates the process for each test case. It calls the appropriate resampling functions, computes the four specified error metrics ($e_{\\text{dose}}, e_{\\text{Na}}, e_{\\text{bin}}, e_{\\text{cat}}$) based on the checks provided in the problem, determines the maximum error for the case, and formats the results for final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are used.\n\ndef solve():\n    \"\"\"\n    Main function to solve the clinical time-series resampling problem.\n    It contains resampling helpers and processes the test cases.\n    \"\"\"\n\n    def resample_intensive(events, grid, tau):\n        \"\"\"\n        Resamples an intensive quantity stream using linear interpolation\n        with a clinical validity horizon.\n        \"\"\"\n        if not events:\n            return np.full(len(grid), np.nan)\n        \n        times = np.array([e[0] for e in events])\n        values = np.array([e[1] for e in events])\n        \n        resampled_values = np.full(len(grid), np.nan)\n        \n        for i, g in enumerate(grid):\n            # Precedence 1: Exact match on the grid point.\n            exact_match_indices = np.where(times == g)[0]\n            if exact_match_indices.size > 0:\n                resampled_values[i] = values[exact_match_indices[0]]\n                continue\n\n            # Precedence 2: Interpolation between two existing points.\n            idx = np.searchsorted(times, g)\n            \n            if idx == 0 or idx == len(times):\n                continue\n\n            t_prev, v_prev = times[idx - 1], values[idx - 1]\n            t_next, v_next = times[idx], values[idx]\n\n            # Check horizon constraints\n            if (g - t_prev) = tau and (t_next - g) = tau:\n                resampled_values[i] = v_prev + (v_next - v_prev) / (t_next - t_prev) * (g - t_prev)\n                \n        return resampled_values\n\n    def resample_extensive(events, grid, delta):\n        \"\"\"\n        Resamples an extensive quantity stream via rate aggregation.\n        \"\"\"\n        num_grid_points = len(grid)\n        rates = np.zeros(num_grid_points)\n        if not events:\n            return rates\n\n        num_bins = num_grid_points - 1\n        bin_sums = np.zeros(num_bins)\n        \n        for t, v in events:\n            if t  grid[0] or t >= grid[-1]:\n                continue\n            \n            j = np.searchsorted(grid, t, side='right') - 1\n            if 0 = j  num_bins:\n                 bin_sums[j] += v\n            \n        rates[:num_bins] = bin_sums / delta\n        return rates\n\n    def resample_binary(events, grid, omega):\n        \"\"\"\n        Resamples a binary event stream using a detection window.\n        \"\"\"\n        num_grid_points = len(grid)\n        flags = np.zeros(num_grid_points, dtype=int)\n        if not events:\n            return flags\n\n        times = np.array([e[0] for e in events])\n\n        for i, g in enumerate(grid):\n            window_start = g - omega\n            # Find events t in (window_start, g]\n            start_idx = np.searchsorted(times, window_start, side='right')\n            end_idx = np.searchsorted(times, g, side='right')\n            \n            if start_idx  end_idx:\n                flags[i] = 1\n                \n        return flags\n\n    def resample_categorical(events, grid, cat_map):\n        \"\"\"\n        Resamples a categorical state stream using last-observation-carried-forward.\n        \"\"\"\n        num_grid_points = len(grid)\n        codes = np.full(num_grid_points, -1, dtype=int)\n        if not events:\n            return codes\n\n        coded_events = sorted([(t, cat_map[v_str]) for t, v_str in events])\n        times = np.array([e[0] for e in coded_events])\n        values = np.array([e[1] for e in coded_events])\n\n        indices = np.searchsorted(times, grid, side='right')\n        \n        for i, idx in enumerate(indices):\n            if idx > 0:\n                codes[i] = values[idx - 1]\n                \n        return codes\n\n    # Define the common grid and parameters\n    grid = np.array([0.0, 4.0, 8.0, 12.0, 16.0, 20.0])\n    delta = 4.0\n    omega = 4.0\n    cat_map = {\"VC\": 0, \"PS\": 1}\n\n    # Define the three test cases from the problem statement.\n    test_cases = [\n        {\n            \"s_na\": [(1, 140), (9, 138), (15, 142)], \"tau\": 8,\n            \"s_dose\": [(2, 5), (6, 3), (11, 4), (19, 2)],\n            \"s_fever\": [(3, 1), (10, 1), (17, 1)],\n            \"s_vent\": [(5, \"VC\"), (14, \"PS\")],\n            \"checks\": {\n                \"dose_conservation_sum\": 5 + 3 + 4 + 2,\n                \"na_check\": {\"g\": 12, \"val\": 140.0},\n                \"bin_check\": {\"g\": 12, \"val\": 1},\n                \"cat_check\": {\"g\": 16, \"val\": 1},\n            }\n        },\n        {\n            \"s_na\": [(0, 135), (12, 145)], \"tau\": 4,\n            \"s_dose\": [(4, 1), (8, 1), (12, 1), (16, 1)],\n            \"s_fever\": [(4, 1), (7, 1)],\n            \"s_vent\": [(0, \"PS\")],\n            \"checks\": {\n                \"dose_conservation_sum\": 1 + 1 + 1 + 1,\n                \"na_check\": {\"g\": 4, \"val\": \"missing\"},\n                \"bin_check\": {\"g\": 4, \"val\": 1},\n                \"cat_check\": {\"g\": 20, \"val\": 1},\n            }\n        },\n        {\n            \"s_na\": [(9, 139)], \"tau\": 12,\n            \"s_dose\": [(5, 2), (6, 1), (6, 3), (7, 1), (15, 2), (15, 3)],\n            \"s_fever\": [(1, 1), (20, 1)],\n            \"s_vent\": [(10, \"VC\")],\n            \"checks\": {\n                \"dose_conservation_sum\": 2 + 1 + 3 + 1 + 2 + 3,\n                \"na_check\": {\"g\": 12, \"val\": \"missing\"},\n                \"bin_check\": {\"g\": 20, \"val\": 1},\n                \"cat_check\": {\"g\": 12, \"val\": 0},\n            }\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Sort event streams by time, as required by the problem\n        s_na = sorted(case[\"s_na\"])\n        s_dose = sorted(case[\"s_dose\"])\n        s_fever = sorted(case[\"s_fever\"])\n        s_vent = sorted(case[\"s_vent\"])\n\n        # Perform resampling for each feature type\n        x_na = resample_intensive(s_na, grid, case[\"tau\"])\n        r_dose = resample_extensive(s_dose, grid, delta)\n        b_fever = resample_binary(s_fever, grid, omega)\n        c_vent = resample_categorical(s_vent, grid, cat_map)\n\n        # Calculate the four invariant errors\n        \n        # e_dose: Conservation error\n        total_grid_amount = np.sum(r_dose[:-1]) * delta\n        total_original_amount = case[\"checks\"][\"dose_conservation_sum\"]\n        e_dose = abs(total_grid_amount - total_original_amount)\n\n        # e_na: Intensive value error\n        check_na = case[\"checks\"][\"na_check\"]\n        g_idx_na = np.where(grid == check_na[\"g\"])[0][0]\n        val_na_calc = x_na[g_idx_na]\n        if check_na[\"val\"] == \"missing\":\n            e_na = 0.0 if np.isnan(val_na_calc) else 1.0\n        else:\n            e_na = abs(val_na_calc - check_na[\"val\"])\n            \n        # e_bin: Binary flag error\n        check_bin = case[\"checks\"][\"bin_check\"]\n        g_idx_bin = np.where(grid == check_bin[\"g\"])[0][0]\n        val_bin_calc = b_fever[g_idx_bin]\n        e_bin = abs(val_bin_calc - check_bin[\"val\"])\n\n        # e_cat: Categorical code error\n        check_cat = case[\"checks\"][\"cat_check\"]\n        g_idx_cat = np.where(grid == check_cat[\"g\"])[0][0]\n        val_cat_calc = c_vent[g_idx_cat]\n        e_cat = abs(val_cat_calc - check_cat[\"val\"])\n\n        # The result for this test case is the maximum of the four errors\n        max_error = max(e_dose, e_na, float(e_bin), float(e_cat))\n        results.append(max_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4563173"}, {"introduction": "A primary goal in medical analytics is to predict a patient's risk of a future event, a task known as survival analysis. Using longitudinal patient data to build these models is fraught with methodological pitfalls, most notably immortal time bias, where information from the future inadvertently leaks into the model's training data. This advanced practice ([@problem_id:4563190]) introduces landmarking, a powerful technique that enforces a strict temporal discipline on feature construction. By creating features and defining outcomes relative to a fixed \"landmark\" time, you will learn how to build predictive models that are not only accurate but also methodologically sound and free from this critical bias.", "problem": "You are given a simplified abstraction of Electronic Health Record (EHR) data and clinical follow-up for multiple subjects, with the goal of constructing landmark features at a fixed landmark time and defining rules that avoid immortal time bias in subsequent outcome labeling. The central task is to implement a landmark analysis procedure that constructs time-dependent covariates at a fixed time origin and labels outcomes within a fixed horizon using principled survival analysis definitions.\n\nFundamental base and definitions:\n- Survival analysis defines each subject by an event time $T_i$ and a censoring time $C_i$, where the observed time is $X_i = \\min(T_i, C_i)$ and the event indicator is $\\Delta_i = \\mathbb{1}\\{T_i \\le C_i\\}$. Immortal time bias arises when post-baseline information is used to classify exposure or outcome in a way that requires survival until the time the information is recorded; to avoid this, landmark analysis constructs covariates and labels at a fixed origin originating from the landmark.\n- Landmark analysis fixes a landmark time $L$ and defines the risk set as subjects event-free and uncensored at $L$. Outcomes are defined relative to $L$, not the original start time.\n\nTask:\n- You must implement a program that constructs landmark features and labels outcomes using the following fixed parameters:\n  1. Landmark time $L = 100$ days.\n  2. Outcome horizon $H = 30$ days.\n  3. Systolic blood pressure lookback window $W_{\\mathrm{sbp}} = 60$ days.\n  4. Visit count lookback window $W_{\\mathrm{vis}} = 90$ days.\n  5. Exponential decay parameter $\\lambda = 0.05$ per day.\n  6. Reference systolic blood pressure $r_{\\mathrm{sbp}} = 120$ millimeters of mercury (mmHg).\n\n- For each subject $i$, the program must enforce the following rules to avoid immortal time bias, expressed mathematically:\n  1. Eligibility (risk set): include a subject if and only if $\\min(T_i, C_i)  L$ and, if $T_i$ is defined, $T_i  L$, and $C_i  L$. If $T_i = L$ or $C_i \\le L$, the subject is not eligible at the landmark.\n  2. Time origin and delayed entry: at the landmark, reset time to $0$ by defining $t' = t - L$. Only covariate information from times $t \\le L$ is admissible for feature construction.\n  3. Outcome label within horizon: define the label $Y_i = \\mathbb{1}\\{L  T_i \\le \\min(C_i, L + H)\\}$ for eligible subjects. This labels an event as occurring in the interval $(L, L + H]$ before censoring. For non-eligible subjects, the label is set to $0$ by convention, and the eligibility flag indicates exclusion from the risk set.\n\nFeature construction from admissible pre-landmark data (all times in days, all systolic blood pressure values in millimeters of mercury):\n- Systolic blood pressure features:\n  1. Last value carried forward within window: let $\\{(t_{ij}, x_{ij})\\}$ be systolic blood pressure measurements with $t_{ij} \\in [L - W_{\\mathrm{sbp}}, L]$. The last value carried forward is $x_{i,\\mathrm{last}} = x_{ij^*}$ where $j^* = \\arg\\max_j t_{ij}$. If no measurements satisfy the window, set $x_{i,\\mathrm{last}} = r_{\\mathrm{sbp}}$.\n  2. Exponentially weighted mean within window: define weights $w_{ij} = \\exp(-\\lambda (L - t_{ij}))$ for $t_{ij} \\in [L - W_{\\mathrm{sbp}}, L]$. The exponentially weighted mean is $$\\bar{x}_{i,\\mathrm{ewm}} = \\begin{cases}\\frac{\\sum_j w_{ij} x_{ij}}{\\sum_j w_{ij}},  \\text{if there exists } j \\text{ with } t_{ij} \\in [L - W_{\\mathrm{sbp}}, L],\\\\ r_{\\mathrm{sbp}},  \\text{otherwise.}\\end{cases}$$\n- Medication exposure feature:\n  1. Active at landmark: given medication intervals $\\{[a_{ik}, b_{ik})\\}$, define $m_{i,\\mathrm{active}} = \\mathbb{1}\\{\\exists k: a_{ik} \\le L  b_{ik}\\}$, that is, active at $L$ if any interval covers $L$.\n- Visit count feature:\n  1. Visit count in window: given visit times $\\{v_{i\\ell}\\}$, define $n_{i,\\mathrm{vis}} = \\left|\\{v_{i\\ell} \\in [L - W_{\\mathrm{vis}}, L]\\}\\right|$.\n\nUnits:\n- Express all times in days.\n- Express systolic blood pressure values in millimeters of mercury (mmHg).\n- The output floats must be rounded to three decimal places.\n\nTest suite:\n- Use the following four subjects with scientifically plausible values.\n\nSubject $1$:\n- Event time $T_1 = 115$, censoring time $C_1 = 200$.\n- Systolic blood pressure times $[20, 85, 95, 99]$ and values $[130, 140, 135, 138]$.\n- Medication intervals $[[50, 120]]$.\n- Visit times $[30, 70, 80, 95]$.\n\nSubject $2$ (boundary case: event at the landmark):\n- Event time $T_2 = 100$, censoring time $C_2 = 300$.\n- Systolic blood pressure times $[40, 55, 100]$ and values $[128, 132, 129]$.\n- Medication intervals $[[101, 200]]$.\n- Visit times $[99, 100]$.\n\nSubject $3$ (missing systolic blood pressure in the lookback window, censoring within horizon):\n- No event observed; treat as $T_3 = +\\infty$, censoring time $C_3 = 120$.\n- Systolic blood pressure times $[10, 20, 30]$ and values $[125, 127, 126]$.\n- Medication intervals $[[0, 90]]$.\n- Visit times $[5, 15, 25, 35, 45]$.\n\nSubject $4$ (event after horizon):\n- Event time $T_4 = 160$, censoring time $C_4 = 200$.\n- Systolic blood pressure times $[70, 110]$ and values $[150, 149]$.\n- Medication intervals $[[20, 80], [95, 105]]$.\n- Visit times $[40, 100, 101]$.\n\nOutput specification:\n- For each subject $i$, compute and output a list $[\\,\\text{eligible}_i,\\, Y_i,\\, x_{i,\\mathrm{last}},\\, \\bar{x}_{i,\\mathrm{ewm}},\\, m_{i,\\mathrm{active}},\\, n_{i,\\mathrm{vis}}\\,]$, where the booleans are standard logical values and the floats are rounded to three decimal places. The final program output must be a single line containing a comma-separated list of these subject-level lists enclosed in square brackets, for example, $[[\\dots],[\\dots],[\\dots],[\\dots]]$.", "solution": "The solution proceeds by principled application of survival analysis and carefully constructed feature engineering rules that explicitly avoid immortal time bias at the landmark. The scientific foundations are:\n\n1. Define event and censoring times using survival analysis. For each subject $i$, let $T_i$ be the time to event and $C_i$ be the censoring time. The observed time is $X_i = \\min(T_i, C_i)$ and the event indicator is $\\Delta_i = \\mathbb{1}\\{T_i \\le C_i\\}$. Immortal time bias arises if we use information that occurs after baseline or landmark to classify exposure or outcome, thereby requiring survival until such information is observed. Landmark analysis mitigates this by fixing an origin at $L$ and restricting both features and outcome labeling to information available at or before $L$ and outcomes defined relative to $L$.\n\n2. Eligibility at the landmark uses delayed entry (left truncation) logic: include only those subjects who are event-free and uncensored at $L$. Formally, subject $i$ is eligible if $X_i  L$ and, when $T_i$ is defined, $T_i  L$. Subject $i$ with $T_i = L$ is not event-free at the landmark and must be excluded. Subject $i$ with $C_i \\le L$ is censored before or at the landmark and must be excluded.\n\n3. We reset the time origin to $0$ at $L$ by mapping any time $t$ to $t' = t - L$. Covariates are computed using only data with original times $t \\le L$. The outcome label within horizon $H$ is $Y_i = \\mathbb{1}\\{L  T_i \\le \\min(C_i, L + H)\\}$ for eligible subjects, corresponding to an event in the interval $(L, L + H]$ prior to censoring. For ineligible subjects, we set $Y_i = 0$ by convention and use the eligibility flag to indicate that they are excluded from the risk set. This definition ensures scientific realism by not attributing post-landmark exposure or future survival to pre-landmark risk.\n\n4. Feature engineering:\n   - Systolic blood pressure last value carried forward within window $[L - W_{\\mathrm{sbp}}, L]$: let $\\{(t_{ij}, x_{ij})\\}$ be measurements with $t_{ij}$ in the valid window. The feature $x_{i,\\mathrm{last}}$ is the value associated with the maximum $t_{ij}$ in the window. If the window has no measurements, set $x_{i,\\mathrm{last}} = r_{\\mathrm{sbp}}$.\n   - Exponentially weighted mean within the same window with decay parameter $\\lambda$: the weights are $w_{ij} = \\exp(-\\lambda (L - t_{ij}))$. The weighted mean is $$\\bar{x}_{i,\\mathrm{ewm}} = \\begin{cases}\\frac{\\sum_j w_{ij} x_{ij}}{\\sum_j w_{ij}},  \\text{if the set of } j \\text{ is nonempty},\\\\ r_{\\mathrm{sbp}},  \\text{otherwise.}\\end{cases}$$ The normalization by the sum of weights ensures a proper weighted average and avoids bias toward more recent values, with larger weights for $t_{ij}$ closer to $L$.\n   - Medication active at landmark: given intervals $[a_{ik}, b_{ik})$ of exposure, define $m_{i,\\mathrm{active}} = \\mathbb{1}\\{\\exists k: a_{ik} \\le L  b_{ik}\\}$ to capture activity at $L$ only. This avoids immortal time bias that would occur if we defined exposure as \"ever during follow-up,\" which would require survival past $L$ for exposed classification.\n   - Visit count within window $[L - W_{\\mathrm{vis}}, L]$: $n_{i,\\mathrm{vis}} = \\left|\\{v_{i\\ell} \\in [L - W_{\\mathrm{vis}}, L]\\}\\right|$.\n\n5. Apply to the test suite:\n   - Fixed parameters: $L = 100$ days, $H = 30$ days, $W_{\\mathrm{sbp}} = 60$ days, $W_{\\mathrm{vis}} = 90$ days, $\\lambda = 0.05$ per day, $r_{\\mathrm{sbp}} = 120$ mmHg.\n   - Subject $1$: $T_1 = 115$, $C_1 = 200$ implies eligibility since $115  100$ and $200  100$. Label $Y_1 = \\mathbb{1}\\{100  115 \\le 130\\} = 1$. Systolic blood pressure window is $[40, 100]$, so times $[85, 95, 99]$ are included with values $[140, 135, 138]$. Last value $x_{1,\\mathrm{last}} = 138$. Exponentially weighted mean uses weights $w = [\\exp(-0.05 \\cdot 15), \\exp(-0.05 \\cdot 5), \\exp(-0.05 \\cdot 1)]$ and yields a value approximately $137.3$ to $137.4$ mmHg after normalization. Medication interval $[50, 120)$ is active at $L$, so $m_{1,\\mathrm{active}} = 1$. Visit count in window $[10, 100]$ includes $[30, 70, 80, 95]$ for $n_{1,\\mathrm{vis}} = 4$.\n   - Subject $2$: $T_2 = 100$, $C_2 = 300$ implies ineligible since $T_2 = L$. Set $Y_2 = 0$ by convention, eligibility flag indicates exclusion. Systolic blood pressure window includes $[40, 55, 100]$ with values $[128, 132, 129]$. Last value $x_{2,\\mathrm{last}} = 129$. Exponentially weighted mean uses weights $[\\exp(-0.05 \\cdot 60), \\exp(-0.05 \\cdot 45), 1]$, yielding approximately $129.1$ to $129.2$ mmHg. Medication $[101, 200)$ is not active at $L$, so $m_{2,\\mathrm{active}} = 0$. Visit count window $[10, 100]$ includes $[99, 100]$ with $n_{2,\\mathrm{vis}} = 2$.\n   - Subject $3$: Treat no event as $T_3 = +\\infty$, $C_3 = 120$ implies eligibility ($120  100$). Label $Y_3 = \\mathbb{1}\\{100  +\\infty \\le \\min(120, 130)\\} = 0$. Systolic blood pressure window $[40, 100]$ has no measurements; use reference $x_{3,\\mathrm{last}} = 120$ and $\\bar{x}_{3,\\mathrm{ewm}} = 120$. Medication $[0, 90)$ ends before $L$, so $m_{3,\\mathrm{active}} = 0$. Visit window $[10, 100]$ includes $[15, 25, 35, 45]$ with $n_{3,\\mathrm{vis}} = 4$.\n   - Subject $4$: $T_4 = 160$, $C_4 = 200$ implies eligibility ($160  100$, $200  100$). Label $Y_4 = \\mathbb{1}\\{100  160 \\le 130\\} = 0$. Systolic blood pressure window $[40, 100]$ includes only $70$ with value $150$; last value $x_{4,\\mathrm{last}} = 150$ and exponentially weighted mean equals $150$ because a single measurement is normalized to itself. Medication intervals $[20, 80)$ and $[95, 105)$ include $L$ in the second interval, so $m_{4,\\mathrm{active}} = 1$. Visit window $[10, 100]$ includes $[40, 100]$ with $n_{4,\\mathrm{vis}} = 2$.\n\n6. Implementation details:\n   - All computations must be performed using only data at or before $L$.\n   - Floats must be rounded to three decimals in the final output.\n   - The program must produce a single line of output containing the list of subject results: $[[\\text{eligible}_1, Y_1, x_{1,\\mathrm{last}}, \\bar{x}_{1,\\mathrm{ewm}}, m_{1,\\mathrm{active}}, n_{1,\\mathrm{vis}}], \\dots, [\\text{eligible}_4, Y_4, x_{4,\\mathrm{last}}, \\bar{x}_{4,\\mathrm{ewm}}, m_{4,\\mathrm{active}}, n_{4,\\mathrm{vis}}]]$.\n\nThis design grounds the algorithm in survival analysis principles, enforces delayed entry at the landmark, restricts covariates to admissible pre-landmark information, and defines outcomes within a fixed horizon, all of which collectively prevent immortal time bias.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_landmark_features(subject, params):\n    \"\"\"\n    Compute landmark features and label for one subject according to specified rules.\n    subject: dict with keys:\n        'event_time': float or None (None => no event, treated as +inf)\n        'censor_time': float\n        'sbp_times': list of floats\n        'sbp_values': list of floats (same length as sbp_times)\n        'med_intervals': list of [start, end) intervals\n        'visit_times': list of floats\n    params: dict with keys:\n        'L': landmark time\n        'H': horizon\n        'W_sbp': systolic BP lookback window\n        'W_vis': visit lookback window\n        'lambda': exponential decay per day\n        'r_sbp': reference sbp\n    Returns: list [eligible_bool, label_bool, last_sbp, ewm_sbp, med_active_int, visit_count_int]\n             floats rounded to 3 decimals as specified.\n    \"\"\"\n    L = params['L']\n    H = params['H']\n    W_sbp = params['W_sbp']\n    W_vis = params['W_vis']\n    lam = params['lambda']\n    r_sbp = params['r_sbp']\n\n    T = subject['event_time']\n    C = subject['censor_time']\n    # Treat None event time as +inf\n    T_eff = np.inf if T is None else T\n    X = min(T_eff, C)\n\n    # Eligibility: event-free and uncensored strictly after L\n    eligible = (X > L) and (C > L) and (T_eff > L)\n\n    # Window bounds\n    sbp_start = L - W_sbp\n    vis_start = L - W_vis\n\n    # Filter SBP measurements within [sbp_start, L]\n    sbp_times = subject['sbp_times']\n    sbp_values = subject['sbp_values']\n    sbp_in_window_indices = [i for i, t in enumerate(sbp_times) if (t >= sbp_start and t = L)]\n    if sbp_in_window_indices:\n        # Last value carried forward: choose max t within window\n        # Determine index of max time within window\n        last_idx = max(sbp_in_window_indices, key=lambda i: sbp_times[i])\n        last_sbp = sbp_values[last_idx]\n        # Exponentially weighted mean\n        times = np.array([sbp_times[i] for i in sbp_in_window_indices], dtype=float)\n        values = np.array([sbp_values[i] for i in sbp_in_window_indices], dtype=float)\n        weights = np.exp(-lam * (L - times))\n        ewm_sbp = float(np.sum(weights * values) / np.sum(weights))\n    else:\n        last_sbp = float(r_sbp)\n        ewm_sbp = float(r_sbp)\n\n    # Medication active at L\n    med_active = 0\n    for interval in subject['med_intervals']:\n        a, b = float(interval[0]), float(interval[1])\n        if a = L  b:\n            med_active = 1\n            break\n\n    # Visit count within [vis_start, L]\n    visit_times = subject['visit_times']\n    visit_count = sum(1 for t in visit_times if (t >= vis_start and t = L))\n\n    # Label within horizon for eligible subjects: event in (L, L+H] before censoring\n    # If ineligible, set label to False by convention\n    if eligible:\n        # Event occurs if T_eff is finite and within horizon and before or at censor\n        label = (T_eff > L) and (T_eff = min(C, L + H))\n    else:\n        label = False\n\n    # Round floats to 3 decimals\n    last_sbp_rounded = round(float(last_sbp), 3)\n    ewm_sbp_rounded = round(float(ewm_sbp), 3)\n\n    return [bool(eligible), bool(label), last_sbp_rounded, ewm_sbp_rounded, int(med_active), int(visit_count)]\n\ndef solve():\n    # Define parameters from the problem statement\n    params = {\n        'L': 100.0,      # days\n        'H': 30.0,       # days\n        'W_sbp': 60.0,   # days\n        'W_vis': 90.0,   # days\n        'lambda': 0.05,  # per day\n        'r_sbp': 120.0   # mmHg\n    }\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            'event_time': 115.0,\n            'censor_time': 200.0,\n            'sbp_times': [20.0, 85.0, 95.0, 99.0],\n            'sbp_values': [130.0, 140.0, 135.0, 138.0],\n            'med_intervals': [(50.0, 120.0)],\n            'visit_times': [30.0, 70.0, 80.0, 95.0]\n        },\n        {\n            'event_time': 100.0,\n            'censor_time': 300.0,\n            'sbp_times': [40.0, 55.0, 100.0],\n            'sbp_values': [128.0, 132.0, 129.0],\n            'med_intervals': [(101.0, 200.0)],\n            'visit_times': [99.0, 100.0]\n        },\n        {\n            'event_time': None,  # No event observed\n            'censor_time': 120.0,\n            'sbp_times': [10.0, 20.0, 30.0],\n            'sbp_values': [125.0, 127.0, 126.0],\n            'med_intervals': [(0.0, 90.0)],\n            'visit_times': [5.0, 15.0, 25.0, 35.0, 45.0]\n        },\n        {\n            'event_time': 160.0,\n            'censor_time': 200.0,\n            'sbp_times': [70.0, 110.0],\n            'sbp_values': [150.0, 149.0],\n            'med_intervals': [(20.0, 80.0), (95.0, 105.0)],\n            'visit_times': [40.0, 100.0, 101.0]\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_landmark_features(case, params)\n        results.append(result)\n\n    # Final print statement in the exact required format: single line, comma-separated list enclosed in brackets.\n    # We convert each sublist to its string representation and join them.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4563190"}]}