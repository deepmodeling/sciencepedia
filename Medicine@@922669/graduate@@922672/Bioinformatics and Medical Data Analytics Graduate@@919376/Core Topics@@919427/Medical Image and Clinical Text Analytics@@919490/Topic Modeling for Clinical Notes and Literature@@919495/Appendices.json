{"hands_on_practices": [{"introduction": "To truly understand Latent Dirichlet Allocation (LDA), it's essential to grasp its core inference engine: the collapsed Gibbs sampler. This exercise walks you through a single, crucial step of this process—resampling a topic for one token. By manually calculating the conditional probability using provided counts, you will gain a concrete understanding of how the model balances a word's affinity for a topic with that topic's prevalence in a document, which is the fundamental mechanism driving topic discovery [@problem_id:4613930].", "problem": "A hospital is building a probabilistic topic model to mine themes in clinical notes and literature extracts. Consider the Latent Dirichlet Allocation (LDA) generative process where, for each document $d$, the document-topic proportion vector $\\boldsymbol{\\theta}_{d}$ is drawn from a Dirichlet distribution $\\mathrm{Dir}(\\boldsymbol{\\alpha})$, and for each topic $k$, the topic-word distribution $\\boldsymbol{\\phi}_{k}$ is drawn from a Dirichlet distribution $\\mathrm{Dir}(\\beta \\mathbf{1}_{V})$, where $\\mathbf{1}_{V}$ is the $V$-dimensional vector of ones. Words are generated by first drawing a topic $z_{i}$ for token $i$ from $\\mathrm{Multinomial}(\\boldsymbol{\\theta}_{d})$, and then drawing a word $w_{i}$ from $\\mathrm{Multinomial}(\\boldsymbol{\\phi}_{z_{i}})$. Assume a vocabulary size $V$ and symmetric word prior $\\beta$, but a potentially asymmetric document-topic prior $\\boldsymbol{\\alpha}$.\n\nFor a specific clinical note $d$, consider resampling the topic assignment for a single occurrence of the token “fever” under collapsed Gibbs sampling, where $\\boldsymbol{\\theta}_{d}$ and all $\\boldsymbol{\\phi}_{k}$ are integrated out. You are given the following quantities that exclude the current token (denoted by the superscript $^{-i}$):\n\n- Number of topics is $K=3$.\n- Vocabulary size is $V=15000$.\n- Symmetric word prior is $\\beta=0.002$.\n- Document-topic Dirichlet prior is $\\boldsymbol{\\alpha}=(\\alpha_{1},\\alpha_{2},\\alpha_{3})=(0.4,0.3,0.2)$.\n- Document-topic counts for $d$: $(n_{d,1}^{-i},n_{d,2}^{-i},n_{d,3}^{-i})=(15,3,6)$.\n- Topic-word counts for the word “fever”: $(n_{1,\\mathrm{fever}}^{-i},n_{2,\\mathrm{fever}}^{-i},n_{3,\\mathrm{fever}}^{-i})=(120,5,10)$.\n- Topic token totals: $(n_{1}^{-i},n_{2}^{-i},n_{3}^{-i})=(8000,6000,7000)$.\n\nStarting from the standard LDA generative assumptions and Dirichlet–Multinomial conjugacy, derive the collapsed Gibbs conditional for assigning this token to topic $k$ and use it to compute the unnormalized weights for $k=1,2,3$. Normalize these weights to obtain a proper distribution over topics for this token. Report the normalized probability of assigning this token to topic $k=2$ as a decimal, and round your final answer to five significant figures. No units are required.", "solution": "The problem is valid. It describes a standard application of collapsed Gibbs sampling for Latent Dirichlet Allocation (LDA), a core algorithm in probabilistic topic modeling. All necessary data and parameters are provided, and the problem is scientifically sound and well-posed.\n\nThe task is to calculate the conditional probability for a single token's topic assignment, given all other topic assignments. In collapsed Gibbs sampling for LDA, the continuous parameters $\\boldsymbol{\\theta}$ (document-topic distributions) and $\\boldsymbol{\\phi}$ (topic-word distributions) are integrated out. This is possible due to the conjugacy of the Dirichlet and Multinomial distributions.\n\nLet $z_i$ be the topic assignment for the $i$-th token in the corpus. We are interested in resampling the topic for a specific token, which is an instance of the word \"fever\" in document $d$. Let this token be at position $i$, with word type $w_i=v$ (where $v$ corresponds to \"fever\"). The full conditional probability for assigning this token to topic $k$, given all other assignments $z_{\\neg i}$ and all words $W$, is given by:\n$$ P(z_i = k | z_{\\neg i}, W, \\boldsymbol{\\alpha}, \\beta) \\propto P(\\text{word } v \\text{ from topic } k) \\times P(\\text{topic } k \\text{ in document } d) $$\nThe two terms on the right-hand side correspond to the predictive distributions of the collapsed Dirichlet-Multinomial models for topics and documents.\n\nThe resulting formula is:\n$$ P(z_i = k | z_{\\neg i}, W, \\boldsymbol{\\alpha}, \\beta) \\propto \\frac{n_{k,v}^{-i} + \\beta}{n_k^{-i} + V\\beta} \\times (n_{d,k}^{-i} + \\alpha_k) $$\nHere, the notation is as follows:\n- $n_{k,v}^{-i}$ is the number of times word type $v$ is assigned to topic $k$, excluding the current token $i$.\n- $n_k^{-i} = \\sum_{v'} n_{k,v'}^{-i}$ is the total number of tokens assigned to topic $k$, excluding the current token $i$.\n- $n_{d,k}^{-i}$ is the number of tokens in document $d$ assigned to topic $k$, excluding the current token $i$.\n- $\\beta$ is the symmetric hyperparameter for the topic-word Dirichlet prior.\n- $V$ is the vocabulary size.\n- $\\alpha_k$ is the hyperparameter for topic $k$ for the document-topic Dirichlet prior.\n\nWe are given the following values:\n- Number of topics $K=3$.\n- Vocabulary size $V=15000$.\n- Symmetric word prior $\\beta=0.002$.\n- Document-topic prior $\\boldsymbol{\\alpha}=(\\alpha_{1}, \\alpha_{2}, \\alpha_{3})=(0.4, 0.3, 0.2)$.\n- Token to be resampled corresponds to the word $v=\\text{\"fever\"}$.\n- Document-topic counts: $(n_{d,1}^{-i}, n_{d,2}^{-i}, n_{d,3}^{-i})=(15, 3, 6)$.\n- Topic-word counts for \"fever\": $(n_{1,v}^{-i}, n_{2,v}^{-i}, n_{3,v}^{-i})=(120, 5, 10)$.\n- Total topic counts: $(n_{1}^{-i}, n_{2}^{-i}, n_{3}^{-i})=(8000, 6000, 7000)$.\n\nFirst, we calculate the term $V\\beta$, which is constant across all topics:\n$$ V\\beta = 15000 \\times 0.002 = 30 $$\n\nNow, we compute the unnormalized weight $w_k$ for assigning the token to each topic $k \\in \\{1, 2, 3\\}$.\n$$ w_k = (n_{d,k}^{-i} + \\alpha_k) \\times \\frac{n_{k,v}^{-i} + \\beta}{n_k^{-i} + V\\beta} $$\n\nFor topic $k=1$:\n$$ w_1 = (n_{d,1}^{-i} + \\alpha_1) \\times \\frac{n_{1,v}^{-i} + \\beta}{n_1^{-i} + V\\beta} = (15 + 0.4) \\times \\frac{120 + 0.002}{8000 + 30} $$\n$$ w_1 = 15.4 \\times \\frac{120.002}{8030} $$\n$$ w_1 \\approx 0.23014082 $$\n\nFor topic $k=2$:\n$$ w_2 = (n_{d,2}^{-i} + \\alpha_2) \\times \\frac{n_{2,v}^{-i} + \\beta}{n_2^{-i} + V\\beta} = (3 + 0.3) \\times \\frac{5 + 0.002}{6000 + 30} $$\n$$ w_2 = 3.3 \\times \\frac{5.002}{6030} $$\n$$ w_2 \\approx 0.00273741 $$\n\nFor topic $k=3$:\n$$ w_3 = (n_{d,3}^{-i} + \\alpha_3) \\times \\frac{n_{3,v}^{-i} + \\beta}{n_3^{-i} + V\\beta} = (6 + 0.2) \\times \\frac{10 + 0.002}{7000 + 30} $$\n$$ w_3 = 6.2 \\times \\frac{10.002}{7030} $$\n$$ w_3 \\approx 0.00882111 $$\n\nTo obtain the normalized probabilities, we sum the unnormalized weights:\n$$ S = w_1 + w_2 + w_3 \\approx 0.23014082 + 0.00273741 + 0.00882111 \\approx 0.24169934 $$\n\nThe normalized probability of assigning the token to topic $k=2$ is:\n$$ P(z_i = 2 | \\dots) = \\frac{w_2}{S} = \\frac{w_2}{w_1 + w_2 + w_3} $$\n$$ P(z_i = 2 | \\dots) \\approx \\frac{0.00273741}{0.24169934} \\approx 0.0113256958 $$\n\nThe problem requires the answer to be rounded to five significant figures.\n$$ 0.0113256958 \\approx 0.011326 $$\nThe fifth significant figure is $5$, and the following digit is $6 \\ge 5$, so we round up.", "answer": "$$ \\boxed{0.011326} $$", "id": "4613930"}, {"introduction": "A key feature of Bayesian models like LDA is the ability to incorporate prior beliefs through hyperparameters. This exercise focuses on the document-topic prior, $\\boldsymbol{\\alpha}$, and its powerful role in shaping the resulting topic structures. By deriving a condition for topic dominance and exploring symmetric versus asymmetric priors, you will learn how to steer the model to produce sparser or denser topic mixtures, a critical skill for tailoring LDA to specific data characteristics like the short, focused nature of clinical notes [@problem_id:4613917].", "problem": "Consider Latent Dirichlet Allocation (LDA), where per-document topic proportions $\\theta_{d}$ follow a Dirichlet prior $\\operatorname{Dir}(\\boldsymbol{\\alpha})$ with concentration parameters $\\boldsymbol{\\alpha} = (\\alpha_{1}, \\ldots, \\alpha_{K})$. For a short clinical note of length $N$, with observed topic-assignment counts $\\boldsymbol{n} = (n_{1}, \\ldots, n_{K})$ from an inference procedure consistent with the LDA generative assumptions, the posterior over $\\theta_{d}$ is $\\operatorname{Dir}(\\alpha_{1} + n_{1}, \\ldots, \\alpha_{K} + n_{K})$. Assume $K$ topics. In the symmetric case, $\\alpha_{k} = \\alpha$ for all $k$, while in the asymmetric case, suppose one clinically dominant topic uses $\\alpha_{+}$ and all others use $\\alpha_{-}$.\n\nYou are to analyze how the choice of symmetric versus asymmetric $\\alpha$ affects sparsity in $\\theta_{d}$ for short clinical notes, using only fundamental properties of the Dirichlet distribution and its posterior under multinomial observations. As a concrete instantiation, consider a short clinical note with $N = 30$ tokens and $K = 20$ topics, where the inferred assignments yield $n_{1} = 28$ tokens in a single dominant topic and the remaining $2$ tokens across the other $K-1$ topics. Define “single-topic dominance” to mean that the expected posterior proportion of the dominant topic satisfies $\\mathbb{E}[\\theta_{d,1} \\mid \\boldsymbol{n}] \\geq 1 - \\delta$ with tolerance $\\delta = 0.1$.\n\n(a) Starting from core definitions of the Dirichlet distribution and its conjugacy with the multinomial, derive an explicit inequality in $\\alpha$ for the symmetric case that guarantees single-topic dominance as defined above. Then, solve this inequality to obtain a closed-form bound on $\\alpha$.\n\n(b) Briefly, and rigorously, explain how choosing an asymmetric prior with $\\alpha_{+} \\ll \\alpha_{-}$ or $\\alpha_{+} \\approx \\alpha_{-}$ influences sparsity in $\\theta_{d}$ for short notes under the same dominance definition, again using only Dirichlet posterior expectations.\n\nFinally, compute the maximal $\\alpha$ in the symmetric case that still guarantees single-topic dominance for the given $K$, $N$, $\\delta$, and $\\boldsymbol{n}$. Express your final numerical answer to four significant figures. The answer is unitless.", "solution": "The problem statement is evaluated for validity.\n\n### Step 1: Extract Givens\n-   **Model**: Latent Dirichlet Allocation (LDA).\n-   **Prior on Topic Proportions**: $\\theta_{d} \\sim \\operatorname{Dir}(\\boldsymbol{\\alpha})$, where $\\boldsymbol{\\alpha} = (\\alpha_{1}, \\ldots, \\alpha_{K})$.\n-   **Posterior on Topic Proportions**: Given observed topic counts $\\boldsymbol{n} = (n_{1}, \\ldots, n_{K})$, the posterior is $\\theta_{d} \\mid \\boldsymbol{n} \\sim \\operatorname{Dir}(\\alpha_{1} + n_{1}, \\ldots, \\alpha_{K} + n_{K})$.\n-   **Symmetric Prior**: $\\alpha_{k} = \\alpha$ for all $k \\in \\{1, \\ldots, K\\}$.\n-   **Asymmetric Prior**: One topic has prior parameter $\\alpha_{+}$, and the other $K-1$ topics have prior parameter $\\alpha_{-}$.\n-   **Document/Data Parameters**:\n    -   Document length: $N = 30$.\n    -   Number of topics: $K = 20$.\n    -   Observed topic counts: $n_{1} = 28$ for a single dominant topic, and $\\sum_{k=2}^{K} n_k = 2$ for the remaining $K-1$ topics.\n-   **Condition for Single-Topic Dominance**: $\\mathbb{E}[\\theta_{d,1} \\mid \\boldsymbol{n}] \\geq 1 - \\delta$.\n-   **Tolerance**: $\\delta = 0.1$.\n-   **Tasks**:\n    1.  (a) For the symmetric case, derive an inequality for $\\alpha$ that ensures single-topic dominance and solve for a closed-form bound on $\\alpha$.\n    2.  (b) Explain the influence of asymmetric priors ($\\alpha_{+} \\ll \\alpha_{-}$ or $\\alpha_{+} \\approx \\alpha_{-}$) on sparsity using only Dirichlet posterior expectations.\n    3.  Compute the maximal value of $\\alpha$ in the symmetric case that satisfies the dominance condition for the given parameters.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is based on the Dirichlet-multinomial conjugacy, a fundamental concept in Bayesian statistics and the mathematical core of LDA. All principles are well-established. The scenario is a standard application in bioinformatics and text analysis.\n-   **Well-Posed**: The problem is clearly defined. All necessary parameters ($N$, $K$, $\\boldsymbol{n}$, $\\delta$) and definitions (e.g., \"single-topic dominance\") are provided, allowing for a unique, computable solution. The question about the distribution of the remaining $2$ counts is irrelevant for the expectation of the first topic, as only their sum is needed, which is provided.\n-   **Objective**: The problem uses precise, quantitative language and avoids any subjective or ambiguous terminology.\n\n### Step 3: Verdict and Action\nThe problem is scientifically sound, self-contained, and well-posed. It does not violate any of the invalidity criteria. Therefore, the problem is deemed **valid**, and a full solution will be provided.\n\n***\n\nThe foundational principle required for this analysis is the formula for the expected value of a component of a random vector drawn from a Dirichlet distribution. If a $K$-dimensional random vector $\\boldsymbol{\\theta} = (\\theta_1, \\ldots, \\theta_K)$ follows a Dirichlet distribution with concentration parameters $\\boldsymbol{\\beta} = (\\beta_1, \\ldots, \\beta_K)$, denoted $\\boldsymbol{\\theta} \\sim \\operatorname{Dir}(\\boldsymbol{\\beta})$, then the expected value of the $i$-th component is given by:\n$$\n\\mathbb{E}[\\theta_i] = \\frac{\\beta_i}{\\sum_{k=1}^{K} \\beta_k}\n$$\n\nIn the context of LDA, the posterior distribution over the per-document topic proportions $\\theta_d$ for a document $d$, given the topic-word assignments which result in counts $\\boldsymbol{n} = (n_1, \\ldots, n_K)$, is also a Dirichlet distribution:\n$$\np(\\theta_d \\mid \\boldsymbol{n}, \\boldsymbol{\\alpha}) = \\operatorname{Dir}(\\boldsymbol{\\alpha} + \\boldsymbol{n}) = \\operatorname{Dir}(\\alpha_1 + n_1, \\ldots, \\alpha_K + n_K)\n$$\nThe total posterior concentration parameter is $\\sum_{k=1}^K (\\alpha_k + n_k) = (\\sum_{k=1}^K \\alpha_k) + (\\sum_{k=1}^K n_k) = \\alpha_0 + N$, where $\\alpha_0 = \\sum_{k=1}^K \\alpha_k$.\n\nThe posterior expectation of the proportion for topic $j$ is therefore:\n$$\n\\mathbb{E}[\\theta_{d,j} \\mid \\boldsymbol{n}, \\boldsymbol{\\alpha}] = \\frac{\\alpha_j + n_j}{\\alpha_0 + N}\n$$\n\n**(a) Derivation for the Symmetric Case**\n\nIn the symmetric case, the prior parameter is the same for all topics: $\\alpha_k = \\alpha$ for all $k \\in \\{1, \\ldots, K\\}$. The sum of prior parameters is $\\alpha_0 = \\sum_{k=1}^K \\alpha = K\\alpha$.\nThe posterior expectation for the dominant topic (topic $1$) is:\n$$\n\\mathbb{E}[\\theta_{d,1} \\mid \\boldsymbol{n}] = \\frac{\\alpha + n_1}{K\\alpha + N}\n$$\nThe problem defines \"single-topic dominance\" by the inequality $\\mathbb{E}[\\theta_{d,1} \\mid \\boldsymbol{n}] \\geq 1 - \\delta$. Substituting the expression for the expectation yields:\n$$\n\\frac{\\alpha + n_1}{K\\alpha + N} \\geq 1 - \\delta\n$$\nTo solve for $\\alpha$, we can multiply by the denominator $K\\alpha + N$. Since $\\alpha > 0$ (a requirement for Dirichlet parameters), $K > 0$, and $N > 0$, the denominator is strictly positive, so the inequality direction is preserved.\n$$\n\\alpha + n_1 \\geq (1 - \\delta)(K\\alpha + N)\n$$\n$$\n\\alpha + n_1 \\geq K\\alpha - K\\delta\\alpha + N - N\\delta\n$$\nWe now gather terms containing $\\alpha$ on one side of the inequality:\n$$\nn_1 - N + N\\delta \\geq K\\alpha - K\\delta\\alpha - \\alpha\n$$\n$$\nn_1 - N(1 - \\delta) \\geq \\alpha(K(1 - \\delta) - 1)\n$$\nAssuming the coefficient of $\\alpha$ is positive, which is $K(1-\\delta) - 1 > 0$ for the given parameters ($20(1 - 0.1) - 1 = 17 > 0$), we can divide to isolate $\\alpha$:\n$$\n\\alpha \\leq \\frac{n_1 - N(1 - \\delta)}{K(1 - \\delta) - 1}\n$$\nThis is the explicit inequality providing the closed-form bound on $\\alpha$ that guarantees single-topic dominance.\n\n**(b) Influence of Asymmetric Priors on Sparsity**\n\nSparsity in the topic proportion vector $\\theta_d$ implies that most of its components $\\theta_{d,k}$ are close to zero. We can analyze this property by examining the posterior expectations $\\mathbb{E}[\\theta_{d,k} \\mid \\boldsymbol{n}]$. The prior parameters $\\alpha_k$ act as \"pseudo-counts\" that are added to the observed data counts $n_k$. Small values of $\\alpha_k$ (e.g., $\\alpha_k  1$) specify a prior belief that the topic mixture is sparse, while large values ($\\alpha_k > 1$) specify a prior belief that the mixture is dense (i.e., multiple topics are significantly represented).\n\nThe posterior expectation for topic $k$ in the asymmetric case (with topic $1$ being the dominant one) is:\n$$\n\\mathbb{E}[\\theta_{d,k} \\mid \\boldsymbol{n}] = \\frac{\\alpha_k + n_k}{\\alpha_{+} + (K-1)\\alpha_{-} + N}\n$$\nwhere $\\alpha_1 = \\alpha_{+}$ and $\\alpha_k = \\alpha_{-}$ for $k > 1$.\n\nCase 1: $\\alpha_{+} \\ll \\alpha_{-}$. For a short note, many topics will have zero counts ($n_k = 0$ for $k > 1$). For such topics, the posterior expectation becomes $\\mathbb{E}[\\theta_{d,k} \\mid \\boldsymbol{n}] = \\frac{\\alpha_{-}}{\\alpha_{+} + (K-1)\\alpha_{-} + N}$. If $\\alpha_{-}$ is large, it inflates the posterior expectation for these unseen topics, moving them away from zero. This makes the posterior topic proportion vector $\\theta_d$ *denser* (less sparse) on average. The prior actively smooths the posterior distribution over the non-dominant topics.\n\nCase 2: $\\alpha_{+} \\approx \\alpha_{-}$. This regime approaches the symmetric case. Let $\\alpha_{+} \\approx \\alpha_{-} \\approx \\alpha$. The effect on sparsity is dictated by the magnitude of $\\alpha$. If $\\alpha$ is small (e.g., $\\alpha  1$), the pseudo-counts are minimal, and the posterior expectations $\\mathbb{E}[\\theta_{d,k}]$ are dominated by the data counts $n_k$. Since the data from a short note is typically sparse (many $n_k=0$), this choice of prior promotes a sparse posterior expectation vector. Conversely, if $\\alpha$ is large ($\\alpha > 1$), the large pseudo-counts ensure that all $\\mathbb{E}[\\theta_{d,k}]$ are non-negligible, promoting a dense posterior.\n\n**Final Computation**\n\nThe final task is to compute the maximal value of $\\alpha$ in the symmetric case that guarantees single-topic dominance. This value is given by the equality in the bound derived in part (a):\n$$\n\\alpha_{\\text{max}} = \\frac{n_1 - N(1 - \\delta)}{K(1 - \\delta) - 1}\n$$\nWe substitute the given numerical values: $N = 30$, $K = 20$, $n_1 = 28$, and $\\delta = 0.1$.\nThe term $(1 - \\delta)$ is $1 - 0.1 = 0.9$.\n\nThe numerator is:\n$$\nn_1 - N(1 - \\delta) = 28 - 30(0.9) = 28 - 27 = 1\n$$\nThe denominator is:\n$$\nK(1 - \\delta) - 1 = 20(0.9) - 1 = 18 - 1 = 17\n$$\nThus, the maximal value of $\\alpha$ is:\n$$\n\\alpha_{\\text{max}} = \\frac{1}{17}\n$$\nTo express this as a numerical value to four significant figures:\n$$\n\\alpha_{\\text{max}} = \\frac{1}{17} \\approx 0.0588235...\n$$\nRounding to four significant figures gives $0.05882$.", "answer": "$$\n\\boxed{0.05882}\n$$", "id": "4613917"}, {"introduction": "While the theory behind LDA is elegant, its practical application to massive corpora of clinical notes or scientific literature hinges on computational efficiency. This practice challenges you to analyze the time complexity of the standard collapsed Gibbs sampler and compare it to an optimized approach that leverages inherent data sparsity. By deriving and calculating the operational costs, you will gain insight into the algorithmic trade-offs essential for developing scalable and performant topic modeling pipelines in bioinformatics [@problem_id:4613954].", "problem": "You are analyzing Latent Dirichlet Allocation (LDA) with collapsed Gibbs sampling (GS) for topic modeling in a corpus composed of clinical notes and biomedical literature abstracts. Let there be a total of $N$ tokens across the corpus, a fixed number of $K$ topics, and a vocabulary of size $V$. In collapsed Gibbs sampling for LDA, the conditional distribution for each token’s topic assignment is computed using document-topic counts and topic-word counts, followed by sampling a new topic. The naive implementation evaluates the unnormalized probability for all $K$ topics for every token in the corpus in each iteration, which imposes a per-iteration computational cost that scales with $K$.\n\nFundamentally, the following well-tested facts apply:\n- In collapsed Gibbs sampling for LDA, updating the assignment of each token requires evaluating an unnormalized probability that depends on document-topic counts and topic-word counts, and then drawing a sample from the resulting categorical distribution over $K$ topics.\n- The naive implementation evaluates all $K$ topics per token, which implies a per-token cost that scales linearly with $K$.\n- In typical corpora such as clinical notes and biomedical abstracts, sparsity arises because each document uses a small subset of topics and each word appears in a small subset of topics. Define $s_d$ as the average number of nonzero document-topic counts encountered per token’s document, and $s_w$ as the average number of nonzero topic-word counts encountered per token’s word. Empirically, $s_d$ and $s_w$ are often much smaller than $K$.\n\nYour tasks are:\n1. Starting from the above fundamental facts, reason from first principles to derive a symbolic expression for the per-iteration operation count of the naive collapsed Gibbs sampler in terms of $N$, $K$, and a per-candidate evaluation constant $c_{\\text{cand}}$. Your derivation should carefully justify the linear dependence on $K$ and explain the role of $c_{\\text{cand}}$.\n2. Propose and theoretically justify an optimization that exploits sparsity via a decomposition into sparse components plus a dense background component. Assume the dense component can be sampled in expected constant time using a Walker alias method (also known as alias tables), which must be built once per iteration at a cost linear in $K$. Let $c_{\\text{bucket}}$ be the constant number of operations to combine the sparse components per token, $c_{\\text{alias}}$ be the expected constant number of operations to sample from the alias table per token, and $c_{\\text{build}}$ be the constant number of operations to build the alias table per topic per iteration. Derive a symbolic expression for the per-iteration operation count of this sparse-plus-alias optimization in terms of $N$, $K$, $s_d$, $s_w$, $c_{\\text{cand}}$, $c_{\\text{bucket}}$, $c_{\\text{alias}}$, and $c_{\\text{build}}$.\n3. Implement a complete, runnable program that computes, for each test case in the suite below, the following three quantities:\n   - The naive per-iteration operation count $C_{\\text{naive}}$.\n   - The optimized per-iteration operation count $C_{\\text{opt}}$ when using the sparse-plus-alias method.\n   - The speedup ratio $R = \\frac{C_{\\text{naive}}}{C_{\\text{opt}}}$ expressed as a decimal (not a percentage).\n4. Use the following fixed constant values in your computations: $c_{\\text{cand}} = 1$, $c_{\\text{bucket}} = 3$, $c_{\\text{alias}} = 1$, and $c_{\\text{build}} = 2$. All counts are scalar operation counts and therefore dimensionless.\n5. The test suite consists of the following parameter sets $(N, K, s_d, s_w)$, designed to cover diverse regimes:\n   - Case $1$ (typical clinical notes scale): $(10^6, 100, 5, 10)$.\n   - Case $2$ (large topic inventory, sparse usage): $(10^5, 10^3, 3, 3)$.\n   - Case $3$ (single topic edge case): $(10^4, 1, 1, 1)$.\n   - Case $4$ (no sparsity boundary condition): $(5 \\cdot 10^5, 200, 200, 200)$.\n6. Your program must compute the three quantities in item $3$ for each test case and produce a single line of output that aggregates the results for all cases as a comma-separated list enclosed in square brackets. Each case’s result must itself be a list of three decimal numbers $[C_{\\text{naive}}, C_{\\text{opt}}, R]$, with each decimal rounded to six places. The final output format should be:\n   - Example: $[[x_1,y_1,z_1],[x_2,y_2,z_2],[x_3,y_3,z_3],[x_4,y_4,z_4]]$.\n\nEnsure scientific realism by justifying the decomposition and the use of the alias method as a constant-time sampler after paying a one-time per-iteration build cost. The problem must be mathematically framed and solvable without external data, adhering to the advanced graduate level in bioinformatics and medical data analytics focused on topic modeling for clinical notes and literature.", "solution": "The problem has been validated and is determined to be a well-posed, scientifically grounded problem in the field of bioinformatics and medical data analytics. It requires the derivation and application of computational complexity models for Latent Dirichlet Allocation (LDA) samplers.\n\n### Part 1: Derivation of Naive Collapsed Gibbs Sampler Operation Count\n\nThe naive implementation of a collapsed Gibbs sampler for LDA performs one full pass over the corpus for each iteration. A single iteration consists of updating the topic assignment for every token in the corpus.\n\nLet $N$ be the total number of tokens in the corpus and $K$ be the number of topics.\nFor each of the $N$ tokens, the sampler must compute the conditional probability of that token belonging to each of the $K$ possible topics. This involves calculating an unnormalized probability score for each of the $K$ candidates.\nThe problem defines $c_{\\text{cand}}$ as a constant representing the number of elementary operations required to evaluate one such candidate probability. This constant encapsulates operations like fetching counts from memory, performing multiplications and divisions, as specified by the LDA conditional probability formula:\n$$P(z_i=k | \\mathbf{z}_{\\neg i}, \\mathbf{w}) \\propto (n_{d,k}^{\\neg i} + \\alpha) \\frac{n_{k,w}^{\\neg i} + \\beta}{n_k^{\\neg i} + V\\beta}$$\nwhere $z_i$ is the topic assignment for the $i$-th token, $d$ is its document, $w$ is its word type, $n$ are various counts, and $\\alpha, \\beta$ are hyperparameters. The computation of this expression for a single topic $k$ corresponds to the cost $c_{\\text{cand}}$.\n\nThe total operational cost for a single token, $C_{\\text{token}}$, is the cost of evaluating all $K$ topics:\n$$C_{\\text{token}} = K \\cdot c_{\\text{cand}}$$\n\nThe total operational cost for one full iteration of the sampler, $C_{\\text{naive}}$, is the cost per token multiplied by the total number of tokens, $N$:\n$$C_{\\text{naive}} = N \\cdot C_{\\text{token}}$$\nSubstituting the expression for $C_{\\text{token}}$, we arrive at the symbolic expression for the per-iteration operation count of the naive sampler:\n$$C_{\\text{naive}} = N \\cdot K \\cdot c_{\\text{cand}}$$\nThis derivation clearly shows the linear dependence of the cost on both $N$ and $K$.\n\n### Part 2: Derivation of Sparse-Plus-Alias Optimized Sampler Operation Count\n\nThe optimization leverages the inherent sparsity in topic models of text corpora: any given document is typically about a few topics, and any given word is strongly associated with a few topics. This means that for a token $i$ (word type $w$ in document $d$), the counts $n_{d,k}$ and $n_{k,w}$ are non-zero for only a small subset of the $K$ topics. The full conditional probability distribution can be decomposed into sparse components (where these counts are non-zero) and a dense background component (driven by the smoothing hyperparameters $\\alpha$ and $\\beta$).\n\nThe total per-iteration cost of the optimized sampler, $C_{\\text{opt}}$, is the sum of a one-time setup cost and the cumulative cost of sampling all $N$ tokens.\n\n**Per-Iteration Setup Cost:**\nThe dense background component of the probability distribution is proportional to a term that depends only on topic-level counts (e.g., $(\\sum_v n_{k,v} + V\\beta)^{-1}$). This distribution over all $K$ topics can be sampled efficiently if a suitable data structure is prepared. The problem specifies using the Walker alias method. Building an alias table for a categorical distribution with $K$ outcomes has a computational cost that is linear in $K$. Let $c_{\\text{build}}$ be the constant number of operations required per topic to construct the table. The total one-time setup cost per iteration is:\n$$C_{\\text{build\\_iter}} = K \\cdot c_{\\text{build}}$$\n\n**Per-Token Sampling Cost:**\nFor each of the $N$ tokens, the sampler performs a series of operations.\n1.  **Sparse Component Evaluation:** Instead of evaluating all $K$ topics, we only explicitly evaluate the topics for which the document-topic count ($n_{d,k}$) or the topic-word count ($n_{k,w}$) is non-zero. The problem provides $s_d$ as the average number of non-zero document-topic counts and $s_w$ as the average number of non-zero topic-word counts encountered for a token. The total number of sparse candidates to evaluate is therefore, on average, $s_d + s_w$. (We assume additivity for simplicity, as is common in such complexity analyses, ignoring potential overlap between the two sets of topics). With a cost of $c_{\\text{cand}}$ per candidate, the cost for this step is $(s_d + s_w) \\cdot c_{\\text{cand}}$.\n\n2.  **Component Combination:** After evaluating the sparse components, their contribution to the total probability mass must be calculated and combined with the mass of the dense component. The problem abstracts the cost of this logic (e.g., managing data structures for the sparse probabilities, summing them, and preparing for the final sampling decision) into a single constant cost, $c_{\\text{bucket}}$.\n\n3.  **Sampling Step:** The final step is to draw a new topic from the combined (mixture) distribution. This involves deciding whether to sample from the sparse set or the dense set, and then performing the sample. The problem simplifies this complex step by stating that sampling from the dense component via the alias table incurs an expected constant cost of $c_{\\text{alias}}$. This cost is understood to represent the final effective sampling operation per token in this optimized scheme.\n\nSumming these costs gives the total average operational cost per token, $C_{\\text{token\\_opt}}$:\n$$C_{\\text{token\\_opt}} = (s_d + s_w) \\cdot c_{\\text{cand}} + c_{\\text{bucket}} + c_{\\text{alias}}$$\n\n**Total Optimized Cost:**\nThe total per-iteration operation count for the optimized sampler, $C_{\\text{opt}}$, is the sum of the initial build cost and the total cost for sampling all $N$ tokens:\n$$C_{\\text{opt}} = C_{\\text{build\\_iter}} + N \\cdot C_{\\text{token\\_opt}}$$\nSubstituting the derived expressions, we obtain the final symbolic formula:\n$$C_{\\text{opt}} = K \\cdot c_{\\text{build}} + N \\cdot ((s_d + s_w) \\cdot c_{\\text{cand}} + c_{\\text{bucket}} + c_{\\text{alias}})$$\nThis expression is valid when $s_d, s_w \\ll K$, where the per-token cost becomes approximately constant or grows much slower than $K$, leading to significant speedup over the naive method. If sparsity is not present (i.e., $s_d, s_w \\approx K$), the overhead of this method makes it less efficient than the naive approach.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and prints the operational costs and speedup for naive and \n    optimized LDA Gibbs samplers based on derived complexity formulas.\n    \"\"\"\n\n    # Define the fixed constant values from the problem statement.\n    # c_cand: per-candidate evaluation constant\n    # c_bucket: constant operations to combine sparse components per token\n    # c_alias: expected constant operations to sample from alias table per token\n    # c_build: constant operations to build alias table per topic per iteration\n    c_cand = 1.0\n    c_bucket = 3.0\n    c_alias = 1.0\n    c_build = 2.0\n\n    # Define the test suite of parameter sets (N, K, s_d, s_w).\n    # N: total number of tokens\n    # K: number of topics\n    # s_d: average number of nonzero document-topic counts\n    # s_w: average number of nonzero topic-word counts\n    test_cases = [\n        (10**6, 100, 5, 10),      # Case 1: typical clinical notes scale\n        (10**5, 10**3, 3, 3),      # Case 2: large topic inventory, sparse usage\n        (10**4, 1, 1, 1),          # Case 3: single topic edge case\n        (5 * 10**5, 200, 200, 200), # Case 4: no sparsity boundary condition\n    ]\n\n    # A list to store the results for each test case.\n    results = []\n\n    for case in test_cases:\n        N, K, s_d, s_w = case\n\n        # Convert integers to floats for calculations to ensure float division.\n        N, K, s_d, s_w = float(N), float(K), float(s_d), float(s_w)\n\n        # 1. Calculate the naive per-iteration operation count (C_naive).\n        # Formula: C_naive = N * K * c_cand\n        c_naive = N * K * c_cand\n\n        # 2. Calculate the optimized per-iteration operation count (C_opt).\n        # Formula: C_opt = K * c_build + N * ((s_d + s_w) * c_cand + c_bucket + c_alias)\n        per_token_cost_opt = (s_d + s_w) * c_cand + c_bucket + c_alias\n        c_opt = K * c_build + N * per_token_cost_opt\n        \n        # 3. Calculate the speedup ratio R.\n        # Formula: R = C_naive / C_opt\n        # Handle division by zero, although not expected with the given formulas/inputs.\n        if c_opt == 0:\n            speedup_ratio = float('inf')\n        else:\n            speedup_ratio = c_naive / c_opt\n\n        # Round the results to six decimal places as required.\n        c_naive_rounded = round(c_naive, 6)\n        c_opt_rounded = round(c_opt, 6)\n        speedup_ratio_rounded = round(speedup_ratio, 6)\n\n        # Append the formatted result for the current case.\n        # Required format for each case is a list of three decimal numbers.\n        results.append(\n            [c_naive_rounded, c_opt_rounded, speedup_ratio_rounded]\n        )\n\n    # Format the final output string according to the problem specification.\n    # We build the string manually to ensure correct formatting with six decimal places,\n    # including trailing zeros, as requested.\n    output_parts = []\n    for res_naive, res_opt, res_ratio in results:\n        output_parts.append(f\"[{res_naive:.6f},{res_opt:.6f},{res_ratio:.6f}]\")\n    \n    # Final print statement in the exact required format: [[...],[...],...]\n    print(f\"[{','.join(output_parts)}]\")\n\n\nsolve()\n```", "id": "4613954"}]}