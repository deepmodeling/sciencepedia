{"hands_on_practices": [{"introduction": "At the heart of every Manhattan plot is the transformation of a statistical test result into a visual data point. This first practice deconstructs this fundamental process, guiding you from the raw output of a genetic association model—the effect size estimate ($\\hat{\\beta}$) and its standard error ($\\mathrm{SE}$)—to the final plotted y-axis value. By working through the calculation of a $p$-value and its subsequent $-\\log_{10}$ transformation, you will solidify your understanding of how statistical evidence against the null hypothesis of no association is quantified and visualized in a Genome-Wide Association Study (GWAS) [@problem_id:4580204].", "problem": "A Genome-Wide Association Study (GWAS) typically summarizes single-nucleotide polymorphism (SNP) association results using Manhattan plots of transformed $p$-values and Quantile-Quantile (QQ) plots to assess the calibration of test statistics under the null hypothesis. Consider a single SNP analyzed via a generalized linear model, where the maximum likelihood estimator $\\hat{\\beta}$ of the genetic effect parameter $\\beta$ is obtained along with its standard error $\\mathrm{SE}$.\n\nStarting from the large-sample theory for maximum likelihood estimators, assume that under standard regularity conditions and the null hypothesis of no association ($\\beta=0$), the standardized test statistic formed from $\\hat{\\beta}$ and $\\mathrm{SE}$ follows a standard normal distribution. Using this fact and the definition of a two-sided test for $\\beta=0$, derive the relationship between the standardized test statistic and the pair $(\\hat{\\beta}, \\mathrm{SE})$, and then express the two-sided $p$-value in terms of this statistic and the standard normal cumulative distribution function.\n\nFinally, compute the Manhattan plot ordinate $-\\log_{10}(p)$ for $\\hat{\\beta}=0.05$ and $\\mathrm{SE}=0.01$ under a two-sided test. Round your final value of $-\\log_{10}(p)$ to four significant figures. Express the final answer as a pure number without units.", "solution": "The user has provided a problem from the field of bioinformatics and statistical genetics, asking for the derivation of a test statistic and $p$-value under the framework of a Genome-Wide Association Study (GWAS), followed by a specific calculation.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- The analysis involves a single-nucleotide polymorphism (SNP) and a generalized linear model.\n- The maximum likelihood estimator of the genetic effect is $\\hat{\\beta}$, with standard error $\\mathrm{SE}$.\n- Under the null hypothesis $H_0: \\beta=0$ and large-sample theory, a standardized test statistic follows a standard normal distribution, $N(0, 1)$.\n- The test is a two-sided test for $\\beta=0$.\n- Specific values for computation are $\\hat{\\beta}=0.05$ and $\\mathrm{SE}=0.01$.\n- The desired output is the Manhattan plot ordinate, $-\\log_{10}(p)$, rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly based on the well-established principles of maximum likelihood theory and hypothesis testing (specifically, the Wald test), which are fundamental to statistical genetics and GWAS analysis. The assumptions (asymptotic normality of MLEs) are standard.\n- **Well-Posed:** The problem is clearly defined, providing all necessary information and a precise objective. The path from the theoretical premise to the numerical calculation is unambiguous and leads to a unique solution.\n- **Objective:** The language is formal and precise, devoid of subjectivity.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid as it is scientifically sound, well-posed, and objective. A detailed solution will be provided.\n\n**Derivation and Solution**\n\nThe problem starts with the premise that for a large sample size, the maximum likelihood estimator $\\hat{\\beta}$ is approximately normally distributed. The distribution is centered on the true parameter value $\\beta$, and its variance is the square of the standard error, $\\mathrm{SE}^2$.\nMathematically, this is expressed as:\n$$\n\\hat{\\beta} \\approx N(\\beta, \\mathrm{SE}^2)\n$$\nThe null hypothesis ($H_0$) posits no genetic association, which corresponds to the parameter $\\beta$ being equal to zero, i.e., $H_0: \\beta=0$. Under this hypothesis, the distribution of the estimator becomes:\n$$\n\\hat{\\beta} \\mid H_0 \\approx N(0, \\mathrm{SE}^2)\n$$\nTo form a standardized test statistic, we subtract the mean under the null hypothesis (which is $0$) and divide by the standard deviation (which is the standard error, $\\mathrm{SE}$). Let this standardized test statistic be denoted by $Z$.\nThe relationship between the test statistic and the pair $(\\hat{\\beta}, \\mathrm{SE})$ is therefore:\n$$\nZ = \\frac{\\hat{\\beta} - 0}{\\mathrm{SE}} = \\frac{\\hat{\\beta}}{\\mathrm{SE}}\n$$\nAs stated in the problem and confirmed by the properties of the normal distribution, this $Z$ statistic follows a standard normal distribution under the null hypothesis:\n$$\nZ \\mid H_0 \\sim N(0, 1)\n$$\nThe next step is to define the $p$-value for a two-sided test. A two-sided test considers evidence against the null hypothesis in either direction ($\\beta  0$ or $\\beta  0$). The $p$-value is the probability of observing a test statistic at least as extreme as the one calculated from the data, assuming $H_0$ is true. For a two-sided test, \"at least as extreme\" corresponds to the absolute value of the statistic.\n\nLet $z_{obs}$ be the observed value of the test statistic calculated from our data. The $p$-value is given by:\n$$\np = P(|Z| \\ge |z_{obs}| \\mid H_0)\n$$\nwhere $Z \\sim N(0, 1)$. This can be expanded as:\n$$\np = P(Z \\ge |z_{obs}|) + P(Z \\le -|z_{obs}|)\n$$\nDue to the symmetry of the standard normal distribution about $0$, we have $P(Z \\le -|z_{obs}|) = P(Z \\ge |z_{obs}|)$. Therefore, the expression for the $p$-value simplifies to:\n$$\np = 2 \\times P(Z \\ge |z_{obs}|)\n$$\nLet $\\Phi(\\cdot)$ denote the cumulative distribution function (CDF) of the standard normal distribution, i.e., $\\Phi(x) = P(Z \\le x)$. The tail probability $P(Z \\ge |z_{obs}|)$ can be expressed in terms of the CDF as $1 - \\Phi(|z_{obs}|)$.\nSubstituting this into the expression for the $p$-value gives the final relationship:\n$$\np = 2 \\times (1 - \\Phi(|z_{obs}|))\n$$\nNow, we can proceed with the calculation using the provided values: $\\hat{\\beta}=0.05$ and $\\mathrm{SE}=0.01$.\n\nFirst, we calculate the observed value of the test statistic, $z_{obs}$:\n$$\nz_{obs} = \\frac{\\hat{\\beta}}{\\mathrm{SE}} = \\frac{0.05}{0.01} = 5\n$$\nNext, we calculate the two-sided $p$-value using the formula derived above:\n$$\np = 2 \\times (1 - \\Phi(5))\n$$\nThe value of $\\Phi(5)$ is extremely close to $1$. Using a high-precision calculator for the standard normal CDF, we find that the one-sided tail probability $P(Z  5)$ is approximately $2.8665157 \\times 10^{-7}$.\nTherefore, the $p$-value is:\n$$\np = 2 \\times P(Z \\ge 5) \\approx 2 \\times (2.8665157 \\times 10^{-7}) = 5.7330314 \\times 10^{-7}\n$$\nFinally, we compute the Manhattan plot ordinate, which is defined as $-\\log_{10}(p)$:\n$$\n-\\log_{10}(p) = -\\log_{10}(5.7330314 \\times 10^{-7})\n$$\nUsing the properties of logarithms, $\\log_{10}(a \\times 10^b) = \\log_{10}(a) + b$:\n$$\n-\\log_{10}(p) = -(\\log_{10}(5.7330314) + \\log_{10}(10^{-7})) = -(\\log_{10}(5.7330314) - 7) = 7 - \\log_{10}(5.7330314)\n$$\nCalculating the logarithm:\n$$\n\\log_{10}(5.7330314) \\approx 0.758384\n$$\nSo, the ordinate is:\n$$\n-\\log_{10}(p) \\approx 7 - 0.758384 = 6.241616\n$$\nThe problem requires rounding this final value to four significant figures. The first four significant figures are $6, 2, 4, 1$. The fifth significant figure is $6$, which is greater than or equal to $5$, so we round up the fourth digit.\n$$\n6.241616 \\rightarrow 6.242\n$$\nThe computed Manhattan plot ordinate is $6.242$.", "answer": "$$\\boxed{6.242}$$", "id": "4580204"}, {"introduction": "After calculating a $p$-value for a single variant, the next crucial step is to determine if it is statistically significant in the context of millions of tests performed across the genome. A nominal $p$-value threshold is insufficient due to the massive multiple testing burden. This exercise will lead you through the derivation of the Bonferroni correction, the principle that underpins the conventional genome-wide significance threshold, and contrasts it with the Šidák correction to explore the impact of simplifying assumptions about test independence [@problem_id:4580282]. Mastering this concept is essential for correctly interpreting the significance lines on a Manhattan plot and avoiding false positive claims.", "problem": "A consortium conducts a large multi-cohort Genome-Wide Association Study (GWAS) on a complex trait, aggregating imputed genotypes across platforms. After quality control, the final association scan evaluates $M = 7.5 \\times 10^{6}$ single nucleotide polymorphisms (SNPs). To interpret the Manhattan plot and the Quantile-Quantile (QQ) plot under family-wise error control, the study targets a family-wise error rate (FWER) of $\\alpha = 0.05$ across all $M$ hypothesis tests.\n\nStarting from first principles appropriate for multiple testing in bioinformatics and medical data analytics:\n- Use the definition of the family-wise error rate as the probability of at least one false rejection across $M$ null hypotheses.\n- Under no distributional assumptions beyond identically distributed marginal $p$-values, derive the per-test threshold that ensures FWER control by applying the union bound.\n- Under the assumption that the $M$ tests are mutually independent, derive the exact per-test threshold that ensures FWER control.\n\nThen, for the given $M$ and $\\alpha$:\n1. Compute the Bonferroni per-test $p$-value threshold implied by the union bound.\n2. Compute the Šidák per-test $p$-value threshold implied by independence.\n3. Compute the multiplicative difference $\\rho$ defined as the ratio of the Šidák threshold to the Bonferroni threshold.\n\nFinally, briefly justify, in terms of linkage disequilibrium among SNPs, which of the two thresholds is more conservative when interpreting Manhattan and QQ plots in realistic GWAS with correlated markers.\n\nRound every reported numerical quantity to four significant figures and express $p$-value thresholds in standard base-$10$ scientific notation. No physical units are required.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard problem in statistical genetics and can be solved using established principles of multiple hypothesis testing.\n\nLet $M$ be the total number of single nucleotide polymorphisms (SNPs) tested, which is $M = 7.5 \\times 10^{6}$. Let $\\alpha$ be the desired family-wise error rate (FWER), given as $\\alpha = 0.05$. For each SNP $i \\in \\{1, 2, \\dots, M\\}$, we test a null hypothesis $H_{0i}$ that the SNP is not associated with the trait. The result of each test is a $p$-value, $P_i$.\n\nThe family-wise error rate (FWER) is defined as the probability of committing at least one Type I error (a false rejection of a true null hypothesis) among all $M$ tests. Let $E_i$ be the event of a Type I error for the $i$-th test. Assuming the worst-case scenario where all null hypotheses are true, the FWER is given by:\n$$\n\\text{FWER} = P\\left(\\bigcup_{i=1}^{M} E_i\\right)\n$$\nWe seek a per-test significance threshold, let's call it $p_{th}$, such that we reject $H_{0i}$ if $P_i \\le p_{th}$. Under the null hypothesis, $p$-values are uniformly distributed on $[0, 1]$, so the probability of a Type I error for a single test is $P(E_i) = P(P_i \\le p_{th}) = p_{th}$.\n\n**Derivation of the Bonferroni Threshold (Union Bound)**\n\nThe Bonferroni correction is derived using Boole's inequality, also known as the union bound. This inequality states that for any collection of events, the probability of their union is less than or equal to the sum of their individual probabilities. This derivation makes no assumptions about the dependence structure of the tests.\n$$\n\\text{FWER} = P\\left(\\bigcup_{i=1}^{M} E_i\\right) \\le \\sum_{i=1}^{M} P(E_i)\n$$\nSubstituting $P(E_i) = p_{th}$:\n$$\n\\text{FWER} \\le \\sum_{i=1}^{M} p_{th} = M \\cdot p_{th}\n$$\nTo ensure that the FWER is controlled at level $\\alpha$, we enforce the condition:\n$$\nM \\cdot p_{th} \\le \\alpha\n$$\nThis yields the Bonferroni per-test threshold, $p_{Bonf}$:\n$$\np_{Bonf} = \\frac{\\alpha}{M}\n$$\n\n**Derivation of the Šidák Threshold (Independence Assumption)**\n\nThe Šidák correction assumes that all $M$ tests are mutually independent. Under this assumption, the probability of the union of events can be calculated exactly. The probability of making *no* Type I errors is the product of the probabilities of not making a Type I error in each independent test:\n$$\nP(\\text{no Type I errors}) = P\\left(\\bigcap_{i=1}^{M} E_i^c\\right) = \\prod_{i=1}^{M} P(E_i^c)\n$$\nwhere $E_i^c$ is the complement of event $E_i$. The probability of not committing a Type I error for a single test is $P(E_i^c) = 1 - P(E_i) = 1 - p_{th}$.\n$$\nP(\\text{no Type I errors}) = \\prod_{i=1}^{M} (1 - p_{th}) = (1 - p_{th})^M\n$$\nThe FWER is the probability of at least one Type I error, which is $1$ minus the probability of no Type I errors:\n$$\n\\text{FWER} = 1 - P(\\text{no Type I errors}) = 1 - (1 - p_{th})^M\n$$\nTo control the FWER exactly at level $\\alpha$, we set:\n$$\n1 - (1 - p_{th})^M = \\alpha\n$$\nSolving for $p_{th}$ gives the Šidák per-test threshold, $p_{Sidak}$:\n$$\n(1 - p_{Sidak})^M = 1 - \\alpha\n$$\n$$\n1 - p_{Sidak} = (1 - \\alpha)^{1/M}\n$$\n$$\np_{Sidak} = 1 - (1 - \\alpha)^{1/M}\n$$\n\n**Numerical Calculations**\n\nGiven $M = 7.5 \\times 10^{6}$ and $\\alpha = 0.05$.\n\n1.  **Bonferroni per-test $p$-value threshold ($p_{Bonf}$):**\n    $$\n    p_{Bonf} = \\frac{\\alpha}{M} = \\frac{0.05}{7.5 \\times 10^{6}} = \\frac{5 \\times 10^{-2}}{7.5 \\times 10^{6}} = \\frac{2}{3} \\times 10^{-8} \\approx 6.6667 \\times 10^{-9}\n    $$\n    Rounding to four significant figures, $p_{Bonf} = 6.667 \\times 10^{-9}$.\n\n2.  **Šidák per-test $p$-value threshold ($p_{Sidak}$):**\n    $$\n    p_{Sidak} = 1 - (1 - 0.05)^{1/(7.5 \\times 10^{6})} = 1 - (0.95)^{1/(7.5 \\times 10^{6})}\n    $$\n    To compute this, we can use the approximation $1 - \\exp(x) \\approx -x$ for small $x$. Let's evaluate the exponent:\n    $$\n    \\frac{1}{M} \\ln(1-\\alpha) = \\frac{1}{7.5 \\times 10^{6}} \\ln(0.95) \\approx \\frac{-0.05129329}{7.5 \\times 10^{6}} \\approx -6.8391059 \\times 10^{-9}\n    $$\n    So, $p_{Sidak} = 1 - \\exp\\left(\\frac{\\ln(0.95)}{7.5 \\times 10^{6}}\\right) \\approx 1 - \\exp(-6.8391059 \\times 10^{-9})$.\n    Using the approximation $p_{Sidak} \\approx -(-6.8391059 \\times 10^{-9}) = 6.8391059 \\times 10^{-9}$.\n    Rounding to four significant figures, $p_{Sidak} = 6.839 \\times 10^{-9}$.\n\n3.  **Multiplicative difference ($\\rho$):**\n    $$\n    \\rho = \\frac{p_{Sidak}}{p_{Bonf}} = \\frac{6.8391059 \\times 10^{-9}}{6.666666... \\times 10^{-9}} \\approx 1.0258659\n    $$\n    Rounding to four significant figures, $\\rho = 1.026$. This shows that the Šidák threshold is slightly less strict (larger) than the Bonferroni threshold.\n\n**Justification of Conservatism in GWAS**\n\nIn the context of a realistic GWAS, the **Bonferroni threshold is more conservative** than the Šidák threshold. A more conservative threshold is a stricter (i.e., smaller) one, which makes it harder to declare a result as statistically significant, thereby reducing the number of findings.\n\nOur calculations show that $p_{Bonf}  p_{Sidak}$. This mathematical relationship holds true for any $M  1$ and $\\alpha \\in (0, 1)$, meaning the Bonferroni threshold is always numerically smaller and thus more stringent.\n\nThe justification in terms of linkage disequilibrium (LD) is as follows:\n-   **Linkage Disequilibrium (LD)** refers to the non-random association of alleles at different loci. In GWAS, SNPs that are physically close on a chromosome are often inherited together, meaning their genotypes (and thus their test statistics) are positively correlated.\n-   The **Šidák correction** explicitly assumes that the $M$ tests are independent. The presence of LD directly violates this assumption.\n-   The **Bonferroni correction**, based on the union bound, makes no assumptions about dependence. It is a universal controller of the FWER. However, because it accounts for the worst-case scenario of dependence, it is known to be overly conservative, especially when tests are positively correlated, as is the case for SNPs in LD. The positive correlation means the effective number of independent tests is much smaller than $M$.\n-   Both methods are, in fact, too conservative for a typical GWAS dataset with extensive LD. However, the question asks which of the two is *more* conservative. Since $p_{Bonf}$ is always smaller than $p_{Sidak}$, and this relationship is not altered by the presence of correlation, the Bonferroni correction remains the more conservative approach. It over-corrects for the raw number of tests ($M$) without any adjustment for the redundancy introduced by LD, resulting in the strictest criterion for significance between the two.", "answer": "$$ \\boxed{ \\begin{pmatrix} 6.667 \\times 10^{-9}  6.839 \\times 10^{-9}  1.026 \\end{pmatrix} } $$", "id": "4580282"}, {"introduction": "Beyond identifying true associations, Manhattan and QQ plots are indispensable diagnostic tools for assessing data quality. Real-world GWAS data is often affected by technical artifacts, such as genotyping errors, which can generate spurious signals that mimic true genetic effects. This practice places you in a realistic scenario where you must act as a data detective, using the patterns in the plots to diagnose a problem and applying principles like the Hardy–Weinberg Equilibrium (HWE) test to design an effective quality control filter [@problem_id:4580225]. This skill is vital for ensuring the robustness and reproducibility of GWAS findings.", "problem": "A case-control genome-wide association study (GWAS) of a binary disease includes $n_{\\text{cases}} = 20{,}000$ cases and $n_{\\text{controls}} = 30{,}000$ controls genotyped on a high-density array, followed by imputation. The Manhattan plot shows a dense cluster of highly significant single-nucleotide polymorphisms (SNPs) on one segment of chromosome $8$, while the quantile-quantile (QQ) plot shows early departure from the null with genomic control (GC) inflation factor $\\lambda_{\\text{GC}} \\approx 1.15$. Variants in the suspicious region have lower imputation quality and exhibit excess heterozygosity among controls. You are told that across the genome there are $m = 8 \\times 10^{6}$ common variants with minor allele frequency (MAF) $\\ge 0.01$ and an additional $m_{\\text{rare}} = 12 \\times 10^{6}$ variants with MAF $ 0.01$.\n\nStart from the fundamental definition of Hardy–Weinberg equilibrium (HWE): for a biallelic locus with allele frequency $p$ for the reference allele and $q = 1 - p$ for the alternative allele in a randomly mating, non-stratified population without selection, mutation, or migration, the expected genotype frequencies are $p^{2}$, $2pq$, and $q^{2}$. The HWE test evaluates whether observed genotype counts deviate from these expectations; under the null of HWE, the HWE test $p$-values are uniformly distributed on $\\left[0,1\\right]$. In GWAS, spurious association peaks in Manhattan plots and inflation in QQ plots can arise from systematic genotyping or imputation artifacts that induce HWE deviations and correlate spuriously with case-control status (for example, batch effects that differ between cases and controls).\n\nWhich of the following strategies most appropriately explains how Hardy–Weinberg equilibrium violations can generate spurious Manhattan peaks and designs a scientifically sound HWE-based variant filter to reduce such artifacts while preserving true associations? Additionally, under your chosen filter, state the expected number of common variants removed under the null model of perfect HWE, using the fact that under the null the HWE $p$-values are uniform.\n\nA. Compute HWE $p$-values in all samples combined and remove any variant with HWE $p  10^{-2}$ genome-wide. This will aggressively suppress spurious peaks without affecting true signals. The expected number of common variants removed under the null is $8 \\times 10^{4}$.\n\nB. Compute HWE $p$-values in controls only, stratified within each ancestry group, and filter common variants (MAF $\\ge 0.01$) with HWE $p  10^{-6}$ in any stratum; for rare variants (MAF $ 0.01$), do not apply an HWE filter and instead rely on orthogonal quality metrics. This targets artifacts that cause heterozygote excess or deficit in controls and avoids removing true disease signals that can deviate from HWE in cases. The expected number of common variants removed under the null is $8$.\n\nC. Compute HWE $p$-values in cases only and remove any variant with HWE $p  10^{-6}$ genome-wide. This preserves power and avoids removing causal variants. The expected number of common variants removed under the null is $8$.\n\nD. Do not use HWE filtering, because the QQ inflation ($\\lambda_{\\text{GC}} \\approx 1.15$) indicates a highly polygenic trait. Instead, rely on a stricter Bonferroni threshold for association. The expected number of common variants removed under the null is $0$.\n\nSelect the single best option.", "solution": "We begin from the Hardy–Weinberg equilibrium (HWE) definition. For a biallelic locus with allele frequencies $p$ and $q = 1 - p$, the expected genotype proportions in an ideal randomly mating population are $p^{2}$ (homozygous reference), $2pq$ (heterozygous), and $q^{2}$ (homozygous alternate). The HWE test compares observed genotype counts to these expectations and produces a $p$-value that, under the null hypothesis of HWE, is uniformly distributed on $\\left[0,1\\right]$. Therefore, if one uses an HWE $p$-value threshold $\\alpha$ as a quality-control filter under the null, the expected fraction of variants removed is $\\alpha$. With $m$ independent common variants, the expected number removed is $m \\alpha$.\n\nLinking HWE to Manhattan and quantile-quantile (QQ) plots: a Manhattan plot summarizes association $p$-values across the genome; a QQ plot compares the distribution of observed association test statistics to their null expectations. Systematic genotyping or imputation errors can create departures from HWE that are not due to true biological effects. For example, allele-calling bias can cause heterozygote deficit or excess, and batch effects that differ between cases and controls can couple such biases to phenotype. This can inflate association statistics and create clusters of low $p$-values (spurious peaks) in regions with lower genotype quality. In QQ plots, this arises as early inflation beyond what is expected from polygenicity alone, especially when the inflation co-occurs with quality issues and HWE deviations concentrated in specific regions.\n\nIn case-control studies, true causal variants may violate HWE in cases due to penetrance effects, while controls should adhere closely to HWE barring population structure or selection. Consequently, computing HWE in controls (and within ancestry strata) is standard to avoid filtering out true signals that legitimately deviate from HWE in cases. Furthermore, HWE tests have lower power and biased behavior at very low minor allele frequency (MAF), so MAF-dependent application is prudent: apply stringent HWE thresholds to common variants, and avoid or relax HWE filters for rare variants, instead relying on orthogonal quality metrics (for example, depth, allele balance, Mendelian consistency, or Hardy–Weinberg equilibrium tests adjusted for genotype uncertainty).\n\nWe now analyze each option.\n\nOption A: HWE computed in all samples with a lenient threshold $p  10^{-2}$. Under the null of uniform $p$-values, with $m = 8 \\times 10^{6}$ common variants, the expected number removed is $m \\alpha = \\left(8 \\times 10^{6}\\right) \\times \\left(10^{-2}\\right) = 8 \\times 10^{4}$, which matches the stated number. However, this approach is flawed for two reasons. First, using all samples can inappropriately remove true associations because disease-related loci can deviate from HWE in cases. Second, a threshold of $10^{-2}$ is not an artifact-enrichment threshold; it is so lenient that it will remove approximately $1\\%$ of all common variants under perfect HWE, including many high-quality variants, thereby reducing power and potentially biasing downstream analyses. It also does not address the need to stratify by ancestry, which is critical to avoid spurious deviations due to population structure. Verdict: Incorrect.\n\nOption B: HWE computed in controls, stratified by ancestry, with a stringent filter for common variants at $p  10^{-6}$, and no HWE filter for rare variants, relying instead on orthogonal quality metrics. This aligns with best-practice reasoning: spurious peaks arise when artifacts induce HWE deviation and differ between cases and controls; testing HWE in controls avoids removing true disease-associated variants. Stratification by ancestry accommodates population substructure. The chosen threshold, $10^{-6}$, is stringent enough to predominantly target gross deviations expected from technical error rather than random fluctuation, especially in large samples. For common variants, under the null, the expected number removed is $m \\alpha = \\left(8 \\times 10^{6}\\right) \\times \\left(10^{-6}\\right) = 8$. For rare variants, HWE tests are unstable; avoiding blanket HWE filtering prevents unjustified removal of true variants and instead leverages metrics tailored to rare-variant quality. This option also directly explains how artifacts generate spurious Manhattan peaks and how the filter addresses them. Verdict: Correct.\n\nOption C: HWE computed in cases only, filter at $p  10^{-6}$. While the expected number of common variants removed under the null would be $8$ as computed above, this design is conceptually flawed. True causal variants can deviate from HWE in the affected group because penetrance alters genotype frequencies among cases, so filtering on HWE in cases would preferentially remove true associations, biasing results toward the null. It also fails to stratify by ancestry and does not address artifact detection in controls. Verdict: Incorrect.\n\nOption D: Abandon HWE filtering, treating QQ inflation as purely polygenic and compensating only by a stricter Bonferroni threshold. This ignores the empirical clues of artifact: clustered Manhattan peaks in a low-quality region and HWE deviations among controls. A stricter significance threshold does not remediate systematic bias; it can leave spurious peaks unaddressed and reduce power to detect true associations. Quality control, including HWE filtering appropriately applied, is necessary to reduce artifacts before association testing and multiple-testing correction. Verdict: Incorrect.\n\nTherefore, the best choice is Option B. Under Option B, the expected number of common variants removed under the null is $8$.", "answer": "$$\\boxed{B}$$", "id": "4580225"}]}