{"hands_on_practices": [{"introduction": "The engine behind modern imputation is the copying Hidden Markov Model (HMM), which pieces together an individual's haplotypes from a reference panel. The model's ability to switch between different reference haplotypes is not random; it is guided by the biological process of recombination. This exercise [@problem_id:4569457] demonstrates how genetic map distance, measured in Morgans, is mathematically translated into the transition probabilities that govern these switches, providing a foundational understanding of the model's mechanics.", "problem": "A central component of modern genotype phasing and imputation methods is the copying Hidden Markov Model (HMM), in which a haplotype copies segments from a panel of donor haplotypes and switches donors at recombination breakpoints. In this framework, assume that recombination events along genetic distance are governed by a homogeneous Poisson process with rate parameter $\\rho$ per Morgan. The number of recombination events $N$ in an interval of genetic distance $d$ is modeled as a Poisson random variable with mean $\\rho d$. The probability of observing zero events in such an interval is given by the well-tested Poisson property for zero counts, and a donor-switch transition between consecutive markers occurs if and only if the interval contains at least one recombination event. Consider two consecutive marker intervals with map distances $d_{1} = 0.0001$ Morgans and $d_{2} = 0.01$ Morgans, and a rate parameter $\\rho = 100$. Define the relative transition probability $R$ to be the ratio of the donor-switch probabilities across the two intervals, $R = \\frac{p_{\\text{switch}}(d_{2})}{p_{\\text{switch}}(d_{1})}$. Compute $R$ and report it as a unitless decimal rounded to four significant figures.", "solution": "The problem statement has been validated and is deemed scientifically sound, well-posed, and self-contained. We may proceed with the solution.\n\nThe problem describes recombination events using a homogeneous Poisson process. Let $N(d)$ be the number of recombination events occurring in a genetic interval of length $d$ Morgans. According to the problem statement, $N(d)$ is a Poisson random variable with a mean parameter $\\lambda = \\rho d$, where $\\rho$ is the rate of recombination per Morgan. The probability mass function for $N(d)$ is given by:\n$$P(N(d) = k) = \\frac{(\\rho d)^k \\exp(-\\rho d)}{k!}$$\nfor $k = 0, 1, 2, \\dots$.\n\nA donor-switch transition between consecutive markers is defined to occur if and only if the interval between them contains at least one recombination event. Therefore, the probability of a donor switch, denoted as $p_{\\text{switch}}(d)$, is the probability that $N(d) \\ge 1$.\n\nIt is more convenient to calculate this probability by considering the complementary event, which is that no recombination events occur, i.e., $N(d) = 0$.\n$$p_{\\text{switch}}(d) = P(N(d) \\ge 1) = 1 - P(N(d) = 0)$$\n\nUsing the Poisson probability mass function for $k=0$, we find the probability of zero events:\n$$P(N(d) = 0) = \\frac{(\\rho d)^0 \\exp(-\\rho d)}{0!} = \\frac{1 \\cdot \\exp(-\\rho d)}{1} = \\exp(-\\rho d)$$\n\nSubstituting this back into the expression for the switch probability gives:\n$$p_{\\text{switch}}(d) = 1 - \\exp(-\\rho d)$$\n\nWe are given two intervals with genetic distances $d_{1} = 0.0001$ Morgans and $d_{2} = 0.01$ Morgans, and a rate parameter $\\rho = 100$.\n\nFor the first interval, the mean number of recombination events is $\\lambda_1 = \\rho d_1$.\n$$\\lambda_1 = 100 \\times 0.0001 = 0.01$$\nThe probability of a donor switch in this interval is:\n$$p_{\\text{switch}}(d_1) = 1 - \\exp(-\\lambda_1) = 1 - \\exp(-0.01)$$\n\nFor the second interval, the mean number of recombination events is $\\lambda_2 = \\rho d_2$.\n$$\\lambda_2 = 100 \\times 0.01 = 1$$\nThe probability of a donor switch in this second interval is:\n$$p_{\\text{switch}}(d_2) = 1 - \\exp(-\\lambda_2) = 1 - \\exp(-1)$$\n\nThe problem asks for the relative transition probability, $R$, defined as the ratio of these two probabilities:\n$$R = \\frac{p_{\\text{switch}}(d_2)}{p_{\\text{switch}}(d_1)}$$\n\nSubstituting the derived expressions, we obtain:\n$$R = \\frac{1 - \\exp(-1)}{1 - \\exp(-0.01)}$$\n\nWe now compute the numerical value for $R$.\nThe numerator is:\n$$1 - \\exp(-1) \\approx 1 - 0.36787944 \\approx 0.63212056$$\nThe denominator is:\n$$1 - \\exp(-0.01) \\approx 1 - 0.99004983 \\approx 0.00995017$$\n\nThe ratio $R$ is therefore:\n$$R \\approx \\frac{0.63212056}{0.00995017} \\approx 63.52857$$\n\nThe problem requires the result to be rounded to four significant figures. The first four significant figures are $6$, $3$, $5$, and $2$. The fifth significant digit is $8$, which is greater than or equal to $5$, so we round up the fourth digit.\n$$R \\approx 63.53$$\nThis value indicates that a donor switch is approximately $63.53$ times more likely to occur in the longer interval ($d_2$) compared to the shorter one ($d_1$).", "answer": "$$\\boxed{63.53}$$", "id": "4569457"}, {"introduction": "After running a phasing algorithm, it is crucial to assess the quality of the resulting haplotypes before using them in downstream analyses. The switch error rate is a fundamental metric for this purpose, quantifying how often the algorithm incorrectly flips between parental chromosomes. This practice [@problem_id:4569545] involves calculating this key metric and, more importantly, interpreting its impact on the reliability of the phased data for applications like imputation and association studies.", "problem": "In genotype phasing and imputation, a common quality metric is the rate at which inferred haplotypes incorrectly switch their parental assignment between consecutive heterozygous loci. Consider a phased chromosome segment evaluated against an external truth set derived from parent-offspring trios, where the segment contains $T$ adjacent heterozygous loci pairs that define potential phase switch positions (heterozygous transitions). Suppose curation identifies $E$ erroneous phase switches across this segment. Using foundational definitions that treat each heterozygous transition as a Bernoulli trial for whether a phase switch is correct or erroneous, and that define the switch error rate as the expected fraction of transitions that are erroneous, compute the switch error rate for $T = 1{,}000$ transitions and $E = 25$ erroneous switches. State your final answer as a decimal, rounded to four significant figures. Then, based on first principles of haplotype-based inference and the behavior of Hidden Markov Model (HMM)-based phasing in regions of varying linkage disequilibrium (LD) and Minor Allele Frequency (MAF), justify how a switch error rate at this level would be expected to impact downstream haplotype-based imputation accuracy and related analyses. Do not use a percentage sign in your numerical answer; express the rate as a unitless decimal.", "solution": "The problem asks for the calculation of the switch error rate from given data and a subsequent justification of its impact on downstream bioinformatic analyses. The validation process confirms that the problem is scientifically grounded, well-posed, and contains all necessary information for a complete solution.\n\nFirst, let us formalize the calculation of the switch error rate ($SER$). The problem states that each of the $T$ adjacent heterozygous loci pairs, or heterozygous transitions, can be treated as a Bernoulli trial. In this context, a trial represents the opportunity for a phase switch error to occur. The outcome of each trial is binary: either the phase is inferred correctly or it is erroneous. The problem provides the total number of such transitions, $T$, and the total number of observed erroneous phase switches, $E$.\n\nThe given values are:\n- The total number of heterozygous transitions, $T = 1,000$.\n- The number of erroneous phase switches, $E = 25$.\n\nThe problem defines the switch error rate as the expected fraction of transitions that are erroneous. For a series of Bernoulli trials, the best estimate for the probability of the event of interest (in this case, an error) is the relative frequency of its occurrence. Therefore, the switch error rate, $SER$, is computed as the ratio of the number of observed errors to the total number of opportunities for an error:\n$$\nSER = \\frac{E}{T}\n$$\nSubstituting the given numerical values into this equation yields:\n$$\nSER = \\frac{25}{1,000} = 0.025\n$$\nThe problem requires the final answer to be stated as a decimal rounded to four significant figures. The number $0.025$ has two significant figures ($2$ and $5$). To express this with four significant figures, we append two zeros, resulting in $0.02500$.\n\nNext, we must justify the impact of a switch error rate at this level on downstream haplotype-based analyses, such as imputation, based on first principles. An $SER$ of $0.025$ signifies that, on average, $2.5\\%$ of the phase connections between adjacent heterozygous sites are incorrect. This means one out of every $40$ such transitions is erroneous.\n\nThe fundamental principle of haplotype-based imputation is to infer ungenotyped variants in a study sample by leveraging a densely genotyped reference panel of known haplotypes. Algorithms, typically based on Hidden Markov Models (HMMs), work by identifying segments of reference haplotypes that match the sparse, phased haplotype framework of a study individual. The algorithm then \"copies\" the alleles from the best-matching reference haplotype(s) to fill in the missing genotypes.\n\nA phase switch error fundamentally corrupts this process. A switch error occurs when the algorithm incorrectly swaps the paternally-derived and maternally-derived chromosomal segments. For example, if the true paternal and maternal haplotypes at two consecutive heterozygous sites are $H_{pat} = \\dots A \\dots T \\dots$ and $H_{mat} = \\dots G \\dots C \\dots$, a switch error between these sites would result in an inferred haplotype of, for instance, $\\dots A \\dots C \\dots$. This creates a *chimeric* or *mosaic* haplotype that does not exist in the individual and is highly unlikely to exist in the reference panel, as it is an artificial construct of two different ancestral backgrounds.\n\nThe impact of this chimeric haplotype on imputation accuracy is severe, particularly in the region immediately following the switch error. When an HMM-based imputation algorithm encounters a switch error, the currently tracked reference haplotype suddenly ceases to match the individual's inferred haplotype. The model is forced to transition to a different reference haplotype to explain the observed data post-switch. This abrupt and artificial transition introduces a high probability of error. Essentially, the switch error breaks the integrity of the haplotype block, causing the imputation algorithm to lose its place and make mistakes as it tries to recover a match. This results in a localized drop in imputation quality (measured by metrics like $r^2$) around the site of the switch error.\n\nThe magnitude of this impact is modulated by local genomic context, specifically linkage disequilibrium ($LD$) and minor allele frequency ($MAF$).\n- In regions of high $LD$, where alleles over long distances are strongly correlated, haplotypes are long and less diverse. A single switch error is highly disruptive because it breaks a long, conserved haplotype block, creating a sequence with very low probability under the population model. This makes the imputation of any variants within that block, especially rare ones that depend on this long-range context, highly prone to error.\n- In regions of low $LD$ (e.g., recombination hotspots), haplotypes are shorter and more shuffled. Phasing algorithms naturally struggle more in these regions, often leading to a higher density of switch errors. While a single error might be seen as less damaging in a region that is already fragmented, the cumulative effect of multiple nearby errors can degrade imputation quality across the entire low-$LD$ segment.\n- The imputation of low-$MAF$ (rare) variants is disproportionately affected by phasing errors. Rare variants often reside on a single, long ancestral haplotype. The integrity of this haplotype is paramount for their accurate imputation. A switch error rate of $0.025$ is high enough to frequently disrupt these rare variant-carrying haplotypes, significantly reducing the power and accuracy of rare-variant association studies that rely on imputed data.\n\nIn summary, a switch error rate of $0.025$ introduces a significant burden of chimeric haplotypes into the data. This directly degrades the performance of downstream imputation by causing local mismatches against reference panels, forcing erroneous state transitions in the underlying HMMs, and ultimately leading to a measurable decrease in imputation accuracy, with a particularly severe impact on the inference of rare genetic variants.", "answer": "$$\n\\boxed{0.02500}\n$$", "id": "4569545"}, {"introduction": "Genotype imputation algorithms rarely produce a single, definitive genotype; instead, they provide a more nuanced output of posterior probabilities for each possible genotype ($0, 1,$ or $2$ minor alleles). To use this information in quantitative analyses like genome-wide association studies (GWAS), we must convert these probabilities into a single, continuous value. This final exercise [@problem_id:4569492] walks through the calculation of genotype \"dosage,\" the expected allele count, which is the standard and most powerful way to represent imputed data.", "problem": "Consider a single bi-allelic single-nucleotide polymorphism (SNP) with a designated minor allele. Let the unobserved genotype be denoted by the random variable $G \\in \\{0,1,2\\}$, representing the count of minor alleles. An imputation algorithm based on a Hidden Markov Model (HMM) produces a posterior distribution over $G$ given observed data, encoded as $P(G=0 \\mid \\text{data}) = 0.1$, $P(G=1 \\mid \\text{data}) = 0.3$, and $P(G=2 \\mid \\text{data}) = 0.6$. Using only the core definition of conditional expectation from probability theory and the interpretation of imputed dosage as the expected minor-allele count under the posterior, compute the imputed dosage $\\hat{d}$ for this SNP.\n\nThen, using only first principles of probability calibration, briefly explain in words how you would assess whether such posterior genotype probabilities are calibrated against true genotypes across a large cohort with available ground-truth genotypes, without computing any numerical metric in this problem.\n\nProvide the imputed dosage as an exact value (no rounding). No units are required for the dosage.", "solution": "The problem is evaluated as valid. It is scientifically grounded in the principles of statistical genetics and probability theory, well-posed with all necessary information provided, and free from ambiguity or contradiction.\n\nThe problem consists of two parts. The first part requires the computation of the imputed dosage for a single-nucleotide polymorphism (SNP), and the second part asks for a conceptual explanation of how to assess the calibration of genotype probabilities.\n\n**Part 1: Computation of Imputed Dosage**\n\nThe problem defines the unobserved genotype as a discrete random variable $G$, which represents the count of the minor allele at a bi-allelic SNP. The possible values for $G$ are $g \\in \\{0, 1, 2\\}$, corresponding to zero, one, or two copies of the minor allele.\n\nAn imputation algorithm has produced a posterior probability distribution for $G$ conditional on observed data. These probabilities are given as:\n$P(G=0 \\mid \\text{data}) = 0.1$\n$P(G=1 \\mid \\text{data}) = 0.3$\n$P(G=2 \\mid \\text{data}) = 0.6$\n\nThe problem states that the imputed dosage, denoted $\\hat{d}$, is to be interpreted as the expected minor-allele count under this posterior distribution. This is equivalent to calculating the conditional expectation of the random variable $G$ given the data, which is written as $E[G \\mid \\text{data}]$.\n\nAccording to the core definition of expectation for a discrete random variable, the expected value is the sum of each possible value multiplied by its corresponding probability. Applying this definition to the conditional context, we have:\n$$ \\hat{d} = E[G \\mid \\text{data}] = \\sum_{g \\in \\{0,1,2\\}} g \\cdot P(G=g \\mid \\text{data}) $$\n\nWe can now substitute the given values into this formula:\n$$ \\hat{d} = (0 \\cdot P(G=0 \\mid \\text{data})) + (1 \\cdot P(G=1 \\mid \\text{data})) + (2 \\cdot P(G=2 \\mid \\text{data})) $$\n$$ \\hat{d} = (0 \\cdot 0.1) + (1 \\cdot 0.3) + (2 \\cdot 0.6) $$\n$$ \\hat{d} = 0 + 0.3 + 1.2 $$\n$$ \\hat{d} = 1.5 $$\n\nThe imputed dosage for this SNP is therefore $1.5$. This value represents the average number of minor alleles we expect this individual to have, given the observed data and the model.\n\n**Part 2: Assessing Probability Calibration**\n\nThe second part of the problem asks for an explanation of how to assess whether the posterior genotype probabilities are calibrated, using first principles and a large cohort with known true genotypes.\n\nProbability calibration, in its most fundamental sense, means that if a model predicts an outcome with probability $p$, that outcome should be observed with a frequency of approximately $p$ over many instances where that prediction was made.\n\nTo assess the calibration of the posterior genotype probabilities (e.g., $P(G=g \\mid \\text{data})$ for $g \\in \\{0, 1, 2\\}$) against ground-truth genotypes, one would perform the following conceptual procedure:\n1.  Take a large cohort of individuals for whom both the imputed posterior probabilities from the model and the true, experimentally determined genotypes are available.\n2.  To check the calibration for a specific genotype, say $G=2$, group individuals into bins based on their predicted posterior probability, $P(G=2 \\mid \\text{data})$. For instance, one bin could contain all individuals for whom the model predicted $P(G=2 \\mid \\text{data})$ to be in the range $[0.55, 0.65]$.\n3.  Within each bin, calculate the empirical frequency: count the number of individuals whose true genotype is actually $G=2$ and divide this count by the total number of individuals in that bin.\n4.  If the model is well-calibrated, this calculated empirical frequency should be close to the average predicted probability for that bin (e.g., approximately $0.6$ for the $[0.55, 0.65]$ bin).\n5.  This process would be repeated for multiple probability bins covering the range from $0$ to $1$, and for all possible genotype states ($G=0$, $G=1$, and $G=2$), to form a complete assessment of the model's calibration. A well-calibrated model will exhibit close agreement between its assigned probabilities and the observed frequencies across all genotypes and all probability levels.", "answer": "$$\\boxed{1.5}$$", "id": "4569492"}]}