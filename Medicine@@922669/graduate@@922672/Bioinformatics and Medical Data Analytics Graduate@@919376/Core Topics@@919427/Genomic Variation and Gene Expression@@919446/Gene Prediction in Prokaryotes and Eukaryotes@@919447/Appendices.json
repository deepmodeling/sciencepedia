{"hands_on_practices": [{"introduction": "At the heart of *de novo* gene prediction lies the ability to statistically distinguish coding DNA from non-coding background. This exercise [@problem_id:4567054] challenges you to build a foundational tool for this task: a classifier based on order-$k$ Markov models. You will implement the complete workflow, from training models on sequence data to scoring a query sequence using the log-likelihood ratio, and finally, to selecting a decision threshold that controls the false positive rate.", "problem": "You are given training sets of deoxyribonucleic acid (DNA) sequences labeled as coding and noncoding for prokaryotic and eukaryotic contexts. The goal is to score a query sequence under an order-$k$ Markov model with additive smoothing, compute the log-likelihood ratio (LLR) score between the coding and noncoding models, and determine a decision threshold that achieves a target false positive rate (FPR) using a validation set of noncoding sequences. Your program must implement the following, starting only from foundational principles: the Central Dogma of Molecular Biology (deoxyribonucleic acid to ribonucleic acid to protein), the observation that coding regions exhibit short-range compositional and periodic biases due to codon structure and selection, and the Markov assumption that local dependencies can be approximated by a finite-order chain over the nucleotide alphabet. From these bases, derive the estimation of conditional emission probabilities by counting $(k+1)$-mers with additive smoothing and compute the LLR by summing the log of the ratio of conditional probabilities along the query sequence. Then, using the empirical distribution of scores on a validation set of noncoding sequences, choose a classification threshold that meets a target FPR constraint. Do not assume any formulas that are not derivable from the stated bases; justify each step mathematically.\n\nDefinitions and conventions to be used by your program:\n- Alphabet is $\\mathcal{A}=\\{A,C,G,T\\}$. Any other character is invalid for this problem.\n- For an order-$k$ Markov model with $k \\in \\{0,1,2,\\dots\\}$, the conditional probability of observing base $b \\in \\mathcal{A}$ given a length-$k$ context $c \\in \\mathcal{A}^k$ is estimated from training sequences by counting occurrences of $(c,b)$ across all training sequences and applying additive smoothing with pseudocount $\\alpha \\in \\mathbb{R}_{0}$.\n- For a query sequence $x=x_0 x_1 \\dots x_{n-1}$, the LLR score is the sum over positions $i$ from $k$ to $n-1$ of the natural logarithm of the ratio between the coding-model conditional probability and the noncoding-model conditional probability, each conditioned on the context $x_{i-k} \\dots x_{i-1}$. Only positions with full $k$-length context inside the sequence are scored.\n- The empirical FPR at a threshold $\\theta \\in \\mathbb{R}$, evaluated on a validation set of noncoding sequences, is defined as the fraction of validation noncoding sequences whose LLR score is greater than or equal to $\\theta$. To produce a deterministic, finite threshold directly from data, restrict $\\theta$ to the set of distinct scores observed on the validation noncoding sequences, and choose the maximum $\\theta^\\star$ among these such that the empirical FPR is less than or equal to the target. If no such $\\theta$ exists because the target is smaller than the minimal achievable positive step in the empirical FPR, then this test case is ill-posed; the provided test suite avoids this condition by construction.\n\nYour program must process the following test suite. For each test case, the noncoding training set doubles as the validation set for threshold selection to ensure reproducibility under the given constraints.\n\n- Test case $1$ (prokaryote-like composition difference, order-$1$):\n  - Coding training set: $[\\text{ATGAAATTTGAA}, \\text{ATGCCCAAAGGGTTT}, \\text{ATGAAAGGGAAGTAG}]$.\n  - Noncoding training and validation set: $[\\text{TATTTAATTAAT}, \\text{GGATTTTAATTA}, \\text{TAATAATATATT}, \\text{TTAATAAATATA}]$.\n  - Query sequence: $\\text{ATGAAAGTTGA}$.\n  - Markov order: $k=1$.\n  - Smoothing pseudocount: $\\alpha=0.5$.\n  - Target FPR: $t=0.25$.\n\n- Test case $2$ (edge case with unseen contexts handled by smoothing, order-$2$):\n  - Coding training set: $[\\text{GCGCGCGCGC}, \\text{GCCGCGCCGC}]$.\n  - Noncoding training and validation set: $[\\text{ATATATATAT}, \\text{TATATATATA}, \\text{AAATTTAAAT}]$.\n  - Query sequence: $\\text{GCGATCGCGC}$.\n  - Markov order: $k=2$.\n  - Smoothing pseudocount: $\\alpha=0.1$.\n  - Target FPR: $t=0.3333333333333333$.\n\n- Test case $3$ (zero-order model as a baseline, order-$0$):\n  - Coding training set: $[\\text{ATGGCC}, \\text{ATGGCG}, \\text{ATGGGC}, \\text{ATGGCCATG}]$.\n  - Noncoding training and validation set: $[\\text{AAAAAA}, \\text{TTTTTT}, \\text{AATTAA}, \\text{TTAAAT}]$.\n  - Query sequence: $\\text{ATGGCCG}$.\n  - Markov order: $k=0$.\n  - Smoothing pseudocount: $\\alpha=1.0$.\n  - Target FPR: $t=0.5$.\n\nOutput requirements:\n- For each test case, compute:\n  - The LLR score $s \\in \\mathbb{R}$ of the query sequence under the coding versus noncoding models.\n  - The threshold $\\theta^\\star \\in \\mathbb{R}$ determined as specified above from the validation noncoding scores.\n  - A binary decision $d \\in \\{0,1\\}$ where $d=1$ if $s \\ge \\theta^\\star$ and $d=0$ otherwise.\n- Round all real-valued outputs $s$ and $\\theta^\\star$ to exactly $6$ decimal places in the final output.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the triplet for one test case formatted as $[s,\\theta^\\star,d]$. For example, a valid overall output structure looks like $[[s_1,\\theta^\\star_1,d_1],[s_2,\\theta^\\star_2,d_2],[s_3,\\theta^\\star_3,d_3]]$ with each $s_i$ and $\\theta^\\star_i$ rounded to $6$ decimal places and each $d_i$ an integer.", "solution": "The problem presented is a valid and well-posed exercise in statistical bioinformatics, specifically in the domain of *de novo* gene prediction. It requires the implementation of a classification system to distinguish between coding and noncoding deoxyribonucleic acid (DNA) sequences. The solution rests upon fundamental principles of molecular biology and statistical modeling, which we shall derive and formalize step-by-step.\n\nThe biological basis for this problem is the Central Dogma of Molecular Biology, which describes the flow of genetic information from DNA to ribonucleic acid (RNA) to protein. Protein-coding regions of DNA (genes) are transcribed and translated into proteins using a triplet code, where sequences of $3$ nucleotides, called codons, specify amino acids. This process imposes specific statistical patterns on coding sequences that are not typically present in noncoding regions. These patterns include a periodic-3 nucleotide composition bias and preferential use of certain codons for the same amino acid (codon usage bias). These compositional biases allow us to statistically distinguish coding from noncoding DNA.\n\nThe problem proposes using a stationary order-$k$ Markov model to capture these local compositional dependencies. A DNA sequence $x=x_0 x_1 \\dots x_{n-1}$ over the alphabet $\\mathcal{A}=\\{A,C,G,T\\}$ is modeled as a realization of a Markov chain. The core assumption of an order-$k$ model is that the probability of observing a nucleotide at a given position depends only on the $k$ preceding nucleotides. This context of length $k$ is the sequence $c = x_{i-k} \\dots x_{i-1}$. Mathematically, this is expressed as:\n$$ P(x_i | x_0 x_1 \\dots x_{i-1}) = P(x_i | x_{i-k} \\dots x_{i-1}) = P(x_i | c) $$\nFor the special case where the order $k=0$, the model is memoryless, and the probability of observing a nucleotide is independent of any context: $P(x_i | c) = P(x_i)$.\n\nThe first step is to train two separate Markov models: one for coding sequences ($M_{cod}$) and one for noncoding sequences ($M_{non}$). Training consists of estimating the conditional probability parameters $P(b|c)$ for each model, where $b \\in \\mathcal{A}$ is a nucleotide and $c \\in \\mathcal{A}^k$ is a context. We use the provided training sequences for this estimation. The probability is estimated by counting the occurrences of the $(k+1)$-mer $cb$ and the $k$-mer $c$ in the concatenated training data. Let $N(cb)$ be the count of the $(k+1)$-mer and $N(c)$ be the count of the $k$-mer context. To prevent probabilities of $0$ for events not seen in the finite training set, we employ additive (Lidstone) smoothing with a pseudocount $\\alpha > 0$. The smoothed estimate of the conditional probability is:\n$$ P(b|c) = \\frac{N(cb) + \\alpha}{N(c) + |\\mathcal{A}|\\alpha} $$\nwhere $|\\mathcal{A}|=4$ is the size of the nucleotide alphabet. The denominator $N(c) + |\\mathcal{A}|\\alpha$ is the sum of counts of all $(k+1)$-mers starting with context $c$, each augmented by the pseudocount $\\alpha$. Note that $N(c) = \\sum_{b' \\in \\mathcal{A}} N(cb')$. If a context $c$ is never observed ($N(c)=0$), then $N(cb)=0$ for all $b$, and the smoothed probability simplifies to $P(b|c) = \\frac{\\alpha}{|\\mathcal{A}|\\alpha} = \\frac{1}{|\\mathcal{A}|} = 0.25$, representing a uniform belief over the next nucleotide. For $k=0$, the context is empty, and the formula estimates the marginal probability of a base $b$:\n$$ P(b) = \\frac{N(b) + \\alpha}{N_{total} + |\\mathcal{A}|\\alpha} $$\nwhere $N(b)$ is the count of base $b$ and $N_{total}$ is the total number of nucleotides in the training data.\n\nOnce both the coding ($P_{cod}(b|c)$) and noncoding ($P_{non}(b|c)$) models are trained, we can score a query sequence $x=x_0 x_1 \\dots x_{n-1}$. The problem requires computing the log-likelihood ratio (LLR) score. This score measures the relative support for the sequence under the two competing models. By the Markov property, the log-ratio of the joint probabilities simplifies to a sum of log-ratios of conditional probabilities over all positions $i$ from $k$ to $n-1$ (i.e., all positions with a full $k$-length context):\n$$ S(x) = \\sum_{i=k}^{n-1} \\log \\frac{P_{cod}(x_i | x_{i-k} \\dots x_{i-1})}{P_{non}(x_i | x_{i-k} \\dots x_{i-1})} $$\nA positive score suggests the sequence is more likely to be coding, while a negative score suggests it is more likely noncoding.\n\nThe final step is to determine a decision threshold $\\theta^\\star$ for classification. A sequence is classified as coding if its score $S(x) \\ge \\theta^\\star$. The threshold is chosen to control the False Positive Rate (FPR), which is the fraction of noncoding sequences that are incorrectly classified as coding. We use a validation set of noncoding sequences to empirically estimate the FPR for any given threshold $\\theta$. The empirical FPR is:\n$$ \\widehat{\\text{FPR}}(\\theta) = \\frac{\\text{Number of noncoding validation sequences with score } \\ge \\theta}{\\text{Total number of noncoding validation sequences}} $$\nThe problem specifies a deterministic procedure to select $\\theta^\\star$ from the set of distinct scores observed on the validation set. We must find the maximum score $\\theta^\\star$ such that $\\widehat{\\text{FPR}}(\\theta^\\star)$ is less than or equal to the target FPR, $t$. To implement this, we can compute the LLR scores for all noncoding validation sequences, identify the unique scores, and sort them in descending order. We then iterate through this sorted list, and the first score $\\theta$ that satisfies the condition $\\widehat{\\text{FPR}}(\\theta) \\le t$ is our optimal threshold $\\theta^\\star$.\n\nThe complete algorithm is as follows:\n$1$. For a given test case with parameters $k$ and $\\alpha$:\n    a. Train the coding model $M_{cod}$ by computing all probabilities $P_{cod}(b|c)$ from the coding training set.\n    b. Train the noncoding model $M_{non}$ by computing all probabilities $P_{non}(b|c)$ from the noncoding training set.\n$2$. For each sequence in the noncoding validation set:\n    a. Compute its LLR score using the trained models.\n$3$. Determine the threshold $\\theta^\\star$ by finding the maximum score from the set of noncoding scores that results in an empirical FPR no greater than the target $t$.\n$4$. For the query sequence:\n    a. Compute its LLR score $s$.\n    b. Compare the score to the threshold to make a decision: $d=1$ if $s \\ge \\theta^\\star$, and $d=0$ otherwise.\n$5$. Report the triplet $[s, \\theta^\\star, d]$, with real numbers rounded to $6$ decimal places.\n\nThis procedure provides a complete and scientifically grounded method for solving the specified problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import product\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases for gene prediction modeling.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"coding_train\": ['ATGAAATTTGAA', 'ATGCCCAAAGGGTTT', 'ATGAAAGGGAAGTAG'],\n            \"noncoding_train\": ['TATTTAATTAAT', 'GGATTTTAATTA', 'TAATAATATATT', 'TTAATAAATATA'],\n            \"query\": 'ATGAAAGTTGA',\n            \"k\": 1,\n            \"alpha\": 0.5,\n            \"target_fpr\": 0.25,\n        },\n        {\n            \"coding_train\": ['GCGCGCGCGC', 'GCCGCGCCGC'],\n            \"noncoding_train\": ['ATATATATAT', 'TATATATATA', 'AAATTTAAAT'],\n            \"query\": 'GCGATCGCGC',\n            \"k\": 2,\n            \"alpha\": 0.1,\n            \"target_fpr\": 1/3,\n        },\n        {\n            \"coding_train\": ['ATGGCC', 'ATGGCG', 'ATGGGC', 'ATGGCCATG'],\n            \"noncoding_train\": ['AAAAAA', 'TTTTTT', 'AATTAA', 'TTAAAT'],\n            \"query\": 'ATGGCCG',\n            \"k\": 0,\n            \"alpha\": 1.0,\n            \"target_fpr\": 0.5,\n        }\n    ]\n\n    alphabet = ['A', 'C', 'G', 'T']\n    \n    def train_model(sequences, k, alpha):\n        \"\"\"\n        Trains an order-k Markov model with additive smoothing.\n\n        Args:\n            sequences (list[str]): A list of DNA training sequences.\n            k (int): The order of the Markov model.\n            alpha (float): The pseudocount for additive smoothing.\n\n        Returns:\n            dict: A dictionary representing the conditional probability table P(base|context).\n        \"\"\"\n        if k == 0:\n            # Handle the zero-order case (memoryless model)\n            counts = {base: 0 for base in alphabet}\n            total_bases = 0\n            for seq in sequences:\n                for base in seq:\n                    counts[base] += 1\n                total_bases += len(seq)\n            \n            probs = {}\n            # Context is empty for k=0\n            probs[''] = {base: (counts[base] + alpha) / (total_bases + len(alphabet) * alpha) for base in alphabet}\n            return probs\n\n        # Handle k > 0\n        kmer_counts = {} # (k+1)-mers\n        context_counts = {} # k-mers\n        \n        for seq in sequences:\n            for i in range(len(seq) - k):\n                context = seq[i:i+k]\n                kmer = seq[i:i+k+1]\n                \n                context_counts[context] = context_counts.get(context, 0) + 1\n                kmer_counts[kmer] = kmer_counts.get(kmer, 0) + 1\n        \n        # Generate all possible contexts\n        contexts = [''.join(p) for p in product(alphabet, repeat=k)]\n        \n        probs = {}\n        for context in contexts:\n            probs[context] = {}\n            n_c = context_counts.get(context, 0)\n            denominator = n_c + len(alphabet) * alpha\n            \n            for base in alphabet:\n                kmer = context + base\n                n_cb = kmer_counts.get(kmer, 0)\n                numerator = n_cb + alpha\n                probs[context][base] = numerator / denominator\n                \n        return probs\n\n    def calculate_llr(sequence, k, prob_cod, prob_non):\n        \"\"\"\n        Calculates the log-likelihood ratio (LLR) score for a query sequence.\n        \n        Args:\n            sequence (str): The query DNA sequence.\n            k (int): The order of the Markov model.\n            prob_cod (dict): The trained coding model probability table.\n            prob_non (dict): The trained noncoding model probability table.\n\n        Returns:\n            float: The LLR score.\n        \"\"\"\n        llr_score = 0.0\n        if len(sequence) = k:\n            return 0.0\n\n        for i in range(k, len(sequence)):\n            context = sequence[i-k:i] if k > 0 else ''\n            base = sequence[i]\n            \n            p_cod = prob_cod[context][base]\n            p_non = prob_non[context][base]\n            \n            llr_score += np.log(p_cod / p_non)\n            \n        return llr_score\n\n    def find_threshold(validation_sequences, k, prob_cod, prob_non, target_fpr):\n        \"\"\"\n        Determines the classification threshold based on a target FPR.\n        \n        Args:\n            validation_sequences (list[str]): A list of noncoding sequences for validation.\n            k (int): The order of the Markov model.\n            prob_cod (dict): The trained coding model.\n            prob_non (dict): The trained noncoding model.\n            target_fpr (float): The target False Positive Rate.\n        \n        Returns:\n            float: The determined threshold theta_star.\n        \"\"\"\n        scores = [calculate_llr(seq, k, prob_cod, prob_non) for seq in validation_sequences]\n        \n        unique_scores = sorted(list(set(scores)), reverse=True)\n        num_validation = len(validation_sequences)\n        \n        np_scores = np.array(scores)\n\n        for theta in unique_scores:\n            num_false_positives = np.sum(np_scores >= theta)\n            fpr = num_false_positives / num_validation\n            if fpr = target_fpr:\n                return theta\n        \n        # As per problem, a solution always exists.\n        # This case would handle if target FPR is smaller than the smallest possible step.\n        return float('inf')\n\n\n    results = []\n    for case in test_cases:\n        # 1. Train models\n        model_cod = train_model(case[\"coding_train\"], case[\"k\"], case[\"alpha\"])\n        model_non = train_model(case[\"noncoding_train\"], case[\"k\"], case[\"alpha\"])\n\n        # 2. Score query sequence\n        s = calculate_llr(case[\"query\"], case[\"k\"], model_cod, model_non)\n\n        # 3. Find threshold using noncoding sequences as validation set\n        theta_star = find_threshold(\n            case[\"noncoding_train\"], case[\"k\"], model_cod, model_non, case[\"target_fpr\"]\n        )\n\n        # 4. Make decision\n        d = 1 if s >= theta_star else 0\n        \n        results.append(f\"[{s:.6f},{theta_star:.6f},{d}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4567054"}, {"introduction": "While prokaryotic genes are typically continuous, eukaryotic genes are mosaics of exons and introns, requiring a predictive model to correctly assemble the final transcript. This practice [@problem_id:4567055] abstracts this complex biological process into a classic computational problem: finding the optimal chain of exons. You will use dynamic programming, a cornerstone algorithm in bioinformatics, to find the highest-scoring, valid gene structure from a set of candidate splice junctions.", "problem": "You are given a finite set of candidate splice junctions along a single genomic coordinate axis. Each candidate splice junction is a pair of positions with an associated real-valued score. The biological foundation is that in eukaryotic messenger ribonucleic acid (mRNA) processing, exons are concatenated by splicing out introns, where each intron is defined by a donor site and an acceptor site. The Central Dogma of molecular biology states that deoxyribonucleic acid (DNA) is transcribed to mRNA, which is translated to protein. For this problem, the formalization is as follows.\n\nDefinitions and assumptions:\n- A candidate junction $j$ is represented by a donor coordinate $d_j \\in \\mathbb{Z}$, an acceptor coordinate $a_j \\in \\mathbb{Z}$, and a real-valued score $s_j \\in \\mathbb{R}$, with $d_j  a_j$.\n- The intron length for $j$ is $L_j = a_j - d_j - 1$ (number of intervening bases spliced out).\n- A junction $j$ is valid under a pair of inclusive bounds $(L_{\\min}, L_{\\max}) \\in \\mathbb{Z} \\times \\mathbb{Z}$ if $L_{\\min} \\le L_j \\le L_{\\max}$.\n- A chain (putative splicing pattern) is an ordered list of distinct valid junction indices $(j_1, j_2, \\dots, j_k)$ such that $a_{j_t}  d_{j_{t+1}}$ for all $t \\in \\{1,\\dots,k-1\\}$. This enforces that introns do not overlap and exons between introns have nonnegative length. The chain score is $\\sum_{t=1}^{k} s_{j_t}$.\n- The empty chain (with $k=0$) has score $0$.\n\nTask:\n- For each provided test case, compute a chain that maximizes the chain score. If multiple chains tie for the maximum score, apply the following deterministic tie-breaking in order:\n  1. Prefer the chain with the smaller number of junctions $k$.\n  2. If still tied, prefer the lexicographically smaller sequence of $0$-based indices when compared elementwise.\n\nYour program must implement a principle-based dynamic programming solution derived from the optimal substructure of chains under the partial order induced by the genomic coordinate constraints, without relying on any external input.\n\nInput to be embedded in code (test suite):\n- Test case $1$:\n  - $L_{\\min} = 20$, $L_{\\max} = 200$.\n  - Candidates (index $j$: $(d_j,a_j,s_j)$):\n    - $0$: $(100,150,3.0)$\n    - $1$: $(140,210,4.0)$\n    - $2$: $(220,300,6.5)$\n    - $3$: $(305,360,2.0)$\n    - $4$: $(170,190,1.0)$\n    - $5$: $(370,430,3.5)$\n    - $6$: $(250,265,2.0)$\n- Test case $2$:\n  - $L_{\\min} = 50$, $L_{\\max} = 70$.\n  - Candidates:\n    - $0$: $(100,151,1.0)$\n    - $1$: $(160,231,1.0)$\n    - $2$: $(232,283,2.0)$\n    - $3$: $(284,355,2.0)$\n- Test case $3$:\n  - $L_{\\min} = 10$, $L_{\\max} = 1000$.\n  - Candidates:\n    - $0$: $(100,220,5.0)$\n    - $1$: $(150,230,6.0)$\n    - $2$: $(180,240,4.5)$\n    - $3$: $(190,250,4.0)$\n- Test case $4$:\n  - $L_{\\min} = 10$, $L_{\\max} = 100$.\n  - Candidates:\n    - $0$: $(100,130,2.0)$\n    - $1$: $(140,170,2.0)$\n    - $2$: $(110,125,1.5)$\n    - $3$: $(126,141,1.0)$\n    - $4$: $(142,157,1.5)$\n- Test case $5$:\n  - $L_{\\min} = 100$, $L_{\\max} = 120$.\n  - Candidates:\n    - $0$: $(10,80,1.0)$\n    - $1$: $(100,300,2.0)$\n    - $2$: $(400,450,3.0)$\n\nOutput specification:\n- For each test case, output a two-element list $[S, I]$ where $S$ is the maximum achievable score as a real number, and $I$ is the list of selected $0$-based indices of candidates in increasing genomic order. If no valid junction exists or the empty chain is optimal, $I$ must be the empty list.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each element formatted as $[S,[i_1,i_2,\\dots,i_k]]$. For example, output must look like $[[S_1,[\\dots]],[S_2,[\\dots]],\\dots]$.\n\nConstraints to observe in the algorithm:\n- All coordinates are integers and satisfy $d_j  a_j$ for each candidate.\n- The only constraints to enforce are the intron length bounds and non-overlap $a_{j_t}  d_{j_{t+1}}$; no other biological features are to be modeled.\n- The dynamic programming must be designed from first principles using the optimal substructure implied by the partial order on candidates defined by the genomic coordinate constraints.\n\nYour task is to implement the program that solves the above test suite and prints the single required output line. The answers must be expressed as raw numbers without any units, and there are no angles or percentages in this task.", "solution": "The problem asks for an optimal chain of non-overlapping splice junctions that maximizes a cumulative score, subject to specific tie-breaking rules. This problem exhibits optimal substructure and overlapping subproblems, making it amenable to a dynamic programming approach. The derivation and implementation are as follows.\n\nFirst, we formalize the problem. We are given a set of candidate junctions $J$.\nA junction $j \\in J$ is defined by a tuple $(d_j, a_j, s_j)$, where $d_j$ and $a_j$ are integer coordinates with $d_j  a_j$, and $s_j$ is a real-valued score. Let $idx_j$ be the original $0$-based index of junction $j$.\n\nThe solution proceeds in several stages:\n\n1.  **Filtering**: A junction $j$ is considered valid only if its intron length, $L_j = a_j - d_j - 1$, falls within the inclusive range $[L_{\\min}, L_{\\max}]$. We begin by filtering the initial set of candidates to retain only these valid junctions. If no junctions are valid, the optimal solution is the empty chain, with a score of $S=0$ and an empty index list $I=[]$.\n\n2.  **Sorting and Defining a Partial Order**: The core constraint for forming a chain $(j_1, j_2, \\dots, j_k)$ is that for any adjacent pair of junctions $(j_t, j_{t+1})$, the acceptor site of the first must precede the donor site of the second, i.e., $a_{j_t}  d_{j_{t+1}}$. This condition defines a partial order on the set of valid junctions. To facilitate a dynamic programming solution, we must process the junctions in an order consistent with this partial ordering. A topological sort is required. A standard and effective method is to sort the valid junctions primarily by their acceptor coordinates $a_j$ in ascending order. Should two junctions share the same acceptor coordinate, a secondary sort on their donor coordinates $d_j$ ensures a deterministic ordering. Let the sorted list of valid junctions be $v_0, v_1, \\dots, v_{m-1}$. This ordering guarantees that for any junction $v_i$, any potential predecessor $v_p$ in a chain (where $a_{v_p}  d_{v_i}$) must have $a_{v_p}  a_{v_i}$, and thus will appear before $v_i$ in our sorted list (i.e., $p  i$).\n\n3.  **Dynamic Programming Formulation**:\n    *   **Optimal Substructure**: The optimal chain ending at a junction $v_i$ is formed by extending an optimal chain that ends at some valid predecessor junction $v_p$ (where $a_{v_p}  d_{v_i}$). The score of such a new chain is the score of the optimal chain ending at $v_p$ plus the score of $v_i$.\n    *   **State Definition**: Let $DP[i]$ be the representation of the optimal chain ending with junction $v_i$. Due to the complex tie-breaking rules, this state must capture more than just the score. We define the state as a tuple: $(S_i, k_i, I_i)$, where $S_i$ is the maximum score of a chain ending at $v_i$, $k_i$ is the number of junctions in that chain, and $I_i$ is the sequence of original indices of the junctions in that chain, ordered by their genomic position.\n    *   **Recurrence Relation**: For each junction $v_i$ in the sorted list, we compute $DP[i]$. The base case is a chain consisting only of $v_i$ itself, for which the state is $(s_{v_i}, 1, [idx_{v_i}])$. We then iterate through all preceding junctions $v_p$ (with $p  i$) and check if they can physically precede $v_i$ (i.e., if $a_{v_p}  d_{v_i}$). If so, we form a candidate new chain by extending the optimal chain ending at $v_p$ (stored in $DP[p]$) with $v_i$. The state of this candidate chain would be $(S_p + s_{v_i}, k_p + 1, I_p \\oplus [idx_{v_i}])$, where $\\oplus$ denotes list concatenation. We compare this candidate chain with the current best chain found for $v_i$ using the specified tie-breaking rules:\n        1.  Primary criterion: Maximize the score $S$.\n        2.  Secondary (tie-breaker): Minimize the number of junctions $k$.\n        3.  Tertiary (tie-breaker): Prefer the lexicographically smaller sequence of indices $I$.\n\n        The recurrence can be expressed as:\n        $$ DP[i] = \\max_{\\prec} \\left( (s_{v_i}, 1, [idx_{v_i}]), \\bigcup_{pi, a_{v_p}  d_{v_i}} \\{ (S_p + s_{v_i}, k_p + 1, I_p \\oplus [idx_{v_i}]) \\} \\right) $$\n        where $(S_p, k_p, I_p) = DP[p]$ and $\\max_{\\prec}$ is the maximization operator according to the tie-breaking relation $\\prec$ defined above.\n\n4.  **Final Solution**: After computing $DP[i]$ for all $i=0, \\dots, m-1$, the globally optimal chain is the best chain among all $DP[i]$ and the empty chain $(S=0.0, k=0, I=[])$. The same tie-breaking logic is applied to find this final, globally optimal solution. The result for the test case is the score and index list of this final chain.\n\nThis systematic approach ensures that for each junction, we find the best possible chain ending there, leveraging previously computed optimal solutions for all valid predecessors. By traversing the junctions in a topologically sorted order, we guarantee that when we calculate $DP[i]$, the values for all potential predecessors $DP[p]$ (with $pi$) have already been finalized.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the gene chaining problem for a fixed set of test cases\n    using dynamic programming.\n    \"\"\"\n    test_cases = [\n        {\n            \"L_bounds\": (20, 200),\n            \"candidates\": [\n                (100, 150, 3.0), (140, 210, 4.0), (220, 300, 6.5),\n                (305, 360, 2.0), (170, 190, 1.0), (370, 430, 3.5),\n                (250, 265, 2.0)\n            ]\n        },\n        {\n            \"L_bounds\": (50, 70),\n            \"candidates\": [\n                (100, 151, 1.0), (160, 231, 1.0), (232, 283, 2.0),\n                (284, 355, 2.0)\n            ]\n        },\n        {\n            \"L_bounds\": (10, 1000),\n            \"candidates\": [\n                (100, 220, 5.0), (150, 230, 6.0), (180, 240, 4.5),\n                (190, 250, 4.0)\n            ]\n        },\n        {\n            \"L_bounds\": (10, 100),\n            \"candidates\": [\n                (100, 130, 2.0), (140, 170, 2.0), (110, 125, 1.5),\n                (126, 141, 1.0), (142, 157, 1.5)\n            ]\n        },\n        {\n            \"L_bounds\": (100, 120),\n            \"candidates\": [\n                (10, 80, 1.0), (100, 300, 2.0), (400, 450, 3.0)\n            ]\n        }\n    ]\n\n    all_results = []\n\n    def is_chain1_better(chain1, chain2):\n        \"\"\"\n        Compares two chains based on the problem's tie-breaking rules.\n        A chain is a tuple: (score, k, index_list).\n        Returns True if chain1 is better than chain2.\n        \"\"\"\n        score1, k1, indices1 = chain1\n        score2, k2, indices2 = chain2\n\n        if score1 > score2:\n            return True\n        if score1  score2:\n            return False\n        \n        # Scores are equal, prefer smaller k\n        if k1  k2:\n            return True\n        if k1 > k2:\n            return False\n\n        # Scores and k are equal, prefer lexicographically smaller index list\n        if indices1  indices2:\n            return True\n        \n        return False\n\n    for case in test_cases:\n        L_min, L_max = case[\"L_bounds\"]\n        candidates = case[\"candidates\"]\n\n        valid_junctions = []\n        for i, (d, a, s) in enumerate(candidates):\n            length = a - d - 1\n            if L_min = length = L_max:\n                valid_junctions.append({'d': d, 'a': a, 's': s, 'idx': i})\n\n        if not valid_junctions:\n            all_results.append(\"[0.0,[]]\")\n            continue\n\n        valid_junctions.sort(key=lambda j: (j['a'], j['d']))\n\n        dp_states = []\n        for i in range(len(valid_junctions)):\n            current_j = valid_junctions[i]\n            \n            # Base case: a chain of one junction\n            best_chain_for_i = (current_j['s'], 1, [current_j['idx']])\n\n            for p in range(i):\n                pred_j = valid_junctions[p]\n\n                if pred_j['a']  current_j['d']:\n                    pred_chain_score, pred_chain_k, pred_chain_indices = dp_states[p]\n                    \n                    candidate_chain = (\n                        pred_chain_score + current_j['s'],\n                        pred_chain_k + 1,\n                        pred_chain_indices + [current_j['idx']]\n                    )\n\n                    if is_chain1_better(candidate_chain, best_chain_for_i):\n                        best_chain_for_i = candidate_chain\n            \n            dp_states.append(best_chain_for_i)\n\n        # The overall best chain is the best among all ending chains and the empty chain\n        best_overall_chain = (0.0, 0, [])\n        for chain in dp_states:\n            if is_chain1_better(chain, best_overall_chain):\n                best_overall_chain = chain\n\n        final_score = best_overall_chain[0]\n        final_indices = best_overall_chain[2]\n        \n        all_results.append(f\"[{final_score},{final_indices}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "4567055"}, {"introduction": "A gene prediction program is only as good as its performance on unseen data, and rigorously evaluating this performance is a critical skill. In this exercise [@problem_id:4567083], you will step into the role of a researcher benchmarking a new tool, calculating key metrics such as the exon-level $F_1$-score and gene-level sensitivity. This practice highlights the crucial, and often subtle, distinctions between different levels of annotation accuracy and their implications for a model's utility.", "problem": "A research group is benchmarking a gene prediction pipeline on a mixed dataset containing both eukaryotic and prokaryotic genes. The biological base is that in eukaryotes, genes may have multiple exons and alternative splicing yields multiple transcripts per gene, whereas in prokaryotes, protein-coding genes are typically single-exon open reading frames. The evaluation uses exact boundary matching at the exon level and requires complete transcript-level correctness for a gene to be considered correctly predicted at the gene level. For exon-level evaluation, an exon is considered correctly predicted only if a predicted exon has identical start and end coordinates and the same strand as a ground truth exon; partial overlaps or boundary shifts are incorrect. For gene-level evaluation, a gene is considered correctly predicted if there exists at least one predicted transcript whose set of exons exactly matches the exon set of at least one ground truth transcript for that gene.\n\nThe ground truth comprises $6$ genes:\n- Eukaryotic gene $\\mathrm{E1}$ with $2$ transcripts:\n  - $\\mathrm{E1}$ Transcript $\\mathrm{T1}$ exons: $[100, 200]$, $[301, 400]$, $[501, 600]$.\n  - $\\mathrm{E1}$ Transcript $\\mathrm{T2}$ exons: $[100, 200]$, $[351, 400]$, $[501, 600]$.\n- Eukaryotic gene $\\mathrm{E2}$ with $1$ transcript:\n  - $\\mathrm{E2}$ exons: $[1000, 1200]$, $[1301, 1400]$.\n- Eukaryotic gene $\\mathrm{E3}$ with $1$ transcript:\n  - $\\mathrm{E3}$ exon: $[2000, 2300]$.\n- Prokaryotic gene $\\mathrm{P1}$ with $1$ transcript (single exon):\n  - $\\mathrm{P1}$ exon: $[5000, 5600]$.\n- Prokaryotic gene $\\mathrm{P2}$ with $1$ transcript (single exon):\n  - $\\mathrm{P2}$ exon: $[5700, 6200]$.\n- Prokaryotic gene $\\mathrm{P3}$ with $1$ transcript (single exon):\n  - $\\mathrm{P3}$ exon: $[6300, 6700]$.\n\nThe pipeline outputs the following predicted transcripts and their exon sets:\n- Predicted Transcript $1$: exons $[100, 200]$, $[301, 400]$, $[501, 600]$.\n- Predicted Transcript $2$: exons $[1000, 1200]$, $[1301, 1405]$.\n- Predicted Transcript $3$: exon $[2000, 2300]$.\n- Predicted Transcript $4$: exon $[5000, 5600]$.\n- Predicted Transcript $5$: exons $[6300, 6700]$, $[6450, 6460]$.\n- Predicted Transcript $6$: exon $[450, 480]$.\n\nAssume all coordinates denote base pair positions on the correct strand and that exon-level evaluation is performed over the set of unique ground truth exons across all transcripts. Compute the exon-level F1-score and the gene-level sensitivity (defined as gene-level recall) for this dataset, adhering to the evaluation criteria stated above. Express both metrics as decimals, and round each to four significant figures. Provide the two values in the order: exon-level F1-score, gene-level sensitivity.", "solution": "The problem is well-defined and grounded in the standard practices of bioinformatics performance evaluation. It requires the calculation of two key metrics, exon-level F1-score and gene-level sensitivity, based on a provided set of ground truth and predicted gene annotations.\n\nFirst, we address the exon-level evaluation. The standard metrics for this evaluation are precision, recall, and the F1-score, which is their harmonic mean. Let $TP_{exon}$ be the number of true positive exons, $FP_{exon}$ be the number of false positive exons, and $FN_{exon}$ be the number of false negative exons. The evaluation is based on an exact match of start and end coordinates.\n\nExon-level precision ($P_{exon}$) and recall ($R_{exon}$) are defined as:\n$$P_{exon} = \\frac{TP_{exon}}{TP_{exon} + FP_{exon}}$$\n$$R_{exon} = \\frac{TP_{exon}}{TP_{exon} + FN_{exon}}$$\nThe F1-score ($F_{1,exon}$) is then:\n$$F_{1,exon} = 2 \\cdot \\frac{P_{exon} \\cdot R_{exon}}{P_{exon} + R_{exon}}$$\n\nTo calculate these, we establish the set of unique ground truth exons ($S_{GT}$) and the set of all predicted exons ($S_{P}$).\nThe set of unique ground truth exons is:\n$S_{GT} = \\{[100, 200], [301, 400], [501, 600], [351, 400], [1000, 1200], [1301, 1400], [2000, 2300], [5000, 5600], [5700, 6200], [6300, 6700]\\}$.\nThe total number of unique ground truth exons is $|S_{GT}| = 10$. This means $TP_{exon} + FN_{exon} = 10$.\n\nThe set of predicted exons is:\n$S_{P} = \\{[100, 200], [301, 400], [501, 600], [1000, 1200], [1301, 1405], [2000, 2300], [5000, 5600], [6300, 6700], [6450, 6460], [450, 480]\\}$.\nThe total number of predicted exons is $|S_{P}| = 10$. This means $TP_{exon} + FP_{exon} = 10$.\n\n$TP_{exon}$ is the number of exons in $S_P$ that are also in $S_{GT}$. By comparing the sets:\n- Correctly predicted exons: $[100, 200], [301, 400], [501, 600], [1000, 1200], [2000, 2300], [5000, 5600], [6300, 6700]$.\n- We find $TP_{exon} = 7$.\n\nNow we calculate the other counts:\n$FP_{exon} = |S_P| - TP_{exon} = 10 - 7 = 3$.\n$FN_{exon} = |S_{GT}| - TP_{exon} = 10 - 7 = 3$.\n\nWe compute exon-level precision and recall:\n$P_{exon} = \\frac{7}{10} = 0.7$\n$R_{exon} = \\frac{7}{10} = 0.7$\n\nAnd the exon-level F1-score:\n$F_{1,exon} = 2 \\cdot \\frac{0.7 \\cdot 0.7}{0.7 + 0.7} = 2 \\cdot \\frac{0.49}{1.4} = 0.7$.\nRounded to four significant figures, this is $0.7000$.\n\nNext, we address the gene-level evaluation. We calculate gene-level sensitivity (recall). Let $N_{genes}$ be the total number of ground truth genes, and $TP_{gene}$ be the number of correctly predicted genes.\n$$S_{gene} = \\frac{TP_{gene}}{N_{genes}}$$\nThere are $N_{genes} = 6$ ground truth genes. A gene is correctly predicted if a predicted transcript's set of exons exactly matches the exon set of at least one of the gene's ground truth transcripts.\n\nWe evaluate each gene:\n1.  **Gene E1**: Ground truth transcripts have exon sets $\\{[100, 200], [301, 400], [501, 600]\\}$ and $\\{[100, 200], [351, 400], [501, 600]\\}$. Predicted Transcript 1 is an exact match for the first transcript. Thus, Gene E1 is a true positive.\n2.  **Gene E2**: Ground truth exon set is $\\{[1000, 1200], [1301, 1400]\\}$. Predicted Transcript 2 has exons $\\{[1000, 1200], [1301, 1405]\\}$, which is not an exact match. Gene E2 is a false negative.\n3.  **Gene E3**: Ground truth exon set is $\\{[2000, 2300]\\}$. Predicted Transcript 3 is an exact match. Gene E3 is a true positive.\n4.  **Gene P1**: Ground truth exon set is $\\{[5000, 5600]\\}$. Predicted Transcript 4 is an exact match. Gene P1 is a true positive.\n5.  **Gene P2**: Ground truth exon set is $\\{[5700, 6200]\\}$. No predicted transcript matches. Gene P2 is a false negative.\n6.  **Gene P3**: Ground truth exon set is $\\{[6300, 6700]\\}$. Predicted Transcript 5 has two exons, so it cannot match. Gene P3 is a false negative.\n\nThe number of correctly predicted genes is $TP_{gene} = 3$ (E1, E3, P1).\nThe gene-level sensitivity is:\n$S_{gene} = \\frac{TP_{gene}}{N_{genes}} = \\frac{3}{6} = 0.5$.\nRounded to four significant figures, this is $0.5000$.\n\nThe two required values are the exon-level F1-score ($0.7000$) and the gene-level sensitivity ($0.5000$).", "answer": "$$\\boxed{\\begin{pmatrix} 0.7000  0.5000 \\end{pmatrix}}$$", "id": "4567083"}]}