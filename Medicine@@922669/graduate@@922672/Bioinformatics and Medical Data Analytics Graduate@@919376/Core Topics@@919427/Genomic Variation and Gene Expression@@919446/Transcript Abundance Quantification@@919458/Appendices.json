{"hands_on_practices": [{"introduction": "Understanding how transcript abundance is estimated starts with the core statistical principle of maximum likelihood. This first exercise [@problem_id:4614663] challenges you to work from first principles, using a simplified toy example to manually calculate the most likely transcript fractions that gave rise to observed RNA-seq fragment counts. Mastering this direct calculation provides a solid conceptual foundation for the more complex, automated algorithms used in practice.", "problem": "A researcher is quantifying transcript abundance from Ribonucleic Acid Sequencing (RNA-Seq) data in a small toy transcriptome comprising three transcripts, labeled $T_{1}$, $T_{2}$, and $T_{3}$. The goal is to compute the maximum likelihood estimates of the transcript fractions $\\pi = (\\pi_{1}, \\pi_{2}, \\pi_{3})$, where $\\pi_{i}$ denotes the probability that a randomly sampled fragment originated from transcript $T_{i}$, subject to $\\sum_{i=1}^{3} \\pi_{i} = 1$ and $\\pi_{i} \\geq 0$.\n\nAssume the standard RNA-Seq sampling model with uniform fragment start positions across each transcriptâ€™s effective length, and that observed fragments are partitioned into equivalence classes by a pseudoalignment procedure. The three equivalence classes are:\n- $E_{1}$: fragments that pseudoalign uniquely to $T_{1}$,\n- $E_{2}$: fragments that pseudoalign ambiguously to $\\{T_{2}, T_{3}\\}$,\n- $E_{3}$: fragments that pseudoalign uniquely to $T_{2}$.\n\nThe effective lengths (in nucleotides) of the transcripts are provided as:\n- $\\tilde{\\ell}_{1} = 900$,\n- $\\tilde{\\ell}_{2} = 600$,\n- $\\tilde{\\ell}_{3} = 300$.\n\nThe compatibility footprints (in nucleotides) defining how much of each transcript contributes reads to each equivalence class are:\n- From $T_{1}$: $900$ to $E_{1}$, $0$ to $E_{2}$, $0$ to $E_{3}$,\n- From $T_{2}$: $0$ to $E_{1}$, $180$ to $E_{2}$, $420$ to $E_{3}$,\n- From $T_{3}$: $0$ to $E_{1}$, $300$ to $E_{2}$, $0$ to $E_{3}$.\n\nA library of $N = 1000$ fragments is observed, with equivalence class counts:\n- $c_{E_{1}} = 400$,\n- $c_{E_{2}} = 500$,\n- $c_{E_{3}} = 100$.\n\nUsing only first-principles reasoning from the stated sampling assumptions and data, compute the maximum likelihood estimates of the transcript fractions $\\pi = (\\pi_{1}, \\pi_{2}, \\pi_{3})$. Round your final numerical values to four significant figures. Express the result as dimensionless probabilities.", "solution": "The problem is to determine the maximum likelihood estimates (MLE) for the transcript fractions $\\pi = (\\pi_{1}, \\pi_{2}, \\pi_{3})$, which represent the probabilities that a randomly sampled fragment originates from transcripts $T_{1}$, $T_{2}$, and $T_{3}$, respectively.\n\nThe problem is first validated.\n**Step 1: Extract Givens**\n- Transcript set: $\\{T_{1}, T_{2}, T_{3}\\}$\n- Parameters to estimate: $\\pi = (\\pi_{1}, \\pi_{2}, \\pi_{3})$ with constraints $\\sum_{i=1}^{3} \\pi_{i} = 1$ and $\\pi_{i} \\geq 0$.\n- Equivalence classes: $E_{1}$ (unique to $T_{1}$), $E_{2}$ (ambiguous to $\\{T_{2}, T_{3}\\}$), $E_{3}$ (unique to $T_{2}$).\n- Effective transcript lengths: $\\tilde{\\ell}_{1} = 900$, $\\tilde{\\ell}_{2} = 600$, $\\tilde{\\ell}_{3} = 300$.\n- Compatibility footprints $f_{ij}$ (length of transcript $i$ compatible with class $j$):\n  - $T_1$: $f_{11}=900$, $f_{12}=0$, $f_{13}=0$.\n  - $T_2$: $f_{21}=0$, $f_{22}=180$, $f_{23}=420$.\n  - $T_3$: $f_{31}=0$, $f_{32}=300$, $f_{33}=0$.\n- Total observed fragments: $N = 1000$.\n- Equivalence class counts: $c_{E_{1}} = 400$, $c_{E_{2}} = 500$, $c_{E_{3}} = 100$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, situated within the standard framework of transcript abundance quantification in bioinformatics. All terms are well-defined in this context. The provided data is self-contained and internally consistent; for each transcript $T_i$, the sum of its compatibility footprints equals its effective length ($\\sum_{j} f_{ij} = \\tilde{\\ell}_i$). The problem is a well-posed maximum likelihood estimation task. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Derivation of the Maximum Likelihood Estimates**\n\nThe core of the problem is to construct the likelihood function for the observed equivalence class counts and maximize it with respect to the transcript fractions $\\pi$.\n\nFirst, we determine the conditional probability, $p(E_{j} | T_{i})$, that a fragment originating from transcript $T_{i}$ is classified into equivalence class $E_{j}$. This probability is the ratio of the compatibility footprint $f_{ij}$ to the transcript's effective length $\\tilde{\\ell}_{i}$:\n$$p(E_{j} | T_{i}) = \\frac{f_{ij}}{\\tilde{\\ell}_{i}}$$\nUsing the provided data:\n- For $T_{1}$: $p(E_{1} | T_{1}) = \\frac{900}{900} = 1$; $p(E_{2} | T_{1}) = 0$; $p(E_{3} | T_{1}) = 0$.\n- For $T_{2}$: $p(E_{1} | T_{2}) = 0$; $p(E_{2} | T_{2}) = \\frac{180}{600} = 0.3$; $p(E_{3} | T_{2}) = \\frac{420}{600} = 0.7$.\n- For $T_{3}$: $p(E_{1} | T_{3}) = 0$; $p(E_{2} | T_{3}) = \\frac{300}{300} = 1$; $p(E_{3} | T_{3}) = 0$.\n\nNext, we express the overall probability of a random fragment belonging to equivalence class $E_{j}$, $p(E_{j})$, using the law of total probability, summing over all possible originating transcripts:\n$$p(E_{j}) = \\sum_{i=1}^{3} p(E_{j} | T_{i}) \\pi_{i}$$\nThis yields:\n- $p(E_{1}) = p(E_{1} | T_{1}) \\pi_{1} + p(E_{1} | T_{2}) \\pi_{2} + p(E_{1} | T_{3}) \\pi_{3} = (1) \\pi_{1} + (0) \\pi_{2} + (0) \\pi_{3} = \\pi_{1}$\n- $p(E_{2}) = p(E_{2} | T_{1}) \\pi_{1} + p(E_{2} | T_{2}) \\pi_{2} + p(E_{2} | T_{3}) \\pi_{3} = (0) \\pi_{1} + (0.3) \\pi_{2} + (1) \\pi_{3} = 0.3 \\pi_{2} + \\pi_{3}$\n- $p(E_{3}) = p(E_{3} | T_{1}) \\pi_{1} + p(E_{3} | T_{2}) \\pi_{2} + p(E_{3} | T_{3}) \\pi_{3} = (0) \\pi_{1} + (0.7) \\pi_{2} + (0) \\pi_{3} = 0.7 \\pi_{2}$\n\nThe observed counts $(c_{E_{1}}, c_{E_{2}}, c_{E_{3}})$ follow a multinomial distribution with parameters $N=1000$ and probabilities $(p(E_{1}), p(E_{2}), p(E_{3}))$. The log-likelihood function $\\mathcal{L}(\\pi)$ of the observed data is, up to a constant term:\n$$\\mathcal{L}(\\pi) = \\sum_{j=1}^{3} c_{E_{j}} \\ln(p(E_{j}))$$\nSubstituting the expressions for $p(E_{j})$ and the given counts:\n$$\\mathcal{L}(\\pi_{1}, \\pi_{2}, \\pi_{3}) = 400 \\ln(\\pi_{1}) + 500 \\ln(0.3 \\pi_{2} + \\pi_{3}) + 100 \\ln(0.7 \\pi_{2})$$\nWe must maximize this function subject to the constraints $\\pi_{1} + \\pi_{2} + \\pi_{3} = 1$ and $\\pi_{i} \\geq 0$ for $i \\in \\{1, 2, 3\\}$.\n\nFrom the structure of $p(E_1)$, the term $400 \\ln(\\pi_1)$ is decoupled from the others. For a multinomial model, the MLE for a category probability is its observed frequency. The probability of a fragment landing in class $E_1$ is $\\pi_1$, and its observed frequency is $c_{E_1}/N$. Thus, the MLE for $\\pi_{1}$ is:\n$$\\hat{\\pi}_{1} = \\frac{c_{E_{1}}}{N} = \\frac{400}{1000} = 0.4$$\nThis simplifies the problem. The constraint on the remaining fractions becomes $\\pi_{2} + \\pi_{3} = 1 - \\hat{\\pi}_{1} = 1 - 0.4 = 0.6$. We can express $\\pi_{3}$ in terms of $\\pi_{2}$:\n$$\\pi_{3} = 0.6 - \\pi_{2}$$\nThe domain for $\\pi_{2}$ is $[0, 0.6]$. We substitute this into the part of the log-likelihood function involving $\\pi_{2}$ and $\\pi_{3}$, which we denote $\\mathcal{L}_{23}$:\n$$\\mathcal{L}_{23}(\\pi_{2}) = 500 \\ln(0.3 \\pi_{2} + (0.6 - \\pi_{2})) + 100 \\ln(0.7 \\pi_{2})$$\n$$\\mathcal{L}_{23}(\\pi_{2}) = 500 \\ln(0.6 - 0.7 \\pi_{2}) + 100 \\ln(0.7 \\pi_{2})$$\nTo find the maximum, we compute the derivative with respect to $\\pi_{2}$ and set it to zero:\n$$\\frac{d\\mathcal{L}_{23}}{d\\pi_{2}} = 500 \\cdot \\frac{-0.7}{0.6 - 0.7 \\pi_{2}} + 100 \\cdot \\frac{0.7}{0.7 \\pi_{2}} = \\frac{-350}{0.6 - 0.7 \\pi_{2}} + \\frac{100}{\\pi_{2}}$$\nSetting the derivative to zero:\n$$\\frac{100}{\\pi_{2}} = \\frac{350}{0.6 - 0.7 \\pi_{2}}$$\n$$100(0.6 - 0.7 \\pi_{2}) = 350 \\pi_{2}$$\n$$60 - 70 \\pi_{2} = 350 \\pi_{2}$$\n$$60 = 420 \\pi_{2}$$\n$$\\hat{\\pi}_{2} = \\frac{60}{420} = \\frac{1}{7}$$\nThis value is in the valid range $[0, 0.6]$. The second derivative is negative, confirming this is a maximum. Now we find the estimate for $\\pi_{3}$:\n$$\\hat{\\pi}_{3} = 0.6 - \\hat{\\pi}_{2} = \\frac{3}{5} - \\frac{1}{7} = \\frac{21 - 5}{35} = \\frac{16}{35}$$\nThe maximum likelihood estimates for the transcript fractions are $\\hat{\\pi} = (\\frac{2}{5}, \\frac{1}{7}, \\frac{16}{35})$.\n\nFinally, we convert these exact fractions to decimal values rounded to four significant figures as requested:\n- $\\hat{\\pi}_{1} = 0.4 = 0.4000$\n- $\\hat{\\pi}_{2} = \\frac{1}{7} \\approx 0.142857 \\dots \\rightarrow 0.1429$\n- $\\hat{\\pi}_{3} = \\frac{16}{35} \\approx 0.457142 \\dots \\rightarrow 0.4571$\nThe sum of the rounded values is $0.4000 + 0.1429 + 0.4571 = 1.0000$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.4000  0.1429  0.4571\n\\end{pmatrix}\n}\n$$", "id": "4614663"}, {"introduction": "While direct maximization is instructive for small examples, real-world transcriptomes with thousands of transcripts require an iterative approach. The Expectation-Maximization (EM) algorithm provides an elegant solution by treating the transcript-of-origin for each fragment as a hidden variable. This coding exercise [@problem_id:4614687] walks you through implementing one cycle of the E-step and M-step, offering a concrete, hands-on understanding of how modern quantification tools iteratively refine abundance estimates.", "problem": "You are given a probabilistic mixture-model setup for transcript abundance quantification in ribonucleic acid sequencing (RNA-seq). In this setting, a set of transcripts acts as mixture components with mixture proportions $\\pi_t$ that satisfy $\\sum_{t=1}^{m} \\pi_t = 1$ and $\\pi_t \\ge 0$ for all $t$. Each sequencing fragment (read) $i$ contributes nonnegative alignment weights $w_{it}$ that are proportional to the likelihood that fragment $i$ originates from transcript $t$. The Expectation-Maximization (EM) algorithm iteratively updates the mixture proportions $\\pi_t$ by alternating between computing posterior transcript-membership probabilities for each read (Expectation step) and averaging those probabilities to obtain new mixture proportions (Maximization step).\n\nFundamental base to use:\n- Probability mixture model: mixture proportions $\\pi_t$ with $\\sum_{t} \\pi_t = 1$ and $\\pi_t \\ge 0$.\n- Bayes rule: posterior membership for read $i$ in transcript $t$ is proportional to the product of prior and likelihood, namely $p(t|i) \\propto \\pi_t \\cdot p(i|t)$; here, $p(i|t)$ is represented by $w_{it}$ up to a proportional constant.\n- The updated mixture proportions are the average of the posterior membership probabilities across all included reads.\n\nImplementation requirements:\n- Implement a single EM iteration numerically using the above principles without introducing any additional heuristic corrections.\n- For each read $i$, compute the posterior membership probabilities $p(t|i)$ using $\\pi_t$ and $w_{it}$. If the normalizing denominator $\\sum_{s=1}^{m} \\pi_s \\cdot w_{is}$ equals $0$ for read $i$, treat that read as uninformative and exclude it from both the numerator and the denominator of the update (do not assign fractional membership for that read).\n- Let $n_{\\mathrm{incl}}$ be the count of included reads (those with strictly positive normalizing denominator). Update the mixture proportions to new values $\\pi_t^{(\\mathrm{new})}$ by averaging the posterior membership probabilities over the included reads.\n- Verify that the updated mixture proportions satisfy $\\sum_{t=1}^{m} \\pi_t^{(\\mathrm{new})} = 1$ within a numerical tolerance of $10^{-12}$, and report a boolean for each test indicating whether this normalization holds.\n- If $n_{\\mathrm{incl}} = 0$ (all reads are excluded), define $\\pi_t^{(\\mathrm{new})} := \\pi_t$ and perform the normalization check on the unchanged values.\n\nYour program must implement the above for the following test suite and produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this top-level list corresponds to one test case and must itself be a two-element list of the form $[\\text{updated\\_pi}, \\text{normalization\\_boolean}]$, where $\\text{updated\\_pi}$ is a list of floats (the updated $\\pi_t^{(\\mathrm{new})}$ values in transcript index order) and $\\text{normalization\\_boolean}$ is a boolean.\n\nTest suite:\n- Test case $1$ (happy path, mixed ambiguity):\n    - Transcripts $m = 3$.\n    - Reads $n = 4$.\n    - Initial mixture proportions $\\pi = [\\,0.5,\\,0.3,\\,0.2\\,]$.\n    - Alignment weights $w$ as rows by reads:\n        - Read $1$: $[\\,0.9,\\,0.1,\\,0.0\\,]$.\n        - Read $2$: $[\\,0.4,\\,0.4,\\,0.2\\,]$.\n        - Read $3$: $[\\,0.0,\\,0.5,\\,0.5\\,]$.\n        - Read $4$: $[\\,0.0,\\,0.0,\\,1.0\\,]$.\n- Test case $2$ (boundary: one read unmapped, deterministic reads):\n    - Transcripts $m = 2$.\n    - Reads $n = 3$.\n    - Initial mixture proportions $\\pi = [\\,0.6,\\,0.4\\,]$.\n    - Alignment weights $w$:\n        - Read $1$: $[\\,0.0,\\,0.0\\,]$.\n        - Read $2$: $[\\,1.0,\\,0.0\\,]$.\n        - Read $3$: $[\\,0.0,\\,1.0\\,]$.\n- Test case $3$ (edge: zero initial mixture weight for one transcript):\n    - Transcripts $m = 3$.\n    - Reads $n = 3$.\n    - Initial mixture proportions $\\pi = [\\,0.0,\\,0.7,\\,0.3\\,]$.\n    - Alignment weights $w$:\n        - Read $1$: $[\\,1.0,\\,0.0,\\,0.0\\,]$.\n        - Read $2$: $[\\,0.4,\\,0.6,\\,0.0\\,]$.\n        - Read $3$: $[\\,0.4,\\,0.0,\\,0.6\\,]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list $[\\,\\text{updated\\_pi},\\,\\text{normalization\\_boolean}\\,]$ for the three test cases in order. For example, the printed structure must look like $[[\\ldots],\\ldots]$ with exactly one top-level list containing three inner two-element lists.", "solution": "The user has provided a problem statement that requires the implementation of a single iteration of the Expectation-Maximization (EM) algorithm for a probabilistic mixture model used in transcript abundance quantification.\n\n### Step 1: Extract Givens\n\nThe provided information consists of:\n- **Model**: A probabilistic mixture model with $m$ transcripts as components.\n- **Mixture Proportions**: $\\pi_t$ for transcript $t$, where $\\sum_{t=1}^{m} \\pi_t = 1$ and $\\pi_t \\ge 0$.\n- **Data**: A set of $n$ sequencing fragments (reads).\n- **Alignment Weights**: Nonnegative weights $w_{it}$ proportional to the likelihood that read $i$ originates from transcript $t$.\n- **EM Algorithm Step (E-step)**: The posterior membership probability $p(t|i)$ is derived from Bayes' rule: $p(t|i) \\propto \\pi_t \\cdot p(i|t)$, where $p(i|t)$ is proportional to $w_{it}$.\n- **EM Algorithm Step (M-step)**: The updated mixture proportions $\\pi_t^{(\\mathrm{new})}$ are the average of the posterior membership probabilities over a set of included reads.\n- **Inclusion Criterion**: A read $i$ is included if its normalizing denominator, $\\sum_{s=1}^{m} \\pi_s \\cdot w_{is}$, is strictly greater than $0$. Let $n_{\\mathrm{incl}}$ be the number of included reads.\n- **Update Rule**: If $n_{\\mathrm{incl}}  0$, $\\pi_t^{(\\mathrm{new})} = \\frac{1}{n_{\\mathrm{incl}}} \\sum_{i \\in \\text{included reads}} p(t|i)$.\n- **Edge Case (No Included Reads)**: If $n_{\\mathrm{incl}} = 0$, the updated proportions are set to the initial proportions: $\\pi_t^{(\\mathrm{new})} := \\pi_t$.\n- **Verification**: Check if the updated proportions sum to $1$ within a numerical tolerance of $10^{-12}$: $|\\sum_{t=1}^{m} \\pi_t^{(\\mathrm{new})} - 1| \\le 10^{-12}$.\n- **Test Suite**:\n    - **Test case 1**: $m = 3$, $n = 4$, $\\pi = [\\,0.5,\\,0.3,\\,0.2\\,]$, $w = [[\\,0.9,\\,0.1,\\,0.0\\,], [\\,0.4,\\,0.4,\\,0.2\\,], [\\,0.0,\\,0.5,\\,0.5\\,], [\\,0.0,\\,0.0,\\,1.0\\,]]$.\n    - **Test case 2**: $m = 2$, $n = 3$, $\\pi = [\\,0.6,\\,0.4\\,]$, $w = [[\\,0.0,\\,0.0\\,], [\\,1.0,\\,0.0\\,], [\\,0.0,\\,1.0\\,]]$.\n    - **Test case 3**: $m = 3$, $n = 3$, $\\pi = [\\,0.0,\\,0.7,\\,0.3\\,]$, $w = [[\\,1.0,\\,0.0,\\,0.0\\,], [\\,0.4,\\,0.6,\\,0.0\\,], [\\,0.4,\\,0.0,\\,0.6\\,]]$.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem describes a simplified but standard application of the Expectation-Maximization algorithm to a finite mixture model. This technique is fundamental in statistics and is widely used in bioinformatics for problems like transcript quantification. The formulation using Bayes' rule for the E-step and averaging posteriors for the M-step is correct for this model.\n- **Well-Posed**: The problem is well-posed. It provides all necessary inputs (initial proportions $\\pi$, alignment weights $w$) and a clear, deterministic procedure for calculating the output. The rules for handling edge cases (denominator of $0$, no includable reads) are explicitly defined, ensuring a unique solution exists for any valid input.\n- **Objective**: The problem is stated in precise, objective mathematical and computational terms. There are no subjective or ambiguous statements.\n\nThe problem statement does not violate any of the specified invalidity criteria. It is scientifically sound, formally specified, complete, and computationally feasible.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution will be provided.\n\n### Principle-Based Design and Solution\n\nThe task is to implement a single iteration of the Expectation-Maximization (EM) algorithm to update transcript abundance estimates, represented by mixture proportions $\\pi_t$. The algorithm consists of two steps: the Expectation (E) step and the Maximization (M) step.\n\nLet $\\pi = (\\pi_1, \\pi_2, \\dots, \\pi_m)$ be the vector of initial mixture proportions for $m$ transcripts. Let $W$ be the $n \\times m$ matrix of alignment weights, where $W_{it} = w_{it}$ is proportional to the likelihood of observing read $i$ given it originated from transcript $t$.\n\n**Expectation (E) Step: Compute Posterior Probabilities**\n\nFor each read $i$ (from $i=1$ to $n$), we compute the posterior probability that it originated from transcript $t$ (for $t=1$ to $m$), given the current estimates of $\\pi_t$. Using Bayes' rule, the posterior $p(t|i)$ is proportional to the prior $\\pi_t$ times the likelihood $p(i|t)$, which is represented by $w_{it}$.\n\n$p(t|i) = \\frac{p(t) p(i|t)}{\\sum_{s=1}^{m} p(s) p(i|s)} \\propto \\frac{\\pi_t \\cdot w_{it}}{\\sum_{s=1}^{m} \\pi_s \\cdot w_{is}}$\n\nLet $Z_i = \\sum_{s=1}^{m} \\pi_s \\cdot w_{is}$ be the normalizing constant for read $i$. This term represents the marginal likelihood of observing read $i$, summed over all possible transcript origins.\n\nA crucial part of the procedure is handling reads that cannot be probabilistically assigned. If $Z_i=0$, it implies that for every transcript $s$, either its initial proportion $\\pi_s$ is $0$ or its alignment weight $w_{is}$ is $0$. Such a read provides no information for updating the proportions and must be excluded from the M-step calculation. Reads for which $Z_i  0$ are considered \"included\". Let the set of included reads be denoted by $\\mathcal{I}$. The number of included reads is $n_{\\mathrm{incl}} = |\\mathcal{I}|$.\n\nFor each included read $i \\in \\mathcal{I}$, the posterior probability is:\n$$ p(t|i) = \\frac{\\pi_t \\cdot w_{it}}{Z_i} $$\nFor excluded reads ($i \\notin \\mathcal{I}$), these posteriors are undefined.\n\n**Maximization (M) Step: Update Mixture Proportions**\n\nThe M-step updates the mixture proportions $\\pi_t$ to new values $\\pi_t^{(\\mathrm{new})}$ by maximizing the expected log-likelihood. For this mixture model, this simplifies to averaging the posterior probabilities (responsibilities) over the included reads.\n\nIf $n_{\\mathrm{incl}}  0$, the update rule is:\n$$ \\pi_t^{(\\mathrm{new})} = \\frac{1}{n_{\\mathrm{incl}}} \\sum_{i \\in \\mathcal{I}} p(t|i) $$\nThis calculation intuitively re-estimates the proportion of a transcript as the average fraction of reads assigned to it.\n\nIf $n_{\\mathrm{incl}} = 0$, no reads can inform an update. The problem specifies that in this case, the proportions remain unchanged:\n$$ \\pi_t^{(\\mathrm{new})} = \\pi_t $$\n\n**Normalization Check**\n\nA fundamental property of the EM algorithm for mixture models is that if the initial proportions $\\pi_t$ sum to $1$, the updated proportions $\\pi_t^{(\\mathrm{new})}$ will also automatically sum to $1$ (barring floating-point inaccuracies). We can prove this:\n$$ \\sum_{t=1}^{m} \\pi_t^{(\\mathrm{new})} = \\sum_{t=1}^{m} \\frac{1}{n_{\\mathrm{incl}}} \\sum_{i \\in \\mathcal{I}} p(t|i) = \\frac{1}{n_{\\mathrm{incl}}} \\sum_{i \\in \\mathcal{I}} \\sum_{t=1}^{m} p(t|i) $$\nSince $\\sum_{t=1}^{m} p(t|i) = \\sum_{t=1}^{m} \\frac{\\pi_t w_{it}}{Z_i} = \\frac{1}{Z_i} \\sum_{t=1}^{m} \\pi_t w_{it} = \\frac{Z_i}{Z_i} = 1$ for any included read $i$, the sum becomes:\n$$ \\sum_{t=1}^{m} \\pi_t^{(\\mathrm{new})} = \\frac{1}{n_{\\mathrm{incl}}} \\sum_{i \\in \\mathcal{I}} 1 = \\frac{n_{\\mathrm{incl}}}{n_{\\mathrm{incl}}} = 1 $$\nWe must verify this property numerically by checking if $|\\sum_{t=1}^{m} \\pi_t^{(\\mathrm{new})} - 1| \\le 10^{-12}$.\n\n**Implementation Using Matrix Operations**\n\nThe process can be efficiently implemented using vector and matrix operations.\n1.  Represent initial proportions as a $1 \\times m$ vector $\\boldsymbol{\\pi}$ and weights as an $n \\times m$ matrix $W$.\n2.  Compute the numerators for all reads and transcripts simultaneously: $N = \\boldsymbol{\\pi} \\odot W$, where $\\odot$ is element-wise multiplication (broadcasting $\\boldsymbol{\\pi}$ across the rows of $W$). This results in an $n \\times m$ matrix $N$ where $N_{it} = \\pi_t \\cdot w_{it}$.\n3.  Compute the denominators for each read by summing the rows of $N$: $\\mathbf{z} = \\sum_{t=1}^{m} N_{it}$ for each $i$. This gives an $n \\times 1$ vector of denominators.\n4.  Identify the included reads where $z_i  0$. Let this be a boolean mask.\n5.  If no reads are included, $\\boldsymbol{\\pi}^{(\\mathrm{new})} = \\boldsymbol{\\pi}$.\n6.  If reads are included, filter the rows of $N$ and the elements of $\\mathbf{z}$ corresponding to included reads. Let these be $N_{\\mathrm{incl}}$ and $\\mathbf{z}_{\\mathrm{incl}}$.\n7.  Calculate the posterior probability matrix for included reads: $P_{\\mathrm{incl}} = N_{\\mathrm{incl}} \\oslash \\mathbf{z}_{\\mathrm{incl}}$, where $\\oslash$ denotes element-wise division with broadcasting of the column vector $\\mathbf{z}_{\\mathrm{incl}}$.\n8.  Calculate the new proportions $\\boldsymbol{\\pi}^{(\\mathrm{new})}$ by taking the column-wise mean of $P_{\\mathrm{incl}}$.\n9.  Perform the normalization check on $\\boldsymbol{\\pi}^{(\\mathrm{new})}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the transcript abundance quantification problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (happy path, mixed ambiguity)\n        {\n            \"pi_initial\": [0.5, 0.3, 0.2],\n            \"weights\": [\n                [0.9, 0.1, 0.0],\n                [0.4, 0.4, 0.2],\n                [0.0, 0.5, 0.5],\n                [0.0, 0.0, 1.0],\n            ],\n        },\n        # Test case 2 (boundary: one read unmapped, deterministic reads)\n        {\n            \"pi_initial\": [0.6, 0.4],\n            \"weights\": [\n                [0.0, 0.0],\n                [1.0, 0.0],\n                [0.0, 1.0],\n            ],\n        },\n        # Test case 3 (edge: zero initial mixture weight for one transcript)\n        {\n            \"pi_initial\": [0.0, 0.7, 0.3],\n            \"weights\": [\n                [1.0, 0.0, 0.0],\n                [0.4, 0.6, 0.0],\n                [0.4, 0.0, 0.6],\n            ],\n        },\n    ]\n\n    results = []\n    \n    # Numerical tolerance for normalization check\n    TOLERANCE = 1e-12\n\n    for case in test_cases:\n        pi = np.array(case[\"pi_initial\"], dtype=np.float64)\n        W = np.array(case[\"weights\"], dtype=np.float64)\n\n        # E-step: Calculate unnormalized posterior probabilities (numerators)\n        # This uses broadcasting: (1, m) * (n, m) - (n, m)\n        numerators = pi * W\n        \n        # Calculate normalizing denominators for each read\n        denominators = np.sum(numerators, axis=1)\n\n        # Identify included reads (those with a positive denominator)\n        included_mask = denominators  0\n        n_incl = np.sum(included_mask)\n\n        if n_incl == 0:\n            # If no reads are included, proportions remain unchanged\n            pi_new = pi\n        else:\n            # Filter to include only informative reads\n            included_numerators = numerators[included_mask]\n            included_denominators = denominators[included_mask]\n            \n            # E-step completion: Calculate posterior probabilities (responsibilities)\n            # for included reads. Shape of included_denominators is (n_incl,),\n            # we reshape to (n_incl, 1) for broadcasting.\n            posteriors = included_numerators / included_denominators[:, np.newaxis]\n\n            # M-step: Update mixture proportions by averaging posteriors\n            pi_new = np.sum(posteriors, axis=0) / n_incl\n\n        # Verify that the new proportions sum to 1 within the specified tolerance\n        normalization_check = np.isclose(np.sum(pi_new), 1.0, atol=TOLERANCE, rtol=0)\n\n        # Format the result as required: [list_of_floats, boolean]\n        result_pair = [pi_new.tolist(), bool(normalization_check)]\n        results.append(result_pair)\n\n    # Custom function to format the final list to avoid extra spaces\n    def format_list(data):\n        if isinstance(data, list):\n            return f\"[{','.join(map(format_list, data))}]\"\n        elif isinstance(data, float):\n            return f\"{data:.16g}\"  # Use general format for floats\n        elif isinstance(data, bool):\n            return \"true\" if data else \"false\"\n        else:\n            return str(data)\n            \n    # The problem asks for standard list string representation, which `str` produces\n    # with spaces. The example output format `[[...],...]` implies a compact\n    # representation.\n    final_output_str = str(results).replace(\" \", \"\").replace(\"True\", \"true\").replace(\"False\", \"false\")\n\n    # Final print statement in the exact required format.\n    print(final_output_str)\n\nsolve()\n```", "id": "4614687"}, {"introduction": "The accuracy of abundance estimation depends not only on the algorithm but also on the statistical properties of the model itself. When different transcripts, such as isoforms, have very similar sequences, their compatibility profiles with read equivalence classes become nearly collinear, making the estimation problem ill-conditioned and the results unstable. This exercise [@problem_id:4614682] challenges you to diagnose this critical issue and explore how regularization techniques, such as a ridge penalty, can stabilize the EM algorithm to produce robust and reliable estimates.", "problem": "A laboratory quantifies transcript abundance from ribonucleic acid sequencing (RNA-seq) reads using a fragment equivalence class model. There are $J=4$ equivalence classes and $K=3$ transcripts. Let $A=\\left[a_{jk}\\right] \\in \\mathbb{R}_{\\ge 0}^{J \\times K}$ be the known transcript compatibility weight matrix, where $a_{jk}$ reflects the effective contribution of transcript $k$ to class $j$. The observed fragment counts per class are $\\mathbf{y}=(y_1,y_2,y_3,y_4)^\\top$. Consider the following scientifically plausible instance:\n- The compatibility matrix is\n$$\nA \\;=\\;\n\\begin{bmatrix}\n1.00  0.98  0.00\\\\\n0.99  1.00  0.00\\\\\n0.01  0.01  1.00\\\\\n0.50  0.49  0.02\n\\end{bmatrix}.\n$$\n- The observed counts are $\\mathbf{y}=(300,\\,290,\\,10,\\,200)^\\top$.\n\nAssume the standard independent Poisson sampling model: for each class $j \\in \\{1,\\dots,J\\}$,\n$$\ny_j \\;\\sim\\; \\mathrm{Poisson}\\!\\left(\\mu_j\\right), \\quad \\text{with } \\mu_j \\;=\\; \\sum_{k=1}^{K} a_{jk}\\,\\theta_k,\n$$\nwhere $\\theta_k \\ge 0$ denotes the unknown abundance parameter proportional to the expected number of fragments generated by transcript $k$. A common estimation approach is the Expectation-Maximization (EM) algorithm, using latent allocations $y_{jk}$ such that $y_{jk} \\sim \\mathrm{Poisson}(a_{jk}\\theta_k)$ independently and $y_j=\\sum_{k=1}^{K} y_{jk}$.\n\nIn this setup, near-collinearity among columns of $A$ can render the maximum likelihood estimation ill-conditioned. A proposed remedy is to augment the maximization step with a ridge penalty (squared $\\ell_2$ norm) of strength $\\lambda0$ on the abundance parameters.\n\nWhich of the following statements are correct in this setting?\n\nA. The first and second columns of $A$ are nearly proportional across classes, implying that the $2\\times 2$ principal submatrix of the Fisher information for $(\\theta_1,\\theta_2)$ is nearly singular. As a result, the unregularized EM may move along a statistically flat ridge, and small perturbations in $\\mathbf{y}$ can induce large swings in $(\\theta_1,\\theta_2)$.\n\nB. Reparameterizing to mixture proportions $\\pi_k=\\theta_k/\\sum_{\\ell=1}^{K}\\theta_\\ell$ and placing a symmetric $\\mathrm{Dirichlet}(\\lambda,\\lambda,\\lambda)$ prior on $\\boldsymbol{\\pi}$ is algebraically equivalent to adding a ridge penalty $\\lambda \\sum_{k=1}^{K} \\theta_k^2$ in the Poisson-rate parameterization, yielding identical EM maximization updates for $\\boldsymbol{\\theta}$.\n\nC. Under the Poisson latent-variable EM with a ridge penalty $\\lambda \\sum_{k=1}^{K} \\theta_k^2$ on $\\boldsymbol{\\theta}$, the penalized maximization step decouples by transcript and, for each $k$, solves a quadratic equation\n$$\n2\\lambda\\,\\theta_k^2 + b_k\\,\\theta_k - c_k \\;=\\; 0,\\quad \\text{where } b_k=\\sum_{j=1}^{J} a_{jk},\\;\\; c_k=\\sum_{j=1}^{J} \\mathbb{E}\\!\\left[y_{jk}\\mid \\mathbf{y},\\boldsymbol{\\theta}\\right],\n$$\nso that the unique positive solution is\n$$\n\\theta_k^{\\mathrm{new}} \\;=\\; \\frac{-\\,b_k + \\sqrt{b_k^2 + 8\\lambda\\,c_k}}{4\\lambda}.\n$$\n\nD. Pre-scaling each column of $A$ to have unit $\\ell_2$ norm guarantees orthogonality of the columns, thereby eliminating near-collinearity and restoring a well-conditioned maximum likelihood estimator without any penalty.\n\nE. If one instead penalizes the log-abundances $\\eta_k=\\log \\theta_k$ via a quadratic penalty $\\lambda \\sum_{k=1}^{K}\\eta_k^2$, then the penalized EM maximization step retains a closed-form solution analogous to the unpenalized case, avoiding the need for inner numerical optimization.", "solution": "### Step 1: Extract Givens\n-   Number of equivalence classes: $J=4$.\n-   Number of transcripts: $K=3$.\n-   Transcript compatibility weight matrix: $A=\\left[a_{jk}\\right] \\in \\mathbb{R}_{\\ge 0}^{J \\times K}$, where\n    $$\n    A \\;=\\;\n    \\begin{bmatrix}\n    1.00  0.98  0.00\\\\\n    0.99  1.00  0.00\\\\\n    0.01  0.01  1.00\\\\\n    0.50  0.49  0.02\n    \\end{bmatrix}.\n    $$\n-   Observed fragment counts: $\\mathbf{y}=(y_1,y_2,y_3,y_4)^\\top = (300,\\,290,\\,10,\\,200)^\\top$.\n-   Statistical model: $y_j \\sim \\mathrm{Poisson}(\\mu_j)$ for $j \\in \\{1, 2, 3, 4\\}$, with mean $\\mu_j = \\sum_{k=1}^{3} a_{jk}\\,\\theta_k$.\n-   Parameters to be estimated: $\\theta_k \\ge 0$, the abundance parameter for transcript $k \\in \\{1, 2, 3\\}$.\n-   Estimation method: Expectation-Maximization (EM) algorithm based on latent allocations $y_{jk} \\sim \\mathrm{Poisson}(a_{jk}\\theta_k)$ where $y_j = \\sum_{k=1}^{K} y_{jk}$.\n-   Identified problem: Near-collinearity of columns in $A$ may lead to an ill-conditioned maximum likelihood estimation.\n-   Proposed solution: Augment the maximization step with a ridge penalty of strength $\\lambda  0$, i.e., penalize a term proportional to $\\sum_{k=1}^{K} \\theta_k^2$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement describes a standard statistical model used in bioinformatics for RNA-seq data quantification.\n-   **Scientifically Grounded**: The model is a well-established Poisson generalized linear model, and the use of equivalence classes and an EM algorithm is central to modern quantification tools like Salmon and Kallisto. The issue of multicollinearity arising from similar transcripts (isoforms) is a real and critical challenge in this field. The use of ridge regularization is a standard statistical technique to address such issues. The provided matrix $A$ and count vector $\\mathbf{y}$ are numerically plausible for a small-scale example. The setup is scientifically and mathematically sound.\n-   **Well-Posed**: The problem is a well-defined statistical estimation problem. The data, model, and parameters are clearly specified. The question asks for an evaluation of several statements related to this setup, which is a valid form of inquiry.\n-   **Objective**: The problem is stated in precise, objective mathematical and statistical terms.\n\nThe problem statement is free of the invalidating flaws listed in the instructions. It is a valid, well-posed, and scientifically relevant problem.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution will proceed by analyzing each of the provided options.\n\n### Option-by-Option Analysis\n\n**A. The first and second columns of $A$ are nearly proportional across classes, implying that the $2\\times 2$ principal submatrix of the Fisher information for $(\\theta_1,\\theta_2)$ is nearly singular. As a result, the unregularized EM may move along a statistically flat ridge, and small perturbations in $\\mathbf{y}$ can induce large swings in $(\\theta_1,\\theta_2)$.**\n\nThe first two columns of $A$, denoted $\\mathbf{a}_1$ and $\\mathbf{a}_2$, are $\\mathbf{a}_1 = (1.00, 0.99, 0.01, 0.50)^\\top$ and $\\mathbf{a}_2 = (0.98, 1.00, 0.01, 0.49)^\\top$. These vectors are visually very similar. To quantify this, we can compute their cosine similarity:\n$$ \\cos(\\mathbf{a}_1, \\mathbf{a}_2) = \\frac{\\mathbf{a}_1 \\cdot \\mathbf{a}_2}{\\|\\mathbf{a}_1\\|_2 \\|\\mathbf{a}_2\\|_2} $$\nThe dot product is $\\mathbf{a}_1 \\cdot \\mathbf{a}_2 = (1.00)(0.98) + (0.99)(1.00) + (0.01)(0.01) + (0.50)(0.49) = 0.98 + 0.99 + 0.0001 + 0.245 = 2.2151$.\nThe squared norms are:\n$\\|\\mathbf{a}_1\\|_2^2 = 1.00^2 + 0.99^2 + 0.01^2 + 0.50^2 = 1.00 + 0.9801 + 0.0001 + 0.25 = 2.2302$.\n$\\|\\mathbfa_2\\|_2^2 = 0.98^2 + 1.00^2 + 0.01^2 + 0.49^2 = 0.9604 + 1.00 + 0.0001 + 0.2401 = 2.2006$.\nThe cosine similarity is $\\frac{2.2151}{\\sqrt{2.2302 \\cdot 2.2006}} = \\frac{2.2151}{\\sqrt{4.9080...}} \\approx 0.99986$.\nA cosine similarity this close to $1$ indicates near-perfect collinearity. This means $\\mathbf{a}_1 \\approx c \\mathbf{a}_2$ for some constant $c$ (here $c \\approx 1$).\n\nThe Fisher information matrix $I(\\boldsymbol{\\theta})$ for the Poisson regression model $y_j \\sim \\mathrm{Poisson}(\\mu_j)$ with $\\mu_j=\\sum_k a_{jk}\\theta_k$ has elements $I_{k,k'} = \\sum_{j=1}^J \\frac{a_{jk}a_{jk'}}{\\mu_j}$.\nThe $2 \\times 2$ principal submatrix for $(\\theta_1, \\theta_2)$ is:\n$$\nI(\\theta_1, \\theta_2) =\n\\begin{bmatrix}\n\\sum_j a_{j1}^2/\\mu_j  \\sum_j a_{j1}a_{j2}/\\mu_j \\\\\n\\sum_j a_{j1}a_{j2}/\\mu_j  \\sum_j a_{j2}^2/\\mu_j\n\\end{bmatrix}\n$$\nIf $\\mathbf{a}_1 \\approx c \\mathbf{a}_2$, then $a_{j1} \\approx c a_{j2}$ for all $j$. The determinant of the submatrix is $(\\sum_j a_{j1}^2/\\mu_j)(\\sum_j a_{j2}^2/\\mu_j) - (\\sum_j a_{j1}a_{j2}/\\mu_j)^2$. By the Cauchy-Schwarz inequality, this determinant is non-negative. It becomes zero if and only if the vectors $(a_{11}/\\sqrt{\\mu_1}, \\dots, a_{J1}/\\sqrt{\\mu_J})$ and $(a_{12}/\\sqrt{\\mu_1}, \\dots, a_{J2}/\\sqrt{\\mu_J})$ are linearly dependent. Near-collinearity of $\\mathbf{a}_1$ and $\\mathbf{a}_2$ implies near-linear-dependence of these weighted vectors, making the determinant close to zero. Thus, the submatrix is nearly singular.\n\nA nearly singular Fisher information matrix implies that the log-likelihood surface is very flat in the direction of the near-null space. This creates a \"ridge\" in the likelihood, where many combinations of $(\\theta_1, \\theta_2)$ yield almost the same likelihood. The unregularized EM algorithm, which attempts to ascend this surface, will converge extremely slowly and be highly sensitive to small changes in the data $\\mathbf{y}$, as these can shift the location of the flat maximum. This leads to large, unstable swings in the estimates of $\\theta_1$ and $\\theta_2$. The statement is a correct diagnosis of multicollinearity in this context.\n\n**Verdict on A:** Correct.\n\n**B. Reparameterizing to mixture proportions $\\pi_k=\\theta_k/\\sum_{\\ell=1}^{K}\\theta_\\ell$ and placing a symmetric $\\mathrm{Dirichlet}(\\lambda,\\lambda,\\lambda)$ prior on $\\boldsymbol{\\pi}$ is algebraically equivalent to adding a ridge penalty $\\lambda \\sum_{k=1}^{K} \\theta_k^2$ in the Poisson-rate parameterization, yielding identical EM maximization updates for $\\boldsymbol{\\theta}$.**\n\nThis statement claims two different regularization approaches are equivalent.\n1.  **Ridge penalty**: The penalized log-likelihood is $\\log P(\\mathbf{y}|\\boldsymbol{\\theta}) - \\lambda \\sum_k \\theta_k^2$. This corresponds to maximizing a posterior with a log-prior of $-\\lambda \\sum_k \\theta_k^2$, i.e., $P(\\boldsymbol{\\theta}) \\propto \\exp(-\\lambda \\sum_k \\theta_k^2)$. This is a product of independent (truncated) Gaussian priors on $\\theta_k$.\n2.  **Dirichlet prior**: A prior on the proportions $\\boldsymbol{\\pi} \\sim \\mathrm{Dirichlet}(\\lambda, \\dots, \\lambda)$ has a probability density function $P(\\boldsymbol{\\pi}) \\propto \\prod_k \\pi_k^{\\lambda-1}$. In terms of $\\boldsymbol{\\theta}$, where $\\pi_k = \\theta_k / \\sum_l \\theta_l$, the log-prior is $\\log P(\\boldsymbol{\\theta}) \\propto \\sum_k (\\lambda-1) \\log \\pi_k = (\\lambda-1) \\sum_k \\log(\\frac{\\theta_k}{\\sum_l \\theta_l}) = (\\lambda-1)(\\sum_k \\log \\theta_k - K \\log(\\sum_l\\theta_l))$.\n\nThe penalty term from the ridge approach is quadratic in $\\theta_k$: $-\\lambda \\theta_k^2$.\nThe penalty term (log-prior) from the Dirichlet approach is logarithmic: $(\\lambda-1)\\log\\theta_k$ plus a term involving $\\log(\\sum_l \\theta_l)$.\nThese two penalties have different functional forms. A quadratic penalty shrinks parameters toward zero. A Dirichlet prior (for $\\lambda  1$) pushes proportions away from $0$ and $1$, encouraging more uniform estimates. They are fundamentally different and will result in different M-step updates and different final estimates. The statement of algebraic equivalence is false.\n\n**Verdict on B:** Incorrect.\n\n**C. Under the Poisson latent-variable EM with a ridge penalty $\\lambda \\sum_{k=1}^{K} \\theta_k^2$ on $\\boldsymbol{\\theta}$, the penalized maximization step decouples by transcript and, for each $k$, solves a quadratic equation $2\\lambda\\,\\theta_k^2 + b_k\\,\\theta_k - c_k \\;=\\; 0$, where $b_k=\\sum_{j=1}^{J} a_{jk}$ and $c_k=\\sum_{j=1}^{J} \\mathbb{E}[y_{jk}\\mid \\mathbf{y},\\boldsymbol{\\theta}]$, so that the unique positive solution is $\\theta_k^{\\mathrm{new}} \\;=\\; \\frac{-\\,b_k + \\sqrt{b_k^2 + 8\\lambda\\,c_k}}{4\\lambda}$.**\n\nIn the EM algorithm, the E-step computes the expectation of the complete-data log-likelihood. The M-step then maximizes this expectation. The penalized M-step maximizes:\n$$ \\tilde{Q}(\\boldsymbol{\\theta}) = \\mathbb{E}[L_c(\\boldsymbol{\\theta})|\\mathbf{y}, \\boldsymbol{\\theta}^{\\text{old}}] - \\lambda \\sum_{k=1}^K \\theta_k^2 $$\nThe complete-data log-likelihood is $L_c(\\boldsymbol{\\theta}) = \\sum_j \\sum_k (y_{jk} \\log(a_{jk}\\theta_k) - a_{jk}\\theta_k - \\log(y_{jk}!))$.\nIts expectation, the Q-function, is:\n$$ Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{\\text{old}}) = \\sum_{j,k} (\\mathbb{E}[y_{jk}|\\mathbf{y}, \\boldsymbol{\\theta}^{\\text{old}}]\\log\\theta_k - a_{jk}\\theta_k) + \\text{const} $$\nLet $c_k = \\sum_j \\mathbb{E}[y_{jk}|\\mathbf{y}, \\boldsymbol{\\theta}^{\\text{old}}]$. The penalized objective for a single $\\theta_k$ is:\n$$ \\tilde{Q}_k(\\theta_k) = c_k \\log\\theta_k - \\left(\\sum_j a_{jk}\\right)\\theta_k - \\lambda \\theta_k^2 $$\nTo maximize, we set the derivative with respect to $\\theta_k$ to zero:\n$$ \\frac{\\partial \\tilde{Q}_k}{\\partial \\theta_k} = \\frac{c_k}{\\theta_k} - \\sum_j a_{jk} - 2\\lambda\\theta_k = 0 $$\nMultiplying by $\\theta_k$ (since we seek $\\theta_k0$) gives:\n$$ c_k - (\\sum_j a_{jk})\\theta_k - 2\\lambda\\theta_k^2 = 0 $$\nRearranging gives the quadratic equation:\n$$ 2\\lambda\\theta_k^2 + (\\sum_j a_{jk})\\theta_k - c_k = 0 $$\nThis matches the form $2\\lambda\\,\\theta_k^2 + b_k\\,\\theta_k - c_k = 0$ with $b_k = \\sum_j a_{jk}$ and $c_k$ as defined in the option (where $\\boldsymbol{\\theta}$ in the expectation is the old value $\\boldsymbol{\\theta}^{\\text{old}}$).\nUsing the quadratic formula for $Ax^2+Bx+C=0 \\implies x=\\frac{-B\\pm\\sqrt{B^2-4AC}}{2A}$, with $A=2\\lambda$, $B=b_k$, and $C=-c_k$:\n$$ \\theta_k = \\frac{-b_k \\pm \\sqrt{b_k^2 - 4(2\\lambda)(-c_k)}}{2(2\\lambda)} = \\frac{-b_k \\pm \\sqrt{b_k^2 + 8\\lambda c_k}}{4\\lambda} $$\nSince $a_{jk} \\ge 0$, we have $b_k \\ge 0$. Also, $c_k = \\sum_j \\mathbb{E}[y_{jk}|...] \\ge 0$ and $\\lambda  0$. The discriminant $b_k^2 + 8\\lambda c_k$ is positive.\nThe solution $\\frac{-b_k - \\sqrt{b_k^2 + 8\\lambda c_k}}{4\\lambda}$ is always negative (as $b_k \\ge 0$).\nThe solution $\\frac{-b_k + \\sqrt{b_k^2 + 8\\lambda c_k}}{4\\lambda}$ is positive because $\\sqrt{b_k^2 + 8\\lambda c_k}  \\sqrt{b_k^2} = |b_k| = b_k$.\nTherefore, this is the unique positive solution for $\\theta_k^{\\text{new}}$. The statement is entirely correct.\n\n**Verdict on C:** Correct.\n\n**D. Pre-scaling each column of $A$ to have unit $\\ell_2$ norm guarantees orthogonality of the columns, thereby eliminating near-collinearity and restoring a well-conditioned maximum likelihood estimator without any penalty.**\n\nThis statement claims that normalizing the columns of $A$ to unit length will make them orthogonal. Let $\\mathbf{a}_k$ be the $k$-th column vector of $A$. The proposed transformation is to replace $\\mathbf{a}_k$ with $\\mathbf{a}'_k = \\mathbf{a}_k / \\|\\mathbf{a}_k\\|_2$. Two vectors $\\mathbf{a}'_i$ and $\\mathbf{a}'_j$ are orthogonal if their dot product is zero: $\\mathbf{a}'_i \\cdot \\mathbf{a}'_j = 0$.\nThe dot product of the normalized vectors is:\n$$ \\mathbf{a}'_i \\cdot \\mathbf{a}'_j = \\frac{\\mathbf{a}_i}{\\|\\mathbf{a}_i\\|_2} \\cdot \\frac{\\mathbf{a}_j}{\\|\\mathbf{a}_j\\|_2} = \\frac{\\mathbf{a}_i \\cdot \\mathbf{a}_j}{\\|\\mathbf{a}_i\\|_2 \\|\\mathbf{a}_j\\|_2} = \\cos(\\mathbf{a}_i, \\mathbf{a}_j) $$\nThis is the cosine of the angle between the original vectors. Normalization does not change the angle between vectors. Therefore, it does not induce orthogonality unless the vectors were already orthogonal.\nAs calculated for option A, the cosine similarity between $\\mathbf{a}_1$ and $\\mathbf{a}_2$ is $\\approx 0.99986$, which is very close to $1$, not $0$. After normalization, the dot product of $\\mathbf{a}'_1$ and $\\mathbf{a}'_2$ will still be $\\approx 0.99986$. The columns remain nearly collinear. Consequently, the claim that this procedure eliminates near-collinearity is false. The corresponding MLE would remain ill-conditioned.\n\n**Verdict on D:** Incorrect.\n\n**E. If one instead penalizes the log-abundances $\\eta_k=\\log \\theta_k$ via a quadratic penalty $\\lambda \\sum_{k=1}^{K}\\eta_k^2$, then the penalized EM maximization step retains a closed-form solution analogous to the unpenalized case, avoiding the need for inner numerical optimization.**\n\nLet's derive the M-step update for this new penalty. The penalized Q-function for $\\theta_k$ is:\n$$ \\tilde{Q}_k(\\theta_k) = \\left(\\sum_j \\mathbb{E}[y_{jk}|...]\\right) \\log\\theta_k - \\left(\\sum_j a_{jk}\\right)\\theta_k - \\lambda (\\log\\theta_k)^2 $$\nLet $c_k = \\sum_j \\mathbb{E}[y_{jk}|...]$ and $b_k = \\sum_j a_{jk}$.\n$$ \\tilde{Q}_k(\\theta_k) = c_k \\log\\theta_k - b_k \\theta_k - \\lambda (\\log\\theta_k)^2 $$\nTo maximize, we set the derivative w.r.t. $\\theta_k$ to zero:\n$$ \\frac{\\partial \\tilde{Q}_k}{\\partial \\theta_k} = \\frac{c_k}{\\theta_k} - b_k - 2\\lambda (\\log\\theta_k) \\cdot \\frac{1}{\\theta_k} = 0 $$\nMultiplying by $\\theta_k$ gives:\n$$ c_k - b_k \\theta_k - 2\\lambda \\log\\theta_k = 0 $$\n$$ b_k \\theta_k + 2\\lambda \\log\\theta_k = c_k $$\nThis is a transcendental equation because it mixes polynomial and logarithmic terms of the unknown $\\theta_k$. It cannot be solved for $\\theta_k$ using elementary functions. The unpenalized case yields a simple ratio, and the ridge penalty on $\\theta_k$ yields a quadratic equation. This case does not have such a \"closed-form solution\". The M-step would require a numerical solver (like Newton-Raphson) for each $\\theta_k$, which contradicts the claim that inner numerical optimization is avoided.\n\n**Verdict on E:** Incorrect.", "answer": "$$\\boxed{AC}$$", "id": "4614682"}]}