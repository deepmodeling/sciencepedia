{"hands_on_practices": [{"introduction": "Before diving into individual variant metrics, a crucial first step is to assess the overall quality of a variant callset. The transition-to-transversion (Ti/Tv) ratio serves as a powerful, high-level sanity check, reflecting both underlying mutational biology and potential systemic errors in sequencing or analysis. This exercise guides you through calculating the Ti/Tv ratio from a list of observed single-nucleotide variants and interpreting its value against established genomic benchmarks. [@problem_id:4617272]", "problem": "A variant callset was generated from human whole-genome sequencing (average coverage $30\\times$) and filtered to include only high-confidence biallelic single-nucleotide variants (SNVs). Insertions and deletions (indels) and multi-allelic sites were excluded. In single-stranded, directed notation, the observed counts of reference-to-alternate base changes are:\n\n- $A \\rightarrow G$: $850{,}250$\n- $G \\rightarrow A$: $845{,}700$\n- $C \\rightarrow T$: $832{,}400$\n- $T \\rightarrow C$: $838{,}100$\n- $A \\rightarrow C$: $220{,}500$\n- $C \\rightarrow A$: $222{,}300$\n- $A \\rightarrow T$: $180{,}200$\n- $T \\rightarrow A$: $179{,}800$\n- $C \\rightarrow G$: $215{,}400$\n- $G \\rightarrow C$: $216{,}100$\n- $G \\rightarrow T$: $230{,}600$\n- $T \\rightarrow G$: $231{,}300$\n\nUse the foundational definitions that transitions are purine-to-purine or pyrimidine-to-pyrimidine substitutions ($A \\leftrightarrow G$ and $C \\leftrightarrow T$) and transversions are purine-to-pyrimidine or pyrimidine-to-purine substitutions (the remaining four base-pair classes). Treat each directed count as one observed SNV, and assume strand symmetry does not change the classification.\n\n- Compute the transition-to-transversion ratio (Ti/Tv) for this callset as the ratio of the total number of transitions to the total number of transversions. Express the final answer as a dimensionless ratio and round your answer to four significant figures.\n\n- Then, starting from well-tested observations in human genetics that typical whole-genome Ti/Tv is approximately $2.0$ and coding-region Ti/Tv is approximately $3.0$ due to purifying selection against transversional nonsynonymous changes, explain how deviations of an observed Ti/Tv from these expectations can indicate quality issues (for example, excess sequencing or mapping errors) or enrichment for coding regions.", "solution": "The problem requires the calculation of the transition-to-transversion (Ti/Tv) ratio from a given set of single-nucleotide variant (SNV) counts, followed by an explanation of the significance of this ratio as a quality control metric in genomics.\n\nFirst, we must categorize the given SNV types as either transitions or transversions. The four nucleotide bases are classified into two chemical groups: purines, which have a double-ring structure ($A$, $G$), and pyrimidines, which have a single-ring structure ($C$, $T$).\nA **transition** is a substitution within the same chemical group:\n- Purine to purine: $A \\leftrightarrow G$\n- Pyrimidine to pyrimidine: $C \\leftrightarrow T$\n\nA **transversion** is a substitution between different chemical groups:\n- Purine to pyrimidine: $A \\leftrightarrow C$, $A \\leftrightarrow T$, $G \\leftrightarrow C$, $G \\leftrightarrow T$\n- Pyrimidine to purine: $C \\leftrightarrow A$, $C \\leftrightarrow G$, $T \\leftrightarrow A$, $T \\leftrightarrow G$\n\nThe problem provides the following directed counts for each SNV type:\n- $N_{A \\rightarrow G} = 850,250$ (Transition)\n- $N_{G \\rightarrow A} = 845,700$ (Transition)\n- $N_{C \\rightarrow T} = 832,400$ (Transition)\n- $N_{T \\rightarrow C} = 838,100$ (Transition)\n- $N_{A \\rightarrow C} = 220,500$ (Transversion)\n- $N_{C \\rightarrow A} = 222,300$ (Transversion)\n- $N_{A \\rightarrow T} = 180,200$ (Transversion)\n- $N_{T \\rightarrow A} = 179,800$ (Transversion)\n- $N_{C \\rightarrow G} = 215,400$ (Transversion)\n- $N_{G \\rightarrow C} = 216,100$ (Transversion)\n- $N_{G \\rightarrow T} = 230,600$ (Transversion)\n- $N_{T \\rightarrow G} = 231,300$ (Transversion)\n\nThe total number of transitions, denoted by $N_{Ti}$, is the sum of the counts of all transition-type substitutions.\n$$N_{Ti} = N_{A \\rightarrow G} + N_{G \\rightarrow A} + N_{C \\rightarrow T} + N_{T \\rightarrow C}$$\n$$N_{Ti} = 850,250 + 845,700 + 832,400 + 838,100 = 3,366,450$$\n\nThe total number of transversions, denoted by $N_{Tv}$, is the sum of the counts of all transversion-type substitutions.\n$$N_{Tv} = N_{A \\rightarrow C} + N_{C \\rightarrow A} + N_{A \\rightarrow T} + N_{T \\rightarrow A} + N_{C \\rightarrow G} + N_{G \\rightarrow C} + N_{G \\rightarrow T} + N_{T \\rightarrow G}$$\n$$N_{Tv} = 220,500 + 222,300 + 180,200 + 179,800 + 215,400 + 216,100 + 230,600 + 231,300$$\n$$N_{Tv} = 1,696,200$$\n\nThe transition-to-transversion ratio, $R_{Ti/Tv}$, is the ratio of the total number of transitions to the total number of transversions.\n$$R_{Ti/Tv} = \\frac{N_{Ti}}{N_{Tv}} = \\frac{3,366,450}{1,696,200}$$\n$$R_{Ti/Tv} \\approx 1.984700035...$$\n\nRounding the result to four significant figures as required by the problem statement, we obtain:\n$$R_{Ti/Tv} \\approx 1.985$$\n\nThe second part of the problem asks for an explanation of how deviations in the observed Ti/Tv ratio from expected values can indicate quality issues or biological context. The calculated ratio of approximately $1.985$ is very close to the established benchmark for human whole-genome sequencing (WGS), which is typically around $2.0$. This proximity suggests that the provided variant callset is of high quality and represents a non-enriched, genome-wide sample.\n\nThe significance of the Ti/Tv ratio stems from both biological and technical factors:\n- **Biological Basis**: Due to the chemical properties of nucleotide bases and the nature of spontaneous mutations (e.g., deamination of methylated cytosine to thymine, a common $C \\rightarrow T$ transition), transitions arise more frequently in the genome than transversions. Transversions are structurally more disruptive (substituting a single-ring base for a double-ring one, or vice-versa), which makes them more likely to be deleterious if they fall within a functional genomic element.\n- **Quality Control**: Deviations from the expected Ti/Tv ratio are a critical quality control metric for variant callsets.\n    1.  **A low Ti/Tv ratio (e.g., significantly less than $2.0$ for WGS)** is a strong indicator of random errors. If base substitutions were to occur purely at random, there are two possible transversion outcomes for each purine or pyrimidine, but only one possible transition outcome. This leads to a $1:2$ ratio of transitions to transversions, or a theoretical Ti/Tv ratio of $0.5$ for random noise. Therefore, an excess of false-positive variant calls, which are often random in nature (due to sequencing errors, mapping artifacts, etc.), will disproportionately increase the number of transversions and drive the overall Ti/Tv ratio down towards $0.5$. A low Ti/Tv ratio is a red flag that suggests the callset may have a high error rate and that variant filtering thresholds may be too lenient.\n    2.  **A high Ti/Tv ratio (e.g., approaching $3.0$)** often indicates enrichment for specific genomic regions, particularly coding sequences (exons). Within protein-coding regions, natural selection acts to preserve function. Transversions are more likely than transitions to result in a non-synonymous amino acid substitution, and these substitutions are more likely to be functionally radical (e.g., changing the chemical class of the amino acid). Consequently, purifying selection acts more strongly against transversions in coding regions. This differential selective pressure leads to a higher observed Ti/Tv ratio in exomes, typically around $3.0$. An unexpectedly high Ti/Tv ratio in a dataset purported to be from WGS could suggest that the sequencing library was unintentionally biased towards capturing exonic regions.\n\nIn summary, the Ti/Tv ratio is a powerful summary statistic. The value calculated here ($1.985$) reflects a high-quality WGS dataset. A lower value would suggest contamination with random errors, while a significantly higher value would suggest enrichment for coding regions where purifying selection has elevated the ratio.", "answer": "$$\\boxed{1.985}$$", "id": "4617272"}, {"introduction": "After assessing the callset globally, we zoom in on the quality of individual variants. The Phred-scaled quality score ($QUAL$) is a cornerstone of variant filtering, but its origin can seem opaque. This practice demystifies the $QUAL$ score by tasking you with its derivation from first principles, combining evidence from sequencing data (in the form of genotype likelihoods) with prior biological expectations using a Bayesian framework. [@problem_id:4617239]", "problem": "A single diploid sample is sequenced at a biallelic locus potentially harboring a Single Nucleotide Polymorphism (SNP). Let $g \\in \\{0/0, 0/1, 1/1\\}$ denote the sample’s genotype and let $D$ denote the observed read data at the locus. The genotype likelihood $L(D \\mid g)$ is the probability of observing $D$ given genotype $g$. The widely used Phred-scaled likelihoods $PL_g$ are defined relative to the most likely genotype as $PL_g = -10 \\log_{10}\\left(L(D \\mid g)/\\max_{h} L(D \\mid h)\\right)$. Assume the following $PL$ values are reported for the locus: $PL_{0/0} = 80$, $PL_{0/1} = 0$, and $PL_{1/1} = 40$.\n\nA site-level prior expresses the prior odds $O$ of “variant” versus “no variant,” where “no variant” means genotype $0/0$ for the single sample. Let $O = 10^{-3}$. Conditional on the event “variant,” assume a prior allocation across the genotypes $0/1$ and $1/1$ given by a heterozygote fraction $\\alpha = 0.9$, so that $P(0/1) = \\alpha \\frac{O}{1+O}$ and $P(1/1) = (1-\\alpha)\\frac{O}{1+O}$, while $P(0/0) = \\frac{1}{1+O}$.\n\nUsing only the definitions above and Bayes’ rule, derive an expression for the posterior probability $P(\\text{no variant} \\mid D)$ in terms of $L(D \\mid g)$ and $P(g)$, and then obtain the site quality score $Q$ by applying the standard Phred transformation to this posterior probability. Finally, compute $Q$ for the locus using the provided $PL$ values and prior parameters. Round your final numerical result to four significant figures.", "solution": "The problem asks for the site quality score $Q$, defined as the Phred-scaled posterior probability of the site being non-variant (genotype $0/0$), given the observed data $D$.\n\n**1. Define the Posterior Probability**\nUsing Bayes' rule, the posterior probability of genotype $g$ given data $D$ is $P(g \\mid D) = \\frac{P(D \\mid g) P(g)}{P(D)}$, where $P(D \\mid g)$ is the likelihood $L(D \\mid g)$, $P(g)$ is the prior probability, and $P(D)$ is the marginal probability of the data, a sum over all genotypes $h$: $P(D) = \\sum_h L(D \\mid h)P(h)$. The posterior probability of \"no variant\" ($g=0/0$) is:\n$$P(g=0/0 \\mid D) = \\frac{L(D \\mid 0/0) P(0/0)}{L(D \\mid 0/0) P(0/0) + L(D \\mid 0/1) P(0/1) + L(D \\mid 1/1) P(1/1)}$$\n\n**2. Convert PL values to Relative Likelihoods**\nThe Phred-scaled likelihood $PL_g$ is defined as $PL_g = -10 \\log_{10}\\left(\\frac{L(D \\mid g)}{L_{\\max}}\\right)$, where $L_{\\max}$ is the maximum likelihood. This rearranges to $\\frac{L(D \\mid g)}{L_{\\max}} = 10^{-PL_g/10}$.\nGiven $PL_{0/0} = 80$, $PL_{0/1} = 0$, and $PL_{1/1} = 40$, the maximum likelihood corresponds to genotype $0/1$ ($PL=0$), so $L_{\\max} = L(D \\mid 0/1)$.\nThe relative likelihoods are:\n- $\\frac{L(D \\mid 0/0)}{L_{\\max}} = 10^{-80/10} = 10^{-8}$\n- $\\frac{L(D \\mid 0/1)}{L_{\\max}} = 10^{-0/10} = 1$\n- $\\frac{L(D \\mid 1/1)}{L_{\\max}} = 10^{-40/10} = 10^{-4}$\n\n**3. Calculate Prior Probabilities**\nGiven the prior odds $O = 10^{-3}$ and heterozygote fraction $\\alpha = 0.9$:\n- $P(0/0) = \\frac{1}{1+O} = \\frac{1}{1.001}$\n- $P(0/1) = \\alpha \\frac{O}{1+O} = 0.9 \\times \\frac{10^{-3}}{1.001}$\n- $P(1/1) = (1-\\alpha) \\frac{O}{1+O} = 0.1 \\times \\frac{10^{-3}}{1.001}$\n\n**4. Calculate the Posterior Probability**\nWe substitute the relative likelihoods and priors into the posterior formula. The normalization constants ($L_{\\max}$ from likelihoods and $1/(1+O)$ from priors) cancel from the numerator and denominator. We can work with terms proportional to likelihoods ($L'_g = 10^{-PL_g/10}$) and priors ($P'(g)$, where $P'(0/0)=1$, $P'(0/1)=\\alpha O$, $P'(1/1)=(1-\\alpha)O$).\n$$P(g=0/0 \\mid D) = \\frac{L'_{0/0} \\times P'(0/0)}{L'_{0/0}P'(0/0) + L'_{0/1}P'(0/1) + L'_{1/1}P'(1/1)}$$\n$$P(g=0/0 \\mid D) = \\frac{(10^{-8})(1)}{(10^{-8})(1) + (1)(0.9 \\times 10^{-3}) + (10^{-4})(0.1 \\times 10^{-3})}$$\n$$P(g=0/0 \\mid D) = \\frac{10^{-8}}{10^{-8} + 9 \\times 10^{-4} + 10^{-8}} = \\frac{10^{-8}}{9 \\times 10^{-4} + 2 \\times 10^{-8}}$$\n$$P(g=0/0 \\mid D) = \\frac{10^{-8}}{9.0002 \\times 10^{-4}} = \\frac{10^{-4}}{9.0002}$$\n\n**5. Calculate the Phred-scaled Quality Score Q**\nThe site quality score $Q$ is the Phred transformation of the posterior probability of no variant.\n$$Q = -10 \\log_{10}(P(g=0/0 \\mid D)) = -10 \\log_{10}\\left(\\frac{10^{-4}}{9.0002}\\right)$$\nUsing logarithm properties $\\log(a/b) = \\log(a) - \\log(b)$:\n$$Q = -10 \\left( \\log_{10}(10^{-4}) - \\log_{10}(9.0002) \\right)$$\n$$Q = -10 \\left( -4 - \\log_{10}(9.0002) \\right)$$\n$$Q = 40 + 10 \\log_{10}(9.0002)$$\nCalculating the value:\n$$Q \\approx 40 + 10 \\times (0.9542514) = 40 + 9.542514 = 49.542514$$\nRounding to four significant figures gives $49.54$.", "answer": "$$\\boxed{49.54}$$", "id": "4617239"}, {"introduction": "Possessing a callset with well-calibrated quality scores is only half the battle; the final step is to apply a principled filtering strategy. Instead of relying on arbitrary 'hard' thresholds, modern genomics employs statistical methods to control the error rate. This capstone exercise puts theory into practice by having you implement an algorithm to determine an optimal $QUAL$ threshold that controls the False Discovery Rate (FDR), directly balancing the trade-off between discovering true variants and including false positives. [@problem_id:4617237]", "problem": "You are given a set of variant calls from whole-genome analysis of Single-Nucleotide Polymorphism (SNP) and insertion and deletion (indel) candidates. Each call has an associated Phred-like variant quality score $QUAL$, and a boolean indicator of whether the call lies within independently verified truth regions used for benchmarking. The objective is to derive, from first principles, a thresholding rule on $QUAL$ that controls the False Discovery Rate (FDR) at or below a target value while maximizing retained calls, then implement it as a program that computes results for a specified test suite.\n\nFundamental base and definitions:\n- The Phred scale interprets a quality value $Q$ as a monotone transformation of error likelihood, and is widely used in genomics to rank variant calls by confidence. No specific calibration is assumed; only that higher $Q$ should not decrease the plausibility of correctness relative to lower $Q$.\n- In statistical hypothesis testing, the False Discovery Rate (FDR) is defined as the expected fraction of false discoveries among all discoveries. In a finite counting approximation appropriate for benchmarking against a truth set, define for a given $QUAL$ threshold $\\tau$:\n  - Let the dataset consist of $n$ calls indexed by $i \\in \\{1,\\dots,n\\}$.\n  - Let $Q_i$ be the $QUAL$ of call $i$.\n  - Let $B_i \\in \\{0,1\\}$ indicate membership in truth regions, with $B_i=1$ meaning the call lies inside truth regions (proxy for a true positive candidate), and $B_i=0$ meaning the call lies outside truth regions (proxy for a false positive candidate).\n  - Retain calls by the inclusive rule: a call $i$ is retained if and only if $Q_i \\ge \\tau$.\n  - Let $S(\\tau)=\\{i \\mid Q_i \\ge \\tau\\}$ be the set of retained calls.\n  - Define the retained false positives $FP(\\tau) = \\sum_{i \\in S(\\tau)} (1 - B_i)$.\n  - Define the retained true positives $TP(\\tau) = \\sum_{i \\in S(\\tau)} B_i$.\n  - Define the finite-sample FDR at threshold $\\tau$ as\n    $$\\mathrm{FDR}(\\tau) = \\begin{cases}\n    \\dfrac{FP(\\tau)}{FP(\\tau) + TP(\\tau)} & \\text{if } FP(\\tau) + TP(\\tau) > 0, \\\\\n    0 & \\text{if } FP(\\tau) + TP(\\tau) = 0,\n    \\end{cases}$$\n    which sets the FDR to $0$ when there are no retained calls (no discoveries implies no false discoveries).\n- The optimal threshold under a target FDR $t$ is the minimal $\\tau$ that satisfies $\\mathrm{FDR}(\\tau) \\le t$, using the inclusive $QUAL$ filter $Q_i \\ge \\tau$. Minimality promotes sensitivity by retaining as many calls as possible subject to the FDR constraint.\n\nAlgorithmic requirements:\n- Consider candidate thresholds given by the sorted unique values of $\\{Q_i\\}$, denoted $\\tau \\in \\mathcal{T} = \\{\\text{sorted unique }Q_i\\}$. Under the inclusive rule $Q_i \\ge \\tau$, these candidates enumerate all distinct retained sets $S(\\tau)$.\n- If no $\\tau \\in \\mathcal{T}$ satisfies $\\mathrm{FDR}(\\tau) \\le t$, define a fallback threshold\n  $$\\tau^\\mathrm{fallback} = \\max_i(Q_i) + \\epsilon,$$\n  where $\\epsilon = 10^{-6}$, which yields $S(\\tau^\\mathrm{fallback}) = \\emptyset$ and thus $\\mathrm{FDR}(\\tau^\\mathrm{fallback}) = 0$ with $0$ retained calls.\n- The optimal threshold is\n  $$\\tau^\\ast = \\begin{cases}\n  \\min\\{\\tau \\in \\mathcal{T} \\mid \\mathrm{FDR}(\\tau) \\le t\\} & \\text{if the set is nonempty}, \\\\\n  \\tau^\\mathrm{fallback} & \\text{otherwise}.\n  \\end{cases}$$\n\nTask:\n- Implement a program that, for each test case, computes $\\tau^\\ast$, $\\mathrm{FDR}(\\tau^\\ast)$, and the retained call count $\\lvert S(\\tau^\\ast) \\rvert$.\n- Use the inclusive filter $Q_i \\ge \\tau$ exactly as defined above.\n- Round $\\tau^\\ast$ and $\\mathrm{FDR}(\\tau^\\ast)$ to $6$ decimal places; the retained call count must be an integer.\n\nTest suite:\nEvaluate your program on the following test cases. Each test case is specified by a list of $QUAL$ values, a corresponding list of truth-region indicators $B_i$, and a target FDR $t$ (expressed as a decimal fraction).\n\n- Case $1$ (general nonmonotone scenario):\n  - $QUAL$: $\\{12,18,35,42,60,75,90,25,55,40\\}$\n  - $B$: $\\{0,0,1,1,1,1,0,1,1,0\\}$\n  - Target $t$: $0.3$\n- Case $2$ (all retained calls are inside truth regions):\n  - $QUAL$: $\\{5,10,20,40\\}$\n  - $B$: $\\{1,1,1,1\\}$\n  - Target $t$: $0.0$\n- Case $3$ (no acceptable threshold except excluding all calls):\n  - $QUAL$: $\\{30,60,90\\}$\n  - $B$: $\\{0,0,0\\}$\n  - Target $t$: $0.0$\n- Case $4$ (ties at the threshold, inclusive retention):\n  - $QUAL$: $\\{20,20,50,50,50\\}$\n  - $B$: $\\{0,1,0,1,1\\}$\n  - Target $t$: $0.35$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list of the form $[\\tau^\\ast,\\mathrm{FDR}(\\tau^\\ast),\\lvert S(\\tau^\\ast) \\rvert]$, with $\\tau^\\ast$ and $\\mathrm{FDR}(\\tau^\\ast)$ rounded to $6$ decimal places and $\\lvert S(\\tau^\\ast) \\rvert$ as an integer.\n- For example, the overall output should look like\n  $$[[\\tau^\\ast_1,\\mathrm{FDR}(\\tau^\\ast_1),\\lvert S(\\tau^\\ast_1)\\rvert],[\\tau^\\ast_2,\\mathrm{FDR}(\\tau^\\ast_2),\\lvert S(\\tau^\\ast_2)\\rvert],\\dots].$$", "solution": "The problem requires the derivation and implementation of an algorithm to determine an optimal quality score threshold for filtering genomic variant calls. The optimization objective is to control the False Discovery Rate (FDR) at or below a specified target level, $t$, while maximizing the number of retained calls, which corresponds to maximizing sensitivity.\n\nThe provided framework is based on a finite-sample approximation of FDR, calculated using a benchmark set. Each variant call $i$ from a total of $n$ calls is characterized by a quality score $Q_i$ and a binary indicator $B_i$. $B_i=1$ signifies that the call is within a trusted \"truth\" region (a proxy for a true positive), and $B_i=0$ signifies it is outside (a proxy for a false positive). The core assumption is that $Q_i$ is a Phred-like score, meaning it is monotonically related to the confidence in the call; a higher $Q_i$ implies a more reliable call.\n\nA call $i$ is retained if its quality score $Q_i$ meets or exceeds a given threshold $\\tau$, i.e., $Q_i \\ge \\tau$. The set of retained calls is denoted by $S(\\tau) = \\{i \\mid Q_i \\ge \\tau\\}$. For this set, we can count the number of retained true positives, $TP(\\tau) = \\sum_{i \\in S(\\tau)} B_i$, and retained false positives, $FP(\\tau) = \\sum_{i \\in S(\\tau)} (1 - B_i)$. The total number of retained calls, or discoveries, is $|S(\\tau)| = TP(\\tau) + FP(\\tau)$.\n\nThe finite-sample FDR for a threshold $\\tau$ is defined as the fraction of false positives among all retained calls:\n$$\n\\mathrm{FDR}(\\tau) = \\begin{cases}\n\\dfrac{FP(\\tau)}{FP(\\tau) + TP(\\tau)} & \\text{if } FP(\\tau) + TP(\\tau) > 0, \\\\\n0 & \\text{if } FP(\\tau) + TP(\\tau) = 0.\n\\end{cases}\n$$\nThe case where the denominator is $0$ implies no calls are retained, hence no discoveries are made, and consequently, no false discoveries.\n\nThe goal is to find an optimal threshold $\\tau^\\ast$. To maximize the number of retained calls (sensitivity) subject to the constraint $\\mathrm{FDR}(\\tau) \\le t$, we should choose the least stringent (i.e., minimal) threshold that satisfies the condition. A lower threshold retains more calls.\n\nThe set of retained calls $S(\\tau)$ only changes when the threshold $\\tau$ crosses one of the quality score values $Q_i$. Therefore, we only need to consider the unique values of $Q_i$ as candidate thresholds. Let $\\mathcal{T}$ be the set of unique quality scores, sorted in ascending order: $\\mathcal{T} = \\{\\tau_1, \\tau_2, \\dots, \\tau_k\\}$ where $\\tau_1 < \\tau_2 < \\dots < \\tau_k$.\n\nThe algorithm to find the optimal threshold $\\tau^\\ast$ proceeds as follows:\n\n1.  **Identify Candidate Thresholds**: Extract all unique quality scores $\\{Q_i\\}$ from the dataset and sort them in ascending order to form the set of candidate thresholds, $\\mathcal{T}$.\n\n2.  **Iterative Search**: Iterate through the candidate thresholds $\\tau_j \\in \\mathcal{T}$ starting from the smallest, $\\tau_1$. For each $\\tau_j$:\n    a.  Define the set of retained calls $S(\\tau_j) = \\{i \\mid Q_i \\ge \\tau_j\\}$. Note that due to the inclusive inequality, all calls with quality equal to $\\tau_j$ are included.\n    b.  Calculate the number of retained false positives, $FP(\\tau_j) = \\sum_{i \\in S(\\tau_j)} (1-B_i)$, and the total number of retained calls, $|S(\\tau_j)|$.\n    c.  Compute the FDR, $\\mathrm{FDR}(\\tau_j)$, using the definition above.\n    d.  Check if the FDR constraint is met: $\\mathrm{FDR}(\\tau_j) \\le t$.\n    e.  If the constraint is satisfied, we have found a valid threshold. Since we are iterating from the smallest $\\tau_j$ upwards, this first valid threshold is the minimal one. We set $\\tau^\\ast = \\tau_j$ and the search is complete. The corresponding values $\\mathrm{FDR}(\\tau^\\ast)$ and $|S(\\tau^\\ast)|$ are recorded.\n\n3.  **Fallback Mechanism**: If the iteration completes and no threshold $\\tau_j \\in \\mathcal{T}$ satisfies the condition $\\mathrm{FDR}(\\tau_j) \\le t$, it implies that no non-trivial filtering can meet the target FDR. In this case, the problem specifies a fallback rule to ensure the FDR constraint is strictly met, albeit at the complete loss of sensitivity. The threshold is set to a value guaranteed to reject all calls:\n    $$ \\tau^\\ast = \\tau^\\mathrm{fallback} = \\max_i(Q_i) + \\epsilon $$\n    where $\\epsilon$ is a small positive constant, given as $10^{-6}$. For this threshold, the set of retained calls $S(\\tau^\\ast)$ is empty. Consequently, $|S(\\tau^\\ast)| = 0$ and, by definition, $\\mathrm{FDR}(\\tau^\\ast) = 0$.\n\nThis procedure is guaranteed to find a unique optimal threshold $\\tau^\\ast$ for any given dataset and target FDR $t$. The implementation will involve sorting the unique quality scores and then iterating through them to perform the calculations as described. For computational efficiency, especially with large datasets, pre-sorting the calls by quality score and using cumulative sums of $B_i$ and $(1-B_i)$ would be more performant than re-scanning the entire dataset for each threshold. However, for the specified test cases, a direct implementation of the above logic is sufficient.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the optimal quality threshold based on FDR control for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"QUAL\": [12, 18, 35, 42, 60, 75, 90, 25, 55, 40],\n            \"B\": [0, 0, 1, 1, 1, 1, 0, 1, 1, 0],\n            \"t\": 0.3\n        },\n        {\n            \"QUAL\": [5, 10, 20, 40],\n            \"B\": [1, 1, 1, 1],\n            \"t\": 0.0\n        },\n        {\n            \"QUAL\": [30, 60, 90],\n            \"B\": [0, 0, 0],\n            \"t\": 0.0\n        },\n        {\n            \"QUAL\": [20, 20, 50, 50, 50],\n            \"B\": [0, 1, 0, 1, 1],\n            \"t\": 0.35\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        quals = np.array(case[\"QUAL\"], dtype=float)\n        bs = np.array(case[\"B\"], dtype=int)\n        target_fdr = case[\"t\"]\n        \n        # Handle cases with no calls\n        if quals.size == 0:\n            # tau_fallback = 1e-6 as max would fail\n            # By definition, |S|=0, FDR=0.\n            # The exact tau doesn't matter much as long as it's positive.\n            # Using 1e-6 aligns with the epsilon definition.\n            all_results.append([1.0e-6, 0.0, 0])\n            continue\n\n        candidate_thresholds = np.unique(quals) # Already sorted in ascending order\n\n        found_solution = False\n        for tau in candidate_thresholds:\n            # Inclusive filter: retain if quality is greater than or equal to tau\n            retained_mask = quals >= tau\n            \n            retained_count = np.sum(retained_mask)\n\n            if retained_count == 0:\n                fdr = 0.0\n            else:\n                # B=0 are false positives, B=1 are true positives\n                # fp_count = sum of (1-B) for retained calls which is sum of (B==0)\n                retained_bs = bs[retained_mask]\n                fp_count = np.sum(retained_bs == 0)\n                fdr = fp_count / retained_count\n\n            if fdr <= target_fdr:\n                opt_tau = tau\n                opt_fdr = fdr\n                opt_count = int(retained_count)\n                found_solution = True\n                break\n\n        if not found_solution:\n            # Fallback case: no threshold met the criteria\n            epsilon = 1e-6\n            opt_tau = np.max(quals) + epsilon\n            opt_fdr = 0.0\n            opt_count = 0\n\n        all_results.append([opt_tau, opt_fdr, opt_count])\n\n    # Format the final output string exactly as required\n    formatted_results = [\n        f\"[{r[0]:.6f},{r[1]:.6f},{r[2]}]\" for r in all_results\n    ]\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4617237"}]}