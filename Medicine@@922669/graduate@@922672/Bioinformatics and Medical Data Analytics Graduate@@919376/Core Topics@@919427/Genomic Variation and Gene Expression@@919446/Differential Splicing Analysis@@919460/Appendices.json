{"hands_on_practices": [{"introduction": "The cornerstone of splicing analysis is the accurate quantification of isoform usage, typically summarized by the 'Percent Spliced In' (PSI or $\\Psi$) metric. However, estimating $\\Psi$ from RNA-seq data is not trivial, as it can be derived from different signals—such as reads spanning specific exon junctions or abundances estimated at the full-transcript level. This exercise [@problem_id:4556822] challenges you to derive and compute $\\Psi$ from both data types, forcing you to confront and correct for common technical artifacts like mapping ambiguity and sequence-specific biases.", "problem": "A single human gene $G$ harbors a Skipped Exon (SE) event: a cassette exon $E$ lies between constitutive exons $A$ (upstream) and $B$ (downstream). There are four annotated transcripts: $T_1$ and $T_2$ include $E$ (inclusion isoforms), while $T_3$ and $T_4$ skip $E$ (skipping isoforms). You are provided with two independent data views for the same sample: junction-spanning read evidence and transcript abundance estimates.\n\nFoundational definitions you must use:\n- Percent Spliced In (PSI) for an SE event, denoted $\\Psi$, is defined as the fraction of mature messenger ribonucleic acid (mRNA) molecules that include the cassette exon among those that either include or skip it.\n- Junction-based evidence originates from reads spanning exon–exon junctions. For an SE event, inclusion molecules can generate junction-spanning fragments for both the upstream-to-exon junction and the exon-to-downstream junction, while skipping molecules generate fragments for the upstream-to-downstream skipping junction. Assume library insert sizes and read lengths make the two inclusion junctions equally accessible to an included molecule.\n- Ambiguously mapped reads are fractionally assigned by their posterior weights (probabilities of originating from the locus/junction in question). Sequence-specific detection bias at each junction is represented by a multiplicative factor $b$ that distorts observed counts; an unbiased correction divides by $b$.\n\nData for junction evidence:\n- Upstream-to-exon junction ($A \\rightarrow E$): uniquely mapped reads $C_{UE}^{\\mathrm{uniq}} = 120$, ambiguous reads $C_{UE}^{\\mathrm{amb}} = 20$ with posterior weight $p_{UE} = 0.6$ to this gene, bias factor $b_{UE} = 0.85$.\n- Exon-to-downstream junction ($E \\rightarrow B$): $C_{ED}^{\\mathrm{uniq}} = 100$, $C_{ED}^{\\mathrm{amb}} = 10$ with $p_{ED} = 0.5$, $b_{ED} = 1.10$.\n- Upstream-to-downstream skipping junction ($A \\rightarrow B$): $C_{UD}^{\\mathrm{uniq}} = 80$, $C_{UD}^{\\mathrm{amb}} = 40$ with $p_{UD} = 0.25$, $b_{UD} = 0.95$.\n\nTranscript abundance data (from a standard transcript quantifier reporting Transcripts Per Million (TPM) and per-transcript bias factors summarizing sequence-composition effects). For $i \\in \\{1,2,3,4\\}$, let $A_i$ denote uncorrected TPM and $g_i$ denote the transcript-specific bias factor; an unbiased correction divides by $g_i$.\n- $T_1$ (include): $A_1 = 18$, $g_1 = 0.90$.\n- $T_2$ (include): $A_2 = 12$, $g_2 = 1.10$.\n- $T_3$ (skip): $A_3 = 15$, $g_3 = 0.95$.\n- $T_4$ (skip): $A_4 = 5$, $g_4 = 0.85$.\n\nTasks:\n- Using only the foundational definitions above, first derive from first principles an unbiased junction-based estimator $\\Psi_{\\mathrm{J}}$ of the inclusion fraction $\\Psi$ for this SE event by appropriately combining bias-corrected, fractionally assigned junction counts from the two inclusion-supporting junctions and the skipping junction.\n- Independently, derive an unbiased transcript-based estimator $\\Psi_{\\mathrm{T}}$ of $\\Psi$ by appropriately combining bias-corrected TPMs across inclusion isoforms and across all isoforms.\n- Compute both $\\Psi_{\\mathrm{J}}$ and $\\Psi_{\\mathrm{T}}$ from the provided data under your derived estimators, then report as your final numerical answer the absolute discrepancy $D = |\\Psi_{\\mathrm{T}} - \\Psi_{\\mathrm{J}}|$.\n\nRound your final answer to four significant figures. Express the final answer as a unitless decimal (do not use a percentage sign).", "solution": "The problem as stated is scientifically sound, well-posed, and objective. It is grounded in established principles of RNA-sequencing data analysis for alternative splicing, specifically the calculation of the Percent Spliced In (PSI) metric from two different data modalities: junction-spanning reads and transcript abundance estimates. All necessary data and definitions for the derivation and calculation are provided. Therefore, the problem is valid and a solution can be constructed.\n\nThe overall goal is to compute two estimators for PSI, denoted $\\Psi$, and find the absolute difference between them. $\\Psi$ is defined as the fraction of mRNA molecules that include a specific cassette exon $E$.\n$$\n\\Psi = \\frac{\\text{Abundance of Inclusion Isoforms}}{\\text{Abundance of Inclusion Isoforms} + \\text{Abundance of Skipping Isoforms}}\n$$\n\n**Part 1: Derivation and Computation of the Junction-Based Estimator $\\Psi_{\\mathrm{J}}$**\n\nThe junction-based estimator uses read counts that span exon-exon junctions as a proxy for isoform abundance. The provided data must be corrected for mapping ambiguity and detection bias.\n\nFirst, for each junction $j$, we calculate the effective read count, $C_{j}^{\\mathrm{eff}}$, by combining the uniquely mapped reads $C_{j}^{\\mathrm{uniq}}$ and the fractionally assigned ambiguous reads $C_{j}^{\\mathrm{amb}}$ using their posterior weight $p_{j}$:\n$$\nC_{j}^{\\mathrm{eff}} = C_{j}^{\\mathrm{uniq}} + C_{j}^{\\mathrm{amb}} p_{j}\n$$\nNext, we obtain an unbiased intensity measure, $I_{j}$, by correcting the effective count for the given multiplicative bias factor $b_{j}$. As per the problem definition, this correction is performed by division:\n$$\nI_{j} = \\frac{C_{j}^{\\mathrm{eff}}}{b_{j}}\n$$\n\nThe skipping event is supported by a single junction, the upstream-to-downstream skipping junction ($A \\rightarrow B$), denoted by the subscript $UD$. Its unbiased intensity, $I_{\\mathrm{skip}}$, is:\n$$\nI_{\\mathrm{skip}} = \\frac{C_{UD}^{\\mathrm{uniq}} + C_{UD}^{\\mathrm{amb}} p_{UD}}{b_{UD}}\n$$\nSubstituting the given values:\n$$\nI_{\\mathrm{skip}} = \\frac{80 + 40 \\times 0.25}{0.95} = \\frac{80 + 10}{0.95} = \\frac{90}{0.95} \\approx 94.73684\n$$\n\nThe inclusion event is supported by two junctions: the upstream exon-to-cassette exon junction ($A \\rightarrow E$, subscript $UE$) and the cassette exon-to-downstream exon junction ($E \\rightarrow B$, subscript $ED$). We first calculate their individual unbiased intensities, $I_{UE}$ and $I_{ED}$:\n$$\nI_{UE} = \\frac{C_{UE}^{\\mathrm{uniq}} + C_{UE}^{\\mathrm{amb}} p_{UE}}{b_{UE}} = \\frac{120 + 20 \\times 0.6}{0.85} = \\frac{120 + 12}{0.85} = \\frac{132}{0.85} \\approx 155.29412\n$$\n$$\nI_{ED} = \\frac{C_{ED}^{\\mathrm{uniq}} + C_{ED}^{\\mathrm{amb}} p_{ED}}{b_{ED}} = \\frac{100 + 10 \\times 0.5}{1.10} = \\frac{100 + 5}{1.10} = \\frac{105}{1.10} \\approx 95.45455\n$$\nThe problem states that these two inclusion junctions are \"equally accessible.\" A robust way to combine the evidence from these two sources into a single measure for inclusion intensity, $I_{\\mathrm{incl}}$, is to take their average. This moderates the effect of any measurement noise or unmodeled bias specific to one of the junctions.\n$$\nI_{\\mathrm{incl}} = \\frac{1}{2} (I_{UE} + I_{ED}) = \\frac{1}{2} \\left( \\frac{132}{0.85} + \\frac{105}{1.10} \\right) \\approx \\frac{1}{2} (155.29412 + 95.45455) \\approx 125.37433\n$$\nThe junction-based PSI estimator, $\\Psi_{\\mathrm{J}}$, is the ratio of the inclusion intensity to the total intensity (inclusion plus skipping):\n$$\n\\Psi_{\\mathrm{J}} = \\frac{I_{\\mathrm{incl}}}{I_{\\mathrm{incl}} + I_{\\mathrm{skip}}} \\approx \\frac{125.37433}{125.37433 + 94.73684} \\approx \\frac{125.37433}{220.11117} \\approx 0.5695936\n$$\n\n**Part 2: Derivation and Computation of the Transcript-Based Estimator $\\Psi_{\\mathrm{T}}$**\n\nThe transcript-based estimator uses quantified transcript abundances, given in Transcripts Per Million ($TPM$). These abundances, $A_i$ for transcript $T_i$, must be corrected for their respective bias factors, $g_i$. The unbiased relative abundance, $A'_i$, is obtained by dividing $A_i$ by $g_i$:\n$$\nA'_i = \\frac{A_i}{g_i}\n$$\nThe total corrected abundance for the inclusion isoforms ($T_1$ and $T_2$) is the sum of their individual corrected abundances:\n$$\nA'_{\\mathrm{incl}} = A'_{1} + A'_{2} = \\frac{A_1}{g_1} + \\frac{A_2}{g_2}\n$$\nSubstituting the given values:\n$$\nA'_{\\mathrm{incl}} = \\frac{18}{0.90} + \\frac{12}{1.10} = 20 + 10.90909... = 30.90909...\n$$\nSimilarly, the total corrected abundance for the skipping isoforms ($T_3$ and $T_4$) is:\n$$\nA'_{\\mathrm{skip}} = A'_{3} + A'_{4} = \\frac{A_3}{g_3} + \\frac{A_4}{g_4}\n$$\nSubstituting the given values:\n$$\nA'_{\\mathrm{skip}} = \\frac{15}{0.95} + \\frac{5}{0.85} \\approx 15.78947... + 5.88235... = 21.67182...\n$$\nThe transcript-based PSI estimator, $\\Psi_{\\mathrm{T}}$, is the ratio of the total corrected inclusion abundance to the total corrected abundance of all isoforms involved in the event:\n$$\n\\Psi_{\\mathrm{T}} = \\frac{A'_{\\mathrm{incl}}}{A'_{\\mathrm{incl}} + A'_{\\mathrm{skip}}} \\approx \\frac{30.90909...}{30.90909... + 21.67182...} \\approx \\frac{30.90909...}{52.58091...} \\approx 0.5878345\n$$\n\n**Part 3: Final Calculation of the Absolute Discrepancy**\n\nThe final task is to compute the absolute discrepancy, $D$, between the two estimators:\n$$\nD = |\\Psi_{\\mathrm{T}} - \\Psi_{\\mathrm{J}}|\n$$\nUsing the computed values:\n$$\nD \\approx |0.5878345 - 0.5695936| \\approx 0.0182409\n$$\nRounding the result to four significant figures as requested gives $0.01824$.", "answer": "$$\n\\boxed{0.01824}\n$$", "id": "4556822"}, {"introduction": "Identifying true differential splicing requires distinguishing changes in isoform proportions from overall changes in the parent gene's expression. A naive comparison of inclusion-supporting read counts can be misleading, as higher gene expression will naturally lead to higher counts for all isoforms. This practice problem [@problem_id:4556820] demonstrates this critical pitfall and guides you through the proper statistical solution: using a Generalized Linear Model (GLM) with an offset term to correctly normalize for gene-level expression, thereby isolating the true splicing change.", "problem": "You are analyzing a cassette exon event in a single gene using Ribonucleic Acid Sequencing (RNA-Seq). The gene has two isoforms: one that includes the exon (inclusion isoform) and one that skips it (skipping isoform). In each sample, exon inclusion and skipping counts are recorded, and the total gene-level count is the sum of inclusion and skipping counts. Two biological conditions are present: control and treatment, each with two samples. The recorded counts for the four samples are:\n- Control sample $1$: inclusion $y_{1} = 120$, skipping $k_{1} = 80$, total $n_{1} = 200$.\n- Control sample $2$: inclusion $y_{2} = 180$, skipping $k_{2} = 120$, total $n_{2} = 300$.\n- Treatment sample $1$: inclusion $y_{3} = 240$, skipping $k_{3} = 160$, total $n_{3} = 400$.\n- Treatment sample $2$: inclusion $y_{4} = 360$, skipping $k_{4} = 240$, total $n_{4} = 600$.\n\nThe biological ground truth is that the relative usage of isoforms (inclusion proportion) is unchanged across conditions; all transcript counts have increased proportionally in the treatment relative to control. Nevertheless, an event-level analysis that fits a Poisson regression to the inclusion counts using only a condition indicator (without controlling for gene-level totals) would erroneously infer a condition effect because the raw inclusion counts are larger in treatment.\n\nTo properly test for differential splicing at the event level, model the inclusion counts with a Poisson Generalized Linear Model (GLM; Generalized Linear Model) with a logarithmic link and sample-specific offsets that account for gene-level exposure. Let $y_{i}$ denote the inclusion count in sample $i$, $n_{i}$ denote the gene-level total in sample $i$, and $x_{i} \\in \\{0,1\\}$ denote the condition indicator ($0$ for control, $1$ for treatment). Fit the model\n$$\n\\ln\\!\\big(\\mu_{i}\\big) \\;=\\; \\alpha \\;+\\; \\beta\\, x_{i} \\;+\\; \\ln\\!\\big(n_{i}\\big),\n$$\nwhere $\\mu_{i}$ is the expected inclusion count, $\\alpha$ is an intercept capturing baseline inclusion given exposure, $\\beta$ is the condition effect on inclusion beyond exposure, and $\\ln(n_{i})$ is the offset.\n\nAssume the Poisson mean-variance relationship holds exactly and treat the offsets as known. Under this model and the provided data, compute the maximum likelihood estimate of the condition effect parameter $\\hat{\\beta}$. Express your final answer as a pure number (dimensionless). No rounding is required; provide the exact value.", "solution": "The user requests the computation of the maximum likelihood estimate (MLE) for the condition effect parameter, $\\beta$, in a specified Poisson Generalized Linear Model (GLM).\n\nFirst, the problem is validated.\n\n### Step 1: Extract Givens\n- **Data for $4$ samples ($i=1, 2, 3, 4$):**\n    - Control sample $1$ ($i=1$): inclusion count $y_{1} = 120$, total count $n_{1} = 200$. Condition indicator $x_1 = 0$.\n    - Control sample $2$ ($i=2$): inclusion count $y_{2} = 180$, total count $n_{2} = 300$. Condition indicator $x_2 = 0$.\n    - Treatment sample $1$ ($i=3$): inclusion count $y_{3} = 240$, total count $n_{3} = 400$. Condition indicator $x_3 = 1$.\n    - Treatment sample $2$ ($i=4$): inclusion count $y_{4} = 360$, total count $n_{4} = 600$. Condition indicator $x_4 = 1$.\n- **Statistical Model:**\n    - The inclusion counts $y_{i}$ are modeled as draws from a Poisson distribution, $y_{i} \\sim \\text{Poisson}(\\mu_{i})$.\n    - The expected inclusion count $\\mu_{i}$ is related to the covariates through a logarithmic link function:\n    $$\n    \\ln(\\mu_{i}) = \\alpha + \\beta x_{i} + \\ln(n_{i})\n    $$\n    where $\\alpha$ is the intercept, $\\beta$ is the condition effect, $x_{i}$ is the condition indicator, and $\\ln(n_{i})$ is a sample-specific offset.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is scientifically grounded. The use of a Poisson GLM with an offset term to model count data from RNA-sequencing is a standard and well-established method in bioinformatics for analyzing differential expression and splicing.\n- **Well-Posedness**: The problem is well-posed. It provides all necessary data and a fully specified model to estimate the parameter of interest. The task is to find the maximum likelihood estimate, which is a uniquely defined objective.\n- **Objectivity**: The problem is stated in objective, technical language, free from bias or subjective claims.\n- The validation concludes that the problem is sound, complete, and consistent.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n### Solution Derivation\n\nThe model for the expected inclusion count $\\mu_{i}$ is given by:\n$$\n\\ln(\\mu_{i}) = \\alpha + \\beta x_{i} + \\ln(n_{i})\n$$\nThis can be rewritten as:\n$$\n\\mu_{i} = \\exp(\\alpha + \\beta x_{i} + \\ln(n_{i})) = n_{i} \\exp(\\alpha + \\beta x_{i})\n$$\nThe probability mass function for a Poisson-distributed random variable $y_{i}$ with mean $\\mu_{i}$ is:\n$$\nP(Y_{i} = y_{i}) = \\frac{\\mu_{i}^{y_{i}} \\exp(-\\mu_{i})}{y_{i}!}\n$$\nThe log-likelihood for a single observation $y_{i}$ is:\n$$\n\\ell_{i}(\\alpha, \\beta) = y_{i} \\ln(\\mu_{i}) - \\mu_{i} - \\ln(y_{i}!)\n$$\nSubstituting the expression for $\\ln(\\mu_{i})$:\n$$\n\\ell_{i}(\\alpha, \\beta) = y_{i}(\\alpha + \\beta x_{i} + \\ln(n_{i})) - n_{i} \\exp(\\alpha + \\beta x_{i}) - \\ln(y_{i}!)\n$$\nThe total log-likelihood for all $N=4$ observations is the sum of the individual log-likelihoods:\n$$\n\\ell(\\alpha, \\beta) = \\sum_{i=1}^{4} \\ell_{i}(\\alpha, \\beta) = \\sum_{i=1}^{4} \\left[ y_{i}(\\alpha + \\beta x_{i} + \\ln(n_{i})) - n_{i} \\exp(\\alpha + \\beta x_{i}) \\right] - \\sum_{i=1}^{4} \\ln(y_{i}!)\n$$\nTo find the maximum likelihood estimates ($\\hat{\\alpha}$, $\\hat{\\beta}$), we take the partial derivatives of the log-likelihood function with respect to $\\alpha$ and $\\beta$ and set them to zero. These are the score equations.\n\nThe partial derivative with respect to $\\alpha$ is:\n$$\n\\frac{\\partial \\ell}{\\partial \\alpha} = \\sum_{i=1}^{4} \\left[ y_{i} - n_{i} \\exp(\\alpha + \\beta x_{i}) \\right] = \\sum_{i=1}^{4} (y_{i} - \\mu_{i})\n$$\nSetting this to zero yields the first score equation:\n$$\n\\sum_{i=1}^{4} y_{i} = \\sum_{i=1}^{4} \\hat{\\mu}_{i} = \\sum_{i=1}^{4} n_{i} \\exp(\\hat{\\alpha} + \\hat{\\beta} x_{i})\n$$\nThe partial derivative with respect to $\\beta$ is:\n$$\n\\frac{\\partial \\ell}{\\partial \\beta} = \\sum_{i=1}^{4} \\left[ y_{i}x_{i} - x_{i}n_{i} \\exp(\\alpha + \\beta x_{i}) \\right] = \\sum_{i=1}^{4} x_{i}(y_{i} - \\mu_{i})\n$$\nSetting this to zero yields the second score equation:\n$$\n\\sum_{i=1}^{4} x_{i}y_{i} = \\sum_{i=1}^{4} x_{i}\\hat{\\mu}_{i} = \\sum_{i=1}^{4} x_{i}n_{i} \\exp(\\hat{\\alpha} + \\hat{\\beta} x_{i})\n$$\nNow, we substitute the provided data into the score equations.\nThe data are:\n$y = (120, 180, 240, 360)$\n$n = (200, 300, 400, 600)$\n$x = (0, 0, 1, 1)$\n\nLet's solve the second score equation first, as it is simpler:\n$$\n\\sum_{i=1}^{4} x_{i}y_{i} = (0)(120) + (0)(180) + (1)(240) + (1)(360) = 600\n$$\n$$\n\\sum_{i=1}^{4} x_{i}n_{i} \\exp(\\hat{\\alpha} + \\hat{\\beta} x_{i}) = (0)n_{1}\\exp(\\hat{\\alpha}) + (0)n_{2}\\exp(\\hat{\\alpha}) + (1)n_{3}\\exp(\\hat{\\alpha}+\\hat{\\beta}) + (1)n_{4}\\exp(\\hat{\\alpha}+\\hat{\\beta})\n$$\n$$\n= (n_{3} + n_{4}) \\exp(\\hat{\\alpha}+\\hat{\\beta}) = (400 + 600) \\exp(\\hat{\\alpha}+\\hat{\\beta}) = 1000 \\exp(\\hat{\\alpha}+\\hat{\\beta})\n$$\nEquating the two parts:\n$$\n600 = 1000 \\exp(\\hat{\\alpha}+\\hat{\\beta}) \\implies \\exp(\\hat{\\alpha}+\\hat{\\beta}) = \\frac{600}{1000} = \\frac{3}{5}\n$$\nNow, let's use the first score equation:\n$$\n\\sum_{i=1}^{4} y_{i} = 120 + 180 + 240 + 360 = 900\n$$\n$$\n\\sum_{i=1}^{4} n_{i} \\exp(\\hat{\\alpha} + \\hat{\\beta} x_{i}) = n_{1}\\exp(\\hat{\\alpha}) + n_{2}\\exp(\\hat{\\alpha}) + n_{3}\\exp(\\hat{\\alpha}+\\hat{\\beta}) + n_{4}\\exp(\\hat{\\alpha}+\\hat{\\beta})\n$$\n$$\n= (n_{1}+n_{2})\\exp(\\hat{\\alpha}) + (n_{3}+n_{4})\\exp(\\hat{\\alpha}+\\hat{\\beta})\n$$\n$$\n= (200+300)\\exp(\\hat{\\alpha}) + (400+600)\\exp(\\hat{\\alpha}+\\hat{\\beta}) = 500\\exp(\\hat{\\alpha}) + 1000\\exp(\\hat{\\alpha}+\\hat{\\beta})\n$$\nEquating the two parts and substituting the result from the second equation:\n$$\n900 = 500\\exp(\\hat{\\alpha}) + 1000\\left(\\frac{3}{5}\\right)\n$$\n$$\n900 = 500\\exp(\\hat{\\alpha}) + 600\n$$\n$$\n300 = 500\\exp(\\hat{\\alpha}) \\implies \\exp(\\hat{\\alpha}) = \\frac{300}{500} = \\frac{3}{5}\n$$\nWe now have the estimates for $\\exp(\\hat{\\alpha})$ and $\\exp(\\hat{\\alpha}+\\hat{\\beta})$:\n$$\n\\exp(\\hat{\\alpha}) = \\frac{3}{5}\n$$\n$$\n\\exp(\\hat{\\alpha}+\\hat{\\beta}) = \\frac{3}{5}\n$$\nTo find $\\hat{\\beta}$, we can use the property of exponents $\\exp(a+b) = \\exp(a)\\exp(b)$:\n$$\n\\exp(\\hat{\\alpha}+\\hat{\\beta}) = \\exp(\\hat{\\alpha})\\exp(\\hat{\\beta})\n$$\n$$\n\\frac{3}{5} = \\left(\\frac{3}{5}\\right)\\exp(\\hat{\\beta})\n$$\nDividing both sides by $3/5$ gives:\n$$\n\\exp(\\hat{\\beta}) = 1\n$$\nTaking the natural logarithm of both sides gives the MLE for $\\beta$:\n$$\n\\hat{\\beta} = \\ln(1) = 0\n$$\nThis result is consistent with the data, where the inclusion proportion $y_{i}/n_{i}$ is constant across all samples: $120/200 = 180/300 = 240/400 = 360/600 = 0.6$. A condition effect of $\\beta=0$ indicates no change in the log-proportion of inclusion between the control and treatment groups, which is exactly what the data shows.", "answer": "$$\n\\boxed{0}\n$$", "id": "4556820"}, {"introduction": "Beyond testing individual splicing events, a powerful application of splicing analysis is to uncover global patterns and identify sample subtypes based on their entire splicing profiles. This requires robustly comparing samples across hundreds or thousands of events, each with its own statistical properties. This hands-on coding challenge [@problem_id:4556866] guides you through building a principled hierarchical clustering pipeline, addressing key steps such as variance stabilization for proportion data, robust feature weighting to highlight informative events, and the selection of an appropriate distance metric.", "problem": "You are given multiple cohorts of samples, each characterized by a vector of Percent Spliced In (PSI) values across a fixed set of exon-skipping events. For a sample indexed by $i$ and exon event indexed by $j$, the PSI estimate is a fraction in the closed interval $[0,1]$ that can be interpreted as a binomial proportion arising from inclusion and exclusion read counts. Your task is to define, justify, and implement a principled distance between samples based on their PSI vectors and then perform hierarchical clustering to identify splicing-based subtypes.\n\nStarting from the fundamental base that RNA sequencing read counts for inclusion and exclusion can be modeled as binomial trials and that PSI is a proportion estimator with heteroscedastic variance across the unit interval due to the factor $\\psi(1-\\psi)$, you must derive a distance that:\n- is variance-stabilized so that differences near the boundaries $0$ and $1$ are not overly downweighted or upweighted relative to mid-range values,\n- is robust to exon outliers and uninformative exons by incorporating per-exon dispersion weights based on a robust statistic computed across samples,\n- can handle missing values in PSI (denoted as missing) in a scientifically sound way that does not introduce bias,\n- is compatible with a hierarchical clustering linkage that has a well-defined objective in Euclidean space and yields compact, variance-homogeneous clusters.\n\nYou must integrate this reasoning into an algorithm that, for each cohort, produces a partition into a pre-specified number of clusters.\n\nImplement the following concrete, fully specified instantiation for automated evaluation:\n1. For each exon $j$, impute missing PSI values by the exon-wise sample median computed over observed values in that cohort.\n2. Apply an appropriate variance-stabilizing transformation for binomial proportions to each PSI entry to obtain transformed values $y_{i,j}$; use the standard arcsine-square-root transformation (no explicit formula is provided here to encourage reasoning from first principles).\n3. Compute a robust dispersion weight per exon $j$ as the Median Absolute Deviation (MAD) of $\\{y_{i,j}\\}_{i}$ across samples, add a small positive constant $\\epsilon$ to avoid zero weights, and normalize weights so that they sum to the number of exons $p$. Use $\\epsilon = 10^{-3}$.\n4. Form a weighted Euclidean geometry by scaling each feature $j$ by the square root of its weight. In this geometry, use the Ward linkage criterion to perform hierarchical clustering on the samples.\n5. Given a desired number of clusters $K$ for the cohort, cut the dendrogram to produce exactly $K$ clusters. Map the cluster labels to integers in $\\{0,1,\\dots,K-1\\}$ by the order of first occurrence in the sample index order $i=0,1,\\dots,n-1$.\n\nYou are given three cohorts as a test suite. In each cohort, PSI values are unitless fractions. Missing values are denoted as not-a-number. For each cohort $c \\in \\{1,2,3\\}$ you are given an $n_c \\times p_c$ PSI matrix $P^{(c)}$ and a target cluster count $K^{(c)}$. Your program must process each cohort independently and output the cluster label list for each cohort in order.\n\nTest suite:\n- Cohort $1$ has $n_1=6$ samples and $p_1=8$ exons. PSI matrix $P^{(1)}$ rows (samples $0$ to $5$) are:\n  - Sample $0$: $[0.82,0.71,0.58,0.93,0.18,0.27,0.39,0.12]$\n  - Sample $1$: $[0.78,0.69,0.62,0.88,0.22,0.33,0.41,0.09]$\n  - Sample $2$: $[0.81,0.73,0.59,0.91,0.21,0.31,0.38,0.15]$\n  - Sample $3$: $[0.19,0.29,0.43,0.11,0.81,0.72,0.64,0.88]$\n  - Sample $4$: $[0.23,0.34,0.39,0.09,0.77,0.68,0.58,0.92]$\n  - Sample $5$: $[0.17,0.27,0.41,0.13,0.83,0.73,0.61,0.87]$\n  Use $K^{(1)}=2$.\n- Cohort $2$ has $n_2=4$ samples and $p_2=5$ exons. PSI matrix $P^{(2)}$ rows are:\n  - Sample $0$: $[0.5,0.5,0.5,0.5,0.5]$\n  - Sample $1$: $[0.5,0.5,0.5,0.5,0.5]$\n  - Sample $2$: $[0.5,0.5,0.5,0.5,0.5]$\n  - Sample $3$: $[0.5,0.5,0.5,0.5,0.5]$\n  Use $K^{(2)}=1$.\n- Cohort $3$ has $n_3=5$ samples and $p_3=7$ exons. PSI matrix $P^{(3)}$ rows are:\n  - Sample $0$: $[0.85,0.78,\\mathrm{NaN},0.52,0.18,0.22,0.60]$\n  - Sample $1$: $[0.80,0.75,\\mathrm{NaN},0.49,0.20,0.25,0.58]$\n  - Sample $2$: $[0.25,0.30,0.55,\\mathrm{NaN},0.82,0.75,0.40]$\n  - Sample $3$: $[0.28,0.33,0.58,\\mathrm{NaN},0.78,0.70,0.42]$\n  - Sample $4$: $[0.55,\\mathrm{NaN},0.20,0.85,0.45,\\mathrm{NaN},0.15]$\n  Use $K^{(3)}=3$.\n\nFinal output format:\n- Your program should produce a single line of output containing the three cluster-label lists as a comma-separated list enclosed in square brackets, with each cohort’s labels listed in the order of sample indices. For example, the output must look like $[[\\dots],[\\dots],[\\dots]]$ with no spaces.\n- Each label list must contain integers in $\\{0,1,\\dots,K^{(c)}-1\\}$, remapped by first occurrence as specified above.\n\nAll numerical inputs in this problem are unitless fractions and all angles introduced by the variance-stabilizing transformation are implicitly in radians. The final output consists only of integer labels, so no unit specification is needed for the output.", "solution": "The problem statement is deemed valid as it presents a scientifically grounded, well-posed, and objective task based on established principles in bioinformatics and statistics. It provides a complete, self-contained specification of an algorithm for clustering biological samples and includes all necessary data for its execution. The methodology described is a standard workflow for the analysis of alternative splicing data. We may therefore proceed with a full solution.\n\nThe overarching goal is to partition cohorts of biological samples into distinct subtypes based on their exon splicing patterns, which are quantified by Percent Spliced In (PSI or $\\psi$) values. PSI values are proportions, and their statistical properties necessitate a carefully designed analysis pipeline. The prescribed algorithm consists of five main steps, each justified by fundamental principles.\n\n### Step 1: Imputation of Missing Values\n\nBiological datasets frequently contain missing values due to technical artifacts or insufficient read coverage for certain events in some samples. The problem specifies that missing PSI values, denoted as not-a-number ($\\mathrm{NaN}$), are to be imputed using the median PSI value for that specific exon across all other samples in the cohort. Let $P^{(c)}$ be the PSI matrix for a cohort $c$, with entries $\\psi_{i,j}$ for sample $i$ and exon $j$. If $\\psi_{i,j}$ is missing, it is replaced by $\\hat{\\psi}_{i,j} = \\mathrm{median}(\\{\\psi_{k,j} \\mid \\psi_{k,j} \\text{ is observed, for sample } k \\text{ in cohort } c\\})$.\n\nThe choice of the median is deliberate. As a robust measure of central tendency, the median is insensitive to extreme outliers, which can be present in splicing data. This prevents a single aberrant sample from skewing the imputed value, thereby introducing less bias than mean-imputation would.\n\n### Step 2: Variance-Stabilizing Transformation (VST)\n\nThe PSI value $\\psi$ is an estimator of a binomial proportion. The variance of such an estimator is dependent on the proportion itself: $\\mathrm{Var}(\\hat{\\psi}) \\propto \\psi(1-\\psi)$. This property, known as heteroscedasticity, means that the variance is maximal at $\\psi=0.5$ and approaches zero near the boundaries $\\psi=0$ and $\\psi=1$. Standard clustering methods like Ward's, which are based on Euclidean distance, perform optimally when the variance is constant (homoscedastic) across the range of the data. Without transformation, differences in PSI values near the boundaries would be inappropriately down-weighted relative to identical differences near the center of the range.\n\nTo address this, a variance-stabilizing transformation is applied. The problem suggests the arcsine-square-root transformation, which is the standard VST for binomial proportions. For each imputed PSI value $\\psi_{i,j}$, the transformed value $y_{i,j}$ is calculated as:\n$$y_{i,j} = \\arcsin(\\sqrt{\\psi_{i,j}})$$\nThe angles are measured in radians. A result of the delta method from statistics shows that for a random variable $X \\sim \\mathrm{Binomial}(N, p)$, the transformed variable $Y = \\arcsin(\\sqrt{X/N})$ has a variance that is approximately constant and independent of $p$:\n$$\\mathrm{Var}(Y) \\approx \\frac{1}{4N}$$\nwhere $N$ is the total number of trials (in this context, related to the sequencing read depth). By stabilizing the variance, this transformation ensures that the distance metric used in the subsequent clustering step treats differences in splicing levels comparably, regardless of their position in the $[0,1]$ interval.\n\n### Step 3: Robust Per-Exon Weighting\n\nNot all exons are equally informative for distinguishing between biological subtypes. Some exons may exhibit little to no variation across all samples and are thus uninformative for clustering. Others may show high variability that is characteristic of distinct sample groups. The algorithm incorporates a weighting scheme to up-weight these informative exons.\n\nThe weight for each exon $j$ is derived from its dispersion across the samples. To ensure robustness against outlier samples, the dispersion is measured by the Median Absolute Deviation (MAD), a robust statistic defined as:\n$$\\mathrm{MAD}_j = \\mathrm{median}_i( | y_{i,j} - \\mathrm{median}_k(y_{k,j}) | )$$\nwhere the medians are computed over all samples $i,k$ in the cohort. A small constant $\\epsilon = 10^{-3}$ is added to the MAD to prevent exons with zero dispersion (i.e., constant across all samples) from having a weight of zero, which would completely remove them from the analysis. The raw weight for exon $j$ is $w'_j = \\mathrm{MAD}_j + \\epsilon$.\n\nThese raw weights are then normalized such that their sum equals the total number of exons, $p$:\n$$w_j = w'_j \\cdot \\frac{p}{\\sum_{k=1}^p w'_k}$$\nThis normalization maintains the total \"informational budget\" while redistributing it among exons based on their robustly measured informativeness.\n\n### Step 4: Hierarchical Clustering in a Weighted Euclidean Space\n\nWith the transformed and weighted data, we can now cluster the samples. The process is conceptualized as performing clustering in a weighted Euclidean space. The squared distance between two samples, indexed $a$ and $b$, is defined as:\n$$d^2(a, b) = \\sum_{j=1}^{p} w_j (y_{a,j} - y_{b,j})^2$$\nThis is equivalent to first scaling each transformed feature column $j$ by the square root of its weight, $\\sqrt{w_j}$, and then computing the standard Euclidean distance in this scaled space. Let the scaled data matrix be $Z$, where $Z_{i,j} = y_{i,j} \\sqrt{w_j}$.\n\nThe problem specifies using Ward's linkage criterion for agglomerative hierarchical clustering. Ward's method seeks to merge pairs of clusters that result in the minimum increase in the total within-cluster sum of squares. The objective function is to minimize the total variance within the clusters. This approach tends to produce compact, spherical clusters, which is often a desirable property when identifying distinct sample groups. The clustering process yields a dendrogram, a tree structure representing the nested merging of samples and clusters.\n\n### Step 5: Dendrogram Partitioning and Label Assignment\n\nThe final step is to partition the samples into a pre-specified number of clusters, $K^{(c)}$, for each cohort $c$. This is achieved by \"cutting\" the dendrogram at a level that produces exactly $K^{(c)}$ distinct clusters.\n\nThe clustering algorithm assigns an arbitrary integer label to each cluster. To ensure a deterministic and standardized output, these labels are remapped to the set $\\{0, 1, \\dots, K^{(c)}-1\\}$. The remapping rule is based on the order of the first appearance of a member of each cluster, following the original sample order ($i=0, 1, \\dots, n-1$). The first-encountered cluster is assigned label $0$, the second-encountered new cluster is assigned label $1$, and so on. This produces a unique and reproducible list of cluster assignments for each cohort.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.cluster.hierarchy import ward, fcluster\n\ndef solve():\n    \"\"\"\n    Main function to run the splicing analysis on all test cohorts.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"P\": np.array([\n                [0.82, 0.71, 0.58, 0.93, 0.18, 0.27, 0.39, 0.12],\n                [0.78, 0.69, 0.62, 0.88, 0.22, 0.33, 0.41, 0.09],\n                [0.81, 0.73, 0.59, 0.91, 0.21, 0.31, 0.38, 0.15],\n                [0.19, 0.29, 0.43, 0.11, 0.81, 0.72, 0.64, 0.88],\n                [0.23, 0.34, 0.39, 0.09, 0.77, 0.68, 0.58, 0.92],\n                [0.17, 0.27, 0.41, 0.13, 0.83, 0.73, 0.61, 0.87]\n            ]),\n            \"K\": 2\n        },\n        {\n            \"P\": np.array([\n                [0.5, 0.5, 0.5, 0.5, 0.5],\n                [0.5, 0.5, 0.5, 0.5, 0.5],\n                [0.5, 0.5, 0.5, 0.5, 0.5],\n                [0.5, 0.5, 0.5, 0.5, 0.5]\n            ]),\n            \"K\": 1\n        },\n        {\n            \"P\": np.array([\n                [0.85, 0.78, np.nan, 0.52, 0.18, 0.22, 0.60],\n                [0.80, 0.75, np.nan, 0.49, 0.20, 0.25, 0.58],\n                [0.25, 0.30, 0.55, np.nan, 0.82, 0.75, 0.40],\n                [0.28, 0.33, 0.58, np.nan, 0.78, 0.70, 0.42],\n                [0.55, np.nan, 0.20, 0.85, 0.45, np.nan, 0.15]\n            ]),\n            \"K\": 3\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        P, K = case[\"P\"], case[\"K\"]\n        result = process_cohort(P, K)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef process_cohort(P, K):\n    \"\"\"\n    Processes a single cohort according to the specified algorithm.\n    \"\"\"\n    n, p = P.shape\n    epsilon = 1e-3\n\n    # Step 1: Impute missing PSI values with the exon-wise sample median.\n    P_imputed = np.copy(P)\n    # np.nanmedian ignores NaNs by default.\n    col_medians = np.nanmedian(P_imputed, axis=0)\n    \n    # Find indices of NaNs and replace them with corresponding column medians\n    nan_inds = np.where(np.isnan(P_imputed))\n    P_imputed[nan_inds] = np.take(col_medians, nan_inds[1])\n\n    # Step 2: Apply the arcsine-square-root variance-stabilizing transformation.\n    # The domain of arcsin is [-1, 1], and sqrt of PSI [0,1] is in [0,1], so this is safe.\n    Y = np.arcsin(np.sqrt(P_imputed))\n\n    # Step 3: Compute robust dispersion weights per exon.\n    # We implement MAD manually to ensure it's unscaled, as specified.\n    # MAD_j = median_i( |y_ij - median_k(y_kj)| )\n    y_medians = np.median(Y, axis=0, keepdims=True)\n    abs_deviations = np.abs(Y - y_medians)\n    mad_weights = np.median(abs_deviations, axis=0)\n\n    # Add epsilon and normalize weights to sum to p.\n    raw_weights = mad_weights + epsilon\n    normalized_weights = raw_weights * (p / np.sum(raw_weights))\n\n    # Step 4: Form a weighted geometry and perform hierarchical clustering.\n    # Scale each feature j by the square root of its weight.\n    Z = Y * np.sqrt(normalized_weights)\n    \n    # Perform hierarchical clustering using Ward's linkage.\n    # Ward's linkage operates on the condensed distance matrix or the original data.\n    # scipy.cluster.hierarchy.ward takes the n x m observation matrix.\n    linkage_matrix = ward(Z)\n\n    # Step 5: Cut the dendrogram and remap cluster labels.\n    # fcluster returns labels from 1 to K.\n    raw_labels = fcluster(linkage_matrix, t=K, criterion='maxclust')\n\n    # Remap labels to {0, 1, ..., K-1} by order of first occurrence.\n    final_labels = np.zeros(n, dtype=int)\n    mapping = {}\n    next_label = 0\n    for i in range(n):\n        raw_label = raw_labels[i]\n        if raw_label not in mapping:\n            mapping[raw_label] = next_label\n            next_label += 1\n        final_labels[i] = mapping[raw_label]\n        \n    return final_labels.tolist()\n\n\nsolve()\n\n```", "id": "4556866"}]}