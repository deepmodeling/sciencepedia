{"hands_on_practices": [{"introduction": "At the heart of differential expression analysis lies the statistical model that relates gene counts to experimental conditions. This practice contrasts two key approaches: a traditional linear model on transformed data (log-CPM) and the more statistically rigorous Generalized Linear Model (GLM) on raw counts. By implementing both methods, you will gain a practical understanding of how design matrices encode experimental factors and how model choice impacts the estimation of biological effects [@problem_id:4556329].", "problem": "You are given a toy single-gene dataset measured across two biological conditions and two technical batches. Your task is to construct a design matrix that encodes condition and batch effects with reference levels, and then estimate the fitted coefficients in two modeling frameworks: a linear model on log-transformed Counts Per Million (CPM) using Ordinary Least Squares (OLS), and a Poisson Generalized Linear Model (GLM) with a logarithmic link using Maximum Likelihood Estimation (MLE). Your program must compute these estimates for a provided test suite and output the condition-effect coefficients for each test case in a single line, in the exact format specified below.\n\nStart from the following fundamental bases and core definitions:\n- Sequencing read counts for a gene in sample $i$, denoted $c_i$, are nonnegative integers and are commonly modeled as conditionally independent Poisson random variables given an expected mean $ \\mu_i $, satisfying $ y_i \\sim \\operatorname{Poisson}(\\mu_i) $ for observed counts $ y_i = c_i $. The Poisson mean $ \\mu_i $ scales with the sample-specific library size $ L_i $ through an offset, with the canonical log-link in Generalized Linear Models (GLMs) defined by $ \\eta_i = \\log(\\mu_i) = \\log(L_i) + \\mathbf{x}_i^\\top \\boldsymbol{\\beta} $, where $ \\mathbf{x}_i $ are the covariates and $ \\boldsymbol{\\beta} $ are regression coefficients.\n- Counts Per Million (CPM) is defined as $ \\operatorname{CPM}_i = 10^6 \\cdot \\frac{c_i}{L_i} $. A stabilized log-transformation is commonly used to mitigate heteroscedasticity and zero counts. Use the natural logarithm with an additive constant $\\delta$ to define $ y_i^{(\\log\\mathrm{CPM})} = \\log\\left(\\operatorname{CPM}_i + \\delta\\right) $, where $ \\delta = 1 $.\n- Ordinary Least Squares (OLS) estimates for linear models on the transformed response follow from minimizing the sum of squared residuals. For a design matrix $ \\mathbf{X} $ and response vector $ \\mathbf{y} $, the OLS estimator is the solution to the normal equations $ \\mathbf{X}^\\top \\mathbf{X} \\, \\widehat{\\boldsymbol{\\beta}} = \\mathbf{X}^\\top \\mathbf{y} $.\n- Maximum Likelihood Estimation (MLE) for Poisson GLM with logarithmic link and offset $ \\log(L_i) $ maximizes the log-likelihood $ \\ell(\\boldsymbol{\\beta}) = \\sum_i \\left( y_i \\eta_i - \\exp(\\eta_i) - \\log(y_i!) \\right) $, where $ \\eta_i = \\log(L_i) + \\mathbf{x}_i^\\top \\boldsymbol{\\beta} $. The Iteratively Reweighted Least Squares (IRLS) algorithm is used to compute the MLE, iterating weighted least squares updates with working weights $ w_i = \\mu_i $ and working response $ z_i = \\eta_i + \\frac{y_i - \\mu_i}{\\mu_i} $, where $ \\mu_i = \\exp(\\eta_i) $.\n\nDesign matrix specification:\n- There are two conditions, denoted $A$ and $B$, and two batches, denoted $1$ and $2$. Use reference coding with condition $A$ and batch $1$ as the reference levels.\n- For $n$ samples, construct $ \\mathbf{X} \\in \\mathbb{R}^{n \\times 3} $ with columns $ [\\text{intercept}, \\mathbb{1}\\{\\text{condition}=B\\}, \\mathbb{1}\\{\\text{batch}=2\\}] $. The intercept column is all ones $1$, the second column is an indicator of condition $B$, and the third column is an indicator of batch $2$.\n\nEstimation tasks per test case:\n1. Compute $ \\operatorname{CPM}_i = 10^6 \\cdot \\frac{c_i}{L_i} $ for each sample $i$, then compute $ y_i^{(\\log\\mathrm{CPM})} = \\log\\left(\\operatorname{CPM}_i + 1\\right) $. Fit the linear model $ y^{(\\log\\mathrm{CPM})} = \\mathbf{X}\\boldsymbol{\\beta} + \\varepsilon $ by OLS and report the condition effect coefficient $ \\widehat{\\beta}_{\\text{cond}} $ (the coefficient on the condition indicator).\n2. Fit the Poisson GLM with log-link and offset $ \\log(L_i) $ by IRLS to maximize the Poisson log-likelihood, and report the condition effect coefficient $ \\widehat{\\beta}_{\\text{cond}} $.\n\nTest suite:\nFor all cases, there are $ n = 8 $ samples indexed $ i = 1, \\dots, 8 $, with conditions $ A $ for $ i \\in \\{1,2,3,4\\} $ and $ B $ for $ i \\in \\{5,6,7,8\\} $, and batches $ 1 $ for $ i \\in \\{1,3,5,7\\} $ and $ 2 $ for $ i \\in \\{2,4,6,8\\} $. The design matrix $ \\mathbf{X} $ uses this encoding.\n\n- Case 1 (balanced, moderate counts):\n    - Library sizes $ L = [5{,}000{,}000, \\; 4{,}500{,}000, \\; 5{,}500{,}000, \\; 4{,}800{,}000, \\; 5{,}200{,}000, \\; 4{,}700{,}000, \\; 5{,}100{,}000, \\; 4{,}900{,}000] $.\n    - Counts $ c = [50, \\; 36, \\; 55, \\; 38, \\; 78, \\; 56, \\; 76, \\; 59] $.\n\n- Case 2 (edge case with zeros and low counts):\n    - Library sizes $ L = [3{,}000{,}000, \\; 3{,}500{,}000, \\; 4{,}000{,}000, \\; 2{,}500{,}000, \\; 3{,}000{,}000, \\; 3{,}500{,}000, \\; 4{,}000{,}000, \\; 2{,}500{,}000] $.\n    - Counts $ c = [0, \\; 1, \\; 0, \\; 2, \\; 3, \\; 0, \\; 4, \\; 1] $.\n\n- Case 3 (null condition effect, batch effect present):\n    - Library sizes $ L = [5{,}000{,}000, \\; 5{,}000{,}000, \\; 5{,}000{,}000, \\; 5{,}000{,}000, \\; 5{,}000{,}000, \\; 5{,}000{,}000, \\; 5{,}000{,}000, \\; 5{,}000{,}000] $.\n    - Counts $ c = [60, \\; 48, \\; 60, \\; 48, \\; 60, \\; 48, \\; 60, \\; 48] $.\n\nAnswer specification:\n- For each test case, return a pair of floats $ [\\widehat{\\beta}_{\\text{cond}}^{(\\mathrm{OLS})}, \\widehat{\\beta}_{\\text{cond}}^{(\\mathrm{MLE})}] $ corresponding to the condition $B$ coefficient from the OLS on $ \\log(\\mathrm{CPM}+1) $ and the Poisson GLM MLE, respectively.\n- Express the outputs as natural logarithm coefficients (unitless), rounded to six decimal places.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the pair for one test case, in order. For example: $ [[x_1,y_1],[x_2,y_2],[x_3,y_3]] $.", "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in established statistical models for RNA-sequencing data analysis, well-posed with a complete and consistent setup, and expressed in objective, formal language. The task is to estimate regression coefficients for a given dataset under two distinct modeling frameworks.\n\n### 1. Design Matrix Construction\nThe experimental design involves $n=8$ samples, categorized by two factors: condition (levels $A$ and $B$) and technical batch (levels $1$ and $2$). The problem specifies a reference coding scheme where condition $A$ and batch $1$ are the reference levels. The design matrix $\\mathbf{X} \\in \\mathbb{R}^{8 \\times 3}$ models the expected value of the response as a function of an intercept, the effect of being in condition $B$ relative to $A$, and the effect of being in batch $2$ relative to $1$.\n\nThe columns of $\\mathbf{X}$ are defined as $[\\text{intercept}, \\mathbb{1}\\{\\text{condition}=B\\}, \\mathbb{1}\\{\\text{batch}=2\\}]$. Given the sample annotations:\n- Samples $1, 2, 3, 4$: Condition $A$\n- Samples $5, 6, 7, 8$: Condition $B$\n- Samples $1, 3, 5, 7$: Batch $1$\n- Samples $2, 4, 6, 8$: Batch $2$\n\nThe resulting design matrix $\\mathbf{X}$ is:\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1  0  0 \\\\\n1  0  1 \\\\\n1  0  0 \\\\\n1  0  1 \\\\\n1  1  0 \\\\\n1  1  1 \\\\\n1  1  0 \\\\\n1  1  1\n\\end{pmatrix}\n$$\nThe coefficient vector to be estimated is $\\boldsymbol{\\beta} = [\\beta_0, \\beta_{\\text{cond}}, \\beta_{\\text{batch}}]^\\top$, where $\\beta_0$ is the intercept, $\\beta_{\\text{cond}}$ is the coefficient for condition $B$, and $\\beta_{\\text{batch}}$ is the coefficient for batch $2$. Our objective is to estimate $\\beta_{\\text{cond}}$ under two different models.\n\n### 2. Model 1: Ordinary Least Squares (OLS) on Log-Transformed Counts\nThis approach models the data after a transformation aimed at stabilizing variance and making the data more amenable to linear modeling.\n\n**Data Transformation**:\nFirst, the raw counts $c_i$ for each sample $i$ are normalized by their respective library sizes $L_i$ to obtain Counts Per Million (CPM), mitigating biases from sequencing depth differences.\n$$\n\\operatorname{CPM}_i = 10^6 \\cdot \\frac{c_i}{L_i}\n$$\nSecond, a natural logarithm transformation is applied with an added offset (pseudocount) of $\\delta=1$. This step further reduces heteroscedasticity (the dependence of variance on the mean) and ensures that zero counts are handled without resulting in undefined values. The transformed response vector $\\mathbf{y}^{(\\log\\mathrm{CPM})}$ has elements:\n$$\ny_i^{(\\log\\mathrm{CPM})} = \\log(\\operatorname{CPM}_i + 1)\n$$\n**Linear Model and Estimation**:\nThe transformed data are then modeled with a standard linear model, assuming the errors $\\varepsilon_i$ are independent and identically distributed with a mean of $0$:\n$$\n\\mathbf{y}^{(\\log\\mathrm{CPM})} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n$$\nThe OLS estimator $\\widehat{\\boldsymbol{\\beta}}_{\\text{OLS}}$ is found by minimizing the sum of squared residuals, which yields the well-known normal equations. The solution is:\n$$\n\\widehat{\\boldsymbol{\\beta}}_{\\text{OLS}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}^{(\\log\\mathrm{CPM})}\n$$\nThe estimated condition effect, $\\widehat{\\beta}_{\\text{cond}}^{(\\mathrm{OLS})}$, is the second component of the vector $\\widehat{\\boldsymbol{\\beta}}_{\\text{OLS}}$.\n\n### 3. Model 2: Poisson Generalized Linear Model (GLM)\nThis approach models the raw counts directly, assuming they follow a Poisson distribution. This is a more principled approach as it respects the discrete, non-negative nature of count data.\n\n**Model Formulation**:\nThe counts $y_i = c_i$ are modeled as independent samples from a Poisson distribution, $y_i \\sim \\operatorname{Poisson}(\\mu_i)$. The mean count $\\mu_i$ is related to the predictors via a logarithmic link function. The library size $L_i$ is incorporated as an offset term, which is a known predictor with a fixed coefficient of $1$. The full model for the linear predictor $\\eta_i$ is:\n$$\n\\eta_i = \\log(\\mu_i) = \\log(L_i) + \\mathbf{x}_i^\\top \\boldsymbol{\\beta}\n$$\nwhere $\\mathbf{x}_i^\\top$ is the $i$-th row of the design matrix $\\mathbf{X}$.\n\n**Maximum Likelihood Estimation via IRLS**:\nThe coefficients $\\boldsymbol{\\beta}$ are estimated by maximizing the Poisson log-likelihood function:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( y_i \\eta_i - e^{\\eta_i} - \\log(y_i!) \\right)\n$$\nThis maximization problem is solved numerically using the Iteratively Reweighted Least Squares (IRLS) algorithm. Starting with an initial guess $\\boldsymbol{\\beta}^{(0)}$ (e.g., a vector of zeros), the algorithm iteratively performs the following steps until the estimates for $\\boldsymbol{\\beta}$ converge:\n1.  **Compute Linear Predictor**: $\\boldsymbol{\\eta}^{(t)} = \\log(\\mathbf{L}) + \\mathbf{X}\\boldsymbol{\\beta}^{(t)}$.\n2.  **Compute Fitted Means**: $\\boldsymbol{\\mu}^{(t)} = \\exp(\\boldsymbol{\\eta}^{(t)})$.\n3.  **Compute Working Weights**: The weights are diagonal entries of a matrix $\\mathbf{W}^{(t)}$, with $w_i^{(t)} = \\mu_i^{(t)}$.\n4.  **Compute Working Response**: The adjusted response vector $\\mathbf{z}^{(t)}$ is computed as $z_i^{(t)} = \\eta_i^{(t)} + \\frac{y_i - \\mu_i^{(t)}}{\\mu_i^{(t)}}$.\n5.  **Update Coefficients**: The next estimate $\\boldsymbol{\\beta}^{(t+1)}$ is obtained by solving a weighted least squares problem. This involves fitting the model $(\\mathbf{z}^{(t)} - \\log(\\mathbf{L})) = \\mathbf{X}\\boldsymbol{\\beta}$ with weights $\\mathbf{W}^{(t)}$. The solution is given by:\n    $$\n    \\boldsymbol{\\beta}^{(t+1)} = \\left(\\mathbf{X}^\\top \\mathbf{W}^{(t)} \\mathbf{X}\\right)^{-1} \\mathbf{X}^\\top \\mathbf{W}^{(t)} \\left(\\mathbf{z}^{(t)} - \\log(\\mathbf{L})\\right)\n    $$\nUpon convergence, the resulting vector $\\widehat{\\boldsymbol{\\beta}}_{\\text{MLE}}$ contains the maximum likelihood estimates. The condition effect, $\\widehat{\\beta}_{\\text{cond}}^{(\\mathrm{MLE})}$, is the second component of this vector.\n\nThe final program implements these two methods for each of the three test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_irls_poisson(X, y, L, max_iter=50, tol=1e-8):\n    \"\"\"\n    Fits a Poisson GLM with a log-link using Iteratively Reweighted Least Squares (IRLS).\n\n    Args:\n        X (np.ndarray): Design matrix of shape (n_samples, n_features).\n        y (np.ndarray): Count data vector of shape (n_samples,).\n        L (np.ndarray): Library sizes vector of shape (n_samples,).\n        max_iter (int): Maximum number of iterations.\n        tol (float): Convergence tolerance for the sum of absolute changes in coefficients.\n\n    Returns:\n        np.ndarray: Estimated coefficients (beta).\n    \"\"\"\n    offset = np.log(L)\n    # Initialize coefficients to zeros\n    beta = np.zeros(X.shape[1])\n\n    for i in range(max_iter):\n        # Linear predictor (eta)\n        eta = offset + X @ beta\n        \n        # Mean response (mu)\n        mu = np.exp(eta)\n        \n        # Add a small epsilon for numerical stability, to avoid division by zero\n        # if mu becomes extremely small.\n        mu = np.maximum(mu, np.finfo(float).eps)\n\n        # Working response (z)\n        z = eta + (y - mu) / mu\n        \n        # Working weights (w) are the means for a Poisson GLM with canonical link\n        w = mu\n        \n        # The response for the Weighted Least Squares (WLS) step is the\n        # working response minus the offset.\n        # y_wls = z - offset\n        y_wls = (X @ beta) + (y - mu) / mu\n\n        # Solve the WLS system: (X.T W X) beta = X.T W y_wls\n        # Use element-wise multiplication for efficiency instead of creating a diagonal matrix W.\n        # X_T_W is equivalent to X.T @ np.diag(w)\n        X_T_W = X.T * w  # Broadcasting w across rows of X.T\n        X_T_W_X = X_T_W @ X\n        X_T_W_y_wls = X_T_W @ y_wls\n        \n        try:\n            beta_new = np.linalg.solve(X_T_W_X, X_T_W_y_wls)\n        except np.linalg.LinAlgError:\n            # If the matrix is singular, use pseudo-inverse as a fallback\n            beta_new = np.linalg.pinv(X_T_W_X) @ X_T_W_y_wls\n\n        # Check for convergence\n        if np.sum(np.abs(beta_new - beta))  tol:\n            beta = beta_new\n            break\n            \n        beta = beta_new\n        \n    return beta\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and compute OLS and MLE coefficients.\n    \"\"\"\n    # Design matrix is constant for all test cases\n    # Columns: Intercept, Condition B, Batch 2\n    X = np.array([\n        [1, 0, 0], [1, 0, 1], [1, 0, 0], [1, 0, 1],  # Condition A\n        [1, 1, 0], [1, 1, 1], [1, 1, 0], [1, 1, 1]   # Condition B\n    ], dtype=float)\n\n    # Test suite from the problem statement\n    test_cases = [\n        {\n            \"L\": [5_000_000, 4_500_000, 5_500_000, 4_800_000, 5_200_000, 4_700_000, 5_100_000, 4_900_000],\n            \"c\": [50, 36, 55, 38, 78, 56, 76, 59]\n        },\n        {\n            \"L\": [3_000_000, 3_500_000, 4_000_000, 2_500_000, 3_000_000, 3_500_000, 4_000_000, 2_500_000],\n            \"c\": [0, 1, 0, 2, 3, 0, 4, 1]\n        },\n        {\n            \"L\": [5_000_000, 5_000_000, 5_000_000, 5_000_000, 5_000_000, 5_000_000, 5_000_000, 5_000_000],\n            \"c\": [60, 48, 60, 48, 60, 48, 60, 48]\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        L = np.array(case[\"L\"], dtype=float)\n        c = np.array(case[\"c\"], dtype=float)\n\n        # Task 1: OLS on log(CPM+1)\n        cpm = 1e6 * c / L\n        y_logcpm = np.log(cpm + 1)\n        beta_ols = np.linalg.solve(X.T @ X, X.T @ y_logcpm)\n        beta_ols_cond = beta_ols[1]\n\n        # Task 2: Poisson GLM MLE via IRLS\n        beta_mle = run_irls_poisson(X, c, L)\n        beta_mle_cond = beta_mle[1]\n        \n        # Store the rounded results for the condition effect coefficient\n        all_results.append([round(beta_ols_cond, 6), round(beta_mle_cond, 6)])\n\n    # Format output string to be exactly [[x1,y1],[x2,y2],...] with no spaces\n    # Example: [[0.518861,0.510826],[-0.076961,0.405465],[0.0,0.0]]\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "4556329"}, {"introduction": "Real-world genomic data are often affected by technical variability, such as batch effects, which can confound the biological signals of interest. This simulation exercise powerfully demonstrates the concept of omitted-variable bias, showing how failing to include a known source of variation in your statistical model can lead to a massive number of false positives. Completing this practice will solidify your understanding of why proper experimental design and comprehensive model specification are essential for valid scientific inference [@problem_id:2385475].", "problem": "Write a complete, runnable program that simulates gene expression data under a linear model with a strong batch effect and evaluates the impact of omitting the batch covariate during differential gene expression analysis. Your program must implement the following scientifically grounded and widely accepted base principles without relying on any shortcut formulas provided in the problem statement.\n\nBase principles to use:\n- Gene expression on an appropriate transformed scale (for example, a logarithmic scale) can be modeled by a linear model with Gaussian noise. For gene index $g$ and sample index $i$, let $y_{g i}$ denote the expression. The model includes a gene-specific baseline term, a condition term, a batch term, and an independent noise term.\n- Ordinary Least Squares (OLS) under the Gaussian noise assumption yields unbiased estimators in correctly specified linear models and provides test statistics for hypotheses on model coefficients that follow the Student's $t$-distribution under the null.\n- Multiple hypothesis testing across many genes should be controlled using the False Discovery Rate (FDR). Use the Benjamini–Hochberg (BH) procedure at target level $q = 0.05$.\n\nSimulation design and hypothesis testing:\n- All genes share the same generative model structure and are simulated under the global null of no true differential expression between the two biological conditions. Specifically, for every gene $g$, the condition effect is $0$; the batch effect is shared across genes as a fixed shift between two batches; the baseline level may vary by gene; and the noise is independent and identically distributed Gaussian across samples within a gene, with constant variance.\n- The program must estimate, for each gene, the two-sided $p$-value for the null hypothesis that the condition coefficient is zero using:\n  1. A misspecified model that omits the batch covariate (condition-only analysis).\n  2. A correctly specified model that includes both condition and batch covariates (condition-plus-batch analysis).\n- Apply the Benjamini–Hochberg (BH) procedure at target level $q = 0.05$ across all genes in each analysis to obtain the number of discoveries (these are all false discoveries because the data are simulated under the global null).\n\nScientific rationale to be demonstrated:\n- When the condition and batch covariates are correlated (confounding) and the batch effect is strong, omitting the batch covariate induces bias in the estimated condition effect and inflates false discoveries, whereas including the batch covariate removes the bias and controls false discoveries near the target FDR level.\n- When the condition and batch covariates are orthogonal, omitting the batch covariate does not induce bias in the estimated condition effect; false discoveries should be close to the target FDR level even without adjustment.\n- As a boundary case, when the batch effect is zero, including or omitting the batch covariate should yield similar behavior.\n\nTest suite:\nSimulate $m = 2000$ genes in each case. Use independent baseline levels $\\mu_g \\sim \\mathcal{N}(0, 1)$ for $g = 1, \\dots, m$. For each case, generate independent Gaussian noise $\\varepsilon_{g i} \\sim \\mathcal{N}(0, \\sigma^2)$ with $\\sigma = 0.5$.\n\nEncode the binary condition covariate as $x_i \\in \\{0, 1\\}$ for the two conditions, and the binary batch covariate as $z_i \\in \\{0, 1\\}$ for the two batches. The batch shift is a constant $\\gamma$ added for $z_i = 1$ and $0$ for $z_i = 0$. The true condition effect is $0$ for all genes. Use the following three cases, each with a specified sample composition across condition and batch:\n- Case $1$ (high confounding, strong batch): batch $1$ has $19$ samples from condition $0$ and $1$ sample from condition $1$; batch $2$ has $1$ sample from condition $0$ and $19$ samples from condition $1$; $\\gamma = 1.5$; total samples $n = 40$.\n- Case $2$ (orthogonal design, strong batch): batch $1$ has $10$ samples from condition $0$ and $10$ samples from condition $1$; batch $2$ has $10$ samples from condition $0$ and $10$ samples from condition $1$; $\\gamma = 1.5$; total samples $n = 40$.\n- Case $3$ (high confounding, no batch): batch $1$ has $9$ samples from condition $0$ and $1$ sample from condition $1$; batch $2$ has $1$ sample from condition $0$ and $9$ samples from condition $1$; $\\gamma = 0.0$; total samples $n = 20$.\n\nRandomness and reproducibility:\n- Use a fixed random seed of $20240513$ to initialize the generator for Case $1$. For Cases $2$ and $3$, you must add the case index to the base seed (that is, $20240513 + 2$ and $20240513 + 3$) to ensure independence across cases while retaining reproducibility.\n\nStatistical analysis requirements:\n- For each gene and each case, compute the two-sided $p$-value testing the null hypothesis that the condition coefficient equals $0$ using:\n  1. A condition-only linear model with an intercept.\n  2. A condition-plus-batch linear model with an intercept.\n- Use the Benjamini–Hochberg (BH) procedure at level $q = 0.05$ on the set of $m$ $p$-values within each analysis to determine the number of discoveries (rejections). Since all nulls are true by construction, every discovery is a false positive. Report the numbers of discoveries for both analyses.\n\nRequired final output format:\n- Your program should produce a single line of output containing $6$ comma-separated integers enclosed in square brackets, in the following order: $[\\text{case1\\_naive}, \\text{case1\\_adjusted}, \\text{case2\\_naive}, \\text{case2\\_adjusted}, \\text{case3\\_naive}, \\text{case3\\_adjusted}]$, where “naive” denotes the condition-only analysis and “adjusted” denotes the condition-plus-batch analysis.\n\nNo external inputs:\n- The program must be entirely self-contained and must not read any input or files or access any network resources. All numerical values must be hard coded as specified above.\n\nAngle units and physical units:\n- No angles or physical units are involved.\n\nAnswer type:\n- Each of the $6$ reported values must be an integer.\n\nYour goal is to implement the simulation and analysis so that the output demonstrates, through these test cases, how omitting a strong, confounded batch effect inflates false discoveries in differential gene expression analysis, while including the batch indicator restores valid inference.", "solution": "The problem presented is a valid, well-posed exercise in computational statistics, designed to demonstrate a critical principle in the analysis of high-throughput biological data: the danger of omitted-variable bias. The specific context is differential gene expression analysis, where unmeasured or unmodeled technical factors, such as experimental batches, can confound the biological signals of interest. The problem is scientifically grounded, using standard linear models, Ordinary Least Squares (OLS), Student's $t$-tests, and the Benjamini-Hochberg (BH) procedure for False Discovery Rate (FDR) control, which are foundational methods in the field. The simulation parameters are clearly specified, ensuring the problem is self-contained and reproducible. We shall proceed with the solution.\n\nThe core of the problem lies in the general linear model, which we write in matrix form for a single gene as $Y = X\\beta + \\epsilon$. Here, $Y$ is an $n \\times 1$ vector of expression measurements for $n$ samples, $X$ is an $n \\times p$ design matrix encoding the experimental covariates for the $p$ parameters, $\\beta$ is a $p \\times 1$ vector of coefficients to be estimated, and $\\epsilon$ is an $n \\times 1$ vector of independent and identically distributed error terms, assumed to follow a Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$.\n\nThe data are simulated under a \"global null\" scenario, meaning there is no true differential expression. For each of $m=2000$ genes, indexed by $g$, and $n$ samples, indexed by $i$, the expression $y_{gi}$ is generated according to the true model:\n$$y_{gi} = \\mu_g + \\gamma z_i + \\varepsilon_{gi}$$\nwhere $\\mu_g \\sim \\mathcal{N}(0, 1)$ is the gene-specific baseline expression, $\\gamma$ is the magnitude of the batch effect, $z_i \\in \\{0, 1\\}$ is the batch indicator covariate, and $\\varepsilon_{gi} \\sim \\mathcal{N}(0, \\sigma^2)$ is the noise term with $\\sigma=0.5$. The true coefficient for the biological condition is zero.\n\nWe will analyze these simulated data using two different models for each gene:\n\n1.  A **misspecified or \"naive\" model**, which omits the batch covariate:\n    $y_{gi} = \\beta_{g0} + \\beta_{g1} x_i + e_{gi}$. The design matrix $X_{\\text{naive}}$ has two columns: an intercept (a vector of ones) and the condition covariate vector $x$. We test the null hypothesis $H_0: \\beta_{g1} = 0$.\n\n2.  A **correctly specified or \"adjusted\" model**, which includes the batch covariate:\n    $y_{gi} = \\beta'_{g0} + \\beta'_{g1} x_i + \\beta'_{g2} z_i + e'_{gi}$. The design matrix $X_{\\text{adj}}$ has three columns: an intercept, the condition vector $x$, and the batch vector $z$. We test the null hypothesis $H_0: \\beta'_{g1} = 0$.\n\nFor both models, the coefficients $\\beta$ are estimated using Ordinary Least Squares (OLS), for which the solution is $\\hat{\\beta} = (X^T X)^{-1}X^T Y$. This can be solved efficiently for all $m$ genes simultaneously.\n\nTo test the significance of the condition coefficient $\\hat{\\beta}_1$ (where the subscript $1$ denotes the coefficient for the condition covariate $x$), we compute the Student's $t$-statistic:\n$$t = \\frac{\\hat{\\beta}_1}{\\text{SE}(\\hat{\\beta}_1)}$$\nThe standard error, $\\text{SE}(\\hat{\\beta}_1)$, is the square root of the estimated variance of the coefficient estimator. The variance-covariance matrix of the estimators is given by $\\text{Var}(\\hat{\\beta}) = \\sigma^2 (X^T X)^{-1}$. We estimate the unknown error variance $\\sigma^2$ with its unbiased estimator, the mean squared error:\n$$\\hat{\\sigma}^2 = \\frac{1}{n-p} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{\\text{RSS}}{n-p}$$\nwhere $\\hat{y}_i$ are the fitted values from the model, $\\text{RSS}$ is the residual sum of squares, $n$ is the number of samples, and $p$ is the number of parameters in the model (the number of columns in $X$). Let $C = (X^T X)^{-1}$. The specific variance for $\\hat{\\beta}_1$ is $\\hat{\\sigma}^2 C_{11}$ (assuming the condition covariate is the second column of $X$, indexed by $1$). The standard error is thus $\\text{SE}(\\hat{\\beta}_1) = \\sqrt{\\hat{\\sigma}^2 C_{11}}$. Under the null hypothesis, this $t$-statistic follows a Student's $t$-distribution with $n-p$ degrees of freedom. From this distribution, we compute a two-sided $p$-value for each gene.\n\nSince we are performing $m=2000$ hypothesis tests, one for each gene, we must correct for multiple testing to control the number of false discoveries. We will use the Benjamini-Hochberg (BH) procedure at a target False Discovery Rate (FDR) of $q = 0.05$. The procedure is as follows:\n1.  Sort the $m$ $p$-values in ascending order: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$.\n2.  Find the largest integer $k$ such that $p_{(k)} \\le \\frac{k}{m} q$.\n3.  If such a $k$ exists, reject the null hypotheses for all tests corresponding to $p_{(1)}, \\dots, p_{(k)}$. The number of discoveries is $k$. If no such $k$ exists, no hypotheses are rejected, and the number of discoveries is $0$.\n\nWe will implement this entire process for the three specified cases, which differ in their experimental design (confounding vs. orthogonal) and the strength of the batch effect.\n\n-   **Case 1 (Confounding):** The condition and batch covariates are strongly correlated. When the batch effect $\\gamma$ is large, omitting the batch covariate $z$ will lead to a biased estimate of the condition coefficient $\\beta_1$. This bias, known as omitted-variable bias, is proportional to the true batch effect and the correlation between the batch and condition covariates. This bias will systematically shift the estimated condition effects away from zero, leading to a large number of small $p$-values and a massive inflation of false discoveries in the naive analysis. The adjusted model, by accounting for $z$, will remove this bias and provide valid inference, with the number of false discoveries properly controlled near the expected level.\n\n-   **Case 2 (Orthogonal):** The condition and batch covariates are uncorrelated (orthogonal design). In this scenario, the omitted-variable bias term is zero. Therefore, even when a strong batch effect is present, the estimate of the condition coefficient in the naive model remains unbiased. Both a naive and an adjusted analysis are expected to control the false discovery rate correctly.\n\n-   **Case 3 (Confounding, No Batch Effect):** The design is confounded as in Case 1, but the batch effect size $\\gamma$ is zero. The omitted-variable bias is proportional to $\\gamma$, so if $\\gamma=0$, there is no bias. Both the naive and adjusted models should perform correctly, similar to Case 2.\n\nThe program will systematically execute these simulations, perform both naive and adjusted analyses for each case, apply the BH procedure, and report the resulting number of false discoveries, thereby quantitatively demonstrating these fundamental statistical principles.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation suite and print the final results.\n    \"\"\"\n\n    # Global parameters\n    m = 2000  # Number of genes\n    sigma = 0.5  # Noise standard deviation\n    q_level = 0.05  # Target FDR level for BH procedure\n\n    # Case 1: High confounding, strong batch\n    case1_params = {\n        'n_cond0_batch1': 19, 'n_cond1_batch1': 1,\n        'n_cond0_batch2': 1, 'n_cond1_batch2': 19\n    }\n    case1_gamma = 1.5\n    case1_seed = 20240513\n    \n    # Case 2: Orthogonal design, strong batch\n    case2_params = {\n        'n_cond0_batch1': 10, 'n_cond1_batch1': 10,\n        'n_cond0_batch2': 10, 'n_cond1_batch2': 10\n    }\n    case2_gamma = 1.5\n    case2_seed = 20240513 + 2\n\n    # Case 3: High confounding, no batch effect\n    case3_params = {\n        'n_cond0_batch1': 9, 'n_cond1_batch1': 1,\n        'n_cond0_batch2': 1, 'n_cond1_batch2': 9\n    }\n    case3_gamma = 0.0\n    case3_seed = 20240513 + 3\n    \n    test_cases = [\n        (case1_params, case1_gamma, m, sigma, q_level, case1_seed),\n        (case2_params, case2_gamma, m, sigma, q_level, case2_seed),\n        (case3_params, case3_gamma, m, sigma, q_level, case3_seed),\n    ]\n\n    results = []\n    for params in test_cases:\n        naive_discoveries, adjusted_discoveries = run_simulation(*params)\n        results.extend([naive_discoveries, adjusted_discoveries])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(case_params, gamma, m, sigma, q, seed):\n    \"\"\"\n    Runs a single simulation case.\n    \n    Generates data, performs naive and adjusted analyses, and returns the number of discoveries for each.\n    \"\"\"\n    np.random.seed(seed)\n\n    # 1. Construct design vectors (x for condition, z for batch)\n    n01 = case_params['n_cond0_batch1']\n    n11 = case_params['n_cond1_batch1']\n    n02 = case_params['n_cond0_batch2']\n    n12 = case_params['n_cond1_batch2']\n    \n    n_batch1 = n01 + n11\n    n_batch2 = n02 + n12\n    n = n_batch1 + n_batch2\n\n    x = np.array([0]*n01 + [1]*n11 + [0]*n02 + [1]*n12)\n    z = np.array([0]*n_batch1 + [1]*n_batch2)\n\n    # 2. Generate gene expression data\n    # True model: y = mu + gamma*z + noise\n    mu_g = np.random.normal(0, 1, size=(m, 1))\n    noise = np.random.normal(0, sigma, size=(m, n))\n    Y = mu_g + gamma * z[np.newaxis, :] + noise\n\n    # 3. Define design matrices\n    X_naive = np.vstack([np.ones(n), x]).T\n    X_adjusted = np.vstack([np.ones(n), x, z]).T\n\n    # 4. Perform analyses and get discovery counts\n    naive_discoveries = perform_analysis(Y, X_naive, q)\n    adjusted_discoveries = perform_analysis(Y, X_adjusted, q)\n\n    return naive_discoveries, adjusted_discoveries\n\ndef perform_analysis(Y, X, q):\n    \"\"\"\n    Performs OLS regression and multiple testing correction for a set of genes.\n    \"\"\"\n    n, p = X.shape # n = samples, p = parameters\n    m = Y.shape[0] # m = genes\n    \n    # 1. Fit linear model for all genes at once using np.linalg.lstsq\n    # Y is (m, n), X is (n, p). We need to solve X @ B.T = Y.T for B.\n    # B will be (m, p). lstsq returns coefficients as (p, m).\n    beta_hat, rss_per_gene, _, _ = np.linalg.lstsq(X, Y.T, rcond=None)\n\n    # 2. Calculate t-statistics for the condition coefficient (at index 1)\n    df = n - p\n    sigma_sq_hat = rss_per_gene / df\n    \n    # The variance of beta_hat is diag(inv(X'X)) * sigma_hat^2\n    # We are interested in the coefficient for the condition 'x', which is at index 1\n    C = np.linalg.inv(X.T @ X)\n    se_beta1 = np.sqrt(sigma_sq_hat * C[1, 1])\n\n    # Avoid division by zero if standard error is somehow zero\n    # This should not happen in this problem's setup\n    t_stats = np.zeros(m)\n    valid_se = se_beta1 > 0\n    t_stats[valid_se] = beta_hat[1, valid_se] / se_beta1[valid_se]\n    \n    # 3. Calculate two-sided p-values\n    p_values = 2 * t.sf(np.abs(t_stats), df=df)\n\n    # 4. Apply Benjamini-Hochberg procedure\n    num_discoveries = bh_procedure(p_values, q)\n    \n    return num_discoveries\n\ndef bh_procedure(p_values, q):\n    \"\"\"\n    Applies the Benjamini-Hochberg procedure to control FDR.\n    \"\"\"\n    m = len(p_values)\n    if m == 0:\n        return 0\n        \n    p_values_sorted = np.sort(p_values)\n    ranks = np.arange(1, m + 1)\n    thresholds = (ranks / m) * q\n    \n    # Find all p-values that are below the BH threshold\n    significant_mask = p_values_sorted = thresholds\n    \n    if np.any(significant_mask):\n        # The number of discoveries is the rank of the last p-value\n        # that is below its threshold\n        k = np.max(np.where(significant_mask))\n        num_discoveries = k + 1\n    else:\n        num_discoveries = 0\n        \n    return num_discoveries\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2385475"}, {"introduction": "A typical transcriptomics study involves testing thousands of genes simultaneously, which necessitates a robust correction for multiple comparisons to control the rate of false discoveries. This exercise guides you through implementing the Benjamini-Hochberg (BH) procedure, a standard and powerful method for controlling the False Discovery Rate (FDR). By coding the BH algorithm from scratch, you will move beyond simply using a function and gain a deep, procedural understanding of how $p$-values are adjusted to maintain statistical rigor in high-dimensional settings [@problem_id:2385494].", "problem": "You are given collections of raw $p$-values arising from $m$ independent gene-level null hypotheses in a differential gene expression experiment comparing two conditions. Let the set of null hypotheses be $\\{H_1,\\dots,H_m\\}$ with corresponding raw $p$-values $p_1,\\dots,p_m \\in [0,1]$. For a target level $q \\in (0,1)$, the Benjamini–Hochberg (BH) procedure for controlling the False Discovery Rate (FDR) at level $q$ is defined as follows. Write the $p$-values in nondecreasing order as $p_{(1)} \\le \\dots \\le p_{(m)}$, where the subscript denotes order statistics. Define the index\n$$\nk \\;=\\; \\max\\left\\{ i \\in \\{1,\\dots,m\\} \\;:\\; p_{(i)} \\le \\frac{i}{m}\\,q \\right\\},\n$$\nwith the convention that if the set is empty then no hypotheses are rejected. The BH rejection set is then all hypotheses with raw $p$-values not exceeding the threshold $p_{(k)}$, that is,\n$$\n\\mathcal{R} \\;=\\; \\{ j \\in \\{1,\\dots,m\\} \\;:\\; p_j \\le p_{(k)} \\},\n$$\nwhen $k$ exists, and $\\mathcal{R}=\\varnothing$ otherwise. The BH adjusted $p$-values (also called BH $q$-values in some contexts) are defined for each hypothesis by first assigning to the $i$-th order statistic the value\n$$\n\\tilde{p}_{(i)} \\;=\\; \\min_{j \\in \\{i,\\dots,m\\}} \\left( \\frac{m}{j}\\,p_{(j)} \\right),\n$$\nthen truncating at $1$ by $\\min\\{\\tilde{p}_{(i)},1\\}$, and finally mapping back to the original hypothesis order. To make the rank assignment deterministic in the presence of ties among equal $p$-values, use a stable ordering that breaks ties by increasing original index, that is, if $p_a=p_b$ and $ab$ then $p_a$ precedes $p_b$ in the order. Index hypotheses by their original positions using $0$-based indices.\n\nTask. For each test case below, given a list of raw $p$-values and a target FDR level $q$, compute:\n- the list of BH adjusted $p$-values in the original hypothesis order, each rounded to $6$ decimal places (expressed as decimals, not as percentages), and\n- the list of rejected hypothesis indices (using $0$-based indexing) in increasing order under the BH procedure at level $q$.\n\nTest suite. Evaluate your implementation on the following four cases, which together cover a typical case, boundary values, ties, and a case with no rejections:\n- Case $1$: $p=(0.001,\\,0.04,\\,0.03,\\,0.2,\\,0.5,\\,0.0005,\\,0.07,\\,0.9)$, $q=0.1$.\n- Case $2$: $p=(0,\\,1,\\,0.5,\\,0.05,\\,0.2)$, $q=0.05$.\n- Case $3$: $p=(0.02,\\,0.02,\\,0.02,\\,0.5,\\,0.8,\\,0.9)$, $q=0.1$.\n- Case $4$: $p=(0.2,\\,0.3,\\,0.4,\\,0.6,\\,0.8)$, $q=0.01$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one element per test case. Each element must itself be a two-element list whose first element is the list of rounded BH adjusted $p$-values (in original order) and whose second element is the list of rejected $0$-based indices in increasing order. No other text should be printed. For example, the overall structure must be of the form\n[[adj_case1,rej_case1],[adj_case2,rej_case2],[adj_case3,rej_case3],[adj_case4,rej_case4]]\nwhere adj_case$k$ and rej_case$k$ are lists for case $k$ as specified above.", "solution": "The problem statement is assessed to be valid. It presents a clear, well-defined computational task based on the standard and scientifically sound Benjamini-Hochberg procedure for controlling the False Discovery Rate, a fundamental method in computational biology and statistics. The definitions, conditions, and test cases are complete, consistent, and objective, permitting a unique and verifiable solution.\n\nThe task is to implement the Benjamini-Hochberg (BH) procedure. Given a set of $m$ raw $p$-values $\\{p_1, \\dots, p_m\\}$ from multiple hypothesis tests and a target False Discovery Rate (FDR) level $q$, we must compute the BH adjusted $p$-values and identify the set of rejected hypotheses. The procedure is deterministic and will be implemented through a sequence of principled steps derived directly from the provided mathematical definitions.\n\nLet $m$ be the total number of hypotheses. The algorithm proceeds as follows:\n\n1.  **Data Structuring and Sorting**: Each raw $p$-value $p_j$ is associated with its original $0$-based index $j$. The resulting pairs $(p_j, j)$ are then sorted in non-decreasing order of $p_j$. The problem specifies a stable sorting mechanism to handle ties: if $p_a=p_b$ for original indices $ab$, the pair corresponding to $a$ must precede the pair for $b$. This step yields the ordered $p$-values $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$, along with their original indices. The rank of $p_{(i)}$ is $i$.\n\n2.  **Identification of the Rejection Threshold**: The core of the BH procedure is to find the largest rank $k \\in \\{1,\\dots,m\\}$ for which the ordered $p$-value $p_{(k)}$ satisfies the condition:\n    $$\n    p_{(k)} \\le \\frac{k}{m}q\n    $$\n    This condition compares the $k$-th smallest $p$-value to a threshold that becomes linearly more lenient with increasing rank $k$. If the set of ranks satisfying this inequality is empty, it is concluded that no hypotheses can be rejected at the specified FDR level $q$, and the rejection set $\\mathcal{R}$ is empty. Otherwise, the value $p_{(k)}$ corresponding to the largest such rank $k$ becomes the significance threshold.\n\n3.  **Determination of the Rejection Set**: The set of rejected hypotheses, $\\mathcal{R}$, consists of all hypotheses whose original, unadjusted $p$-value $p_j$ is less than or equal to the threshold $p_{(k)}$ found in the previous step. That is, $\\mathcal{R} = \\{ j \\in \\{1,\\dots,m\\} : p_j \\le p_{(k)} \\}$. The final output requires the $0$-based indices of these hypotheses, sorted in increasing order.\n\n4.  **Computation of Adjusted $p$-values**: The BH adjusted $p$-value for the hypothesis corresponding to the $i$-th ordered raw $p$-value, $p_{(i)}$, is defined as:\n    $$\n    \\tilde{p}_{(i)} = \\min_{j \\in \\{i,\\dots,m\\}} \\left( \\frac{m}{j}p_{(j)} \\right)\n    $$\n    This is then truncated at $1$ if necessary. This definition ensures that the adjusted $p$-values are monotonically non-decreasing with respect to their rank, i.e., $\\tilde{p}_{(1)} \\le \\tilde{p}_{(2)} \\le \\dots \\le \\tilde{p}_{(m)}$. An efficient computational strategy for this step involves first computing the scaled values $\\frac{m}{j}p_{(j)}$ for all ranks $j=1, \\dots, m$. Then, one iterates backwards from rank $m$ down to $1$, calculating the cumulative minimum at each step. Specifically, the adjusted $p$-value for rank $i$ is the minimum of its own scaled value and the adjusted $p$-value of rank $i+1$. The final adjusted $p$-values are then mapped back to their original hypothesis order and rounded to $6$ decimal places as required.\n\nThe implementation will apply this four-step logic to each of the provided test cases to generate the required outputs.", "answer": "```python\nimport numpy as np\n\ndef _format_output(data):\n    \"\"\"\n    Custom recursive function to format the final list into a string\n    without spaces and with floats formatted to 6 decimal places.\n    \"\"\"\n    if isinstance(data, list):\n        return f\"[{','.join(_format_output(item) for item in data)}]\"\n    if isinstance(data, float):\n        return f\"{data:.6f}\"\n    return str(data)\n\ndef benjamini_hochberg(p_values: np.ndarray, q: float):\n    \"\"\"\n    Performs the Benjamini-Hochberg procedure for FDR control.\n\n    Args:\n        p_values: A numpy array of raw p-values.\n        q: The target False Discovery Rate level.\n\n    Returns:\n        A tuple containing:\n        - A list of BH adjusted p-values, rounded to 6 decimal places, in original order.\n        - A sorted list of 0-based indices of rejected hypotheses.\n    \"\"\"\n    m = len(p_values)\n    if m == 0:\n        return [], []\n        \n    original_indices = np.arange(m)\n    \n    # Sort p-values while keeping track of original indices.\n    # The 'stable' kind ensures that for equal p-values, the original order is preserved,\n    # satisfying the problem's tie-breaking rule.\n    sorted_order_indices = np.argsort(p_values, kind='stable')\n    sorted_p_values = p_values[sorted_order_indices]\n    \n    # --- Step 1: Find rejection threshold ---\n    ranks = np.arange(1, m + 1)\n    bh_thresholds = (ranks / m) * q\n    \n    significant_mask = sorted_p_values = bh_thresholds\n    \n    rejected_indices = []\n    if np.any(significant_mask):\n        # Find the largest rank k satisfying the BH condition\n        k = np.max(ranks[significant_mask])\n        \n        # The rejection threshold is the p-value at this rank k\n        rejection_threshold = sorted_p_values[k - 1]\n        \n        # Identify all hypotheses with original p-values = threshold\n        rejected_mask = p_values = rejection_threshold\n        rejected_indices = original_indices[rejected_mask].tolist()\n\n    # --- Step 2: Calculate adjusted p-values ---\n    # Calculate raw scaled p-values: (m/i) * p_(i)\n    scaled_p_values = (m / ranks) * sorted_p_values\n    \n    # Enforce monotonicity by taking the cumulative minimum from the end (right-to-left)\n    adj_p_sorted = np.minimum.accumulate(scaled_p_values[::-1])[::-1]\n    \n    # Truncate values at 1.0\n    adj_p_sorted = np.minimum(adj_p_sorted, 1.0)\n    \n    # Unsort the adjusted p-values to match the original p-value order\n    adj_p_original = np.empty(m)\n    adj_p_original[sorted_order_indices] = adj_p_sorted\n    \n    # Round to 6 decimal places for final output\n    adj_p_rounded = [round(p, 6) for p in adj_p_original]\n\n    return adj_p_rounded, rejected_indices\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the final result.\n    \"\"\"\n    test_cases = [\n        ((0.001, 0.04, 0.03, 0.2, 0.5, 0.0005, 0.07, 0.9), 0.1),\n        ((0.0, 1.0, 0.5, 0.05, 0.2), 0.05),\n        ((0.02, 0.02, 0.02, 0.5, 0.8, 0.9), 0.1),\n        ((0.2, 0.3, 0.4, 0.6, 0.8), 0.01),\n    ]\n\n    results = []\n    for p_tuple, q_val in test_cases:\n        p_values_np = np.array(p_tuple)\n        adj_p, rej_idx = benjamini_hochberg(p_values_np, q_val)\n        results.append([adj_p, rej_idx])\n    \n    # Print the final result in the specified single-line format\n    print(_format_output(results))\n\nsolve()\n```", "id": "2385494"}]}