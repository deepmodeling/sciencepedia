{"hands_on_practices": [{"introduction": "When analyzing pathways from high-throughput experiments, we perform many statistical tests simultaneously, which increases the chance of finding false positives. To address this, we must adjust our significance thresholds. This exercise provides hands-on practice with the Benjamini-Hochberg procedure, a cornerstone method for controlling the False Discovery Rate (FDR) by converting raw $p$-values into more reliable $q$-values. Mastering this technique is essential for drawing robust conclusions from any large-scale functional analysis [@problem_id:4565348].", "problem": "A case-control gene set enrichment analysis evaluates functional pathways in a disease cohort, producing nominal pathway $p$-values from independent tests of association for $m=12$ predefined pathways. In high-dimensional multiple testing, the False Discovery Rate (FDR) is defined as the expected proportion of rejected null hypotheses that are actually true among all rejections. To control FDR at a target level, the Benjamini-Hochberg (BH) procedure assigns adjusted $q$-values to each pathway. Using the following pathway list and their nominal $p$-values, compute the BH-adjusted $q$-values for all pathways and then determine how many pathways would be declared significant at an FDR threshold $\\alpha=0.05$. Report the count of significant pathways as your final answer.\n\nThe pathways and their $p$-values are:\nDNA damage response: $0.0009$; Interferon gamma response: $0.004$; Nuclear factor kappa-light-chain-enhancer of activated B cells (NF-$\\kappa$B) signaling: $0.012$; Glycolysis and glucose metabolism: $0.018$; Apoptosis: $0.021$; Mitogen-Activated Protein Kinase (MAPK) cascade: $0.035$; T cell receptor signaling: $0.048$; Hypoxia: $0.06$; Phosphoinositide 3-Kinase - Protein Kinase B (PI3K-AKT) signaling: $0.11$; Mechanistic Target of Rapamycin (mTOR) signaling: $0.17$; Wingless-related integration site (WNT) signaling: $0.24$; Angiogenesis: $0.5$.\n\nAssume the pathway-level tests are independent or satisfy positive regression dependence on a subset, so that the BH procedure controls the FDR at the nominal level. Provide a scientifically grounded interpretation of FDR control when explaining your reasoning, but the final reported quantity must be the integer count of significant pathways at $\\alpha=0.05$. No rounding is required for the final answer.", "solution": "The problem requires the application of the Benjamini-Hochberg (BH) procedure to a set of nominal $p$-values from a gene set enrichment analysis in order to control the False Discovery Rate (FDR), and then to determine the number of pathways that are statistically significant at a specified FDR threshold.\n\nFirst, we establish the theoretical background. In multiple hypothesis testing, where many statistical tests are performed simultaneously, there is an increased risk of making Type I errors (false positives). The False Discovery Rate is a statistical method used to correct for this. The FDR is defined as the expected value of the proportion of falsely rejected null hypotheses (false discoveries) among all rejected null hypotheses (total discoveries). Let $V$ be the number of true null hypotheses that are incorrectly rejected, and $R$ be the total number of rejected null hypotheses. The FDR is given by $E\\left[\\frac{V}{R}\\right]$, where the fraction is taken to be $0$ if $R=0$. The Benjamini-Hochberg procedure provides a way to control this FDR at a specified level $\\alpha$.\n\nThe problem provides $m=12$ pathways and their corresponding nominal $p$-values. The procedure is as follows:\n1.  Let the individual $p$-values be $p_1, p_2, \\ldots, p_m$.\n2.  Order these $p$-values in ascending order: $p_{(1)} \\le p_{(2)} \\le \\ldots \\le p_{(m)}$.\n3.  For each ordered $p$-value $p_{(k)}$, the corresponding BH-adjusted $p$-value, or $q$-value, is calculated. The $q$-value for the test with the $k$-th smallest $p$-value, denoted $q_{(k)}$, is given by the formula:\n$$q_{(k)} = \\min_{j=k, \\dots, m} \\left( \\frac{m \\cdot p_{(j)}}{j} \\right)$$\nThis formula ensures that the sequence of adjusted $q$-values is monotonically non-decreasing, i.e., $q_{(1)} \\le q_{(2)} \\le \\ldots \\le q_{(m)}$. A practical way to compute this is to first calculate the intermediate values $\\frac{m \\cdot p_{(k)}}{k}$ for each rank $k$, and then enforce the monotonicity by iterating backwards from $k=m$ to $k=1$. Specifically, $q_{(m)} = p_{(m)}$, and for $k = m-1, \\ldots, 1$, we have $q_{(k)} = \\min\\left(q_{(k+1)}, \\frac{m \\cdot p_{(k)}}{k}\\right)$.\n4.  A hypothesis is rejected (i.e., a pathway is declared significant) if its adjusted $q$-value is less than or equal to the FDR threshold $\\alpha$.\n\nThe given data are:\nTotal number of tests (pathways), $m = 12$.\nFDR threshold, $\\alpha = 0.05$.\nThe nominal $p$-values are already sorted in ascending order:\n$p_{(1)} = 0.0009$\n$p_{(2)} = 0.004$\n$p_{(3)} = 0.012$\n$p_{(4)} = 0.018$\n$p_{(5)} = 0.021$\n$p_{(6)} = 0.035$\n$p_{(7)} = 0.048$\n$p_{(8)} = 0.06$\n$p_{(9)} = 0.11$\n$p_{(10)} = 0.17$\n$p_{(11)} = 0.24$\n$p_{(12)} = 0.5$\n\nWe now compute the adjusted $q$-values, $q_{(k)}$, for each rank $k$ from $1$ to $12$. We will compute them by iterating from $k=12$ down to $k=1$.\n\nFor $k=12$: $q_{(12)} = \\frac{12 \\cdot p_{(12)}}{12} = p_{(12)} = 0.5$.\nFor $k=11$: $q_{(11)} = \\min\\left(q_{(12)}, \\frac{12 \\cdot p_{(11)}}{11}\\right) = \\min\\left(0.5, \\frac{12 \\cdot 0.24}{11}\\right) = \\min\\left(0.5, \\frac{2.88}{11}\\right) = \\min(0.5, 0.26\\overline{18}) = 0.26\\overline{18}$.\nFor $k=10$: $q_{(10)} = \\min\\left(q_{(11)}, \\frac{12 \\cdot p_{(10)}}{10}\\right) = \\min\\left(0.26\\overline{18}, \\frac{12 \\cdot 0.17}{10}\\right) = \\min(0.26\\overline{18}, 0.204) = 0.204$.\nFor $k=9$: $q_{(9)} = \\min\\left(q_{(10)}, \\frac{12 \\cdot p_{(9)}}{9}\\right) = \\min\\left(0.204, \\frac{12 \\cdot 0.11}{9}\\right) = \\min(0.204, 0.14\\overline{6}) = 0.14\\overline{6}$.\nFor $k=8$: $q_{(8)} = \\min\\left(q_{(9)}, \\frac{12 \\cdot p_{(8)}}{8}\\right) = \\min\\left(0.14\\overline{6}, \\frac{12 \\cdot 0.06}{8}\\right) = \\min(0.14\\overline{6}, 0.09) = 0.09$.\nFor $k=7$: $q_{(7)} = \\min\\left(q_{(8)}, \\frac{12 \\cdot p_{(7)}}{7}\\right) = \\min\\left(0.09, \\frac{12 \\cdot 0.048}{7}\\right) = \\min\\left(0.09, \\frac{0.576}{7}\\right) \\approx \\min(0.09, 0.08228) \\approx 0.08228$. We will use the exact fraction $\\frac{0.576}{7}$. So $q_{(7)} = \\frac{0.576}{7}$.\nFor $k=6$: $q_{(6)} = \\min\\left(q_{(7)}, \\frac{12 \\cdot p_{(6)}}{6}\\right) = \\min\\left(\\frac{0.576}{7}, \\frac{12 \\cdot 0.035}{6}\\right) = \\min(\\frac{0.576}{7}, 0.07) \\approx \\min(0.08228, 0.07) = 0.07$.\nFor $k=5$: $q_{(5)} = \\min\\left(q_{(6)}, \\frac{12 \\cdot p_{(5)}}{5}\\right) = \\min\\left(0.07, \\frac{12 \\cdot 0.021}{5}\\right) = \\min(0.07, 0.0504) = 0.0504$.\nFor $k=4$: $q_{(4)} = \\min\\left(q_{(5)}, \\frac{12 \\cdot p_{(4)}}{4}\\right) = \\min\\left(0.0504, \\frac{12 \\cdot 0.018}{4}\\right) = \\min(0.0504, 0.054) = 0.0504$.\nFor $k=3$: $q_{(3)} = \\min\\left(q_{(4)}, \\frac{12 \\cdot p_{(3)}}{3}\\right) = \\min\\left(0.0504, \\frac{12 \\cdot 0.012}{3}\\right) = \\min(0.0504, 0.048) = 0.048$.\nFor $k=2$: $q_{(2)} = \\min\\left(q_{(3)}, \\frac{12 \\cdot p_{(2)}}{2}\\right) = \\min\\left(0.048, \\frac{12 \\cdot 0.004}{2}\\right) = \\min(0.048, 0.024) = 0.024$.\nFor $k=1$: $q_{(1)} = \\min\\left(q_{(2)}, \\frac{12 \\cdot p_{(1)}}{1}\\right) = \\min\\left(0.024, \\frac{12 \\cdot 0.0009}{1}\\right) = \\min(0.024, 0.0108) = 0.0108$.\n\nThe complete list of BH-adjusted $q$-values corresponding to the ordered $p$-values is:\n$q_{(1)} = 0.0108$ (DNA damage response)\n$q_{(2)} = 0.024$ (Interferon gamma response)\n$q_{(3)} = 0.048$ (NF-$\\kappa$B signaling)\n$q_{(4)} = 0.0504$ (Glycolysis and glucose metabolism)\n$q_{(5)} = 0.0504$ (Apoptosis)\n$q_{(6)} = 0.07$ (MAPK cascade)\n$q_{(7)} \\approx 0.0823$ (T cell receptor signaling)\n$q_{(8)} = 0.09$ (Hypoxia)\n$q_{(9)} \\approx 0.1467$ (PI3K-AKT signaling)\n$q_{(10)} = 0.204$ (mTOR signaling)\n$q_{(11)} \\approx 0.2618$ (WNT signaling)\n$q_{(12)} = 0.5$ (Angiogenesis)\n\nThe final step is to identify the pathways that are significant at the FDR threshold $\\alpha = 0.05$. We reject the null hypothesis for any pathway whose $q$-value is less than or equal to $0.05$.\n- Pathway 1 (DNA damage response): $q_{(1)} = 0.0108 \\le 0.05$. Significant.\n- Pathway 2 (Interferon gamma response): $q_{(2)} = 0.024 \\le 0.05$. Significant.\n- Pathway 3 (NF-$\\kappa$B signaling): $q_{(3)} = 0.048 \\le 0.05$. Significant.\n- Pathway 4 (Glycolysis and glucose metabolism): $q_{(4)} = 0.0504 > 0.05$. Not significant.\n\nSince the $q$-values are monotonically non-decreasing, all subsequent pathways (from rank $5$ to $12$) will also have $q$-values greater than $0.05$. Therefore, they are not significant.\n\nCounting the number of significant pathways, we find there are $3$.", "answer": "$$\\boxed{3}$$", "id": "4565348"}, {"introduction": "Gene Set Enrichment Analysis (GSEA) is a powerful method for determining if a predefined set of genes shows statistically significant, concordant differences between two biological states. Rather than just counting significant genes, GSEA evaluates enrichment based on the position of all genes in the pathway within a ranked list of all genes from the experiment. In this exercise, you will calculate the core components of the GSEA algorithm from first principles, including the running-sum statistic and the final Enrichment Score ($ES$), to build a deep, mechanistic understanding of how this widely used tool works [@problem_id:4565335].", "problem": "A cohort study of a complex disease generates a ranked list of genes based on differential expression using a signed statistic $r_{i}$ (for gene $i$), where positive values indicate higher expression in cases relative to controls and negative values indicate lower expression in cases. You will evaluate a single pathway using Gene Set Enrichment Analysis (GSEA) from first principles.\n\nConsider the ordered genome-wide list of $N=12$ genes $(g_{1},\\dots,g_{12})$ with their signed statistics\n$$\n\\begin{aligned}\n&g_{1}:~ r_{1}=2.8,\\quad g_{2}:~ r_{2}=2.6,\\quad g_{3}:~ r_{3}=2.1,\\quad g_{4}:~ r_{4}=1.9,\\quad g_{5}:~ r_{5}=1.5,\\quad g_{6}:~ r_{6}=0.9,\\\\\n&g_{7}:~ r_{7}=0.3,\\quad g_{8}:~ r_{8}=-0.2,\\quad g_{9}:~ r_{9}=-0.8,\\quad g_{10}:~ r_{10}=-1.2,\\quad g_{11}:~ r_{11}=-1.9,\\quad g_{12}:~ r_{12}=-2.4~,\n\\end{aligned}\n$$\nordered from $g_{1}$ (most positive) to $g_{12}$ (most negative). The pathway gene set is $S=\\{g_{2},g_{5},g_{9},g_{11}\\}$.\n\nStarting from foundational principles for enrichment over a ranked list, define a running-sum statistic $R_{k}$ over positions $k=1,\\dots,N$ that (i) increases at positions $k$ where the gene $g_{k}\\in S$ by a positive weight proportional to $\\lvert r_{k}\\rvert^{p}$ and scaled so that the total increments over $S$ sum to $1$, (ii) decreases at positions $k$ where $g_{k}\\notin S$ by a constant step scaled so that the total decrements over the complement of $S$ sum to $-1$, and (iii) starts at $R_{0}=0$ and returns to $R_{N}=0$. Adopt the classic weighting exponent $p=1$.\n\nUsing these definitions, compute the enrichment score $ES$ as the maximum positive deviation of $R_{k}$ from $0$ across $k=1,\\dots,N$. Then, given five null enrichment scores obtained from phenotype-label permutations for this same pathway,\n$$\nES_{\\text{null}}=\\{0.21,\\,-0.12,\\,0.25,\\,0.19,\\,-0.08\\},\n$$\ncompute the normalized enrichment score $NES$ by dividing the observed $ES$ by the mean of the positive values in $ES_{\\text{null}}$.\n\nFinally, interpret the leading-edge subset in quantitative terms by determining the number of pathway genes that occur at or before the position $k$ where the maximum positive deviation $ES$ is attained.\n\nExpress your final answer as a row matrix using the $\\mathrm{pmatrix}$ environment with entries $(ES,\\,NES,\\,\\text{leading-edge count})$. Round $ES$ and $NES$ to four significant figures.", "solution": "The problem requires us to perform a Gene Set Enrichment Analysis (GSEA) calculation from first principles for a single gene pathway. We are given a ranked list of $N=12$ genes with associated signed statistics $r_i$, and a gene set $S=\\{g_{2},g_{5},g_{9},g_{11}\\}$.\n\nFirst, we define and compute the running-sum statistic $R_k$ for the ordered gene list, from $k=1$ to $N=12$. The initial condition is $R_0 = 0$. The update rule for $R_k$ depends on whether the gene $g_k$ is in the gene set $S$ ('hit') or not ('miss').\n\nThe number of genes in the set $S$ is $|S|=4$. The number of genes not in $S$ is $|S^c| = N - |S| = 12 - 4 = 8$.\n\nFor a hit (i.e., $g_k \\in S$), the running sum increases by a step $\\Delta R_k^{\\text{hit}}$ proportional to $|r_k|^p$ with the exponent $p=1$. The sum of all such positive increments must equal $1$. We first compute the normalization factor, which is the sum of the weighted scores for all genes in $S$:\n$$\nN_S = \\sum_{g_i \\in S} |r_i|^p = |r_2|^1 + |r_5|^1 + |r_9|^1 + |r_{11}|^1\n$$\nUsing the given values:\n$$\nN_S = |2.6| + |1.5| + |-0.8| + |-1.9| = 2.6 + 1.5 + 0.8 + 1.9 = 6.8\n$$\nThe increment for a hit at position $k$ is therefore:\n$$\n\\Delta R_k^{\\text{hit}} = \\frac{|r_k|}{N_S} = \\frac{|r_k|}{6.8}\n$$\n\nFor a miss (i.e., $g_k \\notin S$), the running sum decreases by a constant step $\\Delta R_k^{\\text{miss}}$. The sum of all such negative decrements must equal $-1$. Since there are $|S^c|=8$ genes not in $S$, the constant decrement is:\n$$\n\\Delta R_k^{\\text{miss}} = \\frac{-1}{|S^c|} = \\frac{-1}{8} = -0.125\n$$\n\nNow we can compute the running sum $R_k = R_{k-1} + \\Delta R_k$ for $k=1, \\dots, 12$, starting with $R_0 = 0$.\n\n$k=1$: $g_1 \\notin S$. $R_1 = R_0 + \\Delta R_1^{\\text{miss}} = 0 - 0.125 = -0.125$.\n\n$k=2$: $g_2 \\in S$. $|r_2|=2.6$. $R_2 = R_1 + \\Delta R_2^{\\text{hit}} = -0.125 + \\frac{2.6}{6.8} = -\\frac{1}{8} + \\frac{26}{68} = -\\frac{17}{136} + \\frac{52}{136} = \\frac{35}{136} \\approx 0.25735$.\n\n$k=3$: $g_3 \\notin S$. $R_3 = R_2 + \\Delta R_3^{\\text{miss}} = \\frac{35}{136} - \\frac{1}{8} = \\frac{35}{136} - \\frac{17}{136} = \\frac{18}{136} \\approx 0.13235$.\n\n$k=4$: $g_4 \\notin S$. $R_4 = R_3 + \\Delta R_4^{\\text{miss}} = \\frac{18}{136} - \\frac{17}{136} = \\frac{1}{136} \\approx 0.00735$.\n\n$k=5$: $g_5 \\in S$. $|r_5|=1.5$. $R_5 = R_4 + \\Delta R_5^{\\text{hit}} = \\frac{1}{136} + \\frac{1.5}{6.8} = \\frac{1}{136} + \\frac{15}{68} = \\frac{1}{136} + \\frac{30}{136} = \\frac{31}{136} \\approx 0.22794$.\n\n$k=6$: $g_6 \\notin S$. $R_6 = R_5 + \\Delta R_6^{\\text{miss}} = \\frac{31}{136} - \\frac{17}{136} = \\frac{14}{136} \\approx 0.10294$.\n\n$k=7$: $g_7 \\notin S$. $R_7 = R_6 + \\Delta R_7^{\\text{miss}} = \\frac{14}{136} - \\frac{17}{136} = -\\frac{3}{136} \\approx -0.02206$.\n\n$k=8$: $g_8 \\notin S$. $R_8 = R_7 + \\Delta R_8^{\\text{miss}} = -\\frac{3}{136} - \\frac{17}{136} = -\\frac{20}{136} \\approx -0.14706$.\n\n$k=9$: $g_9 \\in S$. $|r_9|=0.8$. $R_9 = R_8 + \\Delta R_9^{\\text{hit}} = -\\frac{20}{136} + \\frac{0.8}{6.8} = -\\frac{20}{136} + \\frac{8}{68} = -\\frac{20}{136} + \\frac{16}{136} = -\\frac{4}{136} \\approx -0.02941$.\n\n$k=10$: $g_{10} \\notin S$. $R_{10} = R_9 + \\Delta R_{10}^{\\text{miss}} = -\\frac{4}{136} - \\frac{17}{136} = -\\frac{21}{136} \\approx -0.15441$.\n\n$k=11$: $g_{11} \\in S$. $|r_{11}|=1.9$. $R_{11} = R_{10} + \\Delta R_{11}^{\\text{hit}} = -\\frac{21}{136} + \\frac{1.9}{6.8} = -\\frac{21}{136} + \\frac{19}{68} = -\\frac{21}{136} + \\frac{38}{136} = \\frac{17}{136} = \\frac{1}{8} = 0.125$.\n\n$k=12$: $g_{12} \\notin S$. $R_{12} = R_{11} + \\Delta R_{12}^{\\text{miss}} = \\frac{1}{8} - \\frac{1}{8} = 0$.\nThe condition $R_N=0$ is satisfied, confirming the calculations.\n\nThe Enrichment Score ($ES$) is the maximum positive deviation of the running sum from $0$, which is the maximum value in the set $\\{R_1, \\dots, R_{12}\\}$.\n$$\nES = \\max_{k \\in \\{1, \\dots, 12\\}} R_k = R_2 = \\frac{35}{136} \\approx 0.2573529...\n$$\nRounding to four significant figures, $ES \\approx 0.2574$.\n\nNext, we calculate the Normalized Enrichment Score ($NES$). We are given a set of null enrichment scores: $ES_{\\text{null}}=\\{0.21, -0.12, 0.25, 0.19, -0.08\\}$. We need the mean of the positive values in this set.\nThe positive values are $\\{0.21, 0.25, 0.19\\}$.\n$$\n\\text{mean}(ES_{\\text{null, pos}}) = \\frac{0.21 + 0.25 + 0.19}{3} = \\frac{0.65}{3} \\approx 0.21666...\n$$\nThe $NES$ is the observed $ES$ divided by this mean:\n$$\nNES = \\frac{ES}{\\text{mean}(ES_{\\text{null, pos}})} = \\frac{35/136}{0.65/3} = \\frac{35}{136} \\times \\frac{3}{0.65} = \\frac{105}{88.4} = \\frac{1050}{884} = \\frac{525}{442} \\approx 1.1877828...\n$$\nRounding to four significant figures, $NES \\approx 1.188$.\n\nFinally, we identify the leading-edge subset. This subset consists of the genes in $S$ that appear in the ranked list at or before the position where the $ES$ is attained. The maximum value of the running sum, $ES$, occurred at position $k=2$. The genes in the ranked list up to this position are $\\{g_1, g_2\\}$.\nWe find the intersection of this set with the gene set $S$:\n$$\n\\text{Leading-Edge Genes} = \\{g_k | k \\le 2\\} \\cap S = \\{g_1, g_2\\} \\cap \\{g_2, g_5, g_9, g_{11}\\} = \\{g_2\\}\n$$\nThe number of genes in the leading-edge subset is the count of elements in this set.\n$$\n\\text{leading-edge count} = |\\{g_2\\}| = 1\n$$\n\nThe final required values are $ES \\approx 0.2574$, $NES \\approx 1.188$, and the leading-edge count is $1$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.2574 & 1.188 & 1 \\end{pmatrix}}\n$$", "id": "4565335"}, {"introduction": "A common temptation in network biology is to equate a gene's topological importance, such as its centrality in a co-expression network, with its causal importance for a biological process. This conceptual practice challenges that assumption, forcing a critical distinction between correlational evidence derived from observational data and causal evidence obtained through experimental perturbation. By evaluating different analytical strategies, you will learn to identify the limitations of network centralities and recognize the principled, \"gold standard\" approach of using interventional data, such as from a CRISPR screen, to validate causal hypotheses [@problem_id:4565379].", "problem": "A consortium builds a gene co-expression network from single-cell RNA sequencing (scRNA-seq) with $n$ genes measured across $N$ cells. For each gene $i$, detection of at least one transcript in a cell occurs with probability $p_i \\in (0,1)$, independently across cells and genes conditional on true expression. The observed adjacency matrix $\\mathbf{A} \\in \\{0,1\\}^{n \\times n}$ is constructed by connecting genes $i$ and $j$ if and only if: (i) both genes are detected in at least a fraction $q \\in (0,1)$ of cells, and (ii) the sample Pearson correlation between their observed expression exceeds a fixed threshold $c \\in (0,1)$. Researchers compute degree centrality $k_i = \\sum_{j \\neq i} A_{ij}$ and eigenvector centrality $e_i$ defined as the principal eigenvector of $\\mathbf{A}$, and plan to prioritize high-centrality genes as putative causal regulators of a viability phenotype $Y$.\n\nSimultaneously, an experimental team performs randomized Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) knockouts targeting each gene. Let $X_i \\in \\{0,1\\}$ denote the intervention on gene $i$ for a given cell, with $X_i=1$ indicating knockout and $X_i=0$ control, assigned independently with probability $r \\in (0,1)$. For each cell, viability $Y \\in \\mathbb{R}$ is measured post-intervention. Guide ribonucleic acid (gRNA) identity $G_i$ serves as an instrument indicating assignment to target gene $i$. Assume randomization satisfies exchangeability for $X_i$ and $Y$ conditional on $G_i$, and that the Stable Unit Treatment Value Assumption (SUTVA) holds at the single-cell level.\n\nConsider the following claims about interpreting centralities and alternatives for ranking gene importance with respect to $Y$:\n\nA. High eigenvector centrality $e_i$ in the co-expression network directly reflects causal importance for $Y$ because highly connected genes carry more information flow; therefore, ranking by $e_i$ is sufficient for target prioritization.\n\nB. Degree centrality $k_i$ is robust to missing edges and measurement bias when $n$ is large; thus $k_i$ is an unbiased proxy for essentiality, obviating the need for perturbation experiments.\n\nC. Betweenness centrality, computed on an aggregated protein-protein interaction (PPI) network across tissues, ensures causal relevance if the network is scale-free; thus betweenness can be treated as a causal score for viability.\n\nD. Estimate per-gene average treatment effects using interventional data, specifically the causal effect $\\operatorname{ATE}_i = \\mathbb{E}[Y \\mid \\operatorname{do}(X_i=1)] - \\mathbb{E}[Y \\mid \\operatorname{do}(X_i=0)]$, leveraging randomization and gRNA assignment $G_i$ as an instrument to adjust for incomplete knockouts and off-target activity; then rank genes by $\\operatorname{ATE}_i$ and perform pathway-level mediation analysis to separate direct and indirect effects.\n\nWhich option is most consistent with a principled approach grounded in experimental perturbation and corrects the overinterpretation of centrality metrics as biological importance? Provide justification rooted in first principles, including the implications of detection probabilities $p_i$ and randomization on bias and causal identification.", "solution": "This problem requires an evaluation of different strategies for identifying genes that causally influence a phenotype, contrasting methods based on observational network centralities with those based on interventional experimental data.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n-   **Observational Data:** A gene co-expression network from single-cell RNA sequencing (scRNA-seq).\n    -   $n$ genes, $N$ cells.\n    -   $p_i \\in (0,1)$: probability of detecting gene $i$ in a cell, independent across cells and genes conditional on true expression.\n    -   Adjacency matrix $\\mathbf{A} \\in \\{0,1\\}^{n \\times n}$.\n    -   Edge condition ($A_{ij}=1$):\n        1.  Both genes $i$ and $j$ are detected in at least a fraction $q \\in (0,1)$ of cells.\n        2.  Sample Pearson correlation of observed expression exceeds a threshold $c \\in (0,1)$.\n    -   Centrality metrics: Degree $k_i = \\sum_{j \\neq i} A_{ij}$ and eigenvector $e_i$ (principal eigenvector of $\\mathbf{A}$).\n    -   Goal: Use centrality to find \"putative causal regulators\" of phenotype $Y$.\n\n-   **Interventional Data:** Randomized CRISPR knockouts.\n    -   Intervention on gene $i$: $X_i \\in \\{0,1\\}$, with $X_i=1$ for knockout and $X_i=0$ for control.\n    -   Random assignment probability: $r \\in (0,1)$.\n    -   Outcome: Cell viability $Y \\in \\mathbb{R}$ measured post-intervention.\n    -   Instrumental Variable: Guide ribonucleic acid (gRNA) identity $G_i$ indicates assignment to target gene $i$.\n    -   Assumptions:\n        1.  Exchangeability: $\\mathbb{E}[X_i, Y \\mid G_i]$ satisfies randomization assumptions.\n        2.  Stable Unit Treatment Value Assumption (SUTVA) holds at the single-cell level.\n\n-   **Question:** Identify the option most consistent with a principled approach using experimental perturbation to correct the overinterpretation of centrality metrics.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem statement is firmly grounded in modern bioinformatics and systems biology. It accurately describes the construction of a co-expression network from scRNA-seq data, including the challenge of sparsity (modeled by detection probability $p_i$). It also correctly outlines a \"gold standard\" functional genomics experiment using CRISPR screens and frames the analysis in the language of causal inference (ATE, do-calculus, instrumental variables). The scenario is scientifically realistic and represents a common challenge in the field.\n-   **Well-Posed:** The problem is well-posed. It presents a clear scenario with two distinct data types (observational and interventional) and asks for a critical evaluation of analytical strategies, contrasting correlational and causal reasoning. A unique, best option can be identified based on established principles of statistics and causal inference.\n-   **Objective:** The problem is stated using precise, objective, and technical language common to statistics, computer science, and biology. All terms are standard and well-defined.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically sound, well-posed, and objective. There are no contradictions, ambiguities, or factual errors. I will proceed with the solution derivation.\n\n**Solution Derivation**\n\nThe core of this problem is the fundamental principle in science and statistics that **correlation does not imply causation**. The co-expression network is built from observational data and relies on Pearson correlation. Centrality metrics ($k_i$, $e_i$) are topological summaries of this correlation structure. Such measures are inherently correlational and can be misleading if interpreted causally.\n\nThe biases in network construction are significant:\n1.  **Sparsity and Detection Bias:** The detection probability $p_i < 1$ (dropout) means that the observed expression data is a distorted version of the true biological expression levels. Calculating correlations on this zero-inflated data is known to introduce severe artifacts. The requirement that genes be detected in a fraction $q$ of cells biases the network towards more highly or widely expressed genes, which are not necessarily more important causally.\n2.  **Correlation Thresholding:** The choice of a threshold $c$ is arbitrary and can dramatically alter network topology, making centrality measures unstable and unreliable.\n3.  **Confounding:** A high correlation between genes $i$ and $j$ can arise for many non-causal reasons, such as being regulated by a common hidden factor (a confounder), or participating in the same protein complex, which imposes stoichiometric constraints on their expression. An undirected edge $A_{ij}=1$ cannot distinguish between $i \\rightarrow j$, $j \\rightarrow i$, or $i \\leftarrow Z \\rightarrow j$.\n\nIn contrast, the CRISPR knockout experiment is an **intervention**. By randomly assigning a treatment (knockout of gene $i$), it breaks the influence of confounding variables. This allows for the estimation of the causal effect of manipulating a gene on the phenotype $Y$. This is the principled way to establish a gene's causal importance for a given phenotype.\n\n**Option-by-Option Analysis**\n\n**A. High eigenvector centrality $e_i$ in the co-expression network directly reflects causal importance for $Y$ because highly connected genes carry more information flow; therefore, ranking by $e_i$ is sufficient for target prioritization.**\n\nThis statement commits the \"correlation is causation\" fallacy. Eigenvector centrality, $e_i$, identifies genes that are connected to other highly-connected genes within the *correlation* network. As established, this network is a biased, non-causal representation of gene relationships. A high $e_i$ score might indicate a gene is part of a large, highly co-expressed module (e.g., ribosomal proteins), but it provides no direct evidence that the gene is a causal driver of the phenotype $Y$. The notion of \"information flow\" in a correlation network is metaphorical and cannot be equated with causal influence. Claiming this is \"sufficient\" for prioritization is a severe and common error in bioinformatics.\n\n**Verdict: Incorrect**\n\n**B. Degree centrality $k_i$ is robust to missing edges and measurement bias when $n$ is large; thus $k_i$ is an unbiased proxy for essentiality, obviating the need for perturbation experiments.**\n\nThis statement is factually incorrect on every point.\n1.  Degree centrality, $k_i$, is the count of edges connected to node $i$. It is, by definition, highly sensitive to the addition or removal of edges, and thus is *not* robust to missing or spurious edges, which are expected due to the biased network construction.\n2.  A large number of genes, $n$, does not mitigate the systematic biases in correlation estimation due to sparsity ($p_i$) or the arbitrary thresholding ($q, c$). The Law of Large Numbers does not cure systematic bias.\n3.  Because $k_i$ is derived from a biased, correlational network, it is a biased proxy for \"essentiality,\" a causal concept. The claim that it is an \"unbiased proxy\" is false.\n4.  The conclusion to obviate perturbation experiments is fundamentally anti-scientific. Interventional experiments are the gold standard for establishing causality, and no correlational summary can replace them.\n\n**Verdict: Incorrect**\n\n**C. Betweenness centrality, computed on an aggregated protein-protein interaction (PPI) network across tissues, ensures causal relevance if the network is scale-free; thus betweenness can be treated as a causal score for viability.**\n\nThis option is flawed and largely irrelevant to the problem setup.\n1.  It switches context from the specified scRNA-seq co-expression network to a completely different data type: an \"aggregated protein-protein interaction (PPI) network\". This is a distractor.\n2.  Even within the context of a PPI network, betweenness centrality is a topological measure, not inherently a causal one. It identifies nodes that act as \"bridges,\" but this does not automatically confer a causal role for an unrelated phenotype $Y$.\n3.  The claim that a scale-free property \"ensures causal relevance\" is a strong, unsupported assertion. The scale-free nature of a network is a topological property of its degree distribution; it does not magically imbue centrality measures with causal meaning for any arbitrary phenotype.\n\n**Verdict: Incorrect**\n\n**D. Estimate per-gene average treatment effects using interventional data, specifically the causal effect $\\operatorname{ATE}_i = \\mathbb{E}[Y \\mid \\operatorname{do}(X_i=1)] - \\mathbb{E}[Y \\mid \\operatorname{do}(X_i=0)]$, leveraging randomization and gRNA assignment $G_i$ as an instrument to adjust for incomplete knockouts and off-target activity; then rank genes by $\\operatorname{ATE}_i$ and perform pathway-level mediation analysis to separate direct and indirect effects.**\n\nThis option presents the most rigorous and causally-principled approach.\n1.  It correctly identifies the need to use the **interventional data** (CRISPR screen) to make causal claims.\n2.  It correctly defines the target quantity of interest as the **Average Treatment Effect (ATE)**, using the formal notation of the do-calculus, which represents a causal contrast.\n3.  It proposes a sophisticated and appropriate estimation strategy. By recognizing that the randomization occurs at the level of gRNA assignment ($G_i$), and that the actual knockout ($X_i$) may be imperfect, it correctly suggests using $G_i$ as an **instrumental variable (IV)**. This is a standard and powerful technique to obtain an unbiased estimate of the causal effect of $X_i$ on $Y$ under real-world experimental imperfections.\n4.  It proposes a rational prioritization strategy: **rank genes by their estimated $\\operatorname{ATE}_i$**. This directly measures their causal impact on viability.\n5.  It suggests a sound follow-up analysis (mediation analysis) to further dissect the causal mechanisms. This comprehensive plan is the antithesis of the simplistic and flawed approaches in A, B, and C. It corrects the overinterpretation of centrality by replacing it with a direct measure of causal effect.\n\n**Verdict: Correct**", "answer": "$$\\boxed{D}$$", "id": "4565379"}]}