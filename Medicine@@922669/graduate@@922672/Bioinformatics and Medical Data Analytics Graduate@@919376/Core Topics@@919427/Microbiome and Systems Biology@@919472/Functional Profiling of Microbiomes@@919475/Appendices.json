{"hands_on_practices": [{"introduction": "The journey from raw sequencing reads to meaningful functional profiles begins with accurate quantification. A fundamental challenge in this process is accounting for technical biases, such as the fact that longer genes or transcripts naturally accumulate more reads. This practice [@problem_id:4565555] guides you through the application of Transcripts Per Million ($TPM$) normalization, a standard method for correcting this length bias. By calculating $TPM$ values and comparing them to raw proportions using Kullback-Leibler divergence, you will gain a quantitative understanding of how normalization shapes our view of functional abundance.", "problem": "A metatranscriptomic dataset from a human gut microbiome was profiled for functional abundance using KEGG Orthology (KO) features. Short reads were quality-filtered and aligned to four KO features with effective sequence lengths (accounting for read mappability and trimming). The following observed alignments and effective lengths were recorded for one sample:\n- KO-A: length $900$ base pairs (bp), aligned reads $360$.\n- KO-B: length $1800$ bp, aligned reads $540$.\n- KO-C: length $300$ bp, aligned reads $120$.\n- KO-D: length $1500$ bp, aligned reads $180$.\n\nAssume the standard length correction and per-million scaling used in Transcripts Per Million (TPM) normalization for functional profiling: counts must be corrected for feature length to yield a rate per kilobase, and then scaled so the sum across features equals $10^{6}$.\n\nStarting from fundamental definitions in sequencing-based quantification (that read coverage is proportional to the product of feature length and its true abundance, and that scaling produces a compositional measure), compute the TPM values for these KO features and form the TPM composition across the four features. Form the raw-count composition from the observed counts. Using the natural logarithm, quantify how length correction alters the relative abundance of functions by computing the Kullback–Leibler divergence from the TPM composition to the raw-count composition.\n\nRound your final answer to four significant figures. Express the final answer as a unitless real number.", "solution": "The problem is valid. It is scientifically grounded in standard bioinformatics procedures for quantifying functional abundance from sequencing data, is well-posed with all necessary information provided, and is stated objectively.\n\nThe task is to compute the Kullback-Leibler (KL) divergence from a Transcripts Per Million (TPM) normalized composition to a raw-count composition for a set of four KEGG Orthology (KO) features. Let the set of features be indexed by $i \\in \\{A, B, C, D\\}$.\n\nThe provided data are:\n- KO-A ($i=A$): raw count $C_A = 360$, length $L_A = 900$ bp.\n- KO-B ($i=B$): raw count $C_B = 540$, length $L_B = 1800$ bp.\n- KO-C ($i=C$): raw count $C_C = 120$, length $L_C = 300$ bp.\n- KO-D ($i=D$): raw count $C_D = 180$, length $L_D = 1500$ bp.\n\nFirst, we compute the raw-count composition, which we denote as the probability distribution $P = \\{p_i\\}$. This is found by dividing the count for each feature by the total number of aligned reads.\nThe total number of reads is:\n$$N = \\sum_{i \\in \\{A,B,C,D\\}} C_i = 360 + 540 + 120 + 180 = 1200$$\nThe proportions $p_i = \\frac{C_i}{N}$ are:\n$$p_A = \\frac{360}{1200} = \\frac{3}{10} = 0.30$$\n$$p_B = \\frac{540}{1200} = \\frac{9}{20} = 0.45$$\n$$p_C = \\frac{120}{1200} = \\frac{1}{10} = 0.10$$\n$$p_D = \\frac{180}{1200} = \\frac{3}{20} = 0.15$$\nThe set $P = \\{0.30, 0.45, 0.10, 0.15\\}$ forms the raw-count compositional distribution.\n\nNext, we compute the TPM composition, denoted as the probability distribution $Q = \\{q_i\\}$. The TPM calculation involves two steps: length normalization and library size scaling.\nFirst, we normalize the raw counts $C_i$ by the feature length in kilobases, $L_{i,kb} = L_i / 1000$. This gives a rate, $R_i$.\n$$L_{A,kb} = \\frac{900}{1000} = 0.9 \\, \\text{kb}$$\n$$L_{B,kb} = \\frac{1800}{1000} = 1.8 \\, \\text{kb}$$\n$$L_{C,kb} = \\frac{300}{1000} = 0.3 \\, \\text{kb}$$\n$$L_{D,kb} = \\frac{1500}{1000} = 1.5 \\, \\text{kb}$$\nThe rates $R_i = \\frac{C_i}{L_{i,kb}}$ are:\n$$R_A = \\frac{360}{0.9} = 400$$\n$$R_B = \\frac{540}{1.8} = 300$$\n$$R_C = \\frac{120}{0.3} = 400$$\n$$R_D = \\frac{180}{1.5} = 120$$\n\nSecond, these rates are scaled to sum to a constant (typically $10^6$ for TPM). The TPM composition is the relative proportion of each feature after this length normalization. The scaling constant of $10^6$ is irrelevant for the composition itself, as it cancels out. The compositional proportions $q_i$ are found by dividing each rate $R_i$ by the sum of all rates.\nThe sum of rates is:\n$$S = \\sum_{i \\in \\{A,B,C,D\\}} R_i = 400 + 300 + 400 + 120 = 1220$$\nThe proportions $q_i = \\frac{R_i}{S}$ are:\n$$q_A = \\frac{400}{1220} = \\frac{20}{61}$$\n$$q_B = \\frac{300}{1220} = \\frac{15}{61}$$\n$$q_C = \\frac{400}{1220} = \\frac{20}{61}$$\n$$q_D = \\frac{120}{1220} = \\frac{6}{61}$$\nThe set $Q = \\{\\frac{20}{61}, \\frac{15}{61}, \\frac{20}{61}, \\frac{6}{61}\\}$ forms the TPM compositional distribution.\n\nFinally, we compute the Kullback-Leibler divergence from the TPM composition ($Q$) to the raw-count composition ($P$), denoted $D_{KL}(Q || P)$, using the natural logarithm as specified.\nThe formula for KL divergence is:\n$$D_{KL}(Q || P) = \\sum_{i \\in \\{A,B,C,D\\}} q_i \\ln\\left(\\frac{q_i}{p_i}\\right)$$\nSubstituting the calculated values for $p_i$ and $q_i$:\n$$D_{KL}(Q || P) = \\frac{20}{61} \\ln\\left(\\frac{20/61}{0.30}\\right) + \\frac{15}{61} \\ln\\left(\\frac{15/61}{0.45}\\right) + \\frac{20}{61} \\ln\\left(\\frac{20/61}{0.10}\\right) + \\frac{6}{61} \\ln\\left(\\frac{6/61}{0.15}\\right)$$\n$$D_{KL}(Q || P) = \\frac{20}{61} \\ln\\left(\\frac{20/61}{3/10}\\right) + \\frac{15}{61} \\ln\\left(\\frac{15/61}{9/20}\\right) + \\frac{20}{61} \\ln\\left(\\frac{20/61}{1/10}\\right) + \\frac{6}{61} \\ln\\left(\\frac{6/61}{3/20}\\right)$$\n$$D_{KL}(Q || P) = \\frac{20}{61} \\ln\\left(\\frac{200}{183}\\right) + \\frac{15}{61} \\ln\\left(\\frac{300}{549}\\right) + \\frac{20}{61} \\ln\\left(\\frac{200}{61}\\right) + \\frac{6}{61} \\ln\\left(\\frac{120}{183}\\right)$$\nSimplifying the fractions inside the logarithms:\n$$D_{KL}(Q || P) = \\frac{20}{61} \\ln\\left(\\frac{200}{183}\\right) + \\frac{15}{61} \\ln\\left(\\frac{100}{183}\\right) + \\frac{20}{61} \\ln\\left(\\frac{200}{61}\\right) + \\frac{6}{61} \\ln\\left(\\frac{40}{61}\\right)$$\nNow we compute the numerical value:\n$$D_{KL}(Q || P) \\approx \\left(\\frac{20}{61}\\right)(0.0888358) + \\left(\\frac{15}{61}\\right)(-0.6043254) + \\left(\\frac{20}{61}\\right)(1.1874253) + \\left(\\frac{6}{61}\\right)(-0.4220261)$$\n$$D_{KL}(Q || P) \\approx (0.32786885)(0.0888358) + (0.24590164)(-0.6043254) + (0.32786885)(1.1874253) + (0.09836066)(-0.4220261)$$\n$$D_{KL}(Q || P) \\approx 0.02912977 - 0.14860721 + 0.38927907 - 0.04151121$$\n$$D_{KL}(Q || P) \\approx 0.22829042$$\nRounding the result to four significant figures gives $0.2283$. This value quantifies the information gain when revising beliefs from the raw-count proportions to the length-corrected proportions.", "answer": "$$\\boxed{0.2283}$$", "id": "4565555"}, {"introduction": "Beyond quantifying individual functions, a primary goal of metabolic profiling is to assess the potential of entire pathways. This requires a framework for integrating the abundances of individual gene families into a coherent pathway-level score. This exercise [@problem_id:4565625] introduces a powerful, rule-based model inspired by leading bioinformatic tools, where you will use logical AND/OR operations to calculate pathway coverage and abundance. You will learn to model biological concepts like metabolic bottlenecks and alternative enzymatic steps, providing a deeper insight into the systems-level functional capacity of a microbiome.", "problem": "You are given a simplified yet scientifically grounded modeling task inspired by the HMP Unified Metabolic Analysis Network (HUMAnN). The aim is to formalize the mapping from read-derived UniRef90 protein family abundances to metabolic reactions and to compute module-based pathway coverage and pathway abundance. The scenario is motivated by the Central Dogma of Molecular Biology, where genes encode protein families (enzymes) that catalyze reactions, which aggregate into modules and pathways. The goal is to derive, from first principles, a precise computational procedure for pathway coverage and abundance scoring under module-based logic.\n\nDefine a finite set of UniRef90 families $F = \\{f_1, f_2, \\dots, f_n\\}$ with non-negative abundance values $a_{f} \\in \\mathbb{R}_{\\ge 0}$ interpreted as normalized read abundance values in arbitrary units of sequencing-derived abundance. Define a finite set of reactions $R = \\{r_1, r_2, \\dots, r_m\\}$. Each reaction $r \\in R$ is associated with a non-empty set $M_r \\subseteq F$ indicating which UniRef90 families map to $r$. The abundance of a reaction $r$ is given by\n$$\nA_r = \\sum_{f \\in M_r} a_f \\, .\n$$\nGiven a non-negative threshold $t \\in \\mathbb{R}_{\\ge 0}$, define the reaction presence indicator\n$$\np_r = \\begin{cases}\n1 & \\text{if } A_r \\ge t \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n$$\n\nA module is a logical expression over reactions constructed using two connectives: logical AND and logical OR. A module is either a leaf reaction $r \\in R$, or an internal node $X(\\cdot, \\cdot)$ where $X \\in \\{\\text{AND}, \\text{OR}\\}$ and its two children are modules. A pathway $P$ is an ordered list of modules $\\{m_1, m_2, \\dots, m_k\\}$.\n\nDefine two recursive functions to evaluate module coverage and module abundance:\n1. For module coverage, define $c(m)$ for a module $m$ as\n$$\nc(m) = \\begin{cases}\n1 & \\text{if } m \\text{ is a leaf reaction } r \\text{ and } p_r = 1 \\\\\n0 & \\text{if } m \\text{ is a leaf reaction } r \\text{ and } p_r = 0 \\\\\n\\min\\left(c(m_{\\text{left}}), c(m_{\\text{right}})\\right) & \\text{if } m = \\text{AND}(m_{\\text{left}}, m_{\\text{right}}) \\\\\n\\max\\left(c(m_{\\text{left}}), c(m_{\\text{right}})\\right) & \\text{if } m = \\text{OR}(m_{\\text{left}}, m_{\\text{right}})\n\\end{cases}\n$$\nwhich generalizes coverage to a continuous score in $[0,1]$ at the module level: a leaf contributes $0$ or $1$ depending on presence, AND propagates the limiting coverage by the minimum, and OR propagates the best-available coverage by the maximum.\n\n2. For module abundance, define $b(m)$ for a module $m$ as\n$$\nb(m) = \\begin{cases}\nA_r & \\text{if } m \\text{ is a leaf reaction } r \\\\\n\\min\\left(b(m_{\\text{left}}), b(m_{\\text{right}})\\right) & \\text{if } m = \\text{AND}(m_{\\text{left}}, m_{\\text{right}}) \\\\\n\\max\\left(b(m_{\\text{left}}), b(m_{\\text{right}})\\right) & \\text{if } m = \\text{OR}(m_{\\text{left}}, m_{\\text{right}})\n\\end{cases}\n$$\nwhich treats the AND operator as a bottleneck (limiting step) and the OR operator as the maximal branch. Leaf nodes contribute their reaction abundance $A_r$.\n\nDefine pathway coverage and pathway abundance by aggregating across the pathway’s modules using the bottleneck principle:\n$$\nC_P = \\min_{i \\in \\{1,\\dots,k\\}} c(m_i), \\qquad B_P = \\min_{i \\in \\{1,\\dots,k\\}} b(m_i).\n$$\nBoth $C_P$ and $B_P$ are real-valued. The pathway coverage $C_P$ is a decimal in $[0,1]$ (not a percentage), and the pathway abundance $B_P$ is in the same arbitrary unit as $A_r$.\n\nFor this problem, use the following concrete mapping and pathways:\n- Families $F = \\{\\text{U1}, \\text{U2}, \\text{U3}, \\text{U4}, \\text{U5}\\}$.\n- Reactions $R = \\{\\text{R1}, \\text{R2}, \\text{R3}, \\text{R4}, \\text{R5}, \\text{R6}\\}$ with mappings\n$$\nM_{\\text{R1}} = \\{\\text{U1}, \\text{U2}\\}, \\quad\nM_{\\text{R2}} = \\{\\text{U2}\\}, \\quad\nM_{\\text{R3}} = \\{\\text{U3}\\}, \\quad\nM_{\\text{R4}} = \\{\\text{U3}, \\text{U4}\\}, \\quad\nM_{\\text{R5}} = \\{\\text{U5}\\}, \\quad\nM_{\\text{R6}} = \\{\\text{U1}\\}.\n$$\n- Pathways:\n  - Pathway $\\text{P\\_A}$ has two modules:\n    $$\n    m_1 = \\text{AND}\\left(\\text{R1}, \\, \\text{OR}(\\text{R2}, \\text{R3})\\right), \\qquad\n    m_2 = \\text{AND}\\left(\\text{R4}, \\text{R5}\\right).\n    $$\n  - Pathway $\\text{P\\_B}$ has one module:\n    $$\n    m_3 = \\text{OR}\\left(\\text{AND}(\\text{R1}, \\text{R6}), \\, \\text{R3}\\right).\n    $$\n\nTest suite. For each test case, you are given a threshold $t$ and abundances $a_f$ for $f \\in F$. Compute $A_r$ for each reaction, then $p_r$, then $c(m)$ and $b(m)$ for each module, and finally $C_P$ and $B_P$ for $\\text{P\\_A}$ and $\\text{P\\_B}$. The test suite contains four cases:\n- Case $1$: threshold $t = 0.5$ and family abundances\n$a_{\\text{U1}} = 0.8,\\; a_{\\text{U2}} = 0.4,\\; a_{\\text{U3}} = 0.9,\\; a_{\\text{U4}} = 0.3,\\; a_{\\text{U5}} = 0.5.$\n- Case $2$: threshold $t = 0.7$ and family abundances\n$a_{\\text{U1}} = 0.7,\\; a_{\\text{U2}} = 0.0,\\; a_{\\text{U3}} = 0.7,\\; a_{\\text{U4}} = 0.0,\\; a_{\\text{U5}} = 0.7.$\n- Case $3$: threshold $t = 0.0$ and family abundances\n$a_{\\text{U1}} = 0.0,\\; a_{\\text{U2}} = 0.0,\\; a_{\\text{U3}} = 0.0,\\; a_{\\text{U4}} = 0.0,\\; a_{\\text{U5}} = 0.0.$\n- Case $4$: threshold $t = 0.6$ and family abundances\n$a_{\\text{U1}} = 0.4,\\; a_{\\text{U2}} = 0.3,\\; a_{\\text{U3}} = 0.55,\\; a_{\\text{U4}} = 0.1,\\; a_{\\text{U5}} = 0.0.$\n\nYour program must implement these definitions exactly and produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a list of two lists: the first inner list contains $[C_{\\text{P\\_A}}, B_{\\text{P\\_A}}]$, and the second inner list contains $[C_{\\text{P\\_B}}, B_{\\text{P\\_B}}]$. All values must be real numbers. Thus the final output format is\n$$\n\\left[ \\left[ [C_{\\text{P\\_A}}^{(1)}, B_{\\text{P\\_A}}^{(1)}], [C_{\\text{P\\_B}}^{(1)}, B_{\\text{P\\_B}}^{(1)}] \\right], \\dots, \\left[ [C_{\\text{P\\_A}}^{(4)}, B_{\\text{P\\_A}}^{(4)}], [C_{\\text{P\\_B}}^{(4)}, B_{\\text{P\\_B}}^{(4)}] \\right] \\right]\n$$\nwhere the superscript denotes the test case index from $1$ to $4$. No other text should be printed.", "solution": "The problem statement provides a formal, quantitative model for deriving pathway-level functional profiles from gene family abundances, a concept central to microbiome metabolic reconstruction. The model is scientifically grounded, mathematically well-posed, and all its components are explicitly defined. The problem is valid.\n\nThe solution proceeds by first implementing the defined mathematical rules and then applying them to the provided test cases. The procedure is structured into four main computational steps.\n\n1.  **Calculation of Reaction Abundance ($A_r$)**: The abundance of each reaction $r$ is the sum of the abundances of all UniRef90 protein families $f$ that catalyze it. This is based on the given mapping $M_r \\subseteq F$, where $F$ is the set of families. The formula is:\n    $$\n    A_r = \\sum_{f \\in M_r} a_f\n    $$\n    where $a_f$ is the abundance of family $f$.\n\n2.  **Determination of Reaction Presence ($p_r$)**: A reaction $r$ is considered 'present' or 'active' if its computed abundance $A_r$ meets or exceeds a specified threshold $t \\in \\mathbb{R}_{\\ge 0}$. This is captured by a binary indicator variable $p_r$:\n    $$\n    p_r = \\begin{cases}\n    1 & \\text{if } A_r \\ge t \\\\\n    0 & \\text{if } A_r < t\n    \\end{cases}\n    $$\n    The value of $p_r$ serves as the basis for module coverage calculations.\n\n3.  **Recursive Evaluation of Module Metrics ($c(m)$ and $b(m)$)**: A module $m$ is a logical composition of reactions. Its coverage $c(m)$ and abundance $b(m)$ are computed recursively.\n    -   For a leaf module, which is a single reaction $r$, the coverage is its presence indicator $p_r$, and its abundance is $A_r$.\n        $$\n        c(r) = p_r, \\qquad b(r) = A_r\n        $$\n    -   For a composite module, the evaluation depends on the logical connective. The $\\text{AND}$ operator represents a bottleneck, requiring all sub-modules to be present, thus propagating the minimum coverage and abundance. The $\\text{OR}$ operator represents alternative pathways, thus propagating the maximum coverage and abundance.\n        $$\n        c(\\text{AND}(m_{\\text{left}}, m_{\\text{right}})) = \\min\\left(c(m_{\\text{left}}), c(m_{\\text{right}})\\right)\n        $$\n        $$\n        b(\\text{AND}(m_{\\text{left}}, m_{\\text{right}})) = \\min\\left(b(m_{\\text{left}}), b(m_{\\text{right}})\\right)\n        $$\n        $$\n        c(\\text{OR}(m_{\\text{left}}, m_{\\text{right}})) = \\max\\left(c(m_{\\text{left}}), c(m_{\\text{right}})\\right)\n        $$\n        $$\n        b(\\text{OR}(m_{\\text{left}}, m_{\\text{right}})) = \\max\\left(b(m_{\\text{left}}), b(m_{\\text{right}})\\right)\n        $$\n\n4.  **Aggregation for Pathway Scores ($C_P$ and $B_P$)**: A pathway $P = \\{m_1, m_2, \\dots, m_k\\}$ is considered a sequence of necessary modules. Applying the bottleneck principle at the pathway level, the overall pathway coverage $C_P$ and abundance $B_P$ are determined by the minimum scores across all its constituent modules.\n    $$\n    C_P = \\min_{i \\in \\{1,\\dots,k\\}} c(m_i), \\qquad B_P = \\min_{i \\in \\{1,\\dots,k\\}} b(m_i)\n    $$\n\nTo illustrate, let us perform the calculation for **Case 1**:\n-   **Givens**: Threshold $t = 0.5$. Family abundances: $a_{\\text{U1}} = 0.8$, $a_{\\text{U2}} = 0.4$, $a_{\\text{U3}} = 0.9$, $a_{\\text{U4}} = 0.3$, $a_{\\text{U5}} = 0.5$.\n-   **Step 1 & 2: Reaction Metrics**:\n    -   $A_{\\text{R1}} = a_{\\text{U1}} + a_{\\text{U2}} = 0.8 + 0.4 = 1.2 \\implies p_{\\text{R1}} = 1$\n    -   $A_{\\text{R2}} = a_{\\text{U2}} = 0.4 \\implies p_{\\text{R2}} = 0$\n    -   $A_{\\text{R3}} = a_{\\text{U3}} = 0.9 \\implies p_{\\text{R3}} = 1$\n    -   $A_{\\text{R4}} = a_{\\text{U3}} + a_{\\text{U4}} = 0.9 + 0.3 = 1.2 \\implies p_{\\text{R4}} = 1$\n    -   $A_{\\text{R5}} = a_{\\text{U5}} = 0.5 \\implies p_{\\text{R5}} = 1$\n    -   $A_{\\text{R6}} = a_{\\text{U1}} = 0.8 \\implies p_{\\text{R6}} = 1$\n-   **Step 3 & 4: Pathway $\\text{P\\_A}$**: Consists of modules $m_1 = \\text{AND}(\\text{R1}, \\text{OR}(\\text{R2}, \\text{R3}))$ and $m_2 = \\text{AND}(\\text{R4}, \\text{R5})$.\n    -   $c(m_1) = \\min(p_{\\text{R1}}, \\max(p_{\\text{R2}}, p_{\\text{R3}})) = \\min(1, \\max(0, 1)) = 1$\n    -   $b(m_1) = \\min(A_{\\text{R1}}, \\max(A_{\\text{R2}}, A_{\\text{R3}})) = \\min(1.2, \\max(0.4, 0.9)) = 0.9$\n    -   $c(m_2) = \\min(p_{\\text{R4}}, p_{\\text{R5}}) = \\min(1, 1) = 1$\n    -   $b(m_2) = \\min(A_{\\text{R4}}, A_{\\text{R5}}) = \\min(1.2, 0.5) = 0.5$\n    -   $C_{\\text{P\\_A}} = \\min(c(m_1), c(m_2)) = \\min(1, 1) = 1.0$\n    -   $B_{\\text{P\\_A}} = \\min(b(m_1), b(m_2)) = \\min(0.9, 0.5) = 0.5$\n-   **Step 3 & 4: Pathway $\\text{P\\_B}$**: Consists of module $m_3 = \\text{OR}(\\text{AND}(\\text{R1}, \\text{R6}), \\text{R3})$.\n    -   $c(m_3) = \\max(\\min(p_{\\text{R1}}, p_{\\text{R6}}), p_{\\text{R3}}) = \\max(\\min(1, 1), 1) = 1$\n    -   $b(m_3) = \\max(\\min(A_{\\text{R1}}, A_{\\text{R6}}), A_{\\text{R3}}) = \\max(\\min(1.2, 0.8), 0.9) = \\max(0.8, 0.9) = 0.9$\n    -   $C_{\\text{P\\_B}} = c(m_3) = 1.0$\n    -   $B_{\\text{P\\_B}} = b(m_3) = 0.9$\n-   **Result for Case 1**: For $\\text{P\\_A}$, $[C_{\\text{P\\_A}}, B_{\\text{P\\_A}}] = [1.0, 0.5]$. For $\\text{P\\_B}$, $[C_{\\text{P\\_B}}, B_{\\text{P\\_B}}] = [1.0, 0.9]$.\n\nThe final program implements this exact logic for all four test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the specified model for pathway coverage and abundance calculation.\n    \"\"\"\n    \n    # Static definitions from the problem statement\n    family_names = ['U1', 'U2', 'U3', 'U4', 'U5']\n    reaction_names = ['R1', 'R2', 'R3', 'R4', 'R5', 'R6']\n    \n    reaction_mappings = {\n        'R1': ['U1', 'U2'],\n        'R2': ['U2'],\n        'R3': ['U3'],\n        'R4': ['U3', 'U4'],\n        'R5': ['U5'],\n        'R6': ['U1']\n    }\n    \n    # Module structures are represented as nested tuples: (operator, child1, child2)\n    # Leaf nodes are reaction name strings.\n    pa_modules = [\n        ('AND', 'R1', ('OR', 'R2', 'R3')),  # m1\n        ('AND', 'R4', 'R5')                 # m2\n    ]\n    pb_modules = [\n        ('OR', ('AND', 'R1', 'R6'), 'R3')   # m3\n    ]\n    \n    test_cases = [\n        {'t': 0.5, 'a': {'U1': 0.8, 'U2': 0.4, 'U3': 0.9, 'U4': 0.3, 'U5': 0.5}},\n        {'t': 0.7, 'a': {'U1': 0.7, 'U2': 0.0, 'U3': 0.7, 'U4': 0.0, 'U5': 0.7}},\n        {'t': 0.0, 'a': {'U1': 0.0, 'U2': 0.0, 'U3': 0.0, 'U4': 0.0, 'U5': 0.0}},\n        {'t': 0.6, 'a': {'U1': 0.4, 'U2': 0.3, 'U3': 0.55, 'U4': 0.1, 'U5': 0.0}},\n    ]\n    \n    all_results = []\n    \n    memo_module_eval = {}\n\n    def evaluate_module(module_struct, Ar_vals, pr_vals):\n        \"\"\"Recursively computes coverage and abundance for a module.\"\"\"\n        # Check memoization table\n        if (module_struct, tuple(sorted(Ar_vals.items())), tuple(sorted(pr_vals.items()))) in memo_module_eval:\n           return memo_module_eval[(module_struct, tuple(sorted(Ar_vals.items())), tuple(sorted(pr_vals.items())))]\n        \n        # Base case: leaf node is a reaction\n        if isinstance(module_struct, str):\n            reaction_name = module_struct\n            coverage = float(pr_vals[reaction_name])\n            abundance = Ar_vals[reaction_name]\n            return coverage, abundance\n        \n        # Recursive step for internal nodes\n        op, left_child, right_child = module_struct\n        \n        c_left, b_left = evaluate_module(left_child, Ar_vals, pr_vals)\n        c_right, b_right = evaluate_module(right_child, Ar_vals, pr_vals)\n        \n        if op == 'AND':\n            c_m = min(c_left, c_right)\n            b_m = min(b_left, b_right)\n        elif op == 'OR':\n            c_m = max(c_left, c_right)\n            b_m = max(b_left, b_right)\n        else:\n            raise ValueError(f\"Unknown operator: {op}\")\n        \n        # Store result in memoization table before returning\n        memo_module_eval[(module_struct, tuple(sorted(Ar_vals.items())), tuple(sorted(pr_vals.items())))] = (c_m, b_m)\n        return c_m, b_m\n\n    for case in test_cases:\n        t = case['t']\n        abundances = case['a']\n        \n        # Step 1: Calculate Reaction Abundances (Ar)\n        Ar = {}\n        for r_name, fam_list in reaction_mappings.items():\n            Ar[r_name] = sum(abundances[f_name] for f_name in fam_list)\n        \n        # Step 2: Calculate Reaction Presences (pr)\n        pr = {r_name: 1 if Ar[r_name] >= t else 0 for r_name in reaction_names}\n        \n        # Step 3 & 4: Calculate Pathway Scores\n        \n        # Pathway P_A\n        pa_module_scores = [evaluate_module(m, Ar, pr) for m in pa_modules]\n        C_PA = min(score[0] for score in pa_module_scores)\n        B_PA = min(score[1] for score in pa_module_scores)\n        \n        # Pathway P_B\n        pb_module_scores = [evaluate_module(m, Ar, pr) for m in pb_modules]\n        C_PB = min(score[0] for score in pb_module_scores)\n        B_PB = min(score[1] for score in pb_module_scores)\n        \n        case_result = [\n            [float(C_PA), float(B_PA)],\n            [float(C_PB), float(B_PB)]\n        ]\n        all_results.append(case_result)\n        \n    # Final print statement in the exact required format.\n    # The default str() representation of a list of lists matches the required format.\n    print(all_results)\n\nsolve()\n\n```", "id": "4565625"}, {"introduction": "The ultimate goal of many microbiome studies is to identify functional differences between conditions, but selecting the right statistical tool can be daunting. Different methods are built on different assumptions about the nature of sequencing data, which can significantly impact results. In this advanced practice [@problem_id:4565558], you will implement the core logic behind three widely-used differential abundance methods, comparing how they handle compositionality, normalization, and variance. This comparative analysis will equip you with the critical knowledge to interpret results from various bioinformatics pipelines and understand the sources of discrepancy between them.", "problem": "You are given three small, synthetic functional count datasets from two groups of microbiome samples. Your task is to implement three differential abundance frameworks that embody the core assumptions of three widely used methods—Analysis of Composition of Microbiomes with Bias Correction (ANCOM-BC), ANalysis Of VAriance-Like Differential Expression version 2 (ALDEx2), and Differential gene expression analysis based on the Negative Binomial distribution version 2 (DESeq2)—and to reconcile differences among the significant features. You must build from the following foundational bases: the Central Dogma of Molecular Biology (DNA to RNA to protein), the compositional nature of sequencing-based abundances (counts constrained by library size), the centered log-ratio transformation for compositional data, linear regression with offsets for log-abundance modeling, and the negative binomial mean-variance relationship for count data. Your program must be fully deterministic and must not use any stochastic sampling.\n\nUse the following definitions and assumptions.\n\n- Let $x_{ij}$ denote the count of feature $i$ in sample $j$. Let $n_j = \\sum_i x_{ij}$ denote the library size of sample $j$. Let $g_j$ denote the geometric mean of the pseudo-counted feature counts in sample $j$, $g_j = \\exp\\left(\\frac{1}{I}\\sum_{i=1}^I \\log(x_{ij} + \\tau)\\right)$, where $\\tau$ is a strictly positive pseudo-count.\n- For the ALDEx2-inspired analysis, compute the centered log-ratio (CLR) values $z_{ij} = \\log(x_{ij} + \\tau) - \\frac{1}{I}\\sum_{k=1}^I \\log(x_{kj} + \\tau) = \\log\\left(\\frac{x_{ij} + \\tau}{g_j}\\right)$. For each feature $i$, test the null hypothesis of no difference between the two groups using a two-sample Welch’s $t$-test on $\\{z_{ij}\\}$ split by group. If both groups have zero within-group variance, set the $p$-value to $0$ if the group means differ and to $1$ if they do not.\n- For the ANCOM-BC-inspired analysis, use a log-linear model with an offset for library size. Define $y_{ij} = \\log(x_{ij} + \\tau) - \\log(n_j)$. Fit the linear model $y_{ij} = \\beta_{0i} + \\beta_{1i} \\cdot G_j + \\varepsilon_{ij}$ by ordinary least squares, where $G_j \\in \\{0, 1\\}$ is the group indicator. Test $H_0: \\beta_{1i} = 0$ using the $t$-statistic with degrees of freedom $N - 2$. If the residual variance estimate is exactly zero, set the $p$-value to $0$ if $\\beta_{1i} \\neq 0$ and to $1$ otherwise.\n- For the DESeq2-inspired analysis, compute size factors $s_j$ using the median-of-ratios method. Let $G_i$ be the geometric mean of counts across all samples, $G_i = \\exp\\left(\\frac{1}{N}\\sum_{j=1}^N \\log(x_{ij})\\right)$, assuming all $x_{ij} > 0$. For each sample $j$, compute ratios $r_{ij} = x_{ij} / G_i$, and set $s_j$ to the median of $\\{r_{ij}\\}_i$. Normalize counts by $\\tilde{x}_{ij} = x_{ij}/s_j$. Let $\\mu_{iA}$ and $\\mu_{iB}$ be the group means of $\\tilde{x}_{ij}$ for groups $A$ and $B$, respectively, with $n_A$ and $n_B$ samples in each group. Assume the negative binomial mean-variance relationship $\\operatorname{Var}(\\tilde{x}_{ij}) \\approx \\mu_i + \\alpha \\mu_i^2$ with a global dispersion $\\alpha \\ge 0$ estimated by the median across features of $\\hat{\\alpha}_i = \\max\\left(\\frac{\\hat{v}_i - \\hat{\\mu}_i}{\\hat{\\mu}_i^2}, 0\\right)$, where $\\hat{\\mu}_i$ and $\\hat{v}_i$ are the mean and variance of $\\{\\tilde{x}_{ij}\\}_{j=1}^N$. Using the delta method, approximate\n$$\n\\operatorname{Var}(\\log \\mu_{iA}) \\approx \\frac{1}{n_A \\mu_{iA}} + \\frac{\\alpha}{n_A}, \\quad\n\\operatorname{Var}(\\log \\mu_{iB}) \\approx \\frac{1}{n_B \\mu_{iB}} + \\frac{\\alpha}{n_B}.\n$$\nForm the log fold-change $L_i = \\log(\\mu_{iB}/\\mu_{iA})$ with variance $V_i = \\operatorname{Var}(\\log \\mu_{iA}) + \\operatorname{Var}(\\log \\mu_{iB})$ and Wald statistic $Z_i = L_i / \\sqrt{V_i}$. Compute two-sided $p$-values under the standard normal distribution.\n\nFor each method, adjust the per-feature $p$-values to control the false discovery rate at level $q = 0.1$ using the Benjamini–Hochberg procedure. A feature is called significant if its adjusted $q$-value is less than or equal to $q$.\n\nAfter computing significant sets for the three methods, define the reconciliation set as features that are significant in at least two of the three methods. Also compute the Jaccard indices between each pair of significant sets. The Jaccard index $J(S, T)$ is $|S \\cap T| / |S \\cup T|$, with the convention that if $|S \\cup T| = 0$ then $J(S, T) = 0$.\n\nYour program must implement the above pipeline deterministically with $\\tau = 0.5$ and $q = 0.1$, and must run on the three datasets below.\n\nTest Suite (three cases, each with $I = 5$ features and $N = 6$ samples; group labels are $G = [0, 0, 0, 1, 1, 1]$ for all cases):\n\n- Case 1 (differential features with varying library sizes):\n  - Group $A$ library sizes: $[10000, 12000, 9000]$, group $B$: $[15000, 11000, 13000]$.\n  - Group $A$ proportions: $[0.2, 0.3, 0.25, 0.05, 0.2]$.\n  - Group $B$ proportions: $[0.4, 0.15, 0.125, 0.025, 0.3]$.\n  - Counts matrix $X^{(1)}$ (features as rows, samples as columns):\n    - Feature $1$: $[2000, 2400, 1800, 6000, 4400, 5200]$.\n    - Feature $2$: $[3000, 3600, 2700, 2250, 1650, 1950]$.\n    - Feature $3$: $[2500, 3000, 2250, 1875, 1375, 1625]$.\n    - Feature $4$: $[500, 600, 450, 375, 275, 325]$.\n    - Feature $5$: $[2000, 2400, 1800, 4500, 3300, 3900]$.\n- Case 2 (no true differential features; strong library size variation):\n  - Proportions for both groups: $[0.05, 0.1, 0.15, 0.2, 0.5]$.\n  - Group $A$ library sizes: $[5000, 10000, 20000]$, group $B$: $[40000, 8000, 1000]$.\n  - Counts matrix $X^{(2)}$:\n    - Feature $1$: $[250, 500, 1000, 2000, 400, 50]$.\n    - Feature $2$: $[500, 1000, 2000, 4000, 800, 100]$.\n    - Feature $3$: $[750, 1500, 3000, 6000, 1200, 150]$.\n    - Feature $4$: $[1000, 2000, 4000, 8000, 1600, 200]$.\n    - Feature $5$: $[2500, 5000, 10000, 20000, 4000, 500]$.\n- Case 3 (one feature increases compositionally, others decrease):\n  - Group $A$ proportions: $[0.25, 0.25, 0.2, 0.2, 0.1]$ with library sizes $[10000, 10000, 10000]$.\n  - Group $B$ proportions: $[0.1388889, 0.1388889, 0.1111111, 0.1111111, 0.5]$ with library sizes $[10000, 10000, 10000]$.\n  - Counts matrix $X^{(3)}$:\n    - Feature $1$: $[2500, 2500, 2500, 1389, 1389, 1389]$.\n    - Feature $2$: $[2500, 2500, 2500, 1389, 1389, 1389]$.\n    - Feature $3$: $[2000, 2000, 2000, 1111, 1111, 1111]$.\n    - Feature $4$: $[2000, 2000, 2000, 1111, 1111, 1111]$.\n    - Feature $5$: $[1000, 1000, 1000, 5000, 5000, 5000]$.\n\nFor each case, produce the following outputs:\n\n- Let $S_{\\mathrm{AL}}$, $S_{\\mathrm{AN}}$, and $S_{\\mathrm{DE}}$ be the sets of significant features (indexed from $1$ to $5$) for the ALDEx2-inspired, ANCOM-BC-inspired, and DESeq2-inspired methods, respectively, under Benjamini–Hochberg adjustment at $q = 0.1$.\n- Let $S_{\\mathrm{C}}$ be the reconciliation set of features significant in at least two of the three methods.\n- Compute the Jaccard indices $J(S_{\\mathrm{AL}}, S_{\\mathrm{AN}})$, $J(S_{\\mathrm{AL}}, S_{\\mathrm{DE}})$, and $J(S_{\\mathrm{AN}}, S_{\\mathrm{DE}})$.\n\nYour program must output a single line that is a list of three lists (one for each case), where each inner list has the form\n[$|S_{\\mathrm{AL}}|$, $|S_{\\mathrm{AN}}|$, $|S_{\\mathrm{DE}}|$, $|S_{\\mathrm{C}}|$, $J(S_{\\mathrm{AL}}, S_{\\mathrm{AN}})$, $J(S_{\\mathrm{AL}}, S_{\\mathrm{DE}})$, $J(S_{\\mathrm{AN}}, S_{\\mathrm{DE}})$].\nThe three Jaccard indices must be rounded to exactly four decimal places. The final output must be printed exactly as a comma-separated list enclosed in square brackets, for example: $[[1,2,3,2,0.5000,0.3333,0.2500],[...],[...]]$.\n\nNo input should be read from standard input and no files should be written or read. All computations must be performed as specified above. The answer must have no physical units. Angles are not involved. Percentages, if any, must be expressed as decimals.", "solution": "The user has provided a well-defined computational biology problem. The task is to implement simplified versions of three common differential abundance analysis methods for microbiome data—ALDEx2, ANCOM-BC, and DESeq2—to identify features (e.g., genes or functions) that have different abundances between two groups of samples. The problem is validated as sound, well-posed, and objective. All necessary data, parameters, and deterministic formulas are provided, ensuring a unique solution can be derived.\n\nThe solution will be structured as a single Python script that executes the complete analysis pipeline for three provided test cases. The core logic will be organized into separate functions for each analytical method and for the Benjamini-Hochberg false discovery rate (FDR) correction, promoting modularity and clarity.\n\n### Data and Parameters\n-   Three count matrices, $X^{(1)}, X^{(2)}, X^{(3)}$, each with $I=5$ features and $N=6$ samples.\n-   Group labels for all cases are $G = [0, 0, 0, 1, 1, 1]$, denoting three samples in group A and three in group B.\n-   A pseudo-count $\\tau = 0.5$ is used for methods involving logarithms of counts.\n-   A false discovery rate (FDR) threshold of $q = 0.1$ is used to determine significance.\n\n### Methodological Implementation\n\n**1. ALDEx2-Inspired Framework:**\nThis method addresses the compositional nature of the data by using a centered log-ratio (CLR) transformation.\n-   For each sample $j$, counts $x_{ij}$ are first augmented with a pseudo-count: $x_{ij} + \\tau$.\n-   The CLR transformation is applied: $z_{ij} = \\log(x_{ij} + \\tau) - \\log(g_j)$, where $g_j$ is the geometric mean of the pseudo-counted features in sample $j$. This transformation centers the data for each sample and places it in a log-ratio space, making it robust to library size differences.\n-   For each feature $i$, a two-sample Welch's $t$-test is performed on the CLR-transformed values $\\{z_{ij}\\}$ to compare group A and group B. Welch's $t$-test does not assume equal variances between groups, which is appropriate for biological data.\n-   A special case is handled as specified: if the within-group variance is zero for both groups, the $p$-value is set to $0$ if the group means differ and $1$ if they are identical. This prevents numerical issues and provides a deterministic outcome for perfectly separated or identical groups.\n\n**2. ANCOM-BC-Inspired Framework:**\nThis method uses a log-linear model with an offset term to account for library size.\n-   The response variable for each feature $i$ in sample $j$ is defined as the log-transformed, pseudo-counted abundance, offset by the log of the sample's library size: $y_{ij} = \\log(x_{ij} + \\tau) - \\log(n_j)$. The term $\\log(n_j)$ acts as an offset, effectively modeling the counts as rates.\n-   A simple linear model, $y_{ij} = \\beta_{0i} + \\beta_{1i} \\cdot G_j + \\varepsilon_{ij}$, is fit using ordinary least squares (OLS) for each feature. The coefficient $\\beta_{1i}$ represents the mean difference in the log-rate between group 1 and group 0.\n-   The null hypothesis $H_0: \\beta_{1i} = 0$ is tested. The $p$-value is obtained from the $t$-statistic of the slope coefficient, using $N-2$ degrees of freedom. The `scipy.stats.linregress` function is suitable as it correctly implements this test and handles the specified corner case of zero residual variance, which occurs if within-group variances are zero.\n\n**3. DESeq2-Inspired Framework:**\nThis method is based on the negative binomial distribution, which is well-suited for modeling overdispersed count data.\n-   Normalization is performed using the median-of-ratios method to compute size factors $s_j$. This method is robust to the presence of highly differentially abundant features. Counts are normalized by these size factors: $\\tilde{x}_{ij} = x_{ij}/s_j$.\n-   The mean-variance relationship is modeled as $\\operatorname{Var}(\\tilde{x}_{ij}) \\approx \\mu_i + \\alpha \\mu_i^2$, where $\\mu_i$ is the mean of the normalized counts for feature $i$ and $\\alpha$ is a global dispersion parameter. $\\alpha$ is estimated as the median of per-feature dispersion estimates. This \"sharing\" of information across features provides more stable variance estimates, especially with small sample sizes.\n-   The log fold change between group B and group A, $L_i = \\log(\\mu_{iB}/\\mu_{iA})$, is calculated.\n-   The variance of the log fold change, $V_i$, is estimated using the delta method and the assumed mean-variance relationship.\n-   A Wald test is conducted by forming the statistic $Z_i = L_i / \\sqrt{V_i}$, which is compared to a standard normal distribution to obtain two-sided $p$-values.\n\n### Post-analysis and Output\n\nFor each of the three methods, the resulting per-feature $p$-values are adjusted for multiple testing using the Benjamini-Hochberg (BH) procedure to control the FDR at $q=0.1$. A feature is deemed significant if its adjusted $p$-value (q-value) is $\\le 0.1$.\n\nThe final steps involve comparing the sets of significant features ($S_{\\mathrm{AL}}, S_{\\mathrm{AN}}, S_{\\mathrm{DE}}$) from the three methods:\n-   The number of significant features for each method is counted.\n-   A reconciliation set, $S_{\\mathrm{C}}$, is created, containing features found significant by at least two of the three methods. Its size is reported.\n-   The pairwise agreement between the methods is quantified using the Jaccard index, $J(S, T) = |S \\cap T| / |S \\cup T|$.\n\nThe results for each test case are compiled into a list containing the sizes of the four feature sets and the three Jaccard indices, which are formatted to four decimal places. These lists are then combined into a final nested list for output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef bh_significant(p_values, q):\n    \"\"\"\n    Performs the Benjamini-Hochberg procedure and returns a set of significant indices.\n    \n    Args:\n        p_values (np.ndarray): Array of p-values.\n        q (float): The false discovery rate level.\n        \n    Returns:\n        set: A set of 1-based indices of features deemed significant.\n    \"\"\"\n    p_values = np.asarray(p_values)\n    I = len(p_values)\n    if I == 0:\n        return set()\n\n    # Get sorted indices of p-values\n    sorted_indices = np.argsort(p_values)\n    \n    # Calculate adjusted p-values on sorted data\n    sorted_pvals = p_values[sorted_indices]\n    ranks = np.arange(1, I + 1)\n    adj_pvals_sorted = sorted_pvals * I / ranks\n    \n    # Enforce monotonicity (cumulative minimum from the right)\n    adj_pvals_sorted_mono = np.minimum.accumulate(adj_pvals_sorted[::-1])[::-1]\n    \n    # Place adjusted p-values back in original order\n    adj_pvals = np.empty_like(p_values)\n    adj_pvals[sorted_indices] = adj_pvals_sorted_mono\n    \n    # Find significant features (1-based index)\n    significant_indices_1based = {i + 1 for i, p_adj in enumerate(adj_pvals) if p_adj <= q}\n    return significant_indices_1based\n\ndef run_aldex2(X, G, tau, q):\n    \"\"\"\n    Implements the ALDEx2-inspired method.\n    \"\"\"\n    I, N = X.shape\n    X_pseudo = X + tau\n    \n    # CLR transformation\n    log_X_pseudo = np.log(X_pseudo)\n    geom_means_samples = np.exp(np.mean(log_X_pseudo, axis=0))\n    Z = log_X_pseudo - np.log(geom_means_samples)\n    \n    p_values = []\n    for i in range(I):\n        z_i = Z[i, :]\n        z_A = z_i[G == 0]\n        z_B = z_i[G == 1]\n        \n        var_A = np.var(z_A, ddof=1) if len(z_A) > 1 else 0\n        var_B = np.var(z_B, ddof=1) if len(z_B) > 1 else 0\n        \n        if var_A == 0 and var_B == 0:\n            if np.mean(z_A) == np.mean(z_B):\n                p_values.append(1.0)\n            else:\n                p_values.append(0.0)\n        else:\n            _, p_val = stats.ttest_ind(z_A, z_B, equal_var=False, nan_policy='raise')\n            p_values.append(p_val)\n            \n    return bh_significant(np.array(p_values), q)\n\ndef run_ancombc(X, G, tau, q):\n    \"\"\"\n    Implements the ANCOM-BC-inspired method.\n    \"\"\"\n    I, N = X.shape\n    lib_sizes = np.sum(X, axis=0)\n    Y = np.log(X + tau) - np.log(lib_sizes.reshape(1, -1))\n    \n    p_values = []\n    for i in range(I):\n        y_i = Y[i, :]\n        res = stats.linregress(G, y_i)\n        p_values.append(res.pvalue)\n\n    return bh_significant(np.array(p_values), q)\n\ndef run_deseq2(X, G, q):\n    \"\"\"\n    Implements the DESeq2-inspired method.\n    \"\"\"\n    I, N = X.shape\n    nA = np.sum(G == 0)\n    nB = np.sum(G == 1)\n\n    log_X = np.log(X)\n    geom_means_features = np.exp(np.mean(log_X, axis=1))\n    \n    ratios = X / geom_means_features[:, np.newaxis]\n    size_factors = np.median(ratios, axis=0)\n    X_norm = X / size_factors\n\n    mu_A = np.mean(X_norm[:, G == 0], axis=1)\n    mu_B = np.mean(X_norm[:, G == 1], axis=1)\n    \n    mu_total = np.mean(X_norm, axis=1)\n    var_total = np.var(X_norm, axis=1, ddof=1)\n    \n    alpha_i_hat = np.zeros(I)\n    non_zero_mu = mu_total > 0\n    alpha_i_hat[non_zero_mu] = (var_total[non_zero_mu] - mu_total[non_zero_mu]) / (mu_total[non_zero_mu]**2)\n    alpha_i_hat = np.maximum(alpha_i_hat, 0)\n    alpha = np.median(alpha_i_hat)\n    \n    p_values = []\n    for i in range(I):\n        if mu_A[i] <= 0 or mu_B[i] <= 0:\n            p_values.append(1.0)\n            continue\n            \n        L_i = np.log(mu_B[i] / mu_A[i])\n        var_log_muA = (1 / (nA * mu_A[i])) + (alpha / nA)\n        var_log_muB = (1 / (nB * mu_B[i])) + (alpha / nB)\n        V_i = var_log_muA + var_log_muB\n        \n        if V_i <= 0:\n            p_values.append(0.0 if L_i != 0 else 1.0)\n            continue\n\n        Z_i = L_i / np.sqrt(V_i)\n        p_val = stats.norm.sf(np.abs(Z_i)) * 2\n        p_values.append(p_val)\n        \n    return bh_significant(np.array(p_values), q)\n\ndef jaccard_index(s1, s2):\n    \"\"\"\n    Computes Jaccard index between two sets.\n    \"\"\"\n    union_size = len(s1.union(s2))\n    if union_size == 0:\n        return 0.0\n    return len(s1.intersection(s2)) / union_size\n\ndef solve():\n    \"\"\"\n    Main function to run the full analysis pipeline.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([\n            [2000, 2400, 1800, 6000, 4400, 5200],\n            [3000, 3600, 2700, 2250, 1650, 1950],\n            [2500, 3000, 2250, 1875, 1375, 1625],\n            [500, 600, 450, 375, 275, 325],\n            [2000, 2400, 1800, 4500, 3300, 3900]\n        ]),\n        np.array([\n            [250, 500, 1000, 2000, 400, 50],\n            [500, 1000, 2000, 4000, 800, 100],\n            [750, 1500, 3000, 6000, 1200, 150],\n            [1000, 2000, 4000, 8000, 1600, 200],\n            [2500, 5000, 10000, 20000, 4000, 500]\n        ]),\n        np.array([\n            [2500, 2500, 2500, 1389, 1389, 1389],\n            [2500, 2500, 2500, 1389, 1389, 1389],\n            [2000, 2000, 2000, 1111, 1111, 1111],\n            [2000, 2000, 2000, 1111, 1111, 1111],\n            [1000, 1000, 1000, 5000, 5000, 5000]\n        ])\n    ]\n\n    G = np.array([0, 0, 0, 1, 1, 1])\n    TAU = 0.5\n    Q_FDR = 0.1\n\n    results = []\n    for X in test_cases:\n        S_AL = run_aldex2(X, G, TAU, Q_FDR)\n        S_AN = run_ancombc(X, G, TAU, Q_FDR)\n        S_DE = run_deseq2(X, G, Q_FDR)\n\n        S_C = (S_AL & S_AN) | (S_AL & S_DE) | (S_AN & S_DE)\n\n        j_al_an = jaccard_index(S_AL, S_AN)\n        j_al_de = jaccard_index(S_AL, S_DE)\n        j_an_de = jaccard_index(S_AN, S_DE)\n\n        case_result_parts = [\n            str(len(S_AL)), str(len(S_AN)), str(len(S_DE)), str(len(S_C)),\n            f\"{j_al_an:.4f}\", f\"{j_al_de:.4f}\", f\"{j_an_de:.4f}\"\n        ]\n        results.append(f\"[{','.join(case_result_parts)}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4565558"}]}