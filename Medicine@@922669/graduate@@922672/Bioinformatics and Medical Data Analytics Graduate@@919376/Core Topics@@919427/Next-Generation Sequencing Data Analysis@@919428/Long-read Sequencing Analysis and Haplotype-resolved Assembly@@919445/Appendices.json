{"hands_on_practices": [{"introduction": "The success of any genome assembly project begins with the quality of the input sequencing data. This practice focuses on a fundamental metric for long-read data, the read N50, which provides a more robust measure of read length than a simple average. By first calculating this value from a sample dataset, you will then explore from first principles how the read length distribution directly influences our ability to resolve complex genomic repeats, a key determinant of final assembly contiguity [@problem_id:4579363].", "problem": "In long-read sequencing, one summary statistic of the read length distribution is the read N50. You are given a sequencing dataset consisting of $14$ single-molecule reads with lengths (in kilobases): $25$, $20$, $18$, $15$, $12$, $12$, $10$, $9$, $8$, $7$, $6$, $5$, $4$, $3$. Using the formal definition that the read N50 is the length $N$ such that reads of length at least $N$ collectively contribute at least one half of the total sequenced bases, compute the read N50 for this dataset. Express the read N50 in $\\mathrm{kb}$. No rounding is necessary.\n\nThen, under an idealized Overlap-Layout-Consensus (OLC) assembler, explain from first principles how the read N50 influences the expected contig N50. In your explanation, you may assume the following widely used foundations without proof: read start positions along a genome of length $G$ are well approximated by a homogeneous Poisson process at coverage depth $c$, repeats of length $R$ require at least one read to span the repeat plus flanking unique sequence of total length $2u$ to be resolved, and contigs tend to break at unresolved repeats and at coverage gaps. Do not compute any numerical quantity for this part; instead, derive and state a symbolic expression that connects the probability that a repeat is resolved to the read length distribution, and use it to argue how increasing read N50 changes the expected contig N50.\n\nYour final reported answer must be the computed read N50 in $\\mathrm{kb}$ only. If any rounding were necessary, you would round to four significant figures, but for this dataset it is not required.", "solution": "The problem presents two distinct tasks. The first is a numerical calculation of the read N50 for a given dataset. The second is a theoretical explanation of the relationship between read N50 and contig N50 based on first principles of genome assembly.\n\nFirst, we address the calculation of the read N50. The formal definition states that the read N50 is the length $N$ such that sequencing reads of length at least $N$ collectively contribute at least one half of the total number of sequenced bases.\n\nThe provided dataset consists of $14$ reads with the following lengths in kilobases (kb): $25$, $20$, $18$, $15$, $12$, $12$, $10$, $9$, $8$, $7$, $6$, $5$, $4$, $3$.\n\nThe procedure to compute the read N50 is as follows:\n1.  Calculate the total number of sequenced bases by summing the lengths of all reads.\n2.  Determine the target value, which is one half of the total number of bases.\n3.  Sort the reads in descending order of their length.\n4.  Iterate through the sorted list, calculating the cumulative sum of read lengths.\n5.  The N50 is the length of the read at which the cumulative sum first meets or exceeds the target value from step 2.\n\nLet's execute these steps.\nStep 1: Calculate the total length of all reads.\nTotal Length $= 25 + 20 + 18 + 15 + 12 + 12 + 10 + 9 + 8 + 7 + 6 + 5 + 4 + 3 = 154$ kb.\n\nStep 2: Determine the halfway point of the total length.\nTarget Length $= \\frac{1}{2} \\times 154 = 77$ kb.\n\nStep 3: Sort the reads by length in descending order. The provided list is already sorted:\n$L_1 = 25$, $L_2 = 20$, $L_3 = 18$, $L_4 = 15$, $L_5 = 12$, $L_6 = 12$, $L_7 = 10$, $L_8 = 9$, $L_9 = 8$, $L_{10} = 7$, $L_{11} = 6$, $L_{12} = 5$, $L_{13} = 4$, $L_{14} = 3$.\n\nStep 4: Compute the cumulative sum of lengths and compare it to the target length of $77$ kb.\n-   Adding the longest read ($L_1 = 25$): Cumulative sum = $25$ kb. This is less than $77$ kb.\n-   Adding the second longest read ($L_2 = 20$): Cumulative sum = $25 + 20 = 45$ kb. This is less than $77$ kb.\n-   Adding the third longest read ($L_3 = 18$): Cumulative sum = $45 + 18 = 63$ kb. This is less than $77$ kb.\n-   Adding the fourth longest read ($L_4 = 15$): Cumulative sum = $63 + 15 = 78$ kb. This sum is greater than or equal to the target of $77$ kb.\n\nStep 5: Identify the N50 value. The cumulative sum first meets the threshold with the inclusion of the read of length $15$ kb. Therefore, by definition, the read N50 for this dataset is $15$ kb.\n\nNext, we address the second part of the problem: explaining from first principles how the read N50 influences the expected contig N50 under an idealized OLC assembler.\n\nThe contig N50 is a metric for assembly quality, where a higher value indicates a more contiguous assembly. The problem states that contigs break at unresolved repeats. An increase in read N50 has a direct, positive influence on the ability to resolve repeats, which in turn increases the contig N50.\n\nLet's formalize this relationship. The problem states that read start positions are modeled by a homogeneous Poisson process. Let us consider reads of a single characteristic length $L$, which can represent the long-read portion of the distribution (e.g., $L \\approx \\text{N50}$). For a repeat of length $R$ flanked by unique sequences of total length $2u$, the total region to be spanned has length $R+2u$. A read of length $L$ can span this region only if $L > R+2u$.\n\nThe number of possible genomic start positions for such a read to successfully span the entire region is the difference between the read length and the region length: $L - (R+2u)$. The rate of read starts per base across the genome is $\\lambda_{\\text{start}}$. Given a sequencing coverage $c$, this rate is $\\lambda_{\\text{start}} = c/L$.\n\nThe expected number of spanning reads, $\\mu$, is the product of the rate of starts and the size of the window of valid start positions:\n$$\n\\mu = \\lambda_{\\text{start}} \\times (L - (R+2u)) = \\frac{c}{L}(L - (R+2u))\n$$\nThe number of spanning reads is well-approximated by a Poisson distribution with mean $\\mu$. The repeat is unresolved if there are zero spanning reads. The probability of this event is given by the Poisson probability for a count of zero:\n$$\nP(\\text{unresolved}) = e^{-\\mu} = \\exp\\left(-\\frac{c}{L}(L - (R+2u))\\right)\n$$\nTherefore, the probability that the repeat is successfully resolved is:\n$$\nP(\\text{resolved}) = 1 - P(\\text{unresolved}) = 1 - \\exp\\left(-\\frac{c}{L}(L - (R+2u))\\right)\n$$\nThis symbolic expression connects the probability of repeat resolution to the read length $L$. Now, we connect this to the read N50. The N50 is a statistic that reflects the typical length of the longer reads in the distribution. A higher N50 implies that a representative long read length $L$ is larger.\n\nAs $L$ increases:\n1. The condition $L > R+2u$ is met for longer and longer repeats $R$.\n2. For a given repeat that can be spanned, the term $(L - (R+2u))$ increases, which increases the expected number of spanning reads, $\\mu$.\n\nAn increase in $\\mu$ makes $e^{-\\mu}$ smaller, thus increasing the probability of resolution, $P(\\text{resolved})$, towards 1.\n\nIn summary, the chain of reasoning is as follows:\n1.  An increased read N50 signifies that the characteristic length $L$ of the contiguity-driving long reads is greater.\n2.  A greater $L$ increases the expected number of reads spanning any given repeat, thereby increasing the probability of resolving that repeat.\n3.  Since contigs break at unresolved repeats, higher resolution probability means fewer contig breaks across the genome.\n4.  Fewer breaks result in a more contiguous assembly, which is measured by a higher expected contig N50.\n\nThus, a higher read N50 is a strong predictor of a higher contig N50 because it directly enhances the ability of an assembler to resolve repeats, which are a primary cause of assembly fragmentation.", "answer": "$$\n\\boxed{15}\n$$", "id": "4579363"}, {"introduction": "After generating a genome assembly, the first step is to assess its quality, particularly its contiguity. This practice introduces two of the most ubiquitous metrics in genomics: the $N50$ and $NG50$ statistics, which measure the contiguity of an assembly relative to its own size and a known genome size, respectively. Working through this calculation will clarify the precise definitions of these metrics and highlight how they provide complementary insights into the quality and completeness of an assembly [@problem_id:4579430].", "problem": "In haplotype-resolved deoxyribonucleic acid (DNA) assembly evaluation for long-read sequencing, contiguity metrics such as $N50$ and $NG50$ are widely reported. Build these metrics from first principles using the following base: a genome assembly is a multiset of contigs with lengths $\\{L_{i}\\}$, which can be sorted in nonincreasing order as $L_{1} \\geq L_{2} \\geq \\dots \\geq L_{n}$. The cumulative sum after $k$ contigs is $S_{k} = \\sum_{i=1}^{k} L_{i}$, and the total assembly length is $S_{n} = \\sum_{i=1}^{n} L_{i}$. The estimated haploid genome size is a known external quantity $G$ that may exceed $S_{n}$ when the assembly is incomplete.\n\nStarting from these definitions and facts, derive formal definitions and stepwise procedures for $N50$ and $NG50$ that rely only on sorted contig lengths and, for $NG50$, the known or estimated genome size. Then apply your procedures to the following maternal haplotype contig set from a long-read assembly, given in megabases (Mb), already sorted in nonincreasing order:\n$$\n25.0,\\;22.0,\\;20.0,\\;18.0,\\;16.0,\\;14.0,\\;12.0,\\;10.0,\\;9.0,\\;8.0,\\;7.5,\\;6.5,\\;5.0,\\;4.5,\\;4.0,\\;3.5,\\;3.0,\\;2.5,\\;2.0,\\;1.5.\n$$\nAssume the estimated haploid genome size is $G = 230$ megabases. Compute $N50$ and $NG50$ in megabases following your derived procedures, and then compute the ratio $N50/NG50$. Round your final ratio to six significant figures and report it as a pure number without units.", "solution": "The problem statement is evaluated and found to be valid. It is scientifically grounded in the established principles of genome assembly evaluation, well-posed with all necessary information provided, and objective in its formulation. We can therefore proceed with a formal solution.\n\nThe problem requires the derivation of formal definitions and procedures for the assembly contiguity metrics $N50$ and $NG50$, followed by their application to a provided dataset of sorted contig lengths.\n\nLet the multiset of contig lengths be $\\{L_{i}\\}_{i=1}^{n}$, sorted in nonincreasing order such that $L_{1} \\geq L_{2} \\geq \\dots \\geq L_{n}$.\nThe total length of the assembly is given by $S_{n} = \\sum_{i=1}^{n} L_{i}$.\nThe estimated haploid genome size is an external parameter, denoted by $G$.\n\n**Procedure for N50**\n\nThe $N50$ metric is defined as the length of the shortest contig in the smallest set of contigs whose cumulative length constitutes at least $50\\%$ of the total assembly length, $S_n$.\n\n1.  Calculate the total assembly length, $S_{n} = \\sum_{i=1}^{n} L_{i}$.\n2.  Determine the target length, which is half of the total assembly length: $T_{N50} = 0.5 \\times S_{n}$.\n3.  Find the smallest integer index $k$ such that the cumulative sum of the lengths of the first $k$ contigs (in descending order of length) is greater than or equal to $T_{N50}$. Mathematically, find the minimum $k \\in \\{1, 2, \\dots, n\\}$ that satisfies the inequality:\n    $$S_{k} = \\sum_{i=1}^{k} L_{i} \\geq T_{N50}$$\n4.  The $N50$ value is the length of the $k$-th contig, $L_k$.\n    $$N50 = L_{k}$$\n\n**Procedure for NG50**\n\nThe $NG50$ metric is analogous to $N50$, but it uses the estimated genome size, $G$, as the reference instead of the total assembly length, $S_n$. This provides a measure of contiguity relative to an external, more objective standard of genome completeness.\n\n1.  Use the given estimated haploid genome size, $G$.\n2.  Determine the target length, which is half of the estimated genome size: $T_{NG50} = 0.5 \\times G$.\n3.  Find the smallest integer index $m$ such that the cumulative sum of the lengths of the first $m$ contigs is greater than or equal to $T_{NG50}$. Mathematically, find the minimum $m \\in \\{1, 2, \\dots, n\\}$ that satisfies the inequality:\n    $$S_{m} = \\sum_{i=1}^{m} L_{i} \\geq T_{NG50}$$\n4.  The $NG50$ value is the length of the $m$-th contig, $L_m$.\n    $$NG50 = L_{m}$$\n\n**Application to the Provided Data**\n\nThe sorted list of contig lengths (in megabases, Mb) is:\n$\\{L_i\\} = \\{25.0, 22.0, 20.0, 18.0, 16.0, 14.0, 12.0, 10.0, 9.0, 8.0, 7.5, 6.5, 5.0, 4.5, 4.0, 3.5, 3.0, 2.5, 2.0, 1.5\\}$.\nThe number of contigs is $n = 20$.\nThe estimated haploid genome size is $G = 230$ Mb.\n\nFirst, we calculate the total assembly length, $S_{n}$:\n$$S_{20} = 25.0 + 22.0 + 20.0 + 18.0 + 16.0 + 14.0 + 12.0 + 10.0 + 9.0 + 8.0 + 7.5 + 6.5 + 5.0 + 4.5 + 4.0 + 3.5 + 3.0 + 2.5 + 2.0 + 1.5$$\n$$S_{20} = 194.0 \\text{ Mb}$$\n\n**Calculation of N50**\n\n1.  The target length is $T_{N50} = 0.5 \\times S_{20} = 0.5 \\times 194.0 = 97.0$ Mb.\n2.  We compute the cumulative sums $S_k$ until the sum is at least $97.0$:\n    $S_{1} = L_{1} = 25.0$\n    $S_{2} = S_{1} + L_{2} = 25.0 + 22.0 = 47.0$\n    $S_{3} = S_{2} + L_{3} = 47.0 + 20.0 = 67.0$\n    $S_{4} = S_{3} + L_{4} = 67.0 + 18.0 = 85.0$\n    $S_{5} = S_{4} + L_{5} = 85.0 + 16.0 = 101.0$\n3.  The cumulative sum $S_5 = 101.0$ is the first to exceed the target of $97.0$. Thus, the index is $k=5$.\n4.  The $N50$ value is the length of the $5$-th contig, $L_5$.\n    $$N50 = L_{5} = 16.0 \\text{ Mb}$$\n\n**Calculation of NG50**\n\n1.  The target length is $T_{NG50} = 0.5 \\times G = 0.5 \\times 230.0 = 115.0$ Mb.\n2.  We continue computing the cumulative sums $S_m$ until the sum is at least $115.0$:\n    We already have $S_5 = 101.0$.\n    $S_{6} = S_{5} + L_{6} = 101.0 + 14.0 = 115.0$\n3.  The cumulative sum $S_6 = 115.0$ is the first to be greater than or equal to the target of $115.0$. Thus, the index is $m=6$.\n4.  The $NG50$ value is the length of the $6$-th contig, $L_6$.\n    $$NG50 = L_{6} = 14.0 \\text{ Mb}$$\n\n**Calculation of the Ratio N50/NG50**\n\nThe final step is to compute the ratio of $N50$ to $NG50$ and round to six significant figures.\n$$\\frac{N50}{NG50} = \\frac{16.0}{14.0} = \\frac{8}{7}$$\nAs a decimal, this is:\n$$\\frac{8}{7} \\approx 1.142857142...$$\nRounding to six significant figures involves examining the seventh significant figure. The first six are $1, 1, 4, 2, 8, 5$. The seventh is $7$. Since $7 \\geq 5$, we round up the sixth digit.\n$$1.142857... \\approx 1.14286$$\nThe final ratio is a pure number.", "answer": "$$\\boxed{1.14286}$$", "id": "4579430"}, {"introduction": "For diploid organisms, a truly high-quality assembly resolves the two parental haplotypes. Evaluating this aspect requires specialized metrics that go beyond simple contiguity, focusing instead on phasing accuracy. This exercise will guide you through the calculation of the switch error rate, a fundamental measure of local phasing correctness, by comparing a candidate assembly's phasing against a known truth set [@problem_id:4579441].", "problem": "A haplotype-resolved assembly generated from long-read sequencing is being evaluated against a high-confidence truth set in a $5$ megabase window of a human chromosome. Consider the following setup grounded in standard phasing definitions. A heterozygous variant is defined as any site where the two homologous chromosomes carry different alleles. A phasing assigns each heterozygous variant to one of two haplotypes, and a phase transition between two adjacent heterozygous variants indicates whether the haplotype assignment stays the same or flips when moving from the first to the second site in coordinate order. The orientation of each phase block is arbitrary, so only relative phase transitions between adjacent heterozygous variants are meaningful for truth evaluation.\n\nYou are given a truth set of heterozygous variants and a corresponding phased call set from the assembly. To compute the switch error rate relative to truth, only adjacent heterozygous variant pairs that are present and phased in both sets and do not cross a phase break in either set are considered. Because missing or unphased variants break adjacency, the comparable adjacent pairs decompose into disjoint runs where consecutive truth heterozygous variants are all present and phased in the call set. In this region, the intersection of truth and call heterozygous variants forms $6$ runs of lengths $50$, $63$, $37$, $42$, $30$, and $58$ respectively. Within these runs, the number of adjacent heterozygous variant pairs whose phase transitions disagree with the truth (i.e., the call’s relative transition differs from the truth’s relative transition) are $5$, $7$, $3$, $2$, $1$, and $4$ respectively.\n\nUsing foundational definitions of haplotype phasing and phase transitions, compute the switch error rate as the fraction of comparable adjacent heterozygous variant pairs with incorrect phase transitions. Express your final answer as a decimal and round to four significant figures.", "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of bioinformatics and genomics, specifically haplotype phasing and assembly evaluation. The problem is well-posed, objective, and contains all necessary information for a unique solution.\n\nThe primary task is to compute the switch error rate for a haplotype-resolved assembly. According to the problem's definition, the switch error rate is the fraction of comparable adjacent heterozygous variant pairs with incorrect phase transitions. This can be expressed mathematically as:\n$$\n\\text{Switch Error Rate} = \\frac{\\text{Total number of switch errors}}{\\text{Total number of comparable adjacent pairs}}\n$$\n\nFirst, we must determine the total number of comparable adjacent heterozygous variant pairs. The problem states that the analysis is restricted to disjoint runs of variants that are present and phased in both the truth and call sets. A run of length $L$, defined as containing $L$ consecutive heterozygous variants, will have $L-1$ adjacent pairs of variants.\n\nThe lengths of the $6$ disjoint runs are given as $L_1 = 50$, $L_2 = 63$, $L_3 = 37$, $L_4 = 42$, $L_5 = 30$, and $L_6 = 58$.\n\nThe number of adjacent pairs in each run, denoted $P_i$, is calculated as $P_i = L_i - 1$.\nFor the first run: $P_1 = L_1 - 1 = 50 - 1 = 49$\nFor the second run: $P_2 = L_2 - 1 = 63 - 1 = 62$\nFor the third run: $P_3 = L_3 - 1 = 37 - 1 = 36$\nFor the fourth run: $P_4 = L_4 - 1 = 42 - 1 = 41$\nFor the fifth run: $P_5 = L_5 - 1 = 30 - 1 = 29$\nFor the sixth run: $P_6 = L_6 - 1 = 58 - 1 = 57$\n\nThe total number of comparable adjacent pairs, $P_{total}$, is the sum of the pairs in all runs:\n$$\nP_{total} = \\sum_{i=1}^{6} P_i = P_1 + P_2 + P_3 + P_4 + P_5 + P_6\n$$\n$$\nP_{total} = 49 + 62 + 36 + 41 + 29 + 57 = 274\n$$\nAlternatively, one could calculate this as the sum of all run lengths minus the number of runs:\n$$\nP_{total} = \\left(\\sum_{i=1}^{6} L_i\\right) - 6 = (50 + 63 + 37 + 42 + 30 + 58) - 6 = 280 - 6 = 274\n$$\n\nNext, we must determine the total number of switch errors. A switch error is defined as an adjacent heterozygous variant pair where the phase transition in the call set disagrees with the truth set. The problem provides the number of such errors for each run: $E_1 = 5$, $E_2 = 7$, $E_3 = 3$, $E_4 = 2$, $E_5 = 1$, and $E_6 = 4$.\n\nThe total number of switch errors, $E_{total}$, is the sum of the errors in all runs:\n$$\nE_{total} = \\sum_{i=1}^{6} E_i = E_1 + E_2 + E_3 + E_4 + E_5 + E_6\n$$\n$$\nE_{total} = 5 + 7 + 3 + 2 + 1 + 4 = 22\n$$\n\nNow, we can compute the switch error rate by dividing the total number of switch errors by the total number of comparable adjacent pairs:\n$$\n\\text{Switch Error Rate} = \\frac{E_{total}}{P_{total}} = \\frac{22}{274}\n$$\nThe fraction simplifies to:\n$$\n\\frac{22}{274} = \\frac{11}{137}\n$$\nTo express this as a decimal, we perform the division:\n$$\n\\frac{11}{137} \\approx 0.08029197...\n$$\nThe problem requires the answer to be rounded to four significant figures. The first significant figure is $8$. The fourth significant figure is $9$ (in the ten-thousandths place). The digit following the $9$ is $1$, which is less than $5$, so we round down (i.e., keep the $9$ as it is).\n\nTherefore, the switch error rate, rounded to four significant figures, is $0.08029$.", "answer": "$$\\boxed{0.08029}$$", "id": "4579441"}]}