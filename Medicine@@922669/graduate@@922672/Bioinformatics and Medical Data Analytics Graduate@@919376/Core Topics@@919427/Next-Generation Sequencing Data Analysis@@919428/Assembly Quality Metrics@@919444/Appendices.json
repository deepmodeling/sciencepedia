{"hands_on_practices": [{"introduction": "The $N_{50}$ statistic is a cornerstone of genome assembly evaluation, providing a single value that summarizes the contiguity of an assembly. Before we can critique its limitations or explore more advanced metrics, it is crucial to understand how $N_{50}$ is derived from first principles. This fundamental exercise [@problem_id:4540123] walks you through the step-by-step process of calculating $N_{50}$ from a raw list of contig lengths, solidifying your grasp of this ubiquitous metric.", "problem": "In a clinical bioinformatics pipeline for metagenomic analysis, a draft genome assembly is represented by a multiset of contig lengths. Quality assessment often requires summarizing the length distribution by a threshold statistic that captures how much of the assembly is covered by the longest contigs. Begin from the fundamental bases that (i) the total assembly span is the sum of contig lengths, and (ii) cumulative coverage arises by ordering contigs by length and accumulating their spans. Using these bases, first provide a rigorous definition of the statistic $N_{50}$ as a length-weighted threshold derived from the cumulative distribution of contig lengths. Then, for the specific contig lengths $[800,\\,200,\\,300,\\,700,\\,500]$ measured in base pairs (bp), execute the following:\n\n- Justify why contigs should be ordered by nonincreasing length to construct the cumulative coverage function from longest to shortest.\n- Compute the total assembly length and the coverage target corresponding to $\\frac{1}{2}$ of the total.\n- Accumulate lengths in the justified order until the coverage target is met, and identify the threshold length that formally corresponds to $N_{50}$.\n\nExpress your final result as a single integer count of base pairs (bp). No rounding is required. Do not provide any intermediate formulas in place of a definition; derive $N_{50}$ from the stated bases and demonstrate each step of the computation.", "solution": "The problem statement is scientifically grounded, well-posed, and objective. It provides a clear set of contig lengths and asks for the computation of a standard, well-defined bioinformatics metric, $N_{50}$. All necessary information is present, and there are no contradictions or ambiguities. The problem is valid.\n\nWe begin by formalizing the definition of the $N_{50}$ statistic from the provided bases. Let the multiset of $n$ contig lengths be denoted by $L = \\{l_1, l_2, \\dots, l_n\\}$.\n\nFirst, the total assembly span, $L_{total}$, is the sum of all individual contig lengths, as stated in the problem's first basis.\n$$L_{total} = \\sum_{i=1}^{n} l_i$$\n\nSecond, to assess the contribution of the longest contigs, we must arrange the lengths in a specific order. Let $L' = \\{l'_1, l'_2, \\dots, l'_n\\}$ be the sequence of contig lengths from the multiset $L$ sorted in nonincreasing order, such that $l'_1 \\ge l'_2 \\ge \\dots \\ge l'_n$.\n\nFrom this ordered sequence, we can define a cumulative length function, $C(k)$, which represents the sum of the lengths of the $k$ longest contigs:\n$$C(k) = \\sum_{i=1}^{k} l'_i$$\n\nThe $N_{50}$ statistic is a length-weighted threshold. It is defined as the length of the shortest contig in the smallest set of longest contigs that collectively cover at least $50\\%$ (or a fraction of $\\frac{1}{2}$) of the total assembly length. To find this, we must first identify the smallest integer index, which we will call $k_{50}$, for which the cumulative sum $C(k)$ meets or exceeds the target coverage of $\\frac{1}{2}L_{total}$.\n$$k_{50} = \\min \\left\\{ k \\in \\{1, \\dots, n\\} \\mid \\sum_{i=1}^{k} l'_i \\ge \\frac{1}{2} L_{total} \\right\\}$$\nThe $N_{50}$ value is then precisely the length of the contig at this index $k_{50}$ in the sorted list $L'$.\n$$N_{50} = l'_{k_{50}}$$\n\nWe now apply this formal procedure to the given data.\n\n**Justification for Ordering by Nonincreasing Length**\nThe goal of metrics like $N_{50}$ is to evaluate assembly contiguity. An assembly with a few very long contigs is generally considered higher quality than one with many short contigs, even if their total lengths are identical. By ordering the contigs from longest to shortest, we prioritize the aformentioned long contigs. Constructing the cumulative sum in this order directly answers the question: \"What is the minimum length of a contig such that all contigs of that length or longer cover half the genome?\" This provides a single-number summary of the upper end of the contig length distribution, which is most relevant for assessing quality. An increasing-order summation would answer a different and less informative question about the contribution of the shortest contigs.\n\n**Computation**\nThe given multiset of contig lengths is $L = \\{800, 200, 300, 700, 500\\}$, with units of base pairs (bp).\n\n1.  **Compute the total assembly length ($L_{total}$):**\n    $$L_{total} = 800 + 200 + 300 + 700 + 500 = 2500 \\text{ bp}$$\n\n2.  **Compute the coverage target:**\n    The target is $\\frac{1}{2}$ of the total assembly length.\n    $$\\text{Target} = \\frac{1}{2} L_{total} = \\frac{1}{2} \\times 2500 \\text{ bp} = 1250 \\text{ bp}$$\n\n3.  **Order contigs by nonincreasing length and accumulate:**\n    First, we sort the lengths in $L$ to create the ordered sequence $L'$.\n    $$L' = \\{800, 700, 500, 300, 200\\}$$\n    Next, we accumulate the lengths in this order and check against the target of $1250$ bp.\n    -   Add the first contig ($l'_1 = 800$): Cumulative length is $C(1) = 800$. This is less than the $1250$ bp target.\n    -   Add the second contig ($l'_2 = 700$): Cumulative length is $C(2) = 800 + 700 = 1500$. This value, $1500$, meets and exceeds the $1250$ bp target.\n\n4.  **Identify the $N_{50}$ value:**\n    The cumulative sum first met the target when we included the second contig ($k_{50}=2$). According to the definition, the $N_{50}$ statistic is the length of this last contig added to cross the threshold.\n    $$N_{50} = l'_{k_{50}} = l'_2 = 700$$\n    Therefore, the $N_{50}$ for this assembly is $700$ bp.", "answer": "$$\\boxed{700}$$", "id": "4540123"}, {"introduction": "While $N_{50}$ is informative, relying on it alone can be misleading. A high $N_{50}$ might mask significant fragmentation in other parts of the assembly. This practice [@problem_id:4540052] introduces the complementary $L_{90}$ metric and challenges you to compare two assemblies with identical total lengths but vastly different structures, helping you develop a more nuanced and critical approach to interpreting quality scores.", "problem": "A clinical bioinformatics team evaluates two draft genome assemblies derived from an oncogenomic specimen to understand structural continuity and to inform downstream variant analysis. In genome assembly evaluation, contigs are ordered by length from longest to shortest, the total assembly span is the sum of all contig lengths, and continuity metrics are computed on that ordered list. Using the standard definitions employed in assembly evaluation: the fifty-percent contig length statistic (N50) is the length of the shortest contig within the minimal prefix of the ordered list whose cumulative span reaches at least one-half of the total assembly span, and the ninety-percent span contig count (L90) is the minimal number of contigs in the ordered list required to reach at least nine-tenths of the total assembly span. The two assemblies have contig length multisets given by $A:[3000, 3000, 3000]$ and $B:[8000, 500, 500]$, where each number denotes a length in base pairs. Compute the values of $N_{50}$ and $L_{90}$ for assemblies $A$ and $B$ based on these definitions. Then, from first principles, articulate the trade-offs revealed by these metrics between assemblies characterized by few very long contigs and those with more balanced distributions, specifically focusing on how the pair $\\left(N_{50}, L_{90}\\right)$ jointly constrains interpretability and downstream utility in medical data analytics. Express all contig lengths in base pairs, and all counts as unitless integers. If any non-integer quantity arises, round it to four significant figures. Provide your final answer as a single row matrix in the order $\\left(N_{50}(A), L_{90}(A), N_{50}(B), L_{90}(B)\\right)$.", "solution": "The user-provided problem has been evaluated against the specified criteria and is determined to be valid. It is scientifically grounded in the principles of bioinformatics and genome assembly evaluation, well-posed with clear definitions and sufficient data, and objective in its formulation. We may therefore proceed with a formal solution.\n\nThe problem requires the calculation of two standard genome assembly quality metrics, the fifty-percent contig length statistic ($N_{50}$) and the ninety-percent span contig count ($L_{90}$), for two given assemblies, designated $A$ and $B$. It also requires an articulation of the trade-offs these metrics reveal.\n\nLet the multiset of contig lengths for a given assembly be $C = \\{c_1, c_2, \\dots, c_n\\}$.\nThe total assembly span, $S$, is the sum of the lengths of all contigs:\n$$S = \\sum_{i=1}^{n} c_i$$\nTo calculate $N_{50}$ and $L_{90}$, the contigs must first be sorted in descending order of length, creating an ordered list $C' = (l_1, l_2, \\dots, l_n)$, where $l_1 \\ge l_2 \\ge \\dots \\ge l_n$.\n\nThe $N_{50}$ statistic is defined as the length $l_k$ of the shortest contig in the minimal prefix of $C'$ such that the cumulative sum of lengths reaches at least $50\\%$ of the total span $S$. That is, find the smallest integer $k$ such that:\n$$\\sum_{i=1}^{k} l_i \\ge 0.5 \\times S$$\nThe $N_{50}$ value is then $l_k$, the length of the shortest contig in the set $\\{l_1, l_2, \\dots, l_k\\}$. Since the list is ordered, $l_k$ is always the shortest.\n\nThe $L_{90}$ statistic is defined as the minimum number of contigs from the ordered list $C'$ whose cumulative length is at least $90\\%$ of the total span $S$. That is, find the smallest integer $k$ such that:\n$$\\sum_{i=1}^{k} l_i \\ge 0.9 \\times S$$\nThe $L_{90}$ value is this integer $k$.\n\nWe will now apply these definitions to assemblies $A$ and $B$.\n\n**Assembly A**\nThe multiset of contig lengths is $A = [3000, 3000, 3000]$.\n1.  **Order Contigs**: The lengths are identical, so the ordered list is $C'_A = (3000, 3000, 3000)$.\n2.  **Total Span**: The total assembly span is $S_A = 3000 + 3000 + 3000 = 9000$ base pairs.\n3.  **Thresholds**: The $50\\%$ threshold is $0.5 \\times 9000 = 4500$. The $90\\%$ threshold is $0.9 \\times 9000 = 8100$.\n\n4.  **Calculate $N_{50}(A)$**: We find the smallest $k$ such that the cumulative sum is $\\ge 4500$.\n    -   For $k=1$: Cumulative sum is $3000$. This is $ 4500$.\n    -   For $k=2$: Cumulative sum is $3000 + 3000 = 6000$. This is $\\ge 4500$.\n    The minimal set of contigs is $(3000, 3000)$. The shortest (and only) length in this set is $3000$.\n    Therefore, $N_{50}(A) = 3000$.\n\n5.  **Calculate $L_{90}(A)$**: We find the smallest $k$ such that the cumulative sum is $\\ge 8100$.\n    -   For $k=1$: Cumulative sum is $3000$. This is $ 8100$.\n    -   For $k=2$: Cumulative sum is $6000$. This is $ 8100$.\n    -   For $k=3$: Cumulative sum is $6000 + 3000 = 9000$. This is $\\ge 8100$.\n    The minimal number of contigs required is $3$.\n    Therefore, $L_{90}(A) = 3$.\n\n**Assembly B**\nThe multiset of contig lengths is $B = [8000, 500, 500]$.\n1.  **Order Contigs**: The ordered list is $C'_B = (8000, 500, 500)$.\n2.  **Total Span**: The total assembly span is $S_B = 8000 + 500 + 500 = 9000$ base pairs.\n3.  **Thresholds**: The thresholds are identical to Assembly $A$: $50\\%$ is $4500$, $90\\%$ is $8100$.\n\n4.  **Calculate $N_{50}(B)$**: We find the smallest $k$ such that the cumulative sum is $\\ge 4500$.\n    -   For $k=1$: Cumulative sum is $8000$. This is $\\ge 4500$.\n    The minimal set of contigs is $(8000)$. The shortest (and only) length in this set is $8000$.\n    Therefore, $N_{50}(B) = 8000$.\n\n5.  **Calculate $L_{90}(B)$**: We find the smallest $k$ such that the cumulative sum is $\\ge 8100$.\n    -   For $k=1$: Cumulative sum is $8000$. This is $ 8100$.\n    -   For $k=2$: Cumulative sum is $8000 + 500 = 8500$. This is $\\ge 8100$.\n    The minimal number of contigs required is $2$.\n    Therefore, $L_{90}(B) = 2$.\n\n**Interpretation of Trade-offs**\nThe calculated metrics are:\n-   Assembly $A$: $(N_{50}, L_{90}) = (3000, 3)$\n-   Assembly $B$: $(N_{50}, L_{90}) = (8000, 2)$\n\nThese pairs reveal fundamental differences in the structure of the two assemblies, despite their identical total size.\n\n-   **$N_{50}$ as a measure of contiguity**: $N_{50}$ indicates the length of contigs that make up the \"bulkier\" half of the assembly. A higher $N_{50}$ is generally better, signifying that at least half of the genome is assembled into large, continuous pieces. Assembly $B$'s $N_{50}$ of $8000$ is substantially higher than Assembly $A$'s $3000$, indicating superior large-scale contiguity for at least half its content.\n\n-   **$L_{90}$ as a measure of fragmentation**: $L_{90}$ indicates how many pieces constitute the vast majority ($90\\%$) of the assembly. A lower $L_{90}$ is better, signifying that the assembly is less fragmented. Assembly $B$'s $L_{90}$ of $2$ is lower than Assembly $A$'s $3$, meaning one can capture $90\\%$ of the genome with fewer contigs in $B$.\n\nThe pair $(N_{50}, L_{90})$ jointly constrains interpretation:\n1.  **Assembly $A$ ($(3000, 3)$)** represents a **balanced but fragmented** assembly. All its contigs are of a uniform, moderate length. The low $N_{50}$ reflects the absence of very long contigs. For downstream analysis, this means that while the quality is consistent, any gene or structural variant larger than $3000$ base pairs will be broken across multiple contigs, complicating analysis.\n\n2.  **Assembly $B$ ($(8000, 2)$)** represents a **top-heavy or skewed** assembly. It is characterized by one very long contig and a \"tail\" of very short ones. The high $N_{50}$ and low $L_{90}$ are both driven by the single $8000$ bp contig.\n    -   **Utility**: This structure is highly advantageous if a region of interest, such as a large oncogene, lies entirely within the long contig. It allows for the unambiguous analysis of variants and local structure in that region.\n    -   **Interpretability Constraint**: The high-quality metrics can be misleading. They mask the fact that $\\approx 11.1\\%$ of the assembly is highly fragmented into small $500$ bp pieces. Information in these fragmented regions is isolated, and their relationship to the main contig is unknown. This illustrates a critical trade-off: pursuing a high $N_{50}$ can sometimes lead to an assembly where a few \"hero\" contigs are generated at the expense of correctly placing the remaining sequence, resulting in a fragmented tail.\n\nIn the context of medical data analytics, Assembly $B$ would be preferred for analyzing long-range features within its single large contig. However, a researcher must be cautious, as the impressive summary statistics $(8000, 2)$ hide the poor quality of a non-trivial fraction of the assembly. Assembly $A$ is less impressive on paper but may be more uniformly reliable, though its utility for studying large genes or structural variants is inherently limited by its fragmentation. The choice between them depends on the specific analytical goal.\n\nThe final result vector is ordered as $(N_{50}(A), L_{90}(A), N_{50}(B), L_{90}(B))$.\nThe values are $(3000, 3, 8000, 2)$. All values are integers, so no rounding is required.", "answer": "$$\\boxed{\\begin{pmatrix} 3000  3  8000  2 \\end{pmatrix}}$$", "id": "4540052"}, {"introduction": "In many real-world scenarios, the true genome size is not known with certainty and must be estimated. The $NG_{50}$ metric normalizes assembly contiguity against this estimated genome size, $G$, rather than the assembly's total length. This exercise [@problem_id:4540059] explores the practical implications of this shift, asking you to quantify how uncertainty in $G$ propagates to the $NG_{50}$ value, a key skill in assessing the robustness of your bioinformatic conclusions.", "problem": "A clinical bacterial isolate is sequenced, and the de novo assembly yields four contigs with lengths $[900, 800, 700, 600]$ kilobase pairs (kbp). The referring laboratory provides an estimated haploid genome size $G$ with an uncertainty of $\\pm 5\\%$. In genome assembly assessment, the $N50$ (weighted median contig length of assembly) is defined as the contig length such that arranging contigs in nonincreasing order and summing their lengths from largest to smallest, the cumulative length first reaches at least half of the total assembly length. The $NG50$ (genome-size-normalized $N50$) is defined identically except that the cumulative length is compared to at least half of the estimated genome size $G$ instead of the total assembly length. Specifically, sorting contigs by length in nonincreasing order and letting $L_{1}, L_{2}, \\dots, L_{n}$ be their lengths, the $NG50$ is the length of the shortest contig in the smallest prefix $\\{L_{1}, \\dots, L_{k}\\}$ whose sum satisfies $\\sum_{i=1}^{k} L_{i} \\geq \\frac{G}{2}$.\n\nAssume $G = 1800$ kbp. To quantify how uncertainty in $G$ propagates to the reported $NG50$, define the symmetric finite-difference sensitivity of $NG50$ with respect to $G$ under a $\\pm 5\\%$ perturbation as\n$$\nS \\equiv \\frac{NG50\\big((1+0.05)G\\big) - NG50\\big((1-0.05)G\\big)}{(1+0.05)G - (1-0.05)G}.\n$$\nCompute $S$ for the given contig lengths. Express your final answer in kilobase pairs per kilobase pair (kbp/kbp) and round your answer to four significant figures.", "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in the field of bioinformatics, well-posed, objective, and internally consistent. All necessary data and definitions for a unique solution are provided.\n\nThe objective is to compute the symmetric finite-difference sensitivity, $S$, of the $NG50$ metric with respect to the estimated genome size, $G$, under a $\\pm 5\\%$ perturbation. The formula for $S$ is given as:\n$$\nS \\equiv \\frac{NG50\\big((1+0.05)G\\big) - NG50\\big((1-0.05)G\\big)}{(1+0.05)G - (1-0.05)G}\n$$\nThe problem provides the following information:\nThe set of contig lengths is $\\{900, 800, 700, 600\\}$ kilobase pairs (kbp). We sort these in nonincreasing order, which they already are:\n$L_1 = 900$ kbp\n$L_2 = 800$ kbp\n$L_3 = 700$ kbp\n$L_4 = 600$ kbp\n\nThe nominal estimated haploid genome size is $G = 1800$ kbp.\n\nThe definition of $NG50$ is the length of the shortest contig in the smallest prefix of sorted contigs $\\{L_{1}, \\dots, L_{k}\\}$ whose cumulative sum is at least half of the genome size, i.e., $\\sum_{i=1}^{k} L_{i} \\geq \\frac{G}{2}$. Since the contigs are sorted in nonincreasing order ($L_1 \\geq L_2 \\geq \\dots$), the shortest contig in the prefix $\\{L_1, \\dots, L_k\\}$ is always $L_k$. Therefore, $NG50$ is the length $L_k$ of the contig that causes the cumulative sum to first meet or exceed the threshold $\\frac{G}{2}$.\n\nThe calculation proceeds in three main steps:\n1.  Determine the perturbed genome sizes, $G_{lower}$ and $G_{upper}$.\n2.  Calculate $NG50(G_{lower})$ and $NG50(G_{upper})$.\n3.  Substitute these values into the formula for $S$.\n\nFirst, we calculate the perturbed genome sizes corresponding to a $\\pm 5\\%$ variation. Let the perturbation factor be $\\delta = 0.05$.\nThe upper-bound genome size is:\n$$\nG_{upper} = (1 + \\delta)G = (1 + 0.05) \\times 1800 = 1.05 \\times 1800 = 1890 \\text{ kbp}\n$$\nThe lower-bound genome size is:\n$$\nG_{lower} = (1 - \\delta)G = (1 - 0.05) \\times 1800 = 0.95 \\times 1800 = 1710 \\text{ kbp}\n$$\nThe denominator of the sensitivity formula is the range of this perturbation:\n$$\nG_{upper} - G_{lower} = 1890 - 1710 = 180 \\text{ kbp}\n$$\n\nNext, we calculate the $NG50$ values for $G_{lower}$ and $G_{upper}$. This requires comparing cumulative contig lengths to the respective thresholds, $\\frac{G_{lower}}{2}$ and $\\frac{G_{upper}}{2}$.\nThe thresholds are:\n$$\n\\text{Threshold}_{lower} = \\frac{G_{lower}}{2} = \\frac{1710}{2} = 855 \\text{ kbp}\n$$\n$$\n\\text{Threshold}_{upper} = \\frac{G_{upper}}{2} = \\frac{1890}{2} = 945 \\text{ kbp}\n$$\nLet us compute the cumulative sums of the sorted contig lengths:\n$C_1 = \\sum_{i=1}^{1} L_i = L_1 = 900$ kbp\n$C_2 = \\sum_{i=1}^{2} L_i = L_1 + L_2 = 900 + 800 = 1700$ kbp\n$C_3 = \\sum_{i=1}^{3} L_i = C_2 + L_3 = 1700 + 700 = 2400$ kbp\n$C_4 = \\sum_{i=1}^{4} L_i = C_3 + L_4 = 2400 + 600 = 3000$ kbp\n\nTo find $NG50(G_{lower})$, we seek the smallest integer $k$ such that $C_k \\geq 855$ kbp.\nFor $k=1$, $C_1 = 900$, and $900 \\geq 855$. The condition is met for $k=1$.\nTherefore, $NG50(G_{lower}) = L_1 = 900$ kbp.\n\nTo find $NG50(G_{upper})$, we seek the smallest integer $k$ such that $C_k \\geq 945$ kbp.\nFor $k=1$, $C_1 = 900$, and $900  945$. The condition is not met.\nFor $k=2$, $C_2 = 1700$, and $1700 \\geq 945$. The condition is met for $k=2$.\nTherefore, $NG50(G_{upper}) = L_2 = 800$ kbp.\n\nFinally, we substitute these values into the formula for the sensitivity $S$:\n$$\nS = \\frac{NG50(G_{upper}) - NG50(G_{lower})}{G_{upper} - G_{lower}} = \\frac{800 \\text{ kbp} - 900 \\text{ kbp}}{180 \\text{ kbp}} = \\frac{-100}{180}\n$$\nSimplifying the fraction gives:\n$$\nS = -\\frac{10}{18} = -\\frac{5}{9}\n$$\nAs a decimal, this is $S = -0.5555...$\nThe problem requires the answer to be rounded to four significant figures.\n$$\nS \\approx -0.5556\n$$\nThe units are kbp/kbp, which is a dimensionless quantity, as expected for sensitivity.", "answer": "$$\\boxed{-0.5556}$$", "id": "4540059"}]}