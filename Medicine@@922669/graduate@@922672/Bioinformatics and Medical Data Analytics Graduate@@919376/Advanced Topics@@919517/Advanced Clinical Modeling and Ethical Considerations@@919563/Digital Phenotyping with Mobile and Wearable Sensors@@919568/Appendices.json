{"hands_on_practices": [{"introduction": "Raw data from wearable sensors are rarely a perfect reflection of reality; they are subject to systematic errors from manufacturing imperfections and environmental factors. This exercise addresses the critical first step in any robust analysis of motion data: accelerometer calibration. You will develop a procedure to estimate and correct for scale factors, biases, and axis misalignments by modeling the sensor's response as a linear transformation and using the Earth's gravitational field as a precise reference vector [@problem_id:4557386]. This practice provides hands-on experience in setting up a linear least-squares estimation problem and, crucially, in analyzing the conditions required for a unique solution, a concept known as identifiability.", "problem": "A tri-axial accelerometer embedded in a mobile or wearable device is used for digital phenotyping by measuring gravitational acceleration during stationary holds in different orientations and during controlled rotations between holds. The instrument exhibits scale errors, constant bias offsets, and axis misalignment that can be modeled as an unknown linear transformation of the true acceleration plus an additive bias. Assume the following physically and statistically grounded model: during a stationary hold $k$, the true specific force vector in the device frame is the gravity vector rotated by a known rotation matrix $R_k \\in \\mathbb{R}^{3 \\times 3}$ applied to the gravity vector in a fixed navigation frame. Let the gravity magnitude be the standard gravitational acceleration $g = 9.80665\\,\\mathrm{m/s^2}$ and angles be measured in degrees. Denote the gravity vector in the navigation frame by $g_0 = [0,0,-g]^\\top$ in $\\mathrm{m/s^2}$, and the corresponding device-frame gravity by $v_k = R_k g_0$. The accelerometer measurement in the device frame during the same stationary hold is modeled as $y_k = A v_k + b + \\epsilon_k$, where $A \\in \\mathbb{R}^{3 \\times 3}$ encodes scale factors and axis misalignment, $b \\in \\mathbb{R}^{3}$ encodes constant biases, and $\\epsilon_k$ is additive noise with zero mean. The calibration objective is to estimate $A$ and $b$ from multiple stationary holds in distinct orientations.\n\nYou must design and implement a calibration procedure that estimates $A$ and $b$ from noisy measurements by constructing an appropriate linear system based on the above model and solving it via a principled estimator. Additionally, you must determine identifiability conditions on the set $\\{v_k\\}$ under which the parameters $(A,b)$ are uniquely determined from the data. In particular, you must express identifiability in terms of the rank of a suitable design matrix that captures the linear relation between $(A,b)$ and the measurements $\\{y_k\\}$, and must check this condition numerically for given test cases. Your derivation must start from fundamental principles: Newton's Second Law, the constancy of the gravity field magnitude near Earth's surface, and the linearity of small-signal sensor calibration models.\n\nPhysical units must be strictly observed: all accelerations are in $\\mathrm{m/s^2}$, all angles are in degrees. When computing errors, express them as decimals (not percentages).\n\nImplement your program to process the following three test cases. In all cases, the true calibration is the same and is known so that you can compute estimation error. Use $A_{\\text{true}}$ and $b_{\\text{true}}$ defined by\n$$\nA_{\\text{true}} = \\begin{bmatrix}\n1  0.02  -0.01 \\\\\n0.01  1  -0.015 \\\\\n-0.005  0.02  1\n\\end{bmatrix}\n\\begin{bmatrix}\n1.03  0  0 \\\\\n0  0.97  0 \\\\\n0  0  0.99\n\\end{bmatrix}, \\quad\nb_{\\text{true}} = \\begin{bmatrix} 0.05 \\\\ -0.03 \\\\ 0.02 \\end{bmatrix} \\ \\ (\\mathrm{m/s^2}).\n$$\nFor each test case, construct $v_k = R_k g_0$ using the specified $R_k$ and generate measurements $y_k = A_{\\text{true}} v_k + b_{\\text{true}} + \\epsilon_k$, where $\\epsilon_k$ is independent Gaussian noise with zero mean and isotropic covariance $\\sigma^2 I_3$. Use a fixed random seed $42$ so the noise is reproducible.\n\nTest Case $1$ (diverse orientations, identifiable):\n- Noise standard deviation $\\sigma = 0.005$ in $\\mathrm{m/s^2}$.\n- Orientations given by $R_k$ as compositions of rotations about the device axes by the following angles (all in degrees):\n  $$\n  R_1 = I, \\quad\n  R_2 = R_x(90), \\quad\n  R_3 = R_x(180), \\quad\n  R_4 = R_y(90), \\quad\n  R_5 = R_z(90), \\quad\n  R_6 = R_x(45) R_y(30), \\quad\n  R_7 = R_y(120) R_z(45), \\quad\n  R_8 = R_x(210) R_z(135),\n  $$\n  where $R_x(\\theta)$, $R_y(\\theta)$, $R_z(\\theta)$ denote right-handed rotations by angle $\\theta$ (in degrees) about the $x$, $y$, and $z$ axes, respectively.\n\nTest Case $2$ (repeated single orientation, not identifiable):\n- Noise standard deviation $\\sigma = 0.005$ in $\\mathrm{m/s^2}$.\n- Orientations given by $R_k = I$ for $k = 1,2,3,4,5,6$, i.e., the device is not reoriented.\n\nTest Case $3$ (coplanar orientations, not identifiable):\n- Noise standard deviation $\\sigma = 0.01$ in $\\mathrm{m/s^2}$.\n- Orientations given by $R_k = R_z(\\psi_k) R_x(2)$ with yaw angles $\\psi_k \\in \\{0,30,60,90,120,150\\}$ (in degrees) and a small pitch of $2$ degrees.\n\nFrom the model and the data, construct a linear design matrix whose columns correspond to the unknown entries of $A$ and $b$, and solve the resulting linear system using a numerically stable method. Explicitly check identifiability by computing the matrix rank of the design matrix and declare the problem identifiable if and only if the rank equals $12$. Compute the relative estimation errors for $A$ and $b$:\n$$\n\\text{rel\\_err}_A = \\frac{\\lVert \\hat{A} - A_{\\text{true}} \\rVert_F}{\\lVert A_{\\text{true}} \\rVert_F}, \\quad\n\\text{rel\\_err}_b = \\frac{\\lVert \\hat{b} - b_{\\text{true}} \\rVert_2}{\\lVert b_{\\text{true}} \\rVert_2}.\n$$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each of the three test cases, append in order:\n- the identifiability boolean (i.e., $True$ if rank equals $12$, else $False$),\n- the integer rank of the design matrix,\n- the floating-point value $\\text{rel\\_err}_A$,\n- the floating-point value $\\text{rel\\_err}_b$.\nThus the final output contains $12$ items in total in the order $[id_1,rank_1,relA_1,relb_1,id_2,rank_2,relA_2,relb_2,id_3,rank_3,relA_3,relb_3]$.\n\nAngle unit: degrees. Acceleration unit: $\\mathrm{m/s^2}$. All floats must be printed in decimal form.", "solution": "The problem of accelerometer calibration is approached by estimating the parameters of a linear model that relates the true specific force (gravity) to the sensor's measurements. The model is given by $y_k = A v_k + b + \\epsilon_k$ for a set of stationary holds $k=1, \\dots, N$. Here, $y_k \\in \\mathbb{R}^3$ is the measured acceleration, $v_k \\in \\mathbb{R}^3$ is the true gravity vector in the device frame, $A \\in \\mathbb{R}^{3 \\times 3}$ is the calibration matrix accounting for scale and misalignment errors, $b \\in \\mathbb{R}^3$ is the bias vector, and $\\epsilon_k$ is measurement noise. The true gravity vector is obtained by rotating the gravity vector from a fixed navigation frame, $g_0 = [0, 0, -g]^\\top$ (where $g=9.80665 \\, \\mathrm{m/s^2}$), using a known rotation matrix for each hold, $v_k = R_k g_0$. The objective is to estimate the $12$ parameters contained in $A$ and $b$.\n\nTo solve for $A$ and $b$, we formulate a linear least-squares problem. The model equation can be expanded for each component of the measurement vector $y_k = [y_{k,x}, y_{k,y}, y_{k,z}]^\\top$. Let the matrix $A$ have rows $a_1^\\top, a_2^\\top, a_3^\\top$ and the bias vector be $b=[b_1, b_2, b_3]^\\top$. The three scalar equations for measurement $k$ are:\n$$\ny_{k,x} = a_1^\\top v_k + b_1 \\\\\ny_{k,y} = a_2^\\top v_k + b_2 \\\\\ny_{k,z} = a_3^\\top v_k + b_3\n$$\nWe can observe that the unknowns are decoupled by row. The parameters $\\{a_1, b_1\\}$ only appear in the equation for $y_{k,x}$, and similarly for the other components. This allows us to set up three independent linear systems. For the first system, involving the x-channel measurements from all $N$ holds, we have:\n$$\n\\begin{bmatrix} y_{1,x} \\\\ y_{2,x} \\\\ \\vdots \\\\ y_{N,x} \\end{bmatrix} =\n\\begin{bmatrix}\nv_{1,x}  v_{1,y}  v_{1,z}  1 \\\\\nv_{2,x}  v_{2,y}  v_{2,z}  1 \\\\\n\\vdots  \\vdots  \\vdots  \\vdots \\\\\nv_{N,x}  v_{N,y}  v_{N,z}  1\n\\end{bmatrix}\n\\begin{bmatrix} a_{11} \\\\ a_{12} \\\\ a_{13} \\\\ b_1 \\end{bmatrix} +\n\\begin{bmatrix} \\epsilon_{1,x} \\\\ \\epsilon_{2,x} \\\\ \\vdots \\\\ \\epsilon_{N,x} \\end{bmatrix}\n$$\nThis can be written compactly as $Y_x = H x_1 + E_x$. The design matrix $H \\in \\mathbb{R}^{N \\times 4}$ is the same for all three channels. The parameter vectors for the other two channels are $x_2 = [a_{21}, a_{22}, a_{23}, b_2]^\\top$ and $x_3 = [a_{31}, a_{32}, a_{33}, b_3]^\\top$.\n\nThe total set of $12$ parameters is identifiable if and only if each of the three systems for $x_1, x_2, x_3$ yields a unique solution. This condition is met if and only if the design matrix $H$ has full column rank, i.e., $\\text{rank}(H) = 4$. Geometrically, this requires that the $N$ augmented vectors $[v_k^\\top, 1] \\in \\mathbb{R}^4$ span a $4$-dimensional space. This implies that the true gravity vectors $v_k$ must not be affinely dependent; specifically, they cannot all lie on a single plane or line. Since $\\|v_k\\|_2=g$ for all $k$, the vectors $v_k$ lie on a sphere of radius $g$. The condition $\\text{rank}(H)=4$ is thus equivalent to requiring at least four non-coplanar gravity vectors $v_k$.\n\nTo match the problem's requirement of checking a $12 \\times 12$ system, we can construct a global design matrix $W \\in \\mathbb{R}^{3N \\times 12}$ for a stacked parameter vector $\\theta = [x_1^\\top, x_2^\\top, x_3^\\top]^\\top$. This matrix is given by the Kronecker product $W = I_3 \\otimes H$, where $I_3$ is the $3 \\times 3$ identity matrix. The rank of this block-diagonal matrix is $\\text{rank}(W) = 3 \\times \\text{rank}(H)$. Consequently, the system is identifiable, with $\\text{rank}(W) = 12$, if and only if $\\text{rank}(H) = 4$.\n\nThe least-squares estimate for each parameter sub-vector $x_i$ is found by solving the normal equations, or more robustly, using a method like SVD decomposition as implemented in `numpy.linalg.lstsq`.\n$$\n\\hat{x_i} = (H^\\top H)^{-1} H^\\top Y_i, \\quad i \\in \\{x,y,z\\}\n$$\nThe estimated parameters $(\\hat{A}, \\hat{b})$ are then assembled from the solutions $\\hat{x_1}, \\hat{x_2}, \\hat{x_3}$. The estimation error is quantified using relative Frobenius and Euclidean norms.\n\n**Analysis of Test Cases:**\n*   **Test Case 1**: Provides $8$ orientations that are diverse and span three-dimensional space. The resulting $v_k$ vectors are non-coplanar, leading to $\\text{rank}(H)=4$ and $\\text{rank}(W)=12$. The problem is identifiable and well-conditioned, so the estimation errors are expected to be small and driven by the noise level $\\sigma$.\n*   **Test Case 2**: Uses a single orientation for all $6$ measurements. All $v_k$ are identical: $v_k = v_1$ for all $k$. The rows of $H$ are identical, so $\\text{rank}(H)=1$. This leads to $\\text{rank}(W)=3$. The system is severely underdetermined (not identifiable). The least-squares solution will correspond to a minimum-norm solution which is arbitrary and will not recover the true parameters.\n*   **Test Case 3**: Employs orientations that are rotations around the device's z-axis after a small, fixed tilt about the x-axis. The true gravity vectors are given by $v_k = R_z(\\psi_k) R_x(2^\\circ) g_0$. The z-component of each vector is $v_{k,z} = -g \\cos(2^\\circ)$, which is constant for all $k$. This means all $v_k$ vectors lie on the plane defined by $z = -g \\cos(2^\\circ)$. They are coplanar. Consequently, the columns of $H$ are linearly dependent, as $1 \\cdot v_{k,z} + (g \\cos(2^\\circ)) \\cdot 1 = 0$ for all $k$. The vector $[0, 0, 1, g \\cos(2^\\circ)]^\\top$ is in the null space of $H$. Therefore, $\\text{rank}(H)=3$, and $\\text{rank}(W)=9$. The system is not identifiable. This contradicts the problem's prose description of this case as \"identifiable but ill-conditioned\". The \"ill-conditioned\" nature arises because the singularity (coplanarity) is exact, making the condition number infinite. A truly \"identifiable but ill-conditioned\" case would arise if the vectors were *nearly* but not *perfectly* coplanar. The implementation will proceed based on the exact mathematical specification of the rotations, which leads to non-identifiability.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import norm, kron\n\ndef solve():\n    \"\"\"\n    Solves the accelerometer calibration problem for three test cases.\n    \"\"\"\n    # --- Problem Constants and True Parameters ---\n    g = 9.80665  # m/s^2\n    g0 = np.array([0., 0., -g])\n\n    M_true = np.array([\n        [1.0, 0.02, -0.01],\n        [0.01, 1.0, -0.015],\n        [-0.005, 0.02, 1.0]\n    ])\n    S_true_diag = np.array([1.03, 0.97, 0.99])\n    S_true = np.diag(S_true_diag)\n    A_true = M_true @ S_true\n    b_true = np.array([0.05, -0.03, 0.02])\n\n    rng = np.random.default_rng(42)\n\n    # --- Rotation Matrix Helper Functions (angles in degrees) ---\n    def Rx(theta_deg):\n        theta_rad = np.deg2rad(theta_deg)\n        c, s = np.cos(theta_rad), np.sin(theta_rad)\n        return np.array([[1., 0., 0.], [0., c, -s], [0., s, c]])\n\n    def Ry(theta_deg):\n        theta_rad = np.deg2rad(theta_deg)\n        c, s = np.cos(theta_rad), np.sin(theta_rad)\n        return np.array([[c, 0., s], [0., 1., 0.], [-s, 0., c]])\n\n    def Rz(theta_deg):\n        theta_rad = np.deg2rad(theta_deg)\n        c, s = np.cos(theta_rad), np.sin(theta_rad)\n        return np.array([[c, -s, 0.], [s, c, 0.], [0., 0., 1.]])\n\n    # --- Test Case Definitions ---\n    test_cases_params = [\n        {\n            \"sigma\": 0.005,\n            \"rotations\": [\n                np.identity(3),\n                Rx(90),\n                Rx(180),\n                Ry(90),\n                Rz(90),\n                Rx(45) @ Ry(30),\n                Ry(120) @ Rz(45),\n                Rx(210) @ Rz(135),\n            ]\n        },\n        {\n            \"sigma\": 0.005,\n            \"rotations\": [np.identity(3)] * 6\n        },\n        {\n            \"sigma\": 0.01,\n            \"rotations\": [Rz(psi) @ Rx(2) for psi in [0., 30., 60., 90., 120., 150.]]\n        }\n    ]\n\n    results = []\n\n    for params in test_cases_params:\n        rotations = params[\"rotations\"]\n        sigma = params[\"sigma\"]\n        num_measurements = len(rotations)\n\n        # 1. Generate true vectors and noisy measurements\n        v_k_list = [R @ g0 for R in rotations]\n        v_k_array = np.array(v_k_list)\n        \n        noise = rng.normal(0., sigma, size=(num_measurements, 3))\n        y_k_list = [(A_true @ v_k) + b_true + noise[k] for k, v_k in enumerate(v_k_list)]\n        y_k_array = np.array(y_k_list)\n\n        # 2. Construct the design matrix H for the decoupled systems\n        H = np.ones((num_measurements, 4))\n        H[:, :3] = v_k_array\n        \n        # Construct the full 3N x 12 matrix W to check global identifiability\n        W_full = kron(np.identity(3), H)\n\n        # 3. Check identifiability by computing the rank of the design matrix\n        rank_W = np.linalg.matrix_rank(W_full)\n        is_identifiable = (rank_W == 12)\n\n        # 4. Solve for A_hat and b_hat using numerically stable least squares\n        A_hat = np.zeros((3, 3))\n        b_hat = np.zeros(3)\n\n        # Solve for each row of A and element of b independently\n        yx = y_k_array[:, 0]\n        yy = y_k_array[:, 1]\n        yz = y_k_array[:, 2]\n\n        sol_x, _, _, _ = np.linalg.lstsq(H, yx, rcond=None)\n        sol_y, _, _, _ = np.linalg.lstsq(H, yy, rcond=None)\n        sol_z, _, _, _ = np.linalg.lstsq(H, yz, rcond=None)\n\n        A_hat[0, :] = sol_x[:3]\n        b_hat[0] = sol_x[3]\n        A_hat[1, :] = sol_y[:3]\n        b_hat[1] = sol_y[3]\n        A_hat[2, :] = sol_z[:3]\n        b_hat[2] = sol_z[3]\n\n        # 5. Compute relative estimation errors\n        norm_A_true = norm(A_true, 'fro')\n        rel_err_A = norm(A_hat - A_true, 'fro') / norm_A_true\n        \n        norm_b_true = norm(b_true)\n        # Avoid division by zero if true bias is zero\n        if norm_b_true > 1e-9:\n            rel_err_b = norm(b_hat - b_true) / norm_b_true\n        else:\n            rel_err_b = norm(b_hat - b_true)\n\n\n        results.extend([is_identifiable, rank_W, rel_err_A, rel_err_b])\n\n    # Format the final output string as specified\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "4557386"}, {"introduction": "Beyond physical activity, mobile sensors offer a powerful lens into our social lives. This practice guides you through a complete digital phenotyping pipeline, transforming raw Bluetooth proximity logs into a quantitative social activity phenotype. You will first construct a social network graph by applying principled thresholds to signal strength and contact duration, then compute fundamental node-level metrics like degree centrality and the local clustering coefficient [@problem_id:4557348]. The exercise culminates in creating a robust, standardized behavioral score, demonstrating how to handle proportional data and make individual metrics comparable across a cohort.", "problem": "You are given week-long proximity events collected passively by Bluetooth Low Energy (BLE) scanners on smartphones, represented as a set of undirected dyadic contact logs. Each event is a tuple of the form $(t_s,t_e,u,v,r)$ where $t_s$ is the contact start time in seconds, $t_e$ is the contact end time in seconds, $u$ and $v$ are user identifiers (integers), and $r$ is the Received Signal Strength Indicator (RSSI) in decibel-milliwatts (dBm). From these logs, you must infer a weekly undirected, unweighted, simple contact network $G=(V,E)$ and compute individual-level graph metrics to derive a social activity phenotype for each user with principled normalization.\n\nStarting from core definitions in graph theory and accepted modeling practice for proximity sensing, use the following foundational base:\n\n- Let $V$ be the set of unique user identifiers present in the logs. \n- For a fixed minimum proximity strength threshold $r_{\\min}$ and a minimum cumulative contact duration threshold $\\tau$ (both specified below), include an undirected edge $\\{u,v\\}$ in $E$ if and only if the sum of durations $\\sum_{(t_s,t_e,u,v,r)\\ \\text{or}\\ (t_s,t_e,v,u,r)} \\max(0, t_e - t_s)$ taken over all events for the unordered pair $\\{u,v\\}$ with $r \\ge r_{\\min}$ is at least $\\tau$ seconds. All times must be interpreted and computed in seconds, and the RSSI threshold is in dBm.\n- For each node $i \\in V$, let the (unnormalized) degree be $k_i = \\deg(i)$. The normalized degree centrality is defined as $\\hat d_i = \\frac{k_i}{|V|-1}$, which lies in $[0,1]$.\n- For each node $i \\in V$ with $k_i \\ge 2$, let $e_i$ denote the number of edges among the neighbors of $i$. The local clustering coefficient is $C_i = \\frac{2 e_i}{k_i(k_i-1)} \\in [0,1]$. For $k_i \\in \\{0,1\\}$, set $C_i = 0$.\n\nThe social activity phenotype for each individual must be constructed with normalization driven by distributional considerations of bounded proportions:\n\n- Apply the arcsine square-root variance-stabilizing transform to each proportion $p \\in [0,1]$: $z(p) = \\arcsin(\\sqrt{p})$, where the angle is in radians.\n- Define the composite score $S_i = \\frac{1}{2} z(\\hat d_i) + \\frac{1}{2} z(C_i)$ for each $i \\in V$.\n- Standardize $S_i$ within the cohort by the population mean and population standard deviation across all $i \\in V$: let $\\mu_S = \\frac{1}{|V|}\\sum_{i \\in V} S_i$ and $\\sigma_S = \\sqrt{\\frac{1}{|V|}\\sum_{i \\in V} (S_i - \\mu_S)^2}$. Define the standardized phenotype as $\\tilde S_i = \\frac{S_i - \\mu_S}{\\sigma_S}$ if $\\sigma_S  0$, and $\\tilde S_i = 0$ for all $i$ if $\\sigma_S = 0$.\n\nYour program must, for each test case below, construct $G$, compute $\\hat d_i$, $C_i$, $S_i$, and $\\tilde S_i$, and then return the pair consisting of:\n- the standardized phenotype $\\tilde S_{i^*}$ for the user $i^*$ with maximum degree (break ties by choosing the smallest user identifier),\n- the standardized phenotype $\\tilde S_{j^*}$ for the user $j^*$ with minimum degree (break ties by choosing the smallest user identifier).\n\nAll angles are in radians. All time differences must be computed in seconds. The final outputs must be real numbers, rounded to $6$ decimal places.\n\nUse the following fixed thresholds for all test cases: $r_{\\min} = -75$ dBm, $\\tau = 120$ seconds.\n\nTest suite:\n\n- Test case $1$ (happy path, mixed sparse structure):\n  - Users: $\\{0,1,2,3,4\\}$.\n  - Events $(t_s,t_e,u,v,r)$ with $t_s,t_e$ in seconds and $r$ in dBm:\n    - $(100,250,0,1,-65)$\n    - $(500,620,1,0,-70)$\n    - $(200,280,1,2,-80)$\n    - $(300,500,1,2,-72)$\n    - $(600,690,2,3,-68)$\n    - $(100,220,3,4,-60)$\n    - $(221,260,3,4,-60)$\n    - $(700,760,0,4,-74)$\n    - $(1000,1100,0,2,-73)$\n    - $(400,430,1,3,-77)$\n\n- Test case $2$ (boundary condition: complete graph):\n  - Users: $\\{0,1,2,3\\}$.\n  - Events:\n    - $(0,200,0,1,-60)$\n    - $(0,180,0,2,-70)$\n    - $(0,180,0,3,-70)$\n    - $(50,200,1,2,-70)$\n    - $(60,200,1,3,-71)$\n    - $(70,200,2,3,-73)$\n\n- Test case $3$ (nontrivial clustering among neighbors):\n  - Users: $\\{0,1,2,3\\}$.\n  - Events:\n    - $(0,200,0,1,-60)$\n    - $(10,150,0,2,-70)$\n    - $(300,450,0,3,-74)$\n    - $(80,210,1,2,-73)$\n    - $(100,150,1,3,-80)$\n    - $(200,210,1,3,-70)$\n    - $(500,560,2,3,-70)$\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\text{case }1\\ i^*, \\text{case }1\\ j^*, \\text{case }2\\ i^*, \\text{case }2\\ j^*, \\text{case }3\\ i^*, \\text{case }3\\ j^*]$, where each entry is the corresponding standardized phenotype value rounded to $6$ decimal places, for example, $[0.123456,-0.654321,0.000000,0.000000,0.281900,-1.706900]$.", "solution": "The user-provided problem statement has been critically validated and is deemed to be scientifically grounded, well-posed, and objective. It provides a clear, multi-step procedure for constructing a social network from sensor data and deriving a social phenotype, based on established principles of network science and statistics. All definitions, constants, and constraints are provided and are internally consistent. The problem requests a computational solution to a set of specified test cases.\n\nThe solution process involves several sequential stages: graph inference, metric computation, statistical transformation, and standardization. Each stage is guided by fundamental principles as detailed below.\n\n### 1. Graph Inference from Proximity Data\nThe foundational step is to derive a static, simple, unweighted, and undirected graph $G=(V, E)$ from a time series of dyadic proximity events. An event is recorded as $(t_s, t_e, u, v, r)$, representing a contact between users $u$ and $v$ from time $t_s$ to $t_e$ with a signal strength of $r$.\n\n- **Principle of Signal-Based Proximity Filtering**: Not all detected signals imply meaningful social contact. The Received Signal Strength Indicator (RSSI) is a proxy for distance, albeit a noisy one. By imposing a minimum RSSI threshold, $r \\ge r_{\\min}$, we filter out events likely corresponding to contacts at a large distance, which are less indicative of social interaction. For this problem, $r_{\\min} = -75$ dBm.\n\n- **Principle of Durational Significance**: Transient or fleeting contacts are often less socially significant than sustained ones. We aggregate the durations of all valid proximity events (i.e., those satisfying $r \\ge r_{\\min}$) for each pair of users $\\{u,v\\}$. An edge $\\{u,v\\}$ is included in the edge set $E$ if and only if this cumulative duration meets or exceeds a minimum threshold $\\tau$. This transforms temporal data into a static network structure representing stable social ties. For this problem, $\\tau = 120$ seconds.\nThe cumulative duration for a pair $\\{u, v\\}$ is given by:\n$$ T_{uv} = \\sum_{(t_s,t_e,u,v,r)\\ \\text{or}\\ (t_s,t_e,v,u,r), r \\ge r_{\\min}} \\max(0, t_e - t_s) $$\nAn edge $\\{u,v\\} \\in E$ if $T_{uv} \\ge \\tau$. The set of vertices $V$ comprises all unique users.\n\n### 2. Individual-Level Network Metrics\nOnce the graph $G$ is constructed, we characterize each user's position within the social network using standard graph-theoretic metrics.\n\n- **Degree Centrality**: The degree of a node, $k_i = \\deg(i)$, is the number of edges connected to it. It is the most direct measure of a node's social activity or connectivity. To make this comparable across networks of different sizes, it is normalized. The normalized degree centrality $\\hat d_i$ is defined as the fraction of other nodes in the network that user $i$ is connected to:\n$$ \\hat d_i = \\frac{k_i}{|V|-1} $$\nThis value lies in the range $[0, 1]$.\n\n- **Local Clustering Coefficient**: This metric, $C_i$, quantifies the social cohesion within a user's immediate neighborhood. It measures how close the neighbors of node $i$ are to being a clique (a fully connected subgraph). It is the ratio of the number of existing edges among the neighbors of $i$, denoted $e_i$, to the maximum possible number of such edges.\n$$ C_i = \\begin{cases} \\frac{2 e_i}{k_i(k_i-1)}  \\text{if } k_i \\ge 2 \\\\ 0  \\text{if } k_i \\in \\{0, 1\\} \\end{cases} $$\nThe condition for $k_i  2$ handles cases where a user has too few neighbors to form a triangle, for which the denominator would be zero or undefined. $C_i$ also lies in $[0, 1]$.\n\n### 3. Phenotype Construction and Standardization\nThe raw metrics $\\hat d_i$ and $C_i$ are proportions. To construct a robust composite score, we first apply a statistical transformation and then combine the metrics.\n\n- **Variance-Stabilizing Transformation**: Proportions often exhibit non-constant variance (variance depends on the mean), which can be problematic for statistical modeling. The arcsine square-root transformation, $z(p) = \\arcsin(\\sqrt{p})$, is a standard technique to stabilize the variance of data that are proportions. The angle is computed in radians.\n\n- **Composite Score**: A social activity phenotype, $S_i$, is defined as a linear combination of the transformed degree and clustering metrics. Here, we use a simple average, giving equal weight to an individual's direct connectivity and the cohesion of their local network:\n$$ S_i = \\frac{1}{2} z(\\hat d_i) + \\frac{1}{2} z(C_i) $$\n\n- **Standardization (Z-scoring)**: To interpret a user's score relative to the cohort, we standardize the $S_i$ values. This is achieved by computing the population mean $\\mu_S$ and population standard deviation $\\sigma_S$ of the $S_i$ scores across all users in $V$. The final standardized phenotype, $\\tilde S_i$, is the Z-score of $S_i$.\n$$ \\mu_S = \\frac{1}{|V|}\\sum_{i \\in V} S_i \\quad \\text{and} \\quad \\sigma_S = \\sqrt{\\frac{1}{|V|}\\sum_{i \\in V} (S_i - \\mu_S)^2} $$\n$$ \\tilde S_i = \\begin{cases} \\frac{S_i - \\mu_S}{\\sigma_S}  \\text{if } \\sigma_S  0 \\\\ 0  \\text{if } \\sigma_S = 0 \\end{cases} $$\nThe resulting $\\tilde S_i$ values represent how many standard deviations an individual's social activity score is from the cohort average. A value of $\\tilde S_i = 0$ indicates the individual is perfectly average, while positive and negative values indicate above-average and below-average social activity, respectively.\n\n### 4. Application to Test Case 1\nLet's illustrate the process for the first test case.\n- **Users**: $V = \\{0, 1, 2, 3, 4\\}$, so $|V|=5$.\n- **Graph Inference**: After filtering events with $r  -75$ dBm and summing durations, we find the following cumulative contacts exceed $\\tau = 120$s: $\\{0, 1\\}$ ($270$s), $\\{1, 2\\}$ ($200$s), and $\\{3, 4\\}$ ($159$s). The resulting edge set is $E = \\{\\{0, 1\\}, \\{1, 2\\}, \\{3, 4\\}\\}$.\n- **Metric Computation**:\n  - Degrees: $k_0=1, k_1=2, k_2=1, k_3=1, k_4=1$.\n  - Normalized Degrees ($\\hat d_i = k_i/4$): $\\hat d_0=0.25, \\hat d_1=0.5, \\hat d_2=0.25, \\hat d_3=0.25, \\hat d_4=0.25$.\n  - Clustering Coefficients: For user $1$, $k_1=2$, neighbors are $\\{0, 2\\}$. No edge exists between $0$ and $2$, so $e_1=0$ and $C_1=0$. All other users have $k_i2$, so their $C_i$ are also $0$. Thus, $C_i = 0$ for all $i \\in V$.\n- **Phenotype Construction**:\n  - $z(\\hat d_0) = \\arcsin(\\sqrt{0.25}) = \\pi/6$. $S_0 = 0.5(\\pi/6) = \\pi/12$. Similarly for users $2, 3, 4$.\n  - $z(\\hat d_1) = \\arcsin(\\sqrt{0.5}) = \\pi/4$. $S_1 = 0.5(\\pi/4) = \\pi/8$.\n- **Standardization**:\n  - Scores are $S = \\{\\pi/12, \\pi/8, \\pi/12, \\pi/12, \\pi/12\\}$.\n  - $\\mu_S = (4 \\cdot \\pi/12 + \\pi/8)/5 = 11\\pi/120$.\n  - $\\sigma_S = \\sqrt{(4(\\pi/12 - 11\\pi/120)^2 + (\\pi/8 - 11\\pi/120)^2)/5} = \\pi/60$.\n  - $\\tilde S_0 = (\\pi/12 - 11\\pi/120) / (\\pi/60) = -0.5$.\n  - $\\tilde S_1 = (\\pi/8 - 11\\pi/120) / (\\pi/60) = 2.0$.\n- **Final Selection**:\n  - Maximum degree is $k_1=2$, so $i^*=1$. Phenotype is $\\tilde S_1=2.0$.\n  - Minimum degree is $k=1$, for users $\\{0, 2, 3, 4\\}$. Tie-breaking by smallest ID gives $j^*=0$. Phenotype is $\\tilde S_0=-0.5$.\n\nThis complete, principled pipeline is implemented to solve for all test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import defaultdict\n\ndef solve():\n    \"\"\"\n    Solves the digital phenotyping problem for all test cases.\n    The overall process for each case is:\n    1. Filter BLE events by RSSI and aggregate cumulative contact durations.\n    2. Construct an undirected, simple graph based on a duration threshold.\n    3. For each user, calculate normalized degree and local clustering coefficient.\n    4. Apply arcsine square-root transform to these metrics.\n    5. Compute a composite score for each user.\n    6. Standardize the composite scores across the cohort.\n    7. Identify users with max/min degree and return their standardized scores.\n    \"\"\"\n    # Fixed thresholds as per the problem statement.\n    r_min = -75.0\n    tau = 120.0\n\n    # Test suite data.\n    test_cases = [\n        {\n            \"users\": {0, 1, 2, 3, 4},\n            \"events\": [\n                (100, 250, 0, 1, -65), (500, 620, 1, 0, -70),\n                (200, 280, 1, 2, -80), (300, 500, 1, 2, -72),\n                (600, 690, 2, 3, -68), (100, 220, 3, 4, -60),\n                (221, 260, 3, 4, -60), (700, 760, 0, 4, -74),\n                (1000, 1100, 0, 2, -73), (400, 430, 1, 3, -77),\n            ],\n        },\n        {\n            \"users\": {0, 1, 2, 3},\n            \"events\": [\n                (0, 200, 0, 1, -60), (0, 180, 0, 2, -70),\n                (0, 180, 0, 3, -70), (50, 200, 1, 2, -70),\n                (60, 200, 1, 3, -71), (70, 200, 2, 3, -73),\n            ],\n        },\n        {\n            \"users\": {0, 1, 2, 3},\n            \"events\": [\n                (0, 200, 0, 1, -60), (10, 150, 0, 2, -70),\n                (300, 450, 0, 3, -74), (80, 210, 1, 2, -73),\n                (100, 150, 1, 3, -80), (200, 210, 1, 3, -70),\n                (500, 560, 2, 3, -70),\n            ],\n        },\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        users = sorted(list(case[\"users\"]))\n        events = case[\"events\"]\n        V_size = len(users)\n\n        # Step 1: Aggregate contact durations for pairs of users.\n        durations = defaultdict(float)\n        for ts, te, u, v, r in events:\n            if r >= r_min:\n                duration = max(0.0, float(te - ts))\n                # Normalize pair order for undirected graph\n                key = tuple(sorted((u, v)))\n                durations[key] += duration\n\n        # Step 2: Construct the graph (adjacency list and edge set).\n        adj = defaultdict(set)\n        edges = set()\n        for (u, v), total_duration in durations.items():\n            if total_duration >= tau:\n                adj[u].add(v)\n                adj[v].add(u)\n                edges.add(tuple(sorted((u, v))))\n\n        # Step 3: Compute graph metrics for each user.\n        degrees = {}\n        norm_degrees = {}\n        clustering_coeffs = {}\n\n        for user in users:\n            # Degree k_i\n            k_i = len(adj[user])\n            degrees[user] = k_i\n            \n            # Normalized Degree d_i\n            if V_size > 1:\n                norm_degrees[user] = k_i / (V_size - 1)\n            else:\n                norm_degrees[user] = 0.0\n\n            # Local Clustering Coefficient C_i\n            if k_i  2:\n                clustering_coeffs[user] = 0.0\n            else:\n                neighbors = list(adj[user])\n                num_neighbor_edges = 0\n                # Count edges between pairs of neighbors.\n                for i in range(len(neighbors)):\n                    for j in range(i + 1, len(neighbors)):\n                        if tuple(sorted((neighbors[i], neighbors[j]))) in edges:\n                            num_neighbor_edges += 1\n                clustering_coeffs[user] = (2.0 * num_neighbor_edges) / (k_i * (k_i - 1))\n        \n        # Step 4: Apply variance-stabilizing transform.\n        def z(p):\n            # Arcsine square-root transform\n            return np.arcsin(np.sqrt(p))\n            \n        # Step 5: Compute composite scores.\n        composite_scores = {}\n        for user in users:\n            score_d = z(norm_degrees[user])\n            score_C = z(clustering_coeffs[user])\n            composite_scores[user] = 0.5 * score_d + 0.5 * score_C\n\n        # Step 6: Standardize scores.\n        score_values = np.array([composite_scores[user] for user in users])\n        mean_S = np.mean(score_values)\n        std_S = np.std(score_values)  # Uses N in denominator (population std dev)\n\n        standardized_scores = {}\n        if std_S > 1e-9: # Use a small threshold for floating point comparison\n            for user in users:\n                standardized_scores[user] = (composite_scores[user] - mean_S) / std_S\n        else:\n            for user in users:\n                standardized_scores[user] = 0.0\n\n        # Step 7: Identify i* (max degree) and j* (min degree).\n        max_degree = -1\n        i_star = -1\n        # Iterating through sorted users ensures smallest ID is chosen on tie.\n        for user in users:\n            if degrees[user] > max_degree:\n                max_degree = degrees[user]\n                i_star = user\n        \n        min_degree = float('inf')\n        j_star = -1\n        for user in users:\n            if degrees[user]  min_degree:\n                min_degree = degrees[user]\n                j_star = user\n        \n        final_results.append(standardized_scores[i_star])\n        final_results.append(standardized_scores[j_star])\n        \n    print(f\"[{','.join([f'{x:.6f}' for x in final_results])}]\")\n\nsolve()\n```", "id": "4557348"}, {"introduction": "A persistent challenge in multimodal sensing is handling missing data, which can occur when sensors fail, batteries die, or participants forget to wear a device. This exercise moves beyond simple imputation methods to a powerful matrix completion framework based on the assumption of a latent low-rank structure in behavioral data. You will tackle the problem of imputing entire missing modalities by formulating and solving a convex optimization problem that leverages the nuclear norm as a proxy for rank [@problem_id:4557361]. By implementing a proximal gradient algorithm with singular value thresholding, you will gain hands-on experience with a state-of-the-art technique for recovering a complete data picture from sparse and incomplete observations.", "problem": "Consider a multimodal digital phenotyping study where participant-level features from mobile and wearable sensors are aggregated into a rectangular data matrix. Let $X^\\star \\in \\mathbb{R}^{n \\times p}$ denote the latent, noise-free feature matrix where each column corresponds to a feature from a specific modality block (for example, accelerometry, heart rate, sleep). In realistic deployments, entire modalities can be missing for certain periods due to device unavailability, and additional random entries may be missing due to asynchronous sampling and transmission failures. Assume a low-rank latent structure generated by a small number of behavioral factors, so that $X^\\star$ is well-approximated by a product of two low-dimensional matrices.\n\nYou are given a noisy measurement matrix $M \\in \\mathbb{R}^{n \\times p}$ and a binary mask operator $P_\\Omega:\\mathbb{R}^{n \\times p}\\rightarrow\\mathbb{R}^{n \\times p}$ that keeps the observed entries and zeros out the missing entries. The observed measurements satisfy\n$$\nM \\;=\\; X^\\star \\;+\\; E,\n$$\nwhere $E$ is additive noise with independent and identically distributed entries. The set of observed indices is represented by $\\Omega \\subseteq \\{1,\\dots,n\\}\\times\\{1,\\dots,p\\}$, and $P_\\Omega$ is defined by\n$$\n\\left[P_\\Omega(X)\\right]_{ij} \\;=\\;\n\\begin{cases}\nX_{ij},  (i,j)\\in\\Omega,\\\\\n0,  (i,j)\\notin\\Omega.\n\\end{cases}\n$$\n\nAssume block-missing modalities: for held-out modality blocks, all entries in the corresponding columns are removed from $\\Omega$ (thus they are unobserved during reconstruction) and used only for evaluation. The reconstruction is posed as a convex optimization problem using a nuclear norm penalty (the nuclear norm is the sum of singular values):\n$$\n\\min_{X\\in\\mathbb{R}^{n\\times p}} \\;\\; \\frac{1}{2}\\,\\big\\|P_\\Omega(X - M)\\big\\|_F^2 \\;+\\; \\lambda\\,\\|X\\|_*,\n$$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm, $\\|\\cdot\\|_*$ denotes the nuclear norm, and $\\lambda0$ is a regularization parameter.\n\nYour task is to implement a solver based on proximal gradient iterations with singular value thresholding to approximately minimize the above objective, and then assess the reconstruction quality on held-out sensors (entire column blocks that were removed from $\\Omega$). Use the Normalized Root Mean Squared Error (NRMSE) restricted to held-out columns:\n$$\n\\mathrm{NRMSE} \\;=\\; \\frac{\\sqrt{\\sum_{(i,j)\\in\\Theta} \\left(X_{ij}-X^\\star_{ij}\\right)^2}}{\\sqrt{\\sum_{(i,j)\\in\\Theta} \\left(X^\\star_{ij}\\right)^2}},\n$$\nwhere $\\Theta$ indexes all entries of the held-out columns. This quantity is unitless and must be reported as a decimal.\n\nFundamental base to assume in your derivation and implementation:\n- The low-rank latent factor model $X^\\star \\approx U V^\\top$ with $U\\in\\mathbb{R}^{n\\times r}$ and $V\\in\\mathbb{R}^{p\\times r}$ for a small $r$.\n- Properties of orthogonal projections for $P_\\Omega$, namely $P_\\Omega^2 = P_\\Omega$ and $\\|P_\\Omega\\|_2 = 1$.\n- The proximal operator of the nuclear norm is singular value soft-thresholding.\n\nImplement a complete program that:\n- Synthesizes $X^\\star$ by sampling $U$ and $V$ with entries from a standard normal distribution, forms $X^\\star = U V^\\top$, and scales columns within modality blocks by modality-specific positive scalars to reflect heterogeneous sensor amplitudes.\n- Adds independent Gaussian noise with standard deviation $\\sigma$ to form $M$.\n- Constructs the observation mask $P_\\Omega$ by removing all entries from specified held-out modality blocks and further removing a random fraction of entries from the remaining columns.\n- Solves the nuclear norm-regularized objective using proximal gradient iterations with step size $\\mu$ and singular value soft-thresholding with threshold $\\mu\\lambda$, using acceleration if desired. Initialize with the zero matrix or the observed data and iterate until reaching a maximum number of iterations or a tolerance on the relative Frobenius change.\n- Computes the NRMSE on held-out modality columns only.\n\nTest suite. Your program must run the following three test cases, each defined by a tuple of parameters $(n,p,r,\\sigma,\\lambda,\\mu,\\mathrm{max\\_iters},\\mathrm{tol},\\mathrm{seed},\\mathrm{block\\_sizes},\\mathrm{held\\_out\\_blocks},\\mathrm{missing\\_frac})$, with modality blocks defined by contiguous column sizes in $\\mathrm{block\\_sizes}$ and indices in $\\mathrm{held\\_out\\_blocks}$ referring to block positions starting at $0$.\n\n- Case $1$ (general case, block-missing with moderate random missingness):\n  - $(n,p,r) = (\\,48,\\,30,\\,3\\,)$,\n  - $\\sigma = 0.05$,\n  - $\\lambda = 0.8$,\n  - $\\mu = 1.0$,\n  - $\\mathrm{max\\_iters} = 500$,\n  - $\\mathrm{tol} = 10^{-6}$,\n  - $\\mathrm{seed} = 42$,\n  - $\\mathrm{block\\_sizes} = [\\,10,\\,10,\\,10\\,]$,\n  - $\\mathrm{held\\_out\\_blocks} = \\{\\,1\\,\\}$,\n  - $\\mathrm{missing\\_frac} = 0.25$ on non-held-out columns.\n\n- Case $2$ (boundary case, noiseless with fully observed training modalities):\n  - $(n,p,r) = (\\,48,\\,30,\\,2\\,)$,\n  - $\\sigma = 0.0$,\n  - $\\lambda = 0.1$,\n  - $\\mu = 1.0$,\n  - $\\mathrm{max\\_iters} = 500$,\n  - $\\mathrm{tol} = 10^{-7}$,\n  - $\\mathrm{seed} = 7$,\n  - $\\mathrm{block\\_sizes} = [\\,10,\\,10,\\,10\\,]$,\n  - $\\mathrm{held\\_out\\_blocks} = \\{\\,2\\,\\}$,\n  - $\\mathrm{missing\\_frac} = 0.0$ on non-held-out columns.\n\n- Case $3$ (edge case, severe missingness and multiple held-out blocks):\n  - $(n,p,r) = (\\,60,\\,36,\\,2\\,)$,\n  - $\\sigma = 0.1$,\n  - $\\lambda = 1.0$,\n  - $\\mu = 1.0$,\n  - $\\mathrm{max\\_iters} = 600$,\n  - $\\mathrm{tol} = 10^{-6}$,\n  - $\\mathrm{seed} = 123$,\n  - $\\mathrm{block\\_sizes} = [\\,12,\\,12,\\,12\\,]$,\n  - $\\mathrm{held\\_out\\_blocks} = \\{\\,0,\\,2\\,\\}$,\n  - $\\mathrm{missing\\_frac} = 0.6$ on non-held-out columns.\n\nFinal output format. Your program should produce a single line of output containing the NRMSE results for the three cases, in order, as a comma-separated list enclosed in square brackets. Each value must be rounded to $4$ decimal places, for example, $\\left[\\,0.1234,0.0000,0.5678\\,\\right]$. No additional text should be printed. The NRMSE is unitless and must be expressed as a decimal in the specified format.", "solution": "The user-provided problem is assessed as **valid** following a rigorous validation process. All provided data and conditions are scientifically sound, consistent, and well-posed. The problem is a standard, albeit complex, implementation task in the field of machine learning and signal processing, specifically low-rank matrix completion, with a direct and realistic application to digital phenotyping data.\n\nHerein, a complete solution is provided, detailing the theoretical foundation, algorithmic design, and implementation strategy.\n\n**1. Problem Formulation and Objective Function**\n\nThe core of the problem is to reconstruct a low-rank matrix $X^\\star \\in \\mathbb{R}^{n \\times p}$ from a noisy and incomplete observation matrix $M \\in \\mathbb{R}^{n \\times p}$. The relationship between the latent true matrix and the observation is given by $M = X^\\star + E$, where $E$ is a matrix of i.i.d. noise. Observations are available only for indices $(i,j)$ in a set $\\Omega$. This is formulated as the following convex optimization problem:\n$$\n\\min_{X\\in\\mathbb{R}^{n\\times p}} \\;\\; \\frac{1}{2}\\,\\big\\|P_\\Omega(X - M)\\big\\|_F^2 \\;+\\; \\lambda\\,\\|X\\|_*\n$$\nThis objective function is composite, consisting of two terms. The first term, $f(X) = \\frac{1}{2}\\,\\big\\|P_\\Omega(X - M)\\big\\|_F^2$, is a data fidelity term that penalizes the squared Frobenius norm of the difference between the estimate $X$ and the measurement $M$ on the observed entries. This function is convex and differentiable. The second term, $g(X) = \\lambda\\,\\|X\\|_*$, is a regularization term. Here, $\\|X\\|_* = \\sum_i \\sigma_i(X)$ is the nuclear norm (the sum of the singular values of $X$), which is a convex proxy for the rank of the matrix. This term encourages the solution $X$ to be of low rank. The parameter $\\lambda  0$ controls the trade-off between data fidelity and low-rank structure.\n\n**2. Algorithm: Proximal Gradient Method**\n\nThe objective function has the structure $\\min_X f(X) + g(X)$, where $f$ is smooth and $g$ is convex but non-differentiable (due to the absolute value function implicit in the singular values). This structure is ideally suited for a proximal gradient algorithm. The iterative update rule for this method is:\n$$\nX_{k+1} = \\mathrm{prox}_{\\mu g}\\left(X_k - \\mu \\nabla f(X_k)\\right)\n$$\nwhere $k$ is the iteration index, $\\mu  0$ is the step size, and $\\mathrm{prox}_{\\mu g}$ is the proximal operator of the function $\\mu g$.\n\n**Gradient Step**: The gradient of the smooth term $f(X)$ is required. Using matrix calculus and the property that the projection $P_\\Omega$ is self-adjoint and idempotent ($P_\\Omega = P_\\Omega^\\top$, $P_\\Omega^2=P_\\Omega$), the gradient is:\n$$\n\\nabla f(X) = P_\\Omega\\left(P_\\Omega(X) - P_\\Omega(M)\\right) = P_\\Omega(X - M)\n$$\nThe gradient descent step updates the current estimate $X_k$ by moving in the negative gradient direction:\n$$\nY_k = X_k - \\mu \\nabla f(X_k) = X_k - \\mu P_\\Omega(X_k - M)\n$$\nFor convergence, the step size $\\mu$ must be less than or equal to $1/L$, where $L$ is the Lipschitz constant of $\\nabla f$. For $\\nabla f(X) = P_\\Omega(X-M)$, the Lipschitz constant is $\\|P_\\Omega\\|_2^2=1$. The problem specifies $\\mu=1$, which satisfies this condition.\n\n**Proximal Step**: The proximal operator of the nuclear norm, $\\mathrm{prox}_{\\tau\\|\\cdot\\|_*}(Y)$, is the singular value thresholding (SVT) operator, denoted $D_\\tau(Y)$. If the Singular Value Decomposition (SVD) of $Y$ is $Y = U \\Sigma V^\\top$, where $\\Sigma = \\mathrm{diag}(\\sigma_1, \\sigma_2, \\dots)$, then the SVT operator is defined as:\n$$\nD_\\tau(Y) = U \\Sigma_\\tau V^\\top \\quad \\text{with} \\quad [\\Sigma_\\tau]_{ii} = \\max(\\sigma_i - \\tau, 0)\n$$\nIn our case, the function is $g(X) = \\lambda \\|X\\|_*$, so the proximal operator is applied with a threshold of $\\tau = \\mu\\lambda$. The full update for the estimate $X$ at iteration $k+1$ becomes:\n$$\nX_{k+1} = D_{\\mu\\lambda}(Y_k) = D_{\\mu\\lambda}\\left(X_k - \\mu P_\\Omega(X_k - M)\\right)\n$$\nThe algorithm is initialized with $X_0 = 0$ and iterates until a maximum number of iterations is reached or the relative change in the Frobenius norm of $X$, $\\|X_{k+1}-X_k\\|_F / \\|X_k\\|_F$, falls below a specified tolerance $\\mathrm{tol}$.\n\n**3. Data Simulation Pipeline**\n\nTo test the algorithm, a data generation pipeline is implemented as specified:\n- **Latent Matrix ($X^\\star$)**: Two matrices, $U \\in \\mathbb{R}^{n \\times r}$ and $V \\in \\mathbb{R}^{p \\times r}$, are sampled from a standard normal distribution. The latent low-rank matrix is formed as $X^\\star = UV^\\top$. To simulate varying sensor scales, columns of $X^\\star$ corresponding to different modality blocks are multiplied by distinct positive scalars, which are sampled from a uniform distribution $\\mathcal{U}(0.5, 1.5)$.\n- **Measurement Matrix ($M$)**: An i.i.d. Gaussian noise matrix $E \\in \\mathbb{R}^{n \\times p}$ with zero mean and standard deviation $\\sigma$ is generated and added to the latent matrix to produce the noisy measurement matrix $M = X^\\star + E$.\n- **Observation and Evaluation Masks**: Two boolean masks are generated. The evaluation mask, $\\Theta$, identifies the columns designated as `held_out_blocks`. The observation mask, $\\Omega$, is its complement, which is then further downsampled by randomly setting a fraction `missing_frac` of its `True` entries to `False`. The projection operator $P_\\Omega(A)$ is implemented as element-wise multiplication of matrix $A$ with the binary mask corresponding to $\\Omega$.\n\nA single random number generator, seeded for each test case, ensures the entire process is deterministic and reproducible.\n\n**4. Performance Evaluation**\n\nThe quality of the reconstructed matrix, $\\hat{X}$, is evaluated using the Normalized Root Mean Squared Error (NRMSE) on the held-out columns, indexed by the set $\\Theta$. The NRMSE is defined as:\n$$\n\\mathrm{NRMSE} = \\frac{\\|P_\\Theta(\\hat{X} - X^\\star)\\|_F}{\\|P_\\Theta(X^\\star)\\|_F} = \\frac{\\sqrt{\\sum_{(i,j)\\in\\Theta} \\left(\\hat{X}_{ij}-X^\\star_{ij}\\right)^2}}{\\sqrt{\\sum_{(i,j)\\in\\Theta} \\left(X^\\star_{ij}\\right)^2}}\n$$\nThis metric quantifies the reconstruction error relative to the magnitude of the true signal in the unobserved modalities, providing a scale-independent measure of performance.\n\n**5. Implementation Summary**\n\nThe solution is encapsulated in a single Python program. A main function iterates through the specified test cases. For each case, it initializes a seeded random number generator and calls helper functions to generate the data and masks. It then runs the proximal gradient solver to obtain the reconstructed matrix. Finally, it computes the NRMSE on the held-out columns and stores the result. After processing all cases, the program prints the NRMSE values as a comma-separated list, formatted to four decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the matrix completion problem for all test cases.\n    \"\"\"\n\n    def generate_data(n, p, r, sigma, block_sizes, rng):\n        \"\"\"\n        Generates the latent low-rank matrix X_star and its noisy observation M.\n        \"\"\"\n        U = rng.standard_normal(size=(n, r))\n        V = rng.standard_normal(size=(p, r))\n        X_star_unscaled = U @ V.T\n        X_star = np.copy(X_star_unscaled)\n\n        # Scale columns by modality blocks to simulate heterogeneous sensor amplitudes\n        num_blocks = len(block_sizes)\n        scales = rng.uniform(0.5, 1.5, size=num_blocks)\n        \n        col_idx = 0\n        for i in range(num_blocks):\n            block_size = block_sizes[i]\n            X_star[:, col_idx : col_idx + block_size] *= scales[i]\n            col_idx += block_size\n            \n        # Add Gaussian noise\n        noise = rng.normal(0, sigma, size=(n, p))\n        M = X_star + noise\n        \n        return X_star, M\n\n    def create_masks(n, p, block_sizes, held_out_blocks, missing_frac, rng):\n        \"\"\"\n        Creates the observation mask (omega_mask) and evaluation mask (theta_mask).\n        \"\"\"\n        # theta_mask identifies held-out columns for evaluation\n        theta_mask = np.zeros((n, p), dtype=bool)\n        \n        block_col_indices = []\n        col_idx = 0\n        for size in block_sizes:\n            block_col_indices.append(list(range(col_idx, col_idx + size)))\n            col_idx += size\n            \n        held_out_cols = []\n        for block_idx in held_out_blocks:\n            held_out_cols.extend(block_col_indices[block_idx])\n            \n        if held_out_cols:\n            theta_mask[:, held_out_cols] = True\n        \n        # omega_mask identifies observed entries for training\n        omega_mask = np.ones((n, p), dtype=bool)\n        if held_out_cols:\n            omega_mask[:, held_out_cols] = False\n        \n        # Introduce additional random missingness in non-held-out columns\n        if missing_frac > 0.0:\n            training_cols_mask = ~theta_mask[0, :]\n            training_cols_indices = np.where(training_cols_mask)[0]\n            \n            if len(training_cols_indices) > 0:\n                n_training_entries = n * len(training_cols_indices)\n                n_to_remove = int(round(n_training_entries * missing_frac))\n                \n                # Get all (row, col) pairs in training columns\n                row_indices_grid, col_indices_grid = np.meshgrid(\n                    np.arange(n), training_cols_indices, indexing='ij'\n                )\n                flat_row_indices = row_indices_grid.flatten()\n                flat_col_indices = col_indices_grid.flatten()\n                \n                # Randomly choose indices to set to False\n                indices_to_remove = rng.choice(\n                    n_training_entries, size=n_to_remove, replace=False\n                )\n                \n                rows_to_remove = flat_row_indices[indices_to_remove]\n                cols_to_remove = flat_col_indices[indices_to_remove]\n                \n                omega_mask[rows_to_remove, cols_to_remove] = False\n                \n        return omega_mask, theta_mask\n\n    def singular_value_thresholding(Y, threshold):\n        \"\"\"\n        Performs the singular value thresholding operation.\n        \"\"\"\n        U, s, Vt = np.linalg.svd(Y, full_matrices=False)\n        s_thresh = np.maximum(s - threshold, 0)\n        return (U * s_thresh) @ Vt\n\n    def matrix_completion_solver(M, omega_mask, lambda_val, mu, max_iters, tol):\n        \"\"\"\n        Solves the nuclear norm regularized matrix completion problem using\n        proximal gradient descent.\n        \"\"\"\n        n, p = M.shape\n        X = np.zeros((n, p))\n        \n        for _ in range(max_iters):\n            X_prev = np.copy(X)\n            \n            # Gradient descent step\n            grad = (X - M) * omega_mask\n            Y = X - mu * grad\n            \n            # Proximal mapping step (SVT)\n            threshold = mu * lambda_val\n            X = singular_value_thresholding(Y, threshold)\n            \n            # Check for convergence\n            change = np.linalg.norm(X - X_prev, 'fro')\n            norm_prev = np.linalg.norm(X_prev, 'fro')\n            if norm_prev > 0:\n                rel_change = change / norm_prev\n            else: # Handle case X_prev is the zero matrix\n                rel_change = change / (1e-9)\n\n            if rel_change  tol:\n                break\n                \n        return X\n\n    def calculate_nrmse(X_recon, X_star, theta_mask):\n        \"\"\"\n        Calculates the Normalized Root Mean Squared Error on held-out columns.\n        \"\"\"\n        # Use boolean indexing to select elements from held-out columns\n        diff_held_out = (X_recon - X_star)[theta_mask]\n        true_held_out = X_star[theta_mask]\n        \n        numerator = np.linalg.norm(diff_held_out)\n        denominator = np.linalg.norm(true_held_out)\n        \n        if denominator == 0:\n            return 0.0 if numerator == 0.0 else np.inf\n            \n        return numerator / denominator\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (48, 30, 3, 0.05, 0.8, 1.0, 500, 1e-6, 42, [10, 10, 10], {1}, 0.25),\n        # Case 2\n        (48, 30, 2, 0.0, 0.1, 1.0, 500, 1e-7, 7, [10, 10, 10], {2}, 0.0),\n        # Case 3\n        (60, 36, 2, 0.1, 1.0, 1.0, 600, 1e-6, 123, [12, 12, 12], {0, 2}, 0.6)\n    ]\n\n    results = []\n    for params in test_cases:\n        (n, p, r, sigma, lambda_val, mu, max_iters, tol, seed, \n         block_sizes, held_out_blocks, missing_frac) = params\n\n        # Seed a random number generator for reproducibility of the entire case\n        rng = np.random.default_rng(seed)\n\n        # Generate data and masks\n        X_star, M = generate_data(n, p, r, sigma, block_sizes, rng)\n        omega_mask, theta_mask = create_masks(n, p, block_sizes, held_out_blocks, missing_frac, rng)\n        \n        # Solve for the reconstructed matrix\n        X_recon = matrix_completion_solver(M, omega_mask, lambda_val, mu, max_iters, tol)\n        \n        # Calculate NRMSE on the held-out data\n        nrmse = calculate_nrmse(X_recon, X_star, theta_mask)\n        results.append(nrmse)\n\n    # Format and print the final output as specified.\n    formatted_results = [f\"{res:.4f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4557361"}]}