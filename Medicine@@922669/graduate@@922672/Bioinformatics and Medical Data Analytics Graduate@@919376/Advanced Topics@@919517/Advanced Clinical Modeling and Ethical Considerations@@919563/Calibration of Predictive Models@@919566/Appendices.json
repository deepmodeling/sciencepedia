{"hands_on_practices": [{"introduction": "The first step in evaluating a predictive model is often to assess for systematic bias. This exercise introduces the concept of \"calibration-in-the-large,\" which provides a global summary of a model's calibration by comparing its average predicted risk to the observed event rate in a population [@problem_id:4544791]. By deriving and computing the calibration intercept on the log-odds scale, you will gain a foundational understanding of how to quantify and interpret systematic over- or underestimation.", "problem": "An Electronic Health Record (EHR) based binary disease risk model outputs predicted probabilities $\\hat{p}_{i}$ for patients $i=1,\\dots,n$. In a held-out cohort, the predictions are tightly clustered, with a narrow distribution centered near $0.9$ such that the mean predicted probability can be taken as $\\bar{p}\\approx 0.9$. The empirically observed disease prevalence in the same cohort is $\\hat{\\pi}=0.7$. Consider calibration assessed under the logistic calibration-in-the-large framework, which evaluates systematic miscalibration as a constant shift on the log-odds scale between predicted probabilities and observed outcomes.\n\nStarting only from the fundamental definitions of probability, prevalence, and the logit function $\\operatorname{logit}(x)=\\ln\\!\\left(\\frac{x}{1-x}\\right)$, derive the signed calibration-in-the-large intercept $a$ that would be added to $\\operatorname{logit}(\\hat{p}_{i})$ to align the average predicted log-odds with the observed log-odds. Interpret the sign of $a$ as the direction of miscalibration (negative indicates overestimation on the probability scale; positive indicates underestimation). Compute $a$ using the given cohort summary and report its numerical value. Round your answer to four significant figures.", "solution": "The problem requires the derivation and computation of the calibration-in-the-large intercept, denoted by $a$, for a binary disease risk model. The derivation must proceed from fundamental definitions.\n\nFirst, we validate the problem statement.\nThe givens are:\n- A set of predicted probabilities $\\hat{p}_{i}$ for patients $i=1,\\dots,n$.\n- The mean predicted probability is $\\bar{p}\\approx 0.9$.\n- The empirically observed disease prevalence is $\\hat{\\pi}=0.7$.\n- The logistic calibration framework applies a constant shift, $a$, on the log-odds scale.\n- The logit function is defined as $\\operatorname{logit}(x)=\\ln\\!\\left(\\frac{x}{1-x}\\right)$.\n- The goal is to find $a$ such that the average predicted log-odds is aligned with the observed log-odds.\n\nThe problem is scientifically grounded in the field of model calibration in biostatistics. The concepts of prevalence, predicted probability, log-odds, and calibration intercept are standard. The provided values, where average prediction ($\\bar{p}=0.9$) exceeds observed prevalence ($\\hat{\\pi}=0.7$), represent a common and realistic scenario of model overestimation. The problem is well-posed, objective, and contains sufficient information for a unique solution under a reasonable interpretation of \"aligning average predicted log-odds\". Therefore, the problem is deemed valid.\n\nWe now proceed with the derivation.\n\n1.  **Fundamental Definitions**\n\nThe empirically observed disease prevalence, $\\hat{\\pi}$, is the mean of the binary outcomes $y_i \\in \\{0, 1\\}$ in the cohort:\n$$ \\hat{\\pi} = \\frac{1}{n} \\sum_{i=1}^{n} y_i $$\nThis represents the average observed risk in the population.\n\nThe mean predicted probability, $\\bar{p}$, is the average of the model's predictions:\n$$ \\bar{p} = \\frac{1}{n} \\sum_{i=1}^{n} \\hat{p}_i $$\n\nThe logit function, which maps a probability $p \\in (0, 1)$ to the log-odds scale $(-\\infty, \\infty)$, is given by:\n$$ \\operatorname{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right) $$\n\n2.  **Calibration-in-the-Large Framework**\n\nThe logistic calibration framework aims to correct systematic miscalibration by applying a constant shift $a$ (the calibration intercept) to the logit-transformed predictions. A calibrated probability, $p'_i$, is obtained from the original prediction $\\hat{p}_i$ via the relation:\n$$ \\operatorname{logit}(p'_i) = a + \\operatorname{logit}(\\hat{p}_i) $$\n\nThe problem specifies that the purpose of $a$ is to align the \"average predicted log-odds\" with the \"observed log-odds\". In the context of calibration-in-the-large using summary statistics, this is interpreted as ensuring that the log-odds corresponding to the mean prediction, when corrected by $a$, equals the log-odds of the observed prevalence.\n\nThe \"observed log-odds\" for the cohort is naturally defined by the logit of the overall prevalence, $\\operatorname{logit}(\\hat{\\pi})$.\nThe \"average predicted log-odds\" for the cohort is represented by the logit of the mean prediction, $\\operatorname{logit}(\\bar{p})$. This is a standard simplification, justified here by the given information that predictions are \"tightly clustered\", which implies that $\\operatorname{logit}(\\bar{p}) \\approx \\frac{1}{n}\\sum\\operatorname{logit}(\\hat{p}_i)$.\n\n3.  **Derivation of the Intercept $a$**\n\nThe alignment condition can be expressed as:\n$$ \\text{Observed Log-Odds} = a + \\text{Average Predicted Log-Odds} $$\nSubstituting the expressions defined above:\n$$ \\operatorname{logit}(\\hat{\\pi}) = a + \\operatorname{logit}(\\bar{p}) $$\nSolving for the calibration intercept $a$, we get:\n$$ a = \\operatorname{logit}(\\hat{\\pi}) - \\operatorname{logit}(\\bar{p}) $$\nNow, we substitute the definition of the logit function:\n$$ a = \\ln\\left(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\right) - \\ln\\left(\\frac{\\bar{p}}{1-\\bar{p}}\\right) $$\nUsing the property of logarithms, $\\ln(x) - \\ln(y) = \\ln(x/y)$, this can be written as:\n$$ a = \\ln\\left( \\frac{\\hat{\\pi}/(1-\\hat{\\pi})}{\\bar{p}/(1-\\bar{p})} \\right) $$\n\n4.  **Numerical Computation**\n\nWe are given $\\hat{\\pi} = 0.7$ and $\\bar{p} \\approx 0.9$. Substituting these values into the expression for $a$:\n$$ a = \\ln\\left(\\frac{0.7}{1-0.7}\\right) - \\ln\\left(\\frac{0.9}{1-0.9}\\right) $$\n$$ a = \\ln\\left(\\frac{0.7}{0.3}\\right) - \\ln\\left(\\frac{0.9}{0.1}\\right) $$\n$$ a = \\ln\\left(\\frac{7}{3}\\right) - \\ln(9) $$\nWe compute the numerical values of the terms:\n$$ \\ln\\left(\\frac{7}{3}\\right) \\approx 0.84729786 $$\n$$ \\ln(9) \\approx 2.19722458 $$\nThus, the intercept $a$ is:\n$$ a \\approx 0.84729786 - 2.19722458 $$\n$$ a \\approx -1.34992672 $$\nRounding to four significant figures, we get $a = -1.350$.\n\n5.  **Interpretation of the Sign**\n\nThe calculated intercept $a$ is negative. Let's analyze its implication. The calibrated log-odds are $\\operatorname{logit}(p') = a + \\operatorname{logit}(\\hat{p})$. Since $a  0$, it follows that $\\operatorname{logit}(p')  \\operatorname{logit}(\\hat{p})$. The logit function is monotonically increasing, so this inequality implies $p'  \\hat{p}$. This means the calibrated probabilities are lower than the original predicted probabilities. The model was systematically overestimating the risk, as the mean prediction $\\bar{p}=0.9$ was higher than the observed prevalence $\\hat{\\pi}=0.7$. A negative intercept $a$ corrects for this overestimation, which is consistent with the problem's stated interpretation.", "answer": "$$\\boxed{-1.350}$$", "id": "4544791"}, {"introduction": "While a global assessment is useful, a deeper analysis requires distinguishing between a model's accuracy and its ability to stratify risk. This practice explores the Brier score decomposition, a powerful technique that separates prediction error into reliability (the calibration component) and resolution (the discrimination component) [@problem_id:4544798]. By comparing two models with identical discriminative power (AUC), you will see firsthand why good calibration is a distinct and crucial property for a trustworthy clinical model.", "problem": "A hospital cohort of $N = 1000$ adult intensive care unit patients is used to assess two binary predictive models of $30$-day mortality, denoted Model $A$ and Model $B$. For each patient $i$, the models output a predicted probability $p_i \\in [0,1]$ for the event $y_i \\in \\{0,1\\}$. Both models have identical Area Under the Receiver Operating Characteristic (ROC) Curve (AUC), with AUC measured as $0.85$ on this cohort. The goal is to evaluate calibration by computing the Brier score and its reliability and resolution components from first principles, and to compare the models’ calibration while acknowledging identical discrimination as measured by AUC.\n\nYou are provided summary data for each model by grouping patients into prediction bins defined by the model’s own predicted probabilities. For each bin $j$ in each model, you are given the number of patients $n_j$, the common predicted probability for all patients in that bin $p_j$, and the number of observed events $e_j = \\sum_{i \\in \\text{bin } j} y_i$. The overall event count in the cohort is $\\sum_j e_j = 210$, so the cohort event rate is $\\bar{y} = 0.21$.\n\nFor Model $A$, the bins are:\n- Bin $1$: $n_1^{A} = 200$, $p_1^{A} = 0.05$, $e_1^{A} = 10$.\n- Bin $2$: $n_2^{A} = 300$, $p_2^{A} = 0.10$, $e_2^{A} = 30$.\n- Bin $3$: $n_3^{A} = 250$, $p_3^{A} = 0.20$, $e_3^{A} = 50$.\n- Bin $4$: $n_4^{A} = 150$, $p_4^{A} = 0.40$, $e_4^{A} = 60$.\n- Bin $5$: $n_5^{A} = 100$, $p_5^{A} = 0.60$, $e_5^{A} = 60$.\n\nFor Model $B$, the bins are:\n- Bin $1$: $n_1^{B} = 250$, $p_1^{B} = 0.15$, $e_1^{B} = 20$.\n- Bin $2$: $n_2^{B} = 250$, $p_2^{B} = 0.20$, $e_2^{B} = 35$.\n- Bin $3$: $n_3^{B} = 200$, $p_3^{B} = 0.25$, $e_3^{B} = 60$.\n- Bin $4$: $n_4^{B} = 200$, $p_4^{B} = 0.35$, $e_4^{B} = 70$.\n- Bin $5$: $n_5^{B} = 100$, $p_5^{B} = 0.55$, $e_5^{B} = 25$.\n\nStarting from the definition that the Brier score is the mean squared error between predicted probabilities and observed binary outcomes, compute the Brier score for each model by correctly aggregating over bins. Then, using the canonical decomposition into uncertainty, reliability, and resolution derived from first principles, compute the reliability and resolution components for each model and use them to interpret which model is better calibrated. Finally, report the numeric difference in Brier scores $\\Delta = \\text{BS}_{B} - \\text{BS}_{A}$. Round your final numeric answer to four significant figures. No units are required.", "solution": "The problem requires the evaluation and comparison of two predictive models, Model $A$ and Model $B$, based on their calibration. The models have identical discrimination, as indicated by their identical Area Under the ROC Curve (AUC) of $0.85$. The primary tool for this evaluation will be the Brier score and its decomposition into reliability, resolution, and uncertainty.\n\nFirst, we define the Brier score (BS) for a set of $N$ predictions. For each patient $i$, we have a predicted probability $p_i$ and a binary outcome $y_i \\in \\{0, 1\\}$. The Brier score is the mean squared error of the predictions:\n$$ \\text{BS} = \\frac{1}{N} \\sum_{i=1}^{N} (p_i - y_i)^2 $$\nThe data is provided in binned format. For each bin $j$ (from $1$ to $K$), we have $n_j$ patients, a common predicted probability $p_j$, and $e_j$ observed events. The total number of patients is $N = \\sum_{j=1}^{K} n_j$. The observed event rate (or mean outcome) in bin $j$ is $\\bar{y}_j = e_j / n_j$. The Brier score can be calculated from binned data as:\n$$ \\text{BS} = \\frac{1}{N} \\sum_{j=1}^{K} \\sum_{i \\in \\text{bin } j} (p_j - y_i)^2 = \\frac{1}{N} \\sum_{j=1}^{K} \\left( e_j(p_j-1)^2 + (n_j - e_j)(p_j-0)^2 \\right) $$\nThis simplifies to:\n$$ \\text{BS} = \\frac{1}{N} \\sum_{j=1}^{K} (n_j p_j^2 - 2 p_j e_j + e_j) $$\n\nThe Brier score can be decomposed into three components:\n$$ \\text{BS} = \\text{Reliability} - \\text{Resolution} + \\text{Uncertainty} $$\nLet $\\bar{y}$ be the overall event rate in the cohort, $\\bar{y} = (\\sum e_j) / N$.\nThe components are defined for binned data as:\n1.  **Reliability (REL)**: Measures the calibration error. It is the weighted mean squared difference between predicted probabilities and observed event rates in the bins. A lower value indicates better calibration.\n    $$ \\text{REL} = \\frac{1}{N} \\sum_{j=1}^{K} n_j (p_j - \\bar{y}_j)^2 $$\n2.  **Resolution (RES)**: Measures the model's ability to separate patients into groups with different outcome rates. It is the weighted mean squared difference between bin-specific event rates and the overall event rate. A higher value is better.\n    $$ \\text{RES} = \\frac{1}{N} \\sum_{j=1}^{K} n_j (\\bar{y}_j - \\bar{y})^2 $$\n3.  **Uncertainty (UNC)**: Represents the inherent uncertainty in the outcomes of the dataset, independent of the model. It is the variance of the binary outcomes.\n    $$ \\text{UNC} = \\bar{y}(1 - \\bar{y}) $$\n\nFirst, we compute the cohort-wide parameters. The total number of patients is $N = 1000$. The total number of events is given as $\\sum e_j = 210$. The overall event rate is:\n$$ \\bar{y} = \\frac{210}{1000} = 0.21 $$\nThe uncertainty component is the same for both models and depends only on the dataset:\n$$ \\text{UNC} = 0.21 \\times (1 - 0.21) = 0.21 \\times 0.79 = 0.1659 $$\n\nNext, we analyze Model $A$. We first compute the observed event rate $\\bar{y}_j^A = e_j^A / n_j^A$ for each bin.\n- Bin $1$: $\\bar{y}_1^A = 10 / 200 = 0.05$. This equals $p_1^A = 0.05$.\n- Bin $2$: $\\bar{y}_2^A = 30 / 300 = 0.10$. This equals $p_2^A = 0.10$.\n- Bin $3$: $\\bar{y}_3^A = 50 / 250 = 0.20$. This equals $p_3^A = 0.20$.\n- Bin $4$: $\\bar{y}_4^A = 60 / 150 = 0.40$. This equals $p_4^A = 0.40$.\n- Bin $5$: $\\bar{y}_5^A = 60 / 100 = 0.60$. This equals $p_5^A = 0.60$.\n\nFor Model $A$, the observed event rate in every bin is identical to the predicted probability for that bin ($\\bar{y}_j^A = p_j^A$ for all $j$). This indicates perfect calibration. We now compute the components.\nReliability for Model $A$:\n$$ \\text{REL}_A = \\frac{1}{1000} \\sum_{j=1}^{5} n_j^A (p_j^A - \\bar{y}_j^A)^2 = \\frac{1}{1000} \\sum_{j=1}^{5} n_j^A (0)^2 = 0 $$\nResolution for Model $A$:\n$$ \\text{RES}_A = \\frac{1}{1000} \\sum_{j=1}^{5} n_j^A (\\bar{y}_j^A - \\bar{y})^2 $$\n$$ \\text{RES}_A = \\frac{1}{1000} [ 200(0.05 - 0.21)^2 + 300(0.10 - 0.21)^2 + 250(0.20 - 0.21)^2 + 150(0.40 - 0.21)^2 + 100(0.60 - 0.21)^2 ] $$\n$$ \\text{RES}_A = \\frac{1}{1000} [ 200(-0.16)^2 + 300(-0.11)^2 + 250(-0.01)^2 + 150(0.19)^2 + 100(0.39)^2 ] $$\n$$ \\text{RES}_A = \\frac{1}{1000} [ 200(0.0256) + 300(0.0121) + 250(0.0001) + 150(0.0361) + 100(0.1521) ] $$\n$$ \\text{RES}_A = \\frac{1}{1000} [ 5.12 + 3.63 + 0.025 + 5.415 + 15.21 ] = \\frac{29.4}{1000} = 0.0294 $$\nBrier score for Model $A$:\n$$ \\text{BS}_A = \\text{REL}_A - \\text{RES}_A + \\text{UNC} = 0 - 0.0294 + 0.1659 = 0.1365 $$\n\nNow, we analyze Model $B$. We compute the observed event rate $\\bar{y}_j^B = e_j^B / n_j^B$ for each bin.\n- Bin $1$: $\\bar{y}_1^B = 20 / 250 = 0.08$. Here $p_1^B = 0.15$.\n- Bin $2$: $\\bar{y}_2^B = 35 / 250 = 0.14$. Here $p_2^B = 0.20$.\n- Bin $3$: $\\bar{y}_3^B = 60 / 200 = 0.30$. Here $p_3^B = 0.25$.\n- Bin $4$: $\\bar{y}_4^B = 70 / 200 = 0.35$. Here $p_4^B = 0.35$.\n- Bin $5$: $\\bar{y}_5^B = 25 / 100 = 0.25$. Here $p_5^B = 0.55$.\n\nFor Model $B$, the observed rates $\\bar{y}_j^B$ are not equal to the predicted probabilities $p_j^B$, indicating miscalibration.\nReliability for Model $B$:\n$$ \\text{REL}_B = \\frac{1}{1000} \\sum_{j=1}^{5} n_j^B (p_j^B - \\bar{y}_j^B)^2 $$\n$$ \\text{REL}_B = \\frac{1}{1000} [ 250(0.15 - 0.08)^2 + 250(0.20 - 0.14)^2 + 200(0.25 - 0.30)^2 + 200(0.35 - 0.35)^2 + 100(0.55 - 0.25)^2 ] $$\n$$ \\text{REL}_B = \\frac{1}{1000} [ 250(0.07)^2 + 250(0.06)^2 + 200(-0.05)^2 + 200(0)^2 + 100(0.30)^2 ] $$\n$$ \\text{REL}_B = \\frac{1}{1000} [ 250(0.0049) + 250(0.0036) + 200(0.0025) + 0 + 100(0.09) ] $$\n$$ \\text{REL}_B = \\frac{1}{1000} [ 1.225 + 0.9 + 0.5 + 0 + 9.0 ] = \\frac{11.625}{1000} = 0.011625 $$\nResolution for Model $B$:\n$$ \\text{RES}_B = \\frac{1}{1000} \\sum_{j=1}^{5} n_j^B (\\bar{y}_j^B - \\bar{y})^2 $$\n$$ \\text{RES}_B = \\frac{1}{1000} [ 250(0.08 - 0.21)^2 + 250(0.14 - 0.21)^2 + 200(0.30 - 0.21)^2 + 200(0.35 - 0.21)^2 + 100(0.25 - 0.21)^2 ] $$\n$$ \\text{RES}_B = \\frac{1}{1000} [ 250(-0.13)^2 + 250(-0.07)^2 + 200(0.09)^2 + 200(0.14)^2 + 100(0.04)^2 ] $$\n$$ \\text{RES}_B = \\frac{1}{1000} [ 250(0.0169) + 250(0.0049) + 200(0.0081) + 200(0.0196) + 100(0.0016) ] $$\n$$ \\text{RES}_B = \\frac{1}{1000} [ 4.225 + 1.225 + 1.62 + 3.92 + 0.16 ] = \\frac{11.15}{1000} = 0.01115 $$\nBrier score for Model $B$:\n$$ \\text{BS}_B = \\text{REL}_B - \\text{RES}_B + \\text{UNC} = 0.011625 - 0.01115 + 0.1659 = 0.166375 $$\n\nInterpretation:\nModel $A$ is superior to Model $B$. While both have identical discriminative power (AUC$=0.85$), their calibration differs significantly. Model $A$ is perfectly calibrated ($\\text{REL}_A = 0$), meaning its predicted probabilities are accurate. Model $B$ is miscalibrated ($\\text{REL}_B > 0$). Furthermore, Model $A$ has a higher resolution ($\\text{RES}_A > \\text{RES}_B$), indicating it does a better job of stratifying patients into distinct risk groups. The superior calibration and resolution of Model $A$ result in a lower (better) overall Brier score ($\\text{BS}_A  \\text{BS}_B$). Therefore, Model $A$ provides more accurate and reliable risk predictions.\n\nFinally, we compute the difference in Brier scores:\n$$ \\Delta = \\text{BS}_{B} - \\text{BS}_{A} = 0.166375 - 0.1365 = 0.029875 $$\nRounding to four significant figures, we get $0.02988$.", "answer": "$$\\boxed{0.02988}$$", "id": "4544798"}, {"introduction": "After diagnosing miscalibration, the next practical step is to correct it. This exercise transitions from analysis to application, guiding you through the implementation of a logistic recalibration map to improve a model's reliability [@problem_id:4544765]. By applying a learned transformation and measuring the resulting improvements in calibration metrics like Expected Calibration Error (ECE), you will gain hands-on experience in the vital task of making predictive models more accurate and trustworthy for decision-making.", "problem": "You are given a set of binary event predictions in the form of predicted risks and corresponding outcomes, together with affine parameters for a logistic calibration map. From first principles of probability calibration and proper scoring rules, implement a program that recalibrates the predicted probabilities, evaluates the Expected Calibration Error (ECE) and the Brier reliability component before and after recalibration, and reports the improvements. All computations must be performed in purely mathematical terms over the unit interval, without using any domain-specific heuristics. No physical units are involved. All fractions should be expressed as decimals.\n\nBackground and definitions:\n- A binary predictive model outputs a predicted risk $p \\in [0,1]$ for an event $Y \\in \\{0,1\\}$. A calibration map $g:[0,1] \\to [0,1]$ transforms $p$ into a recalibrated probability $q = g(p)$.\n- The logistic calibration map is defined by an affine transformation in the log-odds domain followed by the logistic (sigmoid) inverse link. Let $\\alpha \\in \\mathbb{R}$ and $\\beta \\in \\mathbb{R}$ be given. For any $p \\in [0,1]$, define the logit $\\ell(p) = \\log\\left(\\frac{p}{1-p}\\right)$ and the sigmoid $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$. To avoid undefined logarithms at $p \\in \\{0,1\\}$, first clip $p$ to $[\\varepsilon, 1-\\varepsilon]$ for a small $\\varepsilon  0$, denoted $p_{\\text{clip}} = \\min\\{\\max\\{p,\\varepsilon\\},1-\\varepsilon\\}$. Then compute\n$$\nq \\;=\\; g(p) \\;=\\; \\sigma\\!\\left(\\alpha + \\beta \\,\\ell(p_{\\text{clip}})\\right).\n$$\n- The Expected Calibration Error (ECE) approximates the expected absolute difference between predicted probabilities and empirical event frequencies by binning the interval $[0,1]$ into $B$ equal-width bins. Let $N$ be the number of predictions, and let $b \\in \\{1,\\dots,B\\}$ index bins with edges $\\left[0,\\tfrac{1}{B}\\right), \\left[\\tfrac{1}{B},\\tfrac{2}{B}\\right), \\dots, \\left[\\tfrac{B-1}{B},1\\right]$. For a given set of predictions $r_i \\in [0,1]$ (where $r_i$ is either $p_i$ for pre-calibration or $q_i$ for post-calibration) and outcomes $y_i \\in \\{0,1\\}$, let $n_b$ be the number of indices $i$ falling into bin $b$, $\\bar{r}_b$ the average of $r_i$ in bin $b$, and $\\bar{y}_b$ the average of $y_i$ in bin $b$. Define\n$$\n\\mathrm{ECE}(r,y;B) \\;=\\; \\sum_{b=1}^{B} \\frac{n_b}{N}\\,\\left|\\bar{r}_b - \\bar{y}_b\\right|.\n$$\nEmpty bins ($n_b = 0$) contribute $0$ to the sum.\n- The Brier score admits a decomposition into uncertainty, resolution, and reliability. The Brier reliability component measures within-bin miscalibration. Using the same binning and notation as above, define the Brier reliability as\n$$\n\\mathrm{Rel}(r,y;B) \\;=\\; \\sum_{b=1}^{B} \\frac{n_b}{N}\\,\\left(\\bar{r}_b - \\bar{y}_b\\right)^2.\n$$\n- Improvements are defined as nonnegative reductions in error: for a given metric $M \\in \\{\\mathrm{ECE}, \\mathrm{Rel}\\}$, the improvement is $M(r_{\\text{pre}},y;B) - M(r_{\\text{post}},y;B)$, where $r_{\\text{pre}} = p$ and $r_{\\text{post}} = q$. Positive values indicate better calibration after recalibration.\n\nAlgorithmic requirements:\n- Implement the logistic calibration map $g$ as defined. Use the provided $\\varepsilon$ for clipping each test case.\n- Implement binning into $B$ equal-width bins over $[0,1]$ with the last bin right-inclusive, that is, the bin intervals are $[0,\\tfrac{1}{B}), [\\tfrac{1}{B},\\tfrac{2}{B}), \\dots, [\\tfrac{B-1}{B},1]$. A prediction $r_i = 1$ must belong to the last bin.\n- Compute $\\mathrm{ECE}$ and $\\mathrm{Rel}$ for both pre-calibration and post-calibration predictions and report the improvements as differences pre minus post.\n- For each test case, return a list of six decimal values $[e_{\\text{pre}}, e_{\\text{post}}, \\Delta e, \\rho_{\\text{pre}}, \\rho_{\\text{post}}, \\Delta \\rho]$, where $e_{\\text{pre}} = \\mathrm{ECE}(p,y;B)$, $e_{\\text{post}} = \\mathrm{ECE}(q,y;B)$, $\\Delta e = e_{\\text{pre}} - e_{\\text{post}}$, $\\rho_{\\text{pre}} = \\mathrm{Rel}(p,y;B)$, $\\rho_{\\text{post}} = \\mathrm{Rel}(q,y;B)$, and $\\Delta \\rho = \\rho_{\\text{pre}} - \\rho_{\\text{post}}$. Round each reported value to $6$ decimal places.\n\nTest suite:\n- Case $1$ (happy path): $p = [0.02, 0.05, 0.08, 0.15, 0.22, 0.35, 0.50, 0.65, 0.80, 0.92]$, $y = [0, 0, 0, 0, 1, 0, 1, 1, 1, 1]$, $\\alpha = -0.2$, $\\beta = 1.3$, $B = 5$, $\\varepsilon = 10^{-12}$.\n- Case $2$ (identity mapping, no clipping needed): $p = [0.10, 0.12, 0.18, 0.22, 0.28, 0.32, 0.38, 0.42, 0.55, 0.60, 0.72, 0.85]$, $y = [0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1]$, $\\alpha = 0$, $\\beta = 1$, $B = 6$, $\\varepsilon = 10^{-12}$.\n- Case $3$ (extremes with clipping): $p = [0.00, 0.01, 0.05, 0.20, 0.40, 0.60, 0.80, 0.95, 0.99, 1.00]$, $y = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]$, $\\alpha = 0$, $\\beta = 0.5$, $B = 10$, $\\varepsilon = 10^{-9}$.\n- Case $4$ (negative slope): $p = [0.05, 0.10, 0.15, 0.20, 0.85, 0.90, 0.95, 0.99]$, $y = [1, 1, 1, 1, 0, 0, 0, 0]$, $\\alpha = 0$, $\\beta = -1$, $B = 4$, $\\varepsilon = 10^{-12}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of six rounded decimal values as specified. For example, the abstract format is $[[e_{\\text{pre}}, e_{\\text{post}}, \\Delta e, \\rho_{\\text{pre}}, \\rho_{\\text{post}}, \\Delta \\rho], \\dots]$.", "solution": "The problem requires the implementation of a logistic calibration procedure for binary event predictions and the evaluation of its effect on calibration metrics. The solution is partitioned into three main components: a function to apply the logistic calibration map, a function to compute the Expected Calibration Error (ECE) and Brier reliability (Rel) metrics, and a main routine to process the given test cases.\n\nFirst, we define a function to perform the logistic calibration. Let $p$ be a vector of initial predicted risks, and let $\\alpha$ and $\\beta$ be the scalar parameters of the affine transformation in the log-odds space. The recalibrated probability $q$ for a given $p$ is defined as $q = g(p) = \\sigma(\\alpha + \\beta \\cdot \\ell(p_{\\text{clip}}))$. The implementation proceeds in four steps:\n1.  **Clipping**: To prevent numerical instability from $\\log(0)$ or division by zero in the logit function for $p \\in \\{0, 1\\}$, each prediction $p_i$ is first clipped to the interval $[\\varepsilon, 1-\\varepsilon]$, where $\\varepsilon$ is a small positive constant. Let $p_{i, \\text{clip}} = \\min\\{\\max\\{p_i, \\varepsilon\\}, 1-\\varepsilon\\}$.\n2.  **Logit Transformation**: Each clipped prediction $p_{i, \\text{clip}}$ is transformed into log-odds space using the logit function, $\\ell(p) = \\log\\left(\\frac{p}{1-p}\\right)$. The logit-transformed values are $z_{i, \\text{logit}} = \\log\\left(\\frac{p_{i, \\text{clip}}}{1-p_{i, \\text{clip}}}\\right)$.\n3.  **Affine Transformation**: An affine transformation is applied to the logit values: $z_i' = \\alpha + \\beta \\cdot z_{i, \\text{logit}}$.\n4.  **Sigmoid (Inverse Logit) Transformation**: The transformed log-odds $z_i'$ are mapped back to the probability interval $[0, 1]$ using the sigmoid function, $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$, to obtain the recalibrated probabilities $q_i = \\sigma(z_i')$.\n\nNext, we establish a function to compute the ECE and Brier reliability. Both metrics are based on binning the predictions. Let $r$ be a vector of predictions (either pre-calibration $p$ or post-calibration $q$) and $y$ be the vector of true binary outcomes.\n1.  **Binning**: The interval $[0, 1]$ is divided into $B$ equal-width bins: $[0, \\frac{1}{B}), [\\frac{1}{B}, \\frac{2}{B}), \\dots, [\\frac{B-1}{B}, 1]$. Each prediction $r_i$ is assigned to a bin. The bin index $b$ for a prediction $r_i$ is determined by $\\lfloor r_i \\cdot B \\rfloor$. To adhere to the problem specification that the last bin is right-inclusive (i.e., $r_i=1$ falls into the last bin), the bin index is calculated as $\\min(B-1, \\lfloor r_i \\cdot B \\rfloor)$.\n2.  **Bin-wise Statistics**: For each bin $b \\in \\{1, \\dots, B\\}$, we compute the number of predictions in the bin, $n_b$, the average prediction value in the bin, $\\bar{r}_b$, and the average outcome (empirical frequency) in the bin, $\\bar{y}_b$. If a bin is empty ($n_b=0$), its contribution to the total error is $0$.\n3.  **Metric Calculation**:\n    -   The Expected Calibration Error is the weighted average of the absolute differences between average predictions and empirical frequencies across all bins:\n        $$\n        \\mathrm{ECE}(r,y;B) = \\sum_{b=1}^{B} \\frac{n_b}{N}\\left|\\bar{r}_b - \\bar{y}_b\\right|\n        $$\n        where $N$ is the total number of predictions.\n    -   The Brier reliability component is the weighted average of the squared differences:\n        $$\n        \\mathrm{Rel}(r,y;B) = \\sum_{b=1}^{B} \\frac{n_b}{N}\\left(\\bar{r}_b - \\bar{y}_b\\right)^2\n        $$\n\nThe main routine orchestrates these components. For each test case, it performs the following sequence:\n1.  It accepts the vectors of pre-calibration predictions $p$ and outcomes $y$, and the parameters $\\alpha, \\beta, B, \\varepsilon$.\n2.  It calls the metrics calculation function with the pre-calibration predictions $p$ and outcomes $y$ to compute $e_{\\text{pre}} = \\mathrm{ECE}(p, y; B)$ and $\\rho_{\\text{pre}} = \\mathrm{Rel}(p, y; B)$.\n3.  It calls the logistic calibration function with $p, \\alpha, \\beta, \\varepsilon$ to obtain the post-calibration predictions $q$.\n4.  It calls the metrics calculation function with the post-calibration predictions $q$ and outcomes $y$ to compute $e_{\\text{post}} = \\mathrm{ECE}(q, y; B)$ and $\\rho_{\\text{post}} = \\mathrm{Rel}(q, y; B)$.\n5.  It calculates the improvements as $\\Delta e = e_{\\text{pre}} - e_{\\text{post}}$ and $\\Delta \\rho = \\rho_{\\text{pre}} - \\rho_{\\text{post}}$.\n6.  Finally, it gathers the six values $[e_{\\text{pre}}, e_{\\text{post}}, \\Delta e, \\rho_{\\text{pre}}, \\rho_{\\text{post}}, \\Delta \\rho]$, rounds each to $6$ decimal places, and appends the resulting list to a collection of all test case results.\n\nThis structured, step-by-step process ensures that all calculations are performed according to the provided mathematical definitions and that the final output adheres to the specified format. The use of `numpy` arrays facilitates efficient, vectorized computation for all steps.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def apply_logistic_calibration(p, alpha, beta, eps):\n        \"\"\"\n        Applies the logistic calibration map to a set of predicted probabilities.\n        \n        Args:\n            p (np.ndarray): The initial predicted risks.\n            alpha (float): The intercept parameter.\n            beta (float): The slope parameter.\n            eps (float): The clipping parameter.\n            \n        Returns:\n            np.ndarray: The recalibrated probabilities.\n        \"\"\"\n        # Step 1: Clip p to avoid log(0) and division by zero.\n        p_clipped = np.clip(p, eps, 1.0 - eps)\n        \n        # Step 2: Compute logit.\n        logit_p = np.log(p_clipped / (1.0 - p_clipped))\n        \n        # Step 3: Apply affine transformation in log-odds space.\n        z_transformed = alpha + beta * logit_p\n        \n        # Step 4: Apply sigmoid to map back to probabilities.\n        q = 1.0 / (1.0 + np.exp(-z_transformed))\n        \n        return q\n\n    def calculate_calibration_metrics(r, y, B):\n        \"\"\"\n        Calculates the Expected Calibration Error (ECE) and Brier Reliability (Rel).\n        \n        Args:\n            r (np.ndarray): The predicted risks (pre or post calibration).\n            y (np.ndarray): The true binary outcomes.\n            B (int): The number of bins.\n            \n        Returns:\n            tuple: A tuple containing (ECE, Reliability).\n        \"\"\"\n        N = len(r)\n        if N == 0:\n            return 0.0, 0.0\n\n        # Step 1: Assign each prediction to a bin.\n        # The bin index for r_i is floor(r_i * B).\n        # We must handle the edge case r_i = 1.0, which should go into the last bin (B-1).\n        bin_indices = np.floor(r * B).astype(int)\n        bin_indices = np.minimum(bin_indices, B - 1)\n\n        ece_total = 0.0\n        rel_total = 0.0\n\n        # Step 2  3: Iterate through bins to compute stats and metrics.\n        for b in range(B):\n            # Find all predictions and outcomes in the current bin.\n            in_bin_mask = (bin_indices == b)\n            n_b = np.sum(in_bin_mask)\n\n            # If bin is empty, it contributes 0 to the sum.\n            if n_b == 0:\n                continue\n\n            r_in_bin = r[in_bin_mask]\n            y_in_bin = y[in_bin_mask]\n\n            # Compute average prediction and outcome in the bin.\n            r_bar_b = np.mean(r_in_bin)\n            y_bar_b = np.mean(y_in_bin)\n\n            # Accumulate ECE and Reliability.\n            ece_total += (n_b / N) * np.abs(r_bar_b - y_bar_b)\n            rel_total += (n_b / N) * (r_bar_b - y_bar_b)**2\n\n        return ece_total, rel_total\n\n    def process_case(p, y, alpha, beta, B, eps):\n        \"\"\"\n        Processes a single test case.\n        \"\"\"\n        # Convert inputs to numpy arrays for vectorized operations.\n        p_arr = np.array(p, dtype=float)\n        y_arr = np.array(y, dtype=float)\n\n        # Calculate pre-calibration metrics.\n        e_pre, rho_pre = calculate_calibration_metrics(p_arr, y_arr, B)\n        \n        # Perform recalibration.\n        q_arr = apply_logistic_calibration(p_arr, alpha, beta, eps)\n\n        # Calculate post-calibration metrics.\n        e_post, rho_post = calculate_calibration_metrics(q_arr, y_arr, B)\n\n        # Calculate improvements.\n        delta_e = e_pre - e_post\n        delta_rho = rho_pre - rho_post\n\n        # Package and round results.\n        result = [\n            round(e_pre, 6), round(e_post, 6), round(delta_e, 6),\n            round(rho_pre, 6), round(rho_post, 6), round(delta_rho, 6)\n        ]\n        return result\n\n    test_cases = [\n        # Case 1\n        (\n            [0.02, 0.05, 0.08, 0.15, 0.22, 0.35, 0.50, 0.65, 0.80, 0.92],\n            [0, 0, 0, 0, 1, 0, 1, 1, 1, 1],\n            -0.2, 1.3, 5, 1e-12\n        ),\n        # Case 2\n        (\n            [0.10, 0.12, 0.18, 0.22, 0.28, 0.32, 0.38, 0.42, 0.55, 0.60, 0.72, 0.85],\n            [0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1],\n            0.0, 1.0, 6, 1e-12\n        ),\n        # Case 3\n        (\n            [0.00, 0.01, 0.05, 0.20, 0.40, 0.60, 0.80, 0.95, 0.99, 1.00],\n            [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n            0.0, 0.5, 10, 1e-9\n        ),\n        # Case 4\n        (\n            [0.05, 0.10, 0.15, 0.20, 0.85, 0.90, 0.95, 0.99],\n            [1, 1, 1, 1, 0, 0, 0, 0],\n            0.0, -1.0, 4, 1e-12\n        )\n    ]\n\n    all_results = []\n    for case in test_cases:\n        p, y, alpha, beta, B, eps = case\n        result = process_case(p, y, alpha, beta, B, eps)\n        all_results.append(result)\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "4544765"}]}