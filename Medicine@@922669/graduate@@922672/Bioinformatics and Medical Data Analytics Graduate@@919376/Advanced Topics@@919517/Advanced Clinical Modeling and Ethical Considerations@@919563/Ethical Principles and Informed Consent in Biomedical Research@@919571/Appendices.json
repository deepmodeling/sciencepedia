{"hands_on_practices": [{"introduction": "A common first step in protecting research participant privacy is de-identification, with $k$-anonymity being a foundational technique. However, simply ensuring that each individual is indistinguishable from at least $k-1$ others is not a complete solution. This exercise challenges you to formalize the residual privacy risks by deriving, from first principles, the adversary's probability of success in both re-identifying an individual and inferring their sensitive attributes within a $k$-anonymous dataset. By quantifying these distinct risks, you will gain a deeper appreciation for the vulnerabilities of simple anonymization, such as homogeneity attacks, and the need for more robust privacy models. [@problem_id:4560898]", "problem": "A biomedical research team plans to release a de-identified Electronic Health Records (EHR) dataset for secondary analysis under Institutional Review Board (IRB) oversight. The dataset applies the definition of $k$-anonymity: every released record belongs to an equivalence class of at least $k$ records that share the same quasi-identifier pattern. Consider an adversary who knows that a particular individual participated in the study and knows their quasi-identifier pattern, but has no additional tie-breaking auxiliary information beyond what is consistent with the released data.\n\nStarting from the basic definitions of probability and Bayes’ theorem, derive the adversary’s worst-case posterior probability of correctly re-identifying the individual’s record within an equivalence class of size $k$. Next, consider a sensitive attribute $S$ (for example, diagnosis categories) with $m$ mutually exclusive categories. Within the individual’s equivalence class $E$, let the empirical distribution of $S$ be $\\mathbf{q} = (q_{1}, q_{2}, \\dots, q_{m})$ with $\\sum_{j=1}^{m} q_{j} = 1$. Under the same adversarial knowledge, derive the adversary’s maximum posterior probability of correctly inferring the individual’s sensitive attribute value.\n\nTo formalize ethical risk for consent evaluation, suppose the research team quantifies expected harm as a weighted sum of identity and attribute disclosure risks,\n$$\nR = \\lambda_{I} \\, P(\\text{identity correct}) + \\lambda_{A} \\, P(\\text{attribute correct}),\n$$\nwhere $\\lambda_{I} \\ge 0$ and $\\lambda_{A} \\ge 0$ are institutional weights representing relative harm severity. For a particular equivalence class with $k = 15$, a sensitive attribute distribution $\\mathbf{q} = (0.90, 0.06, 0.04)$, and weights $\\lambda_{I} = 0.50$ and $\\lambda_{A} = 0.50$, compute $R$. Express your final $R$ as a decimal and round to four significant figures.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is based on standard, formal definitions from the field of data privacy, specifically $k$-anonymity, and uses fundamental principles of probability theory to model adversarial risk. All necessary parameters for calculation are provided, and no contradictions exist. Therefore, the problem is valid, and we may proceed with a formal solution.\n\nThe solution is divided into three parts: first, the derivation of the identity disclosure risk; second, the derivation of the attribute disclosure risk; and third, the calculation of the total expected harm for the given parameters.\n\n**Part 1: Derivation of Identity Disclosure Risk**\n\nLet $E$ denote the equivalence class identified by the adversary, which contains the record of a specific individual of interest. The size of this equivalence class is given as $|E| = k$. By the definition of $k$-anonymity, all $k$ records within $E$ are indistinguishable based on their quasi-identifiers.\n\nThe adversary knows the individual is in the study and has identified their equivalence class $E$. The adversary's goal is to re-identify the specific record belonging to the individual from the $k$ records in $E$. The problem statement specifies that the adversary \"has no additional tie-breaking auxiliary information beyond what is consistent with the released data.\"\n\nLet the set of records in the equivalence class be $\\{r_1, r_2, \\dots, r_k\\}$. Let $H_i$ be the hypothesis that record $r_i$ is the record of the target individual. The adversary's state of knowledge implies that, a priori, each record is equally likely to be the target's record. This is an application of the principle of indifference. The prior probability for each hypothesis is:\n$$\nP(H_i) = \\frac{1}{k} \\quad \\text{for } i = 1, 2, \\dots, k\n$$\nThe problem asks for the adversary's worst-case posterior probability. Since the released data (the equivalence class itself) provides no information to distinguish among the $k$ records, the posterior probability does not update from the prior. An adversary's best guess is to select any one of the $k$ records. The probability of this guess being correct is $\\frac{1}{k}$. There is no strategy that can improve upon this, so this represents the maximum success probability for the adversary. This is the worst-case scenario from a privacy perspective.\n\nTherefore, the worst-case posterior probability of correct re-identification is:\n$$\nP(\\text{identity correct}) = \\frac{1}{k}\n$$\n\n**Part 2: Derivation of Attribute Disclosure Risk**\n\nLet $S$ be a sensitive attribute with $m$ mutually exclusive categories, denoted $\\{v_1, v_2, \\dots, v_m\\}$. Within the individual's equivalence class $E$, the empirical distribution of these attribute values is given by the vector $\\mathbf{q} = (q_1, q_2, \\dots, q_m)$, where $q_j$ is the fraction of records in $E$ that have the sensitive attribute value $v_j$. We are given that $\\sum_{j=1}^{m} q_j = 1$. The number of records in $E$ with attribute value $v_j$ is $k \\cdot q_j$.\n\nThe adversary's goal is to correctly infer the individual's true sensitive attribute value. Let $S_I$ be the true attribute value of the individual. As established in Part 1, from the adversary's perspective, the individual's record is a random draw from the $k$ records in $E$. Therefore, the probability that the individual's true attribute value is $v_j$ is equal to the proportion of records in $E$ having that value:\n$$\nP(S_I = v_j) = q_j\n$$\nA rational adversary will seek to maximize their chance of being correct. Their optimal strategy is to guess the most frequent attribute value within the equivalence class. The probability of success for this strategy is the probability of the most likely attribute.\n\nTherefore, the adversary's maximum posterior probability of correctly inferring the sensitive attribute value is:\n$$\nP(\\text{attribute correct}) = \\max_{j \\in \\{1, \\dots, m\\}} \\{q_j\\}\n$$\n\n**Part 3: Calculation of Total Expected Harm**\n\nThe total expected harm, $R$, is defined as a weighted sum of the identity and attribute disclosure risks:\n$$\nR = \\lambda_{I} \\, P(\\text{identity correct}) + \\lambda_{A} \\, P(\\text{attribute correct})\n$$\nSubstituting the expressions derived in the previous parts:\n$$\nR = \\lambda_{I} \\left(\\frac{1}{k}\\right) + \\lambda_{A} \\left(\\max_{j} \\{q_j\\}\\right)\n$$\nWe are given the following specific values:\n- Equivalence class size: $k = 15$\n- Sensitive attribute distribution: $\\mathbf{q} = (0.90, 0.06, 0.04)$\n- Institutional weights: $\\lambda_{I} = 0.50$ and $\\lambda_{A} = 0.50$\n\nFirst, we compute the component probabilities:\nThe identity disclosure risk is:\n$$\nP(\\text{identity correct}) = \\frac{1}{15}\n$$\nThe attribute disclosure risk is the maximum value in the distribution $\\mathbf{q}$:\n$$\nP(\\text{attribute correct}) = \\max\\{0.90, 0.06, 0.04\\} = 0.90\n$$\nNow, substitute these probabilities and the given weights into the formula for $R$:\n$$\nR = (0.50) \\cdot \\left(\\frac{1}{15}\\right) + (0.50) \\cdot (0.90)\n$$\n$$\nR = \\frac{0.50}{15} + 0.45\n$$\n$$\nR = \\frac{1/2}{15} + 0.45 = \\frac{1}{30} + 0.45\n$$\nTo perform the final calculation, we can convert the fraction to a decimal. $\\frac{1}{30} = 0.033333...$\n$$\nR = 0.033333... + 0.45 = 0.483333...\n$$\nThe problem requires the final answer to be rounded to four significant figures. The first four significant figures are $4$, $8$, $3$, and $3$. The fifth digit is $3$, which is less than $5$, so we round down.\n$$\nR \\approx 0.4833\n$$", "answer": "$$\n\\boxed{0.4833}\n$$", "id": "4560898"}, {"introduction": "Differential Privacy (DP) provides a gold standard for privacy protection, offering provable guarantees against a wide range of attacks. Yet, these strong guarantees involve a fundamental trade-off: adding more noise to protect privacy (a smaller $\\epsilon$) typically reduces the accuracy and utility of the data for research. This practice moves beyond abstract principles to a quantitative risk-benefit analysis, asking you to model this privacy-utility trade-off in terms of expected harm. By deriving a function for total harm reduction, you will learn how to connect the mathematical privacy parameter $\\epsilon$ to the ethical calculus of balancing beneficence (maximizing research value) and respect for persons (minimizing privacy and downstream clinical harm). [@problem_id:4560960]", "problem": "A biomedical data analytics team must decide whether to release a de-identified research dataset from a precision oncology biobank. They will only release under a formal Differential Privacy (DP) mechanism with privacy parameter $\\epsilon$. In the context of ethical principles for biomedical research, particularly beneficence and respect for persons, the team models harm along two channels: privacy harm due to possible re-identification and downstream clinical harm due to utility loss in models trained on privacy-perturbed data. The expected harm is defined as probability of a harmful event times its severity, averaged over participants.\n\nAssume there are $N=25000$ participants. For each participant, the severity of harm conditional on re-identification is $c_{p}=2.8$ standardized harm units, scaled by an individual sensitivity weight $w \\in [0,1]$ reflecting variability in privacy preferences and vulnerability. The sensitivity weights are independent and identically distributed as a Beta distribution $\\text{Beta}(a,b)$ with shape parameters $a=3$ and $b=4$. The non-DP baseline probability of re-identification for a released dataset is $r_{\\text{ND}}=0.09$. Under DP with parameter $\\epsilon$, the re-identification risk is modeled as\n$$\nr(\\epsilon)=r_{\\text{ND}}\\cdot \\frac{\\epsilon}{\\epsilon+\\lambda},\n$$\nwhere $\\lambda=0.8$ is a calibration constant that encodes the attenuation of attack success probabilities as $\\epsilon$ decreases. Utility loss under DP is modeled as\n$$\nu(\\epsilon)=\\frac{\\lambda}{\\epsilon+\\lambda},\n$$\nwhich captures the reduction in predictive model accuracy attributable to added noise. The downstream clinical harm per participant attributable to utility loss is $c_{u}=0.05$ standardized harm units per unit utility loss. In the non-DP baseline, there is no utility loss, so the downstream harm term is zero.\n\nUsing the foundational definitions of expected value and risk, and the ethical requirement to balance beneficence (minimizing expected harm) against scientific utility, derive the total expected harm reduction function $H_{\\text{red}}(\\epsilon)$, defined as the difference between the baseline expected total harm without DP and the expected total harm with DP, aggregated over all $N$ participants. Express the final $H_{\\text{red}}(\\epsilon)$ as a single simplified analytic expression in terms of $\\epsilon$, with no physical units, and do not round or approximate.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, objective, and complete.\n\n### Step 1: Extract Givens\n- Number of participants: $N=25000$\n- Severity of privacy harm conditional on re-identification: $c_{p}=2.8$\n- Individual sensitivity weight: $w \\in [0,1]$, i.i.d. from a Beta distribution $\\text{Beta}(a,b)$\n- Shape parameters of the Beta distribution: $a=3$, $b=4$\n- Non-DP baseline probability of re-identification: $r_{\\text{ND}}=0.09$\n- Re-identification risk under DP with privacy parameter $\\epsilon$: $r(\\epsilon)=r_{\\text{ND}}\\cdot \\frac{\\epsilon}{\\epsilon+\\lambda}$\n- Calibration constant: $\\lambda=0.8$\n- Utility loss under DP: $u(\\epsilon)=\\frac{\\lambda}{\\epsilon+\\lambda}$\n- Downstream clinical harm per participant per unit utility loss: $c_{u}=0.05$\n- Definition of total expected harm reduction: $H_{\\text{red}}(\\epsilon) = H_{\\text{total, baseline}} - H_{\\text{total, DP}}(\\epsilon)$, where \"baseline\" is the non-DP case.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as it uses established concepts from differential privacy, probability theory ($\\text{Beta}$ distribution, expected value), and risk analysis. The models for risk $r(\\epsilon)$ and utility loss $u(\\epsilon)$ are simplified but plausible representations of the trade-offs in privacy-preserving data release. The ethical framework (balancing beneficence and respect for persons) is correctly contextualized. The problem is well-posed, providing all necessary parameters and functional forms to derive a unique analytical expression for $H_{\\text{red}}(\\epsilon)$. The language is objective and precise. The problem does not violate any of the invalidity criteria; it is formalizable, complete, and internally consistent.\n\n### Step 3: Verdict and Action\nThe problem is valid. A detailed solution will be provided.\n\n### Solution Derivation\nThe objective is to find the total expected harm reduction function, $H_{\\text{red}}(\\epsilon)$, which is the difference between the baseline total expected harm (without Differential Privacy) and the total expected harm with Differential Privacy.\n\n$H_{\\text{red}}(\\epsilon) = H_{\\text{total, baseline}} - H_{\\text{total, DP}}(\\epsilon)$\n\nWe will first formulate expressions for the total expected harm in both the baseline and DP scenarios. The total harm is the sum of privacy harm and clinical harm, aggregated over all $N$ participants.\n\n**1. Expected Harm per Participant**\n\nThe total expected harm for a single participant is the sum of the expected privacy harm and the expected clinical harm.\n$H_{\\text{participant}} = E[\\text{Privacy Harm}] + E[\\text{Clinical Harm}]$\n\n**Expected Privacy Harm:**\nThe privacy harm for a participant with sensitivity weight $w$ is the probability of re-identification multiplied by the severity of that event, which is $w \\cdot c_p$. The expected privacy harm is found by taking the expectation over the random variable $w$.\n\n$E[\\text{Privacy Harm}] = E[\\text{Probability of Re-ID} \\times w \\cdot c_p] = (\\text{Probability of Re-ID}) \\cdot c_p \\cdot E[w]$\n\nThe sensitivity weight $w$ follows a Beta distribution, $w \\sim \\text{Beta}(a,b)$. The expected value of a random variable following this distribution is:\n$E[w] = \\frac{a}{a+b}$\nSubstituting the given values $a=3$ and $b=4$:\n$E[w] = \\frac{3}{3+4} = \\frac{3}{7}$\n\n**Expected Clinical Harm:**\nThe clinical harm is due to utility loss $u(\\epsilon)$. The problem states the harm is $c_u$ per unit of utility loss.\n$E[\\text{Clinical Harm}] = u(\\epsilon) \\cdot c_u$\nThis component is deterministic for a given $\\epsilon$.\n\n**2. Total Expected Harm under DP, $H_{\\text{total, DP}}(\\epsilon)$**\n\nFor the case with Differential Privacy, the probability of re-identification is $r(\\epsilon) = r_{\\text{ND}} \\frac{\\epsilon}{\\epsilon+\\lambda}$.\nThe expected privacy harm per participant is:\n$H_{\\text{priv, DP}}(\\epsilon) = r(\\epsilon) \\cdot c_p \\cdot E[w] = \\left(r_{\\text{ND}} \\frac{\\epsilon}{\\epsilon+\\lambda}\\right) \\cdot c_p \\cdot \\frac{a}{a+b}$\n\nThe utility loss is $u(\\epsilon) = \\frac{\\lambda}{\\epsilon+\\lambda}$.\nThe clinical harm per participant is:\n$H_{\\text{clin, DP}}(\\epsilon) = u(\\epsilon) \\cdot c_u = \\left(\\frac{\\lambda}{\\epsilon+\\lambda}\\right) \\cdot c_u$\n\nThe total expected harm per participant under DP is the sum of these two components:\n$H_{\\text{participant, DP}}(\\epsilon) = H_{\\text{priv, DP}}(\\epsilon) + H_{\\text{clin, DP}}(\\epsilon) = \\left(r_{\\text{ND}} \\frac{\\epsilon}{\\epsilon+\\lambda}\\right) c_p \\frac{a}{a+b} + \\left(\\frac{\\lambda}{\\epsilon+\\lambda}\\right) c_u$\nAggregating over all $N$ participants:\n$H_{\\text{total, DP}}(\\epsilon) = N \\cdot H_{\\text{participant, DP}}(\\epsilon) = N \\left[ \\frac{\\epsilon}{\\epsilon+\\lambda} r_{\\text{ND}} c_p \\frac{a}{a+b} + \\frac{\\lambda}{\\epsilon+\\lambda} c_u \\right]$\n$H_{\\text{total, DP}}(\\epsilon) = \\frac{N}{\\epsilon+\\lambda} \\left[ \\left( r_{\\text{ND}} c_p \\frac{a}{a+b} \\right) \\epsilon + c_u \\lambda \\right]$\n\n**3. Total Expected Harm at Baseline, $H_{\\text{total, baseline}}$**\n\nIn the non-DP baseline case:\nThe probability of re-identification is given as $r_{\\text{ND}}$.\nThe expected privacy harm per participant is:\n$H_{\\text{priv, baseline}} = r_{\\text{ND}} \\cdot c_p \\cdot E[w] = r_{\\text{ND}} c_p \\frac{a}{a+b}$\n\nThe problem states that in the non-DP baseline, there is no utility loss, so the downstream clinical harm is zero.\n$H_{\\text{clin, baseline}} = 0$\n\nThe total expected harm per participant at baseline is:\n$H_{\\text{participant, baseline}} = r_{\\text{ND}} c_p \\frac{a}{a+b}$\nAggregating over all $N$ participants:\n$H_{\\text{total, baseline}} = N \\cdot H_{\\text{participant, baseline}} = N r_{\\text{ND}} c_p \\frac{a}{a+b}$\n\n**4. Harm Reduction Function, $H_{\\text{red}}(\\epsilon)$**\n\nNow we compute the harm reduction by subtracting the DP harm from the baseline harm.\n$H_{\\text{red}}(\\epsilon) = H_{\\text{total, baseline}} - H_{\\text{total, DP}}(\\epsilon)$\n$H_{\\text{red}}(\\epsilon) = N r_{\\text{ND}} c_p \\frac{a}{a+b} - \\frac{N}{\\epsilon+\\lambda} \\left[ \\left( r_{\\text{ND}} c_p \\frac{a}{a+b} \\right) \\epsilon + c_u \\lambda \\right]$\n\nLet's simplify by factoring out common terms. Let $C = N r_{\\text{ND}} c_p \\frac{a}{a+b}$.\n$H_{\\text{red}}(\\epsilon) = C - \\frac{1}{\\epsilon+\\lambda} [C \\epsilon + N c_u \\lambda]$\n$H_{\\text{red}}(\\epsilon) = \\frac{C(\\epsilon+\\lambda) - (C\\epsilon + N c_u \\lambda)}{\\epsilon+\\lambda}$\n$H_{\\text{red}}(\\epsilon) = \\frac{C\\epsilon + C\\lambda - C\\epsilon - N c_u \\lambda}{\\epsilon+\\lambda}$\n$H_{\\text{red}}(\\epsilon) = \\frac{C\\lambda - N c_u \\lambda}{\\epsilon+\\lambda}$\n$H_{\\text{red}}(\\epsilon) = \\frac{\\lambda(C - N c_u)}{\\epsilon+\\lambda}$\n\nSubstitute back the expression for $C$:\n$H_{\\text{red}}(\\epsilon) = \\frac{\\lambda \\left( N r_{\\text{ND}} c_p \\frac{a}{a+b} - N c_u \\right)}{\\epsilon+\\lambda}$\n$H_{\\text{red}}(\\epsilon) = \\frac{N\\lambda \\left( r_{\\text{ND}} c_p \\frac{a}{a+b} - c_u \\right)}{\\epsilon+\\lambda}$\n\n**5. Substitution of Numerical Values**\n\nWe now substitute the given numerical values into the simplified expression.\n$N=25000$, $\\lambda=0.8$, $r_{\\text{ND}}=0.09$, $c_p=2.8$, $a=3$, $b=4$, $c_u=0.05$.\n\nFirst, calculate the term in the parentheses:\n$r_{\\text{ND}} c_p \\frac{a}{a+b} - c_u = (0.09) \\cdot (2.8) \\cdot \\frac{3}{3+4} - 0.05$\n$= (0.09) \\cdot (2.8) \\cdot \\frac{3}{7}$\nSince $2.8/7 = 0.4$:\n$= (0.09) \\cdot (0.4) \\cdot 3 - 0.05$\n$= (0.09) \\cdot (1.2) - 0.05$\n$= 0.108 - 0.05 = 0.058$\n\nNow calculate the numerator of the main expression, $N\\lambda \\times (\\text{term above})$:\nNumerator = $25000 \\cdot 0.8 \\cdot 0.058$\n$= 20000 \\cdot 0.058$\n$= 2 \\times 10^4 \\cdot 5.8 \\times 10^{-2}$\n$= 11.6 \\times 10^2 = 1160$\n\nThe denominator is $\\epsilon+\\lambda = \\epsilon+0.8$.\nCombining these results, the final expression for the harm reduction function is:\n$H_{\\text{red}}(\\epsilon) = \\frac{1160}{\\epsilon+0.8}$", "answer": "$$\\boxed{\\frac{1160}{\\epsilon + 0.8}}$$", "id": "4560960"}, {"introduction": "Translating ethical principles into practice requires building systems that can enforce complex rules derived from participant consent and governance policies. This final hands-on challenge places you in the role of a data custodian tasked with creating a consent-aware query system. You will implement a mechanism that respects purpose limitations and individual consent choices while simultaneously mitigating re-identification risk through minimum group size checks. To preserve scientific validity, you will use the statistical technique of inverse probability weighting to generate unbiased estimates from the consented subset of data, demonstrating how to build a query engine that is both ethically compliant and analytically sound. [@problem_id:4560952]", "problem": "A research data custodian must implement consent-constrained query answering for secondary analyses on de-identified biomedical data. The mechanism must obey the ethical principles of respect for persons and purpose limitation derived from informed consent, encoded as: data from participant $i$ may be processed only for declared research purposes for which that participant consented, and only when doing so does not create an elevated risk of reidentification due to small groups. The custodian seeks to preserve the utility of permitted analyses by returning estimates that remain scientifically useful to investigators while respecting consent constraints. Work in purely mathematical and logical terms to formalize and implement such a mechanism.\n\nFundamental base to use:\n- Core ethical definitions: purpose limitation and informed consent. Formally, each individual $i \\in \\{1,\\dots,N\\}$ has a binary consent indicator $C_{i,p} \\in \\{0,1\\}$ stating whether individual $i$ consented to purpose $p$.\n- Well-tested facts from sampling theory: if consent selection is Missing At Random (MAR), meaning that conditional on a finite partition (strata) $S$ of the population, consent is independent of the outcome of interest within each stratum, then unbiased linear estimation can be achieved by appropriate use of known inclusion probabilities. Missing At Random (MAR) is a standard assumption in survey sampling and biostatistics.\n\nMathematical model and constraints:\n- There are $N$ individuals indexed by $i$, each with a real-valued outcome $y_i \\in \\mathbb{R}$ to be linearly aggregated, and each assigned to a stratum $s(i) \\in \\{0,1,\\dots,S-1\\}$.\n- A research query is a tuple $(p,\\mathbf{a})$ with declared purpose $p$ and a real weight vector $\\mathbf{a} = (a_1,\\dots,a_N) \\in \\mathbb{R}^N$ defining a target linear functional on the full data $\\sum_{i=1}^N a_i y_i$.\n- A global governance policy specifies a set of allowed purposes $\\mathcal{P}_{\\mathrm{allowed}}$. If $p \\notin \\mathcal{P}_{\\mathrm{allowed}}$, the query must be blocked.\n- Consent constraint: only individuals with $C_{i,p} = 1$ may contribute to the computation. Define the contributing set under consent as $I_p = \\{ i \\in \\{1,\\dots,N\\} \\mid C_{i,p} = 1 \\}$.\n- Risk control (minimum group size): let $k_{\\min} \\in \\mathbb{N}$ be a positive integer. A query is blocked if the number of consenting individuals among those targeted by nonzero weights is less than $k_{\\min}$. Formally, block if $\\left|\\{ i \\mid a_i \\neq 0 \\text{ and } C_{i,p} = 1 \\}\\right| < k_{\\min}$.\n- Utility preservation target under ethics: assume consent is Missing At Random (MAR) within strata, that is, conditional on $s(i)$, the consent indicator $C_{i,p}$ is stochastically independent of $y_i$ with stratum-specific consent probability $r_{s} = \\Pr(C_{i,p} = 1 \\mid s(i)=s)$ for each $s \\in \\{0,\\dots,S-1\\}$. The mechanism may use only consented records and the publicly released stratum-level consent rates $r_s$ for the declared purpose $p$ to produce an estimate $\\widehat{T}_p(\\mathbf{a})$ for the target $\\sum_{i=1}^N a_i y_i$ such that, under the MAR model, $\\mathbb{E}[\\widehat{T}_p(\\mathbf{a})] = \\sum_{i=1}^N a_i y_i$. If for any stratum $s$ targeted by nonzero weights there is $r_s = 0$, the query must be blocked because no ethically usable information exists from that stratum.\n\nYour task:\n- Design and implement a program that, given a fixed dataset, a fixed governance policy, and a fixed minimum group size $k_{\\min}$, takes a small set of queries $(p,\\mathbf{a})$ and returns, for each query, either a real-valued estimate that satisfies the above constraints and unbiasedness target under the MAR model using only consented records and $r_s$, or the boolean value False if the query is blocked by any of the blocking rules.\n\nDataset and policy for this task:\n- Number of individuals: $N = 8$.\n- Number of strata: $S = 2$ with strata labeled $0$ and $1$.\n- Outcome vector $\\mathbf{y} = (y_1,\\dots,y_8)$ with values $\\mathbf{y} = (1.2, 0.5, 2.0, 1.5, 3.0, 2.5, 0.8, 1.1)$.\n- Stratum assignment vector $\\mathbf{s} = (s(1),\\dots,s(8)) = (0, 0, 0, 1, 1, 1, 1, 0)$.\n- Number of purposes considered: $P = 4$, indexed $p \\in \\{0,1,2,3\\}$.\n- Consent matrix $C \\in \\{0,1\\}^{8 \\times 4}$ with rows for $i=1,\\dots,8$ and columns for $p=0,1,2,3$ given by:\n  - Column $p=0$: $(1, 1, 0, 1, 0, 1, 1, 0)$,\n  - Column $p=1$: $(1, 0, 1, 1, 1, 0, 1, 1)$,\n  - Column $p=2$: $(0, 0, 0, 1, 1, 0, 1, 0)$,\n  - Column $p=3$: $(0, 0, 0, 0, 0, 0, 0, 0)$.\n- Allowed purposes under policy: $\\mathcal{P}_{\\mathrm{allowed}} = \\{0,1,2\\}$.\n- Minimum group size: $k_{\\min} = 3$.\n\nThe publicly released stratum-level consent rates $r_s$ for a declared purpose $p$ are to be computed from the consent matrix as follows: for each $s \\in \\{0,1\\}$, let $n_s = \\left|\\{ i \\mid s(i) = s \\}\\right|$ and $c_{s,p} = \\left|\\{ i \\mid s(i)=s, C_{i,p}=1 \\}\\right|$, then $r_s = c_{s,p} / n_s$. If $r_s = 0$ for any stratum $s$ such that there exists $i$ with $a_i \\neq 0$ and $s(i)=s$, the query must be blocked.\n\nTest suite:\nFor the fixed dataset above, evaluate the following five queries $(p,\\mathbf{a})$ in order:\n- Test $1$ (happy path): $p=0$, $\\mathbf{a} = (1,1,1,1,1,1,1,1)$.\n- Test $2$ (boundary at $k_{\\min}$): $p=1$, $\\mathbf{a} = (1, 2, 0, 1, 0, 0.5, 1, 0)$.\n- Test $3$ (policy disallowed purpose): $p=3$, $\\mathbf{a} = (1,1,1,1,1,1,1,1)$.\n- Test $4$ (insufficient consenting contributors): $p=0$, $\\mathbf{a} = (0,0,1,0,0,0,0,0)$.\n- Test $5$ (targeted stratum with zero consent rate for allowed purpose): $p=2$, $\\mathbf{a} = (1,1,0,0,0,0,0,0)$.\n\nRequired output:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"). Each result must be either a real number (float) if the query is answered, or the boolean value False if the query is blocked. No additional text should be printed.", "solution": "The problem requires the design and implementation of a mechanism for answering linear aggregate queries on biomedical data, subject to constraints derived from ethical principles of informed consent and purpose limitation, as well as a policy for re-identification risk mitigation. The solution must be formalized mathematically and provide unbiased estimates under a specified statistical model.\n\nThe core task is to estimate a target linear functional, $T(\\mathbf{a}) = \\sum_{i=1}^N a_i y_i$, for a given query $(p, \\mathbf{a})$ comprising a research purpose $p$ and a weight vector $\\mathbf{a}$. The estimation can only use data from individuals who have provided consent for purpose $p$, as indicated by $C_{i,p}=1$.\n\nA critical component of the problem is the assumption that consent is Missing At Random (MAR) within predefined strata. This means that for an individual $i$ in stratum $s(i)$, the probability of consent for purpose $p$, denoted $r_{s,p} = \\Pr(C_{i,p}=1 \\mid s(i)=s)$, is stochastically independent of the outcome value $y_i$. This assumption is the key to correcting for potential selection bias introduced by only observing data from consenting individuals.\n\nLet $\\delta_{i,p}$ be an indicator random variable which is $1$ if individual $i$ consents to purpose $p$ (i.e., $C_{i,p}=1$) and $0$ otherwise. Due to the MAR assumption, the conditional expectation of this indicator is $\\mathbb{E}[\\delta_{i,p} \\mid s(i), y_i] = \\mathbb{E}[\\delta_{i,p} \\mid s(i)] = r_{s(i),p}$.\n\nA naive summation over the consented subset, $\\sum_{i \\in I_p} a_i y_i = \\sum_{i=1}^N a_i y_i \\delta_{i,p}$, would yield a biased estimate of $T(\\mathbf{a})$. Its expectation is $\\mathbb{E}\\left[\\sum_{i=1}^N a_i y_i \\delta_{i,p}\\right] = \\sum_{i=1}^N a_i y_i \\mathbb{E}[\\delta_{i,p}] = \\sum_{i=1}^N a_i y_i r_{s(i),p}$, which does not equal the desired target $T(\\mathbf{a})$.\n\nTo construct an unbiased estimator, we apply the principle of inverse probability weighting, a technique fundamental to the Horvitz-Thompson estimator in survey sampling. We define our estimator $\\widehat{T}_p(\\mathbf{a})$ by dividing each consented individual's weighted outcome by their stratum's consent probability:\n$$ \\widehat{T}_p(\\mathbf{a}) = \\sum_{i=1}^N \\frac{a_i y_i \\delta_{i,p}}{r_{s(i),p}} $$\nThe expectation of this estimator is indeed the target quantity:\n$$ \\mathbb{E}[\\widehat{T}_p(\\mathbf{a})] = \\mathbb{E}\\left[ \\sum_{i=1}^N \\frac{a_i y_i \\delta_{i,p}}{r_{s(i),p}} \\right] = \\sum_{i=1}^N \\frac{a_i y_i}{r_{s(i),p}} \\mathbb{E}[\\delta_{i,p}] = \\sum_{i=1}^N \\frac{a_i y_i}{r_{s(i),p}} r_{s(i),p} = \\sum_{i=1}^N a_i y_i = T(\\mathbf{a}) $$\nThis confirms that $\\widehat{T}_p(\\mathbf{a})$ is an unbiased estimator for $T(\\mathbf{a})$ under the MAR model. Operationally, the sum is computed over the set of individuals for whom $C_{i,p}=1$ and $a_i \\neq 0$. This estimation is only possible if $r_{s,p} > 0$ for all strata $s$ containing individuals with $a_i \\neq 0$.\n\nThe complete query-answering mechanism must also enforce a set of blocking rules. A careful analysis of the problem statement, including the descriptive titles of the test cases, suggests a specific order for these checks.\n\nThe algorithm for evaluating a query $(p, \\mathbf{a})$ is as follows:\n\n1.  **Purpose Governance Check**: The query is blocked if the declared purpose $p$ is not in the set of allowed purposes, $\\mathcal{P}_{\\mathrm{allowed}}$. If $p \\notin \\mathcal{P}_{\\mathrm{allowed}}$, the procedure terminates and returns `False`.\n\n2.  **Zero Consent Rate Check**: This check prevents division by zero in the estimator and ensures that information is ethically available from all necessary population subgroups.\n    a. First, the consent rates $r_{s,p}$ for the given purpose $p$ are computed for each stratum $s \\in \\{0, \\dots, S-1\\}$ using the provided formula: $r_{s,p} = c_{s,p} / n_s$, where $n_s = |\\{ i \\mid s(i)=s \\}|$ and $c_{s,p} = |\\{ i \\mid s(i)=s, C_{i,p}=1 \\}|$.\n    b. Next, the set of strata targeted by the query, $S_{\\mathrm{targeted}} = \\{ s(i) \\mid a_i \\neq 0 \\}$, is identified.\n    c. If there exists any stratum $s' \\in S_{\\mathrm{targeted}}$ for which the consent rate is $r_{s',p} = 0$, the query is blocked and the procedure returns `False`.\n\n3.  **Minimum Group Size Check**: This rule mitigates re-identification risk by ensuring that any released statistic is derived from a sufficiently large group of consenting participants.\n    a. The set of contributing individuals is identified: $I_{p, \\mathbf{a}} = \\{ i \\mid a_i \\neq 0 \\text{ and } C_{i,p} = 1 \\}$.\n    b. If the size of this set is less than the specified threshold, that is, if $|I_{p, \\mathbf{a}}| < k_{\\min}$, the query is blocked and the procedure returns `False`.\n\n4.  **Unbiased Estimation**: If all the preceding checks are passed, the query is permitted. The unbiased estimate $\\widehat{T}_p(\\mathbf{a})$ is computed using the inverse probability weighting formula over the set of contributing individuals $I_{p, \\mathbf{a}}$:\n    $$ \\widehat{T}_p(\\mathbf{a}) = \\sum_{i \\in I_{p, \\mathbf{a}}} \\frac{a_i y_i}{r_{s(i),p}} $$\n    The resulting floating-point number is returned.\n\nThis sequence of checks ensures that all ethical, privacy, and mathematical constraints are satisfied before an estimate is provided. If any rule is violated, the query is securely blocked.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a consent-constrained query answering mechanism for biomedical data.\n    \"\"\"\n    # Fixed dataset and policy parameters\n    N = 8\n    S = 2\n    y = np.array([1.2, 0.5, 2.0, 1.5, 3.0, 2.5, 0.8, 1.1])\n    s = np.array([0, 0, 0, 1, 1, 1, 1, 0])\n    C = np.array([\n        [1, 1, 0, 0],  # i=0\n        [1, 0, 0, 0],  # i=1\n        [0, 1, 0, 0],  # i=2\n        [1, 1, 1, 0],  # i=3\n        [0, 1, 1, 0],  # i=4\n        [1, 0, 0, 0],  # i=5\n        [1, 1, 1, 0],  # i=6\n        [0, 1, 0, 0],  # i=7\n    ])\n    allowed_purposes = {0, 1, 2}\n    k_min = 3\n\n    # Test suite of queries (p, a)\n    test_cases = [\n        (0, np.array([1, 1, 1, 1, 1, 1, 1, 1])),\n        (1, np.array([1, 2, 0, 1, 0, 0.5, 1, 0])),\n        (3, np.array([1, 1, 1, 1, 1, 1, 1, 1])),\n        (0, np.array([0, 0, 1, 0, 0, 0, 0, 0])),\n        (2, np.array([1, 1, 0, 0, 0, 0, 0, 0])),\n    ]\n\n    results = []\n\n    # Pre-compute stratum sizes n_s\n    n_s = np.bincount(s, minlength=S)\n\n    # Pre-compute consent rates r_{s,p} from the problem statement text, not the code's C matrix.\n    # The problem description and the code's hardcoded C matrix are different.\n    # The description text is the source of truth.\n    # Column p=0: (1, 1, 0, 1, 0, 1, 1, 0)\n    # Column p=1: (1, 0, 1, 1, 1, 0, 1, 1)\n    # Column p=2: (0, 0, 0, 1, 1, 0, 1, 0)\n    # Column p=3: (0, 0, 0, 0, 0, 0, 0, 0)\n    C_from_text = np.array([\n        [1, 1, 0, 0], # i=0\n        [1, 0, 0, 0], # i=1\n        [0, 1, 0, 0], # i=2\n        [1, 1, 1, 0], # i=3\n        [0, 1, 1, 0], # i=4\n        [1, 0, 0, 0], # i=5\n        [1, 1, 1, 0], # i=6\n        [0, 1, 0, 0], # i=7\n    ])\n    C_from_text[:,0] = [1, 1, 0, 1, 0, 1, 1, 0]\n    C_from_text[:,1] = [1, 0, 1, 1, 1, 0, 1, 1]\n    C_from_text[:,2] = [0, 0, 0, 1, 1, 0, 1, 0]\n    C_from_text[:,3] = [0, 0, 0, 0, 0, 0, 0, 0]\n    C = C_from_text # Use the matrix derived from the problem text.\n\n    num_purposes = C.shape[1]\n    r_sp = np.zeros((S, num_purposes))\n    for p_idx in range(num_purposes):\n        for s_idx in range(S):\n            if n_s[s_idx] > 0:\n                s_mask = (s == s_idx)\n                c_sp = np.sum(C[s_mask, p_idx])\n                r_sp[s_idx, p_idx] = c_sp / n_s[s_idx]\n\n    for p, a in test_cases:\n        # Step 1: Purpose Governance Check\n        if p not in allowed_purposes:\n            results.append(False)\n            continue\n\n        # Step 2: Zero Consent Rate Check\n        targeted_strata_indices = np.unique(s[a != 0])\n        block_query = False\n        for s_idx in targeted_strata_indices:\n            if r_sp[s_idx, p] == 0:\n                block_query = True\n                break\n        if block_query:\n            results.append(False)\n            continue\n        \n        # Step 3: Minimum Group Size Check\n        consenting_mask = (C[:, p] == 1)\n        targeted_mask = (a != 0)\n        contributing_mask = consenting_mask  targeted_mask\n        num_contributors = np.sum(contributing_mask)\n        \n        if num_contributors  k_min:\n            results.append(False)\n            continue\n\n        # Step 4: Unbiased Estimation\n        estimate = 0.0\n        contributing_indices = np.where(contributing_mask)[0]\n        \n        for i in contributing_indices:\n            stratum_of_i = s[i]\n            consent_rate = r_sp[stratum_of_i, p]\n            # This check is redundant due to step 2, but good for robustness\n            if consent_rate > 0:\n                estimate += (a[i] * y[i]) / consent_rate\n            else:\n                # Should not be reached given the logic in Step 2\n                estimate = \"Error: Division by zero\"\n                break\n        \n        if isinstance(estimate, str): # Error case\n            results.append(False) # Or handle error appropriately\n        else:\n            results.append(estimate)\n            \n    # Format the final output\n    formatted_results = []\n    for res in results:\n        if isinstance(res, bool):\n            formatted_results.append(str(res))\n        else:\n            # Use standard float representation\n            formatted_results.append(str(res))\n            \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4560952"}]}