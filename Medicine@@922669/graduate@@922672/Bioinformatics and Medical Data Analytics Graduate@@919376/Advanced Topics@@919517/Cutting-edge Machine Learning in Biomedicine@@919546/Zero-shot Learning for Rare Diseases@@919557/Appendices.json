{"hands_on_practices": [{"introduction": "To begin, let's explore the fundamental prediction mechanism of a trained Zero-Shot Learning (ZSL) model. This exercise demonstrates how a bilinear compatibility function uses embeddings from patient data and disease descriptions to score potential diagnoses for diseases never seen during training [@problem_id:4618533]. By working through this calculation, you will gain a concrete understanding of how knowledge is transferred from the seen to the unseen.", "problem": "A biobank-based study applies Zero-Shot Learning (ZSL) to prioritize rare diseases that were not present in the training set by leveraging semantic disease descriptions embedded from a curated ontology. In a bilinear compatibility framework, each patient sample $x$ is represented by a feature embedding $\\phi(x) \\in \\mathbb{R}^{d_{x}}$ (e.g., tiered summaries of gene expression and clinical attributes), each disease label $y$ is represented by a semantic embedding $\\psi(y) \\in \\mathbb{R}^{d_{y}}$ (e.g., ontology-derived attributes), and the learned cross-modal mapping is a matrix $W \\in \\mathbb{R}^{d_{x} \\times d_{y}}$. The model scores the compatibility between a patient $x$ and a disease $y$ using the bilinear form\n$$F(x,y) = \\phi(x)^{\\top} W \\,\\psi(y).$$\nFor a held-out patient $x$ and an unseen rare disease set $Y_{\\text{unseen}}=\\{y_{\\mathrm{A}},y_{\\mathrm{B}},y_{\\mathrm{C}},y_{\\mathrm{D}}\\}$, you are given\n$$\\phi(x)=\\begin{pmatrix}1 \\\\ -1 \\\\ 2\\end{pmatrix},\\quad W=\\begin{pmatrix}2 & -1 & 0 & 1 \\\\ 0 & 1 & -2 & 3 \\\\ 1 & 2 & 1 & -1\\end{pmatrix},$$\nand the disease embeddings\n$$\\psi(y_{\\mathrm{A}})=\\begin{pmatrix}1 \\\\ 0 \\\\ 1 \\\\ -1\\end{pmatrix},\\quad \\psi(y_{\\mathrm{B}})=\\begin{pmatrix}0 \\\\ 2 \\\\ -1 \\\\ 0\\end{pmatrix},\\quad \\psi(y_{\\mathrm{C}})=\\begin{pmatrix}1 \\\\ 1 \\\\ 0 \\\\ 1\\end{pmatrix},\\quad \\psi(y_{\\mathrm{D}})=\\begin{pmatrix}-1 \\\\ 0 \\\\ 2 \\\\ 1\\end{pmatrix}.$$\nTasks:\n1. Compute $F(x,y)$ for each $y \\in Y_{\\text{unseen}}$ and select the top-$k$ predictions with $k=2$, ranking by descending score. If there is a tie at the $k$-th position, break ties by lexicographic order $y_{\\mathrm{A}} \\prec y_{\\mathrm{B}} \\prec y_{\\mathrm{C}} \\prec y_{\\mathrm{D}}$.\n2. Using all four scores $\\{F(x,y)\\}_{y \\in Y_{\\text{unseen}}}$, compute the temperature-scaled softmax probability assigned to the top-$1$ predicted disease:\n$$p_{\\text{top}}=\\frac{\\exp\\!\\big(F(x,y_{\\text{top}})/\\tau\\big)}{\\sum\\limits_{y \\in Y_{\\text{unseen}}} \\exp\\!\\big(F(x,y)/\\tau\\big)},$$\nwith temperature parameter $\\tau=4$.\nExpress the final answer as the value of $p_{\\text{top}}$ as a decimal and round to four significant figures. No units are required.", "solution": "Zero-Shot Learning (ZSL) for rare disease prioritization relies on transferring knowledge through shared semantic spaces, where an unseen disease $y$ is linked to a patient $x$ via a compatibility function. In the bilinear model, the compatibility is given by $F(x,y)=\\phi(x)^{\\top}W\\psi(y)$. To compute $F(x,y)$ efficiently for multiple $y$, it is convenient to first compute the intermediate vector $s(x)^{\\top}=\\phi(x)^{\\top}W \\in \\mathbb{R}^{d_{y}}$, and then take a dot product with each $\\psi(y)$.\n\nStep 1: Compute $s(x)^{\\top}=\\phi(x)^{\\top}W$.\nGiven $\\phi(x)=\\begin{pmatrix}1 \\\\ -1 \\\\ 2\\end{pmatrix}$ and\n$$W=\\begin{pmatrix}2 & -1 & 0 & 1 \\\\ 0 & 1 & -2 & 3 \\\\ 1 & 2 & 1 & -1\\end{pmatrix},$$\nwe have\n$$s(x)^{\\top}=\\phi(x)^{\\top}W=\\big(1\\cdot \\text{row}_{1}(W)\\big)+\\big((-1)\\cdot \\text{row}_{2}(W)\\big)+\\big(2\\cdot \\text{row}_{3}(W)\\big).$$\nCompute each weighted row:\n- $1\\cdot \\text{row}_{1}(W)=(2,\\,-1,\\,0,\\,1)$,\n- $(-1)\\cdot \\text{row}_{2}(W)=(0,\\,-1,\\,2,\\,-3)$,\n- $2\\cdot \\text{row}_{3}(W)=(2,\\,4,\\,2,\\,-2)$.\nSumming componentwise,\n$$s(x)^{\\top}=(2+0+2,\\,-1-1+4,\\,0+2+2,\\,1-3-2)=(4,\\,2,\\,4,\\,-4).$$\n\nStep 2: Compute $F(x,y)=s(x)^{\\top}\\psi(y)$ for each unseen disease.\n- For $y_{\\mathrm{A}}$ with $\\psi(y_{\\mathrm{A}})=\\begin{pmatrix}1 \\\\ 0 \\\\ 1 \\\\ -1\\end{pmatrix}$,\n$$F(x,y_{\\mathrm{A}})=(4,2,4,-4)\\cdot(1,0,1,-1)=4\\cdot 1+2\\cdot 0+4\\cdot 1+(-4)\\cdot(-1)=4+0+4+4=12.$$\n- For $y_{\\mathrm{B}}$ with $\\psi(y_{\\mathrm{B}})=\\begin{pmatrix}0 \\\\ 2 \\\\ -1 \\\\ 0\\end{pmatrix}$,\n$$F(x,y_{\\mathrm{B}})=(4,2,4,-4)\\cdot(0,2,-1,0)=4\\cdot 0+2\\cdot 2+4\\cdot(-1)+(-4)\\cdot 0=0+4-4+0=0.$$\n- For $y_{\\mathrm{C}}$ with $\\psi(y_{\\mathrm{C}})=\\begin{pmatrix}1 \\\\ 1 \\\\ 0 \\\\ 1\\end{pmatrix}$,\n$$F(x,y_{\\mathrm{C}})=(4,2,4,-4)\\cdot(1,1,0,1)=4\\cdot 1+2\\cdot 1+4\\cdot 0+(-4)\\cdot 1=4+2+0-4=2.$$\n- For $y_{\\mathrm{D}}$ with $\\psi(y_{\\mathrm{D}})=\\begin{pmatrix}-1 \\\\ 0 \\\\ 2 \\\\ 1\\end{pmatrix}$,\n$$F(x,y_{\\mathrm{D}})=(4,2,4,-4)\\cdot(-1,0,2,1)=4\\cdot(-1)+2\\cdot 0+4\\cdot 2+(-4)\\cdot 1=-4+0+8-4=0.$$\n\nThus the scores are\n$$F(x,y_{\\mathrm{A}})=12,\\quad F(x,y_{\\mathrm{B}})=0,\\quad F(x,y_{\\mathrm{C}})=2,\\quad F(x,y_{\\mathrm{D}})=0.$$\n\nStep 3: Select the top-$k$ predictions with $k=2$.\nOrdering by descending score gives $y_{\\mathrm{A}}$ (score $12$), then $y_{\\mathrm{C}}$ (score $2$), then a tie between $y_{\\mathrm{B}}$ and $y_{\\mathrm{D}}$ (both score $0$). The top-$2$ are therefore $y_{\\mathrm{A}}$ and $y_{\\mathrm{C}}$.\n\nStep 4: Compute the temperature-scaled softmax probability for the top-$1$ label with $\\tau=4$.\nThe top-$1$ label is $y_{\\mathrm{A}}$. The temperature-scaled softmax assigns\n$$p_{\\text{top}}=\\frac{\\exp\\!\\big(F(x,y_{\\mathrm{A}})/4\\big)}{\\exp\\!\\big(F(x,y_{\\mathrm{A}})/4\\big)+\\exp\\!\\big(F(x,y_{\\mathrm{B}})/4\\big)+\\exp\\!\\big(F(x,y_{\\mathrm{C}})/4\\big)+\\exp\\!\\big(F(x,y_{\\mathrm{D}})/4\\big)}.$$\nSubstitute the scores:\n$$p_{\\text{top}}=\\frac{\\exp(12/4)}{\\exp(12/4)+\\exp(0/4)+\\exp(2/4)+\\exp(0/4)}=\\frac{\\exp(3)}{\\exp(3)+1+\\exp(0.5)+1}.$$\nEvaluate the exponentials symbolically first, then numerically:\n$$\\exp(3)\\approx 20.0855369232,\\quad \\exp(0.5)\\approx 1.6487212707.$$\nThe denominator is\n$$20.0855369232+1+1.6487212707+1=23.7342581939.$$\nTherefore,\n$$p_{\\text{top}}\\approx \\frac{20.0855369232}{23.7342581939}\\approx 0.846267735\\ldots.$$\n\nStep 5: Round to four significant figures as a decimal.\nRounding $0.846267735\\ldots$ to four significant figures yields $0.8463$.", "answer": "$$\\boxed{0.8463}$$", "id": "4618533"}, {"introduction": "Moving beyond static embeddings, we can leverage the rich relationships between diseases and phenotypes using Graph Neural Networks (GNNs). This practice will guide you through the process of computing disease embeddings by aggregating information from connected phenotype nodes in a bipartite graph [@problem_id:4618399]. This approach is powerful for ZSL, as it allows the model to create nuanced representations for unseen diseases based on their known clinical manifestations.", "problem": "You are asked to formalize and compute one-layer and two-layer Graph Neural Network (GNN) updates for a toy disease-phenotype bipartite graph to obtain embeddings for an unseen disease and to compare the embeddings produced by the two depths. The broader context is zero-shot learning for rare diseases, where unseen diseases have no labels and rely on shared phenotype descriptions to transfer knowledge. The problem is structured purely in terms of linear algebra with explicit matrices, and all computations are to be performed numerically.\n\nThe fundamental base to use consists of the following well-tested definitions: a bipartite graph encoded by an adjacency matrix, row-normalized neighbor aggregation, and the Rectified Linear Unit (ReLU) nonlinearity. Let there be $N_d$ diseases indexed by $i \\in \\{1,\\dots,N_d\\}$ and $N_p$ phenotypes indexed by $j \\in \\{1,\\dots,N_p\\}$. Let $A \\in \\{0,1\\}^{N_d \\times N_p}$ denote a disease-to-phenotype adjacency matrix, $X_p \\in \\mathbb{R}^{N_p \\times F_p}$ denote phenotype feature vectors, and $W_1 \\in \\mathbb{R}^{F_p \\times F_o}$, $U \\in \\mathbb{R}^{F_p \\times F_h}$, $W_2 \\in \\mathbb{R}^{F_h \\times F_o}$ denote weight matrices. Define the disease row-degree matrix $D_d \\in \\mathbb{R}^{N_d \\times N_d}$ by $D_d(i,i) = \\sum_{j=1}^{N_p} A(i,j)$. Define the row-normalized adjacency $\\tilde{A} = D_d^{-1} A$ with the convention that if $D_d(i,i) = 0$ then the $i$-th row of $\\tilde{A}$ is the all-zero row. Define the Rectified Linear Unit (ReLU) elementwise nonlinearity $\\sigma(x) = \\max\\{0,x\\}$.\n\nYou must implement the following updates:\n\n- One-layer disease embeddings: $H_d^{(1)} = \\sigma\\big(\\tilde{A} X_p W_1\\big)$.\n- Two-layer disease embeddings via a hidden phenotype transformation: first $Z_p = \\sigma\\big(X_p U\\big)$, then $H_d^{(2)} = \\sigma\\big(\\tilde{A} Z_p W_2\\big)$.\n\nIn a zero-shot setup, unseen diseases have no direct label information. In this toy setting, we model this by not using any disease feature vectors and relying solely on phenotype features $X_p$ and the graph structure $A$.\n\nYou must compare the embeddings for a fixed unseen disease index $i^\\star$ across the one-layer and two-layer models using:\n- Cosine similarity: $\\mathrm{cos}(a,b) = \\frac{a^\\top b}{\\lVert a\\rVert_2 \\lVert b\\rVert_2}$, with the convention $\\mathrm{cos}(a,b) = 0$ if $\\lVert a\\rVert_2 = 0$ or $\\lVert b\\rVert_2 = 0$.\n- Euclidean distance: $\\lVert a-b\\rVert_2$.\n\nAll operations are purely numerical; no physical units are involved. Angles do not appear, so no angle unit is needed. Percentages do not appear.\n\nUse the following fixed dimensions and matrices for all test cases:\n- Number of diseases $N_d = 3$, number of phenotypes $N_p = 4$, phenotype feature dimension $F_p = 3$, hidden dimension $F_h = 2$, output dimension $F_o = 2$.\n- Phenotype features $X_p \\in \\mathbb{R}^{4 \\times 3}$:\n$$\nX_p =\n\\begin{bmatrix}\n1 & 0 & -1 \\\\\n0.5 & 1 & 0 \\\\\n0 & -0.5 & 1 \\\\\n1 & 1 & 1\n\\end{bmatrix}.\n$$\n- One-layer weight $W_1 \\in \\mathbb{R}^{3 \\times 2}$:\n$$\nW_1 =\n\\begin{bmatrix}\n0.5 & 0 \\\\\n0 & 1 \\\\\n-0.5 & 0.5\n\\end{bmatrix}.\n$$\n- Phenotype hidden weight $U \\in \\mathbb{R}^{3 \\times 2}$:\n$$\nU =\n\\begin{bmatrix}\n1 & -1 \\\\\n0.5 & 0 \\\\\n0 & 1\n\\end{bmatrix}.\n$$\n- Two-layer output weight $W_2 \\in \\mathbb{R}^{2 \\times 2}$:\n$$\nW_2 =\n\\begin{bmatrix}\n1 & 0.5 \\\\\n-0.5 & 1\n\\end{bmatrix}.\n$$\n\nFix the unseen disease index as $i^\\star = 3$ (that is, the third disease).\n\nImplement the above for the following test suite of three adjacency matrices $A$:\n\n- Test case $1$ (general case):\n$$\nA^{(1)} =\n\\begin{bmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n1 & 0 & 0 & 1\n\\end{bmatrix}.\n$$\n\n- Test case $2$ (single-neighbor boundary for the unseen disease):\n$$\nA^{(2)} =\n\\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}.\n$$\n\n- Test case $3$ (edge case: unseen disease with zero degree):\n$$\nA^{(3)} =\n\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}.\n$$\n\nFor each test case $t \\in \\{1,2,3\\}$, compute the one-layer embedding vector $h^{(1)}_{i^\\star} \\in \\mathbb{R}^{2}$ and the two-layer embedding vector $h^{(2)}_{i^\\star} \\in \\mathbb{R}^{2}$ for disease $i^\\star$, then compute:\n- The cosine similarity between $h^{(1)}_{i^\\star}$ and $h^{(2)}_{i^\\star}$ with the zero-norm convention above.\n- The Euclidean distance between $h^{(1)}_{i^\\star}$ and $h^{(2)}_{i^\\star}$.\n\nYour program must output a single line that aggregates the results for the three test cases as a comma-separated list enclosed in square brackets, where each test case result is a two-element list $[\\mathrm{cos}, \\mathrm{dist}]$ with both numbers rounded to $6$ decimal places. For example, the output format must be exactly like\n\"[$[x_1,y_1],[x_2,y_2],[x_3,y_3]$]\" with no spaces, where each $x_t$ and $y_t$ is a decimal float rounded to $6$ places.\n\nNo user input is required. All data is specified above. The code must be deterministic and self-contained. The outputs for each test case must be floats.", "solution": "The problem is valid. It presents a well-defined computational task based on established principles of Graph Neural Networks (GNNs) and linear algebra. All necessary parameters, matrices, and functions are provided, ensuring the problem is self-contained and permits a unique, verifiable solution.\n\nThe objective is to compute and compare disease embeddings from a one-layer and a two-layer GNN for a specific \"unseen\" disease, identified by index $i^\\star=3$. The GNN architecture operates on a bipartite graph connecting diseases to phenotypes. The core mechanism is message passing, where a disease node's representation is updated by aggregating feature vectors from its associated phenotype nodes.\n\nThe following data and definitions are provided:\n- Number of diseases $N_d = 3$, phenotypes $N_p = 4$.\n- Phenotype feature dimension $F_p = 3$, hidden dimension $F_h = 2$, and output dimension $F_o = 2$.\n- Phenotype features $X_p \\in \\mathbb{R}^{4 \\times 3}$:\n$$\nX_p =\n\\begin{bmatrix}\n1 & 0 & -1 \\\\\n0.5 & 1 & 0 \\\\\n0 & -0.5 & 1 \\\\\n1 & 1 & 1\n\\end{bmatrix}\n$$\n- Weight matrices $W_1 \\in \\mathbb{R}^{3 \\times 2}$, $U \\in \\mathbb{R}^{3 \\times 2}$, and $W_2 \\in \\mathbb{R}^{2 \\times 2}$:\n$$\nW_1 =\n\\begin{bmatrix}\n0.5 & 0 \\\\\n0 & 1 \\\\\n-0.5 & 0.5\n\\end{bmatrix}, \\quad\nU =\n\\begin{bmatrix}\n1 & -1 \\\\\n0.5 & 0 \\\\\n0 & 1\n\\end{bmatrix}, \\quad\nW_2 =\n\\begin{bmatrix}\n1 & 0.5 \\\\\n-0.5 & 1\n\\end{bmatrix}\n$$\n- The GNN update rules utilize a row-normalized adjacency matrix $\\tilde{A}=D_d^{-1}A$ and the element-wise ReLU activation function $\\sigma(x) = \\max\\{0,x\\}$.\n- One-layer disease embeddings: $H_d^{(1)} = \\sigma\\big(\\tilde{A} X_p W_1\\big)$.\n- Two-layer disease embeddings: $H_d^{(2)} = \\sigma\\big(\\tilde{A} \\sigma(X_p U) W_2\\big)$.\n\nTo streamline calculations, we can pre-compute the transformed phenotype features, as they do not depend on the adjacency matrix $A$.\n\nFor the one-layer model, the transformed phenotype features are $Y_p = X_p W_1$:\n$$\nY_p = \\begin{bmatrix}\n1 & 0 & -1 \\\\\n0.5 & 1 & 0 \\\\\n0 & -0.5 & 1 \\\\\n1 & 1 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n0.5 & 0 \\\\\n0 & 1 \\\\\n-0.5 & 0.5\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 & -0.5 \\\\\n0.25 & 1 \\\\\n-0.5 & 0 \\\\\n0 & 1.5\n\\end{bmatrix}\n$$\nFor the two-layer model, the intermediate hidden phenotype features are $Z_p = \\sigma(X_p U)$:\n$$\nX_p U = \\begin{bmatrix}\n1 & 0 & -1 \\\\\n0.5 & 1 & 0 \\\\\n0 & -0.5 & 1 \\\\\n1 & 1 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & -1 \\\\\n0.5 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 & -2 \\\\\n1 & -0.5 \\\\\n-0.25 & 1 \\\\\n1.5 & 0\n\\end{bmatrix}\n$$\nApplying the ReLU activation, we get:\n$$\nZ_p = \\sigma(X_p U) =\n\\begin{bmatrix}\n1 & 0 \\\\\n1 & 0 \\\\\n0 & 1 \\\\\n1.5 & 0\n\\end{bmatrix}\n$$\n\nWe will now proceed with each test case for the chosen disease index $i^\\star = 3$.\n\n### Test Case 1\nThe adjacency matrix is $A^{(1)} = \\begin{bmatrix} 1 & 1 & 0 & 0 \\\\ 0 & 1 & 1 & 0 \\\\ 1 & 0 & 0 & 1 \\end{bmatrix}$.\nThe disease degrees (row sums) are $d_1=2$, $d_2=2$, $d_3=2$. The normalized adjacency matrix is:\n$$\n\\tilde{A}^{(1)} = \\begin{bmatrix} 1/2 & 1/2 & 0 & 0 \\\\ 0 & 1/2 & 1/2 & 0 \\\\ 1/2 & 0 & 0 & 1/2 \\end{bmatrix} = \\begin{bmatrix} 0.5 & 0.5 & 0 & 0 \\\\ 0 & 0.5 & 0.5 & 0 \\\\ 0.5 & 0 & 0 & 0.5 \\end{bmatrix}\n$$\nThe embedding for disease $i^\\star=3$ is determined by the 3rd row of $\\tilde{A}^{(1)}$.\n- **One-layer embedding $h^{(1)}_{3}$**:\n$$ (\\tilde{A}^{(1)} Y_p)_{3,:} = \\begin{bmatrix} 0.5 & 0 & 0 & 0.5 \\end{bmatrix} Y_p = 0.5 \\times \\begin{bmatrix} 1 & -0.5 \\end{bmatrix} + 0.5 \\times \\begin{bmatrix} 0 & 1.5 \\end{bmatrix} = \\begin{bmatrix} 0.5 & 0.5 \\end{bmatrix} $$\n$$ h^{(1)}_{3} = \\sigma(\\begin{bmatrix} 0.5 & 0.5 \\end{bmatrix}) = \\begin{bmatrix} 0.5 & 0.5 \\end{bmatrix} $$\n- **Two-layer embedding $h^{(2)}_{3}$**:\n$$ (\\tilde{A}^{(1)} Z_p)_{3,:} = \\begin{bmatrix} 0.5 & 0 & 0 & 0.5 \\end{bmatrix} Z_p = 0.5 \\times \\begin{bmatrix} 1 & 0 \\end{bmatrix} + 0.5 \\times \\begin{bmatrix} 1.5 & 0 \\end{bmatrix} = \\begin{bmatrix} 1.25 & 0 \\end{bmatrix} $$\n$$ (\\tilde{A}^{(1)} Z_p W_2)_{3,:} = \\begin{bmatrix} 1.25 & 0 \\end{bmatrix} W_2 = \\begin{bmatrix} 1.25 & 0 \\end{bmatrix} \\begin{bmatrix} 1 & 0.5 \\\\ -0.5 & 1 \\end{bmatrix} = \\begin{bmatrix} 1.25 & 0.625 \\end{bmatrix} $$\n$$ h^{(2)}_{3} = \\sigma(\\begin{bmatrix} 1.25 & 0.625 \\end{bmatrix}) = \\begin{bmatrix} 1.25 & 0.625 \\end{bmatrix} $$\n- **Comparison**: Let $a = h^{(1)}_{3}$ and $b = h^{(2)}_{3}$.\n    - Cosine Similarity: $\\mathrm{cos}(a, b) = \\frac{a \\cdot b}{\\|a\\| \\|b\\|} = \\frac{0.5(1.25) + 0.5(0.625)}{\\sqrt{0.5^2+0.5^2}\\sqrt{1.25^2+0.625^2}} = \\frac{0.9375}{\\sqrt{0.5}\\sqrt{1.953125}} \\approx 0.948683$.\n    - Euclidean Distance: $\\|a - b\\|_2 = \\sqrt{(0.5-1.25)^2+(0.5-0.625)^2} = \\sqrt{(-0.75)^2+(-0.125)^2} = \\sqrt{0.578125} \\approx 0.760345$.\n\n### Test Case 2\nThe adjacency matrix is $A^{(2)} = \\begin{bmatrix} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}$.\nThe disease degrees are $d_1=1$, $d_2=1$, $d_3=1$. Thus, $\\tilde{A}^{(2)} = A^{(2)}$.\nThe 3rd row of $\\tilde{A}^{(2)}$ is $\\begin{bmatrix} 0 & 0 & 0 & 1 \\end{bmatrix}$.\n- **One-layer embedding $h^{(1)}_{3}$**:\n$$ (\\tilde{A}^{(2)} Y_p)_{3,:} = \\begin{bmatrix} 0 & 0 & 0 & 1 \\end{bmatrix} Y_p = \\text{4th row of } Y_p = \\begin{bmatrix} 0 & 1.5 \\end{bmatrix} $$\n$$ h^{(1)}_{3} = \\sigma(\\begin{bmatrix} 0 & 1.5 \\end{bmatrix}) = \\begin{bmatrix} 0 & 1.5 \\end{bmatrix} $$\n- **Two-layer embedding $h^{(2)}_{3}$**:\n$$ (\\tilde{A}^{(2)} Z_p)_{3,:} = \\begin{bmatrix} 0 & 0 & 0 & 1 \\end{bmatrix} Z_p = \\text{4th row of } Z_p = \\begin{bmatrix} 1.5 & 0 \\end{bmatrix} $$\n$$ (\\tilde{A}^{(2)} Z_p W_2)_{3,:} = \\begin{bmatrix} 1.5 & 0 \\end{bmatrix} \\begin{bmatrix} 1 & 0.5 \\\\ -0.5 & 1 \\end{bmatrix} = \\begin{bmatrix} 1.5 & 0.75 \\end{bmatrix} $$\n$$ h^{(2)}_{3} = \\sigma(\\begin{bmatrix} 1.5 & 0.75 \\end{bmatrix}) = \\begin{bmatrix} 1.5 & 0.75 \\end{bmatrix} $$\n- **Comparison**: Let $a = h^{(1)}_{3}$ and $b = h^{(2)}_{3}$.\n    - Cosine Similarity: $\\mathrm{cos}(a, b) = \\frac{0(1.5)+1.5(0.75)}{\\sqrt{0^2+1.5^2}\\sqrt{1.5^2+0.75^2}} = \\frac{1.125}{1.5\\sqrt{2.8125}} \\approx 0.447214$.\n    - Euclidean Distance: $\\|a - b\\|_2 = \\sqrt{(0-1.5)^2+(1.5-0.75)^2} = \\sqrt{(-1.5)^2+0.75^2} = \\sqrt{2.8125} \\approx 1.677051$.\n\n### Test Case 3\nThe adjacency matrix is $A^{(3)} = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}$.\nThe disease degrees are $d_1=1$, $d_2=2$, $d_3=0$. For disease $i=3$, the degree is $0$.\nBy convention, the 3rd row of the normalized adjacency matrix $\\tilde{A}^{(3)}$ is an all-zero vector:\n$$\n\\tilde{A}^{(3)} = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0.5 & 0.5 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\n$$\n- **One-layer embedding $h^{(1)}_{3}$**:\nSince the 3rd row of $\\tilde{A}^{(3)}$ is zero, the aggregation results in a zero vector.\n$$ (\\tilde{A}^{(3)} Y_p)_{3,:} = \\begin{bmatrix} 0 & 0 & 0 & 0 \\end{bmatrix} Y_p = \\begin{bmatrix} 0 & 0 \\end{bmatrix} $$\n$$ h^{(1)}_{3} = \\sigma(\\begin{bmatrix} 0 & 0 \\end{bmatrix}) = \\begin{bmatrix} 0 & 0 \\end{bmatrix} $$\n- **Two-layer embedding $h^{(2)}_{3}$**:\nSimilarly, the aggregation is a zero vector.\n$$ (\\tilde{A}^{(3)} Z_p W_2)_{3,:} = (\\begin{bmatrix} 0 & 0 & 0 & 0 \\end{bmatrix} Z_p) W_2 = \\begin{bmatrix} 0 & 0 \\end{bmatrix} W_2 = \\begin{bmatrix} 0 & 0 \\end{bmatrix} $$\n$$ h^{(2)}_{3} = \\sigma(\\begin{bmatrix} 0 & 0 \\end{bmatrix}) = \\begin{bmatrix} 0 & 0 \\end{bmatrix} $$\n- **Comparison**: Let $a = h^{(1)}_{3}$ and $b = h^{(2)}_{3}$. Both are the zero vector.\n    - Cosine Similarity: Since $\\|a\\| = 0$ and $\\|b\\| = 0$, by convention, $\\mathrm{cos}(a, b) = 0$.\n    - Euclidean Distance: $\\|a - b\\|_2 = \\|\\begin{bmatrix} 0 & 0 \\end{bmatrix}\\|_2 = 0$.\n\nThe final results are collected and formatted as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares one-layer and two-layer GNN embeddings for a toy\n    disease-phenotype graph based on the problem specification.\n    \"\"\"\n    # Define fixed matrices and parameters from the problem statement.\n    Xp = np.array([\n        [1.0, 0.0, -1.0],\n        [0.5, 1.0, 0.0],\n        [0.0, -0.5, 1.0],\n        [1.0, 1.0, 1.0]\n    ])\n\n    W1 = np.array([\n        [0.5, 0.0],\n        [0.0, 1.0],\n        [-0.5, 0.5]\n    ])\n\n    U = np.array([\n        [1.0, -1.0],\n        [0.5, 0.0],\n        [0.0, 1.0]\n    ])\n\n    W2 = np.array([\n        [1.0, 0.5],\n        [-0.5, 1.0]\n    ])\n\n    # Index of the unseen disease (third disease, so index 2 in 0-based system).\n    i_star = 2\n    \n    # Test cases for the adjacency matrix A.\n    test_cases = [\n        # Test case 1\n        np.array([\n            [1, 1, 0, 0],\n            [0, 1, 1, 0],\n            [1, 0, 0, 1]\n        ]),\n        # Test case 2\n        np.array([\n            [0, 1, 0, 0],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1]\n        ]),\n        # Test case 3\n        np.array([\n            [1, 0, 0, 0],\n            [0, 1, 1, 0],\n            [0, 0, 0, 0]\n        ])\n    ]\n\n    results = []\n\n    # ReLU activation function\n    relu = lambda x: np.maximum(0, x)\n\n    # Pre-compute phenotype feature transformations\n    Yp = Xp @ W1\n    Zp = relu(Xp @ U)\n\n    for A in test_cases:\n        # 1. Compute the normalized adjacency matrix A_tilde\n        degrees = A.sum(axis=1)\n        A_tilde = np.zeros_like(A, dtype=float)\n        for i in range(A.shape[0]):\n            if degrees[i] > 0:\n                A_tilde[i, :] = A[i, :] / degrees[i]\n        \n        # 2. Compute one-layer embeddings\n        H1 = relu(A_tilde @ Yp)\n        h1_istar = H1[i_star]\n\n        # 3. Compute two-layer embeddings\n        H2 = relu((A_tilde @ Zp) @ W2)\n        h2_istar = H2[i_star]\n        \n        # 4. Compare embeddings\n        norm1 = np.linalg.norm(h1_istar)\n        norm2 = np.linalg.norm(h2_istar)\n\n        # Cosine similarity with zero-norm convention\n        if norm1 == 0 or norm2 == 0:\n            cos_sim = 0.0\n        else:\n            cos_sim = np.dot(h1_istar, h2_istar) / (norm1 * norm2)\n\n        # Euclidean distance\n        euclidean_dist = np.linalg.norm(h1_istar - h2_istar)\n        \n        results.append([cos_sim, euclidean_dist])\n\n    # Format the output string as per the requirements.\n    result_strings = [f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "4618399"}, {"introduction": "A model's true value is only revealed through rigorous evaluation, a step where subtle errors can lead to dangerously inflated performance metrics. This final practice addresses the critical issue of data leakage, a common pitfall in medical datasets where information from the evaluation set inadvertently contaminates the training process [@problem_id:4618537]. You will learn to recalculate key performance metrics like AUROC and AUPRC after removing leaked data, providing a more realistic assessment of a model's generalizability.", "problem": "You are given a binary evaluation scenario for zero-shot learning in rare disease phenotyping. A zero-shot classifier assigns to each patient a similarity score for a target rare disease using semantic alignment between patient phenotype embeddings and disease attribute embeddings. For a set of patients indexed by unique identifiers, suppose you have tuples $(p_i, y_i, s_i)$ where $p_i$ is a unique integer identifier for patient $i$, $y_i \\in \\{0,1\\}$ is the ground-truth disease indicator, and $s_i \\in [0,1]$ is the classifier-assigned similarity score for the target disease. A subset of patient identifiers $L$ is known to be leaked (for example, data leakage due to overlap in feature construction across development and evaluation sets). You must compute the original metrics using the full set and the corrected metrics after removing all patients in $L$, and then quantify the change.\n\nFoundational base and conventions:\n- Use the standard definition of the Area Under the Receiver Operating Characteristic (AUROC), which is the probability that a randomly chosen positive instance receives a higher score than a randomly chosen negative instance, with ties counted as one half. When there are no positives or no negatives, return the non-informative baseline value $0.5$ by convention.\n- Use the standard definition of the Area Under the Precision–Recall curve (AUPRC), interpreted as the Riemann–Stieltjes integral of precision with respect to recall over decreasing-score thresholds, computed by grouping all items at identical scores, updating true positives and false positives cumulatively per unique score, and summing increments in recall times the precision at the end of each score group. When there are no positives, return $0.0$ by convention. When there are no negatives but at least one positive, the AUPRC equals $1.0$.\n- All computations are unitless. Angles are not involved. Percentages, when conceptually referenced, must be expressed as decimals.\n\nAlgorithmic tasks to be implemented for each test case:\n1. Given the full set $I$ of tuples $(p_i,y_i,s_i)$ and the leaked identifier set $L \\subseteq \\{p_i\\}$, define the filtered index set $I' = \\{(p_i,y_i,s_i) \\in I : p_i \\notin L\\}$.\n2. Compute the original AUROC and AUPRC on $I$.\n3. Compute the corrected AUROC and AUPRC on $I'$.\n4. Report the change as $\\Delta \\text{AUROC} = \\text{AUROC}_{\\text{corrected}} - \\text{AUROC}_{\\text{original}}$ and $\\Delta \\text{AUPRC} = \\text{AUPRC}_{\\text{corrected}} - \\text{AUPRC}_{\\text{original}}$.\n\nTest suite:\nFor each case below, the data are given as an ordered list of triples $[(p_i,y_i,s_i)]$ and a leaked set $L$.\n\n- Case A (happy path with mixed leakage):\n  - Data $I_A$: $[(1,1,0.90),(2,1,0.85),(3,1,0.80),(4,0,0.70),(5,0,0.60),(6,0,0.55),(7,1,0.50),(8,0,0.40),(9,0,0.30),(10,1,0.20)]$.\n  - Leaked set $L_A$: $\\{3,7\\}$.\n\n- Case B (boundary case with all positives leaked):\n  - Data $I_B$: $[(11,1,0.90),(12,1,0.80),(13,0,0.70),(14,0,0.60),(15,0,0.40),(16,0,0.20)]$.\n  - Leaked set $L_B$: $\\{11,12\\}$.\n\n- Case C (edge case with ties and leaked negatives):\n  - Data $I_C$: $[(17,1,0.70),(18,0,0.70),(19,1,0.60),(20,0,0.60),(21,1,0.60),(22,0,0.50),(23,0,0.40),(24,1,0.30)]$.\n  - Leaked set $L_C$: $\\{18,22\\}$.\n\nRequired final output format:\n- Your program must compute, for each case in the order A, B, C, a list $[\\text{AUROC}_{\\text{corrected}}, \\text{AUPRC}_{\\text{corrected}}, \\Delta \\text{AUROC}, \\Delta \\text{AUPRC}]$, rounding each value to $4$ decimal places.\n- Aggregate the three per-case lists into a single list and print exactly one line containing this outer list in a comma-separated format enclosed in square brackets, for example $[[a_1,a_2,a_3,a_4],[b_1,b_2,b_3,b_4],[c_1,c_2,c_3,c_4]]$ with each $a_k, b_k, c_k$ replaced by the computed decimals formatted with exactly $4$ digits after the decimal point.\n\nScientific realism and zero-shot context clarification:\n- The task assumes that $s_i$ arises from zero-shot semantic matching between patient phenotype vectors and disease descriptors. However, you are not asked to model or compute $s_i$; treat $s_i$ as given.\n- The leakage removal simulates correcting an evaluation protocol in a rare disease setting where data scarcity and semantic generalization make leakage particularly harmful.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[[0.1234,0.5678,-0.1111,0.2222],[...],[...]]\").", "solution": "The problem statement has been critically validated and is deemed **valid**.\n\n**1. Problem Validation**\n\n*   **Step 1: Extracted Givens**\n    *   **Data:** For a set of patients, tuples of the form $(p_i, y_i, s_i)$ are provided, where $p_i$ is a unique integer patient identifier, $y_i \\in \\{0,1\\}$ is the ground-truth disease indicator ($1$ for positive, $0$ for negative), and $s_i \\in [0,1]$ is a classifier-assigned similarity score.\n    *   **Leakage Set:** A subset of patient identifiers, $L$, is specified as \"leaked\".\n    *   **AUROC Definition:** The Area Under the Receiver Operating Characteristic curve is defined as the probability that a randomly chosen positive instance receives a higher score than a randomly chosen negative instance. Ties are counted as one half. If no positives or no negatives exist, AUROC is $0.5$.\n    *   **AUPRC Definition:** The Area Under the Precision-Recall curve is defined as the Riemann-Stieltjes integral of precision with respect to recall. It is to be computed by grouping items at identical scores, updating true/false positives cumulatively, and summing the product of precision and the increment in recall for each group. If no positives exist, AUPRC is $0.0$. If no negatives exist (but at least one positive does), AUPRC is $1.0$.\n    *   **Tasks:**\n        1.  Define the full set $I$ and the filtered set $I' = \\{(p_i,y_i,s_i) \\in I : p_i \\notin L\\}$.\n        2.  Compute original AUROC and AUPRC on $I$.\n        3.  Compute corrected AUROC and AUPRC on $I'$.\n        4.  Report the changes: $\\Delta \\text{AUROC} = \\text{AUROC}_{\\text{corrected}} - \\text{AUROC}_{\\text{original}}$ and $\\Delta \\text{AUPRC} = \\text{AUPRC}_{\\text{corrected}} - \\text{AUPRC}_{\\text{original}}$.\n    *   **Test Cases:** Three specific cases (A, B, C) with data and leakage sets are provided.\n    *   **Output Format:** A nested list of results, with each numerical value rounded to $4$ decimal places.\n\n*   **Step 2: Validation Using Extracted Givens**\n    *   **Scientifically Grounded:** The problem is firmly rooted in standard machine learning evaluation practices. AUROC and AUPRC are fundamental metrics for binary classification. Data leakage is a well-understood and critical issue in model evaluation. The context of zero-shot learning for rare diseases is a realistic application area in bioinformatics.\n    *   **Well-Posed:** The problem provides all necessary data and explicit, standard definitions for the required metrics, including the handling of edge cases. The tasks are unambiguous and lead to a unique, computable solution.\n    *   **Objective:** The problem is stated in precise, quantitative terms. There are no subjective or opinion-based components.\n    *   **Completeness and Consistency:** The problem is self-contained. All data for the test cases are provided, and the algorithmic instructions are consistent with the definitions.\n    *   **Realism:** The scenario of evaluating a classifier and correcting for data leakage is a practical and common task in applied machine learning.\n    *   **Conclusion:** The problem does not violate any of the invalidity criteria. It is a well-defined, scientifically sound computational problem.\n\n*   **Step 3: Verdict and Action**\n    *   The problem is valid. A complete solution will be provided.\n\n**2. Principles and Algorithmic Design**\n\nThe core of this problem is the computation and comparison of two standard classification metrics, AUROC and AUPRC, on two different datasets: the original set $I$ and a corrected set $I'$ from which leaked data points have been removed.\n\n**AUROC (Area Under the Receiver Operating Characteristic Curve)**\nThe problem specifies the probabilistic definition of AUROC. Let $S_P$ be the set of scores for positive instances and $S_N$ be the set of scores for negative instances. Let $N_P = |S_P|$ and $N_N = |S_N|$. The AUROC is the probability that a score $s_p$ drawn randomly from $S_P$ is greater than a score $s_n$ drawn randomly from $S_N$. This can be computed by enumerating all $(s_p, s_n)$ pairs:\n$$\n\\text{AUROC} = \\frac{1}{N_P N_N} \\sum_{s_p \\in S_P} \\sum_{s_n \\in S_N} \\mathbb{I}(s_p, s_n)\n$$\nwhere $\\mathbb{I}(s_p, s_n)$ is a comparison function:\n$$\n\\mathbb{I}(s_p, s_n) = \\begin{cases} 1 & \\text{if } s_p > s_n \\\\ 0.5 & \\text{if } s_p = s_n \\\\ 0 & \\text{if } s_p < s_n \\end{cases}\n$$\nIn accordance with the problem's convention, if $N_P=0$ or $N_N=0$, the AUROC is taken to be $0.5$. The algorithm will first segregate the instances into positive and negative sets and then implement the double summation above.\n\n**AUPRC (Area Under the Precision-Recall Curve)**\nThe problem provides a specific computational procedure for AUPRC, which corresponds to calculating the area under the PR curve where precision is assumed to be constant between two consecutive recall points. Let the set of all instances be sorted in descending order of their scores. The algorithm proceeds by iterating through unique score values (thresholds), from highest to lowest.\n\nLet the total number of positive instances be $N_P$ and negative instances be $N_N$. At any point in the process, let the cumulative count of true positives be $TP$ and false positives be $FP$. Precision ($P$) and Recall ($R$) are defined as:\n$$\nP = \\frac{TP}{TP + FP} \\quad , \\quad R = \\frac{TP}{N_P}\n$$\nThe algorithm is as follows:\n1.  Handle edge cases: If $N_P = 0$, AUPRC is $0.0$. If $N_N = 0$ (and $N_P > 0$), AUPRC is $1.0$.\n2.  Sort all $(y_i, s_i)$ pairs in descending order of score $s_i$.\n3.  Initialize $\\text{AUPRC} = 0$, $TP = 0$, $FP = 0$, and $\\text{last\\_recall} = 0$.\n4.  Iterate through the sorted list, processing all instances with the same score as a single group. For each group of instances at a unique score:\n    a. Count the number of positives ($\\Delta TP$) and negatives ($\\Delta FP$) in this group.\n    b. Update $TP \\leftarrow TP + \\Delta TP$ and $FP \\leftarrow FP + \\Delta FP$.\n    c. Calculate the new recall $R_{\\text{new}} = TP / N_P$.\n    d. If $R_{\\text{new}} > \\text{last\\_recall}$:\n        i. Calculate precision at this point: $P_{\\text{new}} = TP / (TP + FP)$.\n        ii. Add the area of the rectangle for this recall increment to the total: $\\text{AUPRC} \\leftarrow \\text{AUPRC} + P_{\\text{new}} \\times (R_{\\text{new}} - \\text{last\\_recall})$.\n        iii. Update $\\text{last\\_recall} \\leftarrow R_{\\text{new}}$.\n5.  The final sum is the AUPRC. This procedure correctly implements the specified Riemann-Stieltjes sum approximation.\n\n**Overall Procedure**\nFor each test case, the following steps are executed:\n1.  The full dataset $I$ is processed by the AUROC and AUPRC algorithms to obtain $\\text{AUROC}_{\\text{original}}$ and $\\text{AUPRC}_{\\text{original}}$.\n2.  The filtered dataset $I'$ is constructed by removing all tuples $(p_i,y_i,s_i)$ where $p_i$ is in the leakage set $L$.\n3.  The filtered dataset $I'$ is processed by the same AUROC and AUPRC algorithms to obtain $\\text{AUROC}_{\\text{corrected}}$ and $\\text{AUPRC}_{\\text{corrected}}$.\n4.  The differences, $\\Delta \\text{AUROC}$ and $\\Delta \\text{AUPRC}$, are computed.\n5.  These four resulting values are collected, rounded to four decimal places, and formatted as required.\n\nThis structured approach ensures that the evaluation metrics are computed according to the precise definitions provided and that the impact of data leakage is quantified correctly.", "answer": "```python\nimport numpy as np\n\ndef _compute_auroc(labels: list[int], scores: list[float]) -> float:\n    \"\"\"Computes AUROC based on the pairwise probabilistic definition.\"\"\"\n    pos_scores = [s for s, y in zip(scores, labels) if y == 1]\n    neg_scores = [s for s, y in zip(scores, labels) if y == 0]\n    \n    n_pos = len(pos_scores)\n    n_neg = len(neg_scores)\n\n    if n_pos == 0 or n_neg == 0:\n        return 0.5\n\n    wins = 0\n    ties = 0\n    for s_pos in pos_scores:\n        for s_neg in neg_scores:\n            if s_pos > s_neg:\n                wins += 1\n            elif s_pos == s_neg:\n                ties += 1\n    \n    return (wins + 0.5 * ties) / (n_pos * n_neg)\n\ndef _compute_auprc(labels: list[int], scores: list[float]) -> float:\n    \"\"\"Computes AUPRC by integrating precision over recall increments.\"\"\"\n    n_pos = sum(labels)\n    n_total = len(labels)\n    \n    if n_pos == 0:\n        return 0.0\n    \n    n_neg = n_total - n_pos\n    if n_neg == 0:\n        return 1.0\n\n    # Sort instances by score in descending order\n    sorted_data = sorted(zip(scores, labels), key=lambda x: x[0], reverse=True)\n    \n    tp = 0.0\n    fp = 0.0\n    auprc = 0.0\n    last_recall = 0.0\n    \n    i = 0\n    while i < n_total:\n        current_score = sorted_data[i][0]\n        \n        # Find all instances with the same score\n        j = i\n        while j < n_total and sorted_data[j][0] == current_score:\n            j += 1\n            \n        # Count positives and negatives in the current score group\n        tp_group = sum(y for s, y in sorted_data[i:j])\n        fp_group = (j - i) - tp_group\n        \n        # Update cumulative TP and FP after the group\n        tp += tp_group\n        fp += fp_group\n        \n        recall = tp / n_pos\n        precision = tp / (tp + fp) if (tp + fp) > 0 else 1.0\n        \n        # Add the area of the rectangle for the new recall segment\n        auprc += precision * (recall - last_recall)\n        \n        # Update for the next iteration\n        last_recall = recall\n        i = j\n        \n    return auprc\n\ndef compute_metrics(data: list[tuple[int, int, float]]) -> tuple[float, float]:\n    \"\"\"Computes AUROC and AUPRC for a given dataset.\"\"\"\n    if not data:\n        # No data: return conventional baselines\n        return 0.5, 0.0\n\n    labels = [d[1] for d in data]\n    scores = [d[2] for d in data]\n    \n    auroc = _compute_auroc(labels, scores)\n    auprc = _compute_auprc(labels, scores)\n    \n    return auroc, auprc\n    \ndef solve():\n    \"\"\"\n    Main function to process test cases and produce the final output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"data\": [\n                (1,1,0.90),(2,1,0.85),(3,1,0.80),(4,0,0.70),(5,0,0.60),\n                (6,0,0.55),(7,1,0.50),(8,0,0.40),(9,0,0.30),(10,1,0.20)\n            ],\n            \"leaked_set\": {3, 7}\n        },\n        {\n            \"data\": [\n                (11,1,0.90),(12,1,0.80),(13,0,0.70),(14,0,0.60),\n                (15,0,0.40),(16,0,0.20)\n            ],\n            \"leaked_set\": {11, 12}\n        },\n        {\n            \"data\": [\n                (17,1,0.70),(18,0,0.70),(19,1,0.60),(20,0,0.60),(21,1,0.60),\n                (22,0,0.50),(23,0,0.40),(24,1,0.30)\n            ],\n            \"leaked_set\": {18, 22}\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        original_data = case[\"data\"]\n        leaked_set = case[\"leaked_set\"]\n        \n        # 1. Compute original metrics\n        auroc_orig, auprc_orig = compute_metrics(original_data)\n        \n        # 2. Filter data and compute corrected metrics\n        filtered_data = [d for d in original_data if d[0] not in leaked_set]\n        auroc_corr, auprc_corr = compute_metrics(filtered_data)\n        \n        # 3. Compute deltas\n        delta_auroc = auroc_corr - auroc_orig\n        delta_auprc = auprc_corr - auprc_orig\n        \n        # 4. Store results\n        case_results = [auroc_corr, auprc_corr, delta_auroc, delta_auprc]\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    formatted_cases = []\n    for res_list in all_results:\n        formatted_list = [f\"{val:.4f}\" for val in res_list]\n        formatted_cases.append(f\"[{','.join(formatted_list)}]\")\n    \n    print(f\"[{','.join(formatted_cases)}]\")\n\nsolve()\n```", "id": "4618537"}]}