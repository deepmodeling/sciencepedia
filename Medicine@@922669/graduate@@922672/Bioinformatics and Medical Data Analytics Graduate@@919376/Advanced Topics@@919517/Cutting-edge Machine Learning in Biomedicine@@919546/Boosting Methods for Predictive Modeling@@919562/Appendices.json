{"hands_on_practices": [{"introduction": "The power of gradient boosting lies in its generalization of gradient descent to the realm of functions. This practice demystifies this core concept by having you derive the update steps for a model predicting clinical event counts, a common task in medical data analytics. By deriving the pseudo-residuals for the Poisson loss function from first principles, you will see that they are simply the negative gradients of the loss, connecting the abstract optimization theory to a concrete computational step. [@problem_id:4544511]", "problem": "A hospital informatics team is building a Gradient Boosting Machine (GBM) to predict counts of clinical events (thirty-day acute exacerbations) from high-dimensional electronic health records. Assume the data-generating model for counts is Poisson, where the conditional mean is linked to features $\\mathbf{x}$ via a Generalized Linear Model (GLM) with canonical log link, so that the model response is $f(\\mathbf{x})=\\log \\lambda(\\mathbf{x})$ and the pointwise loss is the negative log-likelihood (up to an additive constant), given by $\\ell(y,f)=\\exp(f)-y f$. The GBM updates the current model $f$ by fitting a regression tree to the pseudo-residuals computed from the pointwise loss and then performing a line search for the terminal-node values.\n\nStarting from the definition of the pseudo-residual as the negative gradient of the empirical risk with respect to the current model response evaluated at each training point, derive the explicit analytical form of the pseudo-residuals $r_i$ for the Poisson loss in terms of $y_i$ and $f(\\mathbf{x}_i)$. Then, explain how one would fit a regression tree to these $r_i$ in order to improve the model for counts of clinical events, beginning from the fundamental objective of minimizing the empirical risk and without assuming any pre-specified update formulas.\n\nFinally, consider a single terminal region $\\mathcal{R}$ of a shallow regression tree constructed on the pseudo-residuals at a given GBM iteration. Let the region contain four patients with observed counts and current log-mean predictions as follows:\n- Patient $1$: $y_1=4$, $f(\\mathbf{x}_1)=1.2$\n- Patient $2$: $y_2=0$, $f(\\mathbf{x}_2)=-0.5$\n- Patient $3$: $y_3=1$, $f(\\mathbf{x}_3)=0.0$\n- Patient $4$: $y_4=3$, $f(\\mathbf{x}_4)=0.7$\n\nTreat the terminal nodeâ€™s output as a constant offset $\\gamma$ added to $f(\\mathbf{x})$ for all $\\mathbf{x}\\in\\mathcal{R}$, and determine the exact value $\\gamma_{\\mathcal{R}}^{\\star}$ that minimizes the empirical Poisson loss restricted to $\\mathcal{R}$. Provide the final numerical value of $\\gamma_{\\mathcal{R}}^{\\star}$ rounded to four significant figures. No units are required for the final answer.", "solution": "The problem requires a three-part analysis of a Gradient Boosting Machine (GBM) for Poisson-distributed count data. First, we must derive the analytical form of the pseudo-residuals. Second, we must explain the procedure of fitting a regression tree to these residuals. Third, we must calculate the optimal update for a specific terminal node of a tree.\n\n**Part 1: Derivation of the Pseudo-Residual**\n\nThe pseudo-residual for the $i$-th training observation, denoted $r_i$, is defined as the negative gradient of the pointwise loss function $\\ell(y_i, f)$ with respect to the model's current prediction $f(\\mathbf{x}_i)$. The total empirical risk is the sum of pointwise losses over all $N$ training points, $R_{\\text{emp}} = \\sum_{i=1}^{N} \\ell(y_i, f(\\mathbf{x}_i))$. The gradient descent step in function space for the $i$-th observation is aligned with the negative partial derivative of the loss at that point.\n\nThe pseudo-residual $r_i$ for observation $(\\mathbf{x}_i, y_i)$ with current model prediction $f_i = f(\\mathbf{x}_i)$ is given by:\n$$\nr_i = - \\left[ \\frac{\\partial \\ell(y, f)}{\\partial f} \\right]_{y=y_i, f=f_i}\n$$\nThe problem specifies the pointwise loss function for the Poisson model with a log link, up to an additive constant, as:\n$$\n\\ell(y, f) = \\exp(f) - y f\n$$\nHere, $f = f(\\mathbf{x}) = \\log \\lambda(\\mathbf{x})$, where $\\lambda(\\mathbf{x})$ is the conditional mean of the Poisson distribution. Thus, $\\lambda(\\mathbf{x}) = \\exp(f(\\mathbf{x}))$.\n\nTo find the pseudo-residual, we first compute the partial derivative of $\\ell(y, f)$ with respect to $f$:\n$$\n\\frac{\\partial \\ell}{\\partial f} = \\frac{\\partial}{\\partial f} \\left( \\exp(f) - y f \\right) = \\exp(f) - y\n$$\nSubstituting this into the definition of the pseudo-residual, we obtain its explicit analytical form:\n$$\nr_i = - (\\exp(f_i) - y_i) = y_i - \\exp(f_i)\n$$\nSince $\\exp(f_i)$ is the current model's predicted mean count $\\hat{\\lambda}_i$ for observation $i$, the pseudo-residual for the Poisson loss with a log link is simply the observed count minus the predicted mean count: $r_i = y_i - \\hat{\\lambda}_i$. This is the standard raw residual.\n\n**Part 2: Fitting a Regression Tree**\n\nIn each iteration of the GBM algorithm, the goal is to improve the current model, let's call it $f_m(\\mathbf{x})$, by adding a new function, typically a regression tree $h(\\mathbf{x})$, to it. The new model is $f_{m+1}(\\mathbf{x}) = f_m(\\mathbf{x}) + h(\\mathbf{x})$. The function $h(\\mathbf{x})$ is chosen to minimize the empirical risk of the new model:\n$$\n\\min_{h} \\sum_{i=1}^{N} \\ell(y_i, f_m(\\mathbf{x}_i) + h(\\mathbf{x}_i))\n$$\nThis is a difficult optimization problem in a high-dimensional function space. Gradient Boosting simplifies this by taking a step in the direction of the negative gradient of the loss function. The negative gradient vector, evaluated at the current predictions $\\{f_m(\\mathbf{x}_i)\\}_{i=1}^N$, is precisely the vector of pseudo-residuals $\\{r_i\\}_{i=1}^N = \\{y_i - \\exp(f_m(\\mathbf{x}_i))\\}_{i=1}^N$.\n\nThe core idea is to find a regression tree $h(\\mathbf{x})$ that best approximates these pseudo-residuals. This is achieved by fitting the tree to the training data $\\{(\\mathbf{x}_i, r_i)\\}_{i=1}^N$. The tree is constructed by recursively partitioning the feature space $\\mathbf{x}$ into a set of disjoint terminal regions (leaves), $\\{\\mathcal{R}_j\\}_{j=1}^{J}$, to minimize an impurity measure. For regression, this is typically the sum of squared errors between the pseudo-residuals $r_i$ and the value assigned to the region containing $\\mathbf{x}_i$. The resulting tree $h(\\mathbf{x})$ provides a piecewise constant approximation to the pseudo-residuals. Instead of directly adding the tree's predictions, a separate line search is performed to find the optimal constant update $\\gamma_j$ for each terminal region $\\mathcal{R}_j$, as demonstrated in the next part.\n\n**Part 3: Optimal Terminal Node Value Calculation**\n\nWe are given a single terminal region $\\mathcal{R}$ of a regression tree. This region contains four patients with the following observed counts $y_i$ and current log-mean predictions $f(\\mathbf{x}_i)$:\n- Patient $1$: $y_1=4$, $f(\\mathbf{x}_1)=1.2$\n- Patient $2$: $y_2=0$, $f(\\mathbf{x}_2)=-0.5$\n- Patient $3$: $y_3=1$, $f(\\mathbf{x}_3)=0.0$\n- Patient $4$: $y_4=3$, $f(\\mathbf{x}_4)=0.7$\n\nFor all patients $i$ whose feature vectors $\\mathbf{x}_i$ fall into this region $\\mathcal{R}$, we update the model by adding a constant offset $\\gamma$. The new prediction is $f(\\mathbf{x}_i) + \\gamma$. The optimal value $\\gamma_{\\mathcal{R}}^{\\star}$ is the one that minimizes the total Poisson loss within this region:\n$$\nL_{\\mathcal{R}}(\\gamma) = \\sum_{i \\in \\mathcal{R}} \\ell(y_i, f(\\mathbf{x}_i) + \\gamma) = \\sum_{i \\in \\mathcal{R}} \\left[ \\exp(f(\\mathbf{x}_i) + \\gamma) - y_i (f(\\mathbf{x}_i) + \\gamma) \\right]\n$$\nTo find the minimum, we take the first derivative of $L_{\\mathcal{R}}(\\gamma)$ with respect to $\\gamma$ and set it to zero.\n$$\n\\frac{dL_{\\mathcal{R}}}{d\\gamma} = \\frac{d}{d\\gamma} \\sum_{i \\in \\mathcal{R}} \\left[ \\exp(f(\\mathbf{x}_i))\\exp(\\gamma) - y_i f(\\mathbf{x}_i) - y_i \\gamma \\right]\n$$\n$$\n\\frac{dL_{\\mathcal{R}}}{d\\gamma} = \\sum_{i \\in \\mathcal{R}} \\left[ \\exp(f(\\mathbf{x}_i))\\exp(\\gamma) - y_i \\right]\n$$\nSetting the derivative to zero:\n$$\n\\sum_{i \\in \\mathcal{R}} \\left[ \\exp(f(\\mathbf{x}_i))\\exp(\\gamma) - y_i \\right] = 0\n$$\n$$\n\\exp(\\gamma) \\sum_{i \\in \\mathcal{R}} \\exp(f(\\mathbf{x}_i)) - \\sum_{i \\in \\mathcal{R}} y_i = 0\n$$\nSolving for $\\exp(\\gamma)$:\n$$\n\\exp(\\gamma) = \\frac{\\sum_{i \\in \\mathcal{R}} y_i}{\\sum_{i \\in \\mathcal{R}} \\exp(f(\\mathbf{x}_i))}\n$$\nThe optimal update $\\gamma_{\\mathcal{R}}^{\\star}$ is therefore:\n$$\n\\gamma_{\\mathcal{R}}^{\\star} = \\ln \\left( \\frac{\\sum_{i \\in \\mathcal{R}} y_i}{\\sum_{i \\in \\mathcal{R}} \\exp(f(\\mathbf{x}_i))} \\right)\n$$\nNow, we substitute the given numerical values.\nThe sum of observed counts in the numerator is:\n$$\n\\sum_{i \\in \\mathcal{R}} y_i = 4 + 0 + 1 + 3 = 8\n$$\nThe sum of predicted means in the denominator is:\n$$\n\\sum_{i \\in \\mathcal{R}} \\exp(f(\\mathbf{x}_i)) = \\exp(1.2) + \\exp(-0.5) + \\exp(0.0) + \\exp(0.7)\n$$\nUsing a calculator for the exponential values:\n$$\n\\exp(1.2) \\approx 3.320117\n$$\n$$\n\\exp(-0.5) \\approx 0.606531\n$$\n$$\n\\exp(0.0) = 1\n$$\n$$\n\\exp(0.7) \\approx 2.013753\n$$\nThe sum is:\n$$\n\\sum_{i \\in \\mathcal{R}} \\exp(f(\\mathbf{x}_i)) \\approx 3.320117 + 0.606531 + 1 + 2.013753 = 6.940401\n$$\nFinally, we compute $\\gamma_{\\mathcal{R}}^{\\star}$:\n$$\n\\gamma_{\\mathcal{R}}^{\\star} = \\ln \\left( \\frac{8}{6.940401} \\right) \\approx \\ln(1.152671) \\approx 0.142103\n$$\nRounding the result to four significant figures gives $0.1421$.", "answer": "$$\\boxed{0.1421}$$", "id": "4544511"}, {"introduction": "While standard gradient boosting uses first-order gradients, state-of-the-art frameworks like XGBoost leverage second-order information for more efficient and robust learning. In this exercise, you will derive the celebrated XGBoost objective function, which approximates the loss using both the gradient ($g_i$) and the Hessian ($h_i$). By deriving the optimal leaf weights that explicitly incorporate regularization, you will uncover the mathematical architecture that makes XGBoost a dominant method in predictive modeling. [@problem_id:4544555]", "problem": "A research team in translational bioinformatics is building an ensemble model to predict inpatient sepsis mortality using heterogeneous clinical and multi-omics features. Let there be $N$ patients with features $x_i \\in \\mathbb{R}^d$ and labels $y_i \\in \\{0,1\\}$, and consider stage-wise additive modeling where the prediction at boosting round $t$ is $ \\hat{y}_i^{(t)} = \\sum_{k=1}^{t} f_k(x_i)$ with $f_k$ a regression tree. Assume a twice-differentiable convex sample-wise loss $\\ell(y_i, \\hat{y}_i)$ and a complexity penalty on each tree that increases with the number of leaves and the squared magnitude of leaf weights. The regularized empirical risk over $t$ rounds is the sum of the empirical risk and the cumulative complexity of the $t$ trees.\n\nFor a fixed tree structure at round $t$ with $T_t$ leaves and leaf weights $w_{tj}$ for $j \\in \\{1,\\dots, T_t\\}$, let $q_t(x)$ map an input $x$ to its leaf index and define the index set $I_j = \\{ i \\in \\{1,\\dots,N\\} : q_t(x_i) = j \\}$ for leaf $j$. Define the first and second derivatives of the loss with respect to the current prediction as $g_i = \\left.\\frac{\\partial \\ell(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i}\\right|_{\\hat{y}_i=\\hat{y}_i^{(t-1)}}$ and $h_i = \\left.\\frac{\\partial^2 \\ell(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i^2}\\right|_{\\hat{y}_i=\\hat{y}_i^{(t-1)}}$, where $t-1$ denotes the state prior to adding the $t$-th tree. Let the tree complexity penalty be given by two nonnegative hyperparameters $\\gamma$ and $\\lambda$ that penalize, respectively, the number of leaves and the squared leaf weights.\n\nStarting from the fundamental definitions of stage-wise additive models, empirical risk minimization, and twice-differentiable convex loss expansion, perform the following:\n\n- Formalize the regularized empirical risk over $t$ trees using a per-tree additive complexity penalty that is proportional to the number of leaves and the sum of squared leaf weights.\n- Derive an approximation of the objective at round $t$ that depends only on $\\{w_{tj}\\}_{j=1}^{T_t}$, the fixed partition $\\{I_j\\}_{j=1}^{T_t}$, and the derivatives $\\{g_i, h_i\\}_{i=1}^{N}$ evaluated at $\\hat{y}_i^{(t-1)}$.\n- Show that, for the fixed tree structure, the approximated objective decomposes into a sum over leaves and identify the per-leaf terms.\n- Minimize the decomposed objective with respect to a single leaf weight $w_{tj}$ and provide the closed-form expression for the optimal weight in terms of the aggregated gradient $G_j = \\sum_{i \\in I_j} g_i$, aggregated curvature $H_j = \\sum_{i \\in I_j} h_i$, and the hyperparameter $\\lambda$.\n\nExpress your final answer as a single closed-form expression for the optimal leaf weight for a given leaf $j$. No numerical evaluation or rounding is required.", "solution": "The problem statement is found to be valid as it is scientifically grounded in the principles of machine learning, specifically gradient boosting models. It is well-posed, objective, and contains all necessary information to derive the requested expression. The derivation proceeds by formalizing the objective function, approximating it using a Taylor series expansion, and minimizing the resulting expression with respect to the model parameters.\n\nLet the prediction for the $i$-th sample at boosting round $t$ be $\\hat{y}_i^{(t)}$. The model is stage-wise additive, so the prediction is constructed as:\n$$ \\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i) $$\nwhere $\\hat{y}_i^{(t-1)} = \\sum_{k=1}^{t-1} f_k(x_i)$ is the prediction from the previous $t-1$ rounds, and $f_t$ is the regression tree added at round $t$.\n\nThe problem asks to formalize the regularized empirical risk over $t$ trees. This is the sum of the empirical loss and a regularization term. The regularization term is the cumulative complexity of all trees in the ensemble. The complexity penalty for a single tree $f_k$ with $T_k$ leaves and leaf weights $\\{w_{kj}\\}_{j=1}^{T_k}$ is given as being proportional to the number of leaves and the sum of squared leaf weights. We can formalize this as $\\Omega(f_k) = \\gamma T_k + \\frac{1}{2}\\lambda\\sum_{j=1}^{T_k} w_{kj}^2$, where $\\gamma$ and $\\lambda$ are non-negative hyperparameters. The factor of $\\frac{1}{2}$ is a convention that simplifies the derivative.\n\nThe overall regularized objective function at round $t$, which we denote as $\\mathcal{L}^{(t)}$, is:\n$$ \\mathcal{L}^{(t)} = \\sum_{i=1}^{N} \\ell(y_i, \\hat{y}_i^{(t)}) + \\sum_{k=1}^{t} \\Omega(f_k) $$\nSubstituting the additive nature of the model, $\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)$, we get:\n$$ \\mathcal{L}^{(t)} = \\sum_{i=1}^{N} \\ell(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) + \\sum_{k=1}^{t} \\Omega(f_k) $$\nAt round $t$, our goal is to find the tree $f_t$ that minimizes this objective. The predictions $\\hat{y}_i^{(t-1)}$ and the complexity of the previous trees $\\sum_{k=1}^{t-1} \\Omega(f_k)$ are fixed from previous rounds. Therefore, they are constants with respect to $f_t$. We can define the objective to be minimized at round $t$, denoted $\\text{Obj}^{(t)}$, by dropping these constant terms:\n$$ \\text{Obj}^{(t)} = \\sum_{i=1}^{N} \\ell(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) + \\Omega(f_t) $$\n\nThe next step is to approximate this objective. Since the loss function $\\ell$ is twice-differentiable, we can use a second-order Taylor expansion of $\\ell(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i))$ around the point $\\hat{y}_i^{(t-1)}$:\n$$ \\ell(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) \\approx \\ell(y_i, \\hat{y}_i^{(t-1)}) + \\left[\\frac{\\partial \\ell(y_i, \\hat{y})}{\\partial \\hat{y}}\\right]_{\\hat{y}=\\hat{y}_i^{(t-1)}} f_t(x_i) + \\frac{1}{2} \\left[\\frac{\\partial^2 \\ell(y_i, \\hat{y})}{\\partial \\hat{y}^2}\\right]_{\\hat{y}=\\hat{y}_i^{(t-1)}} f_t(x_i)^2 $$\nUsing the provided definitions for the first and second derivatives, $g_i = \\left.\\frac{\\partial \\ell(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i}\\right|_{\\hat{y}_i=\\hat{y}_i^{(t-1)}}$ and $h_i = \\left.\\frac{\\partial^2 \\ell(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i^2}\\right|_{\\hat{y}_i=\\hat{y}_i^{(t-1)}}$, the expansion simplifies to:\n$$ \\ell(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) \\approx \\ell(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 $$\nSubstituting this approximation into $\\text{Obj}^{(t)}$:\n$$ \\text{Obj}^{(t)} \\approx \\sum_{i=1}^{N} \\left[ \\ell(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 \\right] + \\Omega(f_t) $$\nThe term $\\sum_{i=1}^{N} \\ell(y_i, \\hat{y}_i^{(t-1)})$ is a constant with respect to $f_t$ and can be removed from the minimization objective. Let the approximated objective be $\\tilde{\\text{Obj}}^{(t)}$:\n$$ \\tilde{\\text{Obj}}^{(t)} = \\sum_{i=1}^{N} \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 \\right] + \\Omega(f_t) $$\nNow, we introduce the structure of the regression tree $f_t$. The tree partitions the input space into $T_t$ disjoint regions (leaves), and assigns a constant weight $w_{tj}$ to each leaf $j \\in \\{1, \\dots, T_t\\}$. The function $f_t(x)$ is thus defined by the weights and the mapping $q_t(x)$ from an input $x$ to a leaf index: $f_t(x) = w_{t,q_t(x)}$. The complexity penalty is $\\Omega(f_t) = \\gamma T_t + \\frac{1}{2}\\lambda\\sum_{j=1}^{T_t} w_{tj}^2$.\nSubstituting these into $\\tilde{\\text{Obj}}^{(t)}$:\n$$ \\tilde{\\text{Obj}}^{(t)} = \\sum_{i=1}^{N} \\left[ g_i w_{t,q_t(x_i)} + \\frac{1}{2} h_i w_{t,q_t(x_i)}^2 \\right] + \\gamma T_t + \\frac{1}{2}\\lambda\\sum_{j=1}^{T_t} w_{tj}^2 $$\nTo show that this objective decomposes over the leaves, we regroup the sum over the samples $i \\in \\{1, \\dots, N\\}$ by the leaf they fall into. Let $I_j = \\{ i \\in \\{1,\\dots,N\\} : q_t(x_i) = j \\}$ be the set of indices of samples in leaf $j$. The sum can be rewritten as:\n$$ \\tilde{\\text{Obj}}^{(t)} = \\sum_{j=1}^{T_t} \\sum_{i \\in I_j} \\left[ g_i w_{tj} + \\frac{1}{2} h_i w_{tj}^2 \\right] + \\gamma T_t + \\frac{1}{2}\\lambda\\sum_{j=1}^{T_t} w_{tj}^2 $$\nSince $w_{tj}$ is constant for all samples $i \\in I_j$, we can pull it out of the inner sum:\n$$ \\tilde{\\text{Obj}}^{(t)} = \\sum_{j=1}^{T_t} \\left[ \\left(\\sum_{i \\in I_j} g_i\\right) w_{tj} + \\frac{1}{2} \\left(\\sum_{i \\in I_j} h_i\\right) w_{tj}^2 \\right] + \\gamma T_t + \\frac{1}{2}\\lambda\\sum_{j=1}^{T_t} w_{tj}^2 $$\nNow we use the provided definitions for the aggregated gradient, $G_j = \\sum_{i \\in I_j} g_i$, and aggregated curvature, $H_j = \\sum_{i \\in I_j} h_i$:\n$$ \\tilde{\\text{Obj}}^{(t)} = \\sum_{j=1}^{T_t} \\left[ G_j w_{tj} + \\frac{1}{2} H_j w_{tj}^2 \\right] + \\gamma T_t + \\frac{1}{2}\\lambda\\sum_{j=1}^{T_t} w_{tj}^2 $$\nCombining the terms under the summation over leaves $j$:\n$$ \\tilde{\\text{Obj}}^{(t)} = \\sum_{j=1}^{T_t} \\left[ G_j w_{tj} + \\frac{1}{2} (H_j + \\lambda) w_{tj}^2 \\right] + \\gamma T_t $$\nThis expression demonstrates the decomposition of the objective. For a fixed tree structure (i.e., fixed partition $\\{I_j\\}$ and fixed number of leaves $T_t$), the term $\\gamma T_t$ is a constant. The optimization problem for the leaf weights decomposes into $T_t$ independent minimizations, one for each leaf. The per-leaf objective for leaf $j$ is:\n$$ \\tilde{\\text{Obj}}_j(w_{tj}) = G_j w_{tj} + \\frac{1}{2} (H_j + \\lambda) w_{tj}^2 $$\nTo find the optimal weight $w_{tj}^*$ that minimizes this quadratic function, we take the derivative with respect to $w_{tj}$ and set it to zero.\n$$ \\frac{\\partial \\tilde{\\text{Obj}}_j(w_{tj})}{\\partial w_{tj}} = G_j + (H_j + \\lambda) w_{tj} $$\nSetting the derivative to $0$:\n$$ G_j + (H_j + \\lambda) w_{tj} = 0 $$\nSolving for $w_{tj}$ gives the optimal weight for leaf $j$:\n$$ w_{tj}^* = - \\frac{G_j}{H_j + \\lambda} $$\nThe convexity of the loss function $\\ell$ ensures $h_i \\ge 0$, which implies $H_j = \\sum_{i \\in I_j} h_i \\ge 0$. The hyperparameter $\\lambda$ is non-negative. Typically, $\\lambda  0$ is used, which guarantees $H_j + \\lambda  0$, ensuring the objective is strictly convex with respect to $w_{tj}$ and that a unique minimum exists.", "answer": "$$\n\\boxed{-\\frac{G_j}{H_j + \\lambda}}\n$$", "id": "4544555"}, {"introduction": "A theoretically sound model can still fail in practice without proper tuning. This exercise shifts our focus from the internal mechanics of a single boosting iteration to the practical art of model development and diagnostics. By analyzing training and validation loss curves from different experimental setups, you will learn to diagnose the classic trade-off between bias and variance, identifying critical issues like overfitting and underfitting. Mastering this skill is essential for building boosting models that are not only powerful but also generalizable and robust. [@problem_id:4544495]", "problem": "A hospital is training a Gradient Boosting Decision Trees (GBDT) model to predict in-hospital mortality from electronic health record features. The outcome is binary, and the model is trained by minimizing binary cross-entropy (also called negative log-likelihood) on the training set. Let $L_{\\text{train}}(t)$ and $L_{\\text{val}}(t)$ denote the empirical training and validation losses after $t$ boosting iterations, respectively. Three experiments vary tree depth and learning rate while keeping standard regularization (such as subsampling and minimum child weight) fixed. In each experiment, the maximum number of boosting iterations $T_{\\max}$ is chosen large enough that training can proceed well past the first validation minimum.\n\nFundamental base to use for reasoning:\n- Empirical risk minimization: the training loss $L_{\\text{train}}(t)$ estimates the empirical risk, while the validation loss $L_{\\text{val}}(t)$ estimates out-of-sample risk. The generalization gap is $L_{\\text{val}}(t) - L_{\\text{train}}(t)$.\n- Bias-variance trade-off: increasing base learner complexity (tree depth) and the number of additive components increases model capacity (reduces bias) but can increase variance; a smaller learning rate shrinks each step (reduces variance) but typically requires more iterations to reach a comparable fit.\n- Gradient boosting builds an additive model $F_t(x) = F_{t-1}(x) + \\eta \\, h_t(x)$, where $\\eta$ is the learning rate and $h_t$ are decision trees of specified maximum depth.\n\nThe three experiments (E$1$, E$2$, E$3$) produce the following patterns:\n- E$1$ (depth $d = 1$, learning rate $\\eta = 0.01$, $T_{\\max} = 5000$):\n  - $L_{\\text{train}}(t)$ decreases slowly and approximately monotonically from about $0.693$ at $t = 0$ to about $0.58$ by $t = 5000$.\n  - $L_{\\text{val}}(t)$ closely tracks $L_{\\text{train}}(t)$, decreasing to about $0.59$ by $t = 5000$. The gap $L_{\\text{val}}(t) - L_{\\text{train}}(t)$ remains small, within about $0.01$ to $0.02$, and there is no upturn.\n- E$2$ (depth $d = 6$, learning rate $\\eta = 0.30$, $T_{\\max} = 500$):\n  - $L_{\\text{train}}(t)$ drops rapidly from about $0.693$ at $t = 0$ to below $0.10$ by $t \\approx 200$.\n  - $L_{\\text{val}}(t)$ initially decreases from $0.693$ to about $0.45$ by $t \\approx 80$, then increases steadily to about $0.52$ by $t \\approx 200$.\n- E$3$ (depth $d = 3$, learning rate $\\eta = 0.05$, $T_{\\max} = 2000$):\n  - $L_{\\text{train}}(t)$ decreases to about $0.35$ by $t \\approx 1200$ and then flattens.\n  - $L_{\\text{val}}(t)$ decreases to about $0.38$ by $t \\approx 1200$, remains within about $0.02$ to $0.04$ above $L_{\\text{train}}(t)$, and does not show an upturn through $t = 2000$.\n\nAssume reasonably large training and validation sets drawn independently and identically distributed from the same hospital population, careful data preprocessing, and no evident label leakage.\n\nWhich option best diagnoses each experiment and proposes a modification that most plausibly improves validation performance (reduces $L_{\\text{val}}$) while preserving model robustness in this clinical setting?\n\nA. E$1$: high bias underfitting; increase model capacity by raising $d$ from $1$ to around $3$ to allow interactions and keep $\\eta$ small or moderately larger (for example, $\\eta \\approx 0.05$), with sufficiently large $T$ and early stopping. E$2$: high variance overfitting; apply early stopping near the first validation minimum (around $t \\approx 80$) and/or reduce $\\eta$ (for example, to $\\eta \\approx 0.05$) while increasing $T_{\\max}$ to spread learning. E$3$: near-optimal bias-variance; maintain settings and use early stopping at the validation minimum.\n\nB. E$1$: overfitting because training loss decreases; reduce $d$ to $1$ and reduce $\\eta$ below $0.01$. E$2$: underfitting because validation loss is larger than training loss; increase $d$ to $10$ and $\\eta$ to $0.5$. E$3$: overfitting because of a nonzero gap; reduce $T_{\\max}$ to $100$.\n\nC. E$1$: high variance overfitting because training and validation curves are close; use stronger regularization by further decreasing $\\eta$ to $0.001$. E$2$: the validation upturn indicates dataset shift; hold hyperparameters fixed and train to $T_{\\max}$ to average out noise. E$3$: underfitting; increase $\\eta$ to $0.3$ to descend faster.\n\nD. E$1$: underfitting; the best fix is only to increase $T_{\\max}$ to $10000$ with the same $d = 1$ and $\\eta = 0.01$. E$2$: stuck in a poor local minimum; increase $\\eta$ to $0.6$ so the model escapes and continues reducing $L_{\\text{val}}(t)$. E$3$: model too constrained; increase $d$ to $8$ to close the training-validation gap.", "solution": "The problem statement has been critically validated and is determined to be a valid exercise in predictive modeling diagnostics.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- **Model**: Gradient Boosting Decision Trees (GBDT).\n- **Task**: Binary prediction of in-hospital mortality.\n- **Loss Function**: Binary cross-entropy (negative log-likelihood).\n- **Notation**: $L_{\\text{train}}(t)$ and $L_{\\text{val}}(t)$ are the training and validation losses at boosting iteration $t$.\n- **Model Formulation**: The model is additive, $F_t(x) = F_{t-1}(x) + \\eta \\, h_t(x)$, where $\\eta$ is the learning rate and $h_t$ is a decision tree.\n- **Fixed Parameters**: Standard regularization techniques like subsampling and minimum child weight are held constant across experiments.\n- **Experiment E1**: Depth $d = 1$, learning rate $\\eta = 0.01$, maximum iterations $T_{\\max} = 5000$.\n    - $L_{\\text{train}}(t)$ decreases from $\\approx 0.693$ to $\\approx 0.58$ by $t = 5000$.\n    - $L_{\\text{val}}(t)$ decreases to $\\approx 0.59$ by $t = 5000$.\n    - Generalization gap $L_{\\text{val}}(t) - L_{\\text{train}}(t)$ is small, $\\approx 0.01$ to $0.02$.\n    - No upturn in validation loss.\n- **Experiment E2**: Depth $d = 6$, learning rate $\\eta = 0.30$, maximum iterations $T_{\\max} = 500$.\n    - $L_{\\text{train}}(t)$ drops rapidly from $\\approx 0.693$ to below $0.10$ by $t \\approx 200$.\n    - $L_{\\text{val}}(t)$ decreases to a minimum of $\\approx 0.45$ around $t \\approx 80$, then increases.\n- **Experiment E3**: Depth $d = 3$, learning rate $\\eta = 0.05$, maximum iterations $T_{\\max} = 2000$.\n    - $L_{\\text{train}}(t)$ decreases to $\\approx 0.35$ by $t \\approx 1200$ and then flattens.\n    - $L_{\\text{val}}(t)$ decreases to $\\approx 0.38$ by $t \\approx 1200$.\n    - Generalization gap is stable, $\\approx 0.02$ to $0.04$.\n    - No upturn in validation loss up to $t = 2000$.\n- **Assumptions**: Large, i.i.d. training and validation sets; no data issues like label leakage.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is based on the standard and well-established theory of ensemble learning, specifically gradient boosting. The concepts of bias-variance trade-off, overfitting, underfitting, early stopping, and the roles of hyperparameters like tree depth ($d$) and learning rate ($\\eta$) are all correctly represented. The initial loss value of $\\approx 0.693$ corresponds to $-\\ln(0.5)$, the cross-entropy for a naive model predicting a $50\\%$ probability for a balanced binary classification problem, which is a scientifically sound starting point.\n- **Well-Posed**: The problem provides three distinct experimental scenarios and asks for a diagnosis and a proposed resolution for each. The provided data on loss curves and hyperparameters are sufficient to make a clear determination about the behavior of the model in each case.\n- **Objective**: The problem is stated using precise, quantitative, and objective language. There are no subjective or ambiguous terms.\n\nThe problem statement is internally consistent, scientifically sound, and well-posed. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be derived.\n\n**Derivation and Option Analysis**\n\nFirst, we analyze the behavior of the model in each experiment based on the provided learning curves and hyperparameters. The goal is to minimize the out-of-sample error, estimated by the validation loss $L_{\\text{val}}(t)$.\n\n**Analysis of Experiment E1**\n- **Parameters**: Very simple base learners ($d=1$, decision stumps) and a very small learning rate ($\\eta=0.01$).\n- **Behavior**: $L_{\\text{train}}(t)$ and $L_{\\text{val}}(t)$ decrease very slowly and never reach a low value, ending at $\\approx 0.58$ and $\\approx 0.59$ respectively. The generalization gap is minimal. This indicates the model lacks the capacity to capture the underlying patterns in the data. The model has high bias.\n- **Diagnosis**: This is a classic case of **underfitting**. The model is too simple.\n- **Plausible Improvement**: To combat high bias, model capacity must be increased. This is most effectively done by increasing the depth of the base learners, for example, from $d=1$ to $d=3$. This allows the model to learn interactions between features. Simply increasing the number of iterations with $d=1$ will likely result in very slow, diminishing returns, as the model is fundamentally limited by its structure. A small to moderate learning rate should be maintained for robustness.\n\n**Analysis of Experiment E2**\n- **Parameters**: Complex base learners ($d=6$) and a large learning rate ($\\eta=0.30$).\n- **Behavior**: $L_{\\text{train}}(t)$ drops extremely rapidly to a very low value ($0.10$), indicating the model is memorizing the training data. $L_{\\text{val}}(t)$ finds a minimum at $t \\approx 80$ and then begins to increase, a definitive sign that the model is fitting noise in the training set which does not generalize. The generalization gap, $L_{\\text{val}}(t) - L_{\\text{train}}(t)$, becomes large and grows with $t$. The model has high variance.\n- **Diagnosis**: This is a classic case of **overfitting**. The model is too complex and aggressive.\n- **Plausible Improvement**: The most immediate action is to apply **early stopping** at the iteration corresponding to the minimum validation loss, i.e., $t \\approx 80$. To build a more robust model, one should reduce its variance. This can be achieved by decreasing the learning rate $\\eta$ (e.g., to $0.05$) and/or decreasing the tree depth $d$. A smaller $\\eta$ necessitates more boosting iterations to reach a good fit, so $T_{\\max}$ should be sufficiently large.\n\n**Analysis of Experiment E3**\n- **Parameters**: Moderately complex base learners ($d=3$) and a moderate learning rate ($\\eta=0.05$).\n- **Behavior**: Both $L_{\\text{train}}(t)$ and $L_{\\text{val}}(t)$ decrease to low values ($\\approx 0.35$ and $\\approx 0.38$ respectively) and then plateau. The validation loss is the lowest achieved across all three experiments. The generalization gap is small and stable.\n- **Diagnosis**: This represents a **good bias-variance trade-off**. The model is complex enough to learn the signal but not so complex that it overfits the noise. The performance is near-optimal for the explored hyperparameter space.\n- **Plausible Improvement**: The hyperparameters appear well-chosen. The standard procedure is to use these settings and employ **early stopping** to select the model at the exact iteration where $L_{\\text{val}}(t)$ is minimal (around $t \\approx 1200$ where it plateaus). No major change to the hyperparameters is warranted; the primary task is to find the optimal number of trees.\n\n**Option-by-Option Analysis**\n\n**A. E$1$: high bias underfitting; increase model capacity by raising $d$ from $1$ to around $3$ to allow interactions and keep $\\eta$ small or moderately larger (for example, $\\eta \\approx 0.05$), with sufficiently large $T$ and early stopping. E$2$: high variance overfitting; apply early stopping near the first validation minimum (around $t \\approx 80$) and/or reduce $\\eta$ (for example, to $\\eta \\approx 0.05$) while increasing $T_{\\max}$ to spread learning. E$3$: near-optimal bias-variance; maintain settings and use early stopping at the validation minimum.**\n\n- **Analysis of E1 statement**: The diagnosis (\"high bias underfitting\") is correct. The proposed fix (increase $d$ to $\\approx 3$, adjust $\\eta$ appropriately, use early stopping) is the standard and most effective way to address this. This is correct.\n- **Analysis of E2 statement**: The diagnosis (\"high variance overfitting\") is correct. The proposed fixes (early stopping at $t \\approx 80$, reducing $\\eta$ to regularize) are the standard and correct procedures. This is correct.\n- **Analysis of E3 statement**: The diagnosis (\"near-optimal bias-variance\") is correct. The proposed action (maintain settings and use early stopping) is the correct procedure for a well-tuned model. This is correct.\n- **Verdict**: **Correct**. This option accurately diagnoses all three scenarios and proposes theoretically sound and practically standard improvements.\n\n**B. E$1$: overfitting because training loss decreases; reduce $d$ to $1$ and reduce $\\eta$ below $0.01$. E$2$: underfitting because validation loss is larger than training loss; increase $d$ to $10$ and $\\eta$ to $0.5$. E$3$: overfitting because of a nonzero gap; reduce $T_{\\max}$ to $100$.**\n\n- **Analysis of E1 statement**: The reasoning \"overfitting because training loss decreases\" is fundamentally flawed. The objective of training is to decrease training loss. The diagnosis is incorrect, and the proposed fix would worsen the existing underfitting. This is incorrect.\n- **Analysis of E2 statement**: The reasoning \"underfitting because validation loss is larger than training loss\" is fundamentally flawed. A large and *increasing* gap indicates overfitting, not underfitting. The proposed fix would dramatically increase model complexity and exacerbate the overfitting problem. This is incorrect.\n- **Analysis of E3 statement**: The reasoning \"overfitting because of a nonzero gap\" is flawed. A small, stable gap is desirable and indicates generalization. The proposed fix to reduce $T_{\\max}$ to $100$ is arbitrary and would prematurely halt training, leading to a suboptimal, underfit model (since the minimum is near $t \\approx 1200$). This is incorrect.\n- **Verdict**: **Incorrect**. Every diagnosis and proposed action in this option is based on a misunderstanding of core machine learning concepts.\n\n**C. E$1$: high variance overfitting because training and validation curves are close; use stronger regularization by further decreasing $\\eta$ to $0.001$. E$2$: the validation upturn indicates dataset shift; hold hyperparameters fixed and train to $T_{\\max}$ to average out noise. E$3$: underfitting; increase $\\eta$ to $0.3$ to descend faster.**\n\n- **Analysis of E1 statement**: The reasoning \"high variance overfitting because training and validation curves are close\" is the opposite of correct; close curves indicate low variance. The diagnosis is incorrect. This is incorrect.\n- **Analysis of E2 statement**: The diagnosis of \"dataset shift\" is not supported by the evidence. A validation loss upturn is the canonical sign of overfitting on i.i.d. data. The proposed fix of training longer is precisely the wrong action, as it will worsen overfitting. This is incorrect.\n- **Analysis of E3 statement**: The diagnosis of \"underfitting\" is incorrect, as E3 gives the best performance. The proposed fix of increasing $\\eta$ to $0.3$ would likely induce the same overfitting seen in E2. This is incorrect.\n- **Verdict**: **Incorrect**. This option demonstrates multiple, severe misconceptions about model diagnostics.\n\n**D. E$1$: underfitting; the best fix is only to increase $T_{\\max}$ to $10000$ with the same $d = 1$ and $\\eta = 0.01$. E$2$: stuck in a poor local minimum; increase $\\eta$ to $0.6$ so the model escapes and continues reducing $L_{\\text{val}}(t)$. E$3$: model too constrained; increase $d$ to $8$ to close the training-validation gap.**\n\n- **Analysis of E1 statement**: The diagnosis of \"underfitting\" is correct. However, the proposed fix is suboptimal. The primary limitation is the model capacity ($d=1$), and the loss curves are already flattening, so simply adding more iterations will provide minimal benefit. A better fix involves increasing $d$. Stating this is the \"best fix\" is incorrect.\n- **Analysis of E2 statement**: The diagnosis of being \"stuck in a poor local minimum\" is incorrect. The training loss is rapidly decreasing, so the optimizer is working. The issue is overfitting, not a suboptimal minimum. Increasing $\\eta$ would worsen the overfitting. This is incorrect.\n- **Analysis of E3 statement**: The diagnosis of \"model too constrained\" is incorrect, as it's the best model. The idea that increasing $d$ will \"close the training-validation gap\" is also incorrect; increasing complexity typically *increases* the gap while lowering training loss. This is incorrect.\n- **Verdict**: **Incorrect**. While one diagnosis is partially correct, the proposed fixes are either suboptimal or actively harmful.\n\nBased on the detailed analysis, Option A is the only choice that provides a correct diagnosis and a sound, standard proposal for improvement for all three experiments.", "answer": "$$\\boxed{A}$$", "id": "4544495"}]}