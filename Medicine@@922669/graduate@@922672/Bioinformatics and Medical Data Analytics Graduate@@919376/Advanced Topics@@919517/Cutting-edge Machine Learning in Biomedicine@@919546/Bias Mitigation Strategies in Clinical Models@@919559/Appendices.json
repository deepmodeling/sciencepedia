{"hands_on_practices": [{"introduction": "Before we can mitigate bias, we must first be able to measure it. This foundational exercise focuses on diagnosing algorithmic bias by translating raw model performance data, such as from confusion matrices, into standardized and interpretable fairness metrics. By calculating demographic parity, equal opportunity, and equalized odds, you will learn how different definitions of fairness can reveal distinct types of disparities in a model's impact on different patient groups [@problem_id:4542429].", "problem": "A tertiary hospital deploys a binary clinical decision support model to trigger early intervention for sepsis based on multimodal electronic health record data and genotype-derived ancestry features. The sensitive attribute is denoted by $A \\in \\{0,1\\}$ and represents two inferred genetic ancestry strata. The outcome is $Y \\in \\{0,1\\}$, where $Y=1$ indicates a confirmed sepsis event within $48$ hours, and the model’s prediction is $\\hat{Y} \\in \\{0,1\\}$. You are given per-group confusion matrices (counts) over a held-out test cohort:\n- For $A=0$: true positives $TP_{0}=180$, false negatives $FN_{0}=60$, false positives $FP_{0}=76$, true negatives $TN_{0}=684$. The total in group $A=0$ is $N_{0}=TP_{0}+FN_{0}+FP_{0}+TN_{0}$.\n- For $A=1$: true positives $TP_{1}=195$, false negatives $FN_{1}=105$, false positives $FP_{1}=105$, true negatives $TN_{1}=595$. The total in group $A=1$ is $N_{1}=TP_{1}+FN_{1}+FP_{1}+TN_{1}$.\n\nUsing standard fairness definitions in clinical machine learning: demographic parity difference is the absolute difference in positive prediction rates between groups; equal opportunity difference is the absolute difference in true positive rates (sensitivity) between groups; equalized odds difference is the maximum of the absolute differences in true positive rate and false positive rate between groups.\n\nStarting from foundational definitions of rates computed from confusion matrices, compute:\n- the demographic parity difference,\n- the equal opportunity difference,\n- the equalized odds difference,\n\nall expressed as decimal fractions. Then, briefly interpret their clinical significance in terms of disparities in alerting and sensitivity across $A=0$ versus $A=1$. Provide the three numerical values in the order listed above. No rounding is required. Express the final answer as a single row matrix containing the three values.", "solution": "The problem is first subjected to a rigorous validation procedure before any attempt at a solution is made.\n\n### Problem Validation\n\n**Step 1: Extracted Givens**\nThe givens are extracted verbatim from the problem statement:\n- Sensitive attribute: $A \\in \\{0,1\\}$\n- Outcome: $Y \\in \\{0,1\\}$, where $Y=1$ indicates a confirmed sepsis event.\n- Model prediction: $\\hat{Y} \\in \\{0,1\\}$.\n- For group $A=0$: true positives $TP_{0}=180$, false negatives $FN_{0}=60$, false positives $FP_{0}=76$, true negatives $TN_{0}=684$.\n- Total in group $A=0$: $N_{0}=TP_{0}+FN_{0}+FP_{0}+TN_{0}$.\n- For group $A=1$: true positives $TP_{1}=195$, false negatives $FN_{1}=105$, false positives $FP_{1}=105$, true negatives $TN_{1}=595$.\n- Total in group $A=1$: $N_{1}=TP_{1}+FN_{1}+FP_{1}+TN_{1}$.\n- Definition of demographic parity difference: absolute difference in positive prediction rates between groups.\n- Definition of equal opportunity difference: absolute difference in true positive rates (sensitivity) between groups.\n- Definition of equalized odds difference: maximum of the absolute differences in true positive rate and false positive rate between groups.\n\n**Step 2: Validation Using Extracted Givens**\nThe problem is assessed against the required criteria:\n- **Scientifically Grounded**: The problem utilizes standard, well-defined fairness metrics (demographic parity, equal opportunity, equalized odds) from the field of algorithmic fairness, specifically applied to a realistic clinical machine learning scenario. The concepts are based on foundational probability and statistics. The problem is scientifically sound.\n- **Well-Posed**: All necessary data (confusion matrix counts for each subgroup) and explicit definitions for the quantities to be calculated are provided. The problem is self-contained and leads to a unique, deterministic solution.\n- **Objective**: The problem is stated using precise, unambiguous technical language. It is free of subjective claims or opinions.\n\nThe problem does not exhibit any of the enumerated flaws (e.g., scientific unsoundness, incompleteness, ambiguity). It is a well-defined computational problem in applied statistics.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution\n\nThe solution requires the computation of three standard fairness metrics from the provided confusion matrix data for two groups, denoted by the sensitive attribute $A \\in \\{0,1\\}$. We begin by calculating the total cohort size and the number of actual positives and actual negatives for each group.\n\nFor group $A=0$:\n- Total size: $N_{0} = TP_{0} + FN_{0} + FP_{0} + TN_{0} = 180 + 60 + 76 + 684 = 1000$.\n- Actual positives (individuals with sepsis): $P_{0} = TP_{0} + FN_{0} = 180 + 60 = 240$.\n- Actual negatives (individuals without sepsis): $N_{\\text{neg},0} = FP_{0} + TN_{0} = 76 + 684 = 760$.\n\nFor group $A=1$:\n- Total size: $N_{1} = TP_{1} + FN_{1} + FP_{1} + TN_{1} = 195 + 105 + 105 + 595 = 1000$.\n- Actual positives (individuals with sepsis): $P_{1} = TP_{1} + FN_{1} = 195 + 105 = 300$.\n- Actual negatives (individuals without sepsis): $N_{\\text{neg},1} = FP_{1} + TN_{1} = 105 + 595 = 700$.\n\nWith these aggregate counts, we can compute the required rates for each group.\n\n**1. Demographic Parity Difference**\nThis metric measures the difference in the rate of positive predictions across groups. The positive prediction rate ($PPR$) for a group $a$ is the probability of a positive prediction, $P(\\hat{Y}=1|A=a)$. It is calculated as the total number of positive predictions divided by the total size of the group.\n$$ PPR_a = \\frac{TP_a + FP_a}{N_a} $$\nFor group $A=0$:\n$$ PPR_{0} = \\frac{TP_{0} + FP_{0}}{N_{0}} = \\frac{180 + 76}{1000} = \\frac{256}{1000} = 0.256 $$\nFor group $A=1$:\n$$ PPR_{1} = \\frac{TP_{1} + FP_{1}}{N_{1}} = \\frac{195 + 105}{1000} = \\frac{300}{1000} = 0.3 $$\nThe demographic parity difference ($DPD$) is the absolute difference between these rates:\n$$ DPD = |PPR_{0} - PPR_{1}| = |0.256 - 0.3| = |-0.044| = 0.044 $$\n\n**2. Equal Opportunity Difference**\nThis metric requires that the probability of a positive prediction is equal across groups for individuals who are actually positive ($Y=1$). This is equivalent to comparing the true positive rates ($TPR$), or sensitivities, of the model for each group. The $TPR$ for a group $a$ is calculated as:\n$$ TPR_a = P(\\hat{Y}=1|Y=1, A=a) = \\frac{TP_a}{TP_a + FN_a} = \\frac{TP_a}{P_a} $$\nFor group $A=0$:\n$$ TPR_{0} = \\frac{TP_{0}}{P_{0}} = \\frac{180}{240} = \\frac{3}{4} = 0.75 $$\nFor group $A=1$:\n$$ TPR_{1} = \\frac{TP_{1}}{P_{1}} = \\frac{195}{300} = \\frac{65}{100} = 0.65 $$\nThe equal opportunity difference ($EOD_{opp}$) is the absolute difference between these rates:\n$$ EOD_{opp} = |TPR_{0} - TPR_{1}| = |0.75 - 0.65| = 0.1 $$\n\n**3. Equalized Odds Difference**\nThis metric is a stricter condition than equal opportunity. It requires parity in both the true positive rates and the false positive rates across groups. The false positive rate ($FPR$) for a group $a$ is the probability of a positive prediction for individuals who are actually negative ($Y=0$).\n$$ FPR_a = P(\\hat{Y}=1|Y=0, A=a) = \\frac{FP_a}{FP_a + TN_a} = \\frac{FP_a}{N_{\\text{neg},a}} $$\nFor group $A=0$:\n$$ FPR_{0} = \\frac{FP_{0}}{N_{\\text{neg},0}} = \\frac{76}{760} = 0.1 $$\nFor group $A=1$:\n$$ FPR_{1} = \\frac{FP_{1}}{N_{\\text{neg},1}} = \\frac{105}{700} = \\frac{15}{100} = 0.15 $$\nThe equalized odds difference ($EOD_{odds}$) is defined as the maximum of the absolute differences in $TPR$ and $FPR$:\n$$ EOD_{odds} = \\max(|TPR_{0} - TPR_{1}|, |FPR_{0} - FPR_{1}|) $$\nWe have already computed $|TPR_{0} - TPR_{1}| = 0.1$. The absolute difference in false positive rates is:\n$$ |FPR_{0} - FPR_{1}| = |0.1 - 0.15| = |-0.05| = 0.05 $$\nTherefore, the equalized odds difference is:\n$$ EOD_{odds} = \\max(0.1, 0.05) = 0.1 $$\n\n**Clinical Interpretation:**\n- **Demographic Parity Difference ($0.044$):** The model issues alerts at a higher rate for individuals in group $A=1$ ($30.0\\%$) compared to group $A=0$ ($25.6\\%$). This means an individual from group $A=1$ is more likely to receive an alert, irrespective of their actual sepsis status.\n- **Equal Opportunity Difference ($0.1$):** This reveals a significant clinical disparity. The model's sensitivity (ability to detect true sepsis cases) is substantially lower for group $A=1$ ($65\\%$) than for group $A=0$ ($75\\%$). This implies that a septic patient from group $A=1$ has a $10$ percentage point higher chance of being missed by the algorithm compared to a septic patient from group $A=0$. This is a failure to provide an equal benefit of the diagnostic tool, potentially leading to worse outcomes for group $A=1$.\n- **Equalized Odds Difference ($0.1$):** This value is driven by the disparity in the true positive rate ($0.1$) rather than the false positive rate ($0.05$). While group $A=1$ also experiences a higher rate of false alarms ($15\\%$ vs. $10\\%$), the more critical disparity is the model's reduced sensitivity for this group. The equalized odds shows that the model does not perform equitably for either sick or healthy individuals across the two groups, with the primary and most clinically significant failure being the inequitable detection of true disease.", "answer": "$$ \\boxed{\\begin{pmatrix} 0.044 & 0.1 & 0.1 \\end{pmatrix}} $$", "id": "4542429"}, {"introduction": "Once bias is identified, one of the most direct ways to address it is by adjusting the data before the model is even trained. This exercise provides direct experience with a common *pre-processing* mitigation strategy, reweighing, which alters the training data distribution to reduce correlations between sensitive attributes and outcomes. By implementing the weighting scheme and observing its effect on the Demographic Parity Difference, you will gain a practical understanding of how data-level interventions can guide a model towards fairer predictions [@problem_id:4542367].", "problem": "You are given a binary classification task in which each instance has a sensitive attribute $A \\in \\{0,1\\}$, a binary outcome $Y \\in \\{0,1\\}$, and a feature vector $X \\in \\mathbb{R}^d$. You will implement a program that, for each test case, does the following from first principles:\n\n1. Using only empirical frequencies, determine nonnegative weights $w_{a,y}$ for each cell $(A=a, Y=y)$ such that if each instance with $(A,Y)=(a,y)$ is assigned weight $w_{a,y}$, then the reweighted empirical joint distribution of $(A,Y)$ factorizes into the product of its empirical marginals. Concretely, the reweighted count assigned to any pair $(a,y)$ must equal the product of the reweighted totals for $A=a$ and $Y=y$ divided by the total number of instances, which is equivalent to enforcing that the reweighted joint equals the product of the original empirical marginals. Assign each instance $i$ with $(A_i,Y_i)=(a,y)$ the corresponding constant weight $w_{a,y}$.\n\n2. Fit two binary logistic regression models with an intercept term using gradient descent on the negative weighted log-likelihood under the Bernoulli model with a logistic link, with and without reweighing. Specifically, minimize the unregularized negative log-likelihood summed over instances for the unweighted model (all instance weights equal to $1$) and for the reweighted model (instance weights set as in step $1$). Include an $\\ell_2$ (pronounced “ell-two”) regularization term on all coefficients except the intercept with coefficient $\\lambda = 10^{-2}$. Use a constant learning rate $\\eta = 10^{-1}$ and run for $T = 2000$ iterations starting from the all-zeros parameter vector. The logistic link is the canonical link that maps a linear score to a Bernoulli parameter. The gradient descent update should use the exact gradient of the weighted objective. Ensure numerical stability in evaluating any exponential or logarithmic terms.\n\n3. For evaluation, compute the Demographic Parity Difference (DPD) defined as the difference between the positive prediction rates across the two sensitive groups, that is, the probability of predicting $\\hat{Y}=1$ for $A=1$ minus that for $A=0$, where the prediction $\\hat{Y}$ is obtained by thresholding the model’s predicted probability at $0.5$. Compute this DPD on the same dataset used for training for both the unweighted model and the reweighted model. Report the change in DPD due to reweighing, defined as $\\Delta = \\mathrm{DPD}_{\\text{reweighted}} - \\mathrm{DPD}_{\\text{unweighted}}$.\n\nFollow these specifications and assumptions:\n- Use the canonical Bernoulli likelihood with the logistic link. Use the negative log-likelihood with instance weights to define the loss. Use full batch gradient descent with the specified hyperparameters.\n- Apply $\\ell_2$ regularization with coefficient $\\lambda = 10^{-2}$ to all coefficients except the intercept. Do not regularize the intercept.\n- For the predicted label, use a threshold of $0.5$. Compute all DPDs on the training data itself.\n- For reweighing, use the empirical frequencies computed from the given dataset of each test case. Do not assume any prior distributions.\n\nTest suite:\nProvide the change $\\Delta$ for each of the following four test cases. Each case supplies $X$, $A$, and $Y$ explicitly. All arrays are given in row-major order, and each row corresponds to one instance. In all cases, use the exact hyperparameters $\\lambda = 10^{-2}$, $\\eta = 10^{-1}$, and $T = 2000$.\n\n- Test Case $1$ (two-dimensional features, moderate imbalance across $(A,Y)$):\n  - $X = \\left[\\begin{array}{cc}\n  2.0 & 2.0 \\\\\n  1.8 & 2.2 \\\\\n  0.0 & 0.2 \\\\\n  0.3 & -0.1 \\\\\n  -0.2 & 0.1 \\\\\n  3.0 & 3.1 \\\\\n  2.7 & 2.9 \\\\\n  3.2 & 2.8 \\\\\n  2.9 & 3.0 \\\\\n  1.2 & 1.0 \\\\\n  1.1 & 1.3 \\\\\n  0.9 & 1.2\n  \\end{array}\\right]$, $A = \\left[0,0,0,0,0,1,1,1,1,1,1,1\\right]$, $Y = \\left[1,1,0,0,0,1,1,1,1,0,0,0\\right]$.\n- Test Case $2$ (one-dimensional features, minority sensitive group with label skew):\n  - $X = \\left[\\begin{array}{c}\n  2.1 \\\\ 1.9 \\\\ 2.3 \\\\ 1.8 \\\\ 2.0 \\\\ 2.2 \\\\ 1.7 \\\\ 2.4 \\\\ 2.1 \\\\ 1.9 \\\\ 2.2 \\\\ 2.0 \\\\ -1.1 \\\\ -0.9 \\\\ -1.2 \\\\ -1.0 \\\\\n  1.5 \\\\ -1.3 \\\\ -1.1 \\\\ -1.2\n  \\end{array}\\right]$, $A = \\left[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1\\right]$, $Y = \\left[1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,0,0,0\\right]$.\n- Test Case $3$ (constant features yielding identical predicted probabilities across groups):\n  - $X = \\left[\\begin{array}{c}\n  0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0\n  \\end{array}\\right]$, $A = \\left[0,0,0,1,1,1\\right]$, $Y = \\left[1,0,1,0,0,1\\right]$.\n- Test Case $4$ (two-dimensional features, near-separable clusters across labels with balanced groups):\n  - $X = \\left[\\begin{array}{cc}\n  4.0 & 4.0 \\\\\n  5.0 & 5.0 \\\\\n  -4.0 & -4.0 \\\\\n  -5.0 & -5.0 \\\\\n  3.5 & 3.5 \\\\\n  4.0 & 3.8 \\\\\n  -3.0 & -3.0 \\\\\n  -4.0 & -3.5\n  \\end{array}\\right]$, $A = \\left[0,0,0,0,1,1,1,1\\right]$, $Y = \\left[1,1,0,0,1,1,0,0\\right]$.\n\nYour program should produce a single line of output containing the changes in Demographic Parity Difference $\\Delta$ for the four test cases, in order, as a comma-separated list enclosed in square brackets (for example, $\\left[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4\\right]$). Each $\\Delta$ must be a real number rounded to six decimal places. No other text should be printed. All quantities are unitless, and any proportions must be expressed as decimals, not as percentages.", "solution": "The problem requires the implementation and evaluation of a bias mitigation technique, specifically reweighing, for a binary logistic regression classifier. The goal is to quantify the change in the Demographic Parity Difference (DPD) metric after applying this technique. The process can be broken down into three main steps: calculating the instance weights, training the logistic regression models, and evaluating the fairness metric.\n\n### Step 1: Instance Reweighing\n\nThe objective of reweighing is to modify the training data distribution such that the sensitive attribute $A$ and the outcome $Y$ become statistically independent. The target distribution $P_{\\text{target}}$ is one where the joint probability of $(A,Y)$ factorizes into the product of the marginal probabilities from the original empirical distribution $P_{\\text{orig}}$.\nLet $N$ be the total number of instances. Let $N_a$ be the count of instances where the sensitive attribute is $A=a$, $N_y$ be the count where the outcome is $Y=y$, and $N_{a,y}$ be the count for the joint event $(A=a, Y=y)$. The original empirical probabilities are:\n$P_{\\text{orig}}(A=a) = \\frac{N_a}{N}$\n$P_{\\text{orig}}(Y=y) = \\frac{N_y}{N}$\n$P_{\\text{orig}}(A=a, Y=y) = \\frac{N_{a,y}}{N}$\n\nThe target joint distribution reflecting independence is $P_{\\text{target}}(A=a, Y=y) = P_{\\text{orig}}(A=a) P_{\\text{orig}}(Y=y)$.\nTo make the reweighted dataset conform to this target distribution, we assign a weight $w_i$ to each instance $i$. For an instance with attributes $(A_i, Y_i)=(a,y)$, the weight $w_{a,y}$ is calculated as the ratio of the target probability to the original probability of that instance's group:\n$$\nw_{a,y} = \\frac{P_{\\text{target}}(A=a, Y=y)}{P_{\\text{orig}}(A=a, Y=y)} = \\frac{P_{\\text{orig}}(A=a) P_{\\text{orig}}(Y=y)}{P_{\\text{orig}}(A=a, Y=y)}\n$$\nSubstituting the empirical frequencies, we get the formula for the weights:\n$$\nw_{a,y} = \\frac{(N_a/N)(N_y/N)}{N_{a,y}/N} = \\frac{N_a N_y}{N N_{a,y}}\n$$\nEach instance $i$ with sensitive attribute $A_i$ and outcome $Y_i$ is then assigned the corresponding weight $w_i = w_{A_i, Y_i}$. For the unweighted model, all weights are implicitly set to $w_i=1$.\n\n### Step 2: Weighted Logistic Regression and Training\n\nThe binary logistic regression model predicts the probability of the positive outcome ($Y=1$) given a feature vector $X \\in \\mathbb{R}^d$. The model is defined by a parameter vector $\\theta = (\\beta_0, \\beta_1, \\dots, \\beta_d)^T$, where $\\beta_0$ is the intercept and $\\beta = (\\beta_1, \\dots, \\beta_d)^T$ are the feature coefficients. The predicted probability $p$ for an instance with features $X$ is:\n$$\np = P(Y=1|X, \\theta) = \\sigma(\\beta_0 + X^T\\beta)\n$$\nwhere $\\sigma(z) = (1+e^{-z})^{-1}$ is the logistic (sigmoid) function.\n\nThe parameters $\\theta$ are learned by minimizing an objective function. For this problem, the objective is the negative weighted log-likelihood of the Bernoulli model, with an added $\\ell_2$ regularization term on the feature coefficients (but not the intercept). For a dataset of $N$ instances, the objective function $L(\\theta)$ is:\n$$\nL(\\theta) = -\\sum_{i=1}^N w_i \\left[ y_i \\log p_i + (1-y_i) \\log(1-p_i) \\right] + \\lambda \\sum_{j=1}^d \\beta_j^2\n$$\nHere, $p_i = \\sigma(\\beta_0 + X_i^T\\beta)$ and $\\lambda > 0$ is the regularization coefficient.\n\nWe use full-batch gradient descent to minimize $L(\\theta)$. The gradient of the objective function with respect to the parameters is required for the update step. The partial derivatives are:\nFor the intercept $\\beta_0$:\n$$\n\\frac{\\partial L}{\\partial \\beta_0} = \\sum_{i=1}^N w_i(p_i - y_i)\n$$\nFor a feature coefficient $\\beta_j$ ($j \\in \\{1, \\dots, d\\}$):\n$$\n\\frac{\\partial L}{\\partial \\beta_j} = \\left(\\sum_{i=1}^N w_i(p_i - y_i) X_{ij}\\right) + 2\\lambda\\beta_j\n$$\nThe gradient descent update rule for the parameter vector $\\theta$ at each iteration $t$ is:\n$$\n\\theta^{(t+1)} \\leftarrow \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta^{(t)})\n$$\nwhere $\\eta$ is the learning rate. We start with $\\theta^{(0)}=\\mathbf{0}$ and iterate for $T=2000$ steps with $\\eta=10^{-1}$ and $\\lambda=10^{-2}$. This procedure is performed twice: once with all $w_i=1$ (unweighted model) and once with the fairness weights $w_i = w_{A_i, Y_i}$ (reweighted model).\n\nFor implementation, the feature matrix $X$ is augmented with a leading column of ones to accommodate the intercept $\\beta_0$. This allows the linear score to be computed as a single matrix-vector product $X_{\\text{aug}}\\theta$.\n\n### Step 3: Evaluation with Demographic Parity Difference\n\nThe effect of reweighing is evaluated by measuring the change in Demographic Parity Difference (DPD). DPD quantifies the disparity in positive prediction rates between the two sensitive groups ($A=1$ and $A=0$).\nFirst, a prediction $\\hat{Y}$ is made for each instance. Using a threshold of $0.5$ on the predicted probability, the decision rule is:\n$$\n\\hat{Y} = 1 \\quad \\text{if } p \\ge 0.5 \\quad (\\text{equivalently, if } \\beta_0 + X^T\\beta \\ge 0)\n$$\nand $\\hat{Y}=0$ otherwise.\nThe DPD is then computed as the difference in the empirical probabilities of a positive prediction for each group:\n$$\n\\mathrm{DPD} = \\frac{\\sum_{i: A_i=1} \\mathbb{I}(\\hat{y}_i = 1)}{N_1} - \\frac{\\sum_{i: A_i=0} \\mathbb{I}(\\hat{y}_i = 1)}{N_0}\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. This metric is calculated for both the unweighted model ($\\mathrm{DPD}_{\\text{unweighted}}$) and the reweighted model ($\\mathrm{DPD}_{\\text{reweighted}}$) on the same dataset used for training. The final reported value is the change in DPD:\n$$\n\\Delta = \\mathrm{DPD}_{\\text{reweighted}} - \\mathrm{DPD}_{\\text{unweighted}}\n$$\nA negative $\\Delta$ indicates that reweighing has reduced the demographic parity difference, bringing the model's behavior closer to the fairness ideal of DPD$=0$.", "answer": "```python\nimport numpy as np\n\ndef _sigmoid(z):\n    \"\"\"Numerically stable sigmoid function.\"\"\"\n    # For z >= 0, use 1 / (1 + exp(-z))\n    # For z  0, use exp(z) / (1 + exp(z)) to avoid overflow in exp(-z)\n    return np.where(z >= 0, 1 / (1 + np.exp(-z)), np.exp(z) / (1 + np.exp(z)))\n\ndef _train_logistic_regression(X, y, weights, lambda_reg, eta, T):\n    \"\"\"\n    Trains a logistic regression model using full-batch gradient descent.\n    \"\"\"\n    n_samples, n_features = X.shape\n    theta = np.zeros(n_features)\n    y = y.flatten()\n    weights = weights.flatten()\n\n    for _ in range(T):\n        scores = X @ theta\n        probabilities = _sigmoid(scores)\n        \n        error = probabilities - y\n        weighted_error = weights * error\n        \n        gradient = X.T @ weighted_error\n        \n        # Add L2 regularization gradient (not on intercept)\n        reg_gradient = 2 * lambda_reg * theta\n        reg_gradient[0] = 0.0  # Do not regularize the intercept term\n        gradient += reg_gradient\n        \n        theta -= eta * gradient\n        \n    return theta\n\ndef _calculate_dpd(X, A, theta):\n    \"\"\"\n    Calculates the Demographic Parity Difference (DPD).\n    \"\"\"\n    A = A.flatten()\n    \n    # Check for presence of both groups to avoid division by zero\n    if np.sum(A == 1) == 0 or np.sum(A == 0) == 0:\n        return 0.0\n\n    scores = X @ theta\n    predictions = (scores >= 0).astype(int)\n    \n    preds_a1 = predictions[A == 1]\n    preds_a0 = predictions[A == 0]\n    \n    ppr_a1 = np.mean(preds_a1) if len(preds_a1) > 0 else 0.0\n    ppr_a0 = np.mean(preds_a0) if len(preds_a0) > 0 else 0.0\n    \n    return ppr_a1 - ppr_a0\n    \ndef solve():\n    \"\"\"\n    Main function to run the complete pipeline for all test cases.\n    \"\"\"\n    # Hyperparameters from the problem statement\n    LAMBDA_REG = 1e-2\n    ETA = 1e-1\n    T_ITER = 2000\n\n    test_cases = [\n        # Test Case 1\n        (np.array([[2.0, 2.0], [1.8, 2.2], [0.0, 0.2], [0.3, -0.1], [-0.2, 0.1], [3.0, 3.1], [2.7, 2.9], [3.2, 2.8], [2.9, 3.0], [1.2, 1.0], [1.1, 1.3], [0.9, 1.2]]),\n         np.array([0,0,0,0,0,1,1,1,1,1,1,1]),\n         np.array([1,1,0,0,0,1,1,1,1,0,0,0])),\n        # Test Case 2\n        (np.array([[2.1], [1.9], [2.3], [1.8], [2.0], [2.2], [1.7], [2.4], [2.1], [1.9], [2.2], [2.0], [-1.1], [-0.9], [-1.2], [-1.0], [1.5], [-1.3], [-1.1], [-1.2]]),\n         np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1]),\n         np.array([1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,0,0,0])),\n        # Test Case 3\n        (np.array([[0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]),\n         np.array([0,0,0,1,1,1]),\n         np.array([1,0,1,0,0,1])),\n        # Test Case 4\n        (np.array([[4.0, 4.0], [5.0, 5.0], [-4.0, -4.0], [-5.0, -5.0], [3.5, 3.5], [4.0, 3.8], [-3.0, -3.0], [-4.0, -3.5]]),\n         np.array([0,0,0,0,1,1,1,1]),\n         np.array([1,1,0,0,1,1,0,0]))\n    ]\n    \n    delta_dpds = []\n\n    for X, A, Y in test_cases:\n        n_samples = X.shape[0]\n        X_aug = np.c_[np.ones(n_samples), X]\n\n        # --- Step 1: Calculate Reweighing Weights ---\n        N = n_samples\n        N_a_counts = {0: np.sum(A == 0), 1: np.sum(A == 1)}\n        N_y_counts = {0: np.sum(Y == 0), 1: np.sum(Y == 1)}\n        \n        N_ay_counts = {}\n        for a_val, y_val in zip(A, Y):\n            N_ay_counts[(a_val, y_val)] = N_ay_counts.get((a_val, y_val), 0) + 1\n\n        weights_map = {}\n        for (a, y), n_ay in N_ay_counts.items():\n            if n_ay > 0:\n                weights_map[(a, y)] = (N_a_counts[a] * N_y_counts[y]) / (N * n_ay)\n\n        reweighing_weights = np.array([weights_map.get((a_val, y_val), 1.0) for a_val, y_val in zip(A, Y)])\n        unweighted_weights = np.ones(n_samples)\n\n        # --- Step 2: Train Models ---\n        # Unweighted model\n        theta_unweighted = _train_logistic_regression(X_aug, Y, unweighted_weights, LAMBDA_REG, ETA, T_ITER)\n        \n        # Reweighted model\n        theta_reweighted = _train_logistic_regression(X_aug, Y, reweighing_weights, LAMBDA_REG, ETA, T_ITER)\n\n        # --- Step 3: Evaluate and Compute Delta ---\n        dpd_unweighted = _calculate_dpd(X_aug, A, theta_unweighted)\n        dpd_reweighted = _calculate_dpd(X_aug, A, theta_reweighted)\n        \n        delta = dpd_reweighted - dpd_unweighted\n        delta_dpds.append(round(delta, 6))\n\n    print(f\"[{','.join(f'{d:.6f}' for d in delta_dpds)}]\")\n\nsolve()\n\n```", "id": "4542367"}, {"introduction": "In many clinical settings, we may not have the ability to retrain an existing model but still need to ensure its equitable deployment. This hands-on problem explores a practical *post-processing* solution, where decision thresholds are adjusted on a per-group basis after a model has been trained. By working with Receiver Operating Characteristic (ROC) curves to equalize error rates, you will learn how to balance overall model performance with fairness constraints, a crucial trade-off in the responsible deployment of clinical algorithms [@problem_id:4542440].", "problem": "You are given per-group estimates of Receiver Operating Characteristic (ROC) curves for a binary clinical classifier. Each group has a calibrated decision score on a common threshold scale and a discrete ROC estimate represented at a finite set of thresholds. The fundamental base for this problem is the standard definitions of true positive rate (TPR), false positive rate (FPR), sensitivity, and specificity, and the fact that the ROC curve is the parametric locus of $(\\mathrm{FPR}(t), \\mathrm{TPR}(t))$ as the decision threshold $t$ varies. Specifically, for each group $g$ with positives $y=1$ and negatives $y=0$, sensitivity equals $\\mathrm{TPR}_g(t) = \\mathbb{P}(\\hat{y}=1 \\mid y=1, g)$, false positive rate equals $\\mathrm{FPR}_g(t) = \\mathbb{P}(\\hat{y}=1 \\mid y=0, g)$, and specificity equals $1 - \\mathrm{FPR}_g(t)$. For a set of groups with counts of positives $N^{+}_g$ and negatives $N^{-}_g$, the overall sensitivity at thresholding policy $\\{t_g\\}_g$ is the weighted average $\\sum_g \\mathrm{TPR}_g(t_g) N^{+}_g \\big/ \\sum_g N^{+}_g$, and the overall specificity is $\\sum_g \\left(1 - \\mathrm{FPR}_g(t_g)\\right) N^{-}_g \\big/ \\sum_g N^{-}_g$. These formulas are well-tested in clinical evaluation and constitute the foundational starting point.\n\nYour task is to implement a program that, for each test case, performs the following steps using only these principles and linear interpolation on the given ROC samples per group.\n\n1) Shared FPR target thresholding per group. Given a target false positive rate $f^{\\star}$, compute, for each group $g$, a threshold $t^{\\mathrm{fair}}_g$ such that $\\mathrm{FPR}_g(t^{\\mathrm{fair}}_g) \\approx f^{\\star}$. Because the ROC is provided as discrete samples at thresholds, you must assume that, as a function of threshold $t$, both $\\mathrm{FPR}_g(t)$ and $\\mathrm{TPR}_g(t)$ are piecewise linear between the given thresholds and monotone with respect to $t$ in the direction consistent with their definitions. Use linear interpolation to invert $\\mathrm{FPR}_g(t)$ and obtain $t^{\\mathrm{fair}}_g$ satisfying $\\mathrm{FPR}_g(t^{\\mathrm{fair}}_g) = f^{\\star}$ when possible. If $f^{\\star}$ lies outside the range spanned by the sampled $\\mathrm{FPR}_g$ values, clamp to the nearest endpoint threshold and use the corresponding endpoint rate. Then compute the per-group $\\mathrm{TPR}_g(t^{\\mathrm{fair}}_g)$ by linear interpolation on the group’s ROC samples. From these, compute the overall sensitivity and specificity under this per-group thresholding policy using the population-weighted formulas above.\n\n2) Single global threshold at the same target. Using the same $f^{\\star}$ and the same ROC samples, compute a single global threshold $t^{\\mathrm{glob}}$ applied to all groups such that the aggregate false positive rate equals $f^{\\star}$ when weighted by the negative-class populations, namely,\n$$\n\\frac{\\sum_g N^{-}_g \\, \\mathrm{FPR}_g(t^{\\mathrm{glob}})}{\\sum_g N^{-}_g} = f^{\\star}.\n$$\nAssume that as $t$ increases, $\\mathrm{FPR}_g(t)$ is non-increasing for each group $g$. Use bisection on $t$ over the common threshold domain spanned by the union of the groups’ thresholds to find $t^{\\mathrm{glob}}$. Evaluate the overall sensitivity and specificity under this single-threshold policy using the population-weighted definitions given above.\n\n3) Report change in overall metrics. For each test case, report the pair of differences\n$$\n\\Delta \\mathrm{Sensitivity} = \\mathrm{Sensitivity}_{\\mathrm{fair}} - \\mathrm{Sensitivity}_{\\mathrm{glob}}, \\quad\n\\Delta \\mathrm{Specificity} = \\mathrm{Specificity}_{\\mathrm{fair}} - \\mathrm{Specificity}_{\\mathrm{glob}}.\n$$\nAll interpolation must be linear, and any extrapolation must clamp to the nearest endpoint of the provided threshold domain. Note that by construction, the aggregate false positive rate equals $f^{\\star}$ under both policies, so $\\Delta \\mathrm{Specificity}$ should be numerically very close to $0$ up to interpolation and rounding.\n\nInput format for each test case is implicit in the code and consists of per-group arrays of thresholds and corresponding $(\\mathrm{TPR}, \\mathrm{FPR})$ samples, along with counts $(N^{+}_g, N^{-}_g)$ and the target $f^{\\star}$. Your program must embed the following test suite and produce results for each case in order. No external input should be read.\n\nTest suite:\n- Test case $1$:\n  - Groups $G = \\{A,B\\}$.\n  - Group $A$ samples: thresholds $[0.0, 0.2, 0.4, 0.6, 0.8, 1.0]$, $\\mathrm{FPR}_A = [1.0, 0.6, 0.3, 0.12, 0.05, 0.0]$, $\\mathrm{TPR}_A = [1.0, 0.9, 0.8, 0.65, 0.4, 0.0]$, counts $(N^{+}_A, N^{-}_A) = (800, 1200)$.\n  - Group $B$ samples: thresholds $[0.0, 0.2, 0.4, 0.6, 0.8, 1.0]$, $\\mathrm{FPR}_B = [1.0, 0.75, 0.5, 0.35, 0.22, 0.0]$, $\\mathrm{TPR}_B = [1.0, 0.85, 0.7, 0.55, 0.38, 0.0]$, counts $(N^{+}_B, N^{-}_B) = (200, 800)$.\n  - Target $f^{\\star} = 0.1$.\n- Test case $2$:\n  - Same groups and samples as in test case $1$, with the same counts.\n  - Target $f^{\\star} = 0.0$.\n- Test case $3$:\n  - Groups $G = \\{A,B,C\\}$.\n  - Group $A$ as in test case $1$, with counts $(N^{+}_A, N^{-}_A) = (500, 500)$.\n  - Group $B$ as in test case $1$, with counts $(N^{+}_B, N^{-}_B) = (1000, 3000)$.\n  - Group $C$ samples: thresholds $[0.0, 0.2, 0.4, 0.6, 0.8, 1.0]$, $\\mathrm{FPR}_C = [1.0, 0.65, 0.42, 0.25, 0.1, 0.0]$, $\\mathrm{TPR}_C = [1.0, 0.88, 0.74, 0.57, 0.35, 0.0]$, counts $(N^{+}_C, N^{-}_C) = (200, 800)$.\n  - Target $f^{\\star} = 0.25$.\n\nFinal output specification:\n- For each test case, output the list $[\\Delta \\mathrm{Sensitivity}, \\Delta \\mathrm{Specificity}]$ computed as explained above, where both entries are real numbers (floats).\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, with each test case’s result enclosed in its own brackets, in the order of the test suite. For example, a valid format is $[[x_1,y_1],[x_2,y_2],[x_3,y_3]]$.", "solution": "The problem is deemed valid as it is scientifically grounded in the principles of clinical model evaluation (ROC analysis), is well-posed with all necessary data and constraints provided, and is formulated objectively. The task is to compare two distinct thresholding strategies for a binary classifier across different demographic groups.\n\nThe procedure is as follows. First, we will implement the \"fair\" thresholding policy, which equalizes the false positive rate across all groups. Second, we will implement the \"global\" thresholding policy, which applies a single decision threshold to all groups. Finally, we will compute the difference in overall sensitivity and specificity between these two policies. A core component of this solution is piecewise linear interpolation, which will be used to determine decision thresholds from target rates and to determine rates at given thresholds.\n\nLet the provided data for a group $g$ be a set of thresholds $\\{t_{g,i}\\}_{i=1}^M$ and corresponding rates $\\{(\\mathrm{FPR}_{g,i}, \\mathrm{TPR}_{g,i})\\}_{i=1}^M$. The counts of positives and negatives for the group are $N_g^+$ and $N_g^-$, respectively. We assume that both $\\mathrm{FPR}_g(t)$ and $\\mathrm{TPR}_g(t)$ are piecewise linear functions of the threshold $t$, defined by the given sample points. The problem states that as $t$ increases, $\\mathrm{FPR}_g(t)$ is non-increasing. The provided data are consistent with this property, as are the corresponding $\\mathrm{TPR}_g(t)$ values.\n\n**Interpolation Function**\nTo find the value of a function $y(x)$ at a point $x'$, given samples $(x_i, y_i)$, we use linear interpolation. We first identify the interval $[x_j, x_{j+1}]$ that contains $x'$. The interpolated value is then given by:\n$$ y(x') = y_j + (y_{j+1} - y_j) \\frac{x' - x_j}{x_{j+1} - x_j} $$\nIf $x'$ is outside the domain of the given $x_i$ values, its value is clamped to the value at the nearest endpoint. This functionality is naturally provided by numerical libraries such as NumPy.\n\n**1. Shared FPR Target Thresholding (\"Fair\" Policy)**\n\nThe objective of this policy is to enforce fairness by ensuring that each group $g$ is subject to the same target false positive rate, $f^{\\star}$. This defines a per-group threshold $t^{\\mathrm{fair}}_g$.\n\nFor each group $g$:\n1.  Find the threshold $t^{\\mathrm{fair}}_g$ such that $\\mathrm{FPR}_g(t^{\\mathrm{fair}}_g) = f^{\\star}$. This is an inverse problem. We solve it by performing linear interpolation on the data points $(\\mathrm{FPR}_{g,i}, t_{g,i})$. Since the provided arrays of $\\mathrm{FPR}_g$ values are sorted in decreasing order, while interpolation functions typically require the independent variable to be in increasing order, we will interpolate on the reversed arrays.\n2.  Once $t^{\\mathrm{fair}}_g$ is found, we compute the corresponding true positive rate, $\\mathrm{TPR}_g^{\\mathrm{fair}} = \\mathrm{TPR}_g(t^{\\mathrm{fair}}_g)$. This is a forward problem, solved by linear interpolation on the data points $(t_{g,i}, \\mathrm{TPR}_{g,i})$.\n3.  The per-group false positive rate is, by construction, $f^{\\star}$ (within the precision of the interpolation).\n\nAfter computing these values for all groups, we calculate the overall metrics:\n-   Overall Sensitivity: $\\mathrm{Sensitivity}_{\\mathrm{fair}} = \\frac{\\sum_g N_g^+ \\cdot \\mathrm{TPR}_g^{\\mathrm{fair}}}{\\sum_g N_g^+}$.\n-   Overall Specificity: Since $\\mathrm{FPR}_g(t^{\\mathrm{fair}}_g) \\approx f^{\\star}$ for all $g$, the overall specificity is $\\frac{\\sum_g (1 - f^{\\star}) N_g^-}{\\sum_g N_g^-} = 1 - f^{\\star}$.\n\n**2. Single Global Threshold (\"Global\" Policy)**\n\nThis policy uses a single threshold, $t^{\\mathrm{glob}}$, for all groups. The threshold is chosen such that the population-weighted aggregate false positive rate equals the target $f^{\\star}$.\n\n1.  Define the aggregate false positive rate as a function of a single threshold $t$:\n    $$ \\mathrm{FPR}_{\\mathrm{agg}}(t) = \\frac{\\sum_g N_g^- \\cdot \\mathrm{FPR}_g(t)}{\\sum_g N_g^-} $$\n    For any given $t$, each $\\mathrm{FPR}_g(t)$ is computed via linear interpolation on the samples $(t_{g,i}, \\mathrm{FPR}_{g,i})$.\n2.  Find the root $t^{\\mathrm{glob}}$ of the equation $\\mathrm{FPR}_{\\mathrm{agg}}(t) - f^{\\star} = 0$. Since $\\mathrm{FPR}_g(t)$ is a non-increasing function of $t$ for all $g$, $\\mathrm{FPR}_{\\mathrm{agg}}(t)$ is also non-increasing. This monotonicity makes the root-finding problem well-suited for the bisection method. We search for $t^{\\mathrm{glob}}$ within the common domain of thresholds, which for the given test cases is $[0.0, 1.0]$.\n3.  Once $t^{\\mathrm{glob}}$ is found, we compute the corresponding true positive rate for each group, $\\mathrm{TPR}_g^{\\mathrm{glob}} = \\mathrm{TPR}_g(t^{\\mathrm{glob}})$, by interpolating on the $(t_{g,i}, \\mathrm{TPR}_{g,i})$ samples.\n\nThe overall metrics for the global policy are:\n-   Overall Sensitivity: $\\mathrm{Sensitivity}_{\\mathrm{glob}} = \\frac{\\sum_g N_g^+ \\cdot \\mathrm{TPR}_g^{\\mathrm{glob}}}{\\sum_g N_g^+}$.\n-   Overall Specificity: By construction, the aggregate FPR is $f^{\\star}$. Thus, $\\mathrm{Specificity}_{\\mathrm{glob}} = 1 - \\mathrm{FPR}_{\\mathrm{agg}}(t^{\\mathrm{glob}}) \\approx 1 - f^{\\star}$.\n\n**3. Report Change in Overall Metrics**\n\nFinally, we compute the differences between the metrics obtained from the two policies:\n-   $\\Delta \\mathrm{Sensitivity} = \\mathrm{Sensitivity}_{\\mathrm{fair}} - \\mathrm{Sensitivity}_{\\mathrm{glob}}$\n-   $\\Delta \\mathrm{Specificity} = \\mathrm{Specificity}_{\\mathrm{fair}} - \\mathrm{Specificity}_{\\mathrm{glob}}$\n\nAs both policies are designed to achieve an aggregate false positive rate of $f^{\\star}$, the value of $\\Delta \\mathrm{Specificity}$ is expected to be numerically close to $0$, serving as a consistency check for the implementation. The primary value of interest is $\\Delta \\mathrm{Sensitivity}$, which quantifies the change in the model's overall effectiveness at identifying positive cases when a fairness constraint (equalized FPRs) is imposed.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite, comparing two thresholding strategies.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"groups\": {\n                \"A\": {\n                    \"thresholds\": np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0]),\n                    \"fpr\": np.array([1.0, 0.6, 0.3, 0.12, 0.05, 0.0]),\n                    \"tpr\": np.array([1.0, 0.9, 0.8, 0.65, 0.4, 0.0]),\n                    \"counts\": (800, 1200) # (N+, N-)\n                },\n                \"B\": {\n                    \"thresholds\": np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0]),\n                    \"fpr\": np.array([1.0, 0.75, 0.5, 0.35, 0.22, 0.0]),\n                    \"tpr\": np.array([1.0, 0.85, 0.7, 0.55, 0.38, 0.0]),\n                    \"counts\": (200, 800)\n                }\n            },\n            \"f_star\": 0.1\n        },\n        {\n            \"groups\": {\n                \"A\": {\n                    \"thresholds\": np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0]),\n                    \"fpr\": np.array([1.0, 0.6, 0.3, 0.12, 0.05, 0.0]),\n                    \"tpr\": np.array([1.0, 0.9, 0.8, 0.65, 0.4, 0.0]),\n                    \"counts\": (800, 1200)\n                },\n                \"B\": {\n                    \"thresholds\": np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0]),\n                    \"fpr\": np.array([1.0, 0.75, 0.5, 0.35, 0.22, 0.0]),\n                    \"tpr\": np.array([1.0, 0.85, 0.7, 0.55, 0.38, 0.0]),\n                    \"counts\": (200, 800)\n                }\n            },\n            \"f_star\": 0.0\n        },\n        {\n            \"groups\": {\n                \"A\": {\n                    \"thresholds\": np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0]),\n                    \"fpr\": np.array([1.0, 0.6, 0.3, 0.12, 0.05, 0.0]),\n                    \"tpr\": np.array([1.0, 0.9, 0.8, 0.65, 0.4, 0.0]),\n                    \"counts\": (500, 500)\n                },\n                \"B\": {\n                    \"thresholds\": np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0]),\n                    \"fpr\": np.array([1.0, 0.75, 0.5, 0.35, 0.22, 0.0]),\n                    \"tpr\": np.array([1.0, 0.85, 0.7, 0.55, 0.38, 0.0]),\n                    \"counts\": (1000, 3000)\n                },\n                \"C\": {\n                    \"thresholds\": np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0]),\n                    \"fpr\": np.array([1.0, 0.65, 0.42, 0.25, 0.1, 0.0]),\n                    \"tpr\": np.array([1.0, 0.88, 0.74, 0.57, 0.35, 0.0]),\n                    \"counts\": (200, 800)\n                }\n            },\n            \"f_star\": 0.25\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        groups = case[\"groups\"]\n        f_star = case[\"f_star\"]\n        \n        total_pos_pop = sum(g['counts'][0] for g in groups.values())\n        total_neg_pop = sum(g['counts'][1] for g in groups.values())\n\n        # --- 1) \"Fair\" Thresholding Policy ---\n        total_weighted_tpr_fair = 0.0\n        for g_name, g_data in groups.items():\n            # Invert FPR vs Threshold to find threshold for f_star\n            # np.interp requires xp to be increasing, so we reverse the arrays\n            fpr_rev = g_data['fpr'][::-1]\n            thresh_rev = g_data['thresholds'][::-1]\n            t_fair = np.interp(f_star, fpr_rev, thresh_rev)\n            \n            # Interpolate to find TPR at the new threshold\n            # Here, thresholds are the x-axis and are already increasing\n            tpr_at_t_fair = np.interp(t_fair, g_data['thresholds'], g_data['tpr'])\n            \n            total_weighted_tpr_fair += tpr_at_t_fair * g_data['counts'][0]\n\n        sensitivity_fair = total_weighted_tpr_fair / total_pos_pop\n        specificity_fair = 1.0 - f_star\n\n        # --- 2) \"Global\" Thresholding Policy ---\n        def get_aggregate_fpr(t):\n            agg_fpr_num = 0.0\n            for g_data in groups.values():\n                fpr_at_t = np.interp(t, g_data['thresholds'], g_data['fpr'])\n                agg_fpr_num += fpr_at_t * g_data['counts'][1]\n            return agg_fpr_num / total_neg_pop\n        \n        # Bisection to find t_glob\n        t_low, t_high = 0.0, 1.0\n        num_iterations = 100 # High precision\n        for _ in range(num_iterations):\n            t_mid = (t_low + t_high) / 2\n            # FPR is a non-increasing function of threshold\n            if get_aggregate_fpr(t_mid) > f_star:\n                t_low = t_mid # FPR is too high, need higher threshold\n            else:\n                t_high = t_mid # FPR is low enough, can try lower threshold\n        t_glob = (t_low + t_high) / 2\n        \n        total_weighted_tpr_glob = 0.0\n        for g_data in groups.values():\n            tpr_at_t_glob = np.interp(t_glob, g_data['thresholds'], g_data['tpr'])\n            total_weighted_tpr_glob += tpr_at_t_glob * g_data['counts'][0]\n            \n        sensitivity_glob = total_weighted_tpr_glob / total_pos_pop\n        \n        # Calculate actual specificity for accuracy\n        final_agg_fpr = get_aggregate_fpr(t_glob)\n        specificity_glob = 1.0 - final_agg_fpr\n        \n        # --- 3) Report Differences ---\n        delta_sensitivity = sensitivity_fair - sensitivity_glob\n        delta_specificity = specificity_fair - specificity_glob\n        \n        results.append([delta_sensitivity, delta_specificity])\n\n    # Format the final output string exactly as specified\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4542440"}]}