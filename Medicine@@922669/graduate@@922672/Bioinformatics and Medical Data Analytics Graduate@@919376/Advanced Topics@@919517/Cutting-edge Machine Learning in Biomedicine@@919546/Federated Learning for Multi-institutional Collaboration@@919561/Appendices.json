{"hands_on_practices": [{"introduction": "A core challenge in federated learning is orchestrating training across institutions with varying computational and network resources. In synchronous protocols like Federated Averaging (FedAvg), the overall training speed is dictated by the slowest participant, known as a \"straggler.\" This exercise [@problem_id:4318598] provides a practical framework for quantifying this bottleneck by calculating the total communication overhead and estimating the wall-clock training time, helping you build an intuition for the real-world performance constraints of collaborative model training.", "problem": "A consortium of hospital biobanks is training a disease risk prediction model using Federated Learning (FL), specifically synchronous Federated Averaging (FedAvg), to comply with data minimization and non-transfer policies mandated by the governing Institutional Review Board (IRB). There are $N=20$ biobank sites. In each federated round, the coordinating server broadcasts the current model parameters to all sites, each site performs local training, and then each site uploads a compressed set of updated model parameters to the server. Governance requires inclusion of all sites per round to preserve representativeness and fairness.\n\nAssume the following scientifically grounded and realistic conditions:\n- The uncompressed model size is $M=50$ megabytes. Use decimal megabytes where $1\\,\\mathrm{MB}=10^{6}$ bytes.\n- A lossless compression scheme applied to both downlink and uplink payloads yields a compression ratio $r=0.2$, so each transmitted payload per site per direction is $rM$.\n- Network transport uses point-to-point links; broadcasting the model to $N$ sites results in $N$ separate transfers of size $rM$ (no multicast aggregation).\n- Per round, all sites must finish the downlink, local computation, and uplink phases before the round completes (synchronous barrier).\n- Nineteen sites have downlink bandwidth $100\\,\\mathrm{Mb/s}$ and uplink bandwidth $100\\,\\mathrm{Mb/s}$ with one-way latencies of $0.05\\,\\mathrm{s}$ (downlink) and $0.10\\,\\mathrm{s}$ (uplink). One straggler site has downlink bandwidth $20\\,\\mathrm{Mb/s}$ and uplink bandwidth $20\\,\\mathrm{Mb/s}$ with the same latencies. Use decimal megabits where $1\\,\\mathrm{Mb}=10^{6}$ bits.\n- Each site’s local training time per round is $30\\,\\mathrm{s}$, assumed equal across sites and independent of network conditions.\n- Ignore server-side aggregation time and cryptographic protocol overhead beyond the stated payload sizes and latencies.\n\nStarting from core definitions of synchronous FL round execution and the relation between data size, bandwidth, and latency, do the following:\n1. Calculate the per-round communication overhead as the total data volume transmitted across all links (both directions combined) in megabytes.\n2. Estimate the total wall-clock training time over $T$ rounds in hours, expressed as a function of $T$, using the synchronous barrier assumption that the round duration is set by the slowest site for the downlink and the slowest site for the uplink.\n\nRound your answers to four significant figures. For part 1, express the data volume in megabytes. For part 2, express the total time in hours.", "solution": "**Problem Validation**\n\nThe provided problem statement is analyzed against the required criteria for validity.\n\n**Step 1: Extract Givens**\n- Number of sites: $N=20$\n- Federated Learning model: Synchronous Federated Averaging (FedAvg)\n- Governance requirement: Inclusion of all sites per round\n- Uncompressed model size: $M=50$ megabytes ($1\\,\\mathrm{MB}=10^6$ bytes)\n- Compression ratio: $r=0.2$\n- Network transport: Point-to-point links\n- Round completion condition: Synchronous barrier for downlink, local computation, and uplink phases\n- Number of normal sites: $19$\n- Normal site bandwidths: $B_{down,normal} = B_{up,normal} = 100\\,\\mathrm{Mb/s}$ ($1\\,\\mathrm{Mb}=10^6$ bits)\n- Normal site one-way latencies: $L_{down,normal} = 0.05\\,\\mathrm{s}$, $L_{up,normal} = 0.10\\,\\mathrm{s}$\n- Number of straggler sites: $1$\n- Straggler site bandwidths: $B_{down,straggler} = B_{up,straggler} = 20\\,\\mathrm{Mb/s}$\n- Straggler site one-way latencies: $L_{down,straggler} = 0.05\\,\\mathrm{s}$, $L_{up,straggler} = 0.10\\,\\mathrm{s}$\n- Local training time per round for each site: $T_{local}=30\\,\\mathrm{s}$\n- Total number of rounds: $T$\n- Ignored factors: Server-side aggregation time, cryptographic protocol overhead\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is evaluated as follows:\n- **Scientifically Grounded**: The problem is built upon established principles of distributed machine learning (Federated Learning) and computer networking. Concepts such as synchronous rounds, communication bottlenecks caused by straggler nodes, and the modeling of data transfer time using bandwidth and latency are standard and scientifically sound. The provided numerical values are realistic for a multi-institutional research setting.\n- **Well-Posed**: The problem is clearly specified, providing all necessary parameters and constraints to arrive at a unique, meaningful solution for the two requested quantities.\n- **Objective**: The problem is stated in precise, quantitative, and unbiased technical language, free from subjective or speculative claims.\n- **Completeness and Consistency**: The problem is self-contained and free of contradictions. The number of normal sites ($19$) and straggler sites ($1$) correctly sums to the total number of sites ($N=20$).\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A complete solution is provided below.\n\n---\n\n**Part 1: Per-round communication overhead**\n\nThe total communication overhead per round, $V_{total}$, is the sum of the total data volume transmitted from the server to all sites (downlink) and from all sites to the server (uplink).\n\nFirst, we determine the size of the compressed model payload, $S$, which is transmitted over each link.\n$$S = rM$$\nGiven the uncompressed model size $M=50\\,\\mathrm{MB}$ and the compression ratio $r=0.2$:\n$$S = 0.2 \\times 50\\,\\mathrm{MB} = 10\\,\\mathrm{MB}$$\nIn the downlink phase, the server sends this payload to each of the $N=20$ sites. Since the links are point-to-point, the total downlink volume, $V_{down}$, is:\n$$V_{down} = N \\times S = 20 \\times 10\\,\\mathrm{MB} = 200\\,\\mathrm{MB}$$\nIn the uplink phase, each of the $N=20$ sites sends its updated (and compressed) model parameters back to the server. The total uplink volume, $V_{up}$, is therefore identical:\n$$V_{up} = N \\times S = 20 \\times 10\\,\\mathrm{MB} = 200\\,\\mathrm{MB}$$\nThe total per-round communication overhead is the sum of the downlink and uplink volumes:\n$$V_{total} = V_{down} + V_{up} = 200\\,\\mathrm{MB} + 200\\,\\mathrm{MB} = 400\\,\\mathrm{MB}$$\nThe problem asks for the answer to four significant figures. Thus, the total overhead is $400.0\\,\\mathrm{MB}$.\n\n**Part 2: Total wall-clock training time over $T$ rounds**\n\nThe total training time, $T_{total}$, for $T$ rounds is the product of the number of rounds and the duration of a single round, $T_{round}$.\n$$T_{total}(T) = T \\times T_{round}$$\nThe duration of a single synchronous round is the sum of the durations of its sequential phases: downlink, local computation, and uplink. The duration of each communication phase is determined by the slowest (straggler) site due to the synchronous barrier.\n\nFirst, we convert the payload size $S$ from megabytes to megabits to be consistent with the bandwidth units ($1\\,\\mathrm{Mb/s} = 10^6\\,\\mathrm{bits/s}$). Using the conversion $1\\,\\mathrm{byte} = 8\\,\\mathrm{bits}$:\n$$S = 10\\,\\mathrm{MB} = 10 \\times 10^6\\,\\mathrm{bytes} = 10 \\times 10^6 \\times 8\\,\\mathrm{bits} = 80 \\times 10^6\\,\\mathrm{bits} = 80\\,\\mathrm{Mb}$$\nThe time for a single data transfer is modeled as the sum of one-way latency ($L$) and serialization time ($S/B$, where $B$ is bandwidth).\n\n**Downlink Phase Duration ($T_{down\\_phase}$)**\nThis is the time until the last site receives the model update. We must compare the total downlink time for a normal site and the straggler.\n- For a normal site:\n$$T_{down,normal} = L_{down,normal} + \\frac{S}{B_{down,normal}} = 0.05\\,\\mathrm{s} + \\frac{80\\,\\mathrm{Mb}}{100\\,\\mathrm{Mb/s}} = 0.05\\,\\mathrm{s} + 0.8\\,\\mathrm{s} = 0.85\\,\\mathrm{s}$$\n- For the straggler site:\n$$T_{down,straggler} = L_{down,straggler} + \\frac{S}{B_{down,straggler}} = 0.05\\,\\mathrm{s} + \\frac{80\\,\\mathrm{Mb}}{20\\,\\mathrm{Mb/s}} = 0.05\\,\\mathrm{s} + 4.0\\,\\mathrm{s} = 4.05\\,\\mathrm{s}$$\nThe phase duration is the maximum of these times:\n$$T_{down\\_phase} = \\max(T_{down,normal}, T_{down,straggler}) = 4.05\\,\\mathrm{s}$$\n\n**Local Computation Phase Duration ($T_{local}$)**\nThis is given as identical for all sites:\n$$T_{local} = 30\\,\\mathrm{s}$$\n\n**Uplink Phase Duration ($T_{up\\_phase}$)**\nThis is the time until the server receives the update from the last site.\n- For a normal site:\n$$T_{up,normal} = L_{up,normal} + \\frac{S}{B_{up,normal}} = 0.10\\,\\mathrm{s} + \\frac{80\\,\\mathrm{Mb}}{100\\,\\mathrm{Mb/s}} = 0.10\\,\\mathrm{s} + 0.8\\,\\mathrm{s} = 0.90\\,\\mathrm{s}$$\n- For the straggler site:\n$$T_{up,straggler} = L_{up,straggler} + \\frac{S}{B_{up,straggler}} = 0.10\\,\\mathrm{s} + \\frac{80\\,\\mathrm{Mb}}{20\\,\\mathrm{Mb/s}} = 0.10\\,\\mathrm{s} + 4.0\\,\\mathrm{s} = 4.10\\,\\mathrm{s}$$\nThe phase duration is the maximum of these times:\n$$T_{up\\_phase} = \\max(T_{up,normal}, T_{up,straggler}) = 4.10\\,\\mathrm{s}$$\n\n**Total Round Duration ($T_{round}$)**\nThe total time for one synchronous round is the sum of the durations of the three sequential phases:\n$$T_{round} = T_{down\\_phase} + T_{local} + T_{up\\_phase} = 4.05\\,\\mathrm{s} + 30\\,\\mathrm{s} + 4.10\\,\\mathrm{s} = 38.15\\,\\mathrm{s}$$\n\n**Total Training Time for $T$ Rounds**\nThe total time over $T$ rounds is expressed in hours. There are $3600\\,\\mathrm{s}$ in one hour.\n$$T_{total}(T) = T \\times T_{round} = T \\times 38.15\\,\\mathrm{s} \\times \\frac{1\\,\\mathrm{hour}}{3600\\,\\mathrm{s}} = T \\times \\frac{38.15}{3600}\\,\\mathrm{hours}$$\n$$T_{total}(T) \\approx T \\times 0.01059722...\\,\\mathrm{hours}$$\nRounding the coefficient to four significant figures yields $0.01060$.\n$$T_{total}(T) \\approx 0.01060\\,T\\,\\mathrm{hours}$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n400.0 & 0.01060T\n\\end{pmatrix}\n}\n$$", "id": "4318598"}, {"introduction": "Perhaps the most fundamental challenge in federated learning is statistical heterogeneity, which arises when data is not independent and identically distributed (Non-IID) across collaborating institutions. This scenario is the norm in medical research, where patient populations vary. This practice problem [@problem_id:4563886] moves beyond simply weighting updates by data size and challenges you to derive an optimal aggregation strategy that minimizes the error between the federated gradient and the true global gradient, offering deep insight into how to counteract the divergent pulls of heterogeneous data.", "problem": "Consider two hospitals, $A$ and $B$, collaborating via Federated Averaging (FedAvg) to train a logistic regression model for a binary disease prediction task using Electronic Health Records (EHR). The data are Non-Independent and Identically Distributed (Non-IID) across hospitals due to differing patient demographics and comorbidity profiles, leading to statistical heterogeneity in local gradients.\n\nAt a particular training round, the server aggregates one-dimensional stochastic gradients from the two hospitals for a single model parameter using weights $w$ for hospital $A$ and $1-w$ for hospital $B$. Let the local stochastic gradients at this round be modeled as $g_A = \\mu_A + \\epsilon_A$ and $g_B = \\mu_B + \\epsilon_B$, where $\\epsilon_A$ and $\\epsilon_B$ are zero-mean random variables representing stochastic gradient noise, independent across hospitals, with variances $\\mathrm{Var}(\\epsilon_A) = \\sigma_A^2$ and $\\mathrm{Var}(\\epsilon_B) = \\sigma_B^2$. The expected local gradient means at this round are $\\mu_A$ and $\\mu_B$, respectively. The centralized gradient that would be obtained if all data were pooled is the data-proportion weighted mean $T = p_A \\mu_A + p_B \\mu_B$, where $p_A$ and $p_B$ are the fractions of total samples held by hospitals $A$ and $B$, respectively, and satisfy $p_A + p_B = 1$.\n\nThe server seeks to choose $w$ to minimize the expected squared deviation between the aggregated federated gradient and the centralized gradient at this round, that is, to minimize\n$$\n\\mathbb{E}\\left[\\left(w g_A + (1-w) g_B - T\\right)^{2}\\right].\n$$\n\nAssume independence of $\\epsilon_A$ and $\\epsilon_B$, unbiasedness of local stochastic gradients, and that $w$ is a real number satisfying $w + (1-w) = 1$. Use the following scientifically plausible values obtained at the current round from local gradient statistics:\n- $\\mu_A = 0.14$, $\\mu_B = -0.02$,\n- $\\sigma_A^2 = 0.0025$, $\\sigma_B^2 = 0.0064$,\n- $p_A = 0.65$, $p_B = 0.35$.\n\nStarting from the above definitions and assumptions, derive the expression for the expected squared deviation, and find the value of $w$ that minimizes it. Express the final answer as a single real number. Round your answer to four significant figures.", "solution": "The problem is well-posed, scientifically grounded in the principles of federated learning and optimization, and contains all necessary information for a unique solution. The numerical values provided are plausible within the context of training a machine learning model. Therefore, I will proceed with the derivation.\n\nThe objective is to find the weighting factor $w$ that minimizes the expected squared deviation between the aggregated federated gradient and the centralized gradient. The objective function is given by:\n$$\nL(w) = \\mathbb{E}\\left[\\left(w g_A + (1-w) g_B - T\\right)^{2}\\right]\n$$\nwhere $g_A$ and $g_B$ are stochastic gradients, and $T$ is the constant centralized gradient target.\n\nWe can decompose the expected squared error using the bias-variance decomposition. For any random variable $X$ and constant $c$, we have $\\mathbb{E}[(X-c)^2] = \\mathrm{Var}(X) + (\\mathbb{E}[X]-c)^2$. Let $X = w g_A + (1-w) g_B$ and $c = T$. The objective function becomes:\n$$\nL(w) = \\mathrm{Var}\\left(w g_A + (1-w) g_B\\right) + \\left(\\mathbb{E}\\left[w g_A + (1-w) g_B\\right] - T\\right)^2\n$$\nWe will analyze the variance and bias terms separately.\n\nFirst, let's compute the variance term. The local stochastic gradients are given as $g_A = \\mu_A + \\epsilon_A$ and $g_B = \\mu_B + \\epsilon_B$. Since $\\mu_A$ and $\\mu_B$ are constants, the variance of the local gradients is:\n$$\n\\mathrm{Var}(g_A) = \\mathrm{Var}(\\mu_A + \\epsilon_A) = \\mathrm{Var}(\\epsilon_A) = \\sigma_A^2\n$$\n$$\n\\mathrm{Var}(g_B) = \\mathrm{Var}(\\mu_B + \\epsilon_B) = \\mathrm{Var}(\\epsilon_B) = \\sigma_B^2\n$$\nThe problem states that the stochastic noise terms $\\epsilon_A$ and $\\epsilon_B$ are independent, which implies that the stochastic gradients $g_A$ and $g_B$ are also independent. For independent random variables, the variance of a weighted sum is the weighted sum of their variances:\n$$\n\\mathrm{Var}\\left(w g_A + (1-w) g_B\\right) = w^2 \\mathrm{Var}(g_A) + (1-w)^2 \\mathrm{Var}(g_B) = w^2 \\sigma_A^2 + (1-w)^2 \\sigma_B^2\n$$\n\nNext, let's compute the squared bias term, $(\\mathbb{E}[w g_A + (1-w) g_B] - T)^2$. We first find the expectation of the aggregated gradient. Using the linearity of expectation and that $\\mathbb{E}[\\epsilon_A] = 0$ and $\\mathbb{E}[\\epsilon_B] = 0$:\n$$\n\\mathbb{E}[g_A] = \\mathbb{E}[\\mu_A + \\epsilon_A] = \\mu_A\n$$\n$$\n\\mathbb{E}[g_B] = \\mathbb{E}[\\mu_B + \\epsilon_B] = \\mu_B\n$$\nThe expectation of the aggregated gradient is:\n$$\n\\mathbb{E}[w g_A + (1-w) g_B] = w \\mathbb{E}[g_A] + (1-w) \\mathbb{E}[g_B] = w \\mu_A + (1-w) \\mu_B\n$$\nThe centralized gradient is $T = p_A \\mu_A + p_B \\mu_B$. Since $p_A + p_B = 1$, we have $p_B = 1 - p_A$. The deviation from the target is:\n$$\n\\mathbb{E}[w g_A + (1-w) g_B] - T = (w \\mu_A + (1-w) \\mu_B) - (p_A \\mu_A + (1-p_A) \\mu_B)\n$$\n$$\n= (w - p_A)\\mu_A + ((1-w) - (1-p_A))\\mu_B = (w - p_A)\\mu_A - (w - p_A)\\mu_B = (w - p_A)(\\mu_A - \\mu_B)\n$$\nThe squared bias term is therefore:\n$$\n\\left((w-p_A)(\\mu_A - \\mu_B)\\right)^2 = (w-p_A)^2 (\\mu_A - \\mu_B)^2\n$$\n\nCombining the variance and squared bias terms, we get the full objective function $L(w)$:\n$$\nL(w) = w^2 \\sigma_A^2 + (1-w)^2 \\sigma_B^2 + (w-p_A)^2 (\\mu_A - \\mu_B)^2\n$$\nTo find the value of $w$ that minimizes $L(w)$, we compute the derivative with respect to $w$ and set it to $0$.\n$$\n\\frac{dL}{dw} = \\frac{d}{dw} \\left[ w^2 \\sigma_A^2 + (1-w)^2 \\sigma_B^2 + (w-p_A)^2 (\\mu_A - \\mu_B)^2 \\right]\n$$\n$$\n\\frac{dL}{dw} = 2w \\sigma_A^2 + 2(1-w)(-1) \\sigma_B^2 + 2(w-p_A) (\\mu_A - \\mu_B)^2 = 0\n$$\nDividing by $2$:\n$$\nw \\sigma_A^2 - (1-w) \\sigma_B^2 + (w-p_A) (\\mu_A - \\mu_B)^2 = 0\n$$\nWe expand and collect terms containing $w$:\n$$\nw \\sigma_A^2 - \\sigma_B^2 + w \\sigma_B^2 + w (\\mu_A - \\mu_B)^2 - p_A (\\mu_A - \\mu_B)^2 = 0\n$$\n$$\nw (\\sigma_A^2 + \\sigma_B^2 + (\\mu_A - \\mu_B)^2) = \\sigma_B^2 + p_A (\\mu_A - \\mu_B)^2\n$$\nSolving for $w$:\n$$\nw = \\frac{\\sigma_B^2 + p_A (\\mu_A - \\mu_B)^2}{\\sigma_A^2 + \\sigma_B^2 + (\\mu_A - \\mu_B)^2}\n$$\nThe second derivative is $\\frac{d^2L}{dw^2} = 2\\sigma_A^2 + 2\\sigma_B^2 + 2(\\mu_A - \\mu_B)^2$, which is strictly positive as the variances are positive, confirming that this value of $w$ corresponds to a minimum.\n\nNow, we substitute the given numerical values:\n- $\\mu_A = 0.14$\n- $\\mu_B = -0.02$\n- $\\sigma_A^2 = 0.0025$\n- $\\sigma_B^2 = 0.0064$\n- $p_A = 0.65$\n\nFirst, we calculate the term related to the divergence of the expected gradients:\n$$\n\\mu_A - \\mu_B = 0.14 - (-0.02) = 0.16\n$$\n$$\n(\\mu_A - \\mu_B)^2 = (0.16)^2 = 0.0256\n$$\nNow, substitute these values into the expression for $w$.\nThe numerator is:\n$$\n\\sigma_B^2 + p_A (\\mu_A - \\mu_B)^2 = 0.0064 + (0.65)(0.0256) = 0.0064 + 0.01664 = 0.02304\n$$\nThe denominator is:\n$$\n\\sigma_A^2 + \\sigma_B^2 + (\\mu_A - \\mu_B)^2 = 0.0025 + 0.0064 + 0.0256 = 0.0345\n$$\nSo, the optimal value of $w$ is:\n$$\nw = \\frac{0.02304}{0.0345} \\approx 0.66782608...\n$$\nRounding the result to four significant figures, we get $w = 0.6678$.", "answer": "$$\n\\boxed{0.6678}\n$$", "id": "4563886"}, {"introduction": "Differential Privacy (DP) provides a rigorous mathematical framework for guaranteeing patient privacy, but this guarantee comes at the cost of introducing statistical noise, which can impact model accuracy. Understanding and quantifying this utility-privacy trade-off is essential for any practitioner of privacy-preserving machine learning. This hands-on exercise [@problem_id:4563891] allows you to analyze this trade-off by deriving the expected error that a DP mechanism introduces into a fundamental data preprocessing task—feature normalization—thereby making the abstract cost of privacy tangible and calculable.", "problem": "A consortium of three hospitals engages in Federated Learning (FL) to jointly normalize a clinically validated inflammatory biomarker for downstream multi-institutional modeling in bioinformatics and medical data analytics. They need to apply secure feature scaling and normalization that preserves patient privacy while enabling cross-site comparability. Each hospital computes local sufficient statistics and contributes to a secure aggregation protocol with Differential Privacy (DP) by adding independent Gaussian noise to both the sum of feature values and the sum of squared feature values prior to secure aggregation.\n\nAssume the following scientifically realistic and internally consistent setup. Across all patients pooled from the three institutions, the total number of patients is $N = 200$, and the true global sufficient statistics are $\\sum_{j=1}^{N} x_{j} = 3400$ and $\\sum_{j=1}^{N} x_{j}^{2} = 62800$, where $x_{j}$ denotes the biomarker value for patient $j$. Each institution $i \\in \\{1,2,3\\}$ adds independent Gaussian noise $\\eta_{S,i} \\sim \\mathcal{N}(0, \\sigma_{S,i}^{2})$ to its contribution to the global sum and independent Gaussian noise $\\eta_{Q,i} \\sim \\mathcal{N}(0, \\sigma_{Q,i}^{2})$ to its contribution to the global sum of squares. The variances are $\\sigma_{S,1}^{2} = 4$, $\\sigma_{S,2}^{2} = 9$, $\\sigma_{S,3}^{2} = 1$, and $\\sigma_{Q,1}^{2} = 900$, $\\sigma_{Q,2}^{2} = 1600$, $\\sigma_{Q,3}^{2} = 2500$. All noise terms are mutually independent and independent of the data.\n\nDefine the true global population mean and variance by the core identities $ \\mu = \\frac{1}{N} \\sum_{j=1}^{N} x_{j}$ and $ \\sigma^{2} = \\frac{1}{N} \\sum_{j=1}^{N} x_{j}^{2} - \\mu^{2}$, and the true global $z$-score for a fixed new patient value $x$ by $ z = \\frac{x - \\mu}{\\sigma}$. The secure aggregator forms noisy estimates using the same core identities, namely\n$$\n\\tilde{\\mu} = \\frac{1}{N} \\left( \\sum_{j=1}^{N} x_{j} + \\sum_{i=1}^{3} \\eta_{S,i} \\right), \\quad\n\\tilde{M}_{2} = \\frac{1}{N} \\left( \\sum_{j=1}^{N} x_{j}^{2} + \\sum_{i=1}^{3} \\eta_{Q,i} \\right), \\quad\n\\tilde{\\sigma} = \\sqrt{ \\tilde{M}_{2} - \\tilde{\\mu}^{2} },\n$$\nand the DP-normalized $z$-score by $ \\tilde{z} = \\frac{x - \\tilde{\\mu}}{\\tilde{\\sigma}}$.\n\nFor a new patient value $x = 20$, derive from first principles the leading-order approximation (in the privacy noise magnitudes) to the expected squared deviation $ \\mathbb{E}\\!\\left[ (\\tilde{z} - z)^{2} \\right] $ induced by the DP noise under the independence assumptions described above, and evaluate it numerically for the given parameters. Report the final result as a single dimensionless decimal number, rounded to four significant figures.", "solution": "The task is to quantify how Differential Privacy (DP) noise injected into the secure aggregation of the global sum and the global sum of squares perturbs the $z$-score normalization. We begin with core definitions of the global sufficient statistics and normalization, and then propagate the noise to leading order.\n\nFirst, compute the true global mean and variance using the foundational identities\n$$\n\\mu = \\frac{1}{N} \\sum_{j=1}^{N} x_{j}, \\quad \\sigma^{2} = \\frac{1}{N} \\sum_{j=1}^{N} x_{j}^{2} - \\mu^{2}.\n$$\nWith $N = 200$, $\\sum_{j=1}^{N} x_{j} = 3400$, and $\\sum_{j=1}^{N} x_{j}^{2} = 62800$, we obtain\n$$\n\\mu = \\frac{3400}{200} = 17, \\quad \\tilde{M}_{2}^{\\text{true}} = \\frac{62800}{200} = 314, \\quad \\sigma^{2} = 314 - 17^{2} = 314 - 289 = 25, \\quad \\sigma = \\sqrt{25} = 5.\n$$\nFor the fixed new patient value $x = 20$, the true global $z$-score is\n$$\nz = \\frac{x - \\mu}{\\sigma} = \\frac{20 - 17}{5} = \\frac{3}{5}.\n$$\n\nUnder secure aggregation with DP, the aggregator observes noisy sums. Define the aggregated noise for the sum as\n$$\n\\eta_{S} = \\eta_{S,1} + \\eta_{S,2} + \\eta_{S,3},\n$$\nand for the sum of squares as\n$$\n\\eta_{Q} = \\eta_{Q,1} + \\eta_{Q,2} + \\eta_{Q,3}.\n$$\nSince the site-specific noises are independent Gaussian random variables, the aggregated noises satisfy\n$$\n\\eta_{S} \\sim \\mathcal{N}\\!\\left(0, \\sigma_{S}^{2}\\right), \\quad \\sigma_{S}^{2} = \\sigma_{S,1}^{2} + \\sigma_{S,2}^{2} + \\sigma_{S,3}^{2} = 4 + 9 + 1 = 14,\n$$\n$$\n\\eta_{Q} \\sim \\mathcal{N}\\!\\left(0, \\sigma_{Q}^{2}\\right), \\quad \\sigma_{Q}^{2} = \\sigma_{Q,1}^{2} + \\sigma_{Q,2}^{2} + \\sigma_{Q,3}^{2} = 900 + 1600 + 2500 = 5000.\n$$\nThe noisy statistics are then\n$$\n\\tilde{\\mu} = \\mu + \\varepsilon_{S}, \\quad \\varepsilon_{S} = \\frac{\\eta_{S}}{N}, \\quad \\mathrm{Var}(\\varepsilon_{S}) = \\frac{\\sigma_{S}^{2}}{N^{2}},\n$$\n$$\n\\tilde{M}_{2} = \\tilde{M}_{2}^{\\text{true}} + \\varepsilon_{Q} = 314 + \\varepsilon_{Q}, \\quad \\varepsilon_{Q} = \\frac{\\eta_{Q}}{N}, \\quad \\mathrm{Var}(\\varepsilon_{Q}) = \\frac{\\sigma_{Q}^{2}}{N^{2}}.\n$$\nThe noisy standard deviation is derived from the identity $ \\sigma = \\sqrt{ M_{2} - \\mu^{2} } $. Therefore,\n$$\n\\tilde{\\sigma} = \\sqrt{ \\tilde{M}_{2} - \\tilde{\\mu}^{2} } = \\sqrt{ (\\sigma^{2} + \\mu^{2} + \\varepsilon_{Q}) - (\\mu + \\varepsilon_{S})^{2} }.\n$$\nExpanding the square and collecting terms,\n$$\n\\tilde{\\sigma} = \\sqrt{ \\sigma^{2} + \\varepsilon_{Q} - 2 \\mu \\varepsilon_{S} - \\varepsilon_{S}^{2} }.\n$$\nFor leading-order analysis in the noise magnitudes, we drop the second-order term $\\varepsilon_{S}^{2}$ inside the square root and apply the first-order Taylor expansion $ \\sqrt{a + \\delta} \\approx \\sqrt{a} + \\frac{\\delta}{2 \\sqrt{a}} $ with $ a = \\sigma^{2} $ and $ \\delta = \\varepsilon_{Q} - 2 \\mu \\varepsilon_{S} $ to obtain\n$$\n\\tilde{\\sigma} \\approx \\sigma + \\frac{1}{2 \\sigma} \\left( \\varepsilon_{Q} - 2 \\mu \\varepsilon_{S} \\right).\n$$\nThe noisy $z$-score is\n$$\n\\tilde{z} = \\frac{x - \\tilde{\\mu}}{\\tilde{\\sigma}} = \\frac{ (x - \\mu) - \\varepsilon_{S} }{ \\sigma + \\frac{1}{2 \\sigma} \\left( \\varepsilon_{Q} - 2 \\mu \\varepsilon_{S} \\right) }.\n$$\nUsing the first-order expansion for the reciprocal, $ \\frac{1}{\\sigma + c} \\approx \\frac{1}{\\sigma} - \\frac{c}{\\sigma^{2}} $, with $ c = \\frac{1}{2 \\sigma} \\left( \\varepsilon_{Q} - 2 \\mu \\varepsilon_{S} \\right) $, we get\n$$\n\\frac{1}{\\tilde{\\sigma}} \\approx \\frac{1}{\\sigma} - \\frac{1}{2 \\sigma^{3}} \\left( \\varepsilon_{Q} - 2 \\mu \\varepsilon_{S} \\right).\n$$\nTherefore,\n$$\n\\tilde{z} \\approx \\left( (x - \\mu) - \\varepsilon_{S} \\right) \\left( \\frac{1}{\\sigma} - \\frac{1}{2 \\sigma^{3}} \\left( \\varepsilon_{Q} - 2 \\mu \\varepsilon_{S} \\right) \\right).\n$$\nMultiplying out and discarding second-order products of zero-mean independent noises, the perturbation of the $z$-score is\n$$\n\\tilde{z} - z \\approx - \\frac{\\varepsilon_{S}}{\\sigma} - \\frac{(x - \\mu)}{2 \\sigma^{3}} \\varepsilon_{Q} + \\frac{(x - \\mu) \\mu}{\\sigma^{3}} \\varepsilon_{S}.\n$$\nCollecting terms in $\\varepsilon_{S}$ and $\\varepsilon_{Q}$,\n$$\n\\tilde{z} - z \\approx c_{S} \\varepsilon_{S} + c_{Q} \\varepsilon_{Q}, \\quad c_{S} = - \\frac{1}{\\sigma} + \\frac{(x - \\mu) \\mu}{\\sigma^{3}}, \\quad c_{Q} = - \\frac{(x - \\mu)}{2 \\sigma^{3}}.\n$$\nBy independence and zero mean, the leading-order mean squared deviation is\n$$\n\\mathbb{E}\\!\\left[ (\\tilde{z} - z)^{2} \\right] \\approx c_{S}^{2} \\, \\mathrm{Var}(\\varepsilon_{S}) + c_{Q}^{2} \\, \\mathrm{Var}(\\varepsilon_{Q}) = c_{S}^{2} \\frac{\\sigma_{S}^{2}}{N^{2}} + c_{Q}^{2} \\frac{\\sigma_{Q}^{2}}{N^{2}}.\n$$\n\nWe now substitute the numeric values. With $\\sigma = 5$, $\\mu = 17$, $x - \\mu = 3$, $N = 200$, $\\sigma_{S}^{2} = 14$, and $\\sigma_{Q}^{2} = 5000$, we compute\n$$\nc_{S} = - \\frac{1}{5} + \\frac{3 \\cdot 17}{5^{3}} = -0.2 + \\frac{51}{125} = -0.2 + 0.408 = 0.208,\n$$\n$$\nc_{Q} = - \\frac{3}{2 \\cdot 5^{3}} = - \\frac{3}{250} = -0.012.\n$$\nThe variances of the normalized noise terms are\n$$\n\\mathrm{Var}(\\varepsilon_{S}) = \\frac{14}{200^{2}} = \\frac{14}{40000} = 0.00035, \\quad \\mathrm{Var}(\\varepsilon_{Q}) = \\frac{5000}{200^{2}} = \\frac{5000}{40000} = 0.125.\n$$\nTherefore,\n$$\n\\mathbb{E}\\!\\left[ (\\tilde{z} - z)^{2} \\right] \\approx (0.208)^{2} \\cdot 0.00035 + (0.012)^{2} \\cdot 0.125.\n$$\nCompute the terms:\n$$\n(0.208)^{2} = 0.043264, \\quad 0.043264 \\cdot 0.00035 = 0.0000151424,\n$$\n$$\n(0.012)^{2} = 0.000144, \\quad 0.000144 \\cdot 0.125 = 0.000018.\n$$\nSumming,\n$$\n\\mathbb{E}\\!\\left[ (\\tilde{z} - z)^{2} \\right] \\approx 0.0000151424 + 0.000018 = 0.0000331424.\n$$\nRounded to four significant figures, the dimensionless expected squared deviation is $3.314 \\times 10^{-5}$.", "answer": "$$\\boxed{3.314 \\times 10^{-5}}$$", "id": "4563891"}]}