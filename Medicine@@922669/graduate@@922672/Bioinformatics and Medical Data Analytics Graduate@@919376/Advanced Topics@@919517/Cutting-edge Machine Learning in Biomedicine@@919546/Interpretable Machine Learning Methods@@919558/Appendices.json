{"hands_on_practices": [{"introduction": "To truly grasp the power of Shapley values in model interpretation, we must begin with their theoretical foundations. This first exercise [@problem_id:4575319] challenges you to derive SHAP values for a simple linear model not by using a pre-built library, but by applying the fundamental axioms of cooperative game theory directly. By working from first principles, you will build a robust intuition for how SHAP fairly distributes a model's prediction among its input features.", "problem": "In a clinical risk stratification scenario for cardiovascular events, consider a linear risk model aggregating three normalized biomarker measurements, denoted by the feature vector $x = (x_1, x_2, x_3)$. The model outputs a scalar risk score given by $f(x) = x_1 + 2 x_2 + 3 x_3$, where the learned weights $1$, $2$, and $3$ reflect the relative importance of each biomarker. Under the Shapley Additive Explanations (SHAP) framework, interpretability is posed as a cooperative game over the set of features $F = \\{1, 2, 3\\}$, with a baseline representing the absence of information. Assume the baseline is the zero vector, so $f(0,0,0) = 0$, and missing features are imputed to their baseline value.\n\nFor the patient instance $x = (1, 1, 1)$, compute the Shapley values (feature attributions) for the three biomarkers using the cooperative game formulation where the value function for a coalition $S \\subseteq F$ is the model output when features in $S$ are present and those in $F \\setminus S$ are set to baseline. Derive your result from fundamental axioms of Shapley values (efficiency, symmetry, dummy, and linearity), and not by appealing to shortcut formulas. Then verify the efficiency property by showing that the sum of the three Shapley values equals the difference $f(x) - f(0,0,0)$.\n\nReport the three Shapley values as a single row vector. No rounding is required, and no physical units apply.", "solution": "The user-provided problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n-   **Model:** A linear risk model for cardiovascular events.\n-   **Feature Vector:** $x = (x_1, x_2, x_3)$ representing three normalized biomarker measurements.\n-   **Risk Score Function:** $f(x) = x_1 + 2 x_2 + 3 x_3$. The weights are $w_1 = 1$, $w_2 = 2$, and $w_3 = 3$.\n-   **Interpretability Framework:** Shapley Additive Explanations (SHAP).\n-   **Cooperative Game:** The game is defined over the set of features $F = \\{1, 2, 3\\}$.\n-   **Baseline:** The zero vector, $(0,0,0)$. The model output at baseline is $f(0,0,0) = 0$.\n-   **Imputation:** Missing features (those not in a coalition) are imputed to their baseline value, which is $0$.\n-   **Instance for Explanation:** The specific patient instance is $x = (1, 1, 1)$.\n-   **Value Function:** For a coalition of features $S \\subseteq F$, the value function $v(S)$ is the model output when features in $S$ are present (i.e., take their values from the instance $x$) and features in $F \\setminus S$ are absent (i.e., take their baseline values).\n-   **Task 1:** Compute the Shapley values ($\\phi_1, \\phi_2, \\phi_3$) for the three biomarkers for the instance $x = (1, 1, 1)$.\n-   **Constraint:** The derivation must be from the fundamental axioms of Shapley values (efficiency, symmetry, linearity) and not from shortcut formulas.\n-   **Task 2:** Verify the efficiency property by showing that the sum of the Shapley values equals $f(x) - f(0,0,0)$.\n-   **Reporting Format:** The three Shapley values are to be reported as a single row vector.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientific Grounding:** The problem is well-grounded. SHAP is a standard, rigorously defined method in interpretable machine learning. Linear models are fundamental in statistics and machine learning. The application to clinical risk stratification is a common and appropriate use case.\n-   **Well-Posedness:** The problem is well-posed. The model, feature set, instance, and baseline are all explicitly defined. The value function of the cooperative game is unambiguously determined by these definitions. The Shapley value for a given cooperative game is unique. The constraint to use axioms guides the method of solution but does not render the problem ill-posed.\n-   **Objectivity:** The problem is stated in precise, objective mathematical language. All terms have standard, non-ambiguous definitions.\n\nThe problem is devoid of scientific unsoundness, incompleteness, contradiction, or any other listed flaws. It is a valid, well-formulated problem in applied mathematics and machine learning theory.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe core of the problem is to compute the Shapley values for the cooperative game defined by the model $f(x)$ for the specific instance $x=(1,1,1)$. The Shapley values $(\\phi_1, \\phi_2, \\phi_3)$ quantify the contribution of each feature to the model's prediction, relative to a baseline.\n\nFirst, we formally define the value function $v(S)$ for any coalition of features $S \\subseteq F = \\{1, 2, 3\\}$. The value $v(S)$ is the prediction of the model when features in $S$ have their values from the instance $x=(1, 1, 1)$ and features not in $S$ have their baseline values of $0$. Let $x_S$ be this input vector.\nThe model is $f(z_1, z_2, z_3) = w_1 z_1 + w_2 z_2 + w_3 z_3$, with weights $w_1=1$, $w_2=2$, $w_3=3$.\nThe value function is then $v(S) = f(x_S) = \\sum_{i=1}^3 w_i (x_S)_i$.\nThe components of $x_S$ are $(x_S)_i = x_i$ if $i \\in S$ and $(x_S)_i = 0$ if $i \\notin S$.\nGiven the instance $x=(1,1,1)$, we have $x_i=1$ for all $i \\in F$.\nThus, the value function simplifies to:\n$$v(S) = \\sum_{i \\in S} w_i x_i = \\sum_{i \\in S} w_i(1) = \\sum_{i \\in S} w_i$$\nThis defines the game we must analyze. The problem requires using the axioms of Shapley values. The key axiom for this problem is linearity.\n\nThe **Linearity Axiom** states that if a game's value function $v$ can be expressed as a linear combination of other value functions $u_k$, $v = \\sum_k c_k u_k$, then the Shapley value for any player $i$ in game $v$ is the same linear combination of its Shapley values in games $u_k$: $\\phi_i(v) = \\sum_k c_k \\phi_i(u_k)$.\n\nWe can decompose our game $v$ into a sum of simpler games. Let's define three games $v_1, v_2, v_3$ based on the contribution of each weight:\n-   $v_1(S) = w_1$ if $1 \\in S$, and $0$ otherwise.\n-   $v_2(S) = w_2$ if $2 \\in S$, and $0$ otherwise.\n-   $v_3(S) = w_3$ if $3 \\in S$, and $0$ otherwise.\n\nIt is clear that for any coalition $S$, $v(S) = \\sum_{i \\in S} w_i = v_1(S) + v_2(S) + v_3(S)$.\nBy the Linearity Axiom, the Shapley value for any feature $j \\in F$ is the sum of its Shapley values in each of these sub-games:\n$$\\phi_j(v) = \\phi_j(v_1) + \\phi_j(v_2) + \\phi_j(v_3)$$\n\nNow, we compute the Shapley values for each simple game $v_k$ using the other axioms. Let's analyze game $v_1$.\nThe players are $\\{1, 2, 3\\}$. The value function is $v_1(S) = w_1$ if $1 \\in S$ and $0$ otherwise.\nConsider player $2$. Its marginal contribution to any coalition $T \\subseteq F \\setminus \\{2\\}$ is $v_1(T \\cup \\{2\\}) - v_1(T)$. Since the value $v_1(S)$ depends only on the presence of player $1$, adding player $2$ to any coalition $T$ does not change the value. Thus, $v_1(T \\cup \\{2\\}) = v_1(T)$ for all $T$ not containing $2$. Player $2$ is a **dummy player**.\nThe **Dummy Axiom** states that if a player contributes nothing to any coalition, its Shapley value is $0$. Therefore, $\\phi_2(v_1) = 0$. By identical reasoning, player $3$ is also a dummy player in game $v_1$, so $\\phi_3(v_1) = 0$.\n\nNow we find $\\phi_1(v_1)$. We use the **Efficiency Axiom**, which states that the sum of the Shapley values of all players must equal the total value generated by the grand coalition $F$ minus the value of the empty coalition $\\emptyset$.\nFor game $v_1$: $\\phi_1(v_1) + \\phi_2(v_1) + \\phi_3(v_1) = v_1(F) - v_1(\\emptyset)$.\nSubstituting the values we found: $\\phi_1(v_1) + 0 + 0 = v_1(\\{1,2,3\\}) - v_1(\\emptyset)$.\nFrom the definition of $v_1$, $v_1(\\{1,2,3\\}) = w_1$ (since $1 \\in \\{1,2,3\\}$) and $v_1(\\emptyset) = 0$ (since $1 \\notin \\emptyset$).\nSo, $\\phi_1(v_1) = w_1 - 0 = w_1$.\n\nWe can apply the exact same logic for games $v_2$ and $v_3$:\n-   For game $v_2$: Players $1$ and $3$ are dummies, so $\\phi_1(v_2)=0$ and $\\phi_3(v_2)=0$. By efficiency, $\\phi_2(v_2) = v_2(F) - v_2(\\emptyset) = w_2 - 0 = w_2$.\n-   For game $v_3$: Players $1$ and $2$ are dummies, so $\\phi_1(v_3)=0$ and $\\phi_2(v_3)=0$. By efficiency, $\\phi_3(v_3) = v_3(F) - v_3(\\emptyset) = w_3 - 0 = w_3$.\n\nNow we use the linearity property to combine these results for the original game $v$:\n-   $\\phi_1(v) = \\phi_1(v_1) + \\phi_1(v_2) + \\phi_1(v_3) = w_1 + 0 + 0 = w_1$.\n-   $\\phi_2(v) = \\phi_2(v_1) + \\phi_2(v_2) + \\phi_2(v_3) = 0 + w_2 + 0 = w_2$.\n-   $\\phi_3(v) = \\phi_3(v_1) + \\phi_3(v_2) + \\phi_3(v_3) = 0 + 0 + w_3 = w_3$.\n\nThe Shapley values are simply the weights of the linear model in this specific scenario where all feature values are $1$ and the baseline is $0$.\nSubstituting the given weights $w_1=1, w_2=2, w_3=3$:\n-   $\\phi_1 = 1$\n-   $\\phi_2 = 2$\n-   $\\phi_3 = 3$\n\nThe Shapley values for the three biomarkers are $(1, 2, 3)$.\n\nFinally, we verify the efficiency property for the overall game as requested. The property states that the sum of the Shapley values must equal the difference between the model's prediction for the instance $x$ and its prediction for the baseline.\nThe sum of the calculated Shapley values is:\n$$\\sum_{i=1}^3 \\phi_i = 1 + 2 + 3 = 6$$\nThe prediction for the instance $x=(1,1,1)$ is:\n$$f(x) = f(1,1,1) = 1(1) + 2(1) + 3(1) = 6$$\nThe prediction for the baseline $(0,0,0)$ is:\n$$f(0,0,0) = 1(0) + 2(0) + 3(0) = 0$$\nThe difference is $f(x) - f(0,0,0) = 6 - 0 = 6$.\nSince $\\sum \\phi_i = 6$ and $f(x) - f(0,0,0) = 6$, the efficiency property is verified.\n\nThe final answer is the vector of Shapley values.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  2  3\n\\end{pmatrix}\n}\n$$", "id": "4575319"}, {"introduction": "Building on the theoretical foundations, we now turn to a practical and widely used application: explaining tree-based models. This exercise [@problem_id:4575307] delves into the TreeSHAP algorithm, a specialized method that efficiently computes SHAP values for decision trees and their ensembles. You will work through a concrete example, learning how conditional expectations are estimated using background data statistics to attribute a tree's prediction to its features.", "problem": "Consider a binary decision tree used for risk scoring in a clinical biomarker setting with two features $x_{1}$ and $x_{2}$. The goal is to compute feature attributions using SHapley Additive exPlanations (SHAP), specialized to decision trees (TreeSHAP). For completeness, note that Local Interpretable Model-agnostic Explanations (LIME) is another method for local interpretability; however, in this task, use the TreeSHAP framework based on the cooperative game-theoretic Shapley value.\n\nThe tree has depth $2$ and the following structure and parameters, learned from a training background dataset:\n\n- Root node splits on $x_{1}$ at threshold $t_{1} = 3$, sending to the left child if $x_{1}  3$ and to the right child otherwise.\n- Left child of the root splits on $x_{2}$ at threshold $t_{2} = 5$, sending to the left leaf if $x_{2}  5$ and to the right leaf otherwise, with leaf outputs $v_{\\text{LL}} = 2$ and $v_{\\text{LR}} = 6$.\n- Right child of the root splits on $x_{2}$ at threshold $t_{3} = 4$, sending to the left leaf if $x_{2}  4$ and to the right leaf otherwise, with leaf outputs $v_{\\text{RL}} = 9$ and $v_{\\text{RR}} = -1$.\n\nThe background data provide node cover counts (number of training samples reaching each node), which determine branch probabilities when a feature is considered “missing” in TreeSHAP:\n- At the root: total cover $N = 100$, with $N_{\\text{L}} = 60$ on the left branch and $N_{\\text{R}} = 40$ on the right branch.\n- In the left subtree: $N_{\\text{LL}} = 45$ and $N_{\\text{LR}} = 15$.\n- In the right subtree: $N_{\\text{RL}} = 10$ and $N_{\\text{RR}} = 30$.\n\nFor the instance $\\mathbf{x} = (x_{1}, x_{2}) = (2, 6)$, perform the following:\n1. Using the Shapley value definition specialized to decision trees, compute the TreeSHAP values $\\phi_{x_{1}}$ and $\\phi_{x_{2}}$ for features $x_{1}$ and $x_{2}$, respectively. When a feature is “missing,” compute expected predictions by distributing mass at splits on that missing feature proportionally to the training cover counts provided.\n2. Verify the additivity property by showing that $\\phi_{0} + \\phi_{x_{1}} + \\phi_{x_{2}} = f(\\mathbf{x})$, where $\\phi_{0}$ is the model’s expected output over the background distribution (the bias term), and $f(\\mathbf{x})$ is the tree’s prediction at $\\mathbf{x}$.\n\nReport $\\phi_{x_{2}}$ as your final answer. Express the final answer as a simplified rational number. No rounding is required, and no units are to be included.", "solution": "The user wants to compute the TreeSHAP values for a given instance and a specified decision tree. The solution requires applying the definition of Shapley values in the context of tree-based models, where the expectation of the model's output conditional on a subset of features is computed by averaging over the predictions for background data samples.\n\nFirst, we validate the problem statement.\n\n### Step 1: Extract Givens\n- **Features**: $x_{1}$, $x_{2}$.\n- **Decision Tree Structure**:\n  - Root: Splits on $x_{1}$ at $t_{1} = 3$.\n  - Left Child (if $x_{1}  3$): Splits on $x_{2}$ at $t_{2} = 5$.\n    - Leaf LL (if $x_{2}  5$): Output $v_{\\text{LL}} = 2$.\n    - Leaf LR (if $x_{2} \\ge 5$): Output $v_{\\text{LR}} = 6$.\n  - Right Child (if $x_{1} \\ge 3$): Splits on $x_{2}$ at $t_{3} = 4$.\n    - Leaf RL (if $x_{2}  4$): Output $v_{\\text{RL}} = 9$.\n    - Leaf RR (if $x_{2} \\ge 4$): Output $v_{\\text{RR}} = -1$.\n- **Cover Counts**:\n  - Root: $N = 100$.\n  - Root Branches: $N_{\\text{L}} = 60$ ($x_{1}  3$), $N_{\\text{R}} = 40$ ($x_{1} \\ge 3$).\n  - Left Subtree Branches: $N_{\\text{LL}} = 45$, $N_{\\text{LR}} = 15$.\n  - Right Subtree Branches: $N_{\\text{RL}} = 10$, $N_{\\text{RR}} = 30$.\n- **Instance to Explain**: $\\mathbf{x} = (x_{1}, x_{2}) = (2, 6)$.\n- **Tasks**:\n  1. Compute TreeSHAP values $\\phi_{x_{1}}$ and $\\phi_{x_{2}}$.\n  2. Verify the additivity property: $\\phi_{0} + \\phi_{x_{1}} + \\phi_{x_{2}} = f(\\mathbf{x})$.\n- **Required Final Answer**: $\\phi_{x_{2}}$ as a simplified rational number.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the well-established theory of Shapley values and their application to machine learning interpretability (TreeSHAP). The provided data is complete and internally consistent: the cover counts at each node sum correctly ($N_L+N_R = 60+40=100=N$; $N_{LL}+N_{LR} = 45+15=60=N_L$; $N_{RL}+N_{RR} = 10+30=40=N_R$). The problem is well-posed, objective, and requires the application of a standard, albeit non-trivial, algorithm. There are no ambiguities or contradictions.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the solution.\n\nThe Shapley value for a feature $i$ is its average marginal contribution to the prediction across all possible orderings of features. For a set of features $F = \\{x_1, x_2\\}$, the Shapley value for feature $i$ is given by:\n$$ \\phi_i = \\sum_{S \\subseteq F \\setminus \\{i\\}} \\frac{|S|! (|F| - |S| - 1)!}{|F|!} \\left( E[f(X) | X_S \\cup \\{x_i\\}] - E[f(X) | X_S] \\right) $$\nwhere $E[f(X) | X_S]$ is the expected prediction conditional on the values of features in subset $S$. For TreeSHAP, this expectation is taken over the background dataset.\n\nFor two features, $F=\\{x_1, x_2\\}$, the possible orderings are $(x_1, x_2)$ and $(x_2, x_1)$, each with a weight of $\\frac{1}{2}$. The formulas for $\\phi_{x_1}$ and $\\phi_{x_2}$ simplify to:\n$$ \\phi_{x_1} = \\frac{1}{2} \\left( \\left(E[f(X)|x_1] - E[f(X)]\\right) + \\left(E[f(X)|x_1, x_2] - E[f(X)|x_2]\\right) \\right) $$\n$$ \\phi_{x_2} = \\frac{1}{2} \\left( \\left(E[f(X)|x_2] - E[f(X)]\\right) + \\left(E[f(X)|x_1, x_2] - E[f(X)|x_1]\\right) \\right) $$\nHere, the conditional expectations are evaluated for the specific instance $\\mathbf{x} = (2, 6)$.\n\n**1. Compute the model prediction $f(\\mathbf{x})$ and the base value $\\phi_0$.**\n\nThe instance to explain is $\\mathbf{x} = (x_1, x_2) = (2, 6)$.\n- At the root, $x_1 = 2  3$, so we traverse to the left child.\n- At the left child, $x_2 = 6 \\ge 5$, so we traverse to the right leaf.\nThis path leads to leaf LR. The model's prediction is the value of this leaf.\n$$ f(\\mathbf{x}) = v_{\\text{LR}} = 6 $$\nThis is equivalent to $E[f(X) | x_1=2, x_2=6]$.\n\nThe base value $\\phi_0$ is the unconditional expected output of the model over the background data distribution, which is the weighted average of all leaf values.\n$$ \\phi_0 = E[f(X)] = \\frac{N_{\\text{LL}}}{N} v_{\\text{LL}} + \\frac{N_{\\text{LR}}}{N} v_{\\text{LR}} + \\frac{N_{\\text{RL}}}{N} v_{\\text{RL}} + \\frac{N_{\\text{RR}}}{N} v_{\\text{RR}} $$\n$$ \\phi_0 = \\left(\\frac{45}{100}\\right) (2) + \\left(\\frac{15}{100}\\right) (6) + \\left(\\frac{10}{100}\\right) (9) + \\left(\\frac{30}{100}\\right) (-1) $$\n$$ \\phi_0 = \\frac{90}{100} + \\frac{90}{100} + \\frac{90}{100} - \\frac{30}{100} = \\frac{240}{100} = 2.4 $$\n\n**2. Compute conditional expectations.**\n\n**a) $E[f(X) | x_1=2]$**\nWe know $x_1=2$. At the root, since $2  3$, we are forced down the left branch. The feature $x_2$ is considered \"missing,\" so its effect is averaged out according to the data distribution within that branch. The expected value is the weighted average of the leaf values in the left subtree.\n$$ E[f(X) | x_1=2] = \\frac{N_{\\text{LL}}}{N_{\\text{L}}} v_{\\text{LL}} + \\frac{N_{\\text{LR}}}{N_{\\text{L}}} v_{\\text{LR}} = \\left(\\frac{45}{60}\\right) (2) + \\left(\\frac{15}{60}\\right) (6) $$\n$$ E[f(X) | x_1=2] = \\left(\\frac{3}{4}\\right) (2) + \\left(\\frac{1}{4}\\right) (6) = \\frac{6}{4} + \\frac{6}{4} = \\frac{12}{4} = 3 $$\n\n**b) $E[f(X) | x_2=6]$**\nWe know $x_2=6$. At the root, the split is on $x_1$, which is \"missing.\" We consider both branches, weighted by their cover proportions.\n- Path 1 (Left branch, prob $\\frac{N_{\\text{L}}}{N} = \\frac{60}{100}$): Split on $x_2$ at $t_2=5$. Since $x_2=6 \\ge 5$, this path leads to leaf LR with value $v_{\\text{LR}} = 6$.\n- Path 2 (Right branch, prob $\\frac{N_{\\text{R}}}{N} = \\frac{40}{100}$): Split on $x_2$ at $t_3=4$. Since $x_2=6 \\ge 4$, this path leads to leaf RR with value $v_{\\text{RR}} = -1$.\nThe expected value is the weighted sum of these outcomes.\n$$ E[f(X) | x_2=6] = \\left(\\frac{60}{100}\\right) (6) + \\left(\\frac{40}{100}\\right) (-1) $$\n$$ E[f(X) | x_2=6] = \\frac{360}{100} - \\frac{40}{100} = \\frac{320}{100} = 3.2 $$\n\n**3. Compute the SHAP values $\\phi_{x_1}$ and $\\phi_{x_2}$.**\n\nUsing the simplified formulas for two features:\n\n**a) For feature $x_1$**:\n$$ \\phi_{x_1} = \\frac{1}{2} \\left[ \\left(E[f(X)|x_1=2] - E[f(X)]\\right) + \\left(E[f(X)|x_1=2, x_2=6] - E[f(X)|x_2=6]\\right) \\right] $$\n$$ \\phi_{x_1} = \\frac{1}{2} \\left[ (3 - 2.4) + (6 - 3.2) \\right] = \\frac{1}{2} [0.6 + 2.8] = \\frac{1}{2} [3.4] = 1.7 $$\n\n**b) For feature $x_2$**:\n$$ \\phi_{x_2} = \\frac{1}{2} \\left[ \\left(E[f(X)|x_2=6] - E[f(X)]\\right) + \\left(E[f(X)|x_1=2, x_2=6] - E[f(X)|x_1=2]\\right) \\right] $$\n$$ \\phi_{x_2} = \\frac{1}{2} \\left[ (3.2 - 2.4) + (6 - 3) \\right] = \\frac{1}{2} [0.8 + 3] = \\frac{1}{2} [3.8] = 1.9 $$\n\n**4. Verify the additivity property.**\n\nThe additivity property of SHAP values states that the sum of the base value and the SHAP values for all features equals the model's prediction for the instance.\n$$ \\phi_0 + \\phi_{x_1} + \\phi_{x_2} = f(\\mathbf{x}) $$\nSubstituting the computed values:\n$$ 2.4 + 1.7 + 1.9 = 6.0 $$\nSince $f(\\mathbf{x})=6$, the property holds: $6.0 = 6$. This confirms the correctness of our calculations.\n\nThe problem asks for the value of $\\phi_{x_2}$ as a simplified rational number.\n$$ \\phi_{x_2} = 1.9 = \\frac{19}{10} $$\nThis fraction is already in simplest form.", "answer": "$$\\boxed{\\frac{19}{10}}$$", "id": "4575307"}, {"introduction": "A key, and often overlooked, aspect of SHAP is that explanations are relative to a chosen background or reference distribution. This final practice problem [@problem_id:4575283] explores the profound implications of this fact in a medical analytics context, where a patient's risk score might be interpreted differently depending on the hospital population used as a baseline. By deriving and implementing the change in SHAP values when the background distribution shifts, you will gain critical insight into the contextual nature of model explanations.", "problem": "You are given a binary classification context for patient risk stratification using a logistic model and the goal is to quantify how patient-level SHapley Additive exPlanations (SHAP) values change when the background (hospital) distribution for laboratory features changes. A patient labeled $B$ with feature vector $x^{B} \\in \\mathbb{R}^{d}$ is evaluated by a logistic model with parameters $w \\in \\mathbb{R}^{d}$ and $b \\in \\mathbb{R}$. The model outputs the probability $p(x) = \\sigma(w^{\\top} x + b)$ where $\\sigma(t) = \\frac{1}{1 + e^{-t}}$. For SHapley Additive exPlanations (SHAP), consider the explanations computed in the log-odds space defined by $g(x) = w^{\\top} x + b$ (that is, the link function is the logit). The background distribution of features at each hospital is modeled as independent Gaussian random variables with means $\\mu \\in \\mathbb{R}^{d}$ and finite variances. Assume feature independence, and use the expected value over the background distribution as the SHAP baseline.\n\nTask:\n1) Starting from the cooperative game theory definition of Shapley values and the axioms of linearity, symmetry, and dummy, derive from first principles the closed-form SHAP values for the linear log-odds model $g(x) = w^{\\top} x + b$ under the independence assumption. Explicitly show that the SHAP value for feature $j$ is $\\phi_{j}(x;\\mu) = w_{j}\\,(x_{j} - \\mu_{j})$, and the base value is $\\phi_{0}(\\mu) = b + \\sum_{j=1}^{d} w_{j}\\,\\mu_{j}$. Do not invoke any shortcut formulas not derived from these principles.\n2) Using the result of part $1$, derive the change in SHAP values for a fixed patient $x^{B}$ when the background distribution changes from hospital $A$ with mean $\\mu^{A}$ to hospital $B$ with mean $\\mu^{B}$. Prove that for each feature $j$, the change is $\\Delta \\phi_{j}(x^{B}; \\mu^{A} \\rightarrow \\mu^{B}) = \\phi_{j}(x^{B}; \\mu^{B}) - \\phi_{j}(x^{B}; \\mu^{A}) = w_{j}\\,(\\mu^{A}_{j} - \\mu^{B}_{j})$.\n3) Implement a complete, runnable program that computes the change in SHAP values $\\Delta \\phi(x^{B}; \\mu^{A} \\rightarrow \\mu^{B}) \\in \\mathbb{R}^{d}$ for the following test suite. Your implementation must not rely on any external files or inputs. It must use the definitions in parts $1$ and $2$ directly.\n\nModel and patient:\n- Dimension $d = 3$.\n- Weights $w = (1.0,\\,-0.5,\\,0.0)$.\n- Intercept $b = -0.3$.\n- Patient $B$ features $x^{B} = (1.2,\\,0.7,\\,-0.3)$.\n\nHospitals and scenarios (feature means are the only quantities needed for SHAP in the log-odds space; variances are finite but unspecified):\n- Scenario $1$ (general shift): $\\mu^{A} = (0.0,\\,0.0,\\,0.0)$ and $\\mu^{B} = (0.8,\\,-0.4,\\,0.3)$.\n- Scenario $2$ (no shift): $\\mu^{A} = (0.8,\\,-0.4,\\,0.3)$ and $\\mu^{B} = (0.8,\\,-0.4,\\,0.3)$.\n- Scenario $3$ (shift only in an irrelevant feature): $\\mu^{A} = (0.8,\\,-0.4,\\,0.3)$ and $\\mu^{B} = (0.8,\\,-0.4,\\,1.3)$.\n- Scenario $4$ (single-feature shift): $\\mu^{A} = (0.8,\\,-0.4,\\,0.3)$ and $\\mu^{B} = (1.8,\\,-0.4,\\,0.3)$.\n\nRequired computations and outputs:\n- For each scenario, compute the vector $\\Delta \\phi(x^{B}; \\mu^{A} \\rightarrow \\mu^{B}) = \\big(w_{1}(\\mu^{A}_{1} - \\mu^{B}_{1}),\\, w_{2}(\\mu^{A}_{2} - \\mu^{B}_{2}),\\, w_{3}(\\mu^{A}_{3} - \\mu^{B}_{3})\\big)$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each scenario’s result must be printed as a Python-style list of floating-point numbers in the order of features $1$ through $3$. For example, the output format must be exactly of the form: $[\\,[\\Delta\\phi^{(1)}_{1},\\Delta\\phi^{(1)}_{2},\\Delta\\phi^{(1)}_{3}],\\,[\\Delta\\phi^{(2)}_{1},\\ldots],\\,[\\Delta\\phi^{(3)}_{1},\\ldots],\\,[\\Delta\\phi^{(4)}_{1},\\ldots]\\,]$ on a single line, without any additional text.\n\nNotes:\n- Light Interpretable Model-agnostic Explanations (LIME) is not required for this task; it is mentioned only for contextual relevance in interpretable machine learning in bioinformatics and medical data analytics.\n- There are no physical units, and no angles are involved.\n- All returned numeric values must be standard floating-point numbers (not fractions).", "solution": "The problem is valid as it is scientifically grounded in cooperative game theory and its application to machine learning interpretability (SHAP), is well-posed with all necessary information provided, and is formulated objectively. We proceed with the solution.\n\n### Part 1: Derivation of SHAP Values for a Linear Model\n\nThe goal is to derive the SHAP value for feature $j$ of a linear model $g(x) = w^{\\top}x + b = \\sum_{i=1}^d w_i x_i + b$. The derivation will be based on the fundamental axioms of Shapley values: Linearity, Dummy, Symmetry, and Efficiency.\n\nLet $N = \\{1, 2, \\dots, d\\}$ be the set of all features (players). A coalition of features is a subset $S \\subseteq N$. In the context of SHAP with a background distribution, the value function $v(S)$ for a coalition $S$ represents the expected output of the model, conditioned on observing the feature values $\\{x_i\\}_{i \\in S}$ for a specific instance, while the remaining features $\\{X_j\\}_{j \\notin S}$ are random variables drawn from their background distribution.\n\nThe problem states that features are independent and their background distributions are Gaussian with means $\\mu_j$ for each feature $j$. Under the independence assumption, the conditional expectation simplifies to an expectation over the unobserved features:\n$$v(S) = E[g(X) | X_S = x_S] = E\\left[\\sum_{i=1}^d w_i X_i + b \\right]$$\nwhere $X_i = x_i$ if $i \\in S$ and $X_i$ is a random variable with mean $\\mu_i$ if $i \\notin S$.\n\nUsing the linearity of expectation:\n$$v(S) = E\\left[\\sum_{i \\in S} w_i x_i + \\sum_{j \\notin S} w_j X_j + b\\right] = \\sum_{i \\in S} w_i x_i + \\sum_{j \\notin S} w_j E[X_j] + b$$\nSince $E[X_j] = \\mu_j$, the value function is:\n$$v(S) = \\sum_{i \\in S} w_i x_i + \\sum_{j \\notin S} w_j \\mu_j + b$$\n\nThe Shapley value formulation is complex. A more direct approach is to use the axioms, particularly the linearity (or additivity) axiom. The model $g(x)$ is additive:\n$$g(x) = b + \\sum_{k=1}^d w_k x_k$$\nWe can decompose the game $g$ into a sum of $d+1$ simpler games: one constant game $g_0(x) = b$ and $d$ single-feature games $g_k(x) = w_k x_k$ for $k \\in \\{1, \\dots, d\\}$.\n$$g(x) = g_0(x) + \\sum_{k=1}^d g_k(x)$$\nThe linearity axiom states that the SHAP value of a sum of games is the sum of the SHAP values for each game. For any feature $j$:\n$$\\phi_j(g) = \\phi_j(g_0) + \\sum_{k=1}^d \\phi_j(g_k)$$\n\nLet's analyze the SHAP values for each component game.\n\n1.  **Constant Game $g_0(x) = b$**:\n    The value function for this game, $v_0(S)$, is $E[b] = b$ for any coalition $S$. The marginal contribution of any feature $j$ to any coalition $S \\not\\ni j$ is $v_0(S \\cup \\{j\\}) - v_0(S) = b - b = 0$. This means every feature is a \"dummy\" player in this game. By the dummy axiom, the SHAP value for any feature is zero:\n    $$\\phi_j(g_0) = 0 \\quad \\text{for all } j \\in \\{1, \\dots, d\\}$$\n\n2.  **Single-Feature Game $g_k(x) = w_k x_k$ for a fixed $k \\in \\{1, \\dots, d\\}$**:\n    Let the value function for this game be $v_k(S)$.\n    -   If feature $k$ is in the coalition ($k \\in S$), $v_k(S) = E[w_k X_k | X_S=x_S] = w_k x_k$.\n    -   If feature $k$ is not in the coalition ($k \\notin S$), $v_k(S) = E[w_k X_k | X_S=x_S] = w_k E[X_k] = w_k \\mu_k$.\n\n    Now we compute the SHAP value $\\phi_j(g_k)$ for any feature $j$.\n    -   **Case $j \\neq k$**: Consider the marginal contribution of feature $j$ to a coalition $S$ where $j \\notin S$.\n        -   If $k \\in S$, then $k \\in S \\cup \\{j\\}$. Thus, $v_k(S \\cup \\{j\\}) = w_k x_k$ and $v_k(S) = w_k x_k$. The difference is $0$.\n        -   If $k \\notin S$, then $k \\notin S \\cup \\{j\\}$. Thus, $v_k(S \\cup \\{j\\}) = w_k \\mu_k$ and $v_k(S) = w_k \\mu_k$. The difference is $0$.\n        Since the marginal contribution of feature $j$ is always zero, $j$ is a dummy player in game $g_k$. By the dummy axiom:\n        $$\\phi_j(g_k) = 0 \\quad \\text{for } j \\neq k$$\n    -   **Case $j = k$**: We need to find $\\phi_k(g_k)$. We know that for game $g_k$, all players $j \\neq k$ are dummy players. The sum of SHAP values for game $g_k$ is thus just $\\phi_k(g_k)$. By the efficiency axiom, the sum of SHAP values equals the total payout difference between the grand coalition ($N$) and the empty coalition ($\\emptyset$).\n        $$\\sum_{j=1}^d \\phi_j(g_k) = \\phi_k(g_k) = v_k(N) - v_k(\\emptyset)$$\n        -   $v_k(N)$: For the grand coalition $N$, feature $k$ is included, so $v_k(N) = w_k x_k$.\n        -   $v_k(\\emptyset)$: For the empty coalition $\\emptyset$, feature $k$ is not included, so $v_k(\\emptyset) = w_k \\mu_k$.\n        Therefore, the SHAP value for player $k$ in game $g_k$ is:\n        $$\\phi_k(g_k) = w_k x_k - w_k \\mu_k = w_k(x_k - \\mu_k)$$\n\n3.  **Combining the Results**:\n    Using the linearity axiom, the SHAP value for feature $j$ in the original game $g$ is:\n    $$\\phi_j(g; x, \\mu) = \\phi_j(g_0) + \\sum_{k=1}^d \\phi_j(g_k)$$\n    The only non-zero term in the sum is when $k=j$.\n    $$\\phi_j(g; x, \\mu) = 0 + \\phi_j(g_j) = w_j(x_j - \\mu_j)$$\n    This completes the derivation for the feature SHAP values.\n\n**Derivation of the Base Value $\\phi_0$**:\nThe base value, $\\phi_0$, is the expected prediction without any feature information, which corresponds to the value of the empty coalition, $v(\\emptyset)$.\n$$\\phi_0(\\mu) = v(\\emptyset) = E[g(X)]$$\nwhere the expectation is over the entire background distribution.\n$$\\phi_0(\\mu) = E\\left[\\sum_{j=1}^d w_j X_j + b\\right] = \\sum_{j=1}^d w_j E[X_j] + b$$\nSubstituting $E[X_j] = \\mu_j$:\n$$\\phi_0(\\mu) = \\sum_{j=1}^d w_j \\mu_j + b$$\nThis completes the derivation for the base value.\n\n### Part 2: Derivation of the Change in SHAP Values\n\nWe are asked to find the change in SHAP values for a fixed patient $x^B$ when the background distribution changes from hospital $A$ (mean $\\mu^A$) to hospital $B$ (mean $\\mu^B$).\n\nThe SHAP value for feature $j$ at hospital $A$ is given by the formula from Part 1, using $\\mu^A$ as the background mean vector:\n$$\\phi_j(x^{B}; \\mu^{A}) = w_j (x^{B}_j - \\mu^{A}_j)$$\n\nSimilarly, the SHAP value for feature $j$ at hospital $B$ uses $\\mu^B$:\n$$\\phi_j(x^{B}; \\mu^{B}) = w_j (x^{B}_j - \\mu^{B}_j)$$\n\nThe change, $\\Delta \\phi_j$, is the difference between these two values:\n$$\\Delta \\phi_{j}(x^{B}; \\mu^{A} \\rightarrow \\mu^{B}) = \\phi_{j}(x^{B}; \\mu^{B}) - \\phi_{j}(x^{B}; \\mu^{A})$$\nSubstituting the expressions:\n$$\\Delta \\phi_j = w_j (x^{B}_j - \\mu^{B}_j) - w_j (x^{B}_j - \\mu^{A}_j)$$\nDistributing the weight $w_j$:\n$$\\Delta \\phi_j = (w_j x^{B}_j - w_j \\mu^{B}_j) - (w_j x^{B}_j - w_j \\mu^{A}_j)$$\n$$\\Delta \\phi_j = w_j x^{B}_j - w_j \\mu^{B}_j - w_j x^{B}_j + w_j \\mu^{A}_j$$\nThe patient-specific terms $w_j x^{B}_j$ cancel out:\n$$\\Delta \\phi_j = w_j \\mu^{A}_j - w_j \\mu^{B}_j$$\nFactoring out $w_j$:\n$$\\Delta \\phi_j(x^{B}; \\mu^{A} \\rightarrow \\mu^{B}) = w_j (\\mu^{A}_j - \\mu^{B}_j)$$\nThis proves that the change in the SHAP value for feature $j$ depends only on the change in the background mean for that feature and its corresponding model weight.\n\n### Part 3: Implementation and Calculation\n\nWe implement a program to compute the vector of SHAP value changes, $\\Delta \\phi$, for each scenario. The formula for the vector is:\n$$\\Delta \\phi(x^{B}; \\mu^{A} \\rightarrow \\mu^{B}) = \\left(w_1(\\mu^{A}_1 - \\mu^{B}_1), w_2(\\mu^{A}_2 - \\mu^{B}_2), \\dots, w_d(\\mu^{A}_d - \\mu^{B}_d)\\right)$$\nThis can be computed element-wise as $w \\odot (\\mu^A - \\mu^B)$, where $\\odot$ is the element-wise product.\n\nGiven data:\n- $d = 3$\n- $w = (1.0, -0.5, 0.0)$\n- Patient features $x^B$ and intercept $b$ are not needed for this calculation, as shown in Part 2.\n\n**Scenario 1:** $\\mu^{A} = (0.0, 0.0, 0.0)$, $\\mu^{B} = (0.8, -0.4, 0.3)$\n- $\\mu^{A} - \\mu^{B} = (-0.8, 0.4, -0.3)$\n- $\\Delta\\phi = (1.0 \\times -0.8, -0.5 \\times 0.4, 0.0 \\times -0.3) = (-0.8, -0.2, 0.0)$\n\n**Scenario 2:** $\\mu^{A} = (0.8, -0.4, 0.3)$, $\\mu^{B} = (0.8, -0.4, 0.3)$\n- $\\mu^{A} - \\mu^{B} = (0.0, 0.0, 0.0)$\n- $\\Delta\\phi = (1.0 \\times 0.0, -0.5 \\times 0.0, 0.0 \\times 0.0) = (0.0, 0.0, 0.0)$\n\n**Scenario 3:** $\\mu^{A} = (0.8, -0.4, 0.3)$, $\\mu^{B} = (0.8, -0.4, 1.3)$\n- $\\mu^{A} - \\mu^{B} = (0.0, 0.0, -1.0)$\n- $\\Delta\\phi = (1.0 \\times 0.0, -0.5 \\times 0.0, 0.0 \\times -1.0) = (0.0, 0.0, 0.0)$\n(The change is zero because the shift occurs only in feature 3, which has a weight $w_3=0.0$.)\n\n**Scenario 4:** $\\mu^{A} = (0.8, -0.4, 0.3)$, $\\mu^{B} = (1.8, -0.4, 0.3)$\n- $\\mu^{A} - \\mu^{B} = (-1.0, 0.0, 0.0)$\n- $\\Delta\\phi = (1.0 \\times -1.0, -0.5 \\times 0.0, 0.0 \\times 0.0) = (-1.0, 0.0, 0.0)$\n\nThe following program will perform these calculations and format the output as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the change in SHAP values for a logistic model when the\n    background distribution of features changes.\n    \"\"\"\n\n    # Model parameters\n    w = np.array([1.0, -0.5, 0.0])\n\n    # Test cases: Sets of background means for hospital A and hospital B\n    # Each case is a tuple (mu_A, mu_B)\n    test_cases = [\n        # Scenario 1 (general shift)\n        (np.array([0.0, 0.0, 0.0]), np.array([0.8, -0.4, 0.3])),\n        # Scenario 2 (no shift)\n        (np.array([0.8, -0.4, 0.3]), np.array([0.8, -0.4, 0.3])),\n        # Scenario 3 (shift only in an irrelevant feature)\n        (np.array([0.8, -0.4, 0.3]), np.array([0.8, -0.4, 1.3])),\n        # Scenario 4 (single-feature shift)\n        (np.array([0.8, -0.4, 0.3]), np.array([1.8, -0.4, 0.3])),\n    ]\n\n    results = []\n    for mu_A, mu_B in test_cases:\n        # The change in SHAP value for feature j is w_j * (mu_A_j - mu_B_j).\n        # This can be computed for all features at once using an element-wise\n        # vector product.\n        delta_phi_vector = w * (mu_A - mu_B)\n\n        # Convert numpy array to a list of standard Python floats\n        results.append(delta_phi_vector.tolist())\n\n    # Format the final output string to be a list of lists.\n    # The problem asks for a specific format without spaces.\n    def format_list(lst):\n        # Creates a string representation of a list like '[v1,v2,v3]'\n        return f\"[{','.join(f'{v:.1f}' for v in lst)}]\"\n    \n    result_strings = [format_list(res) for res in results]\n    final_output_string = f\"[{','.join(result_strings)}]\"\n    \n    # Print the single-line result as required\n    print(final_output_string)\n\nsolve()\n```", "id": "4575283"}]}