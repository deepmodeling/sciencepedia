{"hands_on_practices": [{"introduction": "Modern protein structure prediction models conceptualize proteins as graphs, where residues are nodes and their interactions are edges. To learn from this representation, these models employ Graph Neural Networks (GNNs) to iteratively refine node features by passing \"messages\" between neighbors. This hands-on exercise demystifies the core GNN mechanism by guiding you through a single, concrete message-passing step, providing a bottom-up understanding of how local structural and sequence information is processed and propagated across the residue network [@problem_id:4554878].", "problem": "Consider a protein contact graph with four residues indexed $i \\in \\{1,2,3,4\\}$. Residues are nodes, and undirected edges represent spatial contacts. Each node $i$ carries a two-dimensional feature vector $h_{i} \\in \\mathbb{R}^{2}$ describing learned biochemical attributes, and each edge $(i,j)$ carries a two-dimensional feature $e_{ij} \\in \\mathbb{R}^{2}$ encoding geometry and sequence separation. This graph-based representation is consistent with the physical principle that residue-residue interactions propagate along the contact network and that any learned update must be invariant to node indexing permutations.\n\nThe initial node features are\n$$\nh_{1}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix},\\quad\nh_{2}=\\begin{pmatrix}0 \\\\ 2\\end{pmatrix},\\quad\nh_{3}=\\begin{pmatrix}1 \\\\ -1\\end{pmatrix},\\quad\nh_{4}=\\begin{pmatrix}2 \\\\ 1\\end{pmatrix}.\n$$\nEdges exist between the pairs $(1,2)$, $(2,3)$, $(3,4)$, and $(1,4)$. For each edge $(i,j)$, define $e_{ij}=\\begin{pmatrix}1/d_{ij} \\\\ |i-j|\\end{pmatrix}$, where $d_{ij}$ is the inter-residue distance in Angstroms (a physically meaningful scale for residue contacts). The distances are\n$$\nd_{12}=4,\\quad d_{23}=5,\\quad d_{34}=4,\\quad d_{14}=8,\n$$\nso that\n$$\ne_{12}=\\begin{pmatrix}\\frac{1}{4} \\\\ 1\\end{pmatrix},\\quad\ne_{23}=\\begin{pmatrix}\\frac{1}{5} \\\\ 1\\end{pmatrix},\\quad\ne_{34}=\\begin{pmatrix}\\frac{1}{4} \\\\ 1\\end{pmatrix},\\quad\ne_{14}=\\begin{pmatrix}\\frac{1}{8} \\\\ 3\\end{pmatrix}.\n$$\n\nPerform one iteration of message passing using a Graph Neural Network (GNN), grounded in the principle that a residue $j$ receives messages from its neighbors $i \\in \\mathcal{N}(j)$, with each directed message $m_{i\\to j}$ modulated by a gate derived from the sender’s features and the edge features. Use the following components:\n- The Rectified Linear Unit (ReLU) nonlinearity $\\phi(x)=\\max(0,x)$ applied elementwise.\n- A gating score $s_{ij}=a^{\\top}h_{i}+c^{\\top}e_{ij}$ with $a=\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$ and $c=\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$, and a scalar gate $g_{ij}=\\phi(s_{ij})$.\n- A message function $m_{i\\to j}=g_{ij}\\cdot \\phi\\!\\big(W_{m}h_{i}+U_{m}e_{ij}+b_{m}\\big)$, where\n$$\nW_{m}=\\begin{pmatrix}1 & -1 \\\\ 0 & 2\\end{pmatrix},\\quad\nU_{m}=\\begin{pmatrix}2 & 0 \\\\ 1 & 1\\end{pmatrix},\\quad\nb_{m}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}.\n$$\n- A neighbor aggregation by summation: $s_{j}=\\sum_{i\\in\\mathcal{N}(j)} m_{i\\to j}$.\n- A residual update to the node with ReLU: $h_{j}^{\\text{new}}=\\phi\\!\\big(W_{s}h_{j}+V s_{j}+b_{s}\\big)$, where\n$$\nW_{s}=\\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix},\\quad\nV=\\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix},\\quad\nb_{s}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}.\n$$\n\nCompute the updated feature $h_{3}^{\\text{new}}$ for residue $3$ after this single message-passing iteration, and then compute its squared Euclidean norm $\\|h_{3}^{\\text{new}}\\|_{2}^{2}$. Express your final scalar answer as an exact rational number. No rounding is required and no units are to be reported.", "solution": "The user has requested the computation of the updated feature vector $h_{3}^{\\text{new}}$ a single message-passing iteration and its squared Euclidean norm, $\\|h_{3}^{\\text{new}}\\|_{2}^{2}$. The problem is well-posed, scientifically grounded in the domain of graph neural networks for protein modeling, and provides all necessary data and definitions for a unique solution.\n\nThe process involves computing messages from the neighbors of residue $3$, aggregating them, and then using the aggregated message to update the feature vector of residue $3$.\n\nThe set of neighbors for residue $j=3$ is $\\mathcal{N}(3)=\\{2, 4\\}$, as determined by the provided edges $(2,3)$ and $(3,4)$.\n\nThe update rule for a node feature vector $h_j$ is given by:\n$$\nh_{j}^{\\text{new}}=\\phi\\!\\big(W_{s}h_{j}+V s_{j}+b_{s}\\big)\n$$\nwhere $s_j$ is the aggregated message from the neighbors of $j$. The parameters for the update step are given as the identity matrix for $W_s$ and $V$, and the zero vector for $b_s$:\n$$\nW_{s}=\\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix}, \\quad V=\\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix}, \\quad b_{s}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}\n$$\nThe update rule simplifies to:\n$$\nh_{j}^{\\text{new}}=\\phi(h_{j} + s_{j})\n$$\nThe aggregated message $s_j$ is the sum of incoming messages from its neighbors:\n$$\ns_{j}=\\sum_{i\\in\\mathcal{N}(j)} m_{i\\to j}\n$$\nFor residue $j=3$, this is $s_{3}=m_{2\\to 3}+m_{4\\to 3}$.\n\nThe message function $m_{i\\to j}$ is defined as:\n$$\nm_{i\\to j}=g_{ij}\\cdot \\phi\\!\\big(W_{m}h_{i}+U_{m}e_{ij}+b_{m}\\big)\n$$\nwhere the gate $g_{ij}$ is $g_{ij}=\\phi(s_{ij})$ and the score $s_{ij}$ is $s_{ij}=a^{\\top}h_{i}+c^{\\top}e_{ij}$.\n\nFirst, we compute the message from residue $2$ to residue $3$, denoted $m_{2\\to 3}$.\nThe required features are $h_{2}=\\begin{pmatrix}0 \\\\ 2\\end{pmatrix}$ and $e_{23}=\\begin{pmatrix}1/5 \\\\ 1\\end{pmatrix}$. The gating parameters are $a=\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$ and $c=\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$.\nThe gating score is:\n$$\ns_{23} = a^{\\top}h_{2}+c^{\\top}e_{23} = \\begin{pmatrix}1 & 1\\end{pmatrix}\\begin{pmatrix}0 \\\\ 2\\end{pmatrix} + \\begin{pmatrix}0 & 1\\end{pmatrix}\\begin{pmatrix}\\frac{1}{5} \\\\ 1\\end{pmatrix} = (1 \\cdot 0 + 1 \\cdot 2) + (0 \\cdot \\frac{1}{5} + 1 \\cdot 1) = 2 + 1 = 3\n$$\nThe gate is $g_{23}=\\phi(s_{23})=\\phi(3)=3$.\nThe argument of the ReLU in the message function is calculated using $W_{m}=\\begin{pmatrix}1 & -1 \\\\ 0 & 2\\end{pmatrix}$, $U_{m}=\\begin{pmatrix}2 & 0 \\\\ 1 & 1\\end{pmatrix}$, and $b_{m}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$:\n$$\nW_{m}h_{2} = \\begin{pmatrix}1 & -1 \\\\ 0 & 2\\end{pmatrix}\\begin{pmatrix}0 \\\\ 2\\end{pmatrix} = \\begin{pmatrix}-2 \\\\ 4\\end{pmatrix}\n$$\n$$\nU_{m}e_{23} = \\begin{pmatrix}2 & 0 \\\\ 1 & 1\\end{pmatrix}\\begin{pmatrix}\\frac{1}{5} \\\\ 1\\end{pmatrix} = \\begin{pmatrix}\\frac{2}{5} \\\\ \\frac{1}{5}+1\\end{pmatrix} = \\begin{pmatrix}\\frac{2}{5} \\\\ \\frac{6}{5}\\end{pmatrix}\n$$\nThe pre-activation vector is:\n$$\nW_{m}h_{2}+U_{m}e_{23}+b_{m} = \\begin{pmatrix}-2 \\\\ 4\\end{pmatrix} + \\begin{pmatrix}\\frac{2}{5} \\\\ \\frac{6}{5}\\end{pmatrix} + \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}-\\frac{10}{5}+\\frac{2}{5} \\\\ \\frac{20}{5}+\\frac{6}{5}\\end{pmatrix} = \\begin{pmatrix}-\\frac{8}{5} \\\\ \\frac{26}{5}\\end{pmatrix}\n$$\nApplying the ReLU and multiplying by the gate gives the message:\n$$\nm_{2\\to 3} = g_{23} \\cdot \\phi\\left(\\begin{pmatrix}-\\frac{8}{5} \\\\ \\frac{26}{5}\\end{pmatrix}\\right) = 3 \\cdot \\begin{pmatrix}0 \\\\ \\frac{26}{5}\\end{pmatrix} = \\begin{pmatrix}0 \\\\ \\frac{78}{5}\\end{pmatrix}\n$$\nNext, we compute the message from residue $4$ to residue $3$, denoted $m_{4\\to 3}$.\nThe required features are $h_{4}=\\begin{pmatrix}2 \\\\ 1\\end{pmatrix}$ and $e_{43}=e_{34}=\\begin{pmatrix}1/4 \\\\ 1\\end{pmatrix}$.\nThe gating score is:\n$$\ns_{43} = a^{\\top}h_{4}+c^{\\top}e_{43} = \\begin{pmatrix}1 & 1\\end{pmatrix}\\begin{pmatrix}2 \\\\ 1\\end{pmatrix} + \\begin{pmatrix}0 & 1\\end{pmatrix}\\begin{pmatrix}\\frac{1}{4} \\\\ 1\\end{pmatrix} = (1 \\cdot 2 + 1 \\cdot 1) + (0 \\cdot \\frac{1}{4} + 1 \\cdot 1) = 3 + 1 = 4\n$$\nThe gate is $g_{43}=\\phi(s_{43})=\\phi(4)=4$.\nThe pre-activation vector is:\n$$\nW_{m}h_{4} = \\begin{pmatrix}1 & -1 \\\\ 0 & 2\\end{pmatrix}\\begin{pmatrix}2 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 2\\end{pmatrix}\n$$\n$$\nU_{m}e_{43} = \\begin{pmatrix}2 & 0 \\\\ 1 & 1\\end{pmatrix}\\begin{pmatrix}\\frac{1}{4} \\\\ 1\\end{pmatrix} = \\begin{pmatrix}\\frac{2}{4} \\\\ \\frac{1}{4}+1\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{2} \\\\ \\frac{5}{4}\\end{pmatrix}\n$$\n$$\nW_{m}h_{4}+U_{m}e_{43}+b_{m} = \\begin{pmatrix}1 \\\\ 2\\end{pmatrix} + \\begin{pmatrix}\\frac{1}{2} \\\\ \\frac{5}{4}\\end{pmatrix} + \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}\\frac{3}{2} \\\\ \\frac{8}{4}+\\frac{5}{4}\\end{pmatrix} = \\begin{pmatrix}\\frac{3}{2} \\\\ \\frac{13}{4}\\end{pmatrix}\n$$\nApplying the ReLU and multiplying by the gate gives the message. Since both components are positive, the ReLU has no effect.\n$$\nm_{4\\to 3} = g_{43} \\cdot \\phi\\left(\\begin{pmatrix}\\frac{3}{2} \\\\ \\frac{13}{4}\\end{pmatrix}\\right) = 4 \\cdot \\begin{pmatrix}\\frac{3}{2} \\\\ \\frac{13}{4}\\end{pmatrix} = \\begin{pmatrix}6 \\\\ 13\\end{pmatrix}\n$$\nNow, we aggregate the messages to obtain $s_3$:\n$$\ns_{3} = m_{2\\to 3} + m_{4\\to 3} = \\begin{pmatrix}0 \\\\ \\frac{78}{5}\\end{pmatrix} + \\begin{pmatrix}6 \\\\ 13\\end{pmatrix} = \\begin{pmatrix}6 \\\\ \\frac{78}{5} + \\frac{65}{5}\\end{pmatrix} = \\begin{pmatrix}6 \\\\ \\frac{143}{5}\\end{pmatrix}\n$$\nWe use this aggregated message to update the feature vector for residue $3$, using its initial state $h_{3}=\\begin{pmatrix}1 \\\\ -1\\end{pmatrix}$.\n$$\nh_{3}^{\\text{new}} = \\phi(h_{3} + s_{3}) = \\phi\\left(\\begin{pmatrix}1 \\\\ -1\\end{pmatrix} + \\begin{pmatrix}6 \\\\ \\frac{143}{5}\\end{pmatrix}\\right) = \\phi\\left(\\begin{pmatrix}7 \\\\ -\\frac{5}{5} + \\frac{143}{5}\\end{pmatrix}\\right) = \\phi\\left(\\begin{pmatrix}7 \\\\ \\frac{138}{5}\\end{pmatrix}\\right)\n$$\nSince both components of the argument are positive, the ReLU nonlinearity has no effect.\n$$\nh_{3}^{\\text{new}} = \\begin{pmatrix}7 \\\\ \\frac{138}{5}\\end{pmatrix}\n$$\nFinally, we compute the squared Euclidean norm of $h_{3}^{\\text{new}}$:\n$$\n\\|h_{3}^{\\text{new}}\\|_{2}^{2} = (7)^2 + \\left(\\frac{138}{5}\\right)^2 = 49 + \\frac{138^2}{5^2} = 49 + \\frac{19044}{25}\n$$\nTo express this as a single rational number, we find a common denominator:\n$$\n\\|h_{3}^{\\text{new}}\\|_{2}^{2} = \\frac{49 \\times 25}{25} + \\frac{19044}{25} = \\frac{1225}{25} + \\frac{19044}{25} = \\frac{1225 + 19044}{25} = \\frac{20269}{25}\n$$\nThis fraction is irreducible as the numerator does not end in a $0$ or $5$ and is therefore not divisible by $5$.\nThe final answer is $\\frac{20269}{25}$.", "answer": "$$\n\\boxed{\\frac{20269}{25}}\n$$", "id": "4554878"}, {"introduction": "Once a model can process graph-based information, it must be trained toward a specific goal, defined by a loss function. This practice guides you in constructing a sophisticated, multi-component objective function that mirrors those used in state-of-the-art systems. You will learn to integrate geometrically-defined errors like the Frame Aligned Point Error (FAPE), probabilistic predictions of inter-residue distances (distograms), and penalties for physically unrealistic features into a single, dimensionally-consistent composite loss, revealing the engineering required to guide a network toward producing high-quality structures [@problem_id:3842267].", "problem": "You are tasked with constructing a composite training loss for deep learning-based protein structure prediction that blends four components: a distogram cross-entropy, a Frame Aligned Point Error (FAPE), a torsion angle loss, and a violation penalty. The goal is to derive, implement, and justify a principled weighting scheme that is dimensionally consistent and balances the contributions of heterogeneous terms. All angles must be handled in radians, all distances in Angstroms (written as $\\mathrm{\\AA}$), and all logarithms must be natural logarithms. You must produce a complete, runnable program that computes the composite loss for multiple test cases and outputs the results as a single line list.\n\nStart from the following foundational base and core definitions:\n\n- Probability axioms and the definition of cross-entropy: for a discrete distribution $p(k)$ and a one-hot target on bin $k^\\star$, the cross-entropy is $-\\log p(k^\\star)$.\n- Rigid-body frames in three dimensions defined by a rotation matrix $\\mathbf{R} \\in \\mathbb{R}^{3 \\times 3}$ and a translation vector $\\mathbf{t} \\in \\mathbb{R}^{3}$, and the Euclidean norm $\\|\\cdot\\|_2$.\n- The definition of periodic angular differences on the circle, using a $2\\pi$-periodic wrap to account for angle equivalence modulo $2\\pi$.\n- Steric clash violation penalties defined by a threshold distance and rectified penalty for distances below the threshold.\n\nYour program must implement the following four loss components for each test case:\n\n1. Distogram cross-entropy. A distogram is a discrete distribution over pairwise inter-residue distances. Given $K$ bin edges $\\{b_0, b_1, \\dots, b_K\\}$ with $b_0 < b_1 < \\dots < b_K$ and a predicted probability vector $\\mathbf{p} \\in \\mathbb{R}^K$ with $\\sum_{k=0}^{K-1} p_k = 1$, assign a ground-truth distance $d$ to bin index $k^\\star$ via $k^\\star = \\max\\{k \\in \\{0,\\dots,K-1\\}: b_k \\le d\\}$ with upper clamping for $d \\ge b_K$. The per-pair cross-entropy is $L_{\\mathrm{dist}} = -\\log p_{k^\\star}$ and the distogram loss is the mean of these over all pairs.\n\n2. Frame Aligned Point Error (FAPE). For a set of points indexed by $i$, each with predicted global coordinates $\\mathbf{x}_i^{(p)} \\in \\mathbb{R}^3$, true global coordinates $\\mathbf{x}_i^{(t)} \\in \\mathbb{R}^3$, a predicted frame $(\\mathbf{R}^{(p)}, \\mathbf{t}^{(p)})$, and a true frame $(\\mathbf{R}^{(t)}, \\mathbf{t}^{(t)})$, transform points into the respective local frames via $\\mathbf{y}_i^{(p)} = \\mathbf{R}^{(p)\\top}(\\mathbf{x}_i^{(p)} - \\mathbf{t}^{(p)})$ and $\\mathbf{y}_i^{(t)} = \\mathbf{R}^{(t)\\top}(\\mathbf{x}_i^{(t)} - \\mathbf{t}^{(t)})$. The FAPE per point is $e_i = \\min\\{\\tau, \\|\\mathbf{y}_i^{(p)} - \\mathbf{y}_i^{(t)}\\|_2\\}$ with clamp $\\tau > 0$ in $\\mathrm{\\AA}$. The FAPE loss is the mean $\\overline{e}$ over all points. Use rotation about the $z$-axis with angle $\\phi$, namely $\\mathbf{R}_z(\\phi) = \\begin{bmatrix} \\cos\\phi & -\\sin\\phi & 0 \\\\ \\sin\\phi & \\cos\\phi & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$.\n\n3. Torsion angle loss. For predicted torsion angles $\\theta_j^{(p)}$ and true torsion angles $\\theta_j^{(t)}$ (each in radians), compute the wrapped difference $\\Delta\\theta_j = \\mathrm{wrap}(\\theta_j^{(p)} - \\theta_j^{(t)})$ where $\\mathrm{wrap}(\\alpha) = ((\\alpha + \\pi) \\bmod 2\\pi) - \\pi$. The per-angle loss is $1 - \\cos(\\Delta\\theta_j)$, and the torsion loss is the mean over all angles.\n\n4. Violation penalty. For predicted nonbonded pairwise distances $d_{ij} \\in \\mathbb{R}_{\\ge 0}$ with a threshold $r_{\\mathrm{th}} > 0$ in $\\mathrm{\\AA}$, define the per-pair penalty $v_{ij} = \\max\\{0, r_{\\mathrm{th}} - d_{ij}\\}^2$. Sum over unique unordered pairs $(i,j)$ with $i<j$ and average, yielding $L_{\\mathrm{viol}}$.\n\nTo produce a dimensionally consistent composite loss, first nondimensionalize the terms as follows:\n- The distogram loss $L_{\\mathrm{dist}}$ is dimensionless.\n- The FAPE mean $\\overline{e}$ must be divided by a characteristic length scale $\\ell$ in $\\mathrm{\\AA}$ to yield $\\overline{e}/\\ell$.\n- The torsion loss $L_{\\mathrm{tors}}$ is dimensionless by construction.\n- The violation mean $L_{\\mathrm{viol}}$ has units of $\\mathrm{\\AA}^2$ and must be divided by $\\ell^2$ to yield $L_{\\mathrm{viol}}/\\ell^2$.\n\nLet $L_1 = L_{\\mathrm{dist}}$, $L_2 = \\overline{e}/\\ell$, $L_3 = L_{\\mathrm{tors}}$, and $L_4 = L_{\\mathrm{viol}}/\\ell^2$, all dimensionless. Construct weights $w_1, w_2, w_3, w_4$ that sum to $1$ and adaptively balance component magnitudes via\n$$\n\\tilde{w}_i = \\frac{1}{\\varepsilon + \\overline{L_i}}, \\quad w_i = \\frac{\\tilde{w}_i}{\\sum_{j=1}^{4} \\tilde{w}_j},\n$$\nwhere $\\overline{L_i}$ denotes the mean value of component $L_i$ over its internal elements for the current case (for $L_1$, this is its mean over pairs; for $L_2$, the mean $\\overline{e}/\\ell$; for $L_3$, the mean over angles; for $L_4$, the mean over pairs), and $\\varepsilon$ is a small positive constant ensuring numerical stability. The composite loss is then\n$$\nL_{\\mathrm{comp}} = \\sum_{i=1}^{4} w_i L_i.\n$$\n\nImplement a program that, for each test case, computes $L_{\\mathrm{dist}}$, $\\overline{e}$, $L_{\\mathrm{tors}}$, $L_{\\mathrm{viol}}$, forms $L_1,\\dots,L_4$, derives $w_1,\\dots,w_4$ as above, and outputs $L_{\\mathrm{comp}}$ as a float.\n\nScientific realism and units:\n- Distances must be in $\\mathrm{\\AA}$.\n- Angles must be in radians.\n- Use $\\ell$ in $\\mathrm{\\AA}$ and $\\tau$ in $\\mathrm{\\AA}$.\n- Clamp $d$ to the last bin if $d \\ge b_K$.\n- Exclude diagonal pairs in $L_{\\mathrm{viol}}$ by averaging only over $i<j$.\n\nTest suite:\nProvide three test cases covering typical conditions, boundary angle wrapping, and edge violations. For each case, specify:\n- Distogram bin edges $\\{b_0, \\dots, b_K\\}$ with $K=4$ and ground-truth distances for $P=3$ pairs, along with predicted probability vectors $\\mathbf{p}$ per pair.\n- Predicted and true frames using $z$-axis rotation angles $\\phi^{(p)}$, $\\phi^{(t)}$ and translations $\\mathbf{t}^{(p)}$, $\\mathbf{t}^{(t)}$, and two points ($M=2$) of predicted and true global coordinates.\n- Torsion angles for $J=3$ angles, predicted and true, in radians.\n- Predicted nonbonded distances for $N=3$ atoms (a symmetric $3 \\times 3$ distance matrix with zeros on the diagonal), and a threshold $r_{\\mathrm{th}}$.\n- Use $\\ell = 10\\,\\mathrm{\\AA}$, $\\tau = 10\\,\\mathrm{\\AA}$, and $\\varepsilon = 10^{-8}$.\n\nCase $1$ (typical):\n- Distogram bin edges: $[0, 4, 8, 12, 16]$; ground-truth distances: $[5.2, 8.7, 12.0]$; predicted probabilities per pair: $[0.05, 0.80, 0.10, 0.05]$, $[0.10, 0.10, 0.70, 0.10]$, $[0.05, 0.15, 0.20, 0.60]$.\n- Frames: $\\phi^{(p)} = 0.1$, $\\mathbf{t}^{(p)} = [0.0, 0.0, 0.0]$; $\\phi^{(t)} = 0.2$, $\\mathbf{t}^{(t)} = [0.1, -0.1, 0.0]$.\n- Points: predicted $\\mathbf{x}^{(p)} = [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]$; true $\\mathbf{x}^{(t)} = [[1.1, -0.1, 0.0], [0.1, 0.9, 0.0]]$.\n- Torsions: predicted $[-2.9, 1.5, 3.1]$; true $[-3.0, 1.4, -3.05]$.\n- Violation distances: $\\begin{bmatrix} 0 & 3.8 & 2.9 \\\\ 3.8 & 0 & 5.2 \\\\ 2.9 & 5.2 & 0 \\end{bmatrix}$; $r_{\\mathrm{th}} = 3.2$.\n\nCase $2$ (boundary wrapping and clamp):\n- Distogram bin edges: $[0, 4, 8, 12, 16]$; ground-truth distances: $[15.9, 0.5, 20.0]$; predicted probabilities per pair: $[0.25, 0.25, 0.25, 0.25]$, $[0.90, 0.05, 0.03, 0.02]$, $[0.05, 0.05, 0.05, 0.85]$.\n- Frames: $\\phi^{(p)} = 0.0$, $\\mathbf{t}^{(p)} = [0.0, 0.0, 0.0]$; $\\phi^{(t)} = 0.0$, $\\mathbf{t}^{(t)} = [20.0, 0.0, 0.0]$.\n- Points: predicted $\\mathbf{x}^{(p)} = [[0.0, 0.0, 0.0], [1.0, 0.0, 0.0]]$; true $\\mathbf{x}^{(t)} = [[20.0, 0.0, 0.0], [21.0, 0.0, 0.0]]$.\n- Torsions: predicted $[-\\pi + 0.01, \\pi - 0.02, 0.0]$; true $[\\pi - 0.01, -\\pi + 0.02, 0.0]$.\n- Violation distances: $\\begin{bmatrix} 0 & 3.2 & 3.2 \\\\ 3.2 & 0 & 3.2 \\\\ 3.2 & 3.2 & 0 \\end{bmatrix}$; $r_{\\mathrm{th}} = 3.2$.\n\nCase $3$ (uniform distogram, no FAPE/torsion error, severe clashes):\n- Distogram bin edges: $[0, 4, 8, 12, 16]$; ground-truth distances: $[7.0, 7.0, 7.0]$; predicted probabilities per pair: $[0.25, 0.25, 0.25, 0.25]$ for all three pairs.\n- Frames: $\\phi^{(p)} = 0.5$, $\\mathbf{t}^{(p)} = [1.0, -1.0, 0.0]$; $\\phi^{(t)} = 0.5$, $\\mathbf{t}^{(t)} = [1.0, -1.0, 0.0]$.\n- Points: predicted $\\mathbf{x}^{(p)} = [[2.0, 0.0, 0.0], [0.0, 2.0, 0.0]]$; true $\\mathbf{x}^{(t)} = [[2.0, 0.0, 0.0], [0.0, 2.0, 0.0]]$.\n- Torsions: predicted $[0.5, -1.0, 2.0]$; true $[0.5, -1.0, 2.0]$.\n- Violation distances: $\\begin{bmatrix} 0 & 2.0 & 2.0 \\\\ 2.0 & 0 & 2.0 \\\\ 2.0 & 2.0 & 0 \\end{bmatrix}$; $r_{\\mathrm{th}} = 3.5$.\n\nYour program should compute $L_{\\mathrm{comp}}$ for each of the three cases, using $\\ell = 10$ in $\\mathrm{\\AA}$, $\\tau = 10$ in $\\mathrm{\\AA}$, and $\\varepsilon = 10^{-8}$, and produce a single line of output containing the three results as a comma-separated list enclosed in square brackets (for example, $[0.123456,0.234567,0.345678]$). The results should be floats with default Python string formatting; do not round or format beyond default behavior. No other text should be printed.", "solution": "The problem requires the construction and implementation of a composite loss function for protein structure prediction, comprising four distinct components: a distogram cross-entropy loss, a Frame Aligned Point Error (FAPE), a torsion angle loss, and a steric violation penalty. The solution involves meticulously implementing each component as specified, ensuring dimensional consistency through a principled non-dimensionalization scheme, and finally combining them using an adaptive weighting mechanism.\n\nFirst, we define and implement each of the four loss components. All distances are in Angstroms ($\\mathrm{\\AA}$), angles are in radians, and logarithms are natural.\n\n1.  **Distogram Cross-Entropy ($L_{\\mathrm{dist}}$)**: This component measures the error in the predicted probability distribution of inter-residue distances. A distogram is a discrete probability distribution over a set of distance bins. Given $K$ bins defined by $K+1$ edges $\\{b_0, b_1, \\dots, b_K\\}$, a ground-truth distance $d$ is assigned to a specific bin $k^\\star$. The problem defines the bin index as $k^\\star = \\max\\{k \\in \\{0, \\dots, K-1\\} : b_k \\le d\\}$. This rule correctly handles all cases, including clamping distances $d \\ge b_K$ to the last bin, $k^\\star = K-1$, since for such $d$, $b_k \\le d$ for all $k \\le K-1$. For a predicted probability vector $\\mathbf{p} = (p_0, \\dots, p_{K-1})$, the cross-entropy loss for a single protein residue pair is given by the negative log-probability of the true bin:\n    $$L_{\\mathrm{dist}}^{\\mathrm{(pair)}} = -\\log p_{k^\\star}$$\n    This loss is inherently dimensionless. The total distogram loss, $L_{\\mathrm{dist}}$, is the arithmetic mean of these individual pair losses over all relevant residue pairs in the protein.\n\n2.  **Frame Aligned Point Error (FAPE)**: FAPE quantifies the error in the predicted 3D coordinates of atoms by measuring their deviation from the true coordinates within a local reference frame. For each residue, a rigid-body frame is defined by a rotation matrix $\\mathbf{R} \\in \\mathbb{R}^{3 \\times 3}$ and a translation vector $\\mathbf{t} \\in \\mathbb{R}^{3}$. Given a set of atoms indexed by $i$, let their predicted and true global coordinates be $\\mathbf{x}_i^{(p)}$ and $\\mathbf{x}_i^{(t)}$, and the corresponding predicted and true local frames be $(\\mathbf{R}^{(p)}, \\mathbf{t}^{(p)})$ and $(\\mathbf{R}^{(t)}, \\mathbf{t}^{(t)})$. The coordinates are first transformed into their respective local frames:\n    $$ \\mathbf{y}_i^{(p)} = \\mathbf{R}^{(p)\\top}(\\mathbf{x}_i^{(p)} - \\mathbf{t}^{(p)}) $$\n    $$ \\mathbf{y}_i^{(t)} = \\mathbf{R}^{(t)\\top}(\\mathbf{x}_i^{(t)} - \\mathbf{t}^{(t)}) $$\n    The error for point $i$ is the Euclidean distance between these local coordinates, clamped at a maximum value $\\tau$ to prevent excessively large gradients from a single large error.\n    $$ e_i = \\min\\left\\{\\tau, \\|\\mathbf{y}_i^{(p)} - \\mathbf{y}_i^{(t)}\\|_2\\right\\} $$\n    The total FAPE loss, denoted $\\overline{e}$, is the mean of these errors $e_i$ over all considered points. This loss has units of distance ($\\mathrm{\\AA}$). The rotation matrices are generated from a given angle $\\phi$ of rotation about the $z$-axis, $\\mathbf{R}_z(\\phi)$.\n\n3.  **Torsion Angle Loss ($L_{\\mathrm{tors}}$)**: This loss measures the deviation of predicted backbone and side-chain torsion angles from their ground-truth values. Since angles are periodic, a simple difference is insufficient. We compute the periodic difference on the circle for each angle $j$, $\\Delta\\theta_j$, between the predicted angle $\\theta_j^{(p)}$ and the true angle $\\theta_j^{(t)}$:\n    $$ \\Delta\\theta_j = \\mathrm{wrap}(\\theta_j^{(p)} - \\theta_j^{(t)}) $$\n    where the wrap function maps its argument to the interval $[-\\pi, \\pi]$ via $\\mathrm{wrap}(\\alpha) = ((\\alpha + \\pi) \\bmod 2\\pi) - \\pi$. A suitable loss function that is smooth, minimal at $\\Delta\\theta_j=0$, and maximal at $\\Delta\\theta_j=\\pm\\pi$ is $1 - \\cos(\\Delta\\theta_j)$. The total torsion loss, $L_{\\mathrm{tors}}$, is the mean of these values over all angles $j$:\n    $$ L_{\\mathrm{tors}} = \\frac{1}{J} \\sum_{j=1}^{J} \\left(1 - \\cos(\\Delta\\theta_j)\\right) $$\n    This quantity is dimensionless.\n\n4.  **Violation Penalty ($L_{\\mathrm{viol}}$)**: This term penalizes physically unrealistic structures, specifically steric clashes where non-bonded atoms are too close. For each pair of non-bonded atoms $(i, j)$, a penalty is incurred if their predicted distance $d_{ij}$ is less than a specified threshold $r_{\\mathrm{th}}$. The penalty is defined as a rectified quadratic function:\n    $$ v_{ij} = \\max\\{0, r_{\\mathrm{th}} - d_{ij}\\}^2 $$\n    This penalizes smaller distances more harshly and provides a smooth gradient. The total violation loss, $L_{\\mathrm{viol}}$, is the average of these penalties over all unique non-bonded pairs $(i, j)$ with $i < j$. This loss has units of distance squared ($\\mathrm{\\AA}^2$).\n\nTo combine these heterogeneous loss components into a single scalar value, we first ensure they are dimensionally consistent. The FAPE loss $\\overline{e}$ and violation loss $L_{\\mathrm{viol}}$ are non-dimensionalized by dividing by a characteristic length scale $\\ell = 10\\,\\mathrm{\\AA}$ and its square, respectively. The four dimensionless loss components are:\n$$ L_1 = L_{\\mathrm{dist}} \\quad (\\text{dimensionless}) $$\n$$ L_2 = \\overline{e} / \\ell \\quad (\\text{dimensionless}) $$\n$$ L_3 = L_{\\mathrm{tors}} \\quad (\\text{dimensionless}) $$\n$$ L_4 = L_{\\mathrm{viol}} / \\ell^2 \\quad (\\text{dimensionless}) $$\nAn adaptive weighting scheme is then employed to balance their contributions. The weights $w_i$ are designed to be inversely proportional to the magnitude of their corresponding loss component $L_i$, ensuring that components with smaller loss values (indicating better predictions for that aspect) are not ignored. The weights are calculated as:\n$$ \\tilde{w}_i = \\frac{1}{\\varepsilon + L_i}, \\quad w_i = \\frac{\\tilde{w}_i}{\\sum_{j=1}^{4} \\tilde{w}_j} $$\nHere, $L_i$ represents the mean value of the respective component for the current data sample, and $\\varepsilon = 10^{-8}$ is a small constant to ensure numerical stability when a loss component is zero or very close to it. The final composite loss, $L_{\\mathrm{comp}}$, is the weighted sum of the dimensionless components:\n$$ L_{\\mathrm{comp}} = \\sum_{i=1}^{4} w_i L_i $$\nThis formulation creates a balanced and dimensionally sound objective function for training a protein structure prediction model. The provided program implements these calculations for three distinct test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes a composite loss for protein structure prediction based on the problem statement.\n    The composite loss is a weighted sum of four components: distogram cross-entropy,\n    Frame Aligned Point Error (FAPE), torsion angle loss, and a violation penalty.\n    \"\"\"\n\n    # --- Constants specified in the problem ---\n    L_SCALE = 10.0  # Characteristic length scale l in Angstroms\n    TAU = 10.0      # FAPE clamp value in Angstroms\n    EPSILON = 1e-8  # Stability constant for weighting\n\n    # --- Test Cases ---\n    test_cases = [\n        {\n            # Case 1 (typical)\n            \"distogram\": {\n                \"bin_edges\": np.array([0, 4, 8, 12, 16], dtype=float),\n                \"true_dists\": np.array([5.2, 8.7, 12.0]),\n                \"pred_probs\": np.array([\n                    [0.05, 0.80, 0.10, 0.05],\n                    [0.10, 0.10, 0.70, 0.10],\n                    [0.05, 0.15, 0.20, 0.60]\n                ])\n            },\n            \"fape\": {\n                \"phi_p\": 0.1, \"t_p\": np.array([0.0, 0.0, 0.0]),\n                \"phi_t\": 0.2, \"t_t\": np.array([0.1, -0.1, 0.0]),\n                \"x_p\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]),\n                \"x_t\": np.array([[1.1, -0.1, 0.0], [0.1, 0.9, 0.0]])\n            },\n            \"torsion\": {\n                \"theta_p\": np.array([-2.9, 1.5, 3.1]),\n                \"theta_t\": np.array([-3.0, 1.4, -3.05])\n            },\n            \"violation\": {\n                \"dist_matrix\": np.array([[0, 3.8, 2.9], [3.8, 0, 5.2], [2.9, 5.2, 0]]),\n                \"r_th\": 3.2\n            }\n        },\n        {\n            # Case 2 (boundary wrapping and clamp)\n            \"distogram\": {\n                \"bin_edges\": np.array([0, 4, 8, 12, 16], dtype=float),\n                \"true_dists\": np.array([15.9, 0.5, 20.0]),\n                \"pred_probs\": np.array([\n                    [0.25, 0.25, 0.25, 0.25],\n                    [0.90, 0.05, 0.03, 0.02],\n                    [0.05, 0.05, 0.05, 0.85]\n                ])\n            },\n            \"fape\": {\n                \"phi_p\": 0.0, \"t_p\": np.array([0.0, 0.0, 0.0]),\n                \"phi_t\": 0.0, \"t_t\": np.array([20.0, 0.0, 0.0]),\n                \"x_p\": np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0]]),\n                \"x_t\": np.array([[20.0, 0.0, 0.0], [21.0, 0.0, 0.0]])\n            },\n            \"torsion\": {\n                \"theta_p\": np.array([-np.pi + 0.01, np.pi - 0.02, 0.0]),\n                \"theta_t\": np.array([np.pi - 0.01, -np.pi + 0.02, 0.0])\n            },\n            \"violation\": {\n                \"dist_matrix\": np.array([[0, 3.2, 3.2], [3.2, 0, 3.2], [3.2, 3.2, 0]]),\n                \"r_th\": 3.2\n            }\n        },\n        {\n            # Case 3 (uniform distogram, no FAPE/torsion error, severe clashes)\n            \"distogram\": {\n                \"bin_edges\": np.array([0, 4, 8, 12, 16], dtype=float),\n                \"true_dists\": np.array([7.0, 7.0, 7.0]),\n                \"pred_probs\": np.array([\n                    [0.25, 0.25, 0.25, 0.25],\n                    [0.25, 0.25, 0.25, 0.25],\n                    [0.25, 0.25, 0.25, 0.25]\n                ])\n            },\n            \"fape\": {\n                \"phi_p\": 0.5, \"t_p\": np.array([1.0, -1.0, 0.0]),\n                \"phi_t\": 0.5, \"t_t\": np.array([1.0, -1.0, 0.0]),\n                \"x_p\": np.array([[2.0, 0.0, 0.0], [0.0, 2.0, 0.0]]),\n                \"x_t\": np.array([[2.0, 0.0, 0.0], [0.0, 2.0, 0.0]])\n            },\n            \"torsion\": {\n                \"theta_p\": np.array([0.5, -1.0, 2.0]),\n                \"theta_t\": np.array([0.5, -1.0, 2.0])\n            },\n            \"violation\": {\n                \"dist_matrix\": np.array([[0, 2.0, 2.0], [2.0, 0, 2.0], [2.0, 2.0, 0]]),\n                \"r_th\": 3.5\n            }\n        }\n    ]\n\n    results = []\n    \n    # --- Helper functions for loss components ---\n    \n    def _calculate_distogram_loss(bin_edges, true_dists, pred_probs):\n        losses = []\n        num_bins = len(pred_probs[0])\n        bin_indices = bin_edges[:num_bins]\n        for d, p in zip(true_dists, pred_probs):\n            # Find bin index k_star = max{k in {0..K-1}: b_k = d}\n            valid_indices = np.where(bin_indices = d)[0]\n            k_star = np.max(valid_indices)\n            loss = -np.log(p[k_star])\n            losses.append(loss)\n        return np.mean(losses) if losses else 0.0\n\n    def _calculate_fape_loss(phi_p, t_p, phi_t, t_t, x_p, x_t):\n        # Rotation matrix for rotation about z-axis\n        def get_rot_mat(phi):\n            c_phi, s_phi = np.cos(phi), np.sin(phi)\n            return np.array([\n                [c_phi, -s_phi, 0],\n                [s_phi,  c_phi, 0],\n                [0,          0, 1]\n            ])\n\n        R_p = get_rot_mat(phi_p)\n        R_t = get_rot_mat(phi_t)\n\n        errors = []\n        for i in range(len(x_p)):\n            y_p = R_p.T @ (x_p[i] - t_p)\n            y_t = R_t.T @ (x_t[i] - t_t)\n            dist = np.linalg.norm(y_p - y_t)\n            error = min(TAU, dist)\n            errors.append(error)\n        return np.mean(errors) if errors else 0.0\n\n    def _calculate_torsion_loss(theta_p, theta_t):\n        diff = theta_p - theta_t\n        # Wrap difference to [-pi, pi]\n        wrapped_diff = (diff + np.pi) % (2 * np.pi) - np.pi\n        losses = 1 - np.cos(wrapped_diff)\n        return np.mean(losses) if len(losses) > 0 else 0.0\n\n    def _calculate_violation_loss(dist_matrix, r_th):\n        penalties = []\n        num_atoms = dist_matrix.shape[0]\n        for i in range(num_atoms):\n            for j in range(i + 1, num_atoms):\n                d_ij = dist_matrix[i, j]\n                penalty = max(0, r_th - d_ij)**2\n                penalties.append(penalty)\n        return np.mean(penalties) if penalties else 0.0\n\n    # --- Main computation loop ---\n    for case in test_cases:\n        # Calculate individual loss components\n        l_dist = _calculate_distogram_loss(**case[\"distogram\"])\n        e_bar = _calculate_fape_loss(**case[\"fape\"])\n        l_tors = _calculate_torsion_loss(**case[\"torsion\"])\n        l_viol = _calculate_violation_loss(**case[\"violation\"])\n        \n        # Nondimensionalize\n        L1 = l_dist\n        L2 = e_bar / L_SCALE\n        L3 = l_tors\n        L4 = l_viol / (L_SCALE**2)\n        \n        losses = np.array([L1, L2, L3, L4])\n        \n        # Calculate adaptive weights\n        unnorm_weights = 1.0 / (EPSILON + losses)\n        sum_unnorm_weights = np.sum(unnorm_weights)\n        weights = unnorm_weights / sum_unnorm_weights\n        \n        # Calculate composite loss\n        composite_loss = np.sum(weights * losses)\n        results.append(composite_loss)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3842267"}, {"introduction": "A model's true utility is measured by its ability to generalize to novel proteins not seen during training. This final exercise tackles the critical and often subtle task of creating a rigorous validation framework by simulating a real-world scenario where a model's performance must be assessed on data that became available *after* the training cutoff date, a principle known as temporal causality. By implementing a time-based data split that also filters out homologous structures, you will learn a robust methodology to prevent data leakage and obtain an honest measure of a model's predictive power on future targets [@problem_id:4554918].", "problem": "You are evaluating temporal generalization for deep learning models in protein structure prediction using the Protein Data Bank (PDB). To prevent information leakage from future structures, evaluation must respect release-time causality. Start from the following fundamental base: a model’s generalization must be evaluated on data that could not have been available at training time, which implies temporal ordering. Formally, for a dataset of entries indexed by $i$ with tuples $\\left(x_i, t_i, c_i\\right)$ where $x_i$ is a target identifier, $t_i$ is its release date, and $c_i$ is a cluster label indicating approximate sequence similarity grouping, a valid time-based split around a cutoff time $\\tau$ with a nonnegative buffer $b$ (in days) must satisfy:\n- Training uses only entries with $t_i \\le \\tau - b$.\n- Evaluation uses only entries with $t_i  \\tau$.\n- To avoid homolog leakage, exclude any evaluation entry whose cluster label $c_i$ appeared in training clusters.\n\nDefine the following sets and counts:\n- Training index set $I_{\\mathrm{train}}(\\tau, b) = \\{ i \\mid t_i \\le \\tau - b \\}$.\n- Candidate evaluation set $I_{\\mathrm{post}}(\\tau) = \\{ i \\mid t_i  \\tau \\}$.\n- Training cluster set $C_{\\mathrm{train}}(\\tau, b) = \\{ c_i \\mid i \\in I_{\\mathrm{train}}(\\tau, b) \\}$.\n- Clean evaluation set $I_{\\mathrm{clean}}(\\tau, b) = \\{ i \\in I_{\\mathrm{post}}(\\tau) \\mid c_i \\notin C_{\\mathrm{train}}(\\tau, b) \\}$.\n- Naive evaluation count $N_{\\mathrm{naive}}(\\tau, b) = \\left| I_{\\mathrm{post}}(\\tau) \\right|$.\n- Clean evaluation count $N_{\\mathrm{clean}}(\\tau, b) = \\left| I_{\\mathrm{clean}}(\\tau, b) \\right|$.\n- Leaked count $N_{\\mathrm{leak}}(\\tau, b) = N_{\\mathrm{naive}}(\\tau, b) - N_{\\mathrm{clean}}(\\tau, b)$.\n\nImplement a complete program that computes these counts for a fixed dataset of PDB entries. Each entry is a tuple consisting of a PDB identifier string, an ISO date string \"YYYY-MM-DD\" representing release date, and a cluster identifier string. The dataset is:\n- (\"1A01\", \"2016-12-15\", \"C1\")\n- (\"2B02\", \"2017-06-10\", \"C2\")\n- (\"13M13\", \"2017-08-01\", \"C2\")\n- (\"3C03\", \"2017-12-31\", \"C3\")\n- (\"4D04\", \"2018-01-01\", \"C1\")\n- (\"14N14\", \"2018-01-31\", \"C11\")\n- (\"5E05\", \"2018-03-01\", \"C4\")\n- (\"6F06\", \"2018-05-01\", \"C5\")\n- (\"7G07\", \"2018-07-01\", \"C2\")\n- (\"8H08\", \"2019-01-15\", \"C6\")\n- (\"9I09\", \"2020-06-01\", \"C7\")\n- (\"10J10\", \"2021-01-01\", \"C8\")\n- (\"11K11\", \"2021-03-02\", \"C9\")\n- (\"12L12\", \"2022-01-01\", \"C10\")\n- (\"15O15\", \"2019-12-31\", \"C1\")\n\nYour program must compute, for each specified $(\\tau, b)$ pair, the triplet $\\left[N_{\\mathrm{naive}}(\\tau, b), N_{\\mathrm{clean}}(\\tau, b), N_{\\mathrm{leak}}(\\tau, b)\\right]$ using the definitions above. Dates are to be interpreted in calendar days, and $b$ is measured in days. All times are naive dates without time zones. No physical units beyond days are required in the final answer because only integer counts are requested.\n\nUse the following test suite of $(\\tau, b)$ pairs:\n- $\\tau =$ \"2018-01-01\", $b = 0$.\n- $\\tau =$ \"2018-01-01\", $b = 60$.\n- $\\tau =$ \"2019-01-01\", $b = 30$.\n- $\\tau =$ \"2016-12-15\", $b = 0$.\n- $\\tau =$ \"2022-01-01\", $b = 0$.\n\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list of triplets enclosed in square brackets, with no spaces, for example: \"[[a,b,c],[d,e,f],...]\". The output elements must be integers. The ordering of results must match the ordering of test cases above.", "solution": "The problem requires the implementation of a temporal splitting methodology to partition a dataset of protein structures for the purpose of evaluating a machine learning model. This is a critical procedure in bioinformatics to ensure that a model's performance on a test set is representative of its ability to generalize to new, unseen data, rather than its ability to recognize or interpolate from structurally or evolutionarily related examples present in the training set. The evaluation is based on release dates from the Protein Data Bank (PDB), ensuring a causal ordering.\n\nThe method is formalized through a set of definitions based on a cutoff date $\\tau$ and a temporal buffer $b$. The buffer ensures that the training set and evaluation set are separated by a minimum time interval, further reducing the possibility of information leakage. Let each entry in the dataset be a tuple $(x_i, t_i, c_i)$, where $i$ is the index, $x_i$ is a target identifier, $t_i$ is its release date, and $c_i$ is a cluster label representing sequence similarity (a proxy for evolutionary relatedness, or homology). For date comparisons, all date strings are interpreted as calendar dates.\n\nThe core definitions are as follows:\n1.  The training set, $I_{\\mathrm{train}}(\\tau, b)$, comprises all entries with release dates on or before a specified cutoff time reduced by a buffer period. The training cutoff date is $\\tau - b$.\n    $$I_{\\mathrm{train}}(\\tau, b) = \\{ i \\mid t_i \\le \\tau - b \\}$$\n2.  The set of clusters present in the training set, $C_{\\mathrm{train}}(\\tau, b)$, is collected to identify potential homologs.\n    $$C_{\\mathrm{train}}(\\tau, b) = \\{ c_i \\mid i \\in I_{\\mathrm{train}}(\\tau, b) \\}$$\n3.  The candidate evaluation set, $I_{\\mathrm{post}}(\\tau)$, consists of all entries released after the cutoff time $\\tau$.\n    $$I_{\\mathrm{post}}(\\tau) = \\{ i \\mid t_i  \\tau \\}$$\n4.  The \"clean\" evaluation set, $I_{\\mathrm{clean}}(\\tau, b)$, is a subset of the candidate evaluation set, from which any entry belonging to a cluster already seen in the training set has been removed. This explicitly filters out homologous structures.\n    $$I_{\\mathrm{clean}}(\\tau, b) = \\{ i \\in I_{\\mathrm{post}}(\\tau) \\mid c_i \\notin C_{\\mathrm{train}}(\\tau, b) \\}$$\n5.  From these sets, we define the counts to be computed:\n    -   $N_{\\mathrm{naive}}(\\tau, b) = \\left| I_{\\mathrm{post}}(\\tau) \\right|$: The total number of candidate evaluation entries.\n    -   $N_{\\mathrm{clean}}(\\tau, b) = \\left| I_{\\mathrm{clean}}(\\tau, b) \\right|$: The number of evaluation entries after removing homologs.\n    -   $N_{\\mathrm{leak}}(\\tau, b) = N_{\\mathrm{naive}}(\\tau, b) - N_{\\mathrm{clean}}(\\tau, b)$: The number of entries removed, representing the \"leaked\" set.\n\nWe will now apply these definitions to the provided dataset for each test case.\n\nThe dataset contains $15$ entries:\n- $d_0$: (\"1A01\", \"2016-12-15\", \"C1\")\n- $d_1$: (\"2B02\", \"2017-06-10\", \"C2\")\n- $d_2$: (\"13M13\", \"2017-08-01\", \"C2\")\n- $d_3$: (\"3C03\", \"2017-12-31\", \"C3\")\n- $d_4$: (\"4D04\", \"2018-01-01\", \"C1\")\n- $d_5$: (\"14N14\", \"2018-01-31\", \"C11\")\n- $d_6$: (\"5E05\", \"2018-03-01\", \"C4\")\n- $d_7$: (\"6F06\", \"2018-05-01\", \"C5\")\n- $d_8$: (\"7G07\", \"2018-07-01\", \"C2\")\n- $d_9$: (\"8H08\", \"2019-01-15\", \"C6\")\n- $d_{10}$: (\"9I09\", \"2020-06-01\", \"C7\")\n- $d_{11}$: (\"10J10\", \"2021-01-01\", \"C8\")\n- $d_{12}$: (\"11K11\", \"2021-03-02\", \"C9\")\n- $d_{13}$: (\"12L12\", \"2022-01-01\", \"C10\")\n- $d_{14}$: (\"15O15\", \"2019-12-31\", \"C1\")\n\n**Case 1: $\\tau = \\text{\"2018-01-01\"}$, $b = 0$ days**\n- Training cutoff date: $t_{\\mathrm{cutoff}} = \\text{\"2018-01-01\"} - 0 \\text{ days} = \\text{\"2018-01-01\"}$.\n- $I_{\\mathrm{train}}$: Entries with $t_i \\le \\text{\"2018-01-01\"}$. These are $d_0, d_1, d_2, d_3, d_4$ ($5$ entries).\n- $C_{\\mathrm{train}}$: Clusters from these entries are $\\{c_0, c_1, c_2, c_3, c_4\\} = \\{\\text{\"C1\"}, \\text{\"C2\"}, \\text{\"C3\"}\\}$.\n- $I_{\\mathrm{post}}$: Entries with $t_i  \\text{\"2018-01-01\"}$. These are $d_5, d_6, d_7, d_8, d_9, d_{10}, d_{11}, d_{12}, d_{13}, d_{14}$ ($10$ entries).\n- $N_{\\mathrm{naive}} = 10$.\n- $I_{\\mathrm{clean}}$: Filter $I_{\\mathrm{post}}$ against $C_{\\mathrm{train}}$.\n  - $d_8$ (cluster \"C2\") is removed because \"C2\" $\\in C_{\\mathrm{train}}$.\n  - $d_{14}$ (cluster \"C1\") is removed because \"C1\" $\\in C_{\\mathrm{train}}$.\n  - The remaining $8$ entries form $I_{\\mathrm{clean}}$.\n- $N_{\\mathrm{clean}} = 8$.\n- $N_{\\mathrm{leak}} = 10 - 8 = 2$.\n- Result: $[10, 8, 2]$.\n\n**Case 2: $\\tau = \\text{\"2018-01-01\"}$, $b = 60$ days**\n- Training cutoff date: $t_{\\mathrm{cutoff}} = \\text{\"2018-01-01\"} - 60 \\text{ days} = \\text{\"2017-11-02\"}$.\n- $I_{\\mathrm{train}}$: Entries with $t_i \\le \\text{\"2017-11-02\"}$. These are $d_0, d_1, d_2$ ($3$ entries).\n- $C_{\\mathrm{train}}$: Clusters from these entries are $\\{c_0, c_1, c_2\\} = \\{\\text{\"C1\"}, \\text{\"C2\"}\\}$.\n- $I_{\\mathrm{post}}$: Entries with $t_i  \\text{\"2018-01-01\"}$. This set is identical to Case 1.\n- $N_{\\mathrm{naive}} = 10$.\n- $I_{\\mathrm{clean}}$: Filter $I_{\\mathrm{post}}$ against the new $C_{\\mathrm{train}}$.\n  - $d_8$ (cluster \"C2\") is removed because \"C2\" $\\in C_{\\mathrm{train}}$.\n  - $d_{14}$ (cluster \"C1\") is removed because \"C1\" $\\in C_{\\mathrm{train}}$.\n  - The remaining $8$ entries form $I_{\\mathrm{clean}}$.\n- $N_{\\mathrm{clean}} = 8$.\n- $N_{\\mathrm{leak}} = 10 - 8 = 2$.\n- Result: $[10, 8, 2]$.\n\n**Case 3: $\\tau = \\text{\"2019-01-01\"}$, $b = 30$ days**\n- Training cutoff date: $t_{\\mathrm{cutoff}} = \\text{\"2019-01-01\"} - 30 \\text{ days} = \\text{\"2018-12-02\"}$.\n- $I_{\\mathrm{train}}$: Entries with $t_i \\le \\text{\"2018-12-02\"}$. These are $d_0, d_1, d_2, d_3, d_4, d_5, d_6, d_7, d_8$ ($9$ entries).\n- $C_{\\mathrm{train}}$: Clusters from these entries are $\\{\\text{\"C1\"}, \\text{\"C2\"}, \\text{\"C3\"}, \\text{\"C11\"}, \\text{\"C4\"}, \\text{\"C5\"}\\}$.\n- $I_{\\mathrm{post}}$: Entries with $t_i  \\text{\"2019-01-01\"}$. These are $d_9, d_{10}, d_{11}, d_{12}, d_{13}, d_{14}$ ($6$ entries).\n- $N_{\\mathrm{naive}} = 6$.\n- $I_{\\mathrm{clean}}$: Filter $I_{\\mathrm{post}}$ against $C_{\\mathrm{train}}$.\n  - $d_{14}$ (cluster \"C1\") is removed because \"C1\" $\\in C_{\\mathrm{train}}$.\n  - The remaining $5$ entries form $I_{\\mathrm{clean}}$.\n- $N_{\\mathrm{clean}} = 5$.\n- $N_{\\mathrm{leak}} = 6 - 5 = 1$.\n- Result: $[6, 5, 1]$.\n\n**Case 4: $\\tau = \\text{\"2016-12-15\"}$, $b = 0$ days**\n- Training cutoff date: $t_{\\mathrm{cutoff}} = \\text{\"2016-12-15\"} - 0 \\text{ days} = \\text{\"2016-12-15\"}$.\n- $I_{\\mathrm{train}}$: Entries with $t_i \\le \\text{\"2016-12-15\"}$. Only $d_0$ qualifies ($1$ entry).\n- $C_{\\mathrm{train}} = \\{\\text{\"C1\"}\\}$.\n- $I_{\\mathrm{post}}$: Entries with $t_i  \\text{\"2016-12-15\"}$. This includes all entries except $d_0$ ($14$ entries).\n- $N_{\\mathrm{naive}} = 14$.\n- $I_{\\mathrm{clean}}$: Filter $I_{\\mathrm{post}}$ against $C_{\\mathrm{train}}$.\n  - $d_4$ (cluster \"C1\") is removed.\n  - $d_{14}$ (cluster \"C1\") is removed.\n  - The remaining $12$ entries form $I_{\\mathrm{clean}}$.\n- $N_{\\mathrm{clean}} = 12$.\n- $N_{\\mathrm{leak}} = 14 - 12 = 2$.\n- Result: $[14, 12, 2]$.\n\n**Case 5: $\\tau = \\text{\"2022-01-01\"}$, $b = 0$ days**\n- Training cutoff date: $t_{\\mathrm{cutoff}} = \\text{\"2022-01-01\"} - 0 \\text{ days} = \\text{\"2022-01-01\"}$.\n- $I_{\\mathrm{train}}$: Entries with $t_i \\le \\text{\"2022-01-01\"}$. All $15$ entries in the dataset qualify.\n- $C_{\\mathrm{train}} = \\{\\text{\"C1\"}, \\text{\"C2\"}, \\text{\"C3\"}, \\text{\"C4\"}, \\text{\"C5\"}, \\text{\"C6\"}, \\text{\"C7\"}, \\text{\"C8\"}, \\text{\"C9\"}, \\text{\"C10\"}, \\text{\"C11\"}\\}$.\n- $I_{\\mathrm{post}}$: Entries with $t_i  \\text{\"2022-01-01\"}$. There are no such entries in the dataset ($0$ entries).\n- $N_{\\mathrm{naive}} = 0$.\n- $I_{\\mathrm{clean}}$: Since $I_{\\mathrm{post}}$ is empty, $I_{\\mathrm{clean}}$ is also empty.\n- $N_{\\mathrm{clean}} = 0$.\n- $N_{\\mathrm{leak}} = 0 - 0 = 0$.\n- Result: $[0, 0, 0]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom datetime import date, timedelta\n\ndef solve():\n    \"\"\"\n    Computes naive, clean, and leaked evaluation counts for a temporal\n    splitting methodology on a fixed dataset of PDB entries.\n    \"\"\"\n    \n    # Dataset of PDB entries: (PDB ID, Release Date, Cluster ID)\n    dataset = [\n        (\"1A01\", \"2016-12-15\", \"C1\"),\n        (\"2B02\", \"2017-06-10\", \"C2\"),\n        (\"13M13\", \"2017-08-01\", \"C2\"),\n        (\"3C03\", \"2017-12-31\", \"C3\"),\n        (\"4D04\", \"2018-01-01\", \"C1\"),\n        (\"14N14\", \"2018-01-31\", \"C11\"),\n        (\"5E05\", \"2018-03-01\", \"C4\"),\n        (\"6F06\", \"2018-05-01\", \"C5\"),\n        (\"7G07\", \"2018-07-01\", \"C2\"),\n        (\"8H08\", \"2019-01-15\", \"C6\"),\n        (\"9I09\", \"2020-06-01\", \"C7\"),\n        (\"10J10\", \"2021-01-01\", \"C8\"),\n        (\"11K11\", \"2021-03-02\", \"C9\"),\n        (\"12L12\", \"2022-01-01\", \"C10\"),\n        (\"15O15\", \"2019-12-31\", \"C1\"),\n    ]\n\n    # Parse date strings into date objects for easier comparison.\n    parsed_dataset = [\n        (pdb_id, date.fromisoformat(date_str), cluster_id)\n        for pdb_id, date_str, cluster_id in dataset\n    ]\n\n    # Test suite of (tau, b) pairs.\n    test_cases = [\n        (\"2018-01-01\", 0),\n        (\"2018-01-01\", 60),\n        (\"2019-01-01\", 30),\n        (\"2016-12-15\", 0),\n        (\"2022-01-01\", 0),\n    ]\n\n    results = []\n    for tau_str, b in test_cases:\n        tau_date = date.fromisoformat(tau_str)\n        buffer_delta = timedelta(days=b)\n        train_cutoff_date = tau_date - buffer_delta\n\n        # 1. Determine the training cluster set C_train(tau, b)\n        training_clusters = set()\n        for _, t_i, c_i in parsed_dataset:\n            if t_i = train_cutoff_date:\n                training_clusters.add(c_i)\n\n        # 2. Determine the candidate evaluation set I_post(tau)\n        post_tau_entries = []\n        for entry in parsed_dataset:\n            _, t_i, _ = entry\n            if t_i > tau_date:\n                post_tau_entries.append(entry)\n        \n        n_naive = len(post_tau_entries)\n\n        # 3. Determine the clean evaluation set I_clean(tau, b)\n        clean_entries = []\n        for entry in post_tau_entries:\n            _, _, c_i = entry\n            if c_i not in training_clusters:\n                clean_entries.append(entry)\n        \n        n_clean = len(clean_entries)\n\n        # 4. Calculate the leaked count N_leak\n        n_leak = n_naive - n_clean\n\n        results.append([n_naive, n_clean, n_leak])\n\n    # Format the results into the exact required output string.\n    # e.g., [[1,2,3],[4,5,6]]\n    formatted_results = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]\n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "4554918"}]}