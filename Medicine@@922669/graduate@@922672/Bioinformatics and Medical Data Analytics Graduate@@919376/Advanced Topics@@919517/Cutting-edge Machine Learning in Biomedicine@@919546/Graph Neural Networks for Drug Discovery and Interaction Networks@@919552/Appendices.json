{"hands_on_practices": [{"introduction": "Graph Neural Networks (GNNs) achieve their power by passing messages between connected nodes, an operation often conceptualized as a \"graph convolution.\" To build effective models, it is essential to understand the mathematical operators that enable this process. This first exercise [@problem_id:4570165] grounds this concept in spectral graph theory by focusing on the normalized Laplacian matrix, a fundamental operator in Graph Convolutional Networks (GCNs). By computing this matrix for a simple molecule, you will gain insight into how GCNs perform stable and localized filtering that respects the graph's topology, a crucial feature for handling biological networks with nodes of widely varying connectivity.", "problem": "Consider a molecular graph representation where atoms are vertices and covalent bonds are undirected edges. In spectral graph theory, define the normalized Laplacian as $L = I - D^{-1/2} A D^{-1/2}$, where $A$ is the adjacency matrix, $D$ is the diagonal degree matrix, and $I$ is the identity matrix. In a Graph Neural Network (GNN), and in particular a Graph Convolutional Network (GCN), graph filtering acts on graph signals using operators constructed from $L$ to achieve localized smoothing that respects the topology of the interaction network. Starting from the fundamental base that graph convolution in the spectral domain corresponds to multiplication by a spectral filter on the eigenvalues of $L$ and that polynomials in $L$ implement localized spatial filters, explain why the symmetric normalization $L = I - D^{-1/2} A D^{-1/2}$ is used to stabilize filtering across heterogeneous degrees in biomolecular interaction networks.\n\nNow consider the unweighted, undirected molecular graph of water, where vertex $2$ is oxygen and vertices $1$ and $3$ are hydrogen atoms, with bonds $(1,2)$ and $(2,3)$. Construct $A$, $D$, and compute the normalized Laplacian $L$. Finally, compute the second-smallest eigenvalue (the algebraic connectivity) $\\lambda_{2}$ of $L$ for this graph. Report $\\lambda_{2}$ as your final answer. No rounding is necessary, and the quantity is dimensionless.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of spectral graph theory and Graph Neural Networks (GNNs), well-posed with a uniquely defined computational task, and objective in its language. All necessary information is provided for both the explanatory and computational parts of the problem.\n\nThe problem consists of two parts. The first part requires an explanation for the use of the symmetric normalized Laplacian in GNNs. The second part requires the computation of the normalized Laplacian and its second-smallest eigenvalue for the molecular graph of water.\n\n\\subsection*{Part 1: Explanation of Symmetric Normalization}\n\nIn graph signal processing and GNNs, a graph convolution operation can be interpreted as a filter that smooths a signal defined on the vertices of the graph. A graph signal is a vector $x \\in \\mathbb{R}^N$, where $x_i$ is the value of the signal (e.g., a feature vector) at vertex $i$. The simplest filtering operation involves aggregating information from neighboring nodes. This can be expressed as a multiplication of the signal $x$ by the graph's adjacency matrix $A$. The new signal $y$ is given by $y = Ax$, where $y_i = \\sum_{j \\in \\mathcal{N}(i)} x_j$.\n\nA significant issue arises when using the raw adjacency matrix $A$ in networks with heterogeneous degree distributions, such as biomolecular interaction networks. Nodes with high degrees (hubs) will have a much larger value $y_i$ than nodes with low degrees, simply because they sum over more neighbors. This can lead to the scale of feature vectors being highly dependent on node degree, causing numerical instability and gradient explosion during the training of a deep neural network.\n\nTo counteract this, normalization is introduced. A common initial approach is row-normalization, using the operator $D^{-1}A$, where $D$ is the diagonal degree matrix. The filtering operation becomes $y = D^{-1}Ax$, which computes the entry $y_i = \\frac{1}{\\deg(i)} \\sum_{j \\in \\mathcal{N}(i)} x_j$. This operation replaces the sum with an average of the features of neighboring nodes. While this resolves the issue of exploding feature scales for the receiving node $i$, it introduces a different bias: it does not account for the degree of the sending nodes $j$. A message from a hub node is treated with the same importance as a message from a node with degree $1$.\n\nThe symmetric normalization, which forms the basis of the normalized Laplacian $L = I - D^{-1/2} A D^{-1/2}$, addresses this limitation. The operator used in the Graph Convolutional Network (GCN) propagation rule is effectively $\\tilde{A} = D^{-1/2} A D^{-1/2}$ (often with added self-loops, but we consider the base form here). The filtering operation $y = \\tilde{A}x$ is given element-wise by:\n$$y_i = \\sum_{j=1}^N (\\tilde{A})_{ij} x_j = \\sum_{j=1}^N \\frac{A_{ij}}{\\sqrt{\\deg(i)\\deg(j)}} x_j = \\frac{1}{\\sqrt{\\deg(i)}} \\sum_{j \\in \\mathcal{N}(i)} \\frac{x_j}{\\sqrt{\\deg(j)}}$$\nThis formulation normalizes the information flow by the degrees of both the source and target nodes. It prevents nodes with high degrees from dominating the aggregation process, leading to a more stable and balanced update rule.\n\nFrom a spectral perspective, the use of the normalized Laplacian $L$ is crucial. The eigenvalues of $L$ are guaranteed to lie in the range $[0, 2]$. This property is vital for the stability of deep GNNs. A GCN layer applies a filter to the graph signal. Stacking multiple layers corresponds to applying a polynomial of the graph filter operator. If the operator's eigenvalues have magnitudes greater than $1$, repeated applications can lead to exploding or vanishing signals (and gradients). By ensuring the eigenvalues of $L$ are bounded in a small, stable range, the symmetric normalization allows for the construction of well-behaved polynomial filters, enabling the creation of deep and effective GNN models. In summary, the symmetric normalization $D^{-1/2} A D^{-1/2}$ is used because it (1) spatially balances the influence between nodes of varying degrees, and (2) spectrally ensures the stability of graph filtering operations in deep networks.\n\n\\subsection*{Part 2: Calculation for the Water Molecule}\n\nThe water molecule, H-O-H, is represented as an unweighted, undirected graph with vertices $V = \\{1, 2, 3\\}$, where vertex $2$ is Oxygen (O) and vertices $1$ and $3$ are Hydrogen (H). The bonds are given as edges $E = \\{(1, 2), (2, 3)\\}$.\n\nFirst, we construct the adjacency matrix $A$. This is a $3 \\times 3$ symmetric matrix where $A_{ij} = 1$ if an edge exists between vertices $i$ and $j$, and $A_{ij} = 0$ otherwise.\n$$A = \\begin{pmatrix} 0  1  0 \\\\ 1  0  1 \\\\ 0  1  0 \\end{pmatrix}$$\n\nNext, we construct the diagonal degree matrix $D$. The degree of a vertex is the number of edges connected to it.\n$\\deg(1) = 1$\n$\\deg(2) = 2$\n$\\deg(3) = 1$\nThe degree matrix $D$ has these degrees on its diagonal and zeros elsewhere.\n$$D = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  1 \\end{pmatrix}$$\n\nWe then compute the matrix $D^{-1/2}$, which is obtained by taking the inverse square root of each diagonal element of $D$.\n$$D^{-1/2} = \\begin{pmatrix} 1^{-1/2}  0  0 \\\\ 0  2^{-1/2}  0 \\\\ 0  0  1^{-1/2} \\end{pmatrix} = \\begin{pmatrix} 1  0  0 \\\\ 0  \\frac{1}{\\sqrt{2}}  0 \\\\ 0  0  1 \\end{pmatrix}$$\n\nNow, we compute the symmetrically normalized adjacency matrix, $\\tilde{A} = D^{-1/2} A D^{-1/2}$.\n$$\\tilde{A} = \\begin{pmatrix} 1  0  0 \\\\ 0  \\frac{1}{\\sqrt{2}}  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 0  1  0 \\\\ 1  0  1 \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  \\frac{1}{\\sqrt{2}}  0 \\\\ 0  0  1 \\end{pmatrix}$$\n$$\\tilde{A} = \\begin{pmatrix} 0  1  0 \\\\ \\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{2}} \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  \\frac{1}{\\sqrt{2}}  0 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 0  \\frac{1}{\\sqrt{2}}  0 \\\\ \\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{2}} \\\\ 0  \\frac{1}{\\sqrt{2}}  0 \\end{pmatrix}$$\n\nThe normalized Laplacian $L$ is defined as $L = I - \\tilde{A}$.\n$$L = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} - \\begin{pmatrix} 0  \\frac{1}{\\sqrt{2}}  0 \\\\ \\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{2}} \\\\ 0  \\frac{1}{\\sqrt{2}}  0 \\end{pmatrix} = \\begin{pmatrix} 1  -\\frac{1}{\\sqrt{2}}  0 \\\\ -\\frac{1}{\\sqrt{2}}  1  -\\frac{1}{\\sqrt{2}} \\\\ 0  -\\frac{1}{\\sqrt{2}}  1 \\end{pmatrix}$$\n\nFinally, we compute the eigenvalues of $L$ by solving the characteristic equation $\\det(L - \\lambda I) = 0$.\n$$\\det \\begin{pmatrix} 1-\\lambda  -\\frac{1}{\\sqrt{2}}  0 \\\\ -\\frac{1}{\\sqrt{2}}  1-\\lambda  -\\frac{1}{\\sqrt{2}} \\\\ 0  -\\frac{1}{\\sqrt{2}}  1-\\lambda \\end{pmatrix} = 0$$\nExpanding the determinant along the first row:\n$$(1-\\lambda) \\left| \\begin{matrix} 1-\\lambda  -\\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}}  1-\\lambda \\end{matrix} \\right| - \\left(-\\frac{1}{\\sqrt{2}}\\right) \\left| \\begin{matrix} -\\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{2}} \\\\ 0  1-\\lambda \\end{matrix} \\right| = 0$$\n$$(1-\\lambda) \\left[ (1-\\lambda)^2 - \\left(-\\frac{1}{\\sqrt{2}}\\right)^2 \\right] + \\frac{1}{\\sqrt{2}} \\left[ \\left(-\\frac{1}{\\sqrt{2}}\\right)(1-\\lambda) - 0 \\right] = 0$$\n$$(1-\\lambda) \\left[ (1-\\lambda)^2 - \\frac{1}{2} \\right] - \\frac{1}{2}(1-\\lambda) = 0$$\nWe can factor out the term $(1-\\lambda)$:\n$$(1-\\lambda) \\left[ (1-\\lambda)^2 - \\frac{1}{2} - \\frac{1}{2} \\right] = 0$$\n$$(1-\\lambda) \\left[ (1-\\lambda)^2 - 1 \\right] = 0$$\nThis equation yields three solutions for $\\lambda$:\n1. $1-\\lambda = 0 \\implies \\lambda = 1$\n2. $(1-\\lambda)^2 - 1 = 0 \\implies (1-\\lambda)^2 = 1 \\implies 1-\\lambda = \\pm 1$\n   - $1-\\lambda = 1 \\implies \\lambda = 0$\n   - $1-\\lambda = -1 \\implies \\lambda = 2$\n\nThe eigenvalues of $L$ are $\\lambda_1 = 0$, $\\lambda_2 = 1$, and $\\lambda_3 = 2$, sorted in non-decreasing order. The smallest eigenvalue is $\\lambda_1 = 0$, as expected for a connected graph. The second-smallest eigenvalue (the algebraic connectivity) is $\\lambda_2$.\n\nTherefore, $\\lambda_2 = 1$.", "answer": "$$\\boxed{1}$$", "id": "4570165"}, {"introduction": "While simple graphs are a good starting point, real-world biological interaction networks are often more complex, featuring multiple types of relationships. For instance, a drug might 'inhibit' one protein but only 'bind' to another. The Relational Graph Convolutional Network (R-GCN) architecture was developed specifically to handle such multi-relational data. This practice [@problem_id:4570117] guides you through the derivation and application of the core R-GCN update rule, which uses relation-specific transformations to process information. This exercise will clarify how GNNs can be extended to capture the rich, heterogeneous semantics of complex interaction networks common in drug discovery.", "problem": "Consider a typed molecular interaction network in bioinformatics and medical data analytics, where nodes represent drugs and proteins, and directed edges are annotated with interaction types. A Graph Neural Network (GNN) is required to be permutation-invariant over neighborhood multisets, and a Relational Graph Convolutional Network (R-GCN) extends this by using relation-specific linear transformations for typed edges and an optional self-loop contribution. Starting from the principles that (i) message passing on graphs aggregates information from neighborhoods in a permutation-invariant manner, (ii) typed neighborhoods induce per-relation aggregators, and (iii) linear transformations are the simplest equivariant maps on vector features, derive the per-node update equation for a Relational Graph Convolutional Network (R-GCN) layer that uses relation-specific weight matrices and a self-loop term. Assume the normalization factor is the size of the typed neighborhood, $c_{i,r} = |\\mathcal{N}_i^r|$, and that the activation function is the identity map, $\\sigma(x) = x$.\n\nThen, apply your derived equation to the following small typed subgraph. The node set is $\\mathcal{V} = \\{ d_1, d_2, p_1, p_2 \\}$, with initial feature vectors in $\\mathbb{R}^2$ given by\n$h_{d_1}^{(0)} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, $h_{d_2}^{(0)} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$, $h_{p_1}^{(0)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, and $h_{p_2}^{(0)} = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}$.\nThe set of relation types is $\\mathcal{R} = \\{ \\text{binds}, \\text{inhibits}, \\text{interacts} \\}$. The incoming typed neighborhoods of $d_1$ are\n$\\mathcal{N}_{d_1}^{\\text{binds}} = \\{ p_1 \\}$, $\\mathcal{N}_{d_1}^{\\text{inhibits}} = \\{ p_2 \\}$, and $\\mathcal{N}_{d_1}^{\\text{interacts}} = \\{ d_2 \\}$,\nso that $c_{d_1,\\text{binds}} = c_{d_1,\\text{inhibits}} = c_{d_1,\\text{interacts}} = 1$.\n\nLet the relation-specific weight matrices and the self-loop matrix be\n$W_{\\text{binds}} = \\begin{pmatrix} 2  1 \\\\ 0  1 \\end{pmatrix}$, $W_{\\text{inhibits}} = \\begin{pmatrix} -1  0 \\\\ 0  2 \\end{pmatrix}$, $W_{\\text{interacts}} = \\begin{pmatrix} 1  -1 \\\\ 3  0 \\end{pmatrix}$, and $W_{0} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$.\n\nUsing your derived R-GCN update, compute the single-layer updated feature $h_{d_1}^{(1)}$ and report only its first coordinate as an exact real number. Do not round; no units are required in the final answer.", "solution": "The derivation begins with the core message passing principle for Graph Neural Networks (GNNs): node-wise updates at layer $l+1$ are constructed by aggregating messages from a node’s neighborhood at layer $l$ using a permutation-invariant operator. In a typed or relational graph, each edge carries a relation label $r \\in \\mathcal{R}$, so the neighborhood of node $i$ decomposes into typed neighborhoods $\\mathcal{N}_i^r = \\{ j \\in \\mathcal{V} \\mid (j \\xrightarrow{r} i) \\text{ is an edge} \\}$. To respect the distinct semantics of each relation, the aggregator should act per relation. The simplest choice consistent with linear representation of features is to use a relation-specific linear map $W_r$ on the neighbor feature $h_j^{(l)}$ before aggregation. To prevent scale blow-up as typed neighborhood sizes vary, introduce a normalization factor $c_{i,r}$ that depends on the size of $\\mathcal{N}_i^r$. A self-loop term allows a node to retain and transform its own representation via a matrix $W_0$. With an activation function $\\sigma$, the per-node update thus takes the form\n$$\nh_i^{(l+1)} \\;=\\; \\sigma\\!\\left( \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_i^r} \\frac{1}{c_{i,r}} \\, W_r \\, h_j^{(l)} \\;+\\; W_0 \\, h_i^{(l)} \\right).\n$$\nUnder the assumptions in the problem, $c_{i,r} = |\\mathcal{N}_i^r|$ and $\\sigma(x) = x$ (the identity). Therefore the update simplifies to\n$$\nh_i^{(l+1)} \\;=\\; \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_i^r} \\frac{1}{|\\mathcal{N}_i^r|} \\, W_r \\, h_j^{(l)} \\;+\\; W_0 \\, h_i^{(l)}.\n$$\n\nWe now apply this to node $d_1$ using the provided data. The typed neighborhoods are\n$\\mathcal{N}_{d_1}^{\\text{binds}} = \\{ p_1 \\}$,\n$\\mathcal{N}_{d_1}^{\\text{inhibits}} = \\{ p_2 \\}$,\n$\\mathcal{N}_{d_1}^{\\text{interacts}} = \\{ d_2 \\}$,\nand each has cardinality $1$, so each normalization factor is $1$.\n\nCompute each typed contribution:\n1. The binds contribution:\n$$\nW_{\\text{binds}} \\, h_{p_1}^{(0)} \\;=\\; \\begin{pmatrix} 2  1 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 2 \\cdot 0 + 1 \\cdot 1 \\\\ 0 \\cdot 0 + 1 \\cdot 1 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\n\n2. The inhibits contribution:\n$$\nW_{\\text{inhibits}} \\, h_{p_2}^{(0)} \\;=\\; \\begin{pmatrix} -1  0 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} -1 \\cdot 3 + 0 \\cdot (-1) \\\\ 0 \\cdot 3 + 2 \\cdot (-1) \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} -3 \\\\ -2 \\end{pmatrix}.\n$$\n\n3. The interacts contribution:\n$$\nW_{\\text{interacts}} \\, h_{d_2}^{(0)} \\;=\\; \\begin{pmatrix} 1  -1 \\\\ 3  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 1 \\cdot 2 + (-1) \\cdot 0 \\\\ 3 \\cdot 2 + 0 \\cdot 0 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 2 \\\\ 6 \\end{pmatrix}.\n$$\n\n4. The self-loop contribution:\n$$\nW_0 \\, h_{d_1}^{(0)} \\;=\\; \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}.\n$$\n\nAggregate all contributions to obtain $h_{d_1}^{(1)}$:\n$$\nh_{d_1}^{(1)} \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\;+\\; \\begin{pmatrix} -3 \\\\ -2 \\end{pmatrix}\n\\;+\\; \\begin{pmatrix} 2 \\\\ 6 \\end{pmatrix}\n\\;+\\; \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 1 - 3 + 2 + 1 \\\\ 1 - 2 + 6 + 2 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 1 \\\\ 7 \\end{pmatrix}.\n$$\n\nThe requested quantity is the first coordinate of $h_{d_1}^{(1)}$, which equals $1$.", "answer": "$$\\boxed{1}$$", "id": "4570117"}, {"introduction": "Designing a powerful model architecture is only half the battle; training it effectively on real-world data presents its own challenges. In Drug-Target Interaction (DTI) prediction, a significant hurdle is severe class imbalance, where the number of non-interacting pairs vastly outweighs the number of known interactions. This practice [@problem_id:4570205] moves from model architecture to the training objective, exploring how to select and derive appropriate loss functions. Starting with the standard binary cross-entropy, you will progress to advanced techniques like weighted cross-entropy and focal loss, which are specifically designed to focus learning on the rare positive class and harder-to-classify examples, a critical step for building a practically useful prediction model.", "problem": "Consider a bipartite Drug–Target Interaction (DTI) network in which a Graph Neural Network (GNN) produces, for each candidate drug–target pair (edge), a real-valued logit $z \\in \\mathbb{R}$ that is mapped to a Bernoulli parameter $p$ via the logistic sigmoid $p = \\sigma(z) = \\frac{1}{1 + \\exp(-z)}$. Assume conditional independence of observed binary edge labels given $p$ for a mini-batch of edges. The foundational base for this problem is the Bernoulli likelihood for binary outcomes and the Maximum Likelihood Estimation (MLE) principle, together with the chain rule from elementary calculus.\n\nStarting from the Bernoulli likelihood $p(y \\mid p) = p^{y} (1-p)^{1-y}$, and invoking the MLE principle and the negative log-likelihood construction, do the following:\n\n1. Derive the per-edge Binary Cross Entropy (BCE) loss $L_{\\mathrm{BCE}}(y, p)$ appropriate for DTI link prediction and obtain the gradient $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}$ with respect to the logit $z$.\n\n2. Derive a class-weighted BCE loss $L_{\\mathrm{wBCE}}(y, p)$ that uses distinct positive and negative class weights $w_{1}  0$ and $w_{0}  0$ to handle class imbalance. Obtain the gradient $\\frac{\\partial L_{\\mathrm{wBCE}}}{\\partial z}$.\n\n3. Derive a binary focal loss $L_{\\mathrm{focal}}(y, p)$ with focusing parameter $\\gamma \\ge 0$ and balance parameter $\\alpha \\in (0,1)$, suitable for DTI class imbalance, starting from the Bernoulli likelihood and explaining the role of modulating factors in attenuating easy examples. Obtain the gradient $\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z}$.\n\n4. For a mini-batch comprising $2$ edges with labels and logits $(y_{p}, z_{p}) = (1, 1.2)$ and $(y_{n}, z_{n}) = (0, -0.7)$, use the focal loss with $\\gamma = 2$ and $\\alpha = 0.25$. Compute the ratio\n$$\nR \\;=\\; \\frac{\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{p}} \\right|}{\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{n}} \\right|}.\n$$\nRound your final numerical value of $R$ to five significant figures. Express your final answer as a unitless scalar.", "solution": "The problem statement has been validated and is deemed sound, well-posed, and scientifically grounded in the principles of statistical machine learning. We may proceed with the derivations and computation.\n\nThe core relationship connecting the logit $z$ to the Bernoulli probability $p$ is the logistic sigmoid function, $p = \\sigma(z) = \\frac{1}{1 + \\exp(-z)}$. A crucial property of this function, which will be used repeatedly, is its derivative with respect to $z$:\n$$\n\\frac{dp}{dz} = \\frac{d}{dz} (1 + \\exp(-z))^{-1} = -1 \\cdot (1 + \\exp(-z))^{-2} \\cdot (-\\exp(-z)) = \\frac{\\exp(-z)}{(1 + \\exp(-z))^{2}}\n$$\nThis can be expressed elegantly in terms of $p$ itself:\n$$\n\\frac{dp}{dz} = \\frac{1}{1 + \\exp(-z)} \\cdot \\frac{\\exp(-z)}{1 + \\exp(-z)} = p \\cdot (1-p)\n$$\nAll subsequent gradient calculations will rely on the chain rule, $\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial p} \\frac{dp}{dz}$.\n\n**1. Binary Cross-Entropy (BCE) Loss**\n\nThe Maximum Likelihood Estimation (MLE) principle dictates that we should maximize the likelihood of the observed data. For a single binary observation $y \\in \\{0, 1\\}$, the Bernoulli likelihood is $P(y \\mid p) = p^y(1-p)^{1-y}$. It is computationally more convenient to maximize the log-likelihood:\n$$\n\\ell(p \\mid y) = \\ln(P(y \\mid p)) = y \\ln(p) + (1-y)\\ln(1-p)\n$$\nIn machine learning, optimization is typically framed as minimization of a loss function. The standard choice is the negative log-likelihood (NLL). The per-edge Binary Cross-Entropy loss is thus defined as the NLL of the Bernoulli distribution.\n$$\nL_{\\mathrm{BCE}}(y, p) = -\\ell(p \\mid y) = -[y \\ln(p) + (1-y)\\ln(1-p)]\n$$\nTo find the gradient with respect to the logit $z$, we apply the chain rule:\n$$\n\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = \\frac{\\partial L_{\\mathrm{BCE}}}{\\partial p} \\frac{dp}{dz}\n$$\nFirst, we compute the partial derivative with respect to $p$:\n$$\n\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial p} = - \\left[ y \\frac{1}{p} + (1-y) \\frac{-1}{1-p} \\right] = - \\left[ \\frac{y}{p} - \\frac{1-y}{1-p} \\right] = \\frac{1-y}{1-p} - \\frac{y}{p} = \\frac{p(1-y) - y(1-p)}{p(1-p)} = \\frac{p-y}{p(1-p)}\n$$\nNow, multiplying by the derivative of the sigmoid function:\n$$\n\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = \\left( \\frac{p-y}{p(1-p)} \\right) \\cdot (p(1-p)) = p - y\n$$\nSubstituting $p=\\sigma(z)$, the gradient is $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = \\sigma(z) - y$.\n\n**2. Class-Weighted Binary Cross-Entropy (wBCE) Loss**\n\nTo address class imbalance, the standard BCE loss can be modified by introducing weights for the positive class ($y=1$) and the negative class ($y=0$), denoted by $w_1  0$ and $w_0  0$ respectively. The loss function becomes:\n$$\nL_{\\mathrm{wBCE}}(y, p) = -[w_1 y \\ln(p) + w_0 (1-y)\\ln(1-p)]\n$$\nWe again use the chain rule to find the gradient with respect to $z$. The partial derivative with respect to $p$ is:\n$$\n\\frac{\\partial L_{\\mathrm{wBCE}}}{\\partial p} = - \\left[ w_1 y \\frac{1}{p} + w_0 (1-y) \\frac{-1}{1-p} \\right] = \\frac{w_0(1-y)}{1-p} - \\frac{w_1 y}{p}\n$$\nMultiplying by $\\frac{dp}{dz} = p(1-p)$:\n$$\n\\frac{\\partial L_{\\mathrm{wBCE}}}{\\partial z} = \\left( \\frac{w_0(1-y)}{1-p} - \\frac{w_1 y}{p} \\right) \\cdot p(1-p) = w_0(1-y)p - w_1 y(1-p)\n$$\nThis gradient can be expressed in terms of $z$ as $\\frac{\\partial L_{\\mathrm{wBCE}}}{\\partial z} = w_0(1-y)\\sigma(z) - w_1 y(1-\\sigma(z))$.\n\n**3. Binary Focal Loss**\n\nThe binary focal loss is an advancement over weighted BCE that addresses class imbalance by dynamically scaling the cross-entropy loss based on the model's confidence. It introduces a modulating factor that down-weights the loss contribution from \"easy\" examples (those classified with high confidence), thereby focusing the training on \"hard\" misclassified examples.\n\nLet $p_t$ be a shorthand for the model's estimated probability for the ground-truth class:\n$$\np_t = \\begin{cases} p  \\text{if } y=1 \\\\ 1-p  \\text{if } y=0 \\end{cases}\n$$\nThe standard cross-entropy loss is $-\\ln(p_t)$. The focal loss introduces two components: a static balancing parameter $\\alpha \\in (0,1)$ (similar to $w_0, w_1$) and a dynamic modulating factor $(1-p_t)^{\\gamma}$ with focusing parameter $\\gamma \\ge 0$. The loss is defined as:\n$$\nL_{\\mathrm{focal}}(y, p) = -y \\alpha (1-p)^\\gamma \\ln(p) - (1-y)(1-\\alpha) p^\\gamma \\ln(1-p)\n$$\nThe term $(1-p)^\\gamma$ for a positive example ($y=1$) approaches $0$ as $p \\to 1$ (an easy positive), reducing the loss. Similarly, for a negative example ($y=0$), the term $p^\\gamma$ approaches $0$ as $p \\to 0$ (an easy negative), also reducing the loss. When $\\gamma=0$, the focal loss reduces to the $\\alpha$-balanced cross-entropy loss.\n\nTo find the gradient $\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z}$, we again use the chain rule by differentiating with respect to $p$ and multiplying by $p(1-p)$. We derive the gradient for each case, $y=1$ and $y=0$.\n\nFor $y=1$: $L(p) = -\\alpha(1-p)^\\gamma \\ln(p)$.\n$$\n\\frac{\\partial L}{\\partial p} = -\\alpha \\left[ \\frac{d}{dp}((1-p)^\\gamma) \\ln(p) + (1-p)^\\gamma \\frac{d}{dp}(\\ln p) \\right] = -\\alpha \\left[ -\\gamma(1-p)^{\\gamma-1} \\ln(p) + \\frac{(1-p)^\\gamma}{p} \\right]\n$$\nMultiplying by $\\frac{dp}{dz} = p(1-p)$:\n$$\n\\frac{\\partial L}{\\partial z} = -\\alpha \\left[ -\\gamma(1-p)^{\\gamma-1} \\ln(p) + \\frac{(1-p)^\\gamma}{p} \\right] p(1-p) = -\\alpha \\left[ -\\gamma p(1-p)^\\gamma \\ln(p) + (1-p)^{\\gamma+1} \\right]\n$$\n$$\n\\implies \\frac{\\partial L}{\\partial z} = \\alpha(1-p)^\\gamma (\\gamma p \\ln(p) - (1-p)) = \\alpha(1-p)^\\gamma (\\gamma p \\ln(p) + p-1)\n$$\nFor $y=0$: $L(p) = -(1-\\alpha)p^\\gamma \\ln(1-p)$.\n$$\n\\frac{\\partial L}{\\partial p} = -(1-\\alpha) \\left[ \\gamma p^{\\gamma-1} \\ln(1-p) + p^\\gamma \\frac{-1}{1-p} \\right]\n$$\nMultiplying by $\\frac{dp}{dz} = p(1-p)$:\n$$\n\\frac{\\partial L}{\\partial z} = -(1-\\alpha) \\left[ \\gamma p^{\\gamma-1} \\ln(1-p) - \\frac{p^\\gamma}{1-p} \\right] p(1-p) = -(1-\\alpha) \\left[ \\gamma p^\\gamma(1-p) \\ln(1-p) - p^{\\gamma+1} \\right]\n$$\n$$\n\\implies \\frac{\\partial L}{\\partial z} = (1-\\alpha)p^\\gamma (p - \\gamma(1-p)\\ln(1-p))\n$$\nCombining these results gives the full gradient expression:\n$$\n\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z} = y\\alpha(1-p)^\\gamma(p-1+\\gamma p\\ln p) + (1-y)(1-\\alpha)p^\\gamma(p-\\gamma(1-p)\\ln(1-p))\n$$\n\n**4. Numerical Computation**\n\nWe are given a positive example $(y_p, z_p) = (1, 1.2)$ and a negative example $(y_n, z_n) = (0, -0.7)$, with parameters $\\gamma=2$ and $\\alpha=0.25$. We need to compute $R = \\frac{\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{p}} \\right|}{\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{n}} \\right|}$.\n\nFirst, we compute the probabilities:\n$p_p = \\sigma(z_p) = \\sigma(1.2) = \\frac{1}{1 + \\exp(-1.2)} \\approx 0.76852479$\n$p_n = \\sigma(z_n) = \\sigma(-0.7) = \\frac{1}{1 + \\exp(0.7)} \\approx 0.33181222$\n\nNext, we compute the gradient for the positive example ($y_p=1$):\n$$\n\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{p}} = \\alpha(1-p_p)^\\gamma(\\gamma p_p \\ln p_p + p_p - 1)\n$$\nSubstituting values:\n$1-p_p \\approx 1 - 0.76852479 = 0.23147521$\n$\\ln(p_p) \\approx \\ln(0.76852479) = -0.26328242$\n$$\n\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{p}} \\approx 0.25 \\cdot (0.23147521)^2 \\cdot (2 \\cdot 0.76852479 \\cdot (-0.26328242) + 0.76852479 - 1)\n$$\n$$\n\\approx 0.25 \\cdot (0.05358087) \\cdot (-0.40467566 - 0.23147521) = 0.01339522 \\cdot (-0.63615087) \\approx -0.00852033\n$$\nSo, $\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{p}} \\right| \\approx 0.00852033$.\n\nNext, we compute the gradient for the negative example ($y_n=0$):\n$$\n\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{n}} = (1-\\alpha)p_n^\\gamma(p_n - \\gamma(1-p_n)\\ln(1-p_n))\n$$\nSubstituting values:\n$1-\\alpha = 0.75$\n$1-p_n \\approx 1 - 0.33181222 = 0.66818778$\n$\\ln(1-p_n) \\approx \\ln(0.66818778) = -0.40318043$\n$$\n\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{n}} \\approx 0.75 \\cdot (0.33181222)^2 \\cdot (0.33181222 - 2 \\cdot (0.66818778) \\cdot (-0.40318043))\n$$\n$$\n\\approx 0.75 \\cdot (0.11009972) \\cdot (0.33181222 + 0.53880447) = 0.08257479 \\cdot (0.87061669) \\approx 0.0718797\n$$\nSo, $\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{n}} \\right| \\approx 0.0718797$.\n\nFinally, we compute the ratio $R$:\n$$\nR = \\frac{\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{p}} \\right|}{\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{n}} \\right|} \\approx \\frac{0.00852033}{0.0718797} \\approx 0.1185361\n$$\nRounding to five significant figures, we get $R \\approx 0.11854$. The small value of $R$ confirms that the gradient magnitude for the well-classified positive example ($p_p \\approx 0.77$) is significantly smaller than for the more ambiguous negative example ($p_n \\approx 0.33$), demonstrating the effect of the focal loss.", "answer": "$$\\boxed{0.11854}$$", "id": "4570205"}]}