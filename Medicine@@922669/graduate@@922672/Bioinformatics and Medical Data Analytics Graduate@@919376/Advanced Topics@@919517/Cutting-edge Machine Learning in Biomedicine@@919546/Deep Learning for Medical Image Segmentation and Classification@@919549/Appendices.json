{"hands_on_practices": [{"introduction": "While designing sophisticated deep learning models is intellectually stimulating, a successful implementation hinges on navigating practical hardware constraints. This exercise delves into one of the most critical aspects of training deep networks for medical imaging: estimating and managing GPU memory consumption. By systematically accounting for model parameters, optimizer states, and the activation maps stored for backpropagation, you will develop the essential skill of predicting a model's memory footprint and making informed decisions to ensure it can be trained effectively.", "problem": "Consider a three-dimensional (3D) U-Net architecture for volumetric medical image segmentation, trained on a Graphics Processing Unit (GPU) with a total memory capacity of $12$ gigabytes (GB), where $1 \\, \\mathrm{GB} = 10^{9} \\, \\mathrm{bytes}$. The input patch has spatial dimensions $(128, 128, 128)$ voxels and a single input imaging modality channel. The U-Net uses the following structural assumptions:\n- The encoder consists of $4$ resolution levels followed by a bottleneck level, each encoder level having two $3 \\times 3 \\times 3$ convolutions with \"same\" padding and channels doubling at each downsampling: base channels $32$ at resolution level with spatial size $128^3$, then $64$ at $64^3$, $128$ at $32^3$, $256$ at $16^3$, and bottleneck $512$ at $8^3$.\n- Downsampling is by $2 \\times 2 \\times 2$ max pooling.\n- The decoder mirrors the encoder: each decoding stage first upsamples using a $2 \\times 2 \\times 2$ transposed convolution (stride $2$) to match the next higher spatial resolution, concatenates with the corresponding encoder feature map, and applies two $3 \\times 3 \\times 3$ convolutions. After concatenation, the two convolutions output the number of channels of that decoder stage ($256$ at $16^3$, $128$ at $32^3$, $64$ at $64^3$, $32$ at $128^3$).\n- The final output layer is a $1 \\times 1 \\times 1$ convolution mapping $32$ channels at $128^3$ to $K = 3$ segmentation classes.\n\nAssume training with Adaptive Moment Estimation (Adam) optimizer in single precision (float32). For memory accounting:\n- Each float32 tensor element occupies $4$ bytes.\n- During training, the output activations of every parameterized layer (all convolutions and transposed convolutions) are retained for backpropagation. Ignore the retained memory for pooling, concatenation, and any non-parameterized operations.\n- Count parameter tensors, their gradients, and both Adam first and second moment buffers, all in float32 and all resident on the GPU, so the per-parameter memory is $16$ bytes.\n- A fixed non-model overhead of $1 \\, \\mathrm{GB}$ is reserved for runtime, leaving the remainder of the $12 \\, \\mathrm{GB}$ budget for model parameters, optimizer states, and retained activations.\n\nLet the batch size be $B$. Using only the above assumptions and standard definitions of convolutional parameter counts and activation sizes, derive the total retained activation memory as a function of $B$, the total parameter plus optimizer memory, and determine the largest integer batch size $B$ that fits within the available GPU memory budget.\n\nAdditionally, justify at least two principled strategies to reduce memory consumption (for example, gradient checkpointing or mixed precision) and, for each, explicitly state how it asymptotically modifies the activation-memory term derived in this problem.\n\nExpress your final answer as the maximum integer $B$ that fits, with no units and no rounding instruction needed.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of deep learning and computer architecture, well-posed with sufficient and consistent information, and articulated in objective, formal language. The provided parameters describe a realistic scenario for training a 3D U-Net for medical image segmentation. We may therefore proceed with a formal solution.\n\nThe total available memory for the model and its training-related states is the total GPU memory minus the fixed overhead. Let $M_{GPU} = 12 \\, \\mathrm{GB} = 12 \\times 10^9 \\, \\mathrm{bytes}$ and the overhead $M_{OH} = 1 \\, \\mathrm{GB} = 1 \\times 10^9 \\, \\mathrm{bytes}$.\nThe available memory is $M_{avail} = M_{GPU} - M_{OH} = 11 \\times 10^9 \\, \\mathrm{bytes}$.\nThe total memory consumption during training comprises two main components:\n$1$. The memory for model parameters and optimizer states, which is independent of batch size. Let this be $M_{P}$.\n$2$. The memory for retained layer activations, which scales linearly with the batch size $B$. Let this be $M_{A}(B)$.\n\nThe constraint is $M_{P} + M_{A}(B) \\le M_{avail}$.\n\nWe will first calculate $M_{P}$, then derive the expression for $M_{A}(B)$, and finally solve for the maximum integer $B$.\n\n**1. Calculation of Parameter and Optimizer Memory ($M_{P}$)**\n\nThe memory cost per parameter is given as $16$ bytes, accounting for the parameter itself (float32, $4$ bytes), its gradient (float32, $4$ bytes), and the Adam optimizer's first and second moment estimates ( $2 \\times 4 = 8$ bytes).\nThe number of parameters for a convolutional layer with a kernel of size $k \\times k \\times k$, $C_{in}$ input channels, and $C_{out}$ output channels, including a bias term for each output channel, is given by $N_{params} = (k^3 \\cdot C_{in} + 1) \\cdot C_{out}$. For a transposed convolution, the formula is identical. We calculate the parameters for each layer.\n\nEncoder Path:\n- Level 1 ($128^3$): Two $3 \\times 3 \\times 3$ convolutions.\n  - Conv 1: $C_{in}=1$, $C_{out}=32$. $N_1 = (3^3 \\cdot 1 + 1) \\cdot 32 = 896$.\n  - Conv 2: $C_{in}=32$, $C_{out}=32$. $N_2 = (3^3 \\cdot 32 + 1) \\cdot 32 = 27,680$.\n- Level 2 ($64^3$): Two $3 \\times 3 \\times 3$ convolutions.\n  - Conv 1: $C_{in}=32$, $C_{out}=64$. $N_3 = (3^3 \\cdot 32 + 1) \\cdot 64 = 55,360$.\n  - Conv 2: $C_{in}=64$, $C_{out}=64$. $N_4 = (3^3 \\cdot 64 + 1) \\cdot 64 = 110,656$.\n- Level 3 ($32^3$): Two $3 \\times 3 \\times 3$ convolutions.\n  - Conv 1: $C_{in}=64$, $C_{out}=128$. $N_5 = (3^3 \\cdot 64 + 1) \\cdot 128 = 221,312$.\n  - Conv 2: $C_{in}=128$, $C_{out}=128$. $N_6 = (3^3 \\cdot 128 + 1) \\cdot 128 = 442,496$.\n- Level 4 ($16^3$): Two $3 \\times 3 \\times 3$ convolutions.\n  - Conv 1: $C_{in}=128$, $C_{out}=256$. $N_7 = (3^3 \\cdot 128 + 1) \\cdot 256 = 884,992$.\n  - Conv 2: $C_{in}=256$, $C_{out}=256$. $N_8 = (3^3 \\cdot 256 + 1) \\cdot 256 = 1,769,728$.\n- Bottleneck ($8^3$): Two $3 \\times 3 \\times 3$ convolutions.\n  - Conv 1: $C_{in}=256$, $C_{out}=512$. $N_9 = (3^3 \\cdot 256 + 1) \\cdot 512 = 3,539,456$.\n  - Conv 2: $C_{in}=512$, $C_{out}=512$. $N_{10} = (3^3 \\cdot 512 + 1) \\cdot 512 = 7,078,400$.\n\nDecoder Path:\n- Level 1 ($16^3$): One transposed conv, two regular convs.\n  - Transposed Conv: $C_{in}=512$, $C_{out}=256$ (standard U-Net practice to halve channels), kernel $2^3$. $N_{11} = (2^3 \\cdot 512 + 1) \\cdot 256 = 1,048,832$.\n  - Conv 1: Input is concatenation of upsampled map ($256$ ch) and encoder map ($256$ ch), so $C_{in}=512$. $C_{out}=256$. $N_{12} = (3^3 \\cdot 512 + 1) \\cdot 256 = 3,539,200$.\n  - Conv 2: $C_{in}=256$, $C_{out}=256$. $N_{13} = (3^3 \\cdot 256 + 1) \\cdot 256 = 1,769,728$.\n- Level 2 ($32^3$):\n  - Transposed Conv: $C_{in}=256$, $C_{out}=128$. $N_{14} = (2^3 \\cdot 256 + 1) \\cdot 128 = 262,272$.\n  - Conv 1: $C_{in}=128+128=256$, $C_{out}=128$. $N_{15} = (3^3 \\cdot 256 + 1) \\cdot 128 = 884,864$.\n  - Conv 2: $C_{in}=128$, $C_{out}=128$. $N_{16} = (3^3 \\cdot 128 + 1) \\cdot 128 = 442,496$.\n- Level 3 ($64^3$):\n  - Transposed Conv: $C_{in}=128$, $C_{out}=64$. $N_{17} = (2^3 \\cdot 128 + 1) \\cdot 64 = 65,600$.\n  - Conv 1: $C_{in}=64+64=128$, $C_{out}=64$. $N_{18} = (3^3 \\cdot 128 + 1) \\cdot 64 = 221,248$.\n  - Conv 2: $C_{in}=64$, $C_{out}=64$. $N_{19} = (3^3 \\cdot 64 + 1) \\cdot 64 = 110,656$.\n- Level 4 ($128^3$):\n  - Transposed Conv: $C_{in}=64$, $C_{out}=32$. $N_{20} = (2^3 \\cdot 64 + 1) \\cdot 32 = 16,416$.\n  - Conv 1: $C_{in}=32+32=64$, $C_{out}=32$. $N_{21} = (3^3 \\cdot 64 + 1) \\cdot 32 = 55,328$.\n  - Conv 2: $C_{in}=32$, $C_{out}=32$. $N_{22} = (3^3 \\cdot 32 + 1) \\cdot 32 = 27,680$.\n\nFinal Layer:\n- $1 \\times 1 \\times 1$ convolution: $C_{in}=32$, $C_{out}=3$. $N_{23} = (1^3 \\cdot 32 + 1) \\cdot 3 = 99$.\n\nTotal parameters $N_{P} = \\sum_{i=1}^{23} N_i = 22,575,299$.\nThe total memory for parameters and optimizer states is:\n$M_{P} = N_{P} \\times 16 \\, \\mathrm{bytes/param} = 22,575,299 \\times 16 = 361,204,784 \\, \\mathrm{bytes}$.\n\n**2. Calculation of Activation Memory ($M_{A}(B)$)**\n\nActivation memory is the sum of memory for the output tensors of all parameterized layers. Each float32 element occupies $4$ bytes. The size of an activation tensor is Batch $\\times$ Depth $\\times$ Height $\\times$ Width $\\times$ Channels. Let $V_d = d^3$ be the volume for a spatial dimension $d$. We sum the elements per sample across all layers.\n\n- At spatial size $128^3$:\n  - Encoder: 2 convs $\\times$ $32$ channels $\\implies 2 \\times V_{128} \\times 32$.\n  - Decoder: 1 T-conv $\\times$ $32$ ch, 2 convs $\\times$ $32$ ch $\\implies 3 \\times V_{128} \\times 32$.\n  - Final Layer: 1 conv $\\times$ $3$ channels $\\implies 1 \\times V_{128} \\times 3$.\n  - Total at $128^3$: $V_{128} \\times (2 \\cdot 32 + 3 \\cdot 32 + 3) = 2,097,152 \\times 163 = 341,835,776$ elements.\n\n- At spatial size $64^3$:\n  - Encoder: 2 convs $\\times$ $64$ channels $\\implies 2 \\times V_{64} \\times 64$.\n  - Decoder: 1 T-conv $\\times$ $64$ ch, 2 convs $\\times$ $64$ ch $\\implies 3 \\times V_{64} \\times 64$.\n  - Total at $64^3$: $V_{64} \\times (2 \\cdot 64 + 3 \\cdot 64) = 262,144 \\times 320 = 83,886,080$ elements.\n\n- At spatial size $32^3$:\n  - Encoder: 2 convs $\\times$ $128$ channels $\\implies 2 \\times V_{32} \\times 128$.\n  - Decoder: 1 T-conv $\\times$ $128$ ch, 2 convs $\\times$ $128$ ch $\\implies 3 \\times V_{32} \\times 128$.\n  - Total at $32^3$: $V_{32} \\times (2 \\cdot 128 + 3 \\cdot 128) = 32,768 \\times 640 = 20,971,520$ elements.\n\n- At spatial size $16^3$:\n  - Encoder: 2 convs $\\times$ $256$ channels $\\implies 2 \\times V_{16} \\times 256$.\n  - Decoder: 1 T-conv $\\times$ $256$ ch, 2 convs $\\times$ $256$ ch $\\implies 3 \\times V_{16} \\times 256$.\n  - Total at $16^3$: $V_{16} \\times (2 \\cdot 256 + 3 \\cdot 256) = 4,096 \\times 1280 = 5,242,880$ elements.\n\n- At spatial size $8^3$:\n  - Bottleneck: 2 convs $\\times$ $512$ channels $\\implies 2 \\times V_8 \\times 512$.\n  - Total at $8^3$: $V_8 \\times (2 \\cdot 512) = 512 \\times 1024 = 524,288$ elements.\n\nThe total number of activation elements stored per sample is the sum of these values:\n$E_{sample} = 341,835,776 + 83,886,080 + 20,971,520 + 5,242,880 + 524,288 = 452,460,544$.\n\nThe total activation memory for a batch of size $B$ is:\n$M_{A}(B) = B \\times E_{sample} \\times 4 \\, \\mathrm{bytes/element} = B \\times 452,460,544 \\times 4 = B \\times 1,809,842,176 \\, \\mathrm{bytes}$.\n\n**3. Determining the Maximum Batch Size ($B$)**\n\nWe use the memory constraint:\n$M_{P} + M_{A}(B) \\le M_{avail}$\n$361,204,784 + B \\times 1,809,842,176 \\le 11,000,000,000$\n$B \\times 1,809,842,176 \\le 11,000,000,000 - 361,204,784$\n$B \\times 1,809,842,176 \\le 10,638,795,216$\n$B \\le \\frac{10,638,795,216}{1,809,842,176} \\approx 5.8782$\n\nSince the batch size $B$ must be an integer, the largest possible value is $B=5$.\n\n**4. Strategies for Memory Reduction**\n\nTwo principled strategies to reduce memory consumption are gradient checkpointing and mixed-precision training.\n\n- **Gradient Checkpointing (Activation Recomputation)**: This technique reduces activation memory by not storing all intermediate activations during the forward pass. Instead, it stores only a subset of activations at designated \"checkpoint\" layers. During the backward pass, the activations for layers between checkpoints are recomputed on-the-fly, starting from the nearest checkpoint. This trades increased computation time for a significant reduction in memory.\n  - **Effect on Activation Memory**: The original activation memory term is $M_{A}(B) = B \\cdot C_{act}$, where $C_{act} = (\\sum_{i=1}^{L} |A_i|) \\cdot 4$ and $|A_i|$ is the number of elements in the activation tensor of layer $i$. With checkpointing, the new memory term becomes approximately $M'_{A}(B) = B \\cdot (C_{ckpt} + C_{seg}) \\cdot 4$, where $C_{ckpt}$ is the memory for the checkpointed activations themselves and $C_{seg}$ is the memory for the activations within the computationally heaviest segment between two checkpoints. If the network is divided into $k$ segments, the activation memory is roughly reduced by a factor proportional to $k$. Thus, instead of scaling with the total depth $L$ of the network, the activation memory scales with the depth of the largest segment, $\\max_k L_k$, plus the small overhead of the checkpoints themselves. This allows for a much larger batch size $B$.\n\n- **Mixed-Precision Training**: This involves using lower-precision numerical formats, typically 16-bit floating-point (FP16), for storing activations, gradients, and model weights, while maintaining a master copy of the weights in 32-bit floating-point (FP32) for stable weight updates.\n  - **Effect on Activation Memory**: This strategy directly modifies the memory cost per activation element. The original activation memory term is $M_{A}(B) = B \\cdot E_{sample} \\cdot 4$. By switching from FP32 ($4$ bytes) to FP16 ($2$ bytes) for activations, the per-element cost is halved. The new activation memory term becomes $M'_{A}(B) = B \\cdot E_{sample} \\cdot 2$. This represents a direct asymptotic modification: $M'_{A}(B) = \\frac{1}{2} M_{A}(B)$. This reduction by a factor of $2$ would theoretically allow for a batch size almost twice as large, assuming activation memory is the primary bottleneck after accounting for parameter and optimizer memory.", "answer": "$$\n\\boxed{5}\n$$", "id": "4554578"}, {"introduction": "A trained segmentation model is only useful if it can be applied to full-sized clinical scans, which are often far too large to fit into GPU memory. This practice guides you through the design and analysis of sliding-window inference, a fundamental technique for processing large volumes patch by patch. You will derive a robust procedure for placing overlapping windows to guarantee full coverage and then investigate different strategies for blending the resulting predictions, a critical step for mitigating artifacts and achieving seamless, high-quality segmentations.", "problem": "A three-dimensional Convolutional Neural Network (CNN) performing medical image segmentation often uses sliding-window inference to process a large volumetric input that exceeds the model’s input patch size. Consider an input volume of size $\\left(V_x,V_y,V_z\\right)$ and a cubic or rectangular patch size $\\left(p_x,p_y,p_z\\right)$. A fractional overlap $\\omega \\in [0,1)$ is specified along each axis. Sliding-window inference places patch windows on a regular grid such that every voxel in the volume is covered by at least one window, and windows overlap according to $\\omega$. The stride along axis $d \\in \\{x,y,z\\}$ is defined by the intent that successive windows overlap by fraction $\\omega$, which implies a nominal stride of $p_d \\left(1-\\omega\\right)$. Because window positions are discrete voxel indices, the actual stride along axis $d$ must be a positive integer, denoted $s_d$, that respects coverage.\n\nStarting from the core definition of sliding-window coverage in one dimension:\n- A window of length $p$ at start index $a$ covers all integer voxel indices $i$ with $a \\le i < a+p$.\n- A sequence of window start indices $\\{a_k\\}$ must be chosen such that the union of covered indices equals $\\{0,1,2,\\dots,V-1\\}$.\n- There is a final window anchored at the end if needed, at start index $V-p$ when $V>p$.\n\nFrom these definitions, derive:\n1. A rule to convert the fractional overlap $\\omega$ and patch length $p_d$ to an integer stride $s_d$ satisfying $s_d \\ge 1$ and consistent with overlap intent.\n2. A constructive procedure to generate one-dimensional start indices along axis $d$ that guarantees full coverage and uses $s_d$ for spacing until the last window, which is anchored at $V_d-p_d$ if the next stride would miss the end.\n3. The formula for the number of windows $n_d$ along axis $d$ implied by this construction.\n4. The total number of windows $N = n_x \\, n_y \\, n_z$ in three dimensions.\n\nLet $c_d(i)$ be the one-dimensional coverage multiplicity function giving the number of windows along axis $d$ that cover index $i \\in \\{0,1,\\dots,V_d-1\\}$. In three dimensions, the coverage multiplicity at voxel $\\left(i,j,k\\right)$ factorizes as $m(i,j,k) = c_x(i)\\,c_y(j)\\,c_z(k)$ by construction of the Cartesian grid. Using this, compute:\n- The distribution of $c_d$ values along each axis and its empirical mean $\\mu_d = \\frac{1}{V_d} \\sum_{i=0}^{V_d-1} c_d(i)$.\n- The mean three-dimensional multiplicity $\\bar{m} = \\frac{1}{V_x V_y V_z} \\sum_{i,j,k} m(i,j,k)$ using independence of axes.\n- The maximum three-dimensional multiplicity $m_{\\max} = \\max_{i,j,k} m(i,j,k)$.\n- The fraction of voxels with multi-coverage, defined as those with $m(i,j,k) > 1$.\n\nTo assess stitching artifacts, consider three blending strategies to combine predictions from overlapping windows:\n- Hard stitching: choose one window’s prediction at each voxel and discard others. At a voxel with multiplicity $m$, the per-window weight vector is $(1,0,0,\\dots,0)$ up to permutation. Let the variance across windows at a voxel be $\\sigma^2 = \\frac{1}{m} \\sum_{\\ell=1}^{m} \\left(w_\\ell - \\frac{1}{m}\\right)^2$, where $w_\\ell$ are normalized weights that sum to $1$. Derive $\\sigma^2$ for hard stitching as a function of $m$, and compute the expected variance over the volume by summing over the multiplicity distribution.\n- Uniform averaging: assign equal weights to all overlapping windows at a voxel, i.e., $w_\\ell = \\frac{1}{m}$ for all $\\ell$. Derive the variance $\\sigma^2$ at any voxel and the expected variance over the volume.\n- Raised-cosine (Hann) taper: define a one-dimensional raw weight on a window of length $p$ by $h(u) = \\frac{1}{2}\\left(1-\\cos\\left(2\\pi u\\right)\\right)$ for $u \\in [0,1]$, where $u$ is the normalized position within the patch. For two windows overlapping along an axis by length $L = p - s$ with starts at $0$ and $s$, define raw weights $w_1(x) = h\\left(\\frac{x}{p-1}\\right)$ and $w_2(x) = h\\left(\\frac{x-s}{p-1}\\right)$ for integer $x \\in [s, p-1]$. Normalize at each $x$ by $w'_i(x) = \\frac{w_i(x)}{w_1(x)+w_2(x)}$, and compute the one-dimensional pairwise variance $\\sigma^2_{\\text{pair}} = \\frac{1}{L} \\sum_{x=s}^{p-1} \\frac{1}{2}\\sum_{i=1}^2 \\left(w'_i(x) - \\frac{1}{2}\\right)^2$. Use axis independence to approximate the expected variance over the three-dimensional volume by weighting $\\sigma^2_{\\text{pair}}$ with the fraction of voxels experiencing exactly pairwise overlap along a single axis and single coverage along the other two axes. Explain this approximation and its limitations.\n\nTest Suite. Implement the above for the following parameter sets, ensuring integer voxel indexing and the derived stride and coverage construction:\n- Case $1$: $\\left(V_x,V_y,V_z\\right)=\\left(256,256,128\\right)$, $\\left(p_x,p_y,p_z\\right)=\\left(96,96,96\\right)$, $\\omega=0.5$.\n- Case $2$: $\\left(V_x,V_y,V_z\\right)=\\left(256,256,128\\right)$, $\\left(p_x,p_y,p_z\\right)=\\left(256,256,128\\right)$, $\\omega=0.0$.\n- Case $3$: $\\left(V_x,V_y,V_z\\right)=\\left(200,200,60\\right)$, $\\left(p_x,p_y,p_z\\right)=\\left(64,64,64\\right)$, $\\omega=0.6$.\n- Case $4$: $\\left(V_x,V_y,V_z\\right)=\\left(129,129,65\\right)$, $\\left(p_x,p_y,p_z\\right)=\\left(64,64,64\\right)$, $\\omega=0.75$.\n- Case $5$: $\\left(V_x,V_y,V_z\\right)=\\left(64,64,32\\right)$, $\\left(p_x,p_y,p_z\\right)=\\left(96,96,96\\right)$, $\\omega=0.5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case’s result is a list\n$\\left[N,\\,[n_x,n_y,n_z],\\,\\bar{m},\\,m_{\\max},\\,f_{\\text{multi}},\\,\\mathbb{E}\\left[\\sigma^2_{\\text{hard}}\\right],\\,\\mathbb{E}\\left[\\sigma^2_{\\text{uniform}}\\right],\\,\\widehat{\\mathbb{E}}\\left[\\sigma^2_{\\text{cosine}}\\right]\\right]$,\nwith all real-valued quantities represented as decimal floats. For example, the final line should look like\n$\\left[\\text{case1},\\text{case2},\\dots\\right]$ with each $\\text{case}$ replaced by its list of computed values.", "solution": "The problem requires the derivation and implementation of a comprehensive model for sliding-window inference in three dimensions, including window placement strategy, coverage multiplicity analysis, and a comparison of blending variances for different stitching methods. The solution is presented in three parts, following the structure of the problem statement.\n\n### Part 1: Stride and Window Placement\n\nThe core of sliding-window inference is a deterministic procedure for placing patches (windows) to ensure every voxel of an input volume is processed. This procedure is defined by the patch size $p_d$, the volume dimension $V_d$, and a desired fractional overlap $\\omega$ for each axis $d \\in \\{x,y,z\\}$.\n\n**1. Integer Stride Rule ($s_d$)**\n\nThe nominal, or ideal, stride along an axis $d$ to achieve a fractional overlap $\\omega$ is $s_{nom, d} = p_d (1 - \\omega)$. Since window positions are on a discrete voxel grid, the actual stride $s_d$ must be an integer. To ensure that the actual overlap is at least the desired fraction $\\omega$, the stride should not be larger than the nominal stride. A smaller stride increases overlap, while a larger stride decreases it. A conservative and common choice is to take the floor of the nominal stride. Furthermore, the stride must be a positive integer. This leads to the derived rule for the integer stride $s_d$:\n$$\ns_d = \\max(1, \\lfloor p_d (1 - \\omega) \\rfloor)\n$$\nThis rule guarantees $s_d \\ge 1$ and an overlap fraction greater than or equal to $\\omega$.\n\n**2. Constructive Procedure for Window Start Indices**\n\nTo guarantee full coverage of the volume dimension $V_d$, windows are placed starting at index $0$ and advancing by stride $s_d$. If this regular placement leaves voxels at the end uncovered, a final window is anchored at index $V_d - p_d$. This leads to the following constructive procedure for generating the one-dimensional start indices $\\{a_k\\}$:\n\n1.  If the patch size is greater than or equal to the volume dimension ($p_d \\ge V_d$), a single window at index $0$ is sufficient. The set of starts is $\\{0\\}$.\n2.  If $p_d < V_d$, generate a sequence of starts beginning at $0$ with step size $s_d$: $\\{0, s_d, 2s_d, \\dots, k \\cdot s_d\\}$ as long as $k \\cdot s_d \\le V_d - p_d$.\n3.  If the last generated start index is not $V_d - p_d$, add $V_d - p_d$ to the set of start indices. This ensures the final voxels are covered.\n\nThis procedure generates a unique, sorted list of start indices that guarantees full coverage.\n\n**3. Formula for the Number of Windows ($n_d$)**\n\nThe number of windows, $n_d$, is the count of start indices generated by the above procedure. A closed-form formula can be derived.\n- If $V_d \\le p_d$, then $n_d = 1$.\n- If $V_d > p_d$, the starts are generated by striding across the range of possible start positions, which spans $V_d - p_d$. The number of strides of size $s_d$ needed to cover this range is $\\lceil \\frac{V_d - p_d}{s_d} \\rceil$. Including the initial window at index $0$, this gives a formula for the total number of windows:\n$$\nn_d = \\begin{cases} 1 & \\text{if } V_d \\le p_d \\\\ \\left\\lceil \\frac{V_d - p_d}{s_d} \\right\\rceil + 1 & \\text{if } V_d > p_d \\end{cases}\n$$\n\n**4. Total Number of Windows ($N$)**\n\nThe placement of windows is performed independently along each axis, forming a Cartesian grid of patch locations. The total number of windows $N$ is the product of the number of windows along each axis:\n$$\nN = n_x n_y n_z\n$$\n\n### Part 2: Coverage Multiplicity Analysis\n\nCoverage multiplicity, $m(i,j,k)$, is the number of windows that cover a given voxel $(i,j,k)$. Due to the Cartesian grid construction, it factorizes: $m(i,j,k) = c_x(i)c_y(j)c_z(k)$, where $c_d(i)$ is the 1D multiplicity.\n\n**1. 1D Multiplicity Distribution and Mean**\n\nThe 1D multiplicity function $c_d(i)$ for $i \\in \\{0, 1, \\dots, V_d-1\\}$ is given by $c_d(i) = \\sum_k \\mathbb{I}(a_k \\le i < a_k + p_d)$, where $\\{a_k\\}$ are the start indices. This function can be computed efficiently by creating a differential array, incrementing at each $a_k$ and decrementing at each $a_k+p_d$, and then taking the prefix sum. From the computed $c_d(i)$ array, its distribution (a histogram of values) can be determined.\n\nThe mean 1D multiplicity, $\\mu_d$, is the average of $c_d(i)$ over the volume.\n$$\n\\mu_d = \\frac{1}{V_d} \\sum_{i=0}^{V_d-1} c_d(i)\n$$\nThe sum $\\sum_i c_d(i)$ represents the total number of covered voxels summed over all windows. This is equivalent to summing the lengths of each window that falls within the volume boundaries.\n$$\n\\sum_{i=0}^{V_d-1} c_d(i) = \\sum_{k=0}^{n_d-1} (\\min(V_d, a_k + p_d) - a_k)\n$$\nThus, $\\mu_d = \\frac{1}{V_d} \\sum_k (\\min(V_d, a_k + p_d) - a_k)$.\n\n**2. 3D Multiplicity Metrics**\n\n- **Mean 3D Multiplicity ($\\bar{m}$)**: By independence of axes, the expectation (mean) of the product is the product of the expectations:\n  $$\n  \\bar{m} = \\mathbb{E}[c_x c_y c_z] = \\mathbb{E}[c_x]\\mathbb{E}[c_y]\\mathbb{E}[c_z] = \\mu_x \\mu_y \\mu_z\n  $$\n- **Maximum 3D Multiplicity ($m_{\\max}$)**: Similarly, the maximum of the product is the product of the maxima:\n  $$\n  m_{\\max} = (\\max_i c_x(i)) (\\max_j c_y(j)) (\\max_k c_z(k))\n  $$\n- **Fraction of Multi-coverage ($f_{\\text{multi}}$)**: This is the fraction of voxels with $m(i,j,k) > 1$. It is more easily calculated as $1$ minus the fraction of voxels with single coverage ($m=1$). A voxel has $m=1$ if and only if $c_x(i)=1$, $c_y(j)=1$, and $c_z(k)=1$. Let $N_d(c)$ be the number of voxels on axis $d$ with coverage $c$. The number of voxels with $m=1$ is $N_x(1) N_y(1) N_z(1)$.\n  $$\n  f_{\\text{multi}} = 1 - \\frac{N_x(1) N_y(1) N_z(1)}{V_x V_y V_z}\n  $$\n\n### Part 3: Blending Variance Analysis\n\nThe variance of blending weights $w_\\ell$ at a voxel with multiplicity $m$ is $\\sigma^2 = \\frac{1}{m} \\sum_{\\ell=1}^{m} (w_\\ell - \\frac{1}{m})^2$, since the normalized weights sum to $1$ and their mean is $1/m$. The expected variance is the average of $\\sigma^2(m)$ over the entire volume, which can be computed using the 3D multiplicity distribution $P(m)$: $\\mathbb{E}[\\sigma^2] = \\sum_m P(m) \\sigma^2(m)$.\n\n**1. Hard Stitching**\n\nFor hard stitching, one window is chosen, so the weight vector is $(1, 0, \\dots, 0)$. For $m>1$, the variance is:\n$$\n\\sigma^2_{\\text{hard}}(m) = \\frac{1}{m} \\left[ \\left(1 - \\frac{1}{m}\\right)^2 + (m-1)\\left(0 - \\frac{1}{m}\\right)^2 \\right] = \\frac{1}{m} \\left[ \\frac{(m-1)^2}{m^2} + \\frac{m-1}{m^2} \\right] = \\frac{m-1}{m^2}\n$$\nFor $m=1$, $\\sigma^2(1)=0$. The expected variance $\\mathbb{E}[\\sigma^2_{\\text{hard}}]$ is computed by averaging this value over the 3D multiplicity distribution.\n\n**2. Uniform Averaging**\n\nFor uniform averaging, all weights are equal: $w_\\ell = 1/m$.\n$$\n\\sigma^2_{\\text{uniform}}(m) = \\frac{1}{m} \\sum_{\\ell=1}^m \\left(\\frac{1}{m} - \\frac{1}{m}\\right)^2 = 0\n$$\nThe variance is zero at every voxel, regardless of multiplicity. Thus, the expected variance $\\mathbb{E}[\\sigma^2_{\\text{uniform}}]$ is always $0$.\n\n**3. Raised-Cosine (Hann) Taper**\n\nFor a 1D overlap of length $L=p-s$ between two windows, the pairwise variance is calculated by averaging the local variance over the overlap region. At each position $x$ in the overlap, the normalized weights are $w'_1(x)$ and $w'_2(x)$, and the local variance is $\\sigma^2_x = (w'_1(x) - 1/2)^2$. The 1D pairwise variance is:\n$$\n\\sigma^2_{\\text{pair},d} = \\frac{1}{p_d-s_d} \\sum_{x=s_d}^{p_d-1} \\left( \\frac{w_1(x)}{w_1(x)+w_2(x)} - \\frac{1}{2} \\right)^2\n$$\nwhere $w_1(x) = h(x/(p_d-1))$ and $w_2(x) = h((x-s_d)/(p_d-1))$.\n\nThe expected 3D variance is approximated by considering only voxels with the simplest form of multi-coverage: multiplicity $m=2$ resulting from a pairwise overlap on a single axis, with single coverage on the other two.\n$$\n\\widehat{\\mathbb{E}}[\\sigma^2_{\\text{cosine}}] \\approx \\sigma^2_{\\text{pair},x} P_x(2)P_y(1)P_z(1) + \\sigma^2_{\\text{pair},y} P_x(1)P_y(2)P_z(1) + \\sigma^2_{\\text{pair},z} P_x(1)P_y(1)P_z(2)\n$$\nwhere $P_d(c) = N_d(c)/V_d$ is the fraction of voxels on axis $d$ with coverage $c$.\n\n**Limitations of the Approximation**: This approximation provides a lower bound on the true expected variance. It is limited because:\n1.  It ignores all contributions from voxels with multiplicity $m>2$, which may arise from higher-order overlaps on a single axis or overlaps on multiple axes simultaneously.\n2.  It assumes the 3D variance can be decomposed into independent 1D contributions, whereas a full 3D blending weight is a product of 1D weights, and normalization is non-separable across the 3D overlapping region. The true variance would be more complex to compute.\nThis approximation is most accurate in low-overlap scenarios where $m>2$ is rare.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    def analyze_1d(V_d, p_d, omega):\n        \"\"\"\n        Analyzes a single dimension to compute stride, window starts, and 1D metrics.\n        \n        Returns:\n            - n_d: Number of windows\n            - hist_d: Dictionary of coverage multiplicity to voxel count\n            - mu_d: Mean 1D coverage\n            - max_c_d: Maximum 1D coverage\n            - sigma_pair_d: Pairwise variance for cosine blending\n        \"\"\"\n        # Rule for integer stride s_d\n        s_d = max(1, math.floor(p_d * (1.0 - omega)))\n\n        # Constructive procedure to generate start indices\n        if V_d <= p_d:\n            starts = [0]\n        else:\n            starts = []\n            current_pos = 0\n            # Add windows with stride s_d up to the V_d - p_d anchor point\n            while current_pos < V_d - p_d:\n                starts.append(current_pos)\n                current_pos += s_d\n            \n            # Add the final anchor point if it's not already the last one\n            if not starts or starts[-1] != V_d - p_d:\n                starts.append(V_d - p_d)\n        \n        # Number of windows n_d\n        n_d = len(starts)\n\n        # Coverage multiplicity c_d(i) distribution using a differential array\n        diff_array = np.zeros(V_d + 1, dtype=np.int32)\n        total_covered_voxels = 0\n        for start in starts:\n            end = start + p_d\n            diff_array[start] += 1\n            if end <= V_d:\n                diff_array[end] -= 1\n            total_covered_voxels += min(V_d, end) - start\n\n        coverage_map = np.cumsum(diff_array)[:-1]\n        unique_coverages, counts = np.unique(coverage_map, return_counts=True)\n        hist_d = dict(zip(unique_coverages, counts))\n\n        # Mean 1D multiplicity mu_d\n        mu_d = total_covered_voxels / V_d if V_d > 0 else 0\n        \n        # Max 1D multiplicity\n        max_c_d = 0\n        if len(unique_coverages) > 0:\n            max_c_d = np.max(unique_coverages)\n\n        # Pairwise variance for raised-cosine (Hann) taper\n        sigma_pair_d = 0.0\n        overlap_len = p_d - s_d\n        if overlap_len > 0 and p_d > 1:\n            h = lambda u: 0.5 * (1.0 - np.cos(2.0 * np.pi * u))\n            x_coords = np.arange(s_d, p_d)\n            \n            # Normalized position within the patch [0, 1]\n            u1 = x_coords / (p_d - 1.0)\n            u2 = (x_coords - s_d) / (p_d - 1.0)\n            \n            w1 = h(u1)\n            w2 = h(u2)\n            \n            w_sum = w1 + w2\n            # Avoid division by zero\n            valid_indices = w_sum > 1e-9\n            \n            w1_norm = np.zeros_like(w1)\n            if np.any(valid_indices):\n                w1_norm[valid_indices] = w1[valid_indices] / w_sum[valid_indices]\n            \n            # Variance at each point in the overlap is (w1_norm - 0.5)^2\n            variances = (w1_norm - 0.5)**2\n            sigma_pair_d = np.mean(variances)\n\n        return n_d, hist_d, mu_d, max_c_d, sigma_pair_d\n\n    def compute_case_metrics(V, p, omega):\n        \"\"\"\n        Computes all required metrics for a single test case.\n        \"\"\"\n        dims = ['x', 'y', 'z']\n        n_vals, hists, mu_vals, max_c_vals, sigma_pair_vals = {}, {}, {}, {}, {}\n        \n        for i, d_idx in enumerate(dims):\n            V_d, p_d = V[i], p[i]\n            n_vals[d_idx], hists[d_idx], mu_vals[d_idx], max_c_vals[d_idx], sigma_pair_vals[d_idx] = analyze_1d(V_d, p_d, omega)\n\n        # Total number of windows N\n        N = n_vals['x'] * n_vals['y'] * n_vals['z']\n        n_list = [n_vals['x'], n_vals['y'], n_vals['z']]\n\n        # Mean 3D multiplicity m_bar\n        m_bar = mu_vals['x'] * mu_vals['y'] * mu_vals['z']\n        \n        # Max 3D multiplicity m_max\n        m_max = float(max_c_vals['x'] * max_c_vals['y'] * max_c_vals['z'])\n        \n        # Fraction of multi-coverage f_multi\n        V_tot = V[0] * V[1] * V[2] if all(v > 0 for v in V) else 0\n        n1_x = hists['x'].get(1, 0)\n        n1_y = hists['y'].get(1, 0)\n        n1_z = hists['z'].get(1, 0)\n        f_multi = 1.0 - (n1_x * n1_y * n1_z) / V_tot if V_tot > 0 else 0.0\n\n        # Build 3D multiplicity distribution dist_m\n        dist_m = {}\n        for c_x, count_x in hists['x'].items():\n            for c_y, count_y in hists['y'].items():\n                for c_z, count_z in hists['z'].items():\n                    m = c_x * c_y * c_z\n                    count = count_x * count_y * count_z\n                    dist_m[m] = dist_m.get(m, 0) + count\n\n        # Expected variance for hard stitching\n        E_hard_var = 0.0\n        if V_tot > 0:\n            for m, count in dist_m.items():\n                if m > 1:\n                    var_m = (m - 1.0) / (m**2)\n                    E_hard_var += count * var_m\n            E_hard_var /= V_tot\n\n        # Expected variance for uniform averaging is always 0\n        E_uniform_var = 0.0\n        \n        # Approximate expected variance for cosine tapering\n        P_vals = {}\n        for i, d_idx in enumerate(dims):\n            P_vals[d_idx] = {\n                1: hists[d_idx].get(1, 0) / V[i] if V[i] > 0 else 0,\n                2: hists[d_idx].get(2, 0) / V[i] if V[i] > 0 else 0\n            }\n\n        term_x = sigma_pair_vals['x'] * P_vals['x'][2] * P_vals['y'][1] * P_vals['z'][1]\n        term_y = sigma_pair_vals['y'] * P_vals['x'][1] * P_vals['y'][2] * P_vals['z'][1]\n        term_z = sigma_pair_vals['z'] * P_vals['x'][1] * P_vals['y'][1] * P_vals['z'][2]\n        E_cosine_var = term_x + term_y + term_z\n\n        return [N, n_list, m_bar, m_max, f_multi, E_hard_var, E_uniform_var, E_cosine_var]\n\n    test_cases = [\n        {'V': (256, 256, 128), 'p': (96, 96, 96), 'omega': 0.5},\n        {'V': (256, 256, 128), 'p': (256, 256, 128), 'omega': 0.0},\n        {'V': (200, 200, 60), 'p': (64, 64, 64), 'omega': 0.6},\n        {'V': (129, 129, 65), 'p': (64, 64, 64), 'omega': 0.75},\n        {'V': (64, 64, 32), 'p': (96, 96, 96), 'omega': 0.5},\n    ]\n\n    all_results = [compute_case_metrics(**case) for case in test_cases]\n\n    # Format output according to problem specification\n    case_strings = []\n    for res in all_results:\n        # res = [N, [nx,ny,nz], m_bar, m_max, f_multi, E_hard, E_uniform, E_cosine]\n        n_list_str = f\"[{res[1][0]},{res[1][1]},{res[1][2]}]\"\n        # Convert all numeric values to string representations with required precision\n        vals_str = [str(res[0]), n_list_str] + [f\"{v:.7f}\" for v in res[2:]]\n        case_str = f\"[{','.join(vals_str)}]\"\n        case_strings.append(case_str)\n    \n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```", "id": "4554564"}, {"introduction": "Once a full-volume segmentation is produced, we must rigorously evaluate its quality, but simple accuracy can be highly misleading in medical imaging due to severe class imbalance. This hands-on problem has you step through the foundational process of evaluating a segmentation model's probabilistic output at the pixel level. By constructing Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves from a set of predictions, you will not only master the calculation of these critical metrics but also gain a crucial insight into why the PR curve is often more informative for tasks like lesion detection.", "problem": "A pixel-wise lesion segmentation model produces calibrated predicted probabilities $s_i \\in [0,1]$ for each pixel $i$ in a single image. Ground-truth labels are $y_i \\in \\{0,1\\}$, where $y_i = 1$ indicates lesion pixel and $y_i = 0$ indicates background. Consider the following $n = 20$ pixels, each given as $(s_i, y_i)$ in descending order of $s_i$:\n$(0.92, 1)$, $(0.90, 0)$, $(0.88, 0)$, $(0.85, 1)$, $(0.70, 0)$, $(0.60, 0)$, $(0.55, 0)$, $(0.40, 0)$, $(0.40, 0)$, $(0.35, 0)$, $(0.30, 0)$, $(0.25, 0)$, $(0.20, 0)$, $(0.18, 0)$, $(0.15, 0)$, $(0.12, 0)$, $(0.10, 0)$, $(0.08, 0)$, $(0.05, 0)$, $(0.02, 0)$. The total number of positives is $P = \\sum_i y_i$ and the total number of negatives is $N = n - P$.\n\nUsing only fundamental definitions that apply to binary decision rules induced by thresholding $s_i$ with a threshold $\\tau$, construct the Receiver Operating Characteristic - Area Under the Curve (ROC-AUC) and the Precision-Recall Area Under the Curve (PR-AUC) for this segmentation task. You must:\n- Define and use the True Positive Rate (TPR), False Positive Rate (FPR), Precision, and Recall in terms of true positives ($TP$), false positives ($FP$), true negatives ($TN$), and false negatives ($FN$).\n- Sweep the threshold $\\tau$ over the distinct score values to trace the ROC and PR curves.\n- Compute $ROC\\text{-}AUC$ as the area under the ROC curve and $PR\\text{-}AUC$ as the area under the PR curve, adhering to scientifically standard integration conventions for these curves.\n\nThen, starting from first principles about class prevalence and error normalization, provide a brief derivation-based explanation of why $PR\\text{-}AUC$ is more informative than $ROC\\text{-}AUC$ under extreme class imbalance in pixel-wise lesion segmentation.\n\nRound both areas to four significant figures. Report your final numerical answers as a single row matrix containing $ROC\\text{-}AUC$ followed by $PR\\text{-}AUC$.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- Model type: Pixel-wise lesion segmentation.\n- Predicted probabilities: $s_i \\in [0,1]$.\n- Ground-truth labels: $y_i \\in \\{0,1\\}$, where $y_i=1$ is a lesion pixel.\n- Total number of pixels: $n = 20$.\n- Data points $(s_i, y_i)$ sorted by $s_i$: $(0.92, 1)$, $(0.90, 0)$, $(0.88, 0)$, $(0.85, 1)$, $(0.70, 0)$, $(0.60, 0)$, $(0.55, 0)$, $(0.40, 0)$, $(0.40, 0)$, $(0.35, 0)$, $(0.30, 0)$, $(0.25, 0)$, $(0.20, 0)$, $(0.18, 0)$, $(0.15, 0)$, $(0.12, 0)$, $(0.10, 0)$, $(0.08, 0)$, $(0.05, 0)$, $(0.02, 0)$.\n- Total positives: $P = \\sum_i y_i$.\n- Total negatives: $N = n - P$.\n- Task: Compute $ROC\\text{-}AUC$ and $PR\\text{-}AUC$ by sweeping a threshold $\\tau$.\n- Required definitions: True Positive Rate (TPR), False Positive Rate (FPR), Precision, and Recall in terms of true positives ($TP$), false positives ($FP$), true negatives ($TN$), and false negatives ($FN$).\n- Task: Explain why $PR\\text{-}AUC$ is more informative than $ROC\\text{-}AUC$ under extreme class imbalance.\n- Final answer format: Numerical values for $ROC\\text{-}AUC$ and $PR\\text{-}AUC$ rounded to four significant figures, reported in a row matrix.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, as it deals with standard, fundamental metrics for evaluating binary classification models (ROC and PR curves), a core topic in machine learning and medical data analysis. The problem is well-posed, providing all necessary data and clear instructions for calculation, leading to a unique solution. The language is objective and precise. The problem does not violate any of the invalidity criteria. For instance, it is not scientifically unsound, is formalizable, is complete, and is not trivial.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n**Part 1: Calculation of ROC-AUC and PR-AUC**\n\nFirst, we determine the number of positive and negative samples from the provided data.\nThe positive labels ($y_i=1$) correspond to scores $s_i=0.92$ and $s_i=0.85$.\nThe total number of positive samples is $P = 2$.\nThe total number of negative samples is $N = n - P = 20 - 2 = 18$.\nThis dataset exhibits significant class imbalance, with $N \\gg P$.\n\nThe core metrics are defined as:\n- True Positives ($TP$): Number of positive samples correctly classified as positive ($y_i=1, s_i \\ge \\tau$).\n- False Positives ($FP$): Number of negative samples incorrectly classified as positive ($y_i=0, s_i \\ge \\tau$).\n- True Positive Rate ($TPR$) or Recall: $TPR = \\frac{TP}{P}$.\n- False Positive Rate ($FPR$): $FPR = \\frac{FP}{N}$.\n- Precision: $Precision = \\frac{TP}{TP+FP}$.\n\n**ROC Curve Construction and AUC Calculation**\nThe ROC curve plots $TPR$ against $FPR$ for varying thresholds $\\tau$. We construct the curve by processing the data points in descending order of their scores, which is already done. Each point represents a potential threshold.\nStarting with a threshold $\\tau > 0.92$, no pixels are classified as positive, so $TP=0$ and $FP=0$. This gives the starting point of the ROC curve: $(FPR, TPR) = (0, 0)$.\n\nWe iterate through the sorted data points:\n1. At $s_1=0.92$ ($y_1=1$): When the threshold drops below $0.92$, this pixel is now classified as positive. This is a true positive.\n   - $TP$ increases by $1$, $FP$ is unchanged. $TP=1, FP=0$.\n   - $TPR = TP/P = 1/2 = 0.5$.\n   - $FPR = FP/N = 0/18 = 0$.\n   - The curve moves from $(0,0)$ to $(0, 0.5)$.\n2. At $s_2=0.90$ ($y_2=0$): This is a false positive.\n   - $TP$ is unchanged, $FP$ increases by $1$. $TP=1, FP=1$.\n   - $TPR = 1/2 = 0.5$.\n   - $FPR = 1/18$.\n   - The curve moves from $(0, 0.5)$ to $(1/18, 0.5)$.\n3. At $s_3=0.88$ ($y_3=0$): This is a false positive.\n   - $TP$ is unchanged, $FP$ increases by $1$. $TP=1, FP=2$.\n   - $TPR = 1/2 = 0.5$.\n   - $FPR = 2/18 = 1/9$.\n   - The curve moves from $(1/18, 0.5)$ to $(2/18, 0.5)$.\n4. At $s_4=0.85$ ($y_4=1$): This is a true positive.\n   - $TP$ increases by $1$, $FP$ is unchanged. $TP=2, FP=2$.\n   - $TPR = 2/2 = 1$.\n   - $FPR = 2/18 = 1/9$.\n   - The curve moves from $(2/18, 0.5)$ to $(2/18, 1)$.\n5. For all subsequent points, $y_i=0$. Thus, as the threshold is lowered further, only $FP$ increases, from $2$ up to $18$. $TP$ remains fixed at $2$, so $TPR$ remains fixed at $1$.\n   - The curve moves horizontally from $(2/18, 1)$ to $(18/18, 1) = (1, 1)$.\n\nThe vertices of the ROC curve are $(0,0)$, $(0, 0.5)$, $(1/18, 0.5)$, $(2/18, 0.5)$, $(2/18, 1)$, and $(1, 1)$.\nThe $ROC\\text{-}AUC$ is the area under this curve, which is calculated using the trapezoidal rule. The area is the sum of the areas of trapezoids formed by consecutive points $(FPR_{k-1}, TPR_{k-1})$ and $(FPR_k, TPR_k)$:\n$ROC\\text{-}AUC = \\sum_{k} \\frac{TPR_{k-1} + TPR_k}{2} (FPR_k - FPR_{k-1})$.\nFor a curve composed of horizontal and vertical segments, this simplifies to summing the areas of rectangles under the horizontal segments.\n- Area under segment from $(0, 0.5)$ to $(2/18, 0.5)$: Area$_1 = (2/18 - 0) \\times 0.5 = 1/18$.\n- Area under segment from $(2/18, 1)$ to $(1, 1)$: Area$_2 = (1 - 2/18) \\times 1 = 16/18$.\nTotal $ROC\\text{-}AUC = \\text{Area}_1 + \\text{Area}_2 = \\frac{1}{18} + \\frac{16}{18} = \\frac{17}{18}$.\nNumerically, $ROC\\text{-}AUC = \\frac{17}{18} \\approx 0.94444...$. Rounded to four significant figures, this is $0.9444$.\n\n**PR Curve Construction and AUC Calculation**\nThe PR curve plots Precision against Recall ($=TPR$). A standard method for computing the area under the PR curve, especially in information retrieval and machine learning, is the Average Precision (AP) score. This method computes a weighted average of precisions, where each precision is calculated at each rank $k$ where a positive sample is found, and then averaged over the total number of positives. This is equivalent to a rectangular integration: $PR\\text{-}AUC = \\sum_{k} (R_k - R_{k-1}) P_k$, where $(R_k, P_k)$ are the recall and precision for the $k$-th point in the sorted list.\nThe formula simplifies to $PR\\text{-}AUC = \\frac{1}{P} \\sum_{k=1}^n \\text{Precision}(k) \\times I(y_k=1)$, where $I(y_k=1)$ is an indicator function that is $1$ if the sample at rank $k$ is positive and $0$ otherwise, and $\\text{Precision}(k)$ is the precision considering the top $k$ predictions.\n\nWe sum the precisions only at the ranks where a positive sample appears.\nThe positive samples are at rank $k=1$ and $k=4$.\n- At rank $k=1$ (for $s_1=0.92, y_1=1$):\n  - We have processed $1$ sample. $TP=1, FP=0$.\n  - Precision$(1) = \\frac{TP}{TP+FP} = \\frac{1}{1+0} = 1$.\n- At rank $k=4$ (for $s_4=0.85, y_4=1$):\n  - We have processed $4$ samples. At this point, we have encountered one true positive ($s=0.92$) and two false positives ($s=0.90, s=0.88$). After processing rank 4, we have $TP=2, FP=2$.\n  - Precision$(4) = \\frac{TP}{TP+FP} = \\frac{2}{2+2} = \\frac{2}{4} = 0.5$.\n\nNow, we compute the average precision:\n$PR\\text{-}AUC = \\frac{1}{P} (\\text{Precision}(1) + \\text{Precision}(4)) = \\frac{1}{2} (1 + 0.5) = \\frac{1.5}{2} = 0.75$.\nTo four significant figures, this is $0.7500$.\n\n**Part 2: Explanation of PR-AUC vs. ROC-AUC under Class Imbalance**\n\nIn pixel-wise lesion segmentation, the number of negative pixels (background, $N$) is typically orders of magnitude larger than the number of positive pixels (lesion, $P$), i.e., $N \\gg P$. We analyze why $PR\\text{-}AUC$ is more informative in this setting by examining the definitions of the metrics.\n\nThe ROC curve plots $TPR = \\frac{TP}{P}$ versus $FPR = \\frac{FP}{N}$. The key term here is $FPR$. Because it is normalized by the total number of negatives, $N$, the $FPR$ is insensitive to large changes in the absolute number of false positives ($FP$) when $N$ is very large. For a classifier to achieve a low $FPR$, it can afford to make a substantial number of false positive predictions. For example, if $N=10^6$, even with $1000$ false positives ($FP=1000$), the $FPR$ is only $\\frac{1000}{10^6} = 0.001$. An increase of $FP$ by a factor of $10$ to $10000$ would only increase $FPR$ to $0.01$. The ROC curve would remain close to the ideal point $(0,1)$, and the $ROC\\text{-}AUC$ would be high, suggesting excellent performance. This masks the fact that the model is generating thousands of false alarms, which would be unacceptable in a clinical setting.\n\nThe PR curve plots Precision $=\\frac{TP}{TP+FP}$ versus Recall ($TPR$). The Precision metric's denominator, $TP+FP$, is the total number of pixels predicted as positive. It does not involve the large number of true negatives, $TN$. Therefore, Precision is directly sensitive to the absolute number of false positives $FP$. Using the previous example, if we have $P=100$ and we achieve high recall ($TP \\approx 100$), a model generating $FP=1000$ would have a precision of $\\frac{100}{100+1000} \\approx 0.09$. A model with $FP=10000$ would have a precision of $\\frac{100}{100+10000} \\approx 0.01$. These large changes in $FP$ lead to dramatic drops in precision.\n\nConsequently, the PR curve and its area, $PR\\text{-}AUC$, directly reflect the trade-off between identifying all true lesions (Recall) and the fraction of predicted lesions that are correct (Precision). In a scenario with extreme class imbalance, a high $ROC\\text{-}AUC$ can be misleadingly optimistic, whereas a low $PR\\text{-}AUC$ would correctly indicate that the model's positive predictions are unreliable. Since the clinical utility of a segmentation model often depends on the reliability of its positive predictions (positive predictive value, which is Precision), the PR curve provides a more informative and realistic assessment of model performance under class imbalance.", "answer": "$$\\boxed{\\begin{pmatrix} 0.9444 & 0.7500 \\end{pmatrix}}$$", "id": "4554559"}]}