{"hands_on_practices": [{"introduction": "Constructing a reliable biomedical knowledge graph begins with clean, unambiguous data. This first practice tackles the foundational challenge of entity normalization, the process of mapping ambiguous free-text mentions to canonical identifiers. Through this exercise [@problem_id:4577572], you will build a principled pipeline using a combination of string similarity metrics to handle synonyms and misspellings, and explore the critical precision-recall tradeoff inherent in any real-world data integration task.", "problem": "Construct a complete, runnable program that implements and evaluates a principled entity normalization pipeline for biomedical knowledge graph construction. The pipeline must map free-text mentions to canonical identifiers drawn from curated identifiers for genes and drugs, specifically HUGO Gene Nomenclature Committee (HGNC) identifiers for genes and DrugBank identifiers for drugs. The program must be self-contained, use only the data and parameters specified below, and produce a single-line output in the specified format. The evaluation must quantify precision–recall tradeoffs under synonymy using a tunable scoring threshold and feature-weight vector.\n\nYou must derive the algorithm starting from core definitions and well-tested facts about string normalization and evaluation metrics. The program must implement the following components precisely.\n\n1) Canonical entities and synonyms. Let the set of canonical identifiers be denoted by $\\mathcal{C}$. Each canonical identifier $c \\in \\mathcal{C}$ has a finite set of surface forms (synonyms) $\\mathcal{S}(c)$ and a single preferred label $p(c) \\in \\mathcal{S}(c)$. Use exactly the following set:\n- Genes (HGNC):\n  - $c = \\text{HGNC:11998}$, $\\mathcal{S}(c) = \\{\\text{\"TP53\"}, \\text{\"p53\"}, \\text{\"tumor protein p53\"}\\}$, $p(c) = \\text{\"TP53\"}$.\n  - $c = \\text{HGNC:3236}$, $\\mathcal{S}(c) = \\{\\text{\"EGFR\"}, \\text{\"epidermal growth factor receptor\"}, \\text{\"ERBB1\"}\\}$, $p(c) = \\text{\"EGFR\"}$.\n  - $c = \\text{HGNC:391}$, $\\mathcal{S}(c) = \\{\\text{\"AKT1\"}, \\text{\"PKB\"}, \\text{\"RAC-alpha serine/threonine-protein kinase\"}\\}$, $p(c) = \\text{\"AKT1\"}$.\n- Drugs (DrugBank):\n  - $c = \\text{DrugBank:DB00316}$, $\\mathcal{S}(c) = \\{\\text{\"acetaminophen\"}, \\text{\"paracetamol\"}, \\text{\"APAP\"}\\}$, $p(c) = \\text{\"acetaminophen\"}$.\n  - $c = \\text{DrugBank:DB00945}$, $\\mathcal{S}(c) = \\{\\text{\"aspirin\"}, \\text{\"acetylsalicylic acid\"}, \\text{\"ASA\"}\\}$, $p(c) = \\text{\"aspirin\"}$.\n  - $c = \\text{DrugBank:DB01050}$, $\\mathcal{S}(c) = \\{\\text{\"ibuprofen\"}\\}$, $p(c) = \\text{\"ibuprofen\"}$.\n\n2) Mentions and ground truth. Let the evaluation mention set be $\\mathcal{M}$ with ground-truth mapping $g: \\mathcal{M} \\to \\mathcal{C} \\cup \\{\\varnothing\\}$, where $\\varnothing$ denotes no canonical mapping. Use exactly the following list:\n- $(\\text{\"TP53\"}, \\text{HGNC:11998})$\n- $(\\text{\"p53\"}, \\text{HGNC:11998})$\n- $(\\text{\"tumor prot p53\"}, \\text{HGNC:11998})$\n- $(\\text{\"p 53\"}, \\text{HGNC:11998})$\n- $(\\text{\"EGFR\"}, \\text{HGNC:3236})$\n- $(\\text{\"ERBB1\"}, \\text{HGNC:3236})$\n- $(\\text{\"erbb-1\"}, \\text{HGNC:3236})$\n- $(\\text{\"AKT-1\"}, \\text{HGNC:391})$\n- $(\\text{\"PKB\"}, \\text{HGNC:391})$\n- $(\\text{\"acetaminophen\"}, \\text{DrugBank:DB00316})$\n- $(\\text{\"paracetamol\"}, \\text{DrugBank:DB00316})$\n- $(\\text{\"APAP\"}, \\text{DrugBank:DB00316})$\n- $(\\text{\"aspirin\"}, \\text{DrugBank:DB00945})$\n- $(\\text{\"ASA\"}, \\text{DrugBank:DB00945})$\n- $(\\text{\"acetylsalicylic acid\"}, \\text{DrugBank:DB00945})$\n- $(\\text{\"ibuprofen\"}, \\text{DrugBank:DB01050})$\n- $(\\text{\"ibuprfen\"}, \\text{DrugBank:DB01050})$\n- $(\\text{\"randomword\"}, \\varnothing)$\n- $(\\text{\"EGFR inhibitor\"}, \\varnothing)$\n\n3) Normalization function. Define a deterministic normalization function $N:\\text{strings}\\to\\text{strings}$ for equality and edit-distance comparison and a tokenization function $T:\\text{strings}\\to 2^{\\text{tokens}}$ for Jaccard similarity.\n- Abbreviation expansion: before normalization, apply tokenwise replacements using the map $\\{\\text{\"asa\"}\\mapsto\\text{\"acetylsalicylic acid\"}, \\text{\"apap\"}\\mapsto\\text{\"acetaminophen\"}\\}$ on lowercase tokens.\n- String normalization for equality/edit distance: $N(s)$ is obtained by lowercasing, performing abbreviation expansion, and removing all non-alphanumeric characters. For example, $N(\\text{\"p 53\"}) = \\text{\"p53\"}$ and $N(\\text{\"erbb-1\"}) = \\text{\"erbb1\"}$.\n- Tokenization: $T(s)$ is obtained by lowercasing, performing abbreviation expansion, replacing every non-alphanumeric character with a single space, splitting on whitespace, and removing stopwords from the set $\\{\\text{\"protein\"}, \\text{\"receptor\"}, \\text{\"acid\"}, \\text{\"serine\"}, \\text{\"threonine\"}, \\text{\"alpha\"}, \\text{\"beta\"}, \\text{\"inhibitor\"}\\}$. For example, $T(\\text{\"epidermal growth factor receptor\"}) = \\{\\text{\"epidermal\"}, \\text{\"growth\"}, \\text{\"factor\"}\\}$.\n\n4) Feature functions and scoring. For a mention $m$ and a candidate synonym $s \\in \\mathcal{S}(c)$ of some $c\\in\\mathcal{C}$, define the feature vector $\\phi(m,s) \\in \\mathbb{R}^{4}$ by\n- Exact match feature: $f_{\\mathrm{e}}(m,s) = \\mathbf{1}[N(m) = N(s)]$.\n- Preferred-label exact match feature: $f_{\\mathrm{p}}(m,s) = \\mathbf{1}[N(m) = N(p(c))]$.\n- Token Jaccard feature: $f_{\\mathrm{j}}(m,s) = \\dfrac{|T(m)\\cap T(s)|}{|T(m)\\cup T(s)|}$, with the convention that $0/0$ is $0$.\n- Normalized Levenshtein similarity: $f_{\\mathrm{d}}(m,s) = 1 - \\dfrac{D(N(m),N(s))}{\\max\\{|N(m)|,|N(s)|\\}}$, where $D(\\cdot,\\cdot)$ is the Levenshtein edit distance and $|\\cdot|$ is the string length in characters; if both lengths are $0$, set $f_{\\mathrm{d}}(m,s)=1$.\nLet the weight vector be $w = (w_{\\mathrm{e}}, w_{\\mathrm{j}}, w_{\\mathrm{d}}, w_{\\mathrm{p}}) \\in \\mathbb{R}_{\\ge 0}^{4}$. Normalize the weights to the probability simplex by $\\tilde{w} = w / \\sum_{i} w_{i}$; if $\\sum_{i} w_{i} = 0$, set $\\tilde{w} = (1/4,1/4,1/4,1/4)$. The per-synonym score is\n$$\nS(m,s; \\tilde{w}) = \\tilde{w}_{\\mathrm{e}} f_{\\mathrm{e}}(m,s) + \\tilde{w}_{\\mathrm{j}} f_{\\mathrm{j}}(m,s) + \\tilde{w}_{\\mathrm{d}} f_{\\mathrm{d}}(m,s) + \\tilde{w}_{\\mathrm{p}} f_{\\mathrm{p}}(m,s).\n$$\nThe per-entity score is the maximum over its synonyms:\n$$\nS^{*}(m,c;\\tilde{w}) = \\max_{s \\in \\mathcal{S}(c)} S(m,s;\\tilde{w}).\n$$\n\n5) Decision rule with threshold. Given a threshold $\\tau \\in [0,1]$, predict $\\hat{g}(m) = \\arg\\max_{c\\in\\mathcal{C}} S^{*}(m,c;\\tilde{w})$ if $\\max_{c\\in\\mathcal{C}} S^{*}(m,c;\\tilde{w}) \\ge \\tau$; otherwise predict $\\hat{g}(m) = \\varnothing$. Break ties in $\\arg\\max$ by choosing the lexicographically smallest identifier string.\n\n6) Evaluation metrics. Over the mention set $\\mathcal{M}$ with ground truth $g$, compute true positives $TP = |\\{m \\in \\mathcal{M}: g(m)\\in \\mathcal{C}, \\hat{g}(m) = g(m)\\}|$, false positives $FP = |\\{m \\in \\mathcal{M}: \\hat{g}(m)\\in \\mathcal{C}, \\hat{g}(m) \\neq g(m)\\}|$, and false negatives $FN = |\\{m \\in \\mathcal{M}: g(m)\\in \\mathcal{C}, \\hat{g}(m) = \\varnothing\\}|$. Precision and recall are\n$$\nP = \\begin{cases}\n\\dfrac{TP}{TP+FP} & \\text{if } TP+FP > 0\\\\\n0 & \\text{otherwise}\n\\end{cases}, \\quad\nR = \\begin{cases}\n\\dfrac{TP}{TP+FN} & \\text{if } TP+FN > 0\\\\\n0 & \\text{otherwise}\n\\end{cases}.\n$$\nThe harmonic mean is\n$$\nF_{1} = \\begin{cases}\n\\dfrac{2PR}{P+R} & \\text{if } P+R > 0\\\\\n0 & \\text{otherwise}\n\\end{cases}.\n$$\nAll three quantities must be reported as decimal numbers in $[0,1]$ rounded to four decimal places; do not use a percent sign.\n\n7) Test suite. Evaluate exactly the following $5$ test cases, each consisting of a weight vector $w$ and a threshold $\\tau$:\n- Case $1$: $w=(0.6, 0.1, 0.2, 0.1)$, $\\tau=0.7$.\n- Case $2$: $w=(0.95, 0.0, 0.05, 0.0)$, $\\tau=0.95$.\n- Case $3$: $w=(0.25, 0.35, 0.35, 0.05)$, $\\tau=0.6$.\n- Case $4$: $w=(1.0, 0.0, 0.0, 0.0)$, $\\tau=1.0$.\n- Case $5$: $w=(2.0, 0.5, 0.5, 0.0)$, $\\tau=0.65$.\n\n8) Final output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must flatten the triplets $(P,R,F_{1})$ for the cases in the order above. For example, the format is $[\\text{P}_{1},\\text{R}_{1},\\text{F1}_{1},\\text{P}_{2},\\text{R}_{2},\\text{F1}_{2},\\dots,\\text{P}_{5},\\text{R}_{5},\\text{F1}_{5}]$, where each value is rounded to four decimal places.", "solution": "We construct the solution by deriving a normalization and matching pipeline from fundamental definitions in entity resolution and then translating it into an algorithm with quantifiable evaluation.\n\nFoundational base. We start from the following well-tested definitions: string normalization reduces superficial variation without changing identity; token-based similarity such as Jaccard similarity measures overlap between sets; edit distance quantifies minimal operations to transform one string to another; and classification metrics such as precision and recall measure correctness and completeness of mappings. For a free-text mention set in biomedicine, canonical identifiers (for example, HUGO Gene Nomenclature Committee and DrugBank) each have multiple synonyms, leading to synonymy and near-synonymy that require careful normalization. Given that knowledge graph construction requires linking mentions to canonical nodes, we must implement a deterministic decision rule that trades off between false matches and missed matches via a threshold parameter.\n\nNormalization design. Let $N(\\cdot)$ be a lowercasing, abbreviation-expanding, and non-alphanumeric-stripping function. The rationale is that alphanumeric cores carry the majority of discriminative content for gene symbols and drug names, while punctuation and spacing are often artifacts of writing. Let $T(\\cdot)$ map strings to token sets after lowercasing, abbreviation expansion, punctuation-to-space mapping, and stopword removal. The stopword set includes generic biomedical words that inflate overlap without adding discriminative value, such as $\\{\\text{\"protein\"}, \\text{\"receptor\"}, \\text{\"acid\"}, \\text{\"serine\"}, \\text{\"threonine\"}, \\text{\"alpha\"}, \\text{\"beta\"}, \\text{\"inhibitor\"}\\}$. These are treated as non-informative tokens so that the Jaccard similarity emphasizes specific markers like $\\text{\"p53\"}$ and $\\text{\"epidermal\"}$.\n\nFeature construction. For a mention $m$ and a synonym $s$ of a candidate entity $c$, we define four features:\n- $f_{\\mathrm{e}}(m,s) = \\mathbf{1}[N(m)=N(s)]$ captures robust exact equivalence after normalization, a high-precision signal.\n- $f_{\\mathrm{p}}(m,s) = \\mathbf{1}[N(m)=N(p(c))]$ encodes that preferred labels, when exactly matched, carry extra reliability (for example, official symbols).\n- $f_{\\mathrm{j}}(m,s)$ is the Jaccard similarity between token sets $T(m)$ and $T(s)$, defined as $|A\\cap B|/|A\\cup B|$, a measure with range $[0,1]$ that summarizes overlap robust to word reordering.\n- $f_{\\mathrm{d}}(m,s) = 1 - D(N(m),N(s))/\\max\\{|N(m)|,|N(s)|\\}$ is a normalized Levenshtein-based similarity, also in $[0,1]$, that rewards near-matches, particularly useful for misspellings such as $\\text{\"ibuprfen\"}$ versus $\\text{\"ibuprofen\"}$.\n\nLinear scoring. We build a convex combination of these features using a nonnegative weight vector $w=(w_{\\mathrm{e}},w_{\\mathrm{j}},w_{\\mathrm{d}},w_{\\mathrm{p}})$. To ensure principled interpretability and boundedness, we normalize the weights to the probability simplex: $\\tilde{w} = w / \\sum_{i} w_{i}$ when $\\sum_{i}w_{i} > 0$, and use the uniform weight $\\tilde{w}=(1/4,1/4,1/4,1/4)$ otherwise. The per-synonym score is $S(m,s;\\tilde{w}) = \\tilde{w}_{\\mathrm{e}} f_{\\mathrm{e}} + \\tilde{w}_{\\mathrm{j}} f_{\\mathrm{j}} + \\tilde{w}_{\\mathrm{d}} f_{\\mathrm{d}} + \\tilde{w}_{\\mathrm{p}} f_{\\mathrm{p}}$. The per-entity score $S^{*}(m,c;\\tilde{w})$ is the maximum over its synonyms. This max operator is justified by the logical semantics that an entity should be credited for its best-matching alias.\n\nDecision rule and tradeoff. We predict $\\hat{g}(m)$ as the argument that maximizes $S^{*}(m,c;\\tilde{w})$ if and only if the maximal score exceeds a threshold $\\tau\\in[0,1]$; otherwise we abstain by predicting $\\varnothing$. This creates a tunable precision–recall curve: higher $\\tau$ increases precision by eliminating low-confidence matches at the cost of recall, while lower $\\tau$ increases recall by allowing more near-matches, risking more false positives under synonymy and token overlap. The tie-breaking by lexicographic order ensures determinism.\n\nComputation of features. We compute $f_{\\mathrm{j}}$ using token sets, with the convention that if both token sets are empty, the similarity is $0$, reflecting no evidence. We compute $D(\\cdot,\\cdot)$, the Levenshtein distance, via dynamic programming with time complexity $O(|x|\\cdot|y|)$, which is feasible given short biomedical strings.\n\nEvaluation metrics. Using the ground-truth function $g$, we count true positives $TP$, false positives $FP$, and false negatives $FN$ as set cardinalities based on $\\hat{g}(m)$ and $g(m)$. Precision $P$ and recall $R$ are computed as ratios $TP/(TP+FP)$ and $TP/(TP+FN)$ when denominators are positive, and set to $0$ otherwise. The harmonic mean $F_{1} = 2PR/(P+R)$ provides a single-number summary, set to $0$ when $P+R=0$.\n\nEdge cases and boundary conditions. The weight vector is renormalized to handle inputs that do not sum to $1$ (Case $5$), and all-zero weights default to uniform. The threshold $\\tau=1.0$ (Case $4$) requires perfect scores, testing the strictest precision bias. Misspellings and hyphenation (for example, $\\text{\"erbb-1\"}$, $\\text{\"AKT-1\"}$, $\\text{\"ibuprfen\"}$) are handled via $N(\\cdot)$ and $f_{\\mathrm{d}}$. Context-bearing mentions like $\\text{\"EGFR inhibitor\"}$ are designed to exhibit token overlap but should be filtered out at sufficiently high $\\tau$, illustrating precision under synonymy pressure.\n\nAlgorithmic steps.\n- Precompute for each synonym $s$ its $N(s)$ and $T(s)$, and for each entity $c$ its preferred normalized label $N(p(c))$.\n- For each mention $m$, compute $N(m)$ and $T(m)$, then for each entity $c$, compute $S^{*}(m,c;\\tilde{w})$ by evaluating features against each $s\\in \\mathcal{S}(c)$ and taking the maximum.\n- Select $\\hat{g}(m)$ using the threshold rule with $\\tau$ and tie-breaking as specified.\n- Aggregate counts $TP$, $FP$, $FN$, then compute $P$, $R$, and $F_{1}$.\n- Repeat for the $5$ weight–threshold cases, and output the flattened list $[P_{1},R_{1},F_{1,1},\\dots,P_{5},R_{5},F_{1,5}]$, rounding to four decimal places.\n\nThis design directly operationalizes the core principles: deterministic normalization, convex combination of complementary similarity signals, and thresholded decision theory to express the precision–recall tradeoff under synonymy and near-synonymy. The provided test suite covers a balanced case, an exact-match-dominant high-threshold case, a recall-oriented case, a boundary case requiring perfect matches, and a renormalization case, ensuring coverage of critical behaviors.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef normalize_weights(w):\n    w = np.array(w, dtype=float)\n    s = w.sum()\n    if s > 0:\n        return w / s\n    # default to uniform if all-zero\n    return np.ones_like(w) / len(w)\n\ndef abbrev_expand_tokens(tokens, abbr_map):\n    # Expand tokens using abbreviation map (inputs and keys lowercase).\n    out = []\n    for tok in tokens:\n        if tok in abbr_map:\n            # Expansion may introduce multiple space-separated tokens.\n            expanded = abbr_map[tok].lower().split()\n            out.extend(expanded)\n        else:\n            out.append(tok)\n    return out\n\ndef normalize_string_for_match(s, abbr_map):\n    # Lowercase, tokenize by replacing non-alphanumeric with space, expand, then remove non-alphanumeric entirely.\n    s_low = s.lower()\n    # Replace non-alphanumeric with space to tokenize\n    chars = []\n    for ch in s_low:\n        if ch.isalnum():\n            chars.append(ch)\n        else:\n            chars.append(' ')\n    tokenized = ''.join(chars).split()\n    expanded = abbrev_expand_tokens(tokenized, abbr_map)\n    # Remove non-alphanumeric and join: expanded are alphanumeric tokens already\n    return ''.join(''.join([c for c in tok if c.isalnum()]) for tok in expanded)\n\ndef token_set(s, abbr_map, stopwords):\n    s_low = s.lower()\n    # Replace non-alphanumeric with space to tokenize\n    chars = []\n    for ch in s_low:\n        if ch.isalnum():\n            chars.append(ch)\n        else:\n            chars.append(' ')\n    tokenized = ''.join(chars).split()\n    expanded = abbrev_expand_tokens(tokenized, abbr_map)\n    # Remove stopwords\n    toks = [tok for tok in expanded if tok and tok not in stopwords]\n    return set(toks)\n\ndef jaccard(a_set, b_set):\n    if not a_set and not b_set:\n        return 0.0\n    inter = len(a_set & b_set)\n    union = len(a_set | b_set)\n    return inter / union if union > 0 else 0.0\n\ndef levenshtein(a, b):\n    # Classic DP, space-optimized\n    la, lb = len(a), len(b)\n    if la == 0:\n        return lb\n    if lb == 0:\n        return la\n    # Ensure a is shorter\n    if la > lb:\n        a, b = b, a\n        la, lb = lb, la\n    prev = list(range(lb + 1))\n    for i in range(1, la + 1):\n        curr = [i] + [0] * lb\n        ca = a[i - 1]\n        for j in range(1, lb + 1):\n            cb = b[j - 1]\n            cost = 0 if ca == cb else 1\n            curr[j] = min(\n                curr[j - 1] + 1,      # insertion\n                prev[j] + 1,          # deletion\n                prev[j - 1] + cost    # substitution\n            )\n        prev = curr\n    return prev[lb]\n\ndef normalized_lev_sim(a, b):\n    # a and b are normalized strings\n    maxlen = max(len(a), len(b))\n    if maxlen == 0:\n        return 1.0\n    d = levenshtein(a, b)\n    return 1.0 - (d / maxlen)\n\ndef score_features(m_norm, m_tokens, s_norm, s_tokens, pref_norm):\n    # Compute features: f_e, f_j, f_d, f_p in that order mapping to weights [we, wj, wd, wp]\n    f_e = 1.0 if m_norm == s_norm else 0.0\n    f_p = 1.0 if m_norm == pref_norm else 0.0\n    f_j = jaccard(m_tokens, s_tokens)\n    f_d = normalized_lev_sim(m_norm, s_norm)\n    return np.array([f_e, f_j, f_d, f_p], dtype=float)\n\ndef predict_for_mention(mention, entities, syn_data, weight_vec, tau, abbr_map, stopwords):\n    # Precompute mention normalized and tokens\n    m_norm = normalize_string_for_match(mention, abbr_map)\n    m_tokens = token_set(mention, abbr_map, stopwords)\n    max_score = -1.0\n    best_ids = []\n    for cid, data in syn_data.items():\n        # data: dict with keys 'syn_norms', 'syn_tokens', 'pref_norm'\n        syn_norms = data['syn_norms']\n        syn_tokens = data['syn_tokens']\n        pref_norm = data['pref_norm']\n        # Compute max over synonyms\n        best_c_score = -1.0\n        for s_norm, s_tok in zip(syn_norms, syn_tokens):\n            feats = score_features(m_norm, m_tokens, s_norm, s_tok, pref_norm)\n            s = float(np.dot(weight_vec, feats))\n            if s > best_c_score:\n                best_c_score = s\n        # Track best overall\n        if best_c_score > max_score:\n            max_score = best_c_score\n            best_ids = [cid]\n        elif best_c_score == max_score:\n            best_ids.append(cid)\n    # Decision\n    if max_score >= tau and best_ids:\n        # tie-break lex smallest\n        return sorted(best_ids)[0]\n    else:\n        return None\n\ndef evaluate(weight_vec_raw, tau, entities, mentions, abbr_map, stopwords):\n    w = normalize_weights(weight_vec_raw)\n    # Precompute synonym data\n    syn_data = {}\n    for cid, info in entities.items():\n        syns = info['synonyms']\n        pref = info['preferred']\n        syn_norms = [normalize_string_for_match(s, abbr_map) for s in syns]\n        syn_tokens = [token_set(s, abbr_map, stopwords) for s in syns]\n        pref_norm = normalize_string_for_match(pref, abbr_map)\n        syn_data[cid] = {\n            'syn_norms': syn_norms,\n            'syn_tokens': syn_tokens,\n            'pref_norm': pref_norm\n        }\n    TP = 0\n    FP = 0\n    FN = 0\n    for mention, truth in mentions:\n        pred = predict_for_mention(mention, entities, syn_data, w, tau, abbr_map, stopwords)\n        if truth is None:\n            if pred is not None:\n                FP += 1\n        else:\n            if pred is None:\n                FN += 1\n            elif pred == truth:\n                TP += 1\n            else:\n                FP += 1\n    P = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n    R = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n    F1 = (2 * P * R) / (P + R) if (P + R) > 0 else 0.0\n    return P, R, F1\n\ndef solve():\n    # Define entities and synonyms\n    entities = {\n        \"HGNC:11998\": {\n            \"synonyms\": [\"TP53\", \"p53\", \"tumor protein p53\"],\n            \"preferred\": \"TP53\"\n        },\n        \"HGNC:3236\": {\n            \"synonyms\": [\"EGFR\", \"epidermal growth factor receptor\", \"ERBB1\"],\n            \"preferred\": \"EGFR\"\n        },\n        \"HGNC:391\": {\n            \"synonyms\": [\"AKT1\", \"PKB\", \"RAC-alpha serine/threonine-protein kinase\"],\n            \"preferred\": \"AKT1\"\n        },\n        \"DrugBank:DB00316\": {\n            \"synonyms\": [\"acetaminophen\", \"paracetamol\", \"APAP\"],\n            \"preferred\": \"acetaminophen\"\n        },\n        \"DrugBank:DB00945\": {\n            \"synonyms\": [\"aspirin\", \"acetylsalicylic acid\", \"ASA\"],\n            \"preferred\": \"aspirin\"\n        },\n        \"DrugBank:DB01050\": {\n            \"synonyms\": [\"ibuprofen\"],\n            \"preferred\": \"ibuprofen\"\n        }\n    }\n    # Mentions and ground truth\n    mentions = [\n        (\"TP53\", \"HGNC:11998\"),\n        (\"p53\", \"HGNC:11998\"),\n        (\"tumor prot p53\", \"HGNC:11998\"),\n        (\"p 53\", \"HGNC:11998\"),\n        (\"EGFR\", \"HGNC:3236\"),\n        (\"ERBB1\", \"HGNC:3236\"),\n        (\"erbb-1\", \"HGNC:3236\"),\n        (\"AKT-1\", \"HGNC:391\"),\n        (\"PKB\", \"HGNC:391\"),\n        (\"acetaminophen\", \"DrugBank:DB00316\"),\n        (\"paracetamol\", \"DrugBank:DB00316\"),\n        (\"APAP\", \"DrugBank:DB00316\"),\n        (\"aspirin\", \"DrugBank:DB00945\"),\n        (\"ASA\", \"DrugBank:DB00945\"),\n        (\"acetylsalicylic acid\", \"DrugBank:DB00945\"),\n        (\"ibuprofen\", \"DrugBank:DB01050\"),\n        (\"ibuprfen\", \"DrugBank:DB01050\"),\n        (\"randomword\", None),\n        (\"EGFR inhibitor\", None),\n    ]\n    # Abbreviation map and stopwords\n    abbr_map = {\n        \"asa\": \"acetylsalicylic acid\",\n        \"apap\": \"acetaminophen\",\n    }\n    stopwords = {\n        \"protein\", \"receptor\", \"acid\", \"serine\", \"threonine\", \"alpha\", \"beta\", \"inhibitor\"\n    }\n    # Test cases: (weights, tau)\n    test_cases = [\n        ([0.6, 0.1, 0.2, 0.1], 0.7),\n        ([0.95, 0.0, 0.05, 0.0], 0.95),\n        ([0.25, 0.35, 0.35, 0.05], 0.6),\n        ([1.0, 0.0, 0.0, 0.0], 1.0),\n        ([2.0, 0.5, 0.5, 0.0], 0.65),\n    ]\n    results = []\n    for w, tau in test_cases:\n        P, R, F1 = evaluate(w, tau, entities, mentions, abbr_map, stopwords)\n        # Round to four decimals\n        results.extend([round(P + 1e-12, 4), round(R + 1e-12, 4), round(F1 + 1e-12, 4)])\n    # Format as requested: single line, comma-separated in brackets.\n    # Ensure fixed 4-decimal formatting.\n    formatted = \"[\" + \",\".join(f\"{x:.4f}\" for x in results) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "4577572"}, {"introduction": "Once a knowledge graph is assembled from triples, its structure can offer powerful biomedical insights. This practice explores how to analyze the graph's topology by computing basic network statistics like node degrees. By examining the degree distribution of drug nodes [@problem_id:4577536], you will learn how to identify highly connected \"hub\" entities and connect this structural property to the important pharmacological principle of polypharmacology.", "problem": "A biomedical Resource Description Framework (RDF) knowledge graph consists of typed nodes and directed edges represented as triples of the form $(\\text{subject}, \\text{predicate}, \\text{object})$. Consider the following small, scientifically plausible set of triples capturing Drug–Protein binding, Drug–Disease indication, and Drug–Pathway participation, along with relations pointing into Drugs:\n\n$(D\\_A, \\text{binds}, P1)$; $(D\\_A, \\text{binds}, P2)$; $(D\\_A, \\text{binds}, P3)$; $(D\\_A, \\text{treats}, Dis1)$; $(D\\_A, \\text{participates\\_in}, PW1)$\n\n$(D\\_B, \\text{binds}, P2)$; $(D\\_B, \\text{binds}, P3)$; $(D\\_B, \\text{treats}, Dis1)$; $(D\\_B, \\text{treats}, Dis2)$\n\n$(D\\_C, \\text{binds}, P4)$; $(D\\_C, \\text{binds}, P5)$\n\n$(D\\_D, \\text{binds}, P1)$; $(D\\_D, \\text{binds}, P2)$; $(D\\_D, \\text{binds}, P3)$; $(D\\_D, \\text{binds}, P4)$; $(D\\_D, \\text{treats}, Dis2)$; $(D\\_D, \\text{treats}, Dis3)$\n\n$(D\\_E, \\text{binds}, P1)$\n\n$(D\\_F, \\text{treats}, Dis3)$\n\n$(P1, \\text{inhibited\\_by}, D\\_A)$; $(Dis1, \\text{has\\_contraindicated\\_drug}, D\\_B)$; $(Dis2, \\text{has\\_indicated\\_drug}, D\\_D)$; $(PW1, \\text{has\\_participating\\_drug}, D\\_A)$; $(P3, \\text{activated\\_by}, D\\_D)$; $(Dis3, \\text{has\\_contraindicated\\_drug}, D\\_A)$; $(P4, \\text{inhibited\\_by}, D\\_C)$.\n\nFor each Drug node $d \\in \\{D\\_A, D\\_B, D\\_C, D\\_D, D\\_E, D\\_F\\}$, define the out-degree $k_{\\text{out}}(d)$ as the number of triples in which $d$ appears as the subject, and the in-degree $k_{\\text{in}}(d)$ as the number of triples in which $d$ appears as the object. Construct the empirical out-degree distribution $p_{\\text{out}}(k)$ over Drug nodes by the standard relative-frequency definition, and likewise the empirical in-degree distribution $p_{\\text{in}}(k)$.\n\nAssume that the out-degree tail for Drug nodes follows a continuous power-law above the threshold $x_{\\min} = 2$, that is, for $k \\ge x_{\\min}$ the tail density obeys $p(k) \\propto k^{-\\alpha}$ for some exponent $\\alpha$. Using only fundamental definitions of probability densities and the maximum likelihood principle, derive the maximum likelihood estimator for $\\alpha$ for the continuous power-law tail and compute its value for the Drug out-degrees in this graph with $x_{\\min} = 2$. Round your final numerical estimate of $\\alpha$ to $4$ significant figures. In your reasoning, interpret what a heavy-tailed $p_{\\text{out}}(k)$ implies about polypharmacology in this dataset, grounded in the constructed distributions. Express the final numerical answer without units.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\nThe provided data consists of a set of RDF triples, definitions for node degrees, and a modeling assumption for the out-degree distribution.\n\n**RDF Triples (Edges):**\n*   $(D\\_A, \\text{binds}, P1)$; $(D\\_A, \\text{binds}, P2)$; $(D\\_A, \\text{binds}, P3)$; $(D\\_A, \\text{treats}, Dis1)$; $(D\\_A, \\text{participates\\_in}, PW1)$\n*   $(D\\_B, \\text{binds}, P2)$; $(D\\_B, \\text{binds}, P3)$; $(D\\_B, \\text{treats}, Dis1)$; $(D\\_B, \\text{treats}, Dis2)$\n*   $(D\\_C, \\text{binds}, P4)$; $(D\\_C, \\text{binds}, P5)$\n*   $(D\\_D, \\text{binds}, P1)$; $(D\\_D, \\text{binds}, P2)$; $(D\\_D, \\text{binds}, P3)$; $(D\\_D, \\text{binds}, P4)$; $(D\\_D, \\text{treats}, Dis2)$; $(D\\_D, \\text{treats}, Dis3)$\n*   $(D\\_E, \\text{binds}, P1)$\n*   $(D\\_F, \\text{treats}, Dis3)$\n*   $(P1, \\text{inhibited\\_by}, D\\_A)$; $(Dis1, \\text{has\\_contraindicated\\_drug}, D\\_B)$; $(Dis2, \\text{has\\_indicated\\_drug}, D\\_D)$; $(PW1, \\text{has\\_participating\\_drug}, D\\_A)$; $(P3, \\text{activated\\_by}, D\\_D)$; $(Dis3, \\text{has\\_contraindicated\\_drug}, D\\_A)$; $(P4, \\text{inhibited\\_by}, D\\_C)$\n\n**Definitions and Variables:**\n*   Set of Drug nodes: $d \\in \\{D\\_A, D\\_B, D\\_C, D\\_D, D\\_E, D\\_F\\}$\n*   Out-degree $k_{\\text{out}}(d)$: Number of triples where $d$ is the subject.\n*   In-degree $k_{\\text{in}}(d)$: Number of triples where $d$ is the object.\n*   Empirical distributions $p_{\\text{out}}(k)$ and $p_{\\text{in}}(k)$ are defined by relative frequencies.\n\n**Modeling Assumptions and Tasks:**\n*   The out-degree tail for Drug nodes follows a continuous power-law for $k \\ge x_{\\min}$.\n*   The power-law probability density function (PDF) is $p(k) \\propto k^{-\\alpha}$.\n*   The minimum value for the tail is $x_{\\min} = 2$.\n*   Derive the maximum likelihood estimator (MLE) for $\\alpha$.\n*   Compute the value of $\\alpha$ for the drug out-degrees, rounded to $4$ significant figures.\n*   Interpret the results in the context of polypharmacology.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in network theory and bioinformatics. The concepts of RDF knowledge graphs, degree distributions, power laws, and maximum likelihood estimation are standard. The application to polypharmacology is a well-established area of research. The problem is well-posed, with all necessary data and definitions provided to arrive at a unique solution. The language is objective and formal. The assumption of a continuous power-law distribution for a discrete quantity (node degree) is a standard and widely accepted approximation used in the derivation of the estimator, particularly for simplifying the calculus. It does not constitute a scientific flaw but is a specified modeling choice. Therefore, the problem is deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution Derivation\n\nFirst, we compute the out-degrees and in-degrees for each drug node based on the provided triples.\n\n**Out-Degrees ($k_{\\text{out}}$):**\n*   $k_{\\text{out}}(D\\_A) = 5$ (subject of $5$ triples)\n*   $k_{\\text{out}}(D\\_B) = 4$ (subject of $4$ triples)\n*   $k_{\\text{out}}(D\\_C) = 2$ (subject of $2$ triples)\n*   $k_{\\text{out}}(D\\_D) = 6$ (subject of $6$ triples)\n*   $k_{\\text{out}}(D\\_E) = 1$ (subject of $1$ triple)\n*   $k_{\\text{out}}(D\\_F) = 1$ (subject of $1$ triple)\nThe set of out-degrees is $\\{5, 4, 2, 6, 1, 1\\}$. The total number of drug nodes is $N=6$.\nThe empirical out-degree distribution $p_{\\text{out}}(k)$ is:\n*   $p_{\\text{out}}(1) = \\frac{2}{6} = \\frac{1}{3}$\n*   $p_{\\text{out}}(2) = \\frac{1}{6}$\n*   $p_{\\text{out}}(4) = \\frac{1}{6}$\n*   $p_{\\text{out}}(5) = \\frac{1}{6}$\n*   $p_{\\text{out}}(6) = \\frac{1}{6}$\n*   $p_{\\text{out}}(k) = 0$ for other values of $k$.\n\n**In-Degrees ($k_{\\text{in}}$):**\n*   $k_{\\text{in}}(D\\_A) = 3$ (object of $3$ triples)\n*   $k_{\\text{in}}(D\\_B) = 1$ (object of $1$ triple)\n*   $k_{\\text{in}}(D\\_C) = 1$ (object of $1$ triple)\n*   $k_{\\text{in}}(D\\_D) = 2$ (object of $2$ triples)\n*   $k_{\\text{in}}(D\\_E) = 0$ (object of $0$ triples)\n*   $k_{\\text{in}}(D\\_F) = 0$ (object of $0$ triples)\nThe set of in-degrees is $\\{3, 1, 1, 2, 0, 0\\}$.\nThe empirical in-degree distribution $p_{\\text{in}}(k)$ is:\n*   $p_{\\text{in}}(0) = \\frac{2}{6} = \\frac{1}{3}$\n*   $p_{\\text{in}}(1) = \\frac{2}{6} = \\frac{1}{3}$\n*   $p_{\\text{in}}(2) = \\frac{1}{6}$\n*   $p_{\\text{in}}(3) = \\frac{1}{6}$\n*   $p_{\\text{in}}(k) = 0$ for other values of $k$.\n\nNext, we derive the maximum likelihood estimator for the exponent $\\alpha$ of a continuous power-law distribution. The PDF is given by $p(k) \\propto k^{-\\alpha}$ for $k \\ge x_{\\min}$.\nFirst, we must normalize the PDF. Let $p(k) = Ck^{-\\alpha}$.\n$$ \\int_{x_{\\min}}^{\\infty} Ck^{-\\alpha} \\, dk = 1 $$\n$$ C \\left[ \\frac{k^{-\\alpha+1}}{1-\\alpha} \\right]_{x_{\\min}}^{\\infty} = 1 $$\nFor convergence, we require $1-\\alpha < 0$, or $\\alpha > 1$. Under this condition, the term at $\\infty$ is $0$.\n$$ C \\left( 0 - \\frac{x_{\\min}^{1-\\alpha}}{1-\\alpha} \\right) = 1 \\implies C = (\\alpha-1)x_{\\min}^{\\alpha-1} $$\nThe normalized PDF is $p(k|\\alpha, x_{\\min}) = (\\alpha-1)x_{\\min}^{\\alpha-1} k^{-\\alpha}$.\n\nGiven a set of $n$ observations $\\{k_i\\}$ where each $k_i \\ge x_{\\min}$, the likelihood function is:\n$$ L(\\alpha) = \\prod_{i=1}^{n} p(k_i|\\alpha, x_{\\min}) = \\prod_{i=1}^{n} (\\alpha-1)x_{\\min}^{\\alpha-1} k_i^{-\\alpha} $$\nThe log-likelihood $\\mathcal{L}(\\alpha) = \\ln(L(\\alpha))$ is easier to maximize:\n$$ \\mathcal{L}(\\alpha) = \\sum_{i=1}^{n} \\ln\\left( (\\alpha-1)x_{\\min}^{\\alpha-1} k_i^{-\\alpha} \\right) $$\n$$ \\mathcal{L}(\\alpha) = \\sum_{i=1}^{n} \\left[ \\ln(\\alpha-1) + (\\alpha-1)\\ln(x_{\\min}) - \\alpha\\ln(k_i) \\right] $$\n$$ \\mathcal{L}(\\alpha) = n\\ln(\\alpha-1) + n(\\alpha-1)\\ln(x_{\\min}) - \\alpha\\sum_{i=1}^{n}\\ln(k_i) $$\nTo find the maximum, we set the derivative with respect to $\\alpha$ to zero:\n$$ \\frac{d\\mathcal{L}}{d\\alpha} = \\frac{n}{\\alpha-1} + n\\ln(x_{\\min}) - \\sum_{i=1}^{n}\\ln(k_i) = 0 $$\nSolving for the estimator $\\hat{\\alpha}$:\n$$ \\frac{n}{\\hat{\\alpha}-1} = \\sum_{i=1}^{n}\\ln(k_i) - n\\ln(x_{\\min}) = \\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{x_{\\min}}\\right) $$\n$$ \\hat{\\alpha}-1 = n \\left( \\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{x_{\\min}}\\right) \\right)^{-1} $$\n$$ \\hat{\\alpha} = 1 + n \\left( \\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{x_{\\min}}\\right) \\right)^{-1} $$\nThis is the MLE for $\\alpha$.\n\nNow, we apply this estimator to the drug out-degree data. The set of all out-degrees is $\\{5, 4, 2, 6, 1, 1\\}$. The threshold is $x_{\\min} = 2$. We select the data points $k_i \\ge x_{\\min}$: $\\{5, 4, 2, 6\\}$. The number of these data points is $n=4$.\nWe compute the sum in the denominator:\n$$ \\sum_{i=1}^{4} \\ln\\left(\\frac{k_i}{x_{\\min}}\\right) = \\ln\\left(\\frac{5}{2}\\right) + \\ln\\left(\\frac{4}{2}\\right) + \\ln\\left(\\frac{2}{2}\\right) + \\ln\\left(\\frac{6}{2}\\right) $$\n$$ = \\ln(2.5) + \\ln(2) + \\ln(1) + \\ln(3) $$\nUsing the property $\\ln(a) + \\ln(b) = \\ln(ab)$ and that $\\ln(1)=0$:\n$$ = \\ln(2.5 \\times 2 \\times 3) = \\ln(15) $$\nSubstituting this into the MLE formula:\n$$ \\hat{\\alpha} = 1 + \\frac{4}{\\ln(15)} $$\nNumerically, $\\ln(15) \\approx 2.70805$.\n$$ \\hat{\\alpha} \\approx 1 + \\frac{4}{2.70805} \\approx 1 + 1.47707 \\approx 2.47707 $$\nRounding to $4$ significant figures, we get $\\hat{\\alpha} = 2.477$.\n\n**Interpretation:**\nThe out-degree $k_{\\text{out}}(d)$ of a drug $d$ represents the number of relationships originating from it, such as binding to proteins, treating diseases, or participating in pathways. A heavy-tailed out-degree distribution, as indicated by a power-law with exponent $\\hat{\\alpha} \\approx 2.477$ (a value typical for real-world networks), implies that most drugs have few connections, but a few \"hub\" drugs have a very large number of connections. In this dataset, drugs $D\\_A$ ($k_{\\text{out}}=5$) and $D\\_D$ ($k_{\\text{out}}=6$) are such hubs.\nPolypharmacology is the phenomenon where a single drug acts on multiple molecular targets. The 'binds' relationship is a direct representation of this. Drugs $D\\_A$ and $D\\_D$ bind to $3$ and $4$ proteins, respectively, exemplifying polypharmacology. The heavy-tailed nature of $p_{\\text{out}}(k)$ suggests that this polypharmacology is not evenly distributed but is a characteristic of a select few highly connected drugs. This has profound implications for drug discovery, suggesting that some drugs may achieve efficacy through multi-target engagement, but may also have a higher propensity for off-target side effects.", "answer": "$$\n\\boxed{2.477}\n$$", "id": "4577536"}, {"introduction": "The scientific value of a computational result rests on its reproducibility. This final practice elevates the construction process from a simple script to a formally reproducible scientific workflow. You will learn how to design a deterministic Extract-Transform-Load (ETL) pipeline [@problem_id:4577595] that incorporates explicit versioning and cryptographic hashing to create a verifiable \"provenance digest,\" ensuring that your knowledge graph construction is robust, transparent, and repeatable.", "problem": "You are given the task of designing a deterministic Extract-Transform-Load workflow for constructing a small biomedical knowledge graph. The goal is to formalize reproducibility using deterministic algorithms, explicit versioning metadata, and a cryptographic checksum. The workflow must be modeled as a directed acyclic graph with nodes representing tasks and edges representing data dependencies. The workflow components are: Extract, Transform, Load. The inputs are small tabular datasets of biomedical relations and a table of synonyms that normalizes labels. The output is a knowledge graph with typed nodes and typed edges, and a provenance-aware digest that encodes both the graph content and explicit version metadata.\n\nFundamental base to use:\n- Extract-Transform-Load: The workflow consists of extracting data, transforming it to conform to schema and normalization, and loading it into a graph representation. A knowledge graph is a directed labeled multigraph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ where $\\mathcal{V}$ is a set of entities and $\\mathcal{E}\\subseteq \\mathcal{V}\\times\\mathcal{R}\\times\\mathcal{V}$ is a set of edges labeled by relations from $\\mathcal{R}$.\n- Determinism: For reproducibility, a deterministic function $f$ must satisfy $x=y\\Rightarrow f(x)=f(y)$ for the same inputs and fixed versions, and must be insensitive to row permutations in tabular inputs.\n- Workflow ordering: A Directed Acyclic Graph (DAG) $W=(\\mathcal{T},\\mathcal{D})$ with tasks $\\mathcal{T}=\\{\\text{Extract},\\text{Transform},\\text{Load}\\}$ and data dependencies $\\mathcal{D}\\subseteq \\mathcal{T}\\times\\mathcal{T}$ defines a topological order where predecessors must complete before successors.\n- Cryptographic hashing: Use a cryptographic hash function $H$ (Secure Hash Algorithm $256$) modeled as $H:\\{0,1\\}^\\star\\to\\{0,1\\}^{256}$ to compute a digest. This function is collision-resistant and sensitive to any bit change in its input.\n\nDefinitions required for the program:\n- Canonicalization: Define a canonicalization function $C:\\mathcal{S}\\to\\mathcal{S}$ on strings that lowercases, removes punctuation, and collapses whitespace. Let $M:\\mathcal{S}\\to\\mathcal{S}$ represent synonym mapping provided by an input table. The normalized label is $L(s)=M(C(s))$ if $M(C(s))$ exists, else $C(s)$.\n- Knowledge graph schema: There are three entity types: Gene, Disease, Drug. There are three relation types: associates (Gene-Disease), targets (Drug-Gene), treats (Drug-Disease). The transformed graph must include nodes labeled by their type-suffixed canonical names, e.g., $\\text{\"Gene|tp53\"}$. Edges are typed triples $\\text{\"associates|gene|disease\"}$, $\\text{\"targets|drug|gene\"}$, $\\text{\"treats|drug|disease\"}$ using canonical labels. Duplicates must be removed; the representation must be order-insensitive.\n- Versioning: Each task is parameterized by explicit version identifiers for the container image and for the code, and each dataset has an explicit version identifier. Denote task container version strings $c_{\\text{extract}},c_{\\text{transform}},c_{\\text{load}}$, code version strings $s_{\\text{extract}},s_{\\text{transform}},s_{\\text{load}}$, dataset versions $d_{\\text{GD}}$ (gene-disease), $d_{\\text{DT}}$ (drug-target), $d_{\\text{DD}}$ (drug-disease), $d_{\\text{Syn}}$ (synonyms). The workflow is valid only if all these strings are non-empty and none equals the placeholder $\\text{\"latest\"}$ in any case variant.\n- Provenance: Construct a provenance string $P$ as the concatenation of all explicit versions in a fixed canonical order (lexicographically by key), i.e., $P=\\bigoplus_{k\\in K}{\\text{key}(k)\\Vert\\text{version}(k)}$, where $\\oplus$ denotes concatenation and $\\Vert$ denotes a delimiter. Let the graph content string be $G_c$, a concatenation of sorted nodes and sorted edges in a fixed format. Define the final digest $D=H(P\\Vert G_c)$. Reproducibility holds for a pair of runs if and only if both runs have valid explicit versions and identical $D$.\n\nYour program must:\n- Implement deterministic canonicalization $C$, synonym application $M$, and graph construction to produce $G_c$.\n- Construct $P$ from container, code, and dataset versions; enforce explicit version validity.\n- Compute $D$ using Secure Hash Algorithm $256$ over $P\\Vert G_c$.\n- For each test case, evaluate reproducibility by running the workflow twice (Run A and Run B), and compute a boolean result indicating whether both runs are valid and $D_{\\text{A}}=D_{\\text{B}}$.\n\nTest suite and parameters:\nFor each test case $i\\in\\{1,2,3,4\\}$, you are given Run A and Run B parameters. Each run specifies four datasets and two version maps. Each dataset provides a version string and a list of rows. The container and code version maps provide version strings for Extract, Transform, and Load.\n\n- Test case $1$ (happy path; permutation invariance):\n  - Run A datasets:\n    - gene-disease $d_{\\text{GD}}=\\text{\"gd:v1\"}$, rows: $[(\\text{\"TP53\"},\\text{\"Breast Cancer\"}),(\\text{\"BRCA1\"},\\text{\"Breast carcinoma\"})]$.\n    - drug-target $d_{\\text{DT}}=\\text{\"dt:v1\"}$, rows: $[(\\text{\"Tamoxifen\"},\\text{\"ESR1\"}),(\\text{\"Trastuzumab\"},\\text{\"ERBB2\"})]$.\n    - drug-disease $d_{\\text{DD}}=\\text{\"dd:v1\"}$, rows: $[(\\text{\"Tamoxifen\"},\\text{\"Breast Cancer\"}),(\\text{\"Trastuzumab\"},\\text{\"Breast carcinoma\"})]$.\n    - synonyms $d_{\\text{Syn}}=\\text{\"syn:v1\"}$, rows: $[(\\text{\"breast carcinoma\"},\\text{\"breast cancer\"}),(\\text{\"her2-positive\"},\\text{\"her2 positive\"})]$.\n  - Run A versions: containers $(c_{\\text{extract}},c_{\\text{transform}},c_{\\text{load}})=(\\text{\"ctrE:1.0\"},\\text{\"ctrT:1.1\"},\\text{\"ctrL:1.0\"})$, codes $(s_{\\text{extract}},s_{\\text{transform}},s_{\\text{load}})=(\\text{\"codeE:abc\"},\\text{\"codeT:def\"},\\text{\"codeL:ghi\"})$.\n  - Run B datasets: identical content as Run A but each table’s rows are permuted and one duplicate is introduced: gene-disease rows $[(\\text{\"BRCA1\"},\\text{\"Breast carcinoma\"}),(\\text{\"TP53\"},\\text{\"Breast Cancer\"})]$, drug-target rows $[(\\text{\"Trastuzumab\"},\\text{\"ERBB2\"}),(\\text{\"Tamoxifen\"},\\text{\"ESR1\"})]$, drug-disease rows $[(\\text{\"Trastuzumab\"},\\text{\"Breast carcinoma\"}),(\\text{\"Tamoxifen\"},\\text{\"Breast Cancer\"}),(\\text{\"Tamoxifen\"},\\text{\"Breast Cancer\"})]$, synonyms rows $[(\\text{\"her2-positive\"},\\text{\"her2 positive\"}),(\\text{\"breast carcinoma\"},\\text{\"breast cancer\"})]$.\n  - Run B versions: identical to Run A.\n  - Expected output for this case: boolean indicating reproducibility under permutation and duplicate removal.\n\n- Test case $2$ (version change boundary):\n  - Run A: identical to Test case $1$ Run A.\n  - Run B: identical to Test case $1$ Run A except synonyms dataset version changes to $d_{\\text{Syn}}=\\text{\"syn:v2\"}$ (rows unchanged).\n  - Expected output: boolean indicating non-reproducibility due to version change even when content is unchanged.\n\n- Test case $3$ (invalid due to missing explicit container version):\n  - Run A: identical to Test case $1$ Run A except $c_{\\text{transform}}=\\text{\"\"}$ (empty string).\n  - Run B: identical to Test case $1$ Run A.\n  - Expected output: boolean indicating non-reproducibility because explicit version validity fails.\n\n- Test case $4$ (code version change):\n  - Run A: identical to Test case $1$ Run A.\n  - Run B: identical to Test case $1$ Run A except $s_{\\text{load}}=\\text{\"codeL:xyz\"}$.\n  - Expected output: boolean indicating non-reproducibility due to code version change.\n\nAngle units, physical units, and percentages do not apply. All outputs must be pure booleans. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\text{\"[result1,result2,result3,result4]\"}$) where each $result_i$ is a boolean $\\in\\{\\text{True},\\text{False}\\}$ computed for the corresponding test case.", "solution": "The problem of designing a reproducible biomedical knowledge graph construction workflow is reviewed and validated as scientifically grounded, well-posed, and objective. The problem statement provides a comprehensive and formalizable specification for an Extract-Transform-Load (ETL) pipeline, leveraging established principles of deterministic algorithms, explicit versioning, and cryptographic hashing to ensure reproducibility. The problem is valid and can be solved as specified.\n\nThe solution involves implementing a deterministic function that maps a set of versioned, tabular inputs to a final cryptographic digest. This digest uniquely represents both the content of the constructed knowledge graph and the full provenance of the software, code, and data used in its creation. Two runs of the workflow are deemed reproducible if and only if their respective version metadata are valid and their final digests are identical.\n\nThe overall process is modeled as a function, $\\text{ComputeDigest}$, which maps the set of input datasets and version metadata to a digest string:\n$$ \\text{ComputeDigest}(\\{\\text{datasets}\\}, \\{\\text{versions}\\}) \\to D \\in \\{0, 1\\}^{256} $$\nThe core of the solution is to implement this function and then use it to evaluate the reproducibility for each test case. The implementation is divided into three main stages, mirroring the conceptual tasks of version validation, data transformation, and digest computation.\n\n### 1. Version Validation and Provenance String ($P$) Construction\n\nThe first step is to validate the versioning metadata and construct a canonical provenance string, $P$. Reproducibility requires that all components be explicitly versioned.\n\n**Validation:**\nA run is considered valid only if all version strings provided for datasets ($d_{\\text{GD}}, d_{\\text{DT}}, d_{\\text{DD}}, d_{\\text{Syn}}$), container images ($c_{\\text{extract}}, c_{\\text{transform}}, c_{\\text{load}}$), and code ($s_{\\text{extract}}, s_{\\text{transform}}, s_{\\text{load}}$) are non-empty and do not equal the placeholder string `\"latest\"`. If any version string fails this check, the workflow for that run is invalid, and reproducibility is impossible by definition.\n\n**Provenance String $P$:**\nIf the versions are valid, they are aggregated into a single, canonical provenance string $P$. This is achieved by:\n1.  Creating a set of key-value pairs for all $10$ version identifiers. The keys are strings like `\"d_GD\"`, `\"c_extract\"`, etc.\n2.  Sorting these pairs lexicographically by key.\n3.  For each sorted pair $(k, v)$, a string is formed as $k \\Vert v$, where $\\Vert$ is the literal `|` character.\n4.  The final provenance string $P$ is constructed by concatenating these individual strings, separated by a newline character (`\n`). This operation is denoted by $\\bigoplus$ in the problem, such that $P = \\bigoplus_{i=1}^{10} (k_i \\Vert v_i)$.\n\nThis process ensures that $P$ is a unique, order-independent representation of the complete versioning metadata.\n\n### 2. Data Transformation and Graph Content String ($G_c$) Construction\n\nThis stage corresponds to the Transform and Load steps of the ETL workflow. It processes the input relational data into a canonical knowledge graph representation, $G_c$.\n\n**Entity Normalization:**\nTo handle variations in entity naming (e.g., `\"Breast Cancer\"` vs. `\"Breast carcinoma\"`), a deterministic normalization function, $L(s)$, is applied to all entity labels.\n\n1.  **Canonicalization ($C(s)$):** A function $C: \\mathcal{S} \\to \\mathcal{S}$ is defined to transform any string $s$ into a canonical form. This involves lowercasing the string, removing all punctuation characters (any character not in `a-z`, `0-9`, or whitespace), and collapsing multiple whitespace characters into a single space, and trimming any leading/trailing space.\n2.  **Synonym Mapping ($M(s)$):** A synonym map $M: \\mathcal{S} \\to \\mathcal{S}$ is built from the input synonyms table. Both the synonym and its canonical equivalent from the table are first passed through the function $C(s)$ before being inserted into a dictionary.\n3.  **Normalization ($L(s)$):** The final normalized label is given by $L(s) = M(C(s))$ if a mapping for $C(s)$ exists in $M$; otherwise, $L(s) = C(s)$.\n\n**Knowledge Graph Construction ($\\mathcal{G}$):**\nThe knowledge graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ is constructed from the input relational data:\n-   **Gene-Disease Relations:** For each row $(g, d)$ in the gene-disease table, we generate two nodes, `\"Gene|\" \\oplus L(g)$ and `\"Disease|\" \\oplus L(d)$, and one edge tuple $(\\text{\"Gene|\" \\oplus L(g)}, \\text{\"associates\"}, \\text{\"Disease|\" \\oplus L(d)})$.\n-   **Drug-Target Relations:** For each row $(dr, g)$ in the drug-target table, we generate nodes `\"Drug|\" \\oplus L(dr)$ and `\"Gene|\" \\oplus L(g)$, and an edge tuple $(\\text{\"Drug|\" \\oplus L(dr)}, \\text{\"targets\"}, \\text{\"Gene|\" \\oplus L(g)})$.\n-   **Drug-Disease Relations:** For each row $(dr, d)$ in the drug-disease table, we generate nodes `\"Drug|\" \\oplus L(dr)$ and `\"Disease|\" \\oplus L(d)$, and an edge tuple $(\\text{\"Drug|\" \\oplus L(dr)}, \\text{\"treats\"}, \\text{\"Disease|\" \\oplus L(d)})$.\n\nTo ensure the process is insensitive to input row order and immune to duplicates, the generated node strings are stored in a set $\\mathcal{V}$, and the edge tuples are stored in a set $\\mathcal{E}$.\n\n**Graph Content String ($G_c$):**\nTo create a canonical string representation of the graph, $G_c$:\n1.  The set of nodes $\\mathcal{V}$ is converted to a list of strings, which is then lexicographically sorted.\n2.  The set of edges $\\mathcal{E}$ is converted to a list of string representations. Each edge tuple $(u, r, v) \\in \\mathcal{E}$ is formatted as the string $u \\Vert r \\Vert v$, using `|` as the delimiter $\\Vert$. This list is also lexicographically sorted.\n3.  The final string $G_c$ is formed by joining the sorted node strings with `\n`, followed by a `\n` separator, followed by the sorted edge strings joined with `\n`. This fixed format ensures a deterministic representation of the graph content.\n\n### 3. Final Digest ($D$) Computation and Reproducibility Check\n\nThe final step securely binds the provenance to the content and evaluates reproducibility.\n\n**Digest Calculation:**\nThe final digest $D$ is the SHA-256 hash of the full canonical string representation. The input to the hash function $H$ is the concatenation of the provenance string $P$ and the graph content string $G_c$, separated by a newline character.\n$$ D = H(P \\oplus '\\backslash n' \\oplus G_c) $$\nThis ensures that any change, whether to the input data, the normalization logic, or the version metadata, will result in a different digest.\n\n**Reproducibility Evaluation:**\nFor each test case, the workflow is executed for Run A and Run B, producing digests $D_A$ and $D_B$ and validity statuses $V_A$ and $V_B$. The reproducibility condition is met if and only if both runs are valid and their digests are identical. The boolean result for the test case is thus:\n$$ \\text{is_reproducible} = (V_A \\land V_B) \\land (D_A = D_B) $$\nThis strict definition correctly identifies deviations in either provenance or content, fulfilling the requirements for a formal, verifiable, and reproducible scientific workflow.", "answer": "```python\nimport hashlib\nimport re\nfrom typing import List, Tuple, Dict, Set, Any, Optional\n\ndef solve():\n    \"\"\"\n    Main function to execute the reproducibility checks for all test cases.\n    \"\"\"\n\n    # Helper function for canonicalizing strings\n    def canonicalize(s: str) -> str:\n        s = s.lower()\n        s = re.sub(r'[^a-z0-9\\s]', '', s)\n        s = re.sub(r'\\s+', ' ', s).strip()\n        return s\n\n    def run_workflow(\n        datasets: Dict[str, Tuple[str, List[Tuple[str, str]]]],\n        versions: Dict[str, str]\n    ) -> Tuple[bool, Optional[str]]:\n        \"\"\"\n        Executes the full ETL workflow for a single run and returns its validity and digest.\n\n        Args:\n            datasets: A dictionary mapping dataset names to (version, data rows).\n            versions: A dictionary of container and code versions.\n\n        Returns:\n            A tuple (is_valid, digest_hex), where is_valid is a boolean and\n            digest_hex is the hex string of the SHA-256 digest, or None if invalid.\n        \"\"\"\n        # 1. Version Validation and Provenance String (P) Construction\n        all_versions = versions.copy()\n        for name, (version, _) in datasets.items():\n            all_versions[f\"d_{name}\"] = version\n\n        is_valid = True\n        for key, val in all_versions.items():\n            if not val or val.lower() == \"latest\":\n                is_valid = False\n                break\n        \n        if not is_valid:\n            return False, None\n\n        provenance_items = []\n        for key in sorted(all_versions.keys()):\n            provenance_items.append(f\"{key}|{all_versions[key]}\")\n        \n        provenance_string = \"\\n\".join(provenance_items)\n\n        # 2. Data Transformation and Graph Content String (Gc) Construction\n        \n        # Build synonym map M\n        synonym_map: Dict[str, str] = {}\n        _, syn_rows = datasets[\"Syn\"]\n        for key, value in syn_rows:\n            synonym_map[canonicalize(key)] = canonicalize(value)\n\n        def normalize_label(s: str) -> str:\n            c_s = canonicalize(s)\n            return synonym_map.get(c_s, c_s)\n\n        nodes: Set[str] = set()\n        edges: Set[Tuple[str, str, str]] = set()\n\n        # Process gene-disease\n        _, gd_rows = datasets[\"GD\"]\n        for g, d in set(gd_rows): # Use set to handle duplicates\n            norm_g = normalize_label(g)\n            norm_d = normalize_label(d)\n            node_g = f\"Gene|{norm_g}\"\n            node_d = f\"Disease|{norm_d}\"\n            nodes.add(node_g)\n            nodes.add(node_d)\n            edges.add((node_g, \"associates\", node_d))\n\n        # Process drug-target\n        _, dt_rows = datasets[\"DT\"]\n        for dr, g in set(dt_rows):\n            norm_dr = normalize_label(dr)\n            norm_g = normalize_label(g)\n            node_dr = f\"Drug|{norm_dr}\"\n            node_g = f\"Gene|{norm_g}\"\n            nodes.add(node_dr)\n            nodes.add(node_g)\n            edges.add((node_dr, \"targets\", node_g))\n\n        # Process drug-disease\n        _, dd_rows = datasets[\"DD\"]\n        for dr, d in set(dd_rows):\n            norm_dr = normalize_label(dr)\n            norm_d = normalize_label(d)\n            node_dr = f\"Drug|{norm_dr}\"\n            node_d = f\"Disease|{norm_d}\"\n            nodes.add(node_dr)\n            nodes.add(node_d)\n            edges.add((node_dr, \"treats\", node_d))\n            \n        sorted_nodes = sorted(list(nodes))\n        \n        # Sort edges based on subject, predicate, object\n        sorted_edges_str = sorted([f\"{s}|{p}|{o}\" for s, p, o in edges])\n        \n        node_str = \"\\n\".join(sorted_nodes)\n        edge_str = \"\\n\".join(sorted_edges_str)\n        \n        graph_content_string = f\"{node_str}\\n\\n{edge_str}\"\n\n        # 3. Final Digest (D) Computation\n        full_string_to_hash = f\"{provenance_string}\\n{graph_content_string}\"\n        \n        digest = hashlib.sha256(full_string_to_hash.encode('utf-8')).hexdigest()\n        \n        return True, digest\n\n    # --- Test Cases Definition ---\n\n    # Common data for Test Case 1 Run A\n    base_datasets = {\n        \"GD\": (\"gd:v1\", [(\"TP53\", \"Breast Cancer\"), (\"BRCA1\", \"Breast carcinoma\")]),\n        \"DT\": (\"dt:v1\", [(\"Tamoxifen\", \"ESR1\"), (\"Trastuzumab\", \"ERBB2\")]),\n        \"DD\": (\"dd:v1\", [(\"Tamoxifen\", \"Breast Cancer\"), (\"Trastuzumab\", \"Breast carcinoma\")]),\n        \"Syn\": (\"syn:v1\", [(\"breast carcinoma\", \"breast cancer\"), (\"her2-positive\", \"her2 positive\")]),\n    }\n    base_versions = {\n        \"c_extract\": \"ctrE:1.0\", \"c_transform\": \"ctrT:1.1\", \"c_load\": \"ctrL:1.0\",\n        \"s_extract\": \"codeE:abc\", \"s_transform\": \"codeT:def\", \"s_load\": \"codeL:ghi\",\n    }\n\n    test_cases = [\n        # Test Case 1: Happy path, permutation invariance\n        {\n            \"run_a\": {\"datasets\": base_datasets, \"versions\": base_versions},\n            \"run_b\": {\n                \"datasets\": {\n                    \"GD\": (\"gd:v1\", [(\"BRCA1\", \"Breast carcinoma\"), (\"TP53\", \"Breast Cancer\")]),\n                    \"DT\": (\"dt:v1\", [(\"Trastuzumab\", \"ERBB2\"), (\"Tamoxifen\", \"ESR1\")]),\n                    \"DD\": (\"dd:v1\", [(\"Trastuzumab\", \"Breast carcinoma\"), (\"Tamoxifen\", \"Breast Cancer\"), (\"Tamoxifen\", \"Breast Cancer\")]),\n                    \"Syn\": (\"syn:v1\", [(\"her2-positive\", \"her2 positive\"), (\"breast carcinoma\", \"breast cancer\")]),\n                },\n                \"versions\": base_versions\n            }\n        },\n        # Test Case 2: Version change boundary\n        {\n            \"run_a\": {\"datasets\": base_datasets, \"versions\": base_versions},\n            \"run_b\": {\n                \"datasets\": {\n                    \"GD\": base_datasets[\"GD\"],\n                    \"DT\": base_datasets[\"DT\"],\n                    \"DD\": base_datasets[\"DD\"],\n                    \"Syn\": (\"syn:v2\", base_datasets[\"Syn\"][1]),\n                },\n                \"versions\": base_versions\n            }\n        },\n        # Test Case 3: Invalid due to missing explicit container version\n        {\n            \"run_a\": {\n                \"datasets\": base_datasets,\n                \"versions\": {**base_versions, \"c_transform\": \"\"}\n            },\n            \"run_b\": {\"datasets\": base_datasets, \"versions\": base_versions}\n        },\n        # Test Case 4: Code version change\n        {\n            \"run_a\": {\"datasets\": base_datasets, \"versions\": base_versions},\n            \"run_b\": {\n                \"datasets\": base_datasets,\n                \"versions\": {**base_versions, \"s_load\": \"codeL:xyz\"}\n            }\n        }\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        valid_a, digest_a = run_workflow(case[\"run_a\"][\"datasets\"], case[\"run_a\"][\"versions\"])\n        valid_b, digest_b = run_workflow(case[\"run_b\"][\"datasets\"], case[\"run_b\"][\"versions\"])\n        \n        is_reproducible = valid_a and valid_b and (digest_a == digest_b)\n        results.append(is_reproducible)\n\n    # Final print statement\n    print(f\"[{','.join(map(lambda b: str(b), results))}]\")\n\nsolve()\n```", "id": "4577595"}]}