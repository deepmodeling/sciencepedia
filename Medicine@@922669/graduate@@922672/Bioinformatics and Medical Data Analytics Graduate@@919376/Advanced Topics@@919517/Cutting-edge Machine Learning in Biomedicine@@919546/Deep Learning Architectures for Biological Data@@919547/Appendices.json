{"hands_on_practices": [{"introduction": "Convolutional Neural Networks (CNNs) are a cornerstone of deep learning for genomics, adept at identifying patterns like transcription factor binding motifs in DNA sequences. This exercise delves into the fundamental mechanics of a 1D convolutional layer, exploring how parameters such as kernel size, stride, and padding determine the layer's output and its ability to capture biological signals. Mastering these calculations [@problem_id:4553876] is essential for designing architectures that can effectively process and interpret sequential biological data.", "problem": "You are designing a first-layer one-dimensional convolution in a Convolutional Neural Network (CNN) to detect transcription factor binding motifs in Deoxyribonucleic Acid (DNA) sequences encoded as one-hot along channels. Consider an input sequence of length $L_{\\mathrm{in}} = 1{,}000$ bases. The convolutional layer uses a kernel of length $k = 15$, stride $s = 3$, symmetric zero-padding of $p = 2$ bases on each end, and dilation $d = 1$.\n\nStarting from the definition of discrete one-dimensional convolution with stride and padding on a padded input, derive the general expression that determines the number of output positions as the count of valid kernel alignments. Use this reasoning to compute the output sequence length $L_{\\mathrm{out}}$ for the given parameters. Then, based on the concept of receptive field and the sampling induced by stride, analyze how this configuration affects the ability to detect motifs that span $M = 20$ bases, particularly with respect to full coverage within a single filter application, positional sensitivity, and detection near sequence boundaries.\n\nReport the output length $L_{\\mathrm{out}}$ as a single integer with no units. No rounding is necessary. Your qualitative analysis should be included in your reasoning, but the final answer must be only the computed $L_{\\mathrm{out}}$.", "solution": "The user requires the derivation of the output length of a one-dimensional convolutional layer and a qualitative analysis of its properties for a specific application in bioinformatics. The solution will first establish the general formula for the output dimension, then apply it to the given parameters, and finally analyze the configuration's implications.\n\n**Problem Validation**\n\nThe problem statement is critically validated before proceeding.\n1.  **Givens Extracted**:\n    -   Input sequence length: $L_{\\mathrm{in}} = 1{,}000$ bases.\n    -   Convolutional kernel length: $k = 15$.\n    -   Stride: $s = 3$.\n    -   Symmetric zero-padding: $p = 2$ on each end.\n    -   Dilation: $d = 1$.\n    -   Subject motif length for analysis: $M = 20$ bases.\n\n2.  **Validation Verdict**: The problem is **valid**. It is scientifically grounded in the standard principles of deep learning (specifically, Convolutional Neural Networks) as applied to the well-established field of bioinformatics for genomic analysis. The problem is well-posed, providing a complete and consistent set of parameters ($L_{\\mathrm{in}}, k, s, p, d$) from which a unique, meaningful solution for the output length ($L_{\\mathrm{out}}$) can be determined. The language is objective and employs standard terminology. The parameters are realistic for the described task.\n\n**Derivation of the General Expression for Output Length**\n\nThe number of output positions, or the output sequence length $L_{\\mathrm{out}}$, of a one-dimensional convolutional layer is determined by the number of valid placements of the kernel along the input sequence. This can be derived from first principles.\n\n1.  An input sequence of length $L_{\\mathrm{in}}$ is first subjected to symmetric padding, where $p$ elements are added to each of its two ends. The length of this new, padded sequence, denoted $L_{\\mathrm{padded}}$, is:\n    $$L_{\\mathrm{padded}} = L_{\\mathrm{in}} + 2p$$\n\n2.  The kernel, of length $k$, may be dilated. The dilation factor $d$ introduces $d-1$ gaps between consecutive kernel elements. The effective spatial extent of the kernel on the input, known as the effective kernel size $k_{\\mathrm{eff}}$, is thus:\n    $$k_{\\mathrm{eff}} = k + (k-1)(d-1)$$\n\n3.  The kernel is moved, or 'strided', across the padded sequence. An output is computed for each position. The first placement of the kernel starts at index $0$ of the padded sequence. The extent covered is from index $0$ to $k_{\\mathrm{eff}}-1$. The last possible placement must also fit entirely within the padded sequence. The last element of the kernel must align with an index no greater than $L_{\\mathrm{padded}}-1$. This means the last starting position for the kernel is at index $L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$.\n\n4.  The total span of possible starting positions is therefore from $0$ to $L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$, which comprises $L_{\\mathrm{padded}} - k_{\\mathrm{eff}} + 1$ possible positions if the stride were $1$.\n\n5.  With a stride of $s$, the kernel does not occupy every possible position. Instead, it occupies positions $0, s, 2s, \\dots, Ns$, where $Ns$ is the last valid starting position. This last position must satisfy $Ns \\le L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$. The number of steps, $N$, is therefore the largest integer satisfying this condition, which is found using the floor function:\n    $$N = \\left\\lfloor \\frac{L_{\\mathrm{padded}} - k_{\\mathrm{eff}}}{s} \\right\\rfloor$$\n    The total number of output positions, $L_{\\mathrm{out}}$, is the number of these placements, which is $N+1$ (including the initial placement at position $0$).\n\n6.  Substituting the expressions for $L_{\\mathrm{padded}}$ and $k_{\\mathrm{eff}}$, the general formula for the output length is obtained:\n    $$L_{\\mathrm{out}} = \\left\\lfloor \\frac{(L_{\\mathrm{in}} + 2p) - (k + (k-1)(d-1))}{s} \\right\\rfloor + 1$$\n\n**Computation of $L_{\\mathrm{out}}$ for the Given Parameters**\n\nThe problem provides the following parameters: $L_{\\mathrm{in}} = 1{,}000$, $k = 15$, $s = 3$, $p = 2$, and $d = 1$.\n\nFirst, we substitute these values into the derived general expression. Since the dilation is $d=1$, the effective kernel length $k_{\\mathrm{eff}}$ is simply $k$:\n$$k_{\\mathrm{eff}} = 15 + (15-1)(1-1) = 15 + (14)(0) = 15$$\nThe formula for $L_{\\mathrm{out}}$ simplifies to:\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{L_{\\mathrm{in}} + 2p - k}{s} \\right\\rfloor + 1$$\nSubstituting the numerical values:\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{1{,}000 + 2(2) - 15}{3} \\right\\rfloor + 1$$\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{1{,}000 + 4 - 15}{3} \\right\\rfloor + 1$$\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{989}{3} \\right\\rfloor + 1$$\nThe division yields a non-integer value:\n$$\\frac{989}{3} \\approx 329.667$$\nApplying the floor function gives the greatest integer less than or equal to this value:\n$$\\lfloor 329.667 \\rfloor = 329$$\nFinally, adding $1$ yields the output length:\n$$L_{\\mathrm{out}} = 329 + 1 = 330$$\n\n**Qualitative Analysis of the Configuration**\n\nThe problem further requires an analysis of this configuration's ability to detect motifs of length $M = 20$ bases.\n\n1.  **Full Coverage and Receptive Field**: The receptive field of any single neuron in this convolutional layer is its effective kernel size, $k_{\\mathrm{eff}} = 15$ bases. The motif of interest is $M=20$ bases long. Since $M > k_{\\mathrm{eff}}$, **no single filter application can see the entire motif**. The filter can, at best, learn to recognize fragments of the motif that are no longer than $15$ bases. To identify the full $20$-base motif, the network must learn to combine information from spatially adjacent neurons in the output feature map, a task for subsequent layers (e.g., another convolutional layer or a fully-connected layer).\n\n2.  **Positional Sensitivity and Stride**: The stride of $s=3$ introduces a downsampling of the input. This means the kernel \"jumps\" over $2$ bases for each step it takes. Consequently, the layer is not translationally equivariant to single-base shifts. A motif that starts at input position $i$ will be viewed differently by the kernel than one starting at position $i+1$ or $i+2$. This high sensitivity to positional shifts modulo $s$ can be detrimental, as a small shift in a motif's location might cause it to be misaligned with the kernel's stride, leading to a diminished activation and a possible failed detection.\n\n3.  **Detection Near Sequence Boundaries**: The layer uses symmetric padding of $p=2$. With a kernel of size $k=15$, this padding is insufficient to allow the kernel to be centered over motifs at the extreme ends of the sequence. To center a kernel of size $k$ over the very first base (index $0$), a padding of $p_{\\mathrm{center}} = \\lfloor k/2 \\rfloor = \\lfloor 15/2 \\rfloor = 7$ would be required. With $p=2$, the receptive field of the very first output neuron is centered on original sequence position $7-2 = 5$. This implies that motifs within the first $\\approx 5$ and last $\\approx 5$ bases of the sequence can never be centered in the kernel's view, which can impair the network's ability to detect them.\n\nIn conclusion, this specific layer configuration is structurally limited for the task of detecting a $20$-base motif due to its small kernel size, and its performance is further compromised by a large stride and insufficient padding, which affect positional sensitivity and boundary detection.", "answer": "$$\\boxed{330}$$", "id": "4553876"}, {"introduction": "Many critical problems in biology, such as identifying pathogenic gene variants, are characterized by severe class imbalance where positive cases are rare. This practice examines the nuances of model evaluation in such scenarios, contrasting the widely used Area Under the ROC Curve (AUROC) with the more informative Area Under the Precision-Recall Curve (AUPRC). By working through the definitions and a practical calculation [@problem_id:4553811], you will develop the critical judgment needed to select evaluation metrics that truly reflect a model's performance on challenging, imbalanced biological data.", "problem": "A research team trains a deep neural network to prioritize candidate pathogenic variants from whole-genome sequencing in a clinical pipeline. The model produces calibrated scores $s \\in [0,1]$ for each variant. Evaluation is performed across thresholds $\\tau$, where predictions are formed as $\\hat{y}=1$ if $s \\geq \\tau$ and $\\hat{y}=0$ otherwise. Let the confusion matrix counts at threshold $\\tau$ be true positives $TP(\\tau)$, false positives $FP(\\tau)$, true negatives $TN(\\tau)$, and false negatives $FN(\\tau)$, and define the standard rates at threshold $\\tau$ by recall $R(\\tau) = \\frac{TP(\\tau)}{TP(\\tau)+FN(\\tau)}$, precision $P(\\tau) = \\frac{TP(\\tau)}{TP(\\tau)+FP(\\tau)}$, and false positive rate $FPR(\\tau) = \\frac{FP(\\tau)}{FP(\\tau)+TN(\\tau)}$. The study cohort has severe class imbalance with positive prevalence $\\rho = \\frac{\\text{number of true positives}}{\\text{total number of instances}}$ equal to $\\rho = 0.02$.\n\nTasks:\n\n1. Using the above threshold-dependent rates as the fundamental base, provide rigorous definitions of Area Under the Receiver Operating Characteristic (AUROC) and Area Under the Precision–Recall Curve (AUPRC) as functionals of the score-induced ranking, and justify why AUPRC is more informative than AUROC under severe class imbalance characterized by small $\\rho$.\n\n2. Consider a precision–recall curve summarized by three points obtained at monotonically decreasing thresholds: $(R,P)$ equals $(0,1)$ by the standard convention when no positives are predicted, an interior operating point $(0.35,0.75)$, and the lowest-threshold endpoint $(1,\\rho)$ reflecting that predicting every instance as positive yields precision equal to the prevalence. Assume the precision–recall curve is piecewise linear in recall between these points. Compute the exact area under this piecewise linear curve over recall $R \\in [0,1]$ and report the Area Under the Precision–Recall Curve (AUPRC). Round your numerical answer to four significant figures and express it as a decimal.", "solution": "The problem is assessed as valid. It is scientifically grounded, well-posed, and objective. The provided definitions and data are consistent and sufficient for a complete solution.\n\nThe problem is divided into two tasks. The first is a conceptual explanation of evaluation metrics, and the second is a numerical calculation based on provided data.\n\n**Part 1: Definitions of AUROC and AUPRC and Justification for AUPRC**\n\nLet $P_{\\text{total}}$ be the total number of positive instances and $N_{\\text{total}}$ be the total number of negative instances in the dataset. These quantities are invariant with respect to the classification threshold $\\tau$. The rates given in the problem can be expressed as:\n- Recall: $R(\\tau) = \\frac{TP(\\tau)}{P_{\\text{total}}}$\n- False Positive Rate: $FPR(\\tau) = \\frac{FP(\\tau)}{N_{\\text{total}}}$\n- Precision: $P(\\tau) = \\frac{TP(\\tau)}{TP(\\tau) + FP(\\tau)}$\n\nThe prevalence $\\rho$ is the fraction of positive instances in the entire dataset, given by $\\rho = \\frac{P_{\\text{total}}}{P_{\\text{total}} + N_{\\text{total}}}$.\n\n**Area Under the Receiver Operating Characteristic (AUROC)**\nThe Receiver Operating Characteristic (ROC) curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. It is created by plotting the true positive rate (Recall, $R(\\tau)$) against the false positive rate ($FPR(\\tau)$) at various threshold settings.\nThe AUROC is the area under this curve. It is formally defined by the integral:\n$$\nAUROC = \\int_0^1 R(FPR) \\, d(FPR)\n$$\nwhere the integral is taken over all possible values of the false positive rate, from $0$ to $1$. The AUROC has a crucial statistical interpretation: it is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance. Let $s^+$ be the score of a random positive instance and $s^-$ be the score of a random negative instance. Then, $AUROC = P(s^+ > s^-)$. A random classifier has an AUROC of $0.5$, while a perfect classifier has an AUROC of $1$.\n\n**Area Under the Precision–Recall Curve (AUPRC)**\nThe Precision-Recall (PR) curve is a plot of the precision ($P(\\tau)$) versus the recall ($R(\\tau)$) for different thresholds $\\tau$.\nThe AUPRC is the area under this curve. It is formally defined by the integral of the precision as a function of recall:\n$$\nAUPRC = \\int_0^1 P(R) \\, dR\n$$\nIn practice, since the actual PR curve is a set of discrete points, this integral is approximated, often using the trapezoidal rule, as is done in the second part of this problem. A random classifier has an AUPRC equal to the prevalence $\\rho$, while a perfect classifier has an AUPRC of $1$.\n\n**Justification for AUPRC in Imbalanced Datasets**\nAUPRC is more informative than AUROC under severe class imbalance (small $\\rho$) for the following reasons:\n1.  **Sensitivity to False Positives**: The core of the issue lies in the denominators of the $FPR$ and Precision metrics. The $FPR$ is normalized by $N_{\\text{total}}$, the total number of negatives, while Precision's denominator is $TP(\\tau) + FP(\\tau)$, the total number of positive predictions.\n    In a highly imbalanced dataset where positives are rare, $N_{\\text{total}} \\gg P_{\\text{total}}$. Consequently, a large increase in the absolute number of false positives ($FP$) may lead to only a minuscule increase in the $FPR$, as this increase is divided by a very large $N_{\\text{total}}$. The ROC curve, and hence AUROC, will be insensitive to this change, potentially giving a misleadingly optimistic view of performance. For instance, a model with a low $FPR$ might still generate thousands of false positives, which is a critical failure in a clinical context where follow-up tests are costly or invasive.\n2.  **Focus on Positive Predictions**: Precision, $P(\\tau) = \\frac{TP(\\tau)}{TP(\\tau) + FP(\\tau)}$, directly evaluates the fraction of positive predictions that are correct. It is not influenced by the number of true negatives ($TN$), which form the overwhelming majority of the data in an imbalanced setting. AUPRC, which is based on precision, thus directly measures the performance of the model with respect to the positive class and the errors it makes on that class (false positives). This is often the more relevant business or clinical question: \"Of the variants my model flagged as pathogenic, what proportion are truly pathogenic?\"\n3.  **Informative Baseline**: The baseline performance for AUROC (a random classifier) is always $0.5$, regardless of class imbalance. The baseline for AUPRC is the class prevalence $\\rho$. For the given problem, $\\rho=0.02$. An AUPRC score of, say, $0.6$ is a $30$-fold improvement over random, whereas an AUROC of $0.8$ might look good but could correspond to poor precision. The small value of the AUPRC baseline properly reflects the difficulty of the task, making improvements over the baseline more meaningful.\n\nIn summary, AUROC can be misleadingly high for imbalanced data, whereas AUPRC provides a more realistic and clinically relevant assessment of a classifier's ability to identify rare positive instances without being swamped by false positives.\n\n**Part 2: Computation of AUPRC**\n\nThe problem specifies a piecewise linear precision-recall curve defined by three points in the (Recall, Precision) plane. The area under this curve can be calculated by summing the areas of the trapezoids formed by the line segments.\n\nThe given points are:\n- $P_1 = (R_1, P_1) = (0, 1)$\n- $P_2 = (R_2, P_2) = (0.35, 0.75)$\n- $P_3 = (R_3, P_3) = (1, \\rho) = (1, 0.02)$\n\nThe total area (AUPRC) is the sum of the areas of two trapezoids.\n\n**Trapezoid 1**: This is formed by the segment connecting $P_1$ and $P_2$. The 'base' of the trapezoid is along the Recall axis, from $R_1=0$ to $R_2=0.35$. The 'heights' are the corresponding precision values $P_1=1$ and $P_2=0.75$.\nThe area, $A_1$, is given by the trapezoidal rule:\n$$\nA_1 = \\frac{1}{2} (P_1 + P_2) (R_2 - R_1) = \\frac{1}{2} (1 + 0.75) (0.35 - 0)\n$$\n$$\nA_1 = \\frac{1}{2} (1.75) (0.35) = 0.875 \\times 0.35 = 0.30625\n$$\n\n**Trapezoid 2**: This is formed by the segment connecting $P_2$ and $P_3$. The base is along the Recall axis from $R_2=0.35$ to $R_3=1$. The heights are $P_2=0.75$ and $P_3=0.02$.\nThe area, $A_2$, is:\n$$\nA_2 = \\frac{1}{2} (P_2 + P_3) (R_3 - R_2) = \\frac{1}{2} (0.75 + 0.02) (1 - 0.35)\n$$\n$$\nA_2 = \\frac{1}{2} (0.77) (0.65) = 0.385 \\times 0.65 = 0.25025\n$$\n\nThe total AUPRC is the sum of these two areas:\n$$\nAUPRC = A_1 + A_2 = 0.30625 + 0.25025 = 0.5565\n$$\n\nThe problem requires the answer to be rounded to four significant figures. The calculated value $0.5565$ already has four significant figures.", "answer": "$$\n\\boxed{0.5565}\n$$", "id": "4553811"}, {"introduction": "For high-stakes applications like clinical decision support, knowing a model's confidence is as important as its prediction. This advanced exercise introduces a practical method, Monte Carlo dropout, for estimating the uncertainty in a neural network's predictions. By deriving and applying the estimators for predictive mean and variance [@problem_id:4553839], you will learn how to build models that not only predict but also quantify their own confidence by distinguishing between data uncertainty (aleatoric) and model uncertainty (epistemic).", "problem": "You are modeling log-transformed gene expression for a single cell type using a deep neural network with dropout during both training and inference. Assume that maintaining dropout at inference realizes a variational approximation to Bayesian inference, where the approximate posterior over network weights is induced by random Bernoulli masks. Let the predictive posterior for a new input $x$ be defined by the integral over weights $W$ as $p(y \\mid x, \\mathcal{D}) = \\int p(y \\mid x, W) \\, p(W \\mid \\mathcal{D}) \\, dW$, where $\\mathcal{D}$ denotes the training data.\n\nAt inference, you implement Monte Carlo (MC) dropout by performing $T$ stochastic forward passes with independent dropout masks, obtaining for each pass $t \\in \\{1,\\dots,T\\}$ a pair $(\\mu_t, \\sigma_t^{2})$ representing the conditional mean and the aleatoric variance of a Gaussian likelihood $y \\mid x, W^{(t)} \\sim \\mathcal{N}(\\mu_t, \\sigma_t^{2})$. You may assume that the forward passes are independent and identically distributed draws from the variational posterior over weights. The task is to derive the Monte Carlo estimators for the predictive mean and the predictive variance implied by this setup, starting from the definition of the predictive posterior and standard laws of total expectation and total variance. Then, compute the predictive mean and predictive variance using the following $T$ realizations:\n- $T = 5$,\n- $(\\mu_1, \\sigma_1^{2}) = (1.10, 0.09)$,\n- $(\\mu_2, \\sigma_2^{2}) = (0.95, 0.16)$,\n- $(\\mu_3, \\sigma_3^{2}) = (1.20, 0.04)$,\n- $(\\mu_4, \\sigma_4^{2}) = (1.05, 0.09)$,\n- $(\\mu_5, \\sigma_5^{2}) = (1.15, 0.04)$.\n\nProvide your final numerical results as two quantities in the order: predictive mean, predictive variance. Round your numerical answers to four significant figures. Express the final answer as a row matrix. No units are required.", "solution": "The problem requires the derivation of the Monte Carlo estimators for the predictive mean and predictive variance in a Bayesian deep learning framework using MC dropout, followed by a numerical computation using provided data. The derivation hinges on the laws of total expectation and total variance.\n\n**1. Derivation of Estimators**\n\nLet $y$ be the random variable for the model's prediction for a new input $x$, and let $W$ be the random variable for the model's weights, drawn from the approximate posterior distribution $p(W \\mid \\mathcal{D})$. The problem defines the likelihood for a given set of weights $W$ as a Gaussian distribution:\n$$y \\mid x, W \\sim \\mathcal{N}(\\mu(W), \\sigma^2(W))$$\nHere, $\\mu(W)$ is the conditional mean and $\\sigma^2(W)$ is the conditional variance, known as the aleatoric uncertainty. The Monte Carlo (MC) dropout procedure generates $T$ samples from the approximate posterior, $\\{W^{(t)}\\}_{t=1}^T$, yielding a set of i.i.d. pairs $\\{(\\mu_t, \\sigma_t^2)\\}_{t=1}^T$, where $\\mu_t = \\mu(W^{(t)})$ and $\\sigma_t^2 = \\sigma^2(W^{(t)})$.\n\n**1.1. Predictive Mean**\n\nThe predictive mean, $\\mu_{pred}$, is the expectation of $y$ over the predictive posterior distribution $p(y \\mid x, \\mathcal{D})$. Using the law of total expectation (or law of iterated expectations), we can write this as an expectation over the distribution of the weights $W$:\n$$\\mu_{pred} = E[y \\mid x, \\mathcal{D}] = E_{W \\sim p(W \\mid \\mathcal{D})} \\left[ E[y \\mid x, W] \\right]$$\nFrom the definition of our Gaussian likelihood, the inner expectation is the mean of the Gaussian, $E[y \\mid x, W] = \\mu(W)$. Therefore, the predictive mean is the expectation of the model's mean output over the distribution of weights:\n$$\\mu_{pred} = E_{W \\sim p(W \\mid \\mathcal{D})} [\\mu(W)]$$\nThe Monte Carlo estimator for this expectation, denoted $\\hat{\\mu}_{pred}$, is the sample average of the means $\\mu_t$ obtained from the $T$ stochastic forward passes:\n$$\\hat{\\mu}_{pred} = \\frac{1}{T} \\sum_{t=1}^{T} \\mu_t$$\n\n**1.2. Predictive Variance**\n\nThe predictive variance, $\\sigma_{pred}^2$, is the variance of $y$ over the predictive posterior. We use the law of total variance:\n$$Var(y \\mid x, \\mathcal{D}) = E_{W \\sim p(W \\mid \\mathcal{D})} [Var(y \\mid x, W)] + Var_{W \\sim p(W \\mid \\mathcal{D})} [E[y \\mid x, W]]$$\nFrom our Gaussian likelihood, we identify the terms:\n- The inner variance is the aleatoric variance: $Var(y \\mid x, W) = \\sigma^2(W)$.\n- The inner expectation is the conditional mean: $E[y \\mid x, W] = \\mu(W)$.\n\nSubstituting these into the law of total variance gives the decomposition of the predictive variance:\n$$\\sigma_{pred}^2 = \\underbrace{E_{W \\sim p(W \\mid \\mathcal{D})} [\\sigma^2(W)]}_{\\text{Expected aleatoric uncertainty}} + \\underbrace{Var_{W \\sim p(W \\mid \\mathcal{D})} [\\mu(W)]}_{\\text{Epistemic uncertainty}}$$\nThe predictive variance is the sum of two components:\n1.  **Aleatoric uncertainty**: The average of the variance predicted by the model across different weight configurations. This represents inherent noise in the data that the model cannot reduce.\n2.  **Epistemic uncertainty**: The variance in the model's mean predictions. This represents the model's own uncertainty about its parameters $W$ and can be reduced with more data.\n\nThe Monte Carlo estimator for $\\sigma_{pred}^2$, denoted $\\hat{\\sigma}_{pred}^2$, is obtained by approximating each term with its respective sample estimate:\n- The estimator for the expected aleatoric uncertainty is the sample mean of the variances $\\sigma_t^2$:\n$$ \\hat{E}[\\sigma^2(W)] = \\frac{1}{T} \\sum_{t=1}^{T} \\sigma_t^2 $$\n- The estimator for the epistemic uncertainty is the sample variance of the means $\\mu_t$:\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{T} \\sum_{t=1}^{T} (\\mu_t - \\hat{\\mu}_{pred})^2 $$\nCombining these, the Monte Carlo estimator for the predictive variance is:\n$$ \\hat{\\sigma}_{pred}^2 = \\left( \\frac{1}{T} \\sum_{t=1}^{T} \\sigma_t^2 \\right) + \\left( \\frac{1}{T} \\sum_{t=1}^{T} \\mu_t^2 - \\hat{\\mu}_{pred}^2 \\right) $$\nAn alternative way to express the total variance estimator is:\n$$ \\hat{\\sigma}_{pred}^2 = \\frac{1}{T} \\sum_{t=1}^{T} (\\mu_t^2 + \\sigma_t^2) - \\hat{\\mu}_{pred}^2 $$\n\n**2. Numerical Computation**\n\nThe provided data are:\n- $T = 5$\n- Realizations of $(\\mu_t, \\sigma_t^2)$:\n  - $t=1$: $(\\mu_1, \\sigma_1^2) = (1.10, 0.09)$\n  - $t=2$: $(\\mu_2, \\sigma_2^2) = (0.95, 0.16)$\n  - $t=3$: $(\\mu_3, \\sigma_3^2) = (1.20, 0.04)$\n  - $t=4$: $(\\mu_4, \\sigma_4^2) = (1.05, 0.09)$\n  - $t=5$: $(\\mu_5, \\sigma_5^2) = (1.15, 0.04)$\n\n**2.1. Compute Predictive Mean**\nUsing the derived estimator:\n$$ \\hat{\\mu}_{pred} = \\frac{1}{5} \\sum_{t=1}^{5} \\mu_t = \\frac{1}{5} (1.10 + 0.95 + 1.20 + 1.05 + 1.15) $$\n$$ \\hat{\\mu}_{pred} = \\frac{1}{5} (5.45) = 1.09 $$\n\n**2.2. Compute Predictive Variance**\nWe will compute the two components of variance separately.\n\nFirst, the average aleatoric uncertainty:\n$$ \\hat{E}[\\sigma^2(W)] = \\frac{1}{5} \\sum_{t=1}^{5} \\sigma_t^2 = \\frac{1}{5} (0.09 + 0.16 + 0.04 + 0.09 + 0.04) $$\n$$ \\hat{E}[\\sigma^2(W)] = \\frac{1}{5} (0.42) = 0.084 $$\n\nNext, the epistemic uncertainty (sample variance of the means):\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} \\sum_{t=1}^{5} (\\mu_t - \\hat{\\mu}_{pred})^2 $$\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} \\left( (1.10 - 1.09)^2 + (0.95 - 1.09)^2 + (1.20 - 1.09)^2 + (1.05 - 1.09)^2 + (1.15 - 1.09)^2 \\right) $$\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} \\left( (0.01)^2 + (-0.14)^2 + (0.11)^2 + (-0.04)^2 + (0.06)^2 \\right) $$\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} (0.0001 + 0.0196 + 0.0121 + 0.0016 + 0.0036) $$\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} (0.037) = 0.0074 $$\n\nThe total predictive variance is the sum of these two components:\n$$ \\hat{\\sigma}_{pred}^2 = \\hat{E}[\\sigma^2(W)] + \\widehat{Var}[\\mu(W)] = 0.084 + 0.0074 = 0.0914 $$\n\n**3. Final Results**\n\nThe computed predictive mean is $1.09$ and the predictive variance is $0.0914$. Rounding to four significant figures as requested:\n- Predictive mean: $1.090$\n- Predictive variance: $0.09140$", "answer": "$$\\boxed{\\begin{pmatrix} 1.090 & 0.09140 \\end{pmatrix}}$$", "id": "4553839"}]}