{"hands_on_practices": [{"introduction": "A fundamental challenge in transfer learning is the \"domain shift\" between source and target datasets. This exercise introduces a powerful method to quantify this shift using Maximum Mean Discrepancy (MMD), which measures the distance between the feature distributions of two domains in a Reproducing Kernel Hilbert Space (RKHS). By calculating the MMD and linking it to a theoretical bound on target error, you will gain hands-on experience in diagnosing the severity of domain adaptation problems, a critical skill for developing robust medical imaging models. [@problem_id:4615251]", "problem": "In a transfer learning study for chest radiograph classification, a feature extractor trained on a large source dataset outputs two-dimensional feature embeddings for both source and target domains. Let the source feature set be $\\{\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\mathbf{x}_{3}\\}$ with $\\mathbf{x}_{1} = (0, 0)$, $\\mathbf{x}_{2} = (0.5, 0)$, $\\mathbf{x}_{3} = (0, 0.5)$, and the target feature set be $\\{\\mathbf{y}_{1}, \\mathbf{y}_{2}, \\mathbf{y}_{3}\\}$ with $\\mathbf{y}_{1} = (1, 1)$, $\\mathbf{y}_{2} = (1.5, 1)$, $\\mathbf{y}_{3} = (1, 1.5)$. Consider the Gaussian radial basis function kernel $k(\\mathbf{u}, \\mathbf{v}) = \\exp\\!\\left(-\\|\\mathbf{u} - \\mathbf{v}\\|^{2} / (2 \\sigma^{2})\\right)$ with bandwidth $\\sigma = 0.5$, and let the Reproducing Kernel Hilbert Space (RKHS) be the space associated with $k$. \n\nUsing first principles from kernel mean embeddings and the definition of Maximum Mean Discrepancy (MMD) as the difference between RKHS mean embeddings of distributions, compute the unbiased empirical estimate of the squared MMD between the source and target feature sets and then its square root to obtain the MMD. Next, assume the composed loss function $\\ell \\circ h$ belongs to the RKHS with norm bounded by $c = 0.5$ and that the empirical source-domain expected loss is $0.12$. Using only foundational RKHS inequalities, derive a numerical upper bound on the anticipated target-domain expected loss in terms of the computed MMD. \n\nRound your final numerical bound to four significant figures and report it as a pure number (no units).", "solution": "The user has provided a problem statement that must be validated before a solution is attempted.\n\n### Step 1: Extract Givens\n-   Source feature set: $\\{\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\mathbf{x}_{3}\\}$\n-   Source feature coordinates: $\\mathbf{x}_{1} = (0, 0)$, $\\mathbf{x}_{2} = (0.5, 0)$, $\\mathbf{x}_{3} = (0, 0.5)$.\n-   Target feature set: $\\{\\mathbf{y}_{1}, \\mathbf{y}_{2}, \\mathbf{y}_{3}\\}$\n-   Target feature coordinates: $\\mathbf{y}_{1} = (1, 1)$, $\\mathbf{y}_{2} = (1.5, 1)$, $\\mathbf{y}_{3} = (1, 1.5)$.\n-   Kernel function: Gaussian radial basis function (RBF) kernel, $k(\\mathbf{u}, \\mathbf{v}) = \\exp\\!\\left(-\\|\\mathbf{u} - \\mathbf{v}\\|^{2} / (2 \\sigma^{2})\\right)$.\n-   Kernel bandwidth: $\\sigma = 0.5$.\n-   The Reproducing Kernel Hilbert Space (RKHS) is associated with the kernel $k$.\n-   The composed loss function $\\ell \\circ h$ is an element of the RKHS.\n-   Norm bound for the loss function: $\\|\\ell \\circ h\\|_{\\mathcal{H}} \\le c = 0.5$.\n-   Empirical source-domain expected loss: $0.12$. This is $\\frac{1}{m} \\sum_{i=1}^m \\ell(h(\\mathbf{x}_i)) = 0.12$ with $m=3$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific Grounding**: The problem is firmly rooted in the mathematical theory of machine learning, specifically in domain adaptation, kernel methods, and Reproducing Kernel Hilbert Spaces (RKHS). Maximum Mean Discrepancy (MMD) is a standard metric for comparing distributions, and the theoretical bound on target error is a cornerstone of domain adaptation theory. The context of transfer learning in medical imaging is a primary application area for these methods.\n2.  **Well-Posedness**: The problem provides all necessary definitions, data points, and constants to perform the required calculations. The tasks are specified clearly: first, compute the MMD from the given samples, and second, use this value to derive a numerical bound on the target loss. The solution is unique and meaningful within the provided theoretical framework. The phrasing \"anticipated target-domain expected loss\" licenses a standard simplification in such problems, where empirical estimates are used in place of true population quantities in a theoretical inequality, which is a common pedagogical approach.\n3.  **Objectivity**: The problem is stated in precise, quantitative, and objective language, free of ambiguity or subjective claims.\n\nThe problem does not exhibit any of the invalidity criteria. It is scientifically sound, well-posed, complete, and formalizable.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution\n\nThe solution is divided into two parts as requested: first, the computation of the Maximum Mean Discrepancy (MMD) between the source and target feature sets, and second, the derivation of an upper bound on the target-domain expected loss.\n\n**Part 1: Computation of the Maximum Mean Discrepancy (MMD)**\n\nThe Maximum Mean Discrepancy is defined as the distance between the mean embeddings of two probability distributions, $P_S$ (source) and $P_T$ (target), in a Reproducing Kernel Hilbert Space (RKHS) $\\mathcal{H}$. The squared MMD is given by:\n$$ \\text{MMD}^2(P_S, P_T) = \\|\\mu_{P_S} - \\mu_{P_T}\\|_{\\mathcal{H}}^2 $$\nwhere $\\mu_P$ is the mean embedding of a distribution $P$. For finite samples $X = \\{\\mathbf{x}_i\\}_{i=1}^m$ from $P_S$ and $Y = \\{\\mathbf{y}_j\\}_{j=1}^n$ from $P_T$, an unbiased empirical estimator of the squared MMD is given by:\n$$ \\widehat{\\text{MMD}}_u^2(X, Y) = \\frac{1}{m(m-1)} \\sum_{i \\neq j} k(\\mathbf{x}_i, \\mathbf{x}_j) + \\frac{1}{n(n-1)} \\sum_{i \\neq j} k(\\mathbf{y}_i, \\mathbf{y}_j) - \\frac{2}{mn} \\sum_{i=1}^m \\sum_{j=1}^n k(\\mathbf{x}_i, \\mathbf{y}_j) $$\nIn this problem, we have $m=3$ source samples and $n=3$ target samples. The kernel is the Gaussian RBF with $\\sigma = 0.5$. The kernel function is:\n$$ k(\\mathbf{u}, \\mathbf{v}) = \\exp\\left(-\\frac{\\|\\mathbf{u} - \\mathbf{v}\\|^{2}}{2 \\sigma^2}\\right) = \\exp\\left(-\\frac{\\|\\mathbf{u} - \\mathbf{v}\\|^{2}}{2 (0.5)^2}\\right) = \\exp\\left(-\\frac{\\|\\mathbf{u} - \\mathbf{v}\\|^{2}}{0.5}\\right) = \\exp(-2\\|\\mathbf{u} - \\mathbf{v}\\|^{2}) $$\n\nWe now compute the individual kernel terms.\n\n**1. Within-Source Terms:** $S_1 = \\sum_{i \\neq j} k(\\mathbf{x}_i, \\mathbf{x}_j)$\nThe source points are $\\mathbf{x}_{1} = (0, 0)$, $\\mathbf{x}_{2} = (0.5, 0)$, $\\mathbf{x}_{3} = (0, 0.5)$.\n-   $\\|\\mathbf{x}_1 - \\mathbf{x}_2\\|^2 = \\|(0,0) - (0.5,0)\\|^2 = (-0.5)^2 = 0.25$.\n    $k(\\mathbf{x}_1, \\mathbf{x}_2) = \\exp(-2 \\cdot 0.25) = \\exp(-0.5)$.\n-   $\\|\\mathbf{x}_1 - \\mathbf{x}_3\\|^2 = \\|(0,0) - (0,0.5)\\|^2 = (-0.5)^2 = 0.25$.\n    $k(\\mathbf{x}_1, \\mathbf{x}_3) = \\exp(-2 \\cdot 0.25) = \\exp(-0.5)$.\n-   $\\|\\mathbf{x}_2 - \\mathbf{x}_3\\|^2 = \\|(0.5,0) - (0,0.5)\\|^2 = (0.5)^2 + (-0.5)^2 = 0.25 + 0.25 = 0.5$.\n    $k(\\mathbf{x}_2, \\mathbf{x}_3) = \\exp(-2 \\cdot 0.5) = \\exp(-1)$.\nThe sum is $S_1 = 2k(\\mathbf{x}_1, \\mathbf{x}_2) + 2k(\\mathbf{x}_1, \\mathbf{x}_3) + 2k(\\mathbf{x}_2, \\mathbf{x}_3)$ summed over ordered pairs, or simply $k(\\mathbf{x}_1, \\mathbf{x}_2) + k(\\mathbf{x}_2, \\mathbf{x}_1) + ...$. The sum $\\sum_{i \\neq j}$ has $m(m-1)=6$ terms.\n$S_1 = 2 \\cdot \\left( \\exp(-0.5) + \\exp(-0.5) + \\exp(-1) \\right) = 2 \\cdot (2\\exp(-0.5) + \\exp(-1))$.\nThe first term of $\\widehat{\\text{MMD}}_u^2$ is $\\frac{1}{3(2)} S_1 = \\frac{1}{3}(2\\exp(-0.5) + \\exp(-1))$.\n\n**2. Within-Target Terms:** $S_2 = \\sum_{i \\neq j} k(\\mathbf{y}_i, \\mathbf{y}_j)$\nThe target points are $\\mathbf{y}_{1} = (1, 1)$, $\\mathbf{y}_{2} = (1.5, 1)$, $\\mathbf{y}_{3} = (1, 1.5)$.\nThe geometry of the target points is identical to the source points (a right-angled isosceles triangle with shorter side length $0.5$). Thus, the kernel evaluations are the same.\n-   $k(\\mathbf{y}_1, \\mathbf{y}_2) = \\exp(-0.5)$.\n-   $k(\\mathbf{y}_1, \\mathbf{y}_3) = \\exp(-0.5)$.\n-   $k(\\mathbf{y}_2, \\mathbf{y}_3) = \\exp(-1)$.\nThe second term of $\\widehat{\\text{MMD}}_u^2$ is $\\frac{1}{3(2)} \\sum_{i \\neq j} k(\\mathbf{y}_i, \\mathbf{y}_j) = \\frac{1}{3}(2\\exp(-0.5) + \\exp(-1))$.\n\n**3. Cross-Domain Terms:** $S_3 = \\sum_{i=1}^3 \\sum_{j=1}^3 k(\\mathbf{x}_i, \\mathbf{y}_j)$\n-   $k(\\mathbf{x}_1, \\mathbf{y}_1) = \\exp(-2\\|(0,0)-(1,1)\\|^2) = \\exp(-2(1^2+1^2)) = \\exp(-4)$.\n-   $k(\\mathbf{x}_1, \\mathbf{y}_2) = \\exp(-2\\|(0,0)-(1.5,1)\\|^2) = \\exp(-2(1.5^2+1^2)) = \\exp(-2(3.25)) = \\exp(-6.5)$.\n-   $k(\\mathbf{x}_1, \\mathbf{y}_3) = \\exp(-2\\|(0,0)-(1,1.5)\\|^2) = \\exp(-2(1^2+1.5^2)) = \\exp(-2(3.25)) = \\exp(-6.5)$.\n-   $k(\\mathbf{x}_2, \\mathbf{y}_1) = \\exp(-2\\|(0.5,0)-(1,1)\\|^2) = \\exp(-2((-0.5)^2+(-1)^2)) = \\exp(-2(1.25)) = \\exp(-2.5)$.\n-   $k(\\mathbf{x}_2, \\mathbf{y}_2) = \\exp(-2\\|(0.5,0)-(1.5,1)\\|^2) = \\exp(-2((-1)^2+(-1)^2)) = \\exp(-4)$.\n-   $k(\\mathbf{x}_2, \\mathbf{y}_3) = \\exp(-2\\|(0.5,0)-(1,1.5)\\|^2) = \\exp(-2((-0.5)^2+(-1.5)^2)) = \\exp(-2(2.5)) = \\exp(-5)$.\n-   $k(\\mathbf{x}_3, \\mathbf{y}_1) = \\exp(-2\\|(0,0.5)-(1,1)\\|^2) = \\exp(-2((-1)^2+(-0.5)^2)) = \\exp(-2(1.25)) = \\exp(-2.5)$.\n-   $k(\\mathbf{x}_3, \\mathbf{y}_2) = \\exp(-2\\|(0,0.5)-(1.5,1)\\|^2) = \\exp(-2((-1.5)^2+(-0.5)^2)) = \\exp(-2(2.5)) = \\exp(-5)$.\n-   $k(\\mathbf{x}_3, \\mathbf{y}_3) = \\exp(-2\\|(0,0.5)-(1,1.5)\\|^2) = \\exp(-2((-1)^2+(-1)^2)) = \\exp(-4)$.\nSumming these $9$ terms:\n$S_3 = 3\\exp(-4) + 2\\exp(-6.5) + 2\\exp(-2.5) + 2\\exp(-5)$.\n\n**4. Final MMD Calculation:**\n$$ \\widehat{\\text{MMD}}_u^2(X, Y) = \\frac{1}{3}(2\\exp(-0.5) + \\exp(-1)) + \\frac{1}{3}(2\\exp(-0.5) + \\exp(-1)) - \\frac{2}{9} S_3 $$\n$$ \\widehat{\\text{MMD}}_u^2(X, Y) = \\frac{2}{3}(2\\exp(-0.5) + \\exp(-1)) - \\frac{2}{9}(3\\exp(-4) + 2\\exp(-2.5) + 2\\exp(-5) + 2\\exp(-6.5)) $$\n$$ \\widehat{\\text{MMD}}_u^2(X, Y) = \\frac{4}{3}\\exp(-0.5) + \\frac{2}{3}\\exp(-1) - \\frac{2}{3}\\exp(-4) - \\frac{4}{9}\\exp(-2.5) - \\frac{4}{9}\\exp(-5) - \\frac{4}{9}\\exp(-6.5) $$\nNow, we substitute the numerical values:\n$\\exp(-0.5) \\approx 0.606531$\n$\\exp(-1) \\approx 0.367879$\n$\\exp(-2.5) \\approx 0.082085$\n$\\exp(-4) \\approx 0.018316$\n$\\exp(-5) \\approx 0.006738$\n$\\exp(-6.5) \\approx 0.001503$\n$$ \\widehat{\\text{MMD}}_u^2 \\approx \\frac{4}{3}(0.606531) + \\frac{2}{3}(0.367879) - \\frac{2}{3}(0.018316) - \\frac{4}{9}(0.082085) - \\frac{4}{9}(0.006738) - \\frac{4}{9}(0.001503) $$\n$$ \\widehat{\\text{MMD}}_u^2 \\approx 0.808708 + 0.245253 - 0.012211 - 0.036482 - 0.002995 - 0.000668 $$\n$$ \\widehat{\\text{MMD}}_u^2 \\approx 1.053961 - 0.052356 = 1.001605 $$\nThe problem asks for the MMD, which is the square root of this value:\n$$ \\widehat{\\text{MMD}}_u = \\sqrt{1.001605} \\approx 1.000802 $$\n\n**Part 2: Upper Bound on Target-Domain Expected Loss**\n\nThe foundational RKHS property linking expectations and inner products is $E_{P}[g(\\mathbf{x})] = \\langle g, \\mu_P \\rangle_{\\mathcal{H}}$ for any function $g \\in \\mathcal{H}$. The difference in expectations between the source ($P_S$) and target ($P_T$) distributions for a function $g$ can be written as:\n$$ |E_{P_S}[g] - E_{P_T}[g]| = |\\langle g, \\mu_{P_S} \\rangle_{\\mathcal{H}} - \\langle g, \\mu_{P_T} \\rangle_{\\mathcal{H}}| = |\\langle g, \\mu_{P_S} - \\mu_{P_T} \\rangle_{\\mathcal{H}}| $$\nBy the Cauchy-Schwarz inequality in the RKHS, we have:\n$$ |\\langle g, \\mu_{P_S} - \\mu_{P_T} \\rangle_{\\mathcal{H}}| \\le \\|g\\|_{\\mathcal{H}} \\|\\mu_{P_S} - \\mu_{P_T}\\|_{\\mathcal{H}} $$\nBy definition, $\\|\\mu_{P_S} - \\mu_{P_T}\\|_{\\mathcal{H}} = \\text{MMD}(P_S, P_T)$. This yields the key inequality:\n$$ |E_{P_S}[g] - E_{P_T}[g]| \\le \\|g\\|_{\\mathcal{H}} \\text{MMD}(P_S, P_T) $$\nFrom this, we can bound the target-domain expectation:\n$$ E_{P_T}[g] \\le E_{P_S}[g] + \\|g\\|_{\\mathcal{H}} \\text{MMD}(P_S, P_T) $$\nIn our problem, the function $g$ is the composed loss function, $g = \\ell \\circ h$. We are given:\n-   An upper bound on its norm: $\\|g\\|_{\\mathcal{H}} = \\|\\ell \\circ h\\|_{\\mathcal{H}} \\le c = 0.5$.\n-   The empirical source-domain expected loss: $\\hat{E}_S[g] = \\frac{1}{3}\\sum_{i=1}^3 \\ell(h(\\mathbf{x}_i)) = 0.12$.\n-   We have computed the empirical MMD: $\\widehat{\\text{MMD}}_u(X, Y) \\approx 1.000802$.\n\nThe problem asks for a numerical upper bound on the *anticipated* target-domain expected loss. This directs us to substitute the available empirical estimates into the theoretical inequality, which provides a practical, albeit not statistically rigorous, bound.\n$$ \\text{Anticipated } E_{P_T}[\\ell \\circ h] \\le \\hat{E}_S[\\ell \\circ h] + c \\cdot \\widehat{\\text{MMD}}_u(X, Y) $$\nSubstituting the given and computed values:\n$$ \\text{Upper Bound} \\le 0.12 + (0.5) \\cdot (1.000802) $$\n$$ \\text{Upper Bound} \\le 0.12 + 0.500401 $$\n$$ \\text{Upper Bound} \\le 0.620401 $$\nThe problem requires the final numerical bound to be rounded to four significant figures.\n$$ \\text{Upper Bound} \\approx 0.6204 $$", "answer": "$$\\boxed{0.6204}$$", "id": "4615251"}, {"introduction": "After deciding to adapt a pre-trained model, the next question is how best to fine-tune it. This practice moves beyond simple performance comparisons by introducing a rigorous Bayesian framework to evaluate different fine-tuning strategies. You will compute the expected improvement of selective fine-tuning over full fine-tuning by modeling performance variation across multiple runs, providing a principled method for model selection that is essential for robust and reproducible research. [@problem_id:4615261]", "problem": "A research group is evaluating transfer learning strategies for a convolutional neural network (CNN) in a medical imaging task focused on lung nodule detection using chest radiographs. The network is initialized from a large natural-image pretraining corpus and then trained on labeled medical images. Two training configurations are considered: full fine-tuning of all layers (FF) and selective fine-tuning of only the top layers while freezing earlier layers (SF). Validation performance is measured by the Area Under the Receiver Operating Characteristic curve (AUROC). To control for randomness from initialization and data-order effects, training is repeated across $5$ independent random seeds, and for each seed, validation AUROC is computed over $3$ cross-validation folds.\n\nLet $A_{s,f}^{\\mathrm{SF}}$ and $A_{s,f}^{\\mathrm{FF}}$ denote the fold-level AUROC for seed $s \\in \\{1,2,3,4,5\\}$ and fold $f \\in \\{1,2,3\\}$ under selective fine-tuning and full fine-tuning, respectively. Define the seed-level difference in mean validation performance as\n$$\nd_{s} \\equiv \\frac{1}{3}\\sum_{f=1}^{3}A_{s,f}^{\\mathrm{SF}} - \\frac{1}{3}\\sum_{f=1}^{3}A_{s,f}^{\\mathrm{FF}}.\n$$\nAssume the differences $\\{d_{s}\\}$ are independent and identically distributed draws from a normal likelihood $d_{s} \\sim \\mathcal{N}(\\mu,\\sigma^{2})$, where $\\mu$ is the expected improvement of selective fine-tuning over full fine-tuning, and $\\sigma^{2}$ captures random variability across seeds. Adopt a conjugate Normal–Inverse-Gamma prior,\n$$\n\\mu \\mid \\sigma^{2} \\sim \\mathcal{N}\\!\\left(m_{0},\\frac{\\sigma^{2}}{\\kappa_{0}}\\right), \\quad \\sigma^{2} \\sim \\mathrm{Inv\\!\\text{-}\\Gamma}(\\alpha_{0},\\beta_{0}),\n$$\nwith hyperparameters $m_{0}=0$, $\\kappa_{0}=2$, $\\alpha_{0}=2$, and $\\beta_{0}=10^{-5}$. Under this model, the Bayesian expected improvement is the posterior expectation $\\mathbb{E}[\\mu \\mid \\{d_{s}\\}]$.\n\nYou are given the following fold-level validation AUROC measurements:\n- Seed $1$: $A_{1,1}^{\\mathrm{SF}}=0.892$, $A_{1,2}^{\\mathrm{SF}}=0.887$, $A_{1,3}^{\\mathrm{SF}}=0.894$; $A_{1,1}^{\\mathrm{FF}}=0.888$, $A_{1,2}^{\\mathrm{FF}}=0.883$, $A_{1,3}^{\\mathrm{FF}}=0.890$.\n- Seed $2$: $A_{2,1}^{\\mathrm{SF}}=0.901$, $A_{2,2}^{\\mathrm{SF}}=0.896$, $A_{2,3}^{\\mathrm{SF}}=0.900$; $A_{2,1}^{\\mathrm{FF}}=0.897$, $A_{2,2}^{\\mathrm{FF}}=0.892$, $A_{2,3}^{\\mathrm{FF}}=0.895$.\n- Seed $3$: $A_{3,1}^{\\mathrm{SF}}=0.880$, $A_{3,2}^{\\mathrm{SF}}=0.885$, $A_{3,3}^{\\mathrm{SF}}=0.882$; $A_{3,1}^{\\mathrm{FF}}=0.874$, $A_{3,2}^{\\mathrm{FF}}=0.878$, $A_{3,3}^{\\mathrm{FF}}=0.876$.\n- Seed $4$: $A_{4,1}^{\\mathrm{SF}}=0.895$, $A_{4,2}^{\\mathrm{SF}}=0.898$, $A_{4,3}^{\\mathrm{SF}}=0.897$; $A_{4,1}^{\\mathrm{FF}}=0.890$, $A_{4,2}^{\\mathrm{FF}}=0.892$, $A_{4,3}^{\\mathrm{FF}}=0.891$.\n- Seed $5$: $A_{5,1}^{\\mathrm{SF}}=0.888$, $A_{5,2}^{\\mathrm{SF}}=0.889$, $A_{5,3}^{\\mathrm{SF}}=0.891$; $A_{5,1}^{\\mathrm{FF}}=0.884$, $A_{5,2}^{\\mathrm{FF}}=0.885$, $A_{5,3}^{\\mathrm{FF}}=0.886$.\n\nStarting from foundational statistical definitions (expectation, normal likelihood, and conjugate Bayesian updating), compute the Bayesian expected improvement $\\mathbb{E}[\\mu \\mid \\{d_{s}\\}]$ implied by these data and prior. Round your final numeric answer to four significant figures.", "solution": "The user has provided a problem that requires the computation of a posterior expectation within a Bayesian framework.\n\n### Step 1: Extract Givens\n- **Task**: Compute the Bayesian expected improvement $\\mathbb{E}[\\mu \\mid \\{d_{s}\\}]$.\n- **Data**: Fold-level AUROC scores $A_{s,f}^{\\mathrm{SF}}$ and $A_{s,f}^{\\mathrm{FF}}$ for seeds $s \\in \\{1,2,3,4,5\\}$ and folds $f \\in \\{1,2,3\\}$.\n  - Seed $1$: $\\{A_{1,f}^{\\mathrm{SF}}\\} = \\{0.892, 0.887, 0.894\\}$; $\\{A_{1,f}^{\\mathrm{FF}}\\} = \\{0.888, 0.883, 0.890\\}$.\n  - Seed $2$: $\\{A_{2,f}^{\\mathrm{SF}}\\} = \\{0.901, 0.896, 0.900\\}$; $\\{A_{2,f}^{\\mathrm{FF}}\\} = \\{0.897, 0.892, 0.895\\}$.\n  - Seed $3$: $\\{A_{3,f}^{\\mathrm{SF}}\\} = \\{0.880, 0.885, 0.882\\}$; $\\{A_{3,f}^{\\mathrm{FF}}\\} = \\{0.874, 0.878, 0.876\\}$.\n  - Seed $4$: $\\{A_{4,f}^{\\mathrm{SF}}\\} = \\{0.895, 0.898, 0.897\\}$; $\\{A_{4,f}^{\\mathrm{FF}}\\} = \\{0.890, 0.892, 0.891\\}$.\n  - Seed $5$: $\\{A_{5,f}^{\\mathrm{SF}}\\} = \\{0.888, 0.889, 0.891\\}$; $\\{A_{5,f}^{\\mathrm{FF}}\\} = \\{0.884, 0.885, 0.886\\}$.\n- **Derived Data Definition**: The seed-level difference in mean validation performance is $d_{s} \\equiv \\frac{1}{3}\\sum_{f=1}^{3}A_{s,f}^{\\mathrm{SF}} - \\frac{1}{3}\\sum_{f=1}^{3}A_{s,f}^{\\mathrm{FF}}$.\n- **Likelihood Model**: The differences $\\{d_{s}\\}$ are independent and identically distributed (i.i.d.) draws from a normal distribution, $d_{s} \\sim \\mathcal{N}(\\mu,\\sigma^{2})$. The number of seeds is $n=5$.\n- **Prior Model**: A conjugate Normal–Inverse-Gamma prior is specified for the parameters $(\\mu, \\sigma^2)$:\n  $$\n  \\mu \\mid \\sigma^{2} \\sim \\mathcal{N}\\!\\left(m_{0},\\frac{\\sigma^{2}}{\\kappa_{0}}\\right), \\quad \\sigma^{2} \\sim \\mathrm{Inv\\!\\text{-}\\Gamma}(\\alpha_{0},\\beta_{0})\n  $$\n- **Prior Hyperparameters**: $m_{0}=0$, $\\kappa_{0}=2$, $\\alpha_{0}=2$, $\\beta_{0}=10^{-5}$.\n- **Requirement**: Round the final numeric answer to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n- **Scientifically Grounded**: The problem uses standard methodologies from machine learning and Bayesian statistics (transfer learning, CNNs, AUROC, Normal-Inverse-Gamma conjugate prior analysis) applied to a realistic medical imaging scenario (lung nodule detection).\n- **Well-Posed**: The problem provides all necessary data, a complete statistical model (likelihood and prior), and a clear objective (compute the posterior expectation of $\\mu$). The use of a conjugate prior ensures that a well-defined posterior distribution exists and its parameters can be computed analytically.\n- **Objective**: The problem is stated using precise mathematical definitions and objective numerical data. There are no subjective or ambiguous statements.\n\nThe problem is valid as it does not violate any of the specified criteria for invalidity.\n\n### Step 3: Action\nThe problem is valid. I will now proceed with the solution.\n\nThe objective is to compute the posterior expectation of the mean improvement, $\\mathbb{E}[\\mu \\mid \\{d_{s}\\}]$. The statistical model consists of a normal likelihood for the data $d_s$ and a Normal-Inverse-Gamma prior for the parameters $(\\mu, \\sigma^2)$. This is a standard conjugate setting.\n\nThe posterior distribution $p(\\mu, \\sigma^2 \\mid \\{d_s\\})$ is also a Normal-Inverse-Gamma distribution, $p(\\mu, \\sigma^2 \\mid \\{d_s\\}) = \\mathrm{NIG}(\\mu, \\sigma^2 \\mid m_n, \\kappa_n, \\alpha_n, \\beta_n)$, with updated hyperparameters. The marginal posterior distribution of $\\mu$, after integrating out $\\sigma^2$, is a non-standardized Student's t-distribution centered at $m_n$. The expectation of this distribution is its center, $m_n$. Therefore, the quantity we need to compute is:\n$$\n\\mathbb{E}[\\mu \\mid \\{d_{s}\\}] = m_n\n$$\nThe update rule for the posterior mean parameter $m_n$ is given by a weighted average of the prior mean $m_0$ and the sample mean of the data $\\bar{d}$:\n$$\nm_n = \\frac{\\kappa_0 m_0 + n \\bar{d}}{\\kappa_0 + n}\n$$\nwhere $n$ is the number of data points (seeds) and $\\bar{d} = \\frac{1}{n}\\sum_{s=1}^{n} d_s$.\n\nThe solution process involves three steps:\n1.  Compute the observed differences $d_s$ for each seed $s=1, \\dots, 5$.\n2.  Compute the sample mean $\\bar{d}$ of these differences.\n3.  Substitute the values of $n$, $\\kappa_0$, $m_0$, and $\\bar{d}$ into the formula for $m_n$.\n\n**Step 1: Compute the seed-level differences $d_s$.**\nThe difference $d_s$ is given by $d_s = \\frac{1}{3}\\sum_{f=1}^{3} (A_{s,f}^{\\mathrm{SF}} - A_{s,f}^{\\mathrm{FF}})$.\n\nFor seed $s=1$:\n$$\nd_1 = \\frac{1}{3}((0.892-0.888) + (0.887-0.883) + (0.894-0.890)) = \\frac{1}{3}(0.004 + 0.004 + 0.004) = \\frac{0.012}{3} = 0.004\n$$\nFor seed $s=2$:\n$$\nd_2 = \\frac{1}{3}((0.901-0.897) + (0.896-0.892) + (0.900-0.895)) = \\frac{1}{3}(0.004 + 0.004 + 0.005) = \\frac{0.013}{3}\n$$\nFor seed $s=3$:\n$$\nd_3 = \\frac{1}{3}((0.880-0.874) + (0.885-0.878) + (0.882-0.876)) = \\frac{1}{3}(0.006 + 0.007 + 0.006) = \\frac{0.019}{3}\n$$\nFor seed $s=4$:\n$$\nd_4 = \\frac{1}{3}((0.895-0.890) + (0.898-0.892) + (0.897-0.891)) = \\frac{1}{3}(0.005 + 0.006 + 0.006) = \\frac{0.017}{3}\n$$\nFor seed $s=5$:\n$$\nd_5 = \\frac{1}{3}((0.888-0.884) + (0.889-0.885) + (0.891-0.886)) = \\frac{1}{3}(0.004 + 0.004 + 0.005) = \\frac{0.013}{3}\n$$\n\n**Step 2: Compute the sample mean $\\bar{d}$.**\nThe sample mean is computed over the $n=5$ observed differences $\\{d_s\\}$.\n$$\n\\sum_{s=1}^{5} d_s = \\frac{0.012}{3} + \\frac{0.013}{3} + \\frac{0.019}{3} + \\frac{0.017}{3} + \\frac{0.013}{3} = \\frac{0.012 + 0.013 + 0.019 + 0.017 + 0.013}{3} = \\frac{0.074}{3}\n$$\nThe sample mean $\\bar{d}$ is:\n$$\n\\bar{d} = \\frac{1}{n} \\sum_{s=1}^{5} d_s = \\frac{1}{5} \\left( \\frac{0.074}{3} \\right) = \\frac{0.074}{15}\n$$\n\n**Step 3: Compute the posterior mean $m_n$.**\nUsing the formula for $m_n$ with the given hyperparameters and computed statistics:\n- Number of observations: $n=5$\n- Prior mean: $m_0=0$\n- Prior precision pseudo-count: $\\kappa_0=2$\n- Sample mean: $\\bar{d} = \\frac{0.074}{15}$\n\n$$\nm_n = \\frac{\\kappa_0 m_0 + n \\bar{d}}{\\kappa_0 + n} = \\frac{2 \\cdot 0 + 5 \\cdot \\left(\\frac{0.074}{15}\\right)}{2+5} = \\frac{5 \\cdot \\frac{0.074}{15}}{7} = \\frac{\\frac{0.074}{3}}{7} = \\frac{0.074}{21}\n$$\n\nNow, we compute the numerical value and round to four significant figures as required.\n$$\nm_n = \\frac{0.074}{21} \\approx 0.00352380952...\n$$\nThe first significant figure is the $3$ in the thousandths place. The first four significant figures are $3, 5, 2, 3$. The fifth significant figure is $8$, which is $\\ge 5$, so we round up the fourth significant figure.\n$$\n\\mathbb{E}[\\mu \\mid \\{d_{s}\\}] \\approx 0.003524\n$$", "answer": "$$\\boxed{0.003524}$$", "id": "4615261"}, {"introduction": "High predictive accuracy does not guarantee clinical utility; a model is only useful if it improves patient outcomes. This exercise introduces Decision Curve Analysis (DCA), a powerful framework for evaluating a model's real-world value by computing its 'net benefit'. By calculating this metric for a transferred model at a clinically relevant decision threshold, you will learn to assess whether a model's predictions provide more benefit than harm, a crucial step in translating machine learning models into clinical practice. [@problem_id:4615284]", "problem": "A deep convolutional neural network for malignancy detection in mammography was pre-trained on a large public dataset and then adapted to a new hospital’s data using Transfer Learning (TL). Due to domain shift, the model’s predicted probabilities in the target domain required recalibration using logistic recalibration, yielding a calibration intercept $\\alpha$ and slope $\\beta$ on the log-odds scale. Clinical adoption is based on a decision rule: intervene if the predicted probability of malignancy is at least the threshold probability $p_t$. Decision Curve Analysis (DCA) evaluates clinical utility by quantifying a model’s net benefit at a chosen $p_t$, which encodes the relative harm-to-benefit trade-off of false positives versus true positives.\n\nIn the target-domain test set of $N = 3000$ patients, the disease prevalence is $0.12$. At the threshold probability $p_t = 0.20$, the recalibrated transferred model yields $TP = 230$ true positives and $FP = 220$ false positives. For interpretability, the uncalibrated transferred model at the same threshold produces $TP = 240$ and $FP = 360$.\n\nUsing only the fundamental definitions of expected utility for binary decisions and how a threshold probability $p_t$ encodes the relative weighting of harms and benefits, compute the net benefit of the recalibrated transferred model at $p_t = 0.20$ on this target-domain test set. Express your answer as a decimal and round to four significant figures. No units are required.", "solution": "The problem statement has been validated and found to be scientifically grounded, well-posed, and objective. It presents a standard scenario in medical machine learning model evaluation. All necessary data for the calculation are provided and are internally consistent. The problem is valid.\n\nThe task is to compute the net benefit of the recalibrated transferred model at a given decision threshold. The concept of net benefit is central to Decision Curve Analysis (DCA) and is derived from expected utility theory. For a binary classification problem, a decision rule is to intervene if the predicted probability of an event, $p$, is greater than or equal to a threshold probability, $p_t$. This threshold $p_t$ implies a specific trade-off between the benefit of a true positive (treating a patient who has the disease) and the harm of a false positive (treating a patient who does not have the disease).\n\nThe net benefit is the difference between the proportion of true positives and the weighted proportion of false positives, where the weighting factor is determined by the threshold $p_t$. The fundamental formula for net benefit, $\\text{NB}$, is given by:\n$$\n\\text{NB}(p_t) = \\frac{TP}{N} - \\frac{FP}{N} \\left( \\frac{p_t}{1-p_t} \\right)\n$$\nwhere:\n- $TP$ is the number of true positives.\n- $FP$ is the number of false positives.\n- $N$ is the total number of patients in the cohort.\n- $p_t$ is the threshold probability for the decision rule.\n\nThe term $\\frac{p_t}{1-p_t}$ represents the odds of the event at the threshold. In the context of DCA, it is interpreted as the exchange rate, or the number of false positives one is willing to tolerate for each true positive detected.\n\nThe problem provides the following values for the recalibrated model tested on the target-domain set:\n- Total number of patients, $N = 3000$.\n- Number of true positives, $TP = 230$.\n- Number of false positives, $FP = 220$.\n- The decision threshold probability, $p_t = 0.20$.\n\nThe information regarding the uncalibrated model ($TP = 240$, $FP = 360$), the disease prevalence ($0.12$), and the nature of the model (deep convolutional neural network, transfer learning, logistic recalibration) provides context but is not directly required for the calculation of the net benefit for the specified recalibrated model, given that $TP$ and $FP$ values are explicitly supplied.\n\nFirst, we calculate the weighting factor based on the threshold probability $p_t$:\n$$\n\\frac{p_t}{1-p_t} = \\frac{0.20}{1 - 0.20} = \\frac{0.20}{0.80} = \\frac{1}{4} = 0.25\n$$\nNext, we substitute the given values into the net benefit formula:\n$$\n\\text{NB}(0.20) = \\frac{230}{3000} - \\frac{220}{3000} \\left( 0.25 \\right)\n$$\nWe can factor out $\\frac{1}{3000}$ to simplify the calculation:\n$$\n\\text{NB}(0.20) = \\frac{1}{3000} \\left( 230 - (220 \\times 0.25) \\right)\n$$\nCalculate the product inside the parenthesis:\n$$\n220 \\times 0.25 = 220 \\times \\frac{1}{4} = 55\n$$\nNow substitute this back into the expression for net benefit:\n$$\n\\text{NB}(0.20) = \\frac{1}{3000} (230 - 55) = \\frac{175}{3000}\n$$\nTo obtain the final decimal value, we perform the division:\n$$\n\\text{NB}(0.20) = \\frac{175}{3000} = 0.058333...\n$$\nThe problem requires the answer to be expressed as a decimal rounded to four significant figures. The first four significant figures are $5$, $8$, $3$, and $3$. The digit following the fourth significant figure is $3$, which is less than $5$, so we do not round up.\n$$\n\\text{NB}(0.20) \\approx 0.05833\n$$\nThus, the net benefit of the recalibrated model at the threshold of $p_t = 0.20$ is $0.05833$.", "answer": "$$\\boxed{0.05833}$$", "id": "4615284"}]}