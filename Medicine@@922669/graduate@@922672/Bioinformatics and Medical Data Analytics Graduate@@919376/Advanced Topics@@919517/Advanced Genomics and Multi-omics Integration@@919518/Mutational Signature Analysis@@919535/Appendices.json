{"hands_on_practices": [{"introduction": "The core of mutational signature analysis is the decomposition of a mutation catalog matrix $V$ into a product of a signature matrix $W$ and an exposure matrix $H$. This exercise provides a direct, hands-on calculation to solidify your understanding of this foundational model, $V \\approx WH$. By computing the reconstructed matrix and quantifying the reconstruction error using the Frobenius norm, you will gain a concrete feel for how abstract signatures and their activities combine to explain observed mutational patterns. [@problem_id:4587886]", "problem": "In mutational signature analysis within bioinformatics and medical data analytics, a non-negative mutation catalogue matrix $V \\in \\mathbb{R}_{\\ge 0}^{m \\times n}$ is approximated as the product $W H$ using Non-negative Matrix Factorization (NMF), where $W \\in \\mathbb{R}_{\\ge 0}^{m \\times r}$ encodes $r$ latent mutational signatures as probability distributions over $m$ mutation channels and $H \\in \\mathbb{R}_{\\ge 0}^{r \\times n}$ encodes the exposures of these signatures across $n$ samples. Consider a synthetic catalogue with $m = 6$ mutation channels and $n = 3$ samples, and a rank-$2$ approximation ($r = 2$). The columns of $W$ are normalized to sum to $1$ (each signature is a proper discrete distribution over channels), and $H$ contains non-negative exposures.\n\nYou are given the following matrices:\n$$\nV = \\begin{pmatrix}\n8 & 5 & 12 \\\\\n9 & 3 & 11 \\\\\n8 & 4 & 12 \\\\\n7 & 5 & 13 \\\\\n8 & 4 & 12 \\\\\n4 & 2 & 5\n\\end{pmatrix}, \\quad\nW = \\begin{pmatrix}\n\\frac{3}{12} & \\frac{1}{10} \\\\\n\\frac{2}{12} & \\frac{2}{10} \\\\\n\\frac{1}{12} & \\frac{3}{10} \\\\\n\\frac{3}{12} & \\frac{1}{10} \\\\\n\\frac{2}{12} & \\frac{2}{10} \\\\\n\\frac{1}{12} & \\frac{1}{10}\n\\end{pmatrix}, \\quad\nH = \\begin{pmatrix}\n24 & 12 & 36 \\\\\n20 & 10 & 30\n\\end{pmatrix}.\n$$\n\nCompute the reconstructed catalogue $W H$ and then compute the reconstruction error quantified by the Frobenius norm of the residual, namely the single scalar $\\|V - W H\\|_{F}$. Express your final error as an exact value; do not round. No units are required.", "solution": "The problem is validated as scientifically grounded, well-posed, and self-contained. The dimensions of the matrices are consistent for the required operations, and the contextual framing within mutational signature analysis is appropriate. All provided data and conditions are valid. We may proceed with the solution.\n\nThe objective is to compute the Frobenius norm of the residual matrix, $\\|V - W H\\|_{F}$, where $V$, $W$, and $H$ are given matrices. The Frobenius norm of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined as $\\|A\\|_{F} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^2}$.\n\nFirst, we must compute the reconstructed catalogue, which is the product of the signature matrix $W$ and the exposure matrix $H$.\nThe given matrices are:\n$$\nV = \\begin{pmatrix}\n8 & 5 & 12 \\\\\n9 & 3 & 11 \\\\\n8 & 4 & 12 \\\\\n7 & 5 & 13 \\\\\n8 & 4 & 12 \\\\\n4 & 2 & 5\n\\end{pmatrix}, \\quad\nW = \\begin{pmatrix}\n\\frac{3}{12} & \\frac{1}{10} \\\\\n\\frac{2}{12} & \\frac{2}{10} \\\\\n\\frac{1}{12} & \\frac{3}{10} \\\\\n\\frac{3}{12} & \\frac{1}{10} \\\\\n\\frac{2}{12} & \\frac{2}{10} \\\\\n\\frac{1}{12} & \\frac{1}{10}\n\\end{pmatrix}, \\quad\nH = \\begin{pmatrix}\n24 & 12 & 36 \\\\\n20 & 10 & 30\n\\end{pmatrix}.\n$$\nWe can simplify the fractions in matrix $W$:\n$$\nW = \\begin{pmatrix}\n\\frac{1}{4} & \\frac{1}{10} \\\\\n\\frac{1}{6} & \\frac{1}{5} \\\\\n\\frac{1}{12} & \\frac{3}{10} \\\\\n\\frac{1}{4} & \\frac{1}{10} \\\\\n\\frac{1}{6} & \\frac{1}{5} \\\\\n\\frac{1}{12} & \\frac{1}{10}\n\\end{pmatrix}\n$$\nNow, we compute the matrix product $WH$. Let the resulting matrix be $V' = WH$.\n$$\nV' = WH = \\begin{pmatrix}\n\\frac{1}{4} & \\frac{1}{10} \\\\\n\\frac{1}{6} & \\frac{1}{5} \\\\\n\\frac{1}{12} & \\frac{3}{10} \\\\\n\\frac{1}{4} & \\frac{1}{10} \\\\\n\\frac{1}{6} & \\frac{1}{5} \\\\\n\\frac{1}{12} & \\frac{1}{10}\n\\end{pmatrix}\n\\begin{pmatrix}\n24 & 12 & 36 \\\\\n20 & 10 & 30\n\\end{pmatrix}\n$$\nWe compute each element of $V'$:\n$V'_{11} = (\\frac{1}{4})(24) + (\\frac{1}{10})(20) = 6 + 2 = 8$\n$V'_{12} = (\\frac{1}{4})(12) + (\\frac{1}{10})(10) = 3 + 1 = 4$\n$V'_{13} = (\\frac{1}{4})(36) + (\\frac{1}{10})(30) = 9 + 3 = 12$\n\n$V'_{21} = (\\frac{1}{6})(24) + (\\frac{1}{5})(20) = 4 + 4 = 8$\n$V'_{22} = (\\frac{1}{6})(12) + (\\frac{1}{5})(10) = 2 + 2 = 4$\n$V'_{23} = (\\frac{1}{6})(36) + (\\frac{1}{5})(30) = 6 + 6 = 12$\n\n$V'_{31} = (\\frac{1}{12})(24) + (\\frac{3}{10})(20) = 2 + 6 = 8$\n$V'_{32} = (\\frac{1}{12})(12) + (\\frac{3}{10})(10) = 1 + 3 = 4$\n$V'_{33} = (\\frac{1}{12})(36) + (\\frac{3}{10})(30) = 3 + 9 = 12$\n\n$V'_{41} = (\\frac{1}{4})(24) + (\\frac{1}{10})(20) = 6 + 2 = 8$\n$V'_{42} = (\\frac{1}{4})(12) + (\\frac{1}{10})(10) = 3 + 1 = 4$\n$V'_{43} = (\\frac{1}{4})(36) + (\\frac{1}{10})(30) = 9 + 3 = 12$\n\n$V'_{51} = (\\frac{1}{6})(24) + (\\frac{1}{5})(20) = 4 + 4 = 8$\n$V'_{52} = (\\frac{1}{6})(12) + (\\frac{1}{5})(10) = 2 + 2 = 4$\n$V'_{53} = (\\frac{1}{6})(36) + (\\frac{1}{5})(30) = 6 + 6 = 12$\n\n$V'_{61} = (\\frac{1}{12})(24) + (\\frac{1}{10})(20) = 2 + 2 = 4$\n$V'_{62} = (\\frac{1}{12})(12) + (\\frac{1}{10})(10) = 1 + 1 = 2$\n$V'_{63} = (\\frac{1}{12})(36) + (\\frac{1}{10})(30) = 3 + 3 = 6$\n\nThus, the reconstructed matrix is:\n$$\nWH = \\begin{pmatrix}\n8 & 4 & 12 \\\\\n8 & 4 & 12 \\\\\n8 & 4 & 12 \\\\\n8 & 4 & 12 \\\\\n8 & 4 & 12 \\\\\n4 & 2 & 6\n\\end{pmatrix}\n$$\nNext, we compute the residual matrix, $R = V - WH$.\n$$\nR = \\begin{pmatrix}\n8 & 5 & 12 \\\\\n9 & 3 & 11 \\\\\n8 & 4 & 12 \\\\\n7 & 5 & 13 \\\\\n8 & 4 & 12 \\\\\n4 & 2 & 5\n\\end{pmatrix} - \\begin{pmatrix}\n8 & 4 & 12 \\\\\n8 & 4 & 12 \\\\\n8 & 4 & 12 \\\\\n8 & 4 & 12 \\\\\n8 & 4 & 12 \\\\\n4 & 2 & 6\n\\end{pmatrix} = \\begin{pmatrix}\n8-8 & 5-4 & 12-12 \\\\\n9-8 & 3-4 & 11-12 \\\\\n8-8 & 4-4 & 12-12 \\\\\n7-8 & 5-4 & 13-12 \\\\\n8-8 & 4-4 & 12-12 \\\\\n4-4 & 2-2 & 5-6\n\\end{pmatrix}\n$$\n$$\nR = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & -1 & -1 \\\\\n0 & 0 & 0 \\\\\n-1 & 1 & 1 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & -1\n\\end{pmatrix}\n$$\nFinally, we compute the Frobenius norm of the residual matrix $R$.\n$$\n\\|R\\|_{F} = \\sqrt{\\sum_{i=1}^{6} \\sum_{j=1}^{3} R_{ij}^2}\n$$\nThe sum of the squares of the elements of $R$ is:\n$$\n\\sum R_{ij}^2 = (0^2 + 1^2 + 0^2) + (1^2 + (-1)^2 + (-1)^2) + (0^2 + 0^2 + 0^2) + ((-1)^2 + 1^2 + 1^2) + (0^2 + 0^2 + 0^2) + (0^2 + 0^2 + (-1)^2)\n$$\n$$\n\\sum R_{ij}^2 = (0+1+0) + (1+1+1) + (0) + (1+1+1) + (0) + (1)\n$$\n$$\n\\sum R_{ij}^2 = 1 + 3 + 3 + 1 = 8\n$$\nTherefore, the Frobenius norm is:\n$$\n\\|V - WH\\|_{F} = \\|R\\|_{F} = \\sqrt{8} = \\sqrt{4 \\times 2} = 2\\sqrt{2}\n$$\nThe reconstruction error is $2\\sqrt{2}$.", "answer": "$$\\boxed{2\\sqrt{2}}$$", "id": "4587886"}, {"introduction": "Beyond understanding the mechanics of the model, a critical question in any quantitative analysis is: how much data is enough? This practice moves from pure algebra to statistical theory, guiding you to determine the minimum number of mutations required for a reliable analysis. You will apply principles of maximum likelihood estimation and Fisher information to calculate the sample size needed to estimate a signature's contribution with a pre-defined level of statistical confidence. [@problem_id:4587872]", "problem": "Consider a mutational signature refitting scenario in which each tumor sampleâ€™s observed mutational catalog across $M$ categories is modeled as a multinomial random vector with total count $n$ and category probabilities $\\mathbf{p}(\\mathbf{w})$, where $\\mathbf{p}(\\mathbf{w})$ is a convex mixture of a fixed signature catalog. Specifically, let there be $K=2$ fixed signatures with normalized profiles $\\mathbf{s}^{(1)}$ and $\\mathbf{s}^{(2)}$ over $M=4$ categories, and let the exposure to signature $1$ be $w \\in (0,1)$ and to signature $2$ be $1-w$. The mixture model is\n$$\n\\mathbf{p}(w) \\;=\\; w\\,\\mathbf{s}^{(1)} \\;+\\; (1-w)\\,\\mathbf{s}^{(2)} \\;=\\; \\mathbf{s}^{(2)} + w\\big(\\mathbf{s}^{(1)} - \\mathbf{s}^{(2)}\\big).\n$$\nAssume large-sample maximum likelihood estimation and asymptotic normality for the exposure estimator to signature $1$, $\\hat{w}$, derived under the multinomial model. Reliability is defined in terms of a two-sided Wald confidence interval for $w$ at $95\\%$ confidence having half-width at most $\\delta$. Under the multinomial variance model and differentiable parameterization, the asymptotic variance of the maximum likelihood estimator (MLE) $\\hat{w}$ is determined by the Fisher information for $w$.\n\nYou are given the fixed signatures and the true exposure:\n- $\\mathbf{s}^{(1)} = \\big(0.4,\\,0.3,\\,0.2,\\,0.1\\big)$,\n- $\\mathbf{s}^{(2)} = \\big(0.1,\\,0.2,\\,0.3,\\,0.4\\big)$,\n- $w = 0.6$.\n\nLet the reliability threshold be $\\delta = 0.05$, and use the standard normal quantile $z_{0.975}$ for a $95\\%$ confidence level. Using first principles of likelihood-based inference for multinomial models (deriving the Fisher information for $w$ from the multinomial log-likelihood and the Jacobian of $\\mathbf{p}(w)$ with respect to $w$), determine the minimal number of mutations per sample, $n_{\\min}$, such that the Wald interval for $w$ has half-width at most $\\delta$.\n\nYour final answer must be the single minimal integer $n_{\\min}$ that satisfies this reliability criterion. Express the result as a whole number of mutations per sample. Do not include units inside the final answer box.", "solution": "The user wants to determine the minimal number of mutations, $n_{\\min}$, required to achieve a specified precision for the estimate of a mutational signature exposure, $w$. The precision is defined by the half-width of a $95\\%$ Wald confidence interval for $w$, which must be no larger than a threshold $\\delta$.\n\nFirst, I will perform the problem validation.\n\n### Step 1: Extract Givens\n- **Model**: The observed mutational catalog is a multinomial random vector.\n- **Number of categories**: $M=4$.\n- **Number of signatures**: $K=2$.\n- **Total mutation count**: $n$.\n- **Category probabilities**: $\\mathbf{p}(w) = w\\,\\mathbf{s}^{(1)} + (1-w)\\,\\mathbf{s}^{(2)}$.\n- **Signature 1 profile**: $\\mathbf{s}^{(1)} = (0.4,\\,0.3,\\,0.2,\\,0.1)$.\n- **Signature 2 profile**: $\\mathbf{s}^{(2)} = (0.1,\\,0.2,\\,0.3,\\,0.4)$.\n- **True exposure to signature 1**: $w = 0.6$.\n- **Reliability threshold (maximum half-width)**: $\\delta = 0.05$.\n- **Confidence level**: $95\\%$.\n- **Standard normal quantile**: $z_{0.975}$.\n- **Assumptions**: Large-sample maximum likelihood estimation (MLE) and asymptotic normality of the estimator $\\hat{w}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on a standard statistical model (multinomial mixture model) widely used in bioinformatics for mutational signature analysis. The methods employed, namely maximum likelihood estimation, Fisher information, and Wald confidence intervals, are fundamental principles of statistical inference. The problem is scientifically sound.\n- **Well-Posed**: The problem is clearly stated and provides all necessary information to compute a unique integer value for $n_{\\min}$. The question is mathematically and statistically well-defined.\n- **Objective**: The problem is formulated using precise, quantitative, and unbiased language.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. I will now proceed with the solution.\n\nThe solution is derived from the principles of likelihood-based inference for multinomial models. The goal is to find the minimum sample size, $n$, such that the half-width of the $95\\%$ Wald confidence interval for the exposure parameter $w$ is at most $\\delta$.\n\nThe $95\\%$ Wald confidence interval for $w$ is given by:\n$$ \\hat{w} \\pm z_{0.975} \\sqrt{\\text{Var}(\\hat{w})} $$\nwhere $\\hat{w}$ is the MLE of $w$, and $z_{0.975}$ is the quantile of the standard normal distribution corresponding to a cumulative probability of $0.975$. The standard value is $z_{0.975} \\approx 1.96$.\n\nThe half-width of this interval is $h = z_{0.975} \\sqrt{\\text{Var}(\\hat{w})}$. The reliability criterion is $h \\le \\delta$.\n$$ z_{0.975} \\sqrt{\\text{Var}(\\hat{w})} \\le \\delta $$\nUnder the assumption of large-sample MLE, the variance of $\\hat{w}$ is approximated by the inverse of the Fisher information, $I_n(w)$:\n$$ \\text{Var}(\\hat{w}) \\approx [I_n(w)]^{-1} $$\nFor a sample of $n$ independent observations (mutations), the total Fisher information is $I_n(w) = n I_1(w)$, where $I_1(w)$ is the Fisher information for a single observation. The inequality becomes:\n$$ z_{0.975} \\sqrt{\\frac{1}{n I_1(w)}} \\le \\delta $$\nSquaring both sides and rearranging to solve for $n$, we get:\n$$ \\frac{z_{0.975}^2}{n I_1(w)} \\le \\delta^2 \\implies n \\ge \\frac{z_{0.975}^2}{\\delta^2 I_1(w)} $$\nThe minimal integer number of mutations, $n_{\\min}$, is the smallest integer satisfying this condition, which is the ceiling of the right-hand side.\n$$ n_{\\min} = \\left\\lceil \\frac{z_{0.975}^2}{\\delta^2 I_1(w)} \\right\\rceil $$\nNow, we must derive the expression for the Fisher information, $I_1(w)$. For a single trial from a multinomial distribution with probabilities $\\mathbf{p}(w) = (p_1(w), \\dots, p_M(w))$ that depend on a scalar parameter $w$, the Fisher information is given by:\n$$ I_1(w) = \\sum_{j=1}^{M} \\frac{1}{p_j(w)} \\left( \\frac{dp_j(w)}{dw} \\right)^2 $$\nThe problem defines the probability mixture model as $\\mathbf{p}(w) = w\\,\\mathbf{s}^{(1)} + (1-w)\\,\\mathbf{s}^{(2)}$.\nThe derivative of the vector of probabilities $\\mathbf{p}(w)$ with respect to $w$ is:\n$$ \\frac{d\\mathbf{p}(w)}{dw} = \\mathbf{s}^{(1)} - \\mathbf{s}^{(2)} $$\nLet's compute this vector difference using the given signature profiles:\n$\\mathbf{s}^{(1)} = (0.4, 0.3, 0.2, 0.1)$\n$\\mathbf{s}^{(2)} = (0.1, 0.2, 0.3, 0.4)$\n$$ \\frac{d\\mathbf{p}(w)}{dw} = (0.4-0.1, 0.3-0.2, 0.2-0.3, 0.1-0.4) = (0.3, 0.1, -0.1, -0.3) $$\nLet $\\mathbf{d} = \\frac{d\\mathbf{p}(w)}{dw}$. The components are $d_j = \\frac{dp_j(w)}{dw}$.\n\nNext, we compute the probability vector $\\mathbf{p}(w)$ at the true exposure value $w=0.6$:\n$$ \\mathbf{p}(0.6) = 0.6\\,\\mathbf{s}^{(1)} + (1-0.6)\\,\\mathbf{s}^{(2)} = 0.6\\,\\mathbf{s}^{(1)} + 0.4\\,\\mathbf{s}^{(2)} $$\n$$ p_1(0.6) = 0.6(0.4) + 0.4(0.1) = 0.24 + 0.04 = 0.28 $$\n$$ p_2(0.6) = 0.6(0.3) + 0.4(0.2) = 0.18 + 0.08 = 0.26 $$\n$$ p_3(0.6) = 0.6(0.2) + 0.4(0.3) = 0.12 + 0.12 = 0.24 $$\n$$ p_4(0.6) = 0.6(0.1) + 0.4(0.4) = 0.06 + 0.16 = 0.22 $$\nSo, $\\mathbf{p}(0.6) = (0.28, 0.26, 0.24, 0.22)$.\n\nNow, we can calculate the Fisher information for a single trial, $I_1(w)$, at $w=0.6$:\n$$ I_1(0.6) = \\sum_{j=1}^{4} \\frac{d_j^2}{p_j(0.6)} = \\frac{(0.3)^2}{0.28} + \\frac{(0.1)^2}{0.26} + \\frac{(-0.1)^2}{0.24} + \\frac{(-0.3)^2}{0.22} $$\n$$ I_1(0.6) = \\frac{0.09}{0.28} + \\frac{0.01}{0.26} + \\frac{0.01}{0.24} + \\frac{0.09}{0.22} $$\n$$ I_1(0.6) = \\frac{9}{28} + \\frac{1}{26} + \\frac{1}{24} + \\frac{9}{22} $$\nNumerically, this is:\n$$ I_1(0.6) \\approx 0.32142857 + 0.03846154 + 0.04166667 + 0.40909091 \\approx 0.81064769 $$\nFinally, we can compute $n_{\\min}$ using the given values $\\delta=0.05$ and $z_{0.975} = 1.96$:\n$$ n_{\\min} = \\left\\lceil \\frac{(1.96)^2}{(0.05)^2 \\times I_1(0.6)} \\right\\rceil $$\n$$ n_{\\min} = \\left\\lceil \\frac{3.8416}{0.0025 \\times 0.81064769} \\right\\rceil $$\n$$ n_{\\min} = \\left\\lceil \\frac{3.8416}{0.0020266192} \\right\\rceil $$\n$$ n_{\\min} = \\lceil 1895.5506 \\dots \\rceil $$\nSince the number of mutations must be an integer, we take the ceiling of this value.\n$$ n_{\\min} = 1896 $$\nTherefore, a minimum of $1896$ mutations per sample is required to ensure the $95\\%$ confidence interval for the exposure to signature $1$ has a half-width of at most $0.05$.", "answer": "$$\\boxed{1896}$$", "id": "4587872"}, {"introduction": "With a firm grasp of the theory, the final step is to apply it in a realistic computational scenario. This coding challenge simulates a complete signature refitting workflow, a common task in cancer genomics. You will programmatically construct signature profiles, estimate their contributions to a synthetic tumor by solving a non-negative least squares problem, and assess the model's performance using cosine similarity, a standard metric for comparing profile shapes. [@problem_id:4587927]", "problem": "You are given a reference matrix of Simple Base Substitution (SBS) mutational signatures and observed single-nucleotide variant counts in $96$ categorical mutation channels. The goal is to estimate non-negative signature exposures and evaluate how well the reconstructed profile matches the observed profile using cosine similarity. The task is grounded in linear algebra and convex optimization, and the underlying principles are the following: profiles are represented as non-negative vectors in $\\mathbb{R}^{96}$, signature exposures are non-negative, and the reconstruction is a non-negative linear combination of reference signatures.\n\nStart from the following foundational base:\n\n- Linear superposition: an observed count vector $c \\in \\mathbb{R}_{\\ge 0}^{96}$ is modeled as $c \\approx S e$, where $S \\in \\mathbb{R}_{\\ge 0}^{96 \\times K}$ has columns that are reference signatures and $e \\in \\mathbb{R}_{\\ge 0}^{K}$ are their non-negative exposures.\n- Least squares optimality: the exposure estimate should minimize the squared Euclidean reconstruction error under non-negativity, i.e., minimize $\\lVert S e - c \\rVert_2^2$ subject to $e \\ge 0$.\n- Normalization and identifiability: when each column of $S$ is normalized to have $\\ell_1$ sum equal to $1$, the total reconstructed load $\\sum_{i=1}^{96} (S e)_i$ equals $\\sum_{j=1}^{K} e_j$. This identity enables matching the reconstruction total to the observed total by scaling $e$.\n- Cosine similarity: for two vectors $x, y \\in \\mathbb{R}^{96}$, the cosine similarity is defined as $\\mathrm{cos}(x, y) = \\dfrac{x^\\top y}{\\lVert x \\rVert_2 \\lVert y \\rVert_2}$ when both norms are non-zero.\n\nYour program must implement the following precise specification.\n\n1. Signature construction. Use $K = 4$ reference signatures inspired by Catalogue Of Somatic Mutations In Cancer (COSMIC) Simple Base Substitution (SBS) style $96$-channel profiles. Partition the $96$ channels into four contiguous groups of $24$ indices: group $0$ is indices $0$ through $23$, group $1$ is indices $24$ through $47$, group $2$ is indices $48$ through $71$, and group $3$ is indices $72$ through $95$. For each signature $k \\in \\{0,1,2,3\\}$ and channel index $i \\in \\{0,\\dots,95\\}$, define the unnormalized weight\n$$\nw_{i}^{(k)} = \\begin{cases}\n4, & \\text{if } \\left\\lfloor \\dfrac{i}{24} \\right\\rfloor = k, \\\\\n1, & \\text{otherwise.}\n\\end{cases}\n$$\nThen form each signature column by normalizing to unit $\\ell_1$ sum:\n$$\ns_{i}^{(k)} = \\dfrac{w_{i}^{(k)}}{\\sum_{r=0}^{95} w_{r}^{(k)}}, \\quad \\text{so that } \\sum_{i=0}^{95} s_{i}^{(k)} = 1 \\text{ for each } k.\n$$\nStack the columns to form $S \\in \\mathbb{R}_{\\ge 0}^{96 \\times 4}$.\n\n2. Exposure estimation. Given an observed count vector $c \\in \\mathbb{R}_{\\ge 0}^{96}$ with total $N = \\sum_{i=0}^{95} c_i$, compute an exposure estimate $\\hat e$ as a solution to the non-negative least squares problem\n$$\n\\hat e \\in \\arg\\min_{e \\in \\mathbb{R}_{\\ge 0}^{4}} \\lVert S e - c \\rVert_2^2.\n$$\nThen, if $N > 0$ and $\\sum_{j=0}^{3} \\hat e_j > 0$, rescale the exposure vector to match the observed total:\n$$\n\\tilde e = \\hat e \\cdot \\dfrac{N}{\\sum_{j=0}^{3} \\hat e_j}.\n$$\nIf $N = 0$, define $\\tilde e$ to be the zero vector.\n\n3. Reconstruction and cosine similarity. Form the reconstructed profile $r = S \\tilde e$. Define the cosine similarity between $c$ and $r$ as\n$$\n\\mathrm{cos}(c, r) = \\begin{cases}\n\\dfrac{c^\\top r}{\\lVert c \\rVert_2 \\lVert r \\rVert_2}, & \\text{if } \\lVert c \\rVert_2 > 0 \\text{ and } \\lVert r \\rVert_2 > 0, \\\\\n1.0, & \\text{if } \\lVert c \\rVert_2 = 0 \\text{ and } \\lVert r \\rVert_2 = 0, \\\\\n0.0, & \\text{otherwise.}\n\\end{cases}\n$$\n\n4. Test suite. Construct three observed profiles $c^{(t)} \\in \\mathbb{R}_{\\ge 0}^{96}$ ($t \\in \\{1,2,3\\}$) deterministically from specified exposure vectors $e^{(t)} \\in \\mathbb{R}_{\\ge 0}^{4}$ via the following rounding-to-integers procedure that preserves the total:\n   - For each $t$, define\n   $$\n   e^{(1)} = (200, 400, 0, 100), \\quad e^{(2)} = (0, 0, 0, 0), \\quad e^{(3)} = (0, 0, 150, 0).\n   $$\n   - Let the real-valued expected profile be $u^{(t)} = S e^{(t)} \\in \\mathbb{R}_{\\ge 0}^{96}$, which satisfies $\\sum_{i=0}^{95} u^{(t)}_i = \\sum_{j=0}^{3} e^{(t)}_j$ because each column of $S$ has unit $\\ell_1$ sum.\n   - Define preliminary integers $f_i^{(t)} = \\lfloor u^{(t)}_i \\rfloor$ for all $i$, and compute the deficit\n   $$\n   D^{(t)} = \\left(\\sum_{j=0}^{3} e^{(t)}_j\\right) - \\sum_{i=0}^{95} f_i^{(t)}.\n   $$\n   Note that $D^{(t)} \\in \\{0,1,\\dots,95\\}$.\n   - Let $\\rho_i^{(t)} = u^{(t)}_i - f_i^{(t)}$ be the fractional parts. Select the $D^{(t)}$ indices with largest $\\rho_i^{(t)}$ (breaking ties by smaller index $i$). For those indices, set $c^{(t)}_i = f_i^{(t)} + 1$, and for all other indices set $c^{(t)}_i = f_i^{(t)}$. This yields integer counts with $\\sum_{i=0}^{95} c^{(t)}_i = \\sum_{j=0}^{3} e^{(t)}_j$.\n\n5. Required output. For each test case $t \\in \\{1,2,3\\}$, compute the cosine similarity between $c^{(t)}$ and its reconstruction $r^{(t)} = S \\tilde e^{(t)}$ obtained by the procedure above. Your program should produce a single line of output containing the three cosine similarity values in order $t=1,2,3$, each rounded to $6$ decimal places, as a comma-separated list enclosed in square brackets, e.g., $[x_1,x_2,x_3]$.\n\nConstraints and clarifications:\n\n- All vectors and matrices use real numbers; counts are non-negative integers after rounding.\n- Use radians or degrees only if relevant for angles; here no angles are used explicitly besides cosine similarity as a vector-space measure, which is dimensionless.\n- If any vector is identically zero when evaluating cosine similarity, apply the explicit rule above to return $1.0$ if both are zero and $0.0$ otherwise.\n- Ensure numerical stability by explicitly normalizing the columns of $S$ to unit $\\ell_1$ sum before fitting exposures.\n\nThe problem is universally applicable: it reduces to solving a non-negative least squares problem in $\\mathbb{R}^{96}$ and computing inner products and norms. Your program must be self-contained, must not read any external input, and must adhere to the exact output format specified.", "solution": "The problem requires the implementation of a standard workflow in mutational signature analysis. This involves creating a set of reference mutational signatures, generating synthetic observed mutation counts, estimating the contributions (exposures) of each signature to the observed counts, and finally, evaluating the goodness-of-fit of the model. The entire process is grounded in principles of linear algebra and numerical optimization.\n\nThe solution is implemented by following the precise steps outlined in the problem statement.\n\n**1. Construction of the Signature Matrix $S$**\n\nThe foundation of the model is the signature matrix $S \\in \\mathbb{R}_{\\ge 0}^{96 \\times 4}$, where each of the $K=4$ columns represents a reference mutational signature profile over $96$ mutation channels. The construction is deterministic.\n\nFirst, we define an unnormalized weight matrix $W \\in \\mathbb{R}^{96 \\times 4}$. The $96$ channels are partitioned into four contiguous, non-overlapping groups of $24$ indices: group $k \\in \\{0,1,2,3\\}$ corresponds to indices $i$ from $24k$ to $24k+23$. The weight $w_{i}^{(k)}$ for the $i$-th channel in the $k$-th signature is defined as:\n$$\nw_{i}^{(k)} = \\begin{cases}\n4, & \\text{if } \\left\\lfloor \\dfrac{i}{24} \\right\\rfloor = k, \\\\\n1, & \\text{otherwise.}\n\\end{cases}\n$$\nThis structure implies that each signature $k$ is characterized by a high probability of mutations in its corresponding group of $24$ channels and a lower, uniform background probability in all other channels.\n\nTo ensure that the exposures can be interpreted as mutation counts, each signature (column) must be normalized to have an $\\ell_1$ sum of $1$. The normalization constant for each column $k$ is the sum of its weights:\n$$\n\\sum_{r=0}^{95} w_{r}^{(k)} = (24 \\times 4) + ((96-24) \\times 1) = 96 + 72 = 168.\n$$\nThis sum is constant for all signatures $k \\in \\{0,1,2,3\\}$. The normalized signature-channel probability $s_{i}^{(k)}$ is thus:\n$$\ns_{i}^{(k)} = \\dfrac{w_{i}^{(k)}}{168}.\n$$\nThe final signature matrix $S$ is formed by these columns, i.e., $S_{ik} = s_{i}^{(k)}$.\n\n**2. Generation of Observed Count Vectors $c^{(t)}$**\n\nThree test cases ($t \\in \\{1,2,3\\}$) are created from predefined true exposure vectors: $e^{(1)} = (200, 400, 0, 100)$, $e^{(2)} = (0, 0, 0, 0)$, and $e^{(3)} = (0, 0, 150, 0)$.\n\nFor each test case $t$, an observed integer count vector $c^{(t)} \\in \\mathbb{Z}_{\\ge 0}^{96}$ is generated via a deterministic rounding procedure that preserves the total number of mutations.\nFirst, the expected real-valued profile $u^{(t)} \\in \\mathbb{R}_{\\ge 0}^{96}$ is calculated as a linear combination of the signatures:\n$$\nu^{(t)} = S e^{(t)}.\n$$\nSince each column of $S$ sums to $1$, the total sum of elements in $u^{(t)}$ equals the total sum of exposures: $\\sum_{i=0}^{95} u_i^{(t)} = \\sum_{j=0}^{3} e_j^{(t)}$. Let this total be $E^{(t)}$.\n\nTo convert the real-valued profile $u^{(t)}$ to an integer-valued count vector $c^{(t)}$, we use a largest-remainder rounding method.\n- The integer part of each channel is taken: $f_i^{(t)} = \\lfloor u^{(t)}_i \\rfloor$.\n- The total number of mutations lost due to flooring is the deficit, $D^{(t)} = E^{(t)} - \\sum_{i=0}^{95} f_i^{(t)}$. This deficit is an integer.\n- The fractional parts, $\\rho_i^{(t)} = u^{(t)}_i - f_i^{(t)}$, are calculated.\n- The $D^{(t)}$ mutations are distributed one by one to the channels with the $D^{(t)}$ largest fractional parts $\\rho_i^{(t)}$. Ties are broken by choosing the smaller channel index $i$.\n- The final count vector $c^{(t)}$ is given by $c_i^{(t)} = f_i^{(t)} + 1$ for the selected $D^{(t)}$ indices, and $c_i^{(t)} = f_i^{(t)}$ for all others. By construction, $\\sum_{i=0}^{95} c_i^{(t)} = E^{(t)}$.\n\n**3. Exposure Estimation and Reconstruction**\n\nGiven the synthetic observed counts $c^{(t)}$, the next step is to estimate the exposures that generated them. The model assumes an observed profile $c$ is approximated by $Se$. The best estimate for the exposure vector $e$ is found by solving the Non-Negative Least Squares (NNLS) problem, which minimizes the squared Euclidean distance between the observed and reconstructed profiles under the constraint that exposures cannot be negative:\n$$\n\\hat e^{(t)} = \\arg\\min_{e \\in \\mathbb{R}_{\\ge 0}^{4}} \\lVert S e - c^{(t)} \\rVert_2^2.\n$$\nThis is a convex optimization problem, and its solution can be found using standard numerical algorithms, such as the one provided in `scipy.optimize.nnls`.\n\nThe resulting exposure vector $\\hat e^{(t)}$ is then rescaled to ensure that the total number of mutations in the reconstruction matches the observed total, $N^{(t)} = \\sum_{i=0}^{95} c_i^{(t)}$. The rescaled exposure vector $\\tilde e^{(t)}$ is:\n$$\n\\tilde e^{(t)} = \\hat e^{(t)} \\cdot \\dfrac{N^{(t)}}{\\sum_{j=0}^{3} \\hat e_j^{(t)}},\n$$\nprovided that $N^{(t)} > 0$ and $\\sum_{j=0}^{3} \\hat e_j^{(t)} > 0$. If $N^{(t)} = 0$, $\\tilde e^{(t)}$ is defined as the zero vector.\n\nUsing this final exposure vector, the reconstructed profile $r^{(t)}$ is computed as:\n$$\nr^{(t)} = S \\tilde e^{(t)}.\n$$\n\n**4. Evaluation with Cosine Similarity**\n\nThe quality of the reconstruction is measured by the cosine similarity between the observed vector $c^{(t)}$ and the reconstructed vector $r^{(t)}$. The cosine similarity is defined as:\n$$\n\\mathrm{cos}(c^{(t)}, r^{(t)}) = \\begin{cases}\n\\dfrac{(c^{(t)})^\\top r^{(t)}}{\\lVert c^{(t)} \\rVert_2 \\lVert r^{(t)} \\rVert_2}, & \\text{if } \\lVert c^{(t)} \\rVert_2 > 0 \\text{ and } \\lVert r^{(t)} \\rVert_2 > 0, \\\\\n1.0, & \\text{if } \\lVert c^{(t)} \\rVert_2 = 0 \\text{ and } \\lVert r^{(t)} \\rVert_2 = 0, \\\\\n0.0, & \\text{otherwise.}\n\\end{cases}\n$$\nA value close to $1.0$ indicates a very good fit in terms of profile shape. For the test case where $c^{(t)}$ is the zero vector, the procedure yields $r^{(t)}$ as the zero vector as well, resulting in a cosine similarity of $1.0$.\n\nThis entire procedure is applied to each of the three test cases, and the resulting cosine similarities are collected and formatted as the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import nnls\n\ndef solve():\n    \"\"\"\n    Implements the full mutational signature analysis pipeline as specified.\n    1. Constructs the signature matrix S.\n    2. Generates three observed count vectors c^{(t)} from true exposures e^{(t)}.\n    3. For each c^{(t)}, estimates exposures, reconstructs the profile r^{(t)},\n       and computes the cosine similarity between c^{(t)} and r^{(t)}.\n    4. Prints the results in the specified format.\n    \"\"\"\n\n    # --- 1. Signature construction ---\n    K = 4\n    N_CHANNELS = 96\n    \n    # Construct the unnormalized weight matrix W\n    W = np.ones((N_CHANNELS, K), dtype=np.float64)\n    for k in range(K):\n        start_idx = k * 24\n        end_idx = (k + 1) * 24\n        W[start_idx:end_idx, k] = 4.0\n    \n    # Normalize columns to unit l1 sum to get S\n    col_sums = W.sum(axis=0)\n    S = W / col_sums\n\n    # --- 2. Test suite generation and analysis ---\n    test_exposures = [\n        np.array([200.0, 400.0, 0.0, 100.0]),\n        np.array([0.0, 0.0, 0.0, 0.0]),\n        np.array([0.0, 0.0, 150.0, 0.0])\n    ]\n\n    results = []\n    for e_true in test_exposures:\n        # --- Generate observed profile c^{(t)} from e^{(t)} ---\n        total_exposure = e_true.sum()\n\n        if total_exposure == 0:\n            c = np.zeros(N_CHANNELS, dtype=np.float64)\n        else:\n            # Calculate real-valued expected profile u\n            u = S @ e_true\n            \n            # Get integer and fractional parts\n            f = np.floor(u)\n            rho = u - f\n            \n            # Calculate deficit using integer arithmetic on total exposure\n            # Use rounding to be robust against floating point inaccuracies\n            deficit = int(round(total_exposure - f.sum()))\n            \n            # Find indices of largest fractional parts to increment\n            # Use a stable sort to break ties by smaller index\n            indices_to_increment = np.argsort(rho, kind='stable')[::-1][:deficit]\n            \n            c = f.copy()\n            c[indices_to_increment] += 1.0\n\n        # At this point, c is the observed count vector (as floats, but representing ints)\n\n        # --- 3. Exposure estimation and reconstruction ---\n        \n        # Estimate exposures using Non-Negative Least Squares (NNLS)\n        e_hat, _ = nnls(S, c)\n        \n        # Rescale exposures\n        N_counts = c.sum()\n        e_hat_sum = e_hat.sum()\n        \n        e_tilde = np.zeros_like(e_hat)\n        if N_counts > 0:\n            if e_hat_sum > 0:\n                e_tilde = e_hat * (N_counts / e_hat_sum)\n        # If N_counts is 0, e_tilde remains the zero vector, as per problem spec.\n\n        # Reconstruct the profile\n        r = S @ e_tilde\n\n        # --- 4. Cosine similarity calculation ---\n        norm_c = np.linalg.norm(c)\n        norm_r = np.linalg.norm(r)\n        \n        similarity = 0.0\n        if norm_c > 1e-9 and norm_r > 1e-9:\n            similarity = np.dot(c, r) / (norm_c * norm_r)\n        elif norm_c  1e-9 and norm_r  1e-9:\n            similarity = 1.0\n        # Otherwise, similarity remains 0.0 as initialized\n        \n        results.append(similarity)\n\n    # --- 5. Final output formatting ---\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4587927"}]}