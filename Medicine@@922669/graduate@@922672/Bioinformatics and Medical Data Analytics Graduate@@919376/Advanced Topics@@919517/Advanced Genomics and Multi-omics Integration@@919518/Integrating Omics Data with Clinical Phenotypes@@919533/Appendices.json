{"hands_on_practices": [{"introduction": "Before integrating omics data with clinical phenotypes, the data must be rigorously prepared. This first exercise tackles a fundamental preprocessing step in RNA-sequencing analysis: normalization. You will derive and apply the robust median-of-ratios method to correct for technical variations in sequencing depth, ensuring that gene expression levels are comparable across different samples [@problem_id:4574649]. Mastering this technique is essential, as proper normalization is the bedrock upon which all valid downstream analyses, from differential expression to predictive modeling, are built.", "problem": "You are integrating Ribonucleic Acid sequencing (RNA-seq) transcript abundance with a binary clinical phenotype to enable downstream association modeling using a Generalized Linear Model (GLM). You observe an RNA-seq count matrix $X \\in \\mathbb{R}^{n \\times p}$ with $n$ genes and $p$ samples, where the $g$-th row corresponds to gene $g$ and the $j$-th column corresponds to sample $j$. Suppose $n = 4$ and $p = 3$, and the observed counts are\n$$\nX \\;=\\; \\begin{pmatrix}\n100 & 200 & 50 \\\\\n50 & 100 & 25 \\\\\n20 & 40 & 10 \\\\\n80 & 160 & 40\n\\end{pmatrix}.\n$$\nYou also record a binary phenotype vector $y \\in \\{0,1\\}^{p}$ indicating disease status for the samples, with $y = (0, 1, 0)$, and plan to fit a log-link GLM after between-sample normalization to remove multiplicative library size effects.\n\nStarting only from the fundamental modeling principle that the observed counts for gene $g$ in sample $j$ arise from a multiplicative decomposition of a sample-specific library size factor $s_{j} > 0$ and a gene- and sample-specific underlying expression level $\\theta_{g,j} > 0$, and that a robust normalizing procedure should be invariant to per-gene scaling across samples, derive a robust estimator of the library size factors $\\{s_{j}\\}_{j=1}^{p}$ consistent with these principles. Then, using this estimator, compute the normalized counts $\\tilde{X}$ via element-wise division of $X$ by the corresponding library size factors.\n\nFinally, report the single normalized count value $\\tilde{x}_{4,2}$, that is, the normalized count for gene $g=4$ in sample $j=2$. Express your final answer as a pure number with no units. No rounding is required.", "solution": "The problem requires the derivation and application of a robust estimator for library size factors in RNA-seq data analysis, based on fundamental principles.\n\nLet the observed count for gene $g$ in sample $j$ be denoted by $x_{g,j}$, where $g \\in \\{1, \\dots, n\\}$ and $j \\in \\{1, \\dots, p\\}$. The problem provides a count matrix $X \\in \\mathbb{R}^{n \\times p}$ with $n=4$ genes and $p=3$ samples.\n\nThe fundamental modeling principle states a multiplicative decomposition for the expected count:\n$$ E[x_{g,j}] \\approx s_{j} \\theta_{g,j} $$\nwhere $s_{j} > 0$ is the sample-specific library size factor and $\\theta_{g,j} > 0$ is the underlying expression level for gene $g$ in sample $j$.\n\nThe second principle is that the estimation procedure for the set of size factors $\\{s_j\\}_{j=1}^p$ must be invariant to per-gene scaling. That is, if we construct a new count matrix $X'$ where each row $g$ is scaled by a constant $c_g > 0$ such that $x'_{g,j} = c_g x_{g,j}$, the resulting size factor estimates $s'_j$ should be identical to the original estimates $s_j$.\n\nTo derive an estimator satisfying these properties, we construct a pseudo-reference sample that serves as a common baseline. A robust choice for the expression level of gene $g$ in this pseudo-reference sample, which we denote $\\mu_g$, is the geometric mean of the counts for gene $g$ across all samples:\n$$ \\mu_g = \\left( \\prod_{k=1}^{p} x_{g,k} \\right)^{1/p} $$\nLet's verify how this term behaves under per-gene scaling. If $x'_{g,j} = c_g x_{g,j}$, the new geometric mean $\\mu'_g$ is:\n$$ \\mu'_g = \\left( \\prod_{k=1}^{p} x'_{g,k} \\right)^{1/p} = \\left( \\prod_{k=1}^{p} c_g x_{g,k} \\right)^{1/p} = \\left( c_g^p \\prod_{k=1}^{p} x_{g,k} \\right)^{1/p} = c_g \\left( \\prod_{k=1}^{p} x_{g,k} \\right)^{1/p} = c_g \\mu_g $$\nSo, the pseudo-reference value for gene $g$ scales by the same factor $c_g$.\n\nNow, for each gene $g$ in each sample $j$, we compute the ratio of its observed count to the pseudo-reference count for that gene:\n$$ r_{g,j} = \\frac{x_{g,j}}{\\mu_g} $$\nIf we consider the multiplicative model, this ratio is approximately:\n$$ r_{g,j} \\approx \\frac{s_{j} \\theta_{g,j}}{\\mu_g} $$\nFor many genes, their expression levels might not vary dramatically across the biological conditions represented by the samples, meaning $\\theta_{g,j}$ is relatively constant for a given $g$. In this scenario, $\\mu_g$ can be seen as an estimate of a quantity proportional to this common expression level. The ratio $r_{g,j}$ would then be approximately proportional to $s_j$.\n\nCrucially, let's examine the behavior of this ratio under per-gene scaling:\n$$ r'_{g,j} = \\frac{x'_{g,j}}{\\mu'_g} = \\frac{c_g x_{g,j}}{c_g \\mu_g} = \\frac{x_{g,j}}{\\mu_g} = r_{g,j} $$\nThe ratios are invariant to the per-gene scaling factors $c_g$.\n\nFor a given sample $j$, we now have a set of $n$ ratios, $\\{r_{1,j}, r_{2,j}, \\dots, r_{n,j}\\}$. Each of these ratios is an estimate of the size factor $s_j$. To obtain a single, robust estimate for $s_j$, we take the median of this set of ratios. The median is robust to outliers, such as genes that are strongly differentially expressed, for which the assumption $\\theta_{g,j} \\approx \\theta_{g,k}$ might not hold.\nTherefore, our robust estimator for the size factor of sample $j$ is:\n$$ s_j = \\underset{g}{\\text{median}} \\left\\{ \\frac{x_{g,j}}{\\left( \\prod_{k=1}^{p} x_{g,k} \\right)^{1/p}} \\right\\} $$\nThis is known as the median-of-ratios method.\n\nWe now apply this procedure to the given data. The count matrix is:\n$$\nX = \\begin{pmatrix}\n100 & 200 & 50 \\\\\n50 & 100 & 25 \\\\\n20 & 40 & 10 \\\\\n80 & 160 & 40\n\\end{pmatrix}\n$$\nwith $n=4$ and $p=3$.\n\nFirst, we compute the geometric mean $\\mu_g$ for each gene (row):\n$$ \\mu_1 = (100 \\times 200 \\times 50)^{1/3} = (1000000)^{1/3} = 100 $$\n$$ \\mu_2 = (50 \\times 100 \\times 25)^{1/3} = (125000)^{1/3} = 50 $$\n$$ \\mu_3 = (20 \\times 40 \\times 10)^{1/3} = (8000)^{1/3} = 20 $$\n$$ \\mu_4 = (80 \\times 160 \\times 40)^{1/3} = (512000)^{1/3} = 80 $$\n\nNext, we compute the matrix of ratios $R$, where $r_{g,j} = x_{g,j} / \\mu_g$:\n$$\nR = \\begin{pmatrix}\n100/100 & 200/100 & 50/100 \\\\\n50/50 & 100/50 & 25/50 \\\\\n20/20 & 40/20 & 10/20 \\\\\n80/80 & 160/80 & 40/80\n\\end{pmatrix}\n= \\begin{pmatrix}\n1.0 & 2.0 & 0.5 \\\\\n1.0 & 2.0 & 0.5 \\\\\n1.0 & 2.0 & 0.5 \\\\\n1.0 & 2.0 & 0.5\n\\end{pmatrix}\n$$\n\nNow, we estimate the size factor $s_j$ for each sample $j$ by taking the median of the corresponding column in $R$:\n$$ s_1 = \\text{median}\\{1.0, 1.0, 1.0, 1.0\\} = 1.0 $$\n$$ s_2 = \\text{median}\\{2.0, 2.0, 2.0, 2.0\\} = 2.0 $$\n$$ s_3 = \\text{median}\\{0.5, 0.5, 0.5, 0.5\\} = 0.5 $$\nThe vector of size factors is $(s_1, s_2, s_3) = (1.0, 2.0, 0.5)$.\n\nFinally, we compute the normalized count matrix $\\tilde{X}$ by dividing each element $x_{g,j}$ by its corresponding sample's size factor $s_j$: $\\tilde{x}_{g,j} = x_{g,j} / s_j$. This is equivalent to dividing each column of $X$ by the corresponding size factor.\n$$\n\\tilde{X} = \\begin{pmatrix}\n100/1.0 & 200/2.0 & 50/0.5 \\\\\n50/1.0 & 100/2.0 & 25/0.5 \\\\\n20/1.0 & 40/2.0 & 10/0.5 \\\\\n80/1.0 & 160/2.0 & 40/0.5\n\\end{pmatrix}\n= \\begin{pmatrix}\n100 & 100 & 100 \\\\\n50 & 50 & 50 \\\\\n20 & 20 & 20 \\\\\n80 & 80 & 80\n\\end{pmatrix}\n$$\n\nThe problem asks for the normalized count value $\\tilde{x}_{4,2}$, which is the element in the 4th row and 2nd column of $\\tilde{X}$.\n$$ \\tilde{x}_{4,2} = \\frac{x_{4,2}}{s_2} = \\frac{160}{2.0} = 80 $$\nFrom the computed matrix $\\tilde{X}$, we can directly read this value.\n\nThe phenotype vector $y=(0, 1, 0)$ is provided to establish the context of a downstream analysis (a GLM), but it is not used in the normalization calculation itself, which is a prerequisite step.\nThe final required value is $\\tilde{x}_{4,2}$.", "answer": "$$\n\\boxed{80}\n$$", "id": "4574649"}, {"introduction": "A key challenge in bioinformatics is that bulk tissue omics data represents an average signal from a mixture of cell types, obscuring the specific cellular drivers of disease. This practice guides you through a powerful integration strategy: computational deconvolution [@problem_id:4574619]. By implementing a method based on nonnegative least squares, you will learn to use a single-cell reference atlas to estimate the cellular composition of bulk tissue samples, turning a mixed signal into interpretable features that can be directly associated with clinical outcomes.", "problem": "You are given a framework for integrating Ribonucleic Acid sequencing (RNA-seq) bulk gene expression data with clinical phenotypes by estimating cell-type proportions from a single-cell reference using the linear mixing model and nonnegative least squares. The principle is that bulk expression is a mixture of cell-type-specific expression profiles, consistent with the Central Dogma of Molecular Biology and additivity of transcript counts across constituent cell types within a tissue sample. This yields a linear model: for $G$ genes and $C$ cell types, the bulk expression vector $\\mathbf{y} \\in \\mathbb{R}^{G}$ is modeled as\n$$\n\\mathbf{y} = X \\mathbf{p} + \\boldsymbol{\\varepsilon},\n$$\nwhere $X \\in \\mathbb{R}^{G \\times C}$ is the reference matrix of mean expression for each gene in each cell type derived from single-cell measurements, $\\mathbf{p} \\in \\mathbb{R}^{C}$ is the unknown nonnegative vector of cell-type proportions, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{G}$ is measurement noise. The biological constraints imply $p_c \\geq 0$ for all $c$ and, under the interpretation of $\\mathbf{p}$ as fractions, $\\sum_{c=1}^{C} p_c = 1$. In practice, one can estimate $\\mathbf{p}$ by solving the nonnegative least squares problem\n$$\n\\min_{\\mathbf{p} \\in \\mathbb{R}^{C}} \\|\\;X \\mathbf{p} - \\mathbf{y}\\;\\|_2 \\quad \\text{subject to} \\quad p_c \\geq 0 \\text{ for all } c,\n$$\nand then renormalize $\\mathbf{p}$ to have unit sum, because least squares does not enforce the sum-to-one constraint. Accuracy can be evaluated against simulated mixtures with known proportions.\n\nImplement a program that does the following for the provided test suite:\n- For each test case, construct the specified single-cell reference matrix $X$, true proportions $\\mathbf{p}^{\\star}$ (or its variant), and bulk vector $\\mathbf{y}$ according to the case description.\n- Estimate $\\widehat{\\mathbf{p}}$ by solving the nonnegative least squares problem and renormalize it to satisfy $\\sum_{c=1}^{C} \\widehat{p}_c = 1$ when $\\sum_{c=1}^{C} \\widehat{p}_c > 0$, otherwise set $\\widehat{p}_c = \\frac{1}{C}$ for all $c$.\n- Compute the root mean squared error (RMSE) between $\\widehat{\\mathbf{p}}$ and the specified ground-truth proportions for each case, using\n$$\n\\mathrm{RMSE}(\\widehat{\\mathbf{p}}, \\mathbf{p}^{\\dagger}) = \\sqrt{\\frac{1}{C} \\sum_{c=1}^{C} \\left(\\widehat{p}_c - p^{\\dagger}_c\\right)^2}.\n$$\n- For the clinical phenotype case, simulate a small cohort of bulk samples with known proportions and compute the Pearson correlation coefficient between the estimated fraction for a specified cell type and a clinical severity score vector $\\mathbf{s}$. Use the formula\n$$\nr = \\frac{\\sum_{t=1}^{T} \\left(\\widehat{p}_{j}^{(t)} - \\overline{\\widehat{p}_{j}}\\right)\\left(s^{(t)} - \\overline{s}\\right)}{\\sqrt{\\sum_{t=1}^{T}\\left(\\widehat{p}_{j}^{(t)} - \\overline{\\widehat{p}_{j}}\\right)^2}\\sqrt{\\sum_{t=1}^{T}\\left(s^{(t)} - \\overline{s}\\right)^2}},\n$$\nwhere $\\widehat{p}_{j}^{(t)}$ is the estimated fraction of cell type $j$ for sample $t$, $\\overline{\\widehat{p}_{j}}$ is its mean across samples, $s^{(t)}$ is the clinical severity for sample $t$, and $\\overline{s}$ is its mean. Express proportions as decimals, not percentages.\n\nTest Suite Specification:\n- All matrices and vectors are given explicitly. Use them exactly as specified without any additional normalization or transformation beyond what is described above. All numerical values are decimals, and any derived values should also be treated as decimals.\n\nCase $1$ (well-conditioned reference, noiseless mixture):\n- Genes $G = 5$, cell types $C = 3$.\n- Reference matrix\n$$\nX_1 = \\begin{bmatrix}\n8 & 2 & 1 \\\\\n4 & 1.5 & 2 \\\\\n3 & 3 & 6 \\\\\n5 & 1 & 0.5 \\\\\n7 & 2.5 & 1\n\\end{bmatrix}.\n$$\n- True proportions\n$$\n\\mathbf{p}^{\\star}_1 = \\begin{bmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{bmatrix}.\n$$\n- Bulk vector\n$$\n\\mathbf{y}_1 = X_1 \\mathbf{p}^{\\star}_1 = \\begin{bmatrix} 4.8 \\\\ 2.85 \\\\ 3.6 \\\\ 2.9 \\\\ 4.45 \\end{bmatrix}.\n$$\n- Ground-truth for RMSE: $\\mathbf{p}^{\\dagger} = \\mathbf{p}^{\\star}_1$.\n\nCase $2$ (near-collinearity between cell-type profiles):\n- Reference matrix\n$$\nX_2 = \\begin{bmatrix}\n6 & 6.2 & 1 \\\\\n3 & 3.1 & 2 \\\\\n5 & 5.1 & 6 \\\\\n4 & 4.05 & 0.5 \\\\\n2 & 2.02 & 1\n\\end{bmatrix}.\n$$\n- True proportions\n$$\n\\mathbf{p}^{\\star}_2 = \\begin{bmatrix} 0.4 \\\\ 0.4 \\\\ 0.2 \\end{bmatrix}.\n$$\n- Bulk vector\n$$\n\\mathbf{y}_2 = X_2 \\mathbf{p}^{\\star}_2 = \\begin{bmatrix} 5.08 \\\\ 2.84 \\\\ 5.24 \\\\ 3.32 \\\\ 1.808 \\end{bmatrix}.\n$$\n- Ground-truth for RMSE: $\\mathbf{p}^{\\dagger} = \\mathbf{p}^{\\star}_2$.\n\nCase $3$ (unmodeled cell type present in bulk):\n- Use $X_3 = X_1$.\n- Known cell-type proportions (do not sum to one)\n$$\n\\mathbf{p}^{\\text{known}}_3 = \\begin{bmatrix} 0.5 \\\\ 0.25 \\\\ 0.15 \\end{bmatrix}, \\quad \\text{so} \\quad \\sum_{c=1}^{3} p^{\\text{known}}_{3,c} = 0.9.\n$$\n- Unknown cell type fraction is $0.1$ with expression vector\n$$\n\\mathbf{u} = \\begin{bmatrix} 4 \\\\ 1 \\\\ 3 \\\\ 2 \\\\ 5 \\end{bmatrix}.\n$$\n- Bulk vector\n$$\n\\mathbf{y}_3 = X_3 \\mathbf{p}^{\\text{known}}_3 + 0.1 \\cdot \\mathbf{u} = \\begin{bmatrix} 5.05 \\\\ 2.775 \\\\ 3.45 \\\\ 3.025 \\\\ 4.775 \\end{bmatrix}.\n$$\n- Ground-truth for RMSE: normalize the known proportions to sum to one,\n$$\n\\mathbf{p}^{\\dagger} = \\frac{\\mathbf{p}^{\\text{known}}_3}{0.9} = \\begin{bmatrix} 0.5555555556 \\\\ 0.2777777778 \\\\ 0.1666666667 \\end{bmatrix}.\n$$\n\nCase $4$ (measurement noise):\n- Use $X_4 = X_1$.\n- True proportions\n$$\n\\mathbf{p}^{\\star}_4 = \\begin{bmatrix} 0.3 \\\\ 0.5 \\\\ 0.2 \\end{bmatrix}.\n$$\n- Noiseless bulk\n$$\nX_4 \\mathbf{p}^{\\star}_4 = \\begin{bmatrix} 3.6 \\\\ 2.35 \\\\ 3.6 \\\\ 2.1 \\\\ 3.55 \\end{bmatrix}.\n$$\n- Additive noise vector\n$$\n\\boldsymbol{\\eta} = \\begin{bmatrix} 0.05 \\\\ -0.02 \\\\ 0.03 \\\\ -0.04 \\\\ 0.01 \\end{bmatrix}.\n$$\n- Bulk vector\n$$\n\\mathbf{y}_4 = X_4 \\mathbf{p}^{\\star}_4 + \\boldsymbol{\\eta} = \\begin{bmatrix} 3.65 \\\\ 2.33 \\\\ 3.63 \\\\ 2.06 \\\\ 3.56 \\end{bmatrix}.\n$$\n- Ground-truth for RMSE: $\\mathbf{p}^{\\dagger} = \\mathbf{p}^{\\star}_4$.\n\nCase $5$ (library-size scaling mismatch):\n- Use $X_5 = X_1$.\n- True proportions\n$$\n\\mathbf{p}^{\\star}_5 = \\mathbf{p}^{\\star}_1 = \\begin{bmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{bmatrix}.\n$$\n- Bulk vector is a scaled version of Case $1$,\n$$\n\\mathbf{y}_5 = 2 \\cdot \\mathbf{y}_1 = \\begin{bmatrix} 9.6 \\\\ 5.7 \\\\ 7.2 \\\\ 5.8 \\\\ 8.9 \\end{bmatrix}.\n$$\n- Ground-truth for RMSE: $\\mathbf{p}^{\\dagger} = \\mathbf{p}^{\\star}_5$.\n\nCase $6$ (integration with a clinical severity phenotype):\n- Use $X_6 = X_1$.\n- Number of samples $T = 6$.\n- True proportions for each sample (columns indicate cell types $1,2,3$):\n$$\n\\mathbf{p}^{\\star (1)} = \\begin{bmatrix} 0.2 \\\\ 0.5 \\\\ 0.3 \\end{bmatrix},\\quad\n\\mathbf{p}^{\\star (2)} = \\begin{bmatrix} 0.1 \\\\ 0.5 \\\\ 0.4 \\end{bmatrix},\\quad\n\\mathbf{p}^{\\star (3)} = \\begin{bmatrix} 0.05 \\\\ 0.45 \\\\ 0.5 \\end{bmatrix},\\quad\n\\mathbf{p}^{\\star (4)} = \\begin{bmatrix} 0.3 \\\\ 0.6 \\\\ 0.1 \\end{bmatrix},\\quad\n\\mathbf{p}^{\\star (5)} = \\begin{bmatrix} 0.05 \\\\ 0.35 \\\\ 0.6 \\end{bmatrix},\\quad\n\\mathbf{p}^{\\star (6)} = \\begin{bmatrix} 0.15 \\\\ 0.55 \\\\ 0.3 \\end{bmatrix}.\n$$\n- Clinical severity scores vector\n$$\n\\mathbf{s} = \\begin{bmatrix} 0.3 \\\\ 0.5 \\\\ 0.7 \\\\ 0.2 \\\\ 0.9 \\\\ 0.4 \\end{bmatrix}.\n$$\n- For each sample $t \\in \\{1,\\dots,6\\}$, bulk vector is\n$$\n\\mathbf{y}_6^{(t)} = X_6 \\mathbf{p}^{\\star (t)} + \\boldsymbol{\\delta},\n$$\nwith a small fixed additive noise\n$$\n\\boldsymbol{\\delta} = \\begin{bmatrix} 0.01 \\\\ -0.01 \\\\ 0.0 \\\\ 0.02 \\\\ -0.02 \\end{bmatrix}.\n$$\n- Estimate $\\widehat{\\mathbf{p}}^{(t)}$ for each sample and compute the Pearson correlation coefficient $r$ between the estimated fractions of cell type $3$ (the third component of $\\widehat{\\mathbf{p}}^{(t)}$) and the vector $\\mathbf{s}$. Report $r$ as a decimal.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order: $\\mathrm{RMSE}$ for Case $1$, $\\mathrm{RMSE}$ for Case $2$, $\\mathrm{RMSE}$ for Case $3$, $\\mathrm{RMSE}$ for Case $4$, $\\mathrm{RMSE}$ for Case $5$, and the Pearson correlation $r$ for Case $6$. Each value must be rounded to $6$ decimal places. For example, the output should look like\n$$\n[\\text{value}_1,\\text{value}_2,\\text{value}_3,\\text{value}_4,\\text{value}_5,\\text{value}_6].\n$$\nAll proportions are to be interpreted as decimals (fractions between $0$ and $1$), not percentages.", "solution": "The starting point is the linear mixing model for bulk gene expression. Since transcript counts from different cell types add, a bulk measurement for gene $g$ can be expressed as\n$$\ny_g = \\sum_{c=1}^{C} X_{gc} p_c + \\varepsilon_g,\n$$\nwhere $X_{gc}$ is the mean expression of gene $g$ in cell type $c$, $p_c$ is the fraction of cell type $c$ in the sample, and $\\varepsilon_g$ captures measurement noise and model mismatch. As these quantities are counts or fractions, we have $X_{gc} \\geq 0$, $p_c \\geq 0$, and under the fraction interpretation $\\sum_{c=1}^{C} p_c = 1$. The model across all genes is succinctly written as\n$$\n\\mathbf{y} = X \\mathbf{p} + \\boldsymbol{\\varepsilon}.\n$$\n\nTo estimate $\\mathbf{p}$, we convert this into a convex optimization problem. Without noise, the least squares estimator solves\n$$\n\\min_{\\mathbf{p}} \\left\\| X \\mathbf{p} - \\mathbf{y} \\right\\|_2^2.\n$$\nBiological constraints imply nonnegativity of $\\mathbf{p}$, yielding the nonnegative least squares (NNLS) problem:\n$$\n\\min_{\\mathbf{p} \\in \\mathbb{R}^{C}} \\left\\| X \\mathbf{p} - \\mathbf{y} \\right\\|_2 \\quad \\text{subject to} \\quad p_c \\geq 0 \\text{ for all } c.\n$$\nThis is a convex problem with a unique minimizer when $X$ has full column rank and stable behavior under near-collinearity due to the nonnegativity and the quadratic objective. The Karush–Kuhn–Tucker (KKT) conditions ensure that at the optimum, either $\\widehat{p}_c = 0$ or the corresponding gradient condition is satisfied for active variables. Computationally, we solve NNLS via an active-set method (as implemented in standard libraries), which iteratively identifies a subset of variables constrained at zero and solves unconstrained least squares on the remainder until optimality.\n\nThe NNLS solution $\\widehat{\\mathbf{p}}$ does not enforce the sum-to-one constraint. However, when interpreting $\\widehat{\\mathbf{p}}$ as fractions, renormalization is straightforward:\n$$\n\\widehat{\\mathbf{p}} \\leftarrow \\frac{\\widehat{\\mathbf{p}}}{\\sum_{c=1}^{C} \\widehat{p}_c} \\quad \\text{if} \\quad \\sum_{c=1}^{C} \\widehat{p}_c > 0,\n$$\nensuring $\\sum_{c=1}^{C} \\widehat{p}_c = 1$. If $\\sum_{c=1}^{C} \\widehat{p}_c = 0$ due to degeneracy (all-zero estimate), a reasonable fallback is the uniform distribution\n$$\n\\widehat{p}_c = \\frac{1}{C} \\quad \\text{for all } c.\n$$\n\nAccuracy is quantified by the root mean squared error (RMSE) between $\\widehat{\\mathbf{p}}$ and the ground-truth $\\mathbf{p}^{\\dagger}$:\n$$\n\\mathrm{RMSE}(\\widehat{\\mathbf{p}}, \\mathbf{p}^{\\dagger}) = \\sqrt{\\frac{1}{C} \\sum_{c=1}^{C} \\left(\\widehat{p}_c - p^{\\dagger}_c\\right)^2}.\n$$\nIn Case $1$, the reference is well-conditioned and the mixture is noiseless, so NNLS should recover $\\mathbf{p}^{\\star}_1$ with negligible error after renormalization. In Case $2$, columns of $X_2$ are nearly collinear; NNLS remains applicable but the solution may be less precise due to ill-conditioning, yet renormalization will still enforce interpretability. In Case $3$, the bulk contains an unmodeled cell type. The correct evaluation compares $\\widehat{\\mathbf{p}}$ to the normalized known proportions $\\mathbf{p}^{\\dagger} = \\mathbf{p}^{\\text{known}}_3 / 0.9$ because the unknown fraction is not represented in $X$. In Case $4$, additive noise perturbs $\\mathbf{y}$, and NNLS trades off residual error to fit noisy observations, resulting in a small but nonzero RMSE. In Case $5$, a global scaling of $\\mathbf{y}$ corresponds to library-size differences; NNLS followed by renormalization is invariant to such scaling, thus RMSE should be near zero. \n\nFor integration with a clinical phenotype (Case $6$), we estimate $\\widehat{\\mathbf{p}}^{(t)}$ for each sample $t$ and focus on the fraction of a specific cell type, here cell type $3$. The Pearson correlation coefficient between the estimated fractions $\\{\\widehat{p}_3^{(t)}\\}_{t=1}^{T}$ and severity scores $\\{s^{(t)}\\}_{t=1}^{T}$ is\n$$\nr = \\frac{\\sum_{t=1}^{T} \\left(\\widehat{p}_{3}^{(t)} - \\overline{\\widehat{p}_{3}}\\right)\\left(s^{(t)} - \\overline{s}\\right)}{\\sqrt{\\sum_{t=1}^{T}\\left(\\widehat{p}_{3}^{(t)} - \\overline{\\widehat{p}_{3}}\\right)^2}\\sqrt{\\sum_{t=1}^{T}\\left(s^{(t)} - \\overline{s}\\right)^2}},\n$$\nwhich measures a linear association between estimated cell-type $3$ abundance and clinical severity, a standard approach for integrating omics-derived features with clinical phenotypes.\n\nAlgorithmic steps for each test case:\n- Construct $X$ and $\\mathbf{y}$ exactly as specified.\n- Solve NNLS to obtain $\\widehat{\\mathbf{p}}$.\n- Renormalize $\\widehat{\\mathbf{p}}$ to unit sum if possible, else set it to uniform fractions.\n- Compute $\\mathrm{RMSE}$ against the appropriate $\\mathbf{p}^{\\dagger}$ for Cases $1$–$5$ and Pearson correlation $r$ for Case $6$.\n\nFinally, aggregate the six results into one line formatted as $[\\text{value}_1,\\dots,\\text{value}_6]$, with each value rounded to $6$ decimal places and expressed in decimal form (no percentage signs). This provides a compact summary of deconvolution accuracy across varying conditions and its relevance to clinical phenotype integration.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import nnls\n\ndef nnls_deconvolution(X, y):\n    \"\"\"\n    Solve nonnegative least squares for p in y ≈ X p.\n    Returns p_hat renormalized to sum to 1 (if possible).\n    \"\"\"\n    p_hat, _ = nnls(X, y)\n    s = p_hat.sum()\n    if s > 0:\n        p_hat = p_hat / s\n    else:\n        # Fallback to uniform distribution if degenerate (sum == 0)\n        C = X.shape[1]\n        p_hat = np.full(C, 1.0 / C)\n    return p_hat\n\ndef rmse(p_hat, p_true):\n    \"\"\"\n    Root Mean Squared Error between estimated and true proportions.\n    \"\"\"\n    diff = p_hat - p_true\n    return np.sqrt(np.mean(diff * diff))\n\ndef pearson_correlation(x, y):\n    \"\"\"\n    Pearson correlation coefficient between two 1D arrays.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    x_centered = x - x_mean\n    y_centered = y - y_mean\n    denom = np.sqrt(np.sum(x_centered**2) * np.sum(y_centered**2))\n    if denom == 0:\n        return 0.0\n    return float(np.sum(x_centered * y_centered) / denom)\n\ndef solve():\n    results = []\n\n    # Case 1\n    X1 = np.array([\n        [8.0, 2.0, 1.0],\n        [4.0, 1.5, 2.0],\n        [3.0, 3.0, 6.0],\n        [5.0, 1.0, 0.5],\n        [7.0, 2.5, 1.0]\n    ], dtype=float)\n    p1_true = np.array([0.5, 0.3, 0.2], dtype=float)\n    y1 = np.array([4.8, 2.85, 3.6, 2.9, 4.45], dtype=float)\n    p1_hat = nnls_deconvolution(X1, y1)\n    results.append(rmse(p1_hat, p1_true))\n\n    # Case 2\n    X2 = np.array([\n        [6.0, 6.2, 1.0],\n        [3.0, 3.1, 2.0],\n        [5.0, 5.1, 6.0],\n        [4.0, 4.05, 0.5],\n        [2.0, 2.02, 1.0]\n    ], dtype=float)\n    p2_true = np.array([0.4, 0.4, 0.2], dtype=float)\n    y2 = np.array([5.08, 2.84, 5.24, 3.32, 1.808], dtype=float)\n    p2_hat = nnls_deconvolution(X2, y2)\n    results.append(rmse(p2_hat, p2_true))\n\n    # Case 3 (unmodeled cell type)\n    X3 = X1.copy()\n    p3_known = np.array([0.5, 0.25, 0.15], dtype=float)  # sums to 0.9\n    u = np.array([4.0, 1.0, 3.0, 2.0, 5.0], dtype=float)\n    y3 = X3 @ p3_known + 0.1 * u\n    p3_true_norm = p3_known / 0.9\n    p3_hat = nnls_deconvolution(X3, y3)\n    results.append(rmse(p3_hat, p3_true_norm))\n\n    # Case 4 (measurement noise)\n    X4 = X1.copy()\n    p4_true = np.array([0.3, 0.5, 0.2], dtype=float)\n    y4_noiseless = X4 @ p4_true\n    eta = np.array([0.05, -0.02, 0.03, -0.04, 0.01], dtype=float)\n    y4 = y4_noiseless + eta\n    p4_hat = nnls_deconvolution(X4, y4)\n    results.append(rmse(p4_hat, p4_true))\n\n    # Case 5 (scaling mismatch)\n    X5 = X1.copy()\n    p5_true = p1_true.copy()\n    y5 = 2.0 * y1\n    p5_hat = nnls_deconvolution(X5, y5)\n    results.append(rmse(p5_hat, p5_true))\n\n    # Case 6 (clinical phenotype correlation)\n    X6 = X1.copy()\n    p6_list = [\n        np.array([0.2, 0.5, 0.3], dtype=float),\n        np.array([0.1, 0.5, 0.4], dtype=float),\n        np.array([0.05, 0.45, 0.5], dtype=float),\n        np.array([0.3, 0.6, 0.1], dtype=float),\n        np.array([0.05, 0.35, 0.6], dtype=float),\n        np.array([0.15, 0.55, 0.3], dtype=float),\n    ]\n    delta = np.array([0.01, -0.01, 0.0, 0.02, -0.02], dtype=float)\n    s = np.array([0.3, 0.5, 0.7, 0.2, 0.9, 0.4], dtype=float)\n    est_cell3 = []\n    for p_true in p6_list:\n        y6 = X6 @ p_true + delta\n        p_hat6 = nnls_deconvolution(X6, y6)\n        est_cell3.append(p_hat6[2])\n    r = pearson_correlation(np.array(est_cell3, dtype=float), s)\n    results.append(r)\n\n    # Round each result to 6 decimal places and print in specified format\n    rounded = [f\"{x:.6f}\" for x in results]\n    print(f\"[{','.join(rounded)}]\")\n\nsolve()\n```", "id": "4574619"}, {"introduction": "Real-world clinical and omics datasets are rarely perfect, and handling missing data is a critical skill for any data analyst. This final practice moves into the advanced topic of analytical robustness, focusing on the subtle but significant biases that can arise from missing data. You will perform a theoretical analysis to quantify the asymptotic bias in a logistic regression model when data are \"Missing Not At Random\" (MNAR) and a naive complete-case analysis is performed [@problem_id:4574671]. This exercise develops the essential ability to critically assess the statistical assumptions underlying an analysis, a hallmark of sophisticated and reliable scientific inquiry.", "problem": "A proteomics study integrates a continuous proteomic intensity predictor $X$ (e.g., log-transformed peptide abundance) with a binary clinical phenotype $Y \\in \\{0,1\\}$ (e.g., disease present versus absent) using logistic regression. Let the disease model in the underlying target population be $P(Y=1 \\mid X=x) = \\operatorname{expit}(\\beta_{0} + \\beta_{1} x)$, where $\\operatorname{expit}(t) = 1/(1+\\exp(-t))$. Suppose the proteomics measurement $X$ is subject to missingness captured by an indicator $R \\in \\{0,1\\}$, where $R=1$ if $X$ is observed and $R=0$ otherwise. Consider a Missing Not At Random (MNAR) mechanism $P(R=1 \\mid X=x, Y=y) = \\operatorname{expit}(\\alpha + \\gamma x + \\delta y)$ that depends on both the unobserved value $x$ and the outcome $y$.\n\nA common practice is complete-case analysis, which fits a logistic regression of $Y$ on $X$ using only observations with $R=1$. Under the above data-generating process, analyze the asymptotic bias in the slope coefficient induced by MNAR when performing complete-case analysis. Use the following scientifically plausible assumptions and parameter values:\n- The disease mechanism is $P(Y=1 \\mid X=x) = \\operatorname{expit}(\\beta_{0} + \\beta_{1} x)$, with $\\beta_{1} = 0.8$.\n- The missingness mechanism is $P(R=1 \\mid X=x, Y=y) = \\operatorname{expit}(\\alpha + \\gamma x + \\delta y)$, with $\\alpha = -1$, $\\gamma = 0.5$, and $\\delta = -0.4$.\n- Assume $\\gamma$ and $\\delta$ are of small-to-moderate magnitude such that first-order (linear) approximations in $\\gamma$ and $\\delta$ are appropriate; neglect all second-order and higher-order terms (including products like $\\gamma^{2}$, $\\delta^{2}$, and $\\gamma \\delta$ beyond first-order contributions).\n\nStarting from fundamental definitions of conditional probabilities and the logistic link, derive the first-order approximation for the conditional log-odds of $Y$ given $X$ among complete cases $R=1$, and from it deduce the first-order bias in the estimated slope coefficient $\\tilde{\\beta}_{1}$ from the complete-case analysis relative to the true $\\beta_{1}$, defined as $\\tilde{\\beta}_{1} - \\beta_{1}$. Using the provided parameters, compute the numerical value of this first-order bias. Round your final answer to four significant figures. Express the final answer as a unitless real number.", "solution": "The problem asks for the asymptotic bias in the slope coefficient of a logistic regression estimated from complete cases under a specific Missing Not At Random (MNAR) mechanism. Let's denote the true parameters by $\\beta_0, \\beta_1$ and the parameters estimated from the complete-case analysis by $\\tilde{\\beta}_0, \\tilde{\\beta}_1$.\n\nThe complete-case analysis fits a logistic regression model to the sub-population for which the predictor $X$ is observed, i.e., where $R=1$. The model being estimated is therefore based on the conditional probability $P(Y=1 \\mid X=x, R=1)$. The logistic regression will estimate the parameters of the linear model for the log-odds (logit) of this probability. Let this be $\\operatorname{logit}_{CC}(x)$:\n$$ \\operatorname{logit}_{CC}(x) = \\ln\\left(\\frac{P(Y=1 \\mid X=x, R=1)}{P(Y=0 \\mid X=x, R=1)}\\right) $$\nUsing Bayes' theorem, we can express the conditional probability $P(Y=y \\mid X=x, R=1)$ as:\n$$ P(Y=y \\mid X=x, R=1) = \\frac{P(R=1 \\mid X=x, Y=y) P(Y=y \\mid X=x)}{P(R=1 \\mid X=x)} $$\nThe odds for the complete cases are then:\n$$ \\frac{P(Y=1 \\mid X=x, R=1)}{P(Y=0 \\mid X=x, R=1)} = \\frac{P(R=1 \\mid X=x, Y=1) P(Y=1 \\mid X=x)}{P(R=1 \\mid X=x, Y=0) P(Y=0 \\mid X=x)} $$\nTaking the natural logarithm of both sides gives the log-odds for the complete cases:\n$$ \\operatorname{logit}_{CC}(x) = \\ln\\left(\\frac{P(Y=1 \\mid X=x)}{P(Y=0 \\mid X=x)}\\right) + \\ln\\left(\\frac{P(R=1 \\mid X=x, Y=1)}{P(R=1 \\mid X=x, Y=0)}\\right) $$\nThe first term is the true log-odds, $\\operatorname{logit}(P(Y=1 \\mid X=x)) = \\beta_0 + \\beta_1 x$. The second term is the bias induced by the selection process (i.e., by conditioning on $R=1$). Let's call this entire function $L(x; \\gamma, \\delta)$.\nSubstituting the given probabilistic models:\n$P(Y=1 \\mid X=x) = \\operatorname{expit}(\\beta_0 + \\beta_1 x)$\n$P(R=1 \\mid X=x, Y=y) = \\operatorname{expit}(\\alpha + \\gamma x + \\delta y)$\nThe expression for the complete-case log-odds becomes:\n$$ L(x; \\gamma, \\delta) = (\\beta_0 + \\beta_1 x) + \\ln\\left(\\frac{\\operatorname{expit}(\\alpha + \\gamma x + \\delta \\cdot 1)}{\\operatorname{expit}(\\alpha + \\gamma x + \\delta \\cdot 0)}\\right) $$\n$$ L(x; \\gamma, \\delta) = (\\beta_0 + \\beta_1 x) + \\ln(\\operatorname{expit}(\\alpha + \\gamma x + \\delta)) - \\ln(\\operatorname{expit}(\\alpha + \\gamma x)) $$\nThis can be rewritten using the identity $\\ln(\\operatorname{expit}(t)) = -\\ln(1+\\exp(-t))$:\n$$ L(x; \\gamma, \\delta) = (\\beta_0 + \\beta_1 x) - \\ln(1+\\exp(-(\\alpha + \\gamma x + \\delta))) + \\ln(1+\\exp(-(\\alpha + \\gamma x))) $$\nThe complete-case logistic regression finds the best linear approximation of $L(x; \\gamma, \\delta)$. To determine the asymptotic bias for small $\\gamma$ and $\\delta$, we perform a Taylor series expansion of $L(x; \\gamma, \\delta)$ around $(\\gamma, \\delta) = (0, 0)$. The bias in the slope estimate $\\tilde{\\beta}_1$ arises from terms in this expansion that are linear in $x$.\n\nThe bias in the slope must be zero if either $\\gamma=0$ (missingness does not depend on $X$) or if $\\delta=0$ (missingness does not depend on $Y$, i.e., MAR). This implies that the leading term for the slope bias must involve the product $\\gamma\\delta$. Therefore, we need to expand $L(x; \\gamma, \\delta)$ up to the second order, including the mixed partial derivative term.\n$$ L(x; \\gamma, \\delta) \\approx L(x; 0, 0) + \\gamma \\frac{\\partial L}{\\partial \\gamma}\\bigg|_{(0,0)} + \\delta \\frac{\\partial L}{\\partial \\delta}\\bigg|_{(0,0)} + \\gamma\\delta \\frac{\\partial^2 L}{\\partial \\gamma \\partial \\delta}\\bigg|_{(0,0)} + \\dots $$\nLet's compute the partial derivatives.\n$L(x; 0, 0) = (\\beta_0 + \\beta_1 x) - \\ln(1+\\exp(-\\alpha)) + \\ln(1+\\exp(-\\alpha)) = \\beta_0 + \\beta_1 x$.\n\nFirst partial derivative with respect to $\\gamma$:\n$$ \\frac{\\partial L}{\\partial \\gamma} = - \\frac{ -x \\exp(-(\\alpha+\\gamma x+\\delta)) }{1+\\exp(-(\\alpha+\\gamma x+\\delta))} + \\frac{ -x \\exp(-(\\alpha+\\gamma x)) }{1+\\exp(-(\\alpha+\\gamma x))} $$\n$$ \\frac{\\partial L}{\\partial \\gamma} = x(1-\\operatorname{expit}(\\alpha+\\gamma x+\\delta)) - x(1-\\operatorname{expit}(\\alpha+\\gamma x)) = x(\\operatorname{expit}(\\alpha+\\gamma x)-\\operatorname{expit}(\\alpha+\\gamma x+\\delta)) $$\nEvaluating at $(\\gamma, \\delta) = (0, 0)$:\n$$ \\frac{\\partial L}{\\partial \\gamma}\\bigg|_{(0,0)} = x(\\operatorname{expit}(\\alpha) - \\operatorname{expit}(\\alpha)) = 0 $$\nFirst partial derivative with respect to $\\delta$:\n$$ \\frac{\\partial L}{\\partial \\delta} = - \\frac{ -\\exp(-(\\alpha+\\gamma x+\\delta)) }{1+\\exp(-(\\alpha+\\gamma x+\\delta))} = 1 - \\operatorname{expit}(\\alpha+\\gamma x+\\delta) $$\nEvaluating at $(\\gamma, \\delta) = (0, 0)$:\n$$ \\frac{\\partial L}{\\partial \\delta}\\bigg|_{(0,0)} = 1 - \\operatorname{expit}(\\alpha) $$\nThis term is constant with respect to $x$, so it only contributes to the bias in the intercept.\n\nMixed second partial derivative:\n$$ \\frac{\\partial^2 L}{\\partial \\delta \\partial \\gamma} = \\frac{\\partial}{\\partial \\delta} \\left[ x(\\operatorname{expit}(\\alpha+\\gamma x)-\\operatorname{expit}(\\alpha+\\gamma x+\\delta)) \\right] = -x \\frac{\\partial}{\\partial \\delta} \\operatorname{expit}(\\alpha+\\gamma x+\\delta) $$\nUsing $\\frac{d}{dt}\\operatorname{expit}(t) = \\operatorname{expit}(t)(1-\\operatorname{expit}(t))$:\n$$ \\frac{\\partial^2 L}{\\partial \\delta \\partial \\gamma} = -x \\cdot \\operatorname{expit}(\\alpha+\\gamma x+\\delta)(1-\\operatorname{expit}(\\alpha+\\gamma x+\\delta)) $$\nEvaluating at $(\\gamma, \\delta) = (0, 0)$:\n$$ \\frac{\\partial^2 L}{\\partial \\delta \\partial \\gamma}\\bigg|_{(0,0)} = -x \\cdot \\operatorname{expit}(\\alpha)(1-\\operatorname{expit}(\\alpha)) $$\nSubstituting these into the Taylor expansion:\n$$ L(x; \\gamma, \\delta) \\approx (\\beta_0 + \\beta_1 x) + \\delta(1-\\operatorname{expit}(\\alpha)) - \\gamma\\delta x \\operatorname{expit}(\\alpha)(1-\\operatorname{expit}(\\alpha)) $$\nWe can rearrange this expression to group terms by powers of $x$:\n$$ L(x; \\gamma, \\delta) \\approx [\\beta_0 + \\delta(1-\\operatorname{expit}(\\alpha))] + [\\beta_1 - \\gamma\\delta \\operatorname{expit}(\\alpha)(1-\\operatorname{expit}(\\alpha))] x $$\nThis shows that, to this order of approximation, the log-odds for the complete cases is linear in $x$. The complete-case logistic regression will consistently estimate the intercept and slope of this linear function. The estimated slope, $\\tilde{\\beta}_1$, is the coefficient of $x$:\n$$ \\tilde{\\beta}_1 \\approx \\beta_1 - \\gamma\\delta \\operatorname{expit}(\\alpha)(1-\\operatorname{expit}(\\alpha)) $$\nThe first-order bias in the slope is therefore:\n$$ \\text{Bias} = \\tilde{\\beta}_1 - \\beta_1 \\approx -\\gamma\\delta \\operatorname{expit}(\\alpha)(1-\\operatorname{expit}(\\alpha)) $$\nNow, we substitute the given parameter values: $\\alpha = -1$, $\\gamma = 0.5$, and $\\delta = -0.4$.\n$$ \\text{Bias} \\approx -(0.5)(-0.4) \\operatorname{expit}(-1)(1-\\operatorname{expit}(-1)) $$\n$$ \\text{Bias} \\approx 0.2 \\cdot \\operatorname{expit}(-1)(1-\\operatorname{expit}(-1)) $$\nWe have $\\operatorname{expit}(-1) = \\frac{1}{1+\\exp(1)}$ and $1-\\operatorname{expit}(-1) = 1-\\frac{1}{1+\\exp(1)} = \\frac{\\exp(1)}{1+\\exp(1)}$.\n$$ \\text{Bias} \\approx 0.2 \\cdot \\frac{1}{1+\\exp(1)} \\cdot \\frac{\\exp(1)}{1+\\exp(1)} = \\frac{0.2\\exp(1)}{(1+\\exp(1))^2} $$\nUsing the value $\\exp(1) \\approx 2.71828$:\n$$ \\operatorname{expit}(-1) \\approx \\frac{1}{1+2.71828} = \\frac{1}{3.71828} \\approx 0.26894 $$\n$$ 1-\\operatorname{expit}(-1) \\approx 1 - 0.26894 = 0.73106 $$\n$$ \\text{Bias} \\approx 0.2 \\cdot (0.26894) \\cdot (0.73106) \\approx 0.2 \\cdot 0.19661 \\approx 0.039322 $$\nRounding to four significant figures, the bias is $0.03932$.", "answer": "$$\\boxed{0.03932}$$", "id": "4574671"}]}