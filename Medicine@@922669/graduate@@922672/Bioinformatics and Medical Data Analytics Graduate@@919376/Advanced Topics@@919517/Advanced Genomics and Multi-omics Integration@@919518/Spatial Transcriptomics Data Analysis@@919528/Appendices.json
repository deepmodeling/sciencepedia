{"hands_on_practices": [{"introduction": "Raw gene counts from spatial transcriptomics experiments are directly influenced by technical factors like sequencing depth and capture efficiency, which can vary significantly between spots. To perform meaningful biological comparisons, these technical artifacts must be corrected through normalization. This exercise [@problem_id:4608974] provides hands-on practice with size-factor normalization and log-transformation, two foundational techniques that are essential for preparing data for virtually any downstream spatial analysis.", "problem": "In spatial transcriptomics, a tissue section is partitioned into spatial spots, each capturing messenger ribonucleic acid (mRNA) molecules from nearby cells. The resulting data are summarized in a nonnegative integer matrix $X \\in \\mathbb{N}_0^{n \\times p}$, where $n$ is the number of spots and $p$ is the number of genes. Let $x_{ij}$ denote the raw count of gene $j$ in spot $i$. Because each spot may have a different sequencing depth or capture efficiency, one uses a positive per-spot scaling factor $s_i \\in \\mathbb{R}_{>0}$ to account for these differences. A commonly used transformation for downstream analysis is to produce size-factor-normalized counts and then apply a log transform with a pseudo-count and a user-specified scaling parameter $\\alpha \\in \\mathbb{R}_{\\ge 0}$. The natural logarithm is used.\n\nFundamental base and modeling assumptions:\n- Raw counts are nonnegative integers: for all $i \\in \\{1,\\dots,n\\}$ and $j \\in \\{1,\\dots,p\\}$, $x_{ij} \\in \\mathbb{N}_0$.\n- Size factors are strictly positive real numbers: for all $i \\in \\{1,\\dots,n\\}$, $s_i \\in \\mathbb{R}_{>0}$.\n- The log transform uses the natural logarithm with a pseudo-count of $1$ to ensure finite values when counts are zero; the user provides a nonnegative scaling $\\alpha \\in \\mathbb{R}_{\\ge 0}$.\n- The purpose of size-factor normalization is to remove multiplicative spot-specific sampling depth effects while preserving relative gene expression within a spot.\n- The log transform is applied after normalization to compress dynamic range and stabilize variance while preserving order through the monotonicity of the logarithm.\n\nYour task is to write a program that, for several specified test cases, performs the following for each case:\n1. Given $X$, $s$, and $\\alpha$, compute the size-factor-normalized counts, where for each spot $i$ and gene $j$, the normalized value is the ratio of the raw count for that entry to the size factor of that spot.\n2. Compute the log-normalized values by applying the natural logarithm to one plus $\\alpha$ times the normalized counts for each entry.\n3. Round both the normalized counts and the log-normalized values to $6$ decimal places and report them in row-major order as one-dimensional lists.\n\nUse the following test suite. Each test case provides $X$, $s$, and $\\alpha$:\n\n- Test case $1$ (general case, moderate values):\n  - $X = \\begin{bmatrix} 10 & 0 & 5 \\\\ 4 & 6 & 0 \\end{bmatrix}$, so $n = 2$, $p = 3$.\n  - $s = [10, 10]$.\n  - $\\alpha = 1$.\n\n- Test case $2$ (size factors equal to row sums, checks normalization of totals):\n  - $X = \\begin{bmatrix} 3 & 7 \\\\ 0 & 5 \\\\ 2 & 3 \\end{bmatrix}$, so $n = 3$, $p = 2$.\n  - $s = [10, 5, 5]$.\n  - $\\alpha = 2$.\n\n- Test case $3$ (zero $\\alpha$ producing zero log-normalized values):\n  - $X = \\begin{bmatrix} 0 & 1 & 2 \\\\ 3 & 0 & 4 \\end{bmatrix}$, so $n = 2$, $p = 3$.\n  - $s = [1, 2]$.\n  - $\\alpha = 0$.\n\n- Test case $4$ (extreme scaling, very small size factor and large counts):\n  - $X = \\begin{bmatrix} 0 & 1000000 \\end{bmatrix}$, so $n = 1$, $p = 2$.\n  - $s = [10^{-6}]$.\n  - $\\alpha = 10^{-3}$.\n\nProgram specification:\n- For each test case, output a list containing two lists:\n  - The first is the size-factor-normalized counts flattened in row-major order, each entry rounded to $6$ decimal places.\n  - The second is the log-normalized values flattened in row-major order, each entry rounded to $6$ decimal places.\n- Aggregate the results for all test cases, in order, into a single list.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[result\\_1,result\\_2,\\dots]$). Each $result\\_k$ must be a list of two lists of real numbers as described above.\n\nAll quantities in this problem are dimensionless; thus, no physical units are required. Angles do not appear. Percentages are not used. The natural logarithm must be used for all logarithmic computations.", "solution": "The problem statement has been rigorously validated and is determined to be valid. It is scientifically grounded in the field of bioinformatics, specifically spatial transcriptomics, and is mathematically well-posed. All data, definitions, and constraints are self-contained and consistent, allowing for the computation of a unique, verifiable solution.\n\nThe task is to perform two standard data transformations on raw gene count matrices: size-factor normalization and a subsequent log-transformation. The process for each test case is defined by a set of precise mathematical operations.\n\nLet the raw count matrix be denoted by $X \\in \\mathbb{N}_0^{n \\times p}$, where $x_{ij}$ is the count for spot $i$ and gene $j$. Let $s = [s_1, s_2, \\dots, s_n]$ be the vector of positive per-spot size factors, where $s_i \\in \\mathbb{R}_{>0}$. Let $\\alpha \\in \\mathbb{R}_{\\ge 0}$ be a user-specified scaling parameter.\n\n**Step 1: Size-Factor Normalization**\nThe purpose of size-factor normalization is to adjust for variations in library size or sequencing depth across different spatial spots. This is achieved by dividing the raw count of each gene in a spot by the size factor corresponding to that spot.\n\nLet $N$ be the matrix of size-factor-normalized counts. An element $n_{ij}$ of this matrix is calculated as:\n$$\nn_{ij} = \\frac{x_{ij}}{s_i}\n$$\nThis operation is applied to all entries in the matrix $X$. The resulting matrix $N$ has the same dimensions as $X$. Since $x_{ij} \\ge 0$ and $s_i > 0$, it follows that $n_{ij} \\ge 0$.\n\n**Step 2: Log-Transformation**\nAfter normalization, a log-transformation is applied to compress the range of the data and stabilize the variance, which is often dependent on the mean in count data. The transformation includes a pseudo-count of $1$ to prevent taking the logarithm of zero and a scaling parameter $\\alpha$.\n\nLet $L$ be the matrix of log-normalized values. An element $l_{ij}$ of this matrix is calculated using the natural logarithm ($\\ln$):\n$$\nl_{ij} = \\ln(1 + \\alpha \\cdot n_{ij})\n$$\nSince $\\alpha \\ge 0$ and $n_{ij} \\ge 0$, the argument of the logarithm, $1 + \\alpha \\cdot n_{ij}$, is always greater than or equal to $1$. Consequently, the log-normalized value $l_{ij}$ is always well-defined and non-negative ($l_{ij} \\ge 0$).\n\nThe procedure is now applied to each provided test case.\n\n**Test Case 1:**\n- Given: $X = \\begin{bmatrix} 10 & 0 & 5 \\\\ 4 & 6 & 0 \\end{bmatrix}$, $s = [10, 10]$, $\\alpha = 1$.\n- Normalized counts $N$:\n$$\nN = \\begin{bmatrix} 10/10 & 0/10 & 5/10 \\\\ 4/10 & 6/10 & 0/10 \\end{bmatrix} = \\begin{bmatrix} 1.0 & 0.0 & 0.5 \\\\ 0.4 & 0.6 & 0.0 \\end{bmatrix}\n$$\n- Log-normalized values $L$:\n$$\nL = \\begin{bmatrix} \\ln(1 + 1 \\cdot 1.0) & \\ln(1 + 1 \\cdot 0.0) & \\ln(1 + 1 \\cdot 0.5) \\\\ \\ln(1 + 1 \\cdot 0.4) & \\ln(1 + 1 \\cdot 0.6) & \\ln(1 + 1 \\cdot 0.0) \\end{bmatrix} = \\begin{bmatrix} \\ln(2.0) & \\ln(1.0) & \\ln(1.5) \\\\ \\ln(1.4) & \\ln(1.6) & \\ln(1.0) \\end{bmatrix}\n$$\n- $L \\approx \\begin{bmatrix} 0.693147 & 0.0 & 0.405465 \\\\ 0.336472 & 0.470004 & 0.0 \\end{bmatrix}$\n- Flattened and rounded results:\n  - $N_{flat} \\approx [1.000000, 0.000000, 0.500000, 0.400000, 0.600000, 0.000000]$\n  - $L_{flat} \\approx [0.693147, 0.000000, 0.405465, 0.336472, 0.470004, 0.000000]$\n\n**Test Case 2:**\n- Given: $X = \\begin{bmatrix} 3 & 7 \\\\ 0 & 5 \\\\ 2 & 3 \\end{bmatrix}$, $s = [10, 5, 5]$, $\\alpha = 2$.\n- Normalized counts $N$:\n$$\nN = \\begin{bmatrix} 3/10 & 7/10 \\\\ 0/5 & 5/5 \\\\ 2/5 & 3/5 \\end{bmatrix} = \\begin{bmatrix} 0.3 & 0.7 \\\\ 0.0 & 1.0 \\\\ 0.4 & 0.6 \\end{bmatrix}\n$$\n- Log-normalized values $L$:\n$$\nL = \\begin{bmatrix} \\ln(1 + 2 \\cdot 0.3) & \\ln(1 + 2 \\cdot 0.7) \\\\ \\ln(1 + 2 \\cdot 0.0) & \\ln(1 + 2 \\cdot 1.0) \\\\ \\ln(1 + 2 \\cdot 0.4) & \\ln(1 + 2 \\cdot 0.6) \\end{bmatrix} = \\begin{bmatrix} \\ln(1.6) & \\ln(2.4) \\\\ \\ln(1.0) & \\ln(3.0) \\\\ \\ln(1.8) & \\ln(2.2) \\end{bmatrix}\n$$\n- $L \\approx \\begin{bmatrix} 0.470004 & 0.875469 \\\\ 0.0 & 1.098612 \\\\ 0.587787 & 0.788457 \\end{bmatrix}$\n- Flattened and rounded results:\n  - $N_{flat} \\approx [0.300000, 0.700000, 0.000000, 1.000000, 0.400000, 0.600000]$\n  - $L_{flat} \\approx [0.470004, 0.875469, 0.000000, 1.098612, 0.587787, 0.788457]$\n\n**Test Case 3:**\n- Given: $X = \\begin{bmatrix} 0 & 1 & 2 \\\\ 3 & 0 & 4 \\end{bmatrix}$, $s = [1, 2]$, $\\alpha = 0$.\n- Normalized counts $N$:\n$$\nN = \\begin{bmatrix} 0/1 & 1/1 & 2/1 \\\\ 3/2 & 0/2 & 4/2 \\end{bmatrix} = \\begin{bmatrix} 0.0 & 1.0 & 2.0 \\\\ 1.5 & 0.0 & 2.0 \\end{bmatrix}\n$$\n- Log-normalized values $L$:\n$$\nL = \\ln(1 + 0 \\cdot N) = \\ln(\\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}) = \\begin{bmatrix} 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 \\end{bmatrix}\n$$\n- Flattened and rounded results:\n  - $N_{flat} \\approx [0.000000, 1.000000, 2.000000, 1.500000, 0.000000, 2.000000]$\n  - $L_{flat} \\approx [0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000]$\n\n**Test Case 4:**\n- Given: $X = \\begin{bmatrix} 0 & 1000000 \\end{bmatrix}$, $s = [10^{-6}]$, $\\alpha = 10^{-3}$.\n- Normalized counts $N$:\n$$\nN = \\begin{bmatrix} 0/10^{-6} & 10^6/10^{-6} \\end{bmatrix} = \\begin{bmatrix} 0.0 & 10^{12} \\end{bmatrix}\n$$\n- Log-normalized values $L$:\n$$\nL = \\begin{bmatrix} \\ln(1 + 10^{-3} \\cdot 0.0) & \\ln(1 + 10^{-3} \\cdot 10^{12}) \\end{bmatrix} = \\begin{bmatrix} \\ln(1.0) & \\ln(1 + 10^9) \\end{bmatrix}\n$$\n- $L \\approx \\begin{bmatrix} 0.0 & 20.723266 \\end{bmatrix}$ (Note that for large $z$, $\\ln(1+z) \\approx \\ln(z)$). Here, $\\ln(1+10^9) \\approx \\ln(10^9) = 9 \\ln(10) \\approx 9 \\times 2.302585 = 20.723265$.\n- Flattened and rounded results:\n  - $N_{flat} \\approx [0.000000, 1000000000000.000000]$\n  - $L_{flat} \\approx [0.000000, 20.723266]$\n\nThe implementation will follow these calculations, using numerical libraries for efficiency and precision, and format the output as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the spatial transcriptomics normalization problem for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"X\": np.array([[10, 0, 5], [4, 6, 0]]),\n            \"s\": np.array([10, 10]),\n            \"alpha\": 1.0\n        },\n        {\n            \"X\": np.array([[3, 7], [0, 5], [2, 3]]),\n            \"s\": np.array([10, 5, 5]),\n            \"alpha\": 2.0\n        },\n        {\n            \"X\": np.array([[0, 1, 2], [3, 0, 4]]),\n            \"s\": np.array([1, 2]),\n            \"alpha\": 0.0\n        },\n        {\n            \"X\": np.array([[0, 1000000]]),\n            \"s\": np.array([1e-6]),\n            \"alpha\": 1e-3\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X = case[\"X\"]\n        s = case[\"s\"]\n        alpha = case[\"alpha\"]\n        \n        # Ensure s is a column vector for broadcasting.\n        # This divides each row of X by the corresponding element in s.\n        s_col = s.reshape(-1, 1)\n        \n        # Step 1: Compute size-factor-normalized counts.\n        # The result n_ij = x_ij / s_i.\n        normalized_counts = X / s_col\n        \n        # Step 2: Compute log-normalized values.\n        # The result l_ij = log(1 + alpha * n_ij).\n        log_normalized_values = np.log(1 + alpha * normalized_counts)\n        \n        # Step 3: Round, flatten in row-major order, and convert to lists.\n        # Rounding to 6 decimal places.\n        rounded_normalized = np.round(normalized_counts, 6)\n        rounded_log_normalized = np.round(log_normalized_values, 6)\n        \n        # Flattening (default is row-major).\n        flat_normalized = rounded_normalized.flatten().tolist()\n        flat_log_normalized = rounded_log_normalized.flatten().tolist()\n        \n        # Store the result for this test case.\n        results.append([flat_normalized, flat_log_normalized])\n\n    # Final print statement in the exact required format.\n    # The format is a list of results, where each result is a list of two lists.\n    # Example: [[[norm_list_1], [log_norm_list_1]], [[norm_list_2], [log_norm_list_2]]]\n    # Using str() on the list of lists achieves the desired nested structure string representation.\n    # The join and map approach ensures comma separation between the results of test cases.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4608974"}, {"introduction": "The spatial coordinates of measurement spots are the defining feature of this technology, but raw coordinates are not directly usable for many algorithms. To leverage this information, we must first model the spatial relationships between spots. Constructing a $k$-nearest neighbor ($k$-NN) graph is a powerful and common method to translate continuous Euclidean distances into a discrete network that captures local tissue topology. This practice [@problem_id:4609021] will guide you through building such a spatial graph and computing its fundamental properties, which are prerequisites for advanced tasks like spatial clustering and identifying regional gene expression patterns.", "problem": "You are given two-dimensional coordinates for spots in spatial transcriptomics, measured in micrometers. The goal is to build a $k$-nearest neighbor graph that encodes local spatial relationships and then derive graph-theoretic quantities that are foundational for downstream spatial analyses. Starting from core definitions in geometry and graph theory, implement the following steps for each test case:\n\n1. Treat the array of spots as points in a plane with Euclidean geometry. The Euclidean distance between two points with coordinates $(x_i, y_i)$ and $(x_j, y_j)$ is defined by $d_{ij} = \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}$.\n2. For a given integer $k$, construct an undirected $k$-nearest neighbor graph by connecting two distinct nodes $i$ and $j$ if either $i$ is among the $k$ nearest neighbors of $j$ or $j$ is among the $k$ nearest neighbors of $i$. Use the following tie-breaking rule to ensure determinism: when distances are equal, favor the smaller node index. Exclude self-loops.\n3. Let $A$ be the symmetric adjacency matrix of the graph, where $A_{ij} = 1$ if nodes $i$ and $j$ are connected and $A_{ij} = 0$ otherwise. Define the degree vector $d \\in \\mathbb{R}^N$ by $d_i = \\sum_{j=1}^N A_{ij}$ and the combinatorial graph Laplacian by $L = D - A$, where $D$ is the diagonal matrix with $D_{ii} = d_i$ and $N$ is the number of nodes.\n4. Identify boundary spots as those lying on the convex hull of the point set (the smallest convex polygon containing all points). Let $B$ be the index set of boundary spots (convex hull vertices) and $I$ be the complement set of interior spots. Compute the difference between the interior mean degree and boundary mean degree, defined by $\\Delta = \\left(\\frac{1}{|I|}\\sum_{i \\in I} d_i\\right) - \\left(\\frac{1}{|B|}\\sum_{b \\in B} d_b\\right)$. If $|I| = 0$ (that is, all spots lie on the convex hull), define $\\Delta = 0$.\n5. Compute the Frobenius norm of the Laplacian, $\\|L\\|_F = \\sqrt{\\sum_{i=1}^N \\sum_{j=1}^N L_{ij}^2}$.\n6. Determine whether the graph is connected by checking if there exists a single connected component spanning all $N$ nodes.\n\nFundamental base and constraints:\n- Use Euclidean geometry for distances and the standard graph-theoretic definitions above.\n- Do not introduce weights; the adjacency is binary and undirected as defined.\n- Coordinates are provided in micrometers ($\\mu$m). Since only rank-order of distances is used to construct the $k$-nearest neighbor graph, the outputs are dimensionless.\n- Angles, if any are needed, must be in radians.\n\nTest suite (use $k = 6$ for all cases):\n- Test Case $1$ (regular grid): $N = 9$ points on a $3 \\times 3$ grid with spacing $100\\,\\mu\\mathrm{m}$: $\\{(x, y) \\mid x \\in \\{0, 100, 200\\}, y \\in \\{0, 100, 200\\}\\}$ listed in row-major order: $(0,0)$, $(0,100)$, $(0,200)$, $(100,0)$, $(100,100)$, $(100,200)$, $(200,0)$, $(200,100)$, $(200,200)$.\n- Test Case $2$ (grid with a missing corner): $N = 15$ points from a $4 \\times 4$ grid with spacing $80\\,\\mu\\mathrm{m}$ over $\\{0, 80, 160, 240\\}$ in both axes, excluding the corner $(240,240)$; listed in row-major order of the remaining points.\n- Test Case $3$ (points on a circle): $N = 7$ points equally spaced on a circle of radius $150\\,\\mu\\mathrm{m}$ centered at the origin, with angles $\\theta_n = \\frac{2\\pi n}{7}$ for $n \\in \\{0,1,2,3,4,5,6\\}$, resulting in coordinates $(150\\cos \\theta_n, 150\\sin \\theta_n)$.\n\nRequired outputs for each test case:\n- The degree vector $d$ as a list of integers of length $N$.\n- The Frobenius norm $\\|L\\|_F$ of the Laplacian as a float rounded to six decimal places.\n- The interior-minus-boundary mean degree difference $\\Delta$ as a float rounded to six decimal places (with $\\Delta = 0$ if $|I| = 0$).\n- A boolean indicating whether the graph is connected.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list in the order: $[d, \\|L\\|_F, \\Delta, \\text{connected}]$. For example: $[[\\dots],\\dots]$.\n\nDiscuss qualitatively in your solution how boundary spots influence the degree vector and the Laplacian under the $k$-nearest neighbor construction. Your discussion must be grounded in the above core definitions without using shortcut formulas beyond those definitions.", "solution": "The problem requires the construction and analysis of a $k$-nearest neighbor ($k$-NN) graph from spatial coordinates. This process involves several foundational steps in computational geometry and graph theory. The solution will be systematically constructed based on the definitions provided.\n\n**1. Euclidean Distance Calculation**\nThe first step is to compute the pairwise Euclidean distance between all $N$ spots (nodes). Given a set of points $P = \\{p_1, p_2, \\dots, p_N\\}$ where each $p_i = (x_i, y_i)$, the distance $d_{ij}$ between points $p_i$ and $p_j$ is given a-priori by the formula $d_{ij} = \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}$. We pre-compute all such distances and store them in an $N \\times N$ matrix, which serves as the basis for neighborhood searches.\n\n**2. $k$-Nearest Neighbor Graph Construction**\nAn undirected $k$-NN graph is constructed. An edge connects two distinct nodes $i$ and $j$ if at least one of the following conditions is met:\n- Node $i$ is one of the $k$ nearest neighbors of node $j$.\n- Node $j$ is one of the $k$ nearest neighbors of node $i$.\n\nTo implement this, we first build a directed graph. For each node $j$, we identify its $k$ nearest neighbors. This is achieved by sorting all other nodes $i \\neq j$ based on their distance $d_{ji}$. To ensure a deterministic outcome, any ties in distance are resolved by favoring the node with the smaller index. Once the $k$ nearest neighbors for every node are found, we define a directed edge from $j$ to each of its neighbors.\n\nThe final undirected graph is the symmetric closure of this directed graph. An undirected edge $(i, j)$ exists if a directed edge $j \\to i$ exists, or a directed edge $i \\to j$ exists. The graph's structure is represented by a symmetric adjacency matrix $A$, where $A_{ij} = A_{ji} = 1$ if an edge exists between $i$ and $j$, and $0$ otherwise. By definition, there are no self-loops, so $A_{ii} = 0$.\n\n**3. Graph Laplacian and Degree**\nFrom the adjacency matrix $A$, we derive fundamental graph properties. The degree $d_i$ of a node $i$ is the number of edges connected to it, calculated as the sum of the $i$-th row of $A$: $d_i = \\sum_{j=1}^N A_{ij}$. The set of all degrees forms the degree vector $d$.\n\nThe combinatorial graph Laplacian, $L$, is defined as $L = D - A$, where $D$ is the diagonal matrix formed from the degree vector, i.e., $D_{ii} = d_i$ and $D_{ij} = 0$ for $i \\neq j$. The Laplacian matrix is a cornerstone of spectral graph theory and encodes important structural information about the graph.\n\n**4. Boundary and Interior Spot Analysis**\nThe distinction between boundary and interior spots is based on the geometry of the point set. Boundary spots are defined as the vertices of the convex hull of all spots. The convex hull is the smallest convex polygon that encloses all points. We implement the Monotone Chain algorithm to identify these vertices.\n1. Sort all points lexicographically (by $x$-coordinate, then $y$).\n2. Construct the lower hull by iterating through the sorted points and maintaining a path that always makes a left turn.\n3. Construct the upper hull similarly by iterating in reverse.\nThe union of these hulls gives the vertices of the convex hull, which correspond to the set of boundary spots, denoted $B$. The remaining spots form the set of interior spots, $I$.\n\nThe problem then asks for the calculation of $\\Delta$, the difference between the mean degree of interior spots and the mean degree of boundary spots:\n$$ \\Delta = \\left(\\frac{1}{|I|}\\sum_{i \\in I} d_i\\right) - \\left(\\frac{1}{|B|}\\sum_{b \\in B} d_b\\right) $$\nIf the set of interior spots $I$ is empty, $\\Delta$ is defined to be $0$.\n\n**Qualitative Discussion on Boundary Effects:**\nBoundary spots, being on the periphery of the point cloud, have a fundamentally different local environment than interior spots. An interior spot is typically surrounded by other spots in all directions, whereas a boundary spot has neighbors concentrated on its \"inward\" side. In the $k$-NN graph construction, an interior spot is likely to be included in the neighbor lists of many surrounding points, leading to a higher degree. Conversely, a boundary spot is less likely to be chosen as a neighbor by distant interior points, which have many closer alternatives. Consequently, the degree of a boundary spot is, on average, expected to be lower than that of an interior spot. This implies that $\\Delta$, the interior-minus-boundary mean degree difference, will typically be positive for a point set with a clear interior, reflecting the higher connectivity of centrally located nodes. This effect is a direct consequence of the spatial embedding and the local nature of the $k$-NN definition.\n\n**5. Frobenius Norm of the Laplacian**\nThe Frobenius norm of $L$, denoted $\\|L\\|_F$, is the square root of the sum of the squares of its elements:\n$$ \\|L\\|_F = \\sqrt{\\sum_{i=1}^N \\sum_{j=1}^N L_{ij}^2} $$\nThe squared norm can be computed efficiently from the degree vector. Since $L_{ii} = d_i$ and $L_{ij} = -A_{ij}$ for $i \\neq j$, we have:\n$$ \\|L\\|_F^2 = \\sum_{i=1}^N L_{ii}^2 + \\sum_{i \\neq j} L_{ij}^2 = \\sum_{i=1}^N d_i^2 + \\sum_{i \\neq j} (-A_{ij})^2 = \\sum_{i=1}^N d_i^2 + \\sum_{i \\neq j} A_{ij} $$\nGiven that $\\sum_{i \\neq j} A_{ij} = \\sum_{i=1}^N d_i$, the formula simplifies to:\n$$ \\|L\\|_F^2 = \\sum_{i=1}^N (d_i^2 + d_i) $$\nThis provides a computationally efficient path to the result.\n\n**6. Graph Connectivity**\nTo determine if the graph is connected, we must check if there is a path between any two nodes. This can be accomplished using a graph traversal algorithm, such as Breadth-First Search (BFS) or Depth-First Search (DFS). Starting from an arbitrary node (e.g., node $0$), we traverse all reachable nodes. If the total number of visited nodes equals the total number of nodes in the graph, $N$, the graph is connected. Otherwise, it consists of multiple disconnected components.\n\nBy implementing these steps, we can fully analyze the provided test cases and derive the required quantitative measures.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to solve the spatial transcriptomics analysis problem for all test cases.\n    \"\"\"\n\n    def _generate_test_cases():\n        \"\"\"Generates the coordinates and k-value for each test case.\"\"\"\n        k_val = 6\n        \n        # Test Case 1: 3x3 grid\n        points1 = []\n        for y in [0.0, 100.0, 200.0]:\n            for x in [0.0, 100.0, 200.0]:\n                points1.append((x, y))\n        case1 = (np.array(points1, dtype=float), k_val)\n\n        # Test Case 2: 4x4 grid with a missing corner\n        points2 = []\n        coords = [0.0, 80.0, 160.0, 240.0]\n        for y in coords:\n            for x in coords:\n                if x == 240.0 and y == 240.0:\n                    continue\n                points2.append((x, y))\n        case2 = (np.array(points2, dtype=float), k_val)\n\n        # Test Case 3: Points on a circle\n        points3 = []\n        N3 = 7\n        R = 150.0\n        for n in range(N3):\n            theta = 2.0 * math.pi * n / N3\n            points3.append((R * math.cos(theta), R * math.sin(theta)))\n        case3 = (np.array(points3, dtype=float), k_val)\n        \n        return [case1, case2, case3]\n\n    def _solve_single_case(points, k):\n        \"\"\"\n        Processes a single test case: constructs the k-NN graph and computes all required metrics.\n        \"\"\"\n        N = len(points)\n        if N == 0:\n            return [[], 0.0, 0.0, True]\n\n        # 1. Compute pairwise Euclidean distance matrix\n        dist_matrix = np.zeros((N, N))\n        for i in range(N):\n            for j in range(i + 1, N):\n                dist = np.linalg.norm(points[i] - points[j])\n                dist_matrix[i, j] = dist_matrix[j, i] = dist\n\n        # 2. Construct undirected k-NN graph\n        adj_matrix = np.zeros((N, N), dtype=int)\n        for j in range(N):\n            # For each point j, find its k-nearest neighbors\n            neighbors = []\n            for i in range(N):\n                if i == j:\n                    continue\n                # Store (distance, index) for sorting\n                neighbors.append((dist_matrix[j, i], i))\n            \n            # Sort by distance, then by index for tie-breaking\n            neighbors.sort()\n            \n            # Get indices of the k nearest neighbors\n            k_nearest_indices = [neighbor[1] for neighbor in neighbors[:k]]\n            \n            # Add directed edges j -> i for each neighbor i\n            for i in k_nearest_indices:\n                adj_matrix[j, i] = 1\n        \n        # Symmetrize the graph: an edge (i,j) exists if j->i OR i->j\n        adj_matrix = ((adj_matrix + adj_matrix.T) > 0).astype(int)\n\n        # 3. Compute degree vector and Laplacian\n        degree_vector = np.sum(adj_matrix, axis=1)\n\n        # 4. Identify boundary/interior spots and compute Delta\n        def convex_hull_indices(pts):\n            if len(pts) <= 2:\n                return set(range(len(pts)))\n            \n            # Sort points by x, then y, keeping track of original indices\n            indexed_pts = sorted(enumerate(pts), key=lambda item: (item[1][0], item[1][1]))\n            \n            def cross_product(p1, p2, p3):\n                # p1, p2, p3 are points, not indexed points\n                return (p2[0] - p1[0]) * (p3[1] - p1[1]) - (p2[1] - p1[1]) * (p3[0] - p1[0])\n\n            lower_hull = []\n            for idx, p in indexed_pts:\n                while len(lower_hull) >= 2 and cross_product(lower_hull[-2][1], lower_hull[-1][1], p) <= 0:\n                    lower_hull.pop()\n                lower_hull.append((idx, p))\n\n            upper_hull = []\n            for idx, p in reversed(indexed_pts):\n                while len(upper_hull) >= 2 and cross_product(upper_hull[-2][1], upper_hull[-1][1], p) <= 0:\n                    upper_hull.pop()\n                upper_hull.append((idx, p))\n\n            hull_indices = {item[0] for item in lower_hull[:-1]}\n            hull_indices.update({item[0] for item in upper_hull[:-1]})\n            return hull_indices\n\n        boundary_indices = convex_hull_indices(points)\n        interior_indices = set(range(N)) - boundary_indices\n        \n        delta = 0.0\n        if len(interior_indices) > 0 and len(boundary_indices) > 0:\n            interior_degrees = [degree_vector[i] for i in interior_indices]\n            boundary_degrees = [degree_vector[i] for i in boundary_indices]\n            mean_interior_degree = sum(interior_degrees) / len(interior_degrees)\n            mean_boundary_degree = sum(boundary_degrees) / len(boundary_degrees)\n            delta = mean_interior_degree - mean_boundary_degree\n\n        # 5. Compute Frobenius norm of the Laplacian\n        # ||L||_F^2 = sum(d_i^2 + d_i)\n        l_norm_sq = np.sum(degree_vector**2 + degree_vector)\n        frobenius_norm = np.sqrt(l_norm_sq)\n\n        # 6. Check for graph connectivity using BFS\n        is_connected = False\n        if N > 0:\n            q = [0]\n            visited = {0}\n            head = 0\n            while head < len(q):\n                u = q[head]\n                head += 1\n                for v in range(N):\n                    if adj_matrix[u, v] == 1 and v not in visited:\n                        visited.add(v)\n                        q.append(v)\n            is_connected = (len(visited) == N)\n        elif N == 0:\n            is_connected = True\n\n\n        # Format results\n        final_d = [int(d) for d in degree_vector]\n        final_norm = round(frobenius_norm, 6)\n        final_delta = round(delta, 6)\n        \n        return [final_d, final_norm, final_delta, is_connected]\n\n    test_cases = _generate_test_cases()\n    results = []\n    for points, k in test_cases:\n        result = _solve_single_case(points, k)\n        results.append(str(result))\n    \n    # Final print statement must match the specified format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4609021"}, {"introduction": "A primary goal of spatial transcriptomics is to unravel how cells communicate within their native tissue context, a process often mediated by ligand-receptor pairs. By integrating gene expression levels with spatial proximity, we can formulate and test hypotheses about cell-cell communication. This exercise [@problem_id:4608981] demonstrates how to implement a sophisticated analysis that quantifies potential ligand-receptor interactions and rigorously assesses their statistical significance using a permutation test, providing a framework for moving from descriptive analysis to biological inference.", "problem": "You are analyzing spatial transcriptomics data in which messenger ribonucleic acid (mRNA) counts for a ligand and a receptor are measured across spatially indexed spots. Let there be $N$ spatial spots indexed by $i \\in \\{1,\\dots,N\\}$ and $j \\in \\{1,\\dots,N\\}$. You are given ligand expression $x_{i,L}$ at spot $i$, receptor expression $x_{j,R}$ at spot $j$, and a spatial distance matrix with entries $d_{ij}$ in micrometers. The biological premise is that ligand-receptor communication in tissue is constrained by spatial proximity, consistent with diffusion-limited signaling and local microenvironment interactions. From a base of well-tested facts in spatial biology and biophysics, interactions tend to decay with distance under isotropic diffusion and noise. Therefore, model spatial influence with the following kernel-based weight:\n$$\nw_{ij} = \\exp\\left(-\\left(\\frac{d_{ij}}{\\sigma}\\right)^2\\right),\n$$\nwhere $\\sigma$ is a nonnegative length-scale parameter in micrometers that sets the effective spatial range of interaction. Paracrine signaling excludes self-interaction; thus only pairs with $i \\ne j$ should be included.\n\nDefine the weighted ligand-receptor interaction score as:\n$$\nS = \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\mathbf{1}[i \\ne j] \\, w_{ij} \\, x_{i,L} \\, x_{j,R},\n$$\nwhere $\\mathbf{1}[\\cdot]$ is the indicator function that equals $1$ if the condition holds and $0$ otherwise.\n\nAssess significance of $S$ via a permutation test under the null hypothesis that receptor expression is spatially exchangeable with respect to spot labels, implying no spatial coupling between ligand and receptor beyond chance. Perform $B$ independent permutations by randomly permuting receptor spot indices, yielding permuted scores $S^{(b)}$ for $b \\in \\{1,\\dots,B\\}$. Compute a one-sided p-value for testing whether the observed score is larger than expected by chance using the add-one correction:\n$$\np = \\frac{1 + \\sum_{b=1}^{B} \\mathbf{1}\\left[S^{(b)} \\ge S\\right]}{1 + B}.\n$$\n\nAll distances must be treated as micrometers. The interaction score $S$ is dimensionless (arbitrary units). The p-value $p$ must be expressed as a decimal in $[0,1]$. Angles do not appear in this problem. Your program must implement the above definitions exactly, with $\\mathbf{1}[i \\ne j]$ enforced by zeroing out the diagonal of the weight matrix.\n\nTest Suite:\nFor each test case, you are given $N$, the distance matrix entries $d_{ij}$ (micrometers), the ligand vector $x_{i,L}$, the receptor vector $x_{j,R}$, the kernel length-scale $\\sigma$ (micrometers), the number of permutations $B$, and a pseudorandom seed to ensure reproducibility. The test cases are:\n\n- Test Case A (general case):\n  - $N = 5$\n  - $d_{ij}$ rows: \n    - Row $1$: $0, 10, 20, 15, 30$\n    - Row $2$: $10, 0, 12, 25, 18$\n    - Row $3$: $20, 12, 0, 8, 22$\n    - Row $4$: $15, 25, 8, 0, 16$\n    - Row $5$: $30, 18, 22, 16, 0$\n  - $x_{i,L} = [2.0, 0.5, 1.2, 0.0, 3.0]$\n  - $x_{j,R} = [0.1, 1.0, 0.0, 2.5, 1.2]$\n  - $\\sigma = 15$\n  - $B = 500$\n  - seed $= 123$\n\n- Test Case B (boundary case: zero receptor expression):\n  - $N = 4$\n  - $d_{ij}$ rows:\n    - Row $1$: $0, 5, 10, 15$\n    - Row $2$: $5, 0, 7, 12$\n    - Row $3$: $10, 7, 0, 6$\n    - Row $4$: $15, 12, 6, 0$\n  - $x_{i,L} = [1.0, 2.0, 1.0, 0.5]$\n  - $x_{j,R} = [0.0, 0.0, 0.0, 0.0]$\n  - $\\sigma = 10$\n  - $B = 100$\n  - seed $= 999$\n\n- Test Case C (edge case: effectively zero weights due to very small kernel scale):\n  - $N = 4$\n  - $d_{ij}$ rows:\n    - Row $1$: $0, 100, 150, 200$\n    - Row $2$: $100, 0, 120, 180$\n    - Row $3$: $150, 120, 0, 90$\n    - Row $4$: $200, 180, 90, 0$\n  - $x_{i,L} = [5.0, 3.0, 4.0, 2.0]$\n  - $x_{j,R} = [1.0, 0.5, 2.0, 1.5]$\n  - $\\sigma = 1$\n  - $B = 200$\n  - seed $= 7$\n\n- Test Case D (boundary case: two spots only):\n  - $N = 2$\n  - $d_{ij}$ rows:\n    - Row $1$: $0, 50$\n    - Row $2$: $50, 0$\n  - $x_{i,L} = [1.0, 2.0]$\n  - $x_{j,R} = [3.0, 4.0]$\n  - $\\sigma = 50$\n  - $B = 10$\n  - seed $= 1$\n\n- Test Case E (edge case: distinct spots with zero distance):\n  - $N = 3$\n  - $d_{ij}$ rows:\n    - Row $1$: $0, 0, 10$\n    - Row $2$: $0, 0, 10$\n    - Row $3$: $10, 10, 0$\n  - $x_{i,L} = [0.0, 1.0, 2.0]$\n  - $x_{j,R} = [3.0, 0.0, 1.0]$\n  - $\\sigma = 5$\n  - $B = 100$\n  - seed $= 101$\n\nOutput Specification:\nFor each test case, compute the interaction score $S$ and its permutation p-value $p$ as defined above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test caseâ€™s result must be a list of two floats $[S,p]$. Aggregate the test cases in order A, B, C, D, E into a single list-of-lists. For example, the output should look like:\n$$\n\\text{[[S_A,p_A],[S_B,p_B],[S_C,p_C],[S_D,p_D],[S_E,p_E]]}\n$$\nprinted as a single line with no additional text.", "solution": "The problem requires the calculation of a spatial ligand-receptor interaction score, denoted $S$, and an assessment of its statistical significance through a permutation test, yielding a p-value, $p$. The analysis is performed on spatial transcriptomics data, which includes expression levels of a ligand and a receptor across $N$ spatially indexed spots, along with the distances between these spots.\n\nThe solution is structured as follows: First, we compute the observed interaction score $S$ based on the provided biological model. Second, we perform a permutation test under the null hypothesis of no spatial coupling to generate an empirical null distribution of scores. Finally, we compute the p-value by comparing the observed score to this null distribution.\n\nThe weighted ligand-receptor interaction score, $S$, is defined as:\n$$\nS = \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\mathbf{1}[i \\ne j] \\, w_{ij} \\, x_{i,L} \\, x_{j,R}\n$$\nHere, $x_{i,L}$ is the ligand expression at spot $i$, $x_{j,R}$ is the receptor expression at spot $j$, and $\\mathbf{1}[i \\ne j]$ is an indicator function that enforces the biological principle of paracrine signaling (i.e., excluding self-interaction where $i=j$). The term $w_{ij}$ is a weight derived from a Gaussian kernel, which models the decay of interaction potential with distance:\n$$\nw_{ij} = \\exp\\left(-\\left(\\frac{d_{ij}}{\\sigma}\\right)^2\\right)\n$$\nwhere $d_{ij}$ is the distance between spots $i$ and $j$, and $\\sigma$ is a length-scale parameter defining the effective range of interaction.\n\nFor computational efficiency, this calculation can be expressed in matrix algebra. Let $x_L$ be the $N \\times 1$ column vector of ligand expression values, $x_R$ be the $N \\times 1$ column vector of receptor expression values, and $D$ be the $N \\times N$ matrix of distances $d_{ij}$. First, we compute an intermediate weight matrix $W'$ with elements $w'_{ij} = \\exp(-(d_{ij}/\\sigma)^2)$. Then, to enforce the $i \\ne j$ condition, we construct the final weight matrix $W$ by setting the diagonal elements of $W'$ to $0$. This is equivalent to multiplying element-wise by a matrix of ones with a zero diagonal. With these matrices, the score $S$ can be calculated as a quadratic form:\n$$\nS = x_L^T W x_R\n$$\nThis matrix formulation is computationally advantageous over nested loops, especially for a large number of spots $N$.\n\nNext, to assess the statistical significance of the observed score $S$, we employ a permutation test. The null hypothesis, $H_0$, is that the spatial locations of receptor expressions are random with respect to the ligand expressions. To simulate this null scenario, we generate $B$ permuted datasets. In each permutation $b \\in \\{1, \\dots, B\\}$, the indices of the receptor expression vector $x_R$ are randomly shuffled to create a new vector $x_R^{(b)}$. The ligand expression vector $x_L$ and the weight matrix $W$ remain fixed. A new score, $S^{(b)}$, is calculated for each permuted dataset:\n$$\nS^{(b)} = x_L^T W x_R^{(b)}\n$$\nThis process yields a distribution of $B$ scores $\\{S^{(1)}, S^{(2)}, \\dots, S^{(B)}\\}$ that represents the range of scores expected under the null hypothesis.\n\nFinally, we compute a one-sided p-value to determine the probability of observing a score at least as large as the actual score $S$ by chance. The problem specifies the use of an add-one correction, which is a standard practice to avoid p-values of $0$ and handle cases where the observed score is the most extreme in the permutation set. The p-value $p$ is calculated as:\n$$\np = \\frac{1 + \\sum_{b=1}^{B} \\mathbf{1}\\left[S^{(b)} \\ge S\\right]}{1 + B}\n$$\nThe numerator counts how many permuted scores were greater than or equal to the observed score, with $1$ added to account for the observed score itself. The denominator is the total number of permutations plus one. For reproducibility, the random shuffling process is initialized with a specific seed for each test case.\n\nThe algorithm proceeds by first calculating the weight matrix $W$ from the given distance matrix $D$ and $\\sigma$. Then, the observed score $S$ is computed using the original expression vectors $x_L$ and $x_R$. Following this, a loop runs $B$ times, where in each iteration a permuted receptor vector $x_R^{(b)}$ is generated, the corresponding score $S^{(b)}$ is calculated, and it is compared against $S$. After the loop, the p-value $p$ is calculated from the total count of permuted scores exceeding or equaling $S$. The final output for each test case is the pair $[S, p]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def solve_case(N, d_matrix, x_L, x_R, sigma, B, seed):\n        \"\"\"\n        Solves a single test case for the spatial transcriptomics problem.\n\n        Args:\n            N (int): Number of spatial spots.\n            d_matrix (list of list of float): The distance matrix.\n            x_L (list of float): Ligand expression vector.\n            x_R (list of float): Receptor expression vector.\n            sigma (float): Kernel length-scale parameter.\n            B (int): Number of permutations.\n            seed (int): Pseudorandom seed for reproducibility.\n\n        Returns:\n            list: A list containing the calculated score S and p-value p.\n        \"\"\"\n        \n        # Cast inputs to numpy arrays for vectorized operations\n        D = np.array(d_matrix, dtype=float)\n        x_L = np.array(x_L, dtype=float)\n        x_R = np.array(x_R, dtype=float)\n\n        # Step 1: Calculate the weight matrix W\n        if sigma == 0:\n            # Handle sigma=0 as a limit case: weight is 1 if distance is 0, else 0.\n            # This case is not in the test suite but makes the function more robust.\n            W = (D == 0).astype(float)\n        else:\n            # Standard Gaussian kernel for weights\n            W = np.exp(-(D / sigma)**2)\n        \n        # Enforce paracrine signaling (i != j) by zeroing the diagonal of W\n        np.fill_diagonal(W, 0)\n        \n        # Helper function for score calculation using the efficient matrix form\n        def calculate_score(ligand_vec, receptor_vec, weight_matrix):\n            # S = x_L^T * W * x_R\n            return ligand_vec @ weight_matrix @ receptor_vec\n\n        # Step 2: Calculate the observed score S\n        S_obs = calculate_score(x_L, x_R, W)\n        \n        # Step 3: Perform the permutation test\n        count_greater_equal = 0\n        \n        # Initialize a random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n        \n        for _ in range(B):\n            # Generate a permuted receptor vector by shuffling its elements\n            x_R_permuted = rng.permutation(x_R)\n            \n            # Calculate the score for the permuted data\n            S_permuted = calculate_score(x_L, x_R_permuted, W)\n            \n            # Check if the permuted score is greater than or equal to the observed score\n            if S_permuted >= S_obs:\n                count_greater_equal += 1\n            \n        # Step 4: Calculate the one-sided p-value with add-one correction\n        p_value = (1 + count_greater_equal) / (1 + B)\n        \n        return [S_obs, p_value]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case A\n        {\"N\": 5, \"d_matrix\": [[0, 10, 20, 15, 30], [10, 0, 12, 25, 18], [20, 12, 0, 8, 22], [15, 25, 8, 0, 16], [30, 18, 22, 16, 0]], \"x_L\": [2.0, 0.5, 1.2, 0.0, 3.0], \"x_R\": [0.1, 1.0, 0.0, 2.5, 1.2], \"sigma\": 15, \"B\": 500, \"seed\": 123},\n        # Test Case B\n        {\"N\": 4, \"d_matrix\": [[0, 5, 10, 15], [5, 0, 7, 12], [10, 7, 0, 6], [15, 12, 6, 0]], \"x_L\": [1.0, 2.0, 1.0, 0.5], \"x_R\": [0.0, 0.0, 0.0, 0.0], \"sigma\": 10, \"B\": 100, \"seed\": 999},\n        # Test Case C\n        {\"N\": 4, \"d_matrix\": [[0, 100, 150, 200], [100, 0, 120, 180], [150, 120, 0, 90], [200, 180, 90, 0]], \"x_L\": [5.0, 3.0, 4.0, 2.0], \"x_R\": [1.0, 0.5, 2.0, 1.5], \"sigma\": 1, \"B\": 200, \"seed\": 7},\n        # Test Case D\n        {\"N\": 2, \"d_matrix\": [[0, 50], [50, 0]], \"x_L\": [1.0, 2.0], \"x_R\": [3.0, 4.0], \"sigma\": 50, \"B\": 10, \"seed\": 1},\n        # Test Case E\n        {\"N\": 3, \"d_matrix\": [[0, 0, 10], [0, 0, 10], [10, 10, 0]], \"x_L\": [0.0, 1.0, 2.0], \"x_R\": [3.0, 0.0, 1.0], \"sigma\": 5, \"B\": 100, \"seed\": 101}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(\n            case[\"N\"],\n            case[\"d_matrix\"],\n            case[\"x_L\"],\n            case[\"x_R\"],\n            case[\"sigma\"],\n            case[\"B\"],\n            case[\"seed\"]\n        )\n        results.append(result)\n\n    # Format the output string exactly as specified.\n    inner_strings = [f\"[{s},{p}]\" for s, p in results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "4608981"}]}