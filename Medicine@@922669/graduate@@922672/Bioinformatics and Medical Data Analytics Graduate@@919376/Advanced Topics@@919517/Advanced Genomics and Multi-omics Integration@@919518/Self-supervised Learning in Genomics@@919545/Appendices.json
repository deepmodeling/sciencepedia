{"hands_on_practices": [{"introduction": "A core principle in genomics is that the double-stranded DNA molecule carries equivalent information in both its forward and reverse-complement orientations. A robust genomic representation should therefore be invariant to this transformation, meaning a sequence and its reverse-complement should produce identical embeddings. This exercise challenges you to move from this biological concept to a rigorous quantitative test by designing and implementing a metric that measures the degree of reverse-complement asymmetry in learned embeddings [@problem_id:4606981].", "problem": "You are analyzing learned representations of Deoxyribonucleic Acid (DNA) sequences in the self-supervised learning (SSL) setting. In double-stranded DNA, base pairing is well-defined: the Watson–Crick complement maps $A \\leftrightarrow T$ and $C \\leftrightarrow G$. Given a finite alphabet $\\Sigma = \\{A,C,G,T\\}$, define the complement operator $c:\\Sigma \\to \\Sigma$ by $c(A)=T$, $c(T)=A$, $c(C)=G$, and $c(G)=C$. For a sequence $s = (s_1,s_2,\\dots,s_L) \\in \\Sigma^L$, define the reverse-complement operator $R$ by $R(s) = (c(s_L), c(s_{L-1}), \\dots, c(s_1))$. In many genomics tasks, a learned embedding function $f:\\Sigma^* \\to \\mathbb{R}^d$ ought to be reverse-complement invariant, meaning $f(s) = f(R(s))$ for content-equivalent sequences regardless of orientation. You must propose a test that quantitatively verifies reverse-complement invariance and design a metric that penalizes asymmetry under sequence flipping.\n\nStarting from the fundamental basis that double-stranded DNA encodes the same biological information in both orientations through base complementarity and that complementary base pairs are $A \\leftrightarrow T$ and $C \\leftrightarrow G$, derive a principled algorithmic test and a quantitative metric suitable for $\\mathbb{R}^d$ embeddings learned via self-supervised objectives. Your metric must satisfy the following properties for any finite set $S \\subset \\Sigma^*$ and any embedding function $f$:\n- Non-negativity: $M(f,S) \\ge 0$.\n- Identity of indiscernibles with respect to reverse-complement invariance: $M(f,S) = 0$ if and only if $f(s) = f(R(s))$ for all $s \\in S$.\n- Scale invariance: for any scalar $\\alpha > 0$, replacing $f$ by $\\alpha f$ leaves the metric unchanged.\n- Robust aggregation across heterogeneous sequence lengths without requiring explicit rescaling per input sequence beyond what is implied by the metric design.\n\nYou must also define a decision rule that, given a non-negative threshold $\\tau$, declares whether an embedding function $f$ is reverse-complement invariant on a set $S$.\n\nImplement your metric and test for the following test suite. Each test case consists of a specific embedding function, a sequence set, and a threshold. The embedding functions are defined mathematically as follows; let $L$ denote sequence length and positions be indexed by $p=1,\\dots,L$:\n1. Embedding function $f_{\\mathrm{inv}}:\\Sigma^* \\to \\mathbb{R}^2$ (designed to be reverse-complement invariant):\n   - Define $\\phi_{\\mathrm{inv}}:\\Sigma \\to \\mathbb{R}^2$ by $\\phi_{\\mathrm{inv}}(A)=(1,0)$, $\\phi_{\\mathrm{inv}}(T)=(1,0)$, $\\phi_{\\mathrm{inv}}(C)=(0,1)$, and $\\phi_{\\mathrm{inv}}(G)=(0,1)$.\n   - For $s=(s_1,\\dots,s_L)$, define $f_{\\mathrm{inv}}(s) = \\frac{1}{L}\\sum_{p=1}^{L} \\phi_{\\mathrm{inv}}(s_p)$.\n2. Embedding function $f_{\\mathrm{sens}}:\\Sigma^* \\to \\mathbb{R}^3$ (designed to be orientation-sensitive):\n   - Define $\\phi_{\\mathrm{sens}}:\\Sigma \\to \\mathbb{R}^3$ by $\\phi_{\\mathrm{sens}}(A)=(1.0,0.5,0.2)$, $\\phi_{\\mathrm{sens}}(C)=(0.1,1.0,0.3)$, $\\phi_{\\mathrm{sens}}(G)=(0.3,0.2,1.0)$, and $\\phi_{\\mathrm{sens}}(T)=(1.2,0.8,0.4)$.\n   - Define a position-dependent weight $w(p)=1+0.4(p-1)$ and a position encoding vector $\\psi(p,L)=(0.05p,\\,0.02(L+1-p),\\,0.03)$.\n   - For $s=(s_1,\\dots,s_L)$, define $f_{\\mathrm{sens}}(s) = \\frac{1}{L}\\sum_{p=1}^{L} \\big(w(p)\\,\\phi_{\\mathrm{sens}}(s_p) + \\psi(p,L)\\big)$.\n3. Embedding function $f_{\\mathrm{part}}:\\Sigma^* \\to \\mathbb{R}^2$ (designed to be partially invariant with small orientation sensitivity):\n   - Define $\\phi_{\\mathrm{part}}:\\Sigma \\to \\mathbb{R}^2$ by $\\phi_{\\mathrm{part}}(A)=(1,0)$, $\\phi_{\\mathrm{part}}(T)=(1,0)$, $\\phi_{\\mathrm{part}}(C)=(0,1)$, and $\\phi_{\\mathrm{part}}(G)=(0,1)$.\n   - Define scalar coefficients $c:\\Sigma \\to \\mathbb{R}$ by $c(A)=1.0$, $c(T)=-1.0$, $c(C)=0.5$, and $c(G)=-0.5$, and let $\\varepsilon=0.02$.\n   - For $s=(s_1,\\dots,s_L)$, define $g(s)=\\sum_{p=1}^{L} c(s_p)\\,\\frac{p}{L}$ and $v_{\\mathrm{noise}}(s)=\\varepsilon\\,\\frac{g(s)}{L}\\,(1,-1)$, and then $f_{\\mathrm{part}}(s)=\\frac{1}{L}\\sum_{p=1}^{L}\\phi_{\\mathrm{part}}(s_p) + v_{\\mathrm{noise}}(s)$.\n\nUse the following sequence sets:\n- $S_{\\mathrm{gen}} = \\{\\text{\"ACGTAC\"}, \\text{\"TTGCA\"}, \\text{\"CGAT\"}, \\text{\"GATTACA\"}, \\text{\"CCGGTTAA\"}\\}$.\n- $S_{\\mathrm{pal}} = \\{\\text{\"AT\"}, \\text{\"GC\"}, \\text{\"AGCT\"}, \\text{\"AATT\"}, \\text{\"CGCG\"}\\}$.\n- $S_{\\mathrm{sing}} = \\{\\text{\"A\"}, \\text{\"C\"}, \\text{\"G\"}, \\text{\"T\"}\\}$.\n- $S_{\\mathrm{long}} = \\{\\text{\"ACGTACGTACGTACGTACGT\"}\\}$.\n\nDefine the test cases as ordered triples $(f,S,\\tau)$ in the following order:\n- Case 1: $(f_{\\mathrm{inv}}, S_{\\mathrm{gen}}, 10^{-12})$.\n- Case 2: $(f_{\\mathrm{sens}}, S_{\\mathrm{gen}}, 10^{-1})$.\n- Case 3: $(f_{\\mathrm{part}}, S_{\\mathrm{long}}, 5\\times 10^{-2})$.\n- Case 4: $(f_{\\mathrm{inv}}, S_{\\mathrm{pal}}, 10^{-12})$.\n- Case 5: $(f_{\\mathrm{sens}}, S_{\\mathrm{sing}}, 2\\times 10^{-1})$.\n\nYour program must:\n- Implement a reverse-complement operator $R$ consistent with the definition above.\n- Implement the three embedding functions exactly as specified.\n- Design and compute a quantitative asymmetry metric $M(f,S)$ satisfying the four properties above.\n- Implement a decision rule that returns a boolean indicating whether $M(f,S) \\le \\tau$.\n- Produce the results for the five cases in the specified order.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each result must be a two-item list containing the metric value (rounded to six decimals) and the boolean decision, for example, $[m,\\mathrm{True}]$ or $[m,\\mathrm{False}]$. The final output line must therefore have the form $[[m_1,d_1],[m_2,d_2],[m_3,d_3],[m_4,d_4],[m_5,d_5]]$, where each $m_i$ is a float rounded to six decimals and each $d_i$ is a boolean.", "solution": "The problem requires the design of a quantitative test to verify if a learned embedding function for Deoxyribonucleic Acid (DNA) sequences exhibits reverse-complement invariance. This is a crucial property in genomics, as a double-stranded DNA molecule carries the same biological information in both its forward and reverse-complement strands. We must derive a principled metric $M(f,S)$ for an embedding function $f$ over a set of sequences $S$, and an associated decision rule.\n\nThe fundamental principle of reverse-complement invariance for an embedding function $f:\\Sigma^* \\to \\mathbb{R}^d$ is that for any sequence $s \\in \\Sigma^*$, its embedding should be identical to the embedding of its reverse-complement, denoted $R(s)$. Mathematically, this is expressed as the equality $f(s) = f(R(s))$. The task is to quantify the deviation from this equality.\n\nOur first step is to define a measure of asymmetry for a single sequence $s$. A natural starting point is the vector difference $v_{\\mathrm{diff}}(s) = f(s) - f(R(s))$. In an invariant embedding, $v_{\\mathrm{diff}}(s) = \\vec{0}$. The magnitude of this difference can be measured by a vector norm, such as the Euclidean norm $\\|v_{\\mathrm{diff}}(s)\\|_2 = \\|f(s) - f(R(s))\\|_2$. This quantity is non-negative and is zero if and only if $f(s) = f(R(s))$, satisfying two of the required properties for a single sequence.\n\nHowever, the problem requires the metric to be scale-invariant. If we scale the embedding function by a factor $\\alpha > 0$, to a new function $f' = \\alpha f$, the norm of the difference becomes $\\|\\alpha f(s) - \\alpha f(R(s))\\|_2 = \\alpha \\|f(s) - f(R(s))\\|_2$. This is not scale-invariant. To address this, we must normalize the difference. A standard way to create a scale-invariant, relative measure of difference between two vectors $u$ and $v$ is to normalize the norm of their difference by the sum of their norms. This leads to the definition of our per-sequence asymmetry metric, $m(s, f)$:\n$$\nm(s, f) = \\frac{\\|f(s) - f(R(s))\\|_2}{\\|f(s)\\|_2 + \\|f(R(s))\\|_2}\n$$\nIn the special case where both $f(s)$ and $f(R(s))$ are the zero vector, they are equal, and the expression becomes $\\frac{0}{0}$. We define $m(s, f) = 0$ in this case, which is consistent with the condition of perfect invariance.\n\nThis per-sequence metric $m(s, f)$ satisfies the required properties for a single sequence:\n1.  **Non-negativity**: Since norms are non-negative, $m(s, f) \\ge 0$.\n2.  **Identity of indiscernibles**: $m(s,f) = 0$ if and only if $\\|f(s) - f(R(s))\\|_2 = 0$, which is true if and only if $f(s) = f(R(s))$ (assuming a non-zero denominator, or handling the zero-vector case as defined).\n3.  **Scale invariance**: For $\\alpha > 0$ and $f' = \\alpha f$,\n    $$\n    m(s, f') = \\frac{\\|\\alpha f(s) - \\alpha f(R(s))\\|_2}{\\|\\alpha f(s)\\|_2 + \\|\\alpha f(R(s))\\|_2} = \\frac{\\alpha \\|f(s) - f(R(s))\\|_2}{\\alpha \\left( \\|f(s)\\|_2 + \\|f(R(s))\\|_2 \\right)} = m(s, f)\n    $$\n    The property holds.\n\nNext, we must aggregate these per-sequence scores over a finite set of sequences $S \\subset \\Sigma^*$ to form the final metric $M(f,S)$. The problem requires robust aggregation across sequences of heterogeneous lengths. Averaging the normalized per-sequence scores is a suitable approach, as each score $m(s,f)$ is already a dimensionless quantity between $0$ and $1$ (related to cosine distance). This prevents sequences that produce embeddings with large norms from disproportionately influencing the aggregate score. We therefore define the aggregate metric as the arithmetic mean of the per-sequence scores:\n$$\nM(f,S) = \\frac{1}{|S|} \\sum_{s \\in S} m(s, f) = \\frac{1}{|S|} \\sum_{s \\in S} \\frac{\\|f(s) - f(R(s))\\|_2}{\\|f(s)\\|_2 + \\|f(R(s))\\|_2}\n$$\nThis aggregate metric $M(f,S)$ inherits the desired properties: it is non-negative, scale-invariant, and equals zero if and only if $f(s) = f(R(s))$ for all $s \\in S$. The aggregation by averaging provides robustness as required.\n\nFinally, we define the algorithmic test and decision rule. The test consists of computing the value of $M(f,S)$ for a given function $f$ and sequence set $S$. The decision rule, given a non-negative threshold $\\tau$, is:\n- The embedding function $f$ is declared **reverse-complement invariant** on the set $S$ if $M(f,S) \\le \\tau$.\n- Otherwise, it is declared **not reverse-complement invariant** on $S$.\n\nThis framework provides a complete, principled, and quantitative method for assessing the reverse-complement invariance of DNA sequence embeddings as specified by the problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the analysis, and print results.\n    \"\"\"\n    \n    # ------------------ Operators and Helper Maps ------------------\n\n    def reverse_complement(s: str) -> str:\n        \"\"\"Computes the reverse complement of a DNA sequence.\"\"\"\n        complement_map = str.maketrans('ACGT', 'TGCA')\n        return s.translate(complement_map)[::-1]\n\n    # Maps for embedding function definitions\n    PHI_INV_MAP = {\n        'A': np.array([1.0, 0.0]), 'T': np.array([1.0, 0.0]),\n        'C': np.array([0.0, 1.0]), 'G': np.array([0.0, 1.0]),\n    }\n    PHI_SENS_MAP = {\n        'A': np.array([1.0, 0.5, 0.2]), 'C': np.array([0.1, 1.0, 0.3]),\n        'G': np.array([0.3, 0.2, 1.0]), 'T': np.array([1.2, 0.8, 0.4]),\n    }\n    PHI_PART_MAP = PHI_INV_MAP # Base is the same as f_inv\n    COEFF_PART_MAP = {'A': 1.0, 'T': -1.0, 'C': 0.5, 'G': -0.5}\n\n    # ------------------ Embedding Functions ------------------\n\n    def f_inv(s: str) -> np.ndarray:\n        \"\"\"Invariant embedding function.\"\"\"\n        L = len(s)\n        if L == 0:\n            return np.zeros(2)\n        \n        total = np.zeros(2)\n        for base in s:\n            total += PHI_INV_MAP[base]\n        \n        return total / L\n\n    def f_sens(s: str) -> np.ndarray:\n        \"\"\"Orientation-sensitive embedding function.\"\"\"\n        L = len(s)\n        if L == 0:\n            return np.zeros(3)\n\n        total = np.zeros(3)\n        for p_one_based in range(1, L + 1):\n            idx = p_one_based - 1\n            base = s[idx]\n            \n            w_p = 1.0 + 0.4 * (p_one_based - 1)\n            psi_p = np.array([0.05 * p_one_based, 0.02 * (L + 1 - p_one_based), 0.03])\n            \n            term = w_p * PHI_SENS_MAP[base] + psi_p\n            total += term\n\n        return total / L\n\n    def f_part(s: str) -> np.ndarray:\n        \"\"\"Partially invariant embedding function.\"\"\"\n        L = len(s)\n        if L == 0:\n            return np.zeros(2)\n\n        # Invariant part\n        f_base_total = np.zeros(2)\n        for base in s:\n            f_base_total += PHI_PART_MAP[base]\n        f_base = f_base_total / L\n\n        # Noise part\n        epsilon = 0.02\n        g_s = 0.0\n        for p_one_based in range(1, L + 1):\n            idx = p_one_based - 1\n            base = s[idx]\n            g_s += COEFF_PART_MAP[base] * (p_one_based / L)\n        \n        v_noise_vec = np.array([1.0, -1.0])\n        v_noise = epsilon * (g_s / L) * v_noise_vec\n        \n        return f_base + v_noise\n\n    # ------------------ Metric and Test Implementation ------------------\n\n    def calculate_metric(f, S: list[str]) -> float:\n        \"\"\"Calculates the asymmetry metric M(f, S).\"\"\"\n        if not S:\n            return 0.0\n        \n        total_metric = 0.0\n        for s in S:\n            rs = reverse_complement(s)\n            \n            # For palindromic sequences, s == rs, so metric contribution is 0\n            if s == rs:\n                continue\n\n            v_s = f(s)\n            v_rs = f(rs)\n            \n            norm_s = np.linalg.norm(v_s)\n            norm_rs = np.linalg.norm(v_rs)\n            \n            denominator = norm_s + norm_rs\n            \n            if denominator == 0.0:\n                # If both norms are 0, vectors are equal (both zero), so diff is 0\n                seq_metric = 0.0\n            else:\n                diff_norm = np.linalg.norm(v_s - v_rs)\n                seq_metric = diff_norm / denominator\n            \n            total_metric += seq_metric\n            \n        return total_metric / len(S)\n\n    # ------------------ Test Case Setup and Execution ------------------\n\n    # Sequence Sets\n    S_gen = [\"ACGTAC\", \"TTGCA\", \"CGAT\", \"GATTACA\", \"CCGGTTAA\"]\n    S_pal = [\"AT\", \"GC\", \"AGCT\", \"AATT\", \"CGCG\"]\n    S_sing = [\"A\", \"C\", \"G\", \"T\"]\n    S_long = [\"ACGTACGTACGTACGTACGT\"]\n\n    # Embedding function map\n    func_map = {\n        'f_inv': f_inv,\n        'f_sens': f_sens,\n        'f_part': f_part,\n    }\n    \n    # Sequence set map\n    set_map = {\n        'S_gen': S_gen,\n        'S_pal': S_pal,\n        'S_sing': S_sing,\n        'S_long': S_long,\n    }\n\n    # Test Cases: (function_name, set_name, threshold)\n    test_cases = [\n        ('f_inv', 'S_gen', 1e-12),\n        ('f_sens', 'S_gen', 1e-1),\n        ('f_part', 'S_long', 5e-2),\n        ('f_inv', 'S_pal', 1e-12),\n        ('f_sens', 'S_sing', 2e-1),\n    ]\n\n    results = []\n    for f_name, s_name, tau in test_cases:\n        f = func_map[f_name]\n        S = set_map[s_name]\n        \n        metric_value = calculate_metric(f, S)\n        decision = metric_value <= tau\n        \n        rounded_metric = round(metric_value, 6)\n        results.append(f\"[{rounded_metric},{decision}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4606981"}, {"introduction": "Before a genomic language model can learn from DNA, the raw sequence of characters must be converted into a series of numerical tokens. This tokenization step is critical, as it defines the vocabulary the model sees and can significantly impact its ability to recognize biological patterns. This practice provides a hands-on comparison of two widely used tokenization strategies, Byte-Pair Encoding (BPE) and the Unigram Language Model (ULM), tasking you with evaluating their performance not only on compression efficiency but also on their alignment with structured repetitive elements in the genome [@problem_id:4606969].", "problem": "You are given a set of Deoxyribonucleic Acid (DNA) sequences and are asked to assess the suitability of Byte-Pair Encoding (BPE) and Unigram Language Models (ULM) as tokenization-based compressors in a self-supervised learning setting for genomics. Using fundamental coding theory, define compression as the expected code length induced by a probabilistic tokenizer, and quantify tokenization errors near repetitive regions. Implement both methods from first principles and compute comparative metrics across a test suite.\n\nStart from the following foundational base:\n- The Shannon source coding principle states that for a discrete probability mass function $p(w)$ over tokens $w$, the optimal expected code length for a token $w$ in bits is given by $-\\log_2 p(w)$, and the total code length for a sequence segmented into tokens $\\{w_1, w_2, \\dots, w_T\\}$ is $$L = \\sum_{t=1}^{T} -\\log_2 p(w_t).$$\n- The average code length per nucleotide for a DNA sequence of length $N$ is $$\\bar{\\ell} = \\frac{L}{N}.$$\n- A repetitive region is defined by a motif of fixed length that repeats consecutively. Let $s$ be the starting index of the region in the sequence (zero-based), $m$ be the motif length, and $r$ be the number of motif repeats, so the region spans indices $[s, s + r \\cdot m)$.\n\nYou must implement the following methods:\n1. Byte-Pair Encoding (BPE), defined as starting with a character-level vocabulary $\\{A, C, G, T\\}$ and iteratively merging the most frequent adjacent token pair to form a new token until a specified vocabulary size $V_{\\mathrm{BPE}}$ is reached. At each merge step, break ties by choosing the lexicographically smallest pair when represented as the concatenation $(\\text{left} + \\text{right})$. The training segmentation is obtained by repeatedly applying the learned merges to the training sequence. Probabilities are estimated from token frequencies in the training segmentation using additive smoothing $$p_{\\mathrm{BPE}}(w) = \\frac{c(w) + \\lambda}{\\sum_{u} c(u) + \\lambda \\cdot |V|},$$ where $c(w)$ is the count of token $w$, $|V|$ is the final vocabulary size, and $\\lambda$ is the smoothing parameter. The test sequence is segmented by applying the same merge operations in order.\n\n2. Unigram Language Model (ULM) tokenization: Construct a candidate vocabulary $V_{\\mathrm{ULM}}$ consisting of all substrings of the training sequence of lengths from $1$ to $L_{\\max}$, counted with overlaps. Estimate probabilities using additive smoothing $$p_{\\mathrm{ULM}}(w) = \\frac{c(w) + \\alpha}{\\sum_{u \\in V_{\\mathrm{ULM}}} c(u) + \\alpha \\cdot |V_{\\mathrm{ULM}}|},$$ where $\\alpha$ is the smoothing parameter. Tokenize the test sequence by dynamic programming to find the segmentation that minimizes the total negative log-probability $$\\min_{\\text{segmentations}} \\sum_{t} -\\log_2 p_{\\mathrm{ULM}}(w_t),$$ constrained such that each token $w_t \\in V_{\\mathrm{ULM}}$ and token lengths are at most $L_{\\max}$.\n\nDefine and compute the following error metric near repetitive regions:\n- Let the set of interior motif boundaries be $$B = \\{s + m, s + 2m, \\dots, s + (r - 1)m\\},$$ which has cardinality $|B| = r - 1$ for $r \\geq 2$. Let $T$ be the set of token boundary positions inside the region, where each boundary is the starting index of any token after the first token within the region. Define the boundary misalignment error as $$e = \\begin{cases}1 - \\frac{|B \\cap T|}{|B|}, & \\text{if } |B| > 0, \\\\ 0, & \\text{if } |B| = 0. \\end{cases}$$ This measures the fraction of motif boundaries not captured by token boundaries.\n\nFor each test case, compute:\n- The BPE average bits per nucleotide $\\bar{\\ell}_{\\mathrm{BPE}}$.\n- The ULM average bits per nucleotide $\\bar{\\ell}_{\\mathrm{ULM}}$.\n- The boundary misalignment error $e_{\\mathrm{BPE}}$.\n- The boundary misalignment error $e_{\\mathrm{ULM}}$.\n\nUse the entire given sequence as both the training and the test sequence for both methods. All logarithms must be base $2$. For probability estimation, use additive smoothing with the specified $\\lambda$ and $\\alpha$ values.\n\nImplement the program to solve the following test suite, where indices are zero-based and the sequences are provided explicitly:\n\n- Test case $1$ (happy path): Sequence $S_1 =$ \"ACGTAC\" + \"ATG\" repeated $10$ times + \"GGAAC\". Parameters: $V_{\\mathrm{BPE}} = 12$, $L_{\\max} = 5$, $\\lambda = 1$, $\\alpha = 1$; repetitive region start $s = 6$, motif length $m = 3$, repeats $r = 10$.\n\n- Test case $2$ (boundary, homopolymer): Sequence $S_2 =$ \"CG\" + \"A\" repeated $20$ times + \"TGC\". Parameters: $V_{\\mathrm{BPE}} = 8$, $L_{\\max} = 10$, $\\lambda = 1$, $\\alpha = 0.5$; repetitive region start $s = 2$, motif length $m = 1$, repeats $r = 20$.\n\n- Test case $3$ (edge, sparse repetition): Sequence $S_3 =$ \"GCTATCGAGT\" + \"CGT\" repeated $3$ times + \"ACGTA\". Parameters: $V_{\\mathrm{BPE}} = 6$, $L_{\\max} = 4$, $\\lambda = 1$, $\\alpha = 1$; repetitive region start $s = 10$, motif length $m = 3$, repeats $r = 3$.\n\n- Test case $4$ (mixed GC-rich repeat): Sequence $S_4 =$ \"AT\" + \"GC\" repeated $12$ times + \"TAAC\". Parameters: $V_{\\mathrm{BPE}} = 10$, $L_{\\max} = 4$, $\\lambda = 1$, $\\alpha = 0.1$; repetitive region start $s = 2$, motif length $m = 2$, repeats $r = 12$.\n\nYour program must:\n- Implement both tokenizers exactly as described.\n- Compute the four quantities $(\\bar{\\ell}_{\\mathrm{BPE}}, \\bar{\\ell}_{\\mathrm{ULM}}, e_{\\mathrm{BPE}}, e_{\\mathrm{ULM}})$ for each test case.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case represented as a four-element list in the order $[\\bar{\\ell}_{\\mathrm{BPE}}, \\bar{\\ell}_{\\mathrm{ULM}}, e_{\\mathrm{BPE}}, e_{\\mathrm{ULM}}]$. Express all four values for each test case as decimals rounded to $6$ places. For example, the final output must look like $$[[x_1,y_1,z_1,w_1],[x_2,y_2,z_2,w_2],[x_3,y_3,z_3,w_3],[x_4,y_4,z_4,w_4]].$$", "solution": "The problem requires a comparative analysis of two tokenization algorithms, Byte-Pair Encoding (BPE) and Unigram Language Model (ULM), for their efficacy in compressing and segmenting genomic sequences, particularly around repetitive regions. This analysis will be performed by implementing both algorithms from first principles and evaluating them on a given set of DNA sequences using metrics derived from information theory and sequence analysis.\n\n### Foundational Principles\n\nThe core of the compression metric is Shannon's source coding theorem. For a set of tokens $W$ with a probability distribution $p(w)$, the theoretically optimal code length for a token $w$ is given by its self-information, $I(w) = -\\log_2 p(w)$. For a sequence of tokens $w_1, w_2, \\dots, w_T$, the total code length is the sum of the individual code lengths:\n$$L = \\sum_{t=1}^{T} -\\log_2 p(w_t)$$\nAveraging this over the total number of nucleotides $N$ in the original DNA sequence gives the average bits per nucleotide, $\\bar{\\ell} = L/N$, a measure of compression efficiency. Lower values indicate better compression.\n\nThe second metric, boundary misalignment error $e$, quantifies how well the token boundaries align with the natural boundaries of a repetitive motif. For a region with $r$ repeats of a motif of length $m$ starting at index $s$, the internal motif boundaries are at positions $B = \\{s+m, s+2m, \\dots, s+(r-1)m\\}$. Given a tokenization that produces token boundaries $T$ within the same region, the error is the fraction of motif boundaries that are missed by the tokenizer:\n$$e = 1 - \\frac{|B \\cap T|}{|B|}$$\nfor $|B| > 0$. An error of $e=0$ indicates perfect alignment.\n\n### Method 1: Byte-Pair Encoding (BPE)\n\nBPE is a greedy data compression algorithm that iteratively replaces the most frequent pair of adjacent symbols (tokens) with a new symbol.\n\n**1. Training and Tokenization:**\nThe process begins with the DNA sequence represented as a sequence of its constituent nucleotides (characters), which form the initial vocabulary $V = \\{'A', 'C', 'G', 'T'\\}$. The algorithm then performs a specified number of merge operations ($V_{\\mathrm{BPE}} - |V_{\\text{initial}}|$). In each step:\na. All adjacent pairs of tokens in the current sequence representation are counted.\nb. The pair with the highest frequency is selected for merging. Ties in frequency are broken by selecting the pair $(t_1, t_2)$ for which the concatenated string $t_1+t_2$ is lexicographically smallest.\nc. A new token is created representing this pair, and the new token is added to the vocabulary.\nd. All occurrences of the selected pair in the sequence are replaced by the new token. This greedy replacement is performed from left to right.\n\nSince the problem specifies that the training and test sequences are identical, the final token sequence resulting from the training process serves as the segmentation for both probability estimation and metric calculation.\n\n**2. Probability Estimation and Code Length:**\nAfter the final tokenization $\\{w_1, w_2, \\dots, w_T\\}$ is obtained, the count $c(w)$ for each unique token $w$ in the vocabulary is determined. The probability $p_{\\mathrm{BPE}}(w)$ is estimated using additive (Laplace) smoothing with parameter $\\lambda$:\n$$p_{\\mathrm{BPE}}(w) = \\frac{c(w) + \\lambda}{\\sum_{u \\in V} c(u) + \\lambda \\cdot |V|}$$\nwhere $|V| = V_{\\mathrm{BPE}}$ is the final vocabulary size, and the sum $\\sum_{u \\in V} c(u)$ is the total number of tokens in the final segmentation. The total code length $L_{\\mathrm{BPE}}$ is then calculated as $\\sum_{t=1}^{T} -\\log_2 p_{\\mathrm{BPE}}(w_t)$. The average bits per nucleotide is $\\bar{\\ell}_{\\mathrm{BPE}} = L_{\\mathrm{BPE}} / N$.\n\n### Method 2: Unigram Language Model (ULM)\n\nULM tokenization frames the problem as finding the most likely segmentation of a sequence, where the likelihood is based on a pre-computed probabilistic model of tokens.\n\n**1. Vocabulary and Probability Estimation:**\nFirst, a candidate vocabulary $V_{\\mathrm{ULM}}$ is constructed. It consists of all substrings of the training sequence with lengths from $1$ up to a maximum $L_{\\max}$. The frequency $c(w)$ of each substring $w \\in V_{\\mathrm{ULM}}$ is counted (with overlaps). The probability $p_{\\mathrm{ULM}}(w)$ is then estimated using additive smoothing with parameter $\\alpha$:\n$$p_{\\mathrm{ULM}}(w) = \\frac{c(w) + \\alpha}{\\sum_{u \\in V_{\\mathrm{ULM}}} c(u) + \\alpha \\cdot |V_{\\mathrm{ULM}}|}$$\nFor computational convenience, we work with the negative log-probabilities, which represent the \"cost\" of each token: $\\text{cost}(w) = -\\log_2 p_{\\mathrm{ULM}}(w)$.\n\n**2. Optimal Segmentation via Dynamic Programming:**\nThe optimal segmentation for the test sequence (which is the same as the training sequence) is the one that minimizes the total cost. This is a classic shortest path problem on a directed acyclic graph (DAG), which can be solved efficiently using dynamic programming (Viterbi algorithm).\nLet $dp[i]$ be the minimum cost to segment the prefix of the sequence of length $i$, $S[0 \\dots i-1]$. The recurrence relation is:\n$$dp[i] = \\min_{1 \\le j \\le \\min(i, L_{\\max})} \\left( dp[i-j] + \\text{cost}(S[i-j:i]) \\right)$$\nwith the base case $dp[0] = 0$. The minimum cost for the entire sequence of length $N$ is $dp[N]$, which is precisely the total code length $L_{\\mathrm{ULM}}$. The average bits per nucleotide is $\\bar{\\ell}_{\\mathrm{ULM}} = L_{\\mathrm{ULM}} / N$.\nBy storing backpointers during the DP process, i.e., which $j$ yielded the minimum at each step $i$, the optimal sequence of tokens can be reconstructed.\n\n### Metric Computation\n\nFor both BPE and ULM, once the final tokenization and corresponding start indices are determined, the boundary misalignment error $e$ is calculated.\n\n1.  **Determine True Boundaries ($B$)**: For a repetitive region defined by $(s, m, r)$, the set of internal motif boundaries is $B = \\{s + k \\cdot m \\mid k = 1, 2, \\dots, r-1\\}$. If $r < 2$, $B$ is empty.\n2.  **Determine Token Boundaries ($T$)**:\n    a. Let the start indices of the tokens in the full sequence segmentation be $p_1, p_2, \\dots, p_K$.\n    b. Find the first token that overlaps with the specified repetitive region $[s, s + r \\cdot m)$. Let this token's index in the segmentation be $j$.\n    c. The set of subsequent token start indices is $\\{p_{j+1}, p_{j+2}, \\dots, p_K\\}$.\n    d. The set of token boundaries within the region, $T$, is formed by taking these subsequent start indices that fall within the region's span: $T = \\{ p_k \\mid k > j \\text{ and } s \\le p_k < s+r \\cdot m \\}$.\n3.  **Calculate Error ($e$)**: The error is computed as $e = 1 - \\frac{|B \\cap T|}{|B|}$, or $0$ if $|B|=0$.\n\nThis structured approach allows for a direct and quantitative comparison of the two tokenization strategies on the provided test cases.", "answer": "```python\nimport numpy as np\nimport collections\nfrom math import log2\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and produce the final output.\n    \"\"\"\n\n    def get_bpe_pairs(tokens):\n        \"\"\"Counts adjacent token pairs.\"\"\"\n        return collections.Counter(zip(tokens, tokens[1:]))\n\n    def merge_bpe_tokens(tokens, pair, new_token):\n        \"\"\"Merges a specific pair in a token list.\"\"\"\n        new_tokens = []\n        i = 0\n        while i < len(tokens):\n            if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == pair:\n                new_tokens.append(new_token)\n                i += 2\n            else:\n                new_tokens.append(tokens[i])\n                i += 1\n        return new_tokens\n\n    def bpe_trainer(sequence, vocab_size):\n        \"\"\"Trains BPE and returns the final tokenization and merges.\"\"\"\n        if not sequence:\n            return [], []\n        \n        tokens = list(sequence)\n        initial_vocab_size = len(set(tokens))\n        num_merges = vocab_size - initial_vocab_size\n        \n        merges = []\n        if num_merges <= 0:\n            return tokens, merges\n\n        for _ in range(num_merges):\n            pairs = get_bpe_pairs(tokens)\n            if not pairs:\n                break\n            \n            max_freq = max(pairs.values())\n            # Tie-breaking with lexicographical order\n            best_pairs_candidates = [p for p, freq in pairs.items() if freq == max_freq]\n            best_pairs_candidates.sort(key=lambda p: p[0] + p[1])\n            best_pair = best_pairs_candidates[0]\n            \n            new_token = best_pair[0] + best_pair[1]\n            tokens = merge_bpe_tokens(tokens, best_pair, new_token)\n            merges.append((best_pair, new_token))\n        \n        return tokens, merges\n\n    def ulm_trainer(sequence, l_max):\n        \"\"\"Builds ULM vocabulary and probabilities from substrings.\"\"\"\n        if not sequence:\n            return {}, {}\n            \n        vocab_counts = collections.Counter()\n        total_substrings = 0\n        for length in range(1, l_max + 1):\n            for i in range(len(sequence) - length + 1):\n                substring = sequence[i:i+length]\n                vocab_counts[substring] += 1\n                total_substrings += 1\n        return vocab_counts, total_substrings\n\n    def ulm_tokenizer(sequence, probs, vocab, l_max):\n        \"\"\"Segments a sequence using ULM via dynamic programming.\"\"\"\n        n = len(sequence)\n        costs = {token: -log2(p) for token, p in probs.items()}\n        \n        dp = [np.inf] * (n + 1)\n        backpointers = [-1] * (n + 1)\n        dp[0] = 0\n        \n        for i in range(1, n + 1):\n            for j in range(1, min(i, l_max) + 1):\n                sub = sequence[i-j:i]\n                if sub in costs:\n                    cost = dp[i-j] + costs[sub]\n                    if cost < dp[i]:\n                        dp[i] = cost\n                        backpointers[i] = i-j\n\n        if np.isinf(dp[n]):\n             return [], [], np.inf\n        \n        tokens = []\n        indices = []\n        end = n\n        while end > 0:\n            start = backpointers[end]\n            tokens.append(sequence[start:end])\n            indices.append(start)\n            end = start\n        \n        tokens.reverse()\n        indices.reverse()\n        return tokens, indices, dp[n]\n\n    def calculate_error(tokenization, start_indices, s, m, r, seq_len):\n        \"\"\"Calculates the boundary misalignment error.\"\"\"\n        if r < 2:\n            return 0.0\n        \n        motif_boundaries = {s + k * m for k in range(1, r)}\n        \n        first_token_idx_in_seg = -1\n        region_start, region_end = s, s + r * m\n        \n        for i, token_start in enumerate(start_indices):\n            token_len = len(tokenization[i])\n            if token_start < region_end and token_start + token_len > region_start:\n                first_token_idx_in_seg = i\n                break\n        \n        token_boundaries = set()\n        if first_token_idx_in_seg != -1:\n            for i in range(first_token_idx_in_seg + 1, len(start_indices)):\n                token_start = start_indices[i]\n                if region_start <= token_start < region_end:\n                    token_boundaries.add(token_start)\n                    \n        intersection_size = len(motif_boundaries.intersection(token_boundaries))\n        \n        return 1.0 - (intersection_size / len(motif_boundaries))\n\n\n    test_cases = [\n        {\"seq\": \"ACGTAC\" + \"ATG\" * 10 + \"GGAAC\", \"V_BPE\": 12, \"L_max\": 5, \"lambda\": 1, \"alpha\": 1, \"s\": 6, \"m\": 3, \"r\": 10},\n        {\"seq\": \"CG\" + \"A\" * 20 + \"TGC\", \"V_BPE\": 8, \"L_max\": 10, \"lambda\": 1, \"alpha\": 0.5, \"s\": 2, \"m\": 1, \"r\": 20},\n        {\"seq\": \"GCTATCGAGT\" + \"CGT\" * 3 + \"ACGTA\", \"V_BPE\": 6, \"L_max\": 4, \"lambda\": 1, \"alpha\": 1, \"s\": 10, \"m\": 3, \"r\": 3},\n        {\"seq\": \"AT\" + \"GC\" * 12 + \"TAAC\", \"V_BPE\": 10, \"L_max\": 4, \"lambda\": 1, \"alpha\": 0.1, \"s\": 2, \"m\": 2, \"r\": 12},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        seq = case[\"seq\"]\n        n = len(seq)\n        v_bpe, l_max, lam, alpha = case[\"V_BPE\"], case[\"L_max\"], case[\"lambda\"], case[\"alpha\"]\n        s, m, r = case[\"s\"], case[\"m\"], case[\"r\"]\n\n        # --- BPE ---\n        bpe_tokens, _ = bpe_trainer(seq, v_bpe)\n        bpe_vocab = set(bpe_tokens) | set(list('ACGT')) # Ensure base vocab is included\n        \n        # In case merging reduces vocab size below V_BPE, use actual final size.\n        final_bpe_vocab_size = len(set(bpe_trainer(seq, v_bpe)[0]))\n        \n        bpe_counts = collections.Counter(bpe_tokens)\n        total_bpe_tokens = len(bpe_tokens)\n        \n        bpe_probs = {\n            token: (bpe_counts.get(token, 0) + lam) / (total_bpe_tokens + lam * final_bpe_vocab_size)\n            for token in set(bpe_tokens)\n        }\n        \n        l_bpe = sum(-log2(bpe_probs[token]) for token in bpe_tokens)\n        l_bar_bpe = l_bpe / n if n > 0 else 0\n        \n        bpe_start_indices = [0]\n        pos = 0\n        for token in bpe_tokens[:-1]:\n            pos += len(token)\n            bpe_start_indices.append(pos)\n        \n        e_bpe = calculate_error(bpe_tokens, bpe_start_indices, s, m, r, n)\n\n        # --- ULM ---\n        ulm_vocab_counts, total_substrings = ulm_trainer(seq, l_max)\n        ulm_vocab_size = len(ulm_vocab_counts)\n        denominator = total_substrings + alpha * ulm_vocab_size\n        ulm_probs = {\n            token: (count + alpha) / denominator\n            for token, count in ulm_vocab_counts.items()\n        }\n        \n        ulm_tokens, ulm_indices, l_ulm = ulm_tokenizer(seq, ulm_probs, ulm_vocab_counts, l_max)\n        l_bar_ulm = l_ulm / n if n > 0 else 0\n        \n        e_ulm = calculate_error(ulm_tokens, ulm_indices, s, m, r, n)\n        \n        # Round and append results\n        all_results.append([\n            round(l_bar_bpe, 6),\n            round(l_bar_ulm, 6),\n            round(e_bpe, 6),\n            round(e_ulm, 6)\n        ])\n\n    # Format final output string\n    result_str = \",\".join([str(res) for res in all_results])\n    print(f\"[{result_str.replace(' ', '')}]\")\n\nsolve()\n\n```", "id": "4606969"}, {"introduction": "While powerful, deep learning models are often considered \"black boxes,\" making it difficult to understand the basis for their predictions. In genomics, interpretability is essential for validating that a model has learned meaningful biological signals rather than spurious correlations. In this exercise, you will implement the Integrated Gradients attribution method from first principles to trace a model's prediction for a masked nucleotide back to the specific input bases that influenced it, allowing you to verify if the model's attention aligns with known sequence motifs [@problem_id:4606960].", "problem": "You are given a simplified masked token prediction setting for deoxyribonucleic acid (DNA) sequences, where a sequence of length $L$ has one masked position at index $m$ (zero-based). Consider a linear-softmax model that predicts the masked token (the base at index $m$) from the remaining context. The model operates on a one-hot encoding of the input sequence with the masked position set to the zero vector. Let the input be represented as a vector $x \\in \\mathbb{R}^{4L}$ obtained by concatenating one-hot vectors in the fixed order of bases $\\{A,C,G,T\\}$ at each position. The model defines class logits $z_k(x) = w_k^\\top x$ for $k \\in \\{0,1,2,3\\}$ corresponding to classes $\\{A,C,G,T\\}$, and predicted probabilities $p_k(x) = \\exp(z_k(x)) / \\sum_{j=0}^{3} \\exp(z_j(x))$. The weights are constructed so that the true class $k=0$ corresponding to base $A$ responds strongly to a known biological motif upstream of the mask and weakly elsewhere.\n\nYou must compute Integrated Gradients (IG) attributions of the model’s predicted masked token class to the input bases and evaluate whether attributions align with the known motif. Use the following definitions and constraints.\n\n1. One-hot encoding and baseline:\n   - For each position $j \\in \\{0,\\dots,L-1\\}$ and base $b \\in \\{A,C,G,T\\}$, let $x_{j,b} \\in \\{0,1\\}$ denote the corresponding one-hot component, except that for the masked index $j=m$, set $x_{m,b} = 0$ for all $b$ (mask).\n   - The baseline is $x' = \\mathbf{0} \\in \\mathbb{R}^{4L}$ and the straight-line path is $\\gamma(\\alpha) = x' + \\alpha (x-x') = \\alpha x$ for $\\alpha \\in [0,1]$.\n\n2. Model construction:\n   - Fix a motif of length $4$ immediately upstream of the mask at relative offsets $r \\in \\{-4,-3,-2,-1\\}$ with bases $\\{T, A, T, A\\}$ respectively. For any position $j = m + r$ that is within bounds $0 \\le j < L$, and for base $b$ equal to the motif base at that offset, assign a large weight $w$ to the corresponding feature in the weight vector for class $A$ (class index $0$). For all other features across all classes, assign a small positive background weight $\\epsilon$. Formally, let $M \\in \\mathbb{R}^{4 \\times 4L}$ be the weight matrix with rows $M_k^\\top = w_k^\\top$:\n     - For class $k=0$ (base $A$), if $j=m+r$ is valid and $b$ equals the specified motif base at offset $r$, set $M_{0, (4j + \\text{idx}(b))} = w$; otherwise set $M_{0, (4j + \\text{idx}(b))} = \\epsilon$.\n     - For classes $k \\in \\{1,2,3\\}$ (bases $C,G,T$), set $M_{k, i} = \\epsilon$ for all features $i$.\n   - Use the base index mapping $\\text{idx}(A)=0$, $\\text{idx}(C)=1$, $\\text{idx}(G)=2$, $\\text{idx}(T)=3$.\n\n3. Prediction target for attribution:\n   - Let $p(x) = \\text{softmax}(M x)$. Determine the predicted class $c^\\star = \\arg\\max_{k \\in \\{0,1,2,3\\}} p_k(x)$. Compute IG with respect to the scalar function $F(x) = p_{c^\\star}(x)$.\n\n4. Integrated Gradients definition:\n   - For each feature $i \\in \\{1,\\dots,4L\\}$, the Integrated Gradient is defined by\n     $$\\mathrm{IG}_i(x) = (x_i - x_i') \\int_{0}^{1} \\frac{\\partial F(\\gamma(\\alpha))}{\\partial x_i} \\, d\\alpha,$$\n     and you must use the Riemann sum approximation with $S$ steps:\n     $$\\widehat{\\mathrm{IG}}_i(x) = (x_i - x_i') \\cdot \\frac{1}{S} \\sum_{s=1}^{S} \\left. \\frac{\\partial F(\\gamma(\\alpha))}{\\partial x_i} \\right|_{\\alpha = s/S}.$$\n\n5. Position-wise attribution and motif alignment score:\n   - Aggregate feature-level attributions to position-level by summing absolute values across the four channels for the position, i.e., for position $j$,\n     $$A_j = \\sum_{b \\in \\{A,C,G,T\\}} \\left| \\widehat{\\mathrm{IG}}_{4j + \\text{idx}(b)}(x) \\right|.$$\n   - Exclude the masked position from the total since its one-hot is zero, and compute the total attribution\n     $$T = \\sum_{\\substack{j=0 \\\\ j \\ne m}}^{L-1} A_j.$$\n   - Given a list of annotated motif intervals $[s, e)$ (start inclusive, end exclusive) in absolute positions, define the alignment score as the fraction of total attribution within the union of the intervals:\n     $$\\text{score} = \\frac{\\sum_{[s,e)} \\sum_{j=s}^{e-1} A_j}{T}.$$\n\n6. Mathematical and algorithmic bases to use:\n   - You must start from core definitions: one-hot encodings, linear maps, the softmax function, and the definition of Integrated Gradients as above. Do not use any external machine learning packages or automatic differentiation; compute gradients analytically from these definitions.\n\n7. Required numerical values:\n   - Use $w = 3.0$ and $\\epsilon = 0.05$.\n   - Use the base ordering $\\{A,C,G,T\\}$ and the index mapping as specified.\n   - For each case, use the provided $S$ for the Riemann sum steps.\n\n8. Test suite:\n   Implement the program to process the following cases, each defined by the tuple $(\\text{sequence}, m, \\text{intervals}, S)$ where the sequence is length $L=20$:\n   - Case 1 (clear motif upstream):\n     - Sequence: \"GCCGACTATANGTCCAAGTT\" where the character at index $m$ is 'N' to denote masked input.\n     - Mask index $m = 10$.\n     - Annotated intervals: $[(6,10)]$.\n     - Steps $S = 50$.\n   - Case 2 (two motif occurrences, only the proximal one is annotated; boundary in discretization with minimal steps):\n     - Sequence: \"TATAGCTATANGCGTCAAGT\".\n     - Mask index $m = 10$.\n     - Annotated intervals: $[(6,10)]$.\n     - Steps $S = 1$.\n   - Case 3 (no proximal motif):\n     - Sequence: \"GACGTCGCGCNATGCATAGC\".\n     - Mask index $m = 10$.\n     - Annotated intervals: $[(6,10)]$.\n     - Steps $S = 50$.\n\n9. Program output:\n   - Your program should produce a single line of output containing the alignment scores for the three cases as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,r3]\"). Each $r_i$ must be a floating-point number.\n\nNo physical units, angles, or percentages are required in this problem. All mathematical computations must conform to the definitions above, and all outputs must be numerical floats adhering to the specified format.", "solution": "The problem requires the computation of Integrated Gradients (IG) for a linear-softmax model on DNA sequences and the evaluation of an alignment score based on the resulting attributions. The solution proceeds in five stages: formalizing the model and inputs, deriving the analytical gradient required for IG, specifying the IG calculation, defining the attribution aggregation, and analyzing each test case.\n\n**1. Formalization of the Model and Input**\nThe input is a DNA sequence of length $L=20$ which is converted into a one-hot vector $x \\in \\mathbb{R}^{80}$. For a base $b$ at a non-masked position $j$, the component $x_{4j + \\text{idx}(b)} = 1$, and all other components for that position are $0$. At the masked position $m$, all four components are $0$.\n\nThe model is defined by the weight matrix $M \\in \\mathbb{R}^{4 \\times 80}$. The problem specifies its construction:\n- For rows $k \\in \\{1,2,3\\}$, corresponding to classes 'C', 'G', 'T', all weights are $\\epsilon = 0.05$. So, $M_{k,i} = \\epsilon$ for $k>0$.\n- For row $k=0$, corresponding to class 'A', the weights are $w=3.0$ for features that match the motif $\\{T,A,T,A\\}$ at positions $\\{m-4, m-3, m-2, m-1\\}$. For all other features in this row, the weight is $\\epsilon$. Let $I_{motif}$ be the set of feature indices corresponding to this motif. Then $M_{0,i} = w$ if $i \\in I_{motif}$ and $M_{0,i} = \\epsilon$ if $i \\notin I_{motif}$.\n\nThe logits are $z = Mx$ and probabilities are $p = \\text{softmax}(z)$.\n\n**2. Analytical Gradient Derivation**\nThe core of the IG calculation is the gradient of the target function $F(y) = p_{c^\\star}(y)$ with respect to its input vector $y$. Here $y$ will be the interpolated input $\\gamma(\\alpha)$. The predicted class $c^\\star = \\arg\\max_k p_k(x)$ is determined from the original input $x$ and is fixed throughout the IG calculation.\n\nUsing the chain rule, the derivative of a probability $p_k$ with respect to an input feature $y_i$ is:\n$$ \\frac{\\partial p_k(y)}{\\partial y_i} = \\sum_{l=0}^{3} \\frac{\\partial p_k(y)}{\\partial z_l(y)} \\frac{\\partial z_l(y)}{\\partial y_i} $$\nThe Jacobian of the softmax function is $\\frac{\\partial p_k}{\\partial z_l} = p_k (\\delta_{kl} - p_l)$, where $\\delta_{kl}$ is the Kronecker delta. The derivative of the linear layer is $\\frac{\\partial z_l}{\\partial y_i} = M_{li}$.\nSubstituting these gives:\n$$ \\frac{\\partial p_k(y)}{\\partial y_i} = \\sum_{l=0}^{3} p_k(y)(\\delta_{kl} - p_l(y)) M_{li} = p_k(y) \\left( M_{ki} - \\sum_{l=0}^{3} p_l(y) M_{li} \\right) $$\nLet's analyze the term $E_i(y) = \\sum_{l=0}^{3} p_l(y) M_{li}$. This is the $i$-th component of the vector $p(y)^\\top M$. Given our specific weight matrix $M$, where $M_{li} = \\epsilon$ for $l>0$, we can simplify $E_i(y)$:\n$$ E_i(y) = p_0(y)M_{0i} + p_1(y)M_{1i} + p_2(y)M_{2i} + p_3(y)M_{3i} = p_0(y)M_{0i} + (p_1(y)+p_2(y)+p_3(y))\\epsilon $$\nSince $\\sum_l p_l(y) = 1$, we have $p_1+p_2+p_3 = 1-p_0$.\n$$ E_i(y) = p_0(y)M_{0i} + (1-p_0(y))\\epsilon $$\nWe are interested in the gradient for the class $c^\\star$. If $c^\\star=0$, the gradient is:\n$$ \\frac{\\partial p_0(y)}{\\partial y_i} = p_0(y) (M_{0i} - E_i(y)) = p_0(y)(M_{0i} - [p_0(y)M_{0i} + (1-p_0(y))\\epsilon]) $$\n$$ \\frac{\\partial p_0(y)}{\\partial y_i} = p_0(y)(M_{0i}(1-p_0(y)) - \\epsilon(1-p_0(y))) = p_0(y)(1-p_0(y))(M_{0i} - \\epsilon) $$\nThis is a critical simplification. The gradient for class 'A' with respect to feature $i$ is proportional to $(M_{0i} - \\epsilon)$.\nIf feature $i$ is not part of the 'A'-class motif, $M_{0i}=\\epsilon$, and the gradient is zero.\nIf feature $i$ is part of the motif, $M_{0i}=w$, and the gradient is $p_0(y)(1-p_0(y))(w - \\epsilon)$.\n\n**3. Integrated Gradients Calculation**\nThe Riemann sum approximation for IG is given as:\n$$ \\widehat{\\mathrm{IG}}_i(x) = x_i \\cdot \\frac{1}{S} \\sum_{s=1}^{S} \\left. \\frac{\\partial p_{c^\\star}(y)}{\\partial y_i} \\right|_{y = \\gamma(\\alpha_s)} \\quad \\text{where } \\alpha_s = s/S $$\nThe gradient term is evaluated at the interpolated input $y = \\gamma(\\alpha_s) = \\alpha_s x$. The logits at this point are $z(\\alpha_s) = M(\\alpha_s x) = \\alpha_s z$, where $z=Mx$ are the logits for the original input. The probabilities are $p(\\alpha_s) = \\text{softmax}(\\alpha_s z)$.\n\n**4. Attribution Aggregation and Scoring**\nFeature-level attributions $\\widehat{\\mathrm{IG}}_i$ are aggregated to position-level attributions $A_j$ by summing the absolute values of the four feature attributions at each position $j$:\n$$ A_j = \\sum_{b=0}^{3} |\\widehat{\\mathrm{IG}}_{4j+b}| $$\nThe total attribution $T$ is the sum of $A_j$ over all non-masked positions. The final alignment score is the fraction of total attribution that falls within the annotated motif intervals.\n\n**5. Analysis of Test Cases**\n\n- **Case 1: `seq=\"GCCGACTATANGTCCAAGTT\"`, `m=10`, `intervals=[(6,10)]`, `S=50`**\n  The motif positions relative to the mask $m=10$ are $\\{6,7,8,9\\}$. The sequence has `TATA` at these positions, a perfect match to the motif $\\{T,A,T,A\\}$ the model's 'A'-class weights are sensitive to.\n  The input vector $x$ will have $x_{4j+\\text{idx}(b)}=1$ for the bases in the sequence. Specifically, for the motif positions, we have $x_{27}=1$ (`T` at 6), $x_{28}=1$ (`A` at 7), $x_{35}=1$ (`T` at 8), $x_{36}=1$ (`A` at 9).\n  The logits are $z=Mx$. For $k=0$, $z_0 = 4w + (19-4)\\epsilon = 4(3.0) + 15(0.05) = 12.75$. For $k>0$, $z_k=19\\epsilon=0.95$.\n  Clearly, $z_0$ is maximal, so the predicted class is $c^\\star=0$.\n  We use the gradient formula for $p_0$. The gradient $\\frac{\\partial p_0}{\\partial y_i}$ is non-zero only if $M_{0i} \\ne \\epsilon$, which occurs only for the four motif features $i \\in I_{motif}$.\n  The IG attribution $\\widehat{\\mathrm{IG}}_i$ is non-zero only if $x_i=1$ AND the gradient is non-zero. Both conditions are met for the four motif features, as the sequence matches the motif. For all other features, either the gradient is zero or $x_i$ is zero.\n  Therefore, attribution is non-zero only for features corresponding to positions $\\{6,7,8,9\\}$. Position-wise attributions $A_j$ are non-zero only for $j \\in \\{6,7,8,9\\}$.\n  The total attribution is $T = A_6+A_7+A_8+A_9$.\n  The annotated interval is $[6,10)$, covering positions $\\{6,7,8,9\\}$. The attribution within this interval is also $A_6+A_7+A_8+A_9$.\n  The score is $(A_6+A_7+A_8+A_9) / (A_6+A_7+A_8+A_9) = 1.0$.\n\n- **Case 2: `seq=\"TATAGCTATANGCGTCAAGT\"`, `m=10`, `intervals=[(6,10)]`, `S=1`**\n  The sequence again has a perfect `TATA` motif at positions $\\{6,7,8,9\\}$. The analysis is identical to Case 1. The predicted class is $c^\\star=0$. All attribution is confined to positions $\\{6,7,8,9\\}$. The annotated interval is $[6,10)$. The score is therefore $1.0$. The number of steps $S$ affects the magnitude of the attribution values, but not their ratio in this highly structured problem.\n\n- **Case 3: `seq=\"GACGTCGCGCNATGCATAGC\"`, `m=10`, `intervals=[(6,10)]`, `S=50`**\n  The sequence at the motif-sensitive positions $\\{6,7,8,9\\}$ is `GCGC`. This does not match the `TATA` motif.\n  When calculating the logits $z=Mx$, no input feature $x_i=1$ aligns with a special weight $M_{0,i}=w$.\n  Therefore, for all $k \\in \\{0,1,2,3\\}$, the logit is a sum of $19$ terms of $\\epsilon \\cdot 1$, so $z_k=19\\epsilon=0.95$.\n  All logits are equal. The probabilities are $p_k=0.25$ for all $k$. By convention (e.g., `numpy.argmax`), the predicted class is the first one, $c^\\star=0$.\n  We again compute IG for $p_0$. The gradient formula $\\frac{\\partial p_0}{\\partial y_i} = p_0(y)(1-p_0(y))(M_{0i} - \\epsilon)$ is still valid.\n  The attribution $\\widehat{\\mathrm{IG}}_i$ requires both $x_i=1$ and $M_{0i} \\ne \\epsilon$. The set of features where $M_{0i}=w$ is disjoint from the set of features where $x_i=1$. For any feature $i$ activated in the input ($x_i=1$), the corresponding weight is $M_{0i}=\\epsilon$, making the gradient term $(M_{0i}-\\epsilon)$ zero.\n  Consequently, $\\widehat{\\mathrm{IG}}_i(x) = 0$ for all $i$. All position-wise attributions $A_j$ are zero. The total attribution $T=0$.\n  The score is $\\frac{0}{0}$, which is logically interpreted as $0.0$, since zero attribution falls within the motif region.", "answer": "```python\nimport numpy as np\nfrom scipy.special import softmax\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    \n    # Test cases as defined in the problem statement\n    test_cases = [\n        # (sequence, mask_index, annotated_intervals, num_steps)\n        (\"GCCGACTATANGTCCAAGTT\", 10, [(6, 10)], 50),\n        (\"TATAGCTATANGCGTCAAGT\", 10, [(6, 10)], 1),\n        (\"GACGTCGCGCNATGCATAGC\", 10, [(6, 10)], 50),\n    ]\n\n    results = []\n    for case in test_cases:\n        score = calculate_score(*case)\n        results.append(score)\n\n    # Format the output as a comma-separated list in brackets\n    formatted_results = f\"[{','.join(f'{r:.7f}' for r in results)}]\"\n    print(formatted_results)\n\ndef calculate_score(sequence, m, intervals, S):\n    \"\"\"\n    Calculates the motif alignment score for a single test case.\n    \"\"\"\n    # 1. Define constants and mappings\n    L = 20\n    w = 3.0\n    epsilon = 0.05\n    char_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    num_features = 4 * L\n    \n    # 2. Build one-hot encoded input vector x\n    x = np.zeros(num_features)\n    for j, char in enumerate(sequence):\n        if j != m and char in char_to_idx:\n            base_idx = char_to_idx[char]\n            x[4 * j + base_idx] = 1\n            \n    # 3. Build weight matrix M\n    M = np.full((4, num_features), epsilon)\n    motif_bases = {'T': -4, 'A': -3, 'T': -2, 'A': -1}\n    for base_char, offset in motif_bases.items():\n        pos = m + offset\n        if 0 <= pos < L:\n            base_idx = char_to_idx[base_char]\n            feature_idx = 4 * pos + base_idx\n            M[0, feature_idx] = w  # Class 0 is for 'A'\n            \n    # 4. Determine the predicted class c_star for the input x\n    z = M @ x\n    p = softmax(z)\n    c_star = np.argmax(p)\n    \n    # 5. Compute Integrated Gradients using Riemann sum\n    sum_of_grads = np.zeros(num_features)\n    for s in range(1, S + 1):\n        alpha = s / S\n        \n        # Calculate logits and probabilities for the interpolated input\n        z_alpha = alpha * z\n        p_alpha = softmax(z_alpha)\n        \n        # Calculate the gradient of the predicted class probability w.r.t. the interpolated input\n        # This is the vector form of the analytical gradient:\n        # grad_i = p_alpha[c_star] * (M[c_star, i] - sum_l(p_alpha[l] * M[l, i]))\n        expected_weights = p_alpha @ M\n        grad_at_alpha = p_alpha[c_star] * (M[c_star, :] - expected_weights)\n        \n        sum_of_grads += grad_at_alpha\n\n    # Final IG is the average gradient multiplied by the input feature value\n    ig = x * (sum_of_grads / S)\n    \n    # 6. Aggregate feature attributions to position-wise attributions A_j\n    # Reshape to (L, 4) and sum absolute values over the base channels\n    A = np.sum(np.abs(ig.reshape((L, 4))), axis=1)\n\n    # 7. Compute the final alignment score\n    total_attribution = 0\n    motif_attribution = 0\n    \n    # Create a set of indices for annotated regions for efficient lookup\n    annotated_indices = set()\n    for start, end in intervals:\n        annotated_indices.update(range(start, end))\n\n    for j in range(L):\n        if j != m: # Exclude masked position from total attribution\n            total_attribution += A[j]\n            if j in annotated_indices:\n                motif_attribution += A[j]\n                \n    if total_attribution == 0:\n        score = 0.0\n    else:\n        score = motif_attribution / total_attribution\n        \n    return score\n\nif __name__ == '__main__':\n    solve()\n```", "id": "4606960"}]}