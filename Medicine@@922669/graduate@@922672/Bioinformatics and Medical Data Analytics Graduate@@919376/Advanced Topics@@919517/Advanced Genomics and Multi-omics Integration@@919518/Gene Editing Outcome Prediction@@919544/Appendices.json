{"hands_on_practices": [{"introduction": "The ultimate goal of many gene editing applications is to control protein function, which begins with understanding how edits affect the coding sequence. This first exercise provides a foundational calculation, translating an empirically observed distribution of deletion lengths into the probability of a frameshift mutation [@problem_id:4566205]. Mastering this helps connect the raw output of a sequencing experiment to its most critical biological consequence.", "problem": "In Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) gene editing outcome prediction, the impact of an insertion or deletion event on the protein coding frame is governed by the triplet codon structure derived from the Central Dogma of molecular biology, where amino acids are encoded by trinucleotide codons. A deletion of length $L$ nucleotides in a coding exon yields a frameshift if and only if the reading frame is altered, which occurs when $L \\bmod 3 \\neq 0$. Consider a single edited allele drawn at random from a large population of Next-Generation Sequencing (NGS)-profiled outcomes. Let $L$ denote the deletion length in nucleotides, with empirically observed probabilities $P(L=1)=0.3$, $P(L=2)=0.2$, $P(L=3)=0.1$, and assume the distribution is uniform for $L \\in \\{4,5,6\\}$. Using the law of total probability and the codon-size rule for frameshifts, compute the expected frameshift probability $P(\\text{frameshift})=\\mathbb{P}(L \\bmod 3 \\neq 0)$ for this target. Express your final answer as a rational number in simplest terms. Do not use a percentage sign.", "solution": "The problem asks for the expected frameshift probability, denoted as $P(\\text{frameshift})$, for a deletion event in a CRISPR-edited allele population. The random variable $L$ represents the length of the deletion in nucleotides.\n\nFirst, we must fully define the probability distribution of $L$. The problem provides specific probabilities for lengths $L=1$, $L=2$, and $L=3$:\n$$P(L=1) = 0.3$$\n$$P(L=2) = 0.2$$\n$$P(L=3) = 0.1$$\n\nThe problem also states that the distribution is uniform for $L \\in \\{4, 5, 6\\}$. The total probability over the sample space of a random variable must sum to $1$. Let's assume the sample space is limited to the values for which probabilities are given or described, i.e., $S = \\{1, 2, 3, 4, 5, 6\\}$.\n\nThe probability mass already accounted for by $L=1, 2, 3$ is:\n$$P(L=1) + P(L=2) + P(L=3) = 0.3 + 0.2 + 0.1 = 0.6$$\n\nThe remaining probability mass must be distributed among the other possible outcomes. According to the problem statement, this remaining mass corresponds to the events $L=4$, $L=5$, and $L=6$.\n$$P(L \\in \\{4, 5, 6\\}) = 1 - 0.6 = 0.4$$\n\nSince the distribution is uniform for $L \\in \\{4, 5, 6\\}$, the probability $0.4$ is divided equally among these three outcomes:\n$$P(L=4) = P(L=5) = P(L=6) = \\frac{0.4}{3}$$\n\nTo work with exact rational numbers as requested for the final answer, we convert all probabilities to fractions:\n$$P(L=1) = \\frac{3}{10}$$\n$$P(L=2) = \\frac{2}{10} = \\frac{1}{5}$$\n$$P(L=3) = \\frac{1}{10}$$\n$$P(L=4) = P(L=5) = P(L=6) = \\frac{0.4}{3} = \\frac{4/10}{3} = \\frac{4}{30} = \\frac{2}{15}$$\n\nLet's verify that the total probability is $1$:\n$$\\sum_{l=1}^{6} P(L=l) = \\frac{3}{10} + \\frac{2}{10} + \\frac{1}{10} + \\frac{2}{15} + \\frac{2}{15} + \\frac{2}{15} = \\frac{6}{10} + 3 \\times \\frac{2}{15} = \\frac{3}{5} + \\frac{6}{15} = \\frac{3}{5} + \\frac{2}{5} = \\frac{5}{5} = 1$$\nThe probability distribution is consistent and well-defined.\n\nThe problem states that a frameshift occurs if and only if the deletion length $L$ is not a multiple of $3$. This is mathematically expressed as $L \\bmod 3 \\neq 0$. We need to calculate $P(L \\bmod 3 \\neq 0)$.\n\nIt is often simpler to calculate the probability of the complementary event, which is that the deletion is in-frame. An in-frame deletion occurs when $L$ is a multiple of $3$, or $L \\bmod 3 = 0$. We can then use the identity $P(A) = 1 - P(A^c)$, where $A$ is the frameshift event and $A^c$ is the in-frame event.\n\nLet's identify the outcomes in our sample space $S = \\{1, 2, 3, 4, 5, 6\\}$ that result in an in-frame deletion:\n- $L=3$: $3 \\bmod 3 = 0$. This is in-frame.\n- $L=6$: $6 \\bmod 3 = 0$. This is in-frame.\n\nThe probability of an in-frame deletion is the sum of the probabilities of these mutually exclusive events:\n$$P(\\text{in-frame}) = P(L \\bmod 3 = 0) = P(L=3) + P(L=6)$$\nSubstituting the fractional probabilities:\n$$P(\\text{in-frame}) = \\frac{1}{10} + \\frac{2}{15}$$\nTo sum these fractions, we find a common denominator, which is $30$:\n$$P(\\text{in-frame}) = \\frac{1 \\times 3}{10 \\times 3} + \\frac{2 \\times 2}{15 \\times 2} = \\frac{3}{30} + \\frac{4}{30} = \\frac{7}{30}$$\n\nThe probability of a frameshift is the complement of the probability of an in-frame deletion:\n$$P(\\text{frameshift}) = P(L \\bmod 3 \\neq 0) = 1 - P(\\text{in-frame})$$\n$$P(\\text{frameshift}) = 1 - \\frac{7}{30} = \\frac{30}{30} - \\frac{7}{30} = \\frac{23}{30}$$\n\nThe numerator, $23$, is a prime number, and the denominator, $30$, is not a multiple of $23$. Therefore, the fraction is in its simplest terms.\n\nAlternatively, we could directly sum the probabilities of the frameshift events. The outcomes causing a frameshift ($L \\bmod 3 \\neq 0$) are $L \\in \\{1, 2, 4, 5\\}$.\n$$P(\\text{frameshift}) = P(L=1) + P(L=2) + P(L=4) + P(L=5)$$\n$$P(\\text{frameshift}) = \\frac{3}{10} + \\frac{2}{10} + \\frac{2}{15} + \\frac{2}{15}$$\n$$P(\\text{frameshift}) = \\frac{5}{10} + \\frac{4}{15} = \\frac{1}{2} + \\frac{4}{15}$$\nFinding a common denominator of $30$:\n$$P(\\text{frameshift}) = \\frac{1 \\times 15}{2 \\times 15} + \\frac{4 \\times 2}{15 \\times 2} = \\frac{15}{30} + \\frac{8}{30} = \\frac{23}{30}$$\nBoth methods yield the same result.", "answer": "$$\\boxed{\\frac{23}{30}}$$", "id": "4566205"}, {"introduction": "While predicting individual outcomes is crucial, evaluating the overall performance of a predictive model is equally important for its clinical or research utility. This practice moves from theory to application by asking you to compute a read-weighted confusion matrix and its derived metrics—precision, recall, and F1-score [@problem_id:4566111]. This exercise simulates a realistic bioinformatics task where model performance is assessed against ground-truth data from next-generation sequencing.", "problem": "A laboratory uses clustered regularly interspaced short palindromic repeats (CRISPR) with CRISPR-associated protein 9 (Cas9) to edit a coding gene. Amplicon sequencing reveals a set of distinct edited alleles, each characterized by a deletion length $d$, an insertion length $i$, and a read count $r$. A gene editing outcome is classified as frameshift if the net indel length changes the reading frame; under the standard codon model from the Central Dogma of Molecular Biology (codons of length $3$ nucleotides), this is represented by a frameshift if and only if $|d - i| \\not\\equiv 0 \\ (\\mathrm{mod}\\ 3)$, and in-frame otherwise. A binary classifier outputs, for each allele, a calibrated probability $p$ that the allele is frameshift. The laboratory deploys a decision threshold $t = 0.65$ such that if $p \\geq t$ the predicted label is frameshift and otherwise the predicted label is in-frame. In downstream analytics, performance is evaluated at the level of reads: each allele contributes its read count to the appropriate cell of the confusion matrix.\n\nYou are given $12$ alleles with their attributes:\n- Allele $A_1$: $d = 1$, $i = 0$, $r = 120$, $p = 0.92$.\n- Allele $A_2$: $d = 3$, $i = 0$, $r = 80$, $p = 0.78$.\n- Allele $A_3$: $d = 2$, $i = 0$, $r = 60$, $p = 0.61$.\n- Allele $A_4$: $d = 4$, $i = 1$, $r = 150$, $p = 0.40$.\n- Allele $A_5$: $d = 5$, $i = 2$, $r = 90$, $p = 0.67$.\n- Allele $A_6$: $d = 0$, $i = 2$, $r = 70$, $p = 0.55$.\n- Allele $A_7$: $d = 6$, $i = 0$, $r = 110$, $p = 0.28$.\n- Allele $A_8$: $d = 7$, $i = 1$, $r = 50$, $p = 0.72$.\n- Allele $A_9$: $d = 2$, $i = 1$, $r = 95$, $p = 0.83$.\n- Allele $A_{10}$: $d = 0$, $i = 3$, $r = 130$, $p = 0.62$.\n- Allele $A_{11}$: $d = 8$, $i = 2$, $r = 140$, $p = 0.66$.\n- Allele $A_{12}$: $d = 1$, $i = 1$, $r = 75$, $p = 0.35$.\n\nUsing the rule $|d - i| \\not\\equiv 0 \\ (\\mathrm{mod}\\ 3)$ for ground-truth frameshift versus in-frame, and the threshold $t = 0.65$ for predicted frameshift versus in-frame, compute the read-weighted confusion matrix entries: true positives $\\mathrm{TP}$, false positives $\\mathrm{FP}$, true negatives $\\mathrm{TN}$, and false negatives $\\mathrm{FN}$, where the positive class is frameshift. Then, derive the positive-class precision, recall, and F1-score from first principles.\n\nExpress the confusion matrix entries as integers (read counts) and express precision, recall, and F1-score as reduced fractions. Do not round. Report your final results in the order $\\mathrm{TP}$, $\\mathrm{FP}$, $\\mathrm{TN}$, $\\mathrm{FN}$, precision, recall, F1-score.", "solution": "The problem requires the computation of a read-weighted confusion matrix and derived performance metrics (precision, recall, F1-score) for a frameshift classifier in a gene editing context. The analysis proceeds in three stages: first, for each allele, determining its ground-truth class and its predicted class; second, aggregating the read counts into the confusion matrix; and third, calculating the performance metrics from the matrix entries.\n\nThe positive class is defined as \"frameshift,\" and the negative class is \"in-frame.\"\n\nA. Determination of Ground-Truth and Predicted Classes\n\nThe ground-truth classification of an allele is based on its net indel length, which is the absolute difference between the deletion length $d$ and the insertion length $i$. According to the problem, an allele causes a frameshift if this net length is not a multiple of $3$.\n- Ground Truth Positive (Frameshift): $|d - i| \\not\\equiv 0 \\pmod 3$.\n- Ground Truth Negative (In-frame): $|d - i| \\equiv 0 \\pmod 3$.\n\nThe predicted classification is determined by comparing the classifier's output probability $p$ to a given threshold $t = 0.65$.\n- Predicted Positive (Frameshift): $p \\ge 0.65$.\n- Predicted Negative (In-frame): $p < 0.65$.\n\nWe will now process each of the $12$ alleles to classify it and determine its contribution to the confusion matrix, where TP, FP, TN, and FN stand for true positives, false positives, true negatives, and false negatives, respectively.\n\n1.  Allele $A_1$: $d = 1$, $i = 0$, $r = 120$, $p = 0.92$.\n    - Ground Truth: $|1 - 0| = 1$, and $1 \\not\\equiv 0 \\pmod 3$. Actual: Positive.\n    - Prediction: $p = 0.92 \\ge 0.65$. Predicted: Positive.\n    - Result: True Positive (TP). Contribution: $120$ reads.\n\n2.  Allele $A_2$: $d = 3$, $i = 0$, $r = 80$, $p = 0.78$.\n    - Ground Truth: $|3 - 0| = 3$, and $3 \\equiv 0 \\pmod 3$. Actual: Negative.\n    - Prediction: $p = 0.78 \\ge 0.65$. Predicted: Positive.\n    - Result: False Positive (FP). Contribution: $80$ reads.\n\n3.  Allele $A_3$: $d = 2$, $i = 0$, $r = 60$, $p = 0.61$.\n    - Ground Truth: $|2 - 0| = 2$, and $2 \\not\\equiv 0 \\pmod 3$. Actual: Positive.\n    - Prediction: $p = 0.61 < 0.65$. Predicted: Negative.\n    - Result: False Negative (FN). Contribution: $60$ reads.\n\n4.  Allele $A_4$: $d = 4$, $i = 1$, $r = 150$, $p = 0.40$.\n    - Ground Truth: $|4 - 1| = 3$, and $3 \\equiv 0 \\pmod 3$. Actual: Negative.\n    - Prediction: $p = 0.40 < 0.65$. Predicted: Negative.\n    - Result: True Negative (TN). Contribution: $150$ reads.\n\n5.  Allele $A_5$: $d = 5$, $i = 2$, $r = 90$, $p = 0.67$.\n    - Ground Truth: $|5 - 2| = 3$, and $3 \\equiv 0 \\pmod 3$. Actual: Negative.\n    - Prediction: $p = 0.67 \\ge 0.65$. Predicted: Positive.\n    - Result: False Positive (FP). Contribution: $90$ reads.\n\n6.  Allele $A_6$: $d = 0$, $i = 2$, $r = 70$, $p = 0.55$.\n    - Ground Truth: $|0 - 2| = 2$, and $2 \\not\\equiv 0 \\pmod 3$. Actual: Positive.\n    - Prediction: $p = 0.55 < 0.65$. Predicted: Negative.\n    - Result: False Negative (FN). Contribution: $70$ reads.\n\n7.  Allele $A_7$: $d = 6$, $i = 0$, $r = 110$, $p = 0.28$.\n    - Ground Truth: $|6 - 0| = 6$, and $6 \\equiv 0 \\pmod 3$. Actual: Negative.\n    - Prediction: $p = 0.28 < 0.65$. Predicted: Negative.\n    - Result: True Negative (TN). Contribution: $110$ reads.\n\n8.  Allele $A_8$: $d = 7$, $i = 1$, $r = 50$, $p = 0.72$.\n    - Ground Truth: $|7 - 1| = 6$, and $6 \\equiv 0 \\pmod 3$. Actual: Negative.\n    - Prediction: $p = 0.72 \\ge 0.65$. Predicted: Positive.\n    - Result: False Positive (FP). Contribution: $50$ reads.\n\n9.  Allele $A_9$: $d = 2$, $i = 1$, $r = 95$, $p = 0.83$.\n    - Ground Truth: $|2 - 1| = 1$, and $1 \\not\\equiv 0 \\pmod 3$. Actual: Positive.\n    - Prediction: $p = 0.83 \\ge 0.65$. Predicted: Positive.\n    - Result: True Positive (TP). Contribution: $95$ reads.\n\n10. Allele $A_{10}$: $d = 0$, $i = 3$, $r = 130$, $p = 0.62$.\n    - Ground Truth: $|0 - 3| = 3$, and $3 \\equiv 0 \\pmod 3$. Actual: Negative.\n    - Prediction: $p = 0.62 < 0.65$. Predicted: Negative.\n    - Result: True Negative (TN). Contribution: $130$ reads.\n\n11. Allele $A_{11}$: $d = 8$, $i = 2$, $r = 140$, $p = 0.66$.\n    - Ground Truth: $|8 - 2| = 6$, and $6 \\equiv 0 \\pmod 3$. Actual: Negative.\n    - Prediction: $p = 0.66 \\ge 0.65$. Predicted: Positive.\n    - Result: False Positive (FP). Contribution: $140$ reads.\n\n12. Allele $A_{12}$: $d = 1$, $i = 1$, $r = 75$, $p = 0.35$.\n    - Ground Truth: $|1 - 1| = 0$, and $0 \\equiv 0 \\pmod 3$. Actual: Negative.\n    - Prediction: $p = 0.35 < 0.65$. Predicted: Negative.\n    - Result: True Negative (TN). Contribution: $75$ reads.\n\nB. Aggregation of Read Counts into the Confusion Matrix\n\nWe sum the read counts for each category:\n- $\\mathrm{TP} = (\\text{reads from } A_1) + (\\text{reads from } A_9) = 120 + 95 = 215$.\n- $\\mathrm{FP} = (\\text{reads from } A_2) + (\\text{reads from } A_5) + (\\text{reads from } A_8) + (\\text{reads from } A_{11}) = 80 + 90 + 50 + 140 = 360$.\n- $\\mathrm{TN} = (\\text{reads from } A_4) + (\\text{reads from } A_7) + (\\text{reads from } A_{10}) + (\\text{reads from } A_{12}) = 150 + 110 + 130 + 75 = 465$.\n- $\\mathrm{FN} = (\\text{reads from } A_3) + (\\text{reads from } A_6) = 60 + 70 = 130$.\n\nThe confusion matrix entries are: $\\mathrm{TP} = 215$, $\\mathrm{FP} = 360$, $\\mathrm{TN} = 465$, $\\mathrm{FN} = 130$.\n\nC. Calculation of Performance Metrics\n\nThe metrics are defined as follows:\n- Precision (Positive Predictive Value): $\\mathrm{Precision} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}$\n- Recall (Sensitivity): $\\mathrm{Recall} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}}$\n- F1-Score: $\\mathrm{F1} = 2 \\times \\frac{\\mathrm{Precision} \\times \\mathrm{Recall}}{\\mathrm{Precision} + \\mathrm{Recall}} = \\frac{2\\mathrm{TP}}{2\\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN}}$\n\nSubstituting the aggregated read counts:\n\n- Precision:\n$$ \\mathrm{Precision} = \\frac{215}{215 + 360} = \\frac{215}{575} $$\nTo reduce this fraction, we find the greatest common divisor of the numerator and denominator. Both are divisible by $5$.\n$$ \\frac{215 \\div 5}{575 \\div 5} = \\frac{43}{115} $$\nSince $43$ is a prime number and $115 = 5 \\times 23$, the fraction is irreducible.\n\n- Recall:\n$$ \\mathrm{Recall} = \\frac{215}{215 + 130} = \\frac{215}{345} $$\nBoth are divisible by $5$.\n$$ \\frac{215 \\div 5}{345 \\div 5} = \\frac{43}{69} $$\nSince $43$ is a prime number and $69 = 3 \\times 23$, the fraction is irreducible.\n\n- F1-Score:\nUsing the direct formula for computational stability and simplicity:\n$$ \\mathrm{F1} = \\frac{2 \\times \\mathrm{TP}}{2 \\times \\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN}} = \\frac{2 \\times 215}{2 \\times 215 + 360 + 130} = \\frac{430}{430 + 490} = \\frac{430}{920} $$\nThis fraction can be simplified by dividing by $10$.\n$$ \\frac{43}{92} $$\nSince $43$ is a prime number and $92 = 2^2 \\times 23$, the fraction is irreducible.\n\nThe final results are:\n- $\\mathrm{TP} = 215$\n- $\\mathrm{FP} = 360$\n- $\\mathrm{TN} = 465$\n- $\\mathrm{FN} = 130$\n- $\\mathrm{Precision} = \\frac{43}{115}$\n- $\\mathrm{Recall} = \\frac{43}{69}$\n- $\\mathrm{F1-score} = \\frac{43}{92}$", "answer": "$$ \\boxed{\\begin{pmatrix} 215 & 360 & 465 & 130 & \\frac{43}{115} & \\frac{43}{69} & \\frac{43}{92} \\end{pmatrix}} $$", "id": "4566111"}, {"introduction": "The most powerful predictive models are often not black boxes, but are built upon biophysical first principles. This final practice challenges you to implement a simple but principled probabilistic model for predicting deletion outcomes mediated by the Microhomology-Mediated End Joining (MMEJ) pathway [@problem_id:4566246]. By translating biological hypotheses about microhomology into a scoring function and a softmax probability layer, you will gain insight into how predictive bioinformatics tools are constructed.", "problem": "You are tasked with formalizing and implementing a principled probabilistic model for predicting deletion outcomes following a gene editing double-strand break in deoxyribonucleic acid (DNA), grounded in Microhomology-Mediated End Joining (MMEJ). The design must begin from widely accepted biological observations: longer local microhomology tracts near the cut site increase the likelihood of deletion formation, and microhomology tracts farther from the cut site contribute less strongly to repair decisions. Accept that deletion outcomes can be treated as competing alternatives whose probabilities should be normalized to sum to $1$, consistent with a maximum entropy perspective under constraints.\n\nDefine a scoring function $S_{\\text{MH}}$ that, for each candidate deletion $k$ among $K$ alternatives, assigns a scalar score based on the candidate’s microhomology length $L_k$ (in nucleotides, abbreviated as nt) and its distance from the cut site $D_k$ (in nt). The score must be constructed from a length-indexed weight function that penalizes increasing distance from the cut site. The weight function must be monotonic decreasing in the distance argument and must be parameterized by a non-negative vector of base amplitudes indexed by microhomology length. The mapping from scores to probabilities must use a normalized exponential across the $K$ candidates (a softmax), with an explicit temperature parameter to control sharpness. Distances are to be measured in nucleotides, and probabilities must be reported as decimals (not percentages).\n\nThe implementation must adhere to the following specifications:\n\n- Model ingredients:\n  - A base amplitude vector $\\boldsymbol{\\alpha} = (\\alpha_1, \\alpha_2, \\dots, \\alpha_J)$ with $\\alpha_j \\ge 0$ for $j \\in \\{1, \\dots, J\\}$ that encodes the intrinsic contribution of microhomology of length $j$ nt.\n  - A distance penalty rate $\\gamma \\ge 0$ that ensures weights decrease with increasing $D_k$.\n  - A temperature parameter $\\tau > 0$ that controls the softness of the normalized exponential mapping from scores to probabilities.\n  - For each candidate deletion $k$, a microhomology length $L_k \\in \\{0,1,\\dots,J\\}$ (in nt), where $L_k = 0$ denotes no microhomology, and a distance $D_k \\ge 0$ (in nt).\n- Scoring rule constraints:\n  - The score $S_{\\text{MH}}(k)$ must be a function that selects the appropriate length-indexed weight and applies a monotone decreasing penalty in $D_k$.\n  - If $L_k = 0$ (no microhomology), the score must default to $0$.\n- Probability mapping:\n  - The probability vector $\\mathbf{p} = (p_1, \\dots, p_K)$ must be computed via a normalized exponential over the candidate scores, controlled by $\\tau$, and must satisfy $\\sum_{k=1}^K p_k = 1$.\n- Numerical requirements:\n  - Distances $D_k$ are in nucleotides (nt).\n  - Microhomology lengths $L_k$ are in nucleotides (nt).\n  - Report probabilities as decimal values.\n  - Use numerically stable computation for the normalized exponential.\n\nYour program must implement this model and produce outputs for the following test suite. For each test case, the inputs are $(\\boldsymbol{\\alpha}, \\gamma, \\tau, \\mathbf{L}, \\mathbf{D})$, and the required output is the probability vector $\\mathbf{p}$.\n\nTest suite (units: all $D_k$ and $L_k$ are in nt; probabilities are decimals):\n\n- Case $1$ (general case): $J = 5$, $\\boldsymbol{\\alpha} = (0.2, 0.5, 0.9, 1.4, 2.0)$, $\\gamma = 0.1$, $\\tau = 1.0$, $\\mathbf{L} = (1, 2, 4, 5)$, $\\mathbf{D} = (0, 3, 5, 1)$.\n- Case $2$ (uniformity check with identical scores): $J = 3$, $\\boldsymbol{\\alpha} = (1.0, 1.0, 1.0)$, $\\gamma = 0.0$, $\\tau = 1.0$, $\\mathbf{L} = (2, 2, 2)$, $\\mathbf{D} = (5, 5, 5)$.\n- Case $3$ (no microhomology boundary): $J = 4$, $\\boldsymbol{\\alpha} = (0.3, 0.7, 1.2, 1.6)$, $\\gamma = 0.05$, $\\tau = 1.0$, $\\mathbf{L} = (0, 1, 3)$, $\\mathbf{D} = (2, 2, 2)$.\n- Case $4$ (large distance with moderate penalty): $J = 4$, $\\boldsymbol{\\alpha} = (0.5, 1.0, 1.5, 2.0)$, $\\gamma = 0.1$, $\\tau = 1.0$, $\\mathbf{L} = (1, 4, 3)$, $\\mathbf{D} = (20, 20, 20)$.\n- Case $5$ (temperature sensitivity, sharp distribution): $J = 5$, $\\boldsymbol{\\alpha} = (0.2, 0.5, 0.9, 1.4, 2.0)$, $\\gamma = 0.1$, $\\tau = 0.1$, $\\mathbf{L} = (1, 2, 4, 5)$, $\\mathbf{D} = (0, 3, 5, 1)$.\n- Case $6$ (temperature sensitivity, flat distribution): $J = 5$, $\\boldsymbol{\\alpha} = (0.2, 0.5, 0.9, 1.4, 2.0)$, $\\gamma = 0.1$, $\\tau = 5.0$, $\\mathbf{L} = (1, 2, 4, 5)$, $\\mathbf{D} = (0, 3, 5, 1)$.\n\nFinal output format requirement: Your program should produce a single line of output containing the results for all $6$ cases as a comma-separated list enclosed in square brackets, where each case’s probability vector is itself a list of decimal values rounded to $6$ decimal places (e.g., $[[0.1,0.2],[0.3,0.7]]$). No extra text is permitted.", "solution": "The problem statement has been validated and is determined to be a valid, well-posed scientific modeling task. It is scientifically grounded in established principles of molecular biology, specifically the Microhomology-Mediated End Joining (MMEJ) DNA repair pathway. The required model is specified with sufficient and consistent constraints, making it formalizable and objective. There are no contradictions, ambiguities, or factual inaccuracies.\n\nThe task is to formalize and implement a probabilistic model for predicting deletion outcomes at a DNA double-strand break site. The model's design is guided by key biological observations and is structured within a standard statistical framework.\n\n**1. Model Formalization**\n\nThe model is constructed in two primary stages: first, a scoring function that quantifies the propensity of each potential deletion outcome, and second, a probability mapping that converts these scores into a normalized probability distribution.\n\n**1.1. Scoring Function, $S_{\\text{MH}}(k)$**\n\nThe problem requires a scoring function, $S_{\\text{MH}}(k)$, for each candidate deletion $k$, that incorporates the microhomology length $L_k$ and its distance from the cut site $D_k$. The score must be based on a length-indexed weight function penalized by distance.\n\nA standard and direct interpretation of these constraints leads to a multiplicative form. The base propensity is given by the amplitude $\\alpha_{L_k}$ from the vector $\\boldsymbol{\\alpha} = (\\alpha_1, \\alpha_2, \\dots, \\alpha_J)$. The problem specifies that $\\boldsymbol{\\alpha}$ is a vector of non-negative constants where the index corresponds to the microhomology length in nucleotides. Thus, for a deletion mediated by a microhomology of length $L_k \\in \\{1, \\dots, J\\}$, the base score is $\\alpha_{L_k}$.\n\nThe distance penalty must be a monotonic decreasing function of the distance $D_k$, modulated by a rate parameter $\\gamma \\ge 0$. The canonical functional form for such a penalty is an exponential decay, $e^{-\\gamma D_k}$.\n\nCombining these elements, the score for a candidate deletion $k$ with microhomology length $L_k > 0$ is:\n$$\nS_{\\text{MH}}(k) = \\alpha_{L_k} \\cdot \\exp(-\\gamma D_k)\n$$\nThe problem states that if there is no microhomology, i.e., $L_k = 0$, the score defaults to $0$. This reflects the MMEJ pathway's reliance on microhomology. Thus, the complete scoring function is defined as:\n$$\nS_{\\text{MH}}(k) =\n\\begin{cases}\n    \\alpha_{L_k} \\cdot \\exp(-\\gamma D_k) & \\text{if } L_k > 0 \\\\\n    0 & \\text{if } L_k = 0\n\\end{cases}\n$$\nIn implementation, for a given microhomology length $L_k$, the corresponding amplitude $\\alpha_{L_k}$ is accessed from the input vector $\\boldsymbol{\\alpha}$. If $\\boldsymbol{\\alpha}$ is represented by a $0$-indexed array, $\\alpha_{L_k}$ corresponds to the element at index $L_k - 1$.\n\n**1.2. Probability Mapping**\n\nThe problem specifies that the scores $\\{S_{\\text{MH}}(1), \\dots, S_{\\text{MH}}(K)\\}$ for the $K$ candidate deletions be transformed into a probability vector $\\mathbf{p} = (p_1, \\dots, p_K)$ using a normalized exponential function, commonly known as the softmax function. This choice is consistent with the principle of maximum entropy, providing the most unbiased distribution given the constraints imposed by the scores.\n\nThe mapping includes a temperature parameter $\\tau > 0$, which controls the sharpness of the probability distribution. A low temperature ($\\tau \\to 0$) concentrates probability on the highest-scoring candidate, whereas a high temperature ($\\tau \\to \\infty$) results in a nearly uniform distribution. The probability $p_k$ for candidate $k$ is given by:\n$$\np_k = \\frac{\\exp(S_{\\text{MH}}(k) / \\tau)}{\\sum_{i=1}^{K} \\exp(S_{\\text{MH}}(i) / \\tau)}\n$$\nThis ensures that $p_k \\ge 0$ for all $k$ and $\\sum_{k=1}^{K} p_k = 1$.\n\n**2. Algorithmic Design and Numerical Stability**\n\nDirect computation of the softmax formula can be numerically unstable. If the scores $S_{\\text{MH}}(k)$ are large, the term $\\exp(S_{\\text{MH}}(k) / \\tau)$ can cause a floating-point overflow. To ensure numerical stability, a standard technique is employed, which exploits the property that $\\text{softmax}(\\mathbf{x}) = \\text{softmax}(\\mathbf{x} + c)$ for any constant $c$. By choosing $c = -\\max(\\mathbf{x})$, we can prevent overflow.\n\nLet the vector of scaled scores be $\\mathbf{z} = (S_{\\text{MH}}(1)/\\tau, \\dots, S_{\\text{MH}}(K)/\\tau)$. Let $z_{\\max} = \\max_{i} \\{z_i\\}$. The stable computation for $p_k$ is:\n$$\np_k = \\frac{\\exp(z_k - z_{\\max})}{\\sum_{i=1}^{K} \\exp(z_i - z_{\\max})}\n$$\nThe largest term in the exponent is now $0$, so $\\exp(z_k - z_{\\max}) \\le 1$, preventing overflow. Terms where $z_i$ is much smaller than $z_{\\max}$ will correctly underflow to $0$.\n\n**3. Implementation Strategy**\n\nThe model is implemented in Python, leveraging the `numpy` library for efficient numerical and array-based computations.\nFor each test case, provided as a tuple $(\\boldsymbol{\\alpha}, \\gamma, \\tau, \\mathbf{L}, \\mathbf{D})$:\n1. An array of scores is computed by iterating through the candidate deletions. For each candidate $k$ with properties $(L_k, D_k)$, the score $S_{\\text{MH}}(k)$ is calculated using the formalized scoring function.\n2. The resulting array of scores is divided by the temperature $\\tau$.\n3. The numerically stable softmax function is applied to the scaled scores to compute the final probability vector $\\mathbf{p}$.\n4. The probabilities are rounded to $6$ decimal places as required.\n5. The final results from all test cases are collected and formatted into a single string according to the strict output specification.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the gene editing outcome prediction problem for a given suite of test cases.\n    The implementation follows a principled probabilistic model based on Microhomology-Mediated End Joining (MMEJ).\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: General case\n        (\n            (0.2, 0.5, 0.9, 1.4, 2.0),  # alpha\n            0.1,  # gamma\n            1.0,  # tau\n            (1, 2, 4, 5),  # L\n            (0, 3, 5, 1)   # D\n        ),\n        # Case 2: Uniformity check with identical scores\n        (\n            (1.0, 1.0, 1.0),  # alpha\n            0.0,  # gamma\n            1.0,  # tau\n            (2, 2, 2),  # L\n            (5, 5, 5)   # D\n        ),\n        # Case 3: No microhomology boundary\n        (\n            (0.3, 0.7, 1.2, 1.6),  # alpha\n            0.05,  # gamma\n            1.0,  # tau\n            (0, 1, 3),  # L\n            (2, 2, 2)   # D\n        ),\n        # Case 4: Large distance with moderate penalty\n        (\n            (0.5, 1.0, 1.5, 2.0),  # alpha\n            0.1,  # gamma\n            1.0,  # tau\n            (1, 4, 3),  # L\n            (20, 20, 20)  # D\n        ),\n        # Case 5: Temperature sensitivity, sharp distribution\n        (\n            (0.2, 0.5, 0.9, 1.4, 2.0),  # alpha\n            0.1,  # gamma\n            0.1,  # tau\n            (1, 2, 4, 5),  # L\n            (0, 3, 5, 1)   # D\n        ),\n        # Case 6: Temperature sensitivity, flat distribution\n        (\n            (0.2, 0.5, 0.9, 1.4, 2.0),  # alpha\n            0.1,  # gamma\n            5.0,  # tau\n            (1, 2, 4, 5),  # L\n            (0, 3, 5, 1)    # D\n        ),\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        alpha, gamma, tau, L_vec, D_vec = case\n        \n        # Calculate scores for all candidate deletions\n        scores = []\n        for L_k, D_k in zip(L_vec, D_vec):\n            if L_k == 0:\n                score = 0.0\n            else:\n                # alpha is 1-indexed in the problem description, so we use L_k - 1\n                base_amplitude = alpha[L_k - 1]\n                score = base_amplitude * np.exp(-gamma * D_k)\n            scores.append(score)\n        \n        scores_arr = np.array(scores, dtype=np.float64)\n        \n        # If there are no candidates, the result is an empty list\n        if scores_arr.size == 0:\n            all_results.append([])\n            continue\n\n        # Apply temperature parameter\n        scaled_scores = scores_arr / tau\n        \n        # Compute probabilities using numerically stable softmax\n        # Subtract the maximum score to prevent overflow\n        scaled_scores -= np.max(scaled_scores)\n        exp_scores = np.exp(scaled_scores)\n        probabilities = exp_scores / np.sum(exp_scores)\n        \n        # Round results to 6 decimal places\n        result_vector = [round(p, 6) for p in probabilities]\n        all_results.append(result_vector)\n\n    # Format the final output string exactly as specified: [[p1,p2,...],[q1,q2,...]]\n    # No spaces within the inner lists.\n    formatted_case_results = []\n    for res_vec in all_results:\n        vec_as_str = '[' + ','.join(map(str, res_vec)) + ']'\n        formatted_case_results.append(vec_as_str)\n    \n    final_output_str = '[' + ','.join(formatted_case_results) + ']'\n    print(final_output_str)\n\nsolve()\n```", "id": "4566246"}]}