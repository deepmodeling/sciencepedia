{"hands_on_practices": [{"introduction": "A primary goal in eQTL analysis is to quantify the impact of a genetic variant on gene expression. This exercise walks you through the fundamental calculation of variance explained ($R^2$), connecting the statistical effect size, $\\beta$, to a biologically interpretable measure of a cis-eQTL's strength. Mastering this derivation [@problem_id:4562167] is essential for understanding and contextualizing eQTL discoveries.", "problem": "Consider a cis expression quantitative trait locus (cis-eQTL) mapping analysis in which messenger ribonucleic acid (mRNA) expression for a single gene is modeled with an additive genotype effect. Let the individual-level expression be denoted by $y$, measured on a log-transformed, quantile-normalized scale after removing known covariates, and let the genotype dosage be denoted by $g \\in \\{0,1,2\\}$ for the minor allele count. Assume the following:\n- The linear model $y = \\alpha + \\beta g + \\varepsilon$ holds, where $\\varepsilon$ is a random residual independent of $g$, and $\\alpha$ and $\\beta$ are constants.\n- The minor allele frequency is $p = 0.25$.\n- The population is in Hardy-Weinberg Equilibrium (HWE), so $g$ may be treated as a binomial random variable with two trials and success probability $p$.\n- The marginal variance of the expression phenotype (after covariate adjustment) is $\\mathrm{Var}(y) = 0.8$.\n- The ordinary least squares (OLS) estimate of the per-allele effect on expression is $\\beta = 0.3$.\n\nStarting from the law of total variance and the definition of the linear model with independent residuals, derive an analytic expression for the fraction of expression variance explained by genotype, denoted by $R^2$, in terms of $\\beta$, $p$, and $\\mathrm{Var}(y)$. Then, using the provided values, compute the numerical value of $R^2$. Round your final numeric answer to three significant figures and express it as a decimal number with no units. As part of your reasoning, situate the computed magnitude of $R^2$ within typical ranges observed for cis-eQTL effect sizes, but ensure that your final reported answer is only the requested number.", "solution": "The problem statement will first be validated against the required criteria.\n\n### Step 1: Extract Givens\n- The model for gene expression $y$ is a linear model with an additive genotype effect: $y = \\alpha + \\beta g + \\varepsilon$.\n- $y$ is the individual-level expression on a log-transformed, quantile-normalized scale after removing known covariates.\n- $g$ is the genotype dosage for the minor allele count, with $g \\in \\{0, 1, 2\\}$.\n- $\\varepsilon$ is a random residual independent of $g$.\n- $\\alpha$ and $\\beta$ are constants.\n- The minor allele frequency is $p = 0.25$.\n- The population is in Hardy-Weinberg Equilibrium (HWE).\n- $g$ is treated as a binomial random variable with two trials and success probability $p$.\n- The marginal variance of the expression phenotype is $\\mathrm{Var}(y) = 0.8$.\n- The ordinary least squares (OLS) estimate of the per-allele effect on expression is $\\beta = 0.3$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated for validity:\n- **Scientifically Grounded:** The problem is firmly grounded in the principles of statistical genetics and bioinformatics, specifically in the context of expression quantitative trait locus (eQTL) analysis. The additive linear model, Hardy-Weinberg Equilibrium, and the concept of variance explained are standard components of this field. The given values for minor allele frequency ($p=0.25$), effect size ($\\beta=0.3$), and total expression variance ($\\mathrm{Var}(y)=0.8$) are realistic for cis-eQTLs.\n- **Well-Posed:** The problem is well-posed. It provides all necessary information and clear definitions to derive a unique, meaningful solution. The question is unambiguous.\n- **Objective:** The problem is stated in precise, objective language, free of subjective claims or bias.\n- **Completeness and Consistency:** The setup is self-contained and internally consistent. No information is missing or contradictory.\n- **Feasibility:** The scenario described is not only scientifically plausible but represents a standard, simplified model used in real-world genetic data analysis.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe objective is to derive an expression for the fraction of expression variance explained by genotype, $R^2$, and then to compute its numerical value.\n\nThe fraction of variance explained, $R^2$, is defined as the ratio of the variance explained by the model's predictor variable (genotype $g$) to the total variance of the outcome variable (expression $y$).\n$$\nR^2 = \\frac{\\mathrm{Var}_{\\text{explained}}}{\\mathrm{Var}_{\\text{total}}} = \\frac{\\mathrm{Var}(\\text{E}[y|g])}{\\mathrm{Var}(y)}\n$$\n\nAs stipulated, we begin with the law of total variance:\n$$\n\\mathrm{Var}(y) = \\mathrm{E}[\\mathrm{Var}(y|g)] + \\mathrm{Var}(\\mathrm{E}[y|g])\n$$\nThis partitions the total variance of $y$ into two components: the expected value of the conditional variance (residual variance) and the variance of the conditional expectation (explained variance).\n\nWe use the given linear model, $y = \\alpha + \\beta g + \\varepsilon$, to determine these components.\nFirst, we find the conditional expectation of $y$ given $g$:\n$$\n\\mathrm{E}[y|g] = \\mathrm{E}[\\alpha + \\beta g + \\varepsilon | g]\n$$\nSince $\\alpha$ and $\\beta$ are constants, and $g$ is known in the conditioning, we have:\n$$\n\\mathrm{E}[y|g] = \\alpha + \\beta g + \\mathrm{E}[\\varepsilon|g]\n$$\nThe problem states that the residual $\\varepsilon$ is independent of $g$. Therefore, $\\mathrm{E}[\\varepsilon|g] = \\mathrm{E}[\\varepsilon]$. In an OLS framework, it is assumed that $\\mathrm{E}[\\varepsilon]=0$. Thus, the conditional expectation is:\n$$\n\\mathrm{E}[y|g] = \\alpha + \\beta g\n$$\nThis term represents the part of the expression level that is predicted by the genotype.\n\nNext, we find the variance of this conditional expectation, which is the variance explained by the genotype:\n$$\n\\mathrm{Var}(\\mathrm{E}[y|g]) = \\mathrm{Var}(\\alpha + \\beta g)\n$$\nSince $\\alpha$ is a constant, it does not contribute to the variance. The variance property $\\mathrm{Var}(cX) = c^2\\mathrm{Var}(X)$ gives:\n$$\n\\mathrm{Var}(\\mathrm{E}[y|g]) = \\mathrm{Var}(\\beta g) = \\beta^2 \\mathrm{Var}(g)\n$$\n\nThe problem states that the population is in Hardy-Weinberg Equilibrium (HWE) and the genotype dosage $g$ can be treated as a binomial random variable with $n=2$ trials (for a diploid organism) and success probability $p$ (the minor allele frequency). The variance of a binomial random variable is given by $np(1-p)$.\nTherefore, the variance of the genotype dosage is:\n$$\n\\mathrm{Var}(g) = 2p(1-p)\n$$\nSubstituting this into the expression for the explained variance yields:\n$$\n\\mathrm{Var}(\\mathrm{E}[y|g]) = \\beta^2 [2p(1-p)] = 2p(1-p)\\beta^2\n$$\n\nNow we return to the definition of $R^2$. We have the expression for the explained variance, and the total variance $\\mathrm{Var}(y)$ is given.\nThe analytical expression for the fraction of variance explained by genotype is:\n$$\nR^2 = \\frac{\\mathrm{Var}(\\mathrm{E}[y|g])}{\\mathrm{Var}(y)} = \\frac{2p(1-p)\\beta^2}{\\mathrm{Var}(y)}\n$$\nThis is the required general formula in terms of $\\beta$, $p$, and $\\mathrm{Var}(y)$.\n\nNow, we substitute the provided numerical values into this expression:\n- $\\beta = 0.3$\n- $p = 0.25$\n- $\\mathrm{Var}(y) = 0.8$\n\nFirst, calculate the variance of the genotype, $\\mathrm{Var}(g)$:\n$$\n\\mathrm{Var}(g) = 2p(1-p) = 2(0.25)(1-0.25) = 2(0.25)(0.75) = 0.5(0.75) = 0.375\n$$\nNext, calculate the variance explained by the genotype:\n$$\n\\mathrm{Var}(\\mathrm{E}[y|g]) = \\beta^2 \\mathrm{Var}(g) = (0.3)^2 \\times 0.375 = 0.09 \\times 0.375 = 0.03375\n$$\nFinally, calculate $R^2$:\n$$\nR^2 = \\frac{0.03375}{\\mathrm{Var}(y)} = \\frac{0.03375}{0.8} = 0.0421875\n$$\nThe problem asks to round the answer to three significant figures. The first significant figure is $4$, the second is $2$, and the third is $1$. The following digit is $8$, which is greater than or equal to $5$, so we round up the third significant digit.\n$$\nR^2 \\approx 0.0422\n$$\n\nThis value, representing $4.22\\%$ of the variance in gene expression explained by a single genetic variant, is a plausible and commonly observed effect size for a strong cis-eQTL. In large-scale studies like the Genotype-Tissue Expression (GTEx) project, it is typical for the most significant cis-eQTL for a given gene (the \"eGene\") to explain between $1\\%$ and $20\\%$ of the expression variance, with some exceptional cases exceeding $50\\%$. An $R^2$ of approximately $4\\%$ is therefore a substantial and realistic effect size within this biological context.", "answer": "$$\n\\boxed{0.0422}\n$$", "id": "4562167"}, {"introduction": "Real-world eQTL studies rarely use perfectly observed genotypes, relying instead on imputation, which introduces uncertainty. This problem explores the statistical consequences of using imputed dosages in a regression model, demonstrating how this uncertainty impacts power and the interpretation of association statistics. Understanding these principles [@problem_id:4562182] is critical for correctly performing and interpreting large-scale genetic association analyses.", "problem": "An investigator performs expression quantitative trait loci (eQTL) mapping in a cohort of $n$ individuals. For a single variant with minor allele frequency $p$ under Hardy-Weinberg Equilibrium (HWE), the true genotype count $G_i \\in \\{0,1,2\\}$ is not directly observed. Instead, genotype imputation provides a dosage $D_i = \\mathbb{E}[G_i \\mid \\mathcal{I}_i]$, where $\\mathcal{I}_i$ denotes the imputation information for individual $i$, and a per-individual posterior variance $v_i = \\operatorname{Var}(G_i \\mid \\mathcal{I}_i)$. Gene expression is modeled by a linear regression $Y_i = \\alpha + \\beta G_i + \\varepsilon_i$ with independent and identically distributed errors $\\varepsilon_i$ satisfying $\\mathbb{E}[\\varepsilon_i] = 0$ and $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2$. The analyst, however, fits $Y_i = \\alpha + \\beta D_i + \\varepsilon_i$ using Ordinary Least Squares (OLS).\n\nUsing only first principles such as the law of total variance, the law of total covariance, and the properties of OLS estimators, analyze how imputation uncertainty enters association testing through the dosage variance and its consequences for the $Z$- or Wald statistic. Let $\\operatorname{Var}(G) = 2p(1-p)$ under HWE, and define the imputation quality metric $R^2$ for this variant in terms of population-level quantities and the imputation posterior variance. Based on your derivations, select all statements that are correct:\n\nA. In OLS of $Y$ on $D$, the slope estimator for $\\beta$ is unbiased. Relative to using true genotypes, the expected Wald or $Z$-statistic for testing $H_0:\\beta=0$ is attenuated by a factor $\\sqrt{R^2}$, where $R^2 = 1 - \\mathbb{E}[v_i]/(2p(1-p))$.\n\nB. Modeling dosage uncertainty as $D_i = G_i + \\epsilon_i$ with $\\operatorname{Var}(\\epsilon_i) = v_i$ implies $\\operatorname{Var}(D) = \\operatorname{Var}(G) + \\mathbb{E}[v_i]$, so imputation uncertainty inflates the predictor variance and therefore increases power.\n\nC. The imputation quality metric $R^2$ equals $\\operatorname{Corr}(D,G)$ (not squared), and the association $Z$-statistic is reduced by a factor $R^2$.\n\nD. Filtering variants by an $R^2$ threshold necessarily biases effect estimates toward zero because retained variants have $\\operatorname{Var}(D) < \\operatorname{Var}(G)$.\n\nE. By the law of total variance, $\\operatorname{Var}(G) = \\operatorname{Var}(D) + \\mathbb{E}[v_i]$, which implies $R^2 = \\operatorname{Var}(D)/\\operatorname{Var}(G)$; therefore, tests using dosages have an effective sample size $n_{\\text{eff}} = R^2 n$.\n\nF. Because $R^2$ quantifies the fraction of genotype variance captured by dosages, a principled practice is to filter variants with low $R^2$ (for example, below $0.3$ in large samples or below $0.8$ for rare variants), since they contribute $n_{\\text{eff}} \\approx R^2 n$ and have limited power without improving inference quality.", "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective before a solution is attempted.\n\n### Step 1: Extract Givens\n-   **Cohort size**: $n$ individuals.\n-   **Variant**: A single variant with minor allele frequency $p$.\n-   **Genetic Model**: Hardy-Weinberg Equilibrium (HWE) is assumed.\n-   **True Genotype**: $G_i \\in \\{0, 1, 2\\}$, which is the unobserved count of the minor allele for individual $i$. From HWE, $\\mathbb{E}[G] = 2p$ and $\\operatorname{Var}(G) = 2p(1-p)$.\n-   **Imputed Dosage**: $D_i = \\mathbb{E}[G_i \\mid \\mathcal{I}_i]$, where $\\mathcal{I}_i$ is the individual-specific imputation information.\n-   **Posterior Variance**: $v_i = \\operatorname{Var}(G_i \\mid \\mathcal{I}_i)$, the uncertainty in the imputed genotype for individual $i$.\n-   **True Phenotype Model**: $Y_i = \\alpha + \\beta G_i + \\varepsilon_i$.\n-   **Error Term**: The errors $\\varepsilon_i$ are independent and identically distributed (i.i.d.) with $\\mathbb{E}[\\varepsilon_i] = 0$ and $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2$. The error $\\varepsilon_i$ is independent of the genetic information $G_i$ and $\\mathcal{I}_i$.\n-   **Fitted Model**: An analyst fits the model $Y_i = \\alpha + \\beta D_i + \\varepsilon_i$ using Ordinary Least Squares (OLS).\n-   **Objective**: Analyze the properties of the OLS estimator for $\\beta$ and the associated Wald ($Z$) statistic from the fitted model, and define an imputation quality metric $R^2$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem describes a standard scenario in statistical genetics and eQTL mapping. Genotype imputation, the use of dosages ($D_i$), posterior variances ($v_i$), and linear regression are all fundamental components of modern genetic association studies. The statistical formulations, including the definitions of $D_i$ and $v_i$ as conditional expectation and variance, are correct. The application of first principles like the laws of total variance and covariance is appropriate. The problem is scientifically and mathematically sound.\n-   **Well-Posed**: The problem is clearly stated with all necessary definitions and assumptions to derive the consequences of using imputed dosages in place of true genotypes. The question is specific and leads to a unique analytical solution.\n-   **Objective**: The language is technical, precise, and free from subjective or ambiguous terminology.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A full derivation and evaluation of the options may proceed.\n\n### Derivation of Core Relationships\n\nWe will use population-level quantities, which correspond to the large-sample behavior of their sample-based estimators. The analysis relies on the laws of total expectation, variance, and covariance.\n\n1.  **Relationship between $\\operatorname{Var}(G)$, $\\operatorname{Var}(D)$, and $\\mathbb{E}[v_i]$**:\n    The law of total variance states that for random variables $X$ and $Z$, $\\operatorname{Var}(X) = \\operatorname{Var}(\\mathbb{E}[X \\mid Z]) + \\mathbb{E}[\\operatorname{Var}(X \\mid Z)]$.\n    Let $X=G$ and $Z=\\mathcal{I}$. The quantities in the problem are defined based on conditioning on $\\mathcal{I}_i$ for each individual $i$. We can think of these as population-level expectations over the distribution of individuals and their imputation information $\\mathcal{I}$.\n    -   By definition, $\\mathbb{E}[G \\mid \\mathcal{I}] = D$. Thus, $\\operatorname{Var}(\\mathbb{E}[G \\mid \\mathcal{I}]) = \\operatorname{Var}(D)$.\n    -   By definition, $\\operatorname{Var}(G \\mid \\mathcal{I}) = v$. Thus, $\\mathbb{E}[\\operatorname{Var}(G \\mid \\mathcal{I})] = \\mathbb{E}[v_i]$ (taking the expectation over all individuals).\n    -   Substituting these into the law of total variance gives the fundamental equation:\n        $$ \\operatorname{Var}(G) = \\operatorname{Var}(D) + \\mathbb{E}[v_i] $$\n    This shows that the variance of the imputed dosages is always less than or equal to the variance of the true genotypes: $\\operatorname{Var}(D) = \\operatorname{Var}(G) - \\mathbb{E}[v_i] \\le \\operatorname{Var}(G)$.\n\n2.  **OLS Estimator for $\\beta$**:\n    The analyst regresses $Y$ on $D$. The population OLS slope for this regression is $\\beta_D = \\frac{\\operatorname{Cov}(Y, D)}{\\operatorname{Var}(D)}$. This is the value to which the OLS estimator $\\hat{\\beta}_D$ converges in large samples.\n    -   Let's analyze the covariance term: $\\operatorname{Cov}(Y, D) = \\operatorname{Cov}(\\alpha + \\beta G + \\varepsilon, D) = \\beta \\operatorname{Cov}(G, D) + \\operatorname{Cov}(\\varepsilon, D)$.\n    -   The non-genetic residual $\\varepsilon$ is independent of the genetic imputation process, so $\\operatorname{Cov}(\\varepsilon, D) = 0$.\n    -   Now, we analyze $\\operatorname{Cov}(G, D)$ using the law of total covariance: $\\operatorname{Cov}(X,Y) = \\mathbb{E}[\\operatorname{Cov}(X,Y|Z)] + \\operatorname{Cov}(\\mathbb{E}[X|Z], \\mathbb{E}[Y|Z])$.\n    -   Let $X=G, Y=D, Z=\\mathcal{I}$.\n    -   $\\mathbb{E}[G \\mid \\mathcal{I}]=D$. Since $D$ is a function of $\\mathcal{I}$, it is \"constant\" when conditioned on $\\mathcal{I}$, so $\\mathbb{E}[D \\mid \\mathcal{I}] = D$ and $\\operatorname{Cov}(G, D \\mid \\mathcal{I}) = 0$.\n    -   Therefore, $\\operatorname{Cov}(G, D) = \\operatorname{Cov}(\\mathbb{E}[G \\mid \\mathcal{I}], \\mathbb{E}[D \\mid \\mathcal{I}]) = \\operatorname{Cov}(D, D) = \\operatorname{Var}(D)$.\n    -   Substituting this back into the expression for $\\beta_D$:\n        $$ \\beta_D = \\frac{\\beta \\operatorname{Cov}(G, D)}{\\operatorname{Var}(D)} = \\frac{\\beta \\operatorname{Var}(D)}{\\operatorname{Var}(D)} = \\beta $$\n    This demonstrates that the OLS estimator of the slope, when regressing on imputed dosages defined as conditional expectations, is **unbiased** for the true effect size $\\beta$. This type of model, where $D_i = \\mathbb{E}[G_i | \\mathcal{I}_i]$, is known as a Berkson error model.\n\n3.  **Imputation Quality $R^2$**:\n    The standard measure of imputation quality, denoted $R^2$, is the squared correlation between the true genotype $G$ and the imputed dosage $D$.\n    $$ R^2 = (\\operatorname{Corr}(G, D))^2 = \\frac{(\\operatorname{Cov}(G, D))^2}{\\operatorname{Var}(G) \\operatorname{Var}(D)} $$\n    -   Using our previous result that $\\operatorname{Cov}(G, D) = \\operatorname{Var}(D)$:\n        $$ R^2 = \\frac{(\\operatorname{Var}(D))^2}{\\operatorname{Var}(G) \\operatorname{Var}(D)} = \\frac{\\operatorname{Var}(D)}{\\operatorname{Var}(G)} $$\n    -   This confirms that $R^2$ is the fraction of true genotype variance captured by the imputed dosages.\n    -   Using $\\operatorname{Var}(D) = \\operatorname{Var}(G) - \\mathbb{E}[v_i]$ and the problem's given $\\operatorname{Var}(G) = 2p(1-p)$:\n        $$ R^2 = \\frac{\\operatorname{Var}(G) - \\mathbb{E}[v_i]}{\\operatorname{Var}(G)} = 1 - \\frac{\\mathbb{E}[v_i]}{\\operatorname{Var}(G)} = 1 - \\frac{\\mathbb{E}[v_i]}{2p(1-p)} $$\n\n4.  **Wald ($Z$) Statistic and Power**:\n    The Wald statistic for testing $H_0: \\beta=0$ is $Z = \\hat{\\beta} / \\operatorname{SE}(\\hat{\\beta})$. Power is determined by the non-centrality parameter of the test, which for large $n$ is $\\lambda = \\mathbb{E}[\\hat{\\beta}] / \\operatorname{SE}(\\hat{\\beta})$.\n    -   **With true genotypes ($G$)**: The estimator $\\hat{\\beta}_G$ is unbiased for $\\beta$. The standard error is $\\operatorname{SE}(\\hat{\\beta}_G) \\approx \\sqrt{\\frac{\\sigma^2}{n \\operatorname{Var}(G)}}$. The squared non-centrality parameter is:\n        $$ \\lambda_G^2 = \\frac{\\beta^2}{\\operatorname{Var}(\\hat{\\beta}_G)} \\approx \\frac{n \\beta^2 \\operatorname{Var}(G)}{\\sigma^2} $$\n    -   **With imputed dosages ($D$)**: The estimator $\\hat{\\beta}_D$ is also unbiased for $\\beta$. The standard error used for testing $H_0$ is estimated under the null hypothesis $\\beta=0$. In the fitted model $Y_i = \\alpha + \\beta D_i + u_i$, the error variance under the null is simply $\\operatorname{Var}(\\varepsilon_i)=\\sigma^2$. The predictor variance is $\\operatorname{Var}(D)$. So, the standard error is $\\operatorname{SE}(\\hat{\\beta}_D) \\approx \\sqrt{\\frac{\\sigma^2}{n \\operatorname{Var}(D)}}$. The squared non-centrality parameter is:\n        $$ \\lambda_D^2 = \\frac{\\beta^2}{\\operatorname{Var}(\\hat{\\beta}_D)} \\approx \\frac{n \\beta^2 \\operatorname{Var}(D)}{\\sigma^2} $$\n    -   **Comparison**: Let's find the ratio of the non-centrality parameters.\n        $$ \\frac{\\lambda_D}{\\lambda_G} = \\sqrt{\\frac{n \\beta^2 \\operatorname{Var}(D) / \\sigma^2}{n \\beta^2 \\operatorname{Var}(G) / \\sigma^2}} = \\sqrt{\\frac{\\operatorname{Var}(D)}{\\operatorname{Var}(G)}} = \\sqrt{R^2} $$\n    The expected value of the $Z$-statistic under the alternative hypothesis (i.e., its non-centrality) is attenuated by a factor of $\\sqrt{R^2}$. The test statistic's variance under the null is still $1$, but its mean under the alternative is shifted less, leading to a loss of power.\n    -   **Effective Sample Size ($n_{\\text{eff}}$)**: We can see how power is lost by rewriting $\\lambda_D^2$:\n        $$ \\lambda_D^2 = \\frac{n \\beta^2 (R^2 \\operatorname{Var}(G))}{\\sigma^2} = \\frac{(n R^2) \\beta^2 \\operatorname{Var}(G)}{\\sigma^2} $$\n    This shows that a study of size $n$ using dosages has the same power (non-centrality parameter) as a study of size $n_{\\text{eff}} = n R^2$ using true genotypes.\n\n### Option-by-Option Analysis\n\n**A. In OLS of $Y$ on $D$, the slope estimator for $\\beta$ is unbiased. Relative to using true genotypes, the expected Wald or $Z$-statistic for testing $H_0:\\beta=0$ is attenuated by a factor $\\sqrt{R^2}$, where $R^2 = 1 - \\mathbb{E}[v_i]/(2p(1-p))$.**\n-   **Unbiased estimator**: As derived in section 2, the OLS estimator $\\hat{\\beta}_D$ is asymptotically unbiased for $\\beta$. **Correct**.\n-   **Z-statistic attenuation**: As derived in section 4, the non-centrality parameter of the Z-statistic is attenuated by $\\sqrt{R^2}$. This corresponds to the attenuation of the expected statistic. **Correct**.\n-   **$R^2$ formula**: As derived in section 3, $R^2 = 1 - \\mathbb{E}[v_i]/\\operatorname{Var}(G) = 1 - \\mathbb{E}[v_i]/(2p(1-p))$. **Correct**.\nAll parts of the statement are consistent with our derivations.\n**Verdict: Correct**\n\n**B. Modeling dosage uncertainty as $D_i = G_i + \\epsilon_i$ with $\\operatorname{Var}(\\epsilon_i) = v_i$ implies $\\operatorname{Var}(D) = \\operatorname{Var}(G) + \\mathbb{E}[v_i]$, so imputation uncertainty inflates the predictor variance and therefore increases power.**\n-   The relationship $\\operatorname{Var}(D) = \\operatorname{Var}(G) + \\mathbb{E}[v_i]$ is incorrect. Our derivation from the law of total variance shows $\\operatorname{Var}(G) = \\operatorname{Var}(D) + \\mathbb{E}[v_i]$, which implies $\\operatorname{Var}(D) = \\operatorname{Var}(G) - \\mathbb{E}[v_i]$. Imputation uncertainty *reduces* the predictor variance.\n-   Because predictor variance is reduced, power is *decreased*, not increased. The statement's premise and conclusion are both flawed.\n**Verdict: Incorrect**\n\n**C. The imputation quality metric $R^2$ equals $\\operatorname{Corr}(D,G)$ (not squared), and the association $Z$-statistic is reduced by a factor $R^2$.**\n-   The metric $R^2$ is the *squared* correlation. From section 3, we showed $\\operatorname{Corr}(D,G) = \\sqrt{\\frac{\\operatorname{Var}(D)}{\\operatorname{Var}(G)}} = \\sqrt{R^2}$. So the statement that $R^2 = \\operatorname{Corr}(D,G)$ is false, unless $R^2=1$ or $R^2=0$.\n-   The association $Z$-statistic is attenuated by a factor of $\\sqrt{R^2}$, not $R^2$. The chi-squared statistic, $Z^2$, is attenuated by a factor of $R^2$. Both claims are false.\n**Verdict: Incorrect**\n\n**D. Filtering variants by an $R^2$ threshold necessarily biases effect estimates toward zero because retained variants have $\\operatorname{Var}(D) < \\operatorname{Var}(G)$.**\n-   As shown in section 2, the OLS estimator for $\\beta$ is unbiased, irrespective of the value of $R^2$ (as long as $R^2>0$). Filtering on $R^2$ does not introduce bias into the effect estimate itself. This is different from filtering on statistical significance (p-value), which can cause winner's curse bias.\n-   While the premise \"retained variants have $\\operatorname{Var}(D) < \\operatorname{Var}(G)$\" is true for any imperfectly imputed variant, it is the cause of power loss, not bias in $\\hat{\\beta}$. The conclusion is incorrect.\n**Verdict: Incorrect**\n\n**E. By the law of total variance, $\\operatorname{Var}(G) = \\operatorname{Var}(D) + \\mathbb{E}[v_i]$, which implies $R^2 = \\operatorname{Var}(D)/\\operatorname{Var}(G)$; therefore, tests using dosages have an effective sample size $n_{\\text{eff}} = R^2 n$.**\n-   The variance decomposition $\\operatorname{Var}(G) = \\operatorname{Var}(D) + \\mathbb{E}[v_i]$ is a direct application of the law of total variance, as shown in section 1. **Correct**.\n-   The implication $R^2 = \\operatorname{Var}(D)/\\operatorname{Var(G)}$ is the standard definition of imputation $R^2$ and follows from the definition of correlation and our derived property $\\operatorname{Cov}(G,D)=\\operatorname{Var}(D)$, as shown in section 3. **Correct**.\n-   The effective sample size $n_{\\text{eff}} = R^2 n$ is a direct consequence of the effect of imputation on the non-centrality parameter of the test statistic, as shown in section 4. **Correct**.\nAll parts of the statement are correct and logically connected.\n**Verdict: Correct**\n\n**F. Because $R^2$ quantifies the fraction of genotype variance captured by dosages, a principled practice is to filter variants with low $R^2$ (for example, below $0.3$ in large samples or below $0.8$ for rare variants), since they contribute $n_{\\text{eff}} \\approx R^2 n$ and have limited power without improving inference quality.**\n-   The statement correctly identifies $R^2$ as the fraction of variance captured ($R^2 = \\operatorname{Var}(D)/\\operatorname{Var}(G)$).\n-   It correctly states that the effective sample size is approximately $R^2 n$.\n-   Based on this mathematical fact, it concludes that variants with very low $R^2$ have severely limited power. For example, a variant with $R^2=0.1$ has only $10\\%$ of the effective sample size of a perfectly genotyped variant.\n-   Therefore, filtering such variants is a logical, \"principled\" strategy to avoid wasting statistical power on the multiple testing burden they impose while offering little information. The given thresholds are illustrative of this principle. The statement correctly links the mathematical derivations to sound scientific practice.\n**Verdict: Correct**", "answer": "$$\\boxed{AEF}$$", "id": "4562182"}, {"introduction": "Gene expression data from RNA-sequencing often exhibit complex features like overdispersion and an excess of zeros, particularly for lowly expressed genes. This practice challenges you to think beyond simple linear models and consider how these features can bias eQTL discovery if not handled correctly. By comparing naive approaches to principled statistical models [@problem_id:4562156], you will develop the skills to robustly analyze real-world transcriptomic data.", "problem": "An investigator performs expression quantitative trait loci (eQTL) mapping on bulk ribonucleic acid sequencing (RNA-seq) data for a lowly expressed gene across $n$ individuals. Let $G_i \\in \\{0,1,2\\}$ be the genotype dosage for a single biallelic variant for individual $i$, let $L_i > 0$ denote the library size (sequencing depth) for individual $i$, and let $Y_i \\in \\{0,1,2,\\dots\\}$ be the observed read count for the gene. Suppose that the data generating process for $Y_i$ is governed by two well-tested base facts: (i) transcript counts arise by sampling molecules, which under homogeneous sampling are approximately Poisson with mean proportional to true abundance and library size, and (ii) biological overdispersion relative to Poisson sampling can be captured by a Negative Binomial distribution. Further suppose many observations are zero due to a mixture of biological absence and technical sensitivity limits, leading to zero inflation.\n\nModel this by a two-component mixture: with probability $\\pi_i \\in (0,1)$, $Y_i = 0$; otherwise, $Y_i \\sim \\mathrm{NB}(\\mu_i,\\kappa)$, where $\\mathrm{NB}(\\mu,\\kappa)$ denotes a Negative Binomial with mean $\\mu$ and dispersion parameter $\\kappa>0$. Let the true abundance depend on genotype and library size through a log link with an offset,\n$$\n\\log \\mu_i \\;=\\; \\alpha \\;+\\; \\beta G_i \\;+\\; \\log L_i,\n$$\nand allow the zero-inflation probability $\\pi_i$ to depend on sample-level features such as sensitivity or composition, potentially also on $G_i$ through confounding. An analyst, unaware of zero inflation, considers two naive strategies commonly seen in practice: (a) fit a standard Negative Binomial generalized linear model (GLM) for $Y_i$ with log link and offset $\\log L_i$, ignoring $\\pi_i$, and (b) log-transform counts as $\\log(1+Y_i)$ and fit a homoscedastic Gaussian linear regression on $G_i$.\n\nStarting from the base sampling model (Poisson sampling leading to Negative Binomial through overdispersion, and mixture-induced zero inflation by sensitivity limits), reason about how zero inflation affects eQTL detection, focusing on bias, variance, power, and calibration of type I error under the naive strategies when $\\pi_i$ is large and may vary with $G_i$. Then, propose statistically principled modeling strategies that directly address zero inflation while preserving genotype effect interpretability on abundance and controlling for library size and batch effects.\n\nWhich of the following statements are correct?\n\nA. When many observations are zero, zero inflation reduces power to detect genotype effects and can induce spurious associations if $\\pi_i$ varies with $G_i$ or correlates with $G_i$ via confounders; a principled remedy is a two-part (hurdle) model that regresses $G_i$ on the zero indicator via a logistic link and on positive counts via a Negative Binomial mean, including an offset $\\log L_i$ and random effects for batch.\n\nB. Zero inflation primarily causes upward bias in the estimated genotype effect under the log-normal regression on $\\log(1+Y_i)$, thereby increasing power while maintaining valid type I error; the most effective strategy is to drop all samples with $Y_i=0$.\n\nC. A standard Negative Binomial GLM for $Y_i$ with a fixed dispersion and the addition of a small pseudocount before log-transform completely resolves zero inflation; there is no need to explicitly model the zero probability $\\pi_i$.\n\nD. When zeros are driven by a detection limit rather than a structural absence, they can be represented as censoring at a threshold; a censored (Tobit) regression on log-abundance with genotype effects and sample-level random intercepts can improve calibration relative to naive Gaussian regression, especially when combined with an offset for $\\log L_i$.\n\nE. Rank-based eQTL tests using Spearman correlation on transcripts per million (TPM) and ignoring library size offsets are automatically robust to zero inflation and maximize power in the presence of many zeros.\n\nSelect all that apply.", "solution": "We begin from the base facts: RNA-seq counts can be modeled by Poisson sampling of molecules, with biological overdispersion captured by a Negative Binomial (NB). When many observed counts are zero due to detection limits or true absence, a mixture model introduces zero inflation: with probability $\\pi_i$, $Y_i=0$, and with probability $1-\\pi_i$, $Y_i \\sim \\mathrm{NB}(\\mu_i,\\kappa)$, where the mean $\\mu_i$ scales with library size $L_i$ and true abundance. Genotype effects enter the log-mean abundance via $\\log \\mu_i = \\alpha + \\beta G_i + \\log L_i$. The zero probability $\\pi_i$ may depend on sensitivity, composition, and potentially on $G_i$ through confounding (for example, if $G_i$ correlates with cell type composition that influences detection).\n\nImpact of zero inflation on naive models:\n\n- Expectation and variance under the mixture: \n$$\n\\mathbb{E}[Y_i \\mid G_i] \\;=\\; (1-\\pi_i)\\mu_i,\\quad \n\\mathrm{Var}(Y_i \\mid G_i) \\;=\\; (1-\\pi_i)\\left(\\mu_i + \\frac{\\mu_i^2}{\\kappa}\\right) + \\pi_i(1-\\pi_i)\\mu_i^2.\n$$\nRelative to a pure NB with mean $\\mu_i$, zero inflation reduces the mean to $(1-\\pi_i)\\mu_i$ and inflates variance by the term $\\pi_i(1-\\pi_i)\\mu_i^2$. If $\\pi_i$ is large, the variance can be substantially inflated.\n\n- Naive NB GLM ignoring zero inflation: Fitting $Y_i \\sim \\mathrm{NB}(\\tilde\\mu_i,\\tilde\\kappa)$ with $\\log \\tilde\\mu_i = \\tilde\\alpha + \\tilde\\beta G_i + \\log L_i$ forces extra zeros to be explained by dispersion $\\tilde\\kappa$ or by a smaller mean. In the Kullbackâ€“Leibler sense, the pseudo-true parameters satisfy\n$$\n\\tilde\\mu_i \\approx (1-\\pi_i)\\mu_i,\n$$\nup to adjustments absorbed by $\\tilde\\kappa$. If $\\pi_i$ varies across $G_i$, then $\\log \\tilde\\mu_i \\approx \\alpha + \\beta G_i + \\log L_i + \\log(1-\\pi_i)$, so the regression slope with respect to $G_i$ becomes $\\beta + \\frac{\\partial}{\\partial G_i}\\log(1-\\pi_i)$, conflating abundance effects with zero-probability effects. Even when $\\pi_i$ is constant, the effective mean is attenuated by $(1-\\pi_i)$, which reduces signal strength and hence power. If $\\pi_i$ correlates with $G_i$ due to confounders (for example, batch or cell composition), the naive NB GLM can exhibit spurious associations because $\\log(1-\\pi_i)$ varies with $G_i$.\n\n- Naive Gaussian regression on $\\log(1+Y_i)$: Let $T_i=\\log(1+Y_i)$. Under the mixture,\n$$\n\\mathbb{E}[T_i \\mid G_i] \\;=\\; \\pi_i\\log(1+0) + (1-\\pi_i)\\,\\mathbb{E}\\big[\\log(1+Y_i) \\mid Y_i \\sim \\mathrm{NB}(\\mu_i,\\kappa)\\big] \\;=\\; (1-\\pi_i)\\,\\mathbb{E}\\big[\\log(1+Y_i)\\big].\n$$\nA delta-method approximation yields\n$$\n\\mathbb{E}[\\log(1+Y_i)] \\approx \\log(1+\\mu_i) - \\frac{\\mathrm{Var}(Y_i)}{2(1+\\mu_i)^2}\n$$\nwhen the NB variance is moderate. Differentiating with respect to $G_i$,\n$$\n\\frac{\\partial}{\\partial G_i}\\,\\mathbb{E}[T_i \\mid G_i] \\;\\approx\\; (1-\\pi_i)\\,\\frac{\\partial}{\\partial G_i}\\log(1+\\mu_i) \\;+\\; \\text{terms from variance} \\;-\\; \\Big(\\frac{\\partial \\pi_i}{\\partial G_i}\\Big)\\,\\mathbb{E}[\\log(1+Y_i)].\n$$\nIf $\\pi_i$ is constant, the slope is scaled by $(1-\\pi_i)$ relative to the ideal log-scale slope, yielding attenuation. In addition, heteroscedasticity arises because the variance of $T_i$ depends on $\\mu_i$ and $\\pi_i$; ordinary least squares with homoscedastic errors produces mis-calibrated standard errors. If $\\frac{\\partial \\pi_i}{\\partial G_i} \\neq 0$, the second term can induce apparent genotype effects even when $\\beta=0$, causing type I error inflation. Dropping all zeros (restricting to $Y_i>0$) changes the estimand to the conditional mean among positives and introduces selection bias if positivity depends on $G_i$ or confounders; it also reduces sample size and discards information about the zero mechanism.\n\nPrincipled modeling strategies that follow from the base process:\n\n- Two-part hurdle model: Let $Z_i = \\mathbb{I}\\{Y_i>0\\}$. Model\n$$\n\\mathrm{logit}\\,\\Pr(Z_i = 1 \\mid G_i, L_i, \\text{batch}) \\;=\\; \\gamma_0 + \\gamma_1 G_i + \\delta^\\top \\text{batch}_i + \\eta \\log L_i,\n$$\nand, conditional on $Z_i=1$, model $Y_i \\mid Z_i=1 \\sim \\mathrm{NB}(\\mu_i,\\kappa)$ with\n$$\n\\log \\mu_i \\;=\\; \\alpha + \\beta G_i + \\log L_i + b_i,\n$$\nwhere $b_i$ is a random intercept for batch or sample effects. This separates the zero-generating mechanism from the abundance among positives, avoids conflation of $\\beta$ with $\\pi_i$, and properly accounts for $\\log L_i$ via an offset. Testing genotype effects can be performed on $\\gamma_1$ (presence/absence) and on $\\beta$ (abundance), with multiple-testing control across genes and variants.\n\n- Zero-inflated Negative Binomial (ZINB): Alternatively, model $\\pi_i$ and $\\mu_i$ jointly within a ZINB; include $G_i$ (and $\\log L_i$) in both parts. This directly reflects the mixture and can improve calibration and power when zeros reflect both technical and biological sources.\n\n- Censored (Tobit) regression on log-abundance: If zeros arise primarily from a limit-of-detection, then $A_i=\\log(\\text{abundance}_i)$ is observed only if $A_i>\\tau$; otherwise, it is censored at $\\tau$. A Tobit model,\n$$\nA_i^\\ast \\;=\\; \\alpha + \\beta G_i + \\log L_i + b_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2),\n$$\nwith observation rule $A_i = \\max(A_i^\\ast,\\tau)$, treats zeros as censored rather than structural. Including sample-level random intercepts $b_i$ and depth offset $\\log L_i$ improves calibration relative to naive Gaussian regression on $\\log(1+Y_i)$ when detection limits dominate.\n\n- Robust variance estimation and offsets: Regardless of the chosen mean model, including $\\log L_i$ as an offset and using random effects for batch reduce confounding. Robust (sandwich) standard errors address heteroscedasticity but do not remove bias from ignoring $\\pi_i$.\n\nOption-by-option analysis:\n\nA. This statement aligns with the derivations. Zero inflation reduces the effective mean to $(1-\\pi_i)\\mu_i$, inflates variance, and thus reduces power. If $\\pi_i$ varies with $G_i$ or correlates through confounders, naive models conflate $\\beta$ with $\\pi_i$, producing spurious associations. A two-part hurdle model with a logistic component for zeros and an NB component for positive counts, including $\\log L_i$ as an offset and batch random effects, directly targets the mixture structure and preserves interpretability. Verdict: Correct.\n\nB. The claim that zero inflation primarily causes upward bias and maintains valid type I error under log-normal regression is contradicted by the delta-method analysis: the slope is attenuated by $(1-\\pi_i)$ when $\\pi_i$ is constant, and type I error can be inflated when $\\pi_i$ varies with $G_i$. Dropping all zeros induces selection bias and discards information, not an optimal strategy. Verdict: Incorrect.\n\nC. Adding a pseudocount and fitting a standard NB GLM does not model $\\pi_i$ explicitly; extra zeros cannot be fully absorbed by dispersion, especially when $\\pi_i$ varies with $G_i$. Pseudocounts change the scale but do not resolve bias or confounding from the zero mechanism. Verdict: Incorrect.\n\nD. Treating zeros as censored is appropriate when they arise from a detection limit. A Tobit model on log-abundance with genotype effects, sample-level random intercepts, and a depth offset addresses censoring and improves calibration relative to naive Gaussian regression. This is a principled alternative when structural zeros are unlikely. Verdict: Correct.\n\nE. Rank-based tests ignore offsets and can be sensitive to ties at zero; without modeling $\\log L_i$ or the zero mechanism, ranks do not automatically confer robustness, and power can be reduced or associations biased when zero patterns correlate with $G_i$. Verdict: Incorrect.\n\nTherefore, the correct options are A and D.", "answer": "$$\\boxed{AD}$$", "id": "4562156"}]}