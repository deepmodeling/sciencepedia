{"hands_on_practices": [{"introduction": "A common first step in multi-modal analysis is to simply concatenate all available features into a single, massive vector. This exercise [@problem_id:4574869] challenges you to quantify the staggering computational and storage costs of this \"early integration\" strategy in a realistic biomedical setting. By contrasting this with a principled dimensionality reduction approach, you will gain a concrete appreciation for the curse of dimensionality and the critical need for efficient representation learning.", "problem": "A cohort study aims to integrate multi-modal data per patient: a three-dimensional Magnetic Resonance Imaging (MRI) tensor, a whole-transcriptome gene count vector, and a structured clinical covariate table. For each of $n=1200$ patients, the MRI is a voxel grid of size $224 \\times 224 \\times 128$ preprocessed to $32$-bit floating-point intensities, the gene expression vector has length $G=18000$ stored as $32$-bit floating-point values, and the clinical covariate table contains $K=60$ continuous variables stored as $64$-bit floating-point values. A naive integration constructs a single feature vector per patient by concatenating all modalities and casting to $32$-bit floating-point values for computational uniformity. Consider training a regularized generalized linear model with first-order methods where each epoch’s dominant cost is two dense matrix–vector products on the integrated design matrix.\n\nStarting from first principles of dense linear algebra and information-preserving transforms, perform the following:\n\n1. Compute the total number of features per patient $P$ under naive concatenation. Using $4$ bytes per $32$-bit floating-point number, derive the total storage in bytes required to hold the full integrated data matrix of size $n \\times P$, and convert it to gigabytes by dividing by $10^{9}$. State the result in gigabytes and round to four significant figures.\n\n2. Using the definition that one multiplication and one addition count as two floating-point operations, derive the leading-order floating-point operation count per epoch for computing the gradient in this model under naive concatenation. Express the result in terms of $n$ and $P$ and evaluate it numerically.\n\n3. To manage dimensionality in a principled way, assume each high-dimensional modality admits an orthonormal expansion with geometrically decaying energy. Specifically, for MRI, the ordered component energies follow $\\lambda_{r}^{\\text{MRI}} = \\lambda_{0}^{\\text{MRI}} \\alpha_{\\text{MRI}}^{r-1}$ with $\\alpha_{\\text{MRI}} = 0.9997$, and for genes, $\\lambda_{r}^{\\text{GENE}} = \\lambda_{0}^{\\text{GENE}} \\alpha_{\\text{GENE}}^{r-1}$ with $\\alpha_{\\text{GENE}} = 0.98$. For a geometric spectrum with ratio $\\alpha \\in (0,1)$, the fraction of total energy captured by the first $k$ components is $1 - \\alpha^{k}$. Define the per-modality component counts $k_{\\text{MRI}}$ and $k_{\\text{GENE}}$ to be the smallest integers that satisfy a fairness principle of equal tail mass across modalities, subject to a sample-size constraint that limits the total number of learnable parameters to keep the parameter-to-sample ratio below $0.1$, that is, $p_{\\max} = \\lfloor 0.1 n \\rfloor$. Formally, choose a shared tail mass $\\tau \\in (0,1)$ such that\n$$\nk_{\\text{MRI}}(\\tau) + k_{\\text{GENE}}(\\tau) = p_{\\max} - K, \\quad \\text{where} \\quad k(\\tau) = \\left\\lceil \\frac{\\ln(\\tau)}{\\ln(\\alpha)} \\right\\rceil,\n$$\nand then set $p_{\\text{final}} = k_{\\text{MRI}} + k_{\\text{GENE}} + K$.\n\n4. Compute the reduced per-epoch floating-point operation count $F_{\\text{reduced}}$ after dimensionality reduction, using the same first-order training cost model. Finally, define the efficiency gain\n$$\nE \\equiv \\frac{F_{\\text{naive}}}{F_{\\text{reduced}}}\n$$\nas the ratio of naive to reduced per-epoch floating-point operations, and evaluate $E$ numerically using the above parameters. Round your final answer for $E$ to four significant figures. The final answer must be reported as a single real number without units.", "solution": "The problem statement is evaluated as scientifically grounded, well-posed, objective, and self-contained. It presents a hypothetical but realistic scenario in bioinformatics involving the integration of multi-modal data and subsequent dimensionality reduction. The mathematical models for spectral decay and the constraints for model complexity are clearly defined and do not violate any scientific or mathematical principles. All necessary parameters are provided. Therefore, the problem is valid, and a solution can be constructed.\n\nThe solution proceeds by addressing the four parts of the problem sequentially.\n\n**1. Total Features and Storage for Naive Concatenation**\n\nFirst, we compute the total number of features per patient, $P$, by summing the features from each modality. All modalities are cast to a uniform $32$-bit floating-point representation.\n\nThe number of features from the MRI data is the total number of voxels:\n$$ P_{\\text{MRI}} = 224 \\times 224 \\times 128 = 6,422,528 $$\n\nThe number of features from the gene expression data is given as $G$:\n$$ P_{\\text{GENE}} = G = 18,000 $$\n\nThe number of clinical covariates is given as $K$:\n$$ P_{\\text{CLIN}} = K = 60 $$\n\nThe total number of features per patient, $P$, under naive concatenation is the sum of features from all modalities:\n$$ P = P_{\\text{MRI}} + P_{\\text{GENE}} + P_{\\text{CLIN}} = 6,422,528 + 18,000 + 60 = 6,440,588 $$\n\nThe full integrated data matrix is of size $n \\times P$, where $n=1200$ is the number of patients. Each element is a $32$-bit floating-point number, which corresponds to $4$ bytes. The total storage in bytes is:\n$$ \\text{Storage}_{\\text{bytes}} = n \\times P \\times 4 = 1200 \\times 6,440,588 \\times 4 = 30,914,822,400 \\text{ bytes} $$\n\nTo convert this to gigabytes (GB), we divide by $10^9$:\n$$ \\text{Storage}_{\\text{GB}} = \\frac{30,914,822,400}{10^9} = 30.9148224 \\text{ GB} $$\n\nRounding to four significant figures, the total storage is $30.91$ GB.\n\n**2. Floating-Point Operation (FLOP) Count for Naive Model**\n\nThe dominant cost per epoch is stipulated to be two dense matrix-vector products on the integrated design matrix of size $n \\times P$. A dense matrix-vector product between an $n \\times P$ matrix and a $P \\times 1$ vector involves approximately $n \\times P$ multiplications and $n \\times P$ additions. With the given definition that one multiplication and one addition count as two floating-point operations, the leading-order cost of one such product is $2nP$ FLOPs.\n\nSince the gradient computation requires two such products, the total leading-order FLOP count per epoch, $F_{\\text{naive}}$, is:\n$$ F_{\\text{naive}} = 2 \\times (2nP) = 4nP $$\n\nSubstituting the values for $n$ and $P$:\n$$ F_{\\text{naive}} = 4 \\times 1200 \\times 6,440,588 = 30,914,822,400 \\text{ FLOPs} $$\n\n**3. Dimensionality Reduction via Fairness Principle**\n\nThe goal is to reduce the dimensionality while adhering to a fairness principle and a sample-size constraint. The maximum number of parameters, $p_{\\max}$, is constrained by the sample size $n=1200$:\n$$ p_{\\max} = \\lfloor 0.1 \\times n \\rfloor = \\lfloor 0.1 \\times 1200 \\rfloor = \\lfloor 120 \\rfloor = 120 $$\n\nThe total number of features in the reduced model, $p_{\\text{final}}$, is the sum of the components kept from MRI ($k_{\\text{MRI}}$), genes ($k_{\\text{GENE}}$), and the clinical covariates ($K$), which are fully retained. The constraint equation is given as:\n$$ k_{\\text{MRI}}(\\tau) + k_{\\text{GENE}}(\\tau) = p_{\\max} - K $$\n\nSubstituting the values for $p_{\\max}$ and $K$:\n$$ k_{\\text{MRI}}(\\tau) + k_{\\text{GENE}}(\\tau) = 120 - 60 = 60 $$\n\nThe number of components $k$ required to capture a fraction $1-\\tau$ of the total energy (i.e., to have a tail mass of $\\tau$) for a modality with spectral decay ratio $\\alpha$ is given by $k(\\tau) = \\left\\lceil \\frac{\\ln(\\tau)}{\\ln(\\alpha)} \\right\\rceil$.\nFor MRI, $\\alpha_{\\text{MRI}} = 0.9997$. For genes, $\\alpha_{\\text{GENE}} = 0.98$.\nThe equation becomes:\n$$ \\left\\lceil \\frac{\\ln(\\tau)}{\\ln(0.9997)} \\right\\rceil + \\left\\lceil \\frac{\\ln(\\tau)}{\\ln(0.98)} \\right\\rceil = 60 $$\n\nWe need to find integer component counts $k_{\\text{MRI}}$ and $k_{\\text{GENE}}$ that sum to $60$ and can be generated from a common tail mass $\\tau$. Let's test integer partitions of $60$. The arguments of the ceiling functions, $\\frac{\\ln \\tau}{\\ln \\alpha}$, are positive since both numerator and denominator are negative.\nLet's try the pair $(k_{\\text{MRI}}, k_{\\text{GENE}}) = (59, 1)$.\nFor $k_{\\text{GENE}} = 1$:\n$$ 0  \\frac{\\ln(\\tau)}{\\ln(0.98)} \\le 1 \\implies \\ln(0.98) \\le \\ln(\\tau)  0 \\implies -0.02020 \\lesssim \\ln(\\tau)  0 $$\nFor $k_{\\text{MRI}} = 59$:\n$$ 58  \\frac{\\ln(\\tau)}{\\ln(0.9997)} \\le 59 \\implies 59 \\ln(0.9997) \\le \\ln(\\tau)  58 \\ln(0.9997) $$\n$$ 59 \\times (-0.000300045) \\le \\ln(\\tau)  58 \\times (-0.000300045) $$\n$$ -0.017703 \\lesssim \\ln(\\tau)  -0.017403 $$\nThe interval for $\\ln(\\tau)$ required for $k_{\\text{MRI}}=59$, which is approximately $[-0.0177, -0.0174)$, is a subset of the interval required for $k_{\\text{GENE}}=1$, which is approximately $[-0.0202, 0)$. Therefore, a value of $\\tau$ exists that satisfies both conditions simultaneously. Thus, we find the unique integer solution to be:\n$$ k_{\\text{MRI}} = 59 $$\n$$ k_{\\text{GENE}} = 1 $$\n\nThe total number of features in the reduced representation is:\n$$ p_{\\text{final}} = k_{\\text{MRI}} + k_{\\text{GENE}} + K = 59 + 1 + 60 = 120 $$\n\n**4. Reduced FLOP Count and Efficiency Gain**\n\nThe per-epoch FLOP count for the reduced model, $F_{\\text{reduced}}$, uses the same cost model but with the reduced feature count $p_{\\text{final}}$:\n$$ F_{\\text{reduced}} = 4 n p_{\\text{final}} = 4 \\times 1200 \\times 120 = 576,000 \\text{ FLOPs} $$\n\nThe efficiency gain, $E$, is the ratio of the naive FLOP count to the reduced FLOP count:\n$$ E = \\frac{F_{\\text{naive}}}{F_{\\text{reduced}}} = \\frac{4nP}{4np_{\\text{final}}} = \\frac{P}{p_{\\text{final}}} $$\n\nSubstituting the values for $P$ and $p_{\\text{final}}$:\n$$ E = \\frac{6,440,588}{120} \\approx 53671.5666... $$\n\nRounding the result to four significant figures gives:\n$$ E \\approx 53670 $$", "answer": "$$ \\boxed{53670} $$", "id": "4574869"}, {"introduction": "Beyond naive concatenation, more principled methods seek a joint low-dimensional space that captures shared information across modalities. This practice [@problem_id:4574857] introduces Canonical Correlation Analysis (CCA), a cornerstone of statistical data integration, preceded by Principal Component Analysis (PCA). You will first assess the intrinsic dimensionality of each modality with PCA and then apply CCA to construct a shared latent space that maximizes inter-modality correlation, providing a much more compact and interpretable representation.", "problem": "You are given a mathematical specification for integrating two modalities, an imaging feature vector and a genomics feature vector, using Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA). For each test case, you will be provided either a generative factor model or explicit covariance matrices. Your task is to compute, for each case: (1) the minimal number of PCA components for imaging and genomics that explain at least a fraction $0.9$ of variance separately, and (2) the minimal number of CCA canonical pairs required to explain at least a fraction $0.9$ of the total squared canonical correlation, which justifies a joint low-dimensional representation.\n\nBase definitions:\n- Principal Component Analysis (PCA): For a zero-mean random vector $x \\in \\mathbb{R}^{p}$ with covariance $\\Sigma_{x} \\in \\mathbb{R}^{p \\times p}$ that is symmetric positive definite, the total variance is $\\operatorname{tr}(\\Sigma_{x})$, equal to the sum of eigenvalues of $\\Sigma_{x}$. Ordering the eigenvalues in non-increasing order as $\\lambda_{1} \\ge \\lambda_{2} \\ge \\cdots \\ge \\lambda_{p}  0$, the minimal number of components $k_{x}$ needed to explain at least a fraction $\\alpha$ of the variance is the smallest integer $k$ such that $\\left(\\sum_{i=1}^{k} \\lambda_{i}\\right) / \\left(\\sum_{i=1}^{p} \\lambda_{i}\\right) \\ge \\alpha$.\n- Canonical Correlation Analysis (CCA): For zero-mean random vectors $x \\in \\mathbb{R}^{p}$, $y \\in \\mathbb{R}^{q}$ with covariances $\\Sigma_{x} \\in \\mathbb{R}^{p \\times p}$, $\\Sigma_{y} \\in \\mathbb{R}^{q \\times q}$ and cross-covariance $\\Sigma_{xy} \\in \\mathbb{R}^{p \\times q}$, canonical correlations are obtained by solving the variational problem that maximizes correlation between linear projections subject to variance normalization. Let $\\Sigma_{x}^{-1/2}$ and $\\Sigma_{y}^{-1/2}$ denote the inverse symmetric square roots of $\\Sigma_{x}$ and $\\Sigma_{y}$. The canonical correlations are the singular values of the whitened cross-covariance $K = \\Sigma_{x}^{-1/2}\\,\\Sigma_{xy}\\,\\Sigma_{y}^{-1/2}$. Let these singular values be $\\rho_{1} \\ge \\rho_{2} \\ge \\cdots \\ge \\rho_{r} \\ge 0$, where $r = \\min(p,q)$. Define the total correlation energy as $\\sum_{i=1}^{r} \\rho_{i}^{2}$. The minimal number of canonical pairs $m$ needed to explain at least a fraction $\\beta$ of the total squared canonical correlations is the smallest integer $m$ such that $\\left(\\sum_{i=1}^{m} \\rho_{i}^{2}\\right) / \\left(\\sum_{i=1}^{r} \\rho_{i}^{2}\\right) \\ge \\beta$. If $\\sum_{i=1}^{r} \\rho_{i}^{2} = 0$, define $m = 0$.\n\nFor all computations in this problem, use $\\alpha = 0.9$ for variance explained in PCA and $\\beta = 0.9$ for the fraction of total squared canonical correlations. Angles are not involved. All outputs are unitless integers.\n\nConstructing covariances:\n- In factor-model cases, imaging is $x = A z + \\epsilon_{x}$ and genomics is $y = B z + \\epsilon_{y}$, where $z \\in \\mathbb{R}^{r}$ has covariance $I_{r}$, $A \\in \\mathbb{R}^{p \\times r}$, $B \\in \\mathbb{R}^{q \\times r}$, and independent noises $\\epsilon_{x} \\sim \\mathcal{N}(0, D_{x})$, $\\epsilon_{y} \\sim \\mathcal{N}(0, D_{y})$ with diagonal $D_{x}$ and $D_{y}$. Then $\\Sigma_{x} = A A^{\\top} + D_{x}$, $\\Sigma_{y} = B B^{\\top} + D_{y}$, and $\\Sigma_{xy} = A B^{\\top}$. In the direct-matrix case, $\\Sigma_{x}$, $\\Sigma_{y}$, and $\\Sigma_{xy}$ are given explicitly.\n\nTest suite:\n- Case $1$ (factor model): $p = 5$, $q = 4$, $r = 2$.\n  - $A = \\begin{bmatrix} 1.2  0.2 \\\\ 0.9  -0.1 \\\\ 0.4  0.8 \\\\ 0.2  0.5 \\\\ 0.1  0.3 \\end{bmatrix}$,\n    $B = \\begin{bmatrix} 1.0  0.0 \\\\ 0.5  0.7 \\\\ 0.2  -0.4 \\\\ 0.0  0.6 \\end{bmatrix}$,\n    $\\operatorname{diag}(D_{x}) = \\begin{bmatrix} 0.3  0.2  0.2  0.1  0.1 \\end{bmatrix}$,\n    $\\operatorname{diag}(D_{y}) = \\begin{bmatrix} 0.4  0.3  0.2  0.2 \\end{bmatrix}$.\n- Case $2$ (factor model): $p = 3$, $q = 3$, $r = 1$.\n  - $A = \\begin{bmatrix} 1.8 \\\\ 0.2 \\\\ 0.1 \\end{bmatrix}$,\n    $B = \\begin{bmatrix} 0.5 \\\\ 0.1 \\\\ 0.05 \\end{bmatrix}$,\n    $\\operatorname{diag}(D_{x}) = \\begin{bmatrix} 0.05  0.6  0.7 \\end{bmatrix}$,\n    $\\operatorname{diag}(D_{y}) = \\begin{bmatrix} 0.9  0.9  0.9 \\end{bmatrix}$.\n- Case $3$ (direct matrices): $p = 4$, $q = 4$.\n  - $\\Sigma_{x} = \\operatorname{diag}\\!\\left(\\begin{bmatrix} 1.0  0.8  0.5  0.2 \\end{bmatrix}\\right)$,\n    $\\Sigma_{y} = \\operatorname{diag}\\!\\left(\\begin{bmatrix} 1.2  0.7  0.6  0.3 \\end{bmatrix}\\right)$,\n    $\\Sigma_{xy} = 0_{4 \\times 4}$.\n\nRequired computations for each case:\n- Compute $k_{x}$, the minimal number of PCA components for imaging needed so that the cumulative variance explained is at least $0.9$.\n- Compute $k_{y}$, the analogous number for genomics.\n- Compute $m$, the minimal number of CCA canonical pairs needed so that the cumulative sum of squared canonical correlations is at least $0.9$ times the total squared canonical correlation; if the total squared canonical correlation is $0$, set $m = 0$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of three integers $[k_{x}, k_{y}, m]$. For the three test cases above, the required output format is\n$[[k_{x}^{(1)},k_{y}^{(1)},m^{(1)}],[k_{x}^{(2)},k_{y}^{(2)},m^{(2)}],[k_{x}^{(3)},k_{y}^{(3)},m^{(3)}]]$.", "solution": "The problem as stated is valid. It is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. The definitions for Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) are standard in multivariate statistics, and the problem of determining an appropriate low-dimensional representation is a cornerstone of multi-modal data integration in fields like bioinformatics.\n\nThe objective is to find the minimum number of components required to explain a specified fraction of variance or correlation for two modalities, denoted as imaging ($x$) and genomics ($y$). For each test case, we must compute three integer values: $k_x$, the number of PCA components for the imaging data; $k_y$, the number of PCA components for the genomics data; and $m$, the number of canonical pairs from CCA for the joint data.\n\nFirst, we address the construction of the covariance matrices for cases specified by a factor model. Given a model where the observed vectors are linear transformations of a common latent variable $z$ with additive noise, such that $x = A z + \\epsilon_{x}$ and $y = B z + \\epsilon_{y}$, and given that the latent variables are standard normal ($\\operatorname{cov}(z) = I_r$) and independent of the noise terms ($\\epsilon_x \\sim \\mathcal{N}(0, D_x)$, $\\epsilon_y \\sim \\mathcal{N}(0, D_y)$), the covariance and cross-covariance matrices are constructed as follows:\n$$ \\Sigma_{x} = \\operatorname{cov}(A z + \\epsilon_{x}) = A \\operatorname{cov}(z) A^{\\top} + \\operatorname{cov}(\\epsilon_{x}) = A I_r A^{\\top} + D_{x} = A A^{\\top} + D_{x} $$\n$$ \\Sigma_{y} = \\operatorname{cov}(B z + \\epsilon_{y}) = B \\operatorname{cov}(z) B^{\\top} + \\operatorname{cov}(\\epsilon_{y}) = B I_r B^{\\top} + D_{y} = B B^{\\top} + D_{y} $$\n$$ \\Sigma_{xy} = \\operatorname{cov}(A z + \\epsilon_{x}, B z + \\epsilon_{y}) = A \\operatorname{cov}(z, z) B^{\\top} = A I_r B^{\\top} = A B^{\\top} $$\nThese matrices, whether constructed from a factor model or given directly, form the basis for all subsequent calculations. The problem statement guarantees they are symmetric positive definite, ensuring all required matrix operations are well-defined.\n\nThe procedure for determining the number of principal components, $k_x$ and $k_y$, is as follows. For a given data vector, say $x \\in \\mathbb{R}^{p}$ with covariance matrix $\\Sigma_x$, PCA seeks a new basis where the features are uncorrelated and variance is maximized along the first few dimensions. The total variance in the data is given by the trace of the covariance matrix, $\\operatorname{tr}(\\Sigma_x)$, which is also equal to the sum of its eigenvalues. Let the eigenvalues of $\\Sigma_x$ be $\\{\\lambda_i\\}_{i=1}^p$, sorted in non-increasing order: $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_p  0$. The fraction of variance explained by the first $k$ principal components is the ratio of the sum of the first $k$ eigenvalues to the total sum. To find the minimal number of components $k_x$ that explain at least a fraction $\\alpha = 0.9$ of the variance, we seek the smallest integer $k$ satisfying:\n$$ \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{p} \\lambda_i} \\ge \\alpha $$\nThis is accomplished algorithmically by performing an eigenvalue decomposition of $\\Sigma_x$, sorting the eigenvalues, computing their cumulative sum, and identifying the first index at which the cumulative sum meets or exceeds the threshold of $0.9$ times the total sum. An identical procedure is applied to $\\Sigma_y$ to find $k_y$.\n\nThe procedure for determining the number of canonical pairs, $m$, involves Canonical Correlation Analysis (CCA). CCA identifies the linear relationships between two sets of variables, $x$ and $y$. It finds pairs of projection vectors, one for each modality, such that the correlation between the projected variables (the canonical variates) is maximized. The strength of these relationships is quantified by the canonical correlations, which are the singular values of the matrix $K = \\Sigma_x^{-1/2} \\Sigma_{xy} \\Sigma_y^{-1/2}$. Here, $\\Sigma_x^{-1/2}$ and $\\Sigma_y^{-1/2}$ are the inverse symmetric square roots of their respective covariance matrices. These can be computed from the eigenvalue decomposition of a matrix $M = V L V^\\top$ as $M^{-1/2} = V L^{-1/2} V^\\top$.\n\nLet the singular values of $K$, i.e., the canonical correlations, be sorted in non-increasing order: $\\rho_1 \\ge \\rho_2 \\ge \\dots \\ge \\rho_r \\ge 0$, where $r = \\min(p, q)$. The square of a canonical correlation, $\\rho_i^2$, represents the proportion of variance in the $i$-th canonical variate of one set that is accounted for by the $i$-th canonical variate of the other set. The total squared canonical correlation, $\\sum_{i=1}^r \\rho_i^2$, provides a measure of the total shared information between the two modalities. To find the minimal number of canonical pairs $m$ needed to explain at least a fraction $\\beta = 0.9$ of this total shared information, we seek the smallest integer $m$ satisfying:\n$$ \\frac{\\sum_{i=1}^{m} \\rho_i^2}{\\sum_{i=1}^{r} \\rho_i^2} \\ge \\beta $$\nAlgorithmically, this involves computing the singular value decomposition of $K$, squaring the singular values, calculating their cumulative sum, and finding the first index where the sum meets or exceeds the threshold of $0.9$ times the total sum. A special case, as defined in the problem, occurs when $\\Sigma_{xy}$ is a zero matrix. In this scenario, $K$ is also a zero matrix, all canonical correlations $\\rho_i$ are zero, and the total squared correlation is zero. By definition, we set $m=0$.\n\nThe implementation will apply this complete computational procedure to each of the three test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases, computing the minimal number of components\n    for PCA (k_x, k_y) and CCA (m) to explain 90% of the relevant variance/correlation.\n    \"\"\"\n\n    # Helper function to compute the inverse symmetric square root of a matrix\n    def inv_sqrtm_sym(matrix: np.ndarray) - np.ndarray:\n        \"\"\"\n        Computes the inverse symmetric square root of a positive definite matrix\n        using eigenvalue decomposition: M^{-1/2} = V L^{-1/2} V^T.\n        \"\"\"\n        eigvals, eigvecs = np.linalg.eigh(matrix)\n        # Ensure eigenvalues are positive to avoid domain errors\n        if np.any(eigvals = 0):\n            raise ValueError(\"Matrix is not positive definite, cannot compute inverse square root.\")\n        inv_sqrt_eigvals = np.diag(1.0 / np.sqrt(eigvals))\n        return eigvecs @ inv_sqrt_eigvals @ eigvecs.T\n\n    # Helper function to compute minimal PCA components\n    def get_pca_components(sigma: np.ndarray, alpha: float) - int:\n        \"\"\"\n        Calculates the minimum number of principal components to explain a given\n        fraction of total variance.\n        \"\"\"\n        eigvals = np.linalg.eigh(sigma)[0]\n        # eigh returns eigenvalues in ascending order, so we reverse for descending\n        eigvals = np.sort(eigvals)[::-1]\n        \n        total_variance = np.sum(eigvals)\n        if np.isclose(total_variance, 0):\n            return 0\n        \n        cumulative_variance = np.cumsum(eigvals)\n        # Find the first index where the cumulative variance exceeds the threshold\n        # The number of components is index + 1.\n        k = np.argmax(cumulative_variance = alpha * total_variance) + 1\n        return int(k)\n\n    # Helper function to compute minimal CCA components\n    def get_cca_components(sigma_x: np.ndarray, sigma_y: np.ndarray, sigma_xy: np.ndarray, beta: float) - int:\n        \"\"\"\n        Calculates the minimum number of canonical pairs to explain a given\n        fraction of the total squared canonical correlation.\n        \"\"\"\n        # If cross-covariance is zero, correlation is zero.\n        if np.allclose(sigma_xy, 0):\n            return 0\n            \n        sigma_x_inv_sqrt = inv_sqrtm_sym(sigma_x)\n        sigma_y_inv_sqrt = inv_sqrtm_sym(sigma_y)\n        \n        k_matrix = sigma_x_inv_sqrt @ sigma_xy @ sigma_y_inv_sqrt\n        \n        # Singular values (rho) are the canonical correlations. We need their squares.\n        rho_squared = np.linalg.svd(k_matrix, compute_uv=False)**2\n        \n        total_sq_corr = np.sum(rho_squared)\n        if np.isclose(total_sq_corr, 0):\n            return 0\n\n        cumulative_sq_corr = np.cumsum(rho_squared)\n        # Find the first index where the cumulative squared correlation exceeds the threshold\n        m = np.argmax(cumulative_sq_corr = beta * total_sq_corr) + 1\n        return int(m)\n\n    # Define test cases from the problem statement\n    test_cases = [\n        {\n            \"type\": \"factor_model\",\n            \"A\": np.array([[1.2, 0.2], [0.9, -0.1], [0.4, 0.8], [0.2, 0.5], [0.1, 0.3]]),\n            \"B\": np.array([[1.0, 0.0], [0.5, 0.7], [0.2, -0.4], [0.0, 0.6]]),\n            \"Dx_diag\": np.array([0.3, 0.2, 0.2, 0.1, 0.1]),\n            \"Dy_diag\": np.array([0.4, 0.3, 0.2, 0.2]),\n        },\n        {\n            \"type\": \"factor_model\",\n            \"A\": np.array([[1.8], [0.2], [0.1]]),\n            \"B\": np.array([[0.5], [0.1], [0.05]]),\n            \"Dx_diag\": np.array([0.05, 0.6, 0.7]),\n            \"Dy_diag\": np.array([0.9, 0.9, 0.9]),\n        },\n        {\n            \"type\": \"direct_matrices\",\n            \"Sigma_x\": np.diag([1.0, 0.8, 0.5, 0.2]),\n            \"Sigma_y\": np.diag([1.2, 0.7, 0.6, 0.3]),\n            \"Sigma_xy\": np.zeros((4,4)),\n        }\n    ]\n\n    alpha = 0.9\n    beta = 0.9\n    results = []\n\n    for case in test_cases:\n        if case[\"type\"] == \"factor_model\":\n            A = case[\"A\"]\n            B = case[\"B\"]\n            Dx = np.diag(case[\"Dx_diag\"])\n            Dy = np.diag(case[\"Dy_diag\"])\n\n            sigma_x = A @ A.T + Dx\n            sigma_y = B @ B.T + Dy\n            sigma_xy = A @ B.T\n        else: # direct_matrices\n            sigma_x = case[\"Sigma_x\"]\n            sigma_y = case[\"Sigma_y\"]\n            sigma_xy = case[\"Sigma_xy\"]\n            \n        k_x = get_pca_components(sigma_x, alpha)\n        k_y = get_pca_components(sigma_y, alpha)\n        m = get_cca_components(sigma_x, sigma_y, sigma_xy, beta)\n        \n        results.append([k_x, k_y, m])\n\n    # Format the final output string exactly as required, without spaces in sublists.\n    final_str_parts = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]\n    final_output = f\"[{','.join(final_str_parts)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "4574857"}, {"introduction": "The most sophisticated integration methods learn how to combine modalities in a data-driven way, optimizing the final representation for a specific predictive task. This practice delves into Multiple Kernel Learning (MKL), a state-of-the-art \"late integration\" framework where each modality is mapped to a similarity space via a kernel function [@problem_id:4574855]. You will implement a complete optimization pipeline to learn the ideal weights for combining these kernels, mastering a flexible approach that allows each data type to contribute to a prediction in a learned, non-linear fashion.", "problem": "You are given small training datasets representing three biomedical modalities per patient: imaging features, genomics features, and clinical features. The goal is to compute positive semidefinite kernel matrices for each modality, center them, and then integrate them by learning a convex combination of modality-specific kernels that maximizes an alignment objective with the label kernel. The alignment objective should be optimized by a constrained gradient-based method. Your program must be a complete, runnable program that performs these steps and outputs the learned kernel weights and the achieved alignment for each test case as specified below.\n\nStart from the following fundamental base and core definitions:\n\n- A kernel matrix is a symmetric positive semidefinite matrix that represents inner products in a Reproducing Kernel Hilbert Space (RKHS). For any valid kernel function $\\kappa$, the Gram matrix $K$ with entries $K_{ij} = \\kappa(x_i, x_j)$ is positive semidefinite. A convex combination of positive semidefinite kernels remains positive semidefinite.\n- For training labels $y \\in \\{\\,-1,+1\\,\\}^n$, the label kernel is $Y = y y^\\top$. Kernel alignment is a normalized Frobenius inner product between two centered kernels and is widely used to quantify the agreement of a learned kernel with a target kernel.\n- Centering a kernel $K \\in \\mathbb{R}^{n \\times n}$ is performed by $K_c = H K H$, where $H = I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top$, $I$ is the $n \\times n$ identity matrix, and $\\mathbf{1}$ is the $n$-vector of ones. Centering ensures that alignment is invariant to mean shifts.\n\nYou must implement the following, from first principles:\n\n- Compute modality-specific kernels as follows:\n  - Imaging: use the Radial Basis Function (RBF) kernel $K^{(\\text{img})}_{ij} = \\exp\\!\\big(-\\gamma_{\\text{img}} \\lVert x^{(\\text{img})}_i - x^{(\\text{img})}_j \\rVert_2^2\\big)$ with specified $\\gamma_{\\text{img}}$.\n  - Genomics: use the linear kernel $K^{(\\text{gen})}_{ij} = \\big(x^{(\\text{gen})}_i\\big)^\\top x^{(\\text{gen})}_j$.\n  - Clinical: use the polynomial kernel $K^{(\\text{clin})}_{ij} = \\big(\\big(x^{(\\text{clin})}_i\\big)^\\top x^{(\\text{clin})}_j + c\\big)^d$ with specified offset $c$ and degree $d$.\n- Center each kernel by $K_c^{(m)} = H K^{(m)} H$, where $m \\in \\{\\text{img}, \\text{gen}, \\text{clin}\\}$.\n- Form a convex combination $K(w) = \\sum_{m} w_m K_c^{(m)}$ with weights $w \\in \\mathbb{R}^3$ constrained to the probability simplex, i.e., $w_m \\ge 0$ for all $m$, and $\\sum_{m} w_m = 1$.\n- Define the centered label kernel $Y_c = H Y H$ and the kernel alignment objective\n  $$A(w) = \\frac{\\langle K(w), Y_c \\rangle_F}{\\lVert K(w) \\rVert_F \\, \\lVert Y_c \\rVert_F}.$$\n  Here $\\langle \\cdot, \\cdot \\rangle_F$ is the Frobenius inner product and $\\lVert \\cdot \\rVert_F$ is the Frobenius norm.\n- Optimize $w$ to maximize $A(w)$ using a projected gradient ascent algorithm onto the simplex (no shortcuts or closed-form solutions may be used): initialize $w$ uniformly in the simplex, compute the gradient of $A(w)$ with respect to $w$, take an ascent step, and project back onto the simplex. Repeat until the maximum number of iterations is reached or the improvement in $A(w)$ falls below a tolerance.\n\nYour program must implement the simplex projection $\\Pi_{\\Delta}$ that solves\n$$\\min_{z \\in \\mathbb{R}^3} \\lVert z - v \\rVert_2^2 \\text{ subject to } z_m \\ge 0, \\ \\sum_{m=1}^3 z_m = 1,$$\nfor an arbitrary input $v \\in \\mathbb{R}^3$.\n\nUse the following training data across $n = 6$ patients. Each row corresponds to a patient indexed by $i \\in \\{\\,1,\\dots,6\\,\\}$, and each column is a feature dimension.\n\n- Imaging features $X^{(\\text{img})} \\in \\mathbb{R}^{6 \\times 3}$:\n  $$\n  \\begin{bmatrix}\n  0.9  1.1  0.7 \\\\\n  1.2  0.8  1.0 \\\\\n  0.1  0.2  0.0 \\\\\n  0.2  0.1  0.2 \\\\\n  1.1  1.0  0.9 \\\\\n  0.0  0.1  0.2\n  \\end{bmatrix}\n  $$\n- Genomics features $X^{(\\text{gen})} \\in \\mathbb{R}^{6 \\times 4}$:\n  $$\n  \\begin{bmatrix}\n  2.0  1.0  0.5  3.0 \\\\\n  2.1  0.9  0.6  2.9 \\\\\n  0.1  0.2  0.1  0.3 \\\\\n  0.2  0.1  0.2  0.2 \\\\\n  1.9  1.0  0.4  3.1 \\\\\n  0.0  0.1  0.0  0.2\n  \\end{bmatrix}\n  $$\n- Clinical features $X^{(\\text{clin})} \\in \\mathbb{R}^{6 \\times 2}$:\n  $$\n  \\begin{bmatrix}\n  0.60  1.00 \\\\\n  0.55  0.00 \\\\\n  0.30  1.00 \\\\\n  0.28  0.00 \\\\\n  0.62  1.00 \\\\\n  0.33  0.00\n  \\end{bmatrix}\n  $$\n- Labels $y \\in \\{\\, -1, +1 \\,\\}^6$:\n  $$\n  \\begin{bmatrix}\n  +1 \\\\\n  +1 \\\\\n  -1 \\\\\n  -1 \\\\\n  +1 \\\\\n  -1\n  \\end{bmatrix}\n  $$\n\nTest Suite. Run three test cases that differ in the kernel hyperparameters and optimization settings. For each test case $t \\in \\{\\,1,2,3\\,\\}$, use the same $X^{(\\text{img})}$, $X^{(\\text{gen})}$, $X^{(\\text{clin})}$, and $y$, but change $\\gamma_{\\text{img}}$, $d$, and $c$, as well as optimization parameters. The test cases are:\n\n- Test case $1$:\n  - $\\gamma_{\\text{img}} = 0.8$\n  - Polynomial degree $d = 2$\n  - Polynomial offset $c = 1.0$\n  - Step size $\\eta = 0.05$\n  - Maximum iterations $T = 1000$\n  - Tolerance $\\epsilon = 10^{-10}$\n- Test case $2$ (boundary case: a constant clinical kernel):\n  - $\\gamma_{\\text{img}} = 8.0$\n  - Polynomial degree $d = 0$\n  - Polynomial offset $c = 1.0$\n  - Step size $\\eta = 0.05$\n  - Maximum iterations $T = 600$\n  - Tolerance $\\epsilon = 10^{-10}$\n- Test case $3$ (edge case: weakly smooth imaging kernel and higher-order clinical kernel):\n  - $\\gamma_{\\text{img}} = 0.1$\n  - Polynomial degree $d = 3$\n  - Polynomial offset $c = 0.5$\n  - Step size $\\eta = 0.08$\n  - Maximum iterations $T = 800$\n  - Tolerance $\\epsilon = 10^{-10}$\n\nRequired outputs. For each test case, after optimization, record the final weight vector $w = [w_{\\text{img}}, w_{\\text{gen}}, w_{\\text{clin}}]$ and the achieved alignment value $A(w)$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a bracketed list of four floats $[w_{\\text{img}},w_{\\text{gen}},w_{\\text{clin}},A]$ in that order. For example:\n$$[ [w_{\\text{img}}^{(1)}, w_{\\text{gen}}^{(1)}, w_{\\text{clin}}^{(1)}, A^{(1)}], [w_{\\text{img}}^{(2)}, w_{\\text{gen}}^{(2)}, w_{\\text{clin}}^{(2)}, A^{(2)}], [w_{\\text{img}}^{(3)}, w_{\\text{gen}}^{(3)}, w_{\\text{clin}}^{(3)}, A^{(3)}] ].$$\n\nAll numerical values in the output must be floating-point numbers. There are no physical units or angle units in this problem. The answers are floats and lists of floats. The optimization must respect the constraints $w_m \\ge 0$ and $\\sum_m w_m = 1$, guaranteeing that $K(w)$ remains positive semidefinite when each $K^{(m)}$ is positive semidefinite. No external files or user input should be used, and the program must be fully self-contained and runnable as is.", "solution": "The problem is assessed to be valid as it is scientifically grounded in established principles of kernel methods, mathematically well-posed, and provides a complete, unambiguous, and formalizable specification. The solution proceeds as follows.\n\nThe objective is to find an optimal convex combination of three modality-specific kernel matrices that maximizes alignment with a target label kernel. This is a common strategy in multi-modal data integration for supervised learning, where one seeks a unified representation (the combined kernel) that is highly congruent with the classification task. The process involves kernel computation, centering, and a constrained optimization via projected gradient ascent.\n\n**Step 1: Modality-Specific Kernel Computation**\n\nFor a dataset of $n$ samples, a kernel matrix $K \\in \\mathbb{R}^{n \\times n}$ stores the pairwise similarity between samples. We compute one such matrix for each of the three data modalities using the provided feature matrices $X^{(\\text{img})} \\in \\mathbb{R}^{6 \\times 3}$, $X^{(\\text{gen})} \\in \\mathbb{R}^{6 \\times 4}$, and $X^{(\\text{clin})} \\in \\mathbb{R}^{6 \\times 2}$.\n\n- **Imaging Kernel ($K^{(\\text{img})}$)**: The Radial Basis Function (RBF) kernel is computed as:\n  $$K^{(\\text{img})}_{ij} = \\exp\\!\\left(-\\gamma_{\\text{img}} \\lVert x^{(\\text{img})}_i - x^{(\\text{img})}_j \\rVert_2^2\\right)$$\n  where $x_i^{(\\text{img})}$ is the feature vector for the $i$-th patient, and $\\gamma_{\\text{img}}  0$ is a hyperparameter controlling the width of the kernel. A larger $\\gamma_{\\text{img}}$ leads to a more localized, peaked similarity measure.\n\n- **Genomics Kernel ($K^{(\\text{gen})}$)**: The linear kernel is simply the Gram matrix of the feature vectors:\n  $$K^{(\\text{gen})}_{ij} = \\left(x^{(\\text{gen})}_i\\right)^\\top x^{(\\text{gen})}_j$$\n  This measures similarity as the dot product of the genomic feature vectors in the input space.\n\n- **Clinical Kernel ($K^{(\\text{clin})}$)**: The polynomial kernel is computed as:\n  $$K^{(\\text{clin})}_{ij} = \\left(\\left(x^{(\\text{clin})}_i\\right)^\\top x^{(\\text{clin})}_j + c\\right)^d$$\n  where $d \\ge 0$ is the degree and $c \\ge 0$ is an offset. This kernel computes similarities in a feature space composed of polynomial combinations of the original clinical features. Note that for the special case $d=0$, the kernel matrix becomes a matrix of ones, $K^{(\\text{clin})}_{ij} = 1$ for all $i,j$.\n\nAll three kernel functions produce symmetric positive semidefinite matrices, a necessary property for valid kernels.\n\n**Step 2: Kernel Centering**\n\nKernel alignment is sensitive to shifts in the origin of the feature space. To ensure the measure is invariant to such shifts, all kernel matrices must be centered. Centering is achieved by applying the centering matrix $H \\in \\mathbb{R}^{n \\times n}$:\n$$H = I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top$$\nwhere $I$ is the $n \\times n$ identity matrix and $\\mathbf{1}$ is the $n$-dimensional column vector of ones. For any kernel $K$, the centered kernel $K_c$ is given by:\n$$K_c = H K H$$\nThis operation effectively maps the data into a new feature space where the mean of the data points is the origin. In the case where $K$ is a matrix of ones (as for Test Case $2$'s clinical kernel), centering results in a zero matrix, as $H\\mathbf{1}\\mathbf{1}^\\top H = (I-\\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top)\\mathbf{1}\\mathbf{1}^\\top(I-\\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top) = (\\mathbf{1}\\mathbf{1}^\\top - \\frac{1}{n}n\\mathbf{1}\\mathbf{1}^\\top)H = (0)H = 0$.\n\n**Step 3: Target Kernel and Integrated Kernel**\n\nThe goal of the supervision is provided by the labels $y \\in \\{-1, +1\\}^n$. These are encoded into a target kernel matrix $Y = y y^\\top$. The entries of $Y$ are $Y_{ij} = y_i y_j$, which is $+1$ if samples $i$ and $j$ have the same class and $-1$ otherwise. This matrix captures the ideal similarity structure according to the labels. Like the data kernels, it is also centered:\n$$Y_c = H Y H$$\nThe integrated kernel $K(w)$ is a convex combination of the centered modality-specific kernels:\n$$K(w) = \\sum_{m=1}^{3} w_m K_c^{(m)} = w_{\\text{img}} K_c^{(\\text{img})} + w_{\\text{gen}} K_c^{(\\text{gen})} + w_{\\text{clin}} K_c^{(\\text{clin})}$$\nThe weights $w = [w_{\\text{img}}, w_{\\text{gen}}, w_{\\text{clin}}]^\\top$ are constrained to the probability simplex $\\Delta_3 = \\{ w \\in \\mathbb{R}^3 \\mid w_m \\ge 0, \\sum_m w_m = 1 \\}$. This constraint ensures that if the base kernels $K_c^{(m)}$ are positive semidefinite, the combined kernel $K(w)$ is also positive semidefinite.\n\n**Step 4: Kernel Alignment and Optimization**\n\nThe alignment between the integrated kernel $K(w)$ and the target kernel $Y_c$ is quantified by the normalized Frobenius inner product:\n$$A(w) = \\frac{\\langle K(w), Y_c \\rangle_F}{\\lVert K(w) \\rVert_F \\, \\lVert Y_c \\rVert_F}$$\nwhere $\\langle A, B \\rangle_F = \\text{Tr}(A^\\top B)$ is the Frobenius inner product and $\\lVert A \\rVert_F = \\sqrt{\\langle A, A \\rangle_F}$ is the Frobenius norm. We seek to find the weights $w$ that maximize this objective function.\n\nWe employ projected gradient ascent. The algorithm iteratively updates the weights in the direction of the gradient of $A(w)$ and then projects them back onto the probability simplex $\\Delta_3$. The gradient of $A(w)$ with respect to a weight $w_m$ is:\n$$\\frac{\\partial A(w)}{\\partial w_m} = \\frac{1}{\\lVert Y_c \\rVert_F \\lVert K(w) \\rVert_F} \\left( \\langle K_c^{(m)}, Y_c \\rangle_F - \\frac{\\langle K(w), Y_c \\rangle_F}{\\lVert K(w) \\rVert_F^2} \\langle K(w), K_c^{(m)} \\rangle_F \\right)$$\nThe update rule at iteration $k$ is:\n$$w'_{k+1} = w_k + \\eta \\nabla_w A(w_k)$$\nwhere $\\eta  0$ is the step size (learning rate).\n\n**Step 5: Projection onto the Probability Simplex**\n\nThe updated weight vector $w'_{k+1}$ may not satisfy the simplex constraints. It must be projected onto $\\Delta_3$ by solving the quadratic program:\n$$w_{k+1} = \\Pi_{\\Delta_3}(w'_{k+1}) = \\arg\\min_{z \\in \\Delta_3} \\lVert z - w'_{k+1} \\rVert_2^2$$\nAn efficient algorithm exists for this projection. First, sort the components of the vector to be projected, $v = w'_{k+1}$, in descending order to get $u$. Then, find the largest index $\\rho$ such that $u_\\rho - \\frac{1}{\\rho}(\\sum_{i=1}^\\rho u_i - 1)  0$. A threshold $\\theta$ is computed as $\\theta = \\frac{1}{\\rho}(\\sum_{i=1}^\\rho u_i - 1)$. The final projected vector $z$ has components $z_i = \\max(v_i - \\theta, 0)$.\n\nThe iterative optimization process begins with uniform weights ($w_m = 1/3$) and continues until the change in the alignment value $A(w)$ between successive iterations falls below a tolerance $\\epsilon$ or a maximum number of iterations $T$ is reached.\n\nThis procedure is applied to each of the three test cases, each with distinct hyperparameters, to determine the optimal modality weights and the corresponding maximum alignment.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Solves the multi-modal kernel alignment optimization problem for three test cases.\n    \"\"\"\n\n    # Step 1: Define problem data\n    X_img = np.array([\n        [0.9, 1.1, 0.7],\n        [1.2, 0.8, 1.0],\n        [0.1, 0.2, 0.0],\n        [0.2, 0.1, 0.2],\n        [1.1, 1.0, 0.9],\n        [0.0, 0.1, 0.2]\n    ])\n\n    X_gen = np.array([\n        [2.0, 1.0, 0.5, 3.0],\n        [2.1, 0.9, 0.6, 2.9],\n        [0.1, 0.2, 0.1, 0.3],\n        [0.2, 0.1, 0.2, 0.2],\n        [1.9, 1.0, 0.4, 3.1],\n        [0.0, 0.1, 0.0, 0.2]\n    ])\n\n    X_clin = np.array([\n        [0.60, 1.00],\n        [0.55, 0.00],\n        [0.30, 1.00],\n        [0.28, 0.00],\n        [0.62, 1.00],\n        [0.33, 0.00]\n    ])\n\n    y = np.array([+1, +1, -1, -1, +1, -1]).reshape(-1, 1)\n    \n    n = y.shape[0]\n\n    # Helper functions for kernel computation\n    def rbf_kernel(X, gamma):\n        sq_dists = squareform(pdist(X, 'sqeuclidean'))\n        return np.exp(-gamma * sq_dists)\n\n    def linear_kernel(X):\n        return X @ X.T\n\n    def poly_kernel(X, c, d):\n        return (X @ X.T + c)**d\n\n    # Helper for kernel centering\n    def center_kernel(K):\n        H = np.eye(n) - (1/n) * np.ones((n, n))\n        return H @ K @ H\n\n    # Helper for simplex projection\n    def project_to_simplex(v):\n        \"\"\"Projects a vector v onto the probability simplex.\"\"\"\n        p = len(v)\n        u = np.sort(v)[::-1]\n        s = np.cumsum(u)\n        rho_vals = np.where(u - (s - 1) / np.arange(1, p + 1)  0)[0]\n        if len(rho_vals) == 0: # This case is unlikely but for robustness\n            return np.ones(p) / p\n        rho = rho_vals[-1]\n        theta = (s[rho] - 1) / (rho + 1)\n        w = np.maximum(v - theta, 0)\n        return w\n\n    # Helper for alignment objective and its gradient\n    def get_alignment(K, Y):\n        # Add a small epsilon for numerical stability\n        norm_K = np.linalg.norm(K, 'fro')\n        norm_Y = np.linalg.norm(Y, 'fro')\n        if norm_K  1e-12 or norm_Y  1e-12:\n            return 0.0\n        return np.trace(K.T @ Y) / (norm_K * norm_Y)\n\n    def get_gradient_alignment(w, centered_kernels, Yc):\n        num_kernels = len(centered_kernels)\n        K_w = np.zeros_like(Yc)\n        for i in range(num_kernels):\n            K_w += w[i] * centered_kernels[i]\n\n        norm_Yc = np.linalg.norm(Yc, 'fro')\n        norm_Kw = np.linalg.norm(K_w, 'fro')\n        \n        # Epsilon for stability\n        eps = 1e-12\n        if norm_Kw  eps or norm_Yc  eps:\n            return np.zeros_like(w)\n\n        term1_factor = 1.0 / (norm_Yc * norm_Kw)\n        term2_factor = np.trace(K_w.T @ Yc) / (norm_Kw**2)\n        \n        grad = np.zeros_like(w)\n        for m in range(num_kernels):\n            Kc_m = centered_kernels[m]\n            term1_m = np.trace(Kc_m.T @ Yc)\n            term2_m = np.trace(K_w.T @ Kc_m)\n            grad[m] = term1_factor * (term1_m - term2_factor * term2_m)\n        \n        return grad\n\n    # Test suite definition\n    test_cases = [\n        {\"gamma_img\": 0.8, \"d\": 2, \"c\": 1.0, \"eta\": 0.05, \"T\": 1000, \"epsilon\": 1e-10},\n        {\"gamma_img\": 8.0, \"d\": 0, \"c\": 1.0, \"eta\": 0.05, \"T\": 600, \"epsilon\": 1e-10},\n        {\"gamma_img\": 0.1, \"d\": 3, \"c\": 0.5, \"eta\": 0.08, \"T\": 800, \"epsilon\": 1e-10},\n    ]\n\n    all_results = []\n\n    # Process each test case\n    for case in test_cases:\n        # Compute modality-specific kernels\n        K_img = rbf_kernel(X_img, case[\"gamma_img\"])\n        K_gen = linear_kernel(X_gen)\n        K_clin = poly_kernel(X_clin, case[\"c\"], case[\"d\"])\n\n        # Center all kernels\n        Kc_img = center_kernel(K_img)\n        Kc_gen = center_kernel(K_gen)\n        Kc_clin = center_kernel(K_clin)\n        \n        # Center label kernel\n        Y = y @ y.T\n        Yc = center_kernel(Y)\n\n        centered_kernels = [Kc_img, Kc_gen, Kc_clin]\n\n        # Optimization via Projected Gradient Ascent\n        w = np.array([1/3, 1/3, 1/3])\n        \n        alignment_old = -1.0\n        for _ in range(case[\"T\"]):\n            K_w = w[0] * Kc_img + w[1] * Kc_gen + w[2] * Kc_clin\n            alignment_new = get_alignment(K_w, Yc)\n\n            if abs(alignment_new - alignment_old)  case[\"epsilon\"]:\n                break\n            alignment_old = alignment_new\n            \n            # Gradient step\n            grad = get_gradient_alignment(w, centered_kernels, Yc)\n            w_unproj = w + case[\"eta\"] * grad\n            \n            # Projection step\n            w = project_to_simplex(w_unproj)\n\n        final_alignment = get_alignment(w[0] * Kc_img + w[1] * Kc_gen + w[2] * Kc_clin, Yc)\n        \n        result = list(w) + [final_alignment]\n        all_results.append(result)\n\n    # Format output as specified\n    output_str = f\"[{','.join([f'[{\",\".join(map(str, r))}]' for r in all_results])}]\"\n    print(output_str)\n\nsolve()\n```", "id": "4574855"}]}