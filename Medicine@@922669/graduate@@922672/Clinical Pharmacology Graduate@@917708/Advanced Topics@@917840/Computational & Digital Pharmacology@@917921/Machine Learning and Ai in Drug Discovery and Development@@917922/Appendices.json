{"hands_on_practices": [{"introduction": "Even as Artificial Intelligence (AI) revolutionizes in silico modeling, fundamental pharmacokinetic (PK) principles remain indispensable. This exercise bridges traditional PK with modern machine learning by focusing on Noncompartmental Analysis (NCA), a cornerstone of clinical pharmacology. You will practice calculating the Area Under the Curve ($AUC$), a critical measure of drug exposure, from simulated concentration-time data, reinforcing how foundational calculations provide the high-quality features needed for sophisticated downstream exposure-response models. [@problem_id:4563937]", "problem": "An Artificial Intelligence (AI) driven in silico clinical trial simulator has produced predicted plasma concentration–time data for a single oral dose of a new small-molecule candidate. In Noncompartmental Analysis (NCA), the Area Under the Curve (AUC) from time zero to the last observation, and the extrapolated AUC to infinity based on the terminal elimination slope, are core exposure metrics used in downstream machine learning (ML) exposure–response modeling in clinical pharmacology.\n\nYou are given the following time–concentration pairs for one subject, where time is in hours (h) and concentration is in milligrams per liter (mg/L):\n- $t = 0$, $C = 0$\n- $t = 0.5$, $C = 6.2$\n- $t = 1$, $C = 9.8$\n- $t = 2$, $C = 12.0$\n- $t = 4$, $C = 5.488116361$\n- $t = 6$, $C = 4.065696597$\n- $t = 8$, $C = 3.011942119$\n- $t = 12$, $C = 1.653297706$\n\nStarting from the fundamental definition of the AUC as the integral $\\int C(t)\\,dt$, compute the noncompartmental $AUC_{0-t_{\\text{last}}}$ using the trapezoidal rule between successive observations. Then, under the assumption that the terminal elimination is first-order (so the natural logarithm of concentration is linear in time in the terminal phase), estimate the terminal elimination rate constant $\\lambda_{z}$ by an ordinary least squares (OLS) linear regression of $\\ln(C)$ on $t$ using the last four points ($t = 4, 6, 8, 12$). Use this $\\lambda_{z}$ to extrapolate the residual area from $t_{\\text{last}}$ to infinity and thereby derive $AUC_{0-\\infty}$.\n\nRound your final numeric result to four significant figures. Express the final AUC in mg·h/L. Use the natural logarithm $\\ln(\\cdot)$ wherever needed.", "solution": "The problem is assessed to be valid as it is scientifically grounded in standard pharmacokinetic principles, well-posed with all necessary data and methods provided, and objective in its language and requirements. The task is to calculate the total Area Under the Curve from time zero to infinity, $AUC_{0-\\infty}$. This quantity is the sum of two components: the AUC from time zero to the last observation time, $AUC_{0-t_{\\text{last}}}$, and the extrapolated AUC from the last observation time to infinity, $AUC_{t_{\\text{last}}-\\infty}$.\n\nFirst, we compute $AUC_{0-t_{\\text{last}}}$ using the linear trapezoidal rule. The formula for the area of a single trapezoid between two time points, $t_i$ and $t_{i+1}$, with corresponding concentrations $C_i$ and $C_{i+1}$, is:\n$$AUC_{i \\to i+1} = \\frac{C_i + C_{i+1}}{2} (t_{i+1} - t_i)$$\nThe total $AUC_{0-t_{\\text{last}}}$ is the sum of the areas of the trapezoids formed by successive data points. The given data points are $(t, C)$: $(0, 0)$, $(0.5, 6.2)$, $(1, 9.8)$, $(2, 12.0)$, $(4, 5.488116361)$, $(6, 4.065696597)$, $(8, 3.011942119)$, and $(12, 1.653297706)$. The last observation time is $t_{\\text{last}} = 12$ h.\n\nThe individual trapezoidal areas are calculated as follows:\n- For the interval $[0, 0.5]$:\n$AUC_{0-0.5} = \\frac{0 + 6.2}{2} (0.5 - 0) = 3.1 \\times 0.5 = 1.55$\n- For the interval $[0.5, 1]$:\n$AUC_{0.5-1} = \\frac{6.2 + 9.8}{2} (1 - 0.5) = 8.0 \\times 0.5 = 4.0$\n- For the interval $[1, 2]$:\n$AUC_{1-2} = \\frac{9.8 + 12.0}{2} (2 - 1) = 10.9 \\times 1 = 10.9$\n- For the interval $[2, 4]$:\n$AUC_{2-4} = \\frac{12.0 + 5.488116361}{2} (4 - 2) = 8.7440581805 \\times 2 = 17.488116361$\n- For the interval $[4, 6]$:\n$AUC_{4-6} = \\frac{5.488116361 + 4.065696597}{2} (6 - 4) = 4.776906479 \\times 2 = 9.553812958$\n- For the interval $[6, 8]$:\n$AUC_{6-8} = \\frac{4.065696597 + 3.011942119}{2} (8 - 6) = 3.538819358 \\times 2 = 7.077638716$\n- For the interval $[8, 12]$:\n$AUC_{8-12} = \\frac{3.011942119 + 1.653297706}{2} (12 - 8) = 2.3326199125 \\times 4 = 9.33047965$\n\nSumming these individual areas gives $AUC_{0-t_{\\text{last}}}$:\n$$AUC_{0-t_{\\text{last}}} = 1.55 + 4.0 + 10.9 + 17.488116361 + 9.553812958 + 7.077638716 + 9.33047965$$\n$$AUC_{0-t_{\\text{last}}} = 59.900047685 \\, \\text{mg} \\cdot \\text{h/L}$$\n\nNext, we estimate the terminal elimination rate constant, $\\lambda_z$. We are instructed to use ordinary least squares (OLS) linear regression of the natural logarithm of concentration, $\\ln(C)$, on time, $t$, for the last four data points. The model is $\\ln(C) = \\ln(C_{\\text{intercept}}) - \\lambda_z t$. The slope of this line is $m = -\\lambda_z$.\n\nThe data points for the regression are:\n- $t_4 = 4$, $C_4 = 5.488116361 \\implies \\ln(C_4) \\approx 1.70265$\n- $t_5 = 6$, $C_5 = 4.065696597 \\implies \\ln(C_5) \\approx 1.40265$\n- $t_6 = 8$, $C_6 = 3.011942119 \\implies \\ln(C_6) \\approx 1.10265$\n- $t_7 = 12$, $C_7 = 1.653297706 \\implies \\ln(C_7) \\approx 0.50265$\nLet the regression points be $(x_j, y_j)$, where $x_j=t_j$ and $y_j=\\ln(C_j)$. The points are $(4, 1.70265)$, $(6, 1.40265)$, $(8, 1.10265)$, and $(12, 0.50265)$. A quick check reveals that these points are perfectly collinear. The slope between any two points is constant:\n$$m = \\frac{1.40265 - 1.70265}{6 - 4} = \\frac{-0.3}{2} = -0.15$$\n$$m = \\frac{0.50265 - 1.10265}{12 - 8} = \\frac{-0.6}{4} = -0.15$$\nSince the points lie on a perfect line, the OLS regression will yield this exact slope. Thus, the slope $m = -0.15$. The terminal elimination rate constant is $\\lambda_z = -m = 0.15$ h$^{-1}$.\n\nNow, we calculate the extrapolated area from $t_{\\text{last}}$ to infinity, $AUC_{t_{\\text{last}}-\\infty}$. The standard formula for this extrapolation is:\n$$AUC_{t_{\\text{last}}-\\infty} = \\frac{C_{\\text{last}}}{\\lambda_z}$$\nwhere $C_{\\text{last}}$ is the last observed concentration. Here, $t_{\\text{last}} = 12$ and $C_{\\text{last}} = C(12) = 1.653297706$ mg/L.\n$$AUC_{12-\\infty} = \\frac{1.653297706}{0.15} \\approx 11.02198471 \\, \\text{mg} \\cdot \\text{h/L}$$\nSince the terminal data points are perfectly collinear, the last observed concentration $C_{\\text{last}}$ is identical to the concentration at $t_{\\text{last}}$ predicted by the regression line, which avoids any ambiguity in the formula.\n\nFinally, we compute the total area under the curve, $AUC_{0-\\infty}$, by summing the two components:\n$$AUC_{0-\\infty} = AUC_{0-t_{\\text{last}}} + AUC_{t_{\\text{last}}-\\infty}$$\n$$AUC_{0-\\infty} = 59.900047685 + 11.02198471 = 70.922032395 \\, \\text{mg} \\cdot \\text{h/L}$$\n\nThe problem requires rounding the final result to four significant figures. The value is $70.922032395$. The fifth significant digit is $2$, so we round down.\nThe final result is $70.92$.", "answer": "$$\n\\boxed{70.92}\n$$", "id": "4563937"}, {"introduction": "The performance of a machine learning model is not only dependent on the algorithm but also on how the input data is represented. Physicochemical descriptors of molecules, such as lipophilicity ($\\log_{10} P$) and polar surface area (PSA), often have vastly different numerical scales and units, which can unintentionally bias a model. This practice demonstrates the critical data preprocessing step of normalization, guiding you to derive and apply the standard scaling (or Z-score) transformation, which is essential for building robust and interpretable quantitative structure-property relationship (QSPR) models. [@problem_id:4563981]", "problem": "A quantitative structure–property modeling task in Clinical Pharmacology uses Machine Learning (ML) and Artificial Intelligence (AI) to predict the fraction unbound in plasma from physicochemical descriptors. Three descriptors are considered: acid dissociation constant ($pK_{a}$), octanol–water partition coefficient ($\\log_{10} P$) and polar surface area (PSA). The training dataset for the model comprises molecules with descriptor components that are approximately unimodal and symmetric after routine curation. You are asked to construct a normalization scheme that enables a downstream linear model to treat each descriptor on a comparable scale, avoiding implicit weighting due to differing units and ranges.\n\nBegin from core definitions of sample mean and sample standard deviation, and general principles of affine transformations of random variables. Explain, by examining the physical units and typical ranges of $pK_{a}$ (dimensionless logarithmic constant), $\\log_{10} P$ (dimensionless logarithmic ratio), and PSA (an area measured in square angstroms, $\\text{\\AA}^{2}$), why a per-descriptor transformation that centers and scales each component to unitless zero mean and unit variance is appropriate for many ML algorithms in drug discovery and development. Then derive, from first principles, the affine transformation that maps any descriptor component to have zero training-set mean and unit training-set variance.\n\nYou are provided training-set summary statistics and a new compound’s descriptor vector:\n- Training-set means: $\\mu_{pK_{a}} = 6.80$, $\\mu_{\\log_{10} P} = 2.50$, $\\mu_{\\mathrm{PSA}} = 85.0$ (PSA in $\\text{\\AA}^{2}$).\n- Training-set standard deviations: $\\sigma_{pK_{a}} = 1.20$, $\\sigma_{\\log_{10} P} = 1.10$, $\\sigma_{\\mathrm{PSA}} = 30.0$ (PSA in $\\text{\\AA}^{2}$).\n- New compound descriptors: $x_{pK_{a}} = 7.40$, $x_{\\log_{10} P} = 3.20$, $x_{\\mathrm{PSA}} = 78.0$ (PSA in $\\text{\\AA}^{2}$).\n\nApply your derived transformation componentwise to the new compound’s descriptor vector to obtain a standardized row vector. Round each component to four significant figures. Express the final standardized vector as a row matrix. The standardized components are unitless; do not include units in your final expression.", "solution": "The problem statement is rigorously validated and found to be valid. It is scientifically grounded in the fields of quantitative structure–property relationship (QSPR) modeling, clinical pharmacology, and machine learning. The problem is well-posed, objective, self-contained, and describes a standard, realistic task in computational drug discovery. All necessary data are provided, and there are no internal contradictions or scientifically unsound premises.\n\nThe task is to justify and derive a normalization scheme for physicochemical descriptors and apply it to a new data point. The descriptors are the acid dissociation constant ($pK_{a}$), the octanol–water partition coefficient ($\\log_{10} P$), and the polar surface area (PSA). These are used to predict the fraction of a drug unbound in plasma. The necessity for such a scheme arises from the use of a downstream linear model.\n\nA linear model's prediction, $\\hat{y}$, is a weighted sum of the input features $x_i$:\n$$\n\\hat{y} = \\beta_0 + \\sum_{i=1}^{d} \\beta_i x_i\n$$\nwhere $d$ is the number of descriptors, and the coefficients $\\beta_i$ are learned from the training data. The magnitude of a coefficient $\\beta_i$ is contingent on the scale of its corresponding feature $x_i$. If features have disparate scales, the coefficients cannot be directly compared to gauge feature importance. Furthermore, many algorithms used to find the optimal coefficients, such as those based on gradient descent, converge faster when features are on a similar scale.\n\nLet us examine the provided descriptors and their training-set statistics:\n1.  $pK_a$: A dimensionless logarithmic constant with mean $\\mu_{pK_{a}} = 6.80$ and standard deviation $\\sigma_{pK_{a}} = 1.20$.\n2.  $\\log_{10} P$: A dimensionless logarithmic ratio with mean $\\mu_{\\log_{10} P} = 2.50$ and standard deviation $\\sigma_{\\log_{10} P} = 1.10$.\n3.  PSA: An area measured in square angstroms ($\\text{\\AA}^{2}$) with mean $\\mu_{\\mathrm{PSA}} = 85.0 \\, \\text{\\AA}^{2}$ and standard deviation $\\sigma_{\\mathrm{PSA}} = 30.0 \\, \\text{\\AA}^{2}$.\n\nThe numerical range of PSA is an order of magnitude larger than that of $pK_{a}$ and $\\log_{10} P$. For instance, the mean of PSA ($85.0$) is vastly different from the means of the other two descriptors ($6.80$ and $2.50$). A change of $1.0$ unit in $pK_{a}$ represents a significant change in a molecule's ionization state, whereas a change of $1.0 \\, \\text{\\AA}^{2}$ in PSA is a very small change in its surface properties. A linear model would be disproportionately sensitive to variations in PSA simply because of its larger numerical values, leading to an implicit and undesirable weighting.\n\nTo mitigate this, we must transform the features onto a common scale. The standard method, known as standardization or Z-score normalization, is to transform each feature to have a mean of $0$ and a variance of $1$. This transformation is particularly appropriate here as the problem states the descriptor distributions are \"approximately unimodal and symmetric,\" which implies that the mean and standard deviation are meaningful statistics for describing the center and spread of the data. The resulting standardized features are dimensionless, making them directly comparable.\n\nWe shall now derive the required affine transformation from first principles. An affine transformation of a variable $x$ is of the form $z = ax + b$, where $a$ and $b$ are constants. Let $X$ be a random variable representing a descriptor, with mean $E[X] = \\mu$ and standard deviation $\\mathrm{StdDev}[X] = \\sigma$. We seek a transformation that maps $X$ to a new variable $Z = aX + b$ such that $E[Z] = 0$ and $\\mathrm{StdDev}[Z] = 1$.\n\nUsing the properties of expectation, the mean of the transformed variable is:\n$$\nE[Z] = E[aX + b] = aE[X] + b = a\\mu + b\n$$\nFor the new mean to be zero, we must have:\n$$\na\\mu + b = 0 \\implies b = -a\\mu\n$$\nUsing the properties of variance, the variance of the transformed variable is:\n$$\n\\mathrm{Var}(Z) = \\mathrm{Var}(aX + b) = a^2\\mathrm{Var}(X) = a^2\\sigma^2\n$$\nThe standard deviation is the square root of the variance, $\\sigma_Z = \\sqrt{\\mathrm{Var}(Z)}$. Since we require the new standard deviation to be $1$, we have:\n$$\n\\sigma_Z = \\sqrt{a^2\\sigma^2} = |a|\\sigma = 1\n$$\nAssuming the descriptor is not constant ($\\sigma > 0$), we can solve for $a$. By convention, we choose the positive solution, $a = \\frac{1}{\\sigma}$.\n\nSubstituting this expression for $a$ into the equation for $b$:\n$$\nb = -a\\mu = -\\frac{1}{\\sigma}\\mu = -\\frac{\\mu}{\\sigma}\n$$\nThus, the affine transformation that maps a data point $x$ from the original distribution to the standardized value $z$ is:\n$$\nz = ax + b = \\left(\\frac{1}{\\sigma}\\right)x - \\frac{\\mu}{\\sigma} = \\frac{x - \\mu}{\\sigma}\n$$\nThis transformation, when applied to a specific descriptor, uses the mean $\\mu$ and standard deviation $\\sigma$ calculated from the training set.\n\nWe now apply this transformation component-wise to the new compound's descriptor vector $(x_{pK_{a}}, x_{\\log_{10} P}, x_{\\mathrm{PSA}}) = (7.40, 3.20, 78.0)$. The training-set statistics are given as:\n$\\mu_{pK_{a}} = 6.80$, $\\sigma_{pK_{a}} = 1.20$\n$\\mu_{\\log_{10} P} = 2.50$, $\\sigma_{\\log_{10} P} = 1.10$\n$\\mu_{\\mathrm{PSA}} = 85.0$, $\\sigma_{\\mathrm{PSA}} = 30.0$\n\nLet the standardized vector be $(z_{pK_{a}}, z_{\\log_{10} P}, z_{\\mathrm{PSA}})$.\n\n1.  For $pK_a$:\n    $$\n    z_{pK_{a}} = \\frac{x_{pK_{a}} - \\mu_{pK_{a}}}{\\sigma_{pK_{a}}} = \\frac{7.40 - 6.80}{1.20} = \\frac{0.60}{1.20} = 0.5\n    $$\n    Rounding to four significant figures, this is $0.5000$.\n\n2.  For $\\log_{10} P$:\n    $$\n    z_{\\log_{10} P} = \\frac{x_{\\log_{10} P} - \\mu_{\\log_{10} P}}{\\sigma_{\\log_{10} P}} = \\frac{3.20 - 2.50}{1.10} = \\frac{0.70}{1.10} \\approx 0.636363...\n    $$\n    Rounding to four significant figures, this is $0.6364$.\n\n3.  For PSA:\n    $$\n    z_{\\mathrm{PSA}} = \\frac{x_{\\mathrm{PSA}} - \\mu_{\\mathrm{PSA}}}{\\sigma_{\\mathrm{PSA}}} = \\frac{78.0 - 85.0}{30.0} = \\frac{-7.0}{30.0} \\approx -0.233333...\n    $$\n    Rounding to four significant figures, this is $-0.2333$.\n\nThe final standardized descriptor vector for the new compound is $(0.5000, 0.6364, -0.2333)$. This is expressed as a row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.5000  0.6364  -0.2333\n\\end{pmatrix}\n}\n$$", "id": "4563981"}, {"introduction": "For machine learning models to be trusted in high-stakes decisions like drug development, their predictions must be interpretable. This practice delves into the crucial field of Explainable AI (XAI) by introducing Shapley Additive exPlanations (SHAP), a state-of-the-art method for understanding how each feature contributes to a model's output for an individual prediction. By working through the accessible case of a linear model with independent features, you will gain a foundational understanding of how SHAP values are derived and used to open the \"black box.\" [@problem_id:4563996]", "problem": "A clinical pharmacology group uses a linear surrogate model to predict adult hepatic clearance for orally administered small molecules based on physicochemical descriptors. Let the model output clearance be $f(x)$ in $\\mathrm{L/h}$ for a descriptor vector $x \\in \\mathbb{R}^{3}$ with components $x_{1}$ (logarithm of partition coefficient at pH $7.4$, $\\log P$), $x_{2}$ (molecular weight in daltons), and $x_{3}$ (topological polar surface area in $\\text{\\AA}^{2}$). The model is linear with an intercept, $f(x) = b + w^{\\top} x$, where $w \\in \\mathbb{R}^{3}$.\n\nTo interpret model outputs, the group adopts Shapley Additive exPlanations (SHAP), defined by the following principles for a given instance $x$:\n- Local accuracy (additivity): the attributions $\\{\\phi_{j}\\}_{j=1}^{3}$ and a baseline term $\\phi_{0}$ satisfy $\\phi_{0} + \\sum_{j=1}^{3} \\phi_{j} = f(x)$, with $\\phi_{0} = \\mathbb{E}[f(X)]$ under the data distribution of $X$.\n- Missingness: features not present in a coalition contribute $0$ in that coalition’s value function.\n- Consistency: if a model change increases a feature’s contribution in all coalitions, that feature’s attribution does not decrease.\n\nAssume the following conditions, which are empirically supported for the descriptor distribution used to train the surrogate:\n- The descriptor distribution factorizes, so that $X_{1}$, $X_{2}$, and $X_{3}$ are independent.\n- The coalition value function is the conditional expectation $v(S) = \\mathbb{E}[f(X) \\mid X_{S} = x_{S}]$ for any subset $S \\subseteq \\{1,2,3\\}$.\n\nStarting from the SHAP axioms and these assumptions, derive a closed-form expression for the SHAP attribution $\\phi_{j}$ in terms of $w_{j}$, $x_{j}$, and $\\mathbb{E}[X_{j}]$ for $j \\in \\{1,2,3\\}$, and show how $\\phi_{0}$ relates to $b$, $w$, and $\\mathbb{E}[X]$. Then, for a particular compound with descriptors $x_{1} = 3.1$, $x_{2} = 420$, and $x_{3} = 60$, and a model characterized by\n- $w_{1} = 2.03$, $w_{2} = -0.01$, $w_{3} = -0.04$,\n- intercept $b = 22.034$,\n- descriptor means $\\mathbb{E}[X_{1}] = 2.2$, $\\mathbb{E}[X_{2}] = 350$, $\\mathbb{E}[X_{3}] = 75$,\n\ncompute the SHAP attributions $\\phi_{1}$, $\\phi_{2}$, $\\phi_{3}$, the baseline $\\phi_{0}$, and the predicted clearance $f(x)$. Report only the final predicted clearance in $\\mathrm{L/h}$ as your answer. Round your answer to $4$ significant figures and express the value in $\\mathrm{L/h}$.", "solution": "The problem has been validated as scientifically sound and well-posed, with all necessary information provided for a complete solution. The task requires deriving the SHAP values for a linear model under the assumption of feature independence and then using them to compute the final model prediction.\n\n**1. Derivation of the SHAP Attribution $\\phi_j$**\n\nThe SHAP value for a feature $j$, $\\phi_j$, is its average marginal contribution across all possible feature coalitions. The value of a coalition $S$ is defined as $v(S) = \\mathbb{E}[f(X) | X_S = x_S]$. For our linear model $f(X) = b + \\sum_{k=1}^3 w_k X_k$, the value function is:\n$$ v(S) = \\mathbb{E}[b + \\sum_{k=1}^3 w_k X_k \\mid X_S = x_S] $$\nBy linearity of expectation:\n$$ v(S) = b + \\mathbb{E}[\\sum_{k=1}^3 w_k X_k \\mid X_S = x_S] $$\nWe can split the sum into features within the coalition $S$ and those not in $S$:\n$$ v(S) = b + \\sum_{k \\in S} \\mathbb{E}[w_k X_k \\mid X_k=x_k] + \\sum_{k \\notin S} \\mathbb{E}[w_k X_k] $$\nThe first sum simplifies because we are conditioning on the known values. For the second sum, since features are independent, conditioning on $X_S$ gives no information about features not in $S$.\n$$ v(S) = b + \\sum_{k \\in S} w_k x_k + \\sum_{k \\notin S} w_k \\mathbb{E}[X_k] $$\nThe marginal contribution of adding feature $j$ to a coalition $S$ (where $j \\notin S$) is $v(S \\cup \\{j\\}) - v(S)$.\n$$ v(S \\cup \\{j\\}) = b + w_j x_j + \\sum_{k \\in S} w_k x_k + \\sum_{k \\notin S \\cup \\{j\\}} w_k \\mathbb{E}[X_k] $$\n$$ v(S \\cup \\{j\\}) - v(S) = \\left(b + w_j x_j + \\sum_{k \\in S} w_k x_k + \\sum_{k \\notin S \\cup \\{j\\}} w_k \\mathbb{E}[X_k]\\right) - \\left(b + \\sum_{k \\in S} w_k x_k + \\sum_{k \\notin S} w_k \\mathbb{E}[X_k]\\right) $$\nThe terms involving $b$ and sums over $k \\in S$ and $k \\notin S \\cup \\{j\\}$ cancel out. The remaining terms are:\n$$ v(S \\cup \\{j\\}) - v(S) = w_j x_j - w_j \\mathbb{E}[X_j] = w_j (x_j - \\mathbb{E}[X_j]) $$\nCrucially, this marginal contribution is the same regardless of the coalition $S$. The SHAP value $\\phi_j$ is the weighted average of these identical marginal contributions. Since the weights must sum to 1, the average is simply the value itself.\n$$ \\phi_j = w_j (x_j - \\mathbb{E}[X_j]) $$\n\n**2. Derivation of the Baseline $\\phi_0$**\n\nThe baseline attribution is defined as the expected value of the model output over the data distribution:\n$$ \\phi_0 = \\mathbb{E}[f(X)] = \\mathbb{E}[b + w^\\top X] = b + w^\\top \\mathbb{E}[X] = b + \\sum_{j=1}^3 w_j \\mathbb{E}[X_j] $$\n\n**3. Computation of Attributions and Prediction**\n\nGiven values:\n- $x = (3.1, 420, 60)^\\top$\n- $w = (2.03, -0.01, -0.04)^\\top$\n- $b = 22.034$\n- $\\mathbb{E}[X] = (2.2, 350, 75)^\\top$\n\nFirst, compute the baseline attribution $\\phi_0$:\n$$ \\phi_0 = 22.034 + (2.03 \\times 2.2) + (-0.01 \\times 350) + (-0.04 \\times 75) $$\n$$ \\phi_0 = 22.034 + 4.466 - 3.5 - 3.0 = 20.0 $$\nNext, compute the individual feature attributions $\\phi_1, \\phi_2, \\phi_3$:\n$$ \\phi_1 = w_1(x_1 - \\mathbb{E}[X_1]) = 2.03 (3.1 - 2.2) = 2.03 \\times 0.9 = 1.827 $$\n$$ \\phi_2 = w_2(x_2 - \\mathbb{E}[X_2]) = -0.01 (420 - 350) = -0.01 \\times 70 = -0.7 $$\n$$ \\phi_3 = w_3(x_3 - \\mathbb{E}[X_3]) = -0.04 (60 - 75) = -0.04 \\times (-15) = 0.6 $$\n\n**4. Compute the Final Predicted Clearance $f(x)$**\n\nThe problem asks for the predicted clearance $f(x)$. We can calculate this in two ways as a consistency check.\n\nMethod A: Using the SHAP additivity axiom, $f(x) = \\phi_0 + \\phi_1 + \\phi_2 + \\phi_3$.\n$$ f(x) = 20.0 + 1.827 - 0.7 + 0.6 = 21.727 $$\n\nMethod B: Direct calculation using the linear model equation, $f(x) = b + w^\\top x$.\n$$ f(x) = 22.034 + (2.03 \\times 3.1) + (-0.01 \\times 420) + (-0.04 \\times 60) $$\n$$ f(x) = 22.034 + 6.293 - 4.2 - 2.4 $$\n$$ f(x) = 28.327 - 6.6 = 21.727 $$\nBoth methods yield the same result, confirming the calculations. The predicted clearance is $21.727 \\ \\mathrm{L/h}$.\n\nFinally, we round the answer to 4 significant figures.\n$$ f(x) \\approx 21.73 \\ \\mathrm{L/h} $$", "answer": "$$ \\boxed{21.73} $$", "id": "4563996"}]}