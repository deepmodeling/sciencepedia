{"hands_on_practices": [{"introduction": "Disproportionality analysis forms the bedrock of quantitative signal detection in spontaneous reporting systems. This exercise [@problem_id:4520104] moves beyond simple ratios to a sophisticated Bayesian method, the Information Component ($IC$), used in the BCPNN framework. By working from the foundational principles of a Dirichlet-Multinomial model, you will gain a deep appreciation for how Bayesian shrinkage provides more stable and reliable estimates, especially with sparse data, and learn to quantify the uncertainty of these advanced metrics.", "problem": "A spontaneous reporting database is summarized by a $2 \\times 2$ contingency table for drug $D$ and adverse event $E$ with total reports $N$, joint count $n_{DE}$, and marginals $n_{D \\cdot}$ and $n_{\\cdot E}$. Consider a Bayesian shrinkage model for the four cell probabilities $(p_{11}, p_{10}, p_{01}, p_{00})$ with a symmetric Dirichlet prior with pseudocount $\\alpha$ on each cell, where the data are modeled as multinomially distributed. For the Bayesian Confidence Propagation Neural Network (BCPNN) disproportionality metric, the Information Component (IC) is defined as $IC = \\log_{2}\\!\\left(\\frac{p_{11}}{(p_{11}+p_{10})(p_{11}+p_{01})}\\right)$.\n\nStarting from the definitions of the multinomial likelihood and the Dirichlet conjugate prior, use the posterior distribution of $(p_{11}, p_{10}, p_{01}, p_{00})$ to:\n- compute a point estimate of $IC$ by evaluating the defining function at the posterior mean of $(p_{11}, p_{10}, p_{01}, p_{00})$, and\n- compute the delta-method standard error of $IC$ by propagating the posterior covariance of the Dirichlet distribution through the above transformation.\n\nYou are given $n_{DE} = 20$, $n_{D \\cdot} = 200$, $n_{\\cdot E} = 150$, $N = 5000$, and a symmetric Dirichlet prior with $\\alpha = 1$ on each of the four cells. Report the pair consisting of the point estimate of $IC$ and its delta-method standard error. Round both values to four significant figures. No units are required.", "solution": "The problem requires the computation of a point estimate and a standard error for the Information Component ($IC$) within a Bayesian framework.\n\nFirst, we establish the $2 \\times 2$ contingency table notation and calculate the counts for all four cells. Let cell $(i, j)$ correspond to drug status $i$ and event status $j$, where $i=1$ for drug $D$ present and $i=0$ for drug $D$ absent, and similarly $j=1$ for event $E$ present and $j=0$ for event $E$ absent.\n\nThe given counts are:\nTotal reports: $N=5000$\nDrug $D$ and Event $E$: $n_{11} = n_{DE} = 20$\nDrug $D$ marginal: $n_{D \\cdot} = n_{11} + n_{10} = 200$\nEvent $E$ marginal: $n_{\\cdot E} = n_{11} + n_{01} = 150$\n\nFrom these, we derive the other cell counts:\nDrug $D$, no Event $E$: $n_{10} = n_{D \\cdot} - n_{11} = 200 - 20 = 180$\nNo Drug $D$, Event $E$: $n_{01} = n_{\\cdot E} - n_{11} = 150 - 20 = 130$\nNo Drug $D$, no Event $E$: $n_{00} = N - n_{11} - n_{10} - n_{01} = 5000 - 20 - 180 - 130 = 4670$\nThe full set of counts is $(n_{11}, n_{10}, n_{01}, n_{00}) = (20, 180, 130, 4670)$.\n\nThe model assumes the data $\\mathbf{n} = (n_{11}, n_{10}, n_{01}, n_{00})$ follows a Multinomial distribution with parameters $N$ and cell probabilities $\\mathbf{p} = (p_{11}, p_{10}, p_{01}, p_{00})$. The prior on $\\mathbf{p}$ is a symmetric Dirichlet distribution, $\\mathbf{p} \\sim \\text{Dir}(\\alpha, \\alpha, \\alpha, \\alpha)$, with $\\alpha=1$.\n\nThe Dirichlet distribution is the conjugate prior for the Multinomial likelihood. Therefore, the posterior distribution of $\\mathbf{p}$ given the data $\\mathbf{n}$ is also a Dirichlet distribution:\n$$ \\mathbf{p} \\,|\\, \\mathbf{n} \\sim \\text{Dir}(\\alpha'_{11}, \\alpha'_{10}, \\alpha'_{01}, \\alpha'_{00}) $$\nwhere the posterior parameters are $\\alpha'_{ij} = n_{ij} + \\alpha$.\nWith $\\alpha=1$, the posterior parameters are:\n$\\alpha'_{11} = 20 + 1 = 21$\n$\\alpha'_{10} = 180 + 1 = 181$\n$\\alpha'_{01} = 130 + 1 = 131$\n$\\alpha'_{00} = 4670 + 1 = 4671$\n\nThe sum of the posterior parameters is $\\alpha'_0 = \\sum_{i,j} \\alpha'_{ij} = 21 + 181 + 131 + 4671 = 5004$. Note that this is also $N + 4\\alpha = 5000 + 4(1) = 5004$.\n\n**1. Point Estimate of IC**\n\nThe point estimate is calculated by substituting the posterior mean of the probabilities into the definition of $IC$. The posterior mean of $p_{ij}$ is $E[p_{ij} | \\mathbf{n}] = \\hat{p}_{ij} = \\frac{\\alpha'_{ij}}{\\alpha'_0}$.\nThe IC is defined as $g(\\mathbf{p}) = IC = \\log_{2}\\!\\left(\\frac{p_{11}}{p_{D\\cdot} p_{\\cdot E}}\\right)$, where $p_{D\\cdot} = p_{11}+p_{10}$ and $p_{\\cdot E} = p_{11}+p_{01}$.\nThe point estimate $\\widehat{IC}$ is:\n$$ \\widehat{IC} = g(\\hat{\\mathbf{p}}) = \\log_{2}\\!\\left(\\frac{\\hat{p}_{11}}{\\hat{p}_{D\\cdot}\\hat{p}_{\\cdot E}}\\right) $$\nThe posterior means of the marginal probabilities are $\\hat{p}_{D\\cdot} = \\hat{p}_{11}+\\hat{p}_{10} = \\frac{\\alpha'_{11}+\\alpha'_{10}}{\\alpha'_0}$ and $\\hat{p}_{\\cdot E} = \\hat{p}_{11}+\\hat{p}_{01} = \\frac{\\alpha'_{11}+\\alpha'_{01}}{\\alpha'_0}$.\nLet $\\alpha'_{D\\cdot} = \\alpha'_{11}+\\alpha'_{10} = 21+181 = 202$ and $\\alpha'_{\\cdot E} = \\alpha'_{11}+\\alpha'_{01} = 21+131 = 152$.\nThen,\n$$ \\widehat{IC} = \\log_{2}\\!\\left(\\frac{\\alpha'_{11}/\\alpha'_{0}}{(\\alpha'_{D\\cdot}/\\alpha'_{0})(\\alpha'_{\\cdot E}/\\alpha'_{0})}\\right) = \\log_{2}\\!\\left(\\frac{\\alpha'_{11}\\alpha'_0}{\\alpha'_{D\\cdot}\\alpha'_{\\cdot E}}\\right) $$\nSubstituting the values:\n$$ \\widehat{IC} = \\log_{2}\\!\\left(\\frac{21 \\times 5004}{202 \\times 152}\\right) = \\log_{2}\\!\\left(\\frac{105084}{30704}\\right) \\approx \\log_{2}(3.42248567) $$\n$$ \\widehat{IC} = \\frac{\\ln(3.42248567)}{\\ln(2)} \\approx 1.775193 $$\nRounding to four significant figures, the point estimate is $1.775$.\n\n**2. Delta-Method Standard Error of IC**\n\nThe delta method approximates the variance of a function $g(\\mathbf{p})$ as $\\text{Var}(g(\\mathbf{p})) \\approx (\\nabla g)^T \\Sigma_{\\mathbf{p}} (\\nabla g)$, evaluated at the posterior mean $\\hat{\\mathbf{p}}$. For a posterior Dirichlet distribution, this simplifies to:\n$$ \\text{Var}(g(\\mathbf{p})) \\approx \\frac{1}{\\alpha'_0+1} \\left[ \\sum_{i,j} \\hat{p}_{ij} \\left(\\frac{\\partial g}{\\partial p_{ij}}\\right)^2 - \\left(\\sum_{i,j} \\hat{p}_{ij} \\frac{\\partial g}{\\partial p_{ij}}\\right)^2 \\right] $$\nLet's first express $g(\\mathbf{p})$ using the natural logarithm: $g(\\mathbf{p}) = \\frac{1}{\\ln(2)}[\\ln(p_{11}) - \\ln(p_{11}+p_{10}) - \\ln(p_{11}+p_{01})]$. Let $C = 1/\\ln(2)$.\nThe partial derivatives, evaluated at the posterior mean $\\hat{\\mathbf{p}}$, are:\n$\\frac{\\partial g}{\\partial p_{11}} = C \\left(\\frac{1}{p_{11}} - \\frac{1}{p_{D\\cdot}} - \\frac{1}{p_{\\cdot E}}\\right)$\n$\\frac{\\partial g}{\\partial p_{10}} = C \\left(-\\frac{1}{p_{D\\cdot}}\\right)$\n$\\frac{\\partial g}{\\partial p_{01}} = C \\left(-\\frac{1}{p_{\\cdot E}}\\right)$\n$\\frac{\\partial g}{\\partial p_{00}} = 0$\n\nLet's compute the two terms in the variance formula. The first term is $E_{\\hat{\\mathbf{p}}}[\\nabla g] = \\sum \\hat{p}_{ij}\\frac{\\partial g}{\\partial p_{ij}}$:\n$$ \\sum_{i,j} \\hat{p}_{ij}\\frac{\\partial g}{\\partial p_{ij}} = C \\left[ \\hat{p}_{11}\\left(\\frac{1}{\\hat{p}_{11}} - \\frac{1}{\\hat{p}_{D\\cdot}} - \\frac{1}{\\hat{p}_{\\cdot E}}\\right) - \\hat{p}_{10}\\frac{1}{\\hat{p}_{D\\cdot}} - \\hat{p}_{01}\\frac{1}{\\hat{p}_{\\cdot E}} \\right] $$\n$$ = C \\left[ 1 - \\frac{\\hat{p}_{11}}{\\hat{p}_{D\\cdot}} - \\frac{\\hat{p}_{11}}{\\hat{p}_{\\cdot E}} - \\frac{\\hat{p}_{10}}{\\hat{p}_{D\\cdot}} - \\frac{\\hat{p}_{01}}{\\hat{p}_{\\cdot E}} \\right] = C \\left[ 1 - \\frac{\\hat{p}_{11}+\\hat{p}_{10}}{\\hat{p}_{D\\cdot}} - \\frac{\\hat{p}_{11}+\\hat{p}_{01}}{\\hat{p}_{\\cdot E}} \\right] $$\n$$ = C [1 - 1 - 1] = -C $$\nThe second term is $E_{\\hat{\\mathbf{p}}}[(\\nabla g)^2] = \\sum \\hat{p}_{ij}(\\frac{\\partial g}{\\partial p_{ij}})^2$:\n$$ \\sum_{i,j} \\hat{p}_{ij}\\left(\\frac{\\partial g}{\\partial p_{ij}}\\right)^2 = C^2 \\left[ \\hat{p}_{11}\\left(\\frac{1}{\\hat{p}_{11}}-\\frac{1}{\\hat{p}_{D\\cdot}}-\\frac{1}{\\hat{p}_{\\cdot E}}\\right)^2 + \\frac{\\hat{p}_{10}}{\\hat{p}_{D\\cdot}^2} + \\frac{\\hat{p}_{01}}{\\hat{p}_{\\cdot E}^2} \\right] $$\nExpanding and simplifying this expression yields:\n$$ \\sum_{i,j} \\hat{p}_{ij}\\left(\\frac{\\partial g}{\\partial p_{ij}}\\right)^2 = C^2 \\left[ \\frac{1}{\\hat{p}_{11}} - \\frac{1}{\\hat{p}_{D\\cdot}} - \\frac{1}{\\hat{p}_{\\cdot E}} + \\frac{2\\hat{p}_{11}}{\\hat{p}_{D\\cdot}\\hat{p}_{\\cdot E}} \\right] $$\nNow, substituting these simplified forms into the variance formula:\n$$ \\text{Var}(IC) \\approx \\frac{1}{\\alpha'_0+1} \\left[ C^2 \\left( \\frac{1}{\\hat{p}_{11}} - \\frac{1}{\\hat{p}_{D\\cdot}} - \\frac{1}{\\hat{p}_{\\cdot E}} + \\frac{2\\hat{p}_{11}}{\\hat{p}_{D\\cdot}\\hat{p}_{\\cdot E}} \\right) - (-C)^2 \\right] $$\n$$ \\text{Var}(IC) \\approx \\frac{C^2}{\\alpha'_0+1} \\left[ \\frac{1}{\\hat{p}_{11}} - \\frac{1}{\\hat{p}_{D\\cdot}} - \\frac{1}{\\hat{p}_{\\cdot E}} + \\frac{2\\hat{p}_{11}}{\\hat{p}_{D\\cdot}\\hat{p}_{\\cdot E}} - 1 \\right] $$\nLet's substitute values into the term in the brackets.\n$\\frac{1}{\\hat{p}_{11}} = \\frac{\\alpha'_0}{\\alpha'_{11}} = \\frac{5004}{21} \\approx 238.2857$\n$\\frac{1}{\\hat{p}_{D\\cdot}} = \\frac{\\alpha'_0}{\\alpha'_{D\\cdot}} = \\frac{5004}{202} \\approx 24.7723$\n$\\frac{1}{\\hat{p}_{\\cdot E}} = \\frac{\\alpha'_0}{\\alpha'_{\\cdot E}} = \\frac{5004}{152} \\approx 32.9211$\n$\\frac{2\\hat{p}_{11}}{\\hat{p}_{D\\cdot}\\hat{p}_{\\cdot E}} = \\frac{2\\alpha'_{11}\\alpha'_0}{\\alpha'_{D\\cdot}\\alpha'_{\\cdot E}} = \\frac{2 \\times 21 \\times 5004}{202 \\times 152} \\approx 6.8450$\nThe term in brackets is $238.2857 - 24.7723 - 32.9211 + 6.8450 - 1 = 186.4373$\nSo, the variance is:\n$$ \\text{Var}(IC) \\approx \\frac{1}{(\\ln(2))^2} \\frac{186.4373}{5004+1} \\approx \\frac{1}{0.480453} \\frac{186.4373}{5005} \\approx 2.081368 \\times 0.0372502 \\approx 0.0775317 $$\nThe standard error is the square root of the variance:\n$$ SE(IC) = \\sqrt{\\text{Var}(IC)} \\approx \\sqrt{0.0775317} \\approx 0.278445 $$\nRounding to four significant figures, the standard error is $0.2784$.\n\nThe pair consisting of the point estimate and its delta-method standard error is $(1.775, 0.2784)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.775 & 0.2784\n\\end{pmatrix}\n}\n$$", "id": "4520104"}, {"introduction": "Real-world safety evaluation rarely focuses on a single adverse event in isolation; instead, we analyze clinically related groups of events, often defined by a Standardized MedDRA Query (SMQ). This practice [@problem_id:4520157] tackles the critical task of aggregating signals across multiple event categories. You will apply the robust Mantel-Haenszel method to compute a pooled Proportional Reporting Ratio ($PRR$), and more importantly, critically analyze how heterogeneity in the event data can influence and potentially bias the summary estimate, a key skill for nuanced signal interpretation.", "problem": "A pharmacovigilance team is evaluating a drug of interest, denoted as $D$, for a potential cardiac arrhythmia signal using a Standardized Medical Dictionary for Regulatory Activities (MedDRA) Query (SMQ) narrow scope. The SMQ comprises three non-overlapping Preferred Terms (PTs), and counts are taken from a deduplicated spontaneous reporting system such that each individual report contributes to at most one PT within the SMQ. The Proportional Reporting Ratio (PRR) is defined for a single PT as the ratio of the reporting proportion of that PT among reports with $D$ to the reporting proportion among reports with all other drugs. For each PT, the $2 \\times 2$ table has exposed (reports with $D$) size $N_{1}$ and unexposed (reports with other drugs) size $N_{0}$. For PT $i$, denote $a_{i}$ as the number of $D$ reports with PT $i$ and $c_{i}$ as the number of non-$D$ reports with PT $i$, with the complementary counts given by $b_{i} = N_{1} - a_{i}$ and $d_{i} = N_{0} - c_{i}$.\n\nYou are provided the following scientifically plausible counts:\n- Total reports with $D$: $N_{1} = 8000$.\n- Total reports with other drugs: $N_{0} = 160000$.\n- PT $1$ (e.g., Ventricular arrhythmia): $a_{1} = 120$, $c_{1} = 480$.\n- PT $2$ (e.g., Electrocardiogram QT prolonged): $a_{2} = 35$, $c_{2} = 80$.\n- PT $3$ (e.g., Torsade de pointes): $a_{3} = 12$, $c_{3} = 20$.\n\nStarting from the definition of PRR in a single stratum as a ratio of risks, and assuming a fixed-effect common risk ratio across PT strata, derive from first principles the fixed-effect Mantel–Haenszel pooled estimator for the aggregated SMQ-wide PRR and compute its value using the data above. Then, briefly discuss how heterogeneity across PT strata can bias the aggregated estimate through compositional effects and weighting, even under the fixed-effect assumption.\n\nRound your final numerical PRR to four significant figures and express it as a unitless quantity. Provide only the final numerical value as your answer.", "solution": "The problem requires the derivation and calculation of the Mantel-Haenszel pooled estimator for a Proportional Reporting Ratio (PRR) across three strata defined by Preferred Terms (PTs), and a discussion of potential bias.\n\n**Part 1: Derivation of the Mantel-Haenszel Pooled Estimator for PRR**\n\nLet the strata be indexed by $i=1, 2, 3$. For each stratum $i$, the data are summarized in a $2 \\times 2$ table:\n\n|             | Event (PT $i$) | No Event (Not PT $i$) | Total     |\n|-------------|----------------|-----------------------|-----------|\n| Exposed ($D$) | $a_i$          | $b_i$                 | $N_{1i}$ |\n| Unexposed (¬$D$) | $c_i$          | $d_i$                 | $N_{0i}$ |\n| Total       | $M_{1i}$ | $M_{0i}$  | $T_i$ |\n\nwhere $N_{1i} = a_i+b_i$ is the number of reports for drug $D$, $N_{0i} = c_i+d_i$ is the number of reports for other drugs, and $T_i = N_{1i} + N_{0i}$ is the total number of reports for the stratum.\n\nThe PRR is a measure of disproportionality, defined as a ratio of reporting proportions, which is mathematically equivalent to a risk ratio (RR) in this context. The PRR for stratum $i$ is:\n$$ PRR_i = \\frac{\\text{Risk in exposed}}{\\text{Risk in unexposed}} = \\frac{a_i / N_{1i}}{c_i / N_{0i}} $$\nThe problem states the assumption of a fixed-effect common risk ratio, $\\psi$, across strata, such that $PRR_i \\approx \\psi$ for all $i$.\n\nThe Mantel-Haenszel (MH) estimator for a common risk ratio, $\\hat{\\psi}_{RR-MH}$, across $k$ strata is a weighted average, given by the formula:\n$$ \\hat{\\psi}_{RR-MH} = \\frac{\\sum_{i=1}^{k} \\frac{a_i N_{0i}}{T_i}}{\\sum_{i=1}^{k} \\frac{c_i N_{1i}}{T_i}} $$\nThis can be rewritten using the totals from the $2 \\times 2$ table:\n$$ \\hat{\\psi}_{RR-MH} = \\frac{\\sum_{i=1}^{k} a_i(c_i+d_i)/T_i}{\\sum_{i=1}^{k} c_i(a_i+b_i)/T_i} $$\nThe numerator is a weighted sum of the event counts in the exposed group ($a_i$), and the denominator is a weighted sum of the event counts in the unexposed group ($c_i$).\n\nIn this specific problem, the total number of exposed reports, $N_1$, and unexposed reports, $N_0$, are constant across all strata.\n$N_{1i} = N_1 = 8000$ for all $i=1, 2, 3$.\n$N_{0i} = N_0 = 160000$ for all $i=1, 2, 3$.\nTherefore, the total number of reports per stratum is also constant:\n$T_i = T = N_1 + N_0 = 8000 + 160000 = 168000$.\n\nSubstituting these constant values into the general MH formula:\n$$ \\hat{PRR}_{MH} = \\frac{\\sum_{i=1}^{3} \\frac{a_i N_0}{T}}{\\sum_{i=1}^{3} \\frac{c_i N_1}{T}} $$\nSince $N_0/T$ and $N_1/T$ are constants, they can be factored out of the summations:\n$$ \\hat{PRR}_{MH} = \\frac{\\frac{N_0}{T} \\sum_{i=1}^{3} a_i}{\\frac{N_1}{T} \\sum_{i=1}^{3} c_i} = \\frac{N_0 \\sum_{i=1}^{3} a_i}{N_1 \\sum_{i=1}^{3} c_i} $$\nLet $A = \\sum_{i=1}^{3} a_i$ be the total number of events for drug $D$ across all three PTs, and $C = \\sum_{i=1}^{3} c_i$ be the total number of events for other drugs. The expression simplifies to:\n$$ \\hat{PRR}_{MH} = \\frac{A \\cdot N_0}{C \\cdot N_1} = \\frac{A/N_1}{C/N_0} $$\nThis demonstrates that under the specific condition of constant exposure group sizes across strata, the Mantel-Haenszel pooled estimator is equivalent to the \"crude\" or \"aggregated\" PRR, calculated from a single $2 \\times 2$ table where the counts for the individual PTs have been summed.\n\n**Part 2: Calculation of the Aggregated PRR**\n\nUsing the provided data:\n$N_1 = 8000$\n$N_0 = 160000$\n$a_1 = 120$, $a_2 = 35$, $a_3 = 12$\n$c_1 = 480$, $c_2 = 80$, $c_3 = 20$\n\nFirst, we compute the aggregated event counts, $A$ and $C$:\n$$ A = \\sum a_i = 120 + 35 + 12 = 167 $$\n$$ C = \\sum c_i = 480 + 80 + 20 = 580 $$\n\nNow, we calculate the aggregated PRR:\n$$ \\hat{PRR}_{MH} = \\frac{A/N_1}{C/N_0} = \\frac{167 / 8000}{580 / 160000} $$\nCalculating the proportions:\nProportion in exposed group: $P_1 = \\frac{167}{8000} = 0.020875$\nProportion in unexposed group: $P_0 = \\frac{580}{160000} = 0.003625$\n\nThe estimator is the ratio of these proportions:\n$$ \\hat{PRR}_{MH} = \\frac{0.020875}{0.003625} = \\frac{167}{8000} \\times \\frac{160000}{580} = \\frac{167 \\times 20}{580} = \\frac{167 \\times 2}{58} = \\frac{167}{29} $$\n$$ \\hat{PRR}_{MH} \\approx 5.758620689... $$\nRounding to four significant figures, the result is $5.759$.\n\n**Part 3: Discussion of Bias from Heterogeneity**\n\nThe problem asks to discuss bias from heterogeneity even when applying a fixed-effect model. The core assumption of the fixed-effect model is homogeneity, i.e., a single common effect size ($\\psi$) across all strata. When this assumption is violated (i.e., when true heterogeneity exists), the pooled estimate is not an estimator of a single true parameter, but rather a weighted average of the different stratum-specific effect sizes. The \"bias\" in this context refers to how this weighted average can be a misleading summary of the overall risk, particularly if the weights are correlated with the effect sizes.\n\nFirst, let's examine the heterogeneity by calculating the stratum-specific PRRs:\n$PRR_1 = \\frac{120/8000}{480/160000} = \\frac{0.015}{0.003} = 5.0$\n$PRR_2 = \\frac{35/8000}{80/160000} = \\frac{0.004375}{0.0005} = 8.75$\n$PRR_3 = \\frac{12/8000}{20/160000} = \\frac{0.0015}{0.000125} = 12.0$\nThere is substantial heterogeneity, with PRRs ranging from $5.0$ to $12.0$.\n\nThe Mantel-Haenszel RR estimator is a weighted average of the stratum-specific RRs, $\\hat{\\psi}_{RR-MH} = \\frac{\\sum_i w_i RR_i}{\\sum_i w_i}$, where the weights are $w_i = c_i N_{1i} / T_i$. In this problem, $N_{1i}/T_i$ is constant, so the weights are proportional to the event count in the unexposed group, $w_i \\propto c_i$.\nThe weights for our three PTs are proportional to $c_1=480$, $c_2=80$, and $c_3=20$.\n\nThe bias through compositional effects and weighting arises as follows:\n1.  **Compositional Effect**: The SMQ is a composite endpoint. Its overall characteristics are determined by the properties of its components (the PTs). Here, the PTs differ in both their background reporting frequency (approximated by $c_i$) and their strength of association with the drug ($PRR_i$). PT 1 has a high background frequency but a relatively low PRR. PT 3 has a very low background frequency but the highest PRR.\n2.  **Weighting Bias**: The MH method assigns weights based on stratum precision, which for the RR estimator are proportional to $c_i$. This means that PT 1 (Ventricular arrhythmia, $c_1=480, PRR_1=5.0$) contributes most heavily to the pooled estimate, while PT 3 (Torsade de pointes, $c_3=20, PRR_3=12.0$), which is often a more clinically severe event and shows the strongest signal, contributes the least. The relative weights are approximately $83\\%$ for PT 1, $14\\%$ for PT 2, and $3\\%$ for PT 3.\n3.  **Resulting Misrepresentation**: The pooled estimate of $\\hat{PRR}_{MH} \\approx 5.76$ is heavily pulled towards the $PRR_1=5.0$ of the most common PT. It numerically obscures the much stronger signal of $PRR_3=12.0$ associated with the rare but critical event. An analyst focusing only on the single aggregated value would get a \"biased\" or incomplete picture of the drug's safety profile, underestimating the magnitude of risk for the most severe arrhythmia. The summary measure, while correctly calculated, misrepresents the underlying risk structure due to the confounding between background frequency and effect size across the PT strata.", "answer": "$$\\boxed{5.759}$$", "id": "4520157"}, {"introduction": "Effective pharmacovigilance requires not just analyzing a static database, but actively monitoring incoming data for emerging safety issues. This hands-on practice [@problem_id:4520144] shifts our perspective from cross-sectional analysis to temporal surveillance. You will implement an Exponentially Weighted Moving Average (EWMA) chart, a powerful tool from statistical process control, to track drug-event reporting counts over time and learn how to derive its statistical properties from first principles to create dynamic control limits for timely signal detection.", "problem": "You are asked to construct and apply an Exponentially Weighted Moving Average (EWMA) chart for pharmacovigilance signal detection, grounded in first principles of probability and statistics. Consider a single drug–event pair monitored monthly. Let the observed monthly count at month $t$ be $X_t$, and let the expected count under the null (no true change) be $\\mu_t$. Assume that, under the null, $X_t$ are independent and follow a Poisson distribution with mean $\\mu_t$, i.e., $X_t \\sim \\text{Poisson}(\\mu_t)$ and the sequence $\\{X_t\\}$ is independent over time.\n\nStarting from the fundamental definitions of linearity of expectation and variance of independent random variables, and the definition of an EWMA statistic:\n- Define the EWMA statistic $Z_t$ recursively via a smoothing parameter $\\lambda \\in (0,1)$ as $Z_t = \\lambda X_t + (1-\\lambda) Z_{t-1}$, with a head-start $Z_0 = \\mu_1$.\n- Derive the expected value $m_t = \\mathbb{E}[Z_t]$ recursively from first principles using the linearity of expectation and the independence assumption.\n- Derive the variance $v_t = \\text{Var}(Z_t)$ recursively from first principles using $\\text{Var}(aY + bZ) = a^2 \\text{Var}(Y) + b^2 \\text{Var}(Z) + 2ab\\,\\text{Cov}(Y,Z)$, the independence of $X_t$ from past observations, and the fact that for a Poisson random variable $Y \\sim \\text{Poisson}(\\theta)$, $\\text{Var}(Y) = \\theta$.\n- Using these derived recursions, compute time-varying two-sided control limits at each month $t$ as $m_t \\pm L \\sqrt{v_t}$, with width multiplier $L = 3$. A potential upper signal is flagged at month $t$ if $Z_t > m_t + L \\sqrt{v_t}$, and a potential lower signal is flagged if $Z_t < m_t - L \\sqrt{v_t}$.\n\nTake the smoothing parameter as $\\lambda = 0.2$. All quantities are dimensionless monthly counts; no physical units are required. Angles are not involved. Percentages are not involved.\n\nImplement this logic for each of the following three test cases (each spans $24$ months). For each case, treat the sequence $\\{\\mu_t\\}$ as known and fixed, and the observed counts $\\{x_t\\}$ as given data to be plugged into the EWMA recursion.\n\nTest Suite:\n- Case 1 (constant baseline, single extreme spike):\n    - $\\mu_t = 5$ for $t=1,\\dots,24$.\n    - $x_t = [5,5,5,5,5,5,5,5,5,5,5,5,50,5,5,5,5,5,5,5,5,5,5,5]$.\n- Case 2 (very low baseline, single spike amid zeros):\n    - $\\mu_t = 0.2$ for $t=1,\\dots,24$.\n    - $x_t = [0,0,0,0,0,0,0,0,0,0,5,0,0,0,0,0,0,0,0,0,0,0,0,0]$.\n- Case 3 (stepwise-varying baseline, single extreme spike after a step change):\n    - $\\mu_t = [2,2,2,2,2,2,4,4,4,4,4,4,6,6,6,6,6,6,8,8,8,8,8,8]$.\n    - $x_t$ equals $\\mu_t$ at all months except a spike at month $19$, i.e., $x_t = [2,2,2,2,2,2,4,4,4,4,4,4,6,6,6,6,6,6,40,8,8,8,8,8]$.\n\nRequired computational steps:\n1. For each case, initialize $Z_0 = \\mu_1$, set $m_0 = \\mu_1$, and $v_0 = 0$.\n2. For $t = 1,\\dots,24$, update $Z_t$, $m_t$, and $v_t$ using only fundamental laws (linearity of expectation and variance of independent sums) together with $\\text{Var}(X_t) = \\mu_t$ for Poisson counts.\n3. At each $t$, compute the upper and lower control limits and record the month index $t$ if a potential signal occurs as defined above.\n\nFinal Output Format:\n- Your program should produce a single line of output containing a JSON-like representation of a list of three elements (one per test case).\n- Each element must itself be a list of two lists: the first is the list of months (using $1$-based indexing) with upper signals, and the second is the list of months with lower signals.\n- For example, the output format must be of the form: \"[[ [u1_1,u1_2,...], [l1_1,l1_2,...] ], [ [u2_1,...], [l2_1,...] ], [ [u3_1,...], [l3_1,...] ]]\" with no spaces.", "solution": "We construct the Exponentially Weighted Moving Average (EWMA) statistic and its control limits by starting from the definition of $Z_t$ and applying fundamental properties of expectation and variance.\n\nDefinition of the Exponentially Weighted Moving Average (EWMA):\nFor smoothing parameter $\\lambda \\in (0,1)$ and a head-start $Z_0 = \\mu_1$, define the recursive statistic\n$$\nZ_t = \\lambda X_t + (1 - \\lambda) Z_{t-1}, \\quad t = 1,2,\\dots,24.\n$$\nThis is the standard definition of the Exponentially Weighted Moving Average (EWMA), which emphasizes recent observations while retaining information from the past.\n\nAssumptions under the null (no true change in the underlying process beyond the specified $\\mu_t$):\n- The observed count $X_t$ at time $t$ is a Poisson random variable with mean $\\mu_t$, i.e., $X_t \\sim \\text{Poisson}(\\mu_t)$.\n- The sequence $\\{X_t\\}$ is independent over time.\n\nWe now derive the expected value $m_t = \\mathbb{E}[Z_t]$ and variance $v_t = \\text{Var}(Z_t)$ from first principles.\n\nExpected value recursion:\nUsing linearity of expectation,\n$$\nm_t = \\mathbb{E}[Z_t] = \\mathbb{E}[\\lambda X_t + (1-\\lambda) Z_{t-1}]\n= \\lambda \\mathbb{E}[X_t] + (1-\\lambda) \\mathbb{E}[Z_{t-1}]\n= \\lambda \\mu_t + (1-\\lambda) m_{t-1},\n$$\nwith initialization $m_0 = \\mathbb{E}[Z_0] = \\mu_1$ because $Z_0$ is fixed to $\\mu_1$.\n\nVariance recursion:\nUsing the variance of a linear combination and the independence assumption, note that $Z_{t-1}$ depends only on $X_1,\\dots,X_{t-1}$, while $X_t$ is independent of the past. Thus $\\text{Cov}(Z_{t-1}, X_t) = 0$. Using $\\text{Var}(aY + bZ) = a^2 \\text{Var}(Y) + b^2 \\text{Var}(Z) + 2ab \\,\\text{Cov}(Y,Z)$, we obtain\n$$\nv_t = \\text{Var}(Z_t) = \\text{Var}(\\lambda X_t + (1-\\lambda) Z_{t-1})\n= \\lambda^2 \\text{Var}(X_t) + (1-\\lambda)^2 \\text{Var}(Z_{t-1}) + 2 \\lambda (1-\\lambda) \\text{Cov}(X_t, Z_{t-1}).\n$$\nBy independence, $\\text{Cov}(X_t, Z_{t-1}) = 0$. For Poisson $X_t$, $\\text{Var}(X_t) = \\mu_t$. Therefore,\n$$\nv_t = (1-\\lambda)^2 v_{t-1} + \\lambda^2 \\mu_t,\n$$\nwith initialization $v_0 = \\text{Var}(Z_0) = 0$ because $Z_0$ is fixed.\n\nControl limits at time $t$:\nAt each time $t$, we compute the time-varying center and limits as\n$$\n\\text{Center: } m_t, \\quad\n\\text{Standard deviation: } s_t = \\sqrt{v_t}, \\quad\n\\text{Upper control limit: } \\text{UCL}_t = m_t + L s_t, \\quad\n\\text{Lower control limit: } \\text{LCL}_t = m_t - L s_t,\n$$\nwhere $L = 3$ is the width multiplier.\n\nSignal logic:\n- Upper signal at month $t$ if $Z_t > \\text{UCL}_t$.\n- Lower signal at month $t$ if $Z_t < \\text{LCL}_t$.\n\nWe now apply this to the specified test cases with $\\lambda = 0.2$.\n\nInitialization common to all cases:\n- $Z_0 = \\mu_1$,\n- $m_0 = \\mu_1$,\n- $v_0 = 0$.\n\nCase 1:\n- $\\mu_t = 5$ for $t=1,\\dots,24$.\n- $x_t = [5,5,5,5,5,5,5,5,5,5,5,5,50,5,5,5,5,5,5,5,5,5,5,5]$.\n\nBecause $\\mu_t$ is constant and $Z_0 = \\mu_1$, it follows from the recursion that $m_t = 5$ for all $t$. The variance $v_t$ converges quickly to its steady-state value $v_\\infty = \\frac{\\lambda}{2-\\lambda} \\mu \\approx \\frac{0.2}{1.8}\\cdot 5 \\approx 0.555\\ldots$; numerically, after the initial transient, $s_t \\approx \\sqrt{0.555\\ldots} \\approx 0.745$, so $\\text{UCL}_t \\approx 5 + 3\\times 0.745 \\approx 7.236$ and $\\text{LCL}_t \\approx 2.764$. The long sequence of $x_t=5$ keeps $Z_t$ at $5$ until month $13$, at which $x_{13}=50$ causes a jump:\n$$\nZ_{13} = (1-\\lambda) Z_{12} + \\lambda x_{13} \\approx 0.8 \\cdot 5 + 0.2 \\cdot 50 = 14,\n$$\nwhich is well above $\\text{UCL}_{13}$. Thereafter, with $x_t = 5$, the difference $d_t = Z_t - m_t$ decays by a factor of $(1-\\lambda) = 0.8$ each month. Solving $9 \\cdot 0.8^k > 3 s_t \\approx 2.236$ gives $k = 0,1,2,3,4,5,6$, corresponding to months $13$ through $19$. No lower signals occur.\n\nResult for Case 1:\n- Upper signals at months $[13,14,15,16,17,18,19]$.\n- Lower signals at months $[]$.\n\nCase 2:\n- $\\mu_t = 0.2$ for $t=1,\\dots,24$.\n- $x_t = [0,0,0,0,0,0,0,0,0,0,5,0,0,0,0,0,0,0,0,0,0,0,0,0]$.\n\nWith constant $\\mu_t$ and $Z_0 = \\mu_1$, $m_t = 0.2$ for all $t$. The steady-state variance is $v_\\infty = \\frac{\\lambda}{2-\\lambda} \\mu \\approx \\frac{0.2}{1.8}\\cdot 0.2 \\approx 0.02222\\ldots$, so $s_t \\approx 0.149$ and $\\text{UCL}_t \\approx 0.2 + 3 \\cdot 0.149 \\approx 0.647$. The initial run of zeros drives $Z_t$ down but never below the lower control limit because it remains positive. At month $11$, the spike to $x_{11} = 5$ yields\n$$\nZ_{11} \\approx 0.8 Z_{10} + 0.2 \\cdot 5 \\approx 0.8 \\cdot 0.0215 + 1 \\approx 1.017,\n$$\nwhich is greater than $\\text{UCL}_{11}$. With subsequent zeros, $d_t = Z_t - m_t$ decays by $0.8$ per month, so $d_{11} \\approx 0.817$. Solving $0.817 \\cdot 0.8^k > 3 s_t \\approx 0.447$ gives $k = 0,1,2$, corresponding to months $11,12,13$. No lower signals occur.\n\nResult for Case 2:\n- Upper signals at months $[11,12,13]$.\n- Lower signals at months $[]$.\n\nCase 3:\n- $\\mu_t = [2,2,2,2,2,2,4,4,4,4,4,4,6,6,6,6,6,6,8,8,8,8,8,8]$ (stepwise increases).\n- $x_t = [2,2,2,2,2,2,4,4,4,4,4,4,6,6,6,6,6,6,40,8,8,8,8,8]$ (equals baseline except a spike at month $19$).\n\nWe compute $m_t$ and $v_t$ recursively. Because $Z_0 = \\mu_1$, $m_0 = \\mu_1$. The expected value follows $m_t = (1-\\lambda) m_{t-1} + \\lambda \\mu_t$ and adapts to step changes gradually. The variance follows $v_t = (1-\\lambda)^2 v_{t-1} + \\lambda^2 \\mu_t$; when $\\mu_t$ is higher, the variance increases slightly, and the standard deviation $s_t = \\sqrt{v_t}$ approaches $\\sqrt{\\frac{\\lambda}{2-\\lambda} \\mu_t}$ during periods of stability.\n\nAt month $19$, the spike $x_{19} = 40$ with $\\mu_{19} = 8$ yields\n$$\nZ_{19} \\approx 0.8 Z_{18} + 0.2 \\cdot 40, \\quad m_{19} = 0.8 m_{18} + 0.2 \\cdot 8.\n$$\nSince $m_{18}$ is close to $6$ and $Z_{18}$ is close to $6$ from the prior stable period, we get $Z_{19} \\approx 12.8$ and $m_{19} \\approx 6.4$. The standard deviation $s_{19}$ is approximately $0.86$, so $\\text{UCL}_{19} \\approx 6.4 + 3 \\cdot 0.86 \\approx 8.99$. Thus month $19$ is an upper signal. Thereafter, with $x_t = \\mu_t = 8$, the deviation $d_t = Z_t - m_t$ obeys $d_t = (1-\\lambda) d_{t-1}$ and decays by a factor of $0.8$ each month while $s_t$ approaches $\\sqrt{\\frac{0.2}{1.8}\\cdot 8} \\approx 0.943$. Solving $d_{19} \\cdot 0.8^k > 3 s_t$ with $d_{19} \\approx 6.4$ and $3 s_t \\approx 2.83$ yields $k = 0,1,2,3$, corresponding to months $19,20,21,22$. No lower signals occur.\n\nResult for Case 3:\n- Upper signals at months $[19,20,21,22]$.\n- Lower signals at months $[]$.\n\nTherefore, the final aggregated results to be printed are:\n- Case 1: $[ [13,14,15,16,17,18,19], [] ]$,\n- Case 2: $[ [11,12,13], [] ]$,\n- Case 3: $[ [19,20,21,22], [] ]$.\n\nThe program must output a single line with no spaces: \"[[[13,14,15,16,17,18,19],[]],[[11,12,13],[]],[[19,20,21,22],[]]]\".", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport json\nimport math\n\ndef ewma_signals(x, mu, lam=0.2, L=3.0):\n    n = len(x)\n    # Initialize\n    z_prev = mu[0]\n    m_prev = mu[0]\n    v_prev = 0.0\n\n    upper_signal_months = []\n    lower_signal_months = []\n\n    for t in range(1, n + 1):\n        xt = x[t - 1]\n        mut = mu[t - 1]\n\n        # Update EWMA\n        z_t = lam * xt + (1.0 - lam) * z_prev\n\n        # Update mean and variance under null\n        m_t = (1.0 - lam) * m_prev + lam * mut\n        v_t = (1.0 - lam) ** 2 * v_prev + (lam ** 2) * mut\n\n        s_t = math.sqrt(v_t)\n        ucl = m_t + L * s_t\n        lcl = m_t - L * s_t\n\n        # Check signals (1-based month index)\n        if z_t > ucl:\n            upper_signal_months.append(t)\n        if z_t  lcl:\n            lower_signal_months.append(t)\n\n        # Prepare for next iteration\n        z_prev = z_t\n        m_prev = m_t\n        v_prev = v_t\n\n    return upper_signal_months, lower_signal_months\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Case 1\n    mu1 = [5.0] * 24\n    x1 = [5,5,5,5,5,5,5,5,5,5,5,5,50,5,5,5,5,5,5,5,5,5,5,5]\n\n    # Case 2\n    mu2 = [0.2] * 24\n    x2 = [0,0,0,0,0,0,0,0,0,0,5,0,0,0,0,0,0,0,0,0,0,0,0,0]\n\n    # Case 3\n    mu3 = [2,2,2,2,2,2,4,4,4,4,4,4,6,6,6,6,6,6,8,8,8,8,8,8]\n    x3 = [2,2,2,2,2,2,4,4,4,4,4,4,6,6,6,6,6,6,40,8,8,8,8,8]\n\n    lam = 0.2\n    L = 3.0\n\n    results = []\n    for x, mu in [(x1, mu1), (x2, mu2), (x3, mu3)]:\n        upper, lower = ewma_signals(x, mu, lam=lam, L=L)\n        results.append([upper, lower])\n\n    # Final print statement in the exact required format (no spaces).\n    print(json.dumps(results, separators=(',',':')))\n\nsolve()\n```", "id": "4520144"}]}