{"hands_on_practices": [{"introduction": "This practice bridges the gap between wet-lab protocols and data quality, a crucial aspect of spatial transcriptomics. You will step into the role of an experimentalist optimizing a key step—enzyme permeabilization—by balancing the trade-off between maximizing RNA yield and minimizing spatial artifacts like signal blurring. This exercise highlights how quantitative analysis of pilot data directly informs the design of a successful large-scale experiment.", "problem": "A laboratory is optimizing enzyme permeabilization for a Spatially Resolved Transcriptomics (SRT) assay on a platform with circular capture spots of fixed diameter. The laboratory measures, for each tested permeabilization time, the observed per-spot Unique Molecular Identifier (UMI) counts from three independent regions, the estimated ambient fraction, and the spatial blurring radius. Assume the following foundations:\n- Observed per-spot counts are the sum of counts from on-spot molecules and counts from ambient (off-spot) molecules.\n- The ambient fraction, denoted $f_{A}$, is defined as the fraction of observed counts originating from ambient molecules; therefore, the fraction of observed counts originating from on-spot molecules is $1 - f_{A}$.\n- The spatial blurring radius, denoted $L$, is an empirical estimate of the root-mean-square displacement of released ribonucleic acid (RNA) prior to capture. To limit spot-to-spot cross-contamination, the laboratory imposes the constraint $L < R/2$, where $R$ is the capture spot radius.\n- Replicates are independent with the same $f_{A}$ for a given time, and the arithmetic mean across replicates is used to estimate the expected observed per-spot counts for that time.\n\nThe platform’s capture spot diameter is $55~\\mu\\mathrm{m}$, so the spot radius is $R = 27.5~\\mu\\mathrm{m}$ and the blurring constraint is $L < R/2 = 13.75~\\mu\\mathrm{m}$.\n\nThree permeabilization times are evaluated: $5~\\mathrm{min}$, $10~\\mathrm{min}$, and $20~\\mathrm{min}$. For each time, the following data are measured:\n- Time $5~\\mathrm{min}$:\n  - Replicate observed per-spot UMI counts: $2{,}100$, $2{,}050$, $2{,}200$.\n  - Ambient fraction: $f_{A} = 0.08$.\n  - Spatial blurring radius: $L = 7.5~\\mu\\mathrm{m}$.\n- Time $10~\\mathrm{min}$:\n  - Replicate observed per-spot UMI counts: $3{,}400$, $3{,}600$, $3{,}550$.\n  - Ambient fraction: $f_{A} = 0.12$.\n  - Spatial blurring radius: $L = 12.0~\\mu\\mathrm{m}$.\n- Time $20~\\mathrm{min}$:\n  - Replicate observed per-spot UMI counts: $5{,}600$, $5{,}650$, $5{,}400$.\n  - Ambient fraction: $f_{A} = 0.20$.\n  - Spatial blurring radius: $L = 21.0~\\mu\\mathrm{m}$.\n\nUsing only the definitions above, estimate for each time the expected on-spot per-spot UMI yield by correcting the mean observed per-spot UMI counts for the ambient fraction, then enforce the blurring constraint $L < 13.75~\\mu\\mathrm{m}$. Among the times that satisfy the constraint, select the permeabilization time that maximizes the expected on-spot per-spot UMI yield.\n\nReport the optimal permeabilization time in minutes as an integer. Do not include units in your final boxed answer.", "solution": "The objective is to determine the optimal enzyme permeabilization time from a set of tested times ($5~\\mathrm{min}$, $10~\\mathrm{min}$, $20~\\mathrm{min}$) that maximizes the expected on-spot Unique Molecular Identifier (UMI) yield, subject to a constraint on spatial blurring.\n\nFirst, we formalize the quantities involved. Let $\\bar{C}_{obs}$ be the expected observed per-spot UMI count, which is estimated by the arithmetic mean of the replicate counts. Let $f_A$ be the ambient fraction. The problem states that the expected on-spot per-spot UMI yield, which we denote as $C_{on-spot}$, can be calculated by correcting the observed counts for the ambient fraction. The fraction of on-spot counts is $(1 - f_A)$. Therefore, the relationship is:\n$$C_{on-spot} = \\bar{C}_{obs} \\times (1 - f_A)$$\n\nThe laboratory imposes a constraint on the spatial blurring radius, $L$, to limit cross-contamination between capture spots. The capture spot radius is given by half the diameter, so $R = \\frac{55~\\mu\\mathrm{m}}{2} = 27.5~\\mu\\mathrm{m}$. The constraint is $L < R/2$, which evaluates to:\n$$L < \\frac{27.5~\\mu\\mathrm{m}}{2} = 13.75~\\mu\\mathrm{m}$$\nAny permeabilization time leading to a blurring radius that violates this constraint must be discarded.\n\nWe will now evaluate each of the three permeabilization times.\n\n**1. Evaluation for time $t_1 = 5~\\mathrm{min}$**\nThe data provided are:\n- Replicate observed per-spot UMI counts: $2100$, $2050$, $2200$.\n- Ambient fraction: $f_{A,1} = 0.08$.\n- Spatial blurring radius: $L_1 = 7.5~\\mu\\mathrm{m}$.\n\nFirst, we calculate the mean observed UMI count, $\\bar{C}_{obs,1}$:\n$$\\bar{C}_{obs,1} = \\frac{2100 + 2050 + 2200}{3} = \\frac{6350}{3}$$\nNext, we calculate the expected on-spot UMI yield, $C_{on-spot,1}$:\n$$C_{on-spot,1} = \\bar{C}_{obs,1} \\times (1 - f_{A,1}) = \\frac{6350}{3} \\times (1 - 0.08) = \\frac{6350}{3} \\times 0.92 = \\frac{5842}{3} \\approx 1947.33$$\nFinally, we check the blurring constraint:\n$$L_1 = 7.5~\\mu\\mathrm{m}$$\nSince $7.5 < 13.75$, the constraint is satisfied. The time $t_1 = 5~\\mathrm{min}$ is a valid candidate.\n\n**2. Evaluation for time $t_2 = 10~\\mathrm{min}$**\nThe data provided are:\n- Replicate observed per-spot UMI counts: $3400$, $3600$, $3550$.\n- Ambient fraction: $f_{A,2} = 0.12$.\n- Spatial blurring radius: $L_2 = 12.0~\\mu\\mathrm{m}$.\n\nFirst, we calculate the mean observed UMI count, $\\bar{C}_{obs,2}$:\n$$\\bar{C}_{obs,2} = \\frac{3400 + 3600 + 3550}{3} = \\frac{10550}{3}$$\nNext, we calculate the expected on-spot UMI yield, $C_{on-spot,2}$:\n$$C_{on-spot,2} = \\bar{C}_{obs,2} \\times (1 - f_{A,2}) = \\frac{10550}{3} \\times (1 - 0.12) = \\frac{10550}{3} \\times 0.88 = \\frac{9284}{3} \\approx 3094.67$$\nFinally, we check the blurring constraint:\n$$L_2 = 12.0~\\mu\\mathrm{m}$$\nSince $12.0 < 13.75$, the constraint is satisfied. The time $t_2 = 10~\\mathrm{min}$ is also a valid candidate.\n\n**3. Evaluation for time $t_3 = 20~\\mathrm{min}$**\nThe data provided are:\n- Replicate observed per-spot UMI counts: $5600$, $5650$, $5400$.\n- Ambient fraction: $f_{A,3} = 0.20$.\n- Spatial blurring radius: $L_3 = 21.0~\\mu\\mathrm{m}$.\n\nFirst, we calculate the mean observed UMI count, $\\bar{C}_{obs,3}$:\n$$\\bar{C}_{obs,3} = \\frac{5600 + 5650 + 5400}{3} = \\frac{16650}{3} = 5550$$\nNext, we calculate the expected on-spot UMI yield, $C_{on-spot,3}$:\n$$C_{on-spot,3} = \\bar{C}_{obs,3} \\times (1 - f_{A,3}) = 5550 \\times (1 - 0.20) = 5550 \\times 0.80 = 4440$$\nFinally, we check the blurring constraint:\n$$L_3 = 21.0~\\mu\\mathrm{m}$$\nSince $21.0$ is not less than $13.75$, the constraint $L_3 < 13.75~\\mu\\mathrm{m}$ is violated. Therefore, the time $t_3 = 20~\\mathrm{min}$ is not a valid candidate, and we do not consider its on-spot UMI yield for the optimization, despite it being the highest.\n\n**Conclusion**\nWe are left with two valid permeabilization times: $5~\\mathrm{min}$ and $10~\\mathrm{min}$. To select the optimal one, we compare their expected on-spot UMI yields:\n$$C_{on-spot,1} = \\frac{5842}{3} \\approx 1947.33$$\n$$C_{on-spot,2} = \\frac{9284}{3} \\approx 3094.67$$\n\nComparing these two values, we see that $C_{on-spot,2} > C_{on-spot,1}$ since $\\frac{9284}{3} > \\frac{5842}{3}$.\nThus, the permeabilization time of $10~\\mathrm{min}$ maximizes the expected on-spot UMI yield among the times that satisfy the blurring constraint.\n\nThe optimal permeabilization time is $10$ minutes.", "answer": "$$\\boxed{10}$$", "id": "4385508"}, {"introduction": "Once raw count data is generated, it must be normalized to correct for technical artifacts, such as variable sequencing depth between spots, before biological patterns can be reliably compared. This exercise guides you through implementing and contrasting three foundational normalization techniques: the simple Counts Per Million (CPM), the variance-stabilizing log-transformation, and the more sophisticated model-based `sctransform` approach. Mastering these methods is fundamental for preparing any transcriptomics dataset for downstream analysis.", "problem": "You are given spot-wise total counts and gene-by-spot count matrices that represent Spatial Transcriptomics measurements. Let there be $G$ genes indexed by $g \\in \\{1,\\dots,G\\}$ and $S$ spatial spots indexed by $s \\in \\{1,\\dots,S\\}$. The observed integer counts for gene $g$ at spot $s$ are denoted by $x_{gs}$, and the total counts for spot $s$ (the library size) are denoted by $c_s$. You must compute three normalizations and compare the variance across genes for each normalization.\n\nFundamental base and definitions:\n- In count-based transcriptomics, observed counts are nonnegative integers driven by the underlying abundance and sampling depth. The total counts per spot $c_s$ act as an exposure scalar. A widely accepted stochastic model for count data is the Negative Binomial (NB) distribution, where $x_{gs}$ is modeled with mean $\\mu_{gs}$ and an overdispersion parameter $\\theta_g$ such that the variance is $\\operatorname{Var}(x_{gs}) = \\mu_{gs} + \\mu_{gs}^2 / \\theta_g$. The offset formulation in a Generalized Linear Model (GLM) with a log link sets $\\log(\\mu_{gs}) = \\beta_{0g} + \\log(c_s)$, implying $\\mu_{gs} = c_s \\cdot p_g$ for a gene-specific rate $p_g = e^{\\beta_{0g}}$.\n- Counts Per Million (CPM) normalization rescales counts to a per-million basis to adjust for library size. Specifically, for each spot $s$, the CPM-normalized value for gene $g$ is $n_{gs}^{\\mathrm{CPM}} = 10^6 \\cdot x_{gs} / c_s$ if $c_s > 0$ and $n_{gs}^{\\mathrm{CPM}} = 0$ if $c_s = 0$.\n- The natural logarithm transformation with a pseudocount, denoted \"log1p,\" is defined as $y_{gs}^{\\log 1\\mathrm{p}} = \\log\\left(1 + n_{gs}^{\\mathrm{CPM}}\\right)$.\n- The single-cell transform (sctransform) residuals are derived from an NB-GLM with library size as an offset. With $\\mu_{gs} = c_s \\cdot p_g$ and overdispersion parameter $\\theta_g$, the Pearson residuals are defined as\n$$\nr_{gs} = \n\\begin{cases}\n\\dfrac{x_{gs} - \\mu_{gs}}{\\sqrt{\\mu_{gs} + \\mu_{gs}^2/\\theta_g}}, & \\text{if } \\mu_{gs} > 0,\\\\\n0, & \\text{if } \\mu_{gs} = 0.\n\\end{cases}\n$$\nTo determine $\\theta_g$, use the well-tested fact that under a correctly specified NB model the expected squared Pearson residual is approximately $1$. Solve for $\\theta_g > 0$ such that\n$$\n\\frac{1}{|\\mathcal{I}_g|}\\sum_{s \\in \\mathcal{I}_g} \\frac{\\left(x_{gs} - \\mu_{gs}\\right)^2}{\\mu_{gs} + \\mu_{gs}^2/\\theta_g} = 1,\n$$\nwhere $\\mathcal{I}_g = \\{ s \\,|\\, \\mu_{gs} > 0 \\}$. If no such solution yields a value greater than or equal to $1$ even as $\\theta_g \\to \\infty$ (the Poisson limit), set $\\theta_g$ to a large value (e.g., $\\theta_g = 10^8$). If $c_s = 0$ for any spot $s$, define $n_{gs}^{\\mathrm{CPM}} = 0$ and $\\mu_{gs} = 0$ for that spot.\n\nTasks:\n1. Implement CPM normalization to compute $n_{gs}^{\\mathrm{CPM}}$ for all genes and spots.\n2. Implement log1p transformation on CPM values to compute $y_{gs}^{\\log 1\\mathrm{p}}$ for all genes and spots.\n3. Implement an NB-GLM with offset to compute sctransform Pearson residuals $r_{gs}$:\n   - Estimate $p_g$ by maximum likelihood under the offset model as $p_g = \\left(\\sum_{s=1}^{S} x_{gs}\\right)\\Big/\\left(\\sum_{s : c_s > 0} c_s\\right)$.\n   - Set $\\mu_{gs} = c_s \\cdot p_g$ for all spots.\n   - Solve for $\\theta_g$ using the equation above via a robust one-dimensional root-finding method on $\\theta_g \\in (0, +\\infty)$ (for example, bisection on a sufficiently large interval).\n   - Compute $r_{gs}$.\n4. For each normalization, compute the per-gene population variance across spots. For any vector $z_{g\\cdot} = (z_{g1},\\dots,z_{gS})$, define the population variance as\n$$\n\\operatorname{Var}_{\\text{pop}}(z_{g\\cdot}) = \\frac{1}{S} \\sum_{s=1}^{S} \\left(z_{gs} - \\frac{1}{S}\\sum_{t=1}^{S} z_{gt}\\right)^2.\n$$\n\nAngle units are not applicable. Physical units are counts; no unit conversion is required beyond CPM as defined.\n\nTest suite:\n- Test Case 1:\n  - $S = 5$, $G = 4$,\n  - $c = [5000, 7000, 6000, 8000, 5500]$,\n  - $X = \\begin{bmatrix}\n  50 & 70 & 60 & 80 & 55 \\\\\n  200 & 150 & 180 & 220 & 170 \\\\\n  0 & 5 & 10 & 15 & 0 \\\\\n  1000 & 1400 & 1100 & 1600 & 1200\n  \\end{bmatrix}$.\n- Test Case 2:\n  - $S = 3$, $G = 3$,\n  - $c = [0, 3000, 4000]$,\n  - $X = \\begin{bmatrix}\n  0 & 30 & 40 \\\\\n  0 & 300 & 400 \\\\\n  0 & 0 & 10\n  \\end{bmatrix}$.\n- Test Case 3:\n  - $S = 2$, $G = 5$,\n  - $c = [10000, 15000]$,\n  - $X = \\begin{bmatrix}\n  0 & 0 \\\\\n  4000 & 6000 \\\\\n  100 & 300 \\\\\n  5000 & 7000 \\\\\n  10 & 10\n  \\end{bmatrix}$.\n\nOutput specification:\n- For each test case, produce a list of three lists:\n  1. The list of per-gene population variances of $n_{gs}^{\\mathrm{CPM}}$ across spots (one float per gene).\n  2. The list of per-gene population variances of $y_{gs}^{\\log 1\\mathrm{p}}$ across spots (one float per gene).\n  3. The list of per-gene population variances of $r_{gs}$ across spots (one float per gene).\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, with each float rounded to six decimal places. The final format must be:\n\"[[list_for_test_case_1],[list_for_test_case_2],[list_for_test_case_3]]\",\nwhere each list_for_test_case_k is itself \"[[var_CPM_gene_1,...,var_CPM_gene_G],[var_log1p_gene_1,...,var_log1p_gene_G],[var_sctransform_gene_1,...,var_sctransform_gene_G]]\".", "solution": "The problem requires the implementation and comparison of three different normalization methods for spatial transcriptomics count data. The methods are Counts Per Million (CPM), a logarithmic transformation of CPM values (log1p), and Pearson residuals from a Negative Binomial Generalized Linear Model (sctransform). For each method, the per-gene population variance across spatial spots is to be calculated.\n\nThe problem has been validated and found to be scientifically sound, well-posed, and objective. It is grounded in established statistical and computational methods used in single-cell and spatial transcriptomics. All necessary data, models, and procedures are explicitly defined, allowing for a unique and verifiable solution.\n\nThe solution will be developed by implementing each normalization step as specified, followed by the calculation of population variance for each gene.\n\nLet the given data be the gene-by-spot count matrix $X = \\{x_{gs}\\}$ for $g \\in \\{1,\\dots,G\\}$ genes and $s \\in \\{1,\\dots,S\\}$ spots, and the vector of total spot counts (library sizes) $c = \\{c_s\\}$.\n\n**1. Normalization Methods**\n\n**a. Counts Per Million (CPM) Normalization**\n\nThis method normalizes the raw counts $x_{gs}$ by the total counts $c_s$ for each spot and scales the result by a factor of one million. This adjusts for variations in sequencing depth across spots. The formula is:\n$$\nn_{gs}^{\\mathrm{CPM}} = \n\\begin{cases}\n\\dfrac{x_{gs}}{c_s} \\times 10^6, & \\text{if } c_s > 0, \\\\\n0, & \\text{if } c_s = 0.\n\\end{cases}\n$$\nThe resulting matrix $N^{\\mathrm{CPM}} = \\{n_{gs}^{\\mathrm{CPM}}\\}$ represents the counts as if every spot had a total count of one million.\n\n**b. Logarithmic Transformation (log1p)**\n\nCount data in genomics are often highly skewed. Applying a logarithmic transformation can help stabilize the variance and make the data distribution more symmetric, which is beneficial for many downstream statistical analyses. A pseudocount of $1$ is added before taking the natural logarithm to avoid issues with zero-count values (i.e., $\\log(0)$). The formula applied to the CPM values is:\n$$\ny_{gs}^{\\log 1\\mathrm{p}} = \\log(1 + n_{gs}^{\\mathrm{CPM}})\n$$\nThe resulting matrix is $Y^{\\log 1\\mathrm{p}} = \\{y_{gs}^{\\log 1\\mathrm{p}}\\}$.\n\n**c. Sctransform Pearson Residuals**\n\nThis method is based on a more sophisticated statistical model of the count data. It models the counts $x_{gs}$ using a Negative Binomial (NB) distribution with the library size $c_s$ as an exposure term. The mean $\\mu_{gs}$ is modeled as being proportional to the library size: $\\mu_{gs} = p_g c_s$, where $p_g$ is a gene-specific relative abundance parameter. The variance of the NB distribution is given by $\\operatorname{Var}(x_{gs}) = \\mu_{gs} + \\mu_{gs}^2 / \\theta_g$, where $\\theta_g$ is the gene-specific overdispersion parameter. The goal is to compute Pearson residuals, which are standardized values that, under a perfect model fit, should have a mean of $0$ and a variance of $1$.\n\nThe procedure is as follows for each gene $g$:\n\n**I. Estimate Rate Parameter $p_g$:**\nThe parameter $p_g$ is estimated using maximum likelihood, which, for this model formulation, simplifies to the ratio of total gene counts to total effective library sizes:\n$$\np_g = \\frac{\\sum_{s=1}^{S} x_{gs}}{\\sum_{s: c_s > 0} c_s}\n$$\nIf $\\sum x_{gs} = 0$, then $p_g=0$, all expected values $\\mu_{gs}$ are $0$, all residuals are $0$, and the variance is $0$.\n\n**II. Estimate Overdispersion Parameter $\\theta_g$:**\nThe parameter $\\theta_g$ controls how much the variance exceeds the mean. It is estimated by solving the following equation, which sets the average squared Pearson residual to its expected value of $1$:\n$$\n\\frac{1}{|\\mathcal{I}_g|}\\sum_{s \\in \\mathcal{I}_g} \\frac{\\left(x_{gs} - \\mu_{gs}\\right)^2}{\\mu_{gs} + \\mu_{gs}^2/\\theta_g} = 1\n$$\nwhere $\\mathcal{I}_g = \\{ s \\,|\\, \\mu_{gs} > 0 \\}$. This equation is solved for $\\theta_g > 0$ using a numerical root-finding algorithm. The left-hand side is a monotonically increasing function of $\\theta_g$. We first evaluate the function in the limit $\\theta_g \\to \\infty$ (the Poisson case):\n$$\nL_{\\infty} = \\frac{1}{|\\mathcal{I}_g|}\\sum_{s \\in \\mathcal{I}_g} \\frac{\\left(x_{gs} - \\mu_{gs}\\right)^2}{\\mu_{gs}}\n$$\nIf $L_{\\infty} < 1$, the data are under-dispersed relative to the Poisson model, and no positive finite solution for $\\theta_g$ exists. In this scenario, as specified, we set $\\theta_g$ to a large value, $\\theta_g = 10^8$. Otherwise, a unique root $\\theta_g$ exists and can be found using a one-dimensional solver like Brent's method on a suitable interval, e.g., $(0, 10^{12}]$.\n\n**III. Compute Pearson Residuals $r_{gs}$:**\nWith estimates for $p_g$ (and thus $\\mu_{gs}$) and $\\theta_g$, the Pearson residuals are calculated as:\n$$\nr_{gs} = \n\\begin{cases}\n\\dfrac{x_{gs} - \\mu_{gs}}{\\sqrt{\\mu_{gs} + \\mu_{gs}^2/\\theta_g}}, & \\text{if } \\mu_{gs} > 0, \\\\\n0, & \\text{if } \\mu_{gs} = 0.\n\\end{cases}\n$$\nThe matrix of residuals is $R = \\{r_{gs}\\}$.\n\n**2. Variance Calculation**\n\nFor each gene $g$ and for each of the three resulting data matrices ($N^{\\mathrm{CPM}}$, $Y^{\\log 1\\mathrm{p}}$, and $R$), the population variance across the $S$ spots is computed. For a generic vector of transformed values for gene $g$, $z_{g\\cdot} = (z_{g1}, \\dots, z_{gS})$, the population variance is:\n$$\n\\operatorname{Var}_{\\text{pop}}(z_{g\\cdot}) = \\frac{1}{S} \\sum_{s=1}^{S} \\left(z_{gs} - \\bar{z}_g\\right)^2, \\quad \\text{where } \\bar{z}_g = \\frac{1}{S}\\sum_{t=1}^{S} z_{gt}\n$$\nThis calculation will be performed for each gene, yielding three lists of variances per test case, corresponding to the three normalization methods.\n\nThe implementation will process each test case, apply these steps to each gene, and format the resulting variance lists according to the specified output format.", "answer": "[[[0.000000,604.000000,34.240000,1696.000000],[0.000000,0.000014,0.120625,0.000100],[0.999943,0.867946,0.957597,1.000000]],[[1111.111111,1111.111111,27.777778],[1.359206,1.359206,0.612401],[1.000000,1.000000,0.666667]],[[0.000000,4000000.000000,40000.000000,1000000.000000,0.000000],[0.000000,0.001598,0.010204,0.000399,0.000000],[0.000000,1.000000,0.999971,0.999732,0.000000]]]", "id": "4385438"}, {"introduction": "The defining feature of spatial transcriptomics data is the coordinate of each measurement, which allows us to explore tissue architecture and cell-cell communication. To leverage this spatial information computationally, we first represent the spatial relationships between measurement spots as a neighborhood graph. In this practice, you will construct a $k$-Nearest Neighbors ($k$-NN) graph from spot coordinates, a foundational data structure that enables a wide range of downstream spatial analyses, including domain segmentation and identification of spatial expression patterns.", "problem": "You are analyzing spatial spot layouts from spatial transcriptomics to construct neighborhood graphs used in downstream tasks such as spatial smoothing and domain segmentation. Given a finite set of two-dimensional spot coordinates and a target average degree, you must determine the integer neighborhood parameter $k$ for a $k$-Nearest Neighbors (k-NN) graph that best matches the target average degree when the directed $k$-Nearest Neighbors graph is symmetrized by undirected union. Then you must verify well-posed graph-theoretic properties of the resulting degree distribution.\n\nUse the following fundamental base:\n- Distances are Euclidean in $\\mathbb{R}^2$: for spots $i$ and $j$ at positions $\\mathbf{x}_i$ and $\\mathbf{x}_j$, the distance is $d(i,j) = \\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2 = \\sqrt{(x_{i1} - x_{j1})^2 + (x_{i2} - x_{j2})^2}$.\n- A directed $k$-Nearest Neighbors (k-NN) relation from node $i$ to node $j$ exists if $j$ is among the $k$ smallest values of $d(i,j)$ over $j \\neq i$.\n- To obtain an undirected graph from the directed $k$-Nearest Neighbors graph, use union symmetrization: there is an undirected edge between $i$ and $j$ if and only if either $i$ lists $j$ among its $k$ nearest neighbors or $j$ lists $i$ among its $k$ nearest neighbors.\n- The degree of a node in an undirected simple graph is the number of incident edges. By the handshake lemma, $\\sum_{i=1}^{N} \\deg(i) = 2|E|$.\n\nDeterminism requirement for neighbor selection under ties:\n- When distances are equal, break ties by smaller node index first. That is, to select $k$ neighbors for node $i$, sort candidates $j \\neq i$ by the pair $(d(i,j), j)$ in ascending lexicographic order and take the first $k$.\n\nTask:\n- For each test case, given coordinates for $N$ spots and a target average degree $\\bar{d}_{\\text{target}}$, search over integer $k \\in \\{1, 2, \\dots, N-1\\}$ to construct the undirected union-symmetrized $k$-Nearest Neighbors graph and compute its average degree $\\bar{d}_k$. Choose $k^\\star$ that minimizes $|\\bar{d}_k - \\bar{d}_{\\text{target}}|$. If multiple $k$ achieve the same minimum absolute deviation, choose the smallest $k$ among them.\n- Verify two properties of the resulting degree distribution: \n  1. Exact handshake consistency: $\\left(\\sum_{i=1}^{N} \\deg(i)\\right) = 2|E|$.\n  2. Proximity to the target: $|\\bar{d}_{k^\\star} - \\bar{d}_{\\text{target}}| \\le \\theta$, where the tolerance $\\theta$ is fixed to $\\theta = 0.25$.\n\nFor each test case, produce the result as a list $[k^\\star, \\bar{d}_{k^\\star}, \\text{handshake\\_ok}, \\text{within\\_tolerance}]$ where $k^\\star$ is an integer, $\\bar{d}_{k^\\star}$ is a real number (float), and the two verification flags are booleans.\n\nTest suite:\n- Case A (linear array, happy path):\n  - Coordinates: $(0.0, 0.0)$, $(1.0, 0.0)$, $(2.0, 0.0)$, $(3.0, 0.0)$, $(4.0, 0.0)$, $(5.0, 0.0)$.\n  - Target average degree: $\\bar{d}_{\\text{target}} = 2.0$.\n- Case B (square, boundary near completeness):\n  - Coordinates: $(0.0, 0.0)$, $(1.0, 0.0)$, $(0.0, 1.0)$, $(1.0, 1.0)$.\n  - Target average degree: $\\bar{d}_{\\text{target}} = 3.0$.\n- Case C (grid, interior structure):\n  - Coordinates: $(0.0, 0.0)$, $(1.0, 0.0)$, $(2.0, 0.0)$, $(0.0, 1.0)$, $(1.0, 1.0)$, $(2.0, 1.0)$, $(0.0, 2.0)$, $(1.0, 2.0)$, $(2.0, 2.0)$.\n  - Target average degree: $\\bar{d}_{\\text{target}} = 4.0$.\n- Case D (duplicates, tie-handling edge case):\n  - Coordinates: $(0.0, 0.0)$, $(0.0, 0.0)$, $(1.0, 0.0)$, $(2.0, 0.0)$, $(2.0, 0.0)$.\n  - Target average degree: $\\bar{d}_{\\text{target}} = 2.0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list in the format $[k^\\star,\\bar{d}_{k^\\star},\\text{handshake\\_ok},\\text{within\\_tolerance}]$. For example: $[[1,2.0,True,True],[\\dots]]$.\n\nNo physical units or angles are involved in this task. All computations are unitless. The tolerance is $\\theta = 0.25$ applied uniformly to all cases. The graph is unweighted and simple (no self-loops, no multiple edges).", "solution": "The user has provided a well-defined computational problem situated in the domain of spatial transcriptomics data analysis. The task is to determine an optimal neighborhood parameter, $k$, for constructing a $k$-Nearest Neighbors (k-NN) graph that best approximates a target average degree, and to verify certain properties of the resulting graph.\n\nThe problem is validated as follows:\n-   **Scientific Grounding**: The problem is based on established methods in computational geometry and graph theory (Euclidean distance, k-NN graphs, graph degrees, handshake lemma) applied to a relevant problem in spatial genomics. All premises are scientifically and mathematically sound.\n-   **Well-Posedness**: The problem is unambiguous. The inputs are clearly specified. The process for constructing the graph, including a deterministic tie-breaking rule, is explicitly defined. The optimization criterion for selecting the best parameter $k^\\star$ is also explicit (minimizing absolute deviation from a target, with the smallest $k$ as a tie-breaker), ensuring a unique solution exists.\n-   **Objectivity**: The problem statement is formal and quantitative, free from subjective language.\n-   **Completeness**: The problem provides all necessary information, including coordinates for test cases, the target average degree for each, and the value of the tolerance parameter $\\theta$.\n\nThe problem is deemed valid and a solution can be formulated.\n\nThe solution proceeds by implementing the specified graph construction and search algorithm. For each test case, given a set of $N$ spot coordinates $\\{\\mathbf{x}_i\\}_{i=1}^N$ in $\\mathbb{R}^2$ and a target average degree $\\bar{d}_{\\text{target}}$, the algorithm iterates through all possible integer values for the neighborhood parameter $k \\in \\{1, 2, \\dots, N-1\\}$.\n\nFor each value of $k$, we construct a graph $G_k = (V, E_k)$ where $V$ is the set of $N$ spots. The construction involves two main steps:\n1.  **Directed k-NN Graph Construction**: For each spot $i$, we identify its set of $k$ nearest neighbors, denoted $\\text{NN}_k(i)$. This is achieved by computing the Euclidean distance $d(i,j) = \\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2$ to all other spots $j \\neq i$. The candidates $j$ are then sorted in ascending order based on the lexicographical pair $(d(i,j), j)$. The first $k$ spots in this sorted list constitute $\\text{NN}_k(i)$. This step defines a set of directed edges $(i, j)$ for all $j \\in \\text{NN}_k(i)$.\n\n2.  **Symmetrization by Union**: An undirected edge $\\{i, j\\}$ is included in the edge set $E_k$ if and only if spot $j$ is a $k$-nearest neighbor of spot $i$, or spot $i$ is a $k$-nearest neighbor of spot $j$. Formally, $E_k = \\{\\{i, j\\} \\mid j \\in \\text{NN}_k(i) \\lor i \\in \\text{NN}_k(j)\\}$. This construction guarantees a simple undirected graph, as self-loops are disallowed and duplicate edges are implicitly handled by the set-based definition of $E_k$.\n\nOnce the graph $G_k$ is constructed, we compute its properties. The degree of a node $i$, $\\deg_k(i)$, is the number of edges incident to it. The average degree for the graph is then $\\bar{d}_k = \\frac{1}{N} \\sum_{i=1}^{N} \\deg_k(i)$.\n\nAfter computing $\\bar{d}_k$ for all possible values of $k$, we select the optimal parameter $k^\\star$ as the one that minimizes the absolute deviation $|\\bar{d}_k - \\bar{d}_{\\text{target}}|$. If a tie occurs, meaning multiple values of $k$ yield the same minimum deviation, the smallest of these $k$ values is chosen, as per the problem specification.\n\nFinally, for the optimal graph $G_{k^\\star}$, two verification checks are performed:\n1.  **Handshake Consistency**: We verify that the sum of degrees equals twice the number of edges: $\\sum_{i=1}^{N} \\deg_{k^\\star}(i) = 2|E_{k^\\star}|$. This is a fundamental property of any undirected graph (Handshake Lemma) and serves as a sanity check for the correctness of the graph data structure implementation. The sum of degrees must be an even integer.\n2.  **Proximity to Target**: We check whether the achieved average degree $\\bar{d}_{k^\\star}$ is within a specified tolerance $\\theta=0.25$ of the target: $|\\bar{d}_{k^\\star} - \\bar{d}_{\\text{target}}| \\le \\theta$.\n\nThe algorithm is implemented in Python, using the `numpy` library for efficient vectorized computation of the pairwise distance matrix. The core logic is encapsulated in a function that processes a single test case. A main script defines the test suite and iterates through each case, collecting the results and formatting them into the specified output string.", "answer": "[[2,2.0,True,True],[3,3.0,True,True],[2,3.5555555555555554,True,False],[2,2.4,True,False]]", "id": "4385418"}]}