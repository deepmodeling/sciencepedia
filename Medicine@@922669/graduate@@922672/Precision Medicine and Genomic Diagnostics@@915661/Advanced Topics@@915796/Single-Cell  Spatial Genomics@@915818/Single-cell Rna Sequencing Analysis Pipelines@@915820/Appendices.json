{"hands_on_practices": [{"introduction": "Before uncovering biological insights, we must first ensure the quality of our single-cell data. This foundational practice introduces a robust statistical method for filtering out outlier cells, such as those that are damaged or represent technical artifacts like doublets. By using the median and Median Absolute Deviation (MAD), we can establish quality control thresholds that are resistant to the influence of the very outliers we aim to remove, ensuring a clean dataset for downstream analysis [@problem_id:4382174].", "problem": "You are given three quality control metrics for Single-Cell Ribonucleic Acid sequencing (scRNA-seq) data: Unique Molecular Identifier (UMI) counts per cell, genes detected per cell, and the fraction of reads mapping to mitochondrial genes per cell (mitochondrial fraction). Your goal is to design a robust outlier filtering rule grounded in robust statistics and apply it to concrete, specified test datasets. From first principles, start with the definition of the median and the Median Absolute Deviation (MAD), and, under the Normal model where the standard deviation relates to the MAD by a fixed consistency constant, derive a symmetric outlier rule for UMI counts and genes detected, and a one-sided outlier rule for mitochondrial fraction. Choose an outlier tuning parameter $k$ and justify the choice. Address degeneracies in scale estimation that can arise from discrete counts or tied values by regularizing the scale with scientifically defensible floors. Then implement the filtering rule to determine which cells should be removed.\n\nUse the following foundational definitions and facts:\n- The median of a dataset is the value $m$ such that half the observations are not greater than $m$ and half are not less than $m$.\n- The Median Absolute Deviation (MAD) is defined as $\\mathrm{MAD} = \\mathrm{median}(|x_i - m|)$, where $m$ is the median of the dataset $\\{x_i\\}$.\n- For a Normal distribution, the standard deviation $\\sigma$ is related to the Median Absolute Deviation by $\\sigma \\approx c \\cdot \\mathrm{MAD}$ with a known constant $c$.\n- The mitochondrial fraction is a proportion required to be expressed as a decimal in $[0,1]$, not as a percentage.\n\nDesign requirements:\n1. Derive a robust scale estimator from $\\mathrm{MAD}$ that is consistent with the standard deviation under the Normal model, and construct symmetric thresholds around the median for UMI counts and genes detected using a chosen $k$. Construct a one-sided upper threshold for the mitochondrial fraction using the same scale concept and $k$.\n2. When $\\mathrm{MAD}$ evaluates to $0$ due to ties or discrete counts, regularize the scale to avoid degeneracy. For count-like metrics, use a fractional floor scaled to the median. For bounded proportions, use a floor based on the smallest positive deviation observed from the median if available, otherwise use a small positive constant respecting the $[0,1]$ bounds.\n3. Apply the rule to determine the set of cell indices that should be filtered, where a cell is filtered if it violates any threshold across the three metrics.\n\nInterpretation and expected downstream impact: Explain logically how filtering such outliers affects downstream clustering in scRNA-seq pipelines, including doublet removal and exclusion of stressed or dying cells characterized by high mitochondrial fraction, as well as how over-stringent thresholds may alter cluster boundaries.\n\nTuning parameter:\n- Use $k = 3$.\n\nTest suite and inputs:\nYou must implement and run a program with the following three test cases. Each test case specifies arrays of equal length for the three metrics: UMI counts, genes detected, and mitochondrial fraction. Cell indices are $0$-based. All numbers must be treated as scalars without units; mitochondrial fraction must be treated as a decimal, not a percentage.\n\nTest case $1$ (dataset with medians near $5{,}000$ UMIs, $1{,}600$ genes, mitochondrial fraction centered at $0.08$):\n- UMI: $[5200,4800,5100,4950,5050,5300,4700,5000,5150,4850,5080,4920,4995,5010,4975,16000,600,5400,4600,5005]$\n- Genes: $[1650,1580,1620,1590,1610,1700,1550,1600,1630,1570,1615,1595,1605,1608,1592,3500,400,1680,1520,1602]$\n- Mitochondrial fraction: $[0.07,0.09,0.08,0.085,0.075,0.06,0.1,0.08,0.07,0.09,0.082,0.078,0.08,0.081,0.079,0.04,0.25,0.065,0.11,0.08]$\n\nTest case $2$ (degenerate ties around the specified medians, with a few deviations):\n- UMI: $[5000,5000,5000,5000,5000,5000,5000,5000,5000,2000,8000,5000]$\n- Genes: $[1600,1600,1600,1600,1600,1600,1600,1600,1600,900,2800,1600]$\n- Mitochondrial fraction: $[0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.2,0.06,0.08]$\n\nTest case $3$ (heavy-tailed mixture around the specified centers):\n- UMI: $[5100,4950,5300,4700,5500,4800,6200,3000,5800,4900,5050,5150,4000,7000,5200]$\n- Genes: $[1620,1580,1700,1500,1750,1550,1800,1100,1720,1590,1610,1630,1200,1900,1600]$\n- Mitochondrial fraction: $[0.09,0.07,0.06,0.12,0.05,0.08,0.04,0.18,0.06,0.09,0.08,0.07,0.16,0.03,0.085]$\n\nOutput specification:\n- For each test case, output the list of filtered cell indices (as integers) sorted in ascending order.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of integers for the corresponding test case, for example: $[[i\\_1,i\\_2],[],[j\\_1]]$.", "solution": "The task is to design and implement a robust outlier filtering methodology for single-cell RNA sequencing (scRNA-seq) quality control (QC) metrics. The filtering must be grounded in robust statistical principles, specifically using the median and the Median Absolute Deviation (MAD), and must be robust to degenerate cases where the scale estimate collapses to zero. We will derive the filtering rules from first principles, justify all parameter choices, and apply them to the provided test cases.\n\nFirst, we establish the statistical foundation for our method. In scRNA-seq analysis, outlier cells can severely distort downstream analyses such as normalization, dimensionality reduction, and clustering. Common outlier cell types include empty droplets (very low UMI/gene counts), cellular doublets or multiplets (artificially high UMI/gene counts), and stressed or dying cells (high fraction of mitochondrial gene expression). Classical outlier detection methods based on the mean and standard deviation are themselves sensitive to the presence of outliers. Therefore, robust statistics are required.\n\nThe median is a robust measure of central tendency. For a dataset $\\{x_1, x_2, \\dots, x_n\\}$, the median $m$ is the value separating the higher half from the lower half of the data. For an even number of data points, it is the average of the two central values.\n\nThe Median Absolute Deviation (MAD) is a robust measure of statistical dispersion. It is defined as the median of the absolute deviations from the data's median:\n$$\n\\mathrm{MAD} = \\mathrm{median}(|x_i - m|)\n$$\nwhere $m = \\mathrm{median}(\\{x_i\\})$.\n\nTo use MAD in a framework analogous to the popular \"three-sigma\" rule, we must establish its relationship to the standard deviation, $\\sigma$. For a continuous random variable $X$ following a Normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$, its median is $\\mu$. The variable $Y = |X - \\mu|$ follows a folded normal distribution. The median of $Y$ is the value $y_m$ such that $P(Y \\le y_m) = 1/2$. This is equivalent to $P(|X - \\mu| \\le y_m) = 1/2$, or $P(-y_m \\le X - \\mu \\le y_m) = 1/2$. Standardizing, we get $P(-y_m/\\sigma \\le Z \\le y_m/\\sigma) = 1/2$, where $Z \\sim \\mathcal{N}(0, 1)$. This implies $P(Z \\le y_m/\\sigma) - P(Z \\le -y_m/\\sigma) = 1/2$. By symmetry of the Normal distribution, this simplifies to $2 \\cdot P(Z \\le y_m/\\sigma) - 1 = 1/2$, which yields $P(Z \\le y_m/\\sigma) = 3/4$.\nThus, $y_m/\\sigma = \\Phi^{-1}(0.75)$, where $\\Phi^{-1}$ is the quantile function (or probit function) of the standard Normal distribution. The value is $\\Phi^{-1}(0.75) \\approx 0.6745$.\nThe MAD is the estimator for $y_m$, so we have $\\mathrm{MAD} \\approx \\sigma \\cdot \\Phi^{-1}(0.75)$.\nFrom this, we derive a robust estimator for the standard deviation, $\\hat{\\sigma}_{\\mathrm{rob}}$, by scaling the MAD:\n$$\n\\hat{\\sigma}_{\\mathrm{rob}} = c \\cdot \\mathrm{MAD} = \\frac{1}{\\Phi^{-1}(0.75)} \\cdot \\mathrm{MAD} \\approx 1.4826 \\cdot \\mathrm{MAD}\n$$\nThis scaled MAD, $\\hat{\\sigma}_{\\mathrm{rob}}$, provides an estimate of the standard deviation that is consistent with $\\sigma$ under the Normal model but remains robust to extreme outliers.\n\nWe now construct the outlier detection rules. An outlier is typically defined as a data point lying several standard deviations away from the center. Using our robust estimators, a value $x_i$ is flagged if $|x_i - m| > k \\cdot \\hat{\\sigma}_{\\mathrm{rob}}$. The problem specifies a tuning parameter $k=3$. This choice is a robust analogue of the \"three-sigma rule\", which identifies values outside the central $99.7\\%$ of a Normal distribution as outliers. It provides a stringent but standard threshold for outlier detection.\n\nFor UMI counts and the number of genes detected, both unusually low and high values indicate poor quality. Low values suggest empty droplets or failed single-cell capture, while high values suggest the presence of doublets or multiplets. Therefore, we use a symmetric filtering rule. A cell is filtered if its metric value $x_i$ falls outside the interval:\n$$\n[m - k \\cdot \\hat{\\sigma}_{\\mathrm{rob}}, m + k \\cdot \\hat{\\sigma}_{\\mathrm{rob}}] = [m - 3 \\cdot c \\cdot \\mathrm{MAD}, m + 3 \\cdot c \\cdot \\mathrm{MAD}]\n$$\n\nFor the mitochondrial fraction, a high value indicates cell stress, damage, or apoptosis, where cytosolic mRNA is lost, leaving a higher proportion of mitochondrial transcripts. A low value is not a quality concern. Thus, we employ a one-sided upper threshold. A cell is filtered if its mitochondrial fraction $x_i$ exceeds:\n$$\nm + k \\cdot \\hat{\\sigma}_{\\mathrm{rob}} = m + 3 \\cdot c \\cdot \\mathrm{MAD}\n$$\n\nA critical issue arises when $\\mathrm{MAD} = 0$. This occurs if more than half of the data points are identical, which is common with discrete count data or in datasets with many tied values. If $\\mathrm{MAD}=0$, then $\\hat{\\sigma}_{\\mathrm{rob}}=0$, and the filtering interval becomes degenerate ($[m, m]$), potentially filtering all non-median values. To prevent this, we must regularize the scale estimate.\n1.  For count metrics (UMI, genes), a zero scale is uninformative. We introduce a floor for the scale estimate, $\\hat{\\sigma}_{\\mathrm{rob}}$, proportional to the median. A defensible choice for this floor is a small fraction of the median, ensuring the scale is contextually relevant. We will use $5\\%$ of the median, a value that is small enough not to dominate in non-degenerate cases but large enough to define a reasonable spread. The regularized scale is $\\max(\\hat{\\sigma}_{\\mathrm{rob}}, 0.05 \\cdot m)$. This is only applied if $\\hat{\\sigma}_{\\mathrm{rob}}$ is calculated to be $0$.\n2.  For the mitochondrial fraction, which is a proportion in $[0,1]$, we follow the specified rule. If $\\mathrm{MAD}=0$, the scale is set to the smallest positive deviation from the median, i.e., $\\min(\\{|x_i-m| : |x_i-m|>0\\})$. If all values are identical and no positive deviations exist, we use a small positive constant, for which $0.01$ is a reasonable choice given the $[0,1]$ range of the metric.\n\nThe final filtering decision for a cell is based on the union of the outlier sets from the three metrics. A cell is removed if it is flagged as an outlier for its UMI count, gene count, **or** mitochondrial fraction.\n\nThe impact of this filtering on downstream analysis is profound. By removing low-quality cells, doublets, and stressed cells, we ensure that subsequent normalization and clustering are based on a more homogeneous set of viable single cells. This prevents the formation of spurious clusters driven by technical artifacts (e.g., a \"stressed cell\" cluster or a \"doublet\" cluster) and allows for the recovery of genuine biological variation. However, overly stringent filtering (e.g., a small $k$) risks removing rare cell populations or cells at the tails of a valid biological distribution, which can lead to a biased representation of the tissue's cellular composition and altered cluster boundaries. The use of robust MAD-based thresholds with $k=3$ is a standard practice that balances the need for clean data with the preservation of biological heterogeneity.\n\nWe now proceed to the implementation and application of this method to the specified test cases. For all calculations, the constant $c = 1/\\Phi^{-1}(0.75)$ will be used, with $\\Phi^{-1}(0.75)$ provided by a high-precision numerical library.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the scRNA-seq QC filtering problem for the given test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"umi\": np.array([5200, 4800, 5100, 4950, 5050, 5300, 4700, 5000, 5150, 4850, 5080, 4920, 4995, 5010, 4975, 16000, 600, 5400, 4600, 5005]),\n            \"genes\": np.array([1650, 1580, 1620, 1590, 1610, 1700, 1550, 1600, 1630, 1570, 1615, 1595, 1605, 1608, 1592, 3500, 400, 1680, 1520, 1602]),\n            \"mito\": np.array([0.07, 0.09, 0.08, 0.085, 0.075, 0.06, 0.1, 0.08, 0.07, 0.09, 0.082, 0.078, 0.08, 0.081, 0.079, 0.04, 0.25, 0.065, 0.11, 0.08])\n        },\n        {\n            \"umi\": np.array([5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 2000, 8000, 5000]),\n            \"genes\": np.array([1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 900, 2800, 1600]),\n            \"mito\": np.array([0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.2, 0.06, 0.08])\n        },\n        {\n            \"umi\": np.array([5100, 4950, 5300, 4700, 5500, 4800, 6200, 3000, 5800, 4900, 5050, 5150, 4000, 7000, 5200]),\n            \"genes\": np.array([1620, 1580, 1700, 1500, 1750, 1550, 1800, 1100, 1720, 1590, 1610, 1630, 1200, 1900, 1600]),\n            \"mito\": np.array([0.09, 0.07, 0.06, 0.12, 0.05, 0.08, 0.04, 0.18, 0.06, 0.09, 0.08, 0.07, 0.16, 0.03, 0.085])\n        }\n    ]\n\n    results = []\n    \n    # Tuning parameters and constants\n    k = 3\n    # Consistency constant c = 1 / Φ⁻¹(0.75)\n    c = 1 / norm.ppf(0.75)\n\n    def find_outliers(data, is_symmetric, is_proportion, k_val, c_val):\n        \"\"\"\n        Identifies outliers based on the median and MAD.\n        \n        Args:\n            data (np.array): The input data vector.\n            is_symmetric (bool): True for two-sided filtering, False for one-sided (upper).\n            is_proportion (bool): True if the data is a proportion [0,1], for degeneracy handling.\n            k_val (float): The number of scaled MADs to use for the threshold.\n            c_val (float): The consistency constant for scaling MAD.\n\n        Returns:\n            set: A set of indices corresponding to outlier cells.\n        \"\"\"\n        median = np.median(data)\n        deviations = np.abs(data - median)\n        mad = np.median(deviations)\n        \n        scaled_mad = c_val * mad\n        \n        # Handle degeneracy (MAD = 0)\n        if scaled_mad == 0:\n            if is_proportion:\n                positive_devs = deviations[deviations > 0]\n                if len(positive_devs) > 0:\n                    # Regularize scale with the minimum positive deviation\n                    scaled_mad = np.min(positive_devs)\n                else:\n                    # All values are identical, use a small constant floor\n                    scaled_mad = 0.01 \n            else: # Count data\n                # Regularize scale with a fraction of the median, if median is non-zero\n                if median > 0:\n                    scaled_mad = max(scaled_mad, 0.05 * median)\n                # If median is also 0, a minimal absolute scale might be needed, but\n                # this case is unlikely for UMI/gene counts in a real pre-filtered dataset.\n                # Here, a scale of 0 would lead to no non-zero values being accepted.\n\n        # Define thresholds\n        upper_bound = median + k_val * scaled_mad\n        \n        if is_symmetric:\n            lower_bound = median - k_val * scaled_mad\n            outlier_indices = np.where((data  lower_bound) | (data > upper_bound))[0]\n        else: # One-sided upper threshold\n            outlier_indices = np.where(data > upper_bound)[0]\n            \n        return set(outlier_indices.tolist())\n\n    for case in test_cases:\n        umi_data = case[\"umi\"]\n        genes_data = case[\"genes\"]\n        mito_data = case[\"mito\"]\n        \n        umi_outliers = find_outliers(umi_data, is_symmetric=True, is_proportion=False, k_val=k, c_val=c)\n        genes_outliers = find_outliers(genes_data, is_symmetric=True, is_proportion=False, k_val=k, c_val=c)\n        mito_outliers = find_outliers(mito_data, is_symmetric=False, is_proportion=True, k_val=k, c_val=c)\n        \n        # A cell is filtered if it violates any of the three metric thresholds\n        total_outliers = sorted(list(umi_outliers | genes_outliers | mito_outliers))\n        results.append(total_outliers)\n\n    # Format the final output string exactly as specified\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\nsolve()\n```", "id": "4382174"}, {"introduction": "With a clean dataset in hand, the next challenge is to navigate its high-dimensional nature to find meaningful biological structure. This exercise employs Principal Component Analysis (PCA) to distill the complex gene expression matrix into its most significant axes of variation [@problem_id:4382117]. You will not only apply PCA but also learn a powerful permutation-based method to determine how many of these components represent genuine biological signal versus random noise, a critical step for accurate downstream clustering and interpretation.", "problem": "You are given the task of designing a reproducible and self-contained computational procedure to extract structure from normalized single-cell ribonucleic acid sequencing (scRNA-seq) expression matrices using Principal Component Analysis (PCA), to decide the number of principal components to retain using a permutation-based null model, and to interpret the principal component loadings in terms of predefined biological programs. This task lies within the pipeline of precision medicine and genomic diagnostics and must be formalized using only mathematical and algorithmic primitives.\n\nThe foundational base for this problem is: the Central Dogma of Molecular Biology (deoxyribonucleic acid to ribonucleic acid to protein), the statistical definition of variance and covariance, and the well-tested procedure that PCA finds orthogonal directions of maximal variance by eigen-decomposition of the covariance matrix. These principles are accepted and relied upon for gene expression interpretation and dimensionality reduction in single-cell datasets. Definitions: Principal Component Analysis (PCA) is constructed as an orthogonal transformation that diagonalizes the covariance matrix; its component scores and loadings reflect cell-level projections and gene-level contributions, respectively. All variables in this problem are considered real-valued.\n\nYour procedure must adhere to the following steps for each test case:\n\n1. Data model and preprocessing:\n   - Let the normalized expression matrix be denoted by $X \\in \\mathbb{R}^{n \\times g}$, where $n$ is the number of cells and $g$ is the number of genes. Columns index genes, rows index cells. The inputs are simulated to be log-normalized expressions consistent with scRNA-seq pipelines.\n   - Perform column-wise standardization: for each gene $j \\in \\{1,\\dots,g\\}$, compute the sample mean $\\mu_j = \\frac{1}{n}\\sum_{i=1}^n X_{ij}$, and the sample standard deviation $\\sigma_j = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (X_{ij}-\\mu_j)^2}$. Define the standardized matrix $Z$ by $Z_{ij} = \\frac{X_{ij}-\\mu_j}{\\sigma_j}$ for all $i,j$. If any $\\sigma_j = 0$, replace it with $1$ to avoid division by zero while preserving the column.\n   - Justification: standardization equalizes gene-specific scales, which differ in scRNA-seq because of gene length, capture efficiency, and biological dynamic range, ensuring PCA captures co-variation structure rather than absolute scale.\n\n2. Principal Component Analysis:\n   - Compute the singular value decomposition of $Z$: $Z = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{n \\times r}$, $\\Sigma \\in \\mathbb{R}^{r \\times r}$ diagonal with nonnegative singular values $\\{s_1,\\dots,s_r\\}$, $V \\in \\mathbb{R}^{g \\times r}$ has orthonormal columns, and $r = \\min(n,g)$.\n   - The eigenvalues of the sample covariance $C = \\frac{1}{n-1}Z^\\top Z$ are $\\lambda_i = \\frac{s_i^2}{n-1}$ for $i \\in \\{1,\\dots,r\\}$, which quantify the variance explained by each principal component.\n   - Define cell scores as $S = U \\Sigma \\in \\mathbb{R}^{n \\times r}$ and gene loadings as $L = V \\in \\mathbb{R}^{g \\times r}$, where $L_{\\cdot,i}$ is the loading vector for principal component $i$.\n\n3. Permutation-based null distribution and component retention:\n   - Construct a null model by independently permuting the entries of each gene column across cells to break cell-to-cell correlation, preserving per-gene marginal distributions. For each permutation $b \\in \\{1,\\dots,K\\}$, let $X^{(b)}$ be the permuted matrix, standardize it to $Z^{(b)}$ using the same definition as above, compute its singular values $\\{s_i^{(b)}\\}$, and compute null eigenvalues $\\lambda_i^{(b)} = \\frac{(s_i^{(b)})^2}{n-1}$ for all $i \\in \\{1,\\dots,r\\}$.\n   - For a given quantile level $q \\in (0,1)$ (expressed as a decimal), define the null threshold for the $i$-th component as $T_i(q) = \\operatorname{Quantile}_q\\left(\\{\\lambda_i^{(b)}\\}_{b=1}^K\\right)$.\n   - Retain components in order starting from $i=1$ while $\\lambda_i  T_i(q)$; stop at the first $i$ where this inequality fails. Let the retained count be $R$ (an integer).\n\n4. Interpretation via biological programs:\n   - You are given $P$ biological programs represented as disjoint gene index sets $S_1, S_2, \\dots, S_P \\subset \\{1,\\dots,g\\}$. For each retained principal component $i \\in \\{1,\\dots,R\\}$, compute the program contribution scores $s_{i,p} = \\sum_{j \\in S_p} L_{j,i}^2$ for all $p \\in \\{1,\\dots,P\\}$. Identify the top program for component $i$ as the $p$ maximizing $s_{i,p}$. Ties must be broken by the smallest $p$.\n   - Report program identifiers as integers $(0,1,\\dots,P-1)$ corresponding to $(S_1,S_2,\\dots,S_P)$.\n\nTest suite specification:\n- Use random generation that is fully specified by seeded normal distributions to ensure deterministic outputs. The latent generative model for each test case simulates per-cell latent factors mapped linearly to subsets of genes plus independent Gaussian noise, producing matrices $X$ consistent with log-normalized scRNA-seq structure. All random numbers are generated using a modern pseudo-random number generator seeded as specified.\n\n- Global hyperparameters:\n  - Number of permutations $K = \\;200$ (integer).\n  - Quantile level $q = \\;0.95$ (decimal).\n\n- Test Case $1$ (happy path; multiple structured programs):\n  - Dimensions: $n = \\;120$, $g = \\;30$.\n  - Seed: $s = \\;314159$.\n  - Program sets ($P = \\;3$):\n    - $S_1 = \\;\\{0,1,2,3,4,5,6,7,8,9\\}$,\n    - $S_2 = \\;\\{10,11,12,13,14,15,16,17,18,19\\}$,\n    - $S_3 = \\;\\{20,21,22,23,24\\}$.\n  - Noise-only genes: $\\{25,26,27,28,29\\}$.\n  - Latent factor generation per cell $i$:\n    - Draw $z_{0,i} \\sim \\mathcal{N}(0,1)$, $z_{1,i} \\sim \\mathcal{N}(0,1)$, $z_{2,i} \\sim \\mathcal{N}(0,1)$ independently.\n  - Data generation:\n    - For $j \\in S_1$: $X_{ij} = \\;2.0 \\cdot z_{0,i} + \\epsilon_{ij}$ with $\\epsilon_{ij} \\sim \\mathcal{N}(0,0.3^2)$.\n    - For $j \\in S_2$: $X_{ij} = \\;1.8 \\cdot z_{1,i} + \\epsilon_{ij}$ with $\\epsilon_{ij} \\sim \\mathcal{N}(0,0.3^2)$.\n    - For $j \\in S_3$: $X_{ij} = \\;1.2 \\cdot z_{2,i} + \\epsilon_{ij}$ with $\\epsilon_{ij} \\sim \\mathcal{N}(0,0.3^2)$.\n    - For noise genes: $X_{ij} = \\;\\epsilon_{ij}$ with $\\epsilon_{ij} \\sim \\mathcal{N}(0,1.0^2)$.\n\n- Test Case $2$ (edge case; near-null structure):\n  - Dimensions: $n = \\;120$, $g = \\;30$.\n  - Seed: $s = \\;271828$.\n  - Program sets ($P = \\;3$) identical to Test Case $1$:\n    - $S_1 = \\;\\{0,1,2,3,4,5,6,7,8,9\\}$,\n    - $S_2 = \\;\\{10,11,12,13,14,15,16,17,18,19\\}$,\n    - $S_3 = \\;\\{20,21,22,23,24\\}$.\n  - All genes are noise:\n    - For all $j \\in \\{0,1,\\dots,29\\}$ and cells $i$: $X_{ij} = \\;\\epsilon_{ij}$ with $\\epsilon_{ij} \\sim \\mathcal{N}(0,1.0^2)$.\n\n- Test Case $3$ (boundary case; single dominant program):\n  - Dimensions: $n = \\;120$, $g = \\;30$.\n  - Seed: $s = \\;161803$.\n  - Program sets ($P = \\;3$):\n    - $S_1 = \\;\\{0,1,2,3,4,5,6,7\\}$,\n    - $S_2 = \\;\\{10,11,12,13,14,15,16\\}$,\n    - $S_3 = \\;\\{20,21,22,23,24\\}$.\n  - Noise-only genes: all other indices not in $S_1 \\cup S_2 \\cup S_3$.\n  - Latent factor generation per cell $i$:\n    - Draw $z_{0,i} \\sim \\mathcal{N}(0,1)$ independently.\n  - Data generation:\n    - For $j \\in S_1$: $X_{ij} = \\;2.5 \\cdot z_{0,i} + \\epsilon_{ij}$ with $\\epsilon_{ij} \\sim \\mathcal{N}(0,0.2^2)$.\n    - For all other genes: $X_{ij} = \\;\\epsilon_{ij}$ with $\\epsilon_{ij} \\sim \\mathcal{N}(0,1.0^2)$.\n\nProgram output specification:\n- For each test case, compute the retained principal component count $R$ (integer) and, for the retained components in order, the top program identifiers as a list of integers. If $R = 0$, the program list is empty.\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, with no spaces. Each test case result must be formatted as $[R,[p_1,p_2,\\dots,p_R]]$. For example, the final output should look like $[[2,[0,1]],[0,[]],[1,[0]]]$ although the actual numbers will be determined by computation.", "solution": "The problem statement has been meticulously validated. All givens, including mathematical definitions, procedural steps, and numerical parameters, were extracted and assessed.\n\n**Verdict:** The problem is **valid**.\n\nThe problem is scientifically grounded, employing standard and well-established techniques from computational biology and statistics, such as Principal Component Analysis (PCA), permutation testing for significance estimation, and gene set enrichment analysis for interpretation. The problem is well-posed, with all parameters, data generation models, and algorithmic steps defined unambiguously, ensuring a unique, deterministic solution for each test case given the specified random seeds. The language is objective and precise. It is free of any of the flaws listed in the validation checklist, such as scientific unsoundness, incompleteness, or ambiguity.\n\nTherefore, a complete solution is provided below.\n\n### Methodological Framework\n\nThe specified procedure constitutes a rigorous pipeline for identifying and interpreting significant axes of variation within a single-cell gene expression matrix. This is achieved through four sequential, principled steps: data standardization, dimensionality reduction via PCA, statistical significance assessment of the components, and biological interpretation of those deemed significant.\n\n#### Step 1: Data Model and Standardization\n\nThe input is a normalized gene expression matrix $X \\in \\mathbb{R}^{n \\times g}$, where $n$ represents the number of cells and $g$ represents the number of genes. The entries $X_{ij}$ quantify the expression of gene $j$ in cell $i$. To ensure that the subsequent analysis captures the correlational structure of gene expression rather than being biased by the wide dynamic range of expression values across different genes, each gene's expression vector (a column in $X$) is standardized.\n\nFor each gene $j \\in \\{1,\\dots,g\\}$, we compute its sample mean $\\mu_j$ and sample standard deviation $\\sigma_j$ across all $n$ cells:\n$$\n\\mu_j = \\frac{1}{n}\\sum_{i=1}^n X_{ij}\n$$\n$$\n\\sigma_j = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (X_{ij}-\\mu_j)^2}\n$$\nThe use of $n-1$ in the denominator for $\\sigma_j$ corresponds to Bessel's correction for an unbiased estimate of the population variance. A standardized matrix $Z$ is then constructed where each element $Z_{ij}$ is given by:\n$$\nZ_{ij} = \\frac{X_{ij}-\\mu_j}{\\sigma_j}\n$$\nThis transformation, also known as calculating Z-scores, rescales each gene's expression profile to have a mean of $0$ and a standard deviation of $1$. If a rare case occurs where a gene has zero variance (i.e., $\\sigma_j = 0$), its standard deviation is set to $1$ to prevent division by zero. This leaves the column as all zeros, as the mean-subtracted values will be zero.\n\n#### Step 2: Principal Component Analysis\n\nPCA is applied to the standardized matrix $Z$ to find the orthogonal directions of maximal variance in the high-dimensional gene space. The most computationally stable method to perform PCA is via the Singular Value Decomposition (SVD) of $Z$:\n$$\nZ = U \\Sigma V^\\top\n$$\nHere, $U \\in \\mathbb{R}^{n \\times r}$ is a matrix with orthonormal columns representing the cell-wise projections, $\\Sigma \\in \\mathbb{R}^{r \\times r}$ is a diagonal matrix of non-negative singular values $s_1 \\ge s_2 \\ge \\dots \\ge s_r \\ge 0$, and $V \\in \\mathbb{R}^{g \\times r}$ is a matrix with orthonormal columns representing the principal axes in gene space. The rank $r$ is the minimum of $n$ and $g$.\n\nThe key outputs of PCA are derived from the SVD components:\n-   **Gene Loadings ($L$):** The matrix $L = V$ contains the loading vectors for each principal component. The $i$-th column of $L$, denoted $L_{\\cdot,i}$, is the $i$-th principal axis and indicates the contribution of each gene to that component.\n-   **Cell Scores ($S$):** The matrix $S = U \\Sigma$ contains the coordinates of each cell in the new principal component space. The $i$-th column gives the projection of all cells onto the $i$-th principal component.\n-   **Explained Variance:** The variance captured by the $i$-th principal component is its associated eigenvalue, $\\lambda_i$, of the sample covariance matrix $C = \\frac{1}{n-1}Z^\\top Z$. These eigenvalues are directly related to the singular values from the SVD of $Z$ by the formula:\n    $$\n    \\lambda_i = \\frac{s_i^2}{n-1}\n    $$\n\n#### Step 3: Permutation-based Component Retention\n\nWhile PCA always finds components, not all of them necessarily represent true biological signal; many may simply capture random noise. To distinguish significant components from spurious ones, we construct a null distribution for the eigenvalues. The null hypothesis is that there is no systematic correlation structure between genes across cells.\n\nThis is achieved by permutation testing. We generate $K = 200$ null datasets. For each null dataset $b \\in \\{1, \\dots, K\\}$, we create a permuted matrix $X^{(b)}$ by independently shuffling the expression values for each gene (column) across the cells. This procedure destroys the true cell-level gene-gene correlations while preserving the marginal distribution of each gene's expression.\n\nFor each permuted matrix $X^{(b)}$, we repeat the standardization and PCA process to obtain a set of null eigenvalues $\\{\\lambda_i^{(b)}\\}_{i=1}^r$. After $K$ permutations, we have an empirical null distribution for each eigenvalue $\\lambda_i$. A significance threshold, $T_i(q)$, for each component is then defined as the upper $q$-quantile of its corresponding null distribution, where $q = 0.95$:\n$$\nT_i(q) = \\operatorname{Quantile}_q\\left(\\{\\lambda_i^{(b)}\\}_{b=1}^K\\right)\n$$\nA principal component $i$ is deemed significant if its observed eigenvalue $\\lambda_i$ exceeds this null threshold, i.e., $\\lambda_i  T_i(q)$. Components are evaluated in descending order of their variance contribution (from $i=1$ to $r$), and we retain all components until the first one that fails this test. The total number of retained components is denoted by $R$.\n\n#### Step 4: Interpretation via Biological Programs\n\nThe final step is to assign biological meaning to the $R$ significant principal components. This is done by examining their gene loading vectors. A large absolute loading value $|L_{j,i}|$ implies that gene $j$ is strongly associated with component $i$.\n\nWe are provided with $P$ predefined biological programs, each represented by a set of gene indices $S_p$. To quantify the extent to which a component $i$ represents program $p$, we calculate a program contribution score, $s_{i,p}$. This score is the sum of the squared loadings of all genes belonging to that program:\n$$\ns_{i,p} = \\sum_{j \\in S_p} L_{j,i}^2\n$$\nThe squaring of loadings ensures that the contribution is measured by the magnitude of association, irrespective of whether the gene is positively or negatively correlated with the component's score. For each retained component $i \\in \\{1,\\dots,R\\}$, we identify the program $p$ that maximizes this score, $p^* = \\arg\\max_{p} s_{i,p}$. This program is assigned as the dominant biological theme for that component. In case of a tie, the program with the smallest index $p$ is chosen.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to execute all test cases and print the final result.\n    \"\"\"\n\n    # Global hyperparameters specified in the problem\n    K = 200  # Number of permutations\n    Q = 0.95  # Quantile level\n\n    # Test Case 1 Specifications\n    case1 = {\n        'n': 120, 'g': 30, 'seed': 314159,\n        'programs': [\n            set(range(0, 10)),\n            set(range(10, 20)),\n            set(range(20, 25))\n        ],\n        'gen_rules': {\n            'latent_factors': 3,\n            'noise_std_global': 1.0,\n            'program_rules': [\n                {'indices': set(range(0, 10)), 'factor_idx': 0, 'weight': 2.0, 'noise_std': 0.3},\n                {'indices': set(range(10, 20)), 'factor_idx': 1, 'weight': 1.8, 'noise_std': 0.3},\n                {'indices': set(range(20, 25)), 'factor_idx': 2, 'weight': 1.2, 'noise_std': 0.3},\n            ]\n        }\n    }\n\n    # Test Case 2 Specifications\n    case2 = {\n        'n': 120, 'g': 30, 'seed': 271828,\n        'programs': [\n            set(range(0, 10)),\n            set(range(10, 20)),\n            set(range(20, 25))\n        ],\n        'gen_rules': {\n            'latent_factors': 0,\n            'noise_std_global': 1.0,\n            'program_rules': []\n        }\n    }\n\n    # Test Case 3 Specifications\n    case3 = {\n        'n': 120, 'g': 30, 'seed': 161803,\n        'programs': [\n            set(range(0, 8)),\n            set(range(10, 17)),\n            set(range(20, 25))\n        ],\n        'gen_rules': {\n            'latent_factors': 1,\n            'noise_std_global': 1.0,\n            'program_rules': [\n                {'indices': set(range(0, 8)), 'factor_idx': 0, 'weight': 2.5, 'noise_std': 0.2},\n            ]\n        }\n    }\n    \n    test_cases = [case1, case2, case3]\n    results = []\n\n    for case in test_cases:\n        X = generate_data(case['n'], case['g'], case['seed'], case['gen_rules'])\n        R, top_programs = perform_pca_pipeline(X, K, Q, case['programs'])\n        results.append(f\"[{R},[{','.join(map(str, top_programs))}]]\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef generate_data(n, g, seed, gen_rules):\n    \"\"\"\n    Generates the expression matrix X based on the specified generative model.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Initialize with global noise\n    noise_std_global = gen_rules['noise_std_global']\n    X = rng.normal(loc=0, scale=noise_std_global, size=(n, g))\n\n    # Generate latent factors\n    num_latent = gen_rules['latent_factors']\n    if num_latent > 0:\n        latent_factors = rng.normal(loc=0, scale=1, size=(n, num_latent))\n\n    # Add program-specific signals\n    for rule in gen_rules['program_rules']:\n        indices = list(rule['indices'])\n        factor = latent_factors[:, rule['factor_idx']]\n        weight = rule['weight']\n        noise_std = rule['noise_std']\n        \n        # Override global noise with specific noise for these genes\n        noise = rng.normal(loc=0, scale=noise_std, size=(n, len(indices)))\n        \n        signal = np.outer(factor, np.ones(len(indices))) * weight\n        X[:, indices] = signal + noise\n        \n    return X\n\n\ndef perform_pca_pipeline(X, K, q, programs):\n    \"\"\"\n    Executes the full analysis pipeline: standardization, PCA, permutation test, and interpretation.\n    \"\"\"\n    n, g = X.shape\n    r = min(n, g)\n\n    # Step 1: Standardization\n    def standardize(mat):\n        mean = mat.mean(axis=0)\n        std = mat.std(axis=0, ddof=1)\n        std[std == 0] = 1.0\n        return (mat - mean) / std\n\n    Z = standardize(X)\n    \n    # Step 2: PCA on real data\n    _, s, Vt = np.linalg.svd(Z, full_matrices=False)\n    lambdas = (s**2) / (n - 1)\n    L = Vt.T  # Gene loadings matrix V\n\n    # Step 3: Permutation test\n    null_lambdas = np.zeros((K, r))\n    rng = np.random.default_rng(sum(X.shape) + X.size) # A deterministic seed based on data\n\n    for b in range(K):\n        X_perm = np.zeros_like(X)\n        for j in range(g):\n            X_perm[:, j] = rng.permutation(X[:, j])\n        \n        Z_perm = standardize(X_perm)\n        _, s_perm, _ = np.linalg.svd(Z_perm, full_matrices=False)\n        null_lambdas[b, :] = (s_perm**2) / (n - 1)\n\n    # Component retention\n    thresholds = np.quantile(null_lambdas, q, axis=0)\n    R = 0\n    for i in range(r):\n        if lambdas[i] > thresholds[i]:\n            R += 1\n        else:\n            break\n            \n    # Step 4: Interpretation\n    if R == 0:\n        return 0, []\n\n    top_programs = []\n    for i in range(R):\n        loadings_i = L[:, i]\n        \n        program_scores = []\n        for p_set in programs:\n            p_indices = list(p_set)\n            score = np.sum(loadings_i[p_indices]**2)\n            program_scores.append(score)\n            \n        # argmax breaks ties by smallest index, as required.\n        top_p = np.argmax(program_scores)\n        top_programs.append(top_p)\n        \n    return R, top_programs\n\nif __name__ == '__main__':\n    solve()\n```", "id": "4382117"}, {"introduction": "Single-cell analysis can move beyond static snapshots to reveal the dynamic processes governing cell fate. This advanced practice delves into RNA velocity, a technique that predicts the future state of individual cells by modeling the kinetics of mRNA splicing [@problem_id:4382250]. By estimating the rates of transcription, splicing, and degradation from unspliced and spliced counts, you will infer a velocity vector for each cell and project it onto a low-dimensional embedding to visualize cellular transition paths.", "problem": "You are tasked with designing and implementing a complete, runnable program that constructs a simplified single-cell ribonucleic acid (RNA) velocity analysis pipeline to support precision medicine and genomic diagnostics. The goal is to start from fundamental biochemical kinetics for transcription and splicing, estimate gene-wise kinetic parameters from spliced and unspliced counts over cells ordered in time, infer instantaneous velocities in expression space, and locally project those velocities onto a two-dimensional Uniform Manifold Approximation and Projection (UMAP) embedding to assess the directionality of cellular transitions.\n\nFoundational base and definitions:\n- The Central Dogma of molecular biology states that deoxyribonucleic acid (DNA) is transcribed to messenger ribonucleic acid (mRNA), which is translated to protein. For intron-containing genes, pre-mRNA (unspliced) is spliced to mature mRNA (spliced).\n- Under mass-action kinetics at the single-cell level for each gene, a standard linear dynamical system models unspliced and spliced counts as functions of time. Denote unspliced counts by $u(t)$ and spliced counts by $s(t)$. Transcription at rate $\\alpha$, splicing at rate $\\beta$, and degradation at rate $\\gamma$ yield the system\n$$\n\\frac{\\mathrm{d}u}{\\mathrm{d}t} = \\alpha - \\beta u,\\qquad\n\\frac{\\mathrm{d}s}{\\mathrm{d}t} = \\beta u - \\gamma s.\n$$\nAll parameters $\\alpha,\\beta,\\gamma$ are non-negative. This model is widely used to interpret single-cell RNA sequencing kinetic dynamics and underlies RNA velocity inference.\n- You may assume that small finite-difference approximations of $\\frac{\\mathrm{d}u}{\\mathrm{d}t}$ and $\\frac{\\mathrm{d}s}{\\mathrm{d}t}$ from time-ordered cells are justified when the sampling is sufficiently dense in time.\n- Local linearization of a nonlinear embedding is a well-tested practice: around each cell, approximate the embedding as a locally linear map from spliced expression space to $\\mathbb{R}^2$ by fitting a local Jacobian via least squares on neighbor differences.\n\nProgram requirements:\nStarting from these principles, implement the following pipeline purely in mathematical and algorithmic terms.\n\n1) Compute spliced and unspliced count matrices by aggregating provided read tuples:\n- You will be given a test suite where each test case specifies:\n  - A set of $N$ cells (indexed $0$ to $N-1$).\n  - A set of $G$ genes (indexed $0$ to $G-1$).\n  - A list of aggregated read tuples of the form $(c, g, T, k)$ where $c$ is the cell index, $g$ is the gene index, $T \\in \\{\\text{'S'}, \\text{'U'}\\}$ indicates spliced or unspliced, and $k$ is a non-negative integer count. From these, construct two matrices $U \\in \\mathbb{N}^{N \\times G}$ and $S \\in \\mathbb{N}^{N \\times G}$ containing unspliced and spliced counts, respectively, by summing counts per $(c,g)$.\n\n2) Estimate gene-wise kinetic parameters using ordinary least squares derived from the dynamical model:\n- For each gene $g$, and for each cell $i$ with time $t_i$ in hours, approximate derivatives $\\left.\\frac{\\mathrm{d}u}{\\mathrm{d}t}\\right\\vert_{t_i}$ and $\\left.\\frac{\\mathrm{d}s}{\\mathrm{d}t}\\right\\vert_{t_i}$ using finite differences on the time series $\\{(t_i, u_i), (t_i, s_i)\\}$, where $u_i = U[i,g]$ and $s_i = S[i,g]$. Use central differences for interior points and one-sided differences for the endpoints. Time must be treated explicitly in hours.\n- Fit the linear relation $\\frac{\\mathrm{d}u}{\\mathrm{d}t} = \\alpha - \\beta u$ by ordinary least squares to estimate $\\alpha$ and $\\beta$. Enforce non-negativity by truncation to $\\max(\\cdot,0)$ where appropriate.\n- Given the previously estimated $\\beta$, fit $\\frac{\\mathrm{d}s}{\\mathrm{d}t} = \\beta u - \\gamma s$ by ordinary least squares to estimate $\\gamma$, again enforcing non-negativity.\n- The instantaneous spliced velocity for gene $g$ in cell $i$ is then\n$$\nv_{i,g} = \\beta_g u_{i,g} - \\gamma_g s_{i,g}.\n$$\n\n3) Project gene-space velocity vectors to the two-dimensional embedding:\n- For each test case, you are given an $N \\times 2$ matrix $X$ representing the UMAP coordinates of cells.\n- For each cell $i$, find its $k$ nearest neighbors in spliced-expression space using Euclidean distance, with $k = \\min(3, N-1)$.\n- For these neighbors $j$ of $i$, compute differences $\\Delta S_{j} = S[j,\\cdot] - S[i,\\cdot]$ and $\\Delta X_{j} = X[j,\\cdot] - X[i,\\cdot]$. Fit a local linear map $B_i \\in \\mathbb{R}^{G \\times 2}$ that minimizes the Frobenius norm\n$$\n\\lVert \\Delta S B_i - \\Delta X \\rVert_F^2,\n$$\nvia least squares. Project the gene-velocity vector $v_i \\in \\mathbb{R}^{G}$ to the embedding by $w_i = v_i^\\top B_i \\in \\mathbb{R}^2$.\n\n4) Assess directionality of transitions:\n- For each cell $i$, compute a local temporal gradient $g_i = \\frac{\\mathrm{d}X}{\\mathrm{d}t}\\big\\vert_{t_i}$ on the embedding using the same finite-difference scheme (central for interior points and one-sided at endpoints) applied to $X$ with respect to time $t$.\n- Compute the cosine similarity\n$$\n\\cos\\theta_i = \\frac{w_i \\cdot g_i}{\\lVert w_i \\rVert_2\\,\\lVert g_i \\rVert_2},\n$$\nwith the convention that if either norm is zero, set $\\cos\\theta_i = 0$.\n- Report, for each test case, the mean cosine similarity $\\overline{c} = \\frac{1}{N}\\sum_{i=1}^N \\cos\\theta_i$ as a floating-point number.\n\nTest suite:\nImplement your program to run the following three test cases internally, with all arrays and tuples specified exactly as below. Times are in hours.\n\n- Test case A:\n  - Cells $N = 6$, genes $G = 3$.\n  - Times $t = [0, 1, 2, 3, 4, 5]$ in hours.\n  - Embedding $X$ rows correspond to cells $0\\dots 5$:\n    $$\n    X = \\begin{bmatrix}\n    0.0  0.0 \\\\\n    1.0  0.1 \\\\\n    2.0  0.0 \\\\\n    3.0  -0.1 \\\\\n    4.0  -0.1 \\\\\n    5.0  0.0\n    \\end{bmatrix}.\n    $$\n  - Aggregated read tuples $(c,g,T,k)$ with $T \\in \\{\\text{'U'},\\text{'S'}\\}$:\n    - Gene $0$ across cells $0\\dots 5$: unspliced counts $[0, 2, 4, 6, 7, 8]$, spliced counts $[0, 1, 3, 6, 9, 12]$.\n    - Gene $1$ across cells $0\\dots 5$: unspliced counts $[4, 4, 4, 4, 4, 4]$, spliced counts $[8, 8, 8, 8, 8, 8]$.\n    - Gene $2$ across cells $0\\dots 5$: unspliced counts $[6, 5, 4, 3, 2, 1]$, spliced counts $[12, 10, 8, 6, 5, 4]$.\n\n- Test case B:\n  - Cells $N = 5$, genes $G = 2$.\n  - Times $t = [0.0, 0.5, 1.5, 3.0, 5.0]$ in hours.\n  - Embedding $X$:\n    $$\n    X = \\begin{bmatrix}\n    0.0  0.0 \\\\\n    0.7  0.8 \\\\\n    1.8  1.9 \\\\\n    3.2  3.1 \\\\\n    5.0  5.1\n    \\end{bmatrix}.\n    $$\n  - Aggregated read tuples:\n    - Gene $0$: unspliced counts $[0, 1, 2, 3, 3]$, spliced counts $[0, 1, 2, 3, 4]$.\n    - Gene $1$: unspliced counts $[3, 3, 3, 2, 2]$, spliced counts $[6, 6, 6, 5, 5]$.\n\n- Test case C:\n  - Cells $N = 6$, genes $G = 2$.\n  - Times $t = [0, 1, 2, 3, 4, 5]$ in hours.\n  - Embedding $X$:\n    $$\n    X = \\begin{bmatrix}\n    5.0  0.0 \\\\\n    4.0  0.0 \\\\\n    3.0  0.0 \\\\\n    2.0  0.0 \\\\\n    1.0  0.0 \\\\\n    0.0  0.0\n    \\end{bmatrix}.\n    $$\n  - Aggregated read tuples:\n    - Gene $0$: unspliced counts $[1, 2, 3, 4, 5, 6]$, spliced counts $[1, 2, 3, 5, 7, 9]$.\n    - Gene $1$: unspliced counts $[5, 4, 3, 2, 1, 1]$, spliced counts $[10, 8, 7, 6, 5, 5]$.\n\nImplementation notes:\n- Your program must implement the full pipeline described above and apply it to the three test cases.\n- For neighbor selection in the projection step, use Euclidean distance in spliced-expression space $\\mathbb{R}^G$ and choose $k = \\min(3, N-1)$ neighbors per cell.\n- Handle numerical stability by using least-squares solvers and define any cosine similarity with a zero denominator as zero.\n- No external input is allowed; all data must be embedded as constants taken from the test suite above.\n\nFinal output format:\n- Your program should produce a single line of output containing a comma-separated list of the mean cosine similarities for Test cases A, B, and C, in that order, each rounded to exactly four decimal places and enclosed in square brackets, for example $[0.8123,0.7450,-0.1122]$.", "solution": "The problem requires the implementation of a simplified RNA velocity analysis pipeline. The pipeline starts with a fundamental kinetic model of gene expression, estimates model parameters from single-cell RNA sequencing count data, infers expression velocities, and validates these velocities against a known temporal progression in a low-dimensional cell embedding. The solution is thus structured as a multi-step computational workflow, adhering to the principles outlined in the problem statement.\n\nThe foundation of the analysis is a linear dynamical system modeling the abundance of unspliced pre-mRNA, $u(t)$, and spliced mRNA, $s(t)$, for a given gene over time $t$. The system is described by the coupled ordinary differential equations:\n$$\n\\frac{\\mathrm{d}u}{\\mathrm{d}t} = \\alpha - \\beta u(t)\n$$\n$$\n\\frac{\\mathrm{d}s}{\\mathrm{d}t} = \\beta u(t) - \\gamma s(t)\n$$\nHere, $\\alpha$ is the rate of transcription, $\\beta$ is the rate of splicing, and $\\gamma$ is the rate of degradation of spliced mRNA. All parameters are non-negative.\n\nThe pipeline comprises the following main stages:\n\n**1. Kinetic Parameter Estimation**\n\nFor each gene, the kinetic parameters $\\alpha$, $\\beta$, and $\\gamma$ are estimated from the time-series data of unspliced ($U$) and spliced ($S$) counts across all cells. The cells are ordered by a given time variable $t$.\n\nFirst, the continuous derivatives $\\frac{\\mathrm{d}u}{\\mathrm{d}t}$ and $\\frac{\\mathrm{d}s}{\\mathrm{d}t}$ are approximated from the discrete time-series data using a finite difference scheme. For a data series $y(t_i)$, the derivative at time $t_i$ is estimated using a central difference for interior points ($0  i  N-1$) and one-sided (forward/backward) differences for the endpoints ($i=0$ and $i=N-1$):\n- Forward difference ($i=0$): $\\frac{\\mathrm{d}y}{\\mathrm{d}t}\\big\\vert_{t_0} \\approx \\frac{y(t_1) - y(t_0)}{t_1 - t_0}$\n- Central difference ($0  i  N-1$): $\\frac{\\mathrm{d}y}{\\mathrm{d}t}\\big\\vert_{t_i} \\approx \\frac{y(t_{i+1}) - y(t_{i-1})}{t_{i+1} - t_{i-1}}$\n- Backward difference ($i=N-1$): $\\frac{\\mathrm{d}y}{\\mathrm{d}t}\\big\\vert_{t_{N-1}} \\approx \\frac{y(t_{N-1}) - y(t_{N-2})}{t_{N-1} - t_{N-2}}$\n\nWith the derivatives approximated, the parameters are estimated via ordinary least squares (OLS). For each gene $g$:\n- To find $\\alpha_g$ and $\\beta_g$, the equation $\\frac{\\mathrm{d}u_g}{\\mathrm{d}t} = \\alpha_g - \\beta_g u_g$ is treated as a linear regression problem. We solve for $(\\alpha_g, \\beta_g)$ that best fit the data pairs $\\{(\\frac{\\mathrm{d}u_g}{\\mathrm{d}t})_i, u_{g,i}\\}$ for all cells $i=0, \\dots, N-1$. The resulting estimates are truncated at $0$ to enforce non-negativity.\n- With the estimated $\\beta_g$, the parameter $\\gamma_g$ is found by fitting the second equation, rearranged as $\\frac{\\mathrm{d}s_g}{\\mathrm{d}t} - \\beta_g u_g = -\\gamma_g s_g$. This is a linear regression through the origin for $\\gamma_g$, using the data $\\{(\\frac{\\mathrm{d}s_g}{\\mathrm{d}t})_i, u_{g,i}, s_{g,i}\\}$. Again, the estimate for $\\gamma_g$ is enforced to be non-negative.\n\nOnce the gene-specific parameters $(\\alpha_g, \\beta_g, \\gamma_g)$ are determined, the instantaneous RNA velocity for gene $g$ in cell $i$ is calculated as the rate of change of spliced mRNA, given by:\n$$\nv_{i,g} = \\beta_g u_{i,g} - \\gamma_g s_{i,g}\n$$\nThis results in a velocity vector $v_i \\in \\mathbb{R}^G$ for each cell $i$.\n\n**2. Velocity Projection onto Embedding**\n\nThe inferred velocities exist in the high-dimensional gene expression space ($\\mathbb{R}^G$). To visualize and interpret them in the context of the low-dimensional cell state manifold (given as a $2$D UMAP embedding, $X$), they must be projected. This is achieved by assuming the UMAP embedding is locally a linear transformation of the gene expression space.\n\nFor each cell $i$, we find its $k$ nearest neighbors in the spliced expression space ($\\mathbb{R}^G$) using Euclidean distance, with $k = \\min(3, N-1)$. Let these neighbors be indexed by $j \\in \\mathcal{N}_i$. We then form matrices of differences between the cell and its neighbors in both spliced expression space ($\\Delta S$, a $k \\times G$ matrix) and UMAP space ($\\Delta X$, a $k \\times 2$ matrix). The rows of these matrices are $\\Delta S_j = S[j,:] - S[i,:]$ and $\\Delta X_j = X[j,:] - X[i,:]$.\n\nWe seek a linear map (a transition matrix) $B_i \\in \\mathbb{R}^{G \\times 2}$ that best explains the relationship between these differences, by solving the least-squares problem:\n$$\n\\min_{B_i} \\lVert \\Delta S B_i - \\Delta X \\rVert_F^2\n$$\nThe solution is given by $B_i = (\\Delta S^\\dagger) \\Delta X$, where $\\Delta S^\\dagger$ is the Moore-Penrose pseudoinverse of $\\Delta S$. This approach is robust even when $\\Delta S$ is not full rank.\n\nThe high-dimensional velocity vector $v_i$ for cell $i$ is then projected into the $2$D embedding space by applying this learned local transformation:\n$$\nw_i = v_i^\\top B_i\n$$\nThis yields a $2$D velocity vector $w_i \\in \\mathbb{R}^2$ for each cell, representing the direction and speed of cellular state change within the UMAP plot.\n\n**3. Directionality Validation**\n\nThe final step is to assess whether the inferred velocity directions are consistent with the known temporal progression of the cells. A \"ground truth\" direction vector is computed for each cell in the UMAP space by taking the derivative of the embedding coordinates with respect to time $t$. This temporal gradient, $g_i = \\frac{\\mathrm{d}X}{\\mathrm{d}t}\\big\\vert_{t_i}$, is calculated using the same finite difference scheme as before.\n\nThe alignment between the projected velocity $w_i$ and the temporal gradient $g_i$ is quantified by their cosine similarity:\n$$\n\\cos\\theta_i = \\frac{w_i \\cdot g_i}{\\lVert w_i \\rVert_2 \\lVert g_i \\rVert_2}\n$$\nA value close to $1$ indicates that the velocity points along the direction of temporal progression, a value close to $-1$ indicates it points opposite, and a value near $0$ indicates orthogonality. By convention, if either vector has a zero norm, the cosine similarity is defined as $0$.\n\nThe overall performance of the velocity model for a given dataset is then summarized by the mean cosine similarity, $\\overline{c} = \\frac{1}{N}\\sum_{i=0}^{N-1} \\cos\\theta_i$, averaged over all $N$ cells.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the RNA velocity pipeline on all test cases.\n    \"\"\"\n\n    def compute_derivatives(y_data, t_data):\n        \"\"\"\n        Computes derivatives of a time series using finite differences.\n        y_data can be a 1D array or a 2D array (for multiple dimensions).\n        \"\"\"\n        if y_data.ndim == 1:\n            y_data = y_data.reshape(-1, 1)\n\n        N = y_data.shape[0]\n        dy_dt = np.zeros_like(y_data, dtype=float)\n\n        # Forward difference for the first point\n        dy_dt[0, :] = (y_data[1, :] - y_data[0, :]) / (t_data[1] - t_data[0])\n\n        # Central differences for interior points\n        for i in range(1, N - 1):\n            dy_dt[i, :] = (y_data[i + 1, :] - y_data[i - 1, :]) / (t_data[i + 1] - t_data[i - 1])\n\n        # Backward difference for the last point\n        dy_dt[N - 1, :] = (y_data[N - 1, :] - y_data[N - 2, :]) / (t_data[N - 1] - t_data[N - 2])\n        \n        if dy_dt.shape[1] == 1:\n            return dy_dt.flatten() # Return as 1D if input was 1D\n        return dy_dt\n\n    def process_case(N, G, t, U, S, X):\n        \"\"\"\n        Processes a single test case for the RNA velocity pipeline.\n        \"\"\"\n        # --- 1. Estimate gene-wise kinetic parameters ---\n        alpha_params = np.zeros(G)\n        beta_params = np.zeros(G)\n        gamma_params = np.zeros(G)\n\n        for g in range(G):\n            u_g = U[:, g]\n            s_g = S[:, g]\n\n            du_dt = compute_derivatives(u_g, t)\n            ds_dt = compute_derivatives(s_g, t)\n\n            # Fit alpha and beta: du/dt = alpha - beta * u\n            design_matrix_alpha_beta = np.vstack([np.ones(N), -u_g]).T\n            params_ab, _, _, _ = np.linalg.lstsq(design_matrix_alpha_beta, du_dt, rcond=None)\n            alpha_g = max(0, params_ab[0])\n            beta_g = max(0, params_ab[1])\n            \n            alpha_params[g] = alpha_g\n            beta_params[g] = beta_g\n\n            # Fit gamma: ds/dt - beta * u = -gamma * s\n            target_gamma = ds_dt - beta_g * u_g\n            design_matrix_gamma = -s_g.reshape(-1, 1)\n            \n            if np.sum(design_matrix_gamma**2) > 1e-9:\n                params_g, _, _, _ = np.linalg.lstsq(design_matrix_gamma, target_gamma, rcond=None)\n                gamma_g = max(0, params_g[0])\n            else:\n                gamma_g = 0.0\n            \n            gamma_params[g] = gamma_g\n\n        # --- 2. Compute instantaneous velocity matrix V ---\n        V = beta_params * U - gamma_params * S\n\n        # --- 3. Project velocities to embedding ---\n        k = min(3, N - 1)\n        w_vectors = np.zeros((N, 2))\n\n        for i in range(N):\n            # Find k nearest neighbors in spliced expression space\n            dists = np.linalg.norm(S - S[i, :], axis=1)\n            neighbor_indices = np.argsort(dists)[1:k+1] # Exclude self (dist=0)\n\n            delta_S = S[neighbor_indices] - S[i, :]\n            delta_X = X[neighbor_indices] - X[i, :]\n\n            # Solve for the local linear map B_i\n            B_i, _, _, _ = np.linalg.lstsq(delta_S, delta_X, rcond=None)\n\n            # Project velocity vector\n            v_i = V[i, :]\n            w_i = v_i @ B_i\n            w_vectors[i, :] = w_i\n            \n        # --- 4. Assess directionality ---\n        # Compute temporal gradient of the embedding\n        g_vectors = compute_derivatives(X, t)\n\n        # Compute cosine similarities\n        cos_sims = np.zeros(N)\n        for i in range(N):\n            w_i = w_vectors[i, :]\n            g_i = g_vectors[i, :]\n            \n            norm_w = np.linalg.norm(w_i)\n            norm_g = np.linalg.norm(g_i)\n\n            if norm_w > 1e-9 and norm_g > 1e-9:\n                cos_sims[i] = np.dot(w_i, g_i) / (norm_w * norm_g)\n            else:\n                cos_sims[i] = 0.0\n        \n        return np.mean(cos_sims)\n\n    # --- Test Suite ---\n    test_cases = []\n\n    # Test Case A\n    N_A, G_A = 6, 3\n    t_A = np.array([0, 1, 2, 3, 4, 5], dtype=float)\n    X_A = np.array([[0.0, 0.0], [1.0, 0.1], [2.0, 0.0], [3.0, -0.1], [4.0, -0.1], [5.0, 0.0]], dtype=float)\n    U_A = np.array([\n        [0, 4, 6], [2, 4, 5], [4, 4, 4], \n        [6, 4, 3], [7, 4, 2], [8, 4, 1]\n    ], dtype=float)\n    S_A = np.array([\n        [0, 8, 12], [1, 8, 10], [3, 8, 8], \n        [6, 8, 6], [9, 8, 5], [12, 8, 4]\n    ], dtype=float)\n    test_cases.append((N_A, G_A, t_A, U_A, S_A, X_A))\n\n    # Test Case B\n    N_B, G_B = 5, 2\n    t_B = np.array([0.0, 0.5, 1.5, 3.0, 5.0], dtype=float)\n    X_B = np.array([[0.0, 0.0], [0.7, 0.8], [1.8, 1.9], [3.2, 3.1], [5.0, 5.1]], dtype=float)\n    U_B = np.array([[0, 3], [1, 3], [2, 3], [3, 2], [3, 2]], dtype=float)\n    S_B = np.array([[0, 6], [1, 6], [2, 6], [3, 5], [4, 5]], dtype=float)\n    test_cases.append((N_B, G_B, t_B, U_B, S_B, X_B))\n\n    # Test Case C\n    N_C, G_C = 6, 2\n    t_C = np.array([0, 1, 2, 3, 4, 5], dtype=float)\n    X_C = np.array([[5.0, 0.0], [4.0, 0.0], [3.0, 0.0], [2.0, 0.0], [1.0, 0.0], [0.0, 0.0]], dtype=float)\n    U_C = np.array([[1, 5], [2, 4], [3, 3], [4, 2], [5, 1], [6, 1]], dtype=float)\n    S_C = np.array([[1, 10], [2, 8], [3, 7], [5, 6], [7, 5], [9, 5]], dtype=float)\n    test_cases.append((N_C, G_C, t_C, U_C, S_C, X_C))\n\n    results = []\n    for case in test_cases:\n        N, G, t, U, S, X = case\n        result = process_case(N, G, t, U, S, X)\n        results.append(f\"{result:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4382250"}]}