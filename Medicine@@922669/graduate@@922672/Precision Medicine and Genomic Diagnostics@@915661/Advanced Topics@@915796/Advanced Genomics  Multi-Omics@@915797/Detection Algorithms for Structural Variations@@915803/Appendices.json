{"hands_on_practices": [{"introduction": "One of the most fundamental signals for detecting structural variations, particularly copy number variations (CNVs), is read depth. The core idea is that the number of sequencing reads mapping to a genomic region is proportional to the copy number of that region. This exercise guides you through the statistical principles used to determine if an observed drop in read depth is a significant event or simply due to random sampling variation. By modeling read counts with a Poisson distribution and applying a normal approximation, you will calculate a z-score, a standard measure of statistical significance used in many bioinformatics algorithms [@problem_id:4332052].", "problem": "A read-depth based structural variation detector standardizes windowed counts using a normal approximation rooted in a Poisson sampling model. Assume a whole-genome sequencing experiment with uniform mappability and independent coverage across base pairs, where per-base coverage is modeled as a Poisson random variable with mean $\\lambda$ and variance $\\lambda$. Let the genome-wide mean coverage be $30\\times$, i.e., $\\lambda = 30$. Consider a window of length $100\\,\\text{kb}$, where $1\\,\\text{kb} = 10^{3}$ base pairs, and suppose the observed mean coverage in this window is reduced by $0.15$ relative to the genome-wide mean, yielding an observed mean coverage of $0.85 \\times 30$ in the window.\n\nStarting from the fundamental properties of the Poisson distribution (mean and variance equality) and the additivity of independent Poisson variables across base pairs, derive the standardized normal score (z-score) for the total coverage in this window under the null hypothesis that the window’s coverage follows the genome-wide mean. Use the central limit theorem justification for the normal approximation of the sum of independent Poisson variables when the window length is large.\n\nExpress your final z-score as a real number rounded to four significant figures. No units are required.", "solution": "The problem statement has been evaluated and is deemed valid. It is scientifically grounded in standard biostatistical models for genomic data, is well-posed, and provides all necessary information for a unique solution.\n\nThe objective is to compute the standardized normal score (z-score) for the total read coverage observed in a specified genomic window. The z-score is a measure of how many standard deviations an observation is from the mean of a distribution. It is defined as:\n$$Z = \\frac{X - \\mu}{\\sigma}$$\nwhere $X$ is the observed value, $\\mu$ is the expected value (mean) under the null hypothesis, and $\\sigma$ is the standard deviation under the null hypothesis.\n\nFirst, we must define the random variable of interest and its distribution under the null hypothesis, $H_0$. The null hypothesis states that the genomic window in question has a coverage profile identical to the genome-wide average.\n\nLet $C_i$ be the random variable representing the number of reads covering base pair $i$. According to the problem statement, $C_i$ follows a Poisson distribution with a mean and variance equal to the genome-wide mean coverage, $\\lambda$.\n$$C_i \\sim \\text{Poisson}(\\lambda)$$\nThe givens state that $\\lambda = 30$. Therefore, for each base pair $i$:\n$$E[C_i] = \\lambda = 30$$\n$$\\text{Var}(C_i) = \\lambda = 30$$\n\nThe problem considers a window of length $L = 100\\,\\text{kb}$. We convert this length to base pairs (bp):\n$$L = 100\\,\\text{kb} \\times \\frac{10^3\\,\\text{bp}}{1\\,\\text{kb}} = 100 \\times 10^3\\,\\text{bp} = 10^5\\,\\text{bp}$$\n\nThe total coverage in the window, which we denote as $T$, is the sum of the coverages at each base pair within that window. Assuming the coverage at each base pair is independent, as stated in the problem:\n$$T = \\sum_{i=1}^{L} C_i$$\n\nA fundamental property of the Poisson distribution is that the sum of independent Poisson random variables is also a Poisson random variable, with a mean equal to the sum of the individual means. Under the null hypothesis, all $C_i$ are independent and identically distributed (i.i.d.) with parameter $\\lambda$. Therefore, the total coverage $T$ follows a Poisson distribution with parameter $L\\lambda$.\n$$T \\sim \\text{Poisson}(L\\lambda)$$\n\nThe mean ($\\mu_T$) and variance ($\\sigma_T^2$) of the total coverage $T$ under the null hypothesis are:\n$$\\mu_T = E[T] = L\\lambda$$\n$$\\sigma_T^2 = \\text{Var}(T) = L\\lambda$$\nThe standard deviation ($\\sigma_T$) is the square root of the variance:\n$$\\sigma_T = \\sqrt{L\\lambda}$$\n\nWe can now substitute the given values:\n$$\\mu_T = (10^5) \\times (30) = 3 \\times 10^6$$\n$$\\sigma_T = \\sqrt{(10^5) \\times (30)} = \\sqrt{3 \\times 10^6} = \\sqrt{3} \\times 10^3$$\n\nThe problem states that we should use the central limit theorem (CLT) to justify a normal approximation. For a sufficiently large value of the Poisson parameter $L\\lambda$, the Poisson distribution $\\text{Poisson}(L\\lambda)$ can be accurately approximated by a normal distribution with the same mean and variance, $\\mathcal{N}(L\\lambda, L\\lambda)$. In our case, $L\\lambda = 3 \\times 10^6$, which is a very large number, making the normal approximation exceptionally accurate.\n\nNext, we must determine the observed total coverage in the window, which we denote $T_{\\text{obs}}$. The problem states that the observed *mean* coverage in the window is $0.85$ times the genome-wide mean of $30$.\n$$\\text{Observed mean coverage} = 0.85 \\times 30 = 25.5$$\nThe total observed coverage is this mean coverage multiplied by the window length $L$:\n$$T_{\\text{obs}} = (\\text{Observed mean coverage}) \\times L = 25.5 \\times 10^5 = 2.55 \\times 10^6$$\n\nFinally, we calculate the z-score for the total coverage. We use the formula for $Z$ with our calculated values for the observed total coverage ($T_{\\text{obs}}$), the expected total coverage under $H_0$ ($\\mu_T$), and the standard deviation of total coverage under $H_0$ ($\\sigma_T$):\n$$Z = \\frac{T_{\\text{obs}} - \\mu_T}{\\sigma_T}$$\n$$Z = \\frac{2.55 \\times 10^6 - 3 \\times 10^6}{\\sqrt{3} \\times 10^3}$$\n$$Z = \\frac{-0.45 \\times 10^6}{\\sqrt{3} \\times 10^3}$$\n$$Z = \\frac{-4.5 \\times 10^5}{\\sqrt{3} \\times 10^3}$$\n$$Z = \\frac{-450}{\\sqrt{3}}$$\n\nTo obtain the numerical value, we can rationalize the denominator or compute the value directly:\n$$Z = \\frac{-450}{\\sqrt{3}} = \\frac{-450\\sqrt{3}}{3} = -150\\sqrt{3}$$\nUsing the approximate value $\\sqrt{3} \\approx 1.7320508$:\n$$Z \\approx -150 \\times 1.7320508 \\approx -259.80762$$\nThe problem requires the answer to be rounded to four significant figures.\n$$Z \\approx -259.8$$\nThis z-score indicates that the observed total coverage in the window is approximately $259.8$ standard deviations below the expected coverage under the null hypothesis, which represents an extremely significant deviation.", "answer": "$$\\boxed{-259.8}$$", "id": "4332052"}, {"introduction": "Paired-end sequencing provides powerful information beyond simple read counts by considering the distance and orientation between two linked reads. A key signal for detecting insertions, deletions, and more complex rearrangements arises when this observed distance, or \"insert size,\" deviates significantly from what is expected. This practice focuses on the crucial first step in any paired-end analysis: establishing a statistically rigorous decision rule to identify these \"discordant\" read pairs. Starting from a Gaussian model of the insert size distribution, you will derive the precise thresholds needed to classify read pairs while controlling the false-positive rate, a fundamental skill in genomic signal processing [@problem_id:4332030].", "problem": "In paired-end sequencing for structural variation detection in precision medicine and genomic diagnostics, a read pair is said to be concordant if its observed insert size is consistent with the library insert-size distribution, and discordant otherwise. Consider a library whose concordant insert sizes are modeled by a Gaussian (normal) distribution with mean $\\mu = 350$ base pairs (bp) and standard deviation $\\sigma = 50$ bp. Assume that the mapping process yields an insert size $S$ for each read pair and that, conditional on being truly concordant, $S$ is distributed as $S \\sim \\mathcal{N}(\\mu,\\sigma^{2})$.\n\nUsing the definition of False Positive Rate (FPR) as $\\alpha = \\mathbb{P}(\\text{label discordant} \\mid \\text{truly concordant})$, derive from first principles a two-sided decision rule that labels a read pair as discordant if $S$ falls outside a symmetric acceptance region about $\\mu$ while controlling the FPR at $\\alpha = 0.01$. Your derivation should start from the properties of the normal distribution and the definition of tail probabilities.\n\nThen, specialize your general decision rule to the given parameters $\\mu = 350$ bp and $\\sigma = 50$ bp to obtain explicit numerical lower and upper thresholds for $S$. Express the final thresholds in base pairs (bp) and round your numerical results to four significant figures.", "solution": "The problem statement is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. Therefore, it is deemed valid. We proceed with the derivation.\n\nThe problem asks for a two-sided decision rule to classify a paired-end read as discordant based on its insert size, $S$. We are given that for a truly concordant read pair, the insert size follows a normal distribution, $S \\sim \\mathcal{N}(\\mu, \\sigma^2)$, with mean $\\mu = 350$ base pairs (bp) and standard deviation $\\sigma = 50$ bp. The decision rule must control the False Positive Rate (FPR), $\\alpha$, at $0.01$.\n\nThe FPR is defined as the probability of labeling a read pair as discordant when it is, in fact, truly concordant. In statistical terms, this corresponds to a Type I error. Let $H_0$ be the null hypothesis that the read pair is truly concordant.\n$$H_0: \\text{The read pair is concordant.}$$\nUnder $H_0$, the distribution of the insert size is $S \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n\nThe decision rule is to label a read as discordant if its insert size $S$ falls outside a symmetric acceptance region $[\\mu - \\delta, \\mu + \\delta]$ for some positive value $\\delta$. This means we label as discordant if $S  \\mu - \\delta$ or $S  \\mu + \\delta$. The set of values for which we label as discordant is the rejection region, $\\mathcal{R} = (-\\infty, \\mu - \\delta) \\cup (\\mu + \\delta, \\infty)$.\n\nThe FPR, $\\alpha$, is the probability of this event occurring under the null hypothesis:\n$$\\alpha = \\mathbb{P}(S \\in \\mathcal{R} \\mid H_0) = \\mathbb{P}((S  \\mu - \\delta) \\lor (S  \\mu + \\delta))$$\nSince the events $(S  \\mu - \\delta)$ and $(S  \\mu + \\delta)$ are mutually exclusive, the probability of their union is the sum of their probabilities:\n$$\\alpha = \\mathbb{P}(S  \\mu - \\delta) + \\mathbb{P}(S  \\mu + \\delta)$$\nThe normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ is symmetric about its mean $\\mu$. Therefore, the probability of observing a value at least $\\delta$ below the mean is equal to the probability of observing a value at least $\\delta$ above the mean:\n$$\\mathbb{P}(S  \\mu - \\delta) = \\mathbb{P}(S  \\mu + \\delta)$$\nSubstituting this into the expression for $\\alpha$ gives:\n$$\\alpha = 2 \\cdot \\mathbb{P}(S  \\mu + \\delta)$$\nThis implies that each tail of the rejection region must contain a probability mass of $\\alpha/2$:\n$$\\mathbb{P}(S  \\mu + \\delta) = \\frac{\\alpha}{2}$$\nTo find the value of $\\delta$, we standardize the random variable $S$. Let $Z$ be a standard normal random variable, $Z \\sim \\mathcal{N}(0, 1)$, defined as:\n$$Z = \\frac{S - \\mu}{\\sigma}$$\nWe transform the inequality $S  \\mu + \\delta$ in terms of $Z$:\n$$S - \\mu  \\delta \\implies \\frac{S - \\mu}{\\sigma}  \\frac{\\delta}{\\sigma} \\implies Z  \\frac{\\delta}{\\sigma}$$\nSo, the condition on the probability becomes:\n$$\\mathbb{P}\\left(Z  \\frac{\\delta}{\\sigma}\\right) = \\frac{\\alpha}{2}$$\nBy definition, the upper $q$-quantile of the standard normal distribution, denoted $z_q$, is the value such that $\\mathbb{P}(Z  z_q) = q$. Comparing this with our equation, we identify:\n$$\\frac{\\delta}{\\sigma} = z_{\\alpha/2}$$\nSolving for $\\delta$, we obtain:\n$$\\delta = \\sigma \\cdot z_{\\alpha/2}$$\nThis expression for $\\delta$ defines the width of the acceptance region. The general\ndecision rule is determined by the lower threshold, $L = \\mu - \\delta$, and the upper threshold, $U = \\mu + \\delta$. Substituting the expression for $\\delta$:\n$$L = \\mu - \\sigma \\cdot z_{\\alpha/2}$$\n$$U = \\mu + \\sigma \\cdot z_{\\alpha/2}$$\nA read pair is labeled as discordant if its observed insert size $S$ is less than $L$ or greater than $U$. This completes the derivation of the general rule from first principles.\n\nNow, we specialize this rule using the provided parameters: $\\mu = 350$ bp, $\\sigma = 50$ bp, and $\\alpha = 0.01$.\nWe need to find the value of the quantile $z_{\\alpha/2}$:\n$$z_{\\alpha/2} = z_{0.01/2} = z_{0.005}$$\nThis is the value for which the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(z)$, is equal to $1 - 0.005 = 0.995$. We find this value from standard statistical tables or computation:\n$$z_{0.005} \\approx 2.5758$$\nUsing this value, we can now calculate the numerical thresholds $L$ and $U$:\n$$L = 350 - 50 \\cdot z_{0.005} \\approx 350 - 50 \\cdot 2.5758 = 350 - 128.79 = 221.21 \\text{ bp}$$\n$$U = 350 + 50 \\cdot z_{0.005} \\approx 350 + 50 \\cdot 2.5758 = 350 + 128.79 = 478.79 \\text{ bp}$$\nThe problem requires the final numerical results to be rounded to four significant figures.\nThe lower threshold, $221.21$, rounded to four significant figures is $221.2$.\nThe upper threshold, $478.79$, rounded to four significant figures is $478.8$.\n\nThus, the decision rule is to label a read pair as discordant if its insert size is less than $221.2$ bp or greater than $478.8$ bp.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n221.2  478.8\n\\end{pmatrix}\n}\n$$", "id": "4332030"}, {"introduction": "Developing or deploying an SV detection algorithm is incomplete without a quantitative assessment of its performance. The final and most critical step in the pipeline is to evaluate how well the caller's predictions match a known \"truth\" set. This exercise walks you through the calculation of the most common metrics used for this purpose: precision, recall, and the $F1$ measure. Understanding how to compute these values from the basic counts of true positives, false positives, and false negatives is essential for comparing different algorithms, tuning parameters for a specific application like clinical diagnostics, and interpreting the results of any SV calling tool [@problem_id:4332017].", "problem": "A clinical whole-genome sequencing pipeline evaluates a Structural Variation (SV) caller on a tumor-normal pair to support Precision Medicine decision-making. An SV event is characterized by two genomic breakpoints and a type label (e.g., deletion, duplication, inversion). The evaluation protocol defines a predicted SV as a correct detection of a truth SV if and only if: (i) the SV types match, (ii) both predicted breakpoints are within a tolerance of $\\delta$ base pairs of the corresponding true breakpoints, and (iii) the breakpoint orientations are consistent. To avoid multiple counting of either predictions or truth events, matches are enforced to be one-to-one via maximum bipartite matching over the prediction-truth pairs that satisfy the tolerance and type constraints.\n\nFor a fixed breakpoint tolerance of $\\delta = 75$ base pairs, the gold-standard truth set contains $N_{\\text{true}} = 400$ SVs, and the caller outputs $N_{\\text{pred}} = 380$ SVs. After performing one-to-one matching under the above protocol, the number of matched pairs is $TP = 312$ (true positives). All unmatched predictions are considered false positives and all unmatched truth events are considered false negatives under this evaluation definition.\n\nUsing only the fundamental definitions of confusion counts and set-based evaluation, derive expressions for precision, recall, and the $F1$ measure in terms of $TP$, $FP$, and $FN$, and compute their values for the given counts. Express each metric as a decimal in the interval $[0,1]$, and round your final reported values to four significant figures. Present your final answer as a row vector in the order: precision, recall, $F1$.", "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in the standard practices of bioinformatics and genomics for evaluating algorithm performance. The problem is well-posed, objective, and provides a complete and consistent set of data necessary for its solution. There are no logical contradictions, scientific inaccuracies, or ill-defined terms. We may therefore proceed with a formal solution.\n\nThe problem asks for the calculation of three standard performance metrics—precision, recall, and the $F1$ measure—for a structural variation (SV) caller based on a set of evaluation counts. The fundamental quantities provided are:\n- The total number of true SVs in the gold-standard set: $N_{\\text{true}} = 400$.\n- The total number of SVs predicted by the caller: $N_{\\text{pred}} = 380$.\n- The number of true positive ($TP$) detections, established via a one-to-one matching protocol: $TP = 312$.\n\nThe problem defines false positives ($FP$) as unmatched predictions and false negatives ($FN$) as unmatched truth events. These definitions are standard in set-based evaluation protocols.\n\nFirst, we derive the expressions for the number of false positives ($FP$) and false negatives ($FN$) using the provided data.\n\nThe set of all predictions, with size $N_{\\text{pred}}$, is partitioned into two disjoint subsets: those that are correct matches to a true SV (true positives) and those that are not (false positives). Therefore, the total number of predictions is the sum of true positives and false positives:\n$$N_{\\text{pred}} = TP + FP$$\nFrom this fundamental relationship, we can express the number of false positives in terms of the given quantities:\n$$FP = N_{\\text{pred}} - TP$$\nSubstituting the given values:\n$$FP = 380 - 312 = 68$$\n\nSimilarly, the set of all true SVs, with size $N_{\\text{true}}$, is partitioned into two disjoint subsets: those that were correctly detected by the caller (true positives) and those that were missed (false negatives). This gives the relationship:\n$$N_{\\text{true}} = TP + FN$$\nFrom this, we express the number of false negatives in terms of the given quantities:\n$$FN = N_{\\text{true}} - TP$$\nSubstituting the given values:\n$$FN = 400 - 312 = 88$$\n\nNow we define and compute the required metrics.\n\n**1. Precision (Positive Predictive Value)**\nPrecision measures the accuracy of the positive predictions made by the caller. It is the fraction of predicted SVs that are actual true SVs. The expression for precision is:\n$$\\text{Precision} = \\frac{TP}{TP + FP}$$\nNote that the denominator, $TP + FP$, is the total number of predictions, $N_{\\text{pred}}$.\n$$\\text{Precision} = \\frac{TP}{N_{\\text{pred}}} = \\frac{312}{380}$$\nCalculating the decimal value:\n$$\\text{Precision} = 0.8210526...$$\nRounding to four significant figures, we get $0.8211$.\n\n**2. Recall (Sensitivity or True Positive Rate)**\nRecall measures the ability of the caller to find all the true SVs. It is the fraction of all true SVs that were correctly detected. The expression for recall is:\n$$\\text{Recall} = \\frac{TP}{TP + FN}$$\nNote that the denominator, $TP + FN$, is the total number of true events, $N_{\\text{true}}$.\n$$\\text{Recall} = \\frac{TP}{N_{\\text{true}}} = \\frac{312}{400}$$\nCalculating the decimal value:\n$$\\text{Recall} = 0.78$$\nTo express this with four significant figures, we write $0.7800$.\n\n**3. $F1$ Measure**\nThe $F1$ measure is the harmonic mean of precision and recall. It provides a single score that balances both metrics. The general formula is:\n$$F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\nA more direct expression in terms of the confusion matrix counts ($TP$, $FP$, $FN$) can be derived:\n$$F1 = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}$$\nThis form is often preferred as it avoids using intermediate rounded values for precision and recall. Substituting the values for $TP$, $FP$, and $FN$:\n$$F1 = \\frac{2 \\cdot 312}{2 \\cdot 312 + 68 + 88} = \\frac{624}{624 + 156} = \\frac{624}{780}$$\nCalculating the decimal value:\n$$F1 = 0.8$$\nTo express this with four significant figures, we write $0.8000$.\n\nThe computed values, rounded to four significant figures, are:\n- Precision: $0.8211$\n- Recall: $0.7800$\n- $F1$ Measure: $0.8000$\n\nThese values will be presented as a row vector in the specified order.", "answer": "$$\\boxed{\\begin{pmatrix} 0.8211  0.7800  0.8000 \\end{pmatrix}}$$", "id": "4332017"}]}