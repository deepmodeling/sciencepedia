{"hands_on_practices": [{"introduction": "Any de novo assembly project begins with a crucial question: how much sequencing data is enough? This practice moves beyond simple coverage goals to incorporate real-world complexities that affect experimental outcomes. By working through this problem, you will learn how to statistically model sequencing yield, accounting for practical factors like data quality filters and inherent run-to-run variability, ensuring that an experiment is designed to succeed with a specified level of confidence [@problem_id:4356388]. Mastering this type of calculation is a vital skill for planning cost-effective experiments that meet the rigorous standards of both research and clinical genomics.", "problem": "A clinical de novo human genome assembly is planned using long-read sequencing technology from either Oxford Nanopore Technologies (ONT) or Pacific Biosciences (PacBio). The target haploid coverage is $c=30$ over a human haploid genome of size $G=3.1$ gigabases. Each flowcell has a projected raw yield of $Y$ gigabases. Only a fraction of the raw yield contributes to effective assembly coverage because of library and read-quality effects: a fraction $q=0.85$ of bases pass the quality threshold used in the assembly pipeline, a fraction $m=0.98$ of bases originate from human DNA after removal of adapters and contaminants, and a fraction $r=0.95$ of bases are retained after length filtering to meet the assembler’s minimum read-length requirement. The per-flowcell effective yield is variable because of pore-to-pore and run-to-run differences; assume independent and identically distributed per-flowcell effective yields with coefficient of variation $\\gamma=0.25$ (defined as standard deviation divided by mean). For clinical assurance, the total effective yield across $n$ flowcells must meet or exceed the target coverage with at least $0.95$ one-sided probability.\n\nStarting only from the coverage definition that expected coverage equals total sequenced bases divided by genome size, and standard probability limit theorems appropriate for sums of independent contributions, derive a closed-form expression for the minimal integer $n$ as a function of $Y$ that guarantees, with one-sided probability at least $0.95$, that the effective total yield meets or exceeds the $c$-fold target over $G$. Express the final answer as a single analytic expression in terms of $Y$ only, using the ceiling function to represent the minimal integer number of flowcells. Do not include units in your final expression.", "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, and contains sufficient information to derive a unique solution. The scenario is a standard application of statistical principles to a real-world problem in genomics.\n\nThe objective is to find the minimum integer number of flowcells, $n$, required to achieve a target haploid coverage, $c$, over a genome of size $G$, with a specified one-sided probability.\n\nFirst, we define the parameters and variables provided in the problem statement.\n- Target haploid coverage: $c = 30$.\n- Haploid genome size: $G = 3.1$ gigabases.\n- Projected raw yield per flowcell: $Y$ gigabases.\n- Fraction of bases passing quality threshold: $q = 0.85$.\n- Fraction of bases from human DNA (non-contaminant): $m = 0.98$.\n- Fraction of bases retained after length filtering: $r = 0.95$.\n- Coefficient of variation of per-flowcell effective yield: $\\gamma = 0.25$.\n- The number of flowcells, an integer to be determined: $n$.\n- The required one-sided probability: $P \\ge 0.95$.\n\nThe total number of effective bases required to achieve a coverage of $c$ on a genome of size $G$ is given by the product $c \\times G$. Let us denote this target yield as $B_{target}$.\n$$B_{target} = cG$$\nThe raw yield $Y$ from each flowcell is reduced by several factors. The fraction of the raw yield that contributes to the effective assembly coverage, which we denote as $f$, is the product of the given fractions:\n$$f = q \\times m \\times r = 0.85 \\times 0.98 \\times 0.95 = 0.79135$$\nLet $y_i$ be the random variable representing the effective yield from flowcell $i$. The problem states that the projected raw yield is $Y$, so we take the mean effective yield per flowcell, $\\mu_y$, to be:\n$$\\mu_y = Y \\times f = 0.79135 Y$$\nThe problem specifies that the per-flowcell effective yields are independent and identically distributed (i.i.d.) with a coefficient of variation $\\gamma = 0.25$. The coefficient of variation is defined as the ratio of the standard deviation to the mean. Therefore, the standard deviation of the effective yield per flowcell, $\\sigma_y$, is:\n$$\\sigma_y = \\gamma \\mu_y = 0.25 \\times (0.79135 Y) = 0.1978375 Y$$\nThe total effective yield from $n$ flowcells, $T_n$, is the sum of the individual yields:\n$$T_n = \\sum_{i=1}^n y_i$$\nDue to the i.i.d. property of the $y_i$ variables, the mean of the total yield, $\\mu_{T_n}$, and the variance of the total yield, $\\sigma_{T_n}^2$, are:\n$$\\mu_{T_n} = E[T_n] = n \\mu_y = n(0.79135 Y)$$\n$$\\sigma_{T_n}^2 = \\text{Var}(T_n) = n \\sigma_y^2 = n (\\gamma \\mu_y)^2 = n \\gamma^2 \\mu_y^2 = n(0.1978375 Y)^2$$\nThe standard deviation of the total yield is:\n$$\\sigma_{T_n} = \\sqrt{n \\sigma_y^2} = \\sigma_y \\sqrt{n} = \\gamma \\mu_y \\sqrt{n} = 0.1978375 Y \\sqrt{n}$$\nThe problem requires that the total effective yield $T_n$ meets or exceeds the target yield $B_{target}$ with a probability of at least $0.95$:\n$$P(T_n \\ge cG) \\ge 0.95$$\nAs suggested by the problem statement (\"standard probability limit theorems appropriate for sums of independent contributions\"), we apply the Central Limit Theorem (CLT). For a sufficiently large $n$, the distribution of the sum $T_n$ can be approximated by a normal distribution with mean $\\mu_{T_n}$ and standard deviation $\\sigma_{T_n}$.\n$$T_n \\sim \\mathcal{N}(\\mu_{T_n}, \\sigma_{T_n}^2)$$\nTo use this approximation, we standardize the variable $T_n$ to a standard normal variable $Z \\sim \\mathcal{N}(0, 1)$:\n$$Z = \\frac{T_n - \\mu_{T_n}}{\\sigma_{T_n}}$$\nThe inequality can be rewritten in terms of $Z$:\n$$P\\left(\\frac{T_n - \\mu_{T_n}}{\\sigma_{T_n}} \\ge \\frac{cG - \\mu_{T_n}}{\\sigma_{T_n}}\\right) \\ge 0.95$$\n$$P\\left(Z \\ge \\frac{cG - n\\mu_y}{\\sigma_y \\sqrt{n}}\\right) \\ge 0.95$$\nLet $z_{0.95}$ be the critical value from the standard normal distribution such that $P(Z \\le z_{0.95}) = 0.95$. This corresponds to the $95$th percentile. From standard statistical tables, $z_{0.95} \\approx 1.645$. The condition $P(Z \\ge k) \\ge 0.95$ is equivalent to $P(Z < k) \\le 0.05$. This means the argument $k$ must be less than or equal to the $5$th percentile, which is $-z_{0.95}$.\nTherefore, we must have:\n$$\\frac{cG - n\\mu_y}{\\sigma_y \\sqrt{n}} \\le -z_{0.95}$$\nRearranging the inequality to solve for $n$:\n$$cG - n\\mu_y \\le -z_{0.95} \\sigma_y \\sqrt{n}$$\n$$n\\mu_y - z_{0.95} \\sigma_y \\sqrt{n} - cG \\ge 0$$\nSubstitute $\\mu_y = Yf$ and $\\sigma_y = \\gamma Yf$:\n$$n(Yf) - z_{0.95} (\\gamma Yf) \\sqrt{n} - cG \\ge 0$$\nThis is a quadratic inequality in terms of $x = \\sqrt{n}$:\n$$(Yf)x^2 - (z_{0.95} \\gamma Yf)x - cG \\ge 0$$\nThe quadratic expression corresponds to a parabola opening upwards (since $Yf > 0$). The inequality holds for values of $x$ greater than or equal to the larger root of the corresponding quadratic equation $(Yf)x^2 - (z_{0.95} \\gamma Yf)x - cG = 0$. The roots are given by the quadratic formula:\n$$x = \\frac{z_{0.95} \\gamma Yf \\pm \\sqrt{(-z_{0.95} \\gamma Yf)^2 - 4(Yf)(-cG)}}{2(Yf)}$$\nSince $x = \\sqrt{n}$ must be positive, we take the positive root:\n$$\\sqrt{n} \\ge \\frac{z_{0.95} \\gamma Yf + \\sqrt{(z_{0.95} \\gamma Yf)^2 + 4(Yf)(cG)}}{2Yf}$$\nTo simplify, we can divide the numerator and denominator by $Yf$:\n$$\\sqrt{n} \\ge \\frac{z_{0.95} \\gamma}{2} + \\sqrt{\\frac{(z_{0.95} \\gamma Yf)^2}{(2Yf)^2} + \\frac{4YfcG}{(2Yf)^2}}$$\n$$\\sqrt{n} \\ge \\frac{z_{0.95} \\gamma}{2} + \\sqrt{\\left(\\frac{z_{0.95} \\gamma}{2}\\right)^2 + \\frac{cG}{Yf}}$$\nSquaring both sides gives the condition on $n$:\n$$n \\ge \\left( \\frac{z_{0.95} \\gamma}{2} + \\sqrt{\\left(\\frac{z_{0.95} \\gamma}{2}\\right)^2 + \\frac{cG}{Yf}} \\right)^2$$\nSince $n$ must be an integer, we take the ceiling of this expression to find the minimal integer $n$ that satisfies the condition.\n$$n = \\left\\lceil \\left( \\frac{z_{0.95} \\gamma}{2} + \\sqrt{\\left(\\frac{z_{0.95} \\gamma}{2}\\right)^2 + \\frac{cG}{Yf}} \\right)^2 \\right\\rceil$$\nNow, we substitute the numerical values for the constants:\n- $z_{0.95} \\approx 1.645$\n- $\\gamma = 0.25$\n- $c = 30$\n- $G = 3.1$\n- $f = 0.79135$\nLet's compute the constant terms:\n$$cG = 30 \\times 3.1 = 93$$\n$$\\frac{z_{0.95} \\gamma}{2} = \\frac{1.645 \\times 0.25}{2} = 0.205625$$\n$$\\left(\\frac{z_{0.95} \\gamma}{2}\\right)^2 = (0.205625)^2 = 0.042281640625$$\n$$\\frac{cG}{f} = \\frac{93}{0.79135}$$\nSubstituting these values into the expression for $n$:\n$$n = \\left\\lceil \\left( 0.205625 + \\sqrt{0.042281640625 + \\frac{93}{0.79135 Y}} \\right)^2 \\right\\rceil$$\nThis is the required closed-form expression for the minimal integer $n$ as a function of $Y$ only.", "answer": "$$\\boxed{\\left\\lceil \\left( 0.205625 + \\sqrt{0.042281640625 + \\frac{93}{0.79135Y}} \\right)^2 \\right\\rceil}$$", "id": "4356388"}, {"introduction": "A primary advantage of long-read sequencing is its power to resolve complex repetitive regions like segmental duplications (SDs), which are notoriously difficult for short-read assemblers and often result in \"collapsed\" representations. This exercise explores how to diagnose such collapses using the assembly's read coverage data, a fundamental quality control step [@problem_id:4356402]. The analysis is built on the principle that, in the absence of bias, sequencing coverage is directly proportional to the copy number of a sequence in the source genome. By learning to identify the characteristic coverage signatures of collapsed versus correctly resolved regions, you can critically assess an assembly's quality in its most challenging areas.", "problem": "A researcher assembles a human genome de novo from long-read data and evaluates the assembly for collapsed versus resolved segmental duplication (SD) families. Reads are sampled uniformly and independently from the genome, and per-base coverage in unique sequence follows a Poisson process with mean depth $C$ (fold-coverage). Reads are aligned back to the assembly with an aligner that reports a single best alignment per read and breaks ties uniformly at random among equally good alignments. The assembly is a primary, haploid representation: if a paralogous family is fully resolved, each paralogous copy is represented by its own contig; if it is collapsed, a single contig represents the consensus of several paralogous copies. For the purposes of this problem, assume no mapping or base-composition bias after standard guanine-cytosine (GC) correction, and that within a family all paralogous copies have the same length $L$.\n\nYou observe that unique regions have mean per-base coverage $C = 30$. Consider three SD families:\n- Family $\\mathrm{F1}$: the true genome contains $k_1 = 3$ near-identical copies (each of length $L = 40\\,\\mathrm{kb}$), and the assembly represents them as a single collapsed contig of length $L$.\n- Family $\\mathrm{F2}$: the true genome contains $k_2 = 4$ copies (each of length $L$); the assembly resolves $m_2 = 2$ copies into two distinct contigs (each length $L$) and collapses the remaining $k_2 - m_2 = 2$ copies into one additional contig of length $L$.\n- Family $\\mathrm{F3}$: the true genome contains $k_3 = 2$ copies (each of length $L$), and the assembly fully resolves both into two distinct contigs (each length $L$).\n\nSelect all statements below that are correct under these assumptions about the expected coverage multipliers in collapsed versus resolved regions and a coverage-based method to detect copy number.\n\nA. In $\\mathrm{F1}$, the expected per-base coverage on the single collapsed contig is approximately $3C$, so with $C = 30$ the observed depth concentrates around $90$.\n\nB. In $\\mathrm{F2}$, each of the two resolved paralogous contigs has expected per-base coverage $C$ (approximately $30$), while the collapsed contig has expected per-base coverage $2C$ (approximately $60$).\n\nC. In $\\mathrm{F3}$, because reads from the two paralogous copies split randomly between the two resolved contigs, each contig’s expected per-base coverage is $C/2$ (approximately $15$).\n\nD. A coverage-based copy-number estimator that sets $\\hat{k} = \\mathrm{round}(\\bar{c}/C)$, where $\\bar{c}$ is the mean per-base coverage in a sufficiently long window within the family after GC correction based on unique regions, is the Maximum Likelihood Estimate (MLE) under a Poisson model and is unbiased for the true copy number $k$ under uniform sampling and the stated mapping policy; collapsed regions will then yield $\\hat{k} > 1$.\n\nE. To avoid false positives, expected coverage in a collapsed region must be divided by its contig length before comparing to the genome-wide baseline, because per-base coverage scales with contig length and longer collapsed contigs inflate apparent depth even when copy number is unchanged.", "solution": "The user wants a critical validation of the provided problem statement, followed by a detailed solution and evaluation of all options.\n\n### Step 1: Extract Givens\n\n-   **Sequencing Model:** Reads are sampled uniformly and independently from the genome.\n-   **Coverage Model:** Per-base coverage in unique, single-copy regions follows a Poisson process with a mean depth of $C$.\n-   **Alignment Policy:** An aligner maps each read to its single best alignment location. Ties among equally good alignments are broken uniformly at random.\n-   **Assembly Structure:** The assembly is a primary, haploid representation.\n    -   A resolved segmental duplication (SD) family has each paralogous copy represented by a distinct contig.\n    -   A collapsed SD family has a single contig representing the consensus of multiple paralogous copies.\n-   **Assumptions:** No mapping or base-composition bias (after GC correction). Within an SD family, all paralogous copies have the same length $L$.\n-   **Observed Data:** The mean coverage in unique regions is $C = 30$.\n-   **Family $\\mathrm{F1}$:**\n    -   True genomic copies: $k_1 = 3$.\n    -   Copy length: $L = 40\\,\\mathrm{kb}$.\n    -   Assembly representation: $1$ collapsed contig of length $L$.\n-   **Family $\\mathrm{F2}$:**\n    -   True genomic copies: $k_2 = 4$.\n    -   Assembly representation: $m_2 = 2$ resolved contigs (each of length $L$) and $1$ collapsed contig (of length $L$) representing the remaining $k_2 - m_2 = 2$ copies.\n-   **Family $\\mathrm{F3}$:**\n    -   True genomic copies: $k_3 = 2$.\n    -   Assembly representation: $2$ fully resolved, distinct contigs (each of length $L$).\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded:** The problem is firmly rooted in computational biology and genomics. The use of a Poisson model for read coverage, the concepts of de novo assembly, segmental duplications, collapsed/resolved regions, and coverage-based copy number estimation are all standard and fundamental to the field. The model is a simplification but represents a valid and widely used first-order approximation.\n-   **Well-Posed:** The problem is well-defined. The givens and assumptions provide a clear framework for calculating the expected coverage in each scenario. The questions posed in the options are answerable within this framework.\n-   **Objective:** The problem statement is quantitative and objective, using precise definitions and avoiding ambiguity or subjective claims.\n-   **Flaw Checklist:**\n    1.  **Scientific/Factual Unsoundness:** None. The model is scientifically sound for its purpose.\n    2.  **Non-Formalizable/Irrelevant:** The problem is formalizable and directly relevant to the specified topic.\n    3.  **Incomplete/Contradictory Setup:** The setup is self-consistent and complete. For each family, the relationship between the true genomic state and the assembled representation is clearly described.\n    4.  **Unrealistic/Infeasible:** The parameters ($C=30$, $k=2-4$) are realistic for modern genomics projects. The alignment model is a standard simplification.\n    5.  **Ill-Posed/Poorly Structured:** The problem is well-structured and leads to a unique set of expected values for coverage under the given model.\n    6.  **Pseudo-Profound/Trivial:** The problem requires a correct application of first principles of sequencing coverage and is not trivial.\n    7.  **Outside Scientific Verifiability:** The claims are verifiable through mathematical derivation based on the provided model.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. I will proceed with deriving the solution and evaluating the options.\n\n### Principle-Based Derivation\n\nThe fundamental principle is that the amount of sequence data generated from any region is proportional to its total length in the genome. The mean coverage $C$ is defined for single-copy (unique) regions. Therefore, the total number of bases sequenced from a genomic element of length $L$ that exists in $k$ copies is expected to be proportional to $k \\times L$. The coverage is defined as (total bases sequenced) / (length of sequence), so the expected coverage for a single-copy region of length $L$ is $C \\times L / L = C$.\n\nLet's apply this to an assembled contig. The expected per-base coverage on an assembled contig, $\\bar{c}$, is the total number of bases from all reads aligned to it, divided by the contig's length.\n\n$$\n\\bar{c} = \\frac{E[\\text{total bases of reads aligned to contig}]}{\\text{length of contig}}\n$$\n\nLet's analyze the expected coverage for each family based on the problem's mapping policy.\n\n**Analysis of Family F1:**\n-   The genome contains $k_1 = 3$ near-identical copies of a segment of length $L$.\n-   Total expected bases sequenced from these 3 copies: $3 \\times L \\times C$.\n-   The assembly has a single collapsed contig of length $L$. Because the true copies are \"near-identical,\" reads from all $3$ copies will align to this single contig.\n-   Expected coverage on the collapsed contig:\n    $$\n    \\bar{c}_1 = \\frac{3LC}{L} = 3C\n    $$\n-   With $C = 30$, the expected coverage is $\\bar{c}_1 = 3 \\times 30 = 90$.\n\n**Analysis of Family F2:**\n-   The genome contains $k_2 = 4$ copies.\n-   The assembly has $2$ resolved contigs and $1$ collapsed contig.\n-   **Resolved Contigs:** Each of the $2$ resolved contigs represents a single, distinct genomic copy. \"Resolved\" implies sufficient divergence for the aligner to map reads from a specific genomic copy uniquely to its corresponding assembled contig.\n    -   For each resolved contig, it receives reads from $1$ genomic copy.\n    -   Expected coverage: $\\bar{c}_{2, \\text{resolved}} = \\frac{1 \\times L \\times C}{L} = C$. With $C=30$, this is $30$.\n-   **Collapsed Contig:** This contig represents the remaining $k_2 - m_2 = 2$ genomic copies. Reads from these $2$ copies will align to this single contig.\n    -   Expected coverage: $\\bar{c}_{2, \\text{collapsed}} = \\frac{2 \\times L \\times C}{L} = 2C$. With $C=30$, this is $60$.\n\n**Analysis of Family F3:**\n-   The genome contains $k_3 = 2$ copies.\n-   The assembly has $2$ fully resolved contigs. As with the resolved contigs in F2, \"resolved\" implies reads map uniquely from a specific genomic copy to its corresponding assembled contig.\n-   Each of the $2$ contigs receives reads from exactly $1$ of the genomic copies.\n-   Expected coverage on each contig:\n    $$\n    \\bar{c}_3 = \\frac{1 \\times L \\times C}{L} = C\n    $$\n-   With $C = 30$, the expected coverage is $30$.\n\n### Option-by-Option Analysis\n\n**A. In $\\mathrm{F1}$, the expected per-base coverage on the single collapsed contig is approximately $3C$, so with $C = 30$ the observed depth concentrates around $90$.**\n-   Our derivation for F1 shows that the expected coverage is $\\bar{c}_1 = 3C$.\n-   Substituting $C = 30$, we get an expected coverage of $90$.\n-   The Poisson distribution for a mean of $90$ has a standard deviation of $\\sqrt{90} \\approx 9.49$. Since the standard deviation is small relative to the mean, the observed coverage will indeed be tightly clustered, or \"concentrate,\" around $90$.\n-   The statement is **Correct**.\n\n**B. In $\\mathrm{F2}$, each of the two resolved paralogous contigs has expected per-base coverage $C$ (approximately $30$), while the collapsed contig has expected per-base coverage $2C$ (approximately $60$).**\n-   Our derivation for F2 shows that the $2$ resolved contigs each have an expected coverage of $C$ (which is $30$), and the single collapsed contig (representing $2$ true copies) has an expected coverage of $2C$ (which is $60$).\n-   This statement perfectly matches our analysis.\n-   The statement is **Correct**.\n\n**C. In $\\mathrm{F3}$, because reads from the two paralogous copies split randomly between the two resolved contigs, each contig’s expected per-base coverage is $C/2$ (approximately $15$).**\n-   This statement contains a premise (\"split randomly\") and a conclusion (coverage is $C/2$). Let's analyze the conclusion based on the premise.\n-   If reads from the $k_3 = 2$ true copies were to split randomly between the two assembled contigs, the total amount of sequenced data ($2LC$) would be distributed between them. On average, each contig would receive half of the total data: $\\frac{2LC}{2} = LC$.\n-   The expected coverage on one contig of length $L$ would be $\\frac{LC}{L} = C$.\n-   The statement's conclusion that the coverage is $C/2$ is arithmetically incorrect, even if one accepts its premise. Furthermore, the premise of random splitting contradicts the notion of the copies being \"resolved,\" which implies they are distinguishable by the aligner, leading to unique, not random, mapping. In the case of unique mapping, the coverage would also be $C$.\n-   The statement is **Incorrect**.\n\n**D. A coverage-based copy-number estimator that sets $\\hat{k} = \\mathrm{round}(\\bar{c}/C)$, where $\\bar{c}$ is the mean per-base coverage in a sufficiently long window within the family after GC correction based on unique regions, is the Maximum Likelihood Estimate (MLE) under a Poisson model and is unbiased for the true copy number $k$ under uniform sampling and the stated mapping policy; collapsed regions will then yield $\\hat{k} > 1$.**\n-   Let's check the components of this statement for a collapsed region representing $k$ true copies.\n    1.  **MLE:** The observed mean coverage is $\\bar{c}$. Under a Poisson model, the MLE for the mean parameter is $\\bar{c}$. The model mean is $\\lambda = kC$. We want to find the integer $k$ that maximizes the likelihood of observing $\\bar{c}$. This is achieved by choosing the integer $k$ that makes $kC$ closest to $\\bar{c}$, which is equivalent to finding the integer $k$ closest to $\\bar{c}/C$. This is exactly what $\\hat{k} = \\mathrm{round}(\\bar{c}/C)$ computes. So, this is the MLE for an integer copy number $k$.\n    2.  **Unbiased:** The estimator for the continuous copy number, $\\bar{c}/C$, is unbiased because $E[\\bar{c}/C] = E[\\bar{c}]/C = (kC)/C = k$. For a sufficiently long window, the Central Limit Theorem implies the distribution of $\\bar{c}$ is approximately normal and thus symmetric. The rounding of a symmetrically distributed variable results in an estimator that is also (asymptotically) unbiased.\n    3.  **Outcome for Collapsed Regions:** A collapsed region, by definition, represents $k > 1$ copies. Its expected coverage is $\\bar{c} = kC$, so the estimated copy number is $\\hat{k} = \\mathrm{round}(kC/C) = k > 1$.\n-   The statement accurately describes the statistical foundation and practical application of coverage-based copy number estimation in this context.\n-   The statement is **Correct**.\n\n**E. To avoid false positives, expected coverage in a collapsed region must be divided by its contig length before comparing to the genome-wide baseline, because per-base coverage scales with contig length and longer collapsed contigs inflate apparent depth even when copy number is unchanged.**\n-   This statement fundamentally misunderstands the concept of per-base coverage. Per-base coverage, $\\bar{c}$, is an *intensive* quantity, meaning it is an average and does not scale with the size (length) of the region. It is calculated as $\\bar{c} = (\\sum_{\\text{bases}} \\text{depth}) / \\text{length}$. The quantity that scales with contig length is the total number of aligned reads, not the per-base coverage.\n-   Dividing the per-base coverage $\\bar{c}$ by the contig length $L$ is a meaningless operation in this context. One compares the per-base coverage $\\bar{c}$ directly to the baseline per-base coverage $C$.\n-   The premise that \"per-base coverage scales with contig length\" is false.\n-   The statement is **Incorrect**.\n\n### Final Conclusion\n\nThe correct statements are A, B, and D.", "answer": "$$\\boxed{ABD}$$", "id": "4356402"}, {"introduction": "The ultimate value of a de novo assembly, particularly for clinical applications, is determined by its accuracy in representing the underlying genome. This practice simulates the essential process of benchmarking an assembly's structural variant (SV) calls against a high-quality \"truth\" set, such as the one provided by the Genome in a Bottle (GIAB) consortium [@problem_id:4356396]. You will apply standard performance metrics—sensitivity, specificity, and precision—to quantify the assembly's performance and, critically, connect these statistics back to their root causes. This provides hands-on experience in interpreting validation data and understanding how specific assembly artifacts can systematically bias performance metrics.", "problem": "A clinical whole-genome was assembled de novo using long-read sequencing and aligned to the reference human genome GRCh38, with structural variants evaluated against the Genome in a Bottle (GIAB) structural variant benchmark set. Evaluation was restricted to GIAB high-confidence intervals and to a curated catalog of candidate loci comprising $K = 20{,}000$ distinct sites, each either labeled as a structural variant positive or a structural variant negative in the benchmark. Of these, $P = 2{,}400$ are labeled positive and $N = 17{,}600$ are labeled negative. Assembly-to-reference alignments produced a callset of structural variants, which was matched to the GIAB catalog using a breakpoint tolerance of $\\pm 50$ base pairs at both breakends per event. Calls outside high-confidence intervals were excluded from evaluation.\n\nWithin the high-confidence catalog, the following outcomes were observed after deduplication of overlapping calls at the same locus:\n- $2{,}268$ positives had a matching called event within the breakpoint tolerance window and were counted as true positives.\n- $132$ positives had no matching call within tolerance and were counted as false negatives.\n- $144$ negatives were incorrectly called as structural variants within tolerance and were counted as false positives.\n- All remaining negatives were counted as true negatives.\n\nIndependent assembly quality assessment indicated that chimeric contig joins affected repetitive regions, contributing to $90$ of the false positives, and that local misassemblies shifted breakpoints beyond the matching tolerance for $80$ of the false negatives.\n\nUsing the standard confusion-matrix definitions for sensitivity, specificity, and precision applied to this per-locus evaluation, compute each of these three metrics. Then, compute the harmonic mean of sensitivity and precision (the $F_{1}$ score). Briefly explain, in terms of assembly-to-reference alignment and breakpoint matching, how contig misassemblies bias the three metrics obtained in this locus-based evaluation.\n\nExpress the final answer as the $F_{1}$ score rounded to four significant figures, reported as a dimensionless decimal. No other rounded values are required in the final answer.", "solution": "The problem statement is first validated for scientific soundness, completeness, and objectivity.\n\n### Step 1: Extract Givens\n- Total candidate loci, $K = 20{,}000$.\n- Total positive loci (ground truth structural variants), $P = 2{,}400$.\n- Total negative loci (ground truth negatives), $N = 17{,}600$.\n- Breakpoint matching tolerance: $\\pm 50$ base pairs at both breakends.\n- True Positives (TP): Number of positives with a matching call = $2{,}268$.\n- False Negatives (FN): Number of positives with no matching call = $132$.\n- False Positives (FP): Number of negatives incorrectly called as a structural variant = $144$.\n- True Negatives (TN): All remaining negatives.\n- Contribution of chimeric contig joins to false positives: $90$ cases.\n- Contribution of local misassemblies to false negatives: $80$ cases.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the established field of computational genomics and bioinformatics, using standard terminology (long-read sequencing, de novo assembly, structural variants, GIAB benchmark, confusion matrix). The provided data is self-consistent and complete. The sum of true positives and false negatives, $\\text{TP} + \\text{FN} = 2{,}268 + 132 = 2{,}400$, correctly equals the total number of positive loci $P$. The total number of negative loci is given as $N = 17{,}600$. The number of true negatives can be derived as $\\text{TN} = N - \\text{FP} = 17{,}600 - 144 = 17{,}456$, which is well-defined. The total number of loci $K = P + N = 2{,}400 + 17{,}600 = 20{,}000$ is also consistent. The problem is well-posed, objective, and contains no scientific or factual unsoundness. The scenario described is realistic for a clinical genomics validation study.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A solution will be provided.\n\n### Solution Derivation\n\nThe problem requires calculating three standard performance metrics (sensitivity, specificity, precision) and a derived metric (the $F_1$ score) from a confusion matrix defined by the provided data. It also asks for an explanation of how specific assembly errors bias these metrics.\n\nFirst, we formalize the confusion matrix components based on the givens:\n- True Positives ($\\text{TP}$): The number of correctly identified positive loci.\n  $$ \\text{TP} = 2{,}268 $$\n- False Negatives ($\\text{FN}$): The number of positive loci that were missed by the assembler/caller.\n  $$ \\text{FN} = 132 $$\n- False Positives ($\\text{FP}$): The number of negative loci that were incorrectly identified as positive.\n  $$ \\text{FP} = 144 $$\n- True Negatives ($\\text{TN}$): The number of correctly identified negative loci. This is calculated from the total negatives $N$ and the false positives $\\text{FP}$.\n  $$ \\text{TN} = N - \\text{FP} = 17{,}600 - 144 = 17{,}456 $$\n\nWith these values, we can compute the required metrics.\n\n**1. Sensitivity (Recall or True Positive Rate)**\nSensitivity measures the proportion of actual positives that are correctly identified.\n$$ \\text{Sensitivity} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{2{,}268}{2{,}268 + 132} = \\frac{2{,}268}{2{,}400} = 0.945 $$\n\n**2. Specificity (True Negative Rate)**\nSpecificity measures the proportion of actual negatives that are correctly identified.\n$$ \\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{17{,}456}{17{,}456 + 144} = \\frac{17{,}456}{17{,}600} \\approx 0.9918 $$\n\n**3. Precision (Positive Predictive Value)**\nPrecision measures the proportion of positive identifications that were actually correct.\n$$ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{2{,}268}{2{,}268 + 144} = \\frac{2{,}268}{2{,}412} \\approx 0.93997 $$\n\n**4. $F_1$ Score**\nThe $F_1$ score is the harmonic mean of precision and sensitivity, providing a single metric that balances both.\nThe general formula is:\n$$ F_1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Sensitivity}}{\\text{Precision} + \\text{Sensitivity}} $$\nA more direct formula using the confusion matrix components is:\n$$ F_1 = \\frac{2 \\times \\text{TP}}{2 \\times \\text{TP} + \\text{FP} + \\text{FN}} $$\nUsing this formula with the given values:\n$$ F_1 = \\frac{2 \\times 2{,}268}{2 \\times 2{,}268 + 144 + 132} = \\frac{4{,}536}{4{,}536 + 276} = \\frac{4{,}536}{4{,}812} $$\nCalculating the decimal value:\n$$ F_1 \\approx 0.9426433915... $$\nRounding to four significant figures as required by the problem gives $0.9426$.\n\n**5. Explanation of Bias from Contig Misassemblies**\n\nThe problem asks how contig misassemblies bias the three primary metrics (sensitivity, specificity, precision). The provided information attributes $90$ false positives to chimeric contigs and $80$ false negatives to local misassemblies.\n\n- **Bias from Chimeric Contig Joins (False Positives):**\nChimeric contigs are assembly artifacts where non-contiguous segments of the genome are erroneously joined. When such a contig is aligned to the reference genome, this artificial junction can be misinterpreted by alignment algorithms as a structural variant (e.g., a large-scale translocation, inversion, or deletion) that does not exist in the actual genome. This leads to a false positive call at a locus that is truly negative. An increase in false positives ($\\text{FP}$) directly impacts two metrics:\n1.  **Specificity:** $\\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}$. An increased $\\text{FP}$ count decreases the numerator (since $\\text{TN} = N - \\text{FP}$) and increases the denominator, thus **negatively biasing (lowering) specificity**. The assembly is less capable of correctly identifying true negative sites.\n2.  **Precision:** $\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$. An increased $\\text{FP}$ count increases the denominator, thus **negatively biasing (lowering) precision**. A higher fraction of the \"positive\" calls are incorrect.\nSensitivity is not directly affected by this error type.\n\n- **Bias from Local Misassemblies (False Negatives):**\nLocal misassemblies involve small-scale errors in the sequence of an assembled contig, often in complex or repetitive regions. When a true structural variant exists, its exact breakpoints are defined in the reference genome. A local misassembly in the contig spanning this variant can alter the sequence around one or both breakpoints. This can cause the alignment to report the variant's breakpoints at positions that fall outside the specified matching tolerance window (here, $\\pm 50$ bp). Consequently, the called variant from the assembly is not matched to the ground truth variant in the benchmark catalog, leading to a failure to detect a true variant. This is counted as a false negative ($\\text{FN}$). An increase in false negatives ($\\text{FN}$) directly impacts one metric:\n1.  **Sensitivity:** $\\text{Sensitivity} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$. An increased $\\text{FN}$ count increases the denominator, thus **negatively biasing (lowering) sensitivity**. The assembly-based calling is less capable of detecting the true variants that are present.\nSpecificity and precision are not directly affected by this error type.\n\nIn summary, contig misassemblies introduce systematic biases that degrade performance evaluation. Chimeras artificially inflate the count of structural variants, reducing specificity and precision, while local misassemblies obscure the signal of true variants, reducing sensitivity.", "answer": "$$\n\\boxed{0.9426}\n$$", "id": "4356396"}]}