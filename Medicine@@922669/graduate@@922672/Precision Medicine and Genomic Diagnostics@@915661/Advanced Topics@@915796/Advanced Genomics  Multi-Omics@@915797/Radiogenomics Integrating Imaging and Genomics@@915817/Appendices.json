{"hands_on_practices": [{"introduction": "Building predictive models in radiogenomics requires combining features from disparate sources, such as imaging and genomics, which often have vastly different numerical scales. This can unintentionally bias regularized models like Ridge regression, where a uniform penalty $\\lambda \\lVert \\mathbf{w} \\rVert_2^2$ is applied to the coefficient vector $\\mathbf{w}$. This practice illustrates why feature standardization is a critical modeling choice, demonstrating how it ensures the penalty is applied equitably and fundamentally influences the final solution.", "problem": "You are studying radiogenomics, integrating Magnetic Resonance Imaging (MRI) features with Next-Generation Sequencing (NGS) derived genomic signatures to predict a continuous tumor aggressiveness index. Consider a linear model with an intercept that predicts the aggressiveness index from two features per sample: an imaging-derived radiomic texture feature and a genomics-derived expression signature. The modeling assumptions are that the response is generated by a linear model with additive Gaussian noise and that we estimate parameters by minimizing empirical squared error augmented with an isotropic squared penalty on the non-intercept coefficients.\n\nFundamental base:\n- The linear model with intercept is defined as $y_i = b + \\mathbf{x}_i^\\top \\mathbf{w} + \\varepsilon_i$, where $\\varepsilon_i$ are independent Gaussian noise terms with zero mean and finite variance, $\\mathbf{x}_i \\in \\mathbb{R}^2$ is the feature vector for sample $i$, $b \\in \\mathbb{R}$ is the intercept, and $\\mathbf{w} \\in \\mathbb{R}^2$ are the slope coefficients.\n- Penalized least squares (Tikhonov or ridge regression) estimates $(b, \\mathbf{w})$ by minimizing the empirical risk $J(b,\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - b - \\mathbf{x}_i^\\top \\mathbf{w})^2 + \\lambda \\lVert \\mathbf{w} \\rVert_2^2$, where $\\lambda \\ge 0$ is the regularization strength.\n- Feature standardization transforms features via $\\tilde{x}_{ij} = \\frac{x_{ij} - \\mu_j}{s_j}$, where $\\mu_j$ is the mean and $s_j$ is the standard deviation of feature $j$ across the $n$ samples.\n\nGoal:\nAnalyze, in raw feature space, how applying ridge regularization with and without feature standardization changes the fitted coefficients. Concretely, for each test case, you must compute the Euclidean norm of the difference between the unscaled ridge solution’s slope coefficients and the standardized ridge solution’s slope coefficients after algebraically re-expressing the standardized solution back into the raw feature space. This isolates the effect of uniform isotropic regularization interacting with feature scaling.\n\nAlgorithmic tasks to implement:\n1. For a given dataset, construct the design matrix $Z \\in \\mathbb{R}^{n \\times 3}$ by concatenating a column of ones with the raw feature matrix $X \\in \\mathbb{R}^{n \\times 2}$, so $Z = [\\mathbf{1}, X]$.\n2. Solve the penalized normal equations with an unpenalized intercept: $(\\frac{1}{n} Z^\\top Z + \\lambda P)\\,\\theta = \\frac{1}{n} Z^\\top y$, where $\\theta = \\begin{bmatrix} b \\\\ \\mathbf{w} \\end{bmatrix}$ and $P = \\mathrm{diag}(0,1,1)$.\n3. Standardize the features to obtain $\\tilde{X}$ by subtracting per-feature means $\\mu_j$ and dividing by per-feature standard deviations $s_j$. Construct $\\tilde{Z} = [\\mathbf{1}, \\tilde{X}]$ and solve $(\\frac{1}{n} \\tilde{Z}^\\top \\tilde{Z} + \\lambda P)\\,\\tilde{\\theta} = \\frac{1}{n} \\tilde{Z}^\\top y$ to obtain $\\tilde{\\theta} = \\begin{bmatrix} \\tilde{b} \\\\ \\tilde{\\mathbf{w}} \\end{bmatrix}$.\n4. Convert the standardized solution back to the raw feature space using the affine transform implied by standardization: $b_{\\mathrm{raw}}^{(\\mathrm{std})} = \\tilde{b} - \\sum_{j=1}^2 \\tilde{w}_j \\frac{\\mu_j}{s_j}$ and $\\mathbf{w}_{\\mathrm{raw}}^{(\\mathrm{std})} = \\left[ \\frac{\\tilde{w}_1}{s_1}, \\frac{\\tilde{w}_2}{s_2} \\right]^\\top$.\n5. Compute the Euclidean norm in $\\mathbb{R}^2$, $\\Delta = \\lVert \\mathbf{w}_{\\mathrm{raw}} - \\mathbf{w}_{\\mathrm{raw}}^{(\\mathrm{std})} \\rVert_2$, where $\\mathbf{w}_{\\mathrm{raw}}$ is the slope vector from step $2$.\n\nNote: When $\\lambda = 0$, both procedures should agree exactly after back-transformation, because standardization is an invertible reparameterization of ordinary least squares. For $\\lambda  0$, feature scaling interacts with the isotropic penalty, generally leading to differences in raw-space coefficients.\n\nTest suite:\nFor each case below, $X$ has two columns, where the first is an MRI-derived texture intensity summary (dimensionless, arbitrary units) and the second is a genomics expression signature score (dimensionless), and $y$ is the aggressiveness index (dimensionless). All numbers are given explicitly.\n\n- Case $1$ (balanced feature scales, moderate regularization):\n$$\nX = \\begin{bmatrix}\n12.0  0.80 \\\\\n15.0  1.10 \\\\\n9.0  0.50 \\\\\n14.0  1.00 \\\\\n11.0  0.90\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\n1.50 \\\\\n2.10 \\\\\n1.20 \\\\\n1.90 \\\\\n1.60\n\\end{bmatrix}, \\quad\n\\lambda = 0.50.\n$$\n\n- Case $2$ (highly unbalanced scales: imaging large magnitude, genomics small, same regularization):\n$$\nX = \\begin{bmatrix}\n800.0  0.80 \\\\\n1200.0  1.10 \\\\\n950.0  0.50 \\\\\n1100.0  1.00 \\\\\n1000.0  0.90\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\n1.50 \\\\\n2.30 \\\\\n1.70 \\\\\n2.10 \\\\\n1.90\n\\end{bmatrix}, \\quad\n\\lambda = 0.50.\n$$\n\n- Case $3$ (same as Case $2$, strong regularization):\n$$\nX = \\begin{bmatrix}\n800.0  0.80 \\\\\n1200.0  1.10 \\\\\n950.0  0.50 \\\\\n1100.0  1.00 \\\\\n1000.0  0.90\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\n1.50 \\\\\n2.30 \\\\\n1.70 \\\\\n2.10 \\\\\n1.90\n\\end{bmatrix}, \\quad\n\\lambda = 50.00.\n$$\n\n- Case $4$ (same as Case $2$, no regularization; boundary condition):\n$$\nX = \\begin{bmatrix}\n800.0  0.80 \\\\\n1200.0  1.10 \\\\\n950.0  0.50 \\\\\n1100.0  1.00 \\\\\n1000.0  0.90\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\n1.50 \\\\\n2.30 \\\\\n1.70 \\\\\n2.10 \\\\\n1.90\n\\end{bmatrix}, \\quad\n\\lambda = 0.00.\n$$\n\n- Case $5$ (near-constant genomics signature and tightly clustered imaging, moderate regularization):\n$$\nX = \\begin{bmatrix}\n1000.0  0.95 \\\\\n990.0  0.96 \\\\\n1010.0  0.94 \\\\\n1005.0  0.95 \\\\\n995.0  0.96\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\n1.80 \\\\\n1.82 \\\\\n1.78 \\\\\n1.81 \\\\\n1.79\n\\end{bmatrix}, \\quad\n\\lambda = 0.50.\n$$\n\nRequired computation and output:\n- For each case, compute the scalar $\\Delta$ as defined above.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where each $r_k$ is the floating-point value of $\\Delta$ for Case $k$ in the order listed.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of regularized linear regression, a fundamental topic in statistics and machine learning. Its formulation is mathematically precise, self-contained, and well-posed, providing all necessary data and algorithmic steps to compute a unique, verifiable solution. The context of radiogenomics provides a realistic application domain for the analysis of feature scaling effects in penalized models.\n\nThe goal is to quantify the difference in estimated slope coefficients for a linear model when using ridge regression on raw features versus on standardized features. This difference arises because an isotropic penalty, $\\lambda \\lVert \\mathbf{w} \\rVert_2^2$, is not invariant to the scaling of the features, whereas the unpenalized least squares loss function is.\n\nThe linear model is given by $y_i = b + \\mathbf{x}_i^\\top \\mathbf{w} + \\varepsilon_i$, where $y_i \\in \\mathbb{R}$ is the response for sample $i$, $\\mathbf{x}_i \\in \\mathbb{R}^2$ is the feature vector, $b \\in \\mathbb{R}$ is the intercept, and $\\mathbf{w} \\in \\mathbb{R}^2$ are the slope coefficients. In matrix form for $n$ samples, this is $y = \\mathbf{1}_n b + X \\mathbf{w} + \\varepsilon$, which can be written as $y = Z \\theta + \\varepsilon$, where $Z = [\\mathbf{1}_n, X]$ is the $n \\times (p+1)$ design matrix (with $p=2$ features) and $\\theta = [b, \\mathbf{w}^\\top]^\\top$ is the $(p+1) \\times 1$ vector of coefficients.\n\nThe coefficients are estimated by minimizing the penalized least squares objective function:\n$$\nJ(b, \\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - b - \\mathbf{x}_i^\\top \\mathbf{w})^2 + \\lambda \\lVert \\mathbf{w} \\rVert_2^2\n$$\nThe intercept $b$ is not penalized. This objective function can be expressed in matrix form as:\n$$\nJ(\\theta) = \\frac{1}{n} (y - Z\\theta)^\\top(y - Z\\theta) + \\lambda \\theta^\\top P \\theta\n$$\nwhere $P = \\mathrm{diag}(0, 1, 1)$ is a $3 \\times 3$ matrix that applies the penalty only to the slope coefficients in $\\theta$.\n\nSetting the gradient $\\nabla_\\theta J(\\theta)$ to zero gives the normal equations:\n$$\n\\left(\\frac{1}{n} Z^\\top Z + \\lambda P\\right) \\theta = \\frac{1}{n} Z^\\top y\n$$\nThis system is solved to find the estimated coefficients for a given dataset $(X, y)$ and regularization strength $\\lambda$.\n\nWe will compute the slope coefficients $\\mathbf{w}$ via two distinct procedures and then compare them.\n\n**Procedure 1: Ridge Regression on Raw Features**\n\n1.  Construct the design matrix $Z = [\\mathbf{1}_n, X]$ using the raw feature matrix $X$.\n2.  Solve the normal equations for $\\theta_{\\mathrm{raw}} = [b_{\\mathrm{raw}}, \\mathbf{w}_{\\mathrm{raw}}^\\top]^\\top$:\n    $$\n    \\left(\\frac{1}{n} Z^\\top Z + \\lambda P\\right) \\theta_{\\mathrm{raw}} = \\frac{1}{n} Z^\\top y\n    $$\n3.  The resulting slope vector is $\\mathbf{w}_{\\mathrm{raw}}$.\n\n**Procedure 2: Ridge Regression on Standardized Features and Back-transformation**\n\n1.  Standardize the feature matrix $X$. For each feature column $j \\in \\{1, 2\\}$, calculate its mean $\\mu_j$ and standard deviation $s_j$:\n    $$\n    \\mu_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij}, \\quad s_j = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\mu_j)^2}\n    $$\n    The standardized features are $\\tilde{x}_{ij} = \\frac{x_{ij} - \\mu_j}{s_j}$. Let $\\tilde{X}$ be the matrix of standardized features.\n2.  Construct the design matrix for the standardized problem, $\\tilde{Z} = [\\mathbf{1}_n, \\tilde{X}]$.\n3.  Solve the corresponding normal equations for the coefficients in the standardized space, $\\tilde{\\theta} = [\\tilde{b}, \\tilde{\\mathbf{w}}^\\top]^\\top$:\n    $$\n    \\left(\\frac{1}{n} \\tilde{Z}^\\top \\tilde{Z} + \\lambda P\\right) \\tilde{\\theta} = \\frac{1}{n} \\tilde{Z}^\\top y\n    $$\n4.  The solution $\\tilde{\\theta}$ provides coefficients for the model $y \\approx \\tilde{b} + \\tilde{X}\\tilde{\\mathbf{w}}$. To compare these with $\\mathbf{w}_{\\mathrm{raw}}$, we must transform them back to the original feature space. The relationship between standardized and raw predictors implies the following transformation for the slope coefficients:\n    $$\n    w_{\\mathrm{raw}, j}^{(\\mathrm{std})} = \\frac{\\tilde{w}_j}{s_j}\n    $$\n    This gives the vector $\\mathbf{w}_{\\mathrm{raw}}^{(\\mathrm{std})}$.\n\n**Final Comparison**\n\nThe effect of standardization is isolated by calculating the Euclidean norm of the difference between the two slope vectors derived in the raw feature space:\n$$\n\\Delta = \\lVert \\mathbf{w}_{\\mathrm{raw}} - \\mathbf{w}_{\\mathrm{raw}}^{(\\mathrm{std})} \\rVert_2\n$$\nThis calculation will be performed for each of the five test cases specified. For the case where $\\lambda=0$, we expect $\\Delta \\approx 0$, as ordinary least squares is equivariant to affine transformations of predictors. For $\\lambda  0$, differences in feature scales ($s_1$ vs. $s_2$) will interact with the isotropic penalty, leading to $\\Delta  0$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the effect of feature standardization on ridge regression coefficients.\n    \n    For each test case, this function computes two sets of ridge regression coefficients:\n    1. On the raw, unscaled features.\n    2. On standardized (z-scored) features, which are then algebraically transformed back\n       to the original feature space.\n    \n    It then calculates the Euclidean norm of the difference between the slope coefficients\n    from these two procedures.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1: balanced feature scales, moderate regularization\n        {\n            \"X\": np.array([\n                [12.0, 0.80], [15.0, 1.10], [9.0, 0.50], [14.0, 1.00], [11.0, 0.90]\n            ]),\n            \"y\": np.array([1.50, 2.10, 1.20, 1.90, 1.60]),\n            \"lambda\": 0.50\n        },\n        # Case 2: highly unbalanced scales, moderate regularization\n        {\n            \"X\": np.array([\n                [800.0, 0.80], [1200.0, 1.10], [950.0, 0.50], [1100.0, 1.00], [1000.0, 0.90]\n            ]),\n            \"y\": np.array([1.50, 2.30, 1.70, 2.10, 1.90]),\n            \"lambda\": 0.50\n        },\n        # Case 3: same as Case 2, strong regularization\n        {\n            \"X\": np.array([\n                [800.0, 0.80], [1200.0, 1.10], [950.0, 0.50], [1100.0, 1.00], [1000.0, 0.90]\n            ]),\n            \"y\": np.array([1.50, 2.30, 1.70, 2.10, 1.90]),\n            \"lambda\": 50.00\n        },\n        # Case 4: same as Case 2, no regularization (OLS)\n        {\n            \"X\": np.array([\n                [800.0, 0.80], [1200.0, 1.10], [950.0, 0.50], [1100.0, 1.00], [1000.0, 0.90]\n            ]),\n            \"y\": np.array([1.50, 2.30, 1.70, 2.10, 1.90]),\n            \"lambda\": 0.00\n        },\n        # Case 5: near-constant signatures, moderate regularization\n        {\n            \"X\": np.array([\n                [1000.0, 0.95], [990.0, 0.96], [1010.0, 0.94], [1005.0, 0.95], [995.0, 0.96]\n            ]),\n            \"y\": np.array([1.80, 1.82, 1.78, 1.81, 1.79]),\n            \"lambda\": 0.50\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X = case[\"X\"]\n        y = case[\"y\"]\n        lambda_val = case[\"lambda\"]\n        \n        n, p = X.shape\n        \n        # --- Procedure 1: Ridge Regression on Raw Features ---\n        \n        # Construct design matrix Z = [1, X]\n        Z_raw = np.hstack([np.ones((n, 1)), X])\n        \n        # Penalty matrix P = diag(0, 1, ..., 1)\n        P = np.diag([0.0] + [1.0] * p)\n        \n        # Left-hand side of normal equations: (1/n * Z'Z + lambda * P)\n        A_raw = (1/n) * (Z_raw.T @ Z_raw) + lambda_val * P\n        \n        # Right-hand side of normal equations: (1/n * Z'y)\n        b_raw = (1/n) * (Z_raw.T @ y)\n\n        # Solve for coefficients theta_raw = [b_raw, w_raw]\n        theta_raw = np.linalg.solve(A_raw, b_raw)\n        w_raw = theta_raw[1:]\n\n        # --- Procedure 2: Ridge Regression on Standardized Features ---\n\n        # Standardize features (using population std dev, i.e., ddof=0)\n        mu = np.mean(X, axis=0)\n        s = np.std(X, axis=0)\n        \n        # Handle cases where a feature is constant (s_j = 0) to avoid division by zero.\n        # Although not present in test data, this is good practice.\n        # We replace s_j=0 with 1 to avoid scaling, as the feature is constant anyway.\n        s_safe = np.where(s == 0, 1, s)\n        X_std = (X - mu) / s_safe\n\n        # Construct standardized design matrix\n        Z_std = np.hstack([np.ones((n, 1)), X_std])\n        \n        # Solve the normal equations for the standardized problem\n        A_std = (1/n) * (Z_std.T @ Z_std) + lambda_val * P\n        b_std = (1/n) * (Z_std.T @ y)\n        theta_std = np.linalg.solve(A_std, b_std)\n        w_tilde = theta_std[1:]\n        \n        # --- Back-transform and Compare ---\n        \n        # Convert standardized slope coefficients back to raw feature space\n        w_raw_from_std = w_tilde / s_safe\n        \n        # Compute the Euclidean norm of the difference\n        delta = np.linalg.norm(w_raw - w_raw_from_std)\n        results.append(delta)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4374286"}, {"introduction": "A central goal of radiogenomics is to uncover shared biological signals between high-dimensional imaging and genomic data. A powerful strategy for this is to perform dimensionality reduction on a concatenated data matrix, but this requires careful normalization to prevent one data type from dominating the analysis. Through this exercise, you will implement a principled workflow for multi-block data fusion, learning to balance the influence of different data blocks and use Principal Component Analysis (PCA) to identify and interpret the integrated patterns of variation.", "problem": "You are given paired data matrices representing radiomic features and genomic features for the same cohort of patients in a radiogenomics integration scenario. Let $X_r \\in \\mathbb{R}^{n \\times p_r}$ denote the radiomics block and $X_g \\in \\mathbb{R}^{n \\times p_g}$ denote the genomics block, with $n$ patients, $p_r$ radiomic features, and $p_g$ genomic features. The task is to design a program that performs principled block-wise preprocessing and dimensionality reduction, then quantifies integration-relevant metrics using Principal Component Analysis (PCA).\n\nFundamental base:\n- Z-score standardization: for any feature vector $x \\in \\mathbb{R}^n$, its standardized form is $z = (x - \\bar{x}) / s_x$, where $\\bar{x}$ is the mean and $s_x$ is the sample standard deviation computed with degrees of freedom $1$.\n- Frobenius norm: for any matrix $A \\in \\mathbb{R}^{n \\times p}$, $\\|A\\|_F = \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^p a_{ij}^2}$.\n- Singular Value Decomposition (SVD): for any real matrix $Z \\in \\mathbb{R}^{n \\times p}$, there exists an orthogonal decomposition $Z = U S V^\\top$, where $U \\in \\mathbb{R}^{n \\times m}$ with orthonormal columns, $V \\in \\mathbb{R}^{p \\times m}$ with orthonormal columns, $S \\in \\mathbb{R}^{m \\times m}$ is diagonal with nonnegative entries $s_1 \\ge s_2 \\ge \\dots \\ge s_m \\ge 0$, and $m = \\min(n, p)$.\n\nThe program must implement the following steps from these principles:\n1. For each block $X_r$ and $X_g$, perform per-feature centering and Z-score standardization across patients to obtain $Z_r$ and $Z_g$. If any feature has zero variance ($s_x = 0$), treat it as carrying no standardizable signal by leaving it centered without scaling that would divide by zero (i.e., set its standardized entries to zero after centering).\n2. Perform block scaling to equalize total block energy using Frobenius norms: compute $w_r = 1/\\|Z_r\\|_F$ if $\\|Z_r\\|_F  0$ else $w_r = 1$, and $w_g = 1/\\|Z_g\\|_F$ if $\\|Z_g\\|_F  0$ else $w_g = 1$. Form the concatenated, block-scaled matrix $Z = [\\, w_r Z_r \\;\\; w_g Z_g \\,] \\in \\mathbb{R}^{n \\times (p_r + p_g)}$ by horizontal concatenation.\n3. Compute the Singular Value Decomposition (SVD) of $Z$: $Z = U S V^\\top$. Use this decomposition to compute the following quantities:\n   - Cumulative explained variance ratio up to the first $k$ principal components: $$R_k = \\frac{\\sum_{j=1}^k s_j^2}{\\sum_{j=1}^m s_j^2},$$ where $m = \\min(n, p_r + p_g)$.\n   - Block loading energy for the first principal component: let $v_1 \\in \\mathbb{R}^{p_r + p_g}$ be the first right singular vector (the first column of $V$ or equivalently the first row of $V^\\top$ conjugate-transposed). Define $$B_r = \\sum_{i=1}^{p_r} v_{1,i}^2 \\quad \\text{and} \\quad B_g = \\sum_{i=p_r+1}^{p_r+p_g} v_{1,i}^2.$$ By orthonormality of $V$, $B_r + B_g = 1$ and each is a decimal between $0$ and $1$ indicating the relative contribution of the radiomics versus genomics block to the first component direction.\n   - Relative reconstruction error using the first $k$ principal components: let $U_k \\in \\mathbb{R}^{n \\times k}$, $S_k \\in \\mathbb{R}^{k \\times k}$, and $V_k \\in \\mathbb{R}^{(p_r+p_g) \\times k}$ denote the truncated factors. The rank-$k$ reconstruction is $Z_k = U_k S_k V_k^\\top$. The relative reconstruction error is $$E_k = \\frac{\\|Z - Z_k\\|_F}{\\|Z\\|_F}.$$\n\nYour program must output, for each test case, a list $[R_k, B_r, B_g, E_k]$ where each value is rounded to six decimal places and represented as a decimal (not a percentage). The final output must be a single line containing a list of these per-test-case lists, formatted as a comma-separated list enclosed in square brackets; for example, $\\big[ [ 0.123456, 0.400000, 0.600000, 0.250000 ], [ \\dots ] \\big]$.\n\nTest suite:\nProvide results for the following four cases, which cover a general case, an imbalance in feature counts, a collinearity edge case, and a zero-variance edge case. For each case, the integer $k$ is specified.\n\nCase (a): $n = 6$, $p_r = 3$, $p_g = 4$, $k = 2$, with\n$$\nX_r =\n\\begin{bmatrix}\n1  2  -1 \\\\\n2  3  -1 \\\\\n3  5  -2 \\\\\n4  6  -3 \\\\\n5  8  -4 \\\\\n6  9  -4\n\\end{bmatrix},\n\\quad\nX_g =\n\\begin{bmatrix}\n2  1  0  1 \\\\\n4  1  -1  2 \\\\\n5  2  1  3 \\\\\n8  3  0  4 \\\\\n10  4  1  5 \\\\\n12  4  2  6\n\\end{bmatrix}.\n$$\n\nCase (b): $n = 6$, $p_r = 2$, $p_g = 8$, $k = 3$, with\n$$\nX_r =\n\\begin{bmatrix}\n1  100 \\\\\n2  90 \\\\\n1  110 \\\\\n3  95 \\\\\n2  105 \\\\\n4  100\n\\end{bmatrix},\n\\quad\nX_g =\n\\begin{bmatrix}\n0  1  0  1  2  3  0  5 \\\\\n1  2  1  2  2  3  1  6 \\\\\n0  1  1  1  3  4  2  5 \\\\\n2  3  2  3  2  4  3  7 \\\\\n1  2  3  2  3  5  4  8 \\\\\n3  4  3  4  4  6  5  9\n\\end{bmatrix}.\n$$\n\nCase (c): $n = 5$, $p_r = 3$, $p_g = 3$, $k = 1$, with collinearity in $X_r$,\n$$\nX_r =\n\\begin{bmatrix}\n1  2  -1 \\\\\n2  4  -2 \\\\\n3  6  -3 \\\\\n4  8  -4 \\\\\n5  10  -5\n\\end{bmatrix},\n\\quad\nX_g =\n\\begin{bmatrix}\n0  1  0 \\\\\n1  1  -1 \\\\\n1  2  0 \\\\\n2  3  1 \\\\\n3  5  2\n\\end{bmatrix}.\n$$\n\nCase (d): $n = 4$, $p_r = 3$, $p_g = 2$, $k = 1$, with zero-variance features present,\n$$\nX_r =\n\\begin{bmatrix}\n1  2  5 \\\\\n2  4  5 \\\\\n3  6  5 \\\\\n4  8  5\n\\end{bmatrix},\n\\quad\nX_g =\n\\begin{bmatrix}\n0  0 \\\\\n0  0 \\\\\n1  0 \\\\\n1  0\n\\end{bmatrix}.\n$$\n\nAngle units are not applicable. No physical units are involved. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each case represented by $[R_k, B_r, B_g, E_k]$ rounded to six decimal places, for example $\\big[ [ 0.123456, 0.400000, 0.600000, 0.250000 ], [ \\dots ] \\big]$.", "solution": "The user has provided a problem statement that requires computational analysis of paired radiomic and genomic data matrices. I will first validate the problem statement according to the specified criteria.\n\n### Step 1: Extract Givens\n\n- **Data Blocks**: A radiomics matrix $X_r \\in \\mathbb{R}^{n \\times p_r}$ and a genomics matrix $X_g \\in \\mathbb{R}^{n \\times p_g}$, where $n$ is the number of patients, $p_r$ is the number of radiomic features, and $p_g$ is the number of genomic features.\n- **Fundamental Definitions**:\n    - **Z-score standardization**: For a feature vector $x \\in \\mathbb{R}^n$, the standardized vector is $z = (x - \\bar{x}) / s_x$, where $\\bar{x}$ is the mean and $s_x$ is the sample standard deviation using $n-1$ as the denominator (degrees of freedom is $1$).\n    - **Frobenius norm**: For a matrix $A \\in \\mathbb{R}^{n \\times p}$, its norm is $\\|A\\|_F = \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^p a_{ij}^2}$.\n    - **Singular Value Decomposition (SVD)**: For a real matrix $Z \\in \\mathbb{R}^{n \\times p}$, its SVD is $Z = U S V^\\top$, with properties as described in the problem.\n- **Computational Steps**:\n    1.  **Standardization**: Standardize columns of $X_r$ and $X_g$ to get $Z_r$ and $Z_g$. For features with zero variance, the centered and scaled column becomes a zero vector.\n    2.  **Block Scaling**: Compute weights $w_r = 1/\\|Z_r\\|_F$ and $w_g = 1/\\|Z_g\\|_F$ (if the norms are non-zero, otherwise the weight is $1$). Form the concatenated matrix $Z = [\\, w_r Z_r \\;\\; w_g Z_g \\,]$.\n    3.  **SVD and Metric Calculation**: Compute the SVD of $Z$. From this, calculate:\n        - Cumulative explained variance ratio: $R_k = (\\sum_{j=1}^k s_j^2) / (\\sum_{j=1}^m s_j^2)$, for $m = \\min(n, p_r + p_g)$.\n        - Block loading energy: $B_r = \\sum_{i=1}^{p_r} v_{1,i}^2$ and $B_g = \\sum_{i=p_r+1}^{p_r+p_g} v_{1,i}^2$, using the first right singular vector $v_1$.\n        - Relative reconstruction error: $E_k = \\|Z - Z_k\\|_F / \\|Z\\|_F$, for the rank-$k$ approximation $Z_k$.\n- **Test Cases**: Four specific cases are provided with matrices $X_r$, $X_g$ and an integer $k$.\n- **Output Format**: A single-line string representing a list of lists, where each inner list contains $[R_k, B_r, B_g, E_k]$ with values rounded to six decimal places.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is situated in the field of radiogenomics and employs standard, well-established statistical techniques for data integration, including Z-score standardization, block-wise normalization, and Principal Component Analysis (PCA) via SVD. All definitions and proposed metrics are mathematically and conceptually sound.\n- **Well-Posed**: The problem is clearly defined with all necessary data ($X_r, X_g, k$) and explicit, step-by-step instructions. The sequence of operations leads to a unique, computable set of metrics for each test case.\n- **Objective**: The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n- **Completeness and Consistency**: The instructions are complete and internally consistent. The special handling for zero-variance features is clearly specified, preventing division-by-zero errors and ensuring the procedure is robust.\n- **Realism and Feasibility**: The provided test cases use small, synthetic matrices that are computationally tractable while being representative of the structural challenges (e.g., collinearity, zero variance) encountered in real data.\n- **Structure and Clarity**: The problem is well-structured, with clear definitions and a logical flow from data preprocessing to analysis and metric extraction.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. It is a well-posed, scientifically grounded computational task. I will proceed with the solution.\n\n### Solution\n\nThe solution entails a systematic implementation of the specified multi-step procedure. The core of this procedure is a form of consensus Principal Component Analysis (PCA) applied to two blocks of data, a common strategy in data-fusion tasks like radiogenomics.\n\n**1. Per-Feature Standardization**\n\nFor each data block, $X_r$ and $X_g$, every feature (column) is standardized independently. For a given feature vector $x \\in \\mathbb{R}^n$, standardization transforms it into $z$ such that its mean is $0$ and its sample variance is $1$. The formula is:\n$$\nz = \\frac{x - \\bar{x}}{s_x}\n$$\nwhere $\\bar{x}$ is the mean of $x$ and $s_x$ is the sample standard deviation, calculated with degrees of freedom $n-1$. This step ensures that all features, regardless of their original units or scale, contribute equally to the initial calculation of total variance within each block.\n\nA critical edge case is when a feature has zero variance (i.e., $s_x = 0$). In this scenario, the feature column is constant. The problem correctly specifies the handling: the centered column, which is a vector of zeros, is used as the standardized column. This correctly reflects that a constant feature provides no information about the variance across patients. This operation results in the standardized matrices $Z_r$ and $Z_g$.\n\n**2. Block Energy Equalization**\n\nAfter standardizing features within each block, the blocks themselves might have different \"total energy\", quantified by the square of their Frobenius norms, $\\|Z_r\\|_F^2$ and $\\|Z_g\\|_F^2$. The number of features ($p_r, p_g$) directly influences this energy, as the sum of squares of a standardized feature vector is $n-1$. Therefore, $\\|Z_r\\|_F^2 = p_r^* (n-1)$ where $p_r^*$ is the number of non-constant features in $X_r$.\n\nTo prevent the block with more features from dominating the joint analysis, we perform block scaling. We compute scaling weights $w_r = 1 / \\|Z_r\\|_F$ and $w_g = 1 / \\|Z_g\\|_F$. Applying these weights, we obtain scaled blocks $w_r Z_r$ and $w_g Z_g$. Each of these scaled blocks now has a Frobenius norm of $1$, ensuring they contribute equally to the total variance of the concatenated matrix. The final matrix for analysis is formed by horizontal concatenation:\n$$\nZ = [\\, w_r Z_r \\;\\; w_g Z_g \\,]\n$$\nThe squared Frobenius norm of this combined matrix is $\\|Z\\|_F^2 = \\|w_r Z_r\\|_F^2 + \\|w_g Z_g\\|_F^2 = 1 + 1 = 2$.\n\n**3. Singular Value Decomposition (SVD)**\n\nPCA is performed on the matrix $Z$ by computing its SVD: $Z = U S V^\\top$. The columns of $U S$ are the principal components (scores), and the columns of $V$ are the principal axes (loadings). The diagonal entries of $S$ are the singular values, $s_j$. The total variance in the data is proportional to the sum of the squared singular values, $\\sum_j s_j^2$, which equals $\\|Z\\|_F^2$.\n\n**4. Metric Calculation**\n\nThe SVD provides all the necessary elements to compute the required metrics.\n\n- **Cumulative Explained Variance Ratio ($R_k$)**: The variance explained by the $j$-th principal component is proportional to $s_j^2$. The cumulative explained variance ratio for the first $k$ components is the fraction of the total variance they account for:\n$$\nR_k = \\frac{\\sum_{j=1}^k s_j^2}{\\sum_{j=1}^{\\min(n, p_r+p_g)} s_j^2}\n$$\n\n- **Block Loading Energy ($B_r, B_g$)**: The first principal axis, $v_1$ (the first column of $V$), is a vector of loadings of length $p_r + p_g$. It defines the direction of maximum variance in the high-dimensional feature space. The first $p_r$ elements of $v_1$ correspond to the radiomics features, and the next $p_g$ elements correspond to the genomics features. The squared magnitude of these sub-vectors indicates the contribution of each block to defining this principal direction. We calculate:\n$$\nB_r = \\sum_{i=1}^{p_r} v_{1,i}^2 \\quad \\text{and} \\quad B_g = \\sum_{i=p_r+1}^{p_r+p_g} v_{1,i}^2\n$$\nSince $v_1$ is a unit vector, it is guaranteed that $B_r + B_g = 1$. These metrics quantify the relative importance of radiomics versus genomics in the dominant pattern of covariation.\n\n- **Relative Reconstruction Error ($E_k$)**: The Eckart-Young-Mirsky theorem states that the best rank-$k$ approximation of a matrix $Z$ is $Z_k = U_k S_k V_k^\\top$, where the factors are truncated SVD components. The theorem also provides the norm of the error matrix: $\\|Z - Z_k\\|_F^2 = \\sum_{j=k+1}^m s_j^2$. The relative reconstruction error is thus:\n$$\nE_k = \\frac{\\|Z - Z_k\\|_F}{\\|Z\\|_F} = \\frac{\\sqrt{\\sum_{j=k+1}^m s_j^2}}{\\sqrt{\\sum_{j=1}^m s_j^2}}\n$$\nThis is equivalent to $\\sqrt{1 - R_k}$, providing a computationally efficient way to find the error once $R_k$ is known.\n\nThe following program implements this entire workflow for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef standardize(X):\n    \"\"\"\n    Performs Z-score standardization on the columns of matrix X.\n    Uses sample standard deviation (ddof=1).\n    Columns with zero variance are set to zero.\n    \"\"\"\n    # Ensure input is float for calculations\n    X_float = X.astype(float)\n    \n    # Center the data\n    mean = np.mean(X_float, axis=0)\n    X_centered = X_float - mean\n    \n    # Calculate sample standard deviation\n    std_dev = np.std(X_float, axis=0, ddof=1)\n    \n    # Standardize, handling zero-variance columns\n    Z = np.zeros_like(X_float)\n    non_zero_std_mask = std_dev  1e-15 # Use a tolerance for floating point comparison\n    \n    # Use np.divide for safe division\n    if np.any(non_zero_std_mask):\n        Z[:, non_zero_std_mask] = np.divide(\n            X_centered[:, non_zero_std_mask],\n            std_dev[non_zero_std_mask]\n        )\n        \n    return Z\n\ndef solve():\n    \"\"\"\n    Main function to run the radiogenomics integration analysis for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"k\": 2,\n            \"Xr\": np.array([\n                [1, 2, -1], [2, 3, -1], [3, 5, -2], \n                [4, 6, -3], [5, 8, -4], [6, 9, -4]\n            ]),\n            \"Xg\": np.array([\n                [2, 1, 0, 1], [4, 1, -1, 2], [5, 2, 1, 3],\n                [8, 3, 0, 4], [10, 4, 1, 5], [12, 4, 2, 6]\n            ])\n        },\n        {\n            \"k\": 3,\n            \"Xr\": np.array([\n                [1, 100], [2, 90], [1, 110], \n                [3, 95], [2, 105], [4, 100]\n            ]),\n            \"Xg\": np.array([\n                [0, 1, 0, 1, 2, 3, 0, 5], [1, 2, 1, 2, 2, 3, 1, 6],\n                [0, 1, 1, 1, 3, 4, 2, 5], [2, 3, 2, 3, 2, 4, 3, 7],\n                [1, 2, 3, 2, 3, 5, 4, 8], [3, 4, 3, 4, 4, 6, 5, 9]\n            ])\n        },\n        {\n            \"k\": 1,\n            \"Xr\": np.array([\n                [1, 2, -1], [2, 4, -2], [3, 6, -3], \n                [4, 8, -4], [5, 10, -5]\n            ]),\n            \"Xg\": np.array([\n                [0, 1, 0], [1, 1, -1], [1, 2, 0], \n                [2, 3, 1], [3, 5, 2]\n            ])\n        },\n        {\n            \"k\": 1,\n            \"Xr\": np.array([\n                [1, 2, 5], [2, 4, 5], \n                [3, 6, 5], [4, 8, 5]\n            ]),\n            \"Xg\": np.array([\n                [0, 0], [0, 0], \n                [1, 0], [1, 0]\n            ])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        Xr, Xg, k = case[\"Xr\"], case[\"Xg\"], case[\"k\"]\n        pr = Xr.shape[1]\n\n        # Step 1: Standardization\n        Zr = standardize(Xr)\n        Zg = standardize(Xg)\n\n        # Step 2: Block Scaling\n        norm_Zr = np.linalg.norm(Zr, 'fro')\n        norm_Zg = np.linalg.norm(Zg, 'fro')\n\n        wr = 1.0 / norm_Zr if norm_Zr  0 else 1.0\n        wg = 1.0 / norm_Zg if norm_Zg  0 else 1.0\n\n        Z = np.concatenate((wr * Zr, wg * Zg), axis=1)\n\n        # Step 3: SVD and Metrics\n        # Use full_matrices=False for efficiency\n        U, s, Vt = np.linalg.svd(Z, full_matrices=False)\n        \n        s_squared = s**2\n        total_variance = np.sum(s_squared)\n\n        if total_variance  1e-15:\n            # R_k: Cumulative explained variance ratio\n            Rk = np.sum(s_squared[:k]) / total_variance\n\n            # B_r, B_g: Block loading energy\n            v1 = Vt[0, :]\n            Br = np.sum(v1[:pr]**2)\n            Bg = np.sum(v1[pr:]**2)\n            \n            # E_k: Relative reconstruction error\n            Ek = np.sqrt(1 - Rk)\n        else:\n            # Handle case of zero-variance matrix Z\n            Rk = 0.0\n            # If v1 is not well-defined, split contribution by number of features\n            total_p = Z.shape[1]\n            Br = pr / total_p if total_p  0 else 0.5\n            Bg = 1.0 - Br\n            # Error of approximating 0 with 0 is 0\n            Ek = 0.0\n\n        all_results.append([Rk, Br, Bg, Ek])\n\n    # Format the final output string as specified\n    # e.g., [[val1,val2,...],[val1,val2,...]]\n    output_lists = []\n    for result_list in all_results:\n        # Format each number to have 6 decimal places.\n        inner_list_str = ','.join([f\"{v:.6f}\" for v in result_list])\n        output_lists.append(f\"[{inner_list_str}]\")\n    \n    final_output_str = f\"[{','.join(output_lists)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```", "id": "4374287"}, {"introduction": "When searching for associations across hundreds or thousands of radiomic features, the probability of finding a strong correlation just by chance becomes very high. Standard statistical tests are often inadequate because they don't account for this massive \"multiple testing\" burden. This hands-on exercise introduces permutation testing as a robust, computer-intensive solution, allowing you to generate an empirical null distribution for your statistic and calculate a valid $p$-value that corrects for the multiple comparisons.", "problem": "Consider a radiogenomics setting in which a set of quantitative imaging features is linked to a single genomic measurement vector across the same set of subjects. Let $X \\in \\mathbb{R}^{n \\times p}$ denote the matrix of imaging features, where $n$ is the number of subjects and $p$ is the number of features, and let $y \\in \\mathbb{R}^{n}$ denote the genomic measurement vector for those subjects. The objective is to assess the significance of the maximal association between the genomic measurement and any single imaging feature, using a permutation test based on the principle of exchangeability under the null hypothesis.\n\nFundamental base and assumptions:\n- Under the null hypothesis of no association between imaging features and the genomic measurement, the indices of $y$ are exchangeable across subjects. That is, any permutation of $y$ across subjects is equally likely, so the distribution of any association statistic computed between $X$ and $y$ can be approximated by randomly permuting $y$ and recomputing the statistic.\n- The association between one feature vector $x \\in \\mathbb{R}^{n}$ and the genomic vector $y \\in \\mathbb{R}^{n}$ is measured by the Pearson Correlation Coefficient (PCC), defined for non-degenerate data by\n$$\n\\rho(x,y) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}},\n$$\nwhere $\\bar{x}$ and $\\bar{y}$ denote sample means. If $\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} = 0$ or $\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} = 0$, define $\\rho(x,y) = 0$ to avoid division by zero.\n- Define the maximal association statistic across features as\n$$\nT_{\\mathrm{obs}} = \\max_{1 \\le j \\le p} \\left|\\rho\\left(X_{\\cdot j}, y\\right)\\right|,\n$$\nwhere $X_{\\cdot j}$ is the $j$-th column of $X$.\n\nPermutation testing procedure:\n- Generate $M$ random permutations of the indices $\\{1,2,\\ldots,n\\}$ of $y$, each permutation sampled uniformly at random with a fixed seed $s$ for reproducibility.\n- For each permutation $b \\in \\{1,2,\\ldots,M\\}$, compute\n$$\nT^{(b)} = \\max_{1 \\le j \\le p} \\left|\\rho\\left(X_{\\cdot j}, y^{(b)}\\right)\\right|,\n$$\nwhere $y^{(b)}$ is $y$ with its entries permuted according to the $b$-th random permutation.\n- Compute the empirical $p$-value using the plus-one correction to ensure validity even when $T_{\\mathrm{obs}}$ is extreme:\n$$\np = \\frac{1 + \\sum_{b=1}^{M} \\mathbf{1}\\left\\{T^{(b)} \\ge T_{\\mathrm{obs}}\\right\\}}{1 + M}.\n$$\n\nYour task is to implement a program that, for each test case in the suite below, computes the empirical $p$-value for $T_{\\mathrm{obs}}$ using the procedure above. The program must handle zero-variance features or a zero-variance genomic vector according to the rule $\\rho(x,y)=0$ whenever either variance is zero.\n\nTest suite (each case specifies $n$, $p$, $X$, $y$, number of permutations $M$, and seed $s$):\n- Case $1$ (happy path):\n  - $n = 8$, $p = 5$, $M = 1000$, $s = 42$.\n  - $y = [\\, 0.7,\\ -1.2,\\ 0.3,\\ 0.5,\\ -0.4,\\ 1.1,\\ -0.8,\\ 0.0 \\,]$.\n  - $X$ has columns\n    - $X_{\\cdot 1} = [\\, 1.0,\\ 0.5,\\ -0.2,\\ 0.4,\\ 0.0,\\ 0.9,\\ -0.5,\\ -0.1 \\,]$,\n    - $X_{\\cdot 2} = [\\, 0.2,\\ -0.1,\\ 0.3,\\ 0.1,\\ -0.3,\\ 0.4,\\ -0.2,\\ 0.0 \\,]$,\n    - $X_{\\cdot 3} = [\\, 2.0,\\ 1.5,\\ -1.0,\\ 1.2,\\ -0.5,\\ 1.8,\\ -1.3,\\ -0.2 \\,]$,\n    - $X_{\\cdot 4} = [\\, 0.5,\\ -0.6,\\ 0.4,\\ 0.7,\\ -0.5,\\ 0.9,\\ -0.8,\\ 0.1 \\,]$,\n    - $X_{\\cdot 5} = [\\, -0.3,\\ 0.1,\\ -0.4,\\ 0.2,\\ 0.0,\\ -0.1,\\ 0.3,\\ 0.5 \\,]$.\n- Case $2$ (zero-variance feature present):\n  - $n = 6$, $p = 4$, $M = 500$, $s = 123$.\n  - $y = [\\, 1.0,\\ 0.9,\\ -0.8,\\ 0.7,\\ -0.6,\\ 0.5 \\,]$.\n  - $X$ has columns\n    - $X_{\\cdot 1} = [\\, 2.0,\\ 1.8,\\ -1.6,\\ 1.4,\\ -1.2,\\ 1.0 \\,]$,\n    - $X_{\\cdot 2} = [\\, 0.0,\\ 0.0,\\ 0.0,\\ 0.0,\\ 0.0,\\ 0.0 \\,]$,\n    - $X_{\\cdot 3} = [\\, -0.5,\\ -0.2,\\ 0.0,\\ 0.1,\\ 0.3,\\ 0.4 \\,]$,\n    - $X_{\\cdot 4} = [\\, 0.6,\\ -0.7,\\ 0.8,\\ -0.9,\\ 1.0,\\ -1.1 \\,]$.\n- Case $3$ (tie in maximal feature correlation):\n  - $n = 7$, $p = 3$, $M = 400$, $s = 2023$.\n  - $y = [\\, 0.2,\\ 0.4,\\ 0.6,\\ 0.8,\\ -0.1,\\ -0.3,\\ -0.5 \\,]$.\n  - $X$ has columns\n    - $X_{\\cdot 1} = [\\, 1.0,\\ 2.0,\\ 3.0,\\ 4.0,\\ -0.5,\\ -1.0,\\ -1.5 \\,]$,\n    - $X_{\\cdot 2} = [\\, 1.0,\\ 2.0,\\ 3.0,\\ 4.0,\\ -0.5,\\ -1.0,\\ -1.5 \\,]$,\n    - $X_{\\cdot 3} = [\\, 0.5,\\ -0.2,\\ 0.1,\\ -0.3,\\ 0.2,\\ -0.1,\\ 0.0 \\,]$.\n- Case $4$ (constant genomic vector boundary):\n  - $n = 5$, $p = 3$, $M = 200$, $s = 7$.\n  - $y = [\\, 1.0,\\ 1.0,\\ 1.0,\\ 1.0,\\ 1.0 \\,]$.\n  - $X$ has columns\n    - $X_{\\cdot 1} = [\\, 0.3,\\ -0.2,\\ 0.1,\\ 0.0,\\ 0.2 \\,]$,\n    - $X_{\\cdot 2} = [\\, -0.1,\\ 0.2,\\ -0.3,\\ 0.4,\\ -0.5 \\,]$,\n    - $X_{\\cdot 3} = [\\, 0.5,\\ 0.5,\\ 0.5,\\ 0.5,\\ 0.5 \\,]$.\n\nAlgorithmic and output requirements:\n- Implement the computation of $T_{\\mathrm{obs}}$ and the empirical $p$-value $p$ as defined above for each test case, using exactly $M$ permutations and the specified seed $s$ by initializing a pseudorandom number generator deterministically.\n- For numerical stability and correctness, when computing $\\rho(x,y)$, use the exact definition provided and assign $\\rho(x,y) = 0$ whenever either denominator is $0$.\n- Your program should produce a single line of output containing the results for the four test cases as a comma-separated list of the four $p$-values, rounded to $6$ decimal places, enclosed in square brackets, in the order of the cases given (for example, $[0.123456,0.654321,0.000001,1.000000]$).", "solution": "The problem is valid. It presents a clear, scientifically grounded, and well-posed task in computational statistics, specifically within the domain of radiogenomics. The provided methodology—a permutation test to assess the significance of the maximal Pearson correlation—is a standard and robust technique for handling multiple comparisons in bioinformatics. All necessary data, parameters, and boundary conditions are explicitly defined, permitting a unique and verifiable solution.\n\nThe solution proceeds by implementing the specified permutation testing procedure for each test case. The core of this procedure is based on the principle of exchangeability under the null hypothesis ($H_0$). $H_0$ posits that there is no association between the imaging features $X \\in \\mathbb{R}^{n \\times p}$ and the genomic measurement vector $y \\in \\mathbb{R}^{n}$. If $H_0$ is true, then the observed pairing of subjects' data is merely one of $n!$ equally likely arrangements. We can therefore simulate the distribution of any test statistic under $H_0$ by repeatedly shuffling the elements of $y$ and recomputing the statistic.\n\nThe chosen test statistic is the maximal absolute Pearson correlation coefficient across all $p$ features:\n$$\nT = \\max_{1 \\le j \\le p} \\left|\\rho\\left(X_{\\cdot j}, y\\right)\\right|\n$$\nwhere $X_{\\cdot j}$ is the $j$-th column of $X$. This statistic captures the strongest linear association present in the data, regardless of direction (positive or negative), and its distribution under permutation provides a basis for a single hypothesis test that corrects for the multiple comparisons across $p$ features. The Pearson correlation coefficient, $\\rho(x,y)$, is defined as:\n$$\n\\rho(x,y) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n$$\nAs specified, if either input vector has zero variance (i.e., its corresponding term in the denominator is zero), $\\rho(x,y)$ is defined to be $0$.\n\nThe algorithmic implementation follows these steps for each test case:\n\n1.  **Compute the Observed Statistic ($T_{\\mathrm{obs}}$)**: The first step is to calculate the test statistic on the original, unpermuted data. For efficiency, the Pearson correlation coefficients for all $p$ feature columns against the vector $y$ are computed in a vectorized manner. The feature matrix $X$ and the vector $y$ are centered by subtracting their respective means. The numerators of the correlation formula for all features can be computed simultaneously via a matrix-vector product between the transposed centered feature matrix and the centered genomic vector. The denominators are also computed vectorially. A check for zero-variance columns in $X$ or a zero-variance $y$ vector is performed to apply the $\\rho=0$ rule. The observed statistic $T_{\\mathrm{obs}}$ is then the maximum of the absolute values of these computed correlations.\n\n2.  **Generate the Null Distribution**: A loop is executed $M$ times to approximate the distribution of the test statistic under $H_0$. In each iteration $b \\in \\{1, 2, \\ldots, M\\}$:\n    a. A new genomic vector, $y^{(b)}$, is generated by randomly permuting the elements of the original vector $y$. A pseudorandom number generator is initialized with the specified seed $s$ to ensure reproducibility.\n    b. The test statistic $T^{(b)} = \\max_{1 \\le j \\le p} |\\rho(X_{\\cdot j}, y^{(b)})|$ is computed for this permuted dataset, using the same vectorized procedure as for $T_{\\mathrm{obs}}$.\n\n3.  **Calculate the Empirical p-value**: The empirical $p$-value is the estimated probability of observing a test statistic at least as extreme as $T_{\\mathrm{obs}}$ under the null hypothesis. It is calculated by comparing $T_{\\mathrm{obs}}$ to the collection of $M$ statistics from the null distribution. The count of permutations where $T^{(b)} \\ge T_{\\mathrm{obs}}$ is tallied. The final $p$-value is computed using the \"plus-one\" correction:\n    $$\n    p = \\frac{1 + \\sum_{b=1}^{M} \\mathbf{1}\\left\\{T^{(b)} \\ge T_{\\mathrm{obs}}\\right\\}}{1 + M}\n    $$\n    where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. This correction prevents a $p$-value of $0$ and provides a more conservative and stable estimate.\n\nThis entire procedure is applied to each of the four test cases, which have been designed to test the correctness of the implementation on a standard case, a case with a zero-variance feature, a case with tied maximal correlations, and a boundary case with a zero-variance genomic vector.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the radiogenomics permutation testing problem for the given test suite.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path)\n        (\n            np.array([\n                [1.0, 0.2, 2.0, 0.5, -0.3], [0.5, -0.1, 1.5, -0.6, 0.1],\n                [-0.2, 0.3, -1.0, 0.4, -0.4], [0.4, 0.1, 1.2, 0.7, 0.2],\n                [0.0, -0.3, -0.5, -0.5, 0.0], [0.9, 0.4, 1.8, 0.9, -0.1],\n                [-0.5, -0.2, -1.3, -0.8, 0.3], [-0.1, 0.0, -0.2, 0.1, 0.5]\n            ]),\n            np.array([0.7, -1.2, 0.3, 0.5, -0.4, 1.1, -0.8, 0.0]),\n            1000, 42\n        ),\n        # Case 2 (zero-variance feature present)\n        (\n            np.array([\n                [2.0, 0.0, -0.5, 0.6], [1.8, 0.0, -0.2, -0.7],\n                [-1.6, 0.0, 0.0, 0.8], [1.4, 0.0, 0.1, -0.9],\n                [-1.2, 0.0, 0.3, 1.0], [1.0, 0.0, 0.4, -1.1]\n            ]),\n            np.array([1.0, 0.9, -0.8, 0.7, -0.6, 0.5]),\n            500, 123\n        ),\n        # Case 3 (tie in maximal feature correlation)\n        (\n            np.array([\n                [1.0, 1.0, 0.5], [2.0, 2.0, -0.2], [3.0, 3.0, 0.1],\n                [4.0, 4.0, -0.3], [-0.5, -0.5, 0.2], [-1.0, -1.0, -0.1],\n                [-1.5, -1.5, 0.0]\n            ]),\n            np.array([0.2, 0.4, 0.6, 0.8, -0.1, -0.3, -0.5]),\n            400, 2023\n        ),\n        # Case 4 (constant genomic vector boundary)\n        (\n            np.array([\n                [0.3, -0.1, 0.5], [-0.2, 0.2, 0.5], [0.1, -0.3, 0.5],\n                [0.0, 0.4, 0.5], [0.2, -0.5, 0.5]\n            ]),\n            np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n            200, 7\n        )\n    ]\n    \n    def pearson_corr_matrix(X, y):\n        \"\"\"\n        Computes Pearson correlation between each column of X and vector y.\n        Handles zero-variance cases according to the problem specification.\n        \"\"\"\n        n, p = X.shape\n        \n        # Center y\n        y_mean = np.mean(y)\n        y_centered = y - y_mean\n        ss_y = np.sum(y_centered**2)\n\n        # If y has zero variance, all correlations are 0\n        if ss_y == 0:\n            return np.zeros(p)\n\n        # Center X column-wise\n        X_mean = np.mean(X, axis=0)\n        X_centered = X - X_mean\n        ss_X = np.sum(X_centered**2, axis=0)\n        \n        # Numerator: dot product of centered vectors (covariance term)\n        # transpose X_centered (p, n) and dot with y_centered (n,) - (p,)\n        numerator = X_centered.T @ y_centered\n\n        # Denominator: product of standard deviations\n        denominator = np.sqrt(ss_X * ss_y)\n\n        # Initialize correlations to 0, which is the default for zero-variance cases\n        correlations = np.zeros(p)\n        \n        # Create a mask for columns with non-zero variance\n        non_zero_denom_mask = denominator  0\n        \n        # Calculate correlation only for non-zero variance columns to avoid division by zero\n        correlations[non_zero_denom_mask] = numerator[non_zero_denom_mask] / denominator[non_zero_denom_mask]\n        \n        return correlations\n\n    results = []\n    for X, y, M, s in test_cases:\n        # Step 1: Compute the observed maximal association statistic\n        obs_corrs = pearson_corr_matrix(X, y)\n        T_obs = np.max(np.abs(obs_corrs))\n\n        # Step 2: Generate permutation distribution\n        rng = np.random.default_rng(seed=s)\n        num_exceeding = 0\n        \n        for _ in range(M):\n            # Permute the genomic vector y\n            y_permuted = rng.permutation(y)\n            \n            # Compute maximal correlation for the permuted data\n            perm_corrs = pearson_corr_matrix(X, y_permuted)\n            T_perm = np.max(np.abs(perm_corrs))\n            \n            # Check if the permuted statistic is greater than or equal to the observed one\n            if T_perm = T_obs:\n                num_exceeding += 1\n                \n        # Step 3: Compute the p-value with the +1 correction\n        p_value = (1 + num_exceeding) / (1 + M)\n        results.append(f\"{p_value:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4557637"}]}