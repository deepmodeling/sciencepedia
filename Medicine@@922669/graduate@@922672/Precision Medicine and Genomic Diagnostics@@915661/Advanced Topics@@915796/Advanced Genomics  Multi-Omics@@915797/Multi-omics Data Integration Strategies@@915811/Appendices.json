{"hands_on_practices": [{"introduction": "A robust multi-omics analysis begins not with complex integration algorithms, but with rigorous quality control. The principle of \"garbage in, garbage out\" is especially pertinent when dealing with large, heterogeneous datasets where a single poor-quality sample can skew results and lead to spurious discoveries. This first practice exercise [@problem_id:5033985] guides you through building an automated quality control pipeline to systematically flag outlier samples by integrating signals from both the high-dimensional omics data itself and associated quality metrics. You will implement a strategy that combines Principal Component Analysis (PCA) to detect samples with unusual patterns in a given modality, and robust statistical methods to identify samples with extreme quality metric values, providing a foundational step for any reliable integration study.", "problem": "You are tasked with constructing a quality control pipeline for multi-omics integration in translational medicine that detects outlier samples across omics using Principal Component Analysis (PCA) leverage scores and Median Absolute Deviation (MAD) thresholds applied to key quality metrics. The pipeline must be implemented as a complete, runnable program that takes no input and produces a single line of output. The program must compute outlier indices for a provided test suite.\n\nFoundational base and definitions to use:\n- Let an omics data matrix be denoted by $X \\in \\mathbb{R}^{n \\times p}$ with $n$ samples (rows) and $p$ features (columns). Each column of $X$ must be standardized to zero mean and unit variance before subsequent computations.\n- Principal Component Analysis (PCA) can be defined via the Singular Value Decomposition (SVD), $X = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{n \\times r}$ and $V \\in \\mathbb{R}^{p \\times r}$ have orthonormal columns, $\\Sigma \\in \\mathbb{R}^{r \\times r}$ is diagonal with nonnegative singular values, and $r = \\operatorname{rank}(X)$.\n- For a chosen number of components $k$, with $k \\leq p$, define the projection onto the top-$k$ left singular subspace by $U_k \\in \\mathbb{R}^{n \\times k}$, the first $k$ columns of $U$ if $r \\geq k$, and by $U_k \\in \\mathbb{R}^{n \\times r}$ otherwise. The PCA leverage for sample $i$ is the diagonal element of the projection matrix onto this subspace, $h_i = \\sum_{j=1}^{\\min(k, r)} U_{ij}^2$.\n- The projection matrix $H_k = U_k U_k^\\top$ is symmetric and idempotent, and satisfies $\\operatorname{trace}(H_k) = \\min(k, r)$. Therefore, the mean leverage equals $\\operatorname{trace}(H_k)/n = \\min(k, r)/n$. A leverage-based outlier can be defined by comparing $h_i$ to a threshold $\\alpha \\cdot \\min(k, r)/n$, with amplification factor $\\alpha > 1$.\n- For a quality metric vector $q \\in \\mathbb{R}^n$, define $\\operatorname{median}(q)$, the median absolute deviations $d_i = |q_i - \\operatorname{median}(q)|$, $\\operatorname{MAD}(q) = \\operatorname{median}(d)$, and the robust scaled deviation $z_i = d_i / (c \\cdot \\operatorname{MAD}(q))$ with $c = 1.4826$ (the consistency constant under normality). A metric-based outlier is any $i$ for which $z_i$ exceeds a threshold $t$.\n- Integrated across omics: given $O$ omics matrices $\\{X^{(o)}\\}_{o=1}^O$, define for each sample $i$ the count of leverage exceedances across omics, $C^{\\text{lev}}_i$, and the count of metric exceedances across the provided metrics, $C^{\\text{met}}_i$. A sample $i$ is flagged as an integrated outlier if either $C^{\\text{lev}}_i \\geq \\theta$ or $C^{\\text{met}}_i \\geq \\phi$, for given integer thresholds $\\theta$ and $\\phi$.\n\nImplementation requirements:\n- Standardize each omics matrix column-wise to zero mean and unit variance; if a column has zero variance, treat its standardized values as all zeros.\n- Compute PCA leverage using the SVD-based definition above with $U_k$ and $h_i$.\n- Compute quality metric outliers using the MAD-based robust deviations. If $\\operatorname{MAD}(q) = 0$, define no outliers for that metric unless any $q_i \\neq \\operatorname{median}(q)$; under that latter condition, treat those with $q_i \\neq \\operatorname{median}(q)$ as outliers.\n- Use $0$-based indexing for reporting outlier sample indices.\n\nTest suite:\n- Test Case $1$ (general multi-omics with a clear outlier in one omic and metrics):\n    - Number of samples: $n = 6$; number of omics: $O = 3$; components $k = 2$; leverage amplification $\\alpha = 2.0$; leverage count threshold $\\theta = 1$; metric threshold $t = 3.0$; metric count threshold $\\phi = 1$.\n    - Genomics matrix $X^{(1)}$ ($6 \\times 4$):\n      $\n      \\begin{bmatrix}\n      0.5 & 1.0 & -0.3 & 0.2 \\\\\n      0.6 & 0.8 & -0.1 & 0.0 \\\\\n      0.4 & 1.1 & -0.2 & 0.1 \\\\\n      0.5 & 0.9 & -0.3 & 0.2 \\\\\n      0.6 & 1.0 & -0.2 & 0.3 \\\\\n      0.5 & 0.95 & -0.25 & 0.15\n      \\end{bmatrix}\n      $\n    - Transcriptomics matrix $X^{(2)}$ ($6 \\times 5$):\n      $\n      \\begin{bmatrix}\n      10 & 12 & 9 & 11 & 10 \\\\\n      11 & 12 & 9 & 10 & 11 \\\\\n      10 & 11 & 10 & 11 & 10 \\\\\n      10 & 12 & 9 & 11 & 10 \\\\\n      11 & 11 & 9 & 10 & 12 \\\\\n      10 & 12 & 10 & 11 & 11\n      \\end{bmatrix}\n      $\n    - Proteomics matrix $X^{(3)}$ ($6 \\times 3$):\n      $\n      \\begin{bmatrix}\n      0 & 0 & 1 \\\\\n      0 & 0 & 1 \\\\\n      0 & 0 & 1 \\\\\n      10 & 10 & 1 \\\\\n      0 & 0 & 1 \\\\\n      0 & 0 & 1\n      \\end{bmatrix}\n      $\n    - Quality metrics:\n      - Metric 1 (e.g., RNA quality): $[8.0, 8.2, 7.9, 3.0, 8.1, 8.0]$\n      - Metric 2 (e.g., library size): $[5,000,000, 5,100,000, 4,900,000, 1,000,000, 5,200,000, 5,000,000]$\n    - Expected behavior: a single integrated outlier corresponding to the outlying proteomics sample and metrics.\n- Test Case $2$ (boundary condition: zero-variance features and no metric deviations):\n    - $n = 4$, $O = 2$, $k = 2$, $\\alpha = 2.0$, $\\theta = 1$, $t = 3.0$, $\\phi = 1$.\n    - Omics matrices $X^{(1)} = X^{(2)} = \\begin{bmatrix}1 & 2 & 3 \\\\ 1 & 2 & 3 \\\\ 1 & 2 & 3 \\\\ 1 & 2 & 3 \\end{bmatrix}$\n    - Quality metrics:\n      - Metric 1: $[10, 10, 10, 10]$\n      - Metric 2: $[100, 100, 100, 100]$\n    - Expected behavior: no integrated outliers.\n- Test Case $3$ (edge case: outliers only via metrics):\n    - $n = 5$, $O = 2$, $k = 2$, $\\alpha = 2.0$, $\\theta = 2$, $t = 3.0$, $\\phi = 2$.\n    - Omics matrices:\n      - $X^{(1)} = \\begin{bmatrix} 1 & 2 \\\\ 1 & 2 \\\\ 1.1 & 1.9 \\\\ 0.9 & 2.1 \\\\ 1 & 2 \\end{bmatrix}$\n      - $X^{(2)} = \\begin{bmatrix} 5 & 5 \\\\ 5 & 5 \\\\ 5.1 & 4.9 \\\\ 4.9 & 5.1 \\\\ 5 & 5 \\end{bmatrix}$\n    - Quality metrics:\n      - Metric 1: $[0.1, 5.0, 0.2, 0.1, 0.0]$\n      - Metric 2: $[100, 1000, 105, 98, 97]$\n    - Expected behavior: a single integrated outlier via metrics (second sample).\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each result must be a list of $0$-based indices of samples flagged as integrated outliers for the corresponding test case. For example, an output of the form $[[i_1, i_2], [], [j]]$ is valid. The final output must be printed exactly as a single Python list literal of lists on one line, with no additional text.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in established statistical methods (Principal Component Analysis leverage, Median Absolute Deviation), well-posed with a deterministic algorithm, objective in its language, and complete in its specification of data and parameters. The problem is a formalizable and relevant task in the quality control of multi-omics data within translational medicine. There are no contradictions, no violations of scientific principles, and no ambiguities that would prevent a unique, verifiable solution.\n\nHere follows a complete, reasoned solution.\n\n### **Algorithmic Procedure**\n\nThe quality control pipeline identifies outlier samples by integrating two distinct types of statistical measures across multiple omics datasets and quality metric vectors: PCA-based leverage scores and MAD-based robust deviations. The procedure for a given test case is as follows.\n\n**1. Initialization**\nLet $n$ be the number of samples. Two integer vectors are initialized to store outlier counts for each sample:\n- $C^{\\text{lev}} \\in \\mathbb{Z}^n$, initialized to zeros, to count leverage-based outlier flags.\n- $C^{\\text{met}} \\in \\mathbb{Z}^n$, initialized to zeros, to count metric-based outlier flags.\n\n**2. Leverage-Based Outlier Detection (per omics dataset)**\nFor each of the $O$ omics data matrices $X^{(o)} \\in \\mathbb{R}^{n \\times p}$, where $o \\in \\{1, \\dots, O\\}$:\n\n**a. Standardization:**\nEach column $j$ of $X^{(o)}$ is standardized to have a mean of $0$ and a standard deviation of $1$. Let $X^{(o)}_{:,j}$ be the $j$-th column. Its mean $\\mu_j$ and standard deviation $\\sigma_j$ are computed. The standardized column $X'^{(o)}_{:,j}$ is given by:\n$$\nX'^{(o)}_{i,j} = \\begin{cases} (X^{(o)}_{i,j} - \\mu_j) / \\sigma_j & \\text{if } \\sigma_j > 0 \\\\ 0 & \\text{if } \\sigma_j = 0 \\end{cases}\n$$\nThis creates the standardized matrix $X'^{(o)}$.\n\n**b. Singular Value Decomposition (SVD):**\nThe SVD of the standardized matrix is computed: $X'^{(o)} = U^{(o)} \\Sigma^{(o)} (V^{(o)})^\\top$. Here, $U^{(o)} \\in \\mathbb{R}^{n \\times n}$ is the matrix of left singular vectors.\n\n**c. Rank and Component Selection:**\nThe rank of the matrix, $r^{(o)} = \\operatorname{rank}(X'^{(o)})$, is determined. This corresponds to the number of non-zero singular values. The number of principal components to consider is $k' = \\min(k, r^{(o)})$, where $k$ is the user-specified number of components. We define $U_{k'}^{(o)}$ as the matrix containing the first $k'$ columns of $U^{(o)}$.\n\n**d. Leverage Calculation:**\nThe leverage score $h_i^{(o)}$ for each sample $i \\in \\{1, \\dots, n\\}$ is the sum of the squares of the elements in the $i$-th row of $U_{k'}^{(o)}$:\n$$\nh_i^{(o)} = \\sum_{j=1}^{k'} (U^{(o)}_{ij})^2\n$$\n\n**e. Outlier Flagging:**\nA leverage threshold $\\tau_{\\text{lev}}^{(o)}$ is calculated based on the mean leverage:\n$$\n\\tau_{\\text{lev}}^{(o)} = \\frac{\\alpha \\cdot k'}{n}\n$$\nwhere $\\alpha$ is the given amplification factor. If a sample's leverage $h_i^{(o)}$ exceeds this threshold, its leverage outlier count $C^{\\text{lev}}_i$ is incremented:\n$$\n\\text{if } h_i^{(o)} > \\tau_{\\text{lev}}^{(o)}, \\text{ then } C^{\\text{lev}}_i \\leftarrow C^{\\text{lev}}_i + 1\n$$\n\n**3. Metric-Based Outlier Detection (per quality metric)**\nFor each provided quality metric vector $q \\in \\mathbb{R}^n$:\n\n**a. Median and Median Absolute Deviation (MAD):**\nThe median of the metric, $m = \\operatorname{median}(q)$, is calculated. The absolute deviations from the median, $d_i = |q_i - m|$, are computed for all samples. The MAD is the median of these absolute deviations: $\\operatorname{MAD}(q) = \\operatorname{median}(d)$.\n\n**b. Outlier Flagging:**\nThe method for flagging outliers depends on the value of $\\operatorname{MAD}(q)$:\n- If $\\operatorname{MAD}(q) > 0$: The robust scaled deviation $z_i$ is calculated for each sample:\n  $$\n  z_i = \\frac{d_i}{c \\cdot \\operatorname{MAD}(q)} = \\frac{|q_i - \\operatorname{median}(q)|}{1.4826 \\cdot \\operatorname{MAD}(q)}\n  $$\n  If $z_i$ exceeds the metric threshold $t$, the sample's metric outlier count $C^{\\text{met}}_i$ is incremented.\n- If $\\operatorname{MAD}(q) = 0$: Any sample $i$ for which the metric value $q_i$ is not equal to the median $m$ is considered an outlier. For each such sample, $C^{\\text{met}}_i$ is incremented. If all $q_i$ are equal to the median, no outliers are flagged for this metric.\n\n**4. Integrated Outlier Identification**\nAfter processing all omics matrices and quality metrics, the final outlier status of each sample $i$ is determined by comparing its accumulated counts $C^{\\text{lev}}_i$ and $C^{\\text{met}}_i$ with the respective integer thresholds $\\theta$ and $\\phi$:\n$$\n\\text{Sample } i \\text{ is an integrated outlier if } (C^{\\text{lev}}_i \\geq \\theta) \\lor (C^{\\text{met}}_i \\geq \\phi)\n$$\nThe 0-based indices of all samples satisfying this condition are collected as the final result for the test case.\n\n---\n### **Application to Test Cases**\n\n**Test Case 1**\n- Parameters: $n=6, O=3, k=2, \\alpha=2.0, \\theta=1, t=3.0, \\phi=1$.\n- **Leverage:**\n  - For $X^{(1)}$ and $X^{(2)}$, the data points are relatively homogeneous. After standardization, the SVD yields leverage scores where no single sample dominates. The ranks are $r^{(1)}=4$ and $r^{(2)}=5$. Then $k' = \\min(2,4)=2$ and $k'=\\min(2,5)=2$. The thresholds are $\\tau_{\\text{lev}}^{(1)} = 2.0 \\cdot 2 / 6 \\approx 0.667$ and $\\tau_{\\text{lev}}^{(2)} = 2.0 \\cdot 2 / 6 \\approx 0.667$. All calculated $h_i^{(1)}$ and $h_i^{(2)}$ values are found to be below this threshold.\n  - For $X^{(3)}$, the 4th sample (index $3$) is anomalous: $[10, 10, 1]$. After standardization, the matrix rank is $r^{(3)}=1$. Thus, $k'=\\min(2,1)=1$. The leverage threshold is $\\tau_{\\text{lev}}^{(3)} = 2.0 \\cdot 1 / 6 \\approx 0.333$. The leverage scores are approximately $h^{(3)} = [0.038, 0.038, 0.038, 0.769, 0.038, 0.038]$. Only $h_3^{(3)} \\approx 0.769 > 0.333$.\n  - The leverage counts are $C^{\\text{lev}} = [0, 0, 0, 1, 0, 0]$.\n- **Metrics:**\n  - Metric 1: $q = [8.0, 8.2, 7.9, 3.0, 8.1, 8.0]$. The median is $8.0$. Deviations are $[0.0, 0.2, 0.1, 5.0, 0.1, 0.0]$. The MAD is $0.1$. The z-score for sample 4 (index 3) is $z_3 = 5.0 / (1.4826 \\cdot 0.1) \\approx 33.72 > 3.0$.\n  - Metric 2: $q = [5\\text{e}6, 5.1\\text{e}6, 4.9\\text{e}6, 1\\text{e}6, 5.2\\text{e}6, 5\\text{e}6]$. The median is $5\\text{e}6$. MAD is $1\\text{e}5$. The z-score for sample 4 is $z_3 = 4\\text{e}6 / (1.4826 \\cdot 1\\text{e}5) \\approx 26.98 > 3.0$.\n  - The metric counts are $C^{\\text{met}} = [0, 0, 0, 2, 0, 0]$.\n- **Integration:**\n  - For sample 4 (index 3): $C^{\\text{lev}}_3=1 \\geq \\theta=1$ and $C^{\\text{met}}_3=2 \\geq \\phi=1$. The sample is an outlier.\n  - For all other samples $i \\neq 3$, $C^{\\text{lev}}_i=0 < 1$ and $C^{\\text{met}}_i=0 < 1$. They are not outliers.\n- **Result:** `[3]`\n\n**Test Case 2**\n- Parameters: $n=4, O=2, k=2, \\alpha=2.0, \\theta=1, t=3.0, \\phi=1$.\n- **Leverage:**\n  - Both $X^{(1)}$ and $X^{(2)}$ consist of identical rows. Each column has a standard deviation of $0$.\n  - Per the rule, both standardized matrices $X'^{(1)}$ and $X'^{(2)}$ are zero matrices.\n  - The rank of a zero matrix is $r=0$. Thus $k'=\\min(2, 0)=0$.\n  - The leverage scores $h_i$ are all $0$, and the threshold $\\tau_{\\text{lev}}$ is also $0$. The condition $h_i > \\tau_{\\text{lev}}$ is never met.\n  - The leverage counts are $C^{\\text{lev}} = [0, 0, 0, 0]$.\n- **Metrics:**\n  - Both metric vectors, $[10, 10, 10, 10]$ and $[100, 100, 100, 100]$, consist of identical values.\n  - For both, the median is the constant value, all deviations are $0$, and thus the MAD is $0$.\n  - The special case for $\\operatorname{MAD}=0$ applies. Since no $q_i$ differs from the median, no outliers are flagged for this metric.\n  - The metric counts are $C^{\\text{met}} = [0, 0, 0, 0]$.\n- **Integration:**\n  - For all samples, $C^{\\text{lev}}_i=0$ and $C^{\\text{met}}_i=0$. The conditions $C^{\\text{lev}}_i \\geq 1$ or $C^{\\text{met}}_i \\geq 1$ are never met.\n- **Result:** `[]`\n\n**Test Case 3**\n- Parameters: $n=5, O=2, k=2, \\alpha=2.0, \\theta=2, t=3.0, \\phi=2$.\n- **Leverage:**\n  - The matrices $X^{(1)}$ and $X^{(2)}$ consist of very similar samples.\n  - For $X^{(1)}$, $r^{(1)}=2, k'=2$. $\\tau_{\\text{lev}}^{(1)} = 2.0 \\cdot 2 / 5 = 0.8$. All $h_i^{(1)}$ are below this threshold.\n  - For $X^{(2)}$, $r^{(2)}=2, k'=2$. $\\tau_{\\text{lev}}^{(2)} = 2.0 \\cdot 2 / 5 = 0.8$. All $h_i^{(2)}$ are below this threshold.\n  - No sample is flagged as a leverage outlier in any omic. The leverage counts are $C^{\\text{lev}} = [0, 0, 0, 0, 0]$.\n- **Metrics:**\n  - Metric 1: $q = [0.1, 5.0, 0.2, 0.1, 0.0]$. Median is $0.1$. MAD is $0.1$. The z-score for sample 2 (index 1) is $z_1 = 4.9 / (1.4826 \\cdot 0.1) \\approx 33.05 > 3.0$. Sample 2 is an outlier.\n  - Metric 2: $q = [100, 1000, 105, 98, 97]$. Median is $100$. MAD is $3$. The z-score for sample 2 is $z_1 = 900 / (1.4826 \\cdot 3) \\approx 202.41 > 3.0$. Sample 2 is an outlier.\n  - The metric counts are $C^{\\text{met}} = [0, 2, 0, 0, 0]$.\n- **Integration:**\n  - The thresholds are high: $\\theta=2, \\phi=2$.\n  - For sample 2 (index 1): $C^{\\text{lev}}_1=0 < \\theta=2$, but $C^{\\text{met}}_1=2 \\geq \\phi=2$. The sample is an outlier.\n  - For all other samples, counts are too low.\n- **Result:** `[1]`", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef solve():\n    \"\"\"\n    Main function to run the multi-omics outlier detection pipeline on a predefined test suite.\n    \"\"\"\n    \n    test_cases = [\n        # Test Case 1\n        {\n            \"params\": {\"n\": 6, \"O\": 3, \"k\": 2, \"alpha\": 2.0, \"theta\": 1, \"t\": 3.0, \"phi\": 1},\n            \"omics_data\": [\n                np.array([\n                    [0.5, 1.0, -0.3, 0.2],\n                    [0.6, 0.8, -0.1, 0.0],\n                    [0.4, 1.1, -0.2, 0.1],\n                    [0.5, 0.9, -0.3, 0.2],\n                    [0.6, 1.0, -0.2, 0.3],\n                    [0.5, 0.95, -0.25, 0.15]\n                ]),\n                np.array([\n                    [10, 12, 9, 11, 10],\n                    [11, 12, 9, 10, 11],\n                    [10, 11, 10, 11, 10],\n                    [10, 12, 9, 11, 10],\n                    [11, 11, 9, 10, 12],\n                    [10, 12, 10, 11, 11]\n                ]),\n                np.array([\n                    [0, 0, 1],\n                    [0, 0, 1],\n                    [0, 0, 1],\n                    [10, 10, 1],\n                    [0, 0, 1],\n                    [0, 0, 1]\n                ])\n            ],\n            \"quality_metrics\": [\n                np.array([8.0, 8.2, 7.9, 3.0, 8.1, 8.0]),\n                np.array([5_000_000, 5_100_000, 4_900_000, 1_000_000, 5_200_000, 5_000_000])\n            ]\n        },\n        # Test Case 2\n        {\n            \"params\": {\"n\": 4, \"O\": 2, \"k\": 2, \"alpha\": 2.0, \"theta\": 1, \"t\": 3.0, \"phi\": 1},\n            \"omics_data\": [\n                np.array([[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]]),\n                np.array([[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]])\n            ],\n            \"quality_metrics\": [\n                np.array([10, 10, 10, 10]),\n                np.array([100, 100, 100, 100])\n            ]\n        },\n        # Test Case 3\n        {\n            \"params\": {\"n\": 5, \"O\": 2, \"k\": 2, \"alpha\": 2.0, \"theta\": 2, \"t\": 3.0, \"phi\": 2},\n            \"omics_data\": [\n                np.array([[1, 2], [1, 2], [1.1, 1.9], [0.9, 2.1], [1, 2]]),\n                np.array([[5, 5], [5, 5], [5.1, 4.9], [4.9, 5.1], [5, 5]])\n            ],\n            \"quality_metrics\": [\n                np.array([0.1, 5.0, 0.2, 0.1, 0.0]),\n                np.array([100, 1000, 105, 98, 97])\n            ]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        params = case[\"params\"]\n        n = params[\"n\"]\n        k = params[\"k\"]\n        alpha = params[\"alpha\"]\n        theta = params[\"theta\"]\n        t = params[\"t\"]\n        phi = params[\"phi\"]\n        \n        c_lev = np.zeros(n, dtype=int)\n        c_met = np.zeros(n, dtype=int)\n\n        # Leverage-based outlier detection\n        for X in case[\"omics_data\"]:\n            # Standardize matrix\n            mean = np.mean(X, axis=0)\n            std = np.std(X, axis=0)\n            X_std = np.zeros_like(X, dtype=float)\n            for j in range(X.shape[1]):\n                if std[j] > 1e-9: # Use tolerance for float comparison\n                    X_std[:, j] = (X[:, j] - mean[j]) / std[j]\n            \n            # SVD and rank\n            try:\n                U, s, Vh = svd(X_std, full_matrices=False)\n                # Rank is number of singular values greater than a tolerance\n                rank = np.linalg.matrix_rank(X_std)\n            except np.linalg.LinAlgError:\n                # SVD can fail on some ill-conditioned matrices, though unlikely here\n                continue\n\n            num_components = min(k, rank)\n            if num_components == 0:\n                continue\n\n            # Leverage calculation\n            U_k = U[:, :num_components]\n            h = np.sum(U_k**2, axis=1)\n            \n            # Leverage outlier flagging\n            leverage_threshold = alpha * num_components / n\n            c_lev[h > leverage_threshold] += 1\n\n        # Metric-based outlier detection\n        c_consistency = 1.4826\n        for q in case[\"quality_metrics\"]:\n            median_q = np.median(q)\n            deviations = np.abs(q - median_q)\n            mad_q = np.median(deviations)\n\n            if mad_q > 1e-9: # Use tolerance for float comparison\n                z_scores = deviations / (c_consistency * mad_q)\n                c_met[z_scores > t] += 1\n            else:\n                # Special case for MAD == 0\n                c_met[q != median_q] += 1\n        \n        # Integrated outlier identification\n        integrated_outliers = np.where((c_lev >= theta) | (c_met >= phi))[0].tolist()\n        results.append(integrated_outliers)\n\n    # Final print statement must produce the exact single-line format.\n    # The format `f\"[{','.join(map(str, results))}]\"` is explicitly requested,\n    # resulting in a string like '[[3],[],[1]]'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "5033985"}, {"introduction": "Once we are confident in the quality of our samples, the next critical challenge is data harmonization. Different omics technologies produce data with fundamentally distinct statistical properties—for example, discrete counts from RNA-sequencing versus continuous intensities from proteomics. This practice [@problem_id:4362390] delves into the statistical theory behind making these disparate data types \"speak the same language.\" By starting from a foundational model of count data, you will derive the mean-variance relationship that makes RNA-seq data challenging to work with, justify the need for normalization, and derive a variance-stabilizing transformation that makes the data more amenable to the assumptions of many downstream integration and machine learning models.", "problem": "A translational genomics group plans to integrate Ribonucleic Acid sequencing (RNA-seq) gene-level counts with proteomics intensities for a cohort study in precision oncology. They will compute joint latent factors across concatenated features to discover biomarkers. The RNA-seq counts are known to be affected by sample-specific library sizes and biological overdispersion, and the proteomics intensities are approximately log-normal after instrument-level normalization. To avoid bias from scale and mean-dependent variance, the group considers normalization by sample-specific size factors (such as those estimated by Trimmed Mean of M-values (TMM) or by Differential Expression sequencing 2 (DESeq2) size factors), followed by a variance stabilizing transformation (VST).\n\nStarting from the following foundational base, derive and quantify the necessity of normalization and VST prior to concatenation:\n\n- RNA-seq counts arise from a Poisson sampling process with rate $\\lambda_{ig}$ for sample $i$ and gene $g$, with $\\lambda_{ig} = s_i q_g$, where $s_i$ is the sample-specific library size (a multiplicative size factor) and $q_g$ is the true abundance for gene $g$ in the biological population.\n- Biological and technical overdispersion across replicate realizations can be modeled by assuming the Poisson rate $\\lambda_{ig}$ varies according to a Gamma distribution across replicates for fixed $i$ and $g$, turning the marginal distribution of counts $Y_{ig}$ into a Negative Binomial-like model.\n- Proteomics intensities for features $p$ are approximately log-normal, so that after a logarithmic transformation their variance becomes approximately independent of their mean.\n\nTasks:\n\n1. From the Poisson-Gamma mixture and using the well-tested identity $\\operatorname{Var}(Y)=\\operatorname{E}[\\operatorname{Var}(Y\\mid \\lambda)]+\\operatorname{Var}(\\operatorname{E}[Y\\mid \\lambda])$, derive the mean-variance relationship $v(\\mu)$ for RNA-seq counts $Y_{ig}$ as a function of their mean $\\mu_{ig} = s_i q_g$ and an overdispersion parameter $\\alpha$ that summarizes the Gamma variability. Do not assume any target form; derive it explicitly from the mixture.\n2. Explain, using the derived $v(\\mu)$, why normalization by the size factor $s_i$ (i.e., analyzing $\\tilde{Y}_{ig}=Y_{ig}/s_i$) is necessary to make features comparable across samples, and why heteroscedasticity across genes persists after normalization.\n3. Using the general definition of a variance stabilizing transformation $g$ for a one-parameter mean-variance family, derive an explicit closed-form $g(y;\\alpha)$ such that the delta-method approximation $\\operatorname{Var}(g(Y_{ig})) \\approx \\left(g'(\\mu_{ig})\\right)^{2} v(\\mu_{ig})$ becomes approximately constant across $\\mu_{ig}$. Impose $g(0)=0$ to fix the additive constant.\n4. Consider a single sample $i$ with size factor $s_i = 3$ and two genes with true abundances $q_{1}=20$ and $q_{2}=2000$. Let the dispersion parameter be $\\alpha = 0.02$. Define the heteroscedasticity ratio before transformation as $R_{\\text{before}} = \\frac{v(\\mu_{i2})}{v(\\mu_{i1})}$ and the heteroscedasticity ratio after applying the derived $g$ as $R_{\\text{after}} = \\frac{\\operatorname{Var}(g(Y_{i2}))}{\\operatorname{Var}(g(Y_{i1}))}$ under the delta-method approximation. Compute $R_{\\text{before}}$ and $R_{\\text{after}}$, and report both quantities in the final answer as a single row matrix. No physical units are required.\n\nYour final answer must be a calculation and be presented as the two numbers for $R_{\\text{before}}$ and $R_{\\text{after}}$ in a single row matrix.", "solution": "The problem is valid as it is scientifically grounded in established statistical models for genomic data, is well-posed with sufficient information for a unique solution, and is expressed in objective, technical language.\n\nThe solution is organized into four parts, corresponding to the four tasks in the problem statement.\n\n**Task 1: Derivation of the Mean-Variance Relationship**\n\nWe are given that the RNA-seq count $Y_{ig}$ for sample $i$ and gene $g$, conditional on a rate parameter $\\lambda_{ig}$, follows a Poisson distribution:\n$$Y_{ig} \\mid \\lambda_{ig} \\sim \\text{Poisson}(\\lambda_{ig})$$\nThe properties of the Poisson distribution give us the conditional mean and variance:\n$$\\operatorname{E}[Y_{ig} \\mid \\lambda_{ig}] = \\lambda_{ig}$$\n$$\\operatorname{Var}(Y_{ig} \\mid \\lambda_{ig}) = \\lambda_{ig}$$\nThe rate parameter $\\lambda_{ig}$ itself is a random variable, assumed to follow a Gamma distribution to model overdispersion. We are asked to derive the marginal mean-variance relationship for $Y_{ig}$ using the Law of Total Variance:\n$$\\operatorname{Var}(Y_{ig}) = \\operatorname{E}[\\operatorname{Var}(Y_{ig} \\mid \\lambda_{ig})] + \\operatorname{Var}(\\operatorname{E}[Y_{ig} \\mid \\lambda_{ig}])$$\nSubstituting the conditional moments of the Poisson distribution, we get:\n$$\\operatorname{Var}(Y_{ig}) = \\operatorname{E}[\\lambda_{ig}] + \\operatorname{Var}(\\lambda_{ig})$$\nThe marginal mean of the counts, $\\mu_{ig}$, is found using the Law of Total Expectation:\n$$\\mu_{ig} = \\operatorname{E}[Y_{ig}] = \\operatorname{E}[\\operatorname{E}[Y_{ig} \\mid \\lambda_{ig}]] = \\operatorname{E}[\\lambda_{ig}]$$\nThe problem states that the variability of the Gamma-distributed rate is summarized by an overdispersion parameter $\\alpha$. In the standard parameterization for Negative Binomial models of count data (as used in DESeq2 and other common tools), the variance of the rate parameter is related to its mean by a quadratic term:\n$$\\operatorname{Var}(\\lambda_{ig}) = \\alpha (\\operatorname{E}[\\lambda_{ig}])^2$$\nSubstituting $\\operatorname{E}[\\lambda_{ig}] = \\mu_{ig}$, we have:\n$$\\operatorname{Var}(\\lambda_{ig}) = \\alpha \\mu_{ig}^2$$\nNow, we substitute $\\operatorname{E}[\\lambda_{ig}]$ and $\\operatorname{Var}(\\lambda_{ig})$ back into the expression for $\\operatorname{Var}(Y_{ig})$:\n$$\\operatorname{Var}(Y_{ig}) = \\mu_{ig} + \\alpha \\mu_{ig}^2$$\nThis gives the mean-variance relationship $v(\\mu)$ for the RNA-seq counts:\n$$v(\\mu_{ig}) = \\mu_{ig} + \\alpha \\mu_{ig}^2$$\nThis relationship shows that the variance of the counts is a quadratic function of the mean. The variance has two components: $\\mu_{ig}$, which corresponds to the Poisson \"shot noise\", and $\\alpha \\mu_{ig}^2$, which corresponds to the biological and technical overdispersion modeled by the Gamma distribution.\n\n**Task 2: Justification for Normalization and Explanation of Persistent Heteroscedasticity**\n\nThe mean of the raw count for gene $g$ in sample $i$ is given as $\\mu_{ig} = s_i q_g$, where $s_i$ is the sample-specific size factor (related to sequencing library depth) and $q_g$ is the true underlying abundance of the gene.\n\n**Necessity of Normalization:**\nThe mean count $\\mu_{ig}$ is a product of a technical factor $s_i$ and a biological factor $q_g$. If we were to compare the raw counts $Y_{ig}$ and $Y_{jg}$ for the same gene $g$ across two different samples $i$ and $j$, their expected values $\\mu_{ig} = s_i q_g$ and $\\mu_{jg} = s_j q_g$ would differ if $s_i \\neq s_j$, even if the biological state is identical. When concatenating features for downstream analyses like PCA or latent factor models, samples with larger size factors (i.e., higher sequencing depth) would have systematically larger count values across all genes. This technical artifact would dominate the analysis, obscuring the true biological variation of interest. To make the measurements comparable across samples, we must normalize for these size factors. A common approach is to analyze the normalized counts $\\tilde{Y}_{ig} = Y_{ig} / s_i$. The expectation of this normalized quantity is:\n$$\\operatorname{E}[\\tilde{Y}_{ig}] = \\operatorname{E}[Y_{ig} / s_i] = \\frac{\\mu_{ig}}{s_i} = \\frac{s_i q_g}{s_i} = q_g$$\nAfter normalization, the expected value of the feature for gene $g$ is $q_g$, which is independent of the sample-specific library size and reflects the true biological abundance. This makes the features comparable across different samples.\n\n**Persistence of Heteroscedasticity:**\nWhile normalization corrects the mean, it does not solve the problem of mean-dependent variance (heteroscedasticity). Let's compute the variance of the normalized counts, $\\tilde{Y}_{ig}$:\n$$\\operatorname{Var}(\\tilde{Y}_{ig}) = \\operatorname{Var}\\left(\\frac{Y_{ig}}{s_i}\\right) = \\frac{1}{s_i^2} \\operatorname{Var}(Y_{ig})$$\nUsing the mean-variance relationship $v(\\mu_{ig}) = \\mu_{ig} + \\alpha \\mu_{ig}^2$:\n$$\\operatorname{Var}(\\tilde{Y}_{ig}) = \\frac{1}{s_i^2} (\\mu_{ig} + \\alpha \\mu_{ig}^2)$$\nSubstituting $\\mu_{ig} = s_i q_g$:\n$$\\operatorname{Var}(\\tilde{Y}_{ig}) = \\frac{1}{s_i^2} (s_i q_g + \\alpha (s_i q_g)^2) = \\frac{q_g}{s_i} + \\alpha q_g^2$$\nThe mean of the normalized counts is $\\tilde{\\mu}_g = q_g$. The variance is a function of this mean: $\\operatorname{Var}(\\tilde{Y}_{ig}) = \\tilde{\\mu}_g/s_i + \\alpha \\tilde{\\mu}_g^2$. This variance is clearly not constant; it depends on the gene's abundance $q_g$ (or $\\tilde{\\mu}_g$). Genes with higher abundance will have a larger variance. This heteroscedasticity is problematic for many multivariate techniques that are sensitive to the scale of input features, justifying the need for a subsequent variance-stabilizing transformation.\n\n**Task 3: Derivation of the Variance-Stabilizing Transformation (VST)**\n\nA variance-stabilizing transformation $g(y)$ is a function such that the variance of the transformed random variable $g(Y)$ is approximately constant. Using a first-order Taylor expansion (the delta method), the variance of $g(Y)$ can be approximated as:\n$$\\operatorname{Var}(g(Y)) \\approx (g'(\\mu))^2 \\operatorname{Var}(Y) = (g'(\\mu))^2 v(\\mu)$$\nwhere $\\mu = \\operatorname{E}[Y]$. To achieve a constant variance, say $C$, we must have:\n$$(g'(\\mu))^2 v(\\mu) = C$$\nUsing our derived variance function $v(\\mu) = \\mu + \\alpha \\mu^2$, we have:\n$$(g'(\\mu))^2 (\\mu + \\alpha \\mu^2) = C$$\nSolving for $g'(\\mu)$:\n$$g'(\\mu) = \\frac{\\sqrt{C}}{\\sqrt{\\mu + \\alpha \\mu^2}}$$\nTo find $g(y)$, we integrate $g'(\\mu)$ with respect to $\\mu$. We can absorb the constant $\\sqrt{C}$ into the definition of $g$, effectively setting it to $1$.\n$$g(y) = \\int_0^y \\frac{1}{\\sqrt{u + \\alpha u^2}} du$$\nWe integrate from $0$ to $y$ to satisfy the condition $g(0)=0$. The integral can be solved as follows:\n$$g(y) = \\int_0^y \\frac{1}{\\sqrt{u(1 + \\alpha u)}} du$$\nLet $v = \\sqrt{\\alpha u}$. Then $u = v^2/\\alpha$, and $du = (2v/\\alpha)dv$. The limits of integration become $0$ to $\\sqrt{\\alpha y}$.\n\\begin{align*} g(y) &= \\int_0^{\\sqrt{\\alpha y}} \\frac{1}{\\sqrt{\\frac{v^2}{\\alpha}(1 + v^2)}} \\left(\\frac{2v}{\\alpha}\\right) dv \\\\ &= \\int_0^{\\sqrt{\\alpha y}} \\frac{1}{\\frac{v}{\\sqrt{\\alpha}}\\sqrt{1 + v^2}} \\left(\\frac{2v}{\\alpha}\\right) dv \\\\ &= \\frac{2\\sqrt{\\alpha}}{\\alpha} \\int_0^{\\sqrt{\\alpha y}} \\frac{v}{v\\sqrt{1+v^2}} dv \\\\ &= \\frac{2}{\\sqrt{\\alpha}} \\int_0^{\\sqrt{\\alpha y}} \\frac{1}{\\sqrt{1+v^2}} dv \\end{align*}\nThe integral of $(1+v^2)^{-1/2}$ is $\\arcsinh(v)$.\n$$g(y) = \\frac{2}{\\sqrt{\\alpha}} \\Big[ \\arcsinh(v) \\Big]_0^{\\sqrt{\\alpha y}} = \\frac{2}{\\sqrt{\\alpha}} (\\arcsinh(\\sqrt{\\alpha y}) - \\arcsinh(0))$$\nSince $\\arcsinh(0)=0$, the explicit closed-form VST is:\n$$g(y; \\alpha) = \\frac{2}{\\sqrt{\\alpha}} \\arcsinh(\\sqrt{\\alpha y})$$\nWith this transformation, the approximate variance becomes $\\operatorname{Var}(g(Y)) \\approx 1$, a constant.\n\n**Task 4: Calculation of Heteroscedasticity Ratios**\n\nWe are given the parameters for a single sample $i$: size factor $s_i = 3$, dispersion $\\alpha = 0.02$, and true abundances for two genes $q_1 = 20$ and $q_2 = 2000$.\n\nFirst, we calculate the mean counts for these two genes:\n$$\\mu_{i1} = s_i q_1 = 3 \\times 20 = 60$$\n$$\\mu_{i2} = s_i q_2 = 3 \\times 2000 = 6000$$\n\nNext, we compute the variance for each gene using $v(\\mu) = \\mu + \\alpha \\mu^2$:\n$$v(\\mu_{i1}) = 60 + 0.02 \\times (60)^2 = 60 + 0.02 \\times 3600 = 60 + 72 = 132$$\n$$v(\\mu_{i2}) = 6000 + 0.02 \\times (6000)^2 = 6000 + 0.02 \\times 36,000,000 = 6000 + 720,000 = 726,000$$\n\nThe heteroscedasticity ratio before transformation, $R_{\\text{before}}$, is the ratio of these variances:\n$$R_{\\text{before}} = \\frac{v(\\mu_{i2})}{v(\\mu_{i1})} = \\frac{726,000}{132} = 5500$$\n\nNow, we consider the ratio after applying the VST, $R_{\\text{after}}$. The VST $g(y)$ was specifically derived so that the variance of the transformed variable is approximately constant, independent of the mean. As shown in Task 3, the delta-method approximation gives:\n$$\\operatorname{Var}(g(Y_{ig})) \\approx (g'(\\mu_{ig}))^2 v(\\mu_{ig}) = \\left(\\frac{1}{\\sqrt{\\mu_{ig} + \\alpha \\mu_{ig}^2}}\\right)^2 (\\mu_{ig} + \\alpha \\mu_{ig}^2) = 1$$\nThus, for both genes, the variance after transformation is approximately $1$:\n$$\\operatorname{Var}(g(Y_{i1})) \\approx 1$$\n$$\\operatorname{Var}(g(Y_{i2})) \\approx 1$$\nThe heteroscedasticity ratio after transformation, $R_{\\text{after}}$, is therefore:\n$$R_{\\text{after}} = \\frac{\\operatorname{Var}(g(Y_{i2}))}{\\operatorname{Var}(g(Y_{i1}))} \\approx \\frac{1}{1} = 1$$\nThis calculation quantifies the effectiveness of the VST: it reduces a variance ratio of $5500$ down to $1$, successfully stabilizing the variance across features with vastly different mean expression levels.", "answer": "$$ \\boxed{ \\begin{pmatrix} 5500 & 1 \\end{pmatrix} } $$", "id": "4362390"}, {"introduction": "With high-quality, harmonized data in hand, we can proceed to the ultimate goal of many precision medicine studies: building predictive models that link molecular profiles to clinical outcomes. A key advantage of multi-omics data is the ability to leverage biological structure, such as the relationship between different measurements for the same gene. This final exercise [@problem_id:4362368] shows how to build a predictive model using an overlapping group lasso penalty, a sophisticated regularization technique that encourages sparsity at both the gene and modality level. You will implement an advanced optimization algorithm to solve this problem, learning how to embed biological priors directly into a machine learning model to improve its performance and interpretability.", "problem": "Consider a precision medicine setting where the goal is to predict a continuous clinical outcome from multi-omics features measured per gene across three modalities: Copy Number Variation (CNV), messenger ribonucleic acid (mRNA) expression, and Deoxyribonucleic Acid (DNA) methylation. Let $X \\in \\mathbb{R}^{n \\times p}$ denote the design matrix with $n$ samples and $p$ features. We consider $n = 8$ patients and $p = 9$ features, ordered by modality blocks across $3$ genes as follows: CNV for genes $1,2,3$ (columns $0,1,2$), mRNA expression for genes $1,2,3$ (columns $3,4,5$), DNA methylation for genes $1,2,3$ (columns $6,7,8$). The features are organized into overlapping groups to reflect two biologically coherent structures: (i) gene-wise groups across modalities and (ii) modality-wise groups across genes.\n\nYou are given the explicit data matrix $X$ and a deterministic target vector $y$ constructed with a known ground-truth coefficient vector $\\beta^\\star \\in \\mathbb{R}^p$ as $y = X \\beta^\\star$. The matrix $X$ (rows stacked) and the vector $\\beta^\\star$ are:\n- $X \\in \\mathbb{R}^{8 \\times 9}$ with rows:\n  - Row $1$: $[1, 0, -1, 0.5, -1.2, 0.3, 0.8, 0.1, 0.2]$\n  - Row $2$: $[0, 1, 1, 1.0, 0.3, -0.4, 0.6, 0.2, 0.9]$\n  - Row $3$: $[-1, -1, 0, -0.7, 0.5, 1.2, 0.1, 0.4, 0.3]$\n  - Row $4$: $[2, 0, 1, 0.2, -0.3, -0.5, 0.9, 0.7, 0.5]$\n  - Row $5$: $[0, -2, 1, 1.5, 0.7, 0.0, 0.2, 0.8, 0.6]$\n  - Row $6$: $[1, 1, 0, -0.4, 1.1, -1.3, 0.3, 0.5, 0.4]$\n  - Row $7$: $[-1, 2, -1, 0.0, -0.6, 0.9, 0.4, 0.6, 0.7]$\n  - Row $8$: $[0, 0, 2, 0.8, -0.9, 0.4, 0.5, 0.2, 0.1]$\n- $\\beta^\\star = [0.3, 0.0, -0.2, 1.0, 0.0, 0.5, -0.8, 0.0, 0.2]^\\top$\n- $y = X \\beta^\\star$\n\nDefine the following overlapping groups $\\mathcal{G}$ that reflect gene-level integration across modalities and modality-level integration across genes:\n- Gene-wise groups across modalities:\n  - $G_{\\text{gene},1} = \\{0, 3, 6\\}$\n  - $G_{\\text{gene},2} = \\{1, 4, 7\\}$\n  - $G_{\\text{gene},3} = \\{2, 5, 8\\}$\n- Modality-wise groups across genes:\n  - $G_{\\text{CNV}} = \\{0, 1, 2\\}$\n  - $G_{\\text{Expr}} = \\{3, 4, 5\\}$\n  - $G_{\\text{Meth}} = \\{6, 7, 8\\}$\n\nLet the weight for each group $g \\in \\mathcal{G}$ be $\\alpha_g = \\sqrt{|g|}$, where $|g|$ denotes the cardinality of group $g$. The overlapping group lasso penalty is\n$$\n\\Omega(w) \\;=\\; \\sum_{g \\in \\mathcal{G}} \\alpha_g \\, \\|w_g\\|_2,\n$$\nwhere $w_g$ denotes the subvector of $w \\in \\mathbb{R}^p$ indexed by $g$. The objective to minimize is the penalized empirical risk for linear regression with squared loss,\n$$\nF_\\lambda(w) \\;=\\; \\frac{1}{2n}\\,\\|X w - y\\|_2^2 \\;+\\; \\lambda\\,\\Omega(w),\n$$\nwith regularization parameter $\\lambda \\ge 0$ and $n = 8$.\n\nFundamental base and definitions to use:\n- The squared loss $f(w) = \\frac{1}{2n}\\|X w - y\\|_2^2$ has $\\nabla f(w) = \\frac{1}{n} X^\\top (X w - y)$ and is convex with $\\nabla f$ being $L$-Lipschitz where $L = \\lambda_{\\max}\\!\\left(\\frac{1}{n}X^\\top X\\right)$.\n- The proximal operator of a convex function $\\phi$ is $\\operatorname{prox}_{\\tau \\phi}(v) = \\arg\\min_{x} \\left\\{\\frac{1}{2}\\|x-v\\|_2^2 + \\tau \\phi(x)\\right\\}$ for $\\tau > 0$.\n- For a group $\\ell_2$-norm on indices $g$, $\\phi_g(x) = \\alpha_g \\|x_g\\|_2$, the proximal operator acts as block soft-thresholding on $x_g$ while leaving $x_{-g}$ unchanged.\n\nTasks:\n- Mathematically specify the overlapping group lasso penalty $\\Omega(w)$ using the provided groups and weights, and give the explicit gradient of the smooth loss $f(w)$.\n- Design a proximal gradient algorithm with Nesterov acceleration (Fast Iterative Shrinkage-Thresholding Algorithm) to minimize $F_\\lambda(w)$. Because the groups overlap, the proximal mapping of $\\tau \\lambda \\Omega$ is not separable across coordinates. Use a cyclic Dykstra scheme over the groupwise proximal operators $\\operatorname{prox}_{\\tau \\lambda \\alpha_g \\|\\cdot\\|_{2,g}}$ to compute $\\operatorname{prox}_{\\tau \\lambda \\Omega}$; each groupwise proximal step should perform block soft-thresholding only on the coordinates in $g$ and act as the identity elsewhere. Assume a fixed stepsize $t = 1/L$ with $L = \\lambda_{\\max}\\!\\left(\\frac{1}{n}X^\\top X\\right)$.\n\nProgram requirements:\n- Implement the accelerated proximal gradient method with the cyclic Dykstra proximal subroutine for the overlapping penalty. Initialize at $w^{(0)} = 0$. Use stopping criterion $\\|w^{(k+1)} - w^{(k)}\\|_2 \\le \\varepsilon \\max\\{1, \\|w^{(k)}\\|_2\\}$ with tolerance $\\varepsilon = 10^{-8}$ or a cap of $1000$ iterations, whichever occurs first. For the Dykstra inner loop, use a tolerance of $10^{-10}$ or a cap of $500$ inner iterations per outer iteration.\n- Test suite: run the solver for the regularization parameters $\\lambda \\in \\{0.0, 0.05, 0.2, 10.0\\}$.\n- For each $\\lambda$, compute the final objective value $F_\\lambda(w_\\lambda)$ at the converged solution $w_\\lambda$.\n- Final output format: your program should produce a single line containing a list of the four objective values as floats, in the order of the test suite, each rounded to exactly six digits after the decimal point, printed as a comma-separated list enclosed in square brackets (e.g., $[0.123456,1.234567,2.345678,3.456789]$).", "solution": "The user has provided a well-posed problem in computational statistics, specifically focused on solving a linear regression problem regularized with an overlapping group lasso penalty. All data, definitions, and algorithmic requirements are explicitly stated and are scientifically and mathematically sound.\n\n**Problem Statement Formalization**\n\nThe problem is to find a coefficient vector $w \\in \\mathbb{R}^p$ that minimizes the objective function $F_\\lambda(w)$, which is a composite of a smooth loss function and a non-smooth penalty function:\n$$\nF_\\lambda(w) \\;=\\; f(w) \\;+\\; \\lambda\\,\\Omega(w)\n$$\nwhere:\n- $n=8$ is the number of samples and $p=9$ is the number of features.\n- The loss function is the empirical squared error: $f(w) = \\frac{1}{2n}\\|X w - y\\|_2^2$, with $X \\in \\mathbb{R}^{n \\times p}$ and $y \\in \\mathbb{R}^n$ being the given data matrix and target vector, respectively.\n- The penalty function is the overlapping group lasso penalty: $\\Omega(w) = \\sum_{g \\in \\mathcal{G}} \\alpha_g \\|w_g\\|_2$.\n- $\\lambda \\ge 0$ is a non-negative regularization parameter.\n\n**1. Penalty Term and Loss Gradient Specification**\n\nFirst, we fully specify the components of the objective function using the provided information.\n\nThe set of groups $\\mathcal{G}$ consists of six groups in total:\n- Gene-wise groups: $G_{\\text{gene},1} = \\{0, 3, 6\\}$, $G_{\\text{gene},2} = \\{1, 4, 7\\}$, $G_{\\text{gene},3} = \\{2, 5, 8\\}$.\n- Modality-wise groups: $G_{\\text{CNV}} = \\{0, 1, 2\\}$, $G_{\\text{Expr}} = \\{3, 4, 5\\}$, $G_{\\text{Meth}} = \\{6, 7, 8\\}$.\n\nThe cardinality $|g|$ for each group $g \\in \\mathcal{G}$ is $3$. The weight for each group is therefore $\\alpha_g = \\sqrt{|g|} = \\sqrt{3}$. The penalty function is explicitly given by:\n$$\n\\Omega(w) = \\sqrt{3} \\left( \\|w_{\\{0,3,6\\}}\\|_2 + \\|w_{\\{1,4,7\\}}\\|_2 + \\|w_{\\{2,5,8\\}}\\|_2 + \\|w_{\\{0,1,2\\}}\\|_2 + \\|w_{\\{3,4,5\\}}\\|_2 + \\|w_{\\{6,7,8\\}}\\|_2 \\right)\n$$\nwhere $w_g$ is the subvector of $w$ with indices in $g$.\n\nThe loss function $f(w)$ is smooth and convex. Its gradient is given by:\n$$\n\\nabla f(w) = \\frac{1}{n} X^\\top (Xw - y)\n$$\nThis gradient is Lipschitz continuous with a constant $L = \\lambda_{\\max}\\!\\left(\\frac{1}{n}X^\\top X\\right)$, where $\\lambda_{\\max}(\\cdot)$ denotes the maximum eigenvalue of a matrix.\n\n**2. Algorithm Design: Accelerated Proximal Gradient with Dykstra's Scheme**\n\nTo minimize the composite objective function $F_\\lambda(w)$, we employ an accelerated proximal gradient method, specifically the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA). The core of each FISTA iteration is the computation of the proximal operator of the penalty term, $\\operatorname{prox}_{\\tau \\Omega}(\\cdot)$. Since the penalty $\\Omega(w)$ is a sum of norms on overlapping groups of variables, its proximal operator is not separable and cannot be computed in closed form. We use Dykstra's projection algorithm to compute it iteratively.\n\n**2.1. FISTA (Outer Loop)**\n\nThe algorithm maintains two sequences of iterates, a primary sequence $z^{(k)}$ and an accelerated sequence $w^{(k)}$.\n- **Initialization**:\n  - $z^{(0)} = \\mathbf{0} \\in \\mathbb{R}^p$\n  - $w^{(0)} = z^{(0)}$\n  - $\\theta_0 = 1$\n  - Step size $t = 1/L$ where $L = \\lambda_{\\max}(\\frac{1}{n}X^\\top X)$.\n- **Iteration $k=0, 1, \\dots$**:\n  1. Compute the gradient at the accelerated point: $g^{(k)} = \\nabla f(w^{(k)}) = \\frac{1}{n} X^\\top (Xw^{(k)} - y)$.\n  2. Perform a gradient descent step: $v^{(k)} = w^{(k)} - t \\cdot g^{(k)}$.\n  3. Apply the proximal operator for the entire penalty term: $z^{(k+1)} = \\operatorname{prox}_{t \\lambda \\Omega}(v^{(k)})$. This step is computed using the Dykstra's algorithm described below.\n  4. Update the momentum parameter: $\\theta_{k+1} = \\frac{1 + \\sqrt{1 + 4\\theta_k^2}}{2}$.\n  5. Compute the next accelerated point: $w^{(k+1)} = z^{(k+1)} + \\frac{\\theta_k - 1}{\\theta_{k+1}} (z^{(k+1)} - z^{(k)})$.\n- **Termination**: The loop terminates when $\\|w^{(k+1)} - w^{(k)}\\|_2 \\le \\varepsilon \\max\\{1, \\|w^{(k)}\\|_2\\}$ for a tolerance $\\varepsilon = 10^{-8}$, or after $1000$ iterations.\n\n**2.2. Dykstra's Algorithm for the Proximal Operator (Inner Loop)**\n\nThe key step $z^{(k+1)} = \\operatorname{prox}_{t \\lambda \\Omega}(v^{(k)})$ seeks to solve:\n$$\n\\operatorname{prox}_{t \\lambda \\Omega}(v) = \\arg\\min_{x \\in \\mathbb{R}^p} \\left\\{ \\frac{1}{2}\\|x-v\\|_2^2 + t \\lambda \\sum_{g \\in \\mathcal{G}} \\alpha_g \\|x_g\\|_2 \\right\\}\n$$\nLet $\\phi_g(x) = \\alpha_g \\|x_g\\|_2$ and $\\tau = t \\lambda$. The problem is to compute $\\operatorname{prox}_{\\tau \\sum_g \\phi_g}(v)$. Dykstra's algorithm achieves this by cyclically applying the proximal operator of each individual group penalty.\n- **Initialization**:\n  - Let $M=|\\mathcal{G}|=6$ be the number of groups.\n  - $x_{(0)} = v$.\n  - Initialize auxiliary variables (residuals) $p_{g,(0)} = \\mathbf{0} \\in \\mathbb{R}^p$ for each group $g \\in \\mathcal{G}$.\n- **Iteration $j=0, 1, \\dots$**:\n  1. Let $x_{\\text{prev}} = x_{(j)}$.\n  2. Perform a cycle through all groups $g \\in \\mathcal{G}$:\n     - Argument for the proximal operator: $u_g = x_{(j)} + p_{g,(j)}$.\n     - Apply group-wise proximal operator: $x'_{g} = \\operatorname{prox}_{\\tau \\phi_g}(u_g)$.\n     - Update the auxiliary variable: $p_{g,(j+1)} = u_g - x'_{g}$.\n     - Update the iterate for the next group in the cycle: $x_{(j)} \\leftarrow x'_{g}$.\n  3. After one full cycle, the main iterate is updated: $x_{(j+1)} = x_{(j)}$.\n- **Termination**: The inner loop terminates when $\\|x_{(j+1)} - x_{(j)}\\|_2$ is smaller than a tolerance of $10^{-10}$, or after $500$ iterations. The final $x_{(j+1)}$ is the result of the proximal mapping.\n\n**2.3. Group-wise Proximal Operator**\n\nThe proximal operator for a single group $g$, $\\operatorname{prox}_{\\tau \\phi_g}(u)$, performs block soft-thresholding. It updates the components of $u$ belonging to group $g$ and leaves other components unchanged. Let $z = \\operatorname{prox}_{\\tau \\phi_g}(u)$.\n- For indices $j \\in g$:\n  $$\n  z_g = u_g \\cdot \\max \\left(0, 1 - \\frac{\\tau \\alpha_g}{\\|u_g\\|_2} \\right) = \\left(1 - \\frac{\\tau \\alpha_g}{\\|u_g\\|_2} \\right)_+ u_g\n  $$\n  This operation shrinks the subvector $u_g$ towards the origin. If $\\|u_g\\|_2 \\le \\tau \\alpha_g$, then $z_g$ becomes a zero vector.\n- For indices $j \\notin g$, $z_j = u_j$.\n\nBy combining these components—FISTA for acceleration, Dykstra's algorithm for handling the overlapping penalties, and block soft-thresholding for each group's proximal operator—we can effectively minimize the objective function $F_\\lambda(w)$. The implementation will follow these mathematical specifications.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the overlapping group lasso problem for the given test suite.\n    \"\"\"\n    #\n    # Step 1: Define problem data from the problem statement.\n    #\n    X = np.array([\n        [1.0, 0.0, -1.0, 0.5, -1.2, 0.3, 0.8, 0.1, 0.2],\n        [0.0, 1.0, 1.0, 1.0, 0.3, -0.4, 0.6, 0.2, 0.9],\n        [-1.0, -1.0, 0.0, -0.7, 0.5, 1.2, 0.1, 0.4, 0.3],\n        [2.0, 0.0, 1.0, 0.2, -0.3, -0.5, 0.9, 0.7, 0.5],\n        [0.0, -2.0, 1.0, 1.5, 0.7, 0.0, 0.2, 0.8, 0.6],\n        [1.0, 1.0, 0.0, -0.4, 1.1, -1.3, 0.3, 0.5, 0.4],\n        [-1.0, 2.0, -1.0, 0.0, -0.6, 0.9, 0.4, 0.6, 0.7],\n        [0.0, 0.0, 2.0, 0.8, -0.9, 0.4, 0.5, 0.2, 0.1],\n    ])\n\n    beta_star = np.array([0.3, 0.0, -0.2, 1.0, 0.0, 0.5, -0.8, 0.0, 0.2])\n    n, p = X.shape\n    y = X @ beta_star\n    \n    # Define groups\n    groups = [\n        [0, 3, 6],  # Gene 1\n        [1, 4, 7],  # Gene 2\n        [2, 5, 8],  # Gene 3\n        [0, 1, 2],  # CNV modality\n        [3, 4, 5],  # mRNA modality\n        [6, 7, 8],  # Methylation modality\n    ]\n    \n    # Calculate group weights\n    group_weights = [np.sqrt(len(g)) for g in groups]\n    \n    #\n    # Step 2: Define helper functions for the optimization algorithm.\n    #\n    \n    def soft_threshold(u_g, threshold):\n        \"\"\"Block soft-thresholding for a single group.\"\"\"\n        norm_u_g = np.linalg.norm(u_g)\n        if norm_u_g == 0:\n            return np.zeros_like(u_g)\n        \n        scale = 1 - threshold / norm_u_g\n        return u_g * max(0, scale)\n\n    def dykstra_prox(v, tau, tol=1e-10, max_iter=500):\n        \"\"\"\n        Computes the proximal operator for the sum of group lasso penalties using\n        Dykstra's cyclic projection algorithm.\n        \"\"\"\n        x = v.copy()\n        residuals = np.zeros((len(groups), p))\n        \n        for _ in range(max_iter):\n            x_prev = x.copy()\n            for i, (g_indices, g_weight) in enumerate(zip(groups, group_weights)):\n                # Proximal argument\n                u = x + residuals[i]\n                \n                # Apply block soft-thresholding\n                x_prox = u.copy()\n                u_g = u[g_indices]\n                threshold = tau * g_weight\n                x_prox[g_indices] = soft_threshold(u_g, threshold)\n                \n                # Update residual and iterate\n                residuals[i] = u - x_prox\n                x = x_prox\n                \n            if np.linalg.norm(x - x_prev) <= tol:\n                break\n        return x\n\n    def calculate_objective(w, lambda_reg):\n        \"\"\"Calculates the value of the objective function F_lambda(w).\"\"\"\n        loss = (1 / (2 * n)) * np.linalg.norm(X @ w - y)**2\n        penalty = 0.0\n        for g_indices, g_weight in zip(groups, group_weights):\n            penalty += g_weight * np.linalg.norm(w[g_indices])\n        return loss + lambda_reg * penalty\n\n    def solve_fista_dykstra(lambda_reg, tol=1e-8, max_iter=1000):\n        \"\"\"\n        Solves the overlapping group lasso problem using FISTA with Dykstra's algorithm.\n        \"\"\"\n        # Lipschitz constant and step size\n        L = np.linalg.eigvalsh(X.T @ X / n).max()\n        step_size = 1.0 / L\n\n        # FISTA initialization\n        w = np.zeros(p)\n        z = np.zeros(p)\n        theta = 1.0\n\n        for k in range(max_iter):\n            w_prev = w.copy()\n            z_prev = z.copy()\n            \n            # Gradient step from the accelerated point w\n            grad = (1.0 / n) * X.T @ (X @ w - y)\n            v = w - step_size * grad\n            \n            # Proximal step using Dykstra's algorithm\n            z = dykstra_prox(v, step_size * lambda_reg)\n            \n            # Nesterov momentum update\n            theta_prev = theta\n            theta = (1.0 + np.sqrt(1.0 + 4.0 * theta_prev**2)) / 2.0\n            \n            # Update the accelerated point\n            w = z + ((theta_prev - 1.0) / theta) * (z - z_prev)\n            \n            # Check for convergence on the 'w' iterate\n            stop_crit_val = np.linalg.norm(w - w_prev)\n            stop_crit_threshold = tol * max(1.0, np.linalg.norm(w_prev))\n\n            # Special case for lambda=0: objective can be zero\n            if lambda_reg == 0 and calculate_objective(w, lambda_reg) < 1e-12:\n                break\n                \n            if stop_crit_val <= stop_crit_threshold:\n                break\n        \n        return w\n        \n    #\n    # Step 3: Run the test suite and collect results.\n    #\n    test_suite = [0.0, 0.05, 0.2, 10.0]\n    results = []\n\n    for lambda_reg in test_suite:\n        w_final = solve_fista_dykstra(lambda_reg)\n        objective_value = calculate_objective(w_final, lambda_reg)\n        results.append(f\"{objective_value:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4362368"}]}