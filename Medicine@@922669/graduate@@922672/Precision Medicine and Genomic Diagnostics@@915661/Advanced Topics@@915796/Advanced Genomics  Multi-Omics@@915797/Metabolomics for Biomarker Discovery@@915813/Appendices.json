{"hands_on_practices": [{"introduction": "The foundation of any metabolomics study is the ability to generate accurate and reliable data. In high-resolution mass spectrometry, achieving high mass accuracy is paramount for confidently assigning elemental compositions to detected signals. This exercise provides a hands-on opportunity to understand the principles of mass calibration by contrasting external and internal calibration methods and to compute the mass error from first principles, a fundamental skill for assessing data quality in any biomarker discovery workflow [@problem_id:4358307].", "problem": "In a high-resolution liquid chromatography–mass spectrometry (LC–MS) metabolomics workflow for biomarker discovery, accurate mass measurement underpins confident elemental composition assignment and subsequent integration with genomic diagnostics. Consider the endogenous thiol metabolite glutathione, with neutral molecular formula $\\mathrm{C}_{10}\\mathrm{H}_{17}\\mathrm{N}_{3}\\mathrm{O}_{6}\\mathrm{S}$. A singly protonated ion $\\left[\\mathrm{M}+\\mathrm{H}\\right]^{+}$ is measured on an Orbitrap instrument operating in positive electrospray ionization mode. The instrument was calibrated using a set of external standards prior to the analytical run; a lock-mass internal standard was not used.\n\nA particular feature corresponding to $\\left[\\mathrm{M}+\\mathrm{H}\\right]^{+}$ of glutathione is observed at $m/z = 308.091650000$ under this external calibration. Use monoisotopic atomic masses for the neutral molecule: carbon ($\\mathrm{C}$) $12.000000000$, hydrogen ($\\mathrm{H}$) $1.00782503223$, nitrogen ($\\mathrm{N}$) $14.00307400443$, oxygen ($\\mathrm{O}$) $15.99491461957$, sulfur ($\\mathrm{S}$) $31.9720711744$, and the mass of the proton $\\mathrm{H}^{+}$ $1.00727646688$. Assume unit charge $z=1$ for $\\left[\\mathrm{M}+\\mathrm{H}\\right]^{+}$.\n\nStarting from first principles of mass spectrometry measurement and calibration, briefly explain the difference between external mass calibration and internal mass calibration in terms of how they correct systematic frequency-to-mass mapping and drift during the run. Then, compute the mass error, in parts per million, of the externally calibrated measurement of glutathione’s $\\left[\\mathrm{M}+\\mathrm{H}\\right]^{+}$ feature by first deriving the theoretical $m/z$ from its elemental composition and charge state. Round your final numerical value to four significant figures. Express the final answer as a pure number representing parts per million (do not include any unit symbol).", "solution": "High-resolution mass spectrometry measures the mass-to-charge ratio $m/z$ by mapping an ion’s motion-derived frequency (for example, orbital frequency in an Orbitrap or cyclotron frequency in Fourier Transform Ion Cyclotron Resonance (FT-ICR)) to mass through an instrument-specific transfer function established by calibration. Because this mapping is sensitive to instrument conditions and can drift over time (for example, due to temperature, pressure, or voltage changes), calibration strategies are used to ensure accurate mass measurements.\n\nExternal mass calibration constructs the frequency-to-mass mapping using standard compounds measured prior to the analytical run. The instrument parameters are adjusted so that the known standard masses align with their expected $m/z$ values, and this calibration is then applied to subsequent samples without further correction during acquisition. Internal mass calibration uses reference ions present during the sample run (either spiked calibrants or endogenous lock-mass ions) to continuously or periodically correct the mapping in real time, compensating for intra-run drift. Consequently, internal calibration typically yields superior mass accuracy during the run because it tracks and corrects for time-dependent deviations, whereas external calibration relies on the assumption that conditions remain stable.\n\nTo compute the mass error in parts per million for the externally calibrated measurement, we first derive the theoretical $m/z$ for glutathione’s singly protonated ion $\\left[\\mathrm{M}+\\mathrm{H}\\right]^{+}$ from its monoisotopic elemental composition. The monoisotopic neutral mass $m_{\\mathrm{neutral}}$ of glutathione with formula $\\mathrm{C}_{10}\\mathrm{H}_{17}\\mathrm{N}_{3}\\mathrm{O}_{6}\\mathrm{S}$ is\n$$\nm_{\\mathrm{neutral}} = 10 \\times 12.000000000 + 17 \\times 1.00782503223 + 3 \\times 14.00307400443 + 6 \\times 15.99491461957 + 1 \\times 31.9720711744.\n$$\nCompute each term:\n$$\n10 \\times 12.000000000 = 120.000000000,\n$$\n$$\n17 \\times 1.00782503223 = 17.13302554791,\n$$\n$$\n3 \\times 14.00307400443 = 42.00922201329,\n$$\n$$\n6 \\times 15.99491461957 = 95.96948771742,\n$$\n$$\n1 \\times 31.9720711744 = 31.9720711744.\n$$\nSumming,\n$$\nm_{\\mathrm{neutral}} = 120.000000000 + 17.13302554791 + 42.00922201329 + 95.96948771742 + 31.9720711744 = 307.08380645302.\n$$\nFor the singly protonated ion with charge $z=1$, the theoretical $m/z$ is the neutral mass plus the proton mass:\n$$\nm_{\\mathrm{theo}} = m_{\\mathrm{neutral}} + m_{\\mathrm{H}^{+}} = 307.08380645302 + 1.00727646688 = 308.09108291990.\n$$\nThe observed externally calibrated value is\n$$\nm_{\\mathrm{obs}} = 308.09165000000.\n$$\nThe mass error in parts per million (ppm) is defined by the standard relation between observed and theoretical mass:\n$$\n\\mathrm{ppm} = 10^{6} \\cdot \\frac{m_{\\mathrm{obs}} - m_{\\mathrm{theo}}}{m_{\\mathrm{theo}}}.\n$$\nCompute the numerator difference:\n$$\nm_{\\mathrm{obs}} - m_{\\mathrm{theo}} = 308.09165000000 - 308.09108291990 = 0.00056708010.\n$$\nForm the ratio:\n$$\n\\frac{m_{\\mathrm{obs}} - m_{\\mathrm{theo}}}{m_{\\mathrm{theo}}} = \\frac{0.00056708010}{308.09108291990}.\n$$\nMultiply by $10^{6}$ to obtain parts per million:\n$$\n\\mathrm{ppm} = 10^{6} \\cdot \\frac{0.00056708010}{308.09108291990} = \\frac{567.08010}{308.09108291990}.\n$$\nEvaluate the quotient to obtain the numerical ppm value:\n$$\n\\mathrm{ppm} \\approx 1.84062484.\n$$\nRounding to four significant figures, the mass error is\n$$\n1.841.\n$$\nThis positive value indicates the externally calibrated measurement is higher than the theoretical $m/z$ by approximately $1.841$ parts per million, consistent with the expectation that external calibration does not correct intra-run drift, whereas internal calibration would reduce this deviation by locking the mass scale during acquisition.", "answer": "$$\\boxed{1.841}$$", "id": "4358307"}, {"introduction": "Once high-quality data is acquired, it must be appropriately prepared for statistical analysis. Metabolite intensities can span several orders of magnitude, and their variances can differ dramatically, potentially biasing downstream multivariate models. This practice explores the mathematical impact of common data transformation and scaling methods, such as autoscaling and Pareto scaling, on the underlying variance structure of the data, providing crucial insight into how these choices influence the results of techniques like Principal Component Analysis (PCA) [@problem_id:4358289].", "problem": "A metabolomics study in precision medicine aims to discover biomarkers that differentiate two patient subgroups by quantifying metabolite peak intensities across $n$ subjects. Assume two metabolites, indexed by $i \\in \\{1,2\\}$, have raw peak intensities $I_{i}$ for which the multiplicative nature of biological and technical variance is well described by a log-normal distribution: $I_{i} \\sim \\mathrm{LogNormal}(\\mu_{i}, \\tau_{i}^{2})$, meaning $\\ln(I_{i}) \\sim \\mathcal{N}(\\mu_{i}, \\tau_{i}^{2})$, independently across $i$. Consider three transformation regimes, each applied after mean-centering of the log-transformed intensities:\n- Log-only: $Y_{i} = \\ln(I_{i}) - \\mathbb{E}[\\ln(I_{i})]$.\n- Autoscaling: $A_{i} = \\dfrac{\\ln(I_{i}) - \\mathbb{E}[\\ln(I_{i})]}{\\sqrt{\\operatorname{Var}(\\ln(I_{i}))}}$.\n- Pareto scaling: $P_{i} = \\dfrac{\\ln(I_{i}) - \\mathbb{E}[\\ln(I_{i})]}{\\sqrt{\\sqrt{\\operatorname{Var}(\\ln(I_{i}))}}}$.\n\nStarting from the fundamental definitions of the log-normal distribution and the properties of variance under affine transformations, derive the closed-form analytic expressions for the variance ratio $R_{12}$ of metabolite $1$ relative to metabolite $2$ under each regime, where $R_{12} = \\dfrac{\\operatorname{Var}(T_{1})}{\\operatorname{Var}(T_{2})}$ and $T_{i}$ denotes, respectively, $Y_{i}$, $A_{i}$, or $P_{i}$ for the three regimes. Then, justify from first principles how these variance structures influence downstream multivariate modeling choices, such as Principal Component Analysis (PCA) and Partial Least Squares (PLS), in biomarker discovery for precision medicine, and explain the appropriateness of each regime for different analytic tasks.\n\nProvide the final answer as a single row matrix containing the three expressions for $R_{12}$ corresponding to log-only, autoscaling, and Pareto scaling, respectively. No numerical evaluation or rounding is required.", "solution": "### Derivation of Variance Ratios\n\nLet the random variable $X_{i}$ represent the log-transformed intensity of metabolite $i$, such that $X_{i} = \\ln(I_{i})$. From the problem statement, we know that $X_{i} \\sim \\mathcal{N}(\\mu_{i}, \\tau_{i}^{2})$. Therefore, we have the population mean $\\mathbb{E}[X_{i}] = \\mu_{i}$ and the population variance $\\operatorname{Var}(X_i) = \\tau_i^2$.\n\nThe transformation regimes can be expressed in terms of $X_i$, $\\mu_i$, and $\\tau_i^2$:\n-   Log-only: $Y_{i} = X_{i} - \\mu_{i}$.\n-   Autoscaling: $A_{i} = \\dfrac{X_{i} - \\mu_{i}}{\\sqrt{\\tau_{i}^2}} = \\dfrac{X_{i} - \\mu_{i}}{\\tau_{i}}$.\n-   Pareto scaling: $P_{i} = \\dfrac{X_{i} - \\mu_{i}}{\\sqrt{\\sqrt{\\tau_{i}^2}}} = \\dfrac{X_{i} - \\mu_{i}}{\\sqrt{\\tau_{i}}}$.\n\nWe will use the fundamental property of variance for an affine transformation of a random variable $Z$: $\\operatorname{Var}(aZ + b) = a^{2}\\operatorname{Var}(Z)$, where $a$ and $b$ are constants.\n\n**1. Log-only Scaling ($Y_i$)**\nThe transformed variable is $Y_{i} = X_{i} - \\mu_{i}$. This is an affine transformation with $a=1$ and $b=-\\mu_{i}$. The variance is:\n$$ \\operatorname{Var}(Y_{i}) = \\operatorname{Var}(X_{i} - \\mu_{i}) = 1^2 \\operatorname{Var}(X_{i}) = \\tau_i^2 $$\nThe variance ratio for metabolites $1$ and $2$ is:\n$$ R_{12}^{(Y)} = \\frac{\\operatorname{Var}(Y_{1})}{\\operatorname{Var}(Y_{2})} = \\frac{\\tau_1^2}{\\tau_2^2} $$\n\n**2. Autoscaling ($A_i$)**\nThe transformed variable is $A_{i} = \\dfrac{X_{i} - \\mu_{i}}{\\tau_{i}} = \\left(\\dfrac{1}{\\tau_{i}}\\right)X_{i} - \\dfrac{\\mu_{i}}{\\tau_{i}}$. This is an affine transformation with $a = \\dfrac{1}{\\tau_{i}}$ and $b = -\\dfrac{\\mu_{i}}{\\tau_{i}}$. The variance is:\n$$ \\operatorname{Var}(A_{i}) = \\operatorname{Var}\\left(\\frac{1}{\\tau_{i}}X_{i} - \\frac{\\mu_{i}}{\\tau_{i}}\\right) = \\left(\\frac{1}{\\tau_{i}}\\right)^2 \\operatorname{Var}(X_{i}) = \\frac{1}{\\tau_i^2} \\cdot \\tau_i^2 = 1 $$\nThe variance of every autoscaled metabolite is unity. The variance ratio for metabolites $1$ and $2$ is:\n$$ R_{12}^{(A)} = \\frac{\\operatorname{Var}(A_{1})}{\\operatorname{Var}(A_{2})} = \\frac{1}{1} = 1 $$\n\n**3. Pareto Scaling ($P_i$)**\nThe transformed variable is $P_{i} = \\dfrac{X_{i} - \\mu_{i}}{\\sqrt{\\tau_{i}}}$. This is an affine transformation with $a = \\dfrac{1}{\\sqrt{\\tau_{i}}}$ and $b = -\\dfrac{\\mu_{i}}{\\sqrt{\\tau_{i}}}$. The variance is:\n$$ \\operatorname{Var}(P_{i}) = \\operatorname{Var}\\left(\\frac{1}{\\sqrt{\\tau_{i}}}X_{i} - \\frac{\\mu_{i}}{\\sqrt{\\tau_{i}}}\\right) = \\left(\\frac{1}{\\sqrt{\\tau_{i}}}\\right)^2 \\operatorname{Var}(X_{i}) = \\frac{1}{\\tau_{i}} \\cdot \\tau_i^2 = \\tau_i $$\nThe variance ratio for metabolites $1$ and $2$ is:\n$$ R_{12}^{(P)} = \\frac{\\operatorname{Var}(P_{1})}{\\operatorname{Var}(P_{2})} = \\frac{\\tau_1}{\\tau_2} $$\n\n### Influence on Multivariate Modeling and Appropriateness\n\nMultivariate projection methods such as Principal Component Analysis (PCA) and Partial Least Squares (PLS) are variance-driven. The axes of the new projected space (principal components or latent variables) are defined as linear combinations of the original variables, chosen to maximize a variance or covariance criterion. Consequently, the relative scaling of the original variables profoundly influences the resulting model. Variables with higher variance will have a greater influence on the model structure, potentially dominating it. The choice of scaling regime is therefore a critical decision based on the analytical goals.\n\n-   **Log-only Scaling:** This regime preserves the original variance structure of the log-transformed data, where $\\operatorname{Var}(Y_i) = \\tau_i^2$. In a PCA/PLS model, metabolites with high intrinsic biological or technical variability (large $\\tau_i^2$) will dominate the first few components. This may be desirable if this high variance is believed to be directly related to the biological question of interest. However, in biomarker discovery, it is often a liability, as high-abundance or technically noisy metabolites can obscure subtle but consistent changes in lower-variance metabolites that are genuinely discriminant between patient subgroups. It is appropriate only when the features with the largest variance are known to be the most important.\n\n-   **Autoscaling:** This regime equalizes the variance of all metabolites to unity, $\\operatorname{Var}(A_i) = 1$. Each metabolite is thus given equal weight, or an equal opportunity to influence the model. The analysis shifts from being driven by variance magnitude to being driven by the correlation structure between metabolites. This is often the default choice in untargeted biomarker discovery, as it ensures that low-abundance or low-variance metabolites are not computationally ignored. It allows the model to detect patterns involving metabolites whose absolute changes are small but highly correlated with a phenotype. Its primary drawback is a potential for noise inflation: for metabolites with very small intrinsic variance $\\tau_i^2$, dividing by a small standard deviation $\\tau_i$ can amplify the contribution of measurement error relative to the biological signal.\n\n-   **Pareto Scaling:** This regime, resulting in $\\operatorname{Var}(P_i) = \\tau_i$, represents a compromise. It reduces the overwhelming influence of the highest-variance metabolites without completely removing the variance information. The original variances $\\tau_i^2$ are 'dampened' by the square root transformation. For instance, if $\\tau_1^2 = 100\\tau_2^2$, then after Pareto scaling, $\\operatorname{Var}(P_1) = 10\\operatorname{Var}(P_2)$. This is a significant reduction from the $100$-fold difference in the log-only scaled data. Pareto scaling is often considered an optimal balance in metabolomics because it retains some emphasis on larger-fold-change metabolites (which are often biologically significant) while still allowing lower-variance metabolites to contribute to the model. It is appropriate when both large and subtle effects are of potential interest and one wishes to avoid the extremes of either preserving the original variance or completely equalizing it.", "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{\\tau_1^2}{\\tau_2^2} & 1 & \\frac{\\tau_1}{\\tau_2} \\end{pmatrix}} $$", "id": "4358289"}, {"introduction": "Metabolomics experiments are inherently high-dimensional, often involving thousands of simultaneous statistical tests to identify features that differ between groups. This massive multiplicity inflates the risk of false positives, necessitating robust statistical correction. This final exercise guides you through the logic and application of the Benjamini–Hochberg procedure to control the False Discovery Rate (FDR), empowering you to calculate and interpret q-values, the standard for declaring statistical significance in modern biomarker discovery [@problem_id:4358285].", "problem": "In a large-scale untargeted Liquid Chromatography–Mass Spectrometry (LC–MS) metabolomics study for biomarker discovery in precision medicine and genomic diagnostics, suppose you test differential abundance for a panel of candidate metabolites across two well-matched cohorts. Assume the following foundational conditions commonly used in multiple testing theory: (i) under true null hypotheses the test $p$-values are stochastically dominated by a Uniform distribution on the unit interval, so that for any $t \\in [0,1]$, $\\mathbb{P}(p \\leq t) \\leq t$, and (ii) the collection of null $p$-values is independent of each other and independent of the non-null $p$-values. Let the number of simultaneous hypotheses be $m$, with $m_{0}$ denoting the number of true nulls.\n\nStarting from the definition of False Discovery Rate (FDR), defined as $\\mathrm{FDR} = \\mathbb{E}\\!\\left[\\frac{V}{\\max(R,1)}\\right]$ where $V$ is the number of false rejections and $R$ is the total number of rejections, derive the Benjamini–Hochberg (BH) step-up procedure that controls FDR at a user-specified level $\\alpha \\in (0,1)$ under the independence assumptions above. In particular, derive the rule that determines the largest rank at which to reject and explain why this guarantees $\\mathrm{FDR} \\leq \\alpha$.\n\nThen, using the BH framework, define the $q$-value for each tested feature as the minimal FDR level at which that feature would be called significant by the BH procedure. Using this definition, compute the $q$-values for the following $m=12$ metabolites with observed two-sided $p$-values from the cohort comparison:\n- Citrate: $0.0009$\n- Acylcarnitine C16: $0.0025$\n- Lactate: $0.0041$\n- Kynurenine: $0.0068$\n- Succinate: $0.011$\n- Glycine: $0.013$\n- Lysophosphatidylcholine LPC(18:2): $0.021$\n- Alpha-hydroxybutyrate: $0.031$\n- Sphingomyelin SM(d18{:}1/24{:}0): $0.047$\n- Creatinine: $0.062$\n- Trimethylamine N-oxide (TMAO): $0.084$\n- Glucose: $0.16$\n\nAfter completing the derivation and calculations, report the Benjamini–Hochberg $q$-value for Kynurenine. Express the final answer as a pure number (no units) and round your answer to four significant figures.", "solution": "### Part 1: Derivation and Justification of the Benjamini–Hochberg Procedure\n\nLet there be $m$ simultaneous hypotheses $H_1, \\ldots, H_m$ with corresponding $p$-values $p_1, \\ldots, p_m$. Let $m_0$ be the number of true null hypotheses. Let the ordered $p$-values be $p_{(1)} \\le p_{(2)} \\le \\ldots \\le p_{(m)}$.\n\nThe Benjamini–Hochberg (BH) step-up procedure for controlling the False Discovery Rate (FDR) at a level $\\alpha \\in (0,1)$ is as follows:\n1.  Find the largest integer $k$ such that $p_{(k)} \\le \\frac{k}{m}\\alpha$.\n2.  If such a $k$ exists, reject all null hypotheses $H_{(i)}$ for $i = 1, \\ldots, k$.\n3.  If no such $k$ exists, do not reject any null hypothesis.\n\nThe total number of rejections is $R=k$, and the number of false rejections (Type I errors) is $V$, which is the number of true null hypotheses among the $k$ rejected hypotheses. The False Discovery Rate is defined as $\\mathrm{FDR} = \\mathbb{E}\\!\\left[\\frac{V}{\\max(R,1)}\\right]$. We now show that this procedure guarantees $\\mathrm{FDR} \\leq \\frac{m_0}{m}\\alpha \\leq \\alpha$ under the assumption that the $p$-values corresponding to true null hypotheses are independent.\n\nLet $I_0$ be the set of indices corresponding to the $m_0$ true null hypotheses. Let $R$ be the number of total rejections. The false discovery proportion is $Q = \\frac{V}{R}$ if $R>0$, and $Q=0$ if $R=0$. We wish to show $E[Q] \\le \\frac{m_0}{m} \\alpha$.\n\nBy the linearity of expectation, we can write:\n$$ \\mathrm{FDR} = \\mathbb{E}[Q] = \\mathbb{E}\\left[ \\sum_{i \\in I_0} \\frac{I(H_i \\text{ is rejected})}{\\max(R,1)} \\right] = \\sum_{i \\in I_0} \\mathbb{E}\\left[ \\frac{I(H_i \\text{ is rejected})}{\\max(R,1)} \\right] $$\nwhere $I(\\cdot)$ is the indicator function.\n\nA more direct proof, following the original logic of Benjamini and Hochberg, proceeds as follows. Let $R$ be the random number of rejected hypotheses. If $R=r > 0$, the procedure has rejected all hypotheses with $p$-values up to $p_{(r)}$, and we know from the rule that $p_{(r)} \\le \\frac{r}{m}\\alpha$. Any rejected true null hypothesis $H_i$ must therefore have $p_i \\le p_{(r)} \\le \\frac{r}{m}\\alpha$.\n\nThus, for a given number of rejections $R=r$, the number of false discoveries $V$ is bounded:\n$$ V \\le \\sum_{i \\in I_0} I\\left(p_i \\le \\frac{r}{m}\\alpha\\right) $$\nThe false discovery proportion, conditioned on $R=r$, is $\\frac{V}{r}$. Its expectation is:\n$$ \\mathbb{E}\\left[\\frac{V}{r} \\bigg| R=r\\right] \\le \\mathbb{E}\\left[ \\frac{1}{r} \\sum_{i \\in I_0} I\\left(p_i \\le \\frac{r}{m}\\alpha\\right) \\bigg| R=r \\right] $$\nThe crucial assumption is that the null $p$-values are independent of the non-null $p$-values. The value of $R=r$ is determined by the full set of $p$-values. However, because of the independence assumption, conditioning on $R=r$ does not distort the distribution of the null $p$-values in a way that invalidates the property $\\mathbb{P}(p_i \\le t) \\le t$.\n$$ \\mathbb{E}\\left[\\frac{V}{r} \\bigg| R=r\\right] \\le \\frac{1}{r} \\sum_{i \\in I_0} \\mathbb{P}\\left(p_i \\le \\frac{r}{m}\\alpha \\bigg| R=r\\right) $$\nUnder the specified independence conditions, we can state that $\\mathbb{P}(p_i \\le t | R=r) \\le t$. This is the pivotal step. Therefore:\n$$ \\mathbb{E}\\left[\\frac{V}{r} \\bigg| R=r\\right] \\le \\frac{1}{r} \\sum_{i \\in I_0} \\frac{r}{m}\\alpha = \\frac{1}{r} \\cdot m_0 \\cdot \\frac{r}{m}\\alpha = \\frac{m_0}{m}\\alpha $$\nThis inequality holds for any possible outcome $r>0$ of $R$. We can now take the expectation over $R$:\n$$ \\mathrm{FDR} = \\mathbb{E}\\left[\\frac{V}{\\max(R,1)}\\right] = \\mathbb{E}\\left[\\mathbb{E}\\left[\\frac{V}{\\max(R,1)} \\bigg| R\\right]\\right] $$\n$$ \\mathrm{FDR} = \\sum_{r=1}^m \\mathbb{E}\\left[\\frac{V}{r} \\bigg| R=r\\right]\\mathbb{P}(R=r) \\le \\sum_{r=1}^m \\frac{m_0}{m}\\alpha \\cdot \\mathbb{P}(R=r) = \\frac{m_0}{m}\\alpha \\sum_{r=1}^m \\mathbb{P}(R=r) $$\nSince $\\sum_{r=1}^m \\mathbb{P}(R=r) = \\mathbb{P}(R>0) \\le 1$, we have:\n$$ \\mathrm{FDR} \\le \\frac{m_0}{m}\\alpha $$\nAs the number of true nulls $m_0$ is less than or equal to the total number of tests $m$, we have $\\frac{m_0}{m} \\le 1$, which completes the proof:\n$$ \\mathrm{FDR} \\le \\frac{m_0}{m}\\alpha \\le \\alpha $$\n\n### Part 2: Definition and Calculation of $q$-values\n\nThe $q$-value for a particular feature (hypothesis) is defined as the minimal FDR level $\\alpha$ at which that feature would be declared significant.\nLet the ordered $p$-values be $p_{(1)} \\le p_{(2)} \\le \\ldots \\le p_{(m)}$. For the $i$-th hypothesis in this ordering, $H_{(i)}$, to be declared significant, the BH procedure must reject at least up to rank $i$. This means there must exist some $k \\ge i$ such that $p_{(k)} \\le \\frac{k}{m}\\alpha$.\nThis inequality can be rewritten as $\\alpha \\ge p_{(k)} \\frac{m}{k}$. To find the *minimal* $\\alpha$ that satisfies this condition for at least one $k \\ge i$, we must find the minimum of these lower bounds over all possible $k$ from $i$ to $m$.\nThus, the $q$-value for the $i$-th ordered $p$-value, denoted $q_{(i)}$, is:\n$$ q_{(i)} = \\min_{k=i, \\ldots, m} \\left\\{ \\frac{m \\cdot p_{(k)}}{k} \\right\\} $$\nThis definition ensures that the $q$-values are monotonically non-decreasing, i.e., $q_{(1)} \\le q_{(2)} \\le \\dots \\le q_{(m)}$. This can be seen from the equivalent recursive definition:\n$q_{(m)} = p_{(m)}$, and for $i = m-1, \\ldots, 1$, $q_{(i)} = \\min\\left(q_{(i+1)}, \\frac{m \\cdot p_{(i)}}{i}\\right)$.\n\nWe are given $m=12$ metabolites with their corresponding $p$-values, which are already sorted:\n1. Citrate: $p_{(1)} = 0.0009$\n2. Acylcarnitine C16: $p_{(2)} = 0.0025$\n3. Lactate: $p_{(3)} = 0.0041$\n4. Kynurenine: $p_{(4)} = 0.0068$\n5. Succinate: $p_{(5)} = 0.011$\n6. Glycine: $p_{(6)} = 0.013$\n7. LPC(18:2): $p_{(7)} = 0.021$\n8. Alpha-hydroxybutyrate: $p_{(8)} = 0.031$\n9. SM(d18{:}1/24{:}0): $p_{(9)} = 0.047$\n10. Creatinine: $p_{(10)} = 0.062$\n11. TMAO: $p_{(11)} = 0.084$\n12. Glucose: $p_{(12)} = 0.16$\n\nWe need to compute the $q$-value for Kynurenine, which is the 4th metabolite in the list, so we need to compute $q_{(4)}$. We use the formula $q_{(4)} = \\min_{k=4, \\ldots, 12} \\left\\{ \\frac{12 \\cdot p_{(k)}}{k} \\right\\}$.\n\nFirst, we compute the values of $\\frac{m \\cdot p_{(k)}}{k}$ for $k=4, \\ldots, 12$:\n- For $k=4$: $\\frac{12 \\times 0.0068}{4} = 3 \\times 0.0068 = 0.0204$\n- For $k=5$: $\\frac{12 \\times 0.011}{5} = \\frac{0.132}{5} = 0.0264$\n- For $k=6$: $\\frac{12 \\times 0.013}{6} = 2 \\times 0.013 = 0.026$\n- For $k=7$: $\\frac{12 \\times 0.021}{7} \\approx 0.036$\n- For $k=8$: $\\frac{12 \\times 0.031}{8} = 1.5 \\times 0.031 = 0.0465$\n- For $k=9$: $\\frac{12 \\times 0.047}{9} = \\frac{4}{3} \\times 0.047 \\approx 0.062667$\n- For $k=10$: $\\frac{12 \\times 0.062}{10} = 1.2 \\times 0.062 = 0.0744$\n- For $k=11$: $\\frac{12 \\times 0.084}{11} \\approx 0.091636$\n- For $k=12$: $\\frac{12 \\times 0.16}{12} = 0.16$\n\nThe $q$-value for Kynurenine, $q_{(4)}$, is the minimum of these calculated values:\n$$ q_{(4)} = \\min\\{0.0204, 0.0264, 0.026, 0.036, 0.0465, 0.062667, 0.0744, 0.091636, 0.16\\} $$\nThe minimum value in this set is $0.0204$.\nTherefore, the $q$-value for Kynurenine is $0.0204$.\n\nRounding the answer to four significant figures gives $0.02040$.", "answer": "$$\\boxed{0.02040}$$", "id": "4358285"}]}