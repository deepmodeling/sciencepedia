{"hands_on_practices": [{"introduction": "This first practice will ground you in a fundamental task in proteomics: estimating the relative abundance of proteins from mass spectrometry data. You will derive and apply the Normalized Spectral Abundance Factor ($NSAF$), a classic method that adjusts for protein length bias in spectral counting experiments. This exercise highlights the importance of normalization and the underlying physical and biochemical assumptions required to transform raw experimental signals into meaningful quantitative estimates.", "problem": "In a label-free shotgun proteomics experiment using Liquid Chromatography–Tandem Mass Spectrometry (LC-MS/MS) with Data-Dependent Acquisition (DDA), the number of assigned peptide tandem spectra for a protein, called its spectral count, is often used as a proxy for protein abundance. Under the Central Dogma of molecular biology and the sampling properties of tandem mass spectrometry, assume the following foundational model: the expected spectral count for protein $i$, denoted $SpC_i$, is proportional to the product of its molar abundance $A_i$ and its length $L_i$ in amino acids, because longer proteins produce more distinct tryptic peptides that can be sampled. That is, there exists a constant of proportionality $k$ such that $SpC_i \\approx k\\,A_i\\,L_i$ for all proteins $i$ in a given sample and run. To estimate relative abundance, you decide to compute, for a protein of interest $i$, the fraction of its length-normalized spectral count relative to the sum of length-normalized spectral counts across all detected proteins in the run. This fraction is commonly known as the Normalized Spectral Abundance Factor (NSAF), although you should derive the expression from the stated assumptions rather than invoking any pre-memorized formula.\n\nGiven a single tumor biopsy run with the following proteins, lengths $L_i$ (in amino acids), and spectral counts $SpC_i$:\n- Protein $1$: $L_1 = 450$, $SpC_1 = 120$.\n- Protein $2$: $L_2 = 1100$, $SpC_2 = 150$.\n- Protein $3$: $L_3 = 300$, $SpC_3 = 80$.\n- Protein $4$: $L_4 = 800$, $SpC_4 = 60$.\n- Protein $5$: $L_5 = 1600$, $SpC_5 = 200$.\n\nTasks:\n1. Starting only from the assumption $SpC_i \\approx k\\,A_i\\,L_i$, derive a closed-form expression for the fraction described above for a specific protein $i$ in terms of the observable $SpC_j$ and $L_j$ over all proteins $j$.\n2. Using your derived expression, compute this fraction for Protein $2$. Express your final result as a decimal fraction and round to four significant figures.\n3. Briefly discuss, in no more than three sentences, two mechanistic sources of bias that can affect this length-normalized spectral counting approach in comparison to precursor ion intensity at MS1-based quantification in proteomics, linking each bias to an underlying physical or biochemical cause.\n\nReport only the numerical value requested in task $2$ as your final answer. The quantity is dimensionless; express it as a decimal fraction rounded to four significant figures.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of proteomics, well-posed, and objective. It contains all necessary data and definitions to proceed with a solution.\n\n### Task 1: Derivation of the Expression\n\nThe problem asks for an expression for the \"fraction of its length-normalized spectral count relative to the sum of length-normalized spectral counts across all detected proteins.\" Let us formalize this statement.\n\nLet the set of all $N$ detected proteins in the experiment be indexed by $j=1, 2, \\dots, N$. For any given protein $i$ in this set, we are given its spectral count, $SpC_i$, and its length in amino acids, $L_i$.\n\nThe \"length-normalized spectral count\" for protein $i$ is defined as the ratio of its spectral count to its length. Let us denote this quantity as $q_i$:\n$$q_i = \\frac{SpC_i}{L_i}$$\n\nThe \"sum of length-normalized spectral counts across all detected proteins\" is the summation of $q_j$ over all proteins $j=1, \\dots, N$:\n$$\\sum_{j=1}^{N} q_j = \\sum_{j=1}^{N} \\frac{SpC_j}{L_j}$$\n\nThe desired fraction for protein $i$, which we will call the Normalized Spectral Abundance Factor ($NSAF_i$), is the ratio of its length-normalized spectral count to this sum:\n$$NSAF_i = \\frac{q_i}{\\sum_{j=1}^{N} q_j} = \\frac{\\frac{SpC_i}{L_i}}{\\sum_{j=1}^{N} \\frac{SpC_j}{L_j}}$$\nThis is the closed-form expression derived directly from the problem's definition.\n\nIt is instructive to relate this back to the foundational model provided: $SpC_i \\approx k\\,A_i\\,L_i$. By rearranging this approximation, we can express the molar abundance $A_i$ in terms of the observables:\n$$A_i \\approx \\frac{1}{k} \\frac{SpC_i}{L_i} = \\frac{1}{k} q_i$$\nThe total molar abundance of all proteins is $\\sum_{j=1}^{N} A_j \\approx \\sum_{j=1}^{N} \\frac{1}{k} q_j = \\frac{1}{k} \\sum_{j=1}^{N} q_j$.\nThe relative molar abundance of protein $i$ is the fraction of its abundance over the total abundance:\n$$\\frac{A_i}{\\sum_{j=1}^{N} A_j} \\approx \\frac{\\frac{1}{k} q_i}{\\frac{1}{k} \\sum_{j=1}^{N} q_j} = \\frac{q_i}{\\sum_{j=1}^{N} q_j} = NSAF_i$$\nThus, the derived expression represents an estimate for the relative molar abundance of protein $i$.\n\n### Task 2: Calculation for Protein 2\n\nWe are asked to compute $NSAF_2$ for a dataset of $N=5$ proteins. The expression is:\n$$NSAF_2 = \\frac{\\frac{SpC_2}{L_2}}{\\sum_{j=1}^{5} \\frac{SpC_j}{L_j}}$$\nFirst, we compute the length-normalized spectral count, $q_j = \\frac{SpC_j}{L_j}$, for each of the $5$ proteins:\n$q_1 = \\frac{SpC_1}{L_1} = \\frac{120}{450} = \\frac{12}{45} = \\frac{4}{15}$\n$q_2 = \\frac{SpC_2}{L_2} = \\frac{150}{1100} = \\frac{15}{110} = \\frac{3}{22}$\n$q_3 = \\frac{SpC_3}{L_3} = \\frac{80}{300} = \\frac{8}{30} = \\frac{4}{15}$\n$q_4 = \\frac{SpC_4}{L_4} = \\frac{60}{800} = \\frac{6}{80} = \\frac{3}{40}$\n$q_5 = \\frac{SpC_5}{L_5} = \\frac{200}{1600} = \\frac{2}{16} = \\frac{1}{8}$\n\nNext, we compute the sum of these values for the denominator:\n$$\\sum_{j=1}^{5} q_j = q_1 + q_2 + q_3 + q_4 + q_5 = \\frac{4}{15} + \\frac{3}{22} + \\frac{4}{15} + \\frac{3}{40} + \\frac{1}{8}$$\nTo sum these fractions, we find a common denominator for $15$, $22$, $40$, and $8$. The prime factorizations are $15 = 3 \\times 5$, $22 = 2 \\times 11$, $40 = 2^3 \\times 5$, and $8 = 2^3$. The least common multiple is $2^3 \\times 3 \\times 5 \\times 11 = 8 \\times 3 \\times 5 \\times 11 = 1320$.\n$$\\sum_{j=1}^{5} q_j = \\frac{4 \\times 88}{1320} + \\frac{3 \\times 60}{1320} + \\frac{4 \\times 88}{1320} + \\frac{3 \\times 33}{1320} + \\frac{1 \\times 165}{1320}$$\n$$\\sum_{j=1}^{5} q_j = \\frac{352 + 180 + 352 + 99 + 165}{1320} = \\frac{1148}{1320}$$\n\nNow we can calculate $NSAF_2$:\n$$NSAF_2 = \\frac{q_2}{\\sum_{j=1}^{5} q_j} = \\frac{\\frac{3}{22}}{\\frac{1148}{1320}} = \\frac{3}{22} \\times \\frac{1320}{1148}$$\nSince $1320 = 22 \\times 60$, we can simplify:\n$$NSAF_2 = \\frac{3}{22} \\times \\frac{22 \\times 60}{1148} = \\frac{3 \\times 60}{1148} = \\frac{180}{1148}$$\nSimplifying the fraction by dividing the numerator and denominator by $4$:\n$$NSAF_2 = \\frac{45}{287}$$\nFinally, we compute the decimal value and round to four significant figures:\n$$NSAF_2 = \\frac{45}{287} \\approx 0.156794425...$$\nRounding to four significant figures gives $0.1568$.\n\n### Task 3: Discussion of Biases\n\nTwo primary biases affect length-normalized spectral counting relative to MS1-based precursor intensity methods. First, the stochastic Data-Dependent Acquisition (DDA) sampling of precursors for MS/MS leads to undersampling of less abundant peptides, introducing sampling bias and variance not seen in comprehensive MS1 scans. Second, the method incorrectly assumes all peptides have uniform ionization efficiency and detectability, ignoring major physicochemical variations that MS1 approaches can mitigate by tracking specific, well-behaved proteotypic peptides.", "answer": "$$\n\\boxed{0.1568}\n$$", "id": "4362816"}, {"introduction": "Moving from quantification to statistical inference, this exercise demonstrates how information from multiple omics layers can be integrated to increase confidence in genomic variant detection. You will use a log-likelihood ratio test, a cornerstone of statistical hypothesis testing, to formally weigh the evidence for a true subclonal variant against the null hypothesis of sequencing error. This practice showcases a powerful application of the multi-omics paradigm: using parameters like tumor purity ($\\pi$) and clonal fraction ($c$), often estimated from transcriptomic or proteomic data, to build a more accurate statistical model for interpreting genomic data. [@problem_id:4362814]", "problem": "In a precision oncology setting, a multi-omics workflow integrates information from genomics, transcriptomics, proteomics, and metabolomics to refine variant detection and interpretation. Consider a diploid locus in a tumor sample profiled by high-depth targeted DNA sequencing. You are given the following:\n- DNA read counts at the locus: reference allele reads $r$ and alternate allele reads $a$.\n- An empirically estimated per-base sequencing error rate $\\epsilon$ derived from quality control using control DNA and corroborated by orthogonal Liquid Chromatography–Mass Spectrometry (LC–MS) proteomics calibration of error-prone contexts.\n- Tumor purity $\\pi$ estimated by an integrative proteogenomic deconvolution that combines RNA sequencing (RNA-seq) cell-type deconvolution with protein abundance signatures.\n- A subclonal cancer cell fraction $c$ harboring the variant inferred from single-cell RNA sequencing and copy-number-aware clonal reconstruction.\n\nAssume the following modeling principles as the fundamental base:\n- Central Dogma of Molecular Biology: deoxyribonucleic acid (DNA) encodes ribonucleic acid (RNA), which encodes protein. The presence of a coding DNA variant is necessary (though not sufficient) for the corresponding protein alteration.\n- Sequencing read generation is modeled as independent Bernoulli trials conditional on the true allele fraction.\n- Under the genotype $G=\\text{reference}$, any observed alternate reads arise from sequencing errors at probability $\\epsilon$ per read.\n- Under the genotype $G=\\text{variant}$, the variant is heterozygous and present only in a subclone that constitutes a fraction $c$ of the tumor cells within a tumor purity $\\pi$ mixture, with total diploid copy number and no allelic bias. The expected alternate-allele fraction under this model is therefore $f=\\frac{1}{2}\\pi c$.\n\nGiven the observed counts $r=178$ and $a=22$ (so the total number of reads is $n=r+a$), $\\epsilon=1.5\\times 10^{-3}$, $\\pi=0.64$, and $c=0.35$, compute the natural-log likelihood ratio\n$$\n\\ln\\frac{P(D\\mid G=\\text{variant})}{P(D\\mid G=\\text{reference})},\n$$\nwhere $D$ denotes the observed reads, using the Binomial likelihood for $P(D\\mid G)$ under each hypothesis as implied by the assumptions above.\n\nReport your final score as a pure number (no units) and round to four significant figures. Additionally, in your reasoning, explain how this score is calibrated in terms of evidential strength for the variant model under equal priors, but do not include this interpretation in the final numeric answer.", "solution": "The problem statement is critically validated before attempting a solution.\n\n### Step 1: Extract Givens\nThe data, variables, and assumptions provided in the problem statement are as follows:\n- DNA read counts: reference allele reads $r=178$, alternate allele reads $a=22$.\n- Total number of reads: $n = r + a$.\n- Per-base sequencing error rate: $\\epsilon = 1.5 \\times 10^{-3}$.\n- Tumor purity: $\\pi = 0.64$.\n- Subclonal cancer cell fraction with the variant: $c = 0.35$.\n- Model for sequencing reads: Independent Bernoulli trials, leading to a Binomial distribution for counts.\n- Hypothesis $G=\\text{reference}$: The probability of observing an alternate read is the sequencing error rate, $\\epsilon$.\n- Hypothesis $G=\\text{variant}$: The variant is heterozygous, present in a subclone of fraction $c$ within the tumor cell population of purity $\\pi$. The expected alternate-allele fraction is explicitly defined as $f = \\frac{1}{2}\\pi c$.\n- Task: Compute the natural-log likelihood ratio $\\ln\\frac{P(D\\mid G=\\text{variant})}{P(D\\mid G=\\text{reference})}$, where $D$ represents the observed read counts $(r, a)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria:\n- **Scientifically Grounded**: The problem is set in the context of precision oncology and multi-omics, using standard concepts like tumor purity, clonality, sequencing error, and allele fractions. The models (Binomial likelihood for read counts) and statistical methods (likelihood ratio test) are fundamental to computational biology and bioinformatics. The scenario is realistic and scientifically sound.\n- **Well-Posed**: The problem provides all necessary data and defines the models for the two hypotheses clearly. The objective is to compute a specific, well-defined quantity. A unique, stable, and meaningful solution exists.\n- **Objective**: The problem is stated using precise, quantitative language. The models and parameters are explicitly defined, leaving no room for subjective interpretation.\n- **Completeness**: The problem is self-contained. All values ($r, a, \\epsilon, \\pi, c$) and model definitions required for the calculation are provided.\n- **Consistency**: The givens and assumptions are internally consistent. The formula $f=\\frac{1}{2}\\pi c$ is a standard and correct model for a heterozygous diploid variant in a subclonal population within a mixed sample, assuming no copy number alterations.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid as it is scientifically grounded, well-posed, objective, and self-contained. Therefore, a complete solution will be provided.\n\n### Solution\nThe objective is to calculate the natural-log likelihood ratio for two competing hypotheses about the genotype at a specific genomic locus, given the observed DNA sequencing data. The data consists of $a=22$ alternate allele reads and $r=178$ reference allele reads, for a total of $n = a+r = 22+178 = 200$ reads.\n\nThe observation of $a$ alternate reads in $n$ total reads is modeled by a Binomial distribution. The probability mass function for observing $a$ successes in $n$ trials with a success probability of $p$ is:\n$$\nP(a \\mid n, p) = \\binom{n}{a} p^a (1-p)^{n-a}\n$$\nHere, a \"success\" is observing an alternate allele read. The data $D$ corresponds to the observed counts $(a, r)$.\n\n**Hypothesis 1: $G=\\text{reference}$ (Null Hypothesis)**\nUnder this hypothesis, the true genotype is homozygous reference. Any observed alternate reads are the result of sequencing errors. The problem states that the probability of observing an alternate read is the error rate, $\\epsilon$.\nLet $p_0$ be the probability of an alternate read under the reference model.\n$$\np_0 = \\epsilon = 1.5 \\times 10^{-3}\n$$\nThe likelihood of the data under the reference hypothesis is:\n$$\nP(D \\mid G=\\text{reference}) = \\binom{n}{a} p_0^a (1-p_0)^{n-a} = \\binom{200}{22} \\epsilon^{22} (1-\\epsilon)^{178}\n$$\n\n**Hypothesis 2: $G=\\text{variant}$ (Alternative Hypothesis)**\nUnder this hypothesis, a heterozygous variant exists in a fraction $c$ of the tumor cells, which themselves constitute a fraction $\\pi$ of the total sample. For a diploid locus with no copy number changes or allelic bias, one of the two alleles is the variant. The problem defines the expected alternate-allele fraction, $f$, in the entire DNA sample as:\n$$\nf = \\frac{1}{2} \\pi c\n$$\nThis value $f$ represents the probability of drawing a DNA molecule with the alternate allele from the sequenced pool. The problem implies we should use this directly as the probability parameter for the Binomial model under the variant hypothesis.\nLet $p_1$ be the probability of an alternate read under the variant model.\n$$\np_1 = f = \\frac{1}{2} \\pi c\n$$\nSubstituting the given values for $\\pi$ and $c$:\n$$\np_1 = \\frac{1}{2} (0.64) (0.35) = 0.32 \\times 0.35 = 0.112\n$$\nThe likelihood of the data under the variant hypothesis is:\n$$\nP(D \\mid G=\\text{variant}) = \\binom{n}{a} p_1^a (1-p_1)^{n-a} = \\binom{200}{22} f^{22} (1-f)^{178}\n$$\n\n**Likelihood Ratio Calculation**\nThe likelihood ratio is the ratio of the likelihoods under the two hypotheses:\n$$\n\\frac{P(D \\mid G=\\text{variant})}{P(D \\mid G=\\text{reference})} = \\frac{\\binom{200}{22} f^{22} (1-f)^{178}}{\\binom{200}{22} \\epsilon^{22} (1-\\epsilon)^{178}}\n$$\nThe binomial coefficient $\\binom{200}{22}$ cancels out:\n$$\n\\frac{P(D \\mid G=\\text{variant})}{P(D \\mid G=\\text{reference})} = \\frac{f^{22} (1-f)^{178}}{\\epsilon^{22} (1-\\epsilon)^{178}} = \\left(\\frac{f}{\\epsilon}\\right)^{22} \\left(\\frac{1-f}{1-\\epsilon}\\right)^{178}\n$$\nThe problem asks for the natural-log likelihood ratio, which we denote as $\\mathcal{L}$:\n$$\n\\mathcal{L} = \\ln\\left( \\frac{P(D \\mid G=\\text{variant})}{P(D \\mid G=\\text{reference})} \\right) = \\ln\\left[ \\left(\\frac{f}{\\epsilon}\\right)^{a} \\left(\\frac{1-f}{1-\\epsilon}\\right)^{n-a} \\right]\n$$\nUsing the properties of logarithms, this becomes:\n$$\n\\mathcal{L} = a \\ln\\left(\\frac{f}{\\epsilon}\\right) + (n-a) \\ln\\left(\\frac{1-f}{1-\\epsilon}\\right)\n$$\n$$\n\\mathcal{L} = a (\\ln(f) - \\ln(\\epsilon)) + (n-a) (\\ln(1-f) - \\ln(1-\\epsilon))\n$$\nNow, we substitute the numerical values:\n- $a = 22$\n- $n-a = r = 178$\n- $f = 0.112$\n- $\\epsilon = 0.0015$\n\nThe terms are:\n- $1-f = 1 - 0.112 = 0.888$\n- $1-\\epsilon = 1 - 0.0015 = 0.9985$\n\nSubstituting these into the expression for $\\mathcal{L}$:\n$$\n\\mathcal{L} = 22 \\ln\\left(\\frac{0.112}{0.0015}\\right) + 178 \\ln\\left(\\frac{0.888}{0.9985}\\right)\n$$\nLet's compute the values of the ratios first:\n$$\n\\frac{0.112}{0.0015} \\approx 74.6666...\n$$\n$$\n\\frac{0.888}{0.9985} \\approx 0.889334...\n$$\nNow, compute the natural logarithms:\n$$\n\\ln(74.6666...) \\approx 4.3129517\n$$\n$$\n\\ln(0.889334...) \\approx -0.1172803\n$$\nFinally, compute $\\mathcal{L}$:\n$$\n\\mathcal{L} \\approx 22 \\times (4.3129517) + 178 \\times (-0.1172803)\n$$\n$$\n\\mathcal{L} \\approx 94.8849374 - 20.8759040\n$$\n$$\n\\mathcal{L} \\approx 74.0090334\n$$\nRounding the result to four significant figures gives $74.01$.\n\n**Interpretation of the Evidential Strength**\nThe computed value, $\\mathcal{L} = 74.01$, is the natural logarithm of the likelihood ratio. This quantity is also known as the log-Bayes factor, assuming the models have no free parameters to be integrated out. With equal prior probabilities for the two hypotheses ($P(G=\\text{variant}) = P(G=\\text{reference})$), the posterior odds equal the likelihood ratio (or Bayes factor).\n$$\n\\frac{P(G=\\text{variant} \\mid D)}{P(G=\\text{reference} \\mid D)} = \\frac{P(D \\mid G=\\text{variant})}{P(D \\mid G=\\text{reference})} \\times \\frac{P(G=\\text{variant})}{P(G=\\text{reference})} = \\exp(\\mathcal{L}) \\times 1\n$$\nThe posterior odds in favor of the variant model are $\\exp(74.01)$, which is an astronomically large number (approximately $1.16 \\times 10^{32}$). This indicates that the observed data are overwhelmingly more probable under the variant model than under the reference (error-only) model. Established scales for interpreting Bayes factors, such as the Kass-Raftery scale, categorize a log-Bayes factor greater than $5$ as \"very strong\" evidence. A value of $74.01$ provides exceptionally strong evidence supporting the hypothesis that a real subclonal variant exists, as opposed to the reads arising from sequencing artifacts alone.", "answer": "$$\\boxed{74.01}$$", "id": "4362814"}, {"introduction": "Our final practice addresses a key objective in precision medicine: moving from statistical association to potential causation. When a genome-wide association study (GWAS) implicates a genomic region, you will learn how to use Bayesian statistical methods to pinpoint the most likely causal variant among several candidates that are correlated due to linkage disequilibrium (LD). By calculating approximate Bayes factors and posterior probabilities of causality, you will construct a credible set of variants, a crucial step in prioritizing targets for experimental validation and understanding disease mechanisms. [@problem_id:4362854]", "problem": "A locus implicated by a Genome-Wide Association Study (GWAS) in a precision medicine context contains $6$ Single Nucleotide Polymorphisms (SNPs). The study reports per-variant marginal effect estimates $\\hat{\\beta}_{j}$ with standard errors $s_{j}$ for a standardized continuous phenotype. Fine-mapping seeks to identify the causal variant(s), but the presence of linkage disequilibrium (LD) complicates interpretation: linkage disequilibrium (LD) is the non-random association of alleles at different loci, causing correlation among SNP genotypes and, consequently, correlation among their marginal association statistics. High LD inflates association signals for non-causal variants correlated with a causal one, thereby broadening the set of plausible causal candidates.\n\nAssume the following:\n- There is at most one causal variant in this locus.\n- For a given SNP $j$, the large-sample likelihood for the marginal effect estimate is $\\hat{\\beta}_{j} \\sim \\mathcal{N}(\\beta_{j}, V_{j})$ with $V_{j} = s_{j}^{2}$.\n- Under the alternative model, the causal effect has a prior $\\beta_{j} \\sim \\mathcal{N}(0, W)$ with $W = 0.04$. Under the null, $\\beta_{j} = 0$.\n- The prior probability on which SNP is causal is uniform across the $6$ SNPs, and you should condition on there being exactly one causal variant in the locus when computing posterior probabilities.\n\nUse Bayes’ theorem from first principles and the normal-normal conjugacy to derive the approximate Bayes factor for each SNP as the ratio of the marginal likelihood under the alternative model to that under the null model, expressed in terms of $\\hat{\\beta}_{j}$, $s_{j}$, and $W$. Then compute the posterior probability for each SNP being the unique causal variant by normalizing these Bayes factors across the $6$ SNPs.\n\nData:\n- SNP $1$: $\\hat{\\beta}_{1} = 0.20$, $s_{1} = 0.05$.\n- SNP $2$: $\\hat{\\beta}_{2} = 0.18$, $s_{2} = 0.05$.\n- SNP $3$: $\\hat{\\beta}_{3} = 0.03$, $s_{3} = 0.05$.\n- SNP $4$: $\\hat{\\beta}_{4} = 0.12$, $s_{4} = 0.05$.\n- SNP $5$: $\\hat{\\beta}_{5} = 0.19$, $s_{5} = 0.05$.\n- SNP $6$: $\\hat{\\beta}_{6} = -0.02$, $s_{6} = 0.05$.\n\nConstruct the minimal $0.95$ credible set by sorting SNPs in descending order of their posterior probabilities and taking the smallest prefix whose cumulative posterior probability is at least $0.95$. Report only the size $k$ (an integer) of this credible set. No rounding is required beyond exact arithmetic, and no units are needed.", "solution": "The problem statement is deemed valid. It describes a standard procedure in statistical genetics for fine-mapping a causal variant from Genome-Wide Association Study (GWAS) data. The assumptions, including a single causal variant, specific prior distributions, and the use of an approximate Bayes factor, are common simplifying assumptions in the field, making the problem well-posed and scientifically grounded within its context.\n\nThe task is to determine the size of the minimal $95\\%$ credible set of causal Single Nucleotide Polymorphisms (SNPs). This involves three main steps:\n1. Deriving the approximate Bayes factor (BF) for each SNP.\n2. Calculating the posterior probability of causality (PP) for each SNP.\n3. Constructing the credible set based on the calculated posterior probabilities.\n\n**1. Derivation of the Approximate Bayes Factor**\n\nFor each SNP $j$, we consider two hypotheses:\n- $M_j$: SNP $j$ is the causal variant, with its true effect size $\\beta_j$ drawn from a prior distribution.\n- $M_0$: SNP $j$ is not causal, meaning its true effect size is $\\beta_j=0$.\n\nThe problem provides the following distributional assumptions:\n- The likelihood of the observed effect estimate $\\hat{\\beta}_j$ is given by $\\hat{\\beta}_j | \\beta_j \\sim \\mathcal{N}(\\beta_j, V_j)$, where $V_j = s_j^2$.\n- The prior for the effect size under $M_j$ is $\\beta_j \\sim \\mathcal{N}(0, W)$.\n\nThe Bayes factor, $BF_j$, is the ratio of the marginal likelihood of the data under $M_j$ to the likelihood of the data under $M_0$.\n\n$$BF_j = \\frac{p(\\hat{\\beta}_j | M_j)}{p(\\hat{\\beta}_j | M_0)}$$\n\nFirst, the likelihood under the null hypothesis $M_0$ (where $\\beta_j = 0$) is:\n$$p(\\hat{\\beta}_j | M_0) = \\frac{1}{\\sqrt{2\\pi V_j}} \\exp\\left(-\\frac{\\hat{\\beta}_j^2}{2V_j}\\right)$$\n\nNext, we find the marginal likelihood under the alternative hypothesis $M_j$ by integrating the product of the likelihood and the prior for $\\beta_j$:\n$$p(\\hat{\\beta}_j | M_j) = \\int_{-\\infty}^{\\infty} p(\\hat{\\beta}_j | \\beta_j) p(\\beta_j | M_j) d\\beta_j$$\n$$p(\\hat{\\beta}_j | M_j) = \\int_{-\\infty}^{\\infty} \\left[\\frac{1}{\\sqrt{2\\pi V_j}} \\exp\\left(-\\frac{(\\hat{\\beta}_j - \\beta_j)^2}{2V_j}\\right)\\right] \\left[\\frac{1}{\\sqrt{2\\pi W}} \\exp\\left(-\\frac{\\beta_j^2}{2W}\\right)\\right] d\\beta_j$$\nThis is a standard convolution of two Gaussian probability densities, which results in another Gaussian. The mean of the resulting distribution for $\\hat{\\beta}_j$ is the sum of the means ($0+0=0$), and the variance is the sum of the variances ($V_j+W$). Therefore, the marginal likelihood is:\n$$p(\\hat{\\beta}_j | M_j) = \\frac{1}{\\sqrt{2\\pi(V_j + W)}} \\exp\\left(-\\frac{\\hat{\\beta}_j^2}{2(V_j + W)}\\right)$$\n\nNow, we can compute the Bayes factor by taking the ratio:\n$$BF_j = \\frac{\\frac{1}{\\sqrt{2\\pi(V_j + W)}} \\exp\\left(-\\frac{\\hat{\\beta}_j^2}{2(V_j + W)}\\right)}{\\frac{1}{\\sqrt{2\\pi V_j}} \\exp\\left(-\\frac{\\hat{\\beta}_j^2}{2V_j}\\right)}$$\n$$BF_j = \\sqrt{\\frac{V_j}{V_j + W}} \\exp\\left(\\frac{\\hat{\\beta}_j^2}{2V_j} - \\frac{\\hat{\\beta}_j^2}{2(V_j + W)}\\right)$$\n$$BF_j = \\sqrt{\\frac{V_j}{V_j + W}} \\exp\\left(\\frac{\\hat{\\beta}_j^2}{2} \\left(\\frac{1}{V_j} - \\frac{1}{V_j + W}\\right)\\right)$$\n$$BF_j = \\sqrt{\\frac{V_j}{V_j + W}} \\exp\\left(\\frac{\\hat{\\beta}_j^2}{2} \\left(\\frac{V_j + W - V_j}{V_j(V_j + W)}\\right)\\right)$$\n$$BF_j = \\sqrt{\\frac{V_j}{V_j + W}} \\exp\\left(\\frac{\\hat{\\beta}_j^2 W}{2V_j(V_j + W)}\\right)$$\nSubstituting $V_j = s_j^2$, we arrive at the required expression:\n$$BF_j = \\sqrt{\\frac{s_j^2}{s_j^2 + W}} \\exp\\left(\\frac{\\hat{\\beta}_j^2 W}{2s_j^2(s_j^2 + W)}\\right)$$\nThis formula, often called the Wakefield approximate Bayes factor (ABF), allows calculation of a per-SNP BF using only its summary statistics.\n\n**2. Posterior Probability Calculation**\n\nThe problem states we should condition on there being exactly one causal variant among the $6$ SNPs, with a uniform prior probability for each SNP being causal, i.e., $P(C_j) = 1/6$ for $j=1, \\dots, 6$, where $C_j$ is the event that SNP $j$ is the causal one.\nThe posterior probability that SNP $j$ is causal, given the data, is given by Bayes' theorem:\n$$PP_j = P(C_j | \\text{data}) = \\frac{P(\\text{data} | C_j) P(C_j)}{\\sum_{k=1}^{6} P(\\text{data} | C_k) P(C_k)}$$\nAssuming the association statistics are independent—a common simplifying approximation that ignores linkage disequilibrium (LD)—the likelihood term $P(\\text{data} | C_j)$ is proportional to the Bayes factor $BF_j$. Since the prior probabilities $P(C_j)$ are uniform, they cancel out, leading to:\n$$PP_j \\propto BF_j$$\nNormalizing across all $6$ SNPs gives the posterior probabilities:\n$$PP_j = \\frac{BF_j}{\\sum_{k=1}^{6} BF_k}$$\n\n**3. Numerical Computation and Credible Set Construction**\n\nWe are given the following data:\n- $W = 0.04$\n- For all SNPs $j=1, \\dots, 6$, $s_j = 0.05$, so $s_j^2 = (0.05)^2 = 0.0025$.\n- Effect estimates $\\hat{\\beta}_j$:\n  - $\\hat{\\beta}_1 = 0.20$\n  - $\\hat{\\beta}_2 = 0.18$\n  - $\\hat{\\beta}_3 = 0.03$\n  - $\\hat{\\beta}_4 = 0.12$\n  - $\\hat{\\beta}_5 = 0.19$\n  - $\\hat{\\beta}_6 = -0.02$\n\nLet's compute the constant terms in the $BF_j$ formula.\n- The term in the square root is $\\sqrt{\\frac{s_j^2}{s_j^2 + W}} = \\sqrt{\\frac{0.0025}{0.0025 + 0.04}} = \\sqrt{\\frac{0.0025}{0.0425}} = \\sqrt{\\frac{25}{425}} = \\sqrt{\\frac{1}{17}}$.\n- The coefficient in the exponent is $\\frac{W}{2s_j^2(s_j^2 + W)} = \\frac{0.04}{2(0.0025)(0.0425)} = \\frac{0.04}{0.0002125} = \\frac{3200}{17}$.\nSo, $BF_j = \\sqrt{\\frac{1}{17}} \\exp\\left(\\frac{3200}{17} \\hat{\\beta}_j^2\\right)$.\n\nFor calculating the posterior probabilities $PP_j$, the constant factor $\\sqrt{1/17}$ will cancel. Thus, we only need to compute the exponential term for each SNP, which we denote $E_j$.\n$$E_j = \\exp\\left(\\frac{3200}{17} \\hat{\\beta}_j^2\\right)$$\n- SNP 1: $\\hat{\\beta}_1^2 = (0.20)^2 = 0.04 \\implies E_1 = \\exp\\left(\\frac{3200 \\times 0.04}{17}\\right) = \\exp\\left(\\frac{128}{17}\\right)$\n- SNP 2: $\\hat{\\beta}_2^2 = (0.18)^2 = 0.0324 \\implies E_2 = \\exp\\left(\\frac{3200 \\times 0.0324}{17}\\right) = \\exp\\left(\\frac{103.68}{17}\\right)$\n- SNP 3: $\\hat{\\beta}_3^2 = (0.03)^2 = 0.0009 \\implies E_3 = \\exp\\left(\\frac{3200 \\times 0.0009}{17}\\right) = \\exp\\left(\\frac{2.88}{17}\\right)$\n- SNP 4: $\\hat{\\beta}_4^2 = (0.12)^2 = 0.0144 \\implies E_4 = \\exp\\left(\\frac{3200 \\times 0.0144}{17}\\right) = \\exp\\left(\\frac{46.08}{17}\\right)$\n- SNP 5: $\\hat{\\beta}_5^2 = (0.19)^2 = 0.0361 \\implies E_5 = \\exp\\left(\\frac{3200 \\times 0.0361}{17}\\right) = \\exp\\left(\\frac{115.52}{17}\\right)$\n- SNP 6: $\\hat{\\beta}_6^2 = (-0.02)^2 = 0.0004 \\implies E_6 = \\exp\\left(\\frac{3200 \\times 0.0004}{17}\\right) = \\exp\\left(\\frac{1.28}{17}\\right)$\n\nApproximate values:\n- $E_1 \\approx \\exp(7.529) \\approx 1861.94$\n- $E_2 \\approx \\exp(6.099) \\approx 445.31$\n- $E_3 \\approx \\exp(0.169) \\approx 1.18$\n- $E_4 \\approx \\exp(2.711) \\approx 15.04$\n- $E_5 \\approx \\exp(6.795) \\approx 893.54$\n- $E_6 \\approx \\exp(0.075) \\approx 1.08$\n\nSum of these values: $S = \\sum_{k=1}^{6} E_k \\approx 1861.94 + 445.31 + 1.18 + 15.04 + 893.54 + 1.08 \\approx 3218.09$.\nThe posterior probabilities are $PP_j = E_j/S$:\n- $PP_1 \\approx 1861.94 / 3218.09 \\approx 0.5786$\n- $PP_5 \\approx 893.54 / 3218.09 \\approx 0.2777$\n- $PP_2 \\approx 445.31 / 3218.09 \\approx 0.1384$\n- $PP_4 \\approx 15.04 / 3218.09 \\approx 0.0047$\n- $PP_3 \\approx 1.18 / 3218.09 \\approx 0.0004$\n- $PP_6 \\approx 1.08 / 3218.09 \\approx 0.0003$\nNote that the ranking of posterior probabilities is determined by the magnitude of $\\hat{\\beta}_j$, so we sort the SNPs based on $|\\hat{\\beta}_j|$ in descending order: SNP 1, SNP 5, SNP 2, SNP 4, SNP 3, SNP 6.\n\nTo construct the minimal $0.95$ credible set, we sum the posterior probabilities in descending order until the sum is at least $0.95$:\n- Top 1 (SNP 1): Cumulative probability = $0.5786$\n- Top 2 (SNPs 1, 5): Cumulative probability = $0.5786 + 0.2777 = 0.8563$\n- Top 3 (SNPs 1, 5, 2): Cumulative probability = $0.8563 + 0.1384 = 0.9947$\n\nSince the cumulative probability for the top 2 SNPs ($0.8563$) is less than $0.95$, and the cumulative probability for the top 3 SNPs ($0.9947$) is greater than $0.95$, the minimal credible set that captures at least $95\\%$ of the posterior probability mass must contain these 3 SNPs.\n\nThe size of this credible set, $k$, is $3$.", "answer": "$$\\boxed{3}$$", "id": "4362854"}]}