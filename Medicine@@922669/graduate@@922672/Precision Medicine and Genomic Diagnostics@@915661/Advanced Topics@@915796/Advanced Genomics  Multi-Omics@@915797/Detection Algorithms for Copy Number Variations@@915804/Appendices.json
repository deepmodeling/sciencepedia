{"hands_on_practices": [{"introduction": "This first exercise goes to the heart of depth-of-coverage analysis by asking you to derive a decision threshold from a statistical model of sequencing noise. The Negative Binomial distribution is a more realistic model than the Poisson for read counts as it accounts for additional biological and technical variability, a phenomenon known as overdispersion. By completing this practice [@problem_id:4331520], you will gain a concrete understanding of how statistical theory is used to make rigorous, quantitative calls about copy number state from noisy NGS data.", "problem": "A targeted Next Generation Sequencing (NGS) gene panel is analyzed for copy number variation (CNV) at exon-level resolution using a depth-of-coverage approach. After standard size-factor and guanine-cytosine (GC) content normalization, the normalized patient counts and pooled-reference expected counts for four exons of a single gene are as follows:\n- Exon $1$: patient count $S_{1} = 985$, reference expected $\\mu_{1} = 1000$.\n- Exon $2$: patient count $S_{2} = 1260$, reference expected $\\mu_{2} = 1200$.\n- Exon $3$: patient count $S_{3} = 390$, reference expected $\\mu_{3} = 800$.\n- Exon $4$: patient count $S_{4} = 1520$, reference expected $\\mu_{4} = 1500$.\n\nAssume the following foundational model and conditions:\n- For a diploid exon after normalization, the patient’s normalized count $S_{i}$ is well modeled by a Negative Binomial (NB) distribution with mean $\\mu_{i}$ and variance $\\operatorname{Var}(S_{i}) = \\mu_{i} + \\phi \\mu_{i}^{2}$, where $\\phi$ is an overdispersion parameter arising from biological and technical variability.\n- The pooled-reference expected $\\mu_{i}$ is estimated from a large control cohort so that its uncertainty is negligible compared with the sample’s count variance.\n- The exon-level log-ratio is defined as $x_{i} = \\log_{2}\\!\\left(\\frac{S_{i}}{\\mu_{i}}\\right)$.\n- For detection, use a one-sided Type I error rate $\\alpha = 1.0 \\times 10^{-3}$ for calling a heterozygous deletion (lower tail only).\n- The overdispersion parameter after normalization is $\\phi = 0.03$.\n\nTasks:\n1) Compute the exon-level log-ratios $x_{i}$ for $i \\in \\{1,2,3,4\\}$.\n2) Starting from the stated NB variance model and using an appropriate first-order variance propagation for $x_{i}$ under the diploid state, derive the one-sided lower decision threshold $T_{3}$ (in $\\log_{2}$ copy ratio units) such that $\\mathbb{P}(x_{3} < T_{3} \\mid \\text{diploid}) = \\alpha$.\n3) Based on this threshold and the computed $x_{3}$, determine whether exon $3$ would be called as a heterozygous deletion.\n\nFor grading, report only the numerical value of $T_{3}$ in $\\log_{2}$ copy ratio units. Round your answer to four significant figures. Express the final threshold in $\\log_{2}$ copy ratio units (unitless).", "solution": "The user has provided a problem that requires validation before a solution can be attempted.\n\n### Step 1: Extract Givens\n-   Patient normalized count for Exon 1: $S_{1} = 985$\n-   Reference expected count for Exon 1: $\\mu_{1} = 1000$\n-   Patient normalized count for Exon 2: $S_{2} = 1260$\n-   Reference expected count for Exon 2: $\\mu_{2} = 1200$\n-   Patient normalized count for Exon 3: $S_{3} = 390$\n-   Reference expected count for Exon 3: $\\mu_{3} = 800$\n-   Patient normalized count for Exon 4: $S_{4} = 1520$\n-   Reference expected count for Exon 4: $\\mu_{4} = 1500$\n-   Distribution model for patient count $S_{i}$: Negative Binomial (NB) with mean $\\mu_{i}$.\n-   Variance model for $S_{i}$: $\\operatorname{Var}(S_{i}) = \\mu_{i} + \\phi \\mu_{i}^{2}$.\n-   Overdispersion parameter: $\\phi = 0.03$.\n-   Definition of exon-level log-ratio: $x_{i} = \\log_{2}\\!\\left(\\frac{S_{i}}{\\mu_{i}}\\right)$.\n-   One-sided Type I error rate for heterozygous deletion call: $\\alpha = 1.0 \\times 10^{-3}$.\n-   Condition: The uncertainty in the reference expected count $\\mu_{i}$ is negligible.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity:\n-   **Scientifically Grounded**: The problem is well-grounded in the field of genomic diagnostics. The use of a Negative Binomial distribution to model overdispersed sequencing read counts is a standard and widely accepted practice in bioinformatics. The variance structure $\\operatorname{Var}(S) = \\mu + \\phi \\mu^2$ is a common parameterization. The definition of log-ratio and its use for CNV detection is fundamental to the field.\n-   **Well-Posed**: The problem is well-posed. It provides all necessary parameters ($\\mu_3$, $\\phi$, $\\alpha$) and a clear definition of the statistic ($x_3$) to derive the required threshold ($T_3$). The tasks are sequentially logical, and a unique, stable, and meaningful solution can be determined. The use of variance propagation (delta method) is an appropriate and standard technique for this context.\n-   **Objective**: The problem is stated in objective, precise, and quantitative terms. There are no subjective or opinion-based statements.\n\nThe problem does not exhibit any of the flaws listed in the instructions (e.g., scientific unsoundness, incompleteness, contradiction, ambiguity). All values are plausible within the context of targeted sequencing.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be provided.\n\nThe problem asks for the numerical value of the one-sided lower decision threshold $T_{3}$ for exon $3$. This requires computing the variance of the log-ratio statistic $x_{3}$ under the null hypothesis (diploid state) and then finding the critical value corresponding to the specified Type I error rate $\\alpha$.\n\nFirst, we define the statistic of interest, the log-ratio for exon $3$, as a function of the random variable $S_{3}$:\n$$x_{3} = f(S_{3}) = \\log_{2}\\!\\left(\\frac{S_{3}}{\\mu_{3}}\\right)$$\nTo facilitate differentiation, we can express this using the natural logarithm:\n$$x_{3} = \\frac{\\ln(S_{3}/\\mu_{3})}{\\ln(2)} = \\frac{1}{\\ln(2)} (\\ln(S_{3}) - \\ln(\\mu_{3}))$$\nThe problem specifies using a first-order variance propagation, commonly known as the delta method. For a function $f(Y)$ of a random variable $Y$, the variance is approximated by:\n$$\\operatorname{Var}(f(Y)) \\approx [f'(\\mathbb{E}[Y])]^{2} \\operatorname{Var}(Y)$$\nIn our case, the random variable is $S_{3}$, its expectation under the null hypothesis is $\\mathbb{E}[S_{3}] = \\mu_{3}$, and the function is $f(S_3) = x_3$. We need the derivative of $f(S_{3})$ with respect to $S_{3}$:\n$$\\frac{df}{dS_{3}} = \\frac{d}{dS_{3}}\\left[ \\frac{1}{\\ln(2)} (\\ln(S_{3}) - \\ln(\\mu_{3})) \\right] = \\frac{1}{\\ln(2)} \\cdot \\frac{1}{S_{3}}$$\nWe evaluate this derivative at the expected value $\\mathbb{E}[S_{3}] = \\mu_{3}$:\n$$f'(\\mu_{3}) = \\frac{1}{\\mu_{3} \\ln(2)}$$\nNow we apply the variance propagation formula:\n$$\\operatorname{Var}(x_{3}) \\approx \\left(\\frac{1}{\\mu_{3} \\ln(2)}\\right)^{2} \\operatorname{Var}(S_{3})$$\nThe problem states the variance of $S_{3}$ is $\\operatorname{Var}(S_{3}) = \\mu_{3} + \\phi \\mu_{3}^{2}$. Substituting this into the expression for $\\operatorname{Var}(x_{3})$:\n$$\\operatorname{Var}(x_{3}) \\approx \\frac{1}{\\mu_{3}^{2} (\\ln(2))^{2}} (\\mu_{3} + \\phi \\mu_{3}^{2}) = \\frac{\\mu_{3}}{\\mu_{3}^{2} (\\ln(2))^{2}} + \\frac{\\phi \\mu_{3}^{2}}{\\mu_{3}^{2} (\\ln(2))^{2}}$$\nSimplifying this expression gives the variance of the log-ratio:\n$$\\operatorname{Var}(x_{3}) \\approx \\frac{1}{\\mu_{3} (\\ln(2))^{2}} + \\frac{\\phi}{(\\ln(2))^{2}} = \\frac{1}{(\\ln(2))^{2}} \\left( \\frac{1}{\\mu_{3}} + \\phi \\right)$$\nFor large counts, the distribution of the log-ratio $x_3$ can be approximated by a normal distribution. Under the null hypothesis of a diploid state, the expected ratio $S_3/\\mu_3$ is $1$, so the expected log-ratio is $\\mathbb{E}[x_{3}] \\approx \\log_{2}(\\mathbb{E}[S_3]/\\mu_3) = \\log_{2}(1) = 0$.\nThus, under the null hypothesis, $x_{3} \\sim \\mathcal{N}(0, \\operatorname{Var}(x_{3}))$.\n\nWe need to find the threshold $T_{3}$ such that the probability of observing a log-ratio less than $T_{3}$ is equal to the specified Type I error rate $\\alpha$:\n$$\\mathbb{P}(x_{3} < T_{3} \\mid \\text{diploid}) = \\alpha$$\nTo find $T_{3}$, we standardize the variable $x_{3}$ to a standard normal variable $Z \\sim \\mathcal{N}(0,1)$:\n$$Z = \\frac{x_{3} - \\mathbb{E}[x_{3}]}{\\sqrt{\\operatorname{Var}(x_{3})}} = \\frac{x_{3}}{\\sigma_{x_3}}$$\nwhere $\\sigma_{x_3} = \\sqrt{\\operatorname{Var}(x_{3})}$.\nThe condition becomes:\n$$\\mathbb{P}\\left(Z < \\frac{T_{3}}{\\sigma_{x_3}}\\right) = \\alpha$$\nThis implies that $\\frac{T_{3}}{\\sigma_{x_3}}$ is the $\\alpha$-quantile of the standard normal distribution, denoted $z_{\\alpha}$.\n$$T_{3} = z_{\\alpha} \\sigma_{x_3}$$\nWe are given the values $\\mu_{3} = 800$, $\\phi = 0.03$, and $\\alpha = 1.0 \\times 10^{-3} = 0.001$.\nThe standard deviation $\\sigma_{x_3}$ is:\n$$\\sigma_{x_3} = \\sqrt{\\frac{1}{(\\ln(2))^{2}} \\left( \\frac{1}{\\mu_{3}} + \\phi \\right)} = \\frac{1}{\\ln(2)} \\sqrt{\\frac{1}{800} + 0.03}$$\n$$\\sigma_{x_3} = \\frac{1}{\\ln(2)} \\sqrt{0.00125 + 0.03} = \\frac{\\sqrt{0.03125}}{\\ln(2)}$$\nThe $\\alpha$-quantile of the standard normal distribution for $\\alpha = 0.001$ is $z_{0.001} \\approx -3.09023$.\nNow we can compute the numerical value for $T_{3}$:\n$$T_{3} = z_{0.001} \\cdot \\sigma_{x_3} \\approx -3.09023 \\times \\frac{\\sqrt{0.03125}}{\\ln(2)}$$\n$$T_{3} \\approx -3.09023 \\times \\frac{0.1767767}{0.693147} \\approx -3.09023 \\times 0.255031$$\n$$T_{3} \\approx -0.788114$$\nRounding the result to four significant figures, we get $T_{3} = -0.7881$.\n\nWhile not required for the final answer, for context, the observed log-ratio for exon 3 is $x_{3} = \\log_{2}(390/800) = \\log_{2}(0.4875) \\approx -1.0365$. Since $x_3 < T_3$ (i.e., $-1.0365 < -0.7881$), exon 3 would be confidently called as a heterozygous deletion. The problem, however, only asks for the value of $T_3$.", "answer": "$$\\boxed{-0.7881}$$", "id": "4331520"}, {"introduction": "While analyzing individual genomic loci is a good start, true CNV events span contiguous regions, making segmentation a fundamental step in any detection pipeline. This practice challenges you to implement a dynamic programming algorithm to find the optimal piecewise-constant approximation of a noisy log-ratio signal, a classic problem in signal processing. This exercise [@problem_id:4331596] not only provides experience with a powerful optimization technique but also introduces the critical workflow of parameter tuning and performance evaluation, which are essential for developing and deploying robust bioinformatics tools.", "problem": "You are given multiple one-dimensional genomic coverage profiles, each modeled as a piecewise-constant signal corrupted by additive noise, representing log base $2$ copy ratio measurements for contiguous genomic bins. The fundamental base consists of the following well-tested model and definitions: for bin index $i \\in \\{1,\\dots,n\\}$, the observed value is $x_i = \\mu_{k(i)} + \\varepsilon_i$, where $\\mu_{k(i)}$ is the true mean level of the unknown segment $k(i)$ to which bin $i$ belongs and $\\varepsilon_i$ are independent mean-zero noise terms. Copy Number Variation (CNV) events correspond to contiguous intervals where the true mean deviates from the neutral level $0$.\n\nSegmentation is posed as minimizing an additive cost that balances data fidelity and model complexity. For a segmentation into $K$ contiguous segments with index sets $I_1,\\dots,I_K$ and segment means $\\mu_1,\\dots,\\mu_K$, define the objective\n$$\nJ = \\sum_{k=1}^{K}\\sum_{i \\in I_k} (x_i - \\mu_k)^2 + \\lambda \\cdot K,\n$$\nwhere $\\lambda > 0$ is a penalty parameter controlling the trade-off between fit and the number of segments. The segment mean that minimizes the within-segment squared error is the sample mean of that segment.\n\nFor a fixed $\\lambda$, the globally optimal segmentation can be found by dynamic programming. For evaluation, an altered segment call is any inferred segment whose mean magnitude satisfies $\\lvert \\mu_k \\rvert \\ge \\tau$, where $\\tau$ is a fixed amplitude threshold on the log base $2$ scale. To avoid artificial splitting, adjacent altered segments with the same sign and direct adjacency are merged into a single altered call.\n\nTo compute detection performance, you are given a set of ground-truth CNV intervals for each profile. A ground-truth interval $G$ is considered detected if there exists a predicted altered interval $P$ such that the Intersection over Union (IoU) given by\n$$\n\\mathrm{IoU}(G,P) = \\frac{\\lvert G \\cap P \\rvert}{\\lvert G \\cup P \\rvert}\n$$\nsatisfies $\\mathrm{IoU}(G,P) \\ge \\theta$, where $\\theta$ is a fixed threshold. Let $TP$ be the number of detected ground-truth intervals and $FP$ be the number of predicted altered intervals that do not match any ground-truth interval at IoU $\\ge \\theta$.\n\nYou must sweep a given set of penalty values $\\{\\lambda_1,\\dots,\\lambda_m\\}$, compute $(TP,FP)$ for each $\\lambda_j$, and select an operating point by maximizing the utility\n$$\nU(\\lambda) = TP(\\lambda) - c \\cdot FP(\\lambda),\n$$\nwith a fixed cost ratio $c > 0$. In case of ties in $U(\\lambda)$, choose the largest $\\lambda$.\n\nYour program must implement this segmentation and evaluation procedure exactly as defined and apply it to the following test suite. Indices in intervals are $1$-based and inclusive. The log base $2$ copy ratio measurements $x_i$ for each dataset and the corresponding ground-truth CNV intervals are:\n\n- Common constants for all datasets:\n  - Penalty sweep values: $\\{\\lambda\\} = \\{\\, 0.1,\\, 0.5,\\, 1.0,\\, 2.0,\\, 5.0,\\, 10.0 \\,\\}$.\n  - Alteration calling threshold: $\\tau = 0.2$.\n  - Intersection over Union threshold: $\\theta = 0.3$.\n  - Utility cost ratio: $c = 1.0$.\n\n- Dataset $1$:\n  - $x = [\\, 0.02,\\,-0.03,\\,0.01,\\,-0.05,\\,0.00,\\,-0.55,\\,-0.62,\\,-0.59,\\,-0.60,\\,-0.57,\\,0.03,\\,-0.02,\\,0.04,\\,-0.01,\\,0.00,\\,0.02,\\,-0.03,\\,0.05,\\,-0.02,\\,0.01 \\,]$.\n  - Ground truth intervals: $[\\, (6,10) \\,]$.\n\n- Dataset $2$:\n  - $x = [\\, 0.01,\\,-0.02,\\,0.02,\\,-0.03,\\,0.00,\\,0.03,\\,-0.01,\\,0.35,\\,0.42,\\,0.39,\\,0.41,\\,0.38,\\,0.00,\\,-0.01,\\,0.02,\\,-0.02,\\,0.01,\\,-0.95,\\,-1.02,\\,-0.98,\\,-1.01,\\,-0.97,\\,0.02,\\,-0.01,\\,0.03,\\,-0.02,\\,0.01,\\,0.00,\\,0.02,\\,-0.03 \\,]$.\n  - Ground truth intervals: $[\\, (8,12),\\,(18,22) \\,]$.\n\n- Dataset $3$:\n  - $x = [\\, 0.01,\\,-0.02,\\,0.00,\\,0.03,\\,-0.01,\\,0.02,\\,-0.03,\\,0.01,\\,-0.02,\\,0.02,\\,-0.01,\\,0.00,\\,0.03,\\,-0.02,\\,0.01 \\,]$.\n  - Ground truth intervals: $[\\, \\,]$ (no CNV).\n\n- Dataset $4$:\n  - $x = [\\, 0.52,\\,0.48,\\,0.50,\\,0.00,\\,-0.02,\\,0.03,\\,-0.01,\\,0.02,\\,-0.03,\\,0.01,\\,-0.02,\\,0.00,\\,-0.58,\\,-0.61,\\,-0.57,\\,-0.60 \\,]$.\n  - Ground truth intervals: $[\\, (1,3),\\,(13,16) \\,]$.\n\nYour tasks:\n\n- For each dataset and each $\\lambda$ in the sweep, compute the globally optimal segmentation that minimizes $J$ as defined above.\n- From the segmentation, call altered intervals using threshold $\\tau$ on the absolute segment mean, and merge adjacent altered calls with the same sign.\n- For each $\\lambda$, compute $(TP,FP)$ using IoU threshold $\\theta$ against the given ground truth.\n- For each dataset, select the operating point by maximizing $U(\\lambda) = TP(\\lambda) - c \\cdot FP(\\lambda)$, breaking ties by choosing the largest $\\lambda$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results for the $4$ datasets as a comma-separated list enclosed in square brackets. For each dataset, output the selected $\\lambda$ together with the corresponding $TP$ and $FP$ as a list of three numbers. The final output must therefore be a single line of the form\n  $[\\, [\\lambda^\\star_1, TP_1, FP_1],\\; [\\lambda^\\star_2, TP_2, FP_2],\\; [\\lambda^\\star_3, TP_3, FP_3],\\; [\\lambda^\\star_4, TP_4, FP_4] \\,]$,\n  where $\\lambda^\\star_d$ is the selected penalty for dataset $d$. No units are involved, and numbers must be printed in standard decimal form.", "solution": "The problem requires the implementation of a complete workflow for detecting Copy Number Variations (CNVs) in genomic data. This involves segmenting a noisy signal representing log-base-2 copy ratios, evaluating the segmentation against a ground truth, and selecting an optimal model configuration. The process is based on well-established principles of statistical signal processing and bioinformatics.\n\n### 1. The Model and Objective Function\n\nThe input data for each genomic profile is a one-dimensional sequence of measurements $x_1, x_2, \\dots, x_n$, where each $x_i$ represents the log-ratio of observed copy number to a reference, for genomic bin $i$. The underlying true signal is assumed to be piecewise-constant. An observed value $x_i$ is modeled as $x_i = \\mu_{k(i)} + \\varepsilon_i$, where $\\mu_{k(i)}$ is the true mean of the segment to which bin $i$ belongs, and $\\varepsilon_i$ is an independent, mean-zero noise term.\n\nThe task of segmentation is to partition the sequence $x_1, \\dots, x_n$ into $K$ contiguous segments $I_1, \\dots, I_K$. This partition is found by minimizing a penalized cost function $J$, which balances data fidelity with model complexity:\n$$\nJ = \\sum_{k=1}^{K}\\sum_{i \\in I_k} (x_i - \\mu_k)^2 + \\lambda \\cdot K\n$$\nThe first term, $\\sum_{k=1}^{K}\\sum_{i \\in I_k} (x_i - \\mu_k)^2$, is the total sum of squared errors between the observed data and the segment means. Minimizing this term alone would lead to overfitting, where each data point becomes its own segment. The second term, $\\lambda \\cdot K$, is a penalty proportional to the number of segments $K$. The parameter $\\lambda > 0$ controls the trade-off: a larger $\\lambda$ favors solutions with fewer segments, leadings to a coarser segmentation, while a smaller $\\lambda$ allows for more segments, fitting the data more closely. For any given segment, the mean $\\mu_k$ that minimizes the squared error is the sample mean of the data points within that segment.\n\n### 2. Optimal Segmentation via Dynamic Programming\n\nFor a fixed penalty $\\lambda$, the segmentation that globally minimizes the objective function $J$ can be found efficiently using dynamic programming. Let $J^*(i)$ denote the minimum cost for segmenting the data prefix $x_1, \\dots, x_i$. We can establish a recurrence relation to compute this value. The optimal segmentation of the first $i$ points is achieved by finding an optimal segmentation of the first $j-1$ points, followed by a final segment from $j$ to $i$, for some $1 \\le j \\le i$.\n\nThe recurrence relation is:\n$$\nJ^*(i) = \\min_{1 \\le j \\le i} \\left\\{ J^*(j-1) + C(j,i) + \\lambda \\right\\}\n$$\nwith the base case $J^*(0) = 0$. The term $C(j,i)$ represents the minimum sum of squared errors for a single segment covering data points from index $j$ to $i$:\n$$\nC(j,i) = \\sum_{k=j}^{i} (x_k - \\bar{x}_{j..i})^2, \\quad \\text{where } \\bar{x}_{j..i} = \\frac{1}{i-j+1} \\sum_{k=j}^{i} x_k\n$$\nA direct computation of this recurrence involves an $O(n^2)$ loop structure, with the cost $C(j,i)$ taking $O(n)$ time to compute, leading to an overall $O(n^3)$ complexity. However, we can optimize this. The term $C(j,i)$ can be rewritten as $\\sum_{k=j}^{i} x_k^2 - \\frac{1}{i-j+1}(\\sum_{k=j}^{i} x_k)^2$. By pre-computing prefix sums of both $x_k$ and $x_k^2$, the sums over any interval $[j,i]$ can be found in $O(1)$ time. This reduces the complexity of computing $J^*(n)$ to $O(n^2)$.\n\nTo reconstruct the actual segment boundaries, we store a pointer at each step $i$ to the starting index $j$ that yielded the minimum cost. After computing $J^*(n)$, we can backtrack from $i=n$ to $i=0$ using these pointers to recover the optimal segment boundaries.\n\n### 3. Post-processing and Performance Evaluation\n\nOnce the optimal segmentation is found for a given $\\lambda$, we perform several post-processing and evaluation steps.\n\n**Altered Interval Calling**: A segment $k$ with mean $\\mu_k$ is classified as an \"altered call\" if its mean's magnitude exceeds a specified threshold $\\tau$, i.e., $|\\mu_k| \\ge \\tau$. This step distinguishes segments representing true CNV gains or losses from those representing the neutral state (copy number $2$).\n\n**Merging**: To correct for artificial splits where a single biological event is broken into multiple adjacent segments by the algorithm, any adjacent altered segments with means of the same sign are merged into a single, contiguous predicted interval.\n\n**Performance Metrics**: The set of predicted altered intervals, $\\{P\\}$, is compared against a provided set of ground-truth intervals, $\\{G\\}$.\n- The overlap between a predicted interval $P$ and a ground-truth interval $G$ is quantified using the Intersection over Union (IoU):\n  $$\n  \\mathrm{IoU}(G,P) = \\frac{\\lvert G \\cap P \\rvert}{\\lvert G \\cup P \\rvert}\n  $$\n- A ground-truth interval $G$ is considered detected if there exists at least one predicted interval $P$ such that $\\mathrm{IoU}(G,P) \\ge \\theta$, where $\\theta$ is a minimum IoU threshold. The total number of such detected ground-truth intervals is the number of True Positives ($TP$).\n- A predicted interval $P$ is considered a False Positive ($FP$) if it does not match any ground-truth interval, i.e., $\\mathrm{IoU}(G,P) < \\theta$ for all $G$ in the ground-truth set.\n\n### 4. Model Selection\n\nThe entire procedure is repeated for a sweep of penalty values $\\{\\lambda_1, \\dots, \\lambda_m\\}$. For each $\\lambda_j$, we compute the corresponding pair $(TP(\\lambda_j), FP(\\lambda_j))$. To select the best operating point, we maximize the utility function:\n$$\nU(\\lambda) = TP(\\lambda) - c \\cdot FP(\\lambda)\n$$\nwhere $c$ is a constant that specifies the cost of a false positive relative to a true positive. Should multiple $\\lambda$ values yield the same maximum utility, the largest $\\lambda$ is chosen, following the principle of parsimony (favoring simpler models). The final result for each dataset is the selected triplet $[\\lambda^\\star, TP(\\lambda^\\star), FP(\\lambda^\\star)]$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the full CNV detection workflow:\n    1. Optimal segmentation via dynamic programming for a range of penalties.\n    2. Altered segment calling and merging.\n    3. Performance evaluation (TP, FP) using IoU.\n    4. Model selection based on a utility function.\n    \"\"\"\n    \n    # Common constants for all datasets\n    lambdas = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n    tau = 0.2\n    theta = 0.3\n    c = 1.0\n\n    # Test suite: data and ground truth intervals (1-based, inclusive)\n    test_suite = [\n        {\n            \"x\": [0.02, -0.03, 0.01, -0.05, 0.00, -0.55, -0.62, -0.59, -0.60, -0.57, 0.03, -0.02, 0.04, -0.01, 0.00, 0.02, -0.03, 0.05, -0.02, 0.01],\n            \"gt\": [(6, 10)],\n        },\n        {\n            \"x\": [0.01, -0.02, 0.02, -0.03, 0.00, 0.03, -0.01, 0.35, 0.42, 0.39, 0.41, 0.38, 0.00, -0.01, 0.02, -0.02, 0.01, -0.95, -1.02, -0.98, -1.01, -0.97, 0.02, -0.01, 0.03, -0.02, 0.01, 0.00, 0.02, -0.03],\n            \"gt\": [(8, 12), (18, 22)],\n        },\n        {\n            \"x\": [0.01, -0.02, 0.00, 0.03, -0.01, 0.02, -0.03, 0.01, -0.02, 0.02, -0.01, 0.00, 0.03, -0.02, 0.01],\n            \"gt\": [],\n        },\n        {\n            \"x\": [0.52, 0.48, 0.50, 0.00, -0.02, 0.03, -0.01, 0.02, -0.03, 0.01, -0.02, 0.00, -0.58, -0.61, -0.57, -0.60],\n            \"gt\": [(1, 3), (13, 16)],\n        },\n    ]\n\n    all_results = []\n\n    for dataset in test_suite:\n        x = np.array(dataset[\"x\"])\n        gt_intervals = dataset[\"gt\"]\n        n = len(x)\n\n        # Precompute prefix sums for O(1) segment cost calculation\n        cum_x = np.zeros(n + 1)\n        cum_x2 = np.zeros(n + 1)\n        cum_x[1:] = np.cumsum(x)\n        cum_x2[1:] = np.cumsum(x**2)\n\n        def get_segment_cost(i, j): # 0-based, inclusive\n            length = j - i + 1\n            if length == 0: return 0.0\n            sum_val = cum_x[j + 1] - cum_x[i]\n            sum_sq_val = cum_x2[j + 1] - cum_x2[i]\n            cost = sum_sq_val - (sum_val**2) / length\n            return cost\n\n        best_utility = -np.inf\n        best_result_for_dataset = None\n\n        for l_val in lambdas:\n            # Dynamic Programming for optimal segmentation\n            dp = np.full(n + 1, np.inf)\n            pointers = np.zeros(n + 1, dtype=int)\n            dp[0] = 0.0\n\n            for i in range(1, n + 1):\n                for j in range(i): # Last segment starts at j (0-based)\n                    cost = get_segment_cost(j, i - 1)\n                    current_total_cost = dp[j] + cost + l_val\n                    if current_total_cost  dp[i]:\n                        dp[i] = current_total_cost\n                        pointers[i] = j\n\n            # Backtrack to reconstruct segments\n            segments = []\n            curr_idx = n\n            while curr_idx > 0:\n                prev_idx = pointers[curr_idx]\n                mean_val = np.mean(x[prev_idx:curr_idx])\n                segments.append({'start': prev_idx + 1, 'end': curr_idx, 'mean': mean_val}) # 1-based\n                curr_idx = prev_idx\n            segments.reverse()\n\n            # Call and merge altered segments\n            altered_calls = [seg for seg in segments if abs(seg['mean']) >= tau]\n            \n            merged_calls = []\n            if altered_calls:\n                current_call = {'start': altered_calls[0]['start'], 'end': altered_calls[0]['end']}\n                for i in range(1, len(altered_calls)):\n                    prev = altered_calls[i-1]\n                    curr = altered_calls[i]\n                    if (curr['start'] == prev['end'] + 1 and \n                        np.sign(curr['mean']) == np.sign(prev['mean'])):\n                        current_call['end'] = curr['end']\n                    else:\n                        merged_calls.append(current_call)\n                        current_call = {'start': curr['start'], 'end': curr['end']}\n                merged_calls.append(current_call)\n\n            predicted_intervals = [(call['start'], call['end']) for call in merged_calls]\n\n            # Evaluate TP and FP\n            def calc_iou(g, p):\n                g_s, g_e = g\n                p_s, p_e = p\n                intersection_start = max(g_s, p_s)\n                intersection_end = min(g_e, p_e)\n                intersection_len = max(0, intersection_end - intersection_start + 1)\n                \n                if intersection_len == 0: return 0.0\n                \n                g_len = g_e - g_s + 1\n                p_len = p_e - p_s + 1\n                union_len = g_len + p_len - intersection_len\n                return intersection_len / union_len\n\n            # Calculate True Positives (TP)\n            tp = 0\n            if gt_intervals:\n                gt_detected = [False] * len(gt_intervals)\n                for p in predicted_intervals:\n                    for i, g in enumerate(gt_intervals):\n                        if calc_iou(g, p) >= theta:\n                            gt_detected[i] = True\n                tp = sum(gt_detected)\n            \n            # Calculate False Positives (FP)\n            fp = 0\n            if predicted_intervals:\n                for p in predicted_intervals:\n                    if not any(calc_iou(g, p) >= theta for g in gt_intervals):\n                        fp += 1\n            \n            # Maximize utility function; ties broken by largest lambda\n            utility = tp - c * fp\n            if utility >= best_utility:\n                best_utility = utility\n                best_result_for_dataset = [l_val, tp, fp]\n\n        all_results.append(best_result_for_dataset)\n\n    # Format the final output string\n    # Using \", \".join(map(str, ...)) produces the desired format with spaces\n    # Example: [[1.0, 1, 0], [2.0, 2, 0]]\n    formatted_results = \", \".join(map(str, all_results))\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```", "id": "4331596"}, {"introduction": "In many clinical contexts, particularly cancer genomics, determining the *absolute* integer copy number of a gene is more informative than just its relative change. This advanced exercise requires you to untangle the composite signal from a mixed tumor-normal sample by integrating read depth with B-allele frequency (BAF) data. By applying first-principles mixture models, this practice [@problem_id:4331590] builds the critical skill of inferring underlying biological parameters like tumor purity and absolute ploidy, which is essential for accurate interpretation of cancer genomes.", "problem": "A tumor sample is sequenced with a matched normal, and three disjoint genomic segments within a $150$ megabase (Mb) targeted region are identified with reliable read depth and single-nucleotide polymorphism allelic measurements. Assume the following foundational principles: read depth is proportional to the total mass of DNA, which is, in turn, proportional to total copy number, and the observed minor allele fraction arises from a mixture of tumor and normal contributions at heterozygous loci. The normal cells are diploid with total copy number $2$. Let tumor purity be the fraction $\\pi$ of tumor cells in the sample. Let the global library scaling factor be $k$, which accounts for sample-level effects on the normalized tumor-to-normal depth ratio. For segment $i$, let $L_i$ be its length, let $D_i$ be its normalized depth ratio (tumor over normal) after bias correction, and let $F_i$ be the observed minor allele fraction (computed from B-allele frequency (BAF) clusters by choosing the minor cluster). The three segments have the following measurements:\n- Segment $1$: $L_1 = 80$ Mb, $D_1 = 1.215$, $F_1 = 0.370$.\n- Segment $2$: $L_2 = 40$ Mb, $D_2 = 0.900$, $F_2 = 0.150$.\n- Segment $3$: $L_3 = 30$ Mb, $D_3 = 1.530$, $F_3 = 0.500$.\nAssume Segment $2$ exhibits loss of heterozygosity, Segment $3$ is allelically balanced, and Segment $1$ is imbalanced but not null for the minor allele. Using only first-principles arguments based on the mixture of tumor and normal contributions to read depth and minor allele fraction, infer the absolute integer tumor copy number state in each segment, estimate the tumor purity $\\pi$ and the global scaling factor $k$, and then compute the panel ploidy, defined as the length-weighted average tumor copy number across the $150$ Mb region:\n$$P \\equiv \\frac{L_1 \\, t_1 + L_2 \\, t_2 + L_3 \\, t_3}{L_1 + L_2 + L_3},$$\nwhere $t_i$ is the absolute tumor total copy number of segment $i$. Round your final $P$ to four significant figures and express it as a unitless quantity.", "solution": "We start from two foundational mixture relationships. First, read depth is proportional to the mass of DNA, which for a mixture of tumor and normal is proportional to the total copy number contributed by each population. If $t_i$ denotes the tumor total copy number in segment $i$, and the normal total copy number is $2$, then the expected normalized tumor-to-normal depth ratio in segment $i$ can be written as\n$$D_i = k \\cdot \\frac{\\pi \\, t_i + (1 - \\pi)\\cdot 2}{2},$$\nwhere $k$ is a global library scaling factor. This expression arises because the numerator reflects the mixture of tumor and normal copy numbers, and the denominator $2$ reflects normalization to diploid normal.\n\nSecond, the minor allele fraction among heterozygous loci arises from mixing allele-specific copy numbers. Let the tumor allele-specific copy numbers be $(a_i, b_i)$ with $a_i \\geq b_i$ and $a_i + b_i = t_i$. The normal contribution at a heterozygous locus is $(1,1)$. The expected minor allele fraction in segment $i$ is\n$$F_i = \\frac{\\pi \\, b_i + (1 - \\pi) \\cdot 1}{\\pi \\, t_i + (1 - \\pi) \\cdot 2}.$$\nWe use these relationships to infer $t_i$, $\\pi$, and $k$.\n\nWe are given qualitative constraints: Segment $2$ exhibits loss of heterozygosity (LOH), Segment $3$ is allelically balanced, and Segment $1$ is imbalanced but retains a nonzero minor allele. LOH implies $b_2 = 0$ and $t_2 \\geq 1$. Because Segment $2$ is copy-neutral LOH in many tumors and its depth ratio $D_2$ is close to $1$, an integer total copy number $t_2 = 2$ is consistent with this biology and the measured values. We will verify consistency below.\n\nFor Segment $2$ with $t_2 = 2$ and LOH ($b_2 = 0$), the depth equation becomes\n$$D_2 = k \\cdot \\frac{\\pi \\cdot 2 + (1 - \\pi) \\cdot 2}{2} = k \\cdot \\frac{2}{2} = k.$$\nTherefore,\n$$k = D_2 = 0.900.$$\nThe minor allele fraction for Segment $2$ under LOH with $t_2 = 2$ and $b_2 = 0$ is\n$$F_2 = \\frac{\\pi \\cdot 0 + (1 - \\pi) \\cdot 1}{\\pi \\cdot 2 + (1 - \\pi) \\cdot 2} = \\frac{1 - \\pi}{2}.$$\nWith $F_2 = 0.150$, we solve\n$$\\frac{1 - \\pi}{2} = 0.150 \\quad \\Rightarrow \\quad 1 - \\pi = 0.300 \\quad \\Rightarrow \\quad \\pi = 0.700.$$\n\nNext consider Segment $3$, which is allelically balanced: this implies $a_3 = b_3$ and hence $t_3$ is even with $b_3 = t_3/2$. Among the smallest even integers, $t_3 = 2$ would yield balance but its depth would equal diploid; the observed $D_3 = 1.530$ exceeds $D_2 = 0.900$, so $t_3$ must exceed $2$. The simplest balanced high-copy state is $t_3 = 4$ with $(a_3, b_3) = (2,2)$. We verify consistency with depth and minor allele fraction. Using the depth equation with $t_3 = 4$,\n$$D_3 = k \\cdot \\frac{\\pi \\cdot 4 + (1 - \\pi) \\cdot 2}{2} = k \\cdot \\frac{4\\pi + 2 - 2\\pi}{2} = k \\cdot (\\pi + 1).$$\nPlugging $k = 0.900$ and $D_3 = 1.530$,\n$$\\pi + 1 = \\frac{D_3}{k} = \\frac{1.530}{0.900} = 1.700 \\quad \\Rightarrow \\quad \\pi = 0.700,$$\nwhich matches the purity obtained from Segment $2$.\n\nThe minor allele fraction for Segment $3$ with $(a_3, b_3) = (2,2)$ is\n$$F_3 = \\frac{\\pi \\cdot 2 + (1 - \\pi) \\cdot 1}{\\pi \\cdot 4 + (1 - \\pi) \\cdot 2} = \\frac{2\\pi + 1 - \\pi}{4\\pi + 2 - 2\\pi} = \\frac{\\pi + 1}{2\\pi + 2}.$$\nWith $\\pi = 0.700$,\n$$F_3 = \\frac{0.700 + 1}{2 \\cdot 0.700 + 2} = \\frac{1.700}{3.400} = 0.500,$$\nconsistent with the measured $F_3 = 0.500$. Thus Segment $3$ has $t_3 = 4$.\n\nNow consider Segment $1$, which is imbalanced but retains the minor allele. A parsimonious integer state given $F_1 = 0.370$ is $t_1 = 3$ with $(a_1, b_1) = (2,1)$. We verify this using both the minor allele fraction and the depth ratio. The minor allele fraction for $(2,1)$ is\n$$F_1 = \\frac{\\pi \\cdot 1 + (1 - \\pi) \\cdot 1}{\\pi \\cdot 3 + (1 - \\pi) \\cdot 2} = \\frac{\\pi + 1 - \\pi}{3\\pi + 2 - 2\\pi} = \\frac{1}{\\pi + 2}.$$\nWith $\\pi = 0.700$,\n$$F_1 = \\frac{1}{0.700 + 2} = \\frac{1}{2.700} \\approx 0.37037,$$\nwhich matches the measured $F_1 = 0.370$. The depth ratio for $t_1 = 3$ is\n$$D_1 = k \\cdot \\frac{\\pi \\cdot 3 + (1 - \\pi) \\cdot 2}{2} = k \\cdot \\frac{3\\pi + 2 - 2\\pi}{2} = k \\cdot \\frac{\\pi + 2}{2}.$$\nWith $k = 0.900$ and $\\pi = 0.700$,\n$$D_1 = 0.900 \\cdot \\frac{0.700 + 2}{2} = 0.900 \\cdot \\frac{2.700}{2} = 0.900 \\cdot 1.350 = 1.215,$$\nwhich matches the measured $D_1 = 1.215$. Therefore Segment $1$ has $t_1 = 3$.\n\nWe have inferred absolute copy numbers and parameters:\n- $t_1 = 3$, $t_2 = 2$, $t_3 = 4$,\n- $\\pi = 0.700$, $k = 0.900$.\n\nFinally, compute the panel ploidy $P$ as the length-weighted average:\n$$P = \\frac{L_1 \\, t_1 + L_2 \\, t_2 + L_3 \\, t_3}{L_1 + L_2 + L_3} = \\frac{80 \\cdot 3 + 40 \\cdot 2 + 30 \\cdot 4}{80 + 40 + 30} = \\frac{240 + 80 + 120}{150} = \\frac{440}{150} \\approx 2.933\\overline{3}.$$\nRounded to four significant figures and expressed unitless, the panel ploidy is $2.933$.", "answer": "$$\\boxed{2.933}$$", "id": "4331590"}]}