{"hands_on_practices": [{"introduction": "Real-world genomic datasets are often plagued by batch effects—systematic, non-biological variations arising from differences in experimental conditions. This hands-on practice guides you through simulating and correcting for these distortions, a critical step in building robust predictive models. You will implement a regularized logistic regression model that uses per-batch feature standardization and batch-specific intercepts to disentangle the true biological signal from confounding assay biases [@problem_id:4330521].", "problem": "You are tasked with constructing a principled simulation and estimator to study the influence of batch effects and assay biases on a deep learning classifier for predicting variant pathogenicity. The core phenomenon to be modeled is that measurement pipelines introduce batch-specific distortions in the feature space and in the log-odds of the label. The setting is as follows.\n\nLet the feature vector be $x \\in \\mathbb{R}^d$ and the batch index be $b \\in \\{0,1,\\dots,B-1\\}$. For batch $b$, assume an affine feature distortion consisting of an additive mean shift $\\mu_b \\in \\mathbb{R}^d$ and a component-wise multiplicative scale vector $s_b \\in \\mathbb{R}^d$ with strictly positive entries. Let the biological signal be parameterized by a true weight vector $w_{\\text{true}} \\in \\mathbb{R}^d$ and a batch-specific assay bias in the log-odds parameterized by $\\alpha_{\\text{true}} \\in \\mathbb{R}^B$. The data generation model for an instance in batch $b$ is:\n$$\nz \\sim \\mathcal{N}(0, I_d), \\quad x = \\mu_b + s_b \\odot z, \\quad \\text{logit} = w_{\\text{true}}^\\top x + \\alpha_{\\text{true},b}, \\quad p = \\sigma(\\text{logit}), \\quad y \\sim \\text{Bernoulli}(p),\n$$\nwhere $\\odot$ denotes elementwise multiplication, $\\sigma(t) = \\frac{1}{1+e^{-t}}$ is the logistic function, and $I_d$ is the $d \\times d$ identity matrix.\n\nYou must implement an estimator that first attempts to remove the batch effects by per-batch feature standardization (z-scoring) and then fits a regularized logistic regression with batch-specific intercepts. Specifically, on the training data, for each batch $b$ compute the empirical mean $\\hat{\\mu}_b \\in \\mathbb{R}^d$ and empirical standard deviation vector $\\hat{\\sigma}_b \\in \\mathbb{R}^d$ (computed component-wise), and define the standardized feature for sample $i$ in batch $b(i)$ by\n$$\n\\tilde{x}_i = \\frac{x_i - \\hat{\\mu}_{b(i)}}{\\hat{\\sigma}_{b(i)}},\n$$\nwhere the division is elementwise. Then fit parameters $w \\in \\mathbb{R}^d$ and $\\alpha \\in \\mathbb{R}^B$ by minimizing the regularized empirical risk\n$$\n\\mathcal{L}(w,\\alpha) = \\frac{1}{n}\\sum_{i=1}^n \\left[ - y_i \\log\\!\\big(\\sigma(t_i)\\big) - (1 - y_i)\\log\\!\\big(1 - \\sigma(t_i)\\big) \\right] + \\frac{\\lambda_w}{2} \\|w\\|_2^2 + \\frac{\\lambda_\\alpha}{2} \\|\\alpha\\|_2^2,\n$$\nwhere $t_i = w^\\top \\tilde{x}_i + \\alpha_{b(i)}$ and $n$ is the total number of training samples. The minimization must be performed using batch (full) gradient descent with a fixed learning rate and a fixed number of iterations.\n\nFrom first principles, the negative log-likelihood term corresponds to the cross-entropy of the Bernoulli distribution implied by the logistic model, and $\\ell_2$ penalties enforce weight regularity in the presence of batch distortion. Your program must implement the gradient-based updates derived from this loss function.\n\nAfter training, evaluate the estimator on an independently generated test set from the same generative model using the same per-batch standardization parameters $(\\hat{\\mu}_b, \\hat{\\sigma}_b)$ computed from the training data. For each test case, report the following metrics:\n- The Euclidean norm of the error in the biological signal weights: $e_w = \\|w - w_{\\text{true}}\\|_2$.\n- The Euclidean norm of the error in the batch intercepts: $e_\\alpha = \\|\\alpha - \\alpha_{\\text{true}}\\|_2$.\n- The Area Under the Receiver Operating Characteristic (AUROC), expressed as a decimal in $[0,1]$ computed on the test set using the predicted probabilities $\\hat{p}_i = \\sigma(w^\\top \\tilde{x}_i + \\alpha_{b(i)})$; the AUROC must be computed exactly from the definition via ranking (Mann–Whitney statistic), with ties handled by average ranks.\n\nYour program must implement the following test suite. In all cases, draw $w_{\\text{true}}$ from a standard normal distribution and rescale it to have Euclidean norm equal to $1.5$. For each batch $b$, draw a random unit vector $u_b \\in \\mathbb{R}^d$ from a standard normal distribution and set $\\mu_b = m_b \\cdot u_b$ with $m_b$ drawn uniformly from the specified magnitude interval. For each component $j \\in \\{1,\\dots,d\\}$, draw $s_{b,j}$ independently and uniformly from the specified scale interval. Use fixed random seeds per case for reproducibility.\n\nTest case A (general case):\n- $d = 12$, $B = 3$.\n- Training samples per batch: $[3000, 3000, 3000]$.\n- Test samples per batch: $[1000, 1000, 1000]$.\n- Mean shift magnitudes $m_b \\sim \\text{Uniform}(0.3, 0.7)$ independently per batch.\n- Scales $s_{b,j} \\sim \\text{Uniform}(0.8, 1.5)$ independently.\n- True batch intercepts $\\alpha_{\\text{true}} = [-0.4, 0.2, 0.1]$.\n- Regularization strengths: $\\lambda_w = 0.05$, $\\lambda_\\alpha = 0.10$.\n- Learning rate: $0.10$.\n- Gradient descent iterations: $400$.\n- Random seed: $123$.\n\nTest case B (severe batch distortion):\n- $d = 12$, $B = 3$.\n- Training samples per batch: $[3000, 3000, 3000]$.\n- Test samples per batch: $[1000, 1000, 1000]$.\n- Mean shift magnitudes $m_b \\sim \\text{Uniform}(1.0, 2.0)$ independently per batch.\n- Scales $s_{b,j} \\sim \\text{Uniform}(0.5, 2.5)$ independently.\n- True batch intercepts $\\alpha_{\\text{true}} = [1.2, -0.8, 0.5]$.\n- Regularization strengths: $\\lambda_w = 0.10$, $\\lambda_\\alpha = 0.20$.\n- Learning rate: $0.08$.\n- Gradient descent iterations: $600$.\n- Random seed: $456$.\n\nTest case C (class imbalance due to small batch):\n- $d = 12$, $B = 3$.\n- Training samples per batch: $[200, 3000, 3000]$.\n- Test samples per batch: $[200, 1000, 1000]$.\n- Mean shift magnitudes $m_b \\sim \\text{Uniform}(0.5, 0.9)$ independently per batch.\n- Scales $s_{b,j} \\sim \\text{Uniform}(0.7, 1.8)$ independently.\n- True batch intercepts $\\alpha_{\\text{true}} = [-0.2, 0.3, 0.0]$.\n- Regularization strengths: $\\lambda_w = 0.05$, $\\lambda_\\alpha = 0.10$.\n- Learning rate: $0.10$.\n- Gradient descent iterations: $500$.\n- Random seed: $789$.\n\nYour program must:\n1. Generate training and test data for each case strictly according to the specifications.\n2. Perform per-batch standardization using training data statistics only.\n3. Train the regularized logistic regression with batch intercepts using full-batch gradient descent with the specified hyperparameters.\n4. Compute $e_w$, $e_\\alpha$, and AUROC on the test set for each case.\n\nFinal output format: Your program should produce a single line of output containing the nine results (three per test case in the order $[e_w, e_\\alpha, \\text{AUROC}]$ for A, then B, then C) as a comma-separated list enclosed in square brackets (e.g., \"[rA1,rA2,rA3,rB1,rB2,rB3,rC1,rC2,rC3]\"). No other text should be printed. All values must be represented as Python floats (decimals). There are no physical units or angles involved; express AUROC as a decimal rather than a percentage.", "solution": "The goal is to estimate the biological signal and batch intercepts while counteracting batch-induced distortions in a logistic model for variant pathogenicity. We proceed from first principles and derive the estimator and its gradients, then specify the algorithm that the program must implement.\n\nFundamental base and definitions:\n- The logistic function is $\\sigma(t) = \\frac{1}{1+e^{-t}}$, which parameterizes the Bernoulli probability in logistic regression. The negative log-likelihood for a single observation $(\\tilde{x}, y)$ with linear predictor $t = w^\\top \\tilde{x} + \\alpha_{b}$ is $- y \\log(\\sigma(t)) - (1-y)\\log(1-\\sigma(t))$. Over $n$ samples, the empirical risk is the average of these terms.\n- Batch effects are modeled as affine distortions of the features via $(\\mu_b, s_b)$ and as additive biases in the log-odds via $\\alpha_{\\text{true},b}$. This is consistent with well-tested observations in measurement science that batch processing induces mean and scale shifts, and assay-specific calibration affects baseline logits.\n- Per-batch z-scoring removes affine mean and scale shifts by centering and scaling features using empirical estimates $(\\hat{\\mu}_b, \\hat{\\sigma}_b)$, invoking the Law of Large Numbers to justify that these estimates converge to the true batch parameters as sample size increases.\n\nEstimator and loss:\nWe standardize features per batch in the training data: for sample $i$ in batch $b(i)$ with feature $x_i$, define\n$$\n\\tilde{x}_i = \\frac{x_i - \\hat{\\mu}_{b(i)}}{\\hat{\\sigma}_{b(i)}},\n$$\nwhere the division is component-wise and $\\hat{\\sigma}_{b}$ entries are strictly positive (regularized with a small constant if necessary). We then minimize the regularized empirical risk\n$$\n\\mathcal{L}(w,\\alpha) = \\frac{1}{n}\\sum_{i=1}^n \\left[ - y_i \\log\\!\\big(\\sigma(t_i)\\big) - (1 - y_i)\\log\\!\\big(1 - \\sigma(t_i)\\big) \\right] + \\frac{\\lambda_w}{2} \\|w\\|_2^2 + \\frac{\\lambda_\\alpha}{2} \\|\\alpha\\|_2^2,\n$$\nwith $t_i = w^\\top \\tilde{x}_i + \\alpha_{b(i)}$.\n\nGradient derivation:\nLet $p_i = \\sigma(t_i)$ and note that $\\frac{\\partial}{\\partial t_i} \\left(- y_i \\log(p_i) - (1-y_i)\\log(1-p_i)\\right) = p_i - y_i$ by the standard derivative of the logistic loss. Using the chain rule, the gradient with respect to $w$ is\n$$\n\\nabla_w \\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^n (p_i - y_i)\\, \\tilde{x}_i + \\lambda_w w = \\frac{1}{n} \\left( \\tilde{X}^\\top (p - y) \\right) + \\lambda_w w,\n$$\nwhere $\\tilde{X}$ is the $n \\times d$ matrix of standardized features and $p, y \\in \\mathbb{R}^n$ are the vectors of predicted probabilities and labels, respectively. The gradient with respect to each batch intercept $\\alpha_b$ is\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_b} = \\frac{1}{n} \\sum_{i: b(i)=b} (p_i - y_i) + \\lambda_\\alpha \\alpha_b.\n$$\nCollectively, the gradient vector for $\\alpha \\in \\mathbb{R}^B$ can be written using a one-hot batch incidence matrix or computed via aggregation over batch indices.\n\nOptimization algorithm:\nWe use full-batch gradient descent with a fixed learning rate $\\eta$ and a fixed number of iterations $T$ as specified per test case. Initialize $w$ and $\\alpha$ to zero vectors, and at each iteration:\n1. Compute $t = \\tilde{X} w + \\alpha_{\\text{expanded}}$, where $\\alpha_{\\text{expanded}}$ indexes $\\alpha$ per sample via the batch indices.\n2. Compute $p = \\sigma(t)$.\n3. Compute the gradients:\n$$\ng_w = \\frac{1}{n} \\tilde{X}^\\top (p - y) + \\lambda_w w, \\quad\ng_\\alpha[b] = \\frac{1}{n} \\sum_{i: b(i)=b} (p_i - y_i) + \\lambda_\\alpha \\alpha_b.\n$$\n4. Update $w \\leftarrow w - \\eta g_w$, $\\alpha \\leftarrow \\alpha - \\eta g_\\alpha$.\n\nEvaluation:\n- Error in biological signal: $e_w = \\|w - w_{\\text{true}}\\|_2$.\n- Error in batch intercepts: $e_\\alpha = \\|\\alpha - \\alpha_{\\text{true}}\\|_2$.\n- Area Under the Receiver Operating Characteristic (AUROC): compute on the test set using predicted probabilities $\\hat{p}_i = \\sigma(w^\\top \\tilde{x}_i + \\alpha_{b(i)})$. AUROC is the probability that a randomly chosen positive sample has a higher score than a randomly chosen negative sample, plus half the probability of tied scores. Numerically, AUROC can be computed via the Mann–Whitney statistic using average ranks:\n  - Let $r_i$ be the average rank of score $s_i$ among all $n$ samples when scores are sorted in ascending order; with ties receiving the average of their rank interval.\n  - Let $n_+$ be the number of positive samples and $n_-$ the number of negative samples.\n  - The AUROC is\n  $$\n  \\text{AUROC} = \\frac{\\sum_{i: y_i=1} r_i - \\frac{n_+(n_+ + 1)}{2}}{n_+ n_-}.\n  $$\n  If $n_+=0$ or $n_-=0$, set AUROC to $0.5$ as a neutral value.\n\nData generation details:\n- For each test case, draw $w_{\\text{true}} \\sim \\mathcal{N}(0, I_d)$ and rescale to have Euclidean norm $1.5$.\n- For each batch $b$, draw $u_b \\sim \\mathcal{N}(0, I_d)$ and set $u_b \\leftarrow \\frac{u_b}{\\|u_b\\|_2}$ to be a unit vector. Draw $m_b$ uniformly from the case-specific interval and set $\\mu_b = m_b u_b$.\n- For each component $j$, draw $s_{b,j}$ uniformly from the case-specific scale interval, ensuring strict positivity.\n- Generate $z \\sim \\mathcal{N}(0, I_d)$ independently per sample, set $x = \\mu_b + s_b \\odot z$, compute $p = \\sigma(w_{\\text{true}}^\\top x + \\alpha_{\\text{true},b})$, and sample $y \\sim \\text{Bernoulli}(p)$.\n\nStandardization:\n- Compute $\\hat{\\mu}_b$ and $\\hat{\\sigma}_b$ on the training set for each batch $b$ by empirical mean and standard deviation per feature.\n- Define $\\tilde{x}_i = (x_i - \\hat{\\mu}_{b(i)}) / \\hat{\\sigma}_{b(i)}$ with a small positive constant added to $\\hat{\\sigma}_b$ if any component is zero to avoid division by zero.\n- Apply the same $(\\hat{\\mu}_b, \\hat{\\sigma}_b)$ to standardize the test set.\n\nHyperparameters:\n- Use the specified regularization strengths $(\\lambda_w, \\lambda_\\alpha)$, learning rate $\\eta$, and iteration count $T$ per test case.\n\nOutput:\n- For each test case (A, B, C), produce the triple $(e_w, e_\\alpha, \\text{AUROC})$ as Python floats.\n- Aggregate all nine values into a single list in the order A then B then C and print as a single line string with comma-separated values enclosed in square brackets.\n\nThis approach is justified because per-batch z-scoring removes affine batch effects under the assumed generative model, and batch-specific intercepts $\\alpha_b$ capture assay-induced logit biases. Regularized logistic regression minimizes the expected negative log-likelihood, and gradient descent implements the minimization efficiently in a differentiable model. AUROC computed via ranks provides a threshold-independent measure of discrimination of the learned pathogenicity predictor on the test set.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sigmoid(x):\n    # Numerically stable sigmoid\n    # Clip input to avoid overflow in exp\n    x = np.clip(x, -50, 50)\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef compute_average_ranks(scores):\n    \"\"\"\n    Compute average ranks for scores (ascending).\n    Ties receive the average of their rank interval.\n    Returns an array of float ranks from 1 to n.\n    \"\"\"\n    n = scores.size\n    order = np.argsort(scores, kind='mergesort')  # stable sort\n    sorted_scores = scores[order]\n    ranks = np.empty(n, dtype=float)\n\n    i = 0\n    while i  n:\n        j = i + 1\n        # group equal scores\n        while j  n and sorted_scores[j] == sorted_scores[i]:\n            j += 1\n        # average rank for the group [i, j-1], ranks are 1-based\n        avg_rank = (i + 1 + j) / 2.0\n        ranks[i:j] = avg_rank\n        i = j\n\n    # scatter back to original order\n    inv_order = np.empty(n, dtype=int)\n    inv_order[order] = np.arange(n)\n    ranks_original = ranks[inv_order]\n    return ranks_original\n\ndef auc_roc(y_true, scores):\n    \"\"\"\n    Compute AUROC using the Mann-Whitney U statistic via average ranks.\n    y_true: binary array of 0/1\n    scores: predictor scores (probabilities or logits), higher means more positive\n    \"\"\"\n    y_true = np.asarray(y_true, dtype=int)\n    scores = np.asarray(scores, dtype=float)\n    n = y_true.size\n    n_pos = int(np.sum(y_true))\n    n_neg = n - n_pos\n    if n_pos == 0 or n_neg == 0:\n        return 0.5  # neutral AUROC if only one class present\n\n    ranks = compute_average_ranks(scores)  # ascending\n    # Sum ranks of positives\n    R_pos = np.sum(ranks[y_true == 1])\n    auc = (R_pos - n_pos * (n_pos + 1) / 2.0) / (n_pos * n_neg)\n    return float(auc)\n\ndef generate_case_data(d, B, n_train_per_batch, n_test_per_batch,\n                       shift_range, scale_range, alpha_true, seed):\n    \"\"\"\n    Generate training and test data for one case according to the specification.\n    Returns:\n      w_true: (d,)\n      train_X: (n_train, d)\n      train_y: (n_train,)\n      train_batches: (n_train,)\n      test_X: (n_test, d)\n      test_y: (n_test,)\n      test_batches: (n_test,)\n      mus: list of length B with mean vectors\n      sigmas: list of length B with std vectors (for info; not used directly)\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # True biological weights\n    w_true = rng.normal(size=d)\n    w_true = (1.5 / np.linalg.norm(w_true)) * w_true\n\n    # Batch means and scales\n    mus = []\n    scales = []\n    for b in range(B):\n        u = rng.normal(size=d)\n        u_norm = np.linalg.norm(u)\n        if u_norm == 0:\n            u = np.zeros(d)\n            u[0] = 1.0\n            u_norm = 1.0\n        u = u / u_norm\n        m_mag = rng.uniform(shift_range[0], shift_range[1])\n        mu_b = m_mag * u\n        s_b = rng.uniform(scale_range[0], scale_range[1], size=d)\n        mus.append(mu_b)\n        scales.append(s_b)\n\n    # Generate training data\n    train_X_list = []\n    train_y_list = []\n    train_batches_list = []\n\n    for b in range(B):\n        n_b = n_train_per_batch[b]\n        z = rng.normal(size=(n_b, d))\n        x = mus[b] + z * scales[b]\n        logits = x @ w_true + alpha_true[b]\n        p = sigmoid(logits)\n        y = rng.binomial(1, p)\n        train_X_list.append(x)\n        train_y_list.append(y)\n        train_batches_list.append(np.full(n_b, b, dtype=int))\n\n    train_X = np.vstack(train_X_list)\n    train_y = np.concatenate(train_y_list)\n    train_batches = np.concatenate(train_batches_list)\n\n    # Generate test data\n    test_X_list = []\n    test_y_list = []\n    test_batches_list = []\n\n    for b in range(B):\n        n_b = n_test_per_batch[b]\n        z = rng.normal(size=(n_b, d))\n        x = mus[b] + z * scales[b]\n        logits = x @ w_true + alpha_true[b]\n        p = sigmoid(logits)\n        y = rng.binomial(1, p)\n        test_X_list.append(x)\n        test_y_list.append(y)\n        test_batches_list.append(np.full(n_b, b, dtype=int))\n\n    test_X = np.vstack(test_X_list)\n    test_y = np.concatenate(test_y_list)\n    test_batches = np.concatenate(test_batches_list)\n\n    return w_true, train_X, train_y, train_batches, test_X, test_y, test_batches, mus, scales\n\ndef per_batch_standardize(X, batches, B):\n    \"\"\"\n    Compute per-batch mean and std, and return standardized X along with stats.\n    \"\"\"\n    d = X.shape[1]\n    means = []\n    stds = []\n    X_std = np.empty_like(X)\n    eps = 1e-8\n    for b in range(B):\n        idx = (batches == b)\n        X_b = X[idx]\n        mu_b = X_b.mean(axis=0)\n        sigma_b = X_b.std(axis=0)\n        sigma_b = np.where(sigma_b = 0, eps, sigma_b)\n        means.append(mu_b)\n        stds.append(sigma_b)\n        X_std[idx] = (X_b - mu_b) / sigma_b\n    return X_std, means, stds\n\ndef apply_standardization(X, batches, means, stds):\n    \"\"\"\n    Apply precomputed per-batch means and stds to standardize X.\n    \"\"\"\n    X_std = np.empty_like(X)\n    for b in range(len(means)):\n        idx = (batches == b)\n        X_b = X[idx]\n        X_std[idx] = (X_b - means[b]) / stds[b]\n    return X_std\n\ndef train_logistic_with_batch_intercepts(X, y, batches, B, lambda_w, lambda_alpha, lr, iters):\n    \"\"\"\n    Train logistic regression with batch intercepts via gradient descent.\n    Returns learned w and alpha.\n    \"\"\"\n    n, d = X.shape\n    w = np.zeros(d, dtype=float)\n    alpha = np.zeros(B, dtype=float)\n\n    for _ in range(iters):\n        linear = X @ w + alpha[batches]\n        p = sigmoid(linear)\n        error = (p - y).astype(float)\n\n        # Gradients\n        grad_w = (X.T @ error) / n + lambda_w * w\n\n        # For alpha, aggregate errors per batch\n        grad_alpha = np.zeros(B, dtype=float)\n        # Weighted sum of error by batch divided by n\n        for b in range(B):\n            grad_alpha[b] = error[batches == b].sum() / n + lambda_alpha * alpha[b]\n\n        # Update\n        w -= lr * grad_w\n        alpha -= lr * grad_alpha\n\n    return w, alpha\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\n            \"d\": 12,\n            \"B\": 3,\n            \"n_train_per_batch\": [3000, 3000, 3000],\n            \"n_test_per_batch\": [1000, 1000, 1000],\n            \"shift_range\": (0.3, 0.7),\n            \"scale_range\": (0.8, 1.5),\n            \"alpha_true\": np.array([-0.4, 0.2, 0.1], dtype=float),\n            \"lambda_w\": 0.05,\n            \"lambda_alpha\": 0.10,\n            \"lr\": 0.10,\n            \"iters\": 400,\n            \"seed\": 123,\n        },\n        # Case B\n        {\n            \"d\": 12,\n            \"B\": 3,\n            \"n_train_per_batch\": [3000, 3000, 3000],\n            \"n_test_per_batch\": [1000, 1000, 1000],\n            \"shift_range\": (1.0, 2.0),\n            \"scale_range\": (0.5, 2.5),\n            \"alpha_true\": np.array([1.2, -0.8, 0.5], dtype=float),\n            \"lambda_w\": 0.10,\n            \"lambda_alpha\": 0.20,\n            \"lr\": 0.08,\n            \"iters\": 600,\n            \"seed\": 456,\n        },\n        # Case C\n        {\n            \"d\": 12,\n            \"B\": 3,\n            \"n_train_per_batch\": [200, 3000, 3000],\n            \"n_test_per_batch\": [200, 1000, 1000],\n            \"shift_range\": (0.5, 0.9),\n            \"scale_range\": (0.7, 1.8),\n            \"alpha_true\": np.array([-0.2, 0.3, 0.0], dtype=float),\n            \"lambda_w\": 0.05,\n            \"lambda_alpha\": 0.10,\n            \"lr\": 0.10,\n            \"iters\": 500,\n            \"seed\": 789,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        d = case[\"d\"]\n        B = case[\"B\"]\n        n_train_per_batch = case[\"n_train_per_batch\"]\n        n_test_per_batch = case[\"n_test_per_batch\"]\n        shift_range = case[\"shift_range\"]\n        scale_range = case[\"scale_range\"]\n        alpha_true = case[\"alpha_true\"]\n        lambda_w = case[\"lambda_w\"]\n        lambda_alpha = case[\"lambda_alpha\"]\n        lr = case[\"lr\"]\n        iters = case[\"iters\"]\n        seed = case[\"seed\"]\n\n        # Generate data\n        w_true, X_train, y_train, batches_train, X_test, y_test, batches_test, mus, scales = generate_case_data(\n            d, B, n_train_per_batch, n_test_per_batch, shift_range, scale_range, alpha_true, seed\n        )\n\n        # Per-batch standardization on training data\n        X_train_std, means, stds = per_batch_standardize(X_train, batches_train, B)\n        X_test_std = apply_standardization(X_test, batches_test, means, stds)\n\n        # Train model\n        w_hat, alpha_hat = train_logistic_with_batch_intercepts(\n            X_train_std, y_train, batches_train, B,\n            lambda_w, lambda_alpha, lr, iters\n        )\n\n        # Metrics\n        ew = float(np.linalg.norm(w_hat - w_true))\n        ealpha = float(np.linalg.norm(alpha_hat - alpha_true))\n        test_probs = sigmoid(X_test_std @ w_hat + alpha_hat[batches_test])\n        auc = auc_roc(y_test, test_probs)\n\n        results.extend([ew, ealpha, auc])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4330521"}, {"introduction": "A comprehensive assessment of variant pathogenicity often requires synthesizing evidence from diverse sources, such as DNA sequence, protein structure, and functional assays. This exercise demonstrates how to build a principled aggregator that combines outputs from multiple independent models using a Bayesian framework. You will implement an inverse-variance weighted averaging scheme in log-odds space to produce a single, optimally integrated posterior probability of pathogenicity [@problem_id:4330543].", "problem": "You are tasked to design and implement a principled aggregator for integrating multimodal evidence from independent deep learning models that estimate the pathogenicity of genomic variants. The goal is to produce a single posterior probability of pathogenicity for each variant by combining several modality-specific outputs in a way that is consistent with probability theory and optimal estimation.\n\nStart from a context-appropriate fundamental base:\n- The definition of posterior probability from Bayes' theorem: for a binary label $Y \\in \\{0,1\\}$ and evidence $x$, the posterior odds satisfy $$\\frac{\\mathbb{P}(Y=1 \\mid x)}{\\mathbb{P}(Y=0 \\mid x)} = \\frac{\\pi}{1-\\pi} \\cdot \\Lambda(x),$$ where $\\pi \\in (0,1)$ is the prior probability of pathogenicity and $\\Lambda(x)$ is the likelihood ratio contributed by the observed evidence.\n- The property that the logit transform of a probability is its log-odds: $$\\operatorname{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right).$$\n- The Gauss–Markov theorem (best linear unbiased estimator): given independent, unbiased estimators with known variances for a common latent quantity, the inverse-variance weighted average minimizes mean squared error among linear unbiased estimators.\n\nAssume the following modality model. You have $M$ modalities indexed by $m \\in \\{1,\\dots,M\\}$. Each modality produces an estimated probability $p_m \\in (0,1)$ that a variant is pathogenic based solely on its modality-specific evidence, under an implicit neutral prior of $0.5$. After passing through a calibration step with a positive temperature $t_m  0$, the calibrated modality-specific log-odds is $$z_m = \\frac{\\operatorname{logit}(p_m)}{t_m}.$$ Assume that each $z_m$ is an unbiased noisy estimate of a common latent log-likelihood ratio $\\ell^\\star$ for the variant, i.e., $$z_m = \\ell^\\star + \\varepsilon_m,$$ where the noise terms satisfy $\\varepsilon_m \\sim \\mathcal{N}(0,\\sigma_m^2)$ and are independent across $m$, with known standard deviations $\\sigma_m  0$. Under these assumptions, the inverse-variance weighting implied by the Gauss–Markov theorem yields a single integrated estimate of the latent log-likelihood ratio. Combining this integrated estimate with a specified prior probability $\\pi$ via Bayes' theorem produces the final posterior probability of pathogenicity.\n\nYour task is to implement a program that:\n- Takes a fixed test suite of parameter sets.\n- For each parameter set, computes the integrated posterior probability of pathogenicity as a decimal in $(0,1)$, using the principled approach derived from the base assumptions above.\n- Handles the numerical boundary conditions when any $p_m$ equals $0$ or $1$ by clipping $p_m$ to $[\\varepsilon, 1-\\varepsilon]$ with $\\varepsilon = 10^{-12}$ before applying the logit transform.\n\nPrecisely, for each variant with parameters:\n- Prior probability of pathogenicity $\\pi \\in (0,1)$.\n- Modality probability outputs $\\{p_m\\}_{m=1}^M$, each in $(0,1)$, which must be treated as decimals without a percentage sign.\n- Calibration temperatures $\\{t_m\\}_{m=1}^M$, each in $\\mathbb{R}_{0}$.\n- Modality noise standard deviations $\\{\\sigma_m\\}_{m=1}^M$, each in $\\mathbb{R}_{0}$.\n\nYou must produce a single posterior probability $\\hat{p} \\in (0,1)$ per variant as follows, expressed purely in decimal units with no percent sign:\n- Use the base principles above to derive the integrated estimator of the latent log-likelihood ratio and then the posterior probability via Bayes' theorem. Do not introduce any heuristic combination rule that contradicts the base assumptions.\n\nTest suite:\nUse exactly the following six test cases, each specified by the tuple $(\\pi, \\{p_m\\}_{m=1}^M, \\{t_m\\}_{m=1}^M, \\{\\sigma_m\\}_{m=1}^M)$ with $M=3$:\n1. $(\\pi = 0.01, \\{p_m\\} = \\{0.8, 0.75, 0.6\\}, \\{t_m\\} = \\{1.2, 1.0, 1.5\\}, \\{\\sigma_m\\} = \\{0.6, 0.4, 0.8\\})$,\n2. $(\\pi = 0.5, \\{p_m\\} = \\{0.99, 0.02, 0.01\\}, \\{t_m\\} = \\{1.0, 1.1, 1.3\\}, \\{\\sigma_m\\} = \\{0.5, 0.3, 0.3\\})$,\n3. $(\\pi = 0.001, \\{p_m\\} = \\{1.0, 0.0, 0.5\\}, \\{t_m\\} = \\{1.0, 1.0, 1.0\\}, \\{\\sigma_m\\} = \\{0.2, 0.2, 1.0\\})$,\n4. $(\\pi = 0.9, \\{p_m\\} = \\{0.5, 0.5, 0.5\\}, \\{t_m\\} = \\{1.0, 1.0, 1.0\\}, \\{\\sigma_m\\} = \\{10.0, 10.0, 10.0\\})$,\n5. $(\\pi = 0.05, \\{p_m\\} = \\{0.7, 0.4, 0.65\\}, \\{t_m\\} = \\{2.0, 0.8, 1.4\\}, \\{\\sigma_m\\} = \\{1.5, 0.6, 0.9\\})$,\n6. $(\\pi = 0.2, \\{p_m\\} = \\{0.51, 0.49, 0.5\\}, \\{t_m\\} = \\{1.3, 1.3, 1.3\\}, \\{\\sigma_m\\} = \\{0.1, 5.0, 10.0\\})$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the six posterior probabilities for the six test cases as a comma-separated list enclosed in square brackets. Each probability must be formatted as a decimal rounded to six digits after the decimal point. For example, an output line should look like $[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_6]$ where each $\\text{result}_i$ is a decimal in $(0,1)$.", "solution": "We begin by formalizing the multimodal integration problem as the estimation of a latent log-likelihood ratio from multiple independent, noisy estimates produced by modality-specific deep learning models.\n\nLet $Y \\in \\{0,1\\}$ denote the binary pathogenicity label. Let the prior probability of pathogenicity be $\\pi \\in (0,1)$, so the prior odds are $\\frac{\\pi}{1-\\pi}$ and the prior log-odds are $$\\eta = \\log\\left(\\frac{\\pi}{1-\\pi}\\right).$$ Consider $M$ modalities indexed by $m \\in \\{1,\\dots,M\\}$. Each modality produces an output probability $p_m \\in (0,1)$ which, under an implicit neutral prior of $0.5$, reflects the modality-specific posterior belief using only its evidence. The logit transform of this probability is an estimate of a modality-specific log-odds. To correct for calibration discrepancies, we apply temperature scaling with $t_m  0$, yielding the calibrated modality log-odds\n$$\nz_m = \\frac{\\operatorname{logit}(p_m)}{t_m} = \\frac{1}{t_m} \\log\\left(\\frac{p_m}{1-p_m}\\right).\n$$\nWe posit the model\n$$\nz_m = \\ell^\\star + \\varepsilon_m,\n$$\nwhere $\\ell^\\star$ is the latent log-likelihood ratio contributed by the evidence across modalities, and $\\varepsilon_m \\sim \\mathcal{N}(0,\\sigma_m^2)$ are independent noise terms with known standard deviations $\\sigma_m  0$. The independence and Gaussian noise assumptions are well tested in statistical modeling for aggregating unbiased estimates, and they allow us to apply the Gauss–Markov theorem.\n\nBy the Gauss–Markov theorem, among linear unbiased estimators of $\\ell^\\star$, the inverse-variance weighted average minimizes the mean squared error. Define weights\n$$\nw_m = \\frac{1}{\\sigma_m^2}.\n$$\nThe best linear unbiased estimator of $\\ell^\\star$ is then\n$$\n\\hat{\\ell} = \\frac{\\sum_{m=1}^M w_m z_m}{\\sum_{m=1}^M w_m}.\n$$\nThis $\\hat{\\ell}$ aggregates modality-specific calibrated log-odds into a single integrated log-likelihood ratio estimate.\n\nBayes' theorem connects likelihood ratios with posterior odds. The posterior odds given the joint evidence $x$ are\n$$\n\\frac{\\mathbb{P}(Y=1 \\mid x)}{\\mathbb{P}(Y=0 \\mid x)} = \\frac{\\pi}{1-\\pi} \\cdot \\Lambda(x),\n$$\nand taking logs gives the posterior log-odds\n$$\n\\log\\left(\\frac{\\mathbb{P}(Y=1 \\mid x)}{\\mathbb{P}(Y=0 \\mid x)}\\right) = \\eta + \\log \\Lambda(x).\n$$\nUnder our model, $\\hat{\\ell}$ serves as an estimator of $\\log \\Lambda(x)$. Therefore, the integrated posterior log-odds is\n$$\n\\hat{z} = \\eta + \\hat{\\ell}.\n$$\nFinally, we map $\\hat{z}$ back to a probability using the logistic function (the inverse of the logit),\n$$\n\\hat{p} = \\frac{1}{1 + \\exp(-\\hat{z})}.\n$$\n\nNumerical stability:\nThe logit function is undefined at $p_m = 0$ or $p_m = 1$. To ensure numerical stability while respecting the probabilistic semantics, we clip each $p_m$ to the interval $[\\varepsilon, 1-\\varepsilon]$ with $\\varepsilon = 10^{-12}$ before applying the logit transform. This clipping preserves the intended extremal behavior while avoiding infinities in computation.\n\nAlgorithmic steps for each test case:\n1. Read $\\pi$, $\\{p_m\\}_{m=1}^M$, $\\{t_m\\}_{m=1}^M$, and $\\{\\sigma_m\\}_{m=1}^M$.\n2. Compute the prior log-odds $\\eta = \\log\\left(\\frac{\\pi}{1-\\pi}\\right)$.\n3. For each modality $m$, clip $p_m$ to $[\\varepsilon,1-\\varepsilon]$ with $\\varepsilon = 10^{-12}$, then compute $$z_m = \\frac{1}{t_m} \\log\\left(\\frac{p_m}{1-p_m}\\right).$$\n4. Compute weights $w_m = \\frac{1}{\\sigma_m^2}$ and the integrated estimator $$\\hat{\\ell} = \\frac{\\sum_{m=1}^M w_m z_m}{\\sum_{m=1}^M w_m}.$$\n5. Compute the integrated posterior log-odds $\\hat{z} = \\eta + \\hat{\\ell}$ and the posterior probability $$\\hat{p} = \\frac{1}{1 + \\exp(-\\hat{z})}.$$\n6. Round $\\hat{p}$ to six digits after the decimal point and output.\n\nEdge cases in the provided test suite:\n- When $\\pi$ is very small (e.g., $\\pi = 0.001$), even strongly conflicting modalities may cancel in $\\hat{\\ell}$, and the posterior is dominated by the prior, yielding $\\hat{p}$ close to $\\pi$.\n- When all modalities produce $p_m = 0.5$ and have large $\\sigma_m$, the integrated evidence is uninformative, and the output $\\hat{p}$ tends toward the prior $\\pi$.\n- When one modality is highly reliable (small $\\sigma_m$) and others are unreliable (large $\\sigma_m$), the integrated estimate appropriately emphasizes the reliable modality.\n\nThe program implements these steps for the six specified cases and prints a single line containing the six results as a comma-separated list enclosed in square brackets. Each result is a decimal in $(0,1)$ rounded to six digits after the decimal point, satisfying the final output specification.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef clip_probabilities(p_array, eps=1e-12):\n    # Clip probabilities to avoid logit singularities at 0 and 1.\n    return np.clip(p_array, eps, 1.0 - eps)\n\ndef logit(p):\n    # Compute logit safely: log(p/(1-p))\n    return np.log(p) - np.log1p(-p)\n\ndef integrate_multimodal(pi, p_list, t_list, sigma_list, eps=1e-12):\n    # Convert inputs to numpy arrays\n    p = clip_probabilities(np.array(p_list, dtype=float), eps=eps)\n    t = np.array(t_list, dtype=float)\n    sigma = np.array(sigma_list, dtype=float)\n\n    # Prior log-odds\n    prior_log_odds = np.log(pi) - np.log(1.0 - pi)\n\n    # Calibrated modality log-odds\n    z = logit(p) / t\n\n    # Inverse-variance weights\n    w = 1.0 / (sigma ** 2)\n\n    # Integrated latent log-likelihood ratio via inverse-variance weighted average\n    ell_hat = np.sum(w * z) / np.sum(w)\n\n    # Posterior log-odds and probability\n    z_total = prior_log_odds + ell_hat\n    p_hat = 1.0 / (1.0 + np.exp(-z_total))\n    return p_hat\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (pi, p_list, t_list, sigma_list)\n        (0.01, [0.8, 0.75, 0.6], [1.2, 1.0, 1.5], [0.6, 0.4, 0.8]),\n        (0.5, [0.99, 0.02, 0.01], [1.0, 1.1, 1.3], [0.5, 0.3, 0.3]),\n        (0.001, [1.0, 0.0, 0.5], [1.0, 1.0, 1.0], [0.2, 0.2, 1.0]),\n        (0.9, [0.5, 0.5, 0.5], [1.0, 1.0, 1.0], [10.0, 10.0, 10.0]),\n        (0.05, [0.7, 0.4, 0.65], [2.0, 0.8, 1.4], [1.5, 0.6, 0.9]),\n        (0.2, [0.51, 0.49, 0.5], [1.3, 1.3, 1.3], [0.1, 5.0, 10.0]),\n    ]\n\n    results = []\n    for case in test_cases:\n        pi, p_list, t_list, sigma_list = case\n        result = integrate_multimodal(pi, p_list, t_list, sigma_list, eps=1e-12)\n        # Format to six decimal places\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4330543"}, {"introduction": "In clinical applications, knowing when a model is uncertain is as important as the prediction itself. This practice introduces conformal prediction, a modern, distribution-free framework for creating statistically rigorous prediction sets. Instead of a single \"benign\" or \"pathogenic\" label, you will learn to generate a set of possible labels that is guaranteed to contain the true label with a user-specified probability, providing a reliable way to flag ambiguous variants for further review [@problem_id:4330594].", "problem": "A deep learning classifier for genomic variant interpretation outputs, for each variant, a pair of estimated class probabilities for benign and pathogenic status. Let the two labels be $0$ for benign and $1$ for pathogenic. Consider the following construction of a prediction set for a test variant using class-conditional split inductive conformal prediction. The foundational base is the assumption of exchangeability (equivalently, independence and identical distribution in the calibration and test phases) between the calibration data and the test variant, and the monotonicity of nonconformity scores with respect to error risk. Specifically, suppose a model provides an estimated probability vector $\\hat{p}(x) = (\\hat{p}_0(x), \\hat{p}_1(x))$ for any variant $x$, with $\\hat{p}_0(x) + \\hat{p}_1(x) = 1$. Define a nonconformity score for any candidate label $y \\in \\{0,1\\}$ as $A(x,y) = 1 - \\hat{p}_y(x)$, which is larger when the classifier is less confident in label $y$. For each class $y \\in \\{0,1\\}$, collect calibration scores only from calibration variants whose true label is $y$; denote the multiset of these scores by $\\mathcal{S}_y$. For a test variant $x^{\\ast}$, compute a class-specific conformity assessment by comparing $A(x^{\\ast}, y)$ to the empirical distribution of $\\mathcal{S}_y$. A decision set at target miscoverage level $\\alpha \\in (0,1)$ is obtained by including all labels $y$ whose tail probability under $\\mathcal{S}_y$ is sufficiently large to control miscoverage.\n\nYour task is to derive, from the exchangeability assumption and the definition of $A(x,y)$ above, a finite-sample valid, non-randomized class-conditional decision rule that yields a prediction set $\\Gamma_{\\alpha}(x^{\\ast}) \\subseteq \\{0,1\\}$ with marginal coverage at least $1 - \\alpha$, and to implement it. You must adhere to the following constraints and clarifications.\n\n- Fundamental base to use: exchangeability of calibration and test instances, the definition of the nonconformity score $A(x,y) = 1 - \\hat{p}_y(x)$, and the ranking-based validity of inductive conformal methods under exchangeability. Do not assume any asymptotics or parametric likelihood models beyond these foundations.\n- You must express the decision rule purely in terms of counts over $\\mathcal{S}_y$ and the test score $A(x^{\\ast}, y)$, using only operations justified by the exchangeability-based rank argument.\n- The rule must be class-conditional, meaning calibration scores for each candidate label $y$ are compared only to $\\mathcal{S}_y$.\n- All candidate labels $y$ satisfying the rule must be included in the final decision set $\\Gamma_{\\alpha}(x^{\\ast})$, and no other labels may be included.\n\nUse the following test suite. In each case, you are given:\n- Calibration arrays for each class: for benign ($y=0$), a list of values that are equal to $\\hat{p}_0(x_i)$ for calibration variants with true label $0$; for pathogenic ($y=1$), a list of values that are equal to $\\hat{p}_1(x_i)$ for calibration variants with true label $1$. You must convert each of these to nonconformity scores by applying $A(x_i,y) = 1 - \\hat{p}_y(x_i)$.\n- A test variant’s estimated probability $\\hat{p}_1(x^{\\ast})$ for pathogenic; by conservation, $\\hat{p}_0(x^{\\ast}) = 1 - \\hat{p}_1(x^{\\ast})$.\n- A target miscoverage $\\alpha$.\n\nConstruct $\\Gamma_{\\alpha}(x^{\\ast})$ as above for each case. Encode the final decision set as a list of integers in ascending order, where benign is encoded as $0$ and pathogenic as $1$.\n\nTest cases:\n\n- Case $1$ (happy path): benign calibration $\\{\\hat{p}_0\\} = [0.85, 0.75, 0.65, 0.55, 0.50]$, pathogenic calibration $\\{\\hat{p}_1\\} = [0.92, 0.80, 0.70, 0.60]$, test $\\hat{p}_1(x^{\\ast}) = 0.88$, target $\\alpha = 0.20$.\n- Case $2$ (boundary tie case): benign calibration $\\{\\hat{p}_0\\} = [0.70]$, pathogenic calibration $\\{\\hat{p}_1\\} = [0.70]$, test $\\hat{p}_1(x^{\\ast}) = 0.60$, target $\\alpha = 0.50$.\n- Case $3$ (ambiguous inclusion): benign calibration $\\{\\hat{p}_0\\} = [0.52, 0.62]$, pathogenic calibration $\\{\\hat{p}_1\\} = [0.55, 0.65]$, test $\\hat{p}_1(x^{\\ast}) = 0.58$, target $\\alpha = 0.25$.\n\nYour program must compute the three decision sets in the order listed and produce a single line of output containing the three results as a comma-separated list enclosed in square brackets, with each decision set itself printed as a bracketed, comma-separated list of integers in ascending order and with no spaces anywhere. For example, a valid output format for three results is `[[],[],[]]`. Therefore, your program’s output must look like `[[\\dots],[\\dots],[\\dots]]`, where each inner bracket contains zero, one, or two integers chosen from $\\{0,1\\}$.", "solution": "The problem requires the derivation and implementation of a class-conditional split inductive conformal prediction rule. The derivation must be based on the principle of exchangeability.\n\nLet the two classes be $y \\in \\{0, 1\\}$, representing benign and pathogenic variants, respectively. A machine learning model provides an estimated probability vector $\\hat{p}(x) = (\\hat{p}_0(x), \\hat{p}_1(x))$ for any variant $x$, where $\\hat{p}_0(x) + \\hat{p}_1(x) = 1$. The nonconformity score for a data point $(x,y)$ with respect to its true label $y$ is defined as $A(x,y) = 1 - \\hat{p}_y(x)$. This score measures how \"unusual\" the observation is, with higher scores indicating greater nonconformity.\n\nThe procedure is class-conditional and operates in a split-inductive setting. For each class $y \\in \\{0,1\\}$, we are given a calibration set $\\mathcal{C}_y$ of examples whose true label is $y$. Let $n_y = |\\mathcal{C}_y|$ be the number of calibration examples for class $y$. The multiset of nonconformity scores for class $y$ is computed as $\\mathcal{S}_y = \\{A(x_i, y_i) | (x_i, y_i) \\in \\mathcal{C}_y \\text{ and } y_i=y\\}$.\n\nThe fundamental principle of inductive conformal prediction is based on exchangeability. For a new test point $x^{\\ast}$, we wish to determine if a candidate label $y$ should be included in its prediction set. We form a hypothesis that the true label of $x^{\\ast}$ is $y$. If this hypothesis is true, then the set of $n_y+1$ data points, consisting of the $n_y$ calibration points for class $y$ and the test point $(x^{\\ast}, y)$, is an exchangeable sequence. Consequently, the sequence of their nonconformity scores, $\\{s_{y,1}, \\dots, s_{y,n_y}, s_y^{\\ast}\\}$, where $s_{y,i} = A(x_i, y)$ for calibration points and $s_y^{\\ast} = A(x^{\\ast}, y)$ for the test point, is also a sequence of exchangeable random variables.\n\nUnder exchangeability, the rank of the test score $s_y^{\\ast}$ within the combined set of $n_y+1$ scores is uniformly distributed on the integers $\\{1, 2, \\dots, n_y+1\\}$. This property allows us to define a \"p-value\" for the hypothesis that the true label of $x^{\\ast}$ is $y$. A non-randomized p-value, which provides finite-sample validity, is defined as the proportion of scores in the combined set that are at least as non-conforming as the test score.\n\nLet $k_y$ be the number of calibration scores for class $y$ that are greater than or equal to the test score for that class hypothesis:\n$$k_y = |\\{s \\in \\mathcal{S}_y : s \\ge s_y^{\\ast}\\}| = |\\{s \\in \\mathcal{S}_y : s \\ge A(x^{\\ast}, y)\\}|$$\nThe p-value for the hypothesis that the true label of $x^{\\ast}$ is $y$, denoted $\\pi_y(x^{\\ast})$, is given by:\n$$\\pi_y(x^{\\ast}) = \\frac{k_y + 1}{n_y + 1}$$\nThe term $k_y$ counts the number of calibration scores as non-conforming as the test score, and the '`$+1$`' in the numerator accounts for the test score itself.\n\nThe conformal prediction set, $\\Gamma_{\\alpha}(x^{\\ast})$, is constructed by including all labels $y$ that are not \"too surprising\" at a given significance level $\\alpha \\in (0,1)$. A label is considered not too surprising if its p-value is greater than $\\alpha$. The decision rule is therefore:\n$$\\text{Include } y \\text{ in } \\Gamma_{\\alpha}(x^{\\ast}) \\iff \\pi_y(x^{\\ast})  \\alpha$$\nSubstituting the expression for the p-value, we obtain the final rule:\n$$\\text{Include } y \\text{ in } \\Gamma_{\\alpha}(x^{\\ast}) \\iff \\frac{k_y + 1}{n_y + 1}  \\alpha$$\nThis inequality can also be written as $k_y + 1  \\alpha(n_y + 1)$.\n\nThis procedure is performed independently for each candidate label $y \\in \\{0, 1\\}$.\nFor $y=0$ (benign):\n1.  Compute the calibration scores: $\\mathcal{S}_0 = \\{1 - \\hat{p}_0(x_i) \\text{ for each } x_i \\text{ in the benign calibration set}\\}$. Let $n_0=|\\mathcal{S}_0|$.\n2.  Compute the test score: $s_0^{\\ast} = A(x^{\\ast}, 0) = 1 - \\hat{p}_0(x^{\\ast}) = 1 - (1 - \\hat{p}_1(x^{\\ast})) = \\hat{p}_1(x^{\\ast})$.\n3.  Count $k_0 = |\\{s \\in \\mathcal{S}_0 : s \\ge s_0^{\\ast}\\}|$.\n4.  Include $0$ in $\\Gamma_{\\alpha}(x^{\\ast})$ if $\\frac{k_0 + 1}{n_0 + 1}  \\alpha$.\n\nFor $y=1$ (pathogenic):\n1.  Compute the calibration scores: $\\mathcal{S}_1 = \\{1 - \\hat{p}_1(x_i) \\text{ for each } x_i \\text{ in the pathogenic calibration set}\\}$. Let $n_1=|\\mathcal{S}_1|$.\n2.  Compute the test score: $s_1^{\\ast} = A(x^{\\ast}, 1) = 1 - \\hat{p}_1(x^{\\ast})$.\n3.  Count $k_1 = |\\{s \\in \\mathcal{S}_1 : s \\ge s_1^{\\ast}\\}|$.\n4.  Include $1$ in $\\Gamma_{\\alpha}(x^{\\ast})$ if $\\frac{k_1 + 1}{n_1 + 1}  \\alpha$.\n\nThe resulting set $\\Gamma_{\\alpha}(x^{\\ast})$ is the union of all labels that satisfy their respective conditions. This construction guarantees marginal coverage, i.e., for any class $y$, the probability of including the true label in the prediction set is at least $1-\\alpha$, i.e., $P(y \\in \\Gamma_{\\alpha}(X) | Y=y) \\ge 1-\\alpha$.", "answer": "```python\nimport numpy as np\n\ndef compute_prediction_set(calib_p0, calib_p1, test_p1, alpha):\n    \"\"\"\n    Computes the class-conditional conformal prediction set.\n\n    Args:\n        calib_p0 (list): List of p_0 probabilities for benign calibration set.\n        calib_p1 (list): List of p_1 probabilities for pathogenic calibration set.\n        test_p1 (float): The p_1 probability for the test variant.\n        alpha (float): The target miscoverage level.\n\n    Returns:\n        list: The prediction set, a list of integers (0 or 1).\n    \"\"\"\n    prediction_set = []\n\n    # ----------- Label 0 (Benign) -----------\n    n0 = len(calib_p0)\n    if n0  0:\n        # Nonconformity score for calibration is 1 - p_0\n        scores_0 = 1 - np.array(calib_p0)\n        \n        # Test score for hypothesis y=0 is A(x*, 0) = 1 - p_0(x*) = p_1(x*)\n        test_score_0 = test_p1\n        \n        # Count calibration scores = test score\n        k0 = np.sum(scores_0 = test_score_0)\n        \n        # p-value check\n        p_value_0 = (k0 + 1) / (n0 + 1)\n        if p_value_0  alpha:\n            prediction_set.append(0)\n    # If n0 is 0, p-value is 1/(0+1)=1, which is  alpha. The rule includes the label.\n    # However, problem cases have n_y  0. The definition is upheld for n_y=0.\n    elif n0 == 0:\n         # p-value = 1/(0+1) = 1. Since alpha is in (0,1), 1  alpha is always true.\n         prediction_set.append(0)\n\n\n    # ----------- Label 1 (Pathogenic) -----------\n    n1 = len(calib_p1)\n    if n1  0:\n        # Nonconformity score for calibration is 1 - p_1\n        scores_1 = 1 - np.array(calib_p1)\n        \n        # Test score for hypothesis y=1 is A(x*, 1) = 1 - p_1(x*)\n        test_score_1 = 1 - test_p1\n        \n        # Count calibration scores = test score\n        k1 = np.sum(scores_1 = test_score_1)\n        \n        # p-value check\n        p_value_1 = (k1 + 1) / (n1 + 1)\n        if p_value_1  alpha:\n            prediction_set.append(1)\n    elif n1 == 0:\n        prediction_set.append(1)\n        \n    prediction_set.sort()\n    return prediction_set\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases and prints the result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            'calib_p0': [0.85, 0.75, 0.65, 0.55, 0.50],\n            'calib_p1': [0.92, 0.80, 0.70, 0.60],\n            'test_p1': 0.88,\n            'alpha': 0.20\n        },\n        {\n            'calib_p0': [0.70],\n            'calib_p1': [0.70],\n            'test_p1': 0.60,\n            'alpha': 0.50\n        },\n        {\n            'calib_p0': [0.52, 0.62],\n            'calib_p1': [0.55, 0.65],\n            'test_p1': 0.58,\n            'alpha': 0.25\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_prediction_set(\n            case['calib_p0'],\n            case['calib_p1'],\n            case['test_p1'],\n            case['alpha']\n        )\n        results.append(str(result).replace(\" \", \"\"))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4330594"}]}