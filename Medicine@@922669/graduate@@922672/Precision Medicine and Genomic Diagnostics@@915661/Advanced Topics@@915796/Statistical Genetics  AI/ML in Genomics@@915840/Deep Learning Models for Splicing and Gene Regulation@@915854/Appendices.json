{"hands_on_practices": [{"introduction": "Before a deep learning model can predict splicing outcomes, we need a reliable quantitative target to train it on. This practice grounds your understanding in the fundamental task of measuring splicing from raw RNA-sequencing data by deriving the 'Percent Spliced-In' ($\\Psi$) metric from junction-spanning read counts [@problem_id:4330953]. You will use the principle of maximum likelihood estimation to formalize this common calculation and, critically, examine the experimental assumptions that must hold for this estimate to be biologically meaningful.", "problem": "In a genomic diagnostics study aiming to prioritize splice-altering variants for precision medicine, a deep learning model is trained to predict the cassette exon inclusion probability from sequence context. For each exon in a training cohort, short-read ribonucleic acid sequencing (RNA-seq) provides counts of reads that unambiguously span the inclusion-defining junction(s) and the skipping-defining junction(s). Denote by $I$ the number of reads that support inclusion and by $S$ the number of reads that support skipping for a given cassette exon. Assume that ambiguous reads are filtered and that the exon behaves as a simple cassette event with two mutually exclusive isoforms.\n\nStarting from first principles of probability and a generative count model consistent with standard RNA-seq sampling and mapping assumptions, derive the maximum likelihood estimator for the true inclusion probability $\\psi$ based on the observed counts $(I,S)$. Your derivation must begin from the likelihood implied by independent sampling of junction-spanning reads from a two-isoform mixture and should clearly state all modeling assumptions required for the estimator to be unbiased in expectation. Then, using your derived estimator, compute the estimated inclusion probability for the observed counts $I = 152$ and $S = 48$. Express your final answer as a decimal in $[0,1]$ rounded to four significant figures.", "solution": "The problem asks for the derivation of the maximum likelihood estimator for the cassette exon inclusion probability, denoted by $\\psi$, based on observed read counts from an RNA-seq experiment. Furthermore, it requires a statement of the assumptions under which this estimator is unbiased and a calculation for a specific set of counts.\n\n### Part 1: Derivation of the Maximum Likelihood Estimator ($\\hat{\\psi}$)\n\nThe problem states that for a given cassette exon, we observe $I$ reads supporting the inclusion isoform and $S$ reads supporting the skipping isoform. We are given that these are the only two outcomes, a consequence of the simple cassette event assumption. The total number of informative junction-spanning reads is $N = I + S$.\n\nLet us model the process of sampling these reads from the underlying population of RNA molecules. We define the parameter of interest, $\\psi$, as the true proportion of inclusion isoforms among the two possibilities in the biological sample. Therefore, $\\psi \\in [0, 1]$. The proportion of skipping isoforms is correspondingly $1-\\psi$.\n\nUnder the standard assumption of uniform and independent sampling of molecules for sequencing, the event of observing a single junction-spanning read can be modeled as a Bernoulli trial. Let $X_i$ for $i \\in \\{1, 2, \\dots, N\\}$ be a random variable representing the outcome of the $i$-th read. We can assign $X_i = 1$ if the read supports inclusion and $X_i = 0$ if it supports skipping. The probability of success (observing an inclusion read) is $P(X_i=1) = \\psi$, and the probability of failure is $P(X_i=0) = 1-\\psi$.\n\nThe observed data consists of the total count of inclusion reads, $I$, and skipping reads, $S$. The total number of inclusion reads is the sum of these Bernoulli trials: $I = \\sum_{i=1}^N X_i$. The number of trials $N$ is fixed by the data, $N=I+S$. The random variable $I$ therefore follows a Binomial distribution with parameters $N$ and $\\psi$, denoted as $I \\sim \\text{Binomial}(N, \\psi)$.\n\nThe probability mass function for a Binomial distribution gives the probability of observing exactly $I$ successes in $N$ trials. This function, when viewed as a function of the parameter $\\psi$ for the fixed observed data $(I, S)$, is the likelihood function $L(\\psi | I, S)$.\n\n$$L(\\psi | I, S) = P(I \\text{ successes in } N \\text{ trials}) = \\binom{N}{I} \\psi^I (1-\\psi)^{N-I}$$\nSubstituting $N = I+S$, we get:\n$$L(\\psi | I, S) = \\binom{I+S}{I} \\psi^I (1-\\psi)^S$$\n\nTo find the maximum likelihood estimator (MLE) for $\\psi$, denoted $\\hat{\\psi}$, we must find the value of $\\psi$ that maximizes $L(\\psi | I, S)$. It is computationally simpler to maximize the natural logarithm of the likelihood function, the log-likelihood $\\ell(\\psi) = \\ln(L(\\psi))$. Since the logarithm is a monotonically increasing function, maximizing $\\ell(\\psi)$ is equivalent to maximizing $L(\\psi)$.\n\nThe log-likelihood function is:\n$$\\ell(\\psi) = \\ln \\left( \\binom{I+S}{I} \\psi^I (1-\\psi)^S \\right)$$\n$$\\ell(\\psi) = \\ln\\left(\\binom{I+S}{I}\\right) + I \\ln(\\psi) + S \\ln(1-\\psi)$$\n\nTo find the maximum, we compute the derivative of $\\ell(\\psi)$ with respect to $\\psi$ and set it to zero.\n$$\\frac{d\\ell}{d\\psi} = \\frac{d}{d\\psi} \\left[ \\ln\\left(\\binom{I+S}{I}\\right) + I \\ln(\\psi) + S \\ln(1-\\psi) \\right]$$\n$$\\frac{d\\ell}{d\\psi} = 0 + \\frac{I}{\\psi} + \\frac{S}{1-\\psi} \\cdot (-1) = \\frac{I}{\\psi} - \\frac{S}{1-\\psi}$$\n\nSetting the derivative to zero to find the critical point:\n$$\\frac{I}{\\hat{\\psi}} - \\frac{S}{1-\\hat{\\psi}} = 0$$\n$$\\frac{I}{\\hat{\\psi}} = \\frac{S}{1-\\hat{\\psi}}$$\n$$I(1-\\hat{\\psi}) = S\\hat{\\psi}$$\n$$I - I\\hat{\\psi} = S\\hat{\\psi}$$\n$$I = (I+S)\\hat{\\psi}$$\n\nSolving for $\\hat{\\psi}$, we obtain the maximum likelihood estimator:\n$$\\hat{\\psi} = \\frac{I}{I+S}$$\n\nTo confirm this corresponds to a maximum, we check the second derivative of the log-likelihood:\n$$\\frac{d^2\\ell}{d\\psi^2} = \\frac{d}{d\\psi} \\left( \\frac{I}{\\psi} - \\frac{S}{1-\\psi} \\right) = -\\frac{I}{\\psi^2} - \\frac{S}{(1-\\psi)^2}(-1)^2 = -\\frac{I}{\\psi^2} - \\frac{S}{(1-\\psi)^2}$$\nFor non-trivial cases where $I  0$ and $S  0$, and since $\\psi \\in (0,1)$, both terms are negative. Thus, $\\frac{d^2\\ell}{d\\psi^2}  0$, which confirms that $\\hat{\\psi}$ is indeed a maximum.\n\n### Part 2: Assumptions for Unbiasedness\n\nAn estimator $\\hat{\\theta}$ for a parameter $\\theta$ is unbiased if its expected value equals the true value of the parameter, i.e., $E[\\hat{\\theta}] = \\theta$. Here, we examine $E[\\hat{\\psi}]$.\n\nThe estimator is $\\hat{\\psi} = \\frac{I}{I+S} = \\frac{I}{N}$. The total number of reads $N$ can be treated as a fixed quantity for the expectation calculation, which is conditioned on the total depth of sequencing for this event. The number of inclusion reads $I$ is a random variable, which follows the distribution $I \\sim \\text{Binomial}(N, \\psi)$.\n\nThe expected value of a Binomial random variable is $E[I] = N\\psi$.\nThe expectation of our estimator is:\n$$E[\\hat{\\psi}] = E\\left[\\frac{I}{N}\\right] = \\frac{1}{N}E[I] = \\frac{1}{N}(N\\psi) = \\psi$$\nThus, the estimator $\\hat{\\psi}$ is unbiased.\n\nHowever, this result relies on the critical assumption that the underlying generative model, $\\text{Binomial}(N, \\psi)$, is a correct representation of the physical process. For $E[\\hat{\\psi}]$ to be equal to the true biological proportion $\\psi$, the following assumptions must hold:\n1.  **Independent and Identical Distribution of Reads**: Each junction-spanning read is an independent sample from the same underlying distribution of isoforms. This is the core of the Bernoulli trial model.\n2.  **No Systematic Bias**: The probability of observing a read supporting a given isoform must be equal to the true relative abundance of that isoform. This implies a lack of technical bias in the RNA-seq workflow. Any process that systematically favors the generation, sequencing, or mapping of reads from one isoform over the other will violate this assumption and introduce bias. Specific sources of potential bias include:\n    -   **Sequence-specific biases**: Differences in GC content or secondary structure could lead to differential amplification efficiency during PCR or reverse transcription.\n    -   **Mapping biases**: Differences in sequence complexity or uniqueness of the junctional sequences could cause reads from one isoform to be mapped with higher or lower confidence than the other. For instance, if one junction's sequence is less unique, reads originating from it may be discarded as multi-mappers, biasing the counts.\n    -   **Read length and position biases**: The efficiency of generating a junction-spanning read can depend on the read length and the specific positions of the splice sites, potentially creating different \"effective lengths\" for sampling inclusion versus skipping junctions.\n\nIn summary, for the estimator $\\hat{\\psi} = I/(I+S)$ to be unbiased, we must assume that the experimental and computational pipeline provides an unbiased readout of the relative molecular counts of the two splice isoforms.\n\n### Part 3: Numerical Calculation\n\nWe are given the observed counts $I = 152$ and $S = 48$.\nThe total number of junction-spanning reads is $N = I + S = 152 + 48 = 200$.\n\nUsing the derived maximum likelihood estimator:\n$$\\hat{\\psi} = \\frac{I}{I+S} = \\frac{152}{200}$$\n$$\\hat{\\psi} = \\frac{76}{100} = 0.76$$\n\nThe problem requires the answer to be expressed as a decimal rounded to four significant figures. To express $0.76$ with four significant figures, we add trailing zeros.\n$$\\hat{\\psi} = 0.7600$$\nThis value represents the estimated probability that a cassette exon is included, based on the provided read evidence.", "answer": "$$\\boxed{0.7600}$$", "id": "4330953"}, {"introduction": "Deep learning models often outperform classic methods like Position Weight Matrices (PWMs) for complex genomic prediction tasks. This hands-on coding exercise provides a direct comparison of these two approaches, challenging you to implement both a PWM and a simple Convolutional Neural Network (CNN) to predict a genetic variant's impact on splicing strength [@problem_id:4330978]. By analyzing cases where the models' predictions diverge, you will build concrete intuition for why CNNs excel at capturing the non-linear, context-dependent sequence features that govern gene regulation.", "problem": "You are given a formal comparison task between two sequence-based models of splice donor strength used in precision medicine and genomic diagnostics: a Position Weight Matrix (PWM) and a Convolutional Neural Network (CNN). The goal is to compute the predicted impact of a single-nucleotide variant at a specified donor position and to reconcile disagreements between the two models through explicit computation. All computations must be performed from first principles and implemented in a complete, runnable program.\n\nA donor site window is defined as a nucleotide sequence of length $9$ covering positions $-3,-2,-1,+1,+2,+3,+4,+5,+6$ relative to the exon-intron boundary, where $+1$ is the first intronic base and $+2$ is the second intronic base. The canonical donor dinucleotide $+1,+2$ is often $G,T$ in the human genome, but arbitrary bases may occur in the provided test cases. Let the alphabet be $\\{A,C,G,T\\}$, with one-hot encoding mapping each base to a vector in $\\mathbb{R}^4$ with basis order $(A,C,G,T)$.\n\nModel definitions to implement:\n\n1. Position Weight Matrix (PWM) model.\n   - Assumption: Conditional independence across positions.\n   - Score definition: For a sequence $s$ of length $9$, the PWM log-odds score is\n     $$S_{\\text{PWM}}(s) = \\sum_{p=0}^{8} W_{p, b(s,p)} + b_0,$$\n     where $p$ indexes window positions $0,\\dots,8$ corresponding to $-3,-2,-1,+1,+2,+3,+4,+5,+6$ respectively, $b(s,p)\\in\\{0,1,2,3\\}$ is the index of the base at position $p$ under the order $(A,C,G,T)$, $W \\in \\mathbb{R}^{9\\times 4}$ is the position weight matrix of log-odds contributions, and $b_0\\in\\mathbb{R}$ is a bias term. The predicted impact of a variant is the change in score $\\Delta_{\\text{PWM}} = S_{\\text{PWM}}(s_{\\text{alt}}) - S_{\\text{PWM}}(s_{\\text{ref}})$.\n   - Parameters to use:\n     - Bias: $b_0 = 0$.\n     - Matrix $W$ given by rows $W_{p,:}$ with order $(A,C,G,T)$:\n       - $W_{0,:} = [0.2,0.0,0.0,0.0]$\n       - $W_{1,:} = [0.0,0.0,0.2,0.0]$\n       - $W_{2,:} = [0.0,0.0,0.5,0.0]$\n       - $W_{3,:} = [0.0,0.0,2.5,0.0]$\n       - $W_{4,:} = [0.0,0.0,0.0,2.5]$\n       - $W_{5,:} = [0.3,0.0,0.0,0.0]$\n       - $W_{6,:} = [0.0,0.0,0.2,0.0]$\n       - $W_{7,:} = [-0.1,-0.2,0.4,-0.2]$\n       - $W_{8,:} = [0.1,0.0,0.0,0.1]$\n\n2. Convolutional Neural Network (CNN) model.\n   - Architecture: One-hot input $X\\in\\{0,1\\}^{9\\times 4}$, two one-dimensional convolutional filters of width $6$ with biases, rectified linear unit (ReLU), global max pooling over all valid start positions, followed by a linear head to produce a logit. Let $\\phi(x)=\\max(0,x)$ be the rectified linear unit.\n   - Convolution: For filter $j\\in\\{0,1\\}$ with weights $F^{(j)}\\in \\mathbb{R}^{6\\times 4}$ and bias $c_j\\in \\mathbb{R}$, the valid convolution at start index $k\\in\\{0,1,2,3\\}$ (covering positions $k$ through $k+5$) is\n     $$z^{(j)}_k = c_j + \\sum_{o=0}^{5} \\sum_{n=0}^{3} F^{(j)}_{o,n} \\, X_{k+o, n}.$$\n     The activation for filter $j$ is\n     $$a_j = \\max_{k\\in\\{0,1,2,3\\}} \\phi\\!\\left(z^{(j)}_k\\right).$$\n     The final CNN splice-strength logit is\n     $$Y_{\\text{CNN}}(s) = v_0 a_0 + v_1 a_1 + c,$$\n     where $v\\in\\mathbb{R}^2$ and $c\\in\\mathbb{R}$ are the linear head parameters. The predicted impact is $\\Delta_{\\text{CNN}} = Y_{\\text{CNN}}(s_{\\text{alt}}) - Y_{\\text{CNN}}(s_{\\text{ref}})$.\n   - Parameters to use:\n     - Filter $0$ weights and bias $c_0$:\n       - Nonzero weights: at offset $0$ for $G$: $+2.0$; at offset $1$ for $T$: $+2.0$; at offset $4$ for $G$: $+1.0$. All other entries are $0$.\n       - Bias: $c_0 = -2.0$.\n     - Filter $1$ weights and bias $c_1$:\n       - Nonzero weights: at offset $2$ for $A$: $+1.0$; at offset $3$ for $A$: $+1.0$; at offset $4$ for $A$: $+1.5$. All other entries are $0$.\n       - Bias: $c_1 = -2.0$.\n     - Linear head: $v = [1.0, 1.0]$, $c = 0.0$.\n\nVariant definition and coordinate mapping:\n- The variant is a single-nucleotide change from $G$ to $A$ at position $+5$, which corresponds to window index $p=7$. If the reference base at index $p=7$ is not $G$, then by definition the variant does not alter the sequence, and the predicted impact is $\\Delta_{\\text{PWM}} = 0$ and $\\Delta_{\\text{CNN}} = 0$.\n\nTask:\n- For each test sequence below, compute the reference score and the alternate score under both models and report the changes $\\Delta_{\\text{PWM}}$ and $\\Delta_{\\text{CNN}}$ defined above. Then report whether the two models agree on the direction of effect by comparing the signs of the two changes, where the sign function is $-1$ for negative, $0$ for exactly zero, and $+1$ for positive. Agreement is defined as equality of these sign values.\n- Round all floating-point outputs to three decimal places.\n\nTest suite (three donor windows of length $9$ with the variant applied at index $7$; bases are uppercase letters from $\\{A,C,G,T\\}$):\n1. Case $1$: $s_{\\text{ref}} =$ \"AGGGTCAGA\". The alternate sequence $s_{\\text{alt}}$ is obtained by changing index $7$ from $G$ to $A$.\n2. Case $2$: $s_{\\text{ref}} =$ \"TAGGTAAGT\". The alternate sequence $s_{\\text{alt}}$ is obtained by changing index $7$ from $G$ to $A$.\n3. Case $3$: $s_{\\text{ref}} =$ \"CAGGTGCAT\". The alternate sequence $s_{\\text{alt}}$ is obtained by attempting to change index $7$ from $G$ to $A$, but if the base is not $G$, then $s_{\\text{alt}} = s_{\\text{ref}}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n  $$[\\Delta_{\\text{PWM}}^{(1)}, \\Delta_{\\text{CNN}}^{(1)}, \\text{agree}^{(1)}, \\Delta_{\\text{PWM}}^{(2)}, \\Delta_{\\text{CNN}}^{(2)}, \\text{agree}^{(2)}, \\Delta_{\\text{PWM}}^{(3)}, \\Delta_{\\text{CNN}}^{(3)}, \\text{agree}^{(3)}],$$\n  where $\\text{agree}^{(i)}$ is $1$ if the signs match and $0$ otherwise. All $\\Delta$ values must be rounded to three decimal places, and agreement values must be integers. No additional text should be printed.", "solution": "The user has provided a well-posed computational problem to compare two models of splice donor strength, a Position Weight Matrix (PWM) and a simple Convolutional Neural Network (CNN). The task is to calculate the predicted impact of a single-nucleotide variant on a given DNA sequence for both models and determine if their predictions agree in direction. The problem is scientifically grounded, well-defined, and all necessary parameters are provided.\n\nFirst, we establish the common framework. The DNA alphabet is $\\{A, C, G, T\\}$. We map these bases to integer indices $\\{0, 1, 2, 3\\}$, respectively. A sequence of length $L=9$ is represented as a one-hot encoded matrix $X \\in \\{0, 1\\}^{9 \\times 4}$. The problem defines a specific single-nucleotide variant: a change from base $G$ to base $A$ at position $+5$, which corresponds to array index $p=7$. If the reference base at this position is not $G$, the variant does not apply, and the score change is defined as $0$ for both models.\n\nPWM Model Analysis\n\nThe PWM score for a sequence $s$ is given by the sum of position-specific log-odds, plus a bias term:\n$$S_{\\text{PWM}}(s) = \\sum_{p=0}^{8} W_{p, b(s,p)} + b_0$$\nwhere $W \\in \\mathbb{R}^{9 \\times 4}$ is the weight matrix, $p$ is the position index from $0$ to $8$, $b(s,p)$ is the index of the base at position $p$ of sequence $s$, and the bias $b_0=0$.\n\nThe predicted impact of the variant, $\\Delta_{\\text{PWM}}$, is the difference in scores between the alternate sequence ($s_{\\text{alt}}$) and the reference sequence ($s_{\\text{ref}}$):\n$$\\Delta_{\\text{PWM}} = S_{\\text{PWM}}(s_{\\text{alt}}) - S_{\\text{PWM}}(s_{\\text{ref}})$$\nSince the variant only alters the base at a single position, $p=7$, the contributions from all other positions cancel out:\n$$\\Delta_{\\text{PWM}} = \\left(W_{7, b(s_{\\text{alt}}, 7)} + \\sum_{p \\neq 7} W_{p, b(s,p)}\\right) - \\left(W_{7, b(s_{\\text{ref}}, 7)} + \\sum_{p \\neq 7} W_{p, b(s,p)}\\right)$$\n$$\\Delta_{\\text{PWM}} = W_{7, b(s_{\\text{alt}}, 7)} - W_{7, b(s_{\\text{ref}}, 7)}$$\nThe variant is $G \\to A$, so $b(s_{\\text{ref}}, 7)=2$ and $b(s_{\\text{alt}}, 7)=0$. The weights for position $p=7$ are given as $W_{7,:} = [-0.1, -0.2, 0.4, -0.2]$.\nTherefore, for any sequence where the variant applies:\n$$\\Delta_{\\text{PWM}} = W_{7,0} - W_{7,2} = -0.1 - 0.4 = -0.6$$\n\nCNN Model Analysis\n\nThe CNN architecture is more complex. It consists of a convolutional layer with two filters, a ReLU activation function, a global max-pooling layer, and a final linear layer.\nFor each filter $j \\in \\{0, 1\\}$ of width $6$, we compute its pre-activation $z_k^{(j)}$ for each valid starting position $k \\in \\{0, 1, 2, 3\\}$ of the input sequence $X$:\n$$z^{(j)}_k = c_j + \\sum_{o=0}^{5} \\sum_{n=0}^{3} F^{(j)}_{o,n} \\, X_{k+o, n}$$\nwhere $F^{(j)} \\in \\mathbb{R}^{6 \\times 4}$ is the filter weight matrix and $c_j$ is its bias.\nThe post-activation feature map is passed through a ReLU function, $a_k^{(j)} = \\phi(z_k^{(j)}) = \\max(0, z_k^{(j)})$.\nGlobal max-pooling takes the maximum value over all start positions for each filter: $a_j = \\max_{k} a_k^{(j)}$.\nFinally, the CNN logit is a linear combination of these pooled activations:\n$$Y_{\\text{CNN}}(s) = v_0 a_0 + v_1 a_1 + c$$\nThe parameters are given as $v = [1.0, 1.0]$ and $c=0.0$. The impact is $\\Delta_{\\text{CNN}} = Y_{\\text{CNN}}(s_{\\text{alt}}) - Y_{\\text{CNN}}(s_{\\text{ref}})$.\n\nWe must compute the full scores for $s_{\\text{ref}}$ and $s_{\\text{alt}}$ to find $\\Delta_{\\text{CNN}}$.\n\nCase 1: $s_{\\text{ref}} = \\text{\"AGGGTCAGA\"}$\nThe base at index $7$ is $G$, so the variant applies. $s_{\\text{alt}} = \\text{\"AGGTCAAGA\"}$.\n$\\Delta_{\\text{PWM}}^{(1)} = -0.6$.\n\nFor the CNN, we evaluate $s_{\\text{ref}}$ and $s_{\\text{alt}}$.\nFor $s_{\\text{ref}}$:\nFilter $0$ (weights for $G$ at offset $0$, $T$ at offset $1$, $G$ at offset $4$; bias $c_0=-2.0$):\n- $k \\in \\{0,1,2,3\\}$: pre-activations are $z^{(0)} = [-2.0, 0.0, 0.0, 3.0]$.\n- ReLU outputs: $[0.0, 0.0, 0.0, 3.0]$. Max-pooled activation $a_{0, \\text{ref}} = 3.0$.\nFilter $1$ (weights for $A$ at offsets $2,3,4$; bias $c_1=-2.0$):\n- $k \\in \\{0,1,2,3\\}$: pre-activations are $z^{(1)} = [-2.0, -2.0, -0.5, -1.0]$.\n- ReLU outputs: $[0.0, 0.0, 0.0, 0.0]$. Max-pooled activation $a_{1, \\text{ref}} = 0.0$.\n$Y_{\\text{CNN}}(s_{\\text{ref}}) = 1.0 \\times 3.0 + 1.0 \\times 0.0 + 0.0 = 3.0$.\n\nFor $s_{\\text{alt}} = \\text{\"AGGTCAAGA\"}$, the change at index $7$ affects windows starting at $k=2,3$.\nFilter $0$: The $G$ at offset $4$ in the $k=3$ window is lost. $z^{(0)}_{3,\\text{alt}}=2.0$. The new pre-activations are $z^{(0)} = [-2.0, 0.0, 0.0, 2.0]$. $a_{0, \\text{alt}} = 2.0$.\nFilter $1$: The new $A$ at offset $4$ in the $k=3$ window is matched. $z^{(1)}_{3,\\text{alt}}=0.5$. The new pre-activations are $z^{(1)} = [-2.0, -2.0, -0.5, 0.5]$. $a_{1, \\text{alt}} = 0.5$.\n$Y_{\\text{CNN}}(s_{\\text{alt}}) = 1.0 \\times 2.0 + 1.0 \\times 0.5 + 0.0 = 2.5$.\n$\\Delta_{\\text{CNN}}^{(1)} = 2.5 - 3.0 = -0.5$.\nAgreement: $\\text{sign}(-0.6) = -1$ and $\\text{sign}(-0.5) = -1$. The signs match, so agreement is $1$.\n\nCase 2: $s_{\\text{ref}} = \\text{\"TAGGTAAGT\"}$\nThe base at index $7$ is $G$, so the variant applies. $s_{\\text{alt}} = \\text{\"TAGGTAAAT\"}$.\n$\\Delta_{\\text{PWM}}^{(2)} = -0.6$.\n\nFor the CNN, we evaluate $s_{\\text{ref}}$ and $s_{\\text{alt}}$.\nFor $s_{\\text{ref}}$:\nFilter $0$: $z^{(0)} = [-2.0, -2.0, 0.0, 3.0]$. $a_{0, \\text{ref}} = 3.0$.\nFilter $1$: $z^{(1)} = [-2.0, -0.5, 0.5, 0.0]$. $a_{1, \\text{ref}} = 0.5$.\n$Y_{\\text{CNN}}(s_{\\text{ref}}) = 1.0 \\times 3.0 + 1.0 \\times 0.5 + 0.0 = 3.5$.\n\nFor $s_{\\text{alt}} = \\text{\"TAGGTAAAT\"}$:\nFilter $0$: The $G$ at offset $4$ in the $k=3$ window is lost. $z^{(0)}_{3,\\text{alt}} = 2.0$. $a_{0, \\text{alt}} = 2.0$.\nFilter $1$: The new $A$ at offset $4$ in the $k=3$ window is matched. The pre-activations for this filter become $z^{(1)} = [-2.0, -0.5, 0.5, 1.5]$. $a_{1, \\text{alt}} = 1.5$.\n$Y_{\\text{CNN}}(s_{\\text{alt}}) = 1.0 \\times 2.0 + 1.0 \\times 1.5 + 0.0 = 3.5$.\n$\\Delta_{\\text{CNN}}^{(2)} = 3.5 - 3.5 = 0.0$.\nAgreement: $\\text{sign}(-0.6) = -1$ and $\\text{sign}(0.0) = 0$. The signs do not match, so agreement is $0$.\n\nCase 3: $s_{\\text{ref}} = \\text{\"CAGGTGCAT\"}$\nThe base at index $7$ is $A$. According to the problem definition, the variant does not apply.\nTherefore, $\\Delta_{\\text{PWM}}^{(3)} = 0.0$ and $\\Delta_{\\text{CNN}}^{(3)} = 0.0$.\nAgreement: $\\text{sign}(0.0) = 0$ for both. The signs match, so agreement is $1$.\n\nSummary of Results:\n- Case $1$: $\\Delta_{\\text{PWM}} = -0.600$, $\\Delta_{\\text{CNN}} = -0.500$, Agreement = $1$.\n- Case $2$: $\\Delta_{\\text{PWM}} = -0.600$, $\\Delta_{\\text{CNN}} = 0.000$, Agreement = $0$.\n- Case $3$: $\\Delta_{\\text{PWM}} = 0.000$, $\\Delta_{\\text{CNN}} = 0.000$, Agreement = $1$.\nThe final output is the flattened list of these values, rounded as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model comparison problem for splice site variants.\n    \"\"\"\n    # Define problem parameters\n    BASE_TO_IDX = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    \n    # PWM model parameters\n    W_PWM = np.array([\n        [0.2, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.2, 0.0],\n        [0.0, 0.0, 0.5, 0.0],\n        [0.0, 0.0, 2.5, 0.0],\n        [0.0, 0.0, 0.0, 2.5],\n        [0.3, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.2, 0.0],\n        [-0.1, -0.2, 0.4, -0.2],\n        [0.1, 0.0, 0.0, 0.1]\n    ])\n\n    # CNN model parameters\n    # Filter 0\n    F0_CNN = np.zeros((6, 4))\n    F0_CNN[0, BASE_TO_IDX['G']] = 2.0\n    F0_CNN[1, BASE_TO_IDX['T']] = 2.0\n    F0_CNN[4, BASE_TO_IDX['G']] = 1.0\n    C0_CNN = -2.0\n\n    # Filter 1\n    F1_CNN = np.zeros((6, 4))\n    F1_CNN[2, BASE_TO_IDX['A']] = 1.0\n    F1_CNN[3, BASE_TO_IDX['A']] = 1.0\n    F1_CNN[4, BASE_TO_IDX['A']] = 1.5\n    C1_CNN = -2.0\n\n    # Linear Head\n    V_CNN = np.array([1.0, 1.0])\n    C_HEAD_CNN = 0.0\n\n    def one_hot_encode(seq: str) - np.ndarray:\n        \"\"\"Converts a DNA sequence to its one-hot encoded representation.\"\"\"\n        encoding = np.zeros((len(seq), 4), dtype=np.int8)\n        for i, base in enumerate(seq):\n            if base in BASE_TO_IDX:\n                encoding[i, BASE_TO_IDX[base]] = 1\n        return encoding\n\n    def get_cnn_score(seq_one_hot: np.ndarray) - float:\n        \"\"\"Calculates the CNN logit for a given one-hot encoded sequence.\"\"\"\n        num_positions = seq_one_hot.shape[0]\n        filter_width = 6\n        num_windows = num_positions - filter_width + 1\n\n        # Filter 0 convolution, ReLU, and max-pooling\n        z0 = np.zeros(num_windows)\n        for k in range(num_windows):\n            window = seq_one_hot[k:k+filter_width, :]\n            z0[k] = C0_CNN + np.sum(F0_CNN * window)\n        a0 = np.max(np.maximum(0, z0))\n\n        # Filter 1 convolution, ReLU, and max-pooling\n        z1 = np.zeros(num_windows)\n        for k in range(num_windows):\n            window = seq_one_hot[k:k+filter_width, :]\n            z1[k] = C1_CNN + np.sum(F1_CNN * window)\n        a1 = np.max(np.maximum(0, z1))\n\n        # Linear head\n        logit = V_CNN[0] * a0 + V_CNN[1] * a1 + C_HEAD_CNN\n        return logit\n\n    test_cases = [\n        \"AGGGTCAGA\",\n        \"TAGGTAAGT\",\n        \"CAGGTGCAT\",\n    ]\n\n    results = []\n    for s_ref_str in test_cases:\n        variant_pos = 7\n        ref_base = 'G'\n        alt_base = 'A'\n\n        if s_ref_str[variant_pos] != ref_base:\n            delta_pwm = 0.0\n            delta_cnn = 0.0\n        else:\n            s_alt_str = s_ref_str[:variant_pos] + alt_base + s_ref_str[variant_pos+1:]\n\n            # PWM delta calculation\n            delta_pwm = W_PWM[variant_pos, BASE_TO_IDX[alt_base]] - W_PWM[variant_pos, BASE_TO_IDX[ref_base]]\n            \n            # CNN delta calculation\n            s_ref_one_hot = one_hot_encode(s_ref_str)\n            s_alt_one_hot = one_hot_encode(s_alt_str)\n            \n            score_ref = get_cnn_score(s_ref_one_hot)\n            score_alt = get_cnn_score(s_alt_one_hot)\n            delta_cnn = score_alt - score_ref\n        \n        # Agreement calculation\n        sign_pwm = np.sign(delta_pwm)\n        sign_cnn = np.sign(delta_cnn)\n        agreement = 1 if sign_pwm == sign_cnn else 0\n\n        # Append formatted results\n        results.append(f\"{delta_pwm:.3f}\")\n        results.append(f\"{delta_cnn:.3f}\")\n        results.append(str(agreement))\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4330978"}, {"introduction": "For predictive models to be deployed confidently in genomic diagnostics, we must understand their behavior and limitations. This advanced practice introduces the concept of adversarial examples—minimal input perturbations designed to fool a model—as a powerful technique for probing model decision boundaries and testing robustness [@problem_id:4330944]. You will implement a greedy algorithm to find the fewest nucleotide changes needed to flip a splice site classifier's prediction, providing a tangible method for model interrogation and revealing how sensitive its decisions are to small changes in the input DNA sequence.", "problem": "You are given a binary classifier for splice site detection on deoxyribonucleic acid (DNA) sequences modeled as a linear logit followed by a sigmoid function. A DNA sequence of fixed length $L$ is represented over the alphabet $\\mathcal{A}=\\{\\text{A},\\text{C},\\text{G},\\text{T}\\}$. Each sequence is encoded into a one-hot matrix $\\mathbf{X}\\in\\{0,1\\}^{L\\times 4}$ under a fixed base order $(\\text{A},\\text{C},\\text{G},\\text{T})$, so that at position $i\\in\\{0,\\dots,L-1\\}$ exactly one of the four entries is $1$. The classifier has a weight matrix $\\mathbf{W}\\in\\mathbb{R}^{L\\times 4}$ and a scalar bias $b\\in\\mathbb{R}$. For an input sequence $\\mathbf{X}$, the logit is\n$$\nz(\\mathbf{X})=\\sum_{i=0}^{L-1}\\sum_{j=0}^{3} W_{i,j}\\,X_{i,j} + b,\n$$\nand the predicted probability is $p(\\mathbf{X})=\\sigma\\!\\left(z(\\mathbf{X})\\right)$, where $\\sigma(u)=\\frac{1}{1+\\exp(-u)}$ is the sigmoid function. The predicted label is\n$$\n\\hat{y}(\\mathbf{X})=\\begin{cases}\n1  \\text{if } p(\\mathbf{X})0.5 \\text{ (equivalently } z(\\mathbf{X})0\\text{)},\\\\\n0  \\text{otherwise (equivalently } z(\\mathbf{X})\\le 0\\text{).}\n\\end{cases}\n$$\nAn adversarial nucleotide substitution at position $i$ changes the base at $i$ from its current base $a\\in\\mathcal{A}$ to any other base $b\\in\\mathcal{A}\\setminus\\{a\\}$, altering exactly one row of $\\mathbf{X}$ while maintaining the one-hot constraint. The Hamming distance $d_H$ between the original and the mutated sequences is the number of positions that are changed.\n\nYour task is to construct minimal adversarial substitutions that flip the model’s prediction. Formally, given $\\mathbf{W}$, $b$, and a sequence $\\mathbf{X}$, find the smallest integer $k\\in\\mathbb{N}$ and a sequence $\\mathbf{X}'$ differing from $\\mathbf{X}$ at exactly $k$ positions (i.e., $d_H(\\mathbf{X},\\mathbf{X}')=k$) such that $\\hat{y}(\\mathbf{X}')\\neq \\hat{y}(\\mathbf{X})$. Among all sequences with the same minimal $k$, select any valid $\\mathbf{X}'$. If no such $\\mathbf{X}'$ exists, define $k=-1$ and report failure.\n\nImplement an algorithm that, for each test sequence, computes:\n- The minimal number of substitutions $k$ required to flip the prediction.\n- A boolean indicating whether the constructed mutated sequence indeed flips the prediction under the model.\n\nThe algorithm must rely only on the given linear logit, the one-hot encoding, and the additive effect of substitutions on the logit. Do not perform exhaustive search over all possible multi-position mutations; instead, derive the optimal construction rule from first principles of the linear model and Hamming distance.\n\nUse the following fixed model and test suite:\n- Sequence length $L=12$.\n- Bias $b=-2.0$.\n- Weight matrix $\\mathbf{W}$ is zero everywhere except:\n    - At position $i=4$, the weight for base $\\text{G}$ is $+3.0$ and for bases $\\{\\text{A},\\text{C},\\text{T}\\}$ is $-1.0$.\n    - At position $i=5$, the weight for base $\\text{T}$ is $+3.0$ and for bases $\\{\\text{A},\\text{C},\\text{G}\\}$ is $-1.0$.\n    - At position $i=0$, the weight for base $\\text{A}$ is $+1.0$ and for bases $\\{\\text{C},\\text{G},\\text{T}\\}$ is $0.0$.\n- All other entries of $\\mathbf{W}$ are $0.0$.\n- Base order mapping is $(\\text{A},\\text{C},\\text{G},\\text{T})\\mapsto (0,1,2,3)$.\n\nTest sequences (all of length $12$):\n- Case $1$ (positive, easy to flip): $s_1=\\text{\"CCCCGTCCCCCC\"}$.\n- Case $2$ (negative, requires two edits): $s_2=\\text{\"CCCCCACCCCCC\"}$.\n- Case $3$ (boundary negative, requires one edit): $s_3=\\text{\"CCCCGACCCCCC\"}$.\n- Case $4$ (strong positive, requires two edits): $s_4=\\text{\"ACCCGTCCCCCC\"}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each element being a two-element list of the form $[k,\\text{flip}]$ for the corresponding test case, where $k$ is an integer and $\\text{flip}$ is a boolean. For example, the output format must be exactly like\n$$\n\\text{[[1,True],[2,False],[0,True],[3,True]]}\n$$\nbut with the actual values computed for the provided test suite. No physical units are required. Angles are not used. Percentages are not used. The final outputs for all test cases must be expressible as booleans and integers only.", "solution": "The user-provided problem has been validated and is determined to be sound.\n\n**1. Givens Extraction**\n\n-   **Model**: Linear logit followed by a sigmoid function for binary classification of DNA sequences.\n-   **Input**: A DNA sequence of length $L$ from alphabet $\\mathcal{A}=\\{\\text{A},\\text{C},\\text{G},\\text{T}\\}$.\n-   **Encoding**: One-hot matrix $\\mathbf{X}\\in\\{0,1\\}^{L\\times 4}$ with a fixed base order $(\\text{A},\\text{C},\\text{G},\\text{T})$.\n-   **Model Parameters**: A weight matrix $\\mathbf{W}\\in\\mathbb{R}^{L\\times 4}$ and a scalar bias $b\\in\\mathbb{R}$.\n-   **Logit Function**: $z(\\mathbf{X})=\\sum_{i=0}^{L-1}\\sum_{j=0}^{3} W_{i,j}\\,X_{i,j} + b$.\n-   **Prediction Rule**: $\\hat{y}(\\mathbf{X})=1$ if $z(\\mathbf{X})0$, and $\\hat{y}(\\mathbf{X})=0$ if $z(\\mathbf{X})\\le 0$.\n-   **Adversarial Attack**: A set of nucleotide substitutions at $k$ distinct positions. $d_H$ is the Hamming distance.\n-   **Task**: Find the smallest integer $k\\in\\mathbb{N}$ and a sequence $\\mathbf{X}'$ with $d_H(\\mathbf{X},\\mathbf{X}')=k$ such that $\\hat{y}(\\mathbf{X}')\\neq \\hat{y}(\\mathbf{X})$. If no such $\\mathbf{X}'$ exists, $k=-1$.\n-   **Output per Case**: A list `[k, flip]` where `$k$` is the minimal number of substitutions and `flip` is a boolean verifying the prediction flip for the constructed sequence.\n-   **Fixed Parameters**:\n    -   $L=12$.\n    -   $b=-2.0$.\n    -   $\\mathbf{W}$ is zero except:\n        -   $i=4$: $W_{4,j}=3.0$ for base $\\text{G}$, $W_{4,j}=-1.0$ for bases $\\{\\text{A},\\text{C},\\text{T}\\}$.\n        -   $i=5$: $W_{5,j}=3.0$ for base $\\text{T}$, $W_{5,j}=-1.0$ for bases $\\{\\text{A},\\text{C},\\text{G}\\}$.\n        -   $i=0$: $W_{0,j}=1.0$ for base $\\text{A}$, $W_{0,j}=0.0$ for bases $\\{\\text{C},\\text{G},\\text{T}\\}$.\n    -   Base order mapping: $(\\text{A},\\text{C},\\text{G},\\text{T})\\mapsto (0,1,2,3)$.\n-   **Test Sequences**:\n    1.  $s_1=\\text{\"CCCCGTCCCCCC\"}$\n    2.  $s_2=\\text{\"CCCCCACCCCCC\"}$\n    3.  $s_3=\\text{\"CCCCGACCCCCC\"}$\n    4.  $s_4=\\text{\"ACCCGTCCCCCC\"}$\n\n**2. Validation Verdict**\n\nThe problem is **valid**. It is scientifically grounded, well-posed, objective, self-contained, and consistent. The model is a simplified but principled representation used in bioinformatics, and the task of finding minimal adversarial perturbations is a well-defined problem in machine learning. All necessary parameters and data are provided.\n\n**3. Principle-Based Solution Design**\n\nThe core of the problem is to find the minimum number of substitutions to flip the sign of the logit $z(\\mathbf{X})$. A flip occurs if the logit crosses the threshold of $0$.\n\n**Analysis of the Logit Function**\nLet a DNA sequence be denoted by $s = s_0s_1\\dots s_{L-1}$, where $s_i \\in \\mathcal{A}$. Due to the one-hot encoding $\\mathbf{X}$, at each position $i$, only one element in the $i$-th row, $X_{i,j}$, is $1$, corresponding to the base $s_i$. Let $c(a)$ be the column index for a base $a \\in \\mathcal{A}$. The logit function can be expressed as a sum over the weights of the bases present in the sequence:\n$$z(s) = \\left(\\sum_{i=0}^{L-1} W_{i, c(s_i)}\\right) + b$$\n\n**Effect of a Single Substitution**\nConsider a single substitution at position $i$ changing the base from $s_i$ to $s'_i$. The original sequence is $s$ and the mutated sequence is $s'$. The new logit $z(s')$ is:\n$$z(s') = \\left(\\sum_{k \\neq i} W_{k, c(s_k)} + W_{i, c(s'_i)}\\right) + b$$\nThe change in the logit, $\\Delta z$, is the difference $z(s') - z(s)$:\n$$\\Delta z = \\left(\\left(\\sum_{k \\neq i} W_{k, c(s_k)} + W_{i, c(s'_i)}\\right) + b\\right) - \\left(\\left(\\sum_{k \\neq i} W_{k, c(s_k)} + W_{i, c(s_i)}\\right) + b\\right)$$\n$$\\Delta z = W_{i, c(s'_i)} - W_{i, c(s_i)}$$\nThis is a critical result: the change in the logit from a substitution at position $i$ depends only on the weights at that position for the original and new bases. The effects of substitutions at different positions are additive.\n\n**Greedy Algorithm for Minimal Substitutions**\nThe additive nature of logit changes allows for a greedy strategy to find the minimal number of substitutions, $k$.\n1.  **Calculate Initial State**: For a given sequence $s$, compute its logit $z_{orig}$ and determine the initial prediction $\\hat{y}(s)$.\n2.  **Determine Goal**:\n    -   If $\\hat{y}(s)=1$ (i.e., $z_{orig}  0$), we need to flip the prediction to $0$. This requires achieving a new logit $z_{new} \\le 0$. We must decrease the logit value by at least $z_{orig}$.\n    -   If $\\hat{y}(s)=0$ (i.e., $z_{orig} \\le 0$), we need to flip the prediction to $1$. This requires achieving a new logit $z_{new}  0$. We must increase the logit value by an amount greater than $-z_{orig}$.\n3.  **Identify Optimal Changes**: For each position $i \\in \\{0, \\dots, L-1\\}$, we calculate the best possible logit change that helps achieve our goal.\n    -   To decrease the logit, we find the most negative $\\Delta z$ possible at position $i$: $\\Delta z_i^{\\text{min}} = \\min_{s'_i \\neq s_i} (W_{i, c(s'_i)} - W_{i, c(s_i)})$.\n    -   To increase the logit, we find the most positive $\\Delta z$ possible at position $i$: $\\Delta z_i^{\\text{max}} = \\max_{s'_i \\neq s_i} (W_{i, c(s'_i)} - W_{i, c(s_i)})$.\n    We collect all such beneficial changes from all positions. Changes that are zero or move the logit in the wrong direction are ignored.\n4.  **Greedy Selection**: We create a list of the most effective single-position changes identified in the previous step. We sort this list to prioritize the substitutions with the largest impact:\n    -   If decreasing the logit, sort the $\\Delta z$ values in ascending order (most negative first).\n    -   If increasing the logit, sort the $\\Delta z$ values in descending order (most positive first).\n5.  **Accumulate and Find k**: We iterate through the sorted list of changes, accumulating their effect on the logit. We start with $z_{current} = z_{orig}$ and $k=0$. In each step, we add the next best $\\Delta z$ to $z_{current}$ and increment $k$. The minimal number of substitutions is the first value of $k$ for which the flip condition ($z_{current} \\le 0$ or $z_{current}  0$) is satisfied.\n6.  **Handle Failure**: If we exhaust all beneficial substitutions and the logit has not flipped, it is impossible to flip the prediction. In this case, we set $k=-1$.\n7.  **Verification**: After finding the minimal $k$, we construct the corresponding mutated sequence $\\mathbf{X}'$ by applying the top $k$ substitutions. We then recalculate its logit from scratch to verify that the prediction $\\hat{y}(\\mathbf{X}')$ is indeed different from $\\hat{y}(\\mathbf{X})$. This verification step confirms the correctness of the constructed attack and provides the required boolean output.\n\nThis algorithm is guaranteed to find the minimal $k$ because each substitution provides the maximum possible \"progress\" towards the goal, and the total progress required is a fixed value. By always taking the largest step possible, we reach the goal in the minimum number of steps.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the minimal adversarial substitutions for a linear DNA classifier.\n    \"\"\"\n    # Define model parameters and problem constants\n    L = 12\n    b = -2.0\n    base_map = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    bases = ['A', 'C', 'G', 'T']\n    \n    # Initialize weight matrix W\n    W = np.zeros((L, 4))\n    \n    # Populate non-zero weights as specified\n    # Position 0: A is +1.0, others 0.0\n    W[0, base_map['A']] = 1.0\n    \n    # Position 4: G is +3.0, others -1.0\n    W[4, :] = -1.0\n    W[4, base_map['G']] = 3.0\n    \n    # Position 5: T is +3.0, others -1.0\n    W[5, :] = -1.0\n    W[5, base_map['T']] = 3.0\n    \n    # Test cases\n    test_cases = [\n        \"CCCCGTCCCCCC\",  # Case 1\n        \"CCCCCACCCCCC\",  # Case 2\n        \"CCCCGACCCCCC\",  # Case 3\n        \"ACCCGTCCCCCC\",  # Case 4\n    ]\n\n    results = []\n\n    def calculate_logit(sequence, W_mat, b_val, b_map):\n        \"\"\"Calculates the logit for a given DNA sequence.\"\"\"\n        logit = b_val\n        for i, base in enumerate(sequence):\n            logit += W_mat[i, b_map[base]]\n        return logit\n\n    for seq in test_cases:\n        # 1. Calculate initial logit and prediction\n        z_orig = calculate_logit(seq, W, b, base_map)\n        y_orig_is_1 = z_orig  0\n\n        # 2. Determine goal: increase or decrease logit\n        needs_increase = not y_orig_is_1 # wants to flip 0 - 1, needs z  0\n\n        # 3. Identify all optimal single-position changes\n        best_changes = []\n        for i in range(L):\n            orig_base = seq[i]\n            w_orig = W[i, base_map[orig_base]]\n            \n            position_best_delta = 0\n            position_best_new_base = ''\n\n            if needs_increase:\n                # Find the substitution that gives the maximum positive delta_z\n                max_delta_z = -np.inf\n                for new_base in bases:\n                    if new_base != orig_base:\n                        w_new = W[i, base_map[new_base]]\n                        delta_z = w_new - w_orig\n                        if delta_z  max_delta_z:\n                            max_delta_z = delta_z\n                            position_best_new_base = new_base\n                if max_delta_z  0:\n                    best_changes.append((max_delta_z, i, position_best_new_base))\n            else: # needs decrease\n                # Find the substitution that gives the maximum negative delta_z\n                min_delta_z = np.inf\n                for new_base in bases:\n                    if new_base != orig_base:\n                        w_new = W[i, base_map[new_base]]\n                        delta_z = w_new - w_orig\n                        if delta_z  min_delta_z:\n                            min_delta_z = delta_z\n                            position_best_new_base = new_base\n                if min_delta_z  0:\n                    best_changes.append((min_delta_z, i, position_best_new_base))\n        \n        # 4. Sort changes greedily\n        if needs_increase:\n            best_changes.sort(key=lambda x: x[0], reverse=True) # Descending\n        else:\n            best_changes.sort(key=lambda x: x[0]) # Ascending\n\n        # 5. Accumulate changes to find minimal k\n        min_k = -1\n        current_z = z_orig\n        \n        for i in range(len(best_changes)):\n            delta_z, _, _ = best_changes[i]\n            current_z += delta_z\n            \n            flipped = False\n            if needs_increase and current_z  0:\n                flipped = True\n            elif not needs_increase and current_z = 0:\n                flipped = True\n            \n            if flipped:\n                min_k = i + 1\n                break\n        \n        # 6. Verify the flip with the constructed sequence\n        flip_verified = False\n        if min_k != -1:\n            mutations_to_apply = best_changes[:min_k]\n            seq_list = list(seq)\n            for _, pos_idx, new_base in mutations_to_apply:\n                seq_list[pos_idx] = new_base\n            \n            mutated_seq = \"\".join(seq_list)\n            z_new = calculate_logit(mutated_seq, W, b, base_map)\n            y_new_is_1 = z_new  0\n            \n            if y_new_is_1 != y_orig_is_1:\n                flip_verified = True\n\n        results.append([min_k, flip_verified])\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"[{k},{str(v).lower()}]\" for k, v in results]) + \"]\" # The problem requires a boolean, not a string 'True'/'False'\n    \n    # Corrected format as per instruction example: [[1,True],[2,False]...]\n    final_output_str = str(results).replace(\" \", \"\")\n    print(final_output_str)\n\nsolve()\n```", "id": "4330944"}]}