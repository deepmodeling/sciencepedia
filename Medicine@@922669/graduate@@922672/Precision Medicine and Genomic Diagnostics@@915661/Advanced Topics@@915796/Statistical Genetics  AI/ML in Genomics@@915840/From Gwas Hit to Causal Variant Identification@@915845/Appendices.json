{"hands_on_practices": [{"introduction": "Before any fine-mapping can begin, we need an accurate estimate of the correlation structure between variants, known as the Linkage Disequilibrium (LD) matrix. This practice provides essential hands-on experience in computing the LD matrix from raw genotype data, highlighting the critical choice between sample-derived and external allele frequencies and the necessity of regularization techniques to ensure the numerical stability and accuracy of downstream analyses [@problem_id:4341798]. Mastering this foundational step is paramount for the success of any fine-mapping project.", "problem": "You are given genotype dosage matrices for a small set of individuals in a reference panel and corresponding external allele frequencies for a subset of single-nucleotide polymorphisms (SNPs). Your task is to implement a program that computes the pairwise Linkage Disequilibrium (LD) correlation matrix $R$ for each test case, evaluates the impact of using external versus sample allele frequencies on $R$, and applies a shrinkage-based regularization to $R$ to mitigate sampling variance. Your program must output summary metrics per test case as specified below.\n\nFoundational base and definitions:\n- A Single-Nucleotide Polymorphism (SNP) is coded in a genotype matrix $G \\in \\mathbb{R}^{n \\times m}$ with entries in $\\{0,1,2\\}$ representing the alternate allele dosages for $n$ individuals and $m$ SNPs.\n- Under Hardy–Weinberg Equilibrium (HWE), for a SNP with alternate allele frequency $p \\in (0,1)$, the genotype dosage has expected mean $2p$ and variance $2p(1-p)$.\n- Let $p_j$ denote the allele frequency for SNP $j$, either provided externally or estimated from the sample as $\\hat{p}_j = \\frac{1}{2n}\\sum_{i=1}^{n} G_{ij}$. Define the standardized genotype matrix $X \\in \\mathbb{R}^{n \\times m}$ by\n$$\nX_{ij} \\;=\\; \\frac{G_{ij} - 2p_j}{\\sqrt{2p_j(1-p_j)}} \\, ,\n$$\nwhere the denominator is stabilized by adding a small positive constant $\\varepsilon$ inside the square root if needed to avoid division by zero.\n- The sample LD correlation matrix is estimated as\n$$\nR \\;=\\; \\frac{1}{n-1} X^\\top X \\, ,\n$$\nwhich equals the sample correlation when the standardization is exact.\n- Finite-sample estimation error induces sampling variance in $R$. A commonly used regularization is shrinkage toward the identity matrix:\n$$\nR_{\\lambda} \\;=\\; (1-\\lambda) R \\;+\\; \\lambda I_m \\, ,\n$$\nwhere $I_m$ is the $m \\times m$ identity matrix and $\\lambda \\in [0,1]$ is the shrinkage intensity.\n- Assume a Gaussian–Wishart model for the standardized data. An approximate plug-in estimator of the optimal shrinkage intensity toward $I_m$ minimizes the expected squared Frobenius error and can be implemented as\n$$\n\\lambda^\\star \\;=\\; \\min\\!\\Bigg( 1, \\; \\max\\!\\Big( 0, \\; \\frac{\\phi}{\\delta + \\eta} \\Big) \\Bigg) \\, ,\n$$\nwith small $\\eta > 0$ for numerical stability, and\n$$\n\\phi \\;=\\; \\frac{1}{n-1} \\sum_{i \\neq j} \\big( R_{ij}^2 + 1 \\big) \\, , \\qquad\n\\delta \\;=\\; \\sum_{i \\neq j} R_{ij}^2 \\, .\n$$\n\nRequired computations per test case:\n1. Given $G$ and external allele frequencies $\\{p_j\\}_{j=1}^{m}$, compute $R_{\\text{ext}}$ using the external frequencies in the standardization.\n2. Compute $R_{\\text{samp}}$ by standardizing with sample allele frequencies $\\{\\hat{p}_j\\}_{j=1}^{m}$ instead of the external frequencies.\n3. Using $R_{\\text{ext}}$, compute the shrinkage intensity $\\lambda^\\star$ and the regularized matrix\n$$\nR_{\\text{reg}} \\;=\\; (1-\\lambda^\\star) R_{\\text{ext}} \\;+\\; \\lambda^\\star I_m \\, .\n$$\n\nYour program must output, for each test case, the following three quantities:\n- The maximum absolute entrywise deviation between $R_{\\text{ext}}$ and $R_{\\text{samp}}$:\n$$\nd \\;=\\; \\max_{i,j} \\big| \\big(R_{\\text{ext}}\\big)_{ij} - \\big(R_{\\text{samp}}\\big)_{ij} \\big| \\, .\n$$\n- The Frobenius norm of the difference between $R_{\\text{ext}}$ and $R_{\\text{reg}}$:\n$$\nf \\;=\\; \\big\\| R_{\\text{ext}} - R_{\\text{reg}} \\big\\|_{\\mathrm{F}} \\, .\n$$\n- A boolean indicating whether $R_{\\text{reg}}$ is strictly positive definite, defined as all eigenvalues strictly greater than a threshold $\\tau$, where $\\tau = 10^{-8}$:\n$$\n\\text{is\\_spd} \\;=\\; \\big( \\lambda_{\\min}(R_{\\text{reg}}) > \\tau \\big) \\, .\n$$\n\nNumerical conventions:\n- Use $\\varepsilon = 10^{-8}$ in the variance denominator stabilization of $X_{ij}$ if needed.\n- Use $\\eta = 10^{-12}$ in the denominator of $\\lambda^\\star$ to avoid division by zero.\n- Use $\\tau = 10^{-8}$ to test strict positive definiteness.\n\nTest suite:\nProvide the following four test cases. Each contains $n$, $m$, the genotype matrix $G$ (row-major order: $n$ rows and $m$ columns), and the vector of external allele frequencies $(p_1,\\ldots,p_m)$.\n\n- Test case A (general case; moderate correlations): $n=8$, $m=4$,\n  $$\n  G \\;=\\; \\begin{bmatrix}\n  0  0  2  1 \\\\\n  1  1  1  1 \\\\\n  2  2  0  2 \\\\\n  0  1  1  0 \\\\\n  1  1  2  2 \\\\\n  2  2  1  2 \\\\\n  0  0  0  0 \\\\\n  1  1  1  1\n  \\end{bmatrix} \\, , \\quad\n  (p_1,p_2,p_3,p_4) \\;=\\; (0.4,\\, 0.45,\\, 0.52,\\, 0.55) \\, .\n  $$\n- Test case B (rare allele present): $n=10$, $m=3$,\n  $$\n  G \\;=\\; \\begin{bmatrix}\n  0  0  2 \\\\\n  0  1  1 \\\\\n  0  1  1 \\\\\n  0  0  2 \\\\\n  0  1  0 \\\\\n  0  0  1 \\\\\n  0  1  2 \\\\\n  0  0  0 \\\\\n  1  0  1 \\\\\n  0  1  2\n  \\end{bmatrix} \\, , \\quad\n  (p_1,p_2,p_3) \\;=\\; (0.04,\\, 0.3,\\, 0.55) \\, .\n  $$\n- Test case C (rank-deficient $R$ with $m > n$): $n=5$, $m=6$,\n  $$\n  G \\;=\\; \\begin{bmatrix}\n  0  1  2  0  1  2 \\\\\n  1  2  1  1  2  1 \\\\\n  2  1  0  2  1  0 \\\\\n  0  0  1  0  0  1 \\\\\n  1  1  2  1  1  2\n  \\end{bmatrix} \\, , \\quad\n  (p_1,\\ldots,p_6) \\;=\\; (0.2,\\, 0.5,\\, 0.6,\\, 0.3,\\, 0.5,\\, 0.6) \\, .\n  $$\n- Test case D (perfect correlation between two SNPs): $n=6$, $m=3$,\n  $$\n  G \\;=\\; \\begin{bmatrix}\n  0  0  0 \\\\\n  0  0  1 \\\\\n  1  1  0 \\\\\n  1  1  1 \\\\\n  2  2  0 \\\\\n  2  2  1\n  \\end{bmatrix} \\, , \\quad\n  (p_1,p_2,p_3) \\;=\\; (0.5,\\, 0.5,\\, 0.5) \\, .\n  $$\n\nFinal output format:\n- For each test case, compute $(d, f, \\text{is\\_spd})$ as defined above. Your program must produce a single line of output containing a list of four lists, one per test case, in order A, B, C, D. Each inner list must be of the form $[d,f,\\text{is\\_spd}]$, where $d$ and $f$ are rounded to exactly $6$ decimal places, and $\\text{is\\_spd}$ is a boolean. For example,\n$$\n\\big[ [d_A,f_A,\\text{is\\_spd}_A],\\; [d_B,f_B,\\text{is\\_spd}_B],\\; [d_C,f_C,\\text{is\\_spd}_C],\\; [d_D,f_D,\\text{is\\_spd}_D] \\big] \\, .\n$$\n\nScientific realism requirement:\n- Your implementation should adhere to the definitions above and must be numerically stable given the constants $\\varepsilon$, $\\eta$, and $\\tau$. No stochastic components may be used.", "solution": "The problem is deemed valid as it is scientifically grounded in statistical genetics, well-posed with all necessary information provided, and objective in its formulation. It presents a clear, algorithmically defined task without internal contradictions or infeasible requirements. We will proceed with a complete solution.\n\nThe task is to analyze genotype data for several test cases by computing and comparing different estimates of the Linkage Disequilibrium (LD) correlation matrix, $R$. The process involves standardizing the genotype data, calculating correlation matrices, and applying a shrinkage regularization technique. For each case, we must report three summary metrics: the maximum deviation between two LD estimates ($d$), the magnitude of the regularization adjustment ($f$), and a test for strict positive definiteness of the regularized matrix ($\\text{is\\_spd}$).\n\nLet us formalize the step-by-step procedure.\n\nA given test case provides a genotype matrix $G \\in \\mathbb{R}^{n \\times m}$ for $n$ individuals and $m$ SNPs, and a vector of external allele frequencies $p_{\\text{ext}} = (p_1, \\ldots, p_m)$. The constants $\\varepsilon = 10^{-8}$, $\\eta = 10^{-12}$, and $\\tau = 10^{-8}$ are specified for numerical stability and testing.\n\n**Step 1: Standardization of the Genotype Matrix**\n\nThe first step is to standardize the genotype matrix $G$. The standardization formula is given as:\n$$\nX_{ij} = \\frac{G_{ij} - 2p_j}{\\sqrt{2p_j(1-p_j)}}\n$$\nwhere $p_j$ is the alternate allele frequency for SNP $j$. The denominator, representing the standard deviation under Hardy-Weinberg Equilibrium (HWE), is stabilized to prevent division by zero or by very small numbers. We implement this by calculating the denominator as $\\sqrt{2p_j(1-p_j) + \\varepsilon}$.\n\nThis standardization must be performed twice for each test case:\n1.  Using the provided external allele frequencies $\\{p_j\\}_{j=1}^{m}$ to produce a matrix $X_{\\text{ext}}$.\n2.  Using sample-estimated allele frequencies $\\{\\hat{p}_j\\}_{j=1}^{m}$ to produce a matrix $X_{\\text{samp}}$. The sample allele frequency for SNP $j$ is estimated from the data as:\n    $$\n    \\hat{p}_j = \\frac{1}{2n} \\sum_{i=1}^{n} G_{ij}\n    $$\n\n**Step 2: Computation of LD Correlation Matrices**\n\nWith the standardized matrices, we compute the two LD correlation matrices, $R_{\\text{ext}}$ and $R_{\\text{samp}}$, using the formula for the sample covariance of the standardized data:\n$$\nR = \\frac{1}{n-1} X^\\top X\n$$\nThis yields:\n$$\nR_{\\text{ext}} = \\frac{1}{n-1} X_{\\text{ext}}^\\top X_{\\text{ext}}\n$$\n$$\nR_{\\text{samp}} = \\frac{1}{n-1} X_{\\text{samp}}^\\top X_{\\text{samp}}\n$$\nBoth $R_{\\text{ext}}$ and $R_{\\text{samp}}$ are $m \\times m$ symmetric matrices.\n\n**Step 3: Calculation of Deviation Metric $d$**\n\nThe first required output metric, $d$, quantifies the difference between the LD matrices derived from external versus sample frequencies. It is the maximum absolute entrywise difference:\n$$\nd = \\max_{i,j} \\left| \\left(R_{\\text{ext}}\\right)_{ij} - \\left(R_{\\text{samp}}\\right)_{ij} \\right|\n$$\n\n**Step 4: Shrinkage Regularization**\n\nThe matrix $R_{\\text{ext}}$ is regularized to reduce estimation error due to finite sample size ($n$). The regularization is performed via shrinkage towards the identity matrix $I_m$:\n$$\nR_{\\text{reg}} = (1-\\lambda^\\star) R_{\\text{ext}} + \\lambda^\\star I_m\n$$\nThe optimal shrinkage intensity, $\\lambda^\\star$, is estimated using the provided plug-in formulas. First, we compute the intermediate quantities $\\phi$ and $\\delta$ based on $R_{\\text{ext}}$:\n$$\n\\delta = \\sum_{i \\neq j} \\left(R_{\\text{ext}}\\right)_{ij}^2\n$$\n$$\n\\phi = \\frac{1}{n-1} \\sum_{i \\neq j} \\left( \\left(R_{\\text{ext}}\\right)_{ij}^2 + 1 \\right)\n$$\nThe term $\\sum_{i \\neq j} (\\cdot)$ denotes the sum over all off-diagonal elements of the matrix. This can be computed by taking the sum over all elements and subtracting the sum over the diagonal elements. Let $S = \\sum_{i,j} \\left(R_{\\text{ext}}\\right)_{ij}^2$ and $S_{\\text{diag}} = \\sum_{i} \\left(R_{\\text{ext}}\\right)_{ii}^2$. Then $\\delta = S - S_{\\text{diag}}$. The sum $\\sum_{i \\neq j} 1$ is the number of off-diagonal elements, which is $m(m-1)$. Thus, $\\phi$ can be computed as:\n$$\n\\phi = \\frac{1}{n-1} \\left( \\delta + m(m-1) \\right)\n$$\nThe shrinkage intensity $\\lambda^\\star$ is then calculated, with clipping to the interval $[0,1]$ and numerical stabilization using $\\eta$:\n$$\n\\lambda^\\star = \\min\\left( 1, \\max\\left( 0, \\frac{\\phi}{\\delta + \\eta} \\right) \\right)\n$$\nWith $\\lambda^\\star$, we can now compute the regularized matrix $R_{\\text{reg}}$.\n\n**Step 5: Calculation of Regularization Effect Metric $f$**\n\nThe second output metric, $f$, measures the magnitude of the change applied by the regularization. It is the Frobenius norm of the difference between the original and regularized matrices:\n$$\nf = \\left\\| R_{\\text{ext}} - R_{\\text{reg}} \\right\\|_{\\mathrm{F}}\n$$\nSubstituting the definition of $R_{\\text{reg}}$, we can see that $R_{\\text{ext}} - R_{\\text{reg}} = \\lambda^\\star (R_{\\text{ext}} - I_m)$. Therefore:\n$$\nf = \\lambda^\\star \\left\\| R_{\\text{ext}} - I_m \\right\\|_{\\mathrm{F}}\n$$\nwhere the norm is calculated as $\\|A\\|_{\\mathrm{F}} = \\sqrt{\\sum_{i,j} A_{ij}^2}$.\n\n**Step 6: Test for Strict Positive Definiteness (SPD)**\n\nThe final metric, $\\text{is\\_spd}$, is a boolean value indicating whether $R_{\\text{reg}}$ is strictly positive definite. A matrix is strictly positive definite if all its eigenvalues are strictly positive. We test this condition against a small positive threshold $\\tau = 10^{-8}$:\n$$\n\\text{is\\_spd} = \\left( \\lambda_{\\min}(R_{\\text{reg}}) > \\tau \\right)\n$$\nwhere $\\lambda_{\\min}(R_{\\text{reg}})$ is the minimum eigenvalue of $R_{\\text{reg}}$. Since $R_{\\text{reg}}$ is symmetric by construction, its eigenvalues are real, and efficient numerical algorithms can be used to find them.\n\nWe will apply this entire sequence of computations to each of the four test cases provided and format the results $(d, f, \\text{is\\_spd})$ as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the final output.\n    \"\"\"\n    \n    # Numerical conventions\n    epsilon = 1e-8\n    eta = 1e-12\n    tau = 1e-8\n\n    # Test suite definition\n    test_cases = [\n        (8, 4, np.array([\n            [0, 0, 2, 1], [1, 1, 1, 1], [2, 2, 0, 2], [0, 1, 1, 0],\n            [1, 1, 2, 2], [2, 2, 1, 2], [0, 0, 0, 0], [1, 1, 1, 1]\n        ]), np.array([0.4, 0.45, 0.52, 0.55])),\n        (10, 3, np.array([\n            [0, 0, 2], [0, 1, 1], [0, 1, 1], [0, 0, 2], [0, 1, 0],\n            [0, 0, 1], [0, 1, 2], [0, 0, 0], [1, 0, 1], [0, 1, 2]\n        ]), np.array([0.04, 0.3, 0.55])),\n        (5, 6, np.array([\n            [0, 1, 2, 0, 1, 2], [1, 2, 1, 1, 2, 1], [2, 1, 0, 2, 1, 0],\n            [0, 0, 1, 0, 0, 1], [1, 1, 2, 1, 1, 2]\n        ]), np.array([0.2, 0.5, 0.6, 0.3, 0.5, 0.6])),\n        (6, 3, np.array([\n            [0, 0, 0], [0, 0, 1], [1, 1, 0], [1, 1, 1],\n            [2, 2, 0], [2, 2, 1]\n        ]), np.array([0.5, 0.5, 0.5]))\n    ]\n    \n    all_results = []\n\n    for n, m, G, p_ext in test_cases:\n        G_float = G.astype(np.float64)\n\n        # Helper function for standardization\n        def standardize(genotypes, freqs):\n            mean = 2 * freqs\n            # Stabilized variance term: add epsilon inside square root\n            variance = 2 * freqs * (1 - freqs)\n            std_dev = np.sqrt(variance + epsilon)\n            return (genotypes - mean) / std_dev\n        \n        # Helper function for LD matrix calculation\n        def calculate_R(X_mat, n_samples):\n            return (1 / (n_samples - 1)) * (X_mat.T @ X_mat)\n\n        # 1. Compute R_ext\n        X_ext = standardize(G_float, p_ext)\n        R_ext = calculate_R(X_ext, n)\n\n        # 2. Compute R_samp\n        p_samp = np.sum(G_float, axis=0) / (2 * n)\n        X_samp = standardize(G_float, p_samp)\n        R_samp = calculate_R(X_samp, n)\n        \n        # 3. Compute metric d\n        d = np.max(np.abs(R_ext - R_samp))\n\n        # 4. Compute shrinkage intensity lambda_star using R_ext\n        # Sum of squared off-diagonal elements\n        off_diag_sq_sum = np.sum(R_ext**2) - np.sum(np.diag(R_ext)**2)\n        \n        delta = off_diag_sq_sum\n        phi = (1 / (n - 1)) * (delta + m * (m - 1))\n        \n        lambda_val = phi / (delta + eta)\n        lambda_star = np.clip(lambda_val, 0, 1)\n        \n        # 5. Compute R_reg\n        I_m = np.identity(m)\n        R_reg = (1 - lambda_star) * R_ext + lambda_star * I_m\n\n        # 6. Compute metric f\n        f = np.linalg.norm(R_ext - R_reg, 'fro')\n\n        # 7. Compute metric is_spd\n        # Use eigvalsh for symmetric matrices; it's faster and returns sorted eigenvalues\n        eigenvalues = np.linalg.eigvalsh(R_reg)\n        is_spd = eigenvalues[0] > tau\n        \n        all_results.append([d, f, is_spd])\n\n    # Format the final output string as a list of lists representation\n    output_parts = [\n        f\"[{d:.6f},{f:.6f},{s}]\" for d, f, s in all_results\n    ]\n    final_output_str = f\"[{','.join(output_parts)}]\"\n    \n    # Re-insert spaces to match the visual Python list representation\n    # This is a bit of a hack but ensures the string format is precise\n    # str list representation has `True`, not `true`. Python's f-string does this correctly.\n    # It adds a space after commas.\n    final_output_str = final_output_str.replace(\",\", \", \")\n    final_output_str = final_output_str.replace(\"[,\", \"[\")\n\n    final_output_str = final_output_str.replace(\"[ \", \"[\")\n    for i in range(len(all_results)-1):\n        # target_str = f\"{all_results[i][2]}],[{all_results[i+1][0]:.6f}\"\n        # replace_str = f\"{all_results[i][2]}], [{all_results[i+1][0]:.6f}\"\n        # final_output_str = final_output_str.replace(target_str, replace_str)\n        pass # The default string representation seems to be the target.\n    \n    # A more robust way to get the desired string representation is to build it manually\n    list_of_lists_str = []\n    for d, f, s in all_results:\n        list_of_lists_str.append(f\"[{d:.6f}, {f:.6f}, {s}]\")\n    \n    final_str = f\"[{', '.join(list_of_lists_str)}]\"\n    \n    # The template `print(f\"[{','.join(map(str, results))}]\")` with formatted\n    # floats and bools is simpler and gives a compact, valid JSON-like format.\n    # We will favor that for robustness. The example format might be illustrative.\n    \n    formatted_results = []\n    for d_val, f_val, s_val in all_results:\n        # Create a list with formatted numbers to be converted to a string\n        formatted_sublist = [f\"{d_val:.6f}\", f\"{f_val:.6f}\", str(s_val)]\n        # Create the string for the sublist\n        formatted_results.append(f\"[{','.join(formatted_sublist)}]\")\n\n    print(f\"[[{all_results[0][0]:.6f},{all_results[0][1]:.6f},{str(all_results[0][2]).lower()}], [{all_results[1][0]:.6f},{all_results[1][1]:.6f},{str(all_results[1][2]).lower()}], [{all_results[2][0]:.6f},{all_results[2][1]:.6f},{str(all_results[2][2]).lower()}], [{all_results[3][0]:.6f},{all_results[3][1]:.6f},{str(all_results[3][2]).lower()}]]\".replace(\"'\", \"\"))\n    \n    # The example output uses lowercase booleans, like JSON. Let's match that.\n    # The problem asks for a Python list representation which would be True/False, but the example has lowercase. Let's go with the example format's booleans.\n    list_of_lists_str = []\n    for d, f, s in all_results:\n        s_str = str(s).lower()\n        list_of_lists_str.append(f\"[{d:.6f}, {f:.6f}, {s_str}]\")\n    final_str = f\"[{', '.join(list_of_lists_str)}]\"\n    print(final_str)\n\n\nsolve()\n```", "id": "4341798"}, {"introduction": "A common challenge in post-GWAS analysis is a locus with several highly associated variants, making it difficult to identify the true causal signal. Conditional analysis provides a powerful statistical tool to dissect these signals by testing the association of a secondary variant while accounting for the effect of the lead variant [@problem_id:4341984]. This exercise guides you through the calculation of a conditional test statistic, a core skill for disentangling correlated signals and refining the set of candidate causal variants.", "problem": "In a fine-mapping analysis within precision medicine and genomic diagnostics, you are given marginal association statistics from a Genome-Wide Association Study (GWAS) for a locus with four Single-Nucleotide Polymorphisms (SNPs). Assume genotypes are standardized and that, under large-sample theory for linear association testing, the vector of marginal $z$-scores across SNPs is approximately multivariate normal with correlation equal to the Linkage Disequilibrium (LD) matrix.\n\nThe marginal $z$-scores are\n$$\n\\boldsymbol{z} = \\begin{pmatrix} 6.10 \\\\ 4.30 \\\\ 2.10 \\\\ -1.20 \\end{pmatrix},\n$$\nand the LD correlation matrix is\n$$\nR = \\begin{pmatrix}\n1  0.20  0.05  -0.02 \\\\\n0.20  1  0.10  -0.01 \\\\\n0.05  0.10  1  0 \\\\\n-0.02  -0.01  0  1\n\\end{pmatrix}.\n$$\n\nIdentify the lead SNP as the one with the largest absolute marginal $z$-score and consider SNP $2$ as a candidate secondary signal. Using only first principles about conditioning in a multivariate normal distribution and the assumption that the covariance of $\\boldsymbol{z}$ equals $R$, derive the appropriate conditional test statistic for SNP $2$ after conditioning on the lead SNP. Then determine whether the candidate secondary signal remains significant at a two-sided Bonferroni-corrected family-wise error rate of $\\alpha = 0.05$ across $M = 20$ tests.\n\nReport only the conditional $z$-score for SNP $2$ as your final answer. Round your final answer to four significant figures. No units are required.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. The underlying model, which treats GWAS $z$-scores as draws from a multivariate normal distribution with a covariance structure equal to the Linkage Disequilibrium (LD) matrix, is a standard and widely accepted framework in statistical genetics for fine-mapping and conditional analysis. The provided data are consistent and realistic. Therefore, the problem is valid, and I will proceed with a full solution.\n\nThe goal is to compute the conditional test statistic for SNP $2$ after accounting for the effect of the lead SNP. This procedure, known as conditional analysis, helps to distinguish between a single association signal that affects multiple correlated SNPs through LD, and multiple distinct association signals at a locus.\n\nFirst, we identify the lead SNP, defined as the one with the largest absolute marginal $z$-score. The given vector of $z$-scores is:\n$$\n\\boldsymbol{z} = \\begin{pmatrix} z_1 \\\\ z_2 \\\\ z_3 \\\\ z_4 \\end{pmatrix} = \\begin{pmatrix} 6.10 \\\\ 4.30 \\\\ 2.10 \\\\ -1.20 \\end{pmatrix}\n$$\nThe absolute values are $|z_1| = 6.10$, $|z_2| = 4.30$, $|z_3| = 2.10$, and $|z_4| = |-1.20| = 1.20$. The maximum of these is $6.10$, which corresponds to SNP $1$. Thus, SNP $1$ is the lead SNP.\n\nNext, we derive the conditional test statistic for the candidate secondary signal, SNP $2$, given the observed statistic for the lead SNP, SNP $1$. Let the random variables for the $z$-scores of SNP $1$ and SNP $2$ be $Z_1$ and $Z_2$, respectively. Based on the problem statement, their joint distribution is bivariate normal, with a mean vector that reflects their true genetic effects and a covariance matrix extracted from the provided LD matrix $R$:\n$$\n\\begin{pmatrix} Z_1 \\\\ Z_2 \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\lambda_1 \\\\ \\lambda_2 \\end{pmatrix}, \\begin{pmatrix} R_{11}  R_{12} \\\\ R_{21}  R_{22} \\end{pmatrix} \\right)\n$$\nwhere $\\lambda_1$ and $\\lambda_2$ are the non-centrality parameters representing the true effects, and the covariance submatrix is:\n$$\n\\begin{pmatrix} R_{11}  R_{12} \\\\ R_{21}  R_{22} \\end{pmatrix} = \\begin{pmatrix} 1  0.20 \\\\ 0.20  1 \\end{pmatrix}\n$$\nThe conditional test statistic for SNP $2$, which we denote $z_{2|1}$, is a standardized measure of the association at SNP $2$ that is not explained by its LD with SNP $1$. This is derived from the properties of the conditional normal distribution. The conditional statistic is the observed value of $Z_2$ minus its expected value given $Z_1$, all divided by the conditional standard deviation.\nThe expected value of $Z_2$ given $Z_1 = z_1$ is $E[Z_2 | Z_1=z_1] = \\lambda_2 + R_{21} R_{11}^{-1} (z_1 - \\lambda_1)$. The conditional variance is $\\text{Var}(Z_2 | Z_1=z_1) = R_{22} - R_{21} R_{11}^{-1} R_{12}$.\n\nTo form a test statistic for the null hypothesis that SNP $2$ has no independent effect (i.e., $H_0: \\lambda_2 = 0$), we proceed by approximating the unknown true effect of the lead SNP, $\\lambda_1$, with its observed statistic, $z_1$. This is a standard approximation in methods such as GCTA-COJO. The expected value of $Z_2$ under this approximation, due to LD with SNP $1$, is $R_{21}R_{11}^{-1}z_1$. The conditional statistic is constructed by standardizing the difference between the observed $z_2$ and this expectation:\n$$\nz_{2|1} = \\frac{z_2 - E[Z_2 | Z_1=z_1, \\lambda_1=z_1, \\lambda_2=0]}{\\sqrt{\\text{Var}(Z_2 | Z_1=z_1)}} = \\frac{z_2 - R_{21} R_{11}^{-1} z_1}{\\sqrt{R_{22} - R_{21} R_{11}^{-1} R_{12}}}\n$$\nSince $R$ is a correlation matrix, the diagonal elements $R_{11}$ and $R_{22}$ are equal to $1$. Substituting these values simplifies the formula:\n$$\nz_{2|1} = \\frac{z_2 - R_{21} z_1}{\\sqrt{1 - R_{21}^2}}\n$$\nThis formula effectively computes a standardized residual of the regression of $z_2$ on $z_1$, which represents the portion of the $z_2$ signal that is statistically independent of the $z_1$ signal.\n\nWe now substitute the given numerical values: $z_1 = 6.10$, $z_2 = 4.30$, and $R_{21} = 0.20$.\n$$\nz_{2|1} = \\frac{4.30 - (0.20)(6.10)}{\\sqrt{1 - (0.20)^2}}\n$$\nFirst, calculate the numerator:\n$$\n4.30 - (0.20)(6.10) = 4.30 - 1.22 = 3.08\n$$\nNext, calculate the denominator:\n$$\n\\sqrt{1 - (0.20)^2} = \\sqrt{1 - 0.04} = \\sqrt{0.96}\n$$\nThe conditional $z$-score is therefore:\n$$\nz_{2|1} = \\frac{3.08}{\\sqrt{0.96}} \\approx \\frac{3.08}{0.9797959} \\approx 3.14351\n$$\nRounding to four significant figures, we get $z_{2|1} = 3.144$.\n\nFinally, the problem asks to determine if this secondary signal is significant at a Bonferroni-corrected threshold. The family-wise error rate is $\\alpha = 0.05$ for $M = 20$ tests. The corrected significance level for a single test's $p$-value is $p_{\\text{thresh}} = \\frac{\\alpha}{M} = \\frac{0.05}{20} = 0.0025$.\nThe two-sided $p$-value for our conditional statistic $z_{2|1} \\approx 3.144$ is $p = 2 \\times P(Z > |3.144|)$, where $Z \\sim \\mathcal{N}(0,1)$. This $p$-value is approximately $0.00167$. Since $0.00167  0.0025$, the conditional signal for SNP $2$ is indeed statistically significant after accounting for the lead SNP $1$ and correcting for multiple testing. The final answer required is the value of the conditional $z$-score itself.", "answer": "$$\n\\boxed{3.144}\n$$", "id": "4341984"}, {"introduction": "Ultimately, identifying a causal variant requires a holistic approach that synthesizes multiple, independent lines of evidence. This capstone practice simulates a real-world research scenario, challenging you to integrate trans-ancestry genetic data with a rich array of functional genomics information to build a coherent and compelling causal story [@problem_id:4341927]. By designing an end-to-end strategy—from statistical fine-mapping and colocalization to mechanistic hypothesizing and experimental design—you will practice the critical thinking required to translate a GWAS hit into a testable biological mechanism.", "problem": "You are given a single genomic locus harboring four biallelic variants, denoted $v_1$, $v_2$, $v_3$, and $v_4$. Genome-Wide Association Study (GWAS) summary statistics are available in two populations: European ancestry (EUR) and African ancestry (AFR). For each ancestry, you have $z$-scores for association with a complex trait, ancestry-specific linkage disequilibrium (LD) matrices, functional annotations from disease-relevant cell types, expression Quantitative Trait Locus (eQTL) summary statistics for the nearest expressed gene $G$ in the relevant tissue, protein Quantitative Trait Locus (pQTL) summary statistics for the protein product of gene $G$, and Massively Parallel Reporter Assay (MPRA) measurements of allele-specific regulatory activity for each variant.\n\nDefinitions and fundamental bases to use:\n- Bayes’ theorem: posterior odds are proportional to prior odds multiplied by a likelihood ratio.\n- In single-causal-variant fine-mapping, posterior inclusion probability (PIP) for variant $i$ can be derived from the relative posterior odds across variants, where the likelihood can be approximated from association statistics and ancestry-specific LD under a shared-causality model across ancestries.\n- A $95\\%$ credible set is the smallest set of variants whose cumulative PIP is at least $0.95$.\n- Colocalization assesses whether two traits (e.g., GWAS and eQTL) share a causal variant; the evidence increases when a single variant explains both signals and LD-tagging patterns are consistent across ancestries.\n- Trans-ancestry fine-mapping leverages differences in LD to refine causal resolution.\n- Functional priors must be calibrated from genome-wide enrichment, not used as deterministic filters.\n\nData for the locus:\n- EUR GWAS $z$-scores: $z^{\\mathrm{EUR}}_{v_1} = 6.0$, $z^{\\mathrm{EUR}}_{v_2} = 5.8$, $z^{\\mathrm{EUR}}_{v_3} = 2.0$, $z^{\\mathrm{EUR}}_{v_4} = 0.5$.\n- AFR GWAS $z$-scores: $z^{\\mathrm{AFR}}_{v_1} = 5.0$, $z^{\\mathrm{AFR}}_{v_2} = 1.0$, $z^{\\mathrm{AFR}}_{v_3} = 2.5$, $z^{\\mathrm{AFR}}_{v_4} = 0.2$.\n- Ancestry-specific LD (Pearson correlation $r$) among variants:\n  - EUR: $r_{12}^{\\mathrm{EUR}} = 0.95$, $r_{13}^{\\mathrm{EUR}} = 0.10$, $r_{14}^{\\mathrm{EUR}} = 0.05$, $r_{23}^{\\mathrm{EUR}} = 0.08$, $r_{24}^{\\mathrm{EUR}} = 0.02$, $r_{34}^{\\mathrm{EUR}} = 0.03$.\n  - AFR: $r_{12}^{\\mathrm{AFR}} = 0.40$, $r_{13}^{\\mathrm{AFR}} = 0.05$, $r_{14}^{\\mathrm{AFR}} = 0.02$, $r_{23}^{\\mathrm{AFR}} = 0.03$, $r_{24}^{\\mathrm{AFR}} = 0.01$, $r_{34}^{\\mathrm{AFR}} = 0.02$.\n- Functional annotations in the disease-relevant cell type:\n  - $v_1$: enhancer element with Assay for Transposase-Accessible Chromatin using sequencing (ATAC-seq) peak; alternate allele disrupts a Transcription Factor (TF) $X$ motif.\n  - $v_2$: intergenic, no accessible chromatin, no conservation.\n  - $v_3$: promoter of gene $G$; accessible promoter chromatin; no motif disruption.\n  - $v_4$: intronic, low conservation.\n- eQTL summary statistics for gene $G$ in the relevant tissue:\n  - $v_1$: $z_{\\mathrm{eQTL}} = 4.5$; alternate allele reduces $G$ expression.\n  - $v_2$: $z_{\\mathrm{eQTL}} = 0.5$.\n  - $v_3$: $z_{\\mathrm{eQTL}} = 1.0$.\n  - $v_4$: $z_{\\mathrm{eQTL}} = 0.0$.\n- pQTL for the protein product of $G$:\n  - $v_1$: $z_{\\mathrm{pQTL}} = 3.0$; alternate allele reduces protein abundance.\n- MPRA:\n  - $v_1$: alternate allele lowers reporter activity by $20\\%$ with replicate-consistent signal.\n  - $v_2$: no detectable allelic effect.\n  - $v_3$: small $5\\%$ increase, not statistically significant.\n  - $v_4$: no effect.\n\nAssume case-control GWAS with standardized phenotypes and genotypes, and that the trait has a single dominant signal at this locus. The disease-relevant cell type is well-characterized, and gene $G$ is expressed in this tissue.\n\nQuestion: Which end-to-end analysis plan and corresponding outputs are most scientifically justified and internally consistent with the data and first principles, yielding valid causal variant PIPs, a $95\\%$ credible set, variant-to-gene assignment, mechanism hypotheses, and a prioritized experimental validation plan?\n\nA. Harmonize alleles and summary statistics across ancestries; perform per-ancestry quality control; fine-map in each ancestry using ancestry-specific LD under a single shared-causal-variant model, allowing for multiple signals but confirming only one via conditional analysis; integrate ancestries via a joint likelihood; incorporate functional annotations using a calibrated hierarchical prior that modestly upweights enhancer activity and ATAC-seq peaks; compute PIPs yielding $v_1: 0.96$, $v_2: 0.02$, $v_3: 0.02$, $v_4: 0.00$; define the $95\\%$ credible set as $\\{v_1\\}$; perform colocalization between GWAS and eQTL/pQTL using full summary statistics and LD to obtain a high posterior probability of a shared causal $v_1$; assign $v_1 \\rightarrow G$ via colocalization and enhancer-gene linking using Activity-By-Contact (ABC) scoring and promoter capture Hi-C; propose mechanism that $v_1$ disrupts TF $X$ binding, lowering enhancer activity and reducing $G$ expression and protein abundance; prioritize experiments: CRISPR base editing of $v_1$ in induced pluripotent stem cell (iPSC)-derived disease-relevant cells, allele-specific expression and protein assays, CRISPR interference (CRISPRi) of the enhancer, and Chromatin Immunoprecipitation (ChIP) for TF $X$ with allele-specific binding.\n\nB. Use EUR-only GWAS and LD; treat MPRA as a deterministic filter excluding $v_2$, $v_3$, and $v_4$; compute PIPs with equal priors, yielding $v_1: 0.55$, $v_2: 0.40$, $v_3: 0.05$, $v_4: 0.00$; define the $95\\%$ credible set as $\\{v_1, v_2\\}$; assign the variant to the nearest gene by distance, ignoring eQTL; infer mechanism that the risk allele increases enhancer activity; perform luciferase reporter assays in a generic cell line lacking expression of $G$ (e.g., HEK$293$), without allele-specific genome editing.\n\nC. Pool EUR and AFR $z$-scores via inverse-variance weighting, ignoring LD; compute PIPs by applying a softmax to squared $z$-scores, yielding $v_1: 0.70$, $v_2: 0.20$, $v_3: 0.10$, $v_4: 0.00$; define the $95\\%$ credible set as $\\{v_1, v_2\\}$; perform colocalization using only eQTL $p$-values without LD or fine-mapping; assign the variant to $G$ by proximity; hypothesize mechanism without reconciling MPRA and eQTL directions; propose general CRISPR knockout of $G$ in any convenient cell line.\n\nD. Conduct multi-ancestry fine-mapping but apply an uncalibrated functional prior that heavily upweights promoter annotation, forcing $v_3$ to dominate; compute PIPs yielding $v_3: 0.80$, $v_1: 0.15$, $v_2: 0.04$, $v_4: 0.01$; define the $95\\%$ credible set as $\\{v_3\\}$; claim colocalization with weak eQTL evidence; justify mechanism primarily from the small, non-significant MPRA effect at $v_3$; prioritize promoter editing over enhancer perturbation.\n\nSelect the single best option.", "solution": "The user has provided a problem statement regarding the fine-mapping of a GWAS locus and has asked for an evaluation of several proposed analysis plans. The task is to first validate the problem statement and then, if valid, to determine the most scientifically justified analysis plan and its outcomes.\n\n### Step 1: Extract Givens\n\n-   **Variants:** Four biallelic variants at a single locus: $v_1, v_2, v_3, v_4$.\n-   **Populations:** European ancestry (EUR) and African ancestry (AFR).\n-   **GWAS Association Data:**\n    -   EUR $z$-scores: $z^{\\mathrm{EUR}}_{v_1} = 6.0$, $z^{\\mathrm{EUR}}_{v_2} = 5.8$, $z^{\\mathrm{EUR}}_{v_3} = 2.0$, $z^{\\mathrm{EUR}}_{v_4} = 0.5$.\n    -   AFR $z$-scores: $z^{\\mathrm{AFR}}_{v_1} = 5.0$, $z^{\\mathrm{AFR}}_{v_2} = 1.0$, $z^{\\mathrm{AFR}}_{v_3} = 2.5$, $z^{\\mathrm{AFR}}_{v_4} = 0.2$.\n-   **Linkage Disequilibrium (LD) Data (Pearson correlation, $r$):**\n    -   EUR: $r_{12}^{\\mathrm{EUR}} = 0.95$. All other pairs have $|r| \\le 0.10$.\n    -   AFR: $r_{12}^{\\mathrm{AFR}} = 0.40$. All other pairs have $|r| \\le 0.05$.\n-   **Functional Genomics Data (in a disease-relevant cell type):**\n    -   **Annotations:**\n        -   $v_1$: In an enhancer element (with an ATAC-seq peak); alternate allele disrupts a Transcription Factor (TF) $X$ motif.\n        -   $v_2$: Intergenic, no accessible chromatin, no conservation.\n        -   $v_3$: In the promoter of gene $G$; in accessible chromatin.\n        -   $v_4$: Intronic, low conservation.\n    -   **eQTL for gene $G$:**\n        -   $v_1$: $z_{\\mathrm{eQTL}} = 4.5$; alternate allele reduces $G$ expression.\n        -   $v_2$: $z_{\\mathrm{eQTL}} = 0.5$.\n        -   $v_3$: $z_{\\mathrm{eQTL}} = 1.0$.\n        -   $v_4$: $z_{\\mathrm{eQTL}} = 0.0$.\n    -   **pQTL for protein $G$:**\n        -   $v_1$: $z_{\\mathrm{pQTL}} = 3.0$; alternate allele reduces protein abundance.\n    -   **Massively Parallel Reporter Assay (MPRA):**\n        -   $v_1$: Alternate allele shows a replicate-consistent $20\\%$ reduction in reporter activity.\n        -   $v_2, v_4$: No effect.\n        -   $v_3$: Small, non-significant $5\\%$ increase in activity.\n-   **Definitions and Principles:**\n    -   Bayes’ theorem for posterior odds.\n    -   Single-causal-variant fine-mapping concept.\n    -   Definition of a $95\\%$ credible set.\n    -   Definition of colocalization.\n    -   Principle of trans-ancestry fine-mapping.\n    -   Principle of using calibrated, not deterministic, functional priors.\n-   **Assumptions:** Single dominant signal; well-characterized relevant cell type; gene $G$ is expressed.\n-   **Question:** Identify the most scientifically justified end-to-end analysis plan and its results.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is evaluated for validity.\n\n-   **Scientifically Grounded:** The problem is firmly rooted in the established principles of statistical and population genetics, and molecular biology. The data types (GWAS z-scores, LD matrices, ATAC-seq, eQTL, pQTL, MPRA) and concepts (fine-mapping, colocalization, credible sets) are standard in modern genomics. The scenario presented is a classic example of trans-ancestry fine-mapping.\n-   **Well-Posed:** The problem is well-posed. It provides sufficient, consistent data to allow for a reasoned conclusion about the likely causal variant and mechanism. The combination of multi-ancestry association data and multi-omic functional data is designed to converge on a single, stable solution.\n-   **Objective:** The language is precise and objective. The data are presented numerically, and the principles are stated formally.\n-   **Consistency Check:**\n    -   The GWAS and LD data are mutually consistent and highly informative. In EUR, $v_1$ and $v_2$ have very similar, strong association signals ($z^{\\mathrm{EUR}}_{v_1} = 6.0$, $z^{\\mathrm{EUR}}_{v_2} = 5.8$) because they are in very high LD ($r_{12}^{\\mathrm{EUR}} = 0.95$). In AFR, the LD is substantially weaker ($r_{12}^{\\mathrm{AFR}} = 0.40$), and the association signal is retained only by $v_1$ ($z^{\\mathrm{AFR}}_{v_1} = 5.0$), while the signal for $v_2$ is attenuated to baseline ($z^{\\mathrm{AFR}}_{v_2} = 1.0$). This pattern strongly suggests $v_1$ is the causal variant and $v_2$ is a tag SNP in Europeans.\n    -   The functional data are internally consistent and all point to $v_1$ as the functional variant. $v_1$ is located in an accessible enhancer region (ATAC-seq peak), its alternate allele disrupts a TF motif, shows a significant effect in a reporter assay (MPRA), is a strong eQTL for gene $G$, and a pQTL for protein $G$.\n    -   The direction of effects is also consistent: the alternate allele of $v_1$ is associated with reduced enhancer activity (MPRA), reduced gene expression (eQTL), and reduced protein abundance (pQTL).\n    -   Other variants lack this convergence of evidence. $v_3$ is in a promoter but has very weak and insignificant functional effects and a weaker GWAS association. $v_2$ and $v_4$ have no functional support.\n-   **No Flaws Identified:** The problem does not violate any of the specified invalidity criteria. It is scientifically sound, complete, consistent, realistic, well-posed, and non-trivial.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. The solution process will now proceed.\n\n### Principle-Based Derivation and Option Evaluation\n\nThe optimal analysis plan must leverage all available data sources using statistically and biologically sound principles.\n\n1.  **Fine-Mapping Strategy:** The central challenge is to distinguish between $v_1$ and $v_2$. The most powerful tool provided is the difference in LD between EUR and AFR populations. A trans-ancestry fine-mapping method that jointly analyzes the summary statistics from both ancestries, using their respective LD matrices, is the state-of-the-art approach. This will correctly propagate the evidence, recognizing that a single causal variant ($v_1$) can explain the strong signal at both $v_1$ and $v_2$ in EUR (due to high LD) and the isolated strong signal at $v_1$ in AFR (due to lower LD).\n\n2.  **Integration of Functional Data:** The rich functional data should be incorporated as priors in a Bayesian fine-mapping framework. As per the provided principles, these priors must be *calibrated*—meaning their weight is based on a genome-wide estimate of how much a given annotation enriches for causal variants. This avoids the pitfalls of subjective or overly strong \"deterministic\" priors. Given the data, annotations like \"enhancer activity\", \"TF motif disruption\", and \"significant MPRA effect\" would modestly increase the prior probability for $v_1$.\n\n3.  **Expected Fine-Mapping Outcome:** A joint trans-ancestry analysis, correctly modeling LD and incorporating modest functional priors, would concentrate the posterior probability on $v_1$. The signal for $v_2$ would be fully explained by its LD with $v_1$ in the EUR population. Therefore, the posterior inclusion probability (PIP) for $v_1$ would be very high (e.g., $PIP > 0.95$), and the PIPs for all other variants would be close to zero. The resulting $95\\%$ credible set would contain only $v_1$.\n\n4.  **Variant-to-Gene-to-Mechanism:**\n    *   **Colocalization:** To link the GWAS signal to gene $G$, colocalization analysis is the correct method. This requires full summary statistics and LD for both the GWAS trait and the molecular trait (eQTL/pQTL). Given the strong, coincident signals at $v_1$ for all three traits (GWAS, eQTL, pQTL), a high posterior probability for a shared causal variant is expected.\n    *   **Mechanism Hypothesis:** The evidence converges on a clear and consistent mechanistic story: the alternate allele of $v_1$ disrupts a TF $X$ binding site in an enhancer, which reduces the enhancer's activity. This, in turn, downregulates the expression of gene $G$, leading to lower levels of protein $G$, which influences the disease or trait risk. The link from the enhancer to gene $G$ should be supported by 3D chromatin interaction data (e.g., Promoter Capture Hi-C) or computational scores that integrate this information (e.g., ABC).\n\n5.  **Experimental Validation:** The hypothesis must be tested experimentally. The gold standard is precise genome editing (e.g., CRISPR base editing) of the candidate causal variant ($v_1$) in a biologically relevant cellular context (e.g., iPSC-derived disease-relevant cells) to observe the predicted molecular consequences (changes in TF binding, gene expression, protein levels). Perturbing the regulatory element (e.g., via CRISPRi) is also a key experiment to validate its function.\n\n### Option-by-Option Analysis\n\n**A. Analysis Plan:** This option outlines an analysis that aligns perfectly with the derived ideal workflow.\n-   It correctly proposes trans-ancestry fine-mapping with a joint likelihood and ancestry-specific LD.\n-   It correctly proposes using calibrated hierarchical priors for functional data.\n-   The reported results ($PIP_{v_1} = 0.96$, credible set $\\{v_1\\}$) are precisely what would be expected from a correct analysis of the given data.\n-   It correctly identifies colocalization using full summary statistics as the method for gene assignment and strengthens this with 3D genome evidence (Hi-C/ABC).\n-   The proposed mechanism is fully consistent with every piece of provided data.\n-   The prioritized experimental plan (base editing, CRISPRi, allele-specific assays in relevant cells) represents the current gold standard for functional validation.\n-   **Verdict:** **Correct**.\n\n**B. Analysis Plan:** This option contains multiple fundamental errors.\n-   It discards the AFR data, which is the key to resolving the causal variant. A EUR-only analysis would be underpowered, leading to the incorrect credible set $\\{v_1, v_2\\}$.\n-   It uses MPRA as a deterministic filter, violating a stated principle and representing a crude analytical approach.\n-   It proposes an outdated gene assignment method (nearest gene) while ignoring direct evidence (eQTL).\n-   It states a mechanism (increased activity) that is in direct contradiction to the eQTL and MPRA data.\n-   The experimental plan is weak, using an irrelevant cell line and a less informative assay.\n-   **Verdict:** **Incorrect**.\n\n**C. Analysis Plan:** This option is based on statistically invalid methods.\n-   Pooling z-scores with inverse-variance weighting while ignoring LD is fundamentally incorrect for fine-mapping at a locus with linked variants. This invalidates the entire premise of the analysis.\n-   Calculating PIPs from softmax of z-scores is a crude approximation that is only valid under an assumption of independence, which is violated here.\n-   Performing colocalization with only $p$-values and no LD is not a valid or modern approach.\n-   It resorts to the same poor choices as option B for gene assignment (proximity) and experimental design (non-specific cell line).\n-   **Verdict:** **Incorrect**.\n\n**D. Analysis Plan:** This option describes a biased and unscientific analysis.\n-   It proposes applying a heavy, uncalibrated functional prior to force a desired result ($v_3$ as causal), directly violating a stated principle and ignoring the overwhelming association and functional data pointing to $v_1$. This is an example of confirmation bias, not objective science.\n-   The conclusions—that the credible set is $\\{v_3\\}$ and that weak functional signals are sufficient justification—are artifacts of this biased prior, not the evidence.\n-   It prioritizes experiments on a variant with very little supporting evidence, which would be an inefficient use of resources.\n-   **Verdict:** **Incorrect**.\n\nBased on a thorough evaluation, Option A is the only one that describes a scientifically rigorous, state-of-the-art, and internally consistent plan that correctly interprets all the provided data according to the given principles.", "answer": "$$\\boxed{A}$$", "id": "4341927"}]}