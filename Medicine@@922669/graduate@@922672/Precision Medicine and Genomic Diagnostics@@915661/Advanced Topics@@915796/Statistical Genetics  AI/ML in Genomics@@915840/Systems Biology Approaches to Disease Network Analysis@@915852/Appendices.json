{"hands_on_practices": [{"introduction": "A primary goal in disease network analysis is to identify the most influential molecular players. Eigenvector centrality offers a powerful way to do this by formalizing the intuitive idea that a gene's importance is proportional to the importance of its neighbors. This foundational exercise guides you through deriving the mathematical basis for eigenvector centrality and applying it to calculate the influence of a gene in a simple, illustrative network, providing a concrete understanding of this core spectral method. [@problem_id:4387242]", "problem": "In a systems biology study of a disease-specific gene module used in precision medicine and genomic diagnostics, consider a small undirected gene interaction network of three genes $\\{G_{1}, G_{2}, G_{3}\\}$ with nonnegative interaction strengths captured by the adjacency matrix $A$. Assume the network is connected, interactions are symmetric, and the adjacency matrix has nonnegative entries, reflecting measured functional associations (e.g., co-expression or protein–protein interaction evidence). The matrix is\n$$\nA=\\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix}.\n$$\nA core principle of eigenvector-based centrality used in disease network analysis is that the influence (centrality) $c_{i}$ of a gene $G_{i}$ is proportional to the sum of the influences of its neighbors weighted by the interaction strengths. Starting from this proportionality principle and standard linear algebra results for nonnegative irreducible matrices, derive the defining relationship that the centrality vector $c$ must satisfy. Using this relationship and the leading eigenvector of $A$, compute the eigenvector centrality for $G_{2}$ under the convention that the centrality vector is scaled to have unit Euclidean norm (sum of squares equals $1$). Provide your answer as a simplified exact expression with no units.", "solution": "The core principle of eigenvector centrality states that the centrality of a node, $c_i$, is proportional to the weighted sum of the centralities of its neighbors. The weights are given by the entries of the adjacency matrix $A$. Mathematically, this can be written as:\n$$\nc_i \\propto \\sum_{j} A_{ij} c_j\n$$\nIntroducing a constant of proportionality, $\\frac{1}{\\lambda}$, which must be the same for all nodes $i$, we obtain a system of linear equations:\n$$\nc_i = \\frac{1}{\\lambda} \\sum_{j} A_{ij} c_j\n$$\nThis system of equations can be expressed in matrix form. Let $c$ be the column vector of centralities, $c = (c_1, c_2, c_3)^T$. The equation becomes:\n$$\nc = \\frac{1}{\\lambda} Ac\n$$\nMultiplying by $\\lambda$, we arrive at the defining relationship for the centrality vector:\n$$\nAc = \\lambda c\n$$\nThis is the standard eigenvalue equation. It establishes that the centrality vector $c$ is an eigenvector of the adjacency matrix $A$, and $\\lambda$ is its corresponding eigenvalue. For a non-negative, irreducible matrix such as the given $A$, the Perron-Frobenius theorem guarantees that there is a unique largest real eigenvalue, $\\lambda_{\\text{max}}$, and its corresponding eigenvector has strictly positive components. This \"leading eigenvector\" is the one used for centrality, as it ensures all nodes have a positive influence score.\n\nNow, we compute the eigenvalues and eigenvectors for the given adjacency matrix:\n$$\nA=\\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ are found by solving the characteristic equation $\\det(A - \\lambda I) = 0$, where $I$ is the identity matrix.\n$$\n\\det\\begin{pmatrix}\n-\\lambda & 1 & 0 \\\\\n1 & -\\lambda & 1 \\\\\n0 & 1 & -\\lambda\n\\end{pmatrix} = 0\n$$\nExpanding the determinant along the first row:\n$$\n-\\lambda \\det\\begin{pmatrix} -\\lambda & 1 \\\\ 1 & -\\lambda \\end{pmatrix} - 1 \\det\\begin{pmatrix} 1 & 1 \\\\ 0 & -\\lambda \\end{pmatrix} = 0\n$$\n$$\n-\\lambda(\\lambda^2 - 1) - 1(-\\lambda - 0) = 0\n$$\n$$\n-\\lambda^3 + \\lambda + \\lambda = 0\n$$\n$$\n-\\lambda^3 + 2\\lambda = 0\n$$\n$$\n\\lambda(2 - \\lambda^2) = 0\n$$\nThe eigenvalues are $\\lambda_1 = \\sqrt{2}$, $\\lambda_2 = 0$, and $\\lambda_3 = -\\sqrt{2}$. The largest eigenvalue is $\\lambda_{\\text{max}} = \\sqrt{2}$.\n\nNext, we find the eigenvector $c = (c_1, c_2, c_3)^T$ corresponding to $\\lambda = \\sqrt{2}$ by solving $(A - \\sqrt{2}I)c = 0$.\n$$\n\\begin{pmatrix}\n-\\sqrt{2} & 1 & 0 \\\\\n1 & -\\sqrt{2} & 1 \\\\\n0 & 1 & -\\sqrt{2}\n\\end{pmatrix}\n\\begin{pmatrix} c_1 \\\\ c_2 \\\\ c_3 \\end{pmatrix}\n=\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis yields the system of linear equations:\n1. $-\\sqrt{2}c_1 + c_2 = 0 \\implies c_2 = \\sqrt{2}c_1$\n2. $c_1 - \\sqrt{2}c_2 + c_3 = 0$\n3. $c_2 - \\sqrt{2}c_3 = 0 \\implies c_2 = \\sqrt{2}c_3$\n\nFrom equations (1) and (3), we see that $\\sqrt{2}c_1 = \\sqrt{2}c_3$, which implies $c_1 = c_3$. This is consistent with equation (2), as $c_1 - \\sqrt{2}(\\sqrt{2}c_1) + c_1 = c_1 - 2c_1 + c_1 = 0$.\nThe eigenvector is thus of the form $(k, k\\sqrt{2}, k)^T$ for some constant $k$.\n\nThe problem requires the centrality vector to be scaled to have a unit Euclidean norm, i.e., $c_1^2 + c_2^2 + c_3^2 = 1$.\nSubstituting the components of the eigenvector:\n$$\n(k)^2 + (k\\sqrt{2})^2 + (k)^2 = 1\n$$\n$$\nk^2 + 2k^2 + k^2 = 1\n$$\n$$\n4k^2 = 1\n$$\n$$\nk^2 = \\frac{1}{4} \\implies k = \\frac{1}{2}\n$$\nWe choose the positive root $k = \\frac{1}{2}$ to ensure the centrality scores are positive. The normalized centrality vector is:\n$$\nc = \\begin{pmatrix} c_1 \\\\ c_2 \\\\ c_3 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ (\\sqrt{2})/2 \\\\ 1/2 \\end{pmatrix}\n$$\nThe eigenvector centrality for gene $G_1$ is $c_1 = \\frac{1}{2}$, for gene $G_2$ is $c_2 = \\frac{\\sqrt{2}}{2}$, and for gene $G_3$ is $c_3 = \\frac{1}{2}$.\n\nThe question asks for the eigenvector centrality for gene $G_2$, which is $c_2$.\n$$\nc_2 = \\frac{\\sqrt{2}}{2}\n$$", "answer": "$$\\boxed{\\frac{\\sqrt{2}}{2}}$$", "id": "4387242"}, {"introduction": "While some genes are influential due to their direct connections, others derive their importance from their strategic position as bridges or 'bottlenecks' in biological pathways. Betweenness centrality is designed to quantify this brokerage role by measuring how often a node lies on the shortest communication paths between other nodes. This practice challenges you to derive the betweenness centrality formula from first principles and adapt it to a weighted network, where edge weights reflect interaction reliabilities—a common scenario in real-world genomic data. [@problem_id:4387192]", "problem": "In disease network analysis for precision medicine, genes and their interactions are naturally modeled as graphs where nodes represent molecular entities (for example, genes) and edges encode functional relationships inferred from multi-omics data integration. Consider an undirected graph where a simple path is a sequence of distinct nodes connected by edges, and a geodesic path is any simple path with minimal path length between its endpoints.\n\nStarting from these core definitions only, first derive a general expression for the betweenness centrality of a node in an undirected graph in terms of counts of geodesic paths between node pairs and the subset of those geodesics that pass through the focal node. Your derivation must proceed from the definitional idea that a node’s brokerage is proportional to the fraction of geodesic traffic it intermediates between all other pairs of nodes, without appealing to any pre-stated centrality formulas.\n\nNext, extend the derivation to weighted graphs whose edges encode estimated interaction reliabilities. Assume edge weights $w_{ij}$ lie in the interval $(0,1]$ and represent the estimated probability (or reliability) that an interaction is functionally relevant in a given disease context. Using the widely adopted transformation $l_{ij} = -\\ln(w_{ij})$ to convert reliabilities into additive nonnegative edge lengths, formalize how shortest path computation and the counting of geodesic multiplicities change in the weighted case. Briefly explain why this transformation is appropriate for multiplicative reliabilities along paths.\n\nFinally, consider the following undirected weighted disease module on the node set $\\{G_1,G_2,G_3,G_4\\}$, where the only edges and their reliabilities are:\n- $(G_1,G_2)$ with $w_{12} = \\exp(-0.3)$,\n- $(G_1,G_3)$ with $w_{13} = \\exp(-0.4)$,\n- $(G_2,G_3)$ with $w_{23} = \\exp(-0.2)$,\n- $(G_3,G_4)$ with $w_{34} = \\exp(-0.5)$,\n- $(G_2,G_4)$ with $w_{24} = \\exp(-0.7)$.\n\nNo other edges exist. Using the unnormalized undirected convention that sums over unordered pairs $s<t$ with $s \\neq v \\neq t$, compute the weighted betweenness centrality of node $G_3$ by summing, for each pair $\\{s,t\\}$, the fraction of geodesics between $s$ and $t$ that pass through $v = G_3$, where geodesics are defined using path length equal to the sum of $l_{ij} = -\\ln(w_{ij})$ along the path. Express your final answer as a dimensionless number and round your answer to four significant figures.", "solution": "We begin from the core definitions of paths and geodesics in an undirected graph $G=(V,E)$ with finite $V$. A simple path between nodes $s$ and $t$ is an ordered sequence $(s=v_0,v_1,\\dots,v_k=t)$ such that $(v_{i-1},v_i) \\in E$ for all $i \\in \\{1,\\dots,k\\}$ and all $v_i$ are distinct. In an unweighted graph, the length of a path is the number of edges $k$. A geodesic path between $s$ and $t$ is any simple path with minimal length among all $s$–$t$ paths.\n\nThe intuitive basis for betweenness centrality is the fraction of geodesic traffic that is routed through a given node $v$. Let $\\sigma_{st}$ denote the number of geodesic paths between distinct nodes $s$ and $t$, and let $\\sigma_{st}(v)$ denote the number of those geodesic paths that pass through $v$. Because each geodesic between $s$ and $t$ contributes one unit of “traffic” to the $s$–$t$ flow, and the share controlled by $v$ is the fraction $\\sigma_{st}(v)/\\sigma_{st}$, the total brokerage of $v$ across all other unordered node pairs in an undirected graph is obtained by summing this fraction over all unordered pairs $\\{s,t\\}$ with $s \\neq v \\neq t$. Thus, the unnormalized betweenness centrality for undirected graphs is derived as\n$$\nC_{B}(v) \\;=\\; \\sum_{\\substack{s,t \\in V \\\\ s < t \\\\ s \\neq v \\neq t}} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}.\n$$\nThis expression encodes the principle that each pair $\\{s,t\\}$ contributes an amount proportional to the share of shortest-path connectivity controlled by $v$, and the use of unordered pairs $s<t$ prevents double counting in undirected graphs.\n\nWe now extend these definitions to weighted graphs, where each edge $(i,j)$ has a reliability weight $w_{ij} \\in (0,1]$ encoding the estimated probability that the interaction is functionally relevant in the disease context. A path $(v_0,\\dots,v_k)$ has reliability equal to the product of its edge reliabilities:\n$$\nW(v_0,\\dots,v_k) \\;=\\; \\prod_{i=1}^{k} w_{v_{i-1}v_i}.\n$$\nMaximizing the path reliability is equivalent to maximizing the product. Because the natural logarithm is strictly increasing and $\\ln\\!\\left(\\prod_i w_i\\right) = \\sum_i \\ln(w_i)$, maximizing $\\prod_i w_i$ is equivalent to minimizing $-\\sum_i \\ln(w_i)$. Therefore, defining the edge length\n$$\nl_{ij} \\;=\\; -\\ln(w_{ij}) \\;\\;\\ge 0\n$$\nconverts multiplicative reliabilities into additive nonnegative lengths. The weighted length of a path $(v_0,\\dots,v_k)$ is then\n$$\nL(v_0,\\dots,v_k) \\;=\\; \\sum_{i=1}^{k} l_{v_{i-1}v_i}.\n$$\nA weighted geodesic between $s$ and $t$ is any simple path achieving the minimal sum length among all $s$–$t$ paths. The set of weighted geodesics may contain multiple distinct paths if several achieve the same minimal total length; their count is $\\sigma_{st}$, and the number of those passing through $v$ is $\\sigma_{st}(v)$. The same centrality expression applies,\n$$\nC_{B}(v) \\;=\\; \\sum_{\\substack{s,t \\in V \\\\ s < t \\\\ s \\neq v \\neq t}} \\frac{\\sigma_{st}(v)}{\\sigma_{st}},\n$$\nwith $\\sigma_{st}$ and $\\sigma_{st}(v)$ now computed using weighted geodesics under the additive lengths $l_{ij}=-\\ln(w_{ij})$. Algorithmically, one computes weighted geodesic distances and multiplicities using Dijkstra’s algorithm augmented to track the number of distinct shortest paths to each node, whereas in the unweighted case, Breadth-First Search (BFS) suffices. The transformation $l_{ij}=-\\ln(w_{ij})$ is appropriate because it maps the product of reliabilities along a path into a sum of nonnegative terms, allowing the use of additive shortest-path algorithms to identify the maximum-reliability paths.\n\nWe now compute the weighted betweenness centrality of $G_3$ in the given network. The node set is $V=\\{G_1,G_2,G_3,G_4\\}$. The given reliabilities are:\n- $w_{12} = \\exp(-0.3)$, so $l_{12} = -\\ln(w_{12}) = 0.3$,\n- $w_{13} = \\exp(-0.4)$, so $l_{13} = 0.4$,\n- $w_{23} = \\exp(-0.2)$, so $l_{23} = 0.2$,\n- $w_{34} = \\exp(-0.5)$, so $l_{34} = 0.5$,\n- $w_{24} = \\exp(-0.7)$, so $l_{24} = 0.7$.\n\nThere are $4$ nodes, so the unordered pairs $\\{s,t\\}$ with $s \\neq G_3 \\neq t$ are\n$\\{G_1,G_2\\}$, $\\{G_1,G_4\\}$, and $\\{G_2,G_4\\}$.\n\nWe evaluate each pair:\n\n1. Pair $\\{G_1,G_2\\}$. Candidate paths and lengths:\n- $G_1$–$G_2$: length $0.3$,\n- $G_1$–$G_3$–$G_2$: length $0.4 + 0.2 = 0.6$,\n- any longer alternatives are strictly longer because all $l_{ij} \\ge 0$.\nThe unique geodesic is $G_1$–$G_2$ with length $0.3$, so $\\sigma_{G_1G_2} = 1$ and $\\sigma_{G_1G_2}(G_3) = 0$. Contribution to $C_B(G_3)$ is $0$.\n\n2. Pair $\\{G_1,G_4\\}$. Candidate paths and lengths:\n- $G_1$–$G_3$–$G_4$: $0.4 + 0.5 = 0.9$,\n- $G_1$–$G_2$–$G_4$: $0.3 + 0.7 = 1.0$,\n- $G_1$–$G_2$–$G_3$–$G_4$: $0.3 + 0.2 + 0.5 = 1.0$.\nThe unique geodesic is $G_1$–$G_3$–$G_4$ with length $0.9$, so $\\sigma_{G_1G_4} = 1$ and $\\sigma_{G_1G_4}(G_3) = 1$. Contribution is $1$.\n\n3. Pair $\\{G_2,G_4\\}$. Candidate paths and lengths:\n- $G_2$–$G_4$: $0.7$,\n- $G_2$–$G_3$–$G_4$: $0.2 + 0.5 = 0.7$,\n- $G_2$–$G_1$–$G_3$–$G_4$: $0.3 + 0.4 + 0.5 = 1.2$.\nThere are two geodesics tied at length $0.7$: $G_2$–$G_4$ and $G_2$–$G_3$–$G_4$. Thus $\\sigma_{G_2G_4} = 2$ and $\\sigma_{G_2G_4}(G_3) = 1$ (only the path $G_2$–$G_3$–$G_4$ traverses $G_3$). Contribution is $\\frac{1}{2}$.\n\nSumming the contributions,\n$$\nC_B(G_3) \\;=\\; 0 \\;+\\; 1 \\;+\\; \\frac{1}{2} \\;=\\; \\frac{3}{2} \\;=\\; 1.5.\n$$\nThe centrality is dimensionless. Rounded to four significant figures, the result is $1.500$.", "answer": "$$\\boxed{1.500}$$", "id": "4387192"}, {"introduction": "Moving beyond static network properties, modern systems biology increasingly uses deep learning to learn feature-rich representations of genes based on their network context. Graph Convolutional Networks (GCNs) provide a powerful framework for this by iteratively aggregating and transforming information from a gene's local neighborhood. This advanced practice bridges network theory and machine learning, asking you to derive a GCN layer from fundamental principles like locality and normalization, and then apply it to compute a feature update for a gene in a network. [@problem_id:4387216]", "problem": "Consider a disease-gene interaction network used in precision medicine and genomic diagnostics, where each node represents a gene, and an undirected edge represents a validated functional interaction relevant to a disease phenotype. Let the network be a simple undirected graph with adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$ and degree matrix $D \\in \\mathbb{R}^{n \\times n}$ defined by $D_{ii} = \\sum_{j} A_{ij}$. A node signal $H^{(l)} \\in \\mathbb{R}^{n \\times f}$ holds $f$ features per gene at layer $l$. A Graph Convolutional Network (GCN) layer is a local, permutation-equivariant linear operator followed by a pointwise nonlinearity $\\sigma(\\cdot)$ acting on $H^{(l)}$ and a trainable weight matrix $W^{(l)} \\in \\mathbb{R}^{f \\times g}$ to produce $H^{(l+1)} \\in \\mathbb{R}^{n \\times g}$. Starting from the following foundational principles:\n- Locality and edge-respecting propagation: messages may only pass along existing edges and self-edges.\n- Permutation equivariance: reindexing nodes by any permutation does not change the functional form of the operator.\n- Stability and symmetry: the linear propagation operator should be self-adjoint with respect to an appropriate inner product to avoid degree-induced biases.\n- Degree normalization: message magnitudes should be rescaled to mitigate the disproportionate influence of hub genes and to avoid exploding or vanishing magnitudes across heterogeneous connectivity.\n- Self-information retention: each node should preserve a portion of its own signal at each step.\nDerive a graph convolution layer that satisfies the above properties using the adjacency with self-loops $\\tilde{A} = A + I$ and the corresponding degree matrix $\\tilde{D}$, and express the layer in closed form as an operator acting on $H^{(l)}$ and $W^{(l)}$ followed by $\\sigma(\\cdot)$. Interpret in biological terms why the chosen normalization addresses hub-dominated signals in disease networks.\n\nThen, apply your derived layer to the following concrete example. Consider $n=3$ genes labeled $G_{1}, G_{2}, G_{3}$ with undirected edges $\\{(G_{1},G_{2}), (G_{2},G_{3})\\}$, and include self-loops at all nodes. Thus, the adjacency without self-loops is\n$$\nA=\\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix},\n$$\nand with self-loops $\\tilde{A}=A+I$. Let the feature dimension be $f=1$, the next-layer dimension be $g=1$, and use the identity activation $\\sigma(x)=x$ for this layer. Let the input features be\n$$\nH^{(l)}=\\begin{pmatrix}1.0\\\\ -2.0\\\\ 0.5\\end{pmatrix},\n$$\nand the scalar weight be $W^{(l)}=2.5$. Compute the updated value of the feature for node $G_{2}$ after one layer using your derived operator. Round your answer to four significant figures. No units are required.", "solution": "### Derivation of the Graph Convolutional Layer\n\nA graph convolutional layer aims to update the feature representation of each node, $H^{(l)}$, to a new representation, $H^{(l+1)}$, by aggregating information from the local neighborhood of each node. We will construct the operator satisfying the given principles.\n\n1.  **Locality, Edge-Propagation, and Self-Information:** The most basic form of local aggregation is to sum the features of a node's neighbors. For a node $i$, this would be $\\sum_{j} A_{ij} h_j^{(l)}$, where $h_j^{(l)}$ is the feature vector of node $j$. In matrix form, this is $A H^{(l)}$. To incorporate the principle of self-information retention, we ensure each node receives a message from itself. This is achieved by adding self-loops to the graph, which corresponds to using the modified adjacency matrix $\\tilde{A} = A + I$. The simple propagation rule thus becomes $\\tilde{A} H^{(l)}$.\n\n2.  **Learnable Transformation:** To give the model learnable capacity, we introduce a trainable weight matrix $W^{(l)}$. The features are transformed linearly by this matrix. A standard GCN layer combines the propagation and transformation as $(\\tilde{A} H^{(l)}) W^{(l)}$.\n\n3.  **Permutation Equivariance:** The operator must be equivariant to node permutations. That is, if the nodes are re-indexed, the output features should be correspondingly re-indexed. The chosen matrix multiplication form $\\tilde{A} H^{(l)} W^{(l)}$ satisfies this property. If $P$ is a permutation matrix, the permuted adjacency matrix is $P \\tilde{A} P^T$ and the permuted features are $P H^{(l)}$. The new output is $(P \\tilde{A} P^T) (P H^{(l)}) W^{(l)} = P \\tilde{A} (P^T P) H^{(l)} W^{(l)} = P (\\tilde{A} H^{(l)} W^{(l)})$, which is the permuted version of the original output.\n\n4.  **Degree Normalization, Stability, and Symmetry:** The operator $\\tilde{A}$ causes an issue: the magnitude of the aggregated feature vector at a node becomes proportional to its degree in the graph with self-loops. This can lead to exploding signals at high-degree nodes (hubs) and vanishing signals at low-degree nodes, causing numerical instability during training. To address this and satisfy the stability principle, a normalization scheme is required. A simple scaling by the degree, such as $\\tilde{D}^{-1} \\tilde{A}$, resolves the scaling issue but is not a symmetric operator. A symmetric (self-adjoint) operator is desirable for stability, as it guarantees real eigenvalues. The symmetrically normalized operator is $S = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$. This operator is symmetric since $\\tilde{A}$ and $\\tilde{D}$ are symmetric. The operation for a single node $i$ becomes $\\sum_{j} \\frac{\\tilde{A}_{ij}}{\\sqrt{\\tilde{d}_i \\tilde{d}_j}} h_j^{(l)}$, where $\\tilde{d}_i$ is the degree of node $i$ corresponding to $\\tilde{A}$. This form represents a weighted average of neighbor features, preventing hub domination.\n\nBy combining these principles, we arrive at the linear part of the GCN layer: $\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} H^{(l)} W^{(l)}$. The final step is to apply a pointwise non-linear activation function $\\sigma(\\cdot)$. The complete GCN layer update rule is therefore:\n$$ H^{(l+1)} = \\sigma\\left( \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} H^{(l)} W^{(l)} \\right) $$\n\n### Biological Interpretation of Normalization\n\nIn a disease-gene network, genes do not contribute equally to biological processes. Some genes, often called \"hubs\" (e.g., $TP53, MYC$), interact with a very large number of other genes. In a simple message-passing scheme without normalization, the feature update at any given gene would be disproportionately influenced by the signals from its most highly connected neighbors. The symmetric normalization, $\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$, precisely mitigates this. The signal passed from gene $j$ to gene $i$ is scaled by the factor $1/\\sqrt{\\tilde{d}_i \\tilde{d}_j}$. This has two effects: (1) a signal originating from a hub gene $j$ (large $\\tilde{d}_j$) is down-weighted, preventing it from overwhelming the receiving gene $i$; (2) the aggregate signal received by a hub gene $i$ (large $\\tilde{d}_i$) is also scaled down, preventing its representation from exploding in magnitude. This balanced aggregation allows the network to learn from both hub signals and signals from less-connected, but potentially crucial, disease genes, leading to a more robust and nuanced understanding of the disease mechanism.\n\n### Application to the Concrete Example\n\nWe are asked to compute the updated feature for gene $G_2$. The update rule is $H^{(l+1)} = \\sigma(S H^{(l)} W^{(l)})$, where $S = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$.\n\n1.  **Compute $\\tilde{A}$ and $\\tilde{D}$**:\n    Given $A=\\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix}$, we add the identity matrix $I$.\n    $$ \\tilde{A} = A + I = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} $$\n    The degree matrix $\\tilde{D}$ contains the row sums of $\\tilde{A}$ on its diagonal.\n    $\\tilde{d}_1 = 1+1+0 = 2$\n    $\\tilde{d}_2 = 1+1+1 = 3$\n    $\\tilde{d}_3 = 0+1+1 = 2$\n    $$ \\tilde{D} = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} $$\n\n2.  **Compute $\\tilde{D}^{-1/2}$**:\n    $$ \\tilde{D}^{-1/2} = \\begin{pmatrix} 1/\\sqrt{2} & 0 & 0 \\\\ 0 & 1/\\sqrt{3} & 0 \\\\ 0 & 0 & 1/\\sqrt{2} \\end{pmatrix} $$\n\n3.  **Compute the propagation matrix $S = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$**:\n    $$ S = \\begin{pmatrix} 1/\\sqrt{2} & 0 & 0 \\\\ 0 & 1/\\sqrt{3} & 0 \\\\ 0 & 0 & 1/\\sqrt{2} \\end{pmatrix} \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1/\\sqrt{2} & 0 & 0 \\\\ 0 & 1/\\sqrt{3} & 0 \\\\ 0 & 0 & 1/\\sqrt{2} \\end{pmatrix} $$\n    $$ S = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{3} & 0 \\\\ 1/\\sqrt{6} & 1/\\sqrt{3} & 1/\\sqrt{2} \\\\ 0 & 1/\\sqrt{3} & 1/\\sqrt{2} \\end{pmatrix} \\begin{pmatrix} 1/\\sqrt{2} & 0 & 0 \\\\ 0 & 1/\\sqrt{3} & 0 \\\\ 0 & 0 & 1/\\sqrt{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{\\sqrt{6}} & 0 \\\\ \\frac{1}{\\sqrt{6}} & \\frac{1}{3} & \\frac{1}{\\sqrt{6}} \\\\ 0 & \\frac{1}{\\sqrt{6}} & \\frac{1}{2} \\end{pmatrix} $$\n\n4.  **Compute the feature update for node $G_2$**:\n    We need the second row of the matrix $H^{(l+1)} = \\sigma(S H^{(l)} W^{(l)})$.\n    Given $\\sigma(x)=x$, $W^{(l)}=2.5$, and $H^{(l)}=\\begin{pmatrix}1.0\\\\ -2.0\\\\ 0.5\\end{pmatrix}$.\n    Let's first compute the second element of the propagated feature vector $H'_{prop} = S H^{(l)}$. Let this be $(H'_{prop})_2$.\n    The second row of $S$ is $\\begin{pmatrix} \\frac{1}{\\sqrt{6}} & \\frac{1}{3} & \\frac{1}{\\sqrt{6}} \\end{pmatrix}$.\n    $$ (H'_{prop})_2 = \\left(\\frac{1}{\\sqrt{6}} \\times 1.0\\right) + \\left(\\frac{1}{3} \\times (-2.0)\\right) + \\left(\\frac{1}{\\sqrt{6}} \\times 0.5\\right) $$\n    $$ (H'_{prop})_2 = \\frac{1.0}{\\sqrt{6}} - \\frac{2.0}{3} + \\frac{0.5}{\\sqrt{6}} = \\frac{1.5}{\\sqrt{6}} - \\frac{2}{3} $$\n    Now, we apply the weight $W^{(l)}=2.5$ and the identity activation. The updated feature for $G_2$ is $(H^{(l+1)})_2$.\n    $$ (H^{(l+1)})_2 = \\left( \\frac{1.5}{\\sqrt{6}} - \\frac{2}{3} \\right) \\times 2.5 $$\n    Numerically:\n    $$ \\frac{1.5}{\\sqrt{6}} \\approx \\frac{1.5}{2.44949} \\approx 0.612372 $$\n    $$ \\frac{2}{3} \\approx 0.666667 $$\n    $$ (H^{(l+1)})_2 \\approx (0.612372 - 0.666667) \\times 2.5 $$\n    $$ (H^{(l+1)})_2 \\approx (-0.054295) \\times 2.5 = -0.1357375 $$\n    Rounding to four significant figures, the result is $-0.1357$.", "answer": "$$\\boxed{-0.1357}$$", "id": "4387216"}]}