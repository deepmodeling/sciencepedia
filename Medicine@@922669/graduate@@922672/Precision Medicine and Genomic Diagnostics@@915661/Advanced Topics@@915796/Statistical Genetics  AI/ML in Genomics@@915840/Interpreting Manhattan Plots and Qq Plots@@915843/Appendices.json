{"hands_on_practices": [{"introduction": "The Manhattan plot is the quintessential visualization tool in genomics, offering a panoramic view of statistical association across the entire genome. However, its interpretation hinges on a critical question: where do we draw the line for statistical significance? This exercise provides hands-on practice with the foundational method for answering this, the Bonferroni correction, which is essential for controlling the rate of false positives in the context of massive multiple testing [@problem_id:4353047]. Mastering this calculation is the first step toward a rigorous interpretation of any genome-wide association study.", "problem": "A clinical genomics team is conducting an exome-wide rare variant association study to inform precision medicine in oncology. They test a subset of exonic single-nucleotide variants across the genome, totaling $m=200{,}000$ hypothesis tests for variant-phenotype associations. Under the null hypothesis for each test, the $p$-values are super-uniform on the unit interval due to valid calibration, and the team wants to control the Family-Wise Error Rate (FWER) at $\\alpha=0.05$ when deciding which associations to prioritize for functional validation.\n\nThe standard exome-wide visualization is a Manhattan plot where the $y$-axis records $-\\log_{10}(p)$ for each variant across chromosomes, supplemented by a quantile-quantile (QQ) plot to assess calibration and potential inflation. To maintain FWER control at level $\\alpha$, the team will annotate a horizontal line on the Manhattan plot corresponding to the Bonferroni-corrected per-test decision threshold.\n\nStarting from the fundamental definition of FWER as the probability of at least one false positive across the $m$ tests, and using well-tested probabilistic bounds under multiple testing, derive the Bonferroni per-test $p$-value threshold and then compute the corresponding horizontal annotation level on the Manhattan plot, expressed as $-\\log_{10}$ of that threshold. Round your final $-\\log_{10}$ value to six significant figures. Provide only the final $-\\log_{10}$ threshold value as your answer.", "solution": "The problem is valid as it presents a standard, well-defined task in statistical genetics that is scientifically grounded, objective, and contains all necessary information for a unique solution.\n\nThe objective is to determine the significance threshold on the $-\\log_{10}(p)$ scale for a Manhattan plot, such that the Family-Wise Error Rate (FWER) is controlled at a specified level $\\alpha$ for a given number of hypothesis tests $m$. The method stipulated is the Bonferroni correction.\n\nFirst, we define the Family-Wise Error Rate (FWER). For a family of $m$ hypothesis tests, the FWER is the probability of making at least one Type I error (a false positive) among all tests. Let $V$ be the random variable representing the number of true null hypotheses that are incorrectly rejected. The FWER is then defined as:\n$$\n\\text{FWER} = P(V \\ge 1)\n$$\n\nLet $H_{0_1}, H_{0_2}, \\dots, H_{0_m}$ be the null hypotheses for the $m$ tests, and let $p_1, p_2, \\dots, p_m$ be the corresponding $p$-values. Let $I_0$ be the set of indices for which the null hypothesis is true. A Type I error occurs for test $i \\in I_0$ if we reject $H_{0_i}$. The decision rule is to reject $H_{0_i}$ if its $p$-value $p_i$ is less than or equal to some per-test significance threshold, which we denote as $\\alpha_{BT}$.\n\nThe event $\\{V \\ge 1\\}$ is the union of the events that a true null hypothesis $H_{0_i}$ (where $i \\in I_0$) is rejected. This can be written as:\n$$\n\\text{FWER} = P\\left( \\bigcup_{i \\in I_0} \\{p_i \\le \\alpha_{BT}\\} \\right)\n$$\n\nTo control this probability, we use Boole's inequality, also known as the union bound. This inequality states that for any collection of events, the probability of their union is less than or equal to the sum of their individual probabilities.\n$$\nP\\left( \\bigcup_{i \\in I_0} \\{p_i \\le \\alpha_{BT}\\} \\right) \\le \\sum_{i \\in I_0} P(p_i \\le \\alpha_{BT})\n$$\n\nUnder a true null hypothesis $H_{0_i}$, the $p$-value $p_i$ follows a uniform distribution on the interval $[0, 1]$. The problem states the $p$-values are \"super-uniform\", which means $P(p_i \\le x) \\le x$ for any $x \\in [0, 1]$. This condition is more conservative and strengthens the inequality. For the purpose of finding a general bound, we consider the worst-case scenario allowed, which is the uniform distribution, where $P(p_i \\le \\alpha_{BT}) = \\alpha_{BT}$. Therefore:\n$$\n\\sum_{i \\in I_0} P(p_i \\le \\alpha_{BT}) \\le \\sum_{i \\in I_0} \\alpha_{BT}\n$$\n\nLet $m_0$ be the number of true null hypotheses, so $m_0 = |I_0|$. The sum becomes $m_0 \\alpha_{BT}$. Combining the inequalities, we have:\n$$\n\\text{FWER} \\le m_0 \\alpha_{BT}\n$$\n\nIn a typical experimental setting, the number of true nulls $m_0$ is unknown. To ensure the FWER is controlled under any condition, we must take the most conservative (largest) possible value for $m_0$, which is the total number of tests, $m$. So, $m_0 \\le m$. This leads to the well-known Bonferroni bound:\n$$\n\\text{FWER} \\le m \\alpha_{BT}\n$$\n\nTo control the FWER at a desired level $\\alpha$, we enforce the condition that this upper bound is equal to $\\alpha$:\n$$\nm \\alpha_{BT} = \\alpha\n$$\nSolving for the per-test threshold $\\alpha_{BT}$ gives the Bonferroni correction formula:\n$$\n\\alpha_{BT} = \\frac{\\alpha}{m}\n$$\n\nNow, we substitute the given values into this formula.\nThe total number of hypothesis tests is $m = 200,000$.\nThe desired FWER level is $\\alpha = 0.05$.\n\nThe Bonferroni-corrected per-test $p$-value threshold is:\n$$\n\\alpha_{BT} = \\frac{0.05}{200,000} = \\frac{5 \\times 10^{-2}}{2 \\times 10^5} = 2.5 \\times 10^{-7}\n$$\n\nThe Manhattan plot displays the results on a $-\\log_{10}$ scale. The horizontal line corresponding to this significance threshold will be at a $y$-value of $-\\log_{10}(\\alpha_{BT})$.\n$$\ny_{\\text{threshold}} = -\\log_{10}(\\alpha_{BT}) = -\\log_{10}(2.5 \\times 10^{-7})\n$$\n\nUsing the properties of logarithms, specifically $\\log(ab) = \\log(a) + \\log(b)$ and $\\log(10^k) = k$:\n$$\ny_{\\text{threshold}} = -(\\log_{10}(2.5) + \\log_{10}(10^{-7}))\n$$\n$$\ny_{\\text{threshold}} = -(\\log_{10}(2.5) - 7)\n$$\n$$\ny_{\\text{threshold}} = 7 - \\log_{10}(2.5)\n$$\n\nNow, we compute the numerical value:\n$$\n\\log_{10}(2.5) \\approx 0.39794000867...\n$$\n$$\ny_{\\text{threshold}} \\approx 7 - 0.39794000867 = 6.60205999133...\n$$\n\nThe problem requires rounding the final value to six significant figures. The digits are $6, 6, 0, 2, 0, 5$. The seventh digit is $9$, so we round up the sixth digit.\n$$\ny_{\\text{threshold}} \\approx 6.60206\n$$\nThis is the required annotation level for the Manhattan plot.", "answer": "$$\\boxed{6.60206}$$", "id": "4353047"}, {"introduction": "Before we can trust the significant 'skyscrapers' on a Manhattan plot, we must first verify the integrity of the underlying statistical foundation. The Quantile-Quantile (QQ) plot and its numerical summary, the genomic inflation factor ($\\lambda_{GC}$), are our primary tools for this diagnostic check. This practice demystifies the $\\lambda_{GC}$ metric by guiding you through its calculation from a raw set of test statistics, providing a concrete understanding of how this single value quantifies systematic deviations from the null hypothesis [@problem_id:4353249].", "problem": "A cohort-wide case-control analysis in a precision oncology Genome-Wide Association Study (GWAS) evaluates single nucleotide variant associations with a pharmacogenomic phenotype. For a Manhattan plot and Quantile-Quantile (QQ) plot sanity check, you wish to quantify potential test statistic inflation using the Genomic Control (GC) inflation factor. Under the null hypothesis of no association, single-variant $z$-scores are approximately distributed as a standard normal, and the corresponding squared $z$-scores follow a chi-square distribution with one degree of freedom. The GC inflation factor $\\lambda_{GC}$ is defined as the ratio of the observed median of the chi-square statistics to the null median. In the absence of inflation, the null median of a chi-square distribution with one degree of freedom equals $0.4559$. \n\nYou are given the following vector of $z$-scores for a subset of independent variants assayed in the GWAS: \n$$[\\,0.00,\\,-0.15,\\,0.25,\\,-0.35,\\,0.45,\\,-0.55,\\,0.65,\\,-0.68,\\,0.71,\\,-0.75,\\,0.85,\\,-1.20,\\,1.50,\\,-2.00,\\,2.50,\\,-3.00,\\,3.50\\,].$$\n\nStarting from first principles of the null distribution for $z$-scores, compute the GC inflation factor $\\lambda_{GC}$ by first transforming each $z$-score to a chi-square statistic with one degree of freedom via squaring, then taking the median of these chi-square values, and finally dividing by $0.4559$. Round your answer to $4$ significant figures. Express the final value as a pure number without units.", "solution": "Under the null hypothesis of no association, the single-variant test statistic $z$ is approximately distributed as $\\mathcal{N}(0,1)$, the standard normal distribution. A fundamental property of the standard normal is that $z^{2}$ is distributed as a chi-square random variable with one degree of freedom, denoted $\\chi^{2}_{1}$. The Quantile-Quantile (QQ) plot compares observed quantiles of the test statistics to expected quantiles under $\\chi^{2}_{1}$, and the Genomic Control (GC) inflation factor $\\lambda_{GC}$ quantifies inflation by comparing the observed median of $\\chi^{2}_{1}$ statistics to the theoretical null median.\n\nThe GC inflation factor is defined as:\n$$\\lambda_{GC} \\equiv \\frac{\\mathrm{median}\\big(\\{z_{i}^{2}\\}\\big)}{m_{0}}$$\nwhere $m_{0}$ is the theoretical median of $\\chi^{2}_{1}$, equal to $0.4559$.\n\nWe are given $n=17$ $z$-scores:\n$$[\\,0.00,\\,-0.15,\\,0.25,\\,-0.35,\\,0.45,\\,-0.55,\\,0.65,\\,-0.68,\\,0.71,\\,-0.75,\\,0.85,\\,-1.20,\\,1.50,\\,-2.00,\\,2.50,\\,-3.00,\\,3.50\\,].$$\n\nFirst, square each $z$-score to obtain the observed $\\chi^{2}_{1}$ statistics:\n- $0.00^{2} = 0.0000$\n- $(-0.15)^{2} = 0.0225$\n- $0.25^{2} = 0.0625$\n- $(-0.35)^{2} = 0.1225$\n- $0.45^{2} = 0.2025$\n- $(-0.55)^{2} = 0.3025$\n- $0.65^{2} = 0.4225$\n- $(-0.68)^{2} = 0.4624$\n- $0.71^{2} = 0.5041$\n- $(-0.75)^{2} = 0.5625$\n- $0.85^{2} = 0.7225$\n- $(-1.20)^{2} = 1.4400$\n- $1.50^{2} = 2.2500$\n- $(-2.00)^{2} = 4.0000$\n- $2.50^{2} = 6.2500$\n- $(-3.00)^{2} = 9.0000$\n- $3.50^{2} = 12.2500$\n\nOrder these values in ascending order (they are already in ascending order after squaring given the construction by increasing absolute value):\n$$[\\,0.0000,\\,0.0225,\\,0.0625,\\,0.1225,\\,0.2025,\\,0.3025,\\,0.4225,\\,0.4624,\\,0.5041,\\,0.5625,\\,0.7225,\\,1.4400,\\,2.2500,\\,4.0000,\\,6.2500,\\,9.0000,\\,12.2500\\,].$$\n\nFor an odd number $n=17$, the sample median is the $\\frac{n+1}{2} = 9$-th ordered value. Thus, the median of the observed statistics is:\n$$\\mathrm{median}\\big(\\{z_{i}^{2}\\}\\big) = 0.5041$$\n\nApply the GC definition with $m_{0} = 0.4559$:\n$$\\lambda_{GC} = \\frac{0.5041}{0.4559}$$\n\nNote that $0.5041 = \\frac{5041}{10000}$ and $0.4559 = \\frac{4559}{10000}$, hence:\n$$\\lambda_{GC} = \\frac{\\frac{5041}{10000}}{\\frac{4559}{10000}} = \\frac{5041}{4559}$$\n\nCompute the numerical value and round to $4$ significant figures:\n$$\\lambda_{GC} \\approx 1.105724955\\ldots \\quad \\Rightarrow \\quad \\lambda_{GC} \\text{ (to }4\\text{ significant figures)} = 1.106$$", "answer": "$$\\boxed{1.106}$$", "id": "4353249"}, {"introduction": "In modern, large-scale genome-wide association studies, observing an inflated $\\lambda_{GC}$ is common, but its source is not always obvious. Is it a sign of technical artifacts and confounding, or is it the expected signature of true, widespread polygenic signal? This advanced problem challenges you to move beyond a simple interpretation and use sophisticated tools like Linkage Disequilibrium (LD) Score Regression to dissect the components of genomic inflation [@problem_id:4353250]. Successfully navigating this scenario is a key skill for accurately translating genomic discoveries into precision medicine insights.", "problem": "A pharmacogenomic Genome-Wide Association Study (GWAS) of warfarin maintenance dose in a hospital-based biobank enrolled patients across multiple ancestries, totaling $N = 120{,}000$ unrelated individuals after standard quality control and adjustment for $20$ ancestry principal components, age, sex, recruitment site, and genotyping batch. The study reports a Manhattan plot with numerous sub-genome-wide peaks dispersed across the genome and a Quantile-Quantile (QQ) plot showing early deviation from the null line. The genomic inflation factor (genomic control), $\\lambda_{\\mathrm{GC}}$, computed at the median test statistic, is $1.20$. Linkage Disequilibrium Score Regression (LDSC; Linkage Disequilibrium Score Regression) yields an intercept of $1.02$ (standard error $0.01$) and a positive slope.\n\nUsing the core definitions that, under the global null, single-nucleotide polymorphism (SNP)-level test statistics approximately follow a $\\chi^2$ distribution with $1$ degree of freedom and should align with the QQ-plot null line, that the genomic control factor $\\lambda_{\\mathrm{GC}}$ summarizes global inflation relative to the median of the null, and that the LDSC intercept estimates the confounding-induced component of inflation whereas the slope captures polygenic signal correlated with Linkage Disequilibrium (LD), infer the most likely primary source of the observed inflation and recommend the most appropriate corrective action for valid inference in the context of precision medicine and genomic diagnostics.\n\nChoose the best option.\n\nA. The inflation is predominantly due to residual population stratification. Apply classical genomic control by dividing all $\\chi^2$ statistics by $\\lambda_{\\mathrm{GC}} = 1.20$, then reassess associations at the standard threshold.\n\nB. The inflation is largely due to true polygenic signal given the near-null LDSC intercept. Do not apply wholesale genomic control using $\\lambda_{\\mathrm{GC}}$; at most, adjust by the LDSC intercept (divide $\\chi^2$ by $1.02$) to account for minimal confounding, prefer linear mixed models to further absorb structure, and report LDSC estimates alongside results.\n\nC. The inflation is primarily caused by cryptic relatedness. Remove all related individuals until $\\lambda_{\\mathrm{GC}}$ is approximately $1.00$ and repeat the association scan.\n\nD. The inflation arises from technical batch artifacts that escaped covariate adjustment. Correct by quantile-normalizing $p$-values to the null and lower the genome-wide threshold to $5 \\times 10^{-9}$ to offset residual bias.\n\nE. The inflation indicates systematic bias irrespective of source. Keep the current analysis but increase the genome-wide significance threshold from $5 \\times 10^{-8}$ to $5 \\times 10^{-9}$ to compensate for the elevated false positive rate.", "solution": "The problem requires interpreting a set of GWAS results to determine the primary cause of observed test statistic inflation and recommend the appropriate analytical strategy. The key is to correctly use both the genomic inflation factor ($\\lambda_{\\mathrm{GC}}$) and the Linkage Disequilibrium Score Regression (LDSC) intercept.\n\n1.  **Interpreting the Metrics:**\n    *   The genomic inflation factor, $\\lambda_{\\mathrm{GC}} = 1.20$, is significantly greater than the null expectation of $1.0$. This indicates systemic inflation of test statisticsâ€”the observed $p$-values are smaller across the board than expected by chance.\n    *   This inflation could be due to confounding (like population stratification, cryptic relatedness, or batch effects) or true, widespread polygenic signal (where many variants have real, small effects).\n    *   LD Score Regression is the standard tool to distinguish between these two sources. The LDSC intercept specifically quantifies inflation from confounding. Here, the intercept is $1.02$ (with a standard error of $0.01$), which is very close to the null value of $1.0$. This indicates that confounding bias has been well-controlled and is not the main source of inflation.\n    *   The positive LDSC slope, combined with the near-null intercept, means the observed inflation is almost entirely attributable to the true polygenic architecture of the trait, which is expected for a complex trait like warfarin dosing in a large study ($N=120,000$).\n\n2.  **Evaluating the Options:**\n    *   **A. Incorrect.** The LDSC intercept of $1.02$ refutes the claim that residual population stratification is the *predominant* cause. Applying classical genomic control by dividing all test statistics by $1.20$ is an outdated and overly harsh correction that would erase true polygenic signal and severely reduce statistical power.\n    *   **B. Correct.** This option correctly concludes that the inflation is due to true polygenicity, based on the near-null LDSC intercept. It recommends the current best practices: do not use the crude $\\lambda_{\\mathrm{GC}}$ for correction, as it conflates signal and bias. Instead, rely on methods like linear mixed models that properly account for genetic structure during the analysis, or at most, apply a minor correction using the LDSC intercept ($1.02$) if working with summary statistics. Reporting all LDSC metrics is also essential for transparency.\n    *   **C. Incorrect.** Cryptic relatedness, a form of confounding, would inflate the LDSC intercept. Since the intercept is near $1.0$, this is not the primary issue. Removing individuals until $\\lambda_{\\mathrm{GC}}=1.0$ would wrongly remove true signal.\n    *   **D. Incorrect.** Uncorrected batch effects would also inflate the LDSC intercept. The proposed correction (\"quantile-normalizing $p$-values\") is not a valid statistical procedure for this context and would destroy the results.\n    *   **E. Incorrect.** This approach fails to use the LDSC results to understand the source of inflation. Simply making the significance threshold more stringent is a crude way to reduce hits but does not fix the inflated test statistics, which should be correctly modeled.\n\nTherefore, the evidence points overwhelmingly to true polygenicity as the source of inflation, making option B the only appropriate interpretation and course of action.", "answer": "$$\\boxed{B}$$", "id": "4353250"}]}