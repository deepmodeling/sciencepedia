## Introduction
Haplotype phasing, the process of assigning alleles to their specific parental chromosome, is a foundational task in modern genomics. While standard sequencing identifies an individual's genetic variants, it often leaves their chromosomal arrangement unresolved. This ambiguity presents a significant knowledge gap, as the clinical interpretation of genetic data—for instance, distinguishing a benign carrier state from a disease-causing compound heterozygous state—depends entirely on knowing whether variants are on the same or opposite chromosomes. This article provides a comprehensive guide to navigating this challenge. First, in "Principles and Mechanisms," we will dissect the theoretical basis of phasing, the metrics for evaluating accuracy, and the core computational strategies that leverage either physical linkage or statistical inference. Following this, "Applications and Interdisciplinary Connections" will demonstrate how these techniques are applied in [critical fields](@entry_id:272263) such as pharmacogenomics, Mendelian disease diagnostics, and [cancer genomics](@entry_id:143632), turning raw data into actionable clinical insights. Finally, "Hands-On Practices" will offer an opportunity to apply these concepts by tackling practical problems in the evaluation of phasing accuracy and contiguity.

## Principles and Mechanisms

Haplotype phasing is the computational and statistical process of resolving the parental origin of alleles at heterozygous sites across a chromosome. While standard sequencing determines the diploid genotype at each variant site (e.g., an individual is A/G), it does not, by default, reveal which alleles are co-located on the same parental chromosome. The process of phasing reconstructs these chromosomal sequences, known as haplotypes. This chapter elucidates the fundamental principles that define the phasing problem, the metrics used to evaluate its success, and the core mechanisms—leveraging either physical linkage or [statistical inference](@entry_id:172747)—that enable haplotype reconstruction.

### The Formal Basis and Clinical Imperative of Phasing

At its core, the phasing problem is one of resolving ambiguity. For a diploid organism, a **diplotype** consists of two homologous chromosomes. If we consider a set of $m$ heterozygous variant sites, $S = \{p_1, \dots, p_m\}$, a single **haplotype** can be represented as a sequence that specifies the allele at each site. For simplicity, we can use a binary alphabet, $\mathcal{A} = \{0, 1\}$, to denote the reference and alternate alleles, respectively. A single haplotype is thus a function $h: S \to \{0, 1\}$.

Since all sites in $S$ are heterozygous, the two [haplotypes](@entry_id:177949), which we can call $h_1$ and $h_2$, must carry [complementary alleles](@entry_id:197012) at every site. This biological constraint is formally expressed as $h_1(p) + h_2(p) = 1$ for all sites $p \in S$. Consequently, if one haplotype is $h$, the other must be its bitwise complement, $\bar{h}$. The challenge of phasing arises because the underlying biological state is the *unordered* set of [haplotypes](@entry_id:177949) $\{h_1, h_2\}$. However, computational representations often use an *ordered* pair $(h_1, h_2)$. Swapping the labels of the two homologous chromosomes results in the [ordered pair](@entry_id:148349) $(h_2, h_1)$, but this represents the same biological reality. This means that the ordered pairs $(h_1, h_2)$ and $(h_2, h_1)$ belong to the same [equivalence class](@entry_id:140585). Phasing can be understood as the process of determining the correct pairing of alleles across sites to reconstruct a single representative, such as $(h_1, h_2)$, from this equivalence class [@problem_id:4388682].

The clinical importance of resolving this ambiguity is profound, particularly in the context of autosomal recessive disorders. Such disorders often arise from biallelic loss of function, meaning both copies of a gene are inactivated. Consider a scenario where sequencing identifies two different [pathogenic variants](@entry_id:177247), $v_1$ and $v_2$, in the same gene. Phasing is essential to determine their configuration [@problem_id:4388603]:

*   **Trans configuration**: The variants are on opposite [homologous chromosomes](@entry_id:145316). One chromosome carries $v_1$ and the wild-type allele for the second locus, while the other chromosome carries the [wild-type allele](@entry_id:162987) for the first locus and $v_2$. In this case, no fully functional copy of the gene can be produced, leading to biallelic inactivation. This state is known as **compound [heterozygosity](@entry_id:166208)** and can explain the disease phenotype.

*   **Cis configuration**: Both variants are on the same chromosome. The other homologous chromosome is wild-type at both positions. This individual can still produce functional protein from the wild-type chromosome and is typically an unaffected carrier, not a patient.

Without phasing, these two clinically distinct scenarios are indistinguishable, as both present as a simple heterozygous genotype at two sites. Phasing is therefore a critical step in translating genomic data into an accurate clinical diagnosis.

### Evaluating Phasing Accuracy

To compare and improve phasing methods, we need rigorous metrics to quantify their accuracy against a known "ground truth" (e.g., from trio-based inheritance or [long-read sequencing](@entry_id:268696)). Phasing algorithms often cannot reconstruct a single, chromosome-length haplotype. Instead, they produce a series of discontiguous **phase blocks**, which are segments of the chromosome within which the relative phase is determined.

The two primary metrics for phasing accuracy are the switch error rate and the Hamming error rate [@problem_id:5067213] [@problem_id:4388633]:

*   **Switch Error**: This metric evaluates the local, relative correctness of phasing. A switch error occurs at the junction between two adjacent heterozygous sites, $i$ and $i+1$, *within the same phase block* if their inferred phase relationship is incorrect. Formally, if $\mathbf{t}$ is the vector of true binary haplotype labels and $\mathbf{p}$ is the vector of predicted labels, a switch error occurs if the parity of the prediction differs from the parity of the truth: $p_i \oplus p_{i+1} \neq t_i \oplus t_{i+1}$, where $\oplus$ denotes the [exclusive-or](@entry_id:172120) operation. The **Switch Error Rate (SER)** is the total number of such switch errors divided by the total number of opportunities—that is, the number of adjacent site pairs within all predicted blocks. For a set of $K$ blocks with a total of $M$ sites, the denominator is $M-K$.

*   **Hamming Error**: This metric assesses the per-site accuracy. A naive comparison of predicted labels $\mathbf{p}$ to true labels $\mathbf{t}$ is insufficient because the overall orientation of a phase block (e.g., which haplotype is labeled '0' versus '1') is arbitrary. To account for this, the Hamming error for a given block is calculated after optimally "flipping" the predicted block's labels to best match the truth. The error for a block is the minimum of mismatches between the prediction and the truth, and the flipped prediction and the truth. The total Hamming error is the sum of these minimums over all blocks. The **Hamming Error Rate (HER)** is the total Hamming error divided by the total number of phased sites, $M$. This metric effectively ignores **end-switches** (orientation errors at the boundaries of blocks) by absorbing them into the optimal block orientation.

### Read-Based Phasing: Exploiting Physical Linkage

Read-based phasing methods leverage the most direct evidence available: the physical co-occurrence of alleles on the same DNA molecule. When a single DNA molecule is sequenced, any heterozygous sites covered by that same sequencing read are physically linked, and their phase relationship is directly observed.

The efficacy of this approach is fundamentally limited by sequencing technology. Standard short-read sequencing (e.g., Illumina) generates reads or read-pairs that span a few hundred base pairs. As illustrated in a common clinical scenario, if two variants are separated by a distance greater than the sequencer's insert size (e.g., 600 bp apart with a ~350 bp insert size), no reads will span both sites, making read-based phasing with that technology impossible [@problem_id:4388603]. This technological constraint motivates the use of long-read (e.g., PacBio, Oxford Nanopore) or linked-read (e.g., 10x Genomics) technologies, which can generate linkage information over tens to hundreds of kilobases.

We can formalize the concept of connectivity using a graph model, where heterozygous sites are nodes and an edge exists between two nodes if at least one sequencing molecule spans both sites. A phase block corresponds to a connected component in this graph. The distinction between **local phase** (the relative phase within a block) and **chromosome-scale phase** (the consistent phasing across all blocks on a chromosome) becomes clear: read-based methods excel at establishing local phase, but gaps in connectivity lead to multiple, disconnected blocks whose relative orientation is unknown [@problem_id:4388669].

The size of these blocks depends on sequencing parameters. Under a simplified Poisson model, the probability that a gap in connectivity (a block boundary) occurs between two adjacent sites separated by distance $d$ is $P(\text{boundary}) = \exp(-\lambda(d))$, where $\lambda(d)$ is the expected number of molecules spanning the two sites. This expectation $\lambda(d)$ is proportional to both sequencing depth ($D$) and the effective span of the molecules ($S$). For instance, for reads of length $l$ covering sites a distance $d  l$ apart, $\lambda(d)$ can be modeled as $D \times \frac{l-d}{l}$ [@problem_id:4388669]. This relationship quantitatively demonstrates that increasing [sequencing depth](@entry_id:178191) and, more critically, increasing read or molecule length, directly reduces the probability of block boundaries and thus increases phase block size.

#### Algorithmic Approaches to Read-Based Phasing

Once read evidence is collected, algorithms are needed to assemble the most likely haplotypes. Two major classes of algorithms are probabilistic and combinatorial approaches.

**Probabilistic frameworks** aim to compute the likelihood of a given phase hypothesis based on the observed reads, while accounting for sequencing errors. For example, consider two adjacent heterozygous SNPs and a set of reads that span both. We can derive a [likelihood ratio](@entry_id:170863), $\Lambda$, to compare the cis-phase hypothesis ($H_{\text{cis}}$) to the trans-phase hypothesis ($H_{\text{trans}}$). Given the base quality score $Q$, which implies a sequencing error probability of $p = 10^{-Q/10}$, and the counts of reads supporting the different allele combinations, the likelihood ratio can be derived [@problem_id:4388666]. If we define $n_C$ as the number of reads concordant with the cis hypothesis (e.g., reads showing $AB$ or $ab$) and $n_D$ as the number of discordant reads (e.g., $Ab$ or $aB$), the likelihood ratio simplifies to:
$$ \Lambda = \frac{L(H_{\mathrm{cis}} \mid \text{data})}{L(H_{\mathrm{trans}} \mid \text{data})} = \left( \frac{(1-p)^2 + p^2}{2p(1-p)} \right)^{n_C - n_D} $$
This powerful result shows that the evidence for or against a phase configuration depends on the balance of concordant versus discordant reads, weighted by a factor that reflects the sequencing error rate.

**Combinatorial frameworks** reframe phasing as an optimization problem. A prominent example is the **Minimum Error Correction (MEC)** model. This approach seeks to find a pair of haplotypes that is most consistent with the entire set of reads by minimizing the total number of single-base "corrections" required to make every read match one of the two proposed [haplotypes](@entry_id:177949) perfectly. For a candidate haplotype $H$ and its complement $H^c$, the total error is the sum over all reads of the minimum number of mismatches between the read and either $H$ or $H^c$. The MEC score is the global minimum of this total error over all possible [haplotypes](@entry_id:177949) $H$ [@problem_id:4388651]. While computationally intensive (NP-hard in the general case), this model provides a clear, parsimonious objective for haplotype assembly from conflicting read data.

### Statistical and Genetic Phasing: Inference Without Direct Linkage

When physical linkage information is sparse or absent, haplotypes can be inferred using principles of [genetic inheritance](@entry_id:262521) or statistical patterns from population data.

#### Trio-Based Phasing

The most powerful form of genetic phasing uses data from a family **trio** (two parents and a child). The laws of Mendelian inheritance provide nearly deterministic constraints on the child's haplotypes. By observing the alleles transmitted from parent to child, one can often directly reconstruct the child's phased genome. For example, if a child has a heterozygous genotype A/G at a site where the mother is A/A, the child must have inherited the G allele from the father. By tracing [inheritance patterns](@entry_id:137802) across many sites, long, accurate haplotypes can be constructed. Even in complex cases, such as when both parents and the child are heterozygous at two loci, Mendelian laws dramatically reduce the number of possible phasing configurations, providing substantial information [@problem_id:4388594].

#### Population-Based Statistical Phasing

In the absence of family data, phasing can be performed by leveraging statistical patterns in large, unrelated populations. The foundational principle is **Linkage Disequilibrium (LD)**, the non-random association of alleles at different loci on a chromosome. If loci were independent, the frequency of a haplotype (e.g., $AB$) would simply be the product of the corresponding allele frequencies ($p_A p_B$). LD is the deviation from this expectation. The LD coefficient, $D$, is defined as $D = f_{AB} - p_A p_B$, where $f_{AB}$ is the observed haplotype frequency [@problem_id:4388596].

A non-zero $D$ value implies that certain haplotypes are more or less common in the population than expected by chance. This information can be used to infer the most likely phase for a new individual. For an individual with the unphased genotype $A/a, B/b$, there are two possible phase configurations: $AB/ab$ (cis) or $Ab/aB$ (trans). The relative likelihood of these two states is proportional to the frequencies of the corresponding haplotype pairs in the population. It can be shown that $D = f_{AB}f_{ab} - f_{Ab}f_{aB}$. Therefore, if $D$ is positive, the cis configuration ($AB/ab$) is more probable, and if $D$ is negative, the trans configuration ($Ab/aB$) is more probable [@problem_id:4388596].

This principle is operationalized in **reference-based phasing**. Modern phasing algorithms use a large **reference panel** of previously phased [haplotypes](@entry_id:177949) (e.g., from the 1000 Genomes Project or the Haplotype Reference Consortium). They are often based on models, such as the Li-Stephens model, which posit that any given haplotype in the population can be modeled as a "mosaic" of segments from the [haplotypes](@entry_id:177949) in the reference panel [@problem_id:4388653]. The algorithm finds the pair of panel haplotypes that best explains the target individual's observed genotypes.

The accuracy of this approach is highly dependent on the size and diversity of the reference panel. In a simplified model, if a target individual's true [haplotypes](@entry_id:177949) are $h_1$ and $h_2$ (with population frequencies $p$ and $q$), phasing accuracy is perfect if both $h_1$ and $h_2$ are present in a reference panel of size $n$. If one or both are missing, accuracy degrades to that of a random guess. The expected accuracy, $A(n)$, can be modeled as:
$$ A(n) = \frac{1}{2} + \frac{1}{2} \left[ 1 - (1-p)^n - (1-q)^n + (1-p-q)^n \right] $$
This equation shows that as the panel size $n$ increases, the probability of missing one of the required haplotypes, $(1-p)^n$ or $(1-q)^n$, decreases exponentially. The overall accuracy $A(n)$ thus rapidly approaches 1 for large $n$. This quantitative relationship underscores the immense power of large-scale population sequencing projects for enabling high-quality statistical phasing across entire genomes.