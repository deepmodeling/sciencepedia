{"hands_on_practices": [{"introduction": "A primary challenge in genomic medicine is ensuring that predictive models, such as Polygenic Risk Scores (PRS), perform equitably across diverse populations. This exercise delves into the theoretical underpinnings of this issue by modeling how a PRS's predictive accuracy degrades when transferred from a discovery cohort to a target cohort. By working through a standard random-effects model, you will derive a precise mathematical relationship between the drop in performance and the genetic correlation, $\\rho$, between the two populations [@problem_id:4338602].", "problem": "In the context of cross-population application of genomic prediction in precision medicine, consider a Polygenic Risk Score (PRS) trained in a discovery population and deployed in a target population with a different ancestry profile. To isolate the contribution of cross-population genetic similarity to performance, assume the following scientifically standard random-effects framework:\n\n1. Work in an orthonormal predictor basis obtained by pre-whitening the genotype matrix so that predictors are approximately uncorrelated and standardized with unit variance. Let there be $p$ such predictors. For any individual, the genetic value is the linear combination of standardized genotypes with true effect sizes.\n\n2. Let the true effect sizes in the discovery population be $\\mathbf{b}_{D} \\in \\mathbb{R}^{p}$, modeled as independent and identically distributed random effects with $\\mathbf{b}_{D} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_{b}^{2} \\mathbf{I}_{p})$.\n\n3. Let the true effect sizes in the target population be $\\mathbf{b}_{T} \\in \\mathbb{R}^{p}$, jointly distributed with $\\mathbf{b}_{D}$ so that the per-coordinate genetic correlation between $\\mathbf{b}_{T}$ and $\\mathbf{b}_{D}$ is $\\rho \\in [0,1]$. A convenient representation is $\\mathbf{b}_{T} = \\rho \\mathbf{b}_{D} + \\sqrt{1 - \\rho^{2}}\\,\\mathbf{u}$ where $\\mathbf{u} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_{b}^{2} \\mathbf{I}_{p})$ is independent of $\\mathbf{b}_{D}$.\n\n4. The learned PRS uses estimated discovery effects $\\hat{\\mathbf{b}}_{D}$ obtained from a Genome-Wide Association Study (GWAS) in the discovery population. Model estimation error as isotropic and independent of the true effects: $\\hat{\\mathbf{b}}_{D} = \\mathbf{b}_{D} + \\boldsymbol{\\eta}$ with $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_{\\eta}^{2} \\mathbf{I}_{p})$, independent of $\\mathbf{b}_{D}$ and $\\mathbf{u}$.\n\n5. For a given population, define the population-level $R^{2}$ of the PRS as the squared correlation across individuals between the PRS score $S$ and the true genetic value $G$. In the orthonormal basis and large-$p$ limit, this $R^{2}$ reduces to the squared correlation between the coefficient vectors entering $S$ and $G$.\n\nUnder these assumptions, derive the multiplicative degradation factor $D(\\rho)$, defined as the ratio of the expected $R^{2}$ of the PRS in the target population to the expected $R^{2}$ of the PRS in the discovery population. Express your final answer as a closed-form analytic expression in terms of $\\rho$.", "solution": "The user wants me to solve the following problem.\n\nIn the context of cross-population application of genomic prediction in precision medicine, consider a Polygenic Risk Score (PRS) trained in a discovery population and deployed in a target population with a different ancestry profile. To isolate the contribution of cross-population genetic similarity to performance, assume the following scientifically standard random-effects framework:\n\n1. Work in an orthonormal predictor basis obtained by pre-whitening the genotype matrix so that predictors are approximately uncorrelated and standardized with unit variance. Let there be $p$ such predictors. For any individual, the genetic value is the linear combination of standardized genotypes with true effect sizes.\n\n2. Let the true effect sizes in the discovery population be $\\mathbf{b}_{D} \\in \\mathbb{R}^{p}$, modeled as independent and identically distributed random effects with $\\mathbf{b}_{D} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_{b}^{2} \\mathbf{I}_{p})$.\n\n3. Let the true effect sizes in the target population be $\\mathbf{b}_{T} \\in \\mathbb{R}^{p}$, jointly distributed with $\\mathbf{b}_{D}$ so that the per-coordinate genetic correlation between $\\mathbf{b}_{T}$ and $\\mathbf{b}_{D}$ is $\\rho \\in [0,1]$. A convenient representation is $\\mathbf{b}_{T} = \\rho \\mathbf{b}_{D} + \\sqrt{1 - \\rho^{2}}\\,\\mathbf{u}$ where $\\mathbf{u} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_{b}^{2} \\mathbf{I}_{p})$ is independent of $\\mathbf{b}_{D}$.\n\n4. The learned PRS uses estimated discovery effects $\\hat{\\mathbf{b}}_{D}$ obtained from a Genome-Wide Association Study (GWAS) in the discovery population. Model estimation error as isotropic and independent of the true effects: $\\hat{\\mathbf{b}}_{D} = \\mathbf{b}_{D} + \\boldsymbol{\\eta}$ with $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_{\\eta}^{2} \\mathbf{I}_{p})$, independent of $\\mathbf{b}_{D}$ and $\\mathbf{u}$.\n\n5. For a given population, define the population-level $R^{2}$ of the PRS as the squared correlation across individuals between the PRS score $S$ and the true genetic value $G$. In the orthonormal basis and large-$p$ limit, this $R^{2}$ reduces to the squared correlation between the coefficient vectors entering $S$ and $G$.\n\nUnder these assumptions, derive the multiplicative degradation factor $D(\\rho)$, defined as the ratio of the expected $R^{2}$ of the PRS in the target population to the expected $R^{2}$ of the PRS in the discovery population. Express your final answer as a closed-form analytic expression in terms of $\\rho$. No numerical approximation is required, and no units are applicable.\n\n### Step 1: Extract Givens\n- Framework: Random-effects model for cross-population PRS prediction.\n- Basis: Orthonormal predictors, $p$ total predictors.\n- Discovery population true effects: $\\mathbf{b}_{D} \\in \\mathbb{R}^{p}$, with components $b_{D,i} \\sim \\mathcal{N}(0, \\sigma_{b}^{2})$ i.i.d.\n- Target population true effects: $\\mathbf{b}_{T} \\in \\mathbb{R}^{p}$, related to $\\mathbf{b}_{D}$ by $\\mathbf{b}_{T} = \\rho \\mathbf{b}_{D} + \\sqrt{1 - \\rho^{2}}\\,\\mathbf{u}$.\n- Genetic correlation: $\\rho \\in [0,1]$.\n- Auxiliary random vector: $\\mathbf{u} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_{b}^{2} \\mathbf{I}_{p})$, independent of $\\mathbf{b}_{D}$.\n- Estimated discovery effects: $\\hat{\\mathbf{b}}_{D} = \\mathbf{b}_{D} + \\boldsymbol{\\eta}$.\n- Estimation error: $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_{\\eta}^{2} \\mathbf{I}_{p})$, independent of $\\mathbf{b}_{D}$ and $\\mathbf{u}$.\n- Definition of prediction accuracy: $R^{2}$ is the squared correlation between the coefficient vectors for the PRS score and the true genetic value, in the large-$p$ limit.\n- Quantity to derive: Degradation factor $D(\\rho) = \\frac{\\mathbb{E}[R^{2}_{\\text{target}}]}{\\mathbb{E}[R^{2}_{\\text{discovery}}]}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded, well-posed, and objective.\n1.  **Scientific Grounding**: The model described is a standard and widely used simplified theoretical framework in statistical genetics to study the portability of polygenic risk scores across populations with different ancestries. The random effects model for genetic effects, the use of genetic correlation ($\\rho$), and the model for GWAS estimation error are all well-established concepts.\n2.  **Well-Posedness**: The problem is mathematically well-defined. All random variables and their distributions are specified. The quantity to be derived, $R^2$, is given a precise definition in the large-$p$ limit, which makes the problem tractable and ensures a unique solution exists.\n3.  **Completeness and Consistency**: The problem provides all necessary definitions and statistical properties of the variables involved. There are no contradictions in the setup. The model conserves total genetic variance between populations, which is a consistent and sensible property.\n4.  **Realism**: While a simplification of complex biological reality (e.g., assuming i.i.d. effects and isotropic error), the model is a scientifically plausible and useful abstraction for theoretical investigation.\n\nThe problem does not exhibit any of the flaws listed in the instructions. It is a valid, non-trivial problem in theoretical population genetics.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the derivation.\n\nThe core of the problem lies in calculating the squared correlation between different vectors of coefficients. The problem states that in the large-$p$ limit, the population-level $R^2$ is the squared correlation between the relevant coefficient vectors. For two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^{p}$ with i.i.d. components, the squared correlation is given by:\n$$\n\\text{Corr}(\\mathbf{x}, \\mathbf{y})^2 = \\frac{(\\mathbf{x}^\\top \\mathbf{y})^2}{(\\mathbf{x}^\\top \\mathbf{x})(\\mathbf{y}^\\top \\mathbf{y})} = \\frac{(\\frac{1}{p}\\sum_{i=1}^{p} x_i y_i)^2}{(\\frac{1}{p}\\sum_{i=1}^{p} x_i^2)(\\frac{1}{p}\\sum_{i=1}^{p} y_i^2)}\n$$\nBy the Law of Large Numbers, as $p \\to \\infty$, the sample averages converge to their expectations: $\\frac{1}{p}\\sum z_i \\to \\mathbb{E}[z]$. Since the vectors' components are drawn from distributions with mean $0$, we have $\\mathbb{E}[x_i y_i] = \\text{Cov}(x_i, y_i)$ and $\\mathbb{E}[x_i^2] = \\text{Var}(x_i)$. Therefore, in the large-$p$ limit, the squared correlation becomes a deterministic quantity:\n$$\nR^2 = \\frac{(\\mathbb{E}[x_i y_i])^2}{\\mathbb{E}[x_i^2] \\mathbb{E}[y_i^2]} = \\frac{(\\text{Cov}(x_i, y_i))^2}{\\text{Var}(x_i)\\text{Var}(y_i)}\n$$\nBecause this quantity is deterministic in the limit, the expectation $\\mathbb{E}[R^2]$ is simply the quantity itself. We apply this formula to both the discovery and target populations.\n\nFirst, we calculate the expected $R^2$ in the discovery population, $R^{2}_{\\text{discovery}}$. The PRS is constructed using the estimated effects $\\hat{\\mathbf{b}}_{D}$, and the true genetic value is determined by the true effects $\\mathbf{b}_{D}$. Thus, we need the squared correlation between the vectors $\\hat{\\mathbf{b}}_{D}$ and $\\mathbf{b}_{D}$. Let's consider their $i$-th components, $\\hat{b}_{D,i}$ and $b_{D,i}$.\nThe required variances and covariance are:\n1.  $\\text{Var}(b_{D,i}) = \\sigma_{b}^{2}$, by definition.\n2.  $\\text{Var}(\\hat{b}_{D,i}) = \\text{Var}(b_{D,i} + \\eta_i)$. Since $b_{D,i}$ and $\\eta_i$ are independent, $\\text{Var}(b_{D,i} + \\eta_i) = \\text{Var}(b_{D,i}) + \\text{Var}(\\eta_i) = \\sigma_{b}^{2} + \\sigma_{\\eta}^{2}$.\n3.  $\\text{Cov}(\\hat{b}_{D,i}, b_{D,i}) = \\text{Cov}(b_{D,i} + \\eta_i, b_{D,i}) = \\text{Cov}(b_{D,i}, b_{D,i}) + \\text{Cov}(\\eta_i, b_{D,i})$. Since $\\eta_i$ and $b_{D,i}$ are independent, their covariance is $0$. Thus, $\\text{Cov}(\\hat{b}_{D,i}, b_{D,i}) = \\text{Var}(b_{D,i}) = \\sigma_{b}^{2}$.\n\nSubstituting these into the formula for $R^2$:\n$$\nR^{2}_{\\text{discovery}} = \\frac{(\\text{Cov}(\\hat{b}_{D,i}, b_{D,i}))^2}{\\text{Var}(\\hat{b}_{D,i})\\text{Var}(b_{D,i})} = \\frac{(\\sigma_{b}^{2})^2}{(\\sigma_{b}^{2} + \\sigma_{\\eta}^{2})(\\sigma_{b}^{2})} = \\frac{\\sigma_{b}^{2}}{\\sigma_{b}^{2} + \\sigma_{\\eta}^{2}}\n$$\n\nNext, we calculate the expected $R^2$ in the target population, $R^{2}_{\\text{target}}$. The PRS is still constructed using the same estimated effects from the discovery population, $\\hat{\\mathbf{b}}_{D}$, but it is now applied to predict the true genetic value in the target population, which is determined by the true effects $\\mathbf{b}_{T}$. Thus, we need the squared correlation between the vectors $\\hat{\\mathbf{b}}_{D}$ and $\\mathbf{b}_{T}$. We analyze their $i$-th components, $\\hat{b}_{D,i}$ and $b_{T,i}$.\nThe required variances and covariance are:\n1.  $\\text{Var}(\\hat{b}_{D,i}) = \\sigma_{b}^{2} + \\sigma_{\\eta}^{2}$, as calculated before.\n2.  $\\text{Var}(b_{T,i}) = \\text{Var}(\\rho b_{D,i} + \\sqrt{1 - \\rho^{2}} u_i)$. Since $b_{D,i}$ and $u_i$ are independent, the variance of the sum is the sum of variances:\n    $\\text{Var}(b_{T,i}) = \\rho^2 \\text{Var}(b_{D,i}) + (1 - \\rho^2) \\text{Var}(u_i)$.\n    Given $\\text{Var}(b_{D,i}) = \\sigma_{b}^{2}$ and $\\text{Var}(u_i) = \\sigma_{b}^{2}$, we have:\n    $\\text{Var}(b_{T,i}) = \\rho^2 \\sigma_{b}^{2} + (1 - \\rho^2) \\sigma_{b}^{2} = (\\rho^2 + 1 - \\rho^2)\\sigma_{b}^{2} = \\sigma_{b}^{2}$.\n    This confirms that the total genetic variance per marker is conserved across populations.\n3.  $\\text{Cov}(\\hat{b}_{D,i}, b_{T,i}) = \\text{Cov}(b_{D,i} + \\eta_i, \\rho b_{D,i} + \\sqrt{1 - \\rho^{2}} u_i)$.\n    Using the bilinearity of covariance and the independence of $\\mathbf{b}_{D}$, $\\mathbf{u}$, and $\\boldsymbol{\\eta}$:\n    $\\text{Cov}(\\hat{b}_{D,i}, b_{T,i}) = \\text{Cov}(b_{D,i}, \\rho b_{D,i}) + \\text{Cov}(b_{D,i}, \\sqrt{1 - \\rho^{2}} u_i) + \\text{Cov}(\\eta_i, \\rho b_{D,i}) + \\text{Cov}(\\eta_i, \\sqrt{1 - \\rho^{2}} u_i)$.\n    The last three terms are zero due to independence of the variables. The first term is:\n    $\\text{Cov}(b_{D,i}, \\rho b_{D,i}) = \\rho \\text{Cov}(b_{D,i}, b_{D,i}) = \\rho \\text{Var}(b_{D,i}) = \\rho \\sigma_{b}^{2}$.\n\nSubstituting these into the formula for $R^2$:\n$$\nR^{2}_{\\text{target}} = \\frac{(\\text{Cov}(\\hat{b}_{D,i}, b_{T,i}))^2}{\\text{Var}(\\hat{b}_{D,i})\\text{Var}(b_{T,i})} = \\frac{(\\rho \\sigma_{b}^{2})^2}{(\\sigma_{b}^{2} + \\sigma_{\\eta}^{2})(\\sigma_{b}^{2})} = \\rho^2 \\frac{\\sigma_{b}^{4}}{(\\sigma_{b}^{2} + \\sigma_{\\eta}^{2})\\sigma_{b}^{2}} = \\rho^2 \\frac{\\sigma_{b}^{2}}{\\sigma_{b}^{2} + \\sigma_{\\eta}^{2}}\n$$\n\nFinally, we compute the multiplicative degradation factor $D(\\rho)$, which is the ratio of the expected $R^2$ values.\n$$\nD(\\rho) = \\frac{\\mathbb{E}[R^{2}_{\\text{target}}]}{\\mathbb{E}[R^{2}_{\\text{discovery}}]} = \\frac{R^{2}_{\\text{target}}}{R^{2}_{\\text{discovery}}} = \\frac{\\rho^2 \\frac{\\sigma_{b}^{2}}{\\sigma_{b}^{2} + \\sigma_{\\eta}^{2}}}{\\frac{\\sigma_{b}^{2}}{\\sigma_{b}^{2} + \\sigma_{\\eta}^{2}}}\n$$\nThe term $\\frac{\\sigma_{b}^{2}}{\\sigma_{b}^{2} + \\sigma_{\\eta}^{2}}$, which represents the prediction accuracy within the discovery population, cancels out. This leaves:\n$$\nD(\\rho) = \\rho^2\n$$\nThe result indicates that the PRS accuracy in the target population is the accuracy in the discovery population multiplied by the squared genetic correlation between the two populations. This degradation is independent of the GWAS sample size or heritability, as these factors are embedded in the $R^{2}_{\\text{discovery}}$ term which cancels.", "answer": "$$\n\\boxed{\\rho^{2}}\n$$", "id": "4338602"}, {"introduction": "While understanding the sources of bias is crucial, quantifying its impact requires a clear set of metrics. This hands-on coding practice challenges you to implement the mathematical definitions of key fairness criteria, such as the True Positive Rate ($\\mathrm{TPR}$) and False Positive Rate ($\\mathrm{FPR}$), which form the basis of Equalized Odds. You will construct a scenario where a model is perfectly calibrated—a statistically desirable property—yet still produces unequal error rates across groups, providing a concrete demonstration of the complex trade-offs inherent in algorithmic fairness [@problem_id:4338581].", "problem": "Consider two population groups $G \\in \\{A,B\\}$ and a binary clinical outcome $Y \\in \\{0,1\\}$ indicating presence ($Y=1$) or absence ($Y=0$) of a disease. For each group $g \\in \\{A,B\\}$, suppose an underlying continuous biomarker $X \\in \\mathbb{R}$ is measured. The biomarker follows group- and outcome-specific Gaussian class-conditional distributions that are scientifically plausible in genomic risk modeling: for fixed $g$,\n$$X \\mid (Y=1,G=g) \\sim \\mathcal{N}(\\mu_{g1},\\sigma), \\quad X \\mid (Y=0,G=g) \\sim \\mathcal{N}(\\mu_{g0},\\sigma),$$\nwith a common standard deviation $\\sigma  0$. Also define the group-specific disease prevalence (base rate) $\\pi_g = \\mathbb{P}(Y=1 \\mid G=g)$ with $0  \\pi_g  1$.\n\nDefine the risk score used by the genomic prediction model to be the Bayes posterior probability within groups:\n$$S_g(x) \\triangleq \\mathbb{P}(Y=1 \\mid X=x, G=g) = \\frac{\\pi_g f_{g1}(x)}{\\pi_g f_{g1}(x) + (1-\\pi_g) f_{g0}(x)},$$\nwhere $f_{g1}$ and $f_{g0}$ are the Gaussian densities of $X \\mid (Y=1,G=g)$ and $X \\mid (Y=0,G=g)$, respectively. A classifier is constructed by applying a single decision threshold $t \\in [0,1]$ to the score: predict $\\hat{Y}=1$ if and only if $S_g(X) \\ge t$.\n\nYou must compute, from first principles, the following fairness metrics for each test case:\n- Within-group calibration error, defined as the expected calibration error (ECE) of the score,\n$$\\mathrm{ECE}_g \\triangleq \\mathbb{E}\\left[ \\left| \\mathbb{P}(Y=1 \\mid S_g) - S_g \\right| \\,\\middle|\\, G=g \\right],$$\nexpressed as a decimal (not a percentage).\n- True Positive Rate (TPR) and False Positive Rate (FPR) for each group at threshold $t$,\n$$\\mathrm{TPR}_g(t) \\triangleq \\mathbb{P}(\\hat{Y}=1 \\mid Y=1, G=g), \\quad \\mathrm{FPR}_g(t) \\triangleq \\mathbb{P}(\\hat{Y}=1 \\mid Y=0, G=g),$$\nexpressed as decimals.\n- Equalized Odds gap at threshold $t$,\n$$\\Delta_{\\mathrm{EO}}(t) \\triangleq \\max\\left( \\left| \\mathrm{TPR}_A(t) - \\mathrm{TPR}_B(t) \\right| , \\left| \\mathrm{FPR}_A(t) - \\mathrm{FPR}_B(t) \\right| \\right),$$\nexpressed as a decimal.\n- Demographic Parity difference at threshold $t$,\n$$\\Delta_{\\mathrm{DP}}(t) \\triangleq \\left| \\mathbb{P}(\\hat{Y}=1 \\mid G=A) - \\mathbb{P}(\\hat{Y}=1 \\mid G=B) \\right|,$$\nwhere\n$$\\mathbb{P}(\\hat{Y}=1 \\mid G=g) = \\pi_g \\,\\mathrm{TPR}_g(t) + (1-\\pi_g) \\,\\mathrm{FPR}_g(t),$$\nexpressed as a decimal.\n\nStarting from the fundamental base of Bayes' theorem and the Gaussian density, derive an explicit, fully specified algorithm to compute $\\mathrm{TPR}_g(t)$ and $\\mathrm{FPR}_g(t)$ analytically (without simulation). Use the fact that with equal variances across classes, the log-likelihood ratio $\\log \\left( f_{g1}(x) / f_{g0}(x) \\right)$ is linear in $x$, making the posterior $S_g(x)$ a monotone function of $x$ and reducing the thresholding event $S_g(X) \\ge t$ to a single-sided threshold on $X$.\n\nScientific realism constraints:\n- Use $\\sigma$ and $\\mu$ values that produce overlapping but separable Gaussian class-conditional distributions, ensuring clinically plausible discrimination.\n- Use base rates $\\pi_g$ strictly between $0$ and $1$.\n\nYour program must implement the derived formulas and compute the metrics for the following test suite, each specified by the tuple of parameters $((\\mu_{A1},\\mu_{A0},\\sigma,\\pi_A),(\\mu_{B1},\\mu_{B0},\\sigma,\\pi_B),t)$:\n1. Case $1$ (general, different base rates, equal separation): $((2.0,0.0,1.0,0.1),(2.0,0.0,1.0,0.4),0.5)$.\n2. Case $2$ (boundary, always predict positive): $((2.0,0.0,1.0,0.1),(2.0,0.0,1.0,0.4),0.0)$.\n3. Case $3$ (same base rates, different separation): $((1.0,0.0,1.0,0.3),(2.0,0.0,1.0,0.3),0.5)$.\n4. Case $4$ (boundary, never predict positive): $((2.0,0.0,1.0,0.1),(2.0,0.0,1.0,0.4),1.0)$.\n\nOutput specification:\n- For each test case, output a list of $8$ decimals in the fixed order $[\\mathrm{ECE}_A,\\mathrm{ECE}_B,\\mathrm{TPR}_A,\\mathrm{FPR}_A,\\mathrm{TPR}_B,\\mathrm{FPR}_B,\\Delta_{\\mathrm{EO}},\\Delta_{\\mathrm{DP}}]$.\n- Aggregate the four per-case lists into a single line as a comma-separated list enclosed in square brackets, with no spaces, e.g., \"[[c1],[c2],[c3],[c4]]\".", "solution": "The problem is deemed valid as it is scientifically grounded in statistical decision theory and Bayesian analysis, is mathematically well-posed, objective, and internally consistent. All necessary parameters are provided, and the required metrics are defined unambiguously. The problem setup aligns with standard models in biostatistics and machine learning fairness. We may therefore proceed with a formal derivation and solution.\n\nThe core of the problem is to compute several fairness metrics for a classifier based on a genomic risk score. This requires an analytical derivation of the True Positive Rates ($\\mathrm{TPR}$) and False Positive Rates ($\\mathrm{FPR}$) for two population groups, $A$ and $B$.\n\nThe solution is developed from first principles in the following steps:\n1.  Analysis of the posterior probability score, $S_g(x)$.\n2.  Derivation of an equivalent decision threshold on the biomarker, $x$.\n3.  Formulation of the analytical expressions for $\\mathrm{TPR}_g(t)$ and $\\mathrm{FPR}_g(t)$.\n4.  Evaluation of the Expected Calibration Error ($\\mathrm{ECE}_g$).\n5.  Derivation of the aggregate fairness metrics, $\\Delta_{\\mathrm{EO}}(t)$ and $\\Delta_{\\mathrm{DP}}(t)$.\n6.  Specification of the complete algorithm.\n\nThe biomarker $X$ for an individual in group $g \\in \\{A, B\\}$ follows a class-conditional Gaussian distribution:\n$$X \\mid (Y=y, G=g) \\sim \\mathcal{N}(\\mu_{gy}, \\sigma)$$\nThe probability density function (PDF) for a given class $y \\in \\{0,1\\}$ is:\n$$f_{gy}(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x - \\mu_{gy})^2}{2\\sigma^2}\\right)$$\nThe disease prevalence in group $g$ is $\\pi_g = \\mathbb{P}(Y=1 \\mid G=g)$.\n\nThe risk score $S_g(x)$ is the posterior probability of having the disease given the biomarker value $x$ and group membership $g$. By Bayes' theorem, it is:\n$$S_g(x) \\triangleq \\mathbb{P}(Y=1 \\mid X=x, G=g) = \\frac{\\pi_g f_{g1}(x)}{\\pi_g f_{g1}(x) + (1-\\pi_g) f_{g0}(x)}$$\nThis can be rewritten in terms of the likelihood ratio $\\frac{f_{g1}(x)}{f_{g0}(x)}$:\n$$S_g(x) = \\frac{1}{1 + \\frac{1-\\pi_g}{\\pi_g} \\frac{f_{g0}(x)}{f_{g1}(x)}} = \\left(1 + \\exp\\left[-\\left(\\log\\left(\\frac{\\pi_g}{1-\\pi_g}\\right) + \\log\\left(\\frac{f_{g1}(x)}{f_{g0}(x)}\\right)\\right)\\right]\\right)^{-1}$$\nThis shows that $S_g(x)$ is a logistic (sigmoid) function of the log-odds. The log-likelihood ratio is:\n$$\\log\\left(\\frac{f_{g1}(x)}{f_{g0}(x)}\\right) = \\log\\left(\\frac{\\exp\\left(-\\frac{(x - \\mu_{g1})^2}{2\\sigma^2}\\right)}{\\exp\\left(-\\frac{(x - \\mu_{g0})^2}{2\\sigma^2}\\right)}\\right) = \\frac{1}{2\\sigma^2} \\left[ (x-\\mu_{g0})^2 - (x-\\mu_{g1})^2 \\right]$$\n$$= \\frac{1}{2\\sigma^2} \\left[ (x^2 - 2x\\mu_{g0} + \\mu_{g0}^2) - (x^2 - 2x\\mu_{g1} + \\mu_{g1}^2) \\right] = \\frac{\\mu_{g1} - \\mu_{g0}}{\\sigma^2} x + \\frac{\\mu_{g0}^2 - \\mu_{g1}^2}{2\\sigma^2}$$\nThis log-likelihood ratio is a linear function of $x$. Since the logistic function is strictly monotonic increasing, the score $S_g(x)$ is a monotonic function of $x$. Its direction of monotonicity is determined by the sign of the coefficient of $x$, which is $\\mu_{g1} - \\mu_{g0}$.\n- If $\\mu_{g1}  \\mu_{g0}$, $S_g(x)$ is an increasing function of $x$.\n- If $\\mu_{g1}  \\mu_{g0}$, $S_g(x)$ is a decreasing function of $x$.\n- If $\\mu_{g1} = \\mu_{g0}$, the biomarker carries no information, and $S_g(x) = \\pi_g$ is constant.\n\nThe classifier predicts $\\hat{Y}=1$ if and only if $S_g(X) \\ge t$. Due to the monotonicity of $S_g(x)$, this condition is equivalent to a single-sided threshold on $X$. Let $x_g^*$ be the value of the biomarker where $S_g(x_g^*) = t$. Solving for $x_g^*$ (assuming $0t1$):\n$$\\frac{f_{g1}(x_g^*)}{f_{g0}(x_g^*)} = \\frac{t}{1-t} \\frac{1-\\pi_g}{\\pi_g}$$\nTaking the logarithm of both sides:\n$$\\log\\left(\\frac{f_{g1}(x_g^*)}{f_{g0}(x_g^*)}\\right) = \\log\\left(\\frac{t}{1-t}\\right) - \\log\\left(\\frac{\\pi_g}{1-\\pi_g}\\right) = \\mathrm{logit}(t) - \\mathrm{logit}(\\pi_g)$$\nSubstituting the linear expression for the log-likelihood ratio and solving for $x_g^*$ (assuming $\\mu_{g1} \\neq \\mu_{g0}$):\n$$\\frac{\\mu_{g1} - \\mu_{g0}}{\\sigma^2} x_g^* + \\frac{\\mu_{g0}^2 - \\mu_{g1}^2}{2\\sigma^2} = \\mathrm{logit}(t) - \\mathrm{logit}(\\pi_g)$$\n$$x_g^* = \\frac{\\sigma^2}{\\mu_{g1} - \\mu_{g0}} \\left(\\mathrm{logit}(t) - \\mathrm{logit}(\\pi_g)\\right) + \\frac{\\mu_{g0} + \\mu_{g1}}{2}$$\n\nNow we can derive the TPR and FPR. Let $\\Phi(\\cdot)$ denote the cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0,1)$.\nIf $\\mu_{g1}  \\mu_{g0}$, $S_g(x)$ is increasing, so $S_g(X) \\ge t \\iff X \\ge x_g^*$.\n$$\\mathrm{TPR}_g(t) = \\mathbb{P}(X \\ge x_g^* \\mid Y=1, G=g) = \\mathbb{P}\\left(\\frac{X-\\mu_{g1}}{\\sigma} \\ge \\frac{x_g^*-\\mu_{g1}}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{x_g^*-\\mu_{g1}}{\\sigma}\\right) = \\Phi\\left(\\frac{\\mu_{g1}-x_g^*}{\\sigma}\\right)$$\n$$\\mathrm{FPR}_g(t) = \\mathbb{P}(X \\ge x_g^* \\mid Y=0, G=g) = \\mathbb{P}\\left(\\frac{X-\\mu_{g0}}{\\sigma} \\ge \\frac{x_g^*-\\mu_{g0}}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{x_g^*-\\mu_{g0}}{\\sigma}\\right) = \\Phi\\left(\\frac{\\mu_{g0}-x_g^*}{\\sigma}\\right)$$\nIf $\\mu_{g1}  \\mu_{g0}$, $S_g(x)$ is decreasing, so $S_g(X) \\ge t \\iff X \\le x_g^*$.\n$$\\mathrm{TPR}_g(t) = \\mathbb{P}(X \\le x_g^* \\mid Y=1, G=g) = \\Phi\\left(\\frac{x_g^*-\\mu_{g1}}{\\sigma}\\right)$$\n$$\\mathrm{FPR}_g(t) = \\mathbb{P}(X \\le x_g^* \\mid Y=0, G=g) = \\Phi\\left(\\frac{x_g^*-\\mu_{g0}}{\\sigma}\\right)$$\nAll test cases satisfy $\\mu_{g1}  \\mu_{g0}$.\n\nBoundary cases for the threshold $t$:\n- If $t=0$, the condition $S_g(X) \\ge 0$ is always met (as probabilities are non-negative), so $\\hat{Y}=1$ is always predicted. Thus, $\\mathrm{TPR}_g(0)=1$ and $\\mathrm{FPR}_g(0)=1$.\n- If $t=1$, the condition $S_g(X) \\ge 1$ is met only if $S_g(X)=1$. This requires $f_{g0}(X)=0$, which for a Gaussian distribution occurs with probability $0$. Thus, $\\hat{Y}=1$ is never predicted. This leads to $\\mathrm{TPR}_g(1)=0$ and $\\mathrm{FPR}_g(1)=0$.\n\nThe Expected Calibration Error ($\\mathrm{ECE}$) for group $g$ is defined as $\\mathrm{ECE}_g \\triangleq \\mathbb{E}\\left[ \\left| \\mathbb{P}(Y=1 \\mid S_g) - S_g \\right| \\,\\middle|\\, G=g \\right]$. The score function $S_g(x)$ is constructed to be the true posterior probability, $\\mathbb{P}(Y=1 \\mid X=x, G=g)$. Since $S_g(x)$ is a monotonic, and thus invertible, function of $x$ (for $\\mu_{g1} \\neq \\mu_{g0}$), conditioning on $S_g(X)$ is equivalent to conditioning on $X$. Therefore, $\\mathbb{P}(Y=1 \\mid S_g, G=g) = \\mathbb{P}(Y=1 \\mid X, G=g) = S_g$. The term inside the absolute value is $\\left| S_g - S_g \\right| = 0$. Consequently, the expectation is zero: $\\mathrm{ECE}_g=0$. A score function defined as the true posterior probability is, by definition, perfectly calibrated.\n\nThe remaining metrics are computed from the TPRs and FPRs:\n$$\\Delta_{\\mathrm{EO}}(t) = \\max\\left( \\left| \\mathrm{TPR}_A(t) - \\mathrm{TPR}_B(t) \\right| , \\left| \\mathrm{FPR}_A(t) - \\mathrm{FPR}_B(t) \\right| \\right)$$\n$$\\mathbb{P}(\\hat{Y}=1 \\mid G=g) = \\pi_g \\mathrm{TPR}_g(t) + (1-\\pi_g) \\mathrm{FPR}_g(t)$$\n$$\\Delta_{\\mathrm{DP}}(t) = \\left| \\mathbb{P}(\\hat{Y}=1 \\mid G=A) - \\mathbb{P}(\\hat{Y}=1 \\mid G=B) \\right|$$\n\nAlgorithm for each test case $((\\mu_{A1},\\mu_{A0},\\sigma,\\pi_A), (\\mu_{B1},\\mu_{B0},\\sigma,\\pi_B), t)$:\n1. Set $\\mathrm{ECE}_A = 0.0$ and $\\mathrm{ECE}_B = 0.0$.\n2. If $t=0$, set $\\mathrm{TPR}_A, \\mathrm{FPR}_A, \\mathrm{TPR}_B, \\mathrm{FPR}_B$ to $1.0$.\n3. If $t=1$, set $\\mathrm{TPR}_A, \\mathrm{FPR}_A, \\mathrm{TPR}_B, \\mathrm{FPR}_B$ to $0.0$.\n4. If $0  t  1$, for each group $g \\in \\{A, B\\}$:\n    a. Retrieve parameters $\\mu_{g1}, \\mu_{g0}, \\sigma, \\pi_g$.\n    b. Compute the logit terms: $\\mathrm{logit}(t) = \\log(t/(1-t))$ and $\\mathrm{logit}(\\pi_g) = \\log(\\pi_g/(1-\\pi_g))$.\n    c. Compute the biomarker threshold $x_g^* = \\frac{\\sigma^2}{\\mu_{g1} - \\mu_{g0}} (\\mathrm{logit}(t) - \\mathrm{logit}(\\pi_g)) + \\frac{\\mu_{g0} + \\mu_{g1}}{2}$.\n    d. Compute rates using the standard normal CDF $\\Phi$:\n       $\\mathrm{TPR}_g = \\Phi((\\mu_{g1}-x_g^*)/\\sigma)$\n       $\\mathrm{FPR}_g = \\Phi((\\mu_{g0}-x_g^*)/\\sigma)$\n5. Calculate $\\Delta_{\\mathrm{EO}}$ and $\\Delta_{\\mathrm{DP}}$ using the computed TPR and FPR values.\n6. Assemble the 8-element list: $[\\mathrm{ECE}_A, \\mathrm{ECE}_B, \\mathrm{TPR}_A, \\mathrm{FPR}_A, \\mathrm{TPR}_B, \\mathrm{FPR}_B, \\Delta_{\\mathrm{EO}}, \\Delta_{\\mathrm{DP}}]$.\nThis procedure will be implemented to solve the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing fairness metrics for given test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Format: ((mu_A1, mu_A0, sigma, pi_A), (mu_B1, mu_B0, sigma, pi_B), t)\n    test_cases = [\n        ((2.0, 0.0, 1.0, 0.1), (2.0, 0.0, 1.0, 0.4), 0.5), # Case 1\n        ((2.0, 0.0, 1.0, 0.1), (2.0, 0.0, 1.0, 0.4), 0.0), # Case 2\n        ((1.0, 0.0, 1.0, 0.3), (2.0, 0.0, 1.0, 0.3), 0.5), # Case 3\n        ((2.0, 0.0, 1.0, 0.1), (2.0, 0.0, 1.0, 0.4), 1.0), # Case 4\n    ]\n\n    all_results = []\n    for case in test_cases:\n        params_A, params_B, t = case\n        \n        # Unpack parameters for both groups\n        mu_A1, mu_A0, sigma_A, pi_A = params_A\n        mu_B1, mu_B0, sigma_B, pi_B = params_B\n\n        # The problem states a common sigma\n        sigma = sigma_A\n\n        # ECE is always 0 because the score is the true posterior probability\n        ece_A, ece_B = 0.0, 0.0\n\n        if t == 0.0:\n            # Predict positive always\n            tpr_A, fpr_A = 1.0, 1.0\n            tpr_B, fpr_B = 1.0, 1.0\n        elif t == 1.0:\n            # Predict positive never\n            tpr_A, fpr_A = 0.0, 0.0\n            tpr_B, fpr_B = 0.0, 0.0\n        else: # 0  t  1\n            def calculate_rates(mu1, mu0, sigma, pi, t):\n                \"\"\"Helper function to calculate TPR and FPR for a single group.\"\"\"\n                if mu1 == mu0:\n                    # Biomarker is not informative, score is constant pi\n                    if pi = t:\n                        return 1.0, 1.0\n                    else:\n                        return 0.0, 0.0\n                \n                # logit(p) = log(p / (1-p))\n                logit_t = np.log(t / (1 - t))\n                logit_pi = np.log(pi / (1 - pi))\n                \n                # Calculate biomarker threshold x_star\n                x_star = (sigma**2 / (mu1 - mu0)) * (logit_t - logit_pi) + (mu0 + mu1) / 2\n                \n                # Calculate TPR and FPR based on monotonicity\n                if mu1  mu0:\n                    # S(x) is increasing, rule is X = x_star\n                    tpr = norm.cdf((mu1 - x_star) / sigma)\n                    fpr = norm.cdf((mu0 - x_star) / sigma)\n                else: # mu1  mu0\n                    # S(x) is decreasing, rule is X = x_star\n                    tpr = norm.cdf((x_star - mu1) / sigma)\n                    fpr = norm.cdf((x_star - mu0) / sigma)\n                \n                return tpr, fpr\n\n            tpr_A, fpr_A = calculate_rates(mu_A1, mu_A0, sigma, pi_A, t)\n            tpr_B, fpr_B = calculate_rates(mu_B1, mu_B0, sigma, pi_B, t)\n\n        # Calculate Equalized Odds gap\n        delta_eo = max(abs(tpr_A - tpr_B), abs(fpr_A - fpr_B))\n        \n        # Calculate positive prediction rates for Demographic Parity\n        p_hat_A = pi_A * tpr_A + (1 - pi_A) * fpr_A\n        p_hat_B = pi_B * tpr_B + (1 - pi_B) * fpr_B\n        \n        # Calculate Demographic Parity difference\n        delta_dp = abs(p_hat_A - p_hat_B)\n        \n        # Assemble the results vector for the current case\n        case_results = [ece_A, ece_B, tpr_A, fpr_A, tpr_B, fpr_B, delta_eo, delta_dp]\n        all_results.append(case_results)\n\n    # Format the final output string exactly as specified without spaces\n    list_of_strings = []\n    for sublist in all_results:\n        # A custom formatter could be used for specific decimal places if needed\n        list_of_strings.append('[' + ','.join(map(str, sublist)) + ']')\n    final_output_str = '[' + ','.join(list_of_strings) + ']'\n    \n    print(final_output_str)\n\nsolve()\n```", "id": "4338581"}, {"introduction": "To move beyond merely observing bias to actively correcting it, we can employ powerful tools from causal inference. This advanced simulation exercise places you in the role of a model auditor working with a Structural Causal Model (SCM), where a protected attribute causally influences both genomic and environmental factors. Your task is to implement a 'counterfactual' adjustment—a technique designed to remove the influence of the protected attribute—and rigorously evaluate its effects on a suite of fairness and performance metrics [@problem_id:4338538]. This practice provides invaluable experience with a leading-edge framework for reasoning about and mitigating algorithmic bias.", "problem": "Consider a binary protected attribute $A \\in \\{0,1\\}$ representing socially salient ancestry membership, a real-valued genomic feature summary $G \\in \\mathbb{R}$, an environmental exposure summary $E \\in \\mathbb{R}$, and a continuous clinical risk $Y \\in \\mathbb{R}$. Assume the following Structural Causal Model (SCM) in the sense of Judea Pearl, with the do-operator and exogenous noise terms:\n$$\nA \\sim \\mathrm{Bernoulli}(p_A),\\quad U_G \\sim \\mathcal{N}(0,\\sigma_G^2),\\quad U_E \\sim \\mathcal{N}(0,\\sigma_E^2),\\quad U_Y \\sim \\mathcal{N}(0,\\sigma_Y^2),\n$$\n$$\nG := \\gamma_{AG}\\,A + U_G,\\quad E := \\gamma_{AE}\\,A + U_E,\\quad Y := \\beta_{GY}\\,G + \\beta_{EY}\\,E + U_Y.\n$$\nA linear prediction model is trained by Ordinary Least Squares (OLS) to estimate $Y$ from the pair $(G,E)$, resulting in coefficients $w_G$, $w_E$ and an intercept $b$. Denote the unadjusted prediction score by $S := b + w_G\\,G + w_E\\,E$. For classification-based metrics, define the binary clinical label $D := \\mathbb{I}\\{Y \\ge 0\\}$ and a decision threshold $\\tau$ set to the empirical $\\tau_q$-quantile of $S$ over the whole sample. Consider a counterfactual fairness adjustment that removes the influence of $A$ on $G$ and $E$ under the SCM via an intervention to a baseline $a_0 \\in \\{0,1\\}$, implemented as\n$$\nG^{\\mathrm{cf}} := G - \\gamma_{AG}\\,(A - a_0),\\quad E^{\\mathrm{cf}} := E - \\gamma_{AE}\\,(A - a_0),\\quad S^{\\mathrm{cf}} := b + w_G\\,G^{\\mathrm{cf}} + w_E\\,E^{\\mathrm{cf}}.\n$$\nYour task is to write a complete program that:\n- Simulates $N$ independent individuals from the above SCM for specified parameter values, trains an OLS predictor for $Y$ using $(G,E)$, and computes $S$ and $S^{\\mathrm{cf}}$.\n- Computes the following fairness metrics as real-valued decimals, both before adjustment (using $S$) and after adjustment (using $S^{\\mathrm{cf}}$), using the same decision threshold $\\tau$ computed from $S$:\n  1. Score parity gap: $\\left|\\mathbb{E}[S \\mid A=1] - \\mathbb{E}[S \\mid A=0]\\right|$ and its counterpart with $S^{\\mathrm{cf}}$.\n  2. Mean squared error (MSE) equity gap: $\\left|\\mathrm{MSE}_{A=1} - \\mathrm{MSE}_{A=0}\\right|$, where $\\mathrm{MSE}_{A=a} := \\mathbb{E}\\left[(Y - \\widehat{Y})^2 \\mid A=a\\right]$ with $\\widehat{Y}=S$ or $\\widehat{Y}=S^{\\mathrm{cf}}$ respectively.\n  3. Equalized odds gap: $\\max\\left(\\left|\\mathrm{TPR}_{A=1} - \\mathrm{TPR}_{A=0}\\right|,\\left|\\mathrm{FPR}_{A=1} - \\mathrm{FPR}_{A=0}\\right|\\right)$, where $\\mathrm{TPR}_{A=a} := \\mathbb{P}(S \\ge \\tau \\mid D=1,A=a)$ and $\\mathrm{FPR}_{A=a} := \\mathbb{P}(S \\ge \\tau \\mid D=0,A=a)$; compute the analogous quantity with $S^{\\mathrm{cf}}$ using the same $\\tau$.\n  4. Counterfactual invariance gap: $\\mathbb{E}\\left[\\left|S - S^{\\prime}\\right|\\right]$ and $\\mathbb{E}\\left[\\left|S^{\\mathrm{cf}} - (S^{\\mathrm{cf}})^{\\prime}\\right|\\right]$, where $S^{\\prime}$ denotes the score recomputed under a counterfactual flip $A' := 1-A$ with exogenous noises $(U_G,U_E)$ held fixed, i.e., $G' := \\gamma_{AG}\\,A' + U_G$, $E' := \\gamma_{AE}\\,A' + U_E$, $S' := b + w_G\\,G' + w_E\\,E'$, and $(S^{\\mathrm{cf}})^{\\prime}$ is analogously defined using $G'$, $E'$, and $A'$ through the counterfactual adjustment.\n\nUse the following test suite; each test consists of a tuple of parameters $(N,\\mathrm{seed},p_A,\\gamma_{AG},\\gamma_{AE},\\sigma_G,\\sigma_E,\\sigma_Y,\\beta_{GY},\\beta_{EY},\\tau_q,a_0)$:\n- Test $1$ (general case): $(20000,\\,314159,\\,0.5,\\,0.8,\\,0.5,\\,1.0,\\,1.0,\\,1.0,\\,1.2,\\,0.6,\\,0.75,\\,0)$.\n- Test $2$ (boundary where $A$ does not affect $G$ or $E$): $(20000,\\,271828,\\,0.5,\\,0.0,\\,0.0,\\,1.0,\\,1.0,\\,1.0,\\,1.2,\\,0.6,\\,0.75,\\,0)$.\n- Test $3$ (asymmetric strong $A$ effects and no environmental effect on $Y$): $(20000,\\,161803,\\,0.4,\\,2.0,\\,-1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.5,\\,0.0,\\,0.75,\\,0)$.\n- Test $4$ (high-noise regime): $(20000,\\,123457,\\,0.6,\\,1.0,\\,0.6,\\,2.0,\\,2.0,\\,3.0,\\,1.0,\\,0.8,\\,0.75,\\,0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test’s result is itself a comma-separated list of $8$ decimals in the following order: $[\\text{score\\_parity\\_before},\\text{mse\\_gap\\_before},\\text{eo\\_gap\\_before},\\text{cf\\_gap\\_before},\\text{score\\_parity\\_after},\\text{mse\\_gap\\_after},\\text{eo\\_gap\\_after},\\text{cf\\_gap\\_after}]$. Thus the full output must have the form $[[r_{1,1},\\ldots,r_{1,8}],[r_{2,1},\\ldots,r_{2,8}],\\ldots]$ with no spaces and each $r_{i,j}$ a float.", "solution": "The user requests a program to simulate a specified Structural Causal Model (SCM), train a prediction model, and evaluate the impact of a counterfactual fairness adjustment on several bias metrics. The problem is well-defined, scientifically grounded in causal inference and algorithmic fairness literature, and computationally tractable.\n\n### 1. Model Specification and Simulation\nThe analysis is based on a Structural Causal Model (SCM) defined by the following equations:\n$$\nA \\sim \\mathrm{Bernoulli}(p_A)\n$$\n$$\nU_G \\sim \\mathcal{N}(0,\\sigma_G^2), \\quad U_E \\sim \\mathcal{N}(0,\\sigma_E^2), \\quad U_Y \\sim \\mathcal{N}(0,\\sigma_Y^2)\n$$\n$$\nG := \\gamma_{AG}\\,A + U_G\n$$\n$$\nE := \\gamma_{AE}\\,A + U_E\n$$\n$$\nY := \\beta_{GY}\\,G + \\beta_{EY}\\,E + U_Y\n$$\nHere, $A$ is a binary protected attribute which causally influences a genomic summary $G$ and an environmental summary $E$. Both $G$ and $E$ in turn influence a continuous clinical risk outcome $Y$. This structure implies that $A$ is a confounder for the relationship between the features $(G, E)$ and the outcome $Y$. The terms $U_G$, $U_E$, and $U_Y$ are exogenous, unobserved noise variables, assumed to be independent and normally distributed.\n\nFor each test case, we simulate a dataset of $N$ independent individuals by first drawing samples for $A$ and the noise terms $(U_G, U_E, U_Y)$ from their respective distributions, and then computing $G$, $E$, and $Y$ according to the structural assignments.\n\n### 2. OLS Prediction Model\nAn ordinary least squares (OLS) linear regression model is trained to predict the outcome $Y$ using the features $(G, E)$. The model takes the form:\n$$\n\\widehat{Y} = S := b + w_G\\,G + w_E\\,E\n$$\nwhere $(b, w_G, w_E)$ are the coefficients (intercept and weights) estimated by minimizing the sum of squared residuals $\\sum_i (Y_i - \\widehat{Y}_i)^2$. These coefficients are obtained by solving the linear system $\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$, where $\\mathbf{X}$ is the design matrix with columns for the intercept, $G$, and $E$, and $\\mathbf{y}$ is the vector of outcomes $Y$. The unadjusted prediction score is denoted by $S$.\n\n### 3. Counterfactual Fairness Adjustment\nThe goal of the adjustment is to produce a new score, $S^{\\mathrm{cf}}$, that corresponds to the prediction an individual would have received had their protected attribute $A$ been set to a baseline value $a_0$, while holding their idiosyncratic noise terms $(U_G, U_E)$ constant. This is a counterfactual manipulation. The adjusted features are:\n$$\nG^{\\mathrm{cf}} := G - \\gamma_{AG}\\,(A - a_0) = (\\gamma_{AG}A + U_G) - \\gamma_{AG}(A - a_0) = \\gamma_{AG}a_0 + U_G\n$$\n$$\nE^{\\mathrm{cf}} := E - \\gamma_{AE}\\,(A - a_0) = (\\gamma_{AE}A + U_E) - \\gamma_{AE}(A - a_0) = \\gamma_{AE}a_0 + U_E\n$$\nThese adjusted features represent the values $G$ and $E$ would have taken had the individual's attribute been $a_0$. The adjusted score $S^{\\mathrm{cf}}$ is then computed by applying the original OLS weights to these adjusted features:\n$$\nS^{\\mathrm{cf}} := b + w_G\\,G^{\\mathrm{cf}} + w_E\\,E^{\\mathrm{cf}}\n$$\nThis post-processing method modifies the scores without re-training the model.\n\n### 4. Fairness and Performance Metrics\nWe evaluate the unadjusted scores $S$ and adjusted scores $S^{\\mathrm{cf}}$ using four metrics. All expectations $\\mathbb{E}[\\cdot]$ are estimated by sample means over the simulated population of size $N$.\n\n**1. Score Parity Gap:** Measures the difference in average prediction scores between the two groups defined by $A$.\n$$\n\\text{Gap} = \\left|\\mathbb{E}[S \\mid A=1] - \\mathbb{E}[S \\mid A=0]\\right|\n$$\nThis metric is computed for both $S$ and $S^{\\mathrm{cf}}$. A smaller gap indicates greater fairness in terms of score parity.\n\n**2. MSE Equity Gap:** Measures the difference in model accuracy, as quantified by Mean Squared Error (MSE), between the groups.\n$$\n\\text{Gap} = \\left|\\mathrm{MSE}_{A=1} - \\mathrm{MSE}_{A=0}\\right|, \\quad \\text{where} \\quad \\mathrm{MSE}_{A=a} = \\mathbb{E}\\left[(Y - \\widehat{Y})^2 \\mid A=a\\right]\n$$\nThis is computed with $\\widehat{Y} = S$ (before) and $\\widehat{Y} = S^{\\mathrm{cf}}$ (after).\n\n**3. Equalized Odds Gap:** This classification metric assesses whether the model's true positive rate (TPR) and false positive rate (FPR) are equal across groups. First, a binary clinical label is defined as $D := \\mathbb{I}\\{Y \\ge 0\\}$, and a decision threshold $\\tau$ is set to the $\\tau_q$-quantile of the unadjusted scores $S$. This same threshold is used for all calculations.\n$$\n\\mathrm{TPR}_{a} = \\mathbb{P}(\\widehat{Y} \\ge \\tau \\mid D=1, A=a)\n$$\n$$\n\\mathrm{FPR}_{a} = \\mathbb{P}(\\widehat{Y} \\ge \\tau \\mid D=0, A=a)\n$$\n$$\n\\text{Gap} = \\max\\left(\\left|\\mathrm{TPR}_{A=1} - \\mathrm{TPR}_{A=0}\\right|,\\left|\\mathrm{FPR}_{A=1} - \\mathrm{FPR}_{A=0}\\right|\\right)\n$$\nThe gap is computed for $\\widehat{Y}=S$ and $\\widehat{Y}=S^{\\mathrm{cf}}$.\n\n**4. Counterfactual Invariance Gap:** This metric directly quantifies the score's sensitivity to a counterfactual change in the attribute $A$. For each individual, we define a counterfactual score $S'$ that would result from flipping their attribute $A$ to $A' = 1-A$, while holding their specific exogenous noises $(U_G, U_E)$ fixed.\n$$\nG' := \\gamma_{AG}\\,A' + U_G, \\quad E' := \\gamma_{AE}\\,A' + U_E, \\quad S' := b + w_G\\,G' + w_E\\,E'\n$$\nThe gap is the expected absolute change in the score:\n$$\n\\text{Gap} = \\mathbb{E}\\left[\\left|S - S^{\\prime}\\right|\\right]\n$$\nAn analogous quantity is computed for the adjusted scores, $\\mathbb{E}\\left[\\left|S^{\\mathrm{cf}} - (S^{\\mathrm{cf}})^{\\prime}\\right|\\right]$. By its construction, $S^{\\mathrm{cf}}$ is a function of $U_G, U_E$ and constants, but not $A$. Therefore, flipping $A$ to $A'$ has no effect on $S^{\\mathrm{cf}}$, and this gap is expected to be $0$. This serves as a key validation of the adjustment's intended mechanism.\n\nThe program implements these steps for each provided test case, collecting the eight specified metrics (four before adjustment, four after) and formats them as requested.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation and compute fairness metrics across all test cases.\n    \"\"\"\n    test_cases = [\n        # Test 1 (general case)\n        (20000, 314159, 0.5, 0.8, 0.5, 1.0, 1.0, 1.0, 1.2, 0.6, 0.75, 0),\n        # Test 2 (boundary where A does not affect G or E)\n        (20000, 271828, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.2, 0.6, 0.75, 0),\n        # Test 3 (asymmetric strong A effects and no environmental effect on Y)\n        (20000, 161803, 0.4, 2.0, -1.0, 1.0, 1.0, 1.0, 1.5, 0.0, 0.75, 0),\n        # Test 4 (high-noise regime)\n        (20000, 123457, 0.6, 1.0, 0.6, 2.0, 2.0, 3.0, 1.0, 0.8, 0.75, 0),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result_set = compute_metrics_for_case(params)\n        all_results.append(f\"[{','.join(map(str, result_set))}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\n\ndef compute_metrics_for_case(params):\n    \"\"\"\n    Computes all 8 required metrics for a single parameter set.\n    \"\"\"\n    N, seed, p_A, gamma_AG, gamma_AE, sigma_G, sigma_E, sigma_Y, beta_GY, beta_EY, tau_q, a_0 = params\n    \n    rng = np.random.default_rng(seed)\n\n    # 1. Simulate data from the SCM\n    A = rng.binomial(1, p_A, size=N)\n    U_G = rng.normal(0, sigma_G, size=N)\n    U_E = rng.normal(0, sigma_E, size=N)\n    U_Y = rng.normal(0, sigma_Y, size=N)\n\n    G = gamma_AG * A + U_G\n    E = gamma_AE * A + U_E\n    Y = beta_GY * G + beta_EY * E + U_Y\n\n    # 2. Train OLS model: Y ~ G + E\n    X_design = np.c_[np.ones(N), G, E]\n    coeffs = np.linalg.lstsq(X_design, Y, rcond=None)[0]\n    b, w_G, w_E = coeffs[0], coeffs[1], coeffs[2]\n\n    # 3. Compute unadjusted and adjusted scores\n    S = X_design @ coeffs\n\n    G_cf = G - gamma_AG * (A - a_0)\n    E_cf = E - gamma_AE * (A - a_0)\n    S_cf = b + w_G * G_cf + w_E * E_cf\n\n    # 4. Prepare for metric calculations\n    mask_A0 = (A == 0)\n    mask_A1 = (A == 1)\n    \n    D = (Y = 0)\n    mask_D0 = (D == 0)\n    mask_D1 = (D == 1)\n\n    tau = np.quantile(S, tau_q)\n\n    # 5. Calculate metrics\n    metrics = []\n\n    # Helper for TPR/FPR to handle empty subgroups\n    def get_rate(scores, subgroup_mask, threshold):\n        subgroup_size = np.sum(subgroup_mask)\n        if subgroup_size == 0:\n            return 0.0\n        positives = np.sum(scores[subgroup_mask] = threshold)\n        return positives / subgroup_size\n\n    # --- Metrics Before Adjustment (using S) ---\n    \n    # 1. Score Parity Gap\n    score_parity_before = np.abs(np.mean(S[mask_A1]) - np.mean(S[mask_A0]))\n    metrics.append(score_parity_before)\n\n    # 2. MSE Equity Gap\n    mse_A1 = np.mean((Y[mask_A1] - S[mask_A1])**2)\n    mse_A0 = np.mean((Y[mask_A0] - S[mask_A0])**2)\n    mse_gap_before = np.abs(mse_A1 - mse_A0)\n    metrics.append(mse_gap_before)\n\n    # 3. Equalized Odds Gap\n    tpr_A1 = get_rate(S, mask_A1  mask_D1, tau)\n    tpr_A0 = get_rate(S, mask_A0  mask_D1, tau)\n    fpr_A1 = get_rate(S, mask_A1  mask_D0, tau)\n    fpr_A0 = get_rate(S, mask_A0  mask_D0, tau)\n    eo_gap_before = np.maximum(np.abs(tpr_A1 - tpr_A0), np.abs(fpr_A1 - fpr_A0))\n    metrics.append(eo_gap_before)\n\n    # 4. Counterfactual Invariance Gap\n    A_prime = 1 - A\n    G_prime = gamma_AG * A_prime + U_G\n    E_prime = gamma_AE * A_prime + U_E\n    S_prime = b + w_G * G_prime + w_E * E_prime\n    cf_inv_gap_before = np.mean(np.abs(S - S_prime))\n    metrics.append(cf_inv_gap_before)\n\n    # --- Metrics After Adjustment (using S_cf) ---\n\n    # 5. Score Parity Gap\n    score_parity_after = np.abs(np.mean(S_cf[mask_A1]) - np.mean(S_cf[mask_A0]))\n    metrics.append(score_parity_after)\n\n    # 6. MSE Equity Gap\n    mse_cf_A1 = np.mean((Y[mask_A1] - S_cf[mask_A1])**2)\n    mse_cf_A0 = np.mean((Y[mask_A0] - S_cf[mask_A0])**2)\n    mse_gap_after = np.abs(mse_cf_A1 - mse_cf_A0)\n    metrics.append(mse_gap_after)\n\n    # 7. Equalized Odds Gap\n    tpr_cf_A1 = get_rate(S_cf, mask_A1  mask_D1, tau)\n    tpr_cf_A0 = get_rate(S_cf, mask_A0  mask_D1, tau)\n    fpr_cf_A1 = get_rate(S_cf, mask_A1  mask_D0, tau)\n    fpr_cf_A0 = get_rate(S_cf, mask_A0  mask_D0, tau)\n    eo_gap_after = np.maximum(np.abs(tpr_cf_A1 - tpr_cf_A0), np.abs(fpr_cf_A1 - fpr_cf_A0))\n    metrics.append(eo_gap_after)\n\n    # 8. Counterfactual Invariance Gap\n    G_prime_cf = G_prime - gamma_AG * (A_prime - a_0)\n    E_prime_cf = E_prime - gamma_AE * (A_prime - a_0)\n    S_cf_prime = b + w_G * G_prime_cf + w_E * E_prime_cf\n    cf_inv_gap_after = np.mean(np.abs(S_cf - S_cf_prime))\n    metrics.append(cf_inv_gap_after)\n    \n    return metrics\n\nsolve()\n```", "id": "4338538"}]}