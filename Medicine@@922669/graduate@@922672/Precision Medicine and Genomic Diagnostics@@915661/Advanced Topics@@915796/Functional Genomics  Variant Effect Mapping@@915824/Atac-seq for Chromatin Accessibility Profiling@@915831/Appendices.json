{"hands_on_practices": [{"introduction": "Before diving into complex biological questions, the first essential step in any genomics analysis is rigorous quality control (QC). This practice focuses on interpreting key ATAC-seq QC metrics to ensure your dataset is reliable and possesses a strong signal-to-noise ratio. By evaluating the Fraction of Reads in Peaks ($FRiP$), Transcription Start Site ($TSS$) enrichment, and duplication rate, you will learn to make the critical call on whether a dataset is suitable for downstream clinical or research analysis [@problem_id:4317388].", "problem": "An Assay for Transposase-Accessible Chromatin using sequencing (ATAC-seq) dataset is being evaluated for clinical use in a precision medicine pipeline. The pipeline aims to ensure high signal-to-noise ratio and library complexity by enforcing quality control (QC) thresholds derived from well-tested community standards for chromatin accessibility profiling. The foundational basis is that transposase integration preferentially occurs in open chromatin, leading to read density enrichment at promoters and other accessible regulatory regions, and that sufficient library complexity minimizes clonal duplication artifacts.\n\nThree quantitative metrics are used:\n\n- Fraction of Reads in Peaks (FRiP), defined as $FRiP = N_{\\text{reads in called peaks}} / N_{\\text{uniquely mapped reads}}$, which serves as a proxy for the proportion of informative signal relative to background.\n- Transcription Start Site (TSS) enrichment, defined as the ratio of average read density at aggregated transcription start sites to the local background in flanking regions, reflecting promoter accessibility and nucleosome depletion.\n- Duplication rate, defined as $D = N_{\\text{duplicate reads}} / N_{\\text{uniquely mapped reads}}$, which reflects library complexity and the extent of clonal amplification.\n\nThe clinical pipeline uses acceptance thresholds consistent with widely adopted practices for ATAC-seq in diagnostic contexts: $FRiP \\geq 0.20$, TSS enrichment $\\geq 8$, and duplication rate $\\leq 0.30$.\n\nGiven a dataset with $FRiP = 0.25$, TSS enrichment $= 9$, and duplication rate $= 30\\%$, which of the following statements is most accurate regarding whether it passes typical clinical QC thresholds?\n\nA. It meets all thresholds and passes clinical QC.\n\nB. It fails only the TSS enrichment criterion because the minimum acceptable value is $10$.\n\nC. It fails due to high duplication since $30\\%$ exceeds the maximum allowed duplication rate.\n\nD. It fails because FRiP must be at least $0.30$ to be acceptable.", "solution": "### Step 1: Extract Givens\n\nThe problem provides the following data, definitions, and conditions for evaluating an Assay for Transposase-Accessible Chromatin using sequencing (ATAC-seq) dataset:\n\n1.  **Metric Definitions**:\n    -   Fraction of Reads in Peaks (FRiP): $FRiP = N_{\\text{reads in called peaks}} / N_{\\text{uniquely mapped reads}}$\n    -   Transcription Start Site (TSS) enrichment: Ratio of average read density at aggregated transcription start sites to the local background in flanking regions.\n    -   Duplication rate: $D = N_{\\text{duplicate reads}} / N_{\\text{uniquely mapped reads}}$\n\n2.  **Acceptance Thresholds for Clinical QC**:\n    -   $FRiP \\geq 0.20$\n    -   TSS enrichment $\\geq 8$\n    -   Duplication rate $\\leq 0.30$\n\n3.  **Values for the Specific Dataset**:\n    -   $FRiP = 0.25$\n    -   TSS enrichment $= 9$\n    -   Duplication rate $= 30\\%$\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is subjected to validation.\n\n-   **Scientific Grounding**: The problem is scientifically grounded. ATAC-seq is a standard molecular biology technique for profiling chromatin accessibility. The described principles—transposase preference for open chromatin, read enrichment at regulatory elements like promoters, and the concept of library complexity—are fundamental to the method. The metrics FRiP, TSS enrichment, and duplication rate are established and widely used quality control measures for ATAC-seq and similar assays (e.g., by the ENCODE consortium). The definitions and thresholds provided are realistic and consistent with community standards for high-quality data.\n\n-   **Well-Posed**: The problem is well-posed. It presents a clear set of rules (the QC thresholds) and a specific case (the dataset values) to which these rules must be applied. The question asks for an accurate assessment based on the provided information, which leads to a unique and logical conclusion.\n\n-   **Objectivity**: The problem is stated in objective, quantitative terms. The metrics are defined mathematically or procedurally, and the thresholds are given as explicit numerical inequalities. There is no subjective or ambiguous language.\n\n-   **Completeness and Consistency**: The problem is self-contained and free of contradictions. All information required to evaluate the dataset against the QC thresholds is provided. The values and criteria are internally consistent.\n\n-   **Other Flaws**: The problem is not unrealistic, ill-posed, trivial, or outside the realm of scientific verifiability. It represents a typical data analysis task in genomics.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. The solution will now be derived.\n\n### Derivation of the Solution\n\nThe task is to determine if the given ATAC-seq dataset passes the specified clinical QC thresholds. This is achieved by comparing the dataset's metrics against the acceptance criteria one by one.\n\n1.  **FRiP Analysis**:\n    -   The dataset has a $FRiP$ value of $0.25$.\n    -   The acceptance threshold is $FRiP \\geq 0.20$.\n    -   Comparing the values: $0.25 \\geq 0.20$. This inequality is true.\n    -   Therefore, the dataset **passes** the FRiP criterion.\n\n2.  **TSS Enrichment Analysis**:\n    -   The dataset has a TSS enrichment value of $9$.\n    -   The acceptance threshold is TSS enrichment $\\geq 8$.\n    -   Comparing the values: $9 \\geq 8$. This inequality is true.\n    -   Therefore, the dataset **passes** the TSS enrichment criterion.\n\n3.  **Duplication Rate Analysis**:\n    -   The dataset has a duplication rate of $30\\%$. This must be converted to a decimal for comparison with the threshold: $30\\% = 30 / 100 = 0.30$.\n    -   The acceptance threshold is duplication rate $\\leq 0.30$.\n    -   Comparing the values: $0.30 \\leq 0.30$. This inequality is true, as the value is equal to the maximum permissible limit.\n    -   Therefore, the dataset **passes** the duplication rate criterion.\n\n**Conclusion**: Since the dataset meets all three specified QC thresholds ($FRiP \\geq 0.20$, TSS enrichment $\\geq 8$, and duplication rate $\\leq 0.30$), it passes the clinical QC.\n\n### Option-by-Option Analysis\n\nNow, we evaluate each of the provided options based on this derivation.\n\n**A. It meets all thresholds and passes clinical QC.**\nAs determined above, the dataset's $FRiP$ of $0.25$ meets the $\\geq 0.20$ threshold, its TSS enrichment of $9$ meets the $\\geq 8$ threshold, and its duplication rate of $30\\%$ (or $0.30$) meets the $\\leq 0.30$ threshold. All conditions are satisfied.\nThis statement is **Correct**.\n\n**B. It fails only the TSS enrichment criterion because the minimum acceptable value is $10$.**\nThe problem explicitly states the minimum acceptable TSS enrichment is $8$. The dataset's value of $9$ passes this criterion. The claim that the minimum is $10$ contradicts the givens of the problem.\nThis statement is **Incorrect**.\n\n**C. It fails due to high duplication since $30\\%$ exceeds the maximum allowed duplication rate.**\nThe dataset's duplication rate is $30\\%$, which is equal to $0.30$. The threshold is \"duplication rate $\\leq 0.30$\". A value of $0.30$ does not *exceed* $0.30$; it is equal to it, thus satisfying the \"less than or equal to\" condition. The dataset passes this criterion.\nThis statement is **Incorrect**.\n\n**D. It fails because FRiP must be at least $0.30$ to be acceptable.**\nThe problem explicitly states the minimum acceptable $FRiP$ is $0.20$. The dataset's value of $0.25$ passes this criterion. The claim that the minimum is $0.30$ contradicts the givens of the problem.\nThis statement is **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "4317388"}, {"introduction": "A central application of ATAC-seq is identifying statistically significant changes in chromatin accessibility between different biological conditions, such as tumor and normal tissues. This exercise walks you through the fundamental calculations at the heart of differential accessibility analysis [@problem_id:4317411]. You will apply normalization factors to raw data, compute log-fold changes, and use the Benjamini-Hochberg procedure to control the false discovery rate, translating raw counts into robust biological insights.", "problem": "A research group is using Assay for Transposase-Accessible Chromatin using sequencing (ATAC-seq) to profile chromatin accessibility differences between primary tumor and matched normal tissue in a precision oncology study. Four ATAC-seq libraries were prepared: two tumor replicates and two normal replicates. Library size normalization factors for the four libraries are provided to account for differences in sequencing depth and composition. A set of five peaks was pre-selected based on reproducible signal across replicates.\n\nYou are given:\n- Library size normalization factors:\n  - Tumor replicate $1$: $s_{\\mathrm{T1}} = 1.1$\n  - Tumor replicate $2$: $s_{\\mathrm{T2}} = 0.9$\n  - Normal replicate $1$: $s_{\\mathrm{N1}} = 1.2$\n  - Normal replicate $2$: $s_{\\mathrm{N2}} = 0.8$\n- Raw read counts $x_{ij}$ for each peak $i \\in \\{1,2,3,4,5\\}$ in each library $j \\in \\{\\mathrm{T1}, \\mathrm{T2}, \\mathrm{N1}, \\mathrm{N2}\\}$:\n  - Peak $1$: $\\mathrm{T1}: 500$, $\\mathrm{T2}: 450$, $\\mathrm{N1}: 100$, $\\mathrm{N2}: 90$\n  - Peak $2$: $\\mathrm{T1}: 200$, $\\mathrm{T2}: 210$, $\\mathrm{N1}: 180$, $\\mathrm{N2}: 190$\n  - Peak $3$: $\\mathrm{T1}: 220$, $\\mathrm{T2}: 180$, $\\mathrm{N1}: 80$, $\\mathrm{N2}: 120$\n  - Peak $4$: $\\mathrm{T1}: 300$, $\\mathrm{T2}: 280$, $\\mathrm{N1}: 200$, $\\mathrm{N2}: 180$\n  - Peak $5$: $\\mathrm{T1}: 50$, $\\mathrm{T2}: 55$, $\\mathrm{N1}: 52$, $\\mathrm{N2}: 53$\n- For these five peaks, single-peak raw $p$-values from a Negative Binomial Wald test comparing tumor versus normal have already been computed as:\n  - Peak $1$: $0.0008$\n  - Peak $2$: $0.12$\n  - Peak $3$: $0.007$\n  - Peak $4$: $0.03$\n  - Peak $5$: $0.9$\n\nTasks:\n1. Using library size normalization factors as multiplicative offsets, compute the base-$2$ logarithm of the fold-change for Peak $3$ defined as the base-$2$ logarithm of the ratio of the mean normalized tumor count to the mean normalized normal count. Express the fold-change exactly if possible.\n2. Using the Benjamini–Hochberg (BH) procedure applied over the $m = 5$ hypotheses, compute the BH-adjusted $p$-value for Peak $3$.\n3. Briefly interpret whether Peak $3$ is significantly more accessible in tumor at a False Discovery Rate (FDR) threshold of $0.05$, integrating the sign and magnitude of the log fold-change and the adjusted $p$-value.\n\nImportant: Submit as your final numeric answer only the BH-adjusted $p$-value for Peak $3$ as an exact decimal (no rounding). Do not include units. No other values should appear in the final boxed answer.", "solution": "We begin from core definitions relevant to differential chromatin accessibility analysis for Assay for Transposase-Accessible Chromatin using sequencing (ATAC-seq):\n\n- Normalized counts are obtained by dividing raw counts by library size normalization factors to account for differences in sequencing depth and composition. For peak $i$ in library $j$, the normalized count is $c_{ij} = x_{ij} / s_{j}$.\n- For a two-condition comparison with replicate libraries per condition, the mean normalized count per condition is the arithmetic mean of normalized counts across replicates belonging to that condition.\n- The fold-change is defined as the ratio of the mean normalized count in the tumor condition to that in the normal condition, and the base-$2$ logarithm of the fold-change is $\\log_{2}\\!\\left(\\mathrm{FC}\\right)$, where $\\mathrm{FC}$ is the ratio.\n- For multiple hypothesis testing across $m$ peaks, the Benjamini–Hochberg (BH) procedure adjusts raw $p$-values to control the expected False Discovery Rate (FDR). The procedure ranks the raw $p$-values in ascending order $p_{(1)} \\leq p_{(2)} \\leq \\dots \\leq p_{(m)}$, computes $q_{(i)} = \\frac{m}{i} p_{(i)}$, and then enforces monotonicity by replacing each $q_{(i)}$ with the running minimum from the largest to the smallest rank. The adjusted $p$-value for a given peak is the $q_{(i)}$ corresponding to its rank after applying the monotonicity correction.\n\nStep 1: Compute $\\log_{2}$ fold-change for Peak $3$.\n\nGiven Peak $3$ raw counts and size factors:\n- Tumor replicate $1$: $x_{\\mathrm{P3,T1}} = 220$, $s_{\\mathrm{T1}} = 1.1$, so $c_{\\mathrm{P3,T1}} = \\frac{220}{1.1} = 200$.\n- Tumor replicate $2$: $x_{\\mathrm{P3,T2}} = 180$, $s_{\\mathrm{T2}} = 0.9$, so $c_{\\mathrm{P3,T2}} = \\frac{180}{0.9} = 200$.\n\nMean normalized tumor count:\n$$\n\\bar{c}_{\\mathrm{tumor}} = \\frac{200 + 200}{2} = 200.\n$$\n\n- Normal replicate $1$: $x_{\\mathrm{P3,N1}} = 80$, $s_{\\mathrm{N1}} = 1.2$, so $c_{\\mathrm{P3,N1}} = \\frac{80}{1.2} = \\frac{800}{12} = \\frac{200}{3}$.\n- Normal replicate $2$: $x_{\\mathrm{P3,N2}} = 120$, $s_{\\mathrm{N2}} = 0.8$, so $c_{\\mathrm{P3,N2}} = \\frac{120}{0.8} = 150$.\n\nMean normalized normal count:\n$$\n\\bar{c}_{\\mathrm{normal}} = \\frac{\\frac{200}{3} + 150}{2} = \\frac{\\frac{200}{3} + \\frac{450}{3}}{2} = \\frac{\\frac{650}{3}}{2} = \\frac{650}{6}.\n$$\n\nFold-change:\n$$\n\\mathrm{FC} = \\frac{\\bar{c}_{\\mathrm{tumor}}}{\\bar{c}_{\\mathrm{normal}}} = \\frac{200}{650/6} = 200 \\cdot \\frac{6}{650} = \\frac{1200}{650} = \\frac{24}{13}.\n$$\n\nTherefore,\n$$\n\\log_{2}\\!\\left(\\mathrm{FC}\\right) = \\log_{2}\\!\\left(\\frac{24}{13}\\right).\n$$\n\nThis is an exact symbolic expression for the base-$2$ logarithm of the fold-change.\n\nStep 2: Compute the Benjamini–Hochberg adjusted $p$-value for Peak $3$ across $m = 5$ peaks.\n\nRaw $p$-values:\n- Peak $1$: $0.0008$\n- Peak $2$: $0.12$\n- Peak $3$: $0.007$\n- Peak $4$: $0.03$\n- Peak $5$: $0.9$\n\nOrder the $p$-values in ascending order, keeping track of peak identities:\n$$\np_{(1)} = 0.0008 \\; (\\text{Peak } 1), \\quad\np_{(2)} = 0.007 \\; (\\text{Peak } 3), \\quad\np_{(3)} = 0.03 \\; (\\text{Peak } 4), \\quad\np_{(4)} = 0.12 \\; (\\text{Peak } 2), \\quad\np_{(5)} = 0.9 \\; (\\text{Peak } 5).\n$$\n\nCompute the unadjusted BH quantities $q^{\\ast}_{(i)} = \\frac{m}{i} p_{(i)}$ with $m = 5$:\n- For $i = 1$: $q^{\\ast}_{(1)} = \\frac{5}{1} \\cdot 0.0008 = 0.004$.\n- For $i = 2$: $q^{\\ast}_{(2)} = \\frac{5}{2} \\cdot 0.007 = 0.0175$.\n- For $i = 3$: $q^{\\ast}_{(3)} = \\frac{5}{3} \\cdot 0.03 = 0.05$.\n- For $i = 4$: $q^{\\ast}_{(4)} = \\frac{5}{4} \\cdot 0.12 = 0.15$.\n- For $i = 5$: $q^{\\ast}_{(5)} = \\frac{5}{5} \\cdot 0.9 = 0.9$.\n\nApply the monotonicity correction by taking running minima from largest $i$ to smallest:\n- Start with $i = 5$: $q_{(5)} = 0.9$.\n- $i = 4$: $q_{(4)} = \\min(0.15, 0.9) = 0.15$.\n- $i = 3$: $q_{(3)} = \\min(0.05, 0.15) = 0.05$.\n- $i = 2$: $q_{(2)} = \\min(0.0175, 0.05) = 0.0175$.\n- $i = 1$: $q_{(1)} = \\min(0.004, 0.0175) = 0.004$.\n\nPeak $3$ has rank $i = 2$, so its BH-adjusted $p$-value is $q_{(2)} = 0.0175$.\n\nStep 3: Interpretation at False Discovery Rate (FDR) threshold $0.05$.\n\n- The log fold-change for Peak $3$ is $\\log_{2}\\!\\left(\\frac{24}{13}\\right) > 0$, indicating higher accessibility in tumor than in normal, with magnitude approximately $\\log_{2}(1.846\\ldots) \\approx 0.88$.\n- The BH-adjusted $p$-value for Peak $3$ is $0.0175$, which is less than the FDR threshold $0.05$.\n\nTherefore, Peak $3$ is significantly more accessible in tumor at FDR $0.05$, consistent with potential activation of a regulatory element in the tumor context, which may bear relevance to oncogenic regulatory programs in precision medicine.\n\nThe requested final numeric answer is the BH-adjusted $p$-value for Peak $3$ as an exact decimal.", "answer": "$$\\boxed{0.0175}$$", "id": "4317411"}, {"introduction": "To move from identifying accessible regions to inferring the activity of specific proteins, we can use a high-resolution technique called footprinting, which requires correcting for subtle technical biases. This advanced, hands-on challenge guides you through building a complete, bias-corrected footprinting pipeline from first principles [@problem_id:4317421]. You will implement a workflow that models enzyme bias, denoises the signal, and calculates a quantitative score to predict transcription factor binding, culminating in a rigorous performance evaluation.", "problem": "You are to construct a bias-corrected chromatin footprinting pipeline grounded in first principles appropriate for Assay for Transposase-Accessible Chromatin using sequencing (ATAC-seq) and validate its performance against Chromatin Immunoprecipitation followed by sequencing (ChIP-seq) labels provided as synthetic validation sets. The objective is to translate biological definitions of transposase insertion bias and transcription factor occupancy into a mathematically precise algorithm and evaluate its accuracy.\n\nThe fundamental base to use is the following set of well-tested facts and core definitions:\n\n- In ATAC-seq, transposase Tn5 insertion events at genomic position $i$ can be modeled as draws from a Poisson process with rate parameter $\\lambda b_i \\rho_i$, where $b_i$ is a sequence-dependent bias term and $\\rho_i$ is a depletion factor associated with transcription factor occupancy (with $\\rho_i = 1$ under no occupancy and $\\rho_i \\in (0,1)$ under occupancy). The total expected cut count at position $i$ under the null of no occupancy is therefore proportional to $\\lambda b_i$.\n\n- Maximum likelihood estimation under the Poisson model yields an estimator of the global rate $\\lambda$ from observed counts $O_i$ and known relative biases $b_i$.\n\n- Footprints appear as local depletions of corrected cut density in the motif center relative to flanking regions. A footprint score must therefore quantify the contrast between the center and flanks after correcting for sequence bias and denoising the signal.\n\n- Algorithmic validation against ChIP-seq labels is performed by thresholding the footprint score to produce binary predictions and computing a confusion matrix to obtain a scalar accuracy metric that is invariant to class imbalance.\n\nYour program must implement the following steps for each validation set:\n\n1. Estimate the global rate parameter under the null of no occupancy using observed cut counts $O_i$ and biases $b_i$, and compute the expected counts $E_i$ at each position under the null.\n\n2. Perform sequence bias correction to obtain a corrected signal $C_i$ that reflects deviations from the null, using a numerically stable transformation that respects the Poisson model and preserves nonnegativity.\n\n3. Denoise the corrected signal by convolving $C_i$ with a Gaussian kernel having a specified standard deviation $\\sigma$, producing a smoothed signal $S_i$.\n\n4. For each predefined motif-centered window, compute a footprint score as the contrast between the mean $S_i$ in the flanking regions and the mean $S_i$ in the center.\n\n5. Produce a binary prediction by comparing the footprint score to a specified threshold $\\tau$. Interpret $1$ as predicted occupancy and $0$ as predicted non-occupancy.\n\n6. Compare predictions to provided ground-truth labels from synthetic ChIP-seq validation sets and compute the Matthews correlation coefficient (MCC) using the standard confusion matrix formula, ensuring numerical stability when denominators are zero.\n\nYour program must apply the above pipeline to the following test suite of three synthetic validation sets, each defined in purely mathematical terms. All positions are indexed from $0$ to $L-1$, and all windows are defined by inclusive start and exclusive end indices. The center width for all motif windows is $w_c = 5$ positions. The left flank for a window with center $[s,e)$ is $[\\max(0,s-w_f), s)$ and the right flank is $[e, \\min(L, e+w_f))$, where $w_f$ is specified per test case. If a flank extends beyond boundaries, clip as indicated.\n\nFor each test case, generate $b_i$ and $O_i$ deterministically per the rules below; then run the pipeline and report the MCC.\n\nTest Case A (balanced bias, clear footprints):\n- Length: $L = 60$.\n- Centers: list of five windows with starts $[6, 16, 28, 40, 50]$ and ends $[11, 21, 33, 45, 55]$, i.e., centers $[6,11)$, $[16,21)$, $[28,33)$, $[40,45)$, $[50,55)$.\n- Labels: $\\{1,0,1,0,1\\}$ corresponding to the five centers.\n- Flank width: $w_f = 6$.\n- Gaussian standard deviation: $\\sigma = 1.5$.\n- Stabilizer for numerical safety: $\\epsilon = 10^{-6}$.\n- Bias: $b_i = 1.0 + 0.5\\sin\\left(\\frac{2\\pi i}{20}\\right)$ for all $i \\in \\{0,1,\\dots,59\\}$.\n- Occupancy factor: $f_i = 0.3$ if $i$ lies in any labeled-positive center and $f_i = 1.0$ otherwise.\n- Baseline rate for data generation: $\\lambda_0 = 5$.\n- Deterministic noise: $n_i = 1$ if $i \\bmod 3 = 0$ and $n_i = 0$ otherwise.\n- Observed cuts: $O_i = \\left\\lfloor \\lambda_0 b_i f_i \\right\\rfloor + n_i$.\n- Threshold: $\\tau = 0.15$.\n\nTest Case B (extreme sequence bias in flanks, requiring correction):\n- Length: $L = 60$.\n- Centers: same as Test Case A.\n- Labels: $\\{0,1,0,1,0\\}$.\n- Flank width: $w_f = 6$.\n- Gaussian standard deviation: $\\sigma = 2.0$.\n- Stabilizer: $\\epsilon = 10^{-6}$.\n- Base bias: $\\tilde{b}_i = 0.2 + 0.05\\cos\\left(\\frac{2\\pi i}{10}\\right)$.\n- Flank spike bias: for each of the five centers $[s,e)$, add $2.8$ to bias at positions in the left flank $[\\max(0,s-w_f), s)$ and right flank $[e, \\min(L, e+w_f))$. Thus, $b_i = \\tilde{b}_i + \\sum_{\\text{windows}} \\mathbf{1}\\{i \\in \\text{flanks}\\}\\cdot 2.8$.\n- Occupancy factor: $f_i = 0.25$ if $i$ lies in any labeled-positive center and $f_i = 1.0$ otherwise.\n- Baseline rate: $\\lambda_0 = 4$.\n- Deterministic noise: $n_i = 1$ if $i \\bmod 5 = 0$ and $n_i = 0$ otherwise.\n- Observed cuts: $O_i = \\left\\lfloor \\lambda_0 b_i f_i \\right\\rfloor + n_i$.\n- Threshold: $\\tau = 0.20$.\n\nTest Case C (zero-cut segment, challenging denoising and stability):\n- Length: $L = 60$.\n- Centers: same as Test Case A.\n- Labels: $\\{1,1,0,0,1\\}$.\n- Flank width: $w_f = 6$.\n- Gaussian standard deviation: $\\sigma = 1.0$.\n- Stabilizer: $\\epsilon = 10^{-6}$.\n- Bias: $b_i = 0.7 + 0.3\\sin\\left(\\frac{2\\pi i}{15}\\right)$.\n- Occupancy factor: $f_i = 0.4$ if $i$ lies in any labeled-positive center and $f_i = 1.0$ otherwise.\n- Baseline rate: $\\lambda_0 = 3$.\n- Zero-cut segment: for $i \\in \\{18,19,\\dots,35\\}$, set $O_i = 0$ regardless of bias, occupancy factor, or noise.\n- Deterministic noise outside zero segment: $n_i = 1$ if $i \\bmod 7 = 0$ and $n_i = 0$ otherwise; $n_i = 0$ inside the zero-cut segment.\n- Observed cuts outside the zero-cut segment: $O_i = \\left\\lfloor \\lambda_0 b_i f_i \\right\\rfloor + n_i$.\n- Threshold: $\\tau = 0.12$.\n\nImplementation details required:\n\n- The global rate estimate under the null must be derived from first principles of maximum likelihood under the Poisson model using all positions in a test case. Use this estimate for the expected counts computation.\n\n- The bias correction must be numerically stable and avoid division by zero by means of the stabilizer $\\epsilon$.\n\n- The Gaussian kernel must be properly normalized to sum to $1$ before convolution. You must implement convolution with boundary handling via reflection.\n\n- The footprint score for each window must be computed as the difference between the mean of the smoothed corrected signal $S_i$ in the union of both flanks and the mean in the center.\n\n- For each test case, the predicted labels must be compared to the provided labels, and the Matthews correlation coefficient must be computed via the confusion matrix formula\n$$\n\\mathrm{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\n$$\nwith the convention that if the denominator is $0$, report $\\mathrm{MCC} = 0$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in order, with one floating-point number per test case corresponding to the Matthews correlation coefficient: $[\\mathrm{MCC}_A,\\mathrm{MCC}_B,\\mathrm{MCC}_C]$.\n\n- No other text must be printed.\n\nNo physical units, angle units, or percentages are involved; all quantities are dimensionless. Ensure scientific realism in the design and correct derivation from the stated base. The test suite covers a general case (A), an extreme bias case requiring correction (B), and a numerical stability challenge with zero-count segments (C).", "solution": "The provided problem is scientifically grounded, well-posed, and objective. It outlines a complete, deterministic pipeline for synthetic ATAC-seq data analysis, from first principles to final validation. All necessary parameters and data generation rules are specified, allowing for a unique and verifiable solution. We will therefore proceed with the derivation and implementation of the specified bias-corrected chromatin footprinting algorithm.\n\nThe core of the problem is to translate the biological concept of a transcription factor footprint—a local depletion in transposase cleavage activity—into a quantitative score. The pipeline involves correcting for inherent sequence-specific bias of the Tn5 transposase, denoising the signal, and then calculating a contrast score between a motif's center and its flanking regions.\n\n**Step 1: Maximum Likelihood Estimation of the Global Rate Parameter**\n\nThe problem states that transposase insertion counts $O_i$ at genomic position $i$ can be modeled as draws from a Poisson process. Under the null hypothesis of no protein occupancy ($\\rho_i = 1$ for all $i$), the rate parameter of the Poisson distribution at position $i$ is given by $\\lambda_i = \\lambda b_i$, where $b_i$ is the known relative sequence bias and $\\lambda$ is a global rate parameter reflecting library size and overall chromatin accessibility.\n\nThe probability of observing a count $O_i$ is given by the Poisson probability mass function:\n$$ P(O_i | \\lambda, b_i) = \\frac{(\\lambda b_i)^{O_i} e^{-\\lambda b_i}}{O_i!} $$\nAssuming independence of counts across positions, the likelihood of observing the entire dataset $\\{O_i\\}_{i=0}^{L-1}$ is the product of individual probabilities. It is more convenient to work with the log-likelihood, $\\mathcal{L}(\\lambda)$:\n$$ \\mathcal{L}(\\lambda) = \\ln \\left( \\prod_{i=0}^{L-1} P(O_i | \\lambda, b_i) \\right) = \\sum_{i=0}^{L-1} \\ln(P(O_i | \\lambda, b_i)) $$\n$$ \\mathcal{L}(\\lambda) = \\sum_{i=0}^{L-1} \\left( O_i \\ln(\\lambda b_i) - \\lambda b_i - \\ln(O_i!) \\right) $$\nTo find the maximum likelihood estimate (MLE) of $\\lambda$, denoted $\\hat{\\lambda}$, we differentiate $\\mathcal{L}(\\lambda)$ with respect to $\\lambda$ and set the result to zero:\n$$ \\frac{d\\mathcal{L}}{d\\lambda} = \\sum_{i=0}^{L-1} \\left( \\frac{O_i \\cdot b_i}{\\lambda b_i} - b_i \\right) = \\sum_{i=0}^{L-1} \\left( \\frac{O_i}{\\lambda} - b_i \\right) = 0 $$\nSolving for $\\hat{\\lambda}$ yields:\n$$ \\frac{1}{\\hat{\\lambda}} \\sum_{i=0}^{L-1} O_i = \\sum_{i=0}^{L-1} b_i \\implies \\hat{\\lambda} = \\frac{\\sum_{i=0}^{L-1} O_i}{\\sum_{i=0}^{L-1} b_i} $$\nThis result is intuitive: the best estimate for the global rate is the ratio of the total observed counts to the total integrated bias across the genome. Using this estimate, we compute the expected counts under the null model at each position as $E_i = \\hat{\\lambda} b_i$.\n\n**Step 2: Sequence Bias Correction**\n\nTo isolate the signal component related to protein occupancy, we must correct for the sequence bias encoded in $b_i$. The corrected signal, $C_i$, should reflect the deviation of the observed counts $O_i$ from the expected counts $E_i$. A standard method is to compute a regularized ratio of observed to expected counts. This quantity serves as an empirical estimate of the depletion factor $\\rho_i$. The use of a small stabilizer, $\\epsilon$, prevents division by zero or near-zero values. The corrected signal is thus defined as:\n$$ C_i = \\frac{O_i + \\epsilon}{E_i + \\epsilon} $$\nThis transformation preserves non-negativity. Values of $C_i < 1$ indicate a depletion of cuts relative to expectation (a potential footprint), while $C_i > 1$ indicates enrichment.\n\n**Step 3: Signal Denoising via Gaussian Convolution**\n\nThe corrected signal $C_i$ is a point-wise estimate and is subject to stochastic noise. To obtain a more robust signal, we apply a smoothing filter. The problem specifies convolution with a Gaussian kernel. A discrete, one-dimensional Gaussian kernel $K$ with standard deviation $\\sigma$ is constructed over a finite window of indices $j \\in [-W, W]$, where $W$ is chosen to be sufficiently large (e.g., $W = \\lceil 4\\sigma \\rceil$). The unnormalized kernel is $K_{un}(j) = \\exp\\left(-\\frac{j^2}{2\\sigma^2}\\right)$. This kernel is then normalized to sum to unity:\n$$ K(j) = \\frac{K_{un}(j)}{\\sum_{k=-W}^{W} K_{un}(k)} $$\nThe smoothed signal $S_i$ is the result of the convolution of the corrected signal $C$ with the kernel $K$, denoted $S = C * K$:\n$$ S_i = \\sum_{j=-W}^{W} C_{i-j} K(j) $$\nFor positions near the boundaries of the genomic region (i.e., when $i-j < 0$ or $i-j \\geq L$), the problem specifies that reflection boundary conditions must be used. This involves padding the signal $C$ by reflecting it at its edges.\n\n**Step 4: Footprint Score Calculation**\n\nA footprint is defined by the contrast in signal between a depleted center and its enriched flanks. For a given motif window with a center spanning indices $I_c$ and a set of flanking regions spanning indices $I_f$, the footprint score is calculated as the difference between the mean smoothed signal in the flanks and the mean smoothed signal in the center:\n$$ \\text{Score} = \\left( \\frac{1}{|I_f|} \\sum_{i \\in I_f} S_i \\right) - \\left( \\frac{1}{|I_c|} \\sum_{i \\in I_c} S_i \\right) $$\nA large positive score indicates a strong depletion pattern consistent with a protein footprint.\n\n**Step 5 & 6: Prediction and Performance Evaluation**\n\nA binary prediction of occupancy for each motif window is generated by comparing its footprint score against a specified threshold $\\tau$. If $\\text{Score} > \\tau$, the site is predicted as occupied (label $1$); otherwise, it is predicted as unoccupied (label $0$).\n\nThe accuracy of these predictions is measured against the provided ground-truth labels using the Matthews Correlation Coefficient (MCC). The MCC provides a balanced measure of classifier performance, even with class imbalance. It is calculated from the four elements of the confusion matrix: True Positives ($TP$), True Negatives ($TN$), False Positives ($FP$), and False Negatives ($FN$).\n$$ \\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}} $$\nAs per the problem specification, if the denominator is zero (which occurs if any of the four marginal totals are zero), the MCC is reported as $0$. This completes the full algorithmic pipeline from raw data to performance validation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_data(case_params: dict) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generates the bias (b) and observed counts (O) for a test case.\"\"\"\n    \n    L = case_params['L']\n    centers = case_params['centers']\n    labels = case_params['labels']\n    lambda_0 = case_params['lambda_0']\n    \n    i_coords = np.arange(L)\n    \n    # Generate bias b_i\n    if case_params['id'] == 'A':\n        b = 1.0 + 0.5 * np.sin(2 * np.pi * i_coords / 20)\n    elif case_params['id'] == 'B':\n        w_f = case_params['w_f']\n        b = 0.2 + 0.05 * np.cos(2 * np.pi * i_coords / 10)\n        flank_mask = np.zeros(L, dtype=bool)\n        for s, e in centers:\n            left_flank_start = max(0, s - w_f)\n            right_flank_end = min(L, e + w_f)\n            flank_mask[left_flank_start:s] = True\n            flank_mask[e:right_flank_end] = True\n        b[flank_mask] += 2.8\n    elif case_params['id'] == 'C':\n        b = 0.7 + 0.3 * np.sin(2 * np.pi * i_coords / 15)\n    else:\n        raise ValueError(\"Unknown case ID\")\n\n    # Generate occupancy factor f_i\n    f = np.ones(L)\n    positive_labels_indices = np.where(np.array(labels) == 1)[0]\n    for idx in positive_labels_indices:\n        s, e = centers[idx]\n        f[s:e] = case_params['f_i_occ']\n\n    # Generate noise n_i\n    if case_params['id'] == 'A':\n        n = (i_coords % 3 == 0).astype(int)\n    elif case_params['id'] == 'B':\n        n = (i_coords % 5 == 0).astype(int)\n    elif case_params['id'] == 'C':\n        n = (i_coords % 7 == 0).astype(int)\n        n[18:36] = 0\n    else:\n        raise ValueError(\"Unknown case ID\")\n\n    # Generate observed cuts O_i\n    O = np.floor(lambda_0 * b * f) + n\n    O = O.astype(int)\n    \n    if case_params['id'] == 'C':\n        O[18:36] = 0\n\n    return b, O\n\ndef convolve_with_reflection(signal: np.ndarray, kernel: np.ndarray) -> np.ndarray:\n    \"\"\"Performs 1D convolution with reflection padding.\"\"\"\n    L = len(signal)\n    kernel_width = len(kernel)\n    pad_width = (kernel_width - 1) // 2\n\n    # Create padded signal with reflection\n    # Left pad: signal[pad_width-1:0:-1] is almost right, but needs to include 0.\n    # a[p-1::-1] gives a[p-1], a[p-2], ..., a[0]\n    left_pad = signal[pad_width:0:-1]\n    right_pad = signal[L-2:L-pad_width-2:-1]\n    \n    padded_signal = np.concatenate([left_pad, signal, right_pad])\n    \n    # SciPy's convolve with 'valid' mode is equivalent to a manual loop over the original signal positions\n    # NumPy's convolve is used here as per constraints\n    return np.convolve(padded_signal, kernel, mode='valid')\n\ndef run_pipeline(case_params: dict) -> float:\n    \"\"\"Runs the full analysis pipeline for a single test case.\"\"\"\n    \n    b, O = generate_data(case_params)\n    \n    L = case_params['L']\n    centers = case_params['centers']\n    labels = np.array(case_params['labels'])\n    w_f = case_params['w_f']\n    sigma = case_params['sigma']\n    epsilon = case_params['epsilon']\n    tau = case_params['tau']\n    \n    # 1. Estimate global rate and expected counts\n    lambda_hat = np.sum(O) / np.sum(b)\n    E = lambda_hat * b\n    \n    # 2. Bias correction\n    C = (O + epsilon) / (E + epsilon)\n    \n    # 3. Denoising with Gaussian kernel\n    kernel_half_width = int(np.ceil(4 * sigma))\n    kernel_x = np.arange(-kernel_half_width, kernel_half_width + 1)\n    kernel = np.exp(-kernel_x**2 / (2 * sigma**2))\n    kernel /= np.sum(kernel)\n    \n    S = convolve_with_reflection(C, kernel)\n    \n    # 4. Compute footprint scores\n    scores = []\n    w_c = 5 # center width is fixed\n    for s, e in centers:\n        if e - s != w_c:\n            # This should not happen with the given data but is a good sanity check\n            raise ValueError(f\"Center width is not {w_c}\")\n            \n        center_indices = np.arange(s, e)\n        \n        left_flank_start = max(0, s - w_f)\n        right_flank_end = min(L, e + w_f)\n        flank_indices = np.concatenate([\n            np.arange(left_flank_start, s),\n            np.arange(e, right_flank_end)\n        ]).astype(int)\n\n        if len(flank_indices) == 0:\n            mean_flank = 0.0\n        else:\n            mean_flank = np.mean(S[flank_indices])\n            \n        mean_center = np.mean(S[center_indices])\n        \n        score = mean_flank - mean_center\n        scores.append(score)\n        \n    scores = np.array(scores)\n    \n    # 5. Produce binary predictions\n    predictions = (scores > tau).astype(int)\n    \n    # 6. Compute Matthews Correlation Coefficient (MCC)\n    TP = np.sum((predictions == 1) & (labels == 1))\n    TN = np.sum((predictions == 0) & (labels == 0))\n    FP = np.sum((predictions == 1) & (labels == 0))\n    FN = np.sum((predictions == 0) & (labels == 1))\n    \n    numerator = TP * TN - FP * FN\n    denominator_sq = (TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)\n    \n    if denominator_sq == 0:\n        return 0.0\n    \n    mcc = numerator / np.sqrt(denominator_sq)\n    return mcc\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    \n    centers_list = [\n        (6, 11), (16, 21), (28, 33), (40, 45), (50, 55)\n    ]\n    \n    test_cases = [\n        {\n            'id': 'A', 'L': 60, 'centers': centers_list, 'labels': [1,0,1,0,1],\n            'w_f': 6, 'sigma': 1.5, 'epsilon': 1e-6, 'tau': 0.15,\n            'f_i_occ': 0.3, 'lambda_0': 5,\n        },\n        {\n            'id': 'B', 'L': 60, 'centers': centers_list, 'labels': [0,1,0,1,0],\n            'w_f': 6, 'sigma': 2.0, 'epsilon': 1e-6, 'tau': 0.20,\n            'f_i_occ': 0.25, 'lambda_0': 4,\n        },\n        {\n            'id': 'C', 'L': 60, 'centers': centers_list, 'labels': [1,1,0,0,1],\n            'w_f': 6, 'sigma': 1.0, 'epsilon': 1e-6, 'tau': 0.12,\n            'f_i_occ': 0.4, 'lambda_0': 3,\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mcc_result = run_pipeline(case)\n        results.append(mcc_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "4317421"}]}