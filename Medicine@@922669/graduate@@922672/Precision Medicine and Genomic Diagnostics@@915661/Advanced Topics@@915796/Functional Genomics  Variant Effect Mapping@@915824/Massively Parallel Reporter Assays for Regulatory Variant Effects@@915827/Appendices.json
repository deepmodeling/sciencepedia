{"hands_on_practices": [{"introduction": "The raw output of a Massively Parallel Reporter Assay (MPRA) is sequencing counts for DNA and RNA barcodes. To make sense of this data, we must first convert it into a standardized measure of regulatory activity. This exercise [@problem_id:4357321] introduces the fundamental calculation at the heart of MPRA analysis: normalizing RNA output by DNA input on a logarithmic scale to quantify activity and compare the effects of different alleles.", "problem": "A massively parallel reporter assay (MPRA) quantifies regulatory activity by coupling candidate regulatory sequences to unique barcodes, inserting them into a plasmid library, delivering the library into cells, and sequencing both input deoxyribonucleic acid (DNA) barcodes and output ribonucleic acid (RNA) barcodes. Under the Central Dogma and standard reporter assay design, the transcriptional output attributed to a regulatory sequence is reflected in the abundance of its RNA barcodes, while the input dosage is reflected in the abundance of its DNA barcodes. A core, well-tested practice in MPRA analysis is to define the normalized activity for a sequence as the base-2 logarithm of the ratio of RNA barcode counts to DNA barcode counts, thereby converting multiplicative regulatory effects into additive quantities. The allelic fold-change on the base-2 logarithmic scale is then captured by the difference in normalized activities between alleles.\n\nConsider an assay comparing two alleles of a regulatory variant, measured in the same pool so that library size factors and sequencing depth effects are shared. Assume barcode aggregation has already been performed and that there are no zero counts requiring pseudocounts. For allele $1$, the aggregated RNA barcode count is $300$ and the aggregated DNA barcode count is $100$. For allele $2$, the aggregated RNA barcode count is $150$ and the aggregated DNA barcode count is $150$.\n\nUsing the definitions above, compute the normalized activities $A_{1}$ and $A_{2}$ and the base-2 logarithmic allelic fold-change $\\Delta$ defined as the difference in normalized activities between allele $1$ and allele $2$. Express your answers in exact closed form using base-2 logarithms; do not approximate numerically. No units are required.", "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- The normalized activity for a sequence is defined as the base-2 logarithm of the ratio of RNA barcode counts to DNA barcode counts.\n- The allelic fold-change on the base-2 logarithmic scale, $\\Delta$, is defined as the difference in normalized activities between allele $1$ and allele $2$.\n- For allele $1$: aggregated RNA barcode count is $300$; aggregated DNA barcode count is $100$.\n- For allele $2$: aggregated RNA barcode count is $150$; aggregated DNA barcode count is $150$.\n- Assumption: No zero counts requiring pseudocounts.\n- Required computation: Normalized activities $A_{1}$ and $A_{2}$, and the base-2 logarithmic allelic fold-change $\\Delta = A_1 - A_2$.\n- Required format: Exact closed form using base-2 logarithms.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n- **Scientifically Grounded**: The problem is firmly rooted in the standard practices of massively parallel reporter assay (MPRA) data analysis. The definition of normalized activity as a log-ratio of RNA to DNA counts is a standard method to quantify regulatory function, and calculating the difference between alleles (log fold-change) is the conventional way to quantify a variant's effect. The experimental setup described is a canonical MPRA design.\n- **Well-Posed**: The problem is well-posed. It provides all necessary numerical data and clear, unambiguous definitions for the quantities to be calculated. A unique solution exists and can be determined directly from the provided information.\n- **Objective**: The problem statement is objective and uses precise, technical language common to the field of genomics. It contains no subjective or opinion-based content.\n\nThe problem is free of the listed flaws. It is scientifically sound, fully specified, internally consistent, and requires a direct application of the provided definitions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\nThe normalized activity, $A$, is defined as the base-2 logarithm of the ratio of RNA counts to DNA counts. Let $C_{RNA}$ be the RNA barcode count and $C_{DNA}$ be the DNA barcode count. The formula is:\n$$A = \\log_{2}\\left(\\frac{C_{RNA}}{C_{DNA}}\\right)$$\n\nWe are given the counts for two alleles, designated as allele $1$ and allele $2$.\nFor allele $1$, the counts are $C_{RNA,1} = 300$ and $C_{DNA,1} = 100$.\nThe normalized activity for allele $1$, $A_1$, is calculated as:\n$$A_{1} = \\log_{2}\\left(\\frac{300}{100}\\right) = \\log_{2}(3)$$\n\nFor allele $2$, the counts are $C_{RNA,2} = 150$ and $C_{DNA,2} = 150$.\nThe normalized activity for allele $2$, $A_2$, is calculated as:\n$$A_{2} = \\log_{2}\\left(\\frac{150}{150}\\right) = \\log_{2}(1)$$\nBy the properties of logarithms, $\\log_{b}(1) = 0$ for any base $b > 0, b \\neq 1$. Therefore:\n$$A_{2} = 0$$\n\nThe base-2 logarithmic allelic fold-change, $\\Delta$, is defined as the difference in normalized activities between allele $1$ and allele $2$:\n$$\\Delta = A_{1} - A_{2}$$\nSubstituting the calculated values for $A_1$ and $A_2$:\n$$\\Delta = \\log_{2}(3) - 0 = \\log_{2}(3)$$\n\nThus, the normalized activity for allele $1$ is $\\log_{2}(3)$, the normalized activity for allele $2$ is $0$, and the base-2 logarithmic allelic fold-change is $\\log_{2}(3)$. The problem requests these three values, $A_1$, $A_2$, and $\\Delta$, in exact closed form.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\log_{2}(3) & 0 & \\log_{2}(3)\n\\end{pmatrix}\n}\n$$", "id": "4357321"}, {"introduction": "While direct calculation of log-ratios provides an intuitive starting point, a rigorous analysis of MPRA data requires a statistical model that can properly handle the count-based nature of sequencing data and account for variability across replicates. This practice [@problem_id:4357264] delves into the use of Generalized Linear Models (GLMs), the standard framework in the field, to estimate allelic effects. Understanding how to correctly formulate and interpret these models is essential for extracting reliable biological insights from complex MPRA datasets.", "problem": "A Massively Parallel Reporter Assay (MPRA) measures regulatory activity of many allelic variants in parallel by sequencing barcoded constructs. For each barcode-indexed construct $i$, you observe deoxyribonucleic acid (DNA) input counts $d_i$ (reflecting plasmid abundance) and ribonucleic acid (RNA) output counts $y_i$ (reflecting transcriptional activity). You wish to estimate the allelic regulatory effect of the alternative allele relative to the reference allele while accounting for variation in DNA input. Let $\\text{allele}_i \\in \\{0,1\\}$ indicate the allele carried by construct $i$ ($0$ for reference, $1$ for alternative).\n\nStarting from the following fundamental bases:\n- Central Dogma and reporter assays: RNA output from a reporter construct is an observable proxy for transcriptional activity driven by its regulatory sequence, and DNA input abundance multiplicatively scales expected RNA counts when all else is equal.\n- Generalized Linear Model (GLM) definition: A GLM specifies a distribution for the response with mean $E[y_i \\mid \\mathbf{x}_i] = \\mu_i$, a link function $g(\\mu_i)$ that is a linear function of predictors, and a variance function governed by a dispersion parameter.\n- Negative Binomial distribution for sequencing counts: Under a Negative Binomial (NB) model, $y_i \\sim \\text{NB}(\\mu_i,\\phi)$ with mean $E[y_i]=\\mu_i$ and variance $\\text{Var}(y_i) = \\mu_i + \\phi \\mu_i^2$ for an overdispersion parameter $\\phi>0$.\n\nYou fit the GLM with log link and an offset for DNA input,\n$$\n\\log(\\mu_i) = \\beta_0 + \\beta_1 \\cdot \\text{allele}_i + \\log(d_i),\n$$\nwhere $\\log(d_i)$ enters the linear predictor as an offset rather than an estimated coefficient.\n\nUsing only the core GLM structure, the offset property of the log link, and the NB mean-variance characterization, derive the quantity that $\\beta_1$ estimates in terms of expected RNA activity normalized by DNA input, and state how to interpret $\\exp(\\beta_1)$ as an allelic effect in this MPRA context.\n\nWhich option best captures the correct interpretation of $\\beta_1$ (and $\\exp(\\beta_1)$) under this model?\n\nA. $\\exp(\\beta_1)$ is the multiplicative change in the expected RNA count per unit DNA input for the alternative allele relative to the reference allele; equivalently, it is the ratio of expected RNA-to-DNA means $E[y_i]/d_i$ comparing $\\text{allele}_i=1$ to $\\text{allele}_i=0$.\n\nB. $\\beta_1$ quantifies how overdispersion changes with allele; it measures the change in $\\phi$ attributable to the alternative allele, thereby reflecting variability rather than mean activity.\n\nC. $\\exp(\\beta_1)$ is the odds ratio of observing the alternative allele in RNA relative to DNA, analogous to a logistic regression effect, because the link is a log function.\n\nD. $\\beta_1$ is the absolute change in the expected RNA count $E[y_i]$ when switching from $\\text{allele}_i=0$ to $\\text{allele}_i=1$, holding $d_i$ fixed, and is independent of baseline activity $\\beta_0$.\n\nE. $\\exp(\\beta_1)$ is the multiplicative change in raw expected RNA counts $E[y_i]$ comparing $\\text{allele}_i=1$ to $\\text{allele}_i=0$, ignoring the DNA input offset $d_i$ because the NB mean already accounts for scaling.", "solution": "The user has provided a problem statement describing a Generalized Linear Model (GLM) for analyzing Massively Parallel Reporter Assay (MPRA) data. The task is to validate this statement and, if valid, derive the correct interpretation of the model parameter $\\beta_1$ and its exponentiated form, $\\exp(\\beta_1)$.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Data:** For each construct $i$, we have DNA input counts $d_i$ and RNA output counts $y_i$.\n*   **Predictor Variable:** $\\text{allele}_i \\in \\{0,1\\}$ indicates the allele, with $0$ for reference and $1$ for alternative.\n*   **Contextual Principles:**\n    1.  RNA output is a proxy for transcriptional activity, scaled multiplicatively by DNA input.\n    2.  The model is a GLM with mean $E[y_i \\mid \\mathbf{x}_i] = \\mu_i$ and a link function $g(\\mu_i)$ that is linear in predictors.\n    3.  The response $y_i$ follows a Negative Binomial distribution, $y_i \\sim \\text{NB}(\\mu_i, \\phi)$, with mean $E[y_i]=\\mu_i$ and variance $\\text{Var}(y_i) = \\mu_i + \\phi \\mu_i^2$, where $\\phi > 0$ is the overdispersion parameter.\n*   **Model Specification:** The GLM is defined by the equation:\n    $$\n    \\log(\\mu_i) = \\beta_0 + \\beta_1 \\cdot \\text{allele}_i + \\log(d_i)\n    $$\n    where $\\mu_i = E[y_i]$ and $\\log(d_i)$ is an offset.\n\n**Step 2: Validate Using Extracted Givens**\n\n*   **Scientific Grounding:** The problem is firmly grounded in the principles of quantitative genomics and biostatistics. Using a Negative Binomial GLM to model count data from high-throughput sequencing experiments like MPRA is a standard and well-established method. The model structure, which includes an offset for input library size (here, DNA plasmid abundance), is the canonical approach for analyzing such data (e.g., in software packages like DESeq2 and edgeR). The stated principles are scientifically sound.\n*   **Well-Posedness:** The problem is well-posed. It provides a precise mathematical model and asks for the interpretation of one of its parameters, $\\beta_1$. This interpretation can be uniquely derived from the given equations and definitions.\n*   **Objectivity:** The problem statement is expressed in clear, objective, and standard technical language, free from ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is scientifically sound, well-posed, and objective. It does not violate any of the invalidity criteria. Therefore, the problem is **valid**. I will proceed with the derivation and solution.\n\n### Derivation and Solution\n\nThe objective is to derive the meaning of the parameter $\\beta_1$ from the specified GLM. The model is:\n$$\n\\log(\\mu_i) = \\beta_0 + \\beta_1 \\cdot \\text{allele}_i + \\log(d_i)\n$$\nwhere $\\mu_i$ is the expected RNA count, $E[y_i]$.\n\nThe problem highlights the use of an offset and the log link. A key property of the log link is that an offset in the linear predictor corresponds to a multiplicative scaling factor on the mean scale. We can see this by rearranging the equation:\n$$\n\\log(\\mu_i) - \\log(d_i) = \\beta_0 + \\beta_1 \\cdot \\text{allele}_i\n$$\nUsing the property of logarithms, $\\log(a) - \\log(b) = \\log(a/b)$, we get:\n$$\n\\log\\left(\\frac{\\mu_i}{d_i}\\right) = \\beta_0 + \\beta_1 \\cdot \\text{allele}_i\n$$\nThis rearranged equation provides the most direct interpretation. The quantity on the left, $\\mu_i/d_i = E[y_i]/d_i$, represents the expected RNA count per unit of DNA input. This ratio is a measure of the intrinsic transcriptional activity of the regulatory sequence, normalized for its initial abundance in the plasmid pool.\n\nLet's analyze the model for the two allele types:\n\n1.  **For the reference allele ($\\text{allele}_i = 0$):**\n    The model becomes:\n    $$\n    \\log\\left(\\frac{E[y_i | \\text{allele}_i=0]}{d_i}\\right) = \\beta_0 + \\beta_1 \\cdot 0 = \\beta_0\n    $$\n    Thus, $\\exp(\\beta_0)$ represents the baseline transcriptional activity (expected RNA per DNA) of the reference allele.\n\n2.  **For the alternative allele ($\\text{allele}_i = 1$):**\n    The model becomes:\n    $$\n    \\log\\left(\\frac{E[y_i | \\text{allele}_i=1]}{d_i}\\right) = \\beta_0 + \\beta_1 \\cdot 1 = \\beta_0 + \\beta_1\n    $$\n\nTo interpret $\\beta_1$, we can subtract the equation for the reference allele from the equation for the alternative allele:\n$$\n\\log\\left(\\frac{E[y_i|\\text{allele}_i=1]}{d_i}\\right) - \\log\\left(\\frac{E[y_i|\\text{allele}_i=0]}{d_i}\\right) = (\\beta_0 + \\beta_1) - \\beta_0 = \\beta_1\n$$\nApplying the logarithm subtraction rule again:\n$$\n\\beta_1 = \\log\\left( \\frac{E[y_i|\\text{allele}_i=1]/d_i}{E[y_i|\\text{allele}_i=0]/d_i} \\right)\n$$\nThis shows that $\\beta_1$ is the natural logarithm of the ratio of the normalized transcriptional activities (expected RNA per DNA) of the alternative allele relative to the reference allele.\n\nTo interpret $\\exp(\\beta_1)$, we simply exponentiate both sides:\n$$\n\\exp(\\beta_1) = \\frac{E[y_i|\\text{allele}_i=1]/d_i}{E[y_i|\\text{allele}_i=0]/d_i}\n$$\nTherefore, $\\exp(\\beta_1)$ is the ratio of the expected RNA-to-DNA means, comparing the alternative allele to the reference allele. This is precisely the multiplicative change in the expected RNA count per unit of DNA input for the alternative allele relative to the reference. This quantity is often referred to as the allelic skew or allelic effect on regulatory activity.\n\n### Option-by-Option Analysis\n\n**A. $\\exp(\\beta_1)$ is the multiplicative change in the expected RNA count per unit DNA input for the alternative allele relative to the reference allele; equivalently, it is the ratio of expected RNA-to-DNA means $E[y_i]/d_i$ comparing $\\text{allele}_i=1$ to $\\text{allele}_i=0$.**\nThis statement perfectly matches our derivation. The expression $\\exp(\\beta_1) = \\frac{E[y_i|\\text{allele}_i=1]/d_i}{E[y_i|\\text{allele}_i=0]/d_i}$ shows that it is the ratio of the expected RNA-to-DNA means, which is synonymous with the multiplicative change in the expected RNA count per unit DNA input.\n**Verdict: Correct.**\n\n**B. $\\beta_1$ quantifies how overdispersion changes with allele; it measures the change in $\\phi$ attributable to the alternative allele, thereby reflecting variability rather than mean activity.**\nThis is incorrect. The parameter $\\beta_1$ is part of the linear predictor for the mean, $\\mu_i$. The overdispersion parameter, $\\phi$, is part of the variance function, $\\text{Var}(y_i) = \\mu_i + \\phi\\mu_i^2$. In the specified model, $\\phi$ is treated as a single, constant parameter and is not dependent on any predictors, including $\\text{allele}_i$. Therefore, $\\beta_1$ models the effect on the mean, not the variance or overdispersion.\n**Verdict: Incorrect.**\n\n**C. $\\exp(\\beta_1)$ is the odds ratio of observing the alternative allele in RNA relative to DNA, analogous to a logistic regression effect, because the link is a log function.**\nThis is incorrect on multiple levels. First, the model is a Negative Binomial regression for count data, not a logistic regression for binary outcomes. Odds ratios are the natural interpretation of coefficients in a logistic regression (which uses a logit link), not a NB regression with a log link. Second, the quantity $\\exp(\\beta_1)$ is a ratio of means (or rates), not a ratio of odds. Third, the phrasing \"odds ratio of observing the alternative allele in RNA relative to DNA\" is conceptually incoherent in this context.\n**Verdict: Incorrect.**\n\n**D. $\\beta_1$ is the absolute change in the expected RNA count $E[y_i]$ when switching from $\\text{allele}_i=0$ to $\\text{allele}_i=1$, holding $d_i$ fixed, and is independent of baseline activity $\\beta_0$.**\nThis is incorrect. Because the model uses a log link, the predictors have a multiplicative, not additive, effect on the mean scale, $E[y_i]$. The absolute change in $E[y_i]$ is $E[y | \\text{allele}=1, d_i] - E[y | \\text{allele}=0, d_i] = d_i\\exp(\\beta_0)(\\exp(\\beta_1)-1)$. This change is not equal to $\\beta_1$ and clearly depends on both the baseline activity ($\\beta_0$) and the DNA input ($d_i$).\n**Verdict: Incorrect.**\n\n**E. $\\exp(\\beta_1)$ is the multiplicative change in raw expected RNA counts $E[y_i]$ comparing $\\text{allele}_i=1$ to $\\text{allele_i=0}$, ignoring the DNA input offset $d_i$ because the NB mean already accounts for scaling.**\nThis statement is misleading and its reasoning is flawed. While it is true that for a fixed $d_i$, $\\exp(\\beta_1) = \\frac{E[y_i|\\text{allele}=1, d_i]}{E[y_i|\\text{allele}=0, d_i]}$, the role of the offset is not to be ignored. The offset is precisely the mechanism by which the model accounts for the scaling effect of $d_i$. The model *corrects for* $d_i$, it does not ignore it. The reasoning \"because the NB mean already accounts for scaling\" is incorrect; the NB mean itself does not account for this scaling—the full GLM structure including the offset imposes this relationship. Option A provides a much more accurate and fundamental interpretation by focusing on the normalized activity, which is what the model is designed to estimate.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "4357264"}, {"introduction": "The ultimate goal of characterizing regulatory variants is often to prioritize them for further investigation in a clinical or research context. MPRA results provide a powerful line of functional evidence, but they are most effective when integrated with other data types, such as genetic association and expression Quantitative Trait Loci (eQTL) data. This final practice [@problem_id:4357293] challenges you to build a principled Bayesian scoring function that combines multiple lines of evidence to rank variants, reflecting a key task in modern precision medicine.", "problem": "You are designing a principled, programmatically implementable scoring function to rank noncoding genetic variants for clinical follow-up using data from Massively Parallel Reporter Assay (MPRA) measurements and expression Quantitative Trait Loci (eQTL) colocalization results. Your goal is to derive, from first principles, a scalar score that is monotone in the posterior probability that a variant is clinically relevant as a regulatory variant affecting gene expression in a disease-relevant context, and then to rank variants by their expected clinical utility.\n\nFundamental bases to use:\n- Bayes’ theorem for combining prior odds and likelihood ratios.\n- Independence of evidence sources implies additivity of log-likelihood ratios.\n- Gaussian measurement model for MPRA effect sizes.\n- The logit and logistic functions for mapping between probabilities, odds, and log-odds.\n- Decision theory principle that expected utility is the product of utility and posterior probability when utility is realized only if the hypothesis is true.\n\nSet up the following latent binary hypotheses per variant: hypothesis $\\mathcal{H}_1$ denotes that the variant is a clinically relevant regulatory variant; hypothesis $\\mathcal{H}_0$ denotes that it is not. You are given two evidence sources per variant:\n- An MPRA effect estimate $e$ and its standard error $s$, where under a Gaussian measurement model, the observation is modeled by a Normal distribution.\n- An eQTL colocalization posterior probability $c$ that the same variant underlies both the eQTL and the disease association.\n\nAssume the following modeling choices:\n- Under $\\mathcal{H}_0$, the MPRA effect $e$ is distributed as a Normal distribution with mean $0$ and variance $s^2$.\n- Under $\\mathcal{H}_1$, the MPRA effect $e$ is distributed as a Normal distribution with mean $\\mu_A$ and variance $s^2 + \\tau^2$, where $\\mu_A$ and $\\tau$ are hyperparameters representing the expected shift and additional between-variant variance in the functional state.\n- The eQTL colocalization probability $c$ is treated as a posterior probability about colocalization, and the contribution of this evidence to the log-likelihood ratio can be represented by the difference between its posterior log-odds and a baseline prior log-odds for colocalization, with baseline prior probability $\\pi_c$.\n- The prior probability that any given variant in the locus panel is clinically relevant is $\\pi_0$.\n- For numerical stability, probabilities are clipped to the open interval $(\\epsilon, 1 - \\epsilon)$ before applying log-odds transformations, where $\\epsilon$ is a small positive constant.\n- Each variant has a nonnegative clinical utility weight $u$ representing the relative clinical value of validating that variant if it is truly relevant.\n\nYour task:\n- Derive a scalar score that is monotone in the posterior probability $\\Pr(\\mathcal{H}_1 \\mid \\text{data})$ by combining the MPRA and colocalization evidence using Bayes’ theorem and the independence assumption. Then, define the expected clinical utility score for ranking as $u \\times \\Pr(\\mathcal{H}_1 \\mid \\text{data})$.\n- Implement this as a program that, for each test case, takes arrays of $e$, $s$, $c$, and $u$ along with the constants $\\mu_A$, $\\tau$, $\\pi_c$, $\\pi_0$, and $\\epsilon$, and returns the zero-based index of the top-ranked variant that maximizes expected utility. In the event of exact numerical ties up to a tolerance $\\delta$, return the smallest index among the maximizers. Use tolerance $\\delta = 10^{-12}$.\n\nTest suite:\nImplement your method on the following four test cases. For each case, compute the single integer index of the top-ranked variant and output the list of the four indices in order.\n\nConstants (used in all test cases):\n- $\\mu_A = 0.5$\n- $\\tau = 0.3$\n- $\\pi_c = 0.2$\n- $\\pi_0 = 0.01$\n- $\\epsilon = 10^{-6}$\n- $\\delta = 10^{-12}$\n\nCase $1$ (general case):\n- $e = [0.4, 1.2, -0.1, 0.8, 0.0]$\n- $s = [0.2, 0.5, 0.3, 0.4, 0.1]$\n- $c = [0.6, 0.4, 0.9, 0.2, 0.5]$\n- $u = [1, 1, 1, 1, 1]$\n\nCase $2$ (edge probabilities and precisions):\n- $e = [0.0, 3.0, 0.1]$\n- $s = [$10^6$, 1.0, 0.05]$\n- $c = [0.0, 1.0, 0.01]$\n- $u = [1, 1, 1]$\n\nCase $3$ (exact tie, exercise tie-breaking):\n- $e = [0.5, 0.5]$\n- $s = [0.2, 0.2]$\n- $c = [0.5, 0.5]$\n- $u = [1, 1]$\n\nCase $4$ (different utilities with identical evidence):\n- $e = [0.6, 0.6, 0.6]$\n- $s = [0.3, 0.3, 0.3]$\n- $c = [0.6, 0.6, 0.6]$\n- $u = [1.0, 5.0, 0.5]$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces. For example, an output for four cases should look like \"[0,2,1,3]\". Your program must implement the derivation described above and apply it to the test suite exactly as specified, returning a list of four integers in the specified format.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It provides a complete and consistent set of requirements for deriving and implementing a statistical model for ranking genetic variants.\n\nThe central task is to rank genetic variants by their expected clinical utility. The expected utility for a variant $i$, denoted $U_i$, is defined as the product of its clinical utility weight, $u_i$, and its posterior probability of being a clinically relevant regulatory variant, $\\Pr(\\mathcal{H}_1 \\mid \\text{data}_i)$.\n$$ U_i = u_i \\times \\Pr(\\mathcal{H}_1 \\mid \\text{data}_i) $$\nHere, $\\mathcal{H}_1$ is the hypothesis that the variant is clinically relevant, and $\\mathcal{H}_0$ is the null hypothesis that it is not. The data for each variant consists of a Massively Parallel Reporter Assay (MPRA) effect estimate $e_i$ with standard error $s_i$, and an expression Quantitative Trait Locus (eQTL) colocalization posterior probability $c_i$.\n\nTo calculate the posterior probability $\\Pr(\\mathcal{H}_1 \\mid \\text{data}_i)$, we employ Bayes' theorem. It is most convenient to work with log-odds, as the problem assumes independence of the evidence sources (MPRA and eQTL), which makes their contributions additive in the log-odds space.\n\nThe posterior log-odds of $\\mathcal{H}_1$ given the data is the sum of the prior log-odds and the log-likelihood ratios (LLR) from each evidence source:\n$$ \\underbrace{\\log\\left(\\frac{\\Pr(\\mathcal{H}_1 \\mid \\text{data}_i)}{\\Pr(\\mathcal{H}_0 \\mid \\text{data}_i)}\\right)}_{\\text{Posterior log-odds}} = \\underbrace{\\log\\left(\\frac{\\Pr(\\mathcal{H}_1)}{\\Pr(\\mathcal{H}_0)}\\right)}_{\\text{Prior log-odds}} + \\underbrace{\\log\\left(\\frac{p(e_i, s_i \\mid \\mathcal{H}_1)}{p(e_i, s_i \\mid \\mathcal{H}_0)}\\right)}_{\\text{LLR}_{\\text{MPRA}}} + \\underbrace{\\log\\left(\\frac{p(\\text{evidence from } c_i \\mid \\mathcal{H}_1)}{p(\\text{evidence from } c_i \\mid \\mathcal{H}_0)}\\right)}_{\\text{LLR}_{\\text{eQTL}}} $$\n\nLet's define each term based on the problem statement. For numerical stability, probabilities $p$ are clipped to the interval $(\\epsilon, 1 - \\epsilon)$ before applying the logit transformation, $\\text{logit}(p) = \\log(p/(1-p))$.\n\n1.  **Prior Log-Odds**: The prior probability of a variant being clinically relevant is $\\pi_0$. The prior log-odds term is calculated from the clipped prior probability $\\pi'_0 = \\text{clip}(\\pi_0, \\epsilon, 1-\\epsilon)$:\n    $$ \\text{Prior log-odds} = \\text{logit}(\\pi'_0) = \\log\\left(\\frac{\\pi'_0}{1-\\pi'_0}\\right) $$\n\n2.  **eQTL Log-Likelihood Ratio ($\\text{LLR}_{\\text{eQTL}}$)**: The problem states that this contribution is the difference between the posterior log-odds of colocalization (given by $c_i$) and the prior log-odds of colocalization (given by $\\pi_c$). Let $c'_i = \\text{clip}(c_i, \\epsilon, 1-\\epsilon)$ and $\\pi'_c = \\text{clip}(\\pi_c, \\epsilon, 1-\\epsilon)$.\n    $$ \\text{LLR}_{\\text{eQTL}, i} = \\text{logit}(c'_i) - \\text{logit}(\\pi'_c) = \\log\\left(\\frac{c'_i}{1-c'_i}\\right) - \\log\\left(\\frac{\\pi'_c}{1-\\pi'_c}\\right) $$\n    This term quantifies the evidence for $\\mathcal{H}_1$ provided by the eQTL colocalization analysis.\n\n3.  **MPRA Log-Likelihood Ratio ($\\text{LLR}_{\\text{MPRA}}$)**: This term is derived from the specified Gaussian measurement models. The likelihood of observing an effect size $e_i$ with standard error $s_i$ is given by the probability density function (PDF) of a Normal distribution, $\\mathcal{N}(\\cdot \\mid \\text{mean}, \\text{variance})$.\n    -   Under $\\mathcal{H}_0$: $p(e_i \\mid s_i, \\mathcal{H}_0) = \\mathcal{N}(e_i \\mid 0, s_i^2)$\n    -   Under $\\mathcal{H}_1$: $p(e_i \\mid s_i, \\mathcal{H}_1) = \\mathcal{N}(e_i \\mid \\mu_A, s_i^2 + \\tau^2)$\n    The LLR is the logarithm of the ratio of these two likelihoods. We can compute it as the difference of the log-PDFs:\n    $$ \\text{LLR}_{\\text{MPRA}, i} = \\log\\left(p(e_i \\mid s_i, \\mathcal{H}_1)\\right) - \\log\\left(p(e_i \\mid s_i, \\mathcal{H}_0)\\right) $$\n    Using the formula for the log-PDF of a Normal distribution, $\\log\\mathcal{N}(x \\mid \\mu, \\sigma^2) = -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}$, this becomes:\n    $$ \\text{LLR}_{\\text{MPRA}, i} = \\left[-\\frac{1}{2}\\log(2\\pi(s_i^2 + \\tau^2)) - \\frac{(e_i - \\mu_A)^2}{2(s_i^2 + \\tau^2)}\\right] - \\left[-\\frac{1}{2}\\log(2\\pi s_i^2) - \\frac{e_i^2}{2s_i^2}\\right] $$\n    $$ \\text{LLR}_{\\text{MPRA}, i} = \\frac{1}{2}\\log\\left(\\frac{s_i^2}{s_i^2 + \\tau^2}\\right) + \\frac{e_i^2}{2s_i^2} - \\frac{(e_i - \\mu_A)^2}{2(s_i^2 + \\tau^2)} $$\n\nCombining these terms, the total posterior log-odds for variant $i$, denoted $L_i$, is:\n$$ L_i = \\text{logit}(\\pi'_0) + \\text{LLR}_{\\text{MPRA}, i} + \\text{LLR}_{\\text{eQTL}, i} $$\nThis score $L_i$ is monotone in the posterior probability.\n\nTo obtain the posterior probability $\\Pr(\\mathcal{H}_1 \\mid \\text{data}_i)$, we apply the logistic (sigmoid) function, which is the inverse of the logit function:\n$$ \\Pr(\\mathcal{H}_1 \\mid \\text{data}_i) = \\sigma(L_i) = \\frac{1}{1 + e^{-L_i}} $$\nFinally, the expected utility for variant $i$ is calculated as:\n$$ U_i = u_i \\times \\frac{1}{1 + e^{-L_i}} $$\nThe program will compute $U_i$ for all variants in a given test case. It will then identify the maximum utility value, $U_{\\max} = \\max_i(U_i)$. The top-ranked variant is the one with the smallest index $i$ such that $|U_i - U_{\\max}| \\le \\delta$, where $\\delta = 10^{-12}$ is the specified tolerance for tie-breaking.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n\ndef solve():\n    \"\"\"\n    Solves the variant ranking problem for the given test suite.\n    \"\"\"\n    # Define the constants from the problem statement.\n    mu_A = 0.5\n    tau = 0.3\n    pi_c = 0.2\n    pi_0 = 0.01\n    epsilon = 1e-6\n    delta = 1e-12\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general case)\n        {\n            \"e\": np.array([0.4, 1.2, -0.1, 0.8, 0.0]),\n            \"s\": np.array([0.2, 0.5, 0.3, 0.4, 0.1]),\n            \"c\": np.array([0.6, 0.4, 0.9, 0.2, 0.5]),\n            \"u\": np.array([1, 1, 1, 1, 1]),\n        },\n        # Case 2 (edge probabilities and precisions)\n        {\n            \"e\": np.array([0.0, 3.0, 0.1]),\n            \"s\": np.array([1e6, 1.0, 0.05]),\n            \"c\": np.array([0.0, 1.0, 0.01]),\n            \"u\": np.array([1, 1, 1]),\n        },\n        # Case 3 (exact tie, exercise tie-breaking)\n        {\n            \"e\": np.array([0.5, 0.5]),\n            \"s\": np.array([0.2, 0.2]),\n            \"c\": np.array([0.5, 0.5]),\n            \"u\": np.array([1, 1]),\n        },\n        # Case 4 (different utilities with identical evidence)\n        {\n            \"e\": np.array([0.6, 0.6, 0.6]),\n            \"s\": np.array([0.3, 0.3, 0.3]),\n            \"c\": np.array([0.6, 0.6, 0.6]),\n            \"u\": np.array([1.0, 5.0, 0.5]),\n        },\n    ]\n\n    def logit(p):\n        \"\"\"Computes the logit function log(p / (1 - p)).\"\"\"\n        return np.log(p / (1.0 - p))\n\n    def calculate_expected_utilities(e, s, c, u):\n        \"\"\"\n        Calculates the expected clinical utility for a set of variants.\n        \"\"\"\n        # 1. Clip probabilities for numerical stability\n        pi0_clipped = np.clip(pi_0, epsilon, 1.0 - epsilon)\n        pic_clipped = np.clip(pi_c, epsilon, 1.0 - epsilon)\n        c_clipped = np.clip(c, epsilon, 1.0 - epsilon)\n        \n        # 2. Calculate prior log-odds\n        prior_log_odds = logit(pi0_clipped)\n        \n        # 3. Calculate eQTL log-likelihood ratio (LLR_eQTL)\n        llr_eqtl = logit(c_clipped) - logit(pic_clipped)\n        \n        # 4. Calculate MPRA log-likelihood ratio (LLR_MPRA)\n        # LLR = log(p(data|H1)) - log(p(data|H0))\n        # H0:\n        log_pdf_h0 = norm.logpdf(e, loc=0.0, scale=s)\n        # H1:\n        var_h1 = s**2 + tau**2\n        scale_h1 = np.sqrt(var_h1)\n        log_pdf_h1 = norm.logpdf(e, loc=mu_A, scale=scale_h1)\n        \n        llr_mpra = log_pdf_h1 - log_pdf_h0\n\n        # 5. Calculate total posterior log-odds\n        total_log_odds = prior_log_odds + llr_eqtl + llr_mpra\n        \n        # 6. Convert log-odds to posterior probability\n        # posterior_prob = 1 / (1 + exp(-total_log_odds))\n        posterior_prob = 1.0 / (1.0 + np.exp(-total_log_odds))\n        \n        # 7. Calculate expected utility\n        expected_utility = u * posterior_prob\n        \n        return expected_utility\n\n    results = []\n    for case in test_cases:\n        utilities = calculate_expected_utilities(case[\"e\"], case[\"s\"], case[\"c\"], case[\"u\"])\n        \n        # Find the index of the top-ranked variant\n        max_utility = np.max(utilities)\n        \n        # Find all indices that are maximizers within the tolerance delta\n        maximizer_indices = np.where(np.abs(utilities - max_utility) = delta)[0]\n        \n        # Per the tie-breaking rule, return the smallest index among the maximizers\n        best_index = np.min(maximizer_indices)\n        results.append(best_index)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4357293"}]}