{"hands_on_practices": [{"introduction": "At the heart of predicting a missense variant's impact lies the quantification of the change it induces. This exercise provides a hands-on approach to calculating the Grantham distance, a classic metric that captures the physicochemical dissimilarity between the original and substituted amino acids based on properties like volume, composition, and polarity. By working through this problem, you will learn how to translate these fundamental amino acid properties into a quantitative score and use it within a simple statistical model to estimate the odds of a variant being deleterious [@problem_id:4371782].", "problem": "A missense variant in a clinically relevant kinase gene substitutes arginine ($\\mathrm{Arg}$, denoted $R$) with tryptophan ($\\mathrm{Trp}$, denoted $W$) at a conserved position. To quantify physicochemical divergence relevant to functional impact, assume the Grantham distance is modeled as the Euclidean distance in a standardized three-dimensional property space comprising side-chain composition ($C$), polarity ($P$), and molecular volume ($V$). Each axis is standardized by its population standard deviation to reflect commensurate scales. You are provided the following empirically curated property values (consistent with widely used scales for amino acid substitution analysis) and standard deviations for standardization:\n- $R$: $C_{R} = 95$, $P_{R} = 52$, $V_{R} = 124$.\n- $W$: $C_{W} = 130$, $P_{W} = 61$, $V_{W} = 170$.\n- $S$ (serine): $C_{S} = 73$, $P_{S} = 9$, $V_{S} = 32$.\n- $T$ (threonine): $C_{T} = 93$, $P_{T} = 13$, $V_{T} = 61$.\n- Standard deviations: $s_{C} = 20$, $s_{P} = 10$, $s_{V} = 50$.\n\nA validated logistic model links standardized Grantham distance $D$ to the log-odds of pathogenicity for a missense substitution via\n$$\\ln\\!\\left(\\frac{p}{1 - p}\\right) = \\alpha + \\beta D,$$\nwith $\\alpha = -1$ and $\\beta = 1$, where $p$ is the probability that the substitution is deleterious.\n\nUsing only these premises, do the following:\n1. Compute the standardized Grantham distance $D_{R\\rightarrow W}$ between $R$ and $W$, and $D_{S\\rightarrow T}$ between $S$ and $T$.\n2. Using the logistic model, determine the posterior odds ratio (defined as the ratio of posterior odds of deleteriousness) for $R\\rightarrow W$ relative to $S\\rightarrow T$.\n\nReport the posterior odds ratio as a single real number (dimensionless). Round your answer to four significant figures.", "solution": "The problem is valid as it is scientifically grounded in the principles of bioinformatics and variant effect prediction, is well-posed with sufficient and consistent data, and is expressed in objective, formalizable language.\n\nThe solution proceeds in two main parts, following the structure of the problem statement. First, we compute the standardized Grantham distances for the two specified amino acid substitutions. Second, we use these distances in the provided logistic model to determine the requested posterior odds ratio.\n\n**Part 1: Computation of Standardized Grantham Distances**\n\nThe problem models the Grantham distance $D$ as the Euclidean distance in a three-dimensional property space, where each dimension (composition $C$, polarity $P$, and molecular volume $V$) is standardized. For a substitution from amino acid $A_1$ to amino acid $A_2$, with property values $(C_1, P_1, V_1)$ and $(C_2, P_2, V_2)$, and population standard deviations $(s_C, s_P, s_V)$, the standardized distance $D_{A_1 \\rightarrow A_2}$ is given by:\n$$D_{A_1 \\rightarrow A_2} = \\sqrt{\\left(\\frac{C_1 - C_2}{s_C}\\right)^2 + \\left(\\frac{P_1 - P_2}{s_P}\\right)^2 + \\left(\\frac{V_1 - V_2}{s_V}\\right)^2}$$\n\nFirst, we compute the distance for the arginine to tryptophan substitution, $D_{R\\rightarrow W}$.\nThe given property values are:\n- Arginine ($R$): $C_{R} = 95$, $P_{R} = 52$, $V_{R} = 124$.\n- Tryptophan ($W$): $C_{W} = 130$, $P_{W} = 61$, $V_{W} = 170$.\n- Standard deviations: $s_{C} = 20$, $s_{P} = 10$, $s_{V} = 50$.\n\nThe differences in properties are:\n$\\Delta C = C_{R} - C_{W} = 95 - 130 = -35$.\n$\\Delta P = P_{R} - P_{W} = 52 - 61 = -9$.\n$\\Delta V = V_{R} - V_{W} = 124 - 170 = -46$.\n\nThe standardized distance $D_{R\\rightarrow W}$ is:\n$$D_{R\\rightarrow W} = \\sqrt{\\left(\\frac{-35}{20}\\right)^2 + \\left(\\frac{-9}{10}\\right)^2 + \\left(\\frac{-46}{50}\\right)^2}$$\n$$D_{R\\rightarrow W} = \\sqrt{(-1.75)^2 + (-0.9)^2 + (-0.92)^2}$$\n$$D_{R\\rightarrow W} = \\sqrt{3.0625 + 0.81 + 0.8464} = \\sqrt{4.7189}$$\n\nNext, we compute the distance for the serine to threonine substitution, $D_{S\\rightarrow T}$.\nThe given property values are:\n- Serine ($S$): $C_{S} = 73$, $P_{S} = 9$, $V_{S} = 32$.\n- Threonine ($T$): $C_{T} = 93$, $P_{T} = 13$, $V_{T} = 61$.\n- Standard deviations are the same: $s_{C} = 20$, $s_{P} = 10$, $s_{V} = 50$.\n\nThe differences in properties are:\n$\\Delta C = C_{S} - C_{T} = 73 - 93 = -20$.\n$\\Delta P = P_{S} - P_{T} = 9 - 13 = -4$.\n$\\Delta V = V_{S} - V_{T} = 32 - 61 = -29$.\n\nThe standardized distance $D_{S\\rightarrow T}$ is:\n$$D_{S\\rightarrow T} = \\sqrt{\\left(\\frac{-20}{20}\\right)^2 + \\left(\\frac{-4}{10}\\right)^2 + \\left(\\frac{-29}{50}\\right)^2}$$\n$$D_{S\\rightarrow T} = \\sqrt{(-1)^2 + (-0.4)^2 + (-0.58)^2}$$\n$$D_{S\\rightarrow T} = \\sqrt{1 + 0.16 + 0.3364} = \\sqrt{1.4964}$$\n\n**Part 2: Determination of the Posterior Odds Ratio**\n\nThe problem provides a logistic model that links the standardized Grantham distance $D$ to the log-odds of deleteriousness:\n$$\\ln\\!\\left(\\frac{p}{1 - p}\\right) = \\alpha + \\beta D$$\nwhere $p$ is the probability of the substitution being deleterious. The term $\\frac{p}{1 - p}$ is the posterior odds of deleteriousness.\nThe model parameters are given as $\\alpha = -1$ and $\\beta = 1$.\n\nThe posterior odds for a substitution with distance $D$ are:\n$$\\text{Odds} = \\frac{p}{1 - p} = \\exp(\\alpha + \\beta D) = \\exp(-1 + D)$$\n\nWe need to find the posterior odds ratio for the $R\\rightarrow W$ substitution relative to the $S\\rightarrow T$ substitution. This is the ratio of their respective posterior odds:\n$$\\text{Odds Ratio} = \\frac{\\text{Odds}_{R\\rightarrow W}}{\\text{Odds}_{S\\rightarrow T}} = \\frac{\\exp(-1 + D_{R\\rightarrow W})}{\\exp(-1 + D_{S\\rightarrow T})}$$\nUsing the property of exponents, $\\frac{\\exp(a)}{\\exp(b)} = \\exp(a-b)$, the expression simplifies to:\n$$\\text{Odds Ratio} = \\exp((-1 + D_{R\\rightarrow W}) - (-1 + D_{S\\rightarrow T})) = \\exp(D_{R\\rightarrow W} - D_{S\\rightarrow T})$$\n\nNow, we substitute the calculated values of the distances:\n$$\\text{Odds Ratio} = \\exp(\\sqrt{4.7189} - \\sqrt{1.4964})$$\nNumerically evaluating the terms:\n$D_{R\\rightarrow W} = \\sqrt{4.7189} \\approx 2.172303$\n$D_{S\\rightarrow T} = \\sqrt{1.4964} \\approx 1.223274$\nThe difference in distances is:\n$D_{R\\rightarrow W} - D_{S\\rightarrow T} \\approx 2.172303 - 1.223274 = 0.949029$\nFinally, the posterior odds ratio is:\n$$\\text{Odds Ratio} = \\exp(0.949029) \\approx 2.58321$$\nRounding the result to four significant figures, as requested, we get $2.583$.", "answer": "$$\\boxed{2.583}$$", "id": "4371782"}, {"introduction": "Beyond physicochemical properties, evolutionary conservation provides powerful clues about a residue's functional importance. This practice explores the logic behind conservation-based predictors like SIFT, which assess whether a substitution is likely to be tolerated based on its frequency in a multiple sequence alignment. Critically, this problem moves beyond a simple \"deleterious\" or \"tolerated\" call to address the uncertainty of the prediction, demonstrating how statistical confidence is directly linked to the depth and quality of the underlying sequence data [@problem_id:4371763].", "problem": "A conservation-based predictor such as Sorting Intolerant From Tolerant (SIFT) classifies missense substitutions using a threshold on the estimated probability that a substitution is tolerated given a Multiple Sequence Alignment (MSA). Let the standard SIFT decision threshold be $\\tau = 0.05$, with substitutions predicted as functionally damaging (deleterious) when the tolerated probability estimate $\\hat{p} \\le \\tau$, and as tolerated (benign) when $\\hat{p}  \\tau$. Consider a particular missense substitution for which a simplified alignment-derived tolerated probability is $\\hat{p} = 0.02$. Let $n_{\\mathrm{eff}}$ denote the effective number of independent informative sequences contributing at this position in the MSA (for example, adjusted for redundancy and phylogenetic relatedness). Assume a binomial-proportion model in which the sampling variability of $\\hat{p}$ around the true tolerated probability $p$ is approximately $\\mathrm{Var}(\\hat{p}) \\approx p(1-p)/n_{\\mathrm{eff}}$, and for purposes of uncertainty assessment you may use the normal approximation to construct an approximate $95\\%$ confidence interval for $p$ as $\\hat{p} \\pm 1.96 \\sqrt{\\hat{p}(1-\\hat{p})/n_{\\mathrm{eff}}}$.\n\nWhich option best states the SIFT classification for this substitution and correctly characterizes the uncertainty as a function of alignment depth?\n\nA. The substitution is predicted deleterious because $\\hat{p} = 0.02 \\le \\tau = 0.05$. Moreover, under the normal approximation, the upper $95\\%$ confidence bound for $p$ falls below $\\tau$ only if $n_{\\mathrm{eff}} \\gtrsim 84$; for smaller $n_{\\mathrm{eff}}$ the confidence interval overlaps $\\tau$, so the deleterious call should be flagged as low confidence.\n\nB. The substitution is predicted tolerated because $\\hat{p} = 0.02$ is smaller than $\\tau = 0.05$, and lower alignment depth reduces false negatives, increasing confidence in tolerance.\n\nC. The substitution is predicted deleterious; uncertainty is minimized at low $n_{\\mathrm{eff}}$ because fewer sequences reduce sampling noise, so confidence increases as $n_{\\mathrm{eff}}$ decreases.\n\nD. The result is inconclusive regardless of $n_{\\mathrm{eff}}$ because SIFT scores below $\\tau$ cannot be trusted without a functional assay; alignment depth does not inform uncertainty in any principled way.", "solution": "The problem asks for the classification of a missense substitution according to the Sorting Intolerant From Tolerant (SIFT) algorithm and for a characterization of the uncertainty in this classification as a function of alignment depth.\n\n**Step 1: Problem Validation**\n\nFirst, I will validate the problem statement.\nThe givens are:\n-   A conservation-based predictor, Sorting Intolerant From Tolerant (SIFT).\n-   The SIFT decision threshold is $\\tau = 0.05$.\n-   A substitution is predicted as functionally damaging (deleterious) if the estimated tolerated probability $\\hat{p} \\le \\tau$.\n-   A substitution is predicted as tolerated (benign) if $\\hat{p}  \\tau$.\n-   For the specific substitution, the estimated tolerated probability is $\\hat{p} = 0.02$.\n-   The effective number of independent sequences is $n_{\\mathrm{eff}}$.\n-   The sampling variability of $\\hat{p}$ is modeled by the variance $\\mathrm{Var}(\\hat{p}) \\approx p(1-p)/n_{\\mathrm{eff}}$, where $p$ is the true tolerated probability.\n-   An approximate $95\\%$ confidence interval for $p$ is given by the normal approximation: $\\hat{p} \\pm 1.96 \\sqrt{\\hat{p}(1-\\hat{p})/n_{\\mathrm{eff}}}$.\n\nThe problem is scientifically grounded, describing the actual logic of the SIFT predictor and using standard statistical methods (binomial proportion variance, normal approximation for a confidence interval) to assess uncertainty. The problem is well-posed, providing all necessary definitions, values, and formulas to arrive at a unique conclusion. The terminology is precise and objective. There are no logical contradictions, factual errors, or ambiguities in the problem statement. Therefore, the problem is valid.\n\n**Step 2: Derivation of the Solution**\n\nFirst, I will determine the SIFT classification for the given substitution.\nThe prediction rule states that a substitution is deleterious if $\\hat{p} \\le \\tau$.\nWe are given $\\hat{p} = 0.02$ and $\\tau = 0.05$.\nSince $0.02 \\le 0.05$, the substitution is classified as **deleterious**.\n\nNext, I will characterize the uncertainty of this classification. The uncertainty is related to the width of the confidence interval for the true probability $p$. A wider interval implies higher uncertainty. The confidence in the deleterious call is high if the entire $95\\%$ confidence interval for $p$ is below the threshold $\\tau$. Since $\\hat{p}  \\tau$, this is equivalent to the condition that the upper bound of the confidence interval is less than or equal to $\\tau$.\n\nThe upper $95\\%$ confidence bound for $p$ is given by:\n$$p_{\\text{upper}} = \\hat{p} + 1.96 \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n_{\\mathrm{eff}}}}$$\n\nFor the deleterious call to be considered high-confidence (at the $95\\%$ level), we require $p_{\\text{upper}} \\le \\tau$. Let's determine the condition on $n_{\\mathrm{eff}}$ for this to hold.\nSubstituting the given values $\\hat{p} = 0.02$ and $\\tau = 0.05$:\n$$0.02 + 1.96 \\sqrt{\\frac{0.02(1-0.02)}{n_{\\mathrm{eff}}}} \\le 0.05$$\n\nNow, I will solve this inequality for $n_{\\mathrm{eff}}$:\n$$1.96 \\sqrt{\\frac{0.02(0.98)}{n_{\\mathrm{eff}}}} \\le 0.05 - 0.02$$\n$$1.96 \\sqrt{\\frac{0.0196}{n_{\\mathrm{eff}}}} \\le 0.03$$\n$$\\sqrt{\\frac{0.0196}{n_{\\mathrm{eff}}}} \\le \\frac{0.03}{1.96}$$\n\nSquaring both sides of the inequality:\n$$\\frac{0.0196}{n_{\\mathrm{eff}}} \\le \\left(\\frac{0.03}{1.96}\\right)^2$$\n\nTo solve for $n_{\\mathrm{eff}}$, we can invert the inequality (which reverses the inequality sign):\n$$\\frac{n_{\\mathrm{eff}}}{0.0196} \\ge \\left(\\frac{1.96}{0.03}\\right)^2$$\n$$n_{\\mathrm{eff}} \\ge 0.0196 \\times \\left(\\frac{1.96}{0.03}\\right)^2$$\n\nNow, I will calculate the numerical value:\n$$n_{\\mathrm{eff}} \\ge 0.0196 \\times (65.333...)^2$$\n$$n_{\\mathrm{eff}} \\ge 0.0196 \\times 4268.444...$$\n$$n_{\\mathrm{eff}} \\ge 83.6615...$$\n\nThis means that for the upper $95\\%$ confidence bound to be at or below the threshold of $0.05$, the effective number of sequences $n_{\\mathrm{eff}}$ must be at least approximately $84$. If $n_{\\mathrm{eff}}$ is smaller than this value, the confidence interval for $p$ will contain the threshold $\\tau = 0.05$, indicating that we cannot be $95\\%$ confident that the true probability $p$ is less than $0.05$. In such a case, the deleterious prediction would be flagged as having low confidence.\n\nThe analysis shows that larger alignment depth ($n_{\\mathrm{eff}}$) leads to a narrower confidence interval, which reduces uncertainty and increases confidence in the prediction, provided the point estimate is not too close to the threshold.\n\n**Step 3: Option-by-Option Analysis**\n\nA. The substitution is predicted deleterious because $\\hat{p} = 0.02 \\le \\tau = 0.05$. Moreover, under the normal approximation, the upper $95\\%$ confidence bound for $p$ falls below $\\tau$ only if $n_{\\mathrm{eff}} \\gtrsim 84$; for smaller $n_{\\mathrm{eff}}$ the confidence interval overlaps $\\tau$, so the deleterious call should be flagged as low confidence.\n- The classification as deleterious is correct, as $0.02 \\le 0.05$.\n- The calculation that $n_{\\mathrm{eff}}$ must be approximately $84$ or greater for the upper confidence bound to be below $\\tau$ is correct, based on my derivation ($n_{\\mathrm{eff}} \\ge 83.66...$).\n- The interpretation that for smaller $n_{\\mathrm{eff}}$ the CI overlaps the threshold and the call is low confidence is also correct. This directly follows from the properties of confidence intervals.\n- Verdict: **Correct**.\n\nB. The substitution is predicted tolerated because $\\hat{p} = 0.02$ is smaller than $\\tau = 0.05$, and lower alignment depth reduces false negatives, increasing confidence in tolerance.\n- The classification as \"tolerated\" is incorrect. According to the rule $\\hat{p} \\le \\tau$, the prediction is \"deleterious\".\n- The statement that lower alignment depth increases confidence is incorrect. Lower alignment depth (smaller $n_{\\mathrm{eff}}$) increases the variance of the estimate, widens the confidence interval, and therefore *decreases* confidence.\n- Verdict: **Incorrect**.\n\nC. The substitution is predicted deleterious; uncertainty is minimized at low $n_{\\mathrm{eff}}$ because fewer sequences reduce sampling noise, so confidence increases as $n_{\\mathrm{eff}}$ decreases.\n- The classification as deleterious is correct.\n- The statement that uncertainty is minimized at low $n_{\\mathrm{eff}}$ is fundamentally incorrect. From the formula for the variance, $\\mathrm{Var}(\\hat{p}) \\approx p(1-p)/n_{\\mathrm{eff}}$, it is clear that variance (a measure of uncertainty) is inversely proportional to $n_{\\mathrm{eff}}$. Thus, uncertainty increases as $n_{\\mathrm{eff}}$ decreases. The claim that fewer sequences reduce sampling noise is the opposite of a basic statistical principle.\n- Verdict: **Incorrect**.\n\nD. The result is inconclusive regardless of $n_{\\mathrm{eff}}$ because SIFT scores below $\\tau$ cannot be trusted without a functional assay; alignment depth does not inform uncertainty in any principled way.\n- The claim that the result is inconclusive is contrary to the problem statement, which provides a deterministic rule for classification based on the threshold $\\tau$.\n- The claim that alignment depth ($n_{\\mathrm{eff}}$) does not inform uncertainty in a principled way is false. The problem explicitly provides a statistical model where the confidence interval width, and thus uncertainty, is a direct inverse function of $\\sqrt{n_{\\mathrm{eff}}}$. This is a standard and principled way to model uncertainty.\n- Verdict: **Incorrect**.\n\nBased on the detailed analysis, only option A is fully consistent with the problem statement and correct principles of statistics.", "answer": "$$\\boxed{A}$$", "id": "4371763"}, {"introduction": "In a real-world clinical setting, variant interpretation is rarely based on a single predictive score; instead, it involves synthesizing multiple, independent lines of evidence. This capstone exercise simulates the evidence integration framework used in clinical genomics, such as the ACMG/AMP guidelines, by applying Bayes' theorem in its odds-and-likelihood-ratio form. You will practice combining a prior probability of pathogenicity with disparate data from conservation analysis, functional assays, and population genetics to calculate a final, unified posterior probability [@problem_id:4371766].", "problem": "A missense variant in a clinically actionable gene is being evaluated for pathogenicity in a precision medicine setting following the American College of Medical Genetics and Genomics (ACMG) framework. Let the hypothesis $H$ denote that the variant is pathogenic and $\\bar{H}$ denote that the variant is not pathogenic. A curated gene- and phenotype-informed prior analysis yields a prior odds of pathogenicity $O_{\\text{prior}}=\\frac{P(H)}{P(\\bar{H})}$ equal to $O_{\\text{prior}}=\\frac{1}{49}$. Three independent evidence sources are available: evolutionary conservation across vertebrates (conservation), a quantitative in vitro functional assay, and high-resolution population allele frequency data. These sources are calibrated to yield likelihood ratios, defined as $LR=\\frac{P(\\text{evidence}\\mid H)}{P(\\text{evidence}\\mid \\bar{H})}$, with values $LR_{1}=6.5$ (conservation), $LR_{2}=80$ (functional assay), and $LR_{3}=0.2$ (population data). Assume conditional independence of these evidence sources given $H$ and given $\\bar{H}$, and that the prior odds $O_{\\text{prior}}$ are appropriately specified for the geneâ€“phenotype context without double-counting any of the three evidence sources.\n\nUsing Bayes theorem and the likelihood ratio framework as the fundamental base, infer the posterior probability of pathogenicity $P(H\\mid \\text{data})$ implied by these inputs. Express the final posterior probability as a decimal, and round your answer to four significant figures. No units are required.", "solution": "The problem requires the calculation of the posterior probability of pathogenicity, denoted as $P(H\\mid \\text{data})$, for a missense variant. This is achieved by integrating a prior probability with three independent sources of evidence using the Bayesian framework, specifically its formulation in terms of odds and likelihood ratios.\n\nLet $H$ be the hypothesis that the variant is pathogenic, and $\\bar{H}$ be the hypothesis that it is not. The prior odds of pathogenicity are given as:\n$$O_{\\text{prior}} = \\frac{P(H)}{P(\\bar{H})} = \\frac{1}{49}$$\n\nAccording to Bayes' theorem, when new evidence (data) is introduced, the prior odds are updated to posterior odds, $O_{\\text{post}}$, by multiplication with a likelihood ratio, $LR$:\n$$O_{\\text{post}} = \\frac{P(H \\mid \\text{data})}{P(\\bar{H} \\mid \\text{data})} = O_{\\text{prior}} \\times LR$$\nThe likelihood ratio is defined as the probability of observing the evidence given the hypothesis is true, divided by the probability of observing the evidence given the hypothesis is false:\n$$LR = \\frac{P(\\text{data} \\mid H)}{P(\\text{data} \\mid \\bar{H})}$$\n\nIn this problem, we have three distinct sources of evidence: conservation, functional assay, and population data. A critical piece of information is that these evidence sources are conditionally independent given both $H$ and $\\bar{H}$. This assumption allows us to calculate a combined likelihood ratio, $LR_{\\text{total}}$, by taking the product of the individual likelihood ratios, $LR_{1}$, $LR_{2}$, and $LR_{3}$:\n$$LR_{\\text{total}} = LR_{1} \\times LR_{2} \\times LR_{3}$$\n\nThe specific values for the likelihood ratios are provided:\n$LR_{1} = 6.5$ (conservation)\n$LR_{2} = 80$ (functional assay)\n$LR_{3} = 0.2$ (population data)\n\nWe can now compute the total likelihood ratio:\n$$LR_{\\text{total}} = 6.5 \\times 80 \\times 0.2$$\n$$LR_{\\text{total}} = 520 \\times 0.2$$\n$$LR_{\\text{total}} = 104$$\n\nWith the total likelihood ratio determined, we can calculate the posterior odds of pathogenicity:\n$$O_{\\text{post}} = O_{\\text{prior}} \\times LR_{\\text{total}}$$\n$$O_{\\text{post}} = \\frac{1}{49} \\times 104 = \\frac{104}{49}$$\n\nThe final step is to convert the posterior odds into a posterior probability. The relationship between a probability $P$ and its corresponding odds $O$ is given by the formula $P = \\frac{O}{1+O}$. Applying this to our posterior odds, we find the posterior probability of pathogenicity, $P(H\\mid \\text{data})$:\n$$P(H\\mid \\text{data}) = \\frac{O_{\\text{post}}}{1 + O_{\\text{post}}}$$\nSubstituting the value of $O_{\\text{post}}$:\n$$P(H\\mid \\text{data}) = \\frac{\\frac{104}{49}}{1 + \\frac{104}{49}}$$\nTo simplify this complex fraction, we find a common denominator in the denominator:\n$$P(H\\mid \\text{data}) = \\frac{\\frac{104}{49}}{\\frac{49}{49} + \\frac{104}{49}} = \\frac{\\frac{104}{49}}{\\frac{49 + 104}{49}} = \\frac{\\frac{104}{49}}{\\frac{153}{49}}$$\n$$P(H\\mid \\text{data}) = \\frac{104}{153}$$\n\nThe problem requires the answer as a decimal rounded to four significant figures. We perform the division:\n$$P(H\\mid \\text{data}) = 0.67973856...$$\nRounding to four significant figures, we get:\n$$P(H\\mid \\text{data}) \\approx 0.6797$$", "answer": "$$\\boxed{0.6797}$$", "id": "4371766"}]}