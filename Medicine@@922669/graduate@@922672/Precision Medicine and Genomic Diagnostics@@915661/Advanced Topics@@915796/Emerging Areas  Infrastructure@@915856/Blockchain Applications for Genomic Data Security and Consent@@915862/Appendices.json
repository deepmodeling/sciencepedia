{"hands_on_practices": [{"introduction": "The foundational security promise of a blockchain is its immutability, which stems directly from the cryptographic properties of hash functions. This exercise provides a hands-on analysis of one such property: second-preimage resistance. By deriving the probability of an adversary successfully forging a genomic record without detection, you will develop a quantitative intuition for why a hash function with an output size of $n \\ge 256$ bits provides a security guarantee that is robust enough for mission-critical clinical data [@problem_id:4320246].", "problem": "A precision oncology consortium records the integrity of each patient’s Variant Call Format (VCF) file by committing the canonicalized file to a permissioned blockchain as a cryptographic commitment equal to the output of a hash function with $n$-bit output. The stored commitment is the $n$-bit digest $H(R)$ of the canonicalized record $R$. To corrupt the audit trail without detection, an adversary must produce an altered record $R^{\\prime} \\neq R$ such that $H(R^{\\prime}) = H(R)$, thereby causing a verifier that recomputes the hash to falsely validate the altered record against the on-chain commitment. Assume the following fundamental base:\n- The hash function behaves as a uniform random mapping from the space of bitstrings to the set of $2^{n}$ possible $n$-bit outputs (the random oracle idealization underpinning collision and second-preimage resistance in modern cryptography).\n- Each distinct altered candidate $R^{\\prime}$ that the adversary constructs yields an $n$-bit output distributed independently and uniformly over the $2^{n}$ possible digests, relative to the fixed target digest $H(R)$.\n- The adversary can prepare at most $Q$ distinct, syntactically valid altered records $R^{\\prime}_{1},\\dots,R^{\\prime}_{Q}$ to test against the stored digest for a single target record $R$.\n\nStarting from these base facts and elementary probability theory, derive the exact probability that at least one of the $Q$ attempts yields a false validation, i.e., that $H(R^{\\prime}_{i}) = H(R)$ for some $i \\in \\{1,\\dots,Q\\}$. Express your final answer as a closed-form analytic expression in terms of $n$ and $Q$ only, without approximation.\n\nThen, within your solution (not the final answer), use your expression and first principles to justify why choosing $n \\ge 256$ is appropriate for clinical integrity in precision medicine, in the sense that the aggregate probability of a false validation remains negligible even when $Q$ reflects highly conservative, system-wide bounds on feasible adversarial attempts. No numerical rounding is required, and no units are to be reported for the probability.", "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- A hash function has an $n$-bit output.\n- A cryptographic commitment to a canonicalized record $R$ is its hash digest $H(R)$.\n- An adversary seeks to find an altered record $R^{\\prime} \\neq R$ such that $H(R^{\\prime}) = H(R)$. This is a second-preimage attack.\n- The hash function is modeled as a random oracle: a uniform random mapping to the $2^n$ possible outputs.\n- The hash output for each distinct altered candidate $R^{\\prime}$ is independent and uniformly distributed over the $2^n$ possible digests.\n- The adversary can generate and test a maximum of $Q$ distinct altered records, $R^{\\prime}_{1}, \\dots, R^{\\prime}_{Q}$, for a single target record $R$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n\n- **Scientifically Grounded**: The problem is based on the random oracle model, a standard and fundamental idealization in theoretical cryptography for analyzing the security of hash functions. The scenario describes a second-preimage attack, a classical cryptographic problem. The application context, using blockchain for data integrity in a clinical setting, is a recognized and actively explored field. The premises are scientifically sound.\n- **Well-Posed**: The problem is well-posed. It provides all necessary information: the size of the hash output space ($2^n$), the number of adversarial attempts ($Q$), and the probabilistic model (independent, uniform trials). It requests a specific quantity—the exact probability of at least one success—which can be uniquely determined from the givens.\n- **Objective**: The problem is stated in precise, objective, and formal language, free of ambiguity or subjective claims.\n\nThe problem does not exhibit any of the flaws listed in the validation checklist. It is scientifically sound, formalizable, complete, and well-posed.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be derived.\n\n### Derivation of the Probability\nThe goal is to calculate the probability that an adversary finds a second preimage for a given hash digest $H(R)$ within $Q$ attempts. This is the probability that for at least one of the altered records $R^{\\prime}_i$ in the set $\\{R^{\\prime}_1, \\dots, R^{\\prime}_Q\\}$, the condition $H(R^{\\prime}_i) = H(R)$ holds.\n\nLet $N$ be the total number of possible $n$-bit hash outputs. We have $N = 2^n$. The target digest is a single, fixed value $h = H(R)$.\n\nAccording to the problem statement, for any candidate record $R^{\\prime}_i$, the output $H(R^{\\prime}_i)$ is uniformly distributed over the $N$ possible values. Therefore, the probability that a single attempt succeeds, i.e., that $H(R^{\\prime}_i)$ matches the specific target $h$, is:\n$$ P(H(R^{\\prime}_i) = h) = \\frac{1}{N} = \\frac{1}{2^n} $$\nLet $E_i$ be the event that the $i$-th attempt is successful, i.e., $H(R^{\\prime}_i) = H(R)$. The problem asks for the probability of the event $A = E_1 \\cup E_2 \\cup \\dots \\cup E_Q$, which is the event that at least one of the $Q$ attempts succeeds.\n\nIt is more tractable to first calculate the probability of the complement event, $A^c$, which is the event that *all* $Q$ attempts fail.\nThe probability of failure for a single attempt is:\n$$ P(E_i^c) = P(H(R^{\\prime}_i) \\neq H(R)) = 1 - P(E_i) = 1 - \\frac{1}{2^n} $$\nThe problem states that the outputs for distinct candidates are independent. Thus, the events $E_1, E_2, \\dots, E_Q$ are independent Bernoulli trials. The probability that all $Q$ independent attempts fail is the product of their individual probabilities of failure:\n$$ P(A^c) = P(E_1^c \\cap E_2^c \\cap \\dots \\cap E_Q^c) = \\prod_{i=1}^{Q} P(E_i^c) = \\left(1 - \\frac{1}{2^n}\\right)^Q $$\nThe probability of at least one success, $P(A)$, is therefore $1$ minus the probability of total failure:\n$$ P(A) = 1 - P(A^c) = 1 - \\left(1 - \\frac{1}{2^n}\\right)^Q $$\nThis is the exact, closed-form analytic expression for the probability of a false validation in terms of $n$ and $Q$.\n\n### Justification for $n \\ge 256$\nThe derived probability, $p_{ success} = 1 - \\left(1 - \\frac{1}{2^n}\\right)^Q$, quantifies the security of the commitment scheme against a second-preimage attack. For this probability to be considered \"negligible\" in a high-stakes application such as clinical data integrity, it must be so small as to be physically meaningless. We must assess this for a choice of $n=256$ under a highly conservative estimate for $Q$.\n\nThe term $Q$ represents the total computational work an adversary can perform. Let us establish a physically unrealistic but conservative upper bound on $Q$. The total hash rate of the Bitcoin network, an aggregation of immense specialized computational power, is on the order of $10^{21}$ hashes per second. Let us assume an adversary can command this power for one thousand years ($ \\approx 3.15 \\times 10^{10}$ seconds).\nThe total number of hashes computed would be:\n$$ Q \\approx (10^{21} \\ \\text{s}^{-1}) \\times (3.15 \\times 10^{10} \\ \\text{s}) \\approx 3.15 \\times 10^{31} $$\nTo express this as a power of $2$, we use the approximation $10^3 \\approx 2^{10}$:\n$$ Q \\approx 3.15 \\times (10^3)^{10.33} \\approx 3.15 \\times (2^{10})^{10.33} = 3.15 \\times 2^{103.3} $$\nSince $3.15 < 4 = 2^2$, we can establish a loose but safe upper bound: $Q < 2^2 \\times 2^{103.3} = 2^{105.3}$. For simplicity, let us take $Q = 2^{106}$. This quantity of computation is far beyond any current or projected capability of any single entity.\n\nNow, we evaluate the probability of success with $n=256$ and this monumental value of $Q=2^{106}$.\nFor a very small probability of success per trial, $p = 1/2^n$, the expression for the total probability can be accurately approximated using the first-order Taylor expansion: $(1-p)^Q \\approx 1 - Qp$.\n$$ p_{success} \\approx 1 - \\left(1 - Q \\frac{1}{2^n}\\right) = \\frac{Q}{2^n} $$\nSubstituting our values:\n$$ p_{success} \\approx \\frac{2^{106}}{2^{256}} = 2^{106 - 256} = 2^{-150} $$\nA probability of $2^{-150}$ is astronomically small and lacks any physical intuition. For context, the estimated number of atoms in the observable universe is approximately $10^{80}$, which is less than $2^{266}$. The probability of an adversary succeeding under these wildly generous assumptions is vastly smaller than the probability of, for example, randomly selecting a pre-ordained single atom from a trillion trillion universes.\n\nThe security of a hash function against a second-preimage attack is described by its \"bit security,\" which for an $n$-bit hash is $n$. A $256$-bit security level implies that an attacker would need to perform on the order of $2^{256}$ operations to have a significant chance of success. As demonstrated, even computational efforts that dwarf global capacity by many orders of magnitude yield a success probability that is effectively zero. Therefore, choosing $n \\ge 256$ provides a security margin that is robust against all conceivable advances in classical computation for the foreseeable future, making it an appropriate choice for ensuring the non-repudiable integrity of critical genomic data.", "answer": "$$\n\\boxed{1 - \\left(1 - \\frac{1}{2^n}\\right)^Q}\n$$", "id": "4320246"}, {"introduction": "While blockchains offer unparalleled integrity, the append-only nature of the ledger presents a significant engineering challenge: managing on-chain storage growth. Naively recording every event on-chain is often unsustainable in terms of cost and performance. This practice delves into a critical design pattern for building scalable systems by having you model and compare the on-chain storage footprint of a naive approach versus a sophisticated archival strategy that uses Merkle tree commitments to anchor off-chain data [@problem_id:4320216]. This analysis illuminates the practical trade-offs involved in designing efficient and economically viable blockchain solutions.", "problem": "A hospital deploys a permissioned blockchain to secure and audit access to genomic data as part of precision medicine and genomic diagnostics. The blockchain is operated as an append-only log. Each audit event records the actor address, a timestamp, a digital signature, and a cryptographic hash of the event payload. Off-chain archival is enabled via monthly Merkle tree commitments that anchor batches of audit events per genome on-chain. The hospital processes $M$ whole genomes per year; for each processed genome there are $E$ audit events in that year. Events are of two types: consent updates and access events. Let the fraction of consent updates be $p_c$ and the fraction of access events be $1 - p_c$. Assume the following byte-level structure for on-chain entries and commitments:\n- For a consent update event posted on-chain: payload $256$ bytes, signature $64$ bytes, hash $32$ bytes, actor address $20$ bytes, timestamp $8$ bytes, and transaction overhead $110$ bytes, for a total of $s_{c}$ bytes.\n- For an access event posted on-chain: payload $128$ bytes, signature $64$ bytes, hash $32$ bytes, actor address $20$ bytes, timestamp $8$ bytes, and transaction overhead $110$ bytes, for a total of $s_{a}$ bytes.\n- For a monthly per-genome Merkle commitment posted on-chain under archival: Merkle root $32$ bytes, batch salt $32$ bytes, period identifier $16$ bytes, auditor address $20$ bytes, signature $64$ bytes, and transaction overhead $110$ bytes, for a total of $s_{\\mathrm{commit}}$ bytes. There are $12$ such commitments per genome per year. Under the archival strategy, consent updates continue to be posted individually on-chain, while access events are kept off-chain and only included in the monthly commitments.\n\nAssume $M = 18{,}500$, $E = 320$, $p_c = 0.07$, compute the expected annual on-chain storage growth under:\n1. The naive strategy that posts every audit event on-chain.\n2. The archival strategy that posts monthly commitments per genome and only keeps consent updates on-chain individually.\n\nThen assess the pruning strategy by reporting the ratio of the archival strategy’s expected annual on-chain storage growth to the naive strategy’s expected annual on-chain storage growth. Use the linearity of expectation for event mixtures and assume independence of event type and payload size. Round your final ratio to four significant figures. Express the final ratio as a decimal without units. The final answer must be a single real number.", "solution": "We begin from the core definitions:\n- A blockchain is an append-only log; on-chain storage accumulates by adding entries described in bytes.\n- The expected size of a randomly selected event, when events are drawn from a finite mixture of types, is the mixture-weighted average of the sizes.\n- Linearity of expectation implies that the expected total size is the expected number of events multiplied by the expected size per event when event counts and event sizes are independent.\n\nFirst, compute the per-entry sizes from the byte components given:\n- Consent update event on-chain size:\n$$\ns_{c} = 256 + 64 + 32 + 20 + 8 + 110 = 490 \\text{ bytes}.\n$$\n- Access event on-chain size:\n$$\ns_{a} = 128 + 64 + 32 + 20 + 8 + 110 = 362 \\text{ bytes}.\n$$\n- Monthly Merkle commitment on-chain size:\n$$\ns_{\\mathrm{commit}} = 32 + 32 + 16 + 20 + 64 + 110 = 274 \\text{ bytes}.\n$$\n\nLet $M = 18{,}500$, $E = 320$, and $p_c = 0.07$ with $1 - p_c = 0.93$.\n\nNaive strategy:\n- Expected event size per event under the mixture is\n$$\n\\mathbb{E}[S_{\\text{event}}] = p_c \\, s_{c} + (1 - p_c) \\, s_{a} = 0.07 \\cdot 490 + 0.93 \\cdot 362.\n$$\nCompute the numerical value:\n$$\n0.07 \\cdot 490 = 34.3, \\quad 0.93 \\cdot 362 = 336.66, \\quad \\Rightarrow \\mathbb{E}[S_{\\text{event}}] = 34.3 + 336.66 = 370.96 \\text{ bytes}.\n$$\n- Expected number of events per year across all genomes is $M E = 18{,}500 \\times 320 = 5{,}920{,}000$ events.\n- Therefore, the expected annual on-chain storage under the naive strategy is\n$$\nS_{\\text{naive}} = (M E) \\cdot \\mathbb{E}[S_{\\text{event}}] = 5{,}920{,}000 \\times 370.96 = 2{,}196{,}083{,}200 \\text{ bytes}.\n$$\n\nArchival strategy:\n- Monthly commitments per genome: $12$ commitments per genome per year, each of size $s_{\\mathrm{commit}} = 274$ bytes. Per genome, this contributes\n$$\n12 \\cdot s_{\\mathrm{commit}} = 12 \\cdot 274 = 3{,}288 \\text{ bytes}.\n$$\n- Consent updates remain individually on-chain. The expected number of consent events per genome per year is $p_c E = 0.07 \\times 320 = 22.4$. Each consent event contributes $s_{c} = 490$ bytes, so per genome the expected consent bytes are\n$$\n( p_c E ) \\cdot s_{c} = 22.4 \\cdot 490 = 10{,}976 \\text{ bytes}.\n$$\n- Per genome per year under archival, the expected on-chain storage is the sum:\n$$\n3{,}288 + 10{,}976 = 14{,}264 \\text{ bytes}.\n$$\n- Across all genomes, the expected annual on-chain storage is\n$$\nS_{\\text{archival}} = M \\cdot 14{,}264 = 18{,}500 \\cdot 14{,}264 = 263{,}884{,}000 \\text{ bytes}.\n$$\n\nAssessment via ratio:\n- The pruning assessment ratio is the archival strategy storage divided by the naive strategy storage:\n$$\nR = \\frac{S_{\\text{archival}}}{S_{\\text{naive}}} = \\frac{263{,}884{,}000}{2{,}196{,}083{,}200}.\n$$\nCompute the decimal value. Observe that\n$$\n2{,}196{,}083{,}200 \\times 0.12 = 263{,}529{,}984,\n$$\nand the residual needed is $263{,}884{,}000 - 263{,}529{,}984 = 354{,}016$. The denominator times $0.00016$ is\n$$\n2{,}196{,}083{,}200 \\times 0.00016 = 351{,}373.312,\n$$\nleaving $2{,}642.688$ residual. The denominator times $0.0000012$ is\n$$\n2{,}196{,}083{,}200 \\times 0.0000012 = 2{,}635.29984,\n$$\nwhich nearly fills the residual. Thus\n$$\nR \\approx 0.12 + 0.00016 + 0.0000012 = 0.1201612.\n$$\nRounded to four significant figures, the ratio is\n$$\nR \\approx 0.1202.\n$$\nThis decimal has no units and represents the multiplicative reduction factor of expected annual on-chain storage achieved by off-chain archival with on-chain commitments relative to the naive strategy.", "answer": "$$\\boxed{0.1202}$$", "id": "4320216"}, {"introduction": "Verifying compliance with consent rules often requires checking sensitive attributes about a data requester, a task that conflicts with the goal of preserving privacy on a distributed ledger. This advanced practice explores a powerful solution to this dilemma: Zero-Knowledge Succinct Non-interactive Arguments of Knowledge (zk-SNARKs). You will translate a specific consent policy into the language of an arithmetic circuit and calculate its 'size' in terms of Rank-1 Constraints (R1CS), providing a concrete entry point into understanding how these cryptographic tools can enable complex, private computations on a blockchain [@problem_id:4320232].", "problem": "Consider a consent management system for precision medicine genomic diagnostics, implemented over a blockchain smart contract that verifies proofs from a zero-knowledge Succinct Non-interactive ARgument of Knowledge (zk-SNARK). The objective is to allow a requester to access encrypted genomic data only if the requester’s role attributes and the declared study scope satisfy published consent predicates, while keeping the requester’s detailed attributes hidden from the verifier. Assume the zk-SNARK is instantiated over a finite field and the computation is modeled as a Rank-1 Constraint System (R1CS), where each constraint is of the form $L(\\mathbf{z}) \\cdot R(\\mathbf{z}) = O(\\mathbf{z})$ for affine linear forms $L$, $R$, and $O$ in the vector of variables $\\mathbf{z}$. Define the circuit size to be the total number of such multiplicative constraints.\n\nLet the requester’s role attribute vector be $\\mathbf{a} = (a_{1}, a_{2}, \\dots, a_{n})$, where each $a_{i} \\in \\{0,1\\}$ encodes the presence of a role attribute (for example, $a_{i} = 1$ might indicate “Institutional Review Board approved investigator”). Let the study scope be represented by a single field element $s$ that encodes an enumerated study identifier. Public consent is published as $m$ disjunctive predicates, each predicate $j \\in \\{1,2,\\dots,m\\}$ specifying a required role attribute index $i_{j}$ with required value $c_{j} \\in \\{0,1\\}$, and a required study scope value $s_{j}$ in the field. A request is authorized if and only if one of the $m$ predicates holds, i.e., $(a_{i_{j}} = c_{j})$ and $(s = s_{j})$ for some $j$.\n\nStarting from the definitions of boolean constraints $x \\in \\{0,1\\}$ enforced via $x(x-1)=0$ and from the R1CS formulation of equality and gating, derive an R1CS circuit that:\n1. Enforces booleanity of each role attribute $a_{i}$.\n2. Uses $m$ indicator variables $b_{1}, \\dots, b_{m}$ with $b_{j} \\in \\{0,1\\}$ to gate the $j$-th predicate such that $b_{j}=1$ forces $(a_{i_{j}} = c_{j})$ and $(s = s_{j})$, while $b_{j}=0$ makes the gate vacuous.\n3. Enforces that exactly one predicate is active by requiring $\\sum_{j=1}^{m} b_{j} = 1$.\n\nAssume each booleanity condition $x(x-1)=0$ costs exactly one R1CS multiplicative constraint, each equality check to a constant is enforced inside a gate by one multiplicative constraint of the form $b_{j} \\cdot (x - \\text{const}) = 0$, and the “exactly one active predicate” condition is enforced as a single multiplicative constraint $(\\sum_{j=1}^{m} b_{j} - 1) \\cdot 1 = 0$. Do not include hashing or commitment verification costs; treat $s$ and all constants as field elements available to the circuit. Using these foundations and a first-principles derivation of the circuit, compute a closed-form expression for the approximate circuit size (the total number of multiplicative R1CS constraints) as a function of $m$ and $n$. Express your final answer as a single analytic expression in $m$ and $n$ only, with no units, and do not round.", "solution": "The problem requires the derivation of a closed-form expression for the total number of multiplicative R1CS constraints, which represents the circuit size, for a specific consent verification logic in a zk-SNARK. The total circuit size is the sum of the constraints required to enforce all the specified logical conditions. Let us derive this size by systematically analyzing each requirement described in the problem statement.\n\nAn R1CS constraint is of the form $L(\\mathbf{z}) \\cdot R(\\mathbf{z}) = O(\\mathbf{z})$, where $L$, $R$, and $O$ are affine linear combinations of the variables in the witness vector $\\mathbf{z}$. The total circuit size is the total count of such multiplicative constraints.\n\nThe derivation is broken down into the costs associated with the three main requirements for the circuit:\n1.  Enforcing the boolean nature of the $n$ role attributes $\\mathbf{a} = (a_1, a_2, \\dots, a_n)$.\n2.  Implementing the gated predicate logic using $m$ indicator variables $\\mathbf{b} = (b_1, b_2, \\dots, b_m)$.\n3.  Enforcing that exactly one of the $m$ predicates is active.\n\nLet's calculate the number of constraints for each part based on the provided cost model.\n\n**1. Constraints for Boolean Attributes ($a_i$)**\nThe problem states that each role attribute $a_i$ must be boolean, i.e., $a_i \\in \\{0, 1\\}$. This condition is enforced using the algebraic constraint:\n$$a_i(a_i - 1) = 0$$\nThis is a single multiplicative constraint for each $a_i$. In the R1CS form $L \\cdot R = O$, we can set $L(\\mathbf{z}) = a_i$, $R(\\mathbf{z}) = a_i - 1$, and $O(\\mathbf{z}) = 0$.\nThe problem specifies that there are $n$ such role attributes. Therefore, the total number of constraints for this part is the number of attributes, $n$.\n$$C_a = n$$\n\n**2. Constraints for Gated Predicate Logic**\nThis part of the circuit uses $m$ indicator variables, $b_1, \\dots, b_m$, to select and enforce one of the $m$ disjunctive predicates. The logic involves ensuring the indicators themselves are boolean and using them to gate the conditions.\n\n**2a. Constraints for Boolean Indicators ($b_j$)**\nThe problem specifies that the indicator variables $b_j$ must also be boolean, i.e., $b_j \\in \\{0, 1\\}$. Similar to the attributes $a_i$, this is enforced by the constraint:\n$$b_j(b_j - 1) = 0$$\nfor each $j \\in \\{1, 2, \\dots, m\\}$.\nAccording to the problem's cost model, each such check costs one multiplicative constraint. As there are $m$ indicator variables, the total number of constraints for ensuring their boolean nature is:\n$$C_b = m$$\n\n**2b. Constraints for Gating the Predicate Conditions**\nFor each predicate $j$, if the corresponding indicator $b_j$ is $1$, the circuit must enforce the conditions $(a_{i_j} = c_j)$ and $(s = s_j)$. If $b_j=0$, these conditions must be vacuous. This is achieved through gated equality constraints.\n\nFor the attribute check, the constraint is:\n$$b_j \\cdot (a_{i_j} - c_j) = 0$$\nFor the study scope check, the constraint is:\n$$b_j \\cdot (s - s_j) = 0$$\nThe problem states that \"each equality check to a constant is enforced inside a gate by one multiplicative constraint\". Each predicate $j$ involves two such equality checks: one for the attribute and one for the scope. Thus, for each of the $m$ predicates, we require $2$ multiplicative constraints.\n\nThe total number of constraints for gating all $m$ predicates is:\n$$C_{\\text{gates}} = 2 \\times m = 2m$$\n\n**3. Constraint for \"Exactly One Active Predicate\"**\nThe final requirement is that exactly one predicate must be satisfied. This is enforced by ensuring that exactly one of the indicator variables $b_j$ is equal to $1$, while all others are $0$. This is expressed by the summation constraint:\n$$\\sum_{j=1}^{m} b_j = 1$$\nThe problem explicitly states that this condition is enforced as a single multiplicative R1CS constraint of the form:\n$$\\left(\\sum_{j=1}^{m} b_j - 1\\right) \\cdot 1 = 0$$\nHere, we can set $L(\\mathbf{z}) = \\sum_{j=1}^{m} b_j - 1$, $R(\\mathbf{z}) = 1$, and $O(\\mathbf{z}) = 0$. This perfectly fits the R1CS structure and, as per the problem's directive, costs exactly one constraint. Note that this constraint, combined with the booleanity constraints on each $b_j$ (from part 2a), is sufficient to guarantee that exactly one $b_j$ is $1$.\nThe total number of constraints for this part is:\n$$C_{\\text{sum}} = 1$$\n\n**Total Circuit Size**\nThe total circuit size, denoted by $S(m, n)$, is the sum of the constraints from all parts derived above.\n$$S(m, n) = C_a + C_b + C_{\\text{gates}} + C_{\\text{sum}}$$\nSubstituting the values we found:\n$$S(m, n) = n + m + 2m + 1$$\n$$S(m, n) = n + 3m + 1$$\nThis is the closed-form expression for the total number of R1CS multiplicative constraints as a function of the number of role attributes, $n$, and the number of consent predicates, $m$.", "answer": "$$\n\\boxed{n + 3m + 1}\n$$", "id": "4320232"}]}