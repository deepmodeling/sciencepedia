{"hands_on_practices": [{"introduction": "Extracting meaningful information from raw physiological time series is a cornerstone of creating digital phenotypes. This practice focuses on Heart Rate Variability (HRV), a powerful digital biomarker reflecting autonomic nervous system function. By deriving and calculating two fundamental time-domain metrics, the Root Mean Square of Successive Differences (RMSSD) and the Standard Deviation of NN intervals (SDNN), you will gain hands-on experience in quantifying physiological dynamics from a sequence of heart beat intervals.", "problem": "A research team in precision health is validating Heart Rate Variability (HRV) features derived from wrist-worn photoplethysmography (PPG) sensors to build a digital phenotype of autonomic function relevant to precision medicine and genomic diagnostics. After artifact screening and ectopic beat removal, the team obtains a sequence of normal-to-normal RR intervals (time between consecutive normal beats) measured in milliseconds (ms) from a stable, short recording window in a participant at rest.\n\nStarting from fundamental definitions appropriate to time series analysis in physiology, derive the expressions for Root Mean Square of Successive Differences (RMSSD) and Standard Deviation of NN intervals (SDNN) for a finite sample of RR intervals. Your derivation must begin from the definitions of the sample mean, the concept of mean-square as an empirical proxy for an expectation, and the unbiased sample variance for a stationary process. Clearly justify any normalization by the sample size.\n\nThen, using the derived expressions, compute RMSSD and SDNN for the following sequence of $N=10$ RR intervals (all values in milliseconds): $910$, $915$, $905$, $920$, $900$, $910$, $905$, $915$, $900$, $925$. Assume the intervals are sampled from a stationary segment with negligible trend so that the unbiased estimators are appropriate.\n\nReport the final numeric values for RMSSD and SDNN, rounded to four significant figures. Express both quantities in milliseconds. In your final answer, list the two values in the order RMSSD followed by SDNN.", "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. The context of Heart Rate Variability (HRV) analysis is a standard topic in biomedical signal processing and precision health. The metrics requested, Root Mean Square of Successive Differences (RMSSD) and Standard Deviation of NN intervals (SDNN), are well-defined and fundamental in this field.\n\nWe begin by deriving the expressions for SDNN and RMSSD from fundamental statistical principles as requested. Let the sequence of $N$ normal-to-normal (NN) RR intervals be denoted by the time series $\\{RR_i\\}_{i=1}^{N}$.\n\n**Derivation of Standard Deviation of NN intervals (SDNN)**\n\nThe SDNN is defined as the standard deviation of the RR interval sequence. Its derivation starts with estimators for the mean and variance of the underlying stationary process from which the sample is drawn.\n\n1.  **Sample Mean ($\\bar{RR}$):** The sample mean is the arithmetic average of the observations and serves as an empirical estimate of the true mean $\\mu$ of the process. It is defined as:\n    $$ \\bar{RR} = \\frac{1}{N} \\sum_{i=1}^{N} RR_i $$\n\n2.  **Unbiased Sample Variance ($s^2$):** The problem specifies using the unbiased sample variance. When the true mean $\\mu$ is unknown and estimated by the sample mean $\\bar{RR}$, the sum of squared deviations $\\sum_{i=1}^{N} (RR_i - \\bar{RR})^2$ has $N-1$ degrees of freedom. To obtain an unbiased estimator of the true population variance $\\sigma^2$ (i.e., an estimator whose expected value is $\\sigma^2$), we normalize this sum by $N-1$ rather than $N$. This is known as Bessel's correction. The unbiased sample variance is therefore:\n    $$ s^2 = \\frac{1}{N-1} \\sum_{i=1}^{N} (RR_i - \\bar{RR})^2 $$\n\n3.  **SDNN:** The SDNN is the square root of the unbiased sample variance.\n    $$ \\text{SDNN} = \\sqrt{s^2} = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N} (RR_i - \\bar{RR})^2} $$\n    This is the final expression for SDNN.\n\n**Derivation of Root Mean Square of Successive Differences (RMSSD)**\n\nThe RMSSD quantifies short-term, high-frequency variability and is derived from the differences between consecutive RR intervals.\n\n1.  **Successive Differences ($\\Delta RR_i$):** First, we compute the sequence of differences between adjacent RR intervals. For a sequence of $N$ intervals, there are $N-1$ such differences:\n    $$ \\Delta RR_i = RR_{i+1} - RR_i \\quad \\text{for } i = 1, 2, \\dots, N-1 $$\n\n2.  **Mean Square of Successive Differences (MSSD):** The problem requires using the concept of the mean-square as an empirical proxy for an expectation. We seek to estimate the expected value of the squared successive differences, $E[(\\Delta RR)^2]$. The empirical estimate is the average of the squared differences observed in the sample. Since there are $N-1$ successive differences, the mean is calculated by summing the squares of these differences and dividing by $N-1$.\n    $$ \\text{MSSD} = \\frac{1}{N-1} \\sum_{i=1}^{N-1} (\\Delta RR_i)^2 = \\frac{1}{N-1} \\sum_{i=1}^{N-1} (RR_{i+1} - RR_i)^2 $$\n    The normalization by $N-1$ is intrinsic to the definition of the sample mean for the $N-1$ squared difference values.\n\n3.  **RMSSD:** The RMSSD is, by definition, the square root of the MSSD.\n    $$ \\text{RMSSD} = \\sqrt{\\text{MSSD}} = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N-1} (RR_{i+1} - RR_i)^2} $$\n    This is the final expression for RMSSD.\n\n**Calculation for the Given Data**\n\nThe provided sequence of $N=10$ RR intervals (in milliseconds) is:\n$RR = \\{910, 915, 905, 920, 900, 910, 905, 915, 900, 925\\}$.\n\n**Calculation of SDNN:**\n\n1.  Calculate the sample mean $\\bar{RR}$:\n    $$ \\sum_{i=1}^{10} RR_i = 910 + 915 + 905 + 920 + 900 + 910 + 905 + 915 + 900 + 925 = 9105 $$\n    $$ \\bar{RR} = \\frac{9105}{10} = 910.5 \\text{ ms} $$\n\n2.  Calculate the sum of squared deviations from the mean, $\\sum_{i=1}^{10} (RR_i - \\bar{RR})^2$:\n    $$ (910 - 910.5)^2 + (915 - 910.5)^2 + (905 - 910.5)^2 + (920 - 910.5)^2 + (900 - 910.5)^2 \\\\ + (910 - 910.5)^2 + (905 - 910.5)^2 + (915 - 910.5)^2 + (900 - 910.5)^2 + (925 - 910.5)^2 \\\\ = (-0.5)^2 + (4.5)^2 + (-5.5)^2 + (9.5)^2 + (-10.5)^2 + (-0.5)^2 + (-5.5)^2 + (4.5)^2 + (-10.5)^2 + (14.5)^2 \\\\ = 0.25 + 20.25 + 30.25 + 90.25 + 110.25 + 0.25 + 30.25 + 20.25 + 110.25 + 210.25 = 622.5 \\text{ ms}^2 $$\n\n3.  Calculate SDNN using the derived formula with $N=10$:\n    $$ \\text{SDNN} = \\sqrt{\\frac{1}{10-1} \\sum_{i=1}^{10} (RR_i - \\bar{RR})^2} = \\sqrt{\\frac{622.5}{9}} = \\sqrt{69.166...} \\approx 8.31665 \\text{ ms} $$\n    Rounded to four significant figures, $\\text{SDNN} = 8.317 \\text{ ms}$.\n\n**Calculation of RMSSD:**\n\n1.  Calculate the successive differences, $\\Delta RR_i = RR_{i+1} - RR_i$:\n    $$ \\Delta RR_1 = 915 - 910 = 5 $$\n    $$ \\Delta RR_2 = 905 - 915 = -10 $$\n    $$ \\Delta RR_3 = 920 - 905 = 15 $$\n    $$ \\Delta RR_4 = 900 - 920 = -20 $$\n    $$ \\Delta RR_5 = 910 - 900 = 10 $$\n    $$ \\Delta RR_6 = 905 - 910 = -5 $$\n    $$ \\Delta RR_7 = 915 - 905 = 10 $$\n    $$ \\Delta RR_8 = 900 - 915 = -15 $$\n    $$ \\Delta RR_9 = 925 - 900 = 25 $$\n\n2.  Calculate the sum of the squares of these $N-1=9$ differences, $\\sum_{i=1}^{9} (\\Delta RR_i)^2$:\n    $$ 5^2 + (-10)^2 + 15^2 + (-20)^2 + 10^2 + (-5)^2 + 10^2 + (-15)^2 + 25^2 \\\\ = 25 + 100 + 225 + 400 + 100 + 25 + 100 + 225 + 625 = 1825 \\text{ ms}^2 $$\n\n3.  Calculate RMSSD using the derived formula:\n    $$ \\text{RMSSD} = \\sqrt{\\frac{1}{10-1} \\sum_{i=1}^{9} (\\Delta RR_i)^2} = \\sqrt{\\frac{1825}{9}} = \\sqrt{202.777...} \\approx 14.2399... \\text{ ms} $$\n    Rounded to four significant figures, $\\text{RMSSD} = 14.24 \\text{ ms}$.\n\nThe final values are RMSSD $\\approx 14.24$ ms and SDNN $\\approx 8.317$ ms.", "answer": "$$\\boxed{\\begin{pmatrix} 14.24 & 8.317 \\end{pmatrix}}$$", "id": "4396329"}, {"introduction": "The fidelity of a digital phenotype depends critically on the data acquisition process. This exercise delves into aliasing, a fundamental pitfall in digital signal processing where a signal's true frequency is misinterpreted due to an insufficient sampling rate. By analyzing a hypothetical scenario involving a photoplethysmography (PPG) signal, you will derive the aliasing condition from first principles and understand how to predict the observed frequency, a crucial skill for correctly interpreting data from wearable sensors.", "problem": "A wrist-worn photoplethysmography (PPG) sensor is used in a precision health study to derive a digital phenotype of cardiac rhythm. The underlying clean arterial volume waveform can be modeled as a single-frequency sinusoid with continuous-time frequency $f_0$ (in $\\mathrm{Hz}$), amplitude $A$, and phase $\\phi$, so that the physiological signal is $s(t)=A\\sin(2\\pi f_0 t+\\phi)$. The wearable samples this signal uniformly at rate $f_s$ (in $\\mathrm{Hz}$), with sampling period $T_s=1/f_s$, producing the discrete-time sequence $x[n]=s(nT_s)$.\n\nStarting from the definition of uniform sampling $x[n]=s(nT_s)$, the periodicity of the sine function, and the fact that discrete-time sinusoids are defined modulo $2\\pi$ in angular frequency, derive the aliasing condition that characterizes the set of continuous-time frequencies that produce identical discrete-time samples. Then, from first principles, obtain a closed-form expression for the observed alias frequency in the principal lowpass band $[0,\\,f_s/2]$ in terms of $f_0$ and $f_s$ only.\n\nFinally, evaluate the observed alias frequency when $f_0=3.2\\,\\mathrm{Hz}$ and $f_s=5\\,\\mathrm{Hz}$. Express the final numerical answer in $\\mathrm{Hz}$. No rounding is required.", "solution": "The problem statement is deemed valid. It is scientifically grounded in the principles of digital signal processing, specifically the phenomenon of aliasing in uniform sampling. It is well-posed, objective, and provides all necessary information for a complete derivation and solution.\n\nThe derivation proceeds in three parts as requested: first, establishing the aliasing condition; second, deriving the expression for the observed alias frequency in the principal band; and third, evaluating this expression for the given numerical values.\n\n**Part 1: Derivation of the Aliasing Condition**\n\nThe continuous-time physiological signal is given by the model:\n$$\ns(t) = A\\sin(2\\pi f_0 t + \\phi)\n$$\nwhere $f_0$ is the continuous-time frequency in $\\mathrm{Hz}$.\n\nThis signal is uniformly sampled at a rate of $f_s$ $\\mathrm{Hz}$, with a sampling period $T_s = 1/f_s$. The resulting discrete-time sequence, $x[n]$, is obtained by evaluating $s(t)$ at time instants $t = nT_s$, where $n$ is an integer index.\n$$\nx[n] = s(nT_s) = A\\sin(2\\pi f_0 nT_s + \\phi)\n$$\nSubstituting $T_s = 1/f_s$, the expression becomes:\n$$\nx[n] = A\\sin\\left(2\\pi \\frac{f_0}{f_s} n + \\phi\\right)\n$$\nThis is the standard form of a discrete-time sinusoid, $x[n] = A\\sin(\\omega_0 n + \\phi)$, where the discrete-time angular frequency is $\\omega_0 = 2\\pi \\frac{f_0}{f_s}$.\n\nThe fundamental property of discrete-time sinusoids is their periodicity in the frequency domain. Specifically, a discrete-time angular frequency $\\omega$ is equivalent to any frequency $\\omega + 2\\pi k$ for any integer $k$, because for any integer sample index $n$:\n$$\n\\sin((\\omega + 2\\pi k)n + \\phi) = \\sin(\\omega n + 2\\pi kn + \\phi) = \\sin(\\omega n + \\phi)\n$$\nThe term $2\\pi kn$ is an integer multiple of $2\\pi$, and the sine function is periodic with period $2\\pi$, hence this term does not alter the value of the function.\n\nNow, consider two different continuous-time frequencies, $f_a$ and $f_b$, which are sampled at the same rate $f_s$. The corresponding discrete-time angular frequencies are $\\omega_a = 2\\pi (f_a/f_s)$ and $\\omega_b = 2\\pi (f_b/f_s)$. For these two frequencies to produce identical discrete-time samples (i.e., for them to be aliases of each other), their discrete-time angular frequencies must be equivalent modulo $2\\pi$. That is, there must exist some integer $k$ such that:\n$$\n\\omega_a = \\omega_b + 2\\pi k\n$$\nSubstituting the expressions for $\\omega_a$ and $\\omega_b$:\n$$\n2\\pi \\frac{f_a}{f_s} = 2\\pi \\frac{f_b}{f_s} + 2\\pi k\n$$\nDividing the entire equation by $2\\pi$ yields:\n$$\n\\frac{f_a}{f_s} = \\frac{f_b}{f_s} + k\n$$\nMultiplying by $f_s$, we arrive at the aliasing condition:\n$$\nf_a = f_b + kf_s\n$$\nThis condition characterizes the set of all continuous-time frequencies that are indistinguishable from a frequency $f_b$ when sampled at a rate $f_s$.\n\nFurthermore, we must also consider the identity $\\sin(\\theta) = -\\sin(-\\theta)$, which implies that a frequency $f$ can also be confused with $-f$. The phase will be shifted, but the frequency content is the same in magnitude. The complete aliasing condition states that two continuous frequencies $f_a$ and $f_b$ are aliases if they satisfy $f_a = \\pm f_b + kf_s$ for some integer $k$.\n\n**Part 2: Derivation of the Observed Alias Frequency**\n\nThe observed alias frequency, which we denote as $f_{obs}$, is conventionally defined as the unique frequency within the principal lowpass band $[0, f_s/2]$ that produces the same set of samples as the original frequency $f_0$. This corresponds to the lowest-frequency real sinusoid that can represent the sampled data.\n\nBased on the complete aliasing condition, we are looking for a frequency $f_{obs}$ that satisfies two criteria:\n1. $0 \\le f_{obs} \\le f_s/2$\n2. $f_0 = \\pm f_{obs} + kf_s$ for some integer $k$.\n\nFrom the second criterion, we can write $f_{obs} = \\pm(f_0 - kf_s)$. Since $f_{obs}$ must be non-negative, this simplifies to:\n$$\nf_{obs} = |f_0 - kf_s|\n$$\nWe need to find the specific integer $k$ for which this expression for $f_{obs}$ falls into the range $[0, f_s/2]$. This is equivalent to finding the integer multiple of $f_s$, denoted $kf_s$, that is closest to the original frequency $f_0$. The absolute difference between $f_0$ and this closest multiple will be our observed frequency.\n\nThe integer $k$ that brings $kf_s$ closest to $f_0$ is found by rounding the ratio $f_0/f_s$ to the nearest integer. Let this integer be:\n$$\nk = \\text{round}\\left(\\frac{f_0}{f_s}\\right)\n$$\nThe operator $\\text{round}(y)$ gives the integer nearest to $y$. Substituting this value of $k$ back into our expression for $f_{obs}$ gives the final closed-form expression:\n$$\nf_{obs} = \\left|f_0 - f_s \\cdot \\text{round}\\left(\\frac{f_0}{f_s}\\right)\\right|\n$$\nThis formula maps any positive continuous-time frequency $f_0$ to its alias in the principal band $[0, f_s/2]$.\n\n**Part 3: Numerical Evaluation**\n\nWe are asked to evaluate the observed alias frequency for the specific values $f_0 = 3.2\\,\\mathrm{Hz}$ and $f_s = 5\\,\\mathrm{Hz}$.\n\nFirst, we compute the ratio of the frequencies:\n$$\n\\frac{f_0}{f_s} = \\frac{3.2}{5} = 0.64\n$$\nNext, we find the nearest integer to this ratio:\n$$\nk = \\text{round}(0.64) = 1\n$$\nFinally, we substitute these values into the derived expression for $f_{obs}$:\n$$\nf_{obs} = |f_0 - k \\cdot f_s| = |3.2 - 1 \\cdot 5| = |3.2 - 5| = |-1.8|\n$$\nThe observed frequency is the magnitude of this result:\n$$\nf_{obs} = 1.8\\,\\mathrm{Hz}\n$$\nThis result is in the principal band $[0, f_s/2]$, which is $[0, 2.5\\,\\mathrm{Hz}]$, as required. The sampling rate of $5\\,\\mathrm{Hz}$ is insufficient to uniquely represent the true frequency of $3.2\\,\\mathrm{Hz}$ (since $3.2 > 5/2$), leading it to be aliased to the lower frequency of $1.8\\,\\mathrm{Hz}$.", "answer": "$$\\boxed{1.8}$$", "id": "4396363"}, {"introduction": "Precision health often involves analyzing how biomarkers change over time, accounting for both population trends and individual differences. This advanced practice introduces Linear Mixed Models (LMMs), a powerful framework for studying longitudinal and hierarchical data common in wearable sensor studies. Through the practical task of modeling weekly rhythms in daily step counts, you will implement a model that disentangles fixed population effects from subject-specific random effects and perform a formal hypothesis test.", "problem": "A cohort of $S$ individuals is monitored with wrist-worn accelerometer-based wearable sensors that report daily step counts, an objective digital phenotype relevant to precision health. Daily step counts often exhibit weekly seasonality due to behavioral routines (for example, weekdays versus weekends). The task is to construct and fit a Linear Mixed Model (LMM) that captures weekly seasonality with fixed effects and allows subject-specific random intercepts and random weekend slopes, and then to test whether there is a statistically significant weekend effect at a prespecified significance level. The analysis must be framed entirely in mathematical and algorithmic terms and must rely on the Gaussian modeling framework.\n\nFundamental base to use:\n- Assume a Gaussian linear mixed modeling framework for daily activity levels: the observed daily steps $y$ are modeled as a linear combination of fixed effects and random effects plus Gaussian noise.\n- Assume independent and identically distributed Gaussian measurement noise across days, and independent Gaussian random effects across subjects.\n- Assume weekly seasonality can be represented using a harmonic basis with cosine and sine components that are functions of the day-of-week angle in radians.\n\nModeling requirements:\n- Construct the fixed-effects design matrix $X$ with columns: an intercept, a weekend indicator, and harmonic terms $\\cos\\!\\left(2\\pi d/7\\right)$ and $\\sin\\!\\left(2\\pi d/7\\right)$, where $d \\in \\{0,1,2,3,4,5,6\\}$ is the day-of-week index and angles must be in radians. The weekend indicator must be $1$ for Saturday and Sunday and $0$ otherwise. All answers involving angles must be computed in radians.\n- Construct the random-effects design such that each subject has a random intercept and a random slope associated with the weekend indicator. Random effects must be assumed independent across subjects and independent between intercept and slope within subjects.\n- Fit the LMM by Maximum Likelihood (ML) under the Gaussian assumptions. The residual variance and both random-effects variances must be treated as unknown parameters to be estimated from the data, subject to strict positivity.\n- Perform a likelihood ratio test for the presence of a fixed weekend effect by comparing the full model (including the fixed weekend effect) to the nested null model (with the fixed weekend effect constrained to zero) while keeping the same random-effects structure in both models. Compute the likelihood ratio statistic and its corresponding $p$-value under a chi-squared distribution with one degree of freedom. Use a significance threshold of $\\alpha = 0.05$ (express $\\alpha$ as a decimal).\n\nSynthetic data generation protocol within the program:\n- For subject $s \\in \\{1,\\dots,S\\}$ and day index $t \\in \\{0,1,\\dots,D-1\\}$, let the day-of-week index be $d_t = t \\bmod 7$. Let the weekend indicator be $w_t = 1$ if $d_t \\in \\{5,6\\}$ and $w_t = 0$ otherwise. Generate steps per day as\n$$\ny_{s,t} = \\beta_0 + b_{0,s} + (\\beta_w + b_{1,s})\\, w_t + \\beta_c \\cos\\!\\left(\\frac{2\\pi d_t}{7}\\right) + \\beta_s \\sin\\!\\left(\\frac{2\\pi d_t}{7}\\right) + \\varepsilon_{s,t},\n$$\nwhere $b_{0,s} \\sim \\mathcal{N}(0,\\sigma_{b0}^2)$, $b_{1,s} \\sim \\mathcal{N}(0,\\sigma_{b1}^2)$, $\\varepsilon_{s,t} \\sim \\mathcal{N}(0,\\sigma_e^2)$, and all random quantities are independent. Steps per day are the physical unit in the generative process, but the outputs required by this problem are dimensionless statistical quantities (booleans), so no physical units need to be expressed in the final output.\n\nTest suite specifications:\n- The program must generate and analyze three test cases. In each case, set the random seed to ensure reproducibility and use the following parameters (all numbers are in steps per day or steps per day squared, as appropriate):\n    1. Case A (happy path): $S=20$, $D=28$, $\\beta_0=7000$, $\\beta_w=2000$, $\\beta_c=400$, $\\beta_s=-200$, $\\sigma_{b0}=800$, $\\sigma_{b1}=300$, $\\sigma_e=500$, with a fixed random seed of $12345$.\n    2. Case B (null weekend effect, boundary condition): $S=20$, $D=28$, $\\beta_0=7000$, $\\beta_w=0$, $\\beta_c=400$, $\\beta_s=-200$, $\\sigma_{b0}=800$, $\\sigma_{b1}=400$, $\\sigma_e=600$, with a fixed random seed of $23456$.\n    3. Case C (edge case with small sample and high random slope variance): $S=6$, $D=14$, $\\beta_0=7000$, $\\beta_w=500$, $\\beta_c=300$, $\\beta_s=100$, $\\sigma_{b0}=900$, $\\sigma_{b1}=1000$, $\\sigma_e=700$, with a fixed random seed of $34567$.\n\nComputation of the test statistic and decision rule:\n- For each case, fit the full model and the nested null model by Maximum Likelihood, compute the likelihood ratio statistic\n$$\n\\lambda = 2\\big(\\ell_{\\text{full}} - \\ell_{\\text{null}}\\big),\n$$\nand evaluate the corresponding $p$-value against a chi-squared distribution with $1$ degree of freedom. Declare the weekend effect significant if the $p$-value is strictly less than $\\alpha = 0.05$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of Python booleans enclosed in square brackets, in the order of the cases listed above. For example, the output must look like `[True,False,True]` (this illustrative example is not the correct answer; your program must compute the actual results).", "solution": "The problem requires the construction and fitting of a Gaussian Linear Mixed Model (LMM) to analyze synthetic daily step count data, followed by a hypothesis test for a specific fixed effect. The solution proceeds by first defining the mathematical structure of the model, then detailing the Maximum Likelihood (ML) estimation procedure, and finally specifying the Likelihood Ratio Test (LRT) for the hypothesis of interest.\n\n### 1. Linear Mixed Model Formulation\n\nThe LMM for the daily step count $y_{s,t}$ of subject $s \\in \\{1, \\dots, S\\}$ on day $t \\in \\{0, \\dots, D-1\\}$ is given by:\n$$\ny_{s,t} = \\boldsymbol{x}_{s,t}^T \\boldsymbol{\\beta} + \\boldsymbol{z}_{s,t}^T \\boldsymbol{b}_s + \\varepsilon_{s,t}\n$$\nThis can be expressed in matrix form for each subject. Let $\\boldsymbol{y}_s = [y_{s,0}, \\dots, y_{s,D-1}]^T$ be the $D \\times 1$ vector of observations for subject $s$. The model for subject $s$ is:\n$$\n\\boldsymbol{y}_s = \\boldsymbol{X}_s \\boldsymbol{\\beta} + \\boldsymbol{Z}_s \\boldsymbol{b}_s + \\boldsymbol{\\varepsilon}_s\n$$\nThe components of this model are defined as follows:\n\n- **Fixed Effects**: The fixed-effects design matrix for subject $s$ is $\\boldsymbol{X}_s \\in \\mathbb{R}^{D \\times p}$, and $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is the vector of fixed-effects coefficients. For the full model, we have $p=4$ fixed effects: an intercept, a weekend effect, and two harmonic terms for weekly seasonality. Let $d_t = t \\pmod 7$ be the day-of-week index. The weekend indicator is $w_t=1$ if $d_t \\in \\{5, 6\\}$ (Saturday, Sunday) and $w_t=0$ otherwise. The rows of $\\boldsymbol{X}_s$ are $\\boldsymbol{x}_{s,t}^T = [1, w_t, \\cos(2\\pi d_t/7), \\sin(2\\pi d_t/7)]$. The corresponding fixed-effects vector is $\\boldsymbol{\\beta} = [\\beta_0, \\beta_w, \\beta_c, \\beta_s]^T$.\n\n- **Random Effects**: The random-effects design matrix for subject $s$ is $\\boldsymbol{Z}_s \\in \\mathbb{R}^{D \\times q}$, and $\\boldsymbol{b}_s \\in \\mathbb{R}^q$ is the vector of subject-specific random effects. The model includes a random intercept and a random weekend slope for each subject, making $q=2$. The rows of $\\boldsymbol{Z}_s$ are $\\boldsymbol{z}_{s,t}^T = [1, w_t]$. The random-effects vector is $\\boldsymbol{b}_s = [b_{0,s}, b_{1,s}]^T$.\n\n- **Distributional Assumptions**: The random effects and measurement errors are assumed to be independent and normally distributed:\n    - $\\boldsymbol{b}_s \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{G})$, where $\\boldsymbol{G}$ is the $q \\times q$ covariance matrix of the random effects. Due to the specified independence between the random intercept and slope, $\\boldsymbol{G}$ is diagonal:\n      $$\n      \\boldsymbol{G} = \\begin{pmatrix} \\sigma_{b0}^2 & 0 \\\\ 0 & \\sigma_{b1}^2 \\end{pmatrix}\n      $$\n    - $\\boldsymbol{\\varepsilon}_s \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{R}_s)$, where $\\boldsymbol{R}_s$ is the $D \\times D$ covariance matrix of the residuals. With independent and identically distributed noise, $\\boldsymbol{R}_s = \\sigma_e^2 \\boldsymbol{I}_D$, where $\\boldsymbol{I}_D$ is the $D \\times D$ identity matrix.\n\nThe unknown variance components to be estimated are $\\sigma_{b0}^2$, $\\sigma_{b1}^2$, and $\\sigma_e^2$.\n\n### 2. Maximum Likelihood Estimation\n\nUnder these assumptions, the marginal distribution of the observation vector $\\boldsymbol{y}_s$ for subject $s$ is a multivariate normal distribution:\n$$\n\\boldsymbol{y}_s \\sim \\mathcal{N}(\\boldsymbol{X}_s \\boldsymbol{\\beta}, \\boldsymbol{V}_s)\n$$\nwhere the marginal covariance matrix $\\boldsymbol{V}_s$ is given by:\n$$\n\\boldsymbol{V}_s = \\boldsymbol{Z}_s \\boldsymbol{G} \\boldsymbol{Z}_s^T + \\boldsymbol{R}_s = \\sigma_{b0}^2 (\\boldsymbol{Z}_s \\boldsymbol{J}_1 \\boldsymbol{Z}_s^T) + \\sigma_{b1}^2 (\\boldsymbol{Z}_s \\boldsymbol{J}_2 \\boldsymbol{Z}_s^T) + \\sigma_e^2 \\boldsymbol{I}_D\n$$\nwith $\\boldsymbol{J}_1 = \\text{diag}(1,0)$ and $\\boldsymbol{J}_2 = \\text{diag}(0,1)$.\n\nSince subjects are independent, the total log-likelihood for the entire dataset $\\boldsymbol{y} = \\{\\boldsymbol{y}_1, \\dots, \\boldsymbol{y}_S\\}$ is the sum of the individual log-likelihoods:\n$$\n\\ell(\\boldsymbol{\\beta}, \\sigma_{b0}^2, \\sigma_{b1}^2, \\sigma_e^2 | \\boldsymbol{y}) = \\sum_{s=1}^S \\ell_s(\\boldsymbol{\\beta}, \\sigma_{b0}^2, \\sigma_{b1}^2, \\sigma_e^2 | \\boldsymbol{y}_s)\n$$\nwhere\n$$\n\\ell_s = -\\frac{D}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{V}_s| - \\frac{1}{2}(\\boldsymbol{y}_s - \\boldsymbol{X}_s\\boldsymbol{\\beta})^T \\boldsymbol{V}_s^{-1} (\\boldsymbol{y}_s - \\boldsymbol{X}_s\\boldsymbol{\\beta})\n$$\nTo find the Maximum Likelihood Estimates (MLEs), we maximize $\\ell$ with respect to $\\boldsymbol{\\theta} = (\\boldsymbol{\\beta}, \\sigma_{b0}^2, \\sigma_{b1}^2, \\sigma_e^2)$. This is typically done using a profile likelihood approach. For fixed variance components $\\boldsymbol{\\psi} = (\\sigma_{b0}^2, \\sigma_{b1}^2, \\sigma_e^2)$, the MLE for $\\boldsymbol{\\beta}$ is the generalized least squares (GLS) estimator:\n$$\n\\hat{\\boldsymbol{\\beta}}(\\boldsymbol{\\psi}) = \\left( \\sum_{s=1}^S \\boldsymbol{X}_s^T \\boldsymbol{V}_s^{-1} \\boldsymbol{X}_s \\right)^{-1} \\left( \\sum_{s=1}^S \\boldsymbol{X}_s^T \\boldsymbol{V}_s^{-1} \\boldsymbol{y}_s \\right)\n$$\nSubstituting $\\hat{\\boldsymbol{\\beta}}(\\boldsymbol{\\psi})$ into the log-likelihood function yields the profile log-likelihood, $\\ell_p(\\boldsymbol{\\psi})$, which depends only on the variance components. We then numerically optimize $\\ell_p(\\boldsymbol{\\psi})$ with respect to $\\boldsymbol{\\psi}$, subject to the constraints $\\sigma_{b0}^2 > 0$, $\\sigma_{b1}^2 > 0$, and $\\sigma_e^2 > 0$. The value of the objective function at the minimum, when minimizing the negative log-profile-likelihood, allows computation of the maximized log-likelihood $\\ell_{\\text{max}}$.\n\n### 3. Likelihood Ratio Test for the Fixed Weekend Effect\n\nWe test the null hypothesis $H_0: \\beta_w = 0$ against the alternative hypothesis $H_1: \\beta_w \\neq 0$.\n\n- **Full Model ($H_1$)**: The model described above with $p=4$ fixed effects, including the weekend coefficient $\\beta_w$. The MLE procedure yields a maximized log-likelihood $\\ell_{\\text{full}}$.\n\n- **Null Model ($H_0$)**: A nested model where $\\beta_w$ is constrained to be zero. The fixed-effects design matrix $\\boldsymbol{X}_{s, \\text{null}}$ has $p=3$ columns (intercept, cosine, and sine terms), and the corresponding fixed-effects vector is $\\boldsymbol{\\beta}_{\\text{null}} = [\\beta_0, \\beta_c, \\beta_s]^T$. The random-effects structure remains unchanged. Fitting this model via ML yields a maximized log-likelihood $\\ell_{\\text{null}}$.\n\nThe Likelihood Ratio Test statistic $\\lambda$ is computed as:\n$$\n\\lambda = 2(\\ell_{\\text{full}} - \\ell_{\\text{null}})\n$$\nAccording to Wilks' theorem, under the null hypothesis, $\\lambda$ asymptotically follows a chi-squared distribution with degrees of freedom equal to the difference in the number of parameters between the two models. Here, one parameter ($\\beta_w$) is constrained, so the degrees of freedom is $df = 1$.\n\nThe $p$-value is calculated as the survival function of the $\\chi^2_1$ distribution evaluated at $\\lambda$, i.e., $p = P(\\chi^2_1 \\ge \\lambda)$. The weekend effect is deemed statistically significant if this $p$-value is strictly less than the prespecified significance level $\\alpha = 0.05$.\n\nThe algorithmic implementation will involve:\n1.  Generating synthetic data for each test case according to the specified protocol.\n2.  Defining a function that calculates the negative profile log-likelihood for a given set of variance parameters.\n3.  Using a numerical optimizer (`scipy.optimize.minimize` with method 'L-BFGS-B') to find the MLEs of the variance components for both the full and null models.\n4.  Calculating the LRT statistic $\\lambda$ and its corresponding $p$-value.\n5.  Comparing the $p$-value to $\\alpha=0.05$ to decide on statistical significance for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import chi2\n\ndef generate_synthetic_data(S, D, beta_0, beta_w, beta_c, beta_s, sigma_b0, sigma_b1, sigma_e, seed):\n    \"\"\"Generates synthetic data according to the problem specification.\"\"\"\n    rng = np.random.default_rng(seed)\n    \n    Y_subjects = []\n    \n    # Common design matrix components for all subjects\n    t_indices = np.arange(D)\n    d_indices = t_indices % 7\n    weekend_indicator = np.isin(d_indices, [5, 6]).astype(int)\n\n    intercept_col = np.ones(D)\n    weekend_col = weekend_indicator\n    cos_col = np.cos(2 * np.pi * d_indices / 7)\n    sin_col = np.sin(2 * np.pi * d_indices / 7)\n    \n    X_full_template = np.stack([intercept_col, weekend_col, cos_col, sin_col], axis=1)\n    Z_template = np.stack([intercept_col, weekend_col], axis=1)\n    \n    beta_vector = np.array([beta_0, beta_w, beta_c, beta_s])\n    \n    for s in range(S):\n        b0_s = rng.normal(0, sigma_b0)\n        b1_s = rng.normal(0, sigma_b1)\n        epsilon_s = rng.normal(0, sigma_e, size=D)\n        \n        y_s = (X_full_template @ beta_vector + \n               Z_template @ np.array([b0_s, b1_s]) +\n               epsilon_s)\n               \n        Y_subjects.append(y_s)\n        \n    return Y_subjects\n\ndef get_design_matrices(S, D):\n    \"\"\"Constructs the design matrices X_full, X_null, and Z for S subjects over D days.\"\"\"\n    t_indices = np.arange(D)\n    d_indices = t_indices % 7\n    weekend_indicator = np.isin(d_indices, [5, 6]).astype(int)\n\n    intercept_col = np.ones(D)\n    weekend_col = weekend_indicator\n    cos_col = np.cos(2 * np.pi * d_indices / 7)\n    sin_col = np.sin(2 * np.pi * d_indices / 7)\n    \n    X_full_template = np.stack([intercept_col, weekend_col, cos_col, sin_col], axis=1)\n    X_null_template = np.stack([intercept_col, cos_col, sin_col], axis=1)\n    Z_template = np.stack([intercept_col, weekend_col], axis=1)\n    \n    X_full_subjects = [X_full_template] * S\n    X_null_subjects = [X_null_template] * S\n    Z_subjects = [Z_template] * S\n    \n    return X_full_subjects, X_null_subjects, Z_subjects\n\n\ndef negative_log_likelihood(var_params, Y_subjects, X_subjects, Z_subjects):\n    \"\"\"Calculates the negative profile log-likelihood for the LMM.\"\"\"\n    sigma2_b0, sigma2_b1, sigma2_e = var_params\n    \n    S = len(Y_subjects)\n    if S == 0:\n        return 0.\n    D = Y_subjects[0].shape[0]\n    p = X_subjects[0].shape[1]\n    \n    G = np.diag([sigma2_b0, sigma2_b1])\n    I_D = np.eye(D)\n\n    sum_Xt_V_inv_X = np.zeros((p, p))\n    sum_Xt_V_inv_y = np.zeros(p)\n    profile_loglik_val = 0\n    \n    try:\n        V_inv_list = []\n        for s in range(S):\n            Z_s = Z_subjects[s]\n            V_s = Z_s @ G @ Z_s.T + sigma2_e * I_D\n            \n            V_s_inv = np.linalg.inv(V_s)\n            sign, logdet_V_s = np.linalg.slogdet(V_s)\n            \n            if sign <= 0:\n                return np.inf\n\n            profile_loglik_val += logdet_V_s\n            V_inv_list.append(V_s_inv)\n            \n            X_s = X_subjects[s]\n            y_s = Y_subjects[s]\n            \n            sum_Xt_V_inv_X += X_s.T @ V_s_inv @ X_s\n            sum_Xt_V_inv_y += X_s.T @ V_s_inv @ y_s\n\n        beta_hat = np.linalg.solve(sum_Xt_V_inv_X, sum_Xt_V_inv_y)\n\n        for s in range(S):\n            X_s = X_subjects[s]\n            y_s = Y_subjects[s]\n            V_s_inv = V_inv_list[s]\n            \n            residuals = y_s - X_s @ beta_hat\n            profile_loglik_val += residuals.T @ V_s_inv @ residuals\n    \n    except np.linalg.LinAlgError:\n        return np.inf\n\n    return 0.5 * profile_loglik_val\n\n\ndef fit_lmm(Y_subjects, X_subjects, Z_subjects):\n    \"\"\"Fits the LMM by ML and returns the minimized negative profile log-likelihood value.\"\"\"\n    # Use variance of all data points as a scale for initial guesses\n    initial_scale = np.var(np.concatenate(Y_subjects))\n    initial_guesses = [initial_scale * 0.3, initial_scale * 0.3, initial_scale * 0.4]\n\n    bounds = [(1e-9, None), (1e-9, None), (1e-9, None)]\n    \n    result = minimize(\n        negative_log_likelihood,\n        x0=initial_guesses,\n        args=(Y_subjects, X_subjects, Z_subjects),\n        method='L-BFGS-B',\n        bounds=bounds\n    )\n    return result.fun\n\n\ndef solve():\n    \"\"\"Main function to run the analysis for all test cases.\"\"\"\n    test_cases = [\n        # S, D, beta_0, beta_w, beta_c, beta_s, sigma_b0, sigma_b1, sigma_e, seed\n        (20, 28, 7000, 2000, 400, -200, 800, 300, 500, 12345),\n        (20, 28, 7000, 0, 400, -200, 800, 400, 600, 23456),\n        (6, 14, 7000, 500, 300, 100, 900, 1000, 700, 34567),\n    ]\n\n    alpha = 0.05\n    results = []\n\n    for S, D, b0, bw, bc, bs, sb0, sb1, se, seed in test_cases:\n        Y_subjects = generate_synthetic_data(S, D, b0, bw, bc, bs, sb0, sb1, se, seed)\n        X_full_subjects, X_null_subjects, Z_subjects = get_design_matrices(S, D)\n        \n        # Fit the full model\n        min_neg_loglik_full = fit_lmm(Y_subjects, X_full_subjects, Z_subjects)\n        \n        # Fit the null model\n        min_neg_loglik_null = fit_lmm(Y_subjects, X_null_subjects, Z_subjects)\n        \n        # Likelihood Ratio Test\n        # lambda = 2 * (l_full - l_null)\n        # l = - (SD/2)log(2pi) - min_neg_loglik\n        # lambda = 2 * ((-min_neg_loglik_full) - (-min_neg_loglik_null))\n        lambda_stat = 2 * (min_neg_loglik_null - min_neg_loglik_full)\n        \n        # Handle cases where optimization might fail or result in non-positive lambda\n        if lambda_stat < 0:\n            lambda_stat = 0\n\n        p_value = chi2.sf(lambda_stat, df=1)\n        \n        results.append(p_value < alpha)\n        \n    # The final print statement must match the specified format\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```", "id": "4396397"}]}