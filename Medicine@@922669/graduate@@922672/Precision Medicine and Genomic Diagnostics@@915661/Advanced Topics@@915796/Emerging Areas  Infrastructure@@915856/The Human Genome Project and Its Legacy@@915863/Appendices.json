{"hands_on_practices": [{"introduction": "The journey from a biological sample to a clinical diagnosis begins with sequencing, but the raw output of a sequencer is not the final word on data quantity. This exercise tackles the first critical step in data quality control: determining the *effective* coverage of a genome. By accounting for data loss from quality filtering and artifact removal, you will calculate the true sequencing depth, a metric that directly governs the statistical power to detect genetic variants [@problem_id:4391383].", "problem": "A core operational implication of the Human Genome Project (HGP) in precision medicine is that short-read datasets can be referenced to a high-quality human assembly to quantify effective sequencing depth, which governs sensitivity for variant discovery. Consider a whole-genome clinical sequencing run performed on the human reference genome established in the HGP’s legacy, where the haploid effective genome size is $G = 3.10 \\times 10^{9}$ base pairs. The instrument yields $N = 1.2 \\times 10^{9}$ paired-end read pairs, with each read of length $L = 150$ base pairs (so each read pair contributes $2L$ bases). During quality control, a fraction $0.15$ of read pairs is discarded for low quality or adapter contamination. Among the remaining read pairs, a further fraction $0.20$ is removed as Polymerase Chain Reaction (PCR) duplicates at the read-pair level. Assume that all retained read pairs map uniquely to the reference and contribute their full length, and ignore any overlap between mates or soft clipping.\n\nUsing only first principles and core definitions from sequencing and coverage, compute the effective average sequencing depth (fold coverage) for the haploid genome implied by these data. Express your final answer as a pure number (fold coverage, dimensionless), and round to four significant figures.", "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n-   Haploid effective genome size: $G = 3.10 \\times 10^{9}$ base pairs\n-   Total number of initial paired-end read pairs: $N = 1.2 \\times 10^{9}$\n-   Length of each read: $L = 150$ base pairs\n-   Bases contributed per read pair: $2L$\n-   Fraction of read pairs discarded for low quality: $f_{qc} = 0.15$\n-   Fraction of *remaining* read pairs removed as PCR duplicates: $f_{dup} = 0.20$\n-   Assumption: All retained read pairs map uniquely and contribute their full length.\n-   Objective: Compute the effective average sequencing depth (fold coverage), rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n-   **Scientifically Grounded**: The problem is fundamentally sound. It describes a standard and critical calculation in genomics and bioinformatics, specifically in the quality assessment of next-generation sequencing (NGS) data. The concepts of sequencing depth, read length, paired-end reads, quality filtering, and PCR duplicate removal are all core to the field. The provided values for human genome size, read count, and read length are realistic for a modern whole-genome sequencing experiment.\n-   **Well-Posed**: The problem is well-posed. It provides all necessary numerical data and defines a clear, sequential process for data filtering. The objective is explicit and unambiguous, leading to a unique, computable solution.\n-   **Objective**: The problem is stated in precise, objective, and technical language. It is free from subjectivity, ambiguity, or opinion.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a scientifically coherent, well-posed, and objective exercise in quantitative genomics. The solution process may proceed.\n\nThe effective average sequencing depth, or fold coverage, denoted by $C$, is defined as the total number of sequenced bases that align to the reference genome divided by the size of that reference genome. The unit of coverage is 'X' or 'fold', which is dimensionless.\n\n$$C = \\frac{\\text{Total number of effective mapped bases}}{\\text{Effective genome size}}$$\n\nThe givens are:\n-   Initial number of read pairs, $N = 1.2 \\times 10^{9}$.\n-   Length of a single read, $L = 150$ base pairs. Each pair thus contributes a total length of $2L$.\n-   Haploid genome size, $G = 3.10 \\times 10^{9}$ base pairs.\n-   Fraction of read pairs lost to quality control, $f_{qc} = 0.15$.\n-   Fraction of remaining read pairs lost to PCR duplicate removal, $f_{dup} = 0.20$.\n\nWe must first calculate the number of read pairs that are successfully retained and mapped to the genome.\n\n1.  Start with the initial number of read pairs, $N = 1.2 \\times 10^{9}$.\n\n2.  A fraction $f_{qc} = 0.15$ of these pairs is discarded. The fraction that remains is $(1 - f_{qc})$. The number of read pairs after quality control, $N_{qc}$, is:\n    $$N_{qc} = N \\times (1 - f_{qc})$$\n\n3.  Of these remaining $N_{qc}$ pairs, a further fraction $f_{dup} = 0.20$ is removed as PCR duplicates. The fraction retained is $(1 - f_{dup})$. The final number of retained read pairs, $N_{retained}$, is:\n    $$N_{retained} = N_{qc} \\times (1 - f_{dup}) = N \\times (1 - f_{qc}) \\times (1 - f_{dup})$$\n\n4.  The problem states that each retained read pair contributes its full length of $2L$ bases to the effective mapped data. Therefore, the total number of effective mapped bases is:\n    $$\\text{Total Mapped Bases} = N_{retained} \\times 2L = N \\times (1 - f_{qc}) \\times (1 - f_{dup}) \\times 2L$$\n\n5.  Now, we can compute the average sequencing depth $C$ by dividing the total mapped bases by the genome size $G$:\n    $$C = \\frac{N \\times (1 - f_{qc}) \\times (1 - f_{dup}) \\times 2L}{G}$$\n\nSubstitute the given values into this expression:\n-   $N = 1.2 \\times 10^{9}$\n-   $G = 3.10 \\times 10^{9}$\n-   $L = 150$\n-   $f_{qc} = 0.15$\n-   $f_{dup} = 0.20$\n\nThe calculation proceeds as follows:\n$$C = \\frac{(1.2 \\times 10^{9}) \\times (1 - 0.15) \\times (1 - 0.20) \\times (2 \\times 150)}{3.10 \\times 10^{9}}$$\n\nFirst, evaluate the terms in the numerator:\n-   The retention factors are $(1 - 0.15) = 0.85$ and $(1 - 0.20) = 0.80$.\n-   The length per pair is $2L = 2 \\times 150 = 300$ bases.\n\nSubstitute these into the equation for $C$:\n$$C = \\frac{(1.2 \\times 10^{9}) \\times 0.85 \\times 0.80 \\times 300}{3.10 \\times 10^{9}}$$\n\nThe factor of $10^{9}$ in the numerator and denominator cancels out:\n$$C = \\frac{1.2 \\times 0.85 \\times 0.80 \\times 300}{3.10}$$\n\nNow, compute the product in the numerator:\n-   $1.2 \\times 0.85 = 1.02$\n-   $1.02 \\times 0.80 = 0.816$\n-   $0.816 \\times 300 = 244.8$\n\nSo, the expression for $C$ simplifies to:\n$$C = \\frac{244.8}{3.10}$$\n\nPerforming the final division:\n$$C \\approx 78.9677419...$$\n\nThe problem requires the final answer to be rounded to four significant figures. The first four significant figures are $7$, $8$, $9$, and $6$. The fifth digit is $7$, which is $\\ge 5$, so we round up the fourth digit.\n$$C \\approx 78.97$$\n\nThe effective average sequencing depth is $78.97$X.", "answer": "$$\\boxed{78.97}$$", "id": "4391383"}, {"introduction": "One of the great achievements of the Human Genome Project was the assembly of a highly contiguous reference sequence. The quality of any genome assembly is measured by its continuity, and the N50 metric is the industry standard for this assessment. This practice will guide you through the calculation of contig N50 and, just as importantly, challenge you to explain how modern scaffolding techniques can dramatically improve assembly continuity without altering the underlying contig sequences [@problem_id:4391378]. This distinction is fundamental to understanding how reference genomes are constructed and improved.", "problem": "A central legacy of the Human Genome Project (HGP) was the standardization of quantitative assembly metrics that enable reproducible comparison of genome builds used in precision medicine and genomic diagnostics. Consider a draft genome assembly with contig lengths (each contig is a contiguous deoxyribonucleic acid (DNA) sequence with no gaps) of $1.2$ Mb, $0.9$ Mb, $0.7$ Mb, $0.4$ Mb, and $0.3$ Mb. Using the standard definition of contig N50 adopted in genome assembly evaluation, compute the contig N50 for this dataset. Then, explain why ordering and orienting these contigs into scaffolds using genome-wide long-range maps such as optical maps or High-throughput Chromosome Conformation Capture (Hi-C) can increase the scaffold N50 while not altering the contig continuity.\n\nExpress the final numerical answer for the contig N50 in megabases (Mb) and round your answer to three significant figures. Provide your reasoning from first principles, relying on core definitions and widely accepted facts about genome assembly metrics and long-range scaffolding technologies.", "solution": "The problem is valid as it is scientifically grounded in the field of genomics, well-posed with sufficient data, and objective in its formulation. It requests the calculation of a standard genome assembly metric and an explanation based on established principles of genome scaffolding.\n\nThe problem requires the calculation of the contig N50 for a given set of contig lengths and an explanation of how scaffolding affects assembly metrics. We will address these two parts sequentially.\n\nFirst, we calculate the contig N50. The N50 metric is a widely used measure of genome assembly continuity. It is defined as the minimum contig length such that the sum of lengths of all contigs of that length or longer is at least $50\\%$ of the total assembly size.\n\nThe given contig lengths are $L_1 = 1.2$ Mb, $L_2 = 0.9$ Mb, $L_3 = 0.7$ Mb, $L_4 = 0.4$ Mb, and $L_5 = 0.3$ Mb.\n\nStep 1: Calculate the total assembly size, denoted by $G$.\nThe total assembly size is the sum of the lengths of all contigs:\n$$G = \\sum_{i=1}^{5} L_i = 1.2 + 0.9 + 0.7 + 0.4 + 0.3 = 3.5 \\text{ Mb}$$\n\nStep 2: Determine the $50\\%$ threshold of the total assembly size.\nThis threshold value, which we can call $G_{50}$, is:\n$$G_{50} = 0.50 \\times G = 0.5 \\times 3.5 \\text{ Mb} = 1.75 \\text{ Mb}$$\n\nStep 3: Order the contigs by length in descending order.\nThe provided list is already in descending order:\n$1.2$ Mb, $0.9$ Mb, $0.7$ Mb, $0.4$ Mb, $0.3$ Mb.\n\nStep 4: Calculate the cumulative sum of contig lengths, starting from the longest contig, until the sum is greater than or equal to $G_{50}$.\n- The length of the longest contig is $1.2$ Mb. The cumulative sum is $1.2$ Mb. This is less than $1.75$ Mb.\n- We add the length of the next longest contig, $0.9$ Mb. The new cumulative sum is $1.2 \\text{ Mb} + 0.9 \\text{ Mb} = 2.1$ Mb.\n\nThis cumulative sum of $2.1$ Mb is greater than the target threshold of $1.75$ Mb. The N50 value is the length of the last contig added to the sum to make it exceed the threshold. In this case, that contig has a length of $0.9$ Mb. The problem requests the answer to be rounded to three significant figures, so $0.9$ Mb is expressed as $0.900$ Mb.\n\nTherefore, the contig N50 of this assembly is $0.900$ Mb.\n\nSecond, we explain why scaffolding increases the scaffold N50 while not altering contig continuity.\nA **contig** (from \"contiguous\") is a continuous stretch of DNA sequence assembled from reads, with no gaps of unknown sequence. The lengths of contigs are known precisely.\nA **scaffold** is a higher-order structure created by ordering and orienting multiple contigs along a chromosome. The gaps between contigs within a scaffold represent regions of unknown sequence but have an estimated size. These gaps are typically represented by a specific number of 'N' characters in the sequence file.\n\nThe process of **scaffolding** utilizes long-range information from technologies such as optical mapping or Hi-C to determine the relative positions and orientations of contigs that are physically close in the genome. For example, if Hi-C data indicates that contig A and contig B are neighbors, they can be joined into a single scaffold (e.g., A-NNN...NNN-B).\n\nThis process does not alter the underlying contigs. The sequences of contig A and contig B remain unchanged; they are simply placed into a larger context. Consequently, the set of all contigs and their respective lengths remains identical to the pre-scaffolding set. Since the **contig N50** is calculated based solely on this set of contig lengths, it remains unchanged by the scaffolding process. The \"contig continuity\" is not altered because no new sequence is added to the contigs themselves, nor are they broken or merged.\n\nHowever, scaffolding generates a new set of sequences for analysis: the scaffolds. Some of these scaffolds may be composed of multiple contigs linked together, making them much longer than any individual source contig. The remaining un-scaffolded contigs are treated as scaffolds of length equal to their contig length.\n\nTo calculate the **scaffold N50**, one uses the lengths of these new scaffolds. For example, if the $1.2$ Mb and $0.7$ Mb contigs from our dataset were linked, a new scaffold with a length of at least $1.2 + 0.7 = 1.9$ Mb (plus the estimated gap size) would be created. The new set of sequence lengths for the N50 calculation would be (for example) {$1.9$ Mb, $0.9$ Mb, $0.4$ Mb, $0.3$ Mb}. The total assembly size $G$ remains the same ($3.5$ Mb), as does the $50\\%$ threshold $G_{50}$ ($1.75$ Mb). The longest sequence is now $1.9$ Mb, which is already greater than $1.75$ Mb. Thus, the scaffold N50 would be the length of this new, long scaffold ($1.9$ Mb in this hypothetical case), which is substantially larger than the original contig N50 of $0.900$ Mb.\n\nIn summary, scaffolding increases the N50 value by linking contigs into longer scaffolds, thereby improving the long-range continuity of the assembly (measured by scaffold N50), without changing the fundamental contig set or their internal sequences (and thus not changing the contig N50).", "answer": "$$\n\\boxed{0.900}\n$$", "id": "4391378"}, {"introduction": "With sufficient data and a high-quality reference genome, the next step in genomic diagnostics is to identify genetic variants. A crucial legacy of the HGP is the creation of benchmark datasets that allow us to rigorously evaluate the performance of variant calling algorithms. This exercise places you in the role of a bioinformatician validating a new tool, applying the foundational metrics of sensitivity, specificity, and the $F_1$ score to quantify its accuracy against a \"truth set\" [@problem_id:4391334]. Mastering these concepts is essential for interpreting the reliability of any genomic test.", "problem": "A central legacy of the Human Genome Project (HGP) is the establishment of standardized reference resources and high-confidence benchmarking datasets (for example, Genome in a Bottle (GIAB)) that enable rigorous evaluation of variant calling performance for precision medicine and genomic diagnostics. Consider an evaluation of a single-nucleotide polymorphism (SNP) caller against a GIAB-like truth resource within a defined callable region. The truth resource designates $50{,}000$ curated SNP-positive sites and $5{,}000{,}000$ curated SNP-negative sites in the same callable region. A single run of the caller produces the following adjudicated outcomes against this truth resource: True Positives (TP) $=49{,}000$, False Positives (FP) $=1{,}000$, and False Negatives (FN) $=1{,}000$. Assume that all other sites in the curated SNP-negative set that were not called as SNPs by the caller are True Negatives (TN), and that there are no unresolved or uncallable sites within the benchmarking region.\n\nStarting only from the fundamental definitions of diagnostic test performance based on the confusion matrix for binary classification as applied to variant calling, derive and compute the sensitivity, specificity, and the $F_{1}$ score for this caller on this dataset. Express all three quantities as decimals in the interval $[0,1]$. Round your final numerical values to four significant figures. Provide your final answer as a single row matrix containing, in order, the sensitivity, the specificity, and the $F_{1}$ score.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It provides a complete and consistent set of data for evaluating a binary classifier—in this case, a genomic variant caller—using standard performance metrics. The context is directly relevant to the field of genomic diagnostics and precision medicine, a key legacy of the Human Genome Project. We will proceed to derive and compute the requested metrics.\n\nThe problem provides the following data from the comparison of a single-nucleotide polymorphism (SNP) caller against a high-confidence truth resource:\n\n1.  The number of curated SNP-positive sites (Condition Positive, $P$) is $P = 50,000$.\n2.  The number of curated SNP-negative sites (Condition Negative, $N$) is $N = 5,000,000$.\n3.  The number of True Positives ($TP$), where the caller correctly identifies a SNP, is $TP = 49,000$.\n4.  The number of False Positives ($FP$), where the caller incorrectly identifies a SNP at a negative site, is $FP = 1,000$.\n5.  The number of False Negatives ($FN$), where the caller fails to identify a true SNP, is $FN = 1,000$.\n\nFrom these givens, we must first determine the number of True Negatives ($TN$), which are the curated SNP-negative sites that the caller correctly identified as negative. The problem states that all sites in the SNP-negative set not called as SNPs are True Negatives. The total number of SNP-negative sites is $N$, and the number of these sites incorrectly called as positive is $FP$. Therefore, the number of True Negatives is given by:\n$$\nTN = N - FP\n$$\nSubstituting the provided values:\n$$\nTN = 5,000,000 - 1,000 = 4,999,000\n$$\nWe can verify the consistency of the provided numbers. The total number of condition positive sites must equal the sum of true positives and false negatives:\n$$\nP = TP + FN = 49,000 + 1,000 = 50,000\n$$\nThis matches the given value for $P$. The total number of condition negative sites must equal the sum of false positives and true negatives:\n$$\nN = FP + TN = 1,000 + 4,999,000 = 5,000,000\n$$\nThis matches the given value for $N$. The data are internally consistent.\n\nNow, we derive and compute the three required performance metrics.\n\n**1. Sensitivity (Recall or True Positive Rate)**\n\nSensitivity measures the proportion of actual positive instances that are correctly identified by the classifier. It is defined as the ratio of true positives to the total number of condition positive instances ($P = TP + FN$).\n\nThe fundamental definition is:\n$$\n\\text{Sensitivity} = \\frac{TP}{P} = \\frac{TP}{TP + FN}\n$$\nSubstituting the given values:\n$$\n\\text{Sensitivity} = \\frac{49,000}{49,000 + 1,000} = \\frac{49,000}{50,000} = 0.98\n$$\nRounded to four significant figures, the sensitivity is $0.9800$.\n\n**2. Specificity (True Negative Rate)**\n\nSpecificity measures the proportion of actual negative instances that are correctly identified by the classifier. It is defined as the ratio of true negatives to the total number of condition negative instances ($N = FP + TN$).\n\nThe fundamental definition is:\n$$\n\\text{Specificity} = \\frac{TN}{N} = \\frac{TN}{FP + TN}\n$$\nUsing our calculated value for $TN$:\n$$\n\\text{Specificity} = \\frac{4,999,000}{1,000 + 4,999,000} = \\frac{4,999,000}{5,000,000} = 0.9998\n$$\nThis value is already expressed to four significant figures.\n\n**3. $F_1$ Score**\n\nThe $F_1$ score is the harmonic mean of Precision and Recall (Sensitivity). It provides a single metric that balances the concerns of both precision and recall.\n\nFirst, let us define Precision, also known as Positive Predictive Value (PPV). Precision measures the proportion of predicted positive instances that are actually correct. It is defined as:\n$$\n\\text{Precision} = \\frac{TP}{\\text{Predicted Positive}} = \\frac{TP}{TP + FP}\n$$\nFor this dataset, the precision is:\n$$\n\\text{Precision} = \\frac{49,000}{49,000 + 1,000} = \\frac{49,000}{50,000} = 0.98\n$$\nRecall is another term for Sensitivity, which we have already calculated as $0.98$.\n\nThe $F_1$ score is defined as:\n$$\nF_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n$$\nA more direct formula, derived from the above and expressed in terms of the fundamental counts ($TP, FP, FN$), is:\n$$\nF_1 = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}\n$$\nUsing this direct formula with the given values:\n$$\nF_1 = \\frac{2 \\cdot 49,000}{2 \\cdot 49,000 + 1,000 + 1,000} = \\frac{98,000}{98,000 + 2,000} = \\frac{98,000}{100,000} = 0.98\n$$\nRounded to four significant figures, the $F_1$ score is $0.9800$.\n\nIn summary, the computed performance metrics for the SNP caller, rounded to four significant figures, are:\n- Sensitivity: $0.9800$\n- Specificity: $0.9998$\n- $F_1$ Score: $0.9800$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.9800 & 0.9998 & 0.9800\n\\end{pmatrix}\n}\n$$", "id": "4391334"}]}