{"hands_on_practices": [{"introduction": "A cornerstone of quality control in population genetics is testing for Hardy-Weinberg Equilibrium (HWE). Deviations from HWE within a homogeneous population group can signal systematic genotyping errors or other technical artifacts. This practice will guide you through implementing the Pearson's chi-squared test for HWE in stratified populations, a fundamental skill for anyone analyzing large-scale genotype data from biobanks [@problem_id:4370919].", "problem": "You are given a set of population groups, each representing an ancestry stratum within a population-scale biobank for a single bi-allelic locus. For each group, you are provided the genotype counts for the alternate allele coded as $0$, $1$, or $2$ copies, respectively. Denote the observed counts by $\\left(n_{0}, n_{1}, n_{2}\\right)$, where $n_{0}$ is the number of individuals with $0$ alternate alleles, $n_{1}$ is the number with $1$ alternate allele, and $n_{2}$ is the number with $2$ alternate alleles. Let $N = n_{0} + n_{1} + n_{2}$ be the total number of non-missing genotypes in the group. You must compute ancestry-specific allele frequencies of the alternate allele and test Hardy-Weinberg equilibrium (HWE) within each group to detect stratified deviations and potential genotype calling issues.\n\nUse the following fundamental base:\n- Under random mating with no selection, migration, or mutation at this locus, the Hardy-Weinberg law implies that if the alternate-allele frequency is $p$ and the reference-allele frequency is $q = 1 - p$, the genotype frequencies are $p^{2}$, $2 p q$, and $q^{2}$ for $2$, $1$, and $0$ alternate alleles, respectively.\n- The maximum likelihood estimator of the alternate-allele frequency within a group is $\\hat{p} = \\dfrac{n_{1} + 2 n_{2}}{2 N}$.\n- A standard lack-of-fit test for HWE uses the Pearson chi-squared statistic comparing observed genotype counts to expected counts $N q^{2}$, $2 N p q$, and $N p^{2}$. Under the null hypothesis of HWE and large-sample regularity, the test statistic asymptotically follows a chi-squared distribution with $\\nu = 1$ degree of freedom.\n\nYour task is to implement a program that:\n- For each group, computes the alternate-allele frequency $\\hat{p}$ and the HWE $p$-value using the Pearson chi-squared test with $\\nu = 1$ degree of freedom.\n- Defines a decision threshold $\\alpha = 10^{-3}$ for significance.\n- Flags two conditions per test case:\n  1. any\\_deviation: true if at least one group in the test case shows a $p$-value strictly less than $\\alpha$.\n  2. stratified\\_deviation: true if there exists at least one group with $p$-value strictly less than $\\alpha$ and at least one other group with $p$-value greater than or equal to $\\alpha$. If fewer than two groups have $N  0$, set stratified\\_deviation to false.\n- Handle edge cases in a scientifically realistic manner:\n  - If $N = 0$ for a group, define $\\hat{p} = 0$ and the HWE $p$-value as $1.0$.\n  - When computing the chi-squared statistic, include only terms with strictly positive expected counts; if an expected count is $0$, skip that term. This is coherent with the derivation from likelihood asymptotics when a cell has zero expected probability under the fitted model.\n\nThe test suite consists of the following four test cases, each composed of a list of ancestry groups, where each group is given by the triple $\\left[n_{0}, n_{1}, n_{2}\\right]$:\n- Test case $1$: two groups\n  - Group A: $\\left[490, 420, 90\\right]$\n  - Group B: $\\left[560, 280, 160\\right]$\n- Test case $2$: two groups\n  - Group A: $\\left[500, 0, 0\\right]$\n  - Group B: $\\left[405, 90, 5\\right]$\n- Test case $3$: two groups\n  - Group A: $\\left[27, 3, 0\\right]$\n  - Group B: $\\left[16, 8, 1\\right]$\n- Test case $4$: two groups\n  - Group A: $\\left[0, 0, 0\\right]$\n  - Group B: $\\left[100, 600, 100\\right]$\n\nFor each test case, your program must produce a single list containing, in order:\n- The list of group-wise alternate-allele frequencies $\\left[\\hat{p}_{1}, \\hat{p}_{2}, \\dots \\right]$ as floating-point numbers.\n- The list of group-wise HWE $p$-values $\\left[p\\text{-value}_{1}, p\\text{-value}_{2}, \\dots \\right]$ as floating-point numbers.\n- The boolean any\\_deviation flag.\n- The boolean stratified\\_deviation flag.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case in the exact order given above. Each element must itself be a list structured as described. For example, the outermost structure must look like $\\left[\\text{test\\_result}_{1}, \\text{test\\_result}_{2}, \\text{test\\_result}_{3}, \\text{test\\_result}_{4}\\right]$ with each $\\text{test\\_result}_{i}$ of the stated form. There are no physical units or angle units involved. All proportions and probabilities must be represented as decimals, not percentages. No input is provided to the program; it must use the embedded test suite.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of population genetics, specifically the Hardy-Weinberg equilibrium (HWE) law. The problem is well-posed, with clear definitions, data, and objectives. It is objective, self-contained, and formalizable into a computational algorithm. The provided constants and formulas, such as the maximum likelihood estimator for allele frequency and the use of a Pearson chi-squared test with $\\nu=1$ degree of freedom, are standard and correct for this type of analysis. The edge cases are handled explicitly and appropriately.\n\nThe solution proceeds by implementing an algorithm to analyze genotype counts for multiple population groups across several test cases. For each group, we compute the alternate allele frequency and test for deviation from HWE. These results are then aggregated to derive test-case-level flags for statistical significance.\n\n### Algorithm for a Single Population Group\n\nFor each group, defined by observed genotype counts $(n_0, n_1, n_2)$ for $0$, $1$, and $2$ copies of the alternate allele, we perform the following steps:\n\n1.  **Calculate Sample Size**: The total number of individuals with non-missing genotypes is $N = n_0 + n_1 + n_2$.\n\n2.  **Handle Zero Sample Size**: As specified, if $N=0$, the group provides no information. The alternate allele frequency $\\hat{p}$ is defined as $0$ and the HWE $p$-value is defined as $1.0$.\n\n3.  **Calculate Allele Frequency**: If $N  0$, the maximum likelihood estimate (MLE) of the alternate allele frequency, $\\hat{p}$, is calculated by the allele counting method:\n    $$ \\hat{p} = \\frac{n_1 + 2n_2}{2N} $$\n    The frequency of the reference allele is then $\\hat{q} = 1 - \\hat{p}$.\n\n4.  **Calculate Expected Genotype Counts**: Under the null hypothesis of HWE, the expected genotype counts are derived from the estimated allele frequencies:\n    -   Expected count of $0$ alternate alleles: $E_0 = N \\hat{q}^2$\n    -   Expected count of $1$ alternate allele: $E_1 = 2N\\hat{p}\\hat{q}$\n    -   Expected count of $2$ alternate alleles: $E_2 = N \\hat{p}^2$\n\n5.  **Compute Pearson's Chi-Squared Statistic**: A lack-of-fit test is performed using the Pearson's $\\chi^2$ statistic, which compares the observed counts $(O_0, O_1, O_2) = (n_0, n_1, n_2)$ to the expected counts $(E_0, E_1, E_2)$:\n    $$ \\chi^2 = \\sum_{i=0}^{2} \\frac{(O_i - E_i)^2}{E_i} $$\n    Per the problem specification, terms are included in the sum only if their corresponding expected count $E_i$ is strictly positive. If $\\hat{p}=0$ or $\\hat{p}=1$, the sample is monomorphic, the observed counts perfectly match the expected counts (e.g., $(N, 0, 0)$ observed vs. $(N, 0, 0)$ expected), the $\\chi^2$ statistic is $0$, and the data are in perfect HWE.\n\n6.  **Calculate the p-value**: The $\\chi^2$ statistic, under the null hypothesis, asymptotically follows a chi-squared distribution with $\\nu = 3 - 1 - 1 = 1$ degree of freedom (3 genotype categories minus 1 for the total count constraint, minus 1 for the estimated parameter $\\hat{p}$). The $p$-value is the probability of observing a test statistic at least as extreme as the one calculated, given that the null hypothesis is true. This corresponds to the survival function (1 - CDF) of the $\\chi^2(\\nu=1)$ distribution evaluated at the computed statistic.\n\n### Algorithm for an Entire Test Case\n\nFor each test case, which consists of one or more population groups, the following steps are performed after analyzing each group individually:\n\n1.  **Collect Group-wise Results**: The computed alternate allele frequencies $[\\hat{p}_1, \\hat{p}_2, \\dots]$ and HWE $p$-values $[p\\text{-value}_1, p\\text{-value}_2, \\dots]$ are collected into separate lists.\n\n2.  **Define Significance Threshold**: The significance level for the HWE test is set at $\\alpha = 10^{-3}$.\n\n3.  **Determine `any_deviation` Flag**: This boolean flag is set to `true` if any of the computed group-wise $p$-values are strictly less than $\\alpha$. Otherwise, it is set to `false`.\n    $$ \\text{any\\_deviation} = \\exists i : p\\text{-value}_i  \\alpha $$\n\n4.  **Determine `stratified_deviation` Flag**: This flag indicates whether HWE deviation is present in some, but not all, population strata. Its logic is defined by two conditions:\n    a. First, we count the number of groups with a non-zero sample size ($N  0$). If this count is less than $2$, `stratified_deviation` is set to `false`, as stratification cannot be assessed with fewer than two informative groups.\n    b. If there are two or more informative groups, the flag is set to `true` if and only if there is at least one group with a $p$-value strictly less than $\\alpha$ AND at least one other group with a $p$-value greater than or equal to $\\alpha$.\n\n### Implementation\nThe algorithm is implemented in Python, utilizing the `numpy` library for numerical operations and `scipy.stats.chi2` for calculating the $p$-value from the chi-squared distribution. The final output is a single string representing a list of results, with one result element for each test case, formatted precisely as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Computes ancestry-specific allele frequencies and tests for Hardy-Weinberg\n    Equilibrium (HWE) for a set of population groups.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        [[490, 420, 90], [560, 280, 160]],\n        # Test case 2\n        [[500, 0, 0], [405, 90, 5]],\n        # Test case 3\n        [[27, 3, 0], [16, 8, 1]],\n        # Test case 4\n        [[0, 0, 0], [100, 600, 100]],\n    ]\n\n    alpha = 1e-3\n    all_results = []\n\n    for case in test_cases:\n        p_hats = []\n        p_values = []\n        groups_with_N_gt_0 = 0\n\n        for group_counts in case:\n            n0, n1, n2 = group_counts\n            N = n0 + n1 + n2\n\n            if N == 0:\n                p_hat = 0.0\n                p_val = 1.0\n            else:\n                groups_with_N_gt_0 += 1\n                p_hat = (n1 + 2 * n2) / (2 * N)\n                \n                # For monomorphic populations (p_hat=0 or p_hat=1), there is no\n                # deviation from HWE. The chi-squared statistic is 0.\n                if p_hat == 0.0 or p_hat == 1.0:\n                    chi2_stat = 0.0\n                else:\n                    q_hat = 1.0 - p_hat\n                    \n                    # Expected counts under HWE\n                    E = np.array([N * q_hat**2, N * 2 * p_hat * q_hat, N * p_hat**2])\n                    O = np.array([n0, n1, n2], dtype=float)\n                    \n                    # Pearson's chi-squared statistic\n                    chi2_stat = 0.0\n                    if E[0]  0:\n                        chi2_stat += (O[0] - E[0])**2 / E[0]\n                    if E[1]  0:\n                        chi2_stat += (O[1] - E[1])**2 / E[1]\n                    if E[2]  0:\n                        chi2_stat += (O[2] - E[2])**2 / E[2]\n                \n                # p-value from chi-squared distribution with 1 degree of freedom\n                p_val = chi2.sf(chi2_stat, df=1)\n\n            p_hats.append(p_hat)\n            p_values.append(p_val)\n\n        # Flag for any deviation from HWE\n        any_dev = any(p  alpha for p in p_values)\n        \n        # Flag for stratified deviation\n        strat_dev = False\n        if groups_with_N_gt_0 = 2:\n            has_low_p = any(p  alpha for p in p_values)\n            has_high_p = any(p = alpha for p in p_values)\n            if has_low_p and has_high_p:\n               strat_dev = True\n        \n        # Assemble the final result for the test case\n        case_result = [p_hats, p_values, any_dev, strat_dev]\n        all_results.append(case_result)\n\n    # Custom string formatting to match problem requirements\n    def custom_format(obj):\n        if isinstance(obj, bool):\n            return str(obj).lower()\n        if isinstance(obj, list):\n            return f\"[{','.join(map(custom_format, obj))}]\"\n        if isinstance(obj, float):\n            return f\"{obj:.17g}\" # High precision for scientific values\n        return str(obj)\n\n    results_str_list = [custom_format(res) for res in all_results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n\n```", "id": "4370919"}, {"introduction": "Beyond variant-level quality, ensuring the integrity of each individual sample is paramount in sequencing initiatives. This exercise addresses a common issue: sample contamination, where a sample contains DNA from more than one individual. You will develop a robust statistical model to estimate the contamination fraction $c$ from pooled allele counts at homozygous sites and apply a quality control threshold, a crucial step in maintaining the high fidelity of a biobank's genomic dataset [@problem_id:4370879].", "problem": "A national-scale population biobank sequences many participants and monitors sample contamination in high-throughput pipelines by analyzing allele balance at sites that are homozygous in the index sample. Consider a curated panel of biallelic Single Nucleotide Polymorphisms (SNPs) selected to have high expected heterozygosity in the population. For a given sample, across this panel, define the pooled minor-allele read count $X$ and the pooled total read count $D$ over all sites that are homozygous for the index sample (so any observed minor allele reads arise from sequencing error or contamination). Assume the following fundamental base:\n\n- Each read is an independent Bernoulli trial, reflecting standard random sampling of molecules in short-read sequencing, with a per-read minor-allele probability that depends on whether the read originates from the index sample or a contaminant.\n- Let the contamination fraction be $c \\in [0,1]$, meaning the probability that any given read derives from a contaminant genome is $c$, and from the index genome is $1 - c$.\n- Let the per-base sequencing error rate be $\\epsilon \\in (0, 0.5)$, assumed known from calibration on control data.\n- Let $h \\in [0,1]$ denote the heterozygosity probability at the chosen SNP panel for a randomly drawn contaminant genome in the biobank population, assumed known from population-scale allele frequency data. At a heterozygous contaminant genotype, the expected minor-allele probability per read is $0.5$; at a homozygous contaminant genotype at these sites, minor-allele reads arise only from error at rate $\\epsilon$.\n\nUnder these assumptions, the per-read minor-allele probability is a two-component mixture at the read level: with probability $1-c$ a read comes from the index genome and contributes a minor allele with probability $\\epsilon$, and with probability $c$ a read comes from the contaminant and contributes a minor allele according to the population heterozygosity structure of the panel. By independence of reads, the pooled minor-allele count $X$ over $D$ reads satisfies a Binomial law with a success parameter that is an affine function of $c$. Using only these base assumptions, derive from first principles:\n\n1) The pooled likelihood for $c$ given $X$ and $D$ under the Binomial model implied by the two-component read-level mixture, and the Maximum Likelihood Estimate (MLE) of $c$ expressed in terms of the observed pooled minor-allele fraction $X/D$, $\\epsilon$, and $h$. The estimate must be clipped to the interval $[0,1]$.\n\n2) A two-sided equal-tailed $100(1-\\alpha)\\%$ confidence interval for the pooled Binomial minor-allele probability parameter, using the exact Clopper–Pearson method, and its transformation into a confidence interval for $c$ by inverting the affine mapping. Clip the transformed bounds to $[0,1]$.\n\n3) An exclusion rule suitable for population-scale biobank quality control: exclude a sample if and only if the point estimate of contamination $c$ is greater than or equal to a specified threshold $\\tau$ (express all contamination values as decimal fractions, not percentages).\n\nImplement a program that, for a fixed panel and quality-control configuration, computes for each test sample:\n- the contamination point estimate $\\hat{c}$,\n- the lower and upper bounds of the two-sided Clopper–Pearson $100(1-\\alpha)\\%$ confidence interval for $c$,\n- the exclusion decision as a boolean under the rule above.\n\nUse the following fixed parameters, justified by a high-quality short-read pipeline and a high-heterozygosity SNP panel curated from the biobank:\n- sequencing error rate $\\epsilon = 0.002$,\n- panel heterozygosity probability $h = 0.5$,\n- confidence level $1-\\alpha = 0.95$ (that is, $\\alpha = 0.05$),\n- exclusion threshold $\\tau = 0.03$.\n\nTest suite. For each test case, you are given $(D, X)$ as pooled totals over all homozygous sites for the index sample:\n- Case A (typical low contamination): $D = 150000$, $X = 487$.\n- Case B (near the exclusion threshold): $D = 120000$, $X = 1136$.\n- Case C (high contamination): $D = 100000$, $X = 2192$.\n- Case D (no contamination with sampling fluctuation): $D = 180000$, $X = 355$.\n- Case E (borderline at lower coverage): $D = 5000$, $X = 45$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case in the order A through E and is itself a list of four elements: $[\\hat{c}, c_{\\text{low}}, c_{\\text{high}}, \\text{exclude}]$. The three contamination values must be rounded to exactly $6$ decimal places, and the boolean must be a language-native boolean. For example, an output for two hypothetical cases would look like $[[0.012345,0.010000,0.015000,False],[0.045678,0.043000,0.048500,True]]$.", "solution": "The problem is deemed valid as it is scientifically grounded in standard models of sequencing data, is well-posed with all necessary information, and is expressed objectively. We proceed to the solution.\n\nThe solution is derived in three parts, following the problem statement. First, we derive the Maximum Likelihood Estimate (MLE) for the contamination fraction $c$. Second, we derive the confidence interval for $c$. Third, we specify the exclusion rule.\n\n### Part 1: Derivation of the Contamination Estimator ($\\hat{c}$)\n\nThe core of the model is to determine the probability, let's call it $p$, that a single read at a site homozygous in the index sample will show a minor allele. This probability $p$ is the success parameter of a Binomial distribution governing the pooled minor-allele count $X$ over a total of $D$ reads.\n\nThe probablity $p$ is a mixture, conditioned on the origin of the read (index sample or contaminant).\nLet $c$ be the contamination fraction. A read originates from the contaminant with probability $c$ and from the index sample with probability $1-c$.\nLet $\\epsilon$ be the per-base sequencing error rate.\nLet $h$ be the probability that a contaminant is heterozygous at a given panel SNP.\n\nThe probability of observing a minor allele in a single read, $p$, is given by the law of total probability:\n$$p(c) = P(\\text{minor allele} | \\text{read from index}) P(\\text{read from index}) + P(\\text{minor allele} | \\text{read from contaminant}) P(\\text{read from contaminant})$$\n\n1.  If the read is from the index sample (with probability $1-c$): The index sample is homozygous for the major allele at the selected sites. A minor allele can only be observed due to a sequencing error. Thus, $P(\\text{minor allele} | \\text{read from index}) = \\epsilon$.\n\n2.  If the read is from the contaminant (with probability $c$): The probability of observing a minor allele depends on the contaminant's genotype.\n    -   The contaminant is heterozygous (e.g., A/T at an A/A index site) with probability $h$. In this case, the probability of sampling the minor allele is $0.5$.\n    -   The contaminant is homozygous for the same major allele as the index (e.g., A/A at an A/A index site) with probability $1-h$. In this case, a minor allele is observed only due to sequencing error, with probability $\\epsilon$.\n    Therefore, the probability of a minor allele, given the read is from the contaminant, is:\n    $$P(\\text{minor allele} | \\text{read from contaminant}) = h \\cdot 0.5 + (1-h) \\cdot \\epsilon$$\n\nCombining these terms, the overall minor-allele probability $p$ is:\n$$p(c) = \\epsilon (1-c) + \\left( 0.5h + \\epsilon(1-h) \\right) c$$\nExpanding and simplifying the expression:\n$$p(c) = \\epsilon - \\epsilon c + 0.5hc + \\epsilon c - h\\epsilon c$$\n$$p(c) = \\epsilon + c(0.5h - h\\epsilon)$$\n$$p(c) = \\epsilon + ch(0.5 - \\epsilon)$$\nThis confirms that $p$ is an affine function of $c$. Let the constant slope be $A = h(0.5 - \\epsilon)$. The relationship is $p(c) = \\epsilon + Ac$.\n\nThe number of minor-allele reads $X$ in a total of $D$ reads is modeled as a binomial random variable:\n$$X \\sim \\text{Binomial}(D, p(c))$$\nThe likelihood function for $c$ given the data $(X, D)$ is:\n$$L(c; X, D) = \\binom{D}{X} [p(c)]^X [1 - p(c)]^{D-X}$$\nTo find the Maximum Likelihood Estimate (MLE) of $c$, we maximize the log-likelihood $\\ell(c) = \\log L(c)$. The MLE for a binomial proportion $p$ is famously $\\hat{p} = X/D$. We can show this by differentiating the log-likelihood with respect to $p$ (and by extension, $c$):\n$$\\frac{d\\ell}{dc} = \\frac{d\\ell}{dp} \\frac{dp}{dc} = \\left( \\frac{X}{p} - \\frac{D-X}{1-p} \\right) A$$\nSetting the derivative to zero (and assuming $A \\neq 0$), we find that the term in the parenthesis must be zero, which gives $\\hat{p} = X/D$.\n\nSubstituting this into our affine relationship for $p(c)$:\n$$\\hat{p} = \\epsilon + \\hat{c}h(0.5 - \\epsilon)$$\nSolving for the MLE $\\hat{c}$:\n$$\\hat{c} = \\frac{\\hat{p} - \\epsilon}{h(0.5 - \\epsilon)} = \\frac{X/D - \\epsilon}{h(0.5 - \\epsilon)}$$\nSince $c$ is a fraction, its estimate must lie in the interval $[0,1]$. We enforce this by clipping the estimate:\n$$\\hat{c}_{\\text{clipped}} = \\max\\left(0, \\min\\left(1, \\frac{X/D - \\epsilon}{h(0.5 - \\epsilon)}\\right)\\right)$$\n\n### Part 2: Derivation of the Confidence Interval for $c$\n\nWe first compute a $100(1-\\alpha)\\%$ confidence interval for the binomial proportion $p$, denoted $[p_{\\text{low}}, p_{\\text{high}}]$, using the exact Clopper-Pearson method. This method defines the interval bounds by inverting the binomial test.\n-   The lower bound $p_{\\text{low}}$ is the value of $p$ such that the probability of observing $X$ or more successes is $\\alpha/2$:\n    $$P(Y \\ge X | Y \\sim \\text{Binomial}(D, p_{\\text{low}})) = \\sum_{k=X}^{D} \\binom{D}{k} p_{\\text{low}}^k (1-p_{\\text{low}})^{D-k} = \\frac{\\alpha}{2}$$\n-   The upper bound $p_{\\text{high}}$ is the value of $p$ such that the probability of observing $X$ or fewer successes is $\\alpha/2$:\n    $$P(Y \\le X | Y \\sim \\text{Binomial}(D, p_{\\text{high}})) = \\sum_{k=0}^{X} \\binom{D}{k} p_{\\text{high}}^k (1-p_{\\text{high}})^{D-k} = \\frac{\\alpha}{2}$$\n\nThese equations can be solved using the quantiles of the Beta distribution.\n-   $p_{\\text{low}} = B(\\frac{\\alpha}{2}; X, D-X+1)$, where $B(q; a, b)$ is the $q$-th quantile of the Beta distribution with shape parameters $a$ and $b$. If $X=0$, $p_{\\text{low}}=0$.\n-   $p_{\\text{high}} = B(1-\\frac{\\alpha}{2}; X+1, D-X)$. If $X=D$, $p_{\\text{high}}=1$.\n\nNext, we transform this interval for $p$ into a confidence interval for $c$. The relationship $c(p) = \\frac{p - \\epsilon}{h(0.5 - \\epsilon)}$ is monotonically increasing in $p$, because the denominator $h(0.5 - \\epsilon)$ is positive for the given parameters ($h=0.5  0$ and $\\epsilon=0.002  0.5$).\nTherefore, the confidence interval for $c$, $[c_{\\text{low}}, c_{\\text{high}}]$, is obtained by applying the transformation to the bounds of the interval for $p$:\n$$c_{\\text{low}} = \\frac{p_{\\text{low}} - \\epsilon}{h(0.5 - \\epsilon)}$$\n$$c_{\\text{high}} = \\frac{p_{\\text{high}} - \\epsilon}{h(0.5 - \\epsilon)}$$\nAs with the point estimate, these bounds are clipped to the valid range $[0, 1]$:\n$$c_{\\text{low, clipped}} = \\max(0, \\min(1, c_{\\text{low}}))$$\n$$c_{\\text{high, clipped}} = \\max(0, \\min(1, c_{\\text{high}}))$$\n\n### Part 3: Exclusion Rule\n\nThe quality control rule is to exclude a sample if its point estimate of contamination, $\\hat{c}$, is greater than or equal to a specified threshold $\\tau$.\n$$\\text{exclude} = (\\hat{c} \\ge \\tau)$$\n\nUsing the fixed parameters:\n-   $\\epsilon = 0.002$\n-   $h = 0.5$\n-   $1-\\alpha = 0.95 \\implies \\alpha = 0.05 \\implies \\alpha/2 = 0.025$\n-   $\\tau = 0.03$\n\nThe denominator for the transformations is $h(0.5 - \\epsilon) = 0.5(0.5 - 0.002) = 0.249$.\n\nThe derived formulas are implemented to process the test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta\n\ndef solve():\n    \"\"\"\n    Solves the contamination estimation problem for a series of test cases.\n    \"\"\"\n\n    # Fixed parameters from the problem statement\n    epsilon = 0.002  # sequencing error rate\n    h = 0.5          # panel heterozygosity probability\n    alpha = 0.05     # for 95% confidence interval\n    tau = 0.03       # exclusion threshold\n\n    # Test cases: (D, X)\n    # D: pooled total read count\n    # X: pooled minor-allele read count\n    test_cases = [\n        (150000, 487),   # Case A\n        (120000, 1136),  # Case B\n        (100000, 2192),  # Case C\n        (180000, 355),   # Case D\n        (5000, 45)       # Case E\n    ]\n\n    # Pre-calculate the constant denominator for the transformation\n    # c = (p - epsilon) / denom\n    denom = h * (0.5 - epsilon)\n\n    results_as_strings = []\n    for D, X in test_cases:\n        # 1. Calculate the Maximum Likelihood Estimate (MLE) of c\n        p_hat = X / D\n        c_hat_raw = (p_hat - epsilon) / denom\n        c_hat = max(0.0, min(1.0, c_hat_raw))\n\n        # 2. Calculate the Clopper-Pearson confidence interval for c\n\n        # Calculate CI for the binomial proportion p\n        if X == 0:\n            p_low = 0.0\n        else:\n            p_low = beta.ppf(alpha / 2, X, D - X + 1)\n        \n        if X == D:\n            p_high = 1.0\n        else:\n            p_high = beta.ppf(1 - alpha / 2, X + 1, D - X)\n\n        # Transform the CI for p to a CI for c\n        c_low_raw = (p_low - epsilon) / denom\n        c_high_raw = (p_high - epsilon) / denom\n        \n        # Clip the CI bounds to the [0, 1] interval\n        c_low = max(0.0, min(1.0, c_low_raw))\n        c_high = max(0.0, min(1.0, c_high_raw))\n\n        # 3. Apply the exclusion rule\n        exclude = c_hat = tau\n\n        # Format the results for this case\n        result_string = (\n            f\"[{c_hat:.6f},\"\n            f\"{c_low:.6f},\"\n            f\"{c_high:.6f},\"\n            f\"{str(exclude).lower()}]\"\n        )\n        results_as_strings.append(result_string)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```", "id": "4370879"}, {"introduction": "Large biobanks often include individuals from diverse ancestral backgrounds, and this population structure can confound genetic association studies if not properly accounted for. Principal Component Analysis (PCA) is a powerful, data-driven technique used to identify and visualize this structure from high-dimensional genotype data. In this practice, you will use singular value decomposition (SVD) to perform PCA, quantifying how genetic variation partitions across a simulated admixed cohort, a foundational analysis for any genome-wide study [@problem_id:4370918].", "problem": "A population-scale biobank collects genotype dosages from many individuals across many genomic variants. Consider an admixed cohort comprising multiple continental ancestry groups. The cohort is represented by a genotype matrix with samples as rows and variants as columns. Each entry is the count of the alternative allele per variant per sample and takes values in $\\{0,1,2\\}$. Principal component analysis (PCA) on standardized genotype data is commonly used to capture population structure, which can confound downstream precision medicine analyses if not accounted for. Your task is to compute principal components via singular value decomposition and quantify the fraction of total variance explained by the top components in several scenarios.\n\nUse the following fundamental base:\n- Genotype dosage $g_{ij} \\in \\{0,1,2\\}$ is the count of the alternative allele for sample $i$ and variant $j$.\n- The allele frequency for variant $j$ across all samples is $p_j = \\frac{1}{2n} \\sum_{i=1}^{n} g_{ij}$, where $n$ is the number of samples.\n- Under Hardy–Weinberg equilibrium, the expected variance of the genotype dosage for variant $j$ is $2 p_j (1 - p_j)$, which is widely used to standardize genotype columns for PCA.\n- Let the standardized data matrix be $X \\in \\mathbb{R}^{n \\times m}$ with entries $x_{ij} = \\frac{g_{ij} - 2 p_j}{\\sqrt{2 p_j (1 - p_j)}}$ for all variants with $0  p_j  1$. Variants with $p_j \\in \\{0,1\\}$ have zero variance and must be excluded from PCA.\n- Principal component analysis on $X$ can be obtained via the singular value decomposition $X = U \\Sigma V^\\top$, where $\\Sigma$ has nonnegative singular values $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq 0$.\n\nFrom these bases, derive how to compute the fraction of variance explained by each principal component from the singular values, and implement it algorithmically. You must:\n1. Construct synthetic genotype matrices according to specified parameters for an admixed cohort with continental ancestries. For each ancestry group $g$, generate each genotype $g_{ij}$ by drawing from a binomial distribution with $2$ trials and success probability equal to the group-specific allele frequency $p_{j}^{(g)}$, using a fixed pseudorandom seed per test case to ensure deterministic outputs.\n2. Standardize the genotype matrix using the Hardy–Weinberg scaling described above, removing monomorphic variants with $p_j \\in \\{0,1\\}$.\n3. Compute the singular value decomposition of the standardized matrix and compute the fractions of total variance explained by the top $k$ principal components. Express each fraction as a decimal rounded to four places (no percentage sign).\n\nTest Suite and Parameters:\nImplement the following three test cases. Each case is defined by group sizes, variant count, ancestry-informative marker (AIM) blocks with group-specific allele frequencies, a neutral allele frequency shared across groups for the remaining variants, a pseudorandom seed, and $k$ (the number of top components to report).\n\n- Case 1 (general admixed two-ancestry cohort):\n    - Number of groups: $2$ with group sizes $[100, 100]$.\n    - Number of variants: $500$.\n    - AIM blocks: one block with count $80$ and group-specific allele frequencies $[0.1, 0.9]$ for the two groups, respectively.\n    - Neutral allele frequency for remaining variants: $0.3$ in both groups.\n    - Seed: $123$.\n    - Report top $k = 3$ components.\n\n- Case 2 (boundary case: single ancestry group, minimal structure):\n    - Number of groups: $1$ with group sizes $[120]$.\n    - Number of variants: $400$.\n    - AIM blocks: none (i.e., all variants have the same allele frequency in the single group).\n    - Neutral allele frequency for all variants: $0.3$.\n    - Seed: $456$.\n    - Report top $k = 3$ components.\n\n- Case 3 (three-ancestry cohort with two orthogonal AIM blocks):\n    - Number of groups: $3$ with group sizes $[80, 80, 40]$.\n    - Number of variants: $600$.\n    - AIM blocks: two blocks:\n        - Block 1: count $30$ with group-specific allele frequencies $[0.1, 0.9, 0.9]$.\n        - Block 2: count $30$ with group-specific allele frequencies $[0.9, 0.1, 0.9]$.\n    - Neutral allele frequency for remaining variants: $0.3$ in all groups.\n    - Seed: $789$.\n    - Report top $k = 3$ components.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to a test case and must be a bracketed, comma-separated list of the top $k$ variance-explained fractions, rounded to four decimal places, with no spaces. For example: \"[[0.3210,0.1205,0.0800],[0.0155,0.0132,0.0121],[0.2501,0.1700,0.1200]]\".", "solution": "The problem requires the implementation of a standard population genetics workflow to analyze simulated genotype data. This involves generating synthetic genotypes for admixed populations, standardizing the data, performing principal component analysis (PCA) via singular value decomposition (SVD), and quantifying the population structure by calculating the fraction of total variance explained by the leading principal components.\n\nFirst, we establish the theoretical foundation for calculating the fraction of variance explained from singular values. Let the standardized genotype matrix be $X \\in \\mathbb{R}^{n \\times m'}$, where $n$ is the number of samples and $m'$ is the number of polymorphic variants (i.e., variants with allele frequency $p_j$ such that $0  p_j  1$). The entries of this matrix are given by $x_{ij} = \\frac{g_{ij} - 2 p_j}{\\sqrt{2 p_j (1 - p_j)}}$. By construction, each column of $X$ is centered to have a mean of $0$ and standardized.\n\nThe total variance in the data is a measure of the total spread of the data points. In the context of PCA, it is defined as the trace of the sample covariance matrix, $\\frac{1}{n-1}X^{\\top}X$. The total sum of squares, which is proportional to the total variance, can be computed from the squared Frobenius norm of the matrix $X$:\n$$ \\|X\\|_F^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{m'} x_{ij}^2 $$\nThe singular value decomposition of $X$ is given by $X = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{m' \\times m'}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{n \\times m'}$ is a rectangular diagonal matrix with non-negative singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r  0$ on its diagonal, where $r$ is the rank of $X$.\n\nA key property of SVD is that the squared Frobenius norm of a matrix is equal to the sum of its squared singular values:\n$$ \\|X\\|_F^2 = \\text{Tr}(X^{\\top}X) = \\text{Tr}((U\\Sigma V^{\\top})^{\\top}(U\\Sigma V^{\\top})) = \\text{Tr}(V\\Sigma^{\\top}U^{\\top}U\\Sigma V^{\\top}) = \\text{Tr}(V\\Sigma^{\\top}\\Sigma V^{\\top}) $$\nUsing the cyclic property of the trace, $\\text{Tr}(ABC) = \\text{Tr}(CAB)$, we have:\n$$ \\text{Tr}(V(\\Sigma^{\\top}\\Sigma) V^{\\top}) = \\text{Tr}((\\Sigma^{\\top}\\Sigma)V^{\\top}V) = \\text{Tr}(\\Sigma^{\\top}\\Sigma) = \\sum_{k=1}^{r} \\sigma_k^2 $$\nThe variance captured by the $k$-th principal component is proportional to the square of the $k$-th singular value, $\\sigma_k^2$. Therefore, the fraction of the total variance explained by the $k$-th principal component is the ratio of $\\sigma_k^2$ to the total sum of squared singular values.\n$$ \\text{Fraction of Variance (PC}_k\\text{)} = \\frac{\\sigma_k^2}{\\sum_{l=1}^{r} \\sigma_l^2} $$\nThis formula provides a direct method to compute the desired quantities from the singular values of the standardized genotype matrix.\n\nThe algorithmic procedure to solve the problem for each test case is as follows:\n1.  **Data Simulation**: A genotype matrix $G \\in \\mathbb{R}^{n \\times m}$ is constructed, where $n$ is the total number of samples across all groups and $m$ is the total number of variants.\n    - An allele frequency matrix of size $m \\times (\\text{number of groups})$ is created, populated with frequencies from the specified Ancestry-Informative Marker (AIM) blocks and the neutral background frequency.\n    - A mapping from each sample index $i \\in \\{1, \\dots, n\\}$ to its corresponding ancestry group $g$ is established.\n    - For each sample $i$ in group $g$ and each variant $j$, a genotype dosage $g_{ij}$ is drawn from a binomial distribution, $g_{ij} \\sim \\text{Binomial}(2, p_{j}^{(g)})$, where $p_{j}^{(g)}$ is the allele frequency for variant $j$ in group $g$. A fixed pseudorandom seed ensures reproducibility for each case.\n2.  **Data Standardization**:\n    - The overall allele frequency $p_j$ for each variant $j$ is calculated across all $n$ samples using the formula $p_j = \\frac{1}{2n} \\sum_{i=1}^{n} g_{ij}$.\n    - Variants that are monomorphic (i.e., $p_j = 0$ or $p_j = 1$) are identified and excluded from the matrix $G$, resulting in a new matrix $G'$ of size $n \\times m'$, where $m' \\le m$.\n    - The matrix $G'$ is standardized column-wise to produce the matrix $X$ with entries $x_{ij} = \\frac{g'_{ij} - 2 p'_j}{\\sqrt{2 p'_j (1 - p'_j)}}$, where $p'_j$ are the frequencies for the polymorphic variants.\n3.  **SVD and Variance Calculation**:\n    - The singular value decomposition is computed for the standardized matrix $X$ to obtain the singular values, $\\sigma_k$. We only need the singular values, not the full $U$ and $V$ matrices.\n    - The squared singular values, $\\sigma_k^2$, are calculated. The total variance is computed as their sum, $V_{\\text{total}} = \\sum_k \\sigma_k^2$.\n    - The fraction of variance explained by each of the top $k$ principal components is computed as $\\sigma_k^2 / V_{\\text{total}}$.\n4.  **Reporting**: The resulting fractions for the top $k$ components are collected and rounded to four decimal places as specified. This process is repeated for each test case provided.\n\nThis principled approach ensures a correct and robust implementation that directly follows from the mathematical definitions of PCA and SVD.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n\n    def _solve_case(group_sizes, num_variants, aim_blocks, neutral_af, seed, k):\n        \"\"\"\n        Solves a single test case for genotype simulation and PCA variance analysis.\n        \"\"\"\n        # Initialize a random number generator with the specified seed for reproducibility\n        rng = np.random.default_rng(seed)\n        \n        num_groups = len(group_sizes)\n        n_total_samples = sum(group_sizes)\n        m_total_variants = num_variants\n\n        # 1. Construct allele frequency matrix (p_jg): variants x groups\n        p_variants_groups = np.zeros((m_total_variants, num_groups))\n        \n        current_variant_idx = 0\n        if aim_blocks:\n            for block in aim_blocks:\n                count, freqs = block['count'], block['freqs']\n                p_variants_groups[current_variant_idx:current_variant_idx + count, :] = freqs\n                current_variant_idx += count\n        \n        # Fill remaining variants with the neutral allele frequency\n        if current_variant_idx  m_total_variants:\n            p_variants_groups[current_variant_idx:, :] = neutral_af\n            \n        # 2. Generate genotype matrix G\n        # Create a map from sample index to group index\n        sample_to_group_map = np.repeat(np.arange(num_groups), group_sizes)\n        \n        # Generate genotypes g_ij ~ Binomial(2, p_j^(g))\n        G = np.zeros((n_total_samples, m_total_variants), dtype=np.float64)\n        for i in range(n_total_samples):\n            group_idx = sample_to_group_map[i]\n            # Get all allele frequencies for the variants for that sample's group\n            allele_freqs_for_sample = p_variants_groups[:, group_idx]\n            G[i, :] = rng.binomial(2, p=allele_freqs_for_sample)\n            \n        # 3. Standardize the genotype matrix\n        # Calculate overall allele frequencies p_j for each variant\n        # Use np.sum(G, axis=0, dtype=np.float64) to prevent potential overflow with integer types\n        p_j = np.sum(G, axis=0) / (2 * n_total_samples)\n        \n        # Identify and filter out monomorphic variants (where p_j is 0 or 1)\n        is_polymorphic = (p_j  0)  (p_j  1)\n        \n        G_poly = G[:, is_polymorphic]\n        p_j_poly = p_j[is_polymorphic]\n        \n        # If no polymorphic variants remain, variance is zero.\n        if G_poly.shape[1] == 0:\n            return [0.0] * k\n            \n        # Standardize G_poly to create matrix X\n        mean_g = 2 * p_j_poly\n        std_g = np.sqrt(2 * p_j_poly * (1 - p_j_poly))\n        \n        X = (G_poly - mean_g) / std_g\n        \n        # 4. Compute SVD and calculate variance fractions\n        # We only need the singular values, so compute_uv=False is efficient.\n        singular_values = svd(X, compute_uv=False)\n        \n        # The number of non-zero singular values cannot exceed min(n, m')\n        num_components = len(singular_values)\n        if num_components == 0:\n            return [0.0] * k\n\n        squared_sv = singular_values**2\n        total_variance = np.sum(squared_sv)\n\n        if total_variance == 0:\n            return [0.0] * k\n            \n        variance_fractions = squared_sv / total_variance\n        \n        # 5. Format results for the top k components\n        top_k_fractions = list(variance_fractions[:k])\n        \n        # Pad with zeros if the rank of the matrix is less than k\n        if len(top_k_fractions)  k:\n            top_k_fractions.extend([0.0] * (k - len(top_k_fractions)))\n            \n        return top_k_fractions\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"group_sizes\": [100, 100], \"num_variants\": 500,\n            \"aim_blocks\": [{'count': 80, 'freqs': [0.1, 0.9]}],\n            \"neutral_af\": 0.3, \"seed\": 123, \"k\": 3\n        },\n        # Case 2\n        {\n            \"group_sizes\": [120], \"num_variants\": 400,\n            \"aim_blocks\": [],\n            \"neutral_af\": 0.3, \"seed\": 456, \"k\": 3\n        },\n        # Case 3\n        {\n            \"group_sizes\": [80, 80, 40], \"num_variants\": 600,\n            \"aim_blocks\": [\n                {'count': 30, 'freqs': [0.1, 0.9, 0.9]},\n                {'count': 30, 'freqs': [0.9, 0.1, 0.9]}\n            ],\n            \"neutral_af\": 0.3, \"seed\": 789, \"k\": 3\n        }\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = _solve_case(**params)\n        all_results.append(result)\n\n    # Format the final output string according to the specified format.\n    # \"[[res1_1,res1_2,...],[res2_1,res2_2,...],...]\"\n    # with each fraction rounded to four decimal places.\n    formatted_case_results = []\n    for res_list in all_results:\n        # Format each number to exactly four decimal places\n        formatted_list_str = [f\"{x:.4f}\" for x in res_list]\n        # Join numbers with commas and enclose in brackets\n        formatted_case_results.append(f\"[{','.join(formatted_list_str)}]\")\n    \n    # Join all case results and enclose in the final brackets\n    final_output_string = f\"[{','.join(formatted_case_results)}]\"\n    print(final_output_string)\n\nsolve()\n```", "id": "4370918"}]}