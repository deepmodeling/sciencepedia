{"hands_on_practices": [{"introduction": "In mutational signature analysis, both tumor mutation catalogs and reference signatures are often represented as vectors in a high-dimensional space. A primary step in understanding their relationship involves quantifying the similarity of their mutational profiles. This exercise provides a concrete calculation of cosine similarity, a common metric for vector comparison, and challenges you to think critically about why this simple, pairwise measure is insufficient for the complex task of deconstructing a tumor's history, which is a mixture of multiple mutational processes [@problem_id:4384034].", "problem": "In somatic mutation signature analysis, a tumor’s single-nucleotide variant catalog can be represented as a vector over fixed channels (for example, trinucleotide substitution contexts). A signature is a characteristic distribution over the same channels arising from a particular etiology, often reported as probabilities. Consider a scientifically plausible six-channel abstraction where the sample catalog is given by the count vector $x \\in \\mathbb{R}^{6}$ and each candidate signature is given by a probability vector $s_i \\in \\mathbb{R}^{6}$, all aligned to the same channel ordering. Specifically, let\n$$x = (40,\\,10,\\,5,\\,20,\\,15,\\,10),$$\n$$s_1 = (0.50,\\,0.10,\\,0.05,\\,0.20,\\,0.10,\\,0.05),$$\n$$s_2 = (0.30,\\,0.05,\\,0.15,\\,0.10,\\,0.25,\\,0.15),$$\n$$s_3 = (0.10,\\,0.25,\\,0.30,\\,0.10,\\,0.15,\\,0.10).$$\nUsing the Euclidean geometry of the channel space as a fundamental base—where the direction of a nonzero vector encodes its relative channel composition—compute the cosine similarity between the catalog $x$ and each signature $s_i$. Then, justify from first principles of mixture attribution in precision genomics why cosine similarity to individual signatures alone is insufficient for etiological attribution when multiple signatures may be active. Report only the largest cosine similarity value among $\\{s_1, s_2, s_3\\}$ as your final answer. Express your final answer as a decimal and round to four significant figures. No units are required for the final reported value.", "solution": "The problem requires the computation of cosine similarities between a given mutation catalog vector and three mutational signature vectors, and a subsequent justification for why this metric is insufficient for definitive etiological attribution. The problem is scientifically grounded, well-posed, and contains all necessary information for a valid solution.\n\nThe cosine similarity between two non-zero vectors $A$ and $B$ in a Euclidean space is defined as the cosine of the angle $\\theta$ between them. It is calculated using the dot product and vector norms:\n$$ \\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} $$\nThis metric measures the similarity in the direction of the two vectors, irrespective of their magnitudes. In the context of mutational signatures, it compares the relative proportions of mutations across different channels.\n\nThe given vectors are:\nThe sample mutation catalog count vector:\n$$ x = (40, 10, 5, 20, 15, 10) $$\nThe three candidate signature probability vectors:\n$$ s_1 = (0.50, 0.10, 0.05, 0.20, 0.10, 0.05) $$\n$$ s_2 = (0.30, 0.05, 0.15, 0.10, 0.25, 0.15) $$\n$$ s_3 = (0.10, 0.25, 0.30, 0.10, 0.15, 0.10) $$\n\nFirst, we compute the Euclidean norm (magnitude) of each vector.\nThe norm of the catalog vector $x$:\n$$ \\|x\\| = \\sqrt{40^2 + 10^2 + 5^2 + 20^2 + 15^2 + 10^2} = \\sqrt{1600 + 100 + 25 + 400 + 225 + 100} = \\sqrt{2450} $$\n\nThe norms of the signature vectors $s_1$, $s_2$, and $s_3$:\n$$ \\|s_1\\| = \\sqrt{0.50^2 + 0.10^2 + 0.05^2 + 0.20^2 + 0.10^2 + 0.05^2} = \\sqrt{0.25 + 0.01 + 0.0025 + 0.04 + 0.01 + 0.0025} = \\sqrt{0.315} $$\n$$ \\|s_2\\| = \\sqrt{0.30^2 + 0.05^2 + 0.15^2 + 0.10^2 + 0.25^2 + 0.15^2} = \\sqrt{0.09 + 0.0025 + 0.0225 + 0.01 + 0.0625 + 0.0225} = \\sqrt{0.21} $$\n$$ \\|s_3\\| = \\sqrt{0.10^2 + 0.25^2 + 0.30^2 + 0.10^2 + 0.15^2 + 0.10^2} = \\sqrt{0.01 + 0.0625 + 0.09 + 0.01 + 0.0225 + 0.01} = \\sqrt{0.205} $$\n\nNext, we compute the dot product of $x$ with each signature vector $s_i$.\n$$ x \\cdot s_1 = (40)(0.50) + (10)(0.10) + (5)(0.05) + (20)(0.20) + (15)(0.10) + (10)(0.05) = 20 + 1 + 0.25 + 4 + 1.5 + 0.5 = 27.25 $$\n$$ x \\cdot s_2 = (40)(0.30) + (10)(0.05) + (5)(0.15) + (20)(0.10) + (15)(0.25) + (10)(0.15) = 12 + 0.5 + 0.75 + 2 + 3.75 + 1.5 = 20.5 $$\n$$ x \\cdot s_3 = (40)(0.10) + (10)(0.25) + (5)(0.30) + (20)(0.10) + (15)(0.15) + (10)(0.10) = 4 + 2.5 + 1.5 + 2 + 2.25 + 1 = 13.25 $$\n\nNow, we can calculate the cosine similarity for each signature. Let $\\text{sim}(A, B)$ denote the cosine similarity.\n$$ \\text{sim}(x, s_1) = \\frac{x \\cdot s_1}{\\|x\\| \\|s_1\\|} = \\frac{27.25}{\\sqrt{2450} \\sqrt{0.315}} = \\frac{27.25}{\\sqrt{771.75}} \\approx 0.980906 $$\n$$ \\text{sim}(x, s_2) = \\frac{x \\cdot s_2}{\\|x\\| \\|s_2\\|} = \\frac{20.5}{\\sqrt{2450} \\sqrt{0.21}} = \\frac{20.5}{\\sqrt{514.5}} \\approx 0.903778 $$\n$$ \\text{sim}(x, s_3) = \\frac{x \\cdot s_3}{\\|x\\| \\|s_3\\|} = \\frac{13.25}{\\sqrt{2450} \\sqrt{0.205}} = \\frac{13.25}{\\sqrt{502.25}} \\approx 0.591229 $$\n\nComparing the three values, the largest cosine similarity is with signature $s_1$: $0.980906...$.\n\nThe second part of the task is to justify from first principles why cosine similarity to individual signatures is insufficient for etiological attribution.\nThe fundamental principle of somatic mutation signature analysis is that the mutation catalog observed in a tumor, represented by the vector $x$, is the result of the cumulative action of multiple mutational processes (etiologies) over time. Each process is characterized by a signature vector $s_i$. Therefore, the observed catalog $x$ is not expected to be identical to a single signature, but rather a linear combination of all active signatures. This can be expressed as:\n$$ x \\approx \\sum_{i=1}^{k} a_i s_i = S a $$\nwhere $k$ is the number of active signatures, $s_i$ are the signature vectors, $a_i \\ge 0$ are the \"exposures\" or \"activities\" representing the number of mutations contributed by each signature, $S$ is the matrix whose columns are the signature vectors $s_i$, and $a$ is the vector of exposures.\n\nThe goal of etiological attribution is to solve this mixture problem, i.e., to deconvolve the composite signal $x$ and determine the exposure vector $a$. This is fundamentally a system-level problem, requiring the consideration of all candidate signatures simultaneously.\n\nCosine similarity, $\\text{sim}(x, s_j)$, only measures the alignment between the observed catalog $x$ and a *single* signature vector $s_j$. A high value indicates that the relative proportions of mutations in $x$ are similar to those in $s_j$. However, this is insufficient for attribution for several reasons:\n1.  **Ignoring Mixture Effects**: A vector $x$ can be a linear combination of two or more signature vectors, e.g., $x = a_1 s_1 + a_2 s_2$, yet still have a higher cosine similarity to another vector, $s_3$, or to one of its components, say $s_1$, than the other. In the high-dimensional channel space, the vector $x$ may lie in the conical hull of the set of signatures $\\{s_i\\}$. Simple pairwise angular distance to the basis vectors $\\{s_i\\}$ does not reveal the coordinates (the exposures $a_i$) of $x$ in that basis. For example, the vector sum of two signatures could point in a direction that is very close to a third, unrelated signature, leading to a spurious high similarity score.\n2.  **Lack of Exclusivity**: A high similarity to $s_1$ does not imply that other signatures did not contribute. Our calculation shows $\\text{sim}(x, s_1) \\approx 0.9809$, which is very high. However, the normalized catalog $x_{\\text{norm}} = x / \\sum x_j = (0.4, 0.1, 0.05, 0.2, 0.15, 0.1)$ is not identical to $s_1 = (0.5, 0.1, 0.05, 0.2, 0.1, 0.05)$. The discrepancies might be explained by contributions from other signatures, like $s_2$ and/or $s_3$.\n3.  **Correct Methodology**: The rigorous approach to attribution is to find the vector of non-negative exposures $a$ that minimizes the reconstruction error, typically the squared Euclidean distance:\n    $$ \\min_{a \\ge 0} \\| x - S a \\|^2_2 $$\n    This is a non-negative least squares (NNLS) problem. Solving this optimization problem provides the estimated contribution of every candidate signature, yielding a complete etiological profile. This method accounts for the interplay between all signatures in reconstructing the observed catalog, a crucial aspect that pairwise cosine similarity completely ignores.\n\nIn summary, while cosine similarity is a useful heuristic for an initial assessment of which signatures might be present, it is not a substitute for a formal deconvolution that solves the full mixture problem. Relying solely on the highest cosine similarity for attribution is a scientifically unsound oversimplification that can lead to incorrect conclusions about the mutational processes active in a tumor.\n\nThe largest computed cosine similarity value is approximately $0.980906$. Rounding to four significant figures gives $0.9809$.", "answer": "$$\\boxed{0.9809}$$", "id": "4384034"}, {"introduction": "Accurate mutational signature analysis requires normalizing raw mutation counts by the frequency of the underlying trinucleotide contexts in the genome, a concept known as the mutational \"opportunity.\" This essential step corrects for intrinsic biases where some contexts are simply more common than others. This practice guides you through the fundamental bioinformatic task of building a normalized opportunity vector from a reference sequence, implementing key principles like pyrimidine standardization and accounting for real-world complexities such as ambiguous bases and masked genomic regions [@problem_id:4383920].", "problem": "You are given a simplified setting for constructing a Single Base Substitution ninety-six category (SBS96) opportunity vector used in somatic mutation signature analysis. The SBS96 representation aggregates the count of possible mutation sites across six substitution classes and their flanking trinucleotide contexts. The core scientific facts to start from are: double-stranded Deoxyribonucleic Acid (DNA) is composed of complementary base pairs with Adenine (A) pairing with Thymine (T) and Cytosine (C) pairing with Guanine (G); somatic mutations arise at nucleotides independent of strand orientation; the trinucleotide context that determines SBS96 channels is defined by the immediately adjacent $5'$ and $3'$ bases around the mutated base; and the SBS96 scheme standardizes the central base to a pyrimidine (C or T) by reverse-complementing Adenine (A) and Guanine (G) centered contexts. The SBS96 opportunity vector quantifies the number of available sites per channel in a reference genome after excluding masked regions such as repetitive or low-mappability loci.\n\nDefinitions and core setup:\n- Let the reference genome be represented as a list of contig strings composed of the characters $\\{A, C, G, T, N\\}$, where $N$ denotes an ambiguous base that is excluded from any opportunity count.\n- A trinucleotide window at position $i$ is defined as $(b_{i-1}, b_i, b_{i+1})$ for indices $i$ satisfying $1 \\le i \\le L-2$ on a contig of length $L$.\n- The pyrimidine standardization uses the reverse complement mapping. Let $\\operatorname{comp}(A)=T$, $\\operatorname{comp}(T)=A$, $\\operatorname{comp}(C)=G$, and $\\operatorname{comp}(G)=C$. The reverse complement of a trinucleotide $(x, y, z)$ is $(\\operatorname{comp}(z), \\operatorname{comp}(y), \\operatorname{comp}(x))$. If the central base $y$ is a purine ($A$ or $G$), then the standardized context is the reverse complement trinucleotide, making the central base a pyrimidine ($C$ or $T$).\n- The SBS96 channels are organized in six substitution classes in the order $C \\to A$, $C \\to G$, $C \\to T$, $T \\to A$, $T \\to C$, $T \\to G$. Each substitution class is subdivided by $16$ trinucleotide contexts defined by the $5'$ and $3'$ bases drawn from $\\{A, C, G, T\\}$, ordered lexicographically by $5'$ base in the order $A, C, G, T$ and then $3'$ base in the order $A, C, G, T$. For $C$-centered contexts, only the first three substitution classes apply; for $T$-centered contexts, only the last three substitution classes apply.\n- A site contributes to an opportunity count if and only if all three bases in the trinucleotide are non-ambiguous ($\\neq N$) and none of the three positions in the trinucleotide window is masked.\n\nMasking rules:\n- Repetitive-region masking is provided as half-open index intervals $[a,b)$ per contig; any base position in any such interval is masked.\n- Low-mappability masking is provided as explicit sets of base indices per contig; any listed position is masked.\n- The effective mask is the union: a base is masked if it appears in either the repetitive or low-mappability mask.\n- A trinucleotide is excluded from opportunity counting if any of its three positions is masked.\n\nRequired computation:\n- For each test case, compute the SBS96 opportunity vector as follows:\n  1. Iterate over all contigs and positions $i$ from $1$ to $L-2$.\n  2. Form the trinucleotide $(b_{i-1}, b_i, b_{i+1})$. If any base is $N$, skip.\n  3. If $b_i \\in \\{A, G\\}$, replace the trinucleotide with its reverse complement; otherwise use it as-is. After this step, the central base is guaranteed to be either $C$ or $T$.\n  4. If any of the positions $(i-1)$, $i$, $(i+1)$ is masked in the effective mask for its contig, skip this trinucleotide.\n  5. Count this standardized trinucleotide context into the appropriate $C$-centered or $T$-centered set of $16$ contexts using lexicographic indexing of $5'$ and $3'$ bases.\n  6. Build the SBS96 vector by repeating each $C$-centered context count across the three $C \\to \\{A, G, T\\}$ classes (in order), and each $T$-centered context count across the three $T \\to \\{A, C, G\\}$ classes (in order).\n  7. Normalize the vector so that the sum over all $96$ entries equals $1$. If no opportunities are available (the sum is zero), return a $96$-length vector of zeros.\n- Express all normalized outputs as decimal floats in the interval $[0, 1]$ rounded to six decimal places.\n\nTest suite:\nUse the following fixed reference contigs and masks. Contig indices are $0$-based; base indices in intervals and sets are $0$-based; half-open intervals $[a,b)$ include $a$ and exclude $b$.\n\n- Reference contigs:\n  - Contig $0$: \"ACGTTCCGATTTACGCGTACCTTAGGCTATAGCGTATCGATCGTTACGAACG\" (length $52$)\n  - Contig $1$: \"TTACGNNNCGATCGTTTACGCCCTTAGCGTATCGATCGTTACGCGTAT\" (length $48$)\n\n- Test case $1$ (baseline, no masking):\n  - Repetitive intervals: $\\{\\}$ for both contigs\n  - Low-mappability positions: $\\{\\}$ for both contigs\n\n- Test case $2$ (repetitive masking only):\n  - Repetitive intervals:\n    - Contig $0$: $[10,20)$, $[35,40)$\n    - Contig $1$: $[5,12)$, $[30,36)$\n  - Low-mappability positions: none\n\n- Test case $3$ (low-mappability masking only):\n  - Repetitive intervals: none\n  - Low-mappability positions:\n    - Contig $0$: $\\{2,3,4,25,26,27\\}$\n    - Contig $1$: $\\{0,1,20,21,35\\}$\n\n- Test case $4$ (combined masking: union of test cases $2$ and $3$):\n  - Repetitive intervals as in test case $2$\n  - Low-mappability positions as in test case $3$\n\n- Test case $5$ (fully masked genome):\n  - Repetitive intervals:\n    - Contig $0$: $[0,52)$\n    - Contig $1$: $[0,48)$\n  - Low-mappability positions: none\n\nOutput specification:\n- For each test case, compute the normalized SBS96 opportunity vector as described.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and must be the list of $96$ floats for that case. For example, the output should resemble \"[[v1_0,...,v1_95],[v2_0,...,v2_95],...]\" where $vk_j$ are the normalized decimal floats rounded to six decimals.", "solution": "The problem requires the computation of a Single Base Substitution ninety-six category (SBS96) opportunity vector for a given miniature genome under various masking conditions. The solution is designed by systematically implementing the specified biological and computational rules, ensuring scientific correctness and adherence to the defined data structures.\n\nThe methodological approach unfolds in the following steps:\n\n1.  **Modeling the Genome and Masks**: The reference genome is represented as a list of strings, one for each contig. The masking information, provided as repetitive intervals and discrete low-mappability positions, must be consolidated into an efficient data structure for rapid lookups. For each contig, a single boolean NumPy array, the `effective_mask`, is created. A position $j$ is marked as `True` in this array if it falls within any repetitive interval $[a,b)$ or is explicitly listed in the low-mappability set. This pre-computation of a unified mask allows for constant-time checking of whether a given base position is masked, which is critical for performance when processing large sequences.\n\n2.  **Trinucleotide Context Enumeration**: The core of the calculation involves iterating through every possible trinucleotide site in the genome. Following the problem's definition, we traverse each contig of length $L$, considering each 3-base window $(b_{i-1}, b_i, b_{i+1})$ where the central base index $i$ ranges from $1$ to $L-2$. This corresponds to a sliding window of size $3$ across the sequence.\n\n3.  **Applying Exclusion Criteria**: Before a trinucleotide can be considered a valid mutation opportunity, it must pass two essential filtering steps as per the problem specification:\n    *   **Ambiguity Check**: The trinucleotide is discarded if any of its three constituent bases is the ambiguous character $N$. This ensures that only well-defined sequences contribute to the opportunity counts.\n    *   **Mask Check**: The trinucleotide is discarded if any of its three chromosomal positions, $(i-1, i, i+1)$, is flagged as `True` in the `effective_mask`. This correctly models the biological and technical reality that mutation calls are unreliable in masked regions, and thus these regions do not present valid 'opportunities' for observing mutations.\n\n4.  **Pyrimidine Standardization Principle**: The SBS96 framework standardizes all mutational contexts to be centered on a pyrimidine ($C$ or $T$). This convention is rooted in the complementary nature of double-stranded DNA and the principle of strand-indifference for many mutational processes. The implementation follows this rule rigorously:\n    *   If the central base of a valid trinucleotide is already a pyrimidine ($C$ or $T$), the trinucleotide is used as is.\n    *   If the central base is a purine ($A$ or $G$), its reverse complement is computed. For a trinucleotide $(x, y, z)$, the reverse complement is defined as $(\\operatorname{comp}(z), \\operatorname{comp}(y), \\operatorname{comp}(x))$, where $\\operatorname{comp}$ is the base complement function. This transformation correctly maps, for instance, an $A$-centered context to its equivalent $T$-centered context on the opposite strand, ensuring that all opportunities are counted under a consistent pyrimidine-centered scheme.\n\n5.  **Counting and Aggregation**: The set of valid, standardized trinucleotides is then categorized and counted. Two separate $16$-element arrays, `c_counts` and `t_counts`, are maintained for $C$-centered and $T$-centered contexts, respectively. The $16$ channels for each correspond to the $4 \\times 4$ possible combinations of $5'$ and $3'$ flanking bases. A mapping from bases $(A, C, G, T)$ to integer indices $(0, 1, 2, 3)$ is used to calculate a unique context index for each standardized trinucleotide: $idx = \\text{5'\\_base\\_idx} \\times 4 + \\text{3'\\_base\\_idx}$. This calculation scheme ensures that the counts are aggregated according to the specified lexicographical ordering of flanking bases.\n\n6.  **Constructing the SBS96 Vector**: The final $96$-element vector is constructed from the two $16$-element count arrays. The SBS96 format consists of six substitution classes in the order $C \\to A$, $C \\to G$, $C \\to T$, $T \\to A$, $T \\to C$, $T \\to G$. The opportunity count for a specific trinucleotide context (e.g., $A[C]G$) is assumed to be the same regardless of which base the central $C$ mutates to. Therefore, the `c_counts` array is repeated for each of the three $C$-substitution classes. Similarly, the `t_counts` array is repeated for each of the three $T$-substitution classes. The final vector is formed by concatenating `c_counts` three times, followed by `t_counts` three times.\n\n7.  **Normalization and Formatting**: The raw count vector represents the absolute number of opportunities. To make it comparable across different genomes or regions of varying sizes, it is normalized to a probability distribution. This is achieved by dividing each of the $96$ elements by the total sum of all counts. A special case is handled where the total sum is zero (as occurs in a fully masked genome); in this scenario, a vector of $96$ zeros is returned as specified. Finally, each element of the normalized vector is rounded to six decimal places, and the results for all test cases are formatted into the precise string representation required by the problem statement, which is a list of lists of floating-point numbers.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and prints the SBS96 opportunity vectors for a suite of test cases.\n    The entire logic is self-contained within this function.\n    \"\"\"\n\n    # Define constants and test cases as per the problem statement.\n    CONTIGS = [\n        \"ACGTTCCGATTTACGCGTACCTTAGGCTATAGCGTATCGATCGTTACGAACG\",  # L=52\n        \"TTACGNNNCGATCGTTTACGCCCTTAGCGTATCGATCGTTACGCGTAT\",      # L=48\n    ]\n\n    BASE_MAP = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    COMPLEMENT_MAP = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n    \n    test_cases = [\n        # TC1: No mask (baseline)\n        ({}, {}),\n        # TC2: Repetitive mask only\n        ({0: [[10, 20], [35, 40]], 1: [[5, 12], [30, 36]]}, {}),\n        # TC3: Low-mappability mask only\n        ({}, {0: {2, 3, 4, 25, 26, 27}, 1: {0, 1, 20, 21, 35}}),\n        # TC4: Combined masking (union of TC2 and TC3)\n        ({0: [[10, 20], [35, 40]], 1: [[5, 12], [30, 36]]}, \n         {0: {2, 3, 4, 25, 26, 27}, 1: {0, 1, 20, 21, 35}}),\n        # TC5: Fully masked genome\n        ({0: [[0, 52]], 1: [[0, 48]]}, {}),\n    ]\n\n    def calculate_sbs96_opportunities(rep_masks, lm_masks):\n        \"\"\"\n        Computes the normalized SBS96 opportunity vector for a given set of masks.\n        \"\"\"\n        c_counts = np.zeros(16, dtype=np.int64)\n        t_counts = np.zeros(16, dtype=np.int64)\n\n        # Pre-process masks into a single boolean array per contig.\n        effective_masks = []\n        for c_idx, seq in enumerate(CONTIGS):\n            mask = np.zeros(len(seq), dtype=bool)\n            # Apply repetitive masks (half-open intervals [a, b))\n            if c_idx in rep_masks:\n                for start, end in rep_masks[c_idx]:\n                    mask[start:end] = True\n            # Apply low-mappability masks (sets of indices)\n            if c_idx in lm_masks:\n                for pos in lm_masks[c_idx]:\n                    mask[pos] = True\n            effective_masks.append(mask)\n\n        # Iterate over all possible trinucleotides and count opportunities.\n        for c_idx, seq in enumerate(CONTIGS):\n            L = len(seq)\n            mask = effective_masks[c_idx]\n            # Loop for the central base position 'i' from 1 to L-2 (0-based indexing).\n            for i in range(1, L - 1):\n                # A trinucleotide is excluded if any of its 3 positions is masked.\n                if mask[i - 1] or mask[i] or mask[i + 1]:\n                    continue\n                \n                b_prev, b_mid, b_next = seq[i - 1], seq[i], seq[i + 1]\n                \n                # Exclude trinucleotides with ambiguous bases ('N').\n                if 'N' in (b_prev, b_mid, b_next):\n                    continue\n\n                # Standardize context to be pyrimidine-centered (C or T).\n                if b_mid in ('A', 'G'):  # Purine-centered\n                    std_prev = COMPLEMENT_MAP[b_next]\n                    std_mid = COMPLEMENT_MAP[b_mid]\n                    std_next = COMPLEMENT_MAP[b_prev]\n                else:  # Pyrimidine-centered\n                    std_prev, std_mid, std_next = b_prev, b_mid, b_next\n\n                # Calculate the 16-channel context index based on flanking bases.\n                # Order: 5' base (A,C,G,T), then 3' base (A,C,G,T).\n                context_idx = BASE_MAP[std_prev] * 4 + BASE_MAP[std_next]\n                \n                # Increment the count for the appropriate standardized context.\n                if std_mid == 'C':\n                    c_counts[context_idx] += 1\n                else:  # std_mid == 'T'\n                    t_counts[context_idx] += 1\n                    \n        # Construct the 96-element opportunity vector.\n        # Order: 3 classes for C (C>A,C>G,C>T) + 3 classes for T (T>A,T>C,T>G).\n        sbs96_counts = np.concatenate([c_counts] * 3 + [t_counts] * 3)\n        \n        # Normalize the vector to sum to 1.\n        total_sum = sbs96_counts.sum()\n        if total_sum == 0:\n            return [0.0] * 96\n            \n        normalized_vector = sbs96_counts / total_sum\n        \n        # Round to 6 decimal places and convert to a list of floats.\n        return np.round(normalized_vector, 6).tolist()\n\n    results = []\n    for rep_masks, lm_masks in test_cases:\n        result = calculate_sbs96_opportunities(rep_masks, lm_masks)\n        results.append(result)\n\n    # Format the output as a string representing a list of lists of floats,\n    # with no spaces, as specified by the output format example.\n    inner_parts = [f\"[{','.join(map(str, res_list))}]\" for res_list in results]\n    final_output_str = f\"[{','.join(inner_parts)}]\"\n\n    print(final_output_str)\n\nsolve()\n```", "id": "4383920"}, {"introduction": "To establish a quantitative link between a mutational signature and its etiology, we can use statistical models that relate signature activity to exposure variables like age or environmental factors. The Poisson Generalized Linear Model (GLM) is a powerful and standard framework for modeling such count data. In this exercise, you will implement a Poisson GLM to test for a relationship between mutation counts and exposure levels, correctly incorporating mutation opportunity as a model offset, developing your skills in hypothesis testing and advanced statistical modeling for etiological discovery [@problem_id:4384020].", "problem": "A somatic mutation signature is defined as a reproducible pattern of single-nucleotide variants across trinucleotide contexts attributable to specific mutational processes. A core empirical observation is that, within a genomic region, independently arising mutations can be modeled as events of a Poisson process, where the expected mutation count scales with the number of available trinucleotide contexts (the mutation opportunity) and a context-specific rate. In precision medicine and genomic diagnostics, etiological exposures (for example, tobacco smoke constituents or ultraviolet radiation intensity) are treated as covariates that modulate the mutation rate. Under a Generalized Linear Model (GLM) with a logarithmic link, a multiplicative rate model becomes additive on the log scale, and the opportunity is incorporated as an offset. Concretely, let there be $n$ individuals indexed by $i \\in \\{1,\\dots,n\\}$. For a single signature, let $y_i$ denote the observed mutation count in that individual, $o_i$ the mutation opportunity (a positive number), and $x_{i1}, x_{i2}$ two exposure covariates. Under the Poisson GLM with log link, the linear predictor is $ \\eta_i = \\alpha + \\beta_1 x_{i1} + \\beta_2 f(x_{i2}) + \\log(o_i)$, and the mean satisfies $ \\mu_i = \\exp(\\eta_i)$. The function $f(\\cdot)$ is unknown and may be nonlinear. You are tasked with fitting two nested models: a null model that assumes $f(x_{i2}) = x_{i2}$ (linear effect) and an alternative model that allows a quadratic nonlinearity, $f(x_{i2}) = x_{i2} + \\gamma x_{i2}^2$. Both models include an intercept and the exposure $x_{i1}$ linearly, and both use $\\log(o_i)$ as an offset. Maximum likelihood estimation under the Poisson GLM is required. To assess nonlinearity, you must perform a likelihood ratio test that compares the null and alternative models, with the test statistic defined as twice the difference in maximized log-likelihoods and compared to the chi-square distribution with one degree of freedom.\n\nStarting only from the foundational facts above (Poisson counts for independent mutations; log-link GLM with opportunity offset; maximum likelihood estimation via iteratively reweighted least squares; and likelihood ratio testing using the chi-square reference under regularity), implement a program that, for each dataset in the test suite below, fits both models, computes the likelihood ratio statistic, and outputs the corresponding p-value as a decimal rounded to six places.\n\nAll quantities are unitless for the purposes of this computation. The outputs must be presented strictly in the required format.\n\nThe test suite consists of four independent datasets. Each dataset provides $y_i$, $x_{i1}$, $x_{i2}$, and $o_i$ for $n=12$ individuals. The vectors are:\n\n- Dataset A (nonlinear effect present, heterogeneous opportunity):\n    - Counts $y$: $[1,1,2,5,15,6,3,2,3,11,6,4]$\n    - Exposure $x_1$: $[0,5,10,15,20,25,30,35,40,10,20,30]$\n    - Exposure $x_2$: $[0,1,2,3,4,3,2,1,0,4,3,2]$\n    - Opportunity $o$: $[2.2 \\times 10^{7}, 2.0 \\times 10^{7}, 1.8 \\times 10^{7}, 2.5 \\times 10^{7}, 2.7 \\times 10^{7}, 2.4 \\times 10^{7}, 2.1 \\times 10^{7}, 1.9 \\times 10^{7}, 2.3 \\times 10^{7}, 2.6 \\times 10^{7}, 2.8 \\times 10^{7}, 2.2 \\times 10^{7}]$\n- Dataset B (approximately linear effect across exposures):\n    - Counts $y$: $[0,1,1,2,4,1,2,3,7,1,2,2]$\n    - Exposure $x_1$: $[0,5,10,15,20,10,20,30,40,5,15,25]$\n    - Exposure $x_2$: $[0,1,2,3,4,0,1,2,3,4,2,1]$\n    - Opportunity $o$: $[1.8 \\times 10^{7}, 2.0 \\times 10^{7}, 2.2 \\times 10^{7}, 2.4 \\times 10^{7}, 2.6 \\times 10^{7}, 2.1 \\times 10^{7}, 2.3 \\times 10^{7}, 2.5 \\times 10^{7}, 2.7 \\times 10^{7}, 1.9 \\times 10^{7}, 2.2 \\times 10^{7}, 2.4 \\times 10^{7}]$\n- Dataset C (low counts with many zeros; test numerical stability):\n    - Counts $y$: $[0,0,0,0,0,1,1,1,1,1,1,2]$\n    - Exposure $x_1$: $[0,0,5,5,10,10,0,5,10,0,5,10]$\n    - Exposure $x_2$: $[0,1,0,1,0,1,2,2,2,3,3,3]$\n    - Opportunity $o$: $[2.0 \\times 10^{7}, 2.0 \\times 10^{7}, 2.0 \\times 10^{7}, 2.0 \\times 10^{7}, 2.0 \\times 10^{7}, 2.0 \\times 10^{7}, 2.0 \\times 10^{7}, 2.0 \\times 10^{7}, 2.0 \\times 10^{7}, 2.0 \\times 10^{7}, 2.0 \\times 10^{7}, 2.0 \\times 10^{7}]$\n- Dataset D (strong curvature with wide opportunity range):\n    - Counts $y$: $[0,0,1,2,12,0,0,1,4,0,1,3]$\n    - Exposure $x_1$: $[0,10,20,30,40,15,25,35,5,12,22,32]$\n    - Exposure $x_2$: $[0,2,4,6,8,1,3,5,7,2,4,6]$\n    - Opportunity $o$: $[1.0 \\times 10^{7}, 5.0 \\times 10^{7}, 1.0 \\times 10^{8}, 2.0 \\times 10^{8}, 5.0 \\times 10^{8}, 2.0 \\times 10^{7}, 8.0 \\times 10^{7}, 1.5 \\times 10^{8}, 3.0 \\times 10^{8}, 4.0 \\times 10^{7}, 1.2 \\times 10^{8}, 2.5 \\times 10^{8}]$\n\nImplementation requirements:\n- Fit the null and alternative Poisson GLM models by maximum likelihood using iteratively reweighted least squares.\n- Include an intercept term, model $x_1$ linearly in both models, model $x_2$ linearly in the null model, and model $x_2$ linearly plus $x_2^2$ in the alternative model.\n- Treat $\\log(o_i)$ as an offset added to the linear predictor.\n- Compute the maximized log-likelihood for each model as $\\sum_{i=1}^{n} y_i \\log(\\mu_i) - \\mu_i$, where $\\mu_i = \\exp(\\eta_i)$, and constants that do not depend on parameters may be ignored because the likelihood ratio cancels them.\n- Perform a likelihood ratio test comparing the models, using the chi-square distribution with $1$ degree of freedom, and output the p-value.\n- For each dataset, return the p-value as a float rounded to six decimal places.\n- Final output format: Your program should produce a single line of output containing the p-values for the four datasets as a comma-separated list enclosed in square brackets, for example, $[0.123456,0.000001,0.500000,0.045678]$.\n\nYour program must be self-contained, require no user input, and adhere to the execution environment constraints. The final output values must be decimals rounded to six places.", "solution": "The problem requires the implementation of a statistical analysis pipeline to assess the nonlinearity of an exposure-response relationship in the context of somatic mutation signatures. This involves fitting two nested Poisson Generalized Linear Models (GLMs) using Maximum Likelihood Estimation (MLE) and comparing them via a Likelihood Ratio Test (LRT). The entire process is grounded in established statistical theory.\n\n### 1. The Poisson Generalized Linear Model (GLM)\n\nThe number of somatic mutations, $y_i$, observed in a genomic region for an individual $i$, is modeled as a random variable following a Poisson distribution. The probability mass function for a single observation $y_i$ is given by:\n$$ P(Y_i=y_i | \\mu_i) = \\frac{\\mu_i^{y_i} e^{-\\mu_i}}{y_i!} $$\nwhere $\\mu_i$ is the expected number of mutations for individual $i$.\n\nA GLM relates the mean $\\mu_i$ to a set of covariates through a link function and a linear predictor. For Poisson-distributed count data, a natural choice is the logarithmic link function, which ensures that the predicted mean $\\mu_i$ is always positive:\n$$ \\log(\\mu_i) = \\eta_i $$\nThe term $\\eta_i$ is the linear predictor, which is a linear combination of the model parameters and covariates. A crucial component of this model is the mutation opportunity, $o_i$, which represents the number of genomic sites (e.g., specific trinucleotides) where a mutation could occur. The expected count $\\mu_i$ is directly proportional to $o_i$. This is incorporated into the GLM by including $\\log(o_i)$ as an offset term in the linear predictor. An offset is a predictor variable whose coefficient is fixed at $1$.\nThe linear predictor for individual $i$ is thus:\n$$ \\eta_i = \\boldsymbol{x}_i^T \\boldsymbol{\\beta} + \\log(o_i) $$\nwhere $\\boldsymbol{x}_i$ is the vector of covariates for individual $i$ and $\\boldsymbol{\\beta}$ is the vector of corresponding regression coefficients to be estimated. Consequently, the mean mutation count $\\mu_i$ is modeled as:\n$$ \\mu_i = \\exp(\\eta_i) = \\exp(\\boldsymbol{x}_i^T \\boldsymbol{\\beta} + \\log(o_i)) = o_i \\exp(\\boldsymbol{x}_i^T \\boldsymbol{\\beta}) $$\nThis formulation correctly models the rate of mutation per opportunity, $\\exp(\\boldsymbol{x}_i^T \\boldsymbol{\\beta})$, which is modulated by the exposures.\n\n### 2. Nested Models for Hypothesis Testing\n\nTo test for a nonlinear effect of the exposure covariate $x_{i2}$, we define two nested models.\n\n**Null Model ($M_0$): Linear Effect**\nThis model assumes that the effect of $x_{i2}$ on the log-mutation rate is linear. The linear predictor is:\n$$ \\eta_{i,0} = \\alpha_0 + \\beta_{1,0} x_{i1} + \\beta_{2,0} x_{i2} + \\log(o_i) $$\nThe parameter vector for the null model is $\\boldsymbol{\\beta}_0 = [\\alpha_0, \\beta_{1,0}, \\beta_{2,0}]^T$. The design matrix $\\mathbf{X}_0$ for $n$ individuals has $n$ rows and $3$ columns: a column of ones for the intercept $\\alpha_0$, a column for the covariate $x_{i1}$, and a column for the covariate $x_{i2}$.\n\n**Alternative Model ($M_1$): Quadratic Effect**\nThis model extends the null model by including a quadratic term for $x_{i2}$ to capture potential nonlinearity. The linear predictor is:\n$$ \\eta_{i,1} = \\alpha_1 + \\beta_{1,1} x_{i1} + \\beta_{2,1} x_{i2} + \\gamma_1 x_{i2}^2 + \\log(o_i) $$\nThe parameter vector is $\\boldsymbol{\\beta}_1 = [\\alpha_1, \\beta_{1,1}, \\beta_{2,1}, \\gamma_1]^T$. The design matrix $\\mathbf{X}_1$ has $n$ rows and $4$ columns: columns for the intercept, $x_{i1}$, $x_{i2}$, and a new column for $x_{i2}^2$. The null model is nested within the alternative model, which corresponds to the constraint $\\gamma_1 = 0$.\n\n### 3. Maximum Likelihood Estimation via IRWLS\n\nThe parameters $\\boldsymbol{\\beta}$ for each model are estimated by maximizing the Poisson log-likelihood function. For a sample of $n$ individuals, the log-likelihood is:\n$$ \\mathcal{L}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( y_i \\log(\\mu_i(\\boldsymbol{\\beta})) - \\mu_i(\\boldsymbol{\\beta}) - \\log(y_i!) \\right) $$\nFor comparing models via the LRT, the term $\\log(y_i!)$ is constant and can be ignored. Thus, we maximize:\n$$ \\hat{\\mathcal{L}}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( y_i \\log(\\mu_i) - \\mu_i \\right) = \\sum_{i=1}^{n} (y_i \\eta_i - \\exp(\\eta_i)) $$\nThe maximization is performed using the Iteratively Reweighted Least Squares (IRWLS) algorithm, which is equivalent to Newton's method for this problem. The steps are as follows:\n\n1.  **Initialization**: Start with an initial estimate for the coefficients, e.g., $\\boldsymbol{\\beta}^{(0)} = \\mathbf{0}$.\n2.  **Iteration**: At step $k$, given the current estimate $\\boldsymbol{\\beta}^{(k)}$:\n    a.  Compute the linear predictor: $\\boldsymbol{\\eta}^{(k)} = \\mathbf{X} \\boldsymbol{\\beta}^{(k)} + \\log(\\boldsymbol{o})$.\n    b.  Compute the mean vector: $\\boldsymbol{\\mu}^{(k)} = \\exp(\\boldsymbol{\\eta}^{(k)})$.\n    c.  Define the diagonal weight matrix $\\mathbf{W}^{(k)}$ with entries $W_{ii}^{(k)} = \\mu_i^{(k)}$. These weights arise from the Fisher information matrix for the Poisson GLM with a log link.\n    d.  Construct the adjusted dependent variable $\\boldsymbol{z}^{(k)}$:\n        $$ z_i^{(k)} = \\eta_i^{(k)} + \\frac{y_i - \\mu_i^{(k)}}{\\mu_i^{(k)}} $$\n    e.  Update the coefficient estimate by solving the following weighted least squares problem, where the offset is subtracted from the adjusted variable:\n        $$ \\boldsymbol{\\beta}^{(k+1)} = (\\mathbf{X}^T \\mathbf{W}^{(k)} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W}^{(k)} (\\boldsymbol{z}^{(k)} - \\log(\\boldsymbol{o})) $$\n3.  **Convergence**: Repeat the iterative step until the change in $\\boldsymbol{\\beta}$ between successive iterations is below a pre-defined tolerance.\n\nThis procedure is applied separately to fit the null model $M_0$ (using $\\mathbf{X}_0$) and the alternative model $M_1$ (using $\\mathbf{X}_1$), yielding maximized log-likelihoods $\\hat{\\mathcal{L}}_0$ and $\\hat{\\mathcal{L}}_1$.\n\n### 4. Likelihood Ratio Test (LRT)\n\nThe LRT is used to formally test the null hypothesis $H_0: \\gamma_1 = 0$ against the alternative $H_1: \\gamma_1 \\neq 0$. The test statistic is defined as twice the difference in the maximized log-likelihoods:\n$$ D = 2(\\hat{\\mathcal{L}}_1 - \\hat{\\mathcal{L}}_0) $$\nUnder the null hypothesis, Wilks' theorem states that $D$ asymptotically follows a chi-square ($\\chi^2$) distribution. The degrees of freedom ($df$) for this distribution equals the difference in the number of parameters between the two models. In this case:\n$$ df = (\\text{number of parameters in } M_1) - (\\text{number of parameters in } M_0) = 4 - 3 = 1 $$\nA larger value of $D$ provides stronger evidence against the null model of a purely linear effect. The p-value is the probability of observing a test statistic as extreme as, or more extreme than, $D$ under the $\\chi^2_1$ distribution:\n$$ \\text{p-value} = P(\\chi^2_1 \\ge D) $$\nThis p-value quantifies the statistical evidence for the quadratic term, and thus for the nonlinearity of the relationship between exposure $x_2$ and mutation rate. The final implementation calculates this p-value for each provided dataset.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef fit_poisson_glm(y, X, offset, max_iter=50, tol=1e-8):\n    \"\"\"\n    Fits a Poisson GLM with a log link using Iteratively Reweighted Least Squares (IRWLS).\n\n    Args:\n        y (np.ndarray): The response variable (counts).\n        X (np.ndarray): The design matrix.\n        offset (np.ndarray): The offset term, pre-log-transformed.\n        max_iter (int): Maximum number of iterations for IRWLS.\n        tol (float): Convergence tolerance for the change in coefficients.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The estimated coefficients (beta).\n            - float: The maximized log-likelihood.\n    \"\"\"\n    n_samples, n_features = X.shape\n    beta = np.zeros(n_features)\n\n    for _ in range(max_iter):\n        beta_old = beta.copy()\n        \n        eta = X @ beta + offset\n        mu = np.exp(eta)\n        \n        # Clip mu to prevent numerical instability (division by zero or overflow)\n        mu = np.clip(mu, 1e-10, 1e12)\n        \n        # Weights for the weighted least squares step\n        weights = mu\n        \n        # Adjusted dependent variable (working response)\n        # z_no_offset = (eta - offset) + (y - mu) / mu\n        z_no_offset = (X @ beta) + (y - mu) / mu\n\n        # Perform the weighted least squares update\n        # beta_new = (X^T W X)^-1 X^T W z\n        X_T_W = X.T * weights\n        X_T_W_X = X_T_W @ X\n        X_T_W_z = X_T_W @ z_no_offset\n        \n        try:\n            beta = np.linalg.solve(X_T_W_X, X_T_W_z)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudo-inverse for singular matrices, though not expected here\n            beta = np.linalg.pinv(X_T_W_X) @ X_T_W_z\n            \n        # Check for convergence\n        if np.sum(np.abs(beta - beta_old)) < tol:\n            break\n            \n    # Calculate the maximized log-likelihood\n    final_eta = X @ beta + offset\n    final_mu = np.exp(final_eta)\n    \n    # Handle y_i = 0 cases where log(mu) might be multiplied by zero.\n    # The term y_i * log(mu_i) is zero if y_i is zero.\n    # We use np.where to avoid 0 * log(0) -> nan issues if mu is very small.\n    log_likelihood = np.sum(np.where(y > 0, y * np.log(final_mu), 0) - final_mu)\n\n    return beta, log_likelihood\n\ndef solve():\n    \"\"\"\n    Main function to process datasets, fit models, and compute p-values.\n    \"\"\"\n    test_cases = [\n        # Dataset A (nonlinear effect present, heterogeneous opportunity)\n        {\n            \"y\": np.array([1, 1, 2, 5, 15, 6, 3, 2, 3, 11, 6, 4]),\n            \"x1\": np.array([0, 5, 10, 15, 20, 25, 30, 35, 40, 10, 20, 30]),\n            \"x2\": np.array([0, 1, 2, 3, 4, 3, 2, 1, 0, 4, 3, 2]),\n            \"o\": np.array([2.2e7, 2.0e7, 1.8e7, 2.5e7, 2.7e7, 2.4e7, 2.1e7, 1.9e7, 2.3e7, 2.6e7, 2.8e7, 2.2e7])\n        },\n        # Dataset B (approximately linear effect across exposures)\n        {\n            \"y\": np.array([0, 1, 1, 2, 4, 1, 2, 3, 7, 1, 2, 2]),\n            \"x1\": np.array([0, 5, 10, 15, 20, 10, 20, 30, 40, 5, 15, 25]),\n            \"x2\": np.array([0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 2, 1]),\n            \"o\": np.array([1.8e7, 2.0e7, 2.2e7, 2.4e7, 2.6e7, 2.1e7, 2.3e7, 2.5e7, 2.7e7, 1.9e7, 2.2e7, 2.4e7])\n        },\n        # Dataset C (low counts with many zeros; test numerical stability)\n        {\n            \"y\": np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2]),\n            \"x1\": np.array([0, 0, 5, 5, 10, 10, 0, 5, 10, 0, 5, 10]),\n            \"x2\": np.array([0, 1, 0, 1, 0, 1, 2, 2, 2, 3, 3, 3]),\n            \"o\": np.array([2.0e7, 2.0e7, 2.0e7, 2.0e7, 2.0e7, 2.0e7, 2.0e7, 2.0e7, 2.0e7, 2.0e7, 2.0e7, 2.0e7])\n        },\n        # Dataset D (strong curvature with wide opportunity range)\n        {\n            \"y\": np.array([0, 0, 1, 2, 12, 0, 0, 1, 4, 0, 1, 3]),\n            \"x1\": np.array([0, 10, 20, 30, 40, 15, 25, 35, 5, 12, 22, 32]),\n            \"x2\": np.array([0, 2, 4, 6, 8, 1, 3, 5, 7, 2, 4, 6]),\n            \"o\": np.array([1.0e7, 5.0e7, 1.0e8, 2.0e8, 5.0e8, 2.0e7, 8.0e7, 1.5e8, 3.0e8, 4.0e7, 1.2e8, 2.5e8])\n        }\n    ]\n\n    p_values = []\n    \n    for case in test_cases:\n        y, x1, x2, o = case[\"y\"], case[\"x1\"], case[\"x2\"], case[\"o\"]\n        n = len(y)\n        offset = np.log(o)\n\n        # Null Model (M0): intercept + x1 + x2\n        X0 = np.c_[np.ones(n), x1, x2]\n        _, ll_null = fit_poisson_glm(y, X0, offset)\n\n        # Alternative Model (M1): intercept + x1 + x2 + x2^2\n        X1 = np.c_[np.ones(n), x1, x2, x2**2]\n        _, ll_alt = fit_poisson_glm(y, X1, offset)\n\n        # Likelihood Ratio Test\n        # Test statistic D = 2 * (log-likelihood(M1) - log-likelihood(M0))\n        lrt_stat = 2 * (ll_alt - ll_null)\n        # Numerical precision can sometimes make this slightly negative.\n        lrt_stat = max(0, lrt_stat)\n        \n        # Degrees of freedom = difference in number of parameters = 4 - 3 = 1\n        df = X1.shape[1] - X0.shape[1]\n        \n        # p-value from chi-square distribution\n        p_value = chi2.sf(lrt_stat, df)\n        \n        p_values.append(p_value)\n\n    # Format output as required: list of p-values rounded to 6 decimal places.\n    formatted_p_values = [f\"{p:.6f}\" for p in p_values]\n    print(f\"[{','.join(formatted_p_values)}]\")\n\nsolve()\n```", "id": "4384020"}]}