{"hands_on_practices": [{"introduction": "A foundational step in analyzing RNA-sequencing data is normalization, which corrects for technical variations in sequencing depth between samples. This practice [@problem_id:4333076] guides you through the calculation of size factors using the robust median-of-ratios method, a cornerstone of tools like DESeq2. By working through a carefully constructed example, you will see how these size factors are derived and then incorporated as offsets into a Generalized Linear Model (GLM), ensuring that downstream statistical tests for differential expression are valid and comparable across samples.", "problem": "A cohort of $4$ tumor samples has been profiled by high-throughput sequencing, yielding integer read counts for $5$ genes. Samples $\\{1,2\\}$ belong to condition $\\mathrm{A}$ and samples $\\{3,4\\}$ belong to condition $\\mathrm{B}$. The observed counts $K_{ij}$ (gene $j$ in sample $i$) are:\n\n- Gene $G_{1}$: $(75, 110, 100, 140)$\n- Gene $G_{2}$: $(225, 330, 300, 420)$\n- Gene $G_{3}$: $(38, 55, 50, 70)$\n- Gene $G_{4}$: $(150, 220, 400, 560)$\n- Gene $G_{5}$: $(90, 132, 120, 168)$\n\nAssume standard median-of-ratios normalization is appropriate for these data. Using only genes that have strictly positive counts in all samples, compute sample-specific size factors $s_{i}$ by the geometric mean method: for each gene $j$, compute its across-sample geometric mean $g_{j}$; for each sample $i$, compute the ratios $r_{ij} = K_{ij} / g_{j}$ across the eligible genes; set $s_{i}$ to the sample-specific median of these ratios. You may leave the $s_{i}$ defined up to a common multiplicative constant; do not rescale them.\n\nThen, using these $s_{i}$ as offsets in a Generalized Linear Model (GLM) with Poisson likelihood and log link for gene $G_{4}$, fit the model\n$$\n\\ln \\mu_{i} \\;=\\; \\ln s_{i} \\;+\\; \\beta_{0} \\;+\\; \\beta_{1}\\, x_{i},\n$$\nwhere $\\mu_{i}$ is the expected count for sample $i$, $x_{i} = 0$ for condition $\\mathrm{A}$ and $x_{i} = 1$ for condition $\\mathrm{B}$, and $\\beta_{1}$ encodes the condition effect on gene $G_{4}$.\n\nStarting only from core definitions of the geometric mean and the Poisson GLM with log link, derive from first principles the maximum likelihood estimator for $\\beta_{1}$ in closed form and simplify it fully using the provided counts. Express your final answer as a single exact analytic expression with no numerical rounding. Do not include units.", "solution": "We begin from the definitions. For each gene $j$ that has strictly positive counts in all samples, define its geometric mean across the $n=4$ samples as\n$$\ng_{j} \\;=\\; \\left(\\prod_{i=1}^{4} K_{ij}\\right)^{1/4}.\n$$\nFor each sample $i$, define the ratio $r_{ij} = K_{ij}/g_{j}$ and set the size factor\n$$\ns_{i} \\;=\\; \\operatorname{median}_{j\\in\\mathcal{J}} \\left( \\frac{K_{ij}}{g_{j}} \\right),\n$$\nwhere $\\mathcal{J}$ indexes the eligible genes (strictly positive counts across all samples). In these data, all $5$ genes have strictly positive counts in all samples, so $\\mathcal{J} = \\{1,2,3,4,5\\}$.\n\nWe note an empirical structure in the given counts. For the four genes $G_{1}, G_{2}, G_{3}, G_{5}$, the sample-wise patterns are proportional across genes:\n- The ratios across samples for $G_{1}$ are $(75:110:100:140)$, which reduce to $(15:22:20:28)$.\n- For $G_{2}$, $(225:330:300:420)$ also reduce to $(15:22:20:28)$.\n- For $G_{3}$, $(38:55:50:70)$ also reduce to $(15:22:20:28)$.\n- For $G_{5}$, $(90:132:120:168)$ also reduce to $(15:22:20:28)$.\n\nThis proportionality indicates that, for $j \\in \\{1,2,3,5\\}$, the counts can be written as\n$$\nK_{ij} \\;=\\; s_{i}\\, m_{j},\n$$\nfor some sample-specific factor $s_{i}$ (reflecting library size or sequencing depth) and gene-specific baseline $m_{j}$ that do not depend on condition. Although $s_{i}$ and $m_{j}$ are not known a priori, their existence is guaranteed by the exact proportionality across these genes.\n\nFor any such gene $j \\in \\{1,2,3,5\\}$, the across-sample geometric mean is\n$$\ng_{j} \\;=\\; \\left( \\prod_{i=1}^{4} s_{i}\\, m_{j} \\right)^{1/4} \\;=\\; m_{j} \\left( \\prod_{i=1}^{4} s_{i} \\right)^{1/4}.\n$$\nTherefore, for $j \\in \\{1,2,3,5\\}$,\n$$\n\\frac{K_{ij}}{g_{j}} \\;=\\; \\frac{s_{i}\\, m_{j}}{ m_{j}\\, \\left( \\prod_{i'=1}^{4} s_{i'} \\right)^{1/4} } \\;=\\; \\frac{s_{i}}{ \\left( \\prod_{i'=1}^{4} s_{i'} \\right)^{1/4} }.\n$$\nThe right-hand side no longer depends on the gene index $j$, so for each sample $i$ the four ratios contributed by $G_{1}, G_{2}, G_{3}, G_{5}$ are identical and equal to $s_{i}/S_{\\mathrm{geo}}$, where we abbreviate\n$$\nS_{\\mathrm{geo}} \\;=\\; \\left( \\prod_{i=1}^{4} s_{i} \\right)^{1/4}.\n$$\n\nFor gene $G_{4}$, the counts differ across conditions, indicating a gene-specific condition effect. The observed tuple $(150, 220, 400, 560)$ preserves the same between-sample proportionality as above, multiplied by an additional factor of $2$ in condition $\\mathrm{B}$ relative to condition $\\mathrm{A}$. Concretely, we may write\n$$\nK_{i,4} \\;=\\; s_{i}\\, m_{4}\\, c_{i},\n$$\nwith $c_{i} = 1$ for $i \\in \\{1,2\\}$ (condition $\\mathrm{A}$) and $c_{i} = 2$ for $i \\in \\{3,4\\}$ (condition $\\mathrm{B}$). Its geometric mean is\n$$\ng_{4} \\;=\\; \\left( \\prod_{i=1}^{4} s_{i}\\, m_{4}\\, c_{i} \\right)^{1/4} \\;=\\; m_{4}\\, \\left( \\prod_{i=1}^{4} s_{i} \\right)^{1/4} \\left( \\prod_{i=1}^{4} c_{i} \\right)^{1/4} \\;=\\; m_{4}\\, S_{\\mathrm{geo}} \\, \\left(1\\cdot 1\\cdot 2\\cdot 2\\right)^{1/4} \\;=\\; m_{4}\\, S_{\\mathrm{geo}}\\, \\sqrt{2}.\n$$\nThus, for $G_{4}$,\n$$\n\\frac{K_{i,4}}{g_{4}} \\;=\\; \\frac{s_{i}\\, m_{4}\\, c_{i}}{ m_{4}\\, S_{\\mathrm{geo}}\\, \\sqrt{2} } \\;=\\; \\frac{s_{i}}{S_{\\mathrm{geo}}}\\, \\frac{c_{i}}{\\sqrt{2}}.\n$$\n\nAssembling the five ratios for each sample $i$:\n- Four of them (from $G_{1}, G_{2}, G_{3}, G_{5}$) are exactly $\\dfrac{s_{i}}{S_{\\mathrm{geo}}}$.\n- The fifth (from $G_{4}$) is $\\dfrac{s_{i}}{S_{\\mathrm{geo}}}\\, \\dfrac{1}{\\sqrt{2}}$ for $i \\in \\{1,2\\}$ and $\\dfrac{s_{i}}{S_{\\mathrm{geo}}}\\, \\sqrt{2}$ for $i \\in \\{3,4\\}$.\n\nTherefore, for each $i$, the median across the five values is $\\dfrac{s_{i}}{S_{\\mathrm{geo}}}$, because the four identical values dominate the order statistics and the fifth value is either strictly smaller by a factor $\\dfrac{1}{\\sqrt{2}}$ or strictly larger by a factor $\\sqrt{2}$. Hence the computed size factors are\n$$\ns_{i}^{\\ast} \\;=\\; \\frac{s_{i}}{S_{\\mathrm{geo}}}, \\quad i \\in \\{1,2,3,4\\}.\n$$\nThese $s_{i}^{\\ast}$ are defined up to the common multiplicative constant $S_{\\mathrm{geo}}^{-1}$, which is immaterial in downstream modeling because it is absorbed by the intercept.\n\nWe now fit a Poisson Generalized Linear Model (GLM) for gene $G_{4}$ with log link and offsets $\\ln s_{i}^{\\ast}$. The model is\n$$\nK_{i,4} \\sim \\mathrm{Poisson}(\\mu_{i}), \\qquad \\ln \\mu_{i} \\;=\\; \\ln s_{i}^{\\ast} \\;+\\; \\beta_{0} \\;+\\; \\beta_{1}\\, x_{i},\n$$\nwhere $x_{i} = 0$ for $i \\in \\{1,2\\}$ and $x_{i} = 1$ for $i \\in \\{3,4\\}$.\n\nLet $e_{i} = s_{i}^{\\ast}$ denote the offset exposures. Define $\\theta_{\\mathrm{A}} = \\exp(\\beta_{0})$ and $\\theta_{\\mathrm{B}} = \\exp(\\beta_{0}+\\beta_{1})$. Then the mean for sample $i$ can be written as\n$$\n\\mu_{i} \\;=\\; \\begin{cases}\ne_{i}\\, \\theta_{\\mathrm{A}},  i \\in \\{1,2\\} \\\\\ne_{i}\\, \\theta_{\\mathrm{B}},  i \\in \\{3,4\\}.\n\\end{cases}\n$$\nThe log-likelihood, up to an additive constant $\\sum_{i} -\\ln(K_{i,4}!)$ that does not depend on the parameters, is\n$$\n\\ell(\\theta_{\\mathrm{A}}, \\theta_{\\mathrm{B}}) \\;=\\; \\sum_{i \\in \\{1,2\\}} \\left[ K_{i,4}\\, \\ln(e_{i}\\, \\theta_{\\mathrm{A}}) - e_{i}\\, \\theta_{\\mathrm{A}} \\right] \\;+\\; \\sum_{i \\in \\{3,4\\}} \\left[ K_{i,4}\\, \\ln(e_{i}\\, \\theta_{\\mathrm{B}}) - e_{i}\\, \\theta_{\\mathrm{B}} \\right].\n$$\nMaximizing with respect to $\\theta_{\\mathrm{A}}$ and $\\theta_{\\mathrm{B}}$ separately by setting derivatives to zero yields the standard Poisson exposure-weighted means:\n$$\n\\frac{\\partial \\ell}{\\partial \\theta_{\\mathrm{A}}} \\;=\\; \\sum_{i \\in \\{1,2\\}} \\left( \\frac{K_{i,4}}{\\theta_{\\mathrm{A}}} - e_{i} \\right) \\;=\\; 0 \\;\\;\\Rightarrow\\;\\; \\widehat{\\theta}_{\\mathrm{A}} \\;=\\; \\frac{\\sum_{i \\in \\{1,2\\}} K_{i,4}}{\\sum_{i \\in \\{1,2\\}} e_{i}},\n$$\n$$\n\\frac{\\partial \\ell}{\\partial \\theta_{\\mathrm{B}}} \\;=\\; \\sum_{i \\in \\{3,4\\}} \\left( \\frac{K_{i,4}}{\\theta_{\\mathrm{B}}} - e_{i} \\right) \\;=\\; 0 \\;\\;\\Rightarrow\\;\\; \\widehat{\\theta}_{\\mathrm{B}} \\;=\\; \\frac{\\sum_{i \\in \\{3,4\\}} K_{i,4}}{\\sum_{i \\in \\{3,4\\}} e_{i}}.\n$$\nThus the maximum likelihood estimator of the condition coefficient is\n$$\n\\widehat{\\beta}_{1} \\;=\\; \\ln \\widehat{\\theta}_{\\mathrm{B}} \\;-\\; \\ln \\widehat{\\theta}_{\\mathrm{A}} \\;=\\; \\ln\\!\\left( \\frac{\\sum_{i \\in \\{3,4\\}} K_{i,4}}{\\sum_{i \\in \\{3,4\\}} e_{i}} \\right) \\;-\\; \\ln\\!\\left( \\frac{\\sum_{i \\in \\{1,2\\}} K_{i,4}}{\\sum_{i \\in \\{1,2\\}} e_{i}} \\right).\n$$\n\nWe now evaluate these sums using the provided counts and the structure of $e_{i} = s_{i}^{\\ast} = s_{i}/S_{\\mathrm{geo}}$. First compute the count sums for $G_{4}$:\n$$\n\\sum_{i \\in \\{1,2\\}} K_{i,4} \\;=\\; 150 \\;+\\; 220 \\;=\\; 370, \\qquad \\sum_{i \\in \\{3,4\\}} K_{i,4} \\;=\\; 400 \\;+\\; 560 \\;=\\; 960.\n$$\nNext, compute the offset sums. Because $s_{i}^{\\ast}$ are proportional to the underlying $s_{i}$ with common factor $S_{\\mathrm{geo}}^{-1}$, their groupwise sums are\n$$\n\\sum_{i \\in \\{1,2\\}} e_{i} \\;=\\; \\frac{s_{1} + s_{2}}{S_{\\mathrm{geo}}}, \\qquad \\sum_{i \\in \\{3,4\\}} e_{i} \\;=\\; \\frac{s_{3} + s_{4}}{S_{\\mathrm{geo}}}.\n$$\nThe common factor $S_{\\mathrm{geo}}^{-1}$ cancels in $\\widehat{\\beta}_{1}$. Indeed,\n$$\n\\widehat{\\beta}_{1} \\;=\\; \\ln\\!\\left( \\frac{960}{(s_{3} + s_{4})/S_{\\mathrm{geo}}} \\right) \\;-\\; \\ln\\!\\left( \\frac{370}{(s_{1} + s_{2})/S_{\\mathrm{geo}}} \\right)\n\\;=\\; \\ln(960) \\;-\\; \\ln(370) \\;+\\; \\ln\\!\\left( \\frac{s_{1} + s_{2}}{s_{3} + s_{4}} \\right).\n$$\nWe determine $\\dfrac{s_{1} + s_{2}}{s_{3} + s_{4}}$ directly from the non-differentially expressed genes $G_{1}, G_{2}, G_{3}, G_{5}$. For any such gene $j$, the expected counts obey $K_{ij} \\propto s_{i}$, so the ratio of groupwise sums estimates the ratio $(s_{1} + s_{2})/(s_{3} + s_{4})$ without dependence on $m_{j}$. Using $G_{1}$ for concreteness,\n$$\n\\frac{s_{1} + s_{2}}{s_{3} + s_{4}} \\;=\\; \\frac{K_{1,1} + K_{2,1}}{K_{3,1} + K_{4,1}} \\;=\\; \\frac{75 + 110}{100 + 140} \\;=\\; \\frac{185}{240} \\;=\\; \\frac{37}{48}.\n$$\nThis same ratio is obtained from $G_{2}$, $G_{3}$, and $G_{5}$ because their between-sample proportionality is identical. Therefore,\n$$\n\\widehat{\\beta}_{1} \\;=\\; \\ln(960) \\;-\\; \\ln(370) \\;+\\; \\ln\\!\\left( \\frac{37}{48} \\right)\n\\;=\\; \\ln\\!\\left( \\frac{960}{370} \\cdot \\frac{37}{48} \\right)\n\\;=\\; \\ln\\!\\left( \\frac{960 \\cdot 37}{370 \\cdot 48} \\right).\n$$\nWe simplify the fraction:\n$$\n\\frac{960}{48} \\;=\\; 20, \\qquad \\frac{37}{370} \\;=\\; \\frac{1}{10}.\n$$\nHence,\n$$\n\\frac{960 \\cdot 37}{370 \\cdot 48} \\;=\\; 20 \\cdot \\frac{1}{10} \\;=\\; 2,\n$$\nand therefore\n$$\n\\widehat{\\beta}_{1} \\;=\\; \\ln(2).\n$$\nThis is the exact closed-form maximum likelihood estimate of the condition coefficient for gene $G_{4}$ under the specified normalization and GLM.", "answer": "$$\\boxed{\\ln(2)}$$", "id": "4333076"}, {"introduction": "Building a correct statistical model is paramount for valid scientific inference. This simulation exercise [@problem_id:2385475] powerfully demonstrates the consequences of model misspecification, particularly the danger of omitting a strong confounding variable like a batch effect. You will generate data where there is no true biological difference between conditions but a strong technical effect exists, and observe how failing to include the batch term in the model leads to a massive inflation of false positives. This practice provides an unforgettable lesson on the importance of accounting for known sources of variation to prevent spurious discoveries.", "problem": "Write a complete, runnable program that simulates gene expression data under a linear model with a strong batch effect and evaluates the impact of omitting the batch covariate during differential gene expression analysis. Your program must implement the following scientifically grounded and widely accepted base principles without relying on any shortcut formulas provided in the problem statement.\n\nBase principles to use:\n- Gene expression on an appropriate transformed scale (for example, a logarithmic scale) can be modeled by a linear model with Gaussian noise. For gene index $g$ and sample index $i$, let $y_{g i}$ denote the expression. The model includes a gene-specific baseline term, a condition term, a batch term, and an independent noise term.\n- Ordinary Least Squares (OLS) under the Gaussian noise assumption yields unbiased estimators in correctly specified linear models and provides test statistics for hypotheses on model coefficients that follow the Student's $t$-distribution under the null.\n- Multiple hypothesis testing across many genes should be controlled using the False Discovery Rate (FDR). Use the Benjamini–Hochberg (BH) procedure at target level $q = 0.05$.\n\nSimulation design and hypothesis testing:\n- All genes share the same generative model structure and are simulated under the global null of no true differential expression between the two biological conditions. Specifically, for every gene $g$, the condition effect is $0$; the batch effect is shared across genes as a fixed shift between two batches; the baseline level may vary by gene; and the noise is independent and identically distributed Gaussian across samples within a gene, with constant variance.\n- The program must estimate, for each gene, the two-sided $p$-value for the null hypothesis that the condition coefficient is zero using:\n  1. A misspecified model that omits the batch covariate (condition-only analysis).\n  2. A correctly specified model that includes both condition and batch covariates (condition-plus-batch analysis).\n- Apply the Benjamini–Hochberg (BH) procedure at target level $q = 0.05$ across all genes in each analysis to obtain the number of discoveries (these are all false discoveries because the data are simulated under the global null).\n\nScientific rationale to be demonstrated:\n- When the condition and batch covariates are correlated (confounding) and the batch effect is strong, omitting the batch covariate induces bias in the estimated condition effect and inflates false discoveries, whereas including the batch covariate removes the bias and controls false discoveries near the target FDR level.\n- When the condition and batch covariates are orthogonal, omitting the batch covariate does not induce bias in the estimated condition effect; false discoveries should be close to the target FDR level even without adjustment.\n- As a boundary case, when the batch effect is zero, including or omitting the batch covariate should yield similar behavior.\n\nTest suite:\nSimulate $m = 2000$ genes in each case. Use independent baseline levels $\\mu_g \\sim \\mathcal{N}(0, 1)$ for $g = 1, \\dots, m$. For each case, generate independent Gaussian noise $\\varepsilon_{g i} \\sim \\mathcal{N}(0, \\sigma^2)$ with $\\sigma = 0.5$.\n\nEncode the binary condition covariate as $x_i \\in \\{0, 1\\}$ for the two conditions, and the binary batch covariate as $z_i \\in \\{0, 1\\}$ for the two batches. The batch shift is a constant $\\gamma$ added for $z_i = 1$ and $0$ for $z_i = 0$. The true condition effect is $0$ for all genes. Use the following three cases, each with a specified sample composition across condition and batch:\n- Case $1$ (high confounding, strong batch): batch $1$ has $19$ samples from condition $0$ and $1$ sample from condition $1$; batch $2$ has $1$ sample from condition $0$ and $19$ samples from condition $1$; $\\gamma = 1.5$; total samples $n = 40$.\n- Case $2$ (orthogonal design, strong batch): batch $1$ has $10$ samples from condition $0$ and $10$ samples from condition $1$; batch $2$ has $10$ samples from condition $0$ and $10$ samples from condition $1$; $\\gamma = 1.5$; total samples $n = 40$.\n- Case $3$ (high confounding, no batch): batch $1$ has $9$ samples from condition $0$ and $1$ sample from condition $1$; batch $2$ has $1$ sample from condition $0$ and $9$ samples from condition $1$; $\\gamma = 0.0$; total samples $n = 20$.\n\nRandomness and reproducibility:\n- Use a fixed random seed of $20240513$ to initialize the generator for Case $1$. For Cases $2$ and $3$, you must add the case index to the base seed (that is, $20240513 + 2$ and $20240513 + 3$) to ensure independence across cases while retaining reproducibility.\n\nStatistical analysis requirements:\n- For each gene and each case, compute the two-sided $p$-value testing the null hypothesis that the condition coefficient equals $0$ using:\n  1. A condition-only linear model with an intercept.\n  2. A condition-plus-batch linear model with an intercept.\n- Use the Benjamini–Hochberg (BH) procedure at level $q = 0.05$ on the set of $m$ $p$-values within each analysis to determine the number of discoveries (rejections). Since all nulls are true by construction, every discovery is a false positive. Report the numbers of discoveries for both analyses.\n\nRequired final output format:\n- Your program should produce a single line of output containing $6$ comma-separated integers enclosed in square brackets, in the following order: $[\\text{case1\\_naive}, \\text{case1\\_adjusted}, \\text{case2\\_naive}, \\text{case2\\_adjusted}, \\text{case3\\_naive}, \\text{case3\\_adjusted}]$, where “naive” denotes the condition-only analysis and “adjusted” denotes the condition-plus-batch analysis.\n\nNo external inputs:\n- The program must be entirely self-contained and must not read any input or files or access any network resources. All numerical values must be hard coded as specified above.\n\nAngle units and physical units:\n- No angles or physical units are involved.\n\nAnswer type:\n- Each of the $6$ reported values must be an integer.\n\nYour goal is to implement the simulation and analysis so that the output demonstrates, through these test cases, how omitting a strong, confounded batch effect inflates false discoveries in differential gene expression analysis, while including the batch indicator restores valid inference.", "solution": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation suite and print the final results.\n    \"\"\"\n\n    # Global parameters\n    m = 2000  # Number of genes\n    sigma = 0.5  # Noise standard deviation\n    q_level = 0.05  # Target FDR level for BH procedure\n\n    # Case 1: High confounding, strong batch\n    case1_params = {\n        'n_cond0_batch1': 19, 'n_cond1_batch1': 1,\n        'n_cond0_batch2': 1, 'n_cond1_batch2': 19\n    }\n    case1_gamma = 1.5\n    case1_seed = 20240513\n    \n    # Case 2: Orthogonal design, strong batch\n    case2_params = {\n        'n_cond0_batch1': 10, 'n_cond1_batch1': 10,\n        'n_cond0_batch2': 10, 'n_cond1_batch2': 10\n    }\n    case2_gamma = 1.5\n    case2_seed = 20240513 + 2\n\n    # Case 3: High confounding, no batch effect\n    case3_params = {\n        'n_cond0_batch1': 9, 'n_cond1_batch1': 1,\n        'n_cond0_batch2': 1, 'n_cond1_batch2': 9\n    }\n    case3_gamma = 0.0\n    case3_seed = 20240513 + 3\n    \n    test_cases = [\n        (case1_params, case1_gamma, m, sigma, q_level, case1_seed),\n        (case2_params, case2_gamma, m, sigma, q_level, case2_seed),\n        (case3_params, case3_gamma, m, sigma, q_level, case3_seed),\n    ]\n\n    results = []\n    for params in test_cases:\n        naive_discoveries, adjusted_discoveries = run_simulation(*params)\n        results.extend([naive_discoveries, adjusted_discoveries])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(case_params, gamma, m, sigma, q, seed):\n    \"\"\"\n    Runs a single simulation case.\n    \n    Generates data, performs naive and adjusted analyses, and returns the number of discoveries for each.\n    \"\"\"\n    np.random.seed(seed)\n\n    # 1. Construct design vectors (x for condition, z for batch)\n    n01 = case_params['n_cond0_batch1']\n    n11 = case_params['n_cond1_batch1']\n    n02 = case_params['n_cond0_batch2']\n    n12 = case_params['n_cond1_batch2']\n    \n    n_batch1 = n01 + n11\n    n_batch2 = n02 + n12\n    n = n_batch1 + n_batch2\n\n    x = np.array([0]*n01 + [1]*n11 + [0]*n02 + [1]*n12)\n    z = np.array([0]*n_batch1 + [1]*n_batch2)\n\n    # 2. Generate gene expression data\n    # True model: y = mu + gamma*z + noise\n    mu_g = np.random.normal(0, 1, size=(m, 1))\n    noise = np.random.normal(0, sigma, size=(m, n))\n    Y = mu_g + gamma * z[np.newaxis, :] + noise\n\n    # 3. Define design matrices\n    X_naive = np.vstack([np.ones(n), x]).T\n    X_adjusted = np.vstack([np.ones(n), x, z]).T\n\n    # 4. Perform analyses and get discovery counts\n    naive_discoveries = perform_analysis(Y, X_naive, q)\n    adjusted_discoveries = perform_analysis(Y, X_adjusted, q)\n\n    return naive_discoveries, adjusted_discoveries\n\ndef perform_analysis(Y, X, q):\n    \"\"\"\n    Performs OLS regression and multiple testing correction for a set of genes.\n    \"\"\"\n    n, p = X.shape # n = samples, p = parameters\n    m = Y.shape[0] # m = genes\n    \n    # 1. Fit linear model for all genes at once using np.linalg.lstsq\n    # Y is (m, n), X is (n, p). We need to solve X @ B.T = Y.T for B.\n    # B will be (m, p). lstsq returns coefficients as (p, m).\n    beta_hat, rss_per_gene, _, _ = np.linalg.lstsq(X, Y.T, rcond=None)\n\n    # 2. Calculate t-statistics for the condition coefficient (at index 1)\n    df = n - p\n    sigma_sq_hat = rss_per_gene / df\n    \n    # The variance of beta_hat is diag(inv(X'X)) * sigma_hat^2\n    # We are interested in the coefficient for the condition 'x', which is at index 1\n    C = np.linalg.inv(X.T @ X)\n    se_beta1 = np.sqrt(sigma_sq_hat * C[1, 1])\n\n    # Avoid division by zero if standard error is somehow zero\n    # This should not happen in this problem's setup\n    t_stats = np.zeros(m)\n    valid_se = se_beta1 > 0\n    t_stats[valid_se] = beta_hat[1, valid_se] / se_beta1[valid_se]\n    \n    # 3. Calculate two-sided p-values\n    p_values = 2 * t.sf(np.abs(t_stats), df=df)\n\n    # 4. Apply Benjamini-Hochberg procedure\n    num_discoveries = bh_procedure(p_values, q)\n    \n    return num_discoveries\n\ndef bh_procedure(p_values, q):\n    \"\"\"\n    Applies the Benjamini-Hochberg procedure to control FDR.\n    \"\"\"\n    m = len(p_values)\n    if m == 0:\n        return 0\n        \n    p_values_sorted = np.sort(p_values)\n    ranks = np.arange(1, m + 1)\n    thresholds = (ranks / m) * q\n    \n    # Find all p-values that are below the BH threshold\n    significant_mask = p_values_sorted = thresholds\n    \n    if np.any(significant_mask):\n        # The number of discoveries is the rank of the last p-value\n        # that is below its threshold\n        k = np.max(np.where(significant_mask))\n        num_discoveries = k + 1\n    else:\n        num_discoveries = 0\n        \n    return num_discoveries\n\nif __name__ == '__main__':\n    solve()\n```", "answer": "[1448,89,101,98,99,100]", "id": "2385475"}, {"introduction": "Differential expression analysis involves performing thousands of statistical tests simultaneously, one for each gene. This large number of tests creates a multiple hypothesis testing problem, where the chance of making a false discovery skyrockets if uncorrected. This exercise [@problem_id:4333017] provides essential hands-on practice with the Benjamini-Hochberg (BH) procedure, the standard method for controlling the False Discovery Rate (FDR). By manually calculating the adjusted p-values (or q-values), you will master the mechanics of this critical final step in generating a reliable and statistically rigorous list of differentially expressed genes.", "problem": "In a targeted pharmacogenomic assay used for precision oncology stratification, a subset of $12$ prespecified genes is assessed for differential expression between a therapy-responsive group and a non-responsive group using a negative binomial generalized linear model. The gene-level hypothesis tests produce a vector of $p$-values $\\mathbf{p} \\in [0,1]^{12}$, where smaller $p$-values provide stronger evidence against the null hypothesis of no differential expression. In this context, the goal is to control the False Discovery Rate (FDR), defined as the expected proportion of false positives among the set of rejected null hypotheses, using the Benjamini-Hochberg (BH) procedure. The Benjamini-Hochberg (BH) procedure provides adjusted values often called $q$-values, which are monotonically non-decreasing with respect to the ordered $p$-values and are interpreted as the minimal FDR level at which each gene would be called significant.\n\nAssume the gene-level test statistics are independent or exhibit positive regression dependence, a condition under which the BH procedure is known to control FDR at the target level. The $p$-value vector, indexed by genes $G_{1},G_{2},\\dots,G_{12}$ in the given order, is\n$$\n\\mathbf{p} = \\big(0.0008,\\;0.013,\\;0.22,\\;0.051,\\;0.004,\\;0.17,\\;0.089,\\;0.0003,\\;0.74,\\;0.032,\\;0.0065,\\;0.41\\big).\n$$\nUsing the Benjamini-Hochberg (BH) procedure, compute the BH-adjusted $q$-values for all $12$ genes and determine which genes are significant at an FDR level $q=0.05$. Round each BH-adjusted $q$-value to four significant figures. Report the indices of the significant genes in ascending order as your final answer.", "solution": "The problem is scientifically grounded, well-posed, and objective. It provides a standard set of $p$-values from a realistic pharmacogenomic experiment and asks for the application of the Benjamini-Hochberg (BH) procedure, a cornerstone of multiple hypothesis testing in genomics. All necessary data and conditions are provided, and the task is unambiguous. The problem is valid.\n\nThe Benjamini-Hochberg (BH) procedure is used to control the False Discovery Rate (FDR) when performing multiple simultaneous hypothesis tests. The procedure computes an adjusted $p$-value, often called a $q$-value, for each hypothesis.\n\nThe total number of hypotheses (genes) is $m=12$. The given vector of $p$-values is:\n$$\n\\mathbf{p} = \\big(0.0008,\\;0.013,\\;0.22,\\;0.051,\\;0.004,\\;0.17,\\;0.089,\\;0.0003,\\;0.74,\\;0.032,\\;0.0065,\\;0.41\\big)\n$$\nThe corresponding gene indices are $G_{1}, G_{2}, \\dots, G_{12}$.\n\nThe first step is to sort the $p$-values in ascending order. Let $p_{(i)}$ be the $i$-th smallest $p$-value. We must keep track of the original index of each $p$-value.\nThe sorted $p$-values are:\n- $p_{(1)} = 0.0003$ (from $G_8$)\n- $p_{(2)} = 0.0008$ (from $G_1$)\n- $p_{(3)} = 0.004$ (from $G_5$)\n- $p_{(4)} = 0.0065$ (from $G_{11}$)\n- $p_{(5)} = 0.013$ (from $G_2$)\n- $p_{(6)} = 0.032$ (from $G_{10}$)\n- $p_{(7)} = 0.051$ (from $G_4$)\n- $p_{(8)} = 0.089$ (from $G_7$)\n- $p_{(9)} = 0.17$ (from $G_6$)\n- $p_{(10)} = 0.22$ (from $G_3$)\n- $p_{(11)} = 0.41$ (from $G_{12}$)\n- $p_{(12)} = 0.74$ (from $G_9$)\n\nThe BH-adjusted $q$-value for the $i$-th ordered $p$-value, $p_{(i)}$, is given by the formula:\n$$\nq_{(i)} = \\min_{k=i}^{m} \\left( \\frac{m \\cdot p_{(k)}}{k} \\right)\n$$\nThis formula ensures that the sequence of $q$-values, $q_{(1)}, q_{(2)}, \\dots, q_{(m)}$, is monotonically non-decreasing.\nA practical way to compute this is to first calculate the value $q'_{i} = \\frac{m \\cdot p_{(i)}}{i}$ for each rank $i=1, \\dots, m$. Then, to enforce monotonicity, compute the final $q$-values by taking the cumulative minimum from the end of the list:\n$q_{(m)} = q'_{m}$, and for $i = m-1, \\dots, 1$, $q_{(i)} = \\min(q'_{i}, q_{(i+1)})$.\n\nWith $m=12$, we calculate the $q'_{i}$ values:\n- $q'_{1} = \\frac{12 \\times 0.0003}{1} = 0.0036$\n- $q'_{2} = \\frac{12 \\times 0.0008}{2} = 0.0048$\n- $q'_{3} = \\frac{12 \\times 0.004}{3} = 0.016$\n- $q'_{4} = \\frac{12 \\times 0.0065}{4} = 0.0195$\n- $q'_{5} = \\frac{12 \\times 0.013}{5} = 0.0312$\n- $q'_{6} = \\frac{12 \\times 0.032}{6} = 0.064$\n- $q'_{7} = \\frac{12 \\times 0.051}{7} \\approx 0.08742857$\n- $q'_{8} = \\frac{12 \\times 0.089}{8} = 0.1335$\n- $q'_{9} = \\frac{12 \\times 0.17}{9} \\approx 0.22666667$\n- $q'_{10} = \\frac{12 \\times 0.22}{10} = 0.264$\n- $q'_{11} = \\frac{12 \\times 0.41}{11} \\approx 0.44727273$\n- $q'_{12} = \\frac{12 \\times 0.74}{12} = 0.74$\n\nNow we enforce the monotonicity condition:\n- $q_{(12)} = q'_{12} = 0.74$\n- $q_{(11)} = \\min(q'_{11}, q_{(12)}) = \\min(0.44727..., 0.74) = 0.44727...$\n- $q_{(10)} = \\min(q'_{10}, q_{(11)}) = \\min(0.264, 0.44727...) = 0.264$\n- $q_{(9)} = \\min(q'_{9}, q_{(10)}) = \\min(0.22666..., 0.264) = 0.22666...$\n- $q_{(8)} = \\min(q'_{8}, q_{(9)}) = \\min(0.1335, 0.22666...) = 0.1335$\n- $q_{(7)} = \\min(q'_{7}, q_{(8)}) = \\min(0.08742..., 0.1335) = 0.08742...$\n- $q_{(6)} = \\min(q'_{6}, q_{(7)}) = \\min(0.064, 0.08742...) = 0.064$\n- $q_{(5)} = \\min(q'_{5}, q_{(6)}) = \\min(0.0312, 0.064) = 0.0312$\n- $q_{(4)} = \\min(q'_{4}, q_{(5)}) = \\min(0.0195, 0.0312) = 0.0195$\n- $q_{(3)} = \\min(q'_{3}, q_{(4)}) = \\min(0.016, 0.0195) = 0.016$\n- $q_{(2)} = \\min(q'_{2}, q_{(3)}) = \\min(0.0048, 0.016) = 0.0048$\n- $q_{(1)} = \\min(q'_{1}, q_{(2)}) = \\min(0.0036, 0.0048) = 0.0036$\n\nThe resulting $q$-values, mapped back to the original gene indices and rounded to four significant figures, are:\n- $G_1$ (rank 2): $q = 0.004800$\n- $G_2$ (rank 5): $q = 0.03120$\n- $G_3$ (rank 10): $q = 0.2640$\n- $G_4$ (rank 7): $q = 0.08743$\n- $G_5$ (rank 3): $q = 0.01600$\n- $G_6$ (rank 9): $q = 0.2267$\n- $G_7$ (rank 8): $q = 0.1335$\n- $G_8$ (rank 1): $q = 0.003600$\n- $G_9$ (rank 12): $q = 0.7400$\n- $G_{10}$ (rank 6): $q = 0.06400$\n- $G_{11}$ (rank 4): $q = 0.01950$\n- $G_{12}$ (rank 11): $q = 0.4473$\n\nA gene is considered significant if its $q$-value is less than or equal to the specified FDR level, $q \\le 0.05$. We check this condition for each gene:\n- $G_1$: $0.004800 \\le 0.05$ (Significant)\n- $G_2$: $0.03120 \\le 0.05$ (Significant)\n- $G_3$: $0.2640  0.05$ (Not significant)\n- $G_4$: $0.08743  0.05$ (Not significant)\n- $G_5$: $0.01600 \\le 0.05$ (Significant)\n- $G_6$: $0.2267  0.05$ (Not significant)\n- $G_7$: $0.1335  0.05$ (Not significant)\n- $G_8$: $0.003600 \\le 0.05$ (Significant)\n- $G_9$: $0.7400  0.05$ (Not significant)\n- $G_{10}$: $0.06400  0.05$ (Not significant)\n- $G_{11}$: $0.01950 \\le 0.05$ (Significant)\n- $G_{12}$: $0.4473  0.05$ (Not significant)\n\nThe indices of the significant genes are $1$, $2$, $5$, $8$, and $11$. The problem asks for these indices to be reported in ascending order.\nThe set of significant gene indices is $\\{1, 2, 5, 8, 11\\}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  2  5  8  11\n\\end{pmatrix}\n}\n$$", "id": "4333017"}]}