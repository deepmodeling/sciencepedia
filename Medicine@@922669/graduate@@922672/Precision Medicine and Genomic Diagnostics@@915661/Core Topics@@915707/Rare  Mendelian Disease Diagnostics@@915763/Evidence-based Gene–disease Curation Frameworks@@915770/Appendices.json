{"hands_on_practices": [{"introduction": "A cornerstone of establishing a gene–disease link is demonstrating that pathogenic variants in a gene are significantly more common in affected individuals (cases) than in the general population (controls). This exercise provides direct, hands-on practice with the statistical engine behind many such analyses: Fisher's exact test. By deriving the two-sided $p$-value from the underlying hypergeometric distribution, you will move beyond a \"black box\" understanding and grasp how to rigorously quantify the strength of evidence from a case-control study, a skill essential for interpreting genetic evidence within frameworks like ClinGen [@problem_id:4338123].", "problem": "A gene-level case-control burden analysis is conducted for a single gene using a standardized evidence-based curation workflow consistent with the Clinical Genome Resource (ClinGen) framework for gene–disease validity. A cohort of $500$ clinically well-phenotyped cases and $1500$ ancestry-matched controls is assembled. After rigorous quality control, $12$ cases and $3$ controls carry qualifying variants in the gene. Assume the margins (row totals and column totals) of the $2 \\times 2$ contingency table are fixed by study design, and adopt the null hypothesis that variant carriage is independent of disease status in this study population.\n\nStarting from the definition of Fisher’s exact test under fixed margins and the independence assumption, derive the exact probability mass function appropriate for the number of qualifying variant carriers in the case cohort. Use this to construct the two-sided exact $p$-value for the observed data by summing over all tables that are at least as extreme as the observed table in the sense of having probability less than or equal to the observed table under the null.\n\nLet the cell counts be: cases with qualifying variants $a = 12$, cases without qualifying variants $b = 488$, controls with qualifying variants $c = 3$, and controls without qualifying variants $d = 1497$. The fixed margins are $a + b = 500$, $c + d = 1500$, $a + c = 15$, and $b + d = 1985$, with total $n = 2000$.\n\nCompute the exact two-sided Fisher’s $p$-value as a closed-form analytic expression in terms of binomial coefficients. Then, briefly interpret the magnitude of this $p$-value for evidential usefulness in gene–disease curation within precision medicine and genomic diagnostics, referencing how such statistical significance is typically leveraged (for example, in the Clinical Genome Resource (ClinGen) framework). Your final reported $p$-value must be provided as a single closed-form analytic expression; no numerical rounding is required.", "solution": "The problem asks for the derivation and calculation of a two-sided Fisher's exact test $p$-value for a gene-level case-control burden analysis.\n\nFirst, we formalize the problem by constructing a $2 \\times 2$ contingency table with the given data. Let the cell counts be $a$, $b$, $c$, and $d$.\n- $a$: cases with qualifying variants\n- $b$: cases without qualifying variants\n- $c$: controls with qualifying variants\n- $d$: controls without qualifying variants\n\nThe data provided are: a cohort of $500$ cases and $1500$ controls. Among them, $12$ cases and $3$ controls carry qualifying variants. This gives the following observed table:\n- $a = 12$\n- $c = 3$\n- Total cases = $a+b = 500 \\implies b = 500 - 12 = 488$\n- Total controls = $c+d = 1500 \\implies d = 1500 - 3 = 1497$\n\nThe contingency table is:\n|            | Cases | Controls | Row Total |\n|------------|-------|----------|-----------|\n| Variant    |  $12$ |   $3$    |  $15$     |\n| No Variant | $488$ |  $1497$  |  $1985$   |\n| Col Total  | $500$ |  $1500$  |  $2000$   |\n\nThe problem specifies that the margins of the table are fixed. These fixed margins are:\n- Row 1 Total (Total with variant), $R_1 = a+c = 15$\n- Row 2 Total (Total without variant), $R_2 = b+d = 1985$\n- Column 1 Total (Total cases), $C_1 = a+b = 500$\n- Column 2 Total (Total controls), $C_2 = c+d = 1500$\n- Grand Total, $n = a+b+c+d = 2000$\n\nThe null hypothesis ($H_0$) is that variant carriage is independent of disease status. Under $H_0$ and with fixed margins, the probability of observing a particular set of cell counts ($a, b, c, d$) is given by the hypergeometric distribution. Let $X$ be the random variable representing the number of qualifying variant carriers in the case cohort (the value in cell $a$). The probability mass function (PMF) for $X$ is derived by considering the number of ways to choose $C_1=500$ individuals to be cases from the total population of $n=2000$. The number of ways to get exactly $x$ variant carriers among the cases is to choose $x$ individuals from the $R_1=15$ total variant carriers and $C_1-x=500-x$ individuals from the $R_2=1985$ total non-carriers.\n\nThe PMF for $X=x$ is therefore:\n$$P(X=x) = \\frac{\\binom{R_1}{x} \\binom{R_2}{C_1-x}}{\\binom{n}{C_1}}$$\nSubstituting the fixed margins from our problem, the specific PMF is:\n$$P(X=x) = \\frac{\\binom{15}{x}\\binom{1985}{500-x}}{\\binom{2000}{500}}$$\nThe possible values for $x$ are integers constrained by the arguments of the binomial coefficients, which must be non-negative. This implies $0 \\le x \\le 15$ and $0 \\le 500-x \\le 1985$, which simplifies to the support $x \\in \\{0, 1, \\dots, 15\\}$.\n\nThe observed value in the study is $a_{obs} = 12$. The probability of observing this specific table is:\n$$P_{obs} = P(X=12) = \\frac{\\binom{15}{12}\\binom{1985}{488}}{\\binom{2000}{500}}$$\n\nThe problem requires constructing the two-sided exact $p$-value by summing the probabilities of all tables that are as extreme as or more extreme than the observed one. \"More extreme\" is defined as having a probability less than or equal to the probability of the observed table. The $p$-value is thus:\n$$p = \\sum_{x \\,:\\, P(X=x) \\le P_{obs}} P(X=x)$$\nwhere the sum is over all possible values of $x$ in its support, $\\{0, 1, \\dots, 15\\}$.\n\nTo determine which values of $x$ satisfy this condition, we analyze the shape of the PMF. The hypergeometric distribution is unimodal. The mode occurs near the expected value $E[X] = \\frac{R_1 C_1}{n} = \\frac{15 \\times 500}{2000} = 3.75$. The distribution's peak will be at $x=4$. The probabilities $P(X=x)$ will increase for $x \\le 4$ and decrease for $x \\ge 4$.\n\nOur observed value is $a_{obs} = 12$, which is far into the upper tail of the distribution. Since the PMF is monotonically decreasing for $x \\ge 4$, all values $x > 12$ will have probabilities smaller than $P(X=12)$. Thus, the probabilities for $x=12, 13, 14, 15$ must be included in the sum for the $p$-value.\nWe must also check the lower tail (i.e., $x < 12$). Since the PMF is decreasing for $x \\ge 4$, we have $P(X=12) < P(X=11) < \\dots < P(X=4)$. What remains is to compare $P(X=12)$ with probabilities in the increasing part of the distribution, $P(X=0), P(X=1), P(X=2), P(X=3)$. Due to the significant skew of the observed values ($12$ vs. an expectation of $3.75$), the probabilities in the lower tail are substantially larger than those in the extreme upper tail. For instance, comparing $P(X=0)$ with $P(X=12)$:\n$$ \\frac{P(X=0)}{P(X=12)} = \\frac{\\binom{15}{0}\\binom{1985}{500}}{\\binom{15}{12}\\binom{1985}{488}} = \\frac{1}{\\binom{15}{3}} \\prod_{i=0}^{11} \\frac{1497-i}{500-i} = \\frac{1}{455} \\prod_{i=0}^{11} \\frac{1497-i}{500-i} $$\nEach term in the product is approximately $3$, so the product is a very large number, far exceeding $455$. Thus, $P(X=0) \\gg P(X=12)$. Since the PMF increases from $x=0$ to $x=4$, it is certain that $P(X=x) > P(X=12)$ for all $x \\le 11$.\nTherefore, the only values of $x$ for which $P(X=x) \\le P(X=12)$ are $x=12, 13, 14,$ and $15$.\n\nThe two-sided $p$-value simplifies to the sum of probabilities in the upper tail starting from the observed value:\n$$p = \\sum_{x=12}^{15} P(X=x) = P(X=12) + P(X=13) + P(X=14) + P(X=15)$$\nSubstituting the PMF expression, we obtain the final closed-form analytic expression:\n$$p = \\frac{\\binom{15}{12}\\binom{1985}{488}}{\\binom{2000}{500}} + \\frac{\\binom{15}{13}\\binom{1985}{487}}{\\binom{2000}{500}} + \\frac{\\binom{15}{14}\\binom{1985}{486}}{\\binom{2000}{500}} + \\frac{\\binom{15}{15}\\binom{1985}{485}}{\\binom{2000}{500}}$$\nThis can be written as a single fraction:\n$$p = \\frac{\\binom{15}{12}\\binom{1985}{488} + \\binom{15}{13}\\binom{1985}{487} + \\binom{15}{14}\\binom{1985}{486} + \\binom{15}{15}\\binom{1985}{485}}{\\binom{2000}{500}}$$\n\nFor interpretation, the magnitude of this $p$-value is very small. The observed frequency of variants in cases ($12/500 = 2.4\\%$) is twelve times higher than in controls ($3/1500 = 0.2\\%$), corresponding to a large odds ratio of approximately $12.2$. Such a large deviation from the null expectation ($E[X]=3.75$) results in a $p$-value on the order of $10^{-4}$. In the context of evidence-based gene–disease curation, such as the framework developed by the Clinical Genome Resource (ClinGen), this result constitutes strong statistical evidence. The ClinGen framework assesses evidence from multiple domains (genetic, experimental, etc.) to classify the strength of a gene–disease relationship. A well-designed case-control study showing a large effect size (odds ratio) and a highly significant $p$-value (e.g., $p < 0.001$) provides a substantial number of points toward establishing the gene–disease link as \"Moderate,\" \"Strong,\" or \"Definitive.\" This statistical evidence is critical for translating genomic findings into clinical practice, for example, by justifying the inclusion of the gene on diagnostic panels for precision medicine.", "answer": "$$\\boxed{\\frac{\\binom{15}{12}\\binom{1985}{488} + \\binom{15}{13}\\binom{1985}{487} + \\binom{15}{14}\\binom{1985}{486} + \\binom{15}{15}\\binom{1985}{485}}{\\binom{2000}{500}}}$$", "id": "4338123"}, {"introduction": "Gene curation is rarely based on a single piece of evidence; it is a process of synthesis. This practice simulates the core activity of a gene curator by asking you to integrate multiple, distinct lines of evidence into a single, cohesive score. You will learn to convert raw observations—such as the number of confirmed *de novo* mutations, segregation data from pedigrees expressed as a logarithm of the odds (LOD) score, and results from functional assays—into a standardized point system to determine a gene–disease classification [@problem_id:4338170]. This exercise highlights how frameworks systematically weigh and combine disparate data to produce a defensible clinical validity assessment.", "problem": "A gene $G$ is under evidence-based curation for association with an autosomal dominant neurodevelopmental disorder $D$ using the Clinical Genome Resource (ClinGen) Gene–Disease Clinical Validity Framework. For this problem, assume the following framework elements, each treated as well-tested rules in the curated assessment:\n\n- Genetic evidence is capped at $12$ points and is composed of case-level variant evidence and segregation evidence.\n- Case-level de novo variant evidence for autosomal dominant conditions contributes $2$ points for each confirmed de novo variant in an unrelated proband with a phenotype specific to $D$, and $1$ point for each de novo event that is plausible but not confirmed. All case-level variant evidence contributes toward the $12$-point cap for genetic evidence.\n- Segregation evidence is evaluated by aggregating the logarithm of the odds (LOD) across pedigrees. Under a fully penetrant autosomal dominant model with no recombinants observed, if there are $m$ informative meioses (transmissions where the variant status would be informative for co-segregation), the probability of perfect concordance by chance is $(0.5)^{m}$, and the likelihood ratio for co-segregation versus random assortment is $2^{m}$. The LOD is defined as $\\log_{10}(2^{m}) = m \\log_{10}(2)$. Across pedigrees, LODs are additive. Convert the combined LOD to segregation points by the following mapping: if $1 \\leq \\text{LOD} < 2$, assign $0.5$ points; if $2 \\leq \\text{LOD} < 3$, assign $1.0$ points; if $3 \\leq \\text{LOD} < 5$, assign $1.5$ points; if $\\text{LOD} \\geq 5$, assign $3.0$ points. Segregation points are capped at $3.0$ and contribute to the $12$-point genetic evidence cap.\n- Experimental evidence is capped at $6$ points. For this scenario, limited functional data is available: assign $0.5$ points for a patient-derived cellular assay showing consistent functional alteration and $0.5$ points for expression data showing relevant spatiotemporal expression consistent with $D$; together these contribute to the experimental evidence category.\n- Classification mapping based on the total score $S$ is: “Limited” for $1 \\leq S \\leq 6$, “Moderate” for $7 \\leq S \\leq 11$, and “Strong” for $12 \\leq S \\leq 18$. “Definitive” requires a “Strong” score plus replication of evidence over time; assume replication over time is not satisfied in this scenario.\n\nYou are given a mixed-evidence case for $G$–$D$:\n\n- There are $3$ unrelated probands, each with a confirmed de novo protein-truncating variant in $G$ and a phenotype highly specific to $D$.\n- There are $2$ additional unrelated probands with plausible but unconfirmed de novo missense variants in $G$, predicted damaging and consistent with $D$.\n- There are two pedigrees with an inherited variant in $G$ consistent with the proposed mechanism. In Family $1$, there are $m_{1} = 4$ informative meioses with perfect concordance (no recombinants). In Family $2$, there are $m_{2} = 2$ informative meioses with perfect concordance (no recombinants).\n- Functional data are limited to a single patient-derived cellular assay showing decreased activity of the gene product ($0.5$ points) and expression data showing that $G$ is expressed in the developing brain regions implicated in $D$ ($0.5$ points).\n\nAssume there is no convincing contradictory evidence. Starting strictly from the definitions above, compute the total clinical validity score $S$ in points. Then, based on the computed $S$, determine the resulting classification with reasoning. Round your final numerical answer for $S$ to four significant figures. State no units for $S$.", "solution": "We proceed by aggregating genetic evidence and experimental evidence according to the specified framework, starting from the fundamental definitions of de novo scoring, segregation likelihood and LOD, and the category caps.\n\nFirst, compute case-level de novo points. For autosomal dominant conditions:\n- Each confirmed de novo event contributes $2$ points.\n- Each plausible but unconfirmed de novo event contributes $1$ point.\n\nThere are $3$ confirmed de novo probands and $2$ unconfirmed de novo probands. The case-level de novo points are therefore\n$$\nP_{\\text{de novo}} = 3 \\times 2 + 2 \\times 1 = 6 + 2 = 8.\n$$\n\nSecond, compute segregation points. Under the fully penetrant autosomal dominant model with no recombinants, each pedigree with $m$ informative meioses contributes a LOD of $m \\log_{10}(2)$, and LODs add across pedigrees.\n\nFor Family $1$, $m_{1} = 4$, so\n$$\n\\text{LOD}_{1} = 4 \\log_{10}(2).\n$$\nFor Family $2$, $m_{2} = 2$, so\n$$\n\\text{LOD}_{2} = 2 \\log_{10}(2).\n$$\nThe combined LOD is\n$$\n\\text{LOD}_{\\text{tot}} = \\text{LOD}_{1} + \\text{LOD}_{2} = (4 + 2)\\log_{10}(2) = 6 \\log_{10}(2).\n$$\nUse the well-known identity $\\log_{10}(2) \\approx 0.3010$ (to four significant figures) to obtain a numerical approximation:\n$$\n\\text{LOD}_{\\text{tot}} \\approx 6 \\times 0.3010 = 1.806.\n$$\nNow convert the combined LOD into segregation points using the specified mapping:\n- Since $1 \\leq \\text{LOD}_{\\text{tot}} < 2$ (because $1.806$ lies in this interval), assign $0.5$ points.\n\nThus,\n$$\nP_{\\text{seg}} = 0.5.\n$$\n\nThird, sum genetic evidence points (capped at $12$). Here the sum is\n$$\nP_{\\text{gen}} = P_{\\text{de novo}} + P_{\\text{seg}} = 8 + 0.5 = 8.5,\n$$\nwhich is below the genetic evidence cap of $12$ points, so no capping is needed.\n\nFourth, compute experimental evidence points. The scenario specifies two limited lines of evidence:\n- Patient-derived cellular functional alteration: $0.5$ points.\n- Relevant expression data: $0.5$ points.\n\nTherefore,\n$$\nP_{\\text{exp}} = 0.5 + 0.5 = 1.0,\n$$\nwhich is below the experimental evidence cap of $6$ points.\n\nFifth, compute the total score $S$:\n$$\nS = P_{\\text{gen}} + P_{\\text{exp}} = 8.5 + 1.0 = 9.5.\n$$\nRounded to four significant figures, $S = 9.500$.\n\nFinally, determine the classification. Using the mapping:\n- “Limited” for $1 \\leq S \\leq 6$,\n- “Moderate” for $7 \\leq S \\leq 11$,\n- “Strong” for $12 \\leq S \\leq 18$.\n\nSince $S = 9.5$ lies in $[7, 11]$, the classification is “Moderate.” Replication over time is not satisfied, so “Definitive” is not applicable regardless of point totals.\n\nIn summary, the total clinical validity score computed from the specified mixed evidence is $S = 9.500$ (four significant figures), corresponding to a “Moderate” classification based on multiple de novo events, modest segregation (reflected in the LOD-to-points mapping), and limited but supportive functional evidence.", "answer": "$$\\boxed{9.500}$$", "id": "4338170"}, {"introduction": "The final classification of a gene-disease relationship is not a static endpoint but a conclusion that must be critically appraised for its robustness. This advanced practice introduces the vital concept of sensitivity analysis, moving from manual scoring to computational implementation. You will be tasked with modeling how the final classification holds up as lower-quality evidence is systematically removed, a process that helps identify the \"tipping point\" at which a classification might change [@problem_id:4338175]. Mastering this skill is crucial for understanding the confidence in a curation and for prioritizing which evidence gaps need to be filled to strengthen a claim.", "problem": "You are tasked with implementing a programmatic sensitivity analysis grounded in the Clinical Genome Resource (ClinGen) gene–disease clinical validity framework for evaluating how the removal of low-quality case reports affects the classification of a gene–disease association. The analysis must follow the core definitions of the ClinGen framework: total evidence points are the sum of capped genetic evidence and experimental evidence, and the resulting classification is determined by pre-specified thresholds.\n\nBase definitions and rules:\n- Let the total evidence points be denoted by $P$. Let the genetic evidence subtotal be $G$ and the experimental evidence subtotal be $E$. The total is computed as\n$$\nP \\;=\\; \\min(G_{\\mathrm{raw}},\\,12) \\;+\\; \\min(E_{\\mathrm{raw}},\\,6)\\,,\n$$\nwhere $G_{\\mathrm{raw}}$ and $E_{\\mathrm{raw}}$ are the uncapped genetic and experimental subtotals, respectively, and the caps $12$ and $6$ reflect the maximum allowable points from the ClinGen framework for genetic and experimental evidence, respectively.\n- The raw genetic subtotal $G_{\\mathrm{raw}}$ is the sum of case-level contributions, segregation evidence, and case–control evidence:\n$$\nG_{\\mathrm{raw}} \\;=\\; \\left(\\sum_{i=1}^{n} w_i\\right) \\;+\\; S \\;+\\; C \\,,\n$$\nwhere $w_i$ is the point contribution of the $i$-th case report (with $n$ total case reports), $S$ is the segregation evidence points, and $C$ is the case–control evidence points. All contributions are nonnegative real numbers.\n- Some case reports are flagged as low-quality. A binary indicator $q_i \\in \\{0,1\\}$ designates whether the $i$-th case is low-quality ($q_i = 1$) or not ($q_i = 0$).\n- Removing a set of low-quality case reports corresponds to subtracting their $w_i$ from $G_{\\mathrm{raw}}$ (but never below $0$), recomputing $P$ with the same caps.\n- The final classification code $c$ must be encoded as an integer using these thresholds:\n  - If $P < 1$, then $c = 0$ (No Reported Evidence).\n  - If $1 \\le P \\le 6$, then $c = 1$ (Limited).\n  - If $7 \\le P \\le 11$, then $c = 2$ (Moderate).\n  - If $12 \\le P \\le 18$, then $c = 3$ (Strong), unless replication over time is established (denote this boolean by $R$), in which case $c = 4$ (Definitive). Replication over time requires consistent evidence over a minimum period (for the ClinGen framework this is at least several years) with no valid contradictory evidence; for this problem, this is modeled as the provided boolean $R$.\n- All sums must be treated as real-valued calculations, and capping must be applied exactly as above. If any removal would make $G_{\\mathrm{raw}}$ negative, it must be floored at $0$ before applying the cap.\n\nSensitivity analysis specification:\n- Let $L$ be the multiset of $w_i$ for which $q_i = 1$ (the low-quality case reports). Sort $L$ in descending order to obtain a sequence $v_1 \\ge v_2 \\ge \\dots \\ge v_m$, where $m$ is the number of low-quality reports.\n- Define the removal sequence by cumulatively removing the first $r$ elements of this sorted list for $r \\in \\{0,1,2,\\dots,m\\}$, which produces a nonincreasing trajectory of total points $P_r$. This models a worst-case monotonic sensitivity where the largest low-quality contributors are removed first at each step.\n- Define the tipping point $t$ as the smallest $r \\in \\{0,1,2,\\dots,m\\}$ such that $P_r < 7$. If no such $r$ exists, set $t = -1$. If the baseline already satisfies $P_0 < 7$, then $t = 0$.\n\nYour program must implement the above rules and, for each test case, output a list of three integers $[c_0, c_{\\mathrm{full}}, t]$, where:\n- $c_0$ is the baseline classification code computed from $P_0$ (no removals),\n- $c_{\\mathrm{full}}$ is the classification code after removing all low-quality reports (that is, after $m$ removals),\n- $t$ is the tipping point defined above.\n\nTest suite (each test case is a tuple specifying $($case reports$,$ segregation$,$ case–control$,$ experimental$,$ replication over time$)$):\n- Test case $1$ (borderline Moderate, sensitive to removal):\n  - Case reports: $[(1.0,0),(1.0,0),(1.0,0),(0.5,1),(0.5,1)]$\n  - Segregation: $1.0$\n  - Case–control: $0.0$\n  - Experimental: $2.0$\n  - Replication over time: $\\mathrm{False}$\n- Test case $2$ (capping dominance; removal does not change classification; replication over time true):\n  - Case reports: ten high-quality of $1.0$ each and five low-quality of $1.0$ each:\n    $[(1.0,0)\\times 10,(1.0,1)\\times 5]$\n  - Segregation: $3.0$\n  - Case–control: $0.0$\n  - Experimental: $4.0$\n  - Replication over time: $\\mathrm{True}$\n- Test case $3$ (another borderline Moderate; one low-quality removal drops below Moderate):\n  - Case reports: $[(1.0,0),(1.0,0),(1.0,1)]$\n  - Segregation: $1.5$\n  - Case–control: $1.0$\n  - Experimental: $1.5$\n  - Replication over time: $\\mathrm{False}$\n- Test case $4$ (already Limited; removal can drop below reported-evidence threshold):\n  - Case reports: $[(0.6,1)]$\n  - Segregation: $0.2$\n  - Case–control: $0.2$\n  - Experimental: $0.0$\n  - Replication over time: $\\mathrm{False}$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list $[c_0,c_{\\mathrm{full}},t]$ for a given test case, in the same order as above. For example, a valid output format is\n$[[a_1,b_1,t_1],[a_2,b_2,t_2],[a_3,b_3,t_3],[a_4,b_4,t_4]]$\nwith all entries integers. No additional text should be printed.", "solution": "The solution is designed by first formalizing the calculation of the total evidence score and the subsequent classification, and then implementing the specified iterative removal process to determine the sensitivity of the classification.\n\nThe total evidence points, $P$, are the sum of a capped genetic evidence score and a capped experimental evidence score. The raw genetic score, $G_{\\mathrm{raw}}$, is the sum of contributions from individual case reports, segregation evidence, and case-control evidence. The raw experimental score, $E_{\\mathrm{raw}}$, is given directly. The calculation is:\n$$\nP \\;=\\; \\min(G_{\\mathrm{raw}},\\,12) \\;+\\; \\min(E_{\\mathrm{raw}},\\,6)\n$$\nwhere the genetic evidence is capped at $12$ points and experimental evidence at $6$ points. The raw genetic score is given by:\n$$\nG_{\\mathrm{raw}} \\;=\\; \\left(\\sum_{i=1}^{n} w_i\\right) \\;+\\; S \\;+\\; C\n$$\nHere, $w_i$ is the weight of the $i$-th case report, $S$ represents segregation evidence points, and $C$ represents case-control evidence points.\n\nThe classification code, $c$, is an integer derived from the total points $P$ and a boolean indicator for replication over time, $R$. A helper function, `get_classification`, implements these rules:\n- If $P < 1$, the code is $c = 0$.\n- If $1 \\le P \\le 6$, the code is $c = 1$.\n- If $7 \\le P \\le 11$, the code is $c = 2$.\n- If $12 \\le P \\le 18$, the code is $c = 3$, unless $R$ is true, in which case the code is $c = 4$.\n\nThe core of the analysis involves the following steps for each test case:\n1.  **Baseline Calculation**: The initial state ($r=0$, no removals) is assessed. First, all case report weights $w_i$ are summed. The baseline raw genetic score, $G_{\\mathrm{raw},0}$, is computed by adding segregation ($S$) and case-control ($C$) points. The baseline total score, $P_0$, is then calculated using the capping formula. Finally, the baseline classification, $c_0$, is determined from $P_0$ and $R$.\n\n2.  **Full Removal Calculation**: All case reports flagged as low-quality ($q_i=1$) are identified. The sum of their weights, $\\sum_{i | q_i=1} w_i$, is subtracted from $G_{\\mathrm{raw},0}$ to obtain the fully adjusted genetic score, $G_{\\mathrm{raw,full}}$. The problem specifies that this value must not be negative, so it is floored at $0$. The total score after full removal, $P_{\\mathrm{full}}$, and the corresponding classification, $c_{\\mathrm{full}}$, are then computed.\n\n3.  **Tipping Point Calculation**: The tipping point, $t$, is the minimum number of low-quality reports that must be removed to cause the total score to drop below $7$. The procedure is as follows:\n    a. The weights of all low-quality reports, $L = \\{w_i | q_i=1\\}$, are sorted in descending order to form the sequence $v_1, v_2, \\dots, v_m$.\n    b. The special case where the baseline score $P_0$ is already less than $7$ is handled first; in this scenario, the tipping point is $t=0$.\n    c. Otherwise, an iterative process begins. For each step $r$ from $1$ to $m$ (the total number of low-quality reports), the weights of the first $r$ reports from the sorted sequence ($v_1, \\dots, v_r$) are cumulatively removed.\n    d. At each step $r$, a new genetic score $G_{\\mathrm{raw},r} = G_{\\mathrm{raw},0} - \\sum_{j=1}^{r} v_j$ is computed, followed by the total score $P_r$.\n    e. The first value of $r$ for which $P_r < 7$ is the tipping point $t$. The iteration stops, and this value is recorded.\n    f. If the loop completes without the score ever dropping below $7$ (i.e., $P_m \\ge 7$), it implies the classification is robust to the removal of all low-quality evidence with respect to the \"Moderate\" threshold. In this case, the tipping point is set to $t=-1$.\n\nThis structured algorithm is applied to each test case to produce the required output triplet $[c_0, c_{\\mathrm{full}}, t]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n# No libraries are used.\n\ndef solve():\n    \"\"\"\n    Main function to run the sensitivity analysis on all test cases\n    and print the formatted results.\n    \"\"\"\n    # Test cases defined as per the problem statement.\n    # Format: (case_reports, segregation, case-control, experimental, replication_over_time)\n    test_cases = [\n        # Test case 1\n        (\n            [(1.0, 0), (1.0, 0), (1.0, 0), (0.5, 1), (0.5, 1)],\n            1.0, 0.0, 2.0, False\n        ),\n        # Test case 2\n        (\n            [(1.0, 0)] * 10 + [(1.0, 1)] * 5,\n            3.0, 0.0, 4.0, True\n        ),\n        # Test case 3\n        (\n            [(1.0, 0), (1.0, 0), (1.0, 1)],\n            1.5, 1.0, 1.5, False\n        ),\n        # Test case 4\n        (\n            [(0.6, 1)],\n            0.2, 0.2, 0.0, False\n        ),\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(run_sensitivity_analysis(*case))\n\n    # Final print statement in the exact required format.\n    # e.g., [[c1,c_f1,t1],[c2,c_f2,t2],...]\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef get_classification(p_total, replication_over_time):\n    \"\"\"\n    Determines the classification code based on total points and replication status.\n\n    Args:\n        p_total (float): The total evidence points.\n        replication_over_time (bool): True if replication over time is met.\n\n    Returns:\n        int: The classification code (0-4).\n    \"\"\"\n    if p_total  1:\n        return 0  # No Reported Evidence\n    elif p_total = 6:\n        return 1  # Limited\n    elif p_total = 11:\n        return 2  # Moderate\n    elif p_total = 18:\n        if replication_over_time:\n            return 4  # Definitive\n        else:\n            return 3  # Strong\n    else: # This case should not be reached with the given caps\n        if replication_over_time:\n            return 4\n        else:\n            return 3\n\n\ndef calculate_total_points(g_raw, e_raw):\n    \"\"\"\n    Calculates the total evidence points with capping.\n\n    Args:\n        g_raw (float): The raw genetic evidence subtotal.\n        e_raw (float): The raw experimental evidence subtotal.\n\n    Returns:\n        float: The total capped evidence points.\n    \"\"\"\n    capped_g = min(g_raw, 12.0)\n    capped_e = min(e_raw, 6.0)\n    return capped_g + capped_e\n\ndef run_sensitivity_analysis(case_reports, segregation, case_control, experimental, replication_over_time):\n    \"\"\"\n    Performs the full sensitivity analysis for a single gene-disease case.\n\n    Args:\n        case_reports (list of tuples): List of (weight, quality_flag) for each case.\n        segregation (float): Segregation evidence points.\n        case_control (float): Case-control evidence points.\n        experimental (float): Experimental evidence points.\n        replication_over_time (bool): Replication over time status.\n\n    Returns:\n        list of int: A list containing [c0, c_full, t].\n    \"\"\"\n    \n    # 1. Parse inputs and calculate baseline G_raw\n    total_case_weight = sum(w for w, q in case_reports)\n    g_raw_0 = total_case_weight + segregation + case_control\n    \n    # 2. Calculate baseline classification (c0)\n    p_0 = calculate_total_points(g_raw_0, experimental)\n    c_0 = get_classification(p_0, replication_over_time)\n\n    # 3. Identify and sort low-quality reports\n    low_quality_weights = sorted([w for w, q in case_reports if q == 1], reverse=True)\n    m = len(low_quality_weights)\n    \n    # 4. Calculate classification after full removal (c_full)\n    total_low_quality_weight = sum(low_quality_weights)\n    g_raw_full = max(0, g_raw_0 - total_low_quality_weight)\n    p_full = calculate_total_points(g_raw_full, experimental)\n    c_full = get_classification(p_full, replication_over_time)\n\n    # 5. Determine the tipping point (t)\n    tipping_point = -1 # Default value if score never drops below 7\n    \n    if p_0  7:\n        tipping_point = 0\n    else:\n        weight_removed_so_far = 0.0\n        for r in range(1, m + 1):\n            weight_removed_so_far += low_quality_weights[r-1]\n            g_raw_r = g_raw_0 - weight_removed_so_far\n            p_r = calculate_total_points(g_raw_r, experimental)\n            \n            if p_r  7:\n                tipping_point = r\n                break\n    \n    return [c_0, c_full, tipping_point]\n\n# Execute the main function when the script is run.\nsolve()\n```", "id": "4338175"}]}