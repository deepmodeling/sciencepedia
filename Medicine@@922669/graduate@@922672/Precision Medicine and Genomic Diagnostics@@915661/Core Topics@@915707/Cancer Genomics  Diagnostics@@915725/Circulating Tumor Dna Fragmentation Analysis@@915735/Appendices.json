{"hands_on_practices": [{"introduction": "The journey from a raw sequencing experiment to biological insight begins with a robust computational pipeline. This exercise focuses on the critical first step: transforming paired-end sequencing reads from a cell-free DNA library into a clean, analyzable fragment table. Mastering this process [@problem_id:4322504] is essential, as the accuracy of all downstream fragmentation analyses hinges on the correct handling of unique molecular identifiers, precise read alignment, and the biologically sound definition of fragment coordinates.", "problem": "A plasma cell-free deoxyribonucleic acid (DNA) library has been sequenced with paired-end reads to support circulating tumor DNA fragmentation analysis. The library incorporates Unique Molecular Identifiers (UMIs) of length $K$ nucleotides at the $5'$ end of Read $1$. Each pair has read length $L$ bases. The aligner reports Sequence Alignment/Map (SAM) records with Binary Alignment/Map (BAM) output, including CIGAR strings and Mapping Quality (MAPQ). Assume that fragment ends reflect nuclease cut sites and must be annotated at the $5'$ ends of each strand. The goal is to design a computational pipeline that transforms raw FASTQ files into a fragment table whose rows represent physical DNA molecules and whose minimal columns are chromosome, fragment start coordinate $s$, fragment end coordinate $e$, fragment length $\\ell$, consensus UMI $u$, and a strand-aware end-context annotation derived from the reference genome sequence.\n\nFundamental base for derivation:\n- Paired-end sequencing provides two reads per original DNA fragment. Proper orientation for a typical insert implies that one mate maps to the forward strand and the other to the reverse strand, and the fragment length $\\ell$ equals $e - s + 1$, where $s$ and $e$ are the genomic coordinates of the $5'$ ends on the forward and reverse strands respectively, computed from aligned positions.\n- Mapping quality $Q$ is related to the probability of incorrect mapping by $p_{\\text{err}} = 10^{-Q/10}$; higher $Q$ implies lower misalignment probability for end-coordinate estimation.\n- UMI collapsing aims to merge sequencing or polymerase chain reaction (PCR) duplicates originating from the same original molecule. The risk of UMI collisions increases with the number of original molecules $M$ relative to the UMI space size $U$, where $U = 4^{K}$ for random $K$-mer UMIs.\n\nGiven a study with $L = 150$, $K = 12$, and an estimated $M = 10^{6}$ original molecules, evaluate the following candidate pipelines from FASTQ to a fragment table. Select all pipelines that, by first-principles reasoning, yield unbiased, strand-correct fragment end coordinates and appropriately collapsed molecule counts for circulating tumor DNA fragmentation analysis.\n\nA. Extract UMI sequences from the first $K$ bases of Read $1$, move them to the read header, and hard-trim these $K$ bases from the read before alignment; perform adapter trimming and quality trimming on both mates; map with a high-fidelity, gapped aligner to the human reference; retain only properly paired alignments with $Q \\ge 30$ and concordant insert sizes. For each pair, compute $s$ as the $5'$ coordinate of the forward-strand mate (aligned start) and $e$ as the $5'$ coordinate of the reverse-strand mate (aligned end, accounting for the aligned match length in the CIGAR), then set $\\ell = e - s + 1$. Collapse duplicates by grouping records with identical consensus UMI $u$ and identical $\\{s,e\\}$ (allowing a tolerance $\\delta \\le 1$ base in case of micro-variation from local alignment), creating a single consensus row per group. Annotate both ends by retrieving the trinucleotide context from the reference genome at $s$ and $e$, using reverse-complement context for the reverse-strand end.\n\nB. Do not trim UMIs; align reads with UMIs present in the sequences; retain alignments with $Q \\ge 20$; define fragment ends using the $3'$ coordinates of each mate; collapse duplicates by UMI only, disregarding genomic positions, because with $K = 12$ UMI collisions are negligible at $M = 10^{6}$; annotate end motifs using the sequenced read bases rather than the reference.\n\nC. Merge mates into a single sequence prior to alignment; align merged reads; compute fragment length $\\ell$ as the total aligned length; define end coordinates using the outermost soft-clipped positions to capture potential microhomology; collapse duplicates by requiring equal UMI and equal $\\ell$ within a tolerance $\\pm 5$ bases; annotate end contexts using the aligned read sequence including soft clips.\n\nD. Extract and trim UMIs; perform adapter and quality trimming; align to the human reference; retain properly paired alignments with $20 \\le Q  30$; compute $s$ and $e$ using $5'$ coordinates as in A; collapse duplicates requiring identical UMIs and $\\{s,e\\}$ within a tolerance $\\delta = 10$ bases to accommodate mapping uncertainty; annotate end contexts using the reference sequence.\n\nAnswer choices:\nA, B, C, D", "solution": "The problem statement is found to be valid. It is scientifically grounded in the principles of genomics and bioinformatics, specifically concerning the analysis of cell-free DNA for applications like circulating tumor DNA profiling. The problem is well-posed, providing a clear objective and all necessary parameters and definitions. The terminology and provided values are standard and realistic for a modern sequencing experiment.\n\nThe objective is to identify a computational pipeline that correctly processes paired-end sequencing data with Unique Molecular Identifiers (UMIs) to produce an accurate fragment table for fragmentation analysis. An accurate analysis requires:\n1.  Proper handling of UMIs to prevent alignment artifacts.\n2.  High-confidence alignment to establish precise genomic coordinates.\n3.  Biologically correct definition and calculation of fragment ends ($s$, $e$) and length ($\\ell$).\n4.  Robust deduplication (UMI collapsing) that accounts for both PCR duplicates and stochastic UMI collisions.\n5.  Correct methodology for annotating the sequence context at fragment ends.\n\nWe are given specific parameters: read length $L=150$, UMI length $K=12$, and an estimated $M=10^6$ original molecules.\n\nFirst, let's evaluate the UMI collision risk. The size of the UMI space is $U = 4^K = 4^{12} = 16,777,216 \\approx 1.68 \\times 10^7$. With $M=10^6$ distinct molecules, the birthday problem approximation for the expected number of collisions (pairs of distinct molecules that are assigned the same UMI by chance) is $E[\\text{collisions}] \\approx \\frac{M^2}{2U} = \\frac{(10^6)^2}{2 \\times 4^{12}} = \\frac{10^{12}}{2 \\times 16,777,216} \\approx 29,802$. This is a substantial number of collisions, which demonstrates that collapsing duplicates based on UMI sequence alone is insufficient and will lead to significant under-counting of distinct molecules. Correct collapsing must also leverage the genomic coordinates of the fragment.\n\nNow we evaluate each proposed pipeline.\n\n### Option A Evaluation\n\n1.  **UMI Handling**: \"Extract UMI sequences from the first $K$ bases of Read $1$, move them to the read header, and hard-trim these $K$ bases from the read before alignment\". This is the correct procedure. It isolates the non-biological UMI sequence, preventing it from interfering with the alignment of the biological insert sequence to the reference genome.\n2.  **Trimming  Alignment**: \"perform adapter trimming and quality trimming on both mates; map with a high-fidelity, gapped aligner to the human reference; retain only properly paired alignments with $Q \\ge 30$\". This is a robust protocol. Adapter and quality trimming are standard pre-processing steps. Using a high-fidelity gapped aligner is necessary for accurate mapping. Filtering for properly paired reads with a high mapping quality ($Q \\ge 30$, corresponding to a maximum misalignment probability of $p_{\\text{err}} = 10^{-30/10} = 0.001$) is critical for ensuring that fragment end coordinates are precise.\n3.  **Fragment Coordinate Calculation**: \"compute $s$ as the $5'$ coordinate of the forward-strand mate (aligned start) and $e$ as the $5'$ coordinate of the reverse-strand mate (aligned end, accounting for the aligned match length in the CIGAR), then set $\\ell = e - s + 1$\". This correctly identifies the fragment's boundaries as the $5'$ ends of the original DNA molecule's two strands. The $5'$ end of the forward strand corresponds to the aligned start position of the forward read. The $5'$ end of the reverse strand corresponds to the $3'$ end of the reverse-complemented read sequence, which is calculated as the aligned start position of the reverse read plus its aligned length on the reference. After determining the two end coordinates, they are typically ordered such that $s$ is the start and $e$ is the end. The length calculation $\\ell = e - s + 1$ is correct for 1-based inclusive coordinates.\n4.  **UMI Collapsing**: \"Collapse duplicates by grouping records with identical consensus UMI $u$ and identical $\\{s,e\\}$ (allowing a tolerance $\\delta \\le 1$ base)\". This is the correct method for deduplication. It requires both the UMI and the genomic coordinates to match, thereby disambiguating true PCR duplicates from random UMI collisions. A small tolerance of $\\delta \\le 1$ base is appropriate to account for minor alignment ambiguities (e.g., around indels) without incorrectly merging distinct-but-nearby fragments.\n5.  **End-Context Annotation**: \"Annotate both ends by retrieving the trinucleotide context from the reference genome at $s$ and $e$, using reverse-complement context for the reverse-strand end\". This is the scientifically sound approach. The analysis of nuclease cut-site preference should be based on the invariant reference genome sequence, not the read sequence which may contain sequencing errors or germline/somatic variants. Correctly handling the strandedness of the context by taking the reverse-complement for the reverse-strand end is also critical.\n\n**Verdict on A**: This pipeline is methodologically sound, employing best practices at each step to ensure unbiased and accurate fragment analysis. **Correct**.\n\n### Option B Evaluation\n\n1.  **UMI Handling**: \"Do not trim UMIs; align reads with UMIs present\". This is a major flaw. It will lead to mismatches at the $5'$ end of reads, causing alignment artifacts like soft-clipping (which changes the reported coordinate) or reduced mapping quality.\n2.  **Alignment  Filtering**: \"retain alignments with $Q \\ge 20$\". A mapping quality threshold of $Q=20$ ($p_{\\text{err}}=0.01$) is less stringent than desired for applications requiring precise end coordinates.\n3.  **Fragment Coordinate Calculation**: \"define fragment ends using the $3'$ coordinates of each mate\". This is biologically incorrect. The nuclease cut sites that create the fragment are at the $5'$ ends of the strands.\n4.  **UMI Collapsing**: \"collapse duplicates by UMI only... because with $K=12$ UMI collisions are negligible at $M=10^6$\". This premise is false, as calculated above. The expected number of collisions is in the tens of thousands, which is not negligible. Collapsing by UMI alone would massively under-represent the complexity of the library.\n5.  **End-Context Annotation**: \"annotate end motifs using the sequenced read bases rather than the reference\". This is incorrect as it confounds the analysis of cleavage motifs with sequencing errors and variants.\n\n**Verdict on B**: This pipeline is flawed in every key aspect of its design. **Incorrect**.\n\n### Option C Evaluation\n\n1.  **Mate Handling**: \"Merge mates into a single sequence prior to alignment\". This fundamentally alters the data structure in a way that is incompatible with the goal. Merging reads collapses the two distinct ends of the original fragment into a single sequence, making it impossible to identify the two independent nuclease cut sites, which is the primary goal of fragmentation analysis.\n2.  **Fragment Coordinate Calculation**: \"compute fragment length $\\ell$ as the total aligned length; define end coordinates using the outermost soft-clipped positions\". This is a direct consequence of the flawed merging step and is incorrect. Fragment length is the genomic distance between the outer $5'$ ends, not the length of a merged sequence. Soft-clipped positions are not reliable markers for fragment ends.\n3.  **UMI Collapsing**: \"collapse duplicates by requiring equal UMI and equal $\\ell$ within a tolerance $\\pm 5$ bases\". This is insufficient. It fails to use the genomic coordinates $\\{s,e\\}$, meaning two distinct fragments at different locations that happen to have the same length and UMI would be incorrectly merged.\n4.  **End-Context Annotation**: \"annotate end contexts using the aligned read sequence including soft clips\". Incorrect for using read data instead of reference, and doubly incorrect for using soft-clipped bases, which are by definition not aligned to the reference.\n\n**Verdict on C**: This pipeline's initial step of merging mates makes it fundamentally unsuitable for the stated goal. **Incorrect**.\n\n### Option D Evaluation\n\n1.  **UMI Handling**: \"Extract and trim UMIs; perform adapter and quality trimming\". This step is correct.\n2.  **Alignment  Filtering**: \"retain properly paired alignments with $20 \\le Q  30$\". This filtering criterion is illogical. It specifically selects for reads of medium mapping quality while discarding the highest-quality reads ($Q \\ge 30$). For an analysis that depends on coordinate precision, one must use the highest-confidence alignments available, not exclude them.\n3.  **UMI Collapsing**: \"collapse duplicates requiring identical UMIs and $\\{s,e\\}$ within a tolerance $\\delta = 10$ bases\". While using both UMI and coordinates is the correct principle, a tolerance of $\\delta=10$ bases is excessively large. Such a large window will lead to the incorrect merging of distinct molecules that are physically close on the chromosome, biasing the results by artificially removing short fragments and reducing library complexity estimates.\n4.  **End-Context Annotation**: \"annotate end contexts using the reference sequence\". This step is correct.\n\n**Verdict on D**: Despite some correct components, the pipeline is critically flawed due to the illogical MAPQ filter and the overly permissive collapsing tolerance, both of which would introduce significant bias. **Incorrect**.\n\nBased on the analysis, only Pipeline A represents a complete, robust, and scientifically valid method for achieving the goals of the study.", "answer": "$$\\boxed{A}$$", "id": "4322504"}, {"introduction": "With a curated fragment table, we can begin to test biological hypotheses by comparing fragmentation features between different cohorts. This practice demonstrates how to apply a fundamental statistical method, the two-sample Kolmogorov-Smirnov test, to determine if the distribution of a key metric—the short-to-long fragment ratio—differs significantly between a cancer patient group and a non-malignant control group. This exercise [@problem_id:4322550] provides practical experience in non-parametric hypothesis testing and quantifying effect sizes from ctDNA fragmentation data.", "problem": "In circulating tumor deoxyribonucleic acid (ctDNA) fragmentation analysis within genomic diagnostics, a commonly used summary feature is the short-to-long fragment ratio, defined for each plasma sample as the ratio of the count of short fragments to the count of long fragments measured within fixed base-pair windows. Suppose two cohorts are profiled: Cohort A (non-malignant controls) and Cohort B (patients with confirmed solid tumors). For each individual, the short-to-long ratio is computed from their cell-free deoxyribonucleic acid (cfDNA) fragment-size histogram using the same pipeline. You are given the following ratio measurements:\n- Cohort A, size $n=10$: $0.58, 0.62, 0.66, 0.69, 0.73, 0.77, 0.81, 0.86, 0.92, 0.98$.\n- Cohort B, size $m=12$: $0.70, 0.74, 0.79, 0.83, 0.88, 0.93, 1.00, 1.05, 1.10, 1.15, 1.22, 1.30$.\n\nUsing first principles of nonparametric distribution comparison, specifically the definition of the empirical cumulative distribution function (ECDF) and the two-sample Kolmogorov–Smirnov (KS) test, do the following:\n- Construct the empirical cumulative distribution functions $F_{A}(x)$ and $F_{B}(x)$ from the samples.\n- Compute the two-sample Kolmogorov–Smirnov statistic $D=\\sup_{x}\\left|F_{A}(x)-F_{B}(x)\\right|$.\n- Using the asymptotic form of the two-sample Kolmogorov distribution, assess whether there is evidence at significance level $\\alpha=0.05$ that the distributions of short-to-long ratios differ between the two cohorts. State your conclusion in words.\n- Treat the Kolmogorov–Smirnov statistic $D$ as the effect size. Report the effect size.\n\nYour final reported value must be the effect size $D$. Round your final answer to four significant figures. No units are required.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- **Topic**: Circulating tumor DNA (ctDNA) fragmentation analysis.\n- **Metric**: Short-to-long fragment ratio.\n- **Cohort A (Controls)**: Sample size $n=10$. Data: $0.58, 0.62, 0.66, 0.69, 0.73, 0.77, 0.81, 0.86, 0.92, 0.98$.\n- **Cohort B (Patients)**: Sample size $m=12$. Data: $0.70, 0.74, 0.79, 0.83, 0.88, 0.93, 1.00, 1.05, 1.10, 1.15, 1.22, 1.30$.\n- **Tasks**:\n    1. Construct empirical cumulative distribution functions (ECDFs) $F_{A}(x)$ and $F_{B}(x)$.\n    2. Compute the two-sample Kolmogorov–Smirnov (KS) statistic $D=\\sup_{x}\\left|F_{A}(x)-F_{B}(x)\\right|$.\n    3. Assess for a statistically significant difference at level $\\alpha=0.05$ using the asymptotic KS distribution.\n    4. Report the KS statistic $D$ as the effect size, rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is well-grounded in the field of genomic diagnostics and biostatistics. The use of ctDNA fragment ratios and the Kolmogorov-Smirnov test are standard practices.\n- **Well-Posedness**: The problem provides all necessary data and defines a clear computational task with a unique solution.\n- **Objectivity**: The problem statement is objective and devoid of subjective claims.\n- **Flaw Check**: The problem does not violate any of the invalidity criteria. It is scientifically sound, formalizable, complete, and well-structured.\n\n**Step 3: Verdict and Action**\n- **Verdict**: The problem is valid.\n- **Action**: Proceed with the solution.\n\n**Solution**\n\nThe problem requires a two-sample Kolmogorov-Smirnov (KS) test to compare the distributions of short-to-long fragment ratios from two cohorts, A and B.\n\n**1. Empirical Cumulative Distribution Functions (ECDFs)**\n\nLet the ordered sample for Cohort A be $x_{A,1}, x_{A,2}, \\dots, x_{A,n}$ where $n=10$, and the ordered sample for Cohort B be $x_{B,1}, x_{B,2}, \\dots, x_{B,m}$ where $m=12$.\n\nThe ECDF for a sample is defined as the fraction of sample points less than or equal to a given value $x$.\nFor Cohort A, the ECDF, $F_A(x)$, is given by:\n$$F_A(x) = \\frac{1}{n} \\sum_{i=1}^{n} I(x_{A,i} \\le x) = \\frac{1}{10} \\times (\\text{number of observations in Cohort A} \\le x)$$\nwhere $I(\\cdot)$ is the indicator function. The function $F_A(x)$ is a step function that starts at $0$, increases by a step of $1/n = 1/10 = 0.1$ at each data point from Cohort A, and reaches a value of $1$.\n\nFor Cohort B, the ECDF, $F_B(x)$, is given by:\n$$F_B(x) = \\frac{1}{m} \\sum_{j=1}^{m} I(x_{B,j} \\le x) = \\frac{1}{12} \\times (\\text{number of observations in Cohort B} \\le x)$$\nThe function $F_B(x)$ is a step function that starts at $0$, increases by a step of $1/m = 1/12$ at each data point from Cohort B, and reaches a value of $1$.\n\n**2. Computation of the Two-Sample KS Statistic, $D$**\n\nThe two-sample KS statistic, $D$, is the maximum absolute difference between the two ECDFs over all possible values of $x$:\n$$D = \\sup_{x} |F_A(x) - F_B(x)|$$\nThe supremum of the difference between the two step functions must occur at one of the observed data points from either cohort. To find this maximum difference, we form a single, sorted list of all $n+m=22$ unique data points and evaluate the difference $|F_A(x) - F_B(x)|$ immediately before and at each of these points.\n\nLet the combined and sorted list of observations be $z_1, z_2, \\dots, z_{22}$. We calculate $|F_A(z_k) - F_B(z_k)|$ and $|F_A(z_k^-) - F_B(z_k^-)|$ for all $k \\in \\{1, \\dots, 22\\}$, where $z_k^-$ denotes a value just smaller than $z_k$. The latter is equivalent to $|F_A(z_{k-1}) - F_B(z_{k-1})|$ for $k1$.\n\nThe calculation proceeds by iterating through the sorted combined list of data:\n$0.58, 0.62, 0.66, 0.69, 0.70, 0.73, 0.74, 0.77, 0.79, 0.81, 0.83, 0.86, 0.88, 0.92, 0.93, 0.98, 1.00, 1.05, 1.10, 1.15, 1.22, 1.30$.\n\nThe maximum difference is found by systematically calculating the values of $|F_A(x) - F_B(x)|$ at the boundaries of the intervals defined by these data points.\n- At $x  0.58$, $F_A(x)=0$ and $F_B(x)=0$, so the difference is $0$.\n- At $x=0.69$, $F_A(0.69) = 4/10 = 0.4$ and $F_B(0.69)=0$, yielding a difference of $|0.4-0.0|=0.4$.\n- Just before $x=0.70$, the values are $F_A(0.70^-) = 4/10 = 0.4$ and $F_B(0.70^-)=0$, so the difference remains $0.4$.\n- At $x=0.73$, $F_A(0.73) = 5/10=0.5$ and $F_B(0.73) = 1/12 \\approx 0.0833$, yielding a difference of $|0.5 - 1/12| = 5/12 \\approx 0.4167$.\n- Just before $x=0.98$, the last observation from cohort A, the difference is evaluated at $x = 0.93$. Here, $F_A(0.93)=9/10=0.9$ and $F_B(0.93)=6/12=0.5$, for a difference of $|0.9-0.5|=0.4$.\n- At $x=0.98$, $F_A(0.98) = 10/10 = 1.0$ and $F_B(0.98)=6/12=0.5$. The difference is $|1.0 - 0.5| = 0.5$.\n- Just before $x=1.00$, the ECDFs have values $F_A(1.00^-) = F_A(0.98) = 1.0$ and $F_B(1.00^-) = F_B(0.98) = 0.5$. The difference is $|1.0-0.5|=0.5$.\n- At $x=1.00$, $F_A(1.00)=1.0$ and $F_B(1.00) = 7/12 \\approx 0.5833$. The difference is $|1.0 - 7/12| = 5/12 \\approx 0.4167$.\n\nAfter evaluating the differences at all data points, the maximum difference is found to be $0.5$.\n$$D = 0.5$$\n\n**3. Hypothesis Test at $\\alpha=0.05$**\n\nWe test the following hypotheses:\n- Null Hypothesis, $H_0$: The two samples are drawn from the same underlying continuous distribution.\n- Alternative Hypothesis, $H_1$: The two samples are drawn from different distributions.\n\nThe problem specifies using the asymptotic form for the distribution of the KS statistic. The test statistic is compared to a critical value, $D_{crit}$, which depends on the significance level $\\alpha$ and the sample sizes $n$ and $m$. We reject $H_0$ if our observed statistic $D$ is greater than $D_{crit}$.\nThe critical value is given by the formula:\n$$D_{crit} = c(\\alpha) \\sqrt{\\frac{n+m}{nm}}$$\nwhere $c(\\alpha)$ is a constant derived from the limiting Kolmogorov distribution. For a significance level of $\\alpha=0.05$, the standard value is $c(0.05) \\approx 1.3581$.\n\nUsing the given sample sizes $n=10$ and $m=12$:\n$$D_{crit} \\approx 1.3581 \\sqrt{\\frac{10+12}{10 \\times 12}} = 1.3581 \\sqrt{\\frac{22}{120}} = 1.3581 \\sqrt{0.18333...}$$\n$$D_{crit} \\approx 1.3581 \\times 0.42817 = 0.5816$$\nWe compare our calculated statistic $D=0.5$ with this critical value:\n$$D = 0.5  D_{crit} \\approx 0.5816$$\nSince the observed KS statistic is less than the critical value, we fail to reject the null hypothesis at the $\\alpha=0.05$ significance level.\n\n**Conclusion**: Based on the Kolmogorov-Smirnov test, there is not sufficient statistical evidence at the $5\\%$ significance level to conclude that the distributions of short-to-long ratios for the non-malignant control cohort and the solid tumor patient cohort are different.\n\n**4. Effect Size**\n\nThe problem asks to treat the Kolmogorov-Smirnov statistic $D$ as the effect size.\nThe calculated effect size is $D=0.5$. Rounding to four significant figures gives $0.5000$.", "answer": "$$\n\\boxed{0.5000}\n$$", "id": "4322550"}, {"introduction": "Beyond simply detecting the presence of cancer, ctDNA fragmentation patterns hold the key to identifying a tumor's tissue of origin. This advanced practice challenges you to implement a computational algorithm based on a linear mixture model, a powerful framework for deconvolving complex signals. By solving a regularized non-negative least-squares problem [@problem_id:4322534], you will learn how to translate a mathematical model of tissue-specific promoter footprints into a functional tool that can infer the proportional contribution of different tissues to a cfDNA sample.", "problem": "You are given a formalization of a tissue-of-origin inference problem grounded in circulating tumor deoxyribonucleic acid (ctDNA) fragmentation analysis within the field of precision medicine and genomic diagnostics. In plasma, cell-free deoxyribonucleic acid (cfDNA) fragmentation patterns reflect nucleosome occupancy, and promoter footprints in cfDNA correlate with tissue-specific transcriptional activity. Under a mixture model of cfDNA sources, the promoter footprint vector from a sample can be modeled as a non-negative linear combination of tissue-specific nucleosome occupancy maps across promoters.\n\nStarting from the following foundational elements:\n- Observational basis that cfDNA fragment end densities around promoters are modulated by nucleosome occupancy and transcriptional activity.\n- Linear mixture modeling where a cfDNA sample is a mixture of contributions from distinct tissues.\n- Non-negativity of tissue fractions and the compositional nature of mixtures.\n\nFormally, let there be $K$ tissues and $P$ promoters. Define:\n- $X \\in \\mathbb{R}^{P \\times K}$ as the matrix of tissue-specific, expression-derived nucleosome maps, where column $k$ encodes the expected promoter footprint across $P$ promoters for tissue $k$.\n- $f \\in \\mathbb{R}^{P}$ as the observed cfDNA promoter footprint vector from a sample (fragment end density per promoter, assumed non-negative).\n- $w \\in \\mathbb{R}_{\\ge 0}^{P}$ as per-promoter non-negative reliability weights (e.g., inverse-variance weights, nucleosome mappability weights, or promoter confidence weights).\n- $b \\in \\mathbb{R}_{\\ge 0}^{K}$ as the unknown non-negative tissue mixture coefficients, representing tissue-of-origin fractions.\n- $\\lambda \\in \\mathbb{R}_{\\ge 0}$ as a regularization parameter to stabilize estimation under collinearity.\n\nAssume a linear observation model $f \\approx X b + \\varepsilon$ and adopt a weighted, regularized, non-negative least-squares principle: infer $b$ by minimizing the objective\n$$\nJ(b) \\;=\\; \\left\\| W \\left(f - X b \\right) \\right\\|_2^2 \\;+\\; \\lambda \\left\\| b \\right\\|_2^2\n$$\nsubject to $b \\ge 0$, where $W = \\mathrm{diag}(w) \\in \\mathbb{R}^{P \\times P}$ is the diagonal matrix of weights. After obtaining a minimizer $\\hat{b}$, normalize it to obtain compositional fractions $\\tilde{b}$ via\n$$\n\\tilde{b}_k \\;=\\; \n\\begin{cases}\n\\frac{\\hat{b}_k}{\\sum_{j=1}^{K} \\hat{b}_j},  \\text{if } \\sum_{j=1}^{K} \\hat{b}_j  0, \\\\\n0,  \\text{otherwise},\n\\end{cases}\n$$\nfor $k \\in \\{1,\\dots,K\\}$.\n\nImplement an algorithm that:\n1. Scales rows of $X$ and $f$ by $w$ (i.e., forms $X' = W X$ and $f' = W f$).\n2. Incorporates Tikhonov regularization by augmenting the system with $\\sqrt{\\lambda} I_K$, where $I_K \\in \\mathbb{R}^{K \\times K}$ is the identity matrix, yielding an augmented least-squares problem:\n$$\n\\min_{b \\ge 0} \\left\\| \\begin{bmatrix} X' \\\\ \\sqrt{\\lambda} I_K \\end{bmatrix} b - \\begin{bmatrix} f' \\\\ 0_K \\end{bmatrix} \\right\\|_2^2,\n$$\nwhere $0_K \\in \\mathbb{R}^{K}$ is the zero vector.\n3. Solves the non-negative least-squares problem to obtain $\\hat{b}$.\n4. Normalizes $\\hat{b}$ to $\\tilde{b}$ as defined above.\n\nDesign a program to carry out this algorithm for each of the following test cases. For reproducibility, all numeric matrices and vectors are specified exactly.\n\nTest case $1$ (general case, informative weights, moderate regularization):\n- $P = 5$, $K = 3$.\n- Data:\n$$\nX \\;=\\; \n\\begin{bmatrix}\n0.8  0.1  0.2 \\\\\n1.0  0.2  0.3 \\\\\n0.2  0.9  0.1 \\\\\n0.0  0.7  0.9 \\\\\n0.6  0.3  0.4\n\\end{bmatrix}, \\quad\nf \\;=\\;\n\\begin{bmatrix}\n0.55 \\\\ 0.68 \\\\ 0.40 \\\\ 0.31 \\\\ 0.47\n\\end{bmatrix}, \\quad\nw \\;=\\;\n\\begin{bmatrix}\n1.0 \\\\ 0.8 \\\\ 1.2 \\\\ 1.0 \\\\ 0.9\n\\end{bmatrix}, \\quad\n\\lambda \\;=\\; 0.05.\n$$\n\nTest case $2$ (high collinearity between two tissues, uniform weights, stronger regularization):\n- $P = 5$, $K = 3$.\n- Data:\n$$\nX \\;=\\;\n\\begin{bmatrix}\n0.4  0.39  0.10 \\\\\n0.6  0.59  0.20 \\\\\n0.8  0.79  0.30 \\\\\n0.5  0.49  0.40 \\\\\n0.7  0.69  0.50\n\\end{bmatrix}, \\quad\nf \\;=\\;\n\\begin{bmatrix}\n0.400 \\\\ 0.592 \\\\ 0.799 \\\\ 0.495 \\\\ 0.693\n\\end{bmatrix}, \\quad\nw \\;=\\;\n\\begin{bmatrix}\n1.0 \\\\ 1.0 \\\\ 1.0 \\\\ 1.0 \\\\ 1.0\n\\end{bmatrix}, \\quad\n\\lambda \\;=\\; 0.10.\n$$\n\nTest case $3$ (boundary case of zero signal, heterogeneous weights, minimal regularization):\n- $P = 4$, $K = 2$.\n- Data:\n$$\nX \\;=\\;\n\\begin{bmatrix}\n0.2  0.8 \\\\\n0.3  0.7 \\\\\n0.5  0.5 \\\\\n0.9  0.1\n\\end{bmatrix}, \\quad\nf \\;=\\;\n\\begin{bmatrix}\n0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0\n\\end{bmatrix}, \\quad\nw \\;=\\;\n\\begin{bmatrix}\n1.0 \\\\ 0.5 \\\\ 0.8 \\\\ 0.6\n\\end{bmatrix}, \\quad\n\\lambda \\;=\\; 0.01.\n$$\n\nYour program must implement the above algorithm and produce the normalized tissue fraction vectors $\\tilde{b}$ for each test case. The required final output format is a single line containing a list of lists, where each inner list is the result for one test case, and each float must be rounded to $6$ decimal places. For example, the output format is of the form\n$$\n\\left[ \\left[ \\tilde{b}^{(1)}_1, \\dots, \\tilde{b}^{(1)}_{K_1} \\right], \\left[ \\tilde{b}^{(2)}_1, \\dots, \\tilde{b}^{(2)}_{K_2} \\right], \\left[ \\tilde{b}^{(3)}_1, \\dots, \\tilde{b}^{(3)}_{K_3} \\right] \\right],\n$$\nprinted as a single line, using decimal notation. No physical units or angles are involved; all outputs are dimensionless decimals.", "solution": "The problem requires the implementation of an algorithm to infer tissue-of-origin mixture fractions from circulating tumor DNA (ctDNA) fragmentation patterns. This is framed as a non-negative least-squares (NNLS) optimization problem with Tikhonov regularization. The validation and solution process will proceed as follows.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\n\nThe problem is defined by a mathematical model and an algorithm to solve it, applied to three specific test cases.\n\n**Model and Objective Function:**\n- Let $K$ be the number of tissues and $P$ be the number of promoters.\n- $X \\in \\mathbb{R}^{P \\times K}$: Matrix of tissue-specific promoter footprints.\n- $f \\in \\mathbb{R}^{P}$: Observed cfDNA promoter footprint vector from a sample.\n- $w \\in \\mathbb{R}_{\\ge 0}^{P}$: Per-promoter non-negative reliability weights.\n- $b \\in \\mathbb{R}_{\\ge 0}^{K}$: Unknown non-negative tissue mixture coefficients.\n- $\\lambda \\in \\mathbb{R}_{\\ge 0}$: Regularization parameter.\nThe coefficients $b$ are to be inferred by minimizing the objective function:\n$$\nJ(b) \\;=\\; \\left\\| W \\left(f - X b \\right) \\right\\|_2^2 \\;+\\; \\lambda \\left\\| b \\right\\|_2^2\n$$\nsubject to the constraint $b \\ge 0$, where $W = \\mathrm{diag}(w)$.\n\n**Normalization of Solution:**\nAfter finding the minimizer $\\hat{b}$, it is normalized to compositional fractions $\\tilde{b}$:\n$$\n\\tilde{b}_k \\;=\\; \n\\begin{cases}\n\\frac{\\hat{b}_k}{\\sum_{j=1}^{K} \\hat{b}_j},  \\text{if } \\sum_{j=1}^{K} \\hat{b}_j  0, \\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\n\n**Algorithm:**\n1.  Scale the data matrices: $X' = W X$ and $f' = W f$.\n2.  Formulate an augmented least-squares problem:\n    $$\n    \\min_{b \\ge 0} \\left\\| \\begin{bmatrix} X' \\\\ \\sqrt{\\lambda} I_K \\end{bmatrix} b - \\begin{bmatrix} f' \\\\ 0_K \\end{bmatrix} \\right\\|_2^2\n    $$\n    where $I_K$ is the $K \\times K$ identity matrix and $0_K$ is the $K \\times 1$ zero vector.\n3.  Solve the resulting non-negative least-squares problem for $\\hat{b}$.\n4.  Normalize $\\hat{b}$ to obtain $\\tilde{b}$.\n\n**Test Cases:**\n\n- **Test Case 1:** $P = 5, K = 3, \\lambda = 0.05$.\n  $$\n  X = \\begin{bmatrix} 0.8  0.1  0.2 \\\\ 1.0  0.2  0.3 \\\\ 0.2  0.9  0.1 \\\\ 0.0  0.7  0.9 \\\\ 0.6  0.3  0.4 \\end{bmatrix}, \\ f = \\begin{bmatrix} 0.55 \\\\ 0.68 \\\\ 0.40 \\\\ 0.31 \\\\ 0.47 \\end{bmatrix}, \\ w = \\begin{bmatrix} 1.0 \\\\ 0.8 \\\\ 1.2 \\\\ 1.0 \\\\ 0.9 \\end{bmatrix}\n  $$\n- **Test Case 2:** $P = 5, K = 3, \\lambda = 0.10$.\n  $$\n  X = \\begin{bmatrix} 0.4  0.39  0.10 \\\\ 0.6  0.59  0.20 \\\\ 0.8  0.79  0.30 \\\\ 0.5  0.49  0.40 \\\\ 0.7  0.69  0.50 \\end{bmatrix}, \\ f = \\begin{bmatrix} 0.400 \\\\ 0.592 \\\\ 0.799 \\\\ 0.495 \\\\ 0.693 \\end{bmatrix}, \\ w = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}\n  $$\n- **Test Case 3:** $P = 4, K = 2, \\lambda = 0.01$.\n  $$\n  X = \\begin{bmatrix} 0.2  0.8 \\\\ 0.3  0.7 \\\\ 0.5  0.5 \\\\ 0.9  0.1 \\end{bmatrix}, \\ f = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}, \\ w = \\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ 0.8 \\\\ 0.6 \\end{bmatrix}\n  $$\n\n#### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded:** The problem is firmly located within computational biology and precision medicine. The linear mixture model is a standard and appropriate choice for deconvolution problems like tissue-of-origin inference. The underlying biological premise—that cfDNA fragmentation patterns around promoters are tissue-specific—is based on established research. The use of weighted least-squares to account for measurement reliability and Tikhonov regularization to handle collinearity are canonical methods in numerical analysis and statistical modeling. The non-negativity constraint is a physical necessity. The problem is scientifically sound.\n- **Well-Posed:** The objective function $J(b)$ is a sum of two squared norms, making it strictly convex for $\\lambda > 0$. The constraint set $b \\ge 0$ is a convex set. Minimizing a strictly convex function over a convex set is a well-posed problem that guarantees a unique minimum. The provided algorithm correctly transforms the objective function into a standard non-negative least-squares format, for which efficient and stable solvers exist.\n- **Objective:** The problem statement is formal and mathematical, with all variables, constants, and data defined precisely and unambiguously.\n- **Flaw Checklist:** The problem exhibits no flaws. It is not scientifically unsound, non-formalizable, incomplete, contradictory, unrealistic, ill-posed, or trivial. All data and dimensionalities are consistent.\n\n#### Step 3: Verdict and Action\n\nThe problem is valid. A solution will be developed following the specified algorithm.\n\n### Solution Derivation and Algorithmic Design\n\nThe core task is to solve the constrained optimization problem:\n$$\n\\min_{b \\ge 0} J(b) \\quad \\text{where} \\quad J(b) = \\left\\| W(f - Xb) \\right\\|_2^2 + \\lambda \\left\\| b \\right\\|_2^2\n$$\n\nWe will follow the prescribed algorithmic steps to solve this for the unknown coefficient vector $b$.\n\n**Step 1: Weighting Transformation**\nThe first term of the objective function is a weighted sum of squared residuals. Let $w$ be the vector of weights. The matrix $W = \\mathrm{diag}(w)$ is a diagonal matrix with the weights on its diagonal. Scaling the rows of $f$ and $X$ by the corresponding weights in $w$ simplifies this term. We define:\n- $f' = Wf$\n- $X' = WX$\nComputationally, if $f$ is a column vector and $X$ is a matrix, this is equivalent to element-wise multiplication of each row by the corresponding weight. Specifically, for row $i$, $f'_i = w_i f_i$ and the $i$-th row of $X'$ is $w_i$ times the $i$-th row of $X$.\nWith these new variables, the first term of the objective function becomes:\n$$\n\\left\\| W(f - Xb) \\right\\|_2^2 = \\left\\| Wf - WXb \\right\\|_2^2 = \\left\\| f' - X'b \\right\\|_2^2\n$$\nThis is now a standard, unweighted least-squares term.\n\n**Step 2: Regularization and System Augmentation**\nThe objective function is now $J(b) = \\| f' - X'b \\|_2^2 + \\lambda \\| b \\|_2^2$. The second term, the Tikhonov regularization penalty, can also be expressed as a squared Euclidean norm:\n$$\n\\lambda \\|b\\|_2^2 = \\left(\\sqrt{\\lambda}\\right)^2 \\sum_{k=1}^K b_k^2 = \\sum_{k=1}^K \\left(\\sqrt{\\lambda} b_k\\right)^2 = \\left\\| \\sqrt{\\lambda} b \\right\\|_2^2 = \\left\\| \\sqrt{\\lambda}I_K b \\right\\|_2^2\n$$\nwhere $I_K$ is the $K \\times K$ identity matrix. We can write this as $\\| 0_K - \\sqrt{\\lambda}I_K b \\|_2^2$, where $0_K$ is a zero vector of length $K$.\nThe entire objective function can now be expressed as the sum of two squared norms:\n$$\nJ(b) = \\| f' - X'b \\|_2^2 + \\| 0_K - \\sqrt{\\lambda}I_K b \\|_2^2\n$$\nA key property of the Euclidean norm is that the sum of squared norms of two vectors is equal to the squared norm of their concatenation. We can therefore construct an augmented linear system. Let:\n$$\nA_{\\text{aug}} = \\begin{bmatrix} X' \\\\ \\sqrt{\\lambda} I_K \\end{bmatrix} \\in \\mathbb{R}^{(P+K) \\times K} \\quad \\text{and} \\quad y_{\\text{aug}} = \\begin{bmatrix} f' \\\\ 0_K \\end{bmatrix} \\in \\mathbb{R}^{(P+K)}\n$$\nThe objective function is then equivalent to the squared norm of the residual of this augmented system:\n$$\nJ(b) = \\left\\| y_{\\text{aug}} - A_{\\text{aug}} b \\right\\|_2^2\n$$\nThe original problem has been transformed into a standard non-negative least-squares problem: $\\min_{b \\ge 0} \\| y_{\\text{aug}} - A_{\\text{aug}} b \\|_2^2$.\n\n**Step 3: Solving the Non-Negative Least-Squares (NNLS) Problem**\nThis standard form can be solved efficiently using established numerical algorithms, such as the active-set method developed by Lawson and Hanson. The `scipy.optimize.nnls` function provides an implementation of this algorithm. It takes the matrix $A_{\\text{aug}}$ and vector $y_{\\text{aug}}$ as input and returns the solution vector $\\hat{b}$ which minimizes the norm subject to $\\hat{b} \\ge 0$.\n\n**Step 4: Normalization**\nThe vector $\\hat{b}$ contains the non-negative coefficients that best solve the regularized system. These coefficients represent the relative contributions of each tissue. To interpret them as compositional parts of a whole, they must be normalized to sum to $1$. We compute the sum $S = \\sum_{j=1}^{K} \\hat{b}_j$.\n- If $S  0$, the normalized fractions are $\\tilde{b}_k = \\hat{b}_k / S$ for each $k=1, \\dots, K$.\n- If $S = 0$, which occurs only if $\\hat{b}$ is the zero vector, it implies that no tissue contribution could be detected (e.g., zero signal or signal orthogonal to all tissue profiles). In this case, all fractional contributions are set to zero, so $\\tilde{b}_k = 0$ for all $k$.\n\nThis completes the algorithm. The implementation will systematically apply these four steps to each provided test case. For test case 3, where $f$ is the zero vector, we expect the minimizer of $J(b)=\\|WXb\\|_2^2 + \\lambda\\|b\\|_2^2$ to be $b=0$, as $\\lambda  0$ and the matrix $WX$ has non-negative entries. Thus, we anticipate $\\hat{b}=0$ and consequently $\\tilde{b}=0$.", "answer": "$$ \\boxed{[[0.635399,0.347573,0.017029],[0.398067,0.560731,0.041202],[0.000000,0.000000]]} $$", "id": "4322534"}]}