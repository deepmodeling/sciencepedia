{"hands_on_practices": [{"introduction": "The effectiveness of whole genome sequencing hinges on achieving sufficient coverage across the genome. This foundational exercise guides you through deriving the key relationship between sequencing depth, read length, and genome size from first principles [@problem_id:4397193]. By applying the Poisson distribution, a powerful approximation for random events, you will learn to calculate the probability that a genomic site is missed entirely, a critical metric for quality control and experimental design.", "problem": "In a random-shotgun whole genome sequencing experiment for precision oncology, a haploid human genome of size $G$ base pairs is sequenced by generating $N$ single-end reads of equal length $L$ base pairs. Reads are assumed to be placed uniformly and independently at random along the genome, with start positions equally likely over all possible positions. Assume $G \\gg L$ so that boundary effects are negligible and mapping biases are absent. Coverage depth at a genomic position is defined as the number of reads that overlap that position.\n\nStarting only from the following foundational bases:\n- The Central Dogma establishes that sequencing reports nucleotide content at genomic positions, and the coverage depth is the count of independent observations at a base.\n- Under uniform independent placement, the probability that a given read covers a specified base equals the fraction of start positions that lead to overlap, and the number of covering reads at a base is a sum of independent Bernoulli trials.\n- For rare events in large numbers of trials, a Binomial distribution with parameters $(n, p)$ is approximated by a Poisson distribution with mean $\\lambda = n p$.\n\nTasks:\n1. Derive the expected coverage depth at an arbitrary base in terms of $G$, $N$, and $L$.\n2. Using an appropriate limiting argument justified by the assumptions above, derive the probability that a given base is uncovered (coverage depth equal to zero) in terms of the expected coverage depth.\n3. For a genome of size $G = 3.0 \\times 10^{9}$ base pairs, with $N = 8.0 \\times 10^{7}$ reads of length $L = 150$ base pairs, compute the probability that a given base is uncovered. Round your final probability to four significant figures and express it as a decimal with no units.", "solution": "The problem asks for a derivation of key metrics in a random-shotgun sequencing experiment and a subsequent numerical calculation. The problem is validated as self-contained, scientifically grounded, and well-posed. We shall proceed by addressing the three tasks in order.\n\n### Task 1: Derivation of Expected Coverage Depth\n\nLet $G$ be the size of the haploid genome in base pairs, $N$ be the total number of reads, and $L$ be the length of each read in base pairs. The reads are placed uniformly and independently at random. We are asked to find the expected coverage depth at an arbitrary base. Let us denote the expected coverage depth by $\\lambda$.\n\nConsider a single, arbitrary base at a position $k$ within the genome. Now, consider a single read of length $L$. The start position of this read is chosen uniformly at random from all possible start positions. Assuming a discrete model where reads start at a specific base position, there are $G$ possible start positions, indexed from $1$ to $G$. The problem states that boundary effects are negligible, which is equivalent to assuming the genome is circular or that $G \\gg L$ allows us to ignore edge cases for bases near the ends of a linear genome.\n\nA read of length $L$ starting at position $s$ will cover all bases in the interval $[s, s+L-1]$. For our specific base at position $k$ to be covered by this read, its position must fall within this interval, i.e., $s \\le k \\le s+L-1$. This inequality can be rearranged to define the range of \"successful\" start positions $s$: $k-L+1 \\le s \\le k$.\n\nThe number of integer start positions $s$ that satisfy this condition is $(k) - (k-L+1) + 1 = L$. Since there are $G$ total possible start positions for the read, and each is equally likely, the probability $p$ that a single, randomly placed read covers the specific base at position $k$ is the ratio of favorable outcomes to the total number of outcomes:\n$$p = \\frac{L}{G}$$\nThis probability is the same for any base $k$, given the assumption of no boundary effects.\n\nNow, we consider all $N$ reads. The placement of each read is an independent event. The total coverage depth at position $k$, let's call it $C_k$, is the total number of reads that cover this position. We can define $N$ independent Bernoulli random variables, $X_i$ for $i=1, \\dots, N$, where $X_i=1$ if read $i$ covers base $k$ and $X_i=0$ otherwise. The probability of success for each trial is $P(X_i=1) = p = L/G$.\n\nThe total coverage depth is the sum of these variables: $C_k = \\sum_{i=1}^{N} X_i$.\nBy the linearity of expectation, the expected coverage depth $\\lambda$ is:\n$$\\lambda = E[C_k] = E\\left[\\sum_{i=1}^{N} X_i\\right] = \\sum_{i=1}^{N} E[X_i]$$\nThe expectation of a single Bernoulli variable $X_i$ is its probability of success, $E[X_i] = p = L/G$. Therefore,\n$$\\lambda = \\sum_{i=1}^{N} p = Np = \\frac{NL}{G}$$\nThis is the expected coverage depth at an arbitrary base.\n\n### Task 2: Derivation of the Probability of an Uncovered Base\n\nThe coverage depth $C_k$ at a given base is the sum of $N$ independent and identically distributed Bernoulli trials, each with a success probability of $p=L/G$. Therefore, the random variable $C_k$ follows a Binomial distribution: $C_k \\sim \\text{Binomial}(N, p)$. The probability of observing a coverage of exactly $k$ would be $P(C_k=k) = \\binom{N}{k} p^k (1-p)^{N-k}$.\n\nThe problem statement directs us to use a limiting argument. In a typical whole genome sequencing experiment, the number of reads $N$ is very large (on the order of millions to billions), while the probability $p=L/G$ for a single read to cover a specific base is very small (since $G$ is very large compared to $L$). This scenario—a large number of trials with a small probability of success—is the classic condition under which the Binomial distribution can be accurately approximated by a Poisson distribution.\n\nThe parameter of the approximating Poisson distribution is its mean, $\\lambda$, which must be equal to the mean of the Binomial distribution it is approximating. As derived in Task 1, this mean is $\\lambda = Np = NL/G$.\n\nLet $K$ be the random variable representing the coverage depth at a specific base, following a Poisson distribution with mean $\\lambda$. The probability mass function for the Poisson distribution is:\n$$P(K=k) = \\frac{\\lambda^k \\exp(-\\lambda)}{k!}$$\nwhere $k$ is the number of events (i.e., the coverage depth).\n\nWe are asked for the probability that a given base is uncovered. This corresponds to the case where the coverage depth is zero, i.e., $k=0$. Substituting $k=0$ into the Poisson probability mass function gives:\n$$P(K=0) = \\frac{\\lambda^0 \\exp(-\\lambda)}{0!}$$\nUsing the facts that $\\lambda^0=1$ for any $\\lambda$ and $0!=1$, we find:\n$$P(K=0) = \\exp(-\\lambda)$$\nThis is the probability that a given base is uncovered, expressed in terms of the expected coverage depth $\\lambda$.\n\n### Task 3: Numerical Calculation\n\nWe are given the following values for a sequencing experiment on a human genome:\n- Genome size $G = 3.0 \\times 10^{9}$ base pairs.\n- Number of reads $N = 8.0 \\times 10^{7}$ reads.\n- Read length $L = 150$ base pairs.\n\nFirst, we calculate the expected coverage depth, $\\lambda$, using the formula derived in Task 1:\n$$\\lambda = \\frac{NL}{G} = \\frac{(8.0 \\times 10^{7}) \\times 150}{3.0 \\times 10^{9}}$$\n$$\\lambda = \\frac{1200 \\times 10^{7}}{3.0 \\times 10^{9}} = \\frac{1.2 \\times 10^{10}}{3.0 \\times 10^{9}} = \\frac{12.0}{3.0} = 4.0$$\nThe expected coverage depth is $\\lambda=4.0$.\n\nNext, we use the formula from Task 2 to calculate the probability that a given base is uncovered, $P(K=0)$:\n$$P(K=0) = \\exp(-\\lambda) = \\exp(-4.0)$$\nNow we compute the numerical value:\n$$P(K=0) \\approx 0.0183156388...$$\nThe problem requires this probability to be rounded to four significant figures. The first four significant figures are $1$, $8$, $3$, and $1$. The fifth significant digit is $5$, so we round up the fourth significant digit.\n$$P(K=0) \\approx 0.01832$$\nThis is the final numerical answer.", "answer": "$$\\boxed{0.01832}$$", "id": "4397193"}, {"introduction": "Once reads are mapped, a central task is to discern the true genotype at a given locus from a pileup of potentially noisy data. This practice introduces the Bayesian framework that powers modern variant callers, showing how to formally combine the observed read evidence (the likelihood) with prior genotype probabilities derived from population allele frequencies under Hardy–Weinberg equilibrium [@problem_id:4397190]. You will learn to calculate the posterior probability for each possible genotype, providing a quantitative basis for making the final call while accounting for sequencing error.", "problem": "A biallelic single-nucleotide site in a diploid human genome is sequenced by Whole Genome Sequencing (WGS). You observe a total of $n = 40$ independent, high-quality reads overlapping the site, of which $k = 18$ report the alternate allele and $n - k = 22$ report the reference allele. Assume a per-base, symmetric sequencing error rate $\\epsilon = 10^{-3}$, such that a read from a true reference allele is misread as alternate with probability $\\epsilon$, and a read from a true alternate allele is misread as reference with probability $\\epsilon$. Assume unbiased sampling of alleles from the two chromosomes and independence of reads.\n\nYou are given a population allele frequency $f = 0.01$ for the alternate allele in the relevant ancestry-matched population. Under Hardy–Weinberg equilibrium, use this $f$ to set genotype priors for the three diploid genotypes $G \\in \\{ \\text{hom-ref}, \\text{het}, \\text{hom-alt} \\}$. Using Bayes rule with a binomial sampling model grounded in the above error process, compute the posterior probabilities of each genotype given the data $(n, k)$, and determine which genotype is most probable.\n\nReport only the posterior probability of the most probable genotype, expressed as a decimal number and rounded to $4$ significant figures. No units are required.", "solution": "The user wants to solve a problem in genomic diagnostics using Bayesian inference.\n\n### Step 1: Extract Givens\n- Total number of reads: $n = 40$.\n- Number of reads reporting the alternate allele: $k = 18$.\n- Number of reads reporting the reference allele: $n - k = 22$.\n- Symmetric sequencing error rate: $\\epsilon = 10^{-3}$.\n- Population allele frequency for the alternate allele: $f = 0.01$.\n- Genotypes: $G \\in \\{ \\text{hom-ref}, \\text{het}, \\text{hom-alt} \\}$. Let these be denoted $G_0$, $G_1$, and $G_2$ respectively.\n- The model assumptions are:\n    1. Hardy–Weinberg equilibrium for genotype priors.\n    2. Binomial sampling model for read counts.\n    3. Independent reads.\n    4. Unbiased sampling of alleles from the two chromosomes.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, situated in the standard practice of variant calling in genomics. It uses established principles like Hardy-Weinberg equilibrium and Bayesian inference with a binomial likelihood, which are fundamental to statistical genetics. The problem is well-posed, providing all necessary parameters ($n$, $k$, $\\epsilon$, $f$) and a clear objective. The language is precise and objective. There are no contradictions, missing information, or violations of scientific principles. The problem is therefore deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will now proceed with a full solution.\n\n### Solution Derivation\nThe objective is to compute the posterior probabilities of the three possible genotypes, $P(G|D)$, where $G$ is the genotype and $D$ is the observed data $(n=40, k=18)$. We use Bayes' rule:\n$$P(G_i|D) = \\frac{P(D|G_i) P(G_i)}{\\sum_{j=0}^{2} P(D|G_j) P(G_j)}$$\nfor $i \\in \\{0, 1, 2\\}$, where $G_0 = \\text{hom-ref}$, $G_1 = \\text{het}$, and $G_2 = \\text{hom-alt}$.\n\n#### 1. Genotype Priors, $P(G_i)$\nUnder Hardy-Weinberg equilibrium, the genotype frequencies are determined by the allele frequencies. The frequency of the alternate allele is given as $f = 0.01$. The frequency of the reference allele is $1-f = 0.99$.\nThe prior probabilities for the three genotypes are:\n-   $P(G_0) = P(\\text{hom-ref}) = (1-f)^2 = (0.99)^2 = 0.9801$.\n-   $P(G_1) = P(\\text{het}) = 2f(1-f) = 2(0.01)(0.99) = 0.0198$.\n-   $P(G_2) = P(\\text{hom-alt}) = f^2 = (0.01)^2 = 0.0001$.\nThe sum of priors is $0.9801 + 0.0198 + 0.0001 = 1.0000$.\n\n#### 2. Likelihoods, $P(D|G_i)$\nThe likelihood of observing $k$ alternate reads out of $n$ total reads follows a binomial distribution:\n$$P(D|G_i) = P(k|n, \\theta_{G_i}) = \\binom{n}{k} \\theta_{G_i}^k (1-\\theta_{G_i})^{n-k}$$\nwhere $\\theta_{G_i}$ is the probability of a single read being an alternate allele, given the genotype $G_i$.\n\n- For $G_0$ (hom-ref, genotype RR): Both chromosomes carry the reference allele. An alternate read can only be observed due to a sequencing error. Thus, the probability of a read being alternate is $\\theta_{G_0} = \\epsilon$.\n- For $G_1$ (het, genotype RA): One chromosome is reference, one is alternate. Assuming unbiased sampling from the two chromosomes, a read originates from the reference chromosome with probability $0.5$ and from the alternate chromosome with probability $0.5$.\n    - The probability of reading an alternate allele is: $P(\\text{read from A})P(\\text{not error}) + P(\\text{read from R})P(\\text{error}) = 0.5 \\times (1-\\epsilon) + 0.5 \\times \\epsilon = 0.5$.\n    - Thus, $\\theta_{G_1} = 0.5$.\n- For $G_2$ (hom-alt, genotype AA): Both chromosomes carry the alternate allele. A reference read is observed only due to an error. The probability of a read being alternate is $\\theta_{G_2} = 1-\\epsilon$.\n\nSubstituting the given values $n=40$, $k=18$, and $\\epsilon=10^{-3}$:\n- $\\theta_{G_0} = 0.001$.\n- $\\theta_{G_1} = 0.5$.\n- $\\theta_{G_2} = 1 - 0.001 = 0.999$.\n\nThe likelihoods are:\n- $P(D|G_0) = \\binom{40}{18} (0.001)^{18} (0.999)^{22}$.\n- $P(D|G_1) = \\binom{40}{18} (0.5)^{18} (0.5)^{22} = \\binom{40}{18} (0.5)^{40}$.\n- $P(D|G_2) = \\binom{40}{18} (0.999)^{18} (0.001)^{22}$.\n\n#### 3. Posterior Probabilities, $P(G_i|D)$\nWe need to calculate the unnormalized posterior probabilities, which are proportional to the product of the likelihood and the prior, $P(D|G_i)P(G_i)$. The term $\\binom{40}{18}$ is a common factor and can be omitted when comparing the relative magnitudes.\n\n- Unnormalized posterior for $G_0$: $T_0 \\propto (0.001)^{18}(0.999)^{22} \\times 0.9801$.\n- Unnormalized posterior for $G_1$: $T_1 \\propto (0.5)^{40} \\times 0.0198$.\n- Unnormalized posterior for $G_2$: $T_2 \\propto (0.999)^{18}(0.001)^{22} \\times 0.0001$.\n\nThe observed alternate allele fraction is $k/n = 18/40 = 0.45$. This value is very close to $\\theta_{G_1}=0.5$, and very far from $\\theta_{G_0}=0.001$ and $\\theta_{G_2}=0.999$. This suggests that the likelihood $P(D|G_1)$ will be substantially larger than $P(D|G_0)$ and $P(D|G_2)$. The prior for $G_0$ is the largest, but the likelihood term for $G_0$ is extremely small, containing a factor of $(10^{-3})^{18} = 10^{-54}$. The prior for $G_1$ is small, but its likelihood is much larger. The prior and likelihood for $G_2$ are both very small. Thus, we expect the heterozygous genotype $G_1$ to be the most probable.\n\nThe posterior probability for the most probable genotype, $G_1$, is:\n$$P(G_1|D) = \\frac{P(D|G_1)P(G_1)}{P(D|G_0)P(G_0) + P(D|G_1)P(G_1) + P(D|G_2)P(G_2)}$$\nDividing the numerator and denominator by $P(D|G_1)P(G_1)$:\n$$P(G_1|D) = \\frac{1}{\\frac{P(D|G_0)P(G_0)}{P(D|G_1)P(G_1)} + 1 + \\frac{P(D|G_2)P(G_2)}{P(D|G_1)P(G_1)}}$$\nLet's compute the two ratios. The binomial coefficient $\\binom{40}{18}$ cancels out.\n\nRatio 1:\n$$\\frac{P(D|G_0)P(G_0)}{P(D|G_1)P(G_1)} = \\frac{(0.001)^{18}(0.999)^{22} \\times (0.99)^2}{(0.5)^{40} \\times 2(0.01)(0.99)} = \\frac{(0.001)^{18}(0.999)^{22} \\times 0.99}{(0.5)^{40} \\times 0.02}$$\n- Numerator: $(10^{-3})^{18} \\times (0.999)^{22} \\times 0.99 \\approx 10^{-54} \\times 0.9782 \\times 0.99 \\approx 9.684 \\times 10^{-55}$.\n- Denominator: $(0.5)^{40} \\times 0.02 \\approx (9.095 \\times 10^{-13}) \\times 0.02 \\approx 1.819 \\times 10^{-14}$.\n- Ratio 1: $\\frac{9.684 \\times 10^{-55}}{1.819 \\times 10^{-14}} \\approx 5.324 \\times 10^{-41}$.\n\nRatio 2:\n$$\\frac{P(D|G_2)P(G_2)}{P(D|G_1)P(G_1)} = \\frac{(0.999)^{18}(0.001)^{22} \\times (0.01)^2}{(0.5)^{40} \\times 2(0.01)(0.99)} = \\frac{(0.999)^{18}(0.001)^{22} \\times 0.01}{(0.5)^{40} \\times 1.98}$$\n- Numerator: $(0.999)^{18} \\times (10^{-3})^{22} \\times 0.01 \\approx 0.9822 \\times 10^{-66} \\times 10^{-2} \\approx 9.822 \\times 10^{-69}$.\n- Denominator: $(0.5)^{40} \\times 1.98 \\approx (9.095 \\times 10^{-13}) \\times 1.98 \\approx 1.801 \\times 10^{-12}$.\n- Ratio 2: $\\frac{9.822 \\times 10^{-69}}{1.801 \\times 10^{-12}} \\approx 5.454 \\times 10^{-57}$.\n\nBoth ratios are exceedingly small, confirming that the unnormalized posterior for $G_1$ dominates.\nNow, we calculate the posterior probability of $G_1$:\n$$P(G_1|D) = \\frac{1}{1 + 5.324 \\times 10^{-41} + 5.454 \\times 10^{-57}}$$\nThis value is indistinguishable from $1$ for any practical number of significant figures.\n$P(G_1|D) \\approx 1 - 5.324 \\times 10^{-41}$.\nRounding to $4$ significant figures, we get $1.000$.\n\nThe most probable genotype is heterozygous ($G_1$), and its posterior probability is extremely close to 1.\nThe probability rounded to 4 significant figures is $1.000$.", "answer": "$$\\boxed{1.000}$$", "id": "4397190"}, {"introduction": "Beyond single nucleotide variants, WGS enables the discovery of large-scale structural changes that are often implicated in diseases like cancer. This advanced exercise introduces you to the design of a Hidden Markov Model (HMM), a powerful statistical tool for segmenting the genome into regions of distinct copy number states [@problem_id:4397184]. By integrating complementary data streams—read depth and B-allele frequency—you will see how it is possible to identify not only simple gains and losses but also complex events like copy-neutral loss-of-heterozygosity.", "problem": "You are given two genome-wide signal tracks obtained from whole genome sequencing: read depth and B-allele frequency. The read depth per bin is the number of reads mapped to a genomic bin after standard normalization. The B-allele frequency (BAF) per bin is the fraction of reads supporting the alternate allele at known heterozygous single nucleotide polymorphism sites. To segment the genome into regions of constant copy-number or allelic state, you must design and implement a Hidden Markov Model (HMM) whose hidden state represents the copy-number state and whose emissions jointly use depth and BAF imbalance.\n\nFundamental base and assumptions:\n- The read count in a bin arises from many independent sampling events; by the Central Limit Theorem, the normalized depth per bin $D_t$ is well-approximated by a Gaussian distribution centered at a mean proportional to the copy number. Let $D_t \\sim \\mathcal{N}(\\mu_{d}(X_t), \\sigma_{d}(X_t)^2)$.\n- The B-allele frequency at a heterozygous locus is modeled as a Binomial proportion with expectation depending on allelic composition. Aggregated over many loci per bin, the deviation from $0.5$ (allelic balance) is approximately Gaussian. Define the imbalance feature $B_t = |BAF_t - 0.5|$, and model $B_t \\sim \\mathcal{N}(\\mu_{b}(X_t), \\sigma_{b}(X_t)^2)$.\n- The Hidden Markov Model (HMM) has a discrete hidden state $X_t \\in \\{\\text{CN1}, \\text{CN2}, \\text{CN3}, \\text{LOH}\\}$ where CN1 denotes hemizygous loss, CN2 denotes diploid balanced, CN3 denotes single-copy gain, and LOH denotes copy-neutral loss-of-heterozygosity. Emissions are conditionally independent given the state: $p(D_t, B_t \\mid X_t) = p(D_t \\mid X_t) \\cdot p(B_t \\mid X_t)$ with Gaussian factors as above.\n\nFalse positive control requirement:\n- Let the desired per-bin false positive breakpoint rate be $\\alpha$ (expressed as a decimal). To penalize spurious state changes, use a homogeneous transition model where the self-transition probability is $P(X_t = s \\mid X_{t-1} = s) = 1 - p_t$ and transitions to any other state $s' \\neq s$ have equal probability $P(X_t = s' \\mid X_{t-1} = s) = \\frac{p_t}{K-1}$, where $K$ is the number of states. Choose $p_t$ based on $\\alpha$ and the emission separations. A principled penalty is captured by the log prior ratio threshold $$\\tau = \\log\\left(\\frac{1-p_t}{p_t/(K-1)}\\right),$$ which the Viterbi algorithm implicitly applies when comparing paths. For small $\\alpha$, setting $p_t = \\alpha$ yields a conservative barrier against transitions, which reduces false positive breakpoints in no-change segments.\n\nTask:\n- Implement an HMM with the four states above and Gaussian emission parameters given below. Use the Viterbi algorithm to infer the most likely state sequence from $(D_t, B_t)$ across bins.\n- Incorporate both depth and BAF tracks in the emission likelihood as independent Gaussian factors so that copy-neutral loss-of-heterozygosity (LOH) is identifiable via $B_t$ even when $D_t$ is unchanged.\n- Tune the transition parameter $p_t$ to a small value to control the false positive breakpoint rate. Use $K = 4$ and $\\alpha = 0.001$ and set $p_t = \\alpha$. The initial state distribution should be uniform over the $K$ states.\n\nEmission parameters (means and standard deviations) to use:\n- CN1: $\\mu_{d} = 0.5$, $\\sigma_{d} = 0.05$; $\\mu_{b} = 0.45$, $\\sigma_{b} = 0.05$.\n- CN2: $\\mu_{d} = 1.0$, $\\sigma_{d} = 0.07$; $\\mu_{b} = 0.00$, $\\sigma_{b} = 0.02$.\n- CN3: $\\mu_{d} = 1.5$, $\\sigma_{d} = 0.07$; $\\mu_{b} = 0.17$, $\\sigma_{b} = 0.05$.\n- LOH: $\\mu_{d} = 1.0$, $\\sigma_{d} = 0.07$; $\\mu_{b} = 0.45$, $\\sigma_{b} = 0.05$.\n\nSegmentation output specification:\n- After computing the most likely state path, define the number of breakpoints as the count of indices $t$ where $X_t \\neq X_{t-1}$ for $t \\geq 2$.\n- Your program must simulate synthetic test cases with Gaussian noise using the parameters above and a fixed pseudorandom seed for reproducibility.\n\nTest suite:\n- Case $1$ (happy path): $200$ bins CN2, then $100$ bins CN3, then $200$ bins CN2.\n- Case $2$ (boundary no-change): $500$ bins CN2 only.\n- Case $3$ (copy-neutral LOH): $150$ bins CN2, then $50$ bins LOH, then $150$ bins CN2.\n- Case $4$ (subtle loss): $100$ bins CN2, then $60$ bins CN1, then $100$ bins CN2.\n\nFor each case, generate $(D_t, B_t)$ by sampling from the corresponding Gaussian emission parameters. Use a fixed seed so that the output is deterministic.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). Each result is the integer number of inferred breakpoints for the corresponding test case, in order $1$ through $4$.\n\nExpress any rates or probabilities as decimals, not with a percentage sign. No physical units are required in this problem.", "solution": "The problem requires the design and implementation of a Hidden Markov Model (HMM) to segment genomic signal tracks, specifically normalized read depth and B-allele frequency, into regions of distinct copy-number states. The inference of the most likely sequence of hidden states given a sequence of observations is accomplished using the Viterbi algorithm.\n\nFirst, we formalize the HMM based on the provided specifications. An HMM is defined by a set of states, an initial state probability distribution, a state transition probability matrix, and emission probabilities that define the likelihood of an observation given a state.\n\n- **States ($X_t$)**: The hidden state at genomic bin $t$ belongs to the set $S = \\{\\text{CN1}, \\text{CN2}, \\text{CN3}, \\text{LOH}\\}$. We can map these to integer indices for computation: $S = \\{0, 1, 2, 3\\}$. The total number of states is $K=4$.\n\n- **Observations ($O_t$)**: The observation at bin $t$ is a two-dimensional vector $O_t = (D_t, B_t)$, where $D_t$ is the normalized read depth and $B_t$ is the B-allele frequency imbalance, defined as $B_t = |BAF_t - 0.5|$.\n\n- **Initial State Distribution ($\\pi$)**: The probability distribution for the state at the first bin ($t=1$) is uniform across all $K$ states.\n$$ \\pi_i = P(X_1 = i) = \\frac{1}{K} = \\frac{1}{4} \\quad \\text{for } i \\in \\{0, 1, 2, 3\\} $$\n\n- **Transition Probabilities ($A$)**: The model uses a homogeneous transition probability matrix, meaning the probability of transitioning from state $i$ to state $j$ is independent of time $t$. The probability of staying in the same state is $1 - p_t$, and the probability of moving to any of the $K-1$ other states is uniformly $\\frac{p_t}{K-1}$. With the false positive rate $\\alpha = 0.001$ and the setting $p_t = \\alpha$:\n$$ A_{ij} = P(X_t = j \\mid X_{t-1} = i) = \\begin{cases} 1 - p_t  \\text{if } j=i \\\\ \\frac{p_t}{K-1}  \\text{if } j \\neq i \\end{cases} $$\nSubstituting the values $p_t = 0.001$ and $K=4$:\n$$ A_{ij} = \\begin{cases} 0.999  \\text{if } j=i \\\\ \\frac{0.001}{3}  \\text{if } j \\neq i \\end{cases} $$\nThis structure heavily penalizes state changes, which is desired to control for false positive breakpoints.\n\n- **Emission Probabilities ($b_i(O_t)$)**: The emission probability is the likelihood of observing $O_t=(D_t, B_t)$ given the hidden state is $X_t=i$. The problem specifies that the depth and imbalance features are conditionally independent given the state and that each follows a Gaussian distribution.\n$$ b_i(O_t) = p(D_t, B_t \\mid X_t=i) = p(D_t \\mid X_t=i) \\cdot p(B_t \\mid X_t=i) $$\nEach factor is a Gaussian probability density function (PDF):\n$$ p(y \\mid X_t=i) = \\mathcal{N}(y; \\mu_{y,i}, \\sigma_{y,i}^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y,i}^2}} \\exp\\left(-\\frac{(y - \\mu_{y,i})^2}{2\\sigma_{y,i}^2}\\right) $$\nwhere $y$ can be either $D_t$ or $B_t$, and $(\\mu_{y,i}, \\sigma_{y,i})$ are the mean and standard deviation for the corresponding feature $y$ and state $i$. The specific emission parameters $(\\mu_d, \\sigma_d, \\mu_b, \\sigma_b)$ are provided for each of the $K=4$ states.\n\nThe core of the task is to find the most probable sequence of states $\\mathbf{X}^* = (X_1^*, \\dots, X_T^*)$ that could have generated a given sequence of observations $\\mathbf{O} = (O_1, \\dots, O_T)$. This is achieved using the Viterbi algorithm. To prevent numerical underflow with long sequences of small probabilities, the algorithm is implemented using log-probabilities.\n\nThe Viterbi algorithm proceeds in three steps:\n\n1.  **Initialization ($t=1$)**: We compute the log-probability of starting in each state $i$ and observing $O_1$. We define a matrix $V_{i,t}$ to store the maximum log-probability of any path ending in state $i$ at time $t$.\n$$ V_{i,1} = \\log(\\pi_i) + \\log(b_i(O_1)) \\quad \\text{for } i \\in \\{0, 1, 2, 3\\} $$\nA backpointer matrix $\\psi_{i,t}$ is initialized to store the optimal preceding state. For $t=1$, it is not needed, so we can set $\\psi_{i,1} = 0$.\n\n2.  **Recursion ($t=2, \\dots, T$)**: For each subsequent time step $t$ and for each state $j$, we find the preceding state $i$ that maximizes the path probability. The value $V_{j,t}$ is the log-probability of the best path to state $j$ at time $t$, which is the sum of the log-emission probability for state $j$ at time $t$ and the maximum log-probability of reaching state $j$ from any state $i$ at time $t-1$.\n$$ V_{j,t} = \\max_{i \\in S} \\left( V_{i, t-1} + \\log(A_{ij}) \\right) + \\log(b_j(O_t)) $$\nThe backpointer for state $j$ at time $t$ records which state $i$ yielded this maximum.\n$$ \\psi_{j,t} = \\arg\\max_{i \\in S} \\left( V_{i, t-1} + \\log(A_{ij}) \\right) $$\n\n3.  **Termination and Path Backtracking**: After completing the recursion up to time $T$, the optimal final state $X_T^*$ is the one with the highest overall log-probability.\n$$ X_T^* = \\arg\\max_{j \\in S} (V_{j,T}) $$\nThe rest of the optimal path is recovered by tracing backwards from $t=T-1$ down to $1$ using the backpointers.\n$$ X_t^* = \\psi_{X_{t+1}^*, t+1} \\quad \\text{for } t=T-1, T-2, \\dots, 1 $$\n\nThe final implementation will first generate synthetic data for the four test cases specified. For each bin, a state is assigned according to the case definition, and the observation $(D_t, B_t)$ is drawn from the corresponding Gaussian distributions using a fixed random seed for reproducibility. The Viterbi algorithm is then applied to these observation sequences to infer the most likely state path. Finally, the number of breakpoints is calculated by counting the instances where $X_t^* \\neq X_{t-1}^*$ for $t \\in \\{2, \\dots, T\\}$. This procedure objectively assesses the model's ability to correctly segment the genome under different scenarios, including contiguous regions, copy number changes, and copy-neutral LOH.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Implements an HMM with a Viterbi decoder to segment simulated genomic data,\n    and reports the number of breakpoints found for several test cases.\n    \"\"\"\n\n    # --- HMM Parameters ---\n    # Four states: 0:CN1, 1:CN2, 2:CN3, 3:LOH\n    K = 4\n    STATES = np.arange(K)\n\n    # Emission parameters: (mu_d, sigma_d, mu_b, sigma_b) for each state\n    EMISSION_PARAMS = [\n        (0.5, 0.05, 0.45, 0.05),  # CN1\n        (1.0, 0.07, 0.00, 0.02),  # CN2\n        (1.5, 0.07, 0.17, 0.05),  # CN3\n        (1.0, 0.07, 0.45, 0.05),  # LOH\n    ]\n\n    # Initial state probabilities (uniform)\n    pi = np.full(K, 1.0 / K)\n    log_pi = np.log(pi)\n\n    # Transition probabilities\n    alpha = 0.001\n    p_t = alpha\n    log_A = np.full((K, K), np.log(p_t / (K - 1)))\n    np.fill_diagonal(log_A, np.log(1 - p_t))\n\n    # --- Test Cases ---\n    test_cases_defs = [\n        # Case 1: Happy path\n        # 200 bins CN2, then 100 bins CN3, then 200 bins CN2\n        {'name': 'Case 1', 'segments': [(200, 1), (100, 2), (200, 1)]},\n        # Case 2: Boundary no-change\n        # 500 bins CN2 only\n        {'name': 'Case 2', 'segments': [(500, 1)]},\n        # Case 3: Copy-neutral LOH\n        # 150 bins CN2, then 50 bins LOH, then 150 bins CN2\n        {'name': 'Case 3', 'segments': [(150, 1), (50, 3), (150, 1)]},\n        # Case 4: Subtle loss\n        # 100 bins CN2, then 60 bins CN1, then 100 bins CN2\n        {'name': 'Case 4', 'segments': [(100, 1), (60, 0), (100, 1)]},\n    ]\n\n    # Fixed seed for reproducibility\n    rng = np.random.default_rng(seed=123)\n    \n    results = []\n\n    for case in test_cases_defs:\n        # 1. Generate synthetic data\n        true_states = []\n        for length, state_idx in case['segments']:\n            true_states.extend([state_idx] * length)\n        true_states = np.array(true_states)\n        \n        T = len(true_states)\n        observations = np.zeros((T, 2)) # Columns: D, B\n\n        for t in range(T):\n            state = true_states[t]\n            mu_d, sigma_d, mu_b, sigma_b = EMISSION_PARAMS[state]\n            observations[t, 0] = rng.normal(loc=mu_d, scale=sigma_d)\n            observations[t, 1] = rng.normal(loc=mu_b, scale=sigma_b)\n\n        # 2. Viterbi Algorithm\n        \n        # Pre-compute all log emission probabilities\n        log_emission_probs = np.zeros((T, K))\n        for t in range(T):\n            obs_d, obs_b = observations[t]\n            for s in STATES:\n                mu_d, sigma_d, mu_b, sigma_b = EMISSION_PARAMS[s]\n                log_p_d = norm.logpdf(obs_d, loc=mu_d, scale=sigma_d)\n                log_p_b = norm.logpdf(obs_b, loc=mu_b, scale=sigma_b)\n                log_emission_probs[t, s] = log_p_d + log_p_b\n        \n        # Viterbi matrices\n        V = np.zeros((T, K))  # Viterbi log-probability matrix\n        psi = np.zeros((T, K), dtype=int)  # Backpointer matrix\n\n        # Initialization step (t=0)\n        V[0, :] = log_pi + log_emission_probs[0, :]\n        psi[0, :] = 0\n\n        # Recursion step (t=1 to T-1)\n        for t in range(1, T):\n            for j in STATES:\n                # log_prob = V[t-1, :] + log_A[:, j]\n                # max_prob_idx = np.argmax(log_prob)\n                # psi[t, j] = max_prob_idx\n                # V[t, j] = log_prob[max_prob_idx] + log_emission_probs[t, j]\n                v_t_minus_1 = V[t-1, :]\n                trans_prob_to_j = log_A[:, j]\n                log_prob_sequence = v_t_minus_1 + trans_prob_to_j\n                \n                psi[t, j] = np.argmax(log_prob_sequence)\n                V[t, j] = np.max(log_prob_sequence) + log_emission_probs[t, j]\n\n        # Termination and Backtracking\n        path = np.zeros(T, dtype=int)\n        path[T-1] = np.argmax(V[T-1, :])\n        \n        for t in range(T-2, -1, -1):\n            path[t] = psi[t+1, path[t+1]]\n\n        # 3. Calculate breakpoints\n        breakpoints = np.sum(path[:-1] != path[1:])\n        results.append(breakpoints)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4397184"}]}