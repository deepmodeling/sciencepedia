{"hands_on_practices": [{"introduction": "Understanding the relationship between high-level sequencing run parameters and the resulting per-sample coverage is a cornerstone of effective experimental design. Before committing resources to a sequencing experiment, it's crucial to perform \"back-of-the-envelope\" calculations to ensure the planned sequencing yield will be sufficient for the study's goals. This first exercise guides you through deriving and applying the fundamental formula that connects total instrument output, sample multiplexing, and capture efficiency to the expected mean depth of coverage, a critical first-pass metric for assessing sequencing power [@problem_id:4380718].", "problem": "A clinical laboratory performing Next-Generation Sequencing (NGS) uses a hybrid-capture gene panel of total target size $T$ base pairs to multiplex $S$ samples in a single run. The instrument produces a total of $R$ sequenced bases (the sum of all base calls across both ends and all reads after trimming and filtering). After alignment, an empirically measured on-target rate $\\alpha$ is defined as the fraction of these sequenced bases that map within the intended target intervals. Assume equal library representation so that each sample receives an equal share of the sequenced bases, and assume uniform random sampling of bases across the target regions so that expected coverage is evenly distributed across the target.\n\nUsing only the fundamental definition that the expected mean depth $\\bar{d}$ of coverage for a sample is the ratio of the total number of on-target bases attributed to that sample divided by the target size for that sample, derive an expression for $\\bar{d}$ in terms of $R$, $S$, $\\alpha$, and $T$.\n\nThen, evaluate your expression for the following realistic run parameters: $R = 9.6 \\times 10^{10}$, $S = 30$, $\\alpha = 0.72$, and $T = 4.2 \\times 10^{6}$. Round your final numerical answer for $\\bar{d}$ to four significant figures. Express the final expected depth as a dimensionless number.", "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\nThe data, variables, definitions, and assumptions provided in the problem statement are as follows:\n- $T$: total target size in base pairs.\n- $S$: number of multiplexed samples.\n- $R$: total number of sequenced bases produced by the instrument.\n- $\\alpha$: empirically measured on-target rate, defined as the fraction of sequenced bases that map within the intended target intervals.\n- Assumption 1: Equal library representation, meaning each sample receives an equal share of the sequenced bases.\n- Assumption 2: Uniform random sampling of bases across the target regions, meaning expected coverage is evenly distributed.\n- Definition: The expected mean depth $\\bar{d}$ for a sample is the ratio of the total number of on-target bases attributed to that sample divided by the target size for that sample.\n- Task 1: Derive an expression for $\\bar{d}$ in terms of $R$, $S$, $\\alpha$, and $T$.\n- Task 2: Evaluate the expression for the parameters: $R = 9.6 \\times 10^{10}$, $S = 30$, $\\alpha = 0.72$, and $T = 4.2 \\times 10^{6}$.\n- Task 3: Round the final numerical answer for $\\bar{d}$ to four significant figures.\n- Task 4: Express the final expected depth as a dimensionless number.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded:** The problem is firmly rooted in the principles of Next-Generation Sequencing (NGS) data analysis, a core component of modern genomics and precision medicine. The concepts of sequencing yield ($R$), multiplexing ($S$), target size ($T$), on-target rate ($\\alpha$), and mean coverage depth ($\\bar{d}$) are standard metrics used in this field. The provided numerical values are realistic for a high-throughput sequencing experiment (e.g., a multi-sample exome or large gene panel run on an Illumina NovaSeq instrument).\n- **Well-Posed:** The problem provides a clear and unambiguous definition for the quantity to be derived, $\\bar{d}$. It supplies all necessary variables ($R$, $S$, $\\alpha$, $T$) and simplifying assumptions (equal representation, uniform distribution) required to construct a unique mathematical relationship. The existence and uniqueness of a solution are guaranteed.\n- **Objective:** The problem is stated in precise, quantitative, and unbiased technical language. It requests a derivation and a calculation based on established definitions, free of any subjective or speculative elements.\n\nThe problem does not exhibit any of the flaws listed in the instructions. It is factually sound, formally solvable, complete, realistic, and well-structured.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Derivation and Solution\nThe objective is to derive an expression for the expected mean depth of coverage, $\\bar{d}$, for a single sample. We begin from the fundamental quantities provided.\n\nThe total number of sequenced bases generated by the instrument for all samples combined is $R$.\n\nThe on-target rate, $\\alpha$, is the fraction of these bases that align to the targeted genomic regions. Therefore, the total number of on-target bases across all samples, which we can denote as $B_{\\text{on-target, total}}$, is given by:\n$$B_{\\text{on-target, total}} = \\alpha R$$\n\nThe problem states that there are $S$ samples multiplexed in the run and assumes equal library representation. This means the total on-target bases are distributed equally among the $S$ samples. The number of on-target bases for a single sample, $B_{\\text{on-target, sample}}$, is therefore:\n$$B_{\\text{on-target, sample}} = \\frac{B_{\\text{on-target, total}}}{S} = \\frac{\\alpha R}{S}$$\n\nThe problem provides a precise definition for the expected mean depth, $\\bar{d}$: it is the ratio of the total number of on-target bases for that sample to the target size, $T$. The target size $T$ is the length of the genomic region for which coverage is being measured.\n$$\\bar{d} = \\frac{B_{\\text{on-target, sample}}}{T}$$\nThe units of the numerator are bases, and the units of the denominator are base pairs. The resulting quantity $\\bar{d}$ is dimensionless, representing the average number of times each base in the target region is sequenced. This is often denoted with an \"X\" (e.g., $500$X coverage).\n\nSubstituting the expression for $B_{\\text{on-target, sample}}$ into the equation for $\\bar{d}$, we arrive at the general formula:\n$$\\bar{d} = \\frac{\\left(\\frac{\\alpha R}{S}\\right)}{T}$$\nThis simplifies to the final expression for the expected mean depth of coverage per sample:\n$$\\bar{d} = \\frac{\\alpha R}{S T}$$\n\nNow, we evaluate this expression using the provided numerical parameters:\n- $R = 9.6 \\times 10^{10}$\n- $S = 30$\n- $\\alpha = 0.72$\n- $T = 4.2 \\times 10^{6}$\n\nSubstituting these values into the derived formula:\n$$\\bar{d} = \\frac{(0.72)(9.6 \\times 10^{10})}{(30)(4.2 \\times 10^{6})}$$\n\nFirst, we calculate the numerator and the denominator separately.\nNumerator:\n$$0.72 \\times 9.6 \\times 10^{10} = 6.912 \\times 10^{10}$$\nDenominator:\n$$30 \\times 4.2 \\times 10^{6} = 126 \\times 10^{6} = 1.26 \\times 10^{8}$$\n\nNow, we perform the division:\n$$\\bar{d} = \\frac{6.912 \\times 10^{10}}{1.26 \\times 10^{8}}$$\n$$\\bar{d} = \\frac{6.912}{1.26} \\times 10^{(10-8)}$$\n$$\\bar{d} = 5.4857142... \\times 10^{2}$$\n$$\\bar{d} = 548.57142...$$\n\nThe problem requires the final answer to be rounded to four significant figures. The first four significant figures are $5$, $4$, $8$, and $5$. The fifth digit is $7$, which is greater than or equal to $5$, so we round up the fourth significant digit.\n$$\\bar{d} \\approx 548.6$$\nThis value represents an expected mean coverage depth of approximately $548.6$X.", "answer": "$$\n\\boxed{548.6}\n$$", "id": "4380718"}, {"introduction": "While mean depth provides a useful summary, it can be misleading as it often masks significant non-uniformity in coverage across a targeted region. In clinical diagnostics, what truly matters is not the average depth, but the proportion of the panel that is covered at or above a minimum depth required for confident variant calling. This practice moves beyond the simple average to explore the statistical distribution of coverage, introducing you to the concepts of median depth and breadth of coverage, and asking you to interpret their divergence in the context of inherent biochemical biases in NGS workflows [@problem_id:4380739].", "problem": "A clinical-grade Next-Generation Sequencing (NGS) targeted panel for precision oncology covers $1\\,\\text{megabase}$ ($1{,}000{,}000$ bases) of exonic regions. Post-processing includes removal of polymerase chain reaction (PCR) duplicates and filtering to retain only uniquely aligned read pairs with mapping quality $\\ge$ a stringent threshold, yielding a per-base depth profile (number of independent read observations per base). Consider the following empirically observed depth distribution across the panel, where each base has an integer depth value and the counts sum exactly to the panel size:\n- $40{,}000$ bases at depth $5\\times$,\n- $120{,}000$ bases at depth $10\\times$,\n- $180{,}000$ bases at depth $20\\times$,\n- $300{,}000$ bases at depth $30\\times$,\n- $200{,}000$ bases at depth $40\\times$,\n- $160{,}000$ bases at depth $80\\times$.\n\nUsing only fundamental definitions of per-base depth, central tendency, and coverage breadth, compute the following metrics:\n- the mean depth across the panel,\n- the median depth across the panel,\n- the fraction of bases with depth $\\ge 30\\times$.\n\nReport the mean and median depth as fold coverage ($\\times$). Report the fraction of bases with depth $\\ge 30\\times$ as a decimal (do not use a percentage symbol). If any computation yields a non-terminating decimal, round that value to four significant figures; otherwise, report exact values. Finally, provide a brief interpretation, grounded in the definitions and widely accepted properties of NGS capture and amplification processes, explaining how and why these metrics can differ in a targeted panel used for clinical variant detection.", "solution": "The problem provides a discrete distribution of sequencing depth for a targeted genomic panel and asks for the computation of three key statistical metrics: the mean depth, the median depth, and the fraction of bases covered at or above a specified threshold. Additionally, an interpretation of these metrics in the context of Next-Generation Sequencing (NGS) technology is required.\n\nThe panel size is given as $N = 1{,}000{,}000$ bases. The depth distribution is provided as a set of pairs $(n_i, d_i)$, where $n_i$ is the number of bases having a depth of $d_i$. The data points are:\n\\begin{itemize}\n    \\item $(n_1, d_1) = (40000, 5)$\n    \\item $(n_2, d_2) = (120000, 10)$\n    \\item $(n_3, d_3) = (180000, 20)$\n    \\item $(n_4, d_4) = (300000, 30)$\n    \\item $(n_5, d_5) = (200000, 40)$\n    \\item $(n_6, d_6) = (160000, 80)$\n\\end{itemize}\nThe total number of bases is $\\sum_{i=1}^{6} n_i = 40000 + 120000 + 180000 + 300000 + 200000 + 160000 = 1000000$, which is consistent with the given panel size $N$.\n\n1.  **Mean Depth ($\\bar{d}$)**\n\nThe mean depth is the weighted average of the depth values, where the weights are the number of bases at each depth. It is calculated as the total number of reads sequenced across all bases divided by the total number of bases in the panel.\nThe formula for the mean depth is:\n$$ \\bar{d} = \\frac{\\sum_{i=1}^{6} n_i d_i}{\\sum_{i=1}^{6} n_i} = \\frac{\\sum_{i=1}^{6} n_i d_i}{N} $$\nThe numerator represents the total number of aligned reads:\n$$ \\sum_{i=1}^{6} n_i d_i = (40000 \\times 5) + (120000 \\times 10) + (180000 \\times 20) + (300000 \\times 30) + (200000 \\times 40) + (160000 \\times 80) $$\n$$ \\sum_{i=1}^{6} n_i d_i = 200000 + 1200000 + 3600000 + 9000000 + 8000000 + 12800000 = 34800000 $$\nThe mean depth is therefore:\n$$ \\bar{d} = \\frac{34800000}{1000000} = 34.8 $$\nThe mean depth is $34.8\\times$. This value is exact.\n\n2.  **Median Depth ($d_{\\text{median}}$)**\n\nThe median depth is the value for which half of the bases in the panel have a depth less than or equal to it, and half have a depth greater than or equal to it. To find the median, we must order the $1{,}000{,}000$ bases by their depth and find the value of the base at the midpoint. Since $N=1{,}000{,}000$ is an even number, the median is the average of the depth values of the bases at positions $N/2 = 500{,}000$ and $N/2 + 1 = 500{,}001$.\n\nWe construct a cumulative frequency distribution:\n\\begin{itemize}\n    \\item Bases with depth $\\leq 5\\times$: $40000$\n    \\item Bases with depth $\\leq 10\\times$: $40000 + 120000 = 160000$\n    \\item Bases with depth $\\leq 20\\times$: $160000 + 180000 = 340000$\n    \\item Bases with depth $\\leq 30\\times$: $340000 + 300000 = 640000$\n\\end{itemize}\nThe cumulative count of bases with depth up to $20\\times$ is $340{,}000$. The cumulative count up to $30\\times$ is $640{,}000$. This means that the bases from position $340{,}001$ to $640{,}000$ in the ordered list all have a depth of $30\\times$.\nBoth the $500{,}000$-th and $500{,}001$-st positions fall within this range. Therefore, the depth at both positions is $30\\times$.\nThe median is the average of these two values:\n$$ d_{\\text{median}} = \\frac{30 + 30}{2} = 30 $$\nThe median depth is $30\\times$. This value is exact.\n\n3.  **Fraction of bases with depth $\\geq 30\\times$ ($f_{\\geq 30\\times}$)**\n\nThis metric, a measure of coverage breadth, requires summing the number of bases that meet or exceed the $30\\times$ depth threshold and dividing by the total number of bases.\nThe depths that satisfy this condition are $30\\times$, $40\\times$, and $80\\times$. The number of bases at these depths are:\n\\begin{itemize}\n    \\item Number of bases at $30\\times$: $300000$\n    \\item Number of bases at $40\\times$: $200000$\n    \\item Number of bases at $80\\times$: $160000$\n\\end{itemize}\nThe total number of bases with depth $\\geq 30\\times$ is:\n$$ N_{\\geq 30\\times} = 300000 + 200000 + 160000 = 660000 $$\nThe fraction is:\n$$ f_{\\geq 30\\times} = \\frac{N_{\\geq 30\\times}}{N} = \\frac{660000}{1000000} = 0.66 $$\nThis value is exact.\n\n**Interpretation**\n\nThe calculated metrics are: mean depth $\\bar{d} = 34.8\\times$, median depth $d_{\\text{median}} = 30\\times$, and fraction of bases with depth $\\geq 30\\times$ as $f_{\\geq 30\\times} = 0.66$. The observation that the mean depth is greater than the median depth ($\\bar{d} > d_{\\text{median}}$) is fundamental. This indicates that the depth distribution is right-skewed (or positively skewed), which is a characteristic feature of targeted NGS datasets.\n\nThis skewness arises from two primary sources of systemic bias inherent in the capture-based sequencing workflow:\n1.  **Hybridization/Capture Bias**: Targeted panels use oligonucleotide probes to capture specific DNA fragments from a complex library. The efficiency of this hybridization process is not uniform across all target regions. Factors such as GC content, the presence of repetitive sequences, and DNA secondary structure cause some genomic regions to be captured far more efficiently than others. These 'hotspots' accumulate a very high number of reads.\n2.  **PCR Amplification Bias**: Following capture, the selected fragments are amplified by polymerase chain reaction (PCR) to generate enough material for sequencing. This amplification process is also non-uniform; certain fragments (e.g., those with moderate GC content and shorter length) are amplified more efficiently. Even though PCR duplicates are removed bioinformatically, biases in the initial cycles of amplification can still propagate and exacerbate the unevenness established during capture.\n\nThe implications of these biases are directly reflected in the calculated metrics:\n-   **Mean Depth ($34.8\\times$)**: This metric is sensitive to extreme values. The small number of bases with very high depth (e.g., the $160{,}000$ bases at $80\\times$) disproportionately inflates the mean. While it reflects the total sequencing output per megabase of target, it can present an overly optimistic view of the overall coverage quality by masking underlying non-uniformity.\n-   **Median Depth ($30\\times$)**: As a robust measure of central tendency, the median is not significantly affected by high-depth outliers. It indicates the depth of a 'typical' base in the panel, revealing that $50\\%$ of the target regions are covered at $30\\times$ or less. The lower value of the median compared to the mean is a direct consequence of the right-skewed distribution.\n-   **Fraction of bases $\\geq 30\\times$ ($0.66$)**: For clinical applications like somatic variant detection in oncology, a minimum depth is required for statistical confidence. This metric provides a direct and clinically relevant measure of **coverage breadth**—the proportion of the panel that is actually analyzable at the specified quality threshold. Here, despite a mean depth of nearly $35\\times$, one-third of the panel ($1.0 - 0.66 = 0.34$) fails to reach the $30\\times$ minimum, potentially rendering those regions unusable for high-confidence variant calling.\n\nIn summary, the divergence between the mean and median depth quantifies the non-uniformity of coverage, which is a direct result of biochemical biases in NGS protocols. While the mean and median describe the central tendency of coverage depth, the breadth-of-coverage metric is arguably the most critical for assessing the diagnostic utility of a targeted sequencing panel. A comprehensive evaluation requires all three metrics to understand both the overall sequencing volume and its effective distribution across the target regions.", "answer": "$$\\boxed{\\begin{pmatrix} 34.8 & 30 & 0.66 \\end{pmatrix}}$$", "id": "4380739"}, {"introduction": "A common source of confusion in bioinformatics is the discovery that different software tools can report different depth values for the exact same alignment file. This discrepancy arises not from errors, but from different, equally valid definitions of what constitutes a \"countable\" piece of sequencing evidence. This final exercise demystifies this issue by tasking you with implementing several distinct coverage-counting policies from first principles, taking into account alignment flags, mapping and base qualities, and the distinction between read-level and fragment-level counting, thereby providing a deep, practical understanding of how depth metrics are actually constructed [@problem_id:4380689].", "problem": "You are given a synthetic abstraction of Binary Alignment/Map (BAM) content and are asked to demonstrate, by computation, how different coverage policies produce different depth values over the same genomic regions. The objective is to reconcile these discrepancies by rigorously applying filters derived from Sequence Alignment/Map (SAM) flags, mapping quality, base quality, and fragment-level counting rules. Implement the algorithm from first principles using the following foundational definitions and facts.\n\nFundamental definitions:\n- Coverage depth at a reference position $p$ is defined as the count of sequencing evidence overlapping position $p$ according to a specified counting policy.\n- In SAM/BAM, the Compact Idiosyncratic Gapped Alignment Report (CIGAR) encodes aligned segment operations. The operations used here are Match ($M$), Insertion ($I$), Deletion ($D$), and Soft-clip ($S$). Operation $M$ consumes both the read and the reference: it contributes to coverage at the consumed reference positions. Operation $I$ consumes the read but not the reference: it does not contribute to coverage at any reference position. Operation $D$ consumes the reference but not the read: it contributes zero coverage from that read at the deleted reference positions. Operation $S$ consumes the read but not the reference: it contributes zero coverage at soft-clipped positions. The alignment start coordinate is the first reference position consumed by the CIGAR.\n- SAM flag bits relevant here: paired ($0x1$), properly paired ($0x2$), unmapped ($0x4$), secondary alignment ($0x100$), vendor quality control fail ($0x200$), duplicate ($0x400$), supplementary alignment ($0x800$).\n- Mapping quality ($\\mathrm{MAPQ}$) is a non-negative integer scoring the confidence of placement of an alignment. Base quality is a Phred-scaled quality score ($Q$) associated to each base, which we here define over positions consumed by $M$ only.\n- A fragment refers to the underlying DNA molecule; if paired-end sequencing is performed, two reads (mates) derive from a single fragment and share a fragment identity. Fragment-level counting at position $p$ means a fragment contributes at most $1$ to the depth at $p$ even if both mates overlap $p$.\n\nPolicies to be implemented:\n- Policy $\\mathrm{U}$ (\"unfiltered alignment-count\"): for every alignment and every reference position consumed by $M$, contribute $1$ to the depth at that position. Do not exclude any alignment by flag, mapping quality, or base quality. Count per alignment (overlapping mates add $2$).\n- Policy $\\mathrm{F}$ (\"filtered alignment-count\"): exclude alignments with any of the following flags set: unmapped ($0x4$), secondary ($0x100$), supplementary ($0x800$), duplicate ($0x400$), vendor quality control fail ($0x200$). Require mapping quality $\\ge m_{\\min}$. Ignore base quality in counting. Count per alignment (overlapping mates add $2$).\n- Policy $\\mathrm{C}$ (\"filtered fragment-count with base-quality\"): apply the same alignment-level filters as policy $\\mathrm{F}$. Additionally, require base quality $\\ge q_{\\min}$ at each reference position when deciding whether an alignment contributes. At each position $p$, count at most $1$ per fragment by taking the union of qualifying positions across mates.\n\nAlgorithmic constraints:\n- For a given alignment with start coordinate $s$ and CIGAR string, parse the CIGAR left to right. Maintain the current reference position, initialized to $s$. For an $M$ operation of length $\\ell$, the alignment covers reference positions $s, s+1, \\dots, s+\\ell-1$. For an $I$ operation of length $\\ell$, advance in the read but not in the reference; do not contribute to coverage. For a $D$ operation of length $\\ell$, advance in the reference by $\\ell$; do not contribute to coverage. For an $S$ operation of length $\\ell$, advance in the read but not in the reference; do not contribute to coverage. Base qualities provided correspond in order to bases consumed by $M$ operations only.\n- Restrict counting to positions within the specified region of length $L$, that is, positions in $\\{1,2,\\dots,L\\}$.\n\nTest suite:\nFor each test case $i \\in \\{1,2,3\\}$, compute three outputs:\n- $S^{(i)}_{\\mathrm{U}}$: the sum of depths across all positions in the region under policy $\\mathrm{U}$.\n- $S^{(i)}_{\\mathrm{F}}$: the sum of depths across all positions in the region under policy $\\mathrm{F}$ with $m_{\\min} = 20$.\n- $S^{(i)}_{\\mathrm{C}}$: the sum of depths across all positions in the region under policy $\\mathrm{C}$ with $m_{\\min} = 20$ and $q_{\\min} = 20$.\n\nData model for reads:\nEach read is specified by a tuple: $(\\text{read\\_id}, \\text{fragment\\_id}, s, \\text{CIGAR}, \\mathrm{MAPQ}, \\text{flags}, \\text{base\\_qualities})$, where $s$ is the alignment start coordinate, $\\mathrm{MAPQ}$ is an integer, $\\text{flags}$ is the integer bitmask, and $\\text{base\\_qualities}$ is a list whose length equals the total $M$-length in the CIGAR string.\n\nTest case $1$ (region length $L = 20$):\n- $m_{\\min} = 20$, $q_{\\min} = 20$.\n- Reads:\n  - $(\\text{\"A1\"}, \\text{\"A\"}, 5, \\text{\"10M\"}, 60, 0x3, [30,30,30,30,30,30,30,30,30,30])$.\n  - $(\\text{\"A2\"}, \\text{\"A\"}, 10, \\text{\"10M\"}, 60, 0x3, [30,30,30,30,30,30,30,30,30,30])$.\n  - $(\\text{\"B\"}, \\text{\"B\"}, 8, \\text{\"10M\"}, 60, 0x400, [30,30,30,30,30,30,30,30,30,30])$.\n  - $(\\text{\"C\"}, \\text{\"C\"}, 1, \\text{\"5M\"}, 60, 0x100, [30,30,30,30,30])$.\n  - $(\\text{\"D\"}, \\text{\"D\"}, 15, \\text{\"5M\"}, 60, 0x800, [30,30,30,30,30])$.\n  - $(\\text{\"E\"}, \\text{\"E\"}, 12, \\text{\"5M\"}, 60, 0x200, [30,30,30,30,30])$.\n  - $(\\text{\"F\"}, \\text{\"F\"}, 6, \\text{\"5M\"}, 10, 0x0, [30,30,30,30,30])$.\n  - $(\\text{\"G\"}, \\text{\"G\"}, 9, \\text{\"5M\"}, 60, 0x0, [30,30,10,30,30])$.\n  - $(\\text{\"H\"}, \\text{\"H\"}, 13, \\text{\"2M1D3M\"}, 60, 0x0, [30,30,30,30,30])$.\n  - $(\\text{\"I\"}, \\text{\"I\"}, 1, \\text{\"2S8M\"}, 60, 0x0, [30,30,30,30,30,30,30,30])$.\n\nTest case $2$ (region length $L = 10$):\n- $m_{\\min} = 20$, $q_{\\min} = 20$.\n- Reads:\n  - $(\\text{\"J1\"}, \\text{\"J\"}, 1, \\text{\"5M\"}, 60, 0x3, [30,30,30,30,30])$.\n  - $(\\text{\"J2\"}, \\text{\"J\"}, 4, \\text{\"5M\"}, 60, 0x3, [30,30,30,30,30])$.\n  - $(\\text{\"K\"}, \\text{\"K\"}, 2, \\text{\"2M2I3M\"}, 60, 0x0, [30,30,30,30,30])$.\n  - $(\\text{\"L\"}, \\text{\"L\"}, 3, \\text{\"3M2D3M\"}, 60, 0x0, [30,30,30,30,30,30])$.\n  - $(\\text{\"M\"}, \\text{\"M\"}, 1, \\text{\"10M\"}, 60, 0x400, [30,30,30,30,30,30,30,30,30,30])$.\n  - $(\\text{\"N\"}, \\text{\"N\"}, 5, \\text{\"5M\"}, 60, 0x100, [30,30,30,30,30])$.\n  - $(\\text{\"O\"}, \\text{\"O\"}, 7, \\text{\"3M\"}, 0, 0x0, [30,30,30])$.\n  - $(\\text{\"P\"}, \\text{\"P\"}, 5, \\text{\"5M\"}, 60, 0x0, [30,30,30,10,30])$.\n\nTest case $3$ (region length $L = 30$):\n- $m_{\\min} = 20$, $q_{\\min} = 20$.\n- Reads:\n  - $(\\text{\"Q1\"}, \\text{\"Q\"}, 5, \\text{\"20M\"}, 60, 0x3, [30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30])$.\n  - $(\\text{\"Q2\"}, \\text{\"Q\"}, 10, \\text{\"20M\"}, 60, 0x3, [30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30])$.\n  - $(\\text{\"R\"}, \\text{\"R\"}, 15, \\text{\"10M\"}, 60, 0x400, [30,30,30,30,30,30,30,30,30,30])$.\n  - $(\\text{\"S\"}, \\text{\"S\"}, 20, \\text{\"10M\"}, 60, 0x800, [30,30,30,30,30,30,30,30,30,30])$.\n  - $(\\text{\"T\"}, \\text{\"T\"}, 8, \\text{\"10M\"}, 60, 0x200, [30,30,30,30,30,30,30,30,30,30])$.\n  - $(\\text{\"U\"}, \\text{\"U\"}, 18, \\text{\"10M\"}, 60, 0x100, [30,30,30,30,30,30,30,30,30,30])$.\n  - $(\\text{\"V\"}, \\text{\"V\"}, 12, \\text{\"15M\"}, 15, 0x0, [30,30,30,30,30,30,30,30,30,30,30,30,30,30,30])$.\n  - $(\\text{\"W1\"}, \\text{\"W\"}, 7, \\text{\"10M\"}, 60, 0x3, [30,30,30,30,30,30,30,30,30,30])$.\n  - $(\\text{\"W2\"}, \\text{\"W\"}, 12, \\text{\"10M\"}, 10, 0x3, [30,30,30,30,30,30,30,30,30,30])$.\n  - $(\\text{\"X\"}, \\text{\"X\"}, 9, \\text{\"10M\"}, 60, 0x0, [30,30,30,30,10,30,30,30,30,30])$.\n  - $(\\text{\"Y\"}, \\text{\"Y\"}, 10, \\text{\"5M1D5M\"}, 60, 0x0, [30,30,30,30,30,30,30,30,30,30])$.\n  - $(\\text{\"Z\"}, \\text{\"Z\"}, 14, \\text{\"3M2I7M\"}, 60, 0x0, [30,30,30,30,30,30,30,30,30,30])$.\n\nRequired output format:\nYour program should produce a single line of output containing the $9$ results as a comma-separated list enclosed in square brackets, in the order\n$[S^{(1)}_{\\mathrm{U}}, S^{(1)}_{\\mathrm{F}}, S^{(1)}_{\\mathrm{C}}, S^{(2)}_{\\mathrm{U}}, S^{(2)}_{\\mathrm{F}}, S^{(2)}_{\\mathrm{C}}, S^{(3)}_{\\mathrm{U}}, S^{(3)}_{\\mathrm{F}}, S^{(3)}_{\\mathrm{C}}]$.\n\nNo external inputs are permitted. All computations must be derived from the provided foundational definitions and the specified datasets. All answers are integers (sums of per-position depths across the region for each policy).", "solution": "The solution proceeds by deriving a principled algorithm from foundational definitions of coverage and alignment semantics, then implementing three distinct counting policies. The central scientific concept is that depth is a positional count of sequencing evidence constrained by alignment validity, mapping confidence, base-level confidence, and fragment identity.\n\nStep $1$: CIGAR interpretation and positional coverage.\n- Begin with an alignment described by start coordinate $s$, a CIGAR string, and base qualities attached only to operations $M$.\n- Maintain a reference pointer $r$ initialized to $s$ and an index $i$ into the base-qualities list initialized to $0$.\n- For each CIGAR operation $(\\text{op}, \\ell)$ in order:\n  - If $\\text{op} = M$, then for $k$ from $0$ to $\\ell-1$: position $p = r + k$ is covered by this alignment. Associate base quality $Q[p] = \\text{base\\_qualities}[i + k]$. After processing, increment $r \\leftarrow r + \\ell$ and $i \\leftarrow i + \\ell$.\n  - If $\\text{op} = I$, then advance in the read by $\\ell$ without changing the reference position: coverage does not change and $i$ does not advance because base qualities here are defined only for $M$.\n  - If $\\text{op} = D$, then advance in the reference by $\\ell$: $r \\leftarrow r + \\ell$, and there is no coverage contributed at those deleted positions.\n  - If $\\text{op} = S$, then advance in the read by $\\ell$ without changing $r$: coverage does not change and $i$ does not advance because soft-clipped bases are not aligned.\n- Restrict coverage positions to the region bounds $1 \\leq p \\leq L$.\n\nStep $2$: Alignment-level filters.\n- An alignment is considered passing if and only if it does not have any of the excluded flags set and satisfies the mapping quality threshold. The excluded flags are unmapped ($0x4$), secondary ($0x100$), supplementary ($0x800$), duplicate ($0x400$), and vendor quality control fail ($0x200$). Formally, an alignment with flag bitmask $f$ passes if $(f \\,\\&\\, (0x4 \\,|\\, 0x100 \\,|\\, 0x800 \\,|\\, 0x400 \\,|\\, 0x200)) = 0$ and $\\mathrm{MAPQ} \\geq m_{\\min}$.\n\nStep $3$: Base-level filters.\n- At each covered position $p$ contributed by an $M$ operation, base quality $Q[p]$ is available. A base-level contribution at $p$ is considered passing if $Q[p] \\geq q_{\\min}$.\n\nStep $4$: Counting policies.\n- Policy $\\mathrm{U}$: For each alignment, for each covered position $p$ from step $1$, increment the depth $\\mathrm{depth}_{\\mathrm{U}}[p] \\leftarrow \\mathrm{depth}_{\\mathrm{U}}[p] + 1$ without any filters. Overlapping mates contribute independently, so fragments can contribute $2$ at positions where both mates overlap.\n- Policy $\\mathrm{F}$: For each alignment passing step $2$, for each covered position $p$ from step $1$, increment $\\mathrm{depth}_{\\mathrm{F}}[p] \\leftarrow \\mathrm{depth}_{\\mathrm{F}}[p] + 1$. Base-level quality is not considered; if an alignment passes, all its $M$-covered positions contribute.\n- Policy $\\mathrm{C}$: For each fragment identity $\\phi$, aggregate across all passing alignments (step $2$) belonging to $\\phi$. At a position $p$, the fragment $\\phi$ contributes if at least one of its passing alignments covers $p$ and the corresponding base quality at that alignment satisfies $Q[p] \\geq q_{\\min}$. Formally, define the set $P_{\\phi} = \\{p : \\exists \\text{ alignment } a \\text{ of } \\phi \\text{ such that } a \\text{ passes and covers } p \\text{ with } Q[p] \\geq q_{\\min}\\}$. Then increment fragment-level depth once per position per fragment: for each $p \\in P_{\\phi}$, set $\\mathrm{depth}_{\\mathrm{C}}[p] \\leftarrow \\mathrm{depth}_{\\mathrm{C}}[p] + 1$. This union rule ensures that if both mates cover position $p$, the fragment still contributes at most $1$ to $\\mathrm{depth}_{\\mathrm{C}}[p]$.\n\nStep $5$: Region-summed outputs.\n- For each policy $\\in \\{\\mathrm{U}, \\mathrm{F}, \\mathrm{C}\\}$ and each test case $i$, compute the sum $S^{(i)}_{\\mathrm{policy}} = \\sum_{p=1}^{L} \\mathrm{depth}_{\\mathrm{policy}}[p]$. These sums are integers.\n\nStep $6$: Reconciling discrepancies via flags and filters.\n- Discrepancies between $\\mathrm{U}$ and $\\mathrm{F}$ arise from alignment-level exclusion of secondary, supplementary, duplicates, vendor quality control fail, and low mapping quality alignments in $\\mathrm{F}$. Positions overlapped by both mates are double-counted in both $\\mathrm{U}$ and $\\mathrm{F}$.\n- Discrepancies between $\\mathrm{F}$ and $\\mathrm{C}$ arise from base-quality filtering at the per-position level and fragment-level union counting. Low base-quality sites reduce $\\mathrm{C}$ compared to $\\mathrm{F}$. Overlapping mates contribute twice in $\\mathrm{F}$ but at most once in $\\mathrm{C}$.\n- Deletions ($D$) reduce coverage from an alignment at those positions in all policies; insertions ($I$) never contribute coverage; soft-clips ($S$) never contribute coverage. Thus, CIGAR structure modulates depth independently of filters.\n\nImplementation notes:\n- Parsing of the CIGAR string into $(\\text{op}, \\ell)$ tokens can be done by scanning digits and operation letters.\n- Construct coverage maps at the alignment level: a dictionary from position $p$ to base quality $Q[p]$ for $M$ operations, restricting to $1 \\leq p \\leq L$.\n- Apply filters and combine across alignments and fragments according to steps $2$–$4$.\n- Finally, produce the single-line output in the prescribed order: $[S^{(1)}_{\\mathrm{U}}, S^{(1)}_{\\mathrm{F}}, S^{(1)}_{\\mathrm{C}}, S^{(2)}_{\\mathrm{U}}, S^{(2)}_{\\mathrm{F}}, S^{(2)}_{\\mathrm{C}}, S^{(3)}_{\\mathrm{U}}, S^{(3)}_{\\mathrm{F}}, S^{(3)}_{\\mathrm{C}}]$.\n\nThis approach is grounded in the foundational semantics of SAM/BAM, the interpretation of CIGAR, and the widely used concepts of mapping and base quality. It furnishes a principled reconciliation of depth discrepancies by making explicit the filters and counting rules at play.", "answer": "$$\\boxed{[68, 38, 32, 44, 26, 23, 155, 80, 64]}$$", "id": "4380689"}]}