{"hands_on_practices": [{"introduction": "The integrity of any genomic analysis pipeline begins with the correct interpretation of raw data. This exercise confronts a common but critical pitfall in bioinformatics: the misinterpretation of FASTQ quality score encodings. By quantifying the drastic underestimation of error probabilities that occurs when Phred+64 data is read as Phred+33, this practice [@problem_id:4374725] underscores the importance of format validation and its profound impact on downstream variant calling confidence.", "problem": "A precision oncology pipeline ingests whole-exome sequencing reads supplied in FASTQ (FASTA plus quality) format from a legacy clinical sequencer that encodes base quality scores using the Phred-plus-64 convention. Due to a configuration error, a downstream variant-calling module interprets the same data as Phred-plus-33. Consider a single read where the true per-base Phred quality score is uniform at $Q_{\\text{true}} = 30$ across all positions. Assume independent base-calling errors across positions, each governed by the same true error probability implied by $Q_{\\text{true}}$, and that the module estimates base error probabilities solely from the misinterpreted quality scores. Quantify the impact of this encoding mismatch on downstream error probabilities by computing two multiplicative factors:\n- The single-base underestimation factor $r_{\\text{base}}$, defined as the ratio of the misinterpreted per-base error probability to the true per-base error probability.\n- The read-level underestimation factor $r_{\\text{read}}$, defined as the ratio of the misinterpreted probability that a read of length $L = 150$ contains at least one base-calling error to the corresponding true probability.\n\nRound your final numerical values to four significant figures and express them as decimals (not percentages). Provide your two results in the order ($r_{\\text{base}}$, $r_{\\text{read}}$).", "solution": "The problem statement has been critically validated and is deemed to be self-contained, scientifically grounded, and well-posed. All necessary parameters and definitions are provided, and the scenario described is a realistic and formalizable problem in bioinformatics. Therefore, I will proceed with the solution.\n\nThe problem requires the calculation of two factors, $r_{\\text{base}}$ and $r_{\\text{read}}$, which quantify the impact of a Phred score encoding mismatch. The core of the problem lies in the definition of the Phred quality score, $Q$, and its relationship to the probability of an incorrect base call, $P_e$. This relationship is given by:\n$$Q = -10 \\log_{10}(P_e)$$\nFrom this definition, the probability of an error, $P_e$, can be expressed as a function of the quality score $Q$:\n$$P_e = 10^{-Q/10}$$\n\nFirst, let us establish the true and misinterpreted parameters.\nThe true Phred quality score is given as $Q_{\\text{true}} = 30$.\nThe true per-base error probability, $P_{e, \\text{true}}$, is therefore:\n$$P_{e, \\text{true}} = 10^{-Q_{\\text{true}}/10} = 10^{-30/10} = 10^{-3} = 0.001$$\n\nThe sequencing data is encoded using the Phred-plus-64 convention. This means a quality score $Q$ is stored as an ASCII character with the integer value $Q + 64$. For $Q_{\\text{true}} = 30$, the ASCII value of the character in the FASTQ file is:\n$$\\text{ASCII value} = Q_{\\text{true}} + 64 = 30 + 64 = 94$$\nA downstream module incorrectly interprets this data using the Phred-plus-33 convention. It assumes the quality score is the ASCII value minus $33$. Thus, the misinterpreted quality score, $Q_{\\text{misinterpreted}}$, is:\n$$Q_{\\text{misinterpreted}} = \\text{ASCII value} - 33 = 94 - 33 = 61$$\nUsing this misinterpreted quality score, the module calculates a misinterpreted per-base error probability, $P_{e, \\text{misinterpreted}}$:\n$$P_{e, \\text{misinterpreted}} = 10^{-Q_{\\text{misinterpreted}}/10} = 10^{-61/10} = 10^{-6.1}$$\n\nNow we can compute the first required quantity, the single-base underestimation factor, $r_{\\text{base}}$. This is the ratio of the misinterpreted per-base error probability to the true per-base error probability.\n$$r_{\\text{base}} = \\frac{P_{e, \\text{misinterpreted}}}{P_{e, \\text{true}}} = \\frac{10^{-6.1}}{10^{-3}} = 10^{-6.1 - (-3)} = 10^{-3.1}$$\nTo obtain the numerical value, we compute $10^{-3.1}$:\n$$r_{\\text{base}} \\approx 0.000794328...$$\nRounding to four significant figures, we get:\n$$r_{\\text{base}} \\approx 0.0007943$$\n\nNext, we compute the read-level underestimation factor, $r_{\\text{read}}$. This requires us to first determine the probability that a read of length $L$ contains at least one base-calling error. The probability that a single base is called correctly is $1 - P_e$. Assuming base-calling errors are independent events, the probability that an entire read of length $L$ is free of errors is $(1 - P_e)^L$. Consequently, the probability of a read having at least one error is:\n$$P(\\text{read has} \\ge 1 \\text{ error}) = 1 - (1 - P_e)^L$$\nThe length of the read is given as $L = 150$.\n\nThe true probability that a read contains at least one error, $P(\\text{read error})_{\\text{true}}$, is:\n$$P(\\text{read error})_{\\text{true}} = 1 - (1 - P_{e, \\text{true}})^L = 1 - (1 - 10^{-3})^{150} = 1 - (0.999)^{150}$$\nThe misinterpreted probability that a read contains at least one error, $P(\\text{read error})_{\\text{misinterpreted}}$, is:\n$$P(\\text{read error})_{\\text{misinterpreted}} = 1 - (1 - P_{e, \\text{misinterpreted}})^L = 1 - (1 - 10^{-6.1})^{150}$$\nThe read-level underestimation factor, $r_{\\text{read}}$, is the ratio of these two probabilities:\n$$r_{\\text{read}} = \\frac{P(\\text{read error})_{\\text{misinterpreted}}}{P(\\text{read error})_{\\text{true}}} = \\frac{1 - (1 - 10^{-6.1})^{150}}{1 - (0.999)^{150}}$$\nNow, we compute the numerical values for the numerator and the denominator.\nDenominator:\n$$1 - (0.999)^{150} \\approx 1 - 0.860713303 = 0.139286697$$\nNumerator:\n$$1 - (1 - 10^{-6.1})^{150} \\approx 1 - (1 - 7.94328 \\times 10^{-7})^{150} \\approx 1 - 0.999880856 = 0.000119144$$\nThe value of $P_{e, \\text{misinterpreted}}$ is very small, so the approximation $1 - (1 - x)^n \\approx nx$ is quite accurate for the numerator: $150 \\times 10^{-6.1} \\approx 0.000119149$.\n\nNow we compute the ratio:\n$$r_{\\text{read}} = \\frac{0.000119144}{0.139286697} \\approx 0.000855420...$$\nRounding to four significant figures, we get:\n$$r_{\\text{read}} \\approx 0.0008554$$\n\nThe two required multiplicative factors are $r_{\\text{base}} \\approx 0.0007943$ and $r_{\\text{read}} \\approx 0.0008554$.", "answer": "$$\\boxed{\\begin{pmatrix} 0.0007943  0.0008554 \\end{pmatrix}}$$", "id": "4374725"}, {"introduction": "Beyond individual base quality, the true power of next-generation sequencing lies in aggregating data from multiple reads to achieve high confidence. This practice [@problem_id:4374772] moves from single-read quality to consensus genotyping by applying a Bayesian framework to combine evidence from several independent observations. You will calculate the posterior probability of a base-calling error for a genomic locus, employing log-space computations to ensure numerical stability—a crucial skill for handling the very small probabilities common in genomics.", "problem": "In a targeted oncology panel used in precision medicine, a clinically actionable locus is covered by multiple independent Next-Generation Sequencing (NGS) reads. At a particular position, all reads report the same base call, denoted as nucleotide $\\mathrm{A}$. For each read $i \\in \\{1,2,3,4,5\\}$, the base-caller provides a calibrated per-read error probability $p_i$ that the reported base is incorrect. The five reads have the following error probabilities: $p_1 = 3.2 \\times 10^{-6}$, $p_2 = 7.5 \\times 10^{-7}$, $p_3 = 1.1 \\times 10^{-6}$, $p_4 = 4.0 \\times 10^{-8}$, and $p_5 = 2.3 \\times 10^{-6}$.\n\nTasks:\n1. Convert each $p_i$ to its Phred quality score (Q-score), defined by $Q_i = -10 \\log_{10}(p_i)$.\n2. Assuming reads are conditionally independent given the true base, and that when an error occurs it is equally likely to be any of the three incorrect nucleotides, compute the posterior probability that the true base at this position is not $\\mathrm{A}$, given that all five reads reported $\\mathrm{A}$. Use a uniform prior over nucleotides so that the prior probability the true base is $\\mathrm{A}$ equals $1/4$, and the prior that it is not $\\mathrm{A}$ equals $3/4$. To ensure numerical stability for $p_i  10^{-6}$, carry out the computation in the natural logarithm domain and use the log-sum-exp method when summing likelihood terms.\n\nBase your derivation on fundamental definitions: the Phred mapping between $p$ and $Q$, conditional independence of reads, symmetry of per-read error among the three alternative nucleotides, and Bayes’ theorem. Express the final numerical answer as the posterior error probability (a single decimal number with no units). Round your final answer to four significant figures.", "solution": "The user has provided a problem in genomic diagnostics that requires the calculation of Phred quality scores and a posterior probability of a sequencing error using Bayesian inference.\n\n### Step 1: Extract Givens\n-   Observed data ($D$): Five independent Next-Generation Sequencing (NGS) reads at a specific locus all report the nucleotide $\\mathrm{A}$.\n-   Number of reads, $n = 5$.\n-   Per-read error probabilities ($p_i$):\n    -   $p_1 = 3.2 \\times 10^{-6}$\n    -   $p_2 = 7.5 \\times 10^{-7}$\n    -   $p_3 = 1.1 \\times 10^{-6}$\n    -   $p_4 = 4.0 \\times 10^{-8}$\n    -   $p_5 = 2.3 \\times 10^{-6}$\n-   Definition of Phred quality score (Q-score): $Q_i = -10 \\log_{10}(p_i)$.\n-   Task 1: Convert each $p_i$ to its corresponding $Q_i$.\n-   Task 2: Compute the posterior probability that the true base is not $\\mathrm{A}$, given the observed data $D$.\n-   Assumptions for Task 2:\n    1.  Conditional Independence: The reads are conditionally independent given the true nucleotide.\n    2.  Error Symmetry: When an error occurs in a read, it is equally likely to be any of the three incorrect nucleotides.\n    3.  Prior Probabilities: The prior distribution over the four possible true nucleotides ($\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}$) is uniform. That is, $P(\\text{true base} = k) = 1/4$ for any $k \\in \\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$. This implies the prior for the true base being $\\mathrm{A}$ is $P(H_A) = 1/4$, and the prior for the true base not being $\\mathrm{A}$ is $P(H_{\\neg A}) = 3/4$.\n-   Computational Method: Calculations are to be performed in the natural logarithm domain for numerical stability, using the log-sum-exp method where appropriate.\n-   Final Answer: The posterior error probability, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the principles of genomics, bioinformatics, and Bayesian statistics. The concepts of Phred scores, read error probabilities, and Bayesian inference for genotyping are standard in the field. The provided data values are realistic. The problem is well-posed, with all necessary information and assumptions clearly stated to derive a unique, meaningful solution. The language is objective and precise. The problem does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. A detailed solution will be provided.\n\n### Solution Derivation\n\n**Task 1: Phred Quality Score (Q-score) Calculation**\n\nThe Phred quality score, $Q_i$, is defined as a logarithmic mapping of the error probability, $p_i$:\n$$Q_i = -10 \\log_{10}(p_i)$$\nUsing this formula, we convert each given error probability $p_i$ into its corresponding Q-score.\n\nFor read $1$:\n$Q_1 = -10 \\log_{10}(3.2 \\times 10^{-6}) = -10 (\\log_{10}(3.2) - 6) \\approx -10(0.5051 - 6) \\approx 54.95$\n\nFor read $2$:\n$Q_2 = -10 \\log_{10}(7.5 \\times 10^{-7}) = -10 (\\log_{10}(7.5) - 7) \\approx -10(0.8751 - 7) \\approx 61.25$\n\nFor read $3$:\n$Q_3 = -10 \\log_{10}(1.1 \\times 10^{-6}) = -10 (\\log_{10}(1.1) - 6) \\approx -10(0.0414 - 6) \\approx 59.59$\n\nFor read $4$:\n$Q_4 = -10 \\log_{10}(4.0 \\times 10^{-8}) = -10 (\\log_{10}(4.0) - 8) = -10(0.6021 - 8) \\approx 73.98$\n\nFor read $5$:\n$Q_5 = -10 \\log_{10}(2.3 \\times 10^{-6}) = -10 (\\log_{10}(2.3) - 6) \\approx -10(0.3617 - 6) \\approx 56.38$\n\nThese Q-scores represent the confidence in each individual base call, with higher scores indicating lower error probabilities.\n\n**Task 2: Posterior Probability Calculation**\n\nWe wish to compute the posterior probability that the true nucleotide is not $\\mathrm{A}$, given that all five reads observed the nucleotide $\\mathrm{A}$. Let $D$ be the event that all five reads report $\\mathrm{A}$, i.e., $D = \\{R_1=\\mathrm{A}, R_2=\\mathrm{A}, R_3=\\mathrm{A}, R_4=\\mathrm{A}, R_5=\\mathrm{A}\\}$. Let $H_k$ be the hypothesis that the true nucleotide is $k$, where $k \\in \\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$. We want to calculate $P(H_{\\neg A} | D)$, where $H_{\\neg A}$ is the composite hypothesis that the true base is not $\\mathrm{A}$, i.e., $H_{\\neg A} = H_C \\cup H_G \\cup H_T$.\n\nUsing Bayes' theorem, the posterior probability of any hypothesis $H_k$ is:\n$$P(H_k | D) = \\frac{P(D | H_k) P(H_k)}{P(D)}$$\nThe term $P(D)$ is the marginal likelihood of the data, or evidence, given by the law of total probability:\n$$P(D) = \\sum_{k \\in \\{\\mathrm{A,C,G,T}\\}} P(D|H_k)P(H_k)$$\nThe desired probability is $P(H_{\\neg A} | D) = P(H_C | D) + P(H_G | D) + P(H_T | D)$. This can be calculated as:\n$$P(H_{\\neg A} | D) = \\frac{P(D, H_{\\neg A})}{P(D)} = \\frac{P(D, H_C) + P(D, H_G) + P(D, H_T)}{P(D, H_A) + P(D, H_C) + P(D, H_G) + P(D, H_T)}$$\nwhere $P(D, H_k) = P(D|H_k)P(H_k)$ is the joint probability of the data and a hypothesis.\n\nTo perform calculations in the natural logarithm domain, we first compute the log of the joint probabilities.\nThe prior probabilities are $P(H_k) = 1/4$ for all $k$. Thus, $\\ln(P(H_k)) = \\ln(1/4) = -\\ln(4) \\approx -1.38629$.\n\nNext, we formulate the likelihoods $P(D|H_k)$.\nIf the true base is $\\mathrm{A}$ ($H_A$ is true), observing $\\mathrm{A}$ in read $i$ is a correct call, with probability $1-p_i$. Due to conditional independence, the likelihood is:\n$$L_A = P(D|H_A) = \\prod_{i=1}^{5} (1-p_i)$$\nIf the true base is not $\\mathrm{A}$ (e.g., $H_C$ is true), observing $\\mathrm{A}$ in read $i$ is an error. The total error probability is $p_i$. Given the error symmetry assumption, the probability of this specific error (C $\\to$ A) is $p_i/3$. The likelihood is:\n$$L_{C} = P(D|H_C) = \\prod_{i=1}^{5} \\frac{p_i}{3}$$\nBy symmetry, $L_C = L_G = L_T$. Let's denote this common likelihood as $L_{\\text{err}}$.\n\nNow, we compute the log of the joint probabilities $\\ln(P(D, H_k)) = \\ln(P(D|H_k)) + \\ln(P(H_k))$.\n\nLog-likelihood for $H_A$:\n$\\ln(L_A) = \\sum_{i=1}^{5} \\ln(1-p_i) = \\ln(1-3.2\\times10^{-6}) + \\ldots + \\ln(1-2.3\\times10^{-6}) \\approx -7.390 \\times 10^{-6}$\nLog-joint for $H_A$:\n$\\ln(P(D, H_A)) = \\ln(L_A) + \\ln(1/4) \\approx -7.390 \\times 10^{-6} - 1.38629436 = -1.38629443$\n\nLog-likelihood for $H_C$ (and $H_G$, $H_T$):\n$\\ln(L_{\\text{err}}) = \\sum_{i=1}^{5} \\ln(p_i/3) = \\sum_{i=1}^{5} \\ln(p_i) - 5\\ln(3)$\n$\\sum \\ln(p_i) = \\ln(3.2\\times10^{-6}) + \\ldots + \\ln(2.3\\times10^{-6}) \\approx -12.6524 - 14.1032 - 13.7202 - 17.0344 - 12.9826 \\approx -70.4927$\n$\\ln(L_{\\text{err}}) \\approx -70.4927 - 5(1.0986) \\approx -70.4927 - 5.4931 = -75.9858$\nLog-joint for $H_C$:\n$\\ln(P(D, H_C)) = \\ln(L_{\\text{err}}) + \\ln(1/4) \\approx -75.9858 - 1.3863 = -77.3721$\nDue to symmetry, $\\ln(P(D, H_G)) = \\ln(P(D, H_T)) = \\ln(P(D, H_C))$.\n\nWe need to compute $\\ln(P(D, H_{\\neg A})) = \\ln(P(D, H_C) + P(D, H_G) + P(D, H_T))$. Since the terms are equal, this simplifies to:\n$\\ln(P(D, H_{\\neg A})) = \\ln(3 \\times P(D, H_C)) = \\ln(3) + \\ln(P(D, H_C))$\n$\\ln(P(D, H_{\\neg A})) \\approx 1.0986 + (-77.3721) = -76.2735$\n\nThe log of the total evidence is $\\ln(P(D)) = \\ln(P(D, H_A) + P(D, H_{\\neg A}))$. We use the log-sum-exp trick for this:\n$\\mathrm{logsumexp}(x, y) = \\max(x, y) + \\ln(\\exp(x-\\max(x,y)) + \\exp(y-\\max(x,y)))$\nLet $x = \\ln(P(D, H_A)) \\approx -1.3863$ and $y = \\ln(P(D, H_{\\neg A})) \\approx -76.2735$.\n$\\max(x,y) = x = -1.3863$.\n$\\ln P(D) \\approx x + \\ln(\\exp(0) + \\exp(y-x)) = x + \\ln(1 + \\exp(y-x))$\n$y - x \\approx -76.2735 - (-1.3863) = -74.8872$.\n$\\ln P(D) \\approx -1.3863 + \\ln(1 + \\exp(-74.8872))$. The second term is negligibly small.\nSo, $\\ln P(D) \\approx -1.3863$.\n\nThe log of the posterior probability is:\n$\\ln(P(H_{\\neg A} | D)) = \\ln(P(D, H_{\\neg A})) - \\ln(P(D))$\n$\\ln(P(H_{\\neg A} | D)) \\approx -76.2735 - (-1.3863) = -74.8872$\n\nFinally, we exponentiate this value to get the posterior probability:\n$P(H_{\\neg A} | D) = \\exp(\\ln(P(H_{\\neg A} | D))) \\approx \\exp(-74.8872)$\n\nLet's use more precise values:\n$\\sum \\ln(p_i) = -70.49274387$\n$5\\ln(3) = 5.49306144$\n$\\ln(4) = 1.38629436$\n$\\ln(L_{\\text{err}}) = -70.49274387 - 5.49306144 = -75.98580531$\n$\\ln(P(D, H_{\\neg A})) = \\ln(3) + \\ln(L_{\\text{err}}) + \\ln(1/4) = 1.09861229 - 75.98580531 - 1.38629436 = -76.27348738$\n$\\ln(P(D, H_A)) = -7.390 \\times 10^{-6} - 1.38629436 = -1.38629443$\n$\\ln(P(H_{\\neg A} | D)) = \\ln(P(D, H_{\\neg A})) - \\ln(P(D)) = \\ln(P(D, H_{\\neg A})) - \\mathrm{logsumexp}(\\ln(P(D, H_A)), \\ln(P(D, H_{\\neg A})))$\n$\\ln(P(H_{\\neg A} | D)) = -76.27348738 - \\mathrm{logsumexp}(-1.38629443, -76.27348738)$\n$\\ln(P(H_{\\neg A} | D)) = -76.27348738 - (-1.38629443 + \\ln(1 + \\exp(-74.88719295)))$\n$\\ln(P(H_{\\neg A} | D)) \\approx -74.88719295$\n$P(H_{\\neg A} | D) = \\exp(-74.88719295) \\approx 1.19652 \\times 10^{-33}$\n\nRounding this result to four significant figures gives $1.197 \\times 10^{-33}$. This extremely small probability indicates very high confidence that the true nucleotide is indeed $\\mathrm{A}$, given the overwhelming and consistent evidence from the five high-quality reads.", "answer": "$$\n\\boxed{1.197 \\times 10^{-33}}\n$$", "id": "4374772"}, {"introduction": "Phred scores not only quantify random errors but can also reveal systematic biases inherent in sequencing technologies, such as the characteristic decay in quality over the length of a read. This exercise [@problem_id:4374748] provides a quantitative approach to characterizing this phenomenon by modeling the per-cycle error probability, $P_{\\text{error}}(i)$, as a linear function. By transforming observed $Q$-scores back into probabilities and applying Ordinary Least Squares (OLS) regression, you will estimate the parameters of this error model, a foundational technique for building advanced Base Quality Score Recalibration (BQSR) tools.", "problem": "A sequencing-by-synthesis instrument used in precision oncology workflows exhibits a per-cycle base-call error probability that, to first order, can be approximated as a linear function of the cycle index. Let the cycle index be denoted by $i \\in \\{1,2,\\dots,L\\}$, the per-cycle base-call error probability by $P_{\\text{error}}(i)$, and the Phred quality score by $Q_i$. The Phred quality score is defined by the fundamental relationship $Q_i = -10 \\log_{10}\\left(P_{\\text{error}}(i)\\right)$, where $Q_i$ is dimensionless. Consider a model $P_{\\text{error}}(i) = a + b i$ for two real parameters $a$ and $b$ that are constant across cycles within a read.\n\nYour task is to derive, implement, and execute an algorithm that estimates the parameters $a$ and $b$ from observed Phred quality scores by:\n- Converting the observed quality scores back to error probabilities using the Phred definition.\n- Fitting the linear model $P_{\\text{error}}(i) = a + b i$ by minimizing the sum of squared residuals over cycles using Ordinary Least Squares (OLS).\n\nThe derivation must start from the Phred score definition and proceed to the least-squares estimator. The implementation must follow this derivation exactly and operate on the test suite below. All mathematical entities must be expressed in LaTeX notation.\n\nTest suite specification:\n- For each test case, the length $L$, the true parameters $(a_{\\text{true}}, b_{\\text{true}})$, and a deterministic additive noise vector on the $Q$-scores, $\\Delta Q = [\\delta_1,\\dots,\\delta_L]$, are provided. The observed $Q$-scores are defined by $Q_i^{\\text{obs}} = -10 \\log_{10}\\left(a_{\\text{true}} + b_{\\text{true}} \\cdot i \\right) + \\delta_i$. Your program must reconstruct $(a,b)$ from $\\{Q_i^{\\text{obs}}\\}_{i=1}^L$.\n\nThe test cases are:\n- Case $1$: $L = 10$, $(a_{\\text{true}}, b_{\\text{true}}) = (0.005, 0.0004)$, $\\Delta Q = [0,0,0,0,0,0,0,0,0,0]$.\n- Case $2$: $L = 10$, $(a_{\\text{true}}, b_{\\text{true}}) = (0.002, 0.00025)$, $\\Delta Q = [-0.05, 0.03, -0.02, 0.01, 0.00, -0.01, 0.02, -0.03, 0.04, -0.02]$.\n- Case $3$: $L = 10$, $(a_{\\text{true}}, b_{\\text{true}}) = (0.01, 0.0)$, $\\Delta Q = [0.005, -0.004, 0.003, -0.002, 0.001, 0.0, -0.001, 0.002, -0.003, 0.004]$.\n- Case $4$: $L = 10$, $(a_{\\text{true}}, b_{\\text{true}}) = (0.0001, 0.00001)$, $\\Delta Q = [0.1, -0.1, 0.05, -0.05, 0.02, -0.02, 0.0, 0.03, -0.03, 0.01]$.\n\nOutput specification:\n- For each test case, estimate $(a,b)$ using OLS on the transformed error probabilities.\n- Report the estimates as floating-point numbers rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list $[a,b]$ for one test case. For example: $[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4]]$.", "solution": "The user-provided problem has been validated and is determined to be sound. It is scientifically grounded in the principles of genomics and statistics, is well-posed with a clear objective and sufficient data, and is free from ambiguity or contradiction. We may therefore proceed with the derivation and solution.\n\nThe objective is to estimate the parameters $a$ and $b$ of the linear model for per-cycle base-call error probability, $P_{\\text{error}}(i) = a + b i$, given a set of observed Phred quality scores $\\{Q_i^{\\text{obs}}\\}_{i=1}^L$ for a read of length $L$. The estimation is to be performed by minimizing the sum of squared residuals using Ordinary Least Squares (OLS).\n\nThe derivation proceeds in the following steps.\n\n**1. Transformation of Observed Data from Phred Score to Error Probability**\n\nThe fundamental relationship between the Phred quality score $Q_i$ and the per-cycle error probability $P_{\\text{error}}(i)$ is given as:\n$$Q_i = -10 \\log_{10}\\left(P_{\\text{error}}(i)\\right)$$\nThe observed data are the Phred scores, $Q_i^{\\text{obs}}$. To perform a linear regression on the error probabilities, we must first invert this relationship to convert the observed scores into observed error probabilities, which we denote as $P_i^{\\text{obs}}$.\n\nRearranging the equation to solve for $P_{\\text{error}}(i)$:\n$$-\\frac{Q_i}{10} = \\log_{10}\\left(P_{\\text{error}}(i)\\right)$$\nApplying the antilogarithm (base $10$):\n$$P_{\\text{error}}(i) = 10^{-Q_i/10}$$\nThus, for each observed score $Q_i^{\\text{obs}}$, the corresponding observed error probability is calculated as:\n$$P_i^{\\text{obs}} = 10^{-Q_i^{\\text{obs}}/10}$$\nThese transformed probabilities, $P_i^{\\text{obs}}$, will serve as the dependent variable in our linear regression model.\n\n**2. Formulation of the Ordinary Least Squares (OLS) Problem**\n\nWe are fitting the linear model $P_{\\text{error}}(i) = a + b i$ to the observed data points. Let the cycle index $i$ be the independent variable, which we can denote as $x_i = i$, and the observed error probability $P_i^{\\text{obs}}$ be the dependent variable, denoted as $y_i = P_i^{\\text{obs}}$. The model is thus $y_i \\approx a + b x_i$.\n\nThe OLS method seeks to find the parameter estimates, denoted $\\hat{a}$ and $\\hat{b}$, that minimize the Sum of Squared Residuals (SSR). The residual for the $i$-th observation is the difference between the observed value $y_i$ and the value predicted by the model, $\\hat{y}_i = \\hat{a} + \\hat{b} x_i$. The SSR, which we denote as a function $S(\\hat{a}, \\hat{b})$, is:\n$$S(\\hat{a}, \\hat{b}) = \\sum_{i=1}^{L} (y_i - (\\hat{a} + \\hat{b} x_i))^2$$\n\n**3. Derivation of the Normal Equations**\n\nTo find the values of $\\hat{a}$ and $\\hat{b}$ that minimize $S$, we take the partial derivatives of $S$ with respect to $\\hat{a}$ and $\\hat{b}$ and set them equal to zero.\n\nThe partial derivative with respect to $\\hat{a}$:\n$$\\frac{\\partial S}{\\partial \\hat{a}} = \\sum_{i=1}^{L} 2(y_i - \\hat{a} - \\hat{b} x_i)(-1) = -2 \\left( \\sum_{i=1}^{L} y_i - L\\hat{a} - \\hat{b}\\sum_{i=1}^{L} x_i \\right)$$\nSetting $\\frac{\\partial S}{\\partial \\hat{a}} = 0$:\n$$L\\hat{a} + \\hat{b}\\sum_{i=1}^{L} x_i = \\sum_{i=1}^{L} y_i \\quad (\\text{Normal Equation 1})$$\n\nThe partial derivative with respect to $\\hat{b}$:\n$$\\frac{\\partial S}{\\partial \\hat{b}} = \\sum_{i=1}^{L} 2(y_i - \\hat{a} - \\hat{b} x_i)(-x_i) = -2 \\left( \\sum_{i=1}^{L} x_i y_i - \\hat{a}\\sum_{i=1}^{L} x_i - \\hat{b}\\sum_{i=1}^{L} x_i^2 \\right)$$\nSetting $\\frac{\\partial S}{\\partial \\hat{b}} = 0$:\n$$\\hat{a}\\sum_{i=1}^{L} x_i + \\hat{b}\\sum_{i=1}^{L} x_i^2 = \\sum_{i=1}^{L} x_i y_i \\quad (\\text{Normal Equation 2})$$\n\n**4. Solution of the Normal Equations**\n\nWe now have a system of two linear equations for the two unknowns $\\hat{a}$ and $\\hat{b}$. We can solve this system to obtain the OLS estimators.\n\nLet $\\bar{x} = \\frac{1}{L}\\sum_{i=1}^{L} x_i$ and $\\bar{y} = \\frac{1}{L}\\sum_{i=1}^{L} y_i$ be the sample means. From Normal Equation $1$, we can express $\\hat{a}$ in terms of $\\hat{b}$:\n$$L\\hat{a} = \\sum y_i - \\hat{b}\\sum x_i$$\n$$\\hat{a} = \\frac{\\sum y_i}{L} - \\hat{b}\\frac{\\sum x_i}{L} = \\bar{y} - \\hat{b}\\bar{x}$$\n\nSubstitute this expression for $\\hat{a}$ into Normal Equation $2$:\n$$(\\bar{y} - \\hat{b}\\bar{x})\\sum x_i + \\hat{b}\\sum x_i^2 = \\sum x_i y_i$$\n$$\\bar{y}\\sum x_i - \\hat{b}\\bar{x}\\sum x_i + \\hat{b}\\sum x_i^2 = \\sum x_i y_i$$\nGroup the terms involving $\\hat{b}$:\n$$\\hat{b}\\left(\\sum x_i^2 - \\bar{x}\\sum x_i\\right) = \\sum x_i y_i - \\bar{y}\\sum x_i$$\nSolving for $\\hat{b}$:\n$$\\hat{b} = \\frac{\\sum x_i y_i - \\bar{y}\\sum x_i}{\\sum x_i^2 - \\bar{x}\\sum x_i}$$\nThis can be written more compactly using the definition of sample covariance and variance. The numerator is $\\sum (x_i - \\bar{x})(y_i - \\bar{y})$ and the denominator is $\\sum (x_i - \\bar{x})^2$.\n$$\\hat{b} = \\frac{\\sum_{i=1}^{L} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{L} (x_i - \\bar{x})^2}$$\nOnce $\\hat{b}$ is calculated, $\\hat{a}$ is found from:\n$$\\hat{a} = \\bar{y} - \\hat{b}\\bar{x}$$\n\n**5. Algorithmic Summary and Implementation**\n\nFor each test case with given length $L$, true parameters $(a_{\\text{true}}, b_{\\text{true}})$, and noise vector $\\Delta Q$:\n1.  Generate the cycle indices: $x_i = i$ for $i \\in \\{1, 2, \\dots, L\\}$.\n2.  Calculate the true error probabilities: $p_i^{\\text{true}} = a_{\\text{true}} + b_{\\text{true}} \\cdot x_i$.\n3.  Calculate the observed Phred scores: $Q_i^{\\text{obs}} = -10 \\log_{10}(p_i^{\\text{true}}) + \\delta_i$, where $\\delta_i$ are the elements of $\\Delta Q$.\n4.  Transform the observed scores back to observed error probabilities: $y_i = 10^{-Q_i^{\\text{obs}}/10}$.\n5.  Calculate the sample means of the cycle indices and the observed error probabilities: $\\bar{x} = \\frac{1}{L}\\sum_{i=1}^{L} x_i$ and $\\bar{y} = \\frac{1}{L}\\sum_{i=1}^{L} y_i$.\n6.  Calculate the OLS estimate for the slope, $\\hat{b}$:\n    $$\\hat{b} = \\frac{\\sum_{i=1}^{L} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{L} (x_i - \\bar{x})^2}$$\n7.  Calculate the OLS estimate for the intercept, $\\hat{a}$:\n    $$\\hat{a} = \\bar{y} - \\hat{b}\\bar{x}$$\n8.  Round the estimated parameters $(\\hat{a}, \\hat{b})$ to $6$ decimal places and report the result.\n\nThis procedure is implemented below in Python to solve the provided test cases.\n\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies an OLS estimator for a linear model of sequencing error probability.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'L': 10, 'a_true': 0.005, 'b_true': 0.0004, 'delta_q': np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])},\n        {'L': 10, 'a_true': 0.002, 'b_true': 0.00025, 'delta_q': np.array([-0.05, 0.03, -0.02, 0.01, 0.00, -0.01, 0.02, -0.03, 0.04, -0.02])},\n        {'L': 10, 'a_true': 0.01, 'b_true': 0.0, 'delta_q': np.array([0.005, -0.004, 0.003, -0.002, 0.001, 0.0, -0.001, 0.002, -0.003, 0.004])},\n        {'L': 10, 'a_true': 0.0001, 'b_true': 0.00001, 'delta_q': np.array([0.1, -0.1, 0.05, -0.05, 0.02, -0.02, 0.0, 0.03, -0.03, 0.01])}\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Extract parameters for the current test case\n        L = case['L']\n        a_true = case['a_true']\n        b_true = case['b_true']\n        delta_q = case['delta_q']\n        \n        # 1. Generate cycle indices (independent variable, x)\n        # x_i = i, for i in {1, 2, ..., L}\n        x = np.arange(1, L + 1)\n        \n        # 2. Calculate true error probabilities from the linear model\n        p_true = a_true + b_true * x\n        \n        # 3. Calculate observed Phred scores (Q_obs)\n        # Handle cases where p_true could be non-positive, although not expected with given inputs.\n        with np.errstate(divide='raise', invalid='raise'):\n            try:\n                q_obs = -10 * np.log10(p_true) + delta_q\n            except FloatingPointError:\n                # This block would handle log(0) or log(0), indicating an invalid p_true.\n                # For this problem, all p_true values are positive.\n                pass\n        \n        # 4. Transform observed Q-scores back to observed error probabilities (dependent variable, y)\n        y = 10**(-q_obs / 10)\n        \n        # 5. Perform Ordinary Least Squares (OLS) regression to estimate a and b.\n        # The model is y_i = a + b * x_i\n        \n        # Calculate sample means\n        x_mean = np.mean(x)\n        y_mean = np.mean(y)\n        \n        # Calculate the parameters b_hat and a_hat using the derived OLS formulas\n        # b_hat = sum((x_i - x_mean) * (y_i - y_mean)) / sum((x_i - x_mean)^2)\n        numerator = np.sum((x - x_mean) * (y - y_mean))\n        denominator = np.sum((x - x_mean)**2)\n        \n        if denominator == 0:\n            # This case occurs if all x values are the same, which is not possible here\n            # as x = [1, 2, ..., L]. Handle for robustness.\n            b_hat = 0.0\n        else:\n            b_hat = numerator / denominator\n        \n        # a_hat = y_mean - b_hat * x_mean\n        a_hat = y_mean - b_hat * x_mean\n        \n        # 6. Round the estimates to 6 decimal places\n        a_est = round(a_hat, 6)\n        b_est = round(b_hat, 6)\n        \n        results.append([a_est, b_est])\n\n    # Final print statement in the exact required format.\n    # We construct the string manually to avoid spaces introduced by str(list).\n    inner_strings = [f\"[{res[0]},{res[1]}]\" for res in results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    # This print statement is for verification; the actual answer is the string itself.\n    # print(final_output)\n\n```", "answer": "[[0.005,0.0004],[0.002001,0.00025],[0.01,0.0],[-0.000004,0.000021]]", "id": "4374748"}]}