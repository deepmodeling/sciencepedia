{"hands_on_practices": [{"introduction": "This practice delves into the heart of the Sequencing by Synthesis cycle by modeling the chemical kinetics of the reversible terminator cleavage step. By deriving the fraction of deblocked nucleotides from first principles, you will gain a quantitative understanding of how incomplete chemical reactions directly lead to \"phasing,\" a primary source of error in SBS data [@problem_id:4380045]. This exercise builds a crucial bridge between fundamental reaction rates and their system-level consequences on sequencing accuracy.", "problem": "In Sequencing by Synthesis (SBS) used for precision medicine and genomic diagnostics, each cycle terminates with a reversible terminator nucleotide whose blocking group must be removed by a cleavage step before the next incorporation. Consider a homogeneous population of blocked nucleotides, each independently undergoing a unimolecular cleavage reaction that converts a blocked state to a deblocked state under constant reagent conditions. Assume the cleavage is the sole active pathway, there is no rebinding or reblocking during the cleavage window, and the kinetics are first-order in the blocked species with rate constant $k_{\\text{cleave}}$ (with units of inverse time). At time $t=0$, all incorporated nucleotides are blocked. Starting from the law of mass action for a unimolecular reaction and the definition of first-order kinetics, derive the closed-form analytical expression for the expected fraction $F_{\\text{deblock}}(t)$ of nucleotides that are successfully deblocked by time $t$ as a function of $k_{\\text{cleave}}$ and $t$. Express your final answer as a single analytic expression; no numerical evaluation or rounding is required. Then, briefly discuss how incomplete deblocking within a cycle contributes to phasing and pre-phasing across cycles in SBS, referencing your derived $F_{\\text{deblock}}(t)$ to articulate the mechanism in qualitative terms. Do not include any discussion in your final answer; only the analytic expression for $F_{\\text{deblock}}(t)$ should be provided there.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of chemical kinetics and molecular biology, specifically concerning the sequencing by synthesis (SBS) methodology. It is well-posed, providing all necessary information—the reaction order, initial conditions, and a clearly defined objective—to derive a unique and meaningful analytical solution. The terminology is precise and objective. Therefore, I will proceed with the derivation and subsequent discussion.\n\nLet $N(t)$ represent the number of blocked nucleotides at any given time $t$. The total number of incorporated nucleotides is a constant, which we can denote as $N_0$. At time $t=0$, all nucleotides are in the blocked state, so the initial condition is $N(0) = N_0$.\n\nThe problem states that the cleavage reaction is a unimolecular process that follows first-order kinetics. The law of mass action for this reaction dictates that the rate of change of the concentration (or number) of the reactant is proportional to its current concentration. In this case, the reactant is the population of blocked nucleotides. The rate of disappearance of blocked nucleotides is thus given by the differential equation:\n$$\n-\\frac{dN(t)}{dt} = k_{\\text{cleave}} N(t)\n$$\nwhere $k_{\\text{cleave}}$ is the first-order rate constant. This is a first-order linear ordinary differential equation. We can solve it by separating the variables:\n$$\n\\frac{dN(t)}{N(t)} = -k_{\\text{cleave}} dt\n$$\nNext, we integrate both sides of the equation. We integrate the left side from the initial number of blocked nucleotides, $N(0) = N_0$, to the number at time $t$, $N(t)$. We integrate the right side from the initial time, $t=0$, to time $t$:\n$$\n\\int_{N_0}^{N(t)} \\frac{1}{N'} dN' = \\int_{0}^{t} -k_{\\text{cleave}} dt'\n$$\nPerforming the integration yields:\n$$\n[\\ln(N')]_{N_0}^{N(t)} = [-k_{\\text{cleave}} t']_{0}^{t}\n$$\n$$\n\\ln(N(t)) - \\ln(N_0) = -k_{\\text{cleave}} t\n$$\nUsing the properties of logarithms, this can be written as:\n$$\n\\ln\\left(\\frac{N(t)}{N_0}\\right) = -k_{\\text{cleave}} t\n$$\nTo solve for $N(t)$, we exponentiate both sides:\n$$\n\\frac{N(t)}{N_0} = \\exp(-k_{\\text{cleave}} t)\n$$\n$$\nN(t) = N_0 \\exp(-k_{\\text{cleave}} t)\n$$\nThis expression gives the number of nucleotides that remain blocked at time $t$.\n\nThe problem asks for the fraction of nucleotides that are successfully deblocked, $F_{\\text{deblock}}(t)$. Let $N_{\\text{deblock}}(t)$ be the number of deblocked nucleotides at time $t$. By conservation of the total number of nucleotides, we have:\n$$\nN_{\\text{deblock}}(t) + N(t) = N_0\n$$\nTherefore, the number of deblocked nucleotides is:\n$$\nN_{\\text{deblock}}(t) = N_0 - N(t) = N_0 - N_0 \\exp(-k_{\\text{cleave}} t)\n$$\n$$\nN_{\\text{deblock}}(t) = N_0 (1 - \\exp(-k_{\\text{cleave}} t))\n$$\nThe fraction of successfully deblocked nucleotides, $F_{\\text{deblock}}(t)$, is the number of deblocked nucleotides divided by the total number of nucleotides, $N_0$:\n$$\nF_{\\text{deblock}}(t) = \\frac{N_{\\text{deblock}}(t)}{N_0} = \\frac{N_0 (1 - \\exp(-k_{\\text{cleave}} t))}{N_0}\n$$\nThis simplifies to the final analytical expression for the fraction of deblocked nucleotides as a function of time:\n$$\nF_{\\text{deblock}}(t) = 1 - \\exp(-k_{\\text{cleave}} t)\n$$\n\nThe problem also requires a brief discussion of phasing and pre-phasing in the context of this derivation.\n\nIn an ideal SBS cycle, all incorporated nucleotides are deblocked, allowing for synchronous extension of all DNA templates in the subsequent cycle. However, the cleavage reaction is run for a finite time, let's call it $t_{\\text{cycle}}$. The fraction of successfully deblocked nucleotides is therefore $F_{\\text{deblock}}(t_{\\text{cycle}}) = 1 - \\exp(-k_{\\text{cleave}} t_{\\text{cycle}})$. Since $k_{\\text{cleave}}$ and $t_{\\text{cycle}}$ are finite, this fraction is always less than $1$. The fraction of nucleotides that fail to deblock in a given cycle is $1 - F_{\\text{deblock}}(t_{\\text{cycle}}) = \\exp(-k_{\\text{cleave}} t_{\\text{cycle}})$.\n\nThis incomplete deblocking is the primary cause of a sequencing artifact known as **phasing**. A strand with a blocked nucleotide cannot be extended in the next incorporation cycle. It emits no signal and falls behind the majority of the strands in its cluster. If this lagging strand is successfully deblocked in the subsequent cycle's cleavage step, it will then incorporate the nucleotide corresponding to the previous cycle. For instance, if a strand fails to deblock in cycle $n$, it will not incorporate the nucleotide for cycle $n+1$. If it is then deblocked in cycle $n+1$, it will incorporate the base for cycle $n+1$ during cycle $n+2$. From cycle $n+2$ onwards, its signal will be out of phase with the rest of the cluster, which is one cycle ahead. The per-cycle probability of a strand becoming phased is precisely this failure rate, $\\exp(-k_{\\text{cleave}} t_{\\text{cycle}})$.\n\n**Pre-phasing**, or \"running ahead,\" is caused by mechanisms other than incomplete deblocking. The most common cause is the incorporation of a nucleotide that lacks the reversible 3'-terminator group. This allows the strand to incorporate two or more nucleotides in a single cycle, causing it to be ahead of the main population. Another cause can be the premature loss of the blocking group due to side reactions. The model derived, which only considers the primary cleavage pathway, directly explains phasing but not pre-phasing. To minimize both artifacts, $k_{\\text{cleave}} t_{\\text{cycle}}$ must be large to make $\\exp(-k_{\\text{cleave}} t_{\\text{cycle}})$ negligibly small, while the purity of the blocked nucleotides must be extremely high to prevent pre-phasing.", "answer": "$$\n\\boxed{1 - \\exp(-k_{\\text{cleave}} t)}\n$$", "id": "4380045"}, {"introduction": "Building on the understanding of individual error sources, this exercise challenges you to perform a holistic root-cause analysis of a failed sequencing run. Presented with multiple concurrent failure modes—over-clustering, signal de-synchronization, and background noise—you must apply first principles of surface chemistry, reaction kinetics, and mass transport to diagnose the underlying issues [@problem_id:4380004]. This practice hones the critical diagnostic skills required to troubleshoot complex high-throughput genomic assays.", "problem": "An oncology sequencing core uses sequencing by synthesis (SBS) to genotype circulating tumor DNA for guiding targeted therapy. A recent run showed three concurrent failure modes in raw imaging and quality control metrics: overlapping clusters in diffraction-limited spots, progressive loss of signal synchrony across cycles, and persistent background fluorescence in one emission channel across successive cycles. The instrument is a four-dye SBS platform that cycles between incorporation of reversible terminator nucleotides, chemical cleavage of both the fluorophore and the reversible terminator, and washing. The surface is a random lawn that captures denatured single-stranded library molecules, which seed bridge amplification clusters before imaging.\n\nFrom first principles, consider the following bases:\n- Stochastic seeding on a random surface obeys occupancy statistics in which the expected number of seed molecules per capture area can be modeled by a mean load parameter $\\lambda$, with the multi-occupancy probability increasing monotonically with $\\lambda$.\n- Reversible terminator incorporation is a kinetically controlled process in which the fraction of templates completing a single incorporation within a cycle depends on the relationship between the allotted time $t_{\\mathrm{inc}}$ and an effective incorporation rate constant $k_{\\mathrm{inc}}$, which itself depends on polymerase concentration and substrate availability.\n- Fluorophore cleavage and removal depend on the cleavage rate $k_{\\mathrm{clv}}$ and convective-diffusive wash, with persistent free dye or uncleaved fluorophores contributing to channel-specific background; spectral cross-talk is addressed by a color matrix calibration derived from known-balanced base compositions.\n\nWhich of the following root-cause analyses and corrective actions are valid consequences of these principles for the three observed failure modes? Select all that apply.\n\nA. Overlapping clusters arise because the mean template load per effective capture area $\\lambda$ was too high, inflating the probability $P(\\ge 2)$ of multi-occupancy and thereby producing mixed, spatially co-localized clusters. Reducing the denatured library molarity to lower $\\lambda$, re-quantifying the molarity with an orthogonal method, and tightening the fragment size distribution to reduce heterogeneous diffusion during seeding will decrease overlap without changing optics.\n\nB. Progressive loss of synchrony (phasing and prephasing) can result from $t_{\\mathrm{inc}}$ being too short relative to $k_{\\mathrm{inc}}$, and from incomplete removal of the reversible terminator that blocks the $3'$ position. Increasing $t_{\\mathrm{inc}}$ or polymerase concentration to raise the effective incorporation velocity, verifying balanced and fresh reversible terminator nucleotide pools, and ensuring complete deprotection of the $3'$ block will reduce the per-cycle lag and lead fractions.\n\nC. Persistent background in one emission channel is best corrected by increasing laser power and shortening the wash, which will photobleach carryover dye more quickly. Because detection cross-talk is purely optical, changing cleavage chemistry cannot impact channel-specific background if the optics are unchanged.\n\nD. Persistent background in one channel is consistent with incomplete fluorophore cleavage or transport-limited washout, and with a miscalibrated color matrix. Replacing aged cleavage reagents or extending the cleavage step to increase the extent of reaction given $k_{\\mathrm{clv}}$, increasing wash volume and flow to raise the mass transfer coefficient, and re-estimating the color cross-talk matrix using a balanced control library will reduce residual signal and improve spectral unmixing.\n\nE. To compensate for poor synchrony, increasing library loading to achieve higher cluster density will average over more molecules per pixel and thereby mitigate the apparent phasing; this improves the time-course signal without modifying chemistry.\n\nAnswer choices may be independently correct or incorrect; select all that are correct, based solely on the stated principles without assuming proprietary implementation details.", "solution": "The problem statement has been validated and is determined to be sound.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n-   **Technology:** Sequencing by synthesis (SBS) on a four-dye platform.\n-   **Application:** Genotyping circulating tumor DNA.\n-   **Observed Failure Modes:** ($1$) Overlapping clusters in diffraction-limited spots. ($2$) Progressive loss of signal synchrony across cycles. ($3$) Persistent background fluorescence in one emission channel across successive cycles.\n-   **Instrument Process:** A cycle of ($a$) incorporation of reversible terminator nucleotides, ($b$) chemical cleavage of fluorophore and reversible terminator, and ($c$) washing.\n-   **Library Preparation:** Denatured single-stranded library molecules are seeded on a random lawn surface, followed by bridge amplification to form clusters.\n-   **Governing Principles:**\n    1.  **Seeding:** Stochastic seeding on a random surface is modeled by a mean load parameter $\\lambda$. The probability of multi-occupancy, $P(\\ge 2)$, increases monotonically with $\\lambda$.\n    2.  **Incorporation:** Incorporation is a kinetic process. The fraction of templates completing incorporation depends on the allotted time $t_{\\mathrm{inc}}$ and an effective rate constant $k_{\\mathrm{inc}}$. The rate $k_{\\text{inc}}$ is a function of polymerase concentration and substrate availability.\n    3.  **Cleavage & Wash:** Fluorophore cleavage depends on a cleavage rate $k_{\\mathrm{clv}}$. Washing is a convective-diffusive process. Persistent background fluorescence is caused by free dye or uncleaved fluorophores. Spectral cross-talk is corrected by a calibrated color matrix.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective.\n-   **Scientific Soundness:** The description of SBS chemistry, clonal cluster amplification, and the associated failure modes (e.g., over-clustering, phasing/prephasing, background signal) is a faithful and accurate representation of the principles behind prevalent next-generation sequencing platforms. The provided first principles—stochastic statistics for surface seeding, chemical kinetics for reaction steps, and mass transport phenomena for washing—are the correct, fundamental physical and chemical models for these processes. No scientific laws or established facts are violated.\n-   **Well-Posedness:** The problem presents a set of empirical observations and a set of governing principles, tasking the solver with making logical deductions to connect root causes with corrective actions. The structure is sound for a deductive reasoning problem.\n-   **Objectivity:** The language is technical and precise. The terminology (e.g., $\\lambda$, $k_{\\mathrm{inc}}$, $k_{\\mathrm{clv}}$, phasing, cross-talk) is standard and well-defined within the field of genomics and molecular biology. The problem is free from subjective or speculative claims.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. A full analysis of the options will be performed.\n\n**Solution Derivation**\n\nThe three observed failure modes are analyzed based on the provided principles.\n\n1.  **Overlapping Clusters:** This is a spatial problem where distinct DNA clusters are too close to be resolved by the imaging system. Principle $1$ directly addresses this. The random seeding of molecules on a surface can be modeled by a Poisson distribution. The probability of a single capture site being occupied by $n$ molecules is $P(n) = \\frac{\\lambda^n e^{-\\lambda}}{n!}$, where $\\lambda$ is the average number of molecules per site. The probability of a site being occupied by more than one molecule, $P(n \\ge 2) = 1 - P(0) - P(1) = 1 - e^{-\\lambda} - \\lambda e^{-\\lambda}$, is a monotonically increasing function of $\\lambda$. Overlapping clusters are formed from these multi-occupancy sites. Therefore, a high incidence of overlapping clusters is a direct consequence of $\\lambda$ being too high.\n\n2.  **Progressive Loss of Synchrony:** This is a temporal problem where DNA strands within a single cluster fall out of step with the current synthesis cycle. This manifests as phasing (lagging) and prephasing (leading).\n    -   **Phasing:** A strand fails to incorporate a nucleotide in the current cycle. This can be due to two main reasons: ($i$) The incorporation reaction is incomplete. Principle $2$ states this is a kinetic process dependent on $t_{\\mathrm{inc}}$ and $k_{\\mathrm{inc}}$. If $t_{\\mathrm{inc}}$ is too short or $k_{\\mathrm{inc}}$ is too low (e.g., due to low polymerase concentration or degraded nucleotide substrate), a fraction of strands will not be extended. ($ii$) The $3'$ blocking group from the previous cycle's nucleotide was not cleaved. This prevents the polymerase from adding the next nucleotide.\n    -   **Prephasing:** A strand incorporates more than one nucleotide in a cycle. This occurs if a fraction of the reversible terminator nucleotides lack a functional $3'$ block, allowing a second incorporation event.\n    Both phenomena degrade signal quality by mixing signals from different cycle numbers at each imaging step.\n\n3.  **Persistent Background in One Channel:** This is a signal-to-noise problem specific to one of the four nucleotide/dye channels. Principle $3$ covers this. A high background in, for instance, the 'G' channel implies an excess of the 'G' fluorophore is present during imaging.\n    -   **Cause 1 (Incomplete Cleavage):** The fluorophore from the previous cycle's 'G' incorporation was not chemically cleaved from the DNA strand. Its removal depends on the cleavage reaction, governed by rate $k_{\\mathrm{clv}}$. Inefficient cleavage (low $k_{\\mathrm{clv}}$, due to aged reagents or insufficient time) leaves the dye attached, where it will fluoresce again.\n    -   **Cause 2 (Inefficient Washout):** The fluorophore was successfully cleaved but not fully washed away. The free dye remains on the flow cell surface. Principle $3$ describes this as a convective-diffusive mass transport problem. An inefficient wash (low flow rate, insufficient volume, short duration) will leave residual free dye.\n    -   **Cause 3 (Spectral Crosstalk):** The emission spectrum of one dye bleeds into the photodetector of another. Principle $3$ notes this is corrected by a color matrix. If this matrix is miscalibrated, it can fail to properly subtract this bleed-through, artifactually creating or enhancing a signal in one channel.\n\n**Option-by-Option Analysis**\n\n**A. Overlapping clusters arise because the mean template load per effective capture area $\\lambda$ was too high, inflating the probability $P(\\ge 2)$ of multi-occupancy and thereby producing mixed, spatially co-localized clusters. Reducing the denatured library molarity to lower $\\lambda$, re-quantifying the molarity with an orthogonal method, and tightening the fragment size distribution to reduce heterogeneous diffusion during seeding will decrease overlap without changing optics.**\n-   **Analysis:** This statement correctly diagnoses the cause of overlapping clusters as excessive library loading ($\\lambda$ is too high), directly invoking the multi-occupancy probability $P(\\ge 2)$ as described in Principle $1$. The proposed corrective actions are all valid and standard practice. Reducing library molarity is the direct way to lower $\\lambda$. Re-quantification is a prudent quality control step to ensure the intended $\\lambda$ is achieved. Tightening the size distribution promotes more uniform amplification and can help resolve clusters, complementing the primary fix of reducing molarity.\n-   **Verdict:** **Correct**.\n\n**B. Progressive loss of synchrony (phasing and prephasing) can result from $t_{\\mathrm{inc}}$ being too short relative to $k_{\\mathrm{inc}}$, and from incomplete removal of the reversible terminator that blocks the $3'$ position. Increasing $t_{\\mathrm{inc}}$ or polymerase concentration to raise the effective incorporation velocity, verifying balanced and fresh reversible terminator nucleotide pools, and ensuring complete deprotection of the $3'$ block will reduce the per-cycle lag and lead fractions.**\n-   **Analysis:** This option accurately identifies the primary causes of phasing: incomplete incorporation (a kinetic problem related to $t_{\\mathrm{inc}}$ and $k_{\\mathrm{inc}}$ as per Principle $2$) and incomplete deprotection of the $3'$ block. The proposed solutions are direct remedies: increasing $t_{\\mathrm{inc}}$ or polymerase concentration (to increase $k_{\\mathrm{inc}}$) improves incorporation efficiency. Ensuring reagents are fresh and active addresses both incomplete incorporation (degraded nucleotides) and incomplete deprotection (degraded cleavage/deprotection agents). These actions would reduce the fraction of lagging strands (phasing).\n-   **Verdict:** **Correct**.\n\n**C. Persistent background in one emission channel is best corrected by increasing laser power and shortening the wash, which will photobleach carryover dye more quickly. Because detection cross-talk is purely optical, changing cleavage chemistry cannot impact channel-specific background if the optics are unchanged.**\n-   **Analysis:** This statement is incorrect on multiple grounds. Shortening the wash would be counterproductive, as it would reduce the efficiency of the convective-diffusive removal of free fluorophores, thus increasing background. While increasing laser power can increase photobleaching, it also increases the background fluorescence itself and can cause photo-damage to the library, making it a poor solution. The second sentence contains a logical flaw. Principle $3$ explicitly states that \"uncleaved fluorophores\" contribute to background. This is a chemical problem, not an optical one. Therefore, improving the cleavage chemistry is a primary method for reducing channel-specific background.\n-   **Verdict:** **Incorrect**.\n\n**D. Persistent background in one channel is consistent with incomplete fluorophore cleavage or transport-limited washout, and with a miscalibrated color matrix. Replacing aged cleavage reagents or extending the cleavage step to increase the extent of reaction given $k_{\\mathrm{clv}}$, increasing wash volume and flow to raise the mass transfer coefficient, and re-estimating the color cross-talk matrix using a balanced control library will reduce residual signal and improve spectral unmixing.**\n-   **Analysis:** This option correctly identifies the three main root causes of channel-specific background, consistent with Principle $3$: incomplete cleavage (a kinetic problem related to $k_{\\mathrm{clv}}$), inefficient washing (a mass transport problem), and poor spectral unmixing (a calibration problem). The proposed corrective actions are chemically and physically sound: improving the cleavage reaction by replacing reagents or increasing time, improving the wash by increasing volume/flow, and recalibrating the color matrix. These are the appropriate, standard troubleshooting steps.\n-   **Verdict:** **Correct**.\n\n**E. To compensate for poor synchrony, increasing library loading to achieve higher cluster density will average over more molecules per pixel and thereby mitigate the apparent phasing; this improves the time-course signal without modifying chemistry.**\n-   **Analysis:** This proposal is fundamentally flawed and counterproductive. Increasing library loading leads to higher cluster density, which, as established in the analysis of option A, causes cluster overlap. Overlapping clusters generate a mixed, uninterpretable signal, which is a catastrophic failure, not a form of beneficial \"averaging\". It does not mitigate phasing; it compounds the phasing problem with a spatial deconvolution problem. This action would exacerbate one failure mode (low density) to create a worse one (high density/overlap) and would not solve the original chemistry-based synchrony issue.\n-   **Verdict:** **Incorrect**.", "answer": "$$\\boxed{\\text{ABD}}$$", "id": "4380004"}, {"introduction": "Beyond understanding the chemical and physical limitations of the sequencing process, robust experimental design is paramount. This problem focuses on the design of sample-identifying barcodes, a key component of multiplexed sequencing, by applying principles from coding theory [@problem_id:4380013]. You will use the concept of Hamming distance to determine the requirements for a barcode set that can tolerate substitution errors, providing a hands-on introduction to the mathematical foundations of error-correcting codes in genomics.", "problem": "A multiplexed Next-Generation Sequencing (NGS) assay using Sequencing by Synthesis (SBS) relies on short deoxyribonucleic acid (DNA) sample-identifying barcodes. Consider designing a barcode set over a four-letter nucleotide alphabet, so the alphabet size is $q=4$. Each barcode has fixed length $L=10$. In SBS, the dominant indexing errors after quality trimming are single-nucleotide substitutions, while insertions and deletions are rare and can be largely filtered; assume the decoder aims to correct up to $e=1$ substitution errors in a barcode using bounded-distance decoding on the space of length-$L$ sequences equipped with Hamming distance. Starting from the definition of Hamming distance as a metric on length-$L$ words over an alphabet of size $q$ and without invoking any pre-stated coding formulas, do the following: (i) justify the minimum Hamming distance requirement $d$ that guarantees correction of up to $e$ substitution errors; (ii) derive an upper bound on the maximum number of barcodes $M$ of length $L$ over an alphabet of size $q$ that can be chosen to meet this requirement by ensuring disjoint decoding spheres of radius $e$; (iii) evaluate this upper bound numerically for the given parameters $L=10$, $q=4$, and $e=1$. Express the final count $M$ as an exact integer with no rounding and no units.", "solution": "The appropriate mathematical setting is the Hamming space of length-$L$ words over an alphabet of size $q$, with Hamming distance defined between two words. We begin by recalling core definitions.\n\nThe Hamming distance between two words $x=(x_{1},\\dots,x_{L})$ and $y=(y_{1},\\dots,y_{L})$ over a finite alphabet is\n$$\nd_{H}(x,y)=\\sum_{i=1}^{L}\\mathbf{1}\\{x_{i}\\neq y_{i}\\},\n$$\nwhich counts the number of positions at which $x$ and $y$ differ. The Hamming distance is a metric, so it satisfies non-negativity, symmetry, identity of indiscernibles, and in particular the triangle inequality:\n$$\nd_{H}(x,z)\\leq d_{H}(x,y)+d_{H}(y,z)\\quad\\text{for all words }x,y,z.\n$$\n\nPart (i): To guarantee correction of up to $e$ substitution errors under bounded-distance decoding, the minimum distance $d$ of the code must be large enough that balls of radius $e$ around distinct codewords do not overlap. Suppose the code has minimum distance $d$, meaning that for any two distinct codewords $c_{1}$ and $c_{2}$, one has $d_{H}(c_{1},c_{2})\\geq d$. Consider a received word $r$ that differs from the transmitted codeword $c_{1}$ by at most $e$ substitutions, so $d_{H}(r,c_{1})\\leq e$. If there were another codeword $c_{2}$ such that $d_{H}(r,c_{2})\\leq e$, then by the triangle inequality,\n$$\nd_{H}(c_{1},c_{2})\\leq d_{H}(c_{1},r)+d_{H}(r,c_{2})\\leq e+e=2e.\n$$\nTherefore, to preclude this possibility for any pair of distinct codewords, it is sufficient to require $d\\geq 2e+1$. This ensures that no single received word can lie within Hamming distance $e$ of two different codewords, enabling unique correction of up to $e$ substitution errors.\n\nPart (ii): To derive an upper bound on the maximum number of barcodes $M$ of length $L$ over an alphabet of size $q$ that can correct up to $e$ substitution errors with minimum distance at least $2e+1$, we consider a sphere-packing argument in Hamming space. The space of all length-$L$ words over an alphabet of size $q$ has cardinality $q^{L}$. Around any codeword $c$, the Hamming ball of radius $e$ consists of all words at Hamming distance at most $e$ from $c$. The number of words at exactly Hamming distance $i$ from $c$ is $\\binom{L}{i}(q-1)^{i}$ because one chooses $i$ positions to change (there are $\\binom{L}{i}$ choices), and at each such position one can change the symbol to any of the other $q-1$ symbols. Hence, the volume of a Hamming ball of radius $e$ is\n$$\nV(L,e,q)=\\sum_{i=0}^{e}\\binom{L}{i}(q-1)^{i}.\n$$\nIf the code has minimum distance at least $2e+1$, then the balls of radius $e$ around distinct codewords are disjoint. The union of these disjoint balls is a subset of the full space, so counting yields\n$$\nM\\cdot V(L,e,q)\\leq q^{L}.\n$$\nTherefore, an upper bound on $M$ is\n$$\nM\\leq \\left\\lfloor \\frac{q^{L}}{\\sum_{i=0}^{e}\\binom{L}{i}(q-1)^{i}} \\right\\rfloor,\n$$\nwhere the floor reflects that $M$ must be an integer.\n\nPart (iii): Evaluate for $L=10$, $q=4$, and $e=1$. First, compute $V(10,1,4)$:\n$$\nV(10,1,4)=\\binom{10}{0}(4-1)^{0}+\\binom{10}{1}(4-1)^{1}=1+10\\cdot 3=31.\n$$\nNext, compute $q^{L}$:\n$$\n4^{10}=1{,}048{,}576.\n$$\nThus,\n$$\nM\\leq \\left\\lfloor \\frac{4^{10}}{31} \\right\\rfloor=\\left\\lfloor \\frac{1{,}048{,}576}{31} \\right\\rfloor.\n$$\nPerform the division:\n$$\n31\\cdot 33{,}800=1{,}047{,}800,\\quad 1{,}048{,}576-1{,}047{,}800=776,\n$$\nand\n$$\n31\\cdot 25=775,\\quad 1{,}047{,}800+775=1{,}048{,}575.\n$$\nTherefore,\n$$\n\\left\\lfloor \\frac{1{,}048{,}576}{31} \\right\\rfloor=33{,}825.\n$$\nThis is the maximal code size under the Hamming sphere-packing bound for the given parameters, consistent with designing a barcode set of minimum distance $d\\geq 2e+1=3$ to correct up to $e=1$ substitution errors in SBS demultiplexing.", "answer": "$$\\boxed{33825}$$", "id": "4380013"}]}