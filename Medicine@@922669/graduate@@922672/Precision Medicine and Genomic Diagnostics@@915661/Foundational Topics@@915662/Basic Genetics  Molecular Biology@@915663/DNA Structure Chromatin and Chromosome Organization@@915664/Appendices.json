{"hands_on_practices": [{"introduction": "The immense length of the eukaryotic genome must be compactly organized within the cell nucleus, a feat accomplished primarily by wrapping DNA around histone proteins to form nucleosomes. This practice explores the energetic cost of this fundamental process using the worm-like chain model, a cornerstone of polymer physics. By calculating the bending energy required to form a single nucleosome, you will gain a quantitative appreciation for the physical principles governing chromatin structure [@problem_id:4335315].", "problem": "In precision medicine and genomic diagnostics, the energetics of deoxyribonucleic acid (DNA) bending within chromatin influence nucleosome stability and accessibility to enzymatic probes. Consider the worm-like chain model of a semiflexible polymer where the bending modulus is given by $A = k_{B} T P$, with $k_{B}$ denoting the Boltzmann constant, $T$ the absolute temperature, and $P$ the persistence length. A smooth space curve with arc-length parameter $s$ and curvature $\\kappa(s)$ has elastic bending energy $E$ given by the fundamental worm-like chain expression $E = \\frac{1}{2} A \\int \\kappa(s)^{2} \\, ds$. Assume nucleosomal DNA wraps around the histone octamer at approximately constant curvature set by a geometric radius $R$, so that $\\kappa(s) \\approx \\frac{1}{R}$. Take the histone core radius to be $R = 4.2\\,\\mathrm{nm}$, the DNA persistence length at physiological ionic strength to be $P = 50\\,\\mathrm{nm}$, and the wrapped DNA contour length to be $L = 147 \\times 0.34\\,\\mathrm{nm}$, where $0.34\\,\\mathrm{nm}$ is the rise per base pair. Using these physically justified inputs and the above fundamental relations, derive and compute the bending energy for this wrapped segment. Express the final answer as a dimensionless multiple of Boltzmann constant times temperature (that is, report $\\frac{E}{k_{B} T}$), and round your answer to four significant figures.", "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n- Bending modulus: $A = k_{B} T P$\n- Boltzmann constant: $k_{B}$\n- Absolute temperature: $T$\n- Persistence length: $P$\n- Bending energy expression: $E = \\frac{1}{2} A \\int \\kappa(s)^{2} \\, ds$\n- Arc-length parameter: $s$\n- Curvature: $\\kappa(s)$\n- Approximation for curvature: $\\kappa(s) \\approx \\frac{1}{R}$\n- Histone core radius: $R = 4.2\\,\\mathrm{nm}$\n- DNA persistence length: $P = 50\\,\\mathrm{nm}$\n- Wrapped DNA contour length: $L = 147 \\times 0.34\\,\\mathrm{nm}$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, employing the well-established worm-like chain (WLC) model to describe the elastic properties of DNA, a standard approach in biophysics. The provided equations for bending modulus ($A$) and bending energy ($E$) are the correct fundamental expressions from this model. The physical parameters given ($P = 50\\,\\mathrm{nm}$, $R = 4.2\\,\\mathrm{nm}$, and the contour length derived from $147$ base pairs) are consistent with accepted values in the scientific literature for DNA at physiological conditions. The problem is well-posed, providing all necessary information and a clear objective, leading to a unique, meaningful solution. The language is precise and objective. Therefore, the problem is deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will now be derived.\n\nThe task is to compute the bending energy, $E$, for a segment of DNA wrapped around a histone core and express this energy as a dimensionless multiple of $k_{B} T$. The starting point is the provided expression for the elastic bending energy of a semiflexible polymer, as described by the worm-like chain model:\n$$E = \\frac{1}{2} A \\int_{0}^{L} \\kappa(s)^{2} \\, ds$$\nHere, $A$ is the bending modulus, $\\kappa(s)$ is the local curvature at arc-length position $s$, and the integration is performed over the total contour length, $L$, of the polymer segment.\n\nThe bending modulus, $A$, is given by the relation:\n$$A = k_{B} T P$$\nwhere $k_{B}$ is the Boltzmann constant, $T$ is the absolute temperature, and $P$ is the persistence length of the DNA.\n\nThe problem states that the DNA wraps around the histone octamer at an approximately constant curvature. This curvature is determined by the radius of the histone core, $R$. For a curve with a constant radius of curvature $R$, the curvature $\\kappa$ is given by $\\kappa = \\frac{1}{R}$. We are thus given the approximation:\n$$\\kappa(s) \\approx \\frac{1}{R}$$\n\nSubstituting the expressions for $A$ and $\\kappa(s)$ into the energy equation yields:\n$$E = \\frac{1}{2} (k_{B} T P) \\int_{0}^{L} \\left(\\frac{1}{R}\\right)^{2} \\, ds$$\nSince $R$ is a constant, the term $\\frac{1}{R^2}$ can be moved outside the integral:\n$$E = \\frac{k_{B} T P}{2 R^{2}} \\int_{0}^{L} \\, ds$$\nThe integral of $ds$ from $0$ to $L$ is simply the total contour length, $L$:\n$$\\int_{0}^{L} \\, ds = L$$\nThis simplifies the energy expression to:\n$$E = \\frac{k_{B} T P L}{2 R^{2}}$$\nThe problem requires the final answer to be the dimensionless quantity $\\frac{E}{k_{B} T}$. Dividing the above expression by $k_{B} T$, we get the final analytical formula for the desired quantity:\n$$\\frac{E}{k_{B} T} = \\frac{P L}{2 R^{2}}$$\n\nNow, we substitute the provided numerical values into this expression.\nThe persistence length is $P = 50\\,\\mathrm{nm}$.\nThe radius of the histone core is $R = 4.2\\,\\mathrm{nm}$.\nThe contour length, $L$, is given as the product of the number of base pairs ($147$) and the rise per base pair ($0.34\\,\\mathrm{nm}$):\n$$L = 147 \\times 0.34\\,\\mathrm{nm} = 49.98\\,\\mathrm{nm}$$\nSubstituting these values:\n$$\\frac{E}{k_{B} T} = \\frac{(50\\,\\mathrm{nm}) (49.98\\,\\mathrm{nm})}{2 (4.2\\,\\mathrm{nm})^{2}}$$\nThe units are dimensionally consistent, as $(\\mathrm{nm} \\times \\mathrm{nm}) / \\mathrm{nm}^2$ results in a dimensionless quantity.\nProceeding with the calculation:\n$$\\frac{E}{k_{B} T} = \\frac{2499}{2 \\times (4.2)^{2}} = \\frac{2499}{2 \\times 17.64} = \\frac{2499}{35.28}$$\nPerforming the final division:\n$$\\frac{E}{k_{B} T} \\approx 70.83333...$$\nThe problem requires the answer to be rounded to four significant figures.\nThe result is $70.83$.", "answer": "$$\\boxed{70.83}$$", "id": "4335315"}, {"introduction": "Beyond its physical structure, chromatin carries a rich layer of epigenetic information, such as histone modifications, that regulates gene expression. This exercise addresses a critical challenge in quantitative epigenomics: accurately measuring changes in these marks between different biological states [@problem_id:4335400]. You will implement a spike-in normalization strategy, a state-of-the-art technique used in precision medicine to distinguish true, locus-specific changes from global alterations in the epigenome.", "problem": "A laboratory is quantifying Histone H3 lysine 27 acetylation (H3K27ac) differences between a control and a treatment in a cancer cell line using Chromatin Immunoprecipitation followed by sequencing (ChIP-seq) with an exogenous spike-in reference from Drosophila melanogaster. Equal mass of spike-in chromatin is added to the control and treatment before immunoprecipitation. The treatment globally increases H3K27ac by twofold, which alters the ratio of human to spike-in reads but not the absolute spike-in input mass. The sequencing and mapping yield the following totals:\n- Control mapped human reads: $H_{c} = 3.6 \\times 10^{7}$; control mapped spike-in reads: $S_{c} = 4.0 \\times 10^{6}$.\n- Treatment mapped human reads: $H_{t} = 3.8 \\times 10^{7}$; treatment mapped spike-in reads: $S_{t} = 2.0 \\times 10^{6}$.\n\nFor three representative H3K27ac peaks, the raw human-aligned read counts are:\n- Peak A: $h_{A,c} = 1000$, $h_{A,t} = 2111$.\n- Peak B: $h_{B,c} = 500$, $h_{B,t} = 950$.\n- Peak C: $h_{C,c} = 300$, $h_{C,t} = 1000$.\n\nStarting from first principles of proportional sampling and counting processes, proceed as follows:\n1. Derive the treatment spike-in normalization factor $f_{t}$ such that, after normalization, the ratio of human to spike-in reads in the treatment equals that in the control. Use the definition $R_{c} = \\frac{H_{c}}{S_{c}}$ and $R_{t} = \\frac{H_{t}}{S_{t}}$, and impose $f_{t} R_{t} = R_{c}$.\n2. Use $f_{t}$ to compute normalized treatment peak counts $h'_{i,t} = f_{t} \\cdot h_{i,t}$ for each peak $i \\in \\{A,B,C\\}$, and the corresponding normalized fold change at each peak $\\mathrm{FC}_{i} = \\frac{h'_{i,t}}{h_{i,c}}$.\n3. To determine whether an observed peak-level fold change is artifactual (due solely to the global twofold increase), formalize the null expectation for treatment peak counts under a homogeneous global change as $\\lambda_{i} = h_{i,c} \\cdot \\frac{R_{t}}{R_{c}}$ and model peak counts as Poisson with mean $\\lambda_{i}$. Using a two-sided significance level $\\alpha = 0.05$, classify a peak as “artifactual” if its observed $h_{i,t}$ is statistically consistent with $\\lambda_{i}$ under the Poisson model (i.e., the two-sided $p$-value is greater than $\\alpha$).\n4. Report, as a single value, the fraction of the three peaks that are artifactual under this criterion. Express the final fraction as a simplified fraction or as a decimal. If you choose a decimal, round your answer to four significant figures.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- Control mapped human reads: $H_{c} = 3.6 \\times 10^{7}$\n- Control mapped spike-in reads: $S_{c} = 4.0 \\times 10^{6}$\n- Treatment mapped human reads: $H_{t} = 3.8 \\times 10^{7}$\n- Treatment mapped spike-in reads: $S_{t} = 2.0 \\times 10^{6}$\n- Peak A control reads: $h_{A,c} = 1000$\n- Peak A treatment reads: $h_{A,t} = 2111$\n- Peak B control reads: $h_{B,c} = 500$\n- Peak B treatment reads: $h_{B,t} = 950$\n- Peak C control reads: $h_{C,c} = 300$\n- Peak C treatment reads: $h_{C,t} = 1000$\n- The treatment globally increases H3K27ac by twofold.\n- Definition of read ratios: $R_{c} = H_{c}/S_{c}$ and $R_{t} = H_{t}/S_{t}$.\n- Definition of normalization factor: $f_{t}$ is defined by the condition $f_{t} R_{t} = R_{c}$.\n- Definition of normalized treatment peak counts: $h'_{i,t} = f_{t} \\, h_{i,t}$ for peak $i \\in \\{A,B,C\\}$.\n- Definition of normalized fold change: $\\mathrm{FC}_{i} = h'_{i,t}/h_{i,c}$.\n- Definition of null expectation for treatment peak counts: $\\lambda_{i} = h_{i,c} \\, (R_{t}/R_{c})$.\n- Statistical model: Peak counts are modeled as a Poisson process with mean $\\lambda_{i}$.\n- Significance level for two-sided test: $\\alpha = 0.05$.\n- Definition of \"artifactual\" peak: A peak for which the observed count $h_{i,t}$ is statistically consistent with the null expectation $\\lambda_i$, meaning the two-sided $p$-value is greater than $\\alpha$.\n- Final objective: Report the fraction of peaks that are artifactual.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, describing a standard spike-in normalization procedure for ChIP-seq data, a technique used to account for global changes in histone modification levels. All terms are well-defined, and all necessary data are provided. The values are self-consistent; for instance, the ratio of total read ratios, $(H_t/S_t)/(H_c/S_c) = ((3.8\\times 10^7)/(2.0\\times 10^6))/((3.6\\times 10^7)/(4.0\\times 10^6)) = 19/9 \\approx 2.11$, which is consistent with the stated \"twofold\" global increase. The problem is well-posed, objective, and does not violate any scientific or logical principles.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be derived.\n\nThe solution proceeds by executing the four steps specified in the problem statement.\n\n**1. Derive the treatment spike-in normalization factor $f_{t}$.**\n\nFirst, we calculate the ratio of human reads to spike-in reads for both the control and treatment conditions.\nFor the control:\n$$R_{c} = \\frac{H_{c}}{S_{c}} = \\frac{3.6 \\times 10^{7}}{4.0 \\times 10^{6}} = 9$$\nFor the treatment:\n$$R_{t} = \\frac{H_{t}}{S_{t}} = \\frac{3.8 \\times 10^{7}}{2.0 \\times 10^{6}} = 19$$\nThe normalization factor $f_{t}$ is defined by the condition that it equalizes the read ratios after application, i.e., $f_{t} R_{t} = R_{c}$. Solving for $f_t$:\n$$f_{t} = \\frac{R_{c}}{R_{t}} = \\frac{9}{19}$$\n\n**2. Compute normalized treatment peak counts and fold changes.**\n\nUsing the derived factor $f_{t} = 9/19$, we normalize the raw treatment peak counts $h_{i,t}$ to obtain $h'_{i,t} = f_{t} h_{i,t}$. The normalized fold change is then $\\mathrm{FC}_{i} = h'_{i,t}/h_{i,c}$.\n\nFor Peak A ($h_{A,c}=1000, h_{A,t}=2111$):\n$$h'_{A,t} = \\frac{9}{19} \\times 2111 = \\frac{18999}{19} \\approx 999.947$$\n$$\\mathrm{FC}_{A} = \\frac{h'_{A,t}}{h_{A,c}} = \\frac{18999/19}{1000} = \\frac{18999}{19000} \\approx 0.99995$$\n\nFor Peak B ($h_{B,c}=500, h_{B,t}=950$):\n$$h'_{B,t} = \\frac{9}{19} \\times 950 = 9 \\times 50 = 450$$\n$$\\mathrm{FC}_{B} = \\frac{h'_{B,t}}{h_{B,c}} = \\frac{450}{500} = 0.9$$\n\nFor Peak C ($h_{C,c}=300, h_{C,t}=1000$):\n$$h'_{C,t} = \\frac{9}{19} \\times 1000 = \\frac{9000}{19} \\approx 473.68$$\n$$\\mathrm{FC}_{C} = \\frac{h'_{C,t}}{h_{C,c}} = \\frac{9000/19}{300} = \\frac{30}{19} \\approx 1.5789$$\n\n**3. Classify peaks as artifactual based on a statistical test.**\n\nA peak is classified as \"artifactual\" if its observed treatment count $h_{i,t}$ is consistent with the null hypothesis that the change is driven purely by the global effect. The expected count under this null hypothesis is $\\lambda_{i} = h_{i,c} \\cdot \\frac{R_{t}}{R_{c}}$. The observed count $h_{i,t}$ is modeled as a draw from a Poisson distribution with mean $\\lambda_{i}$, i.e., $h_{i,t} \\sim \\mathrm{Pois}(\\lambda_{i})$. We use a two-sided test with significance level $\\alpha = 0.05$.\n\nThe ratio for the null model is $\\frac{R_{t}}{R_{c}} = \\frac{19}{9}$.\n\nFor Peak A:\n- Null expectation: $\\lambda_{A} = h_{A,c} \\frac{R_{t}}{R_{c}} = 1000 \\times \\frac{19}{9} = \\frac{19000}{9} \\approx 2111.11$.\n- Observed count: $h_{A,t} = 2111$.\n- The observed count is extremely close to the expected mean. Since $\\lambda_{A}$ is large, we can approximate the Poisson distribution with a Normal distribution $N(\\mu=\\lambda_{A}, \\sigma^2=\\lambda_{A})$. The standard deviation is $\\sigma_{A} = \\sqrt{\\lambda_{A}} \\approx \\sqrt{2111.11} \\approx 45.95$. The deviation of the observation from the mean, $2111 - 2111.11 = -0.11$, is a very small fraction of one standard deviation. The corresponding $p$-value will be large, clearly greater than $\\alpha=0.05$.\n- Conclusion: Peak A is artifactual.\n\nFor Peak B:\n- Null expectation: $\\lambda_{B} = h_{B,c} \\frac{R_{t}}{R_{c}} = 500 \\times \\frac{19}{9} = \\frac{9500}{9} \\approx 1055.56$.\n- Observed count: $h_{B,t} = 950$.\n- Using the Normal approximation (justified as $\\lambda_{B}$ is large), with $\\mu_{B} = \\lambda_{B}$ and $\\sigma_{B} = \\sqrt{\\lambda_{B}} \\approx \\sqrt{1055.56} \\approx 32.49$.\n- The test statistic (z-score) with continuity correction is:\n  $Z = \\frac{(h_{B,t} + 0.5) - \\lambda_{B}}{\\sigma_{B}} = \\frac{950.5 - 1055.56}{32.49} \\approx \\frac{-105.06}{32.49} \\approx -3.23$.\n- The two-sided $p$-value is $2 \\times P(Z \\le -3.23)$. This value is approximately $2 \\times 0.00062 = 0.00124$.\n- Since $0.00124 < 0.05$, we reject the null hypothesis.\n- Conclusion: Peak B is not artifactual.\n\nFor Peak C:\n- Null expectation: $\\lambda_{C} = h_{C,c} \\frac{R_{t}}{R_{c}} = 300 \\times \\frac{19}{9} = \\frac{1900}{3} \\approx 666.67$.\n- Observed count: $h_{C,t} = 1000$.\n- Using the Normal approximation, with $\\mu_{C} = \\lambda_{C}$ and $\\sigma_{C} = \\sqrt{\\lambda_{C}} \\approx \\sqrt{666.67} \\approx 25.82$.\n- The test statistic (z-score) with continuity correction is:\n  $Z = \\frac{(h_{C,t} - 0.5) - \\lambda_{C}}{\\sigma_{C}} = \\frac{999.5 - 666.67}{25.82} \\approx \\frac{332.83}{25.82} \\approx 12.89$.\n- The z-score is extremely large. The corresponding two-sided $p$-value is vanishingly small.\n- Since $p \\ll 0.05$, we reject the null hypothesis.\n- Conclusion: Peak C is not artifactual.\n\n**4. Report the fraction of artifactual peaks.**\n\nOut of the three peaks analyzed ($\\{A, B, C\\}$), only Peak A was classified as artifactual.\nThe number of artifactual peaks is $1$.\nThe total number of peaks is $3$.\nThe fraction of artifactual peaks is $\\frac{1}{3}$.", "answer": "$$\\boxed{\\frac{1}{3}}$$", "id": "4335400"}, {"introduction": "Large-scale chromosomal rearrangements, such as copy number variations (CNVs), are hallmarks of many diseases, particularly cancer. This advanced practice guides you through building a diagnostic tool to detect these aberrations from whole-genome sequencing data [@problem_id:4335375]. By implementing a Hidden Markov Model (HMM) that accounts for experimental biases and sample purity, you will learn how to translate raw sequencing read depths into a map of absolute DNA copy numbers, a key step in clinical cancer genomics.", "problem": "You are given binned Whole-Genome Sequencing (WGS) read counts and per-bin guanine-cytosine (GC) bias factors for multiple test cases. Assume read depth is proportional to DNA abundance, and that read sampling follows a Poisson process. Model the genome as a sequence of hidden copy-number states with a Hidden Markov Model (HMM), where emissions are raw read counts per bin and transitions reflect segmental continuity. Incorporate tumor purity and tumor ploidy to convert observed mixture coverage to expected means for each integer tumor copy-number state.\n\nStarting from the following foundational bases:\n- Sequencing depth is proportional to DNA abundance. For a bin with total DNA mass proportional to copy number, expected read counts scale with this abundance.\n- A Poisson process describes read sampling, with probability mass function $P(Y=y \\mid \\mu)=\\frac{\\mu^{y}e^{-\\mu}}{y!}$, where $y$ is the observed count and $\\mu$ is the expected mean.\n- Hidden Markov Model (HMM) segmentation with state set $\\mathcal{S}$, transition matrix $\\mathbf{A}$, and initial distribution $\\boldsymbol{\\pi}$, and Viterbi decoding to find the most likely state path.\n\nDefine the expected read-count mean for a bin under tumor total copy-number state $c \\in \\mathcal{S}$ as follows. Let tumor purity be $p \\in [0,1]$, tumor ploidy (genome-wide average tumor copy number per haploid genome) be $P > 0$, and let the normal copy number be $2$. The expected mixture copy-number ratio for state $c$ relative to the sample’s genome-wide mixture baseline is\n$$\nR(c;p,P)=\\frac{p\\cdot c + (1-p)\\cdot 2}{p\\cdot P + (1-p)\\cdot 2}.\n$$\nLet the GC bias factor for bin $i$ be $g_i>0$ and the unknown baseline expected count scale be $D_0>0$ for the case when $R(c)=1$ and $g_i=1$. To calibrate $D_0$ from data, correct counts by GC bias, $\\tilde{Y}_i=Y_i/g_i$, and use the diploid normal assumption to estimate\n$$\nD_0 \\approx \\frac{\\mathrm{median}\\left(\\{\\tilde{Y}_i\\}\\right)}{R(2;p,P)}.\n$$\nThen the emission mean for bin $i$ under state $c$ is\n$$\n\\mu_{i}(c)=D_0 \\cdot R(c;p,P)\\cdot g_i,\n$$\nand emissions are modeled as $Y_i \\sim \\mathrm{Poisson}\\!\\left(\\mu_i(c)\\right)$.\n\nConstruct an HMM with:\n- State space $\\mathcal{S}$ as the set of candidate integer tumor total copy numbers specified per test case.\n- Transition matrix with high self-transition probability $a_{ss}=\\alpha$ and uniform change probability $a_{s\\neq t}=\\frac{1-\\alpha}{|\\mathcal{S}|-1}$.\n- Initial distribution $\\pi_s=\\frac{1}{|\\mathcal{S}|}$.\n\nDecode the most likely copy-number state per bin using the Viterbi algorithm, then collapse consecutive bins with identical decoded states into segments. For each segment, report the integer tumor absolute copy number $c$.\n\nYour program must implement this procedure and produce results for the following test suite. All arrays below are ordered lists, and every entry is a scalar. No physical units are required; all quantities are dimensionless counts or factors. Angles do not appear. The final outputs must be integers and lists of integers.\n\nTest Suite:\n- Case A (general segmentation with moderate purity and triploid tumor ploidy):\n    - Tumor purity $p=0.6$, tumor ploidy $P=3.0$, state set $\\mathcal{S}=\\{0,1,2,3,4,5,6\\}$, self-transition parameter $\\alpha=0.995$.\n    - Counts $Y$: [$73$,$81$,$73$,$81$,$73$,$81$,$73$,$81$,$73$,$81$,$73$,$81$,$73$,$81$,$73$,$81$,$73$,$81$,$73$,$81$,$117$,$129$,$117$,$129$,$117$,$51$,$57$,$51$,$57$,$51$].\n    - GC factors $g$: [$0.95$,$1.05$,$0.95$,$1.05$,$0.95$,$1.05$,$0.95$,$1.05$,$0.95$,$1.05$,$0.95$,$1.05$,$0.95$,$1.05$,$0.95$,$1.05$,$0.95$,$1.05$,$0.95$,$1.05$,$0.95$,$1.05$,$0.95$,$1.05$,$0.95$,$0.95$,$1.05$,$0.95$,$1.05$,$0.95$].\n- Case B (boundary condition with very low purity, near-diploid mixture):\n    - Tumor purity $p=0.05$, tumor ploidy $P=2.0$, state set $\\mathcal{S}=\\{0,1,2,3,4\\}$, self-transition parameter $\\alpha=0.999$.\n    - Counts $Y$: [$120$,$120$,$120$,$120$,$120$,$120$,$120$,$120$,$120$,$120$,$120$,$120$,$120$,$120$,$120$,$120$,$120$,$120$,$120$,$120$].\n    - GC factors $g$: [$1.0$,$1.0$,$1.0$,$1.0$,$1.0$,$1.0$,$1.0$,$1.0$,$1.0$,$1.0$,$1.0$,$1.0$,$1.0$,$1.0$,$1.0$,$1.0$,$1.0$,$1.0$,$1.0$,$1.0$].\n- Case C (high tumor purity and tetraploid tumor with focal events):\n    - Tumor purity $p=0.9$, tumor ploidy $P=4.0$, state set $\\mathcal{S}=\\{0,1,2,3,4,5,6,7,8\\}$, self-transition parameter $\\alpha=0.997$.\n    - Counts $Y$: [$135$,$165$,$135$,$165$,$135$,$165$,$135$,$165$,$135$,$165$,$135$,$165$,$135$,$165$,$135$,$165$,$135$,$165$,$135$,$165$,$103$,$126$,$103$,$126$,$103$,$126$,$103$,$126$,$103$,$126$,$71$,$87$,$71$,$87$,$71$,$87$,$7$,$9$].\n    - GC factors $g$: [$0.9$,$1.1$,$0.9$,$1.1$,$0.9$,$1.1$,$0.9$,$1.1$,$0.9$,$1.1$,$0.9$,$1.1$,$0.9$,$1.1$,$0.9$,$1.1$,$0.9$,$1.1$,$0.9$,$1.1$,$0.9$,$1.1$,$0.9$,$1.1$,$0.9$,$1.1$,$0.9$,$1.1$,$0.9$,$1.1$,$0.9$,$1.1$,$0.9$,$1.1$,$0.9$,$1.1$,$0.9$,$1.1$].\n- Case D (edge case with null-copy segment and mixed events):\n    - Tumor purity $p=0.8$, tumor ploidy $P=2.5$, state set $\\mathcal{S}=\\{0,1,2,3,4,5,6\\}$, self-transition parameter $\\alpha=0.995$.\n    - Counts $Y$: [$63$,$70$,$63$,$70$,$63$,$70$,$63$,$70$,$63$,$70$,$0$,$14$,$13$,$0$,$14$,$13$,$0$,$14$,$13$,$0$,$139$,$154$,$139$,$154$,$139$,$154$,$139$,$154$,$139$,$154$].\n    - GC factors $g$: [$0.95$,$1.05$,$0.95$,$1.05$,$0.95$,$1.05$,$0.95$,$1.05$,$0.95$,$1.05$,$1.0$,$1.05$,$0.95$,$1.0$,$1.05$,$0.95$,$1.0$,$1.05$,$0.95$,$1.0$,$0.95$,$1.05$,$0.95$,$1.05$,$0.95$,$1.05$,$0.95$,$1.05$,$0.95$,$1.05$].\n\nAlgorithmic requirements:\n- Implement Viterbi decoding with log-probabilities to avoid underflow. Use the transition model defined above and uniform initial log-probabilities.\n- Use the calibration for $D_0$ given above.\n- Use the Poisson log-likelihood for emissions with means $\\mu_i(c)$.\n- Collapse consecutive identical decoded states into segments and output the tumor absolute copy-number per segment as integers.\n\nAnswer format:\n- For each case, output the list of inferred segment-level tumor absolute copy numbers in order. Aggregate the four case results into a single line printed as a comma-separated list enclosed in square brackets, with no spaces, where each case’s result is itself a bracketed comma-separated list. For example: $[\\,[2,2,4,1],[2],\\ldots\\,]$ but with no spaces.", "solution": "The user has provided a well-defined computational problem in the domain of cancer genomics. The task is to implement a Hidden Markov Model (HMM) to infer DNA copy-number segments from binned Whole-Genome Sequencing (WGS) read-count data. The problem statement has been validated and found to be scientifically sound, complete, and well-posed. All parameters, models, and data are provided, enabling a direct and unambiguous implementation.\n\nThe solution proceeds by first formalizing the HMM components based on the provided specifications, and then applying the Viterbi algorithm to decode the most likely sequence of hidden copy-number states. The core of the model lies in its ability to translate observed, noisy read counts into probabilities of underlying copy-number states, a process governed by a statistical model that accounts for tumor purity, tumor ploidy, and technical biases like GC content.\n\n**1. HMM Framework and Parameterization**\n\nAn HMM is defined by its state space $\\mathcal{S}$, transition probabilities $\\mathbf{A}$, emission probabilities, and initial state distribution $\\boldsymbol{\\pi}$.\n\n-   **State Space ($\\mathcal{S}$)**: The hidden states are the integer tumor total copy numbers, provided for each test case. Let $|\\mathcal{S}|$ be the number of states.\n\n-   **Initial Distribution ($\\boldsymbol{\\pi}$)**: A uniform initial distribution is specified, meaning any state is equally likely at the first position. For each state $s \\in \\mathcal{S}$, the initial probability is $\\pi_s = 1/|\\mathcal{S}|$. In log-space, this is $\\log(\\pi_s) = -\\log(|\\mathcal{S}|)$.\n\n-   **Transition Matrix ($\\mathbf{A}$)**: The matrix models the assumption of segmental continuity. Transitions between different states are rare, while self-transitions are common. The probabilities are given by:\n    -   Self-transition: $a_{ss} = \\alpha$\n    -   Change of state: $a_{s \\neq t} = \\frac{1-\\alpha}{|\\mathcal{S}|-1}$\n    These will be converted to log-probabilities: $\\log(a_{ss}) = \\log(\\alpha)$ and $\\log(a_{s \\neq t}) = \\log(1-\\alpha) - \\log(|\\mathcal{S}|-1)$.\n\n-   **Emission Probabilities**: The probability of observing a read count $Y_i$ in bin $i$ given a hidden copy-number state $c$ is modeled by a Poisson distribution, $Y_i \\sim \\mathrm{Poisson}(\\mu_i(c))$. The log-probability mass function is $\\log(P(Y_i=y | \\mu_i(c))) = y \\log(\\mu_i(c)) - \\mu_i(c) - \\log(y!)$. The term $\\log(y!)$ can be computed using `scipy.special.gammaln(y+1)`. `scipy.stats.poisson.logpmf` provides this directly.\n\n**2. Calculation of Poisson Means ($\\mu_i(c)$)**\n\nThe expected mean $\\mu_i(c)$ of the Poisson distribution for bin $i$ and state $c$ integrates several biological and technical factors:\n\n1.  **Mixture Ratio ($R(c;p,P)$)**: A tumor sample is a mixture of tumor cells (purity $p$, ploidy $P$) and normal diploid cells (purity $1-p$, ploidy $2$). The expected relative DNA abundance for a tumor region with copy number $c$ is:\n    $$\n    R(c;p,P) = \\frac{p \\cdot c + (1-p) \\cdot 2}{p \\cdot P + (1-p) \\cdot 2}\n    $$\n    This ratio normalizes the copy number of a specific state to the sample's average DNA content per cell.\n\n2.  **Baseline Depth Calibration ($D_0$)**: The overall sequencing depth is an unknown scaling factor. It is calibrated using the provided formula, which anchors the estimation to the theoretical diploid state ($c=2$). First, read counts are corrected for GC bias ($g_i$): $\\tilde{Y}_i=Y_i/g_i$. Then, $D_0$ is estimated as:\n    $$\n    D_0 \\approx \\frac{\\mathrm{median}(\\{\\tilde{Y}_i\\})}{R(2;p,P)}\n    $$\n    The median provides a robust estimate of the typical read-count level, which the formula assumes corresponds to the diploid state's expected ratio.\n\n3.  **Final Emission Mean**: The mean for bin $i$ under state $c$ is the product of the baseline depth, the state-specific mixture ratio, and the bin-specific GC bias factor:\n    $$\n    \\mu_{i}(c) = D_0 \\cdot R(c;p,P) \\cdot g_i\n    $$\n\n**3. Viterbi Decoding Algorithm**\n\nTo find the most probable sequence of hidden states (copy numbers), we use the Viterbi algorithm, implemented in log-space to prevent numerical underflow.\n\n-   **Initialization (Bin $i=0$)**: The Viterbi matrix $V$ stores the maximum log-probability of any path ending at a given state and position. For the first bin, for each state $s_j \\in \\mathcal{S}$ (with corresponding copy number $c_j$):\n    $$\n    V_{j,0} = \\log(\\pi_j) + \\log(P(Y_0 | \\mu_0(c_j)))\n    $$\n    A backpointer matrix $B$ is also initialized.\n\n-   **Recursion (Bins $i = 1, \\dots, N-1$)**: For each subsequent bin $i$ and each state $s_j$:\n    $$\n    V_{j,i} = \\log(P(Y_i | \\mu_i(c_j))) + \\max_{k \\in \\{1,\\dots,|\\mathcal{S}|\\}} \\left( V_{k, i-1} + \\log(a_{k,j}) \\right)\n    $$\n    The backpointer for this entry stores the index $k$ that achieved the maximum:\n    $$\n    B_{j,i} = \\arg\\max_{k \\in \\{1,\\dots,|\\mathcal{S}|\\}} \\left( V_{k, i-1} + \\log(a_{k,j}) \\right)\n    $$\n\n-   **Termination and Backtracking**: The most likely final state is found by maximizing over the last column of $V$. Then, the backpointer matrix $B$ is traced backward from this final state to reconstruct the entire most likely path of states.\n\n**4. Segmentation**\n\nThe decoded path is a sequence of copy-number states for each bin. The final step is to collapse consecutive bins with the same decoded copy number into a single segment, reporting the shared copy number for that segment. The final output is an ordered list of these segment-level copy numbers. The implementation will process each test case through this pipeline.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import poisson\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        {\n            \"id\": \"A\",\n            \"p\": 0.6, \"P\": 3.0, \"S\": [0, 1, 2, 3, 4, 5, 6], \"alpha\": 0.995,\n            \"Y\": [73, 81, 73, 81, 73, 81, 73, 81, 73, 81, 73, 81, 73, 81, 73, 81, 73, 81, 73, 81, 117, 129, 117, 129, 117, 51, 57, 51, 57, 51],\n            \"g\": [0.95, 1.05, 0.95, 1.05, 0.95, 1.05, 0.95, 1.05, 0.95, 1.05, 0.95, 1.05, 0.95, 1.05, 0.95, 1.05, 0.95, 1.05, 0.95, 1.05, 0.95, 1.05, 0.95, 1.05, 0.95, 0.95, 1.05, 0.95, 1.05, 0.95]\n        },\n        {\n            \"id\": \"B\",\n            \"p\": 0.05, \"P\": 2.0, \"S\": [0, 1, 2, 3, 4], \"alpha\": 0.999,\n            \"Y\": [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120],\n            \"g\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n        },\n        {\n            \"id\": \"C\",\n            \"p\": 0.9, \"P\": 4.0, \"S\": [0, 1, 2, 3, 4, 5, 6, 7, 8], \"alpha\": 0.997,\n            \"Y\": [135, 165, 135, 165, 135, 165, 135, 165, 135, 165, 135, 165, 135, 165, 135, 165, 135, 165, 135, 165, 103, 126, 103, 126, 103, 126, 103, 126, 103, 126, 71, 87, 71, 87, 71, 87, 7, 9],\n            \"g\": [0.9, 1.1, 0.9, 1.1, 0.9, 1.1, 0.9, 1.1, 0.9, 1.1, 0.9, 1.1, 0.9, 1.1, 0.9, 1.1, 0.9, 1.1, 0.9, 1.1, 0.9, 1.1, 0.9, 1.1, 0.9, 1.1, 0.9, 1.1, 0.9, 1.1, 0.9, 1.1, 0.9, 1.1, 0.9, 1.1, 0.9, 1.1]\n        },\n        {\n            \"id\": \"D\",\n            \"p\": 0.8, \"P\": 2.5, \"S\": [0, 1, 2, 3, 4, 5, 6], \"alpha\": 0.995,\n            \"Y\": [63, 70, 63, 70, 63, 70, 63, 70, 63, 70, 0, 14, 13, 0, 14, 13, 0, 14, 13, 0, 139, 154, 139, 154, 139, 154, 139, 154, 139, 154],\n            \"g\": [0.95, 1.05, 0.95, 1.05, 0.95, 1.05, 0.95, 1.05, 0.95, 1.05, 1.0, 1.05, 0.95, 1.0, 1.05, 0.95, 1.0, 1.05, 0.95, 1.0, 0.95, 1.05, 0.95, 1.05, 0.95, 1.05, 0.95, 1.05, 0.95, 1.05]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        segments = perform_viterbi_segmentation(\n            p=case[\"p\"], P=case[\"P\"], S=case[\"S\"],\n            alpha=case[\"alpha\"], Y=case[\"Y\"], g=case[\"g\"]\n        )\n        results.append(f\"[{','.join(map(str, segments))}]\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\ndef perform_viterbi_segmentation(p, P, S, alpha, Y, g):\n    \"\"\"\n    Solves a single test case for CNV segmentation using an HMM.\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    S = np.array(S, dtype=int)\n    Y = np.array(Y, dtype=int)\n    g = np.array(g, dtype=float)\n    \n    num_states = len(S)\n    num_bins = len(Y)\n    \n    # 1. Model Parameter Calculation\n    mixture_baseline = p * P + (1 - p) * 2.0\n    r_numerators = p * S + (1 - p) * 2.0\n    R_c = r_numerators / mixture_baseline\n    \n    Y_corr = Y / g\n    \n    idx_c2 = np.where(S == 2)[0][0]\n    R_c2 = R_c[idx_c2]\n    \n    D_0 = np.median(Y_corr) / R_c2\n    \n    mu_ic = D_0 * R_c[:, np.newaxis] * g[np.newaxis, :]\n    \n    log_alpha = np.log(alpha)\n    if num_states > 1:\n        log_beta = np.log((1 - alpha) / (num_states - 1))\n    else:\n        log_beta = -np.inf # Not encountered in test cases\n\n    log_A = np.full((num_states, num_states), log_beta)\n    np.fill_diagonal(log_A, log_alpha)\n    \n    log_pi = np.full(num_states, -np.log(num_states))\n    \n    # 2. Viterbi Algorithm\n    viterbi_matrix = np.zeros((num_states, num_bins))\n    backpointer_matrix = np.zeros((num_states, num_bins), dtype=int)\n    \n    log_emission_probs_0 = poisson.logpmf(Y[0], mu_ic[:, 0])\n    viterbi_matrix[:, 0] = log_pi + log_emission_probs_0\n    \n    for i in range(1, num_bins):\n        log_emission_probs_i = poisson.logpmf(Y[i], mu_ic[:, i])\n        \n        for j in range(num_states):\n            state_scores = viterbi_matrix[:, i-1] + log_A[:, j]\n            best_prev_state = np.argmax(state_scores)\n            max_log_prob = state_scores[best_prev_state]\n            \n            viterbi_matrix[j, i] = max_log_prob + log_emission_probs_i[j]\n            backpointer_matrix[j, i] = best_prev_state\n\n    # 3. Backtracking\n    path = np.zeros(num_bins, dtype=int)\n    if num_bins > 0:\n        path[num_bins - 1] = np.argmax(viterbi_matrix[:, num_bins - 1])\n        for i in range(num_bins - 2, -1, -1):\n            path[i] = backpointer_matrix[path[i+1], i+1]\n    \n    cn_path = S[path]\n    \n    # 4. Collapse Segments\n    if num_bins == 0:\n        return []\n        \n    segments = [cn_path[0]]\n    for i in range(1, num_bins):\n        if cn_path[i] != segments[-1]:\n            segments.append(cn_path[i])\n            \n    return segments\n\nsolve()\n```", "id": "4335375"}]}