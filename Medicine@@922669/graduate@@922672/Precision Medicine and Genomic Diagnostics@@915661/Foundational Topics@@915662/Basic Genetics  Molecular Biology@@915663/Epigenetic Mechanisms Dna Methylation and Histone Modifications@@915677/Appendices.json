{"hands_on_practices": [{"introduction": "Beyond the canonical DNA bases, cytosine can exist in modified forms like 5-methylcytosine ($5\\text{mC}$) and 5-hydroxymethylcytosine ($5\\text{hmC}$), each with distinct regulatory roles. Standard bisulfite sequencing, a cornerstone of methylation analysis, cannot distinguish between these two marks. This practice [@problem_id:4337409] demonstrates how a clever combination of two chemical treatments, Bisulfite Sequencing (BS) and Oxidative Bisulfite Sequencing (oxBS), allows us to deconvolve these signals and accurately quantify the fraction of each modification at a specific site using fundamental principles of statistical inference.", "problem": "A clinical genomics laboratory applies Bisulfite Sequencing (BS) and Oxidative Bisulfite Sequencing (oxBS) to quantify cytosine modifications at a single cytosine–phosphate–guanine (CpG) site in the promoter of a pharmacogenomically relevant gene. In BS, 5-methylcytosine and 5-hydroxymethylcytosine are protected and read as cytosine, whereas unmodified cytosine is converted and read as thymine. In oxBS, 5-hydroxymethylcytosine is selectively oxidized and subsequently converted, so that protected cytosine calls report 5-methylcytosine only. Let the true fractions at the locus be $\\theta_{m}$ for 5-methylcytosine, $\\theta_{h}$ for 5-hydroxymethylcytosine, and $\\theta_{u}$ for unmodified cytosine, with $\\theta_{m} + \\theta_{h} + \\theta_{u} = 1$. Assume ideal conversion and oxidation chemistry and independent reads.\n\nAt this CpG, the laboratory observes the following read counts:\n- BS: $n_{\\mathrm{BS}} = 120$ total reads, of which $x_{\\mathrm{BS}} = 90$ are called cytosine and $n_{\\mathrm{BS}} - x_{\\mathrm{BS}} = 30$ are called thymine.\n- oxBS: $n_{\\mathrm{ox}} = 100$ total reads, of which $x_{\\mathrm{ox}} = 60$ are called cytosine and $n_{\\mathrm{ox}} - x_{\\mathrm{ox}} = 40$ are called thymine.\n\nModel each experiment’s cytosine-call count as a binomial random variable with success probability equal to the corresponding protected fraction under the chemistry described above. Starting from these definitions and the binomial model, derive the maximum likelihood estimate of the 5-hydroxymethylcytosine fraction $\\theta_{h}$ at this CpG and a two-sided $0.95$-level confidence interval for $\\theta_{h}$ based on a large-sample normal approximation to the difference of independent binomial proportions. Express your final numerical results as decimals, rounding the point estimate and both confidence bounds to four significant figures. In your final answer, report the three numbers in the order $\\hat{\\theta}_{h}$, lower bound, upper bound.", "solution": "The problem requires the derivation of the maximum likelihood estimate (MLE) and a $0.95$-level confidence interval for the fraction of 5-hydroxymethylcytosine, $\\theta_h$, at a specific CpG site, based on data from Bisulfite Sequencing (BS) and Oxidative Bisulfite Sequencing (oxBS) experiments.\n\nFirst, we formalize the probabilistic model for each experiment as stipulated. The number of cytosine calls in each experiment is modeled as a binomial random variable. Let $p_{\\mathrm{BS}}$ be the probability of observing a cytosine in the BS experiment and $p_{\\mathrm{ox}}$ be the probability of observing a cytosine in the oxBS experiment.\n\nAccording to the problem description:\nIn BS, both 5-methylcytosine (fraction $\\theta_m$) and 5-hydroxymethylcytosine (fraction $\\theta_h$) are protected from conversion and are read as cytosine. Therefore, the probability of a cytosine call is the sum of these fractions:\n$$p_{\\mathrm{BS}} = \\theta_{m} + \\theta_{h}$$\nIn oxBS, 5-hydroxymethylcytosine is oxidized and converted, while only 5-methylcytosine is protected. Thus, the probability of a cytosine call is:\n$$p_{\\mathrm{ox}} = \\theta_{m}$$\nThe fraction of unmodified cytosine, $\\theta_u$, is related by the constraint $\\theta_{m} + \\theta_{h} + \\theta_{u} = 1$. The probability of a thymine read in BS is $1 - p_{\\mathrm{BS}} = \\theta_u$, and in oxBS is $1 - p_{\\mathrm{ox}} = \\theta_h + \\theta_u$.\n\nLet $X_{\\mathrm{BS}}$ and $X_{\\mathrm{ox}}$ be the random variables for the number of cytosine calls in the BS and oxBS experiments, respectively. The models are:\n$$X_{\\mathrm{BS}} \\sim \\mathrm{Binomial}(n_{\\mathrm{BS}}, p_{\\mathrm{BS}})$$\n$$X_{\\mathrm{ox}} \\sim \\mathrm{Binomial}(n_{\\mathrm{ox}}, p_{\\mathrm{ox}})$$\nwhere the observed data are $n_{\\mathrm{BS}} = 120$, $x_{\\mathrm{BS}} = 90$, $n_{\\mathrm{ox}} = 100$, and $x_{\\mathrm{ox}} = 60$.\n\nThe maximum likelihood estimate for the success probability $p$ of a binomial distribution $\\mathrm{Binomial}(n, p)$ based on an observation $x$ is given by $\\hat{p} = \\frac{x}{n}$. Applying this to our two independent experiments, we obtain the MLEs for $p_{\\mathrm{BS}}$ and $p_{\\mathrm{ox}}$:\n$$\\hat{p}_{\\mathrm{BS}} = \\frac{x_{\\mathrm{BS}}}{n_{\\mathrm{BS}}} = \\frac{90}{120} = 0.75$$\n$$\\hat{p}_{\\mathrm{ox}} = \\frac{x_{\\mathrm{ox}}}{n_{\\mathrm{ox}}} = \\frac{60}{100} = 0.60$$\n\nOur primary parameter of interest is $\\theta_h$. We can express $\\theta_h$ as a function of $p_{\\mathrm{BS}}$ and $p_{\\mathrm{ox}}$:\n$$\\theta_{h} = (\\theta_{m} + \\theta_{h}) - \\theta_{m} = p_{\\mathrm{BS}} - p_{\\mathrm{ox}}$$\nBy the invariance property of maximum likelihood estimators, the MLE of a function of parameters is the same function evaluated at the MLEs of those parameters. Therefore, the MLE of $\\theta_h$ is:\n$$\\hat{\\theta}_{h} = \\hat{p}_{\\mathrm{BS}} - \\hat{p}_{\\mathrm{ox}}$$\nSubstituting the numerical values:\n$$\\hat{\\theta}_{h} = 0.75 - 0.60 = 0.15$$\n\nNext, we construct a two-sided $0.95$-level confidence interval for $\\theta_h$. The problem specifies using a large-sample normal approximation for the difference of two independent binomial proportions. The general form of such a confidence interval is:\n$$\\text{point estimate} \\pm (\\text{critical value}) \\times (\\text{standard error})$$\nThe point estimate is $\\hat{\\theta}_{h} = \\hat{p}_{\\mathrm{BS}} - \\hat{p}_{\\mathrm{ox}}$. The standard error of this estimate, $\\mathrm{SE}(\\hat{\\theta}_h)$, is derived from the variance of the difference of two independent sample proportions.\n$$\\mathrm{Var}(\\hat{\\theta}_h) = \\mathrm{Var}(\\hat{p}_{\\mathrm{BS}} - \\hat{p}_{\\mathrm{ox}}) = \\mathrm{Var}(\\hat{p}_{\\mathrm{BS}}) + \\mathrm{Var}(\\hat{p}_{\\mathrm{ox}})$$\nThe variance of a sample proportion $\\hat{p}$ is $\\frac{p(1-p)}{n}$. We estimate this variance by substituting the sample proportions $\\hat{p}$ for the true proportions $p$. The estimated standard error is:\n$$\\widehat{\\mathrm{SE}}(\\hat{\\theta}_h) = \\sqrt{\\frac{\\hat{p}_{\\mathrm{BS}}(1-\\hat{p}_{\\mathrm{BS}})}{n_{\\mathrm{BS}}} + \\frac{\\hat{p}_{\\mathrm{ox}}(1-\\hat{p}_{\\mathrm{ox}})}{n_{\\mathrm{ox}}}}$$\nPlugging in the numerical values:\n$$\\widehat{\\mathrm{SE}}(\\hat{\\theta}_h) = \\sqrt{\\frac{0.75(1-0.75)}{120} + \\frac{0.60(1-0.60)}{100}}$$\n$$\\widehat{\\mathrm{SE}}(\\hat{\\theta}_h) = \\sqrt{\\frac{0.75 \\times 0.25}{120} + \\frac{0.60 \\times 0.40}{100}} = \\sqrt{\\frac{0.1875}{120} + \\frac{0.24}{100}}$$\n$$\\widehat{\\mathrm{SE}}(\\hat{\\theta}_h) = \\sqrt{0.0015625 + 0.0024} = \\sqrt{0.0039625} \\approx 0.062948$$\n\nFor a $0.95$-level confidence interval, the confidence level is $1-\\alpha = 0.95$, so $\\alpha = 0.05$. The critical value is the z-score corresponding to the upper tail probability of $\\alpha/2 = 0.025$, which is $z_{1-\\alpha/2} = z_{0.975}$. From the standard normal distribution, this value is $z_{0.975} \\approx 1.96$.\n\nThe margin of error (ME) is:\n$$\\mathrm{ME} = z_{0.975} \\times \\widehat{\\mathrm{SE}}(\\hat{\\theta}_h) \\approx 1.96 \\times 0.062948 \\approx 0.123378$$\nThe confidence interval for $\\theta_h$ is $\\hat{\\theta}_{h} \\pm \\mathrm{ME}$:\n$$0.15 \\pm 0.123378$$\nThe lower bound is:\n$$L = 0.15 - 0.123378 = 0.026622$$\nThe upper bound is:\n$$U = 0.15 + 0.123378 = 0.273378$$\n\nFinally, we round the point estimate and the confidence bounds to four significant figures as required.\n- Point estimate: $\\hat{\\theta}_{h} = 0.15$. To four significant figures, this is $0.1500$.\n- Lower bound: $0.026622...$ rounds to $0.02662$.\n- Upper bound: $0.273378...$ rounds to $0.2734$.\n\nThe maximum likelihood estimate for the fraction of 5-hydroxymethylcytosine is $\\hat{\\theta}_{h} \\approx 0.1500$, and the $0.95$-level confidence interval is approximately $(0.02662, 0.2734)$.\nThe estimate and interval for the other fractions can be similarly derived: $\\hat{\\theta}_m = \\hat{p}_{\\mathrm{ox}} = 0.6000$ and $\\hat{\\theta}_u = 1 - \\hat{p}_{\\mathrm{BS}} = 1 - 0.75 = 0.2500$. All estimates are non-negative, consistent with their definition as fractions.", "answer": "$$\\boxed{\\begin{pmatrix} 0.1500 & 0.02662 & 0.2734 \\end{pmatrix}}$$", "id": "4337409"}, {"introduction": "After quantifying methylation levels, a central task in epigenomics is to identify loci that are differentially methylated between biological conditions, such as in tumors versus normal tissues. This endeavor faces two major statistical hurdles: the count data from sequencing often exhibits more variability than expected under simple models (overdispersion), and performing tests on millions of sites simultaneously requires stringent control for false positives. This exercise [@problem_id:4337386] guides you through the development of a robust statistical test using the beta-binomial model to properly handle overdispersion, and the application of the Benjamini-Hochberg procedure to control the false discovery rate in a genome-scale analysis.", "problem": "You are designing a site-wise test for differential DNA methylation in whole-genome bisulfite sequencing data to be used in a precision oncology cohort. For each cytosine–phosphate–guanine (CpG) site $s \\in \\{1,\\dots,m\\}$, for each sample $j$ assigned to group $g \\in \\{0,1\\}$ (controls $g=0$ and cases $g=1$), you observe the number of methylated reads $y_{sj}$ out of $n_{sj}$ total reads. Biological and technical heterogeneity induce extra-binomial variability across replicates that must be modeled as overdispersion. You must derive a valid likelihood-based hypothesis test at each CpG site that accounts for overdispersion and then describe how to control the false discovery rate (FDR) across the $m$ sites using the Benjamini–Hochberg (BH) procedure. Your derivation should begin from foundational definitions of binomial sampling for bisulfite reads and a well-established overdispersion model; it should specify the null and alternative hypotheses, the form of the test statistic, how its null distribution is obtained, and a principled procedure for multiple testing control.\n\nWhich option gives a correct, self-consistent derivation of such a beta–binomial test and a correct outline of BH FDR control?\n\nA. Model $y_{sj} \\mid p_{sg} \\sim \\mathrm{Binomial}(n_{sj}, p_{sg})$ with a site–group–specific methylation proportion $p_{sg}$. To capture overdispersion across biological replicates, let $p_{sg} \\sim \\mathrm{Beta}(\\alpha_{g}, \\beta_{g})$. The marginal distribution of $y_{sj}$ is then beta–binomial with probability mass function\n$$\n\\Pr(Y=y \\mid n,\\alpha,\\beta) \\;=\\; \\binom{n}{y} \\frac{B(y+\\alpha,\\,n-y+\\beta)}{B(\\alpha,\\beta)},\n$$\nwhere $B(\\cdot,\\cdot)$ is the beta function. At site $s$, define the null $H_{0}$ as shared parameters across groups, $H_{0}\\!:\\,(\\alpha_{0},\\beta_{0})=(\\alpha_{1},\\beta_{1})=(\\alpha,\\beta)$, versus the alternative $H_{1}\\!:\\,(\\alpha_{0},\\beta_{0}) \\neq (\\alpha_{1},\\beta_{1})$. The site-wise log-likelihood under a parameter vector $\\theta$ is the sum of beta–binomial log-probabilities over samples. Obtain the maximum likelihood estimates $\\widehat{\\theta}_{0}$ and $\\widehat{\\theta}_{1}$ by numerical maximization under $H_{0}$ and $H_{1}$, respectively. Use the likelihood ratio statistic\n$$\nT_{s} \\;=\\; 2\\Big(\\ell_{1,s}(\\widehat{\\theta}_{1}) - \\ell_{0,s}(\\widehat{\\theta}_{0})\\Big),\n$$\nwhich is asymptotically $\\chi^{2}$ with $2$ degrees of freedom under $H_{0}$ because $H_{1}$ adds $2$ free parameters. Compute the site-wise $p$-values $p_{s}=\\Pr(\\chi^{2}_{2}\\ge T_{s})$. To control FDR at level $q$, collect all $m$ site-wise $p$-values, sort them $p_{(1)}\\le \\cdots \\le p_{(m)}$, find the largest $k$ such that $p_{(k)} \\le (k/m)\\,q$, and declare the sites with the $k$ smallest $p$-values significant. Effect sizes can be summarized by the group means $\\mu_{g}=\\alpha_{g}/(\\alpha_{g}+\\beta_{g})$.\n\nB. Aggregate counts across samples within each group and test a single binomial proportion difference per site without overdispersion: $Y_{s0}=\\sum_{j:g=0} y_{sj}$ out of $N_{s0}=\\sum_{j:g=0} n_{sj}$ versus $Y_{s1}=\\sum_{j:g=1} y_{sj}$ out of $N_{s1}=\\sum_{j:g=1} n_{sj}$. Use a pooled normal approximation for the difference in proportions to obtain a $z$-test $p$-value per site. Control multiplicity by declaring sites with $p_{s} \\le q/m$ significant.\n\nC. Treat methylated reads as Poisson counts with an exposure offset and no overdispersion: $y_{sj} \\sim \\mathrm{Poisson}(n_{sj}\\lambda_{g})$ and test $H_{0}\\!:\\,\\lambda_{0}=\\lambda_{1}$ using a deviance test from a Poisson generalized linear model with a log link. Use the resulting deviance $p$-values and apply the same per-hypothesis cutoff $p_{s} \\le q$ to control FDR.\n\nD. Use the beta–binomial to model overdispersion but assume a common dispersion under both hypotheses. Parameterize each group $g$ by a mean $\\mu_{g}$ and a shared overdispersion $\\rho$ with $\\mu_{g} \\in (0,1)$ and $\\rho \\in (0,1)$. Test $H_{0}\\!:\\,\\mu_{0}=\\mu_{1}$, $\\rho$ shared, versus $H_{1}\\!:\\,\\mu_{0}\\neq \\mu_{1}$, $\\rho$ shared, using a likelihood ratio with asymptotic $\\chi^{2}$ with $1$ degree of freedom. For multiple testing, sort the $p$-values and declare significant those with $p_{(k)} \\le q/(m-k+1)$.\n\nOnly one option is fully correct.", "solution": "The user wants a critical validation of the problem statement, followed by a rigorous derivation of the solution and evaluation of all provided options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Domain**: Statistical testing for differential DNA methylation in whole-genome bisulfite sequencing (WGBS) data.\n-   **Data**: For each CpG site $s \\in \\{1,\\dots,m\\}$ and sample $j$ in group $g \\in \\{0,1\\}$ (controls and cases), we observe $y_{sj}$ (number of methylated reads) and $n_{sj}$ (total reads).\n-   **Core Constraint**: The model must account for extra-binomial variability (overdispersion) across biological replicates.\n-   **Task**:\n    1.  Derive a valid likelihood-based hypothesis test for each CpG site that accounts for overdispersion.\n    2.  Describe the method to control the false discovery rate (FDR) across all $m$ sites using the Benjamini–Hochberg (BH) procedure.\n    3.  The derivation must originate from foundational principles: binomial sampling and a well-established overdispersion model.\n    4.  The derivation must specify: null and alternative hypotheses, the test statistic, its null distribution, and the multiple testing procedure.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Grounding**: The problem is well-grounded in the field of statistical genomics and bioinformatics. The description of WGBS data as counts of methylated reads out of total reads ($y_{sj}$ out of $n_{sj}$) is standard. The observation that such data exhibits overdispersion (extra-binomial variability) due to biological and technical factors is a critical and universally acknowledged feature that must be addressed for valid inference. The use of hierarchical models like the beta-binomial model is a canonical approach to handle this overdispersion. The Benjamini-Hochberg procedure is the standard method for controlling the false discovery rate in high-throughput genomics experiments. The problem statement is scientifically and statistically sound.\n-   **Well-Posedness**: The problem is well-posed. It clearly defines the data structure, the core statistical challenge (overdispersion), the inferential goal (hypothesis testing for differential methylation), and the required components of a valid solution (model, hypotheses, test statistic, null distribution, FDR control). A unique and standard statistical framework exists to solve this problem.\n-   **Objectivity**: The language is precise, technical, and free of any subjectivity or ambiguity.\n\n**Flaw Checklist Assessment**:\n1.  **Scientific or Factual Unsoundness**: None. The premises are factually correct.\n2.  **Non-Formalizable or Irrelevant**: None. The problem is a formal statistical task.\n3.  **Incomplete or Contradictory Setup**: None. The problem provides all necessary conceptual components to derive the required statistical procedure.\n4.  **Unrealistic or Infeasible**: None. The described analysis is a routine task in modern computational biology.\n5.  **Ill-Posed or Poorly Structured**: None. A standard, stable solution exists.\n6.  **Pseudo-Profound, Trivial, or Tautological**: None. The problem involves non-trivial statistical modeling (hierarchical models, likelihood theory) and multiple testing theory.\n7.  **Outside Scientific Verifiability**: None. The methods are standard and verifiable.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. The user's request for a full derivation and option evaluation will be fulfilled.\n\n### Solution Derivation\n\nThe problem requires a site-wise hypothesis test for differential methylation that accounts for overdispersion, followed by FDR control.\n\n**1. Modeling Overdispersed Binomial Data**\nAt a single CpG site $s$ and for a single sample $j$, the number of methylated reads $y_{sj}$ given a total read count $n_{sj}$ and a true methylation proportion $p_{sj}$ is appropriately modeled by a Binomial distribution:\n$$ y_{sj} \\mid p_{sj} \\sim \\mathrm{Binomial}(n_{sj}, p_{sj}) $$\nThe problem states that there is overdispersion, meaning the proportions $p_{sj}$ are not constant within a group $g$. Instead, they vary from sample to sample according to some distribution. A standard and principled choice for modeling a distribution of proportions is the Beta distribution. For a given site $s$, we assume that for all samples $j$ in group $g$, the proportions $p_{sj}$ are drawn from a common Beta distribution parameterized by shape parameters $\\alpha_{sg}$ and $\\beta_{sg}$:\n$$ p_{sj} \\sim \\mathrm{Beta}(\\alpha_{sg}, \\beta_{sg}), \\quad \\text{for } j \\text{ in group } g $$\nThis hierarchical structure (Binomial data model, Beta prior on the proportion) induces a marginal distribution for the observed counts $y_{sj}$ known as the Beta-Binomial distribution. The probability mass function (PMF) is derived by integrating out the latent proportion $p$:\n$$ \\Pr(Y=y \\mid n, \\alpha, \\beta) = \\int_0^1 \\Pr(Y=y \\mid n, p) \\cdot f(p \\mid \\alpha, \\beta) \\, dp $$\n$$ = \\int_0^1 \\left[ \\binom{n}{y} p^y (1-p)^{n-y} \\right] \\left[ \\frac{p^{\\alpha-1} (1-p)^{\\beta-1}}{B(\\alpha, \\beta)} \\right] \\, dp $$\n$$ = \\binom{n}{y} \\frac{1}{B(\\alpha, \\beta)} \\int_0^1 p^{y+\\alpha-1} (1-p)^{n-y+\\beta-1} \\, dp $$\nThe integral is the definition of the Beta function $B(y+\\alpha, n-y+\\beta)$. Thus,\n$$ \\Pr(Y=y \\mid n, \\alpha, \\beta) = \\binom{n}{y} \\frac{B(y+\\alpha, n-y+\\beta)}{B(\\alpha, \\beta)} $$\n\n**2. Hypothesis Testing Framework**\nWe want to test for a difference in methylation between group $0$ (controls) and group $1$ (cases) at site $s$. This difference is captured by the parameters of the underlying Beta distributions.\n\n-   **Null Hypothesis ($H_0$)**: The methylation distribution is the same in both groups. This implies that the parameters of the Beta distribution are identical for an undisclosed site $s$:\n    $$ H_{0,s}: (\\alpha_{s0}, \\beta_{s0}) = (\\alpha_{s1}, \\beta_{s1}) $$\n-   **Alternative Hypothesis ($H_1$)**: The methylation distributions are different. This implies that the parameter sets are not identical:\n    $$ H_{1,s}: (\\alpha_{s0}, \\beta_{s0}) \\neq (\\alpha_{s1}, \\beta_{s1}) $$\n\n**3. The Likelihood Ratio Test (LRT)**\nAs a likelihood-based test, the LRT is the natural choice. For a given site $s$, the log-likelihood is the sum of the log-PMFs of the beta-binomial distribution over all samples.\n-   Under $H_1$, we fit separate parameters $(\\alpha_{s0}, \\beta_{s0})$ for group $0$ and $(\\alpha_{s1}, \\beta_{s1})$ for group $1$. The maximized log-likelihood is $\\ell_{1,s}(\\widehat{\\theta}_1)$, where $\\widehat{\\theta}_1 = (\\widehat{\\alpha}_{s0}, \\widehat{\\beta}_{s0}, \\widehat{\\alpha}_{s1}, \\widehat{\\beta}_{s1})$ are the Maximum Likelihood Estimates (MLEs). The total number of parameters is $4$.\n-   Under $H_0$, we fit a single set of shared parameters $(\\alpha_s, \\beta_s)$ to all samples. The maximized log-likelihood is $\\ell_{0,s}(\\widehat{\\theta}_0)$, where $\\widehat{\\theta}_0 = (\\widehat{\\alpha}_s, \\widehat{\\beta}_s)$ are the MLEs. The total number of parameters is $2$.\n\nThe LRT statistic for site $s$ is:\n$$ T_s = 2 \\left( \\ell_{1,s}(\\widehat{\\theta}_1) - \\ell_{0,s}(\\widehat{\\theta}_0) \\right) $$\nAccording to Wilks' theorem, under $H_0$, $T_s$ follows an asymptotic chi-squared ($\\chi^2$) distribution. The degrees of freedom ($df$) are the difference in the number of free parameters between the alternative and null models. Here, $df = 4 - 2 = 2$.\nThe $p$-value for site $s$ is then calculated as $p_s = \\Pr(\\chi^2_{2} \\ge T_s)$.\n\n**4. Multiple Testing Correction: Benjamini-Hochberg (BH) Procedure**\nWe perform $m$ independent or weakly dependent tests. To control the False Discovery Rate (FDR) at a target level $q$, we apply the BH procedure:\n1.  Collect the $p$-values from all $m$ sites: $p_1, p_2, \\dots, p_m$.\n2.  Sort these $p$-values in ascending order: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$.\n3.  Find the largest integer $k$ such that $p_{(k)} \\le \\frac{k}{m} q$.\n4.  Declare the CpG sites corresponding to the $p$-values $p_{(1)}, \\dots, p_{(k)}$ as statistically significant (i.e., differentially methylated).\n\n### Option-by-Option Analysis\n\n**A. Model $y_{sj} \\mid p_{sg} \\sim \\mathrm{Binomial}(n_{sj}, p_{sg})$ with a site–group–specific methylation proportion $p_{sg}$. To capture overdispersion across biological replicates, let $p_{sg} \\sim \\mathrm{Beta}(\\alpha_{g}, \\beta_{g})$. The marginal distribution... is then beta–binomial... Use the likelihood ratio statistic $T_{s} = 2\\Big(\\ell_{1,s}(\\widehat{\\theta}_{1}) - \\ell_{0,s}(\\widehat{\\theta}_{0})\\Big)$, which is asymptotically $\\chi^{2}$ with $2$ degrees of freedom... To control FDR at level $q$, collect all $m$ site-wise $p$-values, sort them $p_{(1)}\\le \\cdots \\le p_{(m)}$, find the largest $k$ such that $p_{(k)} \\le (k/m)\\,q$, and declare the sites with the $k$ smallest $p$-values significant...**\n-   **Analysis**: This option perfectly matches the principled derivation above. It correctly identifies the beta-binomial model as the marginal distribution resulting from a Beta-Binomial hierarchy. It correctly formulates the null and alternative hypotheses in their most general form. It correctly specifies the likelihood ratio test statistic and its asymptotic null distribution as $\\chi^2$ with $2$ degrees of freedom. Finally, it provides a precise and correct description of the Benjamini-Hochberg procedure for FDR control. The mention of the group mean $\\mu_g = \\alpha_g / (\\alpha_g + \\beta_g)$ is also a correct interpretation.\n-   **Verdict**: **Correct**.\n\n**B. Aggregate counts across samples within each group... test a single binomial proportion difference per site without overdispersion... Use a pooled normal approximation for the difference in proportions... Control multiplicity by declaring sites with $p_{s} \\le q/m$ significant.**\n-   **Analysis**: This approach is fundamentally flawed. Aggregating counts across samples (i.e., creating \"pseudo-replicates\") completely ignores the inter-sample variability, which is the source of the overdispersion the problem explicitly requires to be modeled. This leads to invalidly small standard errors and a massive inflation of the false positive rate. Furthermore, the multiple testing correction described, $p_s \\le q/m$, is the Bonferroni correction, which controls the Family-Wise Error Rate (FWER), not the False Discovery Rate (FDR) as requested by the problem.\n-   **Verdict**: **Incorrect**.\n\n**C. Treat methylated reads as Poisson counts with an exposure offset and no overdispersion... test $H_{0}\\!:\\,\\lambda_{0}=\\lambda_{1}$ using a deviance test from a Poisson generalized linear model... Use the resulting deviance $p$-values and apply the same per-hypothesis cutoff $p_{s} \\le q$ to control FDR.**\n-   **Analysis**: This option is flawed for multiple reasons. First, it proposes a Poisson model, which is less appropriate than the binomial model for counts with a fixed total. Second, it explicitly states \"no overdispersion,\" directly violating a key requirement of the problem. A standard Poisson model assumes the variance equals the mean, whereas these data are overdispersed. Third, the description of multiple testing control—\"apply the same per-hypothesis cutoff $p_{s} \\le q$ to control FDR\"—is grossly incorrect. This procedure does not control FDR or any meaningful error rate across multiple tests; it is equivalent to performing no correction at all.\n-   **Verdict**: **Incorrect**.\n\n**D. Use the beta–binomial to model overdispersion but assume a common dispersion under both hypotheses... Test $H_{0}\\!:\\,\\mu_{0}=\\mu_{1}$, $\\rho$ shared, versus $H_{1}\\!:\\,\\mu_{0}\\neq \\mu_{1}$, $\\rho$ shared, using a likelihood ratio with asymptotic $\\chi^{2}$ with $1$ degree of freedom. For multiple testing, sort the $p$-values and declare significant those with $p_{(k)} \\le q/(m-k+1)$.**\n-   **Analysis**: This option describes a valid and common statistical test for differential methylation. The test is more restrictive than that in option A, focusing only on a mean shift while assuming constant overdispersion. The LRT for this model correctly has $df = (\\text{params for } H_1) - (\\text{params for } H_0) = (\\mu_0, \\mu_1, \\rho) - (\\mu, \\rho) = 3 - 2 = 1$, so the $\\chi^2_1$ null distribution is correct. However, the final step is flawed with respect to the problem statement. The multiple testing procedure described, finding $k$ such that $p_{(k)} \\le q/(m-k+1)$, is the Benjamini-Yekutieli (BY) procedure, which is more conservative than the Benjamini-Hochberg (BH) procedure. The problem explicitly specified the use of the Benjamini–Hochberg (BH) procedure.\n-   **Verdict**: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "4337386"}, {"introduction": "The ultimate goal of many epigenetic studies is to uncover the functional consequences of changes in DNA methylation and histone modifications. A statistically significant change in methylation is most impactful when it leads to a corresponding change in gene expression and, ultimately, cellular function. This final hands-on practice [@problem_id:4337352] simulates a real-world bioinformatics pipeline designed to identify high-confidence candidate genes that are silenced by epigenetic mechanisms in cancer, integrating data from promoter DNA methylation, gene expression, and repressive histone marks to build a compelling biological narrative.", "problem": "You are given arrays representing paired promoter DNA methylation and gene expression profiles for multiple genes, measured across tumor and normal tissue cohorts. The arrays are grouped by test case and gene. For each gene, there are tumor methylation values, normal methylation values, tumor expression values, normal expression values, and a binary indicator for presence of a repressive histone modification (Histone H3 Lysine 27 trimethylation, written as Histone H3K27me3).\n\nFundamental base for derivation:\n- Epigenetic regulation connects promoter DNA methylation to transcriptional silencing. Promoter hypermethylation is commonly associated with reduced transcriptional output, especially in genes with tumor suppressive roles.\n- Gene expression follows the Central Dogma of Molecular Biology, where messenger ribonucleic acid (mRNA) abundance reflects transcription levels.\n- Methylation values are bounded fractional measurements typically represented as beta-values in the unitless interval $[0,1]$.\n- Statistical testing and multiple hypothesis correction use well-established frequentist procedures.\n\nDefinitions to use:\n- Let a gene be considered a candidate methylation-silenced tumor suppressor if promoter methylation is higher in tumors than normals by a practically meaningful amount, gene expression is lower in tumors than normals by a practically meaningful amount, and there is a negative relationship between promoter methylation and tumor gene expression across tumor samples. Use non-parametric testing for methylation distributions and parametric testing for expression differences, along with multiple hypothesis correction per test case.\n- Use Benjamini–Hochberg False Discovery Rate (FDR) correction within each test case across genes for methylation and expression $p$-values.\n\nDecision thresholds (apply consistently across all test cases):\n- Methylation effect size threshold: mean difference $\\Delta \\beta \\geq 0.2$, where $\\Delta \\beta = \\overline{\\beta}_{\\text{tumor}} - \\overline{\\beta}_{\\text{normal}}$. Beta-values must be treated as decimals in $[0,1]$, not percentages.\n- Methylation significance: two-sided Mann–Whitney $U$ test $p$-value, FDR-adjusted $q$-value $\\leq 0.05$.\n- Expression effect size threshold: log-base-$2$ fold change $\\log_{2}\\left(\\overline{E}_{\\text{tumor}} + \\epsilon\\right) - \\log_{2}\\left(\\overline{E}_{\\text{normal}} + \\epsilon\\right) \\leq -1.0$, where $\\epsilon = 10^{-3}$ to avoid division by zero.\n- Expression significance: two-sided Welch’s $t$-test (unequal variance) $p$-value, FDR-adjusted $q$-value $\\leq 0.05$.\n- Correlation criterion: Spearman rank correlation $\\rho \\leq -0.5$ with two-sided correlation $p$-value $\\leq 0.05$, computed across tumor samples only, pairing each tumor’s promoter methylation value with its tumor expression value.\n- High-confidence classification: genes meeting all above criteria and having Histone H3K27me3 present are marked as high-confidence; otherwise, only standard-confidence.\n\nAlgorithmic requirements:\n- For each test case, compute per-gene $\\Delta \\beta$, methylation Mann–Whitney $U$ $p$-values, expression log-base-$2$ fold changes, expression Welch’s $t$-test $p$-values, Spearman $\\rho$ between tumor methylation and tumor expression, and the associated correlation $p$-values. Apply the Benjamini–Hochberg procedure to methylation and expression $p$-values independently within the test case to get $q$-values.\n- Flag a gene as a standard-confidence hit if it satisfies all three conditions: methylation hyper in tumors (effect and adjusted significance), expression down in tumors (effect and adjusted significance), and negative correlation in tumors (effect and significance). Flag it additionally as a high-confidence hit if Histone H3K27me3 is present.\n\nInput to implement directly in your program (test suite):\n- Test case $1$ has $4$ genes with $6$ tumor and $6$ normal samples per gene. Arrays per gene (tumor methylation, normal methylation, tumor expression, normal expression) and histone indicator:\n  - Gene $0$: tumor methylation $[0.80,0.82,0.83,0.85,0.86,0.88]$, normal methylation $[0.12,0.18,0.15,0.22,0.17,0.19]$, tumor expression $[2.6,2.4,2.3,2.1,2.0,1.9]$, normal expression $[6.0,6.5,5.8,6.2,6.4,6.1]$, histone $1$ (True).\n  - Gene $1$: tumor methylation $[0.60,0.62,0.61,0.59,0.58,0.62]$, normal methylation $[0.40,0.42,0.41,0.39,0.38,0.42]$, tumor expression $[6.0,5.8,6.1,5.9,6.2,6.0]$, normal expression $[5.5,5.7,5.4,5.6,5.5,5.6]$, histone $0$ (False).\n  - Gene $2$: tumor methylation $[0.74,0.75,0.76,0.77,0.78,0.79]$, normal methylation $[0.20,0.21,0.22,0.22,0.23,0.24]$, tumor expression $[2.0,2.1,2.2,2.3,2.4,2.5]$, normal expression $[6.0,6.1,6.2,6.3,6.1,6.2]$, histone $1$ (True).\n  - Gene $3$: tumor methylation $[0.70,0.72,0.74,0.75,0.71,0.73]$, normal methylation $[0.25,0.27,0.26,0.28,0.24,0.26]$, tumor expression $[3.0,2.8,2.6,2.5,2.7,2.6]$, normal expression $[5.5,5.2,5.1,5.3,5.4,5.2]$, histone $0$ (False).\n- Test case $2$ has $3$ genes with $6$ tumor and $6$ normal samples per gene:\n  - Gene $0$: tumor methylation $[0.60,0.61,0.62,0.59,0.58,0.60]$, normal methylation $[0.40,0.41,0.42,0.39,0.38,0.40]$, tumor expression $[4.0,3.9,4.1,3.8,4.0,3.9]$, normal expression $[4.2,4.1,4.2,4.3,4.1,4.2]$, histone $1$ (True).\n  - Gene $1$: tumor methylation $[0.85,0.83,0.84,0.86,0.87,0.85]$, normal methylation $[0.30,0.29,0.31,0.28,0.32,0.30]$, tumor expression $[2.5,2.4,2.3,2.2,2.1,2.0]$, normal expression $[5.0,5.1,5.2,5.1,5.0,5.2]$, histone $0$ (False).\n  - Gene $2$: tumor methylation $[0.77,0.78,0.79,0.80,0.81,0.82]$, normal methylation $[0.20,0.21,0.19,0.22,0.21,0.20]$, tumor expression $[0.20,0.18,0.15,0.12,0.10,0.08]$, normal expression $[3.0,3.2,3.1,3.3,3.2,3.1]$, histone $1$ (True).\n- Test case $3$ has $3$ genes with $3$ tumor and $3$ normal samples per gene:\n  - Gene $0$: tumor methylation $[0.70,0.72,0.74]$, normal methylation $[0.40,0.42,0.41]$, tumor expression $[3.0,2.8,2.6]$, normal expression $[5.0,5.2,5.1]$, histone $0$ (False).\n  - Gene $1$: tumor methylation $[0.68,0.70,0.69]$, normal methylation $[0.50,0.49,0.51]$, tumor expression $[4.5,4.6,4.4]$, normal expression $[4.6,4.5,4.4]$, histone $0$ (False).\n  - Gene $2$: tumor methylation $[0.85,0.87,0.86]$, normal methylation $[0.30,0.32,0.31]$, tumor expression $[1.5,1.4,1.6]$, normal expression $[5.0,4.8,4.9]$, histone $1$ (True).\n\nProgram requirements:\n- Implement the pipeline as specified, compute per-gene flags in each test case, and return for each test case two lists: the first list contains the integer indices of genes meeting standard-confidence criteria in ascending order; the second list contains the integer indices of genes meeting high-confidence criteria in ascending order.\n- Final output format: your program should produce a single line of output containing the results as a comma-separated list of per-test-case results, with each per-test-case result itself being a two-element list of integer lists, and with no spaces anywhere in the output string. For example, if the first test case yields genes $[0,3]$ standard and $[0]$ high-confidence, and the second yields genes $[1]$ standard and $[]$ high-confidence, the single-line output should be in the exact format $[[[0,3],[0]],[[1],[]],...]$.\n\nNote: All percentage-like quantities must be represented as decimals or fractions, not with a percent sign. There are no physical units or angles in this problem. Ensure numerical computations adhere strictly to the thresholds and procedures described.", "solution": "The problem is scientifically and mathematically well-posed, providing a complete set of data, definitions, and procedural requirements for identifying candidate methylation-silenced genes from paired methylation and expression data. The problem is valid. The solution requires a systematic implementation of a multi-step bioinformatics pipeline involving statistical testing, effect size calculation, and multiple hypothesis correction. The core principles are grounded in molecular biology (epigenetic regulation of gene expression) and biostatistics.\n\nThe solution proceeds as follows:\nFirst, for each test case, we must process all genes to gather the necessary statistics. For each gene, we perform three types of analysis: differential methylation, differential expression, and correlation.\n\n1.  **Differential Methylation Analysis**:\n    -   The effect size is the difference in mean methylation between tumor and normal samples, $\\Delta \\beta = \\overline{\\beta}_{\\text{tumor}} - \\overline{\\beta}_{\\text{normal}}$.\n    -   The statistical significance is determined using a two-sided Mann-Whitney $U$ test, which is a non-parametric test suitable for comparing two independent groups when the data may not be normally distributed. This test yields a $p$-value for methylation, $p_{\\text{meth}}$.\n\n2.  **Differential Expression Analysis**:\n    -   The effect size is the log-base-$2$ fold change, calculated as $\\log_{2}\\left(\\overline{E}_{\\text{tumor}} + \\epsilon\\right) - \\log_{2}\\left(\\overline{E}_{\\text{normal}} + \\epsilon\\right)$. A small constant $\\epsilon = 10^{-3}$ is added to avoid issues with $\\log(0)$. The use of a logarithmic scale is standard for gene expression data as it helps normalize skewed distributions and provides a symmetric representation of up- and down-regulation.\n    -   Statistical significance is assessed using a two-sided Welch's $t$-test. This parametric test is appropriate for comparing the means of two independent groups and does not assume equal variances, making it more robust than a standard Student's $t$-test. This test yields a $p$-value for expression, $p_{\\text{expr}}$.\n\n3.  **Correlation Analysis**:\n    -   The relationship between promoter methylation and gene expression within the tumor cohort is quantified using Spearman's rank correlation coefficient, $\\rho$. This non-parametric measure assesses the strength and direction of a monotonic relationship between two variables.\n    -   A $p$-value, $p_{\\text{corr}}$, is computed to test the significance of the observed correlation.\n\nAfter computing these statistics for all genes within a single test case, we must correct for multiple hypothesis testing. This is crucial because performing many statistical tests increases the probability of observing a significant result by chance (a Type I error).\n\n4.  **Multiple Hypothesis Correction**:\n    -   The Benjamini-Hochberg (BH) procedure is applied to control the False Discovery Rate (FDR). The FDR is the expected proportion of rejected null hypotheses that are actually false positives.\n    -   The BH procedure is applied independently to the set of all $p_{\\text{meth}}$ values and the set of all $p_{\\text{expr}}$ values from the genes within the test case. This converts the raw $p$-values into FDR-adjusted $q$-values, $q_{\\text{meth}}$ and $q_{\\text{expr}}$.\n    -   The procedure for calculating $q$-values from a set of $m$ $p$-values is as follows:\n        i.  Rank the $p$-values in ascending order: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$.\n        ii. Calculate the BH-adjusted $p$-value for each rank $i$ as $q_{(i)}^{\\text{raw}} = \\frac{p_{(i)} \\cdot m}{i}$.\n        iii. To enforce monotonicity, the final $q$-value for rank $i$ is $q_{(i)} = \\min(q_{(i+1)}, q_{(i)}^{\\text{raw}})$ for $i = m-1, \\dots, 1$, with $q_{(m)} = q_{(m)}^{\\text{raw}}$. More simply, $q_{(i)}$ is the cumulative minimum of the raw adjusted $p$-values from rank $i$ to $m$.\n    -   The original ordering is restored to associate each gene with its correct $q$-value. Note that the correlation $p$-value, $p_{\\text{corr}}$, is not subject to multiple testing correction in this problem's design, as it's a per-gene-level filter.\n\nFinally, we apply the decision thresholds to classify each gene.\n\n5.  **Classification**:\n    -   A gene is flagged as a **standard-confidence** hit if it meets all of the following criteria:\n        a. **Methylation Hyper-regulation**: $\\Delta\\beta \\geq 0.2$ **and** $q_{\\text{meth}} \\leq 0.05$.\n        b. **Expression Down-regulation**: $\\log_2(\\text{Fold Change}) \\leq -1.0$ **and** $q_{\\text{expr}} \\leq 0.05$.\n        c. **Negative Correlation**: $\\rho \\leq -0.5$ **and** $p_{\\text{corr}} \\leq 0.05$.\n    -   A gene is flagged as a **high-confidence** hit if it is a standard-confidence hit **and** it has the repressive histone mark H3K27me3 present (indicated by a binary flag of $1$).\n\nThis entire pipeline is applied to each test case independently. The final output is structured to present the lists of standard-confidence and high-confidence gene indices for each test case.", "answer": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef benjamini_hochberg(p_values):\n    \"\"\"\n    Performs the Benjamini-Hochberg FDR correction.\n\n    Args:\n        p_values (list or np.ndarray): A list of p-values.\n\n    Returns:\n        np.ndarray: An array of FDR-adjusted q-values, in the same order as the input p_values.\n    \"\"\"\n    p_values = np.asarray(p_values)\n    m = len(p_values)\n    if m == 0:\n        return np.array([])\n    \n    # Sort p-values and keep track of original indices\n    original_indices = np.argsort(p_values)\n    sorted_p_values = p_values[original_indices]\n    \n    # Calculate q-values\n    q_values = np.zeros(m)\n    for i, p_val in enumerate(sorted_p_values):\n        rank = i + 1\n        q_values[i] = (p_val * m) / rank\n    \n    # Enforce monotonicity\n    # q_i = min(q_{i+1}, q_i) for i = m-1 ... 1\n    # This is equivalent to taking the cumulative minimum from the end of the sorted list\n    q_values = np.minimum.accumulate(q_values[::-1])[::-1]\n    \n    # Restore original order\n    final_q_values = np.zeros(m)\n    final_q_values[original_indices] = q_values\n    \n    # Clamp values to be at most 1.0\n    return np.clip(final_q_values, a_min=None, a_max=1.0)\n\ndef solve():\n    \"\"\"\n    Main function to run the epigenetic analysis pipeline on the provided test cases.\n    \"\"\"\n    # Input data structured as:\n    # list of test cases -> list of genes -> tuple of (tumor_meth, normal_meth, tumor_expr, normal_expr, histone_flag)\n    test_cases = [\n        # Test case 1\n        [\n            (np.array([0.80,0.82,0.83,0.85,0.86,0.88]), np.array([0.12,0.18,0.15,0.22,0.17,0.19]), np.array([2.6,2.4,2.3,2.1,2.0,1.9]), np.array([6.0,6.5,5.8,6.2,6.4,6.1]), 1),\n            (np.array([0.60,0.62,0.61,0.59,0.58,0.62]), np.array([0.40,0.42,0.41,0.39,0.38,0.42]), np.array([6.0,5.8,6.1,5.9,6.2,6.0]), np.array([5.5,5.7,5.4,5.6,5.5,5.6]), 0),\n            (np.array([0.74,0.75,0.76,0.77,0.78,0.79]), np.array([0.20,0.21,0.22,0.22,0.23,0.24]), np.array([2.0,2.1,2.2,2.3,2.4,2.5]), np.array([6.0,6.1,6.2,6.3,6.1,6.2]), 1),\n            (np.array([0.70,0.72,0.74,0.75,0.71,0.73]), np.array([0.25,0.27,0.26,0.28,0.24,0.26]), np.array([3.0,2.8,2.6,2.5,2.7,2.6]), np.array([5.5,5.2,5.1,5.3,5.4,5.2]), 0),\n        ],\n        # Test case 2\n        [\n            (np.array([0.60,0.61,0.62,0.59,0.58,0.60]), np.array([0.40,0.41,0.42,0.39,0.38,0.40]), np.array([4.0,3.9,4.1,3.8,4.0,3.9]), np.array([4.2,4.1,4.2,4.3,4.1,4.2]), 1),\n            (np.array([0.85,0.83,0.84,0.86,0.87,0.85]), np.array([0.30,0.29,0.31,0.28,0.32,0.30]), np.array([2.5,2.4,2.3,2.2,2.1,2.0]), np.array([5.0,5.1,5.2,5.1,5.0,5.2]), 0),\n            (np.array([0.77,0.78,0.79,0.80,0.81,0.82]), np.array([0.20,0.21,0.19,0.22,0.21,0.20]), np.array([0.20,0.18,0.15,0.12,0.10,0.08]), np.array([3.0,3.2,3.1,3.3,3.2,3.1]), 1),\n        ],\n        # Test case 3\n        [\n            (np.array([0.70,0.72,0.74]), np.array([0.40,0.42,0.41]), np.array([3.0,2.8,2.6]), np.array([5.0,5.2,5.1]), 0),\n            (np.array([0.68,0.70,0.69]), np.array([0.50,0.49,0.51]), np.array([4.5,4.6,4.4]), np.array([4.6,4.5,4.4]), 0),\n            (np.array([0.85,0.87,0.86]), np.array([0.30,0.32,0.31]), np.array([1.5,1.4,1.6]), np.array([5.0,4.8,4.9]), 1),\n        ]\n    ]\n\n    # --- Decision Thresholds ---\n    METH_EFFECT_THRESHOLD = 0.2\n    Q_VALUE_THRESHOLD = 0.05\n    EXPR_EFFECT_THRESHOLD = -1.0\n    CORR_EFFECT_THRESHOLD = -0.5\n    CORR_P_VALUE_THRESHOLD = 0.05\n    EPSILON = 1e-3\n\n    overall_results = []\n\n    for case_data in test_cases:\n        num_genes = len(case_data)\n        \n        # Lists to store per-gene statistics for the current test case\n        meth_p_values = []\n        expr_p_values = []\n        gene_stats = []\n\n        # Step 1: Calculate all stats for each gene\n        for gene_idx in range(num_genes):\n            t_meth, n_meth, t_expr, n_expr, histone_flag = case_data[gene_idx]\n\n            # Methylation analysis\n            delta_beta = np.mean(t_meth) - np.mean(n_meth)\n            _, meth_p = stats.mannwhitneyu(t_meth, n_meth, alternative='two-sided')\n            meth_p_values.append(meth_p)\n\n            # Expression analysis\n            mean_t_expr = np.mean(t_expr)\n            mean_n_expr = np.mean(n_expr)\n            log2fc = np.log2(mean_t_expr + EPSILON) - np.log2(mean_n_expr + EPSILON)\n            _, expr_p = stats.ttest_ind(t_expr, n_expr, equal_var=False, alternative='two-sided')\n            expr_p_values.append(expr_p)\n            \n            # Correlation analysis\n            spearman_rho, spearman_p = stats.spearmanr(t_meth, t_expr)\n\n            gene_stats.append({\n                'delta_beta': delta_beta,\n                'log2fc': log2fc,\n                'spearman_rho': spearman_rho,\n                'spearman_p': spearman_p,\n                'histone_flag': histone_flag\n            })\n\n        # Step 2: Apply Benjamini-Hochberg correction\n        meth_q_values = benjamini_hochberg(meth_p_values)\n        expr_q_values = benjamini_hochberg(expr_p_values)\n\n        # Step 3: Classify genes based on thresholds\n        standard_confidence_indices = []\n        high_confidence_indices = []\n\n        for gene_idx in range(num_genes):\n            stats_dict = gene_stats[gene_idx]\n            \n            meth_hyper = (stats_dict['delta_beta'] >= METH_EFFECT_THRESHOLD) and (meth_q_values[gene_idx] = Q_VALUE_THRESHOLD)\n            expr_down = (stats_dict['log2fc'] = EXPR_EFFECT_THRESHOLD) and (expr_q_values[gene_idx] = Q_VALUE_THRESHOLD)\n            neg_corr = (stats_dict['spearman_rho'] = CORR_EFFECT_THRESHOLD) and (stats_dict['spearman_p'] = CORR_P_VALUE_THRESHOLD)\n\n            if meth_hyper and expr_down and neg_corr:\n                standard_confidence_indices.append(gene_idx)\n                if stats_dict['histone_flag'] == 1:\n                    high_confidence_indices.append(gene_idx)\n        \n        overall_results.append([sorted(standard_confidence_indices), sorted(high_confidence_indices)])\n\n    # Format output string\n    output_str = str(overall_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "4337352"}]}