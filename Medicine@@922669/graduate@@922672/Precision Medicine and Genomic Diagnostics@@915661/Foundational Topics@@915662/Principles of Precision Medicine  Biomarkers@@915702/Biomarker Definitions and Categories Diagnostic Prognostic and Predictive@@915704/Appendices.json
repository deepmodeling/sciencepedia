{"hands_on_practices": [{"introduction": "A core function of a diagnostic biomarker is to refine our assessment of disease probability for an individual patient. While sensitivity and specificity are intrinsic properties of a test at a given threshold, their true clinical utility lies in their ability to modify a pre-existing (prior) probability of disease into a more accurate post-test probability. This exercise solidifies your understanding of how to translate test characteristics into actionable clinical information by deriving the fundamental relationship between pre-test odds, likelihood ratios, and post-test odds from first principles [@problem_id:4319496]. Mastering this application of Bayes' theorem is essential for interpreting any diagnostic test result in a meaningful context.", "problem": "A continuous cell-free deoxyribonucleic acid (cfDNA) methylation signature, denoted by the scalar biomarker $B$, is being evaluated as a diagnostic biomarker for colorectal cancer in a symptomatic cohort. In the taxonomy of biomarkers used in precision medicine and genomic diagnostics, a diagnostic biomarker indicates the presence or absence of a current disease state, a prognostic biomarker stratifies the risk of future outcomes independent of therapy, and a predictive biomarker estimates the differential treatment response. Here, $B$ is used to classify disease status rather than future risk or treatment response, and a threshold $t^{\\ast}$ is chosen based on Receiver Operating Characteristic (ROC) analysis to produce a binary test result $T^{+}$ if $B \\ge t^{\\ast}$ and $T^{-}$ otherwise.\n\nAt the threshold $t^{\\ast}$ validated on an external cohort, the sensitivity and specificity are observed to be $sens = 0.83$ and $spec = 0.91$, respectively, where sensitivity is defined as $P(T^{+}\\mid D)$ and specificity is defined as $P(T^{-}\\mid \\neg D)$, with $D$ denoting the event \"disease present\" and $\\neg D$ denoting \"disease absent.\" For a particular patient, a well-calibrated clinical risk model provides a prior probability of disease $P(D) = 0.18$ before the biomarker measurement.\n\nStarting only from the fundamental definitions of sensitivity and specificity and Bayes' theorem, (i) derive the expressions for the positive and negative likelihood ratios, $LR^{+}$ and $LR^{-}$, in terms of $sens$ and $spec$, and (ii) derive the odds form of Bayes' theorem that relates post-test odds to pre-test odds and the appropriate likelihood ratio. Then, using these derived results, compute the post-test odds $odds(D\\mid T^{+})$ after a positive test result at threshold $t^{\\ast}$ for this patient.\n\nExpress the final numerical answer for $odds(D\\mid T^{+})$ as a unitless ratio and round your answer to four significant figures.", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in established principles of biostatistics and medical diagnostics, well-posed with all necessary information provided, and objective in its language and definitions. The data provided ($sens=0.83$, $spec=0.91$, $P(D)=0.18$) are internally consistent and plausible for a real-world clinical scenario. The problem is a standard, non-trivial application of Bayesian inference that requires rigorous derivation from first principles, as requested.\n\nThe solution proceeds by first deriving the required expressions for the likelihood ratios and the odds form of Bayes' theorem, and then applying these results to compute the final numerical answer.\n\n(i) Derivation of Likelihood Ratios\n\nThe positive likelihood ratio, $LR^{+}$, is defined as the ratio of the probability of a positive test result in the diseased population to the probability of a positive test result in the non-diseased population.\n$$LR^{+} \\equiv \\frac{P(T^{+}\\mid D)}{P(T^{+}\\mid \\neg D)}$$\nThe numerator is given by the definition of sensitivity, $sens$:\n$$P(T^{+}\\mid D) = sens$$\nThe denominator, $P(T^{+}\\mid \\neg D)$, is the false positive rate. It can be expressed in terms of specificity, $spec$. By definition, specificity is the probability of a negative test result in the non-diseased population:\n$$spec = P(T^{-}\\mid \\neg D)$$\nSince for any given condition (here, $\\neg D$), the test result must be either positive ($T^{+}$) or negative ($T^{-}$), the sum of their probabilities must be $1$:\n$$P(T^{+}\\mid \\neg D) + P(T^{-}\\mid \\neg D) = 1$$\nSolving for $P(T^{+}\\mid \\neg D)$ yields:\n$$P(T^{+}\\mid \\neg D) = 1 - P(T^{-}\\mid \\neg D) = 1 - spec$$\nSubstituting the expressions for the numerator and denominator back into the definition of $LR^{+}$ gives the desired expression:\n$$LR^{+} = \\frac{sens}{1 - spec}$$\n\nSimilarly, the negative likelihood ratio, $LR^{-}$, is defined as the ratio of the probability of a negative test result in the diseased population to the probability of a negative test result in the non-diseased population.\n$$LR^{-} \\equiv \\frac{P(T^{-}\\mid D)}{P(T^{-}\\mid \\neg D)}$$\nThe denominator is given directly by the definition of specificity:\n$$P(T^{-}\\mid \\neg D) = spec$$\nThe numerator, $P(T^{-}\\mid D)$, is the false negative rate. It can be expressed in terms of sensitivity. For the diseased population ($D$), the sum of probabilities of a positive and negative test result must be $1$:\n$$P(T^{+}\\mid D) + P(T^{-}\\mid D) = 1$$\nSolving for $P(T^{-}\\mid D)$ yields:\n$$P(T^{-}\\mid D) = 1 - P(T^{+}\\mid D) = 1 - sens$$\nSubstituting these expressions for the numerator and denominator into the definition of $LR^{-}$ gives the final expression:\n$$LR^{-} = \\frac{1 - sens}{spec}$$\n\n(ii) Derivation of the Odds Form of Bayes' Theorem\n\nThe odds of an event $A$ are defined as the ratio of the probability of the event occurring to the probability of it not occurring, $odds(A) = \\frac{P(A)}{P(\\neg A)}$. The post-test odds of disease given a positive test result $T^{+}$ are therefore:\n$$odds(D\\mid T^{+}) = \\frac{P(D\\mid T^{+})}{P(\\neg D\\mid T^{+})}$$\nWe apply Bayes' theorem to find expressions for the numerator and the denominator. For the numerator:\n$$P(D\\mid T^{+}) = \\frac{P(T^{+}\\mid D)P(D)}{P(T^{+})}$$\nAnd for the denominator:\n$$P(\\neg D\\mid T^{+}) = \\frac{P(T^{+}\\mid \\neg D)P(\\neg D)}{P(T^{+})}$$\nSubstituting these into the odds ratio, the common term $P(T^{+})$ in the denominator of both expressions cancels out:\n$$odds(D\\mid T^{+}) = \\frac{\\frac{P(T^{+}\\mid D)P(D)}{P(T^{+})}}{\\frac{P(T^{+}\\mid \\neg D)P(\\neg D)}{P(T^{+})}} = \\frac{P(T^{+}\\mid D)P(D)}{P(T^{+}\\mid \\neg D)P(\\neg D)}$$\nThis expression can be regrouped to separate the likelihoods from the prior probabilities:\n$$odds(D\\mid T^{+}) = \\left(\\frac{P(T^{+}\\mid D)}{P(T^{+}\\mid \\neg D)}\\right) \\left(\\frac{P(D)}{P(\\neg D)}\\right)$$\nThe first term is the definition of the positive likelihood ratio, $LR^{+}$. The second term is the definition of the pre-test (or prior) odds of disease, $odds(D)$. Therefore, we arrive at the odds form of Bayes' theorem:\n$$odds(D\\mid T^{+}) = LR^{+} \\times odds(D)$$\nThis fundamental relationship states that the post-test odds are equal to the pre-test odds multiplied by the likelihood ratio corresponding to the test result.\n\n(iii) Calculation of Post-Test Odds $odds(D\\mid T^{+})$\n\nUsing the derived results, we can now compute the post-test odds for the patient.\nFirst, we calculate the pre-test odds, $odds(D)$, from the given prior probability $P(D) = 0.18$.\nThe probability of no disease is $P(\\neg D) = 1 - P(D) = 1 - 0.18 = 0.82$.\nThe pre-test odds are:\n$$odds(D) = \\frac{P(D)}{P(\\neg D)} = \\frac{0.18}{0.82}$$\nNext, we calculate the positive likelihood ratio, $LR^{+}$, using the provided sensitivity $sens = 0.83$ and specificity $spec = 0.91$.\n$$LR^{+} = \\frac{sens}{1 - spec} = \\frac{0.83}{1 - 0.91} = \\frac{0.83}{0.09}$$\nFinally, we compute the post-test odds using the odds form of Bayes' theorem:\n$$odds(D\\mid T^{+}) = LR^{+} \\times odds(D) = \\left(\\frac{0.83}{0.09}\\right) \\times \\left(\\frac{0.18}{0.82}\\right)$$\nThe calculation is performed as follows:\n$$odds(D\\mid T^{+}) = \\frac{0.83 \\times 0.18}{0.09 \\times 0.82} = \\frac{0.1494}{0.0738}$$\n$$odds(D\\mid T^{+}) \\approx 2.024389...$$\nRounding the result to four significant figures gives $2.024$.", "answer": "$$\\boxed{2.024}$$", "id": "4319496"}, {"introduction": "In an ideal world, every subject in a diagnostic study would undergo both the new biomarker test and the definitive \"gold standard\" reference test. In reality, logistical, ethical, or economic constraints often lead to a situation where only a subset of subjects are fully verified, a phenomenon known as verification bias. This practice challenges you to confront this common issue by using inverse-probability weighting (IPW), a robust statistical technique, to correct for the bias it introduces. By adjusting for the differential verification rates, you can recover unbiased estimates of sensitivity and specificity, a crucial skill for critically evaluating and interpreting biomarker literature where study designs are often imperfect [@problem_id:4319561].", "problem": "A cancer genomic diagnostic biomarker is being evaluated for its utility as a diagnostic biomarker, distinguishing individuals with disease from those without disease. The binary biomarker test result is denoted by $T \\in \\{+,-\\}$ and the disease status determined by a definitive reference standard (e.g., whole-exome sequencing) is denoted by $D \\in \\{1,0\\}$, where $D=1$ indicates disease present. Subjects are stratified by a pretest clinical risk stratum $S \\in \\{\\mathrm{High}, \\mathrm{Low}\\}$ based on standard clinical features. Due to ethical and logistical constraints, the reference standard is not applied to all screened subjects, leading to partial verification. The decision to verify is based on $S$ and $T$, and the probability of receiving the reference standard, $\\pi_{s,t} = \\Pr(\\text{verified} \\mid S=s, T=t)$, is known from the study design. Assume the reference standard is perfect and that missingness of $D$ is missing at random given $(S,T)$, that is, $D \\perp \\!\\!\\! \\perp \\text{verified} \\mid (S,T)$.\n\nAmong those who received the reference standard, the following counts are observed:\n- High-risk, $T=+$: $n_{\\mathrm{H},+}^{V} = 360$, with $d_{\\mathrm{H},+} = 300$ having $D=1$, and $\\pi_{\\mathrm{H},+} = 0.9$.\n- High-risk, $T=-$: $n_{\\mathrm{H},-}^{V} = 240$, with $d_{\\mathrm{H},-} = 48$ having $D=1$, and $\\pi_{\\mathrm{H},-} = 0.6$.\n- Low-risk, $T=+$: $n_{\\mathrm{L},+}^{V} = 250$, with $d_{\\mathrm{L},+} = 125$ having $D=1$, and $\\pi_{\\mathrm{L},+} = 0.5$.\n- Low-risk, $T=-$: $n_{\\mathrm{L},-}^{V} = 200$, with $d_{\\mathrm{L},-} = 10$ having $D=1$, and $\\pi_{\\mathrm{L},-} = 0.2$.\n\nStarting from the fundamental definitions $\\text{sensitivity} = \\Pr(T=+ \\mid D=1)$ and $\\text{specificity} = \\Pr(T=- \\mid D=0)$, and using probability laws under the given missingness assumption, derive an inverse-probability-weighted estimator that corrects for verification bias and apply it to compute unbiased estimates of sensitivity and specificity. Then compute the corrected Youden's $J$ index, defined as $J = \\text{sensitivity} + \\text{specificity} - 1$. Report the final value of $J$ as a decimal and round your answer to four significant figures.", "solution": "### Step 1: Extract Givens\nThe problem provides the following information:\n-   A binary biomarker test result $T \\in \\{+,-\\}$.\n-   A binary disease status $D \\in \\{1,0\\}$, where $D=1$ indicates disease.\n-   A pretest clinical risk stratum $S \\in \\{\\mathrm{High}, \\mathrm{Low}\\}$.\n-   The verification of disease status $D$ is partial. Let $V$ be an indicator variable for verification, $V=1$ if verified, $V=0$ otherwise.\n-   The probability of verification depends on the stratum $S$ and test result $T$, given by $\\pi_{s,t} = \\Pr(V=1 \\mid S=s, T=t)$.\n-   An assumption that the disease status is missing at random (MAR) given the stratum and test result: $D \\perp \\!\\!\\! \\perp V \\mid (S,T)$.\n-   The reference standard for $D$ is perfect.\n-   Observed counts and verification probabilities from the verified sample:\n    1.  For $(S=\\mathrm{High}, T=+)$: number of verified subjects $n_{\\mathrm{H},+}^{V} = 360$, number of diseased subjects $d_{\\mathrm{H},+} = 300$, and verification probability $\\pi_{\\mathrm{H},+} = 0.9$.\n    2.  For $(S=\\mathrm{High}, T=-)$: $n_{\\mathrm{H},-}^{V} = 240$, $d_{\\mathrm{H},-} = 48$, and $\\pi_{\\mathrm{H},-} = 0.6$.\n    3.  For $(S=\\mathrm{Low}, T=+)$: $n_{\\mathrm{L},+}^{V} = 250$, $d_{\\mathrm{L},+} = 125$, and $\\pi_{\\mathrm{L},+} = 0.5$.\n    4.  For $(S=\\mathrm{Low}, T=-)$: $n_{\\mathrm{L},-}^{V} = 200$, $d_{\\mathrm{L},-} = 10$, and $\\pi_{\\mathrm{L},-} = 0.2$.\n-   Definitions:\n    -   $\\text{sensitivity} = \\Pr(T=+ \\mid D=1)$\n    -   $\\text{specificity} = \\Pr(T=- \\mid D=0)$\n    -   Youden's $J$ index: $J = \\text{sensitivity} + \\text{specificity} - 1$.\n-   Objective: Derive an inverse-probability-weighted (IPW) estimator for sensitivity and specificity to correct for verification bias, and compute the corrected Youden's $J$ index.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It describes a common scenario in diagnostic test evaluation known as verification bias, where the \"gold standard\" test for disease is not applied to all participants. The proposed method for correction, inverse probability weighting (IPW), is a standard and appropriate statistical technique for handling data that is missing at random (MAR), which is an explicitly stated assumption. All necessary data and definitions are provided, and there are no contradictions or ambiguities. The problem is well-posed and objective.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe goal is to estimate sensitivity, $\\text{Se} = \\Pr(T=+ \\mid D=1)$, and specificity, $\\text{Sp} = \\Pr(T=- \\mid D=0)$. These can be expressed using counts from the full (but partially unobserved) study population:\n$$\n\\text{Se} = \\frac{N_{T=+, D=1}}{N_{D=1}} \\quad \\text{and} \\quad \\text{Sp} = \\frac{N_{T=-, D=0}}{N_{D=0}}\n$$\nwhere $N_{t,d}$ is the number of subjects with test result $t$ and disease status $d$, and $N_d$ is the total number of subjects with disease status $d$.\n\nDue to verification bias, we cannot directly use the counts from the verified sample. The missingness assumption, $D \\perp \\!\\!\\! \\perp V \\mid (S,T)$, allows us to use inverse probability weighting to estimate the counts in the full population. The principle of IPW is that each verified subject represents not only themselves but also $\\frac{1}{\\pi_{s,t}} - 1$ other unverified subjects from the same stratum $(s,t)$. Therefore, each verified subject is assigned a weight $w_{s,t} = \\frac{1}{\\pi_{s,t}}$.\n\nWe can estimate the total counts for each of the four cells of the confusion matrix ($T \\times D$) by summing the weighted counts of observed subjects across the risk strata $S$.\n\nLet's denote the number of observed diseased subjects in the verified sample from stratum $(s,t)$ as $d_{s,t}$, and the number of non-diseased as $h_{s,t} = n_{s,t}^V - d_{s,t}$.\n\nThe estimated total number of True Positives ($D=1, T=+$) is:\n$$\n\\hat{N}_{TP} = \\sum_{s \\in \\{\\mathrm{H,L}\\}} \\frac{d_{s,+}}{\\pi_{s,+}} = \\frac{d_{\\mathrm{H},+}}{\\pi_{\\mathrm{H},+}} + \\frac{d_{\\mathrm{L},+}}{\\pi_{\\mathrm{L},+}}\n$$\nThe estimated total number of False Negatives ($D=1, T=-$) is:\n$$\n\\hat{N}_{FN} = \\sum_{s \\in \\{\\mathrm{H,L}\\}} \\frac{d_{s,-}}{\\pi_{s,-}} = \\frac{d_{\\mathrm{H},-}}{\\pi_{\\mathrm{H},-}} + \\frac{d_{\\mathrm{L},-}}{\\pi_{\\mathrm{L},-}}\n$$\nThe estimated total number of False Positives ($D=0, T=+$) is:\n$$\n\\hat{N}_{FP} = \\sum_{s \\in \\{\\mathrm{H,L}\\}} \\frac{n_{s,+}^V - d_{s,+}}{\\pi_{s,+}} = \\frac{n_{\\mathrm{H},+}^V - d_{\\mathrm{H},+}}{\\pi_{\\mathrm{H},+}} + \\frac{n_{\\mathrm{L},+}^V - d_{\\mathrm{L},+}}{\\pi_{\\mathrm{L},+}}\n$$\nThe estimated total number of True Negatives ($D=0, T=-$) is:\n$$\n\\hat{N}_{TN} = \\sum_{s \\in \\{\\mathrm{H,L}\\}} \\frac{n_{s,-}^V - d_{s,-}}{\\pi_{s,-}} = \\frac{n_{\\mathrm{H},-}^V - d_{\\mathrm{H},-}}{\\pi_{\\mathrm{H},-}} + \\frac{n_{\\mathrm{L},-}^V - d_{\\mathrm{L},-}}{\\pi_{\\mathrm{L},-}}\n$$\n\nFrom these, we can estimate the total number of diseased and non-diseased individuals:\n$$\n\\hat{N}_{D=1} = \\hat{N}_{TP} + \\hat{N}_{FN}\n$$\n$$\n\\hat{N}_{D=0} = \\hat{N}_{FP} + \\hat{N}_{TN}\n$$\nThe IPW estimators for sensitivity and specificity are then:\n$$\n\\widehat{\\text{Se}} = \\frac{\\hat{N}_{TP}}{\\hat{N}_{D=1}} \\quad \\text{and} \\quad \\widehat{\\text{Sp}} = \\frac{\\hat{N}_{TN}}{\\hat{N}_{D=0}}\n$$\n\n### Calculation\nFirst, we substitute the given values into the formulas for the estimated counts.\n-   Data for High Risk ($S=\\mathrm{H}$):\n    -   $T=+$: $n_{\\mathrm{H},+}^V=360, d_{\\mathrm{H},+}=300, n_{\\mathrm{H},+}^V-d_{\\mathrm{H},+}=60, \\pi_{\\mathrm{H},+}=0.9$\n    -   $T=-$: $n_{\\mathrm{H},-}^V=240, d_{\\mathrm{H},-}=48, n_{\\mathrm{H},-}^V-d_{\\mathrm{H},-}=192, \\pi_{\\mathrm{H},-}=0.6$\n-   Data for Low Risk ($S=\\mathrm{L}$):\n    -   $T=+$: $n_{\\mathrm{L},+}^V=250, d_{\\mathrm{L},+}=125, n_{\\mathrm{L},+}^V-d_{\\mathrm{L},+}=125, \\pi_{\\mathrm{L},+}=0.5$\n    -   $T=-$: $n_{\\mathrm{L},-}^V=200, d_{\\mathrm{L},-}=10, n_{\\mathrm{L},-}^V-d_{\\mathrm{L},-}=190, \\pi_{\\mathrm{L},-}=0.2$\n\nThe estimated counts are:\n$$\n\\hat{N}_{TP} = \\frac{300}{0.9} + \\frac{125}{0.5} = \\frac{1000}{3} + 250 = \\frac{1000 + 750}{3} = \\frac{1750}{3}\n$$\n$$\n\\hat{N}_{FN} = \\frac{48}{0.6} + \\frac{10}{0.2} = 80 + 50 = 130\n$$\n$$\n\\hat{N}_{FP} = \\frac{60}{0.9} + \\frac{125}{0.5} = \\frac{200}{3} + 250 = \\frac{200 + 750}{3} = \\frac{950}{3}\n$$\n$$\n\\hat{N}_{TN} = \\frac{192}{0.6} + \\frac{190}{0.2} = 320 + 950 = 1270\n$$\n\nNext, we calculate the estimated total number of diseased and non-diseased individuals:\n$$\n\\hat{N}_{D=1} = \\hat{N}_{TP} + \\hat{N}_{FN} = \\frac{1750}{3} + 130 = \\frac{1750 + 390}{3} = \\frac{2140}{3}\n$$\n$$\n\\hat{N}_{D=0} = \\hat{N}_{FP} + \\hat{N}_{TN} = \\frac{950}{3} + 1270 = \\frac{950 + 3810}{3} = \\frac{4760}{3}\n$$\n\nNow, we compute the estimated sensitivity and specificity:\n$$\n\\widehat{\\text{Se}} = \\frac{\\hat{N}_{TP}}{\\hat{N}_{D=1}} = \\frac{1750/3}{2140/3} = \\frac{1750}{2140} = \\frac{175}{214}\n$$\n$$\n\\widehat{\\text{Sp}} = \\frac{\\hat{N}_{TN}}{\\hat{N}_{D=0}} = \\frac{1270}{4760/3} = \\frac{1270 \\times 3}{4760} = \\frac{127 \\times 3}{476} = \\frac{381}{476}\n$$\n\nFinally, we compute the corrected Youden's $J$ index:\n$$\n\\hat{J} = \\widehat{\\text{Se}} + \\widehat{\\text{Sp}} - 1 = \\frac{175}{214} + \\frac{381}{476} - 1\n$$\nTo compute the value, we convert the fractions to decimals:\n$$\n\\widehat{\\text{Se}} \\approx 0.817757009...\n$$\n$$\n\\widehat{\\text{Sp}} \\approx 0.800420168...\n$$\n$$\n\\hat{J} \\approx 0.817757009 + 0.800420168 - 1 = 1.618177177 - 1 = 0.618177177...\n$$\nRounding the result to four significant figures gives $0.6182$.", "answer": "$$\n\\boxed{0.6182}\n$$", "id": "4319561"}, {"introduction": "Developing a powerful biomarker often involves building a statistical model that combines multiple features to predict a clinical outcome, a process that risks \"overfitting.\" An overfit model may show impressive performance on the training data but fail to generalize to new patients, rendering it clinically useless. This hands-on programming exercise guides you through a cornerstone of modern model validation: using bootstrap resampling to estimate and correct for optimistic bias in the Area Under the Curve ($AUC$). By implementing this procedure, you will gain direct experience with a computationally intensive method that provides a more honest assessment of a biomarker model's performance, a critical step towards developing robust tools for precision medicine [@problem_id:4319570].", "problem": "You are given a formal task grounded in biomarker model evaluation within precision medicine and genomic diagnostics. In this setting, a biomarker can serve different roles: a diagnostic biomarker distinguishes disease from non-disease at a point in time, a prognostic biomarker stratifies baseline risk independent of treatment, and a predictive biomarker indicates differential benefit from a specific treatment. Classification models for diagnostic or predictive biomarkers often rely on probabilistic predictions of a binary clinical state. A rigorous measure of ranking performance for such models is the area under the receiver operating characteristic curve (AUC), which can be expressed as the probability that a randomly chosen positive case obtains a higher model score than a randomly chosen negative case. Apparent (in-sample) performance measured on the training data is susceptible to overfitting. Resampling-based methods, such as bootstrap resampling, can be used to estimate and correct the optimism (the expected inflation in apparent performance relative to performance on new data).\n\nFundamental base:\n- Let $X \\in \\mathbb{R}^{n \\times p}$ denote a design matrix of $n$ samples and $p$ features. Let $y \\in \\{0,1\\}^n$ denote binary outcomes representing the presence ($1$) or absence ($0$) of the clinical state associated with the biomarker.\n- Consider a logistic regression model with parameters $\\beta \\in \\mathbb{R}^{p+1}$ (including an intercept), producing predicted probabilities $\\hat{p}_i = s\\!\\left(\\beta_0 + \\sum_{j=1}^{p} x_{ij}\\beta_j \\right)$ for sample $i$, where $s(z) = 1/(1+e^{-z})$ is the logistic function. Parameters are estimated by maximizing the regularized likelihood or equivalently minimizing the regularized negative log-likelihood\n$$\n\\mathcal{L}(\\beta; X, y, \\lambda) = -\\sum_{i=1}^{n} \\left[ y_i \\log \\hat{p}_i + (1-y_i)\\log(1-\\hat{p}_i) \\right] + \\frac{\\lambda}{2}\\sum_{j=1}^{p} \\beta_j^2,\n$$\nwhere $\\lambda \\ge 0$ is an $L_2$ penalty weight applied to non-intercept coefficients to mitigate instability and reduce overfitting.\n- The area under the receiver operating characteristic curve (AUC) for a score vector $r \\in \\mathbb{R}^n$ is defined as the probability that a randomly chosen positive sample has a higher score than a randomly chosen negative sample, with ties contributing half weight. This can be computed via rank-based formulations derived from the Wilcoxon-Mann-Whitney statistic.\n- The bootstrap principle approximates expectations under the sampling distribution by resampling with replacement from the empirical distribution defined by the observed data. Bootstrap-based optimism estimation proceeds by training a model on each bootstrap sample and comparing its performance on the bootstrap sample versus the original sample.\n\nTask:\nImplement a program that, for each test case, performs the following steps:\n1. Generate synthetic data $(X,y)$ as follows. For given $n$, $p$, coefficient vector $\\beta^* \\in \\mathbb{R}^{p}$, and intercept $\\beta_0^* \\in \\mathbb{R}$, draw $X$ with independent standard normal entries. Compute linear predictors $z_i = \\beta_0^* + \\sum_{j=1}^{p} x_{ij}\\beta_j^*$ and sample outcomes $y_i \\sim \\text{Bernoulli}(s(z_i))$ independently for $i=1,\\dots,n$, where $s(\\cdot)$ is the logistic function. Use the specified random seed to ensure reproducibility.\n2. Fit a penalized logistic regression model by minimizing $\\mathcal{L}(\\beta; X, y, \\lambda)$ with respect to $\\beta$ using a numerical optimizer. The penalty term must exclude the intercept.\n3. Compute the apparent AUC on the original data by using the fitted model to produce predicted probabilities on $X$ and then computing the AUC against $y$ using a rank-based method that properly handles ties. If there are no positive or no negative samples, define the AUC as not-a-number.\n4. Perform bootstrap resampling $B$ times:\n   - For each bootstrap replicate $b \\in \\{1,\\dots,B\\}$, draw a bootstrap sample of indices of size $n$ with replacement from $\\{1,\\dots,n\\}$, yielding $(X^{(b)}, y^{(b)})$.\n   - Fit the penalized logistic regression on $(X^{(b)}, y^{(b)})$ to obtain parameters $\\hat{\\beta}^{(b)}$.\n   - Compute $AUC_{\\text{boot}}^{(b)}$ on $(X^{(b)}, y^{(b)})$ using predictions from $\\hat{\\beta}^{(b)}$.\n   - Compute $AUC_{\\text{orig}}^{(b)}$ on $(X, y)$ using predictions from $\\hat{\\beta}^{(b)}$.\n   - Define the optimism for replicate $b$ as $AUC_{\\text{boot}}^{(b)} - AUC_{\\text{orig}}^{(b)}$. If either AUC is not-a-number due to class degeneracy in the bootstrap sample, exclude that replicate from the optimism average.\n5. Compute the optimism-corrected AUC as the apparent AUC minus the average optimism computed over the valid bootstrap replicates.\n6. Return the optimism-corrected AUC for each test case as a decimal number rounded to $6$ decimal places.\n\nYour program must implement the above procedure exactly and handle tie-aware AUC computation based on ranks. It must use a penalized logistic regression with $L_2$ penalty weight $\\lambda$ applied only to non-intercept coefficients. If all bootstrap replicates are invalid due to class degeneracy, return the not-a-number indicator for that test case.\n\nTest suite:\nUse the following four test cases, each specified by a tuple $(\\text{seed}, n, p, \\beta^*, \\beta_0^*, \\lambda, B)$:\n- Case $1$: $(42, 120, 3, [0.6, -0.4, 0.8], -0.2, 1.0, 200)$.\n- Case $2$: $(314, 100, 2, [2.5, -2.0], 0.0, 2.0, 150)$.\n- Case $3$: $(7, 35, 4, [0.2, 0.1, -0.1, 0.0], -0.1, 0.5, 100)$.\n- Case $4$: $(2021, 200, 5, [0.3, 0.0, -0.2, 0.1, 0.05], -2.0, 1.0, 200)$.\n\nAnswer specification:\n- For each test case, compute the optimism-corrected AUC as described above and round to $6$ decimal places.\n- Express each final answer as a decimal (not as a percentage).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[0.731245,0.892310,0.501234,0.763210]$). Do not print any other text.", "solution": "We begin by connecting the biomarker context to a formal statistical learning problem. A diagnostic biomarker distinguishes cases from controls, and a predictive biomarker indicates treatment benefit; both are commonly evaluated through binary classification performance. The area under the receiver operating characteristic curve (AUC) quantifies the model’s ranking ability: given scores $r \\in \\mathbb{R}^n$ and labels $y \\in \\{0,1\\}^n$, it equals the probability that a randomly chosen positive’s score exceeds a randomly chosen negative’s score, with ties contributing half weight. This can be computed via rank-based statistics derived from the Wilcoxon-Mann-Whitney formulation.\n\nOverfitting arises when a model captures idiosyncratic noise in the training data, inflating apparent performance (the performance measured on the same data used to train). The bootstrap principle provides a way to estimate and correct this inflation (optimism) by approximating expectations under the empirical distribution via resampling.\n\nPrinciple-based derivation and algorithmic design:\n1. Model and estimation. For a binary outcome, we model the conditional probability via the logistic function $s(z) = 1/(1+e^{-z})$. For sample $i$, with features $x_i \\in \\mathbb{R}^p$, the model is $\\hat{p}_i = s\\!\\left(\\beta_0 + \\sum_{j=1}^p x_{ij}\\beta_j\\right)$. Parameters $\\beta = (\\beta_0,\\ldots,\\beta_p)$ are estimated by minimizing the regularized negative log-likelihood\n   $$\n   \\mathcal{L}(\\beta; X, y, \\lambda) = -\\sum_{i=1}^n \\left[ y_i \\log \\hat{p}_i + (1-y_i)\\log(1-\\hat{p}_i) \\right] + \\frac{\\lambda}{2}\\sum_{j=1}^p \\beta_j^2,\n   $$\n   which includes an $L_2$ penalty on non-intercept coefficients $\\beta_j$ for $j \\ge 1$. This penalty reduces variance in parameter estimates, stabilizing learning especially when signal is strong or data are limited, thereby directly combating overfitting at the estimation stage.\n\n   The gradient of $\\mathcal{L}$ with respect to $\\beta$ is derived by differentiating the logistic loss. Writing $\\hat{p} = s(X\\tilde{\\beta})$, where $X$ is augmented with a column of ones to encode the intercept and $\\tilde{\\beta}$ is the full parameter vector, the gradient is\n   $$\n   \\nabla \\mathcal{L}(\\beta) = X^\\top(\\hat{p} - y) + \\lambda \\cdot (0, \\beta_1, \\ldots, \\beta_p)^\\top,\n   $$\n   where the intercept’s gradient has no penalty term. This enables efficient optimization via quasi-Newton methods such as Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS).\n\n2. AUC computation by ranks. Let $r \\in \\mathbb{R}^n$ be predicted probabilities. Denote $n_+$ and $n_-$ as the counts of positives and negatives. Let $\\text{rank}(r)$ provide average ranks accounting for ties. Then\n   $$\n   \\text{AUC}(r, y) = \\frac{\\sum_{i: y_i = 1} \\text{rank}(r_i) - \\frac{n_+(n_+ + 1)}{2}}{n_+ n_-},\n   $$\n   provided $n_+ \\ge 1$ and $n_- \\ge 1$, otherwise it is undefined (not-a-number). This expression is algebraically equivalent to the empirical estimate of $\\mathbb{P}(r^+ > r^-)$ plus half the tie probability.\n\n3. Bootstrap optimism correction. The bootstrap approximates expectations under the sampling distribution by resampling with replacement from the observed data. For optimism estimation, we:\n   - Draw bootstrap samples $(X^{(b)}, y^{(b)})$ by sampling $n$ indices with replacement from $\\{1,\\ldots,n\\}$.\n   - Train the model on each bootstrap sample to obtain $\\hat{\\beta}^{(b)}$.\n   - Compute $AUC_{\\text{boot}}^{(b)}$ on the bootstrap sample and $AUC_{\\text{orig}}^{(b)}$ on the original sample using predictions from $\\hat{\\beta}^{(b)}$.\n   - Define optimism per replicate $b$ as $AUC_{\\text{boot}}^{(b)} - AUC_{\\text{orig}}^{(b)}$, excluding replicates where either AUC is undefined.\n   Averaging optimism across replicates yields an estimate of the expected inflation in apparent performance. The apparent AUC is computed by training on the original sample and evaluating on the same sample. Subtracting the average optimism from the apparent AUC yields the optimism-corrected AUC:\n   $$\n   \\text{AUC}_{\\text{corrected}} \\approx \\text{AUC}_{\\text{apparent}} - \\mathbb{E}[\\text{optimism}],\n   $$\n   where the expectation is approximated by the bootstrap average. This correction mitigates overfitting by quantitatively removing the portion of apparent performance that is likely due to fitting noise rather than signal, thereby better approximating out-of-sample performance expected for future data.\n\n4. Data generation for testability. For each test case, generate $X$ with independent standard normal entries using the provided seed, compute $z_i = \\beta_0^* + \\sum_{j} x_{ij} \\beta_j^*$, and sample $y_i \\sim \\text{Bernoulli}(s(z_i))$. This construction is scientifically realistic, consistent with generalized linear models, and enables controlled evaluation across different signal strengths and class imbalances.\n\nAlgorithm summary:\n- For each test case $(\\text{seed}, n, p, \\beta^*, \\beta_0^*, \\lambda, B)$:\n  1. Generate $(X, y)$; fit the penalized logistic regression to obtain $\\hat{\\beta}$; compute $\\text{AUC}_{\\text{apparent}}$.\n  2. For $b = 1,\\ldots,B$: bootstrap resample indices; fit on $(X^{(b)}, y^{(b)})$; compute $AUC_{\\text{boot}}^{(b)}$ and $AUC_{\\text{orig}}^{(b)}$; record optimism if both AUCs are defined.\n  3. Compute the mean optimism across valid replicates and subtract it from $\\text{AUC}_{\\text{apparent}}$ to yield $\\text{AUC}_{\\text{corrected}}$.\n  4. Round $\\text{AUC}_{\\text{corrected}}$ to $6$ decimal places.\n\nThe final program adheres to these principles, producing a single line containing the optimism-corrected AUC values for the specified test suite. The optimism correction directly addresses overfitting in biomarker model development by reducing bias in performance estimates, thus enhancing reliability of diagnostic or predictive biomarker claims for precision medicine.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import rankdata\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef fit_logistic_l2(X, y, lam):\n    \"\"\"\n    Fit penalized logistic regression with L2 penalty on non-intercept coefficients.\n    X: shape (n, p), features (no intercept column).\n    y: shape (n,), binary labels 0/1.\n    lam: float, L2 penalty weight.\n    Returns beta of shape (p+1,), including intercept as beta[0].\n    \"\"\"\n    n, p = X.shape\n    # Augment X with intercept column\n    X_aug = np.hstack([np.ones((n, 1)), X])\n\n    # Objective: regularized negative log-likelihood\n    def nll(beta):\n        z = X_aug @ beta\n        p_hat = sigmoid(z)\n        # Numerical stability: clip p_hat to avoid log(0)\n        eps = 1e-12\n        p_hat = np.clip(p_hat, eps, 1 - eps)\n        # Penalize non-intercept coefficients\n        penalty = 0.5 * lam * np.sum(beta[1:] ** 2)\n        return -np.sum(y * np.log(p_hat) + (1 - y) * np.log(1 - p_hat)) + penalty\n\n    # Gradient of the objective\n    def grad(beta):\n        z = X_aug @ beta\n        p_hat = sigmoid(z)\n        # Gradient from likelihood\n        g = X_aug.T @ (p_hat - y)\n        # Add penalty gradient (excluding intercept)\n        g[1:] += lam * beta[1:]\n        return g\n\n    # Initialize beta to zeros\n    beta0 = np.zeros(p + 1, dtype=float)\n    # Optimize using L-BFGS-B\n    res = minimize(nll, beta0, jac=grad, method=\"L-BFGS-B\")\n    return res.x\n\ndef auc_rank(scores, y):\n    \"\"\"\n    Compute AUC using rank-based method that handles ties.\n    Returns np.nan if there are no positives or no negatives.\n    \"\"\"\n    y = np.asarray(y)\n    scores = np.asarray(scores)\n    pos_mask = (y == 1)\n    neg_mask = (y == 0)\n    n_pos = np.sum(pos_mask)\n    n_neg = np.sum(neg_mask)\n    if n_pos == 0 or n_neg == 0:\n        return np.nan\n    ranks = rankdata(scores, method='average')\n    sum_ranks_pos = np.sum(ranks[pos_mask])\n    auc = (sum_ranks_pos - n_pos * (n_pos + 1) / 2.0) / (n_pos * n_neg)\n    return float(auc)\n\ndef bootstrap_optimism_corrected_auc(X, y, lam, B, rng):\n    \"\"\"\n    Compute optimism-corrected AUC via bootstrap resampling.\n    X: (n, p), y: (n,), lam: L2 penalty weight, B: number of bootstraps.\n    rng: numpy Generator for reproducibility.\n    \"\"\"\n    # Apparent fit\n    beta_app = fit_logistic_l2(X, y, lam)\n    p_app = sigmoid(np.hstack([np.ones((X.shape[0], 1)), X]) @ beta_app)\n    auc_app = auc_rank(p_app, y)\n\n    # Bootstrap optimism\n    n = X.shape[0]\n    optimisms = []\n    for b in range(B):\n        idx = rng.integers(0, n, size=n)\n        Xb = X[idx]\n        yb = y[idx]\n        # Fit on bootstrap sample\n        beta_b = fit_logistic_l2(Xb, yb, lam)\n        # AUC on bootstrap sample\n        pb_boot = sigmoid(np.hstack([np.ones((Xb.shape[0], 1)), Xb]) @ beta_b)\n        auc_boot = auc_rank(pb_boot, yb)\n        # AUC on original sample using bootstrap-fitted model\n        pb_orig = sigmoid(np.hstack([np.ones((X.shape[0], 1)), X]) @ beta_b)\n        auc_orig = auc_rank(pb_orig, y)\n        if not (np.isnan(auc_boot) or np.isnan(auc_orig)):\n            optimisms.append(auc_boot - auc_orig)\n    if len(optimisms) == 0 or np.isnan(auc_app):\n        return np.nan\n    mean_optimism = float(np.mean(optimisms))\n    corrected = auc_app - mean_optimism\n    # Bound within [0,1] for numerical sanity (AUC is in [0,1])\n    corrected = min(max(corrected, 0.0), 1.0)\n    return corrected\n\ndef generate_data(seed, n, p, beta_star, beta0_star):\n    \"\"\"\n    Generate synthetic data X, y with standard normal features and logistic Bernoulli responses.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    X = rng.normal(loc=0.0, scale=1.0, size=(n, p))\n    z = beta0_star + X @ np.asarray(beta_star)\n    prob = sigmoid(z)\n    y = rng.binomial(1, prob, size=n).astype(int)\n    return X, y, rng\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (seed, n, p, beta_star, beta0_star, lambda, B)\n    test_cases = [\n        (42, 120, 3, [0.6, -0.4, 0.8], -0.2, 1.0, 200),\n        (314, 100, 2, [2.5, -2.0], 0.0, 2.0, 150),\n        (7, 35, 4, [0.2, 0.1, -0.1, 0.0], -0.1, 0.5, 100),\n        (2021, 200, 5, [0.3, 0.0, -0.2, 0.1, 0.05], -2.0, 1.0, 200),\n    ]\n\n    results = []\n    for seed, n, p, beta_star, beta0_star, lam, B in test_cases:\n        X, y, rng = generate_data(seed, n, p, beta_star, beta0_star)\n        corrected_auc = bootstrap_optimism_corrected_auc(X, y, lam, B, rng)\n        # Round to 6 decimals as required\n        if np.isnan(corrected_auc):\n            results.append(\"nan\")\n        else:\n            results.append(f\"{np.round(corrected_auc, 6):.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4319570"}]}