{"hands_on_practices": [{"introduction": "A cornerstone of evidence-based practice is the ability to objectively measure clinical outcomes. This exercise provides a practical application of this principle by focusing on the Yale-Brown Obsessive Compulsive Scale (Y-BOCS), the gold standard for assessing OCD severity. By working with hypothetical baseline data, you will learn to apply established psychometric criteria to define a threshold for clinically meaningful change, a fundamental skill for evaluating treatment response in both clinical and research settings. [@problem_id:4710919]", "problem": "A patient with Obsessive-Compulsive Disorder (OCD) is beginning Exposure and Response Prevention (ERP), a behavioral intervention grounded in the principles of classical and operant conditioning where exposure to feared stimuli is paired with prevention of compulsive responses to extinguish anxiety-driven reinforcement. The Yale–Brown Obsessive Compulsive Scale (Y-BOCS) is used to quantify symptom severity before and after ERP. The Y-BOCS is a clinician-rated instrument with 10 items scored from 0 to 4, yielding a total score from 0 to 40. It has two subscales: the Obsessions subscale (items 1–5; score range 0–20) and the Compulsions subscale (items 6–10; score range 0–20). The total score is the sum of the subscales.\n\nA clinician wishes to define a clinically meaningful change threshold as the smallest integer decrease in the total Y-BOCS score that simultaneously satisfies the following criteria, both grounded in proportion-of-baseline reduction standards common in clinical psychometrics: (i) at least a 35% reduction in the total Y-BOCS score, and (ii) at least a 10% reduction in each subscale score. Baseline scores are Obsessions subscale $O_{0} = 13$, Compulsions subscale $C_{0} = 17$, total $T_{0} = O_{0} + C_{0} = 30$.\n\nUsing only fundamental measurement principles (discrete item scoring, total equals the sum of subscales, and proportional reduction defined relative to baseline), derive from first principles the minimal integer threshold for the total score reduction that ensures both criteria can be met simultaneously by some allocation of reductions across the two subscales. Express your final answer as a whole number of points. No rounding instruction is needed beyond the requirement that the threshold is an integer number of points because item scores and subscale totals are integers.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the practice of clinical psychometrics, well-posed with clear quantitative criteria and sufficient data, and objective in its formulation. We can proceed with a formal solution.\n\nLet the baseline scores for the Obsessions subscale, Compulsions subscale, and total Y-BOCS score be denoted by $O_{0}$, $C_{0}$, and $T_{0}$ respectively. Let the post-treatment scores be $O_{1}$, $C_{1}$, and $T_{1}$. The reductions in scores are defined as $\\Delta O = O_{0} - O_{1}$, $\\Delta C = C_{0} - C_{1}$, and $\\Delta T = T_{0} - T_{1}$.\nThe problem states that the total score is the sum of the subscale scores, so $T_0 = O_0 + C_0$ and $T_1 = O_1 + C_1$. This implies that the total reduction is the sum of the subscale reductions:\n$$\n\\Delta T = (O_{0} - O_{1}) + (C_{0} - C_{1}) = \\Delta O + \\Delta C\n$$\nThe Y-BOCS scores are integer-valued, therefore the reductions $\\Delta O$, $\\Delta C$, and $\\Delta T$ must also be integers.\n\nThe given baseline scores are:\n$O_{0} = 13$\n$C_{0} = 17$\n$T_{0} = O_{0} + C_{0} = 13 + 17 = 30$\n\nWe are tasked with finding the smallest integer decrease in the total score, $\\Delta T$, that satisfies two criteria simultaneously. Let us formalize these criteria as mathematical inequalities.\n\nCriterion (i): At least a 35% reduction in the total Y-BOCS score.\nThis can be written as:\n$$\n\\frac{\\Delta T}{T_{0}} \\ge 0.35\n$$\nSubstituting the value $T_{0} = 30$:\n$$\n\\frac{\\Delta T}{30} \\ge 0.35\n$$\n$$\n\\Delta T \\ge 0.35 \\times 30\n$$\n$$\n\\Delta T \\ge 10.5\n$$\nSince $\\Delta T$ must be an integer, the smallest integer value for $\\Delta T$ that satisfies this criterion is 11. Thus, from criterion (i), we must have:\n$$\n\\Delta T \\ge 11\n$$\n\nCriterion (ii): At least a 10% reduction in each subscale score.\nThis criterion consists of two separate conditions.\n\nFor the Obsessions subscale:\n$$\n\\frac{\\Delta O}{O_{0}} \\ge 0.10\n$$\nSubstituting the value $O_{0} = 13$:\n$$\n\\frac{\\Delta O}{13} \\ge 0.10\n$$\n$$\n\\Delta O \\ge 1.3\n$$\nSince $\\Delta O$ must be an integer, the smallest integer value for $\\Delta O$ that satisfies this condition is 2.\n$$\n\\Delta O \\ge 2\n$$\n\nFor the Compulsions subscale:\n$$\n\\frac{\\Delta C}{C_{0}} \\ge 0.10\n$$\nSubstituting the value $C_{0} = 17$:\n$$\n\\frac{\\Delta C}{17} \\ge 0.10\n$$\n$$\n\\Delta C \\ge 1.7\n$$\nSince $\\Delta C$ must be an integer, the smallest integer value for $\\Delta C$ that satisfies this condition is 2.\n$$\n\\Delta C \\ge 2\n$$\n\nThe problem asks for the minimal integer threshold for the total score reduction, $\\Delta T$, such that it is possible to find an allocation of reductions across the subscales ($\\Delta O$ and $\\Delta C$) that meets all criteria. This means we are seeking the minimum integer value of $\\Delta T$ for which there exist integers $\\Delta O$ and $\\Delta C$ that satisfy the following system of conditions:\n1. $\\Delta T \\ge 11$\n2. $\\Delta O \\ge 2$\n3. $\\Delta C \\ge 2$\n4. $\\Delta T = \\Delta O + \\Delta C$\n\nFrom condition (1), the minimum possible integer value for $\\Delta T$ is $11$. We must now verify if a total reduction of $\\Delta T = 11$ can be achieved while also satisfying conditions (2), (3), and (4).\nLet's set $\\Delta T = 11$. We need to check if there exist integers $\\Delta O$ and $\\Delta C$ such that:\n$$\n\\Delta O + \\Delta C = 11 \\quad \\text{with} \\quad \\Delta O \\ge 2 \\quad \\text{and} \\quad \\Delta C \\ge 2\n$$\nWe can test this by choosing a value for one of the subscale reductions and solving for the other. Let's choose the minimum possible value for $\\Delta O$, which is $\\Delta O = 2$. This would require $\\Delta C = 11 - 2 = 9$. The pair of reductions $(\\Delta O, \\Delta C) = (2, 9)$ satisfies the conditions: both are integers, $\\Delta O = 2 \\ge 2$, and $\\Delta C = 9 \\ge 2$.\n\nFurthermore, these reductions must be clinically possible. The reduction in a score cannot exceed the baseline score itself.\nFor $\\Delta O = 2$, we must have $O_1 = O_0 - \\Delta O = 13 - 2 = 11 \\ge 0$, which is valid.\nFor $\\Delta C = 9$, we must have $C_1 = C_0 - \\Delta C = 17 - 9 = 8 \\ge 0$, which is valid.\n\nSince we have found a valid allocation of reductions $(\\Delta O = 2, \\Delta C = 9)$ that sums to a total reduction of $\\Delta T = 11$ while satisfying all criteria, a total reduction of $11$ points is achievable.\nAny integer value for $\\Delta T$ less than $11$, such as $\\Delta T = 10$, would violate criterion (i) ($\\Delta T \\ge 10.5$).\nTherefore, the minimal integer threshold for the total score reduction that ensures all criteria can be met is $11$.", "answer": "$$\n\\boxed{11}\n$$", "id": "4710919"}, {"introduction": "Moving from the individual to the population, this practice delves into the evaluation of treatment efficacy using data from a Randomized Controlled Trial (RCT). You will calculate the Number Needed to Treat (NNT), a key metric in evidence-based medicine that translates trial results into a clinically intuitive measure of impact. This exercise will strengthen your ability to critically appraise clinical research and understand how the effectiveness of Exposure and Response Prevention is quantified and compared to control conditions. [@problem_id:4710972]", "problem": "A Randomized Controlled Trial (RCT) compares Exposure and Response Prevention (ERP) to a waitlist control in adults with Obsessive-Compulsive Disorder (OCD). Remission at 12 weeks is defined a priori using standard clinician-rated criteria. The trial uses an Intention-To-Treat (ITT) principle that counts all randomized participants, classifying dropouts as non-remitters. The ERP arm randomizes $n_{\\text{ERP}} = 120$ participants, of whom 46 meet remission criteria and 14 drop out. The waitlist arm randomizes $n_{\\text{WL}} = 120$ participants, of whom 10 meet remission criteria and 8 drop out. Using the ITT principle and standard definitions of risk, absolute risk reduction, and number needed to treat, compute the Number Needed to Treat (NNT) for ERP relative to waitlist, treating remission as the beneficial outcome. Then, provide a concise clinical interpretation of the magnitude of this NNT in terms of remission. Express the NNT as a pure number with no units and round your final answer to four significant figures.", "solution": "The problem will first be validated for scientific soundness, clarity, and completeness.\n\n### Step 1: Extract Givens\nThe explicit data and conditions provided in the problem statement are:\n- **Study Design:** A Randomized Controlled Trial (RCT).\n- **Intervention Arm:** Exposure and Response Prevention (ERP).\n- **Control Arm:** Waitlist control (WL).\n- **Population:** Adults with Obsessive-Compulsive Disorder (OCD).\n- **Outcome:** Remission at 12 weeks, defined a priori.\n- **Analysis Principle:** Intention-To-Treat (ITT).\n- **ITT Rule for Dropouts:** Dropouts are classified as non-remitters.\n- **ERP Arm Sample Size:** $n_{\\text{ERP}} = 120$ participants randomized.\n- **ERP Arm Outcomes:** 46 participants met remission criteria; 14 participants dropped out.\n- **Waitlist Arm Sample Size:** $n_{\\text{WL}} = 120$ participants randomized.\n- **Waitlist Arm Outcomes:** 10 participants met remission criteria; 8 participants dropped out.\n- **Task:** Compute the Number Needed to Treat (NNT).\n- **Rounding:** The final numerical answer for NNT must be rounded to four significant figures.\n- **Interpretation:** Provide a concise clinical interpretation of the NNT.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the required criteria:\n- **Scientifically Grounded:** The problem is firmly grounded in clinical epidemiology and biostatistics. The concepts of RCTs, ERP for OCD, Intention-to-Treat analysis, Absolute Risk Reduction (ARR), and Number Needed to Treat (NNT) are all standard, well-defined, and central to evidence-based medicine.\n- **Well-Posed:** The problem is well-posed. It provides all necessary data ($n_{\\text{ERP}}$, $n_{\\text{WL}}$, number of remitters in each arm) and a clear definition of the analysis principle (ITT with dropouts as non-remitters) required to compute a unique NNT value.\n- **Objective:** The language is precise and objective, describing a standard quantitative analysis task without subjective or opinion-based elements.\n- **Completeness and Consistency:** The data are complete and consistent. Under the ITT principle, the denominator for rate calculations is the total number of randomized participants. The number of events (remissions) is given. The information about dropouts clarifies that they are counted in the denominator and are considered non-events (non-remitters), which is a common and conservative application of the ITT principle. The problem setup is internally consistent and lacks ambiguity.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid** as it is scientifically sound, well-posed, objective, and self-contained. The solution will proceed.\n\n### Solution\nThe objective is to calculate the Number Needed to Treat (NNT) for ERP compared to a waitlist control, based on remission rates at 12 weeks. The NNT is a measure of the effectiveness of a healthcare intervention.\n\nThe NNT is defined as the reciprocal of the Absolute Risk Reduction (ARR):\n$$ NNT = \\frac{1}{ARR} $$\nThe ARR is the absolute difference in the event rates between the experimental group and the control group. For a beneficial outcome such as remission, the ARR is calculated as:\n$$ ARR = EER - CER $$\nwhere $EER$ is the Experimental Event Rate (the rate of remission in the ERP group) and $CER$ is the Control Event Rate (the rate of remission in the waitlist group).\n\nThe analysis must adhere to the Intention-To-Treat (ITT) principle. This dictates that all participants who were randomized are included in the statistical analysis and analyzed in the groups to which they were originally assigned, regardless of their adherence to the entry criteria, the treatment they actually received, or their withdrawal from the study. The problem specifies that dropouts are counted as non-remitters.\n\nFirst, we calculate the Experimental Event Rate ($EER$) for the ERP arm. The total number of participants randomized to the ERP arm is $n_{\\text{ERP}} = 120$. The number of participants who achieved remission (the \"event\") is given as 46.\n$$ EER = \\frac{\\text{Number of remitters in ERP arm}}{\\text{Total participants randomized to ERP arm}} = \\frac{46}{120} $$\n\nNext, we calculate the Control Event Rate ($CER$) for the waitlist arm. The total number of participants randomized to the waitlist arm is $n_{\\text{WL}} = 120$. The number of participants who achieved remission is given as 10.\n$$ CER = \\frac{\\text{Number of remitters in waitlist arm}}{\\text{Total participants randomized to waitlist arm}} = \\frac{10}{120} $$\n\nNow, we compute the Absolute Risk Reduction (ARR):\n$$ ARR = EER - CER = \\frac{46}{120} - \\frac{10}{120} = \\frac{46 - 10}{120} = \\frac{36}{120} $$\nThe ARR can be simplified:\n$$ ARR = \\frac{36}{120} = \\frac{3 \\times 12}{10 \\times 12} = \\frac{3}{10} = 0.3 $$\n\nFinally, we calculate the Number Needed to Treat (NNT) as the reciprocal of the ARR. An NNT value is conventionally rounded up to the next whole number in clinical practice for decision-making, but here we follow the instruction to round to four significant figures.\n$$ NNT = \\frac{1}{ARR} = \\frac{1}{0.3} = \\frac{1}{3/10} = \\frac{10}{3} $$\nAs a decimal, this is $3.3333...$. Rounding to four significant figures gives:\n$$ NNT \\approx 3.333 $$\n\nThe clinical interpretation of this NNT value is as follows: An NNT of $3.333$ signifies that, on average, for every $3.333$ individuals with OCD treated with ERP for 12 weeks, one additional person will achieve remission who would not have achieved remission if they had been in the waitlist control group. This indicates a highly effective treatment, as a low NNT corresponds to high treatment efficacy. In practical terms, treating approximately 3 to 4 patients with ERP is associated with one additional successful outcome (remission) beyond what would be expected from spontaneous remission or non-specific factors in a waitlist condition.", "answer": "$$\n\\boxed{3.333}\n$$", "id": "4710972"}, {"introduction": "This final practice zooms into the micro-dynamics of a single exposure session, modeling the core process of habituation. You will use patient-reported Subjective Units of Distress Scale (SUDS) ratings to estimate the rate of anxiety reduction by fitting an exponential decay model, $S_t = S_0 \\exp(-\\beta t)$. This exercise provides hands-on experience with data linearization and parameter estimation, bridging the theoretical concept of extinction learning with the practical task of quantifying a patient's progress within a therapy session. [@problem_id:4710967]", "problem": "You are modeling within-session distress reduction during Exposure and Response Prevention (ERP) using the Subjective Units of Distress Scale (SUDS), defined as a patient-reported rating on a bounded numerical scale. Assume that, during a single ERP exposure trial, distress follows an exponential decay function over time. Let $S_t$ denote the observed SUDS rating at time $t$, and suppose the latent trajectory is given by the exponential decay law $S_t = S_0 e^{-\\beta t}$, where $S_0$ is the initial SUDS level and $\\beta$ is the decay rate. Time $t$ is measured in minutes; report any estimated decay rate in units of $1/\\text{minute}$. Your goal is to estimate $\\beta$ from time-stamped ratings under realistic measurement noise and possible floor effects (for example, reports of $S_t = 0$ near the end of a trial).\n\nBase your method on first principles by starting from: (i) the definition of the exponential function and its algebraic properties, and (ii) the principle of least squares, which estimates parameters by minimizing the sum of squared residuals between model-predicted and observed values. You must reason from these bases to construct an algorithm that produces a stable estimate of $\\beta$ given time-stamped SUDS data. Ensure your method is well-defined when some $S_t$ observations are zero, and when early measurements (including the one at $t=0$) may be missing. Do not assume any specific distributional shortcuts beyond what can be justified by these bases. If you use any transformation, clearly justify it on the basis of algebra and the least squares principle.\n\nInput for this problem is embedded directly in the program; no user input is required. Use the following test suite of time-stamped SUDS observations, each test case being a list of pairs $(t_i, s_i)$ with $t_i$ in minutes and $s_i$ an integer in the range $[0, 100]$:\n\n- Case A (typical decay with moderate noise): $[(0, 80), (2, 65), (4, 51), (6, 40), (8, 33), (10, 25), (12, 19), (15, 13)]$.\n- Case B (near-constant distress; boundary case close to $\\beta = 0$): $[(0, 60), (3, 61), (6, 59), (9, 60), (12, 58), (15, 60)]$.\n- Case C (strong decay with floor effects including zeros): $[(0, 40), (2, 22), (4, 12), (6, 7), (8, 3), (10, 0), (12, 0)]$.\n- Case D (no measurement at $t=0$, moderate decay): $[(5, 61), (10, 39), (15, 28), (20, 18), (25, 12), (30, 8)]$.\n- Case E (slow decay with modest noise): $[(0, 70), (5, 65), (10, 59), (15, 53), (20, 47), (25, 43), (30, 39)]$.\n\nYour program must, for each case, estimate the decay rate $\\beta$ in $1/\\text{minute}$, using only the information above. Your algorithm must be deterministic when applied to the provided data. Express each final estimate as a floating-point number rounded to four decimal places.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases A through E (for example, $[b_A,b_B,b_C,b_D,b_E]$). There must be exactly one printed line.\n\nNotes and constraints:\n- Angles are not involved; no angle units are required.\n- All answers are real-valued floats; no percentages may be reported.\n- Scientific realism: Do not violate the bounded nature of SUDS ratings, and ensure your estimation approach remains meaningful when $S_t = 0$ for some observations. Your method should reduce to a standard least squares estimator when all $S_t > 0$.", "solution": "The problem requires the estimation of the exponential decay rate, $\\beta$, from a set of time-stamped Subjective Units of Distress (SUDS) ratings, $(t_i, s_i)$. The underlying model for the distress level, $S_t$, at time $t$ is given by $S_t = S_0 e^{-\\beta t}$, where $S_0$ is the initial distress and $\\beta$ is the decay rate in units of $1/\\text{minute}$. The estimation must be derived from first principles, namely the algebraic properties of the exponential function and the principle of least squares. The method must also be robust to measurement noise, floor effects (i.e., observations where $s_i=0$), and missing data at $t=0$.\n\nThe governing equation, $S_t = S_0 e^{-\\beta t}$, is a non-linear function of the parameters $\\beta$ and $S_0$. Directly applying the principle of least squares would involve minimizing the sum of squared residuals, $SSE = \\sum_i (s_i - S_0 e^{-\\beta t_i})^2$, which requires non-linear optimization methods. However, the problem statement directs us to use the algebraic properties of the exponential function to simplify the estimation.\n\nWe can linearize the model by taking the natural logarithm of both sides. Based on the properties $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(e^x) = x$, we have:\n$$ \\ln(S_t) = \\ln(S_0 e^{-\\beta t}) = \\ln(S_0) + \\ln(e^{-\\beta t}) = \\ln(S_0) - \\beta t $$\nThis equation is in the form of a straight line, $y = c + mx$, where:\n- The dependent variable is $y = \\ln(S_t)$.\n- The independent variable is $x = t$.\n- The intercept is $c = \\ln(S_0)$.\n- The slope is $m = -\\beta$.\n\nThis transformation allows us to use linear least squares, a simpler and more direct method. The principle of least squares, applied to this linearized model, seeks to find the parameters $m$ and $c$ that minimize the sum of squared differences between the transformed observed data and the values predicted by the linear model. The objective function to minimize is:\n$$ SSE_{\\text{log}} = \\sum_i (\\ln(s_i) - (c + m t_i))^2 $$\n\nThis approach must handle the specific data characteristics mentioned.\nFirst, the logarithmic transformation $\\ln(s_i)$ is undefined if an observed SUDS rating $s_i$ is zero. The physical model $S_t = S_0 e^{-\\beta t}$ (for $S_0 > 0$) implies that the true distress level is always positive, $S_t > 0$, for any finite time $t$. An observation of $s_i=0$ is therefore best understood as a \"floor effect,\" where a small but non-zero level of distress is reported as zero due to the limits of the measurement scale. As these points lie outside the domain of the log-transformed model, the principled course of action is to exclude any data point $(t_i, s_i)$ where $s_i \\le 0$ from the regression analysis. The sum of squared errors is thus computed over the subset of data where $s_i > 0$.\n\nSecond, the case where a measurement is not available at $t=0$ poses no issue for this method. The standard formulas for linear regression operate on a set of $(x_i, y_i)$ pairs and do not require one of the $x_i$ values to be zero. The regression will compute the best-fit line for the available data, and the resulting intercept parameter, $c$, remains the model's estimate for $\\ln(S_t)$ at $t=0$.\n\nWith the model linearized and data-handling rules established, we can find the slope $m$ that minimizes $SSE_{\\text{log}}$. For a set of $N$ valid data points, transformed to $(x_i, y_i) = (t_i, \\ln(s_i))$, the slope of the ordinary least squares regression line is given by the well-known formula derived from the normal equations:\n$$ m = \\frac{N \\sum_{i=1}^{N} (x_i y_i) - (\\sum_{i=1}^{N} x_i) (\\sum_{i=1}^{N} y_i)}{N \\sum_{i=1}^{N} (x_i^2) - (\\sum_{i=1}^{N} x_i)^2} $$\nFrom our linear model definition, we have the relationship $m = -\\beta$. Therefore, the estimator for the decay rate, $\\hat{\\beta}$, is:\n$$ \\hat{\\beta} = -m = -\\frac{N \\sum (t_i \\ln(s_i)) - (\\sum t_i)(\\sum \\ln(s_i))}{N \\sum (t_i^2) - (\\sum t_i)^2} $$\nwhere the sums are taken over all $i$ for which $s_i > 0$.\n\nThe algorithm to be implemented is as follows:\n1.  For each test case, receive the list of data pairs $(t_i, s_i)$.\n2.  Filter this list to create a new set of data points containing only pairs where the SUDS rating $s_i$ is strictly positive.\n3.  From this filtered set, construct two vectors: one for time values, $t$, and one for the natural logarithm of the SUDS ratings, $y = \\ln(s)$.\n4.  If there are fewer than two valid data points, a unique line cannot be fit; otherwise, proceed. All test cases provided have sufficient data.\n5.  Apply the ordinary least squares formula to the $(t, y)$ data to compute the slope, $m$. This is most robustly done using established numerical libraries.\n6.  Calculate the decay rate estimate as $\\hat{\\beta} = -m$.\n7.  Round the resulting $\\hat{\\beta}$ to four decimal places as required.\n8.  Repeat for all test cases and format the output as a single list.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Solves the problem of estimating exponential decay rates from SUDS data.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = {\n        'A': [(0, 80), (2, 65), (4, 51), (6, 40), (8, 33), (10, 25), (12, 19), (15, 13)],\n        'B': [(0, 60), (3, 61), (6, 59), (9, 60), (12, 58), (15, 60)],\n        'C': [(0, 40), (2, 22), (4, 12), (6, 7), (8, 3), (10, 0), (12, 0)],\n        'D': [(5, 61), (10, 39), (15, 28), (20, 18), (25, 12), (30, 8)],\n        'E': [(0, 70), (5, 65), (10, 59), (15, 53), (20, 47), (25, 43), (30, 39)],\n    }\n\n    def estimate_beta(data):\n        \"\"\"\n        Estimates the decay rate beta by linearizing the exponential model\n        and performing a least-squares regression.\n\n        The model is S_t = S_0 * exp(-beta * t).\n        Taking the natural log gives: ln(S_t) = ln(S_0) - beta * t.\n        This is a linear equation y = c + m*x, where:\n        y = ln(S_t)\n        x = t\n        m = -beta (slope)\n        c = ln(S_0) (intercept)\n\n        Args:\n            data (list of tuples): A list of (time, suds) observations.\n\n        Returns:\n            float: The estimated decay rate beta.\n        \"\"\"\n        # Filter out observations where SUDS rating is 0 or less,\n        # as ln(s) is undefined for s <= 0.\n        # This handles floor effects where latent distress is small but reported as 0.\n        valid_data = [item for item in data if item[1] > 0]\n        \n        # We need at least two points to fit a line.\n        if len(valid_data) < 2:\n            return np.nan # Or raise an error, but for this problem, data is sufficient.\n\n        # Unpack the valid data into separate arrays for time and SUDS.\n        t_values = np.array([item[0] for item in valid_data])\n        s_values = np.array([item[1] for item in valid_data])\n\n        # Apply the logarithmic transformation to linearize the model.\n        y_values = np.log(s_values)\n        \n        # Perform ordinary least squares linear regression to find the slope.\n        # scipy.stats.linregress returns (slope, intercept, r_value, p_value, std_err).\n        # The slope 'm' is equal to -beta.\n        slope, _, _, _, _ = stats.linregress(t_values, y_values)\n        \n        # The decay rate beta is the negative of the slope.\n        beta = -slope\n        \n        return beta\n\n    results = []\n    # Process each test case in the specified order (A-E).\n    for case_id in sorted(test_cases.keys()):\n        data = test_cases[case_id]\n        beta_estimate = estimate_beta(data)\n        # Format the result to four decimal places.\n        results.append(f\"{beta_estimate:.4f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4710967"}]}