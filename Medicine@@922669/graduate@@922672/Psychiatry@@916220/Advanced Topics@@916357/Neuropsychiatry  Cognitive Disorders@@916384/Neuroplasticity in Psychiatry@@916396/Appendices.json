{"hands_on_practices": [{"introduction": "Understanding neuroplasticity begins at the cellular level. This exercise challenges you to apply the calcium-control hypothesis to a classic experimental scenario, exploring how different calcium sources contribute to Long-Term Potentiation (LTP). By reasoning through the consequences of an N-Methyl-D-Aspartate (NMDA) receptor blockade and designing a rescue protocol [@problem_id:4732935], you will sharpen your intuition for the biophysical logic that governs synaptic strength, a crucial skill for evaluating and designing studies in experimental psychiatry.", "problem": "A hippocampal slice experiment is designed to model synaptic mechanisms relevant to cortical plasticity anomalies seen in N-Methyl-D-Aspartate (NMDA) receptor hypofunction states implicated in schizophrenia. In control artificial cerebrospinal fluid, a standard Theta-Burst Stimulation (TBS) of Schaffer collaterals—bursts of $4$ presynaptic pulses at $100$ Hz, repeated at $5$ Hz for $10$ bursts—reliably produces Long-Term Potentiation (LTP), measured as an increase in the slope of the excitatory postsynaptic potential. You now perfuse the slice with a competitive NMDA receptor antagonist at a concentration known to abolish NMDA currents without affecting $\\alpha$-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA) receptor responses. The postsynaptic pyramidal neuron is recorded in current-clamp and is capable of firing action potentials.\n\nStarting from the calcium-control hypothesis of synaptic plasticity and the canonical role of the NMDA receptor as a coincidence detector, reason about the following:\n\n- Under NMDA receptor blockade, predict the qualitative effect of the same TBS on LTP induction at these synapses.\n- Propose a spike-timing protocol that could rescue potentiation without relying on NMDA receptor calcium entry. Your proposal must specify the relative timing of presynaptic and postsynaptic events, their intraburst frequencies, and the rationale grounded in an alternative source of postsynaptic calcium sufficient to cross the LTP threshold.\n\nAssumptions and foundational facts you may use:\n- The calcium-control hypothesis: there exist thresholds $\\theta_{\\mathrm{LTD}}$ and $\\theta_{\\mathrm{LTP}}$ with $\\theta_{\\mathrm{LTD}}  \\theta_{\\mathrm{LTP}}$ such that if the time-integrated postsynaptic calcium, $C = \\int_{0}^{T} [\\mathrm{Ca}^{2+}]_{\\mathrm{post}}(t)\\, dt$, satisfies $C  \\theta_{\\mathrm{LTP}}$, LTP occurs; if $\\theta_{\\mathrm{LTD}}  C  \\theta_{\\mathrm{LTP}}$, Long-Term Depression (LTD) occurs; otherwise plasticity is minimal.\n- NMDA receptors require both glutamate binding and depolarization to relieve $\\mathrm{Mg}^{2+}$ block to pass $\\mathrm{Ca}^{2+}$.\n- Voltage-Gated Calcium Channels (VGCCs), including L-type channels, can admit substantial $\\mathrm{Ca}^{2+}$ during strong postsynaptic depolarization and bursts of back-propagating action potentials (bAPs).\n- Spike-Timing-Dependent Plasticity (STDP) arises from coincidence between presynaptic input and postsynaptic depolarization, with the sign and magnitude determined by the amplitude and kinetics of postsynaptic $\\mathrm{Ca}^{2+}$ transients.\n\nWhich option best matches the predicted effect and provides a scientifically sound spike-timing protocol to rescue potentiation under NMDA receptor blockade?\n\nA. TBS still induces robust LTP because AMPA receptor conductance ensures sufficient depolarization; no rescue protocol is necessary.\n\nB. TBS-induced LTP is abolished; rescue potentiation by pairing each presynaptic burst with a postsynaptic burst of $3$–$5$ spikes at $100$ Hz that precedes the presynaptic burst by approximately $10$ ms, repeated at theta frequency ($5$ Hz) for $50$–$100$ pairings, thereby recruiting L-type VGCCs to raise $[\\mathrm{Ca}^{2+}]_{\\mathrm{post}}$ above $\\theta_{\\mathrm{LTP}}$ without NMDA receptor current.\n\nC. TBS converts to LTD via AMPA receptor internalization; rescue potentiation by switching to low-frequency pre-before-post pairing at $1$ Hz with a $+50$ ms interval to enhance temporal coincidence.\n\nD. TBS-induced LTP is attenuated but can be rescued by increasing intraburst frequency from $100$ Hz to $200$ Hz in the presynaptic train while keeping the postsynaptic neuron near rest, thereby overcoming $\\mathrm{Mg}^{2+}$ block and restoring NMDA receptor calcium influx.", "solution": "The user has provided a problem in synaptic neurophysiology and requested a detailed validation, derivation, and evaluation of options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **System:** Hippocampal slice experiment, Schaffer collateral synapses.\n-   **Control Protocol:** Theta-Burst Stimulation (TBS) consisting of bursts of $4$ presynaptic pulses at $100$ Hz, repeated at $5$ Hz for $10$ bursts.\n-   **Control Outcome:** TBS reliably produces Long-Term Potentiation (LTP).\n-   **Experimental Manipulation:** Perfusion with a competitive NMDA receptor antagonist.\n-   **Effect of Antagonist:** Abolishes NMDA currents; does not affect AMPA receptor responses.\n-   **Postsynaptic Recording:** Current-clamp mode, cell is capable of firing action potentials.\n-   **Task 1:** Predict the effect of TBS on LTP induction under NMDA receptor blockade.\n-   **Task 2:** Propose a spike-timing protocol to rescue potentiation without NMDA receptor calcium entry, specifying timing, frequency, and rationale.\n-   **Assumption 1 (Calcium-Control Hypothesis):** Plasticity outcome depends on the time-integrated postsynaptic calcium, $C = \\int_{0}^{T} [\\mathrm{Ca}^{2+}]_{\\mathrm{post}}(t)\\, dt$.\n    -   $C  \\theta_{\\mathrm{LTP}} \\Rightarrow$ LTP\n    -   $\\theta_{\\mathrm{LTD}}  C  \\theta_{\\mathrm{LTP}} \\Rightarrow$ LTD\n    -   $\\theta_{\\mathrm{LTD}}  \\theta_{\\mathrm{LTP}}$\n-   **Assumption 2 (NMDA Receptor):** Requires glutamate binding and depolarization to relieve $\\mathrm{Mg}^{2+}$ block for $\\mathrm{Ca}^{2+}$ passage.\n-   **Assumption 3 (VGCCs):** Voltage-Gated Calcium Channels admit substantial $\\mathrm{Ca}^{2+}$ during strong postsynaptic depolarization and back-propagating action potentials (bAPs).\n-   **Assumption 4 (STDP):** Arises from coincidence of presynaptic and postsynaptic events, with outcome determined by postsynaptic $\\mathrm{Ca}^{2+}$ transients.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientific Grounding:** The problem is firmly grounded in canonical principles of synaptic plasticity. The roles of NMDA and AMPA receptors, the calcium-control hypothesis, TBS protocols, STDP, and the function of VGCCs are all central and well-established concepts in neuroscience. The link to NMDA receptor hypofunction in schizophrenia provides a valid clinical context. The problem setup describes a standard electrophysiological experiment. All premises are scientifically sound.\n-   **Well-Posedness:** The problem is well-posed. It provides sufficient constraints and foundational principles (e.g., calcium hypothesis, receptor properties) to deduce a qualitative outcome and to evaluate the logic of proposed rescue protocols. A unique line of reasoning can be followed to arrive at a conclusion.\n-   **Objectivity:** The language is technical, precise, and free of subjective or ambiguous terminology.\n\n-   **Flaw Checklist Assessment:**\n    1.  **Scientific Unsoundness:** None.\n    2.  **Non-Formalizable/Irrelevant:** None. The problem is a direct application of biophysical models.\n    3.  **Incomplete/Contradictory:** None. The information is self-contained and consistent.\n    4.  **Unrealistic/Infeasible:** None. The described experiment is a standard procedure in neuroscience labs.\n    5.  **Ill-Posed/Poorly Structured:** None.\n    6.  **Pseudo-Profound/Trivial:** None. The problem requires integrating multiple concepts in a non-trivial manner.\n    7.  **Outside Verifiability:** None. The predictions are experimentally testable.\n\n**Step 3: Verdict and Action**\n\n-   **Verdict:** The problem statement is **valid**.\n-   **Action:** Proceed with the solution.\n\n### Derivation and Option Analysis\n\n**Part 1: Predicting the Effect of TBS under NMDA Receptor Blockade**\n\n1.  **Canonical LTP Induction:** The standard TBS protocol induces LTP at Schaffer collateral-CA1 synapses through an NMDA receptor-dependent mechanism. The high-frequency ($100$ Hz) presynaptic pulses within each burst cause temporal summation of excitatory postsynaptic potentials (EPSPs) mediated by AMPA receptors.\n2.  This summation leads to strong local dendritic depolarization. According to Assumption 2, this depolarization is sufficient to expel the voltage-dependent $\\mathrm{Mg}^{2+}$ block from the pore of the NMDA receptors.\n3.  Simultaneously, presynaptic terminals release glutamate, which binds to the now-unblocked NMDA receptors. The receptor, acting as a coincidence detector, opens its channel.\n4.  The opening of the NMDA receptor channel results in a large and rapid influx of $\\mathrm{Ca}^{2+}$ ions into the postsynaptic spine.\n5.  According to the calcium-control hypothesis (Assumption 1), this large calcium transient integrates over time, such that the total calcium charge $C$ surpasses the high threshold for LTP induction ($C  \\theta_{\\mathrm{LTP}}$).\n6.  **Effect of the Antagonist:** The problem states the slice is perfused with a competitive NMDA receptor antagonist that *abolishes NMDA currents*. This means the primary pathway for calcium entry during TBS is completely blocked.\n7.  Under these conditions, the TBS protocol will still activate AMPA receptors and cause depolarization, but no significant current will flow through the NMDA receptors. The major source of calcium required to cross $\\theta_{\\mathrm{LTP}}$ is eliminated.\n8.  Therefore, the TBS protocol will fail to induce LTP. The integrated calcium concentration, $C$, will be well below $\\theta_{\\mathrm{LTP}}$. Depending on the magnitude of any residual calcium entry (e.g., through VGCCs minimally activated by subthreshold EPSPs), a small rise in calcium could potentially lead to LTD ($\\theta_{\\mathrm{LTD}}  C  \\theta_{\\mathrm{LTP}}$) or no plasticity ($C  \\theta_{\\mathrm{LTD}}$). The most certain and direct consequence is the abolishment of LTP.\n\n**Part 2: Proposing a Rescue Protocol**\n\n1.  **Objective:** To induce LTP without NMDA receptor function, we must find an alternative calcium source and a protocol to activate it sufficiently to make $C  \\theta_{\\mathrm{LTP}}$.\n2.  **Alternative Calcium Source:** Assumption 3 points to Voltage-Gated Calcium Channels (VGCCs) as a key alternative source. These channels are activated by strong depolarization, particularly that which is achieved during postsynaptic action potentials, which then back-propagate into the dendrites (bAPs).\n3.  **Protocol Design:** To activate VGCCs strongly, we must force the postsynaptic neuron to fire bursts of action potentials. This can be achieved experimentally by injecting depolarizing current pulses into the neuron in current-clamp mode. A successful rescue protocol must pair this postsynaptic spiking with the presynaptic stimulation to maintain synapse specificity.\n4.  **Spike-Timing Logic:** The protocol must ensure that strong depolarization (from bAPs) coincides with presynaptic glutamate release at the target synapses. A burst of postsynaptic spikes (e.g., $3$–$5$ spikes at $100$ Hz), repeated at the same theta frequency ($5$ Hz) as the presynaptic TBS, would be effective. This pairing of pre- and postsynaptic bursts greatly amplifies VGCC activation and the resulting calcium influx. The exact timing of the pre- and post-synaptic bursts determines the outcome. While canonical STDP often involves presynaptic stimulation preceding postsynaptic spikes for LTP, certain protocols using postsynaptic bursts *preceding* presynaptic stimulation (post-before-pre) have been shown to induce VGCC-dependent LTP. This is because the burst of bAPs creates a large and sustained window of depolarization and VGCC availability, which can be acted upon by the subsequent EPSP from the presynaptic burst, leading to a massive, localized calcium signal sufficient to cross $\\theta_{\\mathrm{LTP}}$.\n\n### Option-by-Option Analysis\n\n**A. TBS still induces robust LTP because AMPA receptor conductance ensures sufficient depolarization; no rescue protocol is necessary.**\n-   **Analysis:** This statement is incorrect. While AMPA receptors provide depolarization, the defining feature of this form of LTP is its dependence on NMDA receptor-mediated $\\mathrm{Ca}^{2+}$ influx. Blocking this influx abolishes LTP. Experimental evidence overwhelmingly supports the necessity of NMDA receptors for standard TBS-induced LTP at these synapses.\n-   **Verdict:** Incorrect.\n\n**B. TBS-induced LTP is abolished; rescue potentiation by pairing each presynaptic burst with a postsynaptic burst of $3$–$5$ spikes at $100$ Hz that precedes the presynaptic burst by approximately $10$ ms, repeated at theta frequency ($5$ Hz) for $50$–$100$ pairings, thereby recruiting L-type VGCCs to raise $[\\mathrm{Ca}^{2+}]_{\\mathrm{post}}$ above $\\theta_{\\mathrm{LTP}}$ without NMDA receptor current.**\n-   **Analysis:** This option is fully consistent with our derivation.\n    -   It correctly predicts that TBS-induced LTP is abolished.\n    -   It proposes a valid rescue strategy: pairing presynaptic input with explicitly induced postsynaptic spike bursts.\n    -   The parameters of the postsynaptic burst ($3$–$5$ spikes at $100$ Hz) are appropriate for causing strong depolarization and bAP-mediated signaling.\n    -   The overall paired stimulation at theta frequency ($5$ Hz) mimics the original TBS structure.\n    -   It correctly identifies the mechanism: recruitment of VGCCs (specifically mentioning L-type, a major player) as the alternative $\\mathrm{Ca}^{2+}$ source to bypass the NMDA receptor blockade and exceed $\\theta_{\\mathrm{LTP}}$.\n    -   The specified \"post-before-pre\" timing (post-burst precedes pre-burst by about $10$ ms) is a known, albeit sophisticated, protocol for inducing VGCC-dependent LTP.\n-   **Verdict:** Correct.\n\n**C. TBS converts to LTD via AMPA receptor internalization; rescue potentiation by switching to low-frequency pre-before-post pairing at $1$ Hz with a $+50$ ms interval to enhance temporal coincidence.**\n-   **Analysis:** This option is flawed. While the conversion to LTD is a possibility, the rescue protocol is counterproductive. Low-frequency stimulation (LFS) at $1$ Hz is the classic protocol for inducing LTD, not for rescuing LTP. A a $+50$ ms interval for pairing is also on the longer side for inducing robust LTP, typically requiring tighter coincidence ($ 20$ ms). The proposed rescue method would likely induce or deepen depression, not cause potentiation.\n-   **Verdict:** Incorrect.\n\n**D. TBS-induced LTP is attenuated but can be rescued by increasing intraburst frequency from $100$ Hz to $200$ Hz in the presynaptic train while keeping the postsynaptic neuron near rest, thereby overcoming $\\mathrm{Mg}^{2+}$ block and restoring NMDA receptor calcium influx.**\n-   **Analysis:** This option contains a fundamental error. The problem states that a *competitive antagonist* is used, which *abolishes NMDA currents*. A competitive antagonist works by binding to the glutamate binding site, preventing the channel from opening even if the $\\mathrm{Mg}^{2+}$ block is removed by depolarization. Therefore, it is impossible to \"restore NMDA receptor calcium influx\" by altering stimulation frequency or depolarization. The receptor is pharmacologically silenced. The proposed rescue mechanism directly contradicts the experimental conditions.\n-   **Verdict:** Incorrect.", "answer": "$$\\boxed{B}$$", "id": "4732935"}, {"introduction": "Neuroplasticity is not just functional but also structural, involving the physical creation and elimination of dendritic spines. This problem introduces a powerful method to model these dynamics by treating spine turnover as a stochastic birth-death process. By deriving the steady-state spine density from fundamental rates of formation and pruning [@problem_id:4732886], you will gain hands-on experience with the mathematical tools used to understand and predict the structural equilibrium of neural circuits over time.", "problem": "In the context of dendritic spine turnover relevant to neuroplasticity in psychiatry, model the population of dendritic spines on a cortical pyramidal neuron's apical dendritic segment as a continuous-time birth-death process. Let $N(t)$ denote the number of spines at time $t$ on a segment of length $L$. Assume:\n\n1. Spine formation events are independent and occur as a Poisson process with a constant hazard proportional to segment length, with rate $\\lambda L$ (units: spines $\\mu\\mathrm{m}^{-1}\\,\\mathrm{day}^{-1}$ times $\\mu\\mathrm{m}$ gives spines $\\mathrm{day}^{-1}$).\n2. Spine pruning events affect each existing spine independently, with each spine carrying an individual hazard $\\mu$ (units: $\\mathrm{day}^{-1}$), so the total pruning rate when there are $n$ spines is $\\mu n$.\n3. The process satisfies the Markov property and is time-homogeneous.\n\nStarting from the fundamental definitions of the continuous-time Markov birth-death process and the Kolmogorov forward (master) equation for the probabilities $P(N(t)=n)$, derive the ordinary differential equation governing the time evolution of the mean spine count $\\bar{N}(t) = \\mathbb{E}[N(t)]$. Use this to obtain the steady-state mean spine density $\\rho^{*} = \\lim_{t\\to\\infty} \\bar{N}(t)/L$ as an analytic expression in terms of $\\lambda$ and $\\mu$.\n\nThen, evaluate $\\rho^{*}$ numerically for a dendritic segment of length $L = 200\\,\\mu\\mathrm{m}$ with formation rate $\\lambda = 0.028$ spines $\\mu\\mathrm{m}^{-1}\\,\\mathrm{day}^{-1}$ and pruning rate per spine $\\mu = 0.019\\,\\mathrm{day}^{-1}$. Round your final numeric answer to four significant figures. Express your final spine density in spines per micrometer.", "solution": "The problem statement is first validated to ensure it is scientifically grounded, well-posed, objective, and self-contained.\n\n### Step 1: Extract Givens\n-   **Model**: Continuous-time birth-death process for the number of dendritic spines, $N(t)$, on a dendritic segment.\n-   **Variable**: $N(t)$ is the number of spines at time $t$.\n-   **Parameter**: $L$ is the length of the dendritic segment.\n-   **Birth Rate (Spine Formation)**: $\\lambda_n = \\lambda L$ for all $n \\geq 0$. The formation process is a Poisson process with a constant rate, independent of the current number of spines, $n$.\n-   **Death Rate (Spine Pruning)**: $\\mu_n = \\mu n$ for all $n \\geq 1$, and $\\mu_0 = 0$. Each of the $n$ existing spines has an independent pruning hazard of $\\mu$.\n-   **Assumptions**: The process is a time-homogeneous, continuous-time Markov process.\n-   **Objective 1**: Derive the ordinary differential equation (ODE) for the mean spine count, $\\bar{N}(t) = \\mathbb{E}[N(t)]$.\n-   **Objective 2**: Find the analytical expression for the steady-state mean spine density, $\\rho^{*} = \\lim_{t\\to\\infty} \\bar{N}(t)/L$.\n-   **Objective 3**: Numerically evaluate $\\rho^{*}$ given the parameters:\n    -   $L = 200\\,\\mu\\mathrm{m}$\n    -   $\\lambda = 0.028$ spines $\\mu\\mathrm{m}^{-1}\\,\\mathrm{day}^{-1}$\n    -   $\\mu = 0.019\\,\\mathrm{day}^{-1}$\n-   **Rounding instruction**: The final numeric answer should be rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem uses a standard and accepted mathematical model (a birth-death process, specifically an M/M/$\\infty$ queuing model) to describe a well-studied biological phenomenon (dendritic spine turnover). This is a common approach in computational neuroscience and is scientifically valid.\n-   **Well-Posed**: The problem is clearly stated with all necessary parameters and assumptions. The birth and death rates are defined, ensuring that a unique and stable solution for the mean and its steady state exists.\n-   **Objective**: The problem is formulated with precise, quantitative language, free of subjectivity.\n-   **Consistency and Completeness**: The provided units are consistent: $\\lambda L$ has units of $(\\text{spines}\\,\\mu\\mathrm{m}^{-1}\\,\\mathrm{day}^{-1}) \\times \\mu\\mathrm{m} = \\text{spines}\\,\\mathrm{day}^{-1}$, which is a rate. The pruning rate $\\mu n$ has units of $\\mathrm{day}^{-1} \\times \\text{spines} = \\text{spines}\\,\\mathrm{day}^{-1}$. All information required for the derivation and calculation is present.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution process will now proceed.\n\n### Solution Derivation\n\nThe process is a continuous-time Markov birth-death process with state $n$ representing the number of spines. Let $P_n(t) = P(N(t)=n)$ be the probability of having $n$ spines at time $t$. The time evolution of these probabilities is governed by the Kolmogorov forward equations, also known as the master equation:\n$$ \\frac{d P_n(t)}{dt} = \\lambda_{n-1} P_{n-1}(t) + \\mu_{n+1} P_{n+1}(t) - (\\lambda_n + \\mu_n) P_n(t), \\quad \\text{for } n \\ge 1 $$\n$$ \\frac{d P_0(t)}{dt} = \\mu_1 P_1(t) - \\lambda_0 P_0(t) $$\nUsing the given birth and death rates, $\\lambda_n = \\lambda L$ and $\\mu_n = \\mu n$:\n$$ \\frac{d P_n(t)}{dt} = \\lambda L P_{n-1}(t) + \\mu (n+1) P_{n+1}(t) - (\\lambda L + \\mu n) P_n(t), \\quad \\text{for } n \\ge 1 $$\n$$ \\frac{d P_0(t)}{dt} = \\mu P_1(t) - \\lambda L P_0(t) $$\n\nThe primary goal is to find the ODE for the mean spine count, $\\bar{N}(t) = \\mathbb{E}[N(t)] = \\sum_{n=0}^{\\infty} n P_n(t)$. We differentiate $\\bar{N}(t)$ with respect to time:\n$$ \\frac{d\\bar{N}(t)}{dt} = \\frac{d}{dt} \\sum_{n=0}^{\\infty} n P_n(t) = \\sum_{n=0}^{\\infty} n \\frac{dP_n(t)}{dt} $$\nA general result for birth-death processes states that the rate of change of the mean is the difference between the expected birth rate and the expected death rate:\n$$ \\frac{d\\bar{N}(t)}{dt} = \\mathbb{E}[\\lambda_{N(t)}] - \\mathbb{E}[\\mu_{N(t)}] $$\nWe calculate the expected rates for this specific problem:\nThe expected birth rate is:\n$$ \\mathbb{E}[\\lambda_{N(t)}] = \\mathbb{E}[\\lambda L] = \\sum_{n=0}^{\\infty} (\\lambda L) P_n(t) = \\lambda L \\sum_{n=0}^{\\infty} P_n(t) $$\nSince the sum of all probabilities is $1$, this simplifies to:\n$$ \\mathbb{E}[\\lambda_{N(t)}] = \\lambda L $$\nThe expected death rate is:\n$$ \\mathbb{E}[\\mu_{N(t)}] = \\mathbb{E}[\\mu N(t)] = \\sum_{n=0}^{\\infty} (\\mu n) P_n(t) = \\mu \\sum_{n=0}^{\\infty} n P_n(t) $$\nBy definition, $\\sum_{n=0}^{\\infty} n P_n(t) = \\bar{N}(t)$. Therefore:\n$$ \\mathbb{E}[\\mu_{N(t)}] = \\mu \\bar{N}(t) $$\nSubstituting these expected rates back into the general equation for the mean's time evolution, we obtain the ODE for $\\bar{N}(t)$:\n$$ \\frac{d\\bar{N}(t)}{dt} = \\lambda L - \\mu \\bar{N}(t) $$\nThis is the required ordinary differential equation governing the mean spine count.\n\nNext, we determine the steady-state mean spine count, $\\bar{N}^* = \\lim_{t\\to\\infty} \\bar{N}(t)$. At steady state, the mean count no longer changes, so $\\frac{d\\bar{N}(t)}{dt} = 0$. Setting the left-hand side of the ODE to zero:\n$$ 0 = \\lambda L - \\mu \\bar{N}^* $$\nSolving for $\\bar{N}^*$:\n$$ \\mu \\bar{N}^* = \\lambda L $$\n$$ \\bar{N}^* = \\frac{\\lambda L}{\\mu} $$\nThe problem asks for the steady-state mean spine density, $\\rho^{*}$, which is defined as $\\rho^{*} = \\bar{N}^*/L$. Substituting the expression for $\\bar{N}^*$:\n$$ \\rho^{*} = \\frac{1}{L} \\left( \\frac{\\lambda L}{\\mu} \\right) = \\frac{\\lambda}{\\mu} $$\nThis is the analytic expression for the steady-state mean spine density. The density is independent of the segment length $L$, as expected, being an intrinsic property of the spine turnover dynamics.\n\nFinally, we evaluate $\\rho^{*}$ numerically using the provided parameter values:\n-   $\\lambda = 0.028$ spines $\\mu\\mathrm{m}^{-1}\\,\\mathrm{day}^{-1}$\n-   $\\mu = 0.019\\,\\mathrm{day}^{-1}$\nThe length $L = 200\\,\\mu\\mathrm{m}$ is not needed for this calculation.\n$$ \\rho^{*} = \\frac{0.028}{0.019} \\approx 1.4736842... \\text{ spines } \\mu\\mathrm{m}^{-1} $$\nRounding the result to four significant figures as required:\n$$ \\rho^{*} \\approx 1.474 \\text{ spines } \\mu\\mathrm{m}^{-1} $$", "answer": "$$\n\\boxed{1.474}\n$$", "id": "4732886"}, {"introduction": "The ultimate goal is to understand how changes in plasticity at the cellular level impact circuit function and, ultimately, behavior. This comprehensive exercise guides you through building a spiking neural network from first principles to simulate a specific hypothesis relevant to psychiatry—how adding immature granule cells affects memory interference. By implementing neuron dynamics, Spike-Timing Dependent Plasticity (STDP), and network-level analysis [@problem_id:4732943], you will directly experience how computational modeling serves as a crucial bridge between microscopic mechanisms and macroscopic cognitive phenomena.", "problem": "You are asked to design and implement a reproducible simulation to quantitatively assess how adding immature granule cells with heightened synaptic plasticity affects memory interference in a leaky integrate-and-fire network, in a way that captures core neuroplasticity mechanisms relevant to psychiatry. The simulation must be based on first principles as follows.\n\nBegin from the standard leaky integrate-and-fire neuron model. The subthreshold membrane potential dynamics for each output neuron are modeled by\n$$\n\\frac{dV_j(t)}{dt} = -\\frac{V_j(t) - V_{\\mathrm{rest}}}{\\tau_m} + G \\cdot I_{\\mathrm{syn},j}(t),\n$$\nwhere $V_j(t)$ is the membrane potential of output neuron $j$ at time $t$, $V_{\\mathrm{rest}}$ is the resting potential, $\\tau_m$ is the membrane time constant, $G$ is a gain factor converting synaptic current to voltage, and $I_{\\mathrm{syn},j}(t)$ is the synaptic current. A spike is emitted when $V_j(t)$ crosses the threshold $V_{\\mathrm{th},j}$; after a spike, $V_j(t)$ is set to $V_{\\mathrm{reset}}$ and held for a refractory period. All time quantities must be represented in milliseconds (ms), and membrane potentials in millivolts (mV).\n\nThe synaptic current is the standard filter of presynaptic spikes:\n$$\nI_{\\mathrm{syn},j}(t) = \\sum_{i=1}^{M} w_{ji} \\, x_i(t),\n$$\nwhere $M$ is the number of input units, $w_{ji}$ is the synaptic weight from input unit $i$ to output neuron $j$, and $x_i(t)$ is the synaptic trace of presynaptic unit $i$ obeying\n$$\n\\frac{dx_i(t)}{dt} = -\\frac{x_i(t)}{\\tau_{\\mathrm{syn}}} + s_i(t),\n$$\nwith $s_i(t)\\in\\{0,1\\}$ denoting the presynaptic spike train at time step $t$ and $\\tau_{\\mathrm{syn}}$ the synaptic time constant.\n\nSynaptic plasticity must be implemented using pair-based Spike-Timing Dependent Plasticity (STDP), a well-tested learning rule. Use exponentially decaying eligibility traces for pre- and post-synaptic activity:\n$$\n\\frac{dE^{\\mathrm{pre}}_i(t)}{dt} = -\\frac{E^{\\mathrm{pre}}_i(t)}{\\tau_+} + s_i(t), \\quad\n\\frac{dE^{\\mathrm{post}}_j(t)}{dt} = -\\frac{E^{\\mathrm{post}}_j(t)}{\\tau_-} + s^{\\mathrm{post}}_j(t),\n$$\nwhere $s^{\\mathrm{post}}_j(t)\\in\\{0,1\\}$ indicates a spike of neuron $j$ at time step $t$, and $\\tau_+$ and $\\tau_-$ are the potentiation and depression time constants, respectively. The weight update over one presentation window is\n$$\n\\Delta w_{ji} = A^{(+)}_j \\sum_t s^{\\mathrm{post}}_j(t) \\, E^{\\mathrm{pre}}_i(t) \\;-\\; A^{(-)}_j \\sum_t E^{\\mathrm{post}}_j(t) \\, s_i(t),\n$$\nwhere $A^{(+)}_j$ and $A^{(-)}_j$ are the long-term potentiation and long-term depression amplitudes for neuron $j$. Synaptic weights are nonnegative and bounded:\n$$\nw_{ji} \\leftarrow \\min\\left\\{\\max\\{w_{ji} + \\Delta w_{ji} - \\lambda w_{ji},\\, 0\\},\\, w_{\\max}\\right\\},\n$$\nwith $\\lambda$ a small decay term and $w_{\\max}$ a maximum weight.\n\nDefine immature granule cells as output neurons having heightened plasticity and lower spike threshold compared to mature granule cells. Concretely, for immature neurons $j$, set\n$$\nA^{(+)}_j = \\gamma \\, A^{(+)}_{\\mathrm{base}},\\quad A^{(-)}_j = \\gamma \\, A^{(-)}_{\\mathrm{base}},\\quad V_{\\mathrm{th},j} = V_{\\mathrm{th},\\mathrm{imm}},\n$$\nand for mature neurons $j$,\n$$\nA^{(+)}_j = A^{(+)}_{\\mathrm{base}},\\quad A^{(-)}_j = A^{(-)}_{\\mathrm{base}},\\quad V_{\\mathrm{th},j} = V_{\\mathrm{th},\\mathrm{mat}}.\n$$\nHere $\\gamma1$ is the plasticity scaling factor for immature neurons, $A^{(+)}_{\\mathrm{base}}$ and $A^{(-)}_{\\mathrm{base}}$ are base amplitudes, $V_{\\mathrm{th},\\mathrm{imm}}$ and $V_{\\mathrm{th},\\mathrm{mat}}$ are spike thresholds for immature and mature neurons, respectively.\n\nMemory interference is operationalized as the average Jaccard overlap among output ensembles that fire to different input patterns. For pattern $p$, let $S_p$ be the set of output neurons that fire at least once during retrieval. For patterns $p\\neq q$, define the Jaccard overlap\n$$\nO_{pq} = \\frac{|S_p \\cap S_q|}{|S_p \\cup S_q|},\n$$\nwith $O_{pq}=0$ when $|S_p \\cup S_q|=0$. The interference index is the average over all unordered pairs:\n$$\nI = \\frac{2}{P(P-1)} \\sum_{1\\le pq\\le P} O_{pq},\n$$\nwhere $P$ is the number of patterns. The effect of adding immature granule cells is quantified as\n$$\n\\Delta I = I_{\\mathrm{immature}} - I_{\\mathrm{baseline}},\n$$\nwith $I_{\\mathrm{baseline}}$ computed when all output neurons are mature.\n\nYour program must:\n- Generate $P$ binary input patterns of dimensionality $M$ with activity density $p_{\\mathrm{act}}$ and controlled pairwise correlation as follows. Let $c\\in[0,1]$ be the fraction of active inputs shared across all patterns. Construct a common set of $\\lfloor c\\, p_{\\mathrm{act}}\\, M\\rfloor$ inputs active in every pattern, and for each pattern add $\\lfloor p_{\\mathrm{act}}\\, M\\rfloor - \\lfloor c\\, p_{\\mathrm{act}}\\, M\\rfloor$ additional active inputs sampled uniformly at random (without replacement over the input index set).\n- For each pattern presentation during training, generate presynaptic Poisson spikes with rates $r_{\\mathrm{high}}$ for active inputs and $r_{\\mathrm{low}}$ for inactive inputs. Use discrete time steps of $\\Delta t$ milliseconds over a window of $T$ milliseconds. The per-step spike probability is $p = r \\, \\Delta t / 1000$ since rates are in Hertz.\n- Train the network with STDP over one pass of all $P$ patterns.\n- Perform retrieval without further learning for each pattern to obtain $S_p$.\n- Compute $I_{\\mathrm{baseline}}$ with all neurons mature, then $I_{\\mathrm{immature}}$ when a specified number of output neurons are set to immature with plasticity scaling $\\gamma$ and lower threshold, using identical initial weights and identical presynaptic spike realizations for strict comparability.\n- Report $\\Delta I$ for each test case as a float rounded to three decimals (unitless).\n\nUse the following fixed model constants in all simulations unless otherwise stated: $V_{\\mathrm{rest}}=-65$ mV, $V_{\\mathrm{reset}}=-65$ mV, $V_{\\mathrm{th},\\mathrm{mat}}=-52$ mV, $V_{\\mathrm{th},\\mathrm{imm}}=-55$ mV, $\\tau_m=20$ ms, $\\tau_{\\mathrm{syn}}=5$ ms, $\\tau_+=20$ ms, $\\tau_-=20$ ms, $G=3.0$ mV per unit synaptic current, $\\lambda=0.001$, $w_{\\max}=0.1$, $A^{(+)}_{\\mathrm{base}}=0.0005$, $A^{(-)}_{\\mathrm{base}}=0.0006$, $\\Delta t=1$ ms, $T=150$ ms, $p_{\\mathrm{act}}=0.2$, $r_{\\mathrm{high}}=20$ Hz, $r_{\\mathrm{low}}=2$ Hz, refractory period $=3$ ms.\n\nTest suite:\n- Case $1$ (happy path): $N=100$, $M=50$, $N_{\\mathrm{imm}}=30$, $\\gamma=2.0$, $P=6$, $c=0.2$, seed $=123$.\n- Case $2$ (boundary, no immature): $N=100$, $M=50$, $N_{\\mathrm{imm}}=0$, $\\gamma=2.0$, $P=6$, $c=0.2$, seed $=124$.\n- Case $3$ (edge, high immature fraction and plasticity): $N=120$, $M=60$, $N_{\\mathrm{imm}}=60$, $\\gamma=3.0$, $P=8$, $c=0.3$, seed $=125$.\n- Case $4$ (edge, higher memory load and correlation): $N=140$, $M=60$, $N_{\\mathrm{imm}}=50$, $\\gamma=2.5$, $P=12$, $c=0.4$, seed $=126$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$), where each $result_k$ is the value of $\\Delta I$ for test case $k$ rounded to three decimals.", "solution": "The user wants a reproducible simulation to study the effect of immature neurons on memory interference in a spiking neural network.\n\n### Step 1: Extract Givens\n\n*   **Neuron Model (Leaky Integrate-and-Fire):**\n    *   Dynamics: $\\frac{dV_j(t)}{dt} = -\\frac{V_j(t) - V_{\\mathrm{rest}}}{\\tau_m} + G \\cdot I_{\\mathrm{syn},j}(t)$\n    *   Spike condition: $V_j(t)$ crosses $V_{\\mathrm{th},j}$.\n    *   Post-spike reset: $V_j(t) \\to V_{\\mathrm{reset}}$ and held for a refractory period.\n*   **Synaptic Input Model:**\n    *   Synaptic current: $I_{\\mathrm{syn},j}(t) = \\sum_{i=1}^{M} w_{ji} \\, x_i(t)$\n    *   Presynaptic trace: $\\frac{dx_i(t)}{dt} = -\\frac{x_i(t)}{\\tau_{\\mathrm{syn}}} + s_i(t)$, where $s_i(t)\\in\\{0,1\\}$.\n*   **Plasticity Model (STDP):**\n    *   Presynaptic eligibility trace: $\\frac{dE^{\\mathrm{pre}}_i(t)}{dt} = -\\frac{E^{\\mathrm{pre}}_i(t)}{\\tau_+} + s_i(t)$\n    *   Postsynaptic eligibility trace: $\\frac{dE^{\\mathrm{post}}_j(t)}{dt} = -\\frac{E^{\\mathrm{post}}_j(t)}{\\tau_-} + s^{\\mathrm{post}}_j(t)$\n    *   Weight update rule (per window): $\\Delta w_{ji} = A^{(+)}_j \\sum_t s^{\\mathrm{post}}_j(t) \\, E^{\\mathrm{pre}}_i(t) - A^{(-)}_j \\sum_t E^{\\mathrm{post}}_j(t) \\, s_i(t)$\n    *   Weight dynamics: $w_{ji} \\leftarrow \\min\\left\\{\\max\\{w_{ji} + \\Delta w_{ji} - \\lambda w_{ji},\\, 0\\},\\, w_{\\max}\\right\\}$\n*   **Neuron Types (Immature vs. Mature):**\n    *   Immature: $A^{(+)}_j = \\gamma \\, A^{(+)}_{\\mathrm{base}}$, $A^{(-)}_j = \\gamma \\, A^{(-)}_{\\mathrm{base}}$, $V_{\\mathrm{th},j} = V_{\\mathrm{th},\\mathrm{imm}}$\n    *   Mature: $A^{(+)}_j = A^{(+)}_{\\mathrm{base}}$, $A^{(-)}_j = A^{(-)}_{\\mathrm{base}}$, $V_{\\mathrm{th},j} = V_{\\mathrm{th},\\mathrm{mat}}$\n*   **Interference Metric:**\n    *   Output ensemble for pattern $p$: $S_p = \\{\\text{neurons firing at least once}\\}$\n    *   Jaccard Overlap: $O_{pq} = |S_p \\cap S_q| / |S_p \\cup S_q|$\n    *   Interference Index: $I = \\frac{2}{P(P-1)} \\sum_{1\\le pq\\le P} O_{pq}$\n    *   Quantification of effect: $\\Delta I = I_{\\mathrm{immature}} - I_{\\mathrm{baseline}}$\n*   **Input Generation:**\n    *   $P$ patterns of size $M$, activity density $p_{\\mathrm{act}}$.\n    *   Shared active inputs: $\\lfloor c\\, p_{\\mathrm{act}}\\, M\\rfloor$ inputs are common to all patterns.\n    *   Unique active inputs: $\\lfloor p_{\\mathrm{act}}\\, M\\rfloor - \\lfloor c\\, p_{\\mathrm{act}}\\, M\\rfloor$ inputs are specific to each pattern.\n*   **Simulation Protocol:**\n    *   Discrete time steps of $\\Delta t$ over a window of $T$.\n    *   Presynaptic spikes are Poisson with rate $r_{\\mathrm{high}}$ (active) or $r_{\\mathrm{low}}$ (inactive).\n    *   Training: One pass of all $P$ patterns with STDP.\n    *   Retrieval: Presentation of patterns without STDP to determine $S_p$.\n    *   Comparison: $I_{\\mathrm{baseline}}$ (all neurons mature) vs. $I_{\\mathrm{immature}}$ ($N_{\\mathrm{imm}}$ immature neurons), using identical initial weights and spike trains.\n*   **Fixed Constants:**\n    *   $V_{\\mathrm{rest}}=-65$ mV, $V_{\\mathrm{reset}}=-65$ mV, $V_{\\mathrm{th},\\mathrm{mat}}=-52$ mV, $V_{\\mathrm{th},\\mathrm{imm}}=-55$ mV.\n    *   $\\tau_m=20$ ms, $\\tau_{\\mathrm{syn}}=5$ ms, $\\tau_+=20$ ms, $\\tau_-=20$ ms.\n    *   $G=3.0$ mV/unit current, $\\lambda=0.001$, $w_{\\max}=0.1$.\n    *   $A^{(+)}_{\\mathrm{base}}=0.0005$, $A^{(-)}_{\\mathrm{base}}=0.0006$.\n    *   $\\Delta t=1$ ms, $T=150$ ms, $p_{\\mathrm{act}}=0.2$, $r_{\\mathrm{high}}=20$ Hz, $r_{\\mathrm{low}}=2$ Hz.\n    *   Refractory period $=3$ ms.\n*   **Test Suite:**\n    *   Case 1: $N=100, M=50, N_{\\mathrm{imm}}=30, \\gamma=2.0, P=6, c=0.2$, seed $=123$.\n    *   Case 2: $N=100, M=50, N_{\\mathrm{imm}}=0, \\gamma=2.0, P=6, c=0.2$, seed $=124$.\n    *   Case 3: $N=120, M=60, N_{\\mathrm{imm}}=60, \\gamma=3.0, P=8, c=0.3$, seed $=125$.\n    *   Case 4: $N=140, M=60, N_{\\mathrm{imm}}=50, \\gamma=2.5, P=12, c=0.4$, seed $=126$.\n\n### Step 2: Validate Using Extracted Givens\n\n1.  **Scientifically Grounded:** The problem is well-grounded in computational neuroscience. The LIF model, STDP, and the conceptual link between neurogenesis, plasticity, and memory interference are standard and well-established. The problem operationalizes these concepts into a concrete, scientifically relevant simulation. (No violation)\n2.  **Well-Posed:** All necessary parameters, constants, and a complete simulation protocol are provided. The use of a random seed ensures a unique, reproducible solution for each test case. The problem is constructive and guarantees the existence of a solution. (No violation)\n3.  **Objective:** The problem is stated in precise, quantitative, and objective language. All terms are defined mathematically. (No violation)\n4.  **Incompleteness or Contradiction:**\n    *   Initial conditions for weights and membrane potentials are not explicitly given. However, \"identical initial weights\" implies they should be initialized deterministically from the seed (e.g., random uniform), and membrane potentials are implicitly assumed to start at rest ($V_{rest}$). These are standard, reasonable assumptions for this class of problem.\n    *   There is a minor dimensional inconsistency in the LIF equation $\\frac{dV}{dt} = \\dots + G \\cdot I_{syn}$. Given that $I_{syn}$ is constructed to be dimensionless, and $G$ is given with units of mV, the term $G \\cdot I_{syn}$ (mV) is not consistent with $\\frac{dV}{dt}$ (mV/ms). This is a common notational sloppiness in the field. It is resolved by adopting the standard interpretation that the synaptic input is a current that gets integrated by the membrane. The discretized update rule then becomes $V_j(t+\\Delta t) = V_j(t) + \\frac{\\Delta t}{\\tau_m} [-(V_j(t)-V_{rest}) + G \\cdot I_{syn, j}(t)]$. With this standard interpretation, the problem is consistent. (No violation)\n5.  **Ill-Posed or Poorly Structured:** The problem structure is clear and leads to a unique solution. (No violation)\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. It is a complex but well-defined computational task based on standard models in neuroscience. The minor notational ambiguity is easily resolved by adopting a standard implementation practice.\n\n### Principle-Based Design\n\nThe solution requires implementing a discrete-time simulation of a network of leaky integrate-and-fire (LIF) neurons with spike-timing-dependent plasticity (STDP). The core of the solution is to construct a robust simulation function that can be run under two conditions—a baseline with all mature neurons and a test case with a subpopulation of immature neurons—and then compare the outcomes.\n\n**1. Discretization of Continuous Dynamics:**\nThe provided differential equations must be solved numerically. For linear equations of the form $\\tau \\frac{dy}{dt} = -y + I(t)$, an exact solution can be derived assuming the input $I(t)$ is piecewise constant over the time step $\\Delta t$. This is more accurate than a simple Euler integration and avoids numerical instability.\n\n*   **Synaptic and STDP Traces:** The dynamics $\\frac{dx}{dt} = -x/\\tau + s(t)$ where $s(t)$ represents discrete spike events are discretized as follows. In each time step, the trace first decays exponentially, and then a value of $1$ is added if a spike occurred:\n    $$x_i(t) \\leftarrow x_i(t-\\Delta t) \\cdot \\exp(-\\Delta t / \\tau_{\\mathrm{syn}}) + s_i(t)$$\n    The same update rule applies to the STDP traces $E^{\\mathrm{pre}}$ and $E^{\\mathrm{post}}$ with their respective time constants $\\tau_+$ and $\\tau_-$.\n\n*   **Membrane Potential:** The LIF equation is an Ornstein-Uhlenbeck process. Interpreting $G \\cdot I_{syn}$ as a current input that is filtered by the membrane, the integrated update rule is:\n    $$V_j(t) \\leftarrow V_j(t-\\Delta t) \\cdot \\exp(-\\Delta t/\\tau_m) + \\left(V_{\\mathrm{rest}} + G \\cdot I_{\\mathrm{syn},j}(t)\\right) \\cdot (1 - \\exp(-\\Delta t/\\tau_m))$$\n    This approach correctly treats $G$ as a resistance-like term and ensures dimensional consistency and numerical stability. After this update, if $V_j(t)$ exceeds the threshold $V_{\\mathrm{th},j}$, a spike is registered, and $V_j$ is reset to $V_{\\mathrm{reset}}$ and clamped for the refractory period.\n\n**2. Simulation Protocol Implementation:**\nThe overall simulation follows a strict, reproducible protocol for each test case.\n\n*   **Initialization:** A random number generator is seeded. This ensures that the generated input patterns, spike trains, and initial synaptic weights are identical for both the baseline and immature-cell simulations within a single test case. Initial weights $w_{ji}$ are drawn from a uniform distribution, e.g., $U(0, 0.01)$, to start from small, unbiased values.\n*   **Input Generation:** Input patterns with controlled correlation and presynaptic Poisson spike trains are generated once and stored, ensuring that network training and retrieval phases use the exact same stimuli.\n*   **Network Simulation (`run_network` function):** A core function simulates the network dynamics over time. This function is parameterized to handle both mature and immature neuron properties. It encapsulates two phases:\n    1.  **Training Phase:** The network is presented with each of the $P$ patterns. The simulation proceeds step-by-step, updating all state variables (traces, potentials) and accumulating the potentiation and depression terms for the STDP update ($\\sum s^{\\mathrm{post}} E^{\\mathrm{pre}}$ and $\\sum E^{\\mathrm{post}} s^{\\mathrm{pre}}$). At the end of each pattern presentation, the synaptic weights $W$ are updated according to the full STDP rule, including decay and clipping.\n    2.  **Retrieval Phase:** Using the final weights from the training phase, each pattern is presented again. The simulation is run without any plasticity mechanisms. The set of neurons $S_p$ that fire at least one spike is recorded for each pattern $p$.\n*   **Metric Calculation:** The list of sets $\\{S_p\\}$ is used to compute the interference index $I$ by averaging the Jaccard overlap over all unique pairs of patterns.\n*   **Comparative Analysis:** The `run_network` function is called twice. First, with all neurons configured as \"mature\" to calculate $I_{\\mathrm{baseline}}$. Second, with $N_{\\mathrm{imm}}$ neurons configured as \"immature\" (higher plasticity $\\gamma$, lower threshold $V_{\\mathrm{th},\\mathrm{imm}}$) to calculate $I_{\\mathrm{immature}}$. The final result for the test case is $\\Delta I = I_{\\mathrm{immature}} - I_{\\mathrm{baseline}}$.\n\nThe implementation heavily leverages `numpy` for vectorized operations across neurons and synapses, which is crucial for efficient computation. The logic is carefully ordered within each time step to ensure causal updates, particularly for the STDP mechanism.", "answer": "```python\nimport numpy as np\n\n# Fixed model constants as specified in the problem\nCONSTANTS = {\n    'V_rest': -65.0, 'V_reset': -65.0, 'V_th_mat': -52.0, 'V_th_imm': -55.0,\n    'tau_m': 20.0, 'tau_syn': 5.0, 'tau_plus': 20.0, 'tau_minus': 20.0,\n    'G': 3.0, 'lambda_w': 0.001, 'w_max': 0.1,\n    'A_plus_base': 0.0005, 'A_minus_base': 0.0006,\n    'dt': 1.0, 'T': 150.0, 'p_act': 0.2,\n    'r_high': 20.0, 'r_low': 2.0, 'refractory_period': 3.0\n}\n\ndef generate_patterns(M, P, p_act, c, rng):\n    \"\"\"Generates P correlated binary input patterns of size M.\"\"\"\n    num_active = int(p_act * M)\n    num_common = int(c * p_act * M)\n    num_unique = num_active - num_common\n\n    all_indices = np.arange(M)\n    if num_common > M: num_common = M\n    common_indices = rng.choice(all_indices, size=num_common, replace=False)\n    \n    remaining_indices = np.setdiff1d(all_indices, common_indices, assume_unique=True)\n    \n    patterns = []\n    for _ in range(P):\n        p = np.zeros(M, dtype=bool)\n        p[common_indices] = True\n        if num_unique > 0:\n            if num_unique > len(remaining_indices):\n                unique_indices = rng.choice(remaining_indices, size=len(remaining_indices), replace=False)\n            else:\n                unique_indices = rng.choice(remaining_indices, size=num_unique, replace=False)\n            p[unique_indices] = True\n        patterns.append(p)\n        \n    return np.array(patterns)\n\ndef generate_spikes(patterns, T, dt, r_high, r_low, rng):\n    \"\"\"Generates Poisson spike trains for each pattern.\"\"\"\n    P, M = patterns.shape\n    num_steps = int(T / dt)\n    \n    p_high = r_high * dt / 1000.0\n    p_low = r_low * dt / 1000.0\n    \n    spike_trains = []\n    for p_idx in range(P):\n        pattern = patterns[p_idx]\n        probs = np.where(pattern, p_high, p_low)\n        spikes = rng.random((num_steps, M))  probs\n        spike_trains.append(spikes)\n        \n    return spike_trains\n\ndef calculate_interference(S_list, P):\n    \"\"\"Calculates the average Jaccard overlap.\"\"\"\n    if P  2:\n        return 0.0\n    \n    overlaps = []\n    for i in range(P):\n        for j in range(i + 1, P):\n            set_p = S_list[i]\n            set_q = S_list[j]\n            \n            intersection = len(set_p.intersection(set_q))\n            union = len(set_p.union(set_q))\n            \n            if union == 0:\n                overlaps.append(0.0)\n            else:\n                overlaps.append(intersection / union)\n    \n    return np.mean(overlaps) if overlaps else 0.0\n\ndef run_network(sim_params, W_init, spike_trains):\n    \"\"\"Runs the full network simulation for one configuration.\"\"\"\n    N, M, P = sim_params['N'], sim_params['M'], sim_params['P']\n    V_th, A_plus, A_minus = sim_params['V_th'], sim_params['A_plus'], sim_params['A_minus']\n    \n    dt, T = CONSTANTS['dt'], CONSTANTS['T']\n    num_steps = int(T / dt)\n    \n    # Pre-calculate decay factors for efficiency\n    k_m = np.exp(-dt / CONSTANTS['tau_m'])\n    k_syn = np.exp(-dt / CONSTANTS['tau_syn'])\n    k_plus = np.exp(-dt / CONSTANTS['tau_plus'])\n    k_minus = np.exp(-dt / CONSTANTS['tau_minus'])\n    refractory_steps = int(CONSTANTS['refractory_period'] / dt)\n    \n    # Initialize weights\n    W = np.copy(W_init)\n\n    # --- Training Phase ---\n    for p_idx in range(P):\n        spikes_pattern = spike_trains[p_idx]\n        \n        V = np.full(N, CONSTANTS['V_rest'])\n        ref_counters = np.zeros(N, dtype=int)\n        x_trace, E_pre, E_post = np.zeros(M), np.zeros(M), np.zeros(N)\n        \n        LTP_contrib = np.zeros((N, M))\n        LTD_contrib = np.zeros((N, M))\n        \n        for t in range(num_steps):\n            x_trace *= k_syn\n            E_pre *= k_plus\n            E_post *= k_minus\n            \n            pre_spike_indices = np.where(spikes_pattern[t, :])[0]\n            if pre_spike_indices.size > 0:\n                LTD_contrib[:, pre_spike_indices] += E_post[:, np.newaxis]\n                x_trace[pre_spike_indices] += 1.0\n                E_pre[pre_spike_indices] += 1.0\n\n            I_syn = W @ x_trace\n            \n            not_in_ref = (ref_counters == 0)\n            V = V * k_m + (CONSTANTS['V_rest'] + CONSTANTS['G'] * I_syn) * (1 - k_m)\n            V[~not_in_ref] = CONSTANTS['V_reset']\n            \n            ref_counters[ref_counters > 0] -= 1\n            \n            post_spike_indices = np.where((V > V_th)  not_in_ref)[0]\n            if post_spike_indices.size > 0:\n                LTP_contrib[post_spike_indices, :] += E_pre\n                E_post[post_spike_indices] += 1.0\n                V[post_spike_indices] = CONSTANTS['V_reset']\n                ref_counters[post_spike_indices] = refractory_steps\n\n        delta_W = A_plus[:, np.newaxis] * LTP_contrib - A_minus[:, np.newaxis] * LTD_contrib\n        W += delta_W - CONSTANTS['lambda_w'] * W\n        W = np.clip(W, 0, CONSTANTS['w_max'])\n\n    # --- Retrieval Phase ---\n    S_list = []\n    for p_idx in range(P):\n        spikes_pattern = spike_trains[p_idx]\n        \n        V = np.full(N, CONSTANTS['V_rest'])\n        ref_counters = np.zeros(N, dtype=int)\n        x_trace = np.zeros(M)\n        output_spikes_this_pattern = np.zeros(N, dtype=bool)\n\n        for t in range(num_steps):\n            x_trace *= k_syn\n            pre_spike_indices = np.where(spikes_pattern[t, :])[0]\n            if pre_spike_indices.size > 0:\n                x_trace[pre_spike_indices] += 1.0\n                \n            I_syn = W @ x_trace\n            not_in_ref = (ref_counters == 0)\n\n            V = V * k_m + (CONSTANTS['V_rest'] + CONSTANTS['G'] * I_syn) * (1 - k_m)\n            V[~not_in_ref] = CONSTANTS['V_reset']\n            \n            ref_counters[ref_counters > 0] -= 1\n            \n            post_spike_indices = np.where((V > V_th)  not_in_ref)[0]\n            if post_spike_indices.size > 0:\n                output_spikes_this_pattern[post_spike_indices] = True\n                V[post_spike_indices] = CONSTANTS['V_reset']\n                ref_counters[post_spike_indices] = refractory_steps\n\n        S_list.append(set(np.where(output_spikes_this_pattern)[0]))\n        \n    return calculate_interference(S_list, P)\n\ndef solve():\n    test_cases = [\n        # N, M, N_imm, gamma, P, c, seed\n        (100, 50, 30, 2.0, 6, 0.2, 123),\n        (100, 50, 0, 2.0, 6, 0.2, 124),\n        (120, 60, 60, 3.0, 8, 0.3, 125),\n        (140, 60, 50, 2.5, 12, 0.4, 126),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, M, N_imm, gamma, P, c, seed = case\n        \n        rng = np.random.default_rng(seed)\n        \n        patterns = generate_patterns(M, P, CONSTANTS['p_act'], c, rng)\n        spike_trains = generate_spikes(patterns, CONSTANTS['T'], CONSTANTS['dt'], CONSTANTS['r_high'], CONSTANTS['r_low'], rng)\n        W_init = rng.uniform(0, 0.01, size=(N, M))\n\n        # --- Baseline run (all mature) ---\n        params_base = {\n            'N': N, 'M': M, 'P': P,\n            'V_th': np.full(N, CONSTANTS['V_th_mat']),\n            'A_plus': np.full(N, CONSTANTS['A_plus_base']),\n            'A_minus': np.full(N, CONSTANTS['A_minus_base'])\n        }\n        I_baseline = run_network(params_base, W_init, spike_trains)\n        \n        # --- Immature run ---\n        V_th_imm = np.full(N, CONSTANTS['V_th_mat'])\n        A_plus_imm = np.full(N, CONSTANTS['A_plus_base'])\n        A_minus_imm = np.full(N, CONSTANTS['A_minus_base'])\n        \n        if N_imm > 0:\n            immature_indices = np.arange(N_imm) # Designate first N_imm as immature\n            V_th_imm[immature_indices] = CONSTANTS['V_th_imm']\n            A_plus_imm[immature_indices] = gamma * CONSTANTS['A_plus_base']\n            A_minus_imm[immature_indices] = gamma * CONSTANTS['A_minus_base']\n        \n        params_imm = {\n            'N': N, 'M': M, 'P': P,\n            'V_th': V_th_imm,\n            'A_plus': A_plus_imm,\n            'A_minus': A_minus_imm\n        }\n        I_immature = run_network(params_imm, W_init, spike_trains)\n\n        delta_I = I_immature - I_baseline\n        results.append(round(delta_I, 3))\n\n    print(f\"[{','.join(f'{r:.3f}' for r in results)}]\")\n\nsolve()\n```", "id": "4732943"}]}