{"hands_on_practices": [{"introduction": "Effective clinical practice begins with accurate identification. In settings like primary care, where the prevalence of any single disorder such as Specific Phobia is relatively low, interpreting the results of a screening test requires careful probabilistic reasoning. This exercise [@problem_id:4761015] challenges you to apply Bayes' theorem to determine the actual probability that a patient has a Specific Phobia given a positive result from a screening tool. By deriving the posterior probability from fundamental principles of sensitivity, specificity, and prevalence, you will gain a deeper appreciation for why a test's raw performance metrics are not enough and how base rates critically shape a test's predictive value in the real world.", "problem": "A primary care clinic uses a validated ultrabrief screening instrument for Specific Phobia. In the clinic’s adult population, the point prevalence (prior probability) of Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5) Specific Phobia is $0.05$. The instrument has sensitivity $Se=0.85$ and specificity $Sp=0.90$. A patient screens positive.\n\nStarting from the core definitions of sensitivity, specificity, prevalence, and Bayes’ theorem, derive an expression for the posterior probability that the patient truly has Specific Phobia given a positive screen, and then compute its numerical value using the parameters provided. Express the final posterior probability as a decimal and round your answer to four significant figures.\n\nFinally, succinctly interpret the clinical implications of this posterior probability in terms of decision-making for further diagnostic assessment versus immediate treatment within primary care. Do not include any interpretation in your final numerical answer.", "solution": "Let $D$ denote the event that a patient has Specific Phobia, and let $D^c$ denote the event that the patient does not have Specific Phobia. Let $T+$ denote the event of a positive test result and $T-$ denote the event of a negative test result.\n\nThe problem provides the following information:\n1.  The point prevalence, or prior probability of the disease: $P(D) = 0.05$.\n2.  The sensitivity of the instrument: $Se = P(T+|D) = 0.85$. This is the conditional probability of a positive test given that the patient has the disease.\n3.  The specificity of the instrument: $Sp = P(T-|D^c) = 0.90$. This is the conditional probability of a negative test given that the patient does not have the disease.\n\nWe are asked to derive an expression for the posterior probability that the patient truly has Specific Phobia given a positive screen, which is denoted as $P(D|T+)$, and then to compute its value.\n\nWe begin with the definition of conditional probability, which forms the basis of Bayes' theorem:\n$$P(D|T+) = \\frac{P(D \\cap T+)}{P(T+)}$$\nThe numerator, $P(D \\cap T+)$, can be re-expressed using the definition of conditional probability again: $P(D \\cap T+) = P(T+|D) P(D)$.\nThe denominator, $P(T+)$, is the total probability of a positive test. It can be calculated by considering the two mutually exclusive and exhaustive scenarios in which a positive test can occur: a true positive (patient has the disease and tests positive) and a false positive (patient does not have the disease but tests positive). Using the law of total probability:\n$$P(T+) = P(T+ \\cap D) + P(T+ \\cap D^c)$$\nAgain, applying the definition of conditional probability to each term:\n$$P(T+) = P(T+|D)P(D) + P(T+|D^c)P(D^c)$$\n\nWe are given $P(T+|D) = Se$ and $P(D)$. We need to determine the remaining terms, $P(D^c)$ and $P(T+|D^c)$.\nThe probability of not having the disease, $P(D^c)$, is the complement of the prevalence:\n$$P(D^c) = 1 - P(D)$$\nThe probability $P(T+|D^c)$ is the false positive rate. It is the complement of the specificity, $Sp = P(T-|D^c)$, because a patient who does not have the disease can either test negative (specificity) or test positive (false positive). Thus:\n$$P(T+|D^c) = 1 - P(T-|D^c) = 1 - Sp$$\n\nSubstituting these expressions back into the formula for $P(T+)$:\n$$P(T+) = P(T+|D)P(D) + (1 - Sp)(1 - P(D))$$\n\nNow we substitute the full expression for $P(T+)$ into the initial Bayes' theorem formula for $P(D|T+)$:\n$$P(D|T+) = \\frac{P(T+|D) P(D)}{P(T+|D)P(D) + (1 - Sp)(1 - P(D))}$$\nThis is the derived analytical expression for the posterior probability in terms of prevalence, sensitivity, and specificity.\n\nNext, we compute the numerical value using the given parameters: $P(D) = 0.05$, $Se = 0.85$, and $Sp = 0.90$.\n\nFirst, we calculate the probability of a true positive, which is the numerator:\n$$P(T+ \\cap D) = Se \\cdot P(D) = 0.85 \\times 0.05 = 0.0425$$\n\nNext, we calculate the probability of a false positive, which is the second term in the denominator:\n$$P(T+ \\cap D^c) = P(T+|D^c)P(D^c) = (1 - Sp)(1 - P(D))$$\n$$P(T+ \\cap D^c) = (1 - 0.90)(1 - 0.05) = 0.10 \\times 0.95 = 0.0950$$\n\nThe total probability of a positive test, $P(T+)$, is the sum of the true positive and false positive probabilities:\n$$P(T+) = P(T+ \\cap D) + P(T+ \\cap D^c) = 0.0425 + 0.0950 = 0.1375$$\n\nFinally, we compute the posterior probability, $P(D|T+)$:\n$$P(D|T+) = \\frac{0.0425}{0.1375} \\approx 0.30909090...$$\n\nRounding the result to four significant figures as requested, we get:\n$$P(D|T+) \\approx 0.3091$$\n\nThis value, often called the positive predictive value (PPV) of the test, represents the probability that a patient with a positive screening result actually has the disease.\n\nThe clinical implication of this posterior probability of approximately $31\\%$ is that a positive result on this ultrabrief screening instrument is not, by itself, a sufficient basis for a definitive diagnosis of Specific Phobia or to initiate treatment. Even after a positive screen, a patient is still more likely to not have the disorder (a probability of $1 - 0.3091 \\approx 69\\%$) than to have it. The screen successfully identifies a subpopulation at higher risk than the general population (risk increased from $5\\%$ to $31\\%$), but the rate of false positives is substantial. Therefore, the correct clinical action following a positive screen is to proceed with a more thorough diagnostic assessment, such as a structured clinical interview conducted by a mental health professional, to confirm or rule out the diagnosis before considering a treatment plan.", "answer": "$$ \\boxed{0.3091} $$", "id": "4761015"}, {"introduction": "Once a diagnosis is considered, the next step is to quantify the phobia's severity and track changes during treatment, which requires objective and reliable measurement. This practice [@problem_id:4761048] introduces the Behavioral Approach Test (BAT), a gold standard for assessing avoidance behavior, and asks you to apply principles from Classical Test Theory to determine if a patient's improvement is clinically meaningful. You will derive and calculate the Reliable Change Index (RCI), a powerful psychometric tool that helps clinicians distinguish true therapeutic progress from the inherent noise of measurement error, ensuring that decisions about treatment efficacy are data-driven.", "problem": "A clinician plans to objectively assess and track acrophobia using a Behavioral Approach Test (BAT) structured across discrete height-exposure levels. The BAT comprises $10$ levels, each representing a progressively higher vantage point, with exposure parameters standardized across sessions. The patient’s task is to approach and remain at each level for $2$ minutes. The BAT score is defined as the highest level fully completed without violating stopping rules. Levels are as follows: level $1$ ($3$ m balcony), level $2$ ($6$ m balcony), level $3$ ($9$ m balcony), level $4$ ($12$ m balcony), level $5$ ($15$ m balcony), level $6$ ($18$ m balcony), level $7$ ($21$ m balcony), level $8$ ($24$ m balcony), level $9$ ($27$ m balcony), level $10$ ($30$ m rooftop observation deck). Objective stopping rules are applied uniformly at each level: terminate the level if any of the following occur during the $2$-minute exposure window: (i) Subjective Units of Distress Scale (SUDS) reaches $\\geq 80$ for $\\geq 60$ consecutive seconds, (ii) heart rate exceeds $150$ beats per minute for $\\geq 30$ seconds, (iii) observable panic behaviors (e.g., fleeing, refusal, or freezing) persist for $\\geq 30$ seconds, or (iv) safety behaviors (e.g., gripping rail with both hands, avoiding edge by $\\geq 2$ m) cannot be reduced after a coaching prompt within $30$ seconds. The BAT is administered consistently across sessions under these rules.\n\nAssume that the BAT total score (highest level completed) has established test-retest reliability $r$ and normative standard deviation $\\sigma$ from a stable reference population. A single patient’s BAT score is recorded at session $1$ and session $6$. You are to use the framework of Classical Test Theory as the fundamental base, starting from the decomposition of observed scores into true score and error, and the definition of reliability in terms of variance components. Derive, from first principles, the index that quantifies whether the observed change exceeds what is expected from measurement error alone, and then compute its value for the following parameters: test-retest reliability $r = 0.90$, normative standard deviation $\\sigma = 2.5$ levels, observed BAT at session $1$ is $X_{1} = 3$ levels, and observed BAT at session $6$ is $X_{6} = 8$ levels. Express the final index as a single real-valued number. Round your answer to four significant figures. No units are required.", "solution": "### Derivation and Solution\nThe problem requires the derivation of an index to quantify whether an observed change in scores between two test administrations is statistically meaningful, i.e., greater than what would be expected due to measurement error alone. This index is known as the Reliable Change Index (RCI). The derivation will proceed from the first principles of Classical Test Theory (CTT).\n\nAccording to CTT, any observed score, $X$, is composed of a true score, $T$, and a random error component, $E$.\n$$X = T + E$$\nThe error component $E$ is assumed to have a mean of zero and be uncorrelated with the true score $T$.\n\nThe variance of the observed scores, $\\sigma_X^2$, can be decomposed into the variance of the true scores, $\\sigma_T^2$, and the variance of the error scores, $\\sigma_E^2$:\n$$\\sigma_X^2 = \\sigma_T^2 + \\sigma_E^2$$\n\nThe reliability of a test, $r$, is defined as the proportion of the observed score variance that is attributable to the true score variance. For test-retest reliability, this is denoted as $r_{XX'}$.\n$$r = \\frac{\\sigma_T^2}{\\sigma_X^2}$$\n\nFrom this definition, we can express the error variance, $\\sigma_E^2$, in terms of the observed score variance and reliability.\n$$r = \\frac{\\sigma_X^2 - \\sigma_E^2}{\\sigma_X^2} = 1 - \\frac{\\sigma_E^2}{\\sigma_X^2}$$\nRearranging for $\\sigma_E^2$ gives:\n$$\\sigma_E^2 = \\sigma_X^2 (1 - r)$$\nThe standard deviation of the error distribution, $\\sigma_E$, is called the Standard Error of Measurement (SEM).\n$$SEM = \\sigma_E = \\sigma_X \\sqrt{1 - r}$$\nIn this problem, the normative standard deviation $\\sigma$ is equivalent to $\\sigma_X$.\n$$SEM = \\sigma \\sqrt{1 - r}$$\n\nWe are interested in the change between two observed scores, $X_1$ from session $1$ and $X_6$ from session $6$. The difference score is $D = X_6 - X_1$. To determine if this change is \"reliable,\" we must compare it to the amount of change expected from measurement error alone. This requires calculating the standard deviation of the distribution of difference scores, which is known as the Standard Error of the Difference ($S_{diff}$).\n\nThe difference score $D$ can be written in terms of true and error scores:\n$$D = X_6 - X_1 = (T_6 + E_6) - (T_1 + E_1) = (T_6 - T_1) + (E_6 - E_1)$$\nTo evaluate the reliability of the change, we consider the null hypothesis that no true change has occurred, i.e., $T_6 - T_1 = 0$. Under this hypothesis, any observed difference is due solely to measurement error, $D = E_6 - E_1$.\n\nThe variance of this error of difference, $\\sigma_{diff}^2$, is given by:\n$$\\sigma_{diff}^2 = Var(E_6 - E_1)$$\nAssuming the errors of the two administrations are uncorrelated ($Cov(E_6, E_1) = 0$), a standard assumption in CTT, the variance of the difference is the sum of the variances:\n$$\\sigma_{diff}^2 = Var(E_6) + Var(E_1)$$\nSince both scores come from the same test administered under standard conditions, we assume their error variances are equal to the squared Standard Error of Measurement:\n$$Var(E_6) = Var(E_1) = SEM^2$$\nTherefore,\n$$\\sigma_{diff}^2 = SEM^2 + SEM^2 = 2 \\cdot SEM^2$$\nThe Standard Error of the Difference, $S_{diff}$, is the square root of this variance:\n$$S_{diff} = \\sqrt{2 \\cdot SEM^2} = SEM \\sqrt{2}$$\nSubstituting the expression for $SEM$:\n$$S_{diff} = (\\sigma \\sqrt{1 - r}) \\sqrt{2} = \\sigma \\sqrt{2(1 - r)}$$\nThis is the standard deviation of the distribution of change scores that would be expected if no real change had occurred.\n\nThe Reliable Change Index (RCI) is a $z$-score that standardizes the observed change $X_6 - X_1$ by dividing it by the Standard Error of the Difference, $S_{diff}$.\n$$RCI = \\frac{X_6 - X_1}{S_{diff}}$$\nSubstituting the derived formula for $S_{diff}$:\n$$RCI = \\frac{X_6 - X_1}{\\sigma \\sqrt{2(1 - r)}}$$\nThis is the derived index for quantifying whether the observed change exceeds what is expected from measurement error alone.\n\nNow, we compute its value using the provided parameters:\n-   $X_1 = 3$\n-   $X_6 = 8$\n-   $\\sigma = 2.5$\n-   $r = 0.90$\n\nFirst, calculate the observed difference:\n$$X_6 - X_1 = 8 - 3 = 5$$\nNext, calculate the Standard Error of the Difference, $S_{diff}$:\n$$S_{diff} = 2.5 \\sqrt{2(1 - 0.90)} = 2.5 \\sqrt{2(0.10)} = 2.5 \\sqrt{0.20}$$\nNow, compute the RCI:\n$$RCI = \\frac{5}{2.5 \\sqrt{0.20}} = \\frac{2}{\\sqrt{0.20}}$$\nTo simplify, $\\sqrt{0.20} = \\sqrt{\\frac{1}{5}} = \\frac{1}{\\sqrt{5}}$.\n$$RCI = \\frac{2}{\\frac{1}{\\sqrt{5}}} = 2\\sqrt{5}$$\nFinally, we calculate the numerical value and round to four significant figures.\n$$RCI = 2 \\times 2.2360679... = 4.4721359...$$\nRounding to four significant figures gives $4.472$.\nA conventional threshold for statistical significance of the RCI is $|RCI| \\ge 1.96$, corresponding to a p-value of $0.05$ (two-tailed). The calculated value of $4.472$ far exceeds this threshold, indicating that the patient's improvement from level $3$ to level $8$ is a statistically reliable change and not likely due to random measurement error.", "answer": "$$ \\boxed{4.472} $$", "id": "4761048"}, {"introduction": "Exposure therapy is a cornerstone of treatment for Specific Phobias, but what are the precise cognitive mechanisms that drive its success? This advanced exercise [@problem_id:4761033] invites you to model the process of belief updating using a Bayesian learning framework, a key concept in computational psychiatry. You will implement a model where a patient's fear is represented by a probability distribution that is updated by the evidence from (safe) exposure trials. By exploring concepts like prior strength and Kullback–Leibler divergence, you will develop a formal understanding of how strongly held phobic beliefs are maintained and how they can be modified through new learning experiences.", "problem": "You are modeling belief updating in specific phobia using a Bayesian learner. In this model, a patient holds a prior belief about the probability of harm, denoted $p$, when encountering a feared cue. The prior over $p$ is a Beta distribution $\\mathrm{Beta}(a,b)$. Exposure sessions produce independent Bernoulli outcomes $x_t \\in \\{0,1\\}$, where $x_t = 1$ represents a harmful outcome and $x_t = 0$ represents a safe outcome. Assume independence and identical distribution across exposures and that the Bernoulli likelihood parameter is $p$.\n\nFundamental base to use:\n- Conjugacy of the Beta prior and Bernoulli likelihood: a $\\mathrm{Beta}(a,b)$ prior with Bernoulli data with $k$ successes in $n$ trials yields a $\\mathrm{Beta}(a+k,b+n-k)$ posterior.\n- The mean of $\\mathrm{Beta}(\\alpha,\\beta)$ is $\\frac{\\alpha}{\\alpha + \\beta}$.\n- The digamma function $\\psi(\\cdot)$ and the log Beta function $\\ln B(\\cdot,\\cdot)$ are well-defined special functions. The Kullback–Leibler divergence from $\\mathrm{Beta}(a_0,b_0)$ to $\\mathrm{Beta}(a_1,b_1)$ has a known closed form in terms of these functions.\n\nTask:\nGiven test cases described by tuples $(a,b,n,k)$, where $a$ and $b$ are the Beta prior parameters, $n$ is the number of exposures, and $k$ is the number of harmful outcomes observed during exposure, implement a program that computes, for each test case:\n- The posterior mean $\\mu_{\\text{post}} = \\frac{a+k}{a+b+n}$.\n- The absolute shift in the belief about harm probability $\\Delta = \\left| \\frac{a+k}{a+b+n} - \\frac{a}{a+b} \\right|$.\n- The prior weight factor $w_{\\text{prior}} = \\frac{a+b}{a+b+n}$, which quantifies how strongly the prior anchors the posterior mean relative to the data.\n- The Kullback–Leibler divergence $D_{\\mathrm{KL}}\\big(\\mathrm{Beta}(a,b)\\,\\|\\,\\mathrm{Beta}(a+k,b+n-k)\\big)$, defined by\n\n$$\nD_{\\mathrm{KL}}\\big(\\mathrm{Beta}(a_0,b_0)\\,\\|\\,\\mathrm{Beta}(a_1,b_1)\\big)\n= \\ln B(a_1,b_1) - \\ln B(a_0,b_0) \n+ (a_0-a_1)\\big(\\psi(a_0)-\\psi(a_0+b_0)\\big) \n+ (b_0-b_1)\\big(\\psi(b_0)-\\psi(a_0+b_0)\\big).\n$$\n\n\nYour program should use the following test suite:\n- Case $1$: $(a,b,n,k) = (80,20,20,0)$.\n- Case $2$: $(a,b,n,k) = (2,2,20,0)$.\n- Case $3$: $(a,b,n,k) = (5,5,0,0)$.\n- Case $4$: $(a,b,n,k) = (20,80,20,12)$.\n- Case $5$: $(a,b,n,k) = (1000,1000,50,0)$.\n- Case $6$: $(a,b,n,k) = (10,10,100,5)$.\n\nAll outputs must be real numbers. For each case, round each of the four outputs to $6$ decimal places. There are no physical units involved.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list $[\\mu_{\\text{post}}, \\Delta, w_{\\text{prior}}, D_{\\mathrm{KL}}]$ for a test case, in the same order as the test suite. For example, the printed structure should look like $[[\\mu_1,\\Delta_1,w_1,D_1],[\\mu_2,\\Delta_2,w_2,D_2],\\dots]$ with no spaces anywhere in the line.\n\nThe goal is to demonstrate, in purely mathematical terms, how large values of $a+b$ slow down belief updating during exposure by keeping $w_{\\text{prior}}$ large and thereby anchoring $\\mu_{\\text{post}}$ closer to the prior mean.", "solution": "The problem asks for the computation of several quantities related to Bayesian belief updating for a series of test cases. A patient's belief about the probability of a harmful outcome, $p$, is modeled by a probability distribution. The prior belief is captured by a Beta distribution, $p \\sim \\mathrm{Beta}(a,b)$, where $a$ and $b$ are its parameters. The mean of this prior distribution, which represents the initial estimated probability of harm, is $\\mu_{\\text{prior}} = \\frac{a}{a+b}$.\n\nThe patient then undergoes $n$ exposure sessions, which are modeled as independent Bernoulli trials. Among these $n$ trials, $k$ harmful outcomes are observed. The likelihood of observing $k$ harmful outcomes in $n$ trials, given the probability $p$, follows a Binomial distribution. Due to the conjugacy between the Beta prior and the Bernoulli/Binomial likelihood, the posterior distribution of $p$ after observing the data is also a Beta distribution. Specifically, the posterior is $p \\mid \\text{data} \\sim \\mathrm{Beta}(a', b')$, where the updated parameters are $a' = a+k$ and $b' = b+n-k$.\n\nThe task is to compute four metrics for each given test case $(a,b,n,k)$:\n\n1.  The posterior mean, $\\mu_{\\text{post}}$: This is the mean of the posterior distribution, $\\mathrm{Beta}(a+k, b+n-k)$. Using the formula for the mean of a Beta distribution, $\\mu = \\alpha/(\\alpha+\\beta)$, we have:\n    $$ \\mu_{\\text{post}} = \\frac{a'}{a'+b'} = \\frac{a+k}{(a+k) + (b+n-k)} = \\frac{a+k}{a+b+n} $$\n    This value represents the updated belief about the probability of harm after the exposure sessions.\n\n2.  The absolute belief shift, $\\Delta$: This quantifies the magnitude of change in the mean belief from prior to posterior. It is defined as:\n    $$ \\Delta = \\left| \\mu_{\\text{post}} - \\mu_{\\text{prior}} \\right| = \\left| \\frac{a+k}{a+b+n} - \\frac{a}{a+b} \\right| $$\n\n3.  The prior weight factor, $w_{\\text{prior}}$: This factor reveals how the posterior mean is constructed as a weighted average of the prior mean and the maximum likelihood estimate of $p$ from the data, $\\hat{p}_{\\text{MLE}} = k/n$. The posterior mean can be expressed as:\n    $$ \\mu_{\\text{post}} = \\frac{a+b}{a+b+n} \\left(\\frac{a}{a+b}\\right) + \\frac{n}{a+b+n} \\left(\\frac{k}{n}\\right) $$\n    The problem defines the prior weight factor as the weight on the prior mean:\n    $$ w_{\\text{prior}} = \\frac{a+b}{a+b+n} $$\n    The term $a+b$ can be interpreted as the \"equivalent sample size\" of the prior. A larger $a+b$ leads to a larger $w_{\\text{prior}}$, indicating that the prior belief is stronger and more resistant to change from new data.\n\n4.  The Kullback–Leibler (KL) divergence, $D_{\\mathrm{KL}}$: This measures the information gain, or the \"distance,\" in moving from the prior distribution to the posterior distribution. The problem provides the formula for the KL divergence from a source distribution $\\mathrm{Beta}(a_0,b_0)$ to a target distribution $\\mathrm{Beta}(a_1,b_1)$:\n    $$ D_{\\mathrm{KL}}\\big(\\mathrm{Beta}(a_0,b_0)\\,\\|\\,\\mathrm{Beta}(a_1,b_1)\\big) = \\ln B(a_1,b_1) - \\ln B(a_0,b_0) + (a_0-a_1)\\big(\\psi(a_0)-\\psi(a_0+b_0)\\big) + (b_0-b_1)\\big(\\psi(b_0)-\\psi(a_0+b_0)\\big) $$\n    In our context, the source distribution is the prior, $\\mathrm{Beta}(a_0,b_0) = \\mathrm{Beta}(a,b)$, and the target is the posterior, $\\mathrm{Beta}(a_1,b_1) = \\mathrm{Beta}(a+k, b+n-k)$. Substituting these parameters gives:\n    $a_0 = a$, $b_0 = b$, $a_1 = a+k$, $b_1 = b+n-k$.\n    Thus, $a_0 - a_1 = -k$ and $b_0 - b_1 = -(n-k)$.\n    The specific formula is:\n    $$ D_{\\mathrm{KL}} = \\ln B(a+k, b+n-k) - \\ln B(a,b) - k\\big(\\psi(a) - \\psi(a+b)\\big) - (n-k)\\big(\\psi(b) - \\psi(a+b)\\big) $$\n    where $\\psi(\\cdot)$ is the digamma function and $\\ln B(\\cdot,\\cdot)$ is the log Beta function, which can be computed using the log-gamma function, $\\ln \\Gamma(\\cdot)$, as $\\ln B(\\alpha, \\beta) = \\ln \\Gamma(\\alpha) + \\ln \\Gamma(\\beta) - \\ln \\Gamma(\\alpha+\\beta)$.\n\nThe implementation will proceed by iterating through the provided test cases. For each tuple $(a,b,n,k)$, the four quantities will be calculated using the formulas above. The special functions $\\psi(\\cdot)$ (digamma) and $\\ln\\Gamma(\\cdot)$ (log-gamma) are available in the `scipy.special` library as `psi` and `gammaln`, respectively. The results for each quantity will be rounded to $6$ decimal places and formatted as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import psi, gammaln\n\ndef solve():\n    \"\"\"\n    Computes belief updating metrics for a series of Bayesian learning scenarios.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (a, b, n, k)\n        (80, 20, 20, 0),\n        (2, 2, 20, 0),\n        (5, 5, 0, 0),\n        (20, 80, 20, 12),\n        (1000, 1000, 50, 0),\n        (10, 10, 100, 5)\n    ]\n\n    all_results = []\n    for case in test_cases:\n        a, b, n, k = case\n\n        # Prior parameters\n        a0, b0 = float(a), float(b)\n        \n        # Posterior parameters\n        a1, b1 = a0 + k, b0 + n - k\n\n        # 1. Posterior mean\n        mu_post = a1 / (a1 + b1)\n\n        # 2. Absolute shift in belief\n        # Check for a+b > 0 to avoid division by zero, although not an issue in given test cases.\n        mu_prior = a0 / (a0 + b0) if (a0 + b0) > 0 else 0\n        delta = abs(mu_post - mu_prior)\n\n        # 3. Prior weight factor\n        w_prior = (a0 + b0) / (a0 + b0 + n)\n\n        # 4. Kullback-Leibler divergence\n        # D_KL(prior || posterior)\n        if (a0 == a1 and b0 == b1):\n            # This occurs when n=0, k=0 (e.g., Case 3). The posterior is the prior. D_KL is 0.\n            d_kl = 0.0\n        else:\n            # log Beta function: log(B(alpha, beta)) = gammaln(alpha) + gammaln(beta) - gammaln(alpha + beta)\n            log_B0 = gammaln(a0) + gammaln(b0) - gammaln(a0 + b0)\n            log_B1 = gammaln(a1) + gammaln(b1) - gammaln(a1 + b1)\n            \n            # Digamma function psi is used\n            psi_a0 = psi(a0)\n            psi_b0 = psi(b0)\n            psi_a0b0 = psi(a0 + b0)\n\n            term1 = log_B1 - log_B0\n            term2 = (a0 - a1) * (psi_a0 - psi_a0b0)\n            term3 = (b0 - b1) * (psi_b0 - psi_a0b0)\n            \n            d_kl = term1 + term2 + term3\n\n        # Assemble the results for the current case, rounded to 6 decimal places\n        case_result = [\n            round(mu_post, 6),\n            round(delta, 6),\n            round(w_prior, 6),\n            round(d_kl, 6)\n        ]\n        all_results.append(case_result)\n\n    # Format the final output string as per the requirements: [[v1,v2,...],[v3,v4,...],...] with no spaces.\n    case_strings = []\n    for case_res in all_results:\n        num_strings = [f\"{num:.6f}\" for num in case_res]\n        case_strings.append(f\"[{','.join(num_strings)}]\")\n    \n    final_output_string = f\"[{','.join(case_strings)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output_string)\n\nsolve()\n```", "id": "4761033"}]}