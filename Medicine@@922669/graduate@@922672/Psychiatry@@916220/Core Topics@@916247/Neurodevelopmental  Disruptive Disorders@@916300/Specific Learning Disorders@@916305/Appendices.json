{"hands_on_practices": [{"introduction": "The foundation of any clinical assessment for a Specific Learning Disorder is the objective quantification of academic skills. This process begins by transforming a student's raw performance on a given task into a standardized score that can be compared against normative data. This exercise [@problem_id:4760708] provides hands-on practice in this fundamental skill, using a number-line estimation task—a common probe of mathematical magnitude representation—to demonstrate how a performance metric like Mean Absolute Percent Error (MAPE) is calculated and subsequently converted into a percentile rank. Mastering this conversion is essential for interpreting test results and determining if a student's performance is, in fact, \"substantially below\" what is expected for their age.", "problem": "A graduate-level clinic is evaluating a child for Specific Learning Disorder with impairment in mathematics. One evidence-based behavioral assay of numerical magnitude representation is the $0$–$100$ number-line estimation task. On each trial, a true target value $y_i$ is presented and the child marks an estimated position $\\hat{y}_i$ on the line. The deviation to be computed is the Mean Absolute Percent Error (MAPE), defined by convention in decimal form (no percent sign), and then transformed into a percentile rank using age-normative distributions.\n\nFor a single session of $n = 12$ trials, the child’s data are as follows (all $y_i > 0$ to avoid undefined division), with $y_i$ denoting the true target and $\\hat{y}_i$ denoting the child’s estimate:\n- Trial $1$: $y_1 = 10$, $\\hat{y}_1 = 13$\n- Trial $2$: $y_2 = 18$, $\\hat{y}_2 = 20$\n- Trial $3$: $y_3 = 25$, $\\hat{y}_3 = 30$\n- Trial $4$: $y_4 = 33$, $\\hat{y}_4 = 36$\n- Trial $5$: $y_5 = 40$, $\\hat{y}_5 = 46$\n- Trial $6$: $y_6 = 47$, $\\hat{y}_6 = 54$\n- Trial $7$: $y_7 = 55$, $\\hat{y}_7 = 64$\n- Trial $8$: $y_8 = 62$, $\\hat{y}_8 = 70$\n- Trial $9$: $y_9 = 70$, $\\hat{y}_9 = 80$\n- Trial $10$: $y_{10} = 78$, $\\hat{y}_{10} = 90$\n- Trial $11$: $y_{11} = 85$, $\\hat{y}_{11} = 95$\n- Trial $12$: $y_{12} = 92$, $\\hat{y}_{12} = 100$\n\nAt the child’s age, the normative distribution of the decimal-form MAPE for this task is approximately Gaussian (normal) with population mean $\\mu = 0.10$ and population standard deviation $\\sigma = 0.04$. Using these normative parameters, convert the child’s deviation to a percentile rank by assuming a normal model and using the Standard Normal Cumulative Distribution Function (CDF) after appropriate standardization.\n\nCompute the child’s MAPE from the data above, then convert it to a percentile rank under the stated normative model. Round the final percentile rank to four significant figures and express it as a decimal (no percent sign).", "solution": "The problem requires the computation of the Mean Absolute Percent Error (MAPE) from a given dataset of true values ($y_i$) and estimated values ($\\hat{y}_i$) for $n=12$ trials. Subsequently, this calculated MAPE is to be converted into a percentile rank using a specified normative Gaussian distribution.\n\nThe first step is to calculate the MAPE. The formula for MAPE, as defined in the problem for a sample of size $n$, is:\n$$ \\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| $$\nThe problem specifies that $n=12$ and provides the data pairs $(y_i, \\hat{y}_i)$ for each trial. We compute the Absolute Percent Error (APE) for each trial, which is the term inside the summation: $\\text{APE}_i = \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|$.\n\nThe APE for each of the $12$ trials is calculated as follows:\n- Trial $1$: $\\text{APE}_1 = \\left| \\frac{10 - 13}{10} \\right| = \\left| \\frac{-3}{10} \\right| = 0.3$\n- Trial $2$: $\\text{APE}_2 = \\left| \\frac{18 - 20}{18} \\right| = \\left| \\frac{-2}{18} \\right| = \\frac{1}{9} \\approx 0.11111$\n- Trial $3$: $\\text{APE}_3 = \\left| \\frac{25 - 30}{25} \\right| = \\left| \\frac{-5}{25} \\right| = \\frac{1}{5} = 0.2$\n- Trial $4$: $\\text{APE}_4 = \\left| \\frac{33 - 36}{33} \\right| = \\left| \\frac{-3}{33} \\right| = \\frac{1}{11} \\approx 0.09091$\n- Trial $5$: $\\text{APE}_5 = \\left| \\frac{40 - 46}{40} \\right| = \\left| \\frac{-6}{40} \\right| = \\frac{3}{20} = 0.15$\n- Trial $6$: $\\text{APE}_6 = \\left| \\frac{47 - 54}{47} \\right| = \\left| \\frac{-7}{47} \\right| = \\frac{7}{47} \\approx 0.14894$\n- Trial $7$: $\\text{APE}_7 = \\left| \\frac{55 - 64}{55} \\right| = \\left| \\frac{-9}{55} \\right| = \\frac{9}{55} \\approx 0.16364$\n- Trial $8$: $\\text{APE}_8 = \\left| \\frac{62 - 70}{62} \\right| = \\left| \\frac{-8}{62} \\right| = \\frac{4}{31} \\approx 0.12903$\n- Trial $9$: $\\text{APE}_9 = \\left| \\frac{70 - 80}{70} \\right| = \\left| \\frac{-10}{70} \\right| = \\frac{1}{7} \\approx 0.14286$\n- Trial $10$: $\\text{APE}_{10} = \\left| \\frac{78 - 90}{78} \\right| = \\left| \\frac{-12}{78} \\right| = \\frac{2}{13} \\approx 0.15385$\n- Trial $11$: $\\text{APE}_{11} = \\left| \\frac{85 - 95}{85} \\right| = \\left| \\frac{-10}{85} \\right| = \\frac{2}{17} \\approx 0.11765$\n- Trial $12$: $\\text{APE}_{12} = \\left| \\frac{92 - 100}{92} \\right| = \\left| \\frac{-8}{92} \\right| = \\frac{2}{23} \\approx 0.08696$\n\nNext, we sum these APE values and divide by $n=12$. To maintain precision, it is best to sum the exact fractions, though summing high-precision decimal representations is also acceptable.\n$$ \\sum_{i=1}^{12} \\text{APE}_i = \\frac{3}{10} + \\frac{1}{9} + \\frac{1}{5} + \\frac{1}{11} + \\frac{3}{20} + \\frac{7}{47} + \\frac{9}{55} + \\frac{4}{31} + \\frac{1}{7} + \\frac{2}{13} + \\frac{2}{17} + \\frac{2}{23} $$\nThe sum is approximately $1.79493187$.\nThe MAPE is this sum divided by $12$:\n$$ \\text{MAPE} = \\frac{1}{12} \\sum_{i=1}^{12} \\text{APE}_i \\approx \\frac{1.79493187}{12} \\approx 0.14957766 $$\nLet us denote the child's calculated MAPE as $x$. So, $x \\approx 0.14957766$.\n\nThe second step is to convert this MAPE score into a percentile rank. The problem states that the normative distribution for MAPE is Gaussian with a population mean $\\mu = 0.10$ and a population standard deviation $\\sigma = 0.04$. We first standardize the child's score $x$ by calculating its z-score:\n$$ z = \\frac{x - \\mu}{\\sigma} $$\nSubstituting the values:\n$$ z = \\frac{0.14957766 - 0.10}{0.04} = \\frac{0.04957766}{0.04} \\approx 1.2394415 $$\n\nThe percentile rank is the probability that a random variable from the standard normal distribution is less than or equal to this z-score. This is given by the standard normal cumulative distribution function (CDF), denoted by $\\Phi(z)$.\n$$ \\text{Percentile Rank} = \\Phi(z) = \\Phi(1.2394415) $$\nUsing a standard normal distribution table or a computational tool, we find the value of the CDF for this z-score.\n$$ \\Phi(1.2394415) \\approx 0.892398 $$\nIn psychometric terms, a higher error (MAPE) value corresponds to a higher percentile rank, indicating that the child's performance shows more error than that percentage of the normative population.\n\nFinally, the problem asks to round the resulting percentile rank to four significant figures. The calculated value is $0.892398$. The first four significant figures are $8$, $9$, $2$, and $3$. The fifth significant digit is $9$, which is greater than or equal to $5$, so we round up the fourth significant digit.\nThus, the rounded percentile rank is $0.8924$.", "answer": "$$\\boxed{0.8924}$$", "id": "4760708"}, {"introduction": "A robust diagnosis is rarely built upon a single test score; rather, it emerges from the synthesis of a comprehensive assessment battery. This practice [@problem_id:4760670] simulates the complex diagnostic reasoning required in a real-world clinical scenario. By analyzing a detailed case profile—including data on arithmetic fluency, procedural knowledge, word problem-solving, and specific cognitive functions like working memory—you will learn to identify a specific pattern of strengths and weaknesses. This integrative skill is the cornerstone of differential diagnosis, allowing you to move beyond a simple diagnostic label to a nuanced cognitive formulation that directly informs the selection of targeted, evidence-based interventions.", "problem": "A $12$-year-old student is referred for evaluation of Specific Learning Disorder (SLD) with impairment in mathematics. Intelligence quotient is average and there is no history of sensory deficits, neurological conditions, or psychiatric comorbidity. Reading accuracy and comprehension are age-appropriate. A standardized assessment battery yields the following pattern.\n\nTimed single-digit arithmetic fluency task: $78$ items presented over $120$ seconds. The student attempts all items, produces $62$ correct responses, and exhibits a mean latency of $1.9$ seconds per item. Errors are predominantly near-miss arithmetic fact retrieval mistakes (for example, $7+5=13$, $6\\times4=28$). When the identical items are administered untimed, accuracy increases to $76/78$ correct with careful, slow responding, and error types shift almost entirely to self-corrected slips.\n\nUntimed multi-digit computation: On $12$ mixed problems (multi-digit addition with carry, subtraction with borrow, $2$-by-$2$ multiplication, and a single-step division), written algorithmic execution is accurate on $11/12$ items. There are no place-value alignment errors, and procedural steps are organized correctly; however, the student is slower than peers in setting up problems.\n\nWord problem set: $20$ age-appropriate one- to two-step problems. Without external aids, accuracy is $9/20$ ($45\\%$). Error analysis indicates loss of intermediate quantities and failure to maintain the problem schema over steps rather than misinterpretation of vocabulary. When permitted to use a scratchpad to externalize steps and record intermediate results, accuracy increases to $15/20$ ($75\\%$).\n\nWorking memory and speed: Phonological working memory on Digit Span Forward is $5$ and Backward is $3$ (age norms: Forward mean $8$, Backward mean $6$). Visuospatial working memory (Corsi block span) is $7$ (age norm mean $7$). Processing speed index is $80$ (normative mean $100$). Symbolic magnitude comparison (which is a speeded larger-number judgment) yields age-appropriate accuracy and latency.\n\nUsing core definitions of Specific Learning Disorder (persistent difficulties in academic skills that are substantially below expectations, beginning during school-age years, and not attributable to intellectual disabilities, sensory deficits, or inadequate instruction) and mapping task demands to cognitive constructs (for example, arithmetic fact retrieval, procedural knowledge, processing speed, phonological working memory, visuospatial working memory, and schema-based problem solving), select the diagnostic decision rule that most parsimoniously identifies the likely primary locus or loci of impairment for this student and specifies targeted intervention components that follow from that rule.\n\nWhich option best represents an algorithmic decision rule that integrates timed calculation, math problem solving, and working memory to infer the likely impairment loci and recommend targeted interventions?\n\nA. If symbolic magnitude comparison is age-appropriate, then any timed calculation deficit must reflect a core magnitude system impairment; recommend number line training to recalibrate the approximate number system and deprioritize accommodations for speed or memory externalization.\n\nB. If timed arithmetic accuracy is depressed relative to untimed accuracy by at least $20\\%$ and error types are primarily near-miss facts, infer an arithmetic fact retrieval weakness compounded by reduced processing speed; if word problem accuracy improves by at least $25\\%$ with step externalization and phonological working memory is low with intact visuospatial span, infer a secondary phonological working memory limitation affecting schema maintenance. Recommend spaced retrieval and overlearning of math facts, reduced time pressure and fluency practice to address speed, and routine externalization of intermediate steps (for example, scratchpad, worked-example scaffolds) for problem solving.\n\nC. If word problem performance is poor regardless of external aids, infer a language comprehension disorder as the primary locus; recommend syntax and vocabulary intervention, and do not modify timed calculation demands because such changes will not impact the core linguistic deficit.\n\nD. If untimed multi-digit computations show few place-value errors and correct procedural ordering, infer intact procedural knowledge; therefore, label the case as no SLD in mathematics and avoid academic accommodations, focusing instead on general study skills.\n\nE. If processing speed index is below $85$ and visuospatial span is average, classify the deficit as visuospatial working memory; recommend grid paper, alignment cues, and visuospatial training as primary interventions without emphasis on fact retrieval practice or step externalization.", "solution": "The problem statement has been validated and found to be a scientifically grounded, well-posed, and objective case study suitable for diagnostic reasoning. The data provided are internally consistent and sufficient for evaluation.\n\nThe student is a $12$-year-old with average intelligence and no confounding conditions. The diagnostic question concerns Specific Learning Disorder (SLD) with impairment in mathematics. The provided assessment data must be synthesized to determine the primary cognitive impairments.\n\n**Analysis of Assessment Data**\n\n1.  **Arithmetic Fluency vs. Knowledge:**\n    *   On a timed single-digit arithmetic task ($120$ seconds for $78$ items), the student's accuracy is $62/78$, which is approximately $79.5\\%$. The errors are characteristic of fact retrieval failures (e.g., $7+5=13$).\n    *   On the identical task administered without time limits, accuracy improves dramatically to $76/78$, approximately $97.4\\%$.\n    *   This large discrepancy between timed and untimed performance, with errors on the timed task being near-misses, is a classic indicator of an arithmetic fact retrieval deficit. The student knows the facts but cannot access them quickly and automatically. The processing speed index of $80$ (normative mean $100$, standard deviation typically $15$) is also consistent with a general slowness that would compound a retrieval deficit. The reported mean latency of $1.9$ seconds per item, which would sum to $78 \\times 1.9 = 148.2$ seconds (greater than the allotted $120$ seconds), further underscores this slowness, despite the noted logical inconsistency.\n\n2.  **Procedural Knowledge:**\n    *   On untimed multi-digit computation, the student is highly accurate ($11/12$ items) and demonstrates correct execution of algorithms, including proper place-value alignment and sequencing of steps. This indicates that procedural knowledge in mathematics is intact.\n\n3.  **Problem Solving and Working Memory:**\n    *   On a set of $20$ word problems, performance without aids is poor ($9/20$ correct, or $45\\%$). The error pattern points to \"loss of intermediate quantities and failure to maintain the problem schema,\" not a failure to understand language.\n    *   When allowed a scratchpad to externalize information, accuracy improves substantially to $15/20$ ($75\\%$). The absolute improvement is $30$ percentage points ($75\\% - 45\\% = 30\\%$).\n    *   This pattern strongly suggests a working memory limitation. The cognitive test data specify the nature of this limitation:\n        *   Phonological working memory (Digit Span Forward/Backward) is significantly below average (scores of $5$ and $3$ vs. norms of $8$ and $6$). This is the likely culprit for failing to maintain the verbally-presented information in word problems.\n        *   Visuospatial working memory (Corsi block span) is average (score of $7$ vs. norm of $7$).\n\n4.  **Magnitude Representation:**\n    *   Symbolic magnitude comparison is age-appropriate in accuracy and latency. This indicates that the student's basic \"number sense,\" or the core mental representation of numerical quantity, is intact.\n\n**Summary of Cognitive Profile:**\nThe student presents with a clear, dissociated pattern of strengths and weaknesses.\n*   **Deficits**:\n    1.  Arithmetic fact retrieval fluency.\n    2.  General processing speed.\n    3.  Phonological working memory.\n*   **Intact Abilities**:\n    1.  Procedural knowledge of mathematics.\n    2.  Visuospatial working memory.\n    3.  Core magnitude representation (number sense).\n    4.  Language comprehension.\n\nBased on this profile, a diagnosis of SLD with impairment in mathematics is warranted. The specific impairments are in calculation fluency and mathematical reasoning (specifically, applying concepts in multi-step problems due to a working memory bottleneck).\n\n**Evaluation of Options**\n\n**A. If symbolic magnitude comparison is age-appropriate, then any timed calculation deficit must reflect a core magnitude system impairment; recommend number line training to recalibrate the approximate number system and deprioritize accommodations for speed or memory externalization.**\nThe premise (\"if\" clause) is true; symbolic magnitude comparison is age-appropriate. However, the conclusion (\"then\" clause) is a non-sequitur. An intact magnitude system cannot be the cause of an impairment. The reasoning is fundamentally flawed. Furthermore, the recommended interventions target an intact skill (magnitude representation) while actively deprioritizing accommodations for demonstrated deficits (speed and working memory).\n**Verdict:** Incorrect.\n\n**B. If timed arithmetic accuracy is depressed relative to untimed accuracy by at least $20\\%$ and error types are primarily near-miss facts, infer an arithmetic fact retrieval weakness compounded by reduced processing speed; if word problem accuracy improves by at least $25\\%$ with step externalization and phonological working memory is low with intact visuospatial span, infer a secondary phonological working memory limitation affecting schema maintenance. Recommend spaced retrieval and overlearning of math facts, reduced time pressure and fluency practice to address speed, and routine externalization of intermediate steps (for example, scratchpad, worked-example scaffolds) for problem solving.**\nLet us meticulously check the conditions.\n*   **First \"if\" clause**: \"timed arithmetic accuracy is depressed...by at least $20\\%$\". Timed accuracy is $A_t = 62/78 \\approx 79.5\\%$. Untimed accuracy is $A_u = 76/78 \\approx 97.4\\%$. The absolute drop in percentage points is $A_u - A_t \\approx 97.4\\% - 79.5\\% = 17.9\\%$. The relative drop is $(A_u - A_t) / A_u = (14/78) / (76/78) = 14/76 \\approx 18.4\\%$. In either standard interpretation, the value is less than $20\\%$. Thus, this condition is not strictly met. However, the rest of the clause, \"error types are primarily near-miss facts,\" is true.\n*   **First \"infer\" clause**: The inference of \"an arithmetic fact retrieval weakness compounded by reduced processing speed\" is perfectly supported by the data.\n*   **Second \"if\" clause**: \"word problem accuracy improves by at least $25\\%$ with step externalization\". Accuracy improves from $45\\%$ to $75\\%$, an absolute increase of $30$ percentage points. This satisfies the condition. The additional condition that \"phonological working memory is low with intact visuospatial span\" is also true.\n*   **Second \"infer\" clause**: The inference of \"a secondary phonological working memory limitation affecting schema maintenance\" is an excellent summary of the problem-solving data.\n*   **Recommendations**: The interventions (spaced retrieval for facts, fluency practice and reduced time pressure for speed, externalization for working memory) are precisely targeted to the identified deficits.\nDespite the single numerical condition on accuracy depression not being strictly met ($18.4\\%$ vs $20\\%$), every other component of this option—the qualitative descriptions, the logical inferences, the other quantitative condition, and the suite of interventions—is an exact and sophisticated match to the case data. The other options are grossly incorrect. Therefore, this option represents the best and most parsimonious diagnostic rule. The small numerical discrepancy is likely a minor imprecision in the problem design, not a fatal flaw in the option's core logic.\n**Verdict:** Correct.\n\n**C. If word problem performance is poor regardless of external aids, infer a language comprehension disorder as the primary locus; recommend syntax and vocabulary intervention, and do not modify timed calculation demands because such changes will not impact the core linguistic deficit.**\nThe premise \"word problem performance is poor regardless of external aids\" is factually false. Performance improves significantly with aids. The problem also explicitly states that errors were not due to vocabulary or language misinterpretation.\n**Verdict:** Incorrect.\n\n**D. If untimed multi-digit computations show few place-value errors and correct procedural ordering, infer intact procedural knowledge; therefore, label the case as no SLD in mathematics and avoid academic accommodations, focusing instead on general study skills.**\nThe initial premise and inference are correct: procedural knowledge is intact. However, the subsequent conclusion, \"label the case as no SLD in mathematics,\" is a severe error. It ignores the multiple, significant deficits in fact retrieval, processing speed, and working memory which clearly meet the criteria for SLD. Avoiding accommodations is directly contraindicated by the evidence showing their effectiveness.\n**Verdict:** Incorrect.\n\n**E. If processing speed index is below $85$ and visuospatial span is average, classify the deficit as visuospatial working memory; recommend grid paper, alignment cues, and visuospatial training as primary interventions without emphasis on fact retrieval practice or step externalization.**\nThe \"if\" clause is true (PSI is $80$, visuospatial span is average). However, the classification is illogical: it concludes there is a visuospatial working memory deficit from a data point indicating visuospatial working memory is average. The recommendations are misplaced, targeting a non-existent visuospatial problem, while ignoring the clearly identified primary deficits in fact retrieval and phonological working memory.\n**Verdict:** Incorrect.\n\n**Conclusion**\nOption B provides the most accurate and comprehensive synthesis of the student's cognitive profile, linking specific data patterns to underlying impairments and mapping those impairments to evidence-based interventions. Its diagnostic logic is sound and far superior to all other options, notwithstanding a minor numerical imprecision in one of its stated conditions.", "answer": "$$\\boxed{B}$$", "id": "4760670"}, {"introduction": "Following diagnosis and intervention, the critical task for any clinician is to evaluate progress. An observed improvement in a test score is encouraging, but is the change statistically meaningful, or could it be due to random measurement error? This exercise [@problem_id:4760664] introduces the Reliable Change Index (RCI), a powerful and practical tool for making this determination. By calculating the RCI for a student's reading fluency scores, you will learn to apply principles from Classical Test Theory to rigorously assess whether an intervention has produced a genuine effect. This practice is central to evidence-based care, ensuring clinical accountability and the effective monitoring of treatment outcomes.", "problem": "A clinician is monitoring a $9$-year-old learner diagnosed with Specific Learning Disorder (SLD) in reading, focusing on oral reading fluency as part of a treatment plan in psychiatry. The learner’s observed scores on a standardized fluency measure (correct words per minute) are $X_1 = 87$ in the fall and $X_2 = 105$ in the spring. The measure has a population standard deviation $SD = 15$ and a test–retest reliability coefficient $r = 0.90$ that is stable across the school year for this age group. Assume the fall and spring administrations are independent and that the measurement model follows Classical Test Theory (CTT): observed score $X$ is the sum of true score $T$ and random error $E$, and reliability is defined as $r = \\mathrm{Var}(T) / \\mathrm{Var}(X)$.\n\nUsing only CTT fundamentals and well-tested facts about the propagation of independent errors across repeated measurements, derive from first principles the standard error of measurement and the standard error of the difference for two administrations. Then compute the Reliable Change Index (RCI) for the learner’s fluency change from fall to spring, and interpret whether the change exceeds measurement error at the $0.95$ confidence level under the assumption that standardized change scores follow the standard normal distribution. Round your final numerical RCI to four significant figures. Express the final answer without any units.", "solution": "The problem is valid as it is scientifically grounded in Classical Test Theory (CTT), a standard framework in psychometrics, and provides a well-posed, objective, and complete set of data for analysis. All required components for a rigorous solution are present.\n\nThe objective is to determine if the observed change in a learner's oral reading fluency score is statistically reliable. This involves deriving key CTT metrics, calculating the Reliable Change Index (RCI), and interpreting it within a statistical confidence framework.\n\nFirst, we derive the standard error of measurement ($SEM$) from first principles. Classical Test Theory posits that an observed score $X$ is the sum of a true score $T$ and a random error component $E$.\n$$X = T + E$$\nThe error $E$ is assumed to be a random variable with a mean of zero and to be uncorrelated with the true score $T$. The variance of the observed scores, $\\sigma_X^2$, can thus be expressed as the sum of the variance of the true scores, $\\sigma_T^2$, and the variance of the error scores, $\\sigma_E^2$.\n$$\\sigma_X^2 = \\mathrm{Var}(X) = \\mathrm{Var}(T + E) = \\mathrm{Var}(T) + \\mathrm{Var}(E) = \\sigma_T^2 + \\sigma_E^2$$\nThe standard error of measurement ($SEM$) is defined as the standard deviation of the error scores, $SEM = \\sigma_E$.\n\nThe reliability coefficient, $r$, is defined as the proportion of observed score variance that is attributable to true score variance.\n$$r = \\frac{\\sigma_T^2}{\\sigma_X^2}$$\nFrom this definition, we have $\\sigma_T^2 = r \\cdot \\sigma_X^2$. Substituting this into the variance equation gives:\n$$\\sigma_X^2 = r \\cdot \\sigma_X^2 + \\sigma_E^2$$\nSolving for the error variance, $\\sigma_E^2$:\n$$\\sigma_E^2 = \\sigma_X^2 - r \\cdot \\sigma_X^2 = \\sigma_X^2 (1 - r)$$\nTaking the square root gives the formula for the standard error of measurement ($SEM$):\n$$SEM = \\sigma_E = \\sqrt{\\sigma_X^2 (1 - r)} = \\sigma_X \\sqrt{1 - r}$$\nGiven the population standard deviation $SD = \\sigma_X = 15$ and the test-retest reliability $r = 0.90$, we can calculate the $SEM$:\n$$SEM = 15 \\sqrt{1 - 0.90} = 15 \\sqrt{0.10}$$\n\nNext, we derive the standard error of the difference ($S_{diff}$) for two administrations. The difference score, $D$, is $D = X_2 - X_1$. The error associated with this difference score is the difference between the error components of each measurement, $E_2 - E_1$. The standard error of the difference is the standard deviation of this error difference, $S_{diff} = \\sigma_{E_2 - E_1}$.\nThe variance of the difference in errors is given by:\n$$\\mathrm{Var}(E_2 - E_1) = \\mathrm{Var}(E_2) + \\mathrm{Var}(E_1) - 2 \\mathrm{Cov}(E_1, E_2)$$\nA core assumption of CTT for independent test administrations is that the errors are uncorrelated, so $\\mathrm{Cov}(E_1, E_2) = 0$. Furthermore, the error variance is assumed to be constant for the measure, so $\\mathrm{Var}(E_1) = \\mathrm{Var}(E_2) = \\sigma_E^2 = SEM^2$.\nTherefore, the variance of the difference in errors simplifies to:\n$$\\mathrm{Var}(E_2 - E_1) = SEM^2 + SEM^2 = 2 \\cdot SEM^2$$\nThe standard error of the difference, $S_{diff}$, is the square root of this variance:\n$$S_{diff} = \\sqrt{2 \\cdot SEM^2} = SEM \\sqrt{2}$$\nSubstituting the expression for $SEM$:\n$$S_{diff} = (\\sigma_X \\sqrt{1 - r}) \\sqrt{2} = \\sigma_X \\sqrt{2(1 - r)}$$\nUsing the given values:\n$$S_{diff} = 15 \\sqrt{2(1 - 0.90)} = 15 \\sqrt{2(0.10)} = 15 \\sqrt{0.20}$$\n\nNow, we compute the Reliable Change Index (RCI). The RCI is a standardized score that represents the observed change divided by the standard error of the difference. It measures how many standard error units the observed change represents.\n$$RCI = \\frac{X_2 - X_1}{S_{diff}}$$\nThe learner's scores are $X_1 = 87$ (fall) and $X_2 = 105$ (spring). The observed change is $X_2 - X_1 = 105 - 87 = 18$.\n$$RCI = \\frac{18}{15 \\sqrt{0.20}}$$\nWe now calculate the numerical value:\n$$S_{diff} = 15 \\sqrt{0.20} \\approx 15 \\times 0.447213595... \\approx 6.70820393...$$\n$$RCI = \\frac{18}{6.70820393...} \\approx 2.68328157...$$\nRounding to four significant figures, the RCI is $2.683$.\n\nFinally, we interpret this result. The problem states to interpret whether the change exceeds measurement error at the $0.95$ confidence level, assuming the RCI follows a standard normal ($Z$) distribution. For a $0.95$ confidence level, the corresponding significance level is $\\alpha = 1 - 0.95 = 0.05$. For a two-tailed test, we look for the critical value $Z_{crit}$ that leaves an area of $\\alpha/2 = 0.025$ in each tail of the distribution. This critical value is $Z_{crit} \\approx 1.96$.\n\nThe decision rule is: if $|RCI| > Z_{crit}$, the observed change is statistically significant and considered \"reliable,\" meaning it is unlikely to be due solely to random measurement error.\nIn this case, $|2.683| > 1.96$.\nTherefore, we conclude that the learner's improvement in oral reading fluency from $87$ to $105$ correct words per minute is a statistically reliable change at the $0.95$ confidence level. The observed increase is greater than what would be expected from measurement error alone.", "answer": "$$\\boxed{2.683}$$", "id": "4760664"}]}