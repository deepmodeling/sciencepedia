{"hands_on_practices": [{"introduction": "Raw read counts from an RNA-seq experiment are influenced by both the length of a transcript and the total sequencing depth of the library. To make meaningful comparisons, we must normalize these counts. This foundational exercise [@problem_id:5157658] walks you through the process of deriving and calculating two key normalization metrics, Transcripts Per Million (TPM) and Fragments Per Kilobase of transcript per Million mapped reads (FPKM), directly from first principles, illuminating their subtle but important differences.", "problem": "A targeted ribonucleic acid (RNA) sequencing experiment is used to profile three immune-related genes in a diagnostic panel. The sequencer yields fragment counts for each gene, with effective transcript lengths equal to their annotated lengths and negligible bias. Assume that every sequenced fragment maps uniquely to one of these three genes and that there are no other expressed transcripts in the library.\n\nThe fragment counts are $c = [500, 200, 300]$, and the transcript lengths are $\\ell = [2000, 1000, 1500]$ base pairs. Assume single-end sequencing so that each counted fragment corresponds to one read. Using only the core definitions of sampling from the transcriptome proportional to transcript copy number and length, and the standard definitions of Transcripts Per Million (TPM) and Fragments Per Kilobase of transcript per Million mapped reads (FPKM), proceed as follows, without quoting pre-compiled formulas:\n\n1) From first principles, define a length-normalized abundance proportional to the probability that a read arises from a given transcript, and use it to construct expressions for TPM and FPKM within a single sample.\n\n2) Using the given data, compute the TPM and FPKM values for each gene.\n\n3) Prove that, within a single sample, TPM and FPKM are related by a constant multiplicative factor across all genes, and compute this conversion factor $K$ for this sample.\n\n4) Briefly explain, based on the constructions in step 1, why TPM and FPKM can differ in scale within a sample and why TPM is often preferred for cross-sample comparisons in diagnostic settings.\n\nReport only the value of the conversion factor $K$ from step 3, rounded to four significant figures. Do not include any units with your reported value.", "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and internally consistent. It presents a standard, albeit simplified, scenario in transcriptomic data analysis. All necessary data and definitions are provided to proceed with a rigorous solution.\n\nLet the set of genes be indexed by $i$. The given data are the fragment counts, $c_i$, and the transcript lengths in base pairs, $\\ell_i$. For the three genes in the panel, we have:\n$c = [c_1, c_2, c_3] = [500, 200, 300]$\n$\\ell = [\\ell_1, \\ell_2, \\ell_3] = [2000, 1000, 1500]$\n\nThe problem assumes single-end sequencing, so each fragment corresponds to one read.\n\n1) Derivation of TPM and FPKM from First Principles\n\nThe fundamental assumption in RNA sequencing is that the number of fragments, $c_i$, obtained from a transcript species $i$ is proportional to the number of copies of that transcript, $N_i$, and its length, $\\ell_i$. This can be expressed as:\n$$c_i \\propto N_i \\cdot \\ell_i$$\nThis implies that the underlying transcript abundance, $N_i$, is proportional to the read count normalized by the transcript length:\n$$N_i \\propto \\frac{c_i}{\\ell_i}$$\nThe term $\\frac{c_i}{\\ell_i}$ represents a length-normalized abundance. It is proportional to the molar amount of transcript $i$.\n\nTranscripts Per Million (TPM) is designed to represent the relative abundance of a transcript in a background of one million total transcript molecules. The fractional abundance of transcript $i$ relative to the total pool of all transcripts in the sample is given by $\\frac{N_i}{\\sum_j N_j}$. Using the proportionality above:\n$$\\frac{N_i}{\\sum_j N_j} = \\frac{k \\cdot (c_i / \\ell_i)}{\\sum_j (k \\cdot (c_j / \\ell_j))} = \\frac{c_i / \\ell_i}{\\sum_j (c_j / \\ell_j)}$$\nwhere $k$ is the constant of proportionality.\nTo express this as a \"per million\" value, we scale this fraction by $10^6$. Thus, the definition of TPM for gene $i$ is:\n$$\\text{TPM}_i = \\frac{c_i / \\ell_i}{\\sum_j (c_j / \\ell_j)} \\times 10^6$$\nThis construction ensures that the sum of all TPM values in a sample is always $10^6$.\n\nFragments Per Kilobase of transcript per Million mapped reads (FPKM) is constructed differently. It normalizes the count for a gene first by sequencing depth and then by gene length.\nFirst, we find the number of fragments per million mapped reads. The total number of mapped reads is $C_{\\text{total}} = \\sum_j c_j$. Normalizing $c_i$ by this total and scaling to one million gives: $\\frac{c_i}{\\sum_j c_j} \\times 10^6$.\nSecond, this value is normalized by the transcript length in kilobases (kb). The length of transcript $i$ in kb is $\\ell_i' = \\ell_i / 1000$.\nCombining these normalizations, the FPKM for gene $i$ is:\n$$\\text{FPKM}_i = \\frac{c_i / (\\ell_i / 1000)}{\\sum_j c_j / 10^6} = \\frac{c_i \\cdot 1000 \\cdot 10^6}{\\ell_i \\cdot \\sum_j c_j} = \\frac{c_i}{\\ell_i} \\cdot \\frac{10^9}{\\sum_j c_j}$$\n\n2) Computation of TPM and FPKM values\n\nFirst, we compute the length-normalized counts, $c_i / \\ell_i$:\nFor gene $1$: $\\frac{c_1}{\\ell_1} = \\frac{500}{2000} = 0.25$\nFor gene $2$: $\\frac{c_2}{\\ell_2} = \\frac{200}{1000} = 0.20$\nFor gene $3$: $\\frac{c_3}{\\ell_3} = \\frac{300}{1500} = 0.20$\n\nThe sum of these values is $\\sum_j (c_j / \\ell_j) = 0.25 + 0.20 + 0.20 = 0.65$.\n\nNow we compute the TPM values:\n$\\text{TPM}_1 = \\frac{0.25}{0.65} \\times 10^6 \\approx 384615.38$\n$\\text{TPM}_2 = \\frac{0.20}{0.65} \\times 10^6 \\approx 307692.31$\n$\\text{TPM}_3 = \\frac{0.20}{0.65} \\times 10^6 \\approx 307692.31$\n\nNext, we compute the FPKM values. We need the total number of reads:\n$C_{\\text{total}} = \\sum_j c_j = 500 + 200 + 300 = 1000$.\n\nNow we compute the FPKM values using the derived formula:\n$\\text{FPKM}_1 = \\frac{c_1}{\\ell_1} \\cdot \\frac{10^9}{\\sum_j c_j} = 0.25 \\cdot \\frac{10^9}{1000} = 0.25 \\times 10^6 = 250000$\n$\\text{FPKM}_2 = \\frac{c_2}{\\ell_2} \\cdot \\frac{10^9}{\\sum_j c_j} = 0.20 \\cdot \\frac{10^9}{1000} = 0.20 \\times 10^6 = 200000$\n$\\text{FPKM}_3 = \\frac{c_3}{\\ell_3} \\cdot \\frac{10^9}{\\sum_j c_j} = 0.20 \\cdot \\frac{10^9}{1000} = 0.20 \\times 10^6 = 200000$\n\n3) Proof of Proportionality and Calculation of Factor $K$\n\nWe wish to prove that $\\text{TPM}_i = K \\cdot \\text{FPKM}_i$ for a constant $K$ within a single sample. We start with the derived expressions:\n$$\\text{TPM}_i = \\frac{c_i / \\ell_i}{\\sum_j (c_j / \\ell_j)} \\times 10^6 \\quad (1)$$\n$$\\text{FPKM}_i = \\frac{c_i / \\ell_i}{\\sum_j c_j} \\times 10^9 \\quad (2)$$\nFrom equation $(2)$, we can isolate the term $c_i / \\ell_i$:\n$$\\frac{c_i}{\\ell_i} = \\text{FPKM}_i \\cdot \\frac{\\sum_j c_j}{10^9}$$\nNow substitute this expression for $c_i / \\ell_i$ (and similarly for $c_j / \\ell_j$) into equation $(1)$:\n$$\\text{TPM}_i = \\frac{\\text{FPKM}_i \\cdot (\\sum_k c_k / 10^9)}{\\sum_j \\left( \\text{FPKM}_j \\cdot (\\sum_k c_k / 10^9) \\right)} \\times 10^6$$\nThe term $\\frac{\\sum_k c_k}{10^9}$ is a constant for all genes within the sample and can be factored out of the summation in the denominator:\n$$\\text{TPM}_i = \\frac{\\text{FPKM}_i \\cdot \\left(\\frac{\\sum_k c_k}{10^9}\\right)}{\\left(\\frac{\\sum_k c_k}{10^9}\\right) \\cdot \\left(\\sum_j \\text{FPKM}_j\\right)} \\times 10^6$$\nThe constant term cancels, leaving:\n$$\\text{TPM}_i = \\frac{\\text{FPKM}_i}{\\sum_j \\text{FPKM}_j} \\times 10^6$$\nThis can be written as $\\text{TPM}_i = K \\cdot \\text{FPKM}_i$, where the factor $K$ is:\n$$K = \\frac{10^6}{\\sum_j \\text{FPKM}_j}$$\nSince $\\sum_j \\text{FPKM}_j$ is a single value for a given sample, $K$ is a constant for all genes within that sample. This completes the proof.\n\nTo compute the value of $K$ for the given data, we first sum the FPKM values:\n$$\\sum_j \\text{FPKM}_j = 250000 + 200000 + 200000 = 650000$$\nNow, we calculate $K$:\n$$K = \\frac{10^6}{650000} = \\frac{1000000}{650000} = \\frac{100}{65} = \\frac{20}{13}$$\nNumerically, $K \\approx 1.53846...$. Rounding to four significant figures gives $K = 1.538$.\n\n4) Explanation of Scaling Differences and TPM Preference\n\nThe scaling difference between TPM and FPKM within a sample arises from their different normalization denominators. As shown in the derivations above:\n- TPM normalizes by $\\sum_j (c_j / \\ell_j)$, a quantity proportional to the total number of transcript molecules in the sample.\n- FPKM normalizes by $\\sum_j c_j$, the total number of sequenced reads (library size).\n\nBecause these two sums are generally not equal, the absolute values of TPM and FPKM for a given gene will differ.\n\nTPM is often preferred for cross-sample comparisons for a crucial reason. The sum of all TPM values in any sample is, by definition, fixed at $10^6$. This means TPM represents the relative proportion of a gene's transcripts to the total pool of transcripts (its \"mole fraction\"). In contrast, the sum of FPKM values is not constant and can vary significantly from one sample to another. This is because the FPKM normalization factor, total reads $(\\sum_j c_j)$, is itself dependent on the expression profile. A change in the expression of a few abundant, long genes can dramatically alter the total read count, which in turn changes the FPKM values of all other genes, even those whose expression levels have not changed. By providing a stable \"per-sample\" total, TPM offers a more robust measure for comparing the relative abundances of genes across different samples, which is a common requirement in diagnostic applications.\n\nThe final answer requested is the value of the conversion factor $K$.\n$$K = \\frac{20}{13} \\approx 1.538$$", "answer": "$$\\boxed{1.538}$$", "id": "5157658"}, {"introduction": "Once gene expression is quantified, the next step is often to identify which genes change significantly between experimental conditions. This practice problem [@problem_id:5157644] delves into the statistical heart of differential expression analysis, guiding you to derive and calculate the Wald statistic within a Negative Binomial Generalized Linear Model. By working through this example, you will gain a deeper appreciation for the statistical machinery that powers widely used RNA-seq analysis tools.", "problem": "An immunodiagnostic RNA sequencing (RNA-seq) experiment profiles a cytokine gene across two biological conditions, a control and a stimulated state, with $2$ replicates per condition and equal library sizes. For a single gene, the observed raw counts are: control replicates $y_{1}=100$, $y_{2}=110$; stimulated replicates $y_{3}=200$, $y_{4}=220$. Assume a Negative Binomial (NB) noise model with mean $\\mu_{i}$ and variance $\\operatorname{Var}(Y_{i})=\\mu_{i}+\\alpha \\mu_{i}^{2}$, with known dispersion $\\alpha=0.1$. Use a generalized linear model (GLM) with a logarithmic link, $\\eta_{i}=x_{i}^{\\top}\\beta$ and $\\mu_{i}=\\exp(\\eta_{i})$, where the design matrix has an intercept and a condition indicator: for $i\\in\\{1,2\\}$ (control) $x_{i}=(1,0)^{\\top}$ and for $i\\in\\{3,4\\}$ (stimulated) $x_{i}=(1,1)^{\\top}$. Library sizes are equal, so no offset is needed.\n\nStarting from the NB mean–variance relationship, the logarithmic link, and the large-sample normality of the maximum likelihood estimator (MLE), derive the Wald statistic for testing $H_{0}:\\beta_{1}=0$ (no log fold change between conditions) under this design. Compute its numerical value for the given data and $\\alpha$. Round your final Wald statistic to four significant figures. Express the final answer as a unitless value. Briefly interpret the significance of the result in the context of differential expression under this specified design matrix, but the only value to report is the Wald statistic itself.", "solution": "The user wants to derive and compute the Wald statistic for testing the hypothesis of no differential expression ($H_{0}:\\beta_{1}=0$) in a simplified RNA-seq experiment.\n\nThe problem is first validated.\n\n### Step 1: Extract Givens\n- **Experimental Design**: $2$ conditions (control, stimulated), $2$ replicates per condition.\n- **Observed Counts ($y_{i}$)**: Control: $y_{1}=100$, $y_{2}=110$. Stimulated: $y_{3}=200$, $y_{4}=220$.\n- **Statistical Model**: Negative Binomial (NB) generalized linear model (GLM).\n- **Mean-Variance Relationship**: $\\operatorname{Var}(Y_{i})=\\mu_{i}+\\alpha \\mu_{i}^{2}$.\n- **Dispersion Parameter**: Known constant $\\alpha=0.1$.\n- **Link Function**: Logarithmic, $\\mu_{i}=\\exp(\\eta_{i})$.\n- **Linear Predictor**: $\\eta_{i}=x_{i}^{\\top}\\beta$, with $\\beta=(\\beta_{0}, \\beta_{1})^{\\top}$.\n- **Design Matrix ($X$)**: For control ($i\\in\\{1,2\\}$), $x_{i}=(1,0)^{\\top}$. For stimulated ($i\\in\\{3,4\\}$), $x_{i}=(1,1)^{\\top}$.\n- **Library Sizes**: Assumed equal, no offset term is needed.\n- **Core Task**: Derive the Wald statistic for testing $H_{0}:\\beta_{1}=0$.\n- **Assumption**: Large-sample normality of the Maximum Likelihood Estimator (MLE).\n- **Final Output**: Numerical value of the Wald statistic, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically sound, employing a standard Negative Binomial GLM framework widely used in transcriptome analysis. It is well-posed, providing all necessary data (counts, dispersion, model structure) to compute the requested statistic. The language is objective and precise. The data and parameters are realistic. The assumption of a known dispersion simplifies the calculation but is a valid constraint for a well-defined problem. No contradictions, ambiguities, or logical flaws are present.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution will proceed.\n\n### Derivation and Calculation\n\nThe Wald statistic for testing the null hypothesis $H_{0}: \\beta_{1}=0$ is given by the ratio of the maximum likelihood estimate (MLE) of the parameter to its standard error:\n$$\nW = \\frac{\\hat{\\beta}_{1} - 0}{\\text{SE}(\\hat{\\beta}_{1})} = \\frac{\\hat{\\beta}_{1}}{\\text{SE}(\\hat{\\beta}_{1})}\n$$\nUnder the null hypothesis and for a sufficiently large sample size, this statistic is approximately distributed as a standard normal random variable, $W \\sim \\mathcal{N}(0,1)$. The derivation requires finding the MLE $\\hat{\\beta}_{1}$ and its standard error.\n\n#### 1. Maximum Likelihood Estimation of $\\beta$\n\nThe parameters of the GLM are estimated by maximizing the log-likelihood function. For a Negative Binomial distribution with mean $\\mu_{i}$ and dispersion $\\alpha$, the log-likelihood for a set of $n$ independent observations $\\{y_i\\}_{i=1}^n$ is:\n$$\nL(\\beta) = \\sum_{i=1}^{n} \\left[ \\ln\\left(\\frac{\\Gamma(y_i + \\alpha^{-1})}{\\Gamma(y_i+1)\\Gamma(\\alpha^{-1})}\\right) + y_i\\ln\\left(\\frac{\\alpha\\mu_i}{1+\\alpha\\mu_i}\\right) + \\frac{1}{\\alpha}\\ln\\left(\\frac{1}{1+\\alpha\\mu_i}\\right) \\right]\n$$\nwhere $\\mu_i = \\exp(x_i^\\top \\beta)$. To find the MLEs, we solve the score equations, where the partial derivatives of $L(\\beta)$ with respect to each parameter $\\beta_j$ are set to zero. The score equation for observation $i$ with respect to the vector $\\beta$ is:\n$$\n\\frac{\\partial \\ell_i}{\\partial \\beta} = \\frac{y_i - \\mu_i}{1+\\alpha\\mu_i} x_i\n$$\nThe score equations for the full dataset are $\\sum_{i=1}^{n} \\frac{y_i - \\mu_i}{1+\\alpha\\mu_i} x_i = \\mathbf{0}$.\n\nLet $\\mu_C = \\exp(\\beta_0)$ be the mean for the control group and $\\mu_S = \\exp(\\beta_0 + \\beta_1)$ be the mean for the stimulated group. The design matrix specifies $x_{1,2}=(1,0)^\\top$ and $x_{3,4}=(1,1)^\\top$. The score equations are:\n$$\n\\frac{\\partial L}{\\partial \\beta_0} = \\sum_{i=1}^{2} \\frac{y_i - \\mu_C}{1+\\alpha\\mu_C}(1) + \\sum_{i=3}^{4} \\frac{y_i - \\mu_S}{1+\\alpha\\mu_S}(1) = 0\n$$\n$$\n\\frac{\\partial L}{\\partial \\beta_1} = \\sum_{i=1}^{2} \\frac{y_i - \\mu_C}{1+\\alpha\\mu_C}(0) + \\sum_{i=3}^{4} \\frac{y_i - \\mu_S}{1+\\alpha\\mu_S}(1) = 0\n$$\nFrom the second equation:\n$$\n\\frac{(y_3 - \\mu_S) + (y_4 - \\mu_S)}{1+\\alpha\\mu_S} = 0 \\implies y_3 + y_4 - 2\\mu_S = 0 \\implies \\hat{\\mu}_S = \\frac{y_3+y_4}{2}\n$$\nSubstituting this result into the first equation means its second term is zero, which implies the first term must also be zero:\n$$\n\\frac{(y_1 - \\mu_C) + (y_2 - \\mu_C)}{1+\\alpha\\mu_C} = 0 \\implies y_1 + y_2 - 2\\mu_C = 0 \\implies \\hat{\\mu}_C = \\frac{y_1+y_2}{2}\n$$\nThus, the MLEs for the group means are their respective empirical averages:\n$$\n\\hat{\\mu}_C = \\frac{100+110}{2} = 105\n$$\n$$\n\\hat{\\mu}_S = \\frac{200+220}{2} = 210\n$$\nNow we find the MLEs for $\\beta$:\n$$\n\\hat{\\mu}_C = \\exp(\\hat{\\beta}_0) = 105 \\implies \\hat{\\beta}_0 = \\ln(105)\n$$\n$$\n\\hat{\\mu}_S = \\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1) = 210 \\implies \\hat{\\beta}_0 + \\hat{\\beta}_1 = \\ln(210)\n$$\nThe MLE for the log fold-change $\\beta_1$ is:\n$$\n\\hat{\\beta}_1 = \\ln(210) - \\hat{\\beta}_0 = \\ln(210) - \\ln(105) = \\ln\\left(\\frac{210}{105}\\right) = \\ln(2)\n$$\n\n#### 2. Standard Error of $\\hat{\\beta}_1$\n\nThe standard error is derived from the inverse of the Fisher information matrix, $\\mathcal{I}(\\beta)$. The variance-covariance matrix of $\\hat{\\beta}$ is approximated by $\\mathcal{I}(\\hat{\\beta})^{-1}$. For a GLM, the Fisher information matrix is $\\mathcal{I}(\\beta) = X^\\top W X$, where $W$ is a diagonal matrix of weights $w_i$. The weights are given by:\n$$\nw_i = \\frac{(\\partial\\mu_i/\\partial\\eta_i)^2}{\\operatorname{Var}(Y_i)} = \\frac{\\mu_i^2}{\\mu_i + \\alpha \\mu_i^2} = \\frac{\\mu_i}{1+\\alpha\\mu_i}\n$$\nWe evaluate these weights at the MLEs $\\hat{\\mu}_C$ and $\\hat{\\mu}_S$:\n$$\n\\hat{w}_C = \\frac{\\hat{\\mu}_C}{1+\\alpha\\hat{\\mu}_C} = \\frac{105}{1+(0.1)(105)} = \\frac{105}{1+10.5} = \\frac{105}{11.5}\n$$\n$$\n\\hat{w}_S = \\frac{\\hat{\\mu}_S}{1+\\alpha\\hat{\\mu}_S} = \\frac{210}{1+(0.1)(210)} = \\frac{210}{1+21} = \\frac{210}{22}\n$$\nThe design matrix $X$ and diagonal weight matrix $\\hat{W}$ are:\n$$\nX = \\begin{pmatrix} 1  0 \\\\ 1  0 \\\\ 1  1 \\\\ 1  1 \\end{pmatrix}, \\quad \\hat{W} = \\text{diag}(\\hat{w}_C, \\hat{w}_C, \\hat{w}_S, \\hat{w}_S)\n$$\nThe Fisher information matrix evaluated at $\\hat{\\beta}$ is:\n$$\n\\mathcal{I}(\\hat{\\beta}) = X^\\top \\hat{W} X = \\begin{pmatrix} 1  1  1  1 \\\\ 0  0  1  1 \\end{pmatrix} \\begin{pmatrix} \\hat{w}_C  0  0  0 \\\\ 0  \\hat{w}_C  0  0 \\\\ 0  0  \\hat{w}_S  0 \\\\ 0  0  0  \\hat{w}_S \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 1  0 \\\\ 1  1 \\\\ 1  1 \\end{pmatrix} = \\begin{pmatrix} 2\\hat{w}_C+2\\hat{w}_S  2\\hat{w}_S \\\\ 2\\hat{w}_S  2\\hat{w}_S \\end{pmatrix}\n$$\nThe inverse of this $2 \\times 2$ matrix is:\n$$\n\\mathcal{I}(\\hat{\\beta})^{-1} = \\frac{1}{\\det(\\mathcal{I}(\\hat{\\beta}))} \\begin{pmatrix} 2\\hat{w}_S  -2\\hat{w}_S \\\\ -2\\hat{w}_S  2\\hat{w}_C+2\\hat{w}_S \\end{pmatrix}\n$$\nThe determinant is $\\det(\\mathcal{I}(\\hat{\\beta})) = (2\\hat{w}_C+2\\hat{w}_S)(2\\hat{w}_S) - (2\\hat{w}_S)^2 = 4\\hat{w}_C\\hat{w}_S$.\n$$\n\\mathcal{I}(\\hat{\\beta})^{-1} = \\frac{1}{4\\hat{w}_C\\hat{w}_S} \\begin{pmatrix} 2\\hat{w}_S  -2\\hat{w}_S \\\\ -2\\hat{w}_S  2\\hat{w}_C+2\\hat{w}_S \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2\\hat{w}_C}  -\\frac{1}{2\\hat{w}_C} \\\\ -\\frac{1}{2\\hat{w}_C}  \\frac{1}{2\\hat{w}_C}+\\frac{1}{2\\hat{w}_S} \\end{pmatrix}\n$$\nThe variance of $\\hat{\\beta}_1$ is the $(2,2)$ element of this matrix:\n$$\n\\operatorname{Var}(\\hat{\\beta}_1) = \\frac{1}{2\\hat{w}_C} + \\frac{1}{2\\hat{w}_S}\n$$\nSubstituting the numerical values for the weights:\n$$\n\\operatorname{Var}(\\hat{\\beta}_1) = \\frac{1}{2(105/11.5)} + \\frac{1}{2(210/22)} = \\frac{11.5}{210} + \\frac{22}{420} = \\frac{23}{420} + \\frac{22}{420} = \\frac{45}{420}\n$$\nSimplifying the fraction: $\\frac{45}{420} = \\frac{9}{84} = \\frac{3}{28}$.\nThe standard error is the square root of the variance:\n$$\n\\text{SE}(\\hat{\\beta}_1) = \\sqrt{\\frac{3}{28}}\n$$\n\n#### 3. Wald Statistic Calculation\n\nNow we can compute the Wald statistic:\n$$\nW = \\frac{\\hat{\\beta}_1}{\\text{SE}(\\hat{\\beta}_1)} = \\frac{\\ln(2)}{\\sqrt{3/28}} = \\ln(2) \\sqrt{\\frac{28}{3}}\n$$\nNumerically:\n$$\nW \\approx 0.693147 \\times \\sqrt{9.33333...} \\approx 0.693147 \\times 3.05505... \\approx 2.11763\n$$\nRounding to four significant figures, the Wald statistic is $2.118$.\n\n#### Brief Interpretation\n\nThe calculated Wald statistic $W \\approx 2.118$ is compared against a standard normal distribution. The corresponding two-sided p-value is $P(|Z|  2.118) \\approx 0.034$, where $Z \\sim \\mathcal{N}(0,1)$. Since this p-value is less than the conventional significance level of $0.05$, there is statistically significant evidence to reject the null hypothesis $H_{0}: \\beta_{1}=0$. This implies that the observed twofold increase in mean expression from the control to the stimulated condition is unlikely to be due to random chance alone, given the model. We would conclude that the cytokine gene is significantly differentially expressed (upregulated) in response to stimulation.", "answer": "$$\\boxed{2.118}$$", "id": "5157644"}, {"introduction": "The transcriptome's complexity extends beyond mere gene expression levels to include variations in transcript structure due to alternative splicing. This exercise [@problem_id:5157643] challenges you to quantify an exon skipping event by calculating the \"Percent Spliced In\" (PSI or $\\psi$) value from junction read counts. You will then apply a sophisticated Bayesian framework to estimate the uncertainty of your measurement, providing a robust understanding of how RNA-seq reveals changes in isoform usage.", "problem": "A single-skipped-exon alternative splicing event is profiled by RNA sequencing (RNA-seq) in one sample. Three junctions and the exon body are covered by reads: the upstream–exon junction, the exon–downstream junction, the skipping (upstream–downstream) junction, and the exon body. The goal is to quantify the inclusion level, defined as percent spliced in (PSI), and quantify uncertainty using a Beta-Binomial framework. For this event you observe the following unique read counts: upstream–exon junction reads $J_{UE} = 120$, exon–downstream junction reads $J_{ED} = 100$, skipping junction reads $J_{UD} = 80$, and exon-body reads fully contained within the exon $B = 60$. Assume uniform sampling of molecules across isoforms, unique mappability of junction-spanning reads with a fixed minimal overhang, and that the exon-body reads are length-biased relative to junction-spanning reads and therefore should not enter the estimator of percent spliced in (PSI) under a junction-centric definition.\n\nUse the following fundamental base:\n- Under uniform sampling, the inclusion level parameter $\\psi$ is the probability that a randomly chosen molecule for the event follows the inclusion isoform rather than the skipping isoform.\n- Junction-spanning reads uniquely identify the isoform path. Because the inclusion path produces two distinct inclusion-identifying junction types, the sum of inclusion-junction reads must be normalized to represent the number of inclusion molecules sampled on a common scale with the single skipping junction.\n- A conjugate Beta-Binomial model arises from placing a Beta prior on $\\psi$ and modeling inclusion-annotated junction reads as Binomial trials with success probability $\\psi$.\n\nTasks:\n1. Derive from first principles a junction-centric estimator for the inclusion count and total trials using $J_{UE}$, $J_{ED}$, and $J_{UD}$ that is compatible with the interpretation of $\\psi$ as a Bernoulli success probability for inclusion versus skipping.\n2. Using a symmetric Beta prior with prior mean $\\mu_0 = 0.5$ and prior intraclass correlation $\\rho_0 = 0.05$ (so that $\\rho_0 = 1/(\\alpha_0 + \\beta_0 + 1)$ for prior Beta parameters $\\alpha_0$ and $\\beta_0$), compute the posterior standard deviation of $\\psi$ under the Beta-Binomial model for the observed counts.\n3. Report only the posterior standard deviation of $\\psi$ as a dimensionless fraction, rounded to four significant figures.", "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of molecular biology (alternative splicing, RNA-seq) and statistics (Bayesian inference, Beta-Binomial model). It is well-posed, objective, and contains all necessary information to derive a unique solution.\n\nThe problem requires a three-part solution: first, to derive an estimator for the effective number of inclusion and total events from junction read counts; second, to compute the posterior standard deviation of the percent spliced in ($\\psi$) parameter using a Beta-Binomial model; and third, to report this value.\n\n**Part 1: Derivation of the Junction-Centric Estimator**\n\nThe goal is to estimate the parameter $\\psi$, representing the proportion of transcripts that include the alternative exon. We are given read counts from three types of splice junctions:\n-   $J_{UE} = 120$: Reads spanning the upstream-exon junction, unique to the inclusion isoform.\n-   $J_{ED} = 100$: Reads spanning the exon-downstream junction, also unique to the inclusion isoform.\n-   $J_{UD} = 80$: Reads spanning the skipping junction (upstream-downstream), unique to the skipping isoform.\n\nThe problem specifies a junction-centric model and notes that exon-body reads ($B=60$) are biased and should not be used. The core challenge is to combine these counts into a consistent measure of inclusion vs. skipping events, suitable for a Binomial likelihood framework.\n\nThe number of reads supporting the skipping isoform, $S$, is directly given by the count of skipping junction reads:\n$$ S = J_{UD} = 80 $$\nThe inclusion isoform, however, presents two distinct junction targets for sequencing reads ($UE$ and $ED$). A simple sum, $J_{UE} + J_{ED}$, would over-represent the inclusion isoform relative to the skipping isoform, which has only one junction target ($UD$). The problem states that the inclusion read counts must be \"normalized to represent the number of inclusion molecules sampled on a common scale with the single skipping junction.\"\n\nA standard and principled method to achieve this normalization under the assumption of uniform sampling is to average the counts from the two inclusion-specific junctions. This yields an effective inclusion count, $I$, that is on a comparable scale to the skipping count $S$.\n$$ I = \\frac{J_{UE} + J_{ED}}{2} $$\nSubstituting the given values:\n$$ I = \\frac{120 + 100}{2} = \\frac{220}{2} = 110 $$\nThis value, $I=110$, represents the number of \"successes\" (inclusion events) in our model. The total number of trials, $N$, is the sum of the effective inclusion counts and the skipping counts:\n$$ N = I + S = 110 + 80 = 190 $$\nThus, the data for our binomial model are $I=110$ inclusion events out of $N=190$ total events.\n\n**Part 2: Beta-Binomial Modeling and Posterior Calculation**\n\nWe use a Beta-Binomial model to find the posterior distribution of $\\psi$. The model is defined by:\n-   Prior distribution for $\\psi$: $\\psi \\sim \\text{Beta}(\\alpha_0, \\beta_0)$\n-   Likelihood of observing $I$ inclusion events in $N$ trials: $I | \\psi, N \\sim \\text{Binomial}(N, \\psi)$\n\nDue to the conjugacy of the Beta prior and Binomial likelihood, the posterior distribution is also a Beta distribution:\n$$ \\psi | I, N \\sim \\text{Beta}(\\alpha', \\beta') $$\nwhere $\\alpha' = \\alpha_0 + I$ and $\\beta' = \\beta_0 + S = \\beta_0 + (N - I)$.\n\nFirst, we must determine the prior parameters, $\\alpha_0$ and $\\beta_0$. We are given a symmetric prior with mean $\\mu_0 = 0.5$ and intraclass correlation $\\rho_0 = 0.05$.\nFor a $\\text{Beta}(\\alpha_0, \\beta_0)$ distribution, the mean is $\\mu_0 = \\frac{\\alpha_0}{\\alpha_0 + \\beta_0}$. The condition $\\mu_0 = 0.5$ implies $\\alpha_0 = \\beta_0$.\nThe intraclass correlation is given by the formula $\\rho_0 = \\frac{1}{\\alpha_0 + \\beta_0 + 1}$. Substituting $\\beta_0 = \\alpha_0$ and $\\rho_0 = 0.05$:\n$$ 0.05 = \\frac{1}{\\alpha_0 + \\alpha_0 + 1} = \\frac{1}{2\\alpha_0 + 1} $$\nSolving for $\\alpha_0$:\n$$ 2\\alpha_0 + 1 = \\frac{1}{0.05} = 20 $$\n$$ 2\\alpha_0 = 19 $$\n$$ \\alpha_0 = 9.5 $$\nTherefore, the prior parameters are $\\alpha_0 = 9.5$ and $\\beta_0 = 9.5$.\n\nNext, we calculate the posterior parameters, $\\alpha'$ and $\\beta'$, by updating the prior with our data ($I=110$, $S=80$):\n$$ \\alpha' = \\alpha_0 + I = 9.5 + 110 = 119.5 $$\n$$ \\beta' = \\beta_0 + S = 9.5 + 80 = 89.5 $$\nThe posterior distribution for $\\psi$ is $\\text{Beta}(119.5, 89.5)$.\n\nThe final step is to compute the standard deviation of this posterior distribution. The variance of a $\\text{Beta}(\\alpha, \\beta)$ distribution is:\n$$ \\text{Var}(\\psi) = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)} $$\nSubstituting the posterior parameters $\\alpha' = 119.5$ and $\\beta' = 89.5$:\n$$ \\text{Var}(\\psi|\\text{data}) = \\frac{(119.5)(89.5)}{(119.5 + 89.5)^2 (119.5 + 89.5 + 1)} $$\n$$ \\text{Var}(\\psi|\\text{data}) = \\frac{10695.25}{(209)^2 (210)} = \\frac{10695.25}{(43681)(210)} = \\frac{10695.25}{9173010} $$\nThe posterior standard deviation, $\\sigma_{\\psi|\\text{data}}$, is the square root of the variance:\n$$ \\sigma_{\\psi|\\text{data}} = \\sqrt{\\frac{10695.25}{9173010}} \\approx \\sqrt{0.0011660005} \\approx 0.03414675 $$\n\n**Part 3: Final Answer Reporting**\n\nThe problem requires reporting the posterior standard deviation of $\\psi$ rounded to four significant figures.\n$$ 0.03414675 \\approx 0.03415 $$\nThis value represents the uncertainty in our estimate of the inclusion level $\\psi$.", "answer": "$$\n\\boxed{0.03415}\n$$", "id": "5157643"}]}