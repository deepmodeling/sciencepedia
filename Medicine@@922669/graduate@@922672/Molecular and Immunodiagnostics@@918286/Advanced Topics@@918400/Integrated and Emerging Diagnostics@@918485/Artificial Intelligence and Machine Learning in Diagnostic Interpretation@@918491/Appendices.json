{"hands_on_practices": [{"introduction": "The foundation of evaluating any diagnostic classifier lies in its confusion matrix, which tabulates correct and incorrect predictions against a gold standard. This exercise provides essential practice in calculating fundamental performance metrics from this matrix, including sensitivity, specificity, and the $F_1$ score. Mastering these calculations is the first step toward understanding a model's strengths and weaknesses at a given operational threshold [@problem_id:5094057].", "problem": "An Artificial Intelligence (AI) classifier trained on multiplex immunoassay signals from a $12$-analyte panel is deployed to provide a binary diagnostic interpretation for an infectious disease status in a hospital cohort. The cohort consists of $N=1200$ individuals, of whom $360$ are disease-positive by gold-standard molecular testing and $840$ are disease-negative. The AI classifier, after thresholding its probabilistic output to produce a binary call, yields the following confusion matrix with respect to the gold standard: among the $360$ disease-positive individuals, $300$ are predicted positive and $60$ are predicted negative; among the $840$ disease-negative individuals, $200$ are predicted positive and $640$ are predicted negative.\n\nUsing only foundational definitions from diagnostic test evaluation and probability, compute the following performance metrics for the AI interpretation of the panel assay with respect to the disease-positive class: sensitivity, specificity, $F_1$ score, and diagnostic odds ratio. Additionally, based on first principles, explain in words a realistic scenario in which the diagnostic odds ratio can be misleading for clinical decision-making in molecular and immunodiagnostics, even if it appears numerically large.\n\nReport your final numeric answers in the order: sensitivity, specificity, $F_1$ score, diagnostic odds ratio. Express all four values as unitless decimals and round your answers to four significant figures.", "solution": "The scenario is a binary classification problem evaluated against a gold-standard reference, which allows us to apply core definitions from diagnostic test evaluation. We denote true positives by $TP$, false negatives by $FN$, false positives by $FP$, and true negatives by $TN$. From the problem description, we have $TP=300$, $FN=60$, $FP=200$, and $TN=640$. These counts partition the cohort of $N=1200$ individuals consistently: $TP+FN=360$, $FP+TN=840$, and $TP+FN+FP+TN=1200$.\n\nWe begin from fundamental definitions:\n\n- Sensitivity (also called the true positive rate) is the probability that the test is positive given that the individual is truly positive. In terms of the confusion matrix, the definition is\n$$\n\\text{sensitivity} \\equiv \\frac{TP}{TP+FN}.\n$$\n\n- Specificity (also called the true negative rate) is the probability that the test is negative given that the individual is truly negative. In terms of the confusion matrix, the definition is\n$$\n\\text{specificity} \\equiv \\frac{TN}{TN+FP}.\n$$\n\n- Precision (also called the positive predictive value in a cohort setting) is the probability that an individual is truly positive given that the test is positive. In terms of the confusion matrix, the definition is\n$$\n\\text{precision} \\equiv \\frac{TP}{TP+FP}.\n$$\n\n- Recall is the same quantity as sensitivity,\n$$\n\\text{recall} \\equiv \\frac{TP}{TP+FN}.\n$$\n\n- The $F_1$ score is the harmonic mean of precision and recall, derived from the general definition of a harmonic mean $H$ for two positive numbers $a$ and $b$ as $H=2ab/(a+b)$. Applying this to precision and recall yields\n$$\nF_1 \\equiv \\frac{2 \\times \\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}.\n$$\n\n- The diagnostic odds ratio (DOR) is defined as the ratio of the odds of positivity in disease to the odds of positivity in non-disease. The odds of a positive test among diseased is $TP/FN$, and the odds of a positive test among non-diseased is $FP/TN$. Therefore,\n$$\n\\text{DOR} \\equiv \\frac{TP/FN}{FP/TN}.\n$$\nThis follows from the basic probability-to-odds mapping: for an event with probability $p$, its odds is $p/(1-p)$; applying this to the conditional probabilities in the confusion matrix leads to the stated ratio in terms of counts.\n\nWe now compute each metric symbolically, then numerically:\n\n1. Sensitivity:\n$$\n\\text{sensitivity} = \\frac{TP}{TP+FN} = \\frac{300}{300+60} = \\frac{300}{360} = \\frac{5}{6}.\n$$\nAs a decimal, $\\frac{5}{6} = 0.\\overline{83}$, which rounded to four significant figures yields $0.8333$.\n\n2. Specificity:\n$$\n\\text{specificity} = \\frac{TN}{TN+FP} = \\frac{640}{640+200} = \\frac{640}{840} = \\frac{16}{21}.\n$$\nAs a decimal, $\\frac{16}{21} \\approx 0.76190476$, which rounded to four significant figures yields $0.7619$.\n\n3. $F_1$ score:\nFirst compute precision and recall:\n$$\n\\text{precision} = \\frac{TP}{TP+FP} = \\frac{300}{300+200} = \\frac{300}{500} = \\frac{3}{5},\n$$\n$$\n\\text{recall} = \\frac{TP}{TP+FN} = \\frac{300}{360} = \\frac{5}{6}.\n$$\nThen\n$$\nF_1 = \\frac{2 \\times \\frac{3}{5} \\times \\frac{5}{6}}{\\frac{3}{5} + \\frac{5}{6}}.\n$$\nSimplify the numerator:\n$$\n2 \\times \\frac{3}{5} \\times \\frac{5}{6} = 2 \\times \\frac{15}{30} = 2 \\times \\frac{1}{2} = 1.\n$$\nSimplify the denominator:\n$$\n\\frac{3}{5} + \\frac{5}{6} = \\frac{18}{30} + \\frac{25}{30} = \\frac{43}{30}.\n$$\nTherefore\n$$\nF_1 = \\frac{1}{\\frac{43}{30}} = \\frac{30}{43}.\n$$\nAs a decimal, $\\frac{30}{43} \\approx 0.69767442$, which rounded to four significant figures yields $0.6977$.\n\n4. Diagnostic odds ratio:\n$$\n\\text{DOR} = \\frac{TP/FN}{FP/TN} = \\frac{\\frac{300}{60}}{\\frac{200}{640}} = \\frac{5}{\\frac{5}{16}} = 16.\n$$\nRounded to four significant figures, this remains $16.00$ when expressed with four significant figures.\n\nWe now explain from first principles when the diagnostic odds ratio can be misleading, even if numerically large, in the context of AI-based interpretation of molecular and immunoassay panels.\n\nThe diagnostic odds ratio, by construction, aggregates information from sensitivity and specificity into a single odds-based ratio. It is invariant to disease prevalence because it conditions on disease status and control status separately. This prevalence invariance, while mathematically neat, can be clinically misleading in several realistic scenarios:\n\n- In low-prevalence deployment, a large $\\text{DOR}$ does not guarantee acceptable positive predictive value. By Bayes’ theorem, the post-test probability depends on both the likelihood ratios (which are related to sensitivity and specificity) and the pre-test probability (prevalence). Even with a high $\\text{DOR}$, if prevalence is very low, the number of false positives may dominate the positive calls, leading to many unnecessary follow-ups. Thus, a numerically large $\\text{DOR}$ can mask poor clinical utility when the pre-test probability is small.\n\n- Threshold optimization that maximizes $\\text{DOR}$ implicitly weights sensitivity and specificity on the log-odds scale but does not encode clinical misclassification costs. In AI systems where thresholds are tuned, maximizing $\\text{DOR}$ can select operating points with unacceptable trade-offs (for example, very high sensitivity but low specificity), which may be unsuitable for screening versus confirmatory contexts depending on downstream actions.\n\n- Spectrum bias and case-control sampling amplify $\\text{DOR}$ by making diseased cases unusually “easy” (e.g., severe, prototypical) and controls unusually “clean” (e.g., healthy volunteers). AI systems trained or evaluated on such skewed spectra can yield inflated $\\text{DOR}$ that fails to generalize to routine clinical populations with mild disease, comorbidities, and interfering substances common in immunodiagnostics.\n\n- Instability with sparse cells: When $FN$ or $FP$ is near zero (common with small validation subsets or extreme thresholds), $\\text{DOR}$ can become extremely large or undefined, requiring continuity corrections that are arbitrary. This numerical instability can mislead interpretation by exaggerating performance.\n\nThese issues arise directly from the foundational definitions: $\\text{DOR}$ is constructed from conditional odds ratios that do not incorporate prior probabilities or misclassification utilities. In AI-driven diagnostic interpretation, where calibration, prevalence drift, and clinical utility are central, relying solely on $\\text{DOR}$ without consideration of pre-test probability, decision costs, and sample representativeness can be misleading, even when the computed $\\text{DOR}$ is large.", "answer": "$$\\boxed{\\begin{pmatrix}0.8333 & 0.7619 & 0.6977 & 16.00\\end{pmatrix}}$$", "id": "5094057"}, {"introduction": "A classifier's intrinsic performance, described by sensitivity and specificity, does not tell the whole story in a clinical context. This practice illustrates the critical role of disease prevalence in determining a test's real-world utility. By applying Bayes' theorem, you will translate the model's performance into Positive and Negative Predictive Values (PPV and NPV), which directly address the probability of disease given a test result [@problem_id:5094091].", "problem": "A translational Artificial Intelligence (AI) and Machine Learning (ML) pipeline is deployed to interpret a high-throughput immunoassay for a molecular diagnostic target, producing a binary classification calibrated to a fixed decision threshold. Cross-validated performance estimates yield sensitivity $0.92$ and specificity $0.97$ at the chosen threshold. In a target clinical population, the disease prevalence is $0.04$. The goal is to quantify how the classifier’s performance translates to clinical predictive values and Bayesian updating.\n\nStarting from the fundamental definitions used in diagnostic testing and Bayesian decision theory, proceed as follows. Use the definitions: sensitivity $=$ $\\Pr(\\text{test positive} \\mid \\text{disease})$, specificity $=$ $\\Pr(\\text{test negative} \\mid \\text{no disease})$, and prevalence $=$ $\\Pr(\\text{disease})$. Use Bayes’ theorem and the definition of the positive likelihood ratio ($\\text{LR}^{+}$) as $\\text{LR}^{+}=\\frac{\\text{sensitivity}}{1-\\text{specificity}}$. Derive and compute:\n\n1. The Positive Predictive Value (PPV), defined as $\\Pr(\\text{disease} \\mid \\text{test positive})$.\n2. The Negative Predictive Value (NPV), defined as $\\Pr(\\text{no disease} \\mid \\text{test negative})$.\n3. The posterior odds of disease after a positive test result.\n\nExpress $PPV$ and $NPV$ as decimals and the posterior odds as a dimensionless decimal. Round each quantity to four significant figures. Do not use the percentage symbol.", "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in the principles of Bayesian statistics and epidemiology, well-posed with all necessary information provided, and objective in its formulation. We may proceed with a formal solution.\n\nLet $D$ be the event that an individual from the target population has the disease, and let $D^c$ be the complementary event that the individual does not have the disease. Let $T^+$ be the event that the test result is positive, and $T^-$ be the event that the test result is negative. The provided information can be formally expressed as follows:\n\n- Prevalence, $\\Pr(D) = 0.04$. Consequently, the probability of no disease is $\\Pr(D^c) = 1 - \\Pr(D) = 1 - 0.04 = 0.96$.\n- Sensitivity, $\\text{Sens} = \\Pr(T^+ \\mid D) = 0.92$.\n- Specificity, $\\text{Spec} = \\Pr(T^- \\mid D^c) = 0.97$.\n\nFrom these definitions, we can derive other conditional probabilities:\n- The false negative rate, $\\Pr(T^- \\mid D) = 1 - \\text{Sens} = 1 - 0.92 = 0.08$.\n- The false positive rate, $\\Pr(T^+ \\mid D^c) = 1 - \\text{Spec} = 1 - 0.97 = 0.03$.\n\nThe problem requires the calculation of three quantities: the Positive Predictive Value (PPV), the Negative Predictive Value (NPV), and the posterior odds of disease after a positive test result.\n\n**1. Positive Predictive Value (PPV)**\n\nThe PPV is defined as the probability of having the disease given a positive test result, $\\text{PPV} = \\Pr(D \\mid T^+)$. We apply Bayes' theorem:\n$$\n\\text{PPV} = \\Pr(D \\mid T^+) = \\frac{\\Pr(T^+ \\mid D) \\Pr(D)}{\\Pr(T^+)}\n$$\nThe denominator, $\\Pr(T^+)$, is the total probability of a positive test, which can be found using the law of total probability:\n$$\n\\Pr(T^+) = \\Pr(T^+ \\mid D) \\Pr(D) + \\Pr(T^+ \\mid D^c) \\Pr(D^c)\n$$\nSubstituting the known values:\n$$\n\\Pr(T^+) = (0.92)(0.04) + (0.03)(0.96) = 0.0368 + 0.0288 = 0.0656\n$$\nNow we can compute the PPV:\n$$\n\\text{PPV} = \\frac{0.0368}{0.0656} \\approx 0.5609756...\n$$\nRounding to four significant figures, we get $\\text{PPV} \\approx 0.5610$.\n\n**2. Negative Predictive Value (NPV)**\n\nThe NPV is defined as the probability of not having the disease given a negative test result, $\\text{NPV} = \\Pr(D^c \\mid T^-)$. Applying Bayes' theorem:\n$$\n\\text{NPV} = \\Pr(D^c \\mid T^-) = \\frac{\\Pr(T^- \\mid D^c) \\Pr(D^c)}{\\Pr(T^-)}\n$$\nThe denominator, $\\Pr(T^-)$, is the total probability of a negative test. Using the law of total probability:\n$$\n\\Pr(T^-) = \\Pr(T^- \\mid D^c) \\Pr(D^c) + \\Pr(T^- \\mid D) \\Pr(D)\n$$\nNote that $\\Pr(T^-) = 1 - \\Pr(T^+) = 1 - 0.0656 = 0.9344$. We can also calculate it directly:\n$$\n\\Pr(T^-) = (0.97)(0.96) + (0.08)(0.04) = 0.9312 + 0.0032 = 0.9344\n$$\nNow we can compute the NPV:\n$$\n\\text{NPV} = \\frac{(0.97)(0.96)}{0.9344} = \\frac{0.9312}{0.9344} \\approx 0.9965753...\n$$\nRounding to four significant figures, we get $\\text{NPV} \\approx 0.9966$.\n\n**3. Posterior Odds of Disease**\n\nThe odds of an event $A$ are given by $\\text{Odds}(A) = \\frac{\\Pr(A)}{1 - \\Pr(A)}$. The posterior odds of disease are the odds after observing a positive test result, $\\text{Odds}(D \\mid T^+) = \\frac{\\Pr(D \\mid T^+)}{\\Pr(D^c \\mid T^+)}$.\n\nIn Bayesian terms, posterior odds are calculated by multiplying the prior odds by the likelihood ratio.\n$$\n\\text{Posterior Odds} = \\text{Prior Odds} \\times \\text{Likelihood Ratio}\n$$\nThe prior odds of disease are:\n$$\n\\text{Odds}(D) = \\frac{\\Pr(D)}{\\Pr(D^c)} = \\frac{0.04}{0.96}\n$$\nFor a positive test, the relevant likelihood ratio is the positive likelihood ratio, $\\text{LR}^+$, defined as:\n$$\n\\text{LR}^+ = \\frac{\\text{sensitivity}}{1 - \\text{specificity}} = \\frac{\\Pr(T^+ \\mid D)}{\\Pr(T^+ \\mid D^c)}\n$$\nSubstituting the given values:\n$$\n\\text{LR}^+ = \\frac{0.92}{1 - 0.97} = \\frac{0.92}{0.03}\n$$\nNow, we compute the posterior odds:\n$$\n\\text{Odds}(D \\mid T^+) = \\left(\\frac{0.04}{0.96}\\right) \\times \\left(\\frac{0.92}{0.03}\\right) = \\frac{0.04 \\times 0.92}{0.96 \\times 0.03} = \\frac{0.0368}{0.0288} \\approx 1.2777...\n$$\nRounding to four significant figures, the posterior odds are approximately $1.278$.\n\nThis result can be verified using the previously calculated PPV:\n$$\n\\text{Odds}(D \\mid T^+) = \\frac{\\text{PPV}}{1 - \\text{PPV}} = \\frac{0.5609756...}{1 - 0.5609756...} = \\frac{0.5609756...}{0.4390243...} \\approx 1.2777...\n$$\nThe results are consistent.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.5610 & 0.9966 & 1.278 \\end{pmatrix}}\n$$", "id": "5094091"}, {"introduction": "Deploying a diagnostic AI system requires moving beyond simple accuracy metrics to a formal decision-making framework. This advanced exercise demonstrates how to determine an optimal classification threshold by minimizing the expected risk, which incorporates the prevalence of the disease and the specific costs associated with false positive and false negative errors. This principle from Bayesian decision theory provides a rigorous method for aligning a model's behavior with clinical and economic consequences [@problem_id:5094074].", "problem": "A clinical laboratory has deployed an Artificial Intelligence (AI) and Machine Learning (ML) system to interpret a continuous immunoassay signal as a diagnostic risk score $s \\in \\mathbb{R}$ for an infectious disease. The decision rule is a single-threshold classifier: predict disease if $s > \\tau$ and predict no disease otherwise. The class-conditional score distributions are modeled as Gaussian with equal variance based on prior calibration: $s \\mid \\text{disease} \\sim \\mathcal{N}(\\mu_{1}, \\sigma^{2})$ and $s \\mid \\text{no disease} \\sim \\mathcal{N}(\\mu_{0}, \\sigma^{2})$. The disease prevalence is $\\pi_{1}$ and the non-disease prevalence is $\\pi_{0} = 1 - \\pi_{1}$. The expected risk under this decision rule is defined by the fundamental Bayes decision-theoretic cost formulation for binary classification: $R(\\tau) = c_{FP} \\, \\pi_{0} \\, \\mathbb{P}(s > \\tau \\mid \\text{no disease}) + c_{FN} \\, \\pi_{1} \\, \\mathbb{P}(s \\leq \\tau \\mid \\text{disease})$, where $c_{FP}$ and $c_{FN}$ are the specified costs of a false positive and a false negative, respectively.\n\nStarting from this risk definition and the Gaussian density, derive the analytic expression for the threshold $\\tau$ that minimizes $R(\\tau)$ by equalizing the marginal expected costs for false positives and false negatives at the decision boundary, and then evaluate $\\tau$ numerically for the following scientifically plausible parameters: $\\mu_{0} = 0.5$, $\\mu_{1} = 1.5$, $\\sigma = 0.4$, $\\pi_{1} = 0.2$, $\\pi_{0} = 0.8$, $c_{FP} = 1$, and $c_{FN} = 5$. Express the final threshold as a dimensionless score and round your answer to four significant figures.", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in Bayesian decision theory and statistical signal processing, representing a standard and well-posed optimization problem. The provided parameters are self-consistent, physically plausible for a diagnostic context, and complete. The problem is objective, free of ambiguity, and requires a rigorous, non-trivial derivation.\n\nThe objective is to find the decision threshold $\\tau$ that minimizes the expected risk $R(\\tau)$. The risk function is given by:\n$$R(\\tau) = c_{FP} \\, \\pi_{0} \\, \\mathbb{P}(s > \\tau \\mid \\text{no disease}) + c_{FN} \\, \\pi_{1} \\, \\mathbb{P}(s \\leq \\tau \\mid \\text{disease})$$\nwhere $\\mathbb{P}(s > \\tau \\mid \\text{no disease})$ is the probability of a false positive, and $\\mathbb{P}(s \\leq \\tau \\mid \\text{disease})$ is the probability of a false negative. Let $p(s \\mid \\text{no disease})$ and $p(s \\mid \\text{disease})$ be the probability density functions (PDFs) for the scores $s$ conditioned on the absence or presence of disease, respectively. The probabilities can be written as integrals:\n$$\\mathbb{P}(s > \\tau \\mid \\text{no disease}) = \\int_{\\tau}^{\\infty} p(s \\mid \\text{no disease}) \\, ds$$\n$$\\mathbb{P}(s \\leq \\tau \\mid \\text{disease}) = \\int_{-\\infty}^{\\tau} p(s \\mid \\text{disease}) \\, ds$$\nTo find the minimum of $R(\\tau)$, we differentiate $R(\\tau)$ with respect to $\\tau$ and set the result to zero. Using the Leibniz integral rule, specifically $\\frac{d}{dx}\\int_{x}^{a} f(t)dt = -f(x)$ and $\\frac{d}{dx}\\int_{a}^{x} f(t)dt = f(x)$, we obtain:\n$$\\frac{dR(\\tau)}{d\\tau} = c_{FP} \\, \\pi_{0} \\, \\frac{d}{d\\tau} \\left( \\int_{\\tau}^{\\infty} p(s \\mid \\text{no disease}) \\, ds \\right) + c_{FN} \\, \\pi_{1} \\, \\frac{d}{d\\tau} \\left( \\int_{-\\infty}^{\\tau} p(s \\mid \\text{disease}) \\, ds \\right)$$\n$$\\frac{dR(\\tau)}{d\\tau} = c_{FP} \\, \\pi_{0} \\, (-p(\\tau \\mid \\text{no disease})) + c_{FN} \\, \\pi_{1} \\, (p(\\tau \\mid \\text{disease}))$$\nSetting the derivative to zero gives the optimality condition, which equates the marginal costs at the decision boundary $\\tau$:\n$$c_{FN} \\, \\pi_{1} \\, p(\\tau \\mid \\text{disease}) = c_{FP} \\, \\pi_{0} \\, p(\\tau \\mid \\text{no disease})$$\nThe problem states that the class-conditional distributions are Gaussian with means $\\mu_1$ (disease) and $\\mu_0$ (no disease) and a common variance $\\sigma^2$. The PDFs are:\n$$p(s \\mid \\text{disease}) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(s - \\mu_1)^2}{2\\sigma^2}\\right)$$\n$$p(s \\mid \\text{no disease}) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(s - \\mu_0)^2}{2\\sigma^2}\\right)$$\nSubstituting these expressions for $s = \\tau$ into the optimality condition:\n$$c_{FN} \\, \\pi_{1} \\, \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(\\tau - \\mu_1)^2}{2\\sigma^2}\\right) = c_{FP} \\, \\pi_{0} \\, \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(\\tau - \\mu_0)^2}{2\\sigma^2}\\right)$$\nThe normalization constant $\\frac{1}{\\sqrt{2\\pi}\\sigma}$ cancels from both sides:\n$$c_{FN} \\, \\pi_{1} \\, \\exp\\left(-\\frac{(\\tau - \\mu_1)^2}{2\\sigma^2}\\right) = c_{FP} \\, \\pi_{0} \\, \\exp\\left(-\\frac{(\\tau - \\mu_0)^2}{2\\sigma^2}\\right)$$\nTo solve for $\\tau$, we take the natural logarithm of both sides:\n$$\\ln(c_{FN} \\pi_{1}) - \\frac{(\\tau - \\mu_1)^2}{2\\sigma^2} = \\ln(c_{FP} \\pi_{0}) - \\frac{(\\tau - \\mu_0)^2}{2\\sigma^2}$$\nRearranging the terms to isolate the expressions involving $\\tau$:\n$$\\frac{(\\tau - \\mu_0)^2}{2\\sigma^2} - \\frac{(\\tau - \\mu_1)^2}{2\\sigma^2} = \\ln(c_{FP} \\pi_{0}) - \\ln(c_{FN} \\pi_{1})$$\nMultiplying by $2\\sigma^2$ and combining the logarithm terms:\n$$(\\tau - \\mu_0)^2 - (\\tau - \\mu_1)^2 = 2\\sigma^2 \\ln\\left(\\frac{c_{FP} \\pi_{0}}{c_{FN} \\pi_{1}}\\right)$$\nExpanding the squared terms on the left-hand side:\n$$(\\tau^2 - 2\\tau\\mu_0 + \\mu_0^2) - (\\tau^2 - 2\\tau\\mu_1 + \\mu_1^2) = 2\\sigma^2 \\ln\\left(\\frac{c_{FP} \\pi_{0}}{c_{FN} \\pi_{1}}\\right)$$\nThe $\\tau^2$ terms cancel. Grouping the remaining terms gives:\n$$2\\tau(\\mu_1 - \\mu_0) + (\\mu_0^2 - \\mu_1^2) = 2\\sigma^2 \\ln\\left(\\frac{c_{FP} \\pi_{0}}{c_{FN} \\pi_{1}}\\right)$$\nUsing the difference of squares factorization $\\mu_0^2 - \\mu_1^2 = (\\mu_0 - \\mu_1)(\\mu_0 + \\mu_1)$:\n$$2\\tau(\\mu_1 - \\mu_0) - (\\mu_1 - \\mu_0)(\\mu_0 + \\mu_1) = 2\\sigma^2 \\ln\\left(\\frac{c_{FP} \\pi_{0}}{c_{FN} \\pi_{1}}\\right)$$\nDividing by $2(\\mu_1 - \\mu_0)$, assuming $\\mu_1 \\neq \\mu_0$:\n$$\\tau - \\frac{\\mu_0 + \\mu_1}{2} = \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{c_{FP} \\pi_{0}}{c_{FN} \\pi_{1}}\\right)$$\nFinally, solving for $\\tau$ yields the general analytic expression for the optimal threshold:\n$$\\tau = \\frac{\\mu_0 + \\mu_1}{2} + \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{c_{FP} \\pi_{0}}{c_{FN} \\pi_{1}}\\right)$$\nNow we evaluate this expression numerically using the provided parameters: $\\mu_{0} = 0.5$, $\\mu_{1} = 1.5$, $\\sigma = 0.4$, $\\pi_{1} = 0.2$, $\\pi_{0} = 0.8$, $c_{FP} = 1$, and $c_{FN} = 5$.\n\nFirst, we calculate the components of the expression:\nThe midpoint of the means: $\\frac{\\mu_0 + \\mu_1}{2} = \\frac{0.5 + 1.5}{2} = \\frac{2.0}{2} = 1$.\nThe difference of the means: $\\mu_1 - \\mu_0 = 1.5 - 0.5 = 1$.\nThe variance: $\\sigma^2 = (0.4)^2 = 0.16$.\nThe argument of the logarithm: $\\frac{c_{FP} \\pi_{0}}{c_{FN} \\pi_{1}} = \\frac{1 \\times 0.8}{5 \\times 0.2} = \\frac{0.8}{1.0} = 0.8$.\n\nSubstituting these values into the expression for $\\tau$:\n$$\\tau = 1 + \\frac{0.16}{1} \\ln(0.8)$$\n$$\\tau = 1 + 0.16 \\times (-0.22314355...)$$\n$$\\tau = 1 - 0.035702968...$$\n$$\\tau = 0.96429703...$$\nRounding the result to four significant figures gives $\\tau \\approx 0.9643$.", "answer": "$$\\boxed{0.9643}$$", "id": "5094074"}]}