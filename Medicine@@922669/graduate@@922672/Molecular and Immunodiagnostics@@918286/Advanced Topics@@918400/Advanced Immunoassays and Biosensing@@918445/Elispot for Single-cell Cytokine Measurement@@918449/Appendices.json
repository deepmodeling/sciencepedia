{"hands_on_practices": [{"introduction": "The fundamental promise of the ELISpot assay is the enumeration of individual secreting cells. This promise is broken if spots from adjacent cells overlap, leading to undercounting. The following exercise [@problem_id:5112787] explores this critical limitation from first principles, challenging you to model the well as a geometric space and derive the expected number of spot 'collisions'. This derivation reveals the non-linear relationship between cell density and overlap, underscoring why optimizing the number of cells plated per well is a crucial first step in any successful ELISpot experiment.", "problem": "In Enzyme-Linked ImmunoSpot (ELISpot) assays for single-cell cytokine measurement, each cytokine-secreting cell deposits an insoluble product on the membrane that appears as a circular spot of approximately uniform radius $r$ centered at the location of the cell. Accurate enumeration of cytokine-secreting cells requires that individual spots be resolvable and non-overlapping. Consider a single well of area $A$ in which $N$ viable, cytokine-secreting cells are seeded and assumed to be independently and uniformly distributed across the well surface. Define a \"collision\" as any pair of spots that geometrically overlap (that is, the distance between their centers is less than $2r$). Assume that $r$ is small relative to the characteristic length scale of the well so that boundary effects are negligible.\n\nUsing only fundamental definitions from geometric probability, independence of uniformly distributed points, and linearity of expectation, derive an analytic expression for the expected number of collisions in the well as a function of $N$, $r$, and $A$. Briefly justify why optimizing cell density (cells per unit area) is critical to minimize spot overlap from a first-principles perspective based on your derivation. Express your final result as a single closed-form analytic expression. No numerical evaluation is required, and no units should be included in the final expression.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a solution using the specified first-principles methods.\n\nThe problem requires the derivation of the expected number of collisions, $E[C]$, among $N$ cytokine-secreting cells distributed independently and uniformly over a well of area $A$. Each cell produces a spot of radius $r$. A collision is defined as the geometric overlap of two spots, which occurs if the distance between the centers of the corresponding cells is less than $2r$. The derivation will proceed by applying the principle of linearity of expectation.\n\nLet the $N$ cells be indexed from $1$ to $N$. Let $C$ be the random variable representing the total number of collisions in the well. A collision involves a pair of spots. The total number of unique pairs of cells is given by the binomial coefficient $\\binom{N}{2}$.\n\nLet us define an indicator random variable, $I_{ij}$, for each pair of distinct cells $(i, j)$ with $1 \\le i < j \\le N$. The variable $I_{ij}$ is defined as:\n$$\nI_{ij} =\n\\begin{cases}\n1 & \\text{if spots } i \\text{ and } j \\text{ collide} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nThe total number of collisions $C$ is the sum of these indicator variables over all unique pairs:\n$$\nC = \\sum_{1 \\le i < j \\le N} I_{ij}\n$$\nThe expected number of collisions, $E[C]$, can be found by applying the linearity of expectation, which states that the expectation of a sum of random variables is the sum of their individual expectations:\n$$\nE[C] = E\\left[\\sum_{1 \\le i < j \\le N} I_{ij}\\right] = \\sum_{1 \\le i < j \\le N} E[I_{ij}]\n$$\nFor any indicator variable, its expectation is equal to the probability of the event it indicates. Therefore, $E[I_{ij}] = P(I_{ij} = 1)$, which is the probability that spots $i$ and $j$ collide. Let us denote this probability by $p_{\\text{coll}}$.\n$$\nE[I_{ij}] = p_{\\text{coll}}\n$$\nSince the cells are independently and identically distributed (uniformly over area $A$), the probability of collision, $p_{\\text{coll}}$, is the same for any pair of cells $(i, j)$.\n\nThe total number of terms in the summation is the number of ways to choose $2$ cells from $N$, which is $\\binom{N}{2} = \\frac{N(N-1)}{2}$.\nSubstituting this into the expression for $E[C]$, we get:\n$$\nE[C] = \\sum_{1 \\le i < j \\le N} p_{\\text{coll}} = \\binom{N}{2} p_{\\text{coll}} = \\frac{N(N-1)}{2} p_{\\text{coll}}\n$$\nThe next step is to calculate $p_{\\text{coll}}$ using geometric probability. Let the positions of the centers of two arbitrary cells, $i$ and $j$, be random variables $X_i$ and $X_j$, which are independently and uniformly distributed over the area $A$. A collision occurs if the distance between their centers, $d(X_i, X_j)$, is less than $2r$.\n\nTo calculate this probability, let us fix the position of cell $i$ at some point $x_i$ within the well. For a collision to occur, the center of cell $j$, $X_j$, must lie within a circle of radius $2r$ centered at $x_i$. The area of this \"collision region\" is:\n$$\nA_{\\text{coll}} = \\pi (2r)^2 = 4\\pi r^2\n$$\nThe problem states that boundary effects are negligible. This crucial assumption means that for any position $x_i$ of cell $i$, the entire collision region of area $A_{\\text{coll}}$ is considered to be within the well area $A$.\n\nSince cell $j$ is uniformly distributed over the total area $A$, the probability that it lands within this collision region is the ratio of the collision area to the total area:\n$$\np_{\\text{coll}} = P(d(X_i, X_j) < 2r) = \\frac{A_{\\text{coll}}}{A} = \\frac{4\\pi r^2}{A}\n$$\nThis probability is independent of the specific location of cell $i$, confirming that $p_{\\text{coll}}$ is constant for all pairs.\n\nNow, we substitute this expression for $p_{\\text{coll}}$ back into our equation for the expected number of total collisions:\n$$\nE[C] = \\frac{N(N-1)}{2} \\left(\\frac{4\\pi r^2}{A}\\right)\n$$\nSimplifying this expression gives the final analytic result for the expected number of collisions:\n$$\nE[C] = \\frac{2\\pi r^2 N(N-1)}{A}\n$$\nThis expression provides the expected number of overlapping spots as a function of the number of cells $N$, the spot radius $r$, and the well area $A$.\n\nTo justify why optimizing cell density is critical, we examine the relationship between $E[C]$ and the cell density, $\\rho = N/A$. The derived expression shows that $E[C]$ is proportional to $N(N-1)$. For a large number of cells ($N \\gg 1$), $N(N-1) \\approx N^2$. Therefore, the expected number of collisions scales approximately with the square of the number of cells:\n$$\nE[C] \\propto \\frac{N^2}{A} = \\left(\\frac{N}{A}\\right) N = \\rho N\n$$\nSince $N = \\rho A$, we can also write this as $E[C] \\propto \\rho (\\rho A) = A\\rho^2$. This quadratic dependence on cell density $\\rho$ is the key insight. It means that doubling the cell density results in approximately quadrupling the expected number of spot overlaps. A high rate of overlap compromises the fundamental principle of the ELISpot assay, which is to count individual cytokine-secreting cells. If spots merge, the count becomes inaccurate. Conversely, if the density is too low, the number of observed spots may be too small for statistically reliable measurement. Thus, there is an optimal range for cell density that balances the need for a sufficient number of spots against the need to keep overlaps to a minimum. The strong, non-linear increase in collisions with density makes this optimization a critical step in experimental design.", "answer": "$$\n\\boxed{\\frac{2\\pi r^2 N(N-1)}{A}}\n$$", "id": "5112787"}, {"introduction": "Once an ELISpot plate is developed, the resulting image contains not only the desired cytokine spots but also a variety of artifacts. To achieve objective and reproducible results, we must move beyond manual counting and employ automated analysis. This practice [@problem_id:5112779] places you in the role of designing such a system, using quantitative morphological features to build a statistical classifier. You will apply Linear Discriminant Analysis (LDA) in a realistic scenario with unequal misclassification costs and imbalanced data, learning how to create a decision rule that is not only accurate but also aligned with experimental priorities.", "problem": "In Enzyme-Linked ImmunoSpot (ELISpot), each cytokine-secreting cell yields a developed spot whose morphology can be quantified to discriminate true biological events from artifacts. Consider three per-spot features calculated from segmented images: area $A$ (in $\\mu\\mathrm{m}^2$), circularity $\\gamma$ (unitless, with $\\gamma \\approx 1$ for a perfect circle), and edge gradient magnitude $g$ (in intensity per pixel, summarizing boundary steepness). Assume that true cytokine spots are near-circular with moderate $A$ and high $g$, while artifacts (e.g., dust, scratches, precipitates) are irregular with lower $\\gamma$ and typically lower $g$.\n\nA curated training set contains $n_T = 200$ labeled true spots and $n_A = 800$ labeled artifacts, with class priors $\\pi_T = n_T/(n_T+n_A)$ and $\\pi_A = n_A/(n_T+n_A)$. Empirically, the feature vectors $\\mathbf{x} = (A,\\gamma,g)$ for true spots and artifacts are approximately multivariate Gaussian with equal covariance matrix. The sample means are\n$\\boldsymbol{\\mu}_T = (300,\\,0.85,\\,25)$ and $\\boldsymbol{\\mu}_A = (150,\\,0.55,\\,12)$,\nand the shared covariance matrix is diagonal,\n$\\Sigma = \\mathrm{diag}(900,\\,0.0064,\\,16)$.\nOperationally, missing a true spot is 5 times more costly than accepting an artifact, so $c_{\\mathrm{FN}} = 5$ and $c_{\\mathrm{FP}} = 1$.\n\nAssume a linear decision rule on a scalar discriminant $z$ constructed from $\\mathbf{x}$, and that performance must be reported under substantial class imbalance. Which option most appropriately uses the morphology to distinguish true spots from artifacts, specifies a statistically principled classifier with the correct cost-sensitive decision boundary under the stated generative assumptions, and identifies suitable performance evaluation metrics for this ELISpot setting?\n\nA. Use Linear Discriminant Analysis (LDA) with discriminant $z = \\mathbf{w}^\\top \\mathbf{x} + b$, where $\\mathbf{w} = \\Sigma^{-1}(\\boldsymbol{\\mu}_T - \\boldsymbol{\\mu}_A) = (0.1667,\\,46.8750,\\,0.8125)$ and $b = -\\tfrac{1}{2}\\left(\\boldsymbol{\\mu}_T^\\top \\Sigma^{-1} \\boldsymbol{\\mu}_T - \\boldsymbol{\\mu}_A^\\top \\Sigma^{-1} \\boldsymbol{\\mu}_A\\right) - \\ln\\left(\\frac{c_{\\mathrm{FP}} \\pi_A}{c_{\\mathrm{FN}} \\pi_T}\\right) \\approx -85.12$. Decide “true spot” if $z > 0$; otherwise “artifact.” Use $k$-fold cross-validation with $k = 5$, and evaluate with Precision-Recall (PR) curves, Area Under the Precision-Recall Curve (AUPRC), and the $F_1$ score at the chosen operating point, alongside sensitivity and specificity.\n\nB. Cluster the feature vectors with $k$-means ($k = 2$) on raw $(A,\\gamma,g)$ without standardization; then classify a spot as “true” if $\\gamma > 0.70$ and otherwise “artifact.” Select the operating point to maximize overall accuracy on the training set, and evaluate with Receiver Operating Characteristic (ROC) curves and Area Under the ROC Curve (AUROC).\n\nC. Fit a logistic regression model on raw $(A,\\gamma,g)$ without accounting for class priors or costs; use the default probability threshold of $0.50$ for classification. Evaluate primarily with overall accuracy and AUROC on a held-out validation set.\n\nD. Use LDA with discriminant $z = \\mathbf{w}^\\top \\mathbf{x} + b$, taking $\\mathbf{w} = \\boldsymbol{\\mu}_T - \\boldsymbol{\\mu}_A = (150,\\,0.30,\\,13)$ and $b = -\\tfrac{1}{2}\\left(\\boldsymbol{\\mu}_T^\\top \\boldsymbol{\\mu}_T - \\boldsymbol{\\mu}_A^\\top \\boldsymbol{\\mu}_A\\right) + \\ln\\left(\\frac{\\pi_T}{\\pi_A}\\right)$; decide “true” if $z > 0$. Tune the threshold to maximize accuracy, and report $F_1$ only if accuracy is below $0.95$; otherwise report AUROC.\n\nSelect the single best option.", "solution": "The problem statement has been validated and is determined to be a valid, well-posed problem in statistical pattern recognition applied to immunodiagnostics.\n\nThe problem requires constructing a classifier to distinguish true cytokine spots (class $T$) from artifacts (class $A$) based on a feature vector $\\mathbf{x} = (A, \\gamma, g)$. The problem states that the classes are modeled by multivariate Gaussian distributions with a shared covariance matrix $\\Sigma$. This is the canonical setting for Linear Discriminant Analysis (LDA). The optimal decision rule must also account for unequal class priors ($\\pi_T$, $\\pi_A$) and unequal misclassification costs ($c_{\\mathrm{FN}}$, $c_{\\mathrm{FP}}$).\n\nAccording to Bayes' decision theory, the rule that minimizes the expected cost is to classify an observation $\\mathbf{x}$ as \"true spot\" ($T$) if the expected cost of doing so is less than the expected cost of classifying it as \"artifact\" ($A$). The expected costs are:\n$$R(T|\\mathbf{x}) = c_{\\mathrm{TP}} P(T|\\mathbf{x}) + c_{\\mathrm{FP}} P(A|\\mathbf{x})$$\n$$R(A|\\mathbf{x}) = c_{\\mathrm{FN}} P(T|\\mathbf{x}) + c_{\\mathrm{TN}} P(A|\\mathbf{x})$$\nGiven that the costs of correct classification are zero ($c_{\\mathrm{TP}} = c_{\\mathrm{TN}} = 0$), we decide $T$ if $c_{\\mathrm{FP}} P(A|\\mathbf{x}) < c_{\\mathrm{FN}} P(T|\\mathbf{x})$. Using Bayes' theorem, $P(k|\\mathbf{x}) \\propto p(\\mathbf{x}|k)\\pi_k$, this inequality becomes:\n$$c_{\\mathrm{FP}} p(\\mathbf{x}|A)\\pi_A < c_{\\mathrm{FN}} p(\\mathbf{x}|T)\\pi_T$$\nRearranging and taking the natural logarithm, we decide $T$ if:\n$$\\ln\\left(\\frac{p(\\mathbf{x}|T)}{p(\\mathbf{x}|A)}\\right) > \\ln\\left(\\frac{c_{\\mathrm{FP}}\\pi_A}{c_{\\mathrm{FN}}\\pi_T}\\right)$$\nFor multivariate Gaussian distributions $p(\\mathbf{x}|k) = \\mathcal{N}(\\mathbf{x}; \\boldsymbol{\\mu}_k, \\Sigma)$, the log-likelihood ratio is:\n$$\\ln\\left(\\frac{p(\\mathbf{x}|T)}{p(\\mathbf{x}|A)}\\right) = \\mathbf{x}^\\top \\Sigma^{-1}(\\boldsymbol{\\mu}_T - \\boldsymbol{\\mu}_A) - \\frac{1}{2}(\\boldsymbol{\\mu}_T^\\top \\Sigma^{-1}\\boldsymbol{\\mu}_T - \\boldsymbol{\\mu}_A^\\top \\Sigma^{-1}\\boldsymbol{\\mu}_A)$$\nThus, the full decision rule is to classify as $T$ if:\n$$\\mathbf{x}^\\top \\Sigma^{-1}(\\boldsymbol{\\mu}_T - \\boldsymbol{\\mu}_A) - \\frac{1}{2}(\\boldsymbol{\\mu}_T^\\top \\Sigma^{-1}\\boldsymbol{\\mu}_T - \\boldsymbol{\\mu}_A^\\top \\Sigma^{-1}\\boldsymbol{\\mu}_A) - \\ln\\left(\\frac{c_{\\mathrm{FP}}\\pi_A}{c_{\\mathrm{FN}}\\pi_T}\\right) > 0$$\nThis defines a linear discriminant score $z = \\mathbf{w}^\\top \\mathbf{x} + b$, where:\n$$\\mathbf{w} = \\Sigma^{-1}(\\boldsymbol{\\mu}_T - \\boldsymbol{\\mu}_A)$$\n$$b = -\\frac{1}{2}(\\boldsymbol{\\mu}_T^\\top \\Sigma^{-1}\\boldsymbol{\\mu}_T - \\boldsymbol{\\mu}_A^\\top \\Sigma^{-1}\\boldsymbol{\\mu}_A) - \\ln\\left(\\frac{c_{\\mathrm{FP}}\\pi_A}{c_{\\mathrm{FN}}\\pi_T}\\right)$$\nThe decision is \"true spot\" if $z>0$.\n\nLet us now compute the numerical values.\nGiven:\n$\\boldsymbol{\\mu}_T = (300,\\,0.85,\\,25)$, $\\boldsymbol{\\mu}_A = (150,\\,0.55,\\,12)$\n$\\Sigma = \\mathrm{diag}(900,\\,0.0064,\\,16)$\n$c_{\\mathrm{FN}} = 5$, $c_{\\mathrm{FP}} = 1$\n$\\pi_T = 200/(200+800) = 0.2$, $\\pi_A = 800/(200+800) = 0.8$\n\nThe inverse covariance matrix is $\\Sigma^{-1} = \\mathrm{diag}(1/900,\\, 1/0.0064,\\, 1/16) = \\mathrm{diag}(1/900,\\, 156.25,\\, 0.0625)$.\nThe difference in means is $\\boldsymbol{\\mu}_T - \\boldsymbol{\\mu}_A = (150,\\,0.30,\\,13)$.\n\nThe weight vector $\\mathbf{w}$ is:\n$\\mathbf{w} = \\Sigma^{-1}(\\boldsymbol{\\mu}_T - \\boldsymbol{\\mu}_A) = (150/900,\\, 0.30/0.0064,\\, 13/16) = (1/6,\\, 46.875,\\, 0.8125)$.\n$1/6 \\approx 0.1667$. So, $\\mathbf{w} \\approx (0.1667, 46.875, 0.8125)$.\n\nThe bias term $b$ has two components. First, the quadratic part:\n$\\boldsymbol{\\mu}_T^\\top \\Sigma^{-1}\\boldsymbol{\\mu}_T = 300^2/900 + 0.85^2/0.0064 + 25^2/16 = 100 + 112.890625 + 39.0625 = 251.953125$.\n$\\boldsymbol{\\mu}_A^\\top \\Sigma^{-1}\\boldsymbol{\\mu}_A = 150^2/900 + 0.55^2/0.0064 + 12^2/16 = 25 + 47.265625 + 9 = 81.265625$.\nSo, $-\\frac{1}{2}(\\boldsymbol{\\mu}_T^\\top \\Sigma^{-1}\\boldsymbol{\\mu}_T - \\boldsymbol{\\mu}_A^\\top \\Sigma^{-1}\\boldsymbol{\\mu}_A) = -\\frac{1}{2}(251.953125 - 81.265625) = -\\frac{1}{2}(170.6875) = -85.34375$.\n\nSecond, the cost and prior term:\n$-\\ln\\left(\\frac{c_{\\mathrm{FP}}\\pi_A}{c_{\\mathrm{FN}}\\pi_T}\\right) = -\\ln\\left(\\frac{1 \\times 0.8}{5 \\times 0.2}\\right) = -\\ln\\left(\\frac{0.8}{1.0}\\right) = -\\ln(0.8) \\approx -(-0.22314) = 0.22314$.\n\nThe total bias is $b \\approx -85.34375 + 0.22314 = -85.12061$.\n\nFinally, the problem mentions \"substantial class imbalance,\" which necessitates careful selection of evaluation metrics. Accuracy is a poor choice. Metrics robust to imbalance, such as Precision-Recall curves, AUPRC (Area Under the PR Curve), and $F_1$ score, are appropriate. Sensitivity and specificity are also fundamental for understanding per-class performance. To ensure generalizability, performance should be estimated using a method like $k$-fold cross-validation.\n\nNow we evaluate each option:\n\n**A. Use Linear Discriminant Analysis (LDA) with discriminant $z = \\mathbf{w}^\\top \\mathbf{x} + b$, where $\\mathbf{w} = \\Sigma^{-1}(\\boldsymbol{\\mu}_T - \\boldsymbol{\\mu}_A) = (0.1667,\\,46.8750,\\,0.8125)$ and $b = -\\tfrac{1}{2}\\left(\\boldsymbol{\\mu}_T^\\top \\Sigma^{-1} \\boldsymbol{\\mu}_T - \\boldsymbol{\\mu}_A^\\top \\Sigma^{-1} \\boldsymbol{\\mu}_A\\right) - \\ln\\left(\\frac{c_{\\mathrm{FP}} \\pi_A}{c_{\\mathrm{FN}} \\pi_T}\\right) \\approx -85.12$. Decide “true spot” if $z > 0$; otherwise “artifact.” Use $k$-fold cross-validation with $k = 5$, and evaluate with Precision-Recall (PR) curves, Area Under the Precision-Recall Curve (AUPRC), and the $F_1$ score at the chosen operating point, alongside sensitivity and specificity.**\nThis option correctly identifies LDA as the principled classifier. The formulas for the weight vector $\\mathbf{w}$ and the bias term $b$ are correct, fully incorporating the covariance, priors, and costs to define the Bayes-optimal decision boundary. The numerical calculations for $\\mathbf{w}$ and $b$ match our derivation. The proposed evaluation methodology using $k$-fold cross-validation and metrics like PR curves, AUPRC, $F_1$, sensitivity, and specificity is the state-of-the-art for this type of imbalanced and cost-sensitive classification problem.\nVerdict: **Correct**.\n\n**B. Cluster the feature vectors with $k$-means ($k = 2$) on raw $(A,\\gamma,g)$ without standardization; then classify a spot as “true” if $\\gamma > 0.70$ and otherwise “artifact.” Select the operating point to maximize overall accuracy on the training set, and evaluate with Receiver Operating Characteristic (ROC) curves and Area Under the ROC Curve (AUROC).**\nThis option is fundamentally flawed. First, $k$-means is an unsupervised clustering algorithm and is inappropriate for a supervised classification problem where labeled data is available. Second, applying it to raw features with vastly different scales would lead to the feature with the largest magnitude ($A$) dominating the distance calculations. Third, the decision rule is an arbitrary univariate threshold on $\\gamma$, which discards the majority of the information and ignores the derived model. Fourth, maximizing accuracy is a poor objective for imbalanced datasets.\nVerdict: **Incorrect**.\n\n**C. Fit a logistic regression model on raw $(A,\\gamma,g)$ without accounting for class priors or costs; use the default probability threshold of $0.50$ for classification. Evaluate primarily with overall accuracy and AUROC on a held-out validation set.**\nWhile logistic regression is a valid classification algorithm, this option's approach is naive. It fails to account for the explicit costs ($c_{\\mathrm{FN}} = 5, c_{\\mathrm{FP}} = 1$), which is a critical requirement. The default probability threshold of $0.50$ is only optimal for equal costs and equal priors, neither of which holds here. The optimal probability threshold for deciding $T$ is $P(T|\\mathbf{x}) > c_{\\mathrm{FP}}/(c_{\\mathrm{FP}}+c_{\\mathrm{FN}}) = 1/6 \\approx 0.167$. Using $0.50$ is suboptimal. Also, evaluating primarily with accuracy is inappropriate for an imbalanced problem.\nVerdict: **Incorrect**.\n\n**D. Use LDA with discriminant $z = \\mathbf{w}^\\top \\mathbf{x} + b$, taking $\\mathbf{w} = \\boldsymbol{\\mu}_T - \\boldsymbol{\\mu}_A = (150,\\,0.30,\\,13)$ and $b = -\\tfrac{1}{2}\\left(\\boldsymbol{\\mu}_T^\\top \\boldsymbol{\\mu}_T - \\boldsymbol{\\mu}_A^\\top \\boldsymbol{\\mu}_A\\right) + \\ln\\left(\\frac{\\pi_T}{\\pi_A}\\right)$; decide “true” if $z > 0$. Tune the threshold to maximize accuracy, and report $F_1$ only if accuracy is below $0.95$; otherwise report AUROC.**\nThis option proposes an incorrect formulation of LDA. The weight vector $\\mathbf{w}$ is wrong because it omits the $\\Sigma^{-1}$ term, effectively assuming the covariance is an identity matrix and ignoring feature correlations and scales. The bias term $b$ is also incorrect; it omits $\\Sigma^{-1}$ and, most importantly, it ignores the specified asymmetric costs. The objective of maximizing accuracy is flawed. Finally, the conditional reporting strategy for evaluation metrics is arbitrary and lacks scientific principle.\nVerdict: **Incorrect**.\n\nBased on a thorough analysis, Option A is the only one that presents a statistically principled, quantitatively correct, and methodologically sound approach to the problem.", "answer": "$$\\boxed{A}$$", "id": "5112779"}, {"introduction": "After careful experimental setup and accurate spot enumeration, the final step is to synthesize the data from replicate wells into a single, meaningful biological metric. We are ultimately interested in the frequency $p$ of cytokine-secreting cells in our population, not just the raw spot counts. This exercise [@problem_id:5112724] guides you through the process of statistical inference required to estimate this frequency. By applying the principle of Maximum Likelihood Estimation to a Poisson model of spot formation, you will derive a robust formula for $p$ that represents the most likely value given your experimental observations.", "problem": "A research team uses Enzyme-Linked ImmunoSpot (ELISpot) to quantify the frequency of cytokine-secreting cells in a peripheral blood mononuclear cell sample. They plate $m$ replicate wells, each seeded with exactly $n$ input cells. In each well $i \\in \\{1,2,\\dots,m\\}$, they observe a nonnegative integer spot count $k_{i}$, which they take as their data. Assume that under the limiting dilution regime and independence of spot formation events, the total count $K_{i}$ in well $i$ is modeled as Poisson with mean $\\lambda = n p$, where $p$ is the per-cell secreting frequency (probability that a randomly chosen input cell secretes and forms a detectable spot). Assume the wells are independent and that $p \\in [0,1]$.\n\nStarting from the definition of the Poisson probability mass function and the principle of maximum likelihood, derive the maximum likelihood estimate for $p$ given the observed data $\\{k_{i}\\}_{i=1}^{m}$ and known $n$, subject to the constraint $0 \\leq p \\leq 1$. Express your final answer as a single closed-form analytic expression in terms of $m$, $n$, and the observed $\\{k_{i}\\}$. No numerical rounding is required, and no units are to be reported.", "solution": "The problem is to find the maximum likelihood estimate (MLE) for the per-cell secreting frequency, $p$, given $m$ replicate measurements of spot counts, $\\{k_i\\}_{i=1}^{m}$, from wells each containing $n$ cells.\n\nThe problem statement provides the model for the spot count $K_i$ in a single well $i \\in \\{1, 2, \\dots, m\\}$. The random variable $K_i$ is assumed to follow a Poisson distribution with mean $\\lambda = np$, where $n$ is the number of cells per well and $p$ is the probability that a single cell will secrete cytokine and form a spot. The parameter $p$ is constrained to be within the interval $[0, 1]$.\n\nThe probability mass function (PMF) for observing $k_i$ spots in well $i$ is given by the Poisson distribution:\n$$P(K_i = k_i | \\lambda) = \\frac{\\lambda^{k_i} \\exp(-\\lambda)}{k_i!}$$\nSubstituting $\\lambda = np$, the PMF in terms of $p$ is:\n$$P(K_i = k_i | p, n) = \\frac{(np)^{k_i} \\exp(-np)}{k_i!}$$\nThe observations $\\{k_i\\}_{i=1}^{m}$ are from $m$ independent wells. Therefore, the likelihood function $L(p)$, which is the joint probability of observing this specific data set, is the product of the individual probabilities for each well:\n$$L(p; \\{k_i\\}, n) = \\prod_{i=1}^{m} P(K_i = k_i | p, n) = \\prod_{i=1}^{m} \\frac{(np)^{k_i} \\exp(-np)}{k_i!}$$\nTo find the MLE of $p$, we seek the value of $p$ that maximizes $L(p)$ subject to the constraint $0 \\le p \\le 1$. It is more convenient to work with the natural logarithm of the likelihood function, the log-likelihood function $\\ell(p) = \\ln(L(p))$, because maximizing $\\ell(p)$ is equivalent to maximizing $L(p)$ and the logarithmic transformation simplifies the product into a sum.\n$$\\ell(p) = \\ln\\left( \\prod_{i=1}^{m} \\frac{(np)^{k_i} \\exp(-np)}{k_i!} \\right) = \\sum_{i=1}^{m} \\ln\\left( \\frac{(np)^{k_i} \\exp(-np)}{k_i!} \\right)$$\nUsing the properties of logarithms, we can expand this expression:\n$$\\ell(p) = \\sum_{i=1}^{m} \\left[ \\ln((np)^{k_i}) - \\ln(\\exp(np)) - \\ln(k_i!) \\right]$$\n$$\\ell(p) = \\sum_{i=1}^{m} \\left[ k_i \\ln(np) - np - \\ln(k_i!) \\right]$$\nWe can further separate the terms involving $p$:\n$$\\ell(p) = \\sum_{i=1}^{m} \\left[ k_i (\\ln(n) + \\ln(p)) - np - \\ln(k_i!) \\right]$$\n$$\\ell(p) = \\left(\\sum_{i=1}^{m} k_i\\right) \\ln(n) + \\left(\\sum_{i=1}^{m} k_i\\right) \\ln(p) - mnp - \\sum_{i=1}^{m} \\ln(k_i!)$$\nTo find the value of $p$ that maximizes $\\ell(p)$, we first find the critical points by taking the derivative of $\\ell(p)$ with respect to $p$ and setting it to zero. Terms that do not depend on $p$ will have a derivative of zero.\n$$\\frac{d\\ell}{dp} = \\frac{d}{dp} \\left[ \\left(\\sum_{i=1}^{m} k_i\\right) \\ln(p) - mnp + (\\text{constants}) \\right]$$\n$$\\frac{d\\ell}{dp} = \\frac{1}{p} \\left(\\sum_{i=1}^{m} k_i\\right) - mn$$\nSetting the derivative to zero gives the unconstrained estimate for $p$, which we denote $\\hat{p}_{\\text{unc}}$:\n$$\\frac{1}{\\hat{p}_{\\text{unc}}} \\left(\\sum_{i=1}^{m} k_i\\right) - mn = 0$$\n$$\\frac{1}{\\hat{p}_{\\text{unc}}} \\left(\\sum_{i=1}^{m} k_i\\right) = mn$$\n$$\\hat{p}_{\\text{unc}} = \\frac{\\sum_{i=1}^{m} k_i}{mn}$$\nTo confirm this is a maximum, we check the second derivative:\n$$\\frac{d^2\\ell}{dp^2} = -\\frac{1}{p^2} \\left(\\sum_{i=1}^{m} k_i\\right)$$\nSince $k_i \\ge 0$ for all $i$, the sum $\\sum k_i$ is non-negative. For any $p > 0$, $p^2$ is positive. Thus, if $\\sum k_i > 0$, the second derivative is negative, which confirms that $\\ell(p)$ is a concave function of $p$ and that $\\hat{p}_{\\text{unc}}$ is a maximum.\n\nNow, we must enforce the constraint that $p \\in [0, 1]$. The MLE, $\\hat{p}_{\\text{MLE}}$, must lie within this interval.\nLet $S = \\sum_{i=1}^{m} k_i$. The unconstrained estimate is $\\hat{p}_{\\text{unc}} = S / (mn)$.\n\nCase 1: $S=0$. This means all observed counts $k_i$ are $0$.\nThe log-likelihood becomes $\\ell(p) = -mnp - \\sum_{i=1}^{m} \\ln(0!) = -mnp$. The derivative is $\\frac{d\\ell}{dp} = -mn$, which is strictly negative since $m$ and $n$ are positive integers. The function $\\ell(p)$ is strictly decreasing for $p \\in [0, 1]$. Therefore, the maximum is achieved at the lower boundary, $p=0$. In this case, $\\hat{p}_{\\text{MLE}} = 0$.\n\nCase 2: $S>0$.\nThe log-likelihood function $\\ell(p)$ is concave for $p>0$.\n- If $0 < \\hat{p}_{\\text{unc}} < 1$, the unconstrained maximum lies within the interior of the valid parameter space. Thus, the MLE is the unconstrained estimate: $\\hat{p}_{\\text{MLE}} = \\hat{p}_{\\text{unc}} = \\frac{\\sum_{i=1}^{m} k_i}{mn}$.\n- If $\\hat{p}_{\\text{unc}} \\ge 1$, the unconstrained maximum is outside or on the upper boundary of the valid interval. Since $\\ell(p)$ is concave, its derivative $\\frac{d\\ell}{dp} = \\frac{S}{p} - mn = mn(\\frac{\\hat{p}_{\\text{unc}}}{p} - 1)$ is non-negative for all $p \\in (0, 1]$ because $\\frac{\\hat{p}_{\\text{unc}}}{p} \\ge 1$. This means $\\ell(p)$ is non-decreasing on $(0, 1]$. The maximum value over the interval $[0, 1]$ must therefore be at the upper boundary, $p=1$. So, $\\hat{p}_{\\text{MLE}} = 1$.\n- The case $\\hat{p}_{\\text{unc}} \\le 0$ with $S>0$ is not possible, as $m$, $n$, and $S$ are all positive.\n\nWe can combine these cases into a single expression. Since $\\sum k_i \\ge 0$, $m > 0$, and $n > 0$, the unconstrained estimate $\\hat{p}_{\\text{unc}}$ is always non-negative. The MLE is the value of $\\hat{p}_{\\text{unc}}$ if it is less than or equal to $1$, and $1$ otherwise. This can be written compactly using the minimum function:\n$$\\hat{p}_{\\text{MLE}} = \\min\\left(1, \\hat{p}_{\\text{unc}}\\right) = \\min\\left(1, \\frac{\\sum_{i=1}^{m} k_i}{mn}\\right)$$\nThis expression correctly evaluates to $0$ when all $k_i=0$, to $\\frac{\\sum k_i}{mn}$ when $0 < \\frac{\\sum k_i}{mn} < 1$, and to $1$ when $\\frac{\\sum k_i}{mn} \\ge 1$. This is the final, closed-form expression for the maximum likelihood estimate of $p$ subject to the given constraints.", "answer": "$$\\boxed{\\min\\left(1, \\frac{\\sum_{i=1}^{m} k_i}{mn}\\right)}$$", "id": "5112724"}]}