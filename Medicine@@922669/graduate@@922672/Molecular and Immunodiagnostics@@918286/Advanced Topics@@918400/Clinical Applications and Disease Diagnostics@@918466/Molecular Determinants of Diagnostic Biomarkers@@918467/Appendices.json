{"hands_on_practices": [{"introduction": "A foundational task in biomarker validation is determining the limit of detection (LoD), which defines the boundary between signal and noise. This practice explores how the LoD is not just a statistical value, but is rooted in the molecular determinants of the assay, such as non-specific binding and instrument noise. By analyzing hypothetical blank measurements from an immunoassay, you will calculate the LoD and justify the statistical assumptions that connect the macroscopic measurement to the underlying microscopic processes. [@problem_id:5134067]", "problem": "A sandwich immunoassay is being optimized to quantify a circulating glycoprotein biomarker whose epitope affinity and non-specific binding are known molecular determinants of assay background. The readout is fluorescence intensity in arbitrary units (a.u.). Under blank conditions (no analyte, matched matrix), the laboratory collected $n=12$ independent replicate fluorescence readings: $\\{40.2, 39.8, 41.1, 40.5, 38.9, 39.7, 40.3, 40.0, 41.4, 39.1, 40.6, 40.9\\}$. A linear calibration in the low-concentration regime was obtained from spiked standards, and near the blank the mean signal is described by $S(C) = \\mu_{\\text{blank}} + m\\,C$ with slope $m = 5.0$ a.u. per ng/mL, where $C$ is the analyte concentration in ng/mL. Assume the blank measurements are independent and identically distributed, and that the sources of background signal arise from many small, additive molecular processes such as non-specific antibody-antigen interactions, autofluorescence, and instrument noise.\n\nUsing the definition of the Limit of Detection (LoD) as the smallest signal exceeding the blank with a controlled false-positive risk under a Gaussian noise model, compute the LoD in signal units via $LoD_{\\text{signal}} = \\mu_{\\text{blank}} + 3\\,\\sigma_{\\text{blank}}$, where $\\mu_{\\text{blank}}$ and $\\sigma_{\\text{blank}}$ are the sample mean and the unbiased sample standard deviation of the blank readings. Then convert this decision threshold into concentration units using the provided local linear calibration, $LoD_{\\text{conc}} = (LoD_{\\text{signal}} - \\mu_{\\text{blank}})/m$. Express your final answer as $LoD_{\\text{conc}}$ in ng/mL. Round your answer to four significant figures.\n\nIn addition to the calculation, provide a principled discussion of the assumptions required for the normality of the blank distribution and homoscedasticity (constant variance) across low concentrations, grounded in first principles of measurement noise aggregation and the molecular determinants of non-specific signal.", "solution": "The problem statement has been critically validated and is deemed sound. It is scientifically grounded, well-posed, and objective. All necessary data, definitions, and conditions for a unique and meaningful solution are provided. The problem is a standard application of statistical principles to determine the limit of detection (LoD) in an analytical chemistry context, which is directly relevant to the field of molecular diagnostics. Therefore, I will proceed with a full solution.\n\nThe problem requires two parts: a calculation of the Limit of Detection in concentration units ($LoD_{\\text{conc}}$) and a principled discussion of the underlying statistical assumptions.\n\nFirst, I will address the theoretical justification for the assumptions of normality and homoscedasticity.\nThe problem assumes that the blank measurements follow a Gaussian (normal) distribution. This assumption is justified by the Central Limit Theorem (CLT). The background signal in an immunoassay is not due to a single process, but rather the aggregate effect of a great number of small, independent or weakly correlated stochastic events. As stated in the problem, these include non-specific binding of antibody reagents to the assay surface, autofluorescence from components of the biological matrix and the plastic well, and electronic noise from the measurement instrument (e.g., thermal and shot noise in the photodetector). If the total measured background noise is the sum of a large number of random variables representing these individual molecular and physical events, the CLT predicts that the distribution of this sum will approximate a normal distribution, irrespective of the specific probability distributions of the individual contributing processes, provided no single noise source is overwhelmingly dominant.\n\nThe second key assumption is homoscedasticity, which is the assumption of constant variance ($\\sigma^2$) across the low-concentration range of the calibration curve, including the blank. The total variance of a measurement, $\\sigma^2(C)$, can be modeled as a function of the analyte concentration, $C$. Typically, this variance has at least two components: a constant term, $\\sigma_{\\text{blank}}^2$, which represents the variance of the background noise sources discussed above, and a signal-dependent term, which often increases with the mean signal $S(C)$. For example, photon detection follows Poisson statistics, where the variance is equal to the mean number of detected photons. Thus, a more complete model might be $\\sigma^2(C) = \\sigma_0^2 + k \\cdot S(C)$, where $\\sigma_0^2$ is the signal-independent variance and $k$ is a proportionality constant. For concentrations near zero ($C \\to 0$), the signal $S(C)$ approaches the blank signal $\\mu_{\\text{blank}}$. In this regime, the signal-dependent contribution to the variance is minimal compared to the constant background variance. Therefore, assuming that the variance is constant and equal to the blank variance ($\\sigma^2(C) \\approx \\sigma_{\\text{blank}}^2$) is a reasonable and widely used simplification for determining the LoD.\n\nNow, I will proceed with the calculation. The problem provides $n=12$ independent blank replicate readings:\n$x = \\{40.2, 39.8, 41.1, 40.5, 38.9, 39.7, 40.3, 40.0, 41.4, 39.1, 40.6, 40.9\\}$ in arbitrary units (a.u.).\n\nThe sample mean, $\\mu_{\\text{blank}}$, is calculated as the arithmetic average of these readings:\n$$ \\mu_{\\text{blank}} = \\frac{1}{n} \\sum_{i=1}^{n} x_i $$\n$$ \\mu_{\\text{blank}} = \\frac{1}{12} (40.2 + 39.8 + 41.1 + 40.5 + 38.9 + 39.7 + 40.3 + 40.0 + 41.4 + 39.1 + 40.6 + 40.9) $$\n$$ \\mu_{\\text{blank}} = \\frac{482.5}{12} \\approx 40.20833... \\, \\text{a.u.} $$\n\nThe problem specifies the use of the unbiased sample standard deviation, $\\sigma_{\\text{blank}}$, which is calculated using the formula:\n$$ \\sigma_{\\text{blank}} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\mu_{\\text{blank}})^2} $$\nThe sum of squared differences from the mean, $\\sum_{i=1}^{n} (x_i - \\mu_{\\text{blank}})^2$, is:\n$$ \\sum (x_i - \\mu_{\\text{blank}})^2 \\approx (40.2 - 40.20833)^2 + (39.8 - 40.20833)^2 + \\dots + (40.9 - 40.20833)^2 \\approx 6.3525 $$\nThe unbiased sample variance, $s^2$, is:\n$$ s^2 = \\frac{6.3525}{12-1} = \\frac{6.3525}{11} \\approx 0.5775 $$\nTherefore, the unbiased sample standard deviation is:\n$$ \\sigma_{\\text{blank}} = \\sqrt{\\frac{6.3525}{11}} \\approx 0.7597348... \\, \\text{a.u.} $$\n\nThe problem defines the Limit of Detection in concentration units, $LoD_{\\text{conc}}$, through a two-step process. First, a signal threshold is defined:\n$$ LoD_{\\text{signal}} = \\mu_{\\text{blank}} + 3\\,\\sigma_{\\text{blank}} $$\nThen, this signal is converted to a concentration using the provided linear calibration model, $S(C) = \\mu_{\\text{blank}} + m\\,C$. The corresponding concentration is found by solving for $C$ when $S(C) = LoD_{\\text{signal}}$:\n$$ LoD_{\\text{conc}} = \\frac{LoD_{\\text{signal}} - \\mu_{\\text{blank}}}{m} $$\nSubstituting the expression for $LoD_{\\text{signal}}$ into this equation yields a simplified formula:\n$$ LoD_{\\text{conc}} = \\frac{(\\mu_{\\text{blank}} + 3\\,\\sigma_{\\text{blank}}) - \\mu_{\\text{blank}}}{m} = \\frac{3\\,\\sigma_{\\text{blank}}}{m} $$\nWe are given the calibration slope $m = 5.0$ a.u. per ng/mL. Substituting the calculated value for $\\sigma_{\\text{blank}}$ and the given value for $m$:\n$$ LoD_{\\text{conc}} = \\frac{3 \\times 0.7597348...}{5.0} = \\frac{2.2792044...}{5.0} \\approx 0.45584088... \\, \\text{ng/mL} $$\n\nThe problem requires the final answer to be rounded to four significant figures.\n$$ LoD_{\\text{conc}} \\approx 0.4558 \\, \\text{ng/mL} $$", "answer": "$$\\boxed{0.4558}$$", "id": "5134067"}, {"introduction": "Beyond simple detection, accurate quantification requires a robust calibration model that accounts for the nature of measurement error. This exercise moves into the domain of heteroscedasticity, a common feature in assays where signal variance is not constant, often due to physical determinants like photon shot noise. You will apply weighted least squares (WLS) regression, a powerful technique that uses an explicit error model to construct a more accurate calibration curve for a hypothetical electrochemiluminescence immunoassay. [@problem_id:5134101]", "problem": "An electrochemiluminescence (ECL) immunoassay is used to quantify a protein biomarker whose molecular recognition is mediated by high-affinity capture and detection antibodies. Under conditions where the binding is in the linear dynamic range of the assay and the ECL emission yield per bound analyte is constant, the expected signal is linear in concentration: $y_i = \\beta_0 + \\beta_1 c_i + \\varepsilon_i$, where $c_i$ is the calibrator concentration, $y_i$ is the measured mean photon count for calibrator $i$, and $\\varepsilon_i$ is the measurement error. The dominant noise sources are photon shot noise and instrument baseline read noise; for large counts, the Poisson-distributed shot noise is well approximated by a Gaussian with variance equal to its mean. The instrument baseline contributes an additive Gaussian noise with variance $\\sigma_b^2$. Thus, a validated error model from replicate measurements is $\\operatorname{Var}(y_i \\mid c_i) \\approx y_i + \\sigma_b^2$, with $\\sigma_b^2$ determined from blanks.\n\nYou are provided calibrator concentrations and corresponding mean signals measured on the same instrument session:\n- Concentrations (in nanomolar): $c = \\{0, 5, 10, 20\\}$.\n- Mean signals (in counts): $y = \\{100, 350, 600, 1100\\}$.\n- Baseline variance from blanks: $\\sigma_b^2 = 100$ counts$^2$.\n\nUsing the foundational principles above and the Gaussian approximation to Poisson statistics, construct the calibration curve by performing weighted least squares with weights $w_i = 1/\\operatorname{Var}(y_i \\mid c_i) = 1/(y_i + \\sigma_b^2)$. Then, use the resulting calibration to estimate the concentration $c^{\\ast}$ (in nanomolar) of an unknown sample that produced a mean signal $y^{\\ast} = 860$ counts in the same session. Round your final numerical estimate of $c^{\\ast}$ to three significant figures and express it in $\\mathrm{nM}$.", "solution": "The problem requires the construction of a calibration curve from a set of calibrator concentrations and their corresponding signals using the method of weighted least squares (WLS). Subsequently, this curve is to be used to determine the concentration of an unknown sample.\n\nThe relationship between the mean photon count, $y_i$, and the calibrator concentration, $c_i$, is given by the linear model:\n$$ y_i = \\beta_0 + \\beta_1 c_i + \\varepsilon_i $$\nwhere $\\beta_0$ is the intercept, $\\beta_1$ is the slope, and $\\varepsilon_i$ is the measurement error.\n\nThe error model specifies that the variance of the measured signal $y_i$ is dependent on the signal itself:\n$$ \\operatorname{Var}(y_i \\mid c_i) \\equiv \\sigma_i^2 \\approx y_i + \\sigma_b^2 $$\nThe problem provides the baseline variance from blank measurements as $\\sigma_b^2 = 100$ counts$^2$.\n\nThe method of weighted least squares is appropriate for linear regression when the error variances are not equal (heteroscedasticity). The parameters $\\beta_0$ and $\\beta_1$ are found by minimizing the weighted sum of squared residuals, $S$:\n$$ S(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} w_i (y_i - (\\beta_0 + \\beta_1 c_i))^2 $$\nThe weights, $w_i$, are the reciprocal of the variances:\n$$ w_i = \\frac{1}{\\sigma_i^2} = \\frac{1}{y_i + \\sigma_b^2} $$\n\nThe provided data for the $n=4$ calibrators are:\n-   Concentrations $c_i$ (in $\\mathrm{nM}$): $\\{0, 5, 10, 20\\}$\n-   Mean signals $y_i$ (in counts): $\\{100, 350, 600, 1100\\}$\n\nFirst, we calculate the variance and weight for each data point using $\\sigma_b^2 = 100$:\n-   For $i=1$: $c_1=0$, $y_1=100$. $\\sigma_1^2 = 100 + 100 = 200$. $w_1 = \\frac{1}{200}$.\n-   For $i=2$: $c_2=5$, $y_2=350$. $\\sigma_2^2 = 350 + 100 = 450$. $w_2 = \\frac{1}{450}$.\n-   For $i=3$: $c_3=10$, $y_3=600$. $\\sigma_3^2 = 600 + 100 = 700$. $w_3 = \\frac{1}{700}$.\n-   For $i=4$: $c_4=20$, $y_4=1100$. $\\sigma_4^2 = 1100 + 100 = 1200$. $w_4 = \\frac{1}{1200}$.\n\nMinimizing $S$ with respect to $\\beta_0$ and $\\beta_1$ leads to the normal equations, which can be expressed in matrix form as $(X^T W X) \\hat{\\beta} = X^T W Y$, where $\\hat{\\beta} = [\\beta_0, \\beta_1]^T$, $X$ is the design matrix, $W$ is a diagonal matrix of weights, and $Y$ is the vector of observed signals. The components of the matrices are sums involving the data points:\n$$ \\begin{pmatrix} \\sum w_i & \\sum w_i c_i \\\\ \\sum w_i c_i & \\sum w_i c_i^2 \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix} = \\begin{pmatrix} \\sum w_i y_i \\\\ \\sum w_i c_i y_i \\end{pmatrix} $$\n\nWe compute the required sums using the calculated weights:\n-   $\\sum w_i = \\frac{1}{200} + \\frac{1}{450} + \\frac{1}{700} + \\frac{1}{1200} = \\frac{239}{25200}$\n-   $\\sum w_i c_i = \\frac{1}{200}(0) + \\frac{1}{450}(5) + \\frac{1}{700}(10) + \\frac{1}{1200}(20) = \\frac{53}{1260}$\n-   $\\sum w_i c_i^2 = \\frac{1}{200}(0^2) + \\frac{1}{450}(5^2) + \\frac{1}{700}(10^2) + \\frac{1}{1200}(20^2) = \\frac{67}{126}$\n-   $\\sum w_i y_i = \\frac{1}{200}(100) + \\frac{1}{450}(350) + \\frac{1}{700}(600) + \\frac{1}{1200}(1100) = \\frac{769}{252}$\n-   $\\sum w_i c_i y_i = \\frac{1}{200}(0)(100) + \\frac{1}{450}(5)(350) + \\frac{1}{700}(10)(600) + \\frac{1}{1200}(20)(1100) = \\frac{1940}{63}$\n\nLet the system be denoted by $A \\beta_0 + B \\beta_1 = C$ and $B \\beta_0 + D \\beta_1 = E$, where:\n$A = \\sum w_i = \\frac{239}{25200}$, $B = \\sum w_i c_i = \\frac{53}{1260}$, $D = \\sum w_i c_i^2 = \\frac{67}{126}$, $C = \\sum w_i y_i = \\frac{769}{252}$, $E = \\sum w_i c_i y_i = \\frac{1940}{63}$.\n\nThe solution for $\\beta_0$ and $\\beta_1$ is given by Cramer's rule or matrix inversion:\n$$ \\beta_0 = \\frac{CD - BE}{AD - B^2} \\quad \\text{and} \\quad \\beta_1 = \\frac{AE - BC}{AD - B^2} $$\nWe calculate the determinant of the matrix, $\\Delta = AD - B^2$:\n$$ \\Delta = \\left(\\frac{239}{25200}\\right)\\left(\\frac{67}{126}\\right) - \\left(\\frac{53}{1260}\\right)^2 = \\frac{16013}{3175200} - \\frac{2809}{1587600} = \\frac{16013 - 5618}{3175200} = \\frac{10395}{3175200} = \\frac{11}{3360} $$\nNext, we calculate the numerators for $\\beta_0$ and $\\beta_1$:\nFor $\\beta_1$: $AE - BC = \\left(\\frac{239}{25200}\\right)\\left(\\frac{1940}{63}\\right) - \\left(\\frac{53}{1260}\\right)\\left(\\frac{769}{252}\\right) = \\frac{92732 - 40757}{317520} = \\frac{51975}{317520} = \\frac{55}{336}$.\nFor $\\beta_0$: $CD - BE = \\left(\\frac{769}{252}\\right)\\left(\\frac{67}{126}\\right) - \\left(\\frac{53}{1260}\\right)\\left(\\frac{1940}{63}\\right) = \\frac{51523}{31752} - \\frac{41128}{31752} = \\frac{10395}{31752} = \\frac{55}{168}$.\n\nNow we can solve for the parameters:\n$$ \\beta_1 = \\frac{AE - BC}{\\Delta} = \\frac{55/336}{11/3360} = \\frac{55}{336} \\cdot \\frac{3360}{11} = 5 \\cdot 10 = 50 $$\n$$ \\beta_0 = \\frac{CD - BE}{\\Delta} = \\frac{55/168}{11/3360} = \\frac{55}{168} \\cdot \\frac{3360}{11} = 5 \\cdot 20 = 100 $$\nThe resulting calibration curve is:\n$$ y = 100 + 50c $$\nThis equation happens to pass exactly through all given data points, which is a feature of the specific dataset provided.\n\nThe final step is to estimate the concentration $c^{\\ast}$ for an unknown sample that gives a mean signal of $y^{\\ast} = 860$ counts. We invert the calibration equation:\n$$ c^{\\ast} = \\frac{y^{\\ast} - \\beta_0}{\\beta_1} $$\nSubstituting the values:\n$$ c^{\\ast} = \\frac{860 - 100}{50} = \\frac{760}{50} = \\frac{76}{5} = 15.2 $$\nThe concentration is in nanomolar ($\\mathrm{nM}$). The problem asks for the answer to three significant figures, which $15.2$ already is.", "answer": "$$\\boxed{15.2}$$", "id": "5134101"}, {"introduction": "Modern diagnostics often leverage complex molecular signatures rather than single analyte concentrations. This practice delves into the frontier of liquid biopsies by modeling a classifier for the tissue-of-origin based on cell-free DNA (cfDNA) end-motif frequencies. You will derive a Bayesian classifier from first principles, demonstrating how a probabilistic framework can transform a high-dimensional molecular pattern into a diagnostic conclusion, and explore the computational methods needed for a numerically stable implementation. [@problem_id:5134070]", "problem": "A developer is asked to formalize a principled tissue-of-origin classifier grounded in the molecular determinants of diagnostic biomarkers. The molecular context is as follows: end-motif frequencies of cell-free Deoxyribonucleic Acid (cfDNA) fragments are tissue-dependent because nuclease preferences and chromatin architecture imprint characteristic cleavage patterns. For a fixed vocabulary of end motifs, one can use tissue-specific motif frequency profiles as a molecular signature.\n\nYour task is to derive a classifier that, given observed end-motif counts from a cfDNA sample, computes class posterior probabilities using a likelihood derived from the definition of the multinomial distribution and Bayes’ theorem. The derivation must begin strictly from the core definitions: the probability mass function of the multinomial distribution and the statement of Bayes’ theorem. You must reason from these bases to obtain a computable expression for the class posterior probabilities, then implement the computation in a numerically stable way.\n\nSet up:\n- There are $C = 3$ tissues: Liver, Lung, and Colon. Use the class order $[ \\text{Liver}, \\text{Lung}, \\text{Colon} ]$ throughout.\n- There are $K = 8$ end motifs. For each tissue $c \\in \\{1,2,3\\}$ and motif index $k \\in \\{1,\\dots,8\\}$, let $\\theta_{c,k}$ denote the tissue-specific probability of motif $k$. The tissue-specific probabilities are given by rows of the following matrix (each row sums to $1$ and all entries are positive):\n  - Liver: $\\left[0.30, 0.25, 0.20, 0.10, 0.10, 0.03, 0.01, 0.01\\right]$\n  - Lung: $\\left[0.10, 0.15, 0.25, 0.20, 0.15, 0.10, 0.03, 0.02\\right]$\n  - Colon: $\\left[0.05, 0.10, 0.10, 0.15, 0.20, 0.20, 0.10, 0.10\\right]$\n- The class priors are uniform: for each class $c$, $\\pi_c = 1/3$.\n\nDefinitions you must start from:\n- If $X = (X_1,\\dots,X_K)$ are counts of $K$ categories from $N = \\sum_{k=1}^K X_k$ independent trials with category probabilities $\\Theta = (\\theta_1,\\dots,\\theta_K)$, the multinomial probability mass function is defined by\n  $$P(X \\mid \\Theta) = \\frac{N!}{\\prod_{k=1}^K X_k!} \\prod_{k=1}^K \\theta_k^{X_k}.$$\n- Bayes’ theorem for discrete classes states\n  $$P(c \\mid X) = \\frac{P(X \\mid c)\\,\\pi_c}{\\sum_{c'} P(X \\mid c')\\,\\pi_{c'}}.$$\n\nDerivation requirements:\n- Begin with the above definitions and derive a computable expression for $P(c \\mid X)$ in terms of $\\pi_c$, $\\theta_{c,k}$, and $X_k$, making clear how the combinatorial factor in the multinomial probability mass function is handled when forming the posterior.\n- Explain how to implement the computation in log space to ensure numerical stability, and state how to normalize to obtain valid posterior probabilities that sum to $1$.\n- Describe how the boundary case of $N = 0$ (all counts are zero) reduces to a posterior equal to the prior under the adopted model and definitions.\n\nTest suite:\nFor each of the following observed count vectors $X \\in \\mathbb{N}_0^8$ (ordered by the $8$ motifs), compute the posterior vector $\\left[P(\\text{Liver} \\mid X), P(\\text{Lung} \\mid X), P(\\text{Colon} \\mid X)\\right]$, and round each posterior to exactly $6$ decimal places.\n\n- Case A (general case, $N = 100$): $X = [30, 26, 19, 10, 9, 3, 2, 1]$.\n- Case B (general case, $N = 100$): $X = [9, 16, 24, 20, 15, 10, 3, 3]$.\n- Case C (small-count edge, $N = 5$): $X = [0, 1, 1, 1, 1, 1, 0, 0]$.\n- Case D (ambiguous mix, $N = 100$): $X = [12, 12, 13, 13, 12, 13, 12, 13]$.\n- Case E (zero-count boundary, $N = 0$): $X = [0, 0, 0, 0, 0, 0, 0, 0]$.\n\nFinal output format:\n- Your program must produce a single line of output containing the list of posterior vectors for the test cases in the order A, B, C, D, E. Each posterior vector must be a list of three floats in the fixed class order $[ \\text{Liver}, \\text{Lung}, \\text{Colon} ]$, with each float rounded to exactly $6$ decimal places. The entire output must be a single list of lists printed on one line, for example: $[[a_{11},a_{12},a_{13}],[a_{21},a_{22},a_{23}],\\dots]$.", "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in established principles of molecular biology (tissue-specific cfDNA fragmentation) and statistics (Bayesian classification with a multinomial model). The problem is well-posed, with all necessary parameters, definitions, and data provided. It is objective, mathematically formalizable, and presents a non-trivial but solvable challenge. Therefore, a complete solution is provided.\n\nThe objective is to derive and implement a Bayesian classifier to determine the tissue-of-origin for a cell-free DNA (cfDNA) sample based on observed end-motif counts. The classifier must compute the posterior probability for each tissue class, $P(c \\mid X)$, given an observed count vector $X = (X_1, \\dots, X_K)$.\n\n### Step 1: Derivation of the Posterior Probability Expression\n\nThe derivation begins with the two definitions provided: Bayes' theorem and the probability mass function (PMF) of the multinomial distribution.\n\nBayes' theorem for a discrete set of classes $c \\in \\{1, \\dots, C\\}$ and observed data $X$ is given by:\n$$P(c \\mid X) = \\frac{P(X \\mid c)\\,\\pi_c}{\\sum_{c'=1}^C P(X \\mid c')\\,\\pi_{c'}}$$\nwhere $P(c \\mid X)$ is the posterior probability of class $c$, $P(X \\mid c)$ is the likelihood of observing data $X$ given class $c$, and $\\pi_c$ is the prior probability of class $c$.\n\nThe problem states that the observed end-motif counts $X = (X_1, \\dots, X_K)$ for a given tissue of origin $c$ follow a multinomial distribution. The parameters of this distribution are the total number of observed motifs, $N = \\sum_{k=1}^K X_k$, and the tissue-specific motif probabilities, $\\Theta_c = (\\theta_{c,1}, \\dots, \\theta_{c,K})$. The likelihood $P(X \\mid c)$ is therefore given by the multinomial PMF:\n$$P(X \\mid c) = P(X \\mid \\Theta_c) = \\frac{N!}{\\prod_{k=1}^K X_k!} \\prod_{k=1}^K \\theta_{c,k}^{X_k}$$\n\nSubstituting this likelihood expression into Bayes' theorem yields:\n$$P(c \\mid X) = \\frac{\\left( \\frac{N!}{\\prod_{k=1}^K X_k!} \\prod_{k=1}^K \\theta_{c,k}^{X_k} \\right) \\pi_c}{\\sum_{c'=1}^C \\left( \\frac{N!}{\\prod_{k=1}^K X_k!} \\prod_{k=1}^K \\theta_{c',k}^{X_k} \\right) \\pi_{c'}}$$\n\nA key simplification arises from observing the multinomial coefficient, $\\frac{N!}{\\prod_{k=1}^K X_k!}$. This term depends only on the total count $N$ and the individual counts $X_k$, but it does **not** depend on the class $c$. As such, it is a common factor in the numerator and in every term of the summation in the denominator. We can factor it out and cancel it:\n$$P(c \\mid X) = \\frac{\\cancel{\\left(\\frac{N!}{\\prod_{k=1}^K X_k!}\\right)} \\left( \\prod_{k=1}^K \\theta_{c,k}^{X_k} \\right) \\pi_c}{\\cancel{\\left(\\frac{N!}{\\prod_{k=1}^K X_k!}\\right)} \\sum_{c'=1}^C \\left( \\prod_{k=1}^K \\theta_{c',k}^{X_k} \\right) \\pi_{c'}}$$\n\nThis leads to the simplified, computable expression for the posterior probability:\n$$P(c \\mid X) = \\frac{\\left( \\prod_{k=1}^K \\theta_{c,k}^{X_k} \\right) \\pi_c}{\\sum_{c'=1}^C \\left( \\prod_{k=1}^K \\theta_{c',k}^{X_k} \\right) \\pi_{c'}}$$\nThe numerator is proportional to the joint probability $P(X, c)$, and the denominator is the marginal probability of the data $P(X)$, which serves as a normalization constant ensuring that $\\sum_{c=1}^C P(c \\mid X) = 1$.\n\n### Step 2: Numerically Stable Implementation in Log Space\n\nThe product term $\\prod_{k=1}^K \\theta_{c,k}^{X_k}$ involves multiplying many probabilities, which are numbers less than $1$. For large counts $X_k$, this product can become vanishingly small, leading to numerical underflow in standard floating-point arithmetic. To mitigate this, computations are performed in logarithmic space.\n\nWe define the unnormalized log-posterior for each class $c$, denoted $L_c$, which is the logarithm of the numerator in the simplified expression:\n$$L_c = \\log\\left( \\left( \\prod_{k=1}^K \\theta_{c,k}^{X_k} \\right) \\pi_c \\right) = \\log(\\pi_c) + \\log\\left(\\prod_{k=1}^K \\theta_{c,k}^{X_k}\\right)$$\nUsing the properties of logarithms, this becomes a sum:\n$$L_c = \\log(\\pi_c) + \\sum_{k=1}^K X_k \\log(\\theta_{c,k})$$\nThis form, involving sums of logarithms, is numerically stable and avoids underflow.\n\nThe posterior probability can be recovered by exponentiating and normalizing:\n$$P(c \\mid X) = \\frac{\\exp(L_c)}{\\sum_{c'=1}^C \\exp(L_{c'})}$$\nHowever, if the $L_c$ values are large and positive, or large and negative, $\\exp(L_c)$ can lead to overflow or underflow, respectively. To handle this, we use the log-sum-exp trick. We find the maximum value among all log-posteriors, $L_{\\max} = \\max_{c'} \\{L_{c'}\\}$, and subtract it from each $L_c$ before exponentiating:\n$$P(c \\mid X) = \\frac{\\exp(L_c - L_{\\max})}{\\sum_{c'=1}^C \\exp(L_{c'} - L_{\\max})}$$\nBy construction, the largest exponent is now $0$, and all other exponents are negative. This prevents overflow, and since at least one term in the denominator's sum is $\\exp(0)=1$, the denominator cannot be zero. This provides a robust method for computing the final posterior probabilities.\n\n### Step 3: Analysis of the Boundary Case $N=0$\n\nConsider the case where no motifs are observed, i.e., the count vector is $X = (0, 0, \\dots, 0)$. In this case, the total count is $N = \\sum_{k=1}^K X_k = 0$.\n\nApplying this to our expression for the unnormalized log-posterior $L_c$:\n$$L_c = \\log(\\pi_c) + \\sum_{k=1}^K 0 \\cdot \\log(\\theta_{c,k})$$\nSince $0$ multiplied by any finite number is $0$, the summation term vanishes:\n$$L_c = \\log(\\pi_c) + 0 = \\log(\\pi_c)$$\nThe unnormalized log-posterior is simply the log-prior.\n\nTo find the posterior probability, we exponentiate and normalize:\n$$P(c \\mid X=\\mathbf{0}) = \\frac{\\exp(L_c)}{\\sum_{c'} \\exp(L_{c'})} = \\frac{\\exp(\\log(\\pi_c))}{\\sum_{c'} \\exp(\\log(\\pi_{c'}))}$$\nThis simplifies to:\n$$P(c \\mid X=\\mathbf{0}) = \\frac{\\pi_c}{\\sum_{c'} \\pi_{c'}}$$\nGiven that the prior probabilities must sum to one ($\\sum_{c'} \\pi_{c'} = 1$), the expression reduces to:\n$$P(c \\mid X=\\mathbf{0}) = \\pi_c$$\nThis result is intuitive: in the complete absence of evidence (zero counts), our posterior belief about the tissue of origin is identical to our prior belief. For this problem, where the priors are uniform ($\\pi_c = 1/3$), the posterior probabilities for the $N=0$ case will be $(1/3, 1/3, 1/3)$.", "answer": "```\n[[1.000000,0.000000,0.000000],[0.000268,0.925001,0.074732],[0.250102,0.321017,0.428881],[0.000007,0.187313,0.812680],[0.333333,0.333333,0.333333]]\n```", "id": "5134070"}]}