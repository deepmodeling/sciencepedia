{"hands_on_practices": [{"introduction": "Before any biological interpretation of sequencing data can be made, its technical quality must be rigorously assessed. This exercise simulates a common diagnostic scenario where a challenging sample yields suboptimal quality control (QC) metrics. By synthesizing multiple QC data points, such as duplication rate and coverage uniformity, you will develop the crucial skill of diagnosing the root causes of poor sequencing performance and formulating a comprehensive corrective strategy [@problem_id:5167116].", "problem": "A clinical laboratory uses a targeted hybrid-capture gene panel for solid tumor diagnostics. The panel comprises approximately $2\\times 10^{6}$ targeted bases across cancer-associated genes, and libraries are sequenced with paired-end reads of $2\\times 150\\,\\mathrm{bp}$. For a particular Formalin-Fixed Paraffin-Embedded (FFPE) specimen, the Binary Alignment Map (BAM) quality control metrics report: total read pairs $N=12\\times 10^{6}$, median insert size $\\tilde{s}=110\\,\\mathrm{bp}$ with interquartile range $\\left[80,140\\right]\\,\\mathrm{bp}$, duplication rate $d=40\\%$, and on-target rate $f_{\\mathrm{on}}=50\\%$. The mean on-target coverage is reported as $\\bar{c}\\approx 400\\times$, while the fold-$80$ base penalty is $4.2$, and coverage profiles show under-representation of high guanine-cytosine (GC) content exons and over-representation of adenine-thymine (AT)-rich intervals. Pre-capture electropherograms show abundant adapter dimers, and the library required a high number of Polymerase Chain Reaction (PCR) cycles to reach quantifiable yield.\n\nUsing the following fundamental bases: (i) uniform coverage across targets arises when sampling is approximately homogeneous, which under constant capture efficiency yields per-base coverage modeled by a Poisson process with rate parameter $\\lambda$ and coefficient of variation $\\mathrm{CV}=1/\\sqrt{\\lambda}$; (ii) nonuniformity increases when capture efficiency varies across targets (e.g., due to GC content, probe tiling, and fragment size relative to probe length), effectively producing target-specific rates $\\lambda_{i}$ and a mixture of Poisson processes with heterogeneous means; (iii) the effective number of unique on-target molecules is reduced by duplication and off-target reads, approximated by $N_{\\mathrm{unique,on}}\\approx N\\cdot (1-d)\\cdot f_{\\mathrm{on}}$, and very short inserts increase read overlap and adapter contamination, further diminishing usable unique bases; (iv) PCR amplification bias and polymerase choice introduce GC-dependent amplification efficiency differences; and (v) hybridization kinetics depend on fragment length, GC content, probe length, tiling density, and hybridization stringency, select the most likely primary cause of the observed low uniformity and the most appropriate corrective actions to improve uniformity in subsequent runs.\n\nChoose the single best option.\n\nA. The low uniformity is primarily due to low-complexity, short-fragment FFPE libraries causing high duplication, read overlap, and GC-biased amplification, which jointly create heterogeneous $\\lambda_{i}$ across targets. Corrective actions: increase input DNA to $\\ge 50\\,\\mathrm{ng}$ when feasible, optimize fragmentation to target $200$–$250\\,\\mathrm{bp}$ inserts, reduce PCR cycle number and use a GC-balanced polymerase with appropriate additives (e.g., betaine or dimethyl sulfoxide), extend hybridization time and increase probe tiling for GC-rich exons, and incorporate Unique Molecular Identifiers (UMIs) to collapse duplicates.\n\nB. The low uniformity is primarily due to insufficient wash stringency during capture causing low $f_{\\mathrm{on}}$ and uniform retention of off-target fragments. Corrective actions: increase wash stringency and shorten hybridization to reduce off-target retention; other steps are unnecessary.\n\nC. The low uniformity is primarily caused by paired-end read overlap misalignments from $2\\times 150\\,\\mathrm{bp}$ reads on short inserts. Corrective actions: switch to single-end $75\\,\\mathrm{bp}$ sequencing and keep library preparation unchanged.\n\nD. The low uniformity is primarily due to bait synthesis failure affecting a subset of genes. Corrective actions: re-order probes and rerun capture with the same fragmentation and PCR protocol; other steps are unnecessary.", "solution": "## Problem Validation\n\n### Step 1: Extract Givens\nThe problem statement provides the following data and observations for a targeted hybrid-capture gene panel sequencing run on a Formalin-Fixed Paraffin-Embedded (FFPE) specimen:\n-   **Assay Parameters:**\n    -   Targeted region size: approximately $2\\times 10^{6}$ bases.\n    -   Sequencing mode: paired-end reads of $2\\times 150\\,\\mathrm{bp}$.\n-   **Specimen Type:** Formalin-Fixed Paraffin-Embedded (FFPE).\n-   **BAM Quality Control (QC) Metrics:**\n    -   Total read pairs: $N=12\\times 10^{6}$.\n    -   Median insert size: $\\tilde{s}=110\\,\\mathrm{bp}$.\n    -   Interquartile range of insert size: $\\left[80,140\\right]\\,\\mathrm{bp}$.\n    -   Duplication rate: $d=40\\%$.\n    -   On-target rate: $f_{\\mathrm{on}}=50\\%$.\n    -   Mean on-target coverage: $\\bar{c}\\approx 400\\times$.\n    -   Fold-$80$ base penalty: $4.2$.\n-   **Qualitative Observations:**\n    -   Pre-capture electropherograms show abundant adapter dimers.\n    -   The library required a high number of Polymerase Chain Reaction (PCR) cycles.\n    -   Coverage profiles show under-representation of high guanine-cytosine (GC) content exons and over-representation of adenine-thymine (AT)-rich intervals.\n-   **Provided Fundamental Bases (Principles):**\n    -   (i) Uniform coverage is modeled by a Poisson process with rate $\\lambda$, where non-uniformity is related to the coefficient of variation, $\\mathrm{CV}=1/\\sqrt{\\lambda}$.\n    -   (ii) Nonuniformity increases with variable capture efficiency, modeled as a mixture of Poisson processes with heterogeneous rates $\\lambda_{i}$.\n    -   (iii) The effective number of unique on-target molecules is $N_{\\mathrm{unique,on}}\\approx N\\cdot (1-d)\\cdot f_{\\mathrm{on}}$. Very short inserts increase read overlap.\n    -   (iv) PCR amplification can introduce GC-dependent bias.\n    -   (v) Hybridization kinetics depend on multiple factors including fragment length and GC content.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated based on the established criteria:\n\n1.  **Scientifically Grounded:** The problem is firmly grounded in the principles and practices of Next-Generation Sequencing (NGS) and molecular diagnostics. All terms (FFPE, hybrid-capture, BAM, insert size, duplication rate, on-target rate, Fold-$80$ penalty, GC bias, adapter dimers) are standard in the field. The provided \"fundamental bases\" are accurate, albeit simplified, representations of the underlying statistical and biochemical processes. The scenario described—a challenging FFPE sample yielding poor QC metrics—is highly realistic.\n2.  **Well-Posed:** The problem presents a set of symptoms (QC metrics) and asks for the most likely diagnosis of the underlying cause of poor performance and the most appropriate corrective actions. The data provided is sufficient to allow for a logical deduction and comparison of the given options. The question seeks the \"single best option,\" which is a standard format for complex diagnostic problems.\n3.  **Objective:** The problem is stated using precise, objective, and quantitative language. There is no subjectivity or ambiguity in the data provided.\n\n### Step 3: Verdict and Action\nThe problem statement is **VALID**. It is scientifically sound, well-posed, objective, and provides a realistic and complete set of data for a deductive analysis based on established principles of molecular biology and bioinformatics. The solution process can proceed.\n\n## Solution Derivation\n\nThe goal is to identify the primary cause of low uniformity and the best corrective strategy based on the provided QC metrics and observations. A systematic analysis of the data is required.\n\n1.  **Interpretation of QC Metrics and Observations:**\n    -   **Source Material (FFPE):** DNA extracted from FFPE tissue is known to be of low quality, characterized by fragmentation and chemical modifications (e.g., cytosine deamination) due to formalin fixation and embedding processes. This is the likely origin of the problems.\n    -   **Library Complexity:**\n        -   The high duplication rate ($d=40\\%$) is a primary indicator of low library complexity. This means that a large fraction of sequencing reads are simply PCR copies of a smaller, original set of unique DNA fragments. According to principle (iii), the number of unique on-target read pairs is approximately $N_{\\mathrm{unique,on}} \\approx (12\\times 10^{6}) \\cdot (1-0.4) \\cdot (0.5) = 3.6\\times 10^{6}$.\n        -   The requirement for a \"high number of PCR cycles\" is both a cause and a consequence of low complexity. With few initial template molecules, many cycles are needed to generate enough library material for sequencing, which in turn leads to high duplication rates.\n    -   **Fragmentation and Library Construction:**\n        -   The median insert size of $\\tilde{s}=110\\,\\mathrm{bp}$ is very short, especially for $2\\times 150\\,\\mathrm{bp}$ reads. This is consistent with fragmented FFPE DNA. Such short inserts lead to extensive read overlap. The total length sequenced per pair is $2 \\times 150\\,\\mathrm{bp} = 300\\,\\mathrm{bp}$. For an insert of $110\\,\\mathrm{bp}$, each read will sequence the entire insert and $40\\,\\mathrm{bp}$ of the adapter on the other side. This is inefficient but not the direct cause of inter-target non-uniformity.\n        -   The presence of \"abundant adapter dimers\" indicates an excess of adapter molecules relative to insertable DNA fragments during the ligation step, a common issue with low-quantity or very short-fragment input DNA.\n    -   **Uniformity and Bias:**\n        -   The Fold-$80$ base penalty of $4.2$ is a direct and quantitative measure of severe non-uniformity. It means that to achieve a minimum coverage of $0.2 \\times \\bar{c}$ on $80\\%$ of target bases, $4.2$ times more total sequencing was required than if coverage were perfectly uniform.\n        -   The explicit observation of under-representation of high-GC exons and over-representation of AT-rich intervals is a classic signature of PCR amplification bias (principle iv). Certain DNA polymerases struggle to amplify GC-rich templates, and this bias is exacerbated by a high number of PCR cycles.\n        -   This GC bias and other factors (variable hybridization of short fragments, principle v) lead to different effective amplification and capture efficiencies for different targets, resulting in a mixture of Poisson processes with heterogeneous rates $\\lambda_i$ (principle ii), manifesting as poor uniformity.\n\n2.  **Synthesis and Causality:**\n    The evidence points to a cascade of problems originating from the poor quality of the input FFPE DNA. The low amount of non-fragmented, amplifiable DNA leads to a low-complexity library. To compensate, high PCR cycles are used, which dramatically amplifies inherent polymerase biases, particularly against GC-rich regions. This GC bias is a major driver of the heterogeneous coverage rates ($\\lambda_i$) across targets, resulting in the high Fold-$80$ penalty. The short inserts and adapter dimers are symptoms of this same root cause. Therefore, the **primary cause of low uniformity** is the combination of low library complexity and PCR-induced bias.\n\n## Option-by-Option Analysis\n\n**A. The low uniformity is primarily due to low-complexity, short-fragment FFPE libraries causing high duplication, read overlap, and GC-biased amplification, which jointly create heterogeneous $\\lambda_{i}$ across targets. Corrective actions: increase input DNA to $\\ge 50\\,\\mathrm{ng}$ when feasible, optimize fragmentation to target $200$–$250\\,\\mathrm{bp}$ inserts, reduce PCR cycle number and use a GC-balanced polymerase with appropriate additives (e.g., betaine or dimethyl sulfoxide), extend hybridization time and increase probe tiling for GC-rich exons, and incorporate Unique Molecular Identifiers (UMIs) to collapse duplicates.**\n\n-   **Diagnosis:** This statement accurately identifies the entire chain of causality: low-quality FFPE starting material leads to a low-complexity library (short fragments, high duplication needing more PCR), which in turn causes GC-biased amplification. This directly creates the heterogeneous rates ($\\lambda_i$) responsible for non-uniformity. This aligns perfectly with our synthesis.\n-   **Corrective Actions:** The proposed actions are comprehensive and directly address each identified issue.\n    -   Increasing input DNA and optimizing fragmentation target the root cause of low complexity.\n    -   Reducing PCR cycles and using a better polymerase/additives directly mitigates GC bias.\n    -   Optimizing hybridization can improve capture efficiency for difficult regions.\n    -   Incorporating UMIs provides a powerful method to computationally correct for PCR duplication, improving quantitative accuracy and effective coverage uniformity.\n-   **Verdict:** **Correct**. This option provides the most accurate and complete explanation and a robust, multi-pronged corrective strategy.\n\n**B. The low uniformity is primarily due to insufficient wash stringency during capture causing low $f_{\\mathrm{on}}$ and uniform retention of off-target fragments. Corrective actions: increase wash stringency and shorten hybridization to reduce off-target retention; other steps are unnecessary.**\n\n-   **Diagnosis:** This focuses on the on-target rate ($f_{\\mathrm{on}}=50\\%$). While not ideal, this metric does not explain the primary problem of severe *on-target non-uniformity* (Fold-$80=4.2$) and the observed GC bias. Insufficient washing leads to more off-target reads but does not inherently create GC-dependent coverage patterns *on target*.\n-   **Corrective Actions:** Increasing wash stringency might improve $f_{\\mathrm{on}}$ but can also exacerbate non-uniformity by removing weakly hybridized fragments. Shortening hybridization is generally detrimental to capture efficiency. Declaring other steps \"unnecessary\" is incorrect as it ignores the clear evidence of low library complexity and PCR bias.\n-   **Verdict:** **Incorrect**. The diagnosis is misplaced, and the proposed actions are incomplete and potentially counterproductive.\n\n**C. The low uniformity is primarily caused by paired-end read overlap misalignments from $2\\times 150\\,\\mathrm{bp}$ reads on short inserts. Corrective actions: switch to single-end $75\\,\\mathrm{bp}$ sequencing and keep library preparation unchanged.**\n\n-   **Diagnosis:** Read overlap is a *symptom* of the short-fragment library, not the *cause* of the coverage non-uniformity across different gene targets. Modern alignment software handles overlap by soft-clipping. This issue does not explain why GC-rich exons have lower coverage than AT-rich intervals.\n-   **Corrective Actions:** Switching to shorter, single-end reads is a superficial fix that does not address the underlying library quality issues (low complexity, high duplication, PCR bias). The core problem of non-uniformity would persist. Keeping the library preparation unchanged would guarantee a repeat of the poor results.\n-   **Verdict:** **Incorrect**. The diagnosis confuses symptom with cause, and the corrective action is inadequate.\n\n**D. The low uniformity is primarily due to bait synthesis failure affecting a subset of genes. Corrective actions: re-order probes and rerun capture with the same fragmentation and PCR protocol; other steps are unnecessary.**\n\n-   **Diagnosis:** While bait failure can cause non-uniformity (specifically, drop-outs of entire targets), it is unlikely to produce the observed *systematic GC-dependent bias*. The data shows a correlation with sequence content (GC vs. AT), not a random failure of specific probes. Furthermore, this hypothesis fails to explain the other key metrics: high duplication, high PCR cycles, short inserts, and adapter dimers, which all point to a problem with the sample library itself.\n-   **Corrective Actions:** Re-ordering probes would only be effective if the baits were indeed faulty. Rerunning with the same flawed library preparation protocol would be a waste of resources, as the library quality is the demonstrated problem.\n-   **Verdict:** **Incorrect**. The diagnosis is inconsistent with the full set of evidence, particularly the GC-bias and library complexity metrics.", "answer": "$$\\boxed{A}$$", "id": "5167116"}, {"introduction": "Targeted gene panels are powerful tools for detecting not only single nucleotide variants but also larger structural changes like copy number variations (CNVs). This practice problem guides you through the fundamental process of inferring exon-level deletions from read-depth data, a key application in cancer and constitutional genetics. You will practice the essential steps of data normalization to correct for technical biases and apply a rule-based algorithm to call a heterozygous deletion, providing a clear understanding of how quantitative sequencing data is transformed into a discrete genetic finding [@problem_id:5167176].", "problem": "A targeted gene panel assay is used to interrogate a single autosomal gene with $10$ exons using Next-Generation Sequencing (NGS). In a cohort of reference diploid controls, the expected read depth per exon, denoted $E_i$ for exon $i$, has been estimated. For the test sample, the observed read depth per exon, $S_i$, and a per-exon multiplicative bias factor capturing guanine–cytosine (GC) content and capture efficiency, $g_i$, are provided. A per-sample global scaling factor $s$ was independently computed from a large set of copy-neutral regions to account for library size and composition effects. The data provided for this gene are:\n- Exon $1$: $E_1 = 1000$, $S_1 = 902$, $g_1 = 0.98$\n- Exon $2$: $E_2 = 950$, $S_2 = 891$, $g_2 = 1.02$\n- Exon $3$: $E_3 = 1020$, $S_3 = 948$, $g_3 = 1.01$\n- Exon $4$: $E_4 = 980$, $S_4 = 446$, $g_4 = 0.99$\n- Exon $5$: $E_5 = 1010$, $S_5 = 465$, $g_5 = 1.00$\n- Exon $6$: $E_6 = 990$, $S_6 = 469$, $g_6 = 1.03$\n- Exon $7$: $E_7 = 970$, $S_7 = 866$, $g_7 = 0.97$\n- Exon $8$: $E_8 = 1005$, $S_8 = 943$, $g_8 = 1.02$\n- Exon $9$: $E_9 = 995$, $S_9 = 897$, $g_9 = 0.98$\n- Exon $10$: $E_{10} = 985$, $S_{10} = 915$, $g_{10} = 1.01$\n\nThe per-sample global normalization factor is $s = 0.92$.\n\nUse the following fundamental base and rule-based calling approach, without invoking any additional black-box formulas:\n- Fundamental base: In targeted capture NGS, exon-level read depth is, to first order, proportional to the underlying copy number multiplied by region-specific capture efficiency and GC effects and a per-sample library size factor. After correcting the observed depth by the provided per-exon bias factor and normalizing by the per-sample global factor, the resulting ratio relative to the diploid expected depth reflects the copy-number ratio relative to the diploid baseline. Expressing this ratio on a base-$2$ logarithmic scale yields exon-level scores that are additive across multiplicative effects.\n- Calling rule: A heterozygous exon deletion (one-copy loss relative to the diploid baseline) is called for any segment comprising at least $k = 2$ contiguous exons whose base-$2$ logarithmic scores are all strictly below the deletion threshold $T_{\\mathrm{del}} = -0.6$.\n\nTasks:\n1. Starting from the fundamental base, derive a mathematically explicit expression for the exon-level base-$2$ logarithmic score from the provided quantities $(S_i, g_i, E_i, s)$, and compute these scores for each exon $i \\in \\{1,\\dots,10\\}$.\n2. Apply the calling rule to identify any heterozygous deletion segment(s) within the gene.\n3. Report the total number of exons in the gene that you infer to be heterozygously deleted. Express your final answer as an integer. No rounding is necessary.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. All necessary data and definitions are provided, and there are no internal contradictions or violations of established principles in molecular diagnostics or data analysis. The model for copy number estimation is a standard, simplified representation of workflows used in the field. Therefore, the problem is deemed valid and a solution can be derived.\n\nThe solution proceeds in three steps as delineated by the tasks.\n\n**Task 1: Derivation and computation of exon-level scores**\n\nThe problem provides a \"fundamental base\" for the analysis, which we must formalize into a mathematical expression. It states that the exon-level read depth is proportional to several factors. For the test sample, the observed read depth for exon $i$, $S_i$, is modeled. For the diploid reference controls ($CN=2$), the expected read depth is $E_i$.\n\nThe procedure to obtain the copy-number ratio is described as follows:\n1.  The observed depth $S_i$ is corrected for per-exon multiplicative bias $g_i$. This corresponds to the operation $S_i / g_i$.\n2.  The result is then normalized by the per-sample global scaling factor $s$. This gives $(S_i / g_i) / s$.\n3.  This normalized, corrected depth is then compared to the expected diploid depth $E_i$ by taking a ratio. Let us call this ratio $R_i$.\n    $$R_i = \\frac{(S_i / g_i) / s}{E_i} = \\frac{S_i}{s \\cdot g_i \\cdot E_i}$$\nThe problem states that this ratio $R_i$ \"reflects the copy-number ratio relative to the diploid baseline\". For a diploid baseline where the copy number is $2$, the copy-number ratio is $CN_i / 2$. Therefore, $R_i$ is our estimate for $CN_i / 2$.\n\nThe final step is to express this ratio on a base-$2$ logarithmic scale to obtain the exon-level score, which we denote as $L_i$.\n$$L_i = \\log_2(R_i) = \\log_2\\left(\\frac{S_i}{s \\cdot g_i \\cdot E_i}\\right)$$\nThis is the explicit expression for the exon-level score. For a normal diploid exon, we expect $CN_i=2$, so $R_i \\approx 1$ and $L_i \\approx \\log_2(1) = 0$. For a heterozygous deletion, we expect $CN_i=1$, so $R_i \\approx 1/2$ and $L_i \\approx \\log_2(1/2) = -1$.\n\nWe are given the global scaling factor $s = 0.92$. We now compute the score $L_i$ for each exon $i \\in \\{1, \\dots, 10\\}$.\n\n- Exon $1$: $E_1 = 1000$, $S_1 = 902$, $g_1 = 0.98$.\n$L_1 = \\log_2\\left(\\frac{902}{0.92 \\cdot 0.98 \\cdot 1000}\\right) = \\log_2\\left(\\frac{902}{901.6}\\right) \\approx \\log_2(1.00044) \\approx 0.00064$\n\n- Exon $2$: $E_2 = 950$, $S_2 = 891$, $g_2 = 1.02$.\n$L_2 = \\log_2\\left(\\frac{891}{0.92 \\cdot 1.02 \\cdot 950}\\right) = \\log_2\\left(\\frac{891}{891.24}\\right) \\approx \\log_2(0.99973) \\approx -0.00039$\n\n- Exon $3$: $E_3 = 1020$, $S_3 = 948$, $g_3 = 1.01$.\n$L_3 = \\log_2\\left(\\frac{948}{0.92 \\cdot 1.01 \\cdot 1020}\\right) = \\log_2\\left(\\frac{948}{947.664}\\right) \\approx \\log_2(1.00035) \\approx 0.00051$\n\n- Exon $4$: $E_4 = 980$, $S_4 = 446$, $g_4 = 0.99$.\n$L_4 = \\log_2\\left(\\frac{446}{0.92 \\cdot 0.99 \\cdot 980}\\right) = \\log_2\\left(\\frac{446}{892.704}\\right) \\approx \\log_2(0.49960) \\approx -1.0012$\n\n- Exon $5$: $E_5 = 1010$, $S_5 = 465$, $g_5 = 1.00$.\n$L_5 = \\log_2\\left(\\frac{465}{0.92 \\cdot 1.00 \\cdot 1010}\\right) = \\log_2\\left(\\frac{465}{929.2}\\right) \\approx \\log_2(0.50043) \\approx -0.9988$\n\n- Exon $6$: $E_6 = 990$, $S_6 = 469$, $g_6 = 1.03$.\n$L_6 = \\log_2\\left(\\frac{469}{0.92 \\cdot 1.03 \\cdot 990}\\right) = \\log_2\\left(\\frac{469}{937.524}\\right) \\approx \\log_2(0.50025) \\approx -0.9993$\n\n- Exon $7$: $E_7 = 970$, $S_7 = 866$, $g_7 = 0.97$.\n$L_7 = \\log_2\\left(\\frac{866}{0.92 \\cdot 0.97 \\cdot 970}\\right) = \\log_2\\left(\\frac{866}{865.052}\\right) \\approx \\log_2(1.00110) \\approx 0.00158$\n\n- Exon $8$: $E_8 = 1005$, $S_8 = 943$, $g_8 = 1.02$.\n$L_8 = \\log_2\\left(\\frac{943}{0.92 \\cdot 1.02 \\cdot 1005}\\right) = \\log_2\\left(\\frac{943}{942.876}\\right) \\approx \\log_2(1.00013) \\approx 0.00019$\n\n- Exon $9$: $E_9 = 995$, $S_9 = 897$, $g_9 = 0.98$.\n$L_9 = \\log_2\\left(\\frac{897}{0.92 \\cdot 0.98 \\cdot 995}\\right) = \\log_2\\left(\\frac{897}{897.076}\\right) \\approx \\log_2(0.99992) \\approx -0.00012$\n\n- Exon $10$: $E_{10} = 985$, $S_{10} = 915$, $g_{10} = 1.01$.\n$L_{10} = \\log_2\\left(\\frac{915}{0.92 \\cdot 1.01 \\cdot 985}\\right) = \\log_2\\left(\\frac{915}{914.372}\\right) \\approx \\log_2(1.00069) \\approx 0.00099$\n\n**Task 2: Application of the calling rule**\n\nThe calling rule states that a heterozygous deletion is identified for any segment of at least $k = 2$ contiguous exons where all scores $L_i$ are strictly below the threshold $T_{\\mathrm{del}} = -0.6$.\n\nFirst, we identify all exons whose score $L_i$ satisfies $L_i < -0.6$:\n- $L_1 \\approx 0.00064 \\not< -0.6$\n- $L_2 \\approx -0.00039 \\not< -0.6$\n- $L_3 \\approx 0.00051 \\not< -0.6$\n- $L_4 \\approx -1.0012 < -0.6$\n- $L_5 \\approx -0.9988 < -0.6$\n- $L_6 \\approx -0.9993 < -0.6$\n- $L_7 \\approx 0.00158 \\not< -0.6$\n- $L_8 \\approx 0.00019 \\not< -0.6$\n- $L_9 \\approx -0.00012 \\not< -0.6$\n- $L_{10} \\approx 0.00099 \\not< -0.6$\n\nThe exons that meet the score threshold are exons $4$, $5$, and $6$.\n\nNext, we check for contiguity and segment length. Exons $4$, $5$, and $6$ are contiguous, forming a single segment. The length of this segment is $3$ exons. Since the length $3$ is greater than or equal to the minimum required length $k=2$, this segment qualifies as a heterozygous deletion. There are no other such segments.\n\n**Task 3: Reporting the total number of deleted exons**\n\nBased on the analysis in Task 2, the segment of heterozygously deleted exons is identified as exons $4$, $5$, and $6$. The total number of exons within this identified segment is $3$.", "answer": "$$\n\\boxed{3}\n$$", "id": "5167176"}, {"introduction": "The ultimate goal of diagnostic sequencing is to determine the clinical significance of identified genetic variants, a process governed by formal evidence-based guidelines. This exercise places you in the role of a clinical scientist tasked with classifying a missense variant using the American College of Medical Genetics and Genomics/Association for Molecular Pathology (ACMG/AMP) framework. This case study will challenge you to properly weigh different types of evidence—population frequency, computational predictions, and case observations—to arrive at a defensible classification, highlighting the nuanced reasoning required for accurate clinical reporting [@problem_id:5167184].", "problem": "A targeted gene panel using Next-Generation Sequencing (NGS) is performed for a proband with a clinically suspected autosomal dominant cardiomyopathy. The panel includes genes with established gene–disease validity for hypertrophic cardiomyopathy. A single heterozygous missense variant is identified in a gene with a well-established disease association and a dominant mechanism. The variant has the following characteristics:\n\n- Population data: Observed in the Genome Aggregation Database (gnomAD) at allele frequency $5\\times 10^{-5}$ in the combined dataset, with no subpopulation exceeding $1\\times 10^{-4}$.\n- Computational data: Multiple independent tools (Sorting Intolerant From Tolerant (SIFT), PolyPhen-2, Combined Annotation Dependent Depletion (CADD)) predict a deleterious effect concordantly.\n- Case data: The variant has been reported in $2$ unrelated affected individuals across multiple laboratories, without demonstrated co-segregation in families and without functional assay data. No statistically significant enrichment over controls has been established.\n\nAssume no evidence for a mutational hot spot, no prior pathogenic missense variants affecting the same codon, and no truncating mechanism. The disease prevalence is approximately $2\\times 10^{-3}$, with incomplete penetrance and high allelic heterogeneity, such that generic American College of Medical Genetics and Genomics/Association for Molecular Pathology (ACMG/AMP) thresholds for benign population frequency (BS1 and BA1) are substantially higher than $5\\times 10^{-5}$ and not met here.\n\nUsing the ACMG/AMP framework, and optionally its Bayesian reinterpretation in which supporting, moderate, strong, and very strong evidence correspond to odds of pathogenicity multipliers of approximately $2.08$, $4.33$, $18.7$, and $350$, respectively, determine the most appropriate classification of this variant and justify which criteria apply and at what strength. Choose the best option.\n\nA. Variant of Uncertain Significance (VUS), because population rarity at $5\\times 10^{-5}$ supports PM2_Supporting, multiple deleterious in silico predictions support PP3, and limited case evidence supports PS4_Supporting, yielding only supporting-level evidence overall, which is insufficient for likely pathogenic.\n\nB. Likely Pathogenic, because population rarity at $5\\times 10^{-5}$ meets PM2 (moderate), in silico predictions meet PP3 (supporting), and limited case evidence meets PS4_Supporting, which together reach the threshold for likely pathogenic.\n\nC. Likely Benign, because a population frequency of $5\\times 10^{-5}$ meets BS1 (benign strong) and computational predictions are not considered decisive.\n\nD. Pathogenic, because observations in multiple affected individuals meet PS4 (strong), and deleterious computational predictions meet PP3, which together satisfy pathogenic classification.", "solution": "The user has provided a clinical genetics scenario and requested a variant classification based on the American College of Medical Genetics and Genomics/Association for Molecular Pathology (ACMG/AMP) framework.\n\n### Step 1: Extract Givens\n\nThe problem provides the following information about a single heterozygous missense variant identified in a proband with suspected autosomal dominant cardiomyopathy:\n*   **Gene and Disease:** The variant is in a gene with a well-established gene–disease association for a dominant condition.\n*   **Population Data:** The variant allele frequency in the Genome Aggregation Database (gnomAD) is $5 \\times 10^{-5}$ (combined). The maximum subpopulation frequency is less than $1 \\times 10^{-4}$.\n*   **Computational Data:** Multiple independent *in silico* prediction tools (SIFT, PolyPhen-2, CADD) concordantly predict a deleterious effect.\n*   **Case Data:** The variant has been observed in $2$ unrelated individuals with the phenotype. There is no demonstrated co-segregation in families. There is no available functional assay data. There is no statistically significant enrichment of the variant in cases over controls.\n*   **Disease Characteristics:** The disease prevalence is approximately $2 \\times 10^{-3}$. It exhibits incomplete penetrance and high allelic heterogeneity.\n*   **Constraints and Assumptions:**\n    *   No evidence for a mutational hot spot.\n    *   No prior pathogenic missense variants are known to affect the same codon.\n    *   The variant is not a truncating variant.\n    *   The observed allele frequency of $5 \\times 10^{-5}$ does not meet the established thresholds for benign criteria BS1 or BA1 for this disease.\n*   **Framework:** The analysis should use the ACMG/AMP framework, with the optional use of a Bayesian interpretation where evidence strengths correspond to specific odds multipliers: Supporting ($\\approx 2.08$), Moderate ($\\approx 4.33$), Strong ($\\approx 18.7$), and Very Strong ($\\approx 350$).\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement will now be validated against the required criteria.\n\n*   **Scientifically Grounded:** The problem is grounded in the established principles of medical genetics and molecular diagnostics. It utilizes standard terminology, databases (gnomAD), predictive tools (SIFT, PolyPhen-2, CADD), and the authoritative ACMG/AMP framework for variant interpretation. The disease prevalence, mode of inheritance, and genetic characteristics described (incomplete penetrance, allelic heterogeneity) are realistic for cardiomyopathies. The scenario represents a common and authentic challenge in a clinical-diagnostic setting.\n*   **Well-Posed:** The problem provides a specific set of evidence and asks for a classification within a defined, rule-based system (ACMG/AMP). The inclusion of the Bayesian framework provides a quantitative path to a solution, reinforcing its well-posed nature. The question is structured to have a single, justifiable answer based on the provided evidence and rules.\n*   **Objective:** The problem presents factual data (allele frequencies, computational results, case numbers) and established context (disease prevalence, inheritance pattern) in a neutral, objective manner.\n*   **Flaw Checklist:**\n    1.  **Scientific/Factual Unsoundness:** None. The premises are factually and scientifically sound.\n    2.  **Non-Formalizable/Irrelevant:** The problem is directly formalizable using the ACMG/AMP criteria.\n    3.  **Incomplete/Contradictory Setup:** The problem is well-specified. It provides all the necessary information to apply the relevant ACMG/AMP criteria. The explicit statement that BS1/BA1 are not met is a crucial piece of information that prevents misinterpretation of the population frequency data.\n    4.  **Unrealistic/Infeasible:** The scenario is highly realistic.\n    5.  **Ill-Posed/Poorly Structured:** The problem is well-structured and leads to a determinable solution.\n    6.  **Pseudo-Profound/Trivial:** The problem is not trivial; it requires nuanced application of the ACMG/AMP guidelines, particularly regarding the assignment of evidence strength, which is a key challenge in variant curation.\n    7.  **Outside Scientific Verifiability:** The conclusion is verifiable by applying the published ACMG/AMP guidelines.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A full solution will be derived.\n\n### Derivation of Solution\n\nThe classification of the variant will be determined by identifying the applicable ACMG/AMP criteria and their respective strengths, and then combining them according to the established rules.\n\n1.  **Analysis of Pathogenic Evidence Criteria:**\n\n    *   **PM2 (Population Data):** This criterion applies to a variant that is \"Absent from controls, or at extremely low frequency if recessive\". The variant is present in gnomAD at an allele frequency (AF) of $5 \\times 10^{-5}$. For a dominant disease, this is not \"absent\". The problem statement clarifies that this frequency does not trigger the benign criteria BS1/BA1. According to updated guidance from the ClinGen Sequence Variant Interpretation (SVI) working group, PM2 should be downgraded from moderate to supporting strength when a variant is present in controls at a very low frequency that does not meet benign criteria. Therefore, the most appropriate application is **PM2_Supporting**.\n\n    *   **PP3 (Computational Data):** This criterion is defined as \"Multiple lines of computational evidence support a deleterious effect on the gene or gene product\". The problem states that SIFT, PolyPhen-2, and CADD concordantly predict a deleterious effect. This perfectly matches the definition of PP3. By definition in the ACMG/AMP framework, PP3 is a **supporting** level of evidence. Thus, we apply **PP3** (supporting).\n\n    *   **PS4 (Case Data):** This criterion concerns the prevalence of the variant in affected individuals. The problem states the variant is found in $2$ unrelated affected individuals, but critically, with no demonstrated co-segregation and no statistically significant case-control enrichment. The full \"strong\" strength of PS4 requires robust statistical evidence. The \"moderate\" strength (PS4_moderate) is typically applied for observations in $\\ge 3$ probands. The observation in $2$ probands, given the variant is rare, qualifies for the **supporting** level of evidence, denoted as **PS4_Supporting**.\n\n2.  **Analysis of Benign Evidence Criteria:**\n\n    *   **BS1/BA1 (Population Data):** The problem explicitly states that the thresholds for these criteria are not met by the observed allele frequency. Therefore, no benign evidence can be applied based on population frequency.\n\n3.  **Combination of Evidence and Classification:**\n\n    We have accumulated three pieces of pathogenic evidence, all at the supporting level:\n    *   **PM2_Supporting**\n    *   **PP3** (supporting)\n    *   **PS4_Supporting**\n\n    According to the ACMG/AMP guidelines for combining criteria, there is no rule that allows `3` pieces of supporting-level evidence (or `1 PM_supporting + 2 PP`) to be combined to reach a \"Likely Pathogenic\" classification. The minimum combinations for \"Likely Pathogenic\" involve at least one moderate-strength criterion combined with other criteria (e.g., $\\ge 3$ PM), or a strong-strength criterion (e.g., `1 PS + 2 PP`). As the collected evidence does not meet these thresholds, and there is no conflicting benign evidence, the variant must be classified as a **Variant of Uncertain Significance (VUS)**.\n\n4.  **Verification with Bayesian Framework:**\n\n    The problem provides an optional Bayesian framework. Let's use it to confirm the classification.\n    *   Prior odds of Pathogenicity: $0.1$ (standard assumption).\n    *   Evidence multipliers: We have three pieces of \"supporting\" evidence, each with an odds multiplier of $2.08$.\n    *   Posterior Odds = (Prior Odds) $\\times$ (Multiplier for PM2_supp) $\\times$ (Multiplier for PP3) $\\times$ (Multiplier for PS4_supp)\n        $$ \\text{Posterior Odds} = 0.1 \\times 2.08 \\times 2.08 \\times 2.08 = 0.1 \\times (2.08)^3 \\approx 0.1 \\times 8.9989 \\approx 0.90 $$\n    *   To convert odds to probability, we use the formula $P = \\text{Odds} / (1 + \\text{Odds})$.\n        $$ P(\\text{Pathogenic}) = \\frac{0.90}{1 + 0.90} = \\frac{0.90}{1.90} \\approx 0.474 $$\n    *   The standard probability thresholds for ACMG/AMP classifications are:\n        *   Likely Pathogenic: $\\ge 0.90$\n        *   Variant of Uncertain Significance (VUS): $0.10$ to $< 0.90$\n        *   Likely Benign: $< 0.10$\n    *   Our calculated posterior probability of $\\approx 0.474$ falls squarely within the range for a **VUS**. This quantitative analysis confirms the conclusion from the qualitative rule-based approach.\n\n### Option-by-Option Analysis\n\n*   **A. Variant of Uncertain Significance (VUS), because population rarity at $5\\times 10^{-5}$ supports PM2_Supporting, multiple deleterious in silico predictions support PP3, and limited case evidence supports PS4_Supporting, yielding only supporting-level evidence overall, which is insufficient for likely pathogenic.**\n    This option correctly identifies the classification as VUS. It accurately assigns supporting strength to all three applicable criteria (PM2, PP3, PS4) based on a modern, nuanced interpretation of the ACMG/AMP guidelines. The reasoning that the combined supporting-level evidence is insufficient to reach the likely pathogenic threshold is sound and aligns with both the rule-based and Bayesian analyses.\n    **Verdict: Correct.**\n\n*   **B. Likely Pathogenic, because population rarity at $5\\times 10^{-5}$ meets PM2 (moderate), in silico predictions meet PP3 (supporting), and limited case evidence meets PS4_Supporting, which together reach the threshold for likely pathogenic.**\n    This option is incorrect. First, it overstates the strength of PM2 as \"moderate\"; for a variant present in population databases, the supporting level is more appropriate. Second, even with this inflated strength (`1` PM, `2` PP), the combination does not meet the ACMG/AMP threshold for \"Likely Pathogenic.\" As shown in the Bayesian calculation, this combination of evidence results in a posterior probability of $\\approx 0.65$, which is in the VUS range.\n    **Verdict: Incorrect.**\n\n*   **C. Likely Benign, because a population frequency of $5\\times 10^{-5}$ meets BS1 (benign strong) and computational predictions are not considered decisive.**\n    This option is factually incorrect. The problem statement explicitly stipulates that the allele frequency of $5 \\times 10^{-5}$ does **not** meet the benign criterion BS1 for this particular gene and disease context. This option directly contradicts a given premise of the problem.\n    **Verdict: Incorrect.**\n\n*   **D. Pathogenic, because observations in multiple affected individuals meet PS4 (strong), and deleterious computational predictions meet PP3, which together satisfy pathogenic classification.**\n    This option is incorrect. It grossly overestimates the strength of the case data evidence. PS4 can only be applied at \"strong\" strength with statistically significant evidence from a well-designed case-control study, which is explicitly stated to be absent. The evidence of $2$ probands without segregation data only warrants \"supporting\" strength (PS4_supporting). Furthermore, the a combination of `1` PS and `1` PP criterion is insufficient to reach a \"Pathogenic\" classification.\n    **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "5167184"}]}