{"hands_on_practices": [{"introduction": "The total data yield of a sequencing run is a primary metric of its success, but it is not an arbitrary figure. This exercise demonstrates how to calculate the expected yield from first principles, connecting tangible instrument specifications like cluster density and lane count to the final output in gigabases. By performing this calculation [@problem_id:5140026], you will gain a concrete understanding of how sequencing throughput is determined and how to benchmark a run's performance against its specifications.", "problem": "A core principle of high-throughput sequencing by synthesis in Next-Generation Sequencing (NGS) is that each sequencing cycle adds one nucleotide to each cluster that passes the instrument’s quality screen, commonly called passing filter (PF). Index cycles do not contribute to the reported genomic yield if yield is defined strictly as the sum of bases in Read 1 and Read 2. A patterned flow cell on an Illumina-style instrument is segmented into tiles within lanes; the number of clusters per tile and the PF fraction determine the number of usable clusters that generate base calls per cycle.\n\nConsider a run with the following scientifically realistic and internally consistent parameters:\n- Lanes: $2$ lanes per flow cell.\n- Tiles: $400$ imaging tiles per lane.\n- Clusters: an average of $5.0 \\times 10^{5}$ clusters per tile at the end of cluster generation and prior to PF filtering, uniformly distributed across tiles and lanes.\n- Passing filter (PF) fraction: $0.82$ of clusters pass the vendor’s chastity filter, assumed to apply uniformly across all tiles and lanes.\n- Read structure and cycles: Read $1$ uses $150$ cycles, Index $1$ uses $8$ cycles, Index $2$ uses $8$ cycles, and Read $2$ uses $150$ cycles. Only Read $1$ and Read $2$ contribute to the reported yield.\n- Manufacturer’s listed specification for this flow cell and read structure: $100$ gigabases per flow cell as the nominal yield target.\n\nStarting from first principles that one base is produced per PF cluster per sequencing cycle that contributes to the reported reads, and that the total number of PF clusters equals the product of lanes, tiles per lane, clusters per tile, and the PF fraction, compute the expected total number of bases produced by this run and convert the result to gigabases. Use $1$ gigabase $=$ $10^{9}$ bases.\n\nState in your reasoning whether the result is within $\\pm 10\\%$ of the instrument’s listed specification, but provide only the numerical value of the expected yield in gigabases as your final answer. Round your final answer to four significant figures and express it in gigabases (Gb).", "solution": "The fundamental basis of the calculation is that in sequencing by synthesis, each sequencing cycle appends one nucleotide to each cluster that passes quality metrics, and thus contributes one base call per PF cluster per applicable cycle. Index cycles add bases to index reads but do not contribute to the reported genomic yield if the yield is defined as bases in Read $1$ and Read $2$ only. Therefore, the total number of reported bases is the number of PF clusters multiplied by the total number of cycles in Read $1$ and Read $2$.\n\nFirst, compute the total number of clusters across the entire flow cell prior to PF filtering. With $2$ lanes, $400$ tiles per lane, and $5.0 \\times 10^{5}$ clusters per tile, the total is\n$$\nN_{\\text{clusters, total}} \\;=\\; 2 \\times 400 \\times \\left(5.0 \\times 10^{5}\\right) \\;=\\; 4.0 \\times 10^{8}.\n$$\n\nNext, apply the passing filter fraction $0.82$ to obtain the number of usable clusters:\n$$\nN_{\\text{PF}} \\;=\\; 0.82 \\times \\left(4.0 \\times 10^{8}\\right) \\;=\\; 3.28 \\times 10^{8}.\n$$\n\nNow determine the number of cycles that contribute to the reported yield. Read $1$ contributes $150$ cycles and Read $2$ contributes $150$ cycles, while Index $1$ ($8$ cycles) and Index $2$ ($8$ cycles) are excluded from the reported yield under the stated definition. Thus, the total number of reporting cycles is\n$$\nC_{\\text{reporting}} \\;=\\; 150 + 150 \\;=\\; 300.\n$$\n\nThe total number of bases reported is the product of the number of PF clusters and the number of reporting cycles:\n$$\nB_{\\text{total}} \\;=\\; N_{\\text{PF}} \\times C_{\\text{reporting}} \\;=\\; \\left(3.28 \\times 10^{8}\\right)\\times 300 \\;=\\; 9.84 \\times 10^{10} \\;\\text{bases}.\n$$\n\nConvert this to gigabases using $1$ gigabase $=$ $10^{9}$ bases:\n$$\nY_{\\text{Gb}} \\;=\\; \\frac{9.84 \\times 10^{10}}{10^{9}} \\;=\\; 98.4 \\;\\text{gigabases}.\n$$\n\nTo verify consistency with the instrument’s specification, compare $98.4$ gigabases to the nominal $100$ gigabases. The fractional deviation is\n$$\n\\frac{98.4 - 100}{100} \\;=\\; -0.016 \\;=\\; -1.6 \\times 10^{-2},\n$$\nwhich corresponds to a deviation of magnitude $0.016$, i.e., $1.6 \\times 10^{-2}$ in fractional terms, which lies within $\\pm 0.10$ (the $\\pm 10\\%$ tolerance). Therefore, the expected yield is consistent with the instrument’s listed specification.\n\nFinally, round the numerical yield to four significant figures. The value $98.4$ gigabases has three significant figures; to report four significant figures, write $98.40$.\n\nThus, the expected yield, rounded to four significant figures, is $98.40$ gigabases.", "answer": "$$\\boxed{98.40}$$", "id": "5140026"}, {"introduction": "Multiplexing, the process of pooling multiple samples into a single sequencing run, is essential for cost-effective high-throughput genomics. However, a phenomenon known as \"index hopping\" can lead to the misassignment of reads, causing cross-sample contamination that can confound diagnostic results. This practice [@problem_id:5139951] guides you through building a probabilistic model to quantify this contamination, revealing the stark difference in data integrity between single and unique dual indexing strategies.", "problem": "A multiplexed Next-Generation Sequencing (NGS) run pools $N$ equally sized samples, each tagged by index sequences to enable demultiplexing. Consider two demultiplexing strategies:\n- Single indexing using one index (e.g., the Illumina i7 index).\n- Unique dual indexing (UDI) using two independent indexes (e.g., both i7 and i5), with demultiplexing requiring exact pair matches.\n\nIndex hopping is defined as follows: for each index present on a read, with probability $h$ the index is replaced by a randomly chosen index from the other $N-1$ index sequences present in the pool (that is, the replacement never equals the original index), and with probability $1-h$ it remains unchanged. For dual indexing, the two indexes hop independently according to the same rule. Demultiplexing assigns reads to samples by exact index match for single indexing, and by exact pair match for UDI; any read whose observed index (single indexing) or observed index pair (UDI) does not match any sample’s assigned tag(s) is discarded and does not contribute to contamination.\n\nDefine the contamination fraction for a given sample as the expected fraction of reads assigned to that sample that originated from other samples in the pool. Using only the axioms of probability, the law of total probability, and independence of index hopping events, derive closed-form expressions for the contamination fraction under single indexing and under UDI, as functions of the hopping rate $h$ and pool size $N$. Assume all samples contribute equally to the pool.\n\nExpress the contamination fractions as decimals or fractions. Provide your final answers in terms of $h$ and $N$ as a single row matrix $\\begin{pmatrix}C_{\\text{single}}  C_{\\text{dual}}\\end{pmatrix}$.", "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. The derivation proceeds as follows.\n\nLet $N$ be the number of samples in the pool. Let $h$ be the probability that a single index hops. All samples are assumed to contribute equally to the initial pool of sequencing reads.\n\nThe contamination fraction for a given sample, let us say sample $1$, is defined as the expected fraction of reads assigned to that sample that originated from other samples. Let $R_i$ be the event that a read originates from sample $i$, and let $A_j$ be the event that a read is assigned to sample $j$ after demultiplexing. The contamination fraction for sample $1$, denoted $C$, can be expressed as the ratio of the expected number of contaminating reads assigned to sample $1$ to the total expected number of reads assigned to sample $1$.\n\n$$C = \\frac{\\sum_{i=2}^{N} E[\\text{reads from } i \\text{ assigned to } 1]}{E[\\text{reads from } 1 \\text{ assigned to } 1] + \\sum_{i=2}^{N} E[\\text{reads from } i \\text{ assigned to } 1]}$$\n\nDue to the assumption of equally sized samples, all samples contribute an equal number of reads. Therefore, the expected numbers are directly proportional to the conditional probabilities of assignment given the true origin. Let $P(A_j | R_i)$ be the probability that a read from sample $i$ is assigned to sample $j$. The contamination fraction is then:\n\n$$C = \\frac{\\sum_{i=2}^{N} P(A_1 | R_i)}{\\sum_{i=1}^{N} P(A_1 | R_i)} = \\frac{\\sum_{i=2}^{N} P(A_1 | R_i)}{P(A_1 | R_1) + \\sum_{i=2}^{N} P(A_1 | R_i)}$$\n\nBy symmetry, the probability of a read from any contaminating sample $i \\neq 1$ being mis-assigned to sample $1$ is the same. Let this probability be $P_{\\text{contam}} = P(A_1 | R_i)$ for any $i \\neq 1$. Let the probability of a read from sample $1$ being correctly assigned to sample $1$ be $P_{\\text{correct}} = P(A_1 | R_1)$. The expression simplifies to:\n\n$$C = \\frac{(N-1) P_{\\text{contam}}}{P_{\\text{correct}} + (N-1) P_{\\text{contam}}}$$\n\nWe will now derive expressions for $P_{\\text{correct}}$ and $P_{\\text{contam}}$ for both single and unique dual indexing.\n\n**Single Indexing ($C_{\\text{single}}$)**\n\nIn this case, a read has one index. Demultiplexing assigns a read to sample $1$ if its observed index matches the index originally assigned to sample $1$.\n\n1.  **$P_{\\text{correct, single}}$**: A read originates from sample $1$. To be correctly assigned, its index must not hop. The probability of an index not hopping is given as $1-h$.\n    $$P_{\\text{correct, single}} = 1-h$$\n\n2.  **$P_{\\text{contam, single}}$**: A read originates from another sample, $i \\neq 1$. To be mis-assigned to sample $1$, its index must change to that of sample $1$. This requires two events:\n    a) The index must hop. The probability of this is $h$.\n    b) The hopped index must be replaced by the specific index of sample $1$. The problem states the replacement is a random choice from the other $N-1$ indexes in the pool. The probability of choosing the index of sample $1$ is therefore $\\frac{1}{N-1}$.\n    The combined probability is the product of these independent steps:\n    $$P_{\\text{contam, single}} = h \\times \\frac{1}{N-1} = \\frac{h}{N-1}$$\n\nNow we substitute these into the general formula for the contamination fraction:\n\n$$C_{\\text{single}} = \\frac{(N-1) P_{\\text{contam, single}}}{P_{\\text{correct, single}} + (N-1) P_{\\text{contam, single}}} = \\frac{(N-1) \\left(\\frac{h}{N-1}\\right)}{(1-h) + (N-1) \\left(\\frac{h}{N-1}\\right)} = \\frac{h}{1-h+h} = h$$\n\nThus, the contamination fraction for single indexing is simply the index hopping rate $h$.\n\n**Unique Dual Indexing ($C_{\\text{dual}}$)**\n\nIn this case, a read has two independent indexes. Demultiplexing requires an exact match for the pair of indexes assigned to a sample. The two indexes hop independently, each with probability $h$.\n\n1.  **$P_{\\text{correct, dual}}$**: A read originates from sample $1$. To be correctly assigned, its observed index pair must match the original index pair of sample $1$. This requires that *neither* of the two indexes hops. Since the hopping events are independent, we multiply their probabilities.\n    $$P_{\\text{correct, dual}} = (1-h)(1-h) = (1-h)^2$$\n\n2.  **$P_{\\text{contam, dual}}$**: A read originates from another sample, $i \\neq 1$. To be mis-assigned to sample $1$, its observed index pair must exactly match the index pair of sample $1$. This requires that *both* of its indexes hop to the specific corresponding indexes of sample $1$.\n    a) The first index must hop to sample $1$'s first index. The probability for this is $h \\times \\frac{1}{N-1}$.\n    b) The second index must hop to sample $1$'s second index. The probability for this is also $h \\times \\frac{1}{N-1}$.\n    Since the two hops are independent events, the total probability is the product:\n    $$P_{\\text{contam, dual}} = \\left(\\frac{h}{N-1}\\right) \\left(\\frac{h}{N-1}\\right) = \\frac{h^2}{(N-1)^2}$$\n\nNow we substitute these into the general formula for the contamination fraction:\n\n$$C_{\\text{dual}} = \\frac{(N-1) P_{\\text{contam, dual}}}{P_{\\text{correct, dual}} + (N-1) P_{\\text{contam, dual}}} = \\frac{(N-1) \\frac{h^2}{(N-1)^2}}{(1-h)^2 + (N-1) \\frac{h^2}{(N-1)^2}}$$\n$$C_{\\text{dual}} = \\frac{\\frac{h^2}{N-1}}{(1-h)^2 + \\frac{h^2}{N-1}}$$\n\nTo simplify, we multiply the numerator and denominator by $N-1$:\n\n$$C_{\\text{dual}} = \\frac{h^2}{(N-1)(1-h)^2 + h^2}$$\n\nThe final expressions for the contamination fractions are $C_{\\text{single}} = h$ and $C_{\\text{dual}} = \\frac{h^2}{(N-1)(1-h)^2 + h^2}$.", "answer": "$$\\boxed{\\begin{pmatrix}h  \\frac{h^2}{(N-1)(1-h)^2 + h^2}\\end{pmatrix}}$$", "id": "5139951"}, {"introduction": "After sequencing, a common goal is to assemble the millions of short reads into a coherent genome. The quality of this *de novo* assembly is measured not only by its accuracy but also by its contiguity, which is quantified by metrics like the $N50$ statistic. This exercise [@problem_id:5139932] provides practical experience in calculating the scaffold $N50$, a critical measure of assembly completeness, and illustrates how long-range information is used to link smaller contigs into larger scaffolds, thereby improving the final assembly.", "problem": "An Illumina short-read de novo assembly of a bacterial genome produced the following contigs with lengths in base pairs (bp): $180000$, $120000$, $95000$, $60000$, $55000$, $52000$, $40000$, $35000$, $30000$, $28000$, $25000$, $20000$, $18000$, $15000$, $12000$. The assembly team then applied scaffolding using two orthogonal sources of long-range information: (i) an Illumina mate-pair library with mean insert sizes on the order of several kilobase pairs and (ii) long reads from Pacific Biosciences (PacBio) and Oxford Nanopore Technologies (ONT). After resolving order and orientation, the team proposed the following scaffolds and gap estimates (gaps are represented as stretches of $N$ bases and are to be counted toward scaffold lengths):\n\n- Scaffold $S_{1}$: join contigs of lengths $180000$, $52000$, $40000$, and $25000$ with inter-contig gap estimates $3000$, $2000$, and $1000$ bp, in that order.\n- Scaffold $S_{2}$: join contigs of lengths $120000$, $60000$, and $15000$ with inter-contig gap estimates $5000$ and $2000$ bp.\n- Scaffold $S_{3}$: join contigs of lengths $95000$, $55000$, $35000$, and $30000$ with inter-contig gap estimates $4000$, $3000$, and $2000$ bp.\n- Scaffold $S_{4}$: join contigs of lengths $28000$, $20000$, and $12000$ with inter-contig gap estimates $1000$ and $1000$ bp.\n- Scaffold $S_{5}$: a single contig of length $18000$ is left unscaffolded.\n\nAll contigs listed above are used exactly once, and no additional sequence is introduced aside from the specified gaps. Assume that the total assembled length is the sum of all scaffold lengths including the gaps.\n\nStarting from the core definition that contigs are continuous sequences assembled from overlapping reads and scaffolds are ordered and oriented collections of contigs separated by estimated gaps derived from long-range linkage, and using the standard definition of $N50$ as the length-weighted median of contig or scaffold sizes—namely, the value $L$ such that at least half of the total assembly length is contained in sequences of length $\\geq L$—compute the scaffold $N50$ for this assembly. In your derivation, explain conceptually (without introducing any ad hoc formulas beyond these definitions) why scaffolding using mate pairs and long reads tends to increase $N50$ and how such increases can be accompanied by risks of misassembly that must be validated in molecular and immunodiagnostics workflows.\n\nExpress your final $N50$ in base pairs (bp). No rounding is required.", "solution": "The problem requires the calculation of the scaffold $N50$ for a de novo genome assembly and a conceptual discussion of the implications of scaffolding. The first step is to validate the problem statement. All provided data, including contig lengths and scaffolding instructions, are explicit and internally consistent. The definitions of contig, scaffold, and $N50$ are standard in the field of genomics. The problem is scientifically grounded, well-posed, and objective. We can proceed with the solution.\n\nThe primary task is to compute the scaffold $N50$. The definition of $N50$ is the length $L$ such that at least half of the total assembly length is contained in sequences of length greater than or equal to $L$. To find this value, we must first determine the lengths of all individual scaffolds and the total length of the assembly.\n\n1.  **Calculate the length of each scaffold:**\n    -   Scaffold $S_{1}$: This scaffold joins contigs of lengths $180000$, $52000$, $40000$, and $25000$ bp with three gaps of lengths $3000$, $2000$, and $1000$ bp.\n        $$L(S_{1}) = 180000 + 3000 + 52000 + 2000 + 40000 + 1000 + 25000 = 303000 \\text{ bp}$$\n    -   Scaffold $S_{2}$: This scaffold joins contigs of lengths $120000$, $60000$, and $15000$ bp with two gaps of lengths $5000$ and $2000$ bp.\n        $$L(S_{2}) = 120000 + 5000 + 60000 + 2000 + 15000 = 202000 \\text{ bp}$$\n    -   Scaffold $S_{3}$: This scaffold joins contigs of lengths $95000$, $55000$, $35000$, and $30000$ bp with three gaps of lengths $4000$, $3000$, and $2000$ bp.\n        $$L(S_{3}) = 95000 + 4000 + 55000 + 3000 + 35000 + 2000 + 30000 = 224000 \\text{ bp}$$\n    -   Scaffold $S_{4}$: This scaffold joins contigs of lengths $28000$, $20000$, and $12000$ bp with two gaps of lengths $1000$ and $1000$ bp.\n        $$L(S_{4}) = 28000 + 1000 + 20000 + 1000 + 12000 = 62000 \\text{ bp}$$\n    -   Scaffold $S_{5}$: This is a single, unscaffolded contig. It functions as a scaffold of length $18000$ bp.\n        $$L(S_{5}) = 18000 \\text{ bp}$$\n\n2.  **Calculate the total assembly length:**\n    The total assembled length, $L_{\\text{total}}$, is the sum of the lengths of all scaffolds.\n    $$L_{\\text{total}} = L(S_{1}) + L(S_{2}) + L(S_{3}) + L(S_{4}) + L(S_{5})$$\n    $$L_{\\text{total}} = 303000 + 202000 + 224000 + 62000 + 18000 = 809000 \\text{ bp}$$\n\n3.  **Calculate half of the total assembly length:**\n    The threshold for the $N50$ calculation is half of the total length.\n    $$L_{\\text{threshold}} = \\frac{L_{\\text{total}}}{2} = \\frac{809000}{2} = 404500 \\text{ bp}$$\n\n4.  **Perform the $N50$ calculation:**\n    To find the $N50$, we list the scaffold lengths in descending order and sum them until the cumulative sum meets or exceeds $L_{\\text{threshold}}$. The $N50$ is the length of the last scaffold added to the sum.\n    -   The ordered list of scaffold lengths is: $\\{303000, 224000, 202000, 62000, 18000\\}$.\n    -   Start with the largest scaffold: Cumulative sum = $303000$. This is less than the threshold of $404500$.\n    -   Add the next largest scaffold: Cumulative sum = $303000 + 224000 = 527000$.\n    -   This cumulative sum, $527000$, is greater than the threshold of $404500$. The process stops here.\n    The length of the scaffold that caused the cumulative sum to exceed the threshold is $224000$ bp. Therefore, the scaffold $N50$ is $224000$ bp.\n\nThe problem also asks for a conceptual explanation of why scaffolding increases $N50$ and the associated risks in diagnostics.\n\n**Conceptual Explanation of Scaffolding and $N50$:**\nThe $N50$ metric is a measure of assembly contiguity. A higher $N50$ value signifies that a larger portion of the genome is represented by longer contiguous sequences (contigs) or linked sequences (scaffolds). The process of scaffolding combines multiple, smaller contigs into a single, larger scaffold based on long-range linkage information (from mate-pair libraries or long reads). For example, several contigs with lengths $\\{c_1, c_2, ..., c_k\\}$ are replaced by a single scaffold of length $S = \\sum c_i + \\sum g_j$, where $g_j$ are gap sizes. By construction, $S > c_i$ for any $i$. This transformation systematically replaces smaller numbers in the length distribution with much larger ones. When calculating $N50$, one sorts these lengths and sums them from largest to smallest. By creating larger scaffolds, the cumulative sum reaches the halfway point of the total genome length much faster, and the length of the sequence that crosses this threshold (the $N50$ value) is therefore increased. This reflects an improved, more contiguous assembly graph, even if the gaps between contigs remain unresolved.\n\n**Risks of Misassembly in Molecular and Immunodiagnostics:**\nWhile scaffolding improves contiguity metrics like $N50$, it introduces the risk of misassembly, where the inferred order, orientation, or adjacency of contigs is incorrect. Such errors have serious consequences for diagnostics.\n\n1.  **Chimeric Sequences:** Misassemblies can create chimeric scaffolds, artificially joining regions of the genome that are distant in reality. This may result in the prediction of non-existent \"fusion genes\" or place a gene under the control of an incorrect regulatory element. In molecular diagnostics, a PCR-based test targeting a specific gene might fail (false negative) if the gene is artifactually fragmented. Conversely, primers designed for two separate loci might suddenly yield a product if a chimeric scaffold has erroneously joined them, leading to a false positive or a confounding result.\n\n2.  **Incorrect Gene Annotation:** For sequence-based diagnostics, such as identifying antibiotic resistance or virulence factor genes in a pathogen, a misassembly can be catastrophic. A gene could be truncated, frame-shifted, or appear incomplete, preventing its identification. This could lead to a mischaracterization of a pathogen's threat level or its susceptibility to treatment.\n\n3.  **Impact on Immunodiagnostics:** Immunodiagnostic tools often target specific protein antigens. The genetic sequences encoding these antigens are critical for their discovery and for developing DNA-based confirmatory tests. A misassembled gene sequence would lead to an incorrect inferred protein sequence. An epitope predicted from such an artifactual sequence would not exist in the actual organism. Consequently, any antibody-based assay (e.g., ELISA) developed against this phantom epitope would be useless, failing to detect the pathogen.\n\nGiven these risks, any genome assembly intended for the development or validation of diagnostic tools must be subject to stringent quality control. Scaffolding junctions must be experimentally verified, for example, by targeted PCR across the gaps followed by Sanger sequencing, or by deep coverage with long reads that span entire gaps and flanking contig regions. Without such validation, diagnostic assays built upon a draft genome are fundamentally unreliable.", "answer": "$$\n\\boxed{224000}\n$$", "id": "5139932"}]}