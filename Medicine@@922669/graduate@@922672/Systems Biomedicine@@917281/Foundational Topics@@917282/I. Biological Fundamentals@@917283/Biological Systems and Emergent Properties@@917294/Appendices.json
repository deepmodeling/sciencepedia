{"hands_on_practices": [{"introduction": "Emergent properties often arise from simple network motifs. The genetic toggle switch, composed of two mutually repressing genes, is a canonical example of how a simple architecture can generate bistability—the capacity to exist in two distinct stable states. This practice guides you through the analytical core of systems biology: using linear stability analysis to derive the fundamental conditions for this emergent behavior. By solving this problem [@problem_id:4320312], you will determine precisely how parameters like cooperative binding ($n$) and gene expression strength ($\\alpha$) define the boundary between monostable and bistable regimes.", "problem": "Consider a symmetric two-gene toggle switch in which each gene represses the other through Hill-type binding with repression function $f(x) = \\frac{1}{1 + \\left(\\frac{x}{K}\\right)^{n}}$. Let $x(t)$ and $y(t)$ denote the concentrations of the two gene products. The dynamics are modeled by the system of Ordinary Differential Equations (ODEs)\n$$\n\\frac{dx}{dt} = \\beta\\,f(y) - \\gamma\\,x,\\quad \\frac{dy}{dt} = \\beta\\,f(x) - \\gamma\\,y,\n$$\nwhere $\\beta > 0$ is the maximal production rate and $\\gamma > 0$ is the first-order degradation rate. Introduce the dimensionless parameter $\\alpha = \\frac{\\beta}{\\gamma}$ and rescale time by $\\tau = \\gamma t$ to obtain the dimensionless system\n$$\n\\frac{dx}{d\\tau} = \\alpha\\,f(y) - x,\\quad \\frac{dy}{d\\tau} = \\alpha\\,f(x) - y.\n$$\nStarting from the definitions of equilibrium and linear stability and using only first principles (existence of nullclines and Jacobian-based local stability), derive the conditions under which the symmetric equilibrium on the line $x = y$ loses stability and the system exhibits two distinct stable equilibria (bistability). For fixed $\\alpha > 0$, determine the exact closed-form analytical expression for the bifurcation boundary $K_{\\mathrm{crit}}(n)$ in terms of $\\alpha$ and $n$ that separates the monostable regime (one stable equilibrium) from the bistable regime (two stable equilibria) for the symmetric toggle. Your derivation must start from the equilibrium condition and the Jacobian eigenvalues at the symmetric equilibrium, and must not assume any pre-derived bifurcation formulas. Express your final boundary $K_{\\mathrm{crit}}(n)$ in the same concentration units as $x$ and $y$. No numerical approximation or rounding is required. The final answer must be a single closed-form expression.", "solution": "The user-provided problem is rigorously validated and confirmed to be scientifically grounded, well-posed, and objective. There are no identifiable flaws. The problem is a standard exercise in the stability analysis of dynamical systems, applied to a canonical model in systems biology. All necessary definitions and parameters are provided, and the task is to derive a specific analytical result from first principles.\n\nThe solution proceeds as follows.\n\nThe dynamics of the system are described by the dimensionless Ordinary Differential Equations (ODEs):\n$$\n\\frac{dx}{d\\tau} = \\alpha\\,f(y) - x\n$$\n$$\n\\frac{dy}{d\\tau} = \\alpha\\,f(x) - y\n$$\nwhere $f(z) = \\frac{1}{1 + \\left(\\frac{z}{K}\\right)^{n}}$.\n\nAn equilibrium point, or fixed point, $(x^*, y^*)$ of the system is a point where the time derivatives are zero. This requires:\n$$\nx^* = \\alpha\\,f(y^*)\n$$\n$$\ny^* = \\alpha\\,f(x^*)\n$$\nThe problem asks for the analysis of the symmetric equilibrium, which lies on the line $x = y$. Let this equilibrium be $(x_s^*, x_s^*)$, where $x_s^*$ satisfies the equation:\n$$\nx_s^* = \\alpha\\,f(x_s^*) = \\frac{\\alpha}{1 + \\left(\\frac{x_s^*}{K}\\right)^{n}}\n$$\nTo determine the stability of this equilibrium, we perform a linear stability analysis. This involves computing the Jacobian matrix $J$ of the system, which is a matrix of partial derivatives of the right-hand sides of the ODEs. Let $F(x, y) = \\alpha\\,f(y) - x$ and $G(x, y) = \\alpha\\,f(x) - y$. The Jacobian matrix is:\n$$\nJ(x, y) = \\begin{pmatrix} \\frac{\\partial F}{\\partial x} & \\frac{\\partial F}{\\partial y} \\\\ \\frac{\\partial G}{\\partial x} & \\frac{\\partial G}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} -1 & \\alpha\\,f'(y) \\\\ \\alpha\\,f'(x) & -1 \\end{pmatrix}\n$$\nWe need the derivative of the Hill function, $f'(z)$:\n$$\nf'(z) = \\frac{d}{dz}\\left(1 + \\left(\\frac{z}{K}\\right)^{n}\\right)^{-1} = -\\left(1 + \\left(\\frac{z}{K}\\right)^{n}\\right)^{-2} \\cdot n\\left(\\frac{z}{K}\\right)^{n-1} \\cdot \\frac{1}{K} = -\\frac{n}{K} \\frac{\\left(\\frac{z}{K}\\right)^{n-1}}{\\left(1 + \\left(\\frac{z}{K}\\right)^{n}\\right)^2}\n$$\nNote that for physically relevant parameters ($z>0, K>0, n>0$), $f'(z)$ is always negative.\n\nWe evaluate the Jacobian at the symmetric equilibrium $(x_s^*, x_s^*)$:\n$$\nJ(x_s^*, x_s^*) = \\begin{pmatrix} -1 & \\alpha\\,f'(x_s^*) \\\\ \\alpha\\,f'(x_s^*) & -1 \\end{pmatrix}\n$$\nThe stability of the equilibrium is determined by the eigenvalues $\\lambda$ of this matrix. The eigenvalues are the roots of the characteristic equation $\\det(J - \\lambda I) = 0$:\n$$\n\\det\\begin{pmatrix} -1-\\lambda & \\alpha\\,f'(x_s^*) \\\\ \\alpha\\,f'(x_s^*) & -1-\\lambda \\end{pmatrix} = 0\n$$\n$$\n(-1-\\lambda)^2 - (\\alpha\\,f'(x_s^*))^2 = 0\n$$\n$$\n(1+\\lambda)^2 = (\\alpha\\,f'(x_s^*))^2\n$$\n$$\n1+\\lambda = \\pm \\alpha\\,f'(x_s^*)\n$$\nThis gives two real eigenvalues:\n$$\n\\lambda_1 = -1 + \\alpha\\,f'(x_s^*)\n$$\n$$\n\\lambda_2 = -1 - \\alpha\\,f'(x_s^*)\n$$\nThe symmetric equilibrium is stable if and only if both eigenvalues are negative.\nSince $\\alpha > 0$ and $f'(x_s^*) < 0$, the first eigenvalue $\\lambda_1 = -1 + \\alpha f'(x_s^*)$ is always negative.\nThe stability is therefore determined entirely by the sign of $\\lambda_2$. The equilibrium is stable if $\\lambda_2 < 0$ and unstable if $\\lambda_2 > 0$.\nThe condition for stability is:\n$$\n\\lambda_2 = -1 - \\alpha\\,f'(x_s^*) < 0 \\implies -1 < \\alpha\\,f'(x_s^*)\n$$\nThe system transitions from having one stable symmetric equilibrium (monostability) to having two new stable asymmetric equilibria and an unstable symmetric equilibrium (bistability) through a pitchfork bifurcation. This bifurcation occurs precisely when the symmetric equilibrium loses its stability, i.e., when $\\lambda_2=0$.\nThe bifurcation condition is therefore:\n$$\n\\lambda_2 = -1 - \\alpha\\,f'(x_s^*) = 0 \\implies \\alpha\\,f'(x_s^*) = -1\n$$\nTo find the bifurcation boundary $K_{\\mathrm{crit}}(n)$, we must find the parameter values for which there exists a fixed point $x_s^*$ that simultaneously satisfies the equilibrium condition and the bifurcation condition. This gives a system of two equations for the two unknowns $x_s^*$ and $K$:\n$$\n(1) \\quad x_s^* = \\frac{\\alpha}{1 + (x_s^*/K)^n}\n$$\n$$\n(2) \\quad \\alpha \\left( -\\frac{n}{K} \\frac{(x_s^*/K)^{n-1}}{(1 + (x_s^*/K)^n)^2} \\right) = -1 \\implies \\frac{\\alpha n}{K} \\frac{(x_s^*/K)^{n-1}}{(1 + (x_s^*/K)^n)^2} = 1\n$$\nWe can solve this system. From equation (1), we can express the denominator of equation (2) in a simpler form:\n$$\n1 + \\left(\\frac{x_s^*}{K}\\right)^n = \\frac{\\alpha}{x_s^*}\n$$\nSubstituting this into equation (2):\n$$\n\\frac{\\alpha n}{K} \\frac{(x_s^*/K)^{n-1}}{(\\alpha/x_s^*)^2} = 1\n$$\n$$\n\\frac{\\alpha n}{K} \\frac{x_s^{*(n-1)}}{K^{n-1}} \\frac{x_s^{*2}}{\\alpha^2} = 1\n$$\n$$\n\\frac{n x_s^{*(n-1+2)}}{\\alpha K^{1+n-1}} = 1 \\implies \\frac{n x_s^{*(n+1)}}{\\alpha K^n} = 1\n$$\nSolving for $x_s^*$:\n$$\nx_s^{*(n+1)} = \\frac{\\alpha K^n}{n} \\implies x_s^* = \\left(\\frac{\\alpha K^n}{n}\\right)^{\\frac{1}{n+1}}\n$$\nThis expression gives the concentration at the symmetric fixed point at the moment of bifurcation. Now, we substitute this back into the equilibrium condition, equation (1), to find the relationship between the parameters $K$, $\\alpha$, and $n$ at the boundary. Rearranging equation (1) gives $x_s^*(1 + (x_s^*/K)^n) = \\alpha$. Substituting our expression for $x_s^*$:\n$$\n\\left(\\frac{\\alpha K^n}{n}\\right)^{\\frac{1}{n+1}} \\left( 1 + \\frac{1}{K^n} \\left[ \\left(\\frac{\\alpha K^n}{n}\\right)^{\\frac{1}{n+1}} \\right]^n \\right) = \\alpha\n$$\n$$\n\\left(\\frac{\\alpha K^n}{n}\\right)^{\\frac{1}{n+1}} \\left( 1 + \\frac{1}{K^n} \\left(\\frac{\\alpha K^n}{n}\\right)^{\\frac{n}{n+1}} \\right) = \\alpha\n$$\nLet's simplify the term inside the parenthesis:\n$$\n1 + \\frac{1}{K^n} \\frac{\\alpha^{\\frac{n}{n+1}} K^{\\frac{n^2}{n+1}}}{n^{\\frac{n}{n+1}}} = 1 + \\frac{\\alpha^{\\frac{n}{n+1}} K^{\\frac{n^2}{n+1}-n}}{n^{\\frac{n}{n+1}}} = 1 + \\frac{\\alpha^{\\frac{n}{n+1}} K^{-\\frac{n}{n+1}}}{n^{\\frac{n}{n+1}}}\n$$\nSubstituting this back into the main equation:\n$$\n\\frac{\\alpha^{\\frac{1}{n+1}} K^{\\frac{n}{n+1}}}{n^{\\frac{1}{n+1}}} \\left( 1 + \\frac{\\alpha^{\\frac{n}{n+1}}}{n^{\\frac{n}{n+1}} K^{\\frac{n}{n+1}}} \\right) = \\alpha\n$$\nDistributing the term on the left:\n$$\n\\frac{\\alpha^{\\frac{1}{n+1}} K^{\\frac{n}{n+1}}}{n^{\\frac{1}{n+1}}} + \\frac{\\alpha^{\\frac{1}{n+1}} K^{\\frac{n}{n+1}}}{n^{\\frac{1}{n+1}}} \\cdot \\frac{\\alpha^{\\frac{n}{n+1}}}{n^{\\frac{n}{n+1}} K^{\\frac{n}{n+1}}} = \\alpha\n$$\n$$\n\\frac{\\alpha^{\\frac{1}{n+1}} K^{\\frac{n}{n+1}}}{n^{\\frac{1}{n+1}}} + \\frac{\\alpha^{\\frac{1}{n+1}+\\frac{n}{n+1}}}{n^{\\frac{1}{n+1}+\\frac{n}{n+1}}} = \\alpha\n$$\n$$\n\\frac{\\alpha^{\\frac{1}{n+1}} K^{\\frac{n}{n+1}}}{n^{\\frac{1}{n+1}}} + \\frac{\\alpha}{n} = \\alpha\n$$\nNow, we isolate the term containing $K$:\n$$\n\\frac{\\alpha^{\\frac{1}{n+1}} K^{\\frac{n}{n+1}}}{n^{\\frac{1}{n+1}}} = \\alpha - \\frac{\\alpha}{n} = \\alpha \\left(1 - \\frac{1}{n}\\right) = \\alpha \\frac{n-1}{n}\n$$\nTo solve for $K$, we first move the other factors to the right-hand side:\n$$\nK^{\\frac{n}{n+1}} = \\frac{n-1}{n} \\cdot \\alpha \\cdot \\frac{n^{\\frac{1}{n+1}}}{\\alpha^{\\frac{1}{n+1}}} = \\frac{n-1}{n} \\cdot n^{\\frac{1}{n+1}} \\cdot \\alpha^{1-\\frac{1}{n+1}} = \\frac{n-1}{n} \\cdot n^{\\frac{1}{n+1}} \\cdot \\alpha^{\\frac{n}{n+1}}\n$$\nFinally, we raise both sides to the power of $\\frac{n+1}{n}$ to solve for $K$, which is $K_{\\mathrm{crit}}$:\n$$\nK_{\\mathrm{crit}}(n) = \\left( \\frac{n-1}{n} \\cdot n^{\\frac{1}{n+1}} \\cdot \\alpha^{\\frac{n}{n+1}} \\right)^{\\frac{n+1}{n}}\n$$\n$$\nK_{\\mathrm{crit}}(n) = \\left(\\frac{n-1}{n}\\right)^{\\frac{n+1}{n}} \\cdot (n^{\\frac{1}{n+1}})^{\\frac{n+1}{n}} \\cdot (\\alpha^{\\frac{n}{n+1}})^{\\frac{n+1}{n}}\n$$\n$$\nK_{\\mathrm{crit}}(n) = \\left(\\frac{n-1}{n}\\right)^{\\frac{n+1}{n}} \\cdot n^{\\frac{1}{n}} \\cdot \\alpha\n$$\nThis expression can be simplified further:\n$$\nK_{\\mathrm{crit}}(n) = \\alpha \\cdot n^{\\frac{1}{n}} \\cdot (n-1)^{\\frac{n+1}{n}} \\cdot n^{-\\frac{n+1}{n}} = \\alpha \\cdot (n-1)^{\\frac{n+1}{n}} \\cdot n^{\\frac{1}{n} - \\frac{n+1}{n}}\n$$\n$$\nK_{\\mathrm{crit}}(n) = \\alpha \\cdot (n-1)^{\\frac{n+1}{n}} \\cdot n^{\\frac{1-n-1}{n}} = \\alpha \\cdot (n-1)^{\\frac{n+1}{n}} \\cdot n^{-1}\n$$\nThis gives the final closed-form analytical expression for the bifurcation boundary:\n$$\nK_{\\mathrm{crit}}(n) = \\frac{\\alpha}{n} (n-1)^{\\frac{n+1}{n}}\n$$\nThis boundary separates the monostable and bistable regimes. Note that for this expression to be real and positive, we must have $n>1$, which is the well-known condition for bistability in a toggle switch (i.e., some degree of cooperativity is required).", "answer": "$$\\boxed{\\frac{\\alpha}{n} (n-1)^{\\frac{n+1}{n}}}$$", "id": "4320312"}, {"introduction": "While stability analysis reveals *if* a system can be bistable, exploring its dynamic response to changing inputs reveals *how* this property manifests. A key signature of bistability is hysteresis, where the system's state depends on its history of stimulation. This computational exercise [@problem_id:4320335] provides hands-on experience in simulating a signaling module with positive feedback as it is subjected to a slowly varying stimulus. You will learn to generate hysteresis loops numerically and quantify this emergent dynamic memory, bridging the gap between theoretical fixed points and observable system behavior.", "problem": "Consider a minimal, well-mixed signaling module exhibiting positive feedback and saturating nonlinearities, modeled as a single dynamical variable $x(t)$ representing the concentration of an activated signaling effector. The module receives an external stimulus $S(t)$ that is slowly varied (swept) over time. The time evolution of $x(t)$ is given by the ordinary differential equation\n$$\n\\frac{dx}{dt} \\;=\\; v_{\\mathrm{basal}} \\;+\\; v_{\\mathrm{act}}(S(t), x(t)) \\;-\\; v_{\\mathrm{deg}}(x(t)),\n$$\nwhere $v_{\\mathrm{basal}}$ is a constant basal production, $v_{\\mathrm{act}}(S, x)$ is an activation term with saturating dependence on $S$ and cooperative positive feedback in $x$, and $v_{\\mathrm{deg}}(x)$ is a saturating degradation term. Grounded in enzyme saturation and cooperative binding, model these terms using Michaelis–Menten and Hill-type functions:\n$$\nv_{\\mathrm{act}}(S, x) \\;=\\; k_{\\mathrm{act}} \\cdot \\frac{S}{K_S + S} \\cdot \\frac{x^n}{K_{\\mathrm{fb}}^n + x^n}, \\quad\nv_{\\mathrm{deg}}(x) \\;=\\; k_{\\mathrm{deg}} \\cdot \\frac{x}{K_{\\mathrm{deg}} + x}.\n$$\nAssume the stimulus $S(t)$ is swept linearly from $S_{\\min}$ up to $S_{\\max}$ with a small positive rate $r$ (the \"up-sweep\"),\n$$\nS_{\\uparrow}(t) \\;=\\; S_{\\min} + r\\,t, \\quad t \\in [0, T_{\\uparrow}], \\quad T_{\\uparrow} = \\frac{S_{\\max} - S_{\\min}}{r},\n$$\nand then swept back down from $S_{\\max}$ to $S_{\\min}$ with the same magnitude of rate (the \"down-sweep\"),\n$$\nS_{\\downarrow}(t) \\;=\\; S_{\\max} - r\\,t, \\quad t \\in [0, T_{\\downarrow}], \\quad T_{\\downarrow} = \\frac{S_{\\max} - S_{\\min}}{r}.\n$$\nTo probe hysteresis arising from saturation-induced nonlinearities and positive feedback, define the \"hysteresis loop\" as the pair of functions $x_{\\uparrow}(S)$ and $x_{\\downarrow}(S)$ obtained by integrating the above dynamics during up-sweep and down-sweep, respectively, and sampling $x(t)$ at matched values of $S$ along the sweep trajectories. Quantify hysteresis by the area between these curves,\n$$\n\\mathcal{A} \\;=\\; \\int_{S_{\\min}}^{S_{\\max}} \\left| x_{\\uparrow}(S) - x_{\\downarrow}(S) \\right| \\, dS,\n$$\ncomputed numerically via the trapezoidal rule on a specified grid of $S$ values. Define a hysteresis indicator as a boolean $\\mathsf{H} = (\\mathcal{A} > \\varepsilon)$ for a small threshold $\\varepsilon$.\n\nImplement a program that:\n- Integrates the up-sweep dynamics from an initial condition $x(0) = x_0$ with $S_{\\uparrow}(t)$, then integrates the down-sweep dynamics starting from the final state of the up-sweep with $S_{\\downarrow}(t)$.\n- Samples $x_{\\uparrow}(S)$ and $x_{\\downarrow}(S)$ on a uniform grid of $S$ with $N$ points, computes $\\mathcal{A}$ via the trapezoidal rule, and sets $\\mathsf{H}$ based on a threshold $\\varepsilon$.\n- Uses a stiff-capable solver and tolerances suitable for slow sweeps and potential sharp transitions.\n\nUse the following fixed numerical specifications for all runs unless otherwise stated:\n- Initial condition: $x_0 = 0.01$.\n- $N = 201$ evenly spaced points in $S$ over $[S_{\\min}, S_{\\max}]$.\n- Threshold: $\\varepsilon = 10^{-3}$.\n- Solver absolute tolerance $10^{-10}$, relative tolerance $10^{-8}$.\n- Use a stiff solver method.\n\nTest suite:\nFor each parameter set below, run the simulation and report the hysteresis area $\\mathcal{A}$ as a float and the hysteresis indicator $\\mathsf{H}$ as a boolean.\n\n- Case 1 (expected bistability/hysteresis):\n  - $v_{\\mathrm{basal}} = 0.05$\n  - $k_{\\mathrm{act}} = 2.5$\n  - $K_S = 0.3$\n  - $K_{\\mathrm{fb}} = 0.4$\n  - $n = 4$\n  - $k_{\\mathrm{deg}} = 3.0$\n  - $K_{\\mathrm{deg}} = 0.2$\n  - $S_{\\min} = 0.0$\n  - $S_{\\max} = 2.0$\n  - $r = 5 \\times 10^{-3}$\n\n- Case 2 (near-threshold, weak hysteresis):\n  - $v_{\\mathrm{basal}} = 0.05$\n  - $k_{\\mathrm{act}} = 1.5$\n  - $K_S = 0.3$\n  - $K_{\\mathrm{fb}} = 0.7$\n  - $n = 3$\n  - $k_{\\mathrm{deg}} = 3.0$\n  - $K_{\\mathrm{deg}} = 0.25$\n  - $S_{\\min} = 0.0$\n  - $S_{\\max} = 2.0$\n  - $r = 5 \\times 10^{-3}$\n\n- Case 3 (monostable, no hysteresis):\n  - $v_{\\mathrm{basal}} = 0.05$\n  - $k_{\\mathrm{act}} = 0.8$\n  - $K_S = 0.3$\n  - $K_{\\mathrm{fb}} = 3.0$\n  - $n = 1$\n  - $k_{\\mathrm{deg}} = 3.0$\n  - $K_{\\mathrm{deg}} = 0.4$\n  - $S_{\\min} = 0.0$\n  - $S_{\\max} = 2.0$\n  - $r = 5 \\times 10^{-3}$\n\nFinal output format:\nYour program should produce a single line of output containing the results for the three cases as a comma-separated Python-style list of lists enclosing pairs $[\\mathcal{A}, \\mathsf{H}]$ in the order of the cases specified above. For example, the output should look like\n$$\n\\texttt{[[A\\_1,H\\_1],[A\\_2,H\\_2],[A\\_3,H\\_3]]}\n$$\nwhere each $A_i$ is a float and each $H_i$ is either $\\texttt{True}$ or $\\texttt{False}$. No units are required in the output because the quantities are dimensionless by construction.", "solution": "The problem requires an analysis of hysteresis in a minimalistic biological signaling module described by a single ordinary differential equation (ODE). The analysis involves numerical integration of the system's dynamics under a slowly varying external stimulus, followed by the quantification of the resulting hysteresis loop area.\n\nThe time evolution of the system's state, represented by the concentration of an activated signaling effector $x(t)$, is governed by the ODE:\n$$\n\\frac{dx}{dt} \\;=\\; f(t, x) \\;=\\; v_{\\mathrm{basal}} \\;+\\; v_{\\mathrm{act}}(S(t), x(t)) \\;-\\; v_{\\mathrm{deg}}(x(t))\n$$\nThis equation balances the rate of change of $x$ with three processes:\n1.  A constant basal production rate, $v_{\\mathrm{basal}}$.\n2.  An activation rate, $v_{\\mathrm{act}}(S, x)$, which depends on both the external stimulus $S$ and the concentration $x$ itself. This term models a positive feedback loop.\n3.  A degradation rate, $v_{\\mathrm{deg}}(x)$.\n\nThe specific functional forms for the activation and degradation rates are given by standard biochemical kinetics models:\n$$\nv_{\\mathrm{act}}(S, x) \\;=\\; k_{\\mathrm{act}} \\cdot \\frac{S}{K_S + S} \\cdot \\frac{x^n}{K_{\\mathrm{fb}}^n + x^n}\n$$\n$$\nv_{\\mathrm{deg}}(x) \\;=\\; k_{\\mathrm{deg}} \\cdot \\frac{x}{K_{\\mathrm{deg}} + x}\n$$\nThe activation term incorporates two saturating phenomena: the dependence on the stimulus $S$ follows a Michaelis–Menten-like form, and the positive feedback in $x$ is described by a cooperative Hill function with a Hill coefficient $n$. The degradation term also follows a Michaelis–Menten-like saturation kinetic. The combination of nonlinear positive feedback and saturation is a common motif for generating bistability in biological systems.\n\nBistability, the capacity of a system to exist in two different stable steady states for the same set of external conditions, can be revealed by observing hysteresis. Hysteresis is the dependence of the state of a system on its history. In this context, we probe for hysteresis by slowly sweeping the stimulus $S$ up from a low value $S_{\\min}$ to a high value $S_{\\max}$, and then sweeping it back down. If the system is bistable, the trajectory of $x$ during the up-sweep, $x_{\\uparrow}(S)$, will differ from its trajectory during the down-sweep, $x_{\\downarrow}(S)$.\n\nThe numerical procedure to quantify this phenomenon is as follows:\n1.  **Discretize the Stimulus Domain**: A uniform grid of $N$ points for the stimulus $S$ is defined over the interval $[S_{\\min}, S_{\\max}]$.\n2.  **Up-Sweep Simulation**: The governing ODE is integrated over a time interval $t \\in [0, T_{\\uparrow}]$, where $T_{\\uparrow} = (S_{\\max} - S_{\\min})/r$. The stimulus is defined as $S_{\\uparrow}(t) = S_{\\min} + rt$. The integration starts from the initial condition $x(0) = x_0$. The ODE is solved using a stiff-capable numerical solver, such as the Backward Differentiation Formula (BDF) method, which is suitable for systems with potentially fast and slow dynamics, especially near bifurcation points. The solution $x(t)$ is evaluated at time points corresponding to the predefined $S$ grid, yielding the trajectory $x_{\\uparrow}(S)$.\n3.  **Down-Sweep Simulation**: Starting from the final state of the up-sweep, $x(T_{\\uparrow})$, the ODE is integrated again over a time interval of the same duration, $t \\in [0, T_{\\downarrow}]$. The stimulus is now swept downwards according to $S_{\\downarrow}(t) = S_{\\max} - rt$. The solution $x(t)$ is again evaluated at time points that correspond to the same $S$ grid, yielding the trajectory $x_{\\downarrow}(S)$.\n4.  **Hysteresis Area Calculation**: The area $\\mathcal{A}$ enclosed by the two trajectories is a quantitative measure of hysteresis. It is computed by numerically integrating the absolute difference between the up-sweep and down-sweep curves over the stimulus range:\n    $$\n    \\mathcal{A} \\;=\\; \\int_{S_{\\min}}^{S_{\\max}} \\left| x_{\\uparrow}(S) - x_{\\downarrow}(S) \\right| \\, dS\n    $$\n    This integral is calculated using the trapezoidal rule on the discretized grid.\n5.  **Hysteresis Indicator**: A boolean indicator $\\mathsf{H}$ is defined to classify the system as hysteretic or not, based on whether the calculated area $\\mathcal{A}$ exceeds a small, predefined threshold $\\varepsilon$. $\\mathsf{H} = (\\mathcal{A} > \\varepsilon)$.\n\nThis entire procedure is implemented for three different parameter sets, designed to exhibit strong hysteresis, weak hysteresis, and no hysteresis, respectively, thereby testing the model's behavior across different dynamic regimes.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print results.\n    \"\"\"\n    # Fixed numerical specifications\n    x0 = 0.01\n    N = 201\n    epsilon = 1e-3\n    atol = 1e-10\n    rtol = 1e-8\n    solver_method = 'BDF'\n\n    # Test cases defined in the problem statement\n    test_cases = [\n        # Case 1 (expected bistability/hysteresis)\n        {\n            \"v_basal\": 0.05, \"k_act\": 2.5, \"K_S\": 0.3, \"K_fb\": 0.4, \"n\": 4,\n            \"k_deg\": 3.0, \"K_deg\": 0.2, \"S_min\": 0.0, \"S_max\": 2.0, \"r\": 5e-3\n        },\n        # Case 2 (near-threshold, weak hysteresis)\n        {\n            \"v_basal\": 0.05, \"k_act\": 1.5, \"K_S\": 0.3, \"K_fb\": 0.7, \"n\": 3,\n            \"k_deg\": 3.0, \"K_deg\": 0.25, \"S_min\": 0.0, \"S_max\": 2.0, \"r\": 5e-3\n        },\n        # Case 3 (monostable, no hysteresis)\n        {\n            \"v_basal\": 0.05, \"k_act\": 0.8, \"K_S\": 0.3, \"K_fb\": 3.0, \"n\": 1,\n            \"k_deg\": 3.0, \"K_deg\": 0.4, \"S_min\": 0.0, \"S_max\": 2.0, \"r\": 5e-3\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(params, x0, N, epsilon, atol, rtol, solver_method)\n        results.append(result)\n        \n    # Format the output as specified\n    print(f\"[[{results[0][0]},{results[0][1]}],[{results[1][0]},{results[1][1]}],[{results[2][0]},{results[2][1]}]]\")\n\ndef ode_system(t, x, params, sweep_direction):\n    \"\"\"\n    Defines the right-hand side of the ODE system.\n    dx/dt = f(t, x)\n    \"\"\"\n    x_val = x[0]\n    \n    # Unpack parameters\n    v_basal = params[\"v_basal\"]\n    k_act = params[\"k_act\"]\n    K_S = params[\"K_S\"]\n    K_fb = params[\"K_fb\"]\n    n = params[\"n\"]\n    k_deg = params[\"k_deg\"]\n    K_deg = params[\"K_deg\"]\n    S_min = params[\"S_min\"]\n    S_max = params[\"S_max\"]\n    r = params[\"r\"]\n\n    # Calculate stimulus S(t) based on sweep direction\n    if sweep_direction == 'up':\n        S = S_min + r * t\n    else:  # 'down'\n        S = S_max - r * t\n    \n    # Clamp S to its defined range to avoid floating point inaccuracies near boundaries\n    S = np.clip(S, S_min, S_max)\n    \n    # Calculate rate terms\n    v_act = k_act * (S / (K_S + S)) * (x_val**n / (K_fb**n + x_val**n))\n    v_deg = k_deg * (x_val / (K_deg + x_val))\n    \n    # Return the derivative\n    return [v_basal + v_act - v_deg]\n\ndef run_simulation(params, x0, N, epsilon, atol, rtol, method):\n    \"\"\"\n    Performs the up-sweep and down-sweep simulations for a given set of parameters.\n    \"\"\"\n    S_min = params[\"S_min\"]\n    S_max = params[\"S_max\"]\n    r = params[\"r\"]\n    \n    # Define stimulus grid and time intervals\n    S_grid = np.linspace(S_min, S_max, N)\n    T_sweep = (S_max - S_min) / r\n    t_span = (0, T_sweep)\n\n    # --- Up-sweep simulation ---\n    # `t_eval` points are chosen to correspond to the uniform `S_grid`\n    t_eval_up = (S_grid - S_min) / r\n    sol_up = solve_ivp(\n        ode_system, t_span, [x0],\n        method=method,\n        t_eval=t_eval_up,\n        rtol=rtol, atol=atol,\n        args=(params, 'up')\n    )\n    x_up = sol_up.y[0]\n    \n    # Initial condition for down-sweep is the final state of up-sweep\n    x0_down = x_up[-1]\n\n    # --- Down-sweep simulation ---\n    # `t_eval` must be sorted, so we evaluate at times corresponding to\n    # S from S_max down to S_min, which means t from 0 to T_sweep.\n    # We use a reversed S_grid to generate a sorted t_eval array.\n    S_grid_rev = S_grid[::-1]\n    t_eval_down = (S_max - S_grid_rev) / r\n    sol_down = solve_ivp(\n        ode_system, t_span, [x0_down],\n        method=method,\n        t_eval=t_eval_down,\n        rtol=rtol, atol=atol,\n        args=(params, 'down')\n    )\n    # The solution `sol_down.y` corresponds to `S_grid_rev` (S decreasing).\n    # We reverse it to align with `S_grid` (S increasing).\n    x_down = sol_down.y[0][::-1]\n\n    # --- Hysteresis analysis ---\n    # Calculate area using the trapezoidal rule\n    area = np.trapz(np.abs(x_up - x_down), x=S_grid)\n\n    # Determine hysteresis indicator\n    is_hysteretic = area > epsilon\n    \n    # Python's `True` and `False` need to be lowercase `true` and `false` for some interpreters.\n    # Let's format the output string directly.\n    return [area, str(is_hysteretic)]\n\nif __name__ == \"__main__\":\n    # To match the required output format exactly, we build the string manually.\n    # We cannot use the `solve` function as it will produce boolean `True`/`False` which might not be the desired string format.\n    # The provided code in the answer is slightly modified to print the string version of the boolean.\n    \n    # Rerunning the logic to produce the final string\n    x0 = 0.01\n    N = 201\n    epsilon = 1e-3\n    atol = 1e-10\n    rtol = 1e-8\n    solver_method = 'BDF'\n    test_cases = [\n        {\"v_basal\": 0.05, \"k_act\": 2.5, \"K_S\": 0.3, \"K_fb\": 0.4, \"n\": 4, \"k_deg\": 3.0, \"K_deg\": 0.2, \"S_min\": 0.0, \"S_max\": 2.0, \"r\": 5e-3},\n        {\"v_basal\": 0.05, \"k_act\": 1.5, \"K_S\": 0.3, \"K_fb\": 0.7, \"n\": 3, \"k_deg\": 3.0, \"K_deg\": 0.25, \"S_min\": 0.0, \"S_max\": 2.0, \"r\": 5e-3},\n        {\"v_basal\": 0.05, \"k_act\": 0.8, \"K_S\": 0.3, \"K_fb\": 3.0, \"n\": 1, \"k_deg\": 3.0, \"K_deg\": 0.4, \"S_min\": 0.0, \"S_max\": 2.0, \"r\": 5e-3}\n    ]\n    \n    final_results = []\n    for case in test_cases:\n        area, is_hysteretic_bool = run_simulation(case, x0, N, epsilon, atol, rtol, solver_method)\n        final_results.append(f\"[{area},{is_hysteretic_bool}]\")\n    print(f\"[{','.join(final_results)}]\")\n\n```", "id": "4320335"}, {"introduction": "Biological systems are inherently noisy. For a bistable system, this molecular noise is not just a nuisance but a driving force that enables transitions between stable states, a process crucial for cell-fate decisions and phenotypic plasticity. This advanced practice [@problem_id:4320262] moves beyond deterministic models to the realm of stochastic dynamics. Using the framework of stochastic differential equations and Kramers' escape theory, you will derive an analytical expression for the mean time it takes for a system to escape from one stable state, demonstrating how macroscopic switching emerges from microscopic fluctuations.", "problem": "Consider a coarse-grained, dimensionless model of a self-activating gene exhibiting bistability, where the concentration of the gene product, denoted by $x$, obeys the one-dimensional Stochastic Differential Equation (SDE) in the Stratonovich sense\n$$\ndx = f(x)\\,dt + \\sqrt{2D}\\,g(x)\\circ dW_t,\n$$\nwhere $W_t$ is standard Brownian motion, $D$ is a small, dimensionless effective noise intensity arising from bursty production and promoter switching, $f(x)$ is the deterministic drift generated by the mean-field synthesis and degradation balance, and $g(x)$ captures state-dependent noise amplitude due to copy-number-dependent fluctuations. Assume the bistable drift is\n$$\nf(x) = -\\frac{dU}{dx}(x), \\quad U(x) = \\frac{(x^2 - 1)^2}{4},\n$$\nso that $f(x) = -x(x^2 - 1)$, with stable fixed points at $x=-1$ and $x=+1$, and an unstable fixed point at $x=0$. Assume the multiplicative noise amplitude is\n$$\ng(x) = \\sqrt{1 + \\lambda x^2}, \\quad \\lambda > 0.\n$$\nIn systems biomedicine, a noise-induced transition from the low-expression state $x=-1$ to the high-expression state $x=+1$ proceeds by rare barrier crossing past the separatrix at $x=0$ in the small-noise regime $D \\ll 1$, an emergent property of the coupled nonlinear dynamics and stochastic fluctuations.\n\nUsing only the fundamental definitions and laws for one-dimensional diffusion processes and first-passage problems (namely, the backward Kolmogorov equation and the Kramers asymptotic method for small noise), derive the leading-order, small-$D$ asymptotic expression for the mean first passage time $\\mathcal{T}_{-1\\to 0}(D,\\lambda)$ to reach the separatrix at $x=0$ starting from the bottom of the left well at $x=-1$, with an absorbing boundary at $x=0$ and a reflecting boundary at $x=-1$.\n\nYour derivation must:\n- Start from the backward equation or an equivalent transformation that renders the noise additive in a suitable coordinate.\n- Identify the appropriate effective barrier functional that controls the exponential scaling in the small-noise limit and compute it explicitly for the given $f(x)$ and $g(x)$.\n- Compute the curvature (local linearization) terms that determine the prefactor at the stable ($x=-1$) and unstable ($x=0$) fixed points.\n\nGive the final answer as a single, closed-form analytic expression for $\\mathcal{T}_{-1\\to 0}(D,\\lambda)$ in terms of $D$ and $\\lambda$. Do not perform any numerical rounding; an exact expression is required. State your final answer without units (all quantities are dimensionless in this formulation).", "solution": "The problem asks for the leading-order asymptotic expression for the mean first passage time (MFPT) for a particle, whose dynamics are described by a one-dimensional Stratonovich Stochastic Differential Equation (SDE), to travel from a stable fixed point to a nearby unstable fixed point in the small noise limit, $D \\ll 1$.\n\nThe SDE is given in the Stratonovich sense as:\n$$\ndx = f(x)\\,dt + \\sqrt{2D}\\,g(x)\\circ dW_t\n$$\nwith the drift $f(x) = -x(x^2 - 1) = x - x^3$ and the noise amplitude $g(x) = \\sqrt{1 + \\lambda x^2}$ for $\\lambda > 0$. The process starts at the stable fixed point $x_{st} = -1$ and the first passage time is measured to the unstable fixed point $x_{un} = 0$, which acts as an absorbing boundary.\n\nThe standard Kramers' escape rate theory applies to Ito processes with additive noise. A crucial first step is to transform the given SDE into this form. For a Stratonovich SDE, this can be achieved via the Lamperti transformation.\n\nLet us define a new coordinate $z$ by the transformation:\n$$\nz(x) = \\int_0^x \\frac{dy}{g(y)}\n$$\nThe differential is $dz = \\frac{1}{g(x)}dx$. Because the original SDE is in the Stratonovich form, we can apply the standard chain rule:\n$$\ndz = \\frac{1}{g(x)} dx = \\frac{1}{g(x)} \\left( f(x)\\,dt + \\sqrt{2D}\\,g(x)\\circ dW_t \\right)\n$$\n$$\ndz = \\frac{f(x)}{g(x)}\\,dt + \\sqrt{2D}\\,dW_t\n$$\nThis transformed SDE is an Ito process because the diffusion coefficient is now a constant, $\\sqrt{2D}$. The new drift term is a function of the new coordinate $z$, $F(z) = f(x(z))/g(x(z))$. The process in the $z$ coordinate has additive noise.\n\nThe mean first passage time $\\mathcal{T}$ from a stable point $z_{st}$ of an effective potential $\\Psi(z)$ to a nearby unstable point $z_{un}$ for a process with additive noise is given by the Kramers' formula in the small noise limit $D \\ll 1$:\n$$\n\\mathcal{T}_{z_{st}\\to z_{un}} \\approx \\frac{2\\pi}{\\sqrt{|\\Psi''(z_{st})|\\Psi''(z_{un})|}} \\exp\\left(\\frac{\\Delta\\Psi}{D}\\right)\n$$\nwhere $\\Psi(z)$ is the potential corresponding to the drift $F(z)$, i.e., $F(z) = -\\frac{d\\Psi}{dz}$, and $\\Delta\\Psi = \\Psi(z_{un}) - \\Psi(z_{st})$ is the potential barrier height. The points $z_{st}$ and $z_{un}$ correspond to the original fixed points $x_{st}=-1$ and $x_{un}=0$.\n\nThe derivation proceeds by calculating the two components of this formula: the barrier height $\\Delta\\Psi$ and the curvatures $\\Psi''(z_{st})$ and $\\Psi''(z_{un})$.\n\n**1. Calculation of the Potential Barrier Height $\\Delta\\Psi$**\n\nThe potential barrier is the integral of the drift force:\n$$\n\\Delta\\Psi = \\Psi(z_{un}) - \\Psi(z_{st}) = -\\int_{z_{st}}^{z_{un}} F(z') dz'\n$$\nWe change the integration variable back to $x$. The integration limits are from $x_{st} = -1$ to $x_{un} = 0$. Using $dz = dx/g(x)$ and $F(z) = f(x)/g(x)$, we have:\n$$\n\\Delta\\Psi = -\\int_{-1}^{0} \\frac{f(x)}{g(x)} \\frac{dx}{g(x)} = -\\int_{-1}^{0} \\frac{f(x)}{g(x)^2} dx\n$$\nSubstituting the given expressions for $f(x)$ and $g(x)$:\n$$\n\\Delta\\Psi = -\\int_{-1}^{0} \\frac{x - x^3}{(1 + \\lambda x^2)} dx = \\int_{-1}^{0} \\frac{x^3 - x}{1 + \\lambda x^2} dx\n$$\nWe perform polynomial long division on the integrand:\n$$\n\\frac{x^3 - x}{ \\lambda x^2 + 1} = \\frac{x}{\\lambda} - \\frac{(1+1/\\lambda)x}{\\lambda x^2+1} = \\frac{x}{\\lambda} - \\frac{\\lambda+1}{\\lambda}\\frac{x}{\\lambda x^2+1}\n$$\nNow we integrate this expression from $-1$ to $0$:\n$$\n\\Delta\\Psi = \\int_{-1}^{0} \\left( \\frac{x}{\\lambda} - \\frac{\\lambda+1}{\\lambda^2}\\frac{\\lambda x}{1 + \\lambda x^2} \\right) dx\n$$\nThe integral is:\n$$\n\\Delta\\Psi = \\left[ \\frac{x^2}{2\\lambda} - \\frac{\\lambda+1}{2\\lambda^2}\\ln(1 + \\lambda x^2) \\right]_{-1}^{0}\n$$\nEvaluating at the limits:\n$$\n\\text{at } x=0: \\quad \\frac{0^2}{2\\lambda} - \\frac{\\lambda+1}{2\\lambda^2}\\ln(1) = 0\n$$\n$$\n\\text{at } x=-1: \\quad \\frac{(-1)^2}{2\\lambda} - \\frac{\\lambda+1}{2\\lambda^2}\\ln(1 + \\lambda (-1)^2) = \\frac{1}{2\\lambda} - \\frac{\\lambda+1}{2\\lambda^2}\\ln(1 + \\lambda)\n$$\nThe barrier height is the value at the upper limit minus the value at the lower limit:\n$$\n\\Delta\\Psi = 0 - \\left( \\frac{1}{2\\lambda} - \\frac{\\lambda+1}{2\\lambda^2}\\ln(1 + \\lambda) \\right) = -\\frac{1}{2\\lambda} + \\frac{\\lambda+1}{2\\lambda^2}\\ln(1+\\lambda)\n$$\n\n**2. Calculation of the Prefactor Curvatures**\n\nThe prefactor depends on the second derivatives of the potential $\\Psi(z)$ evaluated at the stable and unstable fixed points.\n$$\n\\Psi''(z) = -\\frac{dF(z)}{dz} = -\\frac{d}{dz}\\left(\\frac{f(x(z))}{g(x(z))}\\right)\n$$\nUsing the chain rule, $\\frac{d}{dz} = \\frac{dx}{dz}\\frac{d}{dx} = g(x)\\frac{d}{dx}$:\n$$\n\\Psi''(z) = -g(x) \\frac{d}{dx}\\left(\\frac{f(x)}{g(x)}\\right) = -g(x) \\left( \\frac{f'(x)g(x) - f(x)g'(x)}{g(x)^2} \\right) = - \\left( f'(x) - \\frac{f(x)g'(x)}{g(x)} \\right)\n$$\nWe need to evaluate this at the fixed points, where by definition $f(x)=0$. Therefore, at a fixed point $x^*$, the expression simplifies significantly:\n$$\n\\Psi''(z(x^*)) = -f'(x^*)\n$$\nWe need to calculate $f'(x)$:\n$$\nf(x) = x - x^3 \\implies f'(x) = 1 - 3x^2\n$$\nNow, we evaluate this at the stable and unstable fixed points:\nAt the stable fixed point $x_{st} = -1$:\n$$\n\\Psi''(z_{st}) = -f'(-1) = -(1 - 3(-1)^2) = -(1-3) = 2\n$$\nThis is positive, as expected for a potential minimum.\nAt the unstable fixed point $x_{un} = 0$:\n$$\n\\Psi''(z_{un}) = -f'(0) = -(1 - 3(0)^2) = -1\n$$\nThis is negative, as expected for a potential maximum.\n\nThe absolute values of the curvatures are $|\\Psi''(z_{st})| = 2$ and $|\\Psi''(z_{un})| = 1$. The prefactor is therefore:\n$$\n\\frac{2\\pi}{\\sqrt{|\\Psi''(z_{st})||\\Psi''(z_{un})|}} = \\frac{2\\pi}{\\sqrt{2 \\cdot 1}} = \\frac{2\\pi}{\\sqrt{2}} = \\pi\\sqrt{2}\n$$\n\n**3. Final Asymptotic Expression**\n\nCombining the prefactor and the exponential term, we obtain the final expression for the mean first passage time $\\mathcal{T}_{-1\\to 0}(D,\\lambda)$:\n$$\n\\mathcal{T}_{-1\\to 0}(D,\\lambda) \\approx \\pi\\sqrt{2} \\exp\\left( \\frac{\\Delta\\Psi}{D} \\right) = \\pi\\sqrt{2} \\exp\\left( \\frac{1}{D} \\left[ -\\frac{1}{2\\lambda} + \\frac{\\lambda+1}{2\\lambda^2}\\ln(1+\\lambda) \\right] \\right)\n$$\nThis expression gives the leading-order behavior of the escape time in the small-noise limit $D \\ll 1$.", "answer": "$$\n\\boxed{\\pi\\sqrt{2} \\exp\\left( \\frac{1}{D} \\left( -\\frac{1}{2\\lambda} + \\frac{\\lambda+1}{2\\lambda^2}\\ln(1+\\lambda) \\right) \\right)}\n$$", "id": "4320262"}]}