{"hands_on_practices": [{"introduction": "Distinguishing correlation from causation begins with a formal understanding of how they differ. This first practice provides a foundational exercise using a Structural Causal Model (SCM) to precisely quantify the gap between the observational expectation $E[Y \\mid X = x]$ and the interventional (causal) expectation $E[Y \\mid do(X = x)]$. By working through a simple linear model with a classic confounding structure [@problem_id:4332362], you will derive from first principles how a shared cause generates a spurious association, a critical first step in robust biomedical inference.", "problem": "Consider a simplified systems biomedicine scenario in which a pro-inflammatory cytokine level $X$ appears correlated with a clinical severity score $Y$. An upstream immune activation state $Z$ both increases the cytokine level $X$ and independently worsens severity $Y$, creating a confounding pathway. Suppose the data-generating mechanism is a linear Gaussian Structural Causal Model (SCM) defined by the following structural equations with mutually independent, zero-mean Gaussian exogenous terms:\n- $Z \\sim \\mathcal{N}(0, 2)$,\n- $X = Z + U_{X}$ with $U_{X} \\sim \\mathcal{N}(0, 2)$,\n- $Y = 1 \\cdot X + 1 \\cdot Z + U_{Y}$ with $U_{Y} \\sim \\mathcal{N}(0, 1)$.\n\nAll exogenous variables $Z$, $U_{X}$, and $U_{Y}$ are mutually independent. The intervention $do(X = x)$ represents setting $X$ to the fixed value $x$ by external manipulation.\n\nUsing only fundamental properties of multivariate normal distributions, definitions of conditional expectations, and the intervention semantics of the SCM (that is, replacing the structural equation for $X$ by $X = x$ under $do(X = x)$), derive from first principles the expressions for $E[Y \\mid X = x]$ and $E[Y \\mid do(X = x)]$, and then compute the difference $E[Y \\mid X = x] - E[Y \\mid do(X = x)]$ as a simplified closed-form analytic expression in $x$.\n\nProvide your final answer as a single exact expression in $x$. No rounding is required and there are no physical units to report.", "solution": "The problem requires the derivation of two quantities, the observational conditional expectation $E[Y \\mid X = x]$ and the interventional conditional expectation $E[Y \\mid do(X = x)]$, and their difference. The derivation will be conducted from first principles using the provided Structural Causal Model (SCM) and properties of multivariate normal distributions.\n\nThe SCM is defined by the following equations:\n$Z \\sim \\mathcal{N}(0, 2)$\n$X = Z + U_{X}$, with $U_{X} \\sim \\mathcal{N}(0, 2)$\n$Y = X + Z + U_{Y}$, with $U_{Y} \\sim \\mathcal{N}(0, 1)$\nThe exogenous variables $Z$, $U_X$, and $U_Y$ are mutually independent.\n\nFirst, we derive the observational conditional expectation, $E[Y \\mid X = x]$.\nUsing the structural equation for $Y$ and the linearity of expectation, we have:\n$$E[Y \\mid X = x] = E[X + Z + U_Y \\mid X = x]$$\n$$E[Y \\mid X = x] = E[X \\mid X = x] + E[Z \\mid X = x] + E[U_Y \\mid X = x]$$\n\nWe evaluate each term separately:\n1.  $E[X \\mid X = x]$: The expectation of $X$ conditioned on its own value being $x$ is simply $x$.\n    $$E[X \\mid X = x] = x$$\n\n2.  $E[U_Y \\mid X = x]$: The variable $X$ is defined as $X = Z + U_X$. Since the exogenous variable $U_Y$ is mutually independent of $Z$ and $U_X$, it is also independent of their sum, $X$. Therefore, conditioning on $X$ does not alter the expectation of $U_Y$.\n    $$E[U_Y \\mid X = x] = E[U_Y] = 0$$\n\n3.  $E[Z \\mid X = x]$: To find this conditional expectation, we must determine the joint distribution of the random variables $X$ and $Z$. Both are linear combinations of independent Gaussian variables, so their joint distribution is a bivariate normal distribution. We need to compute their means, variances, and covariance.\n\n    The means are:\n    $E[Z] = 0$\n    $E[X] = E[Z + U_X] = E[Z] + E[U_X] = 0 + 0 = 0$\n\n    The variances are:\n    $\\text{Var}(Z) = 2$\n    Since $Z$ and $U_X$ are independent,\n    $\\text{Var}(X) = \\text{Var}(Z + U_X) = \\text{Var}(Z) + \\text{Var}(U_X) = 2 + 2 = 4$\n\n    The covariance is:\n    $\\text{Cov}(X, Z) = \\text{Cov}(Z + U_X, Z) = \\text{Cov}(Z, Z) + \\text{Cov}(U_X, Z)$\n    Since $U_X$ and $Z$ are independent, $\\text{Cov}(U_X, Z) = 0$.\n    $\\text{Cov}(Z, Z) = \\text{Var}(Z) = 2$.\n    Thus, $\\text{Cov}(X, Z) = 2$.\n\n    The vector $(X, Z)^T$ follows a bivariate normal distribution with mean vector $\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and covariance matrix $\\Sigma = \\begin{pmatrix} \\text{Var}(X) & \\text{Cov}(X, Z) \\\\ \\text{Cov}(Z, X) & \\text{Var}(Z) \\end{pmatrix} = \\begin{pmatrix} 4 & 2 \\\\ 2 & 2 \\end{pmatrix}$.\n\n    The formula for conditional expectation in a bivariate normal distribution is:\n    $$E[Z \\mid X = x] = E[Z] + \\frac{\\text{Cov}(X, Z)}{\\text{Var}(X)}(x - E[X])$$\n    Substituting the computed values:\n    $$E[Z \\mid X = x] = 0 + \\frac{2}{4}(x - 0) = \\frac{1}{2}x$$\n\n    Combining these terms, we find the expression for $E[Y \\mid X = x]$:\n    $$E[Y \\mid X = x] = x + \\frac{1}{2}x + 0 = \\frac{3}{2}x$$\n\nNext, we derive the interventional expectation, $E[Y \\mid do(X = x)]$.\nThe intervention $do(X = x)$ modifies the SCM by replacing the structural equation for $X$. The new set of equations for the system is:\n$Z \\sim \\mathcal{N}(0, 2)$\n$X = x$\n$Y = X + Z + U_Y = x + Z + U_Y$\n$U_Y \\sim \\mathcal{N}(0, 1)$\n\nThe intervention on $X$ breaks the influence of $Z$ on $X$, but does not affect the distribution of the upstream variable $Z$ or the independent exogenous variable $U_Y$. We compute the expectation of $Y$ in this modified model:\n$$E[Y \\mid do(X = x)] = E[x + Z + U_Y]$$\nBy linearity of expectation:\n$$E[Y \\mid do(X = x)] = E[x] + E[Z] + E[U_Y]$$\nSince $x$ is a constant, $E[x]=x$. The expectations of the exogenous variables remain zero.\n$$E[Y \\mid do(X = x)] = x + 0 + 0 = x$$\n\nFinally, we compute the difference between the observational and interventional expectations:\n$$E[Y \\mid X = x] - E[Y \\mid do(X = x)] = \\frac{3}{2}x - x$$\n$$E[Y \\mid X = x] - E[Y \\mid do(X = x)] = \\frac{1}{2}x$$\nThis difference quantifies the confounding bias. The observational quantity $E[Y \\mid X = x]$ incorporates both the direct causal effect of $X$ on $Y$ and the spurious correlation arising from the common cause $Z$. The interventional quantity $E[Y \\mid do(X = x)]$ isolates the direct causal effect. The difference reveals the magnitude of the confounding pathway's contribution, which is $c_{ZY} \\cdot (E[Z|X=x] - E[Z]) = 1 \\cdot (\\frac{1}{2}x - 0) = \\frac{1}{2}x$.", "answer": "$$\\boxed{\\frac{1}{2}x}$$", "id": "4332362"}, {"introduction": "The consequences of confounding can be dramatic, sometimes completely reversing the apparent effect of an intervention. This exercise illustrates this phenomenon through a classic numerical example of Simpson's Paradox, a common pitfall in analyzing observational biomedical data [@problem_id:4332365]. You will calculate and compare marginal and conditional risk ratios to see how a treatment that appears harmful in a whole population is, in fact, beneficial within every sub-population when a key confounder is properly considered.", "problem": "Consider an observational cohort study in systems biomedicine that investigates a treatment $X$ (binary, $X=1$ for treated, $X=0$ for untreated) and a binary outcome $Y$ (mortality at $30$ days, $Y=1$ for death, $Y=0$ for survival). Let $Z$ be a stratifying variable representing baseline severity (binary, $Z=1$ for high severity, $Z=0$ for low severity). The following quantities are obtained from well-calibrated electronic health records:\n\n1. Conditional outcome risks within severity strata:\n$$\n\\mathbb{P}(Y=1 \\mid X=1, Z=1) = 0.70, \\quad \\mathbb{P}(Y=1 \\mid X=0, Z=1) = 0.80,\n$$\n$$\n\\mathbb{P}(Y=1 \\mid X=1, Z=0) = 0.10, \\quad \\mathbb{P}(Y=1 \\mid X=0, Z=0) = 0.20.\n$$\n\n2. Severity distributions differ by treatment assignment (reflecting non-random treatment allocation):\n$$\n\\mathbb{P}(Z=1 \\mid X=1) = 0.90, \\quad \\mathbb{P}(Z=1 \\mid X=0) = 0.10,\n$$\nand therefore\n$$\n\\mathbb{P}(Z=0 \\mid X=1) = 0.10, \\quad \\mathbb{P}(Z=0 \\mid X=0) = 0.90.\n$$\n\nStarting from the axioms of probability and the law of total probability, and using the standard definition of the risk ratio as the ratio of risks across treatment arms, perform the following:\n\n- Derive the expressions for the marginal risks $\\mathbb{P}(Y=1 \\mid X=1)$ and $\\mathbb{P}(Y=1 \\mid X=0)$ in terms of the given conditional risks and severity distributions.\n- Compute the conditional risk ratios within each stratum of $Z$,\n$$\n\\mathrm{RR}_{Z=z} = \\frac{\\mathbb{P}(Y=1 \\mid X=1, Z=z)}{\\mathbb{P}(Y=1 \\mid X=0, Z=z)} \\quad \\text{for } z \\in \\{0,1\\},\n$$\nand the marginal risk ratio,\n$$\n\\mathrm{RR}_{\\text{marginal}} = \\frac{\\mathbb{P}(Y=1 \\mid X=1)}{\\mathbb{P}(Y=1 \\mid X=0)}.\n$$\n- Using these computations, demonstrate numerically that the conditional risk ratios suggest benefit of treatment within each stratum ($\\mathrm{RR}_{Z=z} < 1$ for both $z=0$ and $z=1$), while the marginal risk ratio suggests harm ($\\mathrm{RR}_{\\text{marginal}} > 1$), thereby exhibiting the correlation-versus-causation paradox under confounding by $Z$.\n\nFinally, report the numerical value of the marginal risk ratio $\\mathrm{RR}_{\\text{marginal}}$, rounded to four significant figures. No units are required.", "solution": "The primary task is to compute and compare stratum-specific and marginal risk ratios to demonstrate the paradox of confounding.\n\nFirst, we derive the expressions for the marginal risks, $\\mathbb{P}(Y=1 \\mid X=1)$ and $\\mathbb{P}(Y=1 \\mid X=0)$. We use the law of total probability, marginalizing over the stratifying variable $Z$. For a fixed treatment status $X=x$, the probability of the outcome $Y=1$ is a weighted average of the stratum-specific probabilities, where the weights are the probabilities of being in each stratum.\n\nThe marginal risk for the treated group ($X=1$) is:\n$$\n\\mathbb{P}(Y=1 \\mid X=1) = \\sum_{z \\in \\{0,1\\}} \\mathbb{P}(Y=1 \\mid X=1, Z=z) \\mathbb{P}(Z=z \\mid X=1)\n$$\n$$\n\\mathbb{P}(Y=1 \\mid X=1) = \\mathbb{P}(Y=1 \\mid X=1, Z=1)\\mathbb{P}(Z=1 \\mid X=1) + \\mathbb{P}(Y=1 \\mid X=1, Z=0)\\mathbb{P}(Z=0 \\mid X=1)\n$$\nThe marginal risk for the untreated group ($X=0$) is:\n$$\n\\mathbb{P}(Y=1 \\mid X=0) = \\sum_{z \\in \\{0,1\\}} \\mathbb{P}(Y=1 \\mid X=0, Z=z) \\mathbb{P}(Z=z \\mid X=0)\n$$\n$$\n\\mathbb{P}(Y=1 \\mid X=0) = \\mathbb{P}(Y=1 \\mid X=0, Z=1)\\mathbb{P}(Z=1 \\mid X=0) + \\mathbb{P}(Y=1 \\mid X=0, Z=0)\\mathbb{P}(Z=0 \\mid X=0)\n$$\nThese are the required general expressions.\n\nNext, we compute the conditional risk ratios for each stratum of $Z$. The risk ratio is defined as the risk in the treated group divided by the risk in the untreated group.\n\nFor the high severity stratum ($Z=1$):\n$$\n\\mathrm{RR}_{Z=1} = \\frac{\\mathbb{P}(Y=1 \\mid X=1, Z=1)}{\\mathbb{P}(Y=1 \\mid X=0, Z=1)} = \\frac{0.70}{0.80} = 0.875\n$$\nFor the low severity stratum ($Z=0$):\n$$\n\\mathrm{RR}_{Z=0} = \\frac{\\mathbb{P}(Y=1 \\mid X=1, Z=0)}{\\mathbb{P}(Y=1 \\mid X=0, Z=0)} = \\frac{0.10}{0.20} = 0.50\n$$\nIn both strata, the conditional risk ratio is less than $1$ ($\\mathrm{RR}_{Z=1} < 1$ and $\\mathrm{RR}_{Z=0} < 1$). This indicates that within each severity level, the treatment is associated with a lower risk of mortality, suggesting a beneficial effect.\n\nNow, we compute the marginal risks by substituting the given numerical values into the derived expressions.\nFor the treated group ($X=1$):\n$$\n\\mathbb{P}(Y=1 \\mid X=1) = (0.70)(0.90) + (0.10)(0.10) = 0.63 + 0.01 = 0.64\n$$\nFor the untreated group ($X=0$):\n$$\n\\mathbb{P}(Y=1 \\mid X=0) = (0.80)(0.10) + (0.20)(0.90) = 0.08 + 0.18 = 0.26\n$$\nThe marginal risk of death is $0.64$ for the treated group and $0.26$ for the untreated group.\n\nFinally, we compute the marginal risk ratio, $\\mathrm{RR}_{\\text{marginal}}$:\n$$\n\\mathrm{RR}_{\\text{marginal}} = \\frac{\\mathbb{P}(Y=1 \\mid X=1)}{\\mathbb{P}(Y=1 \\mid X=0)} = \\frac{0.64}{0.26} = \\frac{32}{13} \\approx 2.461538...\n$$\nThe marginal risk ratio is greater than $1$. This suggests that, overall, the treatment is associated with a higher risk of mortality, suggesting a harmful effect.\n\nThis demonstrates the paradox:\n-   **Conditional Analysis:** Within both low- and high-severity groups, the treatment appears beneficial ($\\mathrm{RR} < 1$).\n-   **Marginal Analysis:** When the groups are combined, the treatment appears harmful ($\\mathrm{RR} > 1$).\n\nThis reversal is a classic manifestation of Simpson's Paradox, caused by the confounding variable $Z$ (severity). The variable $Z$ is a confounder because:\n1.  It is associated with the outcome $Y$ (higher severity leads to higher mortality risk, regardless of treatment).\n2.  It is associated with the treatment $X$ (patients with high severity are much more likely to receive the treatment, as $\\mathbb{P}(Z=1 \\mid X=1) = 0.90$ while $\\mathbb{P}(Z=1 \\mid X=0) = 0.10$). This is known as confounding by indication.\n\nThe marginal risk calculation is a weighted average, but the weights\n($\\mathbb{P}(Z=z \\mid X=x)$) are different for the treated and untreated populations. The treated group is predominantly composed of high-risk patients ($90\\%$), while the untreated group is predominantly composed of low-risk patients ($90\\%$). The marginal comparison is therefore not a comparison of like with like, but rather a comparison of a treated high-risk group with an untreated low-risk group. The stratum-specific risk ratios provide a more meaningful estimate of the treatment's effect by comparing treated and untreated patients with the same baseline severity. The correlation observed in the marginal analysis is spurious and does not reflect a causal relationship.\n\nThe value of the marginal risk ratio, rounded to four significant figures, is $2.462$.", "answer": "$$\n\\boxed{2.462}\n$$", "id": "4332365"}, {"introduction": "A common misconception in statistical modeling is that controlling for more variables always reduces bias. This practice challenges that notion by exploring the concept of collider bias, where conditioning on a common effect of two variables can create a spurious statistical association where none exists causally [@problem_id:4332409]. By deriving the bias analytically, you will learn to recognize this critical data-generating structure and understand why causal inference requires careful thought about the underlying graph, not just indiscriminate statistical adjustment.", "problem": "In a systems biomedicine study, a team investigates whether a molecular exposure $X$ (for example, a pro-inflammatory cytokine level) causally affects a clinical outcome $Y$ (for example, organ dysfunction score). Hospital triage is based on a composite score $M$ (for example, a severity index) that is deterministically computed from routinely collected inputs. Plausibly, $M$ is a common effect of both $X$ and $Y$ because triage incorporates both molecular markers and clinical severity. The data-generating process is modeled as a linear Gaussian Structural Causal Model (SCM) in which $X$ and $Y$ are causally independent but both influence $M$:\n- $X \\sim \\mathcal{N}(0,\\sigma_{X}^{2})$, $Y \\sim \\mathcal{N}(0,\\sigma_{Y}^{2})$, and $\\varepsilon \\sim \\mathcal{N}(0,\\sigma_{\\varepsilon}^{2})$, mutually independent,\n- $M = \\alpha X + \\beta Y + \\varepsilon$,\n- there is no direct or indirect causal effect of $X$ on $Y$ (that is, the causal effect of $X$ on $Y$ is $0$).\n\nA naive analyst, attempting to control for hospital selection, performs an Ordinary Least Squares (OLS) regression of $Y$ on both $X$ and $M$ and interprets the coefficient of $X$ as the causal effect of $X$ on $Y$. This conditioning on the common effect $M$ can induce a spurious association between $X$ and $Y$.\n\nUsing only core properties of the multivariate normal distribution and the definitions of covariance and OLS coefficients, derive a closed-form analytic expression for the signed bias introduced by conditioning on $M$, defined as the OLS coefficient on $X$ in the regression of $Y$ on $X$ and $M$ minus the true causal effect (which is $0$ in this model). Express your final answer as a single simplified symbolic expression in terms of $\\alpha$, $\\beta$, $\\sigma_{X}^{2}$, $\\sigma_{Y}^{2}$, and $\\sigma_{\\varepsilon}^{2}$. No rounding is required. The answer must be a single expression without units.", "solution": "The objective is to derive the signed bias that arises when estimating the causal effect of an exposure $X$ on an outcome $Y$ by performing an Ordinary Least Squares (OLS) regression of $Y$ on both $X$ and a third variable $M$. The variable $M$ is a \"collider\" or \"common effect,\" meaning it is causally influenced by both $X$ and $Y$.\n\nThe specified data-generating process is:\n$X \\sim \\mathcal{N}(0, \\sigma_{X}^{2})$\n$Y \\sim \\mathcal{N}(0, \\sigma_{Y}^{2})$\n$\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^{2})$\nThe variables $X$, $Y$, and $\\varepsilon$ are mutually independent.\nThe structural equation for the collider $M$ is:\n$$M = \\alpha X + \\beta Y + \\varepsilon$$\nThe true causal effect of $X$ on $Y$ is stated to be $0$.\n\nThe signed bias is defined as the estimated OLS coefficient of $X$ minus the true causal effect. Since the true effect is $0$, the bias is simply the OLS coefficient of $X$ in the regression of $Y$ on $X$ and $M$. Let this coefficient be denoted by $\\gamma_{X}$.\n\nFor a multiple regression of a dependent variable $V$ on two predictors $U_1$ and $U_2$, the population OLS coefficient for $U_1$ is given by the formula:\n$$ \\beta_{U_1} = \\frac{\\text{Cov}(V, U_1)\\text{Var}(U_2) - \\text{Cov}(V, U_2)\\text{Cov}(U_1, U_2)}{\\text{Var}(U_1)\\text{Var}(U_2) - (\\text{Cov}(U_1, U_2))^{2}} $$\nIn our case, the dependent variable is $Y$, and the predictors are $X$ and $M$. We want to find the coefficient for $X$, which we denoted $\\gamma_X$. Applying the formula:\n$$ \\gamma_{X} = \\frac{\\text{Cov}(Y, X)\\text{Var}(M) - \\text{Cov}(Y, M)\\text{Cov}(X, M)}{\\text{Var}(X)\\text{Var}(M) - (\\text{Cov}(X, M))^{2}} $$\nTo use this formula, we must compute the required variances and covariances from the model definition. All variables have a mean of $0$.\n\n1.  $\\text{Var}(X)$: This is given directly by the model definition.\n    $$ \\text{Var}(X) = \\sigma_{X}^{2} $$\n\n2.  $\\text{Cov}(Y, X)$: As $X$ and $Y$ are stipulated to be independent, their covariance is $0$.\n    $$ \\text{Cov}(Y, X) = 0 $$\n\n3.  $\\text{Var}(M)$: We compute the variance of $M$ using its structural equation.\n    $$ \\text{Var}(M) = \\text{Var}(\\alpha X + \\beta Y + \\varepsilon) $$\n    Because $X$, $Y$, and $\\varepsilon$ are mutually independent, the variance of their weighted sum is the weighted sum of their variances:\n    $$ \\text{Var}(M) = \\alpha^{2}\\text{Var}(X) + \\beta^{2}\\text{Var}(Y) + \\text{Var}(\\varepsilon) $$\n    $$ \\text{Var}(M) = \\alpha^{2}\\sigma_{X}^{2} + \\beta^{2}\\sigma_{Y}^{2} + \\sigma_{\\varepsilon}^{2} $$\n\n4.  $\\text{Cov}(X, M)$: We use the property of bilinearity of covariance.\n    $$ \\text{Cov}(X, M) = \\text{Cov}(X, \\alpha X + \\beta Y + \\varepsilon) $$\n    $$ \\text{Cov}(X, M) = \\alpha\\text{Cov}(X, X) + \\beta\\text{Cov}(X, Y) + \\text{Cov}(X, \\varepsilon) $$\n    Since $\\text{Cov}(X, X) = \\text{Var}(X)$, and $X, Y, \\varepsilon$ are mutually independent, $\\text{Cov}(X, Y)=0$ and $\\text{Cov}(X, \\varepsilon)=0$.\n    $$ \\text{Cov}(X, M) = \\alpha\\text{Var}(X) = \\alpha\\sigma_{X}^{2} $$\n\n5.  $\\text{Cov}(Y, M)$: Similarly, we compute the covariance of $Y$ and $M$.\n    $$ \\text{Cov}(Y, M) = \\text{Cov}(Y, \\alpha X + \\beta Y + \\varepsilon) $$\n    $$ \\text{Cov}(Y, M) = \\alpha\\text{Cov}(Y, X) + \\beta\\text{Cov}(Y, Y) + \\text{Cov}(Y, \\varepsilon) $$\n    Since $\\text{Cov}(Y, Y) = \\text{Var}(Y)$, and $X, Y, \\varepsilon$ are mutually independent, $\\text{Cov}(Y, X)=0$ and $\\text{Cov}(Y, \\varepsilon)=0$.\n    $$ \\text{Cov}(Y, M) = \\beta\\text{Var}(Y) = \\beta\\sigma_{Y}^{2} $$\n\nNow, we substitute these components back into the formula for $\\gamma_{X}$:\n$$ \\gamma_{X} = \\frac{(0)(\\alpha^{2}\\sigma_{X}^{2} + \\beta^{2}\\sigma_{Y}^{2} + \\sigma_{\\varepsilon}^{2}) - (\\beta\\sigma_{Y}^{2})(\\alpha\\sigma_{X}^{2})}{(\\sigma_{X}^{2})(\\alpha^{2}\\sigma_{X}^{2} + \\beta^{2}\\sigma_{Y}^{2} + \\sigma_{\\varepsilon}^{2}) - (\\alpha\\sigma_{X}^{2})^{2}} $$\nSimplify the numerator:\n$$ \\text{Numerator} = -\\alpha\\beta\\sigma_{X}^{2}\\sigma_{Y}^{2} $$\nSimplify the denominator:\n$$ \\text{Denominator} = \\alpha^{2}\\sigma_{X}^{4} + \\beta^{2}\\sigma_{X}^{2}\\sigma_{Y}^{2} + \\sigma_{X}^{2}\\sigma_{\\varepsilon}^{2} - \\alpha^{2}\\sigma_{X}^{4} $$\nThe $\\alpha^{2}\\sigma_{X}^{4}$ terms cancel out.\n$$ \\text{Denominator} = \\beta^{2}\\sigma_{X}^{2}\\sigma_{Y}^{2} + \\sigma_{X}^{2}\\sigma_{\\varepsilon}^{2} $$\nWe can factor out $\\sigma_{X}^{2}$ from the denominator:\n$$ \\text{Denominator} = \\sigma_{X}^{2}(\\beta^{2}\\sigma_{Y}^{2} + \\sigma_{\\varepsilon}^{2}) $$\nPutting the numerator and denominator back together:\n$$ \\gamma_{X} = \\frac{-\\alpha\\beta\\sigma_{X}^{2}\\sigma_{Y}^{2}}{\\sigma_{X}^{2}(\\beta^{2}\\sigma_{Y}^{2} + \\sigma_{\\varepsilon}^{2})} $$\nAssuming $\\sigma_{X}^{2} > 0$ (i.e., $X$ is not a constant), we can cancel the $\\sigma_{X}^{2}$ term from the numerator and denominator:\n$$ \\gamma_{X} = \\frac{-\\alpha\\beta\\sigma_{Y}^{2}}{\\beta^{2}\\sigma_{Y}^{2} + \\sigma_{\\varepsilon}^{2}} $$\nThis expression represents the OLS coefficient $\\gamma_{X}$. As the true causal effect is $0$, this coefficient is the signed bias introduced by conditioning on the collider $M$. This spurious association is non-zero if and only if $\\alpha \\neq 0$, $\\beta \\neq 0$, and $\\sigma_{Y}^{2} \\neq 0$.", "answer": "$$\\boxed{\\frac{-\\alpha \\beta \\sigma_{Y}^{2}}{\\beta^{2} \\sigma_{Y}^{2} + \\sigma_{\\varepsilon}^{2}}}$$", "id": "4332409"}]}