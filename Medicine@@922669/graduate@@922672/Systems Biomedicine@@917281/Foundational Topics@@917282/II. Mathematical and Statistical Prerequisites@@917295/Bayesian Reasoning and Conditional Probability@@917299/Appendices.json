{"hands_on_practices": [{"introduction": "Before building complex probabilistic models, it is crucial to master the fundamental rules of conditional probability and their application. This exercise [@problem_id:4318462] explores Simpson's paradox, a non-intuitive statistical phenomenon where an association observed within distinct groups of data reverses when those groups are combined. By applying the law of total probability, you will see how conditioning on a confounding variable is essential for drawing correct conclusions, a skill of paramount importance in the analysis of observational biomedical data.", "problem": "A hospital system is evaluating a targeted therapy in observational Electronic Health Records (EHR) data for sepsis patients. Let $X \\in \\{0,1\\}$ denote treatment assignment, with $X=1$ indicating targeted therapy and $X=0$ indicating standard care. Let $Y \\in \\{0,1\\}$ denote $30$-day mortality, with $Y=1$ indicating death. A composite severity index $Z \\in \\{\\mathrm{L}, \\mathrm{H}\\}$, taking values $\\mathrm{L}$ for low severity and $\\mathrm{H}$ for high severity, is a potential confounder because severity is associated with both treatment decisions and mortality.\n\nThe following observational probabilities are estimated:\n- Within each severity stratum:\n  - $P(Y=1 \\mid X=1, Z=\\mathrm{L}) = 0.04$, $P(Y=1 \\mid X=0, Z=\\mathrm{L}) = 0.05$.\n  - $P(Y=1 \\mid X=1, Z=\\mathrm{H}) = 0.20$, $P(Y=1 \\mid X=0, Z=\\mathrm{H}) = 0.24$.\n- Severity distributions differ by treatment group:\n  - $P(Z=\\mathrm{H} \\mid X=1) = 0.90$, $P(Z=\\mathrm{L} \\mid X=1) = 0.10$.\n  - $P(Z=\\mathrm{H} \\mid X=0) = 0.10$, $P(Z=\\mathrm{L} \\mid X=0) = 0.90$.\n\nUsing only the fundamental definitions of conditional probability and the law of total probability, perform the following:\n1. Starting from the definition $P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}$ and the law of total probability, derive an expression for the aggregated mortality $P(Y=1 \\mid X=x)$ in terms of conditional mortality within severity strata $P(Y=1 \\mid X=x, Z=z)$ and severity distributions $P(Z=z \\mid X=x)$ for $x \\in \\{0,1\\}$ and $z \\in \\{\\mathrm{L}, \\mathrm{H}\\}$.\n2. Explain mathematically why aggregation over $Z$ can produce a reversal (Simpson’s paradox) relative to the stratified comparisons, and identify the role of $P(Z=z \\mid X=x)$ in this reversal.\n3. Compute the aggregated risk difference\n$$\\mathrm{RD} = P(Y=1 \\mid X=1) - P(Y=1 \\mid X=0),$$\nusing the numerical values above. Provide your final numerical value for $\\mathrm{RD}$ as an exact fraction. Do not round. Express the final answer with no units and no percentage sign.", "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information to proceed. The objective is to analyze the effect of a targeted therapy on mortality, considering a confounding variable, which is a classic application of conditional probability in biostatistics.\n\nThe solution is divided into three parts as requested by the problem statement. First, we derive the formula for aggregated mortality. Second, we explain the mechanism of Simpson's paradox in this context. Third, we compute the aggregated risk difference using the provided numerical values.\n\n1. Derivation of the aggregated mortality expression\n\nLet $Y \\in \\{0,1\\}$ be the mortality outcome, $X \\in \\{0,1\\}$ be the treatment assignment, and $Z \\in \\{\\mathrm{L}, \\mathrm{H}\\}$ be the severity stratum. We are asked to derive an expression for the aggregated mortality $P(Y=1 \\mid X=x)$ for a given treatment arm $x \\in \\{0,1\\}$.\n\nThe derivation starts from the definition of conditional probability and the law of total probability. The definition of conditional probability states that for any two events $A$ and $B$, with $P(B) > 0$, we have $P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}$.\nLet $A$ be the event $\\{Y=1\\}$ and $B$ be the event $\\{X=x\\}$. We want to find $P(Y=1 \\mid X=x) = \\frac{P(Y=1 \\cap X=x)}{P(X=x)}$.\n\nThe law of total probability allows us to compute the probability of an event by considering a partition of the sample space. The severity index $Z$ partitions the patient population into two disjoint sets: those with low severity ($Z=\\mathrm{L}$) and those with high severity ($Z=\\mathrm{H}$). We can apply the law of total probability to the joint event $\\{Y=1 \\cap X=x\\}$:\n$$P(Y=1 \\cap X=x) = P(Y=1 \\cap X=x \\cap Z=\\mathrm{L}) + P(Y=1 \\cap X=x \\cap Z=\\mathrm{H})$$\nThis can be written in summation form as:\n$$P(Y=1 \\cap X=x) = \\sum_{z \\in \\{\\mathrm{L}, \\mathrm{H}\\}} P(Y=1, X=x, Z=z)$$\nNow, we use the chain rule of probability, which is a repeated application of the definition of conditional probability: $P(E_1 \\cap E_2 \\cap E_3) = P(E_1 \\mid E_2 \\cap E_3) P(E_2 \\cap E_3) = P(E_1 \\mid E_2 \\cap E_3) P(E_2 \\mid E_3) P(E_3)$.\nIn our notation, for a specific stratum $z$:\n$$P(Y=1, X=x, Z=z) = P(Y=1 \\mid X=x, Z=z) P(X=x, Z=z)$$\nWe can further expand the term $P(X=x, Z=z)$ using the definition of conditional probability as $P(Z=z \\mid X=x) P(X=x)$. Substituting this in, we get:\n$$P(Y=1, X=x, Z=z) = P(Y=1 \\mid X=x, Z=z) P(Z=z \\mid X=x) P(X=x)$$\nNow we substitute this back into the sum:\n$$P(Y=1 \\cap X=x) = \\sum_{z \\in \\{\\mathrm{L}, \\mathrm{H}\\}} P(Y=1 \\mid X=x, Z=z) P(Z=z \\mid X=x) P(X=x)$$\nSince $P(X=x)$ is a common factor in the sum, we can factor it out:\n$$P(Y=1 \\cap X=x) = P(X=x) \\sum_{z \\in \\{\\mathrm{L}, \\mathrm{H}\\}} P(Y=1 \\mid X=x, Z=z) P(Z=z \\mid X=x)$$\nFinally, we substitute this expression for the numerator in our original definition of $P(Y=1 \\mid X=x)$:\n$$P(Y=1 \\mid X=x) = \\frac{P(X=x) \\sum_{z \\in \\{\\mathrm{L}, \\mathrm{H}\\}} P(Y=1 \\mid X=x, Z=z) P(Z=z \\mid X=x)}{P(X=x)}$$\nAssuming that each treatment group is non-empty, i.e., $P(X=x) > 0$, we can cancel this term from the numerator and denominator, yielding the desired expression:\n$$P(Y=1 \\mid X=x) = \\sum_{z \\in \\{\\mathrm{L}, \\mathrm{H}\\}} P(Y=1 \\mid X=x, Z=z) P(Z=z \\mid X=x)$$\nThis expression shows that the aggregated (marginal) mortality risk in a treatment group is a weighted average of the stratum-specific risks, where the weights are the probabilities of being in each stratum, given the treatment.\n\n2. Mathematical explanation of Simpson's paradox\n\nSimpson's paradox is a phenomenon in which a trend or association observed in several different groups of data disappears or reverses when these groups are combined. In this problem, the association is between treatment $X$ and mortality $Y$, and the groups are the severity strata $Z$.\n\nFirst, let's examine the stratified risk differences:\nFor the low-severity stratum ($Z=\\mathrm{L}$):\n$$\\mathrm{RD}_{\\mathrm{L}} = P(Y=1 \\mid X=1, Z=\\mathrm{L}) - P(Y=1 \\mid X=0, Z=\\mathrm{L}) = 0.04 - 0.05 = -0.01$$\nFor the high-severity stratum ($Z=\\mathrm{H}$):\n$$\\mathrm{RD}_{\\mathrm{H}} = P(Y=1 \\mid X=1, Z=\\mathrm{H}) - P(Y=1 \\mid X=0, Z=\\mathrm{H}) = 0.20 - 0.24 = -0.04$$\nIn both strata, the risk difference is negative, indicating that the targeted therapy ($X=1$) is associated with a lower risk of mortality compared to standard care ($X=0$).\n\nThe paradox arises when we aggregate over the confounder $Z$. The aggregated risk difference is $\\mathrm{RD} = P(Y=1 \\mid X=1) - P(Y=1 \\mid X=0)$. Using the formula from part 1, we write $P(Y=1 \\mid X=x)$ as a weighted sum:\n$$P(Y=1 \\mid X=x) = P(Y=1 \\mid X=x, Z=\\mathrm{L})P(Z=\\mathrm{L} \\mid X=x) + P(Y=1 \\mid X=x, Z=\\mathrm{H})P(Z=\\mathrm{H} \\mid X=x)$$\nThe reversal of association is caused by the weighting factors, $P(Z=z \\mid X=x)$. These factors represent the distribution of the confounder $Z$ within each treatment group $X=x$. A reversal can occur if the confounder is associated with both the treatment and the outcome. Here, $Z$ is a confounder because:\n1.  It is associated with the outcome $Y$: Mortality is higher in the high-severity group ($0.20$ and $0.24$) than in the low-severity group ($0.04$ and $0.05$) regardless of treatment.\n2.  It is associated with the treatment $X$: The distribution of severity differs between treatment groups. Specifically, $P(Z=\\mathrm{H} \\mid X=1) = 0.90$, while $P(Z=\\mathrm{H} \\mid X=0) = 0.10$.\n\nThis means patients receiving the targeted therapy ($X=1$) are much more likely to be in the high-severity group ($90\\%$), whereas patients receiving standard care ($X=0$) are much more likely to be in the low-severity group ($90\\%$). This is a typical scenario in observational studies where physicians may preferentially give a newer, more aggressive, or experimental therapy to sicker patients.\n\nThe aggregated mortality for the treated group, $P(Y=1 \\mid X=1)$, is a weighted average of the low risk ($0.04$) and high risk ($0.20$), but with a heavy weight ($0.90$) on the high risk.\n$$P(Y=1 \\mid X=1) = (0.04)(0.10) + (0.20)(0.90)$$\nConversely, the aggregated mortality for the control group, $P(Y=1 \\mid X=0)$, is a weighted average of its corresponding risks ($0.05$ and $0.24$), but with a heavy weight ($0.90$) on the low risk.\n$$P(Y=1 \\mid X=0) = (0.05)(0.90) + (0.24)(0.10)$$\nThe comparison of $P(Y=1 \\mid X=1)$ with $P(Y=1 \\mid X=0)$ is thus a comparison between a predominantly high-risk group and a predominantly low-risk group. This confounding by severity can easily overwhelm the true beneficial effect of the treatment within each stratum, leading to a reversal of the conclusion at the aggregated level.\n\n3. Computation of the aggregated risk difference\n\nWe use the derived formula and the given numerical values to compute the aggregated mortalities for each treatment group.\n\nFor the targeted therapy group ($X=1$):\n$$P(Y=1 \\mid X=1) = P(Y=1 \\mid X=1, Z=\\mathrm{L})P(Z=\\mathrm{L} \\mid X=1) + P(Y=1 \\mid X=1, Z=\\mathrm{H})P(Z=\\mathrm{H} \\mid X=1)$$\n$$P(Y=1 \\mid X=1) = (0.04)(0.10) + (0.20)(0.90)$$\n$$P(Y=1 \\mid X=1) = 0.004 + 0.180 = 0.184$$\n\nFor the standard care group ($X=0$):\n$$P(Y=1 \\mid X=0) = P(Y=1 \\mid X=0, Z=\\mathrm{L})P(Z=\\mathrm{L} \\mid X=0) + P(Y=1 \\mid X=0, Z=\\mathrm{H})P(Z=\\mathrm{H} \\mid X=0)$$\n$$P(Y=1 \\mid X=0) = (0.05)(0.90) + (0.24)(0.10)$$\n$$P(Y=1 \\mid X=0) = 0.045 + 0.024 = 0.069$$\n\nThe aggregated risk difference is:\n$$\\mathrm{RD} = P(Y=1 \\mid X=1) - P(Y=1 \\mid X=0) = 0.184 - 0.069 = 0.115$$\nThe aggregated risk difference is positive, suggesting that the targeted therapy is associated with a higher mortality risk. This is the opposite of the conclusion from the stratum-specific analyses, confirming the presence of Simpson's paradox.\n\nTo express this as an exact fraction:\n$$0.115 = \\frac{115}{1000}$$\nWe can simplify this fraction by dividing the numerator and denominator by their greatest common divisor, which is $5$:\n$$\\mathrm{RD} = \\frac{115 \\div 5}{1000 \\div 5} = \\frac{23}{200}$$\nSince $23$ is a prime number and $200 = 2^3 \\cdot 5^2$, the fraction is fully simplified.", "answer": "$$\\boxed{\\frac{23}{200}}$$", "id": "4318462"}, {"introduction": "A central task in Bayesian reasoning is updating our beliefs about model parameters in light of new evidence. This practice problem [@problem_id:4318423] provides a hands-on derivation of one of the most foundational models in statistics: the Bayesian analysis of normally distributed data with an unknown mean and variance. You will construct the Normal-Inverse-Gamma conjugate prior and derive the posterior update rules, providing a complete analytical engine for inference that forms the basis of many more advanced models in systems biomedicine.", "problem": "A systems biomedicine laboratory is quantifying the transcriptional response of a stress-pathway gene to a small-molecule perturbation using patient-derived organoid replicates. Let the observed log$_{2}$ fold-change measurements be modeled as independent and identically distributed draws from a Normal distribution with unknown mean and variance, so that $y_i \\mid \\mu, \\sigma^{2} \\sim \\mathcal{N}(\\mu, \\sigma^{2})$ for $i = 1, \\dots, n$, where the log$_{2}$ fold-change is dimensionless. To enable coherent uncertainty quantification under limited replication, a conjugate prior is chosen for the pair $(\\mu, \\sigma^{2})$.\n\nConstruct the Normal–Inverse-Gamma (NIG) conjugate prior by specifying the conditional prior $p(\\mu \\mid \\sigma^{2})$ as Normal and the marginal prior $p(\\sigma^{2})$ as Inverse-Gamma, with the following hyperparameters:\n$$m_0 = 0.10, \\quad \\kappa_0 = 4, \\quad \\alpha_0 = 3, \\quad \\beta_0 = 0.02.$$\nAdopt the parameterization\n$$p(\\mu \\mid \\sigma^{2}) = \\sqrt{\\frac{\\kappa_0}{2\\pi \\sigma^{2}}}\\,\\exp\\!\\left(-\\frac{\\kappa_0}{2\\sigma^{2}}(\\mu - m_0)^{2}\\right),$$\nand the Inverse-Gamma density\n$$p(\\sigma^{2}) = \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)}\\,(\\sigma^{2})^{-(\\alpha_0+1)}\\,\\exp\\!\\left(-\\frac{\\beta_0}{\\sigma^{2}}\\right), \\quad \\sigma^{2}  0,$$\nwhere $\\Gamma(\\cdot)$ is the Gamma function.\n\nStarting from Bayes’ theorem and the definitions above, derive the joint posterior $p(\\mu, \\sigma^{2} \\mid y_{1:n})$ and show that it is Normal–Inverse-Gamma with updated hyperparameters $\\{m_n, \\kappa_n, \\alpha_n, \\beta_n\\}$ expressed in terms of the sufficient statistics of the observed measurements, namely the sample size $n$, the sample mean $\\bar{y}$, and the corrected sum of squares $S = \\sum_{i=1}^{n}(y_i - \\bar{y})^{2}$.\n\nThen, for the following $n = 12$ observed log$_{2}$ fold-change measurements\n$$y_1 = 0.30,\\; y_2 = -0.05,\\; y_3 = 0.12,\\; y_4 = 0.18,\\; y_5 = 0.26,\\; y_6 = 0.09,$$\n$$y_7 = 0.02,\\; y_8 = 0.21,\\; y_9 = 0.15,\\; y_{10} = 0.11,\\; y_{11} = 0.19,\\; y_{12} = 0.14,$$\ncompute the sufficient statistics $(n, \\bar{y}, S)$ and evaluate the posterior hyperparameters $(m_n, \\kappa_n, \\alpha_n, \\beta_n)$.\n\nRound your final numeric values to four significant figures and present them in a single row matrix in the order $(m_n, \\kappa_n, \\alpha_n, \\beta_n)$.", "solution": "The user wants to solve a Bayesian inference problem. I will first validate the problem statement and, if valid, proceed with a detailed derivation and calculation.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Likelihood Model**: The data $y_i$ for $i=1, \\dots, n$ are independent and identically distributed (i.i.d.) draws from a Normal distribution, $y_i \\mid \\mu, \\sigma^2 \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n-   **Prior Distribution**: A Normal-Inverse-Gamma (NIG) conjugate prior is specified for the parameters $(\\mu, \\sigma^2)$.\n    -   Conditional prior for $\\mu$: $p(\\mu \\mid \\sigma^2)$ is Normal with density $p(\\mu \\mid \\sigma^{2}) = \\sqrt{\\frac{\\kappa_0}{2\\pi \\sigma^{2}}}\\,\\exp\\!\\left(-\\frac{\\kappa_0}{2\\sigma^{2}}(\\mu - m_0)^{2}\\right)$. This corresponds to $\\mu \\mid \\sigma^2 \\sim \\mathcal{N}(m_0, \\sigma^2/\\kappa_0)$.\n    -   Marginal prior for $\\sigma^2$: $p(\\sigma^2)$ is Inverse-Gamma with density $p(\\sigma^{2}) = \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)}\\,(\\sigma^{2})^{-(\\alpha_0+1)}\\,\\exp\\!\\left(-\\frac{\\beta_0}{\\sigma^{2}}\\right)$. This corresponds to $\\sigma^2 \\sim \\text{Inverse-Gamma}(\\alpha_0, \\beta_0)$.\n-   **Prior Hyperparameters**: $m_0 = 0.10$, $\\kappa_0 = 4$, $\\alpha_0 = 3$, $\\beta_0 = 0.02$.\n-   **Observed Data**: $n = 12$ measurements are provided: $y = (0.30, -0.05, 0.12, 0.18, 0.26, 0.09, 0.02, 0.21, 0.15, 0.11, 0.19, 0.14)$.\n-   **Task 1**: Derive the joint posterior $p(\\mu, \\sigma^2 \\mid y_{1:n})$ and show it is NIG with updated hyperparameters $\\{m_n, \\kappa_n, \\alpha_n, \\beta_n\\}$ expressed in terms of the sufficient statistics $(n, \\bar{y}, S)$, where $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$ and $S = \\sum_{i=1}^n (y_i - \\bar{y})^2$.\n-   **Task 2**: Compute the values of the sufficient statistics and the posterior hyperparameters for the given data.\n-   **Output Format**: Final numerical values for $(m_n, \\kappa_n, \\alpha_n, \\beta_n)$ rounded to four significant figures, presented as a row matrix.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded**: The problem is a standard application of Bayesian statistics. The Normal likelihood model with a Normal-Inverse-Gamma conjugate prior is a textbook example used extensively in many scientific fields, including systems biomedicine, for modeling normally distributed data with unknown mean and variance. The problem is scientifically and mathematically sound.\n-   **Well-Posed**: The problem is clearly defined. The use of a conjugate prior ensures that the posterior distribution belongs to the same family as the prior, and its parameters can be derived analytically. The question asks for this derivation and a subsequent calculation, both of which lead to a unique solution.\n-   **Objective**: The problem is stated using precise mathematical language and objective data. There are no subjective or ambiguous statements.\n\nThe problem statement does not violate any of the specified invalidity criteria. It is complete, consistent, scientifically grounded, and well-posed.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. I will proceed with the detailed solution.\n\n### Derivation of the Posterior Distribution\n\nAccording to Bayes' theorem, the joint posterior distribution of the parameters $(\\mu, \\sigma^2)$ given the data $y_{1:n}$ is proportional to the product of the likelihood and the prior distribution:\n$$ p(\\mu, \\sigma^2 \\mid y_{1:n}) \\propto p(y_{1:n} \\mid \\mu, \\sigma^2) p(\\mu, \\sigma^2) $$\nThe joint prior $p(\\mu, \\sigma^2)$ can be expressed as $p(\\mu \\mid \\sigma^2)p(\\sigma^2)$.\n\nThe likelihood function for $n$ i.i.d. observations from $\\mathcal{N}(\\mu, \\sigma^2)$ is:\n$$ p(y_{1:n} \\mid \\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right) = (2\\pi)^{-n/2} (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\\right) $$\n\nThe joint prior distribution is given by the product of the specified conditional and marginal priors:\n$$ p(\\mu, \\sigma^2) = p(\\mu \\mid \\sigma^2) p(\\sigma^2) = \\left[ \\sqrt{\\frac{\\kappa_0}{2\\pi \\sigma^{2}}}\\,\\exp\\!\\left(-\\frac{\\kappa_0}{2\\sigma^{2}}(\\mu - m_0)^{2}\\right) \\right] \\left[ \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)}\\,(\\sigma^{2})^{-(\\alpha_0+1)}\\,\\exp\\!\\left(-\\frac{\\beta_0}{\\sigma^{2}}\\right) \\right] $$\nIgnoring constants that do not depend on $\\mu$ or $\\sigma^2$, the prior is proportional to:\n$$ p(\\mu, \\sigma^2) \\propto (\\sigma^2)^{-1/2} \\exp\\left(-\\frac{\\kappa_0}{2\\sigma^2}(\\mu - m_0)^2\\right) (\\sigma^2)^{-(\\alpha_0+1)} \\exp\\left(-\\frac{\\beta_0}{\\sigma^2}\\right) $$\n$$ p(\\mu, \\sigma^2) \\propto (\\sigma^2)^{-(\\alpha_0 + 3/2)} \\exp\\left(-\\frac{1}{2\\sigma^2} \\left[ \\kappa_0(\\mu-m_0)^2 + 2\\beta_0 \\right] \\right) $$\n\nMultiplying the likelihood and the prior kernels, we obtain the kernel of the posterior distribution:\n$$ p(\\mu, \\sigma^2 \\mid y_{1:n}) \\propto (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\\right) \\times (\\sigma^2)^{-(\\alpha_0 + 3/2)} \\exp\\left(-\\frac{1}{2\\sigma^2} \\left[ \\kappa_0(\\mu-m_0)^2 + 2\\beta_0 \\right] \\right) $$\nCombine the terms:\n$$ p(\\mu, \\sigma^2 \\mid y_{1:n}) \\propto (\\sigma^2)^{-(n/2 + \\alpha_0 + 3/2)} \\exp\\left(-\\frac{1}{2\\sigma^2} \\left[ \\sum_{i=1}^{n} (y_i - \\mu)^2 + \\kappa_0(\\mu-m_0)^2 + 2\\beta_0 \\right] \\right) $$\n\nTo identify the form of the posterior, we analyze the term in the exponent by completing the square with respect to $\\mu$. Let's expand the terms involving $\\mu$:\n$$ \\sum_{i=1}^{n} (y_i - \\mu)^2 + \\kappa_0(\\mu-m_0)^2 $$\nWe use the identity $\\sum (y_i-\\mu)^2 = \\sum (y_i-\\bar{y}+\\bar{y}-\\mu)^2 = \\sum(y_i-\\bar{y})^2 + n(\\bar{y}-\\mu)^2 = S + n(\\bar{y}-\\mu)^2$.\nThe expression becomes:\n$$ S + n(\\bar{y}-\\mu)^2 + \\kappa_0(\\mu-m_0)^2 = S + n(\\mu^2 - 2\\mu\\bar{y} + \\bar{y}^2) + \\kappa_0(\\mu^2 - 2\\mu m_0 + m_0^2) $$\n$$ = S + (n+\\kappa_0)\\mu^2 - 2(n\\bar{y}+\\kappa_0 m_0)\\mu + (n\\bar{y}^2 + \\kappa_0 m_0^2) $$\nLet $\\kappa_n = \\kappa_0 + n$. Completing the square for $\\mu$:\n$$ = (n+\\kappa_0) \\left[ \\mu^2 - 2\\frac{n\\bar{y}+\\kappa_0 m_0}{n+\\kappa_0}\\mu \\right] + S + n\\bar{y}^2 + \\kappa_0 m_0^2 $$\nLet $m_n = \\frac{\\kappa_0 m_0 + n\\bar{y}}{\\kappa_0 + n}$. The expression is:\n$$ \\kappa_n \\left[ (\\mu-m_n)^2 - m_n^2 \\right] + S + n\\bar{y}^2 + \\kappa_0 m_0^2 = \\kappa_n(\\mu-m_n)^2 + S + n\\bar{y}^2 + \\kappa_0 m_0^2 - \\kappa_n m_n^2 $$\nThe terms not involving $\\mu$ simplify to:\n$$ S + n\\bar{y}^2 + \\kappa_0 m_0^2 - (\\kappa_0+n)\\left(\\frac{\\kappa_0 m_0 + n\\bar{y}}{\\kappa_0+n}\\right)^2 = S + \\frac{(n\\bar{y}^2 + \\kappa_0 m_0^2)(\\kappa_0+n) - (\\kappa_0 m_0 + n\\bar{y})^2}{\\kappa_0+n} $$\n$$ = S + \\frac{n\\kappa_0(\\bar{y}-m_0)^2}{\\kappa_0+n} $$\nSubstituting this back into the posterior kernel's exponent term gives:\n$$ \\kappa_n(\\mu-m_n)^2 + S + \\frac{n\\kappa_0}{\\kappa_0+n}(\\bar{y}-m_0)^2 + 2\\beta_0 $$\nThe full posterior kernel is thus:\n$$ p(\\mu, \\sigma^2 \\mid y_{1:n}) \\propto (\\sigma^2)^{-(\\alpha_0 + n/2 + 3/2)} \\exp\\left(-\\frac{1}{2\\sigma^2} \\left[\\kappa_n(\\mu-m_n)^2 + 2\\beta_0 + S + \\frac{n\\kappa_0}{\\kappa_0+n}(\\bar{y}-m_0)^2 \\right] \\right) $$\nThis expression has the form of a Normal-Inverse-Gamma distribution, $p(\\mu, \\sigma^2) \\propto (\\sigma^2)^{-(\\alpha_n+3/2)}\\exp\\left(-\\frac{1}{2\\sigma^2}[\\kappa_n(\\mu-m_n)^2+2\\beta_n]\\right)$.\nBy comparing the terms, we can identify the updated hyperparameters:\n1.  $\\kappa_n = \\kappa_0 + n$\n2.  $m_n = \\frac{\\kappa_0 m_0 + n\\bar{y}}{\\kappa_0 + n}$\n3.  $\\alpha_n = \\alpha_0 + \\frac{n}{2}$\n4.  $2\\beta_n = 2\\beta_0 + S + \\frac{n\\kappa_0}{\\kappa_0+n}(\\bar{y}-m_0)^2 \\implies \\beta_n = \\beta_0 + \\frac{1}{2}S + \\frac{n\\kappa_0}{2(\\kappa_0+n)}(\\bar{y}-m_0)^2$\n\nThis concludes the derivation. The posterior is $p(\\mu, \\sigma^2 \\mid y_{1:n}) \\sim \\text{NIG}(m_n, \\kappa_n, \\alpha_n, \\beta_n)$.\n\n### Calculation of Posterior Hyperparameters\n\nFirst, we compute the sufficient statistics from the data.\n-   Sample size $n = 12$.\n-   Sample sum: $\\sum_{i=1}^{12} y_i = 0.30 - 0.05 + 0.12 + 0.18 + 0.26 + 0.09 + 0.02 + 0.21 + 0.15 + 0.11 + 0.19 + 0.14 = 1.72$.\n-   Sample mean: $\\bar{y} = \\frac{1.72}{12} = \\frac{43}{300} \\approx 0.14333...$\n-   Sum of squares of data: $\\sum_{i=1}^{12} y_i^2 = 0.30^2 + (-0.05)^2 + \\dots + 0.14^2 = 0.3498$.\n-   Corrected sum of squares: $S = \\sum y_i^2 - n\\bar{y}^2 = 0.3498 - 12\\left(\\frac{1.72}{12}\\right)^2 = 0.3498 - \\frac{1.72^2}{12} = 0.3498 - \\frac{2.9584}{12} = 0.3498 - 0.246533... = 0.103266...$\n\nNow, we use the update formulas derived above with the prior hyperparameters $m_0=0.10, \\kappa_0=4, \\alpha_0=3, \\beta_0=0.02$.\n\n-   $\\kappa_n = \\kappa_0 + n = 4 + 12 = 16$.\n-   $\\alpha_n = \\alpha_0 + \\frac{n}{2} = 3 + \\frac{12}{2} = 3 + 6 = 9$.\n-   $m_n = \\frac{\\kappa_0 m_0 + n\\bar{y}}{\\kappa_0 + n} = \\frac{4(0.10) + 12(\\frac{1.72}{12})}{4+12} = \\frac{0.4 + 1.72}{16} = \\frac{2.12}{16} = 0.1325$.\n-   $\\beta_n = \\beta_0 + \\frac{1}{2}S + \\frac{n\\kappa_0}{2(\\kappa_0+n)}(\\bar{y}-m_0)^2$.\n    The individual terms are:\n    -   $\\beta_0 = 0.02$.\n    -   $\\frac{1}{2}S = \\frac{1}{2}(0.103266...) = 0.051633...$\n    -   The last term:\n        $\\frac{n\\kappa_0}{2(\\kappa_0+n)}(\\bar{y}-m_0)^2 = \\frac{12 \\cdot 4}{2(16)}\\left(\\frac{1.72}{12}-0.10\\right)^2 = 1.5 \\left(\\frac{1.72-1.2}{12}\\right)^2 = 1.5\\left(\\frac{0.52}{12}\\right)^2 = 1.5 \\frac{0.2704}{144} = 0.0028166...$\n    -   Summing the terms for $\\beta_n$:\n        $\\beta_n = 0.02 + 0.051633... + 0.0028166... = 0.07445$.\n\nThe posterior hyperparameters are:\n$m_n = 0.1325$\n$\\kappa_n = 16$\n$\\alpha_n = 9$\n$\\beta_n = 0.07445$\n\nRounding these values to four significant figures as requested:\n$m_n = 0.1325$\n$\\kappa_n = 16.00$\n$\\alpha_n = 9.000$\n$\\beta_n = 0.07445$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.1325  16.00  9.000  0.07445\n\\end{pmatrix}\n}\n$$", "id": "4318423"}, {"introduction": "Building on the principles of Bayesian updating, we now turn to hierarchical models, which are exceptionally powerful for analyzing data from multiple, related sources like different patient cohorts or laboratories. In this exercise [@problem_id:4318486], you will work with the Beta-Binomial model to derive the posterior predictive distribution for a new observation. This process will reveal the elegant concept of partial pooling, where the model automatically balances information from local data with a global prior to produce more stable and robust estimates.", "problem": "A consortium of hospital-based systems biomedicine laboratories is monitoring the carriage of a specific antibiotic resistance gene in rectal swabs using quantitative Polymerase Chain Reaction (qPCR). Within any single hospital $j$, patient-level carriage outcomes on a given day are modeled as exchangeable Bernoulli trials with an unknown latent prevalence $\\theta_{j} \\in (0,1)$. Across hospitals, the latent prevalences $\\theta_{j}$ are assumed to arise from a common population-level distribution that encodes between-hospital heterogeneity.\n\nSuppose a previously conducted meta-analysis over many hospitals yields a scientifically justified population-level prior for the latent prevalence as a Beta distribution with hyperparameters $\\alpha$ and $\\beta$, that is, $\\theta_{j} \\sim \\operatorname{Beta}(\\alpha,\\beta)$. In a particular hospital $j^{\\star}$ on a given day, $n$ independent and identically distributed patient swabs are processed, and $x$ of them test positive for the resistance gene. Under the Beta–Binomial exchangeable model for hospital $j^{\\star}$, derive from first principles the posterior predictive distribution for the next Bernoulli trial $Y_{\\text{next}} \\in \\{0,1\\}$ at this hospital by integrating over the latent prevalence $\\theta_{j^{\\star}}$. Explicitly show how partial pooling emerges from this integration as a convex combination of the hospital-specific empirical proportion and the population-level prior mean.\n\nFor a concrete calculation, take $\\alpha = 1.5$, $\\beta = 6.5$, $n = 12$, and $x = 7$ for hospital $j^{\\star}$. Compute the posterior predictive probability $\\mathbb{P}(Y_{\\text{next}} = 1 \\mid \\text{data from } j^{\\star})$ under the model. Express the final probability as a decimal and round your answer to four significant figures.", "solution": "The problem is well-posed, scientifically grounded within the framework of Bayesian statistics and its application to systems biomedicine, and provides all necessary information for a unique solution.\n\nLet $\\theta_{j^{\\star}} \\in (0,1)$ represent the latent prevalence of the antibiotic resistance gene in the specific hospital $j^{\\star}$. We will denote this simply as $\\theta$ for notational convenience. The problem provides a hierarchical model structure.\n\nFirst, we define the prior distribution for the latent prevalence $\\theta$. Based on a prior meta-analysis, this is given as a Beta distribution with hyperparameters $\\alpha$ and $\\beta$. The probability density function (PDF) of the prior is:\n$$\np(\\theta) = \\operatorname{Beta}(\\theta; \\alpha, \\beta) = \\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha, \\beta)}\n$$\nwhere $B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$ is the Beta function.\n\nSecond, we define the likelihood of observing the data given the latent prevalence $\\theta$. The data consist of $n$ independent and identically distributed (i.i.d.) patient swabs, which are modeled as exchangeable Bernoulli trials. The number of positive swabs, $x$, out of $n$ trials follows a Binomial distribution. The likelihood function is:\n$$\np(x \\mid n, \\theta) = \\binom{n}{x} \\theta^x (1-\\theta)^{n-x}\n$$\nFor brevity, we denote the observed data as $D = \\{x, n\\}$.\n\nTo derive the posterior predictive distribution, we must first find the posterior distribution of $\\theta$ given the data $D$. Using Bayes' theorem, the posterior PDF $p(\\theta \\mid D)$ is proportional to the product of the likelihood and the prior:\n$$\np(\\theta \\mid D) \\propto p(D \\mid \\theta) p(\\theta)\n$$\nSubstituting the expressions for the likelihood and the prior:\n$$\np(\\theta \\mid D) \\propto \\left[ \\binom{n}{x} \\theta^x (1-\\theta)^{n-x} \\right] \\left[ \\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha, \\beta)} \\right]\n$$\nSince $\\binom{n}{x}$ and $B(\\alpha, \\beta)$ are constants with respect to $\\theta$, we can combine the terms involving $\\theta$:\n$$\np(\\theta \\mid D) \\propto \\theta^x (1-\\theta)^{n-x} \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1} = \\theta^{\\alpha+x-1} (1-\\theta)^{\\beta+n-x-1}\n$$\nThis expression is the kernel of a Beta distribution. Therefore, the posterior distribution of $\\theta$ is also a Beta distribution, a property known as conjugacy. The updated hyperparameters are $\\alpha' = \\alpha+x$ and $\\beta' = \\beta+n-x$. The full posterior PDF is:\n$$\np(\\theta \\mid D) = \\operatorname{Beta}(\\theta; \\alpha+x, \\beta+n-x) = \\frac{\\theta^{\\alpha+x-1} (1-\\theta)^{\\beta+n-x-1}}{B(\\alpha+x, \\beta+n-x)}\n$$\n\nNow, we can derive the posterior predictive distribution for a new Bernoulli trial, $Y_{\\text{next}} \\in \\{0, 1\\}$. We are interested in the probability that this next trial is a success (i.e., the swab is positive), $\\mathbb{P}(Y_{\\text{next}}=1 \\mid D)$. This probability is obtained by marginalizing (integrating) the likelihood of the new observation over the posterior distribution of the parameter $\\theta$:\n$$\n\\mathbb{P}(Y_{\\text{next}}=1 \\mid D) = \\int_{0}^{1} \\mathbb{P}(Y_{\\text{next}}=1 \\mid \\theta, D) p(\\theta \\mid D) \\,d\\theta\n$$\nGiven the assumption of exchangeability, the outcome of the next trial is conditionally independent of the past data $D$ given the parameter $\\theta$. Thus, $\\mathbb{P}(Y_{\\text{next}}=1 \\mid \\theta, D) = \\mathbb{P}(Y_{\\text{next}}=1 \\mid \\theta) = \\theta$. The integral becomes:\n$$\n\\mathbb{P}(Y_{\\text{next}}=1 \\mid D) = \\int_{0}^{1} \\theta \\cdot p(\\theta \\mid D) \\,d\\theta\n$$\nThis integral is, by definition, the expected value of $\\theta$ under the posterior distribution, $\\mathbb{E}[\\theta \\mid D]$. The mean of a Beta distribution $\\operatorname{Beta}(\\alpha', \\beta')$ is given by $\\frac{\\alpha'}{\\alpha'+\\beta'}$. For our posterior distribution $\\operatorname{Beta}(\\theta; \\alpha+x, \\beta+n-x)$, the mean is:\n$$\n\\mathbb{E}[\\theta \\mid D] = \\frac{\\alpha+x}{(\\alpha+x) + (\\beta+n-x)} = \\frac{\\alpha+x}{\\alpha+\\beta+n}\n$$\nSo, the posterior predictive probability of a positive result is:\n$$\n\\mathbb{P}(Y_{\\text{next}}=1 \\mid D) = \\frac{\\alpha+x}{\\alpha+\\beta+n}\n$$\nThe posterior predictive distribution for the Bernoulli trial $Y_{\\text{next}}$ is a Bernoulli distribution with parameter $p = \\mathbb{P}(Y_{\\text{next}}=1 \\mid D)$.\n\nTo show how this result embodies partial pooling, we can rewrite the expression as a weighted average of the hospital-specific empirical proportion, $\\frac{x}{n}$, and the population-level prior mean, $\\frac{\\alpha}{\\alpha+\\beta}$. The posterior predictive probability can be shown to be equivalent to the following convex combination:\n$$\n\\frac{n}{\\alpha+\\beta+n} \\left(\\frac{x}{n}\\right) + \\frac{\\alpha+\\beta}{\\alpha+\\beta+n} \\left(\\frac{\\alpha}{\\alpha+\\beta}\\right) = \\frac{x}{\\alpha+\\beta+n} + \\frac{\\alpha}{\\alpha+\\beta+n} = \\frac{\\alpha+x}{\\alpha+\\beta+n}\n$$\nThus, the posterior predictive probability is a convex combination:\n$$\n\\mathbb{P}(Y_{\\text{next}}=1 \\mid D) = \\left( \\frac{n}{n + \\alpha+\\beta} \\right) \\left( \\frac{x}{n} \\right) + \\left( \\frac{\\alpha+\\beta}{n + \\alpha+\\beta} \\right) \\left( \\frac{\\alpha}{\\alpha+\\beta} \\right)\n$$\nHere, the weight $\\frac{n}{n+\\alpha+\\beta}$ is applied to the hospital-specific empirical proportion ($\\frac{x}{n}$), and the complementary weight $\\frac{\\alpha+\\beta}{n+\\alpha+\\beta}$ is applied to the population-level prior mean ($\\frac{\\alpha}{\\alpha+\\beta}$). The term $\\alpha+\\beta$ can be interpreted as a \"prior sample size\" or a measure of the strength of the prior belief. The posterior estimate is thus \"pulled\" away from the local data towards the global mean, an effect known as partial pooling or shrinkage. The degree of shrinkage depends on the relative sizes of the observed sample ($n$) and the prior effective sample size ($\\alpha+\\beta$).\n\nFor the concrete calculation, the given values are $\\alpha = 1.5$, $\\beta = 6.5$, $n = 12$, and $x = 7$.\nThe posterior predictive probability $\\mathbb{P}(Y_{\\text{next}} = 1 \\mid D)$ is calculated using the formula derived above:\n$$\n\\mathbb{P}(Y_{\\text{next}}=1 \\mid D) = \\frac{\\alpha+x}{\\alpha+\\beta+n} = \\frac{1.5+7}{1.5+6.5+12} = \\frac{8.5}{8+12} = \\frac{8.5}{20}\n$$\nCalculating the decimal value:\n$$\n\\frac{8.5}{20} = 0.425\n$$\nThe problem requires the answer to be rounded to four significant figures. This gives $0.4250$.\nLet's also compute the components of the partial pooling to verify:\nPrior mean: $\\frac{\\alpha}{\\alpha+\\beta} = \\frac{1.5}{1.5+6.5} = \\frac{1.5}{8} = 0.1875$.\nEmpirical proportion: $\\frac{x}{n} = \\frac{7}{12} \\approx 0.5833$.\nThe weights are:\nWeight on data: $\\frac{n}{n+\\alpha+\\beta} = \\frac{12}{12+8} = \\frac{12}{20} = 0.6$.\nWeight on prior: $\\frac{\\alpha+\\beta}{n+\\alpha+\\beta} = \\frac{8}{12+8} = \\frac{8}{20} = 0.4$.\nThe posterior mean is $(0.6) \\times (\\frac{7}{12}) + (0.4) \\times (0.1875) = 0.6 \\times 0.58333... + 0.4 \\times 0.1875 = 0.35 + 0.075 = 0.425$. The calculation is consistent. The final probability is $0.4250$.", "answer": "$$\\boxed{0.4250}$$", "id": "4318486"}]}