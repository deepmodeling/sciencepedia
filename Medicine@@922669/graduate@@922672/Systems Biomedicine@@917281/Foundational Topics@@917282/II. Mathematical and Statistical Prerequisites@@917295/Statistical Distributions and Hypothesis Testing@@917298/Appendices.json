{"hands_on_practices": [{"introduction": "The normal distribution is a cornerstone of statistical modeling, often describing measurement error in biomedical assays. A powerful tool for analyzing any distribution is its moment generating function (MGF), which can be used to systematically derive all of its moments. This exercise will guide you through the derivation of the MGF for a normal random variable and its application in calculating higher-order central moments, providing deep insight into the distribution's shape.", "problem": "In a systems biomedicine assay, the residual measurement error of a calibrated quantitative biomarker readout is well modeled by a normal random variable $X$ with mean $\\mu$ and variance $\\sigma^{2}$. The probability density function of $X$ is given by\n$$\nf_{X}(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right).\n$$\nStarting from the definition of the moment generating function (MGF),\n$$\nM_{X}(t)=\\mathbb{E}\\!\\left[\\exp(tX)\\right]=\\int_{-\\infty}^{\\infty}\\exp(tx)\\,f_{X}(x)\\,dx,\n$$\nderive a closed-form expression for $M_{X}(t)$ that is valid for all real $t$. Then, using only first principles (the definition above and standard theorems justifying differentiation under the integral sign), justify how repeated differentiation of $M_{X}(t)$ at $t=0$ yields the raw moments $\\mathbb{E}[X^{n}]$ for any nonnegative integer $n$. Finally, apply this framework to the centered variable $Y=X-\\mu$ and compute the fourth central moment $\\mathbb{E}\\!\\left[(X-\\mu)^{4}\\right]$ explicitly by differentiating the appropriate MGF. \n\nProvide the final answer as a single closed-form analytic expression in terms of $\\sigma$. Do not round.", "solution": "The problem is valid as it is a standard, well-posed problem in probability theory and statistics, grounded in correct mathematical and scientific principles. We will proceed with the derivation in three parts as requested.\n\nPart 1: Derivation of the Moment Generating Function (MGF) of a Normal Random Variable\n\nThe problem defines a normal random variable $X$ with mean $\\mu$ and variance $\\sigma^2$, having the probability density function (PDF):\n$$\nf_{X}(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right)\n$$\nThe moment generating function (MGF), $M_X(t)$, is defined as the expectation of $\\exp(tX)$:\n$$\nM_{X}(t) = \\mathbb{E}[\\exp(tX)] = \\int_{-\\infty}^{\\infty} \\exp(tx) f_{X}(x) dx\n$$\nSubstituting the PDF of the normal distribution into this definition, we have:\n$$\nM_{X}(t) = \\int_{-\\infty}^{\\infty} \\exp(tx) \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right) dx\n$$\nWe can combine the arguments of the exponential functions:\n$$\nM_{X}(t) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\left(tx - \\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right) dx\n$$\nTo solve this integral, we complete the square in the exponent. Let's focus on the argument of the exponential:\n$$\ntx - \\frac{(x-\\mu)^{2}}{2\\sigma^{2}} = tx - \\frac{x^2 - 2\\mu x + \\mu^2}{2\\sigma^2} = \\frac{2\\sigma^2 tx - (x^2 - 2\\mu x + \\mu^2)}{2\\sigma^2}\n$$\n$$\n= -\\frac{1}{2\\sigma^2} \\left[ x^2 - 2\\mu x - 2\\sigma^2 tx + \\mu^2 \\right] = -\\frac{1}{2\\sigma^2} \\left[ x^2 - 2(\\mu + \\sigma^2 t)x + \\mu^2 \\right]\n$$\nNow, we complete the square for the terms involving $x$. We add and subtract $(\\mu + \\sigma^2 t)^2$:\n$$\n= -\\frac{1}{2\\sigma^2} \\left[ \\left(x^2 - 2(\\mu + \\sigma^2 t)x + (\\mu + \\sigma^2 t)^2\\right) - (\\mu + \\sigma^2 t)^2 + \\mu^2 \\right]\n$$\n$$\n= -\\frac{1}{2\\sigma^2} \\left[ (x - (\\mu + \\sigma^2 t))^2 - (\\mu^2 + 2\\mu\\sigma^2 t + \\sigma^4 t^2) + \\mu^2 \\right]\n$$\n$$\n= -\\frac{(x - (\\mu + \\sigma^2 t))^2}{2\\sigma^2} + \\frac{2\\mu\\sigma^2 t + \\sigma^4 t^2}{2\\sigma^2} = -\\frac{(x - (\\mu + \\sigma^2 t))^2}{2\\sigma^2} + \\mu t + \\frac{1}{2}\\sigma^2 t^2\n$$\nSubstituting this back into the integral for $M_X(t)$:\n$$\nM_{X}(t) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{(x - (\\mu + \\sigma^2 t))^2}{2\\sigma^2} + \\mu t + \\frac{1}{2}\\sigma^2 t^2 \\right) dx\n$$\nThe term $\\exp(\\mu t + \\frac{1}{2}\\sigma^2 t^2)$ does not depend on $x$ and can be factored out of the integral:\n$$\nM_{X}(t) = \\exp\\left(\\mu t + \\frac{1}{2}\\sigma^2 t^2\\right) \\int_{-\\infty}^{\\infty} \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left( -\\frac{(x - (\\mu + \\sigma^2 t))^2}{2\\sigma^2} \\right) dx\n$$\nThe remaining integral is the integral of a normal PDF with mean $\\mu' = \\mu + \\sigma^2 t$ and variance $\\sigma^2$ over its entire domain $(-\\infty, \\infty)$. The total area under any PDF is equal to $1$. Therefore, the integral evaluates to $1$.\nThis leaves us with the closed-form expression for the MGF of a normal distribution:\n$$\nM_{X}(t) = \\exp\\left(\\mu t + \\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nThis expression is valid for all real values of $t$.\n\nPart 2: Justification of Moment Generation via Differentiation\n\nThe $n$-th raw moment of $X$ is defined as $\\mathbb{E}[X^n]$. We are asked to show how this relates to the derivatives of $M_X(t)$ at $t=0$. Let's differentiate $M_X(t)$ with respect to $t$:\n$$\n\\frac{d^n}{dt^n} M_X(t) = \\frac{d^n}{dt^n} \\int_{-\\infty}^{\\infty} \\exp(tx) f_X(x) dx\n$$\nUnder certain regularity conditions, we can interchange the order of differentiation and integration (a result formally justified by theorems such as the Leibniz integral rule or the Dominated Convergence Theorem). For the normal distribution, the integrand's partial derivatives with respect to $t$ exist and are continuous. Furthermore, for any $t$ in a compact interval around $0$, the integral of the absolute value of the derivative is bounded, because the exponential decay of $f_X(x)$ as $|x| \\to \\infty$ dominates any polynomial growth from the differentiation. This justifies the interchange.\n$$\n\\frac{d^n}{dt^n} M_X(t) = \\int_{-\\infty}^{\\infty} \\frac{\\partial^n}{\\partial t^n} [\\exp(tx)] f_X(x) dx\n$$\nThe $n$-th partial derivative of $\\exp(tx)$ with respect to $t$ is $x^n \\exp(tx)$.\n$$\n\\frac{d^n}{dt^n} M_X(t) = \\int_{-\\infty}^{\\infty} x^n \\exp(tx) f_X(x) dx = \\mathbb{E}[X^n \\exp(tX)]\n$$\nTo find the $n$-th raw moment, we evaluate this expression at $t=0$:\n$$\n\\left. \\frac{d^n}{dt^n} M_X(t) \\right|_{t=0} = \\mathbb{E}[X^n \\exp(0 \\cdot X)] = \\mathbb{E}[X^n \\cdot 1] = \\mathbb{E}[X^n]\n$$\nThis demonstrates that the $n$-th raw moment $\\mathbb{E}[X^n]$ can be obtained by taking the $n$-th derivative of the MGF $M_X(t)$ and evaluating it at $t=0$.\n\nPart 3: Calculation of the Fourth Central Moment\n\nThe fourth central moment is $\\mathbb{E}[(X-\\mu)^4]$. Let's define a centered random variable $Y = X-\\mu$. The moments of $Y$ are the central moments of $X$. We need to find the MGF of $Y$, denoted $M_Y(t)$.\n$$\nM_Y(t) = \\mathbb{E}[\\exp(tY)] = \\mathbb{E}[\\exp(t(X-\\mu))] = \\mathbb{E}[\\exp(tX)\\exp(-t\\mu)]\n$$\nSince $\\exp(-t\\mu)$ is a constant with respect to the random variable $X$, we can factor it out of the expectation:\n$$\nM_Y(t) = \\exp(-t\\mu) \\mathbb{E}[\\exp(tX)] = \\exp(-t\\mu) M_X(t)\n$$\nSubstituting the expression for $M_X(t)$ we derived in Part 1:\n$$\nM_Y(t) = \\exp(-t\\mu) \\exp\\left(\\mu t + \\frac{1}{2}\\sigma^2 t^2\\right) = \\exp\\left(-t\\mu + \\mu t + \\frac{1}{2}\\sigma^2 t^2\\right) = \\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nNote that $Y$ is a normal random variable with mean $\\mathbb{E}[Y] = \\mathbb{E}[X-\\mu] = \\mu-\\mu=0$ and variance $\\mathrm{Var}(Y) = \\mathrm{Var}(X) = \\sigma^2$. The MGF we found for $Y$ is consistent with the general formula for a normal distribution with mean $0$ and variance $\\sigma^2$.\n\nTo find the fourth central moment, $\\mathbb{E}[Y^4]$, we must compute the fourth derivative of $M_Y(t)$ and evaluate it at $t=0$.\n$$\nM_Y(t) = \\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nFirst derivative:\n$$\nM_Y'(t) = \\frac{d}{dt}\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right) = (\\sigma^2 t)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nSecond derivative (using the product rule):\n$$\nM_Y''(t) = \\sigma^2 \\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right) + (\\sigma^2 t)(\\sigma^2 t)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right) = (\\sigma^2 + \\sigma^4 t^2)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nThird derivative:\n$$\nM_Y'''(t) = (2\\sigma^4 t)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right) + (\\sigma^2 + \\sigma^4 t^2)(\\sigma^2 t)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\n$$\n= (2\\sigma^4 t + \\sigma^4 t + \\sigma^6 t^3)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right) = (3\\sigma^4 t + \\sigma^6 t^3)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nFourth derivative:\n$$\nM_Y^{(4)}(t) = (3\\sigma^4 + 3\\sigma^6 t^2)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right) + (3\\sigma^4 t + \\sigma^6 t^3)(\\sigma^2 t)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\n$$\n= (3\\sigma^4 + 3\\sigma^6 t^2 + 3\\sigma^6 t^2 + \\sigma^8 t^4)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\n$$\n= (3\\sigma^4 + 6\\sigma^6 t^2 + \\sigma^8 t^4)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nFinally, we evaluate the fourth derivative at $t=0$ to obtain the fourth central moment:\n$$\n\\mathbb{E}[(X-\\mu)^4] = \\mathbb{E}[Y^4] = M_Y^{(4)}(0) = (3\\sigma^4 + 6\\sigma^6(0)^2 + \\sigma^8(0)^4)\\exp(0) = 3\\sigma^4\n$$\nThe fourth central moment is $3\\sigma^4$.", "answer": "$$\n\\boxed{3\\sigma^{4}}\n$$", "id": "4387180"}, {"introduction": "Choosing the appropriate statistical test is a critical decision in experimental design. While the paired $t$-test is a common choice for matched-pair data, its performance hinges on assumptions that may not hold for all biomedical data. This practice challenges you to compare the $t$-test with the non-parametric sign test using the concept of Asymptotic Relative Efficiency (ARE), revealing how their relative power changes depending on the underlying error distribution.", "problem": "In a paired design in systems biomedicine, an investigator measures a biomarker response under two conditions for each subject (for example, pre-treatment and post-treatment), then forms the matched-pair differences $D_{i} = Y_{i, \\mathrm{post}} - Y_{i, \\mathrm{pre}}$ for $i = 1, \\dots, n$. Suppose a location-shift model $D_{i} = \\theta + \\varepsilon_{i}$, where the errors $\\varepsilon_{i}$ are independent and identically distributed, symmetrically about $0$, with a continuous density $f$ and finite variance $\\sigma^{2}$ when it exists. The null hypothesis is $H_{0}: \\theta = 0$.\n\nFrom first principles appropriate to statistical inference in systems biomedicine, derive the sign test for matched pairs as a test based solely on the signs of the $D_{i}$’s, and establish its large-sample behavior under $H_{0}$ and under contiguous alternatives $H_{1,n}: \\theta = \\delta / \\sqrt{n}$, with fixed $\\delta \\in \\mathbb{R}$. Then, using the fundamental definition of local asymptotic relative efficiency between two tests, compare the efficiency of the sign test to the paired Student’s $t$-test in the location-shift model. Specifically:\n\n- Construct the sign test statistic from $\\{D_{i}\\}$ and justify its null model without invoking any distribution-specific shortcut formulas.\n- Under $H_{1,n}: \\theta = \\delta / \\sqrt{n}$, derive the leading-order mean shift of the appropriately standardized sign statistic using only the continuity of $f$ at $0$.\n- Derive the leading-order mean shift of the appropriately standardized paired $t$-statistic under the same local alternative, using standard large-sample arguments.\n- From these derivations, compute the Pitman asymptotic relative efficiency (ARE) of the sign test relative to the paired $t$-test, expressed in terms of $f(0)$ and $\\sigma^{2}$, where $f(0)$ is the value of the error density at $0$ and $\\sigma^{2}$ is the error variance.\n\nFinally, evaluate this ARE under two scientifically plausible error models for $D_{i}$ in systems biomedicine:\n\n1. A normal error distribution with variance $\\sigma^{2}$, that is, $D_{i} \\sim \\mathcal{N}(\\theta, \\sigma^{2})$.\n2. A heavy-tailed double-exponential (Laplace) error distribution with scale $b  0$, that is, $f(x) = \\frac{1}{2b} \\exp\\!\\left(-\\frac{|x|}{b}\\right)$ for the errors, implying $\\sigma^{2} = 2 b^{2}$.\n\nExpress your final answer as a row vector containing the two ARE values for the normal and Laplace cases, respectively, in exact analytic form. No rounding is required. Do not include any units in the final answer.", "solution": "The posed problem is a standard, well-defined exercise in asymptotic statistical theory, comparing a non-parametric test (the sign test) with a parametric test (the paired Student's $t$-test). The problem is scientifically grounded, internally consistent, and requires a rigorous derivation from first principles. It is therefore deemed valid.\n\nThe problem requires a derivation of the Pitman Asymptotic Relative Efficiency (ARE) of the sign test relative to the paired $t$-test. The ARE is defined for a sequence of tests applied to a sequence of local alternatives. For two test statistics $T_{1,n}$ and $T_{2,n}$ that are asymptotically normal under the null hypothesis $H_0$ and under a sequence of local alternatives $H_{1,n}: \\theta = \\delta/\\sqrt{n}$, such that under $H_{1,n}$, $T_{k,n} \\xrightarrow{d} \\mathcal{N}(\\mu_k, \\sigma_k^2)$ for $k=1, 2$, the Pitman ARE is the ratio of their squared non-centrality parameters, or efficacies, defined as $(\\mu_k/\\sigma_k)^2$. Assuming the statistics are standardized to have unit variance under $H_0$, the ARE simplifies to the ratio of their squared asymptotic means under $H_{1,n}$.\n\nLet's begin by analyzing each test as requested.\n\n**1. The Sign Test**\n\nThe sign test is constructed from the signs of the paired differences $D_i = \\theta + \\varepsilon_i$. Let $S_n$ be the number of positive differences:\n$$S_n = \\sum_{i=1}^{n} \\mathbb{I}(D_i  0)$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function.\n\nFirst, we establish the distribution of $S_n$ under the null hypothesis $H_0: \\theta=0$. Under $H_0$, we have $D_i = \\varepsilon_i$. The problem states that the errors $\\varepsilon_i$ are i.i.d. from a continuous distribution that is symmetric about $0$.\nSymmetry about $0$ implies $P(\\varepsilon_i  0) = P(\\varepsilon_i  0)$.\nContinuity of the density $f$ at $0$ implies $P(\\varepsilon_i = 0) = \\int_0^0 f(x) dx = 0$.\nSince $P(\\varepsilon_i  0) + P(\\varepsilon_i  0) + P(\\varepsilon_i = 0) = 1$, we must have $P(\\varepsilon_i  0) = 1/2$.\nThus, under $H_0$, the random variables $\\mathbb{I}(D_i  0)$ are i.i.d. Bernoulli trials with success probability $p = 1/2$. The test statistic $S_n$ is the sum of these trials, so its exact null distribution is Binomial:\n$$S_n \\sim \\mathrm{Bin}(n, 1/2)$$\nFor large $n$, we standardize $S_n$ to create an asymptotically normal test statistic. The mean and variance of $S_n$ under $H_0$ are $E_0[S_n] = n/2$ and $\\mathrm{Var}_0(S_n) = n(1/2)(1-1/2) = n/4$. The standardized statistic is:\n$$Z_{S,n} = \\frac{S_n - n/2}{\\sqrt{n/4}} = \\frac{2S_n - n}{\\sqrt{n}}$$\nBy the De Moivre-Laplace Central Limit Theorem, $Z_{S,n} \\xrightarrow{d} \\mathcal{N}(0, 1)$ under $H_0$.\n\nNext, we analyze the behavior of $Z_{S,n}$ under the contiguous alternative $H_{1,n}: \\theta = \\delta/\\sqrt{n}$. Under this alternative, $D_i = \\delta/\\sqrt{n} + \\varepsilon_i$. The probability of a \"success\" for each trial is now:\n$$p_n = P(D_i  0) = P(\\varepsilon_i  -\\delta/\\sqrt{n}) = 1 - F(-\\delta/\\sqrt{n})$$\nwhere $F$ is the cumulative distribution function (CDF) of $\\varepsilon_i$. Due to the symmetry of the error distribution ($f(x) = f(-x)$), we have $F(-z) = 1 - F(z)$, so $p_n = F(\\delta/\\sqrt{n})$.\nWe perform a first-order Taylor expansion of the CDF $F$ around $0$:\n$$p_n = F(\\delta/\\sqrt{n}) \\approx F(0) + F'(0) \\cdot (\\delta/\\sqrt{n})$$\nSince the distribution is symmetric and continuous, $F(0) = 1/2$. The derivative of the CDF is the density function, so $F'(0) = f(0)$. This gives:\n$$p_n \\approx \\frac{1}{2} + \\frac{\\delta f(0)}{\\sqrt{n}}$$\nUnder $H_{1,n}$, $S_n \\sim \\mathrm{Bin}(n, p_n)$. Its expectation is $E_n[S_n] = n p_n \\approx n(1/2 + \\delta f(0)/\\sqrt{n}) = n/2 + \\sqrt{n}\\delta f(0)$.\nThe asymptotic mean of the standardized statistic $Z_{S,n}$ is:\n$$E_n[Z_{S,n}] = E_n\\left[\\frac{S_n - n/2}{\\sqrt{n}/2}\\right] = \\frac{E_n[S_n] - n/2}{\\sqrt{n}/2} \\approx \\frac{(n/2 + \\sqrt{n}\\delta f(0)) - n/2}{\\sqrt{n}/2} = 2\\delta f(0)$$\nThe variance of $S_n$ under $H_{1,n}$ is $\\mathrm{Var}_n(S_n) = n p_n (1-p_n)$. Since $p_n \\to 1/2$ as $n \\to \\infty$, the variance converges to $n/4$. Thus, the variance of the standardized statistic $Z_{S,n}$ converges to $1$.\nSo, under $H_{1,n}$, $Z_{S,n}$ converges in distribution to a normal distribution with mean $\\mu_S = 2\\delta f(0)$ and variance $1$:\n$$Z_{S,n} \\xrightarrow{d} \\mathcal{N}(2\\delta f(0), 1)$$\nThe efficacy of the sign test is $(2\\delta f(0))^2$.\n\n**2. The Paired Student's $t$-test**\n\nThe paired $t$-test statistic is based on the sample mean $\\bar{D} = \\frac{1}{n}\\sum_{i=1}^n D_i$ and the sample standard deviation $S_D = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (D_i - \\bar{D})^2}$. The standardized test statistic is:\n$$T_n = \\frac{\\bar{D}}{S_D / \\sqrt{n}} = \\frac{\\sqrt{n}\\bar{D}}{S_D}$$\nWe analyze this statistic under the same sequence of local alternatives $H_{1,n}: \\theta = \\delta/\\sqrt{n}$. The differences are $D_i = \\delta/\\sqrt{n} + \\varepsilon_i$.\nThe expectation of each $D_i$ is $E[D_i] = \\delta/\\sqrt{n}$, and the variance is $\\mathrm{Var}(D_i) = \\mathrm{Var}(\\varepsilon_i) = \\sigma^2$.\nThe expectation of the sample mean is $E[\\bar{D}] = \\delta/\\sqrt{n}$.\nThe numerator of $T_n$ is $\\sqrt{n}\\bar{D}$. Its expectation is $E[\\sqrt{n}\\bar{D}] = \\sqrt{n}E[\\bar{D}] = \\delta$.\nThe variance of $\\sqrt{n}\\bar{D}$ is $\\mathrm{Var}(\\sqrt{n}\\bar{D}) = n \\mathrm{Var}(\\bar{D}) = n (\\sigma^2/n) = \\sigma^2$.\nBy the Central Limit Theorem, the numerator converges in distribution:\n$$\\sqrt{n}\\bar{D} = \\sqrt{n}(\\bar{D} - E[\\bar{D}]) + \\sqrt{n}E[\\bar{D}] = \\sqrt{n}(\\bar{D} - \\delta/\\sqrt{n}) + \\delta \\xrightarrow{d} \\mathcal{N}(\\delta, \\sigma^2)$$\nThe denominator is the sample standard deviation $S_D$. The sample variance is $S_D^2 = \\frac{1}{n-1}\\sum_{i=1}^n (D_i - \\bar{D})^2$. Substituting $D_i = \\varepsilon_i + \\delta/\\sqrt{n}$ and $\\bar{D} = \\bar{\\varepsilon} + \\delta/\\sqrt{n}$, we get $D_i - \\bar{D} = \\varepsilon_i - \\bar{\\varepsilon}$. Thus, $S_D^2$ is simply the sample variance of the unobserved errors $\\varepsilon_i$. By the Law of Large Numbers, $S_D^2$ is a consistent estimator of $\\sigma^2$:\n$$S_D^2 \\xrightarrow{p} \\sigma^2 \\implies S_D \\xrightarrow{p} \\sigma$$\nBy Slutsky's Theorem, combining the convergence in distribution of the numerator and the convergence in probability of the denominator, we get the asymptotic distribution of $T_n$ under $H_{1,n}$:\n$$T_n = \\frac{\\sqrt{n}\\bar{D}}{S_D} \\xrightarrow{d} \\frac{\\mathcal{N}(\\delta, \\sigma^2)}{\\sigma} = \\mathcal{N}(\\delta/\\sigma, 1)$$\nThe asymptotic mean of the $t$-statistic is $\\mu_T = \\delta/\\sigma$ and the variance is $1$. The efficacy of the $t$-test is $(\\delta/\\sigma)^2$.\n\n**3. Asymptotic Relative Efficiency (ARE)**\n\nThe Pitman ARE of the sign test relative to the $t$-test is the ratio of their efficacies:\n$$\\mathrm{ARE}(\\text{Sign}, t) = \\frac{(\\text{Efficacy of Sign Test})}{(\\text{Efficacy of } t\\text{-test})} = \\frac{(2\\delta f(0))^2}{(\\delta/\\sigma)^2} = \\frac{4\\delta^2 f(0)^2}{\\delta^2/\\sigma^2}$$\n$$\\mathrm{ARE}(\\text{Sign}, t) = 4\\sigma^2 f(0)^2$$\n\n**4. Evaluation for Specific Error Distributions**\n\nWe now evaluate this ARE for the two specified cases.\n\n**Case 1: Normal Errors**\nThe errors are $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. The probability density function is $f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)$.\nThe value of the density at $0$ is $f(0) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} = \\frac{1}{\\sigma\\sqrt{2\\pi}}$.\nThe variance is $\\sigma^2$.\nSubstituting these into the ARE formula:\n$$\\mathrm{ARE}_{\\text{Normal}} = 4\\sigma^2 (f(0))^2 = 4\\sigma^2 \\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)^2 = 4\\sigma^2 \\left(\\frac{1}{2\\pi\\sigma^2}\\right) = \\frac{2}{\\pi}$$\n\n**Case 2: Laplace (Double-Exponential) Errors**\nThe error density is given as $f(x) = \\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right)$ with $b0$.\nThe value of the density at $0$ is $f(0) = \\frac{1}{2b}$.\nThe problem states the variance is $\\sigma^2 = 2b^2$. We verify this:\n$\\sigma^2 = E[\\varepsilon^2] - (E[\\varepsilon])^2$. Due to symmetry, $E[\\varepsilon]=0$.\n$E[\\varepsilon^2] = \\int_{-\\infty}^{\\infty} x^2 \\frac{1}{2b} \\exp\\left(-\\frac{|x|}{b}\\right) dx = 2 \\int_{0}^{\\infty} x^2 \\frac{1}{2b} \\exp\\left(-\\frac{x}{b}\\right) dx = \\frac{1}{b} \\int_{0}^{\\infty} x^2 \\exp\\left(-\\frac{x}{b}\\right) dx$.\nThis integral evaluates to $2b^3$ (e.g., using the Gamma function $\\Gamma(3)=2$ with substitution $t=x/b$).\nSo, $\\sigma^2 = \\frac{1}{b}(2b^3) = 2b^2$. The given variance is correct.\nSubstituting into the ARE formula:\n$$\\mathrm{ARE}_{\\text{Laplace}} = 4\\sigma^2 (f(0))^2 = 4(2b^2)\\left(\\frac{1}{2b}\\right)^2 = 8b^2\\left(\\frac{1}{4b^2}\\right) = 2$$\n\nThe final answer is composed of these two results, $\\mathrm{ARE}_{\\text{Normal}} = 2/\\pi$ and $\\mathrm{ARE}_{\\text{Laplace}} = 2$.", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{2}{\\pi}  2 \\end{pmatrix} } $$", "id": "4387140"}, {"introduction": "When distributional assumptions are suspect or sample sizes are small, permutation tests offer a robust and powerful alternative to traditional parametric methods. By leveraging computational power to create an exact null distribution from the data itself, these tests make very few assumptions. In this hands-on computational exercise, you will implement a permutation test from first principles, solidifying your understanding of how a hypothesis test works at its most fundamental level.", "problem": "Consider a two-sample comparison problem arising in systems biomedicine, where the scientific question is whether two biological conditions (for example, control versus treatment) yield different average molecular measurements (for example, log-transformed single-cell protein abundance). You are tasked with developing a rigorous permutation test for the difference in means under the assumption of exchangeability of condition labels and, for small samples, deriving the exact null distribution by enumerating all labelings consistent with the observed sample sizes.\n\nBase your derivation on the following principles:\n- Exchangeability under the null hypothesis: under the null that the two conditions have the same distribution of measurements, observed labels are uninformative, and any reassignment of labels that preserves the original group sizes is equally likely.\n- The randomization principle for hypothesis testing: the reference null distribution of a test statistic is constructed by evaluating the statistic under all labelings admissible by the experimental design, treating these as equally likely under the null.\n- The definition of the difference in means as a test statistic: for pooled observations, the statistic is the difference between the average of measurements assigned to group $A$ and the average of measurements assigned to group $B$.\n\nFormally, suppose you observe $n_A$ measurements in group $A$ and $n_B$ measurements in group $B$, with pooled data $x_1, x_2, \\dots, x_N$ where $N = n_A + n_B$. Under the null hypothesis and exchangeability of labels, all labelings that assign exactly $n_A$ of the pooled observations to $A$ and $n_B$ to $B$ are equally likely. Define the test statistic $T$ as the difference in means $T = \\bar{X}_A - \\bar{X}_B$. The exact null distribution for small samples is the multiset of all values of $T$ obtained by enumerating all labelings, equipped with the uniform probability over labelings. The two-sided $p$-value is the probability, under this null distribution, of observing a statistic with absolute value at least as large as the observed statistic from the original labels.\n\nYour program must:\n- Compute the observed difference in means $T_{\\text{obs}}$ from the provided labeled data.\n- Enumerate all admissible labelings (that preserve the original group sizes) and compute the exact null distribution of $T$ by counting the frequency of each unique value of $T$ and dividing by the total number of labelings.\n- Compute the exact two-sided $p$-value as the total null probability of $|T| \\geq |T_{\\text{obs}}|$, expressed as a decimal.\n- For each test case, output a list containing three components: the two-sided $p$-value, the sorted list of unique support points of the exact null distribution, and the corresponding list of probabilities aligned to this support. All floating-point outputs must be rounded to six decimal places and expressed as decimals (not fractions and not using the percentage sign).\n\nImplement this as a complete, runnable program that produces results for the following test suite:\n- Test case $1$ (balanced, moderate difference): group $A$ measurements $[1.24, 1.37, 1.51, 1.63]$, group $B$ measurements $[1.10, 1.21, 1.28, 1.30]$.\n- Test case $2$ (boundary, smallest nontrivial sample): group $A$ measurements $[2.80]$, group $B$ measurements $[2.10]$.\n- Test case $3$ (unequal group sizes): group $A$ measurements $[0.95, 1.02, 1.08]$, group $B$ measurements $[0.89, 0.92, 1.00, 1.05, 1.10]$.\n- Test case $4$ (balanced with repeated values): group $A$ measurements $[3.20, 3.20, 3.40]$, group $B$ measurements $[3.10, 3.20, 3.30]$.\n- Test case $5$ (degenerate equal values): group $A$ measurements $[1.50, 1.50]$, group $B$ measurements $[1.50, 1.50]$.\n\nYour program should produce a single line of output containing the results for all five test cases as a comma-separated list enclosed in square brackets. Each test case result must itself be a list of the form $[p,\\;S,\\;P]$, where $p$ is the two-sided $p$-value, $S$ is the sorted list of unique support points of the exact null distribution, and $P$ is the list of probabilities aligned to $S$. For example, your output must have the shape $[[p_1,S_1,P_1],[p_2,S_2,P_2],[p_3,S_3,P_3],[p_4,S_4,P_4],[p_5,S_5,P_5]]$ with all floats rounded to six decimals and expressed as decimals.\n\nNo physical units or angle units apply. All probabilities and $p$-values must be decimals (not fractions and not percentages).", "solution": "### Problem Validation\n\nThe problem statement is evaluated against the validation criteria:\n\n1.  **Scientifically Grounded**: The problem is fundamentally sound. The permutation test, based on the randomization principle and exchangeability, is a cornerstone of non-parametric statistics, established by R.A. Fisher. It is a standard, valid, and powerful method for hypothesis testing, especially for small samples where asymptotic approximations may fail. The test statistic, the difference in means, is a common and appropriate choice for comparing central tendencies. The context of systems biomedicine is appropriate.\n2.  **Well-Posed**: The problem is well-posed. It provides all necessary information: the datasets, the precise definition of the test statistic, the method for constructing the null distribution (complete enumeration), and the definition of the $p$-value. The sample sizes are small enough that complete enumeration of all labelings is computationally feasible. The required outputs are unambiguously defined.\n3.  **Objective**: The language is entirely objective, mathematical, and free of subjective or opinion-based statements.\n4.  **Complete and Consistent**: The setup is complete and internally consistent. There are no missing data or contradictory constraints. The specified test cases cover a range of scenarios (balanced, unbalanced, repeated values, degenerate cases) that allow for a thorough validation of the implemented algorithm.\n5.  **Realistic and Feasible**: The numerical values for the measurements are realistic for biological data (e.g., log-transformed abundances). The task of enumerating all permutations for the given small sample sizes is computationally feasible. For instance, the largest case is Test Case 3 with $n_A=3, n_B=5$, leading to $N=8$. The number of permutations is $\\binom{8}{3} = 56$, which is trivial for a computer to handle.\n\nThe problem statement is **valid**. It is scientifically sound, well-posed, and provides a clear, computable task based on established statistical principles. I will now proceed with the solution.\n\n### Solution\n\nThe task is to implement an exact permutation test for the difference in means between two samples, group $A$ and group $B$. The core of the solution rests on the randomization principle, which posits that under the null hypothesis ($H_0$) of no difference between the groups, the labels 'A' and 'B' are arbitrary. Thus, any partition of the pooled data into new groups of the original sizes $n_A$ and $n_B$ is equally likely.\n\nLet the observed data be $X_A = \\{x_{A,1}, \\dots, x_{A,n_A}\\}$ and $X_B = \\{x_{B,1}, \\dots, x_{B,n_B}\\}$. The total number of observations is $N = n_A + n_B$. The pooled dataset is $X_{pool} = X_A \\cup X_B$.\n\n**1. Observed Test Statistic**\n\nThe test statistic is the difference in the sample means, $T = \\bar{X}_A - \\bar{X}_B$. We first compute this value for the original, observed labeling. This is denoted as $T_{\\text{obs}}$.\n$$\nT_{\\text{obs}} = \\frac{1}{n_A} \\sum_{i=1}^{n_A} x_{A,i} - \\frac{1}{n_B} \\sum_{j=1}^{n_B} x_{B,j}\n$$\n\n**2. Constructing the Exact Null Distribution**\n\nUnder $H_0$, all possible ways of assigning $n_A$ labels of 'A' and $n_B$ labels of 'B' to the $N$ observations in $X_{pool}$ are equally probable. The total number of such unique assignments, or permutations of labels, is given by the binomial coefficient:\n$$\nC_{total} = \\binom{N}{n_A} = \\frac{N!}{n_A! (N-n_A)!} = \\frac{N!}{n_A! n_B!}\n$$\nThe algorithm proceeds by enumerating every possible way to form a new group $A'$ of size $n_A$ from the pooled data $X_{pool}$. The remaining $n_B$ elements automatically form the complementary group $B'$. For each such permutation $k = 1, \\dots, C_{total}$, we calculate the test statistic $T_k'$:\n$$\nT_k' = \\bar{X}_{A',k} - \\bar{X}_{B',k}\n$$\nThe collection of these $C_{total}$ values $\\{T_1', T_2', \\dots, T_{C_{total}}'\\}$ constitutes the exact null distribution of the test statistic $T$.\n\nA computational note: let $S_{pool} = \\sum_{i=1}^{N} x_i$ be the sum of all pooled observations. For any permutation, if the sum of elements in group $A'$ is $S_{A'}$, then the sum for group $B'$ is $S_{B'} = S_{pool} - S_{A'}$. The statistic $T'$ can be written as:\n$$\nT' = \\frac{S_{A'}}{n_A} - \\frac{S_{pool} - S_{A'}}{n_B} = S_{A'} \\left( \\frac{1}{n_A} + \\frac{1}{n_B} \\right) - \\frac{S_{pool}}{n_B}\n$$\nThis shows that $T'$ is a monotonic linear function of $S_{A'}$. The distribution can be derived by first finding the distribution of the sums of all $n_A$-element subsets, and then transforming those sums to the corresponding difference-in-means values. In implementation, direct calculation of means is clear and efficient enough for the given constraints.\n\n**3. Computing Support and Probabilities**\n\nThe multiset of $T'$ values is processed to find the support of the null distribution, $S$, which is the set of unique values $\\{t_1, t_2, \\dots, t_m\\}$. We sort these values in ascending order. For each unique value $t_j \\in S$, we find its frequency, $f_j$, in the multiset of all $T'$ values. The probability of observing $t_j$ under the null hypothesis is:\n$$\nP(T = t_j) = \\frac{f_j}{C_{total}}\n$$\nThis yields the list of probabilities $P = \\{P(T=t_1), \\dots, P(T=t_m)\\}$ aligned with the sorted support $S$.\n\n**4. Two-Sided $p$-value Calculation**\n\nThe two-sided $p$-value is the probability, under the null distribution, of obtaining a test statistic at least as extreme as the one observed. \"As extreme\" is measured by the absolute value. The $p$-value is calculated as:\n$$\np = P(|T| \\ge |T_{\\text{obs}}|) = \\frac{\\text{Number of permutations } k \\text{ where } |T_k'| \\ge |T_{\\text{obs}}|}{C_{total}}\n$$\nDue to potential floating-point inaccuracies, the comparison $|T_k'| \\ge |T_{\\text{obs}}|$ should be implemented carefully, for instance, by comparing up to a high degree of precision, e.g., $|T_k'| - |T_{\\text{obs}}| \\ge -\\epsilon$ for a small $\\epsilon0$. Given that the values are derived from rational arithmetic, direct comparison after rounding to a safe number of decimal places (e.g., 12) is a robust strategy.\n\nThe algorithm to be implemented will perform these steps for each test case provided. The use of `itertools.combinations` in Python is ideal for enumerating the partitions of the pooled data.", "answer": "[[0.057143,[-0.222500,-0.167500,-0.145000,-0.112500,-0.090000,-0.057500,-0.035000,0.035000,0.057500,0.090000,0.112500,0.145000,0.167500,0.222500],[0.014286,0.028571,0.014286,0.057143,0.057143,0.085714,0.114286,0.114286,0.085714,0.057143,0.057143,0.014286,0.028571,0.014286]],[1.000000,[-0.700000,0.700000],[0.500000,0.500000]],[0.428571,[-0.144667,-0.128000,-0.106667,-0.090000,-0.081333,-0.064667,-0.056000,-0.048000,-0.040000,-0.023333,-0.014667,-0.006667,0.006667,0.014667,0.023333,0.040000,0.048000,0.056000,0.064667,0.081333,0.090000,0.106667,0.128000,0.144667],[0.017857,0.017857,0.017857,0.017857,0.035714,0.035714,0.017857,0.035714,0.017857,0.035714,0.053571,0.035714,0.035714,0.053571,0.035714,0.017857,0.035714,0.017857,0.035714,0.035714,0.017857,0.017857,0.017857,0.017857]],[0.300000,[-0.166667,-0.100000,-0.033333,0.033333,0.100000,0.166667],[0.100000,0.200000,0.200000,0.200000,0.200000,0.100000]],[1.000000,[0.000000],[1.000000]]]", "id": "4387115"}]}