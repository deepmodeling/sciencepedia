{"hands_on_practices": [{"introduction": "Understanding the statistical properties of biological counts is a cornerstone of quantitative analysis. This first practice focuses on deriving these properties from first principles, using the Poisson distribution as a model for discrete events like cellular divisions or vascular branches. By deriving the moment generating function ($M_X(t)$) and using it to find the distribution's moments, you will reinforce your understanding of how a distribution's core characteristics are mathematically encoded and extracted [@problem_id:4378385].", "problem": "A tissue-engineered construct is imaged to quantify microvasculature branching events within a standardized field-of-view. Under the assumptions that branching events occur independently and sparsely, the count of observed branches in a single field-of-view is modeled by a Poisson random variable $X$ with parameter $\\lambda0$, where $\\lambda$ is the expected number of branches per field-of-view. Starting from the definition of the probability mass function (PMF) of the Poisson distribution and the definition of the moment generating function (MGF), derive an explicit closed-form expression for the MGF $M_{X}(t)$ of $X$. Then, using only the definition of moments in terms of derivatives of the MGF evaluated at $t=0$, compute the first three raw moments $\\mathbb{E}[X]$, $\\mathbb{E}[X^{2}]$, and $\\mathbb{E}[X^{3}]$ in symbolic form.\n\nYour derivation must begin from the following foundational bases:\n- The PMF of a Poisson random variable: for $x\\in\\{0,1,2,\\dots\\}$, $\\mathbb{P}(X=x)$ depends on $\\lambda$.\n- The definition of the moment generating function (MGF): for any real $t$, $M_{X}(t)=\\mathbb{E}[\\exp(tX)]$.\n\nExpress your final results symbolically in terms of $\\lambda$ and $t$; do not introduce any numerical values. No rounding is required. Provide the MGF and the three raw moments.", "solution": "The problem statement is subjected to validation according to the specified criteria.\n\n**Problem Validation**\n\n**Step 1: Extracted Givens**\n- A random variable $X$ represents the count of microvasculature branching events.\n- $X$ follows a Poisson distribution with parameter $\\lambda  0$.\n- $\\lambda$ is the expected number of branches per field-of-view.\n- The probability mass function (PMF) for $X$ is given for $x \\in \\{0, 1, 2, \\dots\\}$, defined as $\\mathbb{P}(X=x)$.\n- The moment generating function (MGF) is defined as $M_{X}(t) = \\mathbb{E}[\\exp(tX)]$.\n- The task is to derive the MGF, $M_{X}(t)$, in closed form.\n- Subsequently, the first three raw moments, $\\mathbb{E}[X]$, $\\mathbb{E}[X^2]$, and $\\mathbb{E}[X^3]$, must be computed using only the MGF and its derivatives.\n- Results must be symbolic in terms of $\\lambda$ and $t$.\n\n**Step 2: Validation Using Extracted Givens**\nThe problem is assessed as valid.\n- **Scientifically Grounded:** The Poisson distribution is the standard and appropriate model for counting rare, independent events in a fixed domain, such as the described biological context. The problem is based on fundamental principles of probability theory.\n- **Well-Posed:** The problem is clearly formulated. It requests the derivation of standard statistical properties (MGF and moments) for a well-defined probability distribution (Poisson). A unique and meaningful solution exists.\n- **Objective:** The problem is expressed using precise, standard mathematical and statistical terminology, free of any subjectivity or ambiguity.\n- **Completeness and Consistency:** All necessary definitions and constraints are provided. The reference to the \"definition of the PMF of the Poisson distribution\" is unambiguous, as this function is universally defined. There are no contradictions.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Derivation of the Moment Generating Function (MGF)**\n\nThe analysis begins with the foundational definitions provided. For a discrete random variable $X$, the expected value of a function of $X$, say $g(X)$, is given by $\\mathbb{E}[g(X)] = \\sum_{x} g(x) \\mathbb{P}(X=x)$, where the sum is over all possible values $x$ that $X$ can take. The random variable $X$ follows a Poisson distribution with parameter $\\lambda$, so its support is the set of non-negative integers $\\{0, 1, 2, \\dots\\}$, and its probability mass function (PMF) is given by:\n$$\n\\mathbb{P}(X=x) = \\frac{\\lambda^{x} \\exp(-\\lambda)}{x!}, \\quad \\text{for } x = 0, 1, 2, \\dots\n$$\nThe moment generating function (MGF) is defined as $M_{X}(t) = \\mathbb{E}[\\exp(tX)]$. Applying the expectation formula with $g(X) = \\exp(tX)$, we have:\n$$\nM_{X}(t) = \\sum_{x=0}^{\\infty} \\exp(tx) \\mathbb{P}(X=x)\n$$\nSubstituting the PMF of the Poisson distribution into this definition yields:\n$$\nM_{X}(t) = \\sum_{x=0}^{\\infty} \\exp(tx) \\frac{\\lambda^{x} \\exp(-\\lambda)}{x!}\n$$\nThe term $\\exp(-\\lambda)$ does not depend on the summation index $x$ and can be factored out of the summation:\n$$\nM_{X}(t) = \\exp(-\\lambda) \\sum_{x=0}^{\\infty} \\frac{\\exp(tx) \\lambda^{x}}{x!}\n$$\nWe can combine the terms raised to the power of $x$: $\\exp(tx) \\lambda^{x} = (\\lambda \\exp(t))^{x}$.\n$$\nM_{X}(t) = \\exp(-\\lambda) \\sum_{x=0}^{\\infty} \\frac{(\\lambda \\exp(t))^{x}}{x!}\n$$\nThe summation is recognized as the Maclaurin series expansion for the exponential function, $\\exp(z) = \\sum_{k=0}^{\\infty} \\frac{z^{k}}{k!}$. In this expression, $z = \\lambda \\exp(t)$. Therefore, the sum evaluates to $\\exp(\\lambda \\exp(t))$.\nSubstituting this result back, we obtain the closed-form expression for the MGF:\n$$\nM_{X}(t) = \\exp(-\\lambda) \\exp(\\lambda \\exp(t)) = \\exp(-\\lambda + \\lambda \\exp(t))\n$$\nFactoring out $\\lambda$ in the exponent gives the final, standard form for the MGF of a Poisson random variable:\n$$\nM_{X}(t) = \\exp(\\lambda(\\exp(t) - 1))\n$$\nThis derivation fulfills the first part of the problem.\n\n**Computation of an Explicit Closed-Form Expression for Raw Moments**\n\nThe $n$-th raw moment of $X$, denoted $\\mathbb{E}[X^n]$, is obtained by computing the $n$-th derivative of the MGF with respect to $t$ and evaluating it at $t=0$.\n$$\n\\mathbb{E}[X^n] = \\frac{d^n}{dt^n} M_{X}(t) \\bigg|_{t=0}\n$$\nWe will now compute the first three raw moments using this property.\n\n**First Raw Moment: $\\mathbb{E}[X]$**\nThe first moment is the first derivative of $M_X(t)$ evaluated at $t=0$.\n$$\nM'_{X}(t) = \\frac{d}{dt} \\exp(\\lambda(\\exp(t) - 1))\n$$\nUsing the chain rule, where the outer function is $\\exp(u)$ and the inner function is $u(t) = \\lambda(\\exp(t) - 1)$, we have $u'(t)=\\lambda\\exp(t)$.\n$$\nM'_{X}(t) = \\exp(\\lambda(\\exp(t) - 1)) \\cdot (\\lambda \\exp(t)) = \\lambda \\exp(t) M_{X}(t)\n$$\nEvaluating at $t=0$:\n$$\n\\mathbb{E}[X] = M'_{X}(0) = \\lambda \\exp(0) M_{X}(0) = \\lambda \\cdot 1 \\cdot \\exp(\\lambda(\\exp(0) - 1)) = \\lambda \\cdot \\exp(\\lambda(1-1)) = \\lambda \\exp(0) = \\lambda\n$$\nThus, the first raw moment is $\\mathbb{E}[X] = \\lambda$.\n\n**Second Raw Moment: $\\mathbb{E}[X^2]$**\nThe second moment is the second derivative of $M_X(t)$ evaluated at $t=0$. We differentiate $M'_{X}(t) = \\lambda \\exp(t) M_{X}(t)$ using the product rule.\n$$\nM''_{X}(t) = \\frac{d}{dt} (\\lambda \\exp(t) M_{X}(t)) = \\left(\\frac{d}{dt} (\\lambda \\exp(t))\\right) M_{X}(t) + (\\lambda \\exp(t)) \\left(\\frac{d}{dt} M_{X}(t)\\right)\n$$\n$$\nM''_{X}(t) = (\\lambda \\exp(t)) M_{X}(t) + (\\lambda \\exp(t)) M'_{X}(t)\n$$\nSubstituting $M'_{X}(t) = \\lambda \\exp(t) M_{X}(t)$:\n$$\nM''_{X}(t) = \\lambda \\exp(t) M_{X}(t) + (\\lambda \\exp(t))(\\lambda \\exp(t) M_{X}(t)) = M_{X}(t)[\\lambda \\exp(t) + \\lambda^2 \\exp(2t)]\n$$\nEvaluating at $t=0$:\n$$\n\\mathbb{E}[X^2] = M''_{X}(0) = M_{X}(0)[\\lambda \\exp(0) + \\lambda^2 \\exp(2 \\cdot 0)] = 1 \\cdot [\\lambda \\cdot 1 + \\lambda^2 \\cdot 1] = \\lambda + \\lambda^2\n$$\nThus, the second raw moment is $\\mathbb{E}[X^2] = \\lambda^2 + \\lambda$.\n\n**Third Raw Moment: $\\mathbb{E}[X^3]$**\nThe third moment is the third derivative of $M_X(t)$ evaluated at $t=0$. We differentiate $M''_{X}(t) = M_{X}(t)[\\lambda \\exp(t) + \\lambda^2 \\exp(2t)]$ using the product rule.\n$$\nM'''_{X}(t) = \\frac{d}{dt} \\left(M_{X}(t)[\\lambda \\exp(t) + \\lambda^2 \\exp(2t)]\\right)\n$$\n$$\nM'''_{X}(t) = M'_{X}(t)[\\lambda \\exp(t) + \\lambda^2 \\exp(2t)] + M_{X}(t)[\\lambda \\exp(t) + 2\\lambda^2 \\exp(2t)]\n$$\nEvaluating at $t=0$, and using the previously computed values $M_X(0)=1$ and $M'_X(0)=\\lambda$:\n$$\n\\mathbb{E}[X^3] = M'''_{X}(0) = M'_{X}(0)[\\lambda \\exp(0) + \\lambda^2 \\exp(0)] + M_X(0)[\\lambda \\exp(0) + 2\\lambda^2 \\exp(0)]\n$$\n$$\n\\mathbb{E}[X^3] = \\lambda[\\lambda + \\lambda^2] + 1[\\lambda + 2\\lambda^2] = \\lambda^2 + \\lambda^3 + \\lambda + 2\\lambda^2\n$$\nCombining terms, we find the third raw moment:\n$$\n\\mathbb{E}[X^3] = \\lambda^3 + 3\\lambda^2 + \\lambda\n$$\nThis completes the required derivations. The expressions for the MGF and the first three raw moments are provided in the final answer.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\exp(\\lambda(\\exp(t) - 1))  \\lambda  \\lambda^2 + \\lambda  \\lambda^3 + 3\\lambda^2 + \\lambda\n\\end{pmatrix}\n}\n$$", "id": "4378385"}, {"introduction": "A central task in biomedicine is to generalize findings from a patient sample to a broader population, which requires quantifying statistical uncertainty. This exercise tackles this challenge by asking you to construct a confidence interval for a population mean, applying the powerful Central Limit Theorem to handle unknown distributions in large samples [@problem_id:4378418]. You will contrast this asymptotic approach with an exact interval derived under normality assumptions, providing deep insight into the foundations and practical trade-offs of statistical inference.", "problem": "A systems biomedicine team is characterizing the plasma interleukin-6 concentration as a systemic inflammation biomarker across a cohort of patients. Let $\\{X_{i}\\}_{i=1}^{n}$ denote the biomarker measurements from $n$ distinct patients, modeled as independent and identically distributed (i.i.d.) draws from an unknown distribution with population mean $\\mu$ and finite variance $\\sigma^{2}$. You are given $n=64$ patient samples with sample mean $\\bar{X}=155$ and sample standard deviation $S=42$, measured in picograms per milliliter (pg/mL).\n\nStarting from the Central Limit Theorem (CLT) and the definitions of the sample mean and sample variance, and invoking Slutsky's theorem to justify replacing the unknown $\\sigma$ by the sample standard deviation $S$ (Studentization), derive an asymptotic two-sided $(1-\\alpha)$ confidence interval for $\\mu$ that accounts for the unknown variance. Then specialize to $\\alpha=0.05$ and compute the numeric endpoints for this CLT-with-Studentization interval.\n\nAdditionally, if you assume the biomarker measurements are exactly Gaussian, derive the exact two-sided $(1-\\alpha)$ confidence interval for $\\mu$ based on the Studentâ€™s $t$ distribution with $n-1$ degrees of freedom, and compute the numeric endpoints for $\\alpha=0.05$.\n\nRound each endpoint to four significant figures. Express the interval endpoints in pg/mL. Return the four endpoint values in the order: CLT-Studentized lower, CLT-Studentized upper, $t$-exact lower, $t$-exact upper.", "solution": "The problem is first validated against the required criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Biomarker measurements $\\{X_{i}\\}_{i=1}^{n}$ are independent and identically distributed (i.i.d.).\n- Population mean: $\\mu$ (unknown).\n- Population variance: $\\sigma^2$ (finite and unknown).\n- Sample size: $n=64$.\n- Sample mean: $\\bar{X}=155$ pg/mL.\n- Sample standard deviation: $S=42$ pg/mL.\n- Significance level: $\\alpha=0.05$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem describes a standard biostatistical task: estimating a population mean from sample data using confidence intervals. This is a fundamental and realistic application of probability theory in biomedicine. The given values are plausible for a biological measurement.\n- **Well-Posed**: The problem is well-posed. It asks for the derivation and calculation of two distinct types of confidence intervals under clearly stated assumptions (one asymptotic, one with an exact distributional assumption). The necessary data are all provided.\n- **Objective**: The problem is stated in precise, objective language without ambiguity or subjective claims.\n- **Conclusion**: The problem is free of scientific unsoundness, incompleteness, contradictions, or other flaws. It is a valid, well-defined problem in applied statistics.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution will now be derived.\n\n### Part 1: Asymptotic Confidence Interval via CLT and Slutsky's Theorem\n\nThe goal is to derive a two-sided $(1-\\alpha)$ confidence interval for the population mean $\\mu$ without assuming the data follows a normal distribution. We are given a sample of size $n=64$ with sample mean $\\bar{X}=155$ and sample standard deviation $S=42$.\n\nThe Central Limit Theorem (CLT) states that for a sequence of i.i.d. random variables $\\{X_i\\}_{i=1}^n$ with a finite mean $\\mu$ and a finite, non-zero variance $\\sigma^2$, the standardized sample mean converges in distribution to a standard normal distribution as $n \\to \\infty$. Mathematically,\n$$ \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\xrightarrow{d} Z \\sim N(0, 1) $$\nwhere $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$ is the sample mean.\n\nIn this problem, the population standard deviation $\\sigma$ is unknown. We estimate it using the sample standard deviation $S$, where the sample variance $S^2$ is defined as $S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2$. The sample variance $S^2$ is a consistent estimator for the population variance $\\sigma^2$, meaning $S^2$ converges in probability to $\\sigma^2$ as $n \\to \\infty$. This is written as $S^2 \\xrightarrow{p} \\sigma^2$. By the continuous mapping theorem, it follows that $S \\xrightarrow{p} \\sigma$.\n\nWe now construct the Studentized statistic, which replaces the unknown $\\sigma$ with its estimate $S$:\n$$ T_n = \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} $$\nThis can be rewritten as:\n$$ T_n = \\left(\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}}\\right) \\left(\\frac{\\sigma}{S}\\right) $$\nWe have two components:\n1. $Y_n = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}}$, which converges in distribution to $Z \\sim N(0, 1)$ by the CLT.\n2. $Z_n = \\frac{S}{\\sigma}$, which converges in probability to $1$ because $S$ is a consistent estimator for $\\sigma$. Therefore, its reciprocal $\\frac{\\sigma}{S}$ also converges in probability to $1$.\n\nSlutsky's theorem states that if $Y_n \\xrightarrow{d} Y$ and $Z_n \\xrightarrow{p} c$ (a constant), then $Y_n Z_n^{-1} \\xrightarrow{d} Y/c$. Applying this to our case, where $Y_n = \\sqrt{n}(\\bar{X}-\\mu)$ and $Z_n=S$, we find that the Studentized statistic $T_n$ converges in distribution to a standard normal random variable:\n$$ T_n = \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} \\xrightarrow{d} Z \\sim N(0, 1) $$\nFor a large sample size $n$, we can therefore approximate the distribution of $T_n$ by the standard normal distribution. This allows us to write the following probability statement for a $(1-\\alpha)$ confidence level:\n$$ P\\left(-z_{\\alpha/2} \\le \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} \\le z_{\\alpha/2}\\right) \\approx 1-\\alpha $$\nwhere $z_{\\alpha/2}$ is the critical value from the standard normal distribution such that $P(Z  z_{\\alpha/2}) = \\alpha/2$.\n\nRearranging the inequality to isolate $\\mu$:\n$$ \\bar{X} - z_{\\alpha/2} \\frac{S}{\\sqrt{n}} \\le \\mu \\le \\bar{X} + z_{\\alpha/2} \\frac{S}{\\sqrt{n}} $$\nThis gives the asymptotic two-sided $(1-\\alpha)$ confidence interval for $\\mu$:\n$$ \\left[ \\bar{X} - z_{\\alpha/2} \\frac{S}{\\sqrt{n}}, \\bar{X} + z_{\\alpha/2} \\frac{S}{\\sqrt{n}} \\right] $$\nFor this problem, we have $n=64$, $\\bar{X}=155$, $S=42$, and $\\alpha=0.05$. This means $\\alpha/2=0.025$. The corresponding critical value is $z_{0.025} \\approx 1.9600$.\nThe standard error of the mean is $SE = \\frac{S}{\\sqrt{n}} = \\frac{42}{\\sqrt{64}} = \\frac{42}{8} = 5.25$.\nThe margin of error is $ME_{CLT} = z_{0.025} \\times SE \\approx 1.9600 \\times 5.25 = 10.29$.\nThe lower endpoint is $\\bar{X} - ME_{CLT} = 155 - 10.29 = 144.71$.\nThe upper endpoint is $\\bar{X} + ME_{CLT} = 155 + 10.29 = 165.29$.\nRounding to four significant figures, the lower endpoint is $144.7$ pg/mL and the upper endpoint is $165.3$ pg/mL.\n\n### Part 2: Exact Confidence Interval assuming Normality (Student's t-distribution)\n\nNow, we add the assumption that the measurements $\\{X_i\\}$ are drawn from a normal distribution, i.e., $X_i \\sim N(\\mu, \\sigma^2)$. Under this assumption, the pivot quantity\n$$ \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} $$\nfollows an exact Student's t-distribution with $n-1$ degrees of freedom.\nThe probability statement for a $(1-\\alpha)$ confidence interval is:\n$$ P\\left(-t_{n-1, \\alpha/2} \\le \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} \\le t_{n-1, \\alpha/2}\\right) = 1-\\alpha $$\nwhere $t_{n-1, \\alpha/2}$ is the critical value from the Student's t-distribution with $n-1$ degrees of freedom such that the area in the upper tail is $\\alpha/2$.\n\nRearranging for $\\mu$ yields the exact $(1-\\alpha)$ confidence interval for $\\mu$:\n$$ \\left[ \\bar{X} - t_{n-1, \\alpha/2} \\frac{S}{\\sqrt{n}}, \\bar{X} + t_{n-1, \\alpha/2} \\frac{S}{\\sqrt{n}} \\right] $$\nFor this problem, $\\alpha=0.05$ and the degrees of freedom are $df = n-1 = 64-1=63$. We need the critical value $t_{63, 0.025}$. Using statistical tables or software, $t_{63, 0.025} \\approx 1.9983$.\nThe standard error is the same as before: $SE = \\frac{S}{\\sqrt{n}} = 5.25$.\nThe margin of error is $ME_t = t_{63, 0.025} \\times SE \\approx 1.9983 \\times 5.25 \\approx 10.4911$.\nThe lower endpoint is $\\bar{X} - ME_t = 155 - 10.4911 = 144.5089$.\nThe upper endpoint is $\\bar{X} + ME_t = 155 + 10.4911 = 165.4911$.\nRounding to four significant figures, the lower endpoint is $144.5$ pg/mL and the upper endpoint is $165.5$ pg/mL.\n\nThe four requested endpoints are:\n1. CLT-Studentized lower: $144.7$\n2. CLT-Studentized upper: $165.3$\n3. $t$-exact lower: $144.5$\n4. $t$-exact upper: $165.5$\n\nThese values will be presented in the final answer as a row matrix. Note that for a relatively large sample size like $n=64$, the t-distribution with $63$ degrees of freedom is very close to the standard normal distribution, so the respective intervals are quite similar.", "answer": "$$\\boxed{\\begin{pmatrix} 144.7  165.3  144.5  165.5 \\end{pmatrix}}$$", "id": "4378418"}, {"introduction": "Biomedical data often takes the form of continuous processes, such as the change in a biomarker's concentration over time. This advanced practice introduces Gaussian Process (GP) regression, a powerful Bayesian framework for modeling such functional data without being constrained by rigid parametric forms [@problem_id:4378382]. By deriving the posterior predictive equations from the fundamental properties of multivariate Gaussian distributions, you will gain a first-principles understanding of how to perform inference directly in the space of functions.", "problem": "A systems biomedicine group models the latent time course of a circulating cytokine concentration, denoted by the latent function $f(t)$ measured in arbitrary concentration units, following a controlled immune stimulus at time $t = 0$. The experimental assay produces noisy observations $y_i$ at times $t_i$, modeled as $y_i = f(t_i) + \\epsilon_i$ with independent noise $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$. The prior over $f$ is a Gaussian process (GP), specifically a Gaussian process (GP) with zero mean and squared exponential kernel, that is $f \\sim \\mathcal{GP}(0, k)$ with\n$$\nk(t, t') = \\sigma_f^2 \\exp\\!\\left(-\\frac{(t - t')^2}{2 \\ell^2}\\right).\n$$\nYou may assume the following widely accepted foundational facts as the starting point: (i) a Gaussian process implies that any finite collection of function values is jointly multivariate normal with covariance given by the kernel; (ii) conditioning a jointly multivariate normal vector on an observed subset yields a multivariate normal distribution whose mean and covariance are given by the standard block matrix conditioning rules.\n\nPart A. Using only these foundations and without invoking any pre-stated Gaussian process regression formulas, derive from first principles the posterior mean function $m_{\\text{post}}(t)$ and posterior covariance function $k_{\\text{post}}(t, t')$ of the latent function $f(t)$ given noisy observations $\\{(t_i, y_i)\\}_{i=1}^n$ with independent Gaussian noise of variance $\\sigma_n^2$.\n\nPart B. Consider a specific immuno-monitoring experiment with $n = 2$ observation times $t_1 = 0$ and $t_2 = 2$ days after stimulus. The hyperparameters are $\\sigma_f^2 = 1$ (in squared concentration units) and $\\ell = 1$ day. The observation noise variance is $\\sigma_n^2 = 0.1$ (in squared concentration units). Compute the predictive variance of the latent cytokine concentration at a new time $t_\\star = 1$ day, that is $\\operatorname{Var}[f(t_\\star) \\mid \\{(t_i, y_i)\\}_{i=1}^{2}]$, in squared concentration units. Round your answer to four significant figures. Express the final predictive variance in squared concentration units.", "solution": "The problem is found to be valid as it is scientifically grounded, mathematically well-posed, objective, and contains all necessary information for a unique solution.\n\nPart A: Derivation of the Posterior Mean and Covariance Functions\n\nThe problem requires deriving the posterior distribution of the latent function $f(t)$ given a set of noisy observations $\\mathbf{y} = \\{y_i\\}_{i=1}^n$ at times $\\mathbf{t} = \\{t_i\\}_{i=1}^n$. The model is specified as:\n1.  Prior on the latent function: $f \\sim \\mathcal{GP}(0, k)$, where $k(t, t') = \\sigma_f^2 \\exp\\left(-\\frac{(t - t')^2}{2 \\ell^2}\\right)$.\n2.  Likelihood of observations: $y_i = f(t_i) + \\epsilon_i$, where the noise terms $\\epsilon_i$ are independent and identically distributed as $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$.\n\nWe proceed from first principles, using the foundational fact that a Gaussian process implies that any finite set of function values is jointly multivariate normal.\n\nLet $\\mathbf{f} = (f(t_1), \\dots, f(t_n))^T$ be the vector of latent function values at the observation times $\\mathbf{t}$. Let $\\mathbf{y} = (y_1, \\dots, y_n)^T$ be the vector of observations. The observation model can be written in vector form as $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\epsilon}$, where $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 I_n)$, with $I_n$ being the $n \\times n$ identity matrix.\n\nOur goal is to find the posterior distribution of the latent function $f$ given the data $(\\mathbf{t}, \\mathbf{y})$. Since the prior is a GP, the posterior is also a GP. A GP is fully specified by its mean and covariance functions. To find these, we determine the predictive distribution for arbitrary test points. Let us consider two arbitrary test points, $t$ and $t'$, and let $f(t)$ and $f(t')$ be the corresponding latent function values.\n\nFirst, we establish the joint distribution of the observations $\\mathbf{y}$ and the latent values $f(t)$ and $f(t')$. According to the GP prior, the collection of random variables $(\\mathbf{f}^T, f(t), f(t'))^T$ is jointly Gaussian with zero mean and a block covariance matrix constructed from the kernel $k$:\n$$\n\\begin{pmatrix} \\mathbf{f} \\\\ f(t) \\\\ f(t') \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K  \\mathbf{k}(t)  \\mathbf{k}(t') \\\\ \\mathbf{k}(t)^T  k(t,t)  k(t,t') \\\\ \\mathbf{k}(t')^T  k(t',t)  k(t',t') \\end{pmatrix} \\right)\n$$\nwhere $K$ is the $n \\times n$ covariance matrix of the training points, with entries $K_{ij} = k(t_i, t_j)$, and $\\mathbf{k}(t)$ is an $n \\times 1$ column vector with entries $(\\mathbf{k}(t))_i = k(t_i, t)$.\n\nThe observations $\\mathbf{y}$ are a linear transformation of $\\mathbf{f}$ with added independent Gaussian noise. The joint distribution of $(\\mathbf{y}^T, f(t), f(t'))^T$ is therefore also Gaussian. Its mean is $E[(\\mathbf{y}^T, f(t), f(t'))^T] = \\mathbf{0}$. The covariance structure is derived as follows:\n- $\\operatorname{Cov}(\\mathbf{y}) = \\operatorname{Cov}(\\mathbf{f} + \\boldsymbol{\\epsilon}) = \\operatorname{Cov}(\\mathbf{f}) + \\operatorname{Cov}(\\boldsymbol{\\epsilon}) = K + \\sigma_n^2 I_n$, since $\\mathbf{f}$ and $\\boldsymbol{\\epsilon}$ are independent.\n- $\\operatorname{Cov}(\\mathbf{y}, f(t)) = \\operatorname{Cov}(\\mathbf{f} + \\boldsymbol{\\epsilon}, f(t)) = \\operatorname{Cov}(\\mathbf{f}, f(t)) + \\operatorname{Cov}(\\boldsymbol{\\epsilon}, f(t)) = \\mathbf{k}(t)$, since $\\boldsymbol{\\epsilon}$ and $f$ are independent.\n- Similarly, $\\operatorname{Cov}(\\mathbf{y}, f(t')) = \\mathbf{k}(t')$.\n\nThus, the joint distribution of the observations and the values at the test points is:\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f(t) \\\\ f(t') \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K + \\sigma_n^2 I_n  \\mathbf{k}(t)  \\mathbf{k}(t') \\\\ \\mathbf{k}(t)^T  k(t,t)  k(t,t') \\\\ \\mathbf{k}(t')^T  k(t',t)  k(t',t') \\end{pmatrix} \\right)\n$$\nWe wish to find the conditional distribution of $(f(t), f(t'))^T$ given $\\mathbf{y}$. We use the standard formulas for conditioning a multivariate Gaussian distribution. For a general joint Gaussian vector partitioned as $(\\mathbf{x}_1^T, \\mathbf{x}_2^T)^T$ with mean $(\\boldsymbol{\\mu}_1^T, \\boldsymbol{\\mu}_2^T)^T$ and block covariance $\\begin{pmatrix} \\Sigma_{11}  \\Sigma_{12} \\\\ \\Sigma_{21}  \\Sigma_{22} \\end{pmatrix}$, the conditional distribution of $\\mathbf{x}_2$ given $\\mathbf{x}_1$ is Gaussian with:\n- Mean: $\\boldsymbol{\\mu}_{2|1} = \\boldsymbol{\\mu}_2 + \\Sigma_{21} \\Sigma_{11}^{-1} (\\mathbf{x}_1 - \\boldsymbol{\\mu}_1)$\n- Covariance: $\\Sigma_{22|1} = \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12}$\n\nIn our case, we identify:\n- $\\mathbf{x}_1 \\equiv \\mathbf{y}$ and $\\mathbf{x}_2 \\equiv (f(t), f(t'))^T$.\n- All prior means are zero: $\\boldsymbol{\\mu}_1 = \\mathbf{0}$, $\\boldsymbol{\\mu}_2 = \\mathbf{0}$.\n- The covariance blocks are:\n  - $\\Sigma_{11} = K + \\sigma_n^2 I_n$\n  - $\\Sigma_{12} = \\begin{pmatrix} \\mathbf{k}(t)  \\mathbf{k}(t') \\end{pmatrix}$, which implies $\\Sigma_{21} = \\Sigma_{12}^T = \\begin{pmatrix} \\mathbf{k}(t)^T \\\\ \\mathbf{k}(t')^T \\end{pmatrix}$\n  - $\\Sigma_{22} = \\begin{pmatrix} k(t,t)  k(t,t') \\\\ k(t',t)  k(t',t') \\end{pmatrix}$\n\nApplying the conditioning formulas, the posterior distribution of $(f(t), f(t'))^T$ given $\\mathbf{y}$ is Gaussian.\nThe posterior mean vector is:\n$$\nE\\left[ \\begin{pmatrix} f(t) \\\\ f(t') \\end{pmatrix} \\mid \\mathbf{y} \\right] = \\mathbf{0} + \\begin{pmatrix} \\mathbf{k}(t)^T \\\\ \\mathbf{k}(t')^T \\end{pmatrix} (K + \\sigma_n^2 I_n)^{-1} (\\mathbf{y} - \\mathbf{0}) = \\begin{pmatrix} \\mathbf{k}(t)^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{y} \\\\ \\mathbf{k}(t')^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{y} \\end{pmatrix}\n$$\nFrom this, we identify the posterior mean function $m_{\\text{post}}(t)$ as the expected value of $f(t)$ given the data:\n$$\nm_{\\text{post}}(t) = \\mathbf{k}(t)^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{y}\n$$\nThe posterior covariance matrix is:\n$$\n\\operatorname{Cov}\\left( \\begin{pmatrix} f(t) \\\\ f(t') \\end{pmatrix} \\mid \\mathbf{y} \\right) = \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12}\n$$\n$$\n= \\begin{pmatrix} k(t,t)  k(t,t') \\\\ k(t',t)  k(t',t') \\end{pmatrix} - \\begin{pmatrix} \\mathbf{k}(t)^T \\\\ \\mathbf{k}(t')^T \\end{pmatrix} (K + \\sigma_n^2 I_n)^{-1} \\begin{pmatrix} \\mathbf{k}(t)  \\mathbf{k}(t') \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} k(t,t) - \\mathbf{k}(t)^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{k}(t)  k(t,t') - \\mathbf{k}(t)^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{k}(t') \\\\ k(t',t) - \\mathbf{k}(t')^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{k}(t)  k(t',t') - \\mathbf{k}(t')^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{k}(t') \\end{pmatrix}\n$$\nThe posterior covariance function $k_{\\text{post}}(t, t')$ is the off-diagonal element of this matrix:\n$$\nk_{\\text{post}}(t, t') = k(t, t') - \\mathbf{k}(t)^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{k}(t')\n$$\n\nPart B: Calculation of Predictive Variance\n\nWe are asked to compute the predictive variance of the latent function at $t_\\star = 1$ day, given observations at $t_1 = 0$ and $t_2 = 2$. This quantity is $\\operatorname{Var}[f(t_\\star) \\mid \\mathbf{y}]$, which is given by the posterior covariance function evaluated at $(t_\\star, t_\\star)$:\n$$\n\\operatorname{Var}[f(t_\\star) \\mid \\mathbf{y}] = k_{\\text{post}}(t_\\star, t_\\star) = k(t_\\star, t_\\star) - \\mathbf{k}(t_\\star)^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{k}(t_\\star)\n$$\nThe given parameters are:\n- Hyperparameters: $\\sigma_f^2 = 1$, $\\ell = 1$ day, $\\sigma_n^2 = 0.1$ squared concentration units.\n- Observation times: $\\mathbf{t} = (t_1, t_2)^T = (0, 2)^T$.\n- Test time: $t_\\star = 1$ day.\n\nThe kernel function is $k(t, t') = \\exp\\left(-\\frac{(t - t')^2}{2}\\right)$. We compute the necessary components:\n1.  The matrix $K$:\n    $K_{11} = k(0, 0) = \\exp(0) = 1$.\n    $K_{22} = k(2, 2) = \\exp(0) = 1$.\n    $K_{12} = K_{21} = k(0, 2) = \\exp\\left(-\\frac{(0-2)^2}{2}\\right) = \\exp(-2)$.\n    So, $K = \\begin{pmatrix} 1  \\exp(-2) \\\\ \\exp(-2)  1 \\end{pmatrix}$.\n\n2.  The vector $\\mathbf{k}(t_\\star)$:\n    $(\\mathbf{k}(t_\\star))_1 = k(t_1, t_\\star) = k(0, 1) = \\exp\\left(-\\frac{(0-1)^2}{2}\\right) = \\exp(-0.5)$.\n    $(\\mathbf{k}(t_\\star))_2 = k(t_2, t_\\star) = k(2, 1) = \\exp\\left(-\\frac{(2-1)^2}{2}\\right) = \\exp(-0.5)$.\n    So, $\\mathbf{k}(t_\\star) = \\begin{pmatrix} \\exp(-0.5) \\\\ \\exp(-0.5) \\end{pmatrix}$.\n\n3.  The scalar $k(t_\\star, t_\\star)$:\n    $k(t_\\star, t_\\star) = k(1, 1) = \\exp(0) = 1$.\n\nNow, we assemble the matrix to be inverted, $C = K + \\sigma_n^2 I_2$:\n$$\nC = \\begin{pmatrix} 1  \\exp(-2) \\\\ \\exp(-2)  1 \\end{pmatrix} + 0.1 \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1.1  \\exp(-2) \\\\ \\exp(-2)  1.1 \\end{pmatrix}\n$$\nThe inverse of a $2 \\times 2$ matrix $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$ is $\\frac{1}{ad-bc}\\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$.\n$$\nC^{-1} = \\frac{1}{(1.1)^2 - (\\exp(-2))^2} \\begin{pmatrix} 1.1  -\\exp(-2) \\\\ -\\exp(-2)  1.1 \\end{pmatrix} = \\frac{1}{1.21 - \\exp(-4)} \\begin{pmatrix} 1.1  -\\exp(-2) \\\\ -\\exp(-2)  1.1 \\end{pmatrix}\n$$\nWe need to compute the quadratic form $\\mathbf{k}(t_\\star)^T C^{-1} \\mathbf{k}(t_\\star)$:\n$$\n\\mathbf{k}(t_\\star)^T C^{-1} \\mathbf{k}(t_\\star) = \\begin{pmatrix} \\exp(-0.5)  \\exp(-0.5) \\end{pmatrix} C^{-1} \\begin{pmatrix} \\exp(-0.5) \\\\ \\exp(-0.5) \\end{pmatrix}\n$$\n$$\n= \\exp(-1) \\begin{pmatrix} 1  1 \\end{pmatrix} \\frac{1}{1.21 - \\exp(-4)} \\begin{pmatrix} 1.1  -\\exp(-2) \\\\ -\\exp(-2)  1.1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n$$\n= \\frac{\\exp(-1)}{1.21 - \\exp(-4)} \\begin{pmatrix} 1.1 - \\exp(-2)  1.1 - \\exp(-2) \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n$$\n= \\frac{\\exp(-1)}{1.21 - \\exp(-4)} (1.1 - \\exp(-2) + 1.1 - \\exp(-2)) = \\frac{2 \\exp(-1) (1.1 - \\exp(-2))}{1.21 - \\exp(-4)}\n$$\nUsing the difference of squares factorization $a^2 - b^2 = (a-b)(a+b)$, where $a=1.1$ and $b=\\exp(-2)$, we have $1.21 - \\exp(-4) = (1.1 - \\exp(-2))(1.1 + \\exp(-2))$. This simplifies the expression:\n$$\n\\mathbf{k}(t_\\star)^T C^{-1} \\mathbf{k}(t_\\star) = \\frac{2 \\exp(-1) (1.1 - \\exp(-2))}{(1.1 - \\exp(-2))(1.1 + \\exp(-2))} = \\frac{2 \\exp(-1)}{1.1 + \\exp(-2)}\n$$\nFinally, the predictive variance is:\n$$\n\\operatorname{Var}[f(t_\\star) \\mid \\mathbf{y}] = 1 - \\frac{2 \\exp(-1)}{1.1 + \\exp(-2)}\n$$\nNow we compute the numerical value:\n$$\n\\operatorname{Var}[f(t_\\star) \\mid \\mathbf{y}] = 1 - \\frac{2 \\times 0.36787944...}{1.1 + 0.13533528...} = 1 - \\frac{0.73575888...}{1.23533528...} \\approx 1 - 0.59558005... = 0.40441994...\n$$\nRounding to four significant figures, the predictive variance is $0.4044$ in squared concentration units.", "answer": "$$\n\\boxed{0.4044}\n$$", "id": "4378382"}]}