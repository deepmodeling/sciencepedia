{"hands_on_practices": [{"introduction": "Effective data visualization hinges on mapping data attributes to visual properties in a way that the human brain can accurately perceive. This practice grounds this principle in psychophysics by applying Weber's law, which describes the limits of our ability to distinguish between different magnitudes of a stimulus. By deriving the minimum size ratio for nodes to be perceptibly different, you will gain a foundational skill for creating encodings that are clear and unambiguous. [@problem_id:4368328]", "problem": "In a systems biomedicine visualization, a weighted protein–protein interaction (PPI) network is displayed where node area encodes a quantitative attribute such as inferred protein activity. Consider a specific node with baseline displayed area $s$ (in square pixels). To make a second node perceptually distinguishable from the baseline node by size, the visual encoding must exceed the viewer’s just-noticeable difference (JND) threshold for area under the current display and viewing conditions.\n\nUse the foundational principle from psychophysics known as Weber’s law: the minimum detectable change in a stimulus is proportional to the baseline stimulus magnitude. Assume the proportionality constant (the Weber fraction) for area judgments under these conditions is a known positive constant $k$. Let the modified node area be $s'$ and define the size ratio $r$ by $r = s'/s$.\n\nStarting from Weber’s law and the definitions above, derive the minimum size ratio $r$ required for a just-noticeable difference in node size. Express your final answer as a closed-form analytic expression in terms of $k$. No numerical evaluation is required, and no rounding should be performed. Your derivation should make clear why the result does or does not depend on the baseline area $s$ under Weber’s law.", "solution": "The problem asks for the derivation of the minimum size ratio, $r$, required for a perceptible difference in node area, based on Weber's law of psychophysics. The validation of the problem statement confirms that it is scientifically grounded, well-posed, and objective. All provided information is consistent and sufficient for a unique solution.\n\nFirst, we formalize Weber's law as described in the problem. The law states that the minimum detectable change in a stimulus, known as the just-noticeable difference (JND), is proportional to the magnitude of the baseline stimulus. Let the baseline stimulus be $S$ and the JND be $\\Delta S_{jnd}$. Weber's law can be expressed as:\n$$\n\\Delta S_{jnd} \\propto S\n$$\nIntroducing the given proportionality constant, the Weber fraction $k$, we obtain the equation:\n$$\n\\Delta S_{jnd} = k S\n$$\n\nIn the context of this problem, the stimulus is the visual area of a node, measured in square pixels. The baseline stimulus is the area of the reference node, which is given as $s$. Therefore, we can substitute $s$ for $S$ in our formulation:\n$$\n\\Delta s_{jnd} = k s\n$$\nHere, $\\Delta s_{jnd}$ represents the just-noticeable difference in area.\n\nThe problem describes a second node with a modified area $s'$ that is to be just perceptually distinguishable from the baseline node of area $s$. For this to occur, the change in area, $\\Delta s$, must be equal to the just-noticeable difference, $\\Delta s_{jnd}$. Assuming the modified area $s'$ is greater than the baseline area $s$ to be perceived as larger, the change in area is:\n$$\n\\Delta s = s' - s\n$$\nTo find the minimum modified area $s'$ that is just noticeably different, we set this change equal to the JND:\n$$\ns' - s = \\Delta s_{jnd}\n$$\nSubstituting our expression for $\\Delta s_{jnd}$ from Weber's law, we have:\n$$\ns' - s = k s\n$$\n\nOur goal is to find the minimum size ratio, $r$, which is defined as $r = s'/s$. To do so, we first solve the above equation for $s'$:\n$$\ns' = s + k s\n$$\nFactoring out the baseline area $s$ from the right-hand side gives:\n$$\ns' = s(1 + k)\n$$\nNow, we can compute the ratio $r$ by dividing both sides of this equation by $s$:\n$$\n\\frac{s'}{s} = 1 + k\n$$\nBy definition, $r = s'/s$, so we arrive at the final expression for the minimum size ratio:\n$$\nr = 1 + k\n$$\nThis result is a closed-form analytic expression in terms of the Weber fraction $k$, as required.\n\nThe problem also asks to explain why the result does or does not depend on the baseline area $s$. As shown in the derivation, the baseline area $s$ is present in the expressions for both the change in area ($s' - s$) and the JND ($k s$). When we form the ratio $r = s'/s$, the term $s$ cancels out from the numerator and the denominator:\n$$\nr = \\frac{s(1 + k)}{s} = 1 + k\n$$\nThis cancellation demonstrates that the minimum size ratio $r$ for a just-noticeable difference is independent of the baseline area $s$. This is a fundamental consequence of Weber's law. The law posits a *proportional* change, meaning the JND is a constant *fraction* of the baseline stimulus. Therefore, the ratio of the new stimulus magnitude ($s'$) to the baseline stimulus magnitude ($s$) required to achieve this fractional change is itself a constant, dependent only on the Weber fraction $k$ and not on the absolute magnitude of the baseline stimulus $s$.", "answer": "$$\\boxed{1 + k}$$", "id": "4368328"}, {"introduction": "Often, the network structure we collect does not directly address our analytical question, requiring us to transform the data. This practice explores one such common transformation: the one-mode projection of a bipartite network, which is essential for studying relationships like ligand co-targeting of receptors. Through this exercise, you will analyze the mathematical mechanics and interpretive pitfalls of projection, learning to critically evaluate when this technique clarifies or confounds biological meaning. [@problem_id:4368322]", "problem": "A systems biomedicine team models ligand–receptor specificity with a bipartite network. The bipartite graph has a ligand set $U=\\{L_1,L_2,L_3\\}$ and a receptor set $V=\\{R_1,R_2,R_3,R_4\\}$. An edge from $L_i$ to $R_j$ carries a nonnegative weight $W_{ij}$ representing the strength of evidence (for example, abundance-weighted affinity). The only nonzero entries are:\n- $W_{L_1,R_1}=10$, $W_{L_1,R_2}=1$\n- $W_{L_2,R_1}=9$, $W_{L_2,R_3}=1$\n- $W_{L_3,R_1}=8$, $W_{L_3,R_4}=1$\n\nAll other $W_{ij}$ are $0$. The group considers a one-mode projection onto ligands to visualize patterns of co-targeting, and also considers normalized similarities to mitigate visualization artifacts. Using only core definitions from graph theory and linear algebra (such as adjacency, degree, walk counting, and matrix multiplication), analyze how projecting a bipartite network into a one-mode network induces edge weights and how this affects interpretability in the context of ligand–receptor analysis. Then decide which statements below are justified.\n\nA. In the weighted case with matrix $W$ as given, the unnormalized ligand–ligand projection equals $P=W W^\\top$, and for this $W$, the strict ordering $P_{12}  P_{13}  P_{23}$ holds.\n\nB. For a binary bipartite network, the number of undirected edges in the ligand–ligand projection is exactly $\\sum_{r \\in V} \\binom{d_r}{2}$, where $d_r$ is the degree of receptor $r$; therefore, projections cannot inflate the apparent edge count beyond this combinatorial sum and never create duplicated edges.\n\nC. Cosine normalization of the projected weights, defined by $S_{ij} = \\dfrac{P_{ij}}{\\sqrt{P_{ii} P_{jj}}}$, mitigates hub-induced domination; in the given $W$, this normalization yields $S_{12} \\approx S_{13} \\approx S_{23}$, reflecting that co-targeting is almost entirely via $R_1$.\n\nD. Projecting ligand–receptor bipartite networks onto cell-type–cell-type communication networks by summing over all ligand–receptor pairs cannot harm interpretability because directionality (sender to receiver) is preserved in the projection.\n\nE. Projection helps interpretability when receptor degrees are low and neighborhoods across ligands are relatively disjoint (inducing small cliques that align with modules), but when a receptor is a high-degree or high-weight hub, projection tends to obscure specificity by creating dense cliques; in such cases, bipartite layouts or degree-normalized edge weighting are preferable for visualization.\n\nSelect all correct options.", "solution": "The problem statement poses a valid and well-defined question in the domain of systems biology, specifically concerning network analysis. It is scientifically grounded, objective, and provides all necessary information to evaluate the accompanying statements. I will proceed with a full derivation and analysis.\n\nThe problem describes a bipartite network with two disjoint sets of nodes: a set of ligands $U=\\{L_1, L_2, L_3\\}$ and a set of receptors $V=\\{R_1, R_2, R_3, R_4\\}$. The interactions are given by a weight matrix $W$, which can be represented as a $3 \\times 4$ matrix where the rows correspond to ligands and the columns to receptors. Based on the provided non-zero weights, the matrix $W$ is:\n$$\nW = \\begin{pmatrix}\n10  1  0  0 \\\\\n9  0  1  0 \\\\\n8  0  0  1\n\\end{pmatrix}\n$$\nThe problem asks to analyze the one-mode projection onto the ligand set $U$.\n\n### Evaluation of Option A\n\nThis statement makes two claims: that the unnormalized ligand-ligand projection matrix $P$ is given by $P = W W^\\top$, and that for the given weights, the strict ordering $P_{12}  P_{13}  P_{23}$ holds.\n\nFirst, let's validate the formula for the projection. A one-mode projection of a bipartite graph onto one of its partitions (here, ligands $U$) creates an edge between two nodes $L_i$ and $L_j$ if they share a common neighbor in the other partition (receptors $V$). In a weighted projection, the weight of the projected edge $P_{ij}$ is a sum of contributions from all common neighbors. A standard definition for the weight $P_{ij}$ is $\\sum_{k \\in V} W_{ik}W_{jk}$. This is precisely the definition of the element $(i,j)$ of the matrix product $W W^\\top$. The dimensions are consistent: $W$ is $3 \\times 4$ and its transpose, $W^\\top$, is $4 \\times 3$. Their product $P = W W^\\top$ is a $3 \\times 3$ matrix representing the ligand-ligand network. The diagonal elements $P_{ii} = \\sum_{k \\in V} W_{ik}^2$ represent a weighted degree of ligand $L_i$. The first claim is correct.\n\nSecond, let's compute the off-diagonal elements of $P$ to check the ordering.\nThe transpose of $W$ is:\n$$\nW^\\top = \\begin{pmatrix}\n10  9  8 \\\\\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n$$\nNow we compute the product $P = W W^\\top$:\n$P_{12} = (10)(9) + (1)(0) + (0)(1) + (0)(0) = 90$\n$P_{13} = (10)(8) + (1)(0) + (0)(0) + (0)(1) = 80$\n$P_{23} = (9)(8) + (0)(0) + (1)(0) + (0)(1) = 72$\n\nComparing these values, we have $90  80  72$, which confirms the strict ordering $P_{12}  P_{13}  P_{23}$.\n\nBoth claims in the statement are correct.\n**Verdict: Correct.**\n\n### Evaluation of Option B\n\nThis statement claims that for a binary (unweighted) bipartite network, the number of undirected edges in the ligand-ligand projection is exactly $\\sum_{r \\in V} \\binom{d_r}{2}$, where $d_r$ is the degree of receptor $r$.\n\nLet's analyze the formula. For a given receptor $r \\in V$ with degree $d_r$, it is connected to $d_r$ distinct ligands. In the projection, every pair of these $d_r$ ligands will be connected because they share $r$ as a common neighbor. The number of such pairs is $\\binom{d_r}{2}$. The formula $\\sum_{r \\in V} \\binom{d_r}{2}$ sums this quantity over all receptors.\n\nThis sum counts the number of paths of length $2$ of the form $L_i - r - L_j$ for $i \\neq j$. However, the number of edges in the projected graph corresponds to the number of unique pairs of ligands $(L_i, L_j)$ that have at least one common neighbor. If a pair of ligands $(L_i, L_j)$ shares more than one common receptor, say $r_1$ and $r_2$, the edge $(L_i, L_j)$ will be counted multiple times by the formula (once for $r_1$, once for $r_2$), but it represents only a single edge in the resulting simple graph.\n\nConsider a simple counterexample:\nLet $U=\\{L_1, L_2\\}$ and $V=\\{R_1, R_2\\}$.\nEdges are $(L_1, R_1)$, $(L_2, R_1)$, $(L_1, R_2)$, and $(L_2, R_2)$. This is a binary network.\nThe degrees of the receptors are $d_{R_1} = 2$ and $d_{R_2} = 2$.\nThe formula yields $\\sum_{r \\in V} \\binom{d_r}{2} = \\binom{2}{2} + \\binom{2}{2} = 1 + 1 = 2$.\nHowever, the projection onto $U$ contains only one edge: the edge between $L_1$ and $L_2$, because they share common neighbors ($R_1$ and $R_2$). The number of edges is $1$.\nSince $2 \\neq 1$, the formula is incorrect. It represents the sum of off-diagonal entries of the projection matrix, not the number of edges in the simple graph. The final clause \"...and never create duplicated edges\" is misleading; a simple graph by definition has no duplicate edges, but the counting method proposed is what leads to the error.\n\n**Verdict: Incorrect.**\n\n### Evaluation of Option C\n\nThis statement concerns cosine normalization, defined as $S_{ij} = \\frac{P_{ij}}{\\sqrt{P_{ii} P_{jj}}}$, and its effects.\n\nFirst, the principle. The unnormalized projected weight $P_{ij}$ is influenced by the \"activity\" of nodes $i$ and $j$. If $L_i$ and $L_j$ connect to a high-weight hub receptor, $P_{ij}$ can be large even if the shared connection is non-specific. The diagonal terms $P_{ii} = \\sum_k W_{ik}^2$ and $P_{jj} = \\sum_k W_{jk}^2$ are the squared norms of the weight vectors for ligands $i$ and $j$. The formula for $S_{ij}$ is the definition of the cosine similarity between the row vectors of $W$ corresponding to ligands $L_i$ and $L_j$. Cosine similarity measures the similarity of the connection *patterns*, independent of their magnitude, thus mitigating the effect of hubs. The first part of the statement is a correct description of a standard normalization technique in network analysis.\n\nSecond, let's perform the calculation for the given $W$. We need the diagonal elements of $P$ first:\n$P_{11} = (10)^2 + (1)^2 + (0)^2 + (0)^2 = 101$\n$P_{22} = (9)^2 + (0)^2 + (1)^2 + (0)^2 = 82$\n$P_{33} = (8)^2 + (0)^2 + (0)^2 + (1)^2 = 65$\nUsing the off-diagonal values from A: $P_{12}=90, P_{13}=80, P_{23}=72$.\n\nNow, we compute the cosine similarities:\n$S_{12} = \\frac{90}{\\sqrt{101 \\times 82}} = \\frac{90}{\\sqrt{8282}} \\approx \\frac{90}{91.005} \\approx 0.9889$\n$S_{13} = \\frac{80}{\\sqrt{101 \\times 65}} = \\frac{80}{\\sqrt{6565}} \\approx \\frac{80}{81.025} \\approx 0.9873$\n$S_{23} = \\frac{72}{\\sqrt{82 \\times 65}} = \\frac{72}{\\sqrt{5330}} \\approx \\frac{72}{73.007} \\approx 0.9862$\n\nThese values ($0.9889, 0.9873, 0.9862$) are indeed very close to each other and all are close to $1$. This confirms that $S_{12} \\approx S_{13} \\approx S_{23}$. This result correctly reflects that the connection profiles of all three ligands are dominated by their strong connection to the hub receptor $R_1$, making their corresponding weight vectors nearly parallel.\n\n**Verdict: Correct.**\n\n### Evaluation of Option D\n\nThis statement claims that aggregating specific ligand-receptor interactions into a summary cell-type–cell-type communication score \"cannot harm interpretability\".\n\nThis is a fallacious argument. Aggregation is a process of summarizing data, which inherently involves a loss of detail. While it is true that a coarse-grained directionality (from a sender cell population to a receiver cell population) is preserved, the fine-grained information about *which molecular agents* mediate the communication is lost.\n\nFor example, if the aggregate score between cell type A and cell type B is high, the aggregated network does not tell us if this is due to interaction $L_1 \\to R_1$ or $L_5 \\to R_8$. This loss of detail is a significant harm to interpretability, especially if the goal is to design a therapeutic intervention to block or enhance a specific signaling pathway. The absolute claim \"cannot harm\" is false. While aggregation can be useful for obtaining a high-level overview, it comes at the cost of molecular specificity.\n\n**Verdict: Incorrect.**\n\n### Evaluation of Option E\n\nThis statement offers a qualitative guideline on the utility and pitfalls of network projection in this context.\n\nPart 1: \"Projection helps interpretability when receptor degrees are low and neighborhoods across ligands are relatively disjoint...\". If a receptor $R_k$ connects to a small, unique set of ligands, it will create a small, dense clique in the projected ligand network. If different receptors connect to disjoint sets of ligands, the projection will consist of separate or loosely connected cliques. This modular structure is easy to visualize and interpret, with each clique corresponding to a group of ligands that co-target a specific receptor. This is a sound principle.\n\nPart 2: \"...but when a receptor is a high-degree or high-weight hub, projection tends to obscure specificity by creating dense cliques; in such cases, bipartite layouts or degree-normalized edge weighting are preferable for visualization.\" This describes the problem of hub-induced artifacts. A hub receptor that connects to many ligands will create a large, dense clique (a \"hairball\") in the projection, making all connected ligands appear similar and obscuring their more specific interactions. Our example with the hub $R_1$ illustrates this perfectly. The statement correctly suggests two standard solutions:\n1. Visualize the original bipartite graph: This avoids information loss and clearly reveals the hub structure.\n2. Use normalized edge weights: As demonstrated in Option C, techniques like cosine similarity can correct for the hub's dominance and reveal more subtle patterns of specificity.\n\nThis statement provides a correct and nuanced summary of the trade-offs of network projection, consistent with established principles of network visualization and analysis.\n\n**Verdict: Correct.**", "answer": "$$\\boxed{ACE}$$", "id": "4368322"}, {"introduction": "In systems biology, inferred networks are often dense with connections, many of which may be statistical noise. Visualizing all edges creates an uninterpretable 'hairball' and can be misleading. This practice introduces a principled solution: statistical filtering based on control of the False Discovery Rate (FDR). By applying the Benjamini-Hochberg procedure, you will learn how to curate a network to highlight only statistically robust connections, a crucial step in producing a visualization that is both legible and scientifically sound. [@problem_id:4368371]", "problem": "A systems biomedicine laboratory is curating a mechanistic gene regulatory network inferred from multi-omics perturbation data. To render the network legibly and emphasize only edges with reproducible evidence, the team will highlight those edges whose statistical evidence survives a multiple-testing correction targeting a controlled expected proportion of spurious highlights. They model each edge’s test statistic under the null as generating a $p$-value that is uniformly distributed on $[0,1]$, and assume $p$-values are independent across edges. The curation policy is to control the false discovery rate (FDR), defined as the expected proportion of false positives among the highlighted edges.\n\nStarting only from the core facts that under the null the $p$-values are uniform on $[0,1]$, that the false discovery rate is the expected ratio of the number of falsely declared discoveries to the total number of declared discoveries, and that a step-up rule can select a data-adaptive cutoff on ordered $p$-values, derive a principled procedure that determines a threshold on the ordered edge-wise $p$-values which controls the false discovery rate at a target level $q$ under independence. Explain why the resulting rule is valid for visualization in this context.\n\nYou are given $m = 24$ candidate edges and their corresponding edge-wise $p$-values (unsorted):\n$\\{0.074,\\ 0.003,\\ 0.12,\\ 0.028,\\ 0.061,\\ 0.046,\\ 0.001,\\ 0.069,\\ 0.091,\\ 0.44,\\ 0.096,\\ 0.017,\\ 0.78,\\ 0.034,\\ 0.052,\\ 0.006,\\ 0.039,\\ 0.23,\\ 0.011,\\ 0.084,\\ 0.57,\\ 0.022,\\ 0.081,\\ 0.94\\}$.\n\nSet the target false discovery rate to $q = 0.10$. Using the procedure you derived, compute the data-adaptive $p$-value threshold $t^{\\ast}$ at which edges are to be emphasized (that is, all edges with $p \\leq t^{\\ast}$ will be rendered emphatically). Report only the threshold $t^{\\ast}$ as a pure number with no units. Do not round; give the exact decimal value implied by the data.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and internally consistent. It describes a canonical statistical problem—the control of the false discovery rate in multiple hypothesis testing—and its application to a realistic scenario in systems biomedicine. All necessary data and definitions are provided. Therefore, I will proceed with a full derivation and solution.\n\nThe task is to derive a principled procedure for controlling the false discovery rate (FDR) and apply it to a given set of data. The procedure must determine a data-adaptive threshold on edge-wise $p$-values for network visualization.\n\nLet there be $m$ null hypotheses, $H_1, \\dots, H_m$, corresponding to the $m$ potential edges in the network. Let their respective $p$-values be $P_1, \\dots, P_m$. We are given that under a true null hypothesis, its corresponding $p$-value is uniformly distributed on $[0,1]$, and all $p$-values are independent. Let $m_0$ be the unknown number of true null hypotheses.\n\nLet $R$ be the number of hypotheses rejected (i.e., edges highlighted), and let $V$ be the number of true null hypotheses that are incorrectly rejected (false discoveries). The False Discovery Proportion (FDP) is the ratio $V/R$, defined to be $0$ when $R=0$. The False Discovery Rate (FDR) is the expectation of this quantity, $FDR = E[FDP]$. We seek a procedure that guarantees $FDR \\le q$ for a chosen level $q$.\n\nThe Benjamini-Hochberg (BH) procedure is a step-up procedure that achieves this control. The procedure is as follows:\n1.  Order the $m$ $p$-values from smallest to largest: $P_{(1)} \\le P_{(2)} \\le \\dots \\le P_{(m)}$. Let $H_{(i)}$ be the null hypothesis corresponding to $P_{(i)}$.\n2.  Find the largest index $k$, which we will denote $k_{max}$, such that its corresponding ordered $p$-value satisfies the condition:\n    $$P_{(k)} \\le \\frac{k}{m}q$$\n3.  If such a $k_{max}$ exists, reject all null hypotheses $H_{(i)}$ for $i = 1, \\dots, k_{max}$. The number of rejected hypotheses is $R = k_{max}$.\n4.  If no such index $k$ exists, reject no hypotheses ($R=0$).\n\nThe data-adaptive threshold for significance, $t^{\\ast}$, is the largest $p$-value among the rejected set, which is $t^{\\ast} = P_{(k_{max})}$. All edges with $p$-values $P_j \\le t^{\\ast}$ are highlighted.\n\nWe now derive why this procedure controls the FDR at the desired level $q$ under the assumption of independence.\nLet $I_0$ be the set of indices corresponding to the $m_0$ true null hypotheses. The number of false discoveries is $V = \\sum_{j \\in I_0} \\mathbb{1}(P_j \\text{ is rejected})$. By the BH rule, a hypothesis is rejected if its $p$-value is less than or equal to the threshold $t^{\\ast} = P_{(k_{max})}$. Thus, $V = \\sum_{j \\in I_0} \\mathbb{1}(P_j \\le t^{\\ast})$. The number of total discoveries is $R = k_{max}$.\n\nThe definition of $k_{max}$ implies that the threshold $t^{\\ast}$ satisfies $t^{\\ast} = P_{(k_{max})} \\le \\frac{k_{max}}{m}q = \\frac{R}{m}q$. This inequality holds whenever $R  0$ and is central to the proof. It can be rearranged to give $R \\ge \\frac{m t^{\\ast}}{q}$.\n\nThe FDR is defined as $FDR = E\\left[\\frac{V}{R}\\right]$. To avoid division by zero, the FDP is typically defined as $V/\\max(R,1)$, but for this derivation, we can consider the expectation conditional on $R0$.\n$$FDR = E\\left[\\frac{V}{R} \\mathbb{1}(R0)\\right]$$\nUsing the inequality $R \\ge \\frac{m t^{\\ast}}{q}$, we have $\\frac{1}{R} \\le \\frac{q}{m t^{\\ast}}$. Substituting this into the FDR expression gives:\n$$FDR \\le E\\left[V \\cdot \\frac{q}{m t^{\\ast}} \\mathbb{1}(R0)\\right] = \\frac{q}{m} E\\left[\\frac{V}{t^{\\ast}} \\mathbb{1}(R0)\\right]$$\nThe random variable $V$ is the sum of indicators for the $m_0$ true nulls: $V = \\sum_{j \\in I_0} \\mathbb{1}(P_j \\le t^{\\ast})$. By linearity of expectation:\n$$E\\left[\\frac{V}{t^{\\ast}} \\mathbb{1}(R0)\\right] = \\sum_{j \\in I_0} E\\left[\\frac{\\mathbb{1}(P_j \\le t^{\\ast})}{t^{\\ast}} \\mathbb{1}(R0)\\right]$$\nThe threshold $t^{\\ast}$ is a function of all $p$-values. A crucial property of the BH procedure is that $t^{\\ast}$ is a non-increasing function of any individual $p$-value. Let us analyze one term of the sum for a specific true null hypothesis $j \\in I_0$. The expectation can be evaluated by conditioning on the other $p$-values $P_{-j}$. The inner expectation is an integral over the distribution of $P_j$, which is uniform on $[0,1]$.\n$$E_{P_j}\\left[\\frac{\\mathbb{1}(P_j \\le t^{\\ast}(P_j, P_{-j}))}{t^{\\ast}(P_j, P_{-j})}\\right] = \\int_0^1 \\frac{\\mathbb{1}(p_j \\le t^{\\ast}(p_j, P_{-j}))}{t^{\\ast}(p_j, P_{-j})} dp_j$$\nBecause $t^{\\ast}$ is non-increasing in $p_j$, the region where $p_j \\le t^{\\ast}(p_j, P_{-j})$ forms an interval $[0, c]$, where $c$ is the supremum of $p_j$ satisfying the inequality. At this boundary, $c = t^{\\ast}(c, P_{-j})$. For any $p_j \\in [0,c]$, $t^{\\ast}(p_j, P_{-j}) \\ge t^{\\ast}(c, P_{-j}) = c$. Thus, the integral is bounded:\n$$\\int_0^c \\frac{1}{t^{\\ast}(p_j, P_{-j})} dp_j \\le \\int_0^c \\frac{1}{c} dp_j = \\frac{c}{c} = 1$$\nTherefore, for each of the $m_0$ true nulls, the corresponding term in the sum is less than or equal to $1$. This gives:\n$$E\\left[\\frac{V}{t^{\\ast}} \\mathbb{1}(R0)\\right] \\le \\sum_{j \\in I_0} 1 = m_0$$\nSubstituting this result back into the main inequality for the FDR, we obtain:\n$$FDR \\le \\frac{q}{m} m_0 = q \\frac{m_0}{m}$$\nSince the number of true nulls $m_0$ cannot exceed the total number of tests $m$, we have $\\frac{m_0}{m} \\le 1$, which confirms that the Benjamini-Hochberg procedure controls the false discovery rate at the desired level $q$: $FDR \\le q$.\n\nThis procedure is particularly well-suited for network visualization in an exploratory context like systems biomedicine. Controlling the Family-Wise Error Rate (FWER), the probability of even one false positive, is often too stringent and would lead to low statistical power, missing many true biological interactions. The FDR provides a robust and interpretable alternative: controlling the expected *proportion* of spurious edges among those highlighted. A choice of $q=0.10$ signifies a willingness to accept that, on average, up to $10\\%$ of the highlighted discoveries may be false. This balances the need for discovery with principled error control, making it an excellent tool for generating a high-confidence, yet sufficiently rich, network for visual analysis and further experimental validation.\n\nNow, we apply this procedure to the given data.\nWe have $m = 24$ candidate edges and a target FDR of $q = 0.10$.\nThe set of $p$-values is $\\{0.074, 0.003, 0.12, 0.028, 0.061, 0.046, 0.001, 0.069, 0.091, 0.44, 0.096, 0.017, 0.78, 0.034, 0.052, 0.006, 0.039, 0.23, 0.011, 0.084, 0.57, 0.022, 0.081, 0.94\\}$.\n\nFirst, we sort the $p$-values in ascending order:\n$P_{(1)} = 0.001$\n$P_{(2)} = 0.003$\n$P_{(3)} = 0.006$\n$P_{(4)} = 0.011$\n$P_{(5)} = 0.017$\n$P_{(6)} = 0.022$\n$P_{(7)} = 0.028$\n$P_{(8)} = 0.034$\n... (and so on)\n\nNext, we find the largest index $k$ for which $P_{(k)} \\le \\frac{k}{m}q$. Here, $m=24$ and $q=0.10$, so the condition is $P_{(k)} \\le \\frac{k \\cdot 0.10}{24} = \\frac{k}{240}$.\nWe test this condition for each index $k$:\n- For $k=1$: $P_{(1)} = 0.001$. Condition: $0.001 \\le \\frac{1}{240} \\approx 0.00417$. True.\n- For $k=2$: $P_{(2)} = 0.003$. Condition: $0.003 \\le \\frac{2}{240} \\approx 0.00833$. True.\n- For $k=3$: $P_{(3)} = 0.006$. Condition: $0.006 \\le \\frac{3}{240} = 0.0125$. True.\n- For $k=4$: $P_{(4)} = 0.011$. Condition: $0.011 \\le \\frac{4}{240} \\approx 0.01667$. True.\n- For $k=5$: $P_{(5)} = 0.017$. Condition: $0.017 \\le \\frac{5}{240} \\approx 0.02083$. True.\n- For $k=6$: $P_{(6)} = 0.022$. Condition: $0.022 \\le \\frac{6}{240} = 0.025$. True.\n- For $k=7$: $P_{(7)} = 0.028$. Condition: $0.028 \\le \\frac{7}{240} \\approx 0.02917$. True.\n- For $k=8$: $P_{(8)} = 0.034$. Condition: $0.034 \\le \\frac{8}{240} \\approx 0.03333$. False.\n\nSince the inequality $P_{(k)} \\le \\frac{k}{m}q$ becomes more stringent as $k$ increases (the line $\\frac{k}{m}q$ grows linearly while the ordered $P_{(k)}$ grow non-linearly and typically faster), we do not need to check further. The largest index $k$ that satisfies the condition is $k_{max}=7$.\n\nThe procedure thus rejects the null hypotheses for the $7$ edges with the smallest $p$-values. The significance threshold $t^{\\ast}$ is the $p$-value at this cutoff, $t^{\\ast} = P_{(k_{max})} = P_{(7)}$.\n\nFrom our sorted list, $P_{(7)} = 0.028$.\nTherefore, the data-adaptive $p$-value threshold is $t^{\\ast} = 0.028$. All edges with a $p$-value less than or equal to this threshold will be highlighted.", "answer": "$$\\boxed{0.028}$$", "id": "4368371"}]}