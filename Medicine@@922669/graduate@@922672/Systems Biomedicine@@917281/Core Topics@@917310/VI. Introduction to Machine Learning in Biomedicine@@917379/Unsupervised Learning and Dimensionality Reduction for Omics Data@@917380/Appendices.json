{"hands_on_practices": [{"introduction": "Principal Component Analysis (PCA) is a cornerstone of dimensionality reduction, and its most robust implementation relies on Singular Value Decomposition (SVD). This first practice provides hands-on experience with the core mathematical engine of PCA. By calculating the explained variance ratio directly from singular values, you will solidify your understanding of how PCA identifies and quantifies the dominant axes of variation within a dataset [@problem_id:4397353].", "problem": "In systems biomedicine, high-dimensional transcriptomic, proteomic, and metabolomic profiles can be modeled as a real-valued data matrix where rows are biological samples and columns are molecular features. Consider a centered data matrix $X_c \\in \\mathbb{R}^{n \\times p}$ whose columns have zero mean across samples. Principal Component Analysis (PCA) is used to find orthogonal directions that maximize variance. Using Singular Value Decomposition (SVD), implement a program that, for each provided test case, computes the top $k$ principal components and outputs the fraction of total variance explained by these $k$ components, defined as the ratio between the variance captured by the top $k$ principal components and the total variance across all admissible principal components. The admissible number of principal components is limited by the degrees of freedom induced by centering and the ambient dimensionality. If the total variance is zero, define the explained variance ratio to be $0$.\n\nYour implementation must:\n- Accept a centered matrix $X_c$ and an integer $k$.\n- Compute the SVD $X_c = U \\Sigma V^\\top$ and use it to obtain the top $k$ principal components as the leading $k$ right singular vectors (principal axes), and compute the explained variance ratio as described above.\n- Use the convention that centering reduces the maximum number of nonzero principal components to at most $\\min(n-1, p)$.\n- Not standardize or rescale features; treat $X_c$ as already centered columns.\n- Handle the boundary case $k=0$ by returning an explained variance ratio of $0$.\n- Handle the case where the total variance is zero (for example, a zero matrix) by returning $0$.\n\nTest Suite:\nProvide a single program that computes the explained variance ratio for the following four test cases. Each $X_c^{(i)}$ is given explicitly, with $k^{(i)}$ specified. All matrices are column-centered. For each case, compute the ratio as a decimal (not a percentage).\n\n- Case $1$:\n  - Dimensions: $n = 6$, $p = 4$, $k^{(1)} = 2$.\n  - Matrix:\n  $$\n  X_c^{(1)} =\n  \\begin{bmatrix}\n  -2 & 3 & 1 & 2 \\\\\n  -1 & -1 & -1 & 2 \\\\\n  0 & 0 & 1 & -1 \\\\\n  1 & -2 & -1 & -1 \\\\\n  2 & 1 & 0 & -1 \\\\\n  0 & -1 & 0 & -1\n  \\end{bmatrix}\n  $$\n\n- Case $2$:\n  - Dimensions: $n = 4$, $p = 6$, $k^{(2)} = 1$.\n  - Matrix:\n  $$\n  X_c^{(2)} =\n  \\begin{bmatrix}\n  3 & 1 & 2 & 1 & 0 & 1 \\\\\n  -1 & 1 & -2 & 0 & 0 & 1 \\\\\n  -2 & -1 & 0 & -1 & 0 & -1 \\\\\n  0 & -1 & 0 & 0 & 0 & -1\n  \\end{bmatrix}\n  $$\n\n- Case $3$:\n  - Dimensions: $n = 3$, $p = 2$, $k^{(3)} = 2$.\n  - Matrix:\n  $$\n  X_c^{(3)} =\n  \\begin{bmatrix}\n  1 & -1 \\\\\n  0 & 0 \\\\\n  -1 & 1\n  \\end{bmatrix}\n  $$\n\n- Case $4$:\n  - Dimensions: $n = 5$, $p = 3$, $k^{(4)} = 0$.\n  - Matrix:\n  $$\n  X_c^{(4)} =\n  \\begin{bmatrix}\n  2 & 1 & 0 \\\\\n  -2 & -1 & 0 \\\\\n  0 & 2 & 1 \\\\\n  1 & -2 & -1 \\\\\n  -1 & 0 & 0\n  \\end{bmatrix}\n  $$\n\nFinal Output Format:\n- Your program should produce a single line of output containing the four explained variance ratios corresponding to the four cases, in order, each rounded to exactly six digits after the decimal point, as a comma-separated list enclosed in square brackets. For example, the output must look like $[\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4]$ where each $\\alpha_i$ is a decimal with exactly six digits after the decimal point.", "solution": "The user has provided a valid, well-posed problem statement grounded in the principles of linear algebra and statistics as applied to computational biology. The task is to compute the fraction of total variance explained by the top $k$ principal components for a given centered data matrix $X_c$. This will be accomplished using Singular Value Decomposition (SVD).\n\n### Principle and Derivation\n\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique that transforms a dataset of potentially correlated variables into a set of linearly uncorrelated variables called principal components. These components are ordered such that the first few retain most of the variation present in the original dataset.\n\nLet the data be represented by a matrix $X_c \\in \\mathbb{R}^{n \\times p}$, where $n$ is the number of samples and $p$ is the number of features. The problem specifies that this matrix is centered, meaning the mean of each column (feature) is zero.\n\nThe sample covariance matrix, which captures the covariance between pairs of features, is given by:\n$$\nC = \\frac{1}{n-1} X_c^\\top X_c\n$$\nPCA finds the eigenvectors of this covariance matrix $C$. These eigenvectors are the principal axes, and the corresponding eigenvalues, $\\lambda_j$, represent the variance of the data when projected onto these axes. The total variance in the dataset is the sum of all the eigenvalues, which is equal to the trace of the covariance matrix:\n$$\n\\text{Total Variance} = \\text{tr}(C) = \\sum_{j=1}^{\\text{rank}(C)} \\lambda_j\n$$\nThe fraction of variance explained by the first $k$ principal components is the ratio of the sum of the first $k$ largest eigenvalues to the sum of all eigenvalues:\n$$\n\\text{Explained Variance Ratio} = \\frac{\\sum_{j=1}^k \\lambda_j}{\\sum_{j=1}^{\\text{rank}(C)} \\lambda_j}\n$$\nThe problem requires implementing this using Singular Value Decomposition (SVD). The SVD of the matrix $X_c$ is a factorization of the form:\n$$\nX_c = U \\Sigma V^\\top\n$$\nwhere $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{p \\times p}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{n \\times p}$ is a rectangular diagonal matrix containing the non-negative singular values $\\sigma_j$ in descending order. The columns of $V$ are the right-singular vectors.\n\nWe can establish a direct relationship between SVD and the covariance matrix $C$:\n$$\nC = \\frac{1}{n-1} X_c^\\top X_c = \\frac{1}{n-1} (U \\Sigma V^\\top)^\\top (U \\Sigma V^\\top) = \\frac{1}{n-1} (V \\Sigma^\\top U^\\top U \\Sigma V^\\top)\n$$\nSince $U$ is orthogonal, $U^\\top U = I$ (the identity matrix). This simplifies the expression to:\n$$\nC = V \\left( \\frac{\\Sigma^\\top \\Sigma}{n-1} \\right) V^\\top\n$$\nThis equation is the eigendecomposition of $C$. The columns of $V$ are the eigenvectors of $C$ (the principal axes), and the diagonal entries of the matrix $\\frac{\\Sigma^\\top \\Sigma}{n-1}$ are the eigenvalues $\\lambda_j$. Specifically, the eigenvalues of $C$ are related to the singular values of $X_c$ by:\n$$\n\\lambda_j = \\frac{\\sigma_j^2}{n-1}\n$$\nSubstituting this into the explained variance ratio formula, the constant factor $\\frac{1}{n-1}$ cancels out:\n$$\n\\text{Explained Variance Ratio} = \\frac{\\sum_{j=1}^k \\frac{\\sigma_j^2}{n-1}}{\\sum_{j=1}^{\\text{rank}(X_c)} \\frac{\\sigma_j^2}{n-1}} = \\frac{\\sum_{j=1}^k \\sigma_j^2}{\\sum_{j=1}^{\\text{rank}(X_c)} \\sigma_j^2}\n$$\nThe rank of $X_c$, denoted $\\text{rank}(X_c)$, is the number of non-zero singular values. The denominator is therefore the sum of all squared singular values. The problem notes that for a centered matrix, the rank is at most $\\min(n-1, p)$. A standard SVD algorithm will compute $\\min(n, p)$ singular values, and any singular values for components beyond the matrix rank will be zero, thus not contributing to the sum. Therefore, the implementation can simply sum all computed squared singular values for the denominator.\n\n### Algorithm\nThe algorithm to compute the explained variance ratio for a given matrix $X_c$ and integer $k$ is as follows:\n1.  Handle the boundary case specified in the problem: if $k=0$, the explained variance ratio is $0$.\n2.  Compute the singular values, $\\sigma_j$, of the matrix $X_c$. Most numerical libraries provide a function for this (e.g., `numpy.linalg.svd`).\n3.  Calculate the squared singular values, $\\sigma_j^2$.\n4.  Calculate the total variance term, which is the sum of all squared singular values: $\\sum_j \\sigma_j^2$.\n5.  Handle the boundary case where the total variance is zero (i.e., $X_c$ is a zero matrix). In this case, the ratio is defined as $0$.\n6.  Calculate the explained variance term, which is the sum of the top $k$ squared singular values: $\\sum_{j=1}^k \\sigma_j^2$.\n7.  The explained variance ratio is the explained variance term divided by the total variance term.\nThis procedure will be applied to each of the four test cases provided.", "answer": "```python\nimport numpy as np\n\ndef compute_explained_variance_ratio(X_c, k):\n    \"\"\"\n    Computes the fraction of total variance explained by the top k principal components.\n\n    Args:\n        X_c (np.ndarray): A centered data matrix of shape (n, p).\n        k (int): The number of top principal components to consider.\n\n    Returns:\n        float: The fraction of total variance explained.\n    \"\"\"\n    # Per the problem statement, if k=0, the ratio is 0.\n    if k == 0:\n        return 0.0\n\n    # The number of samples n is needed for the covariance definition, but the\n    # 1/(n-1) factor cancels in the ratio, so n is not explicitly used in the final formula.\n    # However, if n<=1, the matrix must be a zero matrix (if centered), leading to zero variance.\n    if X_c.shape[0] <= 1:\n        return 0.0\n        \n    # Compute the singular values of the centered data matrix X_c.\n    # We only need the singular values, so we set compute_uv=False for efficiency.\n    singular_values = np.linalg.svd(X_c, compute_uv=False)\n    \n    # The variance of each principal component is proportional to the square of its corresponding singular value.\n    # Eigenvalues of the covariance matrix are lambda_j = sigma_j^2 / (n-1).\n    squared_singular_values = singular_values**2\n    \n    # The total variance is proportional to the sum of all squared singular values.\n    # This corresponds to the trace of X_c.T @ X_c.\n    total_variance_term = np.sum(squared_singular_values)\n    \n    # Per the problem statement, if the total variance is zero, the ratio is 0.\n    # We use a small tolerance for floating point comparison.\n    if total_variance_term < 1e-12:\n        return 0.0\n        \n    # The variance explained by the top k components is proportional to the sum\n    # of the first k squared singular values.\n    explained_variance_term = np.sum(squared_singular_values[:k])\n    \n    # The explained variance ratio is the division of the two terms.\n    # The scaling factor 1/(n-1) from the covariance definition cancels out.\n    ratio = explained_variance_term / total_variance_term\n    \n    return ratio\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite.\n    \"\"\"\n    # Case 1: n=6, p=4, k=2\n    X1 = np.array([\n        [-2., 3., 1., 2.],\n        [-1., -1., -1., 2.],\n        [0., 0., 1., -1.],\n        [1., -2., -1., -1.],\n        [2., 1., 0., -1.],\n        [0., -1., 0., -1.]\n    ])\n    k1 = 2\n\n    # Case 2: n=4, p=6, k=1\n    X2 = np.array([\n        [3., 1., 2., 1., 0., 1.],\n        [-1., 1., -2., 0., 0., 1.],\n        [-2., -1., 0., -1., 0., -1.],\n        [0., -1., 0., 0., 0., -1.]\n    ])\n    k2 = 1\n\n    # Case 3: n=3, p=2, k=2\n    X3 = np.array([\n        [1., -1.],\n        [0., 0.],\n        [-1., 1.]\n    ])\n    k3 = 2\n\n    # Case 4: n=5, p=3, k=0\n    X4 = np.array([\n        [2., 1., 0.],\n        [-2., -1., 0.],\n        [0., 2., 1.],\n        [1., -2., -1.],\n        [-1., 0., 0.]\n    ])\n    k4 = 0\n\n    test_cases = [\n        (X1, k1),\n        (X2, k2),\n        (X3, k3),\n        (X4, k4)\n    ]\n    \n    results = []\n    for X_c, k in test_cases:\n        ratio = compute_explained_variance_ratio(X_c, k)\n        # Format the result to exactly six digits after the decimal point.\n        results.append(f\"{ratio:.6f}\")\n        \n    # Print the final output in the specified format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4397353"}, {"introduction": "Standard machine learning pipelines often include feature scaling as a default step, but the unique statistical properties of omics data demand a more critical approach. This exercise demonstrates how a seemingly innocuous choice, like scaling features to unit variance, can dramatically alter PCA results and obscure meaningful biological signals [@problem_id:4397370]. Working through this example will help you develop crucial intuition about the interplay between data characteristics and algorithmic assumptions, particularly for sparse, count-based data.", "problem": "You are analyzing a gene expression data set with two genes across a very large number of samples (cells), where the features have been mean-centered. Let the empirical covariance matrix of the two genes be:\n$$ S = \\begin{pmatrix} 100 & 9 \\\\ 9 & 1 \\end{pmatrix} $$\nYou will compare Principal Component Analysis (PCA) performed on the unscaled features versus PCA performed after scaling each feature to unit variance.\n\nFor unscaled PCA, define the first loading vector as the unit-norm eigenvector associated with the largest eigenvalue of $S$ (that is, PCA on the covariance matrix).\n\nFor scaled-to-unit-variance PCA, define $D$ as the diagonal matrix of feature variances:\n$$ D = \\mathrm{diag}(S_{11}, S_{22}) = \\mathrm{diag}(100, 1) $$\nThen, define the correlation-similarity matrix:\n$$ C = D^{-1/2} S D^{-1/2} $$\nLet $a$ be the unit-norm eigenvector of $C$ associated with its largest eigenvalue. To express the corresponding first loading direction back in the original feature coordinates, map $a$ back as $v_{\\mathrm{scaled}} \\propto D^{-1/2} a$ and then normalize to unit norm to obtain the scaled first loading vector in the original coordinate system.\n\nCompute the angle $\\theta$ (in radians) between the unscaled and scaled first loading vectors in the original feature coordinates, defined as:\n$$ \\theta = \\arccos\\big(|u^{\\top} v_{\\mathrm{scaled}}|\\big) $$\nwhere $u$ is the unit-norm first loading vector from unscaled PCA and $v_{\\mathrm{scaled}}$ is the unit-norm first loading vector mapped back from scaled PCA as described above. Report $\\theta$ in radians, rounded to four significant figures.\n\nThen, starting from the basic meanâ€“variance relationships for count data (for example, Poisson variance $\\mathrm{Var}(Y)=\\mu$ and Negative Binomial variance $\\mathrm{Var}(Y)=\\mu + \\mu^{2}/\\theta$), reason whether and when scaling each feature to unit variance is detrimental for sparse count data in high-throughput transcriptomics, and explain why in terms of how scaling alters the contribution of low-mean, zero-inflated features to the PCA objective. Your explanation must be justified from these foundational relationships without appealing to pre-derived heuristics.\n\nYour final numerical answer should be only the value of $\\theta$; do not include any units in the final boxed answer. The explanatory reasoning does not need to be included in the final boxed answer but must be present in your solution. Round to four significant figures and express angles in radians.", "solution": "The user has provided a two-part problem. The first part requires a numerical calculation of the angle between the first principal component loading vectors obtained from unscaled and scaled data. The second part requires a conceptual explanation of the potential issues with feature scaling for sparse count data, grounded in fundamental statistical properties of such data.\n\nThe problem statement has been validated and found to be scientifically grounded, well-posed, objective, and complete. I will now proceed with the solution.\n\n### Part 1: Calculation of the Angle $\\theta$\n\nThe first step is to compute the first principal component (PC1) loading vector for the unscaled data. This is defined as the unit-norm eigenvector associated with the largest eigenvalue of the empirical covariance matrix $S$.\n\nThe given covariance matrix is:\n$$ S = \\begin{pmatrix} 100 & 9 \\\\ 9 & 1 \\end{pmatrix} $$\nFor a $2 \\times 2$ symmetric matrix of the form $\\begin{pmatrix} a & b \\\\ b & d \\end{pmatrix}$, the angle $\\phi$ that the major eigenvector makes with the first axis can be found using the relation:\n$$ \\tan(2\\phi) = \\frac{2b}{a-d} $$\nIn our case, $a=100$, $b=9$, and $d=1$.\n$$ \\tan(2 \\phi_u) = \\frac{2 \\times 9}{100 - 1} = \\frac{18}{99} = \\frac{2}{11} $$\nwhere $\\phi_u$ is the angle of the unscaled PC1 loading vector $u$ with the first coordinate axis. As the off-diagonal elements are positive and $a > d$, the eigenvector will lie in the first quadrant, so we can write:\n$$ \\phi_u = \\frac{1}{2} \\arctan\\left(\\frac{2}{11}\\right) $$\nThe loading vector $u$ is given by $u = \\begin{pmatrix} \\cos(\\phi_u) \\\\ \\sin(\\phi_u) \\end{pmatrix}$.\n\nThe second step is to compute the PC1 loading vector for the scaled data. First, we construct the correlation matrix $C$. The diagonal matrix of variances $D$ and its inverse square root $D^{-1/2}$ are:\n$$ D = \\mathrm{diag}(100, 1) = \\begin{pmatrix} 100 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\n$$ D^{-1/2} = \\mathrm{diag}(100^{-1/2}, 1^{-1/2}) = \\begin{pmatrix} 1/10 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\nThe correlation matrix $C$ is then:\n$$ C = D^{-1/2} S D^{-1/2} = \\begin{pmatrix} 1/10 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 100 & 9 \\\\ 9 & 1 \\end{pmatrix} \\begin{pmatrix} 1/10 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 10 & 9/10 \\\\ 9 & 1 \\end{pmatrix} \\begin{pmatrix} 1/10 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 9/10 \\\\ 9/10 & 1 \\end{pmatrix} $$\nNext, we find the first loading vector $a$ of the scaled data, which is the unit-norm eigenvector of $C$ corresponding to its largest eigenvalue. For a correlation matrix of the form $\\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$, the eigenvectors are always proportional to $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. The eigenvector $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ corresponds to the eigenvalue $1+\\rho$, and $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ corresponds to $1-\\rho$. Here, $\\rho = 9/10$, so the largest eigenvalue is $1+9/10=19/10$, and its corresponding un-normalized eigenvector is $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. The unit-norm eigenvector $a$ is:\n$$ a = \\frac{1}{\\sqrt{1^2+1^2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\nThis vector $a$ is in the coordinate system of the scaled features. To find the corresponding direction in the original feature coordinates, we map it back using $D^{-1/2}$:\n$$ v_{\\mathrm{temp}} \\propto D^{-1/2} a = \\begin{pmatrix} 1/10 & 0 \\\\ 0 & 1 \\end{pmatrix} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1/10 \\\\ 1 \\end{pmatrix} $$\nTo obtain the final loading vector $v_{\\mathrm{scaled}}$, we normalize $v_{\\mathrm{temp}}$:\n$$ v_{\\mathrm{scaled}} = \\frac{\\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1/10 \\\\ 1 \\end{pmatrix}}{\\left\\| \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1/10 \\\\ 1 \\end{pmatrix} \\right\\|} = \\frac{\\begin{pmatrix} 1/10 \\\\ 1 \\end{pmatrix}}{\\sqrt{(1/10)^2 + 1^2}} = \\frac{\\begin{pmatrix} 1/10 \\\\ 1 \\end{pmatrix}}{\\sqrt{1/100 + 1}} = \\frac{\\begin{pmatrix} 1/10 \\\\ 1 \\end{pmatrix}}{\\sqrt{101/100}} = \\frac{\\sqrt{100}}{\\sqrt{101}} \\begin{pmatrix} 1/10 \\\\ 1 \\end{pmatrix} = \\frac{10}{\\sqrt{101}} \\begin{pmatrix} 1/10 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{101}} \\begin{pmatrix} 1 \\\\ 10 \\end{pmatrix} $$\nThe angle $\\phi_v$ that $v_{\\mathrm{scaled}}$ makes with the first coordinate axis is given by:\n$$ \\tan(\\phi_v) = \\frac{10}{1} = 10 \\implies \\phi_v = \\arctan(10) $$\nThe problem asks for the angle $\\theta = \\arccos(|u^{\\top} v_{\\mathrm{scaled}}|)$. The dot product $u^{\\top} v_{\\mathrm{scaled}}$ is equal to $\\cos(\\phi_v - \\phi_u)$. Since both $\\phi_u$ and $\\phi_v$ are in the first quadrant and $\\phi_v > \\phi_u$, their difference is an acute angle. Therefore, its cosine is positive, and $\\arccos(\\cos(\\phi_v - \\phi_u)) = \\phi_v - \\phi_u$.\n$$ \\theta = \\phi_v - \\phi_u = \\arctan(10) - \\frac{1}{2}\\arctan\\left(\\frac{2}{11}\\right) $$\nWe now compute the numerical value in radians:\n$$ \\arctan(10) \\approx 1.47112767 \\text{ rad} $$\n$$ \\arctan\\left(\\frac{2}{11}\\right) \\approx 0.18005051 \\text{ rad} $$\n$$ \\theta \\approx 1.47112767 - \\frac{1}{2}(0.18005051) = 1.47112767 - 0.090025255 = 1.381102415 \\text{ rad} $$\nRounding to four significant figures, we get $\\theta \\approx 1.381$.\n\n### Part 2: Rationale on Scaling Sparse Count Data\n\nThe task is to reason why scaling features to unit variance can be detrimental for sparse count data, as found in high-throughput transcriptomics, starting from fundamental mean-variance relationships.\n\n1.  **Mean-Variance Coupling in Count Data**: For count data, the variance of a feature (gene) is intrinsically linked to its mean expression level. The problem provides the Poisson model, $\\mathrm{Var}(Y) = \\mu$, and the Negative Binomial model, $\\mathrm{Var}(Y) = \\mu + \\mu^2/\\phi$ (using $\\phi$ for the dispersion parameter, commonly denoted $\\theta$), where $\\mu$ is the mean count. In both foundational models, variance is an increasing function of the mean. Genes with higher average expression levels naturally exhibit higher variance.\n\n2.  **Nature of Transcriptomic Data**: High-throughput transcriptomic datasets are characterized by high sparsity. Many genes have a very low mean expression ($\\mu_j \\approx 0$) and are detected (i.e., have a count greater than zero) in only a small fraction of cells. For such a gene $j$, its variance $\\sigma_j^2 = \\mathrm{Var}(Y_j)$ will consequently be very low.\n\n3.  **The Effect of Unit-Variance Scaling**: Scaling a feature $j$ involves transforming its values $Y_{ij}$ for each cell $i$ to $Y'_{ij} = Y_{ij} / \\sigma_j$.\n    - For cells where gene $j$ is not detected, $Y_{ij} = 0$, so the scaled value remains $Y'_{ij}=0$.\n    - For the few cells where gene $j$ is detected, $Y_{ij} > 0$. Since its variance $\\sigma_j^2$ (and thus its standard deviation $\\sigma_j$) is very small, the scaled value $Y'_{ij} = Y_{ij}/\\sigma_j$ becomes extremely large.\n    Effectively, scaling transforms a vector of mostly zeros with a few small integer counts into a vector of mostly zeros with a few extremely large numerical values.\n\n4.  **Impact on the PCA Objective**: PCA seeks to find principal components (linear combinations of features) that maximize variance.\n    - **Without scaling**, the total variance is the sum of individual gene variances, $\\sum_j \\sigma_j^2$. Genes with high biological variance (e.g., highly expressed housekeeping genes or genes driving major biological processes like cell cycle) dominate this sum. The first PCs will naturally align with these major axes of variation, which is often biologically desirable. The contribution of a sparse, low-mean gene is negligible because its $\\sigma_j^2$ is small.\n    - **With scaling**, every gene is forced to have a variance of $1$. The PCA objective now gives equal a priori weight to every gene. However, the transformation described in point 3 has a pernicious effect. A single, sparsely expressed gene, after scaling, is now a feature characterized by extreme values in a small subset of cells. Such a feature can exhibit high \"variance contribution\" not because it represents a continuous biological process, but because it acts as a binary-like indicator for a few \"outlying\" cells. A principal component can achieve high variance simply by assigning a large weight to this single, artificially inflated gene.\n\n5.  **Detrimental Outcome**: The resulting PC does not represent a coordinated pattern of expression across multiple genes, which is the goal of dimensionality reduction. Instead, it may simply capture the pattern of a single, noisy, sparse gene, effectively isolating the few cells where it was stochastically expressed. This obscures the more subtle, but more meaningful, biological patterns of co-variation among genes that are central to understanding the system's biology. The scaling process disproportionately amplifies the contribution of low-mean, low-variance features that are often the least reliable and most affected by sampling noise, making them appear as dominant drivers of heterogeneity in the dataset. This artifactual inflation is a direct consequence of dividing small non-zero counts by a small standard deviation that is itself a direct consequence of the fundamental mean-variance coupling in count data.\n\nIn summary, scaling is detrimental because it breaks the natural relationship between a gene's mean expression and its variance, giving undue influence to rare, noisy expression events in sparse genes, which can dominate and corrupt the principal components.", "answer": "$$\n\\boxed{1.381}\n$$", "id": "4397370"}, {"introduction": "While PCA provides a geometric view of variation, other methods like topic modeling offer a parts-based decomposition that can be more interpretable for 'omics' data. This advanced practice guides you through implementing a topic model using Non-negative Matrix Factorization (NMF) and, critically, evaluating its biological relevance using gene ontology enrichment [@problem_id:4397356]. This exercise bridges the gap between fitting an unsupervised model and extracting meaningful biological insights, a central task in systems biomedicine.", "problem": "You are given a single-cell gene-by-cell count matrix and a set of gene ontology terms expressed as predefined gene sets over the same gene universe. Your task is to formalize and implement an unsupervised topic model over the count matrix and to evaluate a topic coherence metric derived from gene ontology enrichment of the top genes per topic.\n\nStart from the following fundamental base: in single-cell transcriptomics, messenger ribonucleic acid (mRNA) molecule counts are discrete nonnegative variables. A classical assumption for modeling such counts is that they are generated by Poisson processes, where for each gene and each cell the observed count is a realization from a Poisson distribution with a rate parameter that depends on latent biological programs. In a topic model for gene expression, one represents these latent programs as nonnegative factors so that the expected rate is decomposed into nonnegative contributions.\n\nFormally, let $X \\in \\mathbb{R}_{\\ge 0}^{G \\times C}$ be the count matrix with $G$ genes and $C$ cells, where $X_{gc}$ is the observed count for gene $g$ in cell $c$. Let $K$ be the number of latent topics. Estimate nonnegative matrices $W \\in \\mathbb{R}_{\\ge 0}^{G \\times K}$ and $H \\in \\mathbb{R}_{\\ge 0}^{K \\times C}$ such that the Poisson rate for gene $g$ in cell $c$ is approximated by $\\lambda_{gc} = \\sum_{k=1}^{K} W_{gk} H_{kc}$. Use the principle that maximizing the Poisson likelihood over $W$ and $H$ under nonnegativity constraints is equivalent to minimizing the generalized Kullback-Leibler divergence between $X$ and $W H$. After estimating $W$ and $H$, define for each topic $k$ a ranking over genes by descending $W_{gk}$, and let the top-$T$ genes be the $T$ highest-ranked genes for topic $k$.\n\nTo evaluate coherence, use Gene Ontology (GO) term enrichment of the top genes as follows. Let $\\mathcal{U}$ be the universe of all $G$ genes, and let $\\mathcal{S} = \\{S_1, \\dots, S_J\\}$ be a collection of GO terms, each represented as a subset $S_j \\subseteq \\mathcal{U}$. For a given topic $k$, consider the overlap $m_{kj}$ between the top-$T$ genes and $S_j$. Under the null hypothesis that the top-$T$ genes are chosen uniformly at random without replacement from $\\mathcal{U}$, the overlap is modeled by a hypergeometric distribution with population size $M = G$, population successes $K_j = |S_j|$, and draws $n = T$. Compute the one-sided enrichment $p$-value for $m_{kj}$ or more overlaps with a hypergeometric tail probability. Define the per-topic coherence score as $-\\log_{10}(\\min_j p_{kj} + \\epsilon)$ for a small $\\epsilon$ to avoid taking a logarithm of zero, and define the overall coherence for a dataset as the arithmetic mean over topics $k = 1, \\dots, K$.\n\nImplement the above in a single program, and evaluate it on the following deterministic test suite. All random number generation must use the specified seeds. No physical units apply in this problem. All outputs are real-valued floats. Angles are not used. Percentages are not used.\n\nTest suite definitions:\n\n- Test case $1$ (structured topics with aligned GO terms):\n  - Universe: $G = 60$ genes with identifiers $\\{ \\text{G}0, \\text{G}1, \\dots, \\text{G}59 \\}$.\n  - GO terms: three sets,\n    - $\\text{GO\\_A} = \\{ \\text{G}0, \\dots, \\text{G}14 \\}$,\n    - $\\text{GO\\_B} = \\{ \\text{G}20, \\dots, \\text{G}34 \\}$,\n    - $\\text{GO\\_C} = \\{ \\text{G}40, \\dots, \\text{G}54 \\}$.\n  - Latent topics: $K = 3$.\n  - Top genes per topic: $T = 10$.\n  - Construct $W_{\\text{true}} \\in \\mathbb{R}_{\\ge 0}^{60 \\times 3}$ with columns normalized to sum to $1$ as follows: for each topic $k \\in \\{1,2,3\\}$, initialize the column with all ones, then add $5$ to entries corresponding to its designated GO term ($\\text{GO\\_A}$ for topic $1$, $\\text{GO\\_B}$ for topic $2$, and $\\text{GO\\_C}$ for topic $3$), then normalize the column to sum to $1$.\n  - Construct $H_{\\text{true}} \\in \\mathbb{R}_{\\ge 0}^{3 \\times 80}$ with $C = 80$ cells partitioned into three consecutive blocks of sizes $27$, $27$, and $26$ cells assigned to topics $1$, $2$, and $3$, respectively. For a cell assigned to topic $t$, set $H_{\\text{true}}(t, c) = 200$ and $H_{\\text{true}}(k \\ne t, c) = 50$.\n  - Generate counts $X \\sim \\text{Poisson}(W_{\\text{true}} H_{\\text{true}})$ using a fixed pseudo-random seed of $7$.\n  - Fit the topic model with $K = 3$ using a nonnegative factorization consistent with maximizing the Poisson likelihood under nonnegativity, initialized with a fixed pseudo-random seed of $1234$ and run for $300$ iterations.\n  - Compute the overall coherence as described with $\\epsilon = 10^{-300}$.\n\n- Test case $2$ (weak structure):\n  - Universe: same as in test case $1$.\n  - GO terms: same as in test case $1$.\n  - Latent topics for fitting: $K = 3$.\n  - Top genes per topic: $T = 10$.\n  - Generate counts $X$ as independent draws $X_{gc} \\sim \\text{Poisson}(3)$ for $G = 60$ and $C = 80$ using a fixed pseudo-random seed of $17$.\n  - Fit the topic model with the same procedure as in test case $1$ using a fixed pseudo-random seed of $2024$ and $300$ iterations.\n  - Compute the overall coherence as described with $\\epsilon = 10^{-300}$.\n\n- Test case $3$ (smaller system, partial alignment):\n  - Universe: $G = 30$ genes with identifiers $\\{ \\text{G}0, \\text{G}1, \\dots, \\text{G}29 \\}$.\n  - GO terms: three sets,\n    - $\\text{GO\\_D} = \\{ \\text{G}0, \\dots, \\text{G}9 \\}$,\n    - $\\text{GO\\_E} = \\{ \\text{G}10, \\dots, \\text{G}19 \\}$,\n    - $\\text{GO\\_F} = \\{ \\text{G}20, \\dots, \\text{G}29 \\}$.\n  - Latent topics: $K = 2$.\n  - Top genes per topic: $T = 5$.\n  - Construct $W_{\\text{true}} \\in \\mathbb{R}_{\\ge 0}^{30 \\times 2}$ with columns normalized to sum to $1$ as follows: for topic $1$, add $5$ to entries for $\\text{GO\\_D}$, and for topic $2$, add $5$ to entries for $\\text{GO\\_E}$, starting from all ones in each column and then normalizing columns to sum to $1$.\n  - Construct $H_{\\text{true}} \\in \\mathbb{R}_{\\ge 0}^{2 \\times 20}$ with $C = 20$ cells: the first $10$ cells assigned to topic $1$ and the remaining $10$ to topic $2$. For a cell assigned to topic $t$, set $H_{\\text{true}}(t, c) = 150$ and $H_{\\text{true}}(k \\ne t, c) = 30$.\n  - Generate counts $X \\sim \\text{Poisson}(W_{\\text{true}} H_{\\text{true}})$ using a fixed pseudo-random seed of $29$.\n  - Fit the topic model with $K = 2$ using the same procedure as in test case $1$ with a fixed pseudo-random seed of $4321$ and $300$ iterations.\n  - Compute the overall coherence as described with $\\epsilon = 10^{-300}$.\n\nImplementation requirements:\n\n- You must implement the topic model estimation using only nonnegative factors and an objective consistent with the Poisson likelihood, and you must not use any external machine learning libraries.\n- For enrichment, under the hypergeometric model with population size $M = G$, population successes $K_j = |S_j|$, and sample size $n = T$, compute the one-sided tail probability for observing at least $m$ overlaps.\n- The final output format must be a single line containing the three overall coherence scores for the three test cases in order, as a comma-separated list enclosed in square brackets. Each score must be rounded to six decimal places. For example, an output could look like $[0.123456,0.234567,0.345678]$ but with values determined by your implementation.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\").", "solution": "The problem asks for the implementation and evaluation of an unsupervised topic model for single-cell gene expression count data. The solution involves three main stages for each test case: data generation, model fitting, and coherence evaluation.\n\n### 1. Theoretical Framework\n\nThe core assumption is that the observed count $X_{gc}$ for gene $g$ in cell $c$ is a sample from a Poisson distribution, $X_{gc} \\sim \\text{Poisson}(\\lambda_{gc})$. The rate parameter $\\lambda_{gc}$ is modeled as a linear combination of $K$ latent topics. This is expressed through a matrix factorization of the rate matrix $\\Lambda = (\\lambda_{gc})$. We seek to find two non-negative matrices, $W \\in \\mathbb{R}_{\\ge 0}^{G \\times K}$ and $H \\in \\mathbb{R}_{\\ge 0}^{K \\times C}$, such that their product approximates the rate matrix:\n$$ \\Lambda = WH $$\nHere, $G$ is the number of genes, $C$ is the number of cells, and $K$ is the number of topics. The matrix $W$ is the \"topic-gene\" matrix, where $W_{gk}$ represents the weight of gene $g$ in topic $k$. The matrix $H$ is the \"cell-topic\" matrix, where $H_{kc}$ represents the contribution of topic $k$ to cell $c$.\n\n### 2. Model Fitting: Non-negative Matrix Factorization (NMF)\n\nThe parameters $W$ and $H$ are estimated by maximizing the log-likelihood of the observed data $X$ under the Poisson model. The log-likelihood function is:\n$$ \\mathcal{L}(W,H | X) = \\sum_{g=1}^{G} \\sum_{c=1}^{C} \\left( X_{gc} \\log(\\lambda_{gc}) - \\lambda_{gc} - \\log(X_{gc}!) \\right) $$\nSubstituting $\\lambda_{gc} = (WH)_{gc}$, and dropping the constant term $\\log(X_{gc}!)$, maximizing the log-likelihood is equivalent to minimizing the following objective function:\n$$ D(W,H) = \\sum_{g=1}^{G} \\sum_{c=1}^{C} \\left( (WH)_{gc} - X_{gc} \\log((WH)_{gc}) \\right) $$\nThis objective is closely related to the generalized Kullback-Leibler (KL) divergence between $X$ and $WH$:\n$$ D_{KL}(X || WH) = \\sum_{g=1}^{G} \\sum_{c=1}^{C} \\left( X_{gc} \\log\\frac{X_{gc}}{(WH)_{gc}} - X_{gc} + (WH)_{gc} \\right) $$\nMinimizing $D_{KL}(X || WH)$ is equivalent to maximizing the Poisson log-likelihood. A standard algorithm to solve this non-negative matrix factorization problem is based on multiplicative updates, which guarantee non-negativity and converge to a local minimum. The update rules for $W$ and $H$ are derived from the gradient of the objective function:\n$$ H_{kc} \\leftarrow H_{kc} \\frac{\\sum_{g=1}^G W_{gk} \\frac{X_{gc}}{(WH)_{gc}}}{\\sum_{g=1}^G W_{gk}} $$\n$$ W_{gk} \\leftarrow W_{gk} \\frac{\\sum_{c=1}^C H_{kc} \\frac{X_{gc}}{(WH)_{gc}}}{\\sum_{c=1}^C H_{kc}} $$\nThese updates are performed iteratively, starting from random initial non-negative matrices $W$ and $H$. For numerical stability, a small positive constant $\\delta$ is added to denominators and to the matrix product $WH$ to prevent division by zero. The process is run for a fixed number of iterations.\n\n### 3. Coherence Evaluation\n\nAfter fitting the model, the biological relevance of the learned topics is assessed. This is done by measuring the \"coherence\" of each topic with respect to a known set of gene annotations, in this case, Gene Ontology (GO) terms.\n\n**Gene Ranking:** For each topic $k$, the genes are ranked in descending order based on their weights in the corresponding column of the $W$ matrix, $W_{:k}$. The set of the top $T$ genes is selected for further analysis.\n\n**Enrichment Analysis:** The coherence of a topic is quantified by the statistical enrichment of its top-$T$ gene set for any of the predefined GO terms. The null hypothesis is that the top $T$ genes are a random sample drawn without replacement from the universe of $G$ genes. Under this null, the number of overlapping genes between a topic's top-$T$ list and a GO term set $S_j$ follows a hypergeometric distribution.\n\nThe parameters of the hypergeometric test are:\n- Population size, $M$: The total number of genes, $G$.\n- Number of successes in the population, $K_j$: The size of the GO term set, $|S_j|$.\n- Sample size, $n$: The number of top genes selected, $T$.\n- Observed successes, $m_{kj}$: The number of genes in the intersection of the top-$T$ set for topic $k$ and the GO set $S_j$.\n\nThe one-sided p-value, $p_{kj}$, is the probability of observing an overlap of $m_{kj}$ or more by chance:\n$$ p_{kj} = P(\\text{overlap} \\ge m_{kj}) = \\sum_{i=m_{kj}}^{\\min(T, |S_j|)} \\frac{\\binom{|S_j|}{i} \\binom{G-|S_j|}{T-i}}{\\binom{G}{T}} $$\nThis is calculated using the survival function (sf) of the hypergeometric distribution.\n\n**Coherence Score:** For each topic $k$, we find the most significant enrichment by taking the minimum p-value over all GO terms $S_j$: $p_{k, \\min} = \\min_j p_{kj}$. The coherence score for topic $k$ is then defined as:\n$$ C_k = -\\log_{10}(p_{k, \\min} + \\epsilon) $$\nThe small constant $\\epsilon$ (given as $10^{-300}$) prevents taking the logarithm of zero. A higher score indicates a more coherent topic.\n\nFinally, the overall coherence for the entire model is the arithmetic mean of the individual topic coherence scores:\n$$ \\bar{C} = \\frac{1}{K} \\sum_{k=1}^K C_k $$\n\n### 4. Implementation for Test Cases\nThe specified test cases are processed following the above methodology.\n1.  **Data Generation**: For each test case, the count matrix $X$ is generated according to the specified parameters (structured or random) and random number generator seeds.\n2.  **Model Fitting**: The `nmf_kl` function is applied to the generated matrix $X$ with the specified number of topics $K$, initialization seed, and number of iterations, yielding the estimated $W$ and $H$ matrices.\n3.  **Coherence Calculation**: The `calculate_coherence` function takes the estimated $W$ matrix, the GO term definitions, and other parameters ($T, G, \\epsilon$) to compute the final overall coherence score.\nThis procedure is repeated for all three test cases, and the results are formatted as requested.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import hypergeom\n\n# A small constant for numerical stability in NMF updates.\nNMF_EPS = 1e-9\n\ndef nmf_kl(X, n_components, seed, max_iter=300):\n    \"\"\"\n    Performs Non-negative Matrix Factorization using Kullback-Leibler\n    divergence with multiplicative updates.\n\n    Args:\n        X (np.ndarray): The input data matrix (G x C).\n        n_components (int): The number of topics (K).\n        seed (int): The seed for random initialization.\n        max_iter (int): The number of update iterations.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: The estimated W and H matrices.\n    \"\"\"\n    G, C = X.shape\n    rng = np.random.default_rng(seed)\n    \n    # Initialize W and H with random positive values scaled to prevent\n    # large initial lambda values destroying the count matrix structure.\n    W = rng.random((G, n_components), dtype=np.float64)\n    H = rng.random((n_components, C), dtype=np.float64)\n    \n    for _ in range(max_iter):\n        # Update rule for H\n        Lambda = W @ H + NMF_EPS\n        H_numerator = W.T @ (X / Lambda)\n        H_denominator = W.sum(axis=0)[:, np.newaxis]\n        H *= (H_numerator / (H_denominator + NMF_EPS))\n        \n        # Update rule for W\n        Lambda = W @ H + NMF_EPS\n        W_numerator = (X / Lambda) @ H.T\n        W_denominator = H.sum(axis=1) # Shape (K,) broadcast to (G, K)\n        W *= (W_numerator / (W_denominator + NMF_EPS))\n        \n    return W, H\n\ndef calculate_coherence(W, go_sets, T, G, epsilon):\n    \"\"\"\n    Calculates the overall coherence score based on GO term enrichment.\n\n    Args:\n        W (np.ndarray): The topic-gene matrix (G x K).\n        go_sets (list[set]): A list of GO terms, each a set of gene indices.\n        T (int): The number of top genes to consider per topic.\n        G (int): The total number of genes in the universe.\n        epsilon (float): A small constant for the log calculation.\n\n    Returns:\n        float: The mean coherence score over all topics.\n    \"\"\"\n    K = W.shape[1]\n    topic_coherences = []\n    \n    for k in range(K):\n        # Get top T genes for topic k\n        top_gene_indices = np.argsort(W[:, k])[::-1][:T]\n        \n        min_p_value = 1.0\n        \n        for go_set in go_sets:\n            K_j = len(go_set)  # Population successes\n            m_kj = len(set(top_gene_indices) & go_set)  # Observed successes\n            \n            # Hypergeometric p-value for m_kj or more overlaps\n            # P(X >= m_kj) = 1 - P(X <= m_kj - 1) = sf(m_kj - 1)\n            # M = G (population size), n = K_j (successes in pop), N = T (sample size)\n            if m_kj > 0:\n                p_val = hypergeom.sf(m_kj - 1, G, K_j, T)\n            else:\n                p_val = 1.0\n\n            if p_val < min_p_value:\n                min_p_value = p_val\n                \n        topic_score = -np.log10(min_p_value + epsilon)\n        topic_coherences.append(topic_score)\n        \n    return np.mean(topic_coherences)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        {\n            \"G\": 60, \"C\": 80, \"K\": 3, \"T\": 10,\n            \"go_sets\": [set(range(15)), set(range(20, 35)), set(range(40, 55))],\n            \"data_gen\": {\n                \"type\": \"structured\", \"seed\": 7,\n                \"W_spec\": [(set(range(15)), 0), (set(range(20, 35)), 1), (set(range(40, 55)), 2)],\n                \"H_spec\": [(range(0, 27), 0, 200, 50), (range(27, 54), 1, 200, 50), (range(54, 80), 2, 200, 50)]\n            },\n            \"fit_params\": {\"seed\": 1234, \"iter\": 300}, \"epsilon\": 1e-300\n        },\n        # Test case 2\n        {\n            \"G\": 60, \"C\": 80, \"K\": 3, \"T\": 10,\n            \"go_sets\": [set(range(15)), set(range(20, 35)), set(range(40, 55))],\n            \"data_gen\": {\"type\": \"random\", \"seed\": 17, \"lambda\": 3},\n            \"fit_params\": {\"seed\": 2024, \"iter\": 300}, \"epsilon\": 1e-300\n        },\n        # Test case 3\n        {\n            \"G\": 30, \"C\": 20, \"K\": 2, \"T\": 5,\n            \"go_sets\": [set(range(10)), set(range(10, 20)), set(range(20, 30))],\n            \"data_gen\": {\n                \"type\": \"structured\", \"seed\": 29,\n                \"W_spec\": [(set(range(10)), 0), (set(range(10, 20)), 1)],\n                \"H_spec\": [(range(0, 10), 0, 150, 30), (range(10, 20), 1, 150, 30)]\n            },\n            \"fit_params\": {\"seed\": 4321, \"iter\": 300}, \"epsilon\": 1e-300\n        }\n    ]\n    \n    results = []\n    for case in test_cases:\n        G, C, K, T = case[\"G\"], case[\"C\"], case[\"K\"], case[\"T\"]\n        go_sets = case[\"go_sets\"]\n        data_gen_params = case[\"data_gen\"]\n        \n        # --- Data Generation ---\n        rng_data = np.random.default_rng(data_gen_params[\"seed\"])\n        if data_gen_params[\"type\"] == \"structured\":\n            W_true = np.ones((G, K), dtype=np.float64)\n            for gene_set, k_idx in data_gen_params[\"W_spec\"]:\n                for g_idx in gene_set:\n                    W_true[g_idx, k_idx] += 5\n            W_true /= W_true.sum(axis=0)\n            \n            H_true = np.zeros((K, C), dtype=np.float64)\n            for cell_range, k_idx, high_val, low_val in data_gen_params[\"H_spec\"]:\n                for k_iter in range(K):\n                    H_true[k_iter, cell_range] = low_val\n                H_true[k_idx, cell_range] = high_val\n            \n            Lambda = W_true @ H_true\n            X = rng_data.poisson(Lambda)\n        elif data_gen_params[\"type\"] == \"random\":\n            X = rng_data.poisson(data_gen_params[\"lambda\"], size=(G, C))\n            \n        # --- Topic Model Fitting ---\n        fit_params = case[\"fit_params\"]\n        W_fit, _ = nmf_kl(X, K, fit_params[\"seed\"], fit_params[\"iter\"])\n        \n        # --- Coherence Calculation ---\n        coherence = calculate_coherence(W_fit, go_sets, T, G, case[\"epsilon\"])\n        results.append(coherence)\n        \n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "4397356"}]}