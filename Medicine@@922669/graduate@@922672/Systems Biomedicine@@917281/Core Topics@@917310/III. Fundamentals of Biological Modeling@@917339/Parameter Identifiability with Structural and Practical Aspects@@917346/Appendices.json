{"hands_on_practices": [{"introduction": "Understanding the root cause of non-identifiability is the first step towards building reliable models. This exercise provides a foundational look into this issue using a simple exponential growth model. You will explore how a 'scaling symmetry' in the parameters—where multiple parameter sets produce the exact same model output—leads directly to structural non-identifiability. By connecting this abstract symmetry to the concrete mathematical properties of the Fisher Information Matrix, this practice illuminates why certain parameter combinations are impossible to estimate independently, no matter how much data is collected [@problem_id:4372101].", "problem": "Consider the single-state linear system in a biochemical signaling motif where a measurable species $x(t)$ obeys the ordinary differential equation $\\dot{x}(t)=\\alpha \\beta x(t)$ with output $y(t)=x(t)$. The initial condition $x(0)=x_{0}>0$ is known, and the parameters $\\alpha>0$ and $\\beta>0$ are unknown. Assume continuous-time measurements $y(t)$ are available over a finite window $t\\in[0,T]$ under two regimes: (i) ideal, noiseless observation and (ii) additive measurement noise modeled as zero-mean Gaussian with constant variance $\\sigma^{2}>0$ independent of $t$. \n\nUsing foundational definitions: structural identifiability means that distinct parameter values generate distinct outputs for all admissible inputs and initial conditions, and a symmetry of the parameterized model is a transformation of parameters that leaves the input-output behavior invariant. Practical identifiability can be assessed in the small-noise limit by the Fisher Information Matrix (FIM), which quantifies local curvature of the likelihood and relates to estimator variance through the Cramér–Rao lower bound.\n\nTasks:\n- From first principles, show that the parameterized model admits a nontrivial one-parameter scaling symmetry in $(\\alpha,\\beta)$ that leaves $y(t)$ invariant for all $t$. Derive the infinitesimal generator of this symmetry as a vector field on the parameter manifold.\n- Use the invariants of this symmetry to determine the structurally identifiable combinations of $(\\alpha,\\beta)$ under noiseless observation and conclude which functions of $(\\alpha,\\beta)$ are structurally identifiable.\n- Construct an identifiable reparameterization by introducing a single parameter $\\phi=\\alpha\\beta$ and write the resulting model in terms of $\\phi$.\n- Under the Gaussian noise regime, write down the log-likelihood for continuous-time observation over $[0,T]$ and derive the Fisher Information Matrix for $(\\alpha,\\beta)$ and the Fisher Information scalar for the identifiable parameter $\\phi$. Show that the Fisher Information Matrix for $(\\alpha,\\beta)$ is rank-deficient (reflecting non-identifiability) while the Fisher Information for $\\phi$ is positive.\n- Provide, as your final calculated answer, the explicit expression for the reparameterized output $y(t)$ in terms of the identifiable parameter $\\phi$, the initial condition $x_{0}$, and time $t$. Your final answer must be a single closed-form analytic expression with no units. No rounding is required.", "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n-   **Model Equation**: $\\dot{x}(t)=\\alpha \\beta x(t)$\n-   **Output Equation**: $y(t)=x(t)$\n-   **Initial Condition**: $x(0)=x_{0}>0$, known.\n-   **Parameters**: $\\alpha>0$, $\\beta>0$, unknown.\n-   **Observation Interval**: $t \\in [0,T]$.\n-   **Observation Regimes**:\n    -   (i) Ideal, noiseless observation.\n    -   (ii) Additive, zero-mean Gaussian measurement noise with constant variance $\\sigma^{2}>0$.\n-   **Definitions**: Structural identifiability, model symmetry, practical identifiability via Fisher Information Matrix (FIM).\n-   **Tasks**:\n    1.  Derive the one-parameter scaling symmetry and its infinitesimal generator.\n    2.  Determine structurally identifiable combinations using symmetry invariants.\n    3.  Construct an identifiable reparameterization with $\\phi=\\alpha\\beta$.\n    4.  Derive the FIM for $(\\alpha,\\beta)$ and for $\\phi$, and analyze their rank/positivity.\n    5.  Provide the final expression for the reparameterized output $y(t)$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is based on a standard linear ordinary differential equation, a fundamental tool in modeling dynamic systems. The concepts of structural and practical identifiability, symmetry analysis, and the Fisher Information Matrix are cornerstone principles in systems theory and parameter estimation, particularly within systems biomedicine. The problem is firmly rooted in established mathematical and statistical theory.\n-   **Well-Posed**: The problem is well-defined. The initial value problem for the ODE has a unique solution. Each task is a specific, answerable question based on the provided model and definitions.\n-   **Objective**: The problem is stated using precise mathematical language and is free from subjective or ambiguous terminology.\n-   **Completeness and Consistency**: The problem is self-contained, providing all necessary equations, conditions, and conceptual definitions to perform the required analysis. There are no internal contradictions.\n-   **Realism and Feasibility**: While the model is a simplification, it represents a valid theoretical framework for identifiability analysis. The assumptions (e.g., continuous-time observation, known initial condition) are standard idealizations in theoretical studies.\n-   **Structure and Triviality**: The problem is well-structured, guiding the analysis from structural (noiseless) to practical (noisy) identifiability. It correctly connects the abstract concept of model symmetries to the practical implications for parameter estimation, as reflected in the FIM. It is a non-trivial pedagogical problem that illustrates fundamental concepts.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically sound, well-posed, and complete. I will proceed with the detailed solution.\n\n### Solution Derivation\n\nThe system is described by the linear first-order ordinary differential equation (ODE):\n$$\n\\frac{dx}{dt} = (\\alpha\\beta) x(t)\n$$\nwith the initial condition $x(0) = x_0$. This is a separable ODE whose solution is found by integration:\n$$\n\\int_{x_0}^{x(t)} \\frac{dx}{x} = \\int_0^t \\alpha\\beta \\,d\\tau\n$$\n$$\n\\ln(x(t)) - \\ln(x_0) = \\alpha\\beta t\n$$\n$$\nx(t) = x_0 \\exp(\\alpha\\beta t)\n$$\nThe system output is $y(t) = x(t)$, so the input-output relationship is given by:\n$$\ny(t) = y(\\alpha, \\beta; t) = x_0 \\exp(\\alpha\\beta t)\n$$\n\n**Task 1: Symmetry Analysis**\n\nA symmetry of the model is a transformation of the parameters $(\\alpha, \\beta) \\to (\\tilde{\\alpha}, \\tilde{\\beta})$ that leaves the output $y(t)$ invariant for all $t \\in [0,T]$.\n$$\ny(\\tilde{\\alpha}, \\tilde{\\beta}; t) = y(\\alpha, \\beta; t)\n$$\n$$\nx_0 \\exp(\\tilde{\\alpha}\\tilde{\\beta} t) = x_0 \\exp(\\alpha\\beta t)\n$$\nSince $x_0 > 0$ and this must hold for all $t$, we must have $\\tilde{\\alpha}\\tilde{\\beta} = \\alpha\\beta$. This equation defines the symmetry. We can define a one-parameter family of transformations, indexed by a scalar $c>0$, as:\n$$\n\\tilde{\\alpha}(c) = c \\alpha\n$$\n$$\n\\tilde{\\beta}(c) = \\frac{1}{c} \\beta\n$$\nThis transformation maps a point $(\\alpha, \\beta)$ along a hyperbola in the parameter space, and for any $c$, the product remains invariant: $\\tilde{\\alpha}\\tilde{\\beta} = (c\\alpha)(\\frac{1}{c}\\beta) = \\alpha\\beta$. This is a non-trivial scaling symmetry.\n\nTo find the infinitesimal generator of this symmetry, we consider the transformation near the identity, which corresponds to $c \\approx 1$. Let $c = \\exp(\\epsilon)$ for an infinitesimal $\\epsilon$.\n$$\n\\tilde{\\alpha} = \\alpha \\exp(\\epsilon) \\approx \\alpha(1 + \\epsilon) = \\alpha + \\epsilon\\alpha\n$$\n$$\n\\tilde{\\beta} = \\beta \\exp(-\\epsilon) \\approx \\beta(1 - \\epsilon) = \\beta - \\epsilon\\beta\n$$\nThe infinitesimal change in the parameters is $(\\delta\\alpha, \\delta\\beta) = (\\tilde{\\alpha}-\\alpha, \\tilde{\\beta}-\\beta) = (\\epsilon\\alpha, -\\epsilon\\beta)$. The infinitesimal generator is the vector field $V$ on the parameter manifold defined by $V = (\\frac{\\delta\\alpha}{\\epsilon}, \\frac{\\delta\\beta}{\\epsilon})$.\n$$\nV = (\\alpha, -\\beta)\n$$\nIn differential operator form, this vector field is $V = \\alpha \\frac{\\partial}{\\partial \\alpha} - \\beta \\frac{\\partial}{\\partial \\beta}$.\n\n**Task 2: Structurally Identifiable Combinations**\n\nThe structurally identifiable combinations of parameters are the invariants of the symmetry group. A function $f(\\alpha, \\beta)$ is an invariant if it is constant along the flow generated by $V$. This is equivalent to the Lie derivative of $f$ with respect to $V$ being zero:\n$$\nL_V f = V(f) = \\alpha \\frac{\\partial f}{\\partial \\alpha} - \\beta \\frac{\\partial f}{\\partial \\beta} = 0\n$$\nThis is a first-order partial differential equation. The method of characteristics gives the characteristic equations:\n$$\n\\frac{d\\alpha}{\\alpha} = \\frac{d\\beta}{-\\beta}\n$$\nIntegrating both sides yields $\\ln|\\alpha| = -\\ln|\\beta| + K$ for some constant $K$. Since $\\alpha>0$ and $\\beta>0$, this simplifies to $\\ln(\\alpha) + \\ln(\\beta) = K$, or $\\ln(\\alpha\\beta) = K$. This implies that $\\alpha\\beta$ is constant along the characteristic curves (the flow of $V$). Therefore, any function of the product $\\alpha\\beta$ is an invariant of the symmetry. The fundamental invariant is the product $\\phi = \\alpha\\beta$.\nSince the output $y(t)$ depends only on this invariant combination, it is impossible to distinguish between different pairs $(\\alpha, \\beta)$ that share the same product. Consequently, only functions of $\\alpha\\beta$, denoted $g(\\alpha\\beta)$, are structurally identifiable. The parameters $\\alpha$ and $\\beta$ are not individually identifiable.\n\n**Task 3: Identifiable Reparameterization**\n\nBased on the symmetry analysis, the identifiable parameter is $\\phi = \\alpha\\beta$. We can reparameterize the model using only this new parameter.\nThe original ODE is $\\dot{x}(t) = (\\alpha\\beta) x(t)$. Substituting $\\phi = \\alpha\\beta$, we get the reparameterized model:\n$$\n\\dot{x}(t) = \\phi x(t)\n$$\nwith output $y(t) = x(t)$ and initial condition $x(0) = x_0$. This is a new model with a single parameter $\\phi > 0$. This model is structurally identifiable because distinct values of $\\phi$ will generate distinct output trajectories $y(t) = x_0 \\exp(\\phi t)$.\n\n**Task 4: Fisher Information Matrix (FIM) Analysis**\n\nUnder the additive Gaussian noise regime, the observed data is $y_{obs}(t) = y(t) + \\epsilon(t)$, where $\\epsilon(t)$ is continuous-time white noise. The log-likelihood functional is proportional to:\n$$\nL(\\boldsymbol{\\theta}) \\propto -\\frac{1}{2\\sigma^2} \\int_0^T [y_{obs}(t) - y(t; \\boldsymbol{\\theta})]^2 dt\n$$\nThe Fisher Information Matrix for a parameter vector $\\boldsymbol{\\theta}$ is given by:\n$$\nF_{ij} = \\frac{1}{\\sigma^2} \\int_0^T \\left( \\frac{\\partial y(t; \\boldsymbol{\\theta})}{\\partial \\theta_i} \\right) \\left( \\frac{\\partial y(t; \\boldsymbol{\\theta})}{\\partial \\theta_j} \\right) dt\n$$\nFirst, for the non-identifiable parameterization $\\boldsymbol{\\theta} = (\\alpha, \\beta)$, with $y(t) = x_0 \\exp(\\alpha\\beta t)$:\nThe partial derivatives (sensitivities) are:\n$$\n\\frac{\\partial y}{\\partial \\alpha} = x_0 (\\beta t) \\exp(\\alpha\\beta t)\n$$\n$$\n\\frac{\\partial y}{\\partial \\beta} = x_0 (\\alpha t) \\exp(\\alpha\\beta t)\n$$\nThe components of the FIM, $F_{(\\alpha,\\beta)}$, are:\n$$\nF_{\\alpha\\alpha} = \\frac{1}{\\sigma^2} \\int_0^T [x_0 \\beta t \\exp(\\alpha\\beta t)]^2 dt = \\frac{x_0^2 \\beta^2}{\\sigma^2} \\int_0^T t^2 \\exp(2\\alpha\\beta t) dt\n$$\n$$\nF_{\\beta\\beta} = \\frac{1}{\\sigma^2} \\int_0^T [x_0 \\alpha t \\exp(\\alpha\\beta t)]^2 dt = \\frac{x_0^2 \\alpha^2}{\\sigma^2} \\int_0^T t^2 \\exp(2\\alpha\\beta t) dt\n$$\n$$\nF_{\\alpha\\beta} = F_{\\beta\\alpha} = \\frac{1}{\\sigma^2} \\int_0^T [x_0 \\beta t \\exp(\\alpha\\beta t)][x_0 \\alpha t \\exp(\\alpha\\beta t)] dt = \\frac{x_0^2 \\alpha\\beta}{\\sigma^2} \\int_0^T t^2 \\exp(2\\alpha\\beta t) dt\n$$\nLet $I = \\int_0^T t^2 \\exp(2\\alpha\\beta t) dt$. Since the integrand is non-negative and not identically zero for $T>0$, we have $I>0$. The FIM is:\n$$\nF_{(\\alpha,\\beta)} = \\frac{x_0^2 I}{\\sigma^2} \\begin{pmatrix} \\beta^2 & \\alpha\\beta \\\\ \\alpha\\beta & \\alpha^2 \\end{pmatrix}\n$$\nThe determinant of this matrix is:\n$$\n\\det(F_{(\\alpha,\\beta)}) = \\left(\\frac{x_0^2 I}{\\sigma^2}\\right)^2 (\\beta^2 \\alpha^2 - (\\alpha\\beta)(\\alpha\\beta)) = 0\n$$\nSince $\\det(F_{(\\alpha,\\beta)}) = 0$, the matrix is singular (rank-deficient). This confirms that the parameters $(\\alpha, \\beta)$ are not practically identifiable, as the Cramér–Rao lower bound on the variance of any unbiased estimator would be infinite. The null space of this matrix is spanned by the vector $(\\alpha, -\\beta)^T$, which corresponds to the infinitesimal generator of the symmetry, confirming the connection between structural non-identifiability (symmetry) and practical non-identifiability (singular FIM).\n\nNext, for the identifiable parameterization with parameter $\\phi = \\alpha\\beta$, with $y(t) = x_0 \\exp(\\phi t)$:\nThe sensitivity is:\n$$\n\\frac{\\partial y}{\\partial \\phi} = x_0 t \\exp(\\phi t)\n$$\nThe FIM is a $1 \\times 1$ matrix (a scalar), $F_{\\phi}$:\n$$\nF_{\\phi} = \\frac{1}{\\sigma^2} \\int_0^T \\left(\\frac{\\partial y}{\\partial \\phi}\\right)^2 dt = \\frac{x_0^2}{\\sigma^2} \\int_0^T [t \\exp(\\phi t)]^2 dt = \\frac{x_0^2}{\\sigma^2} \\int_0^T t^2 \\exp(2\\phi t) dt\n$$\nSince $x_0^2 > 0$, $\\sigma^2 > 0$, and the integral $\\int_0^T t^2 \\exp(2\\phi t) dt$ is strictly positive for $T>0$ and $\\phi>0$, we have $F_{\\phi} > 0$. A positive (non-singular) FIM indicates that the parameter $\\phi$ is practically identifiable.\n\n**Task 5: Final Calculated Answer**\n\nThe final task is to provide the explicit expression for the reparameterized output $y(t)$ in terms of the identifiable parameter $\\phi$, the initial condition $x_0$, and time $t$. As derived from solving the reparameterized ODE $\\dot{x}(t) = \\phi x(t)$ with $x(0)=x_0$ and $y(t)=x(t)$, this expression is:\n$$\ny(t) = x_0 \\exp(\\phi t)\n$$\nThis is the required final answer.", "answer": "$$\n\\boxed{x_{0} \\exp(\\phi t)}\n$$", "id": "4372101"}, {"introduction": "Building on the concept of symmetry, this practice explores how the choice of parameterization can determine a model's identifiability. Using a common two-compartment pharmacokinetic model, you will first prove that a linear parameterization is structurally identifiable, meaning its parameters can be uniquely determined from ideal data. You will then investigate a non-linear variant that describes the same physical system but is structurally non-identifiable, demonstrating that identifiability is a property of the model equations, not just the underlying process [@problem_id:4372069].", "problem": "A dual-exponential pharmacokinetic signal in a controlled in vitro system is modeled as $y(t)=\\theta_{1}\\exp(-a t)+\\theta_{2}\\exp(-b t)$, where $a>0$ and $b>0$ are distinct and known rate constants set by the assay conditions, and $t\\in[0,T]$ denotes time. You are told that the measurement process is additive and that under noise-free measurements both coefficients $\\theta_{1}$ and $\\theta_{2}$ represent unknown abundances of two biochemically distinct species. In applications with instrumentation noise, the readout is $z(t)=y(t)+\\varepsilon(t)$, where $\\varepsilon(t)$ is zero-mean Gaussian white noise with constant spectral density $\\sigma^{2}$.\n\nUsing only fundamental definitions and well-tested facts from system identification and statistics:\n\n- Structural identifiability refers to injectivity of the input-output map with respect to parameters from idealized, noise-free data.\n- Under zero-mean Gaussian white noise with constant spectral density, the Fisher Information Matrix (FIM) for continuous-time observation of a parametric output is $J=(1/\\sigma^{2})\\int_{0}^{T} s(t)s(t)^{\\top}\\,dt$, where $s(t)$ is the sensitivity vector $s(t)=\\partial y(t)/\\partial \\theta$.\n\nTasks:\n\n1. Prove structural identifiability of $(\\theta_{1},\\theta_{2})$ by considering two noise-free measurements $y(t_{1})$ and $y(t_{2})$ at any two distinct times $t_{1}\\neq t_{2}$ with $t_{1},t_{2}\\in(0,T]$. Derive explicit expressions for $\\theta_{1}$ and $\\theta_{2}$ in terms of $y(t_{1})$, $y(t_{2})$, $a$, $b$, $t_{1}$, and $t_{2}$.\n\n2. For the noisy continuous-time observation on $[0,T]$, compute the Fisher Information Matrix $J$ for $\\theta=(\\theta_{1},\\theta_{2})$, and derive a closed-form expression for $\\det(J)$ in terms of $a$, $b$, $T$, and $\\sigma$.\n\n3. Consider a nonlinear variant in which two biochemical branches are modulated by a single catalytic efficiency parameter $\\kappa_{1}$ and two branch-specific factors $\\kappa_{2}$ and $\\kappa_{3}$, yielding $y(t)=\\kappa_{1}\\kappa_{2}\\exp(-a t)+\\kappa_{1}\\kappa_{3}\\exp(-b t)$ with the same known $a\\neq b>0$. Using the definition of structural identifiability, determine whether $(\\kappa_{1},\\kappa_{2},\\kappa_{3})$ are structurally identifiable from ideal, noise-free $y(t)$ on $[0,T]$ and justify your conclusion by explicitly constructing a nontrivial parameter transformation (if any) that leaves $y(t)$ invariant.\n\nProvide the final answer as the single closed-form analytic expression for $\\det(J)$ obtained in Task $2$. No numerical approximation is required. Do not include units in the final answer box.", "solution": "The problem statement is validated as follows.\n\n**Step 1: Extract Givens**\n- Model 1 (linear): $y(t)=\\theta_{1}\\exp(-a t)+\\theta_{2}\\exp(-b t)$\n- Model parameters: $\\theta_{1}$ and $\\theta_{2}$ are unknown abundances.\n- Model constants: $a>0$ and $b>0$ are distinct ($a \\neq b$) and known rate constants.\n- Time domain: $t \\in [0, T]$.\n- Noise model: $z(t)=y(t)+\\varepsilon(t)$, where $\\varepsilon(t)$ is zero-mean Gaussian white noise with constant spectral density $\\sigma^{2}$.\n- Definition of structural identifiability: Injectivity of the input-output map with respect to parameters from idealized, noise-free data.\n- Definition of Fisher Information Matrix (FIM) for continuous-time observation: $J=(1/\\sigma^{2})\\int_{0}^{T} s(t)s(t)^{\\top}\\,dt$, where $s(t)=\\partial y(t)/\\partial \\theta$.\n- Task 1: Prove structural identifiability of $(\\theta_{1},\\theta_{2})$ using two noise-free measurements $y(t_{1})$ and $y(t_{2})$ at any two distinct times $t_{1}\\neq t_{2}$ with $t_{1},t_{2}\\in(0,T]$. Derive explicit expressions for $\\theta_{1}$ and $\\theta_{2}$.\n- Task 2: Compute the FIM $J$ for $\\theta=(\\theta_{1},\\theta_{2})$ for noisy continuous-time observation on $[0,T]$, and derive a closed-form expression for $\\det(J)$.\n- Model 2 (nonlinear): $y(t)=\\kappa_{1}\\kappa_{2}\\exp(-a t)+\\kappa_{1}\\kappa_{3}\\exp(-b t)$ with known $a \\neq b > 0$.\n- Model parameters: $(\\kappa_{1}, \\kappa_{2}, \\kappa_{3})$ are unknown.\n- Task 3: Determine if $(\\kappa_{1},\\kappa_{2},\\kappa_{3})$ are structurally identifiable from noise-free $y(t)$ on $[0,T]$ and justify the conclusion.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective.\n- **Scientific and Factual Soundness:** The model $y(t)=\\theta_{1}\\exp(-a t)+\\theta_{2}\\exp(-b t)$ is a standard representation of a two-compartment pharmacokinetic model or parallel first-order reactions, fundamental in systems biology, chemistry, and engineering. The definitions of structural identifiability and the Fisher Information Matrix are standard and correct within system identification and statistical estimation theory.\n- **Well-Posed and Complete:** Each task is clearly specified. The problem provides all necessary equations, definitions, and constraints (e.g., $a \\neq b$) to perform the required derivations. The questions are unambiguous.\n- **Objectivity and Feasibility:** The problem is stated using precise mathematical language, free from subjective claims. The calculations required are standard integral calculus and linear algebra, making the tasks feasible.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Solution Derivation**\n\n**Task 1: Structural Identifiability of $(\\theta_{1},\\theta_{2})$**\nTo prove structural identifiability for the parameters $\\theta = (\\theta_{1}, \\theta_{2})^{\\top}$, we must show that they can be uniquely determined from noise-free output $y(t)$. As instructed, we consider two measurements at two distinct time points, $t_{1} \\neq t_{2}$, where $t_{1}, t_{2} \\in (0, T]$. This yields a system of two linear equations in two unknowns, $\\theta_{1}$ and $\\theta_{2}$:\n$$\ny(t_{1}) = \\theta_{1}\\exp(-a t_{1}) + \\theta_{2}\\exp(-b t_{1})\n$$\n$$\ny(t_{2}) = \\theta_{1}\\exp(-a t_{2}) + \\theta_{2}\\exp(-b t_{2})\n$$\nThis system can be written in matrix form as:\n$$\n\\begin{pmatrix} y(t_{1}) \\\\ y(t_{2}) \\end{pmatrix} = \\begin{pmatrix} \\exp(-a t_{1}) & \\exp(-b t_{1}) \\\\ \\exp(-a t_{2}) & \\exp(-b t_{2}) \\end{pmatrix} \\begin{pmatrix} \\theta_{1} \\\\ \\theta_{2} \\end{pmatrix}\n$$\nA unique solution for $(\\theta_{1}, \\theta_{2})$ exists if and only if the determinant of the coefficient matrix is non-zero. Let the coefficient matrix be $M$. Its determinant is:\n$$\n\\det(M) = \\exp(-a t_{1}) \\exp(-b t_{2}) - \\exp(-b t_{1}) \\exp(-a t_{2}) = \\exp(-(a t_{1} + b t_{2})) - \\exp(-(b t_{1} + a t_{2}))\n$$\nThe determinant is zero if and only if $a t_{1} + b t_{2} = b t_{1} + a t_{2}$, which rearranges to $a(t_{1}-t_{2}) - b(t_{1}-t_{2}) = 0$, or $(a-b)(t_{1}-t_{2}) = 0$. By the problem statement, the rate constants are distinct ($a \\neq b$) and the measurement times are distinct ($t_{1} \\neq t_{2}$). Therefore, $(a-b) \\neq 0$ and $(t_{1}-t_{2}) \\neq 0$, which implies $\\det(M) \\neq 0$.\nSince the coefficient matrix is invertible for any choice of distinct $t_{1}, t_{2}$, the parameters $(\\theta_{1}, \\theta_{2})$ are uniquely determined. This proves they are structurally identifiable.\n\nThe explicit expressions for $\\theta_{1}$ and $\\theta_{2}$ can be found using Cramer's rule or by matrix inversion. Using Cramer's rule:\n$$\n\\theta_{1} = \\frac{\\begin{vmatrix} y(t_{1}) & \\exp(-b t_{1}) \\\\ y(t_{2}) & \\exp(-b t_{2}) \\end{vmatrix}}{\\det(M)} = \\frac{y(t_{1})\\exp(-b t_{2}) - y(t_{2})\\exp(-b t_{1})}{\\exp(-a t_{1}-b t_{2}) - \\exp(-a t_{2}-b t_{1})}\n$$\n$$\n\\theta_{2} = \\frac{\\begin{vmatrix} \\exp(-a t_{1}) & y(t_{1}) \\\\ \\exp(-a t_{2}) & y(t_{2}) \\end{vmatrix}}{\\det(M)} = \\frac{y(t_{2})\\exp(-a t_{1}) - y(t_{1})\\exp(-a t_{2})}{\\exp(-a t_{1}-b t_{2}) - \\exp(-a t_{2}-b t_{1})}\n$$\n\n**Task 2: Fisher Information Matrix for $\\theta=(\\theta_{1},\\theta_{2})$**\nThe parameter vector is $\\theta = (\\theta_{1}, \\theta_{2})^{\\top}$. The sensitivity vector $s(t)$ is the vector of partial derivatives of the output $y(t)$ with respect to the parameters:\n$$\ns(t) = \\frac{\\partial y(t)}{\\partial \\theta} = \\begin{pmatrix} \\frac{\\partial y(t)}{\\partial \\theta_{1}} \\\\ \\frac{\\partial y(t)}{\\partial \\theta_{2}} \\end{pmatrix} = \\begin{pmatrix} \\exp(-a t) \\\\ \\exp(-b t) \\end{pmatrix}\n$$\nThe Fisher Information Matrix $J$ is given by $J = (1/\\sigma^{2})\\int_{0}^{T} s(t)s(t)^{\\top}\\,dt$. First, we compute the outer product $s(t)s(t)^{\\top}$:\n$$\ns(t)s(t)^{\\top} = \\begin{pmatrix} \\exp(-a t) \\\\ \\exp(-b t) \\end{pmatrix} \\begin{pmatrix} \\exp(-a t) & \\exp(-b t) \\end{pmatrix} = \\begin{pmatrix} \\exp(-2at) & \\exp(-(a+b)t) \\\\ \\exp(-(a+b)t) & \\exp(-2bt) \\end{pmatrix}\n$$\nNow, we integrate each element of this matrix from $t=0$ to $t=T$:\n$$\nJ_{11} = \\frac{1}{\\sigma^{2}}\\int_{0}^{T} \\exp(-2at) \\,dt = \\frac{1}{\\sigma^{2}}\\left[-\\frac{1}{2a}\\exp(-2at)\\right]_{0}^{T} = \\frac{1-\\exp(-2aT)}{2a\\sigma^{2}}\n$$\n$$\nJ_{22} = \\frac{1}{\\sigma^{2}}\\int_{0}^{T} \\exp(-2bt) \\,dt = \\frac{1}{\\sigma^{2}}\\left[-\\frac{1}{2b}\\exp(-2bt)\\right]_{0}^{T} = \\frac{1-\\exp(-2bT)}{2b\\sigma^{2}}\n$$\n$$\nJ_{12} = J_{21} = \\frac{1}{\\sigma^{2}}\\int_{0}^{T} \\exp(-(a+b)t) \\,dt = \\frac{1}{\\sigma^{2}}\\left[-\\frac{1}{a+b}\\exp(-(a+b)t)\\right]_{0}^{T} = \\frac{1-\\exp(-(a+b)T)}{(a+b)\\sigma^{2}}\n$$\nThe full Fisher Information Matrix is:\n$$\nJ = \\frac{1}{\\sigma^{2}} \\begin{pmatrix} \\frac{1-\\exp(-2aT)}{2a} & \\frac{1-\\exp(-(a+b)T)}{a+b} \\\\ \\frac{1-\\exp(-(a+b)T)}{a+b} & \\frac{1-\\exp(-2bT)}{2b} \\end{pmatrix}\n$$\nFinally, we compute the determinant of $J$:\n$$\n\\det(J) = J_{11}J_{22} - J_{12}^{2}\n$$\n$$\n\\det(J) = \\left(\\frac{1-\\exp(-2aT)}{2a\\sigma^{2}}\\right)\\left(\\frac{1-\\exp(-2bT)}{2b\\sigma^{2}}\\right) - \\left(\\frac{1-\\exp(-(a+b)T)}{(a+b)\\sigma^{2}}\\right)^{2}\n$$\n$$\n\\det(J) = \\frac{1}{\\sigma^{4}} \\left( \\frac{(1-\\exp(-2aT))(1-\\exp(-2bT))}{4ab} - \\frac{(1-\\exp(-(a+b)T))^{2}}{(a+b)^{2}} \\right)\n$$\nThis is the required closed-form expression.\n\n**Task 3: Structural Identifiability of $(\\kappa_{1},\\kappa_{2},\\kappa_{3})$**\nThe nonlinear model is $y(t)=\\kappa_{1}\\kappa_{2}\\exp(-a t)+\\kappa_{1}\\kappa_{3}\\exp(-b t)$. This model can be mapped to the linear model by defining new parameters:\n$$\n\\theta_{1} = \\kappa_{1}\\kappa_{2}\n$$\n$$\n\\theta_{2} = \\kappa_{1}\\kappa_{3}\n$$\nFrom Task 1, we established that the parameters $(\\theta_{1}, \\theta_{2})$ are structurally identifiable. The question of whether $(\\kappa_{1}, \\kappa_{2}, \\kappa_{3})$ are identifiable is equivalent to asking if the mapping from $(\\kappa_{1}, \\kappa_{2}, \\kappa_{3})$ to $(\\theta_{1}, \\theta_{2})$ is injective. We have a system of $2$ equations for $3$ unknowns. This system is underdetermined, which suggests that the parameters are not identifiable.\n\nTo prove this formally, we must construct a nontrivial parameter transformation that leaves the output $y(t)$ invariant. Let $\\alpha$ be an arbitrary non-zero and non-unity constant ($\\alpha \\neq 0, \\alpha \\neq 1$). Consider a new set of parameters $(\\kappa'_{1}, \\kappa'_{2}, \\kappa'_{3})$ defined by the transformation:\n$$\n\\kappa'_{1} = \\alpha \\kappa_{1}\n$$\n$$\n\\kappa'_{2} = \\frac{1}{\\alpha} \\kappa_{2}\n$$\n$$\n\\kappa'_{3} = \\frac{1}{\\alpha} \\kappa_{3}\n$$\nThe original parameter set is $(\\kappa_{1}, \\kappa_{2}, \\kappa_{3})$. The new parameter set $(\\kappa'_{1}, \\kappa'_{2}, \\kappa'_{3})$ is different from the original set since $\\alpha \\neq 1$.\nNow we compute the values of $\\theta_{1}$ and $\\theta_{2}$ for this new parameter set, which we denote $\\theta'_{1}$ and $\\theta'_{2}$:\n$$\n\\theta'_{1} = \\kappa'_{1} \\kappa'_{2} = (\\alpha \\kappa_{1}) \\left(\\frac{1}{\\alpha} \\kappa_{2}\\right) = \\kappa_{1} \\kappa_{2} = \\theta_{1}\n$$\n$$\n\\theta'_{2} = \\kappa'_{1} \\kappa'_{3} = (\\alpha \\kappa_{1}) \\left(\\frac{1}{\\alpha} \\kappa_{3}\\right) = \\kappa_{1} \\kappa_{3} = \\theta_{2}\n$$\nSince $(\\theta'_{1}, \\theta'_{2}) = (\\theta_{1}, \\theta_{2})$, the output function $y(t)$ remains unchanged under this parameter transformation. For any given set of parameters $(\\kappa_{1}, \\kappa_{2}, \\kappa_{3})$, there exists an infinite family of parameter sets $(\\alpha\\kappa_{1}, \\kappa_{2}/\\alpha, \\kappa_{3}/\\alpha)$ that produce the exact same output $y(t)$. Therefore, it is impossible to uniquely determine $(\\kappa_{1}, \\kappa_{2}, \\kappa_{3})$ from measurements of $y(t)$.\nConclusion: The parameter set $(\\kappa_{1},\\kappa_{2},\\kappa_{3})$ is structurally non-identifiable.", "answer": "$$\n\\boxed{\\frac{1}{\\sigma^{4}} \\left[ \\frac{\\left(1-\\exp(-2aT)\\right)\\left(1-\\exp(-2bT)\\right)}{4ab} - \\frac{\\left(1-\\exp(-(a+b)T)\\right)^{2}}{(a+b)^{2}} \\right]}\n$$", "id": "4372069"}, {"introduction": "Structural identifiability tells us what is possible with perfect data, but practical identifiability tells us what is achievable with real, noisy measurements. This exercise transitions from theory to application by focusing on quantifying parameter uncertainty from a given dataset. You will use the profile likelihood method, a cornerstone of statistical modeling, to calculate a data-driven confidence interval for a parameter in a simple decay model, providing a direct measure of its practical identifiability [@problem_id:4372099].", "problem": "Consider a simple biomarker decay experiment in systems biomedicine in which a molecular readout is modeled as a single-parameter exponentially decaying trajectory with additive Gaussian noise: the noiseless trajectory is $y(t)=\\theta \\exp(-t)$, and the measured data satisfy $y_{i}=y(t_{i})+\\varepsilon_{i}$ with $\\varepsilon_{i}\\sim\\mathcal{N}(0,\\sigma^{2})$ independent and identically distributed. The amplitude parameter $\\theta$ is unknown, and the noise standard deviation $\\sigma$ is known. You are given $n=5$ measurements at times $t_{1}=0$, $t_{2}=1$, $t_{3}=2$, $t_{4}=3$, $t_{5}=4$, with observed values $y_{1}=2.45$, $y_{2}=0.95$, $y_{3}=0.31$, $y_{4}=0.12$, $y_{5}=0.06$, and known $\\sigma=0.05$. The measurement is dimensionless.\n\nStarting from the Gaussian likelihood definition and the linear-in-parameter structure of the model, do the following:\n\n- Establish whether $\\theta$ is structurally identifiable from noiseless observations of $y(t)$ over $t\\geq 0$, using only fundamental model properties and definitions.\n- Construct the profile likelihood $L_{p}(\\theta)$ for $\\theta$ (which, in this single-parameter case with known $\\sigma$, coincides with the full likelihood) up to a multiplicative constant, and write its corresponding negative twice the log-likelihood as a function of $\\theta$ and the data.\n- Use the likelihood-ratio framework for one parameter to derive an approximate confidence interval for $\\theta$ from the profile likelihood via the threshold $\\Delta\\chi^{2}=1$ (i.e., the set of $\\theta$ values for which the increase in negative twice the log-likelihood relative to its minimum does not exceed $1$).\n- Evaluate this confidence interval numerically for the given data. Round each endpoint to four significant figures. Express the interval in the same (dimensionless) units as $y$.", "solution": "The problem statement will first be validated for scientific soundness, completeness, and objectivity.\n\n### Step 1: Extract Givens\nThe givens extracted verbatim from the problem statement are:\n-   Noiseless trajectory model: $y(t)=\\theta \\exp(-t)$\n-   Measurement model: $y_{i}=y(t_{i})+\\varepsilon_{i}$\n-   Noise distribution: $\\varepsilon_{i}\\sim\\mathcal{N}(0,\\sigma^{2})$, independent and identically distributed\n-   Unknown parameter: $\\theta$\n-   Known noise standard deviation: $\\sigma=0.05$\n-   Number of measurements: $n=5$\n-   Measurement times: $t_{1}=0$, $t_{2}=1$, $t_{3}=2$, $t_{4}=3$, $t_{5}=4$\n-   Observed values: $y_{1}=2.45$, $y_{2}=0.95$, $y_{3}=0.31$, $y_{4}=0.12$, $y_{5}=0.06$\n-   Units: The measurement is dimensionless.\n-   Confidence interval threshold: $\\Delta\\chi^{2}=1$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to validation against established criteria.\n-   **Scientifically Grounded:** The problem is based on fundamental principles. Exponential decay is a ubiquitous model in science and engineering. The assumption of additive, independent, and identically distributed Gaussian noise is a standard and well-established procedure in statistical data modeling and parameter estimation.\n-   **Well-Posed:** The problem is clearly specified. It asks for a determination of structural identifiability and the calculation of a confidence interval for a single parameter using a standard statistical method (likelihood-ratio framework). The data provided are sufficient for a unique solution.\n-   **Objective:** The problem is stated using precise, quantitative, and unbiased language.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity. It is a standard, well-posed problem in parameter estimation relevant to systems biomedicine.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be derived.\n\n### Solution Derivation\n\nThe solution is presented in four parts as requested by the problem statement.\n\nFirst, the structural identifiability of the parameter $\\theta$ is established. Structural identifiability concerns the uniqueness of parameters for a given model structure, assuming perfect, noise-free observations over a continuous time domain. To check if $\\theta$ is structurally identifiable, we assume two parameter values, $\\theta_{1}$ and $\\theta_{2}$, yield the same model output for all time $t \\ge 0$.\n$$\ny(t; \\theta_{1}) = y(t; \\theta_{2})\n$$\n$$\n\\theta_{1} \\exp(-t) = \\theta_{2} \\exp(-t)\n$$\nSince the function $\\exp(-t)$ is not identically zero for $t \\ge 0$, we can divide both sides by $\\exp(-t)$, which yields:\n$$\n\\theta_{1} = \\theta_{2}\n$$\nThis demonstrates that it is impossible to find two distinct values of $\\theta$ that produce the same noiseless trajectory. Therefore, the parameter $\\theta$ is structurally identifiable.\n\nSecond, we construct the likelihood function. The measurements $y_i$ are independent, and each is drawn from a Gaussian distribution with mean $\\theta \\exp(-t_i)$ and known variance $\\sigma^2$. The probability density function for a single measurement $y_i$ given $\\theta$ is:\n$$\np(y_i|\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\theta \\exp(-t_i))^2}{2\\sigma^2}\\right)\n$$\nThe likelihood function $L(\\theta)$ is the product of the probabilities of observing the entire dataset $\\{y_i\\}_{i=1}^n$:\n$$\nL(\\theta) = \\prod_{i=1}^{n} p(y_i|\\theta) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\theta \\exp(-t_i))^2\\right)\n$$\nSince the problem concerns a single unknown parameter $\\theta$ with $\\sigma$ known, the profile likelihood $L_p(\\theta)$ is identical to the full likelihood $L(\\theta)$. Up to a multiplicative constant, the profile likelihood is:\n$$\nL_p(\\theta) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\theta \\exp(-t_i))^2\\right)\n$$\nThe corresponding negative twice the log-likelihood function, which we denote as $\\chi^2(\\theta)$, is derived from $L(\\theta)$:\n$$\n\\ln L(\\theta) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\theta \\exp(-t_i))^2\n$$\n$$\n\\chi^2(\\theta) = -2\\ln L(\\theta) = n\\ln(2\\pi\\sigma^2) + \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\theta \\exp(-t_i))^2\n$$\nAs is conventional, the term $\\chi^2(\\theta)$ often refers to the sum-of-squares part, which is proportional to the negative log-likelihood omitting the constant term. The expression as a function of $\\theta$ and the data is:\n$$\n\\chi^2(\\theta) = \\sum_{i=1}^{n} \\frac{(y_i - \\theta \\exp(-t_i))^2}{\\sigma^2}\n$$\n\nThird, we derive the confidence interval. The confidence interval is found from the set of $\\theta$ values that satisfy the condition $\\chi^2(\\theta) \\leq \\chi^2_{\\text{min}} + \\Delta\\chi^2$, where $\\chi^2_{\\text{min}}$ is the minimum value of $\\chi^2(\\theta)$ and $\\Delta\\chi^{2}=1$ is the specified threshold. The minimum occurs at the maximum likelihood estimate (MLE) of $\\theta$, denoted $\\hat{\\theta}$. To find $\\hat{\\theta}$, we minimize $\\chi^2(\\theta)$ by setting its derivative with respect to $\\theta$ to zero. Let $x_i = \\exp(-t_i)$.\n$$\n\\frac{d\\chi^2(\\theta)}{d\\theta} = \\frac{d}{d\\theta} \\left[\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\theta x_i)^2\\right] = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} 2(y_i - \\theta x_i)(-x_i) = 0\n$$\n$$\n\\sum_{i=1}^{n} (y_i x_i - \\theta x_i^2) = 0 \\implies \\sum_{i=1}^{n} y_i x_i = \\theta \\sum_{i=1}^{n} x_i^2\n$$\nSolving for $\\hat{\\theta}$:\n$$\n\\hat{\\theta} = \\frac{\\sum_{i=1}^{n} y_i x_i}{\\sum_{i=1}^{n} x_i^2}\n$$\nThe confidence interval is defined by the inequality $\\chi^2(\\theta) - \\chi^2(\\hat{\\theta}) \\leq 1$.\n$$\n\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\theta x_i)^2 - \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\hat{\\theta} x_i)^2 \\leq 1\n$$\nExpanding the sums:\n$$\n(\\sum y_i^2 - 2\\theta \\sum y_i x_i + \\theta^2 \\sum x_i^2) - (\\sum y_i^2 - 2\\hat{\\theta} \\sum y_i x_i + \\hat{\\theta}^2 \\sum x_i^2) \\leq \\sigma^2\n$$\nUsing $\\sum y_i x_i = \\hat{\\theta} \\sum x_i^2$:\n$$\n-2\\theta (\\hat{\\theta} \\sum x_i^2) + \\theta^2 \\sum x_i^2 + 2\\hat{\\theta} (\\hat{\\theta} \\sum x_i^2) - \\hat{\\theta}^2 \\sum x_i^2 \\leq \\sigma^2\n$$\n$$\n\\theta^2 \\sum x_i^2 - 2\\theta\\hat{\\theta} \\sum x_i^2 + \\hat{\\theta}^2 \\sum x_i^2 \\leq \\sigma^2\n$$\n$$\n(\\sum x_i^2) (\\theta^2 - 2\\theta\\hat{\\theta} + \\hat{\\theta}^2) \\leq \\sigma^2\n$$\n$$\n(\\sum x_i^2) (\\theta - \\hat{\\theta})^2 \\leq \\sigma^2 \\implies (\\theta - \\hat{\\theta})^2 \\leq \\frac{\\sigma^2}{\\sum_{i=1}^{n} x_i^2}\n$$\nThis gives the interval for $\\theta$:\n$$\n\\hat{\\theta} - \\frac{\\sigma}{\\sqrt{\\sum_{i=1}^{n} x_i^2}} \\leq \\theta \\leq \\hat{\\theta} + \\frac{\\sigma}{\\sqrt{\\sum_{i=1}^{n} x_i^2}}\n$$\n\nFourth, we evaluate the confidence interval numerically. The data are $n=5$, $\\sigma=0.05$, $t_i \\in \\{0, 1, 2, 3, 4\\}$, and $y_i \\in \\{2.45, 0.95, 0.31, 0.12, 0.06\\}$.\nWe calculate the regressor values $x_i = \\exp(-t_i)$:\n$x_1 = \\exp(0) = 1$\n$x_2 = \\exp(-1) \\approx 0.36788$\n$x_3 = \\exp(-2) \\approx 0.13534$\n$x_4 = \\exp(-3) \\approx 0.04979$\n$x_5 = \\exp(-4) \\approx 0.01832$\n\nNext, we compute the required sums: $\\sum_{i=1}^{5} x_i^2$ and $\\sum_{i=1}^{5} y_i x_i$.\n$$\n\\sum_{i=1}^{5} x_i^2 = \\sum_{i=1}^{5} (\\exp(-t_i))^2 = \\sum_{i=1}^{5} \\exp(-2t_i) = \\exp(0) + \\exp(-2) + \\exp(-4) + \\exp(-6) + \\exp(-8) \\approx 1.156465\n$$\n$$\n\\sum_{i=1}^{5} y_i x_i = (2.45 \\times \\exp(0)) + (0.95 \\times \\exp(-1)) + (0.31 \\times \\exp(-2)) + (0.12 \\times \\exp(-3)) + (0.06 \\times \\exp(-4)) \\approx 2.848513\n$$\nNow we compute the MLE for $\\theta$:\n$$\n\\hat{\\theta} = \\frac{\\sum y_i x_i}{\\sum x_i^2} \\approx \\frac{2.848513}{1.156465} \\approx 2.46313\n$$\nThe half-width of the confidence interval is:\n$$\nw = \\frac{\\sigma}{\\sqrt{\\sum x_i^2}} \\approx \\frac{0.05}{\\sqrt{1.156465}} \\approx \\frac{0.05}{1.07539} \\approx 0.046495\n$$\nThe confidence interval $[\\theta_{\\text{lower}}, \\theta_{\\text{upper}}]$ is:\n$\\theta_{\\text{lower}} = \\hat{\\theta} - w \\approx 2.46313 - 0.046495 = 2.416635$\n$\\theta_{\\text{upper}} = \\hat{\\theta} + w \\approx 2.46313 + 0.046495 = 2.509625$\n\nRounding each endpoint to four significant figures, we get:\n$\\theta_{\\text{lower}} \\approx 2.417$\n$\\theta_{\\text{upper}} \\approx 2.510$\nThe confidence interval is approximately $[2.417, 2.510]$.", "answer": "$$\\boxed{\\begin{pmatrix} 2.417 & 2.510 \\end{pmatrix}}$$", "id": "4372099"}]}