{"hands_on_practices": [{"introduction": "A cornerstone of studying gene expression noise is the ability to experimentally separate intrinsic and extrinsic components. This exercise guides you through the statistical foundation of the dual-reporter assay, the classic method for achieving this decomposition [@problem_id:4357122]. By analyzing two identically regulated reporters within the same cells, we can cleverly use their covariance to quantify shared extrinsic fluctuations and the variance of their difference to isolate the intrinsic noise unique to each reporter's expression process.", "problem": "In a dual-reporter assay for gene expression, two identically regulated fluorescent reporters are measured in the same cell to decompose total variability into intrinsic and extrinsic components. Consider a population of $N$ independent cells indexed by $i \\in \\{1,\\dots,N\\}$, with measured expression levels $X_i$ and $Y_i$ for the two reporters. Assume the following mechanistic decomposition grounded in the systems biomedicine view of gene expression variability: each cell has a latent extrinsic state $M_i$ that affects both reporters additively and identically, and each reporter has its own intrinsic fluctuation within the cell. Formally, assume\n$$\nX_i \\;=\\; M_i \\;+\\; \\epsilon_{x,i}, \n\\qquad\nY_i \\;=\\; M_i \\;+\\; \\epsilon_{y,i},\n$$\nwhere cells are independent across $i$, the extrinsic states $\\{M_i\\}$ are independent and identically distributed with mean $\\mu_m$ and variance $\\sigma_{e}^{2}$, the intrinsic fluctuations $\\{\\epsilon_{x,i}\\}$ and $\\{\\epsilon_{y,i}\\}$ are independent and identically distributed with zero mean and variance $\\sigma_{i}^{2}$, and $(M_i,\\epsilon_{x,i},\\epsilon_{y,i})$ are mutually independent for each $i$. Further assume joint Gaussianity so that $(X_i,Y_i)$ are bivariate normal with covariance matrix $\\Sigma$ and the sample covariance matrix has a Wishart distribution.\n\nDefine the intrinsic noise as the variance component attributable to within-cell reporter-specific fluctuations and the extrinsic noise as the variance component attributable to cell-to-cell variation in the latent state. Using the above mechanistic assumptions and only foundational statistical facts for normal samples, derive unbiased estimators for the intrinsic variance $\\sigma_{i}^{2}$ and the extrinsic variance $\\sigma_{e}^{2}$ in terms of the observed data $\\{(X_i,Y_i)\\}_{i=1}^{N}$. Then, under the joint Gaussian assumption, derive the sampling variances $\\operatorname{Var}(\\widehat{\\sigma}_{i}^{2})$ and $\\operatorname{Var}(\\widehat{\\sigma}_{e}^{2})$ of these estimators as functions of the population parameters $\\sigma_{i}^{2}$ and $\\sigma_{e}^{2}$ and the sample size $N$.\n\nYour final answer must be a single row matrix containing, in order, the unbiased estimator for $\\sigma_{i}^{2}$, the unbiased estimator for $\\sigma_{e}^{2}$, the sampling variance of the intrinsic estimator, and the sampling variance of the extrinsic estimator, each expressed in closed form. No numerical approximation or rounding is required. Use the conventional sample mean and unbiased sample variance and covariance definitions with denominator $N-1$.", "solution": "The dual-reporter construction provides a mechanistically justified decomposition of variability into intrinsic and extrinsic components. The fundamental base is the additive model\n$$\nX_i \\;=\\; M_i \\;+\\; \\epsilon_{x,i}, \n\\qquad\nY_i \\;=\\; M_i \\;+\\; \\epsilon_{y,i},\n$$\nwith $M_i$ representing extrinsic cell-to-cell fluctuations and $\\epsilon_{x,i},\\epsilon_{y,i}$ representing intrinsic reporter-specific fluctuations within the same cell. By independence and equal regulation, we have\n$$\n\\mathbb{E}[M_i] \\;=\\; \\mu_m, \\quad \\operatorname{Var}(M_i) \\;=\\; \\sigma_{e}^{2},\n\\quad \\mathbb{E}[\\epsilon_{x,i}] \\;=\\; \\mathbb{E}[\\epsilon_{y,i}] \\;=\\; 0,\n\\quad \\operatorname{Var}(\\epsilon_{x,i}) \\;=\\; \\operatorname{Var}(\\epsilon_{y,i}) \\;=\\; \\sigma_{i}^{2},\n$$\nand $(M_i,\\epsilon_{x,i},\\epsilon_{y,i})$ are mutually independent for each $i$, with independence across cells $i$.\n\nFrom these definitions,\n$$\n\\operatorname{Var}(X_i) \\;=\\; \\operatorname{Var}(M_i) + \\operatorname{Var}(\\epsilon_{x,i}) \\;=\\; \\sigma_{e}^{2} + \\sigma_{i}^{2},\n$$\n$$\n\\operatorname{Var}(Y_i) \\;=\\; \\sigma_{e}^{2} + \\sigma_{i}^{2},\n\\qquad\n\\operatorname{Cov}(X_i,Y_i) \\;=\\; \\operatorname{Var}(M_i) \\;=\\; \\sigma_{e}^{2}.\n$$\nDefine the within-cell difference $D_i = X_i - Y_i$. Then\n$$\n\\mathbb{E}[D_i] \\;=\\; \\mathbb{E}[X_i - Y_i] \\;=\\; \\mathbb{E}[M_i + \\epsilon_{x,i} - M_i - \\epsilon_{y,i}] \\;=\\; 0,\n$$\nand, using independence of $\\epsilon_{x,i}$ and $\\epsilon_{y,i}$,\n$$\n\\operatorname{Var}(D_i) \\;=\\; \\operatorname{Var}(X_i - Y_i) \\;=\\; \\operatorname{Var}(\\epsilon_{x,i} - \\epsilon_{y,i})\n\\;=\\; \\operatorname{Var}(\\epsilon_{x,i}) + \\operatorname{Var}(\\epsilon_{y,i})\n\\;=\\; 2 \\sigma_{i}^{2}.\n$$\nThus the intrinsic variance component is $\\sigma_{i}^{2} = \\frac{1}{2} \\operatorname{Var}(D_i)$, while the extrinsic variance component is the covariance of the two reporters, $\\sigma_{e}^{2} = \\operatorname{Cov}(X_i,Y_i)$.\n\nGiven observed data $\\{(X_i,Y_i)\\}_{i=1}^{N}$, let\n$$\n\\bar{X} = \\frac{1}{N} \\sum_{i=1}^{N} X_i, \n\\quad\n\\bar{Y} = \\frac{1}{N} \\sum_{i=1}^{N} Y_i,\n\\quad\nD_i = X_i - Y_i,\n\\quad\n\\bar{D} = \\frac{1}{N} \\sum_{i=1}^{N} D_i = \\bar{X} - \\bar{Y}.\n$$\nDefine the unbiased sample variance of the differences and the unbiased sample covariance of the reporters as\n$$\ns_{D}^{2} = \\frac{1}{N-1} \\sum_{i=1}^{N} (D_i - \\bar{D})^{2},\n\\qquad\ns_{XY} = \\frac{1}{N-1} \\sum_{i=1}^{N} (X_i - \\bar{X})(Y_i - \\bar{Y}).\n$$\nBy standard properties of unbiased sample variance and covariance estimators for independent and identically distributed samples,\n$$\n\\mathbb{E}[s_{D}^{2}] = \\operatorname{Var}(D_i) = 2 \\sigma_{i}^{2},\n\\qquad\n\\mathbb{E}[s_{XY}] = \\operatorname{Cov}(X_i,Y_i) = \\sigma_{e}^{2}.\n$$\nTherefore, unbiased estimators for the intrinsic and extrinsic variance components are\n$$\n\\widehat{\\sigma}_{i}^{2} = \\frac{1}{2} \\, s_{D}^{2} = \\frac{1}{2(N-1)} \\sum_{i=1}^{N} (D_i - \\bar{D})^{2},\n\\qquad\n\\widehat{\\sigma}_{e}^{2} = s_{XY} = \\frac{1}{N-1} \\sum_{i=1}^{N} (X_i - \\bar{X})(Y_i - \\bar{Y}).\n$$\n\nWe now derive the sampling variances of these estimators under the joint Gaussian assumption. For the intrinsic estimator, note that $D_i$ are independent and normally distributed with mean $0$ and variance $\\sigma_{D}^{2} = \\operatorname{Var}(D_i) = 2 \\sigma_{i}^{2}$. The scaled sample variance has a chi-square distribution:\n$$\n\\frac{(N-1)\\, s_{D}^{2}}{\\sigma_{D}^{2}} \\sim \\chi^{2}_{N-1}.\n$$\nThe variance of a chi-square random variable with $k$ degrees of freedom is $2k$, so\n$$\n\\operatorname{Var}\\!\\left( (N-1) s_{D}^{2} \\right) = 2 (N-1) \\sigma_{D}^{4},\n$$\nand dividing by $(N-1)^{2}$ gives\n$$\n\\operatorname{Var}\\!\\left( s_{D}^{2} \\right) = \\frac{2 \\sigma_{D}^{4}}{N-1}.\n$$\nSubstituting $\\sigma_{D}^{2} = 2 \\sigma_{i}^{2}$ yields $\\sigma_{D}^{4} = 4 \\sigma_{i}^{4}$ and\n$$\n\\operatorname{Var}\\!\\left( s_{D}^{2} \\right) = \\frac{2 \\cdot 4 \\sigma_{i}^{4}}{N-1} = \\frac{8 \\sigma_{i}^{4}}{N-1}.\n$$\nSince $\\widehat{\\sigma}_{i}^{2} = \\frac{1}{2} s_{D}^{2}$, it follows that\n$$\n\\operatorname{Var}\\!\\left( \\widehat{\\sigma}_{i}^{2} \\right) = \\frac{1}{4} \\operatorname{Var}\\!\\left( s_{D}^{2} \\right) = \\frac{1}{4} \\cdot \\frac{8 \\sigma_{i}^{4}}{N-1} = \\frac{2 \\sigma_{i}^{4}}{N-1}.\n$$\n\nFor the extrinsic estimator, consider the unbiased sample covariance $s_{XY}$. Under the bivariate normal assumption with covariance matrix\n$$\n\\Sigma = \\begin{pmatrix}\n\\sigma_{x}^{2}  \\sigma_{xy} \\\\\n\\sigma_{xy}  \\sigma_{y}^{2}\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\sigma_{e}^{2} + \\sigma_{i}^{2}  \\sigma_{e}^{2} \\\\\n\\sigma_{e}^{2}  \\sigma_{e}^{2} + \\sigma_{i}^{2}\n\\end{pmatrix},\n$$\nthe scaled sample covariance matrix $(N-1)\\, S$, where $S$ has entries $s_{XX}, s_{XY}, s_{YY}$, follows a Wishart distribution $\\mathcal{W}_{2}(N-1, \\Sigma)$. A well-tested formula for the Wishart distribution gives, for the off-diagonal element,\n$$\n\\operatorname{Var}\\!\\left( s_{XY} \\right) = \\frac{1}{N-1} \\left( \\sigma_{xy}^{2} + \\sigma_{x}^{2} \\sigma_{y}^{2} \\right).\n$$\nSubstituting $\\sigma_{xy} = \\sigma_{e}^{2}$ and $\\sigma_{x}^{2} = \\sigma_{y}^{2} = \\sigma_{e}^{2} + \\sigma_{i}^{2}$, we obtain\n$$\n\\operatorname{Var}\\!\\left( \\widehat{\\sigma}_{e}^{2} \\right) = \\operatorname{Var}\\!\\left( s_{XY} \\right) = \\frac{1}{N-1} \\left( \\sigma_{e}^{4} + (\\sigma_{e}^{2} + \\sigma_{i}^{2})^{2} \\right)\n= \\frac{1}{N-1} \\left( 2 \\sigma_{e}^{4} + 2 \\sigma_{e}^{2} \\sigma_{i}^{2} + \\sigma_{i}^{4} \\right).\n$$\n\nCollecting results, the unbiased estimators and their sampling variances are:\n$$\n\\widehat{\\sigma}_{i}^{2} = \\frac{1}{2(N-1)} \\sum_{i=1}^{N} (X_i - Y_i - \\bar{X} + \\bar{Y})^{2},\n\\qquad\n\\widehat{\\sigma}_{e}^{2} = \\frac{1}{N-1} \\sum_{i=1}^{N} (X_i - \\bar{X})(Y_i - \\bar{Y}),\n$$\n$$\n\\operatorname{Var}\\!\\left( \\widehat{\\sigma}_{i}^{2} \\right) = \\frac{2 \\sigma_{i}^{4}}{N-1},\n\\qquad\n\\operatorname{Var}\\!\\left( \\widehat{\\sigma}_{e}^{2} \\right) = \\frac{2 \\sigma_{e}^{4} + 2 \\sigma_{e}^{2} \\sigma_{i}^{2} + \\sigma_{i}^{4}}{N-1}.\n$$\nThese expressions quantify estimator precision in terms of the intrinsic and extrinsic variance components and the sample size.", "answer": "$$\\boxed{\\begin{pmatrix}\n\\frac{1}{2(N-1)} \\sum_{i=1}^{N} (X_i - Y_i - \\bar{X} + \\bar{Y})^{2} \n\\frac{1}{N-1} \\sum_{i=1}^{N} (X_i - \\bar{X})(Y_i - \\bar{Y}) \n\\frac{2 \\sigma_{i}^{4}}{N-1} \n\\frac{2 \\sigma_{e}^{4} + 2 \\sigma_{e}^{2} \\sigma_{i}^{2} + \\sigma_{i}^{4}}{N-1}\n\\end{pmatrix}}$$", "id": "4357122"}, {"introduction": "Beyond simply measuring noise, a key goal in systems biology is to infer the underlying molecular mechanisms that generate it. Different biological processes, such as transcriptional bursting, leave distinct statistical signatures in single-cell data. This practice focuses on model discrimination, challenging you to use the relationship between the mean $\\mu$ and variance $\\sigma^2$ of gene expression to determine which of two canonical models—intrinsic bursting or extrinsic heterogeneity—best explains a given dataset [@problem_id:4357081].", "problem": "Single-cell messenger ribonucleic acid (mRNA) counts are measured under a series of induction levels for a single gene. For each induction level, the sample mean $\\,\\mu\\,$ and sample variance $\\,\\sigma^{2}\\,$ across cells are estimated, yielding the following pairs: $(\\mu,\\sigma^{2})$ equal to $(5,10)$, $(15,30)$, $(30,60)$, and $(60,120)$. Consider two mechanistic explanations for overdispersion (variance exceeding the mean) in steady-state mRNA counts under a linear birth-death model with constant degradation rate $\\,\\delta\\,$:\n\n- Intrinsic bursting: transcription occurs as bursts arriving at rate $\\,f\\,$ (dependent on induction), with burst size $\\,B\\,$ drawn independently for each burst. Assume a geometric burst-size distribution with mean $\\,b\\,$ that does not change with induction.\n- Extrinsic heterogeneity: the effective transcription rate $\\,\\lambda\\,$ is cell-to-cell heterogeneous under each induction level, and the heterogeneity is multiplicative in the sense that the coefficient of variation (CV) of $\\,\\lambda\\,$ is approximately constant across induction levels.\n\nUse the following foundational facts to derive moment relationships and decide which explanation is more consistent with the data:\n\n- In a linear birth-death process with constant rate, the steady-state mRNA count is Poisson with variance equal to the mean.\n- The law of total variance: $\\mathrm{Var}(N)=\\mathbb{E}[\\mathrm{Var}(N\\mid \\Theta)]+\\mathrm{Var}(\\mathbb{E}[N\\mid \\Theta])$, where $\\,\\Theta\\,$ encapsulates random parameters (for example, extrinsic rate $\\,\\lambda\\,$).\n- For a Poisson random variable with random rate $\\,\\Lambda\\,$, $\\mathrm{Var}(N)=\\mathbb{E}[\\Lambda]+\\mathrm{Var}(\\Lambda)$.\n\nBased solely on the measured $(\\mu,\\sigma^{2})$ pairs and principled moment relationships implied by the two mechanisms above, which interpretation is most consistent with the data?\n\nA. Intrinsic transcriptional bursting with approximately constant mean burst size $\\,b\\,$ across induction levels and induction modulating burst frequency $\\,f\\,$.\n\nB. Extrinsic heterogeneity in the effective transcription rate $\\,\\lambda\\,$ across cells with approximately constant CV across induction levels.\n\nC. A purely Poisson birth-death process with no true overdispersion; the observed variance is explained by finite-sample estimation error.\n\nD. Intrinsic bursting with mean burst size proportional to $\\,\\mu\\,$ across induction levels, implying variance scales quadratically with the mean.", "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n\nThe problem provides the following information:\n1.  **Data**: A series of sample mean $\\mu$ and sample variance $\\sigma^2$ pairs for single-cell mRNA counts at different induction levels: $(\\mu, \\sigma^2) \\in \\{(5, 10), (15, 30), (30, 60), (60, 120)\\}$.\n2.  **Model Components**: A linear birth-death model with a constant degradation rate $\\delta$.\n3.  **Mechanism 1 (Intrinsic Bursting)**:\n    -   Transcription occurs in bursts.\n    -   Burst arrival rate is $f$, which is dependent on the induction level.\n    -   Burst size $B$ is an independent random variable for each burst.\n    -   The distribution of $B$ is geometric with a mean $\\mathbb{E}[B] = b$.\n    -   The mean burst size $b$ is constant across induction levels.\n4.  **Mechanism 2 (Extrinsic Heterogeneity)**:\n    -   The effective transcription rate $\\lambda$ is heterogeneous from cell to cell.\n    -   The coefficient of variation (CV) of $\\lambda$ is approximately constant across induction levels.\n5.  **Foundational Facts**:\n    -   Fact 1: In a linear birth-death process with a constant rate, the steady-state mRNA count is Poisson, with variance equal to the mean.\n    -   Fact 2: The law of total variance: $\\mathrm{Var}(N)=\\mathbb{E}[\\mathrm{Var}(N\\mid \\Theta)]+\\mathrm{Var}(\\mathbb{E}[N\\mid \\Theta])$, where $\\Theta$ represents random parameters.\n    -   Fact 3: For a Poisson random variable $N$ with a random rate parameter $\\Lambda$, $\\mathrm{Var}(N)=\\mathbb{E}[\\Lambda]+\\mathrm{Var}(\\Lambda)$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is subjected to validation against the specified criteria.\n\n-   **Scientifically Grounded**: The problem is firmly located within the field of systems biology, specifically the study of gene expression noise. The models presented (intrinsic transcriptional bursting and extrinsic noise) are canonical and well-established frameworks for explaining overdispersion in gene expression data. The foundational facts are standard results from probability theory. The problem is scientifically sound.\n-   **Well-Posed**: The problem provides a clear question, sufficient data, and two well-defined, mutually exclusive mechanistic hypotheses to test against the data. The provided theoretical foundations allow for the derivation of distinct, testable predictions for each hypothesis, leading to a unique conclusion.\n-   **Objective**: The problem is stated in precise, quantitative terms. The data consists of numerical pairs, and the hypotheses are described by mathematical properties. The question asks for an interpretation based *solely* on this information, avoiding any subjective or ambiguous language.\n-   **Completeness**: The problem is self-contained. All necessary information to derive the mean-variance relationships for both models and to test them against the data is provided.\n-   **Realism**: The data values and the experimental context (measuring single-cell mRNA counts under varying induction) are realistic and representative of actual biological experiments. The models are simplifications, as all models are, but they are standard and widely used for interpreting such data.\n\nNo flaws are found. The problem is a standard, well-formulated exercise in quantitative biology.\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid. I will proceed with the solution derivation.\n\n## Solution Derivation\n\nThe task is to determine which of the two proposed mechanisms—intrinsic bursting or extrinsic heterogeneity—is more consistent with the observed data pairs $(\\mu, \\sigma^2)$. To do this, we will derive the theoretical relationship between the mean $\\mu$ and variance $\\sigma^2$ for each mechanism and compare these predictions to the experimental data.\n\nThe data points are $(5, 10)$, $(15, 30)$, $(30, 60)$, and $(60, 120)$. A conspicuous pattern in this data is that for every pair, the variance is exactly twice the mean: $\\sigma^2 = 2\\mu$. This implies a linear relationship between variance and mean, with a slope of $2$.\n\n### Analysis of Mechanism 1: Intrinsic Bursting\n\nThis model posits that mRNA is produced in bursts. The process is characterized by a burst arrival rate $f$ and a burst size distribution for $B$ with a constant mean $\\mathbb{E}[B] = b$. The induction level modulates $f$, while $b$ and the degradation rate $\\delta$ are constant.\n\nThe chemical master equation for this process (bursty production at rate $f$ with geometrically distributed burst sizes, and first-order degradation at rate $\\delta$) can be solved exactly. The steady-state distribution of the number of mRNA molecules, $N$, is a negative binomial distribution.\n\nThe moments of this negative binomial distribution are:\n-   Mean: $\\mu = \\mathbb{E}[N] = \\frac{f \\cdot b}{\\delta}$\n-   Variance: $\\sigma^2 = \\mathrm{Var}(N) = \\mu (1+b)$\n\nThis model predicts a linear relationship between the variance and the mean: $\\sigma^2 = k \\mu$, where the slope $k = 1+b$ is a constant because the mean burst size $b$ is assumed to be constant across induction levels.\n\nLet's test this prediction against the data. The data exhibits a relationship $\\sigma^2 = 2\\mu$.\nComparing the model prediction to the data:\n$$ \\sigma^2 = (1+b)\\mu $$\n$$ \\sigma^2 = 2\\mu $$\nThis implies that $1+b = 2$, which gives a mean burst size of $b=1$.\nSince we can find a constant, physically meaningful parameter ($b=1$) that allows the model to perfectly describe the observed linear relationship, we conclude that the data is highly consistent with the intrinsic bursting model. The different induction levels correspond to different burst frequencies $f$, which in turn lead to different mean expression levels $\\mu$.\n\n### Analysis of Mechanism 2: Extrinsic Heterogeneity\n\nThis model assumes that the transcription rate $\\lambda$ varies from cell to cell, even at a fixed induction level. Within any given cell, the transcription is a simple (non-bursty) Poisson process. The number of mRNA molecules $N$ for a cell with transcription rate $\\lambda$ is Poisson-distributed with mean and variance equal to $\\lambda/\\delta$. We are given that the coefficient of variation of $\\lambda$, $\\mathrm{CV}_\\lambda$, is constant across induction levels.\n\nWe use the law of total variance, as suggested. Let the random parameter be the transcription rate $\\lambda$. The steady-state mRNA count is $N$.\n1.  Conditional Mean: $\\mathbb{E}[N \\mid \\lambda] = \\lambda / \\delta$\n2.  Conditional Variance: $\\mathrm{Var}(N \\mid \\lambda) = \\lambda / \\delta$ (from the Poisson property)\n\nThe overall mean $\\mu$ is the expectation of the conditional mean:\n$$ \\mu = \\mathbb{E}[N] = \\mathbb{E}[\\mathbb{E}[N \\mid \\lambda]] = \\mathbb{E}[\\lambda / \\delta] = \\frac{\\mathbb{E}[\\lambda]}{\\delta} $$\n\nThe overall variance $\\sigma^2$ is given by the law of total variance:\n$$ \\sigma^2 = \\mathrm{Var}(N) = \\mathbb{E}[\\mathrm{Var}(N \\mid \\lambda)] + \\mathrm{Var}(\\mathbb{E}[N \\mid \\lambda]) $$\n$$ \\sigma^2 = \\mathbb{E}[\\lambda / \\delta] + \\mathrm{Var}(\\lambda / \\delta) $$\n$$ \\sigma^2 = \\frac{\\mathbb{E}[\\lambda]}{\\delta} + \\frac{\\mathrm{Var}(\\lambda)}{\\delta^2} $$\nSubstituting $\\mu = \\frac{\\mathbb{E}[\\lambda]}{\\delta}$, we get:\n$$ \\sigma^2 = \\mu + \\frac{\\mathrm{Var}(\\lambda)}{\\delta^2} $$\nThe model states that $\\mathrm{CV}_\\lambda = \\frac{\\sqrt{\\mathrm{Var}(\\lambda)}}{\\mathbb{E}[\\lambda]}=C$ is constant. This means $\\mathrm{Var}(\\lambda) = C^2 (\\mathbb{E}[\\lambda])^2$.\nFrom the mean expression, we have $\\mathbb{E}[\\lambda] = \\mu \\delta$.\nSubstituting this into the variance expression for $\\lambda$:\n$$ \\mathrm{Var}(\\lambda) = C^2 (\\mu\\delta)^2 = C^2 \\mu^2 \\delta^2 $$\nFinally, we substitute this back into the equation for $\\sigma^2$:\n$$ \\sigma^2 = \\mu + \\frac{C^2 \\mu^2 \\delta^2}{\\delta^2} = \\mu + C^2 \\mu^2 $$\nThis model predicts a quadratic relationship between variance and mean. Let's see if the data is consistent with this. If this model holds, the quantity $C^2 = \\frac{\\sigma^2 - \\mu}{\\mu^2}$ should be constant across the data points.\n-   For $(\\mu, \\sigma^2) = (5, 10)$: $C^2 = \\frac{10-5}{5^2} = \\frac{5}{25} = 0.2$\n-   For $(\\mu, \\sigma^2) = (15, 30)$: $C^2 = \\frac{30-15}{15^2} = \\frac{15}{225} = \\frac{1}{15} \\approx 0.067$\n-   For $(\\mu, \\sigma^2) = (30, 60)$: $C^2 = \\frac{60-30}{30^2} = \\frac{30}{900} = \\frac{1}{30} \\approx 0.033$\n-   For $(\\mu, \\sigma^2) = (60, 120)$: $C^2 = \\frac{120-60}{60^2} = \\frac{60}{3600} = \\frac{1}{60} \\approx 0.017$\n\nThe calculated value for $C^2$ is not constant; it systematically decreases as the mean expression $\\mu$ increases. Therefore, the data is inconsistent with this model of extrinsic heterogeneity.\n\n### Conclusion\n\nThe data exhibits a perfect linear relationship $\\sigma^2 = 2\\mu$. The intrinsic bursting model with constant mean burst size predicts a linear relationship $\\sigma^2 = (1+b)\\mu$. This provides a perfect fit to the data with $b=1$. The extrinsic heterogeneity model with constant CV predicts a quadratic relationship $\\sigma^2 = \\mu + C^2\\mu^2$, which is clearly not supported by the data. Thus, the intrinsic bursting model is the most consistent explanation.\n\n## Option-by-Option Analysis\n\n**A. Intrinsic transcriptional bursting with approximately constant mean burst size $\\,b\\,$ across induction levels and induction modulating burst frequency $\\,f\\,$.**\nOur analysis showed that the data perfectly fits the theoretical relationship $\\sigma^2 = (1+b)\\mu$ derived from this model, with a constant mean burst size $b=1$. In this model, the mean expression is $\\mu = fb/\\delta$. Changes in induction affect the burst frequency $f$, which in turn changes $\\mu$, while the slope of the variance-mean plot, $(1+b)$, remains constant. This option correctly describes the mechanism that is consistent with the data.\n**Verdict: Correct.**\n\n**B. Extrinsic heterogeneity in the effective transcription rate $\\,\\lambda\\,$ across cells with approximately constant CV across induction levels.**\nOur analysis showed that this model predicts a quadratic relationship $\\sigma^2 = \\mu + C^2\\mu^2$. The experimental data shows a linear relationship $\\sigma^2=2\\mu$ and is therefore inconsistent with the prediction of this model.\n**Verdict: Incorrect.**\n\n**C. A purely Poisson birth-death process with no true overdispersion; the observed variance is explained by finite-sample estimation error.**\nA purely Poisson process is characterized by the property that the variance equals the mean, i.e., $\\sigma^2 = \\mu$. The data systematically shows $\\sigma^2 = 2\\mu$, which is a significant and consistent deviation known as overdispersion. Attributing this perfect twofold difference across a wide range of means to estimation error is not a valid scientific explanation.\n**Verdict: Incorrect.**\n\n**D. Intrinsic bursting with mean burst size proportional to $\\,\\mu\\,$ across induction levels, implying variance scales quadratically with the mean.**\nThis option proposes a different intrinsic bursting model where $b \\propto \\mu$, so let $b=k\\mu$ for some constant $k$. The general relationship is $\\sigma^2 \\propto b\\mu$. Substituting $b=k\\mu$ yields $\\sigma^2 \\propto (k\\mu)\\mu = k\\mu^2$. This predicts that variance scales quadratically with the mean. The data clearly shows a linear scaling, $\\sigma^2 \\propto \\mu$. Therefore, this model is not consistent with the data.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "4357081"}, {"introduction": "Modern 'omics technologies provide a high-dimensional view of the cell, measuring thousands of genes or proteins simultaneously. This perspective allows us to redefine extrinsic noise as coordinated modes of variation that affect large sets of molecules. This practice introduces a powerful, data-driven approach using Principal Component Analysis (PCA) to discover these dominant extrinsic axes from a complex dataset [@problem_id:4357150]. You will learn to apply a result from random matrix theory to rigorously distinguish true biological signals from the bulk of random statistical noise.", "problem": "You are given a conceptual model of single-cell proteomics for many proteins across many cells in which each cell’s measured protein abundances are influenced by two types of variability: intrinsic noise (idiosyncratic to each gene or protein and uncorrelated across proteins) and extrinsic noise (shared cellular factors that jointly modulate many proteins). This problem asks you to formalize and implement a procedure, grounded in a spiked covariance model and random matrix theory, to identify dominant extrinsic axes and to quantify their contributions to total variance using Principal Component Analysis (PCA).\n\nAssume the following generative model for a data matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times P}$ where $N$ is the number of cells and $P$ is the number of proteins:\n1. Intrinsic and measurement variability are modeled as isotropic Gaussian noise $\\mathbf{Z} \\in \\mathbb{R}^{N \\times P}$ with independent entries and unit variance, i.e., $\\mathbf{Z}_{ij} \\sim \\mathcal{N}(0,1)$.\n2. Extrinsic variability is modeled as a low-rank factor model: $\\mathbf{F} \\in \\mathbb{R}^{N \\times r}$ with independent entries $\\mathbf{F}_{ik} \\sim \\mathcal{N}(0,1)$, and a protein loading matrix $\\mathbf{L} \\in \\mathbb{R}^{P \\times r}$ such that $\\mathbf{L} \\mathbf{L}^{\\top}$ has rank $r$ with nonzero eigenvalues $\\{\\beta_1,\\ldots,\\beta_r\\}$ (called population spike strengths).\n3. The observed centered data are generated as\n$$\n\\mathbf{X} \\;=\\; \\mathbf{Z} \\;+\\; \\mathbf{F}\\mathbf{L}^{\\top} \\,,\n$$\nand then centered per protein (zero mean across cells for each protein). Under this model, the population covariance across proteins is\n$$\n\\boldsymbol{\\Sigma} \\;=\\; \\mathbb{E}\\left[\\frac{1}{N}\\mathbf{X}^{\\top}\\mathbf{X}\\right] \\;=\\; \\mathbf{I}_P \\;+\\; \\mathbf{L}\\mathbf{L}^{\\top},\n$$\nso that the extrinsic axes correspond to the eigenvectors associated with the $r$ spikes $\\{1+\\beta_k\\}$ above the isotropic baseline $1$.\n\nGiven a sample covariance estimator computed from the centered data\n$$\n\\mathbf{S} \\;=\\; \\frac{1}{N-1}\\,\\mathbf{X}^{\\top}\\mathbf{X} \\;\\in\\; \\mathbb{R}^{P \\times P},\n$$\nPrincipal Component Analysis (PCA) diagonalizes $\\mathbf{S}$ with eigenvalues $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_P \\ge 0$ and orthonormal eigenvectors. In the absence of extrinsic factors ($r=0$), random matrix theory (the Marchenko–Pastur law) predicts that when $N,P \\to \\infty$ with aspect ratio $c = P/(N-1)$ held fixed, the eigenvalue distribution of $\\mathbf{S}$ concentrates on\n$$\n\\left[\\lambda_{-}, \\lambda_{+}\\right] \\;=\\; \\left[(1-\\sqrt{c})^2,\\,(1+\\sqrt{c})^2\\right].\n$$\nIn the presence of spikes, eigenvalues associated with sufficiently strong extrinsic axes separate above the random bulk and appear as outliers. We will operationally define dominant extrinsic axes as those principal components whose sample eigenvalues exceed the upper Marchenko–Pastur edge\n$$\n\\lambda_{+} \\;=\\; (1+\\sqrt{c})^2 \\quad \\text{with} \\quad c \\;=\\; \\frac{P}{N-1}.\n$$\n\nYour task is to:\n1. Generate synthetic datasets under the model above for a small test suite of parameter settings. For each test, use a fixed random seed to ensure reproducibility. Construct $\\mathbf{L}$ so that it has orthonormal columns scaled to match the desired spike strengths $\\{\\beta_k\\}$, i.e., if $\\mathbf{Q} \\in \\mathbb{R}^{P \\times r}$ has orthonormal columns, set $\\mathbf{L} = \\mathbf{Q}\\,\\mathrm{diag}(\\sqrt{\\beta_1},\\ldots,\\sqrt{\\beta_r})$ so that $\\mathbf{L}\\mathbf{L}^{\\top}$ has nonzero eigenvalues exactly $\\{\\beta_k\\}$.\n2. Center $\\mathbf{X}$ per protein (zero-mean each column), compute $\\mathbf{S} = \\frac{1}{N-1}\\mathbf{X}^{\\top}\\mathbf{X}$, and perform PCA via eigendecomposition of $\\mathbf{S}$.\n3. Identify the number of dominant extrinsic axes $\\hat{r}$ as the count of eigenvalues strictly greater than $\\lambda_{+}$.\n4. Quantify the contribution of each detected extrinsic axis to the total variance by reporting, for each $j \\in \\{1,\\ldots,\\hat{r}\\}$, the fraction\n$$\nf_j \\;=\\; \\frac{\\lambda_j}{\\sum_{i=1}^{P}\\lambda_i},\n$$\nrounded to six decimal places.\n\nTest suite. For each case, use base seed $s_0 = 20231107$ and per-case seed $s = s_0 + \\Delta$, with $\\Delta$ specified below. For all cases, set the variance of $\\mathbf{Z}$ and $\\mathbf{F}$ to $1$ as described above. The spike strengths are given as the list $\\{\\beta_k\\}$ (possibly empty).\n- Case A (happy path): $N = 1000$, $P = 60$, spikes $\\{\\beta_1,\\beta_2\\} = \\{2.5,\\,1.6\\}$, $\\Delta = 0$.\n- Case B (null boundary): $N = 800$, $P = 80$, spikes empty (i.e., $r=0$), $\\Delta = 1$.\n- Case C (small $N$ near $P$, one strong spike): $N = 120$, $P = 100$, spikes $\\{\\beta_1,\\beta_2\\} = \\{5.0,\\,0.8\\}$, $\\Delta = 2$.\n- Case D (multiple spikes, only one above edge): $N = 600$, $P = 50$, spikes $\\{\\beta_1,\\beta_2,\\beta_3\\} = \\{1.5,\\,0.4,\\,0.3\\}$, $\\Delta = 3$.\n\nRequired final output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes one inner list. For each test case, output one inner list whose first element is the integer $\\hat{r}$, followed by $\\hat{r}$ floats giving $f_1,\\ldots,f_{\\hat{r}}$ rounded to six decimal places. For example, an output for two cases might look like\n$$\n[[2,0.123456,0.078901],[0]].\n$$\nNote that there are no physical units in this problem, and angles are not involved. All floats must be rounded to six decimal places in the output.", "solution": "The user has provided a valid problem statement.\n\nThe problem is scientifically grounded in the fields of statistical signal processing, random matrix theory (RMT), and their application to systems biology, specifically single-cell proteomics. It outlines a procedure for distinguishing structured extrinsic variability from unstructured intrinsic noise using a spiked covariance model. The tasks are well-posed, with a clearly defined generative model, a deterministic analysis pipeline, and specific, objective parameter sets for testing. All terms are defined mathematically, and the problem is self-contained and free of contradictions or ambiguities.\n\nHerein, a principled, step-by-step solution is provided, following the user's formal requirements.\n\n### Methodological Framework\n\nThe core task is to identify and quantify the contribution of dominant extrinsic factors of variation in a synthetic single-cell proteomics dataset. The methodology is grounded in a spiked covariance model and leverages results from random matrix theory to establish a threshold for significance. The process is divided into four main stages: data generation, covariance estimation and PCA, identification of significant components, and quantification of variance contribution.\n\n#### 1. Synthetic Data Generation\n\nWe first generate a synthetic data matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times P}$, where $N$ is the number of cells and $P$ is the number of proteins, according to the specified generative model:\n$$\n\\mathbf{X} \\;=\\; \\mathbf{Z} \\;+\\; \\mathbf{F}\\mathbf{L}^{\\top}\n$$\n- The term $\\mathbf{Z} \\in \\mathbb{R}^{N \\times P}$ represents intrinsic noise and measurement error. It is modeled as a matrix of independent and identically distributed random variables, where each entry $\\mathbf{Z}_{ij}$ is drawn from a standard normal distribution, $\\mathbf{Z}_{ij} \\sim \\mathcal{N}(0,1)$.\n- The term $\\mathbf{F}\\mathbf{L}^{\\top}$ represents extrinsic variability arising from $r$ shared cellular factors.\n  - $\\mathbf{F} \\in \\mathbb{R}^{N \\times r}$ is the factor score matrix, whose entries $\\mathbf{F}_{ik}$ are also drawn from a standard normal distribution, $\\mathbf{F}_{ik} \\sim \\mathcal{N}(0,1)$.\n  - $\\mathbf{L} \\in \\mathbb{R}^{P \\times r}$ is the protein loading matrix. It is constructed to embed a specific eigenspectrum into the population covariance. We first generate a matrix $\\mathbf{Q} \\in \\mathbb{R}^{P \\times r}$ with orthonormal columns using the QR decomposition of a random $P \\times r$ matrix. Then, given a set of desired population spike strengths $\\{\\beta_1, \\ldots, \\beta_r\\}$, we set $\\mathbf{L} = \\mathbf{Q} \\, \\mathrm{diag}(\\sqrt{\\beta_1}, \\ldots, \\sqrt{\\beta_r})$. This construction ensures that the matrix $\\mathbf{L}\\mathbf{L}^{\\top} = \\mathbf{Q} \\, \\mathrm{diag}(\\beta_1, \\ldots, \\beta_r) \\, \\mathbf{Q}^{\\top}$ has exactly the desired non-zero eigenvalues $\\{\\beta_k\\}$.\n- In the null case where there are no extrinsic factors ($r=0$), the `spikes` list is empty, and the term $\\mathbf{F}\\mathbf{L}^{\\top}$ is a zero matrix, resulting in $\\mathbf{X} = \\mathbf{Z}$.\n- For reproducibility, the pseudo-random number generator is seeded with a specific value $s = s_0 + \\Delta$ for each test case, where $s_0 = 20231107$.\n\n#### 2. Covariance Estimation and Principal Component Analysis (PCA)\n\nThe raw data matrix $\\mathbf{X}$ is first centered to have zero mean for each protein (column). Let $\\mathbf{X}_{\\text{c}}$ be the centered matrix, where $(\\mathbf{X}_{\\text{c}})_{ij} = \\mathbf{X}_{ij} - \\frac{1}{N}\\sum_{k=1}^{N} \\mathbf{X}_{kj}$.\n\nFrom the centered data, we compute the sample covariance matrix $\\mathbf{S} \\in \\mathbb{R}^{P \\times P}$ using the unbiased estimator:\n$$\n\\mathbf{S} \\;=\\; \\frac{1}{N-1}\\mathbf{X}_{\\text{c}}^{\\top}\\mathbf{X}_{\\text{c}}\n$$\nPrincipal Component Analysis is then performed by computing the eigendecomposition of $\\mathbf{S}$. Since $\\mathbf{S}$ is a real, symmetric matrix, it has $P$ real, non-negative eigenvalues, which we denote in descending order as $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_P \\ge 0$. These eigenvalues represent the variance captured by each principal component.\n\n#### 3. Identification of Dominant Extrinsic Axes\n\nTo distinguish eigenvalues corresponding to true extrinsic factors from those arising purely from random noise, we employ a result from random matrix theory. The Marchenko-Pastur (MP) law describes the limiting distribution of eigenvalues of a sample covariance matrix formed from purely random data. For a matrix of i.i.d. random variables with zero mean and unit variance, in the limit $N, P \\to \\infty$ with a fixed aspect ratio $c = P/(N-1)$, the eigenvalue spectrum of $\\mathbf{S}$ converges to a continuous distribution supported on the interval $[\\lambda_{-}, \\lambda_{+}]$, where:\n$$\n\\lambda_{-} = (1-\\sqrt{c})^2 \\quad \\text{and} \\quad \\lambda_{+} = (1+\\sqrt{c})^2\n$$\nThe value $\\lambda_{+}$ serves as a theoretical upper bound for the \"bulk\" of eigenvalues generated by noise. Any sample eigenvalue $\\lambda_j$ that lies strictly above this threshold, $\\lambda_j  \\lambda_{+}$, is considered a \"spike\" corresponding to a non-random, structured source of variation—in our model, a dominant extrinsic axis. We count the number of such eigenvalues to determine the number of detected dominant axes, $\\hat{r}$.\n\n#### 4. Quantification of Variance Contribution\n\nFor each of the $\\hat{r}$ identified dominant axes, we quantify its contribution to the total variance observed in the data. The total variance is the sum of all eigenvalues, which is equal to the trace of the sample covariance matrix, $\\mathrm{Tr}(\\mathbf{S}) = \\sum_{i=1}^{P}\\lambda_i$. The fractional contribution of the $j$-th principal component is therefore:\n$$\nf_j \\;=\\; \\frac{\\lambda_j}{\\sum_{i=1}^{P}\\lambda_i}\n$$\nThe final results for each case consist of the integer count $\\hat{r}$, followed by the $\\hat{r}$ fractional variance contributions $\\{f_1, \\ldots, f_{\\hat{r}}\\}$, with each fraction rounded to six decimal places.\n\n### Implementation\n\nThe procedure is implemented in Python using the `numpy` library for all numerical operations. A single function encapsulates the logic for each test case: it generates data using the specified parameters and random seed, computes the centered covariance matrix, finds its eigenvalues, applies the Marchenko-Pastur threshold to identify spikes, and calculates the corresponding variance fractions. A main script iterates through the provided test suite, calls this function, and formats the collected results into a single output string as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef process_case(N, P, spikes, seed):\n    \"\"\"\n    Processes a single test case according to the problem description.\n\n    Args:\n        N (int): Number of cells.\n        P (int): Number of proteins.\n        spikes (list of float): List of population spike strengths {beta_k}.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        list: A list containing the number of detected extrinsic axes (r_hat)\n              followed by the fractional variance contributions of each axis.\n    \"\"\"\n    # 1. Generate synthetic data\n    rng = np.random.default_rng(seed)\n\n    # Generate intrinsic noise matrix Z\n    Z = rng.standard_normal((N, P))\n\n    # Generate extrinsic variability part if spikes exist\n    r = len(spikes)\n    if r  0:\n        # Generate factor scores F\n        F = rng.standard_normal((N, r))\n        \n        # Generate protein loading matrix L\n        # First, create a matrix Q with orthonormal columns\n        temp_matrix = rng.standard_normal((P, r))\n        Q, _ = np.linalg.qr(temp_matrix)\n        \n        # Then, construct L\n        sqrt_betas = np.diag(np.sqrt(spikes))\n        L = Q @ sqrt_betas\n        \n        # Combine to form the data matrix X\n        X = Z + F @ L.T\n    else:\n        # If no spikes, X is just the noise matrix\n        X = Z\n\n    # 2. Center data and compute sample covariance S\n    X_centered = X - X.mean(axis=0)\n    S = (X_centered.T @ X_centered) / (N - 1)\n\n    # 3. Perform PCA by eigendecomposition of S\n    # eigvalsh is used for symmetric matrices; it's efficient and guarantees real eigenvalues.\n    eigenvalues = np.linalg.eigvalsh(S)\n    # Sort eigenvalues in descending order\n    eigenvalues = np.sort(eigenvalues)[::-1]\n\n    # 4. Identify dominant extrinsic axes using the Marchenko-Pastur threshold\n    c = P / (N - 1)\n    lambda_plus = (1 + np.sqrt(c))**2\n    \n    # Count eigenvalues strictly greater than the threshold\n    r_hat = np.sum(eigenvalues  lambda_plus)\n\n    # 5. Quantify variance contribution\n    if r_hat  0:\n        total_variance = np.sum(eigenvalues)\n        dominant_eigenvalues = eigenvalues[:r_hat]\n        variance_fractions = dominant_eigenvalues / total_variance\n        \n        # Round to six decimal places\n        rounded_fractions = np.round(variance_fractions, 6).tolist()\n        \n        result = [r_hat] + rounded_fractions\n    else:\n        result = [0]\n        \n    return result\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    s0 = 20231107\n    test_cases = [\n        # Case A: N=1000, P=60, spikes={2.5, 1.6}, delta=0\n        {'N': 1000, 'P': 60, 'spikes': [2.5, 1.6], 'seed': s0 + 0},\n        # Case B: N=800, P=80, spikes={}, delta=1\n        {'N': 800, 'P': 80, 'spikes': [], 'seed': s0 + 1},\n        # Case C: N=120, P=100, spikes={5.0, 0.8}, delta=2\n        {'N': 120, 'P': 100, 'spikes': [5.0, 0.8], 'seed': s0 + 2},\n        # Case D: N=600, P=50, spikes={1.5, 0.4, 0.3}, delta=3\n        {'N': 600, 'P': 50, 'spikes': [1.5, 0.4, 0.3], 'seed': s0 + 3},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        case_result = process_case(case['N'], case['P'], case['spikes'], case['seed'])\n        all_results.append(case_result)\n\n    # Format the final output string exactly as required\n    formatted_cases = []\n    for res in all_results:\n        # Convert each element in the inner list to a string\n        stringified_elements = [str(x) for x in res]\n        # Join them with commas and wrap in brackets\n        formatted_cases.append(f\"[{','.join(stringified_elements)}]\")\n    \n    # Join all formatted cases with commas and wrap in outer brackets\n    final_output = f\"[{','.join(formatted_cases)}]\"\n    \n    print(final_output)\n\nsolve()\n\n```", "id": "4357150"}]}