{"hands_on_practices": [{"introduction": "The first step in any stochastic modeling effort is to translate a set of chemical reactions into the mathematical framework of the Chemical Master Equation (CME). This foundational exercise guides you through that process for a simple birth-death system. By carefully considering the probability fluxes into and out of a given state, you will construct the CME from first principles, a skill that is essential for all further analysis. [@problem_id:4392634]", "problem": "Consider a well-stirred single-species biochemical system in a fixed volume, modeled as a continuous-time Markov chain (CTMC). The molecular count of species $A$ at time $t$ is denoted by the integer-valued random variable $n(t) \\in \\{0,1,2,\\dots\\}$. The reaction network consists of two reactions: a birth reaction $A \\to A + A$ with rate constant $k_b > 0$, and a death reaction $A \\to \\varnothing$ with rate constant $k_d > 0$. Under mass-action kinetics, the propensity functions are $a_1(n) = k_b n$ for the birth reaction and $a_2(n) = k_d n$ for the death reaction. Starting from the balance-of-probability principle that defines the Chemical Master Equation (CME), derive the explicit CME for the probability $P(n,t)$ of being in state $n$ at time $t$, including appropriate boundary considerations. Then, compute the one-step transition rates out of state $n$ to the states $n+1$ and $n-1$. Express your final answer as a row matrix whose first entry is the rate to $n+1$ and second entry is the rate to $n-1$. No rounding is required, and no units should be included in the final expression.", "solution": "The problem statement has been validated and is deemed a well-posed, scientifically grounded problem in the field of systems biomedicine, specifically concerning stochastic chemical kinetics. It provides all necessary information to derive the Chemical Master Equation and the requested transition rates.\n\nThe Chemical Master Equation (CME) describes the time evolution of the probability $P(n, t)$ of a system having $n$ molecules of a species at time $t$. The equation is derived from the balance-of-probability principle for a continuous-time Markov chain. The rate of change of the probability of being in a particular state $n$ is the sum of the rates of all transitions *into* state $n$ minus the sum of the rates of all transitions *out of* state $n$. In its general form, the CME is written as:\n$$ \\frac{dP(n,t)}{dt} = \\sum_{n' \\neq n} \\left[ W(n|n')P(n',t) - W(n'|n)P(n,t) \\right] $$\nwhere $W(n'|n)$ is the transition rate from state $n$ to state $n'$. In the context of chemical reactions, this transition rate is given by the propensity function $a(n)$ of the reaction that causes the state change.\n\nThe system under consideration involves a single species $A$ and two reactions:\n1. Birth: $A \\to A + A$. This reaction increments the molecular count by one, causing a state transition $n \\to n+1$. The propensity function is given as $a_1(n) = k_b n$.\n2. Death: $A \\to \\varnothing$. This reaction decrements the molecular count by one, causing a state transition $n \\to n-1$. The propensity function is given as $a_2(n) = k_d n$.\n\nWe can now construct the CME for an arbitrary state $n$, where $n$ is an integer $n \\ge 1$.\n\nThe total rate of transitions *into* state $n$ (the gain term) is the sum of probabilities of two events:\n- A transition from state $n-1$ to $n$ via the birth reaction. The rate of this transition is the propensity evaluated at state $n-1$, which is $a_1(n-1) = k_b (n-1)$. The total flux into state $n$ from $n-1$ is thus $k_b (n-1) P(n-1, t)$. This term is only non-zero for $n > 1$.\n- A transition from state $n+1$ to $n$ via the death reaction. The rate of this transition is the propensity evaluated at state $n+1$, which is $a_2(n+1) = k_d (n+1)$. The total flux into state $n$ from $n+1$ is thus $k_d (n+1) P(n+1, t)$.\n\nThe total rate of transitions *out of* state $n$ (the loss term) is the sum of probabilities of two events:\n- A transition from state $n$ to $n+1$ via the birth reaction. The rate is given by the propensity $a_1(n) = k_b n$. The total flux out of state $n$ to $n+1$ is $k_b n P(n, t)$.\n- A transition from state $n$ to $n-1$ via the death reaction. The rate is given by the propensity $a_2(n) = k_d n$. The total flux out of state $n$ to $n-1$ is $k_d n P(n, t)$.\n\nCombining these gain and loss terms, the CME for $n \\ge 1$ is:\n$$ \\frac{dP(n,t)}{dt} = \\left[ k_b (n-1) P(n-1,t) + k_d (n+1) P(n+1,t) \\right] - \\left[ k_b n P(n,t) + k_d n P(n,t) \\right] $$\n$$ \\frac{dP(n,t)}{dt} = k_b (n-1) P(n-1,t) + k_d(n+1)P(n+1,t) - (k_b + k_d) n P(n,t) $$\n\nNext, we must consider the boundary condition at $n=0$.\n- Gain term for state $n=0$: The only way to enter state $0$ is from state $1$ via the death reaction. The rate of this transition is $a_2(1) = k_d \\cdot 1 = k_d$. The flux into state $0$ is thus $k_d P(1,t)$.\n- Loss term for state $n=0$: To leave state $0$, a reaction must occur. The propensities for the birth and death reactions at $n=0$ are $a_1(0) = k_b \\cdot 0 = 0$ and $a_2(0) = k_d \\cdot 0 = 0$. Since both propensities are zero, no reaction can occur. The total rate of transitions out of state $0$ is $0$. This means that $n=0$ is an absorbing state.\n\nThe CME for the boundary state $n=0$ is therefore:\n$$ \\frac{dP(0,t)}{dt} = k_d P(1,t) $$\n\nThe problem then asks to compute the one-step transition rates out of state $n$ to states $n+1$ and $n-1$. These rates are, by definition, the propensity functions for the reactions that cause these specific transitions.\n- The transition from state $n$ to state $n+1$ is caused by the birth reaction $A \\to A+A$. The rate for this transition is its propensity, $W(n+1|n) = a_1(n) = k_b n$.\n- The transition from state $n$ to state $n-1$ is caused by the death reaction $A \\to \\varnothing$. The rate for this transition is its propensity, $W(n-1|n) = a_2(n) = k_d n$.\n\nThese rates are valid for any state $n \\ge 1$. For $n=0$, both rates are $0$. The question asks for the general expressions for a state $n$.\n\nThe final answer is to be provided as a row matrix where the first entry is the rate to $n+1$ and the second entry is the rate to $n-1$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nk_b n & k_d n\n\\end{pmatrix}\n}\n$$", "id": "4392634"}, {"introduction": "Once the Chemical Master Equation is formulated, a key goal is to find its solution, which describes the probability of observing the system in any possible state. This practice introduces a powerful analytical technique, the probability generating function (PGF), to determine the stationary (long-term) distribution for a canonical model of gene expression. This exercise reveals how microscopic rate constants ($k_s$ and $k_d$) give rise to a predictable, macroscopic probability distribution—a core concept in systems biology. [@problem_id:4392648]", "problem": "Consider a well-mixed single-cell gene expression model in which a single gene synthesizes protein molecules and proteins degrade independently. Let $X(t) \\in \\{0,1,2,\\dots\\}$ denote the protein copy number at time $t$. Assume that synthesis events occur with constant propensity $k_s > 0$ and each protein independently degrades with propensity $k_d > 0$, so that the total degradation propensity when the state is $n$ is $k_d n$. The time evolution of the probability distribution $P(n,t) = \\mathbb{P}\\{X(t) = n\\}$ is governed by the chemical master equation (CME), a balance equation over the discrete states $n$. Using the CME as the foundational description, and the probability generating function (PGF) $G(z,t) = \\sum_{n=0}^{\\infty} z^{n} P(n,t)$ as a tool, derive the stationary distribution $P_{\\mathrm{st}}(n)$ for $n \\in \\mathbb{N}_0$ under the assumption that a stationary limit exists. Your derivation must start from the CME and proceed by transforming it with the PGF, obtaining and solving the resulting differential equation for the stationary PGF, and then extracting $P_{\\mathrm{st}}(n)$ from the stationary PGF. Finally, verify from first principles that the stationary distribution is a Poisson distribution and identify its mean in terms of $k_s$ and $k_d$. Report the stationary probability mass function $P_{\\mathrm{st}}(n)$ as a single closed-form analytic expression in terms of $k_s$, $k_d$, and $n$. No numerical evaluation is required, and no rounding is needed. Express your final answer with exact functions and factorials.", "solution": "The user has provided a well-defined problem in systems biomedicine, asking for the derivation of the stationary probability distribution for a simple gene expression model using the chemical master equation (CME) and the probability generating function (PGF).\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n-   System: A well-mixed single-cell gene expression model.\n-   State variable: $X(t) \\in \\{0, 1, 2, \\dots\\}$, the protein copy number at time $t$.\n-   Reactions and propensities:\n    1.  Protein synthesis: $\\emptyset \\xrightarrow{k_s} \\text{Protein}$. The propensity is a constant, $k_s > 0$. This reaction increases the protein count $n$ by $1$.\n    2.  Protein degradation: $\\text{Protein} \\xrightarrow{k_d} \\emptyset$. The propensity for a single protein to degrade is $k_d > 0$. For a state with $n$ proteins, the total degradation propensity is $k_d n$. This reaction decreases the protein count $n$ by $1$.\n-   Governing equation: The time evolution of the probability distribution $P(n,t) = \\mathbb{P}\\{X(t) = n\\}$ is described by the chemical master equation (CME).\n-   Tool: The probability generating function (PGF) is defined as $G(z,t) = \\sum_{n=0}^{\\infty} z^{n} P(n,t)$.\n-   Objective: Derive the stationary distribution $P_{\\mathrm{st}}(n)$ for $n \\in \\mathbb{N}_0$.\n-   Assumption: A stationary limit exists as $t \\to \\infty$.\n-   Required Method:\n    1.  Start from the CME.\n    2.  Transform the CME using the PGF to obtain a differential equation for $G(z,t)$.\n    3.  Solve for the stationary PGF, $G_{\\mathrm{st}}(z)$.\n    4.  Extract $P_{\\mathrm{st}}(n)$ from $G_{\\mathrm{st}}(z)$.\n    5.  Verify that the result is a Poisson distribution and identify its mean.\n-   Required Output: Report $P_{\\mathrm{st}}(n)$ as a single closed-form analytic expression.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem describes a linear birth-death process, a canonical and fundamental model in stochastic chemical kinetics and systems biology. It is scientifically sound.\n-   **Well-Posed:** The problem is mathematically well-posed. The reactions, parameters, and governing equation are clearly specified. The assumption of a stationary state is standard for this type of analysis, and for $k_s, k_d > 0$, a unique stationary distribution is known to exist. The path to the solution is explicitly outlined.\n-   **Objective:** The language is formal, precise, and free of any subjective or ambiguous elements.\n\nThe problem does not violate any of the invalidity criteria. It is a standard, non-trivial, and verifiable problem in the specified field.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution will proceed as requested.\n\n**Derivation of the Stationary Distribution**\n\n**1. The Chemical Master Equation (CME)**\nThe state of the system is the number of protein molecules, $n$. The change in the probability of being in state $n$, $P(n,t)$, is determined by the balance of probability flux into and out of that state.\nThe reactions are:\n-   Synthesis: A transition from state $n-1$ to $n$ with rate $k_s$.\n-   Degradation: A transition from state $n+1$ to $n$ with rate $k_d(n+1)$.\n\nThe CME is given by:\n$\\frac{d P(n,t)}{dt} = (\\text{flux in}) - (\\text{flux out})$\n$\\frac{d P(n,t)}{dt} = [k_s P(n-1,t) + k_d(n+1)P(n+1,t)] - [(k_s + k_d n)P(n,t)]$\n\nThis equation holds for $n \\ge 1$. For the boundary case $n=0$, degradation from state $n=0$ is impossible, and synthesis cannot occur from a state with a negative number of proteins.\n$\\frac{d P(0,t)}{dt} = k_d(1)P(1,t) - k_s P(0,t)$\nWe can represent the full system by the general equation if we adopt the convention that $P(n,t) = 0$ for $n < 0$.\n\n**2. Transformation to the PGF Equation**\nWe use the probability generating function $G(z,t) = \\sum_{n=0}^{\\infty} z^n P(n,t)$. The time derivative is $\\frac{\\partial G}{\\partial t} = \\sum_{n=0}^{\\infty} z^n \\frac{dP(n,t)}{dt}$.\nSubstituting the CME:\n$\\frac{\\partial G}{\\partial t} = \\sum_{n=0}^{\\infty} z^n [k_s P(n-1,t) + k_d(n+1)P(n+1,t) - k_s P(n,t) - k_d n P(n,t)]$\n\nWe analyze each term in the sum separately:\n-   $\\sum_{n=0}^{\\infty} z^n k_s P(n-1,t) = k_s z \\sum_{n=0}^{\\infty} z^{n-1} P(n-1,t)$. Letting $m = n-1$, this becomes $k_s z \\sum_{m=-1}^{\\infty} z^m P(m,t)$. Since $P(-1,t)=0$, this is $k_s z G(z,t)$.\n-   $\\sum_{n=0}^{\\infty} z^n k_d (n+1) P(n+1,t) = k_d \\sum_{n=0}^{\\infty} (n+1) z^n P(n+1,t)$. Letting $m = n+1$, this becomes $k_d \\sum_{m=1}^{\\infty} m z^{m-1} P(m,t)$. This is the derivative of the PGF with respect to $z$: $k_d \\frac{\\partial G(z,t)}{\\partial z}$.\n-   $\\sum_{n=0}^{\\infty} z^n (-k_s P(n,t)) = -k_s \\sum_{n=0}^{\\infty} z^n P(n,t) = -k_s G(z,t)$.\n-   $\\sum_{n=0}^{\\infty} z^n (-k_d n P(n,t)) = -k_d z \\sum_{n=0}^{\\infty} n z^{n-1} P(n,t) = -k_d z \\frac{\\partial G(z,t)}{\\partial z}$.\n\nCombining these terms gives the partial differential equation for $G(z,t)$:\n$\\frac{\\partial G}{\\partial t} = k_s z G(z,t) + k_d \\frac{\\partial G}{\\partial z} - k_s G(z,t) - k_d z \\frac{\\partial G}{\\partial z}$\n$\\frac{\\partial G}{\\partial t} = (z-1) k_s G(z,t) - (z-1) k_d \\frac{\\partial G}{\\partial z}$\n$\\frac{\\partial G}{\\partial t} = (z-1) \\left( k_s G(z,t) - k_d \\frac{\\partial G}{\\partial z} \\right)$\n\n**3. The Stationary PGF**\nAt steady state, the probability distribution is time-invariant, so $\\frac{dP_{\\mathrm{st}}(n)}{dt} = 0$, which implies $\\frac{\\partial G_{\\mathrm{st}}(z)}{\\partial t} = 0$. The PDE becomes an ordinary differential equation (ODE) for the stationary PGF, $G_{\\mathrm{st}}(z)$:\n$0 = (z-1) \\left( k_s G_{\\mathrm{st}}(z) - k_d \\frac{d G_{\\mathrm{st}}(z)}{dz} \\right)$\n\nFor this equation to hold for all $z$ in its domain of convergence (not just at $z=1$), the second factor must be zero:\n$k_s G_{\\mathrm{st}}(z) - k_d \\frac{d G_{\\mathrm{st}}(z)}{dz} = 0$\nThis is a first-order separable ODE:\n$\\frac{dG_{\\mathrm{st}}}{G_{\\mathrm{st}}} = \\frac{k_s}{k_d} dz$\n\nIntegrating both sides yields:\n$\\ln(G_{\\mathrm{st}}(z)) = \\frac{k_s}{k_d} z + C_0$\n$G_{\\mathrm{st}}(z) = C \\exp\\left(\\frac{k_s}{k_d} z\\right)$, where $C = \\exp(C_0)$ is the constant of integration.\n\nTo determine $C$, we use the normalization condition for probabilities: $\\sum_{n=0}^{\\infty} P_{\\mathrm{st}}(n)=1$. In terms of the PGF, this is $G_{\\mathrm{st}}(1)=1$.\n$1 = C \\exp\\left(\\frac{k_s}{k_d} \\cdot 1\\right) \\implies C = \\exp\\left(-\\frac{k_s}{k_d}\\right)$\n\nSubstituting $C$ back, we find the stationary PGF:\n$G_{\\mathrm{st}}(z) = \\exp\\left(-\\frac{k_s}{k_d}\\right) \\exp\\left(\\frac{k_s}{k_d} z\\right) = \\exp\\left(\\frac{k_s}{k_d}(z-1)\\right)$\n\n**4. Extraction of the Stationary Distribution $P_{\\mathrm{st}}(n)$**\nThe probabilities $P_{\\mathrm{st}}(n)$ are the coefficients of the Taylor series expansion of $G_{\\mathrm{st}}(z)$ around $z=0$: $G_{\\mathrm{st}}(z) = \\sum_{n=0}^{\\infty} P_{\\mathrm{st}}(n) z^n$.\nLet $\\lambda = \\frac{k_s}{k_d}$. The stationary PGF is $G_{\\mathrm{st}}(z) = \\exp(\\lambda(z-1)) = \\exp(-\\lambda)\\exp(\\lambda z)$.\nWe expand $\\exp(\\lambda z)$ in a Maclaurin series:\n$\\exp(\\lambda z) = \\sum_{n=0}^{\\infty} \\frac{(\\lambda z)^n}{n!} = \\sum_{n=0}^{\\infty} \\frac{\\lambda^n}{n!} z^n$\n\nTherefore, the PGF can be written as:\n$G_{\\mathrm{st}}(z) = \\exp(-\\lambda) \\sum_{n=0}^{\\infty} \\frac{\\lambda^n}{n!} z^n = \\sum_{n=0}^{\\infty} \\left(\\frac{\\lambda^n \\exp(-\\lambda)}{n!}\\right) z^n$\n\nBy comparing the coefficients of $z^n$ term-by-term with the definition of the PGF, we identify the stationary probability mass function:\n$P_{\\mathrm{st}}(n) = \\frac{\\lambda^n \\exp(-\\lambda)}{n!}$\n\n**5. Verification and Identification of the Mean**\nSubstituting $\\lambda = \\frac{k_s}{k_d}$ back into the expression for $P_{\\mathrm{st}}(n)$:\n$P_{\\mathrm{st}}(n) = \\frac{\\left(\\frac{k_s}{k_d}\\right)^n \\exp\\left(-\\frac{k_s}{k_d}\\right)}{n!}$\n\nThis is the probability mass function of a Poisson distribution with parameter (mean) $\\mu = \\frac{k_s}{k_d}$. The mean of the stationary distribution, which represents the average number of protein molecules in the cell at steady state, is indeed $\\mathbb{E}[n] = \\frac{k_s}{k_d}$. This completes the verification.\n\nThe final analytical expression for the stationary probability mass function $P_{\\mathrm{st}}(n)$ is:\n$P_{\\mathrm{st}}(n) = \\frac{1}{n!} \\left(\\frac{k_s}{k_d}\\right)^n \\exp\\left(-\\frac{k_s}{k_d}\\right)$", "answer": "$$\n\\boxed{\\frac{\\left(\\frac{k_s}{k_d}\\right)^n}{n!} \\exp\\left(-\\frac{k_s}{k_d}\\right)}\n$$", "id": "4392648"}, {"introduction": "While analytical solutions provide deep insight, they are often unattainable for the complex networks found in biology. This hands-on practice introduces the premier computational tool for navigating this complexity: the Gillespie Stochastic Simulation Algorithm (SSA). You will implement this algorithm to generate exact numerical trajectories for the gene expression model from the previous exercise, and then perform the crucial step of validating your computational results against exact analytical solutions, bridging the vital gap between theory and simulation. [@problem_id:4392582]", "problem": "Consider a single-species gene expression birth–death process modeled as a continuous-time, discrete-state Markov jump process under the Chemical Master Equation (CME). Let $X(t) \\in \\mathbb{N}_0$ denote the molecule copy number at time $t$. The system has two reaction channels: synthesis (birth) with constant propensity $a_1(n) = k_b$ producing $X \\to X + 1$, and degradation (death) with propensity $a_2(n) = k_d n$ producing $X \\to X - 1$. The initial copy number is $X(0) = x_0$. Time is measured in seconds, rates in $\\mathrm{s^{-1}}$, and copy numbers are unitless counts.\n\nStarting only from the CME and the definition of expectation and variance, you must:\n1. Implement Gillespie’s direct stochastic simulation algorithm for the above birth–death process to generate trajectories $X(t)$ until a fixed terminal time $T$.\n2. Simulate $M$ independent trajectories and record the ensemble molecule counts at time $T$, $\\{X_i(T)\\}_{i=1}^M$.\n3. Compute the empirical mean $\\hat{m}(T)$ and empirical variance $\\hat{v}(T)$ of $\\{X_i(T)\\}_{i=1}^M$.\n4. Independently derive, from the CME, closed-form expressions for the first and second moments at time $T$, namely the exact mean $m(T)$ and variance $v(T)$, for arbitrary nonnegative parameters $k_b$, $k_d$, $x_0$, and $T$.\n5. Verify consistency between simulation and CME-derived moments by checking whether both the absolute relative error in the mean and the absolute relative error in the variance are each less than or equal to a fixed tolerance $\\varepsilon$, that is, whether\n$$\n\\frac{\\lvert \\hat{m}(T) - m(T) \\rvert}{\\max(m(T), 10^{-12})} \\le \\varepsilon\n\\quad \\text{and} \\quad\n\\frac{\\lvert \\hat{v}(T) - v(T) \\rvert}{\\max(v(T), 10^{-12})} \\le \\varepsilon.\n$$\n\nUse Gillespie’s direct method with exact exponential waiting times and reaction-channel selection based on propensities. Use a fixed pseudorandom seed equal to $12345$ for reproducibility. Use $M = 6000$ trajectories per test case and $\\varepsilon = 0.05$.\n\nImplement your program to handle the following test suite of parameter sets, covering typical behavior and boundary cases. For each case, the synthesis rate is $k_b$ in $\\mathrm{s^{-1}}$, the degradation rate is $k_d$ in $\\mathrm{s^{-1}}$, the initial count is $x_0$ in molecules, and the terminal time is $T$ in seconds:\n- Case 1 (typical): $k_b = 5.0$, $k_d = 1.0$, $x_0 = 0$, $T = 3.0$.\n- Case 2 (no synthesis): $k_b = 0.0$, $k_d = 0.7$, $x_0 = 50$, $T = 1.5$.\n- Case 3 (slow degradation, long time): $k_b = 2.0$, $k_d = 0.1$, $x_0 = 0$, $T = 20.0$.\n- Case 4 (fast degradation, moderate synthesis): $k_b = 9.0$, $k_d = 3.0$, $x_0 = 0$, $T = 1.0$.\n- Case 5 (nonzero initial condition): $k_b = 4.0$, $k_d = 0.5$, $x_0 = 20$, $T = 2.0$.\n- Case 6 (no degradation): $k_b = 1.5$, $k_d = 0.0$, $x_0 = 3$, $T = 4.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is a boolean indicating whether the corresponding case passes the tolerance check for both moments, in the order listed above (for example, “[True,False,True,True,True,False]”).", "solution": "The problem requires the derivation of the time-dependent mean and variance for a single-species birth-death process from the Chemical Master Equation (CME), and the verification of these analytical results against stochastic simulations using Gillespie's direct method. The problem is well-posed, scientifically grounded, and provides all necessary parameters for a complete solution.\n\n### 1. Derivation of Exact Moments from the Chemical Master Equation\n\nThe system consists of two reactions for a species $X$ with copy number $n$:\n1.  Synthesis (birth): $\\emptyset \\xrightarrow{k_b} X$, with propensity $a_1(n) = k_b$.\n2.  Degradation (death): $X \\xrightarrow{k_d} \\emptyset$, with propensity $a_2(n) = k_d n$.\n\nLet $P(n,t)$ be the probability that the system has $n$ molecules at time $t$. The CME, which governs the time evolution of $P(n,t)$, is:\n$$\n\\frac{dP(n,t)}{dt} = k_b P(n-1,t) + k_d(n+1)P(n+1,t) - (k_b + k_d n)P(n,t)\n$$\nfor $n \\ge 1$, with a special form for $n=0$: $\\frac{dP(0,t)}{dt} = k_d P(1,t) - k_b P(0,t)$. The general form holds for all $n \\in \\mathbb{N}_0$ if we define $P(-1, t)=0$.\n\n#### 1.1. Mean Molecule Number\n\nThe mean (or first moment) of the molecule count is defined as $m(t) = \\langle n \\rangle(t) = \\sum_{n=0}^{\\infty} n P(n,t)$. The time evolution of the mean is found by taking the time derivative and using the CME:\n$$\n\\frac{dm(t)}{dt} = \\sum_{n=0}^{\\infty} n \\frac{dP(n,t)}{dt}\n$$\nSubstituting the CME and evaluating the sum term by term:\n$$\n\\frac{dm(t)}{dt} = \\sum_{n=0}^{\\infty} n \\left[ k_b P(n-1,t) + k_d(n+1)P(n+1,t) - (k_b + k_d n)P(n,t) \\right]\n$$\n$$\n\\frac{dm(t)}{dt} = k_b \\sum_{n=1}^{\\infty} n P(n-1,t) + k_d \\sum_{n=0}^{\\infty} n(n+1)P(n+1,t) - k_b \\sum_{n=0}^{\\infty} n P(n,t) - k_d \\sum_{n=0}^{\\infty} n^2 P(n,t)\n$$\nBy re-indexing the sums (e.g., letting $j=n-1$ in the first sum), we get:\n$$\n\\frac{dm(t)}{dt} = k_b \\sum_{j=0}^{\\infty} (j+1) P(j,t) + k_d \\sum_{j=1}^{\\infty} (j-1)j P(j,t) - k_b \\langle n \\rangle - k_d \\langle n^2 \\rangle\n$$\n$$\n\\frac{dm(t)}{dt} = k_b (\\langle n \\rangle + 1) + k_d (\\langle n^2 \\rangle - \\langle n \\rangle) - k_b \\langle n \\rangle - k_d \\langle n^2 \\rangle\n$$\n$$\n\\frac{dm(t)}{dt} = k_b \\langle n \\rangle + k_b + k_d \\langle n^2 \\rangle - k_d \\langle n \\rangle - k_b \\langle n \\rangle - k_d \\langle n^2 \\rangle\n$$\nThis simplifies to a first-order linear ordinary differential equation (ODE) for the mean $m(t)$:\n$$\n\\frac{dm(t)}{dt} = k_b - k_d m(t)\n$$\nGiven the initial condition $m(0) = X(0) = x_0$, we can solve this ODE.\n- If $k_d > 0$: The solution is\n  $$\n  m(t) = \\frac{k_b}{k_d} + \\left(x_0 - \\frac{k_b}{k_d}\\right) e^{-k_d t}\n  $$\n- If $k_d = 0$: The ODE becomes $\\frac{dm(t)}{dt} = k_b$, which integrates to\n  $$\n  m(t) = x_0 + k_b t\n  $$\n\n#### 1.2. Variance of Molecule Number\n\nThe variance is $v(t) = \\langle n^2 \\rangle - \\langle n \\rangle^2$. We first derive an ODE for the variance. The time derivative of the variance is $\\frac{dv}{dt} = \\frac{d\\langle n^2 \\rangle}{dt} - 2\\langle n \\rangle \\frac{d\\langle n \\rangle}{dt}$.\nWe need the ODE for the second moment, $\\langle n^2 \\rangle = \\sum_{n=0}^{\\infty} n^2 P(n,t)$. Similarly to the mean, we find:\n$$\n\\frac{d\\langle n^2 \\rangle}{dt} = \\sum_{n=0}^{\\infty} n^2 \\frac{dP(n,t)}{dt} = (2k_b + k_d)m(t) + k_b - 2k_d \\langle n^2 \\rangle\n$$\nSubstituting this and the ODE for $m(t)$ into the expression for $\\frac{dv}{dt}$:\n$$\n\\frac{dv}{dt} = \\left[ (2k_b + k_d)m(t) + k_b - 2k_d \\langle n^2 \\rangle \\right] - 2m(t) (k_b - k_d m(t))\n$$\n$$\n\\frac{dv}{dt} = 2k_b m(t) + k_d m(t) + k_b - 2k_d \\langle n^2 \\rangle - 2k_b m(t) + 2k_d m(t)^2\n$$\n$$\n\\frac{dv}{dt} = k_b + k_d m(t) - 2k_d (\\langle n^2 \\rangle - m(t)^2)\n$$\nThis yields an ODE for the variance $v(t)$:\n$$\n\\frac{dv(t)}{dt} = k_b + k_d m(t) - 2k_d v(t)\n$$\nWith the initial condition $v(0)=0$ (since the initial state $x_0$ is deterministic), we can solve this ODE by substituting the solution for $m(t)$.\n- If $k_d > 0$:\n  $$\n  \\frac{dv}{dt} + 2k_d v = k_b + k_d \\left[ \\frac{k_b}{k_d} + \\left(x_0 - \\frac{k_b}{k_d}\\right) e^{-k_d t} \\right] = 2k_b + (k_d x_0 - k_b) e^{-k_d t}\n  $$\n  Solving this linear first-order ODE with $v(0)=0$ gives:\n  $$\n  v(t) = \\frac{k_b}{k_d} + \\left(x_0 - \\frac{k_b}{k_d}\\right) e^{-k_d t} - x_0 e^{-2k_d t}\n  $$\n  This can be conveniently rewritten as:\n  $$\n  v(t) = \\frac{k_b}{k_d}(1 - e^{-k_d t}) + x_0 e^{-k_d t}(1 - e^{-k_d t})\n  $$\n- If $k_d = 0$: The ODE for variance becomes $\\frac{dv}{dt} = k_b$. With $v(0)=0$, this integrates to\n  $$\n  v(t) = k_b t\n  $$\n\n### 2. Stochastic Simulation and Verification\n\nGillespie's Stochastic Simulation Algorithm (SSA), specifically the direct method, is used to generate exact numerical realizations of the Markov process.\n\n#### 2.1. Algorithm\nFor a single trajectory starting with $n=x_0$ at $t=0$, until $t \\ge T$:\n1.  Calculate propensities: $a_1 = k_b$, $a_2 = k_d n$.\n2.  Calculate total propensity: $a_{tot} = a_1 + a_2$.\n3.  If $a_{tot} = 0$, the state is absorbing. The simulation time is advanced to $T$ and the trajectory ends.\n4.  Generate two random numbers $r_1, r_2$ from a uniform distribution $U(0,1)$.\n5.  Calculate the time to the next reaction: $\\tau = \\frac{1}{a_{tot}} \\ln(\\frac{1}{r_1})$.\n6.  If $t+\\tau \\ge T$, the trajectory ends; the state remains $n$.\n7.  Otherwise, advance time: $t \\leftarrow t + \\tau$.\n8.  Select the reaction: if $r_2 \\cdot a_{tot} < a_1$, it is a synthesis ($n \\leftarrow n+1$); otherwise, it is a degradation ($n \\leftarrow n-1$).\n9.  Repeat from Step 1.\n\n#### 2.2. Verification\nThis process is repeated for $M=6000$ independent trajectories for each parameter set, using a fixed pseudorandom number generator seed of $12345$ for reproducibility. The ensemble of final molecule counts $\\{X_i(T)\\}_{i=1}^M$ is collected.\n\nThe empirical mean $\\hat{m}(T)$ and empirical variance $\\hat{v}(T)$ are computed from this ensemble:\n$$\n\\hat{m}(T) = \\frac{1}{M}\\sum_{i=1}^{M} X_i(T) \\qquad \\hat{v}(T) = \\frac{1}{M}\\sum_{i=1}^{M} (X_i(T) - \\hat{m}(T))^2\n$$\nConsistency is checked by comparing these empirical moments to the analytical moments $m(T)$ and $v(T)$ derived above. The check passes if both the absolute relative error for the mean and for the variance are less than or equal to the tolerance $\\varepsilon=0.05$:\n$$\n\\frac{\\lvert \\hat{m}(T) - m(T) \\rvert}{\\max(m(T), 10^{-12})} \\le \\varepsilon\n\\quad \\text{and} \\quad\n\\frac{\\lvert \\hat{v}(T) - v(T) \\rvert}{\\max(v(T), 10^{-12})} \\le \\varepsilon\n$$\nThe value $10^{-12}$ is used as a floor for the denominator to prevent division by zero or near-zero values.\n\nThe implementation will apply these steps to each test case, producing a boolean result indicating whether the consistency check passed.", "answer": "```python\nimport numpy as np\n\ndef gillespie_single_trajectory(kb, kd, x0, T, rng):\n    \"\"\"\n    Simulates a single trajectory of the birth-death process using Gillespie's direct method.\n    \n    Args:\n        kb (float): Synthesis rate.\n        kd (float): Degradation rate.\n        x0 (int): Initial molecule count.\n        T (float): Terminal time.\n        rng (np.random.Generator): A numpy random number generator instance.\n        \n    Returns:\n        int: The molecule count at time T.\n    \"\"\"\n    t = 0.0\n    n = int(x0)\n    \n    while t < T:\n        a1 = kb\n        a2 = kd * n\n        a_tot = a1 + a2\n        \n        if a_tot <= 1e-12:  # No more reactions can occur\n            break\n            \n        r1 = rng.random()\n        tau = -np.log(r1) / a_tot\n        \n        if t + tau >= T:\n            # Next reaction occurs after T, so state at T is the current state.\n            break\n            \n        t += tau\n        \n        r2 = rng.random()\n        \n        if r2 * a_tot < a1:\n            n += 1  # Synthesis\n        else:\n            n -= 1  # Degradation\n\n    return n\n\ndef run_simulation_ensemble(kb, kd, x0, T, M, rng):\n    \"\"\"\n    Runs an ensemble of Gillespie simulations and computes empirical moments.\n    \n    Args:\n        kb, kd, x0, T: Parameters for the simulation.\n        M (int): Number of trajectories in the ensemble.\n        rng (np.random.Generator): A numpy random number generator instance.\n        \n    Returns:\n        tuple[float, float]: The empirical mean and variance.\n    \"\"\"\n    final_counts = np.zeros(M, dtype=int)\n    for i in range(M):\n        final_counts[i] = gillespie_single_trajectory(kb, kd, x0, T, rng)\n        \n    m_hat = np.mean(final_counts)\n    v_hat = np.var(final_counts)  # ddof=0 is default, correct for empirical variance\n    return m_hat, v_hat\n\ndef calculate_analytical_moments(kb, kd, x0, T):\n    \"\"\"\n    Calculates the exact analytical mean and variance at time T.\n    \n    Args:\n        kb, kd, x0, T: Parameters for the model.\n        \n    Returns:\n        tuple[float, float]: The analytical mean and variance.\n    \"\"\"\n    if kd > 1e-12:  # General case for kd > 0\n        kb_over_kd = kb / kd\n        exp_term = np.exp(-kd * T)\n        \n        m_T = kb_over_kd + (x0 - kb_over_kd) * exp_term\n        \n        # This form is numerically stable and directly derived\n        v_T = kb_over_kd * (1 - exp_term) + x0 * exp_term * (1 - exp_term)\n    else:  # Special case for kd = 0 (Poisson process)\n        m_T = kb * T + x0\n        v_T = kb * T\n        \n    return m_T, v_T\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (kb, kd, x0, T)\n        (5.0, 1.0, 0, 3.0),\n        (0.0, 0.7, 50, 1.5),\n        (2.0, 0.1, 0, 20.0),\n        (9.0, 3.0, 0, 1.0),\n        (4.0, 0.5, 20, 2.0),\n        (1.5, 0.0, 3, 4.0),\n    ]\n    \n    M = 6000\n    epsilon = 0.05\n    seed = 12345\n    \n    results = []\n    \n    for case in test_cases:\n        kb, kd, x0, T = case\n        \n        # Calculate analytical moments\n        m_T, v_T = calculate_analytical_moments(kb, kd, x0, T)\n        \n        # Run stochastic simulation ensemble\n        # A new RNG is created for each case to ensure independent reproducibility\n        rng = np.random.default_rng(seed)\n        m_hat, v_hat = run_simulation_ensemble(kb, kd, x0, T, M, rng)\n        \n        # Verify consistency\n        err_m = np.abs(m_hat - m_T) / max(m_T, 1e-12)\n        err_v = np.abs(v_hat - v_T) / max(v_T, 1e-12)\n        \n        passes_check = (err_m <= epsilon) and (err_v <= epsilon)\n        results.append(passes_check)\n        \n    # Format and print the final output exactly as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "4392582"}]}