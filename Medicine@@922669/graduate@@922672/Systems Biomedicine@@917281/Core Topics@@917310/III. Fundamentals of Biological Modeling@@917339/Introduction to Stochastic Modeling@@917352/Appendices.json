{"hands_on_practices": [{"introduction": "To understand how macroscopic behaviors emerge from microscopic stochastic events, we often start with the Chemical Master Equation (CME). This exercise guides you through deriving the time evolution of the first and second moments for a nonlinear reaction network. This fundamental practice reveals the \"moment closure problem,\" a core challenge in stochastic modeling where the equation for a lower-order moment (like the mean) depends on higher-order moments (like the variance), creating an unclosed hierarchy [@problem_id:4357840].", "problem": "Consider a stochastic chemical reaction network modeling receptor monomer dimerization in a cellular system. Let $X(t) \\in \\mathbb{N}_{0}$ denote the random copy number of monomeric receptors and $Y(t) \\in \\mathbb{N}_{0}$ denote the random copy number of dimeric receptors at time $t$. The network consists of the following reactions, each obeying discrete-state mass-action kinetics under the Chemical Master Equation (CME):\n- Monomer synthesis: $\\varnothing \\xrightarrow{k_{s}} X$, with stoichiometric change $\\nu_{1} = (+1, 0)$ and propensity $a_{1}(x,y) = k_{s}$.\n- Monomer degradation: $X \\xrightarrow{\\gamma} \\varnothing$, with stoichiometric change $\\nu_{2} = (-1, 0)$ and propensity $a_{2}(x,y) = \\gamma x$.\n- Dimerization: $2 X \\xrightarrow{k_{\\mathrm{dim}}} Y$, with stoichiometric change $\\nu_{3} = (-2, +1)$ and propensity $a_{3}(x,y) = k_{\\mathrm{dim}} \\frac{x(x-1)}{2}$.\n- Dimer degradation: $Y \\xrightarrow{\\delta} \\varnothing$, with stoichiometric change $\\nu_{4} = (0, -1)$ and propensity $a_{4}(x,y) = \\delta y$.\n\nStarting from the definition of the Chemical Master Equation as the fundamental law governing the time evolution of the probability mass function of $(X(t), Y(t))$, derive the exact time-evolution equations for the first moments $m_{X}(t) = \\mathbb{E}[X(t)]$ and $m_{Y}(t) = \\mathbb{E}[Y(t)]$, and for the second moments $s_{XX}(t) = \\mathbb{E}[X(t)^{2}]$, $s_{YY}(t) = \\mathbb{E}[Y(t)^{2}]$, and $s_{XY}(t) = \\mathbb{E}[X(t) Y(t)]$. Your derivation must explicitly show where and how higher-order moments (such as $\\mathbb{E}[X(t)^{3}]$ or joint moments involving products of $X(t)$ and $Y(t)$) enter the dynamics due to the nonlinear dimerization reaction.\n\nExpress all steps symbolically without invoking any moment-closure approximations. Clearly indicate each reaction’s contribution to the derivatives of the specified moments by using the stoichiometric changes and propensities, and by applying the CME-based shift operator approach to compute terms of the form $f(N(t)+\\nu_{r}) - f(N(t))$, where $N(t) = (X(t), Y(t))$ and $\\nu_{r}$ is the stoichiometric change vector for reaction $r$.\n\nFor your final answer, provide the single closed-form symbolic expression for the exact contribution of the dimerization reaction to the time derivative of the second moment of the monomer count, that is, the contribution of $2 X \\xrightarrow{k_{\\mathrm{dim}}} Y$ to $\\frac{\\mathrm{d}}{\\mathrm{d}t} \\mathbb{E}[X(t)^{2}]$. No numerical evaluation or rounding is required, and no physical units are needed in the final expression.", "solution": "The problem is valid as it presents a well-defined, scientifically grounded task in stochastic chemical kinetics, a core topic in systems biomedicine. All necessary information is provided, the terminology is precise, and the objective is formalizable. We proceed with the derivation.\n\nThe time evolution of the joint probability mass function $P(x, y, t) = \\mathrm{Prob}(X(t)=x, Y(t)=y)$ for the state vector $(x, y)$ is governed by the Chemical Master Equation (CME). A direct consequence of the CME is the general formula for the time evolution of the expectation of any function $f(X(t), Y(t))$ of the system's state. Let $N(t) = (X(t), Y(t))$ be the state vector. The time derivative of the expectation $\\mathbb{E}[f(N(t))]$ is given by:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} \\mathbb{E}[f(N(t))] = \\sum_{r=1}^{4} \\mathbb{E}\\left[ \\left( f(N(t) + \\nu_{r}) - f(N(t)) \\right) a_{r}(N(t)) \\right]\n$$\nwhere the sum is over the four reactions in the network, $\\nu_r$ is the stoichiometric change vector for reaction $r$, and $a_r(N(t))$ is its propensity function. We apply this formula to derive the equations for the specified moments.\n\n**1. Derivation of First-Moment Equations**\n\nTo find the equation for the mean number of monomers, $m_{X}(t) = \\mathbb{E}[X(t)]$, we let $f(x, y) = x$. The change $f(N+\\nu_r) - f(N)$ for each reaction is simply the change in the number of monomers, which is the first component of the stoichiometric vector $\\nu_r$, denoted $\\nu_{r,X}$.\n\n-   **Reaction 1 (Synthesis):** $f(x+1, y) - f(x, y) = (x+1) - x = 1$. The contribution to the derivative is $\\mathbb{E}[1 \\cdot a_{1}(X(t), Y(t))] = \\mathbb{E}[k_{s}] = k_{s}$.\n-   **Reaction 2 (Monomer Degradation):** $f(x-1, y) - f(x, y) = (x-1) - x = -1$. The contribution is $\\mathbb{E}[-1 \\cdot a_{2}(X(t), Y(t))] = \\mathbb{E}[-\\gamma X(t)] = -\\gamma \\mathbb{E}[X(t)] = -\\gamma m_{X}(t)$.\n-   **Reaction 3 (Dimerization):** $f(x-2, y+1) - f(x, y) = (x-2) - x = -2$. The contribution is $\\mathbb{E}[-2 \\cdot a_{3}(X(t), Y(t))] = \\mathbb{E}[-2 \\cdot k_{\\mathrm{dim}} \\frac{X(t)(X(t)-1)}{2}] = -k_{\\mathrm{dim}} \\mathbb{E}[X(t)^2 - X(t)] = -k_{\\mathrm{dim}}(\\mathbb{E}[X(t)^2] - \\mathbb{E}[X(t)]) = -k_{\\mathrm{dim}}(s_{XX}(t) - m_{X}(t))$.\n-   **Reaction 4 (Dimer Degradation):** $f(x, y-1) - f(x, y) = x - x = 0$. The contribution is $0$.\n\nSumming these contributions gives the full equation for $m_{X}(t)$:\n$$\n\\frac{\\mathrm{d}m_{X}(t)}{\\mathrm{d}t} = k_{s} - \\gamma m_{X}(t) - k_{\\mathrm{dim}}(s_{XX}(t) - m_{X}(t))\n$$\n\nTo find the equation for the mean number of dimers, $m_{Y}(t) = \\mathbb{E}[Y(t)]$, we let $f(x, y) = y$. The relevant change is the second component of $\\nu_r$, denoted $\\nu_{r,Y}$.\n\n-   **Reaction 1:** Contribution is $0$.\n-   **Reaction 2:** Contribution is $0$.\n-   **Reaction 3:** $f(x-2, y+1) - f(x, y) = (y+1) - y = 1$. The contribution is $\\mathbb{E}[1 \\cdot a_{3}(X(t), Y(t))] = \\mathbb{E}[k_{\\mathrm{dim}} \\frac{X(t)(X(t)-1)}{2}] = \\frac{k_{\\mathrm{dim}}}{2}(s_{XX}(t) - m_{X}(t))$.\n-   **Reaction 4:** $f(x, y-1) - f(x, y) = (y-1) - y = -1$. The contribution is $\\mathbb{E}[-1 \\cdot a_{4}(X(t), Y(t))] = \\mathbb{E}[-\\delta Y(t)] = -\\delta m_{Y}(t)$.\n\nSumming these contributions gives the full equation for $m_{Y}(t)$:\n$$\n\\frac{\\mathrm{d}m_{Y}(t)}{\\mathrm{d}t} = \\frac{k_{\\mathrm{dim}}}{2}(s_{XX}(t) - m_{X}(t)) - \\delta m_{Y}(t)\n$$\nNotice that the first moment equations depend on the second moment $s_{XX}(t)$, a feature of the nonlinear dimerization reaction. This is the origin of the moment closure problem.\n\n**2. Derivation of Second-Moment Equations**\n\nTo find the equation for the second moment of monomers, $s_{XX}(t) = \\mathbb{E}[X(t)^2]$, we let $f(x, y) = x^2$.\n\n-   **Reaction 1:** $f(x+1, y) - f(x, y) = (x+1)^2 - x^2 = 2x+1$. The contribution is $\\mathbb{E}[(2X(t)+1)a_{1}(X(t), Y(t))] = \\mathbb{E}[(2X(t)+1)k_{s}] = k_{s}(2m_{X}(t)+1)$.\n-   **Reaction 2:** $f(x-1, y) - f(x, y) = (x-1)^2 - x^2 = -2x+1$. The contribution is $\\mathbb{E}[(-2X(t)+1)a_{2}(X(t), Y(t))] = \\mathbb{E}[(-2X(t)+1)\\gamma X(t)] = \\gamma \\mathbb{E}[-2X(t)^2+X(t)] = \\gamma(-2s_{XX}(t)+m_{X}(t))$.\n-   **Reaction 3:** $f(x-2, y+1) - f(x, y) = (x-2)^2 - x^2 = -4x+4$. The contribution is $\\mathbb{E}[(-4X(t)+4)a_{3}(X(t), Y(t))] = \\mathbb{E}[(-4X(t)+4) k_{\\mathrm{dim}} \\frac{X(t)(X(t)-1)}{2}] = -2k_{\\mathrm{dim}}\\mathbb{E}[(X(t)-1)X(t)(X(t)-1)] = -2k_{\\mathrm{dim}}\\mathbb{E}[X(t)(X(t)-1)^2]$. Expanding this, we get $-2k_{\\mathrm{dim}}\\mathbb{E}[X(t)(X(t)^2-2X(t)+1)] = -2k_{\\mathrm{dim}}\\mathbb{E}[X(t)^3 - 2X(t)^2 + X(t)]$. This term introduces the third moment $\\mathbb{E}[X(t)^3]$, showing how the equation for second moments depends on third moments.\n-   **Reaction 4:** Contribution is $0$.\n\nThe full equation for $s_{XX}(t)$ is:\n$$\n\\frac{\\mathrm{d}s_{XX}(t)}{\\mathrm{d}t} = k_{s}(2m_{X}(t)+1) + \\gamma(-2s_{XX}(t)+m_{X}(t)) - 2k_{\\mathrm{dim}}\\left(\\mathbb{E}[X(t)^3] - 2\\mathbb{E}[X(t)^2] + \\mathbb{E}[X(t)]\\right)\n$$\n\nTo find the equation for $s_{YY}(t) = \\mathbb{E}[Y(t)^2]$, we let $f(x, y) = y^2$.\n\n-   **Reactions 1, 2:** Contribution is $0$.\n-   **Reaction 3:** $f(x-2, y+1) - f(x, y) = (y+1)^2 - y^2 = 2y+1$. The contribution is $\\mathbb{E}[(2Y(t)+1) k_{\\mathrm{dim}} \\frac{X(t)(X(t)-1)}{2}] = \\frac{k_{\\mathrm{dim}}}{2}\\mathbb{E}[(2Y(t)+1)(X(t)^2-X(t))] = k_{\\mathrm{dim}}\\mathbb{E}[X(t)^2 Y(t) - X(t)Y(t)] + \\frac{k_{\\mathrm{dim}}}{2}\\mathbb{E}[X(t)^2-X(t)]$. This term introduces the higher-order mixed moment $\\mathbb{E}[X(t)^2 Y(t)]$.\n-   **Reaction 4:** $f(x, y-1) - f(x, y) = (y-1)^2 - y^2 = -2y+1$. The contribution is $\\mathbb{E}[(-2Y(t)+1)\\delta Y(t)] = \\delta \\mathbb{E}[-2Y(t)^2+Y(t)] = \\delta(-2s_{YY}(t)+m_{Y}(t))$.\n\nThe full equation for $s_{YY}(t)$ involves terms like $\\mathbb{E}[X(t)^2Y(t)]$ and other moments.\n\nTo find the equation for the mixed moment $s_{XY}(t) = \\mathbb{E}[X(t)Y(t)]$, we let $f(x, y) = xy$.\n\n-   **Reaction 1:** $f(x+1, y) - f(x,y) = (x+1)y - xy = y$. Contribution: $\\mathbb{E}[Y(t) k_s] = k_s m_Y(t)$.\n-   **Reaction 2:** $f(x-1, y) - f(x,y) = (x-1)y - xy = -y$. Contribution: $\\mathbb{E}[-Y(t) \\gamma X(t)] = -\\gamma s_{XY}(t)$.\n-   **Reaction 3:** $f(x-2, y+1) - f(x,y) = (x-2)(y+1) - xy = x-2y-2$. Contribution: $\\mathbb{E}[(X(t)-2Y(t)-2) k_{\\mathrm{dim}} \\frac{X(t)(X(t)-1)}{2}]$. This introduces third-order moments like $\\mathbb{E}[X(t)^3]$ and $\\mathbb{E}[X(t)^2 Y(t)]$.\n-   **Reaction 4:** $f(x, y-1) - f(x,y) = x(y-1) - xy = -x$. Contribution: $\\mathbb{E}[-X(t) \\delta Y(t)] = -\\delta s_{XY}(t)$.\n\nThe full equation for $s_{XY}(t)$ also involves higher-order moments.\n\nThe problem specifically requests the contribution of the dimerization reaction ($2X \\xrightarrow{k_{\\mathrm{dim}}} Y$) to the time derivative of the second moment of the monomer count, $\\frac{\\mathrm{d}}{\\mathrm{d}t} \\mathbb{E}[X(t)^{2}]$. As derived above, this contribution arises from the term $\\mathbb{E}\\left[ \\left( (X(t)-2)^2 - X(t)^2 \\right) a_{3}(X(t),Y(t)) \\right]$. It is:\n$$\n-2k_{\\mathrm{dim}} \\left( \\mathbb{E}[X(t)^3] - 2\\mathbb{E}[X(t)^2] + \\mathbb{E}[X(t)] \\right)\n$$", "answer": "$$\n\\boxed{-2k_{\\mathrm{dim}} \\left( \\mathbb{E}[X(t)^{3}] - 2\\mathbb{E}[X(t)^{2}] + \\mathbb{E}[X(t)] \\right)}\n$$", "id": "4357840"}, {"introduction": "When analytical moment equations become intractable, we turn to computational methods to explore the system's dynamics. This practice involves implementing the Gillespie Stochastic Simulation Algorithm (SSA), an exact method for simulating the trajectories of a discrete stochastic system. By simulating a simple gene expression model, you will directly observe the role of intrinsic noise and quantify the deviation of stochastic trajectories from the predictions of a simplified, deterministic mean-field model [@problem_id:4357865].", "problem": "Consider a canonical messenger ribonucleic acid (mRNA) birth–death model used in systems biomedicine for gene expression, represented as a continuous-time Markov chain on the nonnegative integers with state $X(t) \\in \\{0,1,2,\\dots\\}$. The system has two reactions: transcription (immigration) and degradation (death). The reactions and their propensity functions are defined as follows, for any state $x \\in \\mathbb{N}_0$:\n- Transcription: $\\varnothing \\rightarrow \\text{mRNA}$ with propensity $a_1(x) = k$, where $k \\ge 0$ is the transcription rate.\n- Degradation: $\\text{mRNA} \\rightarrow \\varnothing$ with propensity $a_2(x) = \\gamma x$, where $\\gamma \\ge 0$ is the per-molecule degradation rate.\n\nYou are to simulate this system using the exact Stochastic Simulation Algorithm (SSA), also known as the Gillespie direct method, starting from an initial integer copy number $X(0)=x_0$, up to a fixed terminal time $T > 0$. For each parameter set, perform $R$ independent and identically distributed sample path simulations, and compute the following statistics at time $T$:\n- The empirical sample mean $\\widehat{m}(T)$ of $X(T)$ across $R$ paths.\n- The empirical unbiased sample variance $\\widehat{v}(T)$ of $X(T)$ across $R$ paths, computed with denominator $R-1$.\n\nSeparately, compute the deterministic mean-field ordinary differential equation (ODE) solution $x_{\\text{det}}(t)$ governed by the first moment equation\n$$\n\\frac{d}{dt} x_{\\text{det}}(t) = k - \\gamma \\, x_{\\text{det}}(t), \\quad x_{\\text{det}}(0)=x_0,\n$$\nand evaluate $x_{\\text{det}}(T)$. This ODE provides the mean-field prediction for the expected copy number but predicts zero variance.\n\nDefine the following quantitative metrics for each parameter set:\n- Mean absolute error at time $T$: $E_m = \\left| \\widehat{m}(T) - x_{\\text{det}}(T) \\right|$.\n- Analytical variance $v_{\\text{theory}}(T)$ at time $T$, defined by first principles of the immigration–death process:\n  - If $\\gamma > 0$,\n    $$\n    v_{\\text{theory}}(T) = x_0 \\, e^{-\\gamma T} \\bigl(1 - e^{-\\gamma T}\\bigr) + \\frac{k}{\\gamma}\\bigl(1 - e^{-\\gamma T}\\bigr).\n    $$\n  - If $\\gamma = 0$,\n    $$\n    v_{\\text{theory}}(T) = k\\,T.\n    $$\n- Variance absolute error at time $T$: $E_v = \\left| \\widehat{v}(T) - v_{\\text{theory}}(T) \\right|$.\n- Variance discrepancy relative to the mean-field prediction (which is zero), defined as $D_v = \\widehat{v}(T)$.\n\nAlgorithmic requirements:\n- Implement the exact Stochastic Simulation Algorithm (Gillespie direct method). At any state $x$ and time $t$, compute the total propensity $a_0(x) = a_1(x)+a_2(x) = k + \\gamma x$. If $a_0(x) = 0$, the system is absorbing and no further reactions occur. Otherwise, draw the inter-reaction time $\\tau$ from the exponential distribution with rate $a_0(x)$ and advance time by $\\tau$. If $t+\\tau > T$, stop and record the current state as the terminal state. Otherwise, select the reaction type by drawing a uniform variate $u \\in (0,1)$; choose transcription if $u < a_1(x)/a_0(x)$, else choose degradation. Update the state accordingly and iterate until $t \\ge T$.\n- For deterministic mean-field, solve the linear ODE analytically:\n  - If $\\gamma > 0$, $x_{\\text{det}}(T) = x_0 e^{-\\gamma T} + \\dfrac{k}{\\gamma}\\bigl(1 - e^{-\\gamma T}\\bigr)$.\n  - If $\\gamma = 0$, $x_{\\text{det}}(T) = x_0 + k\\,T$.\n- Use a fixed pseudo-random number generator seed $1729$ to ensure reproducibility.\n\nTest suite:\nProvide results for the following four parameter sets, each specified by $(k,\\gamma,x_0,T,R)$:\n- Case $1$: $(k=\\;5.0,\\;\\gamma=\\;1.0,\\;x_0=\\;10,\\;T=\\;2.0,\\;R=\\;3000)$.\n- Case $2$: $(k=\\;0.0,\\;\\gamma=\\;0.7,\\;x_0=\\;25,\\;T=\\;3.5,\\;R=\\;3000)$.\n- Case $3$: $(k=\\;12.0,\\;\\gamma=\\;4.0,\\;x_0=\\;0,\\;T=\\;1.5,\\;R=\\;3000)$.\n- Case $4$: $(k=\\;5.0,\\;\\gamma=\\;0.0,\\;x_0=\\;3,\\;T=\\;1.2,\\;R=\\;3000)$.\n\nYour program must:\n- Implement the Stochastic Simulation Algorithm precisely as specified.\n- For each case, estimate $\\widehat{m}(T)$ and $\\widehat{v}(T)$ from $R$ independent trajectories, compute $x_{\\text{det}}(T)$ and $v_{\\text{theory}}(T)$, and then compute the triplet $(E_m, E_v, D_v)$.\n- Produce a single line of output containing a comma-separated list with all results concatenated in the order of cases $1$ through $4$, each case contributing its three numbers in the order $(E_m, E_v, D_v)$. The output must be a single list string of the form\n  $$\n  [E_m^{(1)},E_v^{(1)},D_v^{(1)},E_m^{(2)},E_v^{(2)},D_v^{(2)},E_m^{(3)},E_v^{(3)},D_v^{(3)},E_m^{(4)},E_v^{(4)},D_v^{(4)}].\n  $$\n\nThere are no physical units in this problem; copy numbers are dimensionless counts and rates are per unit time. Angles and percentages do not appear. The output values must be real numbers (floating-point). No user input is required; the program must run as-is and print the final line as specified.", "solution": "The problem requires the simulation of a canonical mRNA birth–death process using the exact Stochastic Simulation Algorithm (SSA), also known as the Gillespie direct method. The results of the stochastic simulations are to be compared against analytical solutions for the mean and variance of the process.\n\nThe system is described by a continuous-time Markov chain with state $X(t)$, representing the number of mRNA molecules at time $t$. The state space is the set of non-negative integers $\\mathbb{N}_0 = \\{0, 1, 2, \\dots\\}$. The two reactions governing the system dynamics are:\n1.  **Transcription (Birth/Immigration):** $\\varnothing \\rightarrow \\text{mRNA}$, occurring with a constant propensity $a_1(x) = k$. This reaction increases the molecule count by one.\n2.  **Degradation (Death):** $\\text{mRNA} \\rightarrow \\varnothing$, occurring with a propensity proportional to the number of molecules, $a_2(x) = \\gamma x$. This reaction decreases the molecule count by one.\n\nThe parameters $k \\ge 0$ and $\\gamma \\ge 0$ are the transcription rate and per-molecule degradation rate, respectively.\n\n### Stochastic Simulation Algorithm (Gillespie Direct Method)\n\nThe SSA provides a statistically exact method for simulating the time evolution of such a stochastic system. Starting from an initial state $X(0) = x_0$ at time $t=0$, the algorithm proceeds as follows:\n\n1.  **Calculate Propensities:** At a given state $x$, the total propensity for any reaction to occur is the sum of individual propensities:\n    $$\n    a_0(x) = a_1(x) + a_2(x) = k + \\gamma x.\n    $$\n    If $a_0(x) = 0$ (which occurs if and only if $k=0$ and $x=0$), the system is in an absorbing state, and no further reactions can occur. The simulation for that trajectory terminates.\n\n2.  **Sample Time to Next Reaction:** The time interval $\\tau$ until the next reaction is a random variable drawn from an exponential distribution with rate parameter $a_0(x)$:\n    $$\n    \\tau \\sim \\text{Exponential}(a_0(x)).\n    $$\n    The probability density function is $p(\\tau) = a_0(x) e^{-a_0(x)\\tau}$. The simulation time is advanced by this interval, $t \\leftarrow t + \\tau$. If the new time $t+\\tau$ exceeds the specified terminal time $T$, the simulation is halted, and the state of the system is recorded as its value at time $T$.\n\n3.  **Sample Which Reaction Occurs:** The next reaction to occur is chosen based on its relative contribution to the total propensity. A random number $u$ is drawn from a uniform distribution on $(0, 1)$.\n    -   If $u < \\frac{a_1(x)}{a_0(x)} = \\frac{k}{k+\\gamma x}$, transcription occurs, and the state is updated: $x \\leftarrow x+1$.\n    -   Otherwise, degradation occurs, and the state is updated: $x \\leftarrow x-1$.\n\nThis process of calculating propensities, sampling time, and sampling the reaction type is iterated until the simulation time $t$ reaches or exceeds the terminal time $T$.\n\nTo obtain robust statistics, this entire simulation is repeated for $R$ independent trajectories. From the resulting sample of final states $\\{X_1(T), X_2(T), \\dots, X_R(T)\\}$, we compute:\n-   The empirical sample mean: $\\widehat{m}(T) = \\frac{1}{R} \\sum_{i=1}^{R} X_i(T)$.\n-   The empirical unbiased sample variance: $\\widehat{v}(T) = \\frac{1}{R-1} \\sum_{i=1}^{R} (X_i(T) - \\widehat{m}(T))^2$.\n\n### Analytical Solutions\n\nThe linear nature of the propensities allows for the derivation of exact analytical expressions for the moments of the distribution of $X(t)$.\n\n**Mean-Field ODE Solution:**\nThe time evolution of the expected value of $X(t)$, denoted $\\mathbb{E}[X(t)]$, can be approximated by a deterministic ordinary differential equation (ODE), often called the mean-field or rate equation. This is derived by considering the average rate of change:\n$$\n\\frac{d}{dt} \\mathbb{E}[X(t)] \\approx k - \\gamma \\mathbb{E}[X(t)].\n$$\nSolving this linear ODE for $x_{\\text{det}}(t) = \\mathbb{E}[X(t)]$ with the initial condition $x_{\\text{det}}(0) = x_0$ yields distinct solutions based on $\\gamma$:\n-   If $\\gamma > 0$:\n    $$\n    x_{\\text{det}}(T) = x_0 e^{-\\gamma T} + \\frac{k}{\\gamma}\\bigl(1 - e^{-\\gamma T}\\bigr).\n    $$\n-   If $\\gamma = 0$:\n    $$\n    x_{\\text{det}}(T) = x_0 + k T.\n    $$\n\n**Exact Analytical Variance:**\nThe exact variance of the process, $v_{\\text{theory}}(T) = \\text{Var}(X(T))$, can also be derived from the chemical master equation. The solution is:\n-   If $\\gamma > 0$:\n    $$\n    v_{\\text{theory}}(T) = x_0 e^{-\\gamma T} \\bigl(1 - e^{-\\gamma T}\\bigr) + \\frac{k}{\\gamma}\\bigl(1 - e^{-\\gamma T}\\bigr).\n    $$\n    It is noteworthy that for this specific process, the variance can be related to the mean by $v_{\\text{theory}}(T) = x_{\\text{det}}(T) - x_0 e^{-2\\gamma T}$.\n-   If $\\gamma = 0$ (a simple Poisson process for births):\n    $$\n    v_{\\text{theory}}(T) = k T.\n    $$\n\n### Calculation of Metrics\n\nFor each parameter set, the following metrics are computed to quantify the relationship between the stochastic simulation results and the theoretical predictions:\n1.  **Mean Absolute Error:** $E_m = \\left| \\widehat{m}(T) - x_{\\text{det}}(T) \\right|$. This measures the deviation of the empirically estimated mean from the analytically predicted mean.\n2.  **Variance Absolute Error:** $E_v = \\left| \\widehat{v}(T) - v_{\\text{theory}}(T) \\right|$. This measures the deviation of the empirically estimated variance from the exact analytical variance.\n3.  **Variance Discrepancy:** $D_v = \\widehat{v}(T)$. This is the estimated variance itself, serving as a measure of stochastic fluctuation, which is entirely absent in the deterministic mean-field model (where variance is implicitly zero).\n\nThe implementation will proceed by first defining a function to run a single SSA trajectory, then a function to orchestrate $R$ such runs for a given parameter set, calculate the empirical statistics, compute the analytical solutions, and finally derive the three required metrics. This procedure is repeated for all four test cases using a fixed random number generator seed for reproducibility.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print results.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (k, gamma, x0, T, R)\n        (5.0, 1.0, 10, 2.0, 3000),  # Case 1\n        (0.0, 0.7, 25, 3.5, 3000),  # Case 2\n        (12.0, 4.0, 0, 1.5, 3000),  # Case 3\n        (5.0, 0.0, 3, 1.2, 3000),   # Case 4\n    ]\n\n    # Initialize a single random number generator with the specified seed for reproducibility.\n    rng = np.random.default_rng(1729)\n    \n    all_results = []\n    for case in test_cases:\n        k, gamma, x0, T, R = case\n        \n        # --- Stochastic Simulation (SSA) ---\n        final_states = np.empty(R, dtype=int)\n        for i in range(R):\n            final_states[i] = _run_single_ssa_trajectory(k, gamma, x0, T, rng)\n        \n        # Compute empirical statistics\n        m_hat_T = np.mean(final_states)\n        v_hat_T = np.var(final_states, ddof=1) if R > 1 else 0.0\n\n        # --- Analytical Solutions ---\n        # Deterministic mean-field solution\n        if gamma > 0:\n            x_det_T = x0 * np.exp(-gamma * T) + (k / gamma) * (1.0 - np.exp(-gamma * T))\n        else: # gamma == 0\n            x_det_T = x0 + k * T\n            \n        # Analytical variance\n        if gamma > 0:\n            v_theory_T = x0 * np.exp(-gamma * T) * (1.0 - np.exp(-gamma * T)) + (k / gamma) * (1.0 - np.exp(-gamma * T))\n        else: # gamma == 0\n            v_theory_T = k * T\n            \n        # --- Compute Metrics ---\n        E_m = np.abs(m_hat_T - x_det_T)\n        E_v = np.abs(v_hat_T - v_theory_T)\n        D_v = v_hat_T\n        \n        all_results.extend([E_m, E_v, D_v])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef _run_single_ssa_trajectory(k, gamma, x0, T, rng):\n    \"\"\"\n    Implements the Gillespie direct method for a single trajectory of the mRNA birth-death model.\n    \n    Args:\n        k (float): Transcription rate.\n        gamma (float): Degradation rate.\n        x0 (int): Initial number of molecules.\n        T (float): Terminal time.\n        rng (numpy.random.Generator): The random number generator instance.\n        \n    Returns:\n        int: The number of molecules at time T.\n    \"\"\"\n    t = 0.0\n    x = x0\n    \n    while t < T:\n        # Calculate total propensity a0\n        a0 = k + gamma * x\n        \n        # If a0 is zero, no more reactions can occur (absorbing state)\n        if a0 <= 0:\n            break\n        \n        # Sample time to next reaction, tau\n        # tau ~ Exponential(rate=a0) which is equivalent to Exponential(scale=1/a0)\n        tau = rng.exponential(scale=1.0 / a0)\n        \n        # If the next reaction occurs after T, the simulation ends.\n        if t + tau > T:\n            break\n        \n        # Advance time\n        t += tau\n        \n        # Choose which reaction occurs\n        # Draw a uniform random number u\n        u = rng.random()\n        \n        # Probability of transcription is a1/a0 = k/a0\n        if u < k / a0:\n            # Transcription event\n            x += 1\n        else:\n            # Degradation event\n            # Ensure x does not go below 0 (though this is guaranteed by a2=gamma*x)\n            if x > 0:\n                x -= 1\n    \n    # The state at the end of the simulation is the current value of x\n    return x\n\n# Execute the main function\nsolve()\n```", "id": "4357865"}, {"introduction": "A central goal in systems biomedicine is to connect theoretical models with experimental data. This exercise puts you in the role of a data analyst tasked with modeling single-cell gene expression counts, a classic example of stochastic biological data. You will fit both Poisson and Negative Binomial distributions to datasets and use information criteria like AIC and BIC to perform model selection, learning how to balance model accuracy with complexity in a principled way [@problem_id:4357822].", "problem": "Consider a single gene measured across independent single-cell assays, producing nonnegative integer counts $\\{x_1, x_2, \\dots, x_n\\}$, where each $x_i \\in \\{0,1,2,\\dots\\}$. You will compare two stochastic models for these counts: a Poisson model with parameter $\\lambda$ and a Negative Binomial model parameterized by a mean $\\mu$ and a positive dispersion (shape) parameter $r$. Assume the counts are independent and identically distributed. Your task is to implement a program that, for each provided test case, computes the maximum likelihood estimates (Maximum Likelihood Estimation (MLE)) for the model parameters under each model, computes the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) for each fitted model, and outputs a model preference under each criterion.\n\nThe fundamental base for this problem is as follows. Begin from the definition of the likelihood for independent observations under each model, and the principle of Maximum Likelihood Estimation (MLE), which selects parameters that maximize the likelihood (equivalently, the log-likelihood). Use the well-established definitions of the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC), both of which penalize model complexity by the number of estimated free parameters. Decisions are to be made by comparing values of these criteria between models for the same data, preferring the model with the smaller value; in case of ties within a small numerical tolerance, prefer the simpler model with fewer parameters.\n\nFor the Negative Binomial model, use the parameterization with mean $\\mu$ and dispersion $r>0$ whose probability mass function for an observation $x \\in \\{0,1,2,\\dots\\}$ is\n$$\n\\mathbb{P}(X=x \\mid r,\\mu) \\;=\\; \\binom{x+r-1}{x} \\left(\\frac{r}{r+\\mu}\\right)^r \\left(\\frac{\\mu}{r+\\mu}\\right)^x,\n$$\nwhich has mean $\\mu$ and variance $\\mu \\left(1 + \\frac{\\mu}{r}\\right)$.\n\nFor each dataset, treat the Poisson model as having $k_{\\text{Pois}} = 1$ free parameter and the Negative Binomial model as having $k_{\\text{NB}} = 2$ free parameters. When comparing models based on AIC or BIC, prefer the model with the smaller criterion value; in the event the two values are equal within a tolerance of $\\varepsilon = 10^{-8}$, choose the Poisson model as the more parsimonious one.\n\nThere are no physical units involved. All outputs should be pure integers. No percentages or angle units are required. Your program must implement numerical optimization where closed-form MLEs are not available and must ensure numerical stability in evaluating log-likelihoods.\n\nUse the following test suite of five distinct datasets, each representing counts from a single gene across different numbers of cells:\n- Test case $1$: $\\{0,1,1,2,1,0,1,2,1,1,2,1,3,0\\}$.\n- Test case $2$: $\\{0,0,5,10,0,3,20,0,7,15,0,1,12\\}$.\n- Test case $3$: $\\{0,0,0,0,0,0,0,0,0\\}$.\n- Test case $4$: $\\{4,5,6,5,4,5,6,5,5,4,6\\}$.\n- Test case $5$: $\\{0,50,0,0,100,2,3,70,90,0,150,1,0\\}$.\n\nYour program should produce a single line of output containing the results aggregated across all test cases as a comma-separated list enclosed in square brackets. For each test case, output a two-element list $[a,b]$ where $a$ is the preferred model under AIC and $b$ is the preferred model under BIC, encoded as integers, with $0$ representing the Poisson model and $1$ representing the Negative Binomial model. Thus, the final output line should be of the form $[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4],[a_5,b_5]]$, where each $a_i$ and $b_i$ is an integer in $\\{0,1\\}$.", "solution": "The problem statement requires a comparative analysis of two discrete probability distributions, the Poisson and the Negative Binomial, as models for a given set of single-cell gene expression counts. The comparison is to be performed using the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC), which balance model fit with model complexity. The decision for each criterion rests on selecting the model with the lower criterion value.\n\nLet the observed data be a set of $n$ independent and identically distributed (i.i.d.) non-negative integer counts $\\mathbf{x} = \\{x_1, x_2, \\dots, x_n\\}$, where each $x_i \\in \\{0, 1, 2, \\dots\\}$. The core of the task is to find the maximum likelihood estimates (MLEs) for the parameters of each model and then compute the corresponding AIC and BIC values.\n\n### Model 1: The Poisson Distribution\nThe Poisson distribution is characterized by a single rate parameter $\\lambda > 0$, and its probability mass function (PMF) for an observation $x$ is:\n$$ \\mathbb{P}(X=x \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^x}{x!} $$\nGiven the i.i.d. assumption, the likelihood function for the dataset $\\mathbf{x}$ is the product of individual probabilities:\n$$ L(\\lambda; \\mathbf{x}) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{x_i}}{x_i!} $$\nFor analytical and numerical convenience, we work with the log-likelihood function, $\\ell(\\lambda; \\mathbf{x}) = \\log L(\\lambda; \\mathbf{x})$:\n$$ \\ell(\\lambda; \\mathbf{x}) = \\sum_{i=1}^n \\left( x_i \\log \\lambda - \\lambda - \\log(x_i!) \\right) = (\\log \\lambda) \\sum_{i=1}^n x_i - n\\lambda - \\sum_{i=1}^n \\log(x_i!) $$\nTo find the MLE for $\\lambda$, we set the derivative of $\\ell$ with respect to $\\lambda$ to zero:\n$$ \\frac{d\\ell}{d\\lambda} = \\frac{1}{\\lambda} \\sum_{i=1}^n x_i - n = 0 $$\nThis yields the well-known closed-form MLE for $\\lambda$, which is the sample mean:\n$$ \\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n x_i = \\bar{x} $$\nThe maximized log-likelihood, $\\ell_{\\text{Pois,max}}$, is obtained by substituting $\\hat{\\lambda}_{\\text{MLE}}$ back into the log-likelihood function. A special case arises if all $x_i=0$, where $\\bar{x}=0$. In this scenario, the likelihood $L(\\lambda; \\mathbf{x}) = e^{-n\\lambda}$ is maximized as $\\lambda \\to 0^+$, so $\\hat{\\lambda}_{\\text{MLE}} = 0$, and the maximized log-likelihood is $\\ell_{\\text{Pois,max}} = 0$.\n\n### Model 2: The Negative Binomial Distribution\nThe Negative Binomial (NB) distribution is specified by a mean $\\mu \\ge 0$ and a positive dispersion parameter $r > 0$. The PMF is given as:\n$$ \\mathbb{P}(X=x \\mid r, \\mu) = \\binom{x+r-1}{x} \\left(\\frac{r}{r+\\mu}\\right)^r \\left(\\frac{\\mu}{r+\\mu}\\right)^x $$\nThe variance of this distribution is $\\mu + \\mu^2/r$. The term $\\mu^2/r$ models the overdispersion, which is the excess variance compared to a Poisson distribution with the same mean.\n\nThe log-likelihood for the dataset $\\mathbf{x}$ is $\\ell(r, \\mu; \\mathbf{x}) = \\sum_{i=1}^n \\log \\mathbb{P}(X=x_i \\mid r, \\mu)$. Using the identity $\\binom{n}{k} = \\frac{\\Gamma(n+1)}{\\Gamma(k+1)\\Gamma(n-k+1)}$, the log-likelihood for a single observation $x_i$ can be written using the log-gamma function, $\\log \\Gamma(\\cdot)$:\n$$ \\ell_i(r, \\mu) = \\log\\Gamma(x_i+r) - \\log\\Gamma(r) - \\log\\Gamma(x_i+1) + r\\log r + x_i\\log\\mu - (x_i+r)\\log(r+\\mu) $$\nDifferentiating the total log-likelihood $\\ell = \\sum_i \\ell_i$ with respect to $\\mu$ and setting the result to zero reveals that the MLE for the mean parameter is also the sample mean:\n$$ \\hat{\\mu}_{\\text{MLE}} = \\bar{x} $$\nThis simplifies the problem, as we only need to find the MLE for $r$ by maximizing the profile log-likelihood $\\ell(r; \\mathbf{x}, \\hat{\\mu}_{\\text{MLE}})$. There is no closed-form solution for $\\hat{r}_{\\text{MLE}}$, so it must be found using numerical optimization. We seek to maximize the function:\n$$ \\ell(r; \\mathbf{x}) = \\sum_{i=1}^n \\left[ \\log\\Gamma(x_i+r) - \\log\\Gamma(r) - \\log\\Gamma(x_i+1) + r\\log r + x_i\\log\\bar{x} - (x_i+r)\\log(r+\\bar{x}) \\right] $$\nAn important special case occurs when the sample variance, $\\hat{\\sigma}^2 = \\frac{1}{n}\\sum(x_i-\\bar{x})^2$, is less than or equal to the sample mean $\\bar{x}$. This condition, known as under-dispersion, implies that the additional complexity of the NB model is not supported by the data. In this situation, the log-likelihood is a monotonically increasing function of $r$, and the MLE for $r$ tends to infinity ($r \\to \\infty$). As $r \\to \\infty$, the NB distribution converges to a Poisson distribution with mean $\\mu$. Thus, if $\\hat{\\sigma}^2 \\le \\bar{x}$, the Poisson model is inherently preferred.\n\n### Model Selection Criteria\nThe AIC and BIC are defined as:\n$$ \\text{AIC} = 2k - 2\\ell_{\\text{max}} $$\n$$ \\text{BIC} = k \\log(n) - 2\\ell_{\\text{max}} $$\nwhere $k$ is the number of estimated parameters, $n$ is the sample size, and $\\ell_{\\text{max}}$ is the maximized log-likelihood. For this problem, the number of parameters are $k_{\\text{Pois}} = 1$ (for $\\lambda$) and $k_{\\text{NB}} = 2$ (for $\\mu$ and $r$).\n\n### Decision Procedure\nFor each dataset, we follow these steps:\n1.  Calculate the sample size $n$ and sample mean $\\bar{x}$.\n2.  If $\\bar{x}=0$, the data consists entirely of zeros. The Poisson model with $\\hat{\\lambda}=0$ provides a perfect fit ($\\ell_{\\text{Pois,max}}=0$). The NB model with $\\hat{\\mu}=0$ also yields $\\ell_{\\text{NB,max}}=0$, but is more complex ($k=2$ vs $k=1$). Therefore, the Poisson model is preferred for both AIC and BIC.\n3.  Calculate the sample variance $\\hat{\\sigma}^2$. If $\\hat{\\sigma}^2 \\le \\bar{x}$, the data is not overdispersed. The NB model's MLE converges to the Poisson model, so the simpler Poisson model is preferred due to the penalty terms in AIC and BIC.\n4.  If the data is overdispersed ($\\bar{x} > 0$ and $\\hat{\\sigma}^2 > \\bar{x}$), we proceed with the full comparison:\n    a.  Calculate $\\ell_{\\text{Pois,max}}$ using $\\hat{\\lambda} = \\bar{x}$.\n    b.  Numerically optimize the NB log-likelihood with respect to $r > 0$, holding $\\mu = \\bar{x}$, to find $\\hat{r}$ and the corresponding $\\ell_{\\text{NB,max}}$. This is achieved by minimizing the negative log-likelihood function.\n    c.  Compute $\\text{AIC}_{\\text{Pois}}$, $\\text{BIC}_{\\text{Pois}}$, $\\text{AIC}_{\\text{NB}}$, and $\\text{BIC}_{\\text{NB}}$.\n    d.  For each criterion, select the model with the lower value. If the values are within a tolerance of $\\varepsilon = 10^{-8}$, select the more parsimonious Poisson model. The preference is encoded as $0$ for Poisson and $1$ for Negative Binomial.\nThis procedure provides a rigorous framework for model selection, grounded in the principles of maximum likelihood and information theory.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\n\ndef _calculate_model_preference(counts: list[int]) -> list[int]:\n    \"\"\"\n    Calculates model preference for a single dataset.\n    \n    Compares Poisson and Negative Binomial models using AIC and BIC.\n\n    Args:\n        counts: A list of non-negative integer counts.\n\n    Returns:\n        A list [aic_preference, bic_preference] where 0 indicates a preference\n        for the Poisson model and 1 for the Negative Binomial model.\n    \"\"\"\n    x = np.array(counts)\n    n = len(x)\n    mu_mle = np.mean(x)\n    tol = 1e-8\n\n    # Case 1: Mean is zero. Data is all zeros.\n    # Poisson with lambda=0 is a perfect fit (LL=0).\n    # NB(mu=0, r) also has LL=0 but is more complex (k=2 vs k=1).\n    # The more parsimonious Poisson model is always preferred.\n    if mu_mle == 0:\n        return [0, 0]\n        \n    # Case 2: Data is under-dispersed or equi-dispersed.\n    # The MLE for NB dispersion 'r' tends to infinity, meaning the NB model\n    # converges to the Poisson model. The additional parameter is not justified.\n    var_mle = np.var(x, ddof=0)\n    if var_mle <= mu_mle:\n        return [0, 0]\n\n    # ----- Poisson Model Calculation -----\n    # The log-likelihood is sum(x_i*log(lambda) - lambda - log(x_i!)).\n    # We use gammaln(x+1) for log(x!).\n    log_lambda = np.log(mu_mle)\n    ll_pois = np.sum(x * log_lambda - mu_mle - gammaln(x + 1))\n\n    k_pois = 1\n    aic_pois = 2 * k_pois - 2 * ll_pois\n    bic_pois = k_pois * np.log(n) - 2 * ll_pois\n\n    # ----- Negative Binomial Model Calculation -----\n    \n    def neg_log_likelihood_nb_r(r: float, data: np.ndarray, mu: float) -> float:\n        \"\"\"Negative log-likelihood of NB model, to be minimized for r.\"\"\"\n        if r <= 0:\n            return np.inf\n        # The log-likelihood is sum(log(P(X=x_i | r, mu)))\n        log_likelihood = np.sum(\n            gammaln(data + r) - gammaln(r) - gammaln(data + 1) +\n            r * np.log(r) + data * np.log(mu) - (data + r) * np.log(r + mu)\n        )\n        return -log_likelihood\n\n    # Numerically optimize for r, given mu_mle.\n    # The 'bounded' method is used to ensure r > 0.\n    res = minimize_scalar(\n        neg_log_likelihood_nb_r,\n        args=(x, mu_mle),\n        bounds=(1e-8, 1e8),  # Search for r in a reasonable positive range.\n        method='bounded'\n    )\n    \n    # Maximized log-likelihood for NB is the negative of the minimized function value.\n    ll_nb = -res.fun\n    \n    k_nb = 2\n    aic_nb = 2 * k_nb - 2 * ll_nb\n    bic_nb = k_nb * np.log(n) - 2 * ll_nb\n\n    # ----- Model Comparison -----\n    # Prefer the model with the lower criterion value.\n    # If values are within tolerance, prefer the simpler Poisson model (output 0).\n    aic_pref = 1 if aic_nb < aic_pois - tol else 0\n    bic_pref = 1 if bic_nb < bic_pois - tol else 0\n    \n    return [aic_pref, bic_pref]\n\ndef solve():\n    \"\"\"\n    Main function to run the model comparison on all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        [0, 1, 1, 2, 1, 0, 1, 2, 1, 1, 2, 1, 3, 0],\n        [0, 0, 5, 10, 0, 3, 20, 0, 7, 15, 0, 1, 12],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [4, 5, 6, 5, 4, 5, 6, 5, 5, 4, 6],\n        [0, 50, 0, 0, 100, 2, 3, 70, 90, 0, 150, 1, 0]\n    ]\n\n    results = []\n    for case_data in test_cases:\n        a, b = _calculate_model_preference(case_data)\n        results.append(f\"[{a},{b}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4357822"}]}