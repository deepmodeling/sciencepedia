{"hands_on_practices": [{"introduction": "This practice moves beyond simply finding the \"best-fit\" parameters to the crucial step of model validation. By analyzing the pattern of residuals—the differences between your model's predictions and the actual data—you can uncover systematic flaws in your model's assumptions, a fundamental skill in rigorous scientific modeling. This exercise will guide you through calculating a metric to assess whether your model systematically over- or under-predicts over time, a key indicator of a potential model mismatch. [@problem_id:1447309]", "problem": "A systems biologist is analyzing the growth dynamics of a bacterial population. The population density, measured as Optical Density (OD) in arbitrary units, was recorded at several time points. The biologist proposes that the growth can be described by the logistic model:\n$$N(t) = \\frac{K}{1 + \\left(\\frac{K}{N_0} - 1\\right) \\exp(-r t)}$$\nwhere $N(t)$ is the population density at time $t$, $N_0$ is the initial population density, $K$ is the carrying capacity, and $r$ is the intrinsic growth rate.\n\nAfter performing a non-linear regression analysis on the experimental data, the best-fit parameters were determined to be a carrying capacity $K = 1.0$, an initial density $N_0 = 0.1$, and an intrinsic growth rate $r = 0.5 \\text{ hours}^{-1}$.\n\nThe time and corresponding OD measurements are as follows:\n- at $t_1 = 0$ hours, OD = 0.11\n- at $t_2 = 2$ hours, OD = 0.22\n- at $t_3 = 4$ hours, OD = 0.44\n- at $t_4 = 6$ hours, OD = 0.70\n- at $t_5 = 8$ hours, OD = 0.85\n\nTo assess the quality of the fit and check for systematic time-dependent deviations, the biologist decides to compute a trend metric based on the residuals. The residual for the $i$-th data point is defined as $R_i = \\text{OD}_i - N(t_i)$, where $\\text{OD}_i$ is the measured value and $N(t_i)$ is the value predicted by the model at time $t_i$.\n\nCalculate the value of the trend metric $T = \\sum_{i=1}^{n-1} R_i R_{i+1}$, where $n=5$ is the total number of data points. This metric represents the sum of the products of consecutive residuals. Report your final answer as a number rounded to three significant figures.", "solution": "The logistic model is given by\n$$N(t) = \\frac{K}{1 + \\left(\\frac{K}{N_{0}} - 1\\right)\\exp(-r t)}.$$\nWith the best-fit parameters $K=1$, $N_{0}=0.1$, and $r=0.5$, this simplifies to\n$$N(t) = \\frac{1}{1 + 9\\,\\exp(-0.5\\,t)}.$$\nEvaluate $N(t_{i})$ at the given times:\n- At $t_{1}=0$:\n$$N(0) = \\frac{1}{1 + 9\\,\\exp(0)} = \\frac{1}{10} = 0.1.$$\n- At $t_{2}=2$:\n$$N(2) = \\frac{1}{1 + 9\\,\\exp(-1)} \\approx \\frac{1}{1 + 9 \\times 0.367879441171} \\approx \\frac{1}{4.310914970543} \\approx 0.231969315.$$\n- At $t_{3}=4$:\n$$N(4) = \\frac{1}{1 + 9\\,\\exp(-2)} \\approx \\frac{1}{1 + 9 \\times 0.135335283237} \\approx \\frac{1}{2.21801754913} \\approx 0.450852701.$$\n- At $t_{4}=6$:\n$$N(6) = \\frac{1}{1 + 9\\,\\exp(-3)} \\approx \\frac{1}{1 + 9 \\times 0.049787068368} \\approx \\frac{1}{1.44808361531} \\approx 0.690567858.$$\n- At $t_{5}=8$:\n$$N(8) = \\frac{1}{1 + 9\\,\\exp(-4)} \\approx \\frac{1}{1 + 9 \\times 0.018315638889} \\approx \\frac{1}{1.16484075000} \\approx 0.858486450.$$\nCompute residuals $R_{i} = \\text{OD}_{i} - N(t_{i})$:\n$$R_{1} = 0.11 - 0.1 = 0.01,$$\n$$R_{2} = 0.22 - 0.231969315 = -0.011969315,$$\n$$R_{3} = 0.44 - 0.450852701 = -0.010852701,$$\n$$R_{4} = 0.70 - 0.690567858 = 0.009432142,$$\n$$R_{5} = 0.85 - 0.858486450 = -0.008486450.$$\nCompute the trend metric\n$$T = \\sum_{i=1}^{4} R_{i}R_{i+1} = R_{1}R_{2} + R_{2}R_{3} + R_{3}R_{4} + R_{4}R_{5}.$$\nTerm by term:\n$$R_{1}R_{2} = 0.01 \\times (-0.011969315) = -0.00011969315,$$\n$$R_{2}R_{3} = (-0.011969315)\\times(-0.010852701) \\approx 0.000129899401,$$\n$$R_{3}R_{4} = (-0.010852701)\\times(0.009432142) \\approx -0.000102364220,$$\n$$R_{4}R_{5} = (0.009432142)\\times(-0.008486450) \\approx -0.000080045401.$$\nSumming these contributions gives\n$$T \\approx -0.00011969315 + 0.000129899401 - 0.000102364220 - 0.000080045401 \\approx -0.000172203371.$$\nRounded to three significant figures, this is\n$$T \\approx -1.72 \\times 10^{-4}.$$", "answer": "$$\\boxed{-1.72 \\times 10^{-4}}$$", "id": "1447309"}, {"introduction": "A point estimate for a parameter is only part of the story; understanding its uncertainty is equally critical for making robust scientific claims. This exercise introduces bootstrapping, a powerful and intuitive computational method for estimating confidence intervals by resampling your original data. You will practice interpreting the output of a bootstrap analysis to construct a 95% confidence interval for an mRNA degradation rate, allowing you to quantify the reliability of your parameter estimate. [@problem_id:1447275]", "problem": "A systems biologist is investigating the stability of a specific messenger Ribonucleic Acid (mRNA) molecule in eukaryotic cells. The prevailing model for the degradation of this mRNA after the cessation of transcription is first-order decay, described by the equation $M(t) = M_0 \\exp(-\\gamma t)$. In this model, $M(t)$ is the concentration of the mRNA at time $t$, $M_0$ is the initial concentration at $t=0$, and $\\gamma$ is the constant degradation rate.\n\nTo estimate $\\gamma$, an experiment was conducted where transcription was halted at $t=0$, and the mRNA concentration was measured at several subsequent time points. The data collected are as follows:\n\n| Time, $t$ (minutes) | mRNA Concentration, $M$ (arbitrary units) |\n|---|---|\n| 0 | 102.1 |\n| 10 | 59.8 |\n| 20 | 35.5 |\n| 30 | 23.0 |\n| 40 | 14.1 |\n\nTo assess the statistical confidence in the estimate of $\\gamma$, a bootstrap analysis was performed. This involved generating $B=1000$ new datasets by randomly sampling the five original data pairs with replacement. For each of these bootstrap datasets, the degradation rate $\\gamma$ was re-estimated by performing a linear least-squares fit on the transformed model, $\\ln(M(t)) = \\ln(M_0) - \\gamma t$.\n\nThis procedure yielded 1000 bootstrap estimates for the degradation rate, $\\{\\gamma^*_1, \\gamma^*_2, \\dots, \\gamma^*_{1000}\\}$. These estimates were then sorted in ascending order. The following are two excerpts from this sorted list:\n\n- Values at ranks 23 through 28:\n`0.0488, 0.0491, 0.0493, 0.0495, 0.0496, 0.0499`\n\n- Values at ranks 973 through 978:\n`0.0525, 0.0527, 0.0529, 0.0531, 0.0533, 0.0538`\n\nUsing the percentile method on this bootstrap distribution, determine the 95% confidence interval for the degradation rate $\\gamma$. Provide the lower and upper bounds of the interval as your answer. Express your final answer in units of min$^{-1}$. Round each bound to three significant figures.", "solution": "The degradation model is $M(t)=M_{0}\\exp(-\\gamma t)$, which after taking the natural logarithm becomes the linear relation\n$$\n\\ln(M(t))=\\ln(M_{0})-\\gamma t,\n$$\nso the slope of the least-squares fit to the $(t,\\ln M)$ data is $-\\gamma$. A bootstrap with $B=1000$ resamples yields estimates $\\{\\gamma^{*}_{1},\\dots,\\gamma^{*}_{1000}\\}$. For the percentile method at confidence level $1-\\alpha=0.95$, the confidence interval is given by the empirical quantiles at probabilities $p_{L}=\\alpha/2=0.025$ and $p_{U}=1-\\alpha/2=0.975$ of the sorted bootstrap distribution.\n\nLet the ordered statistics be $\\gamma^{*}_{(1)}\\leq\\dots\\leq\\gamma^{*}_{(B)}$. Using the common percentile rule that selects the order statistics at ranks\n$$\nr_{L}=p_{L}B=0.025\\times 1000=25,\\qquad r_{U}=p_{U}B=0.975\\times 1000=975,\n$$\nthe $95$ percent confidence interval is $[\\gamma^{*}_{(25)},\\gamma^{*}_{(975)}]$.\n\nFrom the provided excerpts of the sorted list:\n- Ranks $23$ through $28$ are $0.0488, 0.0491, 0.0493, 0.0495, 0.0496, 0.0499$, so $\\gamma^{*}_{(25)}=0.0493$.\n- Ranks $973$ through $978$ are $0.0525, 0.0527, 0.0529, 0.0531, 0.0533, 0.0538$, so $\\gamma^{*}_{(975)}=0.0529$.\n\nTherefore, the 95% confidence interval for $\\gamma$ is $[0.0493,\\,0.0529]$ in units of $\\text{min}^{-1}$. Rounding each bound to three significant figures leaves the same values.", "answer": "$$\\boxed{\\begin{pmatrix}0.0493 & 0.0529\\end{pmatrix}}$$", "id": "1447275"}, {"introduction": "In a Bayesian framework, parameter estimation yields an entire posterior distribution, not a single value. This advanced problem explores the critical task of selecting a representative point estimate, demonstrating how the optimal choice depends on the specific scientific question and the implicit or explicit costs of estimation error. You will analyze how the properties of a skewed posterior distribution, which can arise from weakly identifiable parameters, influence different estimators like the mean, median, and mode, and how this relates to underlying loss functions. [@problem_id:3925002]", "problem": "A single-gene synthetic circuit is engineered so that its repressor protein undergoes first-order degradation, meaning that if $p(t)$ denotes the number of labeled protein molecules at time $t$, then $dp/dt = -k\\,p$ for some unknown degradation rate $k > 0$. In a time-lapse fluorescence experiment, $m$ individual protein molecules are tracked. Due to phototoxicity constraints, imaging is terminated at censoring time $T_c > 0$ and no degradation events are observed before $T_c$; all $m$ molecules remain. Assume molecules degrade independently and that molecule lifetimes are exponentially distributed with rate $k$ implied by the first-order kinetics. Suppose a prior on $k$ is chosen as a Gamma distribution with shape $a_0 > 0$ and rate $b_0 > 0$.\n\nFrom first principles, use the governing law $dp/dt = -k\\,p$ and independence to write down the survival probability for one molecule and, consequently, the likelihood under right-censoring when no events are observed up to $T_c$. Combine this with the Gamma prior to determine the posterior up to normalization. Then analyze how weak identifiability in this experiment (no observed events) can lead to a skewed posterior for $k$, particularly when $a_0 < 1$.\n\nConsider the following statements about point estimators for $k$ and their dependence on the posterior and the loss function:\n\nA. In this right-censored scenario with $a_0 < 1$, the Maximum A Posteriori (MAP) estimate of $k$ is $0$, whereas the posterior mean is $a_0 / (b_0 + m T_c)$, and under squared-error loss the posterior mean is optimal.\n\nB. Under absolute-error loss, the Bayes estimator for $k$ is the posterior mean, which will always be less than or equal to the MAP in a right-skewed posterior.\n\nC. For a sequence of $0$–$1$ losses that penalize errors outside an $\\epsilon$-ball around the estimate, the Bayes estimator converges to the posterior mode as $\\epsilon \\to 0$.\n\nD. When overestimation incurs twice the penalty of underestimation under an asymmetric linear loss, the optimal Bayes estimator is the posterior $p$-quantile with $p = \\frac{1}{3}$, which lies below the median for a right-skewed posterior.\n\nE. In the given scenario with $a_0 < 1$, the posterior median equals the MAP and therefore also equals $0$.\n\nSelect all statements that are correct.", "solution": "The user wants me to validate the problem statement and, if valid, provide a detailed solution.\n\n### Step 1: Extract Givens\n-   **Governing Law:** The number of protein molecules $p(t)$ at time $t$ follows the first-order degradation kinetics $dp/dt = -k\\,p$.\n-   **Unknown Parameter:** The degradation rate constant is $k > 0$.\n-   **Stochastic Model:** The lifetime of a single molecule is assumed to be exponentially distributed with rate $k$. Molecules degrade independently.\n-   **Experiment:** $m$ individual protein molecules are tracked.\n-   **Data/Observation:** The experiment is terminated at a censoring time $T_c > 0$. No degradation events were observed for any of the $m$ molecules.\n-   **Prior Distribution:** The prior distribution for $k$ is a Gamma distribution with shape parameter $a_0 > 0$ and rate parameter $b_0 > 0$, denoted as $k \\sim \\text{Gamma}(a_0, b_0)$.\n-   **Analysis Condition:** The problem asks to analyze the case where the prior shape parameter $a_0 < 1$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement describes a Bayesian inference task for a parameter in a stochastic model.\n\n-   **Scientific Grounding:** The model is scientifically sound. First-order kinetics is a standard approximation for unimolecular decay processes like protein degradation. The mathematical equivalence between a macroscopic first-order rate law and an underlying exponential distribution for individual molecule lifetimes is a cornerstone of stochastic chemical kinetics. The experimental setup involving time-lapse microscopy and right-censoring (due to stopping the experiment at time $T_c$) is a realistic scenario in single-molecule biophysics.\n-   **Well-Posedness:** The problem is well-posed. It provides a clear model, a description of the data, and a prior distribution. From these components, a posterior distribution for the parameter $k$ can be uniquely determined. The subsequent questions about point estimators (MAP, mean, median) are standard topics in Bayesian decision theory.\n-   **Objectivity:** The problem is stated in precise, objective, and quantitative terms. It is free from ambiguity or subjective claims.\n\nThe problem does not exhibit any flaws. It is not scientifically unsound, incomplete, contradictory, or ill-posed. The scenario described is a non-trivial but standard problem in statistical inference, particularly relevant to biological modeling.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. I will proceed with the solution.\n\n### Derivation of the Posterior Distribution\n\nFirst, we establish the likelihood function from the experimental data. The governing law $dp/dt = -k\\,p$ for a population of molecules implies that the probability of a single molecule surviving past time $t$ (i.e., its lifetime $T$ being greater than $t$) is given by the survival function $S(t) = P(T > t) = e^{-kt}$. This corresponds to an exponential distribution for the lifetime $T$ with rate parameter $k$, having a probability density function $f(t; k) = k e^{-kt}$ for $t \\ge 0$.\n\nThe experiment consists of observing $m$ molecules, none of which degrade before the censoring time $T_c$. This means for each molecule $i=1, \\dots, m$, we observe that its lifetime $T_i > T_c$. Since the molecules are assumed to degrade independently, the total likelihood of observing this data, given the parameter $k$, is the product of the individual probabilities:\n$$L(k | \\text{data}) = \\prod_{i=1}^m P(T_i > T_c) = \\prod_{i=1}^m e^{-k T_c} = (e^{-k T_c})^m = e^{-m T_c k}$$\n\nThe prior distribution for $k$ is given as a Gamma distribution with shape $a_0$ and rate $b_0$:\n$$p(k) = \\text{Gamma}(k; a_0, b_0) = \\frac{b_0^{a_0}}{\\Gamma(a_0)} k^{a_0-1} e^{-b_0 k}$$\nwhere $\\Gamma(\\cdot)$ is the Gamma function.\n\nAccording to Bayes' theorem, the posterior distribution of $k$ is proportional to the product of the likelihood and the prior:\n$$p(k | \\text{data}) \\propto L(k | \\text{data}) \\cdot p(k)$$\n$$p(k | \\text{data}) \\propto (e^{-m T_c k}) \\cdot (k^{a_0-1} e^{-b_0 k})$$\n$$p(k | \\text{data}) \\propto k^{a_0-1} e^{-(b_0 + m T_c) k}$$\n\nThis is the kernel of a Gamma distribution. We can identify the parameters of the posterior distribution by comparing it to the standard Gamma form $p(k) \\propto k^{\\text{shape}-1}e^{-\\text{rate} \\cdot k}$.\nThe posterior distribution for $k$ is therefore a Gamma distribution, $k | \\text{data} \\sim \\text{Gamma}(a_1, b_1)$, with updated parameters:\n-   Posterior shape: $a_1 = a_0$\n-   Posterior rate: $b_1 = b_0 + m T_c$\n\nThe problem specifies the analysis for the case $a_0 < 1$. In this case, the posterior distribution is $k | \\text{data} \\sim \\text{Gamma}(a_0, b_0 + m T_c)$ where $a_0 < 1$.\n\n### Analysis of Point Estimators and Posterior Properties\n\nWe now analyze the properties of this posterior distribution, $\\text{Gamma}(a_0, b_1)$, under the condition $a_0 < 1$.\n-   **Posterior Mode (MAP estimate):** The mode of a Gamma distribution with shape $a$ and rate $b$ is $(a-1)/b$ for $a > 1$. For $a \\le 1$, the density function is monotonically decreasing (or flat for $a=1$) on $k>0$, and its maximum value is approached as $k \\to 0^+$. Therefore, for $a_1 = a_0 < 1$, the mode is at $k=0$. The Maximum A Posteriori (MAP) estimate is $\\hat{k}_{\\text{MAP}} = 0$.\n-   **Posterior Mean:** The mean of a $\\text{Gamma}(a, b)$ distribution is $a/b$. The posterior mean is $E[k | \\text{data}] = a_1/b_1 = \\frac{a_0}{b_0 + m T_c}$. Since $a_0 > 0$, $b_0 > 0$, $m \\ge 1$, and $T_c > 0$, the posterior mean is strictly positive.\n-   **Posterior Median:** The posterior median, $\\hat{k}_{\\text{med}}$, is the value that satisfies $\\int_0^{\\hat{k}_{\\text{med}}} p(k|\\text{data}) dk = 0.5$. Since the posterior density is non-zero for $k>0$, the median must be strictly positive, $\\hat{k}_{\\text{med}} > 0$.\n-   **Skewness:** A Gamma distribution with shape parameter $a_1=a_0 < 1$ is strongly right-skewed. For right-skewed distributions, the general ordering of central tendency measures is mean > median > mode. In our case, this corresponds to $\\frac{a_0}{b_0 + m T_c} > \\hat{k}_{\\text{med}} > 0$.\n\nNow we evaluate each statement.\n\n### Option-by-Option Analysis\n\n**A. In this right-censored scenario with $a_0 < 1$, the Maximum A Posteriori (MAP) estimate of $k$ is $0$, whereas the posterior mean is $a_0 / (b_0 + m T_c)$, and under squared-error loss the posterior mean is optimal.**\n\n-   As derived above, for a posterior distribution $\\text{Gamma}(a_0, b_1)$ with $a_0 < 1$, the mode is at $k=0$. Thus, the MAP estimate is $\\hat{k}_{\\text{MAP}} = 0$. This part is correct.\n-   The posterior mean is indeed $a_1/b_1 = a_0 / (b_0 + m T_c)$. This part is correct.\n-   It is a fundamental principle of Bayesian decision theory that the Bayes estimator that minimizes the posterior expected squared-error loss, $L(\\hat{k}, k) = (\\hat{k}-k)^2$, is the posterior mean, $E[k | \\text{data}]$. This part is correct.\n-   Since all parts of the statement are correct, the entire statement is correct.\n\n**Verdict: Correct**\n\n**B. Under absolute-error loss, the Bayes estimator for $k$ is the posterior mean, which will always be less than or equal to the MAP in a right-skewed posterior.**\n\n-   The Bayes estimator that minimizes the posterior expected absolute-error loss, $L(\\hat{k}, k) = |\\hat{k}-k|$, is the posterior median, not the posterior mean. The first part of the statement is incorrect.\n-   The second part claims the estimator (which it incorrectly identifies as the mean) is less than or equal to the MAP in a right-skewed posterior. For a right-skewed distribution, the mean is generally greater than the mode (MAP). In our specific case, the posterior mean is $\\frac{a_0}{b_0 + m T_c} > 0$ and the MAP is $0$. Thus, the mean is strictly greater than the MAP. This part of the statement is also incorrect.\n\n**Verdict: Incorrect**\n\n**C. For a sequence of $0$–$1$ losses that penalize errors outside an $\\epsilon$-ball around the estimate, the Bayes estimator converges to the posterior mode as $\\epsilon \\to 0$.**\n\n-   Let the loss function be $L_{\\epsilon}(\\hat{k}, k) = 1$ if $|k - \\hat{k}| > \\epsilon$ and $L_{\\epsilon}(\\hat{k}, k) = 0$ if $|k - \\hat{k}| \\le \\epsilon$.\n-   The Bayes estimator $\\hat{k}$ is chosen to minimize the posterior expected loss: $E[L_{\\epsilon}(\\hat{k}, k) | \\text{data}] = \\int p(k|\\text{data}) L_{\\epsilon}(\\hat{k}, k) dk = P(|k-\\hat{k}| > \\epsilon)$.\n-   Minimizing $P(|k-\\hat{k}| > \\epsilon)$ is equivalent to maximizing $1 - P(|k-\\hat{k}| > \\epsilon) = P(|k-\\hat{k}| \\le \\epsilon) = P(k \\in [\\hat{k}-\\epsilon, \\hat{k}+\\epsilon])$.\n-   To maximize the probability mass in an interval of fixed width $2\\epsilon$, one must center the interval where the probability density function $p(k|\\text{data})$ is highest. As $\\epsilon \\to 0$, this point converges to the value of $k$ that maximizes the density, which is by definition the posterior mode (the MAP estimate).\n-   This is a general and correct statement from Bayesian decision theory.\n\n**Verdict: Correct**\n\n**D. When overestimation incurs twice the penalty of underestimation under an asymmetric linear loss, the optimal Bayes estimator is the posterior $p$-quantile with $p = \\frac{1}{3}$, which lies below the median for a right-skewed posterior.**\n\n-   Let the loss function be $L(\\hat{k}, k)$. Let $c_o$ be the cost constant for overestimation ($\\hat{k}>k$) and $c_u$ be the cost for underestimation ($k>\\hat{k}$). The problem states $c_o = 2c_u$.\n-   $L(\\hat{k}, k) = \\begin{cases} c_o(\\hat{k}-k) & \\text{if } \\hat{k} > k \\\\ c_u(k-\\hat{k}) & \\text{if } k > \\hat{k} \\end{cases}$.\n-   The optimal Bayes estimator $\\hat{k}$ minimizes the posterior expected loss $E[L(\\hat{k}, k)|\\text{data}]$. Taking the derivative with respect to $\\hat{k}$ and setting it to $0$ yields the condition $c_o \\int_0^{\\hat{k}} p(k|\\text{data})dk = c_u \\int_{\\hat{k}}^{\\infty} p(k|\\text{data})dk$.\n-   Let $F(\\hat{k})$ be the posterior cumulative distribution function (CDF) at $\\hat{k}$. The condition is $c_o F(\\hat{k}) = c_u (1-F(\\hat{k}))$.\n-   Solving for $F(\\hat{k})$: $(c_o+c_u)F(\\hat{k}) = c_u \\implies F(\\hat{k}) = \\frac{c_u}{c_o+c_u}$.\n-   Substituting $c_o=2c_u$: $F(\\hat{k}) = \\frac{c_u}{2c_u+c_u} = \\frac{c_u}{3c_u} = \\frac{1}{3}$.\n-   The estimator $\\hat{k}$ is thus the value for which the posterior CDF is $1/3$, which is by definition the $p=1/3$ quantile (or the $33.3...^{\\text{rd}}$ percentile). This part is correct.\n-   The median is the $p=1/2$ quantile. Since the quantile function is a non-decreasing function of $p$, the $1/3$-quantile is less than or equal to the $1/2$-quantile. For a continuous distribution like the Gamma posterior, the quantile function is strictly increasing, so the $1/3$-quantile lies strictly below the median. This is true for any continuous distribution, including right-skewed ones. This part is correct.\n\n**Verdict: Correct**\n\n**E. In the given scenario with $a_0 < 1$, the posterior median equals the MAP and therefore also equals $0$.**\n\n-   As established for A, with $a_0 < 1$, the MAP estimate is $\\hat{k}_{\\text{MAP}} = 0$.\n-   The posterior median $\\hat{k}_{\\text{med}}$ is the value such that $P(k \\le \\hat{k}_{\\text{med}} | \\text{data}) = 0.5$.\n-   The posterior distribution has support on $k \\in (0, \\infty)$. The probability of $k$ being exactly $0$ is $0$, and $P(k \\le 0 | \\text{data}) = 0$.\n-   For the median to be $0$, we would require $P(k \\le 0 | \\text{data}) = 0.5$, which is false. The posterior median must be a value strictly greater than $0$.\n-   Therefore, the posterior median does not equal the MAP.\n\n**Verdict: Incorrect**", "answer": "$$\\boxed{ACD}$$", "id": "3925002"}]}