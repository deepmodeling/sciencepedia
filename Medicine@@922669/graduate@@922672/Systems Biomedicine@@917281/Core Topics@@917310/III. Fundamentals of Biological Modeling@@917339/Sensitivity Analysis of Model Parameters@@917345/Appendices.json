{"hands_on_practices": [{"introduction": "To begin our exploration of sensitivity analysis, we start with a fundamental technique: the derivation and solution of sensitivity equations. This method transforms the problem of finding the sensitivity, $\\frac{\\partial x}{\\partial \\theta}$, into solving a new set of ordinary differential equations that govern its dynamics. This first exercise [@problem_id:4385596] provides a clear, step-by-step walkthrough of this process for a basic mono-exponential decay model, a cornerstone of pharmacokinetic and systems models.", "problem": "In a pharmacokinetic model of mono-exponential decay for a short-lived biomolecule, the concentration $x(t)$ follows the ordinary differential equation (ODE) $dx/dt=-k\\,x$ with patient-specific initial condition $x(0)=x_{0}(p)$. Here, $k>0$ is the first-order elimination rate constant, and $x_{0}(p)$ is the baseline concentration determined by a patient covariate $p$. For local parameter sensitivity analysis, treat $k$ and $x_{0}$ as independent parameters.\n\nUsing the definition of a local sensitivity $S_{x,\\theta}(t)=\\partial x(t)/\\partial \\theta$ for a parameter $\\theta$, and starting from the fundamental operations of differentiation and the chain rule applied to the state ODE, perform the following:\n\n1. Derive the sensitivity ODEs and initial conditions for $S_{x,k}(t)$ and $S_{x,x_{0}}(t)$ that follow from differentiating the state ODE with respect to $k$ and $x_{0}$, respectively.\n2. Solve these sensitivity ODEs to obtain closed-form expressions for $S_{x,k}(t)$ and $S_{x,x_{0}}(t)$ in terms of $t$, $k$, and $x_{0}$.\n\nProvide your final answer as exact analytic expressions. Do not approximate, and do not include units. Express both sensitivities together as a single row matrix $\\bigl[S_{x,k}(t)\\;\\;S_{x,x_{0}}(t)\\bigr]$.", "solution": "The problem asks for the derivation and solution of the sensitivity equations for a mono-exponential decay model. The model is described by the ordinary differential equation (ODE) for the concentration $x(t)$:\n$$\n\\frac{dx}{dt} = -k\\,x\n$$\nwith the initial condition:\n$$\nx(0) = x_{0}\n$$\nThe parameters for the sensitivity analysis are the elimination rate constant $k$ and the initial concentration $x_{0}$. The local sensitivity of the state $x(t)$ with respect to a generic parameter $\\theta$ is defined as $S_{x,\\theta}(t) = \\frac{\\partial x(t)}{\\partial \\theta}$.\n\nFirst, we solve the state ODE. This is a separable first-order linear ODE.\n$$\n\\frac{dx}{x} = -k\\,dt\n$$\nIntegrating both sides gives $\\ln(x) = -kt + C$, where $C$ is the integration constant. Exponentiating yields $x(t) = \\exp(-kt+C) = A \\exp(-kt)$, where $A = \\exp(C)$. Applying the initial condition $x(0) = x_{0}$, we find $A = x_{0}$.\nThus, the solution to the state equation is:\n$$\nx(t) = x_{0} \\exp(-kt)\n$$\nThis expression for $x(t)$ will be used in the derivation of the sensitivity equations.\n\n**1. Derivation and Solution for Sensitivity with respect to $k$**\n\nLet $S_{x,k}(t) = \\frac{\\partial x(t)}{\\partial k}$. To find the ODE governing $S_{x,k}(t)$, we differentiate the state ODE with respect to $k$. We can interchange the order of differentiation with respect to $t$ and $k$:\n$$\n\\frac{d}{dt} S_{x,k}(t) = \\frac{d}{dt}\\left(\\frac{\\partial x}{\\partial k}\\right) = \\frac{\\partial}{\\partial k}\\left(\\frac{dx}{dt}\\right)\n$$\nApplying this to the right-hand side of the state ODE, using the product rule:\n$$\n\\frac{\\partial}{\\partial k}(-k\\,x) = -\\left(\\frac{\\partial k}{\\partial k} \\cdot x + k \\cdot \\frac{\\partial x}{\\partial k}\\right) = -(1 \\cdot x + k \\cdot S_{x,k})\n$$\nEquating the results gives the sensitivity ODE for $S_{x,k}(t)$:\n$$\n\\frac{d S_{x,k}}{dt} = -x - k S_{x,k}\n$$\nSubstituting the solution for $x(t) = x_{0} \\exp(-kt)$:\n$$\n\\frac{d S_{x,k}}{dt} + k S_{x,k} = -x_{0} \\exp(-kt)\n$$\nThe initial condition for $S_{x,k}(t)$ is found by differentiating the state initial condition $x(0) = x_{0}$ with respect to $k$. Since $x_{0}$ is treated as an independent parameter from $k$:\n$$\nS_{x,k}(0) = \\frac{\\partial x(0)}{\\partial k} = \\frac{\\partial x_{0}}{\\partial k} = 0\n$$\nWe solve this first-order linear non-homogeneous ODE using an integrating factor, $I(t) = \\exp\\left(\\int k \\, dt\\right) = \\exp(kt)$. Multiplying the ODE by $I(t)$:\n$$\n\\exp(kt)\\frac{d S_{x,k}}{dt} + k\\exp(kt) S_{x,k} = -x_{0} \\exp(-kt) \\exp(kt)\n$$\nThe left side simplifies to the derivative of a product:\n$$\n\\frac{d}{dt}\\left(S_{x,k}(t) \\exp(kt)\\right) = -x_{0}\n$$\nIntegrating with respect to $t$:\n$$\nS_{x,k}(t) \\exp(kt) = \\int -x_{0} \\, dt = -x_{0}t + C_{1}\n$$\nwhere $C_{1}$ is the integration constant. Solving for $S_{x,k}(t)$:\n$$\nS_{x,k}(t) = (-x_{0}t + C_{1}) \\exp(-kt)\n$$\nApplying the initial condition $S_{x,k}(0) = 0$:\n$$\n0 = (-x_{0} \\cdot 0 + C_{1}) \\exp(0) \\implies C_{1} = 0\n$$\nThus, the solution for the sensitivity with respect to $k$ is:\n$$\nS_{x,k}(t) = -x_{0} t \\exp(-kt)\n$$\n\n**2. Derivation and Solution for Sensitivity with respect to $x_0$**\n\nLet $S_{x,x_{0}}(t) = \\frac{\\partial x(t)}{\\partial x_{0}}$. Following the same procedure, we differentiate the state ODE with respect to $x_{0}$:\n$$\n\\frac{d}{dt} S_{x,x_{0}}(t) = \\frac{d}{dt}\\left(\\frac{\\partial x}{\\partial x_{0}}\\right) = \\frac{\\partial}{\\partial x_{0}}\\left(\\frac{dx}{dt}\\right)\n$$\nApplying this to the right-hand side:\n$$\n\\frac{\\partial}{\\partial x_{0}}(-k\\,x) = -k \\frac{\\partial x}{\\partial x_{0}} = -k S_{x,x_{0}}\n$$\nThis yields the sensitivity ODE:\n$$\n\\frac{d S_{x,x_{0}}}{dt} = -k S_{x,x_{0}}\n$$\nThe initial condition is found by differentiating $x(0) = x_{0}$ with respect to $x_{0}$:\n$$\nS_{x,x_{0}}(0) = \\frac{\\partial x(0)}{\\partial x_{0}} = \\frac{\\partial x_{0}}{\\partial x_{0}} = 1\n$$\nThe ODE for $S_{x,x_{0}}(t)$ is a simple exponential decay equation. Its solution is:\n$$\nS_{x,x_{0}}(t) = S_{x,x_{0}}(0) \\exp(-kt)\n$$\nSubstituting the initial condition $S_{x,x_{0}}(0) = 1$:\n$$\nS_{x,x_{0}}(t) = \\exp(-kt)\n$$\nThe problem requires the final answer to be presented as a row matrix $\\bigl[S_{x,k}(t)\\;\\;S_{x,x_{0}}(t)\\bigr]$. Based on our derivations, this is:\n$$\n\\begin{pmatrix} -x_{0} t \\exp(-kt) & \\exp(-kt) \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} -x_{0} t \\exp(-kt) & \\exp(-kt) \\end{pmatrix}}\n$$", "id": "4385596"}, {"introduction": "Having mastered the basics with a linear model, we now advance to a more realistic scenario involving nonlinear dynamics. Biological processes are rarely linear, and the Michaelis-Menten equation for saturable kinetics is a ubiquitous feature in systems biomedicine. This practice [@problem_id:4385597] challenges you to derive the sensitivity for a parameter in this nonlinear context, revealing the important concept that sensitivity itself can be a dynamic quantity that depends on the state of the system.", "problem": "Consider a single-compartment biomolecular species with concentration $x(t)$ undergoing saturable elimination with Michaelis-Menten structure. The state dynamics are given by the ordinary differential equation\n$$\n\\dot{x}(t) \\equiv \\frac{dx}{dt} \\;=\\; -\\,\\frac{V_{\\max}\\,x(t)}{K_{m} + x(t)},\n$$\nwith initial condition $x(0)=x_{0}>0$, where $K_{m}>0$ and $V_{\\max}>0$ are constant parameters. The measured output is $y(t)=x(t)$. Assume that $x_{0}$ does not depend on $V_{\\max}$. Using only the foundational definition of local parametric sensitivity as a partial derivative and standard calculus (e.g., separation of variables, differentiation with respect to a parameter), derive an explicit analytic expression for the unscaled local sensitivity\n$$\nS_{y,V_{\\max}}(t) \\;=\\; \\frac{\\partial y(t)}{\\partial V_{\\max}}\n$$\nevaluated along the solution trajectory $x(t)$, expressed in terms of $x(t)$, $K_{m}$, and $t$. Then, based on your derived expression, state how $S_{y,V_{\\max}}(t)$ varies with $x(t)$ in sign and magnitude. Your final reported answer must be the single explicit analytic expression for $S_{y,V_{\\max}}(t)$. No numerical approximation is required and no rounding is needed. Express the final answer without units.", "solution": "The problem requires the derivation of the unscaled local parametric sensitivity of the state variable $x(t)$ with respect to the parameter $V_{\\max}$. The state dynamics are given by the Michaelis-Menten ordinary differential equation (ODE):\n$$\n\\frac{dx}{dt} = -\\frac{V_{\\max} x(t)}{K_{m} + x(t)}\n$$\nwith the initial condition $x(0) = x_{0} > 0$. The parameters $V_{\\max}$ and $K_{m}$ are positive constants. The measured output is $y(t) = x(t)$. We are asked to find the sensitivity $S_{y,V_{\\max}}(t)$, defined as:\n$$\nS_{y,V_{\\max}}(t) = \\frac{\\partial y(t)}{\\partial V_{\\max}} = \\frac{\\partial x(t)}{\\partial V_{\\max}}\n$$\nWe will refer to this quantity as $S(t)$ for notational simplicity. The problem specifies that the initial condition $x_{0}$ is independent of $V_{\\max}$, which implies $\\frac{\\partial x_{0}}{\\partial V_{\\max}} = 0$.\n\nA direct approach to find $S(t)$ is to first obtain a relationship between $x(t)$, time $t$, and the model parameters by integrating the state ODE, and then differentiate this relationship with respect to $V_{\\max}$.\n\nFirst, we solve the ODE by separation of variables.\n$$\n\\frac{K_{m} + x(t)}{x(t)} dx = -V_{\\max} dt\n$$\nRearranging the left side gives:\n$$\n\\left( \\frac{K_{m}}{x(t)} + 1 \\right) dx = -V_{\\max} dt\n$$\nWe integrate both sides from the initial time $t=0$ to an arbitrary time $t$. The concentration at these times are $x(0)=x_{0}$ and $x(t)$, respectively.\n$$\n\\int_{x_{0}}^{x(t)} \\left( \\frac{K_{m}}{z} + 1 \\right) dz = \\int_{0}^{t} -V_{\\max} d\\tau\n$$\nEvaluating the integrals yields:\n$$\n\\left[ K_{m} \\ln(z) + z \\right]_{x_{0}}^{x(t)} = \\left[ -V_{\\max} \\tau \\right]_{0}^{t}\n$$\nSince $x_{0} > 0$ and the rate of change $\\frac{dx}{dt}$ is negative for $x>0$, the concentration $x(t)$ remains positive for all $t \\geq 0$. Thus, we do not need the absolute value within the logarithm.\n$$\n\\left( K_{m} \\ln(x(t)) + x(t) \\right) - \\left( K_{m} \\ln(x_{0}) + x_{0} \\right) = -V_{\\max} t\n$$\nThis equation provides an implicit solution for $x(t)$.\n\nNext, we find the sensitivity $S(t) = \\frac{\\partial x(t)}{\\partial V_{\\max}}$ by differentiating this implicit solution with respect to $V_{\\max}$. We treat $x(t)$ as a function of $V_{\\max}$ (and $t$), and apply the chain rule where appropriate. The terms involving only $x_{0}$ and $K_{m}$ have zero derivatives with respect to $V_{\\max}$, as per the problem statement ($\\frac{\\partial x_{0}}{\\partial V_{\\max}} = 0$) and the fact that $K_{m}$ is a parameter not dependent on $V_{\\max}$.\n$$\n\\frac{\\partial}{\\partial V_{\\max}} \\left[ K_{m} \\ln(x(t)) + x(t) - K_{m} \\ln(x_{0}) - x_{0} \\right] = \\frac{\\partial}{\\partial V_{\\max}} \\left[ -V_{\\max} t \\right]\n$$\nDifferentiating the left-hand side term by term:\n$$\n\\frac{\\partial}{\\partial V_{\\max}} (K_{m} \\ln(x(t))) = K_{m} \\frac{1}{x(t)} \\frac{\\partial x(t)}{\\partial V_{\\max}} = \\frac{K_{m}}{x(t)} S(t)\n$$\n$$\n\\frac{\\partial}{\\partial V_{\\max}} (x(t)) = \\frac{\\partial x(t)}{\\partial V_{\\max}} = S(t)\n$$\n$$\n\\frac{\\partial}{\\partial V_{\\max}} (- K_{m} \\ln(x_{0}) - x_{0}) = 0\n$$\nSo the derivative of the left-hand side is:\n$$\n\\frac{K_{m}}{x(t)} S(t) + S(t) = \\left( \\frac{K_{m}}{x(t)} + 1 \\right) S(t) = \\left( \\frac{K_{m} + x(t)}{x(t)} \\right) S(t)\n$$\nDifferentiating the right-hand side with respect to $V_{\\max}$ (treating $t$ as a constant in this context) gives:\n$$\n\\frac{\\partial}{\\partial V_{\\max}} (-V_{\\max} t) = -t\n$$\nEquating the derivatives of both sides, we obtain an algebraic equation for $S(t)$:\n$$\n\\left( \\frac{K_{m} + x(t)}{x(t)} \\right) S(t) = -t\n$$\nSolving for $S(t)$ yields the final expression for the sensitivity:\n$$\nS(t) = -t \\left( \\frac{x(t)}{K_{m} + x(t)} \\right)\n$$\nThis is the explicit analytic expression for the unscaled local sensitivity $S_{y,V_{\\max}}(t)$ as requested, expressed in terms of $x(t)$, $K_{m}$, and $t$.\n\nFinally, we analyze how $S_{y,V_{\\max}}(t)$ varies.\nFor $t > 0$, the time $t$ is positive. Since $x(t) > 0$ and $K_{m} > 0$, the fraction $\\frac{x(t)}{K_{m} + x(t)}$ is always positive. Due to the leading negative sign, the sensitivity $S_{y,V_{\\max}}(t)$ is always negative for $t > 0$. At $t=0$, $S(0)=0$. This negative sign is physically consistent: an increase in the maximum elimination rate $V_{\\max}$ will cause the concentration $x(t)$ to be lower at any subsequent time, hence the partial derivative must be negative.\n\nThe magnitude of the sensitivity is $|S(t)| = t \\frac{x(t)}{K_{m} + x(t)}$. This magnitude is a product of two terms: time $t$, which increases monotonically, and the term $\\frac{x(t)}{K_{m} + x(t)}$. Since $x(t)$ is a monotonically decreasing function of time, and the function $f(z) = \\frac{z}{K_{m}+z}$ is monotonically increasing for $z>0$, the term $\\frac{x(t)}{K_{m} + x(t)}$ is also a monotonically decreasing function of time.\nAt $t \\to 0$, $|S(t)| \\to 0$. As $t \\to \\infty$, $x(t) \\to 0$. In the regime where $x(t) \\ll K_{m}$, the state dynamics are approximately $\\frac{dx}{dt} \\approx -\\frac{V_{\\max}}{K_{m}}x(t)$, leading to an exponential decay $x(t) \\sim \\exp(-\\frac{V_{\\max}}{K_{m}}t)$. The magnitude of the sensitivity then behaves as $|S(t)| \\approx t \\frac{x(t)}{K_{m}} \\sim t \\exp(-\\frac{V_{\\max}}{K_{m}}t)$, which approaches $0$ as $t \\to \\infty$ because the exponential decay dominates the linear growth of $t$.\nSince the magnitude of the sensitivity starts at $0$ at $t=0$ and returns to $0$ as $t \\to \\infty$, it must achieve a maximum value at some intermediate time $t > 0$. This implies that the system's output $x(t)$ is most sensitive to changes in $V_{\\max}$ at an intermediate point along the decay curve, not at the beginning or far into the elimination process.", "answer": "$$\\boxed{-t \\frac{x(t)}{K_{m} + x(t)}}$$", "id": "4385597"}, {"introduction": "Our final practice bridges the gap between analytical theory and computational application, a crucial step for any practicing modeler. While local sensitivity gives us a snapshot of a parameter's influence, practical identifiability analysis requires exploring the parameter space in the context of data fitting. This exercise [@problem_id:4385525] guides you through implementing parameter profiling, a robust numerical method to assess parameter uncertainty and identifiability by systematically performing constrained optimizations, showcasing how sensitivity concepts inform the practical challenge of building reliable models from data.", "problem": "You are given a parameterized dynamical system commonly used in Systems Biomedicine to model a two-compartment process with elimination. Let the state vector be $x(t) = [x_1(t), x_2(t)]^\\top$ with the initial condition $x_1(0) = D$ and $x_2(0) = 0$. The input is a bolus dose, so there is no time-varying input after $t=0$. The state dynamics follow linear mass-balance laws:\n$$\n\\frac{dx_1}{dt} = -k_{\\mathrm{el}} x_1 - k_{12} x_1 + k_{21} x_2,\n\\qquad\n\\frac{dx_2}{dt} = k_{12} x_1 - k_{21} x_2,\n$$\nwith strictly positive parameters $k_{\\mathrm{el}}$, $k_{12}$, and $k_{21}$. The measured output is given by a linear observation model\n$$\ny(t) = C_1 x_1(t),\n$$\nwith a strictly positive measurement scaling $C_1$. Let $\\theta = [k_{\\mathrm{el}}, k_{12}, k_{21}, C_1]^\\top$ denote the full parameter vector.\n\nYou are provided with synthetic data $\\{(t_j, y_j)\\}_{j=1}^m$ generated by the model at specified sampling times $t_j$, with additive noise. Define the weighted least-squares cost function\n$$\nJ(\\theta) = \\sum_{j=1}^m \\left( \\frac{y_j - C_1 x_1(t_j;\\, k_{\\mathrm{el}}, k_{12}, k_{21})}{\\sigma} \\right)^2,\n$$\nwhere $\\sigma$ is a known noise scale and $x_1(t_j;\\, k_{\\mathrm{el}}, k_{12}, k_{21})$ is obtained by numerically integrating the dynamical system.\n\nConsider the parameter profiling of the parameter of interest $k_{\\mathrm{el}}$ by constrained optimization over the nuisance parameters $[k_{12}, k_{21}, C_1]^\\top$. For a fixed value of $k_{\\mathrm{el}}$, the profile cost is defined as\n$$\nJ_{\\mathrm{prof}}(k_{\\mathrm{el}}) = \\min_{k_{12},\\,k_{21},\\,C_1} \\, J\\big([k_{\\mathrm{el}}, k_{12}, k_{21}, C_1]^\\top\\big)\n\\quad\n\\text{subject to}\n\\quad\nk_{12} \\ge 0, \\; k_{21} \\ge 0, \\; C_1 \\ge 0.\n$$\nThe minimizers of the nuisance parameters along the profile are denoted by the optimizer path\n$$\n[k_{12}^\\star(k_{\\mathrm{el}}),\\,k_{21}^\\star(k_{\\mathrm{el}}),\\,C_1^\\star(k_{\\mathrm{el}})]^\\top\n\\quad \\text{such that} \\quad\nJ_{\\mathrm{prof}}(k_{\\mathrm{el}}) = J\\big([k_{\\mathrm{el}}, k_{12}^\\star(k_{\\mathrm{el}}), k_{21}^\\star(k_{\\mathrm{el}}), C_1^\\star(k_{\\mathrm{el}})]^\\top\\big).\n$$\n\nYour task is to implement a numerical procedure that:\n- Constructs a grid of values for $k_{\\mathrm{el}}$ over a specified interval.\n- For each grid value, solves the constrained optimization problem to compute $J_{\\mathrm{prof}}(k_{\\mathrm{el}})$ and records the corresponding optimizer path $[k_{12}^\\star(k_{\\mathrm{el}}), k_{21}^\\star(k_{\\mathrm{el}}), C_1^\\star(k_{\\mathrm{el}})]$.\n- Uses warm-starts by seeding each optimization with the optimizer from the previous grid point to track the optimizer path smoothly across the grid.\n- Selects the grid value $k_{\\mathrm{el}}^\\star$ that minimizes the profile $J_{\\mathrm{prof}}(k_{\\mathrm{el}})$.\n\nImplement this procedure for the following three test cases. In each case, synthetic data are generated by integrating the model with a specified $\\theta_{\\text{true}}$ and adding independent Gaussian noise with zero mean and standard deviation $\\sigma$. Use the fixed random seed $42$ for reproducibility. The dose is $D=1$ in all cases.\n\nTest case $1$ (happy path):\n- True parameters: $\\theta_{\\text{true}} = [0.25, 0.40, 0.20, 1.00]^\\top$.\n- Sampling times: $t_j$ uniformly spaced in $[0, 10]$ with $m=21$ points.\n- Noise scale: $\\sigma = 0.02$.\n- Profiling grid: $k_{\\mathrm{el}} \\in [0.05, 0.60]$ with $21$ evenly spaced points.\n- Bounds for nuisance parameters: $0.01 \\le k_{12} \\le 2.00$, $0.01 \\le k_{21} \\le 2.00$, $0.50 \\le C_1 \\le 1.50$.\n\nTest case $2$ (boundary behavior):\n- True parameters: $\\theta_{\\text{true}} = [0.01, 0.30, 0.50, 1.00]^\\top$.\n- Sampling times: $t_j$ uniformly spaced in $[0, 5]$ with $m=26$ points.\n- Noise scale: $\\sigma = 0.02$.\n- Profiling grid: $k_{\\mathrm{el}} \\in [0.01, 0.20]$ with $21$ evenly spaced points.\n- Bounds for nuisance parameters: $0.05 \\le k_{12} \\le 2.00$, $0.05 \\le k_{21} \\le 2.00$, $0.80 \\le C_1 \\le 1.20$.\n\nTest case $3$ (sparse and noisy data, potential identifiability issues):\n- True parameters: $\\theta_{\\text{true}} = [0.50, 0.30, 0.10, 1.00]^\\top$.\n- Sampling times: $t_j = [0, 1, 2, 3, 4]$ so $m=5$.\n- Noise scale: $\\sigma = 0.10$.\n- Profiling grid: $k_{\\mathrm{el}} \\in [0.10, 1.00]$ with $21$ evenly spaced points.\n- Bounds for nuisance parameters: $0.01 \\le k_{12} \\le 2.00$, $0.01 \\le k_{21} \\le 2.00$, $0.50 \\le C_1 \\le 1.50$.\n\nYour program must:\n- Generate synthetic data for each test case.\n- For each test case, compute the full profile $J_{\\mathrm{prof}}(k_{\\mathrm{el}})$ across the specified grid using constrained optimization and warm-start path tracking of the nuisance parameters.\n- Return, for each test case, the grid value $k_{\\mathrm{el}}^\\star$ that minimizes the profile $J_{\\mathrm{prof}}(k_{\\mathrm{el}})$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each result is the minimizing $k_{\\mathrm{el}}^\\star$ for the corresponding test case, represented as a float.", "solution": "The solution proceeds in four main steps:\n$1$. Derivation of the analytical solution for the system of ordinary differential equations (ODEs) to enable efficient computation.\n$2$. Formulation of the numerical optimization problem, including the objective function and the warm-start strategy.\n$3$. Generation of synthetic data as specified for each test case.\n$4$. Implementation of the complete profiling procedure to find the optimal $k_{\\mathrm{el}}^\\star$ for each case.\n\n### 1. Analytical Solution of the Dynamical System\n\nThe state dynamics are given by a linear system of ODEs:\n$$\n\\frac{d}{dt} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} -k_{\\mathrm{el}} - k_{12} & k_{21} \\\\ k_{12} & -k_{21} \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}\n$$\nThis is of the form $\\dot{x} = A x$, where $A$ is the $2 \\times 2$ system matrix. The solution is given by $x(t) = e^{At} x(0)$, where $x(0) = [D, 0]^\\top$. A more direct approach is to use the eigenvalues and eigenvectors of $A$. The characteristic equation is $\\lambda^2 - \\mathrm{tr}(A)\\lambda + \\det(A) = 0$.\n\nThe trace and determinant of $A$ are:\n$$\n\\mathrm{tr}(A) = -k_{\\mathrm{el}} - k_{12} - k_{21}\n$$\n$$\n\\det(A) = (-k_{\\mathrm{el}} - k_{12})(-k_{21}) - (k_{12})(k_{21}) = k_{\\mathrm{el}}k_{21} + k_{12}k_{21} - k_{12}k_{21} = k_{\\mathrm{el}}k_{21}\n$$\nThe eigenvalues $\\lambda_{1,2}$ are the roots of the characteristic polynomial:\n$$\n\\lambda_{1,2} = \\frac{\\mathrm{tr}(A) \\pm \\sqrt{\\mathrm{tr}(A)^2 - 4\\det(A)}}{2} = \\frac{-(k_{\\mathrm{el}} + k_{12} + k_{21}) \\pm \\sqrt{(k_{\\mathrm{el}} + k_{12} + k_{21})^2 - 4k_{\\mathrm{el}}k_{21}}}{2}\n$$\nSince all rate parameters are strictly positive, the discriminant $\\Delta = (k_{\\mathrm{el}} + k_{12} - k_{21})^2 + 4k_{12}k_{21}$ is strictly positive. Thus, the eigenvalues $\\lambda_{1,2}$ are always real and distinct. Furthermore, since $\\sqrt{\\Delta} < (k_{\\mathrm{el}} + k_{12} + k_{21})$, both eigenvalues are strictly negative, corresponding to a stable system.\n\nThe general solution for $x_1(t)$ is a sum of two exponentials, $x_1(t) = A_1 e^{\\lambda_1 t} + A_2 e^{\\lambda_2 t}$. By applying the initial conditions $x_1(0)=D$ and $x_2(0)=0$ (which implies $\\dot{x_1}(0) = -D(k_{\\mathrm{el}}+k_{12})$), we can solve for the coefficients. A more systematic approach using eigenvectors yields the full solution. The solution for the first compartment, $x_1(t)$, can be shown to be:\n$$\nx_1(t; k_{\\mathrm{el}}, k_{12}, k_{21}) = \\frac{D}{\\lambda_1 - \\lambda_2} \\left[ (k_{21} + \\lambda_1) e^{\\lambda_1 t} - (k_{21} + \\lambda_2) e^{\\lambda_2 t} \\right]\n$$\nThe observed output is then $y(t) = C_1 x_1(t)$. Using this analytical formula avoids numerical integration within the optimization loop, leading to significant gains in computational efficiency and accuracy.\n\n### 2. Numerical Optimization and Profiling\n\nFor a fixed value of $k_{\\mathrm{el}}$ from the specified grid, we must find the nuisance parameters $\\psi = [k_{12}, k_{21}, C_1]^\\top$ that minimize the cost function $J$. The cost function is:\n$$\nJ(k_{\\mathrm{el}}, \\psi) = \\sum_{j=1}^m \\left( \\frac{y_j - y(t_j; k_{\\mathrm{el}}, \\psi)}{\\sigma} \\right)^2\n$$\nwhere $y(t_j; k_{\\mathrm{el}}, \\psi)$ is the model prediction $C_1 x_1(t_j; k_{\\mathrm{el}}, k_{12}, k_{21})$ and $(t_j, y_j)$ are the synthetic data points.\n\nThis is a constrained non-linear least-squares problem. We seek:\n$$\n\\psi^\\star(k_{\\mathrm{el}}) = \\arg\\min_{\\psi} J(k_{\\mathrm{el}}, \\psi) \\quad \\text{subject to} \\quad \\psi_{lb} \\le \\psi \\le \\psi_{ub}\n$$\nwhere $\\psi_{lb}$ and $\\psi_{ub}$ are the lower and upper bounds for the nuisance parameters given in each test case.\n\nThis optimization is performed for each $k_{\\mathrm{el}}$ in its grid. The `L-BFGS-B` algorithm, a quasi-Newton method available in `scipy.optimize.minimize`, is well-suited for this task as it efficiently handles box constraints.\n\nA crucial part of the procedure is the warm-start strategy. For the first $k_{\\mathrm{el}}$ value in the grid, the optimization is initialized with a neutral guess, such as the midpoint of the allowed bounds for $\\psi$. For each subsequent $k_{\\mathrm{el}}$ value, the optimization is initialized using the optimal $\\psi^\\star$ found for the previous $k_{\\mathrm{el}}$ value. This path-tracking approach improves convergence speed and helps the optimizer follow a continuous path of local minima, which is the essence of constructing a parameter profile.\n\nThe profile cost is the set of minimum cost values obtained, $J_{\\mathrm{prof}}(k_{\\mathrm{el}}) = J(k_{\\mathrm{el}}, \\psi^\\star(k_{\\mathrm{el}}))$. The final step is to find the grid value $k_{\\mathrm{el}}^\\star$ that corresponds to the minimum of $J_{\\mathrm{prof}}(k_{\\mathrm{el}})$.\n\n### 3. Synthetic Data Ceneration\n\nFor each test case, synthetic data are generated as follows:\n1. The model is solved using the provided `true` parameters $\\theta_{\\text{true}} = [k_{\\mathrm{el,true}}, k_{12,\\text{true}}, k_{21,\\text{true}}, C_{1,\\text{true}}]^\\top$ to obtain the noise-free output $y_{\\text{true}}(t_j) = C_{1,\\text{true}} x_1(t_j; \\theta_{\\text{true}})$.\n2. Independent and identically distributed Gaussian noise, $\\epsilon_j \\sim \\mathcal{N}(0, \\sigma^2)$, is added to the true output. The noisy data are $y_j = y_{\\text{true}}(t_j) + \\epsilon_j$.\n3. To ensure reproducibility, the random number generator is seeded with the fixed value $42$.\n\n### 4. Implementation Summary\n\nThe overall algorithm is implemented in a single Python script.\n- A main function `solve` iterates through the three test cases.\n- For each case, a `numpy.random.Generator` seeded with $42$ is used to generate the noisy data.\n- A helper function, `solve_analytical`, implements the analytical solution for $x_1(t)$ and returns the model output $y(t)$.\n- A factory function, `make_cost_function`, is created to encapsulate the fixed data, $k_{\\mathrm{el}}$ value, and other constants, returning a cost function of only the nuisance parameters $\\psi$.\n- A loop iterates over the $k_{\\mathrm{el}}$ grid. Inside the loop, `scipy.optimize.minimize` is called with the `L-BFGS-B` method, the appropriate bounds, and the warm-start initial guess.\n- The results of the profiling loop (the minimum costs) are stored.\n- `numpy.argmin` is used to find the index of the minimum profile cost, which identifies the optimal grid point $k_{\\mathrm{el}}^\\star$.\n- The resulting $k_{\\mathrm{el}}^\\star$ for each of the three cases are collected and printed in the specified format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to run the parameter profiling for all test cases.\n    \"\"\"\n    \n    def solve_analytical(params, times, D):\n        \"\"\"\n        Calculates the model output using the analytical solution of the ODE.\n        \n        Args:\n            params (list or np.ndarray): vector of parameters [kel, k12, k21, C1].\n            times (np.ndarray): time points for evaluation.\n            D (float): initial dose in compartment 1.\n            \n        Returns:\n            np.ndarray: The predicted model output y(t).\n        \"\"\"\n        kel, k12, k21, C1 = params\n        \n        # Eigenvalue calculation\n        trace = -(kel + k12 + k21)\n        det = kel * k21\n        \n        # The discriminant is (kel + k12 - k21)^2 + 4*k12*k21, which is always > 0\n        # for strictly positive parameters, so eigenvalues are real and distinct.\n        sqrt_delta = np.sqrt(trace**2 - 4 * det)\n        \n        lambda1 = (trace + sqrt_delta) / 2.0\n        lambda2 = (trace - sqrt_delta) / 2.0\n        \n        if np.isclose(lambda1, lambda2):\n            # This case is theoretically not reached with positive params,\n            # but is a safeguard for numerical stability.\n            return np.full_like(times, np.inf)\n\n        # Coefficients from initial conditions x1(0)=D, x2(0)=0\n        # x1(t) = D/(l1-l2) * [ (k21+l1)exp(l1*t) - (k21+l2)exp(l2*t) ]\n        factor = D / (lambda1 - lambda2)\n        term1 = (k21 + lambda1) * np.exp(lambda1 * times)\n        term2 = (k21 + lambda2) * np.exp(lambda2 * times)\n        \n        x1_t = factor * (term1 - term2)\n        \n        return C1 * x1_t\n\n    def make_cost_function(fixed_kel, data_t, data_y, sigma, D):\n        \"\"\"\n        Factory function to create the cost function for a fixed kel.\n        The returned function takes only the nuisance parameters as input.\n        \"\"\"\n        def cost_function(nuisance_params):\n            k12, k21, C1 = nuisance_params\n            full_params = [fixed_kel, k12, k21, C1]\n            \n            y_model = solve_analytical(full_params, data_t, D)\n            \n            residuals = (data_y - y_model) / sigma\n            return np.sum(residuals**2)\n            \n        return cost_function\n\n    test_cases = [\n        {\n            \"theta_true\": [0.25, 0.40, 0.20, 1.00],\n            \"times\": np.linspace(0, 10, 21),\n            \"sigma\": 0.02,\n            \"kel_grid\": np.linspace(0.05, 0.60, 21),\n            \"bounds\": [(0.01, 2.00), (0.01, 2.00), (0.50, 1.50)],\n        },\n        {\n            \"theta_true\": [0.01, 0.30, 0.50, 1.00],\n            \"times\": np.linspace(0, 5, 26),\n            \"sigma\": 0.02,\n            \"kel_grid\": np.linspace(0.01, 0.20, 21),\n            \"bounds\": [(0.05, 2.00), (0.05, 2.00), (0.80, 1.20)],\n        },\n        {\n            \"theta_true\": [0.50, 0.30, 0.10, 1.00],\n            \"times\": np.array([0., 1., 2., 3., 4.]),\n            \"sigma\": 0.10,\n            \"kel_grid\": np.linspace(0.10, 1.00, 21),\n            \"bounds\": [(0.01, 2.00), (0.01, 2.00), (0.50, 1.50)],\n        }\n    ]\n\n    results = []\n    D_dose = 1.0  # Dose is 1 for all cases\n    \n    # Use a single RNG for all cases for consistency\n    rng = np.random.default_rng(42)\n\n    for case in test_cases:\n        # 1. Generate synthetic data\n        y_true = solve_analytical(case[\"theta_true\"], case[\"times\"], D_dose)\n        noise = rng.normal(loc=0.0, scale=case[\"sigma\"], size=len(case[\"times\"]))\n        y_data = y_true + noise\n\n        profile_costs = []\n        \n        # Warm-start initialization\n        # For the first point, use the midpoint of the bounds.\n        initial_guess = np.array([(b[0] + b[1]) / 2 for b in case[\"bounds\"]])\n        last_optimal_nuisance = initial_guess\n\n        # 2. Compute profile likelihood\n        for kel_val in case[\"kel_grid\"]:\n            # Create the cost function for the current k_el\n            cost_func = make_cost_function(kel_val, case[\"times\"], y_data, case[\"sigma\"], D_dose)\n            \n            # Solve the constrained optimization problem for nuisance parameters\n            res = minimize(\n                fun=cost_func,\n                x0=last_optimal_nuisance,\n                method='L-BFGS-B',\n                bounds=case[\"bounds\"],\n                options={'ftol': 1e-9, 'gtol': 1e-7} # Tighter tolerances for accuracy\n            )\n            \n            profile_costs.append(res.fun)\n            \n            # Update the guess for the next iteration (warm-start)\n            last_optimal_nuisance = res.x\n            \n        # 3. Find k_el that minimizes the profile\n        min_cost_idx = np.argmin(profile_costs)\n        best_kel = case[\"kel_grid\"][min_cost_idx]\n        results.append(best_kel)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4385525"}]}