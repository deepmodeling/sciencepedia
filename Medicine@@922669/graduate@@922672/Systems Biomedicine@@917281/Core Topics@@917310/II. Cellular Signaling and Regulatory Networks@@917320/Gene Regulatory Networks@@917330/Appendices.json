{"hands_on_practices": [{"introduction": "Negative autoregulation, where a gene product represses its own synthesis, is a ubiquitous and fundamental motif in gene regulatory networks, often used to maintain homeostasis and stabilize protein levels. This exercise [@problem_id:1435734] provides essential practice in translating a biological scenario into a quantitative model using Hill functions. By setting up and solving the steady-state equation for a simple negative feedback loop, you will practice a foundational skill required for analyzing the dynamics of any gene circuit.", "problem": "A synthetic biologist is engineering a simple genetic circuit in a bacterial host. This circuit is designed to implement negative autoregulation, where a protein, which we will call the Repressor (R), inhibits the expression of its own gene. The concentration of the Repressor protein at time $t$ is denoted by $R(t)$. The dynamics of this system are modeled by the following principles:\n\n1.  **Production:** The rate of synthesis of the Repressor protein is described by a repressive Hill function. When the concentration of R is zero, its production rate is at a maximum value of $k_p$. As the concentration $R(t)$ increases, the protein binds to its own promoter region, inhibiting synthesis. This repression is characterized by a constant $K$, defined as the concentration of R that reduces the production rate to one-half of its maximum. The binding process is non-cooperative, corresponding to a Hill coefficient of $n=1$.\n\n2.  **Degradation:** The Repressor protein is continuously removed from the cell. This removal process follows first-order kinetics, characterized by a degradation rate constant $k_d$.\n\nAll parameters $k_p$, $k_d$, and $K$ are positive real constants. After a sufficient amount of time, the system is observed to reach a stable configuration where the concentration of the Repressor protein no longer changes. Determine the concentration of the Repressor protein R in this final, stable configuration. Your answer should be an analytical expression in terms of the parameters $k_p$, $k_d$, and $K$.", "solution": "Let $R(t)$ denote the concentration of the Repressor. By the problem statement:\n- Production follows a repressive Hill function with $n=1$, maximum rate $k_{p}$ at $R=0$, and half-maximum at $R=K$. Thus the production rate is\n$$\n\\text{Production}(R) = \\frac{k_{p}}{1 + \\frac{R}{K}}.\n$$\n- Degradation follows first-order kinetics with rate constant $k_{d}$, so the degradation rate is\n$$\n\\text{Degradation}(R) = k_{d} R.\n$$\n\nThe dynamical equation is therefore\n$$\n\\frac{dR}{dt} = \\frac{k_{p}}{1 + \\frac{R}{K}} - k_{d} R.\n$$\n\nAt the final stable configuration (steady state), $\\frac{dR}{dt}=0$. Hence $R$ satisfies\n$$\n\\frac{k_{p}}{1 + \\frac{R}{K}} - k_{d} R = 0.\n$$\nRearranging gives\n$$\n\\frac{k_{p}}{1 + \\frac{R}{K}} = k_{d} R.\n$$\nMultiply both sides by $1 + \\frac{R}{K}$:\n$$\nk_{p} = k_{d} R \\left(1 + \\frac{R}{K}\\right) = k_{d} R + \\frac{k_{d}}{K} R^{2}.\n$$\nThus $R$ satisfies the quadratic equation\n$$\n\\frac{k_{d}}{K} R^{2} + k_{d} R - k_{p} = 0.\n$$\nLet $a = \\frac{k_{d}}{K}$, $b = k_{d}$, and $c = -k_{p}$. By the quadratic formula,\n$$\nR = \\frac{-b \\pm \\sqrt{b^{2} - 4 a c}}{2 a} = \\frac{-k_{d} \\pm \\sqrt{k_{d}^{2} + \\frac{4 k_{d} k_{p}}{K}}}{2 \\frac{k_{d}}{K}}.\n$$\nSince $k_{p}, k_{d}, K > 0$, the physically relevant (nonnegative) root uses the plus sign. Simplifying by factoring $k_{d}$ under the square root,\n$$\nR^{\\ast} = \\frac{K}{2 k_{d}} \\left(-k_{d} + \\sqrt{k_{d}^{2} + \\frac{4 k_{d} k_{p}}{K}}\\right)\n= \\frac{K}{2} \\left(-1 + \\sqrt{1 + \\frac{4 k_{p}}{k_{d} K}}\\right).\n$$\nThis is the steady-state concentration of the Repressor in terms of $k_{p}$, $k_{d}$, and $K$.", "answer": "$$\\boxed{\\frac{K}{2}\\left(\\sqrt{1+\\frac{4 k_{p}}{k_{d} K}}-1\\right)}$$", "id": "1435734"}, {"introduction": "The ability of cells to make decisive, switch-like transitions between different states often relies on gene circuits capable of bistability. The toggle switch, composed of two mutually repressing genes, is a classic synthetic motif that exhibits this behavior. This exercise [@problem_id:4345428] moves beyond simple steady-state calculation to introduce linear stability analysis, a cornerstone technique for understanding network dynamics. You will construct the Jacobian matrix for the toggle switch and use its eigenvalues to determine if a steady state is stable, providing insight into the network's capacity for memory and decision-making.", "problem": "Consider a minimal two-gene regulatory network in a single well-mixed cell, modeled at the level of protein concentrations. Let $x(t)$ and $y(t)$ denote the protein concentrations of gene $X$ and gene $Y$, respectively, measured in nanomolar (nM). Gene $Y$ represses production of $X$ via a Hill-type repression with coefficient $n$, and gene $X$ represses production of $Y$ via a Hill-type repression with coefficient $m$. Assume that synthesis and degradation follow standard mass-action kinetics and Hill-type regulatory functions, consistent with the Central Dogma of molecular biology and widely used phenomenological models of gene regulatory networks (GRNs). The ordinary differential equations (ODEs) are\n$$\n\\frac{dx}{dt} \\;=\\; \\frac{\\alpha_{x}}{1 + \\left(\\frac{y}{K_{y}}\\right)^{n}} \\;-\\; \\delta_{x}\\,x,\n\\qquad\n\\frac{dy}{dt} \\;=\\; \\frac{\\alpha_{y}}{1 + \\left(\\frac{x}{K_{x}}\\right)^{m}} \\;-\\; \\delta_{y}\\,y,\n$$\nwhere $K_{x}$ and $K_{y}$ are repression thresholds (nM), $\\alpha_{x}$ and $\\alpha_{y}$ are maximal synthesis rates (nM/min), and $\\delta_{x}$ and $\\delta_{y}$ are first-order degradation rate constants (min$^{-1}$). The parameter values are:\n$$\n\\delta_{x} = 0.20,\\quad \\delta_{y} = 0.25,\\quad K_{x} = 100,\\quad K_{y} = 80,\\quad n = 2,\\quad m = 1,\n$$\nand the synthesis rates are chosen so that the steady state occurs at the repression thresholds:\n$$\n\\alpha_{x} = 2\\,\\delta_{x}\\,K_{x},\\qquad \\alpha_{y} = 2\\,\\delta_{y}\\,K_{y}.\n$$\nStarting from the stated ODEs and definitions, do the following:\n1. Verify that $\\big(x^{\\ast}, y^{\\ast}\\big) = \\big(K_{x}, K_{y}\\big)$ is a steady state under the given parameter choices.\n2. Linearize the system around $\\big(x^{\\ast}, y^{\\ast}\\big)$ and construct the Jacobian matrix $J$ with entries $J_{ij} = \\left.\\frac{\\partial f_{i}}{\\partial z_{j}}\\right|_{(x^{\\ast}, y^{\\ast})}$, where $\\mathbf{f} = \\big(f_{x}, f_{y}\\big)$ are the right-hand sides of the ODEs, and $\\mathbf{z} = (x,y)$.\n3. Using first principles of linear stability analysis, assess the local stability at $\\big(x^{\\ast}, y^{\\ast}\\big)$ by computing the eigenvalues of $J$ and report the largest real part among these eigenvalues.\n\nAnswer specification:\n- Provide the largest real part of the eigenvalues of $J$ as a single real number.\n- Express your final numerical answer in min$^{-1}$.\n- Round your answer to four significant figures.", "solution": "The problem asks for an analysis of the local stability of a two-gene regulatory network at a specified steady state. The analysis proceeds in three steps: first, verification of the steady state; second, construction of the Jacobian matrix by linearizing the system at that steady state; and third, calculation of the eigenvalues of the Jacobian to determine stability.\n\nThe system of ordinary differential equations (ODEs) is given by:\n$$\n\\frac{dx}{dt} = f_{x}(x, y) = \\frac{\\alpha_{x}}{1 + \\left(\\frac{y}{K_{y}}\\right)^{n}} - \\delta_{x}\\,x,\n$$\n$$\n\\frac{dy}{dt} = f_{y}(x, y) = \\frac{\\alpha_{y}}{1 + \\left(\\frac{x}{K_{x}}\\right)^{m}} - \\delta_{y}\\,y.\n$$\n\nThe parameter values are:\n$\\delta_{x} = 0.20$, $\\delta_{y} = 0.25$, $K_{x} = 100$, $K_{y} = 80$, $n = 2$, and $m = 1$.\nThe synthesis rates are defined as $\\alpha_{x} = 2\\,\\delta_{x}\\,K_{x}$ and $\\alpha_{y} = 2\\,\\delta_{y}\\,K_{y}$.\n\n**Step 1: Verification of the Steady State**\n\nA steady state $(x^{\\ast}, y^{\\ast})$ is a point where both time derivatives are zero, i.e., $f_{x}(x^{\\ast}, y^{\\ast}) = 0$ and $f_{y}(x^{\\ast}, y^{\\ast}) = 0$. We must verify that $(x^{\\ast}, y^{\\ast}) = (K_{x}, K_{y})$ is a steady state.\n\nSubstituting $x = K_{x}$ and $y = K_{y}$ into the first ODE:\n$$\n\\left.\\frac{dx}{dt}\\right|_{(K_{x}, K_{y})} = \\frac{\\alpha_{x}}{1 + \\left(\\frac{K_{y}}{K_{y}}\\right)^{n}} - \\delta_{x}K_{x} = \\frac{\\alpha_{x}}{1 + 1^{n}} - \\delta_{x}K_{x} = \\frac{\\alpha_{x}}{2} - \\delta_{x}K_{x}.\n$$\nUsing the given relation $\\alpha_{x} = 2\\,\\delta_{x}\\,K_{x}$, we have:\n$$\n\\frac{2\\,\\delta_{x}\\,K_{x}}{2} - \\delta_{x}K_{x} = \\delta_{x}K_{x} - \\delta_{x}K_{x} = 0.\n$$\nSubstituting $x = K_{x}$ and $y = K_{y}$ into the second ODE:\n$$\n\\left.\\frac{dy}{dt}\\right|_{(K_{x}, K_{y})} = \\frac{\\alpha_{y}}{1 + \\left(\\frac{K_{x}}{K_{x}}\\right)^{m}} - \\delta_{y}K_{y} = \\frac{\\alpha_{y}}{1 + 1^{m}} - \\delta_{y}K_{y} = \\frac{\\alpha_{y}}{2} - \\delta_{y}K_{y}.\n$$\nUsing the given relation $\\alpha_{y} = 2\\,\\delta_{y}\\,K_{y}$, we have:\n$$\n\\frac{2\\,\\delta_{y}\\,K_{y}}{2} - \\delta_{y}K_{y} = \\delta_{y}K_{y} - \\delta_{y}K_{y} = 0.\n$$\nSince both derivatives are zero, $(x^{\\ast}, y^{\\ast}) = (K_{x}, K_{y})$ is confirmed to be a steady state of the system.\n\n**Step 2: Construction of the Jacobian Matrix**\n\nThe Jacobian matrix $J$ of the system is defined as:\n$$\nJ = \\begin{pmatrix} \\frac{\\partial f_x}{\\partial x} & \\frac{\\partial f_x}{\\partial y} \\\\ \\frac{\\partial f_y}{\\partial x} & \\frac{\\partial f_y}{\\partial y} \\end{pmatrix}.\n$$\nWe compute the four partial derivatives:\n$$\nJ_{11} = \\frac{\\partial f_x}{\\partial x} = -\\delta_x\n$$\n$$\nJ_{12} = \\frac{\\partial f_x}{\\partial y} = \\frac{\\partial}{\\partial y} \\left( \\alpha_x \\left(1 + \\left(\\frac{y}{K_y}\\right)^n \\right)^{-1} \\right) = -\\alpha_x \\left(1 + \\left(\\frac{y}{K_y}\\right)^n \\right)^{-2} \\left( n\\frac{y^{n-1}}{K_y^n} \\right)\n$$\n$$\nJ_{21} = \\frac{\\partial f_y}{\\partial x} = \\frac{\\partial}{\\partial x} \\left( \\alpha_y \\left(1 + \\left(\\frac{x}{K_x}\\right)^m \\right)^{-1} \\right) = -\\alpha_y \\left(1 + \\left(\\frac{x}{K_x}\\right)^m \\right)^{-2} \\left( m\\frac{x^{m-1}}{K_x^m} \\right)\n$$\n$$\nJ_{22} = \\frac{\\partial f_y}{\\partial y} = -\\delta_y\n$$\nNow, we evaluate these derivatives at the steady state $(x^{\\ast}, y^{\\ast}) = (K_x, K_y)$:\n$$\nJ_{11} = -\\delta_x\n$$\n$$\nJ_{12} = -\\alpha_x \\left(1 + \\left(\\frac{K_y}{K_y}\\right)^n \\right)^{-2} \\left( n\\frac{K_y^{n-1}}{K_y^n} \\right) = -\\alpha_x (1+1)^{-2} \\left( \\frac{n}{K_y} \\right) = -\\frac{n\\alpha_x}{4K_y}\n$$\n$$\nJ_{21} = -\\alpha_y \\left(1 + \\left(\\frac{K_x}{K_x}\\right)^m \\right)^{-2} \\left( m\\frac{K_x^{m-1}}{K_x^m} \\right) = -\\alpha_y (1+1)^{-2} \\left( \\frac{m}{K_x} \\right) = -\\frac{m\\alpha_y}{4K_x}\n$$\n$$\nJ_{22} = -\\delta_y\n$$\nSubstituting $\\alpha_x = 2\\delta_x K_x$ and $\\alpha_y = 2\\delta_y K_y$:\n$$\nJ_{12} = -\\frac{n(2\\delta_x K_x)}{4K_y} = -\\frac{n\\delta_x K_x}{2K_y}\n$$\n$$\nJ_{21} = -\\frac{m(2\\delta_y K_y)}{4K_x} = -\\frac{m\\delta_y K_y}{2K_x}\n$$\nThus, the symbolic Jacobian at the steady state is:\n$$\nJ = \\begin{pmatrix} -\\delta_x & -\\frac{n\\delta_x K_x}{2K_y} \\\\ -\\frac{m\\delta_y K_y}{2K_x} & -\\delta_y \\end{pmatrix}\n$$\nSubstituting the numerical values:\n$$\nJ_{11} = -0.20\n$$\n$$\nJ_{12} = -\\frac{2 \\times 0.20 \\times 100}{2 \\times 80} = -\\frac{40}{160} = -0.25\n$$\n$$\nJ_{21} = -\\frac{1 \\times 0.25 \\times 80}{2 \\times 100} = -\\frac{20}{200} = -0.10\n$$\n$$\nJ_{22} = -0.25\n$$\nThe numerical Jacobian matrix is:\n$$\nJ = \\begin{pmatrix} -0.20 & -0.25 \\\\ -0.10 & -0.25 \\end{pmatrix}\n$$\n\n**Step 3: Stability Analysis via Eigenvalues**\n\nThe stability of the steady state is determined by the eigenvalues of $J$. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(J - \\lambda I) = 0$, which for a $2 \\times 2$ matrix is $\\lambda^2 - \\text{tr}(J)\\lambda + \\det(J) = 0$.\n\nFirst, we compute the trace and determinant of $J$:\n$$\n\\text{tr}(J) = J_{11} + J_{22} = -0.20 + (-0.25) = -0.45\n$$\n$$\n\\det(J) = J_{11}J_{22} - J_{12}J_{21} = (-0.20)(-0.25) - (-0.25)(-0.10) = 0.050 - 0.025 = 0.025\n$$\nThe characteristic equation is:\n$$\n\\lambda^2 - (-0.45)\\lambda + 0.025 = 0 \\implies \\lambda^2 + 0.45\\lambda + 0.025 = 0\n$$\nWe solve this quadratic equation for $\\lambda$ using the formula $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\n\\lambda = \\frac{-0.45 \\pm \\sqrt{(0.45)^2 - 4(1)(0.025)}}{2} = \\frac{-0.45 \\pm \\sqrt{0.2025 - 0.1}}{2} = \\frac{-0.45 \\pm \\sqrt{0.1025}}{2}\n$$\nThe discriminant $\\Delta = 0.1025$ is positive, so there are two distinct real eigenvalues. The largest real part of the eigenvalues will be the larger of these two real roots.\n$$\n\\lambda_{\\text{max}} = \\frac{-0.45 + \\sqrt{0.1025}}{2}\n$$\nNumerically evaluating this expression:\n$$\n\\sqrt{0.1025} \\approx 0.3201562\n$$\n$$\n\\lambda_{\\text{max}} \\approx \\frac{-0.45 + 0.3201562}{2} = \\frac{-0.1298438}{2} = -0.0649219\n$$\nThe problem requires rounding to four significant figures.\n$$\n\\lambda_{\\text{max}} \\approx -0.06492\n$$\nThe units of the eigenvalues are the same as the units of the diagonal elements of the Jacobian, which are min$^{-1}$. Since the largest real part is negative, the steady state $(K_x, K_y)$ is locally stable.\n\nThe largest real part of the eigenvalues is approximately $-0.06492$ min$^{-1}$.", "answer": "$$\\boxed{-0.06492}$$", "id": "4345428"}, {"introduction": "While the previous practices focused on analyzing networks with known structures, a central challenge in systems biology is to reverse-engineer these structures from experimental data. This advanced exercise [@problem_id:4278329] tackles this inference problem using a sophisticated and widely used method. You will implement a Dynamic Bayesian Network (DBN) model to deduce the most probable regulatory links from a given time-series dataset, learning to work with concepts like marginal likelihoods and sparsity-inducing priors. This practice bridges the gap between theoretical models and real-world data analysis.", "problem": "You are given a time-lagged gene expression time series for a small gene regulatory network modeled as a first-order Dynamic Bayesian Network (DBN). The DBN assumes that, for each gene (target) at time step $t$, its expression is a linear combination of all genes (potential parents, including itself) at time step $t-1$, corrupted by conditionally independent Gaussian noise. Specifically, for each target gene index $g \\in \\{1,\\dots,G\\}$ and time steps $t \\in \\{2,\\dots,T\\}$, the model is\n$$\nx_{t,g} \\mid \\mathbf{x}_{t-1}, \\mathbf{w}_g, \\sigma^2 \\sim \\mathcal{N}\\!\\left(\\sum_{h=1}^{G} w_{g,h}\\, x_{t-1,h},\\, \\sigma^2\\right),\n$$\nwith independent spike-and-slab priors encoding sparsity on the edge indicators. Let $z_{g,h} \\in \\{0,1\\}$ be the indicator of whether an edge from source gene $h$ to target gene $g$ exists. The prior factorizes as\n$$\nz_{g,h} \\stackrel{\\text{i.i.d.}}{\\sim} \\text{Bernoulli}(p), \\quad\nw_{g,h} \\mid z_{g,h} = 1 \\sim \\mathcal{N}(0,\\tau^2), \\quad\nw_{g,h} \\mid z_{g,h} = 0 = 0,\n$$\nwith hyperparameters $p \\in (0,1)$, $\\sigma^2 > 0$, and $\\tau^2 > 0$. For each target gene $g$, let $\\mathcal{S} \\subseteq \\{1,\\dots,G\\}$ denote a subset of active parents (those indices $h$ such that $z_{g,h} = 1$). Then, conditional on $\\mathcal{S}$, the regression weights $\\mathbf{w}_{g,\\mathcal{S}}$ have prior $\\mathcal{N}(\\mathbf{0}, \\tau^2 \\mathbf{I})$, and inactive weights are exactly zero.\n\nFrom the fundamental rules of probability and properties of Gaussian integrals, the marginal likelihood for target $g$ under parent set $\\mathcal{S}$ is obtained by integrating out the Gaussian regression weights:\n$$\np(\\mathbf{y}_g \\mid \\mathbf{X}, \\mathcal{S}, \\sigma^2, \\tau^2) = \\mathcal{N}\\!\\left(\\mathbf{y}_g; \\mathbf{0},\\, \\boldsymbol{\\Sigma}_{\\mathcal{S}}\\right), \\quad\n\\boldsymbol{\\Sigma}_{\\mathcal{S}} = \\sigma^2 \\mathbf{I} + \\tau^2 \\mathbf{X}_{\\mathcal{S}} \\mathbf{X}_{\\mathcal{S}}^\\top,\n$$\nwhere $\\mathbf{y}_g \\in \\mathbb{R}^{N}$ stacks the target values at times $t \\in \\{2,\\dots,T\\}$, $\\mathbf{X} \\in \\mathbb{R}^{N \\times G}$ stacks predictors at times $t-1 \\in \\{1,\\dots,T-1\\}$, $\\mathbf{X}_{\\mathcal{S}}$ denotes the submatrix of $\\mathbf{X}$ with columns restricted to indices in $\\mathcal{S}$, and $N = T-1$.\n\nUsing Bayes' rule with the independent Bernoulli prior on $\\mathcal{S}$,\n$$\n\\Pr(\\mathcal{S} \\mid \\mathbf{y}_g, \\mathbf{X}, p, \\sigma^2, \\tau^2) \\propto p^{|\\mathcal{S}|} (1-p)^{G-|\\mathcal{S}|} \\cdot p(\\mathbf{y}_g \\mid \\mathbf{X}, \\mathcal{S}, \\sigma^2, \\tau^2).\n$$\nThe posterior edge inclusion probability for edge $(h \\to g)$ is then\n$$\n\\Pr(z_{g,h} = 1 \\mid \\mathbf{y}_g, \\mathbf{X}, p, \\sigma^2, \\tau^2) \\;=\\; \\sum_{\\mathcal{S} \\subseteq \\{1,\\dots,G\\}: h \\in \\mathcal{S}} \\Pr(\\mathcal{S} \\mid \\mathbf{y}_g, \\mathbf{X}, p, \\sigma^2, \\tau^2).\n$$\n\nYour task is to implement a program that, given a fixed small dataset and a set of hyperparameters, enumerates all parent sets $\\mathcal{S}$ for each target gene $g$, computes the exact posterior edge inclusion probabilities using the conjugate marginal likelihood and the sparsity-encoding prior, and outputs a flattened matrix of posterior edge probabilities for all ordered pairs $(h \\to g)$.\n\nUse the following fixed dataset with $G = 3$ genes and $T = 6$ time points. The matrix below lists $\\mathbf{x}_t \\in \\mathbb{R}^3$ for $t \\in \\{1,\\dots,6\\}$ as rows in temporal order. All numbers are unitless real-valued expression levels. You must use these exact values:\n- Row $1$: $[\\,1.0,\\;0.2,\\;-0.5\\,]$\n- Row $2$: $[\\,0.7,\\;0.56,\\;-0.22\\,]$\n- Row $3$: $[\\,0.49,\\;0.518,\\;-0.38\\,]$\n- Row $4$: $[\\,0.343,\\;0.4004,\\;-0.3868\\,]$\n- Row $5$: $[\\,0.2401,\\;0.29162,\\;-0.3176\\,]$\n- Row $6$: $[\\,0.16807,\\;0.207536,\\;-0.238492\\,]$\n\nConstruct $\\mathbf{X}$ as the first $T-1$ rows (rows $1$ through $5$) and the response for target $g$ as $\\mathbf{y}_g$ from the corresponding column of rows $2$ through $6$.\n\nTo ensure numerical stability and universal applicability:\n- Compute the log marginal likelihood using the matrix determinant lemma and Woodbury identity to avoid inverting $\\boldsymbol{\\Sigma}_{\\mathcal{S}}$ directly:\n  - Let $\\mathbf{G}_{\\mathcal{S}} = \\mathbf{X}_{\\mathcal{S}}^\\top \\mathbf{X}_{\\mathcal{S}}$, and define $\\mathbf{B}_{\\mathcal{S}} = \\mathbf{I} + \\frac{\\tau^2}{\\sigma^2} \\mathbf{G}_{\\mathcal{S}}$.\n  - Then\n    $$\n    \\log \\lvert \\boldsymbol{\\Sigma}_{\\mathcal{S}} \\rvert = N \\log \\sigma^2 + \\log \\lvert \\mathbf{B}_{\\mathcal{S}} \\rvert,\n    $$\n    $$\n    \\mathbf{y}_g^\\top \\boldsymbol{\\Sigma}_{\\mathcal{S}}^{-1} \\mathbf{y}_g = \\frac{1}{\\sigma^2}\\left(\\mathbf{y}_g^\\top \\mathbf{y}_g - \\frac{\\tau^2}{\\sigma^2}\\,\\mathbf{y}_g^\\top \\mathbf{X}_{\\mathcal{S}} \\mathbf{B}_{\\mathcal{S}}^{-1} \\mathbf{X}_{\\mathcal{S}}^\\top \\mathbf{y}_g\\right).\n    $$\n- For each target $g$, enumerate all $2^G$ subsets $\\mathcal{S} \\subseteq \\{1,2,3\\}$ and compute unnormalized log posterior scores\n  $$\n  \\log \\pi(\\mathcal{S}) = |\\mathcal{S}| \\log p + (G-|\\mathcal{S}|)\\log(1-p) + \\log p(\\mathbf{y}_g \\mid \\mathbf{X}, \\mathcal{S}, \\sigma^2, \\tau^2),\n  $$\n  then normalize via log-sum-exp to obtain $\\Pr(\\mathcal{S} \\mid \\cdot)$ exactly.\n\nTest suite. Your program must compute the posterior edge inclusion probabilities for the following four hyperparameter settings, each applied to the fixed dataset above:\n- Case $1$: $p = 0.3$, $\\sigma^2 = 0.01$, $\\tau^2 = 1.0$.\n- Case $2$: $p = 0.05$, $\\sigma^2 = 0.02$, $\\tau^2 = 1.0$.\n- Case $3$: $p = 0.5$, $\\sigma^2 = 0.1$, $\\tau^2 = 0.1$.\n- Case $4$: $p = 0.5$, $\\sigma^2 = 0.0001$, $\\tau^2 = 10.0$.\n\nRequired output. For each case, produce a list of $G \\times G$ floating-point numbers equal to the posterior edge inclusion probabilities $\\Pr(z_{g,h} = 1 \\mid \\text{data})$ arranged in row-major order with source index $h \\in \\{1,2,3\\}$ as the outer loop and target index $g \\in \\{1,2,3\\}$ as the inner loop. That is, the flattened list is\n$$\n[\\,\\Pr(h{=}1 \\to g{=}1),\\;\\Pr(h{=}1 \\to g{=}2),\\;\\Pr(h{=}1 \\to g{=}3),\\;\\Pr(h{=}2 \\to g{=}1),\\dots,\\Pr(h{=}3 \\to g{=}3)\\,].\n$$\nRound each probability to exactly $6$ decimal places.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list of the four case results, where each case result is itself a comma-separated list enclosed in square brackets. For example, your output must look like\n$$\n[\\,[a_1,a_2,\\dots,a_9],\\,[b_1,\\dots,b_9],\\,[c_1,\\dots,c_9],\\,[d_1,\\dots,d_9]\\,],\n$$\nwith each $a_i$, $b_i$, $c_i$, $d_i$ a float rounded to exactly $6$ decimal places and no additional text. No physical units are involved in this task. Angles are not involved. Percentages must be expressed as decimals, not with a percent sign.", "solution": "The problem is valid. It presents a well-defined, scientifically sound, and computationally feasible task in Bayesian statistical inference for network structure learning. All necessary data, model parameters, and computational procedures are provided without ambiguity or contradiction.\n\nThe objective is to compute the posterior edge inclusion probabilities for a small gene regulatory network modeled as a first-order Dynamic Bayesian Network (DBN). The network has $G=3$ genes, and the expression data is provided over $T=6$ time points. The solution involves a full Bayesian analysis, enumerating all possible network structures and calculating their posterior probabilities.\n\n### Step 1: Data Preparation\nThe provided time series data consists of $T=6$ measurements for $G=3$ genes. Let this data be represented by a matrix $\\mathbf{D} \\in \\mathbb{R}^{T \\times G}$. The DBN model relates the expression at time $t$ to the expression at time $t-1$. We therefore construct a predictor matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times G}$ and a response matrix $\\mathbf{Y} \\in \\mathbb{R}^{N \\times G}$, where $N=T-1=5$.\nThe matrix $\\mathbf{X}$ consists of the first $T-1=5$ rows of $\\mathbf{D}$, representing the gene states at times $t \\in \\{1, 2, 3, 4, 5\\}$.\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1.0 & 0.2 & -0.5 \\\\\n0.7 & 0.56 & -0.22 \\\\\n0.49 & 0.518 & -0.38 \\\\\n0.343 & 0.4004 & -0.3868 \\\\\n0.2401 & 0.29162 & -0.3176\n\\end{pmatrix}\n$$\nThe matrix $\\mathbf{Y}$ consists of the last $T-1=5$ rows of $\\mathbf{D}$, representing the gene states at times $t \\in \\{2, 3, 4, 5, 6\\}$. The $g$-th column of $\\mathbf{Y}$ serves as the target vector $\\mathbf{y}_g \\in \\mathbb{R}^N$ for gene $g$.\n$$\n\\mathbf{Y} = \\begin{pmatrix}\n0.7 & 0.56 & -0.22 \\\\\n0.49 & 0.518 & -0.38 \\\\\n0.343 & 0.4004 & -0.3868 \\\\\n0.2401 & 0.29162 & -0.3176 \\\\\n0.16807 & 0.207536 & -0.238492\n\\end{pmatrix}\n$$\n\n### Step 2: Bayesian Inference Framework\nThe core of the problem is to compute the posterior probability of each potential edge $(h \\to g)$. The model is specified such that, for a given target gene $g$, the inference for its parent set is independent of the other target genes. Therefore, we can perform the analysis for each target gene $g \\in \\{1, 2, 3\\}$ separately.\n\nFor each target gene $g$, there are $G=3$ potential parent genes (including self-regulation). This results in $2^G = 2^3 = 8$ possible parent sets $\\mathcal{S} \\subseteq \\{1, 2, 3\\}$. The task requires us to enumerate all these sets, calculate their posterior probabilities, and then marginalize to find the edge inclusion probabilities.\n\n### Step 3: Log Marginal Likelihood Calculation\nThe central quantity for Bayesian model comparison is the marginal likelihood, $p(\\mathbf{y}_g \\mid \\mathbf{X}, \\mathcal{S})$, obtained by integrating out the model parametersâ€”in this case, the regression weights $\\mathbf{w}_g$. The problem provides the result of this integration for the specified spike-and-slab prior on the weights. The marginal likelihood for a parent set $\\mathcal{S}$ is given by a multivariate normal distribution:\n$$\np(\\mathbf{y}_g \\mid \\mathbf{X}, \\mathcal{S}, \\sigma^2, \\tau^2) = \\mathcal{N}(\\mathbf{y}_g; \\mathbf{0}, \\boldsymbol{\\Sigma}_{\\mathcal{S}})\n$$\nwhere the covariance matrix is $\\boldsymbol{\\Sigma}_{\\mathcal{S}} = \\sigma^2 \\mathbf{I} + \\tau^2 \\mathbf{X}_{\\mathcal{S}} \\mathbf{X}_{\\mathcal{S}}^\\top$. Here, $\\mathbf{X}_{\\mathcal{S}}$ is the submatrix of $\\mathbf{X}$ containing only the columns corresponding to the parent genes in $\\mathcal{S}$.\n\nThe log of this marginal likelihood is:\n$$\n\\log p(\\mathbf{y}_g \\mid \\cdot) = -\\frac{N}{2} \\log(2\\pi) - \\frac{1}{2}\\log |\\boldsymbol{\\Sigma}_{\\mathcal{S}}| - \\frac{1}{2} \\mathbf{y}_g^\\top \\boldsymbol{\\Sigma}_{\\mathcal{S}}^{-1} \\mathbf{y}_g\n$$\nTo compute this stably and efficiently, we use the provided identities derived from the matrix determinant lemma and the Woodbury matrix identity. Let $|\\mathcal{S}| = k$, $\\mathbf{G}_{\\mathcal{S}} = \\mathbf{X}_{\\mathcal{S}}^\\top \\mathbf{X}_{\\mathcal{S}} \\in \\mathbb{R}^{k \\times k}$, and $\\mathbf{B}_{\\mathcal{S}} = \\mathbf{I}_k + \\frac{\\tau^2}{\\sigma^2} \\mathbf{G}_{\\mathcal{S}}$. The components of the log-likelihood are:\n$$\n\\log |\\boldsymbol{\\Sigma}_{\\mathcal{S}}| = N \\log \\sigma^2 + \\log |\\mathbf{B}_{\\mathcal{S}}|\n$$\n$$\n\\mathbf{y}_g^\\top \\boldsymbol{\\Sigma}_{\\mathcal{S}}^{-1} \\mathbf{y}_g = \\frac{1}{\\sigma^2}\\left(\\mathbf{y}_g^\\top \\mathbf{y}_g - \\frac{\\tau^2}{\\sigma^2} \\mathbf{y}_g^\\top \\mathbf{X}_{\\mathcal{S}} \\mathbf{B}_{\\mathcal{S}}^{-1} \\mathbf{X}_{\\mathcal{S}}^\\top \\mathbf{y}_g\\right)\n$$\nFor the special case where the parent set is empty ($\\mathcal{S} = \\emptyset$, $k=0$), $\\mathbf{X}_{\\mathcal{S}}$ is an empty matrix. The formulas simplify correctly: $\\log |\\mathbf{B}_{\\emptyset}| = \\log(1) = 0$ and the term involving $\\mathbf{X}_{\\mathcal{S}}$ in the quadratic form vanishes. Thus, for $\\mathcal{S}=\\emptyset$:\n$$\n\\log |\\boldsymbol{\\Sigma}_{\\emptyset}| = N \\log \\sigma^2 \\quad \\text{and} \\quad \\mathbf{y}_g^\\top \\boldsymbol{\\Sigma}_{\\emptyset}^{-1} \\mathbf{y}_g = \\frac{1}{\\sigma^2} \\mathbf{y}_g^\\top \\mathbf{y}_g\n$$\n\n### Step 4: Posterior Probability of Parent Sets\nUsing Bayes' rule, the posterior probability of a parent set $\\mathcal{S}$ is proportional to its prior probability times its marginal likelihood:\n$$\n\\Pr(\\mathcal{S} \\mid \\mathbf{y}_g, \\mathbf{X}, \\dots) \\propto p(\\mathcal{S}) \\cdot p(\\mathbf{y}_g \\mid \\mathbf{X}, \\mathcal{S}, \\dots)\n$$\nThe prior on the structure is defined by independent Bernoulli trials for each edge: $p(\\mathcal{S}) = p^{|\\mathcal{S}|} (1-p)^{G-|\\mathcal{S}|}$.\n\nTo maintain numerical stability, we work with logarithms. The unnormalized log-posterior score for each set $\\mathcal{S}$ is:\n$$\n\\log \\pi(\\mathcal{S}) = |\\mathcal{S}| \\log p + (G - |\\mathcal{S}|) \\log(1-p) + \\log p(\\mathbf{y}_g \\mid \\mathbf{X}, \\mathcal{S}, \\dots)\n$$\nWe compute this score for all $2^G=8$ parent sets. These scores are then normalized to sum to one by using the log-sum-exp trick. Let $\\{\\mathcal{S}_i\\}_{i=1}^{2^G}$ be the collection of all parent sets. The posterior probability of a specific set $\\mathcal{S}_j$ is:\n$$\n\\Pr(\\mathcal{S}_j \\mid \\cdot) = \\frac{\\exp(\\log \\pi(\\mathcal{S}_j))}{\\sum_{i=1}^{2^G} \\exp(\\log \\pi(\\mathcal{S}_i))}\n$$\nNumerically, this is computed as $\\exp(\\log \\pi(\\mathcal{S}_j) - C)$, where $C = \\log(\\sum_{i} \\exp(\\log \\pi(\\mathcal{S}_i)))$ is the log-normalizing constant, itself computed via log-sum-exp.\n\n### Step 5: Posterior Edge Inclusion Probabilities\nThe final quantity of interest is the posterior probability that a specific edge $(h \\to g)$ exists, which is represented by the indicator $z_{g,h}=1$. This is found by summing the posterior probabilities of all parent sets $\\mathcal{S}$ that include gene $h$ as a parent for gene $g$:\n$$\n\\Pr(z_{g,h}=1 \\mid \\mathbf{y}_g, \\mathbf{X}, \\dots) = \\sum_{\\mathcal{S} : h \\in \\mathcal{S}} \\Pr(\\mathcal{S} \\mid \\mathbf{y}_g, \\mathbf{X}, \\dots)\n$$\n\n### Step 6: Algorithmic Procedure\nThe overall algorithm proceeds as follows for each of the four hyperparameter test cases:\n1. Initialize a $G \\times G$ matrix, `posterior_edge_probs`, to store the results, where `posterior_edge_probs[h, g]` will hold $\\Pr(z_{g,h}=1 \\mid \\text{data})$.\n2. For each target gene $g$ from $1$ to $G$:\n    a. Create a list to store the unnormalized log-posterior scores for all $2^G$ parent sets.\n    b. For each parent set $\\mathcal{S} \\subseteq \\{1, \\dots, G\\}$:\n        i. Extract the corresponding predictor submatrix $\\mathbf{X}_{\\mathcal{S}}$.\n        ii. Calculate the log marginal likelihood $\\log p(\\mathbf{y}_g \\mid \\mathbf{X}, \\mathcal{S}, \\dots)$ using the stable formulas.\n        iii. Calculate the log-prior $\\log p(\\mathcal{S})$.\n        iv. Sum these to get the unnormalized log-posterior $\\log \\pi(\\mathcal{S})$ and add it to the list.\n    c. Normalize the list of log-posteriors to obtain the posterior probabilities $\\Pr(\\mathcal{S} \\mid \\text{data})$ for all $\\mathcal{S}$.\n    d. For each potential source gene $h$ from $1$ to $G$:\n        i. Sum the probabilities $\\Pr(\\mathcal{S} \\mid \\text{data})$ for all sets $\\mathcal{S}$ containing $h$.\n        ii. Store this sum in `posterior_edge_probs[h-1, g-1]`.\n3. After iterating through all target genes, the `posterior_edge_probs` matrix is complete. Flatten this matrix in row-major order to conform to the specified output format: source index $h$ as the outer loop, target index $g$ as the inner.\n4. Round each probability to six decimal places and format the output string as required.\n\nThis procedure provides an exact solution to the Bayesian inference problem for the given small-scale network.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Computes posterior edge inclusion probabilities for a Dynamic Bayesian Network.\n    \"\"\"\n    # Define the dataset as provided in the problem statement.\n    # G = 3 genes, T = 6 time points.\n    G = 3\n    T = 6\n    N = T - 1\n\n    data = np.array([\n        [1.0, 0.2, -0.5],\n        [0.7, 0.56, -0.22],\n        [0.49, 0.518, -0.38],\n        [0.343, 0.4004, -0.3868],\n        [0.2401, 0.29162, -0.3176],\n        [0.16807, 0.207536, -0.238492]\n    ])\n\n    # Construct predictor (X) and response (Y) matrices\n    X = data[:-1, :]  # Shape (N, G) = (5, 3)\n    Y = data[1:, :]   # Shape (N, G) = (5, 3)\n\n    # Define the test cases (hyperparameter settings)\n    test_cases = [\n        {'p': 0.3, 'sigma2': 0.01, 'tau2': 1.0},\n        {'p': 0.05, 'sigma2': 0.02, 'tau2': 1.0},\n        {'p': 0.5, 'sigma2': 0.1, 'tau2': 0.1},\n        {'p': 0.5, 'sigma2': 0.0001, 'tau2': 10.0},\n    ]\n\n    # Generate all 2^G possible parent sets for G=3 genes (indices 0, 1, 2)\n    all_parent_sets = []\n    for i in range(2**G):\n        s = [j for j in range(G) if (i >> j)  1]\n        all_parent_sets.append(s)\n\n    all_case_results = []\n\n    for case in test_cases:\n        p, sigma2, tau2 = case['p'], case['sigma2'], case['tau2']\n        \n        # Matrix to store Pr(h -> g) at posterior_edge_probs[h, g]\n        posterior_edge_probs = np.zeros((G, G))\n\n        # The inference for each target gene is independent. Loop over targets g.\n        for g_idx in range(G):\n            y_g = Y[:, g_idx]\n            \n            log_posterior_unnorm_S = np.zeros(2**G)\n\n            # Enumerate all parent sets S\n            for i, S in enumerate(all_parent_sets):\n                k = len(S)\n\n                # 1. Calculate log prior for parent set S\n                log_prior_S = k * np.log(p) + (G - k) * np.log(1 - p)\n\n                # 2. Calculate log marginal likelihood log p(y_g | X, S, ...)\n                if k == 0:\n                    log_det_Sigma = N * np.log(sigma2)\n                    y_Sigma_inv_y = (y_g @ y_g) / sigma2\n                else:\n                    X_S = X[:, S]\n                    \n                    G_S = X_S.T @ X_S\n                    B_S = np.identity(k) + (tau2 / sigma2) * G_S\n                    \n                    sign, log_det_B_S = np.linalg.slogdet(B_S)\n                    \n                    # B_S is positive definite, so its determinant must be positive.\n                    # This check is for robustness.\n                    if sign = 0:\n                        log_det_B_S = -np.inf\n\n                    log_det_Sigma = N * np.log(sigma2) + log_det_B_S\n\n                    B_S_inv = np.linalg.inv(B_S)\n                    y_X_S = y_g.T @ X_S\n                    quad_term = y_X_S @ B_S_inv @ y_X_S.T\n\n                    y_Sigma_inv_y = (1 / sigma2) * (y_g.T @ y_g - (tau2 / sigma2) * quad_term)\n\n                log_marginal_likelihood = -0.5 * N * np.log(2 * np.pi) - 0.5 * log_det_Sigma - 0.5 * y_Sigma_inv_y\n                \n                # 3. Combine for unnormalized log posterior\n                log_posterior_unnorm_S[i] = log_prior_S + log_marginal_likelihood\n            \n            # 4. Normalize posterior probabilities for all sets S using log-sum-exp\n            log_norm_const = logsumexp(log_posterior_unnorm_S)\n            posterior_probs_S = np.exp(log_posterior_unnorm_S - log_norm_const)\n            \n            # 5. Compute posterior edge inclusion probabilities for target g\n            # Pr(h -> g) = sum over S where h is in S of Pr(S | data)\n            for h_idx in range(G):\n                prob_h_to_g = 0.0\n                for i, S in enumerate(all_parent_sets):\n                    if h_idx in S:\n                        prob_h_to_g += posterior_probs_S[i]\n                posterior_edge_probs[h_idx, g_idx] = prob_h_to_g\n\n        # Flatten the GxG matrix in row-major order ('C' order)\n        # to match the required format [Pr(1->1),Pr(1->2),Pr(1->3),Pr(2->1),...]\n        flattened_probs = posterior_edge_probs.flatten('C')\n        all_case_results.append(flattened_probs)\n\n    # Format the final output string\n    case_strings = []\n    for result_list in all_case_results:\n        # Format each probability to 6 decimal places\n        formatted_probs = [f\"{p:.6f}\" for p in result_list]\n        case_strings.append(f\"[{','.join(formatted_probs)}]\")\n    \n    # Print the final result in the exact required format.\n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```", "id": "4278329"}]}