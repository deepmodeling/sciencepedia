{"hands_on_practices": [{"introduction": "Before attempting to correct for experimental variability, it is essential to first characterize its magnitude and structure. This exercise introduces the Bland-Altman analysis, a cornerstone method for assessing the agreement between two sets of measurements, such as technical replicates. By examining the relationship between the difference and the mean of paired measurements, you can diagnose systematic deviations and determine if the error is constant or proportional to the signal's intensity, a critical step in choosing an appropriate normalization or transformation strategy [@problem_id:4339892].", "problem": "A cytokine quantification assay in plasma, based on a multiplex bead immunoassay, was performed in duplicate for $8$ specimens under identical pre-analytical and analytical conditions. Measurements were normalized post hoc using an internal standard to remove systematic plate effects. Let the two technical replicates for specimen $i$ be denoted by $y_{i1}$ and $y_{i2}$, both in nanograms per milliliter (ng/mL). The paired data are:\n- Specimen $1$: $(y_{11}, y_{12}) = (50, 50)$\n- Specimen $2$: $(y_{21}, y_{22}) = (99, 101)$\n- Specimen $3$: $(y_{31}, y_{32}) = (148, 152)$\n- Specimen $4$: $(y_{41}, y_{42}) = (198.5, 201.5)$\n- Specimen $5$: $(y_{51}, y_{52}) = (247.5, 252.5)$\n- Specimen $6$: $(y_{61}, y_{62}) = (296.5, 303.5)$\n- Specimen $7$: $(y_{71}, y_{72}) = (347, 353)$\n- Specimen $8$: $(y_{81}, y_{82}) = (396, 404)$\n\nDefine, for each specimen $i$, the within-pair mean $m_i = (y_{i1}+y_{i2})/2$ and difference $d_i = y_{i2} - y_{i1}$. Assume the differences $\\{d_i\\}$ are approximately normally distributed with mean $\\bar{d}$ and standard deviation $s_d$ on the original scale, and that any proportional bias manifests as a linear dependence of $d_i$ on $m_i$.\n\nTasks:\n- Compute the mean difference $\\bar{d}$ and the Bland–Altman limits of agreement on the difference scale, defined by the empirical mean and standard deviation of $\\{d_i\\}$, under the normal approximation to $95\\%$ coverage. Express the limits in nanograms per milliliter (ng/mL).\n- Test for proportional bias by fitting the linear regression $d_i = \\alpha + \\beta m_i + \\varepsilon_i$ with an intercept, where $\\varepsilon_i$ are independent residuals with constant variance. Report the estimated slope $\\hat{\\beta}$ and use the classical $t$-statistic for $\\beta$ with $n-2$ degrees of freedom to judge whether proportional bias is present at significance level $\\alpha = 0.05$.\n- If proportional bias is present and consistent with multiplicative error, suggest a variance-stabilizing Box–Cox transformation (Box–Cox) of the form $y^{(\\lambda)} = \\begin{cases}\\frac{y^{\\lambda}-1}{\\lambda},  \\lambda \\neq 0\\\\ \\ln(y),  \\lambda=0\\end{cases}$ and report the recommended exponent $\\lambda$ that would remove proportional scaling under this assumption.\n\nProvide the final answer as a single row vector containing, in order, the quantities $\\bar{d}$, the lower limit of agreement, the upper limit of agreement, $\\hat{\\beta}$, and the recommended $\\lambda$. Use exact analytic expressions without rounding and do not include units in the final row vector. For reporting purposes, the limits of agreement are in nanograms per milliliter (ng/mL).", "solution": "The user wants to solve a biostatistics problem involving method comparison analysis.\n\n### Step 1: Extract Givens\n-   Number of specimens, $n=8$.\n-   Measurements are in duplicate, $(y_{i1}, y_{i2})$, for $i=1, \\dots, 8$.\n-   Specimen 1: $(y_{11}, y_{12}) = (50, 50)$\n-   Specimen 2: $(y_{21}, y_{22}) = (99, 101)$\n-   Specimen 3: $(y_{31}, y_{32}) = (148, 152)$\n-   Specimen 4: $(y_{41}, y_{42}) = (198.5, 201.5)$\n-   Specimen 5: $(y_{51}, y_{52}) = (247.5, 252.5)$\n-   Specimen 6: $(y_{61}, y_{62}) = (296.5, 303.5)$\n-   Specimen 7: $(y_{71}, y_{72}) = (347, 353)$\n-   Specimen 8: $(y_{81}, y_{82}) = (396, 404)$\n-   Definition of within-pair mean: $m_i = (y_{i1}+y_{i2})/2$.\n-   Definition of within-pair difference: $d_i = y_{i2} - y_{i1}$.\n-   Assumption: The set of differences $\\{d_i\\}$ is approximately normally distributed.\n-   Assumption: Proportional bias is modeled by the linear regression $d_i = \\alpha + \\beta m_i + \\varepsilon_i$.\n-   Task 1: Compute the mean difference $\\bar{d}$ and the $95\\%$ Bland–Altman limits of agreement.\n-   Task 2: Fit the linear regression for proportional bias, report the estimated slope $\\hat{\\beta}$, and perform a hypothesis test for $\\beta=0$ at significance level $\\alpha_{sig} = 0.05$.\n-   Task 3: If proportional bias is found, recommend a Box–Cox transformation exponent $\\lambda$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem utilizes standard, well-established biostatistical methods (Bland-Altman analysis, linear regression for proportional bias, Box-Cox transformation) that are fundamental to assessing assay performance, replication, and variability in systems biomedicine. The context and data are realistic for a cytokine immunoassay.\n-   **Well-Posed**: The data are complete, the variables are clearly defined, and the tasks are specific computational exercises. A unique, meaningful solution exists for each requested quantity.\n-   **Objective**: The problem is stated in precise, quantitative terms, free of subjectivity or non-scientific claims.\n\nThe problem does not exhibit any flaws. It is scientifically sound, well-posed, objective, complete, and relevant.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution\n\nFirst, we compute the within-pair means, $m_i$, and differences, $d_i$, for each of the $n=8$ specimens.\n-   $i=1$: $m_1 = (50+50)/2 = 50$, $d_1 = 50-50 = 0$\n-   $i=2$: $m_2 = (99+101)/2 = 100$, $d_2 = 101-99 = 2$\n-   $i=3$: $m_3 = (148+152)/2 = 150$, $d_3 = 152-148 = 4$\n-   $i=4$: $m_4 = (198.5+201.5)/2 = 200$, $d_4 = 201.5-198.5 = 3$\n-   $i=5$: $m_5 = (247.5+252.5)/2 = 250$, $d_5 = 252.5-247.5 = 5$\n-   $i=6$: $m_6 = (296.5+303.5)/2 = 300$, $d_6 = 303.5-296.5 = 7$\n-   $i=7$: $m_7 = (347+353)/2 = 350$, $d_7 = 353-347 = 6$\n-   $i=8$: $m_8 = (396+404)/2 = 400$, $d_8 = 404-396 = 8$\n\nThe set of means is $\\{m_i\\} = \\{50, 100, 150, 200, 250, 300, 350, 400\\}$.\nThe set of differences is $\\{d_i\\} = \\{0, 2, 4, 3, 5, 7, 6, 8\\}$.\n\n**Task 1: Bland–Altman Limits of Agreement**\n\nWe compute the mean difference, $\\bar{d}$, and the standard deviation of the differences, $s_d$.\n$$ \\bar{d} = \\frac{1}{n} \\sum_{i=1}^{n} d_i = \\frac{0+2+4+3+5+7+6+8}{8} = \\frac{35}{8} $$\nFor the sample variance $s_d^2$:\n$$ \\sum_{i=1}^{n} d_i^2 = 0^2 + 2^2 + 4^2 + 3^2 + 5^2 + 7^2 + 6^2 + 8^2 = 0+4+16+9+25+49+36+64 = 203 $$\n$$ s_d^2 = \\frac{1}{n-1} \\left( \\sum_{i=1}^{n} d_i^2 - n \\bar{d}^2 \\right) = \\frac{1}{7} \\left( 203 - 8 \\left(\\frac{35}{8}\\right)^2 \\right) = \\frac{1}{7} \\left( 203 - \\frac{1225}{8} \\right) = \\frac{1}{7} \\left( \\frac{1624 - 1225}{8} \\right) = \\frac{399}{56} = \\frac{57}{8} $$\nThe standard deviation is:\n$$ s_d = \\sqrt{\\frac{57}{8}} = \\frac{\\sqrt{57}}{2\\sqrt{2}} = \\frac{\\sqrt{114}}{4} $$\nThe $95\\%$ limits of agreement (LoA) are defined as $\\bar{d} \\pm z_{0.975} s_d$. For the normal approximation, the z-score for $95\\%$ coverage is $z_{0.975} \\approx 1.96$. We use the exact fractional form $1.96 = \\frac{196}{100} = \\frac{49}{25}$.\nThe limits are:\n$$ \\text{LoA} = \\bar{d} \\pm \\frac{49}{25} s_d = \\frac{35}{8} \\pm \\frac{49}{25} \\frac{\\sqrt{114}}{4} = \\frac{35}{8} \\pm \\frac{49\\sqrt{114}}{100} $$\nLower LoA: $\\frac{35}{8} - \\frac{49\\sqrt{114}}{100}$\nUpper LoA: $\\frac{35}{8} + \\frac{49\\sqrt{114}}{100}$\n\n**Task 2: Test for Proportional Bias**\n\nWe fit the linear model $d_i = \\alpha + \\beta m_i + \\varepsilon_i$. The estimate for the slope $\\beta$ is $\\hat{\\beta} = \\frac{S_{md}}{S_{mm}}$.\nFirst, we compute the mean of the $m_i$ values:\n$$ \\bar{m} = \\frac{1}{n} \\sum_{i=1}^{n} m_i = \\frac{50+100+150+200+250+300+350+400}{8} = \\frac{1800}{8} = 225 $$\nNext, we compute the sum of squares $S_{mm}$ and sum of cross-products $S_{md}$:\n$$ S_{mm} = \\sum_{i=1}^{n} (m_i - \\bar{m})^2 = \\sum_{i=1}^{n} m_i^2 - n \\bar{m}^2 $$\n$$ \\sum_{i=1}^{n} m_i^2 = 50^2 + 100^2 + \\dots + 400^2 = 510000 $$\n$$ S_{mm} = 510000 - 8(225)^2 = 510000 - 8(50625) = 510000 - 405000 = 105000 $$\n$$ S_{md} = \\sum_{i=1}^{n} (m_i - \\bar{m})(d_i - \\bar{d}) = \\sum_{i=1}^{n} m_i d_i - n \\bar{m} \\bar{d} $$\n$$ \\sum_{i=1}^{n} m_i d_i = 50(0) + 100(2) + 150(4) + 200(3) + 250(5) + 300(7) + 350(6) + 400(8) = 0 + 200 + 600 + 600 + 1250 + 2100 + 2100 + 3200 = 10050 $$\n$$ S_{md} = 10050 - 8(225)\\left(\\frac{35}{8}\\right) = 10050 - 225(35) = 10050 - 7875 = 2175 $$\nThe estimated slope is:\n$$ \\hat{\\beta} = \\frac{S_{md}}{S_{mm}} = \\frac{2175}{105000} = \\frac{435}{21000} = \\frac{87}{4200} = \\frac{29}{1400} $$\nTo test the hypothesis $H_0: \\beta=0$, we compute the $t$-statistic, $t = \\frac{\\hat{\\beta}}{SE(\\hat{\\beta})}$, which follows a $t$-distribution with $n-2=6$ degrees of freedom. The standard error of the slope requires the sum of squared errors, $SSE$.\n$$ S_{dd} = \\sum_{i=1}^{n} (d_i - \\bar{d})^2 = (n-1)s_d^2 = 7 \\left(\\frac{57}{8}\\right) = \\frac{399}{8} $$\n$$ SSE = S_{dd} - \\frac{S_{md}^2}{S_{mm}} = \\frac{399}{8} - \\frac{2175^2}{105000} = \\frac{399}{8} - \\frac{4730625}{105000} = \\frac{399}{8} - \\frac{2523}{56} = \\frac{2793 - 2523}{56} = \\frac{270}{56} = \\frac{135}{28} $$\nThe variance of the residuals is $s_e^2 = \\frac{SSE}{n-2} = \\frac{135/28}{6} = \\frac{135}{168} = \\frac{45}{56}$.\nThe standard error of the slope is $SE(\\hat{\\beta}) = \\sqrt{\\frac{s_e^2}{S_{mm}}} = \\sqrt{\\frac{45/56}{105000}}$.\nThe $t$-statistic is:\n$$ t = \\frac{\\hat{\\beta}}{SE(\\hat{\\beta})} = \\frac{29/1400}{\\sqrt{\\frac{45/56}{105000}}} = \\frac{S_{md}/\\sqrt{S_{mm}}}{\\sqrt{SSE/(n-2)}} = \\frac{S_{md}\\sqrt{n-2}}{\\sqrt{S_{mm}S_{dd} - S_{md}^2}} $$\n$$ t = \\frac{2175 \\sqrt{6}}{\\sqrt{105000 \\left(\\frac{399}{8}\\right) - 2175^2}} = \\frac{2175 \\sqrt{6}}{\\sqrt{5236875 - 4730625}} = \\frac{2175 \\sqrt{6}}{\\sqrt{506250}} = \\frac{2175 \\sqrt{6}}{\\sqrt{225^2 \\times 10}} = \\frac{2175 \\sqrt{6}}{225 \\sqrt{10}} $$\n$$ t = \\frac{29 \\times 75 \\sqrt{6}}{3 \\times 75 \\sqrt{10}} = \\frac{29\\sqrt{6}}{3\\sqrt{10}} = \\frac{29\\sqrt{60}}{30} = \\frac{29 \\times 2\\sqrt{15}}{30} = \\frac{29\\sqrt{15}}{15} $$\nNumerically, $|t| \\approx \\frac{29 \\times 3.873}{15} \\approx 7.48$. The critical value from the $t$-distribution for a two-tailed test with $n-2=6$ degrees of freedom at $\\alpha_{sig}=0.05$ is $t_{0.975, 6} = 2.447$. Since $|t| \\approx 7.48  2.447$, we reject the null hypothesis $H_0: \\beta=0$. There is statistically significant evidence of proportional bias.\n\n**Task 3: Box–Cox Transformation**\n\nThe presence of significant proportional bias, where the difference between measurements ($d_i$) is linearly dependent on their mean magnitude ($m_i$), is characteristic of a measurement error that is multiplicative rather than additive. A multiplicative error model often takes the form $y_{ij} = \\mu_i (1+\\epsilon_{ij})$ or $y_{ij} = \\mu_i \\exp(\\eta_{ij})$, where the error term is scaled by the true value $\\mu_i$. In such cases, the standard deviation of the measurements is proportional to the mean, a condition known as heteroscedasticity.\n\nThe purpose of a variance-stabilizing transformation is to find a function $f(y)$ such that the variance of $f(y)$ is approximately constant. For a measurement system where the standard deviation is proportional to the mean (a constant coefficient of variation), the appropriate variance-stabilizing transformation is the logarithm.\nLet's confirm this in the context of the Box–Cox family of transformations, $y^{(\\lambda)} = \\frac{y^{\\lambda}-1}{\\lambda}$. The logarithmic transformation corresponds to the limiting case where $\\lambda \\to 0$.\nThe problem states to assume the proportional bias is consistent with multiplicative error. This implies that the standard deviation of the error is proportional to the mean, $\\sigma_y \\propto \\mu_y$. This specific relationship, $\\sigma_y \\propto \\mu_y^1$, points to a power $\\theta=1$ in the variance model $\\text{Var}(y) \\propto \\mu^{2\\theta}$. The Box-Cox parameter that stabilizes this variance is $\\lambda = 1-\\theta = 1-1 = 0$.\nThus, the recommended transformation is the natural logarithm, which corresponds to the exponent $\\lambda=0$.\n\nFinal quantities to report are $\\bar{d}$, the lower LoA, the upper LoA, $\\hat{\\beta}$, and $\\lambda$.\n1.  $\\bar{d} = \\frac{35}{8}$\n2.  Lower LoA = $\\frac{35}{8} - \\frac{49\\sqrt{114}}{100}$\n3.  Upper LoA = $\\frac{35}{8} + \\frac{49\\sqrt{114}}{100}$\n4.  $\\hat{\\beta} = \\frac{29}{1400}$\n5.  $\\lambda = 0$", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{35}{8}  \\frac{35}{8} - \\frac{49\\sqrt{114}}{100}  \\frac{35}{8} + \\frac{49\\sqrt{114}}{100}  \\frac{29}{1400}  0 \\end{pmatrix} } $$", "id": "4339892"}, {"introduction": "Statistical normalization can correct for many sources of unwanted variation, but it cannot overcome fundamental flaws in experimental design. This exercise demonstrates the critical concept of non-identifiability, where different underlying effects are mathematically inseparable due to confounding. By constructing a scenario where batch effects and biological condition effects are perfectly confounded, you will prove that no post-hoc normalization can uniquely determine the true contribution of each, highlighting the irreplaceable importance of randomized and balanced designs [@problem_id:4339905].", "problem": "Consider a simplified gene expression measurement system typical of high-throughput assays such as RNA sequencing (RNA-seq), in which each observed intensity is modeled as a product of independent factors representing experimental batch and biological condition. For two genes indexed by $g \\in \\{1,2\\}$, two experimental batches indexed by $b \\in \\{1,2\\}$, and two biological conditions indexed by $c \\in \\{\\mathrm{A}, \\mathrm{B}\\}$, suppose the observed intensity in batch $b$ arises from condition $c(b)$ and is given by\n$$\nx_{g,b} \\;=\\; s_{b} \\, t_{c(b)} \\, r_{g},\n$$\nwhere $s_{b}  0$ is the batch effect, $t_{c}  0$ is the condition effect, and $r_{g}  0$ is the gene-specific baseline abundance. Assume that batch $1$ contains only condition $\\mathrm{A}$ and batch $2$ contains only condition $\\mathrm{B}$, so $c(1)=\\mathrm{A}$ and $c(2)=\\mathrm{B}$. There is one sample per batch, and technical noise is negligible for the purpose of this construction.\n\nA common normalization procedure divides each batch’s measurements by the per-batch geometric mean across genes. Define the per-batch geometric mean\n$$\nM_{b} \\;=\\; \\left(\\prod_{g=1}^{2} x_{g,b}\\right)^{1/2} \\;=\\; \\left( s_{b} \\, t_{c(b)} \\right) \\, \\left( r_{1} r_{2} \\right)^{1/2},\n$$\nand the normalized intensities\n$$\ny_{g,b} \\;=\\; \\frac{x_{g,b}}{M_{b}}.\n$$\n\nLet the gene baselines be $r_{1}=1$ and $r_{2}=9$, and let the initial batch and condition effects be $s_{1}=3$, $s_{2}=\\frac{1}{2}$, $t_{\\mathrm{A}}=2$, and $t_{\\mathrm{B}}=1$. You are asked to construct an explicit counterexample demonstrating nonidentifiability under this normalization: show that there exists a family of alternative parameterizations $\\{s_{b}', t_{c}'\\}$ producing identical normalized data $\\{y_{g,b}\\}$ despite different decompositions of $s_{b}$ and $t_{c}$. Constrain the family by requiring that the alternative parameterization is of the form\n$$\ns_{b}' \\;=\\; \\lambda \\, s_{b}, \\qquad t_{c}' \\;=\\; \\frac{t_{c}}{\\lambda},\n$$\nfor a fixed $\\lambda0$ with $\\lambda \\neq 1$, and impose the additional calibration constraint that the geometric mean of the alternative batch effects equals unity:\n$$\n\\left( s_{1}' \\, s_{2}' \\right)^{1/2} \\;=\\; 1.\n$$\n\nStarting only from the multiplicative measurement model and the definition of geometric-mean normalization as given above, derive the condition under which the normalized data are invariant to the reparameterization $\\{s_{b}', t_{c}'\\}$, and determine the exact value of $\\lambda$ that satisfies the calibration constraint. Express the final value of $\\lambda$ as a simplified exact expression with no units. No rounding is required. Your final answer must be a single real-valued number or a single closed-form analytical expression.", "solution": "The user has provided a problem to be solved, following a strict set of instructions.\n\n### Step 1: Problem Validation\n\nI will begin by validating the problem statement as per the required protocol.\n\n#### Extraction of Givens\n-   **Model for observed intensity:** $x_{g,b} = s_{b} \\, t_{c(b)} \\, r_{g}$ for gene $g \\in \\{1,2\\}$, batch $b \\in \\{1,2\\}$, and condition $c \\in \\{\\mathrm{A}, \\mathrm{B}\\}$.\n-   **Parameter definitions:** $s_{b}  0$ is the batch effect, $t_{c}  0$ is the condition effect, and $r_{g}  0$ is the gene-specific baseline abundance.\n-   **Experimental design:** Batch $1$ is condition $\\mathrm{A}$, so $c(1)=\\mathrm{A}$. Batch $2$ is condition $\\mathrm{B}$, so $c(2)=\\mathrm{B}$. This is a perfectly confounded design.\n-   **Per-batch geometric mean:** $M_{b} = \\left(\\prod_{g=1}^{2} x_{g,b}\\right)^{1/2} = \\left( s_{b} \\, t_{c(b)} \\right) \\, \\left( r_{1} r_{2} \\right)^{1/2}$.\n-   **Normalized intensity:** $y_{g,b} = \\frac{x_{g,b}}{M_{b}}$.\n-   **Specific parameter values:** $r_{1}=1$, $r_{2}=9$, $s_{1}=3$, $s_{2}=\\frac{1}{2}$, $t_{\\mathrm{A}}=2$, and $t_{\\mathrm{B}}=1$.\n-   **Alternative parameterization:** A family of parameters $\\{s_{b}', t_{c}'\\}$ is defined by $s_{b}' = \\lambda \\, s_{b}$ and $t_{c}' = \\frac{t_{c}}{\\lambda}$ for a constant $\\lambda  0$ and $\\lambda \\neq 1$.\n-   **Calibration constraint:** The alternative batch effects must satisfy $\\left( s_{1}' \\, s_{2}' \\right)^{1/2} = 1$.\n-   **Objective:** Derive the condition for invariance of normalized data $\\{y_{g,b}\\}$ under the reparameterization and find the specific value of $\\lambda$ that satisfies the calibration constraint.\n\n#### Validation Using Extracted Givens\n1.  **Scientifically Grounded:** The problem is scientifically grounded. The multiplicative model for gene expression is a standard simplification used in bioinformatics. Geometric mean normalization is a common technique for count data (like RNA-seq) or intensity data (like microarrays). The core concept demonstrated—non-identifiability due to a confounded experimental design—is a critical and fundamental topic in statistics and experimental biology.\n2.  **Well-Posed:** The problem is well-posed. It provides a clear model, all necessary definitions, and a specific set of constraints. The objective is to derive a condition and calculate a value, which is achievable with the given information. A unique solution for $\\lambda$ exists.\n3.  **Objective:** The problem is stated in precise, objective mathematical language, free of any subjective or ambiguous terms.\n4.  **Flaw Checklist:**\n    -   No scientific or factual unsoundness.\n    -   The problem is formalizable and directly relevant to its stated topic.\n    -   The setup is complete and not contradictory.\n    -   The conditions and data are mathematically and conceptually plausible for such a theoretical exercise.\n    -   The problem is well-structured and leads to a unique, meaningful solution.\n    -   The problem illustrates a key concept and is not trivial or pseudo-profound.\n    -   The derivation is mathematically verifiable.\n\n#### Verdict and Action\nThe problem is **valid**. I will proceed with the solution.\n\n### Step 2: Solution Derivation\n\nThe problem asks for two things: first, to derive the condition under which the normalized data are invariant to a specific reparameterization, and second, to determine the value of the reparameterization constant $\\lambda$ given a calibration constraint.\n\nLet's begin by analyzing the structure of the normalized intensities, $y_{g,b}$. The observed intensity for gene $g$ in batch $b$ is given by the model:\n$$\nx_{g,b} = s_{b} \\, t_{c(b)} \\, r_{g}\n$$\nThe per-batch geometric mean, $M_b$, is calculated across the two genes ($g=1, 2$) within batch $b$:\n$$\nM_{b} = \\left( x_{1,b} \\cdot x_{2,b} \\right)^{1/2} = \\left( (s_{b} t_{c(b)} r_{1}) \\cdot (s_{b} t_{c(b)} r_{2}) \\right)^{1/2} = \\left( (s_{b} t_{c(b)})^2 r_{1} r_{2} \\right)^{1/2}\n$$\nSince all parameters are positive, we can simplify this to:\n$$\nM_{b} = s_{b} \\, t_{c(b)} \\, (r_{1} r_{2})^{1/2}\n$$\nThis expression matches the one provided in the problem statement. Now, we define the normalized intensity $y_{g,b}$ as the ratio of the observed intensity to the per-batch geometric mean:\n$$\ny_{g,b} = \\frac{x_{g,b}}{M_{b}} = \\frac{s_{b} \\, t_{c(b)} \\, r_{g}}{s_{b} \\, t_{c(b)} \\, (r_{1} r_{2})^{1/2}} = \\frac{r_{g}}{(r_{1} r_{2})^{1/2}}\n$$\nThis result is crucial. It shows that the normalized intensity $y_{g,b}$ for a given gene $g$ is constant across all batches and conditions. Its value depends only on the gene-specific baseline abundances, $r_1$ and $r_2$. The batch effect $s_b$ and the condition effect $t_c$ are completely eliminated from the normalized data by this procedure.\n\nNext, we consider the alternative parameterization $\\{s_{b}', t_{c}'\\}$ where $s_b' = \\lambda s_b$ and $t_c' = t_c / \\lambda$. Let's compute the new normalized intensities, $y_{g,b}'$.\nThe new observed intensities are:\n$$\nx_{g,b}' = s_{b}' \\, t_{c(b)}' \\, r_{g}\n$$\nThe new per-batch geometric mean is:\n$$\nM_{b}' = s_{b}' \\, t_{c(b)}' \\, (r_{1} r_{2})^{1/2}\n$$\nThe new normalized intensity is therefore:\n$$\ny_{g,b}' = \\frac{x_{g,b}'}{M_{b}'} = \\frac{s_{b}' \\, t_{c(b)}' \\, r_{g}}{s_{b}' \\, t_{c(b)}' \\, (r_{1} r_{2})^{1/2}} = \\frac{r_{g}}{(r_{1} r_{2})^{1/2}}\n$$\nBy comparing the expressions for $y_{g,b}$ and $y_{g,b}'$, we see that they are identical:\n$$\ny_{g,b}' = y_{g,b}\n$$\nThis equality holds for any set of positive parameters $s_b, t_c, r_g$ and for any choice of $\\lambda  0$. Therefore, the condition under which the normalized data are invariant is inherent to the structure of the model and the geometric-mean normalization method. The normalized data are always invariant under any reparameterization of $s_b$ and $t_c$ because these effects are cancelled out during normalization. The problem of non-identifiability is made manifest by this normalization: the resulting data $y_{g,b}$ contain no information whatsoever about $s_b$ and $t_c$.\n\nNow we must find the specific value of $\\lambda$ that satisfies the additional calibration constraint. The constraint is given as:\n$$\n\\left( s_{1}' \\, s_{2}' \\right)^{1/2} = 1\n$$\nWe substitute the expressions for the alternative batch effects, $s_1' = \\lambda s_1$ and $s_2' = \\lambda s_2$, into the constraint equation:\n$$\n\\left( (\\lambda s_{1}) \\cdot (\\lambda s_{2}) \\right)^{1/2} = 1\n$$\n$$\n\\left( \\lambda^2 s_{1} s_{2} \\right)^{1/2} = 1\n$$\nSince $\\lambda  0$, we can take the square root of $\\lambda^2$ to get $\\lambda$:\n$$\n\\lambda \\left( s_{1} s_{2} \\right)^{1/2} = 1\n$$\nSolving for $\\lambda$ yields:\n$$\n\\lambda = \\frac{1}{\\left( s_{1} s_{2} \\right)^{1/2}} = (s_{1} s_{2})^{-1/2}\n$$\nThe problem provides the specific initial values for the batch effects: $s_1 = 3$ and $s_2 = \\frac{1}{2}$. We substitute these values into our expression for $\\lambda$:\n$$\n\\lambda = \\left( 3 \\cdot \\frac{1}{2} \\right)^{-1/2} = \\left( \\frac{3}{2} \\right)^{-1/2}\n$$\nThis expression can be simplified:\n$$\n\\lambda = \\left( \\frac{2}{3} \\right)^{1/2} = \\sqrt{\\frac{2}{3}}\n$$\nThis is the exact value of $\\lambda$ that satisfies the calibration constraint. It is a positive number and is not equal to $1$, as required.", "answer": "$$\n\\boxed{\\sqrt{\\frac{2}{3}}}\n$$", "id": "4339905"}, {"introduction": "When technical variability causes entire samples to have different intensity distributions, a powerful strategy is to force them to be the same. This exercise provides a hands-on walkthrough of quantile normalization, a non-parametric method that replaces each data point with the average of its rank across all samples, thereby perfectly aligning their empirical distributions. By executing the algorithm step-by-step on a small matrix, you will gain a deep, mechanical understanding of how this widely-used \"black box\" procedure harmonizes high-throughput data [@problem_id:4339948].", "problem": "In a high-throughput gene expression assay, three biological replicates are profiled across four genes, yielding a $4\\times 3$ matrix of raw intensities. Experimental variability across replicates arises from a mixture of biological and technical sources. A standard approach in systems biomedicine to harmonize marginal intensity distributions across replicates is quantile normalization, which operates on the empirical cumulative distribution functions (Empirical Cumulative Distribution Function (ECDF)) of each sample. Consider the raw intensity matrix\n$$\nX \\;=\\;\n\\begin{pmatrix}\n10  16  22 \\\\\n3  8  26 \\\\\n20  23  13 \\\\\n6  12  21\n\\end{pmatrix},\n$$\nwhere rows index genes ($i\\in\\{1,2,3,4\\}$) and columns index replicates ($j\\in\\{1,2,3\\}$). The quantile normalization procedure is defined as follows: for each sample $j$, sort the intensities to obtain the ordered statistics $x_{(1)j}\\leq x_{(2)j}\\leq x_{(3)j}\\leq x_{(4)j}$. Then define a reference distribution by the rank-wise average $q_{r}=\\frac{1}{3}\\sum_{j=1}^{3} x_{(r)j}$ for $r\\in\\{1,2,3,4\\}$. Finally, for each entry $x_{ij}$, assign the normalized value $y_{ij}=q_{\\mathrm{rank}(x_{ij},j)}$, where $\\mathrm{rank}(x_{ij},j)$ is the rank of $x_{ij}$ within column $j$. This mapping preserves the within-column ranks while forcing all sample columns to share the same marginal distribution given by $\\{q_{r}\\}_{r=1}^{4}$.\n\nTasks:\n- Apply quantile normalization to the matrix $X$ by computing $\\{q_{r}\\}_{r=1}^{4}$ and explicitly showing the step-by-step mapping from each raw entry $x_{ij}$ to its normalized value $y_{ij}$.\n- For each sample $j$, compute the within-sample variance before and after normalization using the unbiased sample variance\n$$\ns_{j}^{2} \\;=\\; \\frac{1}{m-1}\\sum_{i=1}^{m} \\left(x_{ij}-\\bar{x}_{\\cdot j}\\right)^{2},\n$$\nwith $m=4$ and $\\bar{x}_{\\cdot j}=\\frac{1}{m}\\sum_{i=1}^{m} x_{ij}$, and analogously with $y_{ij}$ for the normalized data. Then compute the change in within-sample variance defined as\n$$\n\\Delta \\;=\\; \\left(\\frac{1}{3}\\sum_{j=1}^{3} s_{j,\\mathrm{after}}^{2}\\right) \\;-\\; \\left(\\frac{1}{3}\\sum_{j=1}^{3} s_{j,\\mathrm{before}}^{2}\\right).\n$$\n\nProvide all intermediate computations. Express the final answer for $\\Delta$ as an exact number. No rounding is required, and no physical units apply to intensities in this setting.", "solution": "The user has provided a well-defined computational problem in the context of systems biomedicine. The problem is scientifically grounded, as quantile normalization is a standard technique. It is well-posed, with all necessary data and formulas provided, leading to a unique, verifiable solution. The problem statement is objective and contains no ambiguities or contradictions. Therefore, the problem is valid and a full solution will be provided.\n\nThe problem requires the application of quantile normalization to a given data matrix $X$ and the subsequent calculation of the change in average within-sample variance. The process is executed step-by-step as defined.\n\nThe raw intensity matrix is given as:\n$$\nX \\;=\\;\n\\begin{pmatrix}\n10  16  22 \\\\\n3  8  26 \\\\\n20  23  13 \\\\\n6  12  21\n\\end{pmatrix}\n$$\nThere are $m=4$ genes (rows) and $3$ replicates (columns).\n\n### Part 1: Quantile Normalization\n\n**Step 1: Sort each column.**\nFor each column vector, we sort the intensities in ascending order.\n\nFor column $j=1$: $(10, 3, 20, 6) \\rightarrow (3, 6, 10, 20)$\nFor column $j=2$: $(16, 8, 23, 12) \\rightarrow (8, 12, 16, 23)$\nFor column $j=3$: $(22, 26, 13, 21) \\rightarrow (13, 21, 22, 26)$\n\nWe can represent this in a matrix of ordered statistics, where each column $j$ contains the values $x_{(r)j}$ for $r \\in \\{1, 2, 3, 4\\}$.\n$$\nX_{\\text{sorted}} \\;=\\;\n\\begin{pmatrix}\n3  8  13 \\\\\n6  12  21 \\\\\n10  16  22 \\\\\n20  23  26\n\\end{pmatrix}\n$$\n\n**Step 2: Compute the reference distribution.**\nThe reference quantiles $q_r$ are the rank-wise averages across the replicates.\n$$\nq_r = \\frac{1}{3}\\sum_{j=1}^{3} x_{(r)j}\n$$\nFor $r=1$: $q_1 = \\frac{3+8+13}{3} = \\frac{24}{3} = 8$\nFor $r=2$: $q_2 = \\frac{6+12+21}{3} = \\frac{39}{3} = 13$\nFor $r=3$: $q_3 = \\frac{10+16+22}{3} = \\frac{48}{3} = 16$\nFor $r=4$: $q_4 = \\frac{20+23+26}{3} = \\frac{69}{3} = 23$\n\nThe reference distribution is $\\{q_1, q_2, q_3, q_4\\} = \\{8, 13, 16, 23\\}$.\n\n**Step 3: Assign normalized values.**\nWe first determine the rank of each entry $x_{ij}$ within its column $j$. Let $R$ be the rank matrix.\nFor column $j=1$: $(10, 3, 20, 6)$ have ranks $(3, 1, 4, 2)$ respectively.\nFor column $j=2$: $(16, 8, 23, 12)$ have ranks $(3, 1, 4, 2)$ respectively.\nFor column $j=3$: $(22, 26, 13, 21)$ have ranks $(3, 4, 1, 2)$ respectively.\n$$\nR \\;=\\;\n\\begin{pmatrix}\n3  3  3 \\\\\n1  1  4 \\\\\n4  4  1 \\\\\n2  2  2\n\\end{pmatrix}\n$$\nThe normalized value is $y_{ij} = q_{\\mathrm{rank}(x_{ij},j)}$. We use the rank matrix $R$ to map the reference quantiles back to the original matrix structure.\n$$\nY \\;=\\;\n\\begin{pmatrix}\nq_{R_{11}}  q_{R_{12}}  q_{R_{13}} \\\\\nq_{R_{21}}  q_{R_{22}}  q_{R_{23}} \\\\\nq_{R_{31}}  q_{R_{32}}  q_{R_{33}} \\\\\nq_{R_{41}}  q_{R_{42}}  q_{R_{43}}\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\nq_3  q_3  q_3 \\\\\nq_1  q_1  q_4 \\\\\nq_4  q_4  q_1 \\\\\nq_2  q_2  q_2\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n16  16  16 \\\\\n8  8  23 \\\\\n23  23  8 \\\\\n13  13  13\n\\end{pmatrix}\n$$\nThis is the quantile-normalized matrix $Y$.\n\n### Part 2: Variance Calculation\n\nThe unbiased sample variance for a sample of size $m=4$ is $s^2 = \\frac{1}{3}\\sum_{i=1}^{4} (z_i - \\bar{z})^2$.\n\n**Variance before normalization ($s_{j,\\mathrm{before}}^{2}$):**\n\nFor column $j=1$ with values $(10, 3, 20, 6)$:\nMean $\\bar{x}_{\\cdot 1} = \\frac{10+3+20+6}{4} = \\frac{39}{4}$.\nSum of squared differences: $\\sum_{i=1}^{4} (x_{i1}-\\bar{x}_{\\cdot 1})^2 = (10 - \\frac{39}{4})^2 + (3 - \\frac{39}{4})^2 + (20 - \\frac{39}{4})^2 + (6 - \\frac{39}{4})^2$\n$= (\\frac{1}{4})^2 + (-\\frac{27}{4})^2 + (\\frac{41}{4})^2 + (-\\frac{15}{4})^2 = \\frac{1+729+1681+225}{16} = \\frac{2636}{16} = \\frac{659}{4}$.\nVariance $s_{1,\\mathrm{before}}^{2} = \\frac{1}{3} \\times \\frac{659}{4} = \\frac{659}{12}$.\n\nFor column $j=2$ with values $(16, 8, 23, 12)$:\nMean $\\bar{x}_{\\cdot 2} = \\frac{16+8+23+12}{4} = \\frac{59}{4}$.\nSum of squared differences: $\\sum_{i=1}^{4} (x_{i2}-\\bar{x}_{\\cdot 2})^2 = (16 - \\frac{59}{4})^2 + (8 - \\frac{59}{4})^2 + (23 - \\frac{59}{4})^2 + (12 - \\frac{59}{4})^2$\n$= (\\frac{5}{4})^2 + (-\\frac{27}{4})^2 + (\\frac{33}{4})^2 + (-\\frac{11}{4})^2 = \\frac{25+729+1089+121}{16} = \\frac{1964}{16} = \\frac{491}{4}$.\nVariance $s_{2,\\mathrm{before}}^{2} = \\frac{1}{3} \\times \\frac{491}{4} = \\frac{491}{12}$.\n\nFor column $j=3$ with values $(22, 26, 13, 21)$:\nMean $\\bar{x}_{\\cdot 3} = \\frac{22+26+13+21}{4} = \\frac{82}{4} = \\frac{41}{2}$.\nSum of squared differences: $\\sum_{i=1}^{4} (x_{i3}-\\bar{x}_{\\cdot 3})^2 = (22 - \\frac{41}{2})^2 + (26 - \\frac{41}{2})^2 + (13 - \\frac{41}{2})^2 + (21 - \\frac{41}{2})^2$\n$= (\\frac{3}{2})^2 + (\\frac{11}{2})^2 + (-\\frac{15}{2})^2 + (\\frac{1}{2})^2 = \\frac{9+121+225+1}{4} = \\frac{356}{4} = 89$.\nVariance $s_{3,\\mathrm{before}}^{2} = \\frac{1}{3} \\times 89 = \\frac{89}{3}$.\n\nAverage variance before normalization:\n$\\frac{1}{3}\\sum_{j=1}^{3} s_{j,\\mathrm{before}}^{2} = \\frac{1}{3} \\left(\\frac{659}{12} + \\frac{491}{12} + \\frac{89}{3}\\right) = \\frac{1}{3} \\left(\\frac{659+491+4 \\times 89}{12}\\right) = \\frac{1}{3} \\left(\\frac{1150+356}{12}\\right) = \\frac{1506}{36} = \\frac{251}{6}$.\n\n**Variance after normalization ($s_{j,\\mathrm{after}}^{2}$):**\n\nBy construction, each column of the normalized matrix $Y$ is a permutation of the reference distribution $\\{8, 13, 16, 23\\}$. Therefore, the mean and variance are identical for all columns.\nMean $\\bar{y} = \\frac{8+13+16+23}{4} = \\frac{60}{4} = 15$.\nSum of squared differences: $\\sum (y_i - \\bar{y})^2 = (8-15)^2 + (13-15)^2 + (16-15)^2 + (23-15)^2$\n$= (-7)^2 + (-2)^2 + (1)^2 + (8)^2 = 49+4+1+64 = 118$.\nVariance for any column $j$: $s_{j,\\mathrm{after}}^{2} = \\frac{1}{3} \\times 118 = \\frac{118}{3}$.\n\nAverage variance after normalization:\n$\\frac{1}{3}\\sum_{j=1}^{3} s_{j,\\mathrm{after}}^{2} = \\frac{1}{3} \\left(\\frac{118}{3} + \\frac{118}{3} + \\frac{118}{3}\\right) = \\frac{1}{3} \\left(3 \\times \\frac{118}{3}\\right) = \\frac{118}{3}$.\n\n### Part 3: Change in Variance\n\nThe change in within-sample variance $\\Delta$ is defined as:\n$$\n\\Delta = \\left(\\frac{1}{3}\\sum_{j=1}^{3} s_{j,\\mathrm{after}}^{2}\\right) - \\left(\\frac{1}{3}\\sum_{j=1}^{3} s_{j,\\mathrm{before}}^{2}\\right)\n$$\nSubstituting the calculated average variances:\n$$\n\\Delta = \\frac{118}{3} - \\frac{251}{6} = \\frac{2 \\times 118}{6} - \\frac{251}{6} = \\frac{236 - 251}{6} = -\\frac{15}{6}\n$$\nSimplifying the fraction gives:\n$$\n\\Delta = -\\frac{5}{2}\n$$", "answer": "$$\n\\boxed{-\\frac{5}{2}}\n$$", "id": "4339948"}]}