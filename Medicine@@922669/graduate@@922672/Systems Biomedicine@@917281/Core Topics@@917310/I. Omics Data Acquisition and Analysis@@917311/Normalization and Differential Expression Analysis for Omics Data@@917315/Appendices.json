{"hands_on_practices": [{"introduction": "Before diving into complex models, it is crucial to understand *why* they are necessary. This practice uses a simulation to reveal the pitfalls of applying standard statistical tests to Transcripts Per Million (TPM) normalized data, a common but flawed approach for differential expression analysis. By creating scenarios that lead to false positives and false negatives, you will gain a first-hand appreciation for the artifacts introduced by compositional data constraints and the importance of using appropriate statistical frameworks. [@problem_id:4370568]", "problem": "Consider a simplified ribonucleic acid sequencing setting for gene-level quantification in systems biomedicine, where the goal is normalization and differential expression analysis for 'omics' data. The foundational base is the definition of Transcripts Per Million (TPM) and the properties of compositional data under closure. For a sample indexed by $s$, with $G$ genes indexed by $i \\in \\{1,\\dots,G\\}$, let $c_{i,s}$ denote the observed read counts for gene $i$ in sample $s$, and let $\\ell_i$ denote its effective transcript length in base pairs. The TPM for gene $i$ in sample $s$ is defined as\n$$\n\\mathrm{TPM}_{i,s} \\equiv 10^6 \\cdot \\frac{c_{i,s}/\\ell_i}{\\sum_{j=1}^{G} c_{j,s}/\\ell_j}.\n$$\nUnder this normalization, the vector $\\left(\\mathrm{TPM}_{1,s},\\dots,\\mathrm{TPM}_{G,s}\\right)$ lies on the $G$-dimensional simplex due to the closure constraint $\\sum_{i=1}^{G} \\mathrm{TPM}_{i,s} = 10^6$ for each $s$. In compositional data analysis, for random variables $X_1,\\dots,X_G$ constrained by $\\sum_{i=1}^{G} X_i = K$ deterministically, it follows from covariance properties that\n$$\n\\sum_{j=1}^{G} \\mathrm{cov}(X_i,X_j) = \\mathrm{cov}\\left(X_i,\\sum_{j=1}^{G}X_j\\right) = \\mathrm{cov}(X_i,K) = 0,\n$$\nwhich implies that $\\sum_{j \\neq i} \\mathrm{cov}(X_i,X_j) = -\\mathrm{var}(X_i)$, enforcing a negative balance in the covariances with the other components. This principle shows that closure induces structure in the covariance and correlation across genes that is independent of true biological co-regulation.\n\nYour task is to provide a counterexample demonstrating that TPM normalization cannot be used for differential expression testing without explicitly modeling compositional constraints, and to quantify the induced correlation across genes due to closure. Implement a program that simulates sequencing counts under two conditions for specified gene sets, computes TPM values, performs a naive Welch’s two-sample $t$-test on TPM for a target gene to assess differential expression, and computes the average off-diagonal Pearson correlation across genes using TPM across all simulated samples.\n\nAssume the following simulation model that is standard and scientifically plausible for bulk ribonucleic acid sequencing:\n- For a given condition, define the expected molecules per gene per cell as a vector $\\mathbf{m} = (m_1,\\dots,m_G)$ with strictly positive entries.\n- For a sample with library size $L$ total reads, per-sample gene probabilities are proportional to $m_i \\ell_i$, that is\n$$\np_i = \\frac{m_i \\ell_i}{\\sum_{j=1}^{G} m_j \\ell_j}.\n$$\n- Observed counts are sampled as $(c_{1,s},\\dots,c_{G,s}) \\sim \\mathrm{Multinomial}(L; p_1,\\dots,p_G)$ independently for each replicate sample.\n\nUsing this base, construct the following test suite, each requiring a quantifiable output:\n\nTest Case 1 (Counterexample for false positive on unchanged gene):\n- Parameters: $G=4$, $\\ell = (1000, 1000, 1000, 1000)$, Condition A molecules $\\mathbf{m}^{(A)} = (1000, 1000, 1000, 1000)$, Condition B molecules $\\mathbf{m}^{(B)} = (1000, 1000, 100000, 1000)$, replicates per condition $R=12$, library size $L = 2{,}000{,}000$, random seeds fixed but distinct across conditions, significance level $\\alpha = 0.001$, target gene index $i^\\star = 1$.\n- Task: Simulate counts for A and B, compute TPM per sample, perform Welch’s $t$-test on $\\{\\mathrm{TPM}_{i^\\star,s} : s \\in A\\}$ versus $\\{\\mathrm{TPM}_{i^\\star,s} : s \\in B\\}$, and return a boolean indicating naive differential expression detection on TPM that contradicts the unchanged absolute molecules for gene $i^\\star$. Specifically, return true if the test $p$-value is strictly less than $\\alpha$ and the mean TPM in condition B is strictly less than in condition A, even though $m^{(A)}_{i^\\star} = m^{(B)}_{i^\\star}$.\n\nTest Case 2 (Counterexample for false negative under global scaling):\n- Parameters: $G=4$, $\\ell = (800, 1000, 1200, 1500)$, Condition A molecules $\\mathbf{m}^{(A)} = (1000, 2000, 3000, 4000)$, Condition B molecules $\\mathbf{m}^{(B)} = 2 \\cdot \\mathbf{m}^{(A)}$ (global two-fold increase for all genes), replicates per condition $R=12$, library size $L = 2{,}000{,}000$, random seeds fixed but distinct across conditions, significance level $\\alpha = 0.001$, target gene index $i^\\star = 1$.\n- Task: Simulate counts for A and B, compute TPM per sample, perform Welch’s $t$-test on $\\mathrm{TPM}_{i^\\star}$ across conditions, and return a boolean indicating naive failure to detect the true change caused by global scaling. Specifically, return true if the test $p$-value is greater than or equal to $\\alpha$, even though the absolute molecules for the target gene doubled.\n\nTest Case 3 (Quantifying closure-induced correlation):\n- Reuse Test Case 1’s parameters and combine all TPM samples across both conditions (total of $2R$ samples).\n- Task: Compute the Pearson correlation matrix across the $G$ genes using TPM values across all $2R$ samples. Return the average off-diagonal correlation value as a real number (float), computed as the mean of all pairwise correlations for $i \\neq j$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, in the order: Test Case 1 boolean, Test Case 2 boolean, Test Case 3 float. For example, a valid output could look like \"[true,true,-0.215432]\". Use lowercase \"true\"/\"false\" for booleans and represent the float in standard decimal notation. No additional text should be printed.", "solution": "The problem requires the implementation of a numerical simulation to demonstrate fundamental artifacts arising from the use of Transcripts Per Million (TPM) normalization in differential expression analysis of ribonucleic acid sequencing (RNA-seq) data. The core scientific principle is that TPM values are compositional, meaning they are proportions of a whole that sum to a fixed constant ($10^6$). This closure property induces spurious correlations and can lead to erroneous conclusions in statistical tests that do not explicitly account for the compositional nature of the data. The solution is designed around three test cases that highlight these issues: a false positive, a false negative, and the quantification of induced negative correlation.\n\nThe algorithmic design is structured as follows:\n1.  A function to simulate RNA-seq counts based on a scientifically plausible generative model.\n2.  A function to compute TPM values from raw counts.\n3.  A sequence of three test cases implementing the simulation, normalization, and statistical analysis as specified.\n\n**1. Simulation of RNA-seq Counts**\n\nThe foundation of the analysis is a realistic simulation of the sequencing process. We model the read counts for a sample $s$ with $G$ genes as a draw from a multinomial distribution:\n$$\n(c_{1,s}, \\dots, c_{G,s}) \\sim \\mathrm{Multinomial}(L; p_1, \\dots, p_G)\n$$\nHere, $L$ is the total library size (total number of reads for the sample), and $\\mathbf{p} = (p_1, \\dots, p_G)$ is the vector of probabilities that a given read originates from a specific gene. This model captures the stochastic sampling nature of high-throughput sequencing. The probabilities $p_i$ are determined by the underlying true molecular abundance of each gene, $m_i$, and its effective transcript length, $\\ell_i$. A longer or more abundant transcript is more likely to be sampled. Thus, the probability for gene $i$ is proportional to the product $m_i \\ell_i$, and is normalized to sum to one:\n$$\np_i = \\frac{m_i \\ell_i}{\\sum_{j=1}^{G} m_j \\ell_j}\n$$\nThis simulation is performed independently for each of the $R$ replicate samples within each experimental condition (A and B), using distinct molecular abundance vectors $\\mathbf{m}^{(A)}$ and $\\mathbf{m}^{(B)}$ but fixed random seeds for reproducibility.\n\n**2. TPM Normalization**\n\nFrom the simulated counts $c_{i,s}$, we calculate the TPM values. The formula is:\n$$\n\\mathrm{TPM}_{i,s} = 10^6 \\cdot \\frac{c_{i,s}/\\ell_i}{\\sum_{j=1}^{G} c_{j,s}/\\ell_j}\n$$\nThe term $c_{i,s}/\\ell_i$ represents a rate of reads per base of the transcript, which is a proxy for the transcript's abundance. The normalization step involves dividing this rate by the sum of all such rates in the sample, converting it into a relative proportion, and scaling it to a total of one million. This denominator, $\\sum_{j=1}^{G} c_{j,s}/\\ell_j$, is the critical component that couples all genes' TPM values within a sample, thereby imposing the closure constraint $\\sum_{i=1}^{G} \\mathrm{TPM}_{i,s} = 10^6$.\n\n**3. Test Cases and Statistical Analysis**\n\n**Test Case 1: False Positive Detection**\nThis case demonstrates how a change in one gene can create a statistically significant, yet artificial, change in an unrelated, unchanged gene.\n-   **Principle**: We set the true molecular abundance for the target gene ($i^\\star = 1$) to be identical between Condition A ($\\mathbf{m}^{(A)}_{1} = 1000$) and Condition B ($\\mathbf{m}^{(B)}_{1} = 1000$). However, in Condition B, we massively increase the abundance of another gene ($i=3$).\n-   **Mechanism**: The large increase in $\\mathbf{m}^{(B)}_3$ drastically inflates the denominator of the TPM calculation for all genes in Condition B samples. Consequently, even though the raw counts for gene $1$ might be similar to those in Condition A (modulo sampling noise), its TPM value is artificially suppressed.\n-   **Implementation**: Counts are simulated for $R=12$ replicates per condition. TPM values are computed for all samples. A Welch’s two-sample $t$-test is performed on the TPM values of the target gene ($i^\\star = 1$) between the two conditions. The test is considered a \"false positive\" if the resulting $p$-value is less than the significance level $\\alpha=0.001$ and the mean TPM in B is less than in A, which would naively be interpreted as downregulation.\n\n**Test Case 2: False Negative Detection**\nThis case illustrates the inability of TPM to detect a global, systemic change affecting all genes.\n-   **Principle**: We model a scenario where every gene's true molecular abundance doubles in Condition B compared to Condition A, i.e., $\\mathbf{m}^{(B)} = 2 \\cdot \\mathbf{m}^{(A)}$. This represents a true, biologically significant change.\n-   **Mechanism**: When all $m_i$ are scaled by a constant factor $k$ (here, $k=2$), the multinomial probabilities $p_i$ remain unchanged:\n$$\np_i^{(B)} = \\frac{(k \\cdot m_i^{(A)}) \\ell_i}{\\sum_{j=1}^{G} (k \\cdot m_j^{(A)}) \\ell_j} = \\frac{k (m_i^{(A)} \\ell_i)}{k (\\sum_{j=1}^{G} m_j^{(A)} \\ell_j)} = p_i^{(A)}\n$$\nSince the sampling probabilities are identical for both conditions, the expected distributions of counts, and therefore TPMs, are also identical. The normalization completely masks the underlying two-fold increase in absolute abundance.\n-   **Implementation**: Counts and TPMs are generated as before. A Welch's $t$-test on the TPMs of the target gene ($i^\\star=1$) is expected to yield a high $p$-value, failing to detect the true change. The test is considered a \"false negative\" if the $p$-value is greater than or equal to $\\alpha = 0.001$.\n\n**Test Case 3: Quantifying Induced Correlation**\nThis test measures the negative correlation structure enforced by the closure property.\n-   **Principle**: As stated in the problem, for compositional data $X_i$ with a constant sum, $\\sum_{j \\neq i} \\mathrm{cov}(X_i,X_j) = -\\mathrm{var}(X_i)$. This implies that on average, an increase in one component must be balanced by a decrease in others, fostering negative covariances and correlations that are mathematical artifacts of normalization, not necessarily biological co-regulation.\n-   **Mechanism**: We use the data from Test Case 1, where the stark difference between Condition A and B (driven by gene $3$) creates strong variation in the TPM data. This variation provides a good basis for observing the induced correlations.\n-   **Implementation**: The TPM matrices from both conditions of Test Case 1 are combined into a single dataset of $2R=24$ samples. The Pearson correlation matrix across the $G=4$ genes is then computed. The final output is the average of all off-diagonal elements of this correlation matrix, which quantifies the average pairwise correlation induced by TPM normalization in this specific scenario.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import ttest_ind\n\ndef solve():\n    \"\"\"\n    Solves the three test cases related to RNA-seq data normalization and analysis.\n    \"\"\"\n\n    # Helper function to simulate RNA-seq counts using a multinomial model.\n    def simulate_counts(m, l, L, R, seed):\n        \"\"\"\n        Simulates counts for R replicates.\n        \n        Args:\n            m (np.array): Vector of molecule counts per gene.\n            l (np.array): Vector of gene lengths.\n            L (int): Library size (total reads).\n            R (int): Number of replicates.\n            seed (int): Random seed for reproducibility.\n\n        Returns:\n            np.array: An (R x G) matrix of simulated counts.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        # Calculate sampling probabilities proportional to molecule count * length\n        ml = m * l\n        # Check for sum of ml being zero to avoid division by zero, though unlikely.\n        sum_ml = np.sum(ml)\n        p = ml / sum_ml if sum_ml > 0 else np.zeros_like(ml)\n        \n        counts = rng.multinomial(n=L, pvals=p, size=R)\n        return counts\n\n    # Helper function to calculate TPM from a counts matrix.\n    def calculate_tpm(counts, l):\n        \"\"\"\n        Calculates Transcripts Per Million (TPM).\n\n        Args:\n            counts (np.array): An (R x G) matrix of counts.\n            l (np.array): Vector of gene lengths.\n\n        Returns:\n            np.array: An (R x G) matrix of TPM values.\n        \"\"\"\n        # rate = counts per kilobase\n        rate = counts / l\n        # sum of rates per sample\n        sum_of_rates = np.sum(rate, axis=1, keepdims=True)\n        # Avoid division by zero if a sample has all zero counts\n        sum_of_rates[sum_of_rates == 0] = 1\n        \n        tpm = 1e6 * (rate / sum_of_rates)\n        return tpm\n\n    results = []\n\n    # --- Test Case 1: Counterexample for false positive ---\n    params1 = {\n        'G': 4,\n        'l': np.array([1000, 1000, 1000, 1000], dtype=float),\n        'm_A': np.array([1000, 1000, 1000, 1000], dtype=float),\n        'm_B': np.array([1000, 1000, 100000, 1000], dtype=float),\n        'R': 12,\n        'L': 2_000_000,\n        'alpha': 0.001,\n        'target_gene_idx': 0,  # Gene i* = 1 is at index 0\n        'seed_A': 123,\n        'seed_B': 456\n    }\n    \n    counts_A1 = simulate_counts(params1['m_A'], params1['l'], params1['L'], params1['R'], params1['seed_A'])\n    counts_B1 = simulate_counts(params1['m_B'], params1['l'], params1['L'], params1['R'], params1['seed_B'])\n    \n    tpm_A1 = calculate_tpm(counts_A1, params1['l'])\n    tpm_B1 = calculate_tpm(counts_B1, params1['l'])\n\n    tpm_A1_target = tpm_A1[:, params1['target_gene_idx']]\n    tpm_B1_target = tpm_B1[:, params1['target_gene_idx']]\n\n    _, p_val1 = ttest_ind(tpm_A1_target, tpm_B1_target, equal_var=False)\n    \n    result1 = (p_val1  params1['alpha']) and (np.mean(tpm_B1_target)  np.mean(tpm_A1_target))\n    results.append(str(result1).lower())\n\n    # --- Test Case 2: Counterexample for false negative ---\n    params2 = {\n        'G': 4,\n        'l': np.array([800, 1000, 1200, 1500], dtype=float),\n        'm_A': np.array([1000, 2000, 3000, 4000], dtype=float),\n        'm_B': 2 * np.array([1000, 2000, 3000, 4000], dtype=float),\n        'R': 12,\n        'L': 2_000_000,\n        'alpha': 0.001,\n        'target_gene_idx': 0, # Gene i* = 1 is at index 0\n        'seed_A': 123,\n        'seed_B': 456\n    }\n\n    counts_A2 = simulate_counts(params2['m_A'], params2['l'], params2['L'], params2['R'], params2['seed_A'])\n    counts_B2 = simulate_counts(params2['m_B'], params2['l'], params2['L'], params2['R'], params2['seed_B'])\n    \n    tpm_A2 = calculate_tpm(counts_A2, params2['l'])\n    tpm_B2 = calculate_tpm(counts_B2, params2['l'])\n\n    tpm_A2_target = tpm_A2[:, params2['target_gene_idx']]\n    tpm_B2_target = tpm_B2[:, params2['target_gene_idx']]\n\n    _, p_val2 = ttest_ind(tpm_A2_target, tpm_B2_target, equal_var=False)\n    \n    result2 = p_val2 >= params2['alpha']\n    results.append(str(result2).lower())\n    \n    # --- Test Case 3: Quantifying closure-induced correlation ---\n    # Reuses TPM data from Test Case 1\n    all_tpm = np.vstack((tpm_A1, tpm_B1))\n    \n    # rowvar=False because genes are columns, samples are rows\n    corr_matrix = np.corrcoef(all_tpm, rowvar=False)\n    \n    G = params1['G']\n    # Sum of off-diagonal elements / number of off-diagonal elements\n    num_off_diagonal = G * G - G\n    sum_off_diagonal = np.sum(corr_matrix) - np.trace(corr_matrix)\n    avg_corr = sum_off_diagonal / num_off_diagonal if num_off_diagonal > 0 else 0.0\n    \n    # Format to 6 decimal places as in the example\n    results.append(f\"{avg_corr:.6f}\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4370568"}, {"introduction": "Having established the need for specialized methods, we now build the engine of modern differential expression analysis: the Generalized Linear Model (GLM). This exercise challenges you to implement a Poisson GLM from scratch to analyze count data, learning how to correctly incorporate library size normalization and adjust for batch effects. By deriving and implementing the Iteratively Reweighted Least Squares (IRLS) algorithm, you will gain a fundamental understanding of how effects like log-fold changes are estimated in a statistically rigorous manner. [@problem_id:4370596]", "problem": "You are given a generalized linear modeling task for high-throughput count data, situated in the context of systems biomedicine. The goal is to compute condition-specific effects in the presence of a batch effect while properly normalizing for library sizes via offsets. The model must be derived and implemented from first principles using the canonical definitions of a Poisson generalized linear model (Poisson GLM) with a logarithmic link and known offsets.\n\nStart from the following fundamental base and definitions. For each gene, the observed counts across samples are modeled as $y_i \\sim \\text{Poisson}(\\mu_i)$ for $i \\in \\{1,\\dots,n\\}$, where $n$ is the number of samples. The Poisson GLM with a log link and a known offset satisfies\n$$\n\\log \\mu_i \\;=\\; \\eta_i \\;=\\; x_i^\\top \\beta \\;+\\; o_i,\n$$\nwhere $x_i \\in \\mathbb{R}^p$ is the covariate row vector for sample $i$, $\\beta \\in \\mathbb{R}^p$ are the regression coefficients to be estimated, and $o_i$ is a known offset. In this problem, the offset must encode library size normalization by using the natural logarithm of the library sizes. For numerical stability and interpretability of the intercept, use centered offsets given by $o_i \\;=\\; \\log L_i - \\frac{1}{n}\\sum_{j=1}^n \\log L_j$, where $L_i$ is the library size for sample $i$. The systematic component uses an intercept, a condition indicator, and a batch indicator. The condition-specific effect is the coefficient corresponding to the condition indicator, and the batch effect is the coefficient corresponding to the batch indicator. All fits must be performed via maximum likelihood estimation using Iteratively Reweighted Least Squares (IRLS) derived from the Poisson likelihood with a log link and known offsets, without relying on any black-box modeling library.\n\nConstruct the contrast for the condition effect as a linear functional $c^\\top \\beta$, where $c \\in \\mathbb{R}^p$ selects the condition coefficient while holding the intercept and batch effect constant. In the additive model with no interaction, use $c = [0, 1, 0]^\\top$ where the coefficient ordering is intercept, condition, batch. Interpret $c^\\top \\beta$ as the estimated log rate ratio on the natural logarithm scale for the condition effect adjusted for batch and library sizes. Convert this to a base $2$ logarithm by dividing by $\\log 2$ to obtain the estimated $\\log_2$ fold change.\n\nImplementation requirements. For each gene, solve for $\\beta$ by IRLS using the standard Poisson GLM working response and weights. Specifically, with current $\\beta$, compute $\\eta = X \\beta + o$, $\\mu = \\exp(\\eta)$, the working response $z = \\eta + \\frac{y - \\mu}{\\mu}$, and the diagonal weight matrix $W = \\mathrm{diag}(\\mu)$. Update $\\beta$ by solving the weighted least squares normal equations for $X \\beta \\approx z - o$ using $W$ as weights, that is,\n$$\n\\beta_{\\text{new}} \\;=\\; \\arg\\min_{\\beta \\in \\mathbb{R}^p} \\; \\sum_{i=1}^n \\mu_i \\,\\big(z_i - o_i - x_i^\\top \\beta\\big)^2.\n$$\nTo ensure numerical stability in edge cases, include a small ridge regularization $\\lambda I_p$ on the normal equations with a very small $\\lambda > 0$ (for example, $\\lambda = 10^{-6}$), where $I_p$ is the $p \\times p$ identity matrix. Iterate until the Euclidean norm of the coefficient difference is less than a tolerance $\\varepsilon$ (for example, $\\varepsilon = 10^{-8}$) or a maximum number of iterations is reached. The identity of the final estimates must be independent of any constant shift applied to the offsets (centering offsets as specified ensures a well-scaled intercept).\n\nDesign matrix and sample annotation. There are $n = 4$ samples with an intercept, a binary condition, and a binary batch indicator. The design matrix uses columns in the order intercept, condition, batch. The sample annotations are: sample $1$ has condition $0$ and batch $0$, sample $2$ has condition $0$ and batch $1$, sample $3$ has condition $1$ and batch $0$, and sample $4$ has condition $1$ and batch $1$. Therefore, $X$ has rows $[1,0,0]$, $[1,0,1]$, $[1,1,0]$, and $[1,1,1]$.\n\nTest suite. Implement your program to process the following three independent test cases. In each test case, compute the estimated $\\log_2$ fold changes for the condition effect for each gene, in the order the genes are given. The offsets must be computed as $o_i = \\log L_i - \\frac{1}{4}\\sum_{j=1}^4 \\log L_j$ for the test cases with $4$ samples. The contrast is always $c = [0,1,0]^\\top$. For each test case, the design matrix and column ordering are as specified above.\n\nTest case $1$ (happy path): two genes, four samples, varying library sizes.\n- Library sizes $L = [$ $10^6$, $5\\times 10^5$, $2\\times 10^6$, $1.5\\times 10^6$ $]$.\n- Gene $1$ counts $y^{(1)} = [$ $10$, $5$, $42$, $26$ $]$.\n- Gene $2$ counts $y^{(2)} = [$ $15$, $10$, $28$, $30$ $]$.\n\nTest case $2$ (edge case with zeros and contrasting effects): three genes, four samples, same library sizes as test case $1$.\n- Library sizes $L = [$ $10^6$, $5\\times 10^5$, $2\\times 10^6$, $1.5\\times 10^6$ $]$.\n- Gene $1$ counts $y^{(1)} = [$ $39$, $19$, $18$, $14$ $]$.\n- Gene $2$ counts $y^{(2)} = [$ $8$, $7$, $15$, $19$ $]$.\n- Gene $3$ counts $y^{(3)} = [$ $0$, $0$, $2$, $1$ $]$.\n\nTest case $3$ (boundary case with equal library sizes and zero true condition effect): one gene, four samples, equal library sizes.\n- Library sizes $L = [$ $10^6$, $10^6$, $10^6$, $10^6$ $]$.\n- Gene $1$ counts $y^{(1)} = [$ $12$, $18$, $12$, $18$ $]$.\n\nRequired final output. Your program must produce a single line containing a single list with all estimated $\\log_2$ fold changes aggregated across all test cases in the following order: first all genes of test case $1$ in gene order, followed by all genes of test case $2$ in gene order, followed by all genes of test case $3$ in gene order. Each value must be rounded to exactly $4$ decimal places and printed as a decimal number. The output format must be a single line containing a comma-separated list enclosed in square brackets, for example $[$ $x_1$, $x_2$, $\\dots$, $x_m$ $]$, where $m$ is the total number of gene-specific results across all test cases. No other text must be printed.", "solution": "The problem is valid. It presents a clear, scientifically grounded, and well-posed task in computational statistics, specifically the implementation of a Poisson Generalized Linear Model (GLM) for analyzing high-throughput count data. All necessary data, model specifications, and algorithmic details are provided.\n\nThe solution proceeds by first principles, as requested. We model the observed counts $y_i$ for each gene in sample $i$ as being drawn from a Poisson distribution, $y_i \\sim \\text{Poisson}(\\mu_i)$. The core of the GLM is the relationship between the expected counts $\\mu_i$ and a set of covariates, which is defined by a link function and a linear predictor.\n\nThe problem specifies a logarithmic link function, leading to the model:\n$$\n\\log(\\mu_i) = \\eta_i = x_i^\\top \\beta + o_i\n$$\nHere, $\\eta_i$ is the linear predictor, $x_i^\\top$ is the $i$-th row of the design matrix $X$ encoding experimental factors (intercept, condition, batch), $\\beta$ is the vector of coefficients to be estimated, and $o_i$ is a known offset. The offset is crucial for normalizing the counts to account for variations in library size ($L_i$) across samples. The specified centered offsets, $o_i = \\log L_i - \\frac{1}{n}\\sum_{j=1}^n \\log L_j$, ensure that the intercept coefficient $\\beta_0$ is interpretable as the baseline log-rate for a sample with average library size, while not affecting the estimates of other coefficients.\n\nThe coefficients $\\beta$ are estimated using Maximum Likelihood Estimation (MLE). For GLMs, the likelihood is maximized not via a closed-form solution but through an iterative numerical procedure. The standard algorithm for this is Iteratively Reweighted Least Squares (IRLS). IRLS is equivalent to the Newton-Raphson or Fisher Scoring method for finding the root of the likelihood's gradient (the score function).\n\nEach iteration of the IRLS algorithm involves constructing and solving a weighted least squares (WLS) problem that locally approximates the MLE problem. As specified, at a given estimate $\\beta_{\\text{old}}$, we compute the following quantities for each sample $i=1, \\dots, n$:\n1.  The linear predictor: $\\eta_i = x_i^\\top \\beta_{\\text{old}} + o_i$.\n2.  The estimated mean: $\\mu_i = \\exp(\\eta_i)$.\n3.  The diagonal weight matrix: $W = \\mathrm{diag}(\\mu_1, \\dots, \\mu_n)$. For the Poisson model with a log link, the weights are simply the estimated means.\n4.  The working response: $z_i = \\eta_i + \\frac{y_i - \\mu_i}{\\mu_i}$. This quantity linearizes the model around the current estimate.\n\nThe update to the coefficient vector, $\\beta_{\\text{new}}$, is found by solving the WLS problem of regressing the \"working residuals\" $z_i - o_i$ onto the covariates $x_i$ with weights $\\mu_i$. This corresponds to minimizing the weighted sum of squares:\n$$\n\\beta_{\\text{new}} = \\arg\\min_{\\beta \\in \\mathbb{R}^p} \\sum_{i=1}^n \\mu_i (z_i - o_i - x_i^\\top \\beta)^2\n$$\nThe solution to this WLS problem is given by its normal equations. To enhance numerical stability, particularly in cases of low counts, data separation, or near-collinearity, a small ridge regularization term $\\lambda I_p$ is added. The update for $\\beta$ is thus obtained by solving the linear system:\n$$\n(X^\\top W X + \\lambda I_p) \\beta_{\\text{new}} = X^\\top W (z - o)\n$$\nwhere $z$ and $o$ are column vectors of the working responses and offsets, respectively.\n\nThe implementation will begin with an initial guess for $\\beta$, such as $\\beta = 0$, and will repeatedly solve this linear system, updating $\\beta$ at each step. The process continues until the change in the $\\beta$ vector between iterations, measured by the Euclidean norm $\\|\\beta_{\\text{new}} - \\beta_{\\text{old}}\\|_2$, falls below a specified tolerance $\\varepsilon = 10^{-8}$, or a maximum number of iterations is reached. In implementation, care must be taken when computing the working response $z_i$, as $\\mu_i$ can be close to zero. A common practice, which will be adopted, is to add a small positive constant to the denominator $\\mu_i$ to prevent division by zero.\n\nUpon convergence, the algorithm yields the MLE $\\hat\\beta$. The problem asks for the condition-specific effect, defined by the contrast $c = [0, 1, 0]^\\top$. The estimated log rate ratio (on the natural log scale) for the condition is thus $c^\\top \\hat\\beta = \\hat\\beta_1$. To express this as a $\\log_2$ fold change, which is conventional in genomics, this value is divided by $\\log 2$:\n$$\n\\text{log}_2 \\text{FoldChange} = \\frac{\\hat\\beta_1}{\\log 2}\n$$\nThis procedure is applied independently to each gene in the provided test suite to compute the required values.", "answer": "```python\nimport numpy as np\n\ndef irls_poisson_glm(y, X, L, lambda_reg, tol, max_iter):\n    \"\"\"\n    Fits a Poisson GLM with a log link and library size offsets using IRLS.\n\n    Args:\n        y (np.ndarray): Vector of observed counts.\n        X (np.ndarray): Design matrix.\n        L (np.ndarray): Vector of library sizes.\n        lambda_reg (float): Ridge regularization parameter.\n        tol (float): Convergence tolerance for the norm of beta difference.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        np.ndarray: The estimated coefficient vector beta.\n    \"\"\"\n    n_samples, n_coeffs = X.shape\n\n    # 1. Compute centered log-library size offsets\n    log_L = np.log(L)\n    offsets = log_L - np.mean(log_L)\n\n    # 2. Initialize beta coefficients to zero\n    beta = np.zeros(n_coeffs)\n\n    # 3. Perform Iteratively Reweighted Least Squares (IRLS)\n    for _ in range(max_iter):\n        beta_old = beta.copy()\n\n        # Compute linear predictor and mean response\n        eta = X @ beta + offsets\n        mu = np.exp(eta)\n\n        # Compute working response `z`\n        # A small constant is added to mu in the denominator for numerical stability,\n        # especially important for samples with zero counts where mu can be very small.\n        mu_safe_denominator = np.maximum(mu, 1e-12)\n        z = eta + (y - mu) / mu_safe_denominator\n\n        # Compute diagonal weight matrix W (as a vector for efficiency)\n        W_diag = mu\n        \n        # Solve the regularized normal equations for the WLS problem:\n        # (X^T W X + lambda*I) beta = X^T W (z - o)\n        \n        # Construct the left-hand side matrix, A\n        # Using element-wise multiplication for efficiency (avoids forming large W matrix)\n        A = X.T @ (X * W_diag[:, np.newaxis]) + lambda_reg * np.identity(n_coeffs)\n\n        # Construct the right-hand side vector, b\n        b_vec = W_diag * (z - offsets)\n        b = X.T @ b_vec\n\n        # Solve for the new beta\n        try:\n            beta = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # In case of a singular matrix despite regularization,\n            # we halt and return the last valid estimate.\n            return beta_old\n\n        # Check for convergence\n        if np.linalg.norm(beta - beta_old)  tol:\n            break\n\n    return beta\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    # Define the fixed design matrix for all test cases\n    # Columns: Intercept, Condition, Batch\n    X = np.array([\n        [1, 0, 0],\n        [1, 0, 1],\n        [1, 1, 0],\n        [1, 1, 1]\n    ], dtype=float)\n\n    # Parameters for the IRLS algorithm from the problem description\n    lambda_reg = 1e-6\n    tol = 1e-8\n    max_iter = 100\n\n    # Define the test suite\n    test_cases = [\n        # Test case 1\n        {\n            \"L\": np.array([1e6, 5e5, 2e6, 1.5e6]),\n            \"genes\": [\n                np.array([10, 5, 42, 26]),\n                np.array([15, 10, 28, 30])\n            ]\n        },\n        # Test case 2\n        {\n            \"L\": np.array([1e6, 5e5, 2e6, 1.5e6]),\n            \"genes\": [\n                np.array([39, 19, 18, 14]),\n                np.array([8, 7, 15, 19]),\n                np.array([0, 0, 2, 1])\n            ]\n        },\n        # Test case 3\n        {\n            \"L\": np.array([1e6, 1e6, 1e6, 1e6]),\n            \"genes\": [\n                np.array([12, 18, 12, 18])\n            ]\n        }\n    ]\n\n    all_results = []\n    \n    # Process each test case\n    for case in test_cases:\n        L = case[\"L\"]\n        for y_counts in case[\"genes\"]:\n            # Ensure count vector is a float array for calculations\n            y = np.array(y_counts, dtype=float)\n            \n            # Fit the GLM to get the beta coefficients\n            beta_hat = irls_poisson_glm(y, X, L, lambda_reg, tol, max_iter)\n            \n            # The condition effect coefficient is beta_hat[1]\n            # This corresponds to c^T * beta where c = [0, 1, 0]^T\n            log_rate_ratio = beta_hat[1]\n            \n            # Convert to log2 fold change\n            log2_fold_change = log_rate_ratio / np.log(2)\n            \n            all_results.append(log2_fold_change)\n\n    # Format results to 4 decimal places and create the output string\n    formatted_results = [f\"{val:.4f}\" for val in all_results]\n    \n    # Print the final output in the specified format\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4370596"}, {"introduction": "While GLMs provide robust point estimates of log-fold changes (LFCs), these estimates can be noisy, especially for genes with low counts. This advanced practice introduces the concept of empirical Bayes shrinkage, a powerful technique to improve LFC estimates by borrowing information across all genes. You will implement the core logic behind two popular shrinkage methods and evaluate how they balance bias and variance to produce more accurate and stable results, a key feature of modern differential expression tools. [@problem_id:4370601]", "problem": "You are given a small two-condition RNA sequencing count dataset and asked to implement empirical Bayesian shrinkage of log fold changes. The fundamental base for the design starts from Bayes' theorem, the Negative Binomial model for sequencing counts, the definition of size factor normalization via the median-of-ratios method, and the normal approximation to the maximum likelihood log fold change estimator with a variance derived by the delta method. The goal is to compute shrunk log fold changes for each gene using two priors: a Cauchy prior (apeglm-like) and a mixture-of-zero-centered normals prior (Adaptive Shrinkage, ashr-like), and to quantify the bias–variance trade-off by calculating mean squared error against a specified vector of true log fold changes.\n\nConsider $G=6$ genes and $S=6$ samples with two conditions, $A$ and $B$, each with $n_A=n_B=3$ replicates. The sample ordering is $A_1,A_2,A_3,B_1,B_2,B_3$. The observed raw counts $x_{g,i}$ for gene $g \\in \\{1,\\dots,6\\}$ and sample $i \\in \\{1,\\dots,6\\}$ are:\n\n- Gene $1$: condition $A$ counts $[48,52,55]$, condition $B$ counts $[50,49,53]$.\n- Gene $2$: condition $A$ counts $[30,28,32]$, condition $B$ counts $[60,64,58]$.\n- Gene $3$: condition $A$ counts $[90,85,95]$, condition $B$ counts $[60,70,65]$.\n- Gene $4$: condition $A$ counts $[0,1,0]$, condition $B$ counts $[8,13,11]$.\n- Gene $5$: condition $A$ counts $[6,5,7]$, condition $B$ counts $[5,6,6]$.\n- Gene $6$: condition $A$ counts $[500,520,480]$, condition $B$ counts $[250,240,260]$.\n\nNormalization: Let $s_i$ be the size factor for sample $i$, computed via the median-of-ratios method. For each gene $g$, compute the geometric mean across samples,\n$$\n\\text{gm}_g = \\exp\\left(\\frac{1}{S}\\sum_{i=1}^S \\log(x_{g,i})\\right),\n$$\nexcluding genes with any zero counts when computing $\\text{gm}_g$. For each sample $i$, compute ratios $r_{g,i} = x_{g,i}/\\text{gm}_g$ over genes $g$ with $\\text{gm}_g0$, and set\n$$\ns_i = \\text{median}\\left(\\{r_{g,i} : \\text{gm}_g  0\\}\\right).\n$$\nDefine normalized counts $y_{g,i} = x_{g,i} / s_i$.\n\nLog fold change estimation and variance: For each gene $g$, compute condition means on normalized counts,\n$$\nm_{g,A} = \\frac{1}{n_A}\\sum_{i:\\text{cond}(i)=A} y_{g,i},\\quad m_{g,B} = \\frac{1}{n_B}\\sum_{i:\\text{cond}(i)=B} y_{g,i}.\n$$\nUse a small pseudocount $\\varepsilon=0.5$ to stabilize ratios and define the observed log fold change estimate\n$$\n\\hat{\\beta}_g = \\log\\left(\\frac{m_{g,B} + \\varepsilon}{m_{g,A} + \\varepsilon}\\right).\n$$\nAssume the Negative Binomial variance model $\\operatorname{Var}(Y)=\\mu + \\alpha \\mu^2$. Estimate a per-gene dispersion $\\alpha_g$ by method-of-moments: compute sample variances $v_{g,A}$ and $v_{g,B}$ of $y_{g,i}$ within each condition, set $\\bar{v}_g = (v_{g,A}+v_{g,B})/2$ and $\\bar{m}_g = (m_{g,A}+m_{g,B})/2$, then\n$$\n\\alpha_g = \\max\\left(\\frac{\\bar{v}_g - \\bar{m}_g}{\\bar{m}_g^2},\\,0\\right).\n$$\nUsing the delta method for the logarithm, approximate the variance of $\\hat{\\beta}_g$ as\n$$\ns_g^2 = \\frac{m_{g,B} + \\alpha_g m_{g,B}^2}{n_B\\, m_{g,B}^2} + \\frac{m_{g,A} + \\alpha_g m_{g,A}^2}{n_A\\, m_{g,A}^2}.\n$$\n\nEmpirical Bayes shrinkage methods:\n\n1. Cauchy prior shrinkage (apeglm-like): Assume a prior for $\\beta_g$ given by the Cauchy distribution with scale $\\gamma$: $p(\\beta_g) \\propto \\left(1 + (\\beta_g/\\gamma)^2\\right)^{-1}$. With the normal likelihood $\\hat{\\beta}_g \\sim \\mathcal{N}(\\beta_g, s_g^2)$, compute the posterior mode $\\tilde{\\beta}_g$ by solving\n$$\n\\frac{\\tilde{\\beta}_g - \\hat{\\beta}_g}{s_g^2} - \\frac{2\\tilde{\\beta}_g}{\\gamma^2 + \\tilde{\\beta}_g^2} = 0\n$$\nfor $\\tilde{\\beta}_g$ using Newton's method initialized at $\\hat{\\beta}_g$.\n\n2. Adaptive Shrinkage (ashr-like) mixture-of-normals prior: Assume a zero-centered mixture-of-normals prior for $\\beta_g$,\n$$\np(\\beta_g) = \\sum_{k=0}^{K-1} \\pi_k \\,\\mathcal{N}(0, \\sigma_k^2),\n$$\nwith grid $\\{\\sigma_k\\}_{k=0}^{K-1} = \\{0, 0.1, 0.2, 0.5, 1.0, 2.0\\}$ and mixing weights $\\pi_k$ estimated by maximizing the marginal likelihood across genes via the Expectation-Maximization algorithm. The marginal density of $\\hat{\\beta}_g$ is\n$$\nf(\\hat{\\beta}_g) = \\sum_{k=0}^{K-1} \\pi_k \\,\\mathcal{N}\\left(0, s_g^2 + \\sigma_k^2\\right).\n$$\nIn the E-step, compute responsibilities\n$$\nr_{gk} = \\frac{\\pi_k \\,\\mathcal{N}\\left(\\hat{\\beta}_g; 0, s_g^2 + \\sigma_k^2\\right)}{\\sum_{j=0}^{K-1} \\pi_j \\,\\mathcal{N}\\left(\\hat{\\beta}_g; 0, s_g^2 + \\sigma_j^2\\right)}.\n$$\nIn the M-step, update mixing weights $\\pi_k \\leftarrow \\frac{1}{G}\\sum_{g=1}^G r_{gk}$. After convergence, compute the posterior mean shrinkage for each gene:\n$$\n\\tilde{\\beta}_g = \\sum_{k=0}^{K-1} w_{gk} \\,\\mu_{gk},\\quad \\mu_{gk} = \\frac{\\sigma_k^2}{\\sigma_k^2 + s_g^2} \\hat{\\beta}_g,\n$$\nwhere $w_{gk}$ are the normalized posterior component weights proportional to $\\pi_k \\,\\mathcal{N}\\left(\\hat{\\beta}_g; 0, s_g^2 + \\sigma_k^2\\right)$.\n\nGround truth for the bias–variance analysis: Assume the following true log fold changes (natural logarithm) for the genes:\n- Gene $1$: $\\beta^\\star_1 = 0$.\n- Gene $2$: $\\beta^\\star_2 = \\log(2)$.\n- Gene $3$: $\\beta^\\star_3 = \\log(0.7)$.\n- Gene $4$: $\\beta^\\star_4 = \\log(10)$.\n- Gene $5$: $\\beta^\\star_5 = 0$.\n- Gene $6$: $\\beta^\\star_6 = \\log(0.5)$.\n\nFor each method and specified parameters, compute the mean squared error\n$$\n\\text{MSE} = \\frac{1}{G}\\sum_{g=1}^G \\left(\\tilde{\\beta}_g - \\beta^\\star_g\\right)^2.\n$$\n\nTest suite:\n- Case $1$: Cauchy prior shrinkage with $\\gamma=0.5$.\n- Case $2$: Cauchy prior shrinkage with $\\gamma=1.0$.\n- Case $3$: Cauchy prior shrinkage with $\\gamma=2.5$.\n- Case $4$: Adaptive Shrinkage mixture-of-normals with the grid $\\{0, 0.1, 0.2, 0.5, 1.0, 2.0\\}$, initialized uniform mixing weights and iterated until the maximum absolute change in $\\pi_k$ is less than $10^{-6}$ or a maximum of $1000$ iterations.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases above, with each value a float, for example $[\\text{mse}_{\\gamma=0.5},\\text{mse}_{\\gamma=1.0},\\text{mse}_{\\gamma=2.5},\\text{mse}_{\\text{ashr}}]$. No physical units or angles are involved in this problem; express all numerical outputs as pure decimal floats on that single line.", "solution": "The problem requires the implementation and evaluation of two empirical Bayesian shrinkage methods for log fold changes (LFCs) derived from RNA sequencing (RNA-seq) count data. The analysis proceeds in three main stages: first, initial estimation of LFCs and their variances from raw counts; second, application of shrinkage methods to these estimates; and third, evaluation against true LFCs using Mean Squared Error (MSE).\n\nThe dataset consists of $G=6$ genes and $S=6$ samples, with $n_A=n_B=3$ replicates for two conditions, $A$ and $B$. The raw count matrix $X \\in \\mathbb{N}_0^{G \\times S}$ is given as:\n$$\nX = \\begin{pmatrix}\n48  52  55  50  49  53 \\\\\n30  28  32  60  64  58 \\\\\n90  85  95  60  70  65 \\\\\n0  1  0  8  13  11 \\\\\n6  5  7  5  6  6 \\\\\n500  520  480  250  240  260\n\\end{pmatrix}\n$$\n\n**Step 1: Normalization and Initial Estimation**\n\nNormalization is essential to adjust for differences in sequencing depth and library composition among samples. We use the median-of-ratios method as specified.\n\n1.  **Geometric Mean Calculation**: For each gene $g$, we compute a pseudo-reference sample as the geometric mean of its counts across all samples. Genes with any zero counts are excluded from this step to avoid a geometric mean of zero. In our dataset, gene $4$ has zero counts and is thus excluded. For the remaining genes $g' \\in \\{1, 2, 3, 5, 6\\}$, the geometric mean is:\n    $$\n    \\text{gm}_{g'} = \\left( \\prod_{i=1}^S x_{g',i} \\right)^{1/S} = \\exp\\left(\\frac{1}{S}\\sum_{i=1}^S \\log(x_{g',i})\\right)\n    $$\n2.  **Size Factor Calculation**: For each sample $i$, we compute the ratio of its count for each gene $g'$ to the gene's geometric mean, $r_{g',i} = x_{g',i}/\\text{gm}_{g'}$. The size factor $s_i$ for sample $i$ is the median of these ratios:\n    $$\n    s_i = \\text{median}\\left(\\{r_{g',i} : \\text{gm}_{g'}  0\\}\\right)\n    $$\n3.  **Normalized Counts**: The raw counts $x_{g,i}$ are divided by the corresponding sample's size factor $s_i$ to obtain normalized counts $y_{g,i}$:\n    $$\n    y_{g,i} = \\frac{x_{g,i}}{s_i}\n    $$\n4.  **LFC and Variance Estimation**:\n    -   For each gene $g$, the mean normalized count for each condition is calculated:\n        $$\n        m_{g,A} = \\frac{1}{n_A}\\sum_{i \\in \\text{cond A}} y_{g,i},\\quad m_{g,B} = \\frac{1}{n_B}\\sum_{i \\in \\text{cond B}} y_{g,i}\n        $$\n    -   The maximum likelihood estimate (MLE) of the log fold change, $\\hat{\\beta}_g$, is computed using a pseudocount $\\varepsilon=0.5$ to stabilize the ratio, especially for low counts:\n        $$\n        \\hat{\\beta}_g = \\log\\left(\\frac{m_{g,B} + \\varepsilon}{m_{g,A} + \\varepsilon}\\right)\n        $$\n    -   The variance of the counts is modeled by the Negative Binomial distribution, $\\operatorname{Var}(Y) = \\mu + \\alpha \\mu^2$, where $\\alpha$ is the dispersion parameter. We estimate a per-gene dispersion $\\alpha_g$ using the method of moments. First, we compute the sample variances of normalized counts within each condition, $v_{g,A}$ and $v_{g,B}$, and pool them: $\\bar{v}_g = (v_{g,A}+v_{g,B})/2$. We also pool the means: $\\bar{m}_g = (m_{g,A}+m_{g,B})/2$. The dispersion is then:\n        $$\n        \\alpha_g = \\max\\left(\\frac{\\bar{v}_g - \\bar{m}_g}{\\bar{m}_g^2},\\,0\\right)\n        $$\n    -   The variance of the LFC estimator, $s_g^2 = \\operatorname{Var}(\\hat{\\beta}_g)$, is approximated using the delta method. The variance of $\\log(m)$ is approx. $\\operatorname{Var}(m)/m^2$. For a Negative Binomial with $n$ replicates, $\\operatorname{Var}(m) = (\\mu+\\alpha\\mu^2)/n$. This gives:\n        $$\n        s_g^2 = \\frac{\\operatorname{Var}(m_{g,B})}{m_{g,B}^2} + \\frac{\\operatorname{Var}(m_{g,A})}{m_{g,A}^2} = \\frac{m_{g,B} + \\alpha_g m_{g,B}^2}{n_B\\, m_{g,B}^2} + \\frac{m_{g,A} + \\alpha_g m_{g,A}^2}{n_A\\, m_{g,A}^2} = \\frac{1}{n_B}\\left(\\frac{1}{m_{g,B}} + \\alpha_g\\right) + \\frac{1}{n_A}\\left(\\frac{1}{m_{g,A}} + \\alpha_g\\right)\n        $$\n        With $n_A=n_B=3$.\n\n**Step 2: Empirical Bayes Shrinkage**\n\nWe apply two shrinkage methods to the pairs $(\\hat{\\beta}_g, s_g^2)$, assuming a normal likelihood $\\hat{\\beta}_g | \\beta_g \\sim \\mathcal{N}(\\beta_g, s_g^2)$.\n\n1.  **Cauchy Prior Shrinkage (apeglm-like)**: The true LFC $\\beta_g$ is assumed to follow a heavy-tailed Cauchy prior with scale $\\gamma$: $p(\\beta_g | \\gamma) \\propto (1 + (\\beta_g/\\gamma)^2)^{-1}$. The shrunken estimate $\\tilde{\\beta}_g$ is the posterior mode. The log-posterior is $\\log p(\\beta_g | \\hat{\\beta}_g) \\propto -(\\hat{\\beta}_g-\\beta_g)^2/(2s_g^2) - \\log(1+(\\beta_g/\\gamma)^2)$. Setting its derivative with respect to $\\beta_g$ to zero gives the specified equation:\n    $$\n    \\frac{\\tilde{\\beta}_g - \\hat{\\beta}_g}{s_g^2} - \\frac{2\\tilde{\\beta}_g}{\\gamma^2 + \\tilde{\\beta}_g^2} = 0\n    $$\n    We solve this for $\\tilde{\\beta}_g$ for each gene using Newton's method. Let $f(\\beta) = \\frac{\\beta - \\hat{\\beta}_g}{s_g^2} - \\frac{2\\beta}{\\gamma^2 + \\beta^2}$. The iterative update is $\\beta_{k+1} = \\beta_k - f(\\beta_k)/f'(\\beta_k)$, initialized at $\\beta_0 = \\hat{\\beta}_g$. The derivative is $f'(\\beta) = \\frac{1}{s_g^2} - \\frac{2(\\gamma^2 - \\beta^2)}{(\\gamma^2 + \\beta^2)^2}$. This procedure is repeated for $\\gamma \\in \\{0.5, 1.0, 2.5\\}$.\n\n2.  **Adaptive Shrinkage (ashr-like)**: This method uses a flexible mixture-of-normals prior for the true LFC:\n    $$\n    p(\\beta_g | \\pi, \\Sigma) = \\sum_{k=0}^{K-1} \\pi_k \\,\\mathcal{N}(\\beta_g; 0, \\sigma_k^2)\n    $$\n    The standard deviations are fixed to the grid $\\{\\sigma_k\\} = \\{0, 0.1, 0.2, 0.5, 1.0, 2.0\\}$. The mixing weights $\\pi = (\\pi_0, \\dots, \\pi_{K-1})$ are estimated from the data by maximizing the marginal likelihood of all $\\hat{\\beta}_g$ using the Expectation-Maximization (EM) algorithm.\n    -   **Initialization**: Uniform weights $\\pi_k = 1/K$ for $k=0,\\dots,5$.\n    -   **E-Step**: For each gene $g$ and component $k$, compute the responsibility $r_{gk}$, which is the posterior probability that $\\hat{\\beta}_g$ came from component $k$. This uses the marginal density for component $k$, $f_k(\\hat{\\beta}_g) = \\mathcal{N}(\\hat{\\beta}_g; 0, s_g^2 + \\sigma_k^2)$:\n        $$\n        r_{gk} = \\frac{\\pi_k f_k(\\hat{\\beta}_g)}{\\sum_{j=0}^{K-1} \\pi_j f_j(\\hat{\\beta}_g)}\n        $$\n    -   **M-Step**: Update the mixture weights by averaging responsibilities across all $G$ genes:\n        $$\n        \\pi_k^{\\text{new}} = \\frac{1}{G}\\sum_{g=1}^G r_{gk}\n        $$\n    -   The E and M steps are iterated until the maximum absolute change in any $\\pi_k$ is below $10^{-6}$ or for $1000$ iterations.\n    -   **Posterior Mean Calculation**: After the $\\pi_k$ values converge, the shrunken estimate $\\tilde{\\beta}_g$ is computed as the posterior mean $E[\\beta_g | \\hat{\\beta}_g, s_g^2, \\pi]$. This is a weighted average of the posterior means from each component:\n        $$\n        \\tilde{\\beta}_g = \\sum_{k=0}^{K-1} w_{gk} \\mu_{gk}\n        $$\n        where $w_{gk}$ are the final responsibilities $r_{gk}$, and $\\mu_{gk}$ is the posterior mean for component $k$:\n        $$\n        \\mu_{gk} = E[\\beta_g | \\hat{\\beta}_g, s_g^2, \\text{component}=k] = \\frac{\\sigma_k^2}{\\sigma_k^2+s_g^2}\\hat{\\beta}_g\n        $$\n\n**Step 3: Performance Evaluation**\n\nThe performance of each shrinkage method is quantified by the Mean Squared Error (MSE) against a set of true LFCs, $\\beta^\\star = (\\beta^\\star_1, \\dots, \\beta^\\star_G)$:\n$$\n\\beta^\\star = (0, \\log(2), \\log(0.7), \\log(10), 0, \\log(0.5))\n$$\nThe MSE for a given set of shrunken estimates $\\tilde{\\beta} = (\\tilde{\\beta}_1, \\dots, \\tilde{\\beta}_G)$ is:\n$$\n\\text{MSE} = \\frac{1}{G}\\sum_{g=1}^G \\left(\\tilde{\\beta}_g - \\beta^\\star_g\\right)^2\n$$\nThis calculation is performed for the three Cauchy shrinkage cases and the one ashr-like shrinkage case, yielding four final MSE values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Implements and evaluates empirical Bayesian shrinkage for RNA-seq LFCs.\n    \"\"\"\n    #\n    # Data Setup\n    #\n    G, S = 6, 6\n    n_A, n_B = 3, 3\n    \n    counts = np.array([\n        [48, 52, 55, 50, 49, 53],    # Gene 1\n        [30, 28, 32, 60, 64, 58],    # Gene 2\n        [90, 85, 95, 60, 70, 65],    # Gene 3\n        [0, 1, 0, 8, 13, 11],        # Gene 4\n        [6, 5, 7, 5, 6, 6],          # Gene 5\n        [500, 520, 480, 250, 240, 260] # Gene 6\n    ], dtype=float)\n\n    #\n    # Step 1: Normalization and Initial Estimation\n    #\n\n    # 1.1: Size Factors (Median-of-ratios)\n    # Find genes with no zero counts\n    genes_for_norm = np.all(counts > 0, axis=1)\n    \n    # Geometric means for these genes\n    log_counts_for_norm = np.log(counts[genes_for_norm])\n    geo_means = np.exp(np.mean(log_counts_for_norm, axis=1))\n    \n    # Ratios\n    ratios = counts[genes_for_norm] / geo_means[:, np.newaxis]\n    \n    # Size factors\n    size_factors = np.median(ratios, axis=0)\n\n    # 1.2: Normalized Counts\n    norm_counts = counts / size_factors\n\n    # 1.3: LFC and Variance Estimation\n    means_A = np.mean(norm_counts[:, :n_A], axis=1)\n    means_B = np.mean(norm_counts[:, n_A:], axis=1)\n\n    pseudocount = 0.5\n    beta_hat = np.log((means_B + pseudocount) / (means_A + pseudocount))\n\n    var_A = np.var(norm_counts[:, :n_A], axis=1, ddof=1)\n    var_B = np.var(norm_counts[:, n_A:], axis=1, ddof=1)\n    v_bar = (var_A + var_B) / 2\n    m_bar = (means_A + means_B) / 2\n    \n    # Handle m_bar == 0 case (though not present here)\n    m_bar_sq = m_bar**2\n    alpha = np.zeros_like(m_bar)\n    non_zero_m = m_bar_sq > 0\n    alpha[non_zero_m] = np.maximum(0, (v_bar[non_zero_m] - m_bar[non_zero_m]) / m_bar_sq[non_zero_m])\n\n    s_sq = np.zeros_like(alpha)\n    # Handle means == 0 cases in variance calc (not present, but good practice)\n    valid_A = means_A > 0\n    valid_B = means_B > 0\n    valid_both = valid_A  valid_B\n    \n    if np.any(valid_both):\n        s_sq[valid_both] = (1/n_A) * (1/means_A[valid_both] + alpha[valid_both]) + \\\n                           (1/n_B) * (1/means_B[valid_both] + alpha[valid_both])\n    # For invalid means, variance would be infinite. We have no such cases here.\n    # If s_sq ends up with zeros or negative values, they are invalid.\n    s_sq[s_sq = 0] = np.min(s_sq[s_sq > 0]) # Small positive value for stability if needed\n\n    #\n    # Step 2: Empirical Bayes Shrinkage\n    #\n\n    # 2.1: Cauchy Prior Shrinkage (apeglm-like)\n    def shrink_cauchy(beta_hat_g, s_sq_g, gamma):\n        beta = beta_hat_g  # Initialize with MLE\n        for _ in range(20): # Newton's method iterations\n            beta_sq = beta**2\n            gamma_sq = gamma**2\n            \n            f_beta = (beta - beta_hat_g) / s_sq_g - (2 * beta) / (gamma_sq + beta_sq)\n            \n            f_prime_beta = 1/s_sq_g - (2 * (gamma_sq - beta_sq)) / (gamma_sq + beta_sq)**2\n            \n            if np.abs(f_prime_beta)  1e-9:\n                break # Avoid division by zero\n            \n            beta = beta - f_beta / f_prime_beta\n        return beta\n\n    beta_tilde_cauchy_0_5 = np.array([shrink_cauchy(beta_hat[g], s_sq[g], 0.5) for g in range(G)])\n    beta_tilde_cauchy_1_0 = np.array([shrink_cauchy(beta_hat[g], s_sq[g], 1.0) for g in range(G)])\n    beta_tilde_cauchy_2_5 = np.array([shrink_cauchy(beta_hat[g], s_sq[g], 2.5) for g in range(G)])\n    \n    # 2.2: Adaptive Shrinkage (ashr-like)\n    def shrink_ashr(beta_hat, s_sq):\n        K = 6\n        sigma = np.array([0, 0.1, 0.2, 0.5, 1.0, 2.0])\n        pi = np.full(K, 1/K)\n        \n        # EM Algorithm\n        for _ in range(1000):\n            pi_old = pi.copy()\n            \n            # E-step\n            log_lik_matrix = np.zeros((G, K))\n            for k in range(K):\n                # Likelihood N(beta_hat_g; 0, s_sq_g + sigma_k^2)\n                # scipy.stats.norm.pdf(x, loc=mu, scale=std_dev)\n                total_var = s_sq + sigma[k]**2\n                log_lik_matrix[:, k] = norm.logpdf(beta_hat, loc=0, scale=np.sqrt(total_var))\n\n            log_pi = np.log(pi)\n            joint_log_lik = log_pi + log_lik_matrix\n            \n            # Normalize to get responsibilities r_gk (in log space to avoid underflow)\n            log_marg_lik = np.log(np.sum(np.exp(joint_log_lik - np.max(joint_log_lik, axis=1, keepdims=True)), axis=1)) + np.max(joint_log_lik, axis=1)\n            log_r_gk = joint_log_lik - log_marg_lik[:, np.newaxis]\n            r_gk = np.exp(log_r_gk)\n\n            # M-step\n            pi = np.mean(r_gk, axis=0)\n\n            if np.max(np.abs(pi - pi_old))  1e-6:\n                break\n        \n        # Posterior mean calculation\n        beta_tilde = np.zeros(G)\n        for g in range(G):\n            # component posterior means\n            mu_gk = (sigma**2 / (sigma**2 + s_sq[g])) * beta_hat[g]\n            # final responsibilities/weights are w_gk = r_gk\n            w_gk = r_gk[g, :]\n            beta_tilde[g] = np.sum(w_gk * mu_gk)\n\n        return beta_tilde\n\n    beta_tilde_ashr = shrink_ashr(beta_hat, s_sq)\n\n    #\n    # Step 3: Performance Evaluation\n    #\n    beta_star = np.array([\n        0,                 # Gene 1\n        np.log(2),         # Gene 2\n        np.log(0.7),       # Gene 3\n        np.log(10),        # Gene 4\n        0,                 # Gene 5\n        np.log(0.5)        # Gene 6\n    ])\n\n    def calculate_mse(beta_tilde, beta_star):\n        return np.mean((beta_tilde - beta_star)**2)\n\n    mse_cauchy_0_5 = calculate_mse(beta_tilde_cauchy_0_5, beta_star)\n    mse_cauchy_1_0 = calculate_mse(beta_tilde_cauchy_1_0, beta_star)\n    mse_cauchy_2_5 = calculate_mse(beta_tilde_cauchy_2_5, beta_star)\n    mse_ashr = calculate_mse(beta_tilde_ashr, beta_star)\n\n    results = [mse_cauchy_0_5, mse_cauchy_1_0, mse_cauchy_2_5, mse_ashr]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4370601"}]}