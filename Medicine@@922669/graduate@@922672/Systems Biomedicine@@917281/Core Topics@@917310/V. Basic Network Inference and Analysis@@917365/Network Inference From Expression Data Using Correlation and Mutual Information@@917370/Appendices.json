{"hands_on_practices": [{"introduction": "The cornerstone of co-expression network inference is quantifying the association between pairs of genes. This exercise grounds you in the fundamental statistical methods used for this task. You will start from first principles to compute the Pearson correlation coefficient for a pair of hypothetical genes and then derive and apply the $t$-test to assess its statistical significance, connecting it directly to the theory of linear regression. By working through the calculation and considering the impact of data properties like outliers, you will gain a deeper appreciation for the power and the critical limitations of correlation as a measure of biological association [@problem_id:4365175].", "problem": "You are studying co-expression relationships to infer regulatory interactions in a gene network from steady-state messenger ribonucleic acid (mRNA) expression measured across independent biological samples. Consider two genes, denoted by $G_{1}$ and $G_{2}$, with normalized log-transformed expression values across $n=10$ samples given by\n$G_{1}: \\{2.0,\\ 2.5,\\ 3.0,\\ 3.5,\\ 4.0,\\ 4.5,\\ 5.0,\\ 5.5,\\ 6.0,\\ 8.0\\}$ and $G_{2}: \\{1.9,\\ 2.6,\\ 2.7,\\ 3.6,\\ 3.9,\\ 4.7,\\ 5.1,\\ 5.4,\\ 6.1,\\ 6.5\\}$. The goal is to quantify the linear association between $G_{1}$ and $G_{2}$ and assess its statistical significance under commonly used assumptions in co-expression network inference.\n\nStarting from the definitions of sample mean, sample covariance, and Pearson product-moment correlation coefficient, compute the sample Pearson correlation $r$ between $G_{1}$ and $G_{2}$. Then, starting from the classical linear model assumptions that $(G_{1},G_{2})$ are jointly Gaussian with independent and identically distributed (i.i.d.) samples and test the null hypothesis $H_{0}:\\rho=0$, derive the null distribution of the correlation-based test statistic by connecting it to the ordinary least squares $t$-test for the slope in simple linear regression of $G_{2}$ on $G_{1}$. Use this derivation to compute the value of the $t$-statistic for the data above.\n\nRound the computed $t$-statistic to four significant figures.\n\nFinally, discuss how heteroscedasticity (variance changing with the mean) and outliers (including high-leverage points) in expression data can affect the validity of this correlation-based $t$-test and the downstream network inference; in particular, explain implications for false positive edges and contrast with mutual information-based inference under the Gaussian assumption and with nonparametric estimators.\n\nYour final reported value must be the single real number equal to the $t$-statistic, rounded to four significant figures, with no units.", "solution": "The user has provided a problem that requires the calculation of a statistical measure of association between two variables, the derivation of a related test statistic, and a discussion of the underlying assumptions.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n-   Gene 1 expression data ($G_{1}$): $\\{2.0,\\ 2.5,\\ 3.0,\\ 3.5,\\ 4.0,\\ 4.5,\\ 5.0,\\ 5.5,\\ 6.0,\\ 8.0\\}$\n-   Gene 2 expression data ($G_{2}$): $\\{1.9,\\ 2.6,\\ 2.7,\\ 3.6,\\ 3.9,\\ 4.7,\\ 5.1,\\ 5.4,\\ 6.1,\\ 6.5\\}$\n-   Number of samples ($n$): $10$\n-   Task 1: Compute the sample Pearson correlation coefficient $r$.\n-   Task 2: Derive the null distribution of the correlation-based test statistic by connecting it to the ordinary least squares (OLS) $t$-test for the slope in simple linear regression.\n-   Task 3: Compute the value of the $t$-statistic.\n-   Task 4: Round the $t$-statistic to four significant figures.\n-   Task 5: Discuss the effects of heteroscedasticity and outliers on the validity of the test and compare with mutual information-based inference.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is a standard exercise in biostatistics, specifically in the analysis of gene expression data for network inference. The concepts of Pearson correlation, linear regression, $t$-tests, and the assumptions of these models are fundamental statistical principles. The discussion points (heteroscedasticity, outliers, mutual information) are central to robust statistical inference in genomics. The problem is scientifically sound.\n-   **Well-Posed**: The problem provides all necessary data and clearly specifies the required calculations, derivation, and discussion topics. A unique numerical answer exists for the computational part.\n-   **Objective**: The problem is stated using precise, objective, and standard scientific terminology.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with a full solution.\n\n***\n\n**Part 1: Computation of the Sample Pearson Correlation Coefficient ($r$)**\n\nLet the expression values for gene $G_{1}$ be denoted by the set $\\{x_i\\}$ and for gene $G_{2}$ by $\\{y_i\\}$, for $i=1, \\dots, n$, where $n=10$.\n\nThe sample Pearson correlation coefficient $r$ is defined as:\n$$r = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\sum_{i=1}^{n} (y_i - \\bar{y})^2}} = \\frac{SS_{xy}}{\\sqrt{SS_{xx} SS_{yy}}}$$\nwhere $\\bar{x}$ and $\\bar{y}$ are the sample means, and $SS_{xx}$, $SS_{yy}$, and $SS_{xy}$ are the sums of squares and cross-products.\n\nFirst, we compute the sample means:\n$$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{1}{10}(2.0+2.5+3.0+3.5+4.0+4.5+5.0+5.5+6.0+8.0) = \\frac{44.0}{10} = 4.4$$\n$$\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i = \\frac{1}{10}(1.9+2.6+2.7+3.6+3.9+4.7+5.1+5.4+6.1+6.5) = \\frac{42.5}{10} = 4.25$$\n\nNext, we compute the sums of squares and cross-products:\n$$SS_{xx} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2 = (2.0-4.4)^2 + (2.5-4.4)^2 + \\dots + (8.0-4.4)^2$$\n$$SS_{xx} = (-2.4)^2 + (-1.9)^2 + (-1.4)^2 + (-0.9)^2 + (-0.4)^2 + (0.1)^2 + (0.6)^2 + (1.1)^2 + (1.6)^2 + (3.6)^2$$\n$$SS_{xx} = 5.76 + 3.61 + 1.96 + 0.81 + 0.16 + 0.01 + 0.36 + 1.21 + 2.56 + 12.96 = 29.4$$\n\n$$SS_{yy} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 = (1.9-4.25)^2 + (2.6-4.25)^2 + \\dots + (6.5-4.25)^2$$\n$$SS_{yy} = (-2.35)^2 + (-1.65)^2 + (-1.55)^2 + (-0.65)^2 + (-0.35)^2 + (0.45)^2 + (0.85)^2 + (1.15)^2 + (1.85)^2 + (2.25)^2$$\n$$SS_{yy} = 5.5225 + 2.7225 + 2.4025 + 0.4225 + 0.1225 + 0.2025 + 0.7225 + 1.3225 + 3.4225 + 5.0625 = 21.925$$\n\n$$SS_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) = (2.0-4.4)(1.9-4.25) + \\dots + (8.0-4.4)(6.5-4.25)$$\n$$SS_{xy} = (-2.4)(-2.35) + (-1.9)(-1.65) + (-1.4)(-1.55) + (-0.9)(-0.65) + (-0.4)(-0.35) + (0.1)(0.45) + (0.6)(0.85) + (1.1)(1.15) + (1.6)(1.85) + (3.6)(2.25)$$\n$$SS_{xy} = 5.64 + 3.135 + 2.17 + 0.585 + 0.14 + 0.045 + 0.51 + 1.265 + 2.96 + 8.1 = 24.55$$\n\nFinally, we compute $r$:\n$$r = \\frac{24.55}{\\sqrt{29.4 \\times 21.925}} = \\frac{24.55}{\\sqrt{644.595}} \\approx 0.96696025$$\n\n**Part 2: Derivation of the Null Distribution for the Correlation Test Statistic**\n\nWe aim to test the null hypothesis $H_0: \\rho = 0$, where $\\rho$ is the population correlation coefficient. This is equivalent to testing the null hypothesis $H_0: \\beta_1 = 0$ for the slope in the simple linear regression model $Y = \\beta_0 + \\beta_1 X + \\epsilon$, where we regress $G_2$ (as $Y$) on $G_1$ (as $X$).\n\nThe test statistic for the OLS slope estimate, $b_1$, is given by:\n$$t = \\frac{b_1 - \\beta_1}{SE(b_1)}$$\nUnder $H_0: \\beta_1 = 0$, this simplifies to $t = \\frac{b_1}{SE(b_1)}$. This statistic is known to follow a Student's $t$-distribution with $n-2$ degrees of freedom, provided the model assumptions (linearity, independence, homoscedasticity, normality of errors) hold.\n\nThe OLS estimate for the slope is $b_1 = \\frac{SS_{xy}}{SS_{xx}}$.\nThe standard error of the slope estimate is $SE(b_1) = \\frac{\\hat{\\sigma}}{\\sqrt{SS_{xx}}}$, where $\\hat{\\sigma}^2$ is the mean squared error (MSE), an unbiased estimator of the error variance $\\sigma^2_ \\epsilon$.\nThe MSE is calculated as $MSE = \\frac{SSE}{n-2}$, where $SSE$ is the sum of squared errors (residuals).\nThe SSE can be expressed in terms of the sums of squares: $SSE = SS_{yy} - \\frac{(SS_{xy})^2}{SS_{xx}}$.\n\nNow, we rewrite the $t$-statistic in terms of $r$. First, recognize the relationship between $r$ and $b_1$:\n$$r = \\frac{SS_{xy}}{\\sqrt{SS_{xx} SS_{yy}}} \\implies b_1 = \\frac{SS_{xy}}{SS_{xx}} = \\frac{r \\sqrt{SS_{xx} SS_{yy}}}{SS_{xx}} = r \\sqrt{\\frac{SS_{yy}}{SS_{xx}}}$$\n\nNext, we express $SSE$ in terms of $r$:\n$$SSE = SS_{yy} - \\frac{(r \\sqrt{SS_{xx} SS_{yy}})^2}{SS_{xx}} = SS_{yy} - \\frac{r^2 SS_{xx} SS_{yy}}{SS_{xx}} = SS_{yy} (1 - r^2)$$\n\nNow substitute these expressions back into the formula for the $t$-statistic:\n$$t = \\frac{b_1}{SE(b_1)} = \\frac{b_1}{\\sqrt{\\frac{SSE}{n-2}} / \\sqrt{SS_{xx}}} = \\frac{b_1 \\sqrt{SS_{xx}}}{\\sqrt{\\frac{SS_{yy}(1 - r^2)}{n-2}}}$$\n\nSubstituting the expression for $b_1$:\n$$t = \\frac{\\left(r \\sqrt{\\frac{SS_{yy}}{SS_{xx}}}\\right) \\sqrt{SS_{xx}}}{\\sqrt{\\frac{SS_{yy}(1 - r^2)}{n-2}}} = \\frac{r \\sqrt{SS_{yy}}}{\\frac{\\sqrt{SS_{yy}} \\sqrt{1 - r^2}}{\\sqrt{n-2}}}$$\n$$t = \\frac{r \\sqrt{n-2}}{\\sqrt{1 - r^2}}$$\nThis is the desired test statistic. Under the null hypothesis $H_0: \\rho = 0$, this statistic follows a Student's $t$-distribution with $n-2$ degrees of freedom. This is the null distribution.\n\n**Part 3: Computation of the t-statistic**\n\nUsing the values computed in Part 1:\n$n = 10$\n$r \\approx 0.96696025$\n\n$$t = \\frac{0.96696025 \\sqrt{10-2}}{\\sqrt{1 - (0.96696025)^2}} = \\frac{0.96696025 \\sqrt{8}}{\\sqrt{1 - 0.93501192}} = \\frac{0.96696025 \\times 2.828427}{\\sqrt{0.06498808}} \\approx \\frac{2.73500}{0.2549178} \\approx 10.7290$$\nRounding the result to four significant figures, we get $10.73$.\n\n**Part 4: Discussion on Model Validity and Comparison**\n\nThe validity of the $t$-test for correlation, and by extension co-expression network inference based on it, critically depends on the assumptions of the underlying linear model. Two major violations are heteroscedasticity and the presence of outliers.\n\n**Heteroscedasticity**: This refers to the situation where the variance of the residuals is not constant across a range of predictor values. In gene expression data, it is common for genes with higher mean expression levels to also exhibit higher variance. The OLS procedure assumes homoscedasticity (constant variance). When this assumption is violated, the OLS estimator for the slope ($b_1$) remains unbiased, but the formula for its standard error ($SE(b_1)$) is incorrect and typically biased. This bias results in an incorrectly calculated $t$-statistic, which no longer follows the $t$-distribution under the null hypothesis. The consequence is an invalid p-value and often an inflated Type I error rate. For network inference, this means an increased number of **false positive edges**, where a significant correlation is declared due to the flawed statistical test, not an underlying biological association.\n\n**Outliers and High-Leverage Points**: An outlier is a data point that deviates markedly from other observations. A high-leverage point is an observation with an extreme predictor value (e.g., the point $(x_{10}, y_{10}) = (8.0, 6.5)$ in this problem, where $x_{10}=8.0$ is distant from $\\bar{x}=4.4$). A point that is both an outlier and has high leverage is an influential point. The Pearson correlation coefficient is known to be highly sensitive to such points. A single influential point can create a strong, statistically significant correlation where none exists for the bulk of the data, or it can mask a true underlying correlation. This leads to both **false positives** (inferring an edge due to an artifact) and **false negatives** (failing to infer a true edge). Inferred network connections become highly dependent on single, potentially erroneous, data points rather than the overall biological trend.\n\n**Contrast with Mutual Information (MI)**:\n\n1.  **MI under the Gaussian Assumption**: If the data $(G_1, G_2)$ are truly from a bivariate Gaussian distribution, then the mutual information $I(G_1, G_2)$ is a monotonic function of the squared Pearson correlation coefficient $\\rho^2$: $I(G_1, G_2) = -\\frac{1}{2} \\ln(1 - \\rho^2)$. In this specific, idealized case, testing for $I=0$ is completely equivalent to testing for $\\rho=0$. There is no advantage to using MI over correlation, as they measure the same underlying linear association.\n\n2.  **MI with Nonparametric Estimators**: The true strength of MI is realized when used with nonparametric estimators (e.g., k-nearest neighbor or kernel density estimators) that do not assume a Gaussian (or any other) distribution.\n    -   **Capturing Non-linearity**: MI quantifies any statistical dependency, not just linear ones. For regulatory relationships that are switch-like or sigmoidal, the Pearson correlation may be close to zero, leading to a false negative. MI can detect such non-linear associations.\n    -   **Robustness**: Nonparametric MI is generally more robust to certain types of outliers than Pearson correlation. Because it is based on the probability densities (often estimated from ranks or local densities), it is less distorted by the magnitude of extreme values. It can mitigate the false positives and false negatives driven by influential points that plague correlation-based methods.\n    -   **Disadvantage**: The primary challenge with nonparametric MI is its estimation from limited data. It is prone to systematic estimation bias, especially with small sample sizes ($n=10$ is extremely small for this purpose), and its variance can be high. Accurate MI estimation is data-intensive, which can itself lead to erroneous inferences if not carefully handled.\n\nIn summary, for network inference, a significant correlation-based $t$-test is only a reliable indicator of a linear association under strict assumptions. Violations like heteroscedasticity and outliers, common in real data, can lead to spurious network edges. Nonparametric MI offers a more robust and a more general alternative for detecting dependencies, but its practical application requires larger sample sizes and careful implementation to avoid its own estimation pitfalls.", "answer": "$$\n\\boxed{10.73}\n$$", "id": "4365175"}, {"introduction": "While Pearson correlation is excellent for detecting linear relationships, biological systems are replete with non-linear interactions. This practice introduces Mutual Information (MI) as a more general measure of statistical dependence and computationally explores its relationship with correlation. You will write a program to demonstrate a foundational result: for jointly Gaussian data, MI is a monotonic function of correlation, but for data with non-Gaussian marginals, their rankings of dependencies can diverge significantly. This hands-on simulation clarifies the concept of a copula and illustrates why MI is a more robust tool for capturing diverse types of associations in complex biological data [@problem_id:4365189].", "problem": "You are tasked with implementing a complete, runnable program that investigates the ordering of inferred network edges when scored by correlation versus mutual information, using synthetic gene expression data. The context is systems biomedicine, where a network edge corresponds to a pair of genes whose expression profiles are measured across samples. The program must demonstrate that for jointly Gaussian variables, ranking edges by correlation or by mutual information yields identical orderings, and must quantify how these orderings diverge when the marginal distributions are non-Gaussian while the dependence structure remains Gaussian. All mathematical entities must be written in LaTeX.\n\nBegin from the following fundamental base:\n\n- The Pearson correlation coefficient for two real-valued random variables $X$ and $Y$ with finite second moments is defined by\n$$\n\\rho = \\frac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)}\\sqrt{\\operatorname{Var}(Y)}}.\n$$\n- The mutual information (MI) between two continuous random variables $X$ and $Y$ with joint probability density function $p_{X,Y}(x,y)$ and marginal density functions $p_X(x)$ and $p_Y(y)$ is defined by\n$$\nI(X;Y) = \\iint p_{X,Y}(x,y)\\,\\log\\left(\\frac{p_{X,Y}(x,y)}{p_X(x)\\,p_Y(y)}\\right)\\,\\mathrm{d}x\\,\\mathrm{d}y,\n$$\nwhich is nonnegative and measures statistical dependence without assuming linearity.\n- For a zero-mean, jointly Gaussian vector $(X,Y)$ with covariance matrix\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_X^2 & \\rho\\,\\sigma_X\\sigma_Y \\\\ \\rho\\,\\sigma_X\\sigma_Y & \\sigma_Y^2 \\end{pmatrix},\n$$\nits differential entropy is\n$$\nh(X,Y) = \\frac{1}{2}\\log\\left((2\\pi e)^2 \\det \\Sigma\\right),\n$$\nand the entropy of each marginal is $h(Z) = \\frac{1}{2}\\log\\left(2\\pi e\\,\\sigma_Z^2\\right)$ for $Z \\in \\{X,Y\\}$. Using these facts, one can derive that the mutual information of a bivariate normal depends only on the correlation coefficient:\n$$\nI(X;Y) = -\\frac{1}{2}\\log\\left(1 - \\rho^2\\right).\n$$\n- Mutual information is invariant under invertible transformations of the marginals: if $f$ and $g$ are invertible functions, then $I(X;Y) = I(f(X);g(Y))$. The copula, which captures dependence independent of marginals, is preserved under monotonic transforms.\n\nYour program must implement the following logic:\n\n1. Simulate synthetic latent Gaussian expression data for a set of network edges. Each edge $i$ corresponds to a pair $(X_i, Y_i)$ generated as $n$ independent samples from a bivariate normal distribution with unit variances and specified correlation $\\rho_i$, i.e., covariance matrix\n$$\n\\Sigma_i = \\begin{pmatrix} 1 & \\rho_i \\\\ \\rho_i & 1 \\end{pmatrix}.\n$$\n2. For the Gaussian case, treat $(X_i, Y_i)$ as observed data. Compute, for each edge:\n   - The absolute Pearson correlation $\\lvert \\hat{\\rho}_i \\rvert$ estimated from samples.\n   - The mutual information $I_i$ computed by plugging $\\hat{\\rho}_i$ into the Gaussian formula, i.e.,\n   $$\n   I_i = -\\frac{1}{2}\\log\\left(1 - \\hat{\\rho}_i^2\\right).\n   $$\n   Rank edges in descending order by $\\lvert \\hat{\\rho}_i \\rvert$ and by $I_i$, and determine whether these two rankings are exactly identical (a boolean).\n3. For the non-Gaussian marginal case, apply strictly monotonic transformations to the marginals of each edge’s latent Gaussian samples to obtain observed data $(X'_i,Y'_i)$:\n   - Use the following invertible monotonic functions to induce diverse non-Gaussian marginals, applied per edge $i$:\n     - $f_1(x) = \\log\\left(1 + \\exp\\left(\\operatorname{clip}(x,-20,20)\\right)\\right)$ and $g_1(y) = \\log\\left(1 + \\exp\\left(\\operatorname{clip}(y,-20,20)\\right)\\right)$,\n     - $f_2(x) = \\log\\left(1 + \\exp\\left(\\operatorname{clip}(x,-20,20)\\right)\\right)$ and $g_2(y) = \\sinh\\left(\\operatorname{clip}(y,-4,4)\\right)$,\n     - $f_3(x) = \\log\\left(1 + \\exp\\left(\\operatorname{clip}(x,-20,20)\\right)\\right)$ and $g_3(y) = \\tanh(y)$,\n     - $f_4(x) = \\tanh(x)$ and $g_4(y) = y^3$,\n     - $f_5(x) = \\sinh\\left(\\operatorname{clip}(x,-4,4)\\right)$ and $g_5(y) = \\log\\left(1 + \\exp\\left(\\operatorname{clip}(y,-20,20)\\right)\\right)$.\n     Here $\\operatorname{clip}(z,a,b)$ denotes truncating $z$ to the interval $[a,b]$ to avoid numerical overflow while preserving monotonicity.\n   - For each edge $i$, compute the absolute Pearson correlation $\\lvert \\widehat{\\rho}'_i \\rvert$ on $(X'_i,Y'_i)$.\n   - Compute the mutual information $I_i$ using the known latent correlation parameter $\\rho_i$ via the Gaussian formula $I_i = -\\frac{1}{2}\\log\\left(1 - \\rho_i^2\\right)$, relying on invariance under invertible marginals and the preserved Gaussian copula.\n   - Rank edges by $\\lvert \\widehat{\\rho}'_i \\rvert$ and by $I_i$ (descend both), and quantify divergence between the two rankings using the normalized inversion fraction\n     $$\n     D = \\frac{\\text{number of discordant ordered pairs}}{\\binom{m}{2}},\n     $$\n     where $m$ is the number of edges and an ordered pair is discordant if the relative order of the two edges differs between the two rankings. Report $D$ as a float rounded to six decimal places.\n4. Use a fixed random seed to ensure reproducibility.\n\nTest Suite:\n\n- Case A (Gaussian, happy path): $n=2000$, $m=4$ edges, $\\rho$ values $[0.0, 0.2, 0.5, 0.8]$. Output a boolean indicating whether the correlation-based and mutual information-based rankings are identical.\n- Case B (Non-Gaussian marginals, happy path): $n=2000$, $m=4$ edges, latent $\\rho$ values $[0.0, 0.2, 0.5, 0.8]$, with $(f_1,g_1)$, $(f_2,g_2)$, $(f_3,g_3)$, $(f_4,g_4)$ applied respectively to edges $1$ through $4$. Output the normalized inversion fraction $D$ rounded to six decimal places.\n- Case C (Non-Gaussian marginals, boundary condition with near-perfect correlation and small sample size): $n=200$, $m=5$ edges, latent $\\rho$ values $[0.1, 0.95, 0.6, 0.3, 0.85]$, with $(f_1,g_1)$, $(f_2,g_2)$, $(f_3,g_3)$, $(f_4,g_4)$, $(f_5,g_5)$ applied respectively to edges $1$ through $5$. Output the normalized inversion fraction $D$ rounded to six decimal places.\n\nFinal Output Format:\n\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets. The first entry must be the boolean for Case A. The second and third entries must be the floats $D$ for Case B and Case C respectively, each rounded to six decimal places. For example, the output should look like\n$$\n[\\text{True},0.123456,0.000000].\n$$\nNo other text should be printed.", "solution": "The problem statement is critically evaluated for validity before a solution is constructed.\n\n### Step 1: Extract Givens\n\n- **Pearson Correlation Coefficient**: $\\rho = \\frac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)}\\sqrt{\\operatorname{Var}(Y)}}$\n- **Mutual Information (MI)**: $I(X;Y) = \\iint p_{X,Y}(x,y)\\,\\log\\left(\\frac{p_{X,Y}(x,y)}{p_X(x)\\,p_Y(y)}\\right)\\,\\mathrm{d}x\\,\\mathrm{d}y$\n- **MI for Bivariate Gaussian**: For a zero-mean, jointly Gaussian vector $(X,Y)$ with correlation $\\rho$, the mutual information is $I(X;Y) = -\\frac{1}{2}\\log\\left(1 - \\rho^2\\right)$.\n- **MI Invariance**: For invertible functions $f$ and $g$, $I(X;Y) = I(f(X);g(Y))$.\n- **Simulation Setup**: Synthesize data for $m$ edges. Each edge $i$ corresponds to a pair $(X_i, Y_i)$ generated as $n$ independent samples from a bivariate normal distribution with covariance matrix $\\Sigma_i = \\begin{pmatrix} 1 & \\rho_i \\\\ \\rho_i & 1 \\end{pmatrix}$.\n- **Task 1 (Gaussian Case)**:\n    - Use $(X_i, Y_i)$ as observed data.\n    - Compute estimated absolute correlation $\\lvert \\hat{\\rho}_i \\rvert$.\n    - Compute MI as $I_i = -\\frac{1}{2}\\log\\left(1 - \\hat{\\rho}_i^2\\right)$.\n    - Rank edges by $\\lvert \\hat{\\rho}_i \\rvert$ and $I_i$ and check if rankings are identical.\n- **Task 2 (Non-Gaussian Marginal Case)**:\n    - Transform latent Gaussian samples to $(X'_i,Y'_i)$ using specified strictly monotonic functions.\n    - **Transformations**: Five pairs of functions $(f_k, g_k)$ are provided, using $\\log(1+\\exp(\\cdot))$, $\\sinh(\\cdot)$, $\\tanh(\\cdot)$, and $x^3$. The $\\operatorname{clip}(z,a,b)$ function is defined for numerical stability.\n    - Compute estimated absolute correlation $\\lvert \\widehat{\\rho}'_i \\rvert$ on $(X'_i,Y'_i)$.\n    - Compute MI using the *latent* correlation $\\rho_i$: $I_i = -\\frac{1}{2}\\log\\left(1 - \\rho_i^2\\right)$.\n    - Quantify ranking divergence using the normalized inversion fraction: $D = \\frac{\\text{number of discordant ordered pairs}}{\\binom{m}{2}}$.\n- **Reproducibility**: Use a fixed random seed.\n- **Test Suite**:\n    - **Case A**: Gaussian case, $n=2000$, $m=4$, $\\rho = [0.0, 0.2, 0.5, 0.8]$. Output: boolean.\n    - **Case B**: Non-Gaussian case, $n=2000$, $m=4$, latent $\\rho = [0.0, 0.2, 0.5, 0.8]$, transforms $(f_1,g_1), \\dots, (f_4,g_4)$. Output: float $D$ rounded to $6$ decimal places.\n    - **Case C**: Non-Gaussian case, $n=200$, $m=5$, latent $\\rho = [0.1, 0.95, 0.6, 0.3, 0.85]$, transforms $(f_1,g_1), \\dots, (f_5,g_5)$. Output: float $D$ rounded to $6$ decimal places.\n- **Final Output Format**: A single line `[<boolean>,<float>,<float>]`.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is well-grounded in statistics and information theory. The relationship between mutual information and Pearson correlation for Gaussian variables is a standard textbook result. The use of copula theory (implicitly, via monotonic marginal transformations) to study dependence structures is a valid and widely used statistical concept, particularly in quantitative finance and systems biology. All mathematical formulas and principles are correct.\n- **Well-Posed**: The problem is completely specified. All parameters ($n, m, \\rho$ values), data generation procedures, computational tasks, and output metrics are defined unambiguously. A unique, stable, and meaningful solution can be computed for a given random seed.\n- **Objective**: The problem is stated in precise, objective mathematical and algorithmic language, free from any subjective or speculative claims.\n\nThe problem does not exhibit any of the flaws listed in the invalidity checklist. It is scientifically sound, formalizable, complete, feasible, and well-posed.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A solution will be constructed.\n\n### Solution\n\nThe objective is to programmatically demonstrate a fundamental difference between Pearson correlation and mutual information as measures of statistical dependence for inferring network edges. The core principles are:\n$1$. For jointly Gaussian variables, mutual information is a monotonic function of the squared Pearson correlation, leading to identical rankings.\n$2$. Mutual information is invariant under invertible (e.g., strictly monotonic) transformations of the marginal variables, while Pearson correlation is not. This means MI captures the underlying dependence structure (the \"copula\") regardless of the marginal distributions.\n\nThe algorithm will proceed by first setting a fixed random seed for reproducibility. Then, for each test case, we will generate the required synthetic data and perform the specified calculations.\n\n**Data Generation**\nFor each edge $i$, we require $n$ samples from a bivariate normal distribution with a specified correlation $\\rho_i$ and unit variances. This can be achieved by first generating $n$ samples for two independent standard normal variables, $Z_1 \\sim \\mathcal{N}(0,1)$ and $Z_2 \\sim \\mathcal{N}(0,1)$. The desired correlated variables, $X_i$ and $Y_i$, can then be constructed as linear combinations:\n$$X_i = Z_1$$\n$$Y_i = \\rho_i Z_1 + \\sqrt{1 - \\rho_i^2} Z_2$$\nThis construction ensures that $\\operatorname{Var}(X_i)=1$, $\\operatorname{Var}(Y_i) = \\rho_i^2 \\operatorname{Var}(Z_1) + (1-\\rho_i^2)\\operatorname{Var}(Z_2) = \\rho_i^2 + 1 - \\rho_i^2 = 1$, and $\\operatorname{Cov}(X_i, Y_i) = \\operatorname{E}[X_i Y_i] = \\operatorname{E}[Z_1 (\\rho_i Z_1 + \\sqrt{1-\\rho_i^2} Z_2)] = \\rho_i \\operatorname{E}[Z_1^2] = \\rho_i$. Thus, the correlation is $\\rho(X_i, Y_i) = \\rho_i$.\n\n**Case A: Gaussian Data**\nFor this case, we simulate $m=4$ edge datasets with $n=2000$ samples each, using the true correlations $\\rho_i \\in \\{0.0, 0.2, 0.5, 0.8\\}$. For each edge $i$, we perform the following steps:\n$1$. Generate the data matrix $(X_i, Y_i)$.\n$2$. Compute the sample Pearson correlation coefficient, $\\hat{\\rho}_i$, from the data and take its absolute value, $\\lvert \\hat{\\rho}_i \\rvert$.\n$3$. Compute a score for mutual information using the formula for Gaussian variables, $I_i = -\\frac{1}{2}\\log(1 - \\hat{\\rho}_i^2)$.\nThe function $f(r) = -\\frac{1}{2}\\log(1 - r^2)$ is strictly monotonically increasing for $r \\in [0, 1)$. Therefore, for any two scores $\\lvert \\hat{\\rho}_a \\rvert$ and $\\lvert \\hat{\\rho}_b \\rvert$, if $\\lvert \\hat{\\rho}_a \\rvert > \\lvert \\hat{\\rho}_b \\rvert$, it is guaranteed that $I_a > I_b$. Consequently, ranking the edges in descending order of $\\lvert \\hat{\\rho}_i \\rvert$ must yield the exact same ordering as ranking them by $I_i$. The programmatic-ally determined result should be `True`.\n\n**Case B and Case C: Non-Gaussian Marginals**\nThese cases illustrate the divergence of the two metrics. For each edge $i$, the process is as follows:\n$1$. Generate the latent Gaussian data $(X_i, Y_i)$ with $n$ samples and a specified latent correlation $\\rho_i$.\n$2$. Apply the specified pair of strictly monotonic, invertible transformations $(f_k(x), g_k(y))$ to the marginals to obtain the observed data $(X'_i, Y'_i) = (f_k(X_i), g_k(Y_i))$. For Case B, $m=4$ and for Case C, $m=5$.\n$3$. **Correlation-based Ranking**: Compute the sample Pearson correlation $\\widehat{\\rho}'_i$ from the transformed data $(X'_i, Y'_i)$. The score for this ranking is $\\lvert \\widehat{\\rho}'_i \\rvert$. These transformations are generally non-linear and will distort the linear relationship, so $\\widehat{\\rho}'_i$ will differ from $\\rho_i$.\n$4$. **MI-based Ranking**: The problem correctly leverages the MI invariance property. Since the transformations are invertible, $I(X'_i; Y'_i) = I(X_i; Y_i)$. The mutual information of the original latent Gaussian variables is known from the generating parameter $\\rho_i$: $I(X_i; Y_i) = -\\frac{1}{2}\\log(1 - \\rho_i^2)$. We use this value as the MI score for edge $i$. This represents an ideal MI estimator that has access to the true underlying dependence structure.\n$5$. **Divergence Calculation**: We now have two sets of scores for the $m$ edges: one from the empirical correlation on transformed data, $\\{\\lvert \\widehat{\\rho}'_i \\rvert\\}_{i=1}^m$, and one from the theoretical MI, $\\{I_i\\}_{i=1}^m$. We create two ranked lists of edge indices based on these scores. We then quantify the discordance between these two rankings using the normalized inversion fraction, $D$. This is computed by counting the number of pairs of edges $(j, k)$ whose relative order is different in the two rankings, and dividing by the total number of unique pairs, $\\binom{m}{2}$. For two score lists, `scores_1` and `scores_2` of length $m$, the number of discordant pairs can be calculated by iterating through all pairs of indices $j < k$ and checking if $(\\text{scores\\_1}[j] > \\text{scores\\_1}[k]) \\neq (\\text{scores\\_2}[j] > \\text{scores\\_2}[k])$.\nThe final value of $D$ is then rounded to $6$ decimal places. The expected result is $D > 0$, demonstrating that Pearson correlation can give a misleading ranking of dependencies when the data are not jointly Gaussian, even if the underlying dependence structure (copula) is simple.\n\nThe program will encapsulate this logic, run the three test cases, and print the results in the specified format.\n\n```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Implements the full logic to compare network edge rankings by correlation\n    and mutual information for Gaussian and non-Gaussian data.\n    \"\"\"\n    \n    # Use a fixed random seed for reproducibility.\n    RANDOM_SEED = 42\n\n    # Define the monotonic transformation functions as specified in the problem.\n    def f1(x): return np.log(1 + np.exp(np.clip(x, -20, 20)))\n    def g1(y): return np.log(1 + np.exp(np.clip(y, -20, 20)))\n    \n    def f2(x): return np.log(1 + np.exp(np.clip(x, -20, 20)))\n    def g2(y): return np.sinh(np.clip(y, -4, 4))\n    \n    def f3(x): return np.log(1 + np.exp(np.clip(x, -20, 20)))\n    def g3(y): return np.tanh(y)\n    \n    def f4(x): return np.tanh(x)\n    def g4(y): return np.power(y, 3)\n    \n    def f5(x): return np.sinh(np.clip(x, -4, 4))\n    def g5(y): return np.log(1 + np.exp(np.clip(y, -20, 20)))\n    \n    transforms_list = [(f1, g1), (f2, g2), (f3, g3), (f4, g4), (f5, g5)]\n\n    # Define the test cases.\n    # Each case is a tuple: (case_id, n_samples, n_edges, rho_values, is_gaussian)\n    test_cases = [\n        ('A', 2000, 4, [0.0, 0.2, 0.5, 0.8], True),\n        ('B', 2000, 4, [0.0, 0.2, 0.5, 0.8], False),\n        ('C', 200, 5, [0.1, 0.95, 0.6, 0.3, 0.85], False)\n    ]\n\n    results = []\n    \n    # Helper to generate bivariate normal data.\n    def generate_gaussian_data(n, rho, rng):\n        z1 = rng.standard_normal(n)\n        z2 = rng.standard_normal(n)\n        x = z1\n        y = rho * z1 + np.sqrt(1 - rho**2) * z2\n        return x, y\n\n    # Helper to calculate normalized inversion fraction.\n    def calculate_inversion_fraction(scores1, scores2):\n        m = len(scores1)\n        if m  2:\n            return 0.0\n        \n        discordant_pairs = 0\n        for i in range(m):\n            for j in range(i + 1, m):\n                s1_order = scores1[i] > scores1[j]\n                s2_order = scores2[i] > scores2[j]\n                if s1_order != s2_order:\n                    # Check for ties. A tie in one list but not the other is a partial discordance.\n                    # This standard definition counts it as a full discordance.\n                    if (scores1[i] != scores1[j]) and (scores2[i] != scores2[j]):\n                         discordant_pairs += 1\n        \n        total_pairs = m * (m - 1) / 2\n        return discordant_pairs / total_pairs\n\n    # Master random number generator\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    for case_id, n, m, rhos, is_gaussian in test_cases:\n        corr_scores = []\n        mi_scores = []\n\n        for i in range(m):\n            rho = rhos[i]\n            x_latent, y_latent = generate_gaussian_data(n, rho, rng)\n\n            if is_gaussian:\n                # Case A: Gaussian data\n                x_obs, y_obs = x_latent, y_latent\n                \n                # Estimate correlation from data\n                corr_matrix = np.corrcoef(x_obs, y_obs)\n                est_rho = corr_matrix[0, 1]\n                corr_scores.append(np.abs(est_rho))\n                \n                # Calculate MI from estimated rho\n                # np.clip is used to prevent log(0) if |est_rho| is 1.0\n                mi_scores.append(-0.5 * np.log(1 - np.clip(est_rho**2, 0, 1-1e-15)))\n\n            else:\n                # Case B  C: Non-Gaussian marginals\n                f, g = transforms_list[i]\n                x_obs = f(x_latent)\n                y_obs = g(y_latent)\n\n                # Estimate correlation from transformed data\n                corr_matrix = np.corrcoef(x_obs, y_obs)\n                est_rho_prime = corr_matrix[0, 1]\n                corr_scores.append(np.abs(est_rho_prime))\n                \n                # Calculate MI from the TRUE latent rho, based on invariance\n                mi_scores.append(-0.5 * np.log(1 - np.clip(rho**2, 0, 1-1e-15)))\n\n        if is_gaussian:\n            # For Case A, check if rankings are identical.\n            # Since I(rho) is monotonic in |rho|, the rankings must be identical.\n            # We sort indices based on scores and compare the sorted lists of indices.\n            rank_corr = np.argsort(corr_scores)\n            rank_mi = np.argsort(mi_scores)\n            results.append(np.array_equal(rank_corr, rank_mi))\n        else:\n            # For Cases B and C, calculate the normalized inversion fraction.\n            divergence = calculate_inversion_fraction(corr_scores, mi_scores)\n            results.append(round(divergence, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{results[0]},{results[1]:.6f},{results[2]:.6f}]\")\n\nsolve()\n```", "answer": "$$\n[\\text{True}, 0.333333, 0.300000]\n$$", "id": "4365189"}, {"introduction": "A primary challenge in network inference is distinguishing direct interactions from indirect ones. For instance, a strong correlation between genes $X$ and $Y$ might not reflect a direct link but could arise because both are regulated by a third gene, $Z$. This exercise tackles this problem head-on by using Conditional Mutual Information (CMI), $I(X;Y \\mid Z)$, to prune indirect edges. You will implement a complete inference pipeline to test for mediation in simulated gene triplets, combining the elegance of information theory with the rigor of a permutation-based significance test to decide whether an edge represents a direct or a mediated association [@problem_id:4365154].", "problem": "You are given four synthetic gene-expression triplets $(X,Y,Z)$ generated under zero-mean trivariate Gaussian models with specified correlation matrices. The task is to infer whether the undirected edge between genes $(X,Y)$ should be retained in a gene regulatory network when a potential mediator $Z$ is considered. The decision relies on testing whether the Mutual Information (MI) between $X$ and $Y$ is significantly reduced after conditioning on $Z$, using a permutation-based significance assessment.\n\nStarting from the foundational definitions of differential entropy and Mutual Information (MI) for continuous random variables, assume the following modeling and estimation framework:\n\n- The joint distribution of $(X,Y,Z)$ is trivariate normal with an unknown covariance matrix $\\boldsymbol{\\Sigma}$.\n- Use the maximum likelihood estimators for the covariance based on samples to compute:\n  1. The Mutual Information (MI) $I(X;Y)$ from the estimated Pearson correlation between $X$ and $Y$ under the Gaussian model.\n  2. The Conditional Mutual Information (CMI) $I(X;Y \\mid Z)$ from the estimated partial correlation between $X$ and $Y$ controlling for $Z$ under the Gaussian model.\n- Implement a non-parametric permutation test for mediation: form the empirical null distribution of the reduction $\\Delta = I(X;Y) - I(X;Y \\mid Z)$ by randomly permuting the samples of $Z$ across observations, recomputing $I(X;Y \\mid Z^{\\pi})$ and $\\Delta^{\\pi}$ for each permutation (where $Z^{\\pi}$ denotes a permuted mediator), and estimate the empirical $p$-value as the fraction of permutations for which $\\Delta^{\\pi} \\ge \\Delta$. If the $p$-value is strictly less than a significance level $\\alpha$, declare the reduction significant and the edge $(X,Y)$ should be removed; otherwise retain the edge.\n\nYour program must implement the above logic and apply it to the following test suite. For each test case, generate $n$ independent samples from $\\mathcal{N}(\\boldsymbol{0}, \\mathbf{R})$, where $\\mathbf{R}$ is a correlation matrix with unit variances on the diagonal. Use the specified number of permutations $B$ and significance level $\\alpha$. Use a fixed random seed for reproducibility.\n\nTest Suite (each $\\mathbf{R}$ is expressed as a correlation matrix):\n1. Case A (direct association persists after conditioning):\n   $$\n   n = 500,\\quad\n   \\mathbf{R} =\n   \\begin{bmatrix}\n   1  0.8  0.2 \\\\\n   0.8  1  0.2 \\\\\n   0.2  0.2  1\n   \\end{bmatrix},\\quad\n   B = 300,\\quad\n   \\alpha = 0.05.\n   $$\n2. Case B (correlation fully mediated by $Z$):\n   $$\n   n = 500,\\quad\n   \\mathbf{R} =\n   \\begin{bmatrix}\n   1  0.49  0.7 \\\\\n   0.49  1  0.7 \\\\\n   0.7  0.7  1\n   \\end{bmatrix},\\quad\n   B = 300,\\quad\n   \\alpha = 0.05.\n   $$\n3. Case C (near-boundary partial mediation):\n   $$\n   n = 300,\\quad\n   \\mathbf{R} =\n   \\begin{bmatrix}\n   1  0.3  0.4 \\\\\n   0.3  1  0.4 \\\\\n   0.4  0.4  1\n   \\end{bmatrix},\\quad\n   B = 300,\\quad\n   \\alpha = 0.05.\n   $$\n4. Case D (small sample, weak mediator):\n   $$\n   n = 60,\\quad\n   \\mathbf{R} =\n   \\begin{bmatrix}\n   1  0.3  0.05 \\\\\n   0.3  1  0.05 \\\\\n   0.05  0.05  1\n   \\end{bmatrix},\\quad\n   B = 200,\\quad\n   \\alpha = 0.05.\n   $$\n\nAlgorithmic requirements:\n- Estimate the sample covariance $\\boldsymbol{\\hat{\\Sigma}}$ for each case from the simulated data and compute the Pearson correlation between $X$ and $Y$.\n- Compute the partial correlation between $X$ and $Y$ given $Z$ via the precision matrix $\\mathbf{P} = \\boldsymbol{\\hat{\\Sigma}}^{-1}$.\n- Under the Gaussian model, compute $I(X;Y)$ and $I(X;Y \\mid Z)$ from the correlation and partial correlation respectively.\n- Perform the permutation test with $B$ permutations to compute the empirical $p$-value for the reduction in MI, and decide edge retention: return $\\,\\text{True}\\,$ if the edge $(X,Y)$ is retained, and $\\,\\text{False}\\,$ if it is removed.\n- Use a fixed random seed to ensure reproducibility across runs.\n\nYour program should produce a single line of output containing the four boolean decisions for Cases A–D, as a comma-separated list enclosed in square brackets, for example, $[\\,\\text{True},\\text{False},\\text{True},\\text{False}\\,]$. No physical units or angles are involved, and all outputs are booleans.", "solution": "The problem requires the implementation of a statistical procedure to decide whether to retain or remove an edge between two genes, $X$ and $Y$, in a regulatory network, based on the potential mediating effect of a third gene, $Z$. The framework is based on mutual information under a trivariate Gaussian model, with significance assessed via a permutation test.\n\n### 1. Theoretical Foundation: Mutual Information for Gaussian Variables\n\nThe foundation of this analysis is information theory. For a continuous $d$-dimensional random variable $\\mathbf{X}$ following a multivariate normal distribution $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, its differential entropy $h(\\mathbf{X})$ is given by:\n$$\nh(\\mathbf{X}) = \\frac{1}{2} \\ln \\left( (2\\pi e)^d \\det(\\boldsymbol{\\Sigma}) \\right)\n$$\nMutual Information (MI) quantifies the statistical dependence between two variables. For variables $X$ and $Y$, it is defined as $I(X;Y) = h(X) + h(Y) - h(X,Y)$. Assuming $(X,Y)$ are jointly Gaussian with variances $\\sigma_X^2=1$, $\\sigma_Y^2=1$, and Pearson correlation $\\rho_{XY}$, the MI simplifies to a direct function of the correlation:\n$$\nI(X;Y) = -\\frac{1}{2} \\ln(1 - \\rho_{XY}^2)\n$$\nThis formula shows that MI increases as the magnitude of the correlation $|\\rho_{XY}|$ approaches $1$.\n\n### 2. Conditional Mutual Information and Mediation\n\nConditional Mutual Information (CMI), $I(X;Y \\mid Z)$, measures the dependence between $X$ and $Y$ given knowledge of $Z$. For Gaussian variables, a powerful result connects CMI to partial correlation, $\\rho_{XY \\mid Z}$, which is the correlation between $X$ and $Y$ after regressing out the linear effect of $Z$. The formula is analogous to the MI formula:\n$$\nI(X;Y \\mid Z) = -\\frac{1}{2} \\ln(1 - \\rho_{XY \\mid Z}^2)\n$$\nIf $Z$ fully mediates the relationship between $X$ and $Y$, then $X$ and $Y$ are conditionally independent given $Z$, which implies $\\rho_{XY \\mid Z} = 0$ and thus $I(X;Y \\mid Z) = 0$. This provides a clear criterion for identifying mediation: a significant drop in mutual information upon conditioning on the mediator $Z$.\n\n### 3. Estimation from Sample Data\n\nIn a practical setting, the true covariance matrix $\\mathbf{R}$ is unknown and must be estimated from $n$ samples of $(X,Y,Z)$. Let the $n \\times 3$ data matrix be $\\mathbf{D}$.\n\n-   **Sample Covariance and Correlation**: The maximum likelihood estimate (MLE) of the covariance matrix is $\\boldsymbol{\\hat{\\Sigma}} = \\frac{1}{n} (\\mathbf{D}-\\bar{\\mathbf{D}})^T(\\mathbf{D}-\\bar{\\mathbf{D}})$, where $\\bar{\\mathbf{D}}$ contains the sample means. The sample Pearson correlation $\\hat{\\rho}_{XY}$ is calculated from the elements of $\\boldsymbol{\\hat{\\Sigma}}$. We then compute the estimated MI: $\\hat{I}(X;Y) = -\\frac{1}{2}\\ln(1-\\hat{\\rho}_{XY}^2)$.\n\n-   **Sample Partial Correlation**: The partial correlation $\\hat{\\rho}_{XY \\mid Z}$ is estimated using the sample precision matrix, $\\mathbf{P} = \\boldsymbol{\\hat{\\Sigma}}^{-1}$. The formula is:\n    $$\n    \\hat{\\rho}_{XY \\mid Z} = -\\frac{p_{12}}{\\sqrt{p_{11} p_{22}}}\n    $$\n    where $p_{ij}$ are the elements of $\\mathbf{P}$. The estimated CMI is then $\\hat{I}(X;Y \\mid Z) = -\\frac{1}{2}\\ln(1-\\hat{\\rho}_{XY \\mid Z}^2)$.\n\n### 4. Permutation Test for Significance\n\nThe central hypothesis is whether the observed reduction in MI, $\\hat{\\Delta} = \\hat{I}(X;Y) - \\hat{I}(X;Y \\mid Z)$, is statistically significant. A permutation test is employed to generate an empirical null distribution for this reduction. The null hypothesis $H_0$ is that $Z$ is independent of the $(X,Y)$ pair and thus not a mediator.\n\nThe procedure is as follows:\n1.  From the original data, generate $n$ samples from $\\mathcal{N}(\\boldsymbol{0}, \\mathbf{R})$ and compute the observed statistics $\\hat{I}(X;Y)$, $\\hat{I}(X;Y \\mid Z)$, and the observed reduction $\\hat{\\Delta}$.\n2.  Perform $B$ permutations. In each permutation $b \\in \\{1, \\dots, B\\}$:\n    a. Create a shuffled dataset $(X, Y, Z^{\\pi_b})$ where $Z^{\\pi_b}$ is a random permutation of the original $Z$ data vector. This procedure breaks the specific association of each $z_i$ with its corresponding $(x_i, y_i)$ pair, simulating the null hypothesis.\n    b. Compute the CMI for the permuted data, $\\hat{I}_b(X;Y \\mid Z^{\\pi_b})$.\n    c. Calculate the MI reduction for this permutation: $\\hat{\\Delta}^{\\pi_b} = \\hat{I}(X;Y) - \\hat{I}_b(X;Y \\mid Z^{\\pi_b})$. Note that $\\hat{I}(X;Y)$ is the value from the original data and remains constant throughout the permutations.\n3.  The set of values $\\{\\hat{\\Delta}^{\\pi_b}\\}$ forms the empirical null distribution.\n4.  The empirical $p$-value is the fraction of permutations where the reduction was at least as large as the observed reduction:\n    $$\n    p = \\frac{|\\{b \\mid \\hat{\\Delta}^{\\pi_b} \\ge \\hat{\\Delta}\\}|}{B}\n    $$\n5.  Finally, apply the decision rule: if the $p$-value is strictly less than the significance level $\\alpha$, i.e., $p  \\alpha$, the null hypothesis is rejected. This indicates a significant mediation effect, and the edge $(X,Y)$ is removed (returning `False`). Otherwise, for $p \\ge \\alpha$, the evidence for mediation is insufficient, and the edge is retained (returning `True`).\n\nThis complete algorithm is implemented for each of the $4$ test cases provided, using a fixed random seed to ensure reproducible results.\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the network inference test for all specified cases.\n    \"\"\"\n    \n    # Fixed random seed for reproducibility as required by the problem.\n    seed = 42\n    rng = np.random.default_rng(seed)\n\n    # Test suite from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"n\": 500,\n            \"R\": np.array([[1.0, 0.8, 0.2], [0.8, 1.0, 0.2], [0.2, 0.2, 1.0]]),\n            \"B\": 300,\n            \"alpha\": 0.05,\n        },\n        {\n            \"name\": \"Case B\",\n            \"n\": 500,\n            \"R\": np.array([[1.0, 0.49, 0.7], [0.49, 1.0, 0.7], [0.7, 0.7, 1.0]]),\n            \"B\": 300,\n            \"alpha\": 0.05,\n        },\n        {\n            \"name\": \"Case C\",\n            \"n\": 300,\n            \"R\": np.array([[1.0, 0.3, 0.4], [0.3, 1.0, 0.4], [0.4, 0.4, 1.0]]),\n            \"B\": 300,\n            \"alpha\": 0.05,\n        },\n        {\n            \"name\": \"Case D\",\n            \"n\": 60,\n            \"R\": np.array([[1.0, 0.3, 0.05], [0.3, 1.0, 0.05], [0.05, 0.05, 1.0]]),\n            \"B\": 200,\n            \"alpha\": 0.05,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        decision = perform_mediation_test(\n            n=case[\"n\"],\n            R=case[\"R\"],\n            B=case[\"B\"],\n            alpha=case[\"alpha\"],\n            rng=rng\n        )\n        results.append(decision)\n\n    # Print the final result in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef compute_mi_from_corr(rho):\n    \"\"\"Computes mutual information from correlation for Gaussian variables.\"\"\"\n    # Clamp rho**2 to avoid domain errors with log due to floating point inaccuracies.\n    rho2 = np.clip(rho * rho, 0.0, 1.0 - 1e-15)\n    return -0.5 * np.log(1.0 - rho2)\n\n\ndef get_mi_cmi_stats(data_x, data_y, data_z):\n    \"\"\"\n    Computes MI(X;Y) and CMI(X;Y|Z) from sample data.\n    \"\"\"\n    data = np.column_stack((data_x, data_y, data_z))\n    \n    # Compute the sample covariance matrix (MLE, ddof=0).\n    try:\n        cov_matrix = np.cov(data, rowvar=False, ddof=0)\n    except Exception: # Fails if data is constant\n        return 0.0, 0.0\n\n    # ----- MI(X;Y) from Pearson correlation -----\n    var_x, var_y = cov_matrix[0, 0], cov_matrix[1, 1]\n    if var_x  1e-15 or var_y  1e-15:\n        mi_xy = 0.0\n    else:\n        rho_xy = cov_matrix[0, 1] / np.sqrt(var_x * var_y)\n        mi_xy = compute_mi_from_corr(rho_xy)\n\n    # ----- CMI(X;Y|Z) from partial correlation -----\n    try:\n        # Precision matrix is the inverse of the covariance matrix.\n        prec_matrix = np.linalg.inv(cov_matrix)\n    except np.linalg.LinAlgError:\n        # If matrix is singular, CMI is ill-defined. Treat as maximal dependence.\n        # This implies huge reduction, but since it's a numeric error,\n        # we return a value that will cause retention of the edge.\n        return mi_xy, np.inf\n\n    p11, p22, p12 = prec_matrix[0, 0], prec_matrix[1, 1], prec_matrix[0, 1]\n    if p11  1e-15 or p22  1e-15:\n        cmi_xy_z = 0.0\n    else:\n        rho_xy_z = -p12 / np.sqrt(p11 * p22)\n        cmi_xy_z = compute_mi_from_corr(rho_xy_z)\n\n    return mi_xy, cmi_xy_z\n\n\ndef perform_mediation_test(n, R, B, alpha, rng):\n    \"\"\"\n    Performs the full permutation test for a single case.\n    Returns True to retain the edge, False to remove.\n    \"\"\"\n    # 1. Generate data from the specified trivariate Gaussian model.\n    mean = np.zeros(3)\n    data = rng.multivariate_normal(mean, R, size=n)\n    X, Y, Z = data[:, 0], data[:, 1], data[:, 2]\n\n    # 2. Calculate the observed MI reduction.\n    mi_obs, cmi_obs = get_mi_cmi_stats(X, Y, Z)\n\n    # If CMI is infinite due to singularity, the reduction is undefined/negative.\n    # This leads to retaining the edge, which is a safe choice.\n    if np.isinf(cmi_obs):\n        return True\n    \n    delta_obs = mi_obs - cmi_obs\n\n    # 3. Perform permutation test to build the null distribution of the MI reduction.\n    permuted_deltas_ge_observed = 0\n    Z_perm = Z.copy() # Create a copy to shuffle in-place\n    for _ in range(B):\n        rng.shuffle(Z_perm)\n        # For permuted data, we only need CMI. MI(X;Y) is invariant.\n        _, cmi_perm = get_mi_cmi_stats(X, Y, Z_perm)\n\n        # Handle numerical issues in permuted data\n        if np.isinf(cmi_perm):\n            delta_perm = -np.inf # Effectively ensures delta_perm  delta_obs\n        else:\n            delta_perm = mi_obs - cmi_perm\n        \n        if delta_perm >= delta_obs:\n            permuted_deltas_ge_observed += 1\n\n    # 4. Calculate the empirical p-value.\n    p_value = permuted_deltas_ge_observed / B\n\n    # 5. Make the decision: retain if p >= alpha, remove if p  alpha.\n    return p_value >= alpha\n\n\nif __name__ == \"__main__\":\n    solve()\n```", "answer": "$$\n[\\text{True}, \\text{False}, \\text{False}, \\text{True}]\n$$", "id": "4365154"}]}