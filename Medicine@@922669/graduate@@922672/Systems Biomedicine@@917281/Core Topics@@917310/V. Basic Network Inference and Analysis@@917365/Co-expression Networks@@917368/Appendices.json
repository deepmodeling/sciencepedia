{"hands_on_practices": [{"introduction": "The foundation of a co-expression network lies in transforming a matrix of pairwise gene correlations into a weighted adjacency matrix. This practice guides you through the core mechanics of this process, starting from Pearson correlations ($r_{ij}$), applying a soft-thresholding power ($\\beta$) to compute adjacency values, and finally calculating the Topological Overlap Measure (TOM). Mastering this fundamental calculation [@problem_id:2854762] is the first step toward building robust networks from transcriptomic data.", "problem": "In systems genetics, gene co-expression networks are constructed by transforming pairwise gene-gene correlations into a weighted adjacency, and then quantifying node proximity via the Topological Overlap Measure (TOM). Consider three genes, $G_1$, $G_2$, and $G_3$, with a symmetric matrix of pairwise Pearson correlations given by $r_{12} = 0.8$, $r_{13} = 0.3$, and $r_{23} = -0.4$, and $r_{ii} = 1$ for $i \\in \\{1,2,3\\}$. Use the unsigned Weighted Gene Co-expression Network Analysis (WGCNA) convention with soft-thresholding power $\\beta = 6$ to construct the weighted adjacency, and then compute the weighted Topological Overlap Measure (TOM) between $G_1$ and $G_2$. Assume the standard WGCNA convention that self-adjacencies satisfy $a_{ii} = 0$.\n\nPerform the following:\n- From the given pairwise correlations, compute all off-diagonal adjacency entries $a_{ij}$.\n- Using the standard weighted definition of TOM used in Weighted Gene Co-expression Network Analysis (WGCNA), compute the TOM between $G_1$ and $G_2$.\n\nReport as your final answer only the TOM between $G_1$ and $G_2$, rounded to four significant figures. No units are required.", "solution": "The problem statement is scientifically grounded, well-posed, and provides all necessary information for a unique solution. It is a standard application of the Weighted Gene Co-expression Network Analysis (WGCNA) methodology. We will proceed with the calculation.\n\nThe first step is to construct the weighted adjacency matrix, $A = [a_{ij}]$, from the given Pearson correlation matrix, $R = [r_{ij}]$. For an unsigned network, the adjacency $a_{ij}$ between genes $i$ and $j$ is calculated using a soft-thresholding power $\\beta$. The formula is:\n$$a_{ij} = |r_{ij}|^{\\beta}$$\nThe problem provides the soft-thresholding power $\\beta = 6$. The given non-diagonal Pearson correlations are $r_{12} = 0.8$, $r_{13} = 0.3$, and $r_{23} = -0.4$. The adjacency matrix is symmetric ($a_{ij} = a_{ji}$), and the problem specifies the diagonal elements are zero ($a_{ii} = 0$). We compute the required off-diagonal adjacency entries:\n$$a_{12} = |r_{12}|^{\\beta} = |0.8|^{6} = (0.8)^{6} = 0.262144$$\n$$a_{13} = |r_{13}|^{\\beta} = |0.3|^{6} = (0.3)^{6} = 0.000729$$\n$$a_{23} = |r_{23}|^{\\beta} = |-0.4|^{6} = (0.4)^{6} = 0.004096$$\n\nThe second step is to compute the Topological Overlap Measure (TOM) between genes $G_1$ and $G_2$, denoted as $\\omega_{12}$. The standard formula for the weighted TOM is:\n$$\\omega_{ij} = \\frac{l_{ij} + a_{ij}}{\\min(k_i, k_j) + 1 - a_{ij}}$$\nHere, $k_i$ is the total network connectivity (or node degree) of gene $i$, and $l_{ij}$ represents the shared connectivity between genes $i$ and $j$. These terms are defined as:\n$$k_i = \\sum_{u} a_{iu}$$\n$$l_{ij} = \\sum_{u} a_{iu}a_{ju}$$\nWe must calculate these quantities for the pair $(i, j) = (1, 2)$.\n\nFirst, we compute the connectivities $k_1$ and $k_2$:\n$$k_1 = \\sum_{u} a_{1u} = a_{12} + a_{13} = 0.262144 + 0.000729 = 0.262873$$\n$$k_2 = \\sum_{u} a_{2u} = a_{21} + a_{23} = a_{12} + a_{23} = 0.262144 + 0.004096 = 0.26624$$\n\nNext, we calculate the shared connectivity term $l_{12}$. In this three-gene system, since self-adjacencies are zero, the only non-zero contribution to the sum $\\sum_u a_{1u}a_{u2}$ comes from the path through node $u=3$.\n$$l_{12} = a_{13} a_{32} = a_{13} a_{23}$$\n$$l_{12} = (0.000729) \\times (0.004096) = 0.000002985984 = 2.985984 \\times 10^{-6}$$\n\nNow we have all components to calculate $\\omega_{12}$. The numerator is:\n$$l_{12} + a_{12} = 2.985984 \\times 10^{-6} + 0.262144 = 0.262146985984$$\nThe denominator requires $\\min(k_1, k_2)$:\n$$\\min(k_1, k_2) = \\min(0.262873, 0.26624) = 0.262873$$\nSo, the denominator is:\n$$\\min(k_1, k_2) + 1 - a_{12} = 0.262873 + 1 - 0.262144 = 1.000729$$\n\nFinally, we compute the TOM value $\\omega_{12}$:\n$$\\omega_{12} = \\frac{0.262146985984}{1.000729} \\approx 0.26196232$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\\omega_{12} \\approx 0.2620$$\nThis value represents the topological overlap between genes $G_1$ and $G_2$, which quantifies their similarity based on both their direct connection and their shared connections with other genes in the network.", "answer": "$$\\boxed{0.2620}$$", "id": "2854762"}, {"introduction": "While adjacency captures direct correlation, it can be sensitive to noise and indirect effects. The Topological Overlap Measure (TOM) provides a more robust measure of connection by considering the extent to which two genes share neighbors. This exercise [@problem_id:4328763] offers a conceptual and quantitative exploration of why TOM is a critical tool for network refinement, demonstrating how it down-weights spurious connections while reinforcing edges that are part of coherent topological modules.", "problem": "You are tasked with formalizing, from first principles, why the Topological Overlap Measure (TOM) reduces spurious connections in weighted co-expression networks that arise from single high-correlation edges without broader topological support, and then quantifying this effect on several small, scientifically plausible test networks. Begin from the following foundational bases that are standard in systems biomedicine and gene co-expression analysis.\n\nBase definitions and assumptions:\n- A gene co-expression network is an undirected weighted graph over $n$ genes, where the weight between gene $i$ and gene $j$ is a function of their pairwise Pearson correlation $r_{ij}$, with $-1 \\le r_{ij} \\le 1$ and $r_{ii} = 1$ by definition.\n- A soft-thresholded adjacency for a Weighted Gene Co-expression Network Analysis (WGCNA)-style network is constructed as $a_{ij} = |r_{ij}|^\\beta$ for $i \\ne j$, with $a_{ii} = 0$ and $\\beta > 0$. This is a well-tested approach for emphasizing strong correlations while preserving continuous weights.\n- The Topological Overlap Measure (TOM) between two nodes is a function that depends on the weighted shared neighborhood overlap and their node degrees. It is widely used to refine adjacency by accounting for network topology rather than pairwise correlation alone.\n\nYour program must implement the following, without relying on any external input:\n1. Given a symmetric correlation matrix $R \\in \\mathbb{R}^{n \\times n}$ with $R_{ii} = 1$ and $|R_{ij}| \\le 1$, and a soft-threshold exponent $\\beta > 0$, compute the weighted adjacency $A$ as $A_{ij} = |R_{ij}|^\\beta$ for $i \\ne j$ and $A_{ii} = 0$.\n2. Compute the Weighted Topological Overlap Measure (TOM) matrix $T$ from $A$ based on the standard neighborhood-overlap construction that incorporates shared neighbor connectivity and node degrees. Use the symmetric formulation that depends on the pairwise shared-neighbor sum and a denominator that includes the smaller of the two degrees and a normalization term that prevents inflation when there is no shared neighborhood support.\n3. For a designated edge $(p,q)$ in each test network, compute the reduction factor $r_{pq} = T_{pq} / A_{pq}$, which quantifies how TOM modifies the raw adjacency for that edge. This factor is a dimensionless float.\n\nConceptual target:\n- Provide a conceptual argument, supported by the mathematics codified in your implementation, that explains why TOM down-weights edges that lack shared neighbors despite having high pairwise correlation, and why TOM tends to preserve edges that are embedded within shared neighborhoods. The argument should derive its logic from the interplay between shared neighbor connectivity, node degrees, and normalization.\n\nTest suite:\nUse $\\beta = 6$ for all test cases and compute $r_{pq}$ for the designated $(p,q)$ in each case (note: edges are specified with 1-based indices).\n\n- Test case $1$ (toy network with a single outlier edge and disjoint neighbor sets, $n=6$): \n  $$R^{(1)} = \\begin{pmatrix}\n  1 & 0.1 & 0.1 & 0.1 & 0.6 & 0.1 \\\\\n  0.1 & 1 & 0.1 & 0.1 & 0.6 & 0.1 \\\\\n  0.1 & 0.1 & 1 & 0.1 & 0.1 & 0.6 \\\\\n  0.1 & 0.1 & 0.1 & 1 & 0.1 & 0.6 \\\\\n  0.6 & 0.6 & 0.1 & 0.1 & 1 & 0.9 \\\\\n  0.1 & 0.1 & 0.6 & 0.6 & 0.9 & 1\n  \\end{pmatrix},\\quad (p,q) = (5,6).$$\n  This network contains a single high-correlation edge (5,6) with $R_{56} = 0.9$, while nodes 5 and 6 have moderate correlations to disjoint sets of neighbors, resulting in no shared neighbors between them.\n\n- Test case $2$ (embedded module edge with strong shared neighbors, $n=6$):\n  $$R^{(2)} = \\begin{pmatrix}\n  1 & 0.9 & 0.9 & 0.9 & 0.1 & 0.1 \\\\\n  0.9 & 1 & 0.9 & 0.9 & 0.1 & 0.1 \\\\\n  0.9 & 0.9 & 1 & 0.85 & 0.1 & 0.1 \\\\\n  0.9 & 0.9 & 0.85 & 1 & 0.1 & 0.1 \\\\\n  0.1 & 0.1 & 0.1 & 0.1 & 1 & 0.1 \\\\\n  0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 1\n  \\end{pmatrix},\\quad (p,q) = (1,2).$$\n  Nodes (1,2) share strong neighbors (3,4), forming an embedded module-like structure.\n\n- Test case $3$ (uniform network baseline, $n=5$):\n  $$R^{(3)}_{ij} = \\begin{cases} 1 & \\text{if } i=j, \\\\ 0.8 & \\text{if } i \\ne j, \\end{cases} \\quad (p,q) = (1,2).$$\n  This case examines behavior when all edges are equally strong.\n\n- Test case $4$ (boundary case with a single nonzero edge, $n=5$):\n  $$R^{(4)}_{ij} = \\begin{cases}\n  1 & \\text{if } i=j, \\\\\n  0.9 & \\text{if } \\{i,j\\} = \\{4,5\\}, \\\\\n  0 & \\text{otherwise},\n  \\end{cases} \\quad (p,q) = (4,5).$$\n  This case tests the limit where there is no network context beyond the single edge.\n\nFinal output specification:\n- Your program should produce a single line of output containing the four reduction factors as a comma-separated list enclosed in square brackets, in the order of test cases $1$ through $4$, with each factor rounded to six decimal places (e.g., $[0.914999,1.000000,1.000000,1.000000]$). No additional text may be printed.", "solution": "The problem is valid as it is scientifically grounded in the principles of network biology, specifically Weighted Gene Co-expression Network Analysis (WGCNA), is well-posed with all necessary data provided, and is mathematically formalizable.\n\nThe objective is to formalize the mathematical principles behind the Topological Overlap Measure (TOM) and quantify its effect on re-weighting network edges based on their neighborhood context. We will analyze why TOM tends to suppress the weight of edges that lack shared neighborhood support, while preserving or even amplifying the weight of edges embedded within well-connected modules. We will use the standard formulation of TOM and apply it to several test networks.\n\nLet the network be defined over a set of $n$ genes. The analysis proceeds in three main steps:\n\n**1. From Correlation to Adjacency**\n\nThe initial input is a symmetric Pearson correlation matrix $R \\in \\mathbb{R}^{n \\times n}$, where $R_{ij} = r_{ij}$ is the correlation between the expression profiles of gene $i$ and gene $j$. By definition, $r_{ii} = 1$ and $-1 \\le r_{ij} \\le 1$.\n\nTo construct a weighted co-expression network, we first transform the correlation matrix $R$ into an adjacency matrix $A$ using a soft-thresholding function. The adjacency $A_{ij} = a_{ij}$ between two distinct genes $i$ and $j$ is defined as:\n$$\na_{ij} = |r_{ij}|^\\beta \\quad \\text{for } i \\ne j\n$$\nHere, $\\beta > 0$ is the soft-thresholding power. This operation serves to amplify strong correlations (where $|r_{ij}|$ is close to $1$) and suppress weak correlations (where $|r_{ij}|$ is close to $0$), thereby emphasizing the most significant biological relationships while maintaining the continuous nature of connection strengths. By convention, we set self-connections to zero, $a_{ii} = 0$, to avoid self-loops in topological calculations. For all test cases, we use the specified value $\\beta = 6$.\n\n**2. The Topological Overlap Measure (TOM)**\n\nThe adjacency matrix $A$ only captures the direct relationship between two genes. The core idea of TOM is to refine this measure by incorporating information about the shared network topology of the two genes. The TOM between genes $i$ and $j$, denoted $T_{ij}$, is defined based on the concept of shared neighbors.\n\nThe standard symmetric formulation for the weighted TOM is:\n$$\nT_{ij} = \\frac{l_{ij} + a_{ij}}{\\min(k_i, k_j) + 1 - a_{ij}}\n$$\nEach component of this formula has a distinct topological meaning:\n- $a_{ij}$: This is the direct connection strength between nodes $i$ and $j$, as defined in the adjacency matrix.\n- $k_i = \\sum_{u} a_{iu}$: This is the total connectivity, or weighted degree, of node $i$. It represents the sum of connection strengths from node $i$ to all other nodes in the network.\n- $l_{ij} = \\sum_{u} a_{iu} a_{uj}$: This term quantifies the extent of shared neighborhood connectivity between nodes $i$ and $j$. It is the sum of weighted paths of length two between $i$ and $j$. Note that since $a_{uu}=0$, node $u$ cannot be $i$ or $j$ for a path to have distinct intermediate nodes. Computationally, the matrix of all $l_{ij}$ values is given by the matrix product $L = A^2 = A \\cdot A$.\n\nThe numerator, $l_{ij} + a_{ij}$, represents the total topological relationship between $i$ and $j$, combining their direct connection ($a_{ij}$) and their indirect connection through all possible shared one-step neighbors ($l_{ij}$).\n\nThe denominator, $\\min(k_i, k_j) + 1 - a_{ij}$, serves as a normalization factor. It ensures that $T_{ij}$ is bounded (typically within $[0, 1]$). It relates the shared connectivity $l_{ij}$ to the maximum possible shared connectivity, which is limited by the smaller of the two node connectivities, $\\min(k_i, k_j)$. The term $1 - a_{ij}$ is a crucial stabilizer that prevents the denominator from becoming zero when nodes have low connectivity.\n\n**3. Quantifying the Effect of TOM: The Re-weighting Factor**\n\nTo analyze how TOM modifies the initial adjacency, we compute a re-weighting factor, defined in the problem as $r_{pq} = T_{pq} / A_{pq}$ for a designated edge $(p,q)$. This factor quantifies the change in edge weight from the raw adjacency $A_{pq}$ to the topologically informed measure $T_{pq}$.\n\n- If $r_{pq} < 1$, TOM has down-weighted or \"penalized\" the edge.\n- If $r_{pq} = 1$, TOM has preserved the edge weight.\n- If $r_{pq} > 1$, TOM has up-weighted or \"amplified\" the edge.\n\nThe behavior of this factor is dictated by the interplay between shared neighbors and overall connectivity:\n\n- **Case of an isolated high-correlation edge (Test Cases 1 & 4):**\nConsider two nodes $p$ and $q$ with a high adjacency $A_{pq}$ but few or no shared neighbors. This means $l_{pq} \\approx 0$. The TOM formula becomes $T_{pq} \\approx \\frac{A_{pq}}{\\min(k_p, k_q) + 1 - A_{pq}}$. The connectivities $k_p$ and $k_q$ are composed of the direct edge $A_{pq}$ plus connections to their other, largely disjoint, neighbors. If these other connections are significant, $\\min(k_p, k_q)$ will be substantially larger than $A_{pq}$. This makes the denominator large, resulting in $T_{pq} < A_{pq}$ and a re-weighting factor $r_{pq} < 1$. This is the mechanism by which TOM reduces the influence of \"spurious\" connections that lack broader topological support. Test Case 1 is designed to illustrate this, where nodes 5 and 6 have a strong direct link but connect to disjoint parts of the network, resulting in a reduction. In the extreme scenario of Test Case 4, where the edge (4,5) is the *entire* network, there is no other topological context. Here, $k_p = k_q = A_{pq}$ and $l_{pq}=0$, which yields $T_{pq} = \\frac{A_{pq}}{A_{pq} + 1 - A_{pq}} = A_{pq}$. Thus, $r_{pq}=1$. TOM acts neutrally when there is no external topology to consider.\n\n- **Case of an edge embedded in a module (Test Cases 2 & 3):**\nConsider two nodes $p$ and $q$ that not only have a high adjacency $A_{pq}$ but also share many strongly connected neighbors. This is typical of nodes within a functional module. In this case, the shared neighbor term $l_{pq}$ is large.\nThe factor $r_{pq} = \\frac{l_{pq} + A_{pq}}{A_{pq}(\\min(k_p, k_q) + 1 - A_{pq})} = \\frac{l_{pq}/A_{pq} + 1}{\\min(k_p, k_q) + 1 - A_{pq}}$. If the shared connections are strong, $l_{pq}$ can be substantial relative to the connectivities, leading to $r_{pq} \\ge 1$. In Test Case 2, nodes 1 and 2 are part of a dense cluster, leading to a large $l_{01}$, which results in an amplification of the edge weight ($r_{01} > 1$). This shows that TOM reinforces connections that are structurally central to a neighborhood. In the perfectly uniform network of Test Case 3, every edge has identical and maximal topological support relative to the graph structure. The formula elegantly simplifies such that $T_{pq} = A_{pq}$, yielding $r_{pq}=1$.\n\nIn summary, TOM is not merely a reduction mechanism but a sophisticated re-weighting scheme. It uses the principle of neighborhood overlap to assess whether a direct connection is topologically corroborated. It penalizes topologically isolated edges, preserves edges in uniform contexts, and rewards edges central to dense local neighborhoods. This property is what makes TOM a powerful tool for defining and identifying robust network modules.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    def compute_tom_reduction_factor(R: np.ndarray, p: int, q: int, beta: float) -> float:\n        \"\"\"\n        Computes the adjacency matrix, TOM matrix, and the re-weighting factor for a given edge.\n\n        The function implements the following steps:\n        1.  Computes the soft-thresholded adjacency matrix A from the correlation matrix R.\n        2.  Calculates the Topological Overlap Measure (TOM) matrix T from A.\n        3.  Computes the re-weighting factor r_pq = T_pq / A_pq for the specified edge (p,q).\n\n        Args:\n            R (np.ndarray): The n x n symmetric correlation matrix.\n            p (int): The first node index of the target edge (0-based).\n            q (int): The second node index of the target edge (0-based).\n            beta (float): The soft-thresholding power.\n\n        Returns:\n            float: The re-weighting factor T_pq / A_pq.\n        \"\"\"\n        n = R.shape[0]\n\n        # Step 1: Compute the weighted adjacency matrix A\n        # A_ij = |R_ij|^beta for i != j, and A_ii = 0\n        A = np.abs(R)**beta\n        np.fill_diagonal(A, 0)\n\n        # Step 2: Compute the Weighted Topological Overlap Measure (TOM) matrix T\n        \n        # Calculate connectivity (weighted degree) for each node: k_i = sum_u a_iu\n        k = np.sum(A, axis=1)\n\n        # Calculate the shared neighbor term matrix L: L_ij = sum_u (a_iu * a_uj)\n        # This is efficiently calculated as the matrix product of A with itself.\n        L = A @ A\n\n        # Vectorized calculation of the TOM matrix T.\n        # The formula is T_ij = (L_ij + A_ij) / (min(k_i, k_j) + 1 - A_ij)\n        \n        # To vectorize min(k_i, k_j), we create an n x n matrix of these minimums.\n        k_col = k.reshape(-1, 1)\n        k_row = k.reshape(1, -1)\n        min_k_matrix = np.minimum(k_col, k_row)\n        \n        # Numerator of the TOM formula\n        numerator = L + A\n        \n        # Denominator of the TOM formula. This is guaranteed to be positive for A_ij <= 1.\n        denominator = min_k_matrix + 1 - A\n        \n        # Element-wise division to get the TOM matrix T\n        T = numerator / denominator\n        # By convention, the topological overlap of a node with itself is 1.\n        np.fill_diagonal(T, 1)\n\n        # Step 3: Compute the re-weighting factor r_pq = T_pq / A_pq\n        A_pq = A[p, q]\n        T_pq = T[p, q]\n\n        # The problem's test cases all have A_pq > 0.\n        if A_pq == 0:\n            if T_pq == 0:\n                return 1.0  # Convention for no change when both are zero.\n            else:\n                return np.inf # TOM found a path where no direct adjacency existed.\n        \n        factor = T_pq / A_pq\n        return factor\n\n    # Shared parameter for all test cases\n    beta = 6.0\n\n    # Define the test cases from the problem statement.\n    # Note: problem specifies edges with 1-based indexing, so we convert to 0-based here.\n    test_cases = [\n        # Test Case 1: Outlier edge with disjoint neighbors. Edge (5,6) -> indices (4,5)\n        (\n            np.array([\n                [1.0, 0.1, 0.1, 0.1, 0.6, 0.1],\n                [0.1, 1.0, 0.1, 0.1, 0.6, 0.1],\n                [0.1, 0.1, 1.0, 0.1, 0.1, 0.6],\n                [0.1, 0.1, 0.1, 1.0, 0.1, 0.6],\n                [0.6, 0.6, 0.1, 0.1, 1.0, 0.9],\n                [0.1, 0.1, 0.6, 0.6, 0.9, 1.0]\n            ]),\n            (4, 5) # Designated edge (p,q)\n        ),\n        # Test Case 2: Embedded module edge. Edge (1,2) -> indices (0,1)\n        (\n            np.array([\n                [1.0, 0.9, 0.9, 0.9, 0.1, 0.1],\n                [0.9, 1.0, 0.9, 0.9, 0.1, 0.1],\n                [0.9, 0.9, 1.0, 0.85, 0.1, 0.1],\n                [0.9, 0.9, 0.85, 1.0, 0.1, 0.1],\n                [0.1, 0.1, 0.1, 0.1, 1.0, 0.1],\n                [0.1, 0.1, 0.1, 0.1, 0.1, 1.0]\n            ]),\n            (0, 1)\n        ),\n        # Test Case 3: Uniform network baseline. Edge (1,2) -> indices (0,1)\n        (\n            np.full((5, 5), 0.8, dtype=float) + np.diag([0.2]*5),\n            (0, 1)\n        ),\n        # Test Case 4: Boundary case with a single edge. Edge (4,5) -> indices (3,4)\n        (\n            np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 1.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 1.0, 0.9],\n                [0.0, 0.0, 0.0, 0.9, 1.0]\n            ]),\n            (3, 4)\n        )\n    ]\n\n    results = []\n    for R, (p, q) in test_cases:\n        factor = compute_tom_reduction_factor(R, p, q, beta)\n        results.append(factor)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4328763"}, {"introduction": "A primary goal of co-expression network analysis is to uncover how cellular wiring changes across different biological states, such as disease versus health. This requires moving beyond constructing a single network to comparing networks across conditions. This practice [@problem_id:4328670] introduces a powerful statistical approach using the Fisher z-transformation to formally test whether the correlation between a pair of genes is significantly different between two groups, enabling the identification of differentially co-expressed gene pairs.", "problem": "In systems biomedicine, gene co-expression networks encode edges between genes that exhibit coordinated transcriptional regulation. To identify edges whose co-expression is specific to a biological condition, consider a single gene pair with sample correlations $r_1$ and $r_2$ measured independently under two conditions with sample sizes $n_1$ and $n_2$, respectively. Suppose the joint expression of the two genes is approximately bivariate normal in each condition, and recall the Fisher transformation $z = \\arctanh(r) = \\frac{1}{2}\\ln\\!\\left(\\frac{1+r}{1-r}\\right)$, which yields an approximately normal variable with mean $\\arctanh(\\rho)$ and variance $1/(n-3)$ for large $n$, where $\\rho$ is the population correlation. Assume independence of the two studies.\n\nStarting from these facts and the null hypothesis $H_0:\\ \\rho_1 = \\rho_2$, derive the standardized test statistic that uses the difference between the two Fisher-transformed correlations to assess differential correlation for this edge. Then, for the concrete case $r_1=0.42$, $r_2=0.05$, $n_1=60$, and $n_2=55$, compute the value of this test statistic. Express the final numerical value as a pure number, and round your answer to four significant figures.", "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and objective. It describes a standard and valid statistical procedure for comparing two independent correlation coefficients. All necessary information is provided, and the premises are consistent with established statistical theory.\n\nThe objective is to derive a standardized test statistic for the null hypothesis of no difference between two population correlation coefficients, $H_0: \\rho_1 = \\rho_2$, based on two independent samples, and then to compute its value for a specific case.\n\nLet $r_1$ and $r_2$ be the sample correlation coefficients from two independent studies with sample sizes $n_1$ and $n_2$, respectively. The corresponding population correlation coefficients are $\\rho_1$ and $\\rho_2$. The problem states that the joint distribution of the underlying variables is bivariate normal.\n\nThe Fisher transformation is given by $z = \\arctanh(r) = \\frac{1}{2}\\ln\\left(\\frac{1+r}{1-r}\\right)$. For a large sample of size $n$, the transformed variable $z$ is approximately normally distributed with mean $\\arctanh(\\rho)$ and variance $\\sigma_z^2 = \\frac{1}{n-3}$.\n\nLet us apply this transformation to our two sample correlations, $r_1$ and $r_2$. We obtain two random variables, $z_1$ and $z_2$:\n$$ z_1 = \\arctanh(r_1) \\quad \\text{and} \\quad z_2 = \\arctanh(r_2) $$\nAccording to the problem statement, their distributions are approximately normal:\n$$ z_1 \\sim N\\left(\\mu_1 = \\arctanh(\\rho_1), \\sigma_1^2 = \\frac{1}{n_1-3}\\right) $$\n$$ z_2 \\sim N\\left(\\mu_2 = \\arctanh(\\rho_2), \\sigma_2^2 = \\frac{1}{n_2-3}\\right) $$\nThe two studies are independent, which implies that the random variables $z_1$ and $z_2$ are also independent.\n\nWe are interested in the difference between the two Fisher-transformed correlations, $D = z_1 - z_2$. Since $z_1$ and $z_2$ are independent normal variables, their difference $D$ is also normally distributed.\nThe mean of the difference is the difference of the means:\n$$ E[D] = E[z_1] - E[z_2] = \\arctanh(\\rho_1) - \\arctanh(\\rho_2) $$\nThe variance of the difference of independent variables is the sum of their variances:\n$$ \\text{Var}(D) = \\text{Var}(z_1) + \\text{Var}(z_2) = \\frac{1}{n_1-3} + \\frac{1}{n_2-3} $$\nThe null hypothesis is $H_0: \\rho_1 = \\rho_2$. This is equivalent to stating that $\\arctanh(\\rho_1) = \\arctanh(\\rho_2)$, because $\\arctanh$ is a strictly increasing function. Under this null hypothesis, the expected value of the difference $D$ is zero:\n$$ E[D]_{H_0} = \\arctanh(\\rho_1) - \\arctanh(\\rho_1) = 0 $$\nA standardized test statistic, typically denoted by $Z$, is constructed by subtracting the mean under the null hypothesis from the observed value and dividing by the standard deviation (also known as the standard error). The standard error of the difference is $\\text{SE}_D = \\sqrt{\\text{Var}(D)}$.\nThus, the standardized test statistic is:\n$$ Z = \\frac{(z_1 - z_2) - E[D]_{H_0}}{\\text{SE}_D} = \\frac{z_1 - z_2 - 0}{\\sqrt{\\frac{1}{n_1-3} + \\frac{1}{n_2-3}}} $$\nThis simplifies to the general formula for the test statistic:\n$$ Z = \\frac{\\arctanh(r_1) - \\arctanh(r_2)}{\\sqrt{\\frac{1}{n_1-3} + \\frac{1}{n_2-3}}} $$\nUnder the null hypothesis, this $Z$ statistic follows a standard normal distribution, $Z \\sim N(0,1)$, for sufficiently large $n_1$ and $n_2$. This completes the derivation.\n\nNext, we compute the value of this test statistic for the given data: $r_1=0.42$, $r_2=0.05$, $n_1=60$, and $n_2=55$.\n\nFirst, we calculate the Fisher-transformed correlations $z_1$ and $z_2$:\n$$ z_1 = \\arctanh(0.42) = \\frac{1}{2}\\ln\\left(\\frac{1+0.42}{1-0.42}\\right) = \\frac{1}{2}\\ln\\left(\\frac{1.42}{0.58}\\right) \\approx 0.447675 $$\n$$ z_2 = \\arctanh(0.05) = \\frac{1}{2}\\ln\\left(\\frac{1+0.05}{1-0.05}\\right) = \\frac{1}{2}\\ln\\left(\\frac{1.05}{0.95}\\right) \\approx 0.050042 $$\nThe difference in the numerator is:\n$$ z_1 - z_2 \\approx 0.447675 - 0.050042 = 0.397633 $$\nNext, we calculate the standard error in the denominator:\n$$ \\text{SE}_D = \\sqrt{\\frac{1}{n_1-3} + \\frac{1}{n_2-3}} = \\sqrt{\\frac{1}{60-3} + \\frac{1}{55-3}} = \\sqrt{\\frac{1}{57} + \\frac{1}{52}} $$\n$$ \\frac{1}{57} \\approx 0.01754386 $$\n$$ \\frac{1}{52} \\approx 0.01923077 $$\n$$ \\text{SE}_D = \\sqrt{0.01754386 + 0.01923077} = \\sqrt{0.03677463} \\approx 0.191767 $$\nFinally, we compute the test statistic $Z$:\n$$ Z = \\frac{0.397633}{0.191767} \\approx 2.073539 $$\nRounding the result to four significant figures gives $2.074$.", "answer": "$$\\boxed{2.074}$$", "id": "4328670"}]}