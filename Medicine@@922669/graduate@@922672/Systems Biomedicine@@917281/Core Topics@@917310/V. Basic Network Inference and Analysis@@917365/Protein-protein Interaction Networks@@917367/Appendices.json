{"hands_on_practices": [{"introduction": "Protein-protein interaction networks are not directly observed but are models inferred from high-throughput experimental data. This exercise [@problem_id:1460618] explores how the choice of an inference model—specifically, the conservative spoke model versus the liberal matrix model—fundamentally alters the resulting network topology derived from the same Affinity Purification-Mass Spectrometry (AP-MS) data. Mastering this distinction is crucial for critically evaluating published interactome maps and understanding their inherent biases.", "problem": "A systems biology research team is studying the protein interaction network of a model organism. They perform a series of three Affinity Purification-Mass Spectrometry (AP-MS) experiments to identify protein complexes. In each experiment, a single \"bait\" protein is used to pull down its binding partners, which are then identified as \"prey\" proteins. The results are as follows:\n\n- Experiment 1: The bait protein is P1. The identified prey proteins are {P2, P3, P4}.\n- Experiment 2: The bait protein is P2. The identified prey proteins are {P1, P4, P5}.\n- Experiment 3: The bait protein is P5. The identified prey proteins are {P2, P6}.\n\nTo build a Protein-Protein Interaction (PPI) network from this data, the team considers two different inference models: the spoke model and the matrix model.\n\n- **Spoke Model**: In each experiment, an interaction is inferred only between the bait protein and each of the prey proteins. No interactions are inferred among prey proteins.\n- **Matrix Model**: In each experiment, it is assumed that all proteins identified (the bait and all its preys) form a fully connected complex (a clique). An interaction is thus inferred between every possible pair of proteins within that set.\n\nConsidering the data from all three experiments combined, calculate the total number of unique protein-protein interactions that would be inferred by each model. Let $N_{spoke}$ be the total number of unique interactions from the spoke model, and $N_{matrix}$ be the total number of unique interactions from the matrix model.\n\nPresent your answer as a row matrix with two elements, in the order $N_{spoke}$, then $N_{matrix}$.", "solution": "We interpret protein-protein interactions as undirected pairs. For each experiment, we form the inferred interaction set according to the specified model and then take the union across all experiments to count unique interactions.\n\nSpoke model:\nExperiment 1 (bait $\\mathrm{P1}$, preys $\\{\\mathrm{P2},\\mathrm{P3},\\mathrm{P4}\\}$) yields\n$$\nE_{1}^{s}=\\{(\\mathrm{P1},\\mathrm{P2}),(\\mathrm{P1},\\mathrm{P3}),(\\mathrm{P1},\\mathrm{P4})\\}.\n$$\nExperiment 2 (bait $\\mathrm{P2}$, preys $\\{\\mathrm{P1},\\mathrm{P4},\\mathrm{P5}\\}$) yields\n$$\nE_{2}^{s}=\\{(\\mathrm{P1},\\mathrm{P2}),(\\mathrm{P2},\\mathrm{P4}),(\\mathrm{P2},\\mathrm{P5})\\}.\n$$\nExperiment 3 (bait $\\mathrm{P5}$, preys $\\{\\mathrm{P2},\\mathrm{P6}\\}$) yields\n$$\nE_{3}^{s}=\\{(\\mathrm{P2},\\mathrm{P5}),(\\mathrm{P5},\\mathrm{P6})\\}.\n$$\nThe union of unique pairs is\n$$\nE^{s}=E_{1}^{s}\\cup E_{2}^{s}\\cup E_{3}^{s}=\\{(\\mathrm{P1},\\mathrm{P2}),(\\mathrm{P1},\\mathrm{P3}),(\\mathrm{P1},\\mathrm{P4}),(\\mathrm{P2},\\mathrm{P4}),(\\mathrm{P2},\\mathrm{P5}),(\\mathrm{P5},\\mathrm{P6})\\},\n$$\nso\n$$\nN_{\\text{spoke}}=|E^{s}|=6.\n$$\n\nMatrix model:\nFor each experiment, all identified proteins form a clique. Let the identified sets be\n$$\nS_{1}=\\{\\mathrm{P1},\\mathrm{P2},\\mathrm{P3},\\mathrm{P4}\\},\\quad S_{2}=\\{\\mathrm{P1},\\mathrm{P2},\\mathrm{P4},\\mathrm{P5}\\},\\quad S_{3}=\\{\\mathrm{P2},\\mathrm{P5},\\mathrm{P6}\\}.\n$$\nTheir clique edges are\n$$\nE_{1}^{m}=\\{(\\mathrm{P1},\\mathrm{P2}),(\\mathrm{P1},\\mathrm{P3}),(\\mathrm{P1},\\mathrm{P4}),(\\mathrm{P2},\\mathrm{P3}),(\\mathrm{P2},\\mathrm{P4}),(\\mathrm{P3},\\mathrm{P4})\\},\n$$\n$$\nE_{2}^{m}=\\{(\\mathrm{P1},\\mathrm{P2}),(\\mathrm{P1},\\mathrm{P4}),(\\mathrm{P1},\\mathrm{P5}),(\\mathrm{P2},\\mathrm{P4}),(\\mathrm{P2},\\mathrm{P5}),(\\mathrm{P4},\\mathrm{P5})\\},\n$$\n$$\nE_{3}^{m}=\\{(\\mathrm{P2},\\mathrm{P5}),(\\mathrm{P2},\\mathrm{P6}),(\\mathrm{P5},\\mathrm{P6})\\}.\n$$\nTaking the union,\n$$\nE^{m}=E_{1}^{m}\\cup E_{2}^{m}\\cup E_{3}^{m}=\\{(\\mathrm{P1},\\mathrm{P2}),(\\mathrm{P1},\\mathrm{P3}),(\\mathrm{P1},\\mathrm{P4}),(\\mathrm{P1},\\mathrm{P5}),(\\mathrm{P2},\\mathrm{P3}),(\\mathrm{P2},\\mathrm{P4}),(\\mathrm{P2},\\mathrm{P5}),(\\mathrm{P2},\\mathrm{P6}),(\\mathrm{P3},\\mathrm{P4}),(\\mathrm{P4},\\mathrm{P5}),(\\mathrm{P5},\\mathrm{P6})\\},\n$$\nso\n$$\nN_{\\text{matrix}}=|E^{m}|=11.\n$$\n\nHence, the totals are $N_{\\text{spoke}}=6$ and $N_{\\text{matrix}}=11$.", "answer": "$$\\boxed{\\begin{pmatrix}6 & 11\\end{pmatrix}}$$", "id": "1460618"}, {"introduction": "Once a network is constructed, we can analyze its topology to reveal the functional importance of individual proteins. This practice [@problem_id:4298718] focuses on calculating betweenness centrality, a key metric that identifies proteins acting as critical \"bridges\" for information flow within the network. By working through this problem, you will gain hands-on experience with shortest-path analysis in weighted graphs and develop an intuition for how a protein's position can determine its biological significance as a potential bottleneck or key regulator.", "problem": "Consider a Protein-Protein Interaction Network (PPIN), defined as an undirected, weighted graph where proteins are nodes and interactions are edges with positive weights representing effective path costs inversely related to interaction affinity (lower weight indicates higher affinity). Let the network have nodes $A$, $B$, $C$, $D$, $E$, and $F$. The set of undirected weighted edges is\n$\\{(A,B,1), (A,C,1), (B,C,1), (C,D,1), (D,E,1), (D,F,1), (E,F,1), (A,D,2), (B,D,2), (C,E,2)\\}$,\nwhere each tuple denotes $(\\text{protein}_1,\\ \\text{protein}_2,\\ \\text{weight})$ and all weights are strictly positive. Shortest paths are defined with respect to the sum of weights along a path.\n\nUsing the standard, unnormalized definition of betweenness centrality employed in network science, compute the betweenness centrality of protein $C$, taking shortest paths over unordered pairs of distinct proteins that exclude $C$ as endpoints. In addition, explain—based on first principles of shortest-path structure—how removing protein $C$ affects the flow along shortest paths (i.e., the redundancy and routing of geodesic paths) between the remaining proteins.\n\nReport only the betweenness centrality of protein $C$ as a single reduced fraction with no units. No rounding is required.", "solution": "The problem is evaluated as valid. It is scientifically grounded in network theory, well-posed with a complete and consistent definition, and stated objectively. The solution process may proceed.\n\nThe task is twofold: first, to compute the unnormalized betweenness centrality of protein $C$, denoted $B_C$; and second, to explain the structural impact of removing protein $C$ on the shortest paths within the network.\n\n**Part 1: Computation of Betweenness Centrality**\n\nThe betweenness centrality of a node $v$, $B(v)$, is defined as the sum of the fractions of all-pairs shortest paths that pass through $v$. For an undirected graph, the standard formula is:\n$$\nB(v) = \\sum_{s \\neq v \\neq t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}\n$$\nwhere the sum is over all unordered pairs of distinct nodes $\\{s, t\\}$ in the network, not including $v$. Here, $\\sigma_{st}$ is the total number of shortest paths (geodesics) between node $s$ and node $t$, and $\\sigma_{st}(v)$ is the number of those shortest paths that pass through node $v$.\n\nThe set of nodes is $V = \\{A, B, C, D, E, F\\}$. We must calculate $B_C$ by considering all unordered pairs of nodes from the set $V \\setminus \\{C\\} = \\{A, B, D, E, F\\}$. The number of such pairs is $\\binom{5}{2} = 10$. Let us analyze the shortest paths for each pair.\n\nThe network edges and weights are:\n$w(A,B) = 1$, $w(A,C) = 1$, $w(B,C) = 1$, $w(C,D) = 1$, $w(D,E) = 1$, $w(D,F) = 1$, $w(E,F) = 1$, $w(A,D) = 2$, $w(B,D) = 2$, $w(C,E) = 2$.\n\n1.  **Pair $\\{A, B\\}$**: The shortest path is the direct edge $A-B$ with weight $1$. The path $A-C-B$ has weight $1+1=2$. Thus, $\\sigma_{AB} = 1$, and this path does not pass through $C$. The contribution to $B_C$ is $\\frac{0}{1} = 0$.\n\n2.  **Pair $\\{D, E\\}$**: The shortest path is the direct edge $D-E$ with weight $1$. The path $D-F-E$ has weight $1+1=2$. The path $D-C-E$ has weight $1+2=3$. Thus, $\\sigma_{DE} = 1$, and it does not pass through $C$. Contribution is $0$.\n\n3.  **Pair $\\{D, F\\}$**: The shortest path is the direct edge $D-F$ with weight $1$. The path $D-E-F$ has weight $1+1=2$. Thus, $\\sigma_{DF} = 1$, and it does not pass through $C$. Contribution is $0$.\n\n4.  **Pair $\\{E, F\\}$**: The shortest path is the direct edge $E-F$ with weight $1$. The path $E-D-F$ has weight $1+1=2$. Thus, $\\sigma_{EF} = 1$, and it does not pass through $C$. Contribution is $0$.\n\n5.  **Pair $\\{A, D\\}$**:\n    *   Path $A-C-D$: weight $w(A,C) + w(C,D) = 1+1=2$.\n    *   Path $A-D$: weight $w(A,D) = 2$.\n    *   Path $A-B-D$: weight $w(A,B) + w(B,D) = 1+2=3$.\n    There are two shortest paths of length $2$: $A-C-D$ and $A-D$. Thus, $\\sigma_{AD} = 2$. One of these paths, $A-C-D$, passes through $C$, so $\\sigma_{AD}(C) = 1$. The contribution is $\\frac{1}{2}$.\n\n6.  **Pair $\\{B, D\\}$**: By symmetry with pair $\\{A,D\\}$ (nodes $A$ and $B$ are structurally equivalent with respect to the rest of the network), we find two shortest paths of length $2$: $B-C-D$ and $B-D$. Thus, $\\sigma_{BD} = 2$ and $\\sigma_{BD}(C) = 1$. The contribution is $\\frac{1}{2}$.\n\n7.  **Pair $\\{A, E\\}$**:\n    *   Path $A-D-E$: weight $w(A,D) + w(D,E) = 2+1=3$.\n    *   Path $A-C-D-E$: weight $w(A,C) + w(C,D) + w(D,E) = 1+1+1=3$.\n    *   Path $A-C-E$: weight $w(A,C) + w(C,E) = 1+2=3$.\n    *   Path $A-B-D-E$: weight $1+2+1=4$.\n    There are three shortest paths of length $3$. Thus, $\\sigma_{AE} = 3$. Two of these, $A-C-D-E$ and $A-C-E$, pass through $C$. Thus, $\\sigma_{AE}(C) = 2$. The contribution is $\\frac{2}{3}$.\n\n8.  **Pair $\\{B, E\\}$**: By symmetry with pair $\\{A,E\\}$, there are three shortest paths of length $3$: $B-D-E$, $B-C-D-E$, and $B-C-E$. Thus, $\\sigma_{BE} = 3$. Two of these pass through $C$, so $\\sigma_{BE}(C) = 2$. The contribution is $\\frac{2}{3}$.\n\n9.  **Pair $\\{A, F\\}$**:\n    *   Path $A-D-F$: weight $w(A,D) + w(D,F) = 2+1=3$.\n    *   Path $A-C-D-F$: weight $w(A,C) + w(C,D) + w(D,F) = 1+1+1=3$.\n    *   Path $A-C-E-F$: weight $1+2+1=4$.\n    There are two shortest paths of length $3$: $A-D-F$ and $A-C-D-F$. Thus, $\\sigma_{AF} = 2$. One path passes through $C$, so $\\sigma_{AF}(C) = 1$. The contribution is $\\frac{1}{2}$.\n\n10. **Pair $\\{B, F\\}$**: By symmetry with pair $\\{A,F\\}$, there are two shortest paths of length $3$: $B-D-F$ and $B-C-D-F$. Thus, $\\sigma_{BF} = 2$. One path passes through $C$, so $\\sigma_{BF}(C) = 1$. The contribution is $\\frac{1}{2}$.\n\nNow, we sum all non-zero contributions to find $B_C$:\n$$\nB_C = \\sum_{\\{s,t\\} \\subset V \\setminus \\{C\\}} \\frac{\\sigma_{st}(C)}{\\sigma_{st}} = 0 + 0 + 0 + 0 + \\frac{1}{2} + \\frac{1}{2} + \\frac{2}{3} + \\frac{2}{3} + \\frac{1}{2} + \\frac{1}{2}\n$$\n$$\nB_C = 4 \\times \\frac{1}{2} + 2 \\times \\frac{2}{3} = 2 + \\frac{4}{3} = \\frac{6}{3} + \\frac{4}{3} = \\frac{10}{3}\n$$\n\n**Part 2: Effect of Removing Protein C**\n\nRemoving protein $C$ also removes all edges incident to it: $(A,C)$, $(B,C)$, $(C,D)$, and $(C,E)$. This fundamentally alters the network's topology and the available paths for information or signal flow. The analysis must be based on the first principles of shortest-path structure: the set of shortest paths for any given pair of nodes is determined by finding all paths that achieve the minimum possible sum of edge weights. Removing a node eliminates any path that includes it, forcing a re-evaluation of shortest paths among the remaining alternatives.\n\nThe primary effect of removing $C$ is on paths connecting the sub-network involving nodes $\\{A,B\\}$ to the sub-network involving nodes $\\{D,E,F\\}$. For pairs within these sub-networks (e.g., $\\{A,B\\}$ or $\\{D,E\\}$), the shortest paths are unaffected as they do not involve $C$. Let us examine the pairs whose geodesic paths were mediated by $C$:\n\n*   **Pairs $\\{A,D\\}$ and $\\{B,D\\}$**: Before removal, the shortest path length was $2$, and there were two redundant paths (e.g., $A-D$ and $A-C-D$). After removing $C$, the paths $A-C-D$ and $B-C-D$ are eliminated. For both pairs, only a single shortest path remains ($A-D$ and $B-D$, respectively), both still of length $2$. The effect is a **complete loss of redundancy** in the shortest connections between $\\{A,B\\}$ and $D$, without an increase in path length.\n\n*   **Pairs $\\{A,E\\}$, $\\{B,E\\}$, $\\{A,F\\}$, and $\\{B,F\\}$**:\n    *   For $\\{A,E\\}$, removing $C$ eliminates two of the three shortest paths ($A-C-D-E$ and $A-C-E$). The sole remaining shortest path is $A-D-E$, with its length of $3$ becoming the new unique geodesic distance.\n    *   For $\\{A,F\\}$, removing $C$ eliminates one of the two shortest paths ($A-C-D-F$). The sole remaining shortest path is $A-D-F$, with its length of $3$ becoming the new unique geodesic distance.\n    *   The same logic applies symmetrically to pairs $\\{B,E\\}$ and $\\{B,F\\}$.\n\nIn summary, the removal of protein $C$ has the following consequences for shortest-path routing:\n\n1.  **Loss of Redundancy**: For every pair of proteins $(s,t)$ where $s \\in \\{A,B\\}$ and $t \\in \\{D,E,F\\}$, the number of distinct shortest paths, $\\sigma_{st}$, is strictly reduced. This signifies a decrease in the network's robustness. The flow, which was previously distributed across multiple geodesic paths, is now concentrated onto fewer, or in these cases, single routes.\n\n2.  **Increased Centrality of Other Nodes**: The rerouting of flow makes other nodes and edges more critical. Specifically, all shortest paths from $\\{A,B\\}$ to $\\{D,E,F\\}$ must now pass through node $D$ and utilize the edges $(A,D)$ and $(B,D)$. The betweenness centrality of node $D$ and the edge betweenness of $(A,D)$ and $(B,D)$ would increase significantly. Protein $D$ becomes a critical bottleneck or articulation point for communication between these two parts of the network.\n\n3.  **No Increase in Path Distance**: For this specific network configuration, the removal of $C$ does not increase the shortest path *length* for any pair of remaining nodes. This is a crucial observation: protein $C$ provided alternative routes that were exactly as efficient (in terms of path weight) as the best non-$C$ paths. It functioned as a source of parallel, equally optimal pathways rather than as a \"shortcut.\" If the weights had been different (e.g., if $w(C,D)$ were less than $1$), its removal could have increased path distances.\n\nFrom the first principle of shortest-path computation, removing node $C$ prunes the set of candidate paths. For the affected pairs, this pruning removed paths that were members of the set of minimum-weight paths, thereby reducing the size of this set ($\\sigma_{st}$). Since at least one minimum-weight path remained that did not involve $C$, the shortest path distance itself did not change.", "answer": "$$\\boxed{\\frac{10}{3}}$$", "id": "4298718"}, {"introduction": "A powerful application of PPI networks is to predict novel genes associated with diseases based on their network proximity to known disease genes. This advanced practice [@problem_id:2423157] implements the Random Walk with Restart (RWR) algorithm, a state-of-the-art method for gene prioritization. By coding this algorithm, you will learn how to leverage network topology to quantify the \"relevance\" of every protein to a set of seed proteins, a fundamental technique in computational drug targeting and functional genomics.", "problem": "A Protein-Protein Interaction (PPI) network is modeled as a finite, undirected, simple graph on $n$ nodes, represented by an adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$ with $A_{ij} = 1$ if and only if there is an interaction between proteins $i$ and $j$, and $A_{ij} = 0$ otherwise. Let $S \\subset \\{0,1,\\dots,n-1\\}$ be a nonempty set of seed indices corresponding to known cancer genes. Define the seed distribution vector $s \\in \\mathbb{R}^n$ by $s_i = \\frac{1}{|S|}$ if $i \\in S$ and $s_i = 0$ otherwise. For each node $j$ with degree $d_j = \\sum_{i=1}^n A_{ij}$ equal to zero, set $A_{jj} \\leftarrow 1$ (a self-loop) prior to normalization so that a random walk is well-defined. Define the column-stochastic transition matrix $W \\in \\mathbb{R}^{n \\times n}$ by\n$$\nW = A D^{-1},\n$$\nwhere $D \\in \\mathbb{R}^{n \\times n}$ is diagonal with $D_{jj} = \\sum_{i=1}^n A_{ij}$ after the self-loop adjustment, so that for each column $j$ we have $\\sum_{i=1}^n W_{ij} = 1$.\n\nGiven a restart probability $\\alpha \\in [0,1]$, the Random Walk with Restart (RWR) process on this graph is the sequence $\\{p^{(t)}\\}_{t=0}^{\\infty}$ with $p^{(0)} = s$ and\n$$\np^{(t+1)} = (1-\\alpha) \\, W \\, p^{(t)} + \\alpha \\, s \\quad \\text{for all } t \\geq 0.\n$$\nThe process is iterated until convergence in the $\\ell_1$ norm with tolerance $\\varepsilon > 0$, that is, until $\\lVert p^{(t+1)} - p^{(t)} \\rVert_1 < \\varepsilon$, or until a hard cap of $T_{\\max} \\in \\mathbb{N}$ iterations is reached. Let the converged vector be denoted by $p^\\star$ (if $\\alpha = 1$, then $p^\\star = s$ exactly). The task is to rank candidate proteins (nodes) by descending $p^\\star_i$ values, excluding all seed indices in $S$. Ties must be broken by increasing node index. Given an integer $k$ with $1 \\le k \\le n - |S|$, the required output for each test case is the list of the top-$k$ non-seed node indices according to the described ranking rule.\n\nYour program must implement the mathematics above exactly and apply it to the following test suite. All nodes are indexed by integers starting at $0$. Each test specifies $(A, S, \\alpha, \\varepsilon, T_{\\max}, k)$.\n\nTest A (general connected case):\n- Adjacency $A^{(\\mathrm{A})} \\in \\{0,1\\}^{6 \\times 6}$:\n$$\nA^{(\\mathrm{A})} =\n\\begin{bmatrix}\n0 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 0 & 1 \\\\\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0\n\\end{bmatrix}\n$$\n- Seeds $S^{(\\mathrm{A})} = \\{0,2\\}$.\n- Restart probability $\\alpha^{(\\mathrm{A})} = 0.5$.\n- Tolerance $\\varepsilon^{(\\mathrm{A})} = 10^{-10}$.\n- Maximum iterations $T_{\\max}^{(\\mathrm{A})} = 10000$.\n- Top count $k^{(\\mathrm{A})} = 3$.\n\nTest B (disconnected network to test isolation handling):\n- Adjacency $A^{(\\mathrm{B})} \\in \\{0,1\\}^{6 \\times 6}$:\n$$\nA^{(\\mathrm{B})} =\n\\begin{bmatrix}\n0 & 1 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 0\n\\end{bmatrix}\n$$\n- Seeds $S^{(\\mathrm{B})} = \\{0\\}$.\n- Restart probability $\\alpha^{(\\mathrm{B})} = 0.7$.\n- Tolerance $\\varepsilon^{(\\mathrm{B})} = 10^{-12}$.\n- Maximum iterations $T_{\\max}^{(\\mathrm{B})} = 10000$.\n- Top count $k^{(\\mathrm{B})} = 4$.\n\nTest C (high restart probability emphasizing proximity to seeds):\n- Adjacency $A^{(\\mathrm{C})} \\in \\{0,1\\}^{5 \\times 5}$:\n$$\nA^{(\\mathrm{C})} =\n\\begin{bmatrix}\n0 & 1 & 1 & 1 & 0 \\\\\n1 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 1 & 0\n\\end{bmatrix}\n$$\n- Seeds $S^{(\\mathrm{C})} = \\{0\\}$.\n- Restart probability $\\alpha^{(\\mathrm{C})} = 0.95$.\n- Tolerance $\\varepsilon^{(\\mathrm{C})} = 10^{-12}$.\n- Maximum iterations $T_{\\max}^{(\\mathrm{C})} = 10000$.\n- Top count $k^{(\\mathrm{C})} = 3$.\n\nTest D (boundary case $\\alpha = 1$):\n- Adjacency $A^{(\\mathrm{D})} \\in \\{0,1\\}^{4 \\times 4}$:\n$$\nA^{(\\mathrm{D})} =\n\\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 1 \\\\\n0 & 0 & 1 & 0\n\\end{bmatrix}\n$$\n- Seeds $S^{(\\mathrm{D})} = \\{1,2\\}$.\n- Restart probability $\\alpha^{(\\mathrm{D})} = 1$.\n- Tolerance $\\varepsilon^{(\\mathrm{D})} = 10^{-12}$.\n- Maximum iterations $T_{\\max}^{(\\mathrm{D})} = 10000$.\n- Top count $k^{(\\mathrm{D})} = 2$.\n\nFor each test, compute $p^\\star$ according to the definition above and return the indices of the top-$k$ non-seed nodes ranked by descending $p^\\star_i$, with ties broken by increasing index. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\,\\text{resultA},\\text{resultB},\\text{resultC},\\text{resultD}\\,]$). In this problem, each result is a list of $k$ integers, so the final output must be a single line of the form $[[i_{1},\\dots,i_{k}], [j_{1},\\dots,j_{k}], [\\dots], [\\dots]]$ where all entries are integers and lists are in the order of the tests A, B, C, D.", "solution": "The problem statement has been validated and is deemed scientifically sound, well-posed, and complete. It presents a standard application of the Random Walk with Restart (RWR) algorithm for node prioritization in a network, a common task in computational biology. We shall proceed with a rigorous, step-by-step solution.\n\nThe objective is to identify and rank non-seed proteins in a Protein-Protein Interaction (PPI) network that are most likely associated with a given set of seed proteins, which are known to be involved in a specific pathology such as cancer. The ranking is based on the steady-state probability distribution of a random walker that explores the network, with a constant probability of returning to one of the seed nodes. This methodology quantifies a form of proximity or relevance in the network context.\n\nThe algorithm proceeds in three main stages: network preprocessing, iterative calculation of the RWR probabilities, and ranking of the results.\n\n**1. Network Preprocessing and Initialization**\n\nFirst, we must construct the necessary mathematical objects from the given inputs. The inputs for each test case are an adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$, a set of seed indices $S$, a restart probability $\\alpha \\in [0,1]$, a convergence tolerance $\\varepsilon > 0$, a maximum number of iterations $T_{\\max}$, and the number of top candidates to report, $k$.\n\n**1.1. Modification of the Adjacency Matrix**\nA random walk is undefined on nodes with no outgoing edges (isolated nodes). The problem specifies a standard procedure to handle this: for any node $j$ with degree $d_j = \\sum_{i=0}^{n-1} A_{ij} = 0$, we introduce a self-loop by setting $A_{jj} \\leftarrow 1$. This ensures that every node has at least one outgoing edge, making the degree matrix invertible and the random walk well-defined everywhere. Let us denote the potentially modified adjacency matrix as $A'$.\n\n**1.2. Construction of the Transition Matrix $W$**\nThe random walker moves from a node to one of its neighbors with uniform probability. This is formalized by the column-stochastic transition matrix $W$. We first compute the degree matrix $D'$, which is a diagonal matrix where the entry $D'_{jj}$ is the degree of node $j$ calculated from $A'$:\n$$\nD'_{jj} = \\sum_{i=0}^{n-1} A'_{ij}\n$$\nThe transition matrix $W \\in \\mathbb{R}^{n \\times n}$ is then obtained by normalizing the columns of $A'$:\n$$\nW = A' (D')^{-1}\n$$\nThis is equivalent to setting each element $W_{ij}$ to:\n$$\nW_{ij} = \\frac{A'_{ij}}{D'_{jj}} = \\frac{A'_{ij}}{\\sum_{l=0}^{n-1} A'_{lj}}\n$$\nBy construction, each column of $W$ sums to $1$, i.e., $\\sum_{i=0}^{n-1} W_{ij} = 1$ for all $j \\in \\{0, \\dots, n-1\\}$, correctly representing a probability distribution for the next step of a walker currently at node $j$.\n\n**1.3. Initialization of Probability Vectors**\nThe RWR process tracks a probability distribution over the nodes of the graph, represented by a vector $p^{(t)} \\in \\mathbb{R}^n$, where $p_i^{(t)}$ is the probability of finding the walker at node $i$ at iteration step $t$. The process starts from the seed nodes. The initial distribution, $p^{(0)}$, is defined as a uniform distribution over the seed set $S$. This is captured by the seed distribution vector $s \\in \\mathbb{R}^n$:\n$$\ns_i =\n\\begin{cases}\n\\frac{1}{|S|} & \\text{if } i \\in S \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nWe initialize the process by setting $p^{(0)} = s$.\n\n**2. The Random Walk with Restart Iteration**\n\nThe core of the algorithm is the iterative update rule. At each step $t$, the walker can either move to an adjacent node according to the transition matrix $W$ with probability $1-\\alpha$, or \"restart\" by jumping back to one of the seed nodes (according to the distribution $s$) with probability $\\alpha$. This is captured by the following recurrence relation:\n$$\np^{(t+1)} = (1-\\alpha) \\, W \\, p^{(t)} + \\alpha \\, s\n$$\nThis iteration is performed until a termination condition is met:\n1.  **Convergence:** The change in the probability vector becomes negligible. We measure this using the $\\ell_1$ norm: $\\lVert p^{(t+1)} - p^{(t)} \\rVert_1 = \\sum_{i=0}^{n-1} |p^{(t+1)}_i - p^{(t)}_i| < \\varepsilon$.\n2.  **Maximum Iterations:** The number of iterations reaches the predefined limit $T_{\\max}$.\n\nThe final vector, denoted $p^\\star$, is the converged probability distribution. This vector represents the steady-state probability of finding the walker at each node. A higher probability $p^\\star_i$ suggests that node $i$ is more \"connected\" or \"relevant\" to the initial seed set $S$.\n\nA special case arises when $\\alpha = 1$. The update rule simplifies to $p^{(t+1)} = s$. In this scenario, the walker always restarts, so the distribution never evolves away from the initial seed distribution. Thus, $p^\\star = s$ without any need for iteration.\n\n**3. Ranking and Selection of Candidate Proteins**\n\nOnce the steady-state probability vector $p^\\star$ has been computed, we use it to rank potential candidate genes. The candidates are all nodes that are not in the original seed set $S$. The ranking criteria are:\n1.  **Primary Sort Key:** The score $p^\\star_i$ in descending order.\n2.  **Secondary Sort Key (Tie-breaker):** The node index $i$ in ascending order.\n\nAfter sorting all non-seed nodes according to these rules, we select the top-$k$ nodes from this sorted list. The final result for each test case is the list of these $k$ node indices.\n\nThe implementation will follow these steps precisely for each of the provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_rwr_case(A_list, S_list, alpha, epsilon, T_max, k):\n    \"\"\"\n    Executes the Random Walk with Restart (RWR) algorithm for a single test case.\n\n    Args:\n        A_list (list of lists): The adjacency matrix as a list of lists.\n        S_list (list): The list of seed node indices.\n        alpha (float): The restart probability.\n        epsilon (float): The convergence tolerance.\n        T_max (int): The maximum number of iterations.\n        k (int): The number of top non-seed nodes to return.\n\n    Returns:\n        list: A list of the top-k non-seed node indices.\n    \"\"\"\n    A = np.array(A_list, dtype=np.float64)\n    n = A.shape[0]\n    S = set(S_list)\n\n    # Step 1.1: Handle isolated nodes by adding self-loops.\n    col_sums = np.sum(A, axis=0)\n    isolated_nodes = np.where(col_sums == 0)[0]\n    if isolated_nodes.size > 0:\n        A[isolated_nodes, isolated_nodes] = 1.0\n\n    # Step 1.2: Construct the column-stochastic transition matrix W.\n    D_diag = np.sum(A, axis=0)\n    # Using broadcasting for efficient column-wise division: W_ij = A_ij / D_jj\n    W = A / D_diag\n\n    # Step 1.3: Initialize seed vector s and probability vector p.\n    s = np.zeros(n, dtype=np.float64)\n    if S:\n        s[list(S)] = 1.0 / len(S)\n    \n    p_star = np.zeros(n, dtype=np.float64)\n\n    # Step 2: The RWR Iteration\n    # Handle the boundary case alpha = 1\n    if alpha == 1.0:\n        p_star = s\n    else:\n        p_current = s.copy()\n        for _ in range(T_max):\n            p_next = (1.0 - alpha) * (W @ p_current) + alpha * s\n            \n            # Check for convergence using the L1 norm\n            l1_norm = np.sum(np.abs(p_next - p_current))\n            \n            p_current = p_next\n            \n            if l1_norm  epsilon:\n                break\n        p_star = p_current\n\n    # Step 3: Ranking and Selection\n    # Identify non-seed candidate nodes\n    candidate_indices = [i for i in range(n) if i not in S]\n    \n    # Create a list of tuples (score, index) for sorting\n    candidates_with_scores = [(p_star[i], i) for i in candidate_indices]\n    \n    # Sort candidates: primary key is score (descending), secondary key is index (ascending)\n    # The key lambda x: (-x[0], x[1]) achieves this.\n    candidates_with_scores.sort(key=lambda x: (-x[0], x[1]))\n    \n    # Select the top-k indices from the sorted list\n    top_k_indices = [index for score, index in candidates_with_scores[:k]]\n    \n    return top_k_indices\n\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and print results in the required format.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": [\n                [0, 1, 0, 0, 0, 0],\n                [1, 0, 1, 0, 0, 1],\n                [0, 1, 0, 1, 0, 0],\n                [0, 0, 1, 0, 1, 0],\n                [0, 0, 0, 1, 0, 1],\n                [0, 1, 0, 0, 1, 0]\n            ],\n            \"S\": {0, 2},\n            \"alpha\": 0.5,\n            \"epsilon\": 1e-10,\n            \"T_max\": 10000,\n            \"k\": 3\n        },\n        {\n            \"A\": [\n                [0, 1, 1, 0, 0, 0],\n                [1, 0, 1, 0, 0, 0],\n                [1, 1, 0, 0, 0, 0],\n                [0, 0, 0, 0, 1, 0],\n                [0, 0, 0, 1, 0, 1],\n                [0, 0, 0, 0, 1, 0]\n            ],\n            \"S\": {0},\n            \"alpha\": 0.7,\n            \"epsilon\": 1e-12,\n            \"T_max\": 10000,\n            \"k\": 4\n        },\n        {\n            \"A\": [\n                [0, 1, 1, 1, 0],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 1],\n                [0, 0, 0, 1, 0]\n            ],\n            \"S\": {0},\n            \"alpha\": 0.95,\n            \"epsilon\": 1e-12,\n            \"T_max\": 10000,\n            \"k\": 3\n        },\n        {\n            \"A\": [\n                [0, 1, 0, 0],\n                [1, 0, 1, 0],\n                [0, 1, 0, 1],\n                [0, 0, 1, 0]\n            ],\n            \"S\": {1, 2},\n            \"alpha\": 1.0,\n            \"epsilon\": 1e-12,\n            \"T_max\": 10000,\n            \"k\": 2\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_rwr_case(\n            case[\"A\"],\n            list(case[\"S\"]),\n            case[\"alpha\"],\n            case[\"epsilon\"],\n            case[\"T_max\"],\n            case[\"k\"]\n        )\n        results.append(result)\n\n    # Format the final output string as a list of lists of integers.\n    # e.g., [[1,2],[3,4]]\n    result_strings = [f\"[{','.join(map(str, r))}]\" for r in results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "2423157"}]}