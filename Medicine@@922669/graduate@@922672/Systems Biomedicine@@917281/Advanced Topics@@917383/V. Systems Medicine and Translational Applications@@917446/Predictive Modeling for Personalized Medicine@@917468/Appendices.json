{"hands_on_practices": [{"introduction": "The first step in developing a trustworthy predictive model is establishing a reliable estimate of its performance on new, unseen data. In practice, model development involves tuning hyperparameters, which can lead to optimistic performance estimates if not handled correctly. This exercise [@problem_id:4376899] challenges you to design a validation strategy that avoids this \"optimism bias\" by rigorously separating the data used for model tuning from the data used for final performance evaluation, a critical skill for producing credible results in personalized medicine.", "problem": "You are evaluating a predictive model for personalized oncology in systems biomedicine. The model aims to predict binary response $Y \\in \\{0,1\\}$ to a targeted therapy from high-dimensional multi-omics features $X \\in \\mathbb{R}^p$ with $p \\gg n$ (for instance, $n = 240$ patients and $p = 10000$ transcripts). You consider a family of models $\\{f_j : j = 1,\\dots,M\\}$ indexed by hyperparameters (for instance, regularization strength and mixing for elastic net, or kernel scale for a kernel method), with $M$ candidate hyperparameter configurations.\n\nYou use Cross-Validation (CV) to tune hyperparameters and the Area Under the Receiver Operating Characteristic Curve (AUC) to measure performance. Let the true generalization loss of configuration $j$ be $R_j = \\mathbb{E}[L(f_j(X),Y)]$ for a fixed loss $L(\\cdot,\\cdot)$, and let the true AUC be denoted by $A_j$. Suppose you have CV estimators $\\hat{R}_j$ and $\\hat{A}_j$ obtained on a single development dataset via repeated random partitioning. Assume, for the sake of analysis, that for each fixed $j$, the estimator $\\hat{R}_j$ is unbiased for $R_j$ and has variance $\\sigma^2$ that does not depend on $j$, and similarly for $\\hat{A}_j$.\n\nYou select the hyperparameter index $\\hat{\\jmath}$ by empirical risk minimization, i.e., $\\hat{\\jmath} = \\arg\\min_{j} \\hat{R}_j$ (equivalently, by maximizing $\\hat{A}_j$ if using AUC). In a patient-level setting that may include multiple biopsies per patient and site-specific batch effects, you aim to obtain an unbiased estimate of external performance that would apply to new patients drawn from the same population under exchangeability assumptions.\n\nFrom the following first principles and core definitions:\n- Generalization performance is defined as $R(f) = \\mathbb{E}_{(X,Y)\\sim P}[L(f(X),Y)]$ where $(X,Y)$ are drawn from the population distribution $P$ of interest.\n- Empirical risk estimators $\\hat{R}$ based on finite samples are random variables due to sampling variability.\n- Selection by minimizing (or maximizing) over multiple noisy estimates induces an extremum over random variables.\n- Independence of tuning data and evaluation data is necessary for unbiased external performance estimation.\n\nAnalyze the direction and drivers of optimism in $\\hat{R}_{\\hat{\\jmath}}$ (or $\\hat{A}_{\\hat{\\jmath}}$) arising from hyperparameter tuning on the same resamples, and derive a nested resampling plan that yields an unbiased estimate of external performance under the above assumptions, including proper handling of patient-level grouping and data leakage controls (for example, standardization, feature selection, and probability calibration). Which option best meets these requirements?\n\nA. Perform $K$-fold CV once on the full dataset, standardize features using the full dataset, select $\\hat{\\jmath}$ by maximizing the average $\\hat{A}_j$ across folds, and report that same cross-validated $\\hat{A}_{\\hat{\\jmath}}$ as the external performance. This uses all data once and avoids an extra outer loop.\n\nB. Use nested CV: Partition patients into $K_{\\mathrm{outer}}$ outer folds at the patient level to avoid correlation leakage across biopsies. For each outer training set, repeat all preprocessing steps (for example, standardization using means and variances computed only from inner-training splits, supervised feature selection confined to inner resamples, and probability calibration) strictly within an inner $K_{\\mathrm{inner}}$-fold CV to select $\\hat{\\jmath}$; then refit $f_{\\hat{\\jmath}}$ on the entire outer training set with the inner-derived preprocessing pipeline and evaluate on the untouched outer test set. Average the outer-fold AUCs to estimate external performance.\n\nC. Hold out a single test set once at the start, tune hyperparameters by repeated CV on the remaining data, and, if ties or near-ties occur among configurations, break them by inspecting performance on the held-out test set before finalizing $\\hat{\\jmath}$; then report the test-set AUC as external performance.\n\nD. Use non-nested bootstrap: For each bootstrap resample, standardize using the full resample, tune hyperparameters by maximizing out-of-bag AUC, evaluate the tuned model on the same out-of-bag cases, and average AUC across resamples to report external performance. This reuses out-of-bag cases for both tuning and evaluation.\n\nE. Use nested CV but perform a supervised univariate feature ranking on the full dataset first to select a subset of top features; then conduct inner CV only for hyperparameters on that fixed feature subset and evaluate on outer folds, reporting averaged outer AUC as external performance.\n\nSelect the single best option.", "solution": "The problem asks for the best procedure to obtain an unbiased estimate of a predictive model's external performance in a personalized medicine context. This setting is characterized by high-dimensional data ($p \\gg n$), the need for hyperparameter tuning, and a complex data structure involving multiple samples per patient. The core challenge is to avoid optimism bias, which arises from model selection and data leakage.\n\nLet's begin by validating the problem statement.\n\n### Step 1: Extract Givens\n- **Task**: Predict a binary response $Y \\in \\{0,1\\}$ from high-dimensional features $X \\in \\mathbb{R}^p$.\n- **Dataset**: $n$ patients, $p$ features, with $p \\gg n$ ($n=240$, $p=10000$). The dataset may include multiple biopsies per patient and site-specific batch effects.\n- **Models**: A family of models $\\{f_j : j=1, \\dots, M\\}$ indexed by hyperparameters.\n- **Evaluation Metric**: Area Under the Receiver Operating Characteristic Curve (AUC).\n- **Tuning Method**: Cross-Validation (CV).\n- **Estimators**: $\\hat{R}_j$ and $\\hat{A}_j$ are CV estimators of the true generalization loss $R_j$ and true AUC $A_j$, respectively, for model configuration $j$.\n- **Analytical Assumptions**: For a fixed $j$, $\\hat{R}_j$ is an unbiased estimator of $R_j$ with a constant variance $\\sigma^2$ across all $j$. Similar assumptions apply to $\\hat{A}_j$.\n- **Selection Rule**: $\\hat{\\jmath} = \\arg\\min_{j} \\hat{R}_j$ (or $\\arg\\max_{j} \\hat{A}_j$).\n- **Objective**: Obtain an unbiased estimate of external performance on new patients.\n- **First Principles**:\n    1.  Generalization performance is $R(f) = \\mathbb{E}_{(X,Y)\\sim P}[L(f(X),Y)]$.\n    2.  Empirical estimators $\\hat{R}$ are random variables.\n    3.  Selecting an extremum over multiple noisy estimates induces bias.\n    4.  Independence of tuning data and evaluation data is required for unbiased external performance estimation.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded and well-posed. It addresses a critical and standard topic in statistical machine learning and its application in systems biomedicine: the estimation of generalization performance.\n- **Scientific Soundness**: The concepts of generalization error, empirical risk, hyperparameter tuning, cross-validation, selection bias (optimism), and data leakage are all fundamental and accurately described. The scenario ($p \\gg n$, patient-level data) is a realistic representation of challenges in modern biomedical research.\n- **Completeness and Consistency**: The problem is self-contained. It provides the necessary context (high-dimensional data, hyperparameter tuning), the statistical challenge (selection bias), and the principles required to solve it. The simplifying assumptions (unbiased estimators with constant variance) are standard for theoretical analysis of this phenomenon and do not invalidate the problem's practical implications.\n- **Clarity**: The language is precise and objective. The goal is clearly stated: to find a procedure that yields an unbiased estimate of external performance.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. We may proceed with the solution.\n\n### Derivation of the Correct Procedure\n\nThe core of the problem lies in the fact that the performance estimate of a model selected via a tuning procedure is optimistically biased if the performance is measured on the same data used for tuning. Let $\\hat{A}_j$ be the CV estimate of AUC for hyperparameter setting $j$. The tuning process selects $\\hat{\\jmath} = \\arg\\max_{j} \\hat{A}_j$. The resulting performance estimate is $\\hat{A}_{\\hat{\\jmath}} = \\max_j \\hat{A}_j$.\n\nDue to random sampling variability in the data, each $\\hat{A}_j$ is a random variable. The maximum of a set of random variables will, on average, be greater than the maximum of their true means. That is, $\\mathbb{E}[\\max_j \\hat{A}_j] \\ge \\max_j \\mathbb{E}[\\hat{A}_j]$. If we assume $\\mathbb{E}[\\hat{A}_j] = A_j$ (the true AUC for configuration $j$), then $\\mathbb{E}[\\hat{A}_{\\hat{\\jmath}}] \\ge \\max_j A_j$. This estimate is biased upwards. More formally, the quantity we report, $\\hat{A}_{\\hat{\\jmath}}$, is an estimate of the true performance of the selected model, $A_{\\hat{\\jmath}}$. Because $\\hat{\\jmath}$ was chosen specifically to maximize $\\hat{A}_j$, $\\hat{A}_{\\hat{\\jmath}}$ is an optimistic estimate of $A_{\\hat{\\jmath}}$.\n\nTo obtain an unbiased estimate, we must evaluate the performance of the entire model-building *procedure* (which includes hyperparameter tuning) on data that was held entirely separate from the procedure itself. This is the fourth principle provided: \"Independence of tuning data and evaluation data is necessary for unbiased external performance estimation.\"\n\nThis principle directly leads to a **nested resampling** strategy, such as nested cross-validation.\n\n1.  **Outer Loop (for Performance Estimation)**: The dataset is partitioned into $K_{\\mathrm{outer}}$ folds. One fold is held out as a test set, and the remaining $K_{\\mathrm{outer}}-1$ folds constitute the training set. This loop is repeated $K_{\\mathrm{outer}}$ times, with each fold serving as the test set once. To respect the data structure (\"multiple biopsies per patient\"), this partitioning must be done at the **patient level**. All data from a single patient must reside in the same fold.\n\n2.  **Inner Loop (for Hyperparameter Tuning)**: For each outer loop iteration, the corresponding training set is used to run a complete model-building pipeline. This pipeline includes hyperparameter tuning. To select the best hyperparameters for *this specific training set*, an inner cross-validation is performed on it. The training set is split into $K_{\\mathrm{inner}}$ folds, and the hyperparameter configuration $\\hat{\\jmath}_k$ (for outer fold $k$) that yields the best average performance across these inner folds is selected.\n\n3.  **Data Leakage Control**: Any data-driven preprocessing step is part of the model-fitting procedure and must be learned exclusively from the training data within each outer loop. This includes feature normalization/standardization (computing means and variances), supervised feature selection, and so on. These steps must be re-learned from scratch for each of the $K_{\\mathrm{outer}}$ training sets. Applying a transformation learned on the full dataset before splitting constitutes data leakage and invalidates the performance estimate.\n\n4.  **Evaluation and Aggregation**: Once the inner loop determines the optimal hyperparameter setting $\\hat{\\jmath}_k$ for outer fold $k$, a new model is trained with this setting on the *entire* outer training set. The performance of this model is then evaluated on the pristine, held-out outer test set. The performance metrics (AUCs) from each of the $K_{\\mathrm{outer}}$ test sets are then averaged to produce a single, unbiased estimate of the generalization performance of the overall modeling strategy.\n\nThis complete, rigorous procedure ensures that at no point is the final performance evaluation contaminated by the data used for any aspect of model selection or tuning.\n\n### Option-by-Option Analysis\n\n**A. Perform $K$-fold CV once on the full dataset, standardize features using the full dataset, select $\\hat{\\jmath}$ by maximizing the average $\\hat{A}_j$ across folds, and report that same cross-validated $\\hat{A}_{\\hat{\\jmath}}$ as the external performance. This uses all data once and avoids an extra outer loop.**\n- **Analysis**: This is a non-nested CV procedure. It uses the same CV splits to both select the best model ($\\hat{\\jmath}$) and to report its performance ($\\hat{A}_{\\hat{\\jmath}}$). As derived above, this introduces an optimistic selection bias. Furthermore, it explicitly prescribes standardizing features on the full dataset, a clear case of data leakage where information from test folds contaminates the training folds.\n- **Verdict**: **Incorrect**.\n\n**B. Use nested CV: Partition patients into $K_{\\mathrm{outer}}$ outer folds at the patient level to avoid correlation leakage across biopsies. For each outer training set, repeat all preprocessing steps (for example, standardization using means and variances computed only from inner-training splits, supervised feature selection confined to inner resamples, and probability calibration) strictly within an inner $K_{\\mathrm{inner}}$-fold CV to select $\\hat{\\jmath}$; then refit $f_{\\hat{\\jmath}}$ on the entire outer training set with the inner-derived preprocessing pipeline and evaluate on the untouched outer test set. Average the outer-fold AUCs to estimate external performance.**\n- **Analysis**: This option correctly describes the nested CV procedure. It specifies:\n    1.  A nested structure to separate tuning from evaluation.\n    2.  Patient-level partitioning for the outer folds to handle correlated data.\n    3.  Encapsulation of all preprocessing and tuning steps within the outer loop's training set, thus preventing data leakage.\n    4.  Evaluation on an untouched outer test set in each fold.\n    5.  Averaging outer-fold performance for the final estimate.\nThis procedure rigorously adheres to the principles for obtaining an unbiased performance estimate.\n- **Verdict**: **Correct**.\n\n**C. Hold out a single test set once at the start, tune hyperparameters by repeated CV on the remaining data, and, if ties or near-ties occur among configurations, break them by inspecting performance on the held-out test set before finalizing $\\hat{\\jmath}$; then report the test-set AUC as external performance.**\n- **Analysis**: A single hold-out set is a valid approach (equivalent to $K_{\\mathrm{outer}}=1$ in a sense), but this option describes a critical flaw: \"inspecting performance on the held-out test set before finalizing $\\hat{\\jmath}$\". This act of \"peeking\" at the test set to inform a modeling decision, even for a tie-break, means the test set is no longer independent. It has been used for tuning, thus invalidating its use for unbiased performance estimation.\n- **Verdict**: **Incorrect**.\n\n**D. Use non-nested bootstrap: For each bootstrap resample, standardize using the full resample, tune hyperparameters by maximizing out-of-bag AUC, evaluate the tuned model on the same out-of-bag cases, and average AUC across resamples to report external performance. This reuses out-of-bag cases for both tuning and evaluation.**\n- **Analysis**: This procedure uses the out-of-bag (OOB) samples for both hyperparameter tuning (\"maximizing out-of-bag AUC\") and for final performance evaluation (\"evaluate... on the same out-of-bag cases\"). This is the same fundamental error as in non-nested CV (Option A). The OOB data is used to select the best model, which creates an optimistic bias when that same data is used to report performance. A correct bootstrap-based procedure would also require a nested structure (e.g., a \"`.632+`\" estimator or a nested bootstrap).\n- **Verdict**: **Incorrect**.\n\n**E. Use nested CV but perform a supervised univariate feature ranking on the full dataset first to select a subset of top features; then conduct inner CV only for hyperparameters on that fixed feature subset and evaluate on outer folds, reporting averaged outer AUC as external performance.**\n- **Analysis**: This option correctly identifies the need for nested CV but introduces a major data leakage error upfront. Performing supervised feature selection on the \"full dataset\" before any partitioning means that information from the labels of the outer test sets is used to decide which features are included in the model. This makes the feature set itself biased and will lead to an overly optimistic performance estimate. All feature selection must be done inside the outer loop, using only the outer training data for that fold.\n- **Verdict**: **Incorrect**.", "answer": "$$\\boxed{B}$$", "id": "4376899"}, {"introduction": "Beyond statistical accuracy, the true value of a clinical prediction model lies in its ability to improve decision-making. Decision Curve Analysis (DCA) is a powerful framework for evaluating this clinical utility by weighing the benefits of correct predictions against the harms of incorrect ones. This practice [@problem_id:4376938] guides you through the foundational derivation of DCA's \"net benefit,\" connecting it directly to the decision-maker's tolerance for risk and grounding model evaluation in the principles of utility theory.", "problem": "A clinical prediction model in systems biomedicine outputs for each patient a calibrated risk score interpreted as a probability $p$ of benefiting from a targeted therapy (e.g., carrying a pharmacogenomic feature that makes a drug both effective and safe). In personalized medicine, a clinician uses a threshold probability $\\tau$ to decide treatment: treat if $p \\geq \\tau$, otherwise do not treat. Consider a binary health state $D \\in \\{0,1\\}$, where $D=1$ denotes that the patient would benefit if treated (for example, has the actionable target) and $D=0$ denotes that the patient would not benefit (and might be harmed) if treated. Let the decision be $T \\in \\{0,1\\}$ with $T=1$ meaning treat and $T=0$ meaning do not treat.\n\nAssume an expected utility framework grounded in the following fundamental base:\n\n- Utilities are defined up to affine transformation and can be translated and scaled without changing optimal decisions. Let utilities be anchored so that both no-treatment outcomes have zero utility: $U(T=0,D=1)=0$ and $U(T=0,D=0)=0$.\n- Let the incremental utility of correctly treating a patient who would benefit be $\\Delta_{1} = U(T=1,D=1) - U(T=0,D=1)  0$, representing the benefit of appropriate treatment in those who benefit.\n- Let the incremental utility of treating a patient who would not benefit be $-\\Delta_{0}$, where $\\Delta_{0} = U(T=0,D=0) - U(T=1,D=0)  0$, representing the harm from inappropriate treatment in those who do not benefit.\n- The threshold probability $\\tau$ is defined by indifference at the patient level: at $p=\\tau$, the expected utility of treating equals the expected utility of not treating, so treatment is neither preferred nor disfavored.\n\nIn a cohort of $N$ patients to whom the threshold rule is applied, let $TP$ denote the number of true positives (treated, $T=1$, with $D=1$) and $FP$ denote the number of false positives (treated, $T=1$, with $D=0$). Decision Curve Analysis (DCA) summarizes clinical value via the net benefit, defined as the average expected utility difference between the threshold policy and treating no one, after scaling utilities to the unit of a correctly treated true positive.\n\nStarting from the definitions above and the threshold indifference condition, derive a closed-form expression for the net benefit as a function of the threshold $\\tau$ and the counts $(TP,FP)$ in the cohort, normalized per patient by $N$ and expressed in units of $\\Delta_{1}$ (that is, the net benefit should count each true positive as $+1$ and each false positive as a loss weighted by a threshold-dependent factor). Your final expression must be written in terms of $TP$, $FP$, $N$, and $\\tau$. Provide the expression only; no numerical rounding is required and no units need be reported in the final boxed answer.", "solution": "The problem asks for the derivation of the net benefit (NB) formula used in Decision Curve Analysis (DCA). The derivation will proceed in three main steps: first, establishing a relationship between the utility costs and benefits ($\\Delta_0$, $\\Delta_1$) and the decision threshold $\\tau$; second, calculating the total utility gain of a decision policy relative to a default strategy; and third, normalizing this utility gain to arrive at the final expression for net benefit.\n\nStep 1: Derive the utility ratio from the threshold indifference condition.\n\nThe problem states that the decision threshold $\\tau$ is set at the probability $p$ where a clinician would be indifferent between treating ($T=1$) and not treating ($T=0$). This indifference implies that the expected utility of treating is equal to the expected utility of not treating, conditional on the predicted probability being equal to the threshold, $p=\\tau$.\n\nThe expected utility of a decision is the sum of the utilities of each possible outcome weighted by their respective probabilities. The outcomes depend on the true underlying state of the patient, $D \\in \\{0,1\\}$. For a patient with a risk score $p=\\tau$, the model is assumed to be calibrated, meaning the probability that the patient will actually benefit is $P(D=1|p=\\tau) = \\tau$. Consequently, the probability that the patient will not benefit is $P(D=0|p=\\tau) = 1-\\tau$.\n\nThe expected utility of treating ($T=1$) a patient with risk score $p=\\tau$ is:\n$$E[U|T=1, p=\\tau] = U(T=1, D=1) \\cdot P(D=1|p=\\tau) + U(T=1, D=0) \\cdot P(D=0|p=\\tau)$$\n$$E[U|T=1, p=\\tau] = U(T=1, D=1) \\cdot \\tau + U(T=1, D=0) \\cdot (1-\\tau)$$\n\nThe expected utility of not treating ($T=0$) a patient with risk score $p=\\tau$ is:\n$$E[U|T=0, p=\\tau] = U(T=0, D=1) \\cdot P(D=1|p=\\tau) + U(T=0, D=0) \\cdot P(D=0|p=\\tau)$$\n$$E[U|T=0, p=\\tau] = U(T=0, D=1) \\cdot \\tau + U(T=0, D=0) \\cdot (1-\\tau)$$\n\nThe problem defines the utilities such that $U(T=0, D=1) = 0$ and $U(T=0, D=0) = 0$. Therefore, the expected utility of not treating is always zero:\n$$E[U|T=0, p=\\tau] = 0 \\cdot \\tau + 0 \\cdot (1-\\tau) = 0$$\n\nAt the indifference threshold $\\tau$, we set the expected utilities equal:\n$$E[U|T=1, p=\\tau] = E[U|T=0, p=\\tau]$$\n$$U(T=1, D=1) \\cdot \\tau + U(T=1, D=0) \\cdot (1-\\tau) = 0$$\n\nNow, we substitute the definitions of the incremental utilities. The benefit of correct treatment is $\\Delta_{1} = U(T=1,D=1) - U(T=0,D=1)$. Since $U(T=0,D=1)=0$, this gives $U(T=1,D=1) = \\Delta_{1}$. The harm from incorrect treatment is $-\\Delta_{0}$, where $\\Delta_{0} = U(T=0,D=0) - U(T=1,D=0)$. Since $U(T=0,D=0)=0$, this gives $U(T=1,D=0) = -\\Delta_{0}$.\n\nSubstituting these into the indifference equation:\n$$\\Delta_{1} \\cdot \\tau + (-\\Delta_{0}) \\cdot (1-\\tau) = 0$$\n$$\\Delta_{1} \\tau = \\Delta_{0} (1-\\tau)$$\n\nThis equation provides a critical relationship between the ratio of harm to benefit and the decision threshold:\n$$\\frac{\\Delta_{0}}{\\Delta_{1}} = \\frac{\\tau}{1-\\tau}$$\nThis ratio represents the \"exchange rate\" between the harm of a false positive and the benefit of a true positive that is implied by choosing the threshold $\\tau$.\n\nStep 2: Calculate the total utility of the decision policy.\n\nThe net benefit is defined relative to the policy of treating no one. The total utility for a cohort of $N$ patients under the \"treat none\" policy is $0$, since the utility is $0$ for all patients not receiving treatment, regardless of their true state $D$.\n\nNext, we calculate the total utility for the cohort under the decision policy that uses the threshold $\\tau$. Patients are treated if their risk score $p \\geq \\tau$. In the given cohort, this results in $TP$ true positives and $FP$ false positives.\n- A true positive is a patient who is treated ($T=1$) and would benefit ($D=1$). There are $TP$ such patients. The utility for each is $U(T=1,D=1) = \\Delta_1$.\n- A false positive is a patient who is treated ($T=1$) but would not benefit ($D=0$). There are $FP$ such patients. The utility for each is $U(T=1,D=0) = -\\Delta_0$.\n- Patients who are not treated (true negatives and false negatives) all have $T=0$, so their utility is $0$ according to the problem's definition.\n\nThe total utility for the cohort under this policy is the sum of utilities for all patients:\n$$\\text{Total Utility}_{\\text{policy}} = TP \\cdot U(T=1,D=1) + FP \\cdot U(T=1,D=0)$$\n$$\\text{Total Utility}_{\\text{policy}} = TP \\cdot \\Delta_1 - FP \\cdot \\Delta_0$$\n\nStep 3: Normalize to find the Net Benefit per patient.\n\nThe net benefit is the average utility difference between the policy and the \"treat none\" default, scaled to the unit of a true positive benefit ($\\Delta_1$).\n\nFirst, the total utility difference (the total net utility) is:\n$$\\text{Total Net Utility} = \\text{Total Utility}_{\\text{policy}} - \\text{Total Utility}_{\\text{treat none}}$$\n$$\\text{Total Net Utility} = (TP \\cdot \\Delta_1 - FP \\cdot \\Delta_0) - 0 = TP \\cdot \\Delta_1 - FP \\cdot \\Delta_0$$\n\nTo find the average net utility per patient, we divide by the total number of patients, $N$:\n$$\\text{Average Net Utility} = \\frac{TP \\cdot \\Delta_1 - FP \\cdot \\Delta_0}{N}$$\n\nFinally, this value is scaled by dividing by the benefit of one true positive, $\\Delta_1$, to express the net benefit in units of true positives.\n$$\\text{Net Benefit (NB)} = \\frac{\\text{Average Net Utility}}{\\Delta_1} = \\frac{\\frac{TP \\cdot \\Delta_1 - FP \\cdot \\Delta_0}{N}}{\\Delta_1}$$\n$$\\text{NB} = \\frac{1}{N} \\left( \\frac{TP \\cdot \\Delta_1}{\\Delta_1} - \\frac{FP \\cdot \\Delta_0}{\\Delta_1} \\right)$$\n$$\\text{NB} = \\frac{1}{N} \\left( TP - FP \\cdot \\frac{\\Delta_0}{\\Delta_1} \\right)$$\n\nNow, we substitute the relationship derived in Step 1, $\\frac{\\Delta_0}{\\Delta_1} = \\frac{\\tau}{1-\\tau}$, into this expression:\n$$\\text{NB} = \\frac{1}{N} \\left( TP - FP \\cdot \\frac{\\tau}{1-\\tau} \\right)$$\n\nThis is the final closed-form expression for the net benefit per patient as a function of $TP$, $FP$, $N$, and the decision threshold $\\tau$. It can also be written as the difference between the true positive rate and a weighted false positive rate:\n$$\\text{NB} = \\frac{TP}{N} - \\frac{FP}{N} \\left( \\frac{\\tau}{1-\\tau} \\right)$$\nThis formula shows that the net benefit of a model-based decision strategy is the rate of true positives it identifies, penalized by the rate of false positives it incurs, where the penalty for each false positive is determined by the decision threshold $\\tau$.", "answer": "$$\\boxed{\\frac{1}{N} \\left( TP - FP \\frac{\\tau}{1-\\tau} \\right)}$$", "id": "4376938"}, {"introduction": "Deploying predictive models in diverse populations requires a careful examination of fairness to ensure they do not exacerbate health disparities. However, different mathematical definitions of fairness can be mutually exclusive, presenting complex ethical and statistical trade-offs. Through this exercise [@problem_id:4376953], you will formally prove a foundational result in algorithmic fairness: for an imperfect model, it is impossible to satisfy both calibration and equalized odds simultaneously when outcome rates differ between groups, highlighting the need for deliberate choices when designing equitable systems.", "problem": "A clinical decision support system produces a continuous risk score $S \\in [0,1]$ to predict a binary outcome $Y \\in \\{0,1\\}$ (e.g., risk of a severe adverse drug reaction within $30$ days), for patients belonging to one of two subgroups $A \\in \\{0,1\\}$ (e.g., two ancestries or two clinical sites). Let the subgroup prevalences be $\\pi_a = \\mathbb{P}(Y=1 \\mid A=a)$ with $\\pi_0 \\neq \\pi_1$. The system is deployed as a single-threshold classifier $\\hat{Y}_t = \\mathbb{I}\\{S \\ge t\\}$, where the same threshold $t \\in (0,1)$ is used for all subgroups.\n\nPrecise fairness and reliability notions for such risk models are defined as follows:\n\n- Equalized odds for a risk score: the score $S$ is conditionally independent of subgroup given the true outcome, i.e., $S \\perp A \\mid Y$. Equivalently, for every threshold $t \\in (0,1)$, the classifier $\\hat{Y}_t$ achieves equal true positive rates and equal false positive rates across subgroups, where the True Positive Rate (TPR) and False Positive Rate (FPR) for subgroup $a$ at threshold $t$ are $\\mathrm{TPR}_a(t) = \\mathbb{P}(S \\ge t \\mid Y=1, A=a)$ and $\\mathrm{FPR}_a(t) = \\mathbb{P}(S \\ge t \\mid Y=0, A=a)$, respectively.\n\n- Calibration within groups: for each subgroup $a$ and score value $s \\in (0,1)$, the conditional probability equals the score, i.e., $\\mathbb{P}(Y=1 \\mid S=s, A=a) = s$.\n\nStarting from the definitions above and Bayesâ€™ rule, derive the likelihood ratio $\\frac{f_{S \\mid Y=1,A=a}(s)}{f_{S \\mid Y=0,A=a}(s)}$ under calibration within groups, and explain the incompatibility trade-off between equalized odds and calibration when $\\pi_0 \\neq \\pi_1$ and the score is not perfectly predictive. Then, answer the following multiple choice question by selecting all correct statements.\n\nA. A single-threshold classifier $\\hat{Y}_t = \\mathbb{I}\\{S \\ge t\\}$ applied uniformly to all subgroups can simultaneously satisfy equalized odds and calibration within groups when $\\pi_0 \\neq \\pi_1$, provided the threshold $t$ is chosen appropriately.\n\nB. If $\\pi_0 \\neq \\pi_1$ and the risk score $S$ is not perfectly predictive (i.e., $S$ takes values in $(0,1)$ with nonzero probability in both outcome classes), then no single-threshold classifier applied uniformly can simultaneously achieve equalized odds and preserve calibration within groups of $S$.\n\nC. Under calibration within groups, the likelihood ratio $\\frac{f_{S \\mid Y=1,A=a}(s)}{f_{S \\mid Y=0,A=a}(s)}$ equals $\\frac{s}{1-s}$ and hence does not depend on subgroup prevalence.\n\nD. The incompatibility between equalized odds and calibration within groups disappears if $S$ is perfectly predictive (i.e., $S \\in \\{0,1\\}$ almost surely), even when $\\pi_0 \\neq \\pi_1$.\n\nE. Equalized odds requires that $S$ be independent of $A$ marginally (i.e., $S \\perp A$), therefore equalized odds cannot hold unless prevalences are equal.", "solution": "The problem statement is first validated for scientific soundness, clarity, and completeness.\n\n### Step 1: Extract Givens\n-   A continuous risk score $S \\in [0,1]$.\n-   A binary outcome $Y \\in \\{0,1\\}$.\n-   Two subgroups $A \\in \\{0,1\\}$.\n-   Subgroup-specific outcome prevalences: $\\pi_a = \\mathbb{P}(Y=1 \\mid A=a)$.\n-   Condition on prevalences: $\\pi_0 \\neq \\pi_1$.\n-   A single-threshold classifier: $\\hat{Y}_t = \\mathbb{I}\\{S \\ge t\\}$ for $t \\in (0,1)$.\n-   Definition of Equalized Odds: The score $S$ is conditionally independent of the subgroup $A$ given the true outcome $Y$, denoted as $S \\perp A \\mid Y$. This is stated as being equivalent to equal True Positive Rates (TPR) and False Positive Rates (FPR) across subgroups for every threshold $t$.\n    -   $\\mathrm{TPR}_a(t) = \\mathbb{P}(S \\ge t \\mid Y=1, A=a)$\n    -   $\\mathrm{FPR}_a(t) = \\mathbb{P}(S \\ge t \\mid Y=0, A=a)$\n    -   Equalized odds implies $\\mathrm{TPR}_0(t) = \\mathrm{TPR}_1(t)$ and $\\mathrm{FPR}_0(t) = \\mathrm{FPR}_1(t)$ for all $t \\in (0,1)$. This is equivalent to the score distributions being equal across groups, conditioned on the outcome: $f_{S \\mid Y, A}(s \\mid y, 0) = f_{S \\mid Y, A}(s \\mid y, 1)$ for $y \\in \\{0,1\\}$.\n-   Definition of Calibration within Groups: For each subgroup $a$ and score value $s \\in (0,1)$, the conditional probability of the outcome equals the score: $\\mathbb{P}(Y=1 \\mid S=s, A=a) = s$.\n-   Condition for incompatibility: The score is not perfectly predictive, meaning it takes values in the interval $(0,1)$ with non-zero probability.\n-   Task: Derive the likelihood ratio $\\frac{f_{S \\mid Y=1,A=a}(s)}{f_{S \\mid Y=0,A=a}(s)}$ under calibration, explain the incompatibility of equalized odds and calibration under the given conditions, and evaluate the provided options.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined and scientifically grounded. It presents a classic and important result in the study of fairness in machine learning, first formally shown by Kleinberg, Mullainathan, and Raghavan. The definitions of equalized odds and calibration are standard in the field. The premises ($\\pi_0 \\neq \\pi_1$ and an imperfect score) are the precise conditions under which the incompatibility arises. The problem does not violate any scientific principles, is not incomplete or contradictory, and uses precise, objective language. It is a formal, verifiable mathematical problem.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. The solution will now be derived.\n\n### Derivation of the Likelihood Ratio and Incompatibility\nThe task requires deriving the likelihood ratio $\\frac{f_{S \\mid Y=1,A=a}(s)}{f_{S \\mid Y=0,A=a}(s)}$ assuming the score $S$ is calibrated within groups.\n\nWe start from Bayes' rule, which connects posterior probabilities, prior probabilities, and likelihoods. The posterior odds are equal to the likelihood ratio multiplied by the prior odds. Within a specific subgroup $A=a$, the \"priors\" are the subgroup-specific prevalences.\n$$ \\frac{\\mathbb{P}(Y=1 \\mid S=s, A=a)}{\\mathbb{P}(Y=0 \\mid S=s, A=a)} = \\frac{f_{S \\mid Y=1, A=a}(s)}{f_{S \\mid Y=0, A=a}(s)} \\times \\frac{\\mathbb{P}(Y=1 \\mid A=a)}{\\mathbb{P}(Y=0 \\mid A=a)} $$\n\nHere, $f_{S \\mid Y, A}$ represents the conditional probability density function of the score $S$. We can rearrange this to solve for the likelihood ratio:\n$$ \\frac{f_{S \\mid Y=1, A=a}(s)}{f_{S \\mid Y=0, A=a}(s)} = \\frac{\\mathbb{P}(Y=1 \\mid S=s, A=a)}{\\mathbb{P}(Y=0 \\mid S=s, A=a)} \\times \\frac{\\mathbb{P}(Y=0 \\mid A=a)}{\\mathbb{P}(Y=1 \\mid A=a)} $$\n\nNow, we apply the given definitions:\n1.  **Calibration within groups**: $\\mathbb{P}(Y=1 \\mid S=s, A=a) = s$. It follows that $\\mathbb{P}(Y=0 \\mid S=s, A=a) = 1 - \\mathbb{P}(Y=1 \\mid S=s, A=a) = 1-s$.\n2.  **Subgroup prevalence**: $\\mathbb{P}(Y=1 \\mid A=a) = \\pi_a$. It follows that $\\mathbb{P}(Y=0 \\mid A=a) = 1 - \\pi_a$.\n\nSubstituting these into the equation for the likelihood ratio:\n$$ \\frac{f_{S \\mid Y=1, A=a}(s)}{f_{S \\mid Y=0, A=a}(s)} = \\frac{s}{1-s} \\times \\frac{1-\\pi_a}{\\pi_a} $$\nThis is the expression for the likelihood ratio for subgroup $a$ under the assumption of calibration within groups.\n\nNext, we explain the incompatibility.\n**Equalized odds** requires that the distribution of the score $S$ conditioned on the outcome $Y$ is independent of the group $A$. This means $f_{S \\mid Y,A}(s \\mid y, a)$ does not depend on $a$. Consequently, the distributions for $a=0$ and $a=1$ are identical:\n-   $f_{S \\mid Y=1, A=0}(s) = f_{S \\mid Y=1, A=1}(s)$\n-   $f_{S \\mid Y=0, A=0}(s) = f_{S \\mid Y=0, A=1}(s)$\n\nIf both calibration and equalized odds hold, then the likelihood ratio must be the same for both groups $a=0$ and $a=1$.\n$$ \\frac{f_{S \\mid Y=1, A=0}(s)}{f_{S \\mid Y=0, A=0}(s)} = \\frac{f_{S \\mid Y=1, A=1}(s)}{f_{S \\mid Y=0, A=1}(s)} $$\n\nUsing our derived expression for the likelihood ratio under calibration:\n$$ \\frac{s}{1-s} \\frac{1-\\pi_0}{\\pi_0} = \\frac{s}{1-s} \\frac{1-\\pi_1}{\\pi_1} $$\n\nThe problem states the score is not perfectly predictive, so there exist score values $s \\in (0,1)$ for which the density is non-zero. For any such $s$, the term $\\frac{s}{1-s}$ is a non-zero, finite number and can be cancelled from both sides:\n$$ \\frac{1-\\pi_0}{\\pi_0} = \\frac{1-\\pi_1}{\\pi_1} $$\n$$ \\frac{1}{\\pi_0} - 1 = \\frac{1}{\\pi_1} - 1 $$\n$$ \\frac{1}{\\pi_0} = \\frac{1}{\\pi_1} $$\n$$ \\pi_0 = \\pi_1 $$\n\nThis result, $\\pi_0 = \\pi_1$, contradicts the problem's premise that $\\pi_0 \\neq \\pi_1$. Therefore, for a non-perfect predictor, it is impossible for the score $S$ to satisfy both calibration within groups and equalized odds when the subgroup prevalences are unequal.\n\n### Option-by-Option Analysis\n\n**A. A single-threshold classifier $\\hat{Y}_t = \\mathbb{I}\\{S \\ge t\\}$ applied uniformly to all subgroups can simultaneously satisfy equalized odds and calibration within groups when $\\pi_0 \\neq \\pi_1$, provided the threshold $t$ is chosen appropriately.**\nThis statement is incorrect. Equalized odds and calibration within groups are properties of the risk score $S$ over its entire range of values, not of a classifier $\\hat{Y}_t$ at a single threshold $t$. The derivation above demonstrates a fundamental incompatibility between these two properties of the score $S$ itself, under the conditions of an imperfect predictor and unequal prevalences. No choice of a single threshold $t$ can resolve this fundamental conflict between the properties of the underlying score function.\n**Verdict: Incorrect.**\n\n**B. If $\\pi_0 \\neq \\pi_1$ and the risk score $S$ is not perfectly predictive (i.e., $S$ takes values in $(0,1)$ with nonzero probability in both outcome classes), then no single-threshold classifier applied uniformly can simultaneously achieve equalized odds and preserve calibration within groups of $S$.**\nThis statement correctly summarizes the incompatibility theorem derived above. \"Preserve calibration\" means the score $S$ is calibrated. \"Achieve equalized odds\" means the score $S$ satisfies equalized odds. As shown, these two properties cannot hold simultaneously for an imperfect score $S$ when prevalences $\\pi_a$ differ. The classifier $\\hat{Y}_t$ is merely a downstream application of the score $S$; if $S$ cannot have both properties, then no classifier based on it can be said to operate under these two conditions simultaneously.\n**Verdict: Correct.**\n\n**C. Under calibration within groups, the likelihood ratio $\\frac{f_{S \\mid Y=1,A=a}(s)}{f_{S \\mid Y=0,A=a}(s)}$ equals $\\frac{s}{1-s}$ and hence does not depend on subgroup prevalence.**\nThis is incorrect. As derived previously, the likelihood ratio is:\n$$ \\frac{f_{S \\mid Y=1, A=a}(s)}{f_{S \\mid Y=0, A=a}(s)} = \\frac{s}{1-s} \\times \\frac{1-\\pi_a}{\\pi_a} $$\nThe term $\\frac{s}{1-s}$ is the posterior odds $\\frac{\\mathbb{P}(Y=1 \\mid S=s, A=a)}{\\mathbb{P}(Y=0 \\mid S=s, A=a)}$. The likelihood ratio is this quantity divided by the prior odds $\\frac{\\pi_a}{1-\\pi_a}$. The expression clearly contains the term $\\pi_a$, the subgroup prevalence. Therefore, the likelihood ratio does depend on the subgroup prevalence.\n**Verdict: Incorrect.**\n\n**D. The incompatibility between equalized odds and calibration within groups disappears if $S$ is perfectly predictive (i.e., $S \\in \\{0,1\\}$ almost surely), even when $\\pi_0 \\neq \\pi_1$.**\nThis statement is correct. A perfectly predictive score means $S=Y$ almost surely. Let's check the two conditions.\n1.  **Calibration**: $\\mathbb{P}(Y=1 \\mid S=s, A=a)=s$.\n    -   If $s=1$, we evaluate $\\mathbb{P}(Y=1 \\mid S=1, A=a)$. Since $S=1$ implies $Y=1$, this is $\\mathbb{P}(Y=1 \\mid Y=1, A=a) = 1$. The condition $1=s$ is met.\n    -   If $s=0$, we evaluate $\\mathbb{P}(Y=1 \\mid S=0, A=a)$. Since $S=0$ implies $Y=0$, this is $\\mathbb{P}(Y=1 \\mid Y=0, A=a) = 0$. The condition $0=s$ is met.\n    So, a perfect predictor is calibrated.\n2.  **Equalized Odds**: $S \\perp A \\mid Y$. This means the distribution of $S$ given $Y$ is the same for all $A$.\n    -   For $Y=1$, the distribution of $S$ is a point mass at $S=1$ (since $S=Y$), regardless of $A$.\n    -   For $Y=0$, the distribution of $S$ is a point mass at $S=0$ (since $S=Y$), regardless of $A$.\n    So, a perfect predictor satisfies equalized odds.\nSince a perfect predictor satisfies both conditions simultaneously, the incompatibility does not apply. The mathematical proof of incompatibility relied on $s \\in (0,1)$, which is not the case for a perfect predictor.\n**Verdict: Correct.**\n\n**E. Equalized odds requires that $S$ be independent of $A$ marginally (i.e., $S \\perp A$), therefore equalized odds cannot hold unless prevalences are equal.**\nThis statement is incorrect. The premise is false. Equalized odds is $S \\perp A \\mid Y$ (conditional independence). This does not imply marginal independence ($S \\perp A$). To see this, we can write the marginal distribution of $S$ for a group $a$ using the law of total probability:\n$f_{S|A}(s|a) = f_{S|Y=1,A}(s|1,a) \\mathbb{P}(Y=1|A=a) + f_{S|Y=0,A}(s|0,a) \\mathbb{P}(Y=0|A=a)$.\nUnder equalized odds, $f_{S|Y,A}(s|y,a)$ does not depend on $a$, so we can write it as $f_{S|Y}(s|y)$:\n$f_{S|A}(s|a) = f_{S|Y}(s|1) \\pi_a + f_{S|Y}(s|0) (1-\\pi_a)$.\nFor marginal independence ($S \\perp A$), $f_{S|A}(s|a)$ must not depend on $a$. This would require $f_{S|Y}(s|1) \\pi_a + f_{S|Y}(s|0) (1-\\pi_a) = C$ for some constant $C$ for all $a$. If $\\pi_0 \\neq \\pi_1$, this can only hold if $f_{S|Y}(s|1) = f_{S|Y}(s|0)$, which means the score is not predictive of the outcome. For any useful, non-trivial score, equalized odds combined with unequal prevalences *implies* a violation of marginal independence. The premise is false, and the reasoning is flawed.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{BD}$$", "id": "4376953"}]}