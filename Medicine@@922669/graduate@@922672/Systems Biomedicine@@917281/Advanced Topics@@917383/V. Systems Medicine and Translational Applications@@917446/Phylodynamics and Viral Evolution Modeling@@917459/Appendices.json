{"hands_on_practices": [{"introduction": "A core task in phylodynamics is to infer past population dynamics from the branching patterns of a phylogenetic tree. The coalescent likelihood is the fundamental mathematical link between a tree's coalescent event times and a proposed demographic model. This practice [@problem_id:4374502] will guide you to derive this likelihood from first principles for an exponential growth model, using the concepts of an inhomogeneous Poisson process. Mastering this derivation and its implementation is essential for understanding how parametric models like those with parameters for effective population size ($N_{e0}$) and growth rate ($r$) are fitted in Bayesian phylogenetics.", "problem": "You are given an isochronous, dated phylogeny with all sampling at present time and with known coalescent event times. Consider the Kingman coalescent with time-varying effective population size under an exponential growth model. Let $t \\ge 0$ denote time measured backward from the present in years. Assume an effective population size trajectory $N_{e}(t) = N_{e0} \\exp(- r t)$ with growth rate $r$ in units of $\\text{year}^{-1}$ and present-time effective population size $N_{e0}$ in individuals. The phylogeny has $n$ tips sampled at time $t = 0$, and the $n-1$ internal nodes (coalescent events) occur at strictly increasing times $0  t_1  t_2  \\dots  t_{n-1}$ in years before present. At backward time $t \\in [t_{i-1}, t_{i})$ (with $t_0 = 0$), there are $k_i = n - (i-1)$ lineages.\n\nTask: Starting only from the two foundational facts\n- when there are $k$ lineages under the Kingman coalescent with time-varying effective population size $N_{e}(t)$, the instantaneous total coalescent hazard is the binomial coefficient $\\binom{k}{2}$ divided by $N_{e}(t)$, and\n- the density of event times for an inhomogeneous Poisson process with hazard $h(t)$ over an interval combines the survival factor $\\exp\\!\\left(- \\int h(u)\\,du \\right)$ and the hazard evaluated at the event time,\nderive the coalescent log-likelihood for the given tree under the model $N_{e}(t) = N_{e0} \\exp(- r t)$, expressed as a function of $r$ and $N_{e0}$. Your derivation should make clear what tree-derived quantities are required and how they combine, and it should explicitly handle the boundary case $r = 0$ as the limiting case of the exponential model.\n\nThen, implement a program that computes the natural log-likelihood (a dimensionless real number) for several specified test cases. Each test case provides: the number of tips $n$, the strictly increasing list of coalescent times $[t_1, \\dots, t_{n-1}]$ in years before present, the growth rate $r$ in $\\text{year}^{-1}$, and $N_{e0}$ in individuals. Your program must:\n- compute the log-likelihood using only the principles stated above and your derived expression,\n- treat $r = 0$ using the explicit limit of your formula,\n- assume all times are in years, $r$ is in $\\text{year}^{-1}$, and $N_{e0}$ is in individuals,\n- return a single list containing the log-likelihood for each test case as a floating-point number.\n\nAlso, indicate in your derivation which statistics of the tree are sufficient for evaluating the likelihood under this model.\n\nTest suite to implement inside your program (no input is read):\n- Case A (general): $n = 5$, times $[0.2, 0.5, 1.1, 1.8]$, $r = 0.5$, $N_{e0} = 8000$.\n- Case B (boundary $r=0$): $n = 5$, times $[0.2, 0.5, 1.1, 1.8]$, $r = 0.0$, $N_{e0} = 8000$.\n- Case C (few lineages, moderate growth): $n = 3$, times $[0.1, 2.0]$, $r = 0.2$, $N_{e0} = 1000$.\n- Case D (negative growth parameter, i.e., growth forward in time): $n = 4$, times $[0.05, 0.07, 0.09]$, $r = -0.1$, $N_{e0} = 2000$.\n- Case E (large positive $r$): $n = 4$, times $[0.3, 0.6, 1.2]$, $r = 1.5$, $N_{e0} = 500$.\n\nRequired final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC,resultD,resultE]\"). Each result must be the natural log-likelihood value for the corresponding case in the same order as listed above. No units should be printed, and no additional text should be output.", "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n- **Model:** Kingman coalescent with time-varying effective population size, $N_{e}(t)$.\n- **Population Size Trajectory:** $N_{e}(t) = N_{e0} \\exp(-r t)$, where $t \\ge 0$ is time measured backward from the present in years. $N_{e0}$ is the present-time effective population size, and $r$ is the exponential growth rate in $\\text{year}^{-1}$.\n- **Phylogeny Data:** An isochronous, dated phylogeny with $n$ tips sampled at $t=0$.\n- **Coalescent Times:** A set of $n-1$ strictly increasing event times: $0  t_1  t_2  \\dots  t_{n-1}$ years before present.\n- **Lineage Count:** In any backward time interval $[t_{i-1}, t_i)$ (with $t_0 = 0$), there are $k_i = n - (i-1)$ lineages.\n- **Foundational Fact 1:** For $k$ lineages, the instantaneous total coalescent hazard is $\\lambda_k(t) = \\binom{k}{2} / N_e(t)$.\n- **Foundational Fact 2:** The probability density function for an event time from an inhomogeneous Poisson process with hazard $h(t)$ is the product of the hazard at the event time and the survival factor: $p(t) = h(t) \\exp\\left(-\\int_{t_{start}}^{t} h(u)\\,du \\right)$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on the Kingman coalescent model, a fundamental framework in population genetics and phylodynamics. The exponential growth model for effective population size is a standard and well-characterized model. The relationship between hazard rates and likelihood for inhomogeneous Poisson processes is a core concept in probability theory. The problem is scientifically and mathematically sound.\n- **Well-Posed:** The problem provides all necessary definitions, constants, and data to derive the log-likelihood function. It requests a specific output (the log-likelihood value) for defined inputs, making the objective clear and computationally solvable.\n- **Objective:** The problem is stated in precise mathematical and scientific language, free of subjectivity or ambiguity.\n- **Completeness and Consistency:** The givens are self-contained and consistent. The definitions of $N_e(t)$, coalescent hazard, and likelihood density are standard and mutually compatible.\n- **All other criteria are met:** The problem is formalizable, realistic, well-structured, and verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n## Derivation of the Coalescent Log-Likelihood\n\nThe likelihood of observing the given set of coalescent times $\\{t_1, \\dots, t_{n-1}\\}$ is the product of the probabilities of two types of events for each interval $[t_{i-1}, t_i)$: (1) no coalescent event occurs in the open interval $(t_{i-1}, t_i)$, and (2) a coalescent event occurs at time $t_i$. This is governed by the theory of inhomogeneous Poisson processes.\n\n**1. Define the Hazard Function**\nThe time is measured backwards from the present ($t=0$). In the interval $[t_{i-1}, t_i)$, there are $k_i = n-(i-1)$ lineages. According to the problem statement, the instantaneous coalescent hazard is:\n$$ \\lambda_{k_i}(t) = \\frac{\\binom{k_i}{2}}{N_e(t)} $$\nSubstituting the model for effective population size, $N_e(t) = N_{e0} \\exp(-rt)$, we get:\n$$ \\lambda_{k_i}(t) = \\frac{\\binom{k_i}{2}}{N_{e0} \\exp(-rt)} = \\frac{\\binom{k_i}{2}}{N_{e0}} e^{rt} $$\n\n**2. Formulate the Likelihood Contribution for Each Interval**\nUsing the provided rule for the density of event times for an inhomogeneous Poisson process, the likelihood contribution for observing the $i$-th coalescent event at time $t_i$, conditional on no event happening in $[t_{i-1}, t_i)$, is:\n$$ L_i = \\lambda_{k_i}(t_i) \\exp\\left( - \\int_{t_{i-1}}^{t_i} \\lambda_{k_i}(u) \\, du \\right) $$\n\n**3. Formulate the Total Log-Likelihood**\nThe total likelihood for the entire tree is the product of the likelihood contributions from each coalescent event, as these events are conditionally independent:\n$$ L = \\prod_{i=1}^{n-1} L_i $$\nThe total log-likelihood, $\\mathcal{L} = \\ln(L)$, is the sum of the individual log-likelihoods:\n$$ \\mathcal{L} = \\sum_{i=1}^{n-1} \\ln(L_i) = \\sum_{i=1}^{n-1} \\left[ \\ln(\\lambda_{k_i}(t_i)) - \\int_{t_{i-1}}^{t_i} \\lambda_{k_i}(u) \\, du \\right] $$\nThis separates the log-likelihood into two components: a sum of log-hazards evaluated at the event times, and a sum of integrated hazards over the waiting intervals.\n\n**4. Case 1: General Case ($r \\neq 0$)**\nWe evaluate the two components of the log-likelihood.\n\n*   **Log-Hazard Component:**\n    $$ \\sum_{i=1}^{n-1} \\ln(\\lambda_{k_i}(t_i)) = \\sum_{i=1}^{n-1} \\ln\\left( \\frac{\\binom{k_i}{2}}{N_{e0}} e^{rt_i} \\right) = \\sum_{i=1}^{n-1} \\left[ \\ln\\binom{k_i}{2} - \\ln(N_{e0}) + rt_i \\right] $$\n    $$ = \\left( \\sum_{i=1}^{n-1} \\ln\\binom{k_i}{2} \\right) - (n-1)\\ln(N_{e0}) + r \\sum_{i=1}^{n-1} t_i $$\n\n*   **Integrated Hazard Component:**\n    $$ \\int_{t_{i-1}}^{t_i} \\lambda_{k_i}(u) \\, du = \\int_{t_{i-1}}^{t_i} \\frac{\\binom{k_i}{2}}{N_{e0}} e^{ru} \\, du = \\frac{\\binom{k_i}{2}}{N_{e0}} \\left[ \\frac{e^{ru}}{r} \\right]_{t_{i-1}}^{t_i} = \\frac{\\binom{k_i}{2}}{rN_{e0}} (e^{rt_i} - e^{rt_{i-1}}) $$\n    The total integrated hazard is the sum over all intervals:\n    $$ \\sum_{i=1}^{n-1} \\int_{t_{i-1}}^{t_i} \\lambda_{k_i}(u) \\, du = \\frac{1}{rN_{e0}} \\sum_{i=1}^{n-1} \\binom{k_i}{2} (e^{rt_i} - e^{rt_{i-1}}) $$\n\n*   **Full Log-Likelihood for $r \\neq 0$:**\n    Combining the components gives the final expression:\n    $$ \\mathcal{L}(r, N_{e0}) = \\left(\\sum_{i=1}^{n-1} \\ln\\binom{k_i}{2}\\right) - (n-1)\\ln(N_{e0}) + r\\sum_{i=1}^{n-1} t_i - \\frac{1}{rN_{e0}} \\sum_{i=1}^{n-1} \\binom{k_i}{2} (e^{rt_i} - e^{rt_{i-1}}) $$\n\n**5. Case 2: Boundary Case ($r = 0$)**\nWhen $r=0$, the population size is constant: $N_e(t) = N_{e0}$. The hazard is also constant within each interval with $k_i$ lineages: $\\lambda_{k_i}(t) = \\binom{k_i}{2} / N_{e0}$.\n\n*   **Log-Hazard Component:**\n    $$ \\sum_{i=1}^{n-1} \\ln(\\lambda_{k_i}(t_i)) = \\sum_{i=1}^{n-1} \\ln\\left( \\frac{\\binom{k_i}{2}}{N_{e0}} \\right) = \\left(\\sum_{i=1}^{n-1} \\ln\\binom{k_i}{2}\\right) - (n-1)\\ln(N_{e0}) $$\n\n*   **Integrated Hazard Component:**\n    $$ \\int_{t_{i-1}}^{t_i} \\lambda_{k_i}(u) \\, du = \\int_{t_{i-1}}^{t_i} \\frac{\\binom{k_i}{2}}{N_{e0}} \\, du = \\frac{\\binom{k_i}{2}}{N_{e0}} (t_i - t_{i-1}) $$\n    The total integrated hazard is:\n    $$ \\sum_{i=1}^{n-1} \\frac{\\binom{k_i}{2}}{N_{e0}} (t_i - t_{i-1}) = \\frac{1}{N_{e0}} \\sum_{i=1}^{n-1} \\binom{k_i}{2} (t_i - t_{i-1}) $$\n\n*   **Full Log-Likelihood for $r = 0$:**\n    $$ \\mathcal{L}(0, N_{e0}) = \\left(\\sum_{i=1}^{n-1} \\ln\\binom{k_i}{2}\\right) - (n-1)\\ln(N_{e0}) - \\frac{1}{N_{e0}} \\sum_{i=1}^{n-1} \\binom{k_i}{2} (t_i - t_{i-1}) $$\n\n**6. Consistency Check: Limit as $r \\to 0$**\nTo ensure consistency, we take the limit of the general formula for $\\mathcal{L}(r, N_{e0})$ as $r \\to 0$. The critical term is the integrated hazard. We use the Taylor expansion $e^x \\approx 1+x$ for small $x$:\n$$ \\lim_{r\\to 0} \\frac{1}{rN_{e0}} \\sum_{i=1}^{n-1} \\binom{k_i}{2} (e^{rt_i} - e^{rt_{i-1}}) $$\n$$ = \\lim_{r\\to 0} \\frac{1}{rN_{e0}} \\sum_{i=1}^{n-1} \\binom{k_i}{2} ((1+rt_i) - (1+rt_{i-1}) + O(r^2)) $$\n$$ = \\lim_{r\\to 0} \\frac{1}{rN_{e0}} \\sum_{i=1}^{n-1} \\binom{k_i}{2} (r(t_i - t_{i-1}) + O(r^2)) $$\n$$ = \\frac{1}{N_{e0}} \\sum_{i=1}^{n-1} \\binom{k_i}{2} (t_i - t_{i-1}) $$\nThe term $r \\sum t_i \\to 0$ as $r \\to 0$. Thus, the limit of the general expression equals the expression derived specifically for $r=0$, confirming the correctness of the formulas.\n\n**7. Sufficient Statistics**\nTo evaluate the log-likelihood, one needs the number of samples $n$ and the full set of coalescent times $\\{t_1, t_2, \\dots, t_{n-1}\\}$. The expressions involve sums that depend on each individual $t_i$ and the corresponding interval lengths $(t_i - t_{i-1})$, which cannot be reduced to a smaller set of summary statistics (like only the sum of times or the total tree height). Therefore, the pair $(n, \\{t_1, \\dots, t_{n-1}\\})$ constitutes the sufficient statistics of the tree for this model.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_log_likelihood(n: int, times: list[float], r: float, Ne0: float) - float:\n    \"\"\"\n    Computes the coalescent log-likelihood for a dated phylogeny under an exponential growth model.\n\n    Args:\n        n: The number of tips in the phylogeny.\n        times: A list of n-1 strictly increasing coalescent event times before present.\n        r: The exponential growth rate (r  0 for population decline backwards in time).\n        Ne0: The effective population size at present (t=0).\n\n    Returns:\n        The natural log-likelihood.\n    \"\"\"\n    if n  2:\n        return 0.0\n\n    num_coalescent_events = n - 1\n    # Prepend t_0 = 0 to the list of coalescent times.\n    all_times = [0.0] + times\n\n    # --- Term 1: Sum of log-hazards at event times ---\n    # This term is of the form:\n    # sum(ln(C(k_i, 2))) - (n-1)*ln(Ne0) + r*sum(t_i)\n    \n    # Calculate sum(ln(C(k_i, 2)))\n    sum_log_binom_k_2 = 0.0\n    for i in range(1, num_coalescent_events + 1):\n        # In the interval [t_{i-1}, t_i), there are k_i = n - (i-1) lineages.\n        k_i = n - (i - 1)\n        binom_k_2 = k_i * (k_i - 1) / 2.0\n        # If binom_k_2 is 0 or less (e.g., k_i  2), its log is -inf.\n        # This only happens if n  2, handled at start.\n        sum_log_binom_k_2 += np.log(binom_k_2)\n\n    # Calculate r * sum(t_i)\n    sum_of_times = sum(times)\n    r_sum_t = r * sum_of_times\n\n    log_hazard_term = sum_log_binom_k_2 - num_coalescent_events * np.log(Ne0) + r_sum_t\n\n    # --- Term 2: Sum of integrated hazards over intervals ---\n    # This term is subtracted from Term 1.\n    \n    # This will hold sum( C(k_i, 2) * (t_i - t_{i-1}) ) for r=0\n    # or sum( C(k_i, 2) * (exp(r*t_i) - exp(r*t_{i-1})) ) for r!=0\n    integrated_hazard_sum_raw = 0.0\n\n    if r == 0.0:\n        # Constant population size case\n        for i in range(1, num_coalescent_events + 1):\n            k_i = n - (i - 1)\n            binom_k_2 = k_i * (k_i - 1) / 2.0\n            t_curr = all_times[i]\n            t_prev = all_times[i-1]\n            interval_length = t_curr - t_prev\n            integrated_hazard_sum_raw += binom_k_2 * interval_length\n        integrated_hazard_term = integrated_hazard_sum_raw / Ne0\n    else:\n        # Exponential growth/decline case\n        for i in range(1, num_coalescent_events + 1):\n            k_i = n - (i - 1)\n            binom_k_2 = k_i * (k_i - 1) / 2.0\n            t_curr = all_times[i]\n            t_prev = all_times[i-1]\n            integrated_hazard_sum_raw += binom_k_2 * (np.exp(r * t_curr) - np.exp(r * t_prev))\n        integrated_hazard_term = integrated_hazard_sum_raw / (r * Ne0)\n    \n    # Final log-likelihood is Term 1 - Term 2\n    log_likelihood = log_hazard_term - integrated_hazard_term\n    \n    return log_likelihood\n\n\ndef solve():\n    \"\"\"\n    Solves for the log-likelihood for the predefined test cases.\n    \"\"\"\n    test_cases = [\n        # Case A (general)\n        {'n': 5, 'times': [0.2, 0.5, 1.1, 1.8], 'r': 0.5, 'Ne0': 8000},\n        # Case B (boundary r=0)\n        {'n': 5, 'times': [0.2, 0.5, 1.1, 1.8], 'r': 0.0, 'Ne0': 8000},\n        # Case C (few lineages, moderate growth)\n        {'n': 3, 'times': [0.1, 2.0], 'r': 0.2, 'Ne0': 1000},\n        # Case D (negative growth parameter)\n        {'n': 4, 'times': [0.05, 0.07, 0.09], 'r': -0.1, 'Ne0': 2000},\n        # Case E (large positive r)\n        {'n': 4, 'times': [0.3, 0.6, 1.2], 'r': 1.5, 'Ne0': 500},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_log_likelihood(case['n'], case['times'], case['r'], case['Ne0'])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{res:.8f}' for res in results)}]\")\n\nsolve()\n```", "id": "4374502"}, {"introduction": "Before inferring parameters from a time-stamped phylogeny, it is critical to confirm that the data contains a genuine \"temporal signal\"—a significant correlation between sampling dates and genetic divergence. Without this signal, estimates of substitution rates and timelines are meaningless. This exercise introduces the Date Randomization Test (DRT), a powerful permutation-based method to statistically test this assumption. By implementing a DRT [@problem_id:4374506], you will gain a practical tool for model diagnostics and learn how to quantify the confidence in your data's suitability for molecular clock analysis.", "problem": "You are tasked with implementing a Date Randomization Test (DRT) for assessing temporal signal in a molecular clock analysis within phylodynamics and viral evolution modeling. Consider a dataset consisting of $n = 100$ viral sequences sampled over a span of $3$ years. For each sequence $i$, you are provided two quantities: a sampling time $t_i$ measured in years relative to an arbitrary origin, and a root-to-tip genetic distance $d_i$ measured in substitutions per site from a time-calibrated phylogeny. Under the strict molecular clock assumption, the expected root-to-tip genetic distance is linear in sampling time. Formally, a well-tested model states that\n$$\n\\mathbb{E}[d_i \\mid t_i] = r \\cdot (t_i - t_0),\n$$\nwhere $r$ is the substitution rate in substitutions per site per year, and $t_0$ is the time of the root in the same units.\n\nThe Date Randomization Test (DRT) assesses whether there is a temporal signal by comparing the observed estimate of $r$ to a null distribution formed by randomly permuting sampling times among sequences to remove any association between sampling time and genetic divergence. The following definitions and procedures apply:\n\n1. The ordinary least squares estimate of the clock rate $r$ from data $\\{(t_i, d_i)\\}_{i=1}^n$ is the slope from regressing $d_i$ on $t_i$, given by\n$$\n\\hat{r} = \\frac{\\sum_{i=1}^{n} (t_i - \\bar{t})(d_i - \\bar{d})}{\\sum_{i=1}^{n} (t_i - \\bar{t})^2},\n$$\nwhere $\\bar{t}$ and $\\bar{d}$ are the sample means of $t_i$ and $d_i$ respectively.\n2. Under the null hypothesis of no temporal signal, the sampling times and genetic distances are independent, and permuting $\\{t_i\\}$ among sequences generates the null distribution of the estimator $\\hat{r}$. The empirical null distribution is obtained by computing $\\hat{r}^{(b)}$ for $b = 1, \\dots, B$ independent random permutations of the times.\n3. The one-sided p-value for detecting a positive temporal signal is computed using the permutation distribution as\n$$\np = \\frac{1 + \\sum_{b=1}^{B} \\mathbb{I}\\left(\\hat{r}^{(b)} \\ge \\hat{r}\\right)}{B + 1},\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. The detection decision is then made by comparing $p$ to a significance threshold $\\alpha$; detection occurs if $p  \\alpha$.\n\nYour program must implement the above test and produce results for the following test suite, where synthetic data are generated as follows. For each test case, let $t_i$ be independently drawn from a uniform distribution over the interval $[0, 3]$ years, with $n = 100$ sequences. Let $t_0 = -10$ years, and generate root-to-tip distances according to\n$$\nd_i = r_{\\text{true}} \\cdot (t_i - t_0) + \\epsilon_i,\n$$\nwith $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ independently across $i$. To ensure scientific realism, root-to-tip distances must be nonnegative; after generation, any $d_i$ that is negative must be replaced by a small positive constant $10^{-6}$ substitutions per site. The units for the estimated rate must be substitutions per site per year, and all outputs should be expressed as floating-point numbers without percentage signs. Use a fixed pseudo-random number generator seed $12345$ for reproducibility.\n\nTest suite parameter sets:\n- Case A (happy path, strong temporal signal): $r_{\\text{true}} = 1.0 \\times 10^{-3}$, $\\sigma = 5.0 \\times 10^{-4}$, $B = 1000$, $\\alpha = 0.05$.\n- Case B (weak signal): $r_{\\text{true}} = 2.0 \\times 10^{-4}$, $\\sigma = 1.0 \\times 10^{-3}$, $B = 1000$, $\\alpha = 0.05$.\n- Case C (null, no signal): $r_{\\text{true}} = 0$, $\\sigma = 1.0 \\times 10^{-3}$, $B = 1000$, $\\alpha = 0.05$.\n- Case D (edge case, high noise overshadowing signal): $r_{\\text{true}} = 1.0 \\times 10^{-3}$, $\\sigma = 5.0 \\times 10^{-3}$, $B = 500$, $\\alpha = 0.05$.\n\nFor each case, compute:\n- The estimated clock rate $\\hat{r}$ in substitutions per site per year as defined above.\n- The one-sided permutation p-value $p$.\n- The detection decision as a boolean indicating whether $p  \\alpha$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list of the form $[\\hat{r}, p, \\text{detected}]$. For example, the format should be $\\big[\\,[\\hat{r}_A, p_A, \\text{detected}_A], [\\hat{r}_B, p_B, \\text{detected}_B], [\\hat{r}_C, p_C, \\text{detected}_C], [\\hat{r}_D, p_D, \\text{detected}_D]\\,\\big]$, expressed using standard Python list syntax.", "solution": "This solution details the implementation of a Date Randomization Test (DRT) to assess the temporal signal in simulated viral sequence data, as specified in the problem statement. The methodology is grounded in fundamental statistical principles used in phylodynamics.\n\n### 1. Underlying Principle: The Strict Molecular Clock and Temporal Signal\n\nThe core assumption is the strict molecular clock model, which posits a linear relationship between the genetic divergence of a sequence from a common ancestor and the time elapsed. For a set of sequences $\\{i\\}_{i=1}^n$ with sampling times $\\{t_i\\}$ and root-to-tip genetic distances $\\{d_i\\}$, the expected distance is given by:\n$$\n\\mathbb{E}[d_i \\mid t_i] = r \\cdot (t_i - t_0)\n$$\nHere, $r$ is the constant substitution rate (the \"clock rate\"), and $t_0$ is the time of the most recent common ancestor (the root of the phylogeny). A \"temporal signal\" exists if there is a statistically significant positive correlation between $d_i$ and $t_i$, which implies that the clock rate $r$ is measurably greater than zero.\n\n### 2. The Date Randomization Test (DRT)\n\nThe DRT is a permutation-based approach to test the null hypothesis $H_0$ that there is no temporal signal ($r = 0$). This is equivalent to stating that the genetic distances $d_i$ and sampling times $t_i$ are independent. The test proceeds as follows:\n\n- **Test Statistic**: The strength of the temporal signal is quantified by the estimated clock rate, $\\hat{r}$. This is calculated as the-slope of an ordinary least squares (OLS) linear regression of $d_i$ on $t_i$. The formula for this estimator is:\n$$\n\\hat{r} = \\frac{\\sum_{i=1}^{n} (t_i - \\bar{t})(d_i - \\bar{d})}{\\sum_{i=1}^{n} (t_i - \\bar{t})^2}\n$$\nwhere $\\bar{t}$ and $\\bar{d}$ are the sample means of the times and distances, respectively. A large positive value of $\\hat{r}$ suggests a strong temporal signal.\n\n- **Null Distribution**: To determine if the observed $\\hat{r}$ is statistically significant, it is compared to a null distribution. This distribution is generated by breaking the real association between times and distances. For a large number of permutations, $b = 1, \\dots, B$:\n    1. The set of sampling times $\\{t_i\\}$ is randomly shuffled to create a permuted set $\\{t_i^{(b)}\\}$.\n    2. The genetic distances $\\{d_i\\}$ are kept in their original order.\n    3. A new rate, $\\hat{r}^{(b)}$, is calculated using the permuted times $\\{t_i^{(b)}\\}$ and original distances $\\{d_i\\}$.\nThe collection of these rates, $\\{\\hat{r}^{(b)}\\}$, forms the empirical null distribution of the rate estimator under $H_0$.\n\n- **P-value and Decision**: The one-sided p-value, which is the probability of observing a rate at least as large as the actually observed $\\hat{r}$ under the null hypothesis, is calculated as:\n$$\np = \\frac{1 + \\sum_{b=1}^{B} \\mathbb{I}\\left(\\hat{r}^{(b)} \\ge \\hat{r}\\right)}{B + 1}\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. The inclusion of $1$ in the numerator and denominator accounts for the observed statistic itself and prevents a p-value of $0$. A temporal signal is \"detected\" if this p-value is less than a pre-defined significance level $\\alpha$, i.e., $p  \\alpha$.\n\n### 3. Implementation and Algorithmic Design\n\nThe solution is implemented in Python using the `numpy` library. A single pseudo-random number generator, seeded with $12345$, is used for all stochastic procedures to ensure full reproducibility.\n\n**Step 1: Data Simulation**\nFor each of the four test cases, a synthetic dataset of $n=100$ sequences is generated.\n- A `numpy.random.Generator` instance, initialized with the seed $12345$, is used.\n- Sampling times $t_i$ are drawn from a uniform distribution over the interval $[0, 3]$.\n- Gaussian noise terms $\\epsilon_i$ are drawn from $\\mathcal{N}(0, \\sigma^2)$, with $\\sigma$ specific to the test case.\n- Genetic distances are computed as $d_i = r_{\\text{true}} \\cdot (t_i - t_0) + \\epsilon_i$, where $r_{\\text{true}}$ is the true clock rate for the case and $t_0 = -10$.\n- To enforce physical realism, any resulting negative distance $d_i$ is floor-clipped to a small positive value, $10^{-6}$.\n\n**Step 2: Estimation and Permutation**\nA single function processes each test case.\n- It first calculates the observed rate $\\hat{r}$ from the simulated $(t_i, d_i)$ pairs using the OLS formula.\n- It then enters a loop for $B$ iterations (where $B$ is $1000$ or $500$ depending on the case). In each iteration, it calls the generator's `permutation` method on the time vector $t$ and re-computes the rate to build the null distribution $\\{\\hat{r}^{(b)}\\}$.\n\n**Step 3: Final Calculation**\n- After the permutation loop, the p-value $p$ is calculated by counting the number of null rates greater than or equal to the observed rate $\\hat{r}$ and applying the specified formula.\n- The boolean detection decision is made by comparing $p$ to the given $\\alpha$ of $0.05$.\n- The results for each case—$[\\hat{r}, p, \\text{detected}]$—are collected.\n\n**Step 4: Output Formatting**\nThe final list of results is converted to the required string representation, which is a standard Python list-of-lists format, and printed to standard output. This structured approach ensures that the complex statistical procedure is translated into a correct and verifiable computational algorithm.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Date Randomization Test (DRT) for assessing temporal signal\n    in simulated viral evolution data, as per the problem specification.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Format: (r_true, sigma, B, alpha)\n    test_cases = [\n        (1.0e-3, 5.0e-4, 1000, 0.05),  # Case A: Strong signal\n        (2.0e-4, 1.0e-3, 1000, 0.05),  # Case B: Weak signal\n        (0.0, 1.0e-3, 1000, 0.05),     # Case C: Null, no signal\n        (1.0e-3, 5.0e-3, 500, 0.05),    # Case D: High noise\n    ]\n\n    # Initialize a single pseudo-random number generator for reproducibility.\n    # All random aspects of the simulation will use this generator instance.\n    rng = np.random.default_rng(12345)\n\n    # General parameters\n    n = 100          # Number of sequences\n    t0 = -10.0       # Time of the root in years\n    t_span_max = 3.0 # Sampling time upper bound in years\n    d_min = 1e-6     # Minimum allowed genetic distance\n\n    results = []\n    \n    def calculate_rate(times, distances):\n        \"\"\"\n        Calculates the OLS slope (rate) of distances regressed on times.\n        \"\"\"\n        t_mean = np.mean(times)\n        d_mean = np.mean(distances)\n        \n        # Denominator of OLS slope formula. Handle case of zero variance in times.\n        # This is extremely unlikely with float uniform random numbers but is good practice.\n        t_var_sum = np.sum((times - t_mean)**2)\n        if t_var_sum == 0:\n            return 0.0\n        \n        # Numerator of OLS slope formula\n        td_cov_sum = np.sum((times - t_mean) * (distances - d_mean))\n        \n        return td_cov_sum / t_var_sum\n\n    for r_true, sigma, B, alpha in test_cases:\n        # Step 1: Generate synthetic data for the current test case\n        # Sampling times t_i are drawn from a uniform distribution.\n        t = rng.uniform(low=0.0, high=t_span_max, size=n)\n\n        # Error terms epsilon_i are drawn from a normal distribution.\n        epsilon = rng.normal(loc=0.0, scale=sigma, size=n)\n\n        # Generate root-to-tip distances d_i based on the strict clock model.\n        d = r_true * (t - t0) + epsilon\n\n        # Enforce non-negativity constraint for genetic distances.\n        d[d  0] = d_min\n        \n        # Step 2: Calculate the observed clock rate\n        r_hat = calculate_rate(t, d)\n        \n        # Step 3: Perform the permutation test\n        null_rates = np.empty(B)\n        for i in range(B):\n            # Permute the sampling times randomly.\n            t_permuted = rng.permutation(t)\n            # Calculate the rate for the permuted data.\n            # This builds the null distribution.\n            null_rates[i] = calculate_rate(t_permuted, d)\n            \n        # Step 4: Compute the one-sided p-value and make a decision\n        # Count how many null rates are greater than or equal to the observed rate.\n        count_ge = np.sum(null_rates = r_hat)\n        \n        # Calculate p-value. The +1s account for the observed statistic.\n        p_value = (1.0 + count_ge) / (B + 1.0)\n        \n        # Decision: A temporal signal is detected if p  alpha.\n        detected = p_value  alpha\n        \n        results.append([r_hat, p_value, detected])\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists is desired.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4374506"}, {"introduction": "While simple parametric models are useful, they may not capture complex demographic histories. Gaussian Processes (GPs) provide a powerful, non-parametric framework for reconstructing the effective population size trajectory, $N_e(t)$, without enforcing a rigid functional form. However, the flexibility of a GP is controlled by hyperparameters, such as the length-scale $\\ell$, which must be chosen carefully to avoid over- or under-smoothing. This advanced practice [@problem_id:4374476] introduces a principled approach to this model selection problem, using blocked cross-validation and a predictive loss function grounded in coalescent theory to find the optimal hyperparameter.", "problem": "Design and implement a complete program that selects a smoothing hyperparameter for a Gaussian Process (GP) based reconstruction of effective population size over time, denoted by $N_e(t)$, using a principled, blocked cross-validation scheme and a predictive loss function defined on held-out time intervals. The scientific foundations and definitions you must use are as follows.\n\nStart from Kingman’s coalescent. When there are $k$ contemporaneous lineages at time $t$, the instantaneous coalescent rate is $\\lambda_k(t) = \\binom{k}{2}/N_e(t)$. For an inter-coalescent interval $[t_i, t_{i+1}]$ during which the number of lineages is constant and equal to $k_i$, the likelihood contribution under a model $N_e(t)$ equals the product of the survival probability (no coalescence) over the interval and the hazard at the end if a coalescent event occurs at $t_{i+1}$. In log form, this yields a contribution $-\\int_{t_i}^{t_{i+1}} \\lambda_{k_i}(t) \\, dt + \\log \\lambda_{k_i}(t_{i+1})$ when the interval ends with a coalescent event. Assume here that every interval in the provided data ends with a coalescent event.\n\nLet $g(t) = \\log N_e(t)$ be modeled as a zero-mean Gaussian Process (GP) with a squared-exponential covariance (also called the radial basis function kernel), $k_\\ell(t, t') = \\sigma_f^2 \\exp\\left(-\\frac{(t - t')^2}{2 \\ell^2}\\right)$, with fixed signal variance $\\sigma_f^2 = 1$ and a tunable length-scale $\\ell  0$. Observations used to fit the GP are constructed from a classical skyline-type moment estimator on training intervals only: for an interval $[t_i, t_{i+1}]$ with duration $\\Delta t_i = t_{i+1} - t_i$ and lineage count $k_i$, define a pseudo-observation at the midpoint $s_i = (t_i + t_{i+1})/2$ as $y_i = \\log\\left(\\binom{k_i}{2} \\, \\Delta t_i\\right)$. These $(s_i, y_i)$ pairs (for intervals in the training folds only) are used in a standard Gaussian Process regression with independent and identically distributed observation noise variance $\\sigma_n^2$, to produce a posterior mean function $\\hat{g}_\\ell(t)$ that depends on the choice of $\\ell$.\n\nDefine a blocked $K$-fold cross-validation scheme appropriate for temporally ordered data: partition the sequence of intervals into $K$ contiguous blocks in chronological order with sizes differing by at most one interval. For fold $j$, fit the GP using only the training blocks (all but block $j$) and evaluate a predictive negative log-likelihood on the held-out block $j$ by substituting $N_e(t) = \\exp(\\hat{g}_\\ell(t))$ into the coalescent log-likelihood on those held-out intervals. For each held-out interval $[t_i, t_{i+1}]$ with lineage count $k_i$, define the fold loss as\n$$\n\\mathcal{L}_i(\\ell) \\;=\\; \\int_{t_i}^{t_{i+1}} \\frac{\\binom{k_i}{2}}{\\exp(\\hat{g}_\\ell(t))} \\, dt \\;-\\; \\log\\!\\left(\\frac{\\binom{k_i}{2}}{\\exp(\\hat{g}_\\ell(t_{i+1}))}\\right),\n$$\nand the fold score as the sum over intervals in the held-out block. The cross-validated score for $\\ell$ is the average of the $K$ fold scores. Numerically approximate each time integral using the trapezoidal rule with $M = 64$ equal subintervals per held-out interval. Select the $\\ell$ that minimizes the average held-out score, breaking ties by choosing the smallest $\\ell$ among the minimizers.\n\nImplement the full procedure above and apply it to the following test suite. In each test case, time is measured in arbitrary consistent units, and you must report the selected $\\ell$ in the same unit. Use $\\sigma_f^2 = 1$ and $\\sigma_n^2 = 0.0025$ in all cases. The number of folds is $K = 3$ in all cases. In each case, the input consists of a list of consecutive inter-coalescent intervals for a single tree with present-day sampling only, so that every interval ends with a coalescent event and the lineage count decreases by one across interval boundaries. For interval $i$, you are given $(t_i, t_{i+1}, k_i)$ via durations $\\Delta t_i$ with $t_0 = 0$ and $t_{i+1} = t_i + \\Delta t_i$, and with $k_i$ taking values $k_1 = 10$, $k_2 = 9$, $k_3 = 8$, $k_4 = 7$, $k_5 = 6$, $k_6 = 5$, $k_7 = 4$, $k_8 = 3$, $k_9 = 2$.\n\n- Test case $1$:\n  - Durations $\\Delta t_i$: $[0.12, 0.09, 0.11, 0.14, 0.20, 0.18, 0.25, 0.30, 0.45]$.\n  - Candidate length-scales $\\ell$: $[0.05, 0.10, 0.20, 0.40]$.\n\n- Test case $2$:\n  - Durations $\\Delta t_i$: $[0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.11, 0.12, 0.13]$.\n  - Candidate length-scales $\\ell$: $[0.20, 0.40, 0.80]$.\n\n- Test case $3$:\n  - Durations $\\Delta t_i$: $[0.05, 0.25, 0.06, 0.30, 0.05, 0.28, 0.06, 0.26, 0.05]$.\n  - Candidate length-scales $\\ell$: $[0.05, 0.10, 0.20]$.\n\nYour program must:\n- Construct midpoints $s_i$ and skyline pseudo-observations $y_i$ only from the training folds in each cross-validation split.\n- For each candidate $\\ell$, compute the Gaussian Process posterior mean $\\hat{g}_\\ell(t)$ at the quadrature nodes needed to evaluate the held-out loss, using the kernel $k_\\ell$ with $\\sigma_f^2 = 1$ and observation noise variance $\\sigma_n^2 = 0.0025$.\n- Compute the average held-out negative log-likelihood across the $K$ folds and select the $\\ell$ that minimizes it, with tie-breaking as specified.\n\nFinal output format: Your program should produce a single line of output containing the selected $\\ell$ for the three test cases, as a comma-separated list enclosed in square brackets (for example, $[0.10,0.40,0.05]$). Report each selected $\\ell$ in time units as a decimal number with standard floating-point formatting. No other text should be printed.", "solution": "The problem requires designing a program to select an optimal smoothing hyperparameter, the length-scale $\\ell$, for a Gaussian Process (GP) model of viral effective population size, $N_e(t)$, over time. The selection is based on a principled, blocked cross-validation scheme and a predictive loss function derived from coalescent theory. The methodology is executed step-by-step as follows.\n\n**1. Scientific Model Formulation**\nThe core of the model lies in Kingman's coalescent theory, which states that for $k$ lineages at time $t$, the rate of coalescence is $\\lambda_k(t) = \\binom{k}{2} / N_e(t)$. The log-effective population size, $g(t) = \\log N_e(t)$, is modeled as a zero-mean Gaussian Process. This GP is characterized by a squared-exponential covariance function, $k_\\ell(t, t') = \\sigma_f^2 \\exp(-\\frac{(t - t')^2}{2\\ell^2})$, with a fixed signal variance $\\sigma_f^2 = 1$ and a tunable length-scale $\\ell$. This formulation provides a non-parametric Bayesian framework for inverting the continuous function $g(t)$ from discrete data.\n\n**2. Cross-Validation and Data Partitioning**\nTo select the optimal $\\ell$ from a set of candidates, a blocked $K$-fold cross-validation is employed. Given that the data consists of a temporally ordered sequence of inter-coalescent intervals, this scheme is crucial to preserve the temporal dependency structure and prevent data leakage from the future into the past. The sequence of $9$ intervals is partitioned into $K=3$ contiguous, non-overlapping blocks of equal size ($3$ intervals each). In each of the $K$ iterations, one block is held out as the test set, while the remaining $K-1$ blocks are combined to form the training set.\n\n**3. Gaussian Process Training and Prediction**\nFor each cross-validation fold, the GP is trained using only the intervals in the training set. The training data is not the raw temporal data but a set of derived pseudo-observations. For each training interval $[t_i, t_{i+1}]$ with lineage count $k_i$ and duration $\\Delta t_i = t_{i+1} - t_i$, we construct a single data point $(s_i, y_i)$. The point is located at the interval's midpoint, $s_i = (t_i + t_{i+1})/2$, and its value, $y_i = \\log(\\binom{k_i}{2} \\Delta t_i)$, is the logarithm of a moment-based estimator for $N_e(t)$ assuming it is constant over that interval.\n\nUsing these training pairs $(S_{train}, Y_{train})$ and a chosen $\\ell$, we perform GP regression. The standard GP equations yield a posterior distribution over functions. We are interested in its mean, $\\hat{g}_\\ell(t)$, which serves as our reconstruction of the log-population size history. For a set of prediction points $S_{pred}$, the posterior mean is given by:\n$$ \\hat{g}_\\ell(S_{pred}) = K(S_{pred}, S_{train}) [K(S_{train}, S_{train}) + \\sigma_n^2 I]^{-1} Y_{train} $$\nwhere $K(\\cdot, \\cdot)$ denotes the matrix of kernel evaluations, $I$ is the identity matrix, and $\\sigma_n^2 = 0.0025$ is the specified observation noise variance that regularizes the model. The term $[K(S_{train}, S_{train}) + \\sigma_n^2 I]^{-1} Y_{train}$ is efficiently computed by solving the linear system $(K(S_{train}, S_{train}) + \\sigma_n^2 I) \\alpha = Y_{train}$ for $\\alpha$.\n\n**4. Predictive Loss Calculation**\nThe performance of a given $\\ell$ is evaluated on the held-out test block. The scoring metric is the negative log-likelihood under the coalescent model, using the GP's posterior mean $\\hat{g}_\\ell(t)$ as the estimate for $g(t)$. For each held-out interval $[t_i, t_{i+1}]$ with $k_i$ lineages, the loss $\\mathcal{L}_i(\\ell)$ is:\n$$ \\mathcal{L}_i(\\ell) \\;=\\; \\int_{t_i}^{t_{i+1}} \\frac{\\binom{k_i}{2}}{\\exp(\\hat{g}_\\ell(t))} \\, dt \\;-\\; \\log\\left(\\frac{\\binom{k_i}{2}}{\\exp(\\hat{g}_\\ell(t_{i+1}))}\\right) $$\nThe integral term is computed numerically using the trapezoidal rule with $M=64$ subintervals. This requires evaluating $\\hat{g}_\\ell(t)$ at $M+1$ equally spaced points within each held-out interval. The total score for a given fold is the sum of these losses over all intervals in that fold. The final cross-validated score for $\\ell$ is the average of these scores across all $K$ folds.\n\n**5. Hyperparameter Selection**\nThe entire cross-validation procedure is repeated for each candidate length-scale $\\ell$. The $\\ell$ that yields the minimum average cross-validated score is selected as the optimal hyperparameter. The problem specifies that in case of a tie in scores, the smallest $\\ell$ should be chosen.\n\n**Implementation Summary**\nThe implementation encapsulates this logic. For each test case, it iterates through the candidate $\\ell$ values. Within this loop, it performs $K$-fold cross-validation. In each fold, it assembles the training data, computes the GP posterior mean function (specifically, the vector $\\alpha$), and then evaluates the loss on the held-out block using numerical integration. These losses are aggregated to compute the final score for $\\ell$. Finally, the $\\ell$ with the best score across all candidates is identified and reported. A single function orchestrates this entire process per test case, using `numpy` for efficient numerical operations and `scipy.special.comb` for binomial coefficients.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import comb\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It orchestrates the hyperparameter selection process for each case\n    and prints the final results in the specified format.\n    \"\"\"\n\n    def run_test_case(durations, lineage_counts, candidate_ls):\n        \"\"\"\n        Performs blocked K-fold cross-validation to select the best\n        GP length-scale hyperparameter for a single test case.\n\n        Args:\n            durations (np.ndarray): Array of inter-coalescent interval durations.\n            lineage_counts (np.ndarray): Array of lineage counts for each interval.\n            candidate_ls (list): List of candidate length-scale values.\n\n        Returns:\n            float: The selected optimal length-scale.\n        \"\"\"\n        # Fixed parameters from the problem statement\n        sigma_f_sq = 1.0\n        sigma_n_sq = 0.0025\n        K = 3\n        M = 64\n\n        # Calculate interval boundaries from durations\n        t_boundaries = np.concatenate(([0], np.cumsum(durations)))\n        num_intervals = len(durations)\n        \n        # Partition interval indices into K contiguous blocks for cross-validation\n        all_indices = np.arange(num_intervals)\n        blocks = np.array_split(all_indices, K)\n        \n        cv_scores = []\n\n        # Squared-exponential kernel function\n        def kernel(t1, t2, l_sq):\n            # Uses broadcasting to compute pairwise squared distances\n            dist_sq = np.subtract.outer(t1, t2)**2\n            return sigma_f_sq * np.exp(-dist_sq / (2 * l_sq))\n\n        # Iterate over each candidate length-scale\n        for l_val in candidate_ls:\n            l_val_sq = l_val**2\n            total_fold_loss = 0.0\n\n            # Perform K-fold cross-validation\n            for j in range(K):\n                # a. Split data into training and test sets\n                test_indices = blocks[j]\n                train_indices_list = [blocks[i] for i in range(K) if i != j]\n                train_indices = np.concatenate(train_indices_list)\n\n                # b. Construct training data (midpoints and pseudo-observations)\n                train_midpoints = []\n                train_y = []\n                for i in train_indices:\n                    t_start, t_end = t_boundaries[i], t_boundaries[i+1]\n                    delta_t = t_end - t_start\n                    midpoint = (t_start + t_end) / 2\n                    \n                    k = lineage_counts[i]\n                    binom_coeff = comb(k, 2, exact=True)\n                    \n                    y_val = np.log(binom_coeff * delta_t)\n                    train_y.append(y_val)\n                    train_midpoints.append(midpoint)\n                \n                S_train = np.array(train_midpoints)\n                Y_train = np.array(train_y)\n                \n                # c. Fit GP: solve for alpha = (K + sigma_n^2*I)^-1 * y\n                K_train_train = kernel(S_train, S_train, l_val_sq)\n                K_solve = K_train_train + sigma_n_sq * np.identity(len(S_train))\n                alpha = np.linalg.solve(K_solve, Y_train)\n\n                # d. Calculate fold score on the held-out block\n                fold_loss = 0.0\n                \n                # Collect all unique points for efficient batch prediction\n                prediction_points_set = set()\n                for i in test_indices:\n                    t_start, t_end = t_boundaries[i], t_boundaries[i+1]\n                    quad_points = np.linspace(t_start, t_end, M + 1)\n                    prediction_points_set.update(quad_points)\n\n                if not prediction_points_set:\n                    total_fold_loss += 0.0\n                    continue\n                \n                S_pred = np.array(sorted(list(prediction_points_set)))\n                \n                # Predict g_hat(t) at all required points\n                K_pred_train = kernel(S_pred, S_train, l_val_sq)\n                g_hat_pred = K_pred_train @ alpha\n                pred_lookup = dict(zip(S_pred, g_hat_pred))\n\n                # Calculate loss for each interval in the held-out block\n                for i in test_indices:\n                    t_start, t_end = t_boundaries[i], t_boundaries[i+1]\n                    k = lineage_counts[i]\n                    binom_coeff = comb(k, 2, exact=True)\n                    \n                    quad_points = np.linspace(t_start, t_end, M + 1)\n                    g_hat_at_quad_points = np.array([pred_lookup[p] for p in quad_points])\n                    \n                    # i. Numerically integrate the rate\n                    integrand_vals = binom_coeff / np.exp(g_hat_at_quad_points)\n                    integral_val = np.trapz(integrand_vals, x=quad_points)\n                    \n                    # ii. Calculate the log-hazard term\n                    g_hat_at_end = g_hat_at_quad_points[-1]\n                    log_hazard = np.log(binom_coeff) - g_hat_at_end\n                    \n                    # iii. Interval loss is the negative log-likelihood\n                    interval_loss = integral_val - log_hazard\n                    fold_loss += interval_loss\n                \n                total_fold_loss += fold_loss\n\n            # Average score for the current l_val\n            avg_cv_score = total_fold_loss / K\n            cv_scores.append((avg_cv_score, l_val))\n        \n        # Select best l (primary sort by score, secondary by l for tie-breaking)\n        cv_scores.sort()\n        best_l = cv_scores[0][1]\n        \n        return best_l\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"durations\": [0.12, 0.09, 0.11, 0.14, 0.20, 0.18, 0.25, 0.30, 0.45],\n            \"candidate_ls\": [0.05, 0.10, 0.20, 0.40]\n        },\n        {\n            \"durations\": [0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.11, 0.12, 0.13],\n            \"candidate_ls\": [0.20, 0.40, 0.80]\n        },\n        {\n            \"durations\": [0.05, 0.25, 0.06, 0.30, 0.05, 0.28, 0.06, 0.26, 0.05],\n            \"candidate_ls\": [0.05, 0.10, 0.20]\n        },\n    ]\n\n    # Lineage counts are the same for all test cases\n    lineage_counts = np.arange(10, 1, -1) # k=10, 9, ..., 2\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(\n            np.array(case[\"durations\"]),\n            lineage_counts,\n            case[\"candidate_ls\"]\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4374476"}]}