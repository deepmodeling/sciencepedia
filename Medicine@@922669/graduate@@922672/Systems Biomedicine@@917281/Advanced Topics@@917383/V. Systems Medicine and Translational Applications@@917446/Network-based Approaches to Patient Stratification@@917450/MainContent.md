## Introduction
In the landscape of modern medicine, the challenge of patient heterogeneity in [complex diseases](@entry_id:261077) looms large, rendering one-size-fits-all treatments increasingly obsolete. While high-throughput technologies generate vast amounts of molecular data for each patient, a critical knowledge gap exists in translating this complex information into clinically actionable insights. Network-based approaches, a cornerstone of systems biomedicine, offer a powerful framework to bridge this gap by modeling the intricate web of similarities among patients to uncover hidden subgroups. This article provides a comprehensive guide to patient stratification using these methods. The journey begins in the 'Principles and Mechanisms' section, where you will learn the fundamental theory behind constructing patient similarity networks and partitioning them into meaningful clusters. Following this, the 'Applications and Interdisciplinary Connections' section will explore real-world use cases, demonstrating how these techniques are applied to integrate multi-omic data and link computational findings to clinical outcomes. Finally, the 'Hands-On Practices' section will solidify your understanding through practical exercises, equipping you to apply these powerful tools in your own research. By navigating these sections, you will gain the expertise to transform high-dimensional patient data into a structured understanding of disease heterogeneity.

## Principles and Mechanisms

The construction and analysis of patient similarity networks represent a cornerstone of modern computational systems biomedicine, providing a powerful framework for dissecting cohort heterogeneity and identifying clinically meaningful patient strata. This chapter delves into the core principles and mechanisms that underpin this approach, progressing from the fundamental choices in network construction to the sophisticated algorithms used for stratification and the critical interpretation of the results. Our exploration will be guided by a central objective: to transform high-dimensional patient data into a structured representation that reveals underlying biological and clinical patterns.

### The Architecture of Patient Similarity: Construction and Representation

The first step in any network-based analysis is to construct the graph itself. This involves defining the nodes, the edges that connect them, and the weights that quantify the strength of those connections. While seemingly straightforward, these initial decisions profoundly influence the outcome of any downstream analysis.

#### Foundational Network Models

A patient similarity network, in its most common form, is a **[simple graph](@entry_id:275276)**, where each node represents a patient and a weighted, undirected edge between two nodes quantifies their overall similarity. This model is ideal for tasks where patient-to-patient relationships can be summarized by a single scalar value, forming a unified similarity matrix. However, the complexity of biomedical data often necessitates more expressive network structures. Different research questions demand different representations, and choosing the appropriate model is a critical first step in faithful [data modeling](@entry_id:141456) [@problem_id:4368717].

-   **Bipartite Graphs:** When the goal is to understand the associations between patients and a set of features (e.g., genes, proteins, or clinical variables), a bipartite graph is the natural choice. This graph consists of two distinct sets of nodes—one for patients and one for features—with edges existing only between nodes of different types. An edge between a patient and a gene, for instance, might represent a significant aberration of that gene in that patient. This structure avoids pre-supposing direct patient-to-patient similarities and instead allows patient relationships to emerge from shared connections to features.

-   **Multiplex Networks:** Patients are often characterized by multiple data modalities (e.g., genomics, [proteomics](@entry_id:155660), clinical data). A multiplex network provides a framework for integrating these modalities without prematurely collapsing them. In this model, a single set of patient nodes exists across multiple layers, where each layer represents a different data type. Intra-layer edges capture patient similarity within a specific modality, while inter-layer edges connect the representations of the same patient across different layers. This preserves modality-specific information while enabling methods that can exploit cross-layer concordance and discordance.

-   **Hypergraphs:** Standard graphs are limited to representing pairwise relationships. However, some biological or clinical phenomena are inherently higher-order. For example, a group of patients might share a specific combination of genetic variants, a relationship that cannot be decomposed into independent pairwise ties without loss of information. A hypergraph generalizes the concept of an edge to a **hyperedge**, which can connect any number of nodes. In a patient context, a hyperedge could represent a set of patients who collectively satisfy a complex, multi-component phenotype, thereby preserving the integrity of these higher-order associations.

#### The Mathematics of Similarity: Metrics and Kernels

The heart of a patient similarity network lies in the function used to compute the edge weights, $w_{ij}$, between patients $i$ and $j$. This function, $s(x_i, x_j)$, takes as input the feature vectors for two patients, $x_i$ and $x_j$, and outputs a measure of their similarity. For these measures to be mathematically sound and useful for downstream algorithms, they often must satisfy certain properties, leading us to the concepts of metrics and kernels [@problem_id:4368738].

A function $d(x, y)$ is a **metric** if it satisfies non-negativity ($d(x,y) \ge 0$), identity of indiscernibles ($d(x,y) = 0 \iff x=y$), symmetry ($d(x,y) = d(y,x)$), and the [triangle inequality](@entry_id:143750) ($d(x,z) \le d(x,y) + d(y,z)$). A [distance function](@entry_id:136611) provides a well-defined notion of separation between patients.

A function $s(x,y)$ is a **positive semidefinite (PSD) kernel** if it is symmetric and, for any finite set of patients $\{x_1, \dots, x_n\}$ and any real coefficients $\{a_1, \dots, a_n\}$, the following condition holds:
$$ \sum_{i=1}^n \sum_{j=1}^n a_i a_j s(x_i, x_j) \ge 0 $$
This condition is equivalent to stating that the Gram matrix $K$ with entries $K_{ij} = s(x_i, x_j)$ is positive semidefinite. The power of PSD kernels lies in Mercer's theorem, which guarantees that such a kernel corresponds to an inner product in some high-dimensional Hilbert space $\mathcal{H}$. That is, there exists a mapping $\phi: x \mapsto \phi(x) \in \mathcal{H}$ such that $s(x,y) = \langle \phi(x), \phi(y) \rangle_{\mathcal{H}}$.

This connection provides a powerful way to define a valid metric from a kernel. Specifically, if $s(x,y)$ is a PSD kernel, the function $d(x,y) = \sqrt{s(x,x) - 2s(x,y) + s(y,y)}$ defines the Euclidean distance in the feature space $\mathcal{H}$, i.e., $d(x,y) = \|\phi(x) - \phi(y)\|_{\mathcal{H}}$. This distance will be a true metric on the patient set, provided the mapping $\phi$ is injective (meaning distinct patients map to distinct vectors in $\mathcal{H}$), which ensures the identity of indiscernibles. This condition is met, for instance, if the Gram matrix $K$ is strictly [positive definite](@entry_id:149459) on the cohort [@problem_id:4368738].

It is important to note that not all intuitive similarity-to-distance conversions yield a valid metric. For example, if a similarity function $s(x,y)$ is normalized to $[0,1]$, the transformation $d(x,y) = 1 - s(x,y)$ does not generally satisfy the triangle inequality and is therefore not a metric.

#### A Practical Toolkit of Similarity Measures

The theoretical properties of metrics and kernels guide our selection of practical similarity measures. The choice depends heavily on the characteristics of the input data [@problem_id:4368777].

-   **Euclidean Distance:** Defined as $d_E(x,y) = \sqrt{\sum_k (x_k - y_k)^2}$, this is the most intuitive measure of distance. It is best suited for data where features are measured on a common, comparable scale and are largely independent. Its sensitivity to feature magnitude makes it less appropriate for data with features of vastly different scales or where relative patterns, rather than [absolute values](@entry_id:197463), are more important.

-   **Cosine Similarity:** Defined as $S_C(x,y) = \frac{\langle x, y \rangle}{\|x\| \|y\|}$, this measure computes the cosine of the angle between two patient vectors. It is insensitive to the length (magnitude) of the vectors and captures similarity in the "shape" or "pattern" of the feature profile. This makes it particularly valuable for data where global scaling effects, such as library size differences in RNA-sequencing, are present but the relative expression patterns are the primary signal of interest. However, [cosine similarity](@entry_id:634957) is not robust to additive shifts in the data, such as those caused by certain [batch effects](@entry_id:265859).

-   **Pearson Correlation:** This is one of the most widely used measures in bioinformatics. The Pearson [correlation coefficient](@entry_id:147037) between two patient vectors (where the vector components are the features for that patient) is mathematically equivalent to the [cosine similarity](@entry_id:634957) of the *mean-centered* vectors. By first subtracting the mean feature value from each patient's profile, Pearson correlation becomes insensitive not only to scaling but also to additive shifts. It is therefore ideal for comparing the shapes of profiles while removing patient-specific differences in both overall magnitude and mean feature level.

-   **Mahalanobis Distance:** Defined as $d_M(x,y) = \sqrt{(x-y)^{\top} \Sigma^{-1} (x-y)}$, where $\Sigma$ is the feature covariance matrix, this distance metric accounts for the correlation structure of the feature space. By incorporating the [inverse covariance matrix](@entry_id:138450) $\Sigma^{-1}$, it effectively "whitens" the space, down-weighting contributions from highly [correlated features](@entry_id:636156) and rescaling features with heterogeneous variances. It measures distance in a transformed space where features are uncorrelated and have unit variance. Mahalanobis distance is the most sophisticated of these measures and is appropriate when feature correlations are a known confounding factor. When features are uncorrelated and have unit variance (i.e., $\Sigma=I$), Mahalanobis distance reduces to Euclidean distance.

#### A Coherent Workflow: From Raw Data to a Patient Similarity Network

Constructing a high-quality patient similarity network is a multi-step process that requires careful statistical treatment at every stage. An illustrative, state-of-the-art pipeline for building a network from raw bulk RNA-sequencing (RNA-seq) [count data](@entry_id:270889) demonstrates the integration of these principles [@problem_id:4368746].

1.  **Quality Control (QC):** The process begins with raw count data. Low-quality samples (e.g., those with very low total counts) and uninformative genes (e.g., those expressed in very few samples) are filtered out. Outlier samples can be identified using methods like Principal Component Analysis (PCA) on appropriately transformed data.

2.  **Normalization and Transformation:** Raw RNA-seq counts are not directly comparable due to variations in [sequencing depth](@entry_id:178191) (library size) and are heteroscedastic (variance depends on the mean). First, patient-specific size factors must be estimated using a robust method like the `median-of-ratios` approach. Then, to address heteroscedasticity, a **[variance-stabilizing transformation](@entry_id:273381) (VST)** is applied. This creates a data matrix where the variance is approximately independent of the mean, making it suitable for downstream methods that assume homoscedasticity.

3.  **Batch Effect Correction:** Technical variability arising from processing samples in different batches is a major confounder. On the log-transformed scale, batch effects are often additive. They can be effectively removed by fitting a gene-wise linear model that includes batch indicators as covariates. Methods like **ComBat**, which use Empirical Bayes shrinkage, are particularly powerful as they "borrow strength" across genes to obtain more stable estimates of [batch effects](@entry_id:265859), especially with small batch sizes.

4.  **Feature Selection:** Not all genes carry information relevant for distinguishing patient subgroups. To enrich for biological signal, an unsupervised [feature selection](@entry_id:141699) step is performed. A common approach is to identify **Highly Variable Genes (HVGs)**. This involves modeling the mean-variance relationship across all genes in the clean, transformed data and selecting those genes that exhibit significantly more variability than predicted by the model.

5.  **Similarity Computation:** With a clean, transformed, and filtered data matrix, the final step is to compute pairwise similarities. Given that the preceding steps yield data that are approximately normally distributed, **Pearson correlation** is an excellent choice. The resulting correlation values are then converted into a non-negative weight matrix, for example via $W_{ij} = (r_{ij} + 1)/2$, ready for analysis.

### Uncovering Patient Subgroups Through Graph Partitioning

Once a patient similarity network is constructed, the central task is to partition its nodes into clusters, or "communities," which correspond to the putative patient strata. A good partition is one where connections within clusters are dense, and connections between clusters are sparse.

#### Spectral Clustering and the Graph Laplacian

**Spectral clustering** is a premier family of algorithms for this task, leveraging the eigenspectrum of a graph Laplacian matrix. The **Graph Laplacian** is a matrix representation of a graph that holds fundamental properties about its structure.

The simplest form is the **unnormalized Laplacian**, defined as $L = D - W$, where $W$ is the weighted [adjacency matrix](@entry_id:151010) and $D$ is the diagonal degree matrix with $D_{ii} = \sum_j W_{ij}$. The quadratic form associated with this Laplacian, $x^{\top} L x = \frac{1}{2} \sum_{i,j} W_{ij} (x_i - x_j)^2$, can be interpreted as a measure of the "energy" or "smoothness" of a signal $x$ on the graph. Minimizing this energy encourages assigning similar values of $x$ to strongly connected nodes.

However, in many patient networks, degree distributions are heterogeneous. Some patients—"hubs"—may have high connectivity due to broad, non-specific similarities, potentially arising from common comorbidities or even residual technical artifacts. When using the unnormalized Laplacian $L$ for clustering, these hubs can unduly dominate the optimization, often leading to trivial partitions where small groups of low-degree nodes are isolated from a single, large, heterogeneous cluster containing all the hubs [@problem_id:4368698].

To overcome this, the **symmetric normalized Laplacian** is used:
$$ L_{\text{sym}} = I - D^{-1/2} W D^{-1/2} $$
The key to its effectiveness lies in its [quadratic form](@entry_id:153497), which can be expressed as:
$$ y^{\top} L_{\text{sym}} y = \frac{1}{2} \sum_{i,j=1}^n W_{ij} \left( \frac{y_i}{\sqrt{d_i}} - \frac{y_j}{\sqrt{d_j}} \right)^2 $$
This formulation reveals that the penalty is no longer on the raw difference between node values but on the difference between *degree-normalized* values. For a hub with a large degree $d_i$, its contribution to the penalty is down-weighted. This normalization equalizes the influence of nodes across the degree spectrum, preventing hubs from anchoring the solution and promoting the discovery of more balanced partitions.

#### The Normalized Cut Objective

The superiority of the normalized Laplacian can be understood more formally by examining the objective function that [spectral clustering](@entry_id:155565) on $L_{\text{sym}}$ implicitly minimizes. This objective is the **Normalized Cut (NCut)** [@problem_id:4368719]. For a bipartition of the graph into sets $A$ and $B$, the Ncut is defined as:
$$ \text{Ncut}(A,B) = \frac{\text{cut}(A,B)}{\text{vol}(A)} + \frac{\text{cut}(A,B)}{\text{vol}(B)} $$
Here, $\text{cut}(A,B) = \sum_{i \in A, j \in B} W_{ij}$ is the sum of weights of all edges crossing the partition, and $\text{vol}(S) = \sum_{i \in S} d_i$ is the volume of a set $S$ (the sum of degrees of its nodes).

The Ncut objective has a beautiful interpretation in terms of a random walk on the graph. The term $\frac{\text{cut}(A,B)}{\text{vol}(A)}$ is precisely the probability that a random walker, starting at a random node in set $A$ (chosen with probability proportional to its degree), will transition to a node in set $B$ in a single step. The Ncut is thus the sum of these "escape probabilities" for both sets. Minimizing Ncut discourages partitions where one of the sets has a very small volume, as this would make the denominator small and the overall objective large. It thereby forces the algorithm to find "balanced" partitions, where balance is measured by volume, not simply by the number of nodes.

The eigenvectors of $L_{\text{sym}}$ provide a relaxed solution to the NP-hard Ncut minimization problem. In particular, the eigenvector corresponding to the second-[smallest eigenvalue](@entry_id:177333), known as the **Fiedler vector**, provides a one-dimensional embedding of the graph nodes. The signs of the components of this vector can be used to partition the graph into two well-separated communities [@problem_id:4368702]. For instance, in a simple network of four patients where patients {1,2} and {3,4} form two distinct groups connected by a weak link between patients 2 and 3, the Fiedler vector will have positive entries for patients 1 and 2 and negative entries for patients 3 and 4, perfectly revealing the underlying bipartition.

### Advanced Network-Based Methods and Interpretations

Building on these foundations, a suite of advanced methods has been developed to tackle more complex challenges in patient stratification, such as integrating multiple omics layers and learning complex patterns from the network structure.

#### Integrating Multi-Omic Data with Similarity Network Fusion (SNF)

When patient data is available from multiple modalities (e.g., gene expression, methylation, proteomics), **Similarity Network Fusion (SNF)** provides a powerful framework to integrate them [@problem_id:4368722]. SNF begins by constructing a separate patient similarity network for each data type, using the principles of local scaling and K-nearest neighbor (KNN) sparsification described earlier to ensure comparability and focus on the most reliable similarities.

The core of SNF is an iterative diffusion process that refines these networks by making them more similar to one another. In each iteration, the network for each view is updated based on the information from all *other* views. This update is constrained by the local neighborhood structure of the network being updated. The update rule for the network from view $v$ at iteration $t+1$ is:
$$ P^{(v)}_{t+1} = S^{(v)} \left(\frac{1}{m-1}\sum_{u \neq v} P^{(u)}_t\right) (S^{(v)})^{\top} $$
Here, $P^{(u)}_t$ is the row-[stochastic matrix](@entry_id:269622) for view $u$ at iteration $t$, and $S^{(v)}$ is the sparse, row-normalized adjacency matrix for view $v$. This process iteratively reinforces edges that are present across multiple data types while diminishing the influence of view-specific noise. After convergence, the networks are fused into a single comprehensive patient similarity network by averaging the final iterated matrices. This final network represents a consensus view of patient relationships, primed for robust stratification.

#### Learning Patient Embeddings with Graph Neural Networks

Recent advances in deep learning have introduced **Graph Neural Networks (GNNs)** as a powerful tool for learning directly from graph-structured data. **Graph Convolutional Networks (GCNs)** are a prominent type of GNN that learn node representations ([embeddings](@entry_id:158103)) by aggregating information from local neighborhoods [@problem_id:4368714].

A GCN consists of multiple layers, where each layer updates the feature vector for each patient node by performing a "[graph convolution](@entry_id:190378)." This operation is a form of **[message passing](@entry_id:276725)**, where each node aggregates feature vectors from its neighbors and itself, transforms them via a trainable weight matrix, and applies a non-linear activation function. The canonical GCN layer update rule is:
$$ H^{(l+1)} = \sigma\left( \tilde{D}^{-1/2} \tilde{W} \tilde{D}^{-1/2} H^{(l)} U^{(l)} \right) $$
Here, $H^{(l)}$ is the matrix of node features at layer $l$, $U^{(l)}$ is the trainable weight matrix, and $\sigma$ is an activation function like ReLU. The propagation matrix, $\tilde{D}^{-1/2} \tilde{W} \tilde{D}^{-1/2}$, is a symmetrically normalized adjacency matrix with self-loops added ($\tilde{W} = W+I$). This specific form of aggregation corresponds to a weighted average of neighbor features, where the normalization by $\sqrt{\tilde{d}_i \tilde{d}_j}$ prevents the feature vectors of high-degree nodes from exploding in magnitude, ensuring [numerical stability](@entry_id:146550). The [self-loop](@entry_id:274670) ensures that a patient's own features from the previous layer are retained.

By stacking these layers, a GCN can propagate information across the graph, effectively smoothing features within densely connected communities. This process produces rich, topology-aware patient [embeddings](@entry_id:158103) that are highly effective for stratification, as they capture both the initial patient features and the complex relational structure of the cohort.

The graph Laplacian also appears in the context of **[semi-supervised learning](@entry_id:636420) (SSL)**, where a few patients may have known labels (e.g., treatment responders). In this setting, an objective function can be designed that includes a data-fitting term for the labeled patients and a smoothness regularization term, $\lambda x^{\top} L x$, for all patients [@problem_id:4368755]. This regularization term penalizes differences in the predicted labels $x$ between strongly connected patients, encouraging the learned labels to be smooth across the graph. As the regularization strength $\lambda$ increases, the decision boundary for classification is forced to pass through sparsely connected regions of the graph, effectively propagating label information from the few labeled patients to the many unlabeled ones.

#### Beyond Correlation: The Quest for Causal Subtypes

The ultimate goal of patient stratification is often to identify subgroups that are not just descriptively similar but are rooted in distinct causal mechanisms. This leads to a critical distinction: a patient similarity network is a **correlational graph**, whereas the underlying biology is governed by a **causal graph** [@problem_id:4368706].

A causal graph is a Directed Acyclic Graph (DAG) where nodes are variables (e.g., genes, exposures, symptoms) and a directed edge $X \to Y$ represents a direct causal influence. The relationship between a causal DAG and observational data is governed by two key assumptions:
1.  **Causal Markov Assumption:** A variable is conditionally independent of its non-descendants, given its direct causes (parents).
2.  **Faithfulness Assumption:** The only conditional independencies present in the data are those implied by the causal graph structure (via a criterion called $d$-separation).

A correlation-based patient network, by contrast, operates on undirected marginal associations. A high correlation between the symptom profiles of two patients can arise for many reasons: one patient's disease state might be a direct cause of the other's (e.g., in infectious disease), they might share a common causal factor (confounding), or they might be subject to similar measurement biases. A similarity network conflates these distinct causal scenarios.

Consequently, clusters identified in a patient similarity network may not always correspond to true, mechanistically distinct subtypes. For example, a cluster could be formed by a group of patients who share a strong environmental confounder that influences their symptoms, even if their underlying disease etiology is heterogeneous. Recognizing this limitation is paramount. While network-based stratification is an immensely powerful tool for generating hypotheses and discovering patterns, its results must be interpreted with caution. The journey from a correlational subtype to a validated causal subtype requires further investigation using experimental data, longitudinal studies, and formal causal inference methods.