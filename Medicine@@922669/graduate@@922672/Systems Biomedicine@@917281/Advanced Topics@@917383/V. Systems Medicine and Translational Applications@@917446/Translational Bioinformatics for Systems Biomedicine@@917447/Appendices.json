{"hands_on_practices": [{"introduction": "In translational genomics, integrating data from diverse sources is a fundamental challenge. This exercise simulates a critical data harmonization task: converting raw genetic variant calls into a standardized format using established biological rules and gene models [@problem_id:4396025]. Mastering this process is essential for building robust analysis pipelines and accurately interpreting the functional impact of genetic variation.", "problem": "You are tasked with designing and implementing a computational pipeline that harmonizes single-nucleotide variant annotations across multiple Variant Call Format (VCF) files by converting them into Human Genome Variation Society (HGVS) coding DNA (c.) nomenclature using Ensembl-like gene models, and then computing the proportion of records that exhibit ambiguous transcript mappings. The pipeline must start from core biological definitions and universally accepted facts about gene structure and transcription, and it must be fully implementable in code.\n\nFundamental basis:\n- The Central Dogma of molecular biology states that deoxyribonucleic acid (DNA) is transcribed into ribonucleic acid (RNA) and translated into protein. A gene consists of exons (expressed regions) and introns (intervening regions). A transcript is a spliced messenger RNA (mRNA) composed of exons in a specific order and orientation, defined by a strand: plus strand (\"+\") transcripts follow increasing genomic coordinates, and minus strand (\"-\") transcripts follow decreasing genomic coordinates.\n- Coding DNA sequence (CDS) coordinates for a transcript are measured along the spliced cDNA, which is the concatenation of all exons in transcript order.\n- Base pairing in double-stranded DNA uses Watson–Crick complementarity: adenine–thymine and cytosine–guanine pairs. On the minus strand, the transcript sequence corresponds to the reverse complement of the genomic positive strand. Therefore, when expressing HGVS c. substitutions for minus-strand transcripts, alleles must be complemented.\n\nPipeline requirements and definitions:\n- Input consists of multiple VCF-like collections of single-nucleotide variants (SNVs), each record defined as a tuple $(\\text{chrom}, \\text{pos}, \\text{ref}, \\text{alt})$ where $\\text{pos}$ is $1$-based and inclusive, and $\\text{ref}, \\text{alt} \\in \\{\\text{A}, \\text{C}, \\text{G}, \\text{T}\\}$ and satisfy $|\\text{ref}| = |\\text{alt}| = 1$.\n- Transcripts are defined by tuples $(\\text{id}, \\text{chrom}, \\text{strand}, \\text{exons})$ where $\\text{exons}$ is an ordered list of closed genomic intervals $[\\text{start}, \\text{end}]$ with $\\text{start} \\le \\text{end}$, and all positions are $1$-based and inclusive. Assume the entire exonic span is coding (no untranslated regions), and exon lists are non-overlapping within a transcript.\n- A variant maps to a transcript if its genomic position falls within any exon of that transcript on the same chromosome.\n- The coding DNA coordinate $c$ for a variant at genomic position $g$ within a transcript with exons $E_1, E_2, \\dots, E_n$ is defined as:\n  - For plus strand (\"+\"): let $L_k = \\sum_{i=1}^{k-1} (|E_i|)$ be the cumulative length of exons preceding the exon $E_k$ that contains $g$, and let $E_k = [s_k, e_k]$. Then\n    $$ c = L_k + (g - s_k + 1). $$\n  - For minus strand (\"-\"): define the transcript order as decreasing genomic coordinate. Let $E'_1, E'_2, \\dots, E'_n$ be exons sorted by decreasing start (equivalently, by decreasing end). Let $L'_k = \\sum_{i=1}^{k-1} (|E'_i|)$ and let $E'_k = [s'_k, e'_k]$ be the exon in this order that contains $g$. Then\n    $$ c = L'_k + (e'_k - g + 1). $$\n- The HGVS coding DNA nomenclature for a substitution is expressed as $\\text{id}:\\mathrm{c}.c\\ \\text{r}>\\text{a}$, where $\\text{id}$ is the transcript identifier, $c$ is the coding DNA coordinate, and $\\text{r}$ and $\\text{a}$ are the reference and alternate alleles, respectively. For minus-strand transcripts, use nucleotide complements for both alleles when forming $\\text{r}$ and $\\text{a}$.\n- Ambiguous mapping is defined as a single unique genomic variant $(\\text{chrom}, \\text{pos}, \\text{ref}, \\text{alt})$ that maps to more than one transcript (producing more than one valid HGVS c. string).\n- Harmonization across VCF files must deduplicate identical variant records across files using the key $(\\text{chrom}, \\text{pos}, \\text{ref}, \\text{alt})$.\n- The requested statistic is the proportion of ambiguous records among the set of unique records that map to at least one transcript. Formally, if $A$ is the number of unique variants with at least two transcript mappings and $M$ is the number of unique variants with at least one transcript mapping, report\n  $$ p = \\begin{cases}\n  \\dfrac{A}{M} & \\text{if } M > 0,\\\\\n  0.0 & \\text{if } M = 0.\n  \\end{cases} $$\n- All answers must be real-valued decimals. Report each proportion rounded to exactly $3$ decimal places.\n\nImplementation constraints:\n- Restrict to single-nucleotide variants as defined above.\n- For minus-strand transcripts, generate HGVS c. substitutions using complemented alleles.\n- Exon intervals are inclusive and may be shared across different transcripts.\n\nTest suite:\n- Test Case $1$:\n  - Transcripts:\n    - ENST000001: chromosome \"chr1\", strand \"+\", exons $[100,110]$ and $[200,210]$.\n    - ENST000002: chromosome \"chr1\", strand \"+\", exons $[205,215]$.\n    - ENST000003: chromosome \"chr1\", strand \"-\", exons $[500,505]$ and $[510,515]$.\n  - VCF A unique records:\n    - $(\\text{chr1}, 205, \\text{G}, \\text{A})$.\n    - $(\\text{chr1}, 102, \\text{A}, \\text{C})$.\n    - $(\\text{chr1}, 600, \\text{T}, \\text{C})$.\n  - VCF B unique records:\n    - $(\\text{chr1}, 205, \\text{G}, \\text{A})$ (duplicate of VCF A).\n    - $(\\text{chr1}, 512, \\text{C}, \\text{T})$.\n    - $(\\text{chr1}, 106, \\text{T}, \\text{G})$.\n  - Expected behavior: After harmonization, the unique set is of size $5$; compute $p$ as defined.\n- Test Case $2$:\n  - Transcripts:\n    - ENST000010: chromosome \"chr2\", strand \"+\", exons $[1000,1005]$ and $[1010,1015]$.\n    - ENST000011: chromosome \"chr2\", strand \"+\", exons $[1005,1012]$.\n    - ENST000012: chromosome \"chr2\", strand \"-\", exons $[2000,2003]$ and $[2005,2008]$.\n  - VCF A unique records:\n    - $(\\text{chr2}, 1005, \\text{A}, \\text{G})$.\n    - $(\\text{chr2}, 1012, \\text{C}, \\text{A})$.\n    - $(\\text{chr2}, 2006, \\text{G}, \\text{A})$.\n  - VCF B unique records:\n    - $(\\text{chr2}, 1005, \\text{A}, \\text{G})$ (duplicate of VCF A).\n    - $(\\text{chr2}, 3000, \\text{T}, \\text{G})$.\n    - $(\\text{chr2}, 2002, \\text{T}, \\text{C})$.\n  - Expected behavior: After harmonization, the unique set is of size $5$; compute $p$ as defined.\n- Test Case $3$:\n  - Transcripts:\n    - ENST000020: chromosome \"chr3\", strand \"-\", exons $[400,405]$.\n  - VCF A unique records:\n    - $(\\text{chr3}, 100, \\text{A}, \\text{C})$.\n    - $(\\text{chr3}, 200, \\text{G}, \\text{T})$.\n  - VCF B unique records:\n    - $(\\text{chr3}, 300, \\text{T}, \\text{A})$.\n  - Expected behavior: No variant maps to any transcript; report $p = 0.0$ by definition.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the three proportions for Test Cases $1$, $2$, and $3$, rounded to exactly $3$ decimal places, enclosed in square brackets. For example, the output must look like \"[0.123,0.456,0.789]\" but with the correct values for this problem.", "solution": "The supplied problem is scientifically and computationally valid. It is grounded in fundamental principles of molecular biology, including the Central Dogma, gene structure (exons and introns), transcription, and standardized nomenclature for genetic variation (HGVS). The problem is well-posed, providing a complete and consistent set of definitions, rules, and constraints that permit the derivation of a unique, verifiable solution. The algorithmic task is to implement a bioinformatics pipeline for harmonizing single-nucleotide variant (SNV) data and quantifying transcript mapping ambiguity, a relevant problem in translational genomics.\n\nThe solution is designed by implementing the specified pipeline in a series of logical steps.\n\n**Step 1: Harmonization of Variant Records**\n\nThe initial step involves aggregating variant records from multiple input sources (VCF-like collections) into a single, non-redundant set. A variant is uniquely identified by the tuple $(\\text{chrom}, \\text{pos}, \\text{ref}, \\text{alt})$, where $\\text{chrom}$ is the chromosome, $\\text{pos}$ is the $1$-based genomic position, $\\text{ref}$ is the reference allele, and $\\text{alt}$ is the alternate allele. By inserting all variant tuples from all sources into a set data structure, duplicates are automatically eliminated. This process yields the set of unique genomic variants to be analyzed.\n\n**Step 2: Variant-to-Transcript Mapping**\n\nFor each unique variant, the pipeline must determine which transcripts it affects. A variant at genomic position $g$ on chromosome $\\text{chrom}_v$ is considered to map to a transcript if:\n$1$. The transcript is located on the same chromosome, $\\text{chrom}_t = \\text{chrom}_v$.\n$2$. The variant's position $g$ falls within any of the transcript's exons. An exon is a closed interval $[s, e]$, so the condition is $s \\le g \\le e$.\n\nThe process involves iterating through each unique variant and, for each variant, iterating through the entire list of provided transcripts. A count of mappings is maintained for each variant.\n\n**Step 3: Definition and Identification of Ambiguity**\n\nAn ambiguous mapping occurs when a single unique genomic variant maps to more than one transcript. This is a common scenario in the human genome due to alternative splicing, where a single gene can produce multiple transcript isoforms with different exon compositions. Each distinct mapping results in a different HGVS c. annotation.\n\nThe problem requires the ability to generate these annotations, which depends on the transcript's strand.\n- For a **plus-strand (\"+\")** transcript with exons $E_1, E_2, \\dots, E_n$ sorted by increasing genomic coordinates, the coding DNA (cDNA) coordinate $c$ for a variant at genomic position $g$ within exon $E_k = [s_k, e_k]$ is given by:\n$$ c = \\left( \\sum_{i=1}^{k-1} (e_i - s_i + 1) \\right) + (g - s_k + 1) $$\nThe term $\\sum_{i=1}^{k-1} (e_i - s_i + 1)$ represents the cumulative length of the exons preceding $E_k$.\n\n- For a **minus-strand (\"-\")** transcript, the exons $E'_1, E'_2, \\dots, E'_n$ are considered in order of decreasing genomic coordinates. The cDNA coordinate $c$ for a variant at genomic position $g$ within exon $E'_k = [s'_k, e'_k]$ is calculated as:\n$$ c = \\left( \\sum_{i=1}^{k-1} (e'_i - s'_i + 1) \\right) + (e'_k - g + 1) $$\nFurthermore, because the transcript is encoded on the reverse-complement strand, the reference and alternate alleles must be complemented for the HGVS string (A$\\leftrightarrow$T, C$\\leftrightarrow$G).\n\nA variant mapping to two different transcripts, say, ENST001 and ENST002, would generate two different HGVS strings, e.g., `ENST001:c.150G>A` and `ENST002:c.95G>A`. This satisfies the definition of ambiguity. To calculate the required statistic, it is sufficient to count the number of transcripts each variant maps to, without necessarily constructing the full HGVS string. A count greater than $1$ signifies ambiguity.\n\n**Step 4: Calculation of the Ambiguity Proportion**\n\nThe final objective is to compute the proportion, $p$, of ambiguously mapped variants. This is defined by the following formula:\n$$ p = \\frac{A}{M} $$\nwhere:\n- $M$ is the total count of unique variants that map to at least one transcript.\n- $A$ is the count of unique variants from the set $M$ that map to more than one transcript (i.e., have at least two mappings).\n\nIn the specific case where no variants map to any transcripts, $M=0$, the proportion $p$ is defined to be $0.0$.\n\nThe algorithmic procedure to compute $p$ is as follows:\n$1$. Initialize two counters: `mapped_variants_count` ($M$) to $0$ and `ambiguous_variants_count` ($A$) to $0$.\n$2$. For each unique variant in the harmonized set:\n    a. Count the number of transcripts it maps to, let this be `num_mappings`.\n    b. If `num_mappings` $\\ge 1$, increment `mapped_variants_count`.\n    c. If `num_mappings` $> 1$, increment `ambiguous_variants_count`.\n$3$. After processing all unique variants, if `mapped_variants_count` $> 0$, calculate the proportion $p = \\text{ambiguous\\_variants\\_count} / \\text{mapped\\_variants\\_count}$. Otherwise, $p = 0.0$.\n$4$. The final result is rounded to $3$ decimal places as required. This procedure is applied systematically to each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and compute the ambiguity proportion.\n    \"\"\"\n    \n    # Test Case 1 Data\n    transcripts_1 = [\n        (\"ENST000001\", \"chr1\", \"+\", [(100, 110), (200, 210)]),\n        (\"ENST000002\", \"chr1\", \"+\", [(205, 215)]),\n        (\"ENST000003\", \"chr1\", \"-\", [(500, 505), (510, 515)]),\n    ]\n    vcf_a_1 = [\n        (\"chr1\", 205, \"G\", \"A\"),\n        (\"chr1\", 102, \"A\", \"C\"),\n        (\"chr1\", 600, \"T\", \"C\"),\n    ]\n    vcf_b_1 = [\n        (\"chr1\", 205, \"G\", \"A\"),\n        (\"chr1\", 512, \"C\", \"T\"),\n        (\"chr1\", 106, \"T\", \"G\"),\n    ]\n\n    # Test Case 2 Data\n    transcripts_2 = [\n        (\"ENST000010\", \"chr2\", \"+\", [(1000, 1005), (1010, 1015)]),\n        (\"ENST000011\", \"chr2\", \"+\", [(1005, 1012)]),\n        (\"ENST000012\", \"chr2\", \"-\", [(2000, 2003), (2005, 2008)]),\n    ]\n    vcf_a_2 = [\n        (\"chr2\", 1005, \"A\", \"G\"),\n        (\"chr2\", 1012, \"C\", \"A\"),\n        (\"chr2\", 2006, \"G\", \"A\"),\n    ]\n    vcf_b_2 = [\n        (\"chr2\", 1005, \"A\", \"G\"),\n        (\"chr2\", 3000, \"T\", \"G\"),\n        (\"chr2\", 2002, \"T\", \"C\"),\n    ]\n\n    # Test Case 3 Data\n    transcripts_3 = [\n        (\"ENST000020\", \"chr3\", \"-\", [(400, 405)]),\n    ]\n    vcf_a_3 = [\n        (\"chr3\", 100, \"A\", \"C\"),\n        (\"chr3\", 200, \"G\", \"T\"),\n    ]\n    vcf_b_3 = [\n        (\"chr3\", 300, \"T\", \"A\"),\n    ]\n\n    test_cases = [\n        (transcripts_1, vcf_a_1, vcf_b_1),\n        (transcripts_2, vcf_a_2, vcf_b_2),\n        (transcripts_3, vcf_a_3, vcf_b_3),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        transcripts, vcf_a, vcf_b = case\n        \n        # Step 1: Harmonize variant records into a unique set\n        unique_variants = set(vcf_a) | set(vcf_b)\n        \n        mapped_variants_count = 0  # Counter for M\n        ambiguous_variants_count = 0  # Counter for A\n        \n        # Step 2: Iterate through unique variants to find mappings\n        for variant in unique_variants:\n            v_chrom, v_pos, _, _ = variant\n            \n            num_mappings = 0\n            for transcript in transcripts:\n                t_id, t_chrom, _, t_exons = transcript\n                \n                # Check for chromosome match first\n                if v_chrom != t_chrom:\n                    continue\n                \n                # Check if variant position is within any exon\n                for exon_start, exon_end in t_exons:\n                    if exon_start <= v_pos <= exon_end:\n                        num_mappings += 1\n                        # A variant can't be in two different exons of the same transcript\n                        # as exons are non-overlapping. So we can break.\n                        break\n            \n            # Step 3: Tally counts for M and A\n            if num_mappings >= 1:\n                mapped_variants_count += 1\n            if num_mappings > 1:\n                ambiguous_variants_count += 1\n        \n        # Step 4: Calculate the proportion p\n        if mapped_variants_count > 0:\n            proportion = ambiguous_variants_count / mapped_variants_count\n        else:\n            proportion = 0.0\n            \n        results.append(proportion)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.3f}' for r in results)}]\")\n\nsolve()\n```", "id": "4396025"}, {"introduction": "A core goal of systems biomedicine is to move beyond correlation to establish causal links between biological exposures and clinical outcomes. This practice introduces Mendelian Randomization (MR), a powerful method that uses genetic variants as instrumental variables to infer causality, and explores a key challenge known as pleiotropy [@problem_id:4396045]. You will implement the MR-Egger regression from first principles to understand how to test for and diagnose violations of the method's core assumptions, a critical skill for generating translatable hypotheses.", "problem": "You are given a summary-level Mendelian randomization problem grounded in instrumental variables for translational bioinformatics in systems biomedicine. Consider a set of independent genetic variants (instruments) indexed by $i \\in \\{1,\\dots,n\\}$. For each instrument $i$, denote by $\\hat{\\beta}_{XGi}$ the estimated association of instrument $i$ with the exposure, and by $\\hat{\\beta}_{YGi}$ the estimated association of instrument $i$ with the outcome. Assume known standard errors $\\sigma_{Yi}$ for $\\hat{\\beta}_{YGi}$, with inverse-variance weights $w_i = 1/\\sigma_{Yi}^2$. The structural summary-level model is\n$$\n\\mathbb{E}[\\hat{\\beta}_{YGi} \\mid \\hat{\\beta}_{XGi}] = \\beta_0 + \\beta_{\\text{MR}}\\hat{\\beta}_{XGi}\n$$\nwhere $\\beta_{\\text{MR}}$ is the causal effect parameter to be estimated and $\\beta_0$ is an intercept that captures the average direct (pleiotropic) effect across instruments. The Mendelian Randomization Egger (MR-Egger) approach uses weighted linear regression with an intercept. The intercept provides a test for directional pleiotropy under the Instrument Strength Independent of Direct Effect (InSIDE) assumption, which states that the instrument strengths for the exposure are independent of their direct effects on the outcome.\n\nStarting from the definitions of weighted least squares and linear regression, derive from first principles the weighted normal equations and use them to compute the MR-Egger estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_{\\text{MR}}$ along with their estimated covariance matrix, treating the weights as $w_i = 1/\\sigma_{Yi}^2$. Use the residual weighted sum of squares to estimate the residual variance and then compute a two-sided $t$-test p-value for the intercept $\\beta_0$ with degrees of freedom $n - 2$ to assess directional pleiotropy.\n\nYou must implement a program that:\n- Simulates summary-level data under a realistic linear data-generating mechanism for each test case as described below.\n- Computes the MR-Egger weighted least squares estimates, their standard errors, and the two-sided p-value for the intercept as a test for directional pleiotropy.\n- Produces a single line of output containing the concatenated results across all test cases as a flat list.\n\nData-generating mechanism for each instrument $i$:\n- Draw instrument strength for the exposure $ \\alpha_i \\sim \\mathcal{N}(0, \\sigma_\\alpha^2)$.\n- Draw direct (pleiotropic) effect $ \\delta_i $ such that $ \\delta_i $ has mean $\\mu$ and correlation $\\rho$ with $\\alpha_i$, implemented via\n$$\n\\delta_i = \\mu + \\rho \\frac{\\sigma_\\delta}{\\sigma_\\alpha} \\alpha_i + \\sqrt{1 - \\rho^2}\\,\\sigma_\\delta z_i,\\quad z_i \\sim \\mathcal{N}(0,1).\n$$\n- The true gene-outcome effect is $ \\beta \\alpha_i + \\delta_i $, where $\\beta$ is the true causal effect.\n- Observed summary statistics are\n$$\n\\hat{\\beta}_{XGi} = \\alpha_i + \\varepsilon_{Xi},\\quad \\varepsilon_{Xi} \\sim \\mathcal{N}(0, \\sigma_X^2),\n$$\n$$\n\\hat{\\beta}_{YGi} = \\beta \\alpha_i + \\delta_i + \\varepsilon_{Yi},\\quad \\varepsilon_{Yi} \\sim \\mathcal{N}(0, \\sigma_Y^2),\n$$\nwith $\\sigma_{Yi} = \\sigma_Y$ for all $i$.\n\nYour program must:\n- Use a fixed random seed per test case to ensure reproducibility.\n- For each case, perform a weighted regression of $\\hat{\\beta}_{YGi}$ on $\\hat{\\beta}_{XGi}$ with an intercept using weights $w_i = 1/\\sigma_{Yi}^2$.\n- Return for each test case: the MR-Egger slope estimate $\\hat{\\beta}_{\\text{MR}}$, the intercept estimate $\\hat{\\beta}_0$, and the two-sided p-value for the intercept. Round each value to $4$ decimal places.\n- Aggregate all results into a single line that is a comma-separated list enclosed in square brackets, in the order $[\\hat{\\beta}_{\\text{MR}}^{(1)}, \\hat{\\beta}_0^{(1)}, p_0^{(1)}, \\hat{\\beta}_{\\text{MR}}^{(2)}, \\hat{\\beta}_0^{(2)}, p_0^{(2)}, \\hat{\\beta}_{\\text{MR}}^{(3)}, \\hat{\\beta}_0^{(3)}, p_0^{(3)}]$, where the superscript denotes the test case index.\n\nTest suite (each case is a tuple of parameters $(\\text{seed}, n, \\beta, \\mu, \\rho, \\sigma_\\alpha, \\sigma_delta, \\sigma_X, \\sigma_Y)$):\n- Case $1$ (happy path, no directional pleiotropy under InSIDE): $(\\;12345,\\; 50,\\; 0.2,\\; 0.0,\\; 0.0,\\; 0.1,\\; 0.05,\\; 0.005,\\; 0.05\\;)$.\n- Case $2$ (directional pleiotropy under InSIDE): $(\\;67890,\\; 50,\\; 0.2,\\; 0.1,\\; 0.0,\\; 0.1,\\; 0.05,\\; 0.005,\\; 0.05\\;)$.\n- Case $3$ (InSIDE violation with no directional mean effect): $(\\;54321,\\; 50,\\; 0.2,\\; 0.0,\\; 0.9,\\; 0.1,\\; 0.05,\\; 0.005,\\; 0.05\\;)$.\n\nNo physical units are required. Express all outputs as real numbers rounded to $4$ decimal places with no additional text.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$). The exact required output for this problem is $[\\hat{\\beta}_{\\text{MR}}^{(1)}, \\hat{\\beta}_0^{(1)}, p_0^{(1)}, \\hat{\\beta}_{\\text{MR}}^{(2)}, \\hat{\\beta}_0^{(2)}, p_0^{(2)}, \\hat{\\beta}_{\\text{MR}}^{(3)}, \\hat{\\beta}_0^{(3)}, p_0^{(3)}]$ with all values rounded to $4$ decimal places.", "solution": "The problem requires the derivation and implementation of the Mendelian Randomization (MR) Egger regression estimators from first principles of weighted least squares (WLS), and the computation of a p-value for the intercept to test for directional pleiotropy.\n\n### **1. Problem Formulation**\n\nLet the instrumental variable summary statistics for $n$ independent genetic variants be denoted by $(\\hat{\\beta}_{XGi}, \\hat{\\beta}_{YGi})$ for $i=1, \\dots, n$. The MR-Egger model is a linear regression of the gene-outcome associations $\\hat{\\beta}_{YGi}$ on the gene-exposure associations $\\hat{\\beta}_{XGi}$. To simplify notation, let $Y_i = \\hat{\\beta}_{YGi}$ and $X_i = \\hat{\\beta}_{XGi}$. The model is:\n$$\nY_i = \\beta_0 + \\beta_{\\text{MR}} X_i + \\epsilon_i\n$$\nwhere $\\beta_0$ is the intercept, representing the average pleiotropic effect, and $\\beta_{\\text{MR}}$ is the slope, representing the causal effect estimate. The regression is weighted by $w_i = 1/\\sigma_{Yi}^2$, where $\\sigma_{Yi}^2$ is the variance of the estimate $\\hat{\\beta}_{YGi}$.\n\n### **2. Derivation of Weighted Least Squares Estimators**\n\nThe WLS method finds the parameters $(\\beta_0, \\beta_{\\text{MR}})$ that minimize the weighted sum of squared residuals (WSSR):\n$$\nS(\\beta_0, \\beta_{\\text{MR}}) = \\sum_{i=1}^n w_i (Y_i - \\beta_0 - \\beta_{\\text{MR}} X_i)^2\n$$\nTo find the minimum, we compute the partial derivatives of $S$ with respect to $\\beta_0$ and $\\beta_{\\text{MR}}$ and set them to zero. These are the weighted normal equations.\n\n**Derivative with respect to $\\beta_0$:**\n$$\n\\frac{\\partial S}{\\partial \\beta_0} = \\sum_{i=1}^n 2 w_i (Y_i - \\beta_0 - \\beta_{\\text{MR}} X_i)(-1) = 0\n$$\n$$\n\\implies \\sum_{i=1}^n w_i (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_{\\text{MR}} X_i) = 0\n$$\n$$\n\\implies \\hat{\\beta}_0 \\sum_{i=1}^n w_i + \\hat{\\beta}_{\\text{MR}} \\sum_{i=1}^n w_i X_i = \\sum_{i=1}^n w_i Y_i \\quad (1)\n$$\n\n**Derivative with respect to $\\beta_{\\text{MR}}$:**\n$$\n\\frac{\\partial S}{\\partial \\beta_{\\text{MR}}} = \\sum_{i=1}^n 2 w_i (Y_i - \\beta_0 - \\beta_{\\text{MR}} X_i)(-X_i) = 0\n$$\n$$\n\\implies \\sum_{i=1}^n w_i X_i (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_{\\text{MR}} X_i) = 0\n$$\n$$\n\\implies \\hat{\\beta}_0 \\sum_{i=1}^n w_i X_i + \\hat{\\beta}_{\\text{MR}} \\sum_{i=1}^n w_i X_i^2 = \\sum_{i=1}^n w_i X_i Y_i \\quad (2)\n$$\n\nThese two linear equations can be solved for the estimators $\\hat{\\beta}_0$ and $\\hat{\\beta}_{\\text{MR}}$. A more general and computationally convenient approach is to use matrix algebra.\n\n### **3. Matrix Formulation**\n\nLet $\\mathbf{Y}$ be the $n \\times 1$ vector of outcomes $[Y_1, \\dots, Y_n]^T$, $\\mathbf{W}$ be the $n \\times n$ diagonal matrix of weights with $W_{ii} = w_i$, and $\\mathbf{X}$ be the $n \\times 2$ design matrix with the first column being all ones and the second column containing the predictor values $[X_1, \\dots, X_n]^T$.\n$$\n\\mathbf{Y} = \\begin{pmatrix} Y_1 \\\\ \\vdots \\\\ Y_n \\end{pmatrix}, \\quad\n\\mathbf{X} = \\begin{pmatrix} 1 & X_1 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{pmatrix}, \\quad\n\\mathbf{W} = \\begin{pmatrix} w_1 & & 0 \\\\ & \\ddots & \\\\ 0 & & w_n \\end{pmatrix}\n$$\nThe parameter vector is $\\boldsymbol{\\beta} = [\\beta_0, \\beta_{\\text{MR}}]^T$. The normal equations in matrix form are:\n$$\n(\\mathbf{X}^T \\mathbf{W} \\mathbf{X}) \\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T \\mathbf{W} \\mathbf{Y}\n$$\nThe solution for the estimated parameter vector $\\hat{\\boldsymbol{\\beta}}$ is:\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{Y}\n$$\nwhere $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_{\\text{MR}}]^T$.\n\n### **4. Estimation of Standard Errors and Hypothesis Testing**\n\nTo perform a hypothesis test for the intercept $\\beta_0$, we need its standard error. This requires an estimate of the model's residual variance and the covariance matrix of the estimators.\n\n**Residual Variance:**\nThe residual for the $i$-th observation is $e_i = Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_{\\text{MR}} X_i)$. The residual weighted sum of squares (WSSR) is:\n$$\n\\text{WSSR} = \\sum_{i=1}^n w_i e_i^2\n$$\nAn unbiased estimator for the residual variance, $\\hat{\\sigma}^2$, is the WSSR divided by the degrees of freedom, which is $n-p$ where $n$ is the number of observations and $p$ is the number of estimated parameters. Here, $p=2$ (for $\\beta_0$ and $\\beta_{\\text{MR}}$).\n$$\n\\hat{\\sigma}^2 = \\frac{\\text{WSSR}}{n-2}\n$$\n\n**Covariance Matrix of Estimators:**\nThe estimated covariance matrix of the parameter vector $\\hat{\\boldsymbol{\\beta}}$ is given by:\n$$\n\\widehat{\\text{Cov}}(\\hat{\\boldsymbol{\\beta}}) = \\hat{\\sigma}^2 (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1}\n$$\nThis is a $2 \\times 2$ matrix:\n$$\n\\widehat{\\text{Cov}}(\\hat{\\boldsymbol{\\beta}}) = \\begin{pmatrix} \\widehat{\\text{Var}}(\\hat{\\beta}_0) & \\widehat{\\text{Cov}}(\\hat{\\beta}_0, \\hat{\\beta}_{\\text{MR}}) \\\\ \\widehat{\\text{Cov}}(\\hat{\\beta}_0, \\hat{\\beta}_{\\text{MR}}) & \\widehat{\\text{Var}}(\\hat{\\beta}_{\\text{MR}}) \\end{pmatrix}\n$$\nThe standard error of the intercept, $SE(\\hat{\\beta}_0)$, is the square root of its estimated variance:\n$$\nSE(\\hat{\\beta}_0) = \\sqrt{\\widehat{\\text{Var}}(\\hat{\\beta}_0)} = \\sqrt{[\\widehat{\\text{Cov}}(\\hat{\\boldsymbol{\\beta}})]_{1,1}}\n$$\n\n**t-test for the Intercept:**\nTo test for directional pleiotropy, we test the null hypothesis $H_0: \\beta_0 = 0$. The test statistic is a $t$-statistic:\n$$\nt_{\\text{stat}} = \\frac{\\hat{\\beta}_0 - 0}{SE(\\hat{\\beta}_0)}\n$$\nUnder the null hypothesis, this statistic follows a $t$-distribution with $n-2$ degrees of freedom. The two-sided p-value is calculated as:\n$$\np_0 = 2 \\cdot P(T_{n-2} \\ge |t_{\\text{stat}}|)\n$$\nwhere $T_{n-2}$ is a random variable following the $t$-distribution with $n-2$ degrees of freedom.\n\n### **5. Implementation Strategy**\n\nThe program will implement the following steps for each test case:\n1.  **Data Simulation:** Using the provided parameters ($\\text{seed}, n, \\beta, \\mu, \\rho, \\dots$), generate the summary statistics $\\hat{\\beta}_{XGi}$ and $\\hat{\\beta}_{YGi}$ for $n$ instruments according to the specified data-generating mechanism.\n2.  **WLS Regression:**\n    *   Construct the response vector $\\mathbf{Y}$ from $\\hat{\\beta}_{YGi}$ and the design matrix $\\mathbf{X}$ from a column of ones and the vector of $\\hat{\\beta}_{XGi}$.\n    *   Construct the diagonal weight matrix $\\mathbf{W}$ using $w_i = 1/\\sigma_Y^2$.\n    *   Compute the parameter estimates $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_{\\text{MR}}]^T$ using the matrix formula $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{Y}$.\n3.  **P-value Calculation:**\n    *   Calculate the predicted values $\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$ and the residuals $\\mathbf{e} = \\mathbf{Y} - \\hat{\\mathbf{Y}}$.\n    *   Compute the residual variance estimator $\\hat{\\sigma}^2$.\n    *   Calculate the parameter covariance matrix $\\widehat{\\text{Cov}}(\\hat{\\boldsymbol{\\beta}})$.\n    *   Extract the standard error for the intercept, $SE(\\hat{\\beta}_0)$.\n    *   Compute the $t$-statistic and the corresponding two-sided p-value using the $t$-distribution with $n-2$ degrees of freedom.\n4.  **Output Formatting:** Round the resulting $\\hat{\\beta}_{\\text{MR}}$, $\\hat{\\beta}_0$, and $p_0$ to $4$ decimal places and aggregate them into a single comma-separated list as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to run the MR-Egger simulation and analysis for all test cases.\n    \"\"\"\n    # Test suite (each case is a tuple of parameters \n    # (seed, n, beta, mu, rho, sigma_alpha, sigma_delta, sigma_X, sigma_Y))\n    test_cases = [\n        # Case 1 (happy path, no directional pleiotropy under InSIDE)\n        (12345, 50, 0.2, 0.0, 0.0, 0.1, 0.05, 0.005, 0.05),\n        # Case 2 (directional pleiotropy under InSIDE)\n        (67890, 50, 0.2, 0.1, 0.0, 0.1, 0.05, 0.005, 0.05),\n        # Case 3 (InSIDE violation with no directional mean effect)\n        (54321, 50, 0.2, 0.0, 0.9, 0.1, 0.05, 0.005, 0.05),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = run_mr_egger_analysis(*case)\n        all_results.extend(result)\n    \n    # Format the final list as a string \"[val1,val2,...]\" with 4 decimal places.\n    formatted_results = [f'{val:.4f}' for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef run_mr_egger_analysis(seed, n, beta, mu, rho, sigma_alpha, sigma_delta, sigma_X, sigma_Y):\n    \"\"\"\n    Simulates summary-level data and computes MR-Egger estimates and statistics for one case.\n    \n    Args:\n        seed (int): Random seed for reproducibility.\n        n (int): Number of instruments.\n        beta (float): True causal effect.\n        mu (float): Mean of direct (pleiotropic) effects.\n        rho (float): Correlation between instrument strength and direct effects.\n        sigma_alpha (float): Standard deviation of instrument strength.\n        sigma_delta (float): Standard deviation of direct effects.\n        sigma_X (float): Standard deviation of measurement error for exposure associations.\n        sigma_Y (float): Standard deviation of measurement error for outcome associations.\n\n    Returns:\n        tuple[float, float, float]: A tuple containing the estimated slope (beta_MR_hat),\n                                     intercept (beta0_hat), and the p-value for the intercept.\n    \"\"\"\n    # 1. Data Simulation\n    rng = np.random.default_rng(seed)\n    \n    # Instrument strength for exposure\n    alpha = rng.normal(0, sigma_alpha, n)\n    \n    # Direct (pleiotropic) effect\n    z = rng.normal(0, 1, n)\n    delta = mu + rho * (sigma_delta / sigma_alpha) * alpha + np.sqrt(1 - rho**2) * sigma_delta * z\n    \n    # Measurement errors\n    eps_X = rng.normal(0, sigma_X, n)\n    eps_Y = rng.normal(0, sigma_Y, n)\n    \n    # Observed summary statistics\n    beta_X_hat = alpha + eps_X\n    beta_Y_hat = beta * alpha + delta + eps_Y\n\n    # 2. MR-Egger Weighted Least Squares Regression\n    # Let Y_vec = beta_Y_hat, X_vec = beta_X_hat\n    Y_vec = beta_Y_hat\n    \n    # Construct the design matrix X_mat (n x 2)\n    X_mat = np.vstack((np.ones(n), beta_X_hat)).T\n    \n    # Construct the diagonal weight matrix W (n x n)\n    # Weights are constant since sigma_Yi is constant\n    weights = np.full(n, 1.0 / (sigma_Y**2))\n    W = np.diag(weights)\n\n    # Compute (X^T * W * X)\n    XTW = X_mat.T @ W\n    XTWX = XTW @ X_mat\n    \n    # Compute inverse of (X^T * W * X)\n    try:\n        XTWX_inv = np.linalg.inv(XTWX)\n    except np.linalg.LinAlgError:\n        # Handle cases of singular matrix (collinear predictors)\n        return (np.nan, np.nan, np.nan)\n\n    # Compute (X^T * W * Y)\n    XTWY = XTW @ Y_vec\n\n    # Compute parameter estimates: beta_hat = (X^T*W*X)^-1 * (X^T*W*Y)\n    beta_hats = XTWX_inv @ XTWY\n    beta0_hat = beta_hats[0]\n    beta_MR_hat = beta_hats[1]\n    \n    # 3. Standard Error and P-value Calculation\n    # Predicted values and residuals\n    Y_hat = X_mat @ beta_hats\n    residuals = Y_vec - Y_hat\n    \n    # Residual weighted sum of squares (WSSR)\n    wssr = np.sum(weights * (residuals**2))\n    \n    # Degrees of freedom\n    df = n - 2\n    if df <= 0:\n        return (beta_MR_hat, beta0_hat, np.nan)\n\n    # Residual variance estimate\n    residual_variance = wssr / df\n    \n    # Covariance matrix of parameter estimates\n    cov_beta_hats = residual_variance * XTWX_inv\n    \n    # Standard error of the intercept\n    var_beta0_hat = cov_beta_hats[0, 0]\n    se_beta0_hat = np.sqrt(var_beta0_hat)\n    \n    # t-statistic for the intercept (H0: beta0 = 0)\n    if se_beta0_hat > 0:\n        t_stat = beta0_hat / se_beta0_hat\n        # Two-sided p-value\n        p_value_intercept = 2 * t.sf(np.abs(t_stat), df=df)\n    else:\n        p_value_intercept = np.nan\n\n    return (beta_MR_hat, beta0_hat, p_value_intercept)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "4396045"}, {"introduction": "As complex predictive models become central to personalized medicine, the ability to explain their decisions is no longer a luxury but a necessity for clinical translation. This hands-on practice explores SHAP (SHapley Additive exPlanations), a leading method for model interpretation grounded in cooperative game theory [@problem_id:4396100]. By implementing the calculation of these values from their axiomatic roots, you will gain a fundamental understanding of how to attribute a model's prediction to individual patient features, fostering transparency and trust.", "problem": "You are given a predictive model for individualized treatment response within the translational bioinformatics for systems biomedicine context. The model takes a vector of patient features and outputs a scalar response score. Your task is to compute SHapley Additive exPlanations (SHAP) values for feature attribution and to verify their axiomatic properties: additivity (also called local accuracy or efficiency) and consistency. Work in purely mathematical terms and implement an exact solution by enumerating feature coalitions.\n\nDefinitions and setup: Let there be $M$ features indexed by the set $N = \\{1,2,\\dots,M\\}$. A trained model is a function $f: \\mathbb{R}^M \\to \\mathbb{R}$, and for a target patient with feature vector $x \\in \\mathbb{R}^M$, the cooperative game set function is defined for each coalition $S \\subseteq N$ by\n$$\nv(S) = \\mathbb{E}_{Z}\\big[f\\big(x_S \\oplus Z_{\\overline{S}}\\big)\\big],\n$$\nwhere $\\overline{S} = N \\setminus S$, $Z$ is a background dataset representing the reference population, $x_S \\oplus Z_{\\overline{S}}$ is the hybrid input formed by fixing the features in $S$ to $x$ and imputing the remaining features from a random sample $Z$ drawn from the background dataset, and the expectation $\\mathbb{E}_Z[\\cdot]$ is taken empirically over the provided background dataset. Note that $v(\\emptyset) = \\mathbb{E}_{Z}[f(Z)]$ and $v(N) = f(x)$.\n\nAxioms to satisfy:\n- Additivity (efficiency, local accuracy): For a given $x$, the baseline contribution $v(\\emptyset)$ plus the sum of per-feature attributions equals the prediction $v(N)$.\n- Consistency: If, holding all else fixed, the marginal contribution of feature $i$ to any coalition $S \\subseteq N \\setminus \\{i\\}$ increases from one model to another, then the attribution to feature $i$ should not decrease.\n\nYou must implement an exact attribution that is uniquely determined by the cooperative game axioms above, using enumeration over all coalitions $S \\subseteq N$ for each feature $i$. You must derive the algorithm from first principles and must not use any shortcut formulas in the problem statement.\n\nModel class: For concreteness and scientific realism in translational bioinformatics applications, consider linear response models\n$$\nf(x) = b + \\sum_{j=1}^{M} w_j x_j,\n$$\nwhere $b \\in \\mathbb{R}$ is a bias term and $w_j \\in \\mathbb{R}$ are learned coefficients.\n\nComputation protocol:\n1. Compute $v(S)$ for all $S \\subseteq N$ as the empirical mean over the background dataset.\n2. Compute the per-feature attributions for the target $x$ by exact coalition enumeration consistent with the axioms above.\n3. Verify additivity by checking $v(\\emptyset) + \\sum_{i=1}^{M} \\phi_i \\approx v(N)$ to numerical tolerance $10^{-12}$.\n4. Verify consistency by constructing two models where the marginal contributions of a chosen feature $i$ increase for all coalitions, and show the corresponding attribution $\\phi_i$ does not decrease.\n\nTest suite: Implement the computation for the following parameter sets. All responses are unitless real-valued scores.\n\n- Test 1 (happy path, $M=3$):\n  - Weights $w = [0.8, -0.5, 1.2]$, bias $b = 0.3$.\n  - Background dataset $Z$:\n    $$\n    \\begin{bmatrix}\n    0.9 & 0.2 & 0.1 \\\\\n    1.1 & 0.3 & 0.4 \\\\\n    0.8 & 0.5 & 0.6 \\\\\n    1.0 & 0.4 & 0.3 \\\\\n    0.7 & 0.2 & 0.5\n    \\end{bmatrix}\n    $$\n  - Target $x = [1.0, 0.4, 0.7]$.\n  - Output: the list of attributions $[\\phi_1,\\phi_2,\\phi_3]$ and a boolean indicating additivity holds within tolerance $10^{-12}$.\n\n- Test 2 (boundary: a zero-weight feature, $M=3$):\n  - Weights $w = [0.9, 0.0, 0.7]$, bias $b = -0.1$.\n  - Background dataset $Z$:\n    $$\n    \\begin{bmatrix}\n    0.1 & 2.0 & 0.2 \\\\\n    0.2 & 2.5 & 0.4 \\\\\n    0.0 & 1.5 & 0.3 \\\\\n    0.3 & 3.0 & 0.1\n    \\end{bmatrix}\n    $$\n  - Target $x = [0.2, 5.0, 0.8]$.\n  - Output: the list of attributions $[\\phi_1,\\phi_2,\\phi_3]$, a boolean indicating the zero-weight feature’s attribution is numerically zero within $10^{-12}$, and a boolean indicating additivity holds within tolerance $10^{-12}$.\n\n- Test 3 (consistency across models, $M=3$):\n  - Background dataset $Z$:\n    $$\n    \\begin{bmatrix}\n    0.5 & 0.4 & 0.3 \\\\\n    0.6 & 0.3 & 0.2 \\\\\n    0.4 & 0.5 & 0.1\n    \\end{bmatrix}\n    $$\n  - Target $x = [0.9, 0.1, 0.5]$.\n  - Model $f$: weights $w^{(f)} = [0.5, 0.4, 0.1]$, bias $b^{(f)} = 0.0$.\n  - Model $g$: weights $w^{(g)} = [0.9, 0.4, 0.1]$, bias $b^{(g)} = 0.0$ (only the first weight increases).\n  - Output: the attribution lists for both models $[\\phi^{(f)}_1,\\phi^{(f)}_2,\\phi^{(f)}_3]$ and $[\\phi^{(g)}_1,\\phi^{(g)}_2,\\phi^{(g)}_3]$, a boolean indicating consistency for feature $1$ (i.e., $\\phi^{(g)}_1 \\ge \\phi^{(f)}_1$), and booleans indicating additivity holds for each model within tolerance $10^{-12}$.\n\n- Test 4 (edge case: single feature, $M=1$):\n  - Weights $w = [1.5]$, bias $b = 0.2$.\n  - Background dataset $Z$:\n    $$\n    \\begin{bmatrix}\n    1.0 \\\\\n    0.8 \\\\\n    1.2 \\\\\n    0.9\n    \\end{bmatrix}\n    $$\n  - Target $x = [2.0]$.\n  - Output: the attribution list $[\\phi_1]$ and a boolean indicating additivity holds within tolerance $10^{-12}$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The items must appear in this exact order:\n- Test 1: attribution list, additivity boolean.\n- Test 2: attribution list, zero-weight boolean, additivity boolean.\n- Test 3: attribution list for model $f$, attribution list for model $g$, consistency boolean for feature $1$, additivity boolean for model $f$, additivity boolean for model $g$.\n- Test 4: attribution list, additivity boolean.\nFor example, the output should look like\n`[[\\phi_1,\\phi_2,\\phi_3], bool1, [\\phi_1,\\phi_2,\\phi_3], bool2, bool3, [\\phi^(f)_1,\\phi^(f)_2,\\phi^(f)_3], [\\phi^(g)_1,\\phi^(g)_2,\\phi^(g)_3], bool4, bool5, bool6, [\\phi_1], bool7].`\nAll booleans must be either $True$ or $False$, and all numbers must be real-valued floats. No extraneous text should be printed.", "solution": "The problem is valid. It is scientifically grounded in the established theory of cooperative games and its application to machine learning interpretability (SHAP values), is well-posed with all necessary information provided, and is formulated objectively. We will proceed with a solution.\n\nThe problem requires the computation of SHapley Additive exPlanations (SHAP) values for a linear model by enumerating all possible feature coalitions. This method provides a unique feature attribution that satisfies several desirable axioms, including additivity and consistency, as specified in the problem statement.\n\n### 1. Theoretical Foundation: The Shapley Value\n\nThe core task is to attribute the prediction of a model $f(x)$ relative to a baseline prediction, to its individual features. The problem defines a cooperative game where the \"players\" are the features $i \\in N = \\{1, 2, \\dots, M\\}$. The value of a \"coalition\" (a subset of features $S \\subseteq N$) is given by the function:\n$$\nv(S) = \\mathbb{E}_{Z}\\big[f\\big(x_S \\oplus Z_{\\overline{S}}\\big)\\big]\n$$\nHere, $x_S \\oplus Z_{\\overline{S}}$ denotes a hybrid input vector constructed by taking the values of the target instance $x$ for features in coalition $S$, and imputing the values for the remaining features ($\\overline{S} = N \\setminus S$) from a background dataset $Z$. The expectation $\\mathbb{E}_Z$ is calculated empirically as the mean prediction over all samples in $Z$. The baseline prediction is $v(\\emptyset) = \\mathbb{E}_{Z}[f(Z)]$, and the full model prediction for the specific instance $x$ is $v(N)=f(x)$.\n\nThe unique attribution scheme satisfying the axioms of additivity (efficiency), symmetry, and linearity (or consistency, which along with symmetry and efficiency also yields uniqueness) is the Shapley value, a concept from cooperative game theory. The Shapley value $\\phi_i$ of a feature $i$ is its average marginal contribution to the game value across all possible permutations of feature orderings.\n\nConsider a permutation $\\pi$ of the features in $N$. Let $P_i(\\pi)$ be the set of features that precede feature $i$ in this ordering. The marginal contribution of feature $i$ for this specific ordering is the change in the value function when $i$ is added to its predecessors:\n$$\n\\Delta v(i | \\pi) = v(P_i(\\pi) \\cup \\{i\\}) - v(P_i(\\pi))\n$$\nTo ensure fairness, we average this marginal contribution over all $M!$ possible permutations of the features:\n$$\n\\phi_i = \\frac{1}{M!} \\sum_{\\pi} \\Delta v(i | \\pi)\n$$\nThis formula is computationally challenging. A more practical, equivalent formula is derived by grouping permutations based on the predecessor set $S = P_i(\\pi)$. For a given coalition $S \\subseteq N \\setminus \\{i\\}$ of size $|S|=s$, there are $s!$ ways to order the features within $S$ and $(M-s-1)!$ ways to order the features not in $S \\cup \\{i\\}$. Thus, there are $s!(M-s-1)!$ permutations for which feature $i$ is added to the coalition $S$. The Shapley value can then be expressed as a sum over all coalitions that feature $i$ could join:\n$$\n\\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(M-|S|-1)!}{M!} \\left( v(S \\cup \\{i\\}) - v(S) \\right)\n$$\nThis is the formula we will implement. It satisfies the additivity axiom: $v(\\emptyset) + \\sum_{i=1}^{M} \\phi_i = v(N)$.\n\n### 2. Algorithmic Implementation\n\nFollowing the derived formula, the algorithm proceeds in two main stages:\n\n**Stage 1: Compute the Value Function $v(S)$ for all Coalitions**\nWe must first compute $v(S)$ for all $2^M$ possible subsets $S \\subseteq N$.\n1.  Generate all subsets $S$ of the feature index set $\\{0, 1, \\dots, M-1\\}$.\n2.  For each subset $S$:\n    a. To compute $v(S)$, we iterate through each sample $z^{(k)}$ in the background dataset $Z$.\n    b. For each $z^{(k)}$, we construct a hybrid input vector $x'_{k, S}$. For each feature index $j \\in \\{0, \\dots, M-1\\}$, the value $(x'_{k, S})_j$ is set to $x_j$ if $j \\in S$, and to $z^{(k)}_j$ if $j \\notin S$.\n    c. We compute the model's prediction for this hybrid vector: $f(x'_{k, S}) = b + \\sum_{j=0}^{M-1} w_j (x'_{k, S})_j$.\n    d. $v(S)$ is the average of these predictions over all $k$.\n3.  Store the computed $v(S)$ values, for instance in a dictionary mapping coalitions to their values.\n\n**Stage 2: Compute Feature Attributions $\\phi_i$**\nWith all $v(S)$ values pre-computed, we can now calculate the attributions.\n1. For each feature $i \\in \\{0, \\dots, M-1\\}$:\n    a. Initialize its attribution $\\phi_i$ to $0$.\n    b. Iterate through all subsets $S$ of $N \\setminus \\{i\\}$.\n    c. For each such $S$ of size $s=|S|$, calculate the combinatorial weight $W = \\frac{s!(M-s-1)!}{M!}$.\n    d. Calculate the marginal contribution of feature $i$ to coalition $S$: $\\Delta v = v(S \\cup \\{i\\}) - v(S)$.\n    e. Add the weighted contribution to the total attribution: $\\phi_i \\leftarrow \\phi_i + W \\cdot \\Delta v$.\n\n### 3. Axiomatic Verification\n- **Additivity:** We verify this property by computing $v(\\emptyset) + \\sum_{i=0}^{M-1} \\phi_i$ and checking if the result is equal to $v(N)$ within a numerical tolerance of $10^{-12}$.\n- **Consistency:** For Test 3, we have two models, $f$ and $g$, where $w_1^{(g)} > w_1^{(f)}$ and other weights are equal. For a linear model, the marginal contribution of a feature $i$ is $v(S \\cup \\{i\\}) - v(S) = w_i(x_i - \\mathbb{E}[Z_i])$, which is independent of $S$. Since $x_1 > \\mathbb{E}[Z_1]$ in this test case, the marginal contribution of feature 1 is strictly greater for model $g$ for all coalitions. The consistency axiom states that $\\phi_1$ should not decrease, i.e., $\\phi_1^{(g)} \\ge \\phi_1^{(f)}$. We compute the attributions for both models and verify this inequality.\n\nThis enumerative approach is exact and directly follows the definition of Shapley values, as required.", "answer": "```python\nimport numpy as np\nfrom itertools import combinations\nfrom scipy.special import factorial\n\ndef get_attributions_and_value_function(w, b, background_z, target_x):\n    \"\"\"\n    Computes SHAP values by exactly enumerating all feature coalitions,\n    as derived from the axiomatic definition of Shapley values.\n\n    Args:\n        w (np.ndarray): Model weights.\n        b (float): Model bias.\n        background_z (np.ndarray): Background dataset for expectation calculation.\n        target_x (np.ndarray): The specific instance for which to explain the prediction.\n\n    Returns:\n        tuple: A tuple containing:\n            - list[float]: The list of SHAP values (attributions) for each feature.\n            - dict[frozenset, float]: The computed value function v(S) for all coalitions S.\n    \"\"\"\n    num_features = len(w)\n    \n    # Use frozensets for hashable dictionary keys representing coalitions.\n    value_function = {}\n\n    # Stage 1: Compute the value function v(S) for all 2^M coalitions S.\n    all_feature_indices = set(range(num_features))\n    for s_size in range(num_features + 1):\n        for s_indices_tuple in combinations(all_feature_indices, s_size):\n            coalition_s = frozenset(s_indices_tuple)\n            \n            # Construct hybrid inputs x_S + Z_{\\bar{S}} by iterating through background samples.\n            hybrid_predictions = []\n            for z_sample in background_z:\n                hybrid_instance = np.copy(z_sample)\n                for feature_idx in coalition_s:\n                    hybrid_instance[feature_idx] = target_x[feature_idx]\n                \n                # Calculate model prediction for the hybrid instance.\n                prediction = b + np.dot(w, hybrid_instance)\n                hybrid_predictions.append(prediction)\n            \n            # v(S) is the empirical expectation (mean) of these predictions.\n            value_function[coalition_s] = np.mean(hybrid_predictions)\n\n    # Stage 2: Compute Shapley values phi_i using the pre-computed value function.\n    attributions = [0.0] * num_features\n    fact_M = factorial(num_features)\n\n    for i in range(num_features):\n        # Iterate over all coalitions S that do NOT contain feature i.\n        feature_set_without_i = all_feature_indices - {i}\n        for s_size in range(num_features):\n            for s_indices_tuple in combinations(feature_set_without_i, s_size):\n                coalition_s = frozenset(s_indices_tuple)\n                \n                # Shapley weight for a coalition of size |S|.\n                weight = (factorial(s_size) * factorial(num_features - s_size - 1)) / fact_M\n                \n                # Marginal contribution of feature i to coalition S.\n                coalition_s_with_i = frozenset(s_indices_tuple + (i,))\n                marginal_contribution = value_function[coalition_s_with_i] - value_function[coalition_s]\n                \n                attributions[i] += weight * marginal_contribution\n                \n    return attributions, value_function\n\ndef solve():\n    \"\"\"\n    Solves the problem by running all test cases and formatting the output.\n    \"\"\"\n    results = []\n    TOLERANCE = 1e-12\n\n    # --- Test 1 ---\n    w1 = np.array([0.8, -0.5, 1.2])\n    b1 = 0.3\n    z1 = np.array([\n        [0.9, 0.2, 0.1], [1.1, 0.3, 0.4], [0.8, 0.5, 0.6],\n        [1.0, 0.4, 0.3], [0.7, 0.2, 0.5]\n    ])\n    x1 = np.array([1.0, 0.4, 0.7])\n    \n    phi1, v1 = get_attributions_and_value_function(w1, b1, z1, x1)\n    \n    empty_set_key = frozenset()\n    full_set_key = frozenset(range(len(w1)))\n    v1_empty = v1[empty_set_key]\n    v1_full = v1[full_set_key]\n    additivity_check1 = abs((v1_empty + sum(phi1)) - v1_full) < TOLERANCE\n    results.extend([phi1, additivity_check1])\n\n    # --- Test 2 ---\n    w2 = np.array([0.9, 0.0, 0.7])\n    b2 = -0.1\n    z2 = np.array([\n        [0.1, 2.0, 0.2], [0.2, 2.5, 0.4],\n        [0.0, 1.5, 0.3], [0.3, 3.0, 0.1]\n    ])\n    x2 = np.array([0.2, 5.0, 0.8])\n    \n    phi2, v2 = get_attributions_and_value_function(w2, b2, z2, x2)\n    \n    # Feature 2 (index 1) has zero weight.\n    zeroweight_check2 = abs(phi2[1]) < TOLERANCE\n    \n    empty_set_key_2 = frozenset()\n    full_set_key_2 = frozenset(range(len(w2)))\n    v2_empty = v2[empty_set_key_2]\n    v2_full = v2[full_set_key_2]\n    additivity_check2 = abs((v2_empty + sum(phi2)) - v2_full) < TOLERANCE\n    results.extend([phi2, zeroweight_check2, additivity_check2])\n\n    # --- Test 3 ---\n    z3 = np.array([\n        [0.5, 0.4, 0.3], [0.6, 0.3, 0.2], [0.4, 0.5, 0.1]\n    ])\n    x3 = np.array([0.9, 0.1, 0.5])\n    \n    # Model f\n    w3f = np.array([0.5, 0.4, 0.1])\n    b3f = 0.0\n    phi3f, v3f = get_attributions_and_value_function(w3f, b3f, z3, x3)\n    \n    # Model g\n    w3g = np.array([0.9, 0.4, 0.1])\n    b3g = 0.0\n    phi3g, v3g = get_attributions_and_value_function(w3g, b3g, z3, x3)\n    \n    # Consistency for feature 1 (index 0)\n    consistency_check3 = phi3g[0] >= phi3f[0]\n    \n    # Additivity for model f\n    v3f_empty = v3f[frozenset()]\n    v3f_full = v3f[frozenset(range(len(w3f)))]\n    additivity_check3f = abs((v3f_empty + sum(phi3f)) - v3f_full) < TOLERANCE\n    \n    # Additivity for model g\n    v3g_empty = v3g[frozenset()]\n    v3g_full = v3g[frozenset(range(len(w3g)))]\n    additivity_check3g = abs((v3g_empty + sum(phi3g)) - v3g_full) < TOLERANCE\n    \n    results.extend([phi3f, phi3g, consistency_check3, additivity_check3f, additivity_check3g])\n\n    # --- Test 4 ---\n    w4 = np.array([1.5])\n    b4 = 0.2\n    z4 = np.array([[1.0], [0.8], [1.2], [0.9]])\n    x4 = np.array([2.0])\n    \n    phi4, v4 = get_attributions_and_value_function(w4, b4, z4, x4)\n    \n    v4_empty = v4[frozenset()]\n    v4_full = v4[frozenset(range(len(w4)))]\n    additivity_check4 = abs((v4_empty + sum(phi4)) - v4_full) < TOLERANCE\n    results.extend([phi4, additivity_check4])\n    \n    # Format representations for printing\n    def format_item(item):\n        if isinstance(item, list):\n            # Format list of floats without excessive precision if they are simple\n            return f\"[{','.join(f'{x:.17g}' for x in item)}]\"\n        return str(item)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4396100"}]}