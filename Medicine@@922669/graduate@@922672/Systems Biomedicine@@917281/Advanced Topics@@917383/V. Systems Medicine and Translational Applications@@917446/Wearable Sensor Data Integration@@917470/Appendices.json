{"hands_on_practices": [{"introduction": "Before we can derive meaningful physiological insights, we must first confront the fundamental physics of data acquisition. Wearable sensors convert continuous analog signals into discrete digital data, a process governed by the principles of sampling. This exercise [@problem_id:4399053] explores what happens when sampling is done improperly due to practical constraints, leading to the phenomenon of aliasing, and how this can drastically corrupt downstream analysis like gait monitoring.", "problem": "A wearable Inertial Measurement Unit (IMU) accelerometer used in systems biomedicine for gait monitoring acquires a continuous-time signal with a dominant narrowband component at the per-step frequency $f_g = 2\\,\\text{Hz}$. Due to power constraints, this accelerometer is uniformly sampled at a rate $f_s = 3\\,\\text{Hz}$, and the step-counting algorithm estimates the step count by detecting the dominant frequency in the sampled signal and multiplying this estimate by the observation duration $T$, assuming one step per cycle of the dominant component.\n\nStarting from the sampling theorem and the definition that uniform sampling of a continuous-time signal produces spectral replicas shifted by integer multiples of the sampling rate, derive the frequency content that will be observed after sampling, and determine the aliased frequency $f_a$ that lies within the baseband interval $[0, f_s/2]$. Then, assuming an asymptotically long observation window $T$ and a perfect frequency estimator that outputs the dominant frequency in the sampled, discrete-time data, derive the multiplicative bias in the estimated step count relative to the true count. Express this bias as a single dimensionless decimal number, defined as the ratio of the estimated count to the true count in the limit $T \\to \\infty$. No rounding is required.", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of digital signal processing, specifically the Nyquist-Shannon sampling theorem and the phenomenon of aliasing. The problem is well-posed, objective, and contains all necessary information to derive a unique, meaningful solution. The scenario described is a standard application in systems biomedicine and wearable technology.\n\nThe core of this problem lies in the consequences of sampling a continuous-time signal at a rate below the Nyquist rate. The Nyquist-Shannon sampling theorem states that for a bandlimited signal with a maximum frequency $f_{max}$, the sampling frequency $f_s$ must be strictly greater than twice the maximum frequency, i.e., $f_s > 2 f_{max}$, to avoid the loss of information and enable perfect reconstruction. The frequency $2 f_{max}$ is known as the Nyquist rate.\n\nIn this problem, the continuous-time signal has a dominant frequency component at the gait frequency $f_g = 2\\,\\text{Hz}$. Therefore, the maximum frequency to be preserved is $f_{max} = f_g = 2\\,\\text{Hz}$. The Nyquist rate required to sample this signal without aliasing is $2 \\cdot f_g = 2 \\cdot 2 = 4\\,\\text{Hz}$.\n\nThe accelerometer, however, samples the signal at a rate of $f_s = 3\\,\\text{Hz}$. Since the sampling frequency $f_s = 3\\,\\text{Hz}$ is less than the Nyquist rate of $4\\,\\text{Hz}$, the signal is undersampled, and a phenomenon known as aliasing will occur.\n\nUniform sampling of a continuous-time signal with a spectral component at frequency $f_g$ produces a discrete-time signal whose spectrum contains replicas of the original component, shifted by all integer multiples of the sampling frequency $f_s$. The frequencies present in the spectrum of the sampled signal are given by the relation:\n$$\nf_{observed} = |f_g - k \\cdot f_s|\n$$\nwhere $k$ is any integer ($k \\in \\{\\dots, -2, -1, 0, 1, 2, \\dots\\}$).\n\nThe frequency estimator operates on the sampled data and thus observes frequencies within the principal or baseband interval. For a real-valued signal, this interval is defined as $[0, f_s/2]$. Given $f_s = 3\\,\\text{Hz}$, the baseband interval is $[0, 1.5]\\,\\text{Hz}$. We must find the value of $k$ for which $f_{observed}$ falls into this interval.\n\nLet's evaluate $f_{observed}$ for different integer values of $k$:\n\\begin{itemize}\n    \\item For $k = 0$: $f_{observed} = |2 - 0 \\cdot 3| = 2\\,\\text{Hz}$. This is outside the baseband $[0, 1.5]\\,\\text{Hz}$.\n    \\item For $k = 1$: $f_{observed} = |2 - 1 \\cdot 3| = |-1| = 1\\,\\text{Hz}$. This frequency lies within the baseband $[0, 1.5]\\,\\text{Hz}$.\n    \\item For $k = -1$: $f_{observed} = |2 - (-1) \\cdot 3| = |2 + 3| = 5\\,\\text{Hz}$. This is outside the baseband.\n    \\item For $k = 2$: $f_{observed} = |2 - 2 \\cdot 3| = |-4| = 4\\,\\text{Hz}$. This is outside the baseband.\n\\end{itemize}\nThe only spectral replica that appears in the defined baseband interval is at $1\\,\\text{Hz}$. Therefore, the aliased frequency $f_a$ that will be detected by the perfect frequency estimator is:\n$$\nf_a = 1 \\text{ Hz}\n$$\nThe problem states that the step-counting algorithm estimates the step count by multiplying the estimated frequency by the observation duration $T$. The true step count, $N_{true}$, is based on the actual per-step frequency, $f_g$:\n$$\nN_{true} = f_g \\cdot T\n$$\nThe estimated step count, $N_{est}$, is based on the aliased frequency, $f_a$, which is what the algorithm detects from the sampled data:\n$$\nN_{est} = f_a \\cdot T\n$$\nThe multiplicative bias is defined as the ratio of the estimated count to the true count. In the limit of an asymptotically long observation window ($T \\to \\infty$), the duration $T$ is a common factor and cancels out:\n$$\n\\text{Bias} = \\frac{N_{est}}{N_{true}} = \\frac{f_a \\cdot T}{f_g \\cdot T} = \\frac{f_a}{f_g}\n$$\nSubstituting the known values for the aliased frequency $f_a$ and the true gait frequency $f_g$:\n$$\n\\text{Bias} = \\frac{1 \\text{ Hz}}{2 \\text{ Hz}} = 0.5\n$$\nThe resulting multiplicative bias is a dimensionless decimal number, as required. Due to undersampling, the algorithm will estimate a step count that is half of the true step count.", "answer": "$$\\boxed{0.5}$$", "id": "4399053"}, {"introduction": "When developing or deploying a new wearable sensor, a critical step is to validate its measurements against a more established 'gold standard' method. This practice [@problem_id:4399060] equips you with the standard biostatistical framework for this task: the Bland-Altman analysis. By implementing the analysis from first principles, you will learn how to quantify inter-device bias, establish limits of agreement, and detect proportional errors when comparing two sources of physiological data, such as heart rate from PPG and ECG sensors.", "problem": "Consider inter-device heart rate integration in systems biomedicine using wearable sensors. Electrocardiography (ECG) and Photoplethysmography (PPG) are two modalities for estimating heart rate. Let the underlying true heart rate be denoted by $h_i$ for the $i$-th paired observation. The ECG-derived heart rate is denoted by $y^{\\mathrm{ECG}}_i$ and the PPG-derived heart rate is denoted by $y^{\\mathrm{PPG}}_i$, both measured in beats per minute (bpm). Assume that each observed heart rate is the true heart rate perturbed by device-specific measurement error, that is, $y^{\\mathrm{ECG}}_i = h_i + \\varepsilon^{\\mathrm{ECG}}_i$ and $y^{\\mathrm{PPG}}_i = h_i + \\varepsilon^{\\mathrm{PPG}}_i$, where $\\varepsilon^{\\mathrm{ECG}}_i$ and $\\varepsilon^{\\mathrm{PPG}}_i$ are random errors with finite variance. Define the paired difference $d_i = y^{\\mathrm{PPG}}_i - y^{\\mathrm{ECG}}_i$ and the paired mean $m_i = \\left(y^{\\mathrm{PPG}}_i + y^{\\mathrm{ECG}}_i\\right)/2$. The inter-device bias is the expected value of the difference, $\\mathbb{E}[d_i]$, and a Bland–Altman analysis characterizes agreement between devices under a Normal model for the differences.\n\nYour task is to write a program that, for each test case, performs the following computations from first principles:\n- Estimate the inter-device bias using the paired differences $d_i$ after removing any pair where either $y^{\\mathrm{ECG}}_i$ or $y^{\\mathrm{PPG}}_i$ is missing. Missing values are represented as not-a-number and should be excluded by pairwise deletion.\n- Estimate the dispersion of $d_i$ using the unbiased sample standard deviation.\n- Assuming that the differences $d_i$ are independent and approximately Normally distributed with constant variance, compute the two-sided $95$-coverage Bland–Altman limits of agreement for the differences using the appropriate Normal quantile.\n- Assess proportional bias by fitting the ordinary least squares regression $d_i = a + b\\,m_i + \\eta_i$ and test the null hypothesis of no proportional bias, $b = 0$, using a two-sided significance level $\\alpha = 0.05$ expressed as a decimal. Report the slope $b$ and a boolean indicating whether proportional bias is statistically significant at the given level. If the regression is undefined due to zero variance in $m_i$ or insufficient data, treat the slope as $0$ and the significance as false.\n\nAll bias and limits-of-agreement answers must be expressed in beats per minute (bpm). The slope $b$ is dimensionless in bpm per bpm. Angles are not involved. Percentages must be treated as decimals; do not use the percentage sign.\n\nUse the following test suite of paired observations (each value is in bpm). For each case, the ECG list and PPG list are aligned pairs measured across sessions; treat them as one combined dataset per case.\n\n- Case $1$ (general case with small positive bias):\n  - ECG: $\\{\\,72,\\,75,\\,80,\\,78,\\,90,\\,92,\\,88,\\,85,\\,76,\\,84\\,\\}$\n  - PPG: $\\{\\,74,\\,77,\\,83,\\,80,\\,93,\\,94,\\,90,\\,86,\\,78,\\,86\\,\\}$\n\n- Case $2$ (boundary case with zero bias and zero dispersion):\n  - ECG: $\\{\\,60,\\,65,\\,70,\\,75,\\,80,\\,85\\,\\}$\n  - PPG: $\\{\\,60,\\,65,\\,70,\\,75,\\,80,\\,85\\,\\}$\n\n- Case $3$ (edge case with proportional bias increasing with heart rate):\n  - ECG: $\\{\\,50,\\,60,\\,70,\\,80,\\,90,\\,100,\\,110\\,\\}$\n  - PPG: $\\{\\,52,\\,62,\\,74.5,\\,84,\\,92.5,\\,107,\\,115.5\\,\\}$\n\n- Case $4$ (edge case with missing values requiring pairwise deletion):\n  - ECG: $\\{\\,70,\\,\\text{NaN},\\,85,\\,95,\\,100\\,\\}$\n  - PPG: $\\{\\,72,\\,78,\\,\\text{NaN},\\,99,\\,102\\,\\}$\n\nThe required final output format for your program is a single line containing a list of results for the four cases, in order, where each case’s result is itself a list with the following six elements in this precise order:\n$[\\,\\widehat{B},\\,\\widehat{S}_d,\\,\\mathrm{LOA}_{\\mathrm{lower}},\\,\\mathrm{LOA}_{\\mathrm{upper}},\\,\\widehat{b},\\,\\mathrm{is\\_significant}\\,]$,\nwhere $\\widehat{B}$ is the estimated bias in bpm, $\\widehat{S}_d$ is the unbiased sample standard deviation of $d_i$ in bpm, $\\mathrm{LOA}_{\\mathrm{lower}}$ and $\\mathrm{LOA}_{\\mathrm{upper}}$ are the Bland–Altman limits of agreement in bpm, $\\widehat{b}$ is the regression slope in bpm per bpm, and $\\mathrm{is\\_significant}$ is a boolean for the proportional-bias test at $\\alpha = 0.05$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with each case’s list enclosed in its own square brackets, for example: $[[r_{1,1},r_{1,2},\\ldots],[r_{2,1},r_{2,2},\\ldots],[r_{3,1},\\ldots],[r_{4,1},\\ldots]]$.", "solution": "The problem is valid as it is scientifically grounded in standard biostatistical methods, well-posed with sufficient and consistent information, and objectively formulated. The task is to perform a Bland-Altman analysis and assess proportional bias for paired heart rate measurements from ECG and PPG sensors.\n\nThe solution proceeds by first implementing data preprocessing, followed by the calculation of specified statistical measures from first principles.\n\n**1. Data Preprocessing**\nFor each test case, the paired observations $(y^{\\mathrm{ECG}}_i, y^{\\mathrm{PPG}}_i)$ are processed. Any pair containing a missing value, represented as not-a-number (NaN), is removed. This procedure is known as pairwise deletion. Let $n$ be the number of valid pairs remaining after this step. For all provided test cases, $n \\ge 3$, which is sufficient for all subsequent calculations.\n\n**2. Paired Differences and Means**\nFrom the cleaned data, we compute the paired differences $d_i$ and paired means $m_i$ for each observation $i=1, \\dots, n$:\n$$d_i = y^{\\mathrm{PPG}}_i - y^{\\mathrm{ECG}}_i$$\n$$m_i = \\frac{y^{\\mathrm{PPG}}_i + y^{\\mathrm{ECG}}_i}{2}$$\n\n**3. Bland-Altman Analysis**\nThe Bland-Altman analysis quantifies the agreement between the two measurement devices.\n\n**3.1. Inter-Device Bias Estimation**\nThe inter-device bias, $\\mathbb{E}[d_i]$, is estimated by the sample mean of the differences, denoted $\\widehat{B}$:\n$$\\widehat{B} = \\bar{d} = \\frac{1}{n}\\sum_{i=1}^{n} d_i$$\n\n**3.2. Dispersion Estimation**\nThe dispersion of the differences is estimated using the unbiased sample standard deviation, denoted $\\widehat{S}_d$:\n$$\\widehat{S}_d = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (d_i - \\bar{d})^2}$$\nThis calculation requires at least $n=2$ valid pairs.\n\n**3.3. Limits of Agreement (LOA)**\nAssuming the differences $d_i$ follow a Normal distribution, the $95\\%$ limits of agreement are calculated. These limits define the range within which $95\\%$ of the differences between the two methods are expected to lie.\n$$\\mathrm{LOA} = \\bar{d} \\pm z_{1-\\alpha/2} \\cdot \\widehat{S}_d$$\nFor a two-sided $95\\%$ coverage, the significance level is $\\alpha=0.05$. We use the $1-\\alpha/2 = 0.975$ quantile of the standard Normal distribution, $z_{0.975} \\approx 1.95996$. The lower and upper limits are:\n$$\\mathrm{LOA}_{\\mathrm{lower}} = \\bar{d} - z_{0.975} \\cdot \\widehat{S}_d$$\n$$\\mathrm{LOA}_{\\mathrm{upper}} = \\bar{d} + z_{0.975} \\cdot \\widehat{S}_d$$\n\n**4. Proportional Bias Assessment**\nProportional bias occurs if the difference between the two measurements varies systematically with the magnitude of the measurement. This is assessed by fitting an ordinary least squares (OLS) regression model:\n$$d_i = a + b\\,m_i + \\eta_i$$\nwhere $\\eta_i$ are the error terms.\n\n**4.1. Slope Estimation**\nThe slope coefficient $\\widehat{b}$ is estimated using the OLS formula:\n$$\\widehat{b} = \\frac{\\sum_{i=1}^{n} (m_i - \\bar{m})(d_i - \\bar{d})}{\\sum_{i=1}^{n} (m_i - \\bar{m})^2} = \\frac{\\mathrm{Cov}(m, d)}{\\mathrm{Var}(m)}$$\nwhere $\\bar{m}$ is the sample mean of the $m_i$. This calculation is undefined if the variance of $m_i$ is zero. In such a case, as per the problem statement, we set $\\widehat{b}=0$ and conclude there is no significant proportional bias.\n\n**4.2. Hypothesis Testing for the Slope**\nWe test the null hypothesis $H_0: b = 0$ (no proportional bias) against the alternative $H_1: b \\neq 0$ at a significance level of $\\alpha = 0.05$. The test statistic is the t-statistic, which under $H_0$ follows a t-distribution with $n-2$ degrees of freedom:\n$$t = \\frac{\\widehat{b}}{\\mathrm{SE}(\\widehat{b})}$$\nThe standard error of the slope, $\\mathrm{SE}(\\widehat{b})$, is calculated as:\n$$\\mathrm{SE}(\\widehat{b}) = \\sqrt{\\frac{\\hat{\\sigma}^2}{\\sum_{i=1}^{n} (m_i - \\bar{m})^2}}$$\nwhere $\\hat{\\sigma}^2$ is the unbiased estimator of the variance of the residuals $\\eta_i$:\n$$\\hat{\\sigma}^2 = \\frac{1}{n-2} \\sum_{i=1}^{n} e_i^2$$\nand $e_i = d_i - (\\hat{a} + \\hat{b}m_i)$ are the residuals from the regression, with $\\hat{a} = \\bar{d} - \\hat{b}\\bar{m}$. This test is valid for $n > 2$.\n\nThe null hypothesis is rejected if the absolute value of the test statistic exceeds the critical value from the t-distribution:\n$$|t| > t_{n-2, 1-\\alpha/2}$$\nwhere $t_{n-2, 1-\\alpha/2}$ is the upper critical value for a two-sided test. If $|t| \\le t_{n-2, 1-\\alpha/2}$, we fail to reject $H_0$, and the proportional bias is not statistically significant. The boolean `is_significant` is set accordingly.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm, t\n\ndef solve():\n    \"\"\"\n    Performs Bland-Altman and proportional bias analysis on paired heart rate data.\n    \"\"\"\n    test_cases = [\n        # Case 1: general case with small positive bias\n        {\n            'ECG': [72, 75, 80, 78, 90, 92, 88, 85, 76, 84],\n            'PPG': [74, 77, 83, 80, 93, 94, 90, 86, 78, 86]\n        },\n        # Case 2: boundary case with zero bias and zero dispersion\n        {\n            'ECG': [60, 65, 70, 75, 80, 85],\n            'PPG': [60, 65, 70, 75, 80, 85]\n        },\n        # Case 3: edge case with proportional bias\n        {\n            'ECG': [50, 60, 70, 80, 90, 100, 110],\n            'PPG': [52, 62, 74.5, 84, 92.5, 107, 115.5]\n        },\n        # Case 4: edge case with missing values\n        {\n            'ECG': [70, np.nan, 85, 95, 100],\n            'PPG': [72, 78, np.nan, 99, 102]\n        },\n    ]\n\n    all_results = []\n\n    for case_data in test_cases:\n        y_ecg = np.array(case_data['ECG'], dtype=float)\n        y_ppg = np.array(case_data['PPG'], dtype=float)\n\n        # Step 1: Pairwise deletion for NaN values\n        valid_mask = ~np.isnan(y_ecg)  ~np.isnan(y_ppg)\n        y_ecg_clean = y_ecg[valid_mask]\n        y_ppg_clean = y_ppg[valid_mask]\n\n        n = len(y_ecg_clean)\n\n        # Step 2: Compute paired differences (d) and means (m)\n        d = y_ppg_clean - y_ecg_clean\n        m = (y_ppg_clean + y_ecg_clean) / 2.0\n\n        # Step 3.1: Estimate inter-device bias\n        bias_est = np.mean(d) if n > 0 else 0.0\n\n        # Step 3.2: Estimate dispersion (unbiased sample standard deviation)\n        sd_est = np.std(d, ddof=1) if n > 1 else 0.0\n\n        # Step 3.3: Compute 95% Bland-Altman limits of agreement\n        z_crit = norm.ppf(0.975)\n        loa_margin = z_crit * sd_est\n        loa_lower = bias_est - loa_margin\n        loa_upper = bias_est + loa_margin\n\n        # Step 4: Assess proportional bias\n        slope_est = 0.0\n        is_significant = False\n\n        # Regression and t-test require n >= 3 and Var(m) > 0\n        if n >= 3 and np.var(m) > 1e-12:\n            # Step 4.1: OLS slope estimation from first principles\n            m_mean = np.mean(m)\n            d_mean = np.mean(d)\n            \n            # Covariance term S_md and variance term S_mm\n            cov_md_sum = np.sum((m - m_mean) * (d - d_mean))\n            var_m_sum = np.sum((m - m_mean) ** 2)\n            \n            slope_est = cov_md_sum / var_m_sum\n\n            # Step 4.2: Hypothesis testing for the slope\n            df = n - 2\n            \n            # Sum of squared residuals (SSR)\n            var_d_sum = np.sum((d - d_mean) ** 2)\n            ssr = var_d_sum - slope_est * cov_md_sum\n            \n            # Handle potential floating point inaccuracies\n            ssr = max(0, ssr)\n\n            # Estimated variance of the residuals\n            residual_var_est = ssr / df\n            \n            # Standard error of the slope\n            se_slope = np.sqrt(residual_var_est / var_m_sum)\n            \n            # Perform t-test if SE is non-zero\n            if se_slope > 1e-12:\n                t_stat = slope_est / se_slope\n                alpha = 0.05\n                t_crit = t.ppf(1 - alpha / 2, df=df)\n                is_significant = np.abs(t_stat) > t_crit\n            # If se_slope is zero, it implies a perfect fit. If slope is also\n            # zero, H0 is not rejected (is_significant remains False).\n\n        all_results.append([\n            bias_est, sd_est, loa_lower, loa_upper, slope_est, is_significant\n        ])\n    \n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "4399060"}, {"introduction": "A central goal of systems biomedicine is to fuse multiple data streams to build a more complete picture of a physiological system, but real-world data is often plagued by missing values. This exercise [@problem_id:4399026] introduces a powerful, model-based approach to this challenge: the Expectation-Maximization (EM) algorithm. You will derive and implement this algorithm to learn the relationship between activity intensity and energy expenditure, even when the activity data is incomplete, demonstrating a principled method for inference with latent variables.", "problem": "You are tasked with building a complete, runnable program that implements the Expectation-Maximization (EM) algorithm to handle missing accelerometer segments when inferring daily activity energy expenditure from wearable sensors, situated within the systems biomedicine context of data fusion and estimation under uncertainty. Consider a single-day minute-level model in which the accelerometer-derived activity intensity at minute $t$, denoted by $x_t$, is modeled as an independent Gaussian random variable with a known prior mean $m_0$ and prior variance $s_0^2$, that is $x_t \\sim \\mathcal{N}(m_0, s_0^2)$ independently across $t$. The minute-level energy expenditure $y_t$ (in kilojoules per minute) is related to the activity intensity via a linear-Gaussian calibration model: conditioned on $x_t$, it follows $y_t \\mid x_t \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_t, \\sigma^2)$, where $\\beta_0 \\in \\mathbb{R}$, $\\beta_1 \\in \\mathbb{R}$, and $\\sigma^2  0$ are unknown parameters to be estimated. You observe $y_t$ for all $t$ and the accelerometer $x_t$ only for some $t$, with the remainder missing. Assume that the prior hyperparameters $m_0$ and $s_0^2$ are known and fixed across EM iterations. Your tasks are, from first principles, to derive and implement an EM algorithm for maximum likelihood estimation of $(\\beta_0, \\beta_1, \\sigma^2)$ given the observed $y_t$ and partially observed $x_t$, then to use the estimated parameters to infer the daily total activity energy expenditure for a separate target day that contains only accelerometer data (partially missing) and no $y_t$. The EM must be derived from fundamental probabilistic rules: Bayes’ rule, properties of Gaussian distributions, and the definition of maximum likelihood estimation for incomplete data.\n\nYour derivation must start from the following fundamental base:\n- The joint density factorization $p(y, x; \\theta) = \\prod_{t=1}^T p(y_t \\mid x_t; \\theta) p(x_t)$ with $p(y_t \\mid x_t; \\theta) = \\mathcal{N}(y_t \\mid \\beta_0 + \\beta_1 x_t, \\sigma^2)$ and $p(x_t) = \\mathcal{N}(x_t \\mid m_0, s_0^2)$, where $\\theta = (\\beta_0, \\beta_1, \\sigma^2)$.\n- The Expectation-Maximization (EM) principle: maximize the observed-data log-likelihood by iterating the E-step $Q(\\theta \\mid \\theta^{(old)}) = \\mathbb{E}[\\log p(y, x; \\theta) \\mid y, x_{\\text{obs}}; \\theta^{(old)}]$ and the M-step $\\theta^{(new)} = \\arg\\max_{\\theta} Q(\\theta \\mid \\theta^{(old)})$.\n- Gaussian conjugacy: with a Gaussian prior on $x_t$ and Gaussian likelihood for $y_t \\mid x_t$, the posterior for a missing $x_t$ given $y_t$ and fixed $(\\beta_0, \\beta_1, \\sigma^2, m_0, s_0^2)$ is Gaussian with analytically computable mean and variance.\n\nUsing these bases only, and without shortcut formulas, derive the E-step and M-step updates necessary to estimate $(\\beta_0, \\beta_1, \\sigma^2)$ when some $x_t$ are missing. Then, specify how to compute the predicted daily total energy expenditure for a target day with only accelerometer data following the same model. For a target day without $y_t$, you must provide the logic for computing $\\mathbb{E}[y_t \\mid \\text{available data}]$ minute-wise and summing across the day. The final program must implement this EM algorithm and produce numerical outputs for the following test suite.\n\nTest suite and required outputs:\nFor each case below, the program must:\n- Run EM on the calibration day with observed $y_t$ (in kilojoules per minute) and partially observed $x_t$ (unitless accelerometer intensity; missing values are present), using the fixed prior hyperparameters $(m_0, s_0^2)$ provided for that case.\n- After EM convergence, compute the predicted total activity energy expenditure for the target day, defined as $\\sum_{t=1}^{T_{\\text{target}}} \\mathbb{E}[y_t \\mid \\text{available accelerometer data at } t]$, expressed in kilojoules (kJ) and rounded to three decimal places. For a target minute $t$ where $x_t$ is observed, use $\\mathbb{E}[y_t \\mid x_t] = \\beta_0 + \\beta_1 x_t$. For a target minute $t$ where $x_t$ is missing and no $y_t$ is available, use $\\mathbb{E}[x_t] = m_0$ under the prior, hence $\\mathbb{E}[y_t] = \\beta_0 + \\beta_1 m_0$.\n\nCase A (happy path):\n- Hyperparameters: $m_0 = 0.5$, $s_0^2 = 1.0$.\n- Calibration day length $T = 6$ with $y = [\\,1.50,\\,1.75,\\,3.00,\\,0.20,\\,1.05,\\,2.30\\,]$ and $x = [\\,0.20,\\,\\text{NaN},\\,1.00,\\,-0.50,\\,\\text{NaN},\\,0.70\\,]$, where $\\text{NaN}$ denotes missing values.\n- Target day length $T_{\\text{target}} = 6$ with $x^{\\text{target}} = [\\,0.00,\\,0.50,\\,\\text{NaN},\\,-0.20,\\,0.90,\\,\\text{NaN}\\,]$.\n\nCase B (all accelerometer missing in calibration and target):\n- Hyperparameters: $m_0 = 0.00$, $s_0^2 = 1.0$.\n- Calibration day length $T = 4$ with $y = [\\,0.50,\\,1.25,\\,-0.25,\\,2.00\\,]$ and $x = [\\,\\text{NaN},\\,\\text{NaN},\\,\\text{NaN},\\,\\text{NaN}\\,]$.\n- Target day length $T_{\\text{target}} = 3$ with $x^{\\text{target}} = [\\,\\text{NaN},\\,\\text{NaN},\\,\\text{NaN}\\,]$.\n\nCase C (low observation noise, partial missing):\n- Hyperparameters: $m_0 = 0.30$, $s_0^2 = 0.20$.\n- Calibration day length $T = 5$ with $y = [\\,2.30,\\,2.40,\\,2.005,\\,2.595,\\,2.20\\,]$ and $x = [\\,0.30,\\,\\text{NaN},\\,0.00,\\,0.60,\\,\\text{NaN}\\,]$.\n- Target day length $T_{\\text{target}} = 4$ with $x^{\\text{target}} = [\\,\\text{NaN},\\,0.10,\\,0.20,\\,\\text{NaN}\\,]$.\n\nAngle units are not applicable. Physical units: report the final predicted totals for each case in kilojoules (kJ), rounded to three decimal places. Your program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets (e.g., \"[12.345,67.890,1.234]\"), in the same order as Cases A, B, and C.\n\nScientific realism constraints:\n- Treat minute-level $y_t$ as kilojoules per minute and $x_t$ as unitless accelerometer intensity.\n- The EM algorithm must be derived from the above stated Gaussian model, using Bayes’ rule and properties of Gaussian conditionals, and must not invoke unintroduced shortcut formulas.\n\nYour final program must be fully self-contained, require no external input, and generate the outputs for the test suite as specified above.", "solution": "The user has provided a problem that is scientifically sound, well-posed, and requires the derivation and implementation of a standard statistical algorithm in a relevant application area. The problem specifies a clear model, a defined objective, and verifiable test cases. All necessary data and conditions are provided. Therefore, the problem is deemed valid.\n\n### Analytical Derivation of the Expectation-Maximization (EM) Algorithm\n\nThis problem addresses a central challenge in systems biomedicine: the integration of noisy and incomplete data from multiple sources, such as wearable sensors, to infer underlying physiological states. Here, we aim to estimate daily activity energy expenditure by fusing accelerometer data ($x_t$) with a calibration model for energy expenditure ($y_t$), where the accelerometer stream has missing segments. We will use the Expectation-Maximization (EM) algorithm to find the maximum likelihood estimates (MLE) of the model parameters.\n\nThe parameters to be estimated are $\\theta = (\\beta_0, \\beta_1, \\sigma^2)$. The model is specified as:\n1.  Prior on activity intensity: $x_t \\sim \\mathcal{N}(m_0, s_0^2)$\n2.  Likelihood of energy expenditure: $y_t | x_t \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_t, \\sigma^2)$\n\nThe data consists of fully observed $y = \\{y_t\\}_{t=1}^T$ and partially observed $x = \\{x_t\\}_{t=1}^T$. Let $\\mathcal{O}$ be the set of time indices $t$ where $x_t$ is observed, and $\\mathcal{M}$ be the set where $x_t$ is missing. The complete data is $(y, x)$, and the observed data is $(y, x_{\\mathcal{O}})$.\n\nThe EM algorithm iteratively maximizes the expected complete-data log-likelihood, conditioned on the observed data and current parameter estimates.\n\n**Complete-Data Log-Likelihood**\n\nThe joint probability density of the complete data $(y, x)$ is $p(y, x; \\theta) = \\prod_{t=1}^T p(y_t \\mid x_t; \\theta) p(x_t)$. The log-likelihood, $\\ell(\\theta; y, x) = \\log p(y, x; \\theta)$, is:\n$$\n\\ell(\\theta; y, x) = \\sum_{t=1}^T \\log p(y_t \\mid x_t; \\theta) + \\sum_{t=1}^T \\log p(x_t)\n$$\nSince $p(x_t) = \\mathcal{N}(x_t \\mid m_0, s_0^2)$ does not depend on the parameters $\\theta = (\\beta_0, \\beta_1, \\sigma^2)$, we can treat $\\sum \\log p(x_t)$ as a constant during maximization. We focus on the first term:\n$$\n\\sum_{t=1}^T \\log \\mathcal{N}(y_t \\mid \\beta_0 + \\beta_1 x_t, \\sigma^2) = \\sum_{t=1}^T \\left[ -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(y_t - \\beta_0 - \\beta_1 x_t)^2 \\right]\n$$\n$$\n\\ell_c(\\theta; y, x) = -\\frac{T}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{t=1}^T (y_t - \\beta_0 - \\beta_1 x_t)^2 + \\text{const}\n$$\nwhere $\\ell_c(\\theta; y, x)$ is the part of the log-likelihood relevant to $\\theta$.\n\n**E-Step: Computing the Q-function**\n\nAt iteration $k+1$, the E-step computes the Q-function, which is the expectation of the complete-data log-likelihood with respect to the posterior distribution of the missing data $x_{\\mathcal{M}}$, given the observed data and the current parameter estimates $\\theta^{(k)} = (\\beta_0^{(k)}, \\beta_1^{(k)}, (\\sigma^2)^{(k)})$.\n$$\nQ(\\theta \\mid \\theta^{(k)}) = \\mathbb{E}_{x_{\\mathcal{M}}}[\\ell_c(\\theta; y, x) \\mid y, x_{\\mathcal{O}}; \\theta^{(k)}]\n$$\nDue to independence across time points, the expectation can be distributed over the sum:\n$$\nQ(\\theta \\mid \\theta^{(k)}) = -\\frac{T}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{t=1}^T \\mathbb{E}[(y_t - \\beta_0 - \\beta_1 x_t)^2 \\mid y_t, x_t \\text{ if } t \\in \\mathcal{O}; \\theta^{(k)}]\n$$\nFor $t \\in \\mathcal{O}$, $x_t$ is known, so the expectation is trivial. For $t \\in \\mathcal{M}$, we need to compute expectations of terms involving the missing $x_t$.\n$$\n\\mathbb{E}[(y_t - \\beta_0 - \\beta_1 x_t)^2 \\mid \\dots] = \\mathbb{E}[ (y_t-\\beta_0)^2 - 2\\beta_1(y_t-\\beta_0)x_t + \\beta_1^2 x_t^2 \\mid \\dots]\n$$\n$$\n= (y_t-\\beta_0)^2 - 2\\beta_1(y_t-\\beta_0)\\mathbb{E}[x_t \\mid \\dots] + \\beta_1^2 \\mathbb{E}[x_t^2 \\mid \\dots]\n$$\nThe E-step thus requires computing two conditional moments for each $t \\in \\mathcal{M}$:\n1.  $\\tilde{x}_t^{(k)} = \\mathbb{E}[x_t \\mid y_t; \\theta^{(k)}]$\n2.  $\\widetilde{x_t^2}^{(k)} = \\mathbb{E}[x_t^2 \\mid y_t; \\theta^{(k)}]$\n\nFor $t \\in \\mathcal{O}$, we simply have $\\tilde{x}_t^{(k)} = x_t$ and $\\widetilde{x_t^2}^{(k)} = x_t^2$.\n\nTo find these moments for $t \\in \\mathcal{M}$, we derive the posterior distribution $p(x_t \\mid y_t; \\theta^{(k)})$ using Bayes' rule:\n$$\np(x_t \\mid y_t; \\theta^{(k)}) \\propto p(y_t \\mid x_t; \\theta^{(k)}) p(x_t)\n$$\nBoth are Gaussian distributions. The product of two Gaussian PDFs is proportional to another Gaussian PDF. The exponent of the posterior is a quadratic in $x_t$:\n$$\n-\\frac{1}{2} \\left[ \\frac{(y_t - \\beta_0^{(k)} - \\beta_1^{(k)} x_t)^2}{(\\sigma^2)^{(k)}} + \\frac{(x_t - m_0)^2}{s_0^2} \\right]\n$$\nBy completing the square for $x_t$, we find the posterior $p(x_t \\mid y_t; \\theta^{(k)})$ is a Gaussian $\\mathcal{N}(\\mu_{t,\\text{post}}^{(k)}, (s_{t,\\text{post}}^2)^{(k)})$ with variance:\n$$\n(s_{t,\\text{post}}^2)^{(k)} = \\left( \\frac{(\\beta_1^{(k)})^2}{(\\sigma^2)^{(k)}} + \\frac{1}{s_0^2} \\right)^{-1}\n$$\nand mean:\n$$\n\\mu_{t,\\text{post}}^{(k)} = (s_{t,\\text{post}}^2)^{(k)} \\left( \\frac{\\beta_1^{(k)}(y_t - \\beta_0^{(k)})}{(\\sigma^2)^{(k)}} + \\frac{m_0}{s_0^2} \\right)\n$$\nThe required moments are then:\n$$\n\\tilde{x}_t^{(k)} = \\mu_{t,\\text{post}}^{(k)}\n$$\n$$\n\\widetilde{x_t^2}^{(k)} = \\text{Var}(x_t|y_t;\\theta^{(k)}) + (\\mathbb{E}[x_t|y_t;\\theta^{(k)}])^2 = (s_{t,\\text{post}}^2)^{(k)} + (\\mu_{t,\\text{post}}^{(k)})^2\n$$\n\n**M-Step: Maximizing the Q-function**\n\nThe M-step updates the parameters to $\\theta^{(k+1)}$ by maximizing $Q(\\theta \\mid \\theta^{(k)})$. This is equivalent to minimizing the sum of squared errors term with respect to $\\beta_0$ and $\\beta_1$:\n$$\nS(\\beta_0, \\beta_1) = \\sum_{t=1}^T \\left[ (y_t-\\beta_0)^2 - 2\\beta_1(y_t-\\beta_0)\\tilde{x}_t^{(k)} + \\beta_1^2 \\widetilde{x_t^2}^{(k)} \\right]\n$$\nSetting the partial derivatives $\\frac{\\partial S}{\\partial \\beta_0}$ and $\\frac{\\partial S}{\\partial \\beta_1}$ to zero yields the normal equations for a weighted least squares problem:\n$$\n\\begin{pmatrix} T  \\sum_{t=1}^T \\tilde{x}_t^{(k)} \\\\ \\sum_{t=1}^T \\tilde{x}_t^{(k)}  \\sum_{t=1}^T \\widetilde{x_t^2}^{(k)} \\end{pmatrix}\n\\begin{pmatrix} \\beta_0^{(k+1)} \\\\ \\beta_1^{(k+1)} \\end{pmatrix}\n=\n\\begin{pmatrix} \\sum_{t=1}^T y_t \\\\ \\sum_{t=1}^T y_t \\tilde{x}_t^{(k)} \\end{pmatrix}\n$$\nSolving this $2 \\times 2$ system gives the updates:\n$$\n\\beta_1^{(k+1)} = \\frac{T \\sum y_t \\tilde{x}_t^{(k)} - (\\sum y_t)(\\sum \\tilde{x}_t^{(k)})}{T \\sum \\widetilde{x_t^2}^{(k)} - (\\sum \\tilde{x}_t^{(k)})^2}\n$$\n$$\n\\beta_0^{(k+1)} = \\frac{1}{T} \\left( \\sum y_t - \\beta_1^{(k+1)} \\sum \\tilde{x}_t^{(k)} \\right)\n$$\nTo update $\\sigma^2$, we set $\\frac{\\partial Q}{\\partial (\\sigma^2)} = 0$, which yields:\n$$\n(\\sigma^2)^{(k+1)} = \\frac{1}{T} S(\\beta_0^{(k+1)}, \\beta_1^{(k+1)})\n$$\n$$\n(\\sigma^2)^{(k+1)} = \\frac{1}{T} \\sum_{t=1}^T \\left[ (y_t - \\beta_0^{(k+1)})^2 - 2\\beta_1^{(k+1)}(y_t - \\beta_0^{(k+1)}) \\tilde{x}_t^{(k)} + (\\beta_1^{(k+1)})^2 \\widetilde{x_t^2}^{(k)} \\right]\n$$\n\n**Algorithm Initialization and Prediction**\n\nAn appropriate initialization for the parameters $\\theta^{(0)}$ is crucial.\n- If there are sufficient complete observations ($t \\in \\mathcal{O}$), we can initialize $\\beta_0^{(0)}$ and $\\beta_1^{(0)}$ by performing a simple linear regression of $y_t$ on $x_t$ for $t \\in \\mathcal{O}$.\n- If there are no complete observations (as in Case B), no information exists in the data to estimate $\\beta_1$. A principled initialization is to set $\\beta_1^{(0)}=0$, which implies no relationship. $\\beta_0^{(0)}$ is then the sample mean of $y$, and $(\\sigma^2)^{(0)}$ is the sample variance of $y$.\n- $(\\sigma^2)^{(0)}$ can be initialized to $1.0$ or the residual variance from the initial regression.\n\nAfter the EM algorithm has converged to the final estimates $(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\sigma}^2)$, the predicted total energy expenditure for a target day is calculated. For each minute $t$ in the target day:\n- If $x_t^{\\text{target}}$ is observed: Predicted $y_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_t^{\\text{target}}$.\n- If $x_t^{\\text{target}}$ is missing: Since no $y_t^{\\text{target}}$ is available, we use the prior expectation for $x_t$, which is $m_0$. The predicted $y_t$ is $\\mathbb{E}[y_t] = \\mathbb{E}[\\hat{\\beta}_0 + \\hat{\\beta}_1 x_t] = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\mathbb{E}[x_t] = \\hat{\\beta}_0 + \\hat{\\beta}_1 m_0$.\n\nThe total daily expenditure is the sum of these minute-wise predictions.", "answer": "```python\nimport numpy as np\n\ndef em_algorithm(y_cal, x_cal, m0, s0_sq, n_iter=100):\n    \"\"\"\n    Implements the EM algorithm for a linear-Gaussian model with missing data.\n\n    Args:\n        y_cal (np.ndarray): Observed energy expenditure data.\n        x_cal (np.ndarray): Partially observed accelerometer data (with np.nan for missing).\n        m0 (float): Prior mean for x.\n        s0_sq (float): Prior variance for x.\n        n_iter (int): Number of EM iterations.\n\n    Returns:\n        tuple: Estimated parameters (beta0, beta1, sigma_sq).\n    \"\"\"\n    T = len(y_cal)\n    obs_indices = np.where(~np.isnan(x_cal))[0]\n    mis_indices = np.where(np.isnan(x_cal))[0]\n\n    # Initialization\n    if len(obs_indices) >= 2:\n        x_obs = x_cal[obs_indices]\n        y_obs = y_cal[obs_indices]\n        # Linear regression on complete cases\n        x_mean, y_mean = np.mean(x_obs), np.mean(y_obs)\n        ss_xy = np.sum((x_obs - x_mean) * (y_obs - y_mean))\n        ss_xx = np.sum((x_obs - x_mean) ** 2)\n        if ss_xx > 1e-9:\n            beta1 = ss_xy / ss_xx\n            beta0 = y_mean - beta1 * x_mean\n        else: # not enough variation in observed x\n            beta1 = 0.0\n            beta0 = y_mean\n        # Initialize sigma_sq based on residuals or simply 1.0\n        y_pred = beta0 + beta1 * x_obs\n        sigma_sq = np.mean((y_obs - y_pred)**2)\n        if sigma_sq  1e-9:\n            sigma_sq = 1.0\n    else: # No or too few complete cases (e.g., Case B)\n        beta1 = 0.0\n        beta0 = np.mean(y_cal)\n        sigma_sq = np.var(y_cal) if T > 1 else 1.0\n    \n    if sigma_sq  1e-9: # ensure variance is positive\n        sigma_sq = 1.0\n    \n    for _ in range(n_iter):\n        # --- E-Step ---\n        # For missing x, calculate posterior mean and variance\n        # then compute E[x] and E[x^2]\n        if sigma_sq = 0 or s0_sq = 0: # Check for invalid variances\n            # If variance collapses, we cannot proceed. This might happen with ill-conditioned data.\n            # Return current estimates.\n             break\n\n        s_t_post_sq_inv = (beta1**2 / sigma_sq) + (1 / s0_sq)\n        s_t_post_sq = 1.0 / s_t_post_sq_inv\n        \n        y_mis = y_cal[mis_indices]\n        mu_t_post = s_t_post_sq * (beta1 * (y_mis - beta0) / sigma_sq + m0 / s0_sq)\n\n        x_tilde = np.copy(x_cal)\n        x_sq_tilde = np.copy(x_cal)**2\n\n        x_tilde[mis_indices] = mu_t_post\n        x_sq_tilde[mis_indices] = s_t_post_sq + mu_t_post**2\n        \n        # --- M-Step ---\n        # Update beta0 and beta1\n        s_x = np.sum(x_tilde)\n        s_xx = np.sum(x_sq_tilde)\n        s_y = np.sum(y_cal)\n        s_yx = np.sum(y_cal * x_tilde)\n\n        # Solve the 2x2 system from the normal equations\n        # Denominator = T * S_xx - S_x^2\n        denominator = T * s_xx - s_x**2\n        if abs(denominator)  1e-9:\n            # This can happen if all x_tilde are the same.\n            # In this case, beta1 is not identifiable from the regression.\n            # Keep beta1 as is, or set to 0. We keep it to avoid oscillations.\n            pass\n        else:\n            beta1 = (T * s_yx - s_y * s_x) / denominator\n        \n        beta0 = (s_y - beta1 * s_x) / T\n\n        # Update sigma_sq\n        term1 = np.sum((y_cal - beta0)**2)\n        term2 = -2 * beta1 * np.sum((y_cal - beta0) * x_tilde)\n        term3 = beta1**2 * np.sum(x_sq_tilde)\n        sigma_sq = (term1 + term2 + term3) / T\n        \n        if sigma_sq = 1e-9: # Prevent variance from collapsing to zero\n            sigma_sq = 1e-9\n\n    return beta0, beta1, sigma_sq\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {\n            \"m0\": 0.5, \"s0_sq\": 1.0,\n            \"y_cal\": np.array([1.50, 1.75, 3.00, 0.20, 1.05, 2.30]),\n            \"x_cal\": np.array([0.20, np.nan, 1.00, -0.50, np.nan, 0.70]),\n            \"x_target\": np.array([0.00, 0.50, np.nan, -0.20, 0.90, np.nan])\n        },\n        # Case B\n        {\n            \"m0\": 0.0, \"s0_sq\": 1.0,\n            \"y_cal\": np.array([0.50, 1.25, -0.25, 2.00]),\n            \"x_cal\": np.array([np.nan, np.nan, np.nan, np.nan]),\n            \"x_target\": np.array([np.nan, np.nan, np.nan])\n        },\n        # Case C\n        {\n            \"m0\": 0.30, \"s0_sq\": 0.20,\n            \"y_cal\": np.array([2.30, 2.40, 2.005, 2.595, 2.20]),\n            \"x_cal\": np.array([0.30, np.nan, 0.00, 0.60, np.nan]),\n            \"x_target\": np.array([np.nan, 0.10, 0.20, np.nan])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        m0 = case[\"m0\"]\n        s0_sq = case[\"s0_sq\"]\n        y_cal = case[\"y_cal\"]\n        x_cal = case[\"x_cal\"]\n        x_target = case[\"x_target\"]\n\n        # Run EM algorithm to estimate parameters\n        beta0_hat, beta1_hat, _ = em_algorithm(y_cal, x_cal, m0, s0_sq)\n\n        # Predict total energy expenditure for the target day\n        y_pred_total = 0.0\n        for xt in x_target:\n            if np.isnan(xt):\n                # Use prior mean m0 for missing x_target\n                y_pred_total += beta0_hat + beta1_hat * m0\n            else:\n                # Use observed x_target\n                y_pred_total += beta0_hat + beta1_hat * xt\n        \n        results.append(f\"{y_pred_total:.3f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4399026"}]}