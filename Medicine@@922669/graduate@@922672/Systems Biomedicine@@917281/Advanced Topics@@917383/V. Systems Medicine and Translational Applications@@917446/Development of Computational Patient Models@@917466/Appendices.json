{"hands_on_practices": [{"introduction": "Before we can confidently estimate a model's parameters from data, we must first answer a more fundamental question: is it even possible to uniquely determine the parameters from the model's structure and the planned experiment? This property, known as structural identifiability, is a critical prerequisite for building a reliable digital twin. This exercise [@problem_id:4335072] provides hands-on practice with the differential algebra approach, a rigorous method for testing identifiability by converting the model into an input-output form and examining the uniqueness of the parameter-to-coefficient map.", "problem": "A digital twin of a patient's drug pharmacokinetics is built on a two-compartment linear mass-balance model with intravenous infusion input to the central compartment. Let $x_{1}(t)$ denote the amount of drug in the central compartment, $x_{2}(t)$ the amount in the peripheral compartment, and $y(t)$ the measured plasma concentration. The model is\n$\n\\begin{aligned}\n\\frac{d x_{1}}{d t} = -\\left(k_{10} + k_{12}\\right) x_{1} + k_{21} x_{2} + u(t),\\\\\n\\frac{d x_{2}}{d t} = k_{12} x_{1} - k_{21} x_{2},\\\\\ny(t) = \\frac{x_{1}(t)}{V},\n\\end{aligned}\n$\nwhere $k_{10}0$ is the elimination rate from the central compartment, $k_{12}0$ and $k_{21}0$ are intercompartmental transfer rates, $V0$ is the central volume of distribution, and $u(t)$ is a known, analytic infusion rate. Assume ideal, noise-free measurements and that $u(t)$ is not identically zero on any open interval.\n\nUsing the differential algebra approach, treat the state equations and the output relation as generating a differential ideal over the differential polynomial ring in $x_{1}(t)$, $x_{2}(t)$, and $y(t)$ with coefficients in $\\mathbb{R}(k_{10},k_{12},k_{21},V)$ and with derivation $d/dt$. By eliminating the unobserved state $x_{2}(t)$ and expressing everything in terms of $y(t)$, $u(t)$, and their time derivatives, derive a monic second-order input–output ordinary differential equation (ODE) of the form\n$\ny''(t) + a_{1}\\, y'(t) + a_{0}\\, y(t) = b_{1}\\, u'(t) + b_{0}\\, u(t),\n$\nwhere $a_{1}$, $a_{0}$, $b_{1}$, and $b_{0}$ are algebraic functions of the physiological parameters $k_{10}$, $k_{12}$, $k_{21}$, and $V$. Then, by analyzing the resulting coefficient map from parameters to $a_{1}$, $a_{0}$, $b_{1}$, $b_{0}$, argue structural identifiability by checking the uniqueness of parameter mappings for almost all inputs.\n\nFinally, provide a closed-form analytic expression for the dimensionless ratio $k_{12}/k_{21}$ in terms of the coefficients $a_{1}$, $a_{0}$, $b_{1}$, and $b_{0}$ of the input–output ODE derived above. Your final answer must be a single expression. No rounding is required, and no units are needed because the requested ratio is dimensionless.", "solution": "The problem asks for the derivation of an input-output ordinary differential equation (ODE) for a two-compartment pharmacokinetic model, an analysis of its structural identifiability, and a closed-form expression for a ratio of its parameters.\n\n### Step 1: Problem Validation\n\nThe problem statement is first validated against the required criteria.\n\n**1. Extraction of Givens:**\nThe model is described by the following system of differential equations and an output equation:\nState equations:\n$$\n\\frac{d x_{1}}{d t} = -\\left(k_{10} + k_{12}\\right) x_{1} + k_{21} x_{2} + u(t) \\quad (\\text{Eq. 1})\n$$\n$$\n\\frac{d x_{2}}{d t} = k_{12} x_{1} - k_{21} x_{2} \\quad (\\text{Eq. 2})\n$$\nOutput equation:\n$$\ny(t) = \\frac{x_{1}(t)}{V} \\quad (\\text{Eq. 3})\n$$\nParameters: $k_{10}0$, $k_{12}0$, $k_{21}0$, $V0$.\nVariables: $x_{1}(t)$ and $x_{2}(t)$ are the drug amounts in the central and peripheral compartments, respectively. $y(t)$ is the measured plasma concentration.\nInput: $u(t)$ is a known, analytic infusion rate, not identically zero on any open interval.\nTask: Derive a monic second-order ODE of the form $y''(t) + a_{1}\\, y'(t) + a_{0}\\, y(t) = b_{1}\\, u'(t) + b_{0}\\, u(t)$, determine expressions for coefficients $a_1, a_0, b_1, b_0$, argue for structural identifiability, and find an expression for the ratio $k_{12}/k_{21}$.\n\n**2. Validation:**\n- **Scientifically Grounded:** The problem uses a standard two-compartment linear model, a fundamental concept in pharmacokinetics. The use of differential algebra and input-output relations for identifiability analysis is a rigorous and established method in systems theory and systems biology.\n- **Well-Posed:** The problem is clearly stated, providing all necessary equations and constraints. The objective is specific and achievable with the given information.\n- **Objective:** The problem is formulated using precise mathematical language, free of ambiguity or subjectivity.\n\n**3. Verdict:**\nThe problem is valid. It is scientifically sound, well-posed, and objective. We may proceed with the solution.\n\n### Step 2: Derivation of the Input-Output ODE\n\nThe goal is to eliminate the unobserved state variables $x_1(t)$ and $x_2(t)$ to obtain an ODE relating the output $y(t)$ to the input $u(t)$.\n\nFrom the output equation (Eq. 3), we express $x_1(t)$ in terms of $y(t)$:\n$$\nx_{1}(t) = V y(t)\n$$\nDifferentiating with respect to time $t$ gives the derivative of $x_1(t)$:\n$$\n\\frac{d x_{1}}{d t} = V \\frac{d y}{d t} = V y'(t)\n$$\nSubstitute these expressions for $x_1(t)$ and its derivative into the first state equation (Eq. 1):\n$$\nV y'(t) = -(k_{10} + k_{12}) V y(t) + k_{21} x_{2}(t) + u(t)\n$$\nWe can rearrange this equation to solve for the unobserved state $x_2(t)$:\n$$\nk_{21} x_{2}(t) = V y'(t) + (k_{10} + k_{12}) V y(t) - u(t) \\quad (\\text{Eq. 4})\n$$\nTo eliminate $x_2(t)$, we differentiate Eq. 4 with respect to time:\n$$\nk_{21} \\frac{d x_{2}}{d t} = V y''(t) + (k_{10} + k_{12}) V y'(t) - u'(t)\n$$\nNow we can substitute the expression for $\\frac{dx_2}{dt}$ from the second state equation (Eq. 2), which is $\\frac{dx_2}{dt} = k_{12}x_1 - k_{21}x_2$:\n$$\nk_{21} (k_{12} x_{1} - k_{21} x_{2}) = V y''(t) + (k_{10} + k_{12}) V y'(t) - u'(t)\n$$\nThis gives:\n$$\nk_{12} k_{21} x_{1} - k_{21}^2 x_{2} = V y''(t) + (k_{10} + k_{12}) V y'(t) - u'(t)\n$$\nWe now have two variables to eliminate, $x_1$ and $x_2$. We use $x_1(t) = V y(t)$ and the expression for $k_{21} x_2(t)$ from Eq. 4. Specifically, we need an expression for $k_{21}^2 x_2(t)$, which we get by multiplying Eq. 4 by $k_{21}$:\n$$\nk_{21}^2 x_{2}(t) = k_{21} \\left( V y'(t) + (k_{10} + k_{12}) V y(t) - u(t) \\right)\n$$\nSubstituting this and $x_1 = V y$ into the differentiated equation:\n$$\nk_{12} k_{21} (V y) - k_{21} \\left( V y' + (k_{10} + k_{12}) V y - u \\right) = V y'' + (k_{10} + k_{12}) V y' - u'\n$$\nLet's expand and group terms. All terms have a factor of $V$ except those involving $u$ and $u'$. We will divide by $V$ later.\n$$\nk_{12} k_{21} V y - k_{21} V y' - k_{21}(k_{10} + k_{12}) V y + k_{21} u = V y'' + (k_{10} + k_{12}) V y' - u'\n$$\nMove all terms with $y$ and its derivatives to the left side and terms with $u$ and its derivatives to the right side:\n$$\n-V y'' - k_{21} V y' - (k_{10} + k_{12}) V y' + [k_{12} k_{21} V - k_{21}(k_{10} + k_{12}) V] y = -k_{21} u - u'\n$$\nMultiply by $-1$ and divide by $V$ (since $V>0$):\n$$\ny'' + (k_{21} + k_{10} + k_{12}) y' + [k_{21}(k_{10} + k_{12}) - k_{12} k_{21}] y = \\frac{1}{V} u' + \\frac{k_{21}}{V} u\n$$\nSimplify the coefficient of $y(t)$:\n$$\nk_{21}(k_{10} + k_{12}) - k_{12} k_{21} = k_{10} k_{21} + k_{12} k_{21} - k_{12} k_{21} = k_{10} k_{21}\n$$\nThe resulting monic second-order input-output ODE is:\n$$\ny''(t) + (k_{10} + k_{12} + k_{21}) y'(t) + (k_{10} k_{21}) y(t) = \\frac{1}{V} u'(t) + \\frac{k_{21}}{V} u(t)\n$$\n\n### Step 3: Structural Identifiability Analysis\n\nBy comparing the derived ODE with the general form $y''(t) + a_{1} y'(t) + a_{0} y(t) = b_{1} u'(t) + b_{0} u(t)$, we establish the following map from the physiological parameters $\\theta = (k_{10}, k_{12}, k_{21}, V)$ to the ODE coefficients $p = (a_1, a_0, b_1, b_0)$:\n$$\na_1 = k_{10} + k_{12} + k_{21}\n$$\n$$\na_0 = k_{10} k_{21}\n$$\n$$\nb_1 = \\frac{1}{V}\n$$\n$$\nb_0 = \\frac{k_{21}}{V}\n$$\nStructural identifiability requires that this map be injective, meaning we can uniquely determine the parameter set $\\theta$ from a given coefficient set $p$. Let's solve this system of algebraic equations for the parameters:\n\nFrom $b_1 = 1/V$, we find $V$:\n$$\nV = \\frac{1}{b_1}\n$$\nSince $V  0$, we require $b_1  0$.\n\nFrom $b_0 = k_{21}/V$ and $V=1/b_1$, we find $k_{21}$:\n$$\nk_{21} = b_0 V = b_0 \\left(\\frac{1}{b_1}\\right) = \\frac{b_0}{b_1}\n$$\nSince $k_{21}  0$, we require $b_0/b_1  0$.\n\nFrom $a_0 = k_{10} k_{21}$, we find $k_{10}$:\n$$\nk_{10} = \\frac{a_0}{k_{21}} = \\frac{a_0}{b_0/b_1} = \\frac{a_0 b_1}{b_0}\n$$\nSince $k_{10}  0$, we require $a_0 b_1 / b_0  0$.\n\nFinally, from $a_1 = k_{10} + k_{12} + k_{21}$, we find $k_{12}$:\n$$\nk_{12} = a_1 - k_{10} - k_{21} = a_1 - \\frac{a_0 b_1}{b_0} - \\frac{b_0}{b_1}\n$$\nSince $k_{12}  0$, we require $a_1  \\frac{a_0 b_1}{b_0} + \\frac{b_0}{b_1}$.\n\nSince a unique solution for each parameter $(k_{10}, k_{12}, k_{21}, V)$ can be found in terms of the coefficients $(a_1, a_0, b_1, b_0)$, the mapping is injective. The model is therefore structurally identifiable, provided the input $u(t)$ is sufficiently rich to allow for the unique determination of the coefficients from the input-output data, which is guaranteed by the problem statement (\"known, analytic infusion rate... not identically zero on any open interval\").\n\n### Step 4: Expression for the Ratio $k_{12}/k_{21}$\n\nThe final task is to provide a closed-form expression for the dimensionless ratio $k_{12}/k_{21}$ in terms of the coefficients $a_1, a_0, b_1, b_0$. Using the expressions for $k_{12}$ and $k_{21}$ derived in the identifiability analysis:\n$$\nk_{12} = a_1 - \\frac{a_0 b_1}{b_0} - \\frac{b_0}{b_1}\n$$\n$$\nk_{21} = \\frac{b_0}{b_1}\n$$\nThe ratio is:\n$$\n\\frac{k_{12}}{k_{21}} = \\frac{a_1 - \\frac{a_0 b_1}{b_0} - \\frac{b_0}{b_1}}{\\frac{b_0}{b_1}}\n$$\nWe can simplify this expression by multiplying the numerator by the reciprocal of the denominator:\n$$\n\\frac{k_{12}}{k_{21}} = \\left( a_1 - \\frac{a_0 b_1}{b_0} - \\frac{b_0}{b_1} \\right) \\left( \\frac{b_1}{b_0} \\right)\n$$\nDistributing the term $\\frac{b_1}{b_0}$:\n$$\n\\frac{k_{12}}{k_{21}} = a_1 \\left( \\frac{b_1}{b_0} \\right) - \\left( \\frac{a_0 b_1}{b_0} \\right) \\left( \\frac{b_1}{b_0} \\right) - \\left( \\frac{b_0}{b_1} \\right) \\left( \\frac{b_1}{b_0} \\right)\n$$\n$$\n\\frac{k_{12}}{k_{21}} = \\frac{a_1 b_1}{b_0} - \\frac{a_0 b_1^2}{b_0^2} - 1\n$$\nThis is the final closed-form expression for the ratio.", "answer": "$$\n\\boxed{\\frac{a_{1} b_{1}}{b_{0}} - \\frac{a_{0} b_{1}^{2}}{b_{0}^{2}} - 1}\n$$", "id": "4335072"}, {"introduction": "The parameters of a computational model dictate its behavior, but their influence is rarely uniform; some parameters may be highly influential while others have negligible effects. Sensitivity analysis provides the mathematical framework to quantify these relationships, which is essential for guiding parameter estimation, performing uncertainty analysis, and simplifying models. In this practice [@problem_id:4335020], you will derive the fundamental equations for sensitivity analysis from first principles and apply them to compute the impact of a drug elimination parameter on a predicted concentration time-course.", "problem": "A computational patient model (digital twin) for drug disposition is built from fundamental mass-balance principles and represented as an ordinary differential equation (ODE) system. Let the state be $x(t) \\in \\mathbb{R}^{n}$ evolving under $\\frac{d x}{d t} = f\\!\\left(x(t), \\theta, u(t), t\\right)$, where $\\theta \\in \\mathbb{R}^{p}$ denotes model parameters and $u(t)$ denotes known inputs. Define the local sensitivity matrix $S(t) \\in \\mathbb{R}^{n \\times p}$ by $S_{ij}(t) = \\frac{\\partial x_i(t)}{\\partial \\theta_j}$. Starting from the chain rule and the ODE definition, derive the evolution equation and initial conditions for $S(t)$ that are suitable for forward sensitivity analysis, and explain how, for a scalar objective $J$ that depends on the solution, an adjoint-based method can compute $\\frac{\\partial J}{\\partial \\theta}$ efficiently.\n\nNow specialize to a two-compartment pharmacokinetic (PK) digital twin with oral dosing. The gastrointestinal amount $x_1(t)$ and central compartment amount $x_2(t)$ obey\n$$\n\\frac{d x_1}{d t} = - k_a x_1, \\quad \\frac{d x_2}{d t} = k_a x_1 - k_e x_2,\n$$\nwith initial conditions $x_1(0) = D$ and $x_2(0) = 0$. The observable is the plasma concentration $C(t) = \\frac{x_2(t)}{V}$, where $V$ is the apparent volume of distribution. Use the forward sensitivity framework to derive a closed-form expression for the local sensitivity $\\frac{\\partial C(T)}{\\partial k_e}$ at a fixed time $T$, and explain how the adjoint method for the terminal objective $J = C(T)$ leads to the same expression.\n\nFinally, evaluate $\\frac{\\partial C(T)}{\\partial k_e}$ numerically for $D = 100$ mg, $k_a = 1$ hour$^{-1}$, $k_e = 0.2$ hour$^{-1}$, $V = 10$ L, and $T = 4$ hours. Round your answer to $4$ significant figures. Express the final sensitivity in mg·h/L.", "solution": "The problem is scientifically grounded, well-posed, and objective. It presents a standard task in systems theory and computational biology: the derivation and application of sensitivity analysis for an ordinary differential equation (ODE) model. All provided information is self-contained and consistent.\n\nWe begin by addressing the general theory of forward and adjoint sensitivity analysis for a system of ODEs, then apply this framework to the specific pharmacokinetic model provided.\n\n**1. Forward Sensitivity Analysis**\n\nLet the system be defined by the state vector $x(t) \\in \\mathbb{R}^{n}$ evolving according to the ODE:\n$$\n\\frac{d x}{d t} = f(x(t), \\theta, u(t), t)\n$$\nwith initial condition $x(t_0) = x_0(\\theta)$. Here, $\\theta \\in \\mathbb{R}^{p}$ is a vector of model parameters and $u(t)$ is a known input function.\n\nThe local sensitivity matrix $S(t) \\in \\mathbb{R}^{n \\times p}$ is defined by its components $S_{ij}(t) = \\frac{\\partial x_i(t)}{\\partial \\theta_j}$. To find the differential equation governing $S(t)$, we differentiate the system's ODE with respect to a parameter $\\theta_j$. Assuming sufficient smoothness to interchange the order of differentiation, we have:\n$$\n\\frac{\\partial}{\\partial \\theta_j} \\left( \\frac{d x_i}{d t} \\right) = \\frac{d}{d t} \\left( \\frac{\\partial x_i}{\\partial \\theta_j} \\right) = \\frac{d S_{ij}}{d t}\n$$\nApplying the chain rule to the right-hand side of the ODE gives:\n$$\n\\frac{\\partial f_i}{\\partial \\theta_j} = \\sum_{k=1}^{n} \\frac{\\partial f_i}{\\partial x_k} \\frac{\\partial x_k}{\\partial \\theta_j} + \\frac{\\partial f_i}{\\partial \\theta_j} \\bigg|_{x, t}\n$$\nIn this expression, the second term denotes the explicit partial derivative of $f_i$ with respect to $\\theta_j$. Let $J_x(t) = \\frac{\\partial f}{\\partial x}$ be the Jacobian matrix of the system with components $(J_x)_{ik} = \\frac{\\partial f_i}{\\partial x_k}$. The equation for the $j$-th column of the sensitivity matrix, $s_j(t) = \\frac{\\partial x(t)}{\\partial \\theta_j}$, becomes:\n$$\n\\frac{d s_j}{d t} = J_x(t) s_j(t) + \\frac{\\partial f}{\\partial \\theta_j}\n$$\nThis is a system of linear, time-varying ODEs for the sensitivities. The initial condition for this system is obtained by differentiating the initial state $x(t_0)$ with respect to $\\theta_j$:\n$$\ns_j(t_0) = \\frac{\\partial x(t_0)}{\\partial \\theta_j} = \\frac{\\partial x_0(\\theta)}{\\partial \\theta_j}\n$$\nIf the initial conditions are independent of the parameters, then $s_j(t_0) = 0$. This procedure is called forward sensitivity analysis because the sensitivity equations are integrated forward in time, typically alongside the original state equations.\n\n**2. Adjoint Sensitivity Analysis**\n\nConsider a scalar objective function $J$ that depends on the state evolution and parameters:\n$$\nJ = \\int_{t_0}^{T} g(x(t), \\theta, t) dt + h(x(T), \\theta)\n$$\nThe gradient of $J$ with respect to the parameters $\\theta$ is given by:\n$$\n\\frac{dJ}{d\\theta_j} = \\int_{t_0}^{T} \\left( \\frac{\\partial g}{\\partial x} \\frac{\\partial x}{\\partial \\theta_j} + \\frac{\\partial g}{\\partial \\theta_j} \\right) dt + \\frac{\\partial h}{\\partial x(T)} \\frac{\\partial x(T)}{\\partial \\theta_j} + \\frac{\\partial h}{\\partial \\theta_j}\n$$\nThis calculation requires the full time-course of the sensitivities $\\frac{\\partial x(t)}{\\partial \\theta_j} = s_j(t)$. For a large number of parameters $p$, this becomes computationally expensive as it requires solving $p$ systems of $n$ sensitivity ODEs.\n\nThe adjoint method provides a more efficient alternative. We introduce a vector of adjoint variables (or co-states) $\\lambda(t) \\in \\mathbb{R}^n$ that satisfy the following final-value problem:\n$$\n\\frac{d\\lambda}{dt} = -J_x(t)^T \\lambda(t) - \\left(\\frac{\\partial g}{\\partial x}\\right)^T, \\quad \\lambda(T) = \\left(\\frac{\\partial h}{\\partial x(T)}\\right)^T\n$$\nThis is a linear ODE system that must be integrated backward in time from $t=T$ to $t=t_0$.\n\nBy considering the time derivative of the product $\\lambda(t)^T s_j(t)$ and integrating, one can show that the gradient of the objective function can be expressed as:\n$$\n\\frac{dJ}{d\\theta_j} = \\int_{t_0}^{T} \\left( \\lambda(t)^T \\frac{\\partial f}{\\partial \\theta_j} + \\frac{\\partial g}{\\partial \\theta_j} \\right) dt + \\lambda(t_0)^T \\frac{\\partial x(t_0)}{\\partial \\theta_j} + \\frac{\\partial h}{\\partial \\theta_j}\n$$\nThe efficiency of the adjoint method arises because we only need to solve the $n$-dimensional state system forward in time, then the $n$-dimensional adjoint system backward in time. Once $x(t)$ and $\\lambda(t)$ are known, the gradient with respect to all $p$ parameters can be found by evaluating $p$ one-dimensional integrals (quadratures). This is significantly more efficient than the forward method when the number of parameters $p$ is much larger than the number of states $n$.\n\n**3. Application to the Two-Compartment PK Model: Forward Method**\n\nThe PK model is given by:\n$$\n\\frac{d x_1}{d t} = - k_a x_1, \\quad \\frac{d x_2}{d t} = k_a x_1 - k_e x_2\n$$\nwith $x_1(0) = D$ and $x_2(0) = 0$. The observable is $C(t) = x_2(t)/V$. We want to find $\\frac{\\partial C(T)}{\\partial k_e}$.\n\nFirst, we solve for the states $x_1(t)$ and $x_2(t)$. The solution for $x_1(t)$ is $x_1(t) = D \\exp(-k_a t)$. Substituting this into the equation for $x_2$ and solving the linear ODE (assuming $k_a \\neq k_e$) with $x_2(0)=0$ yields:\n$$\nx_2(t) = \\frac{k_a D}{k_e - k_a} \\left( \\exp(-k_a t) - \\exp(-k_e t) \\right)\n$$\nThe quantity of interest is the sensitivity of the concentration $C(T) = x_2(T)/V$ with respect to $k_e$. This is $\\frac{\\partial C(T)}{\\partial k_e} = \\frac{1}{V} \\frac{\\partial x_2(T)}{\\partial k_e}$. A direct way to find this within the forward sensitivity framework is to differentiate the analytical solution for $x_2(t)$ with respect to $k_e$ and evaluate it at $t=T$.\nUsing the quotient rule on the expression for $x_2(t)$:\n$$\n\\frac{\\partial x_2(t)}{\\partial k_e} = k_a D \\frac{\\partial}{\\partial k_e} \\left( \\frac{\\exp(-k_a t) - \\exp(-k_e t)}{k_e - k_a} \\right)\n$$\n$$\n\\frac{\\partial x_2(t)}{\\partial k_e} = k_a D \\left[ \\frac{(t \\exp(-k_e t))(k_e - k_a) - (\\exp(-k_a t) - \\exp(-k_e t))(1)}{(k_e - k_a)^2} \\right]\n$$\n$$\n\\frac{\\partial x_2(t)}{\\partial k_e} = \\frac{k_a D t \\exp(-k_e t)}{k_e - k_a} - \\frac{k_a D (\\exp(-k_a t) - \\exp(-k_e t))}{(k_e - k_a)^2}\n$$\nThe sensitivity of the concentration at time $T$ is therefore:\n$$\n\\frac{\\partial C(T)}{\\partial k_e} = \\frac{1}{V} \\frac{\\partial x_2(T)}{\\partial k_e} = \\frac{k_a D T \\exp(-k_e T)}{V(k_e-k_a)} - \\frac{k_a D (\\exp(-k_a T) - \\exp(-k_e T))}{V(k_e-k_a)^2}\n$$\n\n**4. Confirmation with the Adjoint Method**\n\nThe objective is a terminal one, $J = C(T) = x_2(T)/V$. So, $g=0$ and $h(x(T)) = x_2(T)/V$. The parameter is $\\theta_j = k_e$. The general adjoint sensitivity formula simplifies. Since the initial conditions are independent of $k_e$ and $h$ is not an explicit function of $k_e$, we have:\n$$\n\\frac{dJ}{dk_e} = \\int_{0}^{T} \\lambda(t)^T \\frac{\\partial f}{\\partial k_e} dt\n$$\nThe vector field is $f = \\begin{pmatrix} -k_a x_1 \\\\ k_a x_1 - k_e x_2 \\end{pmatrix}$, so $\\frac{\\partial f}{\\partial k_e} = \\begin{pmatrix} 0 \\\\ -x_2 \\end{pmatrix}$. The integrand becomes $-\\lambda_2(t) x_2(t)$.\n\nThe adjoint equations are:\n$\\frac{d\\lambda}{dt} = -J_x^T \\lambda$, where $J_x = \\begin{pmatrix} -k_a  0 \\\\ k_a  -k_e \\end{pmatrix}$.\n$$\n\\frac{d}{dt} \\begin{pmatrix} \\lambda_1 \\\\ \\lambda_2 \\end{pmatrix} = - \\begin{pmatrix} -k_a  k_a \\\\ 0  -k_e \\end{pmatrix} \\begin{pmatrix} \\lambda_1 \\\\ \\lambda_2 \\end{pmatrix} = \\begin{pmatrix} k_a \\lambda_1 - k_a \\lambda_2 \\\\ k_e \\lambda_2 \\end{pmatrix}\n$$\nThe terminal condition is $\\lambda(T) = (\\frac{\\partial h}{\\partial x(T)})^T = \\left(\\frac{\\partial}{\\partial x(T)} \\frac{x_2(T)}{V}\\right)^T = \\begin{pmatrix} 0  1/V \\end{pmatrix}^T$.\nSo, $\\lambda_1(T)=0$ and $\\lambda_2(T)=1/V$.\nSolving for $\\lambda_2$ backward from $T$: $\\frac{d\\lambda_2}{dt} = k_e \\lambda_2$ gives $\\lambda_2(t) = \\lambda_2(T) \\exp(k_e(t-T)) = \\frac{1}{V} \\exp(k_e(t-T))$.\n\nThe sensitivity is the integral:\n$$\n\\frac{dJ}{dk_e} = \\int_{0}^{T} -\\lambda_2(t) x_2(t) dt = \\int_{0}^{T} -\\frac{1}{V}\\exp(k_e(t-T)) \\left( \\frac{k_a D}{k_e-k_a} (\\exp(-k_a t) - \\exp(-k_e t)) \\right) dt\n$$\n$$\n= -\\frac{k_a D \\exp(-k_e T)}{V(k_e-k_a)} \\int_{0}^{T} \\exp(k_e t) (\\exp(-k_a t) - \\exp(-k_e t)) dt\n$$\n$$\n= -\\frac{k_a D \\exp(-k_e T)}{V(k_e-k_a)} \\int_{0}^{T} (\\exp((k_e-k_a)t) - 1) dt\n$$\n$$\n= -\\frac{k_a D \\exp(-k_e T)}{V(k_e-k_a)} \\left[ \\frac{\\exp((k_e-k_a)t)}{k_e-k_a} - t \\right]_0^T\n$$\n$$\n= -\\frac{k_a D \\exp(-k_e T)}{V(k_e-k_a)} \\left[ \\left(\\frac{\\exp((k_e-k_a)T)}{k_e-k_a} - T\\right) - \\left(\\frac{1}{k_e-k_a}\\right) \\right]\n$$\n$$\n= -\\frac{k_a D \\exp(-k_e T)}{V(k_e-k_a)^2} (\\exp(k_e T - k_a T) - 1) + \\frac{k_a D T \\exp(-k_e T)}{V(k_e-k_a)}\n$$\n$$\n= -\\frac{k_a D (\\exp(-k_a T) - \\exp(-k_e T))}{V(k_e-k_a)^2} + \\frac{k_a D T \\exp(-k_e T)}{V(k_e-k_a)}\n$$\nThis expression is identical to the one derived using the forward method, demonstrating that both methods yield the same result.\n\n**5. Numerical Evaluation**\n\nWe evaluate the expression for $\\frac{\\partial C(T)}{\\partial k_e}$ using the given values: $D = 100\\,\\text{mg}$, $k_a = 1\\,\\text{h}^{-1}$, $k_e = 0.2\\,\\text{h}^{-1}$, $V = 10\\,\\text{L}$, and $T = 4\\,\\text{h}$.\n$k_e - k_a = 0.2 - 1 = -0.8\\,\\text{h}^{-1}$.\n\nThe sensitivity is:\n$$\n\\frac{\\partial C(T)}{\\partial k_e} = \\frac{(1)(100)(4) \\exp(-0.2 \\times 4)}{10(-0.8)} - \\frac{(1)(100)(\\exp(-1 \\times 4) - \\exp(-0.2 \\times 4))}{10(-0.8)^2}\n$$\n$$\n= \\frac{400 \\exp(-0.8)}{-8} - \\frac{100(\\exp(-4) - \\exp(-0.8))}{10(0.64)}\n$$\n$$\n= -50 \\exp(-0.8) - \\frac{10}{0.64}(\\exp(-4) - \\exp(-0.8))\n$$\n$$\n= -50 \\exp(-0.8) - 15.625(\\exp(-4) - \\exp(-0.8))\n$$\n$$\n= (-50 + 15.625) \\exp(-0.8) - 15.625 \\exp(-4)\n$$\n$$\n= -34.375 \\exp(-0.8) - 15.625 \\exp(-4)\n$$\nUsing numerical values for the exponentials:\n$\\exp(-0.8) \\approx 0.44932896$\n$\\exp(-4) \\approx 0.01831564$\n$$\n\\frac{\\partial C(T)}{\\partial k_e} \\approx -34.375(0.44932896) - 15.625(0.01831564)\n$$\n$$\n\\approx -15.4455547 - 0.28618188\n$$\n$$\n\\approx -15.73173658\n$$\nRounding to $4$ significant figures, we get $-15.73$. The units are $(\\text{mg}/\\text{L}) / (\\text{h}^{-1}) = \\text{mg}\\cdot\\text{h}/\\text{L}$.", "answer": "$$\\boxed{-15.73}$$", "id": "4335020"}, {"introduction": "A robust computational patient model is more than just a set of equations; it is a full inference pipeline that learns from data. Ensuring this entire workflow—from the prior beliefs to the likelihood function to the numerical solver—is correctly implemented and statistically sound is a paramount challenge. This exercise [@problem_id:4334996] introduces Simulation-Based Calibration (SBC), a state-of-the-art method for validating a Bayesian pipeline by challenging it with synthetic data. You will implement the SBC procedure to probe for common pathologies, building confidence in the model's ability to produce reliable, calibrated inferences.", "problem": "You are asked to design and implement a Simulation-Based Calibration (SBC) procedure to detect inference pathologies in a computational patient model (digital twin) Bayesian pipeline. Consider a single-parameter mechanistic digital twin describing one-compartment pharmacokinetics. Let the patient-specific clearance parameter be $k$, with prior $p(k)$ and forward model $f(t;k)$ producing noisily observed concentration trajectories. The generative component comprises: draw $k \\sim \\text{LogNormal}(\\mu,\\sigma_k)$, sample observation times $\\{t_i\\}_{i=1}^{n}$, and simulate noisy observations $y_i = f(t_i;k) + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_y^2)$ independently. The mechanistic forward model is $f(t;k) = C_0 \\exp(-k t)$. The inference component uses Bayes’ rule to produce a posterior $p(k \\mid y)$ and a scalar SBC statistic defined as $u = F(k \\mid y)$, the posterior cumulative distribution function of $k$ evaluated at the ground-truth draw. Under a correctly specified Bayesian pipeline, the distribution of $u$ across independent simulations is expected to be uniform on $[0,1]$. Deviations indicate potential inference pathologies such as mis-specified noise or mis-specified forward models.\n\nStarting from Bayes’ rule and without assuming any closed-form posterior, implement an SBC procedure that:\n- Simulates $S$ independent pairs $(k^{(s)}, y^{(s)})$ from the generative prior-likelihood and computes $u^{(s)} = F(k^{(s)} \\mid y^{(s)})$ by numerically approximating the posterior $p(k \\mid y)$ on a grid over $k$.\n- Applies the one-sample Kolmogorov-Smirnov (KS) test for $H_0$: $u^{(s)} \\sim \\text{Uniform}(0,1)$ at significance level $\\alpha = 0.01$.\n- Reports a boolean for each test case indicating whether a pathology is detected (reject $H_0$).\n\nThe posterior must be computed numerically on a grid in $k$-space using the prior density $p(k)$ and the Gaussian likelihood implied by the forward model. You may approximate integrals by numerical quadrature. All algorithmic choices must be justified by fundamental principles in your solution.\n\nTest Suite. Use the following four test cases to exercise the SBC procedure:\n- Case A (happy path, correctly specified): $S = 500$, $n = 30$, $t_i$ equally spaced on $[0,8]$, $C_0 = 100$, $\\sigma_y = 5$, prior $\\log(k) \\sim \\mathcal{N}(\\mu=-2.0, \\sigma_k=0.3)$, inference uses the same $\\sigma_y$ and the exact forward model $f(t;k) = C_0 \\exp(-k t)$.\n- Case B (mis-specified noise, underdispersion): Same as Case A but the inference pipeline uses $\\tilde{\\sigma}_y = 1.0$ in the likelihood while the data are generated with $\\sigma_y = 5$.\n- Case C (mis-specified forward model): $S = 500$, $n = 31$, $t_i$ equally spaced on $[0,12]$, $C_0 = 100$, $\\sigma_y = 5$, prior $\\log(k) \\sim \\mathcal{N}(\\mu=-2.0, \\sigma_k=0.3)$, inference assumes the linearized forward model $\\tilde{f}(t;k) = C_0 (1 - k t)$ in the likelihood while the data are generated by $f(t;k) = C_0 \\exp(-k t)$.\n- Case D (boundary condition: sparse, high noise but correctly specified): $S = 500$, $n = 3$, $t_i \\in \\{0.0, 0.5, 1.0\\}$, $C_0 = 100$, $\\sigma_y = 20$, prior $\\log(k) \\sim \\mathcal{N}(\\mu=-2.0, \\sigma_k=0.3)$, inference uses the same $\\sigma_y$ and the exact forward model $f(t;k) = C_0 \\exp(-k t)$.\n\nAll simulations should use a fixed random seed to ensure reproducibility. Your program should produce a single line of output containing the results for Cases A–D in order as a comma-separated list enclosed in square brackets, for example, $[r_A,r_B,r_C,r_D]$, where each $r_{\\cdot}$ is a boolean indicating whether a pathology was detected in that case (reject $H_0$ at $\\alpha = 0.01$). No physical units are required in the output. Percentages must not be used; statistical significance must be stated in decimal form as $\\alpha = 0.01$.", "solution": "The user requires the design and implementation of a Simulation-Based Calibration (SBC) procedure to validate a Bayesian inference pipeline for a single-parameter pharmacokinetic model. The process involves simulating data from a known ground truth, running the inference pipeline on the simulated data, and then checking if the resulting posteriors are statistically consistent with the known truths used to generate them.\n\n### Theoretical Framework\n\nThe core of the problem lies in the application of Bayesian inference and its validation via SBC.\n\n**1. Bayesian Inference for the Pharmacokinetic Model**\nThe goal is to infer the patient-specific clearance parameter, $k$, given a set of concentration measurements, $Y = \\{y_i\\}_{i=1}^n$. According to Bayes' rule, the posterior distribution of $k$ given the data $Y$ is proportional to the product of the likelihood and the prior:\n$$\np(k \\mid Y) \\propto p(Y \\mid k) \\, p(k)\n$$\n\n**Generative Model Components:**\n- **Prior Distribution, $p(k)$:** The parameter $k$ is assumed to follow a Log-Normal distribution. This is specified by stating that its logarithm, $\\log(k)$, follows a Normal distribution: $\\log(k) \\sim \\mathcal{N}(\\mu, \\sigma_k^2)$. The problem provides $\\mu = -2.0$ and $\\sigma_k = 0.3$. It is interpreted from standard convention that $\\sigma_k$ represents the standard deviation of the Normal distribution for $\\log(k)$.\n- **Forward Model, $f(t;k)$:** The concentration of a substance at time $t$ is given by a one-compartment model equation: $f(t;k) = C_0 \\exp(-k t)$, where $C_0$ is the initial concentration.\n- **Likelihood, $p(Y \\mid k)$:** The observations $y_i$ are modeled as the output of the forward model plus independent, identically distributed Gaussian noise: $y_i = f(t_i; k) + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_y^2)$. The likelihood for the entire dataset $Y$ is the product of the probabilities of each observation:\n$$\np(Y \\mid k) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} \\exp\\left(-\\frac{(y_i - f(t_i; k))^2}{2\\sigma_y^2}\\right)\n$$\n\n**2. Simulation-Based Calibration (SBC)**\nSBC is a procedure to diagnose pathologies in a Bayesian inference implementation. The fundamental principle is that if the posterior distributions are correctly calculated from a well-specified model, they will be \"calibrated\". This calibration is checked by examining the distribution of rank statistics.\n\nFor a single simulation instance $s$:\n1. A \"ground-truth\" parameter $k^{(s)}$ is drawn from the prior distribution: $k^{(s)} \\sim p(k)$.\n2. A synthetic dataset $Y^{(s)}$ is generated using this parameter: $Y^{(s)} \\sim p(Y \\mid k^{(s)})$.\n3. The inference pipeline is used to compute the posterior distribution $p(k \\mid Y^{(s)})$.\n4. The SBC statistic, $u^{(s)}$, is calculated. This statistic is the value of the posterior cumulative distribution function (CDF) evaluated at the ground-truth parameter $k^{(s)}$:\n   $$\n   u^{(s)} = F(k^{(s)} \\mid Y^{(s)}) = \\int_{0}^{k^{(s)}} p(k' \\mid Y^{(s)}) \\, dk'\n   $$\nIf the entire pipeline (prior, likelihood, and posterior calculation) is correctly specified and implemented, the set of rank statistics $\\{u^{(s)}\\}_{s=1}^S$ collected over many simulations will be uniformly distributed on the interval $[0,1]$.\n\n**3. Hypothesis Testing**\nTo formally check for uniformity, we state a null hypothesis:\n- $H_0$: The SBC statistics $\\{u^{(s)}\\}$ are drawn from a $\\text{Uniform}(0,1)$ distribution.\nThis hypothesis is tested using the one-sample Kolmogorov-Smirnov (KS) test. If the p-value returned by the KS test is less than the chosen significance level $\\alpha = 0.01$, we reject $H_0$ and conclude that there is a pathology in the inference pipeline.\n\n### Numerical Implementation Strategy\n\nSince the posterior distribution $p(k \\mid Y)$ does not have a closed-form analytical solution, it must be approximated numerically. The problem specifies a grid-based approach.\n\n**1. Posterior Computation on a Grid**\n- A discrete grid of $N$ points is defined over the plausible range of $k$: $\\{k_j\\}_{j=1}^N$.\n- For each grid point $k_j$, the unnormalized posterior is computed. To maintain numerical stability, we work with log-probabilities:\n  $$\n  \\log p_{\\text{unnorm}}(k_j) = \\log p(Y \\mid k_j) + \\log p(k_j)\n  $$\n  - The log-likelihood (ignoring constants) is:\n    $$\n    \\log p(Y \\mid k_j) \\propto -\\frac{1}{2\\sigma_{y, \\text{inf}}^2} \\sum_{i=1}^{n} (y_i - f_{\\text{inf}}(t_i; k_j))^2\n    $$\n    Here, $\\sigma_{y, \\text{inf}}$ and $f_{\\text{inf}}$ are the noise standard deviation and forward model assumed by the *inference* pipeline, which may differ from the generative model.\n  - The log-prior for the Log-Normal distribution (ignoring constants) is:\n    $$\n    \\log p(k_j) \\propto -\\log(k_j) - \\frac{(\\log(k_j) - \\mu)^2}{2\\sigma_k^2}\n    $$\n- The log-posterior values are converted back to a linear scale by exponentiation, after shifting them by their maximum value to prevent numerical underflow: $p_{\\text{unnorm}}(k_j) = \\exp(\\log p_{\\text{unnorm}}(k_j) - \\max(\\{\\log p_{\\text{unnorm}}\\}))$.\n- The unnormalized posterior is then normalized to represent a probability density function. This is done by dividing by its total integral, approximated using the trapezoidal rule over the grid:\n  $$\n  p(k_j \\mid Y) = \\frac{p_{\\text{unnorm}}(k_j)}{\\int p_{\\text{unnorm}}(k) \\, dk} \\approx \\frac{p_{\\text{unnorm}}(k_j)}{\\text{trapezoid}(p_{\\text{unnorm}}, \\{k_j\\})}\n  $$\n\n**2. CDF and SBC Statistic Calculation**\n- The posterior CDF, $F(k \\mid Y)$, is computed by performing a cumulative integration of the normalized posterior density $p(k \\mid Y)$ using the cumulative trapezoidal rule.\n- The SBC statistic $u^{(s)} = F(k^{(s)} \\mid Y^{(s)})$ is found by linearly interpolating the computed CDF values at the location of the ground-truth parameter $k^{(s)}$.\n\n### Algorithmic Procedure\n\nThe overall algorithm for each test case is as follows:\n\n1.  Set a fixed random seed for reproducibility.\n2.  Define the parameters for the specific test case: simulation count $S$, sample size $n$, time points $\\{t_i\\}$, initial concentration $C_0$, data-generating parameters $(\\sigma_y, f)$, and inference parameters $(\\tilde{\\sigma}_y, \\tilde{f})$.\n3.  Initialize an empty list, `U_stats`, to store the SBC statistics.\n4.  Execute a loop $S$ times:\n    a.  Draw a true parameter value $k_{\\text{true}}$ from the prior $k \\sim \\text{LogNormal}(\\mu, \\sigma_k)$.\n    b.  Generate a synthetic dataset $\\{y_i\\}$ using the generative forward model $f(t;k_{\\text{true}})$ and noise $\\sigma_y$.\n    c.  Compute the numerical posterior distribution $p(k \\mid \\{y_i\\})$ on a predefined grid for $k$, using the inference forward model $\\tilde{f}$ and noise parameter $\\tilde{\\sigma}_y$.\n    d.  From the posterior density, compute the posterior CDF.\n    e.  Interpolate the CDF at $k_{\\text{true}}$ to obtain the SBC statistic $u$. Append $u$ to `U_stats`.\n5.  After the loop, perform a one-sample KS test on the collected `U_stats` against a `Uniform(0,1)` distribution.\n6.  The result for the test case is `True` (pathology detected) if the test's p-value is less than $\\alpha = 0.01$, and `False` otherwise.\n7.  Repeat for all four test cases and report the results.\n\nThis procedure rigorously tests the alignment between the assumed statistical model and the inference machinery. Deviations from uniformity in the SBC statistics, as detected by the KS test, provide a clear signal of model misspecification or implementation error.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest\nfrom scipy.integrate import cumulative_trapezoid, trapezoid\n\ndef solve():\n    \"\"\"\n    Main function to run the Simulation-Based Calibration (SBC) procedure\n    for all specified test cases and print the results.\n    \"\"\"\n    # Set a global random seed for reproducibility of the entire simulation.\n    RANDOM_SEED = 42\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    # Define a common grid for the parameter k for all computations.\n    # The prior log(k) ~ N(-2, 0.3^2) suggests k is concentrated around exp(-2) ~ 0.135.\n    # A grid covering several standard deviations on the log scale is appropriate.\n    # log(k) in [-2 - 5*0.3, -2 + 5*0.3] = [-3.5, -0.5] - k in [0.03, 0.61]\n    k_grid = np.linspace(0.01, 0.7, 1000)\n\n    # Common prior parameters for all cases\n    LOGK_MU = -2.0\n    LOGK_SIGMA = 0.3\n\n    # Define the test cases as a list of dictionaries.\n    test_cases_params = [\n        { # Case A: Correctly specified model (Happy Path)\n            \"S\": 500, \"n\": 30, \"t_range\": (0, 8),\n            \"C0\": 100.0, \"sigma_y_gen\": 5.0,\n            \"sigma_y_inf\": 5.0, \"f_inf_type\": \"exponential\"\n        },\n        { # Case B: Mis-specified noise (underdispersion in inference)\n            \"S\": 500, \"n\": 30, \"t_range\": (0, 8),\n            \"C0\": 100.0, \"sigma_y_gen\": 5.0,\n            \"sigma_y_inf\": 1.0, \"f_inf_type\": \"exponential\"\n        },\n        { # Case C: Mis-specified forward model\n            \"S\": 500, \"n\": 31, \"t_range\": (0, 12),\n            \"C0\": 100.0, \"sigma_y_gen\": 5.0,\n            \"sigma_y_inf\": 5.0, \"f_inf_type\": \"linear\"\n        },\n        { # Case D: Sparse, high-noise data (correctly specified)\n            \"S\": 500, \"n\": 3, \"t_values\": np.array([0.0, 0.5, 1.0]),\n            \"C0\": 100.0, \"sigma_y_gen\": 20.0,\n            \"sigma_y_inf\": 20.0, \"f_inf_type\": \"exponential\"\n        }\n    ]\n\n    results = []\n    alpha = 0.01\n\n    for params in test_cases_params:\n        # Generate time points based on case parameters\n        if \"t_range\" in params:\n            t_points = np.linspace(params[\"t_range\"][0], params[\"t_range\"][1], params[\"n\"])\n        else:\n            t_points = params[\"t_values\"]\n\n        # Run the SBC procedure for the current case\n        pathology_detected = run_sbc_for_case(\n            S=params[\"S\"],\n            t_points=t_points,\n            C0=params[\"C0\"],\n            sigma_y_gen=params[\"sigma_y_gen\"],\n            logk_mu=LOGK_MU,\n            logk_sigma=LOGK_SIGMA,\n            sigma_y_inf=params[\"sigma_y_inf\"],\n            f_inf_type=params[\"f_inf_type\"],\n            rng=rng,\n            k_grid=k_grid,\n            alpha=alpha\n        )\n        results.append(pathology_detected)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_sbc_for_case(S, t_points, C0, sigma_y_gen,\n                     logk_mu, logk_sigma,\n                     sigma_y_inf, f_inf_type,\n                     rng, k_grid, alpha):\n    \"\"\"\n    Runs the full SBC procedure for a single test case configuration.\n    Returns True if a pathology is detected, False otherwise.\n    \"\"\"\n    u_statistics = []\n    for _ in range(S):\n        # 1. Draw true parameter from prior: log(k) ~ N(mu, sigma^2)\n        k_true = np.exp(rng.normal(loc=logk_mu, scale=logk_sigma))\n\n        # 2. Generate synthetic data from the generative model\n        y_true = C0 * np.exp(-k_true * t_points)\n        y_observed = y_true + rng.normal(loc=0.0, scale=sigma_y_gen, size=t_points.shape)\n        \n        # 3. Compute SBC statistic (posterior CDF at k_true)\n        u_stat = compute_sbc_statistic(\n            k_true=k_true,\n            y_observed=y_observed,\n            t_points=t_points,\n            C0=C0,\n            logk_mu=logk_mu,\n            logk_sigma=logk_sigma,\n            sigma_y_inf=sigma_y_inf,\n            f_inf_type=f_inf_type,\n            k_grid=k_grid\n        )\n        u_statistics.append(u_stat)\n\n    # 4. Perform Kolmogorov-Smirnov test for uniformity\n    ks_statistic, p_value = kstest(np.array(u_statistics), 'uniform')\n    \n    # 5. Report pathology if H0 (uniformity) is rejected\n    return p_value  alpha\n\ndef compute_sbc_statistic(k_true, y_observed, t_points, C0,\n                        logk_mu, logk_sigma,\n                        sigma_y_inf, f_inf_type, k_grid):\n    \"\"\"\n    Computes the posterior distribution on a grid and returns the SBC statistic u.\n    \"\"\"\n    # Define inference forward model\n    if f_inf_type == \"exponential\":\n        f_inf = C0 * np.exp(-k_grid[:, np.newaxis] * t_points)\n    elif f_inf_type == \"linear\":\n        f_inf = C0 * (1 - k_grid[:, np.newaxis] * t_points)\n    else:\n        raise ValueError(\"Unknown inference model type\")\n\n    # Calculate log-likelihood for each k in the grid\n    # Sum of squared errors for each k\n    sse = np.sum((y_observed - f_inf)**2, axis=1)\n    log_likelihood = -sse / (2 * sigma_y_inf**2)\n\n    # Calculate log-prior for each k in the grid\n    # Log-Normal PDF for k where log(k) is Normal(mu, sigma)\n    log_prior = -np.log(k_grid) - (np.log(k_grid) - logk_mu)**2 / (2 * logk_sigma**2)\n    \n    # Calculate unnormalized log-posterior\n    log_posterior = log_likelihood + log_prior\n    \n    # Normalize posterior for numerical stability\n    log_posterior -= np.max(log_posterior)\n    posterior_unnorm = np.exp(log_posterior)\n    \n    # Normalize to a probability density function using trapezoidal rule\n    total_integral = trapezoid(posterior_unnorm, k_grid)\n    if total_integral = 0:\n        # If integral is zero (underflow or bad grid), cannot compute CDF.\n        # This can happen if the true k is far from the grid.\n        # Return a value that indicates failure, e.g., NaN or a random value.\n        # A random value is better to not bias the KS test.\n        return np.random.uniform(0, 1)\n        \n    posterior_pdf = posterior_unnorm / total_integral\n\n    # Compute the cumulative distribution function (CDF)\n    # cumulative_trapezoid returns N-1 values, so we prepend a 0 for the first grid point.\n    cdf_values = np.concatenate(([0.0], cumulative_trapezoid(posterior_pdf, k_grid)))\n\n    # Interpolate the CDF to find the value at k_true\n    u_stat = np.interp(k_true, k_grid, cdf_values)\n\n    # Clamp the result to [0, 1] to handle potential floating point inaccuracies\n    return np.clip(u_stat, 0.0, 1.0)\n\n\n# Execute the main simulation\nsolve()\n```", "id": "4334996"}]}