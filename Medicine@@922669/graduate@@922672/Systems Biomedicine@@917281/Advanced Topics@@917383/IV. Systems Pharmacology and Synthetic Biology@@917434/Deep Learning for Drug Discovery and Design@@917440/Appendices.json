{"hands_on_practices": [{"introduction": "Before any deep learning model can be applied, a molecule's structure must be translated into a numerical format. This practice explores the Extended-Connectivity Fingerprint (ECFP), a foundational method for encoding molecular structure into a feature vector. By manually computing an ECFP, you will gain a concrete understanding of how local atomic environments are systematically aggregated into a global molecular representation, a core concept that also underpins how Graph Neural Networks operate [@problem_id:4333002].", "problem": "A molecule is represented as an undirected labeled graph with an adjacency matrix $\\mathbf{A} \\in \\{0,1\\}^{6 \\times 6}$ and an atom feature matrix $\\mathbf{X} \\in \\mathbb{Z}^{6 \\times 5}$. The adjacency matrix encodes heavy-atom connectivity (hydrogen atoms are not explicit nodes), and the atom features use columns in the order: atomic number $Z$, heavy-atom degree $d$, formal charge $q$, implicit hydrogen count $h$, and aromaticity flag $a \\in \\{0,1\\}$. The data are:\n$$\n\\mathbf{A} \\;=\\;\n\\begin{pmatrix}\n0 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 0\n\\end{pmatrix},\n\\qquad\n\\mathbf{X} \\;=\\;\n\\begin{pmatrix}\n6 & 1 & 0 & 3 & 0 \\\\\n6 & 2 & 0 & 2 & 0 \\\\\n6 & 3 & 0 & 1 & 0 \\\\\n7 & 1 & 0 & 2 & 0 \\\\\n6 & 2 & 0 & 0 & 0 \\\\\n8 & 1 & 0 & 0 & 0\n\\end{pmatrix}.\n$$\nYou will compute the Extended-Connectivity Fingerprint (ECFP, also called the Morgan fingerprint) to radius $2$ under the following explicit, deterministic scheme that is compatible with standard practice while remaining tractable by hand:\n\n1. Initial atom invariants. For each atom $v \\in \\{1,\\dots,6\\}$, define the initial identifier\n$$\nI^{(0)}(v) \\;=\\; Z_v \\cdot 10^{4} \\;+\\; d_v \\cdot 10^{3} \\;+\\; q_v \\cdot 10^{2} \\;+\\; h_v \\cdot 10 \\;+\\; a_v.\n$$\n\n2. Neighborhood expansion. For iteration $t \\in \\{1,2\\}$, define\n$$\nI^{(t)}(v) \\;=\\; I^{(t-1)}(v) \\;+\\; \\sum_{u \\in \\mathcal{N}(v)} I^{(t-1)}(u),\n$$\nwhere $\\mathcal{N}(v)$ is the set of neighbors of $v$ in the graph defined by $\\mathbf{A}$.\n\n3. Bit-vector hashing. Let $m = 16$ be the fixed bit-vector length. Map any identifier $y \\in \\mathbb{Z}$ to a bit position by\n$$\nh(y) \\;=\\; \\left( 7 y \\;+\\; 13 \\right) \\bmod 16.\n$$\nThe radius-$2$ Morgan fingerprint is the set of bit positions $\\{\\, h(y) \\mid y \\in \\mathcal{S} \\,\\}$, where\n$$\n\\mathcal{S} \\;=\\; \\big\\{ I^{(0)}(v), I^{(1)}(v), I^{(2)}(v) \\;\\big|\\; v \\in \\{1,\\dots,6\\} \\big\\}.\n$$\n\nIn fixed-length fingerprints, different identifiers $y$ can hash to the same bit index, creating collisions. Define the collision count as\n$$\nC \\;=\\; |\\mathcal{S}| \\;-\\; \\big|\\{\\, h(y) \\mid y \\in \\mathcal{S} \\,\\}\\big|.\n$$\n\nStarting from the provided $\\mathbf{A}$ and $\\mathbf{X}$, and using only the definitions above, compute $C$. Express your final answer as an integer with no units. No rounding is required.", "solution": "**Step 1: Compute Initial Atom Invariants, $I^{(0)}(v)$**\n\nThe initial identifier for each atom $v$ is given by the formula:\n$$I^{(0)}(v) = Z_v \\cdot 10^4 + d_v \\cdot 10^3 + q_v \\cdot 10^2 + h_v \\cdot 10 + a_v$$\nUsing the rows from the given feature matrix $\\mathbf{X}$:\n- Atom 1 ($Z=6, d=1, q=0, h=3, a=0$):\n$I^{(0)}(1) = 6 \\cdot 10000 + 1 \\cdot 1000 + 0 \\cdot 100 + 3 \\cdot 10 + 0 = 61030$\n- Atom 2 ($Z=6, d=2, q=0, h=2, a=0$):\n$I^{(0)}(2) = 6 \\cdot 10000 + 2 \\cdot 1000 + 0 \\cdot 100 + 2 \\cdot 10 + 0 = 62020$\n- Atom 3 ($Z=6, d=3, q=0, h=1, a=0$):\n$I^{(0)}(3) = 6 \\cdot 10000 + 3 \\cdot 1000 + 0 \\cdot 100 + 1 \\cdot 10 + 0 = 63010$\n- Atom 4 ($Z=7, d=1, q=0, h=2, a=0$):\n$I^{(0)}(4) = 7 \\cdot 10000 + 1 \\cdot 1000 + 0 \\cdot 100 + 2 \\cdot 10 + 0 = 71020$\n- Atom 5 ($Z=6, d=2, q=0, h=0, a=0$):\n$I^{(0)}(5) = 6 \\cdot 10000 + 2 \\cdot 1000 + 0 \\cdot 100 + 0 \\cdot 10 + 0 = 62000$\n- Atom 6 ($Z=8, d=1, q=0, h=0, a=0$):\n$I^{(0)}(6) = 8 \\cdot 10000 + 1 \\cdot 1000 + 0 \\cdot 100 + 0 \\cdot 10 + 0 = 81000$\n\nThe set of initial identifiers is $\\{61030, 62020, 63010, 71020, 62000, 81000\\}$.\n\n**Step 2: Compute First Iteration Invariants, $I^{(1)}(v)$**\n\nThe update rule is $I^{(t)}(v) = I^{(t-1)}(v) + \\sum_{u \\in \\mathcal{N}(v)} I^{(t-1)}(u)$. First, we determine the neighbor sets $\\mathcal{N}(v)$ from the adjacency matrix $\\mathbf{A}$:\n- $\\mathcal{N}(1) = \\{2\\}$\n- $\\mathcal{N}(2) = \\{1, 3\\}$\n- $\\mathcal{N}(3) = \\{2, 4, 5\\}$\n- $\\mathcal{N}(4) = \\{3\\}$\n- $\\mathcal{N}(5) = \\{3, 6\\}$\n- $\\mathcal{N}(6) = \\{5\\}$\n\nNow we compute the $I^{(1)}(v)$ values for $v \\in \\{1,\\dots,6\\}$:\n- $I^{(1)}(1) = I^{(0)}(1) + I^{(0)}(2) = 61030 + 62020 = 123050$\n- $I^{(1)}(2) = I^{(0)}(2) + I^{(0)}(1) + I^{(0)}(3) = 62020 + 61030 + 63010 = 186060$\n- $I^{(1)}(3) = I^{(0)}(3) + I^{(0)}(2) + I^{(0)}(4) + I^{(0)}(5) = 63010 + 62020 + 71020 + 62000 = 258050$\n- $I^{(1)}(4) = I^{(0)}(4) + I^{(0)}(3) = 71020 + 63010 = 134030$\n- $I^{(1)}(5) = I^{(0)}(5) + I^{(0)}(3) + I^{(0)}(6) = 62000 + 63010 + 81000 = 206010$\n- $I^{(1)}(6) = I^{(0)}(6) + I^{(0)}(5) = 81000 + 62000 = 143000$\n\nThe set of first iteration identifiers is $\\{123050, 186060, 258050, 134030, 206010, 143000\\}$.\n\n**Step 3: Compute Second Iteration Invariants, $I^{(2)}(v)$**\n\nWe apply the update rule again, using the $I^{(1)}(v)$ values:\n- $I^{(2)}(1) = I^{(1)}(1) + I^{(1)}(2) = 123050 + 186060 = 309110$\n- $I^{(2)}(2) = I^{(1)}(2) + I^{(1)}(1) + I^{(1)}(3) = 186060 + 123050 + 258050 = 567160$\n- $I^{(2)}(3) = I^{(1)}(3) + I^{(1)}(2) + I^{(1)}(4) + I^{(1)}(5) = 258050 + 186060 + 134030 + 206010 = 784150$\n- $I^{(2)}(4) = I^{(1)}(4) + I^{(1)}(3) = 134030 + 258050 = 392080$\n- $I^{(2)}(5) = I^{(1)}(5) + I^{(1)}(3) + I^{(1)}(6) = 206010 + 258050 + 143000 = 607060$\n- $I^{(2)}(6) = I^{(1)}(6) + I^{(1)}(5) = 143000 + 206010 = 349010$\n\nThe set of second iteration identifiers is $\\{309110, 567160, 784150, 392080, 607060, 349010\\}$.\n\n**Step 4: Construct the Set of All Identifiers, $\\mathcal{S}$**\n\nThe set $\\mathcal{S}$ is the union of all identifiers generated:\n$\\mathcal{S} = \\{I^{(0)}(v), I^{(1)}(v), I^{(2)}(v) \\mid v \\in \\{1,\\dots,6\\}\\}$.\n- $I^{(0)}$ set: $\\{61030, 62020, 63010, 71020, 62000, 81000\\}$\n- $I^{(1)}$ set: $\\{123050, 186060, 258050, 134030, 206010, 143000\\}$\n- $I^{(2)}$ set: $\\{309110, 567160, 784150, 392080, 607060, 349010\\}$\nAll $6 \\times 3 = 18$ identifiers are unique. Thus, the cardinality of $\\mathcal{S}$ is $|\\mathcal{S}|=18$.\n\n**Step 5: Hash All Identifiers and Determine the Set of Unique Hashes**\n\nThe hash function is $h(y) = (7y + 13) \\pmod{16}$. We apply this to each of the $18$ identifiers in $\\mathcal{S}$. For efficiency, we use the property $h(y) = (7(y \\pmod{16}) + 13) \\pmod{16}$.\n\nHashes for $I^{(0)}$ identifiers:\n- $h(61030) = (7(61030 \\bmod 16) + 13) \\bmod 16 = (7 \\cdot 6 + 13) \\bmod 16 = 55 \\bmod 16 = 7$\n- $h(62020) = (7(62020 \\bmod 16) + 13) \\bmod 16 = (7 \\cdot 4 + 13) \\bmod 16 = 41 \\bmod 16 = 9$\n- $h(63010) = (7(63010 \\bmod 16) + 13) \\bmod 16 = (7 \\cdot 2 + 13) \\bmod 16 = 27 \\bmod 16 = 11$\n- $h(71020) = (7(71020 \\bmod 16) + 13) \\bmod 16 = (7 \\cdot 12 + 13) \\bmod 16 = 97 \\bmod 16 = 1$\n- $h(62000) = (7(62000 \\bmod 16) + 13) \\bmod 16 = (7 \\cdot 0 + 13) \\bmod 16 = 13 \\bmod 16 = 13$\n- $h(81000) = (7(81000 \\bmod 16) + 13) \\bmod 16 = (7 \\cdot 8 + 13) \\bmod 16 = 69 \\bmod 16 = 5$\n\nHashes for $I^{(1)}$ identifiers:\n- $h(123050) = (7 \\cdot 10 + 13) \\bmod 16 = 83 \\bmod 16 = 3$\n- $h(186060) = (7 \\cdot 12 + 13) \\bmod 16 = 97 \\bmod 16 = 1$\n- $h(258050) = (7 \\cdot 2 + 13) \\bmod 16 = 27 \\bmod 16 = 11$\n- $h(134030) = (7 \\cdot 14 + 13) \\bmod 16 = 111 \\bmod 16 = 15$\n- $h(206010) = (7 \\cdot 10 + 13) \\bmod 16 = 83 \\bmod 16 = 3$\n- $h(143000) = (7 \\cdot 8 + 13) \\bmod 16 = 69 \\bmod 16 = 5$\n\nHashes for $I^{(2)}$ identifiers:\n- $h(309110) = (7 \\cdot 6 + 13) \\bmod 16 = 55 \\bmod 16 = 7$\n- $h(567160) = (7 \\cdot 8 + 13) \\bmod 16 = 69 \\bmod 16 = 5$\n- $h(784150) = (7 \\cdot 6 + 13) \\bmod 16 = 55 \\bmod 16 = 7$\n- $h(392080) = (7 \\cdot 0 + 13) \\bmod 16 = 13 \\bmod 16 = 13$\n- $h(607060) = (7 \\cdot 4 + 13) \\bmod 16 = 41 \\bmod 16 = 9$\n- $h(349010) = (7 \\cdot 2 + 13) \\bmod 16 = 27 \\bmod 16 = 11$\n\nThe set of all $18$ computed hash values is $\\{7, 9, 11, 1, 13, 5, 3, 1, 11, 15, 3, 5, 7, 5, 7, 13, 9, 11\\}$.\nThe set of unique hash values is the union of these computed values:\n$\\{\\, h(y) \\mid y \\in \\mathcal{S} \\,\\} = \\{1, 3, 5, 7, 9, 11, 13, 15\\}$.\nThe number of unique hash values is $|\\{\\, h(y) \\mid y \\in \\mathcal{S} \\,\\}| = 8$.\n\n**Step 6: Compute the Collision Count, $C$**\n\nThe collision count is defined as $C = |\\mathcal{S}| - |\\{\\, h(y) \\mid y \\in \\mathcal{S} \\,\\}|$.\nSubstituting the calculated values:\n$$C = 18 - 8 = 10$$\nThe collision count is $10$.", "answer": "$$\\boxed{10}$$", "id": "4333002"}, {"introduction": "Modern deep learning models for drug discovery often learn from vast unlabeled chemical datasets through self-supervised objectives. This exercise delves into the mechanics of contrastive learning, a powerful paradigm for learning meaningful molecular embeddings. You will calculate the contrastive loss for a small batch of molecules, providing direct insight into how the model is trained to pull similar molecules together and push dissimilar ones apart in the embedding space [@problem_id:4332990].", "problem": "A research team in systems biomedicine is training a self-supervised representation learning model to embed molecules for downstream drug discovery tasks. Each molecule is represented as a graph, and the model is a Graph Neural Network (GNN). During training, the team uses two stochastic augmentations per molecule to form positive pairs, with all other views in the mini-batch serving as implicit negatives. Embeddings are $\\ell_{2}$-normalized so that the dot product equals the cosine similarity. The training objective follows the maximum likelihood principle: for each anchor view $i$, the model defines a Gibbs distribution over all other views $j \\neq i$ using energies $E_{ij} = - s_{ij}/\\tau$, where $s_{ij}$ is the similarity and $\\tau$ is a temperature. The probability assigned to the correct match $j^{+}$ (the other augmentation of the same molecule) is obtained by normalizing the exponentiated negative energies, and the per-anchor loss is the negative log-likelihood of $j^{+}$. The batch loss is the average of the per-anchor losses over all views.\n\nConsider a mini-batch of $3$ distinct molecules, each with $2$ augmentations, producing a total of $6$ views. Their $\\ell_{2}$-normalized embeddings in $\\mathbb{R}^{2}$ are:\n- Molecule $A$: $a_{1} = (1, 0)$ and $a_{2} = (0.8, 0.6)$.\n- Molecule $B$: $b_{1} = (0, 1)$ and $b_{2} = (-0.6, 0.8)$.\n- Molecule $C$: $c_{1} = (-1, 0)$ and $c_{2} = (-0.8, -0.6)$.\n\nThe temperature is $\\tau = 0.5$. For each anchor (each of the $6$ views), the true positive is its paired augmentation from the same molecule, and all other $5$ views form the candidate set that the Gibbs distribution is normalized over. Using the base definitions above (Gibbs distribution with energy $E_{ij} = -s_{ij}/\\tau$, negative log-likelihood loss per anchor, and averaging across anchors), compute the batch contrastive loss for this mini-batch. Express the final batch loss as a dimensionless quantity and round your answer to four significant figures.\n\nAdditionally, based on the same foundational setup, explain qualitatively and quantitatively how changing the temperature $\\tau$ influences the separation between positive and negative embeddings in the learned representation space. Your explanation must be grounded in first principles (probability normalization, cross-entropy, and gradient behavior for softmax with temperature) without appealing to undocumented shortcut formulas.", "solution": "### **Part 1: Computation of the Batch Contrastive Loss**\n\nLet the set of $6$ views be $V = \\{v_1, v_2, v_3, v_4, v_5, v_6\\}$, corresponding to $\\{a_1, a_2, b_1, b_2, c_1, c_2\\}$, respectively. The temperature is $\\tau = 0.5$, so the scaling factor is $1/\\tau = 2$.\n\nFirst, we compute the pairwise similarity matrix $S$ where $S_{ij} = v_i \\cdot v_j$.\n- **Positive Pair Similarities:**\n  $s_{a_1, a_2} = (1, 0) \\cdot (0.8, 0.6) = 0.8$\n  $s_{b_1, b_2} = (0, 1) \\cdot (-0.6, 0.8) = 0.8$\n  $s_{c_1, c_2} = (-1, 0) \\cdot (-0.8, -0.6) = 0.8$\n  The similarity for every positive pair is $0.8$.\n\n- **Negative Pair Similarities:** We compute all dot products between views from different molecules.\n  $s_{a_1, b_1} = (1, 0) \\cdot (0, 1) = 0$\n  $s_{a_1, b_2} = (1, 0) \\cdot (-0.6, 0.8) = -0.6$\n  $s_{a_1, c_1} = (1, 0) \\cdot (-1, 0) = -1$\n  $s_{a_1, c_2} = (1, 0) \\cdot (-0.8, -0.6) = -0.8$\n  $s_{a_2, b_1} = (0.8, 0.6) \\cdot (0, 1) = 0.6$\n  $s_{a_2, b_2} = (0.8, 0.6) \\cdot (-0.6, 0.8) = -0.48 + 0.48 = 0$\n  $s_{a_2, c_1} = (0.8, 0.6) \\cdot (-1, 0) = -0.8$\n  $s_{a_2, c_2} = (0.8, 0.6) \\cdot (-0.8, -0.6) = -0.64 - 0.36 = -1$\n  $s_{b_1, c_1} = (0, 1) \\cdot (-1, 0) = 0$\n  $s_{b_1, c_2} = (0, 1) \\cdot (-0.8, -0.6) = -0.6$\n  $s_{b_2, c_1} = (-0.6, 0.8) \\cdot (-1, 0) = 0.6$\n  $s_{b_2, c_2} = (-0.6, 0.8) \\cdot (-0.8, -0.6) = 0.48 - 0.48 = 0$\n\nThe per-anchor loss for an anchor $i$ with positive pair partner $j^{+}$ is:\n$$ \\mathcal{L}_i = -\\ln \\left( \\frac{\\exp(s_{ij^{+}}/\\tau)}{\\sum_{k \\neq i} \\exp(s_{ik}/\\tau)} \\right) = -s_{ij^{+}}/\\tau + \\ln \\left( \\sum_{k \\neq i} \\exp(s_{ik}/\\tau) \\right) $$\n\nWe calculate this loss for each of the $6$ anchors. Let $s_{\\text{pos}} = 0.8$ be the positive similarity. Then $s_{\\text{pos}}/\\tau = 0.8 / 0.5 = 1.6$.\n\n- **Anchor $a_1$**: The other views are $\\{a_2, b_1, b_2, c_1, c_2\\}$. The similarities are $\\{0.8, 0, -0.6, -1, -0.8\\}$. The denominator sum is:\n  $D_{a_1} = \\exp(1.6) + \\exp(0/0.5) + \\exp(-0.6/0.5) + \\exp(-1/0.5) + \\exp(-0.8/0.5) = \\exp(1.6) + \\exp(0) + \\exp(-1.2) + \\exp(-2) + \\exp(-1.6)$.\n  $\\mathcal{L}_{a_1} = -\\ln(\\exp(1.6)/D_{a_1}) = \\ln(D_{a_1}) - 1.6$.\n\n- **Anchor $a_2$**: The other views are $\\{a_1, b_1, b_2, c_1, c_2\\}$. The similarities are $\\{0.8, 0.6, 0, -0.8, -1\\}$. The denominator sum is:\n  $D_{a_2} = \\exp(1.6) + \\exp(0.6/0.5) + \\exp(0/0.5) + \\exp(-0.8/0.5) + \\exp(-1/0.5) = \\exp(1.6) + \\exp(1.2) + \\exp(0) + \\exp(-1.6) + \\exp(-2)$.\n  $\\mathcal{L}_{a_2} = \\ln(D_{a_2}) - 1.6$.\n\nDue to the symmetry in the embeddings ($c_1 = -a_1$, $c_2 = -a_2$), we find that the set of similarities for anchors $c_1$ and $c_2$ are related to those for $a_2$ and $a_1$.\n$\\mathcal{L}_{c_1} = \\mathcal{L}_{a_2}$ and $\\mathcal{L}_{c_2} = \\mathcal{L}_{a_1}$.\n\n- **Anchor $b_1$**: The other views are $\\{b_2, a_1, a_2, c_1, c_2\\}$. The similarities are $\\{0.8, 0, 0.6, 0, -0.6\\}$. The denominator sum is:\n  $D_{b_1} = \\exp(1.6) + \\exp(0/0.5) + \\exp(0.6/0.5) + \\exp(0/0.5) + \\exp(-0.6/0.5) = \\exp(1.6) + \\exp(1.2) + 2\\exp(0) + \\exp(-1.2)$.\n  $\\mathcal{L}_{b_1} = \\ln(D_{b_1}) - 1.6$.\n\n- **Anchor $b_2$**: The other views are $\\{b_1, a_1, a_2, c_1, c_2\\}$. The similarities are $\\{0.8, -0.6, 0, 0.6, 0\\}$. This is the same set as for $b_1$.\n  $D_{b_2} = D_{b_1}$, so $\\mathcal{L}_{b_2} = \\mathcal{L}_{b_1}$.\n\nThe total batch loss is $\\mathcal{L}_{\\text{batch}} = \\frac{1}{6} \\sum_i \\mathcal{L}_i = \\frac{1}{6} (2\\mathcal{L}_{a_1} + 2\\mathcal{L}_{a_2} + 2\\mathcal{L}_{b_1}) = \\frac{1}{3} (\\mathcal{L}_{a_1} + \\mathcal{L}_{a_2} + \\mathcal{L}_{b_1})$.\n\nNow, we compute the numerical values:\n$D_{a_1} \\approx 4.95303 + 1 + 0.30119 + 0.13534 + 0.20190 = 6.59146$\n$\\mathcal{L}_{a_1} = \\ln(6.59146) - 1.6 \\approx 1.88577 - 1.6 = 0.28577$\n\n$D_{a_2} \\approx 4.95303 + 3.32012 + 1 + 0.20190 + 0.13534 = 9.61039$\n$\\mathcal{L}_{a_2} = \\ln(9.61039) - 1.6 \\approx 2.26282 - 1.6 = 0.66282$\n\n$D_{b_1} \\approx 4.95303 + 3.32012 + 2(1) + 0.30119 = 10.57434$\n$\\mathcal{L}_{b_1} = \\ln(10.57434) - 1.6 \\approx 2.35846 - 1.6 = 0.75846$\n\n$\\mathcal{L}_{\\text{batch}} = \\frac{1}{3} (0.28577 + 0.66282 + 0.75846) = \\frac{1}{3}(1.70705) \\approx 0.569016...$\n\nRounding to four significant figures, the batch loss is $0.5690$.\n\n### **Part 2: Influence of Temperature $\\tau$**\n\nThe temperature parameter $\\tau$ is a crucial hyperparameter that modulates the separation between positive and negative embeddings in the learned representation space. Its influence can be understood by examining its effect on the probability distribution and the resulting gradients that drive learning.\n\nThe probability of identifying the correct positive pair $j^{+}$ for an anchor $i$ is given by the softmax function applied to the scaled similarities:\n$$ P(j^{+}|i) = \\frac{\\exp(s_{ij^{+}}/\\tau)}{\\sum_{k \\neq i} \\exp(s_{ik}/\\tau)} $$\nThe loss function $\\mathcal{L}_i = -\\ln P(j^{+}|i)$ is the cross-entropy between the model's predicted probability distribution and an ideal one-hot distribution where the probability of the positive pair is $1$.\n\n**Qualitative Influence:**\nThe temperature $\\tau$ controls the \"sharpness\" of the model's output distribution.\n- When $\\tau$ is **low** (approaching $0$), the term $1/\\tau$ becomes very large. This amplifies the differences between similarity scores. The output of the softmax function becomes highly concentrated on the view $k$ with the maximum similarity $s_{ik}$. A low $\\tau$ forces the model to make the similarity of the positive pair, $s_{ij^{+}}$, significantly larger than all negative pair similarities. Any negative sample with a similarity close to the positive one will result in a large loss. This effectively implements a form of \"hard negative mining,\" as the loss and its gradients become dominated by the most challenging negative examples (those most similar to the anchor). This encourages a large margin between positive and negative pairs in the embedding space.\n\n- When $\\tau$ is **high** (approaching $\\infty$), the term $s_{ik}/\\tau$ approaches $0$ for all $k$. Consequently, $\\exp(s_{ik}/\\tau)$ approaches $1$. The probability distribution becomes nearly uniform over all $N-1$ candidate views, with $P(j^{+}|i) \\approx \\frac{1}{N-1}$. The loss approaches a constant value of $\\ln(N-1)$. In this regime, the loss becomes insensitive to the actual similarity values, resulting in vanishingly small gradients. The model receives a very weak learning signal, leading to poor convergence and minimal separation between positive and negative embeddings.\n\n**Quantitative Influence (Gradient Analysis):**\nThe learning process is governed by the gradient of the loss $\\mathcal{L}_i$ with respect to the model's parameters, which is channeled through the similarity scores. Let's analyze the gradient of $\\mathcal{L}_i$ with respect to a similarity score $s_{iz}$. Using the chain rule, $\\frac{\\partial \\mathcal{L}_i}{\\partial s_{iz}} = \\frac{\\partial \\mathcal{L}_i}{\\partial u_z} \\frac{\\partial u_z}{\\partial s_{iz}}$, where $u_z = s_{iz}/\\tau$.\n\nThe gradients with respect to the similarity scores are:\n1.  For the positive pair similarity $s_{ij^{+}}$:\n    $$ \\frac{\\partial \\mathcal{L}_i}{\\partial s_{ij^{+}}} = \\frac{1}{\\tau} (P(j^{+}|i) - 1) $$\n2.  For a negative pair similarity $s_{im}$ (where $m$ is a negative example):\n    $$ \\frac{\\partial \\mathcal{L}_i}{\\partial s_{im}} = \\frac{1}{\\tau} P(m|i) $$\n\nFrom these expressions, we can deduce the quantitative effect of $\\tau$:\n- The factor $1/\\tau$ scales the magnitude of all gradients. A lower $\\tau$ leads to larger gradients, potentially accelerating learning (if not causing instability).\n- Critically, $\\tau$ also shapes the probability values $P(\\cdot|i)$ that determine the relative strength of the gradients.\n- For **low $\\tau$**, the distribution $P$ is sharp. If the model is performing well, $P(j^{+}|i) \\to 1$ and $P(m|i) \\to 0$ for all negatives $m$. The gradient for the positive pair, $\\frac{1}{\\tau}(P(j^{+}|i)-1)$, approaches $0$, meaning the model stops pushing already well-separated positives further. The gradient for an \"easy\" negative (low $s_{im}$) will also be near zero. However, for a \"hard\" negative with non-negligible similarity, $P(m|i)$ will be small but non-zero, and the gradient $\\frac{1}{\\tau}P(m|i)$ can be substantial. The model is thus forced to focus its efforts on pushing these hard negatives away from the anchor.\n- For **high $\\tau$**, the distribution is flat, $P(m|i) \\approx \\frac{1}{N-1}$ for all $m$. The gradient for the positive pair is a small negative constant $\\approx \\frac{1}{\\tau}(\\frac{1}{N-1}-1)$, and the gradient for all negative pairs is a small positive constant $\\approx \\frac{1}{\\tau}\\frac{1}{N-1}$. The update signals are weak and do not differentiate between easy and hard negatives, providing an ineffective penalty structure for learning a discriminative representation space.\n\nIn summary, $\\tau$ adjusts the penalty for mis-ordered similarities. Low temperatures enforce a stronger separation by penalizing hard negatives more severely, while high temperatures weaken the learning signal and fail to enforce a clear margin.", "answer": "$$\\boxed{0.5690}$$", "id": "4332990"}, {"introduction": "A key challenge in applying deep learning to science is interpreting a model's predictions to gain new insights. This final practice introduces Integrated Gradients, a robust method for attributing a model's output back to its input features. By calculating atom-level attribution scores for a simple Graph Neural Network, you will learn how to connect a model's prediction to the underlying chemical substructures, bridging the gap between \"black box\" machine learning and established medicinal chemistry principles like Structure-Activity Relationships (SAR) [@problem_id:4333008].", "problem": "You are given a simplified, but scientifically realistic, attribution task for a trained Graph Neural Network (GNN) used in small-molecule activity prediction. Consider a molecule represented as a chain of three atoms connected as $1 \\text{--} 2 \\text{--} 3$. The learned first message-passing layer is linear and represented by the symmetric mixing matrix\n$$\nM \\;=\\; \\begin{pmatrix}\n1 & 0.5 & 0 \\\\\n0.5 & 1 & 0.5 \\\\\n0 & 0.5 & 1\n\\end{pmatrix}.\n$$\nThere is one scalar input feature per atom, collected into the vector $x \\in \\mathbb{R}^{3}$ with\n$$\nx \\;=\\; \\begin{pmatrix} 1.2 \\\\ 0.7 \\\\ 0.3 \\end{pmatrix}.\n$$\nLet the trained readout after this layer be defined by the weight vector $w \\in \\mathbb{R}^{3}$,\n$$\nw \\;=\\; \\begin{pmatrix} 0.8 \\\\ -0.3 \\\\ 0.5 \\end{pmatrix},\n$$\nand the scalar prediction be given by the composition\n$$\nF(x) \\;=\\; g\\!\\left( w^{\\top} (M x)\\right),\n$$\nwhere $g(z) \\;=\\; \\ln\\!\\big(1 + \\exp(z)\\big)$ is the softplus nonlinearity. Use the straight-line path Integrated Gradients (IG) from the zero baseline $x' = 0$ to the input $x$ to compute atom-level attribution scores $a \\in \\mathbb{R}^{3}$, where for each atom $i$,\n$$\na_{i} \\;=\\; (x_{i} - x'_{i}) \\int_{0}^{1} \\frac{\\partial F\\!\\big(x' + \\alpha (x - x')\\big)}{\\partial x_{i}} \\, d\\alpha.\n$$\nFor medicinal chemistry validation, a structure-activity relationship (SAR) expert panel provides a three-atom reference contribution vector\n$$\nr \\;=\\; \\begin{pmatrix} 1.0 \\\\ -0.5 \\\\ 0.2 \\end{pmatrix}.\n$$\nCompute the cosine similarity between the IG attribution vector $a$ and the SAR vector $r$, defined as\n$$\n\\cos\\big(a, r\\big) \\;=\\; \\frac{a^{\\top} r}{\\|a\\|_{2} \\, \\|r\\|_{2}},\n$$\nusing the straight-line path from the zero baseline. Round your final scalar validation metric to four significant figures. Provide only the final scalar as your answer (unitless).", "solution": "The problem requires the calculation of the cosine similarity between an Integrated Gradients (IG) attribution vector $a$ and a reference structureâ€“activity relationship (SAR) vector $r$.\n\nFirst, we formalize the prediction function $F(x)$ and the quantities involved in the IG calculation. The function is given by $F(x) = g(w^{\\top}(Mx))$, where $g(z) = \\ln(1 + \\exp(z))$ is the softplus function. The IG attribution for each atom $i$ is defined along a straight-line path from a baseline $x' = 0$ to the input $x$:\n$$\na_{i} = (x_{i} - x'_{i}) \\int_{0}^{1} \\frac{\\partial F(x' + \\alpha (x - x'))}{\\partial x_{i}} \\, d\\alpha\n$$\nWith $x' = 0$, this simplifies to:\n$$\na_{i} = x_{i} \\int_{0}^{1} \\frac{\\partial F(\\alpha x)}{\\partial x_{i}} \\, d\\alpha\n$$\nTo proceed, we first compute the gradient of $F(x)$. Let us define an intermediate vector $v = M^{\\top}w$. As the given mixing matrix $M$ is symmetric, $M^{\\top} = M$, so $v = Mw$. The prediction function can then be written as $F(x) = g(v^{\\top}x)$.\nUsing the chain rule, the partial derivative of $F$ with respect to $x_i$ is:\n$$\n\\frac{\\partial F(x)}{\\partial x_i} = g'(v^{\\top}x) \\cdot \\frac{\\partial (v^{\\top}x)}{\\partial x_i}\n$$\nThe derivative of the softplus function $g(z)$ is the sigmoid function, $\\sigma(z) = \\frac{\\exp(z)}{1 + \\exp(z)}$. The derivative of the linear term $v^{\\top}x = \\sum_j v_j x_j$ with respect to $x_i$ is $v_i$. Therefore,\n$$\n\\frac{\\partial F(x)}{\\partial x_i} = \\sigma(v^{\\top}x) v_i\n$$\nNow we evaluate this derivative along the integration path $x(\\alpha) = \\alpha x$:\n$$\n\\frac{\\partial F(\\alpha x)}{\\partial x_{i}} = \\sigma(v^{\\top}(\\alpha x)) v_i = \\sigma(\\alpha(v^{\\top}x)) v_i\n$$\nSubstituting this back into the IG formula for $a_i$:\n$$\na_i = x_i \\int_{0}^{1} \\sigma(\\alpha(v^{\\top}x)) v_i \\, d\\alpha = (x_i v_i) \\int_{0}^{1} \\sigma(\\alpha(v^{\\top}x)) \\, d\\alpha\n$$\nThe integral term $\\int_{0}^{1} \\sigma(\\alpha(v^{\\top}x)) \\, d\\alpha$ is a scalar value, let's call it $k_s$, which is common to all components $a_i$. Thus, the attribution vector $a$ can be written as:\n$$\na = k_s (x \\odot v)\n$$\nwhere $\\odot$ denotes the element-wise (Hadamard) product.\n\nWe need to compute the cosine similarity $\\cos(a, r) = \\frac{a^{\\top} r}{\\|a\\|_{2} \\|r\\|_{2}}$. Substituting the expression for $a$:\n$$\n\\cos(a, r) = \\frac{(k_s (x \\odot v))^{\\top} r}{\\|k_s (x \\odot v)\\|_{2} \\|r\\|_{2}} = \\frac{k_s ((x \\odot v)^{\\top} r)}{|k_s| \\|x \\odot v\\|_{2} \\|r\\|_{2}}\n$$\nThe integrand $\\sigma(\\alpha(v^{\\top}x))$ is always positive since the sigmoid function's range is $(0, 1)$. Therefore, the integral $k_s$ must be positive, which means $|k_s| = k_s$. The scalar constant $k_s$ cancels out, simplifying the expression significantly:\n$$\n\\cos(a, r) = \\frac{(x \\odot v)^{\\top} r}{\\|x \\odot v\\|_{2} \\|r\\|_{2}}\n$$\nThis allows us to compute the cosine similarity without explicitly calculating the integral. Let us define a vector $a' = x \\odot v$. The problem reduces to computing $\\cos(a', r)$.\n\nFirst, we compute the vector $v = Mw$ using the given matrices:\n$$\nv = \\begin{pmatrix} 1 & 0.5 & 0 \\\\ 0.5 & 1 & 0.5 \\\\ 0 & 0.5 & 1 \\end{pmatrix} \\begin{pmatrix} 0.8 \\\\ -0.3 \\\\ 0.5 \\end{pmatrix} = \\begin{pmatrix} 1(0.8) + 0.5(-0.3) + 0(0.5) \\\\ 0.5(0.8) + 1(-0.3) + 0.5(0.5) \\\\ 0(0.8) + 0.5(-0.3) + 1(0.5) \\end{pmatrix} = \\begin{pmatrix} 0.8 - 0.15 \\\\ 0.4 - 0.3 + 0.25 \\\\ -0.15 + 0.5 \\end{pmatrix} = \\begin{pmatrix} 0.65 \\\\ 0.35 \\\\ 0.35 \\end{pmatrix}\n$$\nNext, we compute the vector $a' = x \\odot v$ using the input feature vector $x$:\n$$\na' = \\begin{pmatrix} 1.2 \\\\ 0.7 \\\\ 0.3 \\end{pmatrix} \\odot \\begin{pmatrix} 0.65 \\\\ 0.35 \\\\ 0.35 \\end{pmatrix} = \\begin{pmatrix} 1.2 \\times 0.65 \\\\ 0.7 \\times 0.35 \\\\ 0.3 \\times 0.35 \\end{pmatrix} = \\begin{pmatrix} 0.78 \\\\ 0.245 \\\\ 0.105 \\end{pmatrix}\n$$\nNow we compute the terms required for the cosine similarity with the reference vector $r = \\begin{pmatrix} 1.0 \\\\ -0.5 \\\\ 0.2 \\end{pmatrix}$.\nThe dot product $a'^{\\top} r$ is:\n$$\na'^{\\top} r = (0.78)(1.0) + (0.245)(-0.5) + (0.105)(0.2) = 0.78 - 0.1225 + 0.021 = 0.6785\n$$\nThe squared Euclidean norm of $a'$ is:\n$$\n\\|a'\\|_{2}^2 = (0.78)^2 + (0.245)^2 + (0.105)^2 = 0.6084 + 0.060025 + 0.011025 = 0.67945\n$$\nSo, $\\|a'\\|_{2} = \\sqrt{0.67945}$.\nThe squared Euclidean norm of $r$ is:\n$$\n\\|r\\|_{2}^2 = (1.0)^2 + (-0.5)^2 + (0.2)^2 = 1.0 + 0.25 + 0.04 = 1.29\n$$\nSo, $\\|r\\|_{2} = \\sqrt{1.29}$.\n\nFinally, we compute the cosine similarity:\n$$\n\\cos(a, r) = \\frac{a'^{\\top} r}{\\|a'\\|_{2} \\|r\\|_{2}} = \\frac{0.6785}{\\sqrt{0.67945} \\sqrt{1.29}} = \\frac{0.6785}{\\sqrt{0.67945 \\times 1.29}} = \\frac{0.6785}{\\sqrt{0.8764905}}\n$$\n$$\n\\cos(a, r) \\approx \\frac{0.6785}{0.9362107} \\approx 0.7247334\n$$\nThe problem requires the final answer to be rounded to four significant figures. Rounding $0.7247334...$ gives $0.7247$.", "answer": "$$\\boxed{0.7247}$$", "id": "4333008"}]}