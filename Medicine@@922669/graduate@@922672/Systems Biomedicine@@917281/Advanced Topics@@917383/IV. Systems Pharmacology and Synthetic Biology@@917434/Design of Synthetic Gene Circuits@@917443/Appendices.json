{"hands_on_practices": [{"introduction": "The fundamental building block of any synthetic gene circuit is the promoter, which acts as a processing unit converting an input, such as a transcription factor concentration, into an output, the rate of gene expression. Before designing complex circuits, it's crucial to master the quantitative description of this input-output relationship, often called the dose-response curve or transfer function. This exercise [@problem_id:4334034] guides you through deriving this relationship for a simple repressor from first principles of thermodynamic equilibrium and challenges you to connect this theoretical model to realistic experimental data, accounting for practical issues like background signal and leakiness.", "problem": "Consider a synthetic gene circuit in which a single promoter is regulated by a transcriptional repressor. According to the Central Dogma of molecular biology, transcription produces messenger ribonucleic acid, and transcription initiation requires binding of ribonucleic acid polymerase (RNAP) to the promoter. Assume thermodynamic equilibrium of binding and the standard statistical mechanical occupancy framework for transcriptional regulation: promoter states contribute weights to a partition function, and the probability of RNAP-bound promoter occupancy follows from normalized state weights. Take the following physically motivated assumptions as the fundamental base:\n\n1. RNAP binds the promoter with an effective association weight $w_{P} = [P]/K_{P}$, where $[P]$ is the RNAP concentration and $K_{P}$ is its dissociation constant.\n2. The repressor binds the promoter operator site with an association weight $w_{R}([R]) = [R]/K_{R}$, where $[R]$ is the repressor concentration and $K_{R}$ is its dissociation constant.\n3. RNAP binding and repressor binding are mutually exclusive at the operator (simple repression), so the promoter’s admissible states are: empty, RNAP-bound, or repressor-bound.\n4. Under equilibrium, the RNAP-bound occupancy probability $p_{\\mathrm{bound}}([R])$ is determined by enumerating states and normalizing by the total partition function.\n\nIn quantitative imaging, the measured output is proportional to the RNAP-bound occupancy but includes an additive background due to instrument offset and unspecific fluorescence. Model the measured expression as $E([R]) = b + \\alpha\\, p_{\\mathrm{bound}}([R])$, where $b \\ge 0$ is a background term and $\\alpha \\ge 0$ is the maximal activity scale when RNAP is bound. Thermodynamic consistency requires the state weights to be nonnegative, the occupancy probability to satisfy $0 \\le p_{\\mathrm{bound}}([R]) \\le 1$, and the mapping $E([R]) = b + \\alpha\\, p_{\\mathrm{bound}}([R])$ to have $b \\ge 0$ and $\\alpha \\ge 0$.\n\nYou are given the following experimental dose-response dataset for a single promoter under a repressor titration, with Arbitrary Fluorescence Units (AFU) as the measurement unit:\n\n- Repressor dissociation constant $K_{R} = 50$ $\\mathrm{nM}$, and an effective RNAP association weight $w_{P} = \\frac{5}{2}$ (dimensionless).\n- Measured expression values:\n  - At $[R] = 0$ $\\mathrm{nM}$: $E(0) = \\frac{7050}{7}$ $\\mathrm{AFU}$.\n  - At $[R] = 100$ $\\mathrm{nM}$: $E(100) = \\frac{7650}{11}$ $\\mathrm{AFU}$.\n  - At $[R] = 800$ $\\mathrm{nM}$: $E(800) = \\frac{11850}{39}$ $\\mathrm{AFU}$.\n\nStarting from the equilibrium occupancy framework described above, first argue why and how leakiness arises in a regulated promoter under finite $[R]$. Then, using the given dataset and constraints, determine the minimal nonnegative background term $b_{\\min}$ such that there exists a nonnegative activity scale $\\alpha$ making $E([R_{i}]) = b_{\\min} + \\alpha\\, p_{\\mathrm{bound}}([R_{i}])$ hold simultaneously for all three repressor concentrations $[R_{i}] \\in \\{0, 100, 800\\}$ $\\mathrm{nM}$ without violating thermodynamic consistency.\n\nExpress your final answer for $b_{\\min}$ in $\\mathrm{AFU}$. No rounding is required.", "solution": "The problem asks for two things: first, a qualitative explanation for the origin of leakiness in gene expression, and second, the quantitative determination of the minimal non-negative background fluorescence, $b_{\\min}$, consistent with a given dataset.\n\nFirst, we address the origin of leakiness within the provided thermodynamic framework. The promoter can exist in one of three mutually exclusive states: empty (unbound), bound by RNA polymerase (RNAP), or bound by the repressor protein. The corresponding statistical weights for these states are $1$ (for the empty state, by definition), $w_P$ (for the RNAP-bound state), and $w_R([R])$ (for the repressor-bound state). The partition function, $Z$, which sums the weights of all accessible states, is given by:\n$$Z([R]) = 1 + w_P + w_R([R])$$\nThe probability of the promoter being in the RNAP-bound state, $p_{\\mathrm{bound}}$, is the ratio of the weight of the RNAP-bound state to the partition function:\n$$p_{\\mathrm{bound}}([R]) = \\frac{w_P}{Z([R])} = \\frac{w_P}{1 + w_P + w_R([R])}$$\nTranscription is proportional to this probability. Leakiness refers to the persistence of a basal level of expression even in the presence of the repressor. According to the model, the repressor's association weight is $w_R([R]) = [R]/K_R$. For any finite concentration of repressor, $[R]$, the value of $w_R([R])$ is finite. As long as RNAP is present and can bind (i.e., $w_P > 0$), the numerator $w_P$ is non-zero, and the denominator $Z([R])$ is finite. Consequently, $p_{\\mathrm{bound}}([R])$ is greater than zero for any finite $[R]$. This non-zero probability arises from the stochastic nature of molecule-DNA interactions at thermodynamic equilibrium. The repressor is not permanently fixed to the promoter; it binds and unbinds dynamically. During intervals when the repressor is unbound, the promoter is available for RNAP to bind and initiate transcription, leading to a \"leaky\" or basal level of gene expression. Expression would only approach zero in the physically unattainable limit where $[R] \\to \\infty$.\n\nNext, we determine the value of $b_{\\min}$. The measured expression level, $E([R])$, is modeled as a linear function of the occupancy probability:\n$$E([R]) = b + \\alpha\\, p_{\\mathrm{bound}}([R])$$\nwhere $b \\ge 0$ is the background and $\\alpha \\ge 0$ is the maximal activity scale. We are given the parameters $w_P = \\frac{5}{2}$ and $K_R = 50$ $\\mathrm{nM}$. We first calculate the occupancy probability $p_{\\mathrm{bound}}([R_i])$ for the three given repressor concentrations $[R_i] \\in \\{0, 100, 800\\}$ $\\mathrm{nM}$.\n\nThe general expression for the occupancy probability is:\n$$p_{\\mathrm{bound}}([R]) = \\frac{w_P}{1 + w_P + \\frac{[R]}{K_R}} = \\frac{\\frac{5}{2}}{1 + \\frac{5}{2} + \\frac{[R]}{50}}$$\n\nFor $[R_1] = 0$ $\\mathrm{nM}$:\n$$p_{\\mathrm{bound}}(0) = \\frac{\\frac{5}{2}}{1 + \\frac{5}{2} + \\frac{0}{50}} = \\frac{\\frac{5}{2}}{\\frac{7}{2}} = \\frac{5}{7}$$\n\nFor $[R_2] = 100$ $\\mathrm{nM}$:\n$$p_{\\mathrm{bound}}(100) = \\frac{\\frac{5}{2}}{1 + \\frac{5}{2} + \\frac{100}{50}} = \\frac{\\frac{5}{2}}{1 + \\frac{5}{2} + 2} = \\frac{\\frac{5}{2}}{\\frac{11}{2}} = \\frac{5}{11}$$\n\nFor $[R_3] = 800$ $\\mathrm{nM}$:\n$$p_{\\mathrm{bound}}(800) = \\frac{\\frac{5}{2}}{1 + \\frac{5}{2} + \\frac{800}{50}} = \\frac{\\frac{5}{2}}{1 + \\frac{5}{2} + 16} = \\frac{\\frac{5}{2}}{\\frac{39}{2}} = \\frac{5}{39}$$\n\nWe are given the corresponding measured expression values:\n$E(0) = \\frac{7050}{7}$ $\\mathrm{AFU}$\n$E(100) = \\frac{7650}{11}$ $\\mathrm{AFU}$\n$E(800) = \\frac{11850}{39}$ $\\mathrm{AFU}$\n\nThe problem states that the relation $E([R_i]) = b + \\alpha\\, p_{\\mathrm{bound}}([R_i])$ must hold simultaneously for all three data points. This gives us a system of three linear equations with two unknowns, $b$ and $\\alpha$:\n1. $\\frac{7050}{7} = b + \\alpha \\frac{5}{7}$\n2. $\\frac{7650}{11} = b + \\alpha \\frac{5}{11}$\n3. $\\frac{11850}{39} = b + \\alpha \\frac{5}{39}$\n\nWe can clear the denominators by multiplying each equation by its respective denominator:\n1. $7050 = 7b + 5\\alpha$\n2. $7650 = 11b + 5\\alpha$\n3. $11850 = 39b + 5\\alpha$\n\nFor a solution $(b, \\alpha)$ to exist, the system must be consistent. We can solve for $b$ and $\\alpha$ using any two equations and then verify the solution with the third. Let's subtract equation (1) from equation (2):\n$$(11b + 5\\alpha) - (7b + 5\\alpha) = 7650 - 7050$$\n$$4b = 600$$\n$$b = 150$$\n\nNow, we substitute $b = 150$ back into equation (1) to find $\\alpha$:\n$$7050 = 7(150) + 5\\alpha$$\n$$7050 = 1050 + 5\\alpha$$\n$$6000 = 5\\alpha$$\n$$\\alpha = 1200$$\n\nWe must verify that this solution $(b, \\alpha) = (150, 1200)$ also satisfies equation (3):\n$$39(150) + 5(1200) = 5850 + 6000 = 11850$$\nThis matches the right-hand side of the rearranged third equation, so the system is consistent and has a unique solution.\n\nThe problem requires finding the minimal non-negative background term, $b_{\\min}$. We have found a unique solution for $b$ that allows the model to hold simultaneously for all data points, which is $b = 150$. This value is non-negative ($150 \\ge 0$). The corresponding activity scale is $\\alpha = 1200$, which is also non-negative ($\\alpha \\ge 0$). Since there is only one value of $b$ for which a simultaneous solution exists, this value is necessarily the minimum possible value that satisfies the conditions. Therefore, $b_{\\min} = 150$ $\\mathrm{AFU}$.", "answer": "$$\\boxed{150}$$", "id": "4334034"}, {"introduction": "While steady-state behavior defines what a circuit *can* do, its dynamics determine *how fast* it does it. For many applications, from biosensing to temporal logic, the response time of a circuit is a critical design parameter. This practice [@problem_id:4333975] moves beyond static analysis to explore the dynamics of a transcriptional cascade, a common motif used for signal propagation or creating delays. By solving the underlying differential equations, you will determine how the architecture of the circuit and the biochemical rates of its components shape its temporal response to a step-input signal.", "problem": "Consider a transcriptional two-gene cascade commonly used in the design of synthetic gene circuits: gene product $x(t)$ is produced in response to an upstream input $u(t)$ and degrades with first-order kinetics, and gene product $y(t)$ is produced proportionally to $x(t)$ and also degrades with first-order kinetics. Assume well-mixed conditions and that the reactions are governed by mass-action kinetics. The governing ordinary differential equations are\n$$\n\\frac{dx}{dt} = k_{1} u(t) - \\gamma x(t), \\qquad \\frac{dy}{dt} = k_{2} x(t) - \\gamma y(t),\n$$\nwith initial conditions $x(0)=0$ and $y(0)=0$. Here $k_{1}>0$ and $k_{2}>0$ are production rate constants, and $\\gamma>0$ is the common first-order degradation/dilution rate for both species (for example, dominated by dilution in a balanced exponential growth regime). The input $u(t)$ is a step stimulus modeling induction of the first promoter: $u(t) = u_{0} H(t)$, where $u_{0}>0$ is constant and $H(t)$ is the Heaviside step function.\n\nStarting only from these equations and the Central Dogma of molecular biology (transcription-translation with first-order loss), derive the step response $y(t)$ and its steady state $y_{\\mathrm{ss}}$ for $t \\to \\infty$. Define the $0.9$-time $t_{0.9}$ implicitly by $y(t_{0.9}) = 0.9\\,y_{\\mathrm{ss}}$. Compute a closed-form analytic expression for $t_{0.9}$ and express it in terms of the degradation rate parameter $\\gamma$ only (no numerical substitution). Use the real branches of the Lambert $W$ function as needed, where the Lambert $W$ function $W(z)$ is defined implicitly by $W(z)\\exp(W(z)) = z$. Show in your derivation how $t_{0.9}$ scales with the degradation rate. Assume $\\gamma$ is measured in $\\mathrm{min}^{-1}$ and report $t_{0.9}$ in minutes. Do not include units in your final boxed expression.", "solution": "The problem involves solving a cascade of two first-order linear ODEs. We begin by solving for the concentration of the first gene product, $x(t)$.\n\nFor $t > 0$, the input is constant, $u(t) = u_0$. The first differential equation becomes:\n$$\n\\frac{dx}{dt} + \\gamma x(t) = k_1 u_0\n$$\nThis is a first-order linear inhomogeneous ODE with constant coefficients. The solution is the sum of a homogeneous solution $x_h(t)$ and a particular solution $x_p(t)$. The homogeneous equation is $\\frac{dx_h}{dt} + \\gamma x_h = 0$, which has the solution $x_h(t) = A \\exp(-\\gamma t)$ for some constant $A$. For the particular solution, we assume a constant, $x_p(t) = C_{p}$. Substituting this into the ODE gives $0 + \\gamma C_p = k_1 u_0$, so $C_p = \\frac{k_1 u_0}{\\gamma}$.\nThe general solution is $x(t) = \\frac{k_1 u_0}{\\gamma} + A \\exp(-\\gamma t)$.\nWe use the initial condition $x(0) = 0$ to find $A$:\n$$\nx(0) = 0 = \\frac{k_1 u_0}{\\gamma} + A \\exp(0) \\implies A = -\\frac{k_1 u_0}{\\gamma}\n$$\nThus, the solution for $x(t)$ for $t \\ge 0$ is:\n$$\nx(t) = \\frac{k_1 u_0}{\\gamma} (1 - \\exp(-\\gamma t))\n$$\nNext, we solve for $y(t)$ by substituting the expression for $x(t)$ into the second ODE:\n$$\n\\frac{dy}{dt} + \\gamma y(t) = k_2 x(t) = k_2 \\left( \\frac{k_1 u_0}{\\gamma} (1 - \\exp(-\\gamma t)) \\right) = \\frac{k_1 k_2 u_0}{\\gamma} - \\frac{k_1 k_2 u_0}{\\gamma} \\exp(-\\gamma t)\n$$\nThis is another first-order linear inhomogeneous ODE. We solve it using an integrating factor $I(t) = \\exp\\left(\\int \\gamma \\, dt\\right) = \\exp(\\gamma t)$. Multiplying the ODE by $I(t)$ gives:\n$$\n\\exp(\\gamma t) \\frac{dy}{dt} + \\gamma \\exp(\\gamma t) y(t) = \\frac{k_1 k_2 u_0}{\\gamma} (\\exp(\\gamma t) - 1)\n$$\nThe left side is the derivative of $y(t) \\exp(\\gamma t)$:\n$$\n\\frac{d}{dt} [y(t) \\exp(\\gamma t)] = \\frac{k_1 k_2 u_0}{\\gamma} (\\exp(\\gamma t) - 1)\n$$\nIntegrating both sides with respect to $t$ from $0$ to $t$:\n$$\n\\int_0^t \\frac{d}{d\\tau} [y(\\tau) \\exp(\\gamma \\tau)] \\, d\\tau = \\int_0^t \\frac{k_1 k_2 u_0}{\\gamma} (\\exp(\\gamma \\tau) - 1) \\, d\\tau\n$$\n$$\n[y(\\tau) \\exp(\\gamma \\tau)]_0^t = \\frac{k_1 k_2 u_0}{\\gamma} \\left[ \\frac{1}{\\gamma}\\exp(\\gamma \\tau) - \\tau \\right]_0^t\n$$\nUsing the initial condition $y(0)=0$:\n$$\ny(t) \\exp(\\gamma t) - y(0) \\exp(0) = \\frac{k_1 k_2 u_0}{\\gamma} \\left( \\left(\\frac{1}{\\gamma}\\exp(\\gamma t) - t \\right) - \\left(\\frac{1}{\\gamma}\\exp(0) - 0\\right) \\right)\n$$\n$$\ny(t) \\exp(\\gamma t) = \\frac{k_1 k_2 u_0}{\\gamma} \\left( \\frac{1}{\\gamma}\\exp(\\gamma t) - t - \\frac{1}{\\gamma} \\right)\n$$\nMultiplying by $\\exp(-\\gamma t)$ to isolate $y(t)$:\n$$\ny(t) = \\frac{k_1 k_2 u_0}{\\gamma} \\left( \\frac{1}{\\gamma} - t \\exp(-\\gamma t) - \\frac{1}{\\gamma}\\exp(-\\gamma t) \\right)\n$$\nFactoring out the constant term $\\frac{k_1 k_2 u_0}{\\gamma^2}$:\n$$\ny(t) = \\frac{k_1 k_2 u_0}{\\gamma^2} [1 - \\gamma t \\exp(-\\gamma t) - \\exp(-\\gamma t)]\n$$\nThe step response is therefore:\n$$\ny(t) = \\frac{k_1 k_2 u_0}{\\gamma^2} [1 - (1 + \\gamma t) \\exp(-\\gamma t)]\n$$\nThe steady-state value $y_{\\mathrm{ss}}$ is the limit of $y(t)$ as $t \\to \\infty$. Since $\\gamma > 0$, both $\\exp(-\\gamma t)$ and $t \\exp(-\\gamma t)$ tend to $0$ as $t \\to \\infty$.\n$$\ny_{\\mathrm{ss}} = \\lim_{t \\to \\infty} y(t) = \\frac{k_1 k_2 u_0}{\\gamma^2} [1 - 0] = \\frac{k_1 k_2 u_0}{\\gamma^2}\n$$\nNow, we find the $0.9$-time, $t_{0.9}$, which is defined by the relation $y(t_{0.9}) = 0.9 \\, y_{\\mathrm{ss}}$.\n$$\n\\frac{k_1 k_2 u_0}{\\gamma^2} [1 - (1 + \\gamma t_{0.9}) \\exp(-\\gamma t_{0.9})] = 0.9 \\left( \\frac{k_1 k_2 u_0}{\\gamma^2} \\right)\n$$\nThe constant pre-factor, which is non-zero, cancels out:\n$$\n1 - (1 + \\gamma t_{0.9}) \\exp(-\\gamma t_{0.9}) = 0.9\n$$\n$$\n(1 + \\gamma t_{0.9}) \\exp(-\\gamma t_{0.9}) = 0.1\n$$\nThis is a transcendental equation for $t_{0.9}$. To solve it, we use the Lambert $W$ function, defined by $W(z) \\exp(W(z)) = z$. We need to transform our equation into this form. Let $X = \\gamma t_{0.9}$. The equation is $(1+X)\\exp(-X)=0.1$.\nMultiply both sides by $-1$ and by $\\exp(-1)$:\n$$\n-(1+X)\\exp(-X)\\exp(-1) = -0.1\\exp(-1)\n$$\n$$\n-(1+X)\\exp(-(1+X)) = -\\frac{0.1}{e}\n$$\nLet $Z = -(1+X)$. The equation becomes:\n$$\nZ \\exp(Z) = -\\frac{0.1}{e}\n$$\nBy definition of the Lambert $W$ function, the solution for $Z$ is:\n$$\nZ = W\\left(-\\frac{0.1}{e}\\right)\n$$\nThe argument of the $W$ function is $z_{arg} = -0.1/e$. Since $e \\approx 2.718$, we have $-1/e < z_{arg} < 0$. In this interval, the Lambert $W$ function has two real branches: the principal branch $W_0(z)$ and the lower branch $W_{-1}(z)$.\n- For the principal branch, $W_0(z_{arg}) \\in (-1, 0)$.\n- For the lower branch, $W_{-1}(z_{arg}) < -1$.\n\nLet's substitute back $Z = -(1+\\gamma t_{0.9})$:\n$$\n-(1+\\gamma t_{0.9}) = W\\left(-\\frac{0.1}{e}\\right)\n$$\n$$\n\\gamma t_{0.9} = -1 - W\\left(-\\frac{0.1}{e}\\right)\n$$\n$$\nt_{0.9} = \\frac{1}{\\gamma} \\left(-1 - W\\left(-\\frac{0.1}{e}\\right)\\right)\n$$\nWe must choose the branch that yields a physically meaningful positive time $t_{0.9} > 0$. Since $\\gamma > 0$, the sign of $t_{0.9}$ is determined by the sign of the term in parentheses.\n1.  If we choose the principal branch $W_0$: Since $-1 < W_0(-0.1/e) < 0$, we have $0 < -W_0(-0.1/e) < 1$. Then $-1 - W_0(-0.1/e)$ is between $-1+0 = -1$ and $-1+1 = 0$. This would lead to a negative value for $t_{0.9}$, which is unphysical.\n2.  If we choose the lower branch $W_{-1}$: We have $W_{-1}(-0.1/e) < -1$. Therefore, $-W_{-1}(-0.1/e) > 1$. Then $-1-W_{-1}(-0.1/e)$ is positive. This yields a positive value for $t_{0.9}$, which is the correct physical solution.\n\nThus, the closed-form expression for $t_{0.9}$ is:\n$$\nt_{0.9} = \\frac{1}{\\gamma} \\left(-1 - W_{-1}\\left(-\\frac{0.1}{e}\\right)\\right)\n$$\nThis expression shows how $t_{0.9}$ scales with the degradation rate $\\gamma$. Specifically, $t_{0.9}$ is inversely proportional to $\\gamma$, as the term in parentheses is a dimensionless constant. The time is given in minutes since $\\gamma$ is in $\\mathrm{min}^{-1}$.", "answer": "$$\n\\boxed{\\frac{1}{\\gamma}\\left(-1 - W_{-1}\\left(-\\frac{0.1}{e}\\right)\\right)}\n$$", "id": "4333975"}, {"introduction": "The deterministic models we've explored provide a powerful description of average cellular behavior, but they miss a crucial aspect of biology: randomness. Gene expression is an inherently stochastic process, leading to cell-to-cell variability in protein levels even in a genetically identical population. This exercise [@problem_id:4334010] introduces the Chemical Master Equation (CME), the fundamental framework for modeling this stochasticity. By analyzing a simple birth-death process for a constitutively expressed gene, you will derive key statistical measures of this 'intrinsic noise', providing a quantitative foundation for understanding and engineering cellular heterogeneity.", "problem": "Consider a constitutively expressed protein in a synthetic gene circuit within a growing cell. Protein molecules are produced with a zero-order rate $\\,\\alpha\\,$, independent of the current copy number, and are removed by first-order processes with rate $\\,\\beta\\,$ per molecule due to degradation and dilution by cell growth. Let $\\,n\\,$ denote the protein copy number and assume the dynamics are well described by a continuous-time, memoryless Markov process with transitions $\\,n \\to n+1\\,$ at rate $\\,\\alpha\\,$ and $\\,n \\to n-1\\,$ at rate $\\,\\beta n\\,$. Starting from the Chemical Master Equation (CME) for this birth–death process and using only fundamental definitions of moments and steady state, derive closed-form expressions for the steady-state mean $\\,\\langle n \\rangle\\,$, variance $\\,\\operatorname{Var}(n)\\,$, and the Fano factor $\\,\\mathcal{F} = \\operatorname{Var}(n)/\\langle n \\rangle\\,$ of the protein copy number. Express your final results solely in terms of $\\,\\alpha\\,$ and $\\,\\beta\\,$. Provide your final answer as a row matrix listing, in order, $\\,\\langle n \\rangle\\,$, $\\,\\operatorname{Var}(n)\\,$, and $\\,\\mathcal{F}\\,$. No rounding is required, and no units should be included inside the final boxed expression.", "solution": "The system described is a continuous-time Markov birth-death process for the number of protein molecules, denoted by $n$. The transitions and their associated rates are:\n-   Birth: $n \\to n+1$ with rate $W(n \\to n+1) = \\alpha$.\n-   Death: $n \\to n-1$ with rate $W(n \\to n-1) = \\beta n$.\n\nLet $P(n, t)$ be the probability of having $n$ molecules at time $t$. The time evolution of this probability distribution is governed by the Chemical Master Equation (CME). For a state with $n$ molecules, the probability changes due to fluxes from neighboring states ($n-1$ and $n+1$) and fluxes out to those states. The CME for this process is:\n$$\n\\frac{d P(n, t)}{dt} = [\\text{flux in}] - [\\text{flux out}]\n$$\n$$\n\\frac{d P(n, t)}{dt} = \\alpha P(n-1, t) + \\beta(n+1) P(n+1, t) - (\\alpha + \\beta n) P(n, t)\n$$\nThis equation is valid for $n \\geq 1$. The boundary condition at $n=0$ is:\n$$\n\\frac{d P(0, t)}{dt} = \\beta(1) P(1, t) - \\alpha P(0, t)\n$$\nBy defining $P(-1, t) = 0$, the general form of the CME can be considered valid for all $n \\geq 0$.\n\nThe problem requires deriving the steady-state moments using their fundamental definitions. To do this, we derive the equations for the time evolution of the moments and then apply the steady-state condition ($d/dt = 0$).\n\n**1. Derivation of the steady-state mean $\\langle n \\rangle$**\n\nThe mean (or first moment) of the distribution is defined as $\\langle n \\rangle(t) = \\sum_{n=0}^{\\infty} n P(n, t)$. Its time derivative is:\n$$\n\\frac{d\\langle n \\rangle}{dt} = \\sum_{n=0}^{\\infty} n \\frac{d P(n, t)}{dt}\n$$\nSubstituting the CME into this expression:\n$$\n\\frac{d\\langle n \\rangle}{dt} = \\sum_{n=0}^{\\infty} n \\left[ \\alpha P(n-1, t) + \\beta(n+1) P(n+1, t) - (\\alpha + \\beta n) P(n, t) \\right]\n$$\nWe analyze the sum term by term:\n-   $\\sum_{n=0}^{\\infty} n \\alpha P(n-1, t) = \\alpha \\sum_{n=1}^{\\infty} n P(n-1, t)$. Let $k = n-1$, so $n = k+1$. The sum becomes $\\alpha \\sum_{k=0}^{\\infty} (k+1) P(k, t) = \\alpha(\\sum_{k=0}^{\\infty} k P(k, t) + \\sum_{k=0}^{\\infty} P(k, t)) = \\alpha(\\langle n \\rangle + 1)$, since $\\sum P(k,t)=1$.\n-   $\\sum_{n=0}^{\\infty} n \\beta(n+1) P(n+1, t)$. Let $k = n+1$, so $n=k-1$. The sum becomes $\\beta \\sum_{k=1}^{\\infty} (k-1)k P(k, t) = \\beta(\\sum_{k=0}^{\\infty} k^2 P(k, t) - \\sum_{k=0}^{\\infty} k P(k, t)) = \\beta(\\langle n^2 \\rangle - \\langle n \\rangle)$, since the term for $k=0$ is zero.\n-   $\\sum_{n=0}^{\\infty} n [ -(\\alpha + \\beta n) P(n, t)] = - \\alpha \\sum_{n=0}^{\\infty} n P(n, t) - \\beta \\sum_{n=0}^{\\infty} n^2 P(n, t) = -\\alpha\\langle n \\rangle - \\beta\\langle n^2 \\rangle$.\n\nCombining these results:\n$$\n\\frac{d\\langle n \\rangle}{dt} = \\alpha(\\langle n \\rangle + 1) + \\beta(\\langle n^2 \\rangle - \\langle n \\rangle) - \\alpha\\langle n \\rangle - \\beta\\langle n^2 \\rangle\n$$\n$$\n\\frac{d\\langle n \\rangle}{dt} = \\alpha\\langle n \\rangle + \\alpha + \\beta\\langle n^2 \\rangle - \\beta\\langle n \\rangle - \\alpha\\langle n \\rangle - \\beta\\langle n^2 \\rangle = \\alpha - \\beta\\langle n \\rangle\n$$\nAt steady state, the expectation values are time-independent, so $\\frac{d\\langle n \\rangle}{dt} = 0$. Let $\\langle n \\rangle_{ss}$ be the steady-state mean.\n$$\n0 = \\alpha - \\beta \\langle n \\rangle_{ss} \\implies \\langle n \\rangle_{ss} = \\frac{\\alpha}{\\beta}\n$$\n\n**2. Derivation of the steady-state variance $\\operatorname{Var}(n)$**\n\nTo find the variance, $\\operatorname{Var}(n) = \\langle n^2 \\rangle - \\langle n \\rangle^2$, we must first find the steady-state second moment, $\\langle n^2 \\rangle_{ss}$. We follow a similar procedure for the time evolution of $\\langle n^2 \\rangle(t) = \\sum_{n=0}^{\\infty} n^2 P(n, t)$.\n$$\n\\frac{d\\langle n^2 \\rangle}{dt} = \\sum_{n=0}^{\\infty} n^2 \\frac{d P(n, t)}{dt} = \\sum_{n=0}^{\\infty} n^2 \\left[ \\alpha P(n-1, t) + \\beta(n+1) P(n+1, t) - (\\alpha + \\beta n) P(n, t) \\right]\n$$\nAgain, we analyze the sum term by term:\n-   $\\sum_{n=0}^{\\infty} n^2 \\alpha P(n-1, t)$. Let $k = n-1$. The sum is $\\alpha \\sum_{k=0}^{\\infty} (k+1)^2 P(k, t) = \\alpha \\langle (n+1)^2 \\rangle = \\alpha (\\langle n^2 \\rangle + 2\\langle n \\rangle + 1)$.\n-   $\\sum_{n=0}^{\\infty} n^2 \\beta(n+1) P(n+1, t)$. Let $k = n+1$. The sum is $\\beta \\sum_{k=1}^{\\infty} (k-1)^2 k P(k, t) = \\beta \\langle (n-1)^2 n \\rangle = \\beta \\langle n^3 - 2n^2 + n \\rangle = \\beta(\\langle n^3 \\rangle - 2\\langle n^2 \\rangle + \\langle n \\rangle)$.\n-   $\\sum_{n=0}^{\\infty} n^2 [ -(\\alpha + \\beta n) P(n, t)] = - \\alpha \\langle n^2 \\rangle - \\beta \\langle n^3 \\rangle$.\n\nCombining these results:\n$$\n\\frac{d\\langle n^2 \\rangle}{dt} = [\\alpha(\\langle n^2 \\rangle + 2\\langle n \\rangle + 1)] + [\\beta(\\langle n^3 \\rangle - 2\\langle n^2 \\rangle + \\langle n \\rangle)] - [\\alpha\\langle n^2 \\rangle + \\beta\\langle n^3 \\rangle]\n$$\n$$\n\\frac{d\\langle n^2 \\rangle}{dt} = 2\\alpha\\langle n \\rangle + \\alpha - 2\\beta\\langle n^2 \\rangle + \\beta\\langle n \\rangle\n$$\nAt steady state, $\\frac{d\\langle n^2 \\rangle}{dt} = 0$, and we substitute the known steady-state mean $\\langle n \\rangle_{ss} = \\alpha/\\beta$.\n$$\n0 = 2\\alpha\\langle n \\rangle_{ss} + \\alpha - 2\\beta\\langle n^2 \\rangle_{ss} + \\beta\\langle n \\rangle_{ss}\n$$\nSolving for $\\langle n^2 \\rangle_{ss}$:\n$$\n2\\beta\\langle n^2 \\rangle_{ss} = (2\\alpha + \\beta)\\langle n \\rangle_{ss} + \\alpha\n$$\n$$\n2\\beta\\langle n^2 \\rangle_{ss} = (2\\alpha + \\beta)\\left(\\frac{\\alpha}{\\beta}\\right) + \\alpha = \\frac{2\\alpha^2}{\\beta} + \\alpha + \\alpha = \\frac{2\\alpha^2}{\\beta} + 2\\alpha\n$$\n$$\n\\langle n^2 \\rangle_{ss} = \\frac{1}{2\\beta}\\left(\\frac{2\\alpha^2}{\\beta} + 2\\alpha\\right) = \\frac{\\alpha^2}{\\beta^2} + \\frac{\\alpha}{\\beta}\n$$\nNow we compute the steady-state variance, $\\operatorname{Var}(n)_{ss}$:\n$$\n\\operatorname{Var}(n)_{ss} = \\langle n^2 \\rangle_{ss} - (\\langle n \\rangle_{ss})^2 = \\left(\\frac{\\alpha^2}{\\beta^2} + \\frac{\\alpha}{\\beta}\\right) - \\left(\\frac{\\alpha}{\\beta}\\right)^2\n$$\n$$\n\\operatorname{Var}(n)_{ss} = \\frac{\\alpha^2}{\\beta^2} + \\frac{\\alpha}{\\beta} - \\frac{\\alpha^2}{\\beta^2} = \\frac{\\alpha}{\\beta}\n$$\n\n**3. Derivation of the Fano factor $\\mathcal{F}$**\n\nThe Fano factor is defined as the variance-to-mean ratio, $\\mathcal{F} = \\frac{\\operatorname{Var}(n)}{\\langle n \\rangle}$. At steady state:\n$$\n\\mathcal{F}_{ss} = \\frac{\\operatorname{Var}(n)_{ss}}{\\langle n \\rangle_{ss}} = \\frac{\\alpha/\\beta}{\\alpha/\\beta} = 1\n$$\nThis result confirms that the steady-state protein number distribution is Poisson, for which the variance is equal to the mean.\n\nThe requested quantities expressed solely in terms of $\\alpha$ and $\\beta$ are:\n-   Steady-state mean $\\langle n \\rangle = \\frac{\\alpha}{\\beta}$\n-   Steady-state variance $\\operatorname{Var}(n) = \\frac{\\alpha}{\\beta}$\n-   Fano factor $\\mathcal{F} = 1$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\alpha}{\\beta} & \\frac{\\alpha}{\\beta} & 1\n\\end{pmatrix}\n}\n$$", "id": "4334010"}]}