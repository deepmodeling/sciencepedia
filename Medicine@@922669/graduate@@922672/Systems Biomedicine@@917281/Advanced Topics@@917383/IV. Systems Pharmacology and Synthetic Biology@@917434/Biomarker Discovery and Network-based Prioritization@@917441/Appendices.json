{"hands_on_practices": [{"introduction": "A cornerstone of network-based biomarker discovery is the ability to translate high-dimensional omics data into a network of molecular interactions. This first exercise provides hands-on practice in constructing a weighted gene co-expression network, a foundational technique in systems biology. By calculating Pearson correlations from hypothetical gene expression data and applying a soft-thresholding power, you will learn how to create a weighted adjacency matrix that represents the strength of gene-gene relationships, a critical first step for any downstream network analysis [@problem_id:4320638].", "problem": "You are given the task of constructing a weighted gene co-expression network and computing node degrees for use in biomarker discovery and network-based prioritization. Start from the following foundational principles: the Pearson Product-Moment Correlation Coefficient (PPMCC) between two gene expression profiles across samples, the definition of a weighted adjacency constructed from correlation magnitudes, and the graph-theoretic definition of node degree in a weighted network.\n\nDefinitions and requirements:\n- Let there be $n$ samples and $p$ genes. Denote the expression matrix as $X \\in \\mathbb{R}^{n \\times p}$, where column $i$ is the expression vector $x_i \\in \\mathbb{R}^n$ for gene $i$ across $n$ samples.\n- The Pearson Product-Moment Correlation Coefficient (PPMCC) between genes $i$ and $j$ is\n$$\nr_{ij} = \\frac{\\sum_{t=1}^{n} \\left(x_{it} - \\bar{x}_i\\right)\\left(x_{jt} - \\bar{x}_j\\right)}{\\sqrt{\\left(\\sum_{t=1}^{n} \\left(x_{it} - \\bar{x}_i\\right)^2\\right)\\left(\\sum_{t=1}^{n} \\left(x_{jt} - \\bar{x}_j\\right)^2\\right)}},\n$$\nwhere $\\bar{x}_i$ is the mean of $x_i$ across $n$ samples. If either $\\sum_{t=1}^{n} \\left(x_{it} - \\bar{x}_i\\right)^2 = 0$ or $\\sum_{t=1}^{n} \\left(x_{jt} - \\bar{x}_j\\right)^2 = 0$ (that is, a gene is constant across samples), then define $r_{ij} = 0$.\n- Construct a weighted adjacency matrix $W \\in \\mathbb{R}^{p \\times p}$ with entries\n$$\nw_{ij} = \\begin{cases}\n\\left| r_{ij} \\right|^{\\beta},  i \\neq j \\\\\n0,  i = j\n\\end{cases}\n$$\nwhere $\\beta > 0$ is a given soft-thresholding power parameter.\n- The weighted degree for node $i$ is\n$$\nk_i = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{p} w_{ij}.\n$$\n\nYour program must implement these definitions exactly and compute the weighted degree $k_i$ for each gene in the specified test suite. There are no physical units involved. Express each degree $k_i$ as a floating-point number with at least six decimal digits of precision in the final output.\n\nTest suite:\n- Case $1$: $X$ has $n = 6$ samples and $p = 5$ genes.\n  - $x_1 = [1,2,3,4,5,6]$\n  - $x_2 = [6,5,4,3,2,1]$\n  - $x_3 = [2,4,6,8,10,12]$\n  - $x_4 = [1,1,2,2,3,3]$\n  - $x_5 = [3,3,3,3,3,3]$\n  - $\\beta = 2$\n- Case $2$: $X$ has $n = 5$ samples and $p = 4$ genes.\n  - $x_1 = [0,0,0,0,0]$\n  - $x_2 = [1,2,3,4,5]$\n  - $x_3 = [2,1,0,-1,-2]$\n  - $x_4 = [5,4,3,2,1]$\n  - $\\beta = 1$\n- Case $3$: $X$ has $n = 3$ samples and $p = 2$ genes.\n  - $x_1 = [1,2,3]$\n  - $x_2 = [3,2,1]$\n  - $\\beta = 3$\n- Case $4$: $X$ has $n = 4$ samples and $p = 3$ genes.\n  - $x_1 = [1,0,-1,0]$\n  - $x_2 = [1,1,1,1]$\n  - $x_3 = [1,-1,1,-1]$\n  - $\\beta = 10$\n- Case $5$: $X$ has $n = 6$ samples and $p = 3$ genes.\n  - $x_1 = [0,1,0,1,0,1]$\n  - $x_2 = [0.1,0.9,0.2,0.8,0.05,0.95]$\n  - $x_3 = [-0.1,-0.9,-0.2,-0.8,-0.05,-0.95]$\n  - $\\beta = 4$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this top-level list corresponds to one test case and must itself be a list of the $p$ weighted degrees $[k_1, k_2, \\ldots, k_p]$ for that case. For example, the output should look like $[[\\dots],[\\dots],[\\dots],[\\dots],[\\dots]]$ for the five cases above, with floating-point values having at least six decimal digits of precision.", "solution": "We ground the construction of a weighted co-expression network in two well-established principles: correlation-based similarity from statistics and node degree from graph theory. In systems biomedicine, the network-based prioritization of candidate biomarkers often relies on centrality measures derived from co-expression relationships, as in Weighted Gene Co-expression Network Analysis (WGCNA), where an adjacency is built from a soft-thresholded similarity. We proceed as follows.\n\nThe Pearson Product-Moment Correlation Coefficient (PPMCC) between two genes $i$ and $j$ is a standardized measure of linear association. Given $x_i \\in \\mathbb{R}^n$ and $x_j \\in \\mathbb{R}^n$, with sample means $\\bar{x}_i$ and $\\bar{x}_j$, the numerator\n$$\n\\sum_{t=1}^{n} \\left(x_{it} - \\bar{x}_i\\right)\\left(x_{jt} - \\bar{x}_j\\right)\n$$\nis the unnormalized covariance, and the denominator\n$$\n\\sqrt{\\left(\\sum_{t=1}^{n} \\left(x_{it} - \\bar{x}_i\\right)^2\\right)\\left(\\sum_{t=1}^{n} \\left(x_{jt} - \\bar{x}_j\\right)^2\\right)}\n$$\nis the product of the unnormalized standard deviations. Their ratio removes units and scales, yielding $r_{ij} \\in [-1,1]$. This aligns with well-tested statistical practice and is independent of whether one uses divisor $n$ or $n-1$ in covariance, since the factors cancel in the correlation ratio. When a gene is constant across samples, its variance is zero, the denominator is zero, and $r_{ij}$ is undefined; operationally we set $r_{ij} = 0$ for any pair involving such a gene to avoid introducing artificial edges.\n\nTo construct the weighted adjacency, we use the absolute correlation magnitude, $\\left|r_{ij}\\right|$, to reflect co-expression strength irrespective of direction (positive or negative), which is a common and scientifically motivated choice in network-based prioritization when the goal is to quantify connectivity. We then apply a soft-thresholding power $\\beta > 0$:\n$$\nw_{ij} = \\begin{cases}\n\\left| r_{ij} \\right|^{\\beta},  i \\neq j \\\\\n0,  i = j\n\\end{cases}\n$$\nwith $w_{ii} = 0$ to remove self-loops. The parameter $\\beta$ controls emphasis on strong correlations: larger $\\beta$ dampens weaker edges and accentuates stronger ones, a principle used to promote approximate scale-free topology in co-expression networks.\n\nThe weighted degree (also known as strength) of node $i$ is\n$$\nk_i = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{p} w_{ij},\n$$\nwhich quantifies the total connectivity of gene $i$ to all other genes. In biomarker discovery, a larger $k_i$ suggests higher network centrality and potential regulatory importance.\n\nAlgorithmic steps:\n- For each test case, compute $\\bar{x}_i$ for each gene $i$, center each gene vector by subtracting its mean, and compute the unnormalized sum of squares for each gene. For each pair $(i,j)$, compute the unnormalized covariance as the sum of element-wise products of centered vectors. If either gene has zero sum of squares, set $r_{ij} = 0$; otherwise compute\n$$\nr_{ij} = \\frac{\\sum_{t=1}^{n} \\left(x_{it} - \\bar{x}_i\\right)\\left(x_{jt} - \\bar{x}_j\\right)}{\\sqrt{\\left(\\sum_{t=1}^{n} \\left(x_{it} - \\bar{x}_i\\right)^2\\right)\\left(\\sum_{t=1}^{n} \\left(x_{jt} - \\bar{x}_j\\right)^2\\right)}}.\n$$\n- Construct $W$ by $w_{ij} = \\left|r_{ij}\\right|^{\\beta}$ for $i \\neq j$ and $w_{ii} = 0$.\n- Compute $k_i = \\sum_{j \\neq i} w_{ij}$ for each $i$.\n\nEdge cases covered by the test suite:\n- Case $1$ includes perfect positive and negative correlations and a moderate correlation, plus a constant gene ($x_5$). For instance, $x_1$ and $x_3$ are linearly related ($x_3 = 2 x_1$), yielding $r_{13} = 1$ and thus $w_{13} = 1^{2} = 1$, while $x_1$ and $x_2$ are perfectly anticorrelated ($r_{12} = -1$, $w_{12} = 1$). The correlation between $x_1$ and $x_4$ is high but not perfect; specifically,\n$$\nr_{14} = \\frac{8}{\\sqrt{70}} \\approx 0.956182887,\\quad w_{14} \\approx (0.956182887)^{2} \\approx 0.914297,\n$$\nand $x_5$ is constant so all $r_{i5} = 0$ and $w_{i5} = 0$. Weighted degrees $k_i$ aggregate these contributions.\n- Case $2$ emphasizes handling of a constant gene ($x_1$) and perfect negative correlations (e.g., $r_{23} = -1$, $r_{24} = -1$) with $\\beta = 1$, so $w_{ij} = \\left| r_{ij} \\right|$.\n- Case $3$ is a minimal network ($p = 2$) with perfect anticorrelation and $\\beta = 3$, yielding $w_{12} = 1$ and $k_1 = k_2 = 1$.\n- Case $4$ includes a constant gene ($x_2$) and an orthogonal pair ($x_1$ versus $x_3$) producing $r_{13} = 0$; with $\\beta = 10$, all degrees are $0$.\n- Case $5$ contains strong but not perfect correlations and anticorrelations, demonstrating the effect of $\\beta = 4$ in accentuating strong edges while retaining nuanced differences. For example, with centered vectors $x_1$ and $x_2$, one obtains\n$$\nr_{12} \\approx \\frac{1.15}{\\sqrt{1.5 \\cdot 0.915}} \\approx 0.9827,\\quad w_{12} \\approx (0.9827)^{4} \\approx 0.931,\n$$\nand similarly $r_{23} \\approx -0.989$ yields $w_{23} \\approx 0.957$. Degrees $k_i$ sum the respective incident weights.\n\nImplementation details:\n- Use arrays for $X$ and compute means, centered vectors, sums of squares, and pairwise correlations according to the formulas above.\n- Construct $W$ with zero diagonal and apply the power $\\beta$ to $\\left|r_{ij}\\right|$ off-diagonal.\n- Compute $k_i$ by summing along rows excluding the diagonal.\n\nThe program returns a single line containing the list of degree lists for the five cases, formatted as $[[k_1,\\dots,k_p],\\dots]$, with floating-point values carrying at least six decimal digits of precision to ensure stable downstream prioritization comparisons.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_weighted_degrees(X: np.ndarray, beta: float) - list:\n    \"\"\"\n    Compute weighted degrees for a co-expression network:\n    - Pearson correlation r_ij from centered gene expression profiles\n    - Weighted adjacency w_ij = |r_ij|^beta for i != j, w_ii = 0\n    - Weighted degree k_i = sum_{j != i} w_ij\n    \"\"\"\n    # X: shape (n_samples, n_genes)\n    n, p = X.shape\n    # Center columns\n    means = X.mean(axis=0)\n    centered = X - means\n    # Unnormalized sum of squares per gene\n    ss = (centered ** 2).sum(axis=0)\n    # Initialize correlation matrix\n    R = np.zeros((p, p), dtype=float)\n    # Compute pairwise correlations\n    for i in range(p):\n        for j in range(i + 1, p):\n            denom = np.sqrt(ss[i] * ss[j])\n            if denom == 0.0:\n                rij = 0.0\n            else:\n                cov = (centered[:, i] * centered[:, j]).sum()\n                rij = cov / denom\n            R[i, j] = rij\n            R[j, i] = rij\n    # Construct weighted adjacency\n    W = np.abs(R) ** beta\n    np.fill_diagonal(W, 0.0)\n    # Weighted degrees\n    degrees = W.sum(axis=1)\n    # Round to ensure at least six decimal digits of precision in display\n    return [float(np.round(d, 6)) for d in degrees]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: n=6, p=5, beta=2\n        (\n            np.array([\n                [1, 6, 2, 1, 3],\n                [2, 5, 4, 1, 3],\n                [3, 4, 6, 2, 3],\n                [4, 3, 8, 2, 3],\n                [5, 2, 10, 3, 3],\n                [6, 1, 12, 3, 3]\n            ], dtype=float),\n            2.0\n        ),\n        # Case 2: n=5, p=4, beta=1\n        (\n            np.array([\n                [0, 1, 2, 5],\n                [0, 2, 1, 4],\n                [0, 3, 0, 3],\n                [0, 4, -1, 2],\n                [0, 5, -2, 1]\n            ], dtype=float),\n            1.0\n        ),\n        # Case 3: n=3, p=2, beta=3\n        (\n            np.array([\n                [1, 3],\n                [2, 2],\n                [3, 1]\n            ], dtype=float),\n            3.0\n        ),\n        # Case 4: n=4, p=3, beta=10\n        (\n            np.array([\n                [1, 1, 1],\n                [0, 1, -1],\n                [-1, 1, 1],\n                [0, 1, -1]\n            ], dtype=float),\n            10.0\n        ),\n        # Case 5: n=6, p=3, beta=4\n        (\n            np.array([\n                [0.0, 0.1, -0.1],\n                [1.0, 0.9, -0.9],\n                [0.0, 0.2, -0.2],\n                [1.0, 0.8, -0.8],\n                [0.0, 0.05, -0.05],\n                [1.0, 0.95, -0.95]\n            ], dtype=float),\n            4.0\n        ),\n    ]\n\n    results = []\n    for X, beta in test_cases:\n        degrees = compute_weighted_degrees(X, beta)\n        results.append(degrees)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(res) for res in results)}]\")\n\nsolve()\n```", "id": "4320638"}, {"introduction": "With a biological network in hand, the next challenge is to identify the most relevant nodes within it. This practice delves into network diffusion, a powerful method for prioritizing candidate genes by simulating the flow of information from known disease \"seeds.\" You will implement the Random Walk with Restart (RWR) algorithm and critically compare two common normalization schemes, discovering how the choice of normalization can impact the ranking of network hubs and, consequently, the final list of prioritized biomarkers [@problem_id:4320621].", "problem": "Consider a gene or protein interaction network represented as a finite, simple, undirected, weighted graph with $n$ nodes. Let the network be encoded by an adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$ where $A_{ij} \\ge 0$ and $A_{ij} = A_{ji}$. Let the degree of node $i$ be $d_i = \\sum_{j=1}^{n} A_{ij}$, and the degree matrix be $D = \\mathrm{diag}(d_1,\\dots,d_n)$. In network-based biomarker prioritization, diffusion of an initial signal $s \\in \\mathbb{R}^{n}$ across the network is used to obtain a final prioritization score $x \\in \\mathbb{R}^{n}$, and nodes are ranked by these scores. Two commonly used degree-normalizations for diffusion are a row-stochastic normalization and a symmetric normalization, which may yield different rankings, particularly for high-degree nodes (hubs).\n\nStarting from the foundational principles of probability conservation in a Markov chain and the normalization of graph signals, implement an iterative diffusion process consistent with these principles. Use the following specifications:\n\n1. Diffusion state update must obey the idea of repeated propagation with reinjection of the initial signal, capturing the concept of random walk with restart (RWR). At each iteration, the new state must be computed by combining a propagated component and a reinjected component of the initial signal $s$, controlled by a parameter $\\alpha$ with $0  \\alpha  1$, ensuring a contraction and convergence to a fixed point.\n\n2. Implement two variants of the propagation operator $W$:\n   - Row-stochastic normalization: construct a matrix $W_{\\mathrm{row}}$ such that rows sum to $1$ for nodes with $d_i  0$, based on $D$ and $A$. Nodes with $d_i = 0$ must not propagate any mass.\n   - Symmetric normalization: construct a matrix $W_{\\mathrm{sym}}$ derived from symmetric degree normalization using $D$ and $A$ to reduce hub dominance and account for the geometry of the graph. Nodes with $d_i = 0$ must not propagate any mass.\n\n3. For a given seed set $S \\subset \\{1,\\dots,n\\}$, define an initial signal $s$ that places equal mass on nodes in $S$ and zero on others. The mass must sum to $1$.\n\n4. Iterate the diffusion until convergence, measured by the condition that the norm of the difference between successive iterates falls below a specified tolerance, or until a maximum number of iterations is reached.\n\n5. After convergence, compute the final scores $x_{\\mathrm{row}}$ and $x_{\\mathrm{sym}}$ for $W_{\\mathrm{row}}$ and $W_{\\mathrm{sym}}$, respectively. Rank nodes in descending order by score. In case of ties, break ties by smaller node index (that is, a node with a smaller index gets the better rank). Convert the ordering to rank positions $r_{\\mathrm{row}}(i)$ and $r_{\\mathrm{sym}}(i)$ with $1$ as the best rank.\n\n6. Define the set of high-degree nodes $H$ as the top $k$ nodes by degree $d_i$, with ties broken by smaller index. Compute the signed rank changes for high-degree nodes, $\\Delta r(i) = r_{\\mathrm{sym}}(i) - r_{\\mathrm{row}}(i)$ for all $i \\in H$. A positive $\\Delta r(i)$ means the node ranks worse (higher numeric rank) under symmetric normalization compared to row-stochastic normalization; a negative value means it ranks better.\n\n7. For each test case, produce three outputs:\n   - The mean absolute rank change over $H$ as a real number.\n   - The maximum absolute rank change over $H$ as an integer.\n   - The list of signed rank changes $\\left[\\Delta r(i)\\right]_{i \\in H}$ ordered by increasing node index.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[result\\_1,result\\_2,\\dots]$), where each $result\\_t$ is itself a list $[\\text{mean\\_abs\\_change},\\text{max\\_abs\\_change},[\\Delta r(i) \\text{ for } i \\in H]]$.\n\nImplement the above for the following test suite. Each test case provides $A$, a seed set $S$, a restart parameter $\\alpha$, a high-degree cut $k$, a maximum number of iterations $T_{\\max}$, and a tolerance $\\varepsilon$. The matrices are given in explicit form with rows and columns indexed from $0$ to $n-1$.\n\nTest Case $1$ (happy path, single connected component with one hub):\n- $n = 6$\n- $$A = \\begin{bmatrix}\n0  3  2  4  1  1 \\\\\n3  0  2  0  0  0 \\\\\n2  2  0  1  0  0 \\\\\n4  0  1  0  2  0 \\\\\n1  0  0  2  0  3 \\\\\n1  0  0  0  3  0\n\\end{bmatrix}$$\n- $S = \\{4,5\\}$\n- $\\alpha = 0.85$\n- $k = 2$\n- $T_{\\max} = 10000$\n- $\\varepsilon = 10^{-12}$\n\nTest Case $2$ (edge case with a star-like hub and a local triangle):\n- $n = 7$\n- $$A = \\begin{bmatrix}\n0  2  2  2  2  2  0 \\\\\n2  0  0  0  0  0  0 \\\\\n2  0  0  0  0  0  0 \\\\\n2  0  0  0  0  2  1 \\\\\n2  0  0  0  0  0  0 \\\\\n2  0  0  2  0  0  3 \\\\\n0  0  0  1  0  3  0\n\\end{bmatrix}$$\n- $S = \\{0\\}$\n- $\\alpha = 0.90$\n- $k = 2$\n- $T_{\\max} = 10000$\n- $\\varepsilon = 10^{-12}$\n\nTest Case $3$ (boundary condition with two disconnected components and hubs in each):\n- $n = 8$\n- $$A = \\begin{bmatrix}\n0  4  0  0  0  0  0  0 \\\\\n4  0  3  2  0  0  0  0 \\\\\n0  3  0  1  0  0  0  0 \\\\\n0  2  1  0  0  0  0  0 \\\\\n0  0  0  0  0  1  2  0 \\\\\n0  0  0  0  1  0  2  1 \\\\\n0  0  0  0  2  2  0  2 \\\\\n0  0  0  0  0  1  2  0\n\\end{bmatrix}$$\n- $S = \\{0,6\\}$\n- $\\alpha = 0.70$\n- $k = 3$\n- $T_{\\max} = 10000$\n- $\\varepsilon = 10^{-12}$\n\nAll outputs must be produced without any external input. Your program must use these exact test cases and output a single line in the exact format described above.", "solution": "### Step 1: Extract Givens\n\nThe problem provides the following data and definitions:\n- A gene/protein interaction network is a finite, simple, undirected, weighted graph with $n$ nodes.\n- Adjacency matrix: $A \\in \\mathbb{R}^{n \\times n}$ with $A_{ij} \\ge 0$ and $A_{ij} = A_{ji}$.\n- Degree of node $i$: $d_i = \\sum_{j=1}^{n} A_{ij}$.\n- Degree matrix: $D = \\mathrm{diag}(d_1,\\dots,d_n)$.\n- Initial signal vector: $s \\in \\mathbb{R}^{n}$.\n- Final prioritization score vector: $x \\in \\mathbb{R}^{n}$.\n- Diffusion update rule: Iterative process based on \"repeated propagation with reinjection of the initial signal $s$,\" controlled by a parameter $\\alpha$ with $0  \\alpha  1$.\n- Propagation operators $W$:\n    - Row-stochastic normalization: $W_{\\mathrm{row}}$, where rows sum to $1$ for nodes with $d_i  0$. Nodes with $d_i = 0$ do not propagate mass.\n    - Symmetric normalization: $W_{\\mathrm{sym}}$, derived from symmetric degree normalization using $D$ and $A$. Nodes with $d_i = 0$ do not propagate mass.\n- Initial signal definition: For a seed set $S$, $s$ has equal mass on nodes in $S$ and zero on others, with total mass summing to $1$.\n- Convergence criteria: The L2 norm of the difference between successive iterates is less than a tolerance $\\varepsilon$, or a maximum number of iterations $T_{\\max}$ is reached.\n- Ranking: Nodes are ranked in descending order of score. Ties are broken by smaller node index. Ranks $r(i)$ start from $1$.\n- High-degree nodes $H$: The set of top $k$ nodes by degree $d_i$, with ties broken by smaller node index.\n- Signed rank change for $i \\in H$: $\\Delta r(i) = r_{\\mathrm{sym}}(i) - r_{\\mathrm{row}}(i)$.\n- Required outputs for each test case: a list containing the mean absolute rank change over $H$, the maximum absolute rank change over $H$, and a list of signed rank changes $[\\Delta r(i)]_{i \\in H}$ ordered by increasing node index.\n- Test Case 1: $n=6$, $A=\\begin{bmatrix} 0  3  2  4  1  1 \\\\ 3  0  2  0  0  0 \\\\ 2  2  0  1  0  0 \\\\ 4  0  1  0  2  0 \\\\ 1  0  0  2  0  3 \\\\ 1  0  0  0  3  0 \\end{bmatrix}$, $S=\\{4,5\\}$, $\\alpha=0.85$, $k=2$, $T_{\\max}=10000$, $\\varepsilon=10^{-12}$.\n- Test Case 2: $n=7$, $A=\\begin{bmatrix} 0  2  2  2  2  2  0 \\\\ 2  0  0  0  0  0  0 \\\\ 2  0  0  0  0  0  0 \\\\ 2  0  0  0  0  2  1 \\\\ 2  0  0  0  0  0  0 \\\\ 2  0  0  2  0  0  3 \\\\ 0  0  0  1  0  3  0 \\end{bmatrix}$, $S=\\{0\\}$, $\\alpha=0.90$, $k=2$, $T_{\\max}=10000$, $\\varepsilon=10^{-12}$.\n- Test Case 3: $n=8$, $A=\\begin{bmatrix} 0  4  0  0  0  0  0  0 \\\\ 4  0  3  2  0  0  0  0 \\\\ 0  3  0  1  0  0  0  0 \\\\ 0  2  1  0  0  0  0  0 \\\\ 0  0  0  0  0  1  2  0 \\\\ 0  0  0  0  1  0  2  1 \\\\ 0  0  0  0  2  2  0  2 \\\\ 0  0  0  0  0  1  2  0 \\end{bmatrix}$, $S=\\{0,6\\}$, $\\alpha=0.70$, $k=3$, $T_{\\max}=10000$, $\\varepsilon=10^{-12}$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is evaluated against the validation criteria.\n\n- **Scientifically Grounded**: The problem is based on well-established principles of network science and graph theory, specifically the Random Walk with Restart (RWR) algorithm. The concepts of adjacency matrices, degree normalization (row-stochastic and symmetric), and iterative diffusion are standard in computational systems biology for tasks like gene prioritization. The problem is scientifically sound.\n- **Well-Posed**: The problem is well-posed. It provides all necessary parameters ($A, S, \\alpha, k, T_{\\max}, \\varepsilon$) for each test case. The description of the RWR algorithm as a combination of propagation and reinjection, controlled by $\\alpha$, corresponds to an iterative process $x_{t+1} = \\alpha W x_t + (1-\\alpha)s$. For $0  \\alpha  1$ and $W$ being a valid propagation operator (with spectral radius $\\rho(W) \\le 1$), this process is a contraction mapping, which guarantees the existence of a unique and stable fixed-point solution. The tie-breaking rules for ranking and hub selection ensure a unique outcome.\n- **Objective**: All terms are defined mathematically and procedurally. The language is precise and free of subjective or ambiguous statements.\n- **Completeness and Consistency**: The problem is self-contained. The definitions for the propagation operators, initial signal, convergence, ranking, and final metrics are all provided and are mutually consistent. The handling of nodes with degree $d_i=0$ is specified, preventing division by zero and ensuring the algorithm is robust.\n- **Realism**: The network sizes and parameter values are realistic for a computational problem and pose no feasibility issues.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. It is a well-defined, scientifically grounded, and computationally tractable problem from the field of network biology. I will now proceed to construct the solution.\n\n### Principle-Based Design of the Solution\n\nThe core of the problem is to implement and compare two variants of the Random Walk with Restart (RWR) algorithm on a graph. The algorithm simulates a process where a signal diffuses through the network, and at each step, it can either continue to propagate to neighbors or \"restart\" from a set of seed nodes.\n\n**1. Network and Signal Initialization**\nThe network is represented by its adjacency matrix $A$. The degree $d_i$ of each node $i$ is calculated as the sum of the weights of its incident edges, $d_i = \\sum_{j} A_{ij}$. The initial signal $s$ is a vector representing the starting distribution of the \"signal mass\". For a given set of seed nodes $S$, the signal is distributed uniformly among them, such that $s_i = 1/|S|$ if node $i$ is in $S$, and $s_i = 0$ otherwise. This ensures the total signal mass is normalized, i.e., $\\sum_{i} s_i = 1$.\n\n**2. Propagation Operators**\nThe diffusion process is governed by a propagation operator $W$, which is a normalized version of the adjacency matrix $A$. Two normalization schemes are specified:\n\n- **Row-Stochastic Normalization ($W_{\\mathrm{row}}$)**: This normalization models a classic random walk on the graph. The transition probability from node $i$ to node $j$ is proportional to the edge weight $A_{ij}$. We construct $W_{\\mathrm{row}} = D^{-1}A$, where $D^{-1}$ is the inverse of the degree matrix $D$. For a node $i$ with degree $d_i  0$, the $(i,i)$-th entry of $D^{-1}$ is $1/d_i$. This ensures that each row of $W_{\\mathrm{row}}$ corresponding to a connected node sums to $1$, conserving probability mass locally. If $d_i = 0$, the corresponding entry in $D^{-1}$ is $0$, ensuring isolated nodes do not propagate any signal, as specified.\n\n- **Symmetric Normalization ($W_{\\mathrm{sym}}$)**: This normalization is often preferred as it makes the operator symmetric, which corresponds to a reversible Markov chain and has favorable spectral properties. It is defined as $W_{\\mathrm{sym}} = D^{-1/2} A D^{-1/2}$, where $D^{-1/2}$ is a diagonal matrix with entries $1/\\sqrt{d_i}$ for nodes with $d_i  0$, and $0$ otherwise. This normalization tends to reduce the influence of high-degree nodes (hubs), as the weights of edges connected to them are down-weighted more heavily than in the row-stochastic case.\n\n**3. Iterative Diffusion (Random Walk with Restart)**\nThe diffusion process is an iterative update of the score vector $x$, starting with $x_0 = s$. The update rule combines propagation with reinjection:\n$$x_{t+1} = \\alpha W x_t + (1-\\alpha)s$$\nHere, $\\alpha \\in (0, 1)$ is the continuation probability. At each step $t$, a fraction $\\alpha$ of the signal at each node propagates to its neighbors according to $W$, while the remaining fraction $(1-\\alpha)$ is replenished from the initial source distribution $s$. This process is guaranteed to converge to a unique steady-state distribution $x_{ss}$ because the iterative map is a contraction. The iteration continues until the change between successive score vectors is negligible, i.e., $\\|x_{t+1} - x_t\\|_2  \\varepsilon$, or until a maximum number of iterations $T_{\\max}$ is reached.\n\n**4. Ranking and Comparison**\nUpon convergence, we obtain two final score vectors, $x_{\\mathrm{row}}$ and $x_{\\mathrm{sym}}$, corresponding to the two normalization schemes. To compare their effects, we perform the following steps:\n- **Ranking**: For each score vector, nodes are ranked from $1$ to $n$ based on their scores in descending order. Ties in score are broken by assigning a better rank to the node with the smaller index. This yields two rank vectors, $r_{\\mathrm{row}}$ and $r_{\\mathrm{sym}}$.\n- **High-Degree Node Identification**: The set $H$ of high-degree nodes is identified by finding the $k$ nodes with the highest degrees $d_i$. Ties in degree are broken by selecting the node with the smaller index.\n- **Rank Change Analysis**: For each node $i$ in the high-degree set $H$, the signed rank change $\\Delta r(i) = r_{\\mathrm{sym}}(i) - r_{\\mathrm{row}}(i)$ is computed. A positive $\\Delta r(i)$ indicates that the node is ranked less favorably (i.e., has a larger rank number) under symmetric normalization compared to row-stochastic normalization.\n- **Final Metrics**: The comparison is summarized by three metrics:\n    1. The mean of the absolute rank changes for all nodes in $H$: $\\frac{1}{k} \\sum_{i \\in H} |\\Delta r(i)|$.\n    2. The maximum of the absolute rank changes over $H$: $\\max_{i \\in H} |\\Delta r(i)|$.\n    3. The list of signed rank changes $[\\Delta r(i)]_{i \\in H}$, ordered by increasing node index $i$.\n\nThis structured analysis will systematically reveal how the choice of normalization impacts the prioritization of hub nodes in a network diffusion context.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the execution for all test cases.\n    It defines the test cases and calls the core analysis function for each,\n    then formats and prints the final output.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"A\": np.array([\n                [0, 3, 2, 4, 1, 1],\n                [3, 0, 2, 0, 0, 0],\n                [2, 2, 0, 1, 0, 0],\n                [4, 0, 1, 0, 2, 0],\n                [1, 0, 0, 2, 0, 3],\n                [1, 0, 0, 0, 3, 0]\n            ]),\n            \"S\": {4, 5},\n            \"alpha\": 0.85,\n            \"k\": 2,\n            \"T_max\": 10000,\n            \"epsilon\": 1e-12\n        },\n        {\n            \"A\": np.array([\n                [0, 2, 2, 2, 2, 2, 0],\n                [2, 0, 0, 0, 0, 0, 0],\n                [2, 0, 0, 0, 0, 0, 0],\n                [2, 0, 0, 0, 0, 2, 1],\n                [2, 0, 0, 0, 0, 0, 0],\n                [2, 0, 0, 2, 0, 0, 3],\n                [0, 0, 0, 1, 0, 3, 0]\n            ]),\n            \"S\": {0},\n            \"alpha\": 0.90,\n            \"k\": 2,\n            \"T_max\": 10000,\n            \"epsilon\": 1e-12\n        },\n        {\n            \"A\": np.array([\n                [0, 4, 0, 0, 0, 0, 0, 0],\n                [4, 0, 3, 2, 0, 0, 0, 0],\n                [0, 3, 0, 1, 0, 0, 0, 0],\n                [0, 2, 1, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 1, 2, 0],\n                [0, 0, 0, 0, 1, 0, 2, 1],\n                [0, 0, 0, 0, 2, 2, 0, 2],\n                [0, 0, 0, 0, 0, 1, 2, 0]\n            ]),\n            \"S\": {0, 6},\n            \"alpha\": 0.70,\n            \"k\": 3,\n            \"T_max\": 10000,\n            \"epsilon\": 1e-12\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_diffusion_analysis(\n            A=case[\"A\"],\n            S=case[\"S\"],\n            alpha=case[\"alpha\"],\n            k=case[\"k\"],\n            T_max=case[\"T_max\"],\n            epsilon=case[\"epsilon\"]\n        )\n        results.append(result)\n\n    # Custom string formatting to avoid spaces and ensure required output format\n    result_strs = []\n    for res in results:\n        mean_val, max_val, delta_list = res\n        delta_str = '[' + ','.join(map(str, delta_list)) + ']'\n        result_strs.append(f'[{mean_val},{max_val},{delta_str}]')\n    \n    print(f\"[{','.join(result_strs)}]\")\n\ndef run_diffusion_analysis(A, S, alpha, k, T_max, epsilon):\n    \"\"\"\n    Performs the full diffusion analysis for a single test case.\n    \"\"\"\n    n = A.shape[0]\n\n    # Step 1: Calculate degrees and identify the high-degree node set H\n    degrees = A.sum(axis=1)\n    node_degree_pairs = [(-degrees[i], i) for i in range(n)]\n    node_degree_pairs.sort()\n    H = sorted([node_degree_pairs[i][1] for i in range(k)])\n\n    # Step 2: Construct the initial signal vector s\n    s = np.zeros(n)\n    if S:\n        s[[i for i in S]] = 1.0 / len(S)\n\n    # Step 3: Define the diffusion iterator\n    def perform_rwr(W, s_vec, alpha_param, T_max_iter, eps):\n        x = np.copy(s_vec)\n        reinjection_term = (1 - alpha_param) * s_vec\n        for _ in range(T_max_iter):\n            x_new = alpha_param * (W @ x) + reinjection_term\n            if np.linalg.norm(x_new - x)  eps:\n                return x_new\n            x = x_new\n        return x\n\n    # Step 4: Construct propagation operators W_row and W_sym\n    # Avoid division by zero for nodes with degree 0\n    with np.errstate(divide='ignore', invalid='ignore'):\n        d_inv = np.array([1.0 / d if d  0 else 0 for d in degrees])\n        d_inv_sqrt = np.array([1.0 / np.sqrt(d) if d  0 else 0 for d in degrees])\n    \n    D_inv = np.diag(d_inv)\n    D_inv_sqrt = np.diag(d_inv_sqrt)\n\n    W_row = D_inv @ A\n    W_sym = D_inv_sqrt @ A @ D_inv_sqrt\n\n    # Step 5: Run diffusion for both normalizations\n    x_row = perform_rwr(W_row, s, alpha, T_max, epsilon)\n    x_sym = perform_rwr(W_sym, s, alpha, T_max, epsilon)\n\n    # Step 6: Define a ranking function\n    def get_ranks(scores):\n        n_nodes = len(scores)\n        # Sort by score (desc) and then index (asc)\n        sorted_nodes = sorted(range(n_nodes), key=lambda i: (-scores[i], i))\n        ranks = np.zeros(n_nodes, dtype=int)\n        for rank_pos, node_idx in enumerate(sorted_nodes):\n            ranks[node_idx] = rank_pos + 1\n        return ranks\n\n    # Step 7: Calculate ranks\n    r_row = get_ranks(x_row)\n    r_sym = get_ranks(x_sym)\n\n    # Step 8: Calculate rank changes and summary metrics for nodes in H\n    delta_r_values = [int(r_sym[i] - r_row[i]) for i in H]\n    abs_delta_r_values = np.abs(delta_r_values)\n    \n    mean_abs_change = np.mean(abs_delta_r_values)\n    max_abs_change = int(np.max(abs_delta_r_values)) if delta_r_values else 0\n    \n    return [mean_abs_change, max_abs_change, delta_r_values]\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "4320621"}, {"introduction": "The final step in our workflow is to assess the clinical potential of a prioritized biomarker. This exercise transitions from network prioritization to statistical validation, focusing on how to determine a biomarker's diagnostic utility from its score distribution in diseased and healthy populations. Using the Youden index, you will derive an optimal decision threshold for a hypothetical biomarker and calculate key performance metrics like sensitivity, specificity, and predictive values, providing a quantitative foundation for evaluating its real-world effectiveness [@problem_id:4320645].", "problem": "A systems biomedicine study integrates multi-omics features on a protein-interaction network to produce a continuous network-based risk score for each subject. For a candidate biomarker panel discovered by network diffusion and module prioritization, suppose the aggregated score $S$ follows approximately Gaussian class-conditional distributions with equal variance: in diseased subjects $S \\mid D \\sim \\mathcal{N}(\\mu_{1}, \\sigma^{2})$, and in non-diseased controls $S \\mid \\neg D \\sim \\mathcal{N}(\\mu_{0}, \\sigma^{2})$. A decision rule classifies a subject as biomarker-positive if $S \\ge t$ for some threshold $t$. The Receiver Operating Characteristic (ROC) curve is traced out by varying $t$, where the True Positive Rate (TPR, sensitivity) is $P(S \\ge t \\mid D)$ and the False Positive Rate (FPR, one minus specificity) is $P(S \\ge t \\mid \\neg D)$. The Youden index is $J(t) = \\mathrm{TPR}(t) - \\mathrm{FPR}(t)$.\n\nAssume the following empirically estimated parameters from a discovery cohort: $\\mu_{0} = 0.0$, $\\mu_{1} = 1.2$, and $\\sigma = 0.8$. In the intended clinical application cohort, the disease prevalence is $p = 0.2$. Starting only from the definitions above and standard properties of the Gaussian distribution and Bayesâ€™ rule, determine the threshold $t^{\\star}$ that maximizes the Youden index $J(t)$, and then compute the sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) at $t^{\\star}$. Use the standard normal cumulative distribution function $\\Phi(\\cdot)$ when needed.\n\nReport your final numerical results in the order $t^{\\star}$, sensitivity, specificity, PPV, NPV. Round your answer to four significant figures.", "solution": "The problem is deemed valid as it is scientifically grounded in biostatistics, well-posed with all necessary information provided, and objective in its formulation. It contains no logical contradictions, unrealistic assumptions, or ambiguities.\n\nThe task is to find the optimal decision threshold $t^{\\star}$ that maximizes the Youden index $J(t)$, and subsequently calculate the sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) at this threshold.\n\nThe score $S$ is modeled by class-conditional Gaussian distributions:\n- For diseased subjects ($D$): $S \\mid D \\sim \\mathcal{N}(\\mu_{1}, \\sigma^{2})$\n- For non-diseased controls ($\\neg D$): $S \\mid \\neg D \\sim \\mathcal{N}(\\mu_{0}, \\sigma^{2})$\n\nThe given parameters are $\\mu_{0} = 0.0$, $\\mu_{1} = 1.2$, $\\sigma = 0.8$, and the disease prevalence is $p = P(D) = 0.2$.\n\nFirst, we express the True Positive Rate (TPR) and False Positive Rate (FPR) in terms of the standard normal cumulative distribution function, $\\Phi(z) = P(Z \\le z)$ where $Z \\sim \\mathcal{N}(0, 1)$.\n\nThe TPR, or sensitivity, is the probability of a positive test given disease:\n$$\n\\mathrm{TPR}(t) = P(S \\ge t \\mid D) = P\\left(\\frac{S - \\mu_{1}}{\\sigma} \\ge \\frac{t - \\mu_{1}}{\\sigma}\\right) = P\\left(Z \\ge \\frac{t - \\mu_{1}}{\\sigma}\\right)\n$$\nUsing the identity $P(Z \\ge z) = 1 - P(Z  z) = 1 - \\Phi(z)$, we have:\n$$\n\\mathrm{TPR}(t) = 1 - \\Phi\\left(\\frac{t - \\mu_{1}}{\\sigma}\\right)\n$$\n\nThe FPR is the probability of a positive test given no disease:\n$$\n\\mathrm{FPR}(t) = P(S \\ge t \\mid \\neg D) = P\\left(\\frac{S - \\mu_{0}}{\\sigma} \\ge \\frac{t - \\mu_{0}}{\\sigma}\\right) = P\\left(Z \\ge \\frac{t - \\mu_{0}}{\\sigma}\\right)\n$$\n$$\n\\mathrm{FPR}(t) = 1 - \\Phi\\left(\\frac{t - \\mu_{0}}{\\sigma}\\right)\n$$\n\nThe Youden index, $J(t)$, is defined as:\n$$\nJ(t) = \\mathrm{TPR}(t) - \\mathrm{FPR}(t) = \\left[1 - \\Phi\\left(\\frac{t - \\mu_{1}}{\\sigma}\\right)\\right] - \\left[1 - \\Phi\\left(\\frac{t - \\mu_{0}}{\\sigma}\\right)\\right] = \\Phi\\left(\\frac{t - \\mu_{0}}{\\sigma}\\right) - \\Phi\\left(\\frac{t - \\mu_{1}}{\\sigma}\\right)\n$$\n\nTo find the threshold $t^{\\star}$ that maximizes $J(t)$, we take the derivative of $J(t)$ with respect to $t$ and set it to zero. The derivative of $\\Phi(x)$ is the standard normal probability density function (PDF), $\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)$. Using the chain rule:\n$$\n\\frac{dJ}{dt} = \\frac{d}{dt}\\left[\\Phi\\left(\\frac{t - \\mu_{0}}{\\sigma}\\right) - \\Phi\\left(\\frac{t - \\mu_{1}}{\\sigma}\\right)\\right] = \\frac{1}{\\sigma}\\phi\\left(\\frac{t - \\mu_{0}}{\\sigma}\\right) - \\frac{1}{\\sigma}\\phi\\left(\\frac{t - \\mu_{1}}{\\sigma}\\right)\n$$\nSetting the derivative to zero:\n$$\n\\frac{1}{\\sigma}\\left[\\phi\\left(\\frac{t - \\mu_{0}}{\\sigma}\\right) - \\phi\\left(\\frac{t - \\mu_{1}}{\\sigma}\\right)\\right] = 0 \\implies \\phi\\left(\\frac{t - \\mu_{0}}{\\sigma}\\right) = \\phi\\left(\\frac{t - \\mu_{1}}{\\sigma}\\right)\n$$\nSince $\\phi(x)$ is an even function, i.e., $\\phi(x) = \\phi(-x)$, this equality holds if the arguments are equal or are negatives of each other. The arguments cannot be equal, as that would imply $\\mu_0 = \\mu_1$, which contradicts the given values. Therefore, their arguments must be opposite in sign:\n$$\n\\frac{t - \\mu_{0}}{\\sigma} = -\\left(\\frac{t - \\mu_{1}}{\\sigma}\\right)\n$$\n$$\nt - \\mu_{0} = -t + \\mu_{1} \\implies 2t = \\mu_{0} + \\mu_{1}\n$$\nThe optimal threshold $t^{\\star}$ is the midpoint of the means:\n$$\nt^{\\star} = \\frac{\\mu_{0} + \\mu_{1}}{2}\n$$\nSubstituting the given values $\\mu_{0} = 0.0$ and $\\mu_{1} = 1.2$:\n$$\nt^{\\star} = \\frac{0.0 + 1.2}{2} = 0.6\n$$\n\nNext, we compute the diagnostic performance metrics at $t^{\\star} = 0.6$.\n\n1.  **Sensitivity** (TPR at $t^{\\star}$):\n    $$\n    \\text{Sensitivity} = 1 - \\Phi\\left(\\frac{t^{\\star} - \\mu_{1}}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{0.6 - 1.2}{0.8}\\right) = 1 - \\Phi\\left(\\frac{-0.6}{0.8}\\right) = 1 - \\Phi(-0.75)\n    $$\n    Using the property $\\Phi(-z) = 1 - \\Phi(z)$, sensitivity becomes:\n    $$\n    \\text{Sensitivity} = 1 - (1 - \\Phi(0.75)) = \\Phi(0.75) \\approx 0.77337\n    $$\n    Rounded to four significant figures, Sensitivity $\\approx 0.7734$.\n\n2.  **Specificity**:\n    Specificity is $1 - \\mathrm{FPR}(t^{\\star})$.\n    $$\n    \\mathrm{FPR}(t^{\\star}) = 1 - \\Phi\\left(\\frac{t^{\\star} - \\mu_{0}}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{0.6 - 0.0}{0.8}\\right) = 1 - \\Phi(0.75)\n    $$\n    $$\n    \\text{Specificity} = 1 - \\mathrm{FPR}(t^{\\star}) = 1 - (1 - \\Phi(0.75)) = \\Phi(0.75) \\approx 0.77337\n    $$\n    Rounded to four significant figures, Specificity $\\approx 0.7734$.\n\n3.  **Positive Predictive Value (PPV)**:\n    PPV is the probability of having the disease given a positive test result, $P(D \\mid S \\ge t^{\\star})$. Using Bayes' rule:\n    $$\n    \\mathrm{PPV} = \\frac{P(S \\ge t^{\\star} \\mid D)P(D)}{P(S \\ge t^{\\star} \\mid D)P(D) + P(S \\ge t^{\\star} \\mid \\neg D)P(\\neg D)} = \\frac{\\text{Sensitivity} \\cdot p}{\\text{Sensitivity} \\cdot p + \\mathrm{FPR} \\cdot (1-p)}\n    $$\n    Using $p = 0.2$, Sensitivity $=\\Phi(0.75)$, and FPR $=1-\\Phi(0.75)$:\n    $$\n    \\mathrm{PPV} = \\frac{\\Phi(0.75) \\cdot 0.2}{\\Phi(0.75) \\cdot 0.2 + (1-\\Phi(0.75)) \\cdot 0.8}\n    $$\n    $$\n    \\mathrm{PPV} \\approx \\frac{0.77337 \\times 0.2}{0.77337 \\times 0.2 + (1-0.77337) \\times 0.8} = \\frac{0.154674}{0.154674 + 0.22663 \\times 0.8} = \\frac{0.154674}{0.154674 + 0.181304} = \\frac{0.154674}{0.335978} \\approx 0.460377\n    $$\n    Rounded to four significant figures, PPV $\\approx 0.4604$.\n\n4.  **Negative Predictive Value (NPV)**:\n    NPV is the probability of not having the disease given a negative test result, $P(\\neg D \\mid S  t^{\\star})$. Using Bayes' rule:\n    $$\n    \\mathrm{NPV} = \\frac{P(S  t^{\\star} \\mid \\neg D)P(\\neg D)}{P(S  t^{\\star} \\mid \\neg D)P(\\neg D) + P(S  t^{\\star} \\mid D)P(D)}\n    $$\n    Note that $P(S  t^{\\star} \\mid \\neg D) = 1 - \\mathrm{FPR} = \\text{Specificity}$ and $P(S  t^{\\star} \\mid D) = 1 - \\mathrm{TPR} = 1 - \\text{Sensitivity}$.\n    $$\n    \\mathrm{NPV} = \\frac{\\text{Specificity} \\cdot (1-p)}{\\text{Specificity} \\cdot (1-p) + (1-\\text{Sensitivity}) \\cdot p}\n    $$\n    Since Sensitivity = Specificity at $t^{\\star}$, this simplifies:\n    $$\n    \\mathrm{NPV} = \\frac{\\Phi(0.75) \\cdot 0.8}{\\Phi(0.75) \\cdot 0.8 + (1-\\Phi(0.75)) \\cdot 0.2}\n    $$\n    $$\n    \\mathrm{NPV} \\approx \\frac{0.77337 \\times 0.8}{0.77337 \\times 0.8 + (1-0.77337) \\times 0.2} = \\frac{0.618696}{0.618696 + 0.22663 \\times 0.2} = \\frac{0.618696}{0.618696 + 0.045326} = \\frac{0.618696}{0.664022} \\approx 0.931742\n    $$\n    Rounded to four significant figures, NPV $\\approx 0.9317$.\n\nThe final results, rounded to four significant figures, are:\n- Optimal threshold $t^{\\star} = 0.6000$\n- Sensitivity = $0.7734$\n- Specificity = $0.7734$\n- PPV = $0.4604$\n- NPV = $0.9317$", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.6000  0.7734  0.7734  0.4604  0.9317 \\end{pmatrix}}\n$$", "id": "4320645"}]}