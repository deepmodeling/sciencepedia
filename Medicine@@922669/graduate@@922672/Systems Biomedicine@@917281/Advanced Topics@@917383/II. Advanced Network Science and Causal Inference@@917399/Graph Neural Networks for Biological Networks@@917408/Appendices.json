{"hands_on_practices": [{"introduction": "The power of Graph Neural Networks stems from their ability to learn representations by passing \"messages\" between connected nodes. This fundamental exercise walks you through a single step of this message-passing and aggregation process, providing a concrete calculation of how a protein's feature vector is updated based on its neighbors in a hypothetical protein-protein interaction network. Mastering this core mechanic is the first step toward understanding how GNNs learn complex patterns in biological systems [@problem_id:1436694].", "problem": "In the field of systems biology, Graph Neural Networks (GNNs) are powerful tools for learning from data structured as networks, such as protein-protein interaction (PPI) networks. Consider a simple GNN model designed to update the features of proteins based on their connections.\n\nWe are focused on a small part of a PPI network involving a central protein, Protein V, which interacts with two neighboring proteins, Protein U1 and Protein U2. Each protein is described by a 2-dimensional feature vector. The first element of the vector represents the protein's normalized expression level, and the second element represents its mitochondrial localization score.\n\nThe initial feature vectors are given as:\n- Protein V: $h_V = [0.1, 0.9]$\n- Protein U1: $h_{U1} = [0.2, 0.3]$\n- Protein U2: $h_{U2} = [0.6, 0.1]$\n\nThe GNN performs an update to find a new feature vector for Protein V, denoted as $h'_V$, in a single message-passing step. This process involves two stages:\n\n1.  **Aggregation:** An aggregated neighbor vector, $h_{N(V)}$, is created by taking the element-wise sum of the feature vectors of all of Protein V's neighbors.\n2.  **Update:** The new feature vector $h'_V$ is calculated by adding the aggregated neighbor vector $h_{N(V)}$ to Protein V's original feature vector $h_V$, and then applying an element-wise Rectified Linear Unit (ReLU) activation function to the result. The ReLU function is defined as $\\text{ReLU}(x) = \\max(0, x)$ for each component of the vector.\n\nCalculate the updated feature vector $h'_V$ for Protein V.", "solution": "We are given initial feature vectors for three proteins:\n$$\nh_{V}=\\begin{pmatrix}0.1  0.9\\end{pmatrix},\\quad\nh_{U1}=\\begin{pmatrix}0.2  0.3\\end{pmatrix},\\quad\nh_{U2}=\\begin{pmatrix}0.6  0.1\\end{pmatrix}.\n$$\nAggregation step: the aggregated neighbor vector is the element-wise sum of neighbor features,\n$$\nh_{N(V)}=h_{U1}+h_{U2}\n=\\begin{pmatrix}0.2  0.3\\end{pmatrix}+\\begin{pmatrix}0.6  0.1\\end{pmatrix}\n=\\begin{pmatrix}0.8  0.4\\end{pmatrix}.\n$$\nUpdate step: apply the element-wise ReLU to the sum of $h_{V}$ and $h_{N(V)}$,\n$$\nh'_{V}=\\operatorname{ReLU}\\!\\big(h_{V}+h_{N(V)}\\big)\n=\\operatorname{ReLU}\\!\\left(\\begin{pmatrix}0.1  0.9\\end{pmatrix}+\\begin{pmatrix}0.8  0.4\\end{pmatrix}\\right)\n=\\operatorname{ReLU}\\!\\begin{pmatrix}0.9  1.3\\end{pmatrix}.\n$$\nSince each component is positive, $\\operatorname{ReLU}(x)=x$ for both components, hence\n$$\nh'_{V}=\\begin{pmatrix}0.9  1.3\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}0.9  1.3\\end{pmatrix}}$$", "id": "1436694"}, {"introduction": "Applying GNNs to real biological problems, such as discovering new protein-protein interactions, requires careful consideration of the training process. A crucial aspect of training for link prediction is generating \"negative examples\"—pairs of nodes that do not interact. This exercise explores the subtleties of negative sampling strategies and asks you to calculate the probability of encountering a \"hard negative\" case, highlighting how training data choices can significantly impact model performance [@problem_id:1436726].", "problem": "A systems biology research group is training a Graph Neural Network (GNN) to predict novel Protein-Protein Interactions (PPIs). The complete PPI network for the organism under study contains `M` proteins and `E` known interactions. The training process involves presenting the GNN with positive examples (known interactions) and negative examples (pairs of proteins that do not interact).\n\nThe current negative sampling strategy is straightforward: a negative example is generated by choosing a pair of proteins, `(u, v)`, uniformly at random from all possible pairs that are not among the `E` known interactions.\n\nThe researchers are concerned about how this strategy handles specific network motifs. They focus on a prominent hub protein, `H`, which is known to interact with `N` distinct peripheral proteins, `P_1, P_2, ..., P_N`. In this local neighborhood, the only known interactions are between the hub `H` and each peripheral protein `P_i`. There are no interactions between any two peripheral proteins, i.e., `(P_i, P_j)` is a non-interaction for all `i \\neq j`. Such pairs are considered \"hard negatives\" because the two proteins share a common neighbor, `H`, which might make a simple model erroneously predict an interaction.\n\nGiven `M = 6000` proteins, `E = 25000` interactions, and a hub with `N = 80` peripheral proteins, calculate the probability that a single, randomly chosen negative sample is a \"hard negative\" pair of the form `(P_i, P_j)` where `i \\neq j`.\n\nExpress your answer as a real number rounded to three significant figures.", "solution": "A negative sample is drawn uniformly at random from all non-interacting unordered pairs of proteins. The total number of unordered pairs among $M$ proteins is $\\binom{M}{2}$. Since $E$ pairs are known interactions, the number of available negative pairs is $\\binom{M}{2} - E$.\n\nThe number of \"hard negative\" pairs among the $N$ peripheral proteins is the number of unordered pairs $(P_{i},P_{j})$ with $i \\neq j$, which is $\\binom{N}{2}$, and by assumption all such pairs are non-interactions.\n\nTherefore, the desired probability is\n$$\np=\\frac{\\binom{N}{2}}{\\binom{M}{2}-E}.\n$$\nSubstituting $M=6000$, $E=25000$, and $N=80$,\n$$\n\\binom{6000}{2}=\\frac{6000\\cdot 5999}{2}=17,997,000,\\quad \\binom{80}{2}=\\frac{80\\cdot 79}{2}=3,160,\n$$\nso\n$$\np=\\frac{3160}{17,997,000-25,000}=\\frac{3160}{17,972,000}=\\frac{79}{449,300}\\approx 1.75829\\times 10^{-4}.\n$$\nRounded to three significant figures,\n$$\np\\approx 1.76\\times 10^{-4}.\n$$", "answer": "$$\\boxed{1.76 \\times 10^{-4}}$$", "id": "1436726"}, {"introduction": "The ultimate goal of applying GNNs in systems biomedicine is to generate novel, testable hypotheses that can advance our biological understanding. This advanced practice challenges you to bridge the gap between computational prediction and experimental reality by designing a rigorous wet-lab validation protocol. You must consider statistical power, multiple testing correction, and proper controls to translate a GNN's output into a scientifically sound experimental plan, a critical skill for any computational biologist [@problem_id:4349417].", "problem": "A Graph Neural Network (GNN) trained on a multiplex biological network with node features derived from transcriptomics and phospho-proteomics predicts that under ligand stimulation $L$, perturbing certain source genes will increase a readout marker $Y$ (for example, phosphorylated ribosomal protein S6) via specific protein-protein interaction edges. The GNN outputs a ranked list of $m = 300$ candidate edges. You plan wet-lab validation using Clustered Regularly Interspaced Short Palindromic Repeats interference (CRISPRi) to perturb source genes, measuring $Y$ by quantitative immunoblotting. A pilot study on the same cell line under $L$ yields an empirical variance $\\sigma^2 = 0.09$ for $Y$ (standardized units), and the GNN-predicted minimum effect size of interest is $\\delta = 0.40$ (increase in group mean of the perturbed condition relative to non-targeting control). Plates accommodate $b = 24$ wells, and day-to-day batch effects are non-negligible.\n\nYou must design a validation protocol that links model predictions to wet-lab experiments with appropriate controls, replicates, and statistical significance thresholds. The protocol must:\n\n- Map each edge-level prediction to a falsifiable hypothesis with a null hypothesis $H_0$ stating no change in the mean of $Y$ under the corresponding perturbation and an alternative hypothesis $H_1$ specifying an increase by at least $\\delta$.\n- Include both negative controls (non-targeting guide) and positive controls (a known ligand-responsive interaction that increases $Y$) on every plate.\n- Arrange randomization of guides to wells and blinding of the measurement to avoid allocation and measurement bias.\n- Use biological replicates (independent cultures across days), explicitly distinguishing them from technical replicates (multiple wells per culture), and avoid pseudo-replication by defining the unit of inference as the biological replicate.\n- Control for multiple testing across $m$ hypotheses using a principled procedure with a stated error rate (for example, False Discovery Rate (FDR) at level $q$ or family-wise error rate).\n- Justify per-condition sample sizes from first principles to achieve power $1 - \\beta = 0.80$ at a stated significance level for confirmatory testing, explicitly accounting for the observed $\\sigma^2$ and the minimally interesting effect size $\\delta$.\n\nWhich of the following protocols best satisfies these requirements while remaining scientifically sound and resource-feasible?\n\nA. Screen all $m = 300$ predictions in a single stage with $n = 3$ wells per condition per plate, treat wells as independent replicates, apply an uncorrected two-sided test at $\\alpha = 0.05$, and call hits by combining a fold-change threshold $\\geq \\delta$ with $p  0.05$. Include only a non-targeting negative control. No randomization, no blinding, and no adjustment for batch effects.\n\nB. Use a two-stage design. Stage $1$: for all $m = 300$ predictions, randomize guide assignment across plates and days, blind $Y$ measurements, and include per-plate negative and positive controls. Collect $n_s = 6$ biological replicates per condition across $r_b = 3$ independent days, with $r_t = 2$ technical wells per biological replicate; analyze biological-replicate means with a linear mixed-effects model including day as a random effect to mitigate batch variation. Control the False Discovery Rate (FDR) at $q = 0.10$ across $m$ hypotheses (Benjamini–Hochberg). Stage $2$: for the top $k$ candidates from Stage $1$ and all controls, perform confirmatory experiments with independent cultures, randomization and blinding, and compute per-condition sample size $n_c$ from first principles to achieve power $1 - \\beta = 0.80$ at a one-sided $\\alpha = 0.01$ given $\\sigma^2 = 0.09$ and $\\delta = 0.40$; analyze with a two-sample test on biological-replicate means and report effect sizes with confidence intervals. Declare validated if the confirmatory $p$-value is below $\\alpha$ and the estimated effect is $\\geq \\delta$. \n\nC. Perform a single-stage experiment with $n = 4$ wells per condition, count each well as an independent replicate, and apply Bonferroni correction across $m = 300$ tests with per-test threshold $\\alpha = 0.05 / 300$. Include a non-targeting negative control but omit the positive control. Do not randomize across plates; assign all guides for a given prediction to the same plate. Use the raw well-level data without modeling batch effects.\n\nD. Use a single-stage experiment with $n = 10$ biological replicates per condition, randomize and blind, include negative and positive controls, and test each prediction with an uncorrected one-sided test at $\\alpha = 0.05$. Do not perform multiple testing correction; instead, apply a fold-change filter $\\geq \\delta$ and require $p  0.05$ to declare significance. No confirmatory follow-up.\n\nSelect the best option.", "solution": "The problem asks for the design of a wet-lab validation protocol for computational predictions from a Graph Neural Network (GNN). The protocol's quality is to be judged against a specific set of requirements for scientific and statistical rigor.\n\nFirst, we validate the problem statement.\n\n### Step 1: Extract Givens\n-   **Model:** Graph Neural Network (GNN) on a multiplex biological network.\n-   **GNN Input:** Node features from transcriptomics and phospho-proteomics.\n-   **GNN Output:** Ranked list of $m = 300$ candidate edges predicted to increase readout marker $Y$ upon source gene perturbation under ligand stimulation $L$.\n-   **Validation Technique:** CRISPRi perturbation of source genes.\n-   **Readout Measurement:** Quantitative immunoblotting for $Y$.\n-   **Pilot Data:** Empirical variance of $Y$ is $\\sigma^2 = 0.09$ (standardized units).\n-   **Effect Size:** Minimum effect size of interest is an increase of $\\delta = 0.40$ in the group mean.\n-   **Experimental Constraints:** Plate size is $b = 24$ wells; non-negligible day-to-day batch effects.\n-   **Protocol Requirements:**\n    1.  Map predictions to falsifiable hypotheses ($H_0$: no change, $H_1$: increase $\\ge \\delta$).\n    2.  Include per-plate negative and positive controls.\n    3.  Implement randomization and blinding.\n    4.  Use true biological replicates, distinguish from technical replicates, and avoid pseudo-replication.\n    5.  Control for multiple testing across $m=300$ hypotheses using a principled method.\n    6.  Justify sample size for a power of $1 - \\beta = 0.80$ at a stated significance level, using the given $\\sigma^2$ and $\\delta$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded, well-posed, and objective. It outlines a standard and realistic scenario in systems biology where computational predictions are validated experimentally. All terms (GNN, CRISPRi, FDR, power analysis) are standard in the field. The provided data ($\\sigma^2=0.09$, $\\delta=0.40$, $m=300$) are plausible. The requirements for the protocol correspond to bedrock principles of good experimental design and statistical analysis. The problem is free of scientific unsoundness, ambiguity, or contradiction.\n\n### Step 3: Verdict and Action\nThe problem is valid. We will proceed to analyze the options. A key quantitative requirement is the sample size calculation (Requirement 6), which we perform first.\n\n### Principle-Based Derivation: Sample Size Calculation\n\nThe protocol must justify sample sizes to achieve a power of $1 - \\beta = 0.80$ for detecting an effect size of at least $\\delta = 0.40$. The alternative hypothesis is directional ($H_1$: increase in $Y$), so a one-sided statistical test is appropriate. The formula for the required sample size $n$ per group in a two-sample test (assuming known variance) is:\n$$ n = \\frac{(\\sigma_1^2 + \\sigma_2^2) (z_{1-\\alpha} + z_{1-\\beta})^2}{\\delta^2} $$\nAssuming equal variance in both the control and perturbed groups, $\\sigma_1^2 = \\sigma_2^2 = \\sigma^2$, the formula simplifies to:\n$$ n = \\frac{2 \\sigma^2 (z_{1-\\alpha} + z_{1-\\beta})^2}{\\delta^2} $$\nWe are given:\n-   Variance: $\\sigma^2 = 0.09$, so standard deviation $\\sigma = \\sqrt{0.09} = 0.30$.\n-   Effect size: $\\delta = 0.40$.\n-   Power: $1 - \\beta = 0.80$, which corresponds to a standard normal quantile of $z_{1-\\beta} = z_{0.80} \\approx 0.842$.\n\nOption B specifies a confirmatory significance level of $\\alpha = 0.01$. For a one-sided test, this corresponds to $z_{1-\\alpha} = z_{0.99} \\approx 2.326$.\n\nLet's calculate the required sample size $n_c$ for the confirmatory stage as described in Option B:\n$$ n_c = \\frac{2 \\times 0.09 \\times (z_{0.99} + z_{0.80})^2}{(0.40)^2} $$\n$$ n_c = \\frac{0.18 \\times (2.326 + 0.842)^2}{0.16} $$\n$$ n_c = \\frac{0.18 \\times (3.168)^2}{0.16} $$\n$$ n_c = \\frac{0.18 \\times 10.036224}{0.16} \\approx 11.29 $$\nSince the number of replicates must be an integer, the experiment requires $n_c = 12$ biological replicates per condition (e.g., $12$ for the perturbed group and $12$ for the non-targeting control group) to meet these stringent confirmatory criteria.\n\n### Option-by-Option Analysis\n\n**A. Screen all $m = 300$ predictions in a single stage with $n = 3$ wells per condition per plate, treat wells as independent replicates, apply an uncorrected two-sided test at $\\alpha = 0.05$, and call hits by combining a fold-change threshold $\\geq \\delta$ with $p  0.05$. Include only a non-targeting negative control. No randomization, no blinding, and no adjustment for batch effects.**\n\n*   **Requirement 1 (Hypothesis):** Fails. Uses a two-sided test for a directional prediction (\"increase\"), which reduces statistical power.\n*   **Requirement 2 (Controls):** Fails. Omits the required positive control.\n*   **Requirement 3 (Bias Control):** Fails. Explicitly states \"No randomization, no blinding,\" inviting systematic bias.\n*   **Requirement 4 (Replication):** Fails. Commits the serious error of pseudo-replication by treating wells (technical replicates) as independent biological replicates.\n*   **Requirement 5 (Multiple Testing):** Fails. Uses an \"uncorrected\" test for $m=300$ hypotheses, which would lead to a large number of false positives (on average, $300 \\times 0.05 = 15$). A simple p-value plus fold-change threshold is not a principled method for controlling error rates.\n*   **Requirement 6 (Power):** Fails. The sample size of $n=3$ is arbitrary and grossly insufficient for the desired power.\n\n**Verdict:** Incorrect. This protocol is fundamentally unsound and violates every key principle of robust experimental design.\n\n**B. Use a two-stage design. Stage $1$: for all $m = 300$ predictions, randomize guide assignment across plates and days, blind $Y$ measurements, and include per-plate negative and positive controls. Collect $n_s = 6$ biological replicates per condition across $r_b = 3$ independent days, with $r_t = 2$ technical wells per biological replicate; analyze biological-replicate means with a linear mixed-effects model including day as a random effect to mitigate batch variation. Control the False Discovery Rate (FDR) at $q = 0.10$ across $m$ hypotheses (Benjamini–Hochberg). Stage $2$: for the top $k$ candidates from Stage $1$ and all controls, perform confirmatory experiments with independent cultures, randomization and blinding, and compute per-condition sample size $n_c$ from first principles to achieve power $1 - \\beta = 0.80$ at a one-sided $\\alpha = 0.01$ given $\\sigma^2 = 0.09$ and $\\delta = 0.40$; analyze with a two-sample test on biological-replicate means and report effect sizes with confidence intervals. Declare validated if the confirmatory $p$-value is below $\\alpha$ and the estimated effect is $\\geq \\delta$.**\n\n*   **Requirement 1 (Hypothesis):** Met. The confirmatory stage uses a one-sided test and an effect size criterion, directly testing the specified hypothesis.\n*   **Requirement 2 (Controls):** Met. Includes both negative and positive controls.\n*   **Requirement 3 (Bias Control):** Met. Explicitly includes randomization and blinding.\n*   **Requirement 4 (Replication):** Met. Correctly distinguishes biological ($n_s=6$) and technical ($r_t=2$) replicates, avoids pseudo-replication, and uses a sophisticated mixed-effects model to properly handle batch effects and the data structure.\n*   **Requirement 5 (Multiple Testing):** Met. Uses FDR control via Benjamini-Hochberg, which is the standard and appropriate method for a discovery screen of this nature.\n*   **Requirement 6 (Power):** Met. Explicitly commits to computing the sample size $n_c$ for the confirmatory stage from first principles, using the exact parameters given in the problem statement. The two-stage design is also a highly efficient use of resources.\n\n**Verdict:** Correct. This protocol is an exemplar of rigorous, modern experimental design. It correctly addresses all specified requirements with appropriate statistical methods.\n\n**C. Perform a single-stage experiment with $n = 4$ wells per condition, count each well as an independent replicate, and apply Bonferroni correction across $m = 300$ tests with per-test threshold $\\alpha = 0.05 / 300$. Include a non-targeting negative control but omit the positive control. Do not randomize across plates; assign all guides for a given prediction to the same plate. Use the raw well-level data without modeling batch effects.**\n\n*   **Requirement 1 (Hypothesis):** Partially met, assuming a test is performed.\n*   **Requirement 2 (Controls):** Fails. Omits the positive control.\n*   **Requirement 3 (Bias Control):** Fails. Lack of randomization across plates introduces confounding between treatment and plate effects. No mention of blinding.\n*   **Requirement 4 (Replication):** Fails. Engages in pseudo-replication (\"count each well as an independent replicate\").\n*   **Requirement 5 (Multiple Testing):** Met in principle. It uses Bonferroni correction, which is a valid but extremely conservative way to control the family-wise error rate.\n*   **Requirement 6 (Power):** Fails. With an adjusted significance level of $\\alpha_{adj} = 0.05 / 300 \\approx 0.000167$ and an unjustified sample size of $n=4$, the statistical power to detect any true effects would be infinitesimally small, rendering the experiment futile.\n\n**Verdict:** Incorrect. Although it correctly identifies the multiple testing problem, its other design flaws (pseudo-replication, lack of controls, no randomization) and the impractical combination of Bonferroni correction with a tiny sample size make it a poor design.\n\n**D. Use a single-stage experiment with $n = 10$ biological replicates per condition, randomize and blind, include negative and positive controls, and test each prediction with an uncorrected one-sided test at $\\alpha = 0.05$. Do not perform multiple testing correction; instead, apply a fold-change filter $\\geq \\delta$ and require $p  0.05$ to declare significance. No confirmatory follow-up.**\n\n*   **Requirement 1 (Hypothesis):** Met. Correctly uses a one-sided test.\n*   **Requirement 2 (Controls):** Met. Includes both negative and positive controls.\n*   **Requirement 3 (Bias Control):** Met. Includes randomization and blinding.\n*   **Requirement 4 (Replication):** Met. Correctly uses a reasonable number ($n=10$) of *biological* replicates.\n*   **Requirement 5 (Multiple Testing):** Fails. Critically, it explicitly states \"Do not perform multiple testing correction,\" which is unacceptable for an experiment with $m=300$ hypotheses.\n*   **Requirement 6 (Power):** Partially met. The sample size $n=10$ is reasonable and likely to achieve adequate power (our calculation for $\\alpha=0.05$ yielded $n=7$), but the protocol does not commit to deriving it from first principles.\n\n**Verdict:** Incorrect. This protocol has several good features, but the complete omission of multiple testing correction is a fatal flaw that invalidates any conclusions drawn from the screen.\n\n**Conclusion:** Protocol B is the only option that satisfies all requirements of a rigorous, statistically sound, and well-designed validation experiment. It demonstrates a sophisticated understanding of experimental design principles, including staged design, power analysis, control of error rates, and mitigation of batch effects.", "answer": "$$\\boxed{B}$$", "id": "4349417"}]}