{"hands_on_practices": [{"introduction": "A core task in systems biomedicine is to update our knowledge about a system's parameters as we collect new data. This exercise provides a foundational hands-on experience in Bayesian inference, demonstrating how to combine prior beliefs about a transcription factor's activity states with new experimental counts. You will apply the elegant properties of conjugate priors—specifically the Dirichlet-Multinomial model—to derive the posterior distribution and quantify your updated certainty about the system's underlying probabilities. [@problem_id:4318094]", "problem": "In a Bayesian network (BN) representing a causal model of a transcription factor’s tri-modal activity state in a systems biomedicine setting, consider a single ternary node with latent probability vector $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\theta_3)$ over three mutually exclusive cellular states. Prior knowledge from earlier RNA sequencing (RNA-seq) studies is encoded as a Dirichlet prior $\\operatorname{Dirichlet}(\\alpha_1, \\alpha_2, \\alpha_3)$ with hyperparameters $(\\alpha_1, \\alpha_2, \\alpha_3) = (2.4, 1.6, 0.8)$. A new cohort of $20$ matched perturbation experiments yields observed counts $(n_1, n_2, n_3) = (10, 7, 3)$ across the three states.\n\nStarting from the definition of the multinomial likelihood for counts given $\\boldsymbol{\\theta}$ and the Dirichlet prior as a conjugate family for the multinomial, derive the posterior distribution over $\\boldsymbol{\\theta}$ and, from first principles of the Dirichlet family, compute the posterior mean $\\mathbb{E}[\\theta_i \\mid \\text{data}]$ and the posterior marginal variance $\\operatorname{Var}[\\theta_i \\mid \\text{data}]$ for each category $i \\in \\{1,2,3\\}$.\n\nProvide the final numerical values for all three posterior means followed by all three posterior variances, in that order as a single row matrix. Round each value to four significant figures. No units are required.", "solution": "The user has provided a valid problem statement.\n\nThe problem requires the derivation of the posterior distribution for a latent probability vector $\\boldsymbol{\\theta}$ and the computation of its posterior mean and marginal variance. This is a classic Bayesian inference problem involving a multinomial likelihood and a Dirichlet prior, which are a conjugate pair.\n\n**Step 1: Define the Model Components (Likelihood and Prior)**\n\nLet the latent probability vector for the three cellular states be $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\theta_3)$, where $\\theta_i \\ge 0$ for all $i$ and $\\sum_{i=1}^3 \\theta_i = 1$. The observed data consists of counts $\\mathbf{n} = (n_1, n_2, n_3) = (10, 7, 3)$ from a total of $N = \\sum n_i = 20$ experiments.\n\nThe likelihood of observing these counts, given the probabilities $\\boldsymbol{\\theta}$, is described by a multinomial distribution. The probability mass function is:\n$$P(\\mathbf{n} \\mid \\boldsymbol{\\theta}) = \\frac{N!}{n_1! n_2! n_3!} \\theta_1^{n_1} \\theta_2^{n_2} \\theta_3^{n_3}$$\nFor the purpose of finding the posterior distribution of $\\boldsymbol{\\theta}$, the multinomial coefficient $\\frac{N!}{n_1! n_2! n_3!}$ is a constant. Therefore, the likelihood function is proportional to:\n$$P(\\mathbf{n} \\mid \\boldsymbol{\\theta}) \\propto \\prod_{i=1}^3 \\theta_i^{n_i}$$\n\nThe prior knowledge about $\\boldsymbol{\\theta}$ is encoded as a Dirichlet distribution with hyperparameter vector $\\boldsymbol{\\alpha} = (\\alpha_1, \\alpha_2, \\alpha_3) = (2.4, 1.6, 0.8)$. The probability density function of the prior is:\n$$P(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\alpha}) = \\frac{\\Gamma(\\sum_{i=1}^3 \\alpha_i)}{\\prod_{i=1}^3 \\Gamma(\\alpha_i)} \\prod_{i=1}^3 \\theta_i^{\\alpha_i - 1} = \\frac{\\Gamma(\\alpha_0)}{\\prod_{i=1}^3 \\Gamma(\\alpha_i)} \\prod_{i=1}^3 \\theta_i^{\\alpha_i - 1}$$\nwhere $\\alpha_0 = \\sum_{i=1}^3 \\alpha_i = 2.4 + 1.6 + 0.8 = 4.8$.\nThe prior distribution is thus $P(\\boldsymbol{\\theta}) \\propto \\prod_{i=1}^3 \\theta_i^{\\alpha_i - 1}$.\n\n**Step 2: Derive the Posterior Distribution**\n\nUsing Bayes' theorem, the posterior distribution $P(\\boldsymbol{\\theta} \\mid \\mathbf{n}, \\boldsymbol{\\alpha})$ is proportional to the product of the likelihood and the prior:\n$$P(\\boldsymbol{\\theta} \\mid \\mathbf{n}, \\boldsymbol{\\alpha}) \\propto P(\\mathbf{n} \\mid \\boldsymbol{\\theta}) P(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\alpha})$$\nSubstituting the proportional forms of the likelihood and prior:\n$$P(\\boldsymbol{\\theta} \\mid \\mathbf{n}, \\boldsymbol{\\alpha}) \\propto \\left( \\prod_{i=1}^3 \\theta_i^{n_i} \\right) \\left( \\prod_{i=1}^3 \\theta_i^{\\alpha_i - 1} \\right)$$\n$$P(\\boldsymbol{\\theta} \\mid \\mathbf{n}, \\boldsymbol{\\alpha}) \\propto \\prod_{i=1}^3 \\theta_i^{n_i + \\alpha_i - 1}$$\nThis resulting functional form is the kernel of a Dirichlet distribution. Thus, the posterior distribution is also a Dirichlet distribution, a property known as conjugacy. The posterior distribution is $\\operatorname{Dirichlet}(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\alpha}')$, where the updated hyperparameters $\\boldsymbol{\\alpha}' = (\\alpha'_1, \\alpha'_2, \\alpha'_3)$ are given by:\n$$\\alpha'_i = \\alpha_i + n_i$$\nLet's compute the specific values for the posterior hyperparameters:\n$$\\alpha'_1 = \\alpha_1 + n_1 = 2.4 + 10 = 12.4$$\n$$\\alpha'_2 = \\alpha_2 + n_2 = 1.6 + 7 = 8.6$$\n$$\\alpha'_3 = \\alpha_3 + n_3 = 0.8 + 3 = 3.8$$\nSo, the posterior distribution is $\\boldsymbol{\\theta} \\mid \\mathbf{n} \\sim \\operatorname{Dirichlet}(12.4, 8.6, 3.8)$.\n\n**Step 3: Compute Posterior Mean and Variance from First Principles**\n\nFor a random vector $\\boldsymbol{\\theta} \\sim \\operatorname{Dirichlet}(\\boldsymbol{\\alpha}')$, the sum of the hyperparameters is $\\alpha'_0 = \\sum_{i=1}^3 \\alpha'_i$. The expected value (mean) and variance of each component $\\theta_i$ are given by standard formulas for the Dirichlet distribution.\n\nThe sum of the posterior hyperparameters is:\n$$\\alpha'_0 = \\alpha'_1 + \\alpha'_2 + \\alpha'_3 = 12.4 + 8.6 + 3.8 = 24.8$$\nAs a consistency check, $\\alpha'_0 = \\alpha_0 + N = 4.8 + 20 = 24.8$.\n\nThe posterior mean for each component $\\theta_i$ is given by:\n$$\\mathbb{E}[\\theta_i \\mid \\mathbf{n}] = \\frac{\\alpha'_i}{\\alpha'_0}$$\nWe compute the mean for each state:\n$$\\mathbb{E}[\\theta_1 \\mid \\mathbf{n}] = \\frac{12.4}{24.8} = 0.5$$\n$$\\mathbb{E}[\\theta_2 \\mid \\mathbf{n}] = \\frac{8.6}{24.8} \\approx 0.346774...$$\n$$\\mathbb{E}[\\theta_3 \\mid \\mathbf{n}] = \\frac{3.8}{24.8} \\approx 0.153225...$$\n\nThe posterior marginal variance for each component $\\theta_i$ is given by:\n$$\\operatorname{Var}[\\theta_i \\mid \\mathbf{n}] = \\frac{\\alpha'_i (\\alpha'_0 - \\alpha'_i)}{\\alpha'_0^2 (\\alpha'_0 + 1)}$$\nWe compute the variance for each state:\n$$\\operatorname{Var}[\\theta_1 \\mid \\mathbf{n}] = \\frac{12.4 \\times (24.8 - 12.4)}{24.8^2 \\times (24.8 + 1)} = \\frac{12.4 \\times 12.4}{24.8^2 \\times 25.8} = \\frac{153.76}{615.04 \\times 25.8} = \\frac{153.76}{15868.032} \\approx 0.0096900...$$\n$$\\operatorname{Var}[\\theta_2 \\mid \\mathbf{n}] = \\frac{8.6 \\times (24.8 - 8.6)}{24.8^2 \\times (24.8 + 1)} = \\frac{8.6 \\times 16.2}{24.8^2 \\times 25.8} = \\frac{139.32}{15868.032} \\approx 0.0087799...$$\n$$\\operatorname{Var}[\\theta_3 \\mid \\mathbf{n}] = \\frac{3.8 \\times (24.8 - 3.8)}{24.8^2 \\times (24.8 + 1)} = \\frac{3.8 \\times 21.0}{24.8^2 \\times 25.8} = \\frac{79.8}{15868.032} \\approx 0.0050289...$$\n\n**Step 4: Final Numerical Values**\n\nRounding the results to four significant figures as requested:\nPosterior Means:\n$$\\mathbb{E}[\\theta_1 \\mid \\mathbf{n}] = 0.5000$$\n$$\\mathbb{E}[\\theta_2 \\mid \\mathbf{n}] \\approx 0.3468$$\n$$\\mathbb{E}[\\theta_3 \\mid \\mathbf{n}] \\approx 0.1532$$\n\nPosterior Variances:\n$$\\operatorname{Var}[\\theta_1 \\mid \\mathbf{n}] \\approx 0.009690$$\n$$\\operatorname{Var}[\\theta_2 \\mid \\mathbf{n}] \\approx 0.008780$$\n$$\\operatorname{Var}[\\theta_3 \\mid \\mathbf{n}] \\approx 0.005029$$\n\nThe final answer is a row matrix containing these six values in the specified order.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.5000 & 0.3468 & 0.1532 & 0.009690 & 0.008780 & 0.005029\n\\end{pmatrix}\n}\n$$", "id": "4318094"}, {"introduction": "Before we can quantify relationships in a biological network, we must first understand the conditional independence structure implied by its causal graph. This practice introduces d-separation, the graphical \"grammar\" for reading these dependencies directly from a Directed Acyclic Graph (DAG). By analyzing the paths between nodes in a simple gene regulation model, you will learn to identify when variables are independent and, crucially, how conditioning on a \"collider\" node can create a dependency where none existed before. [@problem_id:4318044]", "problem": "A researcher models a cellular gene regulation scenario as a Directed Acyclic Graph (DAG) in a Bayesian Network (BN) with nodes gene expression potential $G$, external stimulus $S$, and measured messenger RNA expression $E$. The graph has directed edges $G \\to E$, $S \\to E$, and $G \\to S$. Assume standard BN semantics under the Markov property and the commonly used faithfulness assumption. The researcher wants to decide structural conditional independence relations using the notion of d-separation, defined as follows: a path between two variables is blocked by a conditioning set if either there exists a non-collider on the path that is in the conditioning set, or there exists a collider on the path such that neither the collider nor any of its descendants is in the conditioning set; otherwise the path is active. Under the Markov and faithfulness assumptions, d-separation implies conditional independence.\n\nBased on this, determine which independence relations hold among $(G, S, E)$ in the given DAG, and explain the collider structure. Choose the single best option.\n\nA. $G \\perp S$ and $G \\perp S \\mid E$ both hold; $E$ is a collider and conditioning on $E$ blocks the path $G \\to E \\leftarrow S$.\n\nB. $G \\not\\perp S$ but $G \\perp S \\mid E$ holds; $E$ is a collider and conditioning on $E$ blocks the path $G \\to E \\leftarrow S$, leaving no active path between $G$ and $S$.\n\nC. $G \\not\\perp S$ and $G \\not\\perp S \\mid E$ both hold; $E$ is a collider on the path $G \\to E \\leftarrow S$, and conditioning on $E$ opens that collider path while the direct edge $G \\to S$ remains an active path.\n\nD. $G \\perp S$ but $G \\not\\perp S \\mid E$ holds; $E$ is a collider and conditioning on $E$ opens the path $G \\to E \\leftarrow S$.", "solution": "### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Model:** A Directed Acyclic Graph (DAG) representing a Bayesian Network (BN).\n- **Nodes:**\n    - $G$: Gene expression potential\n    - $S$: External stimulus\n    - $E$: Measured messenger RNA expression\n- **Directed Edges:** The graph structure is defined by the edges $G \\to E$, $S \\to E$, and $G \\to S$.\n- **Assumptions:**\n    - Standard Bayesian Network semantics.\n    - Markov property applied to the DAG.\n    - Faithfulness assumption.\n- **Key Definition (d-separation):** A path between two variables is blocked by a conditioning set if either:\n    1. there exists a non-collider on the path that is in the conditioning set, or\n    2. there exists a collider on the path such that neither the collider nor any of its descendants is in the conditioning set.\n    Otherwise, the path is active.\n- **Inference Rule:** Under the stated assumptions, d-separation implies conditional independence. By the faithfulness assumption, the reverse is also true: conditional independence implies d-separation.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem statement is evaluated against the validation criteria:\n\n- **Scientifically Grounded:** The problem uses standard, well-established concepts from the field of graphical models and Bayesian statistics (DAGs, BNs, d-separation, Markov property, faithfulness). The application to gene regulation is a common and appropriate use case in systems biology. The model is a simplification but is scientifically sound for the purpose of illustrating these principles.\n- **Well-Posed:** The problem provides a completely specified DAG and a precise rule (d-separation) for determining conditional independence. The question is unambiguous and asks for a specific set of relations to be determined from the given structure, which leads to a unique solution.\n- **Objective:** The problem is stated in precise, formal language common to statistics and computer science. It is free from subjective or ambiguous terminology.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, missing information, or logical contradictions. The DAG given by the edges $G \\to S$, $G \\to E$, and $S \\to E$ is indeed acyclic. The premises are consistent and sufficient to arrive at a definitive answer.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A full solution will be derived.\n\n### Derivation of the Solution\n\nThe problem requires us to determine the independence relationships between the nodes $G$, $S$, and $E$ in the given Bayesian network. The network structure is a DAG with nodes $\\{G, S, E\\}$ and directed edges $\\{G \\to S, G \\to E, S \\to E\\}$. We use the principle of d-separation to assess independence. Two nodes are independent (or conditionally independent) if all paths between them are blocked.\n\n**1. Analysis of the paths between $G$ and $S$**\n\nThere are two paths between node $G$ and node $S$:\n- **Path 1:** The direct edge $G \\to S$.\n- **Path 2:** The path $G \\to E \\leftarrow S$.\n\n**2. Analysis of the marginal independence $G \\perp S$**\n\nTo check for marginal independence, we use an empty conditioning set, $\\emptyset$. We must check if all paths between $G$ and $S$ are blocked by $\\emptyset$.\n\n- **Path 1 ($G \\to S$):** This is a direct active path. A path of length $1$ cannot be blocked by an empty set as there are no intermediate nodes (colliders or non-colliders) on it. Thus, this path is **active**.\n- **Path 2 ($G \\to E \\leftarrow S$):** On this path, node $E$ is a **collider** because two arrows point into it ($G \\to E$ and $S \\to E$). According to the d-separation rule, a path containing a collider is blocked if neither the collider nor any of its descendants are in the conditioning set. Here, the conditioning set is $\\emptyset$. The collider is $E$. $E$ is not in $\\emptyset$. Node $E$ has no descendants in this graph. Therefore, the condition for blocking is met. This path is **blocked**.\n\nSince Path 1 ($G \\to S$) is active, not all paths between $G$ and $S$ are blocked. Therefore, $G$ and $S$ are not d-separated. Under the faithfulness assumption, this implies they are not marginally independent: $G \\not\\perp S$.\n\n**3. Analysis of the conditional independence $G \\perp S \\mid E$**\n\nTo check for conditional independence given $E$, we use the conditioning set $\\{E\\}$. We must check if all paths between $G$ and $S$ are blocked by $\\{E\\}$.\n\n- **Path 1 ($G \\to S$):** The path does not contain any intermediate nodes. The conditioning variable $E$ is not on this path. Therefore, conditioning on $E$ has no effect on this path. It remains **active**.\n- **Path 2 ($G \\to E \\leftarrow S$):** This path contains the collider $E$. The d-separation rule states that a collider path is blocked *unless* the collider or a descendant is in the conditioning set. Here, the collider $E$ is *in* the conditioning set $\\{E\\}$. Therefore, conditioning on $E$ **unblocks** or **opens** this path. The path becomes **active**. This is a classic \"explaining away\" effect: learning the value of a common effect ($E$) can induce a dependency between its causes ($G$ and $S$).\n\nSince both Path 1 and Path 2 are active when conditioning on $E$, $G$ and $S$ are not d-separated by $E$. Under the faithfulness assumption, this implies they are not conditionally independent given $E$: $G \\not\\perp S \\mid E$.\n\n**Summary of Findings:**\n- Marginal Independence: $G \\not\\perp S$.\n- Conditional Independence: $G \\not\\perp S \\mid E$.\n- Collider Structure: The node $E$ is a collider on the path $G \\to E \\leftarrow S$.\n\n### Evaluation of Options\n\nNow, we evaluate each option based on our derivation.\n\n**A. $G \\perp S$ and $G \\perp S \\mid E$ both hold; $E$ is a collider and conditioning on $E$ blocks the path $G \\to E \\leftarrow S$.**\n- The statement \"$G \\perp S$ ... holds\" is **Incorrect**. We found $G \\not\\perp S$.\n- The statement \"$G \\perp S \\mid E$ ... holds\" is **Incorrect**. We found $G \\not\\perp S \\mid E$.\n- The statement \"conditioning on $E$ blocks the path $G \\to E \\leftarrow S$\" is **Incorrect**. Conditioning on a collider *unblocks* the path.\nTherefore, this option is **Incorrect**.\n\n**B. $G \\not\\perp S$ but $G \\perp S \\mid E$ holds; $E$ is a collider and conditioning on $E$ blocks the path $G \\to E \\leftarrow S$, leaving no active path between $G$ and $S$.**\n- The statement \"$G \\not\\perp S$\" is correct.\n- The statement \"$G \\perp S \\mid E$ holds\" is **Incorrect**. We found $G \\not\\perp S \\mid E$.\n- The statement \"conditioning on $E$ blocks the path\" is **Incorrect**.\n- The phrase \"leaving no active path between $G$ and $S$\" is **Incorrect**, as the path $G \\to S$ remains active, and the path $G \\to E \\leftarrow S$ becomes active.\nTherefore, this option is **Incorrect**.\n\n**C. $G \\not\\perp S$ and $G \\not\\perp S \\mid E$ both hold; $E$ is a collider on the path $G \\to E \\leftarrow S$, and conditioning on $E$ opens that collider path while the direct edge $G \\to S$ remains an active path.**\n- The statement \"$G \\not\\perp S$ ... holds\" is **Correct**.\n- The statement \"$G \\not\\perp S \\mid E$ ... holds\" is **Correct**.\n- The description \"$E$ is a collider on the path $G \\to E \\leftarrow S$\" is **Correct**.\n- The explanation \"conditioning on $E$ opens that collider path\" is **Correct**.\n- The explanation \"the direct edge $G \\to S$ remains an active path\" is **Correct**.\nThis option accurately describes all aspects of the relationship between $G$ and $S$ according to our analysis. Therefore, this option is **Correct**.\n\n**D. $G \\perp S$ but $G \\not\\perp S \\mid E$ holds; $E$ is a collider and conditioning on $E$ opens the path $G \\to E \\leftarrow S$.**\n- The statement \"$G \\perp S$ ... holds\" is **Incorrect**. We found $G \\not\\perp S$ due to the direct edge.\n- The rest of the statement, \"$G \\not\\perp S \\mid E$ holds\" and the reason provided, are correct, but the initial premise is false.\nTherefore, this option is **Incorrect**.", "answer": "$$\\boxed{C}$$", "id": "4318044"}, {"introduction": "The concept of a collider, explored in the previous practice, has profound practical implications for data analysis. This exercise moves from graphical rules to quantitative consequences, demonstrating how naively \"adjusting for\" a variable can lead to erroneous conclusions. Using a linear Structural Causal Model, you will compute how conditioning on a collider node not only induces a spurious correlation but can even reverse the sign of the true underlying association, a critical lesson for anyone interpreting statistical relationships in complex biological data. [@problem_id:4318057]", "problem": "In a systems biomedicine study of inflammatory response, consider a causal data-generating process modeled as a linear Gaussian Structural Causal Model (SCM). Let $U$ be an unobserved systemic inflammation factor that affects two circulating biomarkers $X$ and $Y$. A triage score $Z$ is computed from the two biomarkers and used for admission decisions. The causal structure is given by the directed acyclic graph with arrows $U \\to X$, $U \\to Y$, $X \\to Z$, and $Y \\to Z$, so that $Z$ is a collider of $X$ and $Y$. Suppose the variables are generated by the following linear system with independent Gaussian noise:\n$$\nU \\sim \\mathcal{N}(0,1), \\quad \\varepsilon_{1} \\sim \\mathcal{N}(0,1), \\quad \\varepsilon_{2} \\sim \\mathcal{N}(0,1), \\quad \\varepsilon_{3} \\sim \\mathcal{N}\\!\\left(0,\\tfrac{1}{10}\\right),\n$$\n$$\nX = U + \\varepsilon_{1}, \\quad Y = U + \\varepsilon_{2}, \\quad Z = X + Y + \\varepsilon_{3},\n$$\nwith all four noise terms mutually independent and independent of $U$.\n\nUsing only fundamental definitions and well-tested facts about expectations, variances, covariances, and correlations for multivariate normal variables, do the following:\n\n1) Determine the marginal Pearson correlation between $X$ and $Y$ and state its sign.\n\n2) An analyst mistakenly adjusts for the collider $Z$. Compute the Pearson correlation between $X$ and $Y$ conditional on $Z$ in this linear Gaussian setting. Report this spurious conditioned correlation as a single exact real number.\n\nYour final answer must be the single value of the spurious conditioned correlation from part 2). No units are involved. Do not round; provide the exact value.", "solution": "The user has provided a problem statement regarding a linear Gaussian Structural Causal Model (SCM) and has asked for the calculation of a marginal correlation and a conditional correlation.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\nThe problem provides the following information:\n- A causal structure defined by the directed acyclic graph: $U \\to X$, $U \\to Y$, $X \\to Z$, $Y \\to Z$.\n- A set of structural equations:\n  - $X = U + \\varepsilon_{1}$\n  - $Y = U + \\varepsilon_{2}$\n  - $Z = X + Y + \\varepsilon_{3}$\n- The distributions of the exogenous variables:\n  - $U \\sim \\mathcal{N}(0,1)$\n  - $\\varepsilon_{1} \\sim \\mathcal{N}(0,1)$\n  - $\\varepsilon_{2} \\sim \\mathcal{N}(0,1)$\n  - $\\varepsilon_{3} \\sim \\mathcal{N}(0,\\frac{1}{10})$\n- An independence assumption: $U, \\varepsilon_{1}, \\varepsilon_{2}, \\varepsilon_{3}$ are mutually independent.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is grounded in the well-established framework of Structural Causal Models, a standard tool in statistics, econometrics, and systems biology for modeling causal relationships. The model described is a canonical example used to illustrate confounding and collider bias. It is scientifically sound.\n- **Well-Posed**: The problem is well-posed. The variables $(X, Y, Z)$ are linear combinations of independent Gaussian variables, which means they form a multivariate normal distribution. For such a distribution, all marginal and conditional correlations are uniquely defined and computable from the covariance matrix.\n- **Objective**: The problem is stated using precise mathematical language, free of subjective or ambiguous terminology.\n- The problem is self-contained, consistent, and does not violate any physical or mathematical principles. It is a standard exercise in causal inference.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution will now be derived.\n\n### Solution Derivation\n\nThe solution requires calculating two quantities: the marginal correlation $\\operatorname{Corr}(X, Y)$ and the conditional correlation $\\operatorname{Corr}(X, Y | Z)$. The final answer is the value of the second quantity.\n\nThe Pearson correlation coefficient between two random variables $A$ and $B$ is defined as $\\operatorname{Corr}(A, B) = \\frac{\\operatorname{Cov}(A, B)}{\\sqrt{\\operatorname{Var}(A) \\operatorname{Var}(B)}}$.\n\nFirst, we determine the means, variances, and covariances of the variables $X$ and $Y$.\nThe means are:\n$E[X] = E[U + \\varepsilon_{1}] = E[U] + E[\\varepsilon_{1}] = 0 + 0 = 0$.\n$E[Y] = E[U + \\varepsilon_{2}] = E[U] + E[\\varepsilon_{2}] = 0 + 0 = 0$.\n\nThe variances are calculated using the independence of the source variables:\n$\\operatorname{Var}(X) = \\operatorname{Var}(U + \\varepsilon_{1}) = \\operatorname{Var}(U) + \\operatorname{Var}(\\varepsilon_{1}) = 1 + 1 = 2$.\n$\\operatorname{Var}(Y) = \\operatorname{Var}(U + \\varepsilon_{2}) = \\operatorname{Var}(U) + \\operatorname{Var}(\\varepsilon_{2}) = 1 + 1 = 2$.\n\nThe covariance between $X$ and $Y$ is:\n$\\operatorname{Cov}(X, Y) = \\operatorname{Cov}(U + \\varepsilon_{1}, U + \\varepsilon_{2})$.\nUsing the bilinearity of covariance and the mutual independence of $U, \\varepsilon_{1}, \\varepsilon_{2}$:\n$\\operatorname{Cov}(X, Y) = \\operatorname{Cov}(U, U) + \\operatorname{Cov}(U, \\varepsilon_{2}) + \\operatorname{Cov}(\\varepsilon_{1}, U) + \\operatorname{Cov}(\\varepsilon_{1}, \\varepsilon_{2})$\n$\\operatorname{Cov}(X, Y) = \\operatorname{Var}(U) + 0 + 0 + 0 = 1$.\n\n**1) Marginal Correlation between $X$ and $Y$**\n\nUsing the quantities derived above:\n$$\n\\operatorname{Corr}(X, Y) = \\frac{\\operatorname{Cov}(X, Y)}{\\sqrt{\\operatorname{Var}(X) \\operatorname{Var}(Y)}} = \\frac{1}{\\sqrt{2 \\cdot 2}} = \\frac{1}{2}\n$$\nThe marginal correlation is positive, which is expected as $U$ is a common cause of both $X$ and $Y$.\n\n**2) Spurious Correlation between $X$ and $Y$ conditional on $Z$**\n\nSince $(X, Y, Z)$ are jointly Gaussian, the correlation between $X$ and $Y$ conditional on $Z$ is the partial correlation $\\rho_{XY \\cdot Z}$, which can be calculated using the formula:\n$$\n\\rho_{XY \\cdot Z} = \\frac{\\rho_{XY} - \\rho_{XZ}\\rho_{YZ}}{\\sqrt{(1 - \\rho_{XZ}^2)(1 - \\rho_{YZ}^2)}}\n$$\nwhere $\\rho_{AB}$ denotes $\\operatorname{Corr}(A, B)$. We have already found $\\rho_{XY} = \\frac{1}{2}$. We now need to compute $\\rho_{XZ}$ and $\\rho_{YZ}$. This requires the variance of $Z$ and the covariances $\\operatorname{Cov}(X, Z)$ and $\\operatorname{Cov}(Y, Z)$.\n\nFirst, express $Z$ in terms of the independent source variables:\n$Z = X + Y + \\varepsilon_{3} = (U + \\varepsilon_{1}) + (U + \\varepsilon_{2}) + \\varepsilon_{3} = 2U + \\varepsilon_{1} + \\varepsilon_{2} + \\varepsilon_{3}$.\n\nThe variance of $Z$ is:\n$\\operatorname{Var}(Z) = \\operatorname{Var}(2U + \\varepsilon_{1} + \\varepsilon_{2} + \\varepsilon_{3})$\nDue to the mutual independence of $U, \\varepsilon_{1}, \\varepsilon_{2}, \\varepsilon_{3}$:\n$\\operatorname{Var}(Z) = \\operatorname{Var}(2U) + \\operatorname{Var}(\\varepsilon_{1}) + \\operatorname{Var}(\\varepsilon_{2}) + \\operatorname{Var}(\\varepsilon_{3}) = 2^2\\operatorname{Var}(U) + \\operatorname{Var}(\\varepsilon_{1}) + \\operatorname{Var}(\\varepsilon_{2}) + \\operatorname{Var}(\\varepsilon_{3})$\n$\\operatorname{Var}(Z) = 4(1) + 1 + 1 + \\frac{1}{10} = 6 + \\frac{1}{10} = \\frac{61}{10}$.\n\nNow, we compute the covariance $\\operatorname{Cov}(X, Z)$:\n$\\operatorname{Cov}(X, Z) = \\operatorname{Cov}(X, X + Y + \\varepsilon_{3}) = \\operatorname{Cov}(X, X) + \\operatorname{Cov}(X, Y) + \\operatorname{Cov}(X, \\varepsilon_{3})$\n$\\operatorname{Cov}(X, Z) = \\operatorname{Var}(X) + \\operatorname{Cov}(X, Y) + \\operatorname{Cov}(U + \\varepsilon_{1}, \\varepsilon_{3})$\nSince $U$ and $\\varepsilon_{1}$ are independent of $\\varepsilon_{3}$, $\\operatorname{Cov}(U + \\varepsilon_{1}, \\varepsilon_{3}) = 0$.\n$\\operatorname{Cov}(X, Z) = 2 + 1 + 0 = 3$.\n\nBy symmetry in the model structure with respect to $X$ and $Y$, we can deduce $\\operatorname{Cov}(Y, Z) = 3$. Let's verify:\n$\\operatorname{Cov}(Y, Z) = \\operatorname{Cov(Y, X + Y + \\varepsilon_{3})} = \\operatorname{Cov}(Y, X) + \\operatorname{Var}(Y) + \\operatorname{Cov}(Y, \\varepsilon_{3}) = 1 + 2 + 0 = 3$.\n\nNow we can calculate the correlations $\\rho_{XZ}$ and $\\rho_{YZ}$:\n$$\n\\rho_{XZ} = \\operatorname{Corr}(X, Z) = \\frac{\\operatorname{Cov}(X, Z)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Z)}} = \\frac{3}{\\sqrt{2 \\cdot \\frac{61}{10}}} = \\frac{3}{\\sqrt{\\frac{61}{5}}}\n$$\n$$\n\\rho_{YZ} = \\operatorname{Corr}(Y, Z) = \\frac{\\operatorname{Cov}(Y, Z)}{\\sqrt{\\operatorname{Var}(Y)\\operatorname{Var}(Z)}} = \\frac{3}{\\sqrt{2 \\cdot \\frac{61}{10}}} = \\frac{3}{\\sqrt{\\frac{61}{5}}}\n$$\nWe need the squares of these correlations:\n$\\rho_{XZ}^2 = \\rho_{YZ}^2 = \\frac{3^2}{(\\sqrt{61/5})^2} = \\frac{9}{61/5} = \\frac{45}{61}$.\n\nNow we substitute all the correlation values into the partial correlation formula:\n$$\n\\rho_{XY \\cdot Z} = \\frac{\\rho_{XY} - \\rho_{XZ}\\rho_{YZ}}{\\sqrt{(1 - \\rho_{XZ}^2)(1 - \\rho_{YZ}^2)}} = \\frac{\\frac{1}{2} - \\left(\\frac{45}{61}\\right)}{\\sqrt{\\left(1 - \\frac{45}{61}\\right)\\left(1 - \\frac{45}{61}\\right)}} = \\frac{\\frac{1}{2} - \\frac{45}{61}}{1 - \\frac{45}{61}}\n$$\nWe compute the numerator and denominator:\nNumerator: $\\frac{1}{2} - \\frac{45}{61} = \\frac{61 \\cdot 1 - 45 \\cdot 2}{122} = \\frac{61 - 90}{122} = -\\frac{29}{122}$.\nDenominator: $1 - \\frac{45}{61} = \\frac{61 - 45}{61} = \\frac{16}{61}$.\n\nFinally, the conditional correlation is:\n$$\n\\rho_{XY \\cdot Z} = \\frac{-29/122}{16/61} = -\\frac{29}{122} \\cdot \\frac{61}{16}\n$$\nSince $122 = 2 \\cdot 61$, we can simplify:\n$$\n\\rho_{XY \\cdot Z} = -\\frac{29}{2 \\cdot 61} \\cdot \\frac{61}{16} = -\\frac{29}{2 \\cdot 16} = -\\frac{29}{32}\n$$\nThis result demonstrates the phenomenon of collider bias. Conditioning on the collider $Z$ has induced a strong negative correlation between $X$ and $Y$, altering their initial positive marginal correlation.\nThe spurious conditioned correlation is exactly $-\\frac{29}{32}$.", "answer": "$$\\boxed{-\\frac{29}{32}}$$", "id": "4318057"}]}