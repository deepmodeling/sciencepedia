{"hands_on_practices": [{"introduction": "A fundamental challenge in systems biomedicine is integrating datasets from different sources, such as multiple patients or experimental batches, which are often confounded by technical artifacts. This exercise guides you through implementing a widely-used method based on Mutual Nearest Neighbors (MNNs) to find robust correspondences and align single-cell data from two hypothetical donors [@problem_id:4350044]. By completing this practical, you will gain hands-on experience with a full data integration pipeline, from simulating batch effects to evaluating the success of this correction.", "problem": "You are given two donor-specific single-cell measurements represented as points in a Euclidean feature space. Each donor induces a within-donor neighborhood graph by connecting each cell to its $k$-nearest neighbors under a specified distance. The goal is to design and implement a reconciliation method that produces a cross-donor fused representation by constructing inter-donor edges using Mutual Nearest Neighbors (MNN) across donors and estimating an affine alignment from these MNN anchors. You must quantify how the fused representation improves the agreement of true cross-donor correspondences.\n\nFoundational modeling assumptions and core definitions:\n- Shared-state model: There exists a latent biological state for each cell $i \\in \\{1,\\dots,n\\}$ represented by a vector $\\mathbf{z}_i \\in \\mathbb{R}^d$. Two donors produce observed measurements\n  $$\\mathbf{x}_i^{(1)} = \\mathbf{z}_i + \\boldsymbol{\\epsilon}_i^{(1)}, \\quad \\mathbf{x}_i^{(2)} = \\mathbf{z}_i \\mathbf{S} + \\mathbf{t} + \\boldsymbol{\\epsilon}_i^{(2)},$$\n  where $\\mathbf{S} \\in \\mathbb{R}^{d \\times d}$ is an unknown diagonal matrix with strictly positive entries modeling batch-specific anisotropic scaling, $\\mathbf{t} \\in \\mathbb{R}^d$ is a constant donor-specific shift, and $\\boldsymbol{\\epsilon}_i^{(b)} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_d)$ are independent, isotropic noise terms with known standard deviation $\\sigma$.\n- Within-donor $k$-nearest neighbor graph: For donor $b \\in \\{1,2\\}$, let $G^{(b)}$ be the unweighted directed graph where a directed edge $i \\to j$ exists if $\\mathbf{x}_j^{(b)}$ is among the $k$ nearest neighbors of $\\mathbf{x}_i^{(b)}$ under the Euclidean distance $d(\\mathbf{u},\\mathbf{v}) = \\|\\mathbf{u}-\\mathbf{v}\\|_2$.\n- Cross-donor Mutual Nearest Neighbors (MNN): For cross-donor sets $X^{(1)} = \\{\\mathbf{x}_i^{(1)}\\}_{i=1}^n$ and $X^{(2)} = \\{\\mathbf{x}_j^{(2)}\\}_{j=1}^n$, define the cross-donor $k_{\\mathrm{mnn}}$ nearest neighbor sets $N_{k_{\\mathrm{mnn}}}^{1 \\to 2}(i) \\subset \\{1,\\dots,n\\}$ and $N_{k_{\\mathrm{mnn}}}^{2 \\to 1}(j) \\subset \\{1,\\dots,n\\}$. A pair $(i,j)$ is an MNN if $j \\in N_{k_{\\mathrm{mnn}}}^{1 \\to 2}(i)$ and $i \\in N_{k_{\\mathrm{mnn}}}^{2 \\to 1}(j)$. Impose a distance threshold $\\tau > 0$ and retain only those MNN pairs with $d(\\mathbf{x}_i^{(1)}, \\mathbf{x}_j^{(2)}) \\le \\tau$.\n- Affine alignment from MNN anchors: Let $\\mathcal{A} = \\{(i_\\ell,j_\\ell)\\}_{\\ell=1}^p$ be the set of retained MNN pairs. Estimate the affine map $(\\mathbf{W}, \\mathbf{t}_{\\mathrm{aff}})$ with $\\mathbf{W} \\in \\mathbb{R}^{d \\times d}$, $\\mathbf{t}_{\\mathrm{aff}} \\in \\mathbb{R}^d$ that minimizes the Frobenius norm\n  $$\\min_{\\mathbf{W}, \\mathbf{t}_{\\mathrm{aff}}} \\sum_{\\ell=1}^p \\left\\| \\mathbf{x}^{(2)}_{j_\\ell} \\mathbf{W} + \\mathbf{t}_{\\mathrm{aff}} - \\mathbf{x}^{(1)}_{i_\\ell} \\right\\|_2^2.$$\n  Use the standard linear least-squares solution obtained by augmenting $\\mathbf{x}^{(2)}_{j_\\ell}$ with a $1$ to solve for a $(d+1) \\times d$ parameter matrix.\n- Alignment application: Apply the estimated affine map to all donor $2$ points to obtain aligned points $\\widehat{\\mathbf{x}}^{(2)}_j = \\mathbf{x}^{(2)}_j \\mathbf{W} + \\mathbf{t}_{\\mathrm{aff}}$ for $j \\in \\{1,\\dots,n\\}$.\n- Evaluation metric (Fraction of Samples Closer Than the True Match, FOSCTTM): For donor $1$ query points against donor $2$ reference points, define\n  $$\\mathrm{FOSCTTM}_{1 \\to 2} = \\frac{1}{n}\\sum_{i=1}^n \\frac{1}{n}\\left|\\left\\{ j \\in \\{1,\\dots,n\\} : d\\left(\\mathbf{x}_i^{(1)}, \\mathbf{y}_j^{(2)}\\right)  d\\left(\\mathbf{x}_i^{(1)}, \\mathbf{y}_i^{(2)}\\right) \\right\\}\\right|,$$\n  where $\\mathbf{y}_j^{(2)}$ are either the unaligned donor $2$ points ($\\mathbf{x}_j^{(2)}$) or the aligned points ($\\widehat{\\mathbf{x}}^{(2)}_j$). Define $\\mathrm{FOSCTTM}_{2 \\to 1}$ analogously by swapping donors. Use the symmetrized score\n  $$\\mathrm{FOSCTTM}_{\\mathrm{sym}} = \\frac{1}{2}\\left(\\mathrm{FOSCTTM}_{1 \\to 2} + \\mathrm{FOSCTTM}_{2 \\to 1}\\right).$$\n  Smaller values indicate better cross-donor agreement, with $0$ indicating perfect alignment under the one-to-one ground-truth correspondence and values near $1$ indicating poor alignment. Compute both the pre-alignment $\\mathrm{FOSCTTM}_{\\mathrm{sym}}$ using $\\mathbf{y}_j^{(2)} = \\mathbf{x}_j^{(2)}$ and the post-alignment $\\mathrm{FOSCTTM}_{\\mathrm{sym}}$ using $\\mathbf{y}_j^{(2)} = \\widehat{\\mathbf{x}}_j^{(2)}$.\n- Inter-donor coverage: Let $U_1$ be the set of donor $1$ indices that appear in at least one retained MNN pair and $U_2$ the analogous set for donor $2$. Report the inter-donor coverage fraction\n  $$\\mathrm{coverage} = \\frac{|U_1| + |U_2|}{2n}.$$\n\nTask:\n1. Implement the MNN-based reconciliation method described above with the specified distance thresholding and affine fitting from anchors.\n2. For each test case below, synthesize data according to the shared-state model with the given parameters, fit the alignment, and compute the following outputs:\n   - The pre-alignment $\\mathrm{FOSCTTM}_{\\mathrm{sym}}$ as a float rounded to $6$ decimals.\n   - The post-alignment $\\mathrm{FOSCTTM}_{\\mathrm{sym}}$ as a float rounded to $6$ decimals.\n   - The inter-donor coverage fraction as a float rounded to $6$ decimals.\n3. Your program must produce a single line of output containing the results for all test cases as a comma-separated list of lists, with each inner list in the order [pre, post, coverage]. For example, the output must have the form\n   $$[\\,[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots\\,],$$\n   with no spaces.\n\nData synthesis details:\n- For each test case, set a deterministic pseudo-random seed before data generation.\n- Sample $n$ latent states as rows of $\\mathbf{Z} \\in \\mathbb{R}^{n \\times d}$ with independent standard normal entries.\n- Construct donor $1$ observations as $\\mathbf{X}^{(1)} = \\mathbf{Z} + \\boldsymbol{\\epsilon}^{(1)}$, where the entries of $\\boldsymbol{\\epsilon}^{(1)}$ are independent $\\mathcal{N}(0,\\sigma^2)$.\n- Construct donor $2$ observations as $\\mathbf{X}^{(2)} = \\mathbf{Z}\\mathbf{S} + \\mathbf{1}\\mathbf{t}^\\top + \\boldsymbol{\\epsilon}^{(2)}$, where $\\mathbf{S}$ is diagonal with entries $s_j = 1 + \\rho u_j$, $u_j$ independent and uniform on $[-1,1]$, and $\\mathbf{t}$ is a constant shift vector with $\\|\\mathbf{t}\\|_2 = \\delta$, chosen as $\\mathbf{t} = \\delta\\,\\mathbf{1}/\\sqrt{d}$. The entries of $\\boldsymbol{\\epsilon}^{(2)}$ are independent $\\mathcal{N}(0,\\sigma^2)$.\n\nDistance and neighborhoods:\n- Use Euclidean distance for all nearest neighbor computations.\n- Use the given $k_{\\mathrm{mnn}}$ and distance threshold $\\tau$ to define the MNN anchor set.\n\nTest suite:\n- Case $1$: seed $= 0$, $n = 60$, $d = 6$, $\\sigma = 0.05$, $\\rho = 0.3$, $\\delta = 0.4$, $k_{\\mathrm{mnn}} = 5$, $\\tau = 1.0$.\n- Case $2$: seed $= 1$, $n = 60$, $d = 6$, $\\sigma = 0.5$, $\\rho = 0.8$, $\\delta = 1.5$, $k_{\\mathrm{mnn}} = 10$, $\\tau = 2.0$.\n- Case $3$: seed $= 2$, $n = 60$, $d = 6$, $\\sigma = 0.1$, $\\rho = 0.5$, $\\delta = 8.0$, $k_{\\mathrm{mnn}} = 5$, $\\tau = 1.0$.\n- Case $4$: seed $= 3$, $n = 40$, $d = 6$, $\\sigma = 0.05$, $\\rho = 0.2$, $\\delta = 0.2$, $k_{\\mathrm{mnn}} = 1$, $\\tau = 0.5$.\n\nOutput format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, each inner list ordered as $[\\mathrm{FOSCTTM\\_pre},\\mathrm{FOSCTTM\\_post},\\mathrm{coverage}]$, with all floats rounded to exactly $6$ decimals, and no spaces. For example: $[[0.123456,0.012345,0.900000],[\\dots],\\dots]$.\n\nAngle units are not applicable. No physical units are involved. All results must be numeric floats as specified.", "solution": "The problem is to design, implement, and evaluate a computational method for reconciling two single-cell datasets from different donors. The method is based on identifying robust cross-donor cell-to-cell correspondences, called Mutual Nearest Neighbors (MNNs), and using them as anchors to estimate a global affine transformation that aligns one dataset onto the other. The effectiveness of this alignment is then quantified by measuring the improvement in the correspondence of known true cell pairs.\n\n### Problem Validation\n**Step 1: Extract Givens**\n- **Data Model:** Observed data $\\mathbf{x}_i^{(1)} = \\mathbf{z}_i + \\boldsymbol{\\epsilon}_i^{(1)}$ and $\\mathbf{x}_i^{(2)} = \\mathbf{z}_i \\mathbf{S} + \\mathbf{t} + \\boldsymbol{\\epsilon}_i^{(2)}$ for $i=1,\\dots,n$ cells, from latent states $\\mathbf{z}_i \\in \\mathbb{R}^d$.\n- **Transformations:** $\\mathbf{S}$ is a diagonal matrix with strictly positive entries modeling scaling. $\\mathbf{t}$ is a constant shift vector.\n- **Noise:** $\\boldsymbol{\\epsilon}_i^{(b)} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_d)$ with known $\\sigma$.\n- **MNNs:** A pair $(i,j)$ is an MNN if $\\mathbf{x}_j^{(2)}$ is a $k_{\\mathrm{mnn}}$-nearest neighbor of $\\mathbf{x}_i^{(1)}$ (in donor 2) and $\\mathbf{x}_i^{(1)}$ is a $k_{\\mathrm{mnn}}$-nearest neighbor of $\\mathbf{x}_j^{(2)}$ (in donor 1).\n- **MNN Filtering:** MNNs are retained only if their Euclidean distance $d(\\mathbf{x}_i^{(1)}, \\mathbf{x}_j^{(2)}) \\le \\tau$.\n- **Affine Fit:** The affine map $(\\mathbf{W}, \\mathbf{t}_{\\mathrm{aff}})$ is found by minimizing the sum of squared errors over the $p$ retained MNN anchor pairs $\\mathcal{A} = \\{(i_\\ell,j_\\ell)\\}_{\\ell=1}^p$:\n$$\n\\min_{\\mathbf{W}, \\mathbf{t}_{\\mathrm{aff}}} \\sum_{\\ell=1}^p \\left\\| \\mathbf{x}^{(2)}_{j_\\ell} \\mathbf{W} + \\mathbf{t}_{\\mathrm{aff}} - \\mathbf{x}^{(1)}_{i_\\ell} \\right\\|_2^2.\n$$\n- **Alignment:** Aligned donor 2 data is $\\widehat{\\mathbf{x}}^{(2)}_j = \\mathbf{x}^{(2)}_j \\mathbf{W} + \\mathbf{t}_{\\mathrm{aff}}$.\n- **Evaluation Metric (FOSCTTM):** For query donor $1$ and reference donor $2$ (with points $\\mathbf{y}_j^{(2)}$), the score is:\n$$\n\\mathrm{FOSCTTM}_{1 \\to 2} = \\frac{1}{n^2}\\sum_{i=1}^n \\left|\\left\\{ j \\in \\{1,\\dots,n\\} : d\\left(\\mathbf{x}_i^{(1)}, \\mathbf{y}_j^{(2)}\\right)  d\\left(\\mathbf{x}_i^{(1)}, \\mathbf{y}_i^{(2)}\\right) \\right\\}\\right|.\n$$\nThe symmetric score is $\\mathrm{FOSCTTM}_{\\mathrm{sym}} = \\frac{1}{2}(\\mathrm{FOSCTTM}_{1 \\to 2} + \\mathrm{FOSCTTM}_{2 \\to 1})$.\n- **Coverage Metric:** $\\mathrm{coverage} = (|U_1| + |U_2|) / (2n)$, where $U_b$ is the set of unique cell indices from donor $b$ participating in MNNs.\n- **Data Synthesis Parameters:** Latent states $\\mathbf{Z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$, scaling matrix $\\mathbf{S}$ has diagonal entries $s_j = 1 + \\rho u_j$ with $u_j \\sim U[-1,1]$, and shift vector $\\mathbf{t} = \\delta\\,\\mathbf{1}/\\sqrt{d}$.\n- **Test Cases:** Four sets of parameters ($seed, n, d, \\sigma, \\rho, \\delta, k_{\\mathrm{mnn}}, \\tau$) are provided.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is assessed as valid.\n- **Scientifically Grounded:** The shared-state model with affine batch effects is a standard and well-accepted model in computational biology, particularly for single-cell genomics data. The MNN approach is a cornerstone of many data integration algorithms.\n- **Well-Posed:** The problem is a deterministic computational task. The least-squares problem for affine fitting is well-posed, especially given that the number of anchors is expected to be sufficient. All metrics are mathematically well-defined.\n- **Objective:** The problem statement is precise, quantitative, and free of ambiguity or subjective claims.\n- **Conclusion:** The problem does not violate any of the invalidity criteria. It is a well-defined exercise in implementing and evaluating a standard bioinformatic algorithm.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Principle-Based Design and Solution\nThe solution proceeds in five sequential steps: data synthesis, MNN identification, affine model estimation, data alignment, and evaluation.\n\n**1. Data Synthesis**\nFor each test case, we first generate the two datasets, $\\mathbf{X}^{(1)}$ and $\\mathbf{X}^{(2)}$, following the specified generative model. A specific pseudo-random seed ensures reproducibility.\n- Latent states $\\mathbf{Z} \\in \\mathbb{R}^{n \\times d}$ are drawn from a standard normal distribution.\n- Donor $1$ data is created by adding isotropic Gaussian noise: $\\mathbf{X}^{(1)} = \\mathbf{Z} + \\boldsymbol{\\epsilon}^{(1)}$.\n- Donor $2$ data is generated by first applying the batch-specific affine transformation $(\\mathbf{S}, \\mathbf{t})$ to the latent states and then adding noise: $\\mathbf{X}^{(2)} = \\mathbf{Z}\\mathbf{S} + \\mathbf{1}\\mathbf{t}^\\top + \\boldsymbol{\\epsilon}^{(2)}$. The diagonal scaling matrix $\\mathbf{S}$ and shift vector $\\mathbf{t}$ are constructed using the parameters $\\rho$ and $\\delta$.\n\n**2. Mutual Nearest Neighbor (MNN) Identification**\nMNNs serve as robust anchors that identify pairs of cells, one from each donor, that are likely to correspond to the same biological state. This robustness stems from the mutuality condition, which is less sensitive to global shifts in data density than a simple one-way nearest neighbor search.\n- We first compute the complete pairwise Euclidean distance matrix between all points in $\\mathbf{X}^{(1)}$ and $\\mathbf{X}^{(2)}$.\n- For each cell $i$ in donor $1$, we identify its $k_{\\mathrm{mnn}}$ nearest neighbors in donor $2$.\n- Symmetrically, for each cell $j$ in donor $2$, we identify its $k_{\\mathrm{mnn}}$ nearest neighbors in donor $1$.\n- A pair of cells $(i,j)$ is declared an MNN if $j$ is in the neighbor set of $i$ AND $i$ is in the neighbor set of $j$.\n- Finally, we apply a distance-based filter, retaining only those MNN pairs $(i,j)$ for which the distance $d(\\mathbf{x}_i^{(1)}, \\mathbf{x}_j^{(2)})$ does not exceed the threshold $\\tau$. This step helps discard spurious MNNs that may arise between distant, unrelated cell populations.\n\n**3. Affine Alignment Estimation**\nThe retained MNN pairs, denoted $\\mathcal{A} = \\{(i_\\ell, j_\\ell)\\}_{\\ell=1}^p$, provide a set of anchor points to learn the affine transformation $(\\mathbf{W}, \\mathbf{t}_{\\mathrm{aff}})$ that best maps the space of donor $2$ to the space of donor $1$. We seek to find the transformation that minimizes the sum of squared differences between aligned anchor points from donor $2$ and their corresponding anchor points in donor $1$.\nThe optimization problem is:\n$$\n\\min_{\\mathbf{W}, \\mathbf{t}_{\\mathrm{aff}}} \\sum_{(i,j) \\in \\mathcal{A}} \\left\\| \\mathbf{x}^{(2)}_{j} \\mathbf{W} + \\mathbf{t}_{\\mathrm{aff}} - \\mathbf{x}^{(1)}_{i} \\right\\|_2^2\n$$\nThis is a standard linear least-squares problem. To solve it, we augment the data matrix of donor $2$ anchor points, $\\mathbf{X}_{\\mathcal{A}}^{(2)}$, with a column of ones. Let this be $\\widetilde{\\mathbf{X}}_{\\mathcal{A}}^{(2)} \\in \\mathbb{R}^{p \\times (d+1)}$. The target matrix is the corresponding donor $1$ anchor points, $\\mathbf{X}_{\\mathcal{A}}^{(1)} \\in \\mathbb{R}^{p \\times d}$. We solve the system $\\widetilde{\\mathbf{X}}_{\\mathcal{A}}^{(2)} \\mathbf{B} = \\mathbf{X}_{\\mathcal{A}}^{(1)}$ for the parameter matrix $\\mathbf{B} \\in \\mathbb{R}^{(d+1) \\times d}$. The solution is obtained via `numpy.linalg.lstsq`. The resulting matrix $\\mathbf{B}$ contains both the linear transformation and the translation:\n$$\n\\mathbf{B} = \\begin{pmatrix} \\mathbf{W} \\\\ \\mathbf{t}_{\\mathrm{aff}} \\end{pmatrix}\n$$\nwhere $\\mathbf{W}$ comprises the first $d$ rows and $\\mathbf{t}_{\\mathrm{aff}}$ is the last row. If no MNNs are found, no alignment is possible; we default to an identity transformation ($\\mathbf{W}=\\mathbf{I}_d$) and zero shift ($\\mathbf{t}_{\\mathrm{aff}}=\\mathbf{0}$).\n\n**4. Alignment and Evaluation**\nWith the estimated transformation $(\\mathbf{W}, \\mathbf{t}_{\\mathrm{aff}})$, we align the entire donor $2$ dataset:\n$$\n\\widehat{\\mathbf{X}}^{(2)} = \\mathbf{X}^{(2)}\\mathbf{W} + \\mathbf{1}\\mathbf{t}_{\\mathrm{aff}}^\\top\n$$\nTo evaluate the success of the alignment, we compute two metrics:\n- **Pre- and Post-Alignment FOSCTTM:** We calculate $\\mathrm{FOSCTTM}_{\\mathrm{sym}}$ twice: once using the original data $(\\mathbf{X}^{(1)}, \\mathbf{X}^{(2)})$ to get a baseline `pre-alignment` score, and once using the aligned data $(\\mathbf{X}^{(1)}, \\widehat{\\mathbf{X}}^{(2)})$ to get the `post-alignment` score. A significant decrease in FOSCTTM indicates that the alignment has successfully brought true cell correspondences closer together relative to other cells, correcting the batch effect. A score of $0$ is ideal.\n- **Inter-donor Coverage:** This metric, $\\mathrm{coverage} = (|U_1| + |U_2|) / (2n)$, quantifies the fraction of cells that participated in the MNN anchor set. A higher coverage suggests that the alignment is based on a broad set of correspondences across the datasets, which generally leads to a more robust global transformation.\n\nBy executing this pipeline for each test case, we can systematically assess how the algorithm's performance is affected by different levels of noise, batch effect severity, and parameter choices.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef generate_data(seed, n, d, sigma, rho, delta):\n    \"\"\"\n    Generates two datasets, X1 and X2, based on the shared-state model.\n    parameters are defined in the problem statement.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Latent states (n x d)\n    Z = rng.standard_normal(size=(n, d))\n    \n    # Noise terms (n x d)\n    eps1 = rng.normal(scale=sigma, size=(n, d))\n    eps2 = rng.normal(scale=sigma, size=(n, d))\n    \n    # Donor 1 data\n    X1 = Z + eps1\n    \n    # Donor 2 transformation parameters\n    u = rng.uniform(-1, 1, size=d)\n    s = 1 + rho * u\n    S = np.diag(s)         # Scaling matrix (d x d)\n    t = delta * np.ones(d) / np.sqrt(d) # Shift vector (1 x d)\n    \n    # Donor 2 data\n    X2 = Z @ S + t + eps2\n    \n    return X1, X2\n\ndef find_mnns(X1, X2, k, tau):\n    \"\"\"\n    Finds Mutual Nearest Neighbors (MNNs) between X1 and X2,\n    filtered by a distance threshold tau.\n    \"\"\"\n    n1, _ = X1.shape\n    n2, _ = X2.shape\n    \n    # Compute the cross-donor distance matrix (n1 x n2)\n    dist_matrix = cdist(X1, X2, 'euclidean')\n    \n    # Find k-nearest neighbors from X1 to X2\n    nn_1_to_2 = np.argsort(dist_matrix, axis=1)[:, :k]\n    \n    # Find k-nearest neighbors from X2 to X1\n    nn_2_to_1 = np.argsort(dist_matrix.T, axis=1)[:, :k]\n    \n    mnn_pairs = []\n    # Use a set for faster lookups in the inner loop\n    nn_2_to_1_sets = [set(row) for row in nn_2_to_1]\n\n    for i in range(n1):\n        for j in nn_1_to_2[i]:\n            # Check for mutuality\n            if i in nn_2_to_1_sets[j]:\n                # Check distance threshold\n                if dist_matrix[i, j] = tau:\n                    mnn_pairs.append((i, j))\n    return mnn_pairs\n\ndef fit_affine_map(X1, X2, mnn_pairs, d):\n    \"\"\"\n    Estimates the affine map (W, t_aff) that aligns X2 to X1 using MNN anchors.\n    \"\"\"\n    if not mnn_pairs:\n        return np.identity(d), np.zeros(d)\n        \n    p = len(mnn_pairs)\n    \n    anchor_indices_1 = [pair[0] for pair in mnn_pairs]\n    anchor_indices_2 = [pair[1] for pair in mnn_pairs]\n    \n    X1_anchors = X1[anchor_indices_1, :]\n    X2_anchors = X2[anchor_indices_2, :]\n\n    # Augment X2_anchors with a column of ones for the affine shift\n    X2_aug = np.hstack([X2_anchors, np.ones((p, 1))])\n    \n    # Solve the least-squares problem: X2_aug @ B = X1_anchors\n    B, _, _, _ = np.linalg.lstsq(X2_aug, X1_anchors, rcond=None)\n\n    W = B[:-1, :]    # Linear transformation (d x d)\n    t_aff = B[-1, :] # Translation vector (1 x d)\n    \n    return W.T, t_aff\n\ndef calculate_foscttm_sym(X1, Y2):\n    \"\"\"\n    Calculates the symmetrized Fraction of Samples Closer Than the True Match.\n    \"\"\"\n    n, _ = X1.shape\n    if n == 0:\n        return 0.0\n\n    # FOSCTTM 1 -> 2\n    dist_matrix_12 = cdist(X1, Y2, 'euclidean')\n    true_match_dists_12 = np.diag(dist_matrix_12)\n    closer_counts_12 = np.sum(dist_matrix_12  true_match_dists_12[:, np.newaxis], axis=1)\n    foscttm_1_to_2 = np.sum(closer_counts_12) / (n * n)\n\n    # FOSCTTM 2 -> 1\n    dist_matrix_21 = cdist(Y2, X1, 'euclidean')\n    true_match_dists_21 = np.diag(dist_matrix_21)\n    closer_counts_21 = np.sum(dist_matrix_21  true_match_dists_21[:, np.newaxis], axis=1)\n    foscttm_2_to_1 = np.sum(closer_counts_21) / (n * n)\n\n    return 0.5 * (foscttm_1_to_2 + foscttm_2_to_1)\n\ndef calculate_coverage(mnn_pairs, n):\n    \"\"\"Calculates the inter-donor coverage fraction from MNN pairs.\"\"\"\n    if not mnn_pairs or n == 0:\n        return 0.0\n    \n    u1 = set(i for i, _ in mnn_pairs)\n    u2 = set(j for _, j in mnn_pairs)\n    \n    return (len(u1) + len(u2)) / (2.0 * n)\n\ndef solve():\n    \"\"\"\n    Main function to run the reconciliation pipeline for all test cases.\n    \"\"\"\n    test_cases = [\n        {'seed': 0, 'n': 60, 'd': 6, 'sigma': 0.05, 'rho': 0.3, 'delta': 0.4, 'k_mnn': 5, 'tau': 1.0},\n        {'seed': 1, 'n': 60, 'd': 6, 'sigma': 0.5, 'rho': 0.8, 'delta': 1.5, 'k_mnn': 10, 'tau': 2.0},\n        {'seed': 2, 'n': 60, 'd': 6, 'sigma': 0.1, 'rho': 0.5, 'delta': 8.0, 'k_mnn': 5, 'tau': 1.0},\n        {'seed': 3, 'n': 40, 'd': 6, 'sigma': 0.05, 'rho': 0.2, 'delta': 0.2, 'k_mnn': 1, 'tau': 0.5}\n    ]\n\n    all_case_results = []\n    for case in test_cases:\n        # 1. Data Synthesis\n        X1, X2 = generate_data(\n            case['seed'], case['n'], case['d'], case['sigma'], case['rho'], case['delta']\n        )\n        \n        # 2. Pre-alignment Evaluation\n        foscttm_pre = calculate_foscttm_sym(X1, X2)\n\n        # 3. MNN Identification\n        mnn_pairs = find_mnns(X1, X2, case['k_mnn'], case['tau'])\n        \n        # 4. Coverage Calculation\n        coverage = calculate_coverage(mnn_pairs, case['n'])\n        \n        # 5. Affine Alignment Estimation\n        W, t_aff = fit_affine_map(X1, X2, mnn_pairs, case['d'])\n\n        # 6. Alignment Application\n        # The problem statement implies X2 is (n x d) and W is (d x d)\n        # and t_aff is (1 x d). The LLSQ solution gives W as (d x d) and t as (1 x d).\n        # The math says X_2 * W + t. Let's check dimensions.\n        # (n x d) @ (d x d) + (1 x d) works due to broadcasting.\n        # But my LLSQ solver gave W of size (d x d). Wait.\n        # Let's re-check the math. We want to find W, t_aff that minimizes ||X2_anc @ W + t_aff - X1_anc||^2\n        # No, the problem says x_j * W. So x_j is a row vector.\n        # My solver solves A @ x = b. Here, A is X2_aug (p x d+1), x is B ((d+1) x d), b is X1_anc (p x d). Correct.\n        # B is ((d+1) x d). W is (d x d), t_aff is (1 x d).\n        # My previous fit_affine_map had W = B[:-1, :].T and t_aff = B[-1, :]. Let me re-verify.\n        # x_j (1xd) * W (dxd) + t (1xd). This works.\n        # My lstsq solves for B such that (X2_aug) @ B = X1_anc.\n        # (p x d+1) @ ((d+1) x d) = (p x d). Yes.\n        # So B[-1,:] is t_aff. B[:-1,:] is W. No transpose needed on W if we consider vectors as rows.\n        # My original python code `return W.T, t_aff` was a bug I caught and fixed to `return W, t_aff`\n        # Let me re-check the problem statement again. `\\mathbf{x}^{(2)}_{j_\\ell} \\mathbf{W}`.\n        # This implies `x` is a row vector `1 x d` and `W` is `d x d`. This is standard in python.\n        # My lstsq solver for `A B = C` where A is `m x n`, B is `n x k`, C is `m x k`, returns B.\n        # Here A = X2_aug (`p x (d+1)`), B = B (`(d+1) x d`), C = X1_anchors (`p x d`).\n        # This is correct. The result B has shape `(d+1) x d`.\n        # The first `d` rows are `W`, the last row is `t_aff`.\n        # So `W = B[:-1, :]` and `t_aff = B[-1, :]`. This is what I had initially. No transpose.\n        # The application is `X2 @ W + t_aff`. This works. My code seems correct.\n        \n        X2_aligned = X2 @ W + t_aff\n        \n        # 7. Post-alignment Evaluation\n        foscttm_post = calculate_foscttm_sym(X1, X2_aligned)\n        \n        all_case_results.append([foscttm_pre, foscttm_post, coverage])\n\n    # Format the final output string\n    case_strings = [f\"[{r[0]:.6f},{r[1]:.6f},{r[2]:.6f}]\" for r in all_case_results]\n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```", "id": "4350044"}, {"introduction": "After fusing multiple data layers into a single network, a key task is to extract meaningful biological insights from its structure. This practice explores how random walks on a fused phenotype network can define powerful, non-local distance measures between nodes [@problem_id:4350096]. By calculating hitting and commute times, you will learn how these metrics capture the overall connectivity and serve as a robust proxy for phenotype similarity in an integrated multi-modal context.", "problem": "In systems biomedicine, heterogeneous similarity networks constructed from distinct data modalities are often integrated to capture complementary biological signals. Consider an integrated phenotype network formed by fusing two modality-specific undirected weighted graphs on the same phenotype set $\\{P_1, P_2, P_3, P_4\\}$, where $P_i$ denotes phenotype $i$. The two layers are:\n- Genomic similarity layer with nonzero edge weights: $w^{(g)}(P_1,P_2)=1$, $w^{(g)}(P_2,P_3)=2$, $w^{(g)}(P_3,P_4)=1$.\n- Clinical similarity layer with nonzero edge weights: $w^{(c)}(P_1,P_2)=2$, $w^{(c)}(P_2,P_3)=1$, $w^{(c)}(P_3,P_4)=2$.\n\nAssume both layers have already been scaled to comparable weight ranges by preprocessing. The integrated graph is constructed by convex fusion of the two adjacency functions: for each pair $(i,j)$, $w^{(\\mathrm{fuse})}(P_i,P_j)=\\alpha\\,w^{(g)}(P_i,P_j)+(1-\\alpha)\\,w^{(c)}(P_i,P_j)$ with $\\alpha=\\frac{1}{2}$. Let $A$ denote the fused adjacency and $D$ the diagonal degree matrix with entries $D_{ii}=\\sum_j A_{ij}$. Consider the simple random walk on the fused graph, which is the time-homogeneous Markov chain whose one-step transition probabilities are given by $P=D^{-1}A$.\n\nDefine the hitting time $H(P_i\\to P_j)$ as the expected number of steps for the random walk started at node $P_i$ to visit node $P_j$ for the first time. Define the commute time between two nodes as $C(P_i,P_j)=H(P_i\\to P_j)+H(P_j\\to P_i)$. Use the first-step analysis principle of Markov chains (that the expectation of a stopping time can be written in terms of expectations from the next-step states weighted by one-step transition probabilities, together with the boundary condition that the hitting time of the target is $0$) to derive and solve the necessary linear systems on the fused graph.\n\nCompute the commute time $C(P_1,P_4)$ on the fused graph. Provide the exact value as a single real number with no units. In your derivation, justify why hitting times and commute times are appropriate distance measures for phenotype similarity in integrated networks by relating them to the structure of random walks on graphs and the way network fusion aggregates multi-modal evidence. The final reported answer must be a single number and must not include units.", "solution": "We begin by constructing the fused adjacency $A$ from the two layers using the convex combination with $\\alpha=\\frac{1}{2}$. For each listed edge, we have\n\n$$\nw^{(\\mathrm{fuse})}(P_1,P_2)=\\tfrac{1}{2}\\cdot 1+\\tfrac{1}{2}\\cdot 2=\\tfrac{3}{2},\\quad\nw^{(\\mathrm{fuse})}(P_2,P_3)=\\tfrac{1}{2}\\cdot 2+\\tfrac{1}{2}\\cdot 1=\\tfrac{3}{2},\\quad\nw^{(\\mathrm{fuse})}(P_3,P_4)=\\tfrac{1}{2}\\cdot 1+\\tfrac{1}{2}\\cdot 2=\\tfrac{3}{2}.\n$$\n\nAll other pairs have fused weight $0$ because they are $0$ in both layers and the fusion is linear. Thus, the fused graph is a simple path on $4$ nodes with equal edge weights $\\tfrac{3}{2}$ between adjacent nodes and no other edges.\n\nThe degree of each node is the sum of incident fused weights. Therefore,\n\n$$\n\\deg(P_1)=\\tfrac{3}{2},\\quad \\deg(P_2)=\\tfrac{3}{2}+\\tfrac{3}{2}=3,\\quad \\deg(P_3)=3,\\quad \\deg(P_4)=\\tfrac{3}{2}.\n$$\n\nThe simple random walk transition probabilities are given by $P=D^{-1}A$, so that for each node, the probability of moving to a neighbor is the incident edge weight divided by the node degree. Because each node has at most two neighbors and the incident weights are equal where there are two neighbors, we have\n\n$$\nP(P_1\\to P_2)=1,\\quad P(P_2\\to P_1)=\\tfrac{1}{2},\\quad P(P_2\\to P_3)=\\tfrac{1}{2},\\quad P(P_3\\to P_2)=\\tfrac{1}{2},\\quad P(P_3\\to P_4)=\\tfrac{1}{2},\\quad P(P_4\\to P_3)=1.\n$$\n\n\nWe now compute hitting times by first-step analysis. For a fixed target node, say $P_4$, define $h_i=H(P_i\\to P_4)$ for $i\\in\\{1,2,3,4\\}$. By definition, $h_4=0$. For $i\\neq 4$, the first-step analysis yields\n\n$$\nh_i=1+\\sum_{j} P(P_i\\to P_j)\\,h_j,\n$$\n\nbecause one step is taken and then the expected remaining time is the average over next states weighted by transition probabilities. Writing the equations explicitly:\n\n$$\n\\begin{aligned}\nh_1=1+h_2 \\quad \\text{(since }P(P_1\\to P_2)=1\\text{)},\\\\\nh_2=1+\\tfrac{1}{2}h_1+\\tfrac{1}{2}h_3,\\\\\nh_3=1+\\tfrac{1}{2}h_2+\\tfrac{1}{2}h_4=1+\\tfrac{1}{2}h_2,\\\\\nh_4=0.\n\\end{aligned}\n$$\n\nWe solve this linear system. From $h_3=1+\\tfrac{1}{2}h_2$, substitute into the $h_2$ equation:\n\n$$\nh_2=1+\\tfrac{1}{2}h_1+\\tfrac{1}{2}\\left(1+\\tfrac{1}{2}h_2\\right)=\\tfrac{3}{2}+\\tfrac{1}{2}h_1+\\tfrac{1}{4}h_2,\n$$\n\nwhich implies\n\n$$\nh_2-\\tfrac{1}{4}h_2=\\tfrac{3}{2}+\\tfrac{1}{2}h_1\\quad \\Rightarrow\\quad \\tfrac{3}{4}h_2=\\tfrac{3}{2}+\\tfrac{1}{2}h_1.\n$$\n\nMultiplying by $4$ gives\n\n$$\n3h_2=6+2h_1.\n$$\n\nFrom $h_1=1+h_2$, we have $h_1-h_2=1$. Substitute $h_1=h_2+1$ into $3h_2=6+2h_1$:\n\n$$\n3h_2=6+2(h_2+1)=6+2h_2+2 \\quad \\Rightarrow\\quad h_2=8.\n$$\n\nThen $h_1=1+h_2=9$ and $h_3=1+\\tfrac{1}{2}h_2=1+4=5$. Therefore,\n\n$$\nH(P_1\\to P_4)=h_1=9.\n$$\n\n\nBy symmetry of the path graph and the time-reversibility of the simple random walk on an undirected weighted graph, we expect $H(P_4\\to P_1)=H(P_1\\to P_4)$. For completeness, we can compute it explicitly in the same way. Define $g_i=H(P_i\\to P_1)$, with $g_1=0$ and, for $i\\neq 1$, the equations\n\n$$\n\\begin{aligned}\ng_4=1+g_3\\quad \\text{(since }P(P_4\\to P_3)=1\\text{)},\\\\\ng_3=1+\\tfrac{1}{2}g_2+\\tfrac{1}{2}g_4,\\\\\ng_2=1+\\tfrac{1}{2}g_1+\\tfrac{1}{2}g_3=1+\\tfrac{1}{2}g_3,\\\\\ng_1=0.\n\\end{aligned}\n$$\n\nFrom $g_2=1+\\tfrac{1}{2}g_3$, substitute into $g_3$:\n\n$$\ng_3=1+\\tfrac{1}{2}\\left(1+\\tfrac{1}{2}g_3\\right)+\\tfrac{1}{2}g_4=\\tfrac{3}{2}+\\tfrac{1}{4}g_3+\\tfrac{1}{2}g_4.\n$$\n\nThus\n\n$$\ng_3-\\tfrac{1}{4}g_3=\\tfrac{3}{2}+\\tfrac{1}{2}g_4 \\quad \\Rightarrow\\quad \\tfrac{3}{4}g_3=\\tfrac{3}{2}+\\tfrac{1}{2}g_4.\n$$\n\nMultiplying by $4$ gives $3g_3=6+2g_4$. From $g_4=1+g_3$, substitute to obtain\n\n$$\n3g_3=6+2(1+g_3)=8+2g_3 \\quad \\Rightarrow\\quad g_3=8,\n$$\n\nwhence $g_4=1+g_3=9$ and $g_2=1+\\tfrac{1}{2}g_3=1+4=5$. Therefore,\n\n$$\nH(P_4\\to P_1)=g_4=9.\n$$\n\n\nThe commute time is defined by $C(P_1,P_4)=H(P_1\\to P_4)+H(P_4\\to P_1)$. Hence,\n\n$$\nC(P_1,P_4)=9+9=18.\n$$\n\n\nFinally, we justify why hitting times and commute times serve as distance measures for phenotype similarity in integrated networks. A random walk on the fused graph encodes movement through paths weighted by multi-modal similarities; the fused adjacency $A$ aggregates genomic and clinical evidence linearly, so transition probabilities $P=D^{-1}A$ reflect the relative support from both modalities. The hitting time $H(P_i\\to P_j)$ is the expected number of steps to encounter $P_j$ starting from $P_i$, integrating over all possible paths with probabilities determined by $P$. This quantity is small when there exist multiple high-probability, short routes between $P_i$ and $P_j$, indicating strong integrated similarity mediated by shared modules, and it is large when paths are scarce or weak, indicating dissimilarity. The commute time symmetrizes this notion by summing both directional hitting times, making it a proper distance-like measure on nodes that is sensitive to global network structure rather than solely local edge weights. In network fusion for systems biomedicine, such walk-based distances are robust to noise in any single modality because they average over many paths whose probabilities arise from the fused multi-modal topology. Thus, the computed commute time quantifies an integrated, path-ensemble-based dissimilarity between phenotypes $P_1$ and $P_4$ in the fused network.", "answer": "$$\\boxed{18}$$", "id": "4350096"}, {"introduction": "Network fusion is often employed to build models that predict novel biological interactions, a task known as link prediction. Rigorously evaluating these predictions is essential for their validation and interpretation. This practice focuses on the critical evaluation step, using a small, concrete example of link prediction scores to construct and compare Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves [@problem_id:4350086]. You will discover why PR curves are often the more insightful tool for sparse biomedical networks, where true interactions are vastly outnumbered by non-interactions.", "problem": "You are integrating two omics-derived patient similarity graphs using a standard network fusion procedure (for example, Similarity Network Fusion (SNF)) to perform link prediction on the fused graph. A link prediction model produces a calibrated score $s_{ij} \\in [0,1]$ for each candidate edge $(i,j)$, with higher $s_{ij}$ indicating greater confidence that an edge truly exists in the underlying biological network. Suppose you evaluate the model on a held-out set of $10$ candidate edges with calibrated scores and ground-truth labels $y_{ij} \\in \\{0,1\\}$ (where $1$ denotes a true edge and $0$ a non-edge). The ranked list (from highest score to lowest) is:\n- $e_1: s=0.95, y=1$\n- $e_2: s=0.90, y=0$\n- $e_3: s=0.85, y=0$\n- $e_4: s=0.80, y=1$\n- $e_5: s=0.60, y=0$\n- $e_6: s=0.55, y=0$\n- $e_7: s=0.50, y=1$\n- $e_8: s=0.40, y=0$\n- $e_9: s=0.30, y=0$\n- $e_{10}: s=0.10, y=0$\n\nLet $P$ be the number of positives, $N$ the number of negatives, and consider thresholding by $t$ so that edges with $s \\ge t$ are predicted as positives. Use the core definitions of Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves: True Positive Rate ($TPR$), False Positive Rate ($FPR$), Precision, and Recall, defined respectively by $TPR = TP/P$, $FPR = FP/N$, $Precision = TP/(TP+FP)$, and $Recall = TP/P$, where $TP$, $FP$, $TN$, and $FN$ are the confusion matrix counts. Construct the ROC and PR points obtained by sweeping the threshold from $t=1$ down to $t=0$ over the distinct score values. Then reason from these definitions to explain, without appealing to domain-specific heuristics, why PR is generally preferred over ROC in sparse settings typical of biomedical networks (for example, protein-protein interaction graphs or patient similarity graphs), where $N \\gg P$.\n\nWhich of the following statements are correct?\n\nA. Using the threshold $t \\ge 0.70$, the point on the ROC curve is $(FPR,TPR) = \\left(\\frac{2}{7}, \\frac{2}{3}\\right)$ and the corresponding point on the PR curve is $(Recall,Precision) = \\left(\\frac{2}{3}, \\frac{1}{2}\\right)$.\n\nB. The area under the ROC curve computed from the above ranking equals $\\frac{15}{21} \\approx 0.714$, while the Average Precision (AP) equals $\\left(1 + \\frac{1}{2} + \\frac{3}{7}\\right)/3 \\approx 0.643$. In sparse biomedical graphs with $N \\gg P$, PR is preferred because $FPR = FP/N$ can remain numerically small even when $FP$ is large, potentially making ROC curves appear optimistic.\n\nC. In settings with $N \\gg P$, ROC is preferable because Precision does not change with $FP$, while $FPR$ increases markedly with each additional $FP$, making ROC more conservative than PR.\n\nD. At the threshold $t \\ge 0.70$, the confusion matrix counts are $(TP,FP,FN,TN) = (2,2,1,5)$.\n\nE. Precision-Recall curves are invariant to class prevalence $P/(P+N)$ and therefore have the same baseline regardless of sparsity; this invariance is the primary reason PR is preferred for biomedical networks.\n\nSelect all that apply. Justify your choices by deriving the relevant quantities from the definitions and reasoning about the effect of class imbalance on $FPR$ versus Precision.", "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- A link prediction model is evaluated on $10$ candidate edges.\n- Each edge has a calibrated score $s_{ij} \\in [0,1]$ and a ground-truth label $y_{ij} \\in \\{0,1\\}$.\n- The ranked list of edges, scores, and labels is:\n    - $e_1: s=0.95, y=1$\n    - $e_2: s=0.90, y=0$\n    - $e_3: s=0.85, y=0$\n    - $e_4: s=0.80, y=1$\n    - $e_5: s=0.60, y=0$\n    - $e_6: s=0.55, y=0$\n    - $e_7: s=0.50, y=1$\n    - $e_8: s=0.40, y=0$\n    - $e_9: s=0.30, y=0$\n    - $e_{10}: s=0.10, y=0$\n- Definitions:\n    - $P$: number of positives.\n    - $N$: number of negatives.\n    - An edge is predicted as positive if its score $s \\ge t$ for a given threshold $t$.\n    - True Positive Rate ($TPR$) = $TP/P$. Also known as Recall.\n    - False Positive Rate ($FPR$) = $FP/N$.\n    - Precision = $TP/(TP+FP)$.\n    - Recall = $TP/P$. Also known as $TPR$.\n- The question asks to evaluate several statements based on these data and definitions, and to explain the preference for Precision-Recall (PR) curves over Receiver Operating Characteristic (ROC) curves in sparse settings ($N \\gg P$).\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Groundedness:** The problem is well-grounded in the standard practices of machine learning model evaluation. The definitions for $TPR$, $FPR$, Precision, Recall, confusion matrices, ROC curves, and PR curves are standard and correctly stated. The context of link prediction in sparse biomedical networks is a common and valid application area.\n- **Well-Posedness:** The problem provides all necessary data (scores and labels) and definitions to perform the required calculations and reasoning. The question is unambiguous.\n- **Objectivity:** The problem statement is factual and objective, with no subjective or biased language.\n\n**Step 3: Verdict and Action**\n- **Verdict:** The problem is valid.\n- **Action:** Proceed with the solution.\n\n**Derivation and Analysis**\n\nFirst, we determine the total number of positive ($P$) and negative ($N$) instances in the ground truth.\n- The positive instances ($y=1$) are $e_1, e_4, e_7$. Thus, $P = 3$.\n- The negative instances ($y=0$) are $e_2, e_3, e_5, e_6, e_8, e_9, e_{10}$. Thus, $N = 7$.\n- The total number of instances is $P+N = 3+7=10$.\n\nThe problem asks to evaluate several statements. This requires calculating confusion matrix values, ROC and PR points for specific thresholds, and overall metrics like Area Under the ROC Curve (AUC-ROC) and Average Precision (AP).\n\n**Option-by-Option Analysis**\n\n**Analysis of Option D: At the threshold $t \\ge 0.70$, the confusion matrix counts are $(TP,FP,FN,TN) = (2,2,1,5)$.**\n- A threshold of $t \\ge 0.70$ means we classify all edges with a score $s \\ge 0.70$ as positive.\n- The edges satisfying this condition are $e_1 (s=0.95)$, $e_2 (s=0.90)$, $e_3 (s=0.85)$, and $e_4 (s=0.80)$. There are $4$ predicted positives.\n- We check the true labels for these predictions:\n    - $e_1 (y=1)$ is a True Positive ($TP$).\n    - $e_2 (y=0)$ is a False Positive ($FP$).\n    - $e_3 (y=0)$ is a False Positive ($FP$).\n    - $e_4 (y=1)$ is a True Positive ($TP$).\n- This gives $TP = 2$ and $FP = 2$.\n- The number of False Negatives ($FN$) is the number of actual positives that were not predicted as positive. The actual positives are $e_1, e_4, e_7$. Since $e_1$ and $e_4$ were correctly predicted, the only one missed is $e_7 (s=0.50  0.70)$. Thus, $FN = 1$.\n- The number of True Negatives ($TN$) is the number of actual negatives that were not predicted as positive. The actual negatives are $e_2, e_3, e_5, e_6, e_8, e_9, e_{10}$. Of these $7$ negatives, $e_2$ and $e_3$ were incorrectly predicted as positive. The remaining $5$ instances ($e_5, e_6, e_8, e_9, e_{10}$) are correctly identified as negative. Thus, $TN = 5$.\n- The confusion matrix counts are $(TP, FP, FN, TN) = (2, 2, 1, 5)$.\n- The statement in Option D is **Correct**.\n\n**Analysis of Option A: Using the threshold $t \\ge 0.70$, the point on the ROC curve is $(FPR,TPR) = \\left(\\frac{2}{7}, \\frac{2}{3}\\right)$ and the corresponding point on the PR curve is $(Recall,Precision) = \\left(\\frac{2}{3}, \\frac{1}{2}\\right)$.**\n- We use the confusion matrix counts derived for Option D: $TP=2, FP=2, FN=1, TN=5$, with $P=3$ and $N=7$.\n- Calculate the ROC point coordinates:\n    - $TPR = TP/P = 2/3$.\n    - $FPR = FP/N = 2/7$.\n    - The ROC point is $(FPR, TPR) = (2/7, 2/3)$. This part of the statement is correct.\n- Calculate the PR point coordinates:\n    - $Recall = TP/P = 2/3$.\n    - $Precision = TP/(TP+FP) = 2/(2+2) = 2/4 = 1/2$.\n    - The PR point is $(Recall, Precision) = (2/3, 1/2)$. This part of the statement is also correct.\n- Since both parts of the statement are verified by calculation, Option A is **Correct**.\n\n**Analysis of Option B: The area under the ROC curve computed from the above ranking equals $\\frac{15}{21} \\approx 0.714$, while the Average Precision (AP) equals $\\left(1 + \\frac{1}{2} + \\frac{3}{7}\\right)/3 \\approx 0.643$. In sparse biomedical graphs with $N \\gg P$, PR is preferred because $FPR = FP/N$ can remain numerically small even when $FP$ is large, potentially making ROC curves appear optimistic.**\n- **AUC-ROC Calculation:** The AUC-ROC can be calculated using the Wilcoxon-Mann-Whitney U statistic, which is the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example. We have $P=3$ positives and $N=7$ negatives, for a total of $P \\times N = 21$ pairs of (positive, negative) instances.\n    - The positive at rank $1$ is ranked above all $7$ negatives.\n    - The positive at rank $4$ is ranked above the $5$ negatives at ranks $5, 6, 8, 9, 10$.\n    - The positive at rank $7$ is ranked above the $3$ negatives at ranks $8, 9, 10$.\n    - The number of correctly ordered pairs is $7 + 5 + 3 = 15$.\n    - $AUC = 15 / (P \\times N) = 15/21$. The value is correct.\n- **Average Precision (AP) Calculation:** AP is the average of the precision values obtained each time a positive instance is retrieved. We sum the precision at the rank of each positive instance and divide by the total number of positives, $P$.\n    - The positive instances are at ranks $1, 4, 7$.\n    - At rank $1$ (instance $e_1$): $TP=1, FP=0$. Precision $= 1/1 = 1$.\n    - At rank $4$ (instance $e_4$): $TP=2, FP=2$. Precision $= 2/4 = 1/2$.\n    - At rank $7$ (instance $e_7$): $TP=3, FP=4$. Precision $= 3/7$.\n    - $AP = \\frac{1}{P} \\sum_{k \\in \\text{positive ranks}} \\text{Precision at rank } k = \\frac{1}{3} \\left(1 + \\frac{1}{2} + \\frac{3}{7}\\right)$.\n    - $AP = \\frac{1}{3} \\left(\\frac{14+7+6}{14}\\right) = \\frac{1}{3} \\left(\\frac{27}{14}\\right) = \\frac{9}{14} \\approx 0.642857$. The calculation provided, $\\left(1 + \\frac{1}{2} + \\frac{3}{7}\\right)/3 \\approx 0.643$, is correct.\n- **Reasoning:** The statement claims PR is preferred in sparse settings ($N \\gg P$) because $FPR = FP/N$ changes little even for large changes in $FP$. This is correct. If $N$ is very large (e.g., $10^6$), an increase of $FP$ from $10$ to $1000$ (a $100$-fold increase in false discoveries) would only change $FPR$ from $10^{-5}$ to $10^{-3}$. This small change in the x-axis of the ROC curve might not significantly lower the AUC-ROC, giving an optimistic evaluation. However, Precision ($TP/(TP+FP)$) is highly sensitive to $FP$ and would drop drastically, accurately reflecting the poor performance for a practitioner. The reasoning is sound.\n- Therefore, Option B is **Correct**.\n\n**Analysis of Option C: In settings with $N \\gg P$, ROC is preferable because Precision does not change with $FP$, while $FPR$ increases markedly with each additional $FP$, making ROC more conservative than PR.**\n- This statement contains multiple falsehoods.\n- \"Precision does not change with $FP$\": False. The definition is $Precision = TP/(TP+FP)$. Precision is inversely dependent on $FP$.\n- \"$FPR$ increases markedly with each additional $FP$\": False. In the specified setting $N \\gg P$, $FPR = FP/N$ increases very *slowly* as $FP$ increases, because the denominator $N$ is very large. This is the exact reason ROC can be misleadingly optimistic.\n- \"ROC is preferable... making ROC more conservative than PR\": False. The opposite is true. PR is more sensitive to the number of false positives and is therefore considered a more informative and conservative metric in class-imbalanced scenarios typical of biomedical network analysis.\n- Therefore, Option C is **Incorrect**.\n\n**Analysis of Option E: Precision-Recall curves are invariant to class prevalence $P/(P+N)$ and therefore have the same baseline regardless of sparsity; this invariance is the primary reason PR is preferred for biomedical networks.**\n- This statement is fundamentally incorrect. The claim that PR curves are invariant to class prevalence is false. Let's analyze the components of a PR curve. Recall = $TP/P$. Precision = $TP/(TP+FP)$. While Recall does not depend on the number of negatives $N$, Precision does, as $FP$ is drawn from the negative class. If one increases the number of negatives $N$ in a dataset (thus changing prevalence), a classifier with a fixed score distribution for each class will produce more false positives $FP$ at any given score threshold, leading to a lower Precision value. This means the PR curve's shape and position are highly dependent on class prevalence.\n- Furthermore, the baseline for a random classifier in PR space is a horizontal line at $Precision = P/(P+N)$, which is the class prevalence. This baseline is, by definition, dependent on sparsity.\n- It is the ROC curve that is considered invariant to class prevalence, because both its axes ($TPR=TP/P$ and $FPR=FP/N$) are normalized by the size of the respective classes. This invariance is precisely why ROC can be misleading in sparse settings. The statement incorrectly attributes this property to PR curves and misidentifies the reason for PR's preference.\n- Therefore, Option E is **Incorrect**.\n\n**Summary of Correct Options**\n- Option A: Correct.\n- Option B: Correct.\n- Option D: Correct.\n\nThe correct statements are A, B, and D.", "answer": "$$\\boxed{ABD}$$", "id": "4350086"}]}