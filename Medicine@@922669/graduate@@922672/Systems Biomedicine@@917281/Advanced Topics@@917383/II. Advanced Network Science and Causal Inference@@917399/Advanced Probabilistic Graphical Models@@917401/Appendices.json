{"hands_on_practices": [{"introduction": "A primary strength of probabilistic graphical models is their ability to represent complex dependency structures. The concept of $d$-separation provides the formal rules for reading conditional independencies directly from a Directed Acyclic Graph (DAG). This practice [@problem_id:4313534] focuses on a critical and often counter-intuitive case: the collider, where two arrows meet at a single node. You will explore how conditioning on a collider or its descendants can induce a statistical dependence between previously independent causes, a phenomenon known as collider bias or Berkson's paradox, which is a common pitfall in the design and interpretation of biomedical studies.", "problem": "Consider a Directed Acyclic Graph (DAG) for a case-control study in systems biomedicine with binary variables: two upstream risk factors $X_1 \\in \\{0,1\\}$ and $X_2 \\in \\{0,1\\}$, a disease indicator $D \\in \\{0,1\\}$, and a patient selection indicator $S \\in \\{0,1\\}$. The graph has edges $X_1 \\rightarrow D \\leftarrow X_2$ and $D \\rightarrow S$. Unconditionally, $X_1$ and $X_2$ are independent: $X_1 \\sim \\operatorname{Bern}(p_1)$, $X_2 \\sim \\operatorname{Bern}(p_2)$, and $P(X_1, X_2) = P(X_1) P(X_2)$.\n\nThe disease follows a noisy-or mechanism: \n$$\nP(D = 1 \\mid X_1 = x_1, X_2 = x_2) = 1 - (1 - q_0) (1 - q_1)^{x_1} (1 - q_2)^{x_2},\n$$\nwhere $q_0$ is the baseline activation probability and $q_1, q_2$ quantify the causal strengths of $X_1$ and $X_2$ on $D$. Patient selection $S$ depends on disease $D$ only:\n$$\nP(S = 1 \\mid D = 1) = s_1, \\quad P(S = 1 \\mid D = 0) = s_0,\n$$\nwith $0 < s_0 < s_1 < 1$. Use the definition of $d$-separation and the probabilistic semantics of Bayesian networks to reason about the (in)dependence structure among $X_1$ and $X_2$ both unconditionally and conditional on $S = 1$.\n\nFor a concrete evaluation, take parameters $p_1 = 0.2$, $p_2 = 0.2$, $q_0 = 0.05$, $q_1 = 0.4$, $q_2 = 0.4$, $s_1 = 0.9$, and $s_0 = 0.1$. Define the conditional odds ratio in the selected cohort $S = 1$ as\n$$\n\\operatorname{OR}(X_1, X_2 \\mid S = 1) = \\frac{P(X_1 = 1, X_2 = 1 \\mid S = 1) \\cdot P(X_1 = 0, X_2 = 0 \\mid S = 1)}{P(X_1 = 1, X_2 = 0 \\mid S = 1) \\cdot P(X_1 = 0, X_2 = 1 \\mid S = 1)}.\n$$\n\nWhich of the following statements are correct? Select all that apply.\n\nA. By $d$-separation, $X_1$ and $X_2$ are independent unconditionally and remain independent under conditioning on $S = 1$, because conditioning on a descendant of a collider does not affect the collider path.\n\nB. Conditioning on $S = 1$ induces statistical dependence between $X_1$ and $X_2$ by opening the path $X_1 \\rightarrow D \\leftarrow X_2$ through conditioning on a descendant of the collider $D$.\n\nC. With the specified parameters, $\\operatorname{OR}(X_1, X_2 \\mid S = 1) < 1$.\n\nD. With the specified parameters, $\\operatorname{Cov}(X_1, X_2 \\mid S = 1) = 0$.\n\nE. Conditioning on $D = 1$ would preserve the independence between $X_1$ and $X_2$ because $D$ is a collider.", "solution": "The user wants me to validate the problem statement and then provide a detailed solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Graph Structure:** A Directed Acyclic Graph (DAG) with binary variables and edges $X_1 \\rightarrow D \\leftarrow X_2$ and $D \\rightarrow S$.\n- **Variables:**\n    - $X_1 \\in \\{0,1\\}$, $X_2 \\in \\{0,1\\}$: Upstream risk factors.\n    - $D \\in \\{0,1\\}$: Disease indicator.\n    - $S \\in \\{0,1\\}$: Patient selection indicator.\n- **Probabilistic Model:**\n    - $X_1 \\sim \\operatorname{Bern}(p_1)$, $X_2 \\sim \\operatorname{Bern}(p_2)$.\n    - Unconditional independence: $P(X_1, X_2) = P(X_1)P(X_2)$.\n    - Conditional probability of $D$ (noisy-or): $P(D = 1 \\mid X_1 = x_1, X_2 = x_2) = 1 - (1 - q_0) (1 - q_1)^{x_1} (1 - q_2)^{x_2}$.\n    - Conditional probability of $S$: $P(S = 1 \\mid D = 1) = s_1$ and $P(S = 1 \\mid D = 0) = s_0$, with the constraint $0 < s_0 < s_1 < 1$.\n- **Parameters for Evaluation:**\n    - $p_1 = 0.2$\n    - $p_2 = 0.2$\n    - $q_0 = 0.05$\n    - $q_1 = 0.4$\n    - $q_2 = 0.4$\n    - $s_1 = 0.9$\n    - $s_0 = 0.1$\n- **Definition:**\n    - Conditional odds ratio in the selected cohort:\n      $$\n      \\operatorname{OR}(X_1, X_2 \\mid S = 1) = \\frac{P(X_1 = 1, X_2 = 1 \\mid S = 1) \\cdot P(X_1 = 0, X_2 = 0 \\mid S = 1)}{P(X_1 = 1, X_2 = 0 \\mid S = 1) \\cdot P(X_1 = 0, X_2 = 1 \\mid S = 1)}\n      $$\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded:** The problem is firmly grounded in the theory of probabilistic graphical models and their application to epidemiology and systems biomedicine. The structure $X_1 \\rightarrow D \\leftarrow X_2$ with $D \\rightarrow S$ is a classic example of collider bias (also known as Berkson's paradox or selection bias), a critical concept in causal inference and the study of confounding. The noisy-or model is a standard parameterization for the interaction of multiple causes.\n- **Well-Posed:** The problem provides a complete specification of a Bayesian network: a DAG and the conditional probability distributions for each node. All parameters are given, and the question asks for the evaluation of specific statements based on this model, which is a well-defined task. The constraints on the parameters are consistent.\n- **Objective:** The problem statement is expressed in precise, mathematical language, free from any subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is scientifically sound, well-posed, and objective, representing a canonical problem in its domain. I will proceed with the solution.\n\n### Solution Derivation\n\nThe problem concerns the (in)dependence relationship between two risk factors, $X_1$ and $X_2$, both unconditionally and conditionally on a selection variable $S$.\n\n**Analysis of the DAG using d-separation:**\nThe graph is $X_1 \\rightarrow D \\leftarrow X_2$ and $D \\rightarrow S$. The only path between $X_1$ and $X_2$ is $X_1 \\rightarrow D \\leftarrow X_2$.\n1.  **Unconditional Independence:** On the path $X_1 \\rightarrow D \\leftarrow X_2$, the node $D$ is a \"collider\" because two arrows point into it. According to the rules of d-separation, a path containing an unconditioned collider is blocked. Since this is the only path between $X_1$ and $X_2$, $X_1$ and $X_2$ are d-separated. This implies they are unconditionally independent, written as $X_1 \\perp \\! \\! \\! \\perp X_2$. This is consistent with the problem statement $P(X_1, X_2) = P(X_1)P(X_2)$.\n\n2.  **Conditional Independence:**\n    *   Conditioning on the collider $D$: The d-separation rules state that conditioning on a collider *opens* the path. Therefore, $X_1$ and $X_2$ are not d-separated given $D$, which implies they are conditionally dependent given $D$. We write this as $X_1 \\not\\perp \\! \\! \\! \\perp X_2 \\mid D$.\n    *   Conditioning on a descendant of the collider, $S$: The node $S$ is a descendant of the collider $D$. Conditioning on a descendant of a collider also opens the path. Thus, $X_1$ and $X_2$ are not d-separated given $S$, implying they become conditionally dependent. We write this as $X_1 \\not\\perp \\! \\! \\! \\perp X_2 \\mid S$. This phenomenon is a form of selection bias. Intuitively, if we select subjects based on a variable ($S$) that is caused by a disease ($D$), and that disease is caused by two independent factors ($X_1, X_2$), then within the selected group, the two factors will appear to be statistically dependent.\n\nWith these principles established, we can evaluate each option.\n\n**Option-by-Option Analysis**\n\n**A. By $d$-separation, $X_1$ and $X_2$ are independent unconditionally and remain independent under conditioning on $S = 1$, because conditioning on a descendant of a collider does not affect the collider path.**\n\nThis statement makes two claims. The first claim, that $X_1$ and $X_2$ are independent unconditionally, is correct as per our d-separation analysis. However, the second claim, that they remain independent under conditioning on $S=1$, is incorrect. The reasoning provided, \"conditioning on a descendant of a collider does not affect the collider path,\" is a false statement of the d-separation rule. Conditioning on a descendant of a collider *opens* the path, inducing dependence.\n**Verdict: Incorrect.**\n\n**B. Conditioning on $S = 1$ induces statistical dependence between $X_1$ and $X_2$ by opening the path $X_1 \\rightarrow D \\leftarrow X_2$ through conditioning on a descendant of the collider $D$.**\n\nThis statement is a precise and correct application of the rules of d-separation. The path $X_1 \\rightarrow D \\leftarrow X_2$ is blocked unconditionally by the collider $D$. Conditioning on $S$, a descendant of $D$, unblocks this path and induces a statistical association between $X_1$ and $X_2$.\n**Verdict: Correct.**\n\n**C. With the specified parameters, $\\operatorname{OR}(X_1, X_2 \\mid S = 1) < 1$.**\n\nTo verify this, we must compute the odds ratio. An odds ratio of $1$ corresponds to independence. Since we have established from d-separation that $X_1$ and $X_2$ become dependent conditional on $S=1$, we expect $\\operatorname{OR} \\neq 1$. The \"explaining away\" effect typical of collider bias with two synergistic causes suggests a negative association, i.e., $\\operatorname{OR} < 1$. Let's perform the calculation.\n\nThe odds ratio is $\\operatorname{OR}(X_1, X_2 \\mid S = 1) = \\frac{P(X_1=1,X_2=1 \\mid S=1) \\cdot P(X_1=0,X_2=0 \\mid S=1)}{P(X_1=1,X_2=0 \\mid S=1) \\cdot P(X_1=0,X_2=1 \\mid S=1)}$.\nBy Bayes' rule, $P(x_1, x_2 \\mid S=1) = \\frac{P(S=1 \\mid x_1, x_2) P(x_1, x_2)}{P(S=1)}$. The denominator $P(S=1)$ cancels in the OR calculation. So,\n$$ \\operatorname{OR}(X_1, X_2 \\mid S = 1) = \\frac{\\left[P(S=1 \\mid X_1=1,X_2=1) P(X_1=1,X_2=1)\\right] \\cdot \\left[P(S=1 \\mid X_1=0,X_2=0) P(X_1=0,X_2=0)\\right]}{\\left[P(S=1 \\mid X_1=1,X_2=0) P(X_1=1,X_2=0)\\right] \\cdot \\left[P(S=1 \\mid X_1=0,X_2=1) P(X_1=0,X_2=1)\\right]} $$\nSince $X_1$ and $X_2$ are unconditionally independent, $P(x_1, x_2) = P(x_1)P(x_2)$, this simplifies to:\n$$ \\operatorname{OR}(X_1, X_2 \\mid S = 1) = \\frac{P(S=1 \\mid X_1=1,X_2=1) \\cdot P(S=1 \\mid X_1=0,X_2=0)}{P(S=1 \\mid X_1=1,X_2=0) \\cdot P(S=1 \\mid X_1=0,X_2=1)} $$\nFirst, find $P(S=1 \\mid x_1, x_2) = \\sum_d P(S=1 \\mid D=d) P(D=d \\mid x_1, x_2) = s_1 P(D=1 \\mid x_1, x_2) + s_0 (1 - P(D=1 \\mid x_1, x_2)) = s_0 + (s_1 - s_0) P(D=1 \\mid x_1, x_2)$.\nWith $s_0 = 0.1$ and $s_1 = 0.9$, this is $P(S=1 \\mid x_1, x_2) = 0.1 + 0.8 \\cdot P(D=1 \\mid x_1, x_2)$.\nNext, compute $P(D=1 \\mid x_1, x_2)$ for each combination, using $q_0=0.05, q_1=0.4, q_2=0.4$:\n$P(D=1|X_1=0, X_2=0) = 1_D(0,0) = 1 - (1-0.05) = 0.05$.\n$P(D=1|X_1=1, X_2=0) = 1_D(1,0) = 1 - (1-0.05)(1-0.4) = 1 - (0.95)(0.6) = 1 - 0.57 = 0.43$.\n$P(D=1|X_1=0, X_2=1) = 1_D(0,1) = 1 - (1-0.05)(1-0.4) = 1 - (0.95)(0.6) = 1 - 0.57 = 0.43$.\n$P(D=1|X_1=1, X_2=1) = 1_D(1,1) = 1 - (1-0.05)(1-0.4)(1-0.4) = 1 - (0.95)(0.36) = 1 - 0.342 = 0.658$.\n\nNow compute $P(S=1 \\mid x_1, x_2)$:\n$P(S=1|X_1=0, X_2=0) = 0.1 + 0.8 \\cdot (0.05) = 0.1 + 0.04 = 0.14$.\n$P(S=1|X_1=1, X_2=0) = 0.1 + 0.8 \\cdot (0.43) = 0.1 + 0.344 = 0.444$.\n$P(S=1|X_1=0, X_2=1) = 0.1 + 0.8 \\cdot (0.43) = 0.1 + 0.344 = 0.444$.\n$P(S=1|X_1=1, X_2=1) = 0.1 + 0.8 \\cdot (0.658) = 0.1 + 0.5264 = 0.6264$.\n\nFinally, compute the odds ratio:\n$$ \\operatorname{OR}(X_1, X_2 \\mid S = 1) = \\frac{(0.6264) \\cdot (0.14)}{(0.444) \\cdot (0.444)} = \\frac{0.087696}{0.197136} \\approx 0.44485 $$\nSince $0.44485 < 1$, the statement is correct.\n**Verdict: Correct.**\n\n**D. With the specified parameters, $\\operatorname{Cov}(X_1, X_2 \\mid S = 1) = 0$.**\n\nThe covariance is defined as $\\operatorname{Cov}(X_1, X_2 \\mid S = 1) = E[X_1 X_2 \\mid S=1] - E[X_1 \\mid S=1]E[X_2 \\mid S=1]$.\nFor binary variables, $X_1$ and $X_2$ are conditionally independent given $S=1$ if and only if their conditional covariance is zero. That is, $\\operatorname{Cov}(X_1, X_2 \\mid S = 1) = 0 \\iff X_1 \\perp \\! \\! \\! \\perp X_2 \\mid S=1$.\nFrom the analysis in B, we know that conditioning on $S=1$ induces dependence, so $X_1 \\not\\perp \\! \\! \\! \\perp X_2 \\mid S=1$. This is also confirmed by the calculation in C, which shows the odds ratio is not $1$. Therefore, the covariance cannot be zero.\n**Verdict: Incorrect.**\n\n**E. Conditioning on $D = 1$ would preserve the independence between $X_1$ and $X_2$ because $D$ is a collider.**\n\nThis statement contains flawed reasoning. As established in our initial d-separation analysis, conditioning on a collider like $D$ *opens* the path $X_1 \\rightarrow D \\leftarrow X_2$, thus *inducing* a dependence between $X_1$ and $X_2$. The statement claims the opposite, that conditioning on a collider preserves independence.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{BC}$$", "id": "4313534"}, {"introduction": "Moving from the qualitative structure of a graph to quantitative predictions requires mastering the mechanics of probabilistic conditioning. For the important class of Gaussian graphical models, this operation can be performed analytically. This exercise [@problem_id:4313546] challenges you to derive the formula for the conditional distribution of a multivariate normal from first principles. By completing the square in the quadratic form of the joint density, you will gain a deep appreciation for the algebraic underpinnings of Gaussian inference and see how the Schur complement naturally arises as the precision of the conditional distribution.", "problem": "In a systems biomedicine study, the joint log-expression of two gene modules, denoted by the random vectors $X_A \\in \\mathbb{R}^2$ and $X_B \\in \\mathbb{R}^2$, is modeled by a multivariate normal distribution. The joint distribution is $X = \\begin{pmatrix} X_A \\\\ X_B \\end{pmatrix} \\sim \\mathcal{N}(\\mu, \\Sigma)$ with the block-partitioned mean and covariance\n$$\n\\mu \\;=\\; \\begin{pmatrix} \\mu_A \\\\ \\mu_B \\end{pmatrix}, \n\\qquad\n\\Sigma \\;=\\; \\begin{pmatrix} \\Sigma_{AA} & \\Sigma_{AB} \\\\ \\Sigma_{BA} & \\Sigma_{BB} \\end{pmatrix},\n$$\nwhere\n$$\n\\mu_A \\;=\\; \\begin{pmatrix} 0.5 \\\\ -0.2 \\end{pmatrix}, \n\\quad\n\\mu_B \\;=\\; \\begin{pmatrix} 0.1 \\\\ 0.3 \\end{pmatrix}, \n$$\n$$\n\\Sigma_{AA} \\;=\\; \\begin{pmatrix} 1.8 & 0.3 \\\\ 0.3 & 1.2 \\end{pmatrix}, \n\\quad\n\\Sigma_{BB} \\;=\\; \\begin{pmatrix} 2.0 & 0.5 \\\\ 0.5 & 1.5 \\end{pmatrix}, \n\\quad\n\\Sigma_{AB} \\;=\\; \\begin{pmatrix} 0.6 & 0.2 \\\\ 0.1 & 0.5 \\end{pmatrix},\n$$\nand $\\Sigma_{BA} = \\Sigma_{AB}^{\\top}$.\n\nTask:\n1) Starting from the joint multivariate normal density and using only basic linear algebra facts such as completion of the square and standard block-matrix identities, derive the conditional distribution $p(X_A \\mid X_B = x_B)$ in closed form. Your derivation must make explicit which Schur complement appears in the conditional covariance, and it must not assume any specialized conditional-Gaussian formula as a starting point.\n\n2) With the observation $X_B = b$ where $b = \\begin{pmatrix} 0.2 \\\\ 0.0 \\end{pmatrix}$ and the evaluation point $x_A = \\begin{pmatrix} 0.4 \\\\ -0.1 \\end{pmatrix}$, compute the conditional density value $p(X_A = x_A \\mid X_B = b)$. Express your final answer as a single real number rounded to four significant figures.\n\nAll matrices and vectors are dimensionally consistent with the biological context and assumed to define a valid positive definite covariance. Use the natural units of log-expression; no unit conversion is required. The answer must be given as a real number, rounded to four significant figures.", "solution": "The problem is valid. It is a well-posed mathematical problem grounded in the theory of multivariate statistics, with all necessary data provided and no internal contradictions.\n\nThe solution is presented in two parts as requested: first the derivation of the conditional distribution, and second the numerical evaluation.\n\n**Part 1: Derivation of the conditional distribution $p(X_A \\mid X_B = x_B)$**\n\nThe joint probability density function of a multivariate normal random vector $X = \\begin{pmatrix} X_A \\\\ X_B \\end{pmatrix}$ with mean $\\mu = \\begin{pmatrix} \\mu_A \\\\ \\mu_B \\end{pmatrix}$ and covariance $\\Sigma = \\begin{pmatrix} \\Sigma_{AA} & \\Sigma_{AB} \\\\ \\Sigma_{BA} & \\Sigma_{BB} \\end{pmatrix}$ is given by\n$$\np(x_A, x_B) = \\frac{1}{\\sqrt{(2\\pi)^d |\\Sigma|}} \\exp \\left( -\\frac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu) \\right)\n$$\nwhere $x = \\begin{pmatrix} x_A \\\\ x_B \\end{pmatrix}$ and $d$ is the total dimension of $X$. The conditional density $p(x_A \\mid x_B)$ is proportional to the joint density, i.e., $p(x_A \\mid x_B) \\propto p(x_A, x_B)$, where we treat $x_B$ as a fixed vector.\n\nThe core of the derivation is to analyze the quadratic form in the exponent of the joint density. Let $\\Lambda = \\Sigma^{-1}$ be the precision matrix, partitioned conformally with $\\Sigma$:\n$$\n\\Lambda = \\begin{pmatrix} \\Lambda_{AA} & \\Lambda_{AB} \\\\ \\Lambda_{BA} & \\Lambda_{BB} \\end{pmatrix}\n$$\nThe quadratic form is\n$$\nQ(x_A, x_B) = \\begin{pmatrix} x_A - \\mu_A \\\\ x_B - \\mu_B \\end{pmatrix}^\\top \\begin{pmatrix} \\Lambda_{AA} & \\Lambda_{AB} \\\\ \\Lambda_{BA} & \\Lambda_{BB} \\end{pmatrix} \\begin{pmatrix} x_A - \\mu_A \\\\ x_B - \\mu_B \\end{pmatrix}\n$$\nExpanding this gives\n$$\nQ(x_A, x_B) = (x_A-\\mu_A)^\\top \\Lambda_{AA} (x_A-\\mu_A) + (x_A-\\mu_A)^\\top \\Lambda_{AB} (x_B-\\mu_B) + (x_B-\\mu_B)^\\top \\Lambda_{BA} (x_A-\\mu_A) + (x_B-\\mu_B)^\\top \\Lambda_{BB} (x_B-\\mu_B)\n$$\nSince $\\Lambda$ is symmetric, $\\Lambda_{BA} = \\Lambda_{AB}^\\top$, and the two cross-terms are equal scalars.\n$$\nQ(x_A, x_B) = (x_A-\\mu_A)^\\top \\Lambda_{AA} (x_A-\\mu_A) + 2(x_A-\\mu_A)^\\top \\Lambda_{AB} (x_B-\\mu_B) + (\\text{terms not involving } x_A)\n$$\nTo find the form of $p(x_A \\mid x_B)$, we complete the square with respect to $x_A$. The terms involving $x_A$ are:\n$$\nx_A^\\top \\Lambda_{AA} x_A - 2\\mu_A^\\top \\Lambda_{AA} x_A + 2(x_B-\\mu_B)^\\top \\Lambda_{BA} x_A = x_A^\\top \\Lambda_{AA} x_A - 2(\\mu_A^\\top \\Lambda_{AA} - (x_B-\\mu_B)^\\top \\Lambda_{BA}) x_A\n$$\n$$\n= x_A^\\top \\Lambda_{AA} x_A - 2(\\Lambda_{AA}\\mu_A - \\Lambda_{AB}(x_B-\\mu_B))^\\top x_A\n$$\nThis is a quadratic form in $x_A$. Completing the square, we can identify the mean and covariance of the conditional Gaussian. A quadratic form $(x-m)^\\top C^{-1} (x-m)$ expands to $x^\\top C^{-1} x - 2m^\\top C^{-1} x + m^\\top C^{-1} m$.\nComparing the terms, we identify the conditional precision matrix as $\\Lambda_{AA}$ and the conditional mean $\\mu_{A|B}$ satisfying $\\Lambda_{AA}\\mu_{A|B} = \\Lambda_{AA}\\mu_A - \\Lambda_{AB}(x_B-\\mu_B)$.\n\nThe covariance of the conditional distribution $p(X_A \\mid X_B=x_B)$ is therefore:\n$$\n\\Sigma_{A|B} = \\Lambda_{AA}^{-1}\n$$\nThe mean of the conditional distribution is:\n$$\n\\mu_{A|B} = \\Lambda_{AA}^{-1}(\\Lambda_{AA}\\mu_A - \\Lambda_{AB}(x_B-\\mu_B)) = \\mu_A - \\Lambda_{AA}^{-1}\\Lambda_{AB}(x_B-\\mu_B)\n$$\nTo express these in terms of the blocks of $\\Sigma$, we use the block matrix inversion formulas:\n$$\n\\Lambda_{AA} = (\\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA})^{-1}\n\\qquad\n\\Lambda_{AB} = -\\Lambda_{AA}\\Sigma_{AB}\\Sigma_{BB}^{-1}\n$$\nFrom the first identity, the conditional covariance is:\n$$\n\\Sigma_{A|B} = \\Lambda_{AA}^{-1} = \\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA}\n$$\nThis expression, $\\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA}$, is the **Schur complement** of the block $\\Sigma_{BB}$ in the matrix $\\Sigma$.\n\nSubstituting the expression for $\\Lambda_{AB}$ into the equation for the mean:\n$$\n\\mu_{A|B} = \\mu_A - (\\Lambda_{AA}^{-1})(-\\Lambda_{AA}\\Sigma_{AB}\\Sigma_{BB}^{-1})(x_B-\\mu_B) = \\mu_A + \\Sigma_{AB}\\Sigma_{BB}^{-1}(x_B-\\mu_B)\n$$\nThus, the conditional distribution is a multivariate normal distribution $p(X_A \\mid X_B=x_B) = \\mathcal{N}(\\mu_{A|B}, \\Sigma_{A|B})$ with the derived mean and covariance.\n\n**Part 2: Numerical Evaluation**\n\nWe need to compute $p(X_A = x_A \\mid X_B = b)$ for $x_A = \\begin{pmatrix} 0.4 \\\\ -0.1 \\end{pmatrix}$ and $b = \\begin{pmatrix} 0.2 \\\\ 0.0 \\end{pmatrix}$.\n\n1.  **Calculate $\\Sigma_{BB}^{-1}$**:\n    $\\det(\\Sigma_{BB}) = (2.0)(1.5) - (0.5)(0.5) = 2.75$.\n    $\\Sigma_{BB}^{-1} = \\frac{1}{2.75} \\begin{pmatrix} 1.5 & -0.5 \\\\ -0.5 & 2.0 \\end{pmatrix}$.\n\n2.  **Calculate conditional mean $\\mu_{A|B}$**:\n    $\\mu_{A|B} = \\mu_A + \\Sigma_{AB}\\Sigma_{BB}^{-1}(b-\\mu_B)$.\n    $b - \\mu_B = \\begin{pmatrix} 0.2 \\\\ 0.0 \\end{pmatrix} - \\begin{pmatrix} 0.1 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 0.1 \\\\ -0.3 \\end{pmatrix}$.\n    $\\Sigma_{AB}\\Sigma_{BB}^{-1} = \\begin{pmatrix} 0.6 & 0.2 \\\\ 0.1 & 0.5 \\end{pmatrix} \\frac{1}{2.75} \\begin{pmatrix} 1.5 & -0.5 \\\\ -0.5 & 2.0 \\end{pmatrix} = \\frac{1}{2.75} \\begin{pmatrix} 0.8 & 0.1 \\\\ -0.1 & 0.95 \\end{pmatrix}$.\n    $\\mu_{A|B} = \\begin{pmatrix} 0.5 \\\\ -0.2 \\end{pmatrix} + \\frac{1}{2.75} \\begin{pmatrix} 0.8 & 0.1 \\\\ -0.1 & 0.95 \\end{pmatrix} \\begin{pmatrix} 0.1 \\\\ -0.3 \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ -0.2 \\end{pmatrix} + \\frac{1}{2.75} \\begin{pmatrix} 0.05 \\\\ -0.295 \\end{pmatrix} = \\begin{pmatrix} 0.5 + 1/55 \\\\ -0.2 - 59/550 \\end{pmatrix} = \\begin{pmatrix} 57/110 \\\\ -169/550 \\end{pmatrix}$.\n    $\\mu_{A|B} \\approx \\begin{pmatrix} 0.51818 \\\\ -0.30727 \\end{pmatrix}$.\n\n3.  **Calculate conditional covariance $\\Sigma_{A|B}$**:\n    $\\Sigma_{A|B} = \\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA}$.\n    $\\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA} = \\frac{1}{2.75} \\begin{pmatrix} 0.8 & 0.1 \\\\ -0.1 & 0.95 \\end{pmatrix} \\begin{pmatrix} 0.6 & 0.1 \\\\ 0.2 & 0.5 \\end{pmatrix} = \\frac{1}{2.75} \\begin{pmatrix} 0.5 & 0.13 \\\\ 0.13 & 0.465 \\end{pmatrix} = \\begin{pmatrix} 2/11 & 13/275 \\\\ 13/275 & 93/550 \\end{pmatrix}$.\n    $\\Sigma_{A|B} = \\begin{pmatrix} 1.8 & 0.3 \\\\ 0.3 & 1.2 \\end{pmatrix} - \\begin{pmatrix} 2/11 & 13/275 \\\\ 13/275 & 93/550 \\end{pmatrix} = \\begin{pmatrix} 89/55 & 139/550 \\\\ 139/550 & 567/550 \\end{pmatrix}$.\n\n4.  **Evaluate the PDF**:\n    The conditional PDF is $p(x_A \\mid b) = \\frac{1}{\\sqrt{(2\\pi)^2 |\\Sigma_{A|B}|}} \\exp\\left(-\\frac{1}{2} (x_A - \\mu_{A|B})^\\top \\Sigma_{A|B}^{-1} (x_A - \\mu_{A|B})\\right)$.\n    \n    Determinant: $|\\Sigma_{A|B}| = (\\frac{89}{55})(\\frac{567}{550}) - (\\frac{139}{550})^2 = \\frac{485309}{302500}$.\n    \n    Vector difference: $\\Delta x = x_A - \\mu_{A|B} = \\begin{pmatrix} 0.4 \\\\ -0.1 \\end{pmatrix} - \\begin{pmatrix} 57/110 \\\\ -169/550 \\end{pmatrix} = \\begin{pmatrix} -13/110 \\\\ 114/550 \\end{pmatrix} = \\frac{1}{550}\\begin{pmatrix} -65 \\\\ 114 \\end{pmatrix}$.\n    \n    Quadratic form: $Q_A = (\\Delta x)^\\top \\Sigma_{A|B}^{-1} (\\Delta x)$.\n    $\\Sigma_{A|B}^{-1} = \\frac{1}{|\\Sigma_{A|B}|} \\begin{pmatrix} 567/550 & -139/550 \\\\ -139/550 & 89/55 \\end{pmatrix} = \\frac{302500}{485309} \\frac{1}{550} \\begin{pmatrix} 567 & -139 \\\\ -139 & 890 \\end{pmatrix} = \\frac{550}{485309} \\begin{pmatrix} 567 & -139 \\\\ -139 & 890 \\end{pmatrix}$.\n    $Q_A = \\left(\\frac{1}{550}\\begin{pmatrix} -65 & 114 \\end{pmatrix}\\right) \\left(\\frac{550}{485309} \\begin{pmatrix} 567 & -139 \\\\ -139 & 890 \\end{pmatrix}\\right) \\left(\\frac{1}{550}\\begin{pmatrix} -65 \\\\ 114 \\end{pmatrix}\\right)$.\n    $Q_A = \\frac{1}{550 \\cdot 485309} \\left[ (-65)(567(-65) - 139(114)) + 114(-139(-65) + 890(114)) \\right]$.\n    $Q_A = \\frac{1}{266919450} \\left[ 16021995 \\right] \\approx 0.06002626$.\n    \n    The density value is:\n    $p(x_A \\mid b) = \\frac{1}{2\\pi \\sqrt{\\frac{485309}{302500}}} \\exp\\left(-\\frac{1}{2} \\cdot \\frac{16021995}{266919450}\\right)$.\n    $p(x_A \\mid b) = \\frac{550}{2\\pi \\sqrt{485309}} \\exp(-0.03001313) \\approx \\frac{275}{\\pi (696.641)} \\times 0.970431$.\n    $p(x_A \\mid b) \\approx 0.125655 \\times 0.970431 \\approx 0.121939$.\n\nRounding to four significant figures gives $0.1219$.", "answer": "$$\\boxed{0.1219}$$", "id": "4313546"}, {"introduction": "The ultimate goal of many graphical models is to perform inferenceâ€”that is, to compute the posterior probability distribution of a set of variables given evidence about others. The belief propagation algorithm, applied to a junction tree, is a general and powerful method for performing exact inference. In this practice [@problem_id:4313547], you will execute the core steps of this algorithm on a model of a biological pathway. By initializing potentials, calculating, and passing messages between cliques, you will see firsthand how local computations are orchestrated to arrive at a globally consistent marginal posterior distribution.", "problem": "Consider a curated Systems Biomedicine pathway fragment represented as a Bayesian network with binary variables capturing the activation and measurement states of a transcription factor, a downstream gene, a phosphorylation event, and an assay readout. Let $X_A \\in \\{0,1\\}$ denote the activation state of a transcription factor, $X_B \\in \\{0,1\\}$ denote the expression state of a target gene, $X_C \\in \\{0,1\\}$ denote the phosphorylation state of the encoded protein, and $X_E \\in \\{0,1\\}$ denote a binary immunoassay readout. The directed edges are $X_A \\rightarrow X_B \\rightarrow X_C \\rightarrow X_E$, encoding the following factorization of the joint distribution: \n$$p(x_A,x_B,x_C,x_E) = p(x_A)\\,p(x_B \\mid x_A)\\,p(x_C \\mid x_B)\\,p(x_E \\mid x_C).$$\nAssume the conditional probability tables (CPTs) below are scientifically validated from prior experiments:\n- Prior on the transcription factor: $p(X_A=1) = \\frac{2}{5}$, $p(X_A=0) = \\frac{3}{5}$.\n- Gene expression given transcription factor: $p(X_B=1 \\mid X_A=1) = \\frac{3}{5}$, $p(X_B=0 \\mid X_A=1) = \\frac{2}{5}$, $p(X_B=1 \\mid X_A=0) = \\frac{1}{5}$, $p(X_B=0 \\mid X_A=0) = \\frac{4}{5}$.\n- Phosphorylation given gene expression: $p(X_C=1 \\mid X_B=1) = \\frac{4}{5}$, $p(X_C=0 \\mid X_B=1) = \\frac{1}{5}$, $p(X_C=1 \\mid X_B=0) = \\frac{1}{10}$, $p(X_C=0 \\mid X_B=0) = \\frac{9}{10}$.\n- Assay readout given phosphorylation: $p(X_E=1 \\mid X_C=1) = \\frac{9}{10}$, $p(X_E=1 \\mid X_C=0) = \\frac{1}{5}$, $p(X_E=0 \\mid X_C=1) = \\frac{1}{10}$, $p(X_E=0 \\mid X_C=0) = \\frac{4}{5}$.\nSuppose the assay readout is observed as positive, that is, $X_E = 1$ is evidence.\n\nConstruct a junction tree with cliques $\\{X_A,X_B\\}$, $\\{X_B,X_C\\}$, and $\\{X_C,X_E\\}$. Identify the separator sets (sepsets) between adjacent cliques and initialize clique potentials using the given CPTs and prior. Using Belief Propagation (BP), also known as the sum-product algorithm, on this tree:\n1. Compute the message from the clique $\\{X_A,X_B\\}$ to the clique $\\{X_B,X_C\\}$ across the sepset $\\{X_B\\}$, and explicitly normalize the message so that the entries sum to $1$, reporting the normalization constant.\n2. Compute the message from the clique $\\{X_C,X_E\\}$ to the clique $\\{X_B,X_C\\}$ across the sepset $\\{X_C\\}$, incorporating the evidence $X_E=1$, and explicitly normalize the message so that the entries sum to $1$, reporting the normalization constant.\n3. Form the updated potential at the root clique $\\{X_B,X_C\\}$ by multiplying its initial potential with the incoming normalized messages, and explicitly normalize this updated potential so that its entries sum to $1$, reporting the normalization constant.\n4. From the normalized root clique potential, marginalize to obtain the posterior $p(X_B=1 \\mid X_E=1)$.\n\nYou must verify the normalization (i.e., that the sums of message entries and the sum of the updated clique potential entries equal $1$) at each step. Express your final answer as an irreducible fraction. No rounding is required, and no units are applicable.", "solution": "The problem statement provides a valid, well-posed probabilistic inference task on a Bayesian network. All necessary conditional probability tables (CPTs), the network structure, the evidence, and the target query are specified. The problem is scientifically grounded in the context of systems biomedicine modeling and is free of contradictions or ambiguities. I will proceed with the solution.\n\nThe problem asks to perform belief propagation on a junction tree derived from the Bayesian network $X_A \\rightarrow X_B \\rightarrow X_C \\rightarrow X_E$. The junction tree is given as a chain of cliques $C_1 - C_2 - C_3$, where $C_1 = \\{X_A, X_B\\}$, $C_2 = \\{X_B, X_C\\}$, and $C_3 = \\{X_C, X_E\\}$. The separator sets are $S_{12} = C_1 \\cap C_2 = \\{X_B\\}$ and $S_{23} = C_2 \\cap C_3 = \\{X_C\\}$. We designate $C_2$ as the root clique. The belief propagation algorithm will proceed with a \"collect-to-root\" phase, where messages are passed from the leaf cliques ($C_1$ and $C_3$) to the root clique ($C_2$).\n\nFirst, we assign the probability factors from the joint distribution factorization $p(x_A,x_B,x_C,x_E) = p(x_A)\\,p(x_B \\mid x_A)\\,p(x_C \\mid x_B)\\,p(x_E \\mid x_C)$ to the cliques to initialize their potentials.\n- The factor $p(x_A)p(x_B|x_A)$ is assigned to $C_1$, so the initial potential is $\\psi_1(x_A, x_B) = p(x_A)p(x_B|x_A)$.\n- The factor $p(x_C|x_B)$ is assigned to $C_2$, so the initial potential is $\\psi_2(x_B, x_C) = p(x_C|x_B)$.\n- The factor $p(x_E|x_C)$ is assigned to $C_3$, so the initial potential is $\\psi_3(x_C, x_E) = p(x_E|x_C)$.\n\nThe evidence is given as $X_E=1$. This evidence must be incorporated into the potential of the clique containing $X_E$, which is $C_3$. We define a new potential for $C_3$ by setting $X_E=1$: $\\psi'_3(x_C) = \\psi_3(x_C, X_E=1) = p(X_E=1|x_C)$.\n\nWe now proceed with the four steps outlined in the problem.\n\n**1. Compute the message from $\\{X_A,X_B\\}$ to $\\{X_B,X_C\\}$**\n\nThe message $m_{1 \\to 2}$ is sent from clique $C_1$ to $C_2$ across the separator set $S_{12}=\\{X_B\\}$. The unnormalized message $\\tilde{m}_{1 \\to 2}(x_B)$ is computed by marginalizing out the variables in $C_1$ that are not in $S_{12}$.\n$$ \\tilde{m}_{1 \\to 2}(x_B) = \\sum_{x_A \\in \\{0,1\\}} \\psi_1(x_A, x_B) = \\sum_{x_A} p(x_A) p(x_B | x_A) $$\nThis sum corresponds to the marginal probability $p(x_B)$. Let's compute its values:\n$$ \\tilde{m}_{1 \\to 2}(X_B=1) = p(X_A=1)p(X_B=1|X_A=1) + p(X_A=0)p(X_B=1|X_A=0) = \\left(\\frac{2}{5}\\right)\\left(\\frac{3}{5}\\right) + \\left(\\frac{3}{5}\\right)\\left(\\frac{1}{5}\\right) = \\frac{6}{25} + \\frac{3}{25} = \\frac{9}{25} $$\n$$ \\tilde{m}_{1 \\to 2}(X_B=0) = p(X_A=1)p(X_B=0|X_A=1) + p(X_A=0)p(X_B=0|X_A=0) = \\left(\\frac{2}{5}\\right)\\left(\\frac{2}{5}\\right) + \\left(\\frac{3}{5}\\right)\\left(\\frac{4}{5}\\right) = \\frac{4}{25} + \\frac{12}{25} = \\frac{16}{25} $$\nThe problem requires normalizing this message. The normalization constant $Z_1$ is the sum of the message entries:\n$$ Z_1 = \\tilde{m}_{1 \\to 2}(X_B=1) + \\tilde{m}_{1 \\to 2}(X_B=0) = \\frac{9}{25} + \\frac{16}{25} = \\frac{25}{25} = 1 $$\nThe normalization constant is $1$. The normalized message $m_{1 \\to 2}(x_B) = \\tilde{m}_{1 \\to 2}(x_B) / Z_1$ is therefore identical to the unnormalized one: $m_{1 \\to 2}(X_B=1) = \\frac{9}{25}$ and $m_{1 \\to 2}(X_B=0) = \\frac{16}{25}$. As a verification, the sum of a normalized message's entries must be $1$: $\\frac{9}{25} + \\frac{16}{25} = 1$.\n\n**2. Compute the message from $\\{X_C,X_E\\}$ to $\\{X_B,X_C\\}$**\n\nThe message $m_{3 \\to 2}$ is sent from clique $C_3$ to $C_2$ across the separator set $S_{23}=\\{X_C\\}$. We first incorporate the evidence $X_E=1$ into the potential for $C_3$, resulting in $\\psi'_3(x_C) = p(X_E=1|x_C)$. The unnormalized message $\\tilde{m}_{3 \\to 2}(x_C)$ is obtained by summing over the variables in $C_3$ not in $S_{23}$, after incorporating evidence. Here, the only variable remaining in $C_3$ is $X_C$, which is in the sepset. Thus, the message is simply the evidence-updated potential:\n$$ \\tilde{m}_{3 \\to 2}(x_C) = \\psi'_3(x_C) = p(X_E=1|x_C) $$\nLet's compute its values:\n$$ \\tilde{m}_{3 \\to 2}(X_C=1) = p(X_E=1|X_C=1) = \\frac{9}{10} $$\n$$ \\tilde{m}_{3 \\to 2}(X_C=0) = p(X_E=1|X_C=0) = \\frac{1}{5} $$\nThe normalization constant $Z_3$ is the sum of the message entries:\n$$ Z_3 = \\tilde{m}_{3 \\to 2}(X_C=1) + \\tilde{m}_{3 \\to 2}(X_C=0) = \\frac{9}{10} + \\frac{1}{5} = \\frac{9}{10} + \\frac{2}{10} = \\frac{11}{10} $$\nThe normalization constant is $\\frac{11}{10}$. The normalized message $m_{3 \\to 2}(x_C) = \\tilde{m}_{3 \\to 2}(x_C) / Z_3$ is:\n$$ m_{3 \\to 2}(X_C=1) = \\frac{9/10}{11/10} = \\frac{9}{11} $$\n$$ m_{3 \\to 2}(X_C=0) = \\frac{1/5}{11/10} = \\frac{2/10}{11/10} = \\frac{2}{11} $$\nAs a verification, the sum is $\\frac{9}{11} + \\frac{2}{11} = 1$.\n\n**3. Form and normalize the updated potential at the root clique $\\{X_B,X_C\\}$**\n\nThe unnormalized updated potential (or belief) at the root clique $C_2 = \\{X_B, X_C\\}$ is the product of its initial potential $\\psi_2(x_B, x_C)$ and all incoming normalized messages:\n$$ \\tilde{\\beta}_2(x_B, x_C) = \\psi_2(x_B, x_C) \\times m_{1 \\to 2}(x_B) \\times m_{3 \\to 2}(x_C) $$\nWe compute the four entries for this potential:\n$$ \\tilde{\\beta}_2(1,1) = p(X_C=1|X_B=1) \\times m_{1 \\to 2}(1) \\times m_{3 \\to 2}(1) = \\left(\\frac{4}{5}\\right) \\left(\\frac{9}{25}\\right) \\left(\\frac{9}{11}\\right) = \\frac{324}{1375} $$\n$$ \\tilde{\\beta}_2(1,0) = p(X_C=0|X_B=1) \\times m_{1 \\to 2}(1) \\times m_{3 \\to 2}(0) = \\left(\\frac{1}{5}\\right) \\left(\\frac{9}{25}\\right) \\left(\\frac{2}{11}\\right) = \\frac{18}{1375} $$\n$$ \\tilde{\\beta}_2(0,1) = p(X_C=1|X_B=0) \\times m_{1 \\to 2}(0) \\times m_{3 \\to 2}(1) = \\left(\\frac{1}{10}\\right) \\left(\\frac{16}{25}\\right) \\left(\\frac{9}{11}\\right) = \\frac{144}{2750} = \\frac{72}{1375} $$\n$$ \\tilde{\\beta}_2(0,0) = p(X_C=0|X_B=0) \\times m_{1 \\to 2}(0) \\times m_{3 \\to 2}(0) = \\left(\\frac{9}{10}\\right) \\left(\\frac{16}{25}\\right) \\left(\\frac{2}{11}\\right) = \\frac{288}{2750} = \\frac{144}{1375} $$\nThe normalization constant $Z_2$ is the sum of these entries:\n$$ Z_2 = \\frac{324}{1375} + \\frac{18}{1375} + \\frac{72}{1375} + \\frac{144}{1375} = \\frac{324+18+72+144}{1375} = \\frac{558}{1375} $$\nThe normalization constant is $\\frac{558}{1375}$. The normalized updated potential is $\\beta_2(x_B, x_C) = \\tilde{\\beta}_2(x_B, x_C) / Z_2$. This normalized potential represents the posterior probability $p(x_B, x_C|X_E=1)$. Summing its entries must yield $1$, which is verified since we are dividing by the sum.\n\n**4. Marginalize to obtain the posterior $p(X_B=1 \\mid X_E=1)$**\n\nThe posterior probability of $X_B$ given the evidence is found by marginalizing the normalized root clique potential $\\beta_2(x_B, x_C)$ over $X_C$.\n$$ p(X_B=1|X_E=1) = \\sum_{x_C \\in \\{0,1\\}} \\beta_2(X_B=1, x_C) = \\beta_2(1,1) + \\beta_2(1,0) $$\nUsing the unnormalized values and the final normalization constant:\n$$ p(X_B=1|X_E=1) = \\frac{\\tilde{\\beta}_2(1,1) + \\tilde{\\beta}_2(1,0)}{Z_2} = \\frac{\\frac{324}{1375} + \\frac{18}{1375}}{\\frac{558}{1375}} = \\frac{\\frac{342}{1375}}{\\frac{558}{1375}} = \\frac{342}{558} $$\nTo find the final answer, we must reduce this fraction to its simplest form. Both numerator and denominator are divisible by $2$:\n$$ \\frac{342}{558} = \\frac{171}{279} $$\nThe sum of the digits of the numerator is $1+7+1=9$, and the sum of the digits of the denominator is $2+7+9=18$. Both are divisible by $9$:\n$$ \\frac{171}{279} = \\frac{171 \\div 9}{279 \\div 9} = \\frac{19}{31} $$\nSince $19$ and $31$ are both prime numbers, this is the irreducible fraction.", "answer": "$$ \\boxed{\\frac{19}{31}} $$", "id": "4313547"}]}