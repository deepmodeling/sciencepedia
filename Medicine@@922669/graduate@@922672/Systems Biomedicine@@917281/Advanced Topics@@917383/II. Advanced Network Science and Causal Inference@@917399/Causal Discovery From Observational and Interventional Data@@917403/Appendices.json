{"hands_on_practices": [{"introduction": "A central goal of causal inference is to predict the effects of interventions from observational data. This exercise provides practice with do-calculus, a formal framework for manipulating distributions under interventions. By applying its rules, you will see how to identify a causal effect even in the presence of unmeasured confounding, a common challenge in systems biology [@problem_id:4322794]. This problem specifically illustrates the powerful \"front-door\" criterion, building your skills in translating graphical conditions into algebraic manipulations.", "problem": "Consider a toy biochemical signaling network in a single cell, represented as a Directed Acyclic Graph (DAG). A latent cell state $S$ modulates ligand secretion $L$ and downstream phenotype $Y$, a membrane receptor phosphorylation state $R$ is activated by $L$, and $Y$ is driven by $R$. The structural assumptions are:\n- The DAG has directed edges $S \\to L$, $S \\to Y$, $L \\to R$, and $R \\to Y$.\n- There is no direct causal edge $L \\to Y$.\n- There is no edge $S \\to R$.\nThus all directed causal paths from $L$ to $Y$ pass through $R$, there is no unmeasured confounding for the causal effect $L \\to R$, and the only unmeasured confounding is between $L$ and $Y$ via $S$. The variables are binary: $L \\in \\{0,1\\}$, $R \\in \\{0,1\\}$, $Y \\in \\{0,1\\}$.\n\nUse the semantics of the do-operator $\\mathrm{do}(\\cdot)$, d-separation in modified graphs created by interventions (incoming arrows into intervened nodes removed), and do-calculus to derive the causal effect $P(Y=1 \\mid \\mathrm{do}(L=1))$. You must justify each transformation by an appropriate d-separation statement in the corresponding modified graph.\n\nYou are given observational distributions:\n$$\nP(L=1) = \\frac{1}{2}, \\quad P(L=0) = \\frac{1}{2},\n$$\n$$\nP(R=1 \\mid L=0) = \\frac{1}{5}, \\quad P(R=1 \\mid L=1) = \\frac{4}{5},\n$$\n$$\nP(Y=1 \\mid R=0, L=0) = \\frac{1}{10}, \\quad P(Y=1 \\mid R=0, L=1) = \\frac{3}{10},\n$$\n$$\nP(Y=1 \\mid R=1, L=0) = \\frac{3}{5}, \\quad P(Y=1 \\mid R=1, L=1) = \\frac{7}{10}.\n$$\nFor each conditional probability above, the complement for $Y=0$ is $1$ minus the listed value.\n\nStarting only from core definitions—Markov factorization for a causal DAG, the do-operator semantics (incoming arrows into intervened variables are removed), d-separation, and do-calculus rules—derive an exact closed-form expression for $P(Y=1 \\mid \\mathrm{do}(L=1))$ in terms of the given observational quantities, and evaluate it numerically. Express the final probability as an exact rational number. No rounding is required.", "solution": "The problem asks for the causal effect of ligand secretion $L$ on the downstream phenotype $Y$, denoted as $P(Y=1 \\mid \\mathrm{do}(L=1))$, within a specified biochemical signaling network. The network's causal structure is given by a Directed Acyclic Graph (DAG), which we shall call $G$. The nodes of the graph are $S$ (latent cell state), $L$ (ligand), $R$ (receptor), and $Y$ (phenotype). The directed edges are $S \\to L$, $S \\to Y$, $L \\to R$, and $R \\to Y$. The variable $S$ is unobserved, acting as a confounder for the relationship between $L$ and $Y$ through the back-door path $L \\leftarrow S \\to Y$. Due to this confounding, the causal effect cannot be identified by simple conditioning, i.e., $P(Y=1 \\mid \\mathrm{do}(L=1)) \\neq P(Y=1 \\mid L=1)$.\n\nThe problem can be solved by applying the principles of do-calculus to derive an expression for the causal effect in terms of the given observational probabilities. The specified graph structure is a classic example of a scenario where the \"front-door\" criterion is met, allowing for the identification of the causal effect via the mediating variable $R$. We will derive the front-door adjustment formula from first principles.\n\nOur goal is to compute $P(Y=y \\mid \\mathrm{do}(L=l))$, where we will later set $y=1$ and $l=1$. The derivation proceeds as follows:\n\nFirst, we introduce the mediating variable $R$ into the expression by applying the law of total probability to the post-intervention distribution:\n$$ P(Y=y \\mid \\mathrm{do}(L=l)) = \\sum_{r} P(Y=y \\mid R=r, \\mathrm{do}(L=l)) P(R=r \\mid \\mathrm{do}(L=l)) $$\nHere, the sum is over all possible values of $R$, which are $r \\in \\{0, 1\\}$. We must now convert the two terms in the summation, which involve the $\\mathrm{do}$-operator, into expressions that can be estimated from the observational data.\n\n1.  **Analysis of $P(R=r \\mid \\mathrm{do}(L=l))$**\n    This term represents the causal effect of $L$ on $R$. We can exchange the intervention $\\mathrm{do}(L=l)$ with the observation $L=l$ using Rule 2 of do-calculus if there are no open back-door paths from $L$ to $R$. Formally, $P(R=r \\mid \\mathrm{do}(L=l)) = P(R=r \\mid L=l)$ if $(R \\perp\\!\\!\\perp L)_{G_{\\underline{L}}}$, where $G_{\\underline{L}}$ is the graph $G$ with the edge $L \\to R$ removed.\n    In our graph $G$, the only back-door path between $L$ and $R$ is $L \\leftarrow S \\to Y \\leftarrow R$. This path is blocked because it contains a collider, $Y$. Thus, the d-separation condition $(R \\perp\\!\\!\\perp L)_{G_{\\underline{L}}}$ holds.\n    Therefore, we can write:\n    $$ P(R=r \\mid \\mathrm{do}(L=l)) = P(R=r \\mid L=l) $$\n    This quantity is available from the provided observational data.\n\n2.  **Analysis of $P(Y=y \\mid R=r, \\mathrm{do}(L=l))$**\n    This term is more complex. Our strategy will be to sequentially apply the rules of do-calculus.\n    a. First, we use Rule 2 to convert the observation $R=r$ into an intervention $\\mathrm{do}(R=r)$. This is permissible if $(Y \\perp\\!\\!\\perp R \\mid L)_{G_{\\bar{L}\\underline{R}}}$. The graph $G_{\\bar{L}\\underline{R}}$ is formed by removing the incoming edge to $L$ (i.e., $S \\to L$) and the outgoing edge from $R$ (i.e., $R \\to Y$). In this modified graph, the edges are $L \\to R$ and $S \\to Y$. There are no paths between $Y$ and $R$, so the d-separation condition holds trivially.\n    Thus, $P(Y=y \\mid R=r, \\mathrm{do}(L=l)) = P(Y=y \\mid \\mathrm{do}(R=r), \\mathrm{do}(L=l))$.\n\n    b. Next, we use Rule 3 to remove the intervention $\\mathrm{do}(L=l)$. This is allowed if $(Y \\perp\\!\\!\\perp L \\mid R)_{G_{\\bar{L}\\bar{R}}}$. The graph $G_{\\bar{L}\\bar{R}}$ is formed by removing all incoming edges to $L$ and $R$ (i.e., $S \\to L$ and $L \\to R$). The remaining edges are $S \\to Y$ and $R \\to Y$. In this graph, $L$ is an isolated node and has no path to $Y$. Thus, the d-separation criterion holds.\n    Therefore, $P(Y=y \\mid \\mathrm{do}(R=r), \\mathrm{do}(L=l)) = P(Y=y \\mid \\mathrm{do}(R=r))$.\n\n    c. Finally, we must identify the expression $P(Y=y \\mid \\mathrm{do}(R=r))$, which is the causal effect of $R$ on $Y$. This effect is confounded by the back-door path $R \\leftarrow L \\leftarrow S \\to Y$. We can block this path by conditioning on $L$, using the back-door adjustment formula. This formula itself is derivable from do-calculus. We need to show that $L$ satisfies the back-door criterion for the pair $(R, Y)$:\n    (i) $L$ is not a descendant of $R$ in $G$. This is true.\n    (ii) $L$ blocks every back-door path between $R$ and $Y$. The only such path is $R \\leftarrow L \\leftarrow S \\to Y$, which is blocked by $L$. This is also true.\n    The back-door adjustment formula is therefore applicable:\n    $$ P(Y=y \\mid \\mathrm{do}(R=r)) = \\sum_{l'} P(Y=y \\mid R=r, L=l') P(L=l') $$\n    where the sum is over all values $l'$ of $L$. All quantities in this expression are observational.\n\nCombining these steps, we have derived that $P(Y=y \\mid R=r, \\mathrm{do}(L=l)) = \\sum_{l'} P(Y=y \\mid R=r, L=l') P(L=l')$.\n\nNow, we substitute these results back into the original equation:\n$$ P(Y=y \\mid \\mathrm{do}(L=l)) = \\sum_{r} P(R=r \\mid L=l) \\left( \\sum_{l'} P(Y=y \\mid R=r, L=l') P(L=l') \\right) $$\nThis is the front-door adjustment formula. We can now substitute the given numerical values to find $P(Y=1 \\mid \\mathrm{do}(L=1))$.\n\nThe parameters are $y=1$ and $l=1$:\n$$ P(Y=1 \\mid \\mathrm{do}(L=1)) = \\sum_{r \\in \\{0, 1\\}} P(R=r \\mid L=1) \\left( \\sum_{l' \\in \\{0, 1\\}} P(Y=1 \\mid R=r, L=l') P(L=l') \\right) $$\nLet's first compute the inner sum for each value of $r$. The problem states $P(L=0) = \\frac{1}{2}$ and $P(L=1) = \\frac{1}{2}$.\n\nFor $r=0$:\n$$ \\sum_{l'} P(Y=1 \\mid R=0, L=l') P(L=l') = P(Y=1 \\mid R=0, L=0)P(L=0) + P(Y=1 \\mid R=0, L=1)P(L=1) $$\n$$ = \\left(\\frac{1}{10}\\right) \\left(\\frac{1}{2}\\right) + \\left(\\frac{3}{10}\\right) \\left(\\frac{1}{2}\\right) = \\frac{1}{20} + \\frac{3}{20} = \\frac{4}{20} = \\frac{1}{5} $$\n\nFor $r=1$:\n$$ \\sum_{l'} P(Y=1 \\mid R=1, L=l') P(L=l') = P(Y=1 \\mid R=1, L=0)P(L=0) + P(Y=1 \\mid R=1, L=1)P(L=1) $$\n$$ = \\left(\\frac{3}{5}\\right) \\left(\\frac{1}{2}\\right) + \\left(\\frac{7}{10}\\right) \\left(\\frac{1}{2}\\right) = \\frac{3}{10} + \\frac{7}{20} = \\frac{6}{20} + \\frac{7}{20} = \\frac{13}{20} $$\n\nNow we can compute the final expression. We need $P(R=r \\mid L=1)$. We are given $P(R=1 \\mid L=1) = \\frac{4}{5}$.\nTherefore, $P(R=0 \\mid L=1) = 1 - P(R=1 \\mid L=1) = 1 - \\frac{4}{5} = \\frac{1}{5}$.\n\nSubstituting these into the main formula:\n$$ P(Y=1 \\mid \\mathrm{do}(L=1)) = P(R=0 \\mid L=1) \\left(\\frac{1}{5}\\right) + P(R=1 \\mid L=1) \\left(\\frac{13}{20}\\right) $$\n$$ = \\left(\\frac{1}{5}\\right) \\left(\\frac{1}{5}\\right) + \\left(\\frac{4}{5}\\right) \\left(\\frac{13}{20}\\right) $$\n$$ = \\frac{1}{25} + \\frac{52}{100} = \\frac{1}{25} + \\frac{13}{25} $$\n$$ = \\frac{14}{25} $$\nThe causal effect of setting $L=1$ on the probability of $Y=1$ is $\\frac{14}{25}$.", "answer": "$$\\boxed{\\frac{14}{25}}$$", "id": "4322794"}, {"introduction": "While do-calculus operates on a known causal graph, a primary task in causal discovery is to infer this structure from data. This problem simulates the core logic of constraint-based algorithms, such as the PC algorithm, which use conditional independence tests to learn a causal skeleton and orient its edges. By working from a given set of observational and interventional constraints, you will construct a Completed Partially Directed Acyclic Graph (CPDAG) that represents all causally equivalent models, sharpening your understanding of d-separation and Markov equivalence classes [@problem_id:4322795].", "problem": "A systems biomedicine team studies a cytokine signaling module involving six molecular variables measured at steady state under both observational and perfectly interventional regimes. Let the variables be $X_J$ (Janus kinase activity, JAK), $X_S$ (Signal Transducer and Activator of Transcription 3 phosphorylation, STAT3), $X_C$ (Suppressor of Cytokine Signaling 3 expression, SOCS3), $X_L$ (interleukin-6 concentration, IL-6), $X_K$ (nuclear factor kappa-light-chain-enhancer of activated B cells activation, NF-$\\kappa$B), and $X_T$ (tumor necrosis factor concentration, TNF). Assume causal sufficiency, acyclicity, and that the observational joint distribution is faithful to the underlying causal Directed Acyclic Graph (DAG). The team estimates the following minimal conditional independence constraints from large-sample observational data (conditioning sets shown are minimal and no other conditional independences hold given any subset of the remaining variables):\n- $X_J \\perp X_L \\mid \\emptyset$\n- $X_J \\perp X_K \\mid \\emptyset$\n- $X_J \\perp X_T \\mid \\emptyset$\n- $X_J \\perp X_C \\mid \\{X_S\\}$\n- $X_L \\perp X_C \\mid \\{X_S\\}$\n- $X_K \\perp X_S \\mid \\{X_L\\}$\n- $X_T \\perp X_S \\mid \\{X_L\\}$\n- $X_K \\perp X_C \\mid \\{X_L, X_S\\}$\n- $X_T \\perp X_C \\mid \\{X_L, X_S\\}$\n\nThe team also performs perfect interventions in the sense of Pearl’s $\\mathrm{do}(\\cdot)$ operator:\n- Under $\\mathrm{do}(X_J=x)$, only the distributions of $X_S$ and $X_C$ change; the distributions of $X_L$, $X_K$, and $X_T$ remain invariant.\n- Under $\\mathrm{do}(X_S=s)$, only the distribution of $X_C$ changes; the distributions of $X_J$, $X_L$, $X_K$, and $X_T$ remain invariant.\n- No interventions are performed on $X_L$, $X_K$, or $X_T$.\n\nTask: Using only fundamental definitions of d-separation, the Causal Markov condition, faithfulness, and the semantics of perfect interventions that sever incoming arrows to the intervened node, first construct the Completed Partially Directed Acyclic Graph (CPDAG) representing the Markov equivalence class consistent with the above constraints. Then, using the definition of a Markov equivalence class and properties of chain components in a CPDAG, determine the number of distinct DAGs in this equivalence class. Provide your final answer as a single exact integer. No rounding is required and no units are needed.", "solution": "The task is two-fold: first, to construct the Completed Partially Directed Acyclic Graph (CPDAG) representing the Markov equivalence class of DAGs consistent with the given data, and second, to determine the number of distinct DAGs in this class.\n\n#### Part 1: Constructing the Causal Graph from Observational Data\n\nWe use a constraint-based approach, akin to the PC algorithm, which consists of three main stages: skeleton discovery, v-structure orientation, and orientation propagation.\n\n**1. Skeleton Discovery:**\nThe skeleton of the causal graph is an undirected graph where an edge exists between two variables if and only if they are not conditionally independent given any subset of the other variables. The problem states that the given $9$ CIs are the only ones that hold. Therefore, an edge exists between any pair of variables not in this list. The total number of pairs of variables is $\\binom{6}{2} = 15$. The $9$ non-adjacent pairs are given. The remaining $15 - 9 = 6$ pairs must be adjacent.\n\nThe non-adjacent pairs (no edge) are:\n$(X_J, X_L), (X_J, X_K), (X_J, X_T), (X_J, X_C), (X_L, X_C), (X_K, X_S), (X_T, X_S), (X_K, X_C), (X_T, X_C)$.\n\nThe adjacent pairs (with an edge in the skeleton) are:\n1.  $X_J - X_S$\n2.  $X_S - X_C$\n3.  $X_S - X_L$\n4.  $X_L - X_K$\n5.  $X_L - X_T$\n6.  $X_K - X_T$\n\nThe skeleton is thus formed by these $6$ edges.\n\n**2. V-Structure Orientation:**\nWe identify unshielded colliders (v-structures). An unshielded triple is a set of three nodes $X-Y-Z$ where $X$ and $Z$ are not adjacent. Such a triple is oriented as a v-structure $X \\to Y \\leftarrow Z$ if the intermediate node $Y$ is NOT in the minimal separating set $S_{XZ}$ that makes $X$ and $Z$ conditionally independent.\n\nLet's examine all unshielded triples in the skeleton:\n-   $X_J-X_S-X_L$: The nodes $X_J$ and $X_L$ are not adjacent. The CI is $X_J \\perp X_L \\mid \\emptyset$. The separating set is $S_{JL} = \\emptyset$. The intermediate node $X_S$ is not in $S_{JL}$. Therefore, this is a v-structure: $X_J \\to X_S \\leftarrow X_L$.\n-   $X_J-X_S-X_C$: The nodes $X_J$ and $X_C$ are not adjacent. The CI is $X_J \\perp X_C \\mid \\{X_S\\}$. The separating set is $S_{JC} = \\{X_S\\}$. The intermediate node $X_S$ is in $S_{JC}$. This is not a v-structure.\n-   $X_C-X_S-X_L$: The nodes $X_C$ and $X_L$ are not adjacent. The CI is $X_L \\perp X_C \\mid \\{X_S\\}$. The separating set is $S_{LC} = \\{X_S\\}$. The intermediate node $X_S$ is in $S_{LC}$. This is not a v-structure.\n-   $X_S-X_L-X_K$: The nodes $X_S$ and $X_K$ are not adjacent. The CI is $X_K \\perp X_S \\mid \\{X_L\\}$. The separating set is $S_{SK} = \\{X_L\\}$. The intermediate node $X_L$ is in $S_{SK}$. This is not a v-structure.\n-   $X_S-X_L-X_T$: The nodes $X_S$ and $X_T$ are not adjacent. The CI is $X_T \\perp X_S \\mid \\{X_L\\}$. The separating set is $S_{ST} = \\{X_L\\}$. The intermediate node $X_L$ is in $S_{ST}$. This is not a v-structure.\n\nThe only v-structure identified is $X_J \\to X_S \\leftarrow X_L$.\n\n**3. Orientation Propagation (Meek's Rules):**\nWe apply orientation rules to the partially directed graph to orient more edges, ensuring no new v-structures or cycles are created.\n-   **Rule 1 (Avoid new v-structures):** If we have $A \\to B - C$ and $A, C$ are not adjacent, we must orient the edge as $B \\to C$.\n    -   We have the structure $X_J \\to X_S - X_C$. The nodes $X_J$ and $X_C$ are not adjacent. Applying this rule, we must orient $X_S-X_C$ as $X_S \\to X_C$. If we were to orient it as $X_S \\leftarrow X_C$, we would create a new v-structure $X_J \\to X_S \\leftarrow X_C$, which would imply $X_J \\perp X_C \\mid \\emptyset$ by faithfulness. This contradicts the given CI $X_J \\perp X_C \\mid \\{X_S\\}$.\n-   Repeated application of this and other Meek's rules finds no further compelled orientations based on the observational data. For example, considering a non-v-structure triple like $X_S-X_L-X_K$, we know $X_S \\leftarrow X_L$ (from the v-structure). The triple is either a chain $X_S \\leftarrow X_L \\leftarrow X_K$ or a fork $X_S \\leftarrow X_L \\to X_K$. Both are consistent with $X_L$ being in the separating set $S_{SK}$, so the edge $X_L-X_K$ cannot be oriented from this information alone.\n\nThe CPDAG derived from observational data is:\n-   Directed edges: $X_J \\to X_S$, $X_L \\to X_S$, $X_S \\to X_C$.\n-   Undirected edges: $X_L - X_K$, $X_L - X_T$, $X_K - X_T$. These form a clique on the vertex set $\\{X_L, X_K, X_T\\}$.\n\n#### Part 2: Incorporating Interventional Data\n\nThe effect of a perfect intervention $\\mathrm{do}(X=x)$ is to remove all incoming edges to $X$ in the true DAG. Consequently, only $X$ and its descendants can change their distribution.\n-   $\\mathrm{do}(X_J=x)$: $X_S$ and $X_C$ change, while $X_L, X_K, X_T$ are invariant. This implies that in the true DAG, $\\{X_S, X_C\\}$ must be descendants of $X_J$, and $\\{X_L, X_K, X_T\\}$ must not be. Our current CPDAG has the directed path $X_J \\to X_S \\to X_C$, which is consistent. It also shows no directed path from $X_J$ to any of $\\{X_L, X_K, X_T\\}$. This intervention confirms the orientations $X_J \\to X_S$ and $X_S \\to X_C$ but adds no new information to orient the remaining edges.\n-   $\\mathrm{do}(X_S=s)$: $X_C$ changes, while $X_J, X_L, X_K, X_T$ are invariant. This implies $X_C$ is a descendant of $X_S$, and $\\{X_J, X_L, X_K, X_T\\}$ are not.\n    -   The orientation $X_S \\to X_C$ is consistent.\n    -   $X_J$ and $X_L$ are parents of $X_S$ in our CPDAG ($X_J \\to X_S, X_L \\to S$), so they are ancestors, not descendants. This is consistent.\n    -   $X_K$ and $X_T$ must not be descendants of $X_S$. For a node in $\\{X_K, X_T\\}$ to be a descendant of $X_S$, there must be a directed path $X_S \\to \\dots \\to X_K$ (or $X_T$). Such a path would have to pass through $X_L$. However, the edge between $X_L$ and $X_S$ is a compelled edge $X_L \\to X_S$ in the CPDAG. Any DAG in the equivalence class must contain this edge. A directed path from $X_S$ to $X_L$ would create a cycle ($X_S \\to \\dots \\to X_L \\to X_S$), which is forbidden. Therefore, no directed path from $X_S$ to any node in $\\{X_L, X_K, X_T\\}$ can exist. This is already an entailment of the observational CPDAG.\n\nThe interventional data are consistent with the CPDAG derived from observational data but do not provide additional constraints to orient the remaining undirected edges. The final CPDAG is unchanged.\n\n#### Part 3: Counting the DAGs in the Equivalence Class\n\nThe final CPDAG has the following structure:\n-   Directed edges: $X_J \\to X_S$, $X_L \\to X_S$, $X_S \\to X_C$.\n-   An undirected clique on the vertices $\\{X_L, X_K, X_T\\}$.\n\nThis CPDAG represents a Markov equivalence class of DAGs. All DAGs in this class must contain the three directed edges. The differences between them lie in the orientation of the undirected edges. The set of nodes $\\{X_L, X_K, X_T\\}$ and the undirected edges between them form a single chain component.\n\nTo find the number of DAGs in the class, we must count the number of ways to orient the edges of this chain component without creating cycles or new v-structures.\n1.  **Cycles:** Orienting the edges within the $\\{X_L, X_K, X_T\\}$ clique can only create cycles within that clique. No cycle can be formed with the rest of the graph, as the only connection is $X_L \\to X_S$, and there is no path from $\\{J, S, C\\}$ back to $\\{L, K, T\\}$. Thus, any acyclic orientation of the clique is globally acyclic.\n2.  **New v-structures:** A new v-structure $A \\to B \\leftarrow C$ requires $A$ and $C$ to be non-adjacent. If we orient edges within the clique $\\{X_L, X_K, X_T\\}$, any potential collider $B \\in \\{X_L, X_K, X_T\\}$ would have parents $A, C$ that are also in the clique. But all nodes in a clique are adjacent to each other. Therefore, any $A \\to B \\leftarrow C$ structure formed would be a shielded collider, not a v-structure.\n\nThus, the number of DAGs in the equivalence class is simply the number of ways to orient the edges of the undirected clique $\\{X_L, X_K, X_T\\}$ to form a DAG. A complete undirected graph on $k$ vertices (a clique) can be oriented into a DAG in $k!$ ways, corresponding to the $k!$ possible topological orderings of the vertices.\n\nFor the clique on $\\{X_L, X_K, X_T\\}$, we have $k=3$ vertices.\nThe number of distinct DAGs is $k! = 3! = 3 \\times 2 \\times 1 = 6$.", "answer": "$$\\boxed{6}$$", "id": "4322795"}, {"introduction": "Many causal discovery algorithms begin with the strong assumption of causal sufficiency, meaning no unobserved common causes. This final exercise challenges that assumption and explores how algorithms can represent latent confounding. You will derive the mathematical discrepancy between observational association and the true causal effect in a simple system with a hidden variable [@problem_id:4322754]. This provides a foundational understanding of how algorithms like FCI detect confounding and represent it using a bidirected edge in a Partial Ancestral Graph (PAG).", "problem": "A systems biomedicine team studies a small gene regulatory module with two observed messenger RNA expression levels, $G_{1}$ and $G_{2}$, and an unobserved transcription factor $L$ that binds promoters of both genes. They collect two datasets: an observational dataset under baseline conditions and an interventional dataset in which $G_{1}$ is forcibly set to a fixed level by a CRISPR interference perturbation (modeled as a perfect intervention). The module is modeled by a linear Gaussian structural causal model with the following structural equations:\n$$\nG_{1} \\;=\\; a_{1} L + \\epsilon_{1}, \\quad G_{2} \\;=\\; a_{2} L + \\epsilon_{2},\n$$\nwhere $L \\sim \\mathcal{N}(0,\\sigma_{L}^{2})$, $\\epsilon_{1} \\sim \\mathcal{N}(0,\\sigma_{1}^{2})$, and $\\epsilon_{2} \\sim \\mathcal{N}(0,\\sigma_{2}^{2})$, with $L$, $\\epsilon_{1}$, and $\\epsilon_{2}$ mutually independent. There is no direct causal effect $G_{1} \\to G_{2}$; all association between $G_{1}$ and $G_{2}$, if present, arises through $L$. The intervention is the do-operator $\\mathrm{do}(G_{1} = g)$, which severs all incoming edges into $G_{1}$ and sets $G_{1}$ to the value $g$.\n\nFrom foundational principles in causal discovery, the Partial Ancestral Graph (PAG) is the appropriate summary graph for the Markov equivalence class when latent confounders may be present, and the Fast Causal Inference (FCI) algorithm outputs a PAG whose bidirected edges indicate potential latent confounding between observed variables.\n\nUsing only these definitions and the model above, carry out the following derivation:\n- Compute the observational least-squares regression coefficient of $G_{2}$ on $G_{1}$, denoted $\\beta_{\\mathrm{obs}}$, based on the covariance structure implied by the structural equations.\n- Compute the interventional causal effect of $G_{1}$ on $G_{2}$, defined as the derivative $\\beta_{\\mathrm{do}} = \\frac{d}{dg}\\,\\mathbb{E}[\\,G_{2} \\mid \\mathrm{do}(G_{1}=g)\\,]$.\n- Let $\\Delta = \\beta_{\\mathrm{obs}} - \\beta_{\\mathrm{do}}$. Provide a closed-form analytic expression for $\\Delta$ in terms of $a_{1}$, $a_{2}$, and $\\sigma_{L}^{2}$, $\\sigma_{1}^{2}$, and explain how the sign and magnitude of $\\Delta$ inform the presence of a bidirected edge in the PAG between $G_{1}$ and $G_{2}$ due to the unobserved transcription factor $L$.\n\nYour final answer must be the single closed-form expression for $\\Delta$. No rounding is required and no units are needed.", "solution": "The problem asks for the derivation of three quantities based on the provided linear Gaussian structural causal model (SCM): the observational regression coefficient $\\beta_{\\mathrm{obs}}$, the interventional causal effect $\\beta_{\\mathrm{do}}$, and their difference $\\Delta = \\beta_{\\mathrm{obs}} - \\beta_{\\mathrm{do}}$.\n\nThe SCM is given by:\n$$\nG_{1} = a_{1} L + \\epsilon_{1}\n$$\n$$\nG_{2} = a_{2} L + \\epsilon_{2}\n$$\nwhere $L \\sim \\mathcal{N}(0,\\sigma_{L}^{2})$, $\\epsilon_{1} \\sim \\mathcal{N}(0,\\sigma_{1}^{2})$, and $\\epsilon_{2} \\sim \\mathcal{N}(0,\\sigma_{2}^{2})$ are mutually independent random variables.\n\n**1. Computation of the observational regression coefficient, $\\beta_{\\mathrm{obs}}$**\n\nThe observational least-squares regression coefficient of $G_{2}$ on $G_{1}$ is defined as $\\beta_{\\mathrm{obs}} = \\frac{\\mathrm{Cov}(G_{1}, G_{2})}{\\mathrm{Var}(G_{1})}$. We first compute the variance of $G_{1}$ and the covariance between $G_{1}$ and $G_{2}$.\n\nThe variance of $G_{1}$ is:\n$$\n\\mathrm{Var}(G_{1}) = \\mathrm{Var}(a_{1} L + \\epsilon_{1})\n$$\nSince $L$ and $\\epsilon_{1}$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\mathrm{Var}(G_{1}) = \\mathrm{Var}(a_{1} L) + \\mathrm{Var}(\\epsilon_{1}) = a_{1}^{2} \\mathrm{Var}(L) + \\mathrm{Var}(\\epsilon_{1})\n$$\nSubstituting the given variances:\n$$\n\\mathrm{Var}(G_{1}) = a_{1}^{2} \\sigma_{L}^{2} + \\sigma_{1}^{2}\n$$\n\nThe covariance between $G_{1}$ and $G_{2}$ is:\n$$\n\\mathrm{Cov}(G_{1}, G_{2}) = \\mathrm{Cov}(a_{1} L + \\epsilon_{1}, a_{2} L + \\epsilon_{2})\n$$\nUsing the bilinearity property of covariance:\n$$\n\\mathrm{Cov}(G_{1}, G_{2}) = \\mathrm{Cov}(a_{1} L, a_{2} L) + \\mathrm{Cov}(a_{1} L, \\epsilon_{2}) + \\mathrm{Cov}(\\epsilon_{1}, a_{2} L) + \\mathrm{Cov}(\\epsilon_{1}, \\epsilon_{2})\n$$\nSince $L$, $\\epsilon_{1}$, and $\\epsilon_{2}$ are mutually independent, the covariance terms involving different variables are zero:\n$$\n\\mathrm{Cov}(G_{1}, G_{2}) = a_{1}a_{2}\\mathrm{Cov}(L, L) + 0 + 0 + 0 = a_{1}a_{2}\\mathrm{Var}(L)\n$$\nSubstituting the given variance of $L$:\n$$\n\\mathrm{Cov}(G_{1}, G_{2}) = a_{1} a_{2} \\sigma_{L}^{2}\n$$\n\nNow, we can compute $\\beta_{\\mathrm{obs}}$:\n$$\n\\beta_{\\mathrm{obs}} = \\frac{\\mathrm{Cov}(G_{1}, G_{2})}{\\mathrm{Var}(G_{1})} = \\frac{a_{1} a_{2} \\sigma_{L}^{2}}{a_{1}^{2} \\sigma_{L}^{2} + \\sigma_{1}^{2}}\n$$\n\n**2. Computation of the interventional causal effect, $\\beta_{\\mathrm{do}}$**\n\nThe interventional causal effect is defined as $\\beta_{\\mathrm{do}} = \\frac{d}{dg}\\,\\mathbb{E}[\\,G_{2} \\mid \\mathrm{do}(G_{1}=g)\\,]$. The intervention $\\mathrm{do}(G_{1}=g)$ modifies the SCM by replacing the structural equation for $G_{1}$ with the assignment $G_{1}=g$, severing all incoming causal arrows to $G_{1}$. The modified SCM is:\n$$\nG_{1} = g\n$$\n$$\nG_{2} = a_{2} L + \\epsilon_{2}\n$$\nThe distributions of the exogenous variables $L$ and $\\epsilon_{2}$ are unaffected by this intervention. We compute the expectation of $G_{2}$ in this post-intervention model:\n$$\n\\mathbb{E}[\\,G_{2} \\mid \\mathrm{do}(G_{1}=g)\\,] = \\mathbb{E}[a_{2} L + \\epsilon_{2}]\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[\\,G_{2} \\mid \\mathrm{do}(G_{1}=g)\\,] = a_{2}\\mathbb{E}[L] + \\mathbb{E}[\\epsilon_{2}]\n$$\nGiven $L \\sim \\mathcal{N}(0,\\sigma_{L}^{2})$ and $\\epsilon_{2} \\sim \\mathcal{N}(0,\\sigma_{2}^{2})$, their expectations are $\\mathbb{E}[L]=0$ and $\\mathbb{E}[\\epsilon_{2}]=0$.\n$$\n\\mathbb{E}[\\,G_{2} \\mid \\mathrm{do}(G_{1}=g)\\,] = a_{2}(0) + 0 = 0\n$$\nThe expected value of $G_{2}$ under the intervention is $0$, which is a constant that does not depend on the intervention value $g$. The causal effect $\\beta_{\\mathrm{do}}$ is the derivative of this expectation with respect to $g$:\n$$\n\\beta_{\\mathrm{do}} = \\frac{d}{dg}(0) = 0\n$$\nThis result is expected, as there is no direct causal path from $G_{1}$ to $G_{2}$ in the model. Any change in $G_1$ via intervention has no effect on $G_2$.\n\n**3. Computation and interpretation of $\\Delta$**\n\nThe quantity $\\Delta$ is the difference between the observational regression coefficient and the true causal effect:\n$$\n\\Delta = \\beta_{\\mathrm{obs}} - \\beta_{\\mathrm{do}}\n$$\nSubstituting the derived expressions for $\\beta_{\\mathrm{obs}}$ and $\\beta_{\\mathrm{do}}$:\n$$\n\\Delta = \\frac{a_{1} a_{2} \\sigma_{L}^{2}}{a_{1}^{2} \\sigma_{L}^{2} + \\sigma_{1}^{2}} - 0 = \\frac{a_{1} a_{2} \\sigma_{L}^{2}}{a_{1}^{2} \\sigma_{L}^{2} + \\sigma_{1}^{2}}\n$$\nThis quantity, $\\Delta$, represents the confounding bias. It is the difference between the association measured from observational data ($\\beta_{\\mathrm{obs}}$) and the true causal effect ($\\beta_{\\mathrm{do}}$). In this model, the true causal effect is zero, so $\\Delta=\\beta_{\\mathrm{obs}}$.\n\nThe presence of a bidirected edge $G_{1} \\leftrightarrow G_{2}$ in a Partial Ancestral Graph (PAG) signifies a latent common cause. An algorithm like FCI infers this structure by detecting statistical dependencies that cannot be explained by direct causation or other observed variables. In this two-variable system, the algorithm would test for the marginal dependence of $G_{1}$ and $G_{2}$. They are dependent if and only if their covariance is non-zero. From our calculation, $\\mathrm{Cov}(G_{1}, G_{2}) = a_{1} a_{2} \\sigma_{L}^{2}$. This covariance is non-zero provided that $a_{1} \\neq 0$, $a_{2} \\neq 0$, and $\\sigma_{L}^{2} > 0$, which are the conditions for $L$ to be an active common cause.\n\nThe quantity $\\Delta$ is a direct measure of this confounding.\n- If $\\Delta = 0$, it implies that either $a_{1}=0$, $a_{2}=0$, or $\\sigma_{L}^{2}=0$. In any of these cases, the confounding path $G_{1} \\leftarrow L \\rightarrow G_{2}$ is broken, there is no association between $G_{1}$ and $G_{2}$ ($\\beta_{\\mathrm{obs}}=0$), and thus no evidence for a bidirected edge.\n- If $\\Delta \\neq 0$, there is a non-zero statistical association ($\\beta_{\\mathrm{obs}} \\neq 0$) that is purely due to the common cause $L$ because the true causal effect $\\beta_{\\mathrm{do}}$ is zero. This discrepancy between association and causation ($\\Delta \\neq 0$) is precisely the signature of confounding. A causal discovery algorithm would detect this association and, in the absence of an explanation via a direct causal path, infer the presence of a latent confounder, which is represented by the bidirected edge $G_{1} \\leftrightarrow G_{2}$ in the PAG.\n- The sign and magnitude of $\\Delta$ quantify the confounding. The sign, determined by $a_{1}a_{2}$, indicates whether the confounder induces a positive or negative correlation. The magnitude reflects the strength of the confounding path relative to the total variance of $G_1$.\n\nIn summary, $\\Delta$ is the statistical evidence for confounding by $L$, which justifies drawing a bidirected edge between $G_{1}$ and $G_{2}$ in the corresponding PAG.", "answer": "$$\\boxed{\\frac{a_{1} a_{2} \\sigma_{L}^{2}}{a_{1}^{2} \\sigma_{L}^{2} + \\sigma_{1}^{2}}}$$", "id": "4322754"}]}