{"hands_on_practices": [{"introduction": "The first step in any single-cell analysis is rigorous quality control (QC) to remove data from dead or damaged cells, which often exhibit high mitochondrial gene expression or low total transcript counts. Rather than relying on arbitrary cutoffs, this practice formalizes the process using a cost-sensitive Bayesian decision rule, treating QC as a classification problem. By deriving optimal thresholds from first principles, you will learn to balance the risk of discarding viable cells against the cost of including technical artifacts in downstream analysis. [@problem_id:4324377]", "problem": "A pipeline for single-cell ribonucleic acid sequencing (scRNA-seq) preprocessing must set two quality-control thresholds used prior to downstream cell type identification and annotation: the mitochondrial read fraction threshold and the Unique Molecular Identifier (UMI) count threshold. The goal is to exclude likely technical failures (e.g., dead or damaged cells) while retaining biologically informative cells. Formalize this trade-off using a cost-sensitive Bayesian decision rule derived from first principles, beginning with Bayes’ theorem and expected loss minimization.\n\nAssume the following generative model for a randomly sampled cell. Let the latent class be $Y \\in \\{F, V\\}$ where $F$ denotes a technical failure cell and $V$ denotes a viable cell. The prior probabilities are $\\pi_{F} = 0.05$ and $\\pi_{V} = 0.95$. The decision is $D \\in \\{\\text{keep}, \\text{discard}\\}$. The loss of keeping a failure is $\\lambda_{F} = 10$ and the loss of discarding a viable cell is $\\lambda_{V} = 1$. Correct decisions incur zero loss.\n\nTwo observable quality metrics are measured per cell: the mitochondrial read fraction $m \\in [0, 1]$ and the total UMI count $c \\in [0, \\infty)$. For the mitochondrial read fraction, the conditional densities are modeled as Beta distributions\n$$\nf_{M}(m \\mid F) = \\frac{1}{B(2, 10)} \\, m^{2-1} (1 - m)^{10-1}, \\quad f_{M}(m \\mid V) = \\frac{1}{B(1, 10)} \\, m^{1-1} (1 - m)^{10-1},\n$$\nwhere $B(\\cdot, \\cdot)$ denotes the Beta function. For the UMI counts, the conditional densities are modeled as exponential distributions\n$$\nf_{C}(c \\mid F) = \\theta_{F} \\exp(-\\theta_{F} c), \\quad f_{C}(c \\mid V) = \\theta_{V} \\exp(-\\theta_{V} c),\n$$\nwith $\\theta_{F} = 0.005$ and $\\theta_{V} = 0.00125$.\n\nUsing only one metric at a time, define thresholds $m^{\\ast}$ and $c^{\\ast}$ such that a cell is discarded if $m > m^{\\ast}$ or if $c < c^{\\ast}$. Starting from Bayes’ theorem and the expected loss principle, derive the decision rule and solve analytically for $m^{\\ast}$ and $c^{\\ast}$ in terms of the given $\\pi_{F}$, $\\pi_{V}$, $\\lambda_{F}$, $\\lambda_{V}$, and the distributional parameters. Compute the numerical values for $m^{\\ast}$ and $c^{\\ast}$ using the provided parameters.\n\nReport the pair $(m^{\\ast}, c^{\\ast})$ as a row matrix in the final answer. Express $m^{\\ast}$ as a unitless decimal between $0$ and $1$, and $c^{\\ast}$ as a count. Round both numbers to four significant figures.", "solution": "The problem asks for the derivation of optimal decision thresholds for quality control in single-cell RNA sequencing data, based on a cost-sensitive Bayesian decision rule. The goal is to minimize the expected loss associated with misclassifying cells as either viable or technical failures.\n\nLet the latent class of a cell be $Y \\in \\{F, V\\}$, where $F$ denotes a technical failure and $V$ denotes a viable cell. The prior probabilities are given as $\\pi_{F} = P(Y=F) = 0.05$ and $\\pi_{V} = P(Y=V) = 0.95$.\nThe decision to be made is $D \\in \\{\\text{keep}, \\text{discard}\\}$.\nThe loss function $\\mathcal{L}(D, Y)$ is defined by the costs of incorrect decisions:\n-   Loss of keeping a failure cell: $\\mathcal{L}(\\text{keep}, F) = \\lambda_{F} = 10$.\n-   Loss of discarding a viable cell: $\\mathcal{L}(\\text{discard}, V) = \\lambda_{V} = 1$.\n-   Correct decisions incur zero loss: $\\mathcal{L}(\\text{keep}, V) = 0$ and $\\mathcal{L}(\\text{discard}, F) = 0$.\n\nThe decision rule is derived by minimizing the expected loss, conditioned on an observation $x$. The observation $x$ will be either the mitochondrial read fraction $m$ or the UMI count $c$.\n\nThe conditional expected loss for the action \"discard\" is:\n$$\nR(\\text{discard} \\mid x) = \\mathcal{L}(\\text{discard}, V) P(Y=V \\mid x) + \\mathcal{L}(\\text{discard}, F) P(Y=F \\mid x) = \\lambda_{V} P(Y=V \\mid x)\n$$\nThe conditional expected loss for the action \"keep\" is:\n$$\nR(\\text{keep} \\mid x) = \\mathcal{L}(\\text{keep}, V) P(Y=V \\mid x) + \\mathcal{L}(\\text{keep}, F) P(Y=F \\mid x) = \\lambda_{F} P(Y=F \\mid x)\n$$\nThe Bayesian decision rule is to choose the action that minimizes the conditional expected loss. We therefore decide to \"discard\" the cell if $R(\\text{discard} \\mid x) < R(\\text{keep} \\mid x)$, which translates to:\n$$\n\\lambda_{V} P(Y=V \\mid x) < \\lambda_{F} P(Y=F \\mid x)\n$$\nUsing Bayes' theorem, the posterior probabilities are $P(Y \\mid x) = \\frac{f(x \\mid Y) P(Y)}{f(x)}$, where $f(x \\mid Y)$ is the class-conditional probability density (likelihood), $P(Y)$ is the prior probability, and $f(x)$ is the marginal probability of the evidence. Substituting these into the inequality gives:\n$$\n\\lambda_{V} \\frac{f(x \\mid V) \\pi_V}{f(x)} < \\lambda_{F} \\frac{f(x \\mid F) \\pi_F}{f(x)}\n$$\nSince $f(x)$ is a density and thus positive, we can multiply both sides by $f(x)$ and rearrange the terms to form a likelihood ratio test. The region for discarding a cell is given by:\n$$\n\\frac{f(x \\mid F)}{f(x \\mid V)} > \\frac{\\lambda_V \\pi_V}{\\lambda_F \\pi_F}\n$$\nLet's define the decision threshold $\\tau = \\frac{\\lambda_V \\pi_V}{\\lambda_F \\pi_F}$. We will analyze the two metrics separately.\n\n**1. Mitochondrial Read Fraction Threshold ($m^{\\ast}$)**\n\nFor this metric, the observation is $x = m$. The conditional densities are given by Beta distributions:\n$$\nf_{M}(m \\mid F) = \\frac{1}{B(2, 10)} m^{1} (1 - m)^{9}\n$$\n$$\nf_{M}(m \\mid V) = \\frac{1}{B(1, 10)} m^{0} (1 - m)^{9}\n$$\nThe likelihood ratio is:\n$$\n\\frac{f_{M}(m \\mid F)}{f_{M}(m \\mid V)} = \\frac{\\frac{m(1-m)^9}{B(2, 10)}}{\\frac{(1-m)^9}{B(1, 10)}} = m \\frac{B(1, 10)}{B(2, 10)}\n$$\nUsing the identity for the Beta function $B(a, b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$, we can simplify the ratio of Beta functions:\n$$\n\\frac{B(1, 10)}{B(2, 10)} = \\frac{\\Gamma(1)\\Gamma(10)/\\Gamma(11)}{\\Gamma(2)\\Gamma(10)/\\Gamma(12)} = \\frac{\\Gamma(1)\\Gamma(12)}{\\Gamma(2)\\Gamma(11)} = \\frac{0! \\cdot 11!}{1! \\cdot 10!} = \\frac{11!}{10!} = 11\n$$\nSo, the likelihood ratio is simply $11m$. The decision rule is to discard if:\n$$\n11m > \\tau \\implies m > \\frac{\\tau}{11}\n$$\nThis matches the problem's discard condition $m > m^{\\ast}$. Thus, the threshold $m^{\\ast}$ is:\n$$\nm^{\\ast} = \\frac{\\tau}{11} = \\frac{1}{11} \\left( \\frac{\\lambda_V \\pi_V}{\\lambda_F \\pi_F} \\right)\n$$\nSubstituting the given numerical values: $\\lambda_V = 1$, $\\pi_V = 0.95$, $\\lambda_F = 10$, $\\pi_F = 0.05$.\n$$\nm^{\\ast} = \\frac{1}{11} \\left( \\frac{1 \\times 0.95}{10 \\times 0.05} \\right) = \\frac{1}{11} \\left( \\frac{0.95}{0.50} \\right) = \\frac{1.9}{11} \\approx 0.172727...\n$$\nRounding to four significant figures, $m^{\\ast} = 0.1727$.\n\n**2. UMI Count Threshold ($c^{\\ast}$)**\n\nFor this metric, the observation is $x = c$. The conditional densities are given by exponential distributions:\n$$\nf_{C}(c \\mid F) = \\theta_{F} \\exp(-\\theta_{F} c)\n$$\n$$\nf_{C}(c \\mid V) = \\theta_{V} \\exp(-\\theta_{V} c)\n$$\nThe likelihood ratio is:\n$$\n\\frac{f_{C}(c \\mid F)}{f_{C}(c \\mid V)} = \\frac{\\theta_{F} \\exp(-\\theta_{F} c)}{\\theta_{V} \\exp(-\\theta_{V} c)} = \\frac{\\theta_{F}}{\\theta_{V}} \\exp(-(\\theta_{F} - \\theta_{V})c)\n$$\nThe rule is to discard if this ratio is greater than $\\tau$:\n$$\n\\frac{\\theta_{F}}{\\theta_{V}} \\exp(-(\\theta_{F} - \\theta_{V})c) > \\tau\n$$\nGiven $\\theta_{F} = 0.005$ and $\\theta_{V} = 0.00125$, we have $\\theta_{F} > \\theta_{V}$. Therefore, the term $\\theta_{F} - \\theta_{V}$ is positive, and the function $\\exp(-(\\theta_{F} - \\theta_{V})c)$ is a monotonically decreasing function of $c$. This means the inequality holds for values of $c$ that are *less than* some threshold. This is consistent with the stated discard rule $c < c^{\\ast}$. To find $c^{\\ast}$, we solve for $c$ at the boundary of the decision region:\n$$\n\\frac{\\theta_{F}}{\\theta_{V}} \\exp(-(\\theta_{F} - \\theta_{V})c^{\\ast}) = \\tau\n$$\n$$\n\\exp(-(\\theta_{F} - \\theta_{V})c^{\\ast}) = \\tau \\frac{\\theta_{V}}{\\theta_{F}}\n$$\nTaking the natural logarithm of both sides:\n$$\n-(\\theta_{F} - \\theta_{V})c^{\\ast} = \\ln\\left(\\tau \\frac{\\theta_{V}}{\\theta_{F}}\\right)\n$$\n$$\nc^{\\ast} = -\\frac{1}{\\theta_{F} - \\theta_{V}} \\ln\\left(\\tau \\frac{\\theta_{V}}{\\theta_{F}}\\right) = \\frac{1}{\\theta_{F} - \\theta_{V}} \\ln\\left(\\frac{1}{\\tau} \\frac{\\theta_{F}}{\\theta_{V}}\\right)\n$$\nSubstituting $\\tau = \\frac{\\lambda_V \\pi_V}{\\lambda_F \\pi_F}$, we get the analytical expression for $c^{\\ast}$:\n$$\nc^{\\ast} = \\frac{1}{\\theta_{F} - \\theta_{V}} \\ln\\left(\\frac{\\lambda_F \\pi_F \\theta_F}{\\lambda_V \\pi_V \\theta_V}\\right)\n$$\nNow, we substitute the numerical values:\n$$\nc^{\\ast} = \\frac{1}{0.005 - 0.00125} \\ln\\left(\\frac{10 \\times 0.05 \\times 0.005}{1 \\times 0.95 \\times 0.00125}\\right)\n$$\n$$\nc^{\\ast} = \\frac{1}{0.00375} \\ln\\left(\\frac{0.0025}{0.0011875}\\right) = \\frac{1}{0.00375} \\ln\\left(\\frac{40}{19}\\right)\n$$\n$$\nc^{\\ast} \\approx \\frac{1}{0.00375} \\times 0.744432 \\approx 198.5152\n$$\nRounding to four significant figures, $c^{\\ast} = 198.5$.\n\nThe derived thresholds are $m^{\\ast} \\approx 0.1727$ and $c^{\\ast} \\approx 198.5$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.1727 & 198.5 \\end{pmatrix}}\n$$", "id": "4324377"}, {"introduction": "A major challenge in cell type annotation is that gene expression is influenced by dynamic biological processes, most notably the cell cycle, which can confound the stable signals that define cell identity. This practice delves into the common technique of \"regressing out\" such unwanted variation using linear models. You will not only derive the mathematical machinery for this process using orthogonal projections but also critically analyze the inherent trade-off, quantifying how much of a true biological signal might be lost when it is correlated with the confounder being removed. [@problem_id:4324318]", "problem": "Consider single-cell ribonucleic acid sequencing (scRNA-seq) measurements organized as a matrix $X \\in \\mathbb{R}^{n \\times p}$, where $n$ is the number of cells and $p$ is the number of genes. Rows of $X$ are cells and columns are genes. To identify and annotate cell types, one often wishes to remove variation arising from the cell cycle that can confound type-specific signals. Let $S \\in \\mathbb{R}^{n \\times q}$ be a matrix of $q$ cell-level covariates quantifying cell cycle state (for example, $q=2$ scores for $\\mathrm{G1/S}$ and $\\mathrm{G2/M}$ programs), and let $\\mathbf{1} \\in \\mathbb{R}^{n}$ denote the intercept vector of ones. Define the design matrix $Z = [\\mathbf{1}\\;\\;S] \\in \\mathbb{R}^{n \\times (q+1)}$. Assume that $X$ has been transformed to a scale on which linear effects are appropriate and that variation in each gene’s expression across cells can be decomposed into additive contributions from the intercept, the cell cycle covariates, and residuals.\n\nStarting only from the definition of ordinary least squares (OLS) as the minimizer of the sum of squared residuals and basic properties of Euclidean projections in $\\mathbb{R}^{n}$, do the following:\n\n1. Derive, for an arbitrary gene indexed by $j \\in \\{1,\\dots,p\\}$, the OLS solution for the regression of the cell expression vector $X_{\\cdot j} \\in \\mathbb{R}^{n}$ onto the columns of $Z$, and then obtain a matrix expression for the residualized expression matrix $\\tilde{X} \\in \\mathbb{R}^{n \\times p}$ that has cell cycle effects removed across all genes simultaneously.\n\n2. To interrogate when removing cell cycle effects is appropriate versus harmful to true type differences, let $T \\in \\mathbb{R}^{n}$ be a centered cell type contrast vector (for example, $T$ might indicate two types with entries summing to zero), representing a direction in cell space along which true type identity varies. Let $P_{S}$ denote the orthogonal projector onto the column space of $S$, and let $\\theta \\in [0,\\pi/2]$ be the angle between the vector $T$ and the subspace spanned by the columns of $S$, defined by $\\cos(\\theta) = \\|P_{S}T\\|/\\|T\\|$. Using only the definitions of projection and angle in Euclidean space, derive the fraction $f(\\theta)$ of the true type variance in $T$ that is removed by regressing out (and subtracting) the cell cycle covariates.\n\nProvide your final answer as the closed-form analytic expression for $f(\\theta)$. No numerical rounding is required. If you introduce any acronym, define it on first use by its full name followed by the acronym in parentheses.", "solution": "The problem is divided into two parts. The first part requires the derivation of the residualized expression matrix after regressing out confounding effects. The second part requires the calculation of the fraction of variance removed from a specific signal vector by this procedure.\n\n1.  Derivation of the residualized expression matrix $\\tilde{X}$.\n\nLet $X_{\\cdot j} \\in \\mathbb{R}^{n}$ denote the expression vector for an arbitrary gene $j$, where $j \\in \\{1, \\dots, p\\}$. The linear model for this gene's expression as a function of the intercept and cell cycle covariates is given by:\n$$X_{\\cdot j} = Z \\beta_j + \\epsilon_j$$\nwhere $Z = [\\mathbf{1}\\;\\;S] \\in \\mathbb{R}^{n \\times (q+1)}$ is the design matrix, $\\beta_j \\in \\mathbb{R}^{q+1}$ is the vector of regression coefficients for gene $j$, and $\\epsilon_j \\in \\mathbb{R}^{n}$ is the vector of residuals.\n\nThe method of ordinary least squares (OLS) seeks to find the estimate $\\hat{\\beta}_j$ that minimizes the sum of squared residuals (SSR), which is equivalent to minimizing the squared Euclidean norm of the residual vector $\\epsilon_j$.\n$$SSR(\\beta_j) = \\|\\epsilon_j\\|^2 = \\|X_{\\cdot j} - Z\\beta_j\\|^2$$\nTo find the minimum, we can expand the squared norm and differentiate with respect to $\\beta_j$.\n$$SSR(\\beta_j) = (X_{\\cdot j} - Z\\beta_j)^T (X_{\\cdot j} - Z\\beta_j) = X_{\\cdot j}^T X_{\\cdot j} - 2 X_{\\cdot j}^T Z \\beta_j + \\beta_j^T Z^T Z \\beta_j$$\nThe gradient of the SSR with respect to $\\beta_j$ is:\n$$\\frac{\\partial(SSR)}{\\partial \\beta_j} = -2 Z^T X_{\\cdot j} + 2 Z^T Z \\beta_j$$\nSetting the gradient to zero to find the minimum gives the normal equations:\n$$Z^T Z \\hat{\\beta}_j = Z^T X_{\\cdot j}$$\nAssuming the matrix $Z^T Z$ is invertible (which is true if the columns of $Z$ are linearly independent, a standard assumption in OLS), we can solve for the OLS estimate of the coefficients:\n$$\\hat{\\beta}_j = (Z^T Z)^{-1} Z^T X_{\\cdot j}$$\nThe vector of fitted values, $\\hat{X}_{\\cdot j}$, is the projection of the original data vector $X_{\\cdot j}$ onto the column space of $Z$, $\\mathrm{col}(Z)$.\n$$\\hat{X}_{\\cdot j} = Z \\hat{\\beta}_j = Z (Z^T Z)^{-1} Z^T X_{\\cdot j}$$\nWe define the orthogonal projection matrix onto $\\mathrm{col}(Z)$ as $P_Z = Z (Z^T Z)^{-1} Z^T$. Thus, the fitted values are $\\hat{X}_{\\cdot j} = P_Z X_{\\cdot j}$.\n\nThe residual vector for gene $j$, which represents the gene's expression after the effects of the covariates in $Z$ have been removed, is the difference between the observed and fitted values:\n$$\\tilde{X}_{\\cdot j} = X_{\\cdot j} - \\hat{X}_{\\cdot j} = X_{\\cdot j} - P_Z X_{\\cdot j} = (I - P_Z) X_{\\cdot j}$$\nwhere $I$ is the $n \\times n$ identity matrix. The matrix $I - P_Z$ is the orthogonal projection matrix onto the subspace orthogonal to $\\mathrm{col}(Z)$.\n\nTo obtain the entire residualized expression matrix $\\tilde{X} \\in \\mathbb{R}^{n \\times p}$, we apply this operation to every gene (i.e., every column of $X$) simultaneously. This can be expressed in a single matrix equation:\n$$\\tilde{X} = X - P_Z X = (I - P_Z) X$$\n\n2.  Derivation of the fraction of variance removed from the true type contrast vector $T$.\n\nWe are asked to find the fraction $f(\\theta)$ of the true type variance in a centered contrast vector $T \\in \\mathbb{R}^n$ that is removed by regressing out the cell cycle covariates, which are the columns of the matrix $S \\in \\mathbb{R}^{n \\times q}$.\n\nThe \"variance\" of a vector in the geometric context of regression analysis corresponds to its sum of squares. Since the vector $T$ is centered (its entries sum to zero), its total sum of squares is simply its squared Euclidean norm, $\\|T\\|^2$.\n\nThe process of \"regressing out\" the effects of the covariates in $S$ from the vector $T$ means projecting $T$ onto the column space of $S$, denoted $\\mathrm{col}(S)$. The resulting vector of fitted values, which represents the part of $T$ that is explained by (and thus \"removed\" by) the cell cycle covariates, is given by:\n$$\\hat{T}_S = P_S T$$\nwhere $P_S$ is the orthogonal projection matrix onto $\\mathrm{col}(S)$.\n\nThe variance of this removed component is its sum of squares, $\\|\\hat{T}_S\\|^2 = \\|P_S T\\|^2$. This quantity is the explained sum of squares in the regression of $T$ on $S$. Since $T$ is centered, this is the correct expression for the explained variance.\n\nThe fraction of the total variance of $T$ that is removed by the regression is the ratio of the removed variance to the total variance:\n$$f = \\frac{\\text{Removed Variance}}{\\text{Total Variance}} = \\frac{\\|P_S T\\|^2}{\\|T\\|^2}$$\nThe problem provides the definition of the angle $\\theta \\in [0, \\pi/2]$ between the vector $T$ and the subspace $\\mathrm{col}(S)$:\n$$\\cos(\\theta) = \\frac{\\|P_S T\\|}{\\|T\\|}$$\nThis is the cosine of the angle between a vector and its projection onto a subspace. Squaring both sides gives:\n$$\\cos^2(\\theta) = \\left( \\frac{\\|P_S T\\|}{\\|T\\|} \\right)^2 = \\frac{\\|P_S T\\|^2}{\\|T\\|^2}$$\nBy direct substitution, we find that the fraction of variance removed, $f$, is equal to the square of the cosine of the angle $\\theta$:\n$$f(\\theta) = \\cos^2(\\theta)$$\nThis result quantifies the extent to which the biological signal of interest ($T$) is confounded with the cell cycle signal ($S$). If $T$ is orthogonal to the cell cycle subspace ($\\theta = \\pi/2$), then $\\cos^2(\\theta) = 0$ and no variance is removed. If $T$ lies entirely within the cell cycle subspace ($\\theta = 0$), then $\\cos^2(\\theta) = 1$ and all the variance is removed, indicating that the 'true type' signal is indistinguishable from cell cycle effects under this model.", "answer": "$$\\boxed{\\cos^2(\\theta)}$$", "id": "4324318"}, {"introduction": "Evaluating the performance of a cell type classifier is not always straightforward, as biological samples typically feature a highly imbalanced distribution of cell types. In such scenarios, overall accuracy can be a misleading metric, as it is dominated by the most abundant classes. This exercise demonstrates how to move beyond simple accuracy by calculating and contrasting macro- and micro-averaged F1 scores from a confusion matrix, providing a more robust and nuanced assessment of annotation performance, particularly for the identification of rare but biologically significant cell populations. [@problem_id:4324372]", "problem": "In a single-cell transcriptomics study, a supervised classifier assigns each cell to one of four immunologic cell types during cell type identification and annotation: T cells ($\\mathrm{T}$), B cells ($\\mathrm{B}$), monocytes ($\\mathrm{Mo}$), and dendritic cells ($\\mathrm{DC}$). There are $400$ cells total. The ground-truth counts are $250$ for $\\mathrm{T}$, $100$ for $\\mathrm{B}$, $40$ for $\\mathrm{Mo}$, and $10$ for $\\mathrm{DC}$. The resulting confusion matrix $M$ (rows indexed by ground-truth class in the order $\\mathrm{T}, \\mathrm{B}, \\mathrm{Mo}, \\mathrm{DC}$; columns indexed by predicted class in the same order) is\n$$\nM \\;=\\;\n\\begin{pmatrix}\n220 & 15 & 10 & 5 \\\\\n8 & 85 & 5 & 2 \\\\\n4 & 3 & 30 & 3 \\\\\n1 & 1 & 3 & 5\n\\end{pmatrix}.\n$$\nStarting only from the core definitions of true positives, false positives, false negatives, precision, recall, and the harmonic mean, derive the class-wise F1 scores for each of the four classes, then compute the macro-averaged F1 score (the unweighted arithmetic mean of the class-wise F1 scores) and the micro-averaged F1 score (computed by pooling all decisions across classes before calculating precision and recall). Round both final numeric answers to $4$ significant figures. In your derivation, explain which averaging scheme better reflects performance for rare cell types in this setting.", "solution": "The problem involves multi-class, single-label classification. The fundamental base consists of the following core definitions. For a given class $c$:\n- The true positives are $TP_c$, the number of instances of class $c$ correctly predicted as $c$.\n- The false positives are $FP_c$, the number of instances predicted as $c$ that are not truly $c$.\n- The false negatives are $FN_c$, the number of instances truly $c$ that are not predicted as $c$.\n- Precision is $P_c = \\frac{TP_c}{TP_c + FP_c}$.\n- Recall is $R_c = \\frac{TP_c}{TP_c + FN_c}$.\n- The F1 score is the harmonic mean of precision and recall, which from the harmonic mean definition can be written as\n$$\nF1_c \\;=\\; \\frac{2\\,P_c\\,R_c}{P_c + R_c}.\n$$\nUsing $P_c = \\frac{TP_c}{TP_c + FP_c}$ and $R_c = \\frac{TP_c}{TP_c + FN_c}$, substitution and algebra yield the equivalent expression\n$$\nF1_c \\;=\\; \\frac{2\\,TP_c}{2\\,TP_c + FP_c + FN_c}.\n$$\nFor the macro-averaged F1 score, we compute the unweighted mean of $F1_c$ over classes. For the micro-averaged F1 score, we first sum $TP_c$, $FP_c$, and $FN_c$ across classes to obtain global $TP$, $FP$, and $FN$, then compute precision and recall using the same definitions and finally their harmonic mean.\n\nWe extract $TP_c$, $FP_c$, and $FN_c$ per class from the confusion matrix $M$. Let the classes be ordered as $\\mathrm{T}, \\mathrm{B}, \\mathrm{Mo}, \\mathrm{DC}$. For each class $c$:\n- $TP_c$ is the diagonal element $M_{cc}$.\n- The column sums are the total predicted counts per class, and the row sums are the total ground-truth counts per class. Then $FP_c$ equals the column sum for class $c$ minus $TP_c$, and $FN_c$ equals the row sum for class $c$ minus $TP_c$.\n\nCompute row sums (ground truth totals): $\\mathrm{T}$ has $250$, $\\mathrm{B}$ has $100$, $\\mathrm{Mo}$ has $40$, $\\mathrm{DC}$ has $10$; these match the given totals. Compute column sums (predicted totals):\n- Predicted $\\mathrm{T}$: $220 + 8 + 4 + 1 = 233$.\n- Predicted $\\mathrm{B}$: $15 + 85 + 3 + 1 = 104$.\n- Predicted $\\mathrm{Mo}$: $10 + 5 + 30 + 3 = 48$.\n- Predicted $\\mathrm{DC}$: $5 + 2 + 3 + 5 = 15$.\n\nNow compute per-class quantities and class-wise F1 scores.\n\nFor $\\mathrm{T}$:\n- $TP_{\\mathrm{T}} = 220$.\n- $FP_{\\mathrm{T}} = 233 - 220 = 13$.\n- $FN_{\\mathrm{T}} = 250 - 220 = 30$.\n- $F1_{\\mathrm{T}} = \\frac{2 \\cdot 220}{2 \\cdot 220 + 13 + 30} = \\frac{440}{483}$.\n\nFor $\\mathrm{B}$:\n- $TP_{\\mathrm{B}} = 85$.\n- $FP_{\\mathrm{B}} = 104 - 85 = 19$.\n- $FN_{\\mathrm{B}} = 100 - 85 = 15$.\n- $F1_{\\mathrm{B}} = \\frac{2 \\cdot 85}{2 \\cdot 85 + 19 + 15} = \\frac{170}{204} = \\frac{85}{102} = \\frac{5}{6}$.\n\nFor $\\mathrm{Mo}$:\n- $TP_{\\mathrm{Mo}} = 30$.\n- $FP_{\\mathrm{Mo}} = 48 - 30 = 18$.\n- $FN_{\\mathrm{Mo}} = 40 - 30 = 10$.\n- $F1_{\\mathrm{Mo}} = \\frac{2 \\cdot 30}{2 \\cdot 30 + 18 + 10} = \\frac{60}{88} = \\frac{15}{22}$.\n\nFor $\\mathrm{DC}$:\n- $TP_{\\mathrm{DC}} = 5$.\n- $FP_{\\mathrm{DC}} = 15 - 5 = 10$.\n- $FN_{\\mathrm{DC}} = 10 - 5 = 5$.\n- $F1_{\\mathrm{DC}} = \\frac{2 \\cdot 5}{2 \\cdot 5 + 10 + 5} = \\frac{10}{25} = \\frac{2}{5}$.\n\nCompute the macro-averaged F1 score:\n$$\nF1_{\\mathrm{macro}} \\;=\\; \\frac{1}{4} \\left( \\frac{440}{483} + \\frac{5}{6} + \\frac{15}{22} + \\frac{2}{5} \\right).\n$$\nTo evaluate numerically, approximate each term:\n- $\\frac{440}{483} \\approx 0.910971$,\n- $\\frac{5}{6} \\approx 0.833333$,\n- $\\frac{15}{22} \\approx 0.681818$,\n- $\\frac{2}{5} = 0.4$.\nSum: $0.910971 + 0.833333 + 0.681818 + 0.4 = 2.826122$. Divide by $4$ to obtain\n$$\nF1_{\\mathrm{macro}} \\approx 0.7065305.\n$$\nRounded to $4$ significant figures: $0.7065$.\n\nFor the micro-averaged F1 score, compute global $TP$, $FP$, and $FN$ by summing over classes:\n- $TP = 220 + 85 + 30 + 5 = 340$.\n- The total number of predictions is $400$, so $FP = 400 - TP = 60$. By the single-label property, $FN$ also equals $60$ because each misclassification contributes one $FP$ for the predicted class and one $FN$ for the true class.\nGlobal precision and recall are\n$$\nP_{\\mathrm{micro}} = \\frac{TP}{TP + FP} = \\frac{340}{340 + 60} = \\frac{340}{400} = \\frac{17}{20} = 0.85,\n$$\n$$\nR_{\\mathrm{micro}} = \\frac{TP}{TP + FN} = \\frac{340}{340 + 60} = \\frac{340}{400} = \\frac{17}{20} = 0.85.\n$$\nThus\n$$\nF1_{\\mathrm{micro}} = \\frac{2\\,P_{\\mathrm{micro}}\\,R_{\\mathrm{micro}}}{P_{\\mathrm{micro}} + R_{\\mathrm{micro}}} = \\frac{2 \\cdot 0.85 \\cdot 0.85}{0.85 + 0.85} = 0.85,\n$$\nequivalently,\n$$\nF1_{\\mathrm{micro}} = \\frac{2\\,TP}{2\\,TP + FP + FN} = \\frac{680}{800} = \\frac{17}{20} = 0.85.\n$$\nRounded to $4$ significant figures: $0.8500$.\n\nRegarding which averaging scheme better reflects performance for rare cell types, macro-averaging weights each class equally in the mean, independent of class prevalence. In this dataset, $\\mathrm{DC}$ and $\\mathrm{Mo}$ are rare with $10$ and $40$ cells, respectively, yet their class-wise F1 scores ($\\frac{2}{5}$ and $\\frac{15}{22}$) contribute equally to $F1_{\\mathrm{macro}}$ as the more prevalent classes. In contrast, micro-averaging pools all decisions and is dominated by the prevalent classes ($\\mathrm{T}$ and $\\mathrm{B}$, totaling $350$ cells), yielding a high $F1_{\\mathrm{micro}}$ that largely reflects performance on abundant types. Therefore, for assessing performance on rare cell types in systems biomedicine cell annotation, $F1_{\\mathrm{macro}}$ better reflects performance sensitivity to rare classes than $F1_{\\mathrm{micro}}$, which largely reflects overall accuracy dominated by common classes.", "answer": "$$\\boxed{\\begin{pmatrix}0.7065 & 0.8500\\end{pmatrix}}$$", "id": "4324372"}]}