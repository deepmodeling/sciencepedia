{"hands_on_practices": [{"introduction": "The first step in analyzing spatial data is often to formally define which measurement locations are \"neighbors,\" allowing us to model interactions and dependencies. This practice explores the construction of an $\\epsilon$-radius graph, a fundamental method where neighbors are defined as any two points within a certain distance $\\epsilon$ of each other. By implementing this from first principles and observing how graph connectivity changes with the distance threshold [@problem_id:4315795], you will develop a core skill for building spatial models and gain intuition for how the choice of neighborhood size impacts downstream analysis.", "problem": "You are given spatial coordinates representing measurement locations in a spatially resolved omics assay, treated as points in a two-dimensional Euclidean space. The objective is to construct, for a given nonnegative distance threshold $\\,\\epsilon\\,$ expressed in micrometers ($\\,\\mu \\mathrm{m}\\,$), an $\\epsilon$-radius graph over the points, compute its adjacency matrix, and evaluate graph connectivity as a function of $\\,\\epsilon\\,$. The program must implement the following from first principles.\n\nFundamental base and definitions:\n- The Central Dogma of Molecular Biology establishes measurement of molecular species at spatial locations, but for computation we abstract locations as points in a metric space. In a Euclidean metric space, the distance between two points $\\,p_i=(x_i,y_i)\\,$ and $\\,p_j=(x_j,y_j)\\,$ is defined by the Euclidean metric\n$$\nd_{ij} \\equiv \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}.\n$$\n- A simple undirected graph over $\\,N\\,$ points is defined by its adjacency matrix $\\,A^{(\\epsilon)} \\in \\{0,1\\}^{N \\times N}\\,$, where\n$$\nA^{(\\epsilon)}_{ij} \\equiv \\begin{cases}\n1, & \\text{if } i \\ne j \\text{ and } d_{ij} \\le \\epsilon,\\\\\n0, & \\text{otherwise.}\n\\end{cases}\n$$\nSelf-loops are excluded, so $\\,A^{(\\epsilon)}_{ii} = 0\\,$ for all $\\,i\\,$.\n- Connectivity is evaluated via connected components. Let $\\,K(\\epsilon)\\,$ denote the integer number of connected components of the graph induced by $\\,A^{(\\epsilon)}\\,$. Let $\\,L(\\epsilon)\\,$ denote the integer size of the largest connected component. Define the connectivity fraction\n$$\nF(\\epsilon) \\equiv \\frac{L(\\epsilon)}{N},\n$$\na real number rounded to four decimal places.\n\nProgram requirements:\n- Given a list of coordinates $\\,\\{(x_i,y_i)\\}_{i=1}^N\\,$ in $\\,\\mu \\mathrm{m}\\,$ and a set of thresholds $\\,\\{\\epsilon_t\\}_{t=1}^T\\,$ also in $\\,\\mu \\mathrm{m}\\,$, construct $\\,A^{(\\epsilon_t)}\\,$ for each $\\,\\epsilon_t\\,$ and compute $\\,K(\\epsilon_t)\\,$ and $\\,F(\\epsilon_t)\\,$.\n- The final output must not include physical units; only numbers as specified. Internally, distances must be treated in $\\,\\mu \\mathrm{m}\\,$, and the threshold $\\,\\epsilon\\,$ values below are all in $\\,\\mu \\mathrm{m}\\,$.\n\nTest suite and coverage:\nProvide results for the following four independent test cases, each consisting of a coordinate set and a threshold set. These test cases probe a general case, boundary conditions, duplicated coordinates, and grid structure with an outlier.\n\n- Test case $\\,1\\,$ (two compact clusters separated in space; $\\,N=6\\,$):\n  - Coordinates (in $\\,\\mu \\mathrm{m}\\,$): $\\,(0,0),\\,(1,0),\\,(0.5,0.5),\\,(10,0),\\,(11,0),\\,(10.5,-0.5)\\,$.\n  - Thresholds (in $\\,\\mu \\mathrm{m}\\,$): $\\,\\epsilon \\in [0.4,\\,0.71,\\,1.0,\\,8.9,\\,9.0]\\,$.\n\n- Test case $\\,2\\,$ (duplicates and exact-distance boundary; $\\,N=3\\,$):\n  - Coordinates (in $\\,\\mu \\mathrm{m}\\,$): $\\,(0,0),\\,(0,0),\\,(3,4)\\,$.\n  - Thresholds (in $\\,\\mu \\mathrm{m}\\,$): $\\,\\epsilon \\in [0.0,\\,0.0001,\\,5.0]\\,$.\n\n- Test case $\\,3\\,$ (linear chain with equal spacing; $\\,N=5\\,$):\n  - Coordinates (in $\\,\\mu \\mathrm{m}\\,$): $\\,(0,0),\\,(2,0),\\,(4,0),\\,(6,0),\\,(8,0)\\,$.\n  - Thresholds (in $\\,\\mu \\mathrm{m}\\,$): $\\,\\epsilon \\in [1.0,\\,2.0,\\,2.1,\\,8.0]\\,$.\n\n- Test case $\\,4\\,$ (grid with an outlier; $\\,N=5\\,$):\n  - Coordinates (in $\\,\\mu \\mathrm{m}\\,$): $\\,(0,0),\\,(0,1),\\,(1,0),\\,(1,1),\\,(4,4)\\,$.\n  - Thresholds (in $\\,\\mu \\mathrm{m}\\,$): $\\,\\epsilon \\in [0.9,\\,1.0,\\,1.5,\\,5.0]\\,$.\n\nAnswer specification:\n- For each test case, compute the list $\\,\\{K(\\epsilon_t)\\}_{t=1}^T\\,$ and the list $\\,\\{F(\\epsilon_t)\\}_{t=1}^T\\,$ where $\\,F(\\epsilon_t)\\,$ is rounded to four decimal places and expressed as a decimal number.\n- Your program should produce a single line of output containing, for each test case in the above order, a pair of lists $\\,\\big([\\text{list of }K],\\,[\\text{list of }F]\\big)\\,$, and aggregate these pairs into one outer list. Concretely, the output must be a single line in the form\n$$\n\\big[\\,\\big([K_{1,1},\\ldots,K_{1,T_1}],\\,[F_{1,1},\\ldots,F_{1,T_1}]\\big),\\,\\ldots,\\,\\big([K_{4,1},\\ldots,K_{4,T_4}],\\,[F_{4,1},\\ldots,F_{4,T_4}]\\big)\\,\\big].\n$$\nOnly numbers and brackets are permitted in the output line; do not include units or additional text.", "solution": "The user's request will be addressed by first validating the problem statement, then—contingent on its validity—providing a complete solution that includes both the underlying scientific reasoning and a full implementation in Python.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\n\n- **Metric Space**: The problem is set in a two-dimensional Euclidean space.\n- **Euclidean Distance**: The distance between two points $\\,p_i=(x_i,y_i)\\,$ and $\\,p_j=(x_j,y_j)\\,$ is defined as $d_{ij} \\equiv \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}$.\n- **Graph Definition**: An $\\epsilon$-radius graph is constructed over $\\,N\\,$ points. Its adjacency matrix $\\,A^{(\\epsilon)} \\in \\{0,1\\}^{N \\times N}\\,$ is defined by the rule: $A^{(\\epsilon)}_{ij} = 1\\,$ if $\\,i \\ne j\\,$ and $\\,d_{ij} \\le \\epsilon\\,$, and $\\,A^{(\\epsilon)}_{ij} = 0\\,$ otherwise. Self-loops are excluded, meaning $\\,A^{(\\epsilon)}_{ii} = 0\\,$ for all $\\,i\\,$.\n- **Connectivity Metrics**:\n  - $\\,K(\\epsilon)\\,$: The integer number of connected components in the graph for a given $\\,\\epsilon\\,$.\n  - $\\,L(\\epsilon)\\,$: The integer size of the largest connected component.\n  - $\\,F(\\epsilon) \\equiv \\frac{L(\\epsilon)}{N}\\,$: The connectivity fraction, a real number rounded to four decimal places.\n- **Input Data**: The program receives a list of coordinates $\\,\\{(x_i,y_i)\\}_{i=1}^N\\,$ and a set of distance thresholds $\\,\\{\\epsilon_t\\}_{t=1}^T\\,$. All spatial and distance units are in micrometers ($\\,\\mu \\mathrm{m}\\,$).\n- **Test Cases**:\n    - **Case 1**: $\\,N=6\\,$ coordinates $\\,(0,0),\\,(1,0),\\,(0.5,0.5),\\,(10,0),\\,(11,0),\\,(10.5,-0.5)\\,$ with thresholds $\\,\\epsilon \\in [0.4,\\,0.71,\\,1.0,\\,8.9,\\,9.0]\\,$.\n    - **Case 2**: $\\,N=3\\,$ coordinates $\\,(0,0),\\,(0,0),\\,(3,4)\\,$ with thresholds $\\,\\epsilon \\in [0.0,\\,0.0001,\\,5.0]\\,$.\n    - **Case 3**: $\\,N=5\\,$ coordinates $\\,(0,0),\\,(2,0),\\,(4,0),\\,(6,0),\\,(8,0)\\,$ with thresholds $\\,\\epsilon \\in [1.0,\\,2.0,\\,2.1,\\,8.0]\\,$.\n    - **Case 4**: $\\,N=5\\,$ coordinates $\\,(0,0),\\,(0,1),\\,(1,0),\\,(1,1),\\,(4,4)\\,$ with thresholds $\\,\\epsilon \\in [0.9,\\,1.0,\\,1.5,\\,5.0]\\,$.\n- **Output Specification**: A single-line string representing a list of tuples. Each tuple corresponds to a test case and contains two lists: the list of computed $\\,K\\,$ values and the list of computed $\\,F\\,$ values. The specified format is: $\\big[\\,\\big([K_{1,1},\\ldots],\\,[F_{1,1},\\ldots]\\big),\\,\\ldots,\\,\\big([K_{4,1},\\ldots],\\,[F_{4,1},\\ldots]\\big)\\,\\big]$.\n\n#### Step 2: Validate Using Extracted Givens\n\nThe problem statement is evaluated against the validation criteria.\n\n- **Scientifically Grounded**: The problem is based on fundamental and standard concepts from graph theory (undirected graphs, adjacency matrices, connected components) and geometry (Euclidean distance). The application context, spatially resolved omics, is a valid and significant area of research in systems biology where such graph-based analyses are employed to model cell-cell interactions. The problem is free of any pseudoscience or factual inaccuracies.\n- **Well-Posed**: For any given set of $\\,N\\,$ points and a non-negative threshold $\\,\\epsilon\\,$, the $\\epsilon$-radius graph is uniquely defined. The number of connected components and the size of the largest component are well-defined and computable properties of any graph. The problem is structured to yield a unique solution for each test case.\n- **Objective**: The problem is stated using precise mathematical definitions and objective language, leaving no room for subjective interpretation.\n- **Completeness and Consistency**: The problem is self-contained. It provides all necessary definitions, data (coordinates and thresholds), and a clear specification for the output. The definition of the adjacency matrix, including the exclusion of self-loops ($\\,i \\ne j\\,$) and the non-strict inequality ($\\,d_{ij} \\le \\epsilon\\,$), is unambiguous and internally consistent.\n- **Feasibility**: The provided coordinates and thresholds are realistic. The computational task is feasible and can be solved with standard algorithms within a reasonable time.\n- **Structure**: The problem is well-structured. It requires the implementation of an algorithm from first principles, which is a clear directive. The test cases are thoughtfully designed to cover various scenarios, including clustered data, duplicate coordinates (which are treated as distinct nodes), linear structures, and grids with outliers.\n\n#### Step 3: Verdict and Action\n\nThe problem is **valid**. It is scientifically sound, well-posed, and complete. I will now proceed with the solution.\n\n### Principle-Based Solution Design\n\nThe core of the problem is to analyze the connectivity of a series of graphs generated from a fixed set of points by varying a distance threshold $\\,\\epsilon\\,$. The solution must be implemented from first principles, particularly the computation of connected components.\n\n1.  **Distance Calculation**: For a set of $\\,N\\,$ points, the first step is to compute the pairwise Euclidean distances for all pairs of points. To optimize the process, we can pre-compute a symmetric $\\,N \\times N\\,$ distance matrix, $\\,D\\,$, where $\\,D_{ij} = d_{ij}\\,$. This avoids redundant distance calculations for each threshold $\\,\\epsilon\\,$.\n\n2.  **Graph Construction**: For each given threshold $\\,\\epsilon_t\\,$, we construct the adjacency matrix $\\,A^{(\\epsilon_t)}\\,$. Based on the definition, an element $\\,A^{(\\epsilon_t)}_{ij}\\,$ is set to $\\,1\\,$ if and only if the corresponding distance $\\,D_{ij}\\,$ satisfies $\\,0 < D_{ij} \\le \\epsilon_t\\,$. The condition $\\,D_{ij} > 0\\,$ implicitly handles the $\\,i \\ne j\\,$ requirement, as $\\,D_{ii} = 0\\,$. This can be efficiently achieved by applying a thresholding operation to the pre-computed distance matrix.\n\n3.  **Connected Components Algorithm**: The problem requires finding the number of connected components, $\\,K(\\epsilon_t)\\,$, and the size of the largest one, $\\,L(\\epsilon_t)\\,$. This is a classic graph traversal problem. We will implement an algorithm from first principles, such as a Depth-First Search (DFS) or Breadth-First Search (BFS), to identify these components. The algorithm proceeds as follows:\n    a. Initialize a `visited` array of size $\\,N\\,$ to track visited nodes, all initially `False`.\n    b. Initialize an empty list, `component_sizes`, to store the size of each component found.\n    c. Iterate through each node $\\,i\\,$ from $\\,0\\,$ to $\\,N-1\\,$.\n    d. If node $\\,i\\,$ has not been visited, it signifies the start of a new, undiscovered component.\n        i.  Start a graph traversal (e.g., iterative DFS using a stack) from node $\\,i\\,$.\n        ii. Mark node $\\,i\\,$ as visited and push it onto the stack.\n        iii. Initialize a counter for the current component's size to $\\,0\\,$.\n        iv. While the stack is not empty, pop a node $\\,u\\,$, increment the size counter, and examine all its neighbors $\\,v\\,$ (i.e., where $\\,A^{(\\epsilon_t)}_{uv} = 1\\,$). For each unvisited neighbor $\\,v\\,$, mark it as visited and push it onto the stack.\n    e. Once the traversal from node $\\,i\\,$ is complete, the final value of the size counter is the size of that component. Append this size to the `component_sizes` list.\n    f. After iterating through all nodes, the number of connected components, $\\,K(\\epsilon_t)\\,$, is the total number of traversals initiated, which is simply the length of the `component_sizes` list.\n    g. The size of the largest connected component, $\\,L(\\epsilon_t)\\,$, is the maximum value in the `component_sizes` list.\n\n4.  **Metric Calculation and Formatting**: With $\\,K(\\epsilon_t)\\,$ and $\\,L(\\epsilon_t)\\,$ computed, the connectivity fraction $\\,F(\\epsilon_t) = L(\\epsilon_t) / N\\,$ is calculated and rounded to four decimal places as required. This process is repeated for all thresholds in the set $\\,\\{\\epsilon_t\\}\\,$.\n\n5.  **Output Generation**: The final step involves formatting the collected lists of $\\,K\\,$ and $\\,F\\,$ values for all test cases into the exact string format specified in the problem description, ensuring no extraneous characters or incorrect spacing.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef find_connected_components(adj_matrix: np.ndarray):\n    \"\"\"\n    Finds connected components in a graph from its adjacency matrix.\n    Implementation is from first principles using an iterative Depth-First Search (DFS).\n\n    Args:\n        adj_matrix: An N x N numpy array representing the graph's adjacency matrix.\n\n    Returns:\n        A tuple (num_components, largest_component_size).\n    \"\"\"\n    n = adj_matrix.shape[0]\n    if n == 0:\n        return 0, 0\n\n    visited = [False] * n\n    component_sizes = []\n\n    for i in range(n):\n        if not visited[i]:\n            # Found the start of a new component\n            current_component_size = 0\n            stack = [i]\n            visited[i] = True\n\n            while stack:\n                u = stack.pop()\n                current_component_size += 1\n                # Find neighbors of u where there is an edge (value is 1)\n                neighbors = np.where(adj_matrix[u] == 1)[0]\n                for v in neighbors:\n                    if not visited[v]:\n                        visited[v] = True\n                        stack.append(v)\n            \n            component_sizes.append(current_component_size)\n\n    num_components = len(component_sizes)\n    largest_component_size = max(component_sizes) if component_sizes else 0\n\n    return num_components, largest_component_size\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            # Test Case 1\n            [(0, 0), (1, 0), (0.5, 0.5), (10, 0), (11, 0), (10.5, -0.5)],\n            [0.4, 0.71, 1.0, 8.9, 9.0]\n        ),\n        (\n            # Test Case 2\n            [(0, 0), (0, 0), (3, 4)],\n            [0.0, 0.0001, 5.0]\n        ),\n        (\n            # Test Case 3\n            [(0, 0), (2, 0), (4, 0), (6, 0), (8, 0)],\n            [1.0, 2.0, 2.1, 8.0]\n        ),\n        (\n            # Test Case 4\n            [(0, 0), (0, 1), (1, 0), (1, 1), (4, 4)],\n            [0.9, 1.0, 1.5, 5.0]\n        )\n    ]\n\n    all_results = []\n    for coordinates_list, thresholds in test_cases:\n        coords = np.array(coordinates_list, dtype=float)\n        N = coords.shape[0]\n\n        # Use numpy broadcasting to efficiently compute the pairwise distance matrix.\n        # coords[:, np.newaxis, :] -> shape (N, 1, 2)\n        # coords[np.newaxis, :, :] -> shape (1, N, 2)\n        # Difference results in a shape (N, N, 2) array of coordinate differences.\n        dist_matrix = np.sqrt(np.sum((coords[:, np.newaxis, :] - coords[np.newaxis, :, :])**2, axis=-1))\n\n        k_results = []\n        f_results = []\n\n        for epsilon in thresholds:\n            # Construct adjacency matrix A_ij = 1 if d_ij = epsilon and i != j, else 0.\n            adj_matrix = (dist_matrix = epsilon).astype(int)\n            np.fill_diagonal(adj_matrix, 0)\n\n            K, L = find_connected_components(adj_matrix)\n            \n            # For N=0, F would be undefined. Problem constraints ensure N >= 3.\n            F = round(L / N, 4) if N > 0 else 0.0\n\n            k_results.append(K)\n            f_results.append(F)\n        \n        all_results.append((k_results, f_results))\n\n    # Construct the final output string exactly as specified.\n    # Manually build the string to control spacing and format.\n    case_strings = []\n    for k_list, f_list in all_results:\n        k_str = f\"[{','.join(map(str, k_list))}]\"\n        f_str = f\"[{','.join(map(str, f_list))}]\"\n        case_strings.append(f\"({k_str},{f_str})\")\n        \n    final_output_string = f\"[{','.join(case_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output_string)\n\nsolve()\n```", "id": "4315795"}, {"introduction": "After establishing spatial relationships, we can investigate whether the molecular features we measure exhibit spatial patterns, such as the clustering of cells with similar protein expression. This exercise guides you through calculating a key measure of spatial patterning, Moran's I, using protein intensity data from segmented cells [@problem_id:4315652]. This practice bridges the gap between raw imaging data and quantitative spatial statistics, a crucial workflow for identifying non-random spatial organization in tissues.", "problem": "You are given synthetic but scientifically plausible data representing a simplified spatially resolved proteomics scenario in systems biomedicine. The setting follows the fundamental base that the Central Dogma of Molecular Biology describes the flow from deoxyribonucleic acid (DNA) to ribonucleic acid (RNA) to protein, and the measurement of protein marker intensities in a tissue section can be treated as quantitative variables per segmented cell. A segmentation mask assigns each pixel to a cell identifier, enabling geometric inference of cell positions. The task is to compute summary statistics per cell type and to assess spatial autocorrelation of marker expression using a distance-based neighbor definition. The core quantities are the sample mean and unbiased sample variance of per-cell intensities per cell type, and a spatial autocorrelation measure computed over cells using inverse-distance weights within a finite radius.\n\nFundamental laws and core definitions that serve as the base of this problem are: the definition of expected value and variance for random variables, the construction of centroids for labeled regions by averaging pixel center coordinates, and the concept of spatial autocorrelation as the tendency for nearby observations to be similar. Measurements are assumed to be in arbitrary intensity units for the marker, while spatial distances use micrometers.\n\nFor each test case, the following data are provided:\n\n- A two-dimensional segmentation mask specifying cell identifiers for each pixel. The background is labeled with $0$.\n- A mapping from each cell identifier to a categorical cell type label.\n- A mapping from each cell identifier to a per-cell protein intensity for a single marker.\n- A pixel size $p$ in micrometers, specifying the physical size of one pixel edge.\n- A neighbor radius $r$ in micrometers, defining the spatial neighborhood: two distinct cells are considered neighbors if the Euclidean distance between their centroids is less than or equal to $r$. The spatial weight for a neighbor pair is defined as the inverse of their centroid distance, and zero otherwise.\n\nYour program must, for each test case:\n- Compute the centroid of each cell from the segmentation mask. The centroid of a labeled region is the arithmetic mean of the physical coordinates of the centers of the pixels belonging to that region, where the center of a pixel at row index $i$ and column index $j$ has coordinates $\\left((i + 0.5) \\cdot p, (j + 0.5) \\cdot p\\right)$ in micrometers.\n- For each cell type, compute the sample mean and unbiased sample variance (with divisor $k - 1$ for $k$ samples) of the per-cell intensities $x_c$ among the cells of that type. If a cell type has $k = 1$, report the variance as $0$.\n- Assess spatial autocorrelation of marker expression across all cells using a standard distance-based weighting scheme defined above. If there are fewer than $2$ cells, if the total weight is $0$, or if the sample variance of intensities across all cells is $0$, report the spatial autocorrelation value as $0$.\n\nExpress all distances in micrometers. The final outputs are dimensionless quantities. The angle unit does not apply.\n\nTest suite specification:\n- Test Case $1$ (happy path):\n  - Segmentation mask $M_1$ of size $5 \\times 5$:\n    - Row $0$: [$1$, $1$, $2$, $2$, $0$]\n    - Row $1$: [$1$, $1$, $2$, $2$, $0$]\n    - Row $2$: [$0$, $0$, $0$, $0$, $0$]\n    - Row $3$: [$3$, $3$, $4$, $4$, $0$]\n    - Row $4$: [$3$, $3$, $4$, $4$, $0$]\n  - Pixel size $p = 1$ micrometer.\n  - Neighbor radius $r = 3$ micrometers.\n  - Cell type mapping: $1 \\mapsto$ \"A\", $2 \\mapsto$ \"B\", $3 \\mapsto$ \"A\", $4 \\mapsto$ \"B\".\n  - Per-cell intensities: $x_1 = 1.0$, $x_2 = 2.5$, $x_3 = 1.5$, $x_4 = 3.0$.\n- Test Case $2$ (boundary condition with no neighbors):\n  - Segmentation mask $M_2 = M_1$.\n  - Pixel size $p = 1$ micrometer.\n  - Neighbor radius $r = 1$ micrometer.\n  - Cell type mapping: $1 \\mapsto$ \"A\", $2 \\mapsto$ \"B\", $3 \\mapsto$ \"A\", $4 \\mapsto$ \"B\".\n  - Per-cell intensities: $x_1 = 1.0$, $x_2 = 2.5$, $x_3 = 1.5$, $x_4 = 3.0$.\n- Test Case $3$ (edge case with zero variance):\n  - Segmentation mask $M_3 = M_1$.\n  - Pixel size $p = 1$ micrometer.\n  - Neighbor radius $r = 3$ micrometers.\n  - Cell type mapping: $1 \\mapsto$ \"A\", $2 \\mapsto$ \"B\", $3 \\mapsto$ \"A\", $4 \\mapsto$ \"B\".\n  - Per-cell intensities: $x_1 = 2.0$, $x_2 = 2.0$, $x_3 = 2.0$, $x_4 = 2.0$.\n\nOutput specification:\n- For each test case, return a list with three elements:\n  - The list of sample means per type in lexicographic order of type labels,\n  - The list of unbiased sample variances per type in the same order,\n  - The spatial autocorrelation value as a float.\n- Aggregate the results of all test cases into a single line containing a list of these per-case lists. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, \"[[means_case1,vars_case1,moran_case1],[means_case2,vars_case2,moran_case2],[means_case3,vars_case3,moran_case3]]\"). All numeric results must be floats or lists of floats. No other text should be printed.", "solution": "The supplied problem is scientifically grounded, well-posed, and complete. It presents a standard computational workflow in spatial bioinformatics, requiring the calculation of cell centroids from a segmentation mask, the computation of summary statistics for different cell populations, and the assessment of spatial patterns using a defined autocorrelation index. The problem is valid and can be solved by implementing the specified calculations.\n\nThe solution process is divided into three main parts:\n1.  Calculation of cell centroids from the segmentation mask.\n2.  Computation of per-cell-type summary statistics (mean and variance).\n3.  Calculation of the spatial autocorrelation of marker intensity across all cells.\n\nEach step will be described mathematically, followed by a worked-through example using the data from Test Case $1$.\n\n### 1. Cell Centroid Calculation\n\nA cell's geometric centroid is the arithmetic mean of the physical coordinates of the centers of all pixels belonging to that cell. Given a pixel at matrix index $(i, j)$ (row, column) and a pixel size of $p$ micrometers, the physical coordinate of the pixel's center is given by:\n$$ (y, z) = ((i + 0.5) \\cdot p, (j + 0.5) \\cdot p) $$\nFor a cell $c$ composed of a set of $N_c$ pixels with indices $S_c = \\{(i_1, j_1), (i_2, j_2), \\dots, (i_{N_c}, j_{N_c})\\}$, its centroid $(\\bar{y}_c, \\bar{z}_c)$ is calculated as:\n$$ \\bar{y}_c = \\frac{p}{N_c} \\sum_{k=1}^{N_c} (i_k + 0.5) $$\n$$ \\bar{z}_c = \\frac{p}{N_c} \\sum_{k=1}^{N_c} (j_k + 0.5) $$\n\n**Example (Test Case 1):**\n- Pixel size $p = 1\n\\mu m$.\n- Cell $1$ consists of pixels at indices $(0,0), (0,1), (1,0), (1,1)$. There are $N_1 = 4$ pixels.\n- The centroid is:\n  $$ \\bar{y}_1 = \\frac{1}{4} ((0+0.5) + (0+0.5) + (1+0.5) + (1+0.5)) = \\frac{0.5+0.5+1.5+1.5}{4} = 1.0 \\, \\mu m $$\n  $$ \\bar{z}_1 = \\frac{1}{4} ((0+0.5) + (1+0.5) + (0+0.5) + (1+0.5)) = \\frac{0.5+1.5+0.5+1.5}{4} = 1.0 \\, \\mu m $$\n  So, the centroid of cell $1$ is $C_1 = (1.0, 1.0)$.\n- Similarly, we find the centroids for the other cells:\n  - Cell $2$ (pixels $(0,2), (0,3), (1,2), (1,3)$): $C_2 = (1.0, 3.0)$.\n  - Cell $3$ (pixels $(3,0), (3,1), (4,0), (4,1)$): $C_3 = (4.0, 1.0)$.\n  - Cell $4$ (pixels $(3,2), (3,3), (4,2), (4,3)$): $C_4 = (4.0, 3.0)$.\n\n### 2. Per-Type Summary Statistics\n\nFor each cell type, we collect the intensities of all cells belonging to that type and compute the sample mean and unbiased sample variance.\n\n- The **sample mean** for a cell type with $k$ cells having intensities $\\{x_1, x_2, \\dots, x_k\\}$ is:\n  $$ \\bar{x}_{\\text{type}} = \\frac{1}{k} \\sum_{i=1}^k x_i $$\n- The **unbiased sample variance** is:\n  $$ s^2_{\\text{type}} = \\frac{1}{k-1} \\sum_{i=1}^k (x_i - \\bar{x}_{\\text{type}})^2 $$\n  If $k=1$, the variance is defined as $0$.\n\n**Example (Test Case 1):**\n- Cell types are sorted lexicographically: \"A\", \"B\".\n- **Type \"A\"**: Cells $1$ and $3$, with intensities $x_1=1.0, x_3=1.5$. Here $k=2$.\n  - Mean: $\\bar{x}_A = \\frac{1.0 + 1.5}{2} = 1.25$.\n  - Variance: $s^2_A = \\frac{(1.0 - 1.25)^2 + (1.5 - 1.25)^2}{2-1} = (-0.25)^2 + (0.25)^2 = 0.0625 + 0.0625 = 0.125$.\n- **Type \"B\"**: Cells $2$ and $4$, with intensities $x_2=2.5, x_4=3.0$. Here $k=2$.\n  - Mean: $\\bar{x}_B = \\frac{2.5 + 3.0}{2} = 2.75$.\n  - Variance: $s^2_B = \\frac{(2.5 - 2.75)^2 + (3.0 - 2.75)^2}{2-1} = (-0.25)^2 + (0.25)^2 = 0.0625 + 0.0625 = 0.125$.\n- The results are `means = [1.25, 2.75]` and `variances = [0.125, 0.125]`.\n\n### 3. Spatial Autocorrelation (Moran's I)\n\nThe problem requires assessing spatial autocorrelation using a standard distance-based weighting scheme. This corresponds to the calculation of Moran's I index, defined as:\n$$ I = \\frac{N}{\\sum_{i=1}^N \\sum_{j=1}^N w_{ij}} \\frac{\\sum_{i=1}^N \\sum_{j=1}^N w_{ij} (x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_{i=1}^N (x_i - \\bar{x})^2} $$\nwhere:\n- $N$ is the total number of cells.\n- $x_i$ is the intensity of cell $i$.\n- $\\bar{x}$ is the mean intensity over all cells.\n- $w_{ij}$ is the spatial weight between cell $i$ and cell $j$.\nThe weight $w_{ij}$ is defined as the inverse of the Euclidean distance between the centroids of cells $i$ and $j$ if this distance is less than or equal to a given radius $r$, and $0$ otherwise. $w_{ii} = 0$.\n\n$$ w_{ij} = \\begin{cases} 1/d_{ij}  \\text{if } 0  d_{ij} \\le r \\\\ 0  \\text{otherwise} \\end{cases} $$\nwhere $d_{ij} = \\sqrt{(\\bar{y}_i - \\bar{y}_j)^2 + (\\bar{z}_i - \\bar{z}_j)^2}$.\n\nThe value is reported as $0$ if $N2$, the total weight $W=\\sum_i\\sum_j w_{ij}$ is $0$, or the total variance (proportional to $\\sum_i (x_i - \\bar{x})^2$) is $0$.\n\n**Example (Test Case 1):**\n- Number of cells $N=4$. Intensities $\\{1.0, 2.5, 1.5, 3.0\\}$.\n- Global mean intensity: $\\bar{x} = \\frac{1.0+2.5+1.5+3.0}{4} = 2.0$.\n- Deviations from mean $(x_i - \\bar{x})$: $\\{-1.0, 0.5, -0.5, 1.0\\}$.\n- Sum of squared deviations: $\\sum_i (x_i - \\bar{x})^2 = (-1.0)^2 + (0.5)^2 + (-0.5)^2 + (1.0)^2 = 2.5$.\n- Neighbor radius $r = 3.0 \\, \\mu m$. Centroids: $C_1(1,1), C_2(1,3), C_3(4,1), C_4(4,3)$.\n- Distances and weights $(w_{ij} = w_{ji})$:\n  - $d_{12} = \\sqrt{(1-1)^2 + (1-3)^2} = 2.0 \\le 3.0 \\implies w_{12} = 1/2.0 = 0.5$.\n  - $d_{13} = \\sqrt{(1-4)^2 + (1-1)^2} = 3.0 \\le 3.0 \\implies w_{13} = 1/3.0$.\n  - $d_{14} = \\sqrt{(1-4)^2 + (1-3)^2} = \\sqrt{13} \\approx 3.61  3.0 \\implies w_{14} = 0$.\n  - $d_{24} = \\sqrt{(1-4)^2 + (3-3)^2} = 3.0 \\le 3.0 \\implies w_{24} = 1/3.0$.\n  - $d_{34} = \\sqrt{(4-4)^2 + (1-3)^2} = 2.0 \\le 3.0 \\implies w_{34} = 1/2.0$.\n  - $d_{23} = \\sqrt{(1-4)^2 + (3-1)^2} = \\sqrt{13} \\approx 3.61  3.0 \\implies w_{23} = 0$.\n- Total weight $W = \\sum_i \\sum_j w_{ij} = 2 \\cdot (w_{12} + w_{13} + w_{24} + w_{34}) = 2 \\cdot (0.5 + 1/3 + 1/3 + 0.5) = 2 \\cdot (1 + 2/3) = 10/3$.\n- Numerator of Moran's I: $\\sum_i \\sum_j w_{ij}(x_i - \\bar{x})(x_j - \\bar{x}) = 2 \\cdot [w_{12}(-1.0)(0.5) + w_{13}(-1.0)(-0.5) + w_{24}(0.5)(1.0) + w_{34}(-0.5)(1.0)] = 2 \\cdot [0.5(-0.5) + (1/3)(0.5) + (1/3)(0.5) + 0.5(-0.5)] = 2 \\cdot [-0.25 + 1/6 + 1/6 - 0.25] = 2 \\cdot [-0.5 + 1/3] = 2 \\cdot [-1/6] = -1/3$.\n- Moran's I: $I = \\frac{4}{10/3} \\frac{-1/3}{2.5} = \\frac{12}{10} \\frac{-1}{7.5} = 1.2 \\cdot (-0.1333...) = -0.16$.\n\nThe final result for Test Case $1$ is `[[1.25, 2.75], [0.125, 0.125], -0.16]`.\n\n- **Test Case 2** is identical except $r=1.0$. All inter-cell distances are greater than $1.0$, so all weights $w_{ij}$ are $0$. Total weight $W=0$, so Moran's I is $0.0$ by definition.\n- **Test Case 3** is identical to Case 1 except all intensities are $2.0$. The global mean is $2.0$, so all deviations $(x_i - \\bar{x})$ are $0$. The total variance is $0$, so Moran's I is $0.0$ by definition. The per-type variances are also $0.0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import euclidean\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and generate the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"mask\": np.array([\n                [1, 1, 2, 2, 0],\n                [1, 1, 2, 2, 0],\n                [0, 0, 0, 0, 0],\n                [3, 3, 4, 4, 0],\n                [3, 3, 4, 4, 0]\n            ]),\n            \"pixel_size\": 1.0,\n            \"neighbor_radius\": 3.0,\n            \"type_map\": {1: \"A\", 2: \"B\", 3: \"A\", 4: \"B\"},\n            \"intensity_map\": {1: 1.0, 2: 2.5, 3: 1.5, 4: 3.0}\n        },\n        {\n            \"mask\": np.array([\n                [1, 1, 2, 2, 0],\n                [1, 1, 2, 2, 0],\n                [0, 0, 0, 0, 0],\n                [3, 3, 4, 4, 0],\n                [3, 3, 4, 4, 0]\n            ]),\n            \"pixel_size\": 1.0,\n            \"neighbor_radius\": 1.0,\n            \"type_map\": {1: \"A\", 2: \"B\", 3: \"A\", 4: \"B\"},\n            \"intensity_map\": {1: 1.0, 2: 2.5, 3: 1.5, 4: 3.0}\n        },\n        {\n            \"mask\": np.array([\n                [1, 1, 2, 2, 0],\n                [1, 1, 2, 2, 0],\n                [0, 0, 0, 0, 0],\n                [3, 3, 4, 4, 0],\n                [3, 3, 4, 4, 0]\n            ]),\n            \"pixel_size\": 1.0,\n            \"neighbor_radius\": 3.0,\n            \"type_map\": {1: \"A\", 2: \"B\", 3: \"A\", 4: \"B\"},\n            \"intensity_map\": {1: 2.0, 2: 2.0, 3: 2.0, 4: 2.0}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(\n            case[\"mask\"],\n            case[\"pixel_size\"],\n            case[\"neighbor_radius\"],\n            case[\"type_map\"],\n            case[\"intensity_map\"]\n        )\n        results.append(result)\n\n    # Format output according to specification\n    # str() on a list correctly formats it with brackets, e.g., '[1.0, 2.0]'\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef process_case(segmentation_mask, pixel_size, neighbor_radius, type_map, intensity_map):\n    \"\"\"\n    Processes a single test case to compute statistics and spatial autocorrelation.\n    \"\"\"\n    # Step 1: Collect cell data and compute centroids\n    cell_pixels = {}\n    rows, cols = segmentation_mask.shape\n    for i in range(rows):\n        for j in range(cols):\n            cell_id = segmentation_mask[i, j]\n            if cell_id > 0:\n                if cell_id not in cell_pixels:\n                    cell_pixels[cell_id] = []\n                cell_pixels[cell_id].append((i, j))\n\n    cell_ids = sorted(cell_pixels.keys())\n    centroids = {}\n    for cell_id in cell_ids:\n        pixel_coords = np.array(cell_pixels[cell_id])\n        # Centroid in pixel index space\n        mean_pixel_idx = np.mean(pixel_coords, axis=0)\n        # Convert to physical coordinates\n        centroid_phys = (mean_pixel_idx + 0.5) * pixel_size\n        centroids[cell_id] = centroid_phys\n    \n    # Step 2: Compute per-type statistics\n    type_data = {}\n    for cell_id in cell_ids:\n        cell_type = type_map[cell_id]\n        if cell_type not in type_data:\n            type_data[cell_type] = []\n        type_data[cell_type].append(intensity_map[cell_id])\n\n    sorted_types = sorted(type_data.keys())\n    type_means = []\n    type_vars = []\n    for cell_type in sorted_types:\n        intensities = np.array(type_data[cell_type], dtype=float)\n        type_means.append(np.mean(intensities))\n        if len(intensities) > 1:\n            # ddof=1 for unbiased sample variance\n            type_vars.append(np.var(intensities, ddof=1))\n        else:\n            type_vars.append(0.0)\n\n    # Step 3: Compute spatial autocorrelation (Moran's I)\n    num_cells = len(cell_ids)\n    if num_cells  2:\n        moran_i = 0.0\n    else:\n        intensities_all = np.array([intensity_map[cid] for cid in cell_ids], dtype=float)\n        mean_intensity_all = np.mean(intensities_all)\n        deviations = intensities_all - mean_intensity_all\n        sum_sq_dev = np.sum(deviations**2)\n\n        if sum_sq_dev == 0:\n            moran_i = 0.0\n        else:\n            total_weight = 0.0\n            moran_numerator = 0.0\n\n            for i in range(num_cells):\n                for j in range(num_cells):\n                    if i == j:\n                        continue\n                    \n                    id_i = cell_ids[i]\n                    id_j = cell_ids[j]\n                    \n                    dist = euclidean(centroids[id_i], centroids[id_j])\n                    \n                    weight = 0.0\n                    if dist = neighbor_radius and dist > 0: # dist > 0 is implicit with i!=j\n                        # Inverse distance weight\n                        weight = 1.0 / dist\n                    \n                    total_weight += weight\n                    moran_numerator += weight * deviations[i] * deviations[j]\n\n            if total_weight == 0:\n                moran_i = 0.0\n            else:\n                moran_i = (num_cells / total_weight) * (moran_numerator / sum_sq_dev)\n\n    return [type_means, type_vars, moran_i]\n\nsolve()\n```", "id": "4315652"}, {"introduction": "A primary goal in spatial omics is to identify distinct functional regions or \"domains\" within a tissue, which are often not known beforehand. This problem introduces a powerful method for spatial segmentation based on a Markov Random Field (MRF) [@problem_id:4315740]. You will implement a model that finds the optimal domain label for each spot by minimizing an energy function that balances data fidelity with spatial smoothness, understanding the critical role of the regularization parameter $\\beta$ in controlling the trade-off between data and spatial context.", "problem": "You are given a spatial segmentation task for a tissue represented by a rectangular grid graph with nodes corresponding to measured locations (spots) and labels corresponding to putative tissue domains. The measurement at each spot is a two-dimensional vector representing two marker gene abundances. The segmentation is defined as a labeling vector $z \\in \\{0,1,2\\}^N$ that minimizes the energy\n$$\nE(z;\\beta) \\;=\\; \\sum_{i=1}^{N} \\ell_i(z_i) \\;+\\; \\beta \\sum_{(i,j)\\in \\mathcal{E}} \\mathbb{1}[z_i \\neq z_j],\n$$\nwhere $N$ is the number of spots, $\\mathcal{E}$ is the set of undirected edges of the $4$-neighborhood grid, $\\beta \\ge 0$ is a scalar regularization parameter that weights the spatial smoothness, and $\\mathbb{1}[\\cdot]$ is the indicator function. The term $\\ell_i(k)$ is the negative log-likelihood of observing the data at spot $i$ if it belonged to label $k$ under an isotropic Gaussian observation model with known class prototypes. Specifically, assume\n$$\n\\ell_i(k) \\;=\\; \\frac{1}{2 \\sigma^2} \\,\\| y_i - \\mu_k \\|_2^2,\n$$\nwhere $y_i \\in \\mathbb{R}^2$ is the observed expression vector at spot $i$, $\\mu_k \\in \\mathbb{R}^2$ is the prototype (class mean) for label $k$, and $\\sigma^2  0$ is the known variance. Throughout this problem, use $\\sigma^2 = 1$ (dimensionless units).\n\nUse the following fixed data:\n- Grid size: $3 \\times 3$ so that $N = 9$. Use row-major indexing: for row $r \\in \\{0,1,2\\}$ and column $c \\in \\{0,1,2\\}$, the node index is $i = 3r + c \\in \\{0,1,\\dots,8\\}$.\n- Adjacency $\\mathcal{E}$: Undirected $4$-neighborhood on the grid (horizontal and vertical neighbors only).\n- Class prototypes (means): $\\mu_0 = (0,0)$, $\\mu_1 = (3,0)$, $\\mu_2 = (0,3)$.\n- Observations $y_i$ for $i=0,\\dots,8$ in row-major order:\n  - $y_0 = (0.2,-0.1)$, $y_1 = (1.8,0.2)$, $y_2 = (3.1,-0.2)$,\n  - $y_3 = (0.1,0.2)$, $y_4 = (1.8,1.5)$, $y_5 = (2.7,0.4)$,\n  - $y_6 = (0.1,3.1)$, $y_7 = (0.2,2.1)$, $y_8 = (-0.1,2.9)$.\n\nYour task is to write a complete, runnable program that, for each $\\beta$ in the test suite specified below, computes a globally optimal labeling\n$$\nz^\\star(\\beta) \\in \\arg\\min_{z \\in \\{0,1,2\\}^9} E(z;\\beta),\n$$\nand the corresponding minimized energy $E^\\star(\\beta) = E(z^\\star(\\beta);\\beta)$, together with the number of disagreeing neighbor pairs\n$$\nM^\\star(\\beta) \\;=\\; \\sum_{(i,j)\\in \\mathcal{E}} \\mathbb{1}\\big[z^\\star(\\beta)_i \\neq z^\\star(\\beta)_j\\big].\n$$\nIf there are multiple minimizers, return the lexicographically smallest labeling with respect to the index order $i=0,1,\\dots,8$. Lexicographic order here means that $z \\prec \\tilde{z}$ if there exists the smallest index $i$ such that $z_i \\neq \\tilde{z}_i$ and $z_i  \\tilde{z}_i$.\n\nIn addition to reporting $E^\\star(\\beta)$ and $M^\\star(\\beta)$, assess sensitivity to $\\beta$ by comparing optimal labelings across consecutive $\\beta$ values using the Hamming distance. For two labelings $z, \\tilde{z} \\in \\{0,1,2\\}^9$, define the Hamming distance\n$$\nH(z,\\tilde{z}) \\;=\\; \\sum_{i=0}^{8} \\mathbb{1}[z_i \\neq \\tilde{z}_i].\n$$\n\nTest suite of regularization parameters:\n- $\\beta \\in \\{0.0, 0.4, 1.2, 10.0\\}$.\n\nProgram requirements:\n- For each $\\beta$ in the test suite, compute $E^\\star(\\beta)$ and $M^\\star(\\beta)$ exactly by solving the discrete minimization problem over the $3 \\times 3$ grid.\n- Use the specified data and definitions without approximation.\n- In case of ties, enforce the lexicographic tie-breaking rule as defined above.\n- Compute the Hamming distances between the optimal labelings for consecutive $\\beta$ values in the test suite, that is $H\\big(z^\\star(0.0), z^\\star(0.4)\\big)$, $H\\big(z^\\star(0.4), z^\\star(1.2)\\big)$, and $H\\big(z^\\star(1.2), z^\\star(10.0)\\big)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- The list must contain, in order:\n  1. The four minimized energies $E^\\star(\\beta)$ for $\\beta = 0.0, 0.4, 1.2, 10.0$, each rounded to three decimal places and represented as decimal numbers.\n  2. The four integers $M^\\star(\\beta)$ for the same $\\beta$ values in the same order.\n  3. The three integers for the Hamming distances between consecutive optimal labelings as described above.\n- For example, the output format should look like $[e_1,e_2,e_3,e_4,m_1,m_2,m_3,m_4,h_{12},h_{23},h_{34}]$ where $e_i$ are floats rounded to three decimals and $m_i,h_{ij}$ are integers.", "solution": "The user wants me to solve a discrete optimization problem to perform spatial segmentation on a $3 \\times 3$ grid.\n\n### Problem Validation\n\nI will first validate the problem statement according to the specified criteria.\n\n**Step 1: Extract Givens**\n\n- **Energy Function:** The objective is to find a labeling vector $z \\in \\{0,1,2\\}^N$ that minimizes the energy\n$$\nE(z;\\beta) = \\sum_{i=1}^{N} \\ell_i(z_i) + \\beta \\sum_{(i,j)\\in \\mathcal{E}} \\mathbb{1}[z_i \\neq z_j]\n$$\n- **Grid and Labels:** The grid is $3 \\times 3$, so the number of spots is $N = 9$. The labels $z_i$ are in the set $\\{0, 1, 2\\}$.\n- **Indexing:** Row-major indexing is used: for row $r \\in \\{0,1,2\\}$ and column $c \\in \\{0,1,2\\}$, the node index is $i = 3r + c$.\n- **Neighborhood:** $\\mathcal{E}$ is the set of undirected edges for a $4$-neighborhood grid (horizontal and vertical neighbors).\n- **Regularization:** $\\beta \\ge 0$ is a scalar regularization parameter.\n- **Unary Term (Data Cost):** The negative log-likelihood is $\\ell_i(k) = \\frac{1}{2 \\sigma^2} \\| y_i - \\mu_k \\|_2^2$, based on an isotropic Gaussian model.\n- **Model Parameters:**\n    - Variance: $\\sigma^2 = 1$ (dimensionless).\n    - Class prototypes: $\\mu_0 = (0,0)$, $\\mu_1 = (3,0)$, $\\mu_2 = (0,3)$.\n- **Observed Data ($y_i$):**\n    - $y_0 = (0.2,-0.1)$, $y_1 = (1.8,0.2)$, $y_2 = (3.1,-0.2)$\n    - $y_3 = (0.1,0.2)$, $y_4 = (1.8,1.5)$, $y_5 = (2.7,0.4)$\n    - $y_6 = (0.1,3.1)$, $y_7 = (0.2,2.1)$, $y_8 = (-0.1,2.9)$\n- **Task:**\n    - Find the globally optimal labeling $z^\\star(\\beta)$ for each given $\\beta$.\n    - In case of multiple minimizers, select the lexicographically smallest labeling.\n    - Compute the minimized energy $E^\\star(\\beta) = E(z^\\star(\\beta);\\beta)$.\n    - Compute the number of disagreeing neighbors $M^\\star(\\beta) = \\sum_{(i,j)\\in \\mathcal{E}} \\mathbb{1}[z^\\star(\\beta)_i \\neq z^\\star(\\beta)_j]$.\n    - Compute the Hamming distance $H(z,\\tilde{z}) = \\sum_{i=0}^{8} \\mathbb{1}[z_i \\neq \\tilde{z}_i]$ between optimal labelings for consecutive $\\beta$ values.\n- **Test Suite:** $\\beta \\in \\{0.0, 0.4, 1.2, 10.0\\}$.\n- **Output Format:** A single comma-separated list `[e1,e2,e3,e4,m1,m2,m3,m4,h12,h23,h34]` where `e` values are energies rounded to three decimal places.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded:** The problem describes energy minimization for a Markov Random Field (MRF), specifically a Potts model, which is a standard and fundamental technique in image processing, computer vision, and spatial data analysis. The application to spatial 'omics' data is a relevant and modern use case in systems biomedicine. The model is scientifically sound.\n- **Well-Posed:** The problem is to find the minimum of a function over a finite set of states. The set of all possible labelings $z$ has size $3^9 = 19,683$. Since the domain is finite, a minimum energy value is guaranteed to exist. The specified lexicographical tie-breaking rule ensures that a unique optimal labeling $z^\\star(\\beta)$ is selected. Thus, the problem is well-posed.\n- **Objective:** The problem is stated with precise mathematical definitions and objective numerical data. There is no ambiguity in the terms used.\n\n**Flaw Checklist:**\n1.  **Scientific or Factual Unsoundness:** None. The model is standard.\n2.  **Non-Formalizable or Irrelevant:** None. The problem is fully formalized and relevant to the specified domain.\n3.  **Incomplete or Contradictory Setup:** None. All necessary information (grid structure, model parameters, data) is provided.\n4.  **Unrealistic or Infeasible:** None. The chosen grid size ($3 \\times 3$) makes the problem computationally feasible via exhaustive search. The data values are reasonable.\n5.  **Ill-Posed or Poorly Structured:** None. A unique solution is guaranteed.\n6.  **Pseudo-Profound, Trivial, or Tautological:** None. While solvable by brute force, the problem requires careful implementation of the model and search procedure, and it illustrates the fundamental trade-off between a data-fidelity term and a spatial-smoothness prior, which is a core concept.\n7.  **Outside Scientific Verifiability:** None. The results are deterministic and can be verified by an independent implementation.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. I will proceed with providing a solution.\n\n### Principle-Based Solution Design\n\nThe problem requires finding the labeling vector $z \\in \\{0, 1, 2\\}^9$ that globally minimizes the energy function $E(z; \\beta)$. This is a discrete optimization problem. Given the small size of the configuration space ($3^9 = 19,683$ possible labelings), a global minimum can be found reliably and exactly using an exhaustive search (brute-force) algorithm.\n\nThe algorithm proceeds as follows:\n\n1.  **Pre-computation of the Data Term:** The energy function $E(z; \\beta)$ consists of a data term $\\sum_{i} \\ell_i(z_i)$ and a smoothness term $\\beta \\sum_{(i,j)} \\mathbb{1}[z_i \\neq z_j]$. The data term $\\ell_i(k)$ for each spot $i$ and each possible label $k \\in \\{0, 1, 2\\}$ is independent of the other spots' labels and the parameter $\\beta$. We can pre-compute these values and store them in a $9 \\times 3$ matrix $L$, where $L_{ik} = \\ell_i(k)$. With $\\sigma^2=1$, the formula is:\n    $$\n    L_{ik} = \\ell_i(k) = \\frac{1}{2} \\|y_i - \\mu_k\\|_2^2\n    $$\n    The total data term for a given labeling $z = (z_0, \\dots, z_8)$ is then $\\sum_{i=0}^{8} L_{i, z_i}$.\n\n2.  **Neighborhood Definition:** The $4$-neighborhood structure on the $3 \\times 3$ grid is fixed. We define the set of edges $\\mathcal{E}$ connecting adjacent spots (horizontally and vertically). There are $12$ such unique edges.\n\n3.  **Exhaustive Search for Each $\\beta$:** For each value of $\\beta$ in the test suite $\\{0.0, 0.4, 1.2, 10.0\\}$, we perform a search over all $3^9$ possible labelings.\n    - We will iterate through every possible labeling $z$. The `itertools.product` function in Python is suitable for this, as it generates all combinations in lexicographical order.\n    - For each labeling $z$, we calculate its total energy:\n        a. The data term is computed by summing the pre-calculated values: $D(z) = \\sum_{i=0}^{8} L_{i, z_i}$.\n        b. The smoothness term's disagreement count is calculated: $M(z) = \\sum_{(i,j) \\in \\mathcal{E}} \\mathbb{1}[z_i \\neq z_j]$.\n        c. The total energy is $E(z;\\beta) = D(z) + \\beta \\cdot M(z)$.\n    - We maintain a running minimum energy, `min_energy`, and the corresponding optimal labeling, `best_z`.\n    - When a labeling $z$ with energy $E(z;\\beta)$ is evaluated, we compare it to `min_energy`. If $E(z;\\beta)  \\text{min\\_energy}$, we update `min_energy` to $E(z;\\beta)$ and `best_z` to $z$.\n    - **Tie-Breaking:** The problem specifies that in case of a tie in energy, the lexicographically smallest labeling should be chosen. By iterating through the labelings in lexicographical order (as `itertools.product` does) and only updating `best_z` on a *strict* inequality (``), we automatically satisfy this tie-breaking rule. The first minimizer found will be the lexicographically smallest.\n\n4.  **Calculation of Outputs:**\n    - After the exhaustive search for a given $\\beta$ is complete, the final `min_energy` is $E^\\star(\\beta)$ and the final `best_z` is $z^\\star(\\beta)$.\n    - We then compute $M^\\star(\\beta)$ by counting the disagreements in the final $z^\\star(\\beta)$.\n    - We store each $z^\\star(\\beta)$ to compute Hamming distances later.\n\n5.  **Sensitivity Analysis:** After computing the optimal labelings $z^\\star(\\beta)$ for all $\\beta$ values, we calculate the Hamming distance between the solutions for consecutive $\\beta$ values in the test suite: $H(z^\\star(0.0), z^\\star(0.4))$, $H(z^\\star(0.4), z^\\star(1.2))$, and $H(z^\\star(1.2), z^\\star(10.0))$. The Hamming distance $H(z, \\tilde{z})$ is the count of positions where the labels differ.\n\n6.  **Final Formatting:** The computed values—four energies $E^\\star(\\beta)$, four disagreement counts $M^\\star(\\beta)$, and three Hamming distances—are collected into a single list. The energies are rounded to three decimal places, and all values are formatted into the required comma-separated string.\n\nThis brute-force approach guarantees that the found solution is the global minimum, satisfying all problem constraints.", "answer": "```python\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Solves the spatial segmentation problem by finding the optimal labeling that minimizes the energy function\n    for a given set of regularization parameters (beta).\n    \"\"\"\n\n    # 1. Define problem data and constants from the problem statement.\n    y_obs = np.array([\n        [0.2, -0.1], [1.8, 0.2], [3.1, -0.2],\n        [0.1, 0.2], [1.8, 1.5], [2.7, 0.4],\n        [0.1, 3.1], [0.2, 2.1], [-0.1, 2.9]\n    ])\n    mu_prototypes = np.array([[0.0, 0.0], [3.0, 0.0], [0.0, 3.0]])\n    beta_values = [0.0, 0.4, 1.2, 10.0]\n    sigma_sq = 1.0\n    N = 9  # Number of spots (3x3 grid)\n    K = 3  # Number of labels {0, 1, 2}\n\n    # 2. Define grid structure: edges for a 4-neighborhood on a 3x3 grid\n    # using 0-indexed row-major nodes 0..8.\n    # Grid: 0 1 2\n    #       3 4 5\n    #       6 7 8\n    edges = [\n        (0, 1), (1, 2),  # Horizontal edges\n        (3, 4), (4, 5),\n        (6, 7), (7, 8),\n        (0, 3), (1, 4), (2, 5),  # Vertical edges\n        (3, 6), (4, 7), (5, 8)\n    ]\n\n    # 3. Pre-compute the data term matrix L, where L[i,k] = l_i(k).\n    # l_i(k) = (1 / (2*sigma^2)) * ||y_i - mu_k||_2^2\n    data_cost_matrix = np.zeros((N, K))\n    for i in range(N):\n        for k in range(K):\n            dist_sq = np.sum((y_obs[i] - mu_prototypes[k])**2)\n            data_cost_matrix[i, k] = 0.5 / sigma_sq * dist_sq\n\n    # 4. Initialize lists to store results for each beta.\n    optimal_labelings = []\n    e_stars = []\n    m_stars = []\n\n    # 5. Iterate over beta values and find the optimal labeling for each.\n    for beta in beta_values:\n        min_energy = float('inf')\n        best_z = None\n\n        # Iterate through all 3^9 = 19,683 possible labelings.\n        # itertools.product generates tuples in lexicographical order.\n        # This handles the tie-breaking rule if we update only on strictly smaller energy.\n        for z in itertools.product(range(K), repeat=N):\n            # Calculate data term using the pre-computed matrix.\n            data_energy = np.sum(data_cost_matrix[np.arange(N), z])\n\n            # Calculate the number of disagreeing neighbors.\n            disagreements = 0\n            for i, j in edges:\n                if z[i] != z[j]:\n                    disagreements += 1\n\n            total_energy = data_energy + beta * disagreements\n\n            if total_energy  min_energy:\n                min_energy = total_energy\n                best_z = z\n\n        # Store results for this beta value.\n        e_stars.append(min_energy)\n        optimal_labelings.append(np.array(best_z))\n        \n        # Calculate M_star for the found optimal labeling.\n        m_star_val = 0\n        for i, j in edges:\n            if best_z[i] != best_z[j]:\n                m_star_val += 1\n        m_stars.append(m_star_val)\n\n    # 6. Calculate Hamming distances between consecutive optimal labelings.\n    h_distances = []\n    for i in range(len(optimal_labelings) - 1):\n        z1 = optimal_labelings[i]\n        z2 = optimal_labelings[i+1]\n        h_dist = np.sum(z1 != z2)\n        h_distances.append(int(h_dist))\n\n    # 7. Format and print the final output as a single line.\n    # Energies must be rounded to three decimal places.\n    formatted_energies = [f\"{e:.3f}\" for e in e_stars]\n    \n    # Combine all results into a single list in the specified order.\n    # [E_vals, M_vals, H_vals]\n    final_results = formatted_energies + m_stars + h_distances\n    \n    # Print in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\n# Execute the solver function.\nsolve()\n```", "id": "4315740"}]}