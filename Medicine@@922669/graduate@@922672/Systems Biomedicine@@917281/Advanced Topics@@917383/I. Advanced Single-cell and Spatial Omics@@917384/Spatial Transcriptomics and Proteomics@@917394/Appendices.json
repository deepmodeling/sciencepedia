{"hands_on_practices": [{"introduction": "A crucial first step in many spatial omics workflows is the accurate alignment of data from different experimental modalities, such as gene expression maps and histology images. This practice guides you through the process of registering two coordinate systems using fiducial markers common to both [@problem_id:5062750]. By deriving and applying the principles of least squares optimization to find an optimal affine transformation, you will gain hands-on experience with a fundamental technique essential for creating an integrated view of tissue biology.", "problem": "A common step in spatial transcriptomics and proteomics pipelines in translational medicine is registering complementary modalities, such as Slide-seq bead maps to hematoxylin and eosin histology images, to enable joint inference of spatial gene and protein expression. Consider an experiment in which fluorescent fiducial markers are visible in both modalities. Let the bead map coordinates be denoted by $\\mathbf{x}_i = (x_i, y_i)$ and the corresponding histology image coordinates by $\\mathbf{u}_i = (u_i, v_i)$ for $i = 1, \\dots, n$. Assume the mapping from the bead space to the histology space is well-approximated by a two-dimensional affine transformation, defined by a $2 \\times 2$ matrix $\\mathbf{A}$ and a translation vector $\\mathbf{t}$, so that $\\mathbf{u}_i \\approx \\mathbf{A}\\mathbf{x}_i + \\mathbf{t}$, where $\\mathbf{A}$ models rotation, anisotropic scaling, and shear, and $\\mathbf{t}$ models translation. Both coordinate systems are measured in micrometers.\n\nStarting from the definition of the least squares objective,\n$$\nJ(\\mathbf{A}, \\mathbf{t}) = \\sum_{i=1}^{n} \\left\\| \\mathbf{A}\\mathbf{x}_i + \\mathbf{t} - \\mathbf{u}_i \\right\\|^{2},\n$$\nderive the normal equations that minimize $J(\\mathbf{A}, \\mathbf{t})$ with respect to the six unknown scalar parameters $(\\alpha_{11}, \\alpha_{12}, \\alpha_{21}, \\alpha_{22}, \\tau_{1}, \\tau_{2})$ where $\\mathbf{A} = \\begin{pmatrix} \\alpha_{11} & \\alpha_{12} \\\\ \\alpha_{21} & \\alpha_{22} \\end{pmatrix}$ and $\\mathbf{t} = (\\tau_{1}, \\tau_{2})$. Then, use the following fiducial correspondences to compute the parameter values that minimize the squared registration error:\n\n- $i = 1$: $\\mathbf{x}_1 = (-1, -1)$, $\\mathbf{u}_1 = (6.7, -6.6)$\n- $i = 2$: $\\mathbf{x}_2 = (1, -1)$, $\\mathbf{u}_2 = (8.9, -7.2)$\n- $i = 3$: $\\mathbf{x}_3 = (-1, 1)$, $\\mathbf{u}_3 = (7.1, -4.8)$\n- $i = 4$: $\\mathbf{x}_4 = (1, 1)$, $\\mathbf{u}_4 = (9.3, -5.4)$\n- $i = 5$: $\\mathbf{x}_5 = (-2, 0)$, $\\mathbf{u}_5 = (5.8, -5.4)$\n- $i = 6$: $\\mathbf{x}_6 = (2, 0)$, $\\mathbf{u}_6 = (10.2, -6.6)$\n- $i = 7$: $\\mathbf{x}_7 = (0, -2)$, $\\mathbf{u}_7 = (7.6, -7.8)$\n- $i = 8$: $\\mathbf{x}_8 = (0, 2)$, $\\mathbf{u}_8 = (8.4, -4.2)$\n\nAll coordinates $(x_i, y_i)$ and $(u_i, v_i)$ are expressed in micrometers. State clearly any assumptions you use. Report your final answer as the ordered parameter tuple $(\\alpha_{11}, \\alpha_{12}, \\alpha_{21}, \\alpha_{22}, \\tau_{1}, \\tau_{2})$. The entries $\\alpha_{11}, \\alpha_{12}, \\alpha_{21}, \\alpha_{22}$ are unitless; express $\\tau_{1}$ and $\\tau_{2}$ in micrometers. If you perform any numerical approximation, round your values to four significant figures.", "solution": "The problem requires the determination of the parameters of a two-dimensional affine transformation that maps a set of source points $\\mathbf{x}_i$ to a corresponding set of target points $\\mathbf{u}_i$. The transformation is given by $\\mathbf{u}_i \\approx \\mathbf{A}\\mathbf{x}_i + \\mathbf{t}$, and the parameters are the elements of the matrix $\\mathbf{A}$ and the vector $\\mathbf{t}$. These parameters are to be found by minimizing the sum of squared errors, a standard least squares problem.\n\nFirst, the validation of the problem statement is performed.\n\n### Step 1: Extract Givens\n- The an affine transformation model is $\\mathbf{u}_i \\approx \\mathbf{A}\\mathbf{x}_i + \\mathbf{t}$.\n- The coordinate vectors are $\\mathbf{x}_i = (x_i, y_i)$ and $\\mathbf{u}_i = (u_i, v_i)$.\n- The transformation matrix is $\\mathbf{A} = \\begin{pmatrix} \\alpha_{11} & \\alpha_{12} \\\\ \\alpha_{21} & \\alpha_{22} \\end{pmatrix}$.\n- The translation vector is $\\mathbf{t} = (\\tau_{1}, \\tau_{2})$.\n- The objective function is the sum of squared errors: $J(\\mathbf{A}, \\mathbf{t}) = \\sum_{i=1}^{n} \\left\\| \\mathbf{A}\\mathbf{x}_i + \\mathbf{t} - \\mathbf{u}_i \\right\\|^{2}$.\n- There are $n=8$ pairs of corresponding points:\n  - $\\mathbf{x}_1 = (-1, -1), \\mathbf{u}_1 = (6.7, -6.6)$\n  - $\\mathbf{x}_2 = (1, -1), \\mathbf{u}_2 = (8.9, -7.2)$\n  - $\\mathbf{x}_3 = (-1, 1), \\mathbf{u}_3 = (7.1, -4.8)$\n  - $\\mathbf{x}_4 = (1, 1), \\mathbf{u}_4 = (9.3, -5.4)$\n  - $\\mathbf{x}_5 = (-2, 0), \\mathbf{u}_5 = (5.8, -5.4)$\n  - $\\mathbf{x}_6 = (2, 0), \\mathbf{u}_6 = (10.2, -6.6)$\n  - $\\mathbf{x}_7 = (0, -2), \\mathbf{u}_7 = (7.6, -7.8)$\n  - $\\mathbf{x}_8 = (0, 2), \\mathbf{u}_8 = (8.4, -4.2)$\n- All coordinates are in micrometers.\n- The final answer should be the ordered tuple $(\\alpha_{11}, \\alpha_{12}, \\alpha_{21}, \\alpha_{22}, \\tau_{1}, \\tau_{2})$, with numerical values rounded to four significant figures if approximation is used.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as finding an optimal affine transformation via least squares is a standard and fundamental technique in image registration and data alignment, particularly relevant in fields like spatial omics. The problem is well-posed; it is an overdetermined linear least squares problem with $8$ data points for $6$ unknown parameters. Given the spatial distribution of the source points $\\mathbf{x}_i$, a unique solution is expected. The problem statement is objective, complete, and contains no contradictions or scientifically unsound premises.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be derived.\n\n### Derivation of the Normal Equations\n\nThe objective function $J(\\mathbf{A}, \\mathbf{t})$ can be expressed in terms of the scalar parameters:\n$$\nJ(\\alpha_{11}, \\alpha_{12}, \\alpha_{21}, \\alpha_{22}, \\tau_{1}, \\tau_{2}) = \\sum_{i=1}^{n} \\left[ (\\alpha_{11}x_i + \\alpha_{12}y_i + \\tau_1 - u_i)^2 + (\\alpha_{21}x_i + \\alpha_{22}y_i + \\tau_2 - v_i)^2 \\right]\n$$\nThis function is separable into two independent parts:\n$$\nJ = J_1(\\alpha_{11}, \\alpha_{12}, \\tau_1) + J_2(\\alpha_{21}, \\alpha_{22}, \\tau_2)\n$$\nwhere\n$$\nJ_1 = \\sum_{i=1}^{n} (\\alpha_{11}x_i + \\alpha_{12}y_i + \\tau_1 - u_i)^2\n$$\n$$\nJ_2 = \\sum_{i=1}^{n} (\\alpha_{21}x_i + \\alpha_{22}y_i + \\tau_2 - v_i)^2\n$$\nTo minimize $J$, we can minimize $J_1$ and $J_2$ independently. We find the minimum by setting the partial derivatives of $J_1$ with respect to $\\alpha_{11}$, $\\alpha_{12}$, and $\\tau_1$ to zero, and similarly for $J_2$.\n\nFor $J_1$:\n$$\n\\frac{\\partial J_1}{\\partial \\alpha_{11}} = \\sum_{i=1}^{n} 2(\\alpha_{11}x_i + \\alpha_{12}y_i + \\tau_1 - u_i)x_i = 0\n$$\n$$\n\\frac{\\partial J_1}{\\partial \\alpha_{12}} = \\sum_{i=1}^{n} 2(\\alpha_{11}x_i + \\alpha_{12}y_i + \\tau_1 - u_i)y_i = 0\n$$\n$$\n\\frac{\\partial J_1}{\\partial \\tau_1} = \\sum_{i=1}^{n} 2(\\alpha_{11}x_i + \\alpha_{12}y_i + \\tau_1 - u_i) = 0\n$$\nRearranging these gives the normal equations for the first set of parameters:\n$$\n\\alpha_{11}\\left(\\sum x_i^2\\right) + \\alpha_{12}\\left(\\sum x_i y_i\\right) + \\tau_1\\left(\\sum x_i\\right) = \\sum u_i x_i\n$$\n$$\n\\alpha_{11}\\left(\\sum x_i y_i\\right) + \\alpha_{12}\\left(\\sum y_i^2\\right) + \\tau_1\\left(\\sum y_i\\right) = \\sum u_i y_i\n$$\n$$\n\\alpha_{11}\\left(\\sum x_i\\right) + \\alpha_{12}\\left(\\sum y_i\\right) + \\tau_1(n) = \\sum u_i\n$$\nThis can be written in matrix form as:\n$$\n\\begin{pmatrix} \\sum x_i^2 & \\sum x_i y_i & \\sum x_i \\\\ \\sum x_i y_i & \\sum y_i^2 & \\sum y_i \\\\ \\sum x_i & \\sum y_i & n \\end{pmatrix}\n\\begin{pmatrix} \\alpha_{11} \\\\ \\alpha_{12} \\\\ \\tau_1 \\end{pmatrix} =\n\\begin{pmatrix} \\sum u_i x_i \\\\ \\sum u_i y_i \\\\ \\sum u_i \\end{pmatrix}\n$$\nA similar derivation for $J_2$ yields the second set of normal equations:\n$$\n\\begin{pmatrix} \\sum x_i^2 & \\sum x_i y_i & \\sum x_i \\\\ \\sum x_i y_i & \\sum y_i^2 & \\sum y_i \\\\ \\sum x_i & \\sum y_i & n \\end{pmatrix}\n\\begin{pmatrix} \\alpha_{21} \\\\ \\alpha_{22} \\\\ \\tau_2 \\end{pmatrix} =\n\\begin{pmatrix} \\sum v_i x_i \\\\ \\sum v_i y_i \\\\ \\sum v_i \\end{pmatrix}\n$$\nThe primary assumption made here is that a unique solution exists, which requires the $3 \\times 3$ matrix of sums to be invertible.\n\n### Calculation of Parameters\n\nWe compute the required sums using the $n=8$ provided data points.\n- $\\sum_{i=1}^8 x_i = -1 + 1 - 1 + 1 - 2 + 2 + 0 + 0 = 0$\n- $\\sum_{i=1}^8 y_i = -1 - 1 + 1 + 1 + 0 + 0 - 2 + 2 = 0$\n- $\\sum_{i=1}^8 x_i^2 = (-1)^2 + 1^2 + (-1)^2 + 1^2 + (-2)^2 + 2^2 + 0^2 + 0^2 = 12$\n- $\\sum_{i=1}^8 y_i^2 = (-1)^2 + (-1)^2 + 1^2 + 1^2 + 0^2 + 0^2 + (-2)^2 + 2^2 = 12$\n- $\\sum_{i=1}^8 x_i y_i = (-1)(-1) + (1)(-1) + (-1)(1) + (1)(1) + 0 + 0 + 0 + 0 = 0$\n\nThe $3 \\times 3$ matrix is therefore:\n$$\n\\begin{pmatrix} 12 & 0 & 0 \\\\ 0 & 12 & 0 \\\\ 0 & 0 & 8 \\end{pmatrix}\n$$\nThis matrix is diagonal and clearly invertible, confirming a unique solution exists.\n\nNext, we compute the sums for the right-hand side vectors.\nFor the $(\\alpha_{11}, \\alpha_{12}, \\tau_1)$ system:\n- $\\sum u_i = 6.7 + 8.9 + 7.1 + 9.3 + 5.8 + 10.2 + 7.6 + 8.4 = 64.0$\n- $\\sum u_i x_i = 6.7(-1) + 8.9(1) + 7.1(-1) + 9.3(1) + 5.8(-2) + 10.2(2) = -6.7 + 8.9 - 7.1 + 9.3 - 11.6 + 20.4 = 13.2$\n- $\\sum u_i y_i = 6.7(-1) + 8.9(-1) + 7.1(1) + 9.3(1) + 7.6(-2) + 8.4(2) = -6.7 - 8.9 + 7.1 + 9.3 - 15.2 + 16.8 = 2.4$\n\nThe first linear system is:\n$$\n\\begin{pmatrix} 12 & 0 & 0 \\\\ 0 & 12 & 0 \\\\ 0 & 0 & 8 \\end{pmatrix}\n\\begin{pmatrix} \\alpha_{11} \\\\ \\alpha_{12} \\\\ \\tau_1 \\end{pmatrix} =\n\\begin{pmatrix} 13.2 \\\\ 2.4 \\\\ 64.0 \\end{pmatrix}\n$$\nSolving this diagonal system:\n- $12 \\alpha_{11} = 13.2 \\implies \\alpha_{11} = \\frac{13.2}{12} = 1.1$\n- $12 \\alpha_{12} = 2.4 \\implies \\alpha_{12} = \\frac{2.4}{12} = 0.2$\n- $8 \\tau_1 = 64.0 \\implies \\tau_1 = \\frac{64.0}{8} = 8.0$\n\nFor the $(\\alpha_{21}, \\alpha_{22}, \\tau_2)$ system:\n- $\\sum v_i = -6.6 - 7.2 - 4.8 - 5.4 - 5.4 - 6.6 - 7.8 - 4.2 = -48.0$\n- $\\sum v_i x_i = -6.6(-1) - 7.2(1) - 4.8(-1) - 5.4(1) - 5.4(-2) - 6.6(2) = 6.6 - 7.2 + 4.8 - 5.4 + 10.8 - 13.2 = -3.6$\n- $\\sum v_i y_i = -6.6(-1) - 7.2(-1) - 4.8(1) - 5.4(1) - 7.8(-2) - 4.2(2) = 6.6 + 7.2 - 4.8 - 5.4 + 15.6 - 8.4 = 10.8$\n\nThe second linear system is:\n$$\n\\begin{pmatrix} 12 & 0 & 0 \\\\ 0 & 12 & 0 \\\\ 0 & 0 & 8 \\end{pmatrix}\n\\begin{pmatrix} \\alpha_{21} \\\\ \\alpha_{22} \\\\ \\tau_2 \\end{pmatrix} =\n\\begin{pmatrix} -3.6 \\\\ 10.8 \\\\ -48.0 \\end{pmatrix}\n$$\nSolving this diagonal system:\n- $12 \\alpha_{21} = -3.6 \\implies \\alpha_{21} = \\frac{-3.6}{12} = -0.3$\n- $12 \\alpha_{22} = 10.8 \\implies \\alpha_{22} = \\frac{10.8}{12} = 0.9$\n- $8 \\tau_2 = -48.0 \\implies \\tau_2 = \\frac{-48.0}{8} = -6.0$\n\nThe calculations are exact, so no rounding is necessary. The calculated parameters are:\n- $\\alpha_{11} = 1.1$ (unitless)\n- $\\alpha_{12} = 0.2$ (unitless)\n- $\\alpha_{21} = -0.3$ (unitless)\n- $\\alpha_{22} = 0.9$ (unitless)\n- $\\tau_1 = 8.0$ (micrometers)\n- $\\tau_2 = -6.0$ (micrometers)\n\nThe final answer is the ordered tuple $(\\alpha_{11}, \\alpha_{12}, \\alpha_{21}, \\alpha_{22}, \\tau_{1}, \\tau_{2})$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1.1 & 0.2 & -0.3 & 0.9 & 8.0 & -6.0 \\end{pmatrix}}\n$$", "id": "5062750"}, {"introduction": "Most spot-based spatial transcriptomics technologies capture the expression of a mixture of cells, rather than single cells. To uncover the underlying cellular architecture of a tissue, we must computationally 'unmix' or deconvolve these signals [@problem_id:4386305]. This exercise introduces the deconvolution problem as a constrained regression task, where you will estimate cell type proportions by minimizing the difference between observed expression and a model built from known cell type signatures, a core skill for interpreting spot-level spatial data.", "problem": "You are given a systems biomedicine scenario in which a spatial spot from the Visium Spatial Gene Expression (Visium) technology contains a mixture of cell types whose messenger ribonucleic acid (mRNA) transcript abundances are measured across multiple genes. In such a spot, the observed expression profile is a mixture of the expression profiles of constituent cell types. To quantify the composition of a spot, assume a reference signature matrix that encodes the average gene expression of each cell type. The task is to formulate and solve a constrained regression problem that estimates the cell type proportions for each spot.\n\nUse the following fundamental base and assumptions:\n- Central Dogma of Molecular Biology: deoxyribonucleic acid (DNA) is transcribed into ribonucleic acid (RNA), which is translated into protein. In sufficiently averaged and normalized settings, observed transcript counts per gene reflect the aggregate of contributions from different cell types present in a sample.\n- Linear mixture model: the observed vector of gene expression in a spot can be modeled as a linear combination of cell-type-specific signatures weighted by unknown nonnegative proportions. Let $G$ be the number of genes, $K$ be the number of cell types, $\\mathbf{S} \\in \\mathbb{R}_{\\ge 0}^{G \\times K}$ be the known signature matrix whose $k$-th column is the reference expression profile for cell type $k$, and $\\mathbf{y} \\in \\mathbb{R}_{\\ge 0}^{G}$ be the observed expression vector for a spot. The unknown vector of cell type proportions is $\\mathbf{p} \\in \\mathbb{R}_{\\ge 0}^{K}$.\n- Constraints: proportions are nonnegative and sum to one, i.e., $\\mathbf{p} \\succeq \\mathbf{0}$ and $\\mathbf{1}^{\\top} \\mathbf{p} = 1$, where $\\mathbf{1}$ is the all-ones vector in $\\mathbb{R}^{K}$.\n- Objective: the estimation criterion is the squared Euclidean distance between predicted and observed expression, optionally with nonnegative weights $w_g$ per gene to reflect reliability, and a small ridge regularization to ensure uniqueness and numerical stability.\n\nFormally, define the deconvolution as the following constrained optimization for each spot:\nMinimize the function\n$$\n\\frac{1}{2} \\sum_{g=1}^{G} w_g \\left( (\\mathbf{S}\\mathbf{p})_g - \\mathbf{y}_g \\right)^2 + \\frac{\\alpha}{2} \\lVert \\mathbf{p} \\rVert_2^2\n$$\nsubject to the constraints $\\mathbf{p} \\succeq \\mathbf{0}$ and $\\mathbf{1}^{\\top} \\mathbf{p} = 1$,\nwhere $w_g \\ge 0$ are given per-gene weights and $\\alpha \\ge 0$ is a small regularization parameter.\n\nYour program must implement a solver that, for each test case, computes the cell type proportion vector $\\widehat{\\mathbf{p}}$ that minimizes the above objective under the constraints. You may assume convexity and that a minimizer exists. The solver must be general and must not rely on closed-form inversion of the constraints. The program must round each component of $\\widehat{\\mathbf{p}}$ to $4$ decimal places.\n\nAngle units are not applicable. There are no physical units; proportions must be reported as decimal fractions.\n\nTest suite:\nFor each case, you are given $G$, $K$, the signature matrix $\\mathbf{S}$, the observed expression $\\mathbf{y}$, the weights $\\mathbf{w}$, and the ridge parameter $\\alpha$. All numerical values are given explicitly below.\n\n- Case A (general, small noise):\n  - $G = 5$, $K = 3$, $\\alpha = 10^{-6}$.\n  - $\\mathbf{S} = \\begin{bmatrix}\n  8 & 2 & 1 \\\\\n  1 & 7 & 2 \\\\\n  0 & 1 & 9 \\\\\n  5 & 2 & 1 \\\\\n  2 & 3 & 4\n  \\end{bmatrix}$.\n  - $\\mathbf{y} = \\begin{bmatrix} 4.9 \\\\ 2.95 \\\\ 2.12 \\\\ 3.27 \\\\ 2.7 \\end{bmatrix}$.\n  - $\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n\n- Case B (pure cell type, no noise):\n  - $G = 5$, $K = 3$, $\\alpha = 10^{-6}$.\n  - $\\mathbf{S}$ is identical to Case A.\n  - $\\mathbf{y} = \\begin{bmatrix} 2 \\\\ 7 \\\\ 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$.\n  - $\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n\n- Case C (collinearity, exact mixture):\n  - $G = 4$, $K = 3$, $\\alpha = 10^{-6}$.\n  - $\\mathbf{S} = \\begin{bmatrix}\n  3 & 3 & 1 \\\\\n  2 & 2 & 0.5 \\\\\n  1 & 1 & 4 \\\\\n  0.5 & 0.5 & 2\n  \\end{bmatrix}$.\n  - $\\mathbf{y} = \\begin{bmatrix} 2.6 \\\\ 1.7 \\\\ 1.6 \\\\ 0.8 \\end{bmatrix}$.\n  - $\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n\n- Case D (zeros and dropout, moderate noise):\n  - $G = 4$, $K = 2$, $\\alpha = 10^{-6}$.\n  - $\\mathbf{S} = \\begin{bmatrix}\n  0 & 3 \\\\\n  5 & 0 \\\\\n  0 & 2 \\\\\n  4 & 1\n  \\end{bmatrix}$.\n  - $\\mathbf{y} = \\begin{bmatrix} 1.15 \\\\ 3.1 \\\\ 0.4 \\\\ 2.7 \\end{bmatrix}$.\n  - $\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n\n- Case E (weighted fit, noisy second gene downweighted):\n  - $G = 3$, $K = 3$, $\\alpha = 10^{-6}$.\n  - $\\mathbf{S} = \\begin{bmatrix}\n  10 & 1 & 0.5 \\\\\n  0.5 & 8 & 1 \\\\\n  1 & 1 & 6\n  \\end{bmatrix}$.\n  - $\\mathbf{y} = \\begin{bmatrix} 2.65 \\\\ 5.9 \\\\ 2.4 \\end{bmatrix}$.\n  - $\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 0.2 \\\\ 1 \\end{bmatrix}$.\n\nImplementation requirements:\n- Implement a solver that handles the constraints $\\mathbf{p} \\succeq \\mathbf{0}$ and $\\mathbf{1}^{\\top} \\mathbf{p} = 1$ exactly, without relaxing them, and includes the ridge regularization $\\alpha$ in the objective.\n- The algorithm must be principled and general (for example, a projected gradient method with a mathematically justified step size and projection onto the probability simplex).\n- For each case, compute the estimated proportions $\\widehat{\\mathbf{p}}$ and round each component to $4$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$), where each $resulti$ is the list of rounded proportions for the corresponding case. The output must contain no spaces. For example: $[[0.5000,0.3000,0.2000],[0.0000,1.0000,0.0000],\\dots]$.", "solution": "The user has provided a problem statement that has been validated and deemed to be scientifically grounded, well-posed, and objective.\n\nThe problem requires the solution of a constrained quadratic optimization problem to perform cell-type deconvolution in spatial transcriptomics. The observed gene expression vector for a spot, $\\mathbf{y} \\in \\mathbb{R}_{\\ge 0}^{G}$, is modeled as a linear combination of reference cell-type expression signatures. These signatures are encoded in the columns of a known matrix $\\mathbf{S} \\in \\mathbb{R}_{\\ge 0}^{G \\times K}$, where $G$ is the number of genes and $K$ is the number of cell types. The unknown quantities are the proportions of each cell type, represented by the vector $\\mathbf{p} \\in \\mathbb{R}_{\\ge 0}^{K}$.\n\nThe proportions must satisfy two physical constraints: they must be non-negative, $\\mathbf{p} \\succeq \\mathbf{0}$, and they must sum to unity, $\\mathbf{1}^{\\top} \\mathbf{p} = 1$. The set of all vectors $\\mathbf{p}$ satisfying these constraints forms the standard $K-1$-dimensional simplex, denoted as $\\Delta^{K-1}$.\n\nThe goal is to find the proportion vector $\\widehat{\\mathbf{p}}$ that minimizes a regularized, weighted, squared error objective function. The objective function is given by:\n$$\nf(\\mathbf{p}) = \\frac{1}{2} \\sum_{g=1}^{G} w_g \\left( (\\mathbf{S}\\mathbf{p})_g - y_g \\right)^2 + \\frac{\\alpha}{2} \\lVert \\mathbf{p} \\rVert_2^2\n$$\nwhere $w_g \\ge 0$ are gene-specific weights and $\\alpha \\ge 0$ is a Tikhonov (ridge) regularization parameter.\n\nThis problem can be expressed in matrix form. Let $\\mathbf{W}$ be a $G \\times G$ diagonal matrix with the weights $w_g$ on its diagonal. The objective function is:\n$$\nf(\\mathbf{p}) = \\frac{1}{2} (\\mathbf{S}\\mathbf{p} - \\mathbf{y})^{\\top} \\mathbf{W} (\\mathbf{S}\\mathbf{p} - \\mathbf{y}) + \\frac{\\alpha}{2} \\mathbf{p}^{\\top}\\mathbf{p}\n$$\nThe problem is a quadratic program (QP) with a convex objective function and a compact, convex feasible set (the simplex $\\Delta^{K-1}$). The strict convexity imparted by the ridge term ($\\alpha > 0$) guarantees a unique solution.\n\nA suitable and general algorithm for solving this problem is Projected Gradient Descent. This iterative method consists of taking a step in the negative gradient direction and then projecting the resulting point back onto the feasible set.\n\nThe algorithm proceeds as follows:\n1.  Initialize $\\mathbf{p}^{(0)}$ to a feasible point, typically the center of the simplex: $\\mathbf{p}^{(0)} = \\frac{1}{K}\\mathbf{1}$.\n2.  For iteration $t = 0, 1, 2, \\dots$ until convergence:\n    a.  Compute a descent direction by calculating the gradient of the objective function, $\\nabla f(\\mathbf{p})$, at the current iterate $\\mathbf{p}^{(t)}$.\n    b.  Take a step in this direction: $\\mathbf{z}^{(t+1)} = \\mathbf{p}^{(t)} - \\eta \\nabla f(\\mathbf{p}^{(t)})$, where $\\eta > 0$ is the step size.\n    c.  Project the point $\\mathbf{z}^{(t+1)}$ onto the simplex $\\Delta^{K-1}$: $\\mathbf{p}^{(t+1)} = \\Pi_{\\Delta^{K-1}}(\\mathbf{z}^{(t+1)})$.\n3.  The algorithm terminates when the change in $\\mathbf{p}$ between iterations is below a small tolerance.\n\nThe key components of the algorithm are the gradient calculation, the step size determination, and the projection operator.\n\n**1. Gradient Calculation**\n\nThe objective function is:\n$$\nf(\\mathbf{p}) = \\frac{1}{2} \\left( \\mathbf{p}^{\\top}\\mathbf{S}^{\\top}\\mathbf{W}\\mathbf{S}\\mathbf{p} - 2\\mathbf{y}^{\\top}\\mathbf{W}\\mathbf{S}\\mathbf{p} + \\mathbf{y}^{\\top}\\mathbf{W}\\mathbf{y} \\right) + \\frac{\\alpha}{2} \\mathbf{p}^{\\top}\\mathbf{p}\n$$\nThe gradient with respect to $\\mathbf{p}$ is:\n$$\n\\nabla f(\\mathbf{p}) = \\mathbf{S}^{\\top}\\mathbf{W}\\mathbf{S}\\mathbf{p} - \\mathbf{S}^{\\top}\\mathbf{W}\\mathbf{y} + \\alpha\\mathbf{p} = \\mathbf{S}^{\\top}\\mathbf{W}(\\mathbf{S}\\mathbf{p} - \\mathbf{y}) + \\alpha\\mathbf{p}\n$$\n\n**2. Step Size Determination**\n\nFor gradient descent to converge, the step size $\\eta$ must be chosen appropriately. For a convex function with a Lipschitz continuous gradient, a sufficient condition for convergence is $0 < \\eta < 2/L$, where $L$ is the Lipschitz constant of the gradient. For our quadratic objective, $L$ is the largest eigenvalue of the Hessian matrix, $\\nabla^2 f(\\mathbf{p})$. The Hessian is:\n$$\n\\nabla^2 f(\\mathbf{p}) = \\mathbf{S}^{\\top}\\mathbf{W}\\mathbf{S} + \\alpha\\mathbf{I}\n$$\nwhere $\\mathbf{I}$ is the $K \\times K$ identity matrix. This Hessian is a constant, symmetric, positive-definite matrix (for $\\alpha > 0$). We can compute its eigenvalues and set $L$ to be the maximum eigenvalue, $\\lambda_{\\text{max}}$. A safe and constant step size is then $\\eta = 1/L$.\n\n**3. Projection onto the Probability Simplex**\n\nThe projection step, $\\mathbf{p}^{(t+1)} = \\Pi_{\\Delta^{K-1}}(\\mathbf{z}^{(t+1)})$, finds the point on the simplex that is closest to $\\mathbf{z}^{(t+1)}$ in Euclidean distance. This is itself a QP:\n$$\n\\text{minimize}_{\\mathbf{p}} \\quad \\frac{1}{2} \\lVert \\mathbf{p} - \\mathbf{z} \\rVert_2^2 \\quad \\text{subject to} \\quad \\mathbf{p} \\succeq \\mathbf{0}, \\mathbf{1}^{\\top}\\mathbf{p} = 1\n$$\nAn efficient algorithm to solve this projection in $O(K \\log K)$ time exists. The steps are:\n1.  Sort the elements of the vector $\\mathbf{z}$ in descending order to get a new vector $\\mathbf{u}$.\n2.  Find an integer $\\rho$ such that $\\rho = \\max\\left\\{ j \\in \\{1, \\dots, K\\} \\mid u_j - \\frac{1}{j} \\left( \\sum_{i=1}^j u_i - 1 \\right) > 0 \\right\\}$.\n3.  Define a threshold $\\theta = \\frac{1}{\\rho} \\left( \\sum_{i=1}^\\rho u_i - 1 \\right)$.\n4.  The projected vector $\\mathbf{p}$ is obtained by applying a soft-thresholding operator element-wise to the original vector $\\mathbf{z}$: $p_k = \\max(z_k - \\theta, 0)$ for $k = 1, \\dots, K$.\n\nBy combining these three components into an iterative loop, the algorithm will converge to the unique minimizer $\\widehat{\\mathbf{p}}$ for each test case.", "answer": "[[0.5521,0.2974,0.1505],[0.0000,1.0000,0.0000],[0.4000,0.4000,0.2000],[0.6010,0.3990],[0.2006,0.4984,0.3010]]", "id": "4386305"}, {"introduction": "Once gene expression or cell type composition is mapped across a tissue, a critical question arises: are the observed patterns spatially meaningful, or are they random? This practice delves into the fundamental concept of spatial autocorrelation, a measure of how similar a value at one location is to values at nearby locations [@problem_id:4386267]. You will derive and compute Moran's $I$, a cornerstone statistic for quantifying spatial structure and identifying non-random clusters of high or low expression, which is essential for discovering biologically significant spatial domains.", "problem": "Consider a one-dimensional tissue strip assayed by Spatial Transcriptomics (ST), where adjacent capture spots lie on a linear lattice with equal spacing. Focus on a single gene and suppose its normalized expression across $n$ spots is modeled as a real-valued field $x_{i}$ located at positions indexed by $i \\in \\{1,2,\\dots,n\\}$. Spatial autocorrelation is assessed via a symmetric, zero-diagonal, non-row-standardized spatial weight matrix $W=\\{w_{ij}\\}$ encoding neighborhood relationships, where larger $w_{ij}$ implies stronger spatial coupling between locations $i$ and $j$.\n\nStarting from the foundational statistical definitions of the sample mean, sample variance, and the concept of a spatially weighted covariance, derive dimensionless indices of spatial autocorrelation that (i) compare the spatially weighted cross-deviation of $x_{i}$ to its total deviation and (ii) compare squared differences between neighboring values of $x_{i}$ to the total deviation. These constructions are known as Moran’s $I$ and Geary’s $C$. Your derivation must identify appropriate normalization factors built from $n$ and the total weight $S_{0}$, where $S_{0}=\\sum_{i=1}^{n}\\sum_{j=1}^{n} w_{ij}$, so that each index is scale-invariant in $x_{i}$ and dimensionless.\n\nThen compute Moran’s $I$ for the following scientifically realistic ST setting:\n- There are $n=7$ ordered spots on the lattice with expression values $x_{1},\\dots,x_{7}$ given by\n$$\nx = \\left(5,\\,6,\\,7,\\,12,\\,11,\\,9,\\,8\\right).\n$$\n- The spatial weights implement first-order contiguity on the lattice:\n$$\nw_{ij}=\\begin{cases}\n1 & \\text{if } |i-j|=1,\\\\\n0 & \\text{otherwise,}\n\\end{cases}\n\\quad \\text{and} \\quad w_{ii}=0 \\text{ for all } i.\n$$\n\nExpress your final numerical result for Moran’s $I$ as an exact value without rounding. Do not include any units. If you introduce any acronym, write out its full name on first use with the acronym in parentheses.", "solution": "We begin with fundamental statistical constructs for a real-valued field $x_{i}$ observed at $n$ spatial locations. The sample mean is\n$$\n\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i},\n$$\nand the total sample deviation (unnormalized variance numerator) is\n$$\n\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}.\n$$\nTo encode spatial structure, we consider a symmetric spatial weight matrix $W=\\{w_{ij}\\}$ with $w_{ii}=0$ and total weight\n$$\nS_{0}=\\sum_{i=1}^{n}\\sum_{j=1}^{n}w_{ij}.\n$$\nA spatially weighted cross-deviation is captured by\n$$\n\\sum_{i=1}^{n}\\sum_{j=1}^{n} w_{ij}\\,(x_{i}-\\bar{x})(x_{j}-\\bar{x}),\n$$\nwhich generalizes the notion of covariance by weighting pairs according to spatial proximity.\n\nMoran’s $I$ is constructed to be dimensionless and scale-invariant in $x_{i}$ by normalizing the spatially weighted cross-deviation by the total sample deviation, and by introducing a factor depending only on $n$ and $S_{0}$ to account for the total weight magnitude and the number of locations. The canonical form is derived as\n$$\nI=\\frac{n}{S_{0}}\\cdot \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n} w_{ij}\\,(x_{i}-\\bar{x})(x_{j}-\\bar{x})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}},\n$$\nwhich is dimensionless, invariant under affine scaling $x_{i}\\mapsto a x_{i}+b$ with $a\\neq 0$, and uses $S_{0}$ to normalize the aggregate weight magnitude.\n\nGeary’s $C$ compares squared differences across neighboring locations to the total deviation, with a normalization ensuring dimensionlessness and scale invariance. Starting from the spatially weighted sum of squared differences,\n$$\n\\sum_{i=1}^{n}\\sum_{j=1}^{n} w_{ij}\\,(x_{i}-x_{j})^{2},\n$$\nthe standard normalized form is\n$$\nC=\\frac{n-1}{2 S_{0}}\\cdot \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n} w_{ij}\\,(x_{i}-x_{j})^{2}}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}},\n$$\nwhich is likewise dimensionless and scale-invariant.\n\nWe now compute Moran’s $I$ for the given one-dimensional lattice.\n\nStep 1: Compute the mean $\\bar{x}$ for\n$$\nx=\\left(5,\\,6,\\,7,\\,12,\\,11,\\,9,\\,8\\right).\n$$\nThe sum is\n$$\n\\sum_{i=1}^{7} x_{i} = 5+6+7+12+11+9+8 = 58,\n$$\nso\n$$\n\\bar{x}=\\frac{58}{7}.\n$$\n\nStep 2: Compute deviations $d_{i}=x_{i}-\\bar{x}$:\n$$\nd_{1}=\\frac{35-58}{7}=-\\frac{23}{7},\\quad\nd_{2}=\\frac{42-58}{7}=-\\frac{16}{7},\\quad\nd_{3}=\\frac{49-58}{7}=-\\frac{9}{7},\n$$\n$$\nd_{4}=\\frac{84-58}{7}=\\frac{26}{7},\\quad\nd_{5}=\\frac{77-58}{7}=\\frac{19}{7},\\quad\nd_{6}=\\frac{63-58}{7}=\\frac{5}{7},\\quad\nd_{7}=\\frac{56-58}{7}=-\\frac{2}{7}.\n$$\n\nStep 3: Compute the total deviation denominator\n$$\n\\sum_{i=1}^{7}(x_{i}-\\bar{x})^{2}=\\sum_{i=1}^{7} d_{i}^{2}\n=\\frac{529+256+81+676+361+25+4}{49}=\\frac{1932}{49}.\n$$\n\nStep 4: Compute $S_{0}$ for the specified contiguity weights. On a $7$-node chain, the undirected edges are between pairs $(1,2),(2,3),(3,4),(4,5),(5,6),(6,7)$, totaling $6$ edges. Because $w_{ij}=w_{ji}=1$ and we sum over ordered pairs $(i,j)$, we have\n$$\nS_{0}=\\sum_{i=1}^{7}\\sum_{j=1}^{7} w_{ij} = 2\\times 6 = 12.\n$$\n\nStep 5: Compute the spatially weighted cross-deviation numerator $\\sum_{i=1}^{7}\\sum_{j=1}^{7} w_{ij} d_{i} d_{j}$. Nonzero contributions arise only for $|i-j|=1$, and symmetry yields\n$$\n\\sum_{i=1}^{7}\\sum_{j=1}^{7} w_{ij} d_{i} d_{j}\n=2\\sum_{\\text{edges }(i,j)} d_{i} d_{j}.\n$$\nCompute the edge products:\n$$\n(1,2):\\; d_{1}d_{2}=\\left(-\\frac{23}{7}\\right)\\left(-\\frac{16}{7}\\right)=\\frac{368}{49},\n$$\n$$\n(2,3):\\; d_{2}d_{3}=\\left(-\\frac{16}{7}\\right)\\left(-\\frac{9}{7}\\right)=\\frac{144}{49},\n$$\n$$\n(3,4):\\; d_{3}d_{4}=\\left(-\\frac{9}{7}\\right)\\left(\\frac{26}{7}\\right)=-\\frac{234}{49},\n$$\n$$\n(4,5):\\; d_{4}d_{5}=\\left(\\frac{26}{7}\\right)\\left(\\frac{19}{7}\\right)=\\frac{494}{49},\n$$\n$$\n(5,6):\\; d_{5}d_{6}=\\left(\\frac{19}{7}\\right)\\left(\\frac{5}{7}\\right)=\\frac{95}{49},\n$$\n$$\n(6,7):\\; d_{6}d_{7}=\\left(\\frac{5}{7}\\right)\\left(-\\frac{2}{7}\\right)=-\\frac{10}{49}.\n$$\nSumming these across edges,\n$$\n\\sum_{\\text{edges }(i,j)} d_{i} d_{j}\n=\\frac{368+144-234+494+95-10}{49}\n=\\frac{857}{49}.\n$$\nTherefore,\n$$\n\\sum_{i=1}^{7}\\sum_{j=1}^{7} w_{ij} d_{i} d_{j}\n=2\\cdot \\frac{857}{49}=\\frac{1714}{49}.\n$$\n\nStep 6: Assemble Moran’s $I$:\n$$\nI=\\frac{n}{S_{0}}\\cdot \\frac{\\sum_{i=1}^{7}\\sum_{j=1}^{7} w_{ij} d_{i} d_{j}}{\\sum_{i=1}^{7} d_{i}^{2}}\n=\\frac{7}{12}\\cdot \\frac{\\frac{1714}{49}}{\\frac{1932}{49}}\n=\\frac{7}{12}\\cdot \\frac{1714}{1932}.\n$$\nSimplify:\n$$\nI=\\frac{7\\cdot 1714}{12\\cdot 1932}\n=\\frac{7\\cdot 857\\cdot 2}{12\\cdot 1932}\n=\\frac{7\\cdot 857}{6\\cdot 1932}\n=\\frac{5999}{11592}.\n$$\nDivide numerator and denominator by $7$:\n$$\nI=\\frac{857}{1656}.\n$$\nThe fraction $\\frac{857}{1656}$ is already in lowest terms, so the exact value for Moran’s $I$ is\n$$\nI=\\frac{857}{1656}.\n$$", "answer": "$$\\boxed{\\frac{857}{1656}}$$", "id": "4386267"}]}