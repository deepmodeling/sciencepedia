{"hands_on_practices": [{"introduction": "Before we can model cell fate decisions, we must first address technical artifacts inherent in single-cell data, such as variable sequencing depth per cell. This exercise guides you through the derivation of a robust size-factor normalization method from first principles, a technique fundamental to many single-cell analysis workflows [@problem_id:4361230]. By grounding this practical step in a clear statistical model, you will gain a deeper understanding of how to separate biological signals from technical noise.", "problem": "A central task in modeling cell fate decisions from single-cell RNA sequencing (scRNA-seq) data is to remove technical sampling-depth variation while preserving biologically meaningful relative expression patterns across genes within each cell. Consider $n$ single cells indexed by $i \\in \\{1,\\dots,n\\}$ and $G$ genes indexed by $g \\in \\{1,\\dots,G\\}$. Let $x_{ig} \\in \\mathbb{N}$ denote the observed unique molecular identifier count for gene $g$ in cell $i$. Assume a generative model in which, conditional on latent expression and sampling depth, counts are well-approximated as independent Poisson variables,\n$$\nx_{ig} \\sim \\text{Poisson}\\!\\left(s_{i}\\,\\theta_{ig}\\right),\n$$\nwhere $s_{i} > 0$ is a cell-specific sampling depth (library size) and $\\theta_{ig} > 0$ is the underlying expected expression level for gene $g$ in cell $i$. To relate $\\theta_{ig}$ to fate-dependent programs, suppose $\\theta_{ig} = \\phi_{g}\\,r_{ig}$, where $\\phi_{g} > 0$ is a gene-specific baseline and $r_{ig} > 0$ captures cell-state-dependent relative expression for gene $g$ in cell $i$. The aim is to estimate $s_{i}$ in a way that removes the confounding effect of $s_{i}$ while minimally distorting the distribution of $r_{ig}$ across genes, thereby preserving within-cell relative expression required to infer fate decisions.\n\nStarting from this model and the well-tested observation that the Poisson approximation to multinomial sampling holds for large total counts, derive from first principles a robust estimator of $s_{i}$ based on minimizing the sum of absolute deviations in log-space across genes against a per-gene pseudo-reference constructed solely from the observed counts. Concretely, for each gene $g$, take its pseudo-reference level to be the geometric mean of its counts across all $n$ cells. Restrict attention to genes with strictly positive counts across all cells to ensure well-defined logarithms. Use this setup to derive an analytic expression for the estimator $s_{i}$ in terms of the observed counts $x_{ig}$ only, and explain why scaling $x_{ig}$ by this $s_{i}$ approximately removes the sampling-depth confounder $s_{i}$ without distorting within-cell relative expression across genes.\n\nProvide your final result as a single closed-form analytic expression for $s_{i}$ in terms of $\\{x_{ig}\\}_{g=1}^{G}$ and $\\{x_{jg}\\}_{j=1}^{n}$, with no units. No rounding is required.", "solution": "The problem is valid. It is scientifically grounded in the principles of single-cell data analysis, arithmetically consistent, and well-posed. All terms are defined, and the task is to derive a specific estimator from a clearly stated optimization criterion.\n\nWe begin by formally stating the problem for a single cell $i$. The goal is to find an estimator for the cell-specific sampling depth, $s_i$, by minimizing a specific objective function. The problem asks us to minimize the sum of absolute deviations in log-space. The deviation is defined for each gene $g$ as the difference between the log of the normalized count for cell $i$ and the log of a pseudo-reference level for that gene.\n\nLet's define the components:\n1.  The observed count for gene $g$ in cell $i$ is $x_{ig}$.\n2.  The estimator for the sampling depth for cell $i$ is denoted $\\hat{s_i}$.\n3.  The normalized count, after correcting for sampling depth, is $x_{ig} / \\hat{s_i}$. In log-space, this is $\\ln(x_{ig}) - \\ln(\\hat{s_i})$.\n4.  The pseudo-reference for gene $g$, as defined in the problem, is the geometric mean of its counts across all $n$ cells. Let's call this $p_g$.\n    $$\n    p_g = \\left( \\prod_{j=1}^{n} x_{jg} \\right)^{1/n}\n    $$\n    In log-space, the pseudo-reference is:\n    $$\n    \\ln(p_g) = \\ln\\left( \\left( \\prod_{j=1}^{n} x_{jg} \\right)^{1/n} \\right) = \\frac{1}{n} \\sum_{j=1}^{n} \\ln(x_{jg})\n    $$\n    The problem specifies that we only consider genes with $x_{jg} > 0$ for all $j \\in \\{1,\\dots,n\\}$, which ensures that $p_g$ is well-defined and positive.\n\nThe deviation for gene $g$ in log-space is the difference between the normalized log-count and the reference log-count:\n$$\nd_{ig} = (\\ln(x_{ig}) - \\ln(\\hat{s_i})) - \\ln(p_g)\n$$\nThe objective is to find the value of $\\hat{s_i}$ that minimizes the sum of the absolute values of these deviations across all $G$ genes for a fixed cell $i$. We define the loss function $L(\\hat{s_i})$ as:\n$$\nL(\\hat{s_i}) = \\sum_{g=1}^{G} |d_{ig}| = \\sum_{g=1}^{G} \\left| (\\ln(x_{ig}) - \\ln(p_g)) - \\ln(\\hat{s_i}) \\right|\n$$\nWe seek the value of $\\hat{s_i}$ that minimizes $L(\\hat{s_i})$. Let $c_{ig} = \\ln(x_{ig}) - \\ln(p_g)$ and let $l_i = \\ln(\\hat{s_i})$. The problem is transformed into finding the value $l_i$ that minimizes:\n$$\nL(l_i) = \\sum_{g=1}^{G} | c_{ig} - l_i |\n$$\nThis is a classic problem in robust statistics. The quantity $\\sum_{g=1}^{G} |c_{ig} - l_i|$ is minimized when $l_i$ is the median of the set of values $\\{c_{ig}\\}_{g=1}^{G}$.\nTherefore, the estimator for the logarithm of the sampling depth, which we denote as $\\ln(\\hat{s_i})$, is:\n$$\n\\ln(\\hat{s_i}) = \\text{median}_{g \\in \\{1,\\dots,G\\}} \\{ c_{ig} \\} = \\text{median}_{g \\in \\{1,\\dots,G\\}} \\{ \\ln(x_{ig}) - \\ln(p_g) \\}\n$$\nSubstituting the expression for $\\ln(p_g)$:\n$$\n\\ln(\\hat{s_i}) = \\text{median}_{g \\in \\{1,\\dots,G\\}} \\left\\{ \\ln(x_{ig}) - \\frac{1}{n} \\sum_{j=1}^{n} \\ln(x_{jg}) \\right\\}\n$$\nTo find the estimator $\\hat{s_i}$ itself, we exponentiate the result:\n$$\n\\hat{s_i} = \\exp\\left( \\ln(\\hat{s_i}) \\right) = \\exp\\left( \\text{median}_{g \\in \\{1,\\dots,G\\}} \\left\\{ \\ln(x_{ig}) - \\frac{1}{n} \\sum_{j=1}^{n} \\ln(x_{jg}) \\right\\} \\right)\n$$\nThe exponential function $\\exp(\\cdot)$ is strictly monotonic. This property implies that the exponential of the median of a set of numbers is equal to the median of the exponentials of those numbers. That is, $\\exp(\\text{median}\\{y_k\\}) = \\text{median}\\{\\exp(y_k)\\}$. Applying this rule, we can move the exponential function inside the median operator:\n$$\n\\hat{s_i} = \\text{median}_{g \\in \\{1,\\dots,G\\}} \\left\\{ \\exp\\left( \\ln(x_{ig}) - \\frac{1}{n} \\sum_{j=1}^{n} \\ln(x_{jg}) \\right) \\right\\}\n$$\nUsing the properties of logarithms and exponentials, $\\exp(a-b) = \\exp(a)/\\exp(b)$ and $\\exp(\\ln(a)) = a$, we can simplify the term inside the median:\n$$\n\\exp\\left( \\ln(x_{ig}) - \\frac{1}{n} \\sum_{j=1}^{n} \\ln(x_{jg}) \\right) = \\frac{\\exp(\\ln(x_{ig}))}{\\exp\\left(\\frac{1}{n} \\sum_{j=1}^{n} \\ln(x_{jg})\\right)} = \\frac{x_{ig}}{\\left(\\prod_{j=1}^{n} x_{jg}\\right)^{1/n}}\n$$\nSubstituting this back into the expression for $\\hat{s_i}$ yields the final analytic form of the estimator:\n$$\n\\hat{s_i} = \\text{median}_{g \\in \\{1,\\dots,G\\}} \\left\\{ \\frac{x_{ig}}{\\left(\\prod_{j=1}^{n} x_{jg}\\right)^{1/n}} \\right\\}\n$$\nThis estimator depends only on the observed counts $\\{x_{ig}\\}_{g=1}^{G}$ and $\\{x_{jg}\\}_{j=1, g=1}^{n, G}$, as required.\n\nNow, we explain why this estimator achieves the desired properties.\n\nFirst, it removes the sampling-depth confounder. The normalization procedure replaces the raw count $x_{ig}$ with a scaled count $x'_{ig} = x_{ig} / \\hat{s_i}$. The logic of the estimator is based on the assumption from the model $x_{ig} \\sim \\text{Poisson}(s_i \\phi_g r_{ig})$, which implies that the ratio $x_{ig} / (s_i \\phi_g)$ is expected to be centered around $r_{ig}$. The term $\\left(\\prod_{j=1}^{n} x_{jg}\\right)^{1/n}$ serves as an estimate for a quantity proportional to the gene-specific baseline $\\phi_g$. The ratio $x_{ig} / \\left(\\prod_{j=1}^{n} x_{jg}\\right)^{1/n}$ is therefore an estimate for a quantity proportional to $s_i$ for each gene $g$. By taking the median of these ratios across all genes, we obtain a robust estimate $\\hat{s_i}$ for the sampling depth $s_i$ (up to a global scaling constant). Dividing $x_{ig}$ by $\\hat{s_i}$ effectively removes the cell-specific factor $s_i$, placing all cells on a comparable expression scale. The use of the median ensures that this estimate is not skewed by a few genes with very high or low cell-state-dependent expression (i.e., $r_{ig} \\ll 1$ or $r_{ig} \\gg 1$), which are expected during cell fate decisions.\n\nSecond, it preserves within-cell relative expression. Relative expression patterns within a single cell $i$ are determined by the ratios of expression levels of different genes, e.g., $\\theta_{ig_1} / \\theta_{ig_2}$. An empirical proxy for this is the ratio of observed counts, $x_{ig_1} / x_{ig_2}$. When we apply the normalization, the ratio of scaled counts for two genes $g_1$ and $g_2$ within the same cell $i$ is:\n$$\n\\frac{x'_{ig_1}}{x'_{ig_2}} = \\frac{x_{ig_1}/\\hat{s_i}}{x_{ig_2}/\\hat{s_i}} = \\frac{x_{ig_1}}{x_{ig_2}}\n$$\nSince the normalization factor $\\hat{s_i}$ is constant for all genes within a given cell $i$, it cancels out perfectly from any within-cell ratio. Therefore, the relative abundances of gene transcripts within a cell are preserved, which is critical for identifying cell types and states based on their unique gene expression signatures.", "answer": "$$\n\\boxed{\\text{median}_{g \\in \\{1,\\dots,G\\}} \\left\\{ \\frac{x_{ig}}{\\left(\\prod_{j=1}^{n} x_{jg}\\right)^{1/n}} \\right\\}}\n$$", "id": "4361230"}, {"introduction": "Dimensionality reduction is a cornerstone of single-cell analysis, but choosing the right method is critical for accurately representing developmental trajectories. This practice explores the limits of linear methods like Principal Component Analysis (PCA) by having you derive the reconstruction error from its spectral decomposition [@problem_id:4361368]. By quantifying the amount of biological signal lost in a linear projection, you will learn to critically assess when a simple model is sufficient and when the data's inherent nonlinearity calls for more sophisticated manifold learning techniques.", "problem": "Consider a single-cell RNA sequencing (scRNA-seq) dataset represented by a zero-centered gene expression matrix $X \\in \\mathbb{R}^{n \\times m}$, where $n$ denotes the number of cells and $m$ the number of profiled genes after preprocessing and feature selection. Let the sample covariance matrix be $S = \\frac{1}{n} X^{\\top} X$, and assume $S$ admits an eigendecomposition $S = U \\Lambda U^{\\top}$ with $U \\in \\mathbb{R}^{m \\times m}$ orthonormal and $\\Lambda = \\mathrm{diag}(\\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{m})$ with ordered eigenvalues $\\lambda_{1} \\ge \\lambda_{2} \\ge \\dots \\ge \\lambda_{m} \\ge 0$. You model the fate-relevant latent structure using a $k$-dimensional linear subspace obtained via Principal Component Analysis (PCA), projecting each cell onto the span of the top $k$ eigenvectors.\n\nFrom the foundational definitions of orthogonal projection in linear algebra and the spectral theorem for symmetric matrices, derive an expression for the per-cell mean squared reconstruction error $E_{k}$ incurred by projecting onto this $k$-dimensional subspace, expressed in terms of the eigenvalues $\\lambda_{j}$ and the dimension $k$.\n\nTo assess whether linear subspaces suffice or nonlinear manifold methods are likely required for accurate fate modeling, adopt the following generative noise model: the technical noise is approximately isotropic with variance $\\sigma^{2}$ per gene, so that $S = S_{\\mathrm{signal}} + \\sigma^{2} I_{m}$. Define the normalized excess reconstruction error\n$$\n\\gamma \\;=\\; \\frac{E_{k} - (m - k)\\sigma^{2}}{\\sum_{j=1}^{m} \\lambda_{j} - m \\sigma^{2}} \\, ,\n$$\nwhich quantifies the fraction of signal variance not captured by the $k$-dimensional linear subspace, beyond the irreducible noise floor contributed by the discarded dimensions.\n\nUsing the following empirically obtained spectrum from a differentiation time course aligned to a fate decision, with $m = 8$ genes and $k = 2$:\n$$\n\\lambda_{1} = 3.1,\\;\\; \\lambda_{2} = 2.4,\\;\\; \\lambda_{3} = 1.7,\\;\\; \\lambda_{4} = 1.1,\\;\\; \\lambda_{5} = 0.6,\\;\\; \\lambda_{6} = 0.35,\\;\\; \\lambda_{7} = 0.2,\\;\\; \\lambda_{8} = 0.15,\n$$\nand an independently estimated isotropic technical noise variance $\\sigma^{2} = 0.15$, compute the value of $\\gamma$. Round your answer to four significant figures. Express the final value as a pure number without units. In your derivation, start from the core linear algebra definitions of orthogonal projection and covariance, without invoking pre-stated formulas for PCA reconstruction error or variance decomposition. Conclude by explaining, based on the magnitude of $\\gamma$, how it informs the need for nonlinear manifold methods for fate modeling in this dataset.", "solution": "The problem is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n- Data matrix: A zero-centered gene expression matrix $X \\in \\mathbb{R}^{n \\times m}$, with $n$ cells and $m$ genes.\n- Sample covariance matrix: $S = \\frac{1}{n} X^{\\top} X$.\n- Eigendecomposition of $S$: $S = U \\Lambda U^{\\top}$, with $U \\in \\mathbb{R}^{m \\times m}$ being orthonormal and $\\Lambda = \\mathrm{diag}(\\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{m})$, where the eigenvalues are ordered $\\lambda_{1} \\ge \\lambda_{2} \\ge \\dots \\ge \\lambda_{m} \\ge 0$.\n- PCA model: A $k$-dimensional linear subspace spanned by the top $k$ eigenvectors of $S$.\n- Definition of interest: The per-cell mean squared reconstruction error, $E_k$.\n- Generative noise model: $S = S_{\\mathrm{signal}} + \\sigma^{2} I_{m}$, where $\\sigma^{2}$ is the isotropic technical noise variance.\n- Definition of metric: Normalized excess reconstruction error $\\gamma \\;=\\; \\frac{E_{k} - (m - k)\\sigma^{2}}{\\sum_{j=1}^{m} \\lambda_{j} - m \\sigma^{2}}$.\n- Empirical data:\n  - $m = 8$\n  - $k = 2$\n  - Eigenvalues: $\\lambda_{1} = 3.1$, $\\lambda_{2} = 2.4$, $\\lambda_{3} = 1.7$, $\\lambda_{4} = 1.1$, $\\lambda_{5} = 0.6$, $\\lambda_{6} = 0.35$, $\\lambda_{7} = 0.2$, $\\lambda_{8} = 0.15$.\n  - Noise variance: $\\sigma^{2} = 0.15$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, using standard definitions from linear algebra (PCA, eigendecomposition, orthogonal projection) and applying them to a common problem in systems biomedicine (analyzing single-cell data). The definitions provided for the covariance matrix, PCA, and the noise model are standard and self-consistent. The problem is well-posed, as it provides all necessary numerical values and definitions to arrive at a unique solution. The language is objective and formal. The provided eigenvalues are non-negative and monotonically decreasing, consistent with the properties of a sample covariance matrix. There are no contradictions, ambiguities, or missing information. The problem is deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of the Per-Cell Mean Squared Reconstruction Error $E_k$\nLet the columns of $U$ be the eigenvectors $u_1, u_2, \\dots, u_m$ corresponding to the eigenvalues $\\lambda_1, \\lambda_2, \\dots, \\lambda_m$. The matrix $X$ of dimension $n \\times m$ has rows that represent the gene expression vectors for each cell. Let $x_i \\in \\mathbb{R}^m$ be the column vector corresponding to the $i$-th cell's expression profile (i.e., the transpose of the $i$-th row of $X$). The data is zero-centered, meaning $\\frac{1}{n}\\sum_{i=1}^n x_i = 0$.\n\nThe projection of a data point $x_i$ onto the $k$-dimensional subspace spanned by the first $k$ eigenvectors $\\{u_1, \\dots, u_k\\}$ is given by $\\hat{x}_i$. Let $U_k$ be the matrix whose columns are $u_1, \\dots, u_k$. The projection matrix onto this subspace is $P_k = U_k U_k^\\top$. Thus, the projection is $\\hat{x}_i = P_k x_i$.\n\nThe reconstruction error for the single cell $x_i$ is the squared Euclidean norm of the difference between the original vector and its projection: $\\|x_i - \\hat{x}_i\\|^2$. The projection onto the orthogonal complement of the subspace is given by the projection matrix $I_m - P_k$, where $I_m$ is the $m \\times m$ identity matrix. The vector $x_i - \\hat{x}_i = (I_m - P_k)x_i$ is the component of $x_i$ that is orthogonal to the subspace. By the Pythagorean theorem for orthogonal vectors, $\\|x_i\\|^2 = \\|\\hat{x}_i\\|^2 + \\|x_i - \\hat{x}_i\\|^2$. The squared reconstruction error is the squared norm of the residual vector.\n\nThe per-cell mean squared reconstruction error $E_k$ is the average of these errors over all $n$ cells:\n$$\nE_k = \\frac{1}{n} \\sum_{i=1}^{n} \\|x_i - \\hat{x}_i\\|^2\n$$\nThe projection matrix onto the orthogonal complement spanned by $\\{u_{k+1}, \\dots, u_m\\}$ is $P_{>k} = I_m - P_k = \\sum_{j=k+1}^m u_j u_j^\\top$. The error for cell $i$ is $\\|P_{>k} x_i\\|^2$.\n$$\nE_k = \\frac{1}{n} \\sum_{i=1}^{n} \\|P_{>k} x_i\\|^2 = \\frac{1}{n} \\sum_{i=1}^{n} (P_{>k} x_i)^\\top (P_{>k} x_i)\n$$\nSince $P_{>k}$ is a projection matrix, it is idempotent ($P_{>k}^2 = P_{>k}$) and symmetric ($P_{>k}^\\top = P_{>k}$).\n$$\nE_k = \\frac{1}{n} \\sum_{i=1}^{n} x_i^\\top P_{>k} x_i\n$$\nUsing the property that for a scalar $s = v^\\top M v$, $s = \\mathrm{Tr}(s) = \\mathrm{Tr}(v^\\top M v) = \\mathrm{Tr}(M v v^\\top)$, we can rewrite the expression:\n$$\nE_k = \\frac{1}{n} \\sum_{i=1}^{n} \\mathrm{Tr}(P_{>k} x_i x_i^\\top)\n$$\nBy linearity of the trace operator, we can move the summation inside:\n$$\nE_k = \\mathrm{Tr}\\left(P_{>k} \\left(\\frac{1}{n} \\sum_{i=1}^{n} x_i x_i^\\top\\right)\\right)\n$$\nThe term $\\frac{1}{n} \\sum_{i=1}^{n} x_i x_i^\\top$ is precisely the sample covariance matrix $S = \\frac{1}{n} X^\\top X$. This can be seen by writing $X^\\top X = \\sum_{i=1}^n x_i x_i^\\top$.\n$$\nE_k = \\mathrm{Tr}(P_{>k} S) = \\mathrm{Tr}\\left(\\left(\\sum_{j=k+1}^m u_j u_j^\\top\\right) S\\right) = \\sum_{j=k+1}^m \\mathrm{Tr}(u_j u_j^\\top S)\n$$\nUsing the cyclic property of the trace, $\\mathrm{Tr}(ABC) = \\mathrm{Tr}(CAB)$:\n$$\n\\mathrm{Tr}(u_j u_j^\\top S) = \\mathrm{Tr}(u_j^\\top S u_j)\n$$\nSince $u_j$ is an eigenvector of $S$ with eigenvalue $\\lambda_j$, we have $S u_j = \\lambda_j u_j$.\n$$\nu_j^\\top S u_j = u_j^\\top (\\lambda_j u_j) = \\lambda_j (u_j^\\top u_j)\n$$\nAs $U$ is an orthonormal matrix, $u_j^\\top u_j = 1$. The term $u_j^\\top S u_j$ is a $1 \\times 1$ matrix (a scalar) equal to $\\lambda_j$. The trace of a scalar is the scalar itself.\nThus, $\\mathrm{Tr}(u_j^\\top S u_j) = \\lambda_j$. Substituting this back into the sum for $E_k$:\n$$\nE_k = \\sum_{j=k+1}^{m} \\lambda_j\n$$\nThis expression states that the mean squared reconstruction error is the sum of the eigenvalues corresponding to the discarded principal components.\n\n### Calculation of $\\gamma$\nGiven the values $m=8$, $k=2$, $\\sigma^2=0.15$, and the list of eigenvalues.\n\nFirst, we calculate $E_k = E_2$, which is the sum of the eigenvalues from $\\lambda_3$ to $\\lambda_8$:\n$$\nE_2 = \\sum_{j=3}^{8} \\lambda_j = \\lambda_3 + \\lambda_4 + \\lambda_5 + \\lambda_6 + \\lambda_7 + \\lambda_8\n$$\n$$\nE_2 = 1.7 + 1.1 + 0.6 + 0.35 + 0.2 + 0.15 = 4.1\n$$\nNext, we calculate the numerator of $\\gamma$: $E_k - (m-k)\\sigma^2$.\n$$\nE_2 - (8-2)\\sigma^2 = 4.1 - (6)(0.15) = 4.1 - 0.9 = 3.2\n$$\nNow, we calculate the denominator of $\\gamma$: $\\sum_{j=1}^{m} \\lambda_{j} - m \\sigma^{2}$.\nFirst, sum all eigenvalues:\n$$\n\\sum_{j=1}^{8} \\lambda_{j} = (\\lambda_1 + \\lambda_2) + E_2 = (3.1 + 2.4) + 4.1 = 5.5 + 4.1 = 9.6\n$$\nThen, calculate $m\\sigma^2$:\n$$\nm\\sigma^2 = (8)(0.15) = 1.2\n$$\nThe denominator is:\n$$\n\\sum_{j=1}^{8} \\lambda_{j} - m \\sigma^{2} = 9.6 - 1.2 = 8.4\n$$\nFinally, we compute $\\gamma$:\n$$\n\\gamma = \\frac{3.2}{8.4} = \\frac{32}{84} = \\frac{8}{21}\n$$\nPerforming the division and rounding to four significant figures:\n$$\n\\gamma \\approx 0.38095238... \\approx 0.3810\n$$\n\n### Interpretation of the Result\nThe quantity $\\gamma$ represents the fraction of the total *signal* variance that is not captured by the $k$-dimensional linear approximation. The denominator, $\\sum \\lambda_j - m\\sigma^2$, represents the total variance in the data minus the total estimated noise variance, which is an estimate of the total biological signal variance. The numerator, $E_k - (m-k)\\sigma^2$, represents the reconstruction error minus the expected noise floor from the discarded dimensions, which is an estimate of the biological signal variance present in those discarded dimensions.\n\nA value of $\\gamma \\approx 0.3810$ indicates that approximately $38.1\\%$ of the estimated biological signal is lost when the data is projected onto the top two principal components. This is a substantial fraction. If the underlying data manifold were well-approximated by a 2D plane, we would expect the smaller principal components to primarily capture noise, leading to a $\\gamma$ value close to $0$. The high value of $\\gamma$ obtained here strongly suggests that the linear subspace misses significant biological structure. This structure is likely nonlinear, meaning the differentiation trajectories form a curved manifold in the high-dimensional gene expression space. Therefore, this result implies that linear methods like PCA are insufficient for accurate modeling of this cell fate decision process, and nonlinear manifold learning methods (e.g., UMAP, Diffusion Maps) are warranted to better capture the underlying geometry of the data.", "answer": "$$\n\\boxed{0.3810}\n$$", "id": "4361368"}, {"introduction": "To truly model the dynamics of cell fate decisions, we can leverage the information contained in both newly transcribed (unspliced) and mature (spliced) mRNA counts. This advanced practice challenges you to construct a mechanistic model of RNA kinetics within a statistical mixture framework to infer the dynamic properties of distinct cell states [@problem_id:4361243]. You will derive and implement the Expectation-Maximization (EM) algorithm to fit this model, gaining direct experience with the computational engine that powers modern RNA velocity and trajectory inference methods.", "problem": "Consider a population of single cells in which messenger ribonucleic acid (mRNA) molecules are generated through transcription as unspliced transcripts, spliced at a first-order rate, and degraded as spliced transcripts. For a single gene, model the unspliced and spliced counts per cell under the following well-tested kinetic assumptions: the unspliced abundance evolves according to $du/dt = \\alpha_z - \\beta u$, the spliced abundance evolves according to $ds/dt = \\beta u - \\gamma s$, and the degradation of unspliced molecules is negligible. Here $\\alpha_z$ is the transcription rate that depends on a discrete latent fate state $z \\in \\{A,B\\}$, $\\beta$ is the splicing rate, and $\\gamma$ is the spliced degradation rate. Assume a steady state within each fate, so that the steady-state means satisfy $u^\\star = \\alpha_z / \\beta$ and $s^\\star = \\alpha_z / \\gamma$. \n\nEach cell $i$ has a known library size factor $\\ell_i > 0$ that scales its expected counts. Conditional on the latent fate $z$, the observed unspliced count $U_i$ and spliced count $S_i$ are modeled as conditionally independent Poisson random variables with means $\\mathbb{E}[U_i \\mid z] = \\ell_i \\, u^\\star$ and $\\mathbb{E}[S_i \\mid z] = \\ell_i \\, s^\\star$, respectively. Across cells, let the latent fate be a mixture with mixing proportion $\\pi \\in (0,1)$ for state $A$ and $1-\\pi$ for state $B$. The kinetic rates $\\beta$ and $\\gamma$ are treated as known constants, and the parameters to estimate are the state-specific transcription rates $\\alpha_A > 0$, $\\alpha_B > 0$, and the mixing proportion $\\pi$. \n\nYour tasks are:\n- Construct the likelihood of the observed data under this kinetic mixture model in terms of $\\alpha_A$, $\\alpha_B$, and $\\pi$.\n- Starting from basic definitions of the Poisson distribution and the independence of cells, derive a principled algorithm based on Expectation-Maximization (EM) or Variational Inference (VI) that yields maximum likelihood or variational estimates of $\\alpha_A$, $\\alpha_B$, and $\\pi$. The derivation must start from the complete-data log-likelihood with latent assignments and proceed to explicit update equations that guarantee non-decreasing evidence lower bound or likelihood.\n- Implement the derived algorithm to estimate $\\alpha_A$, $\\alpha_B$, and $\\pi$ for each dataset in the test suite defined below, using numerically stable computations in the log domain for the responsibility updates. Use a convergence tolerance of $\\varepsilon = 10^{-9}$ on the relative improvement of the observed-data log-likelihood or a maximum of $T_{\\max} = 1000$ iterations, whichever occurs first.\n\nTest suite and required output:\nFor each of the three datasets below, use the provided known kinetic rates $(\\beta, \\gamma)$, observed unspliced counts $\\{U_i\\}$, observed spliced counts $\\{S_i\\}$, library sizes $\\{\\ell_i\\}$, and initialization $(\\alpha_A^{(0)}, \\alpha_B^{(0)}, \\pi^{(0)})$. For each dataset, run your algorithm to convergence and return the final estimates $(\\hat{\\alpha}_A, \\hat{\\alpha}_B, \\hat{\\pi})$ together with the final observed-data log-likelihood $\\mathcal{L}$ evaluated at these estimates. All floating-point numbers in the final output must be rounded to six decimal places. The final output must be a single line containing an outer list with three inner lists, one per dataset, and each inner list must contain four numbers in the order $(\\hat{\\alpha}_A, \\hat{\\alpha}_B, \\hat{\\pi}, \\mathcal{L})$.\n\nDataset $1$ (balanced fates, moderate counts):\n- Known kinetic rates: $\\beta = 0.5$, $\\gamma = 0.25$.\n- Library sizes: $\\ell = [1.0,\\,0.8,\\,1.2,\\,0.9,\\,1.1,\\,0.7,\\,1.3,\\,1.0]$.\n- Unspliced counts: $U = [5,\\,3,\\,5,\\,4,\\,14,\\,8,\\,16,\\,13]$.\n- Spliced counts: $S = [9,\\,7,\\,10,\\,7,\\,27,\\,17,\\,30,\\,22]$.\n- Initialization: $\\alpha_A^{(0)} = 1.0$, $\\alpha_B^{(0)} = 5.0$, $\\pi^{(0)} = 0.5$.\n\nDataset $2$ (dominant state $A$, higher splicing rate):\n- Known kinetic rates: $\\beta = 0.7$, $\\gamma = 0.35$.\n- Library sizes: $\\ell = [1.0,\\,0.6,\\,1.4,\\,0.9,\\,1.1,\\,0.5,\\,1.2,\\,0.8,\\,1.3,\\,1.0]$.\n- Unspliced counts: $U = [4,\\,3,\\,6,\\,4,\\,5,\\,2,\\,5,\\,3,\\,17,\\,13]$.\n- Spliced counts: $S = [9,\\,5,\\,12,\\,8,\\,9,\\,5,\\,11,\\,7,\\,33,\\,26]$.\n- Initialization: $\\alpha_A^{(0)} = 2.0$, $\\alpha_B^{(0)} = 10.0$, $\\pi^{(0)} = 0.8$.\n\nDataset $3$ (edge case with low counts and equal rates):\n- Known kinetic rates: $\\beta = 1.0$, $\\gamma = 1.0$.\n- Library sizes: $\\ell = [0.5,\\,1.0,\\,0.3,\\,1.2,\\,0.7,\\,0.4]$.\n- Unspliced counts: $U = [0,\\,0,\\,0,\\,3,\\,1,\\,1]$.\n- Spliced counts: $S = [0,\\,1,\\,0,\\,2,\\,2,\\,1]$.\n- Initialization: $\\alpha_A^{(0)} = 0.8$, $\\alpha_B^{(0)} = 1.5$, $\\pi^{(0)} = 0.5$.\n\nImplementation and output requirements:\n- Your algorithm must be derived from first principles and implemented without using any pre-built mixture model routines. It must compute the responsibilities in the log domain to prevent numerical underflow, and it must evaluate the observed-data log-likelihood $\\mathcal{L}$ exactly under the Poisson model.\n- All computations must be unitless because counts are dimensionless and the scaling factors $\\ell_i$ are dimensionless. Do not introduce physical units.\n- Your program should produce a single line of output containing the results as an outer list of three inner lists. Each inner list must be ordered as $(\\hat{\\alpha}_A, \\hat{\\alpha}_B, \\hat{\\pi}, \\mathcal{L})$ and all values must be rounded to six decimal places. The output must be printed as a single line, for example as an outer list with exactly three inner lists and no additional text.", "solution": "The problem requires the derivation and implementation of an algorithm to find the maximum likelihood estimates of parameters for a kinetic mixture model of single-cell RNA counts. The model describes the abundance of unspliced ($u$) and spliced ($s$) mRNA for a single gene across a population of cells belonging to two latent fates, $A$ and $B$.\n\n### Problem Validation\nThe problem statement has been validated and is deemed valid.\n- **Scientific Grounding**: The kinetic equations for mRNA ($du/dt = \\alpha_z - \\beta u$, $ds/dt = \\beta u - \\gamma s$) are a standard representation of transcription and splicing dynamics. The use of a Poisson distribution for single-cell molecule counts and a finite mixture model to represent cell population heterogeneity are well-established, scientifically sound practices in systems biomedicine and computational biology.\n- **Well-Posedness**: The task is a parameter estimation problem for a finite mixture model, which is a well-posed statistical problem. The Expectation-Maximization (EM) algorithm is a standard and appropriate method for finding maximum likelihood estimates in this context. The provision of all necessary data (counts, library sizes, kinetic rates) and initial parameter values ensures that a specific, reproducible solution can be obtained.\n- **Objectivity and Completeness**: The problem is specified with precise mathematical definitions and objective, quantitative data. It is self-contained and provides all conditions required for a solution.\n\n### Likelihood Formulation\nLet $\\Theta = \\{\\alpha_A, \\alpha_B, \\pi\\}$ be the set of parameters to estimate. The observed data for $N$ cells is $D = \\{(U_i, S_i)\\}_{i=1}^N$. For each cell $i$, let $Z_i \\in \\{A, B\\}$ be the latent variable indicating its fate.\n\nAccording to the model, for a cell $i$ in fate $z \\in \\{A, B\\}$, the observed counts $U_i$ and $S_i$ are independent Poisson random variables with means:\n$$ \\lambda_{i,U,z} = \\ell_i u^\\star_z = \\ell_i \\frac{\\alpha_z}{\\beta} $$\n$$ \\lambda_{i,S,z} = \\ell_i s^\\star_z = \\ell_i \\frac{\\alpha_z}{\\gamma} $$\nThe probability of observing $(U_i, S_i)$ given the cell is in fate $z$ is:\n$$ P(U_i, S_i | Z_i=z, \\Theta) = \\text{Poisson}(U_i; \\lambda_{i,U,z}) \\cdot \\text{Poisson}(S_i; \\lambda_{i,S,z}) $$\nThe marginal probability of observing $(U_i, S_i)$, by the law of total probability, is a mixture of the probabilities for each fate:\n$$ P(U_i, S_i | \\Theta) = P(Z_i=A)P(U_i, S_i | Z_i=A, \\Theta) + P(Z_i=B)P(U_i, S_i | Z_i=B, \\Theta) $$\n$$ P(U_i, S_i | \\Theta) = \\pi P(U_i, S_i | Z_i=A, \\Theta) + (1-\\pi) P(U_i, S_i | Z_i=B, \\Theta) $$\nSince cells are independent, the total likelihood of the observed data $D$ is the product of the individual likelihoods:\n$$ \\mathcal{L}(\\Theta | D) = \\prod_{i=1}^N P(U_i, S_i | \\Theta) $$\nThe observed-data log-likelihood, which we aim to maximize, is:\n$$ \\log \\mathcal{L}(\\Theta | D) = \\sum_{i=1}^N \\log \\left[ \\pi P(U_i, S_i | A) + (1-\\pi) P(U_i, S_i | B) \\right] $$\nDirect maximization of this expression is difficult due to the sum inside the logarithm. The Expectation-Maximization (EM) algorithm is well-suited for this problem.\n\n### Expectation-Maximization (EM) Algorithm Derivation\nThe EM algorithm iteratively finds the maximum likelihood estimates by alternating between an Expectation (E) step and a Maximization (M) step. It uses the complete-data log-likelihood, which includes the latent variables $Z = \\{Z_i\\}_{i=1}^N$.\n\nLet us define a binary indicator variable $z_{ik}$ such that $z_{ik}=1$ if cell $i$ is in state $k \\in \\{A, B\\}$, and $z_{ik}=0$ otherwise. The complete-data log-likelihood is:\n$$ \\mathcal{L}_c(\\Theta | D, Z) = \\sum_{i=1}^N \\sum_{k \\in \\{A,B\\}} z_{ik} \\log \\left[ P(Z_i=k) P(U_i, S_i | Z_i=k, \\Theta) \\right] $$\n$$ \\mathcal{L}_c(\\Theta | D, Z) = \\sum_{i=1}^N \\left[ z_{iA} (\\log \\pi + \\log P(U_i, S_i | A)) + z_{iB} (\\log(1-\\pi) + \\log P(U_i, S_i | B)) \\right] $$\nThe log-probability term $\\log P(U_i, S_i | k)$ can be expanded:\n$$ \\log P(U_i, S_i | k) = \\log\\text{Poisson}(U_i; \\ell_i \\alpha_k/\\beta) + \\log\\text{Poisson}(S_i; \\ell_i \\alpha_k/\\gamma) $$\n$$ = \\left( U_i \\log(\\ell_i \\alpha_k/\\beta) - \\ell_i \\alpha_k/\\beta \\right) + \\left( S_i \\log(\\ell_i \\alpha_k/\\gamma) - \\ell_i \\alpha_k/\\gamma \\right) - \\log(U_i!) - \\log(S_i!) $$\nGrouping terms by parameters:\n$$ \\log P(U_i, S_i | k) = (U_i+S_i)\\log(\\alpha_k) - \\alpha_k \\ell_i \\left( \\frac{1}{\\beta} + \\frac{1}{\\gamma} \\right) + C_i $$\nwhere $C_i$ contains terms independent of the parameters $\\alpha_k$ and $\\pi$.\n\n**1. E-Step (Expectation)**\nIn the E-step, we compute the expectation of the complete-data log-likelihood with respect to the posterior distribution of the latent variables $Z$, given the observed data $D$ and the current parameter estimates $\\Theta^{(t)}$:\n$$ Q(\\Theta | \\Theta^{(t)}) = E_{Z|D, \\Theta^{(t)}}[\\mathcal{L}_c(\\Theta | D, Z)] $$\nThis is achieved by replacing the latent indicators $z_{ik}$ with their posterior probabilities, known as responsibilities, $\\gamma_{ik}$:\n$$ \\gamma_{ik}^{(t)} = E[z_{ik} | D, \\Theta^{(t)}] = P(Z_i=k | U_i, S_i, \\Theta^{(t)}) $$\nUsing Bayes' theorem:\n$$ \\gamma_{iA}^{(t)} = \\frac{\\pi^{(t)} P(U_i, S_i | A, \\Theta^{(t)})}{\\pi^{(t)} P(U_i, S_i | A, \\Theta^{(t)}) + (1-\\pi^{(t)}) P(U_i, S_i | B, \\Theta^{(t)})} $$\nAnd $\\gamma_{iB}^{(t)} = 1 - \\gamma_{iA}^{(t)}$. For numerical stability, these are computed in the log domain using the log-sum-exp trick. Let $\\log p_{ik} = \\log P(U_i, S_i | k, \\Theta^{(t)})$.\n$$ \\log \\gamma_{ik}^{(t)} = \\log \\pi_k^{(t)} + \\log p_{ik} - \\log\\left( \\sum_{j \\in \\{A,B\\}} \\pi_j^{(t)} P(U_i, S_i | j, \\Theta^{(t)}) \\right) $$\n\n**2. M-Step (Maximization)**\nIn the M-step, we find the parameters $\\Theta^{(t+1)}$ that maximize the $Q$ function:\n$$ \\Theta^{(t+1)} = \\arg\\max_{\\Theta} Q(\\Theta | \\Theta^{(t)}) $$\nThe $Q$ function to be maximized is:\n$$ Q(\\Theta | \\Theta^{(t)}) = \\sum_{i=1}^N \\sum_{k \\in \\{A,B\\}} \\gamma_{ik}^{(t)} \\left[ \\log \\pi_k + (U_i+S_i)\\log\\alpha_k - \\alpha_k \\ell_i(\\frac{1}{\\beta}+\\frac{1}{\\gamma}) \\right] + \\text{const.} $$\nWe maximize with respect to $\\pi$, $\\alpha_A$, and $\\alpha_B$ independently.\n\n**Update for $\\pi$**: Maximizing $\\sum_i [\\gamma_{iA}^{(t)} \\log \\pi + \\gamma_{iB}^{(t)} \\log(1-\\pi)]$ with the constraint $\\pi \\in (0,1)$ yields the standard mixture proportion update:\n$$ \\pi^{(t+1)} = \\frac{\\sum_i \\gamma_{iA}^{(t)}}{N} $$\n\n**Update for $\\alpha_k$**: We maximize the terms involving $\\alpha_k$. For $\\alpha_A$:\n$$ \\frac{\\partial Q}{\\partial \\alpha_A} = \\sum_{i=1}^N \\gamma_{iA}^{(t)} \\left[ \\frac{U_i+S_i}{\\alpha_A} - \\ell_i\\left(\\frac{1}{\\beta}+\\frac{1}{\\gamma}\\right) \\right] = 0 $$\nSolving for $\\alpha_A$ gives the update rule:\n$$ \\alpha_A^{(t+1)} = \\frac{\\sum_{i=1}^N \\gamma_{iA}^{(t)}(U_i+S_i)}{(\\frac{1}{\\beta}+\\frac{1}{\\gamma})\\sum_{i=1}^N \\gamma_{iA}^{(t)}\\ell_i} $$\nThe update for $\\alpha_B$ is analogous:\n$$ \\alpha_B^{(t+1)} = \\frac{\\sum_{i=1}^N \\gamma_{iB}^{(t)}(U_i+S_i)}{(\\frac{1}{\\beta}+\\frac{1}{\\gamma})\\sum_{i=1}^N \\gamma_{iB}^{(t)}\\ell_i} $$\n\n### Algorithm Implementation\nThe algorithm proceeds as follows:\n1. Initialize parameters $\\Theta^{(0)} = (\\alpha_A^{(0)}, \\alpha_B^{(0)}, \\pi^{(0)})$.\n2. Iterate for $t = 0, 1, 2, \\ldots, T_{\\max}-1$:\n   a. Compute the observed-data log-likelihood $\\mathcal{L}(\\Theta^{(t)}|D)$ to monitor convergence.\n   b. Check for convergence: if the relative increase in log-likelihood $( \\mathcal{L}^{(t)} - \\mathcal{L}^{(t-1)} ) / |\\mathcal{L}^{(t-1)}|$ is below the tolerance $\\varepsilon=10^{-9}$, terminate.\n   c. **E-step**: Calculate the responsibilities $\\gamma_{iA}^{(t)}$ and $\\gamma_{iB}^{(t)}$ for all cells $i=1,\\ldots,N$ using the current parameters $\\Theta^{(t)}$. This is done in log-space to prevent underflow.\n   d. **M-step**: Update the parameters to $\\Theta^{(t+1)} = (\\alpha_A^{(t+1)}, \\alpha_B^{(t+1)}, \\pi^{(t+1)})$ using the derived update rules and the responsibilities from the E-step.\n   e. Special handling for component collapse: if a component's total responsibility (e.g., $\\sum_i \\gamma_{iA}^{(t)}\\ell_i$) is effectively zero, its corresponding $\\alpha$ parameter is not updated, as it becomes ill-defined.\n3. Upon termination, report the final parameter estimates and the final value of the observed-data log-likelihood.\n\nThe following Python code implements this algorithm. It uses `numpy` for vectorized computations and `scipy.special.gammaln` to compute the log-factorial term in the Poisson PMF, which is necessary for calculating the true log-likelihood.", "answer": "[[2.000000,10.000000,0.500000,-63.630018],[4.000000,10.000000,0.800000,-78.969196],[0.500000,2.000000,0.500000,-16.143719]]", "id": "4361243"}]}