{"hands_on_practices": [{"introduction": "A fundamental way to characterize any dynamic process is through its temporal memory, quantified by the autocorrelation function (ACF). This exercise guides you through the analytical derivation of the ACF for the foundational two-state promoter model. By completing this practice [@problem_id:4388478], you will not only sharpen your skills with continuous-time Markov chains but also uncover a crucial limitation regarding parameter identifiability from time-series data alone.", "problem": "A single gene promoter in a mammalian cell toggles stochastically between an inactive state and an active state according to a continuous-time two-state Markov process (telegraph model). Let the promoter state be $S(t) \\in \\{0,1\\}$, where $S(t)=1$ denotes the active state and $S(t)=0$ denotes the inactive state. Transitions occur with rates $k_{on}$ for $0 \\to 1$ and $k_{off}$ for $1 \\to 0$. Assume the process is stationary and ergodic. You can idealize the measurement as a sufficiently fast optical reporter such that the measured signal is directly $S(t)$.\n\nStarting from first principles for continuous-time Markov chains and the definition of autocorrelation for a weakly stationary process, derive the normalized autocorrelation function $C(\\tau)$ of the centered promoter signal $S(t)-\\mathbb{E}[S(t)]$ as a function of the time lag $\\tau \\ge 0$ and the parameters $k_{on}$ and $k_{off}$. Then compute the local sensitivities of $C(\\tau)$ to each parameter, defined as $S_{on}(\\tau) \\equiv \\partial C(\\tau)/\\partial k_{on}$ and $S_{off}(\\tau) \\equiv \\partial C(\\tau)/\\partial k_{off}$, and form the ratio\n$$\nR(\\tau) \\equiv \\frac{S_{on}(\\tau)}{S_{off}(\\tau)} \\, .\n$$\n\nUse your derivation to discuss whether the shape of the normalized autocorrelation function alone can identify $k_{on}$ and $k_{off}$ separately, and propose a single complementary steady-state measurement that, when combined with the autocorrelation shape, renders the pair $(k_{on},k_{off})$ identifiable. For the calculation part, report the closed-form analytic expression for $R(\\tau)$ only. No rounding is required and no units are needed for $R(\\tau)$.", "solution": "The user wants me to solve a problem about a stochastic two-state gene promoter model.\n\nHere is a plan:\n1.  **Validate the Problem:**\n    *   **Step 1: Extract Givens:** I will list all the information provided in the problem statement.\n        *   Model: Continuous-time two-state Markov process (telegraph model).\n        *   States: $S(t) \\in \\{0, 1\\}$, where $0$ is inactive and $1$ is active.\n        *   Transition rates: $k_{on}$ for $0 \\to 1$, $k_{off}$ for $1 \\to 0$.\n        *   Process properties: Stationary and ergodic.\n        *   Measurement: Idealized, directly measures $S(t)$.\n        *   Task 1: Derive the normalized autocorrelation function $C(\\tau)$ of the centered signal $S(t)-\\mathbb{E}[S(t)]$.\n        *   Task 2: Compute local sensitivities $S_{on}(\\tau) = \\partial C(\\tau)/\\partial k_{on}$ and $S_{off}(\\tau) = \\partial C(\\tau)/\\partial k_{off}$.\n        *   Task 3: Form the ratio $R(\\tau) = S_{on}(\\tau) / S_{off}(\\tau)$.\n        *   Task 4: Discuss parameter identifiability from $C(\\tau)$ alone.\n        *   Task 5: Propose a complementary measurement for full identifiability.\n        *   Task 6: Report the closed-form expression for $R(\\tau)$ as the final answer.\n\n    *   **Step 2: Validate:** I will check the problem against my validation criteria.\n        *   *Scientifically Grounded*: Yes, the two-state telegraph model is a cornerstone of stochastic gene expression modeling. It's a standard and well-accepted model in systems biology and biophysics. All concepts (Markov process, autocorrelation, sensitivities) are standard in mathematics and physics.\n        *   *Well-Posed*: Yes, the problem is well-defined. It asks for the derivation of a standard quantity (autocorrelation) for a standard model and then to perform standard mathematical operations (differentiation) and analysis (identifiability). The stationarity assumption is key. A unique solution exists.\n        *   *Objective*: Yes, the language is precise, technical, and free of subjectivity.\n        *   *No Flaws*:\n            1.  *Scientific/Factual Unsoundness*: None.\n            2.  *Non-Formalizable/Irrelevant*: It's a formal problem directly in the specified field.\n            3.  *Incomplete/Contradictory*: The problem provides all necessary information. The stationary assumption allows for the calculation of steady-state probabilities and the autocorrelation function.\n            4.  *Unrealistic/Infeasible*: The model is an idealization, but a scientifically valuable and widely used one. The \"idealized measurement\" is a standard simplifying assumption for theoretical derivations.\n            5.  *Ill-Posed*: No, the problem is a standard textbook-style derivation.\n            6.  *Pseudo-Profound/Trivial*: The derivation requires several steps of logical reasoning based on probability theory and calculus. It is not trivial.\n            7.  *Outside Scientific Verifiability*: The results can be verified mathematically and compared to simulations of the stochastic process.\n\n    *   **Step 3: Verdict:** The problem is **valid**. I will proceed with the solution.\n\n2.  **Solve the Problem:**\n    *   **Part 1: Steady-state probabilities.**\n        *   The process is a continuous-time Markov chain on the state space $\\{0, 1\\}$.\n        *   Let $p_0(t) = \\mathbb{P}(S(t)=0)$ and $p_1(t) = \\mathbb{P}(S(t)=1)$.\n        *   The Master Equation is:\n            $$ \\frac{d p_1}{dt} = k_{on} p_0(t) - k_{off} p_1(t) $$\n            $$ \\frac{d p_0}{dt} = k_{off} p_1(t) - k_{on} p_0(t) $$\n        *   With constraint $p_0(t) + p_1(t) = 1$.\n        *   At stationarity, $d p_1/dt = 0$ and $d p_0/dt = 0$. Let the steady-state probabilities be $\\pi_0$ and $\\pi_1$.\n        *   $k_{on} \\pi_0 - k_{off} \\pi_1 = 0$.\n        *   Also, $\\pi_0 + \\pi_1 = 1$.\n        *   Substituting $\\pi_0 = 1 - \\pi_1$: $k_{on} (1 - \\pi_1) - k_{off} \\pi_1 = 0 \\implies k_{on} - k_{on} \\pi_1 - k_{off} \\pi_1 = 0 \\implies k_{on} = (k_{on} + k_{off}) \\pi_1$.\n        *   So, $\\pi_1 = \\frac{k_{on}}{k_{on} + k_{off}}$.\n        *   And $\\pi_0 = 1 - \\pi_1 = 1 - \\frac{k_{on}}{k_{on} + k_{off}} = \\frac{k_{off}}{k_{on} + k_{off}}$.\n        *   These are the stationary probabilities.\n\n    *   **Part 2: Expectation and Variance of S(t).**\n        *   The signal is $S(t) \\in \\{0, 1\\}$.\n        *   The expected value under stationary conditions is $\\mathbb{E}[S(t)] = 0 \\cdot \\pi_0 + 1 \\cdot \\pi_1 = \\pi_1$.\n        *   $\\mathbb{E}[S(t)] = \\frac{k_{on}}{k_{on} + k_{off}}$. Let's call this $\\mu_S$.\n        *   The variance of the signal is $\\text{Var}(S(t)) = \\mathbb{E}[S(t)^2] - (\\mathbb{E}[S(t)])^2$.\n        *   Since $S(t)$ can only be $0$ or $1$, $S(t)^2 = S(t)$.\n        *   So $\\mathbb{E}[S(t)^2] = \\mathbb{E}[S(t)] = \\pi_1$.\n        *   $\\text{Var}(S(t)) = \\pi_1 - \\pi_1^2 = \\pi_1(1-\\pi_1) = \\pi_1 \\pi_0$.\n        *   $\\text{Var}(S(t)) = \\frac{k_{on}}{k_{on} + k_{off}} \\frac{k_{off}}{k_{on} + k_{off}} = \\frac{k_{on} k_{off}}{(k_{on} + k_{off})^2}$.\n\n    *   **Part 3: Autocorrelation function.**\n        *   The problem asks for the normalized autocorrelation of the centered signal $X(t) = S(t) - \\mu_S$.\n        *   The unnormalized autocorrelation of $X(t)$ for a lag $\\tau \\ge 0$ is $R_X(\\tau) = \\mathbb{E}[X(t+\\tau)X(t)]$.\n        *   $R_X(\\tau) = \\mathbb{E}[(S(t+\\tau) - \\mu_S)(S(t) - \\mu_S)] = \\mathbb{E}[S(t+\\tau)S(t)] - \\mu_S \\mathbb{E}[S(t+\\tau)] - \\mu_S \\mathbb{E}[S(t)] + \\mu_S^2$.\n        *   Since the process is stationary, $\\mathbb{E}[S(t+\\tau)] = \\mathbb{E}[S(t)] = \\mu_S$.\n        *   So, $R_X(\\tau) = \\mathbb{E}[S(t+\\tau)S(t)] - \\mu_S^2 - \\mu_S^2 + \\mu_S^2 = \\mathbb{E}[S(t+\\tau)S(t)] - \\mu_S^2$.\n        *   This is the auto-covariance function of $S(t)$, denoted $\\text{Cov}(S(t+\\tau), S(t))$.\n        *   Now I need to compute $\\mathbb{E}[S(t+\\tau)S(t)]$.\n        *   $\\mathbb{E}[S(t+\\tau)S(t)] = \\sum_{i,j \\in \\{0,1\\}} (j \\cdot i) \\mathbb{P}(S(t+\\tau)=j, S(t)=i)$.\n        *   The only non-zero term is for $i=1, j=1$.\n        *   So, $\\mathbb{E}[S(t+\\tau)S(t)] = \\mathbb{P}(S(t+\\tau)=1, S(t)=1)$.\n        *   Using conditional probability: $\\mathbb{P}(S(t+\\tau)=1, S(t)=1) = \\mathbb{P}(S(t+\\tau)=1 | S(t)=1) \\mathbb{P}(S(t)=1)$.\n        *   $\\mathbb{P}(S(t)=1) = \\pi_1$.\n        *   I need to find the transition probability $P_{11}(\\tau) = \\mathbb{P}(S(t+\\tau)=1 | S(t)=1)$.\n        *   The transition probabilities $P_{ij}(\\tau) = \\mathbb{P}(S(t+\\tau)=j | S(t)=i)$ can be found by solving the Master Equation with a specific initial condition.\n        *   Let $p_1(t)$ be the probability of being in state 1 at time $t$. The equation is $\\frac{dp_1}{dt} = k_{on} p_0 - k_{off} p_1$.\n        *   Using $p_0 = 1 - p_1$, we get $\\frac{dp_1}{dt} = k_{on} (1 - p_1) - k_{off} p_1 = k_{on} - (k_{on} + k_{off}) p_1$.\n        *   This is a linear first-order ODE. Let $k = k_{on} + k_{off}$. $\\frac{dp_1}{dt} + k p_1 = k_{on}$.\n        *   The general solution is $p_1(t) = A e^{-kt} + \\frac{k_{on}}{k}$.\n        *   To find $P_{11}(\\tau)$, we want the solution starting from state 1, so $p_1(0)=1$. We are solving for $p_1(\\tau)$.\n        *   $1 = A e^0 + \\frac{k_{on}}{k} \\implies A = 1 - \\frac{k_{on}}{k} = \\frac{k-k_{on}}{k} = \\frac{k_{off}}{k}$.\n        *   So, $P_{11}(\\tau) = \\frac{k_{off}}{k_{on}+k_{off}} e^{-(k_{on}+k_{off})\\tau} + \\frac{k_{on}}{k_{on}+k_{off}}$.\n        *   $P_{11}(\\tau) = \\pi_0 e^{-(k_{on}+k_{off})\\tau} + \\pi_1$.\n        *   Now, $\\mathbb{E}[S(t+\\tau)S(t)] = P_{11}(\\tau) \\pi_1 = (\\pi_0 e^{-(k_{on}+k_{off})\\tau} + \\pi_1) \\pi_1 = \\pi_0 \\pi_1 e^{-(k_{on}+k_{off})\\tau} + \\pi_1^2$.\n        *   The auto-covariance is $R_X(\\tau) = \\mathbb{E}[S(t+\\tau)S(t)] - \\mu_S^2 = (\\pi_0 \\pi_1 e^{-(k_{on}+k_{off})\\tau} + \\pi_1^2) - \\pi_1^2 = \\pi_0 \\pi_1 e^{-(k_{on}+k_{off})\\tau}$.\n        *   The auto-covariance at lag $\\tau=0$ is $R_X(0) = \\pi_0 \\pi_1$. This is the variance $\\text{Var}(S(t))$, as expected.\n        *   The normalized autocorrelation function is $C(\\tau) = \\frac{R_X(\\tau)}{R_X(0)}$.\n        *   $C(\\tau) = \\frac{\\pi_0 \\pi_1 e^{-(k_{on}+k_{off})\\tau}}{\\pi_0 \\pi_1} = e^{-(k_{on}+k_{off})\\tau}$.\n        *   So, $C(\\tau) = \\exp(-(k_{on}+k_{off})\\tau)$. This is a simple exponential decay.\n\n    *   **Alternative way to derive transition probabilities using the generator matrix.**\n        *   The generator matrix (or rate matrix) $Q$ is:\n            $$ Q = \\begin{pmatrix} -k_{on} & k_{on} \\\\ k_{off} & -k_{off} \\end{pmatrix} $$\n            (rows are FROM state, columns are TO state; or the transpose, need to be careful. Let's say $\\frac{d\\vec{p}}{dt} = \\vec{p} Q$, where $\\vec{p} = (p_0, p_1)$ is a row vector.\n            $\\frac{dp_0}{dt} = -k_{on} p_0 + k_{off} p_1$.\n            $\\frac{dp_1}{dt} = k_{on} p_0 - k_{off} p_1$.\n            This matches my master equation. So the row vector convention is correct).\n        *   The transition probability matrix is $P(\\tau) = \\exp(Q\\tau)$.\n        *   Eigenvalues of $Q$: $\\det(Q - \\lambda I) = (-k_{on}-\\lambda)(-k_{off}-\\lambda) - k_{on}k_{off} = \\lambda^2 + (k_{on}+k_{off})\\lambda + k_{on}k_{off} - k_{on}k_{off} = \\lambda(\\lambda + k_{on}+k_{off}) = 0$.\n        *   Eigenvalues are $\\lambda_1 = 0$ and $\\lambda_2 = -(k_{on}+k_{off})$.\n        *   The eigenvalue $\\lambda_1 = 0$ corresponds to the stationary distribution.\n        *   The time evolution is governed by the non-zero eigenvalue.\n        *   The autocorrelation function for a stationary two-state Markov process is known to be a single exponential with a rate equal to the magnitude of the non-zero eigenvalue.\n        *   $C(\\tau) = \\exp(\\lambda_2 \\tau) = \\exp(-(k_{on}+k_{off})\\tau)$.\n        *   This confirms my previous derivation. The result is correct.\n\n    *   **Part 4: Sensitivities.**\n        *   $C(\\tau) = \\exp(-(k_{on}+k_{off})\\tau)$.\n        *   $S_{on}(\\tau) = \\frac{\\partial C(\\tau)}{\\partial k_{on}} = \\frac{\\partial}{\\partial k_{on}} \\exp(-(k_{on}+k_{off})\\tau)$.\n        *   Using the chain rule: $S_{on}(\\tau) = \\exp(-(k_{on}+k_{off})\\tau) \\cdot (-\\tau)$.\n        *   $S_{off}(\\tau) = \\frac{\\partial C(\\tau)}{\\partial k_{off}} = \\frac{\\partial}{\\partial k_{off}} \\exp(-(k_{on}+k_{off})\\tau)$.\n        *   Using the chain rule: $S_{off}(\\tau) = \\exp(-(k_{on}+k_{off})\\tau) \\cdot (-\\tau)$.\n        *   They are identical.\n\n    *   **Part 5: Ratio R(Ï„).**\n        *   $R(\\tau) = \\frac{S_{on}(\\tau)}{S_{off}(\\tau)} = \\frac{-\\tau \\exp(-(k_{on}+k_{off})\\tau)}{-\\tau \\exp(-(k_{on}+k_{off})\\tau)}$.\n        *   For $\\tau \\neq 0$, $R(\\tau) = 1$. The problem statement implies $\\tau \\ge 0$. The derivatives are not defined at $\\tau=0$ (the function is not differentiable there). However, if we take the limit as $\\tau \\to 0^+$, the ratio is still 1. Let's assume $\\tau > 0$ for the ratio.\n        *   The final answer is just the expression for $R(\\tau)$. So, it's just $1$. This seems too simple. Let me re-read the problem very carefully.\n        *   \"normalized autocorrelation function $C(\\tau)$ of the centered promoter signal $S(t)-\\mathbb{E}[S(t)]$\"\n        *   Yes, this is what I calculated. $C(\\tau) = \\frac{\\text{Cov}(S(t+\\tau), S(t))}{\\text{Var}(S(t))}$.\n        *   My derivation seems sound. $\\text{Cov}(S(t+\\tau), S(t)) = \\pi_1 \\pi_0 e^{-(k_{on}+k_{off})\\tau}$ and $\\text{Var}(S(t)) = \\pi_1 \\pi_0$. The division cancels the prefactors.\n        *   The problem is standard. My derivation of $C(\\tau)$ is standard. The result is a single exponential decay with rate $k = k_{on}+k_{off}$.\n        *   Therefore, $C(\\tau) = \\exp(-k\\tau)$ where $k=k_{on}+k_{off}$.\n        *   $\\frac{\\partial C}{\\partial k_{on}} = \\frac{\\partial C}{\\partial k} \\frac{\\partial k}{\\partial k_{on}} = (-\\tau e^{-k\\tau}) \\cdot 1 = -\\tau e^{-(k_{on}+k_{off})\\tau}$.\n        *   $\\frac{\\partial C}{\\partial k_{off}} = \\frac{\\partial C}{\\partial k} \\frac{\\partial k}{\\partial k_{off}} = (-\\tau e^{-k\\tau}) \\cdot 1 = -\\tau e^{-(k_{on}+k_{off})\\tau}$.\n        *   The ratio is indeed 1 for $\\tau > 0$.\n        *   Perhaps I misinterpreted the \"normalized autocorrelation function\". Could it be something else?\n        *   In some contexts, normalization might mean something different, but in time series analysis, it almost universally means dividing the auto-covariance by the variance.\n        *   Let's consider if the question meant the *unnormalized* autocorrelation of the centered signal (i.e., the auto-covariance). Let's call it $C_{\\text{un}}(\\tau) = \\pi_1\\pi_0 e^{-(k_{on}+k_{off})\\tau} = \\frac{k_{on}k_{off}}{(k_{on}+k_{off})^2} e^{-(k_{on}+k_{off})\\tau}$.\n        *   Let's compute the sensitivities for this one.\n        *   $S'_{on} = \\frac{\\partial C_{\\text{un}}}{\\partial k_{on}} = \\frac{k_{off}(k_{on}+k_{off})^2 - k_{on}k_{off}(2(k_{on}+k_{off}))}{(k_{on}+k_{off})^4} e^{-(k_{on}+k_{off})\\tau} + \\frac{k_{on}k_{off}}{(k_{on}+k_{off})^2} (-\\tau) e^{-(k_{on}+k_{off})\\tau}$\n        *   $S'_{on} = \\frac{k_{off}(k_{on}+k_{off}) - 2k_{on}k_{off}}{(k_{on}+k_{off})^3} e^{-(k_{on}+k_{off})\\tau} - \\frac{\\tau k_{on}k_{off}}{(k_{on}+k_{off})^2} e^{-(k_{on}+k_{off})\\tau}$\n        *   $S'_{on} = \\frac{k_{off}^2 - k_{on}k_{off}}{(k_{on}+k_{off})^3} e^{-(k_{on}+k_{off})\\tau} - \\frac{\\tau k_{on}k_{off}}{(k_{on}+k_{off})^2} e^{-(k_{on}+k_{off})\\tau}$\n        *   $S'_{on} = \\frac{k_{off}(k_{off} - k_{on}) - \\tau k_{on}k_{off}(k_{on}+k_{off})}{(k_{on}+k_{off})^3} e^{-(k_{on}+k_{off})\\tau}$\n        *   By symmetry, $S'_{off}$ would be:\n        *   $S'_{off} = \\frac{k_{on}(k_{on} - k_{off}) - \\tau k_{on}k_{off}(k_{on}+k_{off})}{(k_{on}+k_{off})^3} e^{-(k_{on}+k_{off})\\tau}$\n        *   The ratio $S'_{on} / S'_{off}$ would be:\n        *   $\\frac{k_{off}(k_{off} - k_{on}) - \\tau k_{on}k_{off}(k_{on}+k_{off})}{k_{on}(k_{on} - k_{off}) - \\tau k_{on}k_{off}(k_{on}+k_{off})}$.\n        *   This is a complicated expression. The problem explicitly says \"normalized autocorrelation function\". In signal processing and statistics, this means $C(\\tau) = \\text{ACF}(\\tau) / \\text{ACF}(0)$. My first derivation is standard and correct.\n        *   The fact that the result is simple doesn't mean it's wrong. In fact, it leads directly into the discussion part of the question.\n        *   I will stick to my first derivation. The result $R(\\tau)=1$ is correct based on the standard definition of the normalized autocorrelation function.\n\n    *   **Part 6: Discussion on identifiability.**\n        *   The normalized autocorrelation function is $C(\\tau) = \\exp(-(k_{on}+k_{off})\\tau)$.\n        *   When we fit this function to experimental data (e.g., from Fluorescence Correlation Spectroscopy, FCS), we can only determine the decay rate, which is the sum $k_{decay} = k_{on} + k_{off}$.\n        *   We cannot determine $k_{on}$ and $k_{off}$ separately from the *shape* of the autocorrelation curve. The shape is defined entirely by the sum $k_{on}+k_{off}$. Any pair $(k_{on}', k_{off}')$ such that $k_{on}' + k_{off}' = k_{on} + k_{off}$ will produce the same normalized autocorrelation function.\n        *   This is the core of the identifiability problem. The shape of $C(\\tau)$ only gives us one constraint on two parameters.\n        *   The problem asks to propose a complementary *steady-state* measurement.\n        *   What steady-state quantities do we have?\n            *   The mean promoter activity: $\\mu_S = \\mathbb{E}[S(t)] = \\pi_1 = \\frac{k_{on}}{k_{on}+k_{off}}$.\n            *   The variance of promoter activity: $\\text{Var}(S(t)) = \\pi_1 \\pi_0 = \\frac{k_{on}k_{off}}{(k_{on}+k_{off})^2}$.\n        *   Let's see if combining the ACF shape with one of these helps.\n        *   From the ACF shape, we get the value of $k = k_{on} + k_{off}$.\n        *   If we also measure the mean activity, $\\mu_S$, we have a system of two equations for two unknowns:\n            1.  $k_{on} + k_{off} = k$\n            2.  $\\frac{k_{on}}{k_{on}+k_{off}} = \\mu_S$\n        *   From (2), we get $k_{on} / k = \\mu_S$, so $k_{on} = k \\mu_S$.\n        *   Substituting this into (1): $k \\mu_S + k_{off} = k \\implies k_{off} = k(1 - \\mu_S)$.\n        *   This gives unique solutions for $k_{on}$ and $k_{off}$:\n            *   $k_{on} = (k_{on}+k_{off}) \\frac{k_{on}}{k_{on}+k_{off}}$\n            *   $k_{off} = (k_{on}+k_{off}) \\frac{k_{off}}{k_{on}+k_{off}}$\n        *   So, if we measure the mean promoter occupancy (fraction of time the promoter is active) in addition to the autocorrelation function, the system becomes identifiable.\n        *   What if we used the variance instead?\n            1.  $k_{on} + k_{off} = k$\n            2.  $\\frac{k_{on}k_{off}}{(k_{on}+k_{off})^2} = \\text{Var}(S)$\n        *   From (2), $\\frac{k_{on}k_{off}}{k^2} = \\text{Var}(S) \\implies k_{on}k_{off} = k^2 \\text{Var}(S)$.\n        *   We have the sum $k_{on}+k_{off}=k$ and the product $k_{on}k_{off} = P = k^2 \\text{Var}(S)$.\n        *   $k_{on}$ and $k_{off}$ are the roots of the quadratic equation $x^2 - (\\text{sum of roots})x + (\\text{product of roots}) = 0$.\n        *   $x^2 - kx + P = 0$.\n        *   The roots are $x = \\frac{k \\pm \\sqrt{k^2 - 4P}}{2}$.\n        *   So we get two solutions, one for $k_{on}$ and one for $k_{off}$. But which is which? We get the set $\\{k_{on}, k_{off}\\}$, but we cannot assign them. E.g., if the roots are $2$ and $5$, we don't know if $(k_{on}, k_{off}) = (2,5)$ or $(5,2)$.\n        *   So, measuring the variance is not sufficient to uniquely identify the pair $(k_{on}, k_{off})$. It only identifies the set $\\{k_{on}, k_{off}\\}$.\n        *   However, measuring the mean $\\mu_S$ *is* sufficient. $\\mu_S = k_{on} / (k_{on}+k_{off})$. This ratio is not symmetric w.r.t swapping $k_{on}$ and $k_{off}$.\n        *   Therefore, the proposed complementary measurement is the steady-state mean activity, $\\mathbb{E}[S(t)] = \\pi_1$. Experimentally this corresponds to measuring the average reporter signal over a long time in a large population of cells (due to ergodicity).\n\n    *   **Structure the full response.**\n        *   Start with the problem validation. It's valid.\n        *   Begin the solution.\n        *   Define the states and rates. Write down the master equation.\n        *   Solve for the stationary probabilities $\\pi_0$ and $\\pi_1$.\n        *   Calculate the mean $\\mu_S = \\mathbb{E}[S(t)]$ and variance $\\text{Var}(S(t))$. Note that $\\text{Var}(S(t))$ is the auto-covariance at $\\tau=0$.\n        *   Derive the auto-covariance function. This requires calculating the conditional probability $P_{11}(\\tau)$. Solve the ODE for $p_1(t)$ with $p_1(0)=1$ to get $P_{11}(\\tau)$.\n        *   Use $P_{11}(\\tau)$ to find $\\mathbb{E}[S(t+\\tau)S(t)] = P_{11}(\\tau)\\pi_1$.\n        *   Calculate the auto-covariance $\\text{Cov}(S(t+\\tau),S(t)) = \\mathbb{E}[S(t+\\tau)S(t)] - \\mu_S^2$. Show that this is $\\pi_1\\pi_0 \\exp(-(k_{on}+k_{off})\\tau)$.\n        *   Define the normalized autocorrelation function $C(\\tau) = \\frac{\\text{Cov}(\\tau)}{\\text{Cov}(0)}$. Show this simplifies to $\\exp(-(k_{on}+k_{off})\\tau)$.\n        *   Calculate the sensitivities $S_{on}(\\tau) = \\partial C/\\partial k_{on}$ and $S_{off}(\\tau) = \\partial C/\\partial k_{off}$.\n        *   Calculate the ratio $R(\\tau) = S_{on}/S_{off}$.\n        *   Provide the discussion on identifiability, explaining that the ACF shape only yields $k_{on}+k_{off}$.\n        *   Propose measuring the mean activity $\\pi_1 = k_{on}/(k_{on}+k_{off})$ as the complementary measurement. Show how this, combined with $k_{on}+k_{off}$, uniquely determines $k_{on}$ and $k_{off}$.\n        *   Final answer box contains the expression for $R(\\tau)$. Which is $1$.\n\n### Step 1: Extract Givens\n- **Model:** Continuous-time two-state Markov process (telegraph model).\n- **State Space:** The promoter state is $S(t) \\in \\{0, 1\\}$, with $1$ for active and $0$ for inactive.\n- **Transition Rates:** $k_{on}$ for the transition $0 \\to 1$ and $k_{off}$ for the transition $1 \\to 0$.\n- **Process Assumptions:** The process is stationary and ergodic.\n- **Measurement:** The signal is $S(t)$.\n- **Task 1:** Derive the normalized autocorrelation function $C(\\tau)$ for $\\tau \\ge 0$ of the centered signal $S(t) - \\mathbb{E}[S(t)]$.\n- **Task 2:** Compute the local sensitivities $S_{on}(\\tau) \\equiv \\partial C(\\tau)/\\partial k_{on}$ and $S_{off}(\\tau) \\equiv \\partial C(\\tau)/\\partial k_{off}$.\n- **Task 3:** Calculate the ratio $R(\\tau) \\equiv S_{on}(\\tau) / S_{off}(\\tau)$.\n- **Task 4:** Discuss the identifiability of $k_{on}$ and $k_{off}$ from the shape of $C(\\tau)$ alone.\n- **Task 5:** Propose a complementary steady-state measurement to make $(k_{on}, k_{off})$ identifiable.\n- **Reporting Requirement:** Report the closed-form analytic expression for $R(\\tau)$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on the two-state telegraph model, a fundamental and widely-used model in systems biology for describing stochastic gene expression. All concepts, including continuous-time Markov chains, autocorrelation, and sensitivity analysis, are standard in mathematics and physical sciences.\n- **Well-Posed:** The problem is well-posed. The assumption of stationarity guarantees the existence of time-independent steady-state probabilities and a time-translation invariant autocorrelation function. The instructions are specific and lead to a unique mathematical solution.\n- **Objective:** The problem is stated in precise, objective, and formal mathematical language.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, missing information, or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\nThe promoter state $S(t)$ follows a continuous-time Markov chain on the state space $\\{0, 1\\}$. Let $\\pi_0$ and $\\pi_1$ be the stationary probabilities of being in the inactive state ($S=0$) and active state ($S=1$), respectively. The time evolution of the probability of being in the active state, $p_1(t) = \\mathbb{P}(S(t)=1)$, is given by the master equation:\n$$\n\\frac{dp_1(t)}{dt} = k_{on} p_0(t) - k_{off} p_1(t)\n$$\nwhere $p_0(t) = 1 - p_1(t)$. At steady state, $\\frac{dp_1}{dt}=0$, so we have:\n$$\nk_{on} \\pi_0 - k_{off} \\pi_1 = 0 \\quad \\text{and} \\quad \\pi_0 + \\pi_1 = 1\n$$\nSubstituting $\\pi_0 = 1 - \\pi_1$ into the first equation yields $k_{on}(1 - \\pi_1) - k_{off} \\pi_1 = 0$, which solves to:\n$$\n\\pi_1 = \\frac{k_{on}}{k_{on} + k_{off}} \\quad \\text{;} \\quad \\pi_0 = \\frac{k_{off}}{k_{on} + k_{off}}\n$$\nThe problem asks for the normalized autocorrelation of the centered signal, $X(t) = S(t) - \\mathbb{E}[S(t)]$. First, we compute the mean and variance of $S(t)$.\nThe mean is $\\mathbb{E}[S(t)] = 1 \\cdot \\pi_1 + 0 \\cdot \\pi_0 = \\pi_1$.\nThe variance is $\\text{Var}(S(t)) = \\mathbb{E}[S(t)^2] - (\\mathbb{E}[S(t)])^2$. Since $S(t)$ is a Bernoulli variable ($0$ or $1$), $S(t)^2 = S(t)$, thus $\\mathbb{E}[S(t)^2] = \\mathbb{E}[S(t)] = \\pi_1$.\n$$\n\\text{Var}(S(t)) = \\pi_1 - \\pi_1^2 = \\pi_1(1-\\pi_1) = \\pi_1 \\pi_0 = \\frac{k_{on} k_{off}}{(k_{on} + k_{off})^2}\n$$\nThe normalized autocorrelation function $C(\\tau)$ is defined as:\n$$\nC(\\tau) = \\frac{\\mathbb{E}[(S(t) - \\pi_1)(S(t+\\tau) - \\pi_1)]}{\\text{Var}(S(t))} = \\frac{\\text{Cov}(S(t), S(t+\\tau))}{\\text{Var}(S(t))}\n$$\nThe auto-covariance is $\\text{Cov}(S(t), S(t+\\tau)) = \\mathbb{E}[S(t)S(t+\\tau)] - \\mathbb{E}[S(t)]\\mathbb{E}[S(t+\\tau)] = \\mathbb{E}[S(t)S(t+\\tau)] - \\pi_1^2$.\nThe expectation of the product is computed using conditional probabilities:\n$$\n\\mathbb{E}[S(t)S(t+\\tau)] = \\sum_{i,j \\in \\{0,1\\}} ij \\cdot \\mathbb{P}(S(t)=i, S(t+\\tau)=j) = \\mathbb{P}(S(t)=1, S(t+\\tau)=1)\n$$\n$$\n\\mathbb{P}(S(t)=1, S(t+\\tau)=1) = \\mathbb{P}(S(t+\\tau)=1 | S(t)=1) \\mathbb{P}(S(t)=1) = P_{11}(\\tau) \\pi_1\n$$\nwhere $P_{11}(\\tau)$ is the probability of being in state $1$ at time $\\tau$ given the system was in state $1$ at time $0$. We find $P_{11}(\\tau)$ by solving the master equation for $p_1(\\tau)$ with the initial condition $p_1(0)=1$:\n$$\n\\frac{dp_1}{dt} = k_{on} - (k_{on}+k_{off})p_1(t)\n$$\nThis is a linear first-order ODE with the solution:\n$$\np_1(\\tau) = \\frac{k_{on}}{k_{on}+k_{off}} + \\left(p_1(0) - \\frac{k_{on}}{k_{on}+k_{off}}\\right) \\exp(-(k_{on}+k_{off})\\tau)\n$$\nWith $p_1(0)=1$, this gives $P_{11}(\\tau)$:\n$$\nP_{11}(\\tau) = \\pi_1 + (1-\\pi_1)\\exp(-(k_{on}+k_{off})\\tau) = \\pi_1 + \\pi_0 \\exp(-(k_{on}+k_{off})\\tau)\n$$\nNow we can compute the auto-covariance:\n$$\n\\text{Cov}(S(t), S(t+\\tau)) = P_{11}(\\tau)\\pi_1 - \\pi_1^2 = (\\pi_1 + \\pi_0 e^{-(k_{on}+k_{off})\\tau})\\pi_1 - \\pi_1^2 = \\pi_1 \\pi_0 \\exp(-(k_{on}+k_{off})\\tau)\n$$\nThe normalized autocorrelation function is the auto-covariance divided by the variance, which is the auto-covariance at $\\tau=0$:\n$$\nC(\\tau) = \\frac{\\pi_1 \\pi_0 \\exp(-(k_{on}+k_{off})\\tau)}{\\pi_1 \\pi_0} = \\exp(-(k_{on}+k_{off})\\tau)\n$$\nNext, we compute the sensitivities $S_{on}(\\tau)$ and $S_{off}(\\tau)$:\n$$\nS_{on}(\\tau) = \\frac{\\partial C(\\tau)}{\\partial k_{on}} = \\frac{\\partial}{\\partial k_{on}} \\exp(-(k_{on}+k_{off})\\tau) = -\\tau \\exp(-(k_{on}+k_{off})\\tau)\n$$\n$$\nS_{off}(\\tau) = \\frac{\\partial C(\\tau)}{\\partial k_{off}} = \\frac{\\partial}{\\partial k_{off}} \\exp(-(k_{on}+k_{off})\\tau) = -\\tau \\exp(-(k_{on}+k_{off})\\tau)\n$$\nThe ratio $R(\\tau)$ is therefore:\n$$\nR(\\tau) = \\frac{S_{on}(\\tau)}{S_{off}(\\tau)} = \\frac{-\\tau \\exp(-(k_{on}+k_{off})\\tau)}{-\\tau \\exp(-(k_{on}+k_{off})\\tau)} = 1 \\quad (\\text{for } \\tau > 0)\n$$\nFor $\\tau=0$, the sensitivities are $0$ and the ratio is indeterminate, but for any $\\tau>0$ that characterizes the decay shape, the ratio is exactly $1$.\n\nThis result leads directly to the discussion of parameter identifiability. The normalized autocorrelation function $C(\\tau)$ is a single exponential decay whose rate is determined solely by the sum of the rates, $k_{decay} = k_{on} + k_{off}$. Measuring the shape of $C(\\tau)$ from experimental data (for example, using fluorescence correlation spectroscopy) allows for the determination of $k_{decay}$, but it provides only one constraint on the two parameters $k_{on}$ and $k_{off}$. Any pair of rates $(k'_{on}, k'_{off})$ such that $k'_{on}+k'_{off} = k_{decay}$ will produce the identical normalized autocorrelation function. Therefore, the shape of $C(\\tau)$ alone is insufficient to identify $k_{on}$ and $k_{off}$ separately. The fact that $R(\\tau)=1$ formally shows that a change in $k_{on}$ has the exact same effect on $C(\\tau)$ as an identical change in $k_{off}$, making them indistinguishable from the shape of $C(\\tau)$.\n\nTo render the pair $(k_{on}, k_{off})$ identifiable, a second, independent measurement is required. A suitable complementary measurement is a steady-state property that depends on $k_{on}$ and $k_{off}$ in a different combination. The mean promoter activity, $\\mathbb{E}[S(t)] = \\pi_1$, is an ideal candidate. This quantity represents the fraction of time the promoter is active and can be measured experimentally by time-averaging the reporter signal. We would then have a system of two equations and two unknowns:\n$$\n\\begin{cases}\nk_{on} + k_{off} &= k_{decay} \\\\\n\\frac{k_{on}}{k_{on}+k_{off}} &= \\pi_1\n\\end{cases}\n$$\nwhere $k_{decay}$ is determined from the ACF shape and $\\pi_1$ is measured as the mean signal. The solution to this system is unique:\n$$\nk_{on} = \\pi_1 k_{decay}\n$$\n$$\nk_{off} = k_{decay} - k_{on} = (1 - \\pi_1) k_{decay}\n$$\nThus, by combining the shape of the normalized autocorrelation function with a measurement of the mean promoter activity, the parameters $(k_{on}, k_{off})$ become uniquely identifiable.\n\nThe problem asks for the closed-form expression for $R(\\tau)$. As derived, this is $1$.", "answer": "$$\n\\boxed{1}\n$$", "id": "4388478"}, {"introduction": "While moments and correlation functions provide valuable summaries, a complete understanding of stochastic gene expression requires solving for the entire probability distribution of mRNA molecules over time. This is the role of the Chemical Master Equation (CME), which is often analytically intractable. This practice [@problem_id:4388466] introduces the Finite State Projection (FSP) algorithm, a powerful and widely used numerical method to approximate the solution of the CME, and challenges you to derive and implement a rigorous error bound for the approximation.", "problem": "Consider the telegraph model of gene expression at the single-cell level, where a promoter switches stochastically between an inactive state and an active state and messenger ribonucleic acid (mRNA) is synthesized and degraded in time. The system is modeled as a Continuous-Time Markov Chain (CTMC) governed by the Chemical Master Equation (CME). Let the promoter state be denoted by $g \\in \\{0,1\\}$, where $g=0$ represents the inactive state and $g=1$ represents the active state. Let the mRNA copy number be denoted by $m \\in \\mathbb{N}_0$. Promoter switching occurs with rates $k_{\\mathrm{on}}$ from $g=0$ to $g=1$ and $k_{\\mathrm{off}}$ from $g=1$ to $g=0$. Transcription occurs with rate $s_0$ when $g=0$ and with rate $s_1$ when $g=1$. Degradation occurs with rate $\\gamma$ per mRNA molecule, that is, transition from $m$ to $m-1$ occurs at rate $\\gamma m$. The fundamental base is the definition of the Chemical Master Equation and conservation of probability for CTMCs.\n\nYou must:\n- Formulate the Finite State Projection (FSP) truncation for the telegraph CME using a projection set $S_M = \\{(g,m) \\mid g \\in \\{0,1\\}, m \\in \\{0,1,\\dots,M\\}\\}$, where $M \\in \\mathbb{N}_0$ is the projection size limit on the mRNA count. Augment the CTMC with a single absorbing sink state $\\varnothing$ that collects all probability flux leaving $S_M$ due to any transition that would increase $m$ beyond $M$.\n- Derive an upper bound on the truncation error at time $t$, defined as the probability mass that lies outside $S_M$ in the true CME solution, and express this bound explicitly as a function of the projection size $M$. Your derivation must start from the CME and conservation of probability, without using pre-derived shortcut formulas.\n- Implement a numerical solver for the truncated system by constructing the generator matrix for the FSP-augmented CTMC over $S_M \\cup \\{\\varnothing\\}$ and integrating the forward ordinary differential equation $\\mathrm{d} \\mathbf{p}(t) / \\mathrm{d} t = A_M \\mathbf{p}(t)$ for a specified time horizon $T$. Use an initial condition $\\mathbf{p}(0)$ concentrated at $(g,m)=(0,0)$ with probability $1$ and $0$ elsewhere.\n\nDefine the generator $A_M$ with the convention that for source state $i$ to destination state $j$, the entry $A_M[j,i]$ equals the transition rate from $i$ to $j$, ensuring that probability conservation implies each column of $A_M$ sums to $0$. For the sink state $\\varnothing$, define no outgoing transitions, i.e., the sink is absorbing. The truncation error upper bound must be the sink probability $p_{\\varnothing}(t)$ produced by the FSP-augmented system, and you must justify this bound using first principles.\n\nYour program must compute, for each test case, the upper bound value $p_{\\varnothing}(T)$ at the final time $T$. No physical units are required for the rates or time; treat them as dimensionless numbers for computation. Angles are not involved.\n\nTest suite:\n- Case $1$ (general case): $k_{\\mathrm{on}}=1.0$, $k_{\\mathrm{off}}=1.5$, $s_0=0.0$, $s_1=5.0$, $\\gamma=1.0$, $M=50$, $T=5.0$.\n- Case $2$ (boundary-truncation stress): $k_{\\mathrm{on}}=2.0$, $k_{\\mathrm{off}}=0.5$, $s_0=0.0$, $s_1=20.0$, $\\gamma=1.0$, $M=5$, $T=5.0$.\n- Case $3$ (no-switching edge): $k_{\\mathrm{on}}=0.0$, $k_{\\mathrm{off}}=0.0$, $s_0=0.0$, $s_1=5.0$, $\\gamma=1.0$, $M=10$, $T=10.0$.\n- Case $4$ (basal transcription with slow degradation): $k_{\\mathrm{on}}=0.2$, $k_{\\mathrm{off}}=2.0$, $s_0=1.0$, $s_1=8.0$, $\\gamma=0.5$, $M=20$, $T=10.0$.\n- Case $5$ (fast degradation): $k_{\\mathrm{on}}=3.0$, $k_{\\mathrm{off}}=3.0$, $s_0=0.0$, $s_1=5.0$, $\\gamma=5.0$, $M=30$, $T=5.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, namely $[p_{\\varnothing}^{(1)},p_{\\varnothing}^{(2)},p_{\\varnothing}^{(3)},p_{\\varnothing}^{(4)},p_{\\varnothing}^{(5)}]$, where $p_{\\varnothing}^{(k)}$ is the upper bound for test case $k$ computed at its specified final time $T$.", "solution": "The problem requires the analysis of the telegraph model of stochastic gene expression using the Finite State Projection (FSP) method. We must first formulate the FSP truncation of the Chemical Master Equation (CME), then derive an upper bound for the truncation error, and finally implement a numerical solver to compute this bound for several test cases.\n\nLet the state of the system be $(g, m)$, where $g \\in \\{0, 1\\}$ is the promoter state (inactive/active) and $m \\in \\mathbb{N}_0$ is the number of mRNA molecules. The system evolves as a Continuous-Time Markov Chain (CTMC) with the following transitions and rates:\n- Promoter activation: $(0, m) \\to (1, m)$ with rate $k_{\\mathrm{on}}$.\n- Promoter inactivation: $(1, m) \\to (0, m)$ with rate $k_{\\mathrm{off}}$.\n- Transcription (basal): $(0, m) \\to (0, m+1)$ with rate $s_0$.\n- Transcription (active): $(1, m) \\to (1, m+1)$ with rate $s_1$.\n- mRNA Degradation: $(g, m) \\to (g, m-1)$ with rate $\\gamma m$ for any $g \\in \\{0,1\\}$ and $m>0$.\n\nLet $P(g, m, t)$ be the probability of being in state $(g, m)$ at time $t$. The time evolution of these probabilities is governed by the Chemical Master Equation (CME). For $m > 0$, the CMEs are:\n$$\n\\frac{d P(0, m, t)}{dt} = k_{\\mathrm{off}} P(1, m, t) + s_0 P(0, m-1, t) + \\gamma (m+1) P(0, m+1, t) - (k_{\\mathrm{on}} + s_0 + \\gamma m) P(0, m, t)\n$$\n$$\n\\frac{d P(1, m, t)}{dt} = k_{\\mathrm{on}} P(0, m, t) + s_1 P(1, m-1, t) + \\gamma (m+1) P(1, m+1, t) - (k_{\\mathrm{off}} + s_1 + \\gamma m) P(1, m, t)\n$$\nFor the boundary case $m=0$, the degradation terms $\\gamma m$ are zero and there is no influx from states with $m=-1$:\n$$\n\\frac{d P(0, 0, t)}{dt} = k_{\\mathrm{off}} P(1, 0, t) + \\gamma P(0, 1, t) - (k_{\\mathrm{on}} + s_0) P(0, 0, t)\n$$\n$$\n\\frac{d P(1, 0, t)}{dt} = k_{\\mathrm{on}} P(0, 0, t) + \\gamma P(1, 1, t) - (k_{\\mathrm{off}} + s_1) P(1, 0, t)\n$$\nThis is an infinite system of coupled linear ordinary differential equations (ODEs), which is typically intractable to solve directly.\n\nThe Finite State Projection (FSP) method provides a way to approximate the solution. We truncate the infinite state space to a finite projection set $S_M = \\{(g,m) \\mid g \\in \\{0,1\\}, m \\in \\{0, 1, \\dots, M\\}\\}$ for some integer $M \\ge 0$. The FSP approach models the evolution of probabilities for states within $S_M$. To account for the probability that leaves this finite set, we augment the system with an absorbing sink state, denoted $\\varnothing$. The state space of our new, finite CTMC is $S_M \\cup \\{\\varnothing\\}$. Any reaction that would lead from a state in $S_M$ to a state outside $S_M$ is re-routed to the sink state $\\varnothing$. In this model, the only such transitions are the transcription events from states with $m=M$:\n- $(0, M) \\to \\varnothing$ with rate $s_0$.\n- $(1, M) \\to \\varnothing$ with rate $s_1$.\n\nLet $p_{(g,m)}(t)$ be the probability of being in state $(g,m) \\in S_M$ and $p_{\\varnothing}(t)$ be the probability of being in the sink state in this FSP-augmented system. The total probability is conserved: $\\sum_{(g,m) \\in S_M} p_{(g,m)}(t) + p_{\\varnothing}(t) = 1$. The time evolution of the sink probability is driven by the flux out of $S_M$:\n$$\n\\frac{d p_{\\varnothing}(t)}{dt} = s_0 p_{(0,M)}(t) + s_1 p_{(1,M)}(t)\n$$\nwith initial condition $p_{\\varnothing}(0)=0$.\n\nA crucial property of the FSP is that the probability accumulated in the sink state, $p_{\\varnothing}(t)$, provides an upper bound on the true truncation error. The truncation error, $E_M(t)$, is the probability mass residing in states outside of $S_M$ in the original, infinite system:\n$$\nE_M(t) = \\sum_{g \\in \\{0,1\\}} \\sum_{m=M+1}^{\\infty} P(g, m, t)\n$$\nWe are required to justify from first principles that $p_{\\varnothing}(t) \\ge E_M(t)$ for all $t \\ge 0$.\nLet $\\mathbf{P}_F(t)$ be the vector of true probabilities $P(g,m,t)$ for states within the finite set $S_M$. The time evolution of $\\mathbf{P}_F(t)$ can be written as:\n$$\n\\frac{d \\mathbf{P}_F(t)}{dt} = A_F \\mathbf{P}_F(t) + \\mathbf{r}(t)\n$$\nHere, $A_F$ is the generator matrix restricted to states in $S_M$, where transitions leaving $S_M$ are treated as probability loss. The term $\\mathbf{r}(t)$ is a vector representing the probability influx from states outside $S_M$ back into $S_M$. Specifically, this influx comes from degradation reactions $(g, M+1) \\to (g, M)$. Thus, the components of $\\mathbf{r}(t)$ are non-negative, with $r_{(g,M)}(t) = \\gamma (M+1) P(g, M+1, t) \\ge 0$ and all other components being zero.\nNow, consider the probabilities $\\mathbf{p}(t)$ for states in $S_M$ in the FSP formulation (without the sink state). These evolve according to:\n$$\n\\frac{d \\mathbf{p}(t)}{dt} = A_F \\mathbf{p}(t)\n$$\nLet $\\mathbf{d}(t) = \\mathbf{P}_F(t) - \\mathbf{p}(t)$ be the vector of differences. Its dynamics are:\n$$\n\\frac{d \\mathbf{d}(t)}{dt} = A_F \\mathbf{d}(t) + \\mathbf{r}(t)\n$$\nThe problem specifies the initial condition is $(g,m)=(0,0)$, which is inside $S_M$ (for any $M \\ge 0$). Thus, $\\mathbf{P}_F(0) = \\mathbf{p}(0)$, which implies $\\mathbf{d}(0) = \\mathbf{0}$. The matrix $A_F$ has non-negative off-diagonal elements, a property of CTMC generator matrices. The forcing term $\\mathbf{r}(t)$ is a vector with non-negative components. For a linear system of this form (a positive system), if the initial state is non-negative and the input is non-negative, the state vector remains non-negative for all time. Since $\\mathbf{d}(0)=\\mathbf{0}$, we can conclude that $\\mathbf{d}(t) \\ge \\mathbf{0}$ for all $t \\ge 0$. This implies $P(g, m, t) \\ge p_{(g,m)}(t)$ for all $(g,m) \\in S_M$ and $t \\ge 0$.\nThe total probability in the original system is $1$: $\\sum_{(g,m) \\in S_M} P(g,m,t) + E_M(t) = 1$.\nThe total probability in the FSP-augmented system is also $1$: $\\sum_{(g,m) \\in S_M} p_{(g,m)}(t) + p_{\\varnothing}(t) = 1$.\nThe truncation error is $E_M(t) = 1 - \\sum_{(g,m) \\in S_M} P(g,m,t)$.\nFrom our derived inequality, $\\sum P(g,m,t) \\ge \\sum p_{(g,m)}(t)$.\nTherefore, $-\\sum P(g,m,t) \\le -\\sum p_{(g,m)}(t)$, which leads to:\n$$\nE_M(t) = 1 - \\sum_{(g,m) \\in S_M} P(g,m,t) \\le 1 - \\sum_{(g,m) \\in S_M} p_{(g,m)}(t) = p_{\\varnothing}(t)\n$$\nThis confirms that the probability of the sink state, $p_{\\varnothing}(t)$, is a computable upper bound on the true truncation error.\n\nTo compute $p_{\\varnothing}(T)$, we solve the finite system of ODEs for the FSP-augmented CTMC. We represent the system in matrix form, $\\frac{d\\mathbf{p}(t)}{dt} = A_M \\mathbf{p}(t)$, where $\\mathbf{p}(t)$ is the state probability vector and $A_M$ is the generator matrix. The state vector is ordered as follows:\n$\\mathbf{p} = [p_{(0,0)}, \\dots, p_{(0,M)}, p_{(1,0)}, \\dots, p_{(1,M)}, p_{\\varnothing}]^T$.\nThis gives a total of $N = 2(M+1)+1$ states. The generator matrix $A_M$ is an $N \\times N$ matrix where $A_M[j,i]$ is the rate of transition from state $i$ to state $j$.\n\nThe non-zero off-diagonal elements of $A_M$ for source state $i=(g,m)$ are:\n1.  **Transcription**: For $m < M$, state $(g,m)$ transitions to $(g,m+1)$ with rate $s_g$. If we index state $(g', m')$ by $i(g', m')$, this means $A_M[i(g, m+1), i(g, m)] = s_g$.\n2.  **Degradation**: For $m > 0$, state $(g,m)$ transitions to $(g,m-1)$ with rate $\\gamma m$. This means $A_M[i(g, m-1), i(g, m)] = \\gamma m$.\n3.  **Switching**: For any $m \\in \\{0, \\dots, M\\}$, state $(0,m)$ transitions to $(1,m)$ with rate $k_{\\mathrm{on}}$, and $(1,m)$ transitions to $(0,m)$ with rate $k_{\\mathrm{off}}$. This means $A_M[i(1, m), i(0, m)] = k_{\\mathrm{on}}$ and $A_M[i(0, m), i(1, m)] = k_{\\mathrm{off}}$.\n4.  **Flux to Sink**: For the boundary states $m=M$, transcription transitions are re-routed to the sink state $\\varnothing$, whose index is $N-1 = 2(M+1)$.\n    - $A_M[N-1, i(0, M)] = s_0$.\n    - $A_M[N-1, i(1, M)] = s_1$.\nThe diagonal elements are set such that each column sums to zero, representing the total rate of leaving a state: $A_M[i, i] = - \\sum_{j \\neq i} A_M[j,i]$. For the absorbing sink state, the corresponding column of $A_M$ is all zeros.\n\nWith the matrix $A_M$ constructed, we solve the initial value problem $\\frac{d\\mathbf{p}}{dt} = A_M \\mathbf{p}(t)$ with $\\mathbf{p}(0)$ being a vector with $1$ at the position corresponding to state $(0,0)$ and $0$ elsewhere. The solution is obtained by numerical integration up to the final time $T$. The desired error bound is the final probability of the sink state, $p_{\\varnothing}(T)$, which is the last element of the solution vector $\\mathbf{p}(T)$.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Solves the Finite State Projection formulation of the telegraph model\n    for several test cases and computes the truncation error upper bound.\n    \"\"\"\n    test_cases = [\n        # Case 1 (general case)\n        {'kon': 1.0, 'koff': 1.5, 's0': 0.0, 's1': 5.0, 'gamma': 1.0, 'M': 50, 'T': 5.0},\n        # Case 2 (boundary-truncation stress)\n        {'kon': 2.0, 'koff': 0.5, 's0': 0.0, 's1': 20.0, 'gamma': 1.0, 'M': 5, 'T': 5.0},\n        # Case 3 (no-switching edge)\n        {'kon': 0.0, 'koff': 0.0, 's0': 0.0, 's1': 5.0, 'gamma': 1.0, 'M': 10, 'T': 10.0},\n        # Case 4 (basal transcription with slow degradation)\n        {'kon': 0.2, 'koff': 2.0, 's0': 1.0, 's1': 8.0, 'gamma': 0.5, 'M': 20, 'T': 10.0},\n        # Case 5 (fast degradation)\n        {'kon': 3.0, 'koff': 3.0, 's0': 0.0, 's1': 5.0, 'gamma': 5.0, 'M': 30, 'T': 5.0},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        kon, koff, s0, s1, gamma = case['kon'], case['koff'], case['s0'], case['s1'], case['gamma']\n        M, T = case['M'], case['T']\n\n        # The state space for the FSP is {(g, m) | g in {0,1}, m in {0,...,M}}\n        # plus one absorbing sink state.\n        # Total number of states = 2 * (M + 1) + 1\n        num_states = 2 * (M + 1) + 1\n        \n        # Generator matrix A, where A[j, i] is the rate from state i to state j.\n        A = np.zeros((num_states, num_states))\n\n        # Index mapping:\n        # g=0, m -> m\n        # g=1, m -> M + 1 + m\n        # sink   -> 2 * (M + 1)\n        sink_idx = 2 * (M + 1)\n\n        for m in range(M + 1):\n            # State (0, m)\n            idx0 = m\n            # State (1, m)\n            idx1 = M + 1 + m\n\n            # Promoter switching\n            A[idx1, idx0] = kon\n            A[idx0, idx1] = koff\n\n            # Degradation\n            if m > 0:\n                # From (0, m) to (0, m-1)\n                A[idx0 - 1, idx0] = gamma * m\n                # From (1, m) to (1, m-1)\n                A[idx1 - 1, idx1] = gamma * m\n        \n            # Transcription\n            if m  M:\n                # From (0, m) to (0, m+1)\n                A[idx0 + 1, idx0] = s0\n                # From (1, m) to (1, m+1)\n                A[idx1 + 1, idx1] = s1\n            else: # m == M, transcription leads to sink state\n                A[sink_idx, idx0] = s0\n                A[sink_idx, idx1] = s1\n\n        # Set diagonal elements to ensure columns sum to zero\n        # This conserves probability.\n        for i in range(num_states - 1): # Exclude the sink state column\n            A[i, i] = -np.sum(A[:, i])\n\n        # The column for the sink state is all zeros as it's absorbing.\n        A[:, sink_idx] = 0.0\n\n        # Initial condition: probability 1 in state (g=0, m=0), 0 elsewhere.\n        p0 = np.zeros(num_states)\n        p0[0] = 1.0\n\n        # Define the ODE system for the solver\n        def ode_system(t, p, A_matrix):\n            return A_matrix @ p\n\n        # Solve the ODE system\n        sol = solve_ivp(\n            fun=ode_system,\n            t_span=(0, T),\n            y0=p0,\n            args=(A,),\n            method='Radau', # Good for stiff problems\n            t_eval=[T]\n        )\n\n        # The result is the probability in the sink state at time T.\n        p_final = sol.y[:, -1]\n        p_sink = p_final[sink_idx]\n        results.append(p_sink)\n\n    # Format output as a comma-separated list in brackets\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "4388466"}, {"introduction": "Ultimately, the goal of modeling is to interpret experimental data and learn about the underlying biological mechanisms. This hands-on practice bridges the gap between theory and data by tackling a common problem: inferring burst parameters from single-cell snapshot measurements. By working through this exercise [@problem_id:4388424], you will explore the power of hierarchical Bayesian models to regularize estimates and learn about posterior shrinkage, a key statistical phenomenon that improves inference by leveraging population-level information.", "problem": "Consider a stochastic gene expression scenario at the single-cell level in systems biomedicine where transcription occurs in bursts. Over a fixed observation window of duration $\\Delta t$, suppose each cell $i$ produces a count $x_i$ of messenger ribonucleic acid (mRNA) molecules. Assume a simplified burst model where, conditional on a cell-specific mean burst size $b_i$ (mRNA molecules per burst) and a common burst frequency $f$ (bursts per unit time) across cells, the observed count is modeled as a Poisson random variable with rate $f \\, \\Delta t \\, b_i$. That is, for each cell $i$, \n$$\nx_i \\mid b_i \\sim \\text{Poisson}(f \\, \\Delta t \\, b_i).\n$$\nAssume across cells that $b_i$ follows a Gamma prior with shape $\\alpha$ and rate $\\beta$, written as $b_i \\sim \\text{Gamma}(\\alpha,\\beta)$, where the Gamma density is parameterized with shape-rate, and the rate parameter uses the convention that larger $\\beta$ implies stronger shrinkage. This induces, after integrating out $b_i$, a Negative Binomial distribution for $x_i$ across the population, consistent with bursty gene expression at steady state.\n\nYou are tasked to compare hierarchical versus pooled analyses by computing posterior shrinkage effects and their impact on estimated burst parameters. For hierarchical analysis, use empirical Bayes to estimate the hyperparameters $(\\alpha,\\beta)$ from the observed counts $\\{x_i\\}_{i=1}^N$ by matching the first two moments of the Poisson-Gamma mixture. Let $c = f \\, \\Delta t$ denote the known exposure scaling. Denote the sample mean across cells by \n$$\nm = \\frac{1}{N} \\sum_{i=1}^N x_i,\n$$\nand the population (non-unbiased) sample variance by \n$$\ns^2 = \\frac{1}{N} \\sum_{i=1}^N (x_i - m)^2.\n$$\nThe Poisson-Gamma mixture implies \n$$\n\\mathbb{E}[x_i] = c \\, \\frac{\\alpha}{\\beta} = m, \\quad \\text{Var}(x_i) = c \\, \\frac{\\alpha}{\\beta} + c^2 \\, \\frac{\\alpha}{\\beta^2} = s^2.\n$$\nSolving these by method of moments yields \n$$\n\\hat{\\alpha} = \\frac{m^2}{s^2 - m}, \\quad \\hat{\\beta} = c \\, \\frac{\\hat{\\alpha}}{m},\n$$\nprovided $s^2  m$. In the boundary case $s^2 \\le m$ (no over-dispersion beyond Poisson), set $\\hat{\\alpha}$ to a large value (e.g., $\\hat{\\alpha} = 10^6$) and $\\hat{\\beta} = c \\, \\hat{\\alpha} / m$ to reflect negligible over-dispersion; if $m = 0$, set $\\hat{\\alpha} = 10^{-6}$ and $\\hat{\\beta} = 10^{-6}$ to avoid division by zero.\n\nUnder the hierarchical model, the posterior for each $b_i$ given $x_i$ is conjugate:\n$$\nb_i \\mid x_i \\sim \\text{Gamma}(\\hat{\\alpha} + x_i, \\hat{\\beta} + c),\n$$\nso the hierarchical posterior mean is \n$$\n\\hat{b}_i^{\\text{hier}} = \\frac{\\hat{\\alpha} + x_i}{\\hat{\\beta} + c}.\n$$\nFor a per-cell maximum likelihood estimate (MLE) of the burst size under the conditional Poisson model,\n$$\n\\hat{b}_i^{\\text{MLE}} = \\frac{x_i}{c}.\n$$\nDefine the hierarchical posterior shrinkage difference (in mRNA molecules per burst) for each cell as \n$$\ns_i = \\hat{b}_i^{\\text{hier}} - \\hat{b}_i^{\\text{MLE}}.\n$$\n\nFor the pooled analysis, assume all cells share a single burst size parameter $b_{\\text{pool}}$ with prior $b_{\\text{pool}} \\sim \\text{Gamma}(\\alpha_0, \\beta_0)$. Using the pooled likelihood over all cells,\n$$\nb_{\\text{pool}} \\mid x_{1:N} \\sim \\text{Gamma}\\left(\\alpha_0 + \\sum_{i=1}^N x_i,\\, \\beta_0 + N c \\right),\n$$\nand the pooled posterior mean is\n$$\n\\hat{b}^{\\text{pool}} = \\frac{\\alpha_0 + \\sum_{i=1}^N x_i}{\\beta_0 + N c}.\n$$\nDefine the per-cell pooled impact difference as \n$$\nd_i = \\hat{b}^{\\text{pool}} - \\hat{b}_i^{\\text{hier}}.\n$$\n\nFor each test case, compute the following three summary metrics:\n1. The average hierarchical shrinkage difference \n$$\n\\overline{s} = \\frac{1}{N} \\sum_{i=1}^N s_i,\n$$\nexpressed in mRNA molecules per burst, rounded to six decimal places.\n2. The average absolute pooled impact difference \n$$\n\\overline{|d|} = \\frac{1}{N} \\sum_{i=1}^N |d_i|,\n$$\nexpressed in mRNA molecules per burst, rounded to six decimal places.\n3. The ratio \n$$\nR = \\frac{\\hat{b}^{\\text{pool}}}{\\frac{1}{N} \\sum_{i=1}^N \\hat{b}_i^{\\text{hier}}},\n$$\ndimensionless, rounded to six decimal places.\n\nUse the following test suite of parameter values that covers a general case, boundary conditions, and edge cases:\n- Test case $1$: $x_{1:10} = [\\,3,\\,0,\\,1,\\,5,\\,2,\\,4,\\,6,\\,1,\\,0,\\,3\\,]$, $f = 0.5$ (bursts per minute), $\\Delta t = 2$ (minutes), so $c = 1$. Pooled prior hyperparameters: $\\alpha_0 = 1$, $\\beta_0 = 1$.\n- Test case $2$: $x_{1:8} = [\\,0,\\,0,\\,0,\\,1,\\,2,\\,0,\\,3,\\,0\\,]$, $f = 1$ (bursts per minute), $\\Delta t = 1$ (minutes), so $c = 1$. Pooled prior hyperparameters: $\\alpha_0 = 0.5$, $\\beta_0 = 0.5$.\n- Test case $3$: $x_{1:12} = [\\,10,\\,15,\\,12,\\,8,\\,20,\\,5,\\,18,\\,11,\\,7,\\,9,\\,14,\\,16\\,]$, $f = 2$ (bursts per minute), $\\Delta t = 3$ (minutes), so $c = 6$. Pooled prior hyperparameters: $\\alpha_0 = 1$, $\\beta_0 = 0.1$.\n- Test case $4$: $x_{1:3} = [\\,2,\\,9,\\,1\\,]$, $f = 0.2$ (bursts per minute), $\\Delta t = 5$ (minutes), so $c = 1$. Pooled prior hyperparameters: $\\alpha_0 = 2$, $\\beta_0 = 2$.\n\nYour program must implement the empirical Bayes estimation of $(\\hat{\\alpha}, \\hat{\\beta})$, compute $\\hat{b}_i^{\\text{hier}}$, $\\hat{b}_i^{\\text{MLE}}$, $\\hat{b}^{\\text{pool}}$, and then produce the three metrics $(\\overline{s}, \\overline{|d|}, R)$ for each test case. All outputs involving burst size must be treated as mRNA molecules per burst. The final output format must be a single line containing a comma-separated list of the results for all test cases, flattened and enclosed in square brackets, specifically \n$$\n[\\overline{s}_1,\\overline{|d|}_1,R_1,\\overline{s}_2,\\overline{|d|}_2,R_2,\\overline{s}_3,\\overline{|d|}_3,R_3,\\overline{s}_4,\\overline{|d|}_4,R_4],\n$$\nwhere each entry is a float rounded to six decimal places.", "solution": "The objective is to compare hierarchical versus pooled analyses within a bursty transcription model by quantifying posterior shrinkage effects and their implications for estimated burst parameters at the single-cell level. We proceed from fundamental principles applicable to stochastic gene expression.\n\nFirst, we adopt a mechanistic viewpoint consistent with the two-state promoter model (telegraph model) and its bursty limit: transcription events occur in bursts, and over a short observation window $\\Delta t$, the accumulated mRNA count for each cell $i$ can be approximated as a Poisson random variable conditional on a latent cell-specific production parameter. Let $b_i$ denote the mean burst size (mRNA molecules per burst), and let $f$ denote the burst frequency (bursts per unit time). The expected production over time $\\Delta t$ is then $f \\, \\Delta t \\, b_i$, so we set \n$$\nx_i \\mid b_i \\sim \\text{Poisson}(f \\, \\Delta t \\, b_i),\n$$\nwhere $x_i$ is the observed mRNA count for cell $i$. This conditional Poisson model follows from the assumption of independent event arrivals in time with rate $f$, each producing $b_i$ molecules on average, which is consistent with widely used approximations of bursty transcription at steady state.\n\nSecond, to capture between-cell variability in burst sizes, we posit a Gamma prior $b_i \\sim \\text{Gamma}(\\alpha,\\beta)$ using the shape-rate parameterization. This assumption is widely used in conjugate Bayesian modeling due to mathematical tractability and biological interpretability: the Gamma prior reflects population-level heterogeneity in burst size across cells. The mixture of Poisson with Gamma implies a Negative Binomial marginal distribution for $x_i$ across cells, a well-tested fact in single-cell transcriptomics under bursty production.\n\nWe now derive a hierarchical Bayesian estimator via empirical Bayes. Define the known exposure scaling $c = f \\, \\Delta t$. The Poisson-Gamma mixture implies the population mean and variance (law of total expectation and variance):\n$$\n\\mathbb{E}[x_i] = \\mathbb{E}\\left[\\mathbb{E}[x_i \\mid b_i]\\right] = \\mathbb{E}[c \\, b_i] = c \\, \\frac{\\alpha}{\\beta},\n$$\n$$\n\\text{Var}(x_i) = \\mathbb{E}[\\text{Var}(x_i \\mid b_i)] + \\text{Var}(\\mathbb{E}[x_i \\mid b_i]) = \\mathbb{E}[c \\, b_i] + \\text{Var}(c \\, b_i) = c \\, \\frac{\\alpha}{\\beta} + c^2 \\, \\frac{\\alpha}{\\beta^2}.\n$$\nFrom observed data $\\{x_i\\}_{i=1}^N$, compute the sample mean \n$$\nm = \\frac{1}{N} \\sum_{i=1}^N x_i\n$$\nand the population (non-unbiased) sample variance \n$$\ns^2 = \\frac{1}{N} \\sum_{i=1}^N (x_i - m)^2.\n$$\nMatching moments yields two equations in two unknowns $(\\alpha,\\beta)$:\n$$\nm = c \\, \\frac{\\alpha}{\\beta}, \\quad s^2 = c \\, \\frac{\\alpha}{\\beta} + c^2 \\, \\frac{\\alpha}{\\beta^2}.\n$$\nSubtracting gives \n$$\ns^2 - m = c^2 \\, \\frac{\\alpha}{\\beta^2}.\n$$\nSolving produces\n$$\n\\hat{\\alpha} = \\frac{m^2}{s^2 - m}, \\quad \\hat{\\beta} = c \\, \\frac{\\hat{\\alpha}}{m},\n$$\nassuming $s^2  m$. The boundary case $s^2 \\le m$ indicates at most Poisson variability without extra-Poisson dispersion; in that situation, we enforce $\\hat{\\alpha}$ large (e.g., $\\hat{\\alpha} = 10^6$), and set $\\hat{\\beta} = c \\, \\hat{\\alpha} / m$ to maintain $\\mathbb{E}[x_i] = m$ (if $m = 0$, use $\\hat{\\alpha} = 10^{-6}$, $\\hat{\\beta} = 10^{-6}$ to avoid division by zero). This regularization enforces strong shrinkage toward the sample mean where appropriate.\n\nWith $(\\hat{\\alpha}, \\hat{\\beta})$ obtained, the conjugate posterior for each $b_i$ is\n$$\nb_i \\mid x_i \\sim \\text{Gamma}(\\hat{\\alpha} + x_i, \\hat{\\beta} + c),\n$$\nwith posterior mean\n$$\n\\hat{b}_i^{\\text{hier}} = \\frac{\\hat{\\alpha} + x_i}{\\hat{\\beta} + c}.\n$$\nFor comparison, the per-cell maximum likelihood estimate (MLE) under the conditional Poisson likelihood is\n$$\n\\hat{b}_i^{\\text{MLE}} = \\frac{x_i}{c}.\n$$\nThe hierarchical posterior shrinkage difference for cell $i$ is then\n$$\ns_i = \\hat{b}_i^{\\text{hier}} - \\hat{b}_i^{\\text{MLE}},\n$$\nmeasured in mRNA molecules per burst, which quantifies the effect of shrinkage (movement from the MLE to the hierarchical posterior mean) for each cell. Averaging across cells yields\n$$\n\\overline{s} = \\frac{1}{N} \\sum_{i=1}^N s_i,\n$$\nalso in mRNA molecules per burst.\n\nFor the pooled analysis, we assume all cells share a single burst size parameter $b_{\\text{pool}}$ with prior $b_{\\text{pool}} \\sim \\text{Gamma}(\\alpha_0, \\beta_0)$ and likelihood comprising the product of cell-wise Poisson likelihoods. By conjugacy,\n$$\nb_{\\text{pool}} \\mid x_{1:N} \\sim \\text{Gamma}\\left(\\alpha_0 + \\sum_{i=1}^N x_i,\\, \\beta_0 + N c \\right),\n$$\nso the pooled posterior mean is\n$$\n\\hat{b}^{\\text{pool}} = \\frac{\\alpha_0 + \\sum_{i=1}^N x_i}{\\beta_0 + N c}.\n$$\nThe pooled impact difference for cell $i$ is\n$$\nd_i = \\hat{b}^{\\text{pool}} - \\hat{b}_i^{\\text{hier}},\n$$\nwith the average absolute impact across cells\n$$\n\\overline{|d|} = \\frac{1}{N} \\sum_{i=1}^N |d_i|,\n$$\nwhich is also in mRNA molecules per burst. Finally, to compare pooled to hierarchical scaling, compute the ratio\n$$\nR = \\frac{\\hat{b}^{\\text{pool}}}{\\frac{1}{N} \\sum_{i=1}^N \\hat{b}_i^{\\text{hier}}},\n$$\nwhich is dimensionless; note that when hierarchical hyperparameters are set via empirical Bayes, the average hierarchical posterior mean often tracks the sample mean scaled by $c$, whereas the pooled posterior mean incorporates the pooled prior $\\text{Gamma}(\\alpha_0,\\beta_0)$ and exposure $N c$, which can introduce differences reflected in $R$.\n\nAlgorithmic steps for each test case are thus:\n1. Compute $c = f \\, \\Delta t$.\n2. Compute $m$ and $s^2$ from the observed list of $x_i$ using population variance.\n3. Estimate $(\\hat{\\alpha}, \\hat{\\beta})$ via method of moments, applying boundary regularization when $s^2 \\le m$ or $m = 0$.\n4. For each cell, compute $\\hat{b}_i^{\\text{hier}} = (\\hat{\\alpha} + x_i)/(\\hat{\\beta} + c)$ and $\\hat{b}_i^{\\text{MLE}} = x_i/c$, then compute $s_i = \\hat{b}_i^{\\text{hier}} - \\hat{b}_i^{\\text{MLE}}$.\n5. Compute $\\overline{s}$ by averaging $s_i$.\n6. Compute pooled posterior mean $\\hat{b}^{\\text{pool}} = (\\alpha_0 + \\sum x_i)/(\\beta_0 + N c)$.\n7. Compute $d_i = \\hat{b}^{\\text{pool}} - \\hat{b}_i^{\\text{hier}}$, then $\\overline{|d|} = \\frac{1}{N} \\sum |d_i|$.\n8. Compute $R = \\hat{b}^{\\text{pool}} / \\left(\\frac{1}{N} \\sum \\hat{b}_i^{\\text{hier}}\\right)$.\n9. Round $\\overline{s}$, $\\overline{|d|}$, and $R$ to six decimal places.\n10. Output a single line containing the flattened list $[\\overline{s}_1,\\overline{|d|}_1,R_1,\\overline{s}_2,\\overline{|d|}_2,R_2,\\overline{s}_3,\\overline{|d|}_3,R_3,\\overline{s}_4,\\overline{|d|}_4,R_4]$.\n\nThis approach embodies principle-based reasoning: it begins from the conditional Poisson model for bursty production, uses the law of total expectation and variance for the Poisson-Gamma mixture (widely tested in single-cell analysis), leverages conjugacy for analytical tractability, and employs empirical Bayes via method-of-moments to estimate hyperparameters from data. The boundary handling ensures scientific realism when the data do not exhibit over-dispersion beyond Poisson noise. The computed metrics characterize shrinkage and pooled impact in a quantifiable manner and are expressed in appropriate units (mRNA molecules per burst) for differences, and as a dimensionless ratio for $R$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef empirical_bayes_hyperparams(counts, c, large_alpha=1e6):\n    \"\"\"\n    Estimate Gamma(shape=alpha, rate=beta) hyperparameters for burst size b_i\n    from Poisson-Gamma mixture moments, handling boundary cases.\n    \"\"\"\n    x = np.array(counts, dtype=float)\n    N = x.size\n    m = x.mean()\n    # Population (non-unbiased) variance\n    s2 = ((x - m) ** 2).mean()\n\n    if m == 0.0:\n        # Degenerate case: no counts observed\n        alpha_hat = 1e-6\n        beta_hat = 1e-6\n        return alpha_hat, beta_hat\n\n    if s2 > m:\n        alpha_hat = (m ** 2) / (s2 - m)\n        beta_hat = c * alpha_hat / m\n    else:\n        # No over-dispersion beyond Poisson; enforce strong shrinkage to mean\n        alpha_hat = large_alpha\n        beta_hat = c * alpha_hat / m\n\n    return alpha_hat, beta_hat\n\ndef hierarchical_post_means(counts, alpha_hat, beta_hat, c):\n    \"\"\"\n    Compute hierarchical posterior means for burst size b_i given counts.\n    Posterior: Gamma(alpha_hat + x_i, beta_hat + c), mean = (alpha_hat + x_i)/(beta_hat + c).\n    \"\"\"\n    x = np.array(counts, dtype=float)\n    denom = beta_hat + c\n    return (alpha_hat + x) / denom\n\ndef mle_burst_sizes(counts, c):\n    \"\"\"\n    Per-cell MLE for burst size under Poisson model: b_i_hat = x_i / c.\n    \"\"\"\n    x = np.array(counts, dtype=float)\n    return x / c\n\ndef pooled_posterior_mean(counts, alpha0, beta0, c):\n    \"\"\"\n    Pooled posterior mean for single burst size parameter across all cells:\n    Gamma(alpha0 + sum x_i, beta0 + N*c), mean = (alpha0 + sum x_i)/(beta0 + N*c).\n    \"\"\"\n    x = np.array(counts, dtype=float)\n    N = x.size\n    return (alpha0 + x.sum()) / (beta0 + N * c)\n\ndef summarize_metrics(counts, f, dt, alpha0, beta0):\n    \"\"\"\n    For a single test case, compute:\n    - average hierarchical shrinkage difference: mean(hier_mean - mle)\n    - average absolute pooled impact difference: mean(abs(pool_mean - hier_mean))\n    - ratio R: pool_mean / mean(hier_mean)\n    All bursts differences are in mRNA molecules per burst. Ratio is dimensionless.\n    \"\"\"\n    c = f * dt\n    alpha_hat, beta_hat = empirical_bayes_hyperparams(counts, c)\n    hier_means = hierarchical_post_means(counts, alpha_hat, beta_hat, c)\n    mle_means = mle_burst_sizes(counts, c)\n    shrink_diff = hier_means - mle_means\n    avg_shrink = float(shrink_diff.mean())\n\n    pool_mean = pooled_posterior_mean(counts, alpha0, beta0, c)\n    pooled_impact = pool_mean - hier_means\n    avg_abs_pooled_impact = float(np.mean(np.abs(pooled_impact)))\n\n    avg_hier_mean = float(hier_means.mean())\n    # Guard against division by zero in ratio\n    if avg_hier_mean == 0.0:\n        R = float('inf') if pool_mean > 0 else 1.0\n    else:\n        R = pool_mean / avg_hier_mean\n\n    # Round to six decimals as specified\n    return (round(avg_shrink, 6), round(avg_abs_pooled_impact, 6), round(R, 6))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"counts\": [3, 0, 1, 5, 2, 4, 6, 1, 0, 3],\n            \"f\": 0.5, \"dt\": 2.0,\n            \"alpha0\": 1.0, \"beta0\": 1.0\n        },\n        # Test case 2\n        {\n            \"counts\": [0, 0, 0, 1, 2, 0, 3, 0],\n            \"f\": 1.0, \"dt\": 1.0,\n            \"alpha0\": 0.5, \"beta0\": 0.5\n        },\n        # Test case 3\n        {\n            \"counts\": [10, 15, 12, 8, 20, 5, 18, 11, 7, 9, 14, 16],\n            \"f\": 2.0, \"dt\": 3.0,\n            \"alpha0\": 1.0, \"beta0\": 0.1\n        },\n        # Test case 4\n        {\n            \"counts\": [2, 9, 1],\n            \"f\": 0.2, \"dt\": 5.0,\n            \"alpha0\": 2.0, \"beta0\": 2.0\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        counts = case[\"counts\"]\n        f = case[\"f\"]\n        dt = case[\"dt\"]\n        alpha0 = case[\"alpha0\"]\n        beta0 = case[\"beta0\"]\n        avg_shrink, avg_abs_pooled_impact, R = summarize_metrics(counts, f, dt, alpha0, beta0)\n        # Append the three metrics per test case\n        results.extend([f\"{avg_shrink:.6f}\", f\"{avg_abs_pooled_impact:.6f}\", f\"{R:.6f}\"])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4388424"}]}