{"hands_on_practices": [{"introduction": "Before applying sophisticated algorithms, a strong grasp of the foundational statistics is essential. This first practice grounds your understanding of how to compare the binary presence-absence vectors at the heart of phylogenetic profiling. You will derive, from first principles, the mathematical identity between the familiar Pearson correlation coefficient and the phi coefficient for $2 \\times 2$ contingency tables, demonstrating precisely what is being measured when we assess the co-occurrence of genes across species [@problem_id:4375029].", "problem": "In systems biomedicine, phylogenetic profiling represents the presence or absence of genomic features (for example, orthologous gene families) across a panel of species as binary vectors. Consider two such profiles, $x,y \\in \\{0,1\\}^{N}$, for $N$ species. Define the empirical $2 \\times 2$ contingency counts by\n$$\nn_{11}=\\sum_{i=1}^{N}\\mathbf{1}\\{x_{i}=1,y_{i}=1\\},\\quad\nn_{10}=\\sum_{i=1}^{N}\\mathbf{1}\\{x_{i}=1,y_{i}=0\\},\\quad\nn_{01}=\\sum_{i=1}^{N}\\mathbf{1}\\{x_{i}=0,y_{i}=1\\},\\quad\nn_{00}=\\sum_{i=1}^{N}\\mathbf{1}\\{x_{i}=0,y_{i}=0\\},\n$$\nand note that $N=n_{11}+n_{10}+n_{01}+n_{00}$. Let $n_{1.}=n_{11}+n_{10}$, $n_{0.}=n_{01}+n_{00}$, $n_{.1}=n_{11}+n_{01}$, and $n_{.0}=n_{10}+n_{00}$ denote the marginal counts.\n\nStarting from first principles, use the definition of the Pearson correlation coefficient (PCC) for empirical data,\n$$\nr_{xy}=\\frac{\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sqrt{\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)^{2}}\\sqrt{\\sum_{i=1}^{N}\\left(y_{i}-\\bar{y}\\right)^{2}}},\n$$\nwhere $\\bar{x}=\\frac{1}{N}\\sum_{i=1}^{N}x_{i}$ and $\\bar{y}=\\frac{1}{N}\\sum_{i=1}^{N}y_{i}$, and the core definition of the phi coefficient as the standardized measure of association for $2 \\times 2$ contingency tables, to derive a single closed-form analytic expression for $r_{xy}$ entirely in terms of the four joint counts $n_{11}$, $n_{10}$, $n_{01}$, and $n_{00}$. Then, explain in words, without computing any additional formulas, the condition under which the Pearson correlation and the phi coefficient yield identical rankings of gene-pair associations in phylogenetic profiling.\n\nExpress the final answer as a single analytic expression in terms of $n_{11}$, $n_{10}$, $n_{01}$, and $n_{00}$ only. No units and no rounding are required.", "solution": "The user wants me to first validate and then solve a problem concerning the relationship between the Pearson correlation coefficient (PCC) and the phi coefficient for binary phylogenetic profiles.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Data:** Two binary vectors (phylogenetic profiles) $x, y \\in \\{0,1\\}^{N}$ for $N$ species.\n*   **Contingency Counts:**\n    *   $n_{11} = \\sum_{i=1}^{N}\\mathbf{1}\\{x_{i}=1,y_{i}=1\\}$\n    *   $n_{10} = \\sum_{i=1}^{N}\\mathbf{1}\\{x_{i}=1,y_{i}=0\\}$\n    *   $n_{01} = \\sum_{i=1}^{N}\\mathbf{1}\\{x_{i}=0,y_{i}=1\\}$\n    *   $n_{00} = \\sum_{i=1}^{N}\\mathbf{1}\\{x_{i}=0,y_{i}=0\\}$\n*   **Total Count:** $N=n_{11}+n_{10}+n_{01}+n_{00}$.\n*   **Marginal Counts:**\n    *   $n_{1.} = n_{11}+n_{10}$\n    *   $n_{0.} = n_{01}+n_{00}$\n    *   $n_{.1} = n_{11}+n_{01}$\n    *   $n_{.0} = n_{10}+n_{00}$\n*   **Pearson Correlation Coefficient (PCC) Definition:**\n    $$\n    r_{xy}=\\frac{\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sqrt{\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)^{2}}\\sqrt{\\sum_{i=1}^{N}\\left(y_{i}-\\bar{y}\\right)^{2}}}\n    $$\n*   **Mean Definitions:** $\\bar{x}=\\frac{1}{N}\\sum_{i=1}^{N}x_{i}$ and $\\bar{y}=\\frac{1}{N}\\sum_{i=1}^{N}y_{i}$.\n*   **Task:**\n    1.  Derive a closed-form expression for $r_{xy}$ in terms of $n_{11}$, $n_{10}$, $n_{01}$, and $n_{00}$.\n    2.  Explain the condition under which PCC and the phi coefficient yield identical rankings for gene-pair associations. The problem statement refers to the phi coefficient via its definition as \"the standardized measure of association for $2 \\times 2$ contingency tables.\"\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically and mathematically sound. It asks for a standard derivation in statistics, specifically the proof that the Pearson correlation coefficient for binary variables is equivalent to the phi coefficient.\n\n*   **Scientific Grounding:** The problem is firmly grounded in biostatistics and bioinformatics. Phylogenetic profiling and the use of contingency tables to measure association between gene presences/absences are standard techniques. The Pearson correlation and phi coefficient are fundamental statistical measures.\n*   **Well-Posed:** The problem is well-posed. The definitions are precise, and the objective is clear: to perform an algebraic derivation. The result is a unique, exact analytical expression.\n*   **Objective:** The language is formal, precise, and free of any subjective or ambiguous terminology.\n*   **Flaw Checklist:**\n    1.  **Scientific/Factual Unsoundness:** None. The premises are correct.\n    2.  **Non-Formalizable/Irrelevant:** None. The problem is formal and directly relevant to the specified field.\n    3.  **Incomplete/Contradictory:** None. While the formula for the phi coefficient is not explicitly given, the problem directs the user to derive the formula for $r_{xy}$ from first principles. The context implies that this derived formula will be that of the phi coefficient. This is a standard result, and all information needed for the derivation is provided.\n    4.  **Unrealistic/Infeasible:** None. The context is a standard application in bioinformatics.\n    5.  **Ill-Posed/Poorly Structured:** None. A unique solution exists.\n    6.  **Pseudo-Profound/Trivial/Tautological:** None. The derivation requires multiple non-trivial algebraic steps.\n    7.  **Outside Scientific Verifiability:** None. The derivation is a mathematical proof.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. I will proceed with the derivation and explanation.\n\n### Solution\n\nThe objective is to derive an expression for the Pearson correlation coefficient, $r_{xy}$, for binary data entirely in terms of the four joint contingency counts $n_{11}$, $n_{10}$, $n_{01}$, and $n_{00}$. We will start from the provided definition of $r_{xy}$ and systematically express each of its components using the given counts.\n\nThe Pearson correlation coefficient is defined as:\n$$\nr_{xy}=\\frac{\\text{Cov}(x, y)}{\\sigma_x \\sigma_y} = \\frac{\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sqrt{\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)^{2}}\\sqrt{\\sum_{i=1}^{N}\\left(y_{i}-\\bar{y}\\right)^{2}}}\n$$\n\nFirst, we express the sample means, $\\bar{x}$ and $\\bar{y}$, in terms of the counts.\nThe sum $\\sum_{i=1}^{N}x_{i}$ counts the number of species where gene $x$ is present (i.e., $x_i=1$). This corresponds to the cases where ($x_i=1, y_i=1$) or ($x_i=1, y_i=0$). Therefore, $\\sum_{i=1}^{N}x_{i} = n_{11} + n_{10} = n_{1.}$. The mean $\\bar{x}$ is:\n$$\n\\bar{x} = \\frac{1}{N}\\sum_{i=1}^{N}x_{i} = \\frac{n_{1.}}{N} = \\frac{n_{11}+n_{10}}{N}\n$$\nBy symmetry, the mean $\\bar{y}$ is:\n$$\n\\bar{y} = \\frac{1}{N}\\sum_{i=1}^{N}y_{i} = \\frac{n_{.1}}{N} = \\frac{n_{11}+n_{01}}{N}\n$$\n\nNext, we evaluate the terms in the denominator, which are related to the variances.\nFor the term $\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)^{2}$, we use the identity $\\sum (z - \\bar{z})^2 = \\sum z^2 - N\\bar{z}^2$. Since the data are binary, $x_i \\in \\{0,1\\}$, we have the property that $x_i^2 = x_i$. Thus, $\\sum_{i=1}^{N}x_i^2 = \\sum_{i=1}^{N}x_i = n_{1.}$.\nThe term becomes:\n$$\n\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)^{2} = \\sum_{i=1}^{N}x_{i}^2 - N\\bar{x}^2 = n_{1.} - N\\left(\\frac{n_{1.}}{N}\\right)^2 = n_{1.} - \\frac{n_{1.}^2}{N} = \\frac{Nn_{1.} - n_{1.}^2}{N} = \\frac{n_{1.}(N-n_{1.})}{N}\n$$\nSince $N = n_{1.} + n_{0.}$, we have $N - n_{1.} = n_{0.}$. So, we get:\n$$\n\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)^{2} = \\frac{n_{1.}n_{0.}}{N}\n$$\nBy symmetry for the variable $y$:\n$$\n\\sum_{i=1}^{N}\\left(y_{i}-\\bar{y}\\right)^{2} = \\frac{n_{.1}n_{.0}}{N}\n$$\n\nNow, we evaluate the numerator, which is related to the covariance.\nWe use the identity $\\sum (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum x_iy_i - N\\bar{x}\\bar{y}$.\nThe product $x_i y_i$ is equal to $1$ if and only if both $x_i=1$ and $y_i=1$. In all other cases, the product is $0$. Therefore, the sum $\\sum_{i=1}^{N}x_i y_i$ simply counts the number of species for which both genes are present, which is by definition $n_{11}$.\nThe numerator term becomes:\n$$\n\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right) = n_{11} - N\\left(\\frac{n_{1.}}{N}\\right)\\left(\\frac{n_{.1}}{N}\\right) = n_{11} - \\frac{n_{1.}n_{.1}}{N} = \\frac{Nn_{11} - n_{1.}n_{.1}}{N}\n$$\nTo simplify the term $Nn_{11} - n_{1.}n_{.1}$, we substitute the definitions of $N$, $n_{1.}$, and $n_{.1}$:\n\\begin{align*}\nNn_{11} - n_{1.}n_{.1} &= (n_{11}+n_{10}+n_{01}+n_{00})n_{11} - (n_{11}+n_{10})(n_{11}+n_{01}) \\\\\n&= (n_{11}^2 + n_{10}n_{11} + n_{01}n_{11} + n_{00}n_{11}) - (n_{11}^2 + n_{11}n_{01} + n_{10}n_{11} + n_{10}n_{01}) \\\\\n&= n_{11}^2 + n_{10}n_{11} + n_{01}n_{11} + n_{00}n_{11} - n_{11}^2 - n_{11}n_{01} - n_{10}n_{11} - n_{10}n_{01}\n\\end{align*}\nCanceling terms, we are left with:\n$$\nNn_{11} - n_{1.}n_{.1} = n_{11}n_{00} - n_{10}n_{01}\n$$\nSo the numerator of $r_{xy}$ is $\\frac{n_{11}n_{00} - n_{10}n_{01}}{N}$.\n\nFinally, we assemble the complete expression for $r_{xy}$:\n$$\nr_{xy} = \\frac{\\frac{n_{11}n_{00} - n_{10}n_{01}}{N}}{\\sqrt{\\left(\\frac{n_{1.}n_{0.}}{N}\\right)\\left(\\frac{n_{.1}n_{.0}}{N}\\right)}} = \\frac{\\frac{n_{11}n_{00} - n_{10}n_{01}}{N}}{\\frac{\\sqrt{n_{1.}n_{0.}n_{.1}n_{.0}}}{N}}\n$$\nThe factor of $N$ in the denominator of the numerator and the denominator of the overall expression cancels out, yielding:\n$$\nr_{xy} = \\frac{n_{11}n_{00} - n_{10}n_{01}}{\\sqrt{n_{1.}n_{0.}n_{.1}n_{.0}}}\n$$\nThis expression is the definition of the phi coefficient, $\\phi$. To provide the final answer solely in terms of the four joint counts, we substitute the definitions of the marginal counts ($n_{1.}, n_{0.}, n_{.1}, n_{.0}$):\n$$\nr_{xy} = \\frac{n_{11}n_{00} - n_{10}n_{01}}{\\sqrt{(n_{11}+n_{10})(n_{01}+n_{00})(n_{11}+n_{01})(n_{10}+n_{00})}}\n$$\nThis completes the derivation.\n\nRegarding the second part of the question: \"explain in words, without computing any additional formulas, the condition under which the Pearson correlation and the phi coefficient yield identical rankings of gene-pair associations in phylogenetic profiling.\"\n\nThe derivation above demonstrates that for binary variables, such as the presence/absence vectors in phylogenetic profiling, the Pearson correlation coefficient is algebraically identical to the phi coefficient. They are not two different measures that happen to coincide under certain conditions; rather, the phi coefficient *is* the Pearson correlation coefficient specialized for the case of two binary variables. Consequently, since $r_{xy}$ and $\\phi$ represent the same mathematical quantity in this context, they will always produce identical values for any given gene pair. Therefore, they will necessarily yield identical rankings of associations across any set of gene pairs. The only \"condition\" for this identity to hold is that the data being correlated must be binary, which is a foundational assumption of the problem statement itself (\"phylogenetic profiling represents the presence or absence ... as binary vectors\").", "answer": "$$\n\\boxed{\\frac{n_{11}n_{00} - n_{10}n_{01}}{\\sqrt{(n_{11}+n_{10})(n_{01}+n_{00})(n_{11}+n_{01})(n_{10}+n_{00})}}}\n$$", "id": "4375029"}, {"introduction": "Moving from idealized theory to real-world application requires us to consider the impact of data imperfections. Orthology inference, a critical upstream step for building phylogenetic profiles, is prone to errors such as the over-splitting of a single gene family into multiple groups. This exercise challenges you to build a probabilistic model to quantify how such an error systematically degrades a common association metric, the Jaccard similarity, providing crucial intuition about the robustness of functional linkage predictions [@problem_id:4375036].", "problem": "In phylogenetic profiling, presence or absence of genes across a panel of $N$ species is used to infer functional association via set-similarity measures. Consider a true functional partner pair of genes with a single Orthologous Group (OG) for gene $A$ and a single OG for gene $B$. Suppose their true evolutionary fates are perfectly coupled: in any species, $B$ is truly present if and only if $A$ is truly present. Let the probability that a randomly sampled species truly contains the pair be $p \\in (0,1)$, independently across species. Assume a detection process with random missingness: if a gene is truly present in a species, it is observed independently with probability $r \\in (0,1)$; if it is truly absent, it is never observed. An analyst computes similarity between the observed presence vectors using the Jaccard similarity coefficient (Jaccard), defined for two sets $X$ and $Y$ of species indices as $|X \\cap Y|/|X \\cup Y|$.\n\nNow consider an over-splitting error in the orthology inference for gene $A$ that produces two disjoint subgroups, $A_{1}$ and $A_{2}$. Conditional on true presence of $A$ in a species, the species is assigned to $A_{1}$ with probability $s \\in (0,1)$ and to $A_{2}$ with probability $1-s$, independently across species. Detection applies independently to each subgroup as above. The analyst, unaware of the split, computes the Jaccard between $B$ and $A_{1}$.\n\nAssume $N \\to \\infty$ so that by the Law of Large Numbers (LLN), the empirical Jaccard converges almost surely to the ratio of the corresponding probabilities of joint observation and union of observations under the generative model.\n\nStarting from the definitions above and standard probability rules, derive a closed-form analytic expression for the change in the large-sample expected Jaccard induced by the split, defined as the difference between the limiting Jaccard for the pair $(B,A_{1})$ and the limiting Jaccard for the unsplit pair $(B,A)$. Express your final answer as a function of $r$ and $s$ only. No numerical evaluation is required, and no units are involved. Provide a single closed-form expression as your final answer.", "solution": "The problem is deemed valid as it is scientifically grounded in probabilistic modeling, well-posed, objective, and self-contained. It describes a plausible scenario in bioinformatics and asks for a rigorous mathematical derivation.\n\nLet us begin by formalizing the problem. For any two sets of observations $X$ and $Y$, the Jaccard similarity is defined as $J(X,Y) = \\frac{|X \\cap Y|}{|X \\cup Y|}$. Under the assumption that the number of species $N \\to \\infty$, the Law of Large Numbers allows us to equate the empirical Jaccard coefficient to the ratio of probabilities of the underlying events for a single randomly sampled species. If $O_X$ is the event that gene $X$ is observed and $O_Y$ is the event that gene $Y$ is observed in a species, the limiting Jaccard similarity is:\n$$ J(X,Y) = \\frac{P(O_X \\cap O_Y)}{P(O_X \\cup O_Y)} $$\nLet us define the following events for a single, randomly sampled species:\n- $T$: The gene pair $(A, B)$ is truly present. By problem statement, $P(T) = p$.\n- $T^c$: The gene pair is not truly present. $P(T^c) = 1-p$.\n- $O_A$: Gene $A$ is observed.\n- $O_B$: Gene $B$ is observed.\n- $C_{A1}$: A present gene $A$ is assigned to subgroup $A_1$. $P(C_{A1}|T) = s$.\n- $O_{A1}$: Gene $A$ is observed as a member of subgroup $A_1$.\n\nThe detection process is such that a gene is observed with probability $r$ if and only if it is truly present. The detection events for different genes are independent, conditional on true presence. The splitting assignment is also an independent process.\n\nFirst, we calculate the Jaccard similarity for the unsplit pair $(B, A)$, denoted as $J_{unsplit}$.\nAn observation of a gene, say $A$, can only occur if the gene is truly present. Thus, the event $O_A$ is a subset of the event $T$.\n$$ P(O_A) = P(O_A|T)P(T) + P(O_A|T^c)P(T^c) = r \\cdot p + 0 \\cdot (1-p) = pr $$\nSimilarly, $P(O_B) = pr$.\n\nThe probability of joint observation is $P(O_A \\cap O_B)$. For both to be observed, the pair must be truly present ($T$), and both detection events must occur. Conditional on $T$, the detections of $A$ and $B$ are independent events, each with probability $r$.\n$$ P(O_A \\cap O_B) = P(O_A \\cap O_B | T)P(T) = (P(O_A|T) \\cdot P(O_B|T)) \\cdot P(T) = (r \\cdot r) \\cdot p = pr^2 $$\nThe probability of the union of observations is found using the principle of inclusion-exclusion:\n$$ P(O_A \\cup O_B) = P(O_A) + P(O_B) - P(O_A \\cap O_B) = pr + pr - pr^2 = p(2r - r^2) = pr(2-r) $$\nNow, we can compute $J_{unsplit}$:\n$$ J_{unsplit} = \\frac{P(O_A \\cap O_B)}{P(O_A \\cup O_B)} = \\frac{pr^2}{pr(2-r)} = \\frac{r}{2-r} $$\n\nNext, we calculate the Jaccard similarity for the pair $(B, A_1)$, denoted as $J_{split}$. The analyst compares the observations of gene $B$ with observations of subgroup $A_1$.\nThe event $O_{A1}$ (observing $A_1$) occurs if gene $A$ is truly present ($T$), it is assigned to subgroup $A_1$ ($C_{A1}$), and it is detected. These are independent processes conditional on $T$.\n$$ P(O_{A1}) = P(O_{A1}|T)P(T) $$\nThe event of observing $A_1$ conditional on $T$ consists of successful assignment to $A_1$ and detection.\n$$ P(O_{A1}|T) = P(C_{A1} \\text{ and detection}|T) = P(C_{A1}|T) \\cdot P(\\text{detection}|T) = s \\cdot r = sr $$\nTherefore, the total probability of observing $A_1$ is:\n$$ P(O_{A1}) = (sr) \\cdot p = prs $$\nThe observation of gene $B$, $O_B$, is unaffected by the split of $A$, so $P(O_B) = pr$.\n\nThe probability of joint observation $P(O_{A1} \\cap O_B)$ requires the pair to be truly present ($T$), $B$ to be detected, $A$ to be assigned to $A_1$, and $A$ to be detected. Conditional on $T$, these three events are independent.\n$$ P(O_{A1} \\cap O_B) = P(O_{A1} \\cap O_B | T) P(T) $$\n$$ P(O_{A1} \\cap O_B | T) = P(\\text{detection of } B|T) \\cdot P(C_{A1}|T) \\cdot P(\\text{detection of } A|T) = r \\cdot s \\cdot r = sr^2 $$\nTherefore, the joint probability is:\n$$ P(O_{A1} \\cap O_B) = (sr^2) \\cdot p = psr^2 $$\nThe probability of the union of observations is:\n$$ P(O_{A1} \\cup O_B) = P(O_{A1}) + P(O_B) - P(O_{A1} \\cap O_B) = prs + pr - psr^2 = p(rs + r - r^2s) = pr(s+1-rs) $$\nNow, we can compute $J_{split}$:\n$$ J_{split} = \\frac{P(O_{A1} \\cap O_B)}{P(O_{A1} \\cup O_B)} = \\frac{psr^2}{pr(1+s-rs)} = \\frac{sr}{1+s-rs} $$\n\nFinally, we derive the change in the large-sample expected Jaccard, $\\Delta J = J_{split} - J_{unsplit}$.\n$$ \\Delta J = \\frac{sr}{1+s-rs} - \\frac{r}{2-r} $$\nTo simplify, we find a common denominator:\n$$ \\Delta J = r \\left( \\frac{s}{1+s-rs} - \\frac{1}{2-r} \\right) = r \\left( \\frac{s(2-r) - 1(1+s-rs)}{(1+s-rs)(2-r)} \\right) $$\nExpanding the terms in the numerator of the fraction:\n$$ \\Delta J = r \\left( \\frac{2s - sr - 1 - s + rs}{(1+s-rs)(2-r)} \\right) $$\nThe terms $-sr$ and $+rs$ cancel each other out.\n$$ \\Delta J = r \\left( \\frac{2s - 1 - s}{(1+s-rs)(2-r)} \\right) = r \\left( \\frac{s-1}{(1+s-rs)(2-r)} \\right) $$\nThis gives the final closed-form expression for the change in Jaccard similarity.\n$$ \\Delta J = \\frac{r(s-1)}{(1+s-rs)(2-r)} $$\nAs required, this expression is a function of $r$ and $s$ only. The true presence probability $p$ is cancelled out. Since $s \\in (0,1)$ and $r \\in (0,1)$, the term $s-1$ is negative, while all other factors are positive, correctly indicating that an orthology split always reduces the inferred similarity.", "answer": "$$\\boxed{\\frac{r(s-1)}{(1+s-sr)(2-r)}}$$", "id": "4375036"}, {"introduction": "This capstone practice brings together theory and implementation to address the most significant confounder in comparative genomics: the shared evolutionary history of species. Simply comparing profiles as if each species were an independent data point is statistically invalid and can lead to spurious conclusions. This exercise guides you through the implementation of a phylogenetically-aware association score using generalized least squares, a cornerstone of modern comparative methods, and further asks you to propagate uncertainty from the species tree to construct a robust confidence interval for your finding [@problem_id:4375055].", "problem": "You are given a collection of species-level covariance matrices that arise from bootstrap replicates of an estimated species phylogeny under the Brownian motion model of trait evolution. In phylogenetic profiling for functional annotation, an association score between two gene profiles across species should account for shared evolutionary history. Under the Brownian motion model, if a continuous trait evolves along a species tree with total root-to-tip time $T$, then the trait covariance matrix across species equals the shared evolutionary time among species, which can be estimated from the species tree. Bootstrap replicates of the species tree induce a set of covariance matrices that approximate the uncertainty due to tree inference.\n\nYour task is to write a program that, for each test case, computes a phylogenetically corrected association score between two gene profiles by propagating the variation across bootstrap covariance matrices, and then reports a two-sided confidence interval. The program must implement the following procedure from first principles:\n\n1. For a given covariance matrix $\\mathbf{C} \\in \\mathbb{R}^{n \\times n}$ that is symmetric positive definite, compute a phylogenetically corrected association score between two vectors $\\mathbf{x} \\in \\mathbb{R}^{n}$ and $\\mathbf{y} \\in \\mathbb{R}^{n}$ representing gene profiles across $n$ species. Use the following steps:\n   - Compute the Cholesky factorization $\\mathbf{C} = \\mathbf{L}\\mathbf{L}^{\\top}$ with $\\mathbf{L}$ lower triangular.\n   - Solve $\\mathbf{L}\\mathbf{z}_{x} = \\mathbf{x}$ and $\\mathbf{L}\\mathbf{z}_{y} = \\mathbf{y}$ to obtain whitened vectors $\\mathbf{z}_{x}$ and $\\mathbf{z}_{y}$.\n   - Center each whitened vector by subtracting its arithmetic mean: $\\tilde{\\mathbf{z}}_{x} = \\mathbf{z}_{x} - \\bar{z}_{x}\\mathbf{1}$ and $\\tilde{\\mathbf{z}}_{y} = \\mathbf{z}_{y} - \\bar{z}_{y}\\mathbf{1}$, where $\\bar{z}_{x} = \\frac{1}{n}\\sum_{i=1}^{n} z_{x,i}$ and $\\bar{z}_{y} = \\frac{1}{n}\\sum_{i=1}^{n} z_{y,i}$, and $\\mathbf{1}$ is the vector of ones in $\\mathbb{R}^{n}$.\n   - Define the phylogenetically corrected association score as the Pearson correlation of the centered whitened vectors,\n     $$ r = \\frac{\\tilde{\\mathbf{z}}_{x}^{\\top}\\tilde{\\mathbf{z}}_{y}}{\\|\\tilde{\\mathbf{z}}_{x}\\|_{2}\\,\\|\\tilde{\\mathbf{z}}_{y}\\|_{2}}. $$\n     If $\\|\\tilde{\\mathbf{z}}_{x}\\|_{2} = 0$ or $\\|\\tilde{\\mathbf{z}}_{y}\\|_{2} = 0$, define $r = 0$ for that replicate to avoid division by zero.\n   This construction implements generalized least squares via whitening, justified by the Brownian motion model and the assumption of Gaussian increments.\n\n2. For a set of $K$ bootstrap covariance matrices $\\{\\mathbf{C}^{(k)}\\}_{k=1}^{K}$, compute $K$ association scores $\\{r^{(k)}\\}_{k=1}^{K}$, one per replicate covariance matrix. For a given two-sided confidence level $1-\\alpha$, with $\\alpha \\in (0,1)$, compute the lower and upper confidence limits as empirical quantiles at $q_{\\mathrm{low}} = \\alpha/2$ and $q_{\\mathrm{high}} = 1 - \\alpha/2$. Use the following quantile definition (piecewise linear interpolation between order statistics, also known as the type-$7$ quantile):\n   - Let the sorted values be $v_{(1)} \\le \\cdots \\le v_{(K)}$.\n   - For a quantile level $q \\in [0,1]$, compute\n     $$ h = (K - 1)q + 1, \\quad k = \\lfloor h \\rfloor, \\quad \\gamma = h - k. $$\n   - The quantile is\n     $$ Q(q) = \\begin{cases}\n     v_{(k)} & \\text{if } \\gamma = 0,\\\\\n     v_{(k)} + \\gamma\\left(v_{(k+1)} - v_{(k)}\\right) & \\text{if } \\gamma \\in (0,1).\n     \\end{cases} $$\n\n3. Apply this procedure to each test case below with $\\alpha = 0.05$ (that is, $q_{\\mathrm{low}} = 0.025$ and $q_{\\mathrm{high}} = 0.975$). For numerical stability, if a covariance matrix is nearly singular, you may add a small diagonal jitter $\\varepsilon \\mathbf{I}$ with $\\varepsilon = 10^{-10}$ before Cholesky factorization.\n\nTest suite. Each test case consists of a pair $(\\mathbf{x}, \\mathbf{y})$ and a list of bootstrap covariance matrices. All numbers are real-valued. Vectors are listed in species order $\\{s_{1}, s_{2}, s_{3}, s_{4}\\}$.\n\n- Test case $1$ (happy path; strong concordance):\n  - $\\mathbf{x} = [\\,1,\\,1,\\,0,\\,0\\,]$, $\\mathbf{y} = [\\,1,\\,1,\\,0,\\,0\\,]$.\n  - Bootstrap covariance matrices ($K = 5$):\n    - $\\mathbf{C}^{(1)} = \\begin{bmatrix}\n      1.0 & 0.8 & 0.2 & 0.2\\\\\n      0.8 & 1.0 & 0.2 & 0.2\\\\\n      0.2 & 0.2 & 1.0 & 0.7\\\\\n      0.2 & 0.2 & 0.7 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(2)} = \\begin{bmatrix}\n      1.0 & 0.75 & 0.25 & 0.25\\\\\n      0.75 & 1.0 & 0.25 & 0.25\\\\\n      0.25 & 0.25 & 1.0 & 0.65\\\\\n      0.25 & 0.25 & 0.65 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(3)} = \\begin{bmatrix}\n      1.0 & 0.85 & 0.15 & 0.15\\\\\n      0.85 & 1.0 & 0.15 & 0.15\\\\\n      0.15 & 0.15 & 1.0 & 0.6\\\\\n      0.15 & 0.15 & 0.6 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(4)} = \\begin{bmatrix}\n      1.0 & 0.7 & 0.3 & 0.2\\\\\n      0.7 & 1.0 & 0.2 & 0.3\\\\\n      0.3 & 0.2 & 1.0 & 0.55\\\\\n      0.2 & 0.3 & 0.55 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(5)} = \\begin{bmatrix}\n      1.0 & 0.78 & 0.22 & 0.22\\\\\n      0.78 & 1.0 & 0.22 & 0.22\\\\\n      0.22 & 0.22 & 1.0 & 0.68\\\\\n      0.22 & 0.22 & 0.68 & 1.0\n      \\end{bmatrix}$.\n\n- Test case $2$ (boundary; near-star trees, perfect concordance):\n  - $\\mathbf{x} = [\\,0,\\,1,\\,0,\\,1\\,]$, $\\mathbf{y} = [\\,0,\\,1,\\,0,\\,1\\,]$.\n  - Bootstrap covariance matrices ($K = 5$):\n    - $\\mathbf{C}^{(1)} = \\begin{bmatrix}\n      1.0 & 0.4 & 0.4 & 0.4\\\\\n      0.4 & 1.0 & 0.4 & 0.4\\\\\n      0.4 & 0.4 & 1.0 & 0.4\\\\\n      0.4 & 0.4 & 0.4 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(2)} = \\begin{bmatrix}\n      1.0 & 0.3 & 0.3 & 0.3\\\\\n      0.3 & 1.0 & 0.3 & 0.3\\\\\n      0.3 & 0.3 & 1.0 & 0.3\\\\\n      0.3 & 0.3 & 0.3 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(3)} = \\begin{bmatrix}\n      1.0 & 0.2 & 0.2 & 0.2\\\\\n      0.2 & 1.0 & 0.2 & 0.2\\\\\n      0.2 & 0.2 & 1.0 & 0.2\\\\\n      0.2 & 0.2 & 0.2 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(4)} = \\begin{bmatrix}\n      1.0 & 0.5 & 0.5 & 0.5\\\\\n      0.5 & 1.0 & 0.5 & 0.5\\\\\n      0.5 & 0.5 & 1.0 & 0.5\\\\\n      0.5 & 0.5 & 0.5 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(5)} = \\begin{bmatrix}\n      1.0 & 0.35 & 0.35 & 0.35\\\\\n      0.35 & 1.0 & 0.35 & 0.35\\\\\n      0.35 & 0.35 & 1.0 & 0.35\\\\\n      0.35 & 0.35 & 0.35 & 1.0\n      \\end{bmatrix}$.\n\n- Test case $3$ (edge case; discordant patterns across bootstrap trees):\n  - $\\mathbf{x} = [\\,1,\\,1,\\,0,\\,0\\,]$, $\\mathbf{y} = [\\,1,\\,0,\\,1,\\,0\\,]$.\n  - Bootstrap covariance matrices ($K = 5$):\n    - $\\mathbf{C}^{(1)} = \\begin{bmatrix}\n      1.0 & 0.8 & 0.2 & 0.2\\\\\n      0.8 & 1.0 & 0.2 & 0.2\\\\\n      0.2 & 0.2 & 1.0 & 0.7\\\\\n      0.2 & 0.2 & 0.7 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(2)} = \\begin{bmatrix}\n      1.0 & 0.2 & 0.8 & 0.2\\\\\n      0.2 & 1.0 & 0.2 & 0.75\\\\\n      0.8 & 0.2 & 1.0 & 0.2\\\\\n      0.2 & 0.75 & 0.2 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(3)} = \\begin{bmatrix}\n      1.0 & 0.2 & 0.2 & 0.8\\\\\n      0.2 & 1.0 & 0.75 & 0.2\\\\\n      0.2 & 0.75 & 1.0 & 0.2\\\\\n      0.8 & 0.2 & 0.2 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(4)} = \\begin{bmatrix}\n      1.0 & 0.6 & 0.4 & 0.3\\\\\n      0.6 & 1.0 & 0.3 & 0.4\\\\\n      0.4 & 0.3 & 1.0 & 0.6\\\\\n      0.3 & 0.4 & 0.6 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(5)} = \\begin{bmatrix}\n      1.0 & 0.3 & 0.3 & 0.3\\\\\n      0.3 & 1.0 & 0.3 & 0.3\\\\\n      0.3 & 0.3 & 1.0 & 0.3\\\\\n      0.3 & 0.3 & 0.3 & 1.0\n      \\end{bmatrix}$.\n\nImplementation requirements:\n- Use the exact quantile definition above with $q_{\\mathrm{low}} = 0.025$ and $q_{\\mathrm{high}} = 0.975$.\n- If Cholesky factorization fails due to numerical issues, you may add a jitter $\\varepsilon = 10^{-10}$ to the diagonal.\n- For each test case, report the confidence interval as a pair $[L, U]$, where $L$ and $U$ are real numbers rounded to $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three test cases as a list of three intervals, each interval itself being a two-element list, with no spaces, and numbers rounded to $6$ decimal places. For example: $[[0.123456,0.654321],[0.100000,0.900000],[\\dots,\\dots]]$.", "solution": "The problem requires the implementation of a procedure to compute a phylogenetically corrected association score between two gene profiles and to construct a confidence interval for this score based on bootstrap replicates of the underlying species phylogeny. The problem is scientifically sound, mathematically well-defined, and provides all necessary data and specifications. We may therefore proceed with a solution.\n\nThe solution is grounded in three main principles:\n1.  Correcting for non-independence of data using Generalized Least Squares (GLS) via data whitening.\n2.  Calculating the association score as a Pearson correlation coefficient on the whitened data.\n3.  Quantifying uncertainty using a bootstrap percentile confidence interval.\n\n**1. Phylogenetic Correction via Data Whitening**\n\nIn evolutionary biology, traits measured on a set of related species are not statistically independent. The shared evolutionary history induces a covariance structure that must be accounted for in statistical analyses. The Brownian motion model of trait evolution posits that a continuous trait vector $\\mathbf{x} \\in \\mathbb{R}^{n}$ for $n$ species follows a multivariate normal distribution, $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{C})$, where $\\boldsymbol{\\mu}$ is the mean vector and $\\mathbf{C} \\in \\mathbb{R}^{n \\times n}$ is the covariance matrix. The elements $C_{ij}$ of this matrix are proportional to the shared branch length (evolutionary time) from the root of the phylogeny to the most recent common ancestor of species $i$ and $j$.\n\nStandard statistical methods like Pearson correlation assume independent and identically distributed (i.i.d.) observations. To use such methods, we must first transform the data to remove the phylogenetic correlations. This process is known as \"whitening.\" We seek a linear transformation matrix $\\mathbf{M}$ such that the transformed vector $\\mathbf{z} = \\mathbf{M}\\mathbf{x}$ has a covariance matrix equal to the identity matrix $\\mathbf{I}$. The covariance of $\\mathbf{z}$ is given by:\n$$ \\text{Cov}(\\mathbf{z}) = \\text{Cov}(\\mathbf{M}\\mathbf{x}) = \\mathbf{M} \\text{Cov}(\\mathbf{x}) \\mathbf{M}^{\\top} = \\mathbf{M}\\mathbf{C}\\mathbf{M}^{\\top} $$\nWe require $\\mathbf{M}\\mathbf{C}\\mathbf{M}^{\\top} = \\mathbf{I}$.\n\nA standard method to find such a matrix $\\mathbf{M}$ is through the Cholesky factorization of the symmetric positive definite covariance matrix $\\mathbf{C}$. The Cholesky factorization is $\\mathbf{C} = \\mathbf{L}\\mathbf{L}^{\\top}$, where $\\mathbf{L}$ is a unique lower triangular matrix with positive diagonal entries.\n\nBy choosing the whitening matrix to be the inverse of the Cholesky factor, $\\mathbf{M} = \\mathbf{L}^{-1}$, we achieve the desired property:\n$$ \\mathbf{M}\\mathbf{C}\\mathbf{M}^{\\top} = \\mathbf{L}^{-1} (\\mathbf{L}\\mathbf{L}^{\\top}) (\\mathbf{L}^{-1})^{\\top} = (\\mathbf{L}^{-1}\\mathbf{L}) (\\mathbf{L}^{\\top}(\\mathbf{L}^{\\top})^{-1}) = \\mathbf{I} \\mathbf{I} = \\mathbf{I} $$\nThus, the whitened vector is $\\mathbf{z} = \\mathbf{L}^{-1}\\mathbf{x}$. Instead of explicitly computing the inverse $\\mathbf{L}^{-1}$, it is more numerically stable to obtain $\\mathbf{z}$ by solving the lower triangular linear system $\\mathbf{L}\\mathbf{z} = \\mathbf{x}$ using forward substitution. A small diagonal jitter $\\varepsilon\\mathbf{I}$ (with $\\varepsilon = 10^{-10}$) may be added to $\\mathbf{C}$ to ensure it is numerically positive definite before factorization.\n\n**2. Calculation of the Phylogenetically Corrected Association Score**\n\nOnce we have transformed the gene profiles $\\mathbf{x}$ and $\\mathbf{y}$ into their whitened counterparts $\\mathbf{z}_{x}$ and $\\mathbf{z}_{y}$, the components of these new vectors are uncorrelated and have unit variance. We can now compute a meaningful association score using the standard Pearson correlation coefficient.\n\nThe Pearson correlation coefficient measures the linear relationship between two datasets. It is defined for two vectors $\\mathbf{a}$ and $\\mathbf{b}$ as the covariance of the two vectors divided by the product of their standard deviations. This is equivalent to computing the dot product of their centered versions, normalized by their Euclidean norms.\n\nFirst, we center the whitened vectors by subtracting their respective arithmetic means:\n$$ \\tilde{\\mathbf{z}}_{x} = \\mathbf{z}_{x} - \\bar{z}_{x}\\mathbf{1}, \\quad \\text{where} \\quad \\bar{z}_{x} = \\frac{1}{n}\\sum_{i=1}^{n} z_{x,i} $$\n$$ \\tilde{\\mathbf{z}}_{y} = \\mathbf{z}_{y} - \\bar{z}_{y}\\mathbf{1}, \\quad \\text{where} \\quad \\bar{z}_{y} = \\frac{1}{n}\\sum_{i=1}^{n} z_{y,i} $$\nHere, $\\mathbf{1}$ is the vector of ones. The phylogenetically corrected association score, $r$, is the Pearson correlation of $\\mathbf{z}_{x}$ and $\\mathbf{z}_{y}$:\n$$ r = \\frac{\\tilde{\\mathbf{z}}_{x}^{\\top}\\tilde{\\mathbf{z}}_{y}}{\\|\\tilde{\\mathbf{z}}_{x}\\|_{2}\\,\\|\\tilde{\\mathbf{z}}_{y}\\|_{2}} $$\nIf the norm of either centered whitened vector is zero (i.e., $\\|\\tilde{\\mathbf{z}}_{x}\\|_{2} = 0$ or $\\|\\tilde{\\mathbf{z}}_{y}\\|_{2} = 0$), which occurs if all components of the original whitened vector are identical, the correlation is undefined. In this specific scenario, the problem states to define the score as $r = 0$.\n\n**3. Bootstrap Confidence Interval**\n\nThe covariance matrix $\\mathbf{C}$ is derived from an estimated phylogeny, which itself is subject to uncertainty. To propagate this uncertainty into our association score, we use a set of $K$ bootstrap replicate phylogenies, which yields a set of $K$ covariance matrices $\\{\\mathbf{C}^{(k)}\\}_{k=1}^{K}$.\n\nFor each matrix $\\mathbf{C}^{(k)}$, we compute the association score $r^{(k)}$ as described above. This gives us a sample distribution $\\{r^{(k)}\\}_{k=1}^{K}$ of the association score, which reflects the uncertainty in the phylogeny.\n\nA two-sided confidence interval at level $1-\\alpha$ can be constructed by finding the empirical quantiles of this bootstrap distribution. The lower limit is the quantile $q_{\\mathrm{low}} = \\alpha/2$ and the upper limit is the quantile $q_{\\mathrm{high}} = 1 - \\alpha/2$. The problem specifies the type-7 quantile definition, which involves linear interpolation.\n\nLet the sorted scores be $v_{(1)} \\le v_{(2)} \\le \\cdots \\le v_{(K)}$. For a desired quantile $q \\in [0,1]$, we calculate a 1-based index $h = (K - 1)q + 1$. We find its integer part $k = \\lfloor h \\rfloor$ and fractional part $\\gamma = h - k$. The quantile value $Q(q)$ is then interpolated as:\n$$ Q(q) = v_{(k)} + \\gamma(v_{(k+1)} - v_{(k)}) $$\nwith the edge case $Q(q) = v_{(k)}$ if $\\gamma = 0$. This method is a standard approach for estimating quantiles from a finite sample. For the given problem, $\\alpha = 0.05$, so we compute the quantiles for $q=0.025$ and $q=0.975$.\n\nThis completes the formal specification of the algorithm. We will now proceed with its implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\nfrom scipy.linalg.lapack import dpotrf\n\ndef solve():\n    \"\"\"\n    Computes phylogenetically corrected association scores and confidence intervals\n    for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"x\": np.array([1.0, 1.0, 0.0, 0.0]),\n            \"y\": np.array([1.0, 1.0, 0.0, 0.0]),\n            \"C_list\": [\n                np.array([\n                    [1.0, 0.8, 0.2, 0.2],\n                    [0.8, 1.0, 0.2, 0.2],\n                    [0.2, 0.2, 1.0, 0.7],\n                    [0.2, 0.2, 0.7, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.75, 0.25, 0.25],\n                    [0.75, 1.0, 0.25, 0.25],\n                    [0.25, 0.25, 1.0, 0.65],\n                    [0.25, 0.25, 0.65, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.85, 0.15, 0.15],\n                    [0.85, 1.0, 0.15, 0.15],\n                    [0.15, 0.15, 1.0, 0.6],\n                    [0.15, 0.15, 0.6, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.7, 0.3, 0.2],\n                    [0.7, 1.0, 0.2, 0.3],\n                    [0.3, 0.2, 1.0, 0.55],\n                    [0.2, 0.3, 0.55, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.78, 0.22, 0.22],\n                    [0.78, 1.0, 0.22, 0.22],\n                    [0.22, 0.22, 1.0, 0.68],\n                    [0.22, 0.22, 0.68, 1.0]\n                ])\n            ]\n        },\n        {\n            \"x\": np.array([0.0, 1.0, 0.0, 1.0]),\n            \"y\": np.array([0.0, 1.0, 0.0, 1.0]),\n            \"C_list\": [\n                np.array([\n                    [1.0, 0.4, 0.4, 0.4], [0.4, 1.0, 0.4, 0.4],\n                    [0.4, 0.4, 1.0, 0.4], [0.4, 0.4, 0.4, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.3, 0.3, 0.3], [0.3, 1.0, 0.3, 0.3],\n                    [0.3, 0.3, 1.0, 0.3], [0.3, 0.3, 0.3, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.2, 0.2, 0.2], [0.2, 1.0, 0.2, 0.2],\n                    [0.2, 0.2, 1.0, 0.2], [0.2, 0.2, 0.2, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.5, 0.5, 0.5], [0.5, 1.0, 0.5, 0.5],\n                    [0.5, 0.5, 1.0, 0.5], [0.5, 0.5, 0.5, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.35, 0.35, 0.35], [0.35, 1.0, 0.35, 0.35],\n                    [0.35, 0.35, 1.0, 0.35], [0.35, 0.35, 0.35, 1.0]\n                ])\n            ]\n        },\n        {\n            \"x\": np.array([1.0, 1.0, 0.0, 0.0]),\n            \"y\": np.array([1.0, 0.0, 1.0, 0.0]),\n            \"C_list\": [\n                np.array([\n                    [1.0, 0.8, 0.2, 0.2], [0.8, 1.0, 0.2, 0.2],\n                    [0.2, 0.2, 1.0, 0.7], [0.2, 0.2, 0.7, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.2, 0.8, 0.2], [0.2, 1.0, 0.2, 0.75],\n                    [0.8, 0.2, 1.0, 0.2], [0.2, 0.75, 0.2, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.2, 0.2, 0.8], [0.2, 1.0, 0.75, 0.2],\n                    [0.2, 0.75, 1.0, 0.2], [0.8, 0.2, 0.2, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.6, 0.4, 0.3], [0.6, 1.0, 0.3, 0.4],\n                    [0.4, 0.3, 1.0, 0.6], [0.3, 0.4, 0.6, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.3, 0.3, 0.3], [0.3, 1.0, 0.3, 0.3],\n                    [0.3, 0.3, 1.0, 0.3], [0.3, 0.3, 0.3, 1.0]\n                ])\n            ]\n        }\n    ]\n\n    alpha = 0.05\n    q_low = alpha / 2.0\n    q_high = 1.0 - alpha / 2.0\n    epsilon = 1e-10\n\n    results = []\n    \n    for case in test_cases:\n        x = case[\"x\"]\n        y = case[\"y\"]\n        C_list = case[\"C_list\"]\n        n = x.shape[0]\n\n        r_scores = []\n        for C_orig in C_list:\n            C = C_orig.copy()\n            # Perform Cholesky factorization, adding jitter if not positive definite\n            try:\n                # Use raw LAPACK call to check for non-positive-definiteness\n                # dpotrf returns a positive info value if not positive definite\n                _, info = dpotrf(C, lower=True, overwrite_a=False, clean=True)\n                if info > 0:\n                    raise np.linalg.LinAlgError(\"Matrix is not positive definite\")\n                L = cholesky(C, lower=True)\n            except np.linalg.LinAlgError:\n                C_jittered = C + np.eye(n) * epsilon\n                L = cholesky(C_jittered, lower=True)\n\n            # Whiten the vectors by solving L * z = x (and y)\n            zx = solve_triangular(L, x, lower=True)\n            zy = solve_triangular(L, y, lower=True)\n\n            # Center the whitened vectors\n            zx_tilde = zx - np.mean(zx)\n            zy_tilde = zy - np.mean(zy)\n\n            # Compute norms\n            norm_zx = np.linalg.norm(zx_tilde)\n            norm_zy = np.linalg.norm(zy_tilde)\n\n            # Compute the phylogenetically corrected association score (Pearson correlation)\n            if norm_zx == 0 or norm_zy == 0:\n                r = 0.0\n            else:\n                numerator = np.dot(zx_tilde, zy_tilde)\n                denominator = norm_zx * norm_zy\n                r = numerator / denominator\n            \n            r_scores.append(r)\n\n        # Compute the confidence interval using type-7 quantiles\n        # np.quantile with interpolation='linear' implements type-7\n        lower_bound = np.quantile(r_scores, q_low, interpolation='linear')\n        upper_bound = np.quantile(r_scores, q_high, interpolation='linear')\n        \n        results.append([round(lower_bound, 6), round(upper_bound, 6)])\n\n    # Format the final output string\n    output_str = \"[[\" + \"],[\".join([\",\".join([f\"{v:.6f}\" for v in r]) for r in results]) + \"]]\"\n    print(output_str)\n\nsolve()\n```", "id": "4375055"}]}