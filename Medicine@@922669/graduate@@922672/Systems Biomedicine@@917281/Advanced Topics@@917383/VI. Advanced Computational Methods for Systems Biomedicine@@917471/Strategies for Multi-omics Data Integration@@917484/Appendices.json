{"hands_on_practices": [{"introduction": "Real-world multi-omics datasets are rarely complete, and how we handle missing values can drastically affect our conclusions. This first practice confronts the pervasive issue of missing data, contrasting the simple but often detrimental method of listwise deletion with the principles behind modern latent variable models. By working through a scenario with different missingness rates [@problem_id:4389260], you will quantify the loss of statistical power and understand why model-based imputation can more effectively recover the underlying biological signal.", "problem": "You are integrating two matched omics layers across a cohort of $n$ individuals: a transcriptome block $X$ and a proteome block $Y$. The proteome block has a marginal missingness rate of $0.30$ per individual (that is, the probability that a given individual’s proteome measurement is missing is $0.30$), while the transcriptome block has a marginal missingness rate of $0.05$ per individual. Assume Missing Completely at Random (MCAR), independence of missingness across blocks and across individuals, and that the decision to include an individual under listwise deletion requires both $X$ and $Y$ to be observed for that individual.\n\na) Starting from the product rule for independent events in elementary probability and the definition of MCAR, derive the expected fraction of individuals that are retained under listwise deletion (that is, the expected fraction of complete cases across both blocks), and compute its numerical value. Report your final numeric answer as a decimal and round to three significant figures.\n\nb) Now consider estimating the cross-omics covariance between $X$ and $Y$. Using a linear shared latent variable model in which both $X$ and $Y$ are generated from a common low-dimensional latent factor $f$ with independent residuals, argue from first principles why a blockwise latent imputation strategy (for example, Multi-Omics Factor Analysis (MOFA)-style estimation) can recover the cross-omics covariance more effectively than listwise deletion. Your argument should explicitly connect (i) the reduction in effective sample size under listwise deletion and its impact on estimator variance, and (ii) the covariance decomposition implied by the latent factor model that enables consistent recovery of cross-omics covariance from partially observed data.", "solution": "The problem requires a two-part analysis concerning the integration of two omics data blocks, $X$ (transcriptome) and $Y$ (proteome), with differing rates of missingness. The first part is a probabilistic calculation, and the second is a conceptual argument based on statistical modeling principles.\n\nFirst, we address the derivation of the expected fraction of individuals retained under listwise deletion. Let $O_{X,i}$ be the event that the data for the transcriptome block $X$ are observed for individual $i$, and let $O_{Y,i}$ be the event that the data for the proteome block $Y$ are observed for individual $i$. The problem provides the marginal probabilities of missingness for each block.\nThe probability of the transcriptome block being missing is given as $P(M_{X,i}) = 0.05$.\nThe probability of the proteome block being missing is given as $P(M_{Y,i}) = 0.30$.\n\nThe events of observing the data are the complements of the missingness events. Therefore, their probabilities are:\n$$P(O_{X,i}) = 1 - P(M_{X,i}) = 1 - 0.05 = 0.95$$\n$$P(O_{Y,i}) = 1 - P(M_{Y,i}) = 1 - 0.30 = 0.70$$\n\nThe problem states that an individual is retained under listwise deletion only if both data blocks $X$ and $Y$ are observed. This corresponds to the joint event $O_{X,i} \\cap O_{Y,i}$. The problem also specifies that missingness is independent across blocks. This means the events $O_{X,i}$ and $O_{Y,i}$ are statistically independent.\n\nAccording to the product rule for independent events, the probability of the joint event is the product of the individual event probabilities:\n$$P(O_{X,i} \\cap O_{Y,i}) = P(O_{X,i}) \\times P(O_{Y,i})$$\nSubstituting the calculated probabilities:\n$$P(\\text{individual } i \\text{ is retained}) = 0.95 \\times 0.70 = 0.665$$\n\nThis probability represents the probability that any single, randomly chosen individual from the cohort is a \"complete case\" and is retained. The problem assumes that missingness is independent across individuals. Let $R_i$ be a Bernoulli random variable such that $R_i=1$ if individual $i$ is retained and $R_i=0$ otherwise. The probability of success is $p = P(R_i=1) = 0.665$. The total number of retained individuals in a cohort of size $n$ is $N_{retained} = \\sum_{i=1}^n R_i$. The expected number of retained individuals is $\\mathbb{E}[N_{retained}] = \\sum_{i=1}^n \\mathbb{E}[R_i] = \\sum_{i=1}^n p = np$. The expected fraction of retained individuals is therefore $\\frac{\\mathbb{E}[N_{retained}]}{n} = \\frac{np}{n} = p$.\n\nThus, the expected fraction of individuals retained under listwise deletion is $0.665$. This value is already stated with three significant figures.\n\nNext, we argue from first principles why a blockwise latent imputation strategy is more effective than listwise deletion for estimating the cross-omics covariance, $\\text{Cov}(X, Y)$. The argument is built upon two pillars: the impact of sample size reduction on estimator variance and the structural advantages of the latent variable model.\n\nA linear shared latent variable model posits that the high-dimensional data in blocks $X$ and $Y$ are generated from a common low-dimensional latent factor $f$. For a single individual, the model is specified as:\n$$x = W_X f + \\epsilon_X$$\n$$y = W_Y f + \\epsilon_Y$$\nwhere $x$ and $y$ are data vectors for the individual, $f$ is the latent factor vector, $W_X$ and $W_Y$ are loading matrices that map the latent space to the data spaces, and $\\epsilon_X$ and $\\epsilon_Y$ are independent residual noise terms. A key assumption is that the latent factors $f$ capture all the shared variation between $X$ and $Y$, meaning $\\epsilon_X$ and $\\epsilon_Y$ are independent of each other and of $f$.\n\n(i) Impact of sample size reduction on estimator variance:\nAs calculated above, listwise deletion retains only an expected fraction of $0.665$ of the total individuals. This means that for a cohort of size $n$, the effective sample size for any downstream analysis, including the estimation of the cross-omics covariance matrix, is reduced to approximately $n_{eff} \\approx 0.665n$. The variance of most statistical estimators, including the sample covariance, is inversely proportional to the sample size. For an estimator $\\hat{\\theta}$ of a parameter $\\theta$ based on a sample of size $N$, its variance is typically of the form $\\text{Var}(\\hat{\\theta}) \\propto \\frac{1}{N}$. Consequently, an estimate of $\\text{Cov}(X, Y)$ based on the $n_{eff}$ complete cases will have a substantially higher variance than an estimate that could leverage information from all $n$ individuals. This increased variance means the estimate is less reliable, more susceptible to sampling noise, and yields wider confidence intervals. The statistical power to detect true cross-omics associations is significantly diminished.\n\n(ii) Covariance decomposition and recovery from partially observed data:\nThe latent variable model provides a mechanism to overcome the data loss issue. Under this model, the cross-covariance between the data blocks $X$ and $Y$ can be decomposed. Assuming the data are centered, the covariance is:\n$$\\text{Cov}(X, Y) = \\mathbb{E}[xy^T] = \\mathbb{E}[(W_X f + \\epsilon_X)(W_Y f + \\epsilon_Y)^T]$$\nExpanding and using the independence of $f$, $\\epsilon_X$, and $\\epsilon_Y$, the cross-terms involving residuals have zero expectation:\n$$\\text{Cov}(X, Y) = \\mathbb{E}[W_X f f^T W_Y^T] = W_X \\mathbb{E}[f f^T] W_Y^T$$\nIf we assume the latent factors are standardized, $\\mathbb{E}[f f^T] = I$ (the identity matrix), the cross-covariance simplifies to:\n$$\\text{Cov}(X, Y) = W_X W_Y^T$$\nThis decomposition is crucial. It shows that the entire cross-omics covariance structure is determined by the loading matrices $W_X$ and $W_Y$. A blockwise latent imputation strategy, such as that used in Multi-Omics Factor Analysis (MOFA), estimates these loading matrices. The estimation process (e.g., maximizing the data likelihood) uses all available data, not just complete cases.\n- For an individual with only block $X$ observed, the data contributes to the estimation of $W_X$ and that individual's latent factor $f$.\n- For an individual with only block $Y$ observed, the data contributes to the estimation of $W_Y$ and that individual's latent factor $f$.\n- For an individual with both blocks observed, the data informs the estimation of both $W_X$ and $W_Y$ and their shared latent factor $f$.\n\nBy pooling information from all $n$ individuals, the model \"borrows strength\" across the dataset. The loading matrix $W_X$ is estimated using data from all individuals for whom $X$ is present (an expected $0.95n$ individuals), and $W_Y$ is estimated using data from all individuals for whom $Y$ is present (an expected $0.70n$ individuals). These sample sizes are significantly larger than the $0.665n$ available to listwise deletion. Once robust estimates of the loading matrices, $\\hat{W}_X$ and $\\hat{W}_Y$, are obtained, the cross-omics covariance can be consistently reconstructed as $\\widehat{\\text{Cov}}(X, Y) = \\hat{W}_X \\hat{W}_Y^T$. This reconstructed covariance is based on parameters estimated from a much larger effective sample size, resulting in a lower-variance, more stable, and more accurate estimate than one obtained from the severely reduced complete-case dataset.", "answer": "$$\n\\boxed{0.665}\n$$", "id": "4389260"}, {"introduction": "Many powerful integration methods assume that data from different omics layers follow comparable, often Gaussian, distributions. This exercise explores the critical pre-processing step of data transformation to meet these assumptions, focusing on highly skewed data typical of metabolomics. You will compare the standard log-transform with the more flexible Box-Cox family of transformations, developing an intuition for when and why a particular approach is scientifically justified [@problem_id:4389267].", "problem": "In multi-omics integration workflows (for example, Canonical Correlation Analysis and Multi-Omics Factor Analysis), it is common to harmonize marginal distributions across layers so that they approach Gaussianity with stabilized variance. Consider two layers: metabolomics peak intensities and transcriptomics normalized expression. For each layer, assume feature-wise marginal distributions are strictly positive and approximately log-normal, denoted by $X \\sim \\mathrm{LogNormal}(\\mu, \\sigma^{2})$, so that $\\ln X$ is Gaussian. You are given the following empirical sample skewness (third standardized moment) estimates for the untransformed features:\n- Metabolomics: $\\gamma_{1,\\mathrm{meta}} = 2.5$.\n- Transcriptomics: $\\gamma_{1,\\mathrm{rna}} = 0.3$.\n\nUse the following foundational definitions and facts:\n- Skewness of a random variable $X$ with finite moments is $\\gamma_{1}(X) = \\mathbb{E}\\!\\left[\\left(\\frac{X - \\mathbb{E}[X]}{\\sqrt{\\mathrm{Var}(X)}}\\right)^{3}\\right]$.\n- If $X \\sim \\mathrm{LogNormal}(\\mu, \\sigma^{2})$, then $X = \\exp(Z)$ where $Z \\sim \\mathcal{N}(\\mu, \\sigma^{2})$, and the theoretical skewness of $X$ is $\\gamma_{1}(X) = \\left(\\exp(\\sigma^{2}) + 2\\right)\\sqrt{\\exp(\\sigma^{2}) - 1}$.\n- The Box–Cox family is defined by $T_{\\lambda}(x) = \\frac{x^{\\lambda} - 1}{\\lambda}$ for $\\lambda \\neq 0$ and $T_{0}(x) = \\ln x$.\n\nTasks:\n1) Under the log-normal approximation, apply the log transformation $Y = \\ln X$ and determine the skewness $\\gamma_{1}(Y)$.\n2) Define the transformation impact on skewness as $\\Delta = \\gamma_{1}(\\ln X) - \\gamma_{1}(X)$. Compute $\\Delta_{\\mathrm{meta}}$ and $\\Delta_{\\mathrm{rna}}$ for the metabolomics and transcriptomics layers using the reported empirical skewness values as proxies for $\\gamma_{1}(X)$ under the log-normal approximation.\n3) Briefly argue, based on first principles and your results, whether a Box–Cox transformation with $\\lambda \\neq 0$ is preferable for the metabolomics layer, given its reported skewness and the stated approximation.\n\nReport the final numeric answer for part (2) as a row vector $\\begin{pmatrix}\\Delta_{\\mathrm{meta}} & \\Delta_{\\mathrm{rna}}\\end{pmatrix}$. The skewness values are dimensionless; no units are required. If a nonzero approximation is needed, round to four significant figures; otherwise, give exact values.", "solution": "This problem is solved by addressing each of the three tasks in order.\n\n**1) Skewness of the log-transformed variable**\n\nLet $X$ be a random variable following a log-normal distribution, $X \\sim \\mathrm{LogNormal}(\\mu, \\sigma^{2})$. By the definition provided in the problem statement, this implies that the natural logarithm of $X$ follows a normal (Gaussian) distribution. Let $Y = \\ln X$. Then, $Y \\sim \\mathcal{N}(\\mu, \\sigma^{2})$. The skewness of a random variable measures its asymmetry. The normal distribution $\\mathcal{N}(\\mu, \\sigma^{2})$ is perfectly symmetric about its mean, $\\mu$. All odd-order central moments of a symmetric distribution are zero, and skewness is the third standardized central moment. The skewness of $Y$, denoted $\\gamma_{1}(Y)$, is therefore exactly zero.\n$$\n\\gamma_{1}(Y) = \\gamma_{1}(\\ln X) = 0\n$$\n\n**2) Computation of the transformation impact, $\\Delta$**\n\nThe transformation impact on skewness is defined as $\\Delta = \\gamma_{1}(\\ln X) - \\gamma_{1}(X)$. From the result of the first task, we know that $\\gamma_{1}(\\ln X) = 0$. Substituting this into the definition of $\\Delta$ yields:\n$$\n\\Delta = 0 - \\gamma_{1}(X) = -\\gamma_{1}(X)\n$$\nThe problem states that we should use the given empirical sample skewness values as proxies for the theoretical population skewness, $\\gamma_{1}(X)$, for each layer.\n\nFor the metabolomics layer, the empirical skewness is $\\gamma_{1,\\mathrm{meta}} = 2.5$. The transformation impact is:\n$$\n\\Delta_{\\mathrm{meta}} = -\\gamma_{1,\\mathrm{meta}} = -2.5\n$$\n\nFor the transcriptomics layer, the empirical skewness is $\\gamma_{1,\\mathrm{rna}} = 0.3$. The transformation impact is:\n$$\n\\Delta_{\\mathrm{rna}} = -\\gamma_{1,\\mathrm{rna}} = -0.3\n$$\n\nThe final numeric answer is the row vector $\\begin{pmatrix}\\Delta_{\\mathrm{meta}} & \\Delta_{\\mathrm{rna}}\\end{pmatrix} = \\begin{pmatrix}-2.5 & -0.3\\end{pmatrix}$.\n\n**3) Argument for Box-Cox transformation for the metabolomics layer**\n\nThe primary goal of the transformation is to make the marginal distributions approach Gaussianity, a key feature of which is zero skewness. The metabolomics layer has a high initial skewness of $\\gamma_{1,\\mathrm{meta}} = 2.5$. The problem states the data is *approximately* log-normal.\n\n- If the data were *exactly* log-normal, the log transformation ($T_{0}(x) = \\ln x$) would be theoretically perfect, transforming the data to an exactly normal distribution with zero skewness.\n\n- However, the term \"approximately log-normal\" implies the true underlying distribution may deviate from this ideal model. Real-world data rarely conforms perfectly to a theoretical distribution.\n\n- The Box-Cox transformation family, $T_{\\lambda}(x)$, is more flexible. It includes the log transformation as a special case when $\\lambda \\to 0$. By estimating $\\lambda$ from the data (e.g., via maximum likelihood), one can find a transformation that may achieve better normality than the fixed log transform.\n\n- Given the high skewness of the metabolomics data, it is likely the log-normal assumption is only a rough approximation. A rigid transformation like $\\ln(x)$ might either under-correct (leaving residual positive skew) or over-correct (inducing negative skew). An optimized Box-Cox transformation with a data-driven $\\lambda$ (which may be close to, but not exactly, zero) can fine-tune the power of the transformation to more effectively reduce the skewness toward zero.\n\nTherefore, because the log-normal model is an approximation, the more flexible Box-Cox transformation is preferable. It provides a data-driven correction that is more robust to deviations from the idealized model, making it more likely to achieve the desired Gaussianity for the highly skewed metabolomics data.", "answer": "$$\n\\boxed{\\begin{pmatrix}-2.5 & -0.3\\end{pmatrix}}\n$$", "id": "4389267"}, {"introduction": "At the heart of many integration strategies is the goal of identifying shared patterns of variation across different molecular layers. This practice demystifies the popular Partial Least Squares (PLS) method by guiding you through a concrete numerical example. By manually computing the first PLS components [@problem_id:4389264], you will gain a first-principles understanding of how the algorithm finds projections of the data that maximize cross-omics covariance, revealing the mathematical machinery behind this powerful technique.", "problem": "Consider a matched multi-omics study with $n=3$ biological samples profiled across two transcript features (messenger ribonucleic acid expression) and two metabolite features (mass-spectrometry intensities). Let $X \\in \\mathbb{R}^{3 \\times 2}$ denote the transcript block and $Y \\in \\mathbb{R}^{3 \\times 2}$ denote the metabolite block. Both blocks are mean-centered across samples (each column has sample mean zero). The matrices are\n$$\nX = \\begin{pmatrix}\n1 & -1 \\\\\n0 & 1 \\\\\n-1 & 0\n\\end{pmatrix},\n\\qquad\nY = \\begin{pmatrix}\n2 & 0 \\\\\n-1 & 1 \\\\\n-1 & -1\n\\end{pmatrix}.\n$$\nIn two-block Partial Least Squares (PLS), the first latent variables are defined as $t_{1} = X w_{1}$ and $u_{1} = Y c_{1}$, where $w_{1} \\in \\mathbb{R}^{2}$ and $c_{1} \\in \\mathbb{R}^{2}$ are weight vectors. The objective of the first component is to maximize the sample covariance between $t_{1}$ and $u_{1}$ subject to unit Euclidean norm constraints on the weights:\n$$\n\\text{maximize } \\operatorname{cov}(t_{1}, u_{1}) = \\frac{1}{n-1} t_{1}^{\\top} u_{1} = \\frac{1}{n-1} w_{1}^{\\top} X^{\\top} Y c_{1}\n\\quad \\text{subject to} \\quad \\|w_{1}\\|_{2} = 1, \\ \\|c_{1}\\|_{2} = 1.\n$$\nDefine the cross-covariance matrix $S = \\frac{1}{n-1} X^{\\top} Y \\in \\mathbb{R}^{2 \\times 2}$. The second PLS latent variables $t_{2} = X w_{2}$ and $u_{2} = Y c_{2}$ are defined analogously, but must satisfy the orthogonality constraints $w_{1}^{\\top} w_{2} = 0$ and $c_{1}^{\\top} c_{2} = 0$, in addition to the same unit norm constraints, and they maximize $\\operatorname{cov}(t_{2}, u_{2})$.\n\nUsing only the definitions of sample covariance, Euclidean norm, orthogonality in $\\mathbb{R}^{2}$, and the well-tested characterization that for any fixed real matrix $S$, the extrema of the bilinear form $w^{\\top} S c$ under unit norm constraints are determined by the singular values and singular vectors of $S$, do the following:\n\n1. Compute $S = \\frac{1}{n-1} X^{\\top} Y$ explicitly.\n2. Derive $w_{1}$ and $c_{1}$ that maximize $\\operatorname{cov}(t_{1}, u_{1})$ subject to $\\|w_{1}\\|_{2} = 1$ and $\\|c_{1}\\|_{2} = 1$, and compute the maximized covariance value $\\operatorname{cov}(t_{1}, u_{1})$.\n3. Under the constraints $w_{1}^{\\top} w_{2} = 0$ and $c_{1}^{\\top} c_{2} = 0$ with $\\|w_{2}\\|_{2} = 1$ and $\\|c_{2}\\|_{2} = 1$, derive $w_{2}$ and $c_{2}$ that maximize $\\operatorname{cov}(t_{2}, u_{2})$, and compute the maximized covariance value $\\operatorname{cov}(t_{2}, u_{2})$.\n4. Verify via first principles (e.g., Lagrange multipliers or the singular value decomposition characterization of bilinear forms) that each pair indeed achieves the stated maxima under the given constraints.\n\nReport as your final answer a single closed-form analytic expression equal to the sum of the two maximized covariances, $\\operatorname{cov}(t_{1}, u_{1}) + \\operatorname{cov}(t_{2}, u_{2})$. Treat all quantities as dimensionless due to standardization, and express the final answer in exact form (no rounding).", "solution": "We begin with the definitions. The sample covariance between latent variables $t = X w$ and $u = Y c$ is\n$$\n\\operatorname{cov}(t,u) = \\frac{1}{n-1} t^{\\top} u = \\frac{1}{n-1} w^{\\top} X^{\\top} Y c.\n$$\nFor $n=3$, $1/(n-1) = 1/2$. Define the cross-covariance matrix\n$$\nS = \\frac{1}{n-1} X^{\\top} Y = \\frac{1}{2} X^{\\top} Y.\n$$\nStep 1: Compute $S$ explicitly. First compute $X^{\\top} Y$:\n$$\nX^{\\top} Y =\n\\begin{pmatrix}\n1 & 0 & -1 \\\\\n-1 & 1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n2 & 0 \\\\\n-1 & 1 \\\\\n-1 & -1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 & 1 \\\\\n-3 & 1\n\\end{pmatrix}.\n$$\nTherefore,\n$$\nS = \\frac{1}{2} \\begin{pmatrix} 3 & 1 \\\\ -3 & 1 \\end{pmatrix}\n= \\begin{pmatrix} 1.5 & 0.5 \\\\ -1.5 & 0.5 \\end{pmatrix}.\n$$\n\nStep 2: Maximize $\\operatorname{cov}(t_{1}, u_{1}) = w_{1}^{\\top} S c_{1}$ subject to $\\|w_{1}\\|_{2} = 1$ and $\\|c_{1}\\|_{2} = 1$. A well-tested fact from matrix analysis is that for any real matrix $S$, the maximum of $w^{\\top} S c$ over unit vectors $w$ and $c$ is the largest singular value $\\sigma_{1}$ of $S$, achieved when $c$ is the right singular vector associated with $\\sigma_{1}$ and $w$ is the corresponding left singular vector. To compute singular values and vectors, examine $S^{\\top} S$:\n$$\nS^{\\top} S = \n\\begin{pmatrix}\n1.5 & -1.5 \\\\\n0.5 & 0.5\n\\end{pmatrix}\n\\begin{pmatrix}\n1.5 & 0.5 \\\\\n-1.5 & 0.5\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n4.5 & 0 \\\\\n0 & 0.5\n\\end{pmatrix}.\n$$\nThus the eigenvalues of $S^{\\top} S$ are $\\lambda_{1} = 4.5$ and $\\lambda_{2} = 0.5$, so the singular values are\n$$\n\\sigma_{1} = \\sqrt{4.5} = \\frac{3}{\\sqrt{2}}, \\qquad \\sigma_{2} = \\sqrt{0.5} = \\frac{1}{\\sqrt{2}}.\n$$\nSince $S^{\\top} S$ is diagonal, the normalized eigenvectors (right singular vectors) are $c_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $c_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. The corresponding left singular vectors are given by $w_{i} = \\frac{S c_{i}}{\\|S c_{i}\\|_{2}}$. Compute $w_{1}$:\n$$\nS c_{1} = \\begin{pmatrix} 1.5 \\\\ -1.5 \\end{pmatrix}, \\quad \\|S c_{1}\\|_{2} = \\sqrt{(1.5)^{2} + (-1.5)^{2}} = \\sqrt{4.5} = \\frac{3}{\\sqrt{2}}.\n$$\nTherefore,\n$$\nw_{1} = \\frac{1}{\\sqrt{4.5}} \\begin{pmatrix} 1.5 \\\\ -1.5 \\end{pmatrix}\n= \\begin{pmatrix} \\frac{1.5}{\\sqrt{4.5}} \\\\ \\frac{-1.5}{\\sqrt{4.5}} \\end{pmatrix}\n= \\begin{pmatrix} \\frac{\\sqrt{2}}{2} \\\\ -\\frac{\\sqrt{2}}{2} \\end{pmatrix}\n= \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix}.\n$$\nBy construction, $\\|w_{1}\\|_{2} = 1$ and $\\|c_{1}\\|_{2} = 1$, and\n$$\n\\operatorname{cov}(t_{1}, u_{1}) = w_{1}^{\\top} S c_{1} = \\|S c_{1}\\|_{2} = \\sigma_{1} = \\frac{3}{\\sqrt{2}}.\n$$\n\nStep 3: Impose $w_{1}^{\\top} w_{2} = 0$ and $c_{1}^{\\top} c_{2} = 0$, along with unit norms, and maximize $w_{2}^{\\top} S c_{2}$. From the orthogonality constraint $c_{1}^{\\top} c_{2} = 0$ and the structure of the right singular vectors, take $c_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. Then\n$$\nS c_{2} = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix}, \\quad \\|S c_{2}\\|_{2} = \\sqrt{(0.5)^{2} + (0.5)^{2}} = \\sqrt{0.5} = \\frac{1}{\\sqrt{2}},\n$$\nso the unit-norm left singular vector is\n$$\nw_{2} = \\frac{S c_{2}}{\\|S c_{2}\\|_{2}} = \\begin{pmatrix} \\frac{0.5}{\\sqrt{0.5}} \\\\ \\frac{0.5}{\\sqrt{0.5}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}.\n$$\nNote that $w_{1}^{\\top} w_{2} = \\frac{1}{\\sqrt{2}} \\cdot \\frac{1}{\\sqrt{2}} + \\left(-\\frac{1}{\\sqrt{2}}\\right) \\cdot \\frac{1}{\\sqrt{2}} = \\frac{1}{2} - \\frac{1}{2} = 0$, so orthogonality holds. The maximized covariance for the second component is\n$$\n\\operatorname{cov}(t_{2}, u_{2}) = w_{2}^{\\top} S c_{2} = \\|S c_{2}\\|_{2} = \\sigma_{2} = \\frac{1}{\\sqrt{2}}.\n$$\n\nStep 4: Verification of optimality. For any unit vectors $w$ and $c$, the bilinear form satisfies\n$$\n|w^{\\top} S c| \\leq \\sigma_{1},\n$$\nwhere $\\sigma_{1}$ is the largest singular value of $S$. This follows from the singular value decomposition characterization: writing $S = U \\Sigma V^{\\top}$ with $U, V$ orthogonal and $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2})$, set $\\tilde{w} = U^{\\top} w$ and $\\tilde{c} = V^{\\top} c$, both unit vectors, so\n$$\nw^{\\top} S c = \\tilde{w}^{\\top} \\Sigma \\tilde{c} = \\sigma_{1} \\tilde{w}_{1} \\tilde{c}_{1} + \\sigma_{2} \\tilde{w}_{2} \\tilde{c}_{2}.\n$$\nBy the Cauchy–Schwarz inequality and $|\\tilde{w}_{i}| \\leq 1$, $|\\tilde{c}_{i}| \\leq 1$, the maximum is $\\sigma_{1}$, achieved when $\\tilde{w} = e_{1}$ and $\\tilde{c} = e_{1}$, equivalently when $w$ and $c$ are the left and right singular vectors associated with $\\sigma_{1}$, as constructed above. Under the additional orthogonality constraints $c_{1}^{\\top} c_{2} = 0$ and $w_{1}^{\\top} w_{2} = 0$, the maximization over unit vectors constrained to the orthogonal complement of the first singular directions achieves the next singular value $\\sigma_{2}$, attained by the corresponding singular vectors, which we have explicitly computed. Therefore, the computed pairs $(w_{1}, c_{1})$ and $(w_{2}, c_{2})$ attain the global maxima of the constrained problems.\n\nFinally, the requested sum of the two maximized covariances is\n$$\n\\operatorname{cov}(t_{1}, u_{1}) + \\operatorname{cov}(t_{2}, u_{2}) = \\frac{3}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} = \\frac{4}{\\sqrt{2}} = 2 \\sqrt{2}.\n$$", "answer": "$$\\boxed{2\\sqrt{2}}$$", "id": "4389264"}]}