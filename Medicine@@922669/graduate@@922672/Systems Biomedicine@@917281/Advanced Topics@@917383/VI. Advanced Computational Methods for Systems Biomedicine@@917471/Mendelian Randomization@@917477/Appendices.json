{"hands_on_practices": [{"introduction": "The simplest application of Mendelian randomization involves using a single genetic variant as an instrumental variable. This exercise focuses on calculating the Wald ratio, which is the fundamental estimator for the causal effect in a two-sample MR setting. By working through the calculation of the effect estimate and its standard error using the delta method, you will gain a foundational understanding of how MR leverages genetic associations to infer causality [@problem_id:4611689].", "problem": "A single genetic variant is proposed as an instrumental variable for a continuous exposure in a two-sample Mendelian randomization (MR) analysis. Assume the standard instrumental variable conditions hold: relevance (the genetic variant affects the exposure), independence (the genetic variant is independent of confounders of the exposure-outcome relationship), and exclusion restriction (the genetic variant affects the outcome only through the exposure). In a large sample from Study A, a linear regression of the exposure on the genetic variant yields a summary association estimate $\\hat{\\beta}_{XG}$ with standard error $SE(\\hat{\\beta}_{XG})$. In an independent large sample from Study B, a logistic regression of the binary outcome on the same genetic variant yields a log-odds association estimate $\\hat{\\beta}_{YG}$ with standard error $SE(\\hat{\\beta}_{YG})$. Treat the two samples as independent and interpret the effect of the genetic variant on the outcome on the log-odds scale.\n\nYou are given the following summary statistics:\n- Study A: $\\hat{\\beta}_{XG} = 0.08$ and $SE(\\hat{\\beta}_{XG}) = 0.01$.\n- Study B: $\\hat{\\beta}_{YG} = 0.024$ and $SE(\\hat{\\beta}_{YG}) = 0.006$.\n\nUsing the core instrumental variable identity that under these assumptions the causal effect equals the ratio of the genetic association with the outcome to the genetic association with the exposure, construct the ratio estimator for the causal log-odds effect of the exposure on the outcome. Then, using a first-order Taylor series (delta method) approximation for a smooth function of independent estimates, derive the approximate standard error of the ratio estimator. Finally, compute the Wald test statistic $z = \\hat{\\beta}_{W} / SE(\\hat{\\beta}_{W})$ for testing the null hypothesis that the causal effect is zero.\n\nReport only the value of $z$, rounded to four significant figures. Do not include any units in your final answer.", "solution": "The user wants to compute the Wald test statistic for a causal effect estimated using two-sample Mendelian randomization (MR).\n\n### Step 1: Problem Validation\nThe problem statement is validated according to the specified criteria.\n\n**1. Extract Givens:**\n- A single genetic variant is an instrumental variable (IV) for a continuous exposure ($X$) and a binary outcome ($Y$).\n- The three core IV assumptions (relevance, independence, exclusion restriction) are assumed to hold.\n- The analysis is a two-sample MR, with estimates from two independent studies.\n- Study A (exposure sample):\n    - Genetic association with exposure: $\\hat{\\beta}_{XG} = 0.08$.\n    - Standard error of the association: $SE(\\hat{\\beta}_{XG}) = 0.01$.\n- Study B (outcome sample):\n    - Genetic association with the log-odds of the outcome: $\\hat{\\beta}_{YG} = 0.024$.\n    - Standard error of the association: $SE(\\hat{\\beta}_{YG}) = 0.006$.\n- The task is to compute the Wald test statistic $z = \\hat{\\beta}_{W} / SE(\\hat{\\beta}_{W})$ for the null hypothesis of no causal effect, where $\\hat{\\beta}_{W}$ is the ratio estimator of the causal effect.\n\n**2. Validate Using Extracted Givens:**\n- **Scientifically Grounded:** The problem describes a standard two-sample Mendelian randomization analysis, a widely used and validated method in epidemiology. The instrumental variable assumptions, the ratio estimator, and the use of the delta method are all core components of this methodology.\n- **Well-Posed:** The problem provides all necessary data and clearly specifies the methods to be used (ratio estimation, delta method) to compute a single, well-defined quantity (the Wald statistic).\n- **Objective:** The problem is stated using precise, quantitative, and objective language.\n\n**3. Verdict and Action:**\nThe problem is scientifically sound, well-posed, objective, and complete. It is therefore deemed **valid**. The solution will now be constructed.\n\n### Step 2: Solution\nThe causal effect of the exposure $X$ on the log-odds of the outcome $Y$ is denoted by $\\beta_W$. Under the instrumental variable assumptions, the genetic association with the outcome ($\\beta_{YG}$) is the product of the causal effect and the genetic association with the exposure ($\\beta_{XG}$):\n$$ \\beta_{YG} = \\beta_W \\cdot \\beta_{XG} $$\nThe ratio estimator, also known as the Wald estimator, for the causal effect is therefore given by the ratio of the two association estimates:\n$$ \\hat{\\beta}_W = \\frac{\\hat{\\beta}_{YG}}{\\hat{\\beta}_{XG}} $$\nUsing the provided values, we can compute the point estimate of the causal effect:\n$$ \\hat{\\beta}_W = \\frac{0.024}{0.08} = 0.3 $$\nThis value represents an estimated increase of $0.3$ in the log-odds of the outcome for each one-unit increase in the exposure.\n\nTo construct the Wald test statistic, we need the standard error of this estimator, $SE(\\hat{\\beta}_W)$. The problem specifies using a first-order Taylor series approximation (the delta method) to find this. Let our estimator be a function of two random variables, $f(x, y) = y/x$, where $x = \\hat{\\beta}_{XG}$ and $y = \\hat{\\beta}_{YG}$. The variance of this function can be approximated as:\n$$ \\text{Var}(f(x, y)) \\approx \\left(\\frac{\\partial f}{\\partial x}\\right)^2 \\text{Var}(x) + \\left(\\frac{\\partial f}{\\partial y}\\right)^2 \\text{Var}(y) + 2 \\left(\\frac{\\partial f}{\\partial x}\\right) \\left(\\frac{\\partial f}{\\partial y}\\right) \\text{Cov}(x, y) $$\nThe partial derivatives of $f(x, y)$ are:\n$$ \\frac{\\partial f}{\\partial x} = -\\frac{y}{x^2} \\quad \\text{and} \\quad \\frac{\\partial f}{\\partial y} = \\frac{1}{x} $$\nSince the estimates $\\hat{\\beta}_{XG}$ and $\\hat{\\beta}_{YG}$ come from two independent samples, their covariance is zero, i.e., $\\text{Cov}(\\hat{\\beta}_{XG}, \\hat{\\beta}_{YG}) = 0$. The variance formula simplifies to:\n$$ \\text{Var}(\\hat{\\beta}_W) \\approx \\left(-\\frac{\\hat{\\beta}_{YG}}{\\hat{\\beta}_{XG}^2}\\right)^2 \\text{Var}(\\hat{\\beta}_{XG}) + \\left(\\frac{1}{\\hat{\\beta}_{XG}}\\right)^2 \\text{Var}(\\hat{\\beta}_{YG}) $$\nThe variance of an estimate is the square of its standard error, so $\\text{Var}(\\hat{\\beta}) = [SE(\\hat{\\beta})]^2$. Substituting this relationship into the formula gives the expression for the squared standard error of the causal effect estimate:\n$$ [SE(\\hat{\\beta}_W)]^2 \\approx \\frac{\\hat{\\beta}_{YG}^2}{\\hat{\\beta}_{XG}^4} [SE(\\hat{\\beta}_{XG})]^2 + \\frac{1}{\\hat{\\beta}_{XG}^2} [SE(\\hat{\\beta}_{YG})]^2 $$\nNow we substitute the given numerical values:\n- $\\hat{\\beta}_{XG} = 0.08$\n- $SE(\\hat{\\beta}_{XG}) = 0.01$\n- $\\hat{\\beta}_{YG} = 0.024$\n- $SE(\\hat{\\beta}_{YG}) = 0.006$\n\n$$ [SE(\\hat{\\beta}_W)]^2 \\approx \\frac{(0.024)^2}{(0.08)^4} (0.01)^2 + \\frac{1}{(0.08)^2} (0.006)^2 $$\n$$ [SE(\\hat{\\beta}_W)]^2 \\approx \\frac{0.000576}{0.00004096} (0.0001) + \\frac{1}{0.0064} (0.000036) $$\n$$ [SE(\\hat{\\beta}_W)]^2 \\approx (14.0625) \\times (0.0001) + (156.25) \\times (0.000036) $$\n$$ [SE(\\hat{\\beta}_W)]^2 \\approx 0.00140625 + 0.005625 $$\n$$ [SE(\\hat{\\beta}_W)]^2 \\approx 0.00703125 $$\nThe standard error is the square root of the variance:\n$$ SE(\\hat{\\beta}_W) = \\sqrt{0.00703125} \\approx 0.08385256... $$\nFinally, we compute the Wald test statistic, $z$, for the null hypothesis $H_0: \\beta_W = 0$:\n$$ z = \\frac{\\hat{\\beta}_W}{SE(\\hat{\\beta}_W)} $$\n$$ z = \\frac{0.3}{0.08385256...} \\approx 3.57770876... $$\nRounding to four significant figures, we get:\n$$ z \\approx 3.578 $$", "answer": "$$\\boxed{3.578}$$", "id": "4611689"}, {"introduction": "While single-instrument analysis is instructive, most modern MR studies combine information from many genetic variants to increase statistical power. This practice guides you through implementing the most common multi-instrument methods, Inverse-Variance Weighted (IVW) and MR-Egger regression, and introduces critical diagnostic tools like Cochran's $Q$ statistic and funnel plots. Mastering these techniques is essential for conducting a robust MR analysis and for identifying potential violations of core assumptions, such as horizontal pleiotropy [@problem_id:2404096].", "problem": "You are given summary association data from Genome-Wide Association Study (GWAS) instruments, each being a Single Nucleotide Polymorphism (SNP), in the setting of Mendelian randomization (MR). For each SNP, you have the association with an exposure, denoted $ \\beta_{GX,i} $, and the association with an outcome, denoted $ \\beta_{GY,i} $, together with the standard error of the outcome association $ \\sigma_{GY,i} $. Assume the following fundamental base: (i) a linear causal model for the exposure–outcome relationship; (ii) instruments affect the outcome only through the exposure apart from any potential horizontal pleiotropy; (iii) sampling variability in $ \\beta_{GY,i} $ is quantified by $ \\sigma_{GY,i} $, and uncertainty in $ \\beta_{GX,i} $ is negligible relative to $ \\sigma_{GY,i} $ for the purpose of weighting; (iv) the causal effect is estimable by aggregating per-variant information using weighted least squares.\n\nYour task is to write a complete program that, for each test case described below, computes the numerical objects required to generate (a) a scatter plot of $ \\beta_{GY} $ versus $ \\beta_{GX} $ with both the intercept-constrained Inverse-Variance Weighted (IVW) regression line and the Mendelian randomization Egger (MR-Egger) regression line, and (b) a funnel plot of ratio estimates versus their standard errors to visually inspect for heterogeneity and pleiotropy. Instead of drawing any plot, your program must return the precise numerical quantities that define those plots.\n\nStarting only from the base principles above, implement the following computations for each test case:\n- Use weights $ w_i $ defined by the inverse of the outcome variance, i.e., $ w_i $ proportional to $ 1 / \\sigma_{GY,i}^2 $.\n- Compute the intercept-constrained IVW estimate of the causal slope, by solving the weighted least squares problem with intercept fixed to zero that minimizes $ \\sum_i w_i \\left( \\beta_{GY,i} - b \\, \\beta_{GX,i} \\right)^2 $ over $ b $.\n- Compute the MR-Egger weighted regression line with an unconstrained intercept, by minimizing $ \\sum_i w_i \\left( \\beta_{GY,i} - a - b \\, \\beta_{GX,i} \\right)^2 $ over $ a $ and $ b $.\n- Compute Cochran’s $ Q $ statistic for heterogeneity under the IVW fit and the corresponding $ I^2 $ heterogeneity metric, where $ Q $ compares the dispersion of the weighted residuals to their expected value under homogeneity.\n- For the funnel plot, compute the per-variant ratio estimate $ \\theta_i $ and its approximate standard error $ s_i $ under the assumption that uncertainty in $ \\beta_{GX,i} $ is negligible relative to $ \\sigma_{GY,i} $. Then compute the pseudo $ 95\\% $ funnel bounds for each variant around the pooled IVW effect as $ \\theta_{\\text{IVW}} \\pm 1.96 \\, s_i $.\n\nYour program must apply these computations to the following test suite. Each test case is defined by three lists of equal length: $ \\beta_{GX} $, $ \\beta_{GY} $, and $ \\sigma_{GY} $.\n\nTest case A (happy path; consistent instruments):\n- $ \\beta_{GX} = [\\, 0.08, \\, 0.12, \\, 0.10, \\, 0.15, \\, 0.07, \\, 0.11 \\,] $\n- $ \\beta_{GY} = [\\, 0.040, \\, 0.060, \\, 0.051, \\, 0.072, \\, 0.033, \\, 0.057 \\,] $\n- $ \\sigma_{GY} = [\\, 0.020, \\, 0.018, \\, 0.022, \\, 0.019, \\, 0.021, \\, 0.020 \\,] $\n\nTest case B (directional pleiotropy; nonzero intercept expected):\n- $ \\beta_{GX} = [\\, 0.05, \\, -0.04, \\, 0.09, \\, 0.12, \\, 0.03, \\, 0.07 \\,] $\n- $ \\beta_{GY} = [\\, 0.037, \\, 0.007, \\, 0.048, \\, 0.054, \\, 0.029, \\, 0.042 \\,] $\n- $ \\sigma_{GY} = [\\, 0.020, \\, 0.021, \\, 0.019, \\, 0.018, \\, 0.022, \\, 0.020 \\,] $\n\nTest case C (heterogeneity and a weak instrument):\n- $ \\beta_{GX} = [\\, 0.20, \\, 0.15, \\, 0.10, \\, 0.05, \\, 0.004 \\,] $\n- $ \\beta_{GY} = [\\, 0.080, \\, 0.070, \\, 0.045, \\, 0.050, \\, 0.010 \\,] $\n- $ \\sigma_{GY} = [\\, 0.015, \\, 0.015, \\, 0.016, \\, 0.020, \\, 0.020 \\,] $\n\nTest case D (balanced pleiotropy; heterogeneity with approximately zero intercept):\n- $ \\beta_{GX} = [\\, 0.10, \\, 0.12, \\, 0.09, \\, 0.11, \\, 0.08 \\,] $\n- $ \\beta_{GY} = [\\, 0.080, \\, 0.052, \\, 0.064, \\, 0.056, \\, 0.048 \\,] $\n- $ \\sigma_{GY} = [\\, 0.020, \\, 0.020, \\, 0.020, \\, 0.020, \\, 0.020 \\,] $\n\nImplementation and numerical requirements:\n- Treat all weights as $ w_i = 1 / \\sigma_{GY,i}^2 $.\n- For the funnel plot, compute $ \\theta_i = \\beta_{GY,i} / \\beta_{GX,i} $ and $ s_i = \\sigma_{GY,i} / \\lvert \\beta_{GX,i} \\rvert $.\n- Use the IVW slope for the pooled effect in the funnel plot bounds $ \\theta_{\\text{IVW}} \\pm 1.96 \\, s_i $.\n- For Cochran’s heterogeneity statistic under the IVW fit, compute $ Q $ and then $ I^2 = \\max \\left( 0, \\, \\dfrac{Q - (M - 1)}{Q} \\right) $ with $ M $ the number of variants. If $ Q = 0 $, set $ I^2 = 0 $.\n- Your program must output, for each test case, a list of nine elements in the following order:\n  1. the IVW slope (a float),\n  2. the MR-Egger slope (a float),\n  3. the MR-Egger intercept (a float),\n  4. the IVW Cochran’s $ Q $ (a float),\n  5. the IVW $ I^2 $ (a float),\n  6. the list of ratio estimates $ [ \\theta_i ] $,\n  7. the list of ratio standard errors $ [ s_i ] $,\n  8. the list of lower funnel bounds $ [ \\theta_{\\text{IVW}} - 1.96 \\, s_i ] $,\n  9. the list of upper funnel bounds $ [ \\theta_{\\text{IVW}} + 1.96 \\, s_i ] $.\n- Express all floats rounded to six decimal places.\n- Final output format: Your program should produce a single line of output containing the four per-test-case results aggregated as a comma-separated list enclosed in square brackets, with no spaces. That is, a single line of the form $ [r_A, r_B, r_C, r_D] $ where each $ r_\\cdot $ is the nine-element list described above.\n\nEdge conditions and scientific realism:\n- Enforce that $ \\lvert \\beta_{GX,i} \\rvert $ is not zero to avoid division by zero in ratio computations. The provided test suite satisfies this; in general, if any $ \\lvert \\beta_{GX,i} \\rvert $ were below a small threshold $ \\varepsilon $, the variant should be excluded from the ratio and funnel components while remaining consistent in regression fits if handled appropriately. In this test suite, no exclusions are necessary.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the established principles of Mendelian randomization (MR), a standard method in genetic epidemiology. The problem is well-posed, providing all necessary data and explicit mathematical definitions for the required computations. The language is objective and formal, free of ambiguity or subjective claims. It presents a solvable computational task based on verifiable statistical and mathematical principles.\n\nWe will now proceed with a systematic derivation of the required quantities. The context is the estimation of a causal effect of an exposure on an outcome using genetic variants as instrumental variables. For each of $M$ genetic variants (SNPs), we are given its estimated association with the exposure, $\\beta_{GX,i}$, its estimated association with the outcome, $\\beta_{GY,i}$, and the standard error of the latter, $\\sigma_{GY,i}$.\n\nThe weights for all weighted calculations are defined by the inverse of the outcome variance, assuming uncertainty in $\\beta_{GX,i}$ is negligible for this purpose:\n$$\nw_i = \\frac{1}{\\sigma_{GY,i}^2}\n$$\n\n**1. Intercept-Constrained Inverse-Variance Weighted (IVW) Slope**\n\nThe IVW method estimates the causal effect, $b$, by solving a weighted least squares problem that forces the regression line through the origin. This corresponds to the assumption of no horizontal pleiotropy. The objective is to minimize the sum of weighted squared residuals:\n$$\nS(b) = \\sum_{i=1}^{M} w_i \\left( \\beta_{GY,i} - b \\, \\beta_{GX,i} \\right)^2\n$$\nTo find the minimum, we set the derivative with respect to $b$ to zero:\n$$\n\\frac{dS}{db} = -2 \\sum_{i=1}^{M} w_i \\beta_{GX,i} \\left( \\beta_{GY,i} - b \\, \\beta_{GX,i} \\right) = 0\n$$\nSolving for $b$ yields the IVW estimate, which we denote $\\theta_{\\text{IVW}}$:\n$$\n\\theta_{\\text{IVW}} = \\frac{\\sum_{i=1}^{M} w_i \\beta_{GX,i} \\beta_{GY,i}}{\\sum_{i=1}^{M} w_i \\beta_{GX,i}^2}\n$$\n\n**2. Mendelian Randomization Egger (MR-Egger) Regression**\n\nThe MR-Egger method relaxes the no-pleiotropy assumption of the IVW method by allowing for a non-zero intercept in the regression of $\\beta_{GY,i}$ on $\\beta_{GX,i}$. The intercept, $a$, can be interpreted as an estimate of the average directional pleiotropic effect, while the slope, $b$, remains the estimate of the causal effect. We minimize the following objective function over both $a$ and $b$:\n$$\nS(a, b) = \\sum_{i=1}^{M} w_i \\left( \\beta_{GY,i} - a - b \\, \\beta_{GX,i} \\right)^2\n$$\nThis is a standard weighted linear regression problem. The solutions for the MR-Egger slope ($b_{\\text{Egger}}$) and intercept ($a_{\\text{Egger}}$) are given by the normal equations:\n$$\nb_{\\text{Egger}} = \\frac{ \\left(\\sum w_i\\right) \\left(\\sum w_i \\beta_{GX,i} \\beta_{GY,i}\\right) - \\left(\\sum w_i \\beta_{GX,i}\\right) \\left(\\sum w_i \\beta_{GY,i}\\right) }{ \\left(\\sum w_i\\right) \\left(\\sum w_i \\beta_{GX,i}^2\\right) - \\left(\\sum w_i \\beta_{GX,i}\\right)^2 }\n$$\n$$\na_{\\text{Egger}} = \\frac{\\sum w_i \\beta_{GY,i}}{\\sum w_i} - b_{\\text{Egger}} \\frac{\\sum w_i \\beta_{GX,i}}{\\sum w_i}\n$$\nThese formulas correspond to the standard solution for weighted least squares regression coefficients.\n\n**3. Cochran’s Q Statistic and I² Heterogeneity Metric**\n\nHeterogeneity among the instrument-specific causal estimates can indicate either violation of the MR assumptions (such as pleiotropy) or that the true causal effect differs for subsets of the population targeted by different instruments. Cochran’s $Q$ statistic for the IVW model quantifies this heterogeneity by summing the weighted squared differences between the individual ratio estimates and the pooled IVW estimate. It is calculated as:\n$$\nQ = \\sum_{i=1}^{M} w_i \\left( \\frac{\\beta_{GY,i}}{\\beta_{GX,i}} - \\theta_{\\text{IVW}} \\right)^2 \\beta_{GX,i}^2 = \\sum_{i=1}^{M} w_i \\left( \\beta_{GY,i} - \\theta_{\\text{IVW}} \\beta_{GX,i} \\right)^2\n$$\nUnder the null hypothesis of homogeneity (i.e., all instruments estimate the same causal effect), $Q$ follows a chi-squared distribution with $M-1$ degrees of freedom.\n\nThe $I^2$ statistic describes the percentage of variation across instruments that is due to heterogeneity rather than sampling error. It is derived from $Q$:\n$$\nI^2 = \\max\\left(0, \\frac{Q - (M-1)}{Q}\\right)\n$$\nIf $Q=0$, which is highly unlikely in practice, $I^2$ is defined as $0$.\n\n**4. Funnel Plot Components**\n\nA funnel plot is a visual tool to investigate heterogeneity and publication bias. It plots the effect size of each instrument against a measure of its precision.\n\n-   **Per-variant ratio estimate ($\\theta_i$):** This is the causal effect estimated from a single instrument $i$:\n    $$\n    \\theta_i = \\frac{\\beta_{GY,i}}{\\beta_{GX,i}}\n    $$\n-   **Standard error of the ratio estimate ($s_i$):** Using the delta method and the assumption that $\\beta_{GX,i}$ is measured with negligible error, the standard error of $\\theta_i$ is approximated as:\n    $$\n    s_i = \\text{SE}(\\theta_i) \\approx \\frac{\\sigma_{GY,i}}{\\lvert \\beta_{GX,i} \\rvert}\n    $$\n-   **Funnel plot bounds:** The funnel is constructed around the pooled IVW causal estimate, $\\theta_{\\text{IVW}}$. For a pseudo $95\\%$ confidence interval, the bounds for each variant $i$ are:\n    $$\n    \\text{Bounds}_i = \\theta_{\\text{IVW}} \\pm 1.96 \\, s_i\n    $$\n    The lower and upper bounds are $\\theta_{\\text{IVW}} - 1.96 \\, s_i$ and $\\theta_{\\text{IVW}} + 1.96 \\, s_i$, respectively.\n\nThe implementation will compute these nine quantities for each provided test case: the IVW slope, the MR-Egger slope and intercept, the Cochran's $Q$ and $I^2$ statistics for the IVW fit, and the lists of ratio estimates, their standard errors, and the corresponding lower and upper funnel bounds. All floating-point numbers will be rounded to six decimal places as required.", "answer": "```python\nimport numpy as np\n\ndef calculate_mr_metrics(beta_gx: list[float], beta_gy: list[float], sigma_gy: list[float]) -> list:\n    \"\"\"\n    Computes Mendelian randomization metrics for a given set of summary statistics.\n\n    Args:\n        beta_gx: List of SNP-exposure associations.\n        beta_gy: List of SNP-outcome associations.\n        sigma_gy: List of standard errors for SNP-outcome associations.\n\n    Returns:\n        A list containing nine elements as specified in the problem description.\n    \"\"\"\n    # Convert lists to NumPy arrays for vectorized operations\n    bgx = np.array(beta_gx)\n    bgy = np.array(beta_gy)\n    sgy = np.array(sigma_gy)\n    \n    # 1. Weights\n    # w_i = 1 / sigma_GY,i^2\n    w = 1.0 / (sgy**2)\n    \n    # 2. IVW Slope (Intercept-constrained)\n    # theta_ivw = (sum w_i * beta_gx_i * beta_gy_i) / (sum w_i * beta_gx_i^2)\n    ivw_numerator = np.sum(w * bgx * bgy)\n    ivw_denominator = np.sum(w * bgx**2)\n    ivw_slope = ivw_numerator / ivw_denominator\n    \n    # 3. MR-Egger Slope and Intercept\n    # Weighted least squares regression of bgy on bgx with weights w\n    W = np.sum(w)\n    Swx = np.sum(w * bgx)\n    Swy = np.sum(w * bgy)\n    Swxx = np.sum(w * bgx**2)\n    Swxy = np.sum(w * bgx * bgy)\n    \n    egger_denominator = (W * Swxx - Swx**2)\n    if egger_denominator == 0:\n        # This case is unlikely with real data but handle for robustness\n        mr_egger_slope = np.nan\n        mr_egger_intercept = np.nan\n    else:\n        mr_egger_slope = (W * Swxy - Swx * Swy) / egger_denominator\n        mr_egger_intercept = (Swy / W) - mr_egger_slope * (Swx / W)\n\n    # 4. Cochran's Q for IVW\n    # Q = sum w_i * (beta_gy_i - theta_ivw * beta_gx_i)^2\n    cochran_q = np.sum(w * (bgy - ivw_slope * bgx)**2)\n    \n    # 5. I^2 for IVW\n    M = len(bgx)\n    df = M - 1\n    if cochran_q == 0:\n        i_squared = 0.0\n    else:\n        i_squared = max(0.0, (cochran_q - df) / cochran_q)\n\n    # 6. Ratio estimates (theta_i)\n    # theta_i = beta_gy_i / beta_gx_i\n    theta_i = bgy / bgx\n    \n    # 7. Ratio standard errors (s_i)\n    # s_i = sigma_gy_i / |beta_gx_i|\n    s_i = sgy / np.abs(bgx)\n    \n    # 8. & 9. Funnel plot bounds\n    # lower/upper = theta_ivw +/- 1.96 * s_i\n    z_score = 1.96\n    funnel_lower_bounds = ivw_slope - z_score * s_i\n    funnel_upper_bounds = ivw_slope + z_score * s_i\n    \n    # Assemble results and round to 6 decimal places\n    results = [\n        round(ivw_slope, 6),\n        round(mr_egger_slope, 6),\n        round(mr_egger_intercept, 6),\n        round(cochran_q, 6),\n        round(i_squared, 6),\n        [round(val, 6) for val in theta_i],\n        [round(val, 6) for val in s_i],\n        [round(val, 6) for val in funnel_lower_bounds],\n        [round(val, 6) for val in funnel_upper_bounds],\n    ]\n    \n    return results\n\ndef format_result_list(res_list: list) -> str:\n    \"\"\"Formats a single test case result list into the required string format.\"\"\"\n    str_parts = []\n    for item in res_list:\n        if isinstance(item, list):\n            formatted_list = f\"[{','.join([f'{x:.6f}' for x in item])}]\"\n            str_parts.append(formatted_list)\n        else:\n            str_parts.append(f\"{item:.6f}\")\n    return f\"[{','.join(str_parts)}]\"\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = {\n        'A': {\n            \"beta_gx\": [0.08, 0.12, 0.10, 0.15, 0.07, 0.11],\n            \"beta_gy\": [0.040, 0.060, 0.051, 0.072, 0.033, 0.057],\n            \"sigma_gy\": [0.020, 0.018, 0.022, 0.019, 0.021, 0.020]\n        },\n        'B': {\n            \"beta_gx\": [0.05, -0.04, 0.09, 0.12, 0.03, 0.07],\n            \"beta_gy\": [0.037, 0.007, 0.048, 0.054, 0.029, 0.042],\n            \"sigma_gy\": [0.020, 0.021, 0.019, 0.018, 0.022, 0.020]\n        },\n        'C': {\n            \"beta_gx\": [0.20, 0.15, 0.10, 0.05, 0.004],\n            \"beta_gy\": [0.080, 0.070, 0.045, 0.050, 0.010],\n            \"sigma_gy\": [0.015, 0.015, 0.016, 0.020, 0.020]\n        },\n        'D': {\n            \"beta_gx\": [0.10, 0.12, 0.09, 0.11, 0.08],\n            \"beta_gy\": [0.080, 0.052, 0.064, 0.056, 0.048],\n            \"sigma_gy\": [0.020, 0.020, 0.020, 0.020, 0.020]\n        }\n    }\n\n    all_results_str = []\n    # Process cases in alphabetical order to match output format\n    for key in sorted(test_cases.keys()):\n        case = test_cases[key]\n        result = calculate_mr_metrics(case[\"beta_gx\"], case[\"beta_gy\"], case[\"sigma_gy\"])\n        all_results_str.append(format_result_list(result))\n\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "2404096"}, {"introduction": "A common challenge in epidemiology is that risk factors are often correlated, making it difficult to isolate the causal effect of a single exposure. This is where Multivariable Mendelian Randomization (MVMR) becomes a powerful tool, allowing for the simultaneous estimation of causal effects from several exposures on a single outcome. This exercise will guide you through the implementation of an MVMR analysis to partition the effects of a composite \"healthy lifestyle\" exposure, demonstrating how to disentangle complex causal pathways using genetic data [@problem_id:2404098].", "problem": "You are given summary-level genetic association data from independent single nucleotide polymorphisms (SNPs) that instrument three components of a composite exposure often called a \"healthy lifestyle\": diet quality, exercise frequency, and smoking intensity. The scientific goal is to partition the total causal effect on a disease outcome into components attributable to each exposure using Multivariable Mendelian Randomization (MVMR). Assume the following fundamental base: the instrumental variable assumptions hold for each SNP with respect to the three exposures and the outcome, the relationships are linear on the chosen effect scale, there is no horizontal pleiotropy outside the three exposures, and the SNPs are mutually independent (no linkage disequilibrium). Under these assumptions, the causal decomposition can be formalized by the structural model\n$$\nY = \\beta_{D} \\cdot D + \\beta_{E} \\cdot E + \\beta_{S} \\cdot S + \\varepsilon,\n$$\nwhere $Y$ is the disease outcome, $D$ is diet quality, $E$ is exercise frequency, $S$ is smoking intensity, and $(\\beta_{D}, \\beta_{E}, \\beta_{S})$ are the causal parameters to be estimated. Let there be $m$ SNPs. For each SNP $i \\in \\{1,\\dots,m\\}$, let $b_{Y,i}$ denote the SNP-outcome association estimate and $s_{Y,i}$ its standard error, and let $(b_{D,i}, b_{E,i}, b_{S,i})$ denote the SNP-exposure association estimates for diet, exercise, and smoking, respectively. Under the Generalized Method of Moments with inverse-variance weighting and zero intercept (no unbalanced pleiotropy), the MVMR estimator is the solution to the weighted normal equations implied by the moment conditions $\\mathbb{E}\\left[Z_i \\cdot (Y - \\beta_{D} D - \\beta_{E} E - \\beta_{S} S)\\right] = 0$, where $Z_i$ encodes the genotype at SNP $i$. In matrix terms, letting $X \\in \\mathbb{R}^{m \\times 3}$ have rows $(b_{D,i}, b_{E,i}, b_{S,i})$, $y \\in \\mathbb{R}^{m}$ have entries $b_{Y,i}$, and $W \\in \\mathbb{R}^{m \\times m}$ be diagonal with entries $w_i = 1/s_{Y,i}^{2}$, the estimator $(\\hat{\\beta}_{D}, \\hat{\\beta}_{E}, \\hat{\\beta}_{S})$ is the unique vector (when it exists) that satisfies the weighted normal equations. If the system is ill-conditioned, use a numerically stable solver that returns the minimum-norm solution.\n\nYour task is to write a program that, for each test case below, constructs $X$, $y$, and $W$ from the provided numbers, solves for $(\\hat{\\beta}_{D}, \\hat{\\beta}_{E}, \\hat{\\beta}_{S})$, and outputs the three component estimates rounded to four decimal places.\n\nThere are no physical units to report for the effect sizes; report numerical values only, as real numbers rounded to four decimal places. Do not use a percentage sign; express all numerical results as decimals.\n\nTest Suite:\nFor each test case, $m$ is the number of SNPs, the matrix $X$ is given by listing each SNP’s exposure association vector $[b_{D,i}, b_{E,i}, b_{S,i}]$, the vector $y$ lists $b_{Y,i}$, and the vector $s_Y$ lists $s_{Y,i}$.\n\nTest Case 1 (general well-conditioned case):\n- $m = 8$\n- $X$ rows:\n  $[0.08, 0.00, 0.02]$,\n  $[0.05, 0.03, 0.00]$,\n  $[0.00, 0.06, 0.04]$,\n  $[0.10, 0.02, 0.01]$,\n  $[-0.04, 0.01, 0.05]$,\n  $[0.02, 0.07, 0.00]$,\n  $[0.00, 0.00, 0.09]$,\n  $[0.03, -0.02, 0.02]$\n- $y$ entries:\n  $-0.014$,\n  $-0.029$,\n  $0.018$,\n  $-0.037$,\n  $0.058$,\n  $-0.029$,\n  $0.081$,\n  $0.012$\n- $s_Y$ entries:\n  $0.02$,\n  $0.02$,\n  $0.02$,\n  $0.02$,\n  $0.03$,\n  $0.025$,\n  $0.02$,\n  $0.03$\n\nTest Case 2 (exactly identified case: $m = 3$ equals the number of exposures):\n- $m = 3$\n- $X$ rows:\n  $[0.10, 0.00, 0.02]$,\n  $[0.00, 0.08, 0.01]$,\n  $[0.02, 0.01, 0.09]$\n- $y$ entries:\n  $-0.006$,\n  $-0.025$,\n  $0.055$\n- $s_Y$ entries:\n  $0.01$,\n  $0.01$,\n  $0.01$\n\nTest Case 3 (near-collinearity between diet and exercise):\n- $m = 6$\n- $X$ rows:\n  $[0.10, 0.099, 0.00]$,\n  $[0.08, 0.081, 0.02]$,\n  $[0.12, 0.121, 0.01]$,\n  $[0.09, 0.091, -0.01]$,\n  $[0.11, 0.109, 0.00]$,\n  $[0.07, 0.071, 0.03]$\n- $y$ entries:\n  $-0.0597$,\n  $-0.0383$,\n  $-0.0673$,\n  $-0.0593$,\n  $-0.0657$,\n  $-0.0273$\n- $s_Y$ entries:\n  $0.02$,\n  $0.02$,\n  $0.02$,\n  $0.02$,\n  $0.02$,\n  $0.02$\n\nAlgorithmic requirements:\n- Construct $W$ with diagonal entries $w_i = 1 / s_{Y,i}^{2}$.\n- Solve for $(\\hat{\\beta}_{D}, \\hat{\\beta}_{E}, \\hat{\\beta}_{S})$ as the weighted least squares solution to the normal equations. If $(X^{\\top} W X)$ is ill-conditioned or singular, compute the minimum-norm solution using a numerically stable pseudo-inverse.\n- Round each component estimate to four decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a JSON-like list of lists, where each inner list corresponds to a test case in order and contains the three rounded estimates $[\\hat{\\beta}_{D}, \\hat{\\beta}_{E}, \\hat{\\beta}_{S}]$. For example, the printed output must look like\n$[[a_{11},a_{12},a_{13}],[a_{21},a_{22},a_{23}],[a_{31},a_{32},a_{33}]]$\nwith each $a_{ij}$ a real number rounded to four decimal places and no additional text.", "solution": "The problem requires the estimation of causal effects for a composite exposure using Multivariable Mendelian Randomization (MVMR). The problem statement has been validated and is deemed scientifically sound, objective, and well-posed. We may proceed with a formal derivation and solution.\n\nThe stated structural model for the outcome $Y$ as a function of three exposures—diet quality $D$, exercise frequency $E$, and smoking intensity $S$—is a linear equation:\n$$\nY = \\beta_{D} \\cdot D + \\beta_{E} \\cdot E + \\beta_{S} \\cdot S + \\varepsilon\n$$\nHere, $(\\beta_{D}, \\beta_{E}, \\beta_{S})$ are the causal parameters of interest. The principles of two-sample Mendelian randomization establish a relationship between the SNP-outcome genetic association, $b_{Y,i}$, and the SNP-exposure genetic associations, $(b_{D,i}, b_{E,i}, b_{S,i})$, for each of a set of $m$ instrumental variable SNPs. Assuming the instrumental variable assumptions hold and there is no horizontal pleiotropy beyond the measured exposures, this relationship is expressed as:\n$$\n\\mathbb{E}[b_{Y,i}] = \\beta_{D} \\cdot b_{D,i} + \\beta_{E} \\cdot b_{E,i} + \\beta_{S} \\cdot b_{S,i}\n$$\nThis forms the basis for estimating the causal parameter vector $\\beta = [\\beta_{D}, \\beta_{E}, \\beta_{S}]^T$.\n\nAcross all $m$ independent SNPs, we can express this system in matrix form. Let $y \\in \\mathbb{R}^{m}$ be the vector of SNP-outcome associations with entries $y_i = b_{Y,i}$, and let $X \\in \\mathbb{R}^{m \\times 3}$ be the matrix of SNP-exposure associations, where the $i$-th row is $[b_{D,i}, b_{E,i}, b_{S,i}]$. The system of equations can be written as:\n$$\ny \\approx X\\beta\n$$\nThe problem specifies an inverse-variance weighted (IVW) estimator derived from the Generalized Method of Moments. This is equivalent to solving a weighted least squares (WLS) problem. The objective is to find the vector $\\hat{\\beta}$ that minimizes the sum of squared residuals, weighted by the inverse variance of each SNP-outcome association estimate:\n$$\nS(\\beta) = \\sum_{i=1}^{m} w_i (y_i - (X\\beta)_i)^2 = (y - X\\beta)^T W (y - X\\beta)\n$$\nThe weight matrix $W$ is a diagonal matrix of dimension $m \\times m$ with diagonal entries $W_{ii} = w_i = 1/s_{Y,i}^2$, where $s_{Y,i}$ is the standard error of the estimate $b_{Y,i}$.\n\nTo find the minimum of $S(\\beta)$, we differentiate with respect to $\\beta$ and set the result to zero:\n$$\n\\frac{\\partial S(\\beta)}{\\partial \\beta} = -2X^T W (y - X\\beta) = 0\n$$\nThis gives rise to the system of normal equations for the WLS estimator $\\hat{\\beta}$:\n$$\n(X^T W X) \\hat{\\beta} = X^T W y\n$$\nIf the matrix $A = X^T W X$ is non-singular, the unique solution is given by $\\hat{\\beta} = A^{-1} (X^T W y)$. However, the problem statement prudently requires a numerically stable approach that can handle cases where $A$ is singular or ill-conditioned, which occurs if the instruments are weak or exhibit collinearity. For such scenarios, the minimum-norm solution is required.\n\nA robust and standard method is to transform the WLS problem into an equivalent ordinary least squares (OLS) problem. Let $W^{1/2}$ be the diagonal matrix with entries $\\sqrt{w_i} = 1/s_{Y,i}$. We can rewrite the objective function as:\n$$\nS(\\beta) = \\|W^{1/2}(y - X\\beta)\\|_2^2\n$$\nLet $y' = W^{1/2}y$ and $X' = W^{1/2}X$. The problem is now equivalent to finding the OLS solution that minimizes $\\|y' - X'\\beta\\|_2^2$. This is a standard linear least-squares problem.\n\nThis formulation is numerically advantageous as it avoids the explicit computation of $X^T W X$, which can have a much higher condition number than $X'$ itself, thus mitigating issues of numerical instability. We will solve for $\\hat{\\beta}$ by applying a numerically stable least-squares solver, such as one based on Singular Value Decomposition (SVD), to the transformed system $y' \\approx X'\\beta$. The `numpy.linalg.lstsq` function implements such a solver and correctly yields the minimum-norm solution in rank-deficient cases, satisfying all algorithmic requirements.\n\nThe computational procedure for each test case is as follows:\n1.  Construct the matrix of SNP-exposure associations $X$ and the vectors of SNP-outcome associations $y$ and their standard errors $s_Y$.\n2.  Calculate the vector of weights' square roots, whose elements are $\\sqrt{w_i} = 1/s_{Y,i}$.\n3.  Compute the transformed matrix $X' = W^{1/2}X$ and vector $y' = W^{1/2}y$ via element-wise multiplication. In code, this is achieved by multiplying each row of $X$ and each element of $y$ by the corresponding $\\sqrt{w_i}$.\n4.  Solve the least-squares problem for $\\hat{\\beta}$ using `numpy.linalg.lstsq(X', y')`.\n5.  Extract the solution vector $\\hat{\\beta} = [\\hat{\\beta}_D, \\hat{\\beta}_E, \\hat{\\beta}_S]^T$ and round each component to four decimal places as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the causal effects in Multivariable Mendelian Randomization\n    for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1 (general well-conditioned case)\n        {\n            \"m\": 8,\n            \"X\": np.array([\n                [0.08, 0.00, 0.02],\n                [0.05, 0.03, 0.00],\n                [0.00, 0.06, 0.04],\n                [0.10, 0.02, 0.01],\n                [-0.04, 0.01, 0.05],\n                [0.02, 0.07, 0.00],\n                [0.00, 0.00, 0.09],\n                [0.03, -0.02, 0.02]\n            ]),\n            \"y\": np.array([\n                -0.014, -0.029, 0.018, -0.037, 0.058, \n                -0.029, 0.081, 0.012\n            ]),\n            \"s_Y\": np.array([\n                0.02, 0.02, 0.02, 0.02, 0.03, \n                0.025, 0.02, 0.03\n            ])\n        },\n        # Test Case 2 (exactly identified case)\n        {\n            \"m\": 3,\n            \"X\": np.array([\n                [0.10, 0.00, 0.02],\n                [0.00, 0.08, 0.01],\n                [0.02, 0.01, 0.09]\n            ]),\n            \"y\": np.array([-0.006, -0.025, 0.055]),\n            \"s_Y\": np.array([0.01, 0.01, 0.01])\n        },\n        # Test Case 3 (near-collinearity)\n        {\n            \"m\": 6,\n            \"X\": np.array([\n                [0.10, 0.099, 0.00],\n                [0.08, 0.081, 0.02],\n                [0.12, 0.121, 0.01],\n                [0.09, 0.091, -0.01],\n                [0.11, 0.109, 0.00],\n                [0.07, 0.071, 0.03]\n            ]),\n            \"y\": np.array([\n                -0.0597, -0.0383, -0.0673, -0.0593, \n                -0.0657, -0.0273\n            ]),\n            \"s_Y\": np.array([0.02, 0.02, 0.02, 0.02, 0.02, 0.02])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case[\"X\"]\n        y = case[\"y\"]\n        s_Y = case[\"s_Y\"]\n\n        # Calculate the square root of weights for the WLS transformation.\n        # sqrt_w_i = 1 / s_Y_i\n        sqrt_w = 1.0 / s_Y\n\n        # Transform inputs into an Ordinary Least Squares (OLS) equivalent problem.\n        # X' = W^(1/2) * X\n        # y' = W^(1/2) * y\n        # The operation is equivalent to multiplying each row of X and element of y\n        # by the corresponding sqrt(weight).\n        X_prime = X * sqrt_w[:, np.newaxis]\n        y_prime = y * sqrt_w\n\n        # Solve the least-squares problem for beta in y' = X' * beta.\n        # np.linalg.lstsq is numerically stable and returns the minimum-norm\n        # solution for ill-conditioned or singular systems as required.\n        beta_hat, _, _, _ = np.linalg.lstsq(X_prime, y_prime, rcond=None)\n        \n        results.append(beta_hat)\n\n    # Format the output string exactly as specified in the problem.\n    # Each beta_hat vector is formatted to a string \"[b1,b2,b3]\" with\n    # numbers rounded to four decimal places.\n    outer_list_str = []\n    for res_list in results:\n        inner_list_str = f\"[{','.join([f'{x:.4f}' for x in res_list])}]\"\n        outer_list_str.append(inner_list_str)\n\n    # The final output is a string representation of a list of lists.\n    output_str = f\"[{','.join(outer_list_str)}]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "2404098"}]}