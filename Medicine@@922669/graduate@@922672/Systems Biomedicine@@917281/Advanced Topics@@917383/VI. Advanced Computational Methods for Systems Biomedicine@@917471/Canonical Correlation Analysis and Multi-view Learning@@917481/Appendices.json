{"hands_on_practices": [{"introduction": "This first exercise takes you directly to the heart of Canonical Correlation Analysis. Starting with given covariance matrices, you will translate the constrained optimization problem of maximizing correlation into a solvable generalized eigenvalue problem, a fundamental skill for understanding how CCA works under the hood. This practice reinforces the mathematical bridge between the statistical objective and the algebraic solution. [@problem_id:4322614]", "problem": "In systems biomedicine, integrating complementary molecular views (for example, messenger ribonucleic acid (mRNA) gene expression and metabolite abundances) can reveal coordinated biological variation across subjects. Consider two centered data views with $p=2$ gene expression variables collected alongside $q=2$ metabolite variables across a cohort. Assume the sample covariance between the gene expression view is given by\n$$\n\\Sigma_{XX} = \\begin{pmatrix}\n1 & 0.3 \\\\\n0.3 & 1.2\n\\end{pmatrix},\n$$\nthe sample covariance between the metabolite view is given by\n$$\n\\Sigma_{YY} = \\begin{pmatrix}\n1.5 & 0.4 \\\\\n0.4 & 0.8\n\\end{pmatrix},\n$$\nand the cross-covariance is given by\n$$\n\\Sigma_{XY} = \\begin{pmatrix}\n0.7 & 0.2 \\\\\n0.1 & 0.5\n\\end{pmatrix}.\n$$\nCanonical Correlation Analysis (CCA) seeks linear projections $a \\in \\mathbb{R}^{2}$ and $b \\in \\mathbb{R}^{2}$ that maximize the covariance of projected views subject to unit variance constraints in their respective covariance geometries. Formally, solve the constrained maximization problem\n$$\n\\max_{a \\in \\mathbb{R}^{2},\\, b \\in \\mathbb{R}^{2}} \\; a^{\\top}\\Sigma_{XY} b \\quad \\text{subject to} \\quad a^{\\top}\\Sigma_{XX} a = 1,\\; b^{\\top}\\Sigma_{YY} b = 1.\n$$\nDerive the first canonical correlation and associated canonical directions by starting from the fundamental definition above, formulating the necessary optimality conditions, and reducing the problem to an eigenvalue problem that you solve explicitly for the provided matrices. Present the numerical value of the first canonical correlation rounded to four significant figures. Express the final numerical answer as a pure decimal (no units). Only the numerical value of the first canonical correlation should appear in your final answer box.", "solution": "The problem is to find the first canonical correlation and associated canonical directions for two data views, $X$ and $Y$, with given sample covariance matrices $\\Sigma_{XX}$ and $\\Sigma_{YY}$, and a cross-covariance matrix $\\Sigma_{XY}$. The problem is formulated as a constrained maximization problem:\n$$ \\max_{a \\in \\mathbb{R}^{2},\\, b \\in \\mathbb{R}^{2}} \\; a^{\\top}\\Sigma_{XY} b \\quad \\text{subject to} \\quad a^{\\top}\\Sigma_{XX} a = 1,\\; b^{\\top}\\Sigma_{YY} b = 1. $$\nThe quantity $a^{\\top}\\Sigma_{XY} b$ represents the covariance between the projected variables $X a$ and $Y b$. The constraints normalize the variances of these projected variables to $1$. The quantity being maximized is therefore the correlation between the canonical variates, $\\text{corr}(a^{\\top}X, b^{\\top}Y)$.\n\nTo solve this constrained optimization problem, we use the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}$ is:\n$$ \\mathcal{L}(a, b, \\lambda_a, \\lambda_b) = a^{\\top}\\Sigma_{XY} b - \\frac{\\lambda_a}{2}(a^{\\top}\\Sigma_{XX} a - 1) - \\frac{\\lambda_b}{2}(b^{\\top}\\Sigma_{YY} b - 1) $$\nwhere $\\frac{\\lambda_a}{2}$ and $\\frac{\\lambda_b}{2}$ are Lagrange multipliers (the factor of $\\frac{1}{2}$ is for algebraic convenience).\n\nTo find the optimal $a$ and $b$, we take the partial derivatives of $\\mathcal{L}$ with respect to $a$ and $b$ and set them to zero.\nUsing the identities $\\frac{\\partial}{\\partial v}(u^{\\top}Av) = A^{\\top}u$ and $\\frac{\\partial}{\\partial v}(v^{\\top}Bv) = 2Bv$ for a symmetric matrix $B$:\n\n1.  Derivative with respect to $a^{\\top}$:\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial a} = \\Sigma_{XY} b - \\lambda_a \\Sigma_{XX} a = 0 \\implies \\Sigma_{XY} b = \\lambda_a \\Sigma_{XX} a $$\n\n2.  Derivative with respect to $b^{\\top}$:\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial b} = \\Sigma_{XY}^{\\top} a - \\lambda_b \\Sigma_{YY} b = 0 \\implies \\Sigma_{XY}^{\\top} a = \\lambda_b \\Sigma_{YY} b $$\n\nNow, let's determine the meaning of the Lagrange multipliers. Left-multiply the first equation by $a^{\\top}$:\n$$ a^{\\top}\\Sigma_{XY} b = \\lambda_a a^{\\top}\\Sigma_{XX} a $$\nUsing the constraint $a^{\\top}\\Sigma_{XX} a = 1$, we find that $\\lambda_a = a^{\\top}\\Sigma_{XY} b$. This is the quantity we want to maximize.\n\nSimilarly, left-multiply the second equation by $b^{\\top}$:\n$$ b^{\\top}\\Sigma_{XY}^{\\top} a = \\lambda_b b^{\\top}\\Sigma_{YY} b $$\nUsing the constraint $b^{\\top}\\Sigma_{YY} b = 1$, we find that $\\lambda_b = b^{\\top}\\Sigma_{XY}^{\\top} a$.\nSince $a^{\\top}\\Sigma_{XY} b$ is a scalar, it is equal to its transpose, $(a^{\\top}\\Sigma_{XY} b)^{\\top} = b^{\\top}\\Sigma_{XY}^{\\top} a$. Therefore, $\\lambda_a = \\lambda_b$. Let us denote this common value by $\\rho$, which is the canonical correlation.\n\nThe optimality conditions become a system of two coupled equations:\n$$ \\Sigma_{XY} b = \\rho \\Sigma_{XX} a \\quad (1) $$\n$$ \\Sigma_{XY}^{\\top} a = \\rho \\Sigma_{YY} b \\quad (2) $$\n\nTo solve this system, we can reduce it to an eigenvalue problem. The covariance matrices $\\Sigma_{XX}$ and $\\Sigma_{YY}$ are given as symmetric and are positive definite (as their determinants are $1.11 > 0$ and $1.04 > 0$ with positive diagonal entries), so their inverses $\\Sigma_{XX}^{-1}$ and $\\Sigma_{YY}^{-1}$ exist.\n\nFrom equation (1), we can express $a$ in terms of $b$ (assuming $\\rho \\neq 0$):\n$$ a = \\frac{1}{\\rho} \\Sigma_{XX}^{-1} \\Sigma_{XY} b $$\nSubstituting this expression for $a$ into equation (2):\n$$ \\Sigma_{XY}^{\\top} \\left( \\frac{1}{\\rho} \\Sigma_{XX}^{-1} \\Sigma_{XY} b \\right) = \\rho \\Sigma_{YY} b $$\n$$ \\frac{1}{\\rho} \\Sigma_{XY}^{\\top} \\Sigma_{XX}^{-1} \\Sigma_{XY} b = \\rho \\Sigma_{YY} b $$\nMultiplying by $\\rho$ and then by $\\Sigma_{YY}^{-1}$ from the left gives:\n$$ \\Sigma_{YY}^{-1} \\Sigma_{XY}^{\\top} \\Sigma_{XX}^{-1} \\Sigma_{XY} b = \\rho^2 b $$\nThis is a standard eigenvalue problem of the form $K b = \\lambda b$, where the matrix is $K = \\Sigma_{YY}^{-1} \\Sigma_{XY}^{\\top} \\Sigma_{XX}^{-1} \\Sigma_{XY}$ and the eigenvalues are $\\lambda = \\rho^2$, the squared canonical correlations. The eigenvectors $b$ are the canonical direction vectors for the $Y$ view.\n\nWe now perform the explicit calculations with the given matrices:\n$$ \\Sigma_{XX} = \\begin{pmatrix} 1 & 0.3 \\\\ 0.3 & 1.2 \\end{pmatrix}, \\quad \\Sigma_{YY} = \\begin{pmatrix} 1.5 & 0.4 \\\\ 0.4 & 0.8 \\end{pmatrix}, \\quad \\Sigma_{XY} = \\begin{pmatrix} 0.7 & 0.2 \\\\ 0.1 & 0.5 \\end{pmatrix} $$\nFirst, we compute the inverse matrices:\n$$ \\det(\\Sigma_{XX}) = 1(1.2) - (0.3)^2 = 1.11 \\implies \\Sigma_{XX}^{-1} = \\frac{1}{1.11} \\begin{pmatrix} 1.2 & -0.3 \\\\ -0.3 & 1 \\end{pmatrix} $$\n$$ \\det(\\Sigma_{YY}) = 1.5(0.8) - (0.4)^2 = 1.04 \\implies \\Sigma_{YY}^{-1} = \\frac{1}{1.04} \\begin{pmatrix} 0.8 & -0.4 \\\\ -0.4 & 1.5 \\end{pmatrix} $$\nNow we construct the matrix $K$ step-by-step.\nLet's compute $M_1 = \\Sigma_{XX}^{-1} \\Sigma_{XY}$:\n$$ M_1 = \\frac{1}{1.11} \\begin{pmatrix} 1.2 & -0.3 \\\\ -0.3 & 1 \\end{pmatrix} \\begin{pmatrix} 0.7 & 0.2 \\\\ 0.1 & 0.5 \\end{pmatrix} = \\frac{1}{1.11} \\begin{pmatrix} 0.84 - 0.03 & 0.24 - 0.15 \\\\ -0.21 + 0.1 & -0.06 + 0.5 \\end{pmatrix} = \\frac{1}{1.11} \\begin{pmatrix} 0.81 & 0.09 \\\\ -0.11 & 0.44 \\end{pmatrix} $$\nNext, we compute $M_2 = \\Sigma_{XY}^{\\top} M_1$:\n$$ M_2 = \\begin{pmatrix} 0.7 & 0.1 \\\\ 0.2 & 0.5 \\end{pmatrix} \\frac{1}{1.11} \\begin{pmatrix} 0.81 & 0.09 \\\\ -0.11 & 0.44 \\end{pmatrix} = \\frac{1}{1.11} \\begin{pmatrix} 0.567 - 0.011 & 0.063 + 0.044 \\\\ 0.162 - 0.055 & 0.018 + 0.22 \\end{pmatrix} = \\frac{1}{1.11} \\begin{pmatrix} 0.556 & 0.107 \\\\ 0.107 & 0.238 \\end{pmatrix} $$\nFinally, we compute $K = \\Sigma_{YY}^{-1} M_2$:\n$$ K = \\frac{1}{1.04} \\begin{pmatrix} 0.8 & -0.4 \\\\ -0.4 & 1.5 \\end{pmatrix} \\frac{1}{1.11} \\begin{pmatrix} 0.556 & 0.107 \\\\ 0.107 & 0.238 \\end{pmatrix} $$\n$$ K = \\frac{1}{1.1544} \\begin{pmatrix} 0.8(0.556) - 0.4(0.107) & 0.8(0.107) - 0.4(0.238) \\\\ -0.4(0.556) + 1.5(0.107) & -0.4(0.107) + 1.5(0.238) \\end{pmatrix} $$\n$$ K = \\frac{1}{1.1544} \\begin{pmatrix} 0.4448 - 0.0428 & 0.0856 - 0.0952 \\\\ -0.2224 + 0.1605 & -0.0428 + 0.357 \\end{pmatrix} = \\frac{1}{1.1544} \\begin{pmatrix} 0.402 & -0.0096 \\\\ -0.0619 & 0.3142 \\end{pmatrix} $$\n$$ K \\approx \\begin{pmatrix} 0.348233 & -0.008316 \\\\ -0.053612 & 0.272176 \\end{pmatrix} $$\nThe eigenvalues $\\lambda=\\rho^2$ of $K$ are found by solving the characteristic equation $\\det(K-\\lambda I)=0$:\n$$ (0.348233 - \\lambda)(0.272176 - \\lambda) - (-0.008316)(-0.053612) = 0 $$\n$$ \\lambda^2 - (\\text{tr}(K))\\lambda + \\det(K) = 0 $$\n$$ \\text{tr}(K) \\approx 0.348233 + 0.272176 = 0.620409 $$\n$$ \\det(K) \\approx (0.348233)(0.272176) - (0.0004458) \\approx 0.094781 - 0.000446 = 0.094335 $$\nThe characteristic equation is approximately:\n$$ \\lambda^2 - 0.620409 \\lambda + 0.094335 = 0 $$\nSolving this quadratic equation for $\\lambda$:\n$$ \\lambda = \\frac{0.620409 \\pm \\sqrt{(0.620409)^2 - 4(0.094335)}}{2} = \\frac{0.620409 \\pm \\sqrt{0.384907 - 0.37734}}{2} $$\n$$ \\lambda = \\frac{0.620409 \\pm \\sqrt{0.007567}}{2} = \\frac{0.620409 \\pm 0.086988}{2} $$\nThe two eigenvalues are:\n$$ \\lambda_1 = \\frac{0.620409 + 0.086988}{2} = \\frac{0.707397}{2} \\approx 0.35370 $$\n$$ \\lambda_2 = \\frac{0.620409 - 0.086988}{2} = \\frac{0.533421}{2} \\approx 0.26671 $$\nThe goal is to maximize the correlation, so we take the largest eigenvalue, $\\lambda_1$. The first canonical correlation $\\rho_1$ is its square root:\n$$ \\rho_1 = \\sqrt{\\lambda_1} \\approx \\sqrt{0.35370} \\approx 0.594726 $$\nThe problem requests the value rounded to four significant figures.\n$$ \\rho_1 \\approx 0.5947 $$\nThe associated canonical direction $b_1$ is the eigenvector of $K$ for $\\lambda_1$.\nThe direction vector $a_1$ is then found via $a_1 = \\frac{1}{\\rho_1} \\Sigma_{XX}^{-1} \\Sigma_{XY} b_1$, where $b_1$ and $a_1$ are scaled to satisfy the unit variance constraints. The explicit computation of these vectors, while part of a full analysis, is secondary to finding the correlation value itself as per the problem's final answer requirement.", "answer": "$$\\boxed{0.5947}$$", "id": "4322614"}, {"introduction": "This practice introduces an elegant and computationally stable method for performing CCA through data \"whitening\". By transforming the variables into a space where they are uncorrelated and have unit variance, the CCA problem simplifies to finding the Singular Value Decomposition (SVD) of the transformed cross-covariance matrix. This exercise provides a geometric intuition for CCA and illustrates the procedure used in many modern software packages. [@problem_id:4322598]", "problem": "A systems biomedicine study investigates how gene expression measurements and protein abundance measurements co-vary across a cohort of patients. Let the random vectors be $x \\in \\mathbb{R}^{2}$ (gene expression) and $y \\in \\mathbb{R}^{2}$ (protein abundance), each centered to have zero mean across patients. Assume the following empirically estimated second-order statistics are scientifically plausible and internally consistent:\n- The within-modality covariance matrices are $\\Sigma_{xx} = \\begin{pmatrix}4 & 0 \\\\ 0 & 1\\end{pmatrix}$ and $\\Sigma_{yy} = \\begin{pmatrix}9 & 0 \\\\ 0 & 4\\end{pmatrix}$, which are symmetric and positive definite.\n- The cross-modality covariance matrix is $\\Sigma_{xy} = \\begin{pmatrix}3 & 1 \\\\ \\tfrac{3}{4} & 1\\end{pmatrix}$.\n\nCanonical Correlation Analysis (CCA) seeks linear combinations $a^{\\top} x$ and $b^{\\top} y$ that maximize the correlation under the normalization constraints $a^{\\top} \\Sigma_{xx} a = 1$ and $b^{\\top} \\Sigma_{yy} b = 1$. In multi-view learning, a standard approach to solve this is to whiten each view and analyze the singular values of an appropriately normalized cross-covariance.\n\nStarting from the fundamental definitions of covariance, correlation under linear transformations, and the spectral properties of symmetric positive definite matrices, perform the following tasks:\n- Compute the symmetric inverse square roots $\\Sigma_{xx}^{-1/2}$ and $\\Sigma_{yy}^{-1/2}$.\n- Using these, compute the whitened cross-covariance $C = \\Sigma_{xx}^{-1/2} \\Sigma_{xy} \\Sigma_{yy}^{-1/2}$.\n- Perform the Singular Value Decomposition (SVD), that is, the factorization $C = U \\,\\mathrm{diag}(\\rho_{1}, \\rho_{2})\\, V^{\\top}$, where $U$ and $V$ are orthonormal, and identify the top singular components $(\\rho_{1}, u_{1}, v_{1})$.\n- Reconstruct the top canonical directions $a$ and $b$ in the original variable space such that $a^{\\top} \\Sigma_{xx} a = 1$ and $b^{\\top} \\Sigma_{yy} b = 1$.\n\nReport, as your final answer, the largest canonical correlation $\\rho_{1}$ as a single real number. No rounding is required; provide the exact value with no units.", "solution": "Canonical Correlation Analysis (CCA) seeks to find linear combinations of the variables, $u = a^{\\top}x$ and $v = b^{\\top}y$, that are maximally correlated. The correlation is given by:\n$$\n\\rho = \\frac{\\text{Cov}(u, v)}{\\sqrt{\\text{Var}(u) \\text{Var}(v)}} = \\frac{a^{\\top} \\Sigma_{xy} b}{\\sqrt{(a^{\\top} \\Sigma_{xx} a)(b^{\\top} \\Sigma_{yy} b)}}\n$$\nThe problem specifies normalization constraints $a^{\\top} \\Sigma_{xx} a = 1$ and $b^{\\top} \\Sigma_{yy} b = 1$, which simplifies the objective to maximizing $\\rho = a^{\\top} \\Sigma_{xy} b$.\n\nThis constrained optimization problem can be solved by first \"whitening\" the variables. We define whitened variables $\\tilde{x} = \\Sigma_{xx}^{-1/2} x$ and $\\tilde{y} = \\Sigma_{yy}^{-1/2} y$. The covariance matrices of these new variables are identity matrices:\n$$\n\\text{Cov}(\\tilde{x}) = \\Sigma_{xx}^{-1/2} \\Sigma_{xx} (\\Sigma_{xx}^{-1/2})^{\\top} = \\Sigma_{xx}^{-1/2} \\Sigma_{xx} \\Sigma_{xx}^{-1/2} = I\n$$\n$$\n\\text{Cov}(\\tilde{y}) = \\Sigma_{yy}^{-1/2} \\Sigma_{yy} (\\Sigma_{yy}^{-1/2})^{\\top} = \\Sigma_{yy}^{-1/2} \\Sigma_{yy} \\Sigma_{yy}^{-1/2} = I\n$$\nThe cross-covariance matrix of the whitened variables is:\n$$\nC = \\text{Cov}(\\tilde{x}, \\tilde{y}) = \\Sigma_{xx}^{-1/2} \\Sigma_{xy} (\\Sigma_{yy}^{-1/2})^{\\top} = \\Sigma_{xx}^{-1/2} \\Sigma_{xy} \\Sigma_{yy}^{-1/2}\n$$\nIn terms of the whitened variables, the canonical variables are defined by projection vectors $u$ and $v$ such that $a = \\Sigma_{xx}^{-1/2} u$ and $b = \\Sigma_{yy}^{-1/2} v$. The normalization constraints become:\n$$\na^{\\top} \\Sigma_{xx} a = (\\Sigma_{xx}^{-1/2} u)^{\\top} \\Sigma_{xx} (\\Sigma_{xx}^{-1/2} u) = u^{\\top} \\Sigma_{xx}^{-1/2} \\Sigma_{xx} \\Sigma_{xx}^{-1/2} u = u^{\\top}u = 1\n$$\n$$\nb^{\\top} \\Sigma_{yy} b = (\\Sigma_{yy}^{-1/2} v)^{\\top} \\Sigma_{yy} (\\Sigma_{yy}^{-1/2} v) = v^{\\top} \\Sigma_{yy}^{-1/2} \\Sigma_{yy} \\Sigma_{yy}^{-1/2} v = v^{\\top}v = 1\n$$\nThe correlation to maximize is $\\rho = u^{\\top} C v$, subject to $u^{\\top}u = 1$ and $v^{\\top}v = 1$. The solutions to this problem are given by the Singular Value Decomposition (SVD) of the matrix $C$. The maximum value of $\\rho$ is the largest singular value of $C$.\n\nThe steps are as follows:\n\n1.  **Compute the symmetric inverse square roots $\\Sigma_{xx}^{-1/2}$ and $\\Sigma_{yy}^{-1/2}$.**\n    The given covariance matrices are diagonal:\n    $$\n    \\Sigma_{xx} = \\begin{pmatrix}4 & 0 \\\\ 0 & 1\\end{pmatrix}, \\quad \\Sigma_{yy} = \\begin{pmatrix}9 & 0 \\\\ 0 & 4\\end{pmatrix}\n    $$\n    For a diagonal matrix, the inverse square root is found by taking the inverse square root of each diagonal element.\n    $$\n    \\Sigma_{xx}^{-1/2} = \\begin{pmatrix}4^{-1/2} & 0 \\\\ 0 & 1^{-1/2}\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{2} & 0 \\\\ 0 & 1\\end{pmatrix}\n    $$\n    $$\n    \\Sigma_{yy}^{-1/2} = \\begin{pmatrix}9^{-1/2} & 0 \\\\ 0 & 4^{-1/2}\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{3} & 0 \\\\ 0 & \\frac{1}{2}\\end{pmatrix}\n    $$\n\n2.  **Compute the whitened cross-covariance matrix $C$.**\n    $C = \\Sigma_{xx}^{-1/2} \\Sigma_{xy} \\Sigma_{yy}^{-1/2}$, with $\\Sigma_{xy} = \\begin{pmatrix}3 & 1 \\\\ \\frac{3}{4} & 1\\end{pmatrix}$.\n    $$\n    C = \\begin{pmatrix}\\frac{1}{2} & 0 \\\\ 0 & 1\\end{pmatrix} \\begin{pmatrix}3 & 1 \\\\ \\frac{3}{4} & 1\\end{pmatrix} \\begin{pmatrix}\\frac{1}{3} & 0 \\\\ 0 & \\frac{1}{2}\\end{pmatrix}\n    $$\n    First, we compute the product $\\Sigma_{xx}^{-1/2} \\Sigma_{xy}$:\n    $$\n    \\begin{pmatrix}\\frac{1}{2} & 0 \\\\ 0 & 1\\end{pmatrix} \\begin{pmatrix}3 & 1 \\\\ \\frac{3}{4} & 1\\end{pmatrix} = \\begin{pmatrix}(\\frac{1}{2})(3) & (\\frac{1}{2})(1) \\\\ (1)(\\frac{3}{4}) & (1)(1)\\end{pmatrix} = \\begin{pmatrix}\\frac{3}{2} & \\frac{1}{2} \\\\ \\frac{3}{4} & 1\\end{pmatrix}\n    $$\n    Next, we multiply this result by $\\Sigma_{yy}^{-1/2}$:\n    $$\n    C = \\begin{pmatrix}\\frac{3}{2} & \\frac{1}{2} \\\\ \\frac{3}{4} & 1\\end{pmatrix} \\begin{pmatrix}\\frac{1}{3} & 0 \\\\ 0 & \\frac{1}{2}\\end{pmatrix} = \\begin{pmatrix}(\\frac{3}{2})(\\frac{1}{3}) & (\\frac{1}{2})(\\frac{1}{2}) \\\\ (\\frac{3}{4})(\\frac{1}{3}) & (1)(\\frac{1}{2})\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{2} & \\frac{1}{4} \\\\ \\frac{1}{4} & \\frac{1}{2}\\end{pmatrix}\n    $$\n\n3.  **Find the singular values of $C$.**\n    The canonical correlations are the singular values of $C$. The singular values of a matrix are the square roots of the eigenvalues of $C^{\\top}C$. Since our matrix $C$ is symmetric, its singular values are the absolute values of its eigenvalues. We find the eigenvalues $\\rho$ of $C$ by solving the characteristic equation $\\det(C - \\rho I) = 0$.\n    $$\n    \\det\\begin{pmatrix}\\frac{1}{2} - \\rho & \\frac{1}{4} \\\\ \\frac{1}{4} & \\frac{1}{2} - \\rho\\end{pmatrix} = 0\n    $$\n    $$\n    \\left(\\frac{1}{2} - \\rho\\right)^2 - \\left(\\frac{1}{4}\\right)^2 = 0\n    $$\n    $$\n    \\left(\\frac{1}{2} - \\rho\\right)^2 = \\frac{1}{16}\n    $$\n    Taking the square root of both sides:\n    $$\n    \\frac{1}{2} - \\rho = \\pm\\frac{1}{4}\n    $$\n    This gives two eigenvalues:\n    $$\n    \\rho = \\frac{1}{2} \\mp \\frac{1}{4}\n    $$\n    The eigenvalues are $\\rho_{1} = \\frac{1}{2} + \\frac{1}{4} = \\frac{3}{4}$ and $\\rho_{2} = \\frac{1}{2} - \\frac{1}{4} = \\frac{1}{4}$.\n    Since both are positive, they are also the singular values of $C$. The largest canonical correlation is the largest singular value, $\\rho_{1}$.\n\nTherefore, the largest canonical correlation is $\\frac{3}{4}$. The other steps requested, finding the canonical directions $a$ and $b$, are not required for the final answer but are derived from the singular vectors corresponding to this singular value.", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "4322598"}, {"introduction": "To synthesize your understanding, this final exercise provides a complete, end-to-end scenario starting from raw, hypothetical data. You will perform all the necessary steps: computing means and covariances, setting up and solving the CCA problem, and interpreting the results. Furthermore, this practice prompts you to consider the critical real-world challenges of applying CCA in high-dimensional settings, where the number of samples is small relative to the number of features. [@problem_id:4322590]", "problem": "A systems biomedicine study collects two omics views on the same cohort of $n$ patients. Let $X \\in \\mathbb{R}^{n \\times p}$ denote transcript abundance for $p$ genes and $Y \\in \\mathbb{R}^{n \\times q}$ denote metabolite intensities for $q$ metabolites, both measured on the same $n$ patients. Consider the following empirical data from $n = 4$ patients with $p = 2$ and $q = 2$:\n$$\nX \\;=\\; \\begin{pmatrix}\n2 & 0 \\\\\n0 & 2 \\\\\n2 & 2 \\\\\n4 & 2\n\\end{pmatrix},\n\\qquad\nY \\;=\\; \\begin{pmatrix}\n1 & 1 \\\\\n1 & 3 \\\\\n2 & 3 \\\\\n3 & 4\n\\end{pmatrix}.\n$$\nStarting from first principles (that is, the definitions of empirical mean and covariance as sample analogs of population mean and covariance), do the following:\n1) Compute the column-wise empirical means for $X$ and $Y$ and form the column-centered data matrices. Using the maximum-likelihood normalization for a multivariate normal model with unknown mean, compute the empirical covariances $\\hat{\\Sigma}_{XX} \\in \\mathbb{R}^{p \\times p}$, $\\hat{\\Sigma}_{YY} \\in \\mathbb{R}^{q \\times q}$, and the empirical cross-covariance $\\hat{\\Sigma}_{XY} \\in \\mathbb{R}^{p \\times q}$, each defined with a factor of $1/n$.\n2) Briefly discuss, in the context of multi-view learning for systems biomedicine, how the relationship of $n$ to $p$ and $q$ affects bias and conditioning of these estimators, and what principled remedies are commonly adopted when $n$ is not large relative to $p$ or $q$.\n3) Using only the variational definition of Canonical Correlation Analysis (CCA)—namely, maximizing the empirical correlation between $a^{\\top}X$ and $b^{\\top}Y$ over linear combinations $a \\in \\mathbb{R}^{p}$ and $b \\in \\mathbb{R}^{q}$ subject to unit empirical variance constraints computed from the $1/n$-normalized covariances—derive the algebraic condition that determines the canonical correlations and then evaluate the largest canonical correlation for the given $X$ and $Y$.\n\nGive your final numeric answer to part $3$ rounded to four significant figures. No physical units are required, and angles are not involved.", "solution": "We proceed by addressing the three parts of the problem in sequence.\n\nThe given data matrices are $X \\in \\mathbb{R}^{n \\times p}$ and $Y \\in \\mathbb{R}^{n \\times q}$, with $n=4$, $p=2$, and $q=2$.\n$$\nX \\;=\\; \\begin{pmatrix}\n2 & 0 \\\\\n0 & 2 \\\\\n2 & 2 \\\\\n4 & 2\n\\end{pmatrix},\n\\qquad\nY \\;=\\; \\begin{pmatrix}\n1 & 1 \\\\\n1 & 3 \\\\\n2 & 3 \\\\\n3 & 4\n\\end{pmatrix}.\n$$\n\nPart 1: Computation of Empirical Means and Covariances\n\nFirst, we compute the column-wise empirical means, $\\bar{x} \\in \\mathbb{R}^{p}$ and $\\bar{y} \\in \\mathbb{R}^{q}$.\nFor matrix $X$, the mean vector $\\bar{x}$ has components:\n$$ \\bar{x}_1 = \\frac{1}{4}(2+0+2+4) = \\frac{8}{4} = 2 $$\n$$ \\bar{x}_2 = \\frac{1}{4}(0+2+2+2) = \\frac{6}{4} = \\frac{3}{2} = 1.5 $$\nSo, $\\bar{x} = \\begin{pmatrix} 2 \\\\ 1.5 \\end{pmatrix}$.\n\nFor matrix $Y$, the mean vector $\\bar{y}$ has components:\n$$ \\bar{y}_1 = \\frac{1}{4}(1+1+2+3) = \\frac{7}{4} = 1.75 $$\n$$ \\bar{y}_2 = \\frac{1}{4}(1+3+3+4) = \\frac{11}{4} = 2.75 $$\nSo, $\\bar{y} = \\begin{pmatrix} 1.75 \\\\ 2.75 \\end{pmatrix}$.\n\nNext, we form the column-centered data matrices, $X_c = X - \\mathbf{1}\\bar{x}^{\\top}$ and $Y_c = Y - \\mathbf{1}\\bar{y}^{\\top}$, where $\\mathbf{1}$ is a column vector of $n=4$ ones.\n$$\nX_c = \\begin{pmatrix}\n2-2 & 0-1.5 \\\\\n0-2 & 2-1.5 \\\\\n2-2 & 2-1.5 \\\\\n4-2 & 2-1.5\n\\end{pmatrix}\n= \\begin{pmatrix}\n0 & -1.5 \\\\\n-2 & 0.5 \\\\\n0 & 0.5 \\\\\n2 & 0.5\n\\end{pmatrix}\n$$\n$$\nY_c = \\begin{pmatrix}\n1-1.75 & 1-2.75 \\\\\n1-1.75 & 3-2.75 \\\\\n2-1.75 & 3-2.75 \\\\\n3-1.75 & 4-2.75\n\\end{pmatrix}\n= \\begin{pmatrix}\n-0.75 & -1.75 \\\\\n-0.75 & 0.25 \\\\\n0.25 & 0.25 \\\\\n1.25 & 1.25\n\\end{pmatrix}\n$$\nNow, we compute the empirical covariance matrices using the specified maximum-likelihood normalization factor of $1/n$.\nThe empirical covariance for $X$ is $\\hat{\\Sigma}_{XX} = \\frac{1}{n} X_c^{\\top}X_c$:\n$$\n\\hat{\\Sigma}_{XX} = \\frac{1}{4} \\begin{pmatrix} 0 & -2 & 0 & 2 \\\\ -1.5 & 0.5 & 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} 0 & -1.5 \\\\ -2 & 0.5 \\\\ 0 & 0.5 \\\\ 2 & 0.5 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 8 & 0 \\\\ 0 & 3 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 0.75 \\end{pmatrix}\n$$\nThe empirical covariance for $Y$ is $\\hat{\\Sigma}_{YY} = \\frac{1}{n} Y_c^{\\top}Y_c$:\n$$\n\\hat{\\Sigma}_{YY} = \\frac{1}{4} \\begin{pmatrix} -0.75 & -0.75 & 0.25 & 1.25 \\\\ -1.75 & 0.25 & 0.25 & 1.25 \\end{pmatrix} \\begin{pmatrix} -0.75 & -1.75 \\\\ -0.75 & 0.25 \\\\ 0.25 & 0.25 \\\\ 1.25 & 1.25 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 2.75 & 2.75 \\\\ 2.75 & 4.75 \\end{pmatrix} = \\begin{pmatrix} 11/16 & 11/16 \\\\ 11/16 & 19/16 \\end{pmatrix}\n$$\nThe empirical cross-covariance is $\\hat{\\Sigma}_{XY} = \\frac{1}{n} X_c^{\\top}Y_c$:\n$$\n\\hat{\\Sigma}_{XY} = \\frac{1}{4} \\begin{pmatrix} 0 & -2 & 0 & 2 \\\\ -1.5 & 0.5 & 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} -0.75 & -1.75 \\\\ -0.75 & 0.25 \\\\ 0.25 & 0.25 \\\\ 1.25 & 1.25 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 4 & 2 \\\\ 1.5 & 3.5 \\end{pmatrix} = \\begin{pmatrix} 1 & 0.5 \\\\ 0.375 & 0.875 \\end{pmatrix}\n$$\n\nPart 2: Discussion of Estimator Properties\n\nIn multi-view learning applications like systems biomedicine, it is common to encounter a \"high-dimension, low-sample-size\" setting where the number of features ($p$ and $q$) vastly exceeds the number of samples ($n$), i.e., $p \\gg n$ and $q \\gg n$. This scenario severely affects the properties of the empirical covariance estimators.\n\n1.  Bias and Variance: The MLE estimator for covariance (with $1/n$ normalization) is known to be biased, underestimating the true variance. While the unbiased estimator (with $1/(n-1)$ normalization) corrects this, the difference is marginal for large $n$. In high-dimensional settings, the dominant problem is the high variance of the estimator. The sample eigenvalues are more dispersed than the population eigenvalues, leading to instability and poor out-of-sample performance.\n\n2.  Conditioning and Singularity: When $p > n-1$ or $q > n-1$, the centered data matrix $X_c$ or $Y_c$ will be rank-deficient (its column rank is at most $n-1$). Consequently, the empirical covariance matrices $\\hat{\\Sigma}_{XX} = \\frac{1}{n} X_c^{\\top}X_c$ and $\\hat{\\Sigma}_{YY} = \\frac{1}{n} Y_c^{\\top}Y_c$ become singular (non-invertible). This is catastrophic for classical CCA, as its solution requires the inversion of these matrices.\n\nPrincipled remedies are necessary to address these issues:\n- Regularization: The most common approach is Regularized CCA (RCCA). A multiple of the identity matrix is added to the empirical covariance matrices, i.e., $\\hat{\\Sigma}_{XX} + \\lambda_x I$ and $\\hat{\\Sigma}_{YY} + \\lambda_y I$. This Tikhonov regularization makes the matrices positive definite and invertible, improving their conditioning. The regularization parameters $\\lambda_x, \\lambda_y > 0$ are typically chosen via cross-validation. This introduces bias but reduces variance, leading to a better overall model.\n- Sparsity: In high dimensions, it is often assumed that only a subset of features is relevant. Sparse CCA methods impose penalties (e.g., L1-norm) on the canonical vectors $a$ and $b$. This encourages sparsity (many zero elements), effectively performing feature selection. This not only regularizes the problem but also enhances interpretability, a crucial aspect in systems biomedicine for identifying key biomarkers.\n\nPart 3: Derivation and Evaluation of the Largest Canonical Correlation\n\nCanonical Correlation Analysis (CCA) seeks linear combinations of the variables in $X$ and $Y$ that are maximally correlated. Let the canonical variates be $u = X_c a$ and $v = Y_c b$, where $a \\in \\mathbb{R}^p$ and $b \\in \\mathbb{R}^q$ are the canonical weight vectors. The variational problem is to maximize the correlation $\\rho = \\text{corr}(u,v)$ subject to normalization constraints.\nThe correlation is defined as:\n$$ \\rho = \\frac{\\text{cov}(u, v)}{\\sqrt{\\text{var}(u) \\text{var}(v)}} = \\frac{a^{\\top}\\hat{\\Sigma}_{XY}b}{\\sqrt{(a^{\\top}\\hat{\\Sigma}_{XX}a)(b^{\\top}\\hat{\\Sigma}_{YY}b)}} $$\nThe optimization problem is formulated as maximizing the covariance $a^{\\top}\\hat{\\Sigma}_{XY}b$ subject to unit variance constraints:\n$$ \\max_{a,b} a^{\\top}\\hat{\\Sigma}_{XY}b \\quad \\text{subject to} \\quad a^{\\top}\\hat{\\Sigma}_{XX}a = 1 \\text{ and } b^{\\top}\\hat{\\Sigma}_{YY}b = 1 $$\nWe solve this using Lagrange multipliers. The Lagrangian is:\n$$ \\mathcal{L}(a, b, \\lambda_a, \\lambda_b) = a^{\\top}\\hat{\\Sigma}_{XY}b - \\frac{\\lambda_a}{2}(a^{\\top}\\hat{\\Sigma}_{XX}a - 1) - \\frac{\\lambda_b}{2}(b^{\\top}\\hat{\\Sigma}_{YY}b - 1) $$\nTaking derivatives and setting them to zero yields the coupled equations:\n$$ \\frac{\\partial\\mathcal{L}}{\\partial a} = \\hat{\\Sigma}_{XY}b - \\lambda_a \\hat{\\Sigma}_{XX}a = 0 \\implies \\hat{\\Sigma}_{XY}b = \\lambda_a \\hat{\\Sigma}_{XX}a $$\n$$ \\frac{\\partial\\mathcal{L}}{\\partial b} = \\hat{\\Sigma}_{YX}a - \\lambda_b \\hat{\\Sigma}_{YY}b = 0 \\implies \\hat{\\Sigma}_{YX}a = \\lambda_b \\hat{\\Sigma}_{YY}b $$\nwhere $\\hat{\\Sigma}_{YX} = \\hat{\\Sigma}_{XY}^{\\top}$. Left-multiplying the first equation by $a^{\\top}$ and the second by $b^{\\top}$ and applying the constraints shows that $\\lambda_a = \\lambda_b = a^{\\top}\\hat{\\Sigma}_{XY}b = \\rho$. The canonical correlation $\\rho$ is the Lagrange multiplier.\n\nTo derive the algebraic condition, we solve for $a$ and $b$. Assuming $\\hat{\\Sigma}_{XX}$ and $\\hat{\\Sigma}_{YY}$ are invertible (which they are for this problem, as $n-1 > p,q$):\n$$ a = \\frac{1}{\\rho}\\hat{\\Sigma}_{XX}^{-1}\\hat{\\Sigma}_{XY}b $$\nSubstituting this into the second equation:\n$$ \\hat{\\Sigma}_{YX}\\left(\\frac{1}{\\rho}\\hat{\\Sigma}_{XX}^{-1}\\hat{\\Sigma}_{XY}b\\right) = \\rho\\hat{\\Sigma}_{YY}b $$\n$$ \\hat{\\Sigma}_{YX}\\hat{\\Sigma}_{XX}^{-1}\\hat{\\Sigma}_{XY}b = \\rho^2\\hat{\\Sigma}_{YY}b $$\n$$ (\\hat{\\Sigma}_{YY}^{-1}\\hat{\\Sigma}_{YX}\\hat{\\Sigma}_{XX}^{-1}\\hat{\\Sigma}_{XY})b = \\rho^2 b $$\nThis is a standard eigenvalue problem. The squared canonical correlations $\\rho^2$ are the eigenvalues of the matrix $K = \\hat{\\Sigma}_{YY}^{-1}\\hat{\\Sigma}_{YX}\\hat{\\Sigma}_{XX}^{-1}\\hat{\\Sigma}_{XY}$. The largest canonical correlation is the square root of the largest eigenvalue of $K$. An equivalent problem finds $\\rho^2$ as the eigenvalues of $K' = \\hat{\\Sigma}_{XX}^{-1}\\hat{\\Sigma}_{XY}\\hat{\\Sigma}_{YY}^{-1}\\hat{\\Sigma}_{YX}$.\n\nWe now compute the largest canonical correlation for the given data. We use the matrix $K'$:\n$$ \\hat{\\Sigma}_{XX}^{-1} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3/4 \\end{pmatrix}^{-1} = \\begin{pmatrix} 1/2 & 0 \\\\ 0 & 4/3 \\end{pmatrix} $$\n$$ \\det(\\hat{\\Sigma}_{YY}) = (\\frac{11}{16})(\\frac{19}{16}) - (\\frac{11}{16})^2 = \\frac{11}{16}\\frac{8}{16} = \\frac{11}{32} $$\n$$ \\hat{\\Sigma}_{YY}^{-1} = \\frac{32}{11} \\begin{pmatrix} 19/16 & -11/16 \\\\ -11/16 & 11/16 \\end{pmatrix} = \\frac{2}{11} \\begin{pmatrix} 19 & -11 \\\\ -11 & 11 \\end{pmatrix} $$\nWe compute $K' = (\\hat{\\Sigma}_{XX}^{-1} \\hat{\\Sigma}_{XY}) (\\hat{\\Sigma}_{YY}^{-1} \\hat{\\Sigma}_{YX})$:\n$$ \\hat{\\Sigma}_{XX}^{-1} \\hat{\\Sigma}_{XY} = \\begin{pmatrix} 1/2 & 0 \\\\ 0 & 4/3 \\end{pmatrix} \\begin{pmatrix} 1 & 1/2 \\\\ 3/8 & 7/8 \\end{pmatrix} = \\begin{pmatrix} 1/2 & 1/4 \\\\ 1/2 & 7/6 \\end{pmatrix} $$\n$$ \\hat{\\Sigma}_{YY}^{-1} \\hat{\\Sigma}_{YX} = \\frac{2}{11} \\begin{pmatrix} 19 & -11 \\\\ -11 & 11 \\end{pmatrix} \\begin{pmatrix} 1 & 3/8 \\\\ 1/2 & 7/8 \\end{pmatrix} = \\frac{1}{11} \\begin{pmatrix} 27 & -5 \\\\ -11 & 11 \\end{pmatrix} $$\n$$ K' = \\begin{pmatrix} 1/2 & 1/4 \\\\ 1/2 & 7/6 \\end{pmatrix} \\frac{1}{11} \\begin{pmatrix} 27 & -5 \\\\ -11 & 11 \\end{pmatrix} = \\frac{1}{11} \\begin{pmatrix} 27/2-11/4 & -5/2+11/4 \\\\ 27/2-77/6 & -5/2+77/6 \\end{pmatrix} $$\n$$ K' = \\frac{1}{11} \\begin{pmatrix} 43/4 & 1/4 \\\\ 4/6 & 62/6 \\end{pmatrix} = \\frac{1}{11} \\begin{pmatrix} 43/4 & 1/4 \\\\ 2/3 & 31/3 \\end{pmatrix} = \\begin{pmatrix} 43/44 & 1/44 \\\\ 2/33 & 31/33 \\end{pmatrix} $$\nThe eigenvalues $\\lambda = \\rho^2$ of $K'$ are the roots of the characteristic equation $\\det(K' - \\lambda I) = 0$:\n$$ ( \\frac{43}{44} - \\lambda ) ( \\frac{31}{33} - \\lambda ) - ( \\frac{1}{44} ) ( \\frac{2}{33} ) = 0 $$\n$$ \\lambda^2 - (\\frac{43}{44}+\\frac{31}{33})\\lambda + \\frac{43 \\cdot 31 - 2}{44 \\cdot 33} = 0 $$\nThe trace is $\\text{tr}(K') = \\frac{129+124}{132} = \\frac{253}{132} = \\frac{23 \\cdot 11}{12 \\cdot 11} = \\frac{23}{12}$.\nThe determinant is $\\det(K') = \\frac{1333-2}{1452} = \\frac{1331}{1452} = \\frac{11^3}{12 \\cdot 11^2} = \\frac{11}{12}$.\nThe characteristic equation is $\\lambda^2 - \\frac{23}{12}\\lambda + \\frac{11}{12} = 0$, which is $12\\lambda^2 - 23\\lambda + 11 = 0$.\nFactoring this quadratic equation: $(12\\lambda - 11)(\\lambda - 1) = 0$.\nThe roots are $\\lambda_1 = 1$ and $\\lambda_2 = 11/12$.\nThe eigenvalues are the squared canonical correlations, $\\rho^2$. The largest eigenvalue is $\\lambda_{\\max} = 1$. The largest canonical correlation is thus $\\rho_{\\max} = \\sqrt{1} = 1$.\nThis result is expected. A canonical correlation of $1$ is guaranteed if $p+q > n-1$. In this problem, $p+q = 2+2=4$ and $n-1 = 4-1=3$. Since $4>3$, at least one perfect correlation is guaranteed.\nThe value rounded to four significant figures is $1.000$.", "answer": "$$\\boxed{1.000}$$", "id": "4322590"}]}