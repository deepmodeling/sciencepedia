{"hands_on_practices": [{"introduction": "The success of a pooled CRISPR screen hinges on careful experimental design, particularly in controlling the delivery of the sgRNA library into the target cells. To ensure that the observed phenotype of a cell is attributable to a single genetic perturbation, it is crucial to minimize the number of cells that receive multiple sgRNAs. This practice delves into the statistical principles governing lentiviral transduction, allowing you to derive the distribution of integration events per cell and understand how the multiplicity of infection ($MOI$) dictates the balance between transduction efficiency and single-integration fidelity [@problem_id:4344697].", "problem": "In a pooled Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) perturbation screen, mammalian cells are transduced with a lentiviral library such that the number of successful integration events per cell is governed by many independent trials with a small per-trial success probability and a finite mean number of successes per cell. The experimental multiplicity of infection (MOI) is defined as the expected number of successful integration events per cell and is denoted by $\\lambda$. Assuming independence of infection events across virions and cells, and that each cell is exposed to a large number of virions with small per-virion infection probability such that the expected number of successful infections per cell remains equal to $\\lambda$, derive from first principles the analytic expressions for the fractions of cells that are uninfected (exactly $0$ integrations), singly infected (exactly $1$ integration), and multiply infected (at least $2$ integrations) in terms of $\\lambda$. Report your final answer as a single row vector containing the three expressions in the order uninfected, singly infected, and multiply infected. Provide the analytical expressions only; no numerical evaluation or rounding is required, and no units should be included in your final expression.", "solution": "The user has provided a problem statement that requires validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- **Context**: A pooled Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) perturbation screen using a lentiviral library to transduce mammalian cells.\n- **Process Description**: The number of successful integration events per cell is governed by many independent trials, each with a small per-trial success probability.\n- **Variable Definition**: The experimental multiplicity of infection (MOI), denoted by $\\lambda$, is the expected number of successful integration events per cell.\n- **Assumptions**:\n    1.  Independence of infection events across virions and cells.\n    2.  Each cell is exposed to a large number of virions.\n    3.  Each virion has a small per-virion infection probability.\n    4.  The expected number of successful infections per cell is fixed at $\\lambda$.\n- **Objective**: Derive from first principles the analytic expressions for the fractions of cells that are:\n    1.  Uninfected (exactly $0$ integrations).\n    2.  Singly infected (exactly $1$ integration).\n    3.  Multiply infected (at least $2$ integrations).\n- **Required Output Format**: A single row vector containing the three expressions in terms of $\\lambda$, in the specified order.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is subjected to critical evaluation.\n\n- **Scientific Grounding**: The problem is well-grounded in the principles of virology and molecular biology genetics. The use of lentiviral vectors for gene delivery, the concept of Multiplicity of Infection (MOI), and the statistical modeling of transduction events are standard and fundamental concepts in the field of systems biomedicine and functional genomics. The description of the infection process as \"many independent trials with a small per-trial success probability\" is the classic physical motivation for the Poisson distribution.\n- **Well-Posedness**: The problem is well-posed. It defines a random process, specifies its key parameters and limiting behaviors, and asks for the calculation of specific probabilities from the resulting distribution. The question is unambiguous and admits a unique, stable, and meaningful analytical solution based on the provided information.\n- **Objectivity**: The problem is stated in objective, formal language. There are no subjective or opinion-based claims.\n- **Flaw Analysis**:\n    1.  **Scientific/Factual Unsoundness**: None. The described model is a standard and accurate representation of lentiviral transduction at a given MOI.\n    2.  **Non-Formalizable/Irrelevant**: None. The problem is directly formalizable into a standard probability theory framework and is highly relevant to its specified topic.\n    3.  **Incomplete/Contradictory Setup**: None. The problem provides sufficient constraints (large number of virions, small infection probability, fixed mean $\\lambda$) to uniquely specify the governing probability distribution.\n    4.  **Unrealistic/Infeasible**: None. The scenario describes a routine experimental procedure in biomedical research.\n    5.  **Ill-Posed/Poorly Structured**: None. The objective is clearly stated, and the path to the solution is logically derivable from the premises.\n    6.  **Pseudo-Profound/Trivial**: None. While the result is a well-known formula, the demand to \"derive from first principles\" requires a rigorous demonstration of the Poisson limit of the binomial distribution, which is a substantive reasoning exercise.\n    7.  **Outside Scientific Verifiability**: None. The model's predictions can be and are regularly compared against experimental data from CRISPR screens.\n\n### Step 3: Verdict and Action\nThe problem is **valid** as it is scientifically sound, well-posed, and free from any of the specified flaws. I will proceed with a full derivation of the solution.\n\nThe problem asks for a derivation from first principles. The description of the transduction process—a large number of independent trials ($N$, the number of virions a cell is exposed to), each with a small probability of success ($p$, the probability of a single virion successfully integrating)—suggests that the number of successful integrations per cell follows a binomial distribution. Let the random variable $K$ denote the number of successful integration events in a single cell.\n\nUnder the binomial model, the probability of observing exactly $k$ successes in $N$ trials is given by the probability mass function (PMF):\n$$P(K=k) = \\binom{N}{k} p^k (1-p)^{N-k}$$\nThe problem states that the mean number of integrations is the multiplicity of infection, $\\lambda$. For a binomial distribution, the mean is $E[K] = Np$. Thus, we have the constraint:\n$$\\lambda = Np$$\nThe problem also specifies that $N$ is large and $p$ is small. This is the precise condition under which the binomial distribution converges to the Poisson distribution. We can demonstrate this by taking the limit as $N \\to \\infty$ while holding $\\lambda = Np$ constant.\n\nFrom $\\lambda = Np$, we can express $p$ as $p = \\frac{\\lambda}{N}$. Substituting this into the binomial PMF:\n$$P(K=k) = \\binom{N}{k} \\left(\\frac{\\lambda}{N}\\right)^k \\left(1-\\frac{\\lambda}{N}\\right)^{N-k}$$\nExpanding the binomial coefficient $\\binom{N}{k} = \\frac{N!}{k!(N-k)!} = \\frac{N(N-1)(N-2)\\cdots(N-k+1)}{k!}$:\n$$P(K=k) = \\frac{N(N-1)\\cdots(N-k+1)}{k!} \\frac{\\lambda^k}{N^k} \\left(1-\\frac{\\lambda}{N}\\right)^{N-k}$$\nWe can rearrange this expression:\n$$P(K=k) = \\frac{\\lambda^k}{k!} \\left[\\frac{N(N-1)\\cdots(N-k+1)}{N^k}\\right] \\left(1-\\frac{\\lambda}{N}\\right)^N \\left(1-\\frac{\\lambda}{N}\\right)^{-k}$$\nNow, we evaluate the limit of each part as $N \\to \\infty$:\n1.  The term $\\frac{N(N-1)\\cdots(N-k+1)}{N^k}$ is a ratio of polynomials in $N$ of degree $k$.\n    $$\\lim_{N\\to\\infty} \\frac{N(N-1)\\cdots(N-k+1)}{N^k} = \\lim_{N\\to\\infty} \\left(\\frac{N}{N}\\right)\\left(\\frac{N-1}{N}\\right)\\cdots\\left(\\frac{N-k+1}{N}\\right) = \\lim_{N\\to\\infty} (1)\\left(1-\\frac{1}{N}\\right)\\cdots\\left(1-\\frac{k-1}{N}\\right) = 1$$\n2.  The term $\\left(1-\\frac{\\lambda}{N}\\right)^N$ has a well-known limit which defines the exponential function:\n    $$\\lim_{N\\to\\infty} \\left(1-\\frac{\\lambda}{N}\\right)^N = \\exp(-\\lambda)$$\n3.  For a fixed value of $k$, the last term becomes:\n    $$\\lim_{N\\to\\infty} \\left(1-\\frac{\\lambda}{N}\\right)^{-k} = (1-0)^{-k} = 1$$\nCombining these results, we find the limiting probability:\n$$\\lim_{N\\to\\infty} P(K=k) = \\frac{\\lambda^k}{k!} \\cdot 1 \\cdot \\exp(-\\lambda) \\cdot 1 = \\frac{\\lambda^k \\exp(-\\lambda)}{k!}$$\nThis is the PMF of the Poisson distribution with mean $\\lambda$. Thus, the number of integration events per cell, $K$, is described by $K \\sim \\text{Pois}(\\lambda)$. The fraction of cells with exactly $k$ integrations is given by $P(K=k)$.\n\nWe can now calculate the required fractions:\n1.  **Fraction of uninfected cells**: This corresponds to $k=0$ integrations.\n    $$P(K=0) = \\frac{\\lambda^0 \\exp(-\\lambda)}{0!} = \\frac{1 \\cdot \\exp(-\\lambda)}{1} = \\exp(-\\lambda)$$\n2.  **Fraction of singly infected cells**: This corresponds to $k=1$ integration.\n    $$P(K=1) = \\frac{\\lambda^1 \\exp(-\\lambda)}{1!} = \\frac{\\lambda \\exp(-\\lambda)}{1} = \\lambda \\exp(-\\lambda)$$\n3.  **Fraction of multiply infected cells**: This corresponds to $k \\geq 2$ integrations. The probability is $P(K \\geq 2)$. It is more convenient to calculate this as the complement of the cells having $0$ or $1$ integration.\n    $$P(K \\geq 2) = 1 - P(K < 2) = 1 - \\left(P(K=0) + P(K=1)\\right)$$\n    Substituting the expressions we found:\n    $$P(K \\geq 2) = 1 - \\left(\\exp(-\\lambda) + \\lambda \\exp(-\\lambda)\\right) = 1 - (1+\\lambda)\\exp(-\\lambda)$$\n\nThe three requested fractions are $\\exp(-\\lambda)$, $\\lambda \\exp(-\\lambda)$, and $1 - (1+\\lambda)\\exp(-\\lambda)$. These are to be reported as a single row vector.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\exp(-\\lambda) & \\lambda \\exp(-\\lambda) & 1 - (1+\\lambda)\\exp(-\\lambda)\n\\end{pmatrix}\n}\n$$", "id": "4344697"}, {"introduction": "After conducting the screen and sequencing the sgRNA populations, the next challenge is to accurately model the resulting count data to identify functional effects. While simple counting events can often be described by a Poisson distribution, the reality of complex biological experiments introduces extra variability, or 'overdispersion', that violates the Poisson assumption. This exercise guides you through the rationale for using the more flexible Negative Binomial distribution, exploring its statistical derivation as a Poisson-Gamma mixture and its implementation within a generalized linear model to robustly analyze screen data [@problem_id:4344572].", "problem": "A pooled Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) loss-of-function screen is performed with many single-guide RNA (sgRNA) constructs across biological replicates. For replicate $i$ and sgRNA (targeting a gene or control) $g$, the observed sequencing count is denoted $Y_{ig}$. Assume the following fundamental base: sequencing counts arise from summing many independent, rare sampling events, for which a homogeneous rate model yields a Poisson law with equality of mean and variance, and unobserved heterogeneity in the effective rate across cells and technical steps (for example, differences in editing efficiency, growth, and library preparation) inflates variance relative to the Poisson law. Let $s_i$ denote a known sequencing depth or library size normalization factor for replicate $i$, $z_i \\in \\{0,1\\}$ an indicator of a treatment condition, and let the baseline abundance and treatment effect for sgRNA $g$ be latent log-scale parameters.\n\nWhich option correctly specifies a negative binomial model for $Y_{ig}$ that (i) gives a valid probability mass function for the marginal count distribution, (ii) uses an appropriate generalized linear model (GLM) link with an offset for $s_i$, (iii) yields a mean-variance relationship that captures overdispersion relative to the Poisson law, and (iv) provides a mechanistic explanation of overdispersion from biological and technical variability starting from the Poisson base?\n\nA. For each $i,g$, let the conditional sampling model be $Y_{ig}\\mid \\Lambda_{ig} \\sim \\mathrm{Poisson}(\\Lambda_{ig})$ with a Gamma-distributed random rate $\\Lambda_{ig} \\sim \\mathrm{Gamma}(k, k/\\mu_{ig})$ (shape-rate parameterization), where $k&gt;0$. Then marginally $Y_{ig} \\sim \\mathrm{NB}(k,\\mu_{ig})$ with\n$$\n\\mathbb{P}(Y_{ig}=y) \\;=\\; \\frac{\\Gamma(y+k)}{\\Gamma(k)\\,y!}\\left(\\frac{k}{k+\\mu_{ig}}\\right)^{k}\\left(\\frac{\\mu_{ig}}{k+\\mu_{ig}}\\right)^{y},\\quad y\\in \\{0,1,2,\\dots\\},\n$$\n$\\mathbb{E}[Y_{ig}]=\\mu_{ig}$ and $\\mathrm{Var}(Y_{ig})=\\mu_{ig}+\\mu_{ig}^{2}/k=\\mu_{ig}+\\alpha\\,\\mu_{ig}^{2}$ with $\\alpha=1/k$. Use a log link with sequencing-depth offset,\n$$\n\\log \\mu_{ig} \\;=\\; \\log s_i \\;+\\; \\alpha_g \\;+\\; \\beta_g\\, z_i,\n$$\nwhere $\\alpha_g$ is a baseline log-abundance and $\\beta_g$ is a condition effect for sgRNA $g$. Overdispersion relative to $\\mathrm{Poisson}$ arises because unobserved heterogeneity in the effective rate (for example, variable editing efficiency, differential growth, and polymerase chain reaction amplification) induces a Gamma distribution for $\\Lambda_{ig}$, and the Poisson-Gamma mixture yields the negative binomial law.\n\nB. Model $Y_{ig} \\sim \\mathrm{Poisson}(\\mu_{ig})$ with a log link $\\log \\mu_{ig}=\\log s_i+\\alpha_g+\\beta_g z_i$ and assume $\\mathrm{Var}(Y_{ig})=\\phi\\,\\mu_{ig}$ for a constant $\\phi&gt;1$ to account for overdispersion. Overdispersion is due solely to polymerase chain reaction amplification, which rescales the Poisson variance linearly in $\\mu_{ig}$.\n\nC. Model $Y_{ig} \\sim \\mathrm{NB}(r,p_{ig})$ with identity link $\\mu_{ig}=s_i+\\alpha_g+\\beta_g z_i$ and variance $\\mathrm{Var}(Y_{ig})=\\mu_{ig}+\\alpha\\,\\mu_{ig}$ for some $\\alpha&gt;0$. Explain overdispersion by assuming a Beta-Poisson mixture for the rate.\n\nD. Use a lognormal-Poisson mixture: $Y_{ig}\\mid \\eta_{ig}\\sim \\mathrm{Poisson}(\\mu_{ig} e^{\\eta_{ig}})$ with $\\eta_{ig}\\sim \\mathcal{N}(0,\\sigma^{2})$ and $\\log \\mu_{ig}=\\log s_i+\\alpha_g+\\beta_g z_i$. State that this mixture is equivalent to a negative binomial with $\\mathrm{Var}(Y_{ig})=\\mu_{ig}+\\alpha\\,\\mu_{ig}^{2}$ and that overdispersion arises from multiplicative noise.\n\nE. Use a zero-inflated Poisson model with $\\mathbb{P}(Y_{ig}=0)=\\pi+(1-\\pi)e^{-\\mu_{ig}}$, $\\mathbb{P}(Y_{ig}=y&gt;0)=(1-\\pi)e^{-\\mu_{ig}}\\mu_{ig}^{y}/y!$, $\\log \\mu_{ig}=\\log s_i+\\alpha_g+\\beta_g z_i$, and variance $\\mathrm{Var}(Y_{ig})=\\mu_{ig}(1-\\pi)$. Explain overdispersion by the excess zeros caused by non-cutting guides.", "solution": "The user requires a critical validation of the problem statement, followed by a detailed derivation and evaluation of the provided options for modeling count data from a CRISPR screen.\n\n### Step 1: Extract Givens\n\n-   **Experiment**: Pooled CRISPR loss-of-function screen.\n-   **Data**: Sequencing counts, $Y_{ig}$, for replicate $i$ and sgRNA $g$.\n-   **Covariates**:\n    -   $s_i$: a known sequencing depth/library size normalization factor for replicate $i$.\n    -   $z_i \\in \\{0,1\\}$: an indicator for a treatment condition for replicate $i$.\n-   **Parameters**: Latent log-scale parameters for baseline abundance ($\\alpha_g$) and treatment effect ($\\beta_g$) for sgRNA $g$.\n-   **Fundamental Base Model**:\n    1.  Sequencing counts are sums of many independent, rare sampling events, justifying a base Poisson law.\n    2.  Unobserved heterogeneity in the effective rate (due to factors like editing efficiency, growth, library preparation) inflates the variance relative to the Poisson model. This is the source of overdispersion.\n-   **Task**: Identify the option that correctly specifies a negative binomial model for $Y_{ig}$ satisfying four criteria:\n    1.  Provides a valid probability mass function (PMF) for the marginal count distribution.\n    2.  Uses an appropriate generalized linear model (GLM) link function with an offset for $s_i$.\n    3.  Yields a mean-variance relationship that captures overdispersion.\n    4.  Provides a mechanistic explanation of overdispersion consistent with the fundamental base model.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement describes a standard scenario in functional genomics and computational biology: the analysis of overdispersed count data from a high-throughput sequencing experiment.\n\n-   **Scientifically Grounded**: The problem is firmly rooted in the established principles of statistical genomics. The use of CRISPR screens, the generation of sequencing counts, and the challenges of normalization and overdispersion are all central to the field. The statistical concepts—Poisson distribution as a baseline for counts, overdispersion, the negative binomial (NB) distribution as a model for overdispersed counts, and Generalized Linear Models (GLMs) with log links and offsets—are state-of-the-art and textbook methods for this type of data analysis (e.g., as implemented in widely used software like DESeq2 and edgeR).\n-   **Well-Posed**: The question is clearly defined. It requests the identification of a correct model specification based on four precise and verifiable technical criteria. There is no ambiguity in what constitutes a correct answer.\n-   **Objective**: The problem is stated in precise, quantitative, and objective language, free of any subjective or non-scientific content.\n\nThe problem does not violate any of the invalidity criteria. It is scientifically sound, well-posed, objective, complete, and relevant. It poses a substantive question that tests the understanding of the statistical theory underlying a common and important analysis pipeline in systems biomedicine.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. The solution will proceed by evaluating each option against the four specified criteria.\n\n### Derivation and Option-by-Option Analysis\n\nThe core of the problem is to formalize the statement: \"heterogeneity in the effective rate ... inflates variance\". This is canonically modeled as a mixed-distribution model. The base model for counts is Poisson, so we start with $Y_{ig} \\mid \\Lambda_{ig} \\sim \\mathrm{Poisson}(\\Lambda_{ig})$, where $\\Lambda_{ig}$ is the unobserved \"true\" rate parameter for the count $Y_{ig}$. The heterogeneity is modeled by assuming that $\\Lambda_{ig}$ is itself a random variable drawn from a distribution.\n\nFor the marginal distribution of $Y_{ig}$ to be a Negative Binomial, the distribution for the rate $\\Lambda_{ig}$ must be a Gamma distribution. This is a well-known result in statistics: the Poisson-Gamma mixture produces a Negative Binomial distribution. Let's formalize this.\n\nIf $Y \\mid \\Lambda \\sim \\mathrm{Poisson}(\\Lambda)$ and $\\Lambda \\sim \\mathrm{Gamma}(\\text{shape}=k, \\text{scale}=\\theta)$, then the marginal distribution of $Y$ is $\\mathrm{NB}(\\text{r}=k, \\text{p}=1/(1+\\theta))$. The mean and variance of this NB distribution are $\\mathbb{E}[Y]=k\\theta$ and $\\mathrm{Var}(Y)=k\\theta(1+\\theta)$. Let's define the mean as $\\mu \\equiv k\\theta$. Then the variance can be re-written as $\\mathrm{Var}(Y) = \\mu + k\\theta^2 = \\mu + (k\\theta)^2/k = \\mu + \\mu^2/k$. This is the standard NB2 parameterization, often written as $\\mathrm{Var}(Y) = \\mu + \\alpha\\mu^2$ where $\\alpha=1/k$ is the dispersion parameter. The variance is a quadratic function of the mean, and for any $\\alpha>0$, $\\mathrm{Var}(Y) > \\mathbb{E}[Y]$, which defines overdispersion relative to the Poisson model where $\\mathrm{Var}(Y) = \\mathbb{E}[Y]$.\n\nThe GLM component relates the mean $\\mu_{ig}$ to the covariates. For count data, a log link is used to ensure $\\mu_{ig} > 0$. The total number of reads in a library, $s_i$, acts as a scaling factor on the underlying relative abundances. Thus, we expect $\\mu_{ig} \\propto s_i$. In a log-linear model, this becomes $\\log(\\mu_{ig}) = \\log(s_i) + \\text{other terms}$. Here, $\\log(s_i)$ is a known term and is therefore an \"offset\". The model becomes $\\log(\\mu_{ig}) = \\log s_i + \\alpha_g + \\beta_g z_i$.\n\nWith these principles established, we evaluate the options.\n\n#### **Option A**\n\n-   **(i) Valid PMF**: This option proposes a Poisson-Gamma mixture: $Y_{ig}\\mid \\Lambda_{ig} \\sim \\mathrm{Poisson}(\\Lambda_{ig})$ with $\\Lambda_{ig} \\sim \\mathrm{Gamma}(k, k/\\mu_{ig})$ (shape-rate). The mean of this Gamma distribution is $\\mathbb{E}[\\Lambda_{ig}] = \\text{shape}/\\text{rate} = k / (k/\\mu_{ig}) = \\mu_{ig}$. The marginal distribution of $Y_{ig}$ is therefore Negative Binomial with mean $\\mu_{ig}$. The provided PMF is the correct PMF for an NB distribution parameterized by its mean $\\mu_{ig}$ and a shape/dispersion parameter $k$. Criterion (i) is met.\n-   **(ii) Appropriate GLM**: It proposes the log link model $\\log \\mu_{ig} = \\log s_i + \\alpha_g + \\beta_g z_i$. This correctly uses the log link to ensure a positive mean and includes $\\log s_i$ as an offset to account for sequencing depth. This is the standard and correct GLM structure for this type of data. Criterion (ii) is met.\n-   **(iii) Mean-Variance Relationship**: It states $\\mathbb{E}[Y_{ig}]=\\mu_{ig}$ and $\\mathrm{Var}(Y_{ig})=\\mu_{ig}+\\mu_{ig}^{2}/k$. As derived above from the law of total variance on the Poisson-Gamma mixture, this is the correct variance for the NB2 model. Since $k>0$, the variance is greater than the mean, and the quadratic term $\\mu_{ig}^2/k$ explicitly models the overdispersion. Criterion (iii) is met.\n-   **(iv) Mechanistic Explanation**: It correctly attributes overdispersion to unobserved heterogeneity in the effective rate, formalizes this as a Gamma distribution for the Poisson rate, and correctly identifies the resulting mixture distribution as Negative Binomial. This aligns perfectly with the problem statement's premise. Criterion (iv) is met.\n\nVerdict on A: **Correct**. All four criteria are satisfied.\n\n#### **Option B**\n\n-   This option proposes a quasi-Poisson model, not a Negative Binomial model. This immediately violates the problem's explicit requirement for a \"negative binomial model\". Quasi-likelihood is an estimation method, not a fully specified probability distribution with a PMF. Therefore, criterion (i) is not met. Furthermore, it posits a linear mean-variance relationship, $\\mathrm{Var}(Y_{ig})=\\phi\\,\\mu_{ig}$, which is often less accurate for sequencing data than the quadratic relationship of the NB model.\n\nVerdict on B: **Incorrect**.\n\n#### **Option C**\n\n-   This option uses an identity link for the mean: $\\mu_{ig}=s_i+\\alpha_g+\\beta_g z_i$. This is incorrect. An identity link does not guarantee a positive mean ($\\mu_{ig} \\ge 0$). Moreover, it models the effect of sequencing depth $s_i$ as additive, whereas it is fundamentally a multiplicative scaling factor. The correct approach is a log link with a log offset, as in option A. This fails criterion (ii).\n-   It also proposes a linear mean-variance relationship $\\mathrm{Var}(Y_{ig})=\\mu_{ig}+\\alpha\\,\\mu_{ig}$, which is less standard and often less appropriate than the quadratic one. This is a weakness under criterion (iii).\n-   Finally, it mentions a \"Beta-Poisson mixture\". This is not the standard derivation for the NB distribution; the correct one is a Poisson-Gamma mixture. This fails criterion (iv).\n\nVerdict on C: **Incorrect**.\n\n#### **Option D**\n\n-   This option proposes a lognormal-Poisson mixture model. The marginal distribution resulting from this mixture is not a Negative Binomial distribution, and it does not have a closed-form PMF. The statement that \"this mixture is equivalent to a negative binomial\" is factually incorrect. While they are both overdispersed count models, they are mathematically distinct distributions. This fails criterion (i).\n\nVerdict on D: **Incorrect**.\n\n#### **Option E**\n\n-   This option proposes a zero-inflated Poisson (ZIP) model. A ZIP model is not a Negative Binomial model. It addresses overdispersion via a different mechanism (excess zeros) than the NB model (general rate heterogeneity). This fails the explicit requirement of the question for an NB model, thus failing criterion (i).\n-   The formula provided for the variance, $\\mathrm{Var}(Y_{ig})=\\mu_{ig}(1-\\pi)$, is incorrect. The actual mean of the specified ZIP model is $\\mathbb{E}[Y_{ig}]=(1-\\pi)\\mu_{ig}$, and the variance is $\\mathrm{Var}(Y_{ig}) = (1-\\pi)\\mu_{ig} + \\pi(1-\\pi)\\mu_{ig}^2$. The formula in the option is wrong and does not even describe overdispersion correctly. This fails criterion (iii).\n\nVerdict on E: **Incorrect**.\n\n### Conclusion\n\nOnly option A correctly and comprehensively satisfies all four criteria laid out in the problem statement. It presents the standard and theoretically sound Poisson-Gamma mixture derivation of the Negative Binomial GLM as applied to sequencing count data.", "answer": "$$\\boxed{A}$$", "id": "4344572"}, {"introduction": "A genome-wide screen involves performing tens of thousands of statistical tests simultaneously, one for each gene. This massive scale of testing introduces a significant statistical challenge: controlling the number of false positives among the declared 'hits'. Simply using a conventional $p$-value threshold is inadequate and leads to an unacceptably high rate of false discoveries. This final practice introduces the formal concept of the False Discovery Rate (FDR) and walks you through the application of the Benjamini-Hochberg procedure, a standard method for maintaining statistical rigor when identifying essential genes from a large-scale perturbation screen [@problem_id:4344579].", "problem": "A pooled clustered regularly interspaced short palindromic repeats (CRISPR) knockout viability screen in systems biomedicine tests $m$ genes for essentiality by fitting a generalized linear model per gene and generating one $p$-value per gene. Assume that the $p$-values corresponding to truly nonessential genes are valid (uniform on $[0,1]$) and that the tests are either independent or satisfy Positive Regression Dependence on a Subset (PRDS). The investigator wants to control the False Discovery Rate (FDR) while declaring essential genes.\n\nConsider a pilot plate with $m = 12$ genes producing the following ordered $p$-values (already sorted in ascending order): $p_{(1)} = 0.001$, $p_{(2)} = 0.009$, $p_{(3)} = 0.020$, $p_{(4)} = 0.031$, $p_{(5)} = 0.045$, $p_{(6)} = 0.060$, $p_{(7)} = 0.120$, $p_{(8)} = 0.200$, $p_{(9)} = 0.330$, $p_{(10)} = 0.500$, $p_{(11)} = 0.700$, $p_{(12)} = 0.900$. The target FDR level is $q = 0.10$.\n\nWhich option correctly states (i) the formal definition of the False Discovery Rate (FDR) in terms of the number of false discoveries $V$ and the total number of discoveries $R$, (ii) the Benjamini–Hochberg (BH) step-up procedure to control FDR under independent or PRDS tests, including how to choose the rejection threshold, (iii) the conditions and bound under which BH controls FDR, and (iv) the number of rejections for the given pilot $p$-values at $q = 0.10$?\n\nA. FDR is $P(V > 0)$. The BH procedure sorts $p$-values in descending order and rejects all $p_{(i)}$ such that $p_{(i)} \\ge \\frac{i}{m} q$. It controls FDR at most $q$ regardless of the dependence structure. For the pilot plate, it rejects $5$ genes.\n\nB. FDR is $\\mathbb{E}\\!\\left[\\dfrac{V}{R \\vee 1}\\right]$, where $R \\vee 1$ denotes $\\max(R, 1)$. The BH step-up procedure sorts the $p$-values in ascending order, finds the largest index $k$ such that $p_{(k)} \\le \\dfrac{k}{m} q$, and rejects all hypotheses with indices $i \\le k$. Under independence or Positive Regression Dependence on a Subset (PRDS), BH controls FDR at $\\mathbb{E}\\!\\left[\\dfrac{V}{R \\vee 1}\\right] \\le \\dfrac{m_{0}}{m} q$, where $m_{0}$ is the number of true null hypotheses; in particular, if all hypotheses are null, $\\mathbb{E}\\!\\left[\\dfrac{V}{R \\vee 1}\\right] \\le q$. For the pilot plate at $q = 0.10$, it rejects $4$ genes.\n\nC. FDR is $\\mathbb{E}\\!\\left[\\dfrac{V}{m}\\right]$. The BH method is a step-down variant of Holm’s procedure that rejects the smallest $p$-values while checking $p_{(i)} \\le \\dfrac{q}{m - i + 1}$ sequentially. It controls the Family-Wise Error Rate (FWER) at level $q$. For the pilot plate, it rejects $6$ genes.\n\nD. FDR is $\\mathbb{E}\\!\\left[\\dfrac{R}{m}\\right]$. The BH procedure reduces to Bonferroni, rejecting all $p_{i} \\le \\dfrac{q}{m}$; it ensures FDR at most $q$ only when $m_{0}$ is unknown. For the pilot plate, it rejects $1$ gene.", "solution": "The problem statement is subjected to validation before proceeding to a solution.\n\n### Step 1: Extract Givens\n-   The context is a pooled CRISPR knockout viability screen in systems biomedicine.\n-   The number of genes tested is $m = 12$.\n-   A statistical test is performed for each gene, producing one $p$-value per gene.\n-   The null hypothesis is that a gene is nonessential.\n-   The assumptions are:\n    1.  The $p$-values corresponding to truly nonessential genes follow a uniform distribution on $[0,1]$.\n    2.  The tests are either independent or satisfy Positive Regression Dependence on a Subset (PRDS).\n-   The objective is to control the False Discovery Rate (FDR).\n-   The target FDR level is $q = 0.10$.\n-   The ordered $p$-values are: $p_{(1)} = 0.001$, $p_{(2)} = 0.009$, $p_{(3)} = 0.020$, $p_{(4)} = 0.031$, $p_{(5)} = 0.045$, $p_{(6)} = 0.060$, $p_{(7)} = 0.120$, $p_{(8)} = 0.200$, $p_{(9)} = 0.330$, $p_{(10)} = 0.500$, $p_{(11)} = 0.700$, $p_{(12)} = 0.900$.\n-   The question asks for the correct statement covering four components: (i) the formal definition of FDR, (ii) the description of the Benjamini–Hochberg (BH) procedure, (iii) the conditions and bound for FDR control by the BH procedure, and (iv) the number of rejections for the provided data.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically and mathematically sound.\n-   **Scientifically Grounded**: The scenario of a CRISPR screen, testing for gene essentiality, and using multiple hypothesis testing correction is a standard and fundamental workflow in modern functional genomics and systems biology. The statistical concepts, including $p$-values, FDR, independence, and PRDS, are well-established in the statistical literature.\n-   **Well-Posed**: The problem is clearly defined. It provides all necessary data (the set of $p$-values, $m$, and $q$) and the necessary assumptions (independence or PRDS) to uniquely determine the outcome of the Benjamini-Hochberg procedure.\n-   **Objective**: The language is precise and uses standard, unambiguous terminology from statistics and bioinformatics.\n-   **Completeness and Consistency**: The number of provided $p$-values ($12$) matches the stated number of genes, $m=12$. All data are consistent and sufficient for a solution.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be derived.\n\n### Derivation of the Correct Answer\n\nThe problem requires a comprehensive understanding of the False Discovery Rate and the Benjamini-Hochberg (BH) procedure. We will address each of the four required components before evaluating the options.\n\nLet's organize the outcomes of $m$ hypothesis tests in a table:\n| | Declared non-significant (Null Accepted) | Declared significant (Null Rejected) | Total |\n| :--- | :---: | :---: | :---: |\n| **Null hypothesis is true** | $U$ | $V$ (False Discoveries) | $m_0$ |\n| **Null hypothesis is false**| $T$ | $S$ (True Discoveries) | $m_1$ |\n| **Total** | $m-R$ | $R$ (Total Discoveries) | $m$ |\n\nHere, $V$ is the number of false discoveries (Type I errors) and $R$ is the total number of rejections (discoveries). $m_0$ is the number of true null hypotheses.\n\n**(i) Formal Definition of the False Discovery Rate (FDR)**\nThe False Discovery Proportion (FDP) is the proportion of false discoveries among all discoveries. It is defined as $Q = V/R$ if $R > 0$, and $Q = 0$ if $R = 0$. To avoid division by zero, this is compactly written as $Q = \\dfrac{V}{\\max(R, 1)}$, which can also be denoted as $\\dfrac{V}{R \\vee 1}$. The False Discovery Rate (FDR) is the statistical expectation of the FDP:\n$$\n\\text{FDR} = \\mathbb{E}[Q] = \\mathbb{E}\\left[\\frac{V}{R \\vee 1}\\right]\n$$\n\n**(ii) The Benjamini–Hochberg (BH) Step-Up Procedure**\nThe procedure to control the FDR at a target level $q$ is as follows:\n1.  Order the $m$ $p$-values from smallest to largest: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$.\n2.  Find the largest index $k \\in \\{1, 2, \\dots, m\\}$ such that the following inequality holds:\n    $$\n    p_{(k)} \\le \\frac{k}{m} q\n    $$\n3.  If such a $k$ exists, reject all null hypotheses $H_{(i)}$ for which $i \\le k$. If no such $k$ exists, do not reject any null hypothesis.\n\n**(iii) Conditions and Bound for FDR Control with BH**\nThe BH procedure was proven by Benjamini and Hochberg (1995) to control the FDR for independent test statistics. Benjamini and Yekutieli (2001) extended this guarantee to test statistics that satisfy the Positive Regression Dependence on a Subset (PRDS) condition. Under these conditions (independence or PRDS), the BH procedure guarantees that:\n$$\n\\text{FDR} = \\mathbb{E}\\left[\\frac{V}{R \\vee 1}\\right] \\le \\frac{m_0}{m} q\n$$\nSince the number of true nulls $m_0$ is always less than or equal to the total number of tests $m$ (i.e., $m_0 \\le m$), this implies that $\\text{FDR} \\le q$.\n\n**(iv) Number of Rejections for the Given Data**\nWe apply the BH procedure to the given data.\n-   $m = 12$\n-   $q = 0.10$\n-   The ordered $p$-values are $p_{(1)} = 0.001, p_{(2)} = 0.009, \\dots$.\n-   We need to find the largest $k$ such that $p_{(k)} \\le \\frac{k}{12} \\times 0.10 = \\frac{k}{120}$.\n\nLet's check the condition for increasing values of $k$:\n-   For $k=1$: $p_{(1)} = 0.001$. The threshold is $\\frac{1}{120} \\approx 0.00833$. Since $0.001 \\le 0.00833$, the condition is met.\n-   For $k=2$: $p_{(2)} = 0.009$. The threshold is $\\frac{2}{120} \\approx 0.01667$. Since $0.009 \\le 0.01667$, the condition is met.\n-   For $k=3$: $p_{(3)} = 0.020$. The threshold is $\\frac{3}{120} = 0.025$. Since $0.020 \\le 0.025$, the condition is met.\n-   For $k=4$: $p_{(4)} = 0.031$. The threshold is $\\frac{4}{120} \\approx 0.03333$. Since $0.031 \\le 0.03333$, the condition is met.\n-   For $k=5$: $p_{(5)} = 0.045$. The threshold is $\\frac{5}{120} \\approx 0.04167$. Since $0.045 > 0.04167$, the condition is not met.\n-   For any $k > 5$, the condition will also not be met, as the $p$-values are non-decreasing.\n\nThe largest index $k$ for which the condition $p_{(k)} \\le \\frac{k}{m} q$ is satisfied is $k=4$. Therefore, the BH procedure rejects the $4$ null hypotheses corresponding to the $4$ smallest $p$-values. The number of rejected hypotheses is $4$.\n\n### Evaluation of Options\n\n**Option A**: FDR is $P(V > 0)$. The BH procedure sorts $p$-values in descending order and rejects all $p_{(i)}$ such that $p_{(i)} \\ge \\frac{i}{m} q$. It controls FDR at most $q$ regardless of the dependence structure. For the pilot plate, it rejects $5$ genes.\n-   (i) The definition \"FDR is $P(V > 0)$\" is the definition of the Family-Wise Error Rate (FWER), not the FDR.\n-   (ii) The procedure description is incorrect. BH sorts in ascending order, and the inequality is $\\le$, not $\\ge$.\n-   (iii) The claim that BH controls FDR \"regardless of the dependence structure\" is false. Standard BH requires independence or PRDS.\n-   (iv) The number of rejections is stated as $5$, which contradicts our calculation of $4$.\n**Verdict**: Incorrect.\n\n**Option B**: FDR is $\\mathbb{E}\\!\\left[\\dfrac{V}{R \\vee 1}\\right]$, where $R \\vee 1$ denotes $\\max(R, 1)$. The BH step-up procedure sorts the $p$-values in ascending order, finds the largest index $k$ such that $p_{(k)} \\le \\dfrac{k}{m} q$, and rejects all hypotheses with indices $i \\le k$. Under independence or Positive Regression Dependence on a Subset (PRDS), BH controls FDR at $\\mathbb{E}\\!\\left[\\dfrac{V}{R \\vee 1}\\right] \\le \\dfrac{m_{0}}{m} q$, where $m_{0}$ is the number of true null hypotheses; in particular, if all hypotheses are null, $\\mathbb{E}\\!\\left[\\dfrac{V}{R \\vee 1}\\right] \\le q$. For the pilot plate at $q = 0.10$, it rejects $4$ genes.\n-   (i) The definition $\\mathbb{E}\\!\\left[\\dfrac{V}{R \\vee 1}\\right]$ is the correct formal definition of FDR.\n-   (ii) The description of the BH procedure is perfectly accurate.\n-   (iii) The conditions (independence or PRDS) and the resulting bound ($\\le \\frac{m_0}{m} q$) are stated correctly and precisely.\n-   (iv) The number of rejections is stated as $4$, which matches our calculation.\n**Verdict**: Correct.\n\n**Option C**: FDR is $\\mathbb{E}\\!\\left[\\dfrac{V}{m}\\right]$. The BH method is a step-down variant of Holm’s procedure that rejects the smallest $p$-values while checking $p_{(i)} \\le \\dfrac{q}{m - i + 1}$ sequentially. It controls the Family-Wise Error Rate (FWER) at level $q$. For the pilot plate, it rejects $6$ genes.\n-   (i) The definition $\\mathbb{E}\\!\\left[\\dfrac{V}{m}\\right]$ is the Per-Comparison Error Rate (PCER), not the FDR.\n-   (ii) The description is of the Holm-Bonferroni method (a step-down procedure), not the BH method (a step-up procedure). The threshold $\\frac{q}{m - i + 1}$ belongs to the Holm method.\n-   (iii) The assertion that the procedure controls FWER is true for the Holm method but false for the BH method, which controls FDR.\n-   (iv) The number of rejections is stated as $6$, which contradicts our calculation of $4$ for the BH procedure.\n**Verdict**: Incorrect.\n\n**Option D**: FDR is $\\mathbb{E}\\!\\left[\\dfrac{R}{m}\\right]$. The BH procedure reduces to Bonferroni, rejecting all $p_{i} \\le \\dfrac{q}{m}$; it ensures FDR at most $q$ only when $m_{0}$ is unknown. For the pilot plate, it rejects $1$ gene.\n-   (i) The definition $\\mathbb{E}\\!\\left[\\dfrac{R}{m}\\right]$ is the expected proportion of rejections, which is related to statistical power, not the FDR.\n-   (ii) The BH procedure does not reduce to the Bonferroni correction. The Bonferroni correction uses a single threshold $\\frac{q}{m}$ for all tests to control FWER, while BH uses adaptive thresholds $\\frac{k}{m} q$.\n-   (iii) The statement about $m_0$ is nonsensical. The BH procedure's validity does not depend on whether $m_0$ is known.\n-   (iv) The number of rejections is stated as $1$. The Bonferroni correction would indeed reject $1$ gene ($p_{(1)}=0.001 \\le 0.10/12 \\approx 0.00833$, but $p_{(2)}=0.009 > 0.00833$). However, the question asks about the BH procedure, for which the number of rejections is $4$.\n**Verdict**: Incorrect.\n\nFinal conclusion is that option B is the only one that correctly states all four components.", "answer": "$$\\boxed{B}$$", "id": "4344579"}]}