{"hands_on_practices": [{"introduction": "Mastering Variational Autoencoders begins with a deep understanding of their objective function, the Evidence Lower Bound (ELBO). This foundational exercise guides you through deriving the ELBO for a canonical VAE with Gaussian assumptions, a common starting point for modeling continuous omics data like log-normalized gene expression. By breaking the ELBO into its reconstruction and regularization components from first principles, you will gain crucial insight into the trade-offs at the heart of variational inference. [@problem_id:4397996]", "problem": "Consider a single omics sample represented by a log-normalized gene expression vector $x \\in \\mathbb{R}^{d}$ obtained from bulk ribonucleic acid sequencing (RNA-seq). A Variational Autoencoder (VAE) is specified with latent variable $z \\in \\mathbb{R}^{k}$ and a standard Gaussian prior $p(z) = \\mathcal{N}(z; 0, I)$. The generative model (decoder) is $p_{\\theta}(x \\mid z) = \\mathcal{N}(x; \\mu(z), \\sigma^{2} I)$, where the mean is linear in $z$ as $\\mu(z) = W z + b$ with $W \\in \\mathbb{R}^{d \\times k}$ and $b \\in \\mathbb{R}^{d}$, and the scalar observation variance $\\sigma^{2} > 0$ is known. The inference model (encoder) is $q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^{2}(x)))$, with mean $\\mu_{\\phi}(x) \\in \\mathbb{R}^{k}$ and elementwise variances $\\sigma_{\\phi}^{2}(x) \\in \\mathbb{R}_{>0}^{k}$.\n\nStarting from the definition of the Evidence Lower Bound (ELBO) as the difference between the expected log-likelihood under the encoder and the Kullback–Leibler divergence between the encoder and the prior, derive a closed-form analytic expression for the ELBO for the given sample $x$ in terms of $x$, $W$, $b$, $\\sigma^{2}$, $\\mu_{\\phi}(x)$, and $\\sigma_{\\phi}^{2}(x)$, along with $d$ and $k$. Your derivation must begin from fundamental definitions of probability densities of multivariate normal distributions and the Kullback–Leibler divergence, and must not assume any shortcut formulas. Express your final answer as a single closed-form analytic expression. No numerical rounding is required.", "solution": "The problem is valid. It presents a standard, well-posed task in probabilistic machine learning, providing all necessary information and definitions without scientific or logical contradictions. We proceed to the derivation.\n\nThe Evidence Lower Bound (ELBO) is defined as:\n$$\n\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] - D_{KL}(q_{\\phi}(z \\mid x) \\| p(z))\n$$\nWe will derive the two terms on the right-hand side separately.\n\n**Part 1: The Expected Log-Likelihood Term**\n\nThe first term is the expected log-likelihood of the data under the approximate posterior $q_{\\phi}(z \\mid x)$. The generative model (decoder) is given by $p_{\\theta}(x \\mid z) = \\mathcal{N}(x; W z + b, \\sigma^{2} I)$.\n\nThe probability density function (PDF) of this $d$-dimensional multivariate normal distribution is:\n$$\np_{\\theta}(x \\mid z) = \\frac{1}{(2\\pi)^{d/2} \\det(\\sigma^2 I)^{1/2}} \\exp\\left(-\\frac{1}{2}(x - (Wz+b))^T (\\sigma^2 I)^{-1} (x - (Wz+b))\\right)\n$$\nThe determinant of the covariance matrix is $\\det(\\sigma^2 I) = (\\sigma^2)^d$. The inverse is $(\\sigma^2 I)^{-1} = \\frac{1}{\\sigma^2}I$. Substituting these in gives:\n$$\np_{\\theta}(x \\mid z) = \\frac{1}{(2\\pi \\sigma^2)^{d/2}} \\exp\\left(-\\frac{1}{2\\sigma^2}\\|x - Wz - b\\|_2^2\\right)\n$$\nTaking the natural logarithm, we get:\n$$\n\\log p_{\\theta}(x \\mid z) = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|x - Wz - b\\|_2^2\n$$\nNow, we must take the expectation of this quantity with respect to the encoder distribution $q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^{2}(x)))$. For notational simplicity, let $\\mu_q = \\mu_{\\phi}(x)$ and $\\Sigma_q = \\operatorname{diag}(\\sigma_{\\phi}^{2}(x))$.\n$$\n\\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] = \\mathbb{E}_{q_{\\phi}}\\left[-\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|x - Wz - b\\|_2^2\\right]\n$$\nBy linearity of expectation, this becomes:\n$$\n\\mathbb{E}_{q_{\\phi}}[\\log p_{\\theta}(x \\mid z)] = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\mathbb{E}_{q_{\\phi}}[\\|x - Wz - b\\|_2^2]\n$$\nWe need to evaluate the expectation of the squared norm. Let's expand the term inside the expectation:\n$$\n\\|x - Wz - b\\|_2^2 = (x - b - Wz)^T(x - b - Wz) = (x-b)^T(x-b) - 2(x-b)^T W z + z^T W^T W z\n$$\nNow, we take the expectation with respect to $z \\sim q_{\\phi}$:\n$$\n\\mathbb{E}_{q_{\\phi}}[\\|x - Wz - b\\|_2^2] = \\mathbb{E}_{q_{\\phi}}[(x-b)^T(x-b)] - 2(x-b)^T W \\mathbb{E}_{q_{\\phi}}[z] + \\mathbb{E}_{q_{\\phi}}[z^T W^T W z]\n$$\nWe know that $\\mathbb{E}_{q_{\\phi}}[z] = \\mu_q$. For the quadratic term, we use the identity $\\mathbb{E}[y^T A y] = \\text{Tr}(A \\text{Cov}[y]) + \\mathbb{E}[y]^T A \\mathbb{E}[y]$. Here, $y=z$, $A = W^T W$, $\\mathbb{E}[z]=\\mu_q$, and $\\text{Cov}[z]=\\Sigma_q$.\n$$\n\\mathbb{E}_{q_{\\phi}}[z^T W^T W z] = \\text{Tr}(W^T W \\Sigma_q) + \\mu_q^T W^T W \\mu_q\n$$\nSubstituting these back, the expected squared norm is:\n$$\n\\mathbb{E}_{q_{\\phi}}[\\|x - Wz - b\\|_2^2] = \\|x-b\\|_2^2 - 2(x-b)^T W \\mu_q + \\mu_q^T W^T W \\mu_q + \\text{Tr}(W^T W \\Sigma_q)\n$$\nThe first three terms can be regrouped into a squared norm:\n$$\n\\|x-b\\|_2^2 - 2(x-b)^T W \\mu_q + \\|W\\mu_q\\|_2^2 = \\|(x-b) - W\\mu_q\\|_2^2 = \\|x - W\\mu_q - b\\|_2^2\n$$\nSo, we have:\n$$\n\\mathbb{E}_{q_{\\phi}}[\\|x - Wz - b\\|_2^2] = \\|x - W\\mu_q - b\\|_2^2 + \\text{Tr}(W^T W \\Sigma_q)\n$$\nThe trace term can be explicitly written out. Let $w_j$ be the $j$-th column of $W$, and $(\\sigma_q^2)_j$ be the $j$-th component of the variance vector $\\sigma_{\\phi}^2(x)$.\n$$\n\\text{Tr}(W^T W \\Sigma_q) = \\sum_{j=1}^k (W^T W \\Sigma_q)_{jj} = \\sum_{j=1}^k \\left(\\sum_{l=1}^k (W^T W)_{jl} (\\Sigma_q)_{lj}\\right)\n$$\nSince $\\Sigma_q$ is diagonal, $(\\Sigma_q)_{lj} = (\\sigma_q^2)_j \\delta_{lj}$.\n$$\n\\text{Tr}(W^T W \\Sigma_q) = \\sum_{j=1}^k (W^T W)_{jj} (\\sigma_q^2)_j = \\sum_{j=1}^k \\left(\\sum_{i=1}^d W_{ij}^2\\right) (\\sigma_q^2)_j = \\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_q^2)_j\n$$\nFinally, substituting this back gives the first term of the ELBO:\n$$\n\\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\left( \\|x - W\\mu_{\\phi}(x) - b\\|_2^2 + \\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_{\\phi}^2(x))_j \\right)\n$$\n\n**Part 2: The Kullback-Leibler Divergence Term**\n\nThe second term is the KL divergence between the encoder distribution $q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu_q, \\Sigma_q)$ and the prior $p(z) = \\mathcal{N}(z; 0, I)$.\nThe KL divergence is defined as:\n$$\nD_{KL}(q_{\\phi} \\| p) = \\int q_{\\phi}(z \\mid x) \\log \\frac{q_{\\phi}(z \\mid x)}{p(z)} dz = \\mathbb{E}_{q_{\\phi}}[\\log q_{\\phi}(z \\mid x) - \\log p(z)]\n$$\nSince both $q_{\\phi}$ and $p$ are diagonal-covariance Gaussians, they factorize over the dimensions of $z$. Thus, the KL divergence is the sum of the KL divergences for each dimension:\n$$\nD_{KL}(q_{\\phi} \\| p) = \\sum_{j=1}^k D_{KL}(\\mathcal{N}(z_j; (\\mu_q)_j, (\\sigma_q^2)_j) \\| \\mathcal{N}(z_j; 0, 1))\n$$\nFor a single dimension $j$, we have $q_j(z_j) = \\mathcal{N}(z_j; (\\mu_q)_j, (\\sigma_q^2)_j)$ and $p_j(z_j) = \\mathcal{N}(z_j; 0, 1)$. Their log-PDFs are:\n$$\n\\log q_j(z_j) = -\\frac{1}{2}\\log(2\\pi(\\sigma_q^2)_j) - \\frac{(z_j - (\\mu_q)_j)^2}{2(\\sigma_q^2)_j}\n$$\n$$\n\\log p_j(z_j) = -\\frac{1}{2}\\log(2\\pi) - \\frac{z_j^2}{2}\n$$\nThe difference is:\n$$\n\\log q_j(z_j) - \\log p_j(z_j) = -\\frac{1}{2}\\log((\\sigma_q^2)_j) - \\frac{(z_j - (\\mu_q)_j)^2}{2(\\sigma_q^2)_j} + \\frac{z_j^2}{2}\n$$\nTaking the expectation with respect to $z_j \\sim q_j(z_j)$:\n$$\nD_{KL}(q_j \\| p_j) = \\mathbb{E}_{q_j}\\left[ -\\frac{1}{2}\\log((\\sigma_q^2)_j) - \\frac{(z_j - (\\mu_q)_j)^2}{2(\\sigma_q^2)_j} + \\frac{z_j^2}{2} \\right]\n$$\nUsing linearity of expectation:\n$$\nD_{KL}(q_j \\| p_j) = -\\frac{1}{2}\\log((\\sigma_q^2)_j) - \\frac{1}{2(\\sigma_q^2)_j}\\mathbb{E}_{q_j}[(z_j - (\\mu_q)_j)^2] + \\frac{1}{2}\\mathbb{E}_{q_j}[z_j^2]\n$$\nWe have $\\mathbb{E}_{q_j}[(z_j - (\\mu_q)_j)^2] = \\text{Var}_{q_j}(z_j) = (\\sigma_q^2)_j$, and $\\mathbb{E}_{q_j}[z_j^2] = \\text{Var}_{q_j}(z_j) + (\\mathbb{E}_{q_j}[z_j])^2 = (\\sigma_q^2)_j + ((\\mu_q)_j)^2$.\n$$\nD_{KL}(q_j \\| p_j) = -\\frac{1}{2}\\log((\\sigma_q^2)_j) - \\frac{(\\sigma_q^2)_j}{2(\\sigma_q^2)_j} + \\frac{(\\sigma_q^2)_j + ((\\mu_q)_j)^2}{2}\n$$\n$$\nD_{KL}(q_j \\| p_j) = \\frac{1}{2} \\left[ -\\log((\\sigma_q^2)_j) - 1 + (\\sigma_q^2)_j + ((\\mu_q)_j)^2 \\right]\n$$\nSumming over all $k$ dimensions:\n$$\nD_{KL}(q_{\\phi}(z \\mid x) \\| p(z)) = \\frac{1}{2} \\sum_{j=1}^k \\left( ((\\mu_{\\phi}(x))_j)^2 + (\\sigma_{\\phi}^2(x))_j - \\log((\\sigma_{\\phi}^2(x))_j) - 1 \\right)\n$$\n\n**Final Expression for ELBO**\n\nCombining the two parts, ELBO = (Part 1) - (Part 2):\n$$\n\\mathcal{L}(x; \\theta, \\phi) = \\left[ -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\left( \\|x - W\\mu_{\\phi}(x) - b\\|_2^2 + \\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_{\\phi}^2(x))_j \\right) \\right] - \\left[ \\frac{1}{2} \\sum_{j=1}^k \\left( ((\\mu_{\\phi}(x))_j)^2 + (\\sigma_{\\phi}^2(x))_j - \\log((\\sigma_{\\phi}^2(x))_j) - 1 \\right) \\right]\n$$\nRearranging the terms yields the final single expression:\n$$\n\\mathcal{L}(x; \\theta, \\phi) = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|x - W\\mu_{\\phi}(x) - b\\|_2^2 - \\frac{1}{2\\sigma^2}\\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_{\\phi}^2(x))_j + \\frac{1}{2} \\sum_{j=1}^k \\left(1 - ((\\mu_{\\phi}(x))_j)^2 - (\\sigma_{\\phi}^2(x))_j + \\log((\\sigma_{\\phi}^2(x))_j) \\right)\n$$\nwhere $w_j$ is the $j$-th column of the matrix $W$.", "answer": "$$ \\boxed{-\\frac{d}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|x - W\\mu_{\\phi}(x) - b\\|_2^2 - \\frac{1}{2\\sigma^2}\\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_{\\phi}^2(x))_j + \\frac{1}{2} \\sum_{j=1}^k \\left(1 + \\log((\\sigma_{\\phi}^2(x))_j) - (\\mu_{\\phi}(x))_j^2 - (\\sigma_{\\phi}^2(x))_j \\right)} $$", "id": "4397996"}, {"introduction": "While Gaussian models are a useful starting point, accurately capturing the characteristics of raw omics count data, such as from single-cell RNA sequencing, requires more suitable statistical tools. This problem challenges you to construct the generative heart of a VAE designed for count data by using the Negative Binomial distribution, which properly models overdispersion. By deriving the joint and marginal likelihoods from the fundamental Poisson-Gamma mixture model, you will practice tailoring the generative process to the unique nature of biological data. [@problem_id:4397872]", "problem": "Consider single-cell ribonucleic acid sequencing (scRNA-seq) count data for $G$ genes, represented as a vector $x \\in \\mathbb{N}_{0}^{G}$, where $\\mathbb{N}_{0}$ denotes the set of non-negative integers. A latent-variable generative model suitable for a Variational Autoencoder (VAE) in systems biomedicine assumes a latent embedding $z \\in \\mathbb{R}^{L}$ drawn from a standard multivariate normal prior, and a conditional observation model for counts that is negative binomial with a mean parameter provided by a decoder that maps the latent variable to the positive orthant. Specifically, the generative assumptions are\n$$\nz \\sim \\mathcal{N}\\!\\left(0, I_{L}\\right), \\quad x \\mid z \\sim \\text{NB}\\!\\left(\\mu(z), \\alpha\\right),\n$$\nwhere $\\mu(z) \\in \\mathbb{R}_{+}^{G}$ is the decoder mean vector with strictly positive components and $\\alpha \\in \\mathbb{R}_{+}^{G}$ is a vector of positive inverse-dispersion parameters; conditional on $z$, the $G$ gene-specific counts are independent. Using only foundational probability definitions (joint density or mass factorization in latent-variable models) and the well-tested characterization of the negative binomial distribution as a Poisson-Gamma mixture, derive from first principles:\n\n1. The explicit joint likelihood $p(x, z)$.\n2. The explicit marginal likelihood $p(x)$ obtained by integrating out the latent variable $z$.\n\nYour derivations must make the parameterization of the negative binomial in terms of its mean $\\mu_{g}(z)$ and inverse dispersion $\\alpha_{g}$ explicit for each gene $g \\in \\{1, \\dots, G\\}$, and must include all normalization constants. Express your final answers as closed-form analytic expressions in terms of $x$, $z$, $\\mu(z)$, $\\alpha$, $G$, and $L$. Do not invoke any approximations, and do not take logarithms. No rounding is required.", "solution": "The user wants to derive the explicit joint likelihood $p(x, z)$ and the marginal likelihood $p(x)$ for a latent variable model of single-cell RNA sequencing data.\n\n### Step 1: Extract Givens\n-   Data: $x \\in \\mathbb{N}_{0}^{G}$ is a vector of counts for $G$ genes.\n-   Latent variable: $z \\in \\mathbb{R}^{L}$.\n-   Latent prior distribution: $z \\sim \\mathcal{N}(0, I_{L})$, where $I_L$ is the $L \\times L$ identity matrix.\n-   Conditional observation model: $x | z \\sim \\text{NB}(\\mu(z), \\alpha)$.\n-   Decoder function: $\\mu(z) \\in \\mathbb{R}_{+}^{G}$ is a vector of strictly positive mean parameters.\n-   Inverse-dispersion parameters: $\\alpha \\in \\mathbb{R}_{+}^{G}$ is a vector of positive parameters.\n-   Conditional independence: The $G$ gene counts, $x_g$ for $g \\in \\{1, \\dots, G\\}$, are independent conditional on $z$.\n-   Methodological constraint: Use the Poisson-Gamma mixture characterization of the negative binomial distribution.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a standard generative model used in systems biomedicine, specifically a variant of a Variational Autoencoder for count data like scRNA-seq (e.g., scVI). The components are scientifically grounded and well-defined:\n-   **Scientifically Grounded**: The choice of a Negative Binomial (NB) distribution is standard for modeling overdispersed count data like scRNA-seq. The Poisson-Gamma mixture is a fundamental characterization of the NB distribution. The use of a standard normal prior for the latent space is a common convention in VAEs. The model is a cornerstone of modern computational biology.\n-   **Well-Posed**: The problem is well-defined. It asks for the derivation of two probability distributions based on the provided generative process. While the integral for the marginal likelihood $p(x)$ is generally intractable for a non-linear decoder $\\mu(z)$, the problem asks for the explicit *expression* for this likelihood, which is a well-defined integral. The term \"closed-form analytic expression\" is interpreted to include such an integral, which is standard in the field of probabilistic machine learning.\n-   **Objective**: The problem is stated in precise mathematical language, free from ambiguity or subjective claims.\n-   **Complete and Consistent**: All necessary definitions and parameters ($G, L, x, z, \\alpha, \\mu(z)$) and distributional assumptions are provided. There are no contradictions.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full derivation will be provided.\n\n### Derivation\n\nThe solution requires deriving two quantities: the joint likelihood $p(x, z)$ and the marginal likelihood $p(x)$.\n\n#### 1. Derivation of the Joint Likelihood $p(x, z)$\n\nThe joint likelihood is defined by the chain rule of probability as $p(x, z) = p(x|z) p(z)$. We will derive the expression for each term separately.\n\nFirst, the prior distribution on the latent variable $z$ is a standard multivariate normal distribution in $L$ dimensions:\n$$\np(z) = \\mathcal{N}(z | 0, I_L) = \\frac{1}{(2\\pi)^{L/2} |I_L|^{1/2}} \\exp\\left(-\\frac{1}{2} z^T I_L^{-1} z\\right)\n$$\nSince $|I_L| = 1$ and $I_L^{-1} = I_L$, this simplifies to:\n$$\np(z) = \\frac{1}{(2\\pi)^{L/2}} \\exp\\left(-\\frac{1}{2} z^T z\\right)\n$$\n\nSecond, the conditional likelihood of the data $x$ given the latent variable $z$, $p(x|z)$, is given. Due to the conditional independence of gene counts, this is the product of the individual gene likelihoods:\n$$\np(x|z) = \\prod_{g=1}^{G} p(x_g | z)\n$$\nEach $p(x_g | z)$ follows a negative binomial distribution, $\\text{NB}(x_g | \\mu_g(z), \\alpha_g)$, where $\\mu_g(z)$ is the $g$-th component of the mean vector $\\mu(z)$ and $\\alpha_g$ is the $g$-th component of the inverse-dispersion vector $\\alpha$.\n\nAs per the problem's instruction, we derive the probability mass function (PMF) of the negative binomial distribution from its characterization as a Poisson-Gamma mixture. A random variable $X$ follows an NB distribution with mean $\\mu$ and inverse dispersion $\\alpha$ if it is a mixture where a rate parameter $\\lambda$ is drawn from a Gamma distribution, and $X$ is then drawn from a Poisson distribution with rate $\\lambda$.\nThe specific parameterization is:\n$$\n\\lambda_g \\sim \\text{Gamma}(\\text{shape}=\\alpha_g, \\text{rate}=\\alpha_g / \\mu_g(z))\n$$\n$$\nx_g | \\lambda_g \\sim \\text{Poisson}(\\lambda_g)\n$$\nThe probability density function of the Gamma distribution (shape $k$, rate $\\theta$) is $p(\\lambda) = \\frac{\\theta^k}{\\Gamma(k)} \\lambda^{k-1} e^{-\\theta \\lambda}$. The PMF of the Poisson distribution is $p(x|\\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}$.\nThe marginal distribution $p(x_g | z)$ is found by integrating out $\\lambda_g$:\n$$\np(x_g | z) = \\int_0^\\infty p(x_g | \\lambda_g) p(\\lambda_g | z) d\\lambda_g\n$$\nSubstituting the PMF and PDF with $k=\\alpha_g$ and $\\theta = \\alpha_g / \\mu_g(z)$:\n$$\np(x_g | z) = \\int_0^\\infty \\left( \\frac{\\lambda_g^{x_g} e^{-\\lambda_g}}{x_g!} \\right) \\left( \\frac{(\\alpha_g / \\mu_g(z))^{\\alpha_g}}{\\Gamma(\\alpha_g)} \\lambda_g^{\\alpha_g-1} \\exp\\left(-\\frac{\\alpha_g}{\\mu_g(z)}\\lambda_g\\right) \\right) d\\lambda_g\n$$\nCombining terms that depend on $\\lambda_g$ inside the integral:\n$$\np(x_g | z) = \\frac{1}{\\Gamma(x_g+1)\\Gamma(\\alpha_g)} \\left(\\frac{\\alpha_g}{\\mu_g(z)}\\right)^{\\alpha_g} \\int_0^\\infty \\lambda_g^{x_g+\\alpha_g-1} \\exp\\left(-\\left(1 + \\frac{\\alpha_g}{\\mu_g(z)}\\right)\\lambda_g\\right) d\\lambda_g\n$$\nThe integral is the unnormalized kernel of a Gamma distribution. Using the identity $\\int_0^\\infty t^{a-1} e^{-bt} dt = \\frac{\\Gamma(a)}{b^a}$ with $a = x_g+\\alpha_g$ and $b = 1 + \\frac{\\alpha_g}{\\mu_g(z)} = \\frac{\\mu_g(z)+\\alpha_g}{\\mu_g(z)}$, the integral evaluates to $\\frac{\\Gamma(x_g+\\alpha_g)}{\\left(\\frac{\\mu_g(z)+\\alpha_g}{\\mu_g(z)}\\right)^{x_g+\\alpha_g}}$.\nSubstituting this back:\n$$\np(x_g | z) = \\frac{1}{\\Gamma(x_g+1)\\Gamma(\\alpha_g)} \\left(\\frac{\\alpha_g}{\\mu_g(z)}\\right)^{\\alpha_g} \\frac{\\Gamma(x_g+\\alpha_g)}{\\left(\\frac{\\mu_g(z)+\\alpha_g}{\\mu_g(z)}\\right)^{x_g+\\alpha_g}}\n$$\nSimplifying the expression:\n$$\np(x_g | z) = \\frac{\\Gamma(x_g+\\alpha_g)}{\\Gamma(x_g+1)\\Gamma(\\alpha_g)} \\frac{\\alpha_g^{\\alpha_g}}{\\mu_g(z)^{\\alpha_g}} \\frac{\\mu_g(z)^{x_g+\\alpha_g}}{(\\mu_g(z)+\\alpha_g)^{x_g+\\alpha_g}}\n$$\n$$\np(x_g | z) = \\frac{\\Gamma(x_g+\\alpha_g)}{\\Gamma(x_g+1)\\Gamma(\\alpha_g)} \\frac{\\alpha_g^{\\alpha_g} \\mu_g(z)^{x_g}}{(\\mu_g(z)+\\alpha_g)^{x_g+\\alpha_g}}\n$$\nThis can be written more compactly as:\n$$\np(x_g | z) = \\frac{\\Gamma(x_g+\\alpha_g)}{\\Gamma(\\alpha_g)\\Gamma(x_g+1)} \\left(\\frac{\\alpha_g}{\\alpha_g+\\mu_g(z)}\\right)^{\\alpha_g} \\left(\\frac{\\mu_g(z)}{\\alpha_g+\\mu_g(z)}\\right)^{x_g}\n$$\nNow, we can write the full joint likelihood $p(x, z) = p(z) p(x|z)$:\n$$\np(x, z) = \\left[ \\frac{1}{(2\\pi)^{L/2}} \\exp\\left(-\\frac{1}{2} z^T z\\right) \\right] \\times \\prod_{g=1}^{G} \\left[ \\frac{\\Gamma(x_g+\\alpha_g)}{\\Gamma(\\alpha_g)\\Gamma(x_g+1)} \\left(\\frac{\\alpha_g}{\\alpha_g+\\mu_g(z)}\\right)^{\\alpha_g} \\left(\\frac{\\mu_g(z)}{\\alpha_g+\\mu_g(z)}\\right)^{x_g} \\right]\n$$\nThis is the final expression for the joint likelihood.\n\n#### 2. Derivation of the Marginal Likelihood $p(x)$\n\nThe marginal likelihood of the data, $p(x)$, is obtained by integrating (marginalizing) out the latent variable $z$ from the joint likelihood $p(x, z)$. The integration is over the entire domain of $z$, which is $\\mathbb{R}^L$.\n$$\np(x) = \\int_{\\mathbb{R}^L} p(x, z) dz\n$$\nSubstituting the expression for $p(x,z)$ derived above:\n$$\np(x) = \\int_{\\mathbb{R}^L} \\left[ \\frac{1}{(2\\pi)^{L/2}} \\exp\\left(-\\frac{1}{2} z^T z\\right) \\right] \\prod_{g=1}^{G} \\left[ \\frac{\\Gamma(x_g+\\alpha_g)}{\\Gamma(\\alpha_g)\\Gamma(x_g+1)} \\left(\\frac{\\alpha_g}{\\alpha_g+\\mu_g(z)}\\right)^{\\alpha_g} \\left(\\frac{\\mu_g(z)}{\\alpha_g+\\mu_g(z)}\\right)^{x_g} \\right] dz\n$$\nThe terms that do not depend on the integration variable $z$ can be factored out of the integral. The decoder mean $\\mu(z)$ is a function of $z$ and must remain inside.\n$$\np(x) = \\left( \\prod_{g=1}^{G} \\frac{\\Gamma(x_g+\\alpha_g)}{\\Gamma(\\alpha_g)\\Gamma(x_g+1)} \\right) \\int_{\\mathbb{R}^L} \\frac{1}{(2\\pi)^{L/2}} \\exp\\left(-\\frac{1}{2} z^T z\\right) \\prod_{g=1}^{G} \\left[ \\left(\\frac{\\alpha_g}{\\alpha_g+\\mu_g(z)}\\right)^{\\alpha_g} \\left(\\frac{\\mu_g(z)}{\\alpha_g+\\mu_g(z)}\\right)^{x_g} \\right] dz\n$$\nThis integral does not have a general analytical closed-form solution (in terms of elementary functions) because the decoder $\\mu(z)$ is typically a complex non-linear function (a neural network). However, the expression itself is the explicit, formal definition of the marginal likelihood. This integral is the central quantity that VAEs are designed to approximate. The problem asks for the explicit marginal likelihood obtained by integrating out $z$, and this expression is precisely that.\n\nThe final answers for $p(x,z)$ and $p(x)$ are thus the expressions derived.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{(2\\pi)^{L/2}} \\exp\\left(-\\frac{1}{2} z^T z\\right) \\prod_{g=1}^{G} \\left[ \\frac{\\Gamma(x_g+\\alpha_g)}{\\Gamma(\\alpha_g)\\Gamma(x_g+1)} \\left(\\frac{\\alpha_g}{\\alpha_g+\\mu_g(z)}\\right)^{\\alpha_g} \\left(\\frac{\\mu_g(z)}{\\alpha_g+\\mu_g(z)}\\right)^{x_g} \\right]\n&\n\\left( \\prod_{g=1}^{G} \\frac{\\Gamma(x_g+\\alpha_g)}{\\Gamma(\\alpha_g)\\Gamma(x_g+1)} \\right) \\int_{\\mathbb{R}^L} \\frac{\\exp\\left(-\\frac{1}{2} z^T z\\right)}{(2\\pi)^{L/2}} \\prod_{g=1}^{G} \\left[ \\left(\\frac{\\alpha_g}{\\alpha_g+\\mu_g(z)}\\right)^{\\alpha_g} \\left(\\frac{\\mu_g(z)}{\\alpha_g+\\mu_g(z)}\\right)^{x_g} \\right] dz\n\\end{pmatrix}\n}\n$$", "id": "4397872"}, {"introduction": "A key strength of VAEs in systems biomedicine is their ability to model and remove unwanted variation, such as batch effects, and to simulate the effects of perturbations. This hands-on exercise transitions from theory to application by having you implement a Conditional VAE (CVAE) that incorporates external information, such as treatment status. You will compute not only the standard ELBO but also a counterfactual reconstruction, providing a powerful demonstration of how CVAEs can be used to predict a biological system's response to a change in conditions. [@problem_id:4397973]", "problem": "You are given a linear-Gaussian Conditional Variational Autoencoder (CVAE) tailored to high-dimensional biological measurements such as gene expression. The CVAE conditions on known batch labels and treatment indicators encoded in a conditioning vector. The goal is to compute, for several test cases, the Evidence Lower BOund (ELBO) for the observed conditioning and the counterfactual reconstruction mean under an altered conditioning. Your implementation must be a complete, runnable program that uses only the provided constants and test inputs, performs deterministic computations based on first principles, and prints the required outputs in the specified format.\n\nModel definition and assumptions:\n- The prior over latent variables is standard normal: $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z}\\,;\\,\\mathbf{0}, \\mathbf{I})$ with latent dimension $L = 2$.\n- The likelihood is conditionally Gaussian with a linear decoder:\n  $$p(\\mathbf{x}\\mid \\mathbf{z}, \\mathbf{c}) = \\mathcal{N}\\!\\left(\\mathbf{x}\\,;\\, \\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c} + \\mathbf{b},\\, \\sigma_x^2 \\mathbf{I}\\right),$$\n  where the data dimension is $D = 4$ and the condition dimension is $C = 3$.\n- The approximate posterior (encoder) is a diagonal Gaussian with parameters affine in $(\\mathbf{x}, \\mathbf{c})$:\n  $$q(\\mathbf{z}\\mid \\mathbf{x}, \\mathbf{c}) = \\mathcal{N}\\!\\left(\\mathbf{z}\\,;\\, \\boldsymbol{\\mu}_z(\\mathbf{x},\\mathbf{c}),\\, \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z(\\mathbf{x},\\mathbf{c}))\\right),$$\n  with\n  $$\\boldsymbol{\\mu}_z(\\mathbf{x},\\mathbf{c}) = \\mathbf{M}\\mathbf{x} + \\mathbf{V}\\mathbf{c} + \\mathbf{a}, \\quad \\log \\boldsymbol{\\sigma}^2_z(\\mathbf{x},\\mathbf{c}) = \\mathbf{S}_x \\mathbf{x} + \\mathbf{S}_c \\mathbf{c} + \\mathbf{s}_0,$$\n  where the logarithm is applied elementwise.\n\nAll parameters are fixed and given below. Vectors are column vectors unless noted.\n\nParameter values to be used:\n- Decoder parameters:\n  - $$\\mathbf{W} = \\begin{bmatrix}\n  0.8 & -0.3\\\\\n  0.1 & 0.5\\\\\n  -0.4 & 0.2\\\\\n  0.0 & 0.3\n  \\end{bmatrix}, \\quad\n  \\mathbf{U} = \\begin{bmatrix}\n  0.6 & -0.2 & 0.3\\\\\n  -0.1 & 0.4 & -0.5\\\\\n  0.2 & 0.1 & -0.2\\\\\n  0.5 & -0.3 & 0.2\n  \\end{bmatrix}, \\quad\n  \\mathbf{b} = \\begin{bmatrix} 0.05\\\\ -0.02\\\\ 0.01\\\\ 0.0 \\end{bmatrix}, \\quad \\sigma_x = 0.1.$$\n- Encoder parameters:\n  - $$\\mathbf{M} = \\begin{bmatrix}\n  0.2 & 0.0 & -0.1 & 0.3\\\\\n  -0.2 & 0.1 & 0.4 & -0.1\n  \\end{bmatrix}, \\quad\n  \\mathbf{V} = \\begin{bmatrix}\n  0.1 & -0.2 & 0.3\\\\\n  0.0 & 0.2 & -0.1\n  \\end{bmatrix}, \\quad\n  \\mathbf{a} = \\begin{bmatrix} 0.05\\\\ -0.05 \\end{bmatrix},$$\n  $$\\mathbf{S}_x = \\begin{bmatrix}\n  -0.1 & 0.2 & 0.0 & -0.2\\\\\n  0.1 & -0.1 & 0.3 & 0.0\n  \\end{bmatrix}, \\quad\n  \\mathbf{S}_c = \\begin{bmatrix}\n  0.05 & 0.05 & -0.1\\\\\n  -0.05 & 0.1 & 0.0\n  \\end{bmatrix}, \\quad\n  \\mathbf{s}_0 = \\begin{bmatrix} -1.0\\\\ -1.2 \\end{bmatrix}.$$\n\nConditioning vector construction:\n- The condition vector $\\mathbf{c} \\in \\mathbb{R}^3$ concatenates batch label and treatment indicator as follows: the first two entries are one-hot batch indicators for two batches (Batch A as $[1,0]^\\top$, Batch B as $[0,1]^\\top$), and the third entry is the treatment indicator in $\\{0,1\\}$.\n\nQuantities to compute:\n- For each test case with observed pair $(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$ and an altered counterfactual condition $\\mathbf{c}_{\\mathrm{alt}}$, compute:\n  1. The Evidence Lower BOund (ELBO) for the observed pair:\n     $$\\mathcal{L}(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}}) = \\mathbb{E}_{q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})}\\!\\left[\\log p(\\mathbf{x}_{\\mathrm{obs}} \\mid \\mathbf{z}, \\mathbf{c}_{\\mathrm{obs}})\\right] - \\mathrm{KL}\\!\\left(q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})\\,\\|\\, p(\\mathbf{z})\\right).$$\n     You must compute the expectation in closed form using linear-Gaussian identities, without Monte Carlo sampling.\n  2. The counterfactual reconstruction mean under the altered condition:\n     $$\\mathbb{E}_{q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})}\\!\\left[\\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b}\\right],$$\n     again in closed form.\n\nMathematical facts to start from:\n- For a Gaussian likelihood $p(\\mathbf{x}\\mid \\mathbf{z}) = \\mathcal{N}(\\mathbf{x}; \\mathbf{A}\\mathbf{z}+\\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I})$ and a diagonal Gaussian $q(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z}; \\boldsymbol{\\mu}_z, \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z))$, the expectation\n  $$\\mathbb{E}_q\\!\\left[\\lVert \\mathbf{x} - (\\mathbf{A}\\mathbf{z}+\\boldsymbol{\\mu}) \\rVert_2^2\\right] = \\lVert \\mathbf{x} - (\\mathbf{A}\\boldsymbol{\\mu}_z+\\boldsymbol{\\mu}) \\rVert_2^2 + \\operatorname{tr}\\!\\left(\\mathbf{A}\\,\\operatorname{diag}(\\boldsymbol{\\sigma}^2_z)\\,\\mathbf{A}^\\top\\right).$$\n- The Kullback–Leibler divergence between a diagonal Gaussian $q(\\mathbf{z}) = \\mathcal{N}(\\boldsymbol{\\mu}_z, \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z))$ and the standard normal $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ is\n  $$\\mathrm{KL}(q\\|p) = \\tfrac{1}{2} \\sum_{i=1}^{L} \\left( \\sigma_{z,i}^2 + \\mu_{z,i}^2 - 1 - \\log \\sigma_{z,i}^2 \\right).$$\n\nTest suite:\n- Use the following three test cases. For each case, you are given $\\mathbf{x}_{\\mathrm{obs}}$, $\\mathbf{c}_{\\mathrm{obs}}$, and $\\mathbf{c}_{\\mathrm{alt}}$.\n  - Case $1$ (happy path):\n    $$\\mathbf{x}_{\\mathrm{obs}}^{(1)} = \\begin{bmatrix} 1.2\\\\ 0.3\\\\ -0.5\\\\ 0.0 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{obs}}^{(1)} = \\begin{bmatrix} 1\\\\ 0\\\\ 1 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{alt}}^{(1)} = \\begin{bmatrix} 1\\\\ 0\\\\ 0 \\end{bmatrix}.$$\n  - Case $2$ (batch counterfactual):\n    $$\\mathbf{x}_{\\mathrm{obs}}^{(2)} = \\begin{bmatrix} -0.2\\\\ 0.5\\\\ 1.0\\\\ -1.0 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{obs}}^{(2)} = \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{alt}}^{(2)} = \\begin{bmatrix} 1\\\\ 0\\\\ 0 \\end{bmatrix}.$$\n  - Case $3$ (edge case with zero input):\n    $$\\mathbf{x}_{\\mathrm{obs}}^{(3)} = \\begin{bmatrix} 0.0\\\\ 0.0\\\\ 0.0\\\\ 0.0 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{obs}}^{(3)} = \\begin{bmatrix} 0\\\\ 1\\\\ 1 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{alt}}^{(3)} = \\begin{bmatrix} 1\\\\ 0\\\\ 1 \\end{bmatrix}.$$\n\nRequired outputs:\n- For each case $k \\in \\{1,2,3\\}$, compute:\n  - The scalar ELBO value $\\mathcal{L}(\\mathbf{x}_{\\mathrm{obs}}^{(k)}, \\mathbf{c}_{\\mathrm{obs}}^{(k)})$.\n  - The $D$-dimensional counterfactual reconstruction mean vector under $\\mathbf{c}_{\\mathrm{alt}}^{(k)}$.\n- Your program should produce a single line of output containing a list of three elements, one per test case, where each element is a two-element list of the form $[\\text{ELBO}, [\\text{cf}_1,\\dots,\\text{cf}_D]]$, with all floating-point numbers rounded to exactly $6$ decimal places. For example:\n  $$[[\\ell_1,[r_{1,1},r_{1,2},r_{1,3},r_{1,4}]],[\\ell_2,[\\dots]],[\\ell_3,[\\dots]]].$$\n\nImplementation constraints:\n- No randomness or sampling is allowed; use only closed-form expectations based on the given linear-Gaussian identities.\n- The code must be entirely self-contained and must not read any input.\n- Use only the Python standard library and the specified numerical libraries.", "solution": "The problem is evaluated as valid, being scientifically grounded, well-posed, and objective. It specifies a linear-Gaussian Conditional Variational Autoencoder (CVAE) and requires the computation of the Evidence Lower Bound (ELBO) and a counterfactual reconstruction mean. All necessary parameters and input data are provided, and the required quantities can be derived in closed form. We shall proceed with the analytical derivation followed by the implementation.\n\nThe objective is to compute two quantities for each test case, which provides an observed data vector $\\mathbf{x}_{\\mathrm{obs}}$, an observed conditioning vector $\\mathbf{c}_{\\mathrm{obs}}$, and a counterfactual conditioning vector $\\mathbf{c}_{\\mathrm{alt}}$. The quantities are:\n1. The ELBO, $\\mathcal{L}(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$.\n2. The counterfactual reconstruction mean, $\\mathbb{E}_{q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})}\\!\\left[\\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b}\\right]$.\n\nFirst, for a given pair $(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$, we must determine the parameters of the approximate posterior distribution $q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$. The problem defines this as a diagonal Gaussian, $q(\\mathbf{z}\\mid \\mathbf{x}, \\mathbf{c}) = \\mathcal{N}\\!\\left(\\mathbf{z}\\,;\\, \\boldsymbol{\\mu}_z(\\mathbf{x},\\mathbf{c}),\\, \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z(\\mathbf{x},\\mathbf{c}))\\right)$. The parameters are computed as follows:\n\nThe posterior mean vector $\\boldsymbol{\\mu}_z \\in \\mathbb{R}^L$ is:\n$$ \\boldsymbol{\\mu}_z(\\mathbf{x}_{\\mathrm{obs}},\\mathbf{c}_{\\mathrm{obs}}) = \\mathbf{M}\\mathbf{x}_{\\mathrm{obs}} + \\mathbf{V}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{a} $$\n\nThe posterior element-wise log-variance vector $\\log \\boldsymbol{\\sigma}^2_z \\in \\mathbb{R}^L$ is:\n$$ \\log \\boldsymbol{\\sigma}^2_z(\\mathbf{x}_{\\mathrm{obs}},\\mathbf{c}_{\\mathrm{obs}}) = \\mathbf{S}_x \\mathbf{x}_{\\mathrm{obs}} + \\mathbf{S}_c \\mathbf{c}_{\\mathrm{obs}} + \\mathbf{s}_0 $$\nFrom this, the posterior variance vector $\\boldsymbol{\\sigma}^2_z \\in \\mathbb{R}^L$ is obtained by element-wise exponentiation:\n$$ \\boldsymbol{\\sigma}^2_z(\\mathbf{x}_{\\mathrm{obs}},\\mathbf{c}_{\\mathrm{obs}}) = \\exp(\\log \\boldsymbol{\\sigma}^2_z(\\mathbf{x}_{\\mathrm{obs}},\\mathbf{c}_{\\mathrm{obs}})) $$\nFor brevity, we will denote these specific posterior parameters as $\\boldsymbol{\\mu}_z$ and $\\boldsymbol{\\sigma}_z^2$, and the corresponding distribution as $q(\\mathbf{z})$.\n\nWith the posterior parameters determined, we can now derive the expressions for the two required quantities.\n\n**1. The Evidence Lower Bound (ELBO)**\n\nThe ELBO is defined as:\n$$ \\mathcal{L}(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}}) = \\mathbb{E}_{q(\\mathbf{z})}\\!\\left[\\log p(\\mathbf{x}_{\\mathrm{obs}} \\mid \\mathbf{z}, \\mathbf{c}_{\\mathrm{obs}})\\right] - \\mathrm{KL}\\!\\left(q(\\mathbf{z})\\,\\|\\, p(\\mathbf{z})\\right) $$\n\nWe will compute each of the two terms separately.\n\nThe second term is the Kullback-Leibler (KL) divergence between the approximate posterior $q(\\mathbf{z}) = \\mathcal{N}(\\boldsymbol{\\mu}_z, \\operatorname{diag}(\\boldsymbol{\\sigma}_z^2))$ and the standard normal prior $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. The provided formula is:\n$$ \\mathrm{KL}(q\\|p) = \\frac{1}{2} \\sum_{i=1}^{L} \\left( \\sigma_{z,i}^2 + \\mu_{z,i}^2 - 1 - \\log \\sigma_{z,i}^2 \\right) $$\nwhere $\\mu_{z,i}$ and $\\sigma_{z,i}^2$ are the elements of the vectors $\\boldsymbol{\\mu}_z$ and $\\boldsymbol{\\sigma}_z^2$ computed previously.\n\nThe first term is the expected reconstruction log-likelihood. The likelihood is given as $p(\\mathbf{x}\\mid \\mathbf{z}, \\mathbf{c}) = \\mathcal{N}\\!\\left(\\mathbf{x}\\,;\\, \\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c} + \\mathbf{b},\\, \\sigma_x^2 \\mathbf{I}\\right)$. The log-probability is:\n$$ \\log p(\\mathbf{x} \\mid \\mathbf{z}, \\mathbf{c}) = -\\frac{D}{2} \\log(2\\pi \\sigma_x^2) - \\frac{1}{2\\sigma_x^2} \\|\\mathbf{x} - (\\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c} + \\mathbf{b})\\|^2_2 $$\nWe must compute the expectation of this quantity under $q(\\mathbf{z})$. The first term is a constant with respect to $\\mathbf{z}$.\n$$ \\mathbb{E}_{q(\\mathbf{z})}\\!\\left[\\log p(\\mathbf{x}_{\\mathrm{obs}} \\mid \\mathbf{z}, \\mathbf{c}_{\\mathrm{obs}})\\right] = -\\frac{D}{2} \\log(2\\pi \\sigma_x^2) - \\frac{1}{2\\sigma_x^2} \\mathbb{E}_{q(\\mathbf{z})}\\!\\left[ \\|\\mathbf{x}_{\\mathrm{obs}} - (\\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{b})\\|^2_2 \\right] $$\nTo evaluate the expectation of the squared norm, we use the provided identity: $\\mathbb{E}_q\\!\\left[\\lVert \\mathbf{x} - (\\mathbf{A}\\mathbf{z}+\\boldsymbol{\\mu}) \\rVert_2^2\\right] = \\lVert \\mathbf{x} - (\\mathbf{A}\\boldsymbol{\\mu}_z+\\boldsymbol{\\mu}) \\rVert_2^2 + \\operatorname{tr}\\!\\left(\\mathbf{A}\\,\\operatorname{diag}(\\boldsymbol{\\sigma}^2_z)\\,\\mathbf{A}^\\top\\right)$. We make the substitutions: $\\mathbf{x} \\rightarrow \\mathbf{x}_{\\mathrm{obs}}$, $\\mathbf{A} \\rightarrow \\mathbf{W}$, and $\\boldsymbol{\\mu} \\rightarrow \\mathbf{U}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{b}$. The expectation becomes:\n$$ \\|\\mathbf{x}_{\\mathrm{obs}} - (\\mathbf{W}\\boldsymbol{\\mu}_z + \\mathbf{U}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{b})\\|^2_2 + \\operatorname{tr}\\!\\left(\\mathbf{W}\\,\\operatorname{diag}(\\boldsymbol{\\sigma}^2_z)\\,\\mathbf{W}^\\top\\right) $$\nThe first term is the squared Euclidean distance between the input $\\mathbf{x}_{\\mathrm{obs}}$ and its reconstruction from the posterior mean. The trace term can be simplified using the cyclic property of the trace and the diagonal nature of the covariance matrix $\\boldsymbol{\\Sigma}_z = \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z)$:\n$$ \\operatorname{tr}(\\mathbf{W} \\boldsymbol{\\Sigma}_z \\mathbf{W}^\\top) = \\operatorname{tr}(\\mathbf{W}^\\top \\mathbf{W} \\boldsymbol{\\Sigma}_z) = \\sum_{i=1}^L (\\mathbf{W}^\\top \\mathbf{W})_{ii} \\sigma_{z,i}^2 = \\sum_{i=1}^L \\|\\mathbf{W}_{:,i}\\|_2^2 \\sigma_{z,i}^2 $$\nwhere $\\mathbf{W}_{:,i}$ is the $i$-th column of $\\mathbf{W}$.\n\nCombining everything, the ELBO is:\n$$ \\mathcal{L} = \\left[ -\\frac{D}{2}\\log(2\\pi\\sigma_x^2) - \\frac{1}{2\\sigma_x^2}\\left( \\|\\mathbf{x}_{\\mathrm{obs}} - (\\mathbf{W}\\boldsymbol{\\mu}_z + \\mathbf{U}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{b})\\|^2_2 + \\sum_{i=1}^L \\|\\mathbf{W}_{:,i}\\|_2^2 \\sigma_{z,i}^2 \\right) \\right] - \\left[ \\frac{1}{2}\\sum_{i=1}^L(\\mu_{z,i}^2 + \\sigma_{z,i}^2 - 1 - \\log\\sigma_{z,i}^2) \\right] $$\n\n**2. The Counterfactual Reconstruction Mean**\n\nThe second quantity to be computed is the mean of the reconstructed data distribution, where the latent variable $\\mathbf{z}$ is drawn from the posterior inferred from the observed data $(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$, but the decoding is performed using the altered condition $\\mathbf{c}_{\\mathrm{alt}}$.\n$$ \\mathbf{x}_{\\mathrm{cf}} = \\mathbb{E}_{q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})}\\!\\left[ \\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b} \\right] $$\nBy linearity of expectation, and since $\\mathbf{c}_{\\mathrm{alt}}$ and $\\mathbf{b}$ are constant with respect to the expectation over $\\mathbf{z}$:\n$$ \\mathbf{x}_{\\mathrm{cf}} = \\mathbf{W} \\mathbb{E}_{q(\\mathbf{z})}[\\mathbf{z}] + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b} $$\nThe expectation of $\\mathbf{z}$ under $q(\\mathbf{z})$ is simply its mean, $\\boldsymbol{\\mu}_z$. Therefore, the counterfactual reconstruction mean is:\n$$ \\mathbf{x}_{\\mathrm{cf}} = \\mathbf{W}\\boldsymbol{\\mu}_z + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b} $$\nwhere $\\boldsymbol{\\mu}_z$ is the posterior mean computed from $(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$.\n\nThe implementation will follow these derived closed-form expressions for each test case. All vector and matrix operations will be carried out using the `numpy` library.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the CVAE problem by computing the ELBO and counterfactual reconstruction mean\n    for three test cases based on the provided linear-Gaussian model.\n    \"\"\"\n    \n    # Model dimensions\n    L = 2  # Latent dimension\n    D = 4  # Data dimension\n    C = 3  # Condition dimension\n    \n    # Decoder parameters\n    W = np.array([\n        [0.8, -0.3],\n        [0.1, 0.5],\n        [-0.4, 0.2],\n        [0.0, 0.3]\n    ])\n    U = np.array([\n        [0.6, -0.2, 0.3],\n        [-0.1, 0.4, -0.5],\n        [0.2, 0.1, -0.2],\n        [0.5, -0.3, 0.2]\n    ])\n    b = np.array([0.05, -0.02, 0.01, 0.0]).reshape(D, 1)\n    sigma_x = 0.1\n    \n    # Encoder parameters\n    M = np.array([\n        [0.2, 0.0, -0.1, 0.3],\n        [-0.2, 0.1, 0.4, -0.1]\n    ])\n    V = np.array([\n        [0.1, -0.2, 0.3],\n        [0.0, 0.2, -0.1]\n    ])\n    a = np.array([0.05, -0.05]).reshape(L, 1)\n    \n    S_x = np.array([\n        [-0.1, 0.2, 0.0, -0.2],\n        [0.1, -0.1, 0.3, 0.0]\n    ])\n    S_c = np.array([\n        [0.05, 0.05, -0.1],\n        [-0.05, 0.1, 0.0]\n    ])\n    s_0 = np.array([-1.0, -1.2]).reshape(L, 1)\n    \n    # Test suite\n    test_cases = [\n        {\n            \"x_obs\": np.array([1.2, 0.3, -0.5, 0.0]).reshape(D, 1),\n            \"c_obs\": np.array([1, 0, 1]).reshape(C, 1),\n            \"c_alt\": np.array([1, 0, 0]).reshape(C, 1)\n        },\n        {\n            \"x_obs\": np.array([-0.2, 0.5, 1.0, -1.0]).reshape(D, 1),\n            \"c_obs\": np.array([0, 1, 0]).reshape(C, 1),\n            \"c_alt\": np.array([1, 0, 0]).reshape(C, 1)\n        },\n        {\n            \"x_obs\": np.array([0.0, 0.0, 0.0, 0.0]).reshape(D, 1),\n            \"c_obs\": np.array([0, 1, 1]).reshape(C, 1),\n            \"c_alt\": np.array([1, 0, 1]).reshape(C, 1)\n        }\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        x_obs = case[\"x_obs\"]\n        c_obs = case[\"c_obs\"]\n        c_alt = case[\"c_alt\"]\n        \n        # 1. Compute posterior parameters (mu_z, sigma_z^2)\n        # Use observed data and conditions\n        mu_z = M @ x_obs + V @ c_obs + a\n        log_sigma_z_sq = S_x @ x_obs + S_c @ c_obs + s_0\n        sigma_z_sq = np.exp(log_sigma_z_sq)\n        \n        # 2. Compute the KL divergence term of the ELBO\n        # KL(q(z|x,c) || p(z))\n        kl_div = 0.5 * np.sum(sigma_z_sq + mu_z**2 - 1 - log_sigma_z_sq)\n        \n        # 3. Compute the expected log-likelihood term of the ELBO\n        # E_q[log p(x|z,c)]\n        \n        # Term 1: Reconstruction error from posterior mean\n        recon_mean_obs = W @ mu_z + U @ c_obs + b\n        recon_error_sq_norm = np.sum((x_obs - recon_mean_obs)**2)\n        \n        # Term 2: Trace term from variance propagation\n        w_col_sq_norms = np.sum(W**2, axis=0) # shape (L,)\n        trace_term = np.sum(w_col_sq_norms * sigma_z_sq.flatten())\n        \n        # Constant from Gaussian PDF\n        log_p_constant = -D / 2.0 * np.log(2 * np.pi * sigma_x**2)\n        \n        # Combine parts for expected log-likelihood\n        expected_log_p = log_p_constant - (1 / (2 * sigma_x**2)) * (recon_error_sq_norm + trace_term)\n        \n        # 4. Compute the ELBO\n        elbo = expected_log_p - kl_div\n        \n        # 5. Compute the counterfactual reconstruction mean\n        # E_q[decoder(z, c_alt)]\n        cf_recon_mean = W @ mu_z + U @ c_alt + b\n        \n        # Format the results as required\n        elbo_rounded = round(elbo, 6)\n        cf_recon_mean_list = [round(x, 6) for x in cf_recon_mean.flatten()]\n        \n        results.append([elbo_rounded, cf_recon_mean_list])\n        \n    # Print the final result in the exact required format\n    # Manual string construction to avoid spaces added by standard list-to-string conversion\n    result_strings = []\n    for elbo_val, cf_mean_list in results:\n        cf_mean_str = f\"[{','.join(f'{x:.6f}' for x in cf_mean_list)}]\"\n        result_strings.append(f\"[{elbo_val:.6f},{cf_mean_str}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "4397973"}]}