{"hands_on_practices": [{"introduction": "A central goal of experimental design is to gain the most information possible about unknown parameters. This first practice serves as a cornerstone for Bayesian Optimal Experimental Design (OED), demonstrating how to quantify the expected reduction in uncertainty for a simple linear model. By deriving the expected posterior variance, you will see how it is possible to evaluate and compare the utility of different experimental designs *before* committing resources, a key principle in planning efficient experiments [@problem_id:4313149].", "problem": "In systems biomedicine, consider the task of Active Learning (AL) for Optimal Experimental Design (OED) in calibrating a linear, dose-response relationship for a single transcription factor regulating a reporter gene. Let the measurement model be $y=\\theta x+\\varepsilon$, where $x$ is a controllable experimental design variable (for example, ligand concentration), $\\theta$ is an unknown sensitivity parameter, and $\\varepsilon$ is measurement noise. Assume $\\varepsilon\\sim\\mathcal{N}(0,\\sigma^{2})$ and the prior on $\\theta$ is $\\theta\\sim\\mathcal{N}(\\mu_{0},\\tau_{0}^{2})$. You perform one experiment at design $x$ and obtain one measurement $y$.\n\nStarting from Bayes' rule and the definitions of Gaussian likelihood and Gaussian prior, derive the posterior distribution $p(\\theta\\mid y,x)$ by explicitly completing the square in the exponent, and then compute the expected posterior variance after observing one measurement at design $x$, defined as $\\mathbb{E}_{y\\mid x}\\big[\\operatorname{Var}(\\theta\\mid y,x)\\big]$. Express your final answer as a closed-form analytic expression in terms of $x$, $\\sigma^{2}$, and $\\tau_{0}^{2}$. No numerical evaluation is required. The final answer must be a single expression with no units.", "solution": "The user has requested the derivation of the expected posterior variance for a parameter in a linear model, a task central to Bayesian optimal experimental design. I will first validate the problem statement.\n\n### Step 1: Extract Givens\n- Measurement model: $y = \\theta x + \\varepsilon$\n- Experimental design variable: $x$\n- Unknown parameter: $\\theta$\n- Measurement noise distribution: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$\n- Prior distribution of the parameter: $\\theta \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$\n- Task: Derive the posterior distribution $p(\\theta \\mid y, x)$ by completing the square.\n- Task: Compute the expected posterior variance, $\\mathbb{E}_{y \\mid x}[\\operatorname{Var}(\\theta \\mid y, x)]$.\n- Final answer format: A closed-form expression in terms of $x$, $\\sigma^2$, and $\\tau_0^2$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It presents a standard Bayesian inference scenario involving conjugate Gaussian distributions, which is a fundamental and widely used model in many scientific disciplines, including systems biomedicine. The model $y = \\theta x + \\varepsilon$ is a basic linear regression. The use of a Gaussian prior for the parameter $\\theta$ and Gaussian likelihood for the data $y$ is a canonical example. The quantity to be computed, the expected posterior variance, is a standard criterion (related to A-optimality) in the field of optimal experimental design. All components are clearly defined, and there are no contradictions, ambiguities, or factual errors. The problem is a valid, solvable mathematical statistics problem.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the detailed solution.\n\n### Derivation of the Posterior Distribution and Expected Posterior Variance\n\nThe objective is to find the expected posterior variance $\\mathbb{E}_{y\\mid x}\\big[\\operatorname{Var}(\\theta\\mid y,x)\\big]$. We begin by deriving the posterior distribution $p(\\theta\\mid y,x)$ using Bayes' rule.\n\nThe two components required for Bayes' rule are the likelihood of the data and the prior distribution of the parameter.\n\n$1$. **Likelihood**: The measurement model is $y = \\theta x + \\varepsilon$, where the noise term $\\varepsilon$ is drawn from a normal distribution with mean $0$ and variance $\\sigma^2$, i.e., $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$. This implies that for a given $\\theta$ and $x$, the measurement $y$ is also normally distributed. Specifically, $y \\mid \\theta, x \\sim \\mathcal{N}(\\theta x, \\sigma^2)$. The likelihood function $p(y \\mid \\theta, x)$ is therefore:\n$$ p(y \\mid \\theta, x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y - \\theta x)^2}{2\\sigma^2} \\right) $$\n\n$2$. **Prior**: The prior belief about the parameter $\\theta$ is given as a normal distribution, $\\theta \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$. The prior probability density function $p(\\theta)$ is:\n$$ p(\\theta) = \\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left( -\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2} \\right) $$\n\n$3$. **Posterior Distribution**: According to Bayes' rule, the posterior distribution $p(\\theta \\mid y, x)$ is proportional to the product of the likelihood and the prior:\n$$ p(\\theta \\mid y, x) \\propto p(y \\mid \\theta, x) p(\\theta) $$\nSubstituting the expressions for the likelihood and prior, and dropping the normalization constants which do not depend on $\\theta$:\n$$ p(\\theta \\mid y, x) \\propto \\exp\\left( -\\frac{(y - \\theta x)^2}{2\\sigma^2} \\right) \\exp\\left( -\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2} \\right) $$\nWe can combine the exponents:\n$$ p(\\theta \\mid y, x) \\propto \\exp\\left( -\\frac{(y - \\theta x)^2}{2\\sigma^2} - \\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2} \\right) $$\nLet's focus on the argument of the exponential, which we will denote as $\\Phi$:\n$$ \\Phi = -\\frac{1}{2} \\left( \\frac{(y - \\theta x)^2}{\\sigma^2} + \\frac{(\\theta - \\mu_0)^2}{\\tau_0^2} \\right) $$\nTo find the form of the posterior distribution, we complete the square with respect to $\\theta$. We expand the squared terms inside the parenthesis:\n$$ \\Phi = -\\frac{1}{2} \\left( \\frac{y^2 - 2y\\theta x + \\theta^2 x^2}{\\sigma^2} + \\frac{\\theta^2 - 2\\theta\\mu_0 + \\mu_0^2}{\\tau_0^2} \\right) $$\nNow, we collect terms based on the powers of $\\theta$:\n$$ \\Phi = -\\frac{1}{2} \\left[ \\theta^2 \\left( \\frac{x^2}{\\sigma^2} + \\frac{1}{\\tau_0^2} \\right) - 2\\theta \\left( \\frac{yx}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\right) + \\left( \\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2} \\right) \\right] $$\nThe posterior distribution for $\\theta$ is known to be a Gaussian, as the prior and likelihood are conjugate. A general Gaussian density for $\\theta$ has an exponent of the form $-\\frac{(\\theta - \\mu_1)^2}{2\\tau_1^2}$, where $\\mu_1$ is the posterior mean and $\\tau_1^2$ is the posterior variance. Expanding this gives:\n$$ -\\frac{1}{2\\tau_1^2}(\\theta^2 - 2\\theta\\mu_1 + \\mu_1^2) = -\\frac{1}{2} \\left[ \\theta^2 \\left(\\frac{1}{\\tau_1^2}\\right) - 2\\theta \\left(\\frac{\\mu_1}{\\tau_1^2}\\right) + \\frac{\\mu_1^2}{\\tau_1^2} \\right] $$\nBy comparing the coefficient of the $\\theta^2$ term in our expression for $\\Phi$ with the general form, we can identify the reciprocal of the posterior variance, which is also known as the posterior precision.\n$$ \\frac{1}{\\tau_1^2} = \\frac{x^2}{\\sigma^2} + \\frac{1}{\\tau_0^2} $$\nThe posterior variance, $\\operatorname{Var}(\\theta \\mid y, x)$, is $\\tau_1^2$. We can solve for it:\n$$ \\frac{1}{\\tau_1^2} = \\frac{x^2\\tau_0^2 + \\sigma^2}{\\sigma^2\\tau_0^2} $$\n$$ \\tau_1^2 = \\operatorname{Var}(\\theta \\mid y, x) = \\frac{\\sigma^2\\tau_0^2}{\\sigma^2 + x^2\\tau_0^2} $$\nAlthough not required for the final answer, we could also find the posterior mean $\\mu_1$ by comparing the coefficients of the $\\theta$ term:\n$$ \\frac{\\mu_1}{\\tau_1^2} = \\frac{yx}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\implies \\mu_1 = \\tau_1^2 \\left( \\frac{yx}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\right) = \\frac{\\tau_0^2 yx + \\sigma^2 \\mu_0}{\\sigma^2 + x^2\\tau_0^2} $$\nSo, the posterior distribution is $p(\\theta \\mid y, x) = \\mathcal{N}(\\mu_1, \\tau_1^2)$.\n\n$4$. **Expected Posterior Variance**: The final step is to compute the expected posterior variance, $\\mathbb{E}_{y\\mid x}\\big[\\operatorname{Var}(\\theta\\mid y,x)\\big]$. The expectation is taken over the distribution of possible measurements $y$ for a given experimental design $x$. This distribution is the marginal likelihood or evidence, $p(y \\mid x) = \\int p(y \\mid \\theta, x)p(\\theta) d\\theta$.\n\nThe posterior variance we derived is:\n$$ \\operatorname{Var}(\\theta \\mid y, x) = \\frac{\\sigma^2\\tau_0^2}{\\sigma^2 + x^2\\tau_0^2} $$\nCrucially, we observe that this expression for the posterior variance depends only on the experimental design $x$, the measurement noise variance $\\sigma^2$, and the prior variance $\\tau_0^2$. It does **not** depend on the specific measurement value $y$.\n\nTherefore, when we take the expectation with respect to $y$, we are taking the expectation of a quantity that is constant with respect to $y$. For any random variable $Y$ and any constant $C$, the expectation is $\\mathbb{E}[C] = C$.\n$$ \\mathbb{E}_{y\\mid x}\\big[\\operatorname{Var}(\\theta\\mid y,x)\\big] = \\mathbb{E}_{y\\mid x}\\left[\\frac{\\sigma^2\\tau_0^2}{\\sigma^2 + x^2\\tau_0^2}\\right] $$\nSince the term inside the expectation is constant with respect to $y$, the expectation is simply the term itself:\n$$ \\mathbb{E}_{y\\mid x}\\big[\\operatorname{Var}(\\theta\\mid y,x)\\big] = \\frac{\\sigma^2\\tau_0^2}{\\sigma^2 + x^2\\tau_0^2} $$\nThis result signifies that for this linear-Gaussian model, the reduction in uncertainty about the parameter $\\theta$ can be calculated before the experiment is even performed. It depends only on where we choose to measure (the design $x$), not on the outcome of the measurement ($y$). This is a key principle used in optimal experimental design to choose $x$ such that this expected posterior variance is minimized.", "answer": "$$\n\\boxed{\\frac{\\sigma^{2}\\tau_{0}^{2}}{\\sigma^{2} + x^{2}\\tau_{0}^{2}}}\n$$", "id": "4313149"}, {"introduction": "Biological processes are inherently dynamic, and our models must often reflect this. This practice elevates the principles of OED to the domain of systems described by ordinary differential equations (ODEs), a common scenario in systems biomedicine. You will use sensitivity analysis to construct the Fisher Information Matrix and learn how its properties relate to the crucial concept of parameter identifiability, ensuring your experimental design can actually distinguish the effects of different parameters [@problem_id:4313143].", "problem": "A signaling biomarker in a mammalian cell line is modeled as a single-compartment process with constant ligand infusion. The state dynamics and the measurement model are given by the ordinary differential equation and output equation\n$$\n\\frac{dx(t)}{dt} \\;=\\; -\\theta_{1}\\,x(t) \\;+\\; \\theta_{2}\\,u(t), \\quad x(0)=0, \\quad y(t)=x(t),\n$$\nwhere $u(t)=U$ is a known constant infusion amplitude with $U0$, and the parameter vector is $\\theta = (\\theta_{1},\\theta_{2})$ with $\\theta_{1}0$ and $\\theta_{2}\\neq 0$. Measurements are collected at two sampling times $t_{1}0$ and $t_{2}0$, with independent, homoscedastic Gaussian noise of variance $\\sigma^{2}$ added to each $y(t_{i})$. Define the sensitivity matrix with entries $S_{ij}=\\partial y(t_{i};\\theta)/\\partial \\theta_{j}$, $i\\in\\{1,2\\}$, $j\\in\\{1,2\\}$.\n\nStarting from the model equations and core definitions of sensitivities and local identifiability in nonlinear regression, do the following:\n\n1. Solve for $y(t)$ and derive the exact expressions for $\\partial y(t)/\\partial \\theta_{1}$ and $\\partial y(t)/\\partial \\theta_{2}$.\n\n2. Using only first principles, derive conditions on $(t_{1},t_{2},U,\\theta_{1},\\theta_{2})$ under which the $2\\times 2$ sensitivity matrix $S$ has full column rank. Then, explain how full column rank of $S$ at a design $(t_{1},t_{2})$ relates to local structural identifiability of $\\theta$ at a nominal parameter in the sense of the linearized inverse problem.\n\n3. In the context of active learning for optimal experimental design, suppose you choose $t_{2}$ to be sufficiently large so that $t_{2}\\gg 1/\\theta_{1}$ (i.e., $e^{-\\theta_{1}t_{2}}\\approx 0$). For this two-time design, maximize the determinant of the Fisher Information Matrix (FIM) for Gaussian noise, which is defined as $F=\\sigma^{-2} S^{\\top} S$. Show that maximizing $\\det(F)$ is equivalent to maximizing $|\\det(S)|$, and then determine the exact analytic expression for the optimal first sampling time $t_{1}^{\\star}$ (expressed in terms of $\\theta_{1}$) that maximizes $|\\det(S)|$ in the limiting regime $t_{2}\\to\\infty$.\n\nYour final answer must be the single closed-form analytic expression for $t_{1}^{\\star}$. Do not round; provide the exact expression. If you choose to report a numerical value, express time in seconds, but the final boxed answer must be the symbolic expression only.", "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of chemical kinetics and systems theory, well-posed, and objective. The problem is a standard exercise in sensitivity analysis and optimal experimental design for parameter estimation in dynamical systems. We may therefore proceed with a full solution.\n\nThe analysis is structured into three parts as requested by the problem statement.\n\nPart 1: Solution of the ODE and derivation of sensitivity functions.\n\nThe state equation is a first-order linear ordinary differential equation (ODE) with a constant input term:\n$$\n\\frac{dx(t)}{dt} = -\\theta_{1}x(t) + \\theta_{2}u(t)\n$$\nGiven that the input $u(t)$ is a constant infusion amplitude $U$, the equation becomes:\n$$\n\\frac{dx(t)}{dt} + \\theta_{1}x(t) = \\theta_{2}U\n$$\nThis is a standard form that can be solved using an integrating factor, $I(t) = \\exp(\\int \\theta_{1} dt) = \\exp(\\theta_{1}t)$. Multiplying the ODE by $I(t)$ yields:\n$$\n\\exp(\\theta_{1}t)\\frac{dx(t)}{dt} + \\theta_{1}\\exp(\\theta_{1}t)x(t) = \\theta_{2}U\\exp(\\theta_{1}t)\n$$\nThe left-hand side is the result of the product rule for differentiation, $\\frac{d}{dt}[x(t)\\exp(\\theta_{1}t)]$. Thus,\n$$\n\\frac{d}{dt}\\left[x(t)\\exp(\\theta_{1}t)\\right] = \\theta_{2}U\\exp(\\theta_{1}t)\n$$\nIntegrating both sides with respect to $t$ gives:\n$$\nx(t)\\exp(\\theta_{1}t) = \\int \\theta_{2}U\\exp(\\theta_{1}t) dt = \\frac{\\theta_{2}U}{\\theta_{1}}\\exp(\\theta_{1}t) + C\n$$\nwhere $C$ is the constant of integration. Dividing by $\\exp(\\theta_1 t)$ gives the general solution for $x(t)$:\n$$\nx(t) = \\frac{\\theta_{2}U}{\\theta_{1}} + C\\exp(-\\theta_{1}t)\n$$\nWe use the initial condition $x(0)=0$ to find $C$:\n$$\nx(0) = 0 = \\frac{\\theta_{2}U}{\\theta_{1}} + C\\exp(0) \\implies C = -\\frac{\\theta_{2}U}{\\theta_{1}}\n$$\nSubstituting $C$ back into the general solution, we obtain the particular solution for $x(t)$, which is equal to the measurement function $y(t)$:\n$$\ny(t) = x(t) = \\frac{\\theta_{2}U}{\\theta_{1}} - \\frac{\\theta_{2}U}{\\theta_{1}}\\exp(-\\theta_{1}t) = \\frac{\\theta_{2}U}{\\theta_{1}}(1 - \\exp(-\\theta_{1}t))\n$$\nNext, we derive the sensitivity functions, which are the partial derivatives of $y(t)$ with respect to the parameters $\\theta_1$ and $\\theta_2$.\n\nThe sensitivity with respect to $\\theta_1$ is $\\frac{\\partial y(t)}{\\partial \\theta_{1}}$. Using the product and chain rules:\n$$\n\\frac{\\partial y(t)}{\\partial \\theta_{1}} = \\frac{\\partial}{\\partial \\theta_{1}}\\left[ \\theta_{2}U \\left( \\frac{1 - \\exp(-\\theta_{1}t)}{\\theta_{1}} \\right) \\right]\n$$\n$$\n\\frac{\\partial y(t)}{\\partial \\theta_{1}} = \\theta_{2}U \\left[ \\frac{\\theta_{1}\\frac{\\partial}{\\partial\\theta_{1}}(1 - \\exp(-\\theta_{1}t)) - (1 - \\exp(-\\theta_{1}t))\\frac{\\partial}{\\partial\\theta_{1}}(\\theta_{1})}{\\theta_{1}^2} \\right]\n$$\n$$\n\\frac{\\partial y(t)}{\\partial \\theta_{1}} = \\theta_{2}U \\left[ \\frac{\\theta_{1}(t\\exp(-\\theta_{1}t)) - (1 - \\exp(-\\theta_{1}t))}{\\theta_{1}^2} \\right]\n$$\n$$\n\\frac{\\partial y(t)}{\\partial \\theta_{1}} = \\frac{\\theta_{2}U}{\\theta_{1}^2} \\left[ \\theta_{1}t\\exp(-\\theta_{1}t) - 1 + \\exp(-\\theta_{1}t) \\right]\n$$\n$$\n\\frac{\\partial y(t)}{\\partial \\theta_{1}} = \\frac{\\theta_{2}U}{\\theta_{1}^2} \\left[ -1 + (1 + \\theta_{1}t)\\exp(-\\theta_{1}t) \\right]\n$$\nThe sensitivity with respect to $\\theta_2$ is $\\frac{\\partial y(t)}{\\partial \\theta_{2}}$:\n$$\n\\frac{\\partial y(t)}{\\partial \\theta_{2}} = \\frac{\\partial}{\\partial \\theta_{2}}\\left[ \\frac{\\theta_{2}U}{\\theta_{1}}(1 - \\exp(-\\theta_{1}t)) \\right] = \\frac{U}{\\theta_{1}}(1 - \\exp(-\\theta_{1}t))\n$$\n\nPart 2: Conditions for full rank of the sensitivity matrix and relation to identifiability.\n\nThe sensitivity matrix $S$ for measurements at times $t_1$ and $t_2$ is a $2 \\times 2$ matrix:\n$$\nS = \\begin{pmatrix} \\frac{\\partial y(t_{1})}{\\partial \\theta_{1}}  \\frac{\\partial y(t_{1})}{\\partial \\theta_{2}} \\\\ \\frac{\\partial y(t_{2})}{\\partial \\theta_{1}}  \\frac{\\partial y(t_{2})}{\\partial \\theta_{2}} \\end{pmatrix}\n$$\n$S$ has full column rank if and only if its columns are linearly independent. For a square matrix, this is equivalent to the matrix being invertible, which requires its determinant to be non-zero, $\\det(S) \\neq 0$. The columns are linearly dependent if one is a scalar multiple of the other. The rows are linearly dependent if the ratio of their components is equal, which is written as:\n$$\n\\frac{\\partial y(t_{1})/\\partial \\theta_{1}}{\\partial y(t_{1})/\\partial \\theta_{2}} = \\frac{\\partial y(t_{2})/\\partial \\theta_{1}}{\\partial y(t_{2})/\\partial \\theta_{2}}\n$$\nThis condition is equivalent to $\\det(S) = 0$. Let's define the ratio function $g(t) = \\frac{\\partial y(t)/\\partial \\theta_{1}}{\\partial y(t)/\\partial \\theta_{2}}$:\n$$\ng(t) = \\frac{\\frac{\\theta_{2}U}{\\theta_{1}^2} \\left[ -1 + (1 + \\theta_{1}t)\\exp(-\\theta_{1}t) \\right]}{\\frac{U}{\\theta_{1}}(1 - \\exp(-\\theta_{1}t))} = \\frac{\\theta_{2}}{\\theta_{1}} \\left( \\frac{-1 + (1 + \\theta_{1}t)\\exp(-\\theta_{1}t)}{1 - \\exp(-\\theta_{1}t)} \\right)\n$$\nThe matrix $S$ has full rank if and only if $g(t_1) \\neq g(t_2)$. This condition holds for any $t_1 \\neq t_2$ if $g(t)$ is a strictly monotonic function for $t0$. Let's analyze the monotonicity of the core part of $g(t)$, letting $z = \\theta_{1}t  0$:\n$$\nh(z) = \\frac{-1 + (1+z)\\exp(-z)}{1-\\exp(-z)}\n$$\nWe compute the derivative $h'(z)$:\n$$\nh'(z) = \\frac{[-z\\exp(-z)](1-\\exp(-z)) - [-1 + (1+z)\\exp(-z)](\\exp(-z))}{(1-\\exp(-z))^2}\n$$\n$$\nh'(z) = \\frac{-z\\exp(-z) + z\\exp(-2z) + \\exp(-z) - (1+z)\\exp(-2z)}{(1-\\exp(-z))^2}\n$$\n$$\nh'(z) = \\frac{\\exp(-z)(1-z) - \\exp(-2z)}{(1-\\exp(-z))^2} = \\frac{\\exp(-z)(1-z-\\exp(-z))}{(1-\\exp(-z))^2}\n$$\nThe sign of $h'(z)$ is determined by the term $k(z) = 1-z-\\exp(-z)$. We analyze $k(z)$ for $z0$.\n$k(0) = 1-0-1 = 0$. The derivative is $k'(z) = -1+\\exp(-z)$. For $z0$, $\\exp(-z)1$, so $k'(z)0$. This means $k(z)$ is strictly decreasing for $z0$. Since $k(0)=0$, $k(z)0$ for all $z0$.\nConsequently, $h'(z)0$ for all $z0$. This proves that $h(z)$ is strictly monotonic. Since $g(t)$ is a scaled version of $h(\\theta_1 t)$, $g(t)$ is also strictly monotonic for $t0$.\nTherefore, $g(t_1) = g(t_2)$ if and only if $t_1 = t_2$. For $S$ to have full rank, we require $t_1 \\neq t_2$.\nThe problem specifies $t_1  0$, $t_2  0$, $U0$, $\\theta_10$, and $\\theta_2 \\neq 0$. These ensure the denominators in $g(t)$ are non-zero and the sensitivities are not identically zero.\nThe condition for full rank is thus $t_1 \\neq t_2$.\n\nThe relationship to local structural identifiability is direct. Local structural identifiability at a nominal parameter vector $\\theta$ implies that there exists a neighborhood around $\\theta$ where no other parameter vector yields the same model output. In the context of nonlinear regression, a change in parameters $\\Delta\\theta$ results in a first-order change in the vector of observables $\\mathbf{y} = (y(t_1), y(t_2))^\\top$ given by $\\Delta\\mathbf{y} \\approx S \\Delta\\theta$. To uniquely determine the parameter change $\\Delta\\theta$ from an observed change $\\Delta\\mathbf{y}$, one must be able to solve this linear system for $\\Delta\\theta$. A unique solution exists if and only if the matrix $S$ has full column rank. Therefore, full column rank of the sensitivity matrix for a given experimental design $(t_1, t_2)$ is the necessary and sufficient condition for local structural identifiability of the parameters $\\theta$ with that design.\n\nPart 3: Optimal experimental design.\n\nThe Fisher Information Matrix (FIM) for homoscedastic Gaussian noise is $F = \\frac{1}{\\sigma^2} S^{\\top}S$. We wish to maximize its determinant with respect to the choice of $t_1$ in the limit $t_2 \\to \\infty$.\nUsing properties of determinants, where $S$ is a $2\\times 2$ matrix:\n$$\n\\det(F) = \\det\\left(\\frac{1}{\\sigma^2} S^{\\top}S\\right) = \\left(\\frac{1}{\\sigma^2}\\right)^2 \\det(S^{\\top}S) = \\frac{1}{\\sigma^4} \\det(S^{\\top})\\det(S) = \\frac{1}{\\sigma^4} (\\det(S))^2\n$$\nSince $\\sigma^2$ is a positive constant, maximizing $\\det(F)$ is equivalent to maximizing $(\\det(S))^2$, which is equivalent to maximizing $|\\det(S)|$.\nThe determinant is $\\det(S) = \\frac{\\partial y(t_{1})}{\\partial \\theta_{1}}\\frac{\\partial y(t_{2})}{\\partial \\theta_{2}} - \\frac{\\partial y(t_{2})}{\\partial \\theta_{1}}\\frac{\\partial y(t_{1})}{\\partial \\theta_{2}}$.\nWe examine the limit of the sensitivity functions as $t \\to \\infty$:\n$$\n\\lim_{t \\to \\infty} \\frac{\\partial y(t)}{\\partial \\theta_{1}} = \\lim_{t \\to \\infty} \\frac{\\theta_{2}U}{\\theta_{1}^2} \\left[ -1 + (1 + \\theta_{1}t)\\exp(-\\theta_{1}t) \\right] = \\frac{\\theta_{2}U}{\\theta_{1}^2}(-1+0) = -\\frac{\\theta_{2}U}{\\theta_{1}^2}\n$$\n$$\n\\lim_{t \\to \\infty} \\frac{\\partial y(t)}{\\partial \\theta_{2}} = \\lim_{t \\to \\infty} \\frac{U}{\\theta_{1}}(1 - \\exp(-\\theta_{1}t)) = \\frac{U}{\\theta_{1}}(1-0) = \\frac{U}{\\theta_{1}}\n$$\nLet $D(t_1) = \\lim_{t_2 \\to \\infty} \\det(S)$.\n$$\nD(t_1) = \\frac{\\partial y(t_{1})}{\\partial \\theta_{1}} \\left(\\frac{U}{\\theta_{1}}\\right) - \\left(-\\frac{\\theta_{2}U}{\\theta_{1}^2}\\right) \\frac{\\partial y(t_{1})}{\\partial \\theta_{2}}\n$$\nSubstituting the expressions for the sensitivities at $t_1$:\n$$\nD(t_1) = \\left( \\frac{\\theta_{2}U}{\\theta_{1}^2} \\left[ -1 + (1 + \\theta_{1}t_1)\\exp(-\\theta_{1}t_1) \\right] \\right) \\left(\\frac{U}{\\theta_{1}}\\right) + \\left(\\frac{\\theta_{2}U}{\\theta_{1}^2}\\right) \\left( \\frac{U}{\\theta_{1}}(1 - \\exp(-\\theta_{1}t_1)) \\right)\n$$\nFactor out the common term $\\frac{\\theta_{2}U^2}{\\theta_{1}^3}$:\n$$\nD(t_1) = \\frac{\\theta_{2}U^2}{\\theta_{1}^3} \\left( \\left[ -1 + (1 + \\theta_{1}t_1)\\exp(-\\theta_{1}t_1) \\right] + \\left[ 1 - \\exp(-\\theta_{1}t_1) \\right] \\right)\n$$\n$$\nD(t_1) = \\frac{\\theta_{2}U^2}{\\theta_{1}^3} \\left( -1 + \\exp(-\\theta_{1}t_1) + \\theta_{1}t_1\\exp(-\\theta_{1}t_1) + 1 - \\exp(-\\theta_{1}t_1) \\right)\n$$\n$$\nD(t_1) = \\frac{\\theta_{2}U^2}{\\theta_{1}^3} \\left( \\theta_{1}t_1\\exp(-\\theta_{1}t_1) \\right) = \\frac{\\theta_{2}U^2}{\\theta_{1}^2} t_1 \\exp(-\\theta_{1}t_1)\n$$\nTo maximize $|D(t_1)|$, we must maximize the function $f(t_1) = t_1 \\exp(-\\theta_{1}t_1)$ for $t_1  0$, as the pre-factor $\\frac{\\theta_{2}U^2}{\\theta_{1}^2}$ is a constant. We find the maximum by setting the first derivative of $f(t_1)$ to zero:\n$$\n\\frac{df(t_1)}{dt_1} = \\frac{d}{dt_1} \\left(t_1 \\exp(-\\theta_{1}t_1)\\right) = 1 \\cdot \\exp(-\\theta_{1}t_1) + t_1 \\cdot (-\\theta_1 \\exp(-\\theta_{1}t_1))\n$$\n$$\n\\frac{df(t_1)}{dt_1} = \\exp(-\\theta_{1}t_1) (1 - \\theta_{1}t_1)\n$$\nSetting the derivative to zero gives $1 - \\theta_{1}t_1 = 0$, since $\\exp(-\\theta_{1}t_1) \\neq 0$.\nThis yields the optimal time $t_{1}^{\\star}$:\n$$\nt_{1}^{\\star} = \\frac{1}{\\theta_{1}}\n$$\nTo confirm this is a maximum, we use the second derivative test:\n$$\n\\frac{d^2f(t_1)}{dt_1^2} = -\\theta_1 \\exp(-\\theta_{1}t_1)(1 - \\theta_{1}t_1) + \\exp(-\\theta_{1}t_1)(-\\theta_1) = -\\theta_1 \\exp(-\\theta_{1}t_1)(2 - \\theta_{1}t_1)\n$$\nEvaluating at $t_1 = 1/\\theta_1$:\n$$\n\\left. \\frac{d^2f(t_1)}{dt_1^2} \\right|_{t_1=1/\\theta_1} = -\\theta_1 \\exp(-1)(2 - 1) = -\\frac{\\theta_1}{e}\n$$\nSince $\\theta_1  0$, the second derivative is negative, confirming a local maximum. As $f(0)=0$ and $\\lim_{t_1 \\to \\infty} f(t_1) = 0$, and $f(t_1)  0$ for $t_10$, this local maximum is the global maximum. The optimal first sampling time is $1/\\theta_{1}$.", "answer": "$$\\boxed{\\frac{1}{\\theta_1}}$$", "id": "4313143"}, {"introduction": "Not all experimental design is about planning a single, optimal batch of experiments; often, we learn and adapt our strategy in real time. This practice shifts our focus to sequential decision-making, framing the problem of choosing between experimental conditions as a multi-armed bandit problem. You will work with the elegant and powerful Thompson sampling algorithm to navigate the fundamental exploration-exploitation trade-off, learning to balance trying new options with leveraging the current best-known strategy [@problem_id:4313130].", "problem": "In a systems biomedicine optimization task, you are designing follow-up experiments on three microenvironmental conditions to improve a binary assay’s success rate (success equals $1$, failure equals $0$). Each condition $i \\in \\{1,2,3\\}$ has an unknown success probability $\\theta_{i} \\in (0,1)$. You model outcomes as independent and identically distributed Bernoulli random variables with parameter $\\theta_{i}$ for condition $i$, and you place a Beta prior on each $\\theta_{i}$.\n\nFundamental base:\n- A Bernoulli likelihood for aggregated binary data under condition $i$ with $S_{i}$ successes and $F_{i}$ failures is $p(D_{i} \\mid \\theta_{i}) \\propto \\theta_{i}^{S_{i}} (1 - \\theta_{i})^{F_{i}}$.\n- A Beta prior satisfies $p(\\theta_{i}) \\propto \\theta_{i}^{\\alpha_{i}-1} (1 - \\theta_{i})^{\\beta_{i}-1}$.\n- Bayes’ rule gives $p(\\theta_{i} \\mid D_{i}) \\propto p(D_{i} \\mid \\theta_{i}) p(\\theta_{i})$.\n\nYou initialize each condition with the same prior $\\theta_{i} \\sim \\mathrm{Beta}(\\alpha_{i}, \\beta_{i})$ with $(\\alpha_{i}, \\beta_{i}) = (1,1)$. A pilot study yields the following observed counts:\n- Condition $1$: $S_{1} = 1$, $F_{1} = 0$.\n- Condition $2$: $S_{2} = 0$, $F_{2} = 1$.\n- Condition $3$: $S_{3} = 2$, $F_{3} = 0$.\n\nYou intend to choose the next condition to assay using Thompson sampling: independently draw one sample $\\tilde{\\theta}_{i}$ from the posterior of each $\\theta_{i}$ and select the condition whose sampled value is maximal. To make the sampling reproducible, you will use inverse transform sampling with independent uniform variates $u_{i} \\sim \\mathrm{Uniform}(0,1)$ supplied as:\n- $u_{1} = 0.36$,\n- $u_{2} = 0.49$,\n- $u_{3} = 0.20$.\n\nTasks:\n1. Using Bayes’ rule and the definitions above, derive the posterior distribution for each $\\theta_{i}$ after observing the pilot data.\n2. Using inverse transform sampling, compute the sampled values $\\tilde{\\theta}_{i}$ given the provided $u_{i}$ and the posterior distributions from Task $1$, exploiting the fact that when either the first or second Beta parameter equals $1$, the cumulative distribution function simplifies and admits a closed-form inverse.\n3. Under Thompson sampling, determine the index $i \\in \\{1,2,3\\}$ of the next condition to assay, i.e., the condition whose $\\tilde{\\theta}_{i}$ is largest.\n\nAnswer form requirement:\n- Report only the single index $i$ of the selected condition as your final answer. No rounding is necessary and no units are required.", "solution": "The problem statement is first validated against the required criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Model:** Three conditions $i \\in \\{1,2,3\\}$ with unknown success probabilities $\\theta_{i} \\in (0,1)$. Outcomes are modeled as independent Bernoulli trials with parameter $\\theta_{i}$.\n- **Likelihood:** The likelihood for $S_{i}$ successes and $F_{i}$ failures is $p(D_{i} \\mid \\theta_{i}) \\propto \\theta_{i}^{S_{i}} (1 - \\theta_{i})^{F_{i}}$.\n- **Prior:** A Beta prior is placed on each $\\theta_{i}$, $p(\\theta_{i}) \\propto \\theta_{i}^{\\alpha_{i}-1} (1 - \\theta_{i})^{\\beta_{i}-1}$.\n- **Bayes' Rule:** $p(\\theta_{i} \\mid D_{i}) \\propto p(D_{i} \\mid \\theta_{i}) p(\\theta_{i})$.\n- **Initial Prior:** For each condition, the prior is $\\theta_{i} \\sim \\mathrm{Beta}(\\alpha_{i}, \\beta_{i})$ with $(\\alpha_{i}, \\beta_{i}) = (1,1)$.\n- **Pilot Study Data:**\n  - Condition $1$: $S_{1} = 1$, $F_{1} = 0$.\n  - Condition $2$: $S_{2} = 0$, $F_{2} = 1$.\n  - Condition $3$: $S_{3} = 2$, $F_{3} = 0$.\n- **Sampling Method:** Thompson sampling, which requires drawing a sample $\\tilde{\\theta}_{i}$ from each posterior distribution $p(\\theta_{i} \\mid D_{i})$. The condition with the maximal $\\tilde{\\theta}_{i}$ is selected.\n- **Random Variates:** Sampling is performed via inverse transform sampling using independent uniform variates $u_{i} \\sim \\mathrm{Uniform}(0,1)$:\n  - $u_{1} = 0.36$\n  - $u_{2} = 0.49$\n  - $u_{3} = 0.20$\n- **Hint:** The cumulative distribution function (CDF) of the Beta distribution has a closed-form inverse when either the first or second parameter equals $1$.\n- **Tasks:**\n  1. Derive the posterior distribution for each $\\theta_{i}$.\n  2. Compute the sampled values $\\tilde{\\theta}_{i}$ using inverse transform sampling.\n  3. Determine the index $i$ of the condition with the largest $\\tilde{\\theta}_{i}$.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded:** The problem is grounded in standard Bayesian statistics and machine learning principles. The Beta-Bernoulli conjugate prior model is a fundamental tool for modeling rates and proportions. Thompson sampling is a state-of-the-art algorithm for reinforcement learning and optimal experimental design. Inverse transform sampling is a fundamental method for generating random variates. The application to optimizing experiments in systems biomedicine is a valid and common use case.\n- **Well-Posed:** All necessary components are provided: priors, data, a clear rule for updating beliefs (Bayes' rule), a clear decision-making protocol (Thompson sampling), and specific inputs for the sampling method ($u_i$). The tasks are defined unambiguously and lead to a unique solution.\n- **Objective:** The problem is stated in precise, formal language, free of subjectivity or ambiguity.\n\nThe problem does not exhibit any of the flaws listed in the validation checklist. It is scientifically sound, self-contained, and well-posed.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A solution will be provided.\n\n### Solution\n\n**Task 1: Derive the Posterior Distributions**\n\nThe Beta distribution is a conjugate prior for the Bernoulli likelihood. Given a prior distribution $\\theta \\sim \\mathrm{Beta}(\\alpha, \\beta)$ and data consisting of $S$ successes and $F$ failures, the posterior distribution is given by:\n$$\np(\\theta \\mid S, F) \\sim \\mathrm{Beta}(\\alpha + S, \\beta + F)\n$$\nThe initial prior for each condition $i \\in \\{1,2,3\\}$ is given as $\\theta_{i} \\sim \\mathrm{Beta}(\\alpha_{i}, \\beta_{i})$ where $\\alpha_{i}=1$ and $\\beta_{i}=1$. We update each of these priors with the corresponding pilot study data.\n\nFor Condition $1$:\n- Prior: $\\theta_{1} \\sim \\mathrm{Beta}(1, 1)$.\n- Data: $S_{1} = 1$, $F_{1} = 0$.\n- Posterior parameters: $\\alpha'_{1} = \\alpha_{1} + S_{1} = 1 + 1 = 2$, $\\beta'_{1} = \\beta_{1} + F_{1} = 1 + 0 = 1$.\n- Posterior distribution: $p(\\theta_{1} \\mid D_{1}) \\sim \\mathrm{Beta}(2, 1)$.\n\nFor Condition $2$:\n- Prior: $\\theta_{2} \\sim \\mathrm{Beta}(1, 1)$.\n- Data: $S_{2} = 0$, $F_{2} = 1$.\n- Posterior parameters: $\\alpha'_{2} = \\alpha_{2} + S_{2} = 1 + 0 = 1$, $\\beta'_{2} = \\beta_{2} + F_{2} = 1 + 1 = 2$.\n- Posterior distribution: $p(\\theta_{2} \\mid D_{2}) \\sim \\mathrm{Beta}(1, 2)$.\n\nFor Condition $3$:\n- Prior: $\\theta_{3} \\sim \\mathrm{Beta}(1, 1)$.\n- Data: $S_{3} = 2$, $F_{3} = 0$.\n- Posterior parameters: $\\alpha'_{3} = \\alpha_{3} + S_{3} = 1 + 2 = 3$, $\\beta'_{3} = \\beta_{3} + F_{3} = 1 + 0 = 1$.\n- Posterior distribution: $p(\\theta_{3} \\mid D_{3}) \\sim \\mathrm{Beta}(3, 1)$.\n\n**Task 2: Compute Sampled Values $\\tilde{\\theta}_{i}$**\n\nInverse transform sampling requires finding the inverse of the cumulative distribution function (CDF), $F^{-1}$. For a Beta distribution, the CDF is the regularized incomplete beta function, $F(x; \\alpha, \\beta) = I_x(\\alpha, \\beta)$. We need to solve $I_{\\tilde{\\theta}}(\\alpha, \\beta) = u$ for $\\tilde{\\theta}$. The problem provides a hint for special cases where one parameter is $1$.\n\nCase 1: $\\mathrm{Beta}(\\alpha, 1)$ distribution.\nThe PDF is $p(x;\\alpha,1) = \\alpha x^{\\alpha-1}$ for $x \\in (0,1)$.\nThe CDF is $F(x;\\alpha,1) = \\int_{0}^{x} \\alpha t^{\\alpha-1} dt = x^{\\alpha}$.\nSetting $F(\\tilde{\\theta}) = u$ gives $\\tilde{\\theta}^{\\alpha} = u$, so the inverse CDF is $F^{-1}(u) = u^{1/\\alpha}$.\n\nCase 2: $\\mathrm{Beta}(1, \\beta)$ distribution.\nThe PDF is $p(x;1, \\beta) = \\beta (1-x)^{\\beta-1}$ for $x \\in (0,1)$.\nThe CDF is $F(x;1,\\beta) = \\int_{0}^{x} \\beta(1-t)^{\\beta-1} dt = [-(1-t)^{\\beta}]_{0}^{x} = 1 - (1-x)^{\\beta}$.\nSetting $F(\\tilde{\\theta}) = u$ gives $1 - (1-\\tilde{\\theta})^{\\beta} = u$, so $(1-\\tilde{\\theta})^{\\beta} = 1-u$, which yields the inverse CDF $F^{-1}(u) = 1 - (1-u)^{1/\\beta}$.\n\nNow, we apply these formulas to our posteriors using the given uniform variates $u_{i}$.\n\nFor Condition $1$:\n- Posterior: $\\mathrm{Beta}(2, 1)$. This is Case 1 with $\\alpha'_{1} = 2$.\n- Uniform variate: $u_{1} = 0.36$.\n- Sampled value: $\\tilde{\\theta}_{1} = (u_{1})^{1/\\alpha'_{1}} = (0.36)^{1/2} = \\sqrt{0.36} = 0.6$.\n\nFor Condition $2$:\n- Posterior: $\\mathrm{Beta}(1, 2)$. This is Case 2 with $\\beta'_{2} = 2$.\n- Uniform variate: $u_{2} = 0.49$.\n- Sampled value: $\\tilde{\\theta}_{2} = 1 - (1-u_{2})^{1/\\beta'_{2}} = 1 - (1-0.49)^{1/2} = 1 - (0.51)^{1/2}$.\n\nFor Condition $3$:\n- Posterior: $\\mathrm{Beta}(3, 1)$. This is Case 1 with $\\alpha'_{3} = 3$.\n- Uniform variate: $u_{3} = 0.20$.\n- Sampled value: $\\tilde{\\theta}_{3} = (u_{3})^{1/\\alpha'_{3}} = (0.20)^{1/3} = \\sqrt[3]{0.20}$.\n\n**Task 3: Determine the Selected Condition**\n\nAccording to Thompson sampling, we select the condition $i$ that maximizes the sampled value $\\tilde{\\theta}_{i}$. We must compare the values of $\\tilde{\\theta}_{1}$, $\\tilde{\\theta}_{2}$, and $\\tilde{\\theta}_{3}$.\n\n- $\\tilde{\\theta}_{1} = 0.6$.\n- $\\tilde{\\theta}_{2} = 1 - \\sqrt{0.51}$.\n- $\\tilde{\\theta}_{3} = \\sqrt[3]{0.20}$.\n\nLet's compare $\\tilde{\\theta}_{1}$ with $\\tilde{\\theta}_{2}$ and $\\tilde{\\theta}_{3}$.\n\nComparison of $\\tilde{\\theta}_{1}$ and $\\tilde{\\theta}_{2}$:\nWe want to determine the sign of $\\tilde{\\theta}_{1} - \\tilde{\\theta}_{2} = 0.6 - (1 - \\sqrt{0.51}) = \\sqrt{0.51} - 0.4$.\nWe can compare $\\sqrt{0.51}$ and $0.4$ by squaring both positive numbers: $(\\sqrt{0.51})^2 = 0.51$ and $(0.4)^2 = 0.16$.\nSince $0.51  0.16$, it follows that $\\sqrt{0.51}  0.4$.\nTherefore, $\\tilde{\\theta}_{1} - \\tilde{\\theta}_{2}  0$, which means $\\tilde{\\theta}_{1}  \\tilde{\\theta}_{2}$.\n\nComparison of $\\tilde{\\theta}_{1}$ and $\\tilde{\\theta}_{3}$:\nWe want to compare $\\tilde{\\theta}_{1} = 0.6$ with $\\tilde{\\theta}_{3} = (0.20)^{1/3}$.\nWe can compare them by cubing both positive numbers: $(\\tilde{\\theta}_{1})^3 = (0.6)^3 = 0.216$ and $(\\tilde{\\theta}_{3})^3 = ((0.20)^{1/3})^3 = 0.20$.\nSince $0.216  0.20$, it follows that $0.6  (0.20)^{1/3}$.\nTherefore, $\\tilde{\\theta}_{1}  \\tilde{\\theta}_{3}$.\n\nSince $\\tilde{\\theta}_{1}$ is greater than both $\\tilde{\\theta}_{2}$ and $\\tilde{\\theta}_{3}$, it is the maximum value among the three samples.\n$$\n\\tilde{\\theta}_{1} = \\max(\\tilde{\\theta}_{1}, \\tilde{\\theta}_{2}, \\tilde{\\theta}_{3})\n$$\nThus, under Thompson sampling, the next condition to assay is condition $1$. The index is $i=1$.", "answer": "$$\n\\boxed{1}\n$$", "id": "4313130"}]}