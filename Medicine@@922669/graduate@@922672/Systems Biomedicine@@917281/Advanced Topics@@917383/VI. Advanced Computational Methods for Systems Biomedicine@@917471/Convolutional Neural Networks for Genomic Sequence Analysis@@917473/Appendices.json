{"hands_on_practices": [{"introduction": "This first practice grounds our exploration in the fundamental building block of any CNN: the convolutional layer. When analyzing genomic sequences, the parameters of this layer—kernel size $k$, stride $s$, and padding $p$—directly determine the model's ability to \"see\" and learn sequence motifs. This exercise challenges you to calculate the output dimensions of a convolutional layer and analyze how its configuration impacts the detection of motifs that are larger than the filter's receptive field. Mastering this calculation is a critical first step in designing architectures that are well-suited to the scale of biological features you aim to capture.", "problem": "You are designing a first-layer one-dimensional convolution in a Convolutional Neural Network (CNN) to detect transcription factor binding motifs in Deoxyribonucleic Acid (DNA) sequences encoded as one-hot along channels. Consider an input sequence of length $L_{\\mathrm{in}} = 1{,}000$ bases. The convolutional layer uses a kernel of length $k = 15$, stride $s = 3$, symmetric zero-padding of $p = 2$ bases on each end, and dilation $d = 1$.\n\nStarting from the definition of discrete one-dimensional convolution with stride and padding on a padded input, derive the general expression that determines the number of output positions as the count of valid kernel alignments. Use this reasoning to compute the output sequence length $L_{\\mathrm{out}}$ for the given parameters. Then, based on the concept of receptive field and the sampling induced by stride, analyze how this configuration affects the ability to detect motifs that span $M = 20$ bases, particularly with respect to full coverage within a single filter application, positional sensitivity, and detection near sequence boundaries.\n\nReport the output length $L_{\\mathrm{out}}$ as a single integer with no units. No rounding is necessary. Your qualitative analysis should be included in your reasoning, but the final answer must be only the computed $L_{\\mathrm{out}}$.", "solution": "The user requires the derivation of the output length of a one-dimensional convolutional layer and a qualitative analysis of its properties for a specific application in bioinformatics. The solution will first establish the general formula for the output dimension, then apply it to the given parameters, and finally analyze the configuration's implications.\n\n**Problem Validation**\n\nThe problem statement is critically validated before proceeding.\n1.  **Givens Extracted**:\n    -   Input sequence length: $L_{\\mathrm{in}} = 1{,}000$ bases.\n    -   Convolutional kernel length: $k = 15$.\n    -   Stride: $s = 3$.\n    -   Symmetric zero-padding: $p = 2$ on each end.\n    -   Dilation: $d = 1$.\n    -   Subject motif length for analysis: $M = 20$ bases.\n\n2.  **Validation Verdict**: The problem is **valid**. It is scientifically grounded in the standard principles of deep learning (specifically, Convolutional Neural Networks) as applied to the well-established field of bioinformatics for genomic analysis. The problem is well-posed, providing a complete and consistent set of parameters ($L_{\\mathrm{in}}, k, s, p, d$) from which a unique, meaningful solution for the output length ($L_{\\mathrm{out}}$) can be determined. The language is objective and employs standard terminology. The parameters are realistic for the described task.\n\n**Derivation of the General Expression for Output Length**\n\nThe number of output positions, or the output sequence length $L_{\\mathrm{out}}$, of a one-dimensional convolutional layer is determined by the number of valid placements of the kernel along the input sequence. This can be derived from first principles.\n\n1.  An input sequence of length $L_{\\mathrm{in}}$ is first subjected to symmetric padding, where $p$ elements are added to each of its two ends. The length of this new, padded sequence, denoted $L_{\\mathrm{padded}}$, is:\n    $$L_{\\mathrm{padded}} = L_{\\mathrm{in}} + 2p$$\n\n2.  The kernel, of length $k$, may be dilated. The dilation factor $d$ introduces $d-1$ gaps between consecutive kernel elements. The effective spatial extent of the kernel on the input, known as the effective kernel size $k_{\\mathrm{eff}}$, is thus:\n    $$k_{\\mathrm{eff}} = k + (k-1)(d-1)$$\n\n3.  The kernel is moved, or 'strided', across the padded sequence. An output is computed for each position. The first placement of the kernel starts at index $0$ of the padded sequence. The extent covered is from index $0$ to $k_{\\mathrm{eff}}-1$. The last possible placement must also fit entirely within the padded sequence. The last element of the kernel must align with an index no greater than $L_{\\mathrm{padded}}-1$. This means the last starting position for the kernel is at index $L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$.\n\n4.  The total span of possible starting positions is therefore from $0$ to $L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$, which comprises $L_{\\mathrm{padded}} - k_{\\mathrm{eff}} + 1$ possible positions if the stride were $1$.\n\n5.  With a stride of $s$, the kernel does not occupy every possible position. Instead, it occupies positions $0, s, 2s, \\dots, Ns$, where $Ns$ is the last valid starting position. This last position must satisfy $Ns \\le L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$. The number of steps, $N$, is therefore the largest integer satisfying this condition, which is found using the floor function:\n    $$N = \\left\\lfloor \\frac{L_{\\mathrm{padded}} - k_{\\mathrm{eff}}}{s} \\right\\rfloor$$\n    The total number of output positions, $L_{\\mathrm{out}}$, is the number of these placements, which is $N+1$ (including the initial placement at position $0$).\n\n6.  Substituting the expressions for $L_{\\mathrm{padded}}$ and $k_{\\mathrm{eff}}$, the general formula for the output length is obtained:\n    $$L_{\\mathrm{out}} = \\left\\lfloor \\frac{(L_{\\mathrm{in}} + 2p) - (k + (k-1)(d-1))}{s} \\right\\rfloor + 1$$\n\n**Computation of $L_{\\mathrm{out}}$ for the Given Parameters**\n\nThe problem provides the following parameters: $L_{\\mathrm{in}} = 1{,}000$, $k = 15$, $s = 3$, $p = 2$, and $d = 1$.\n\nFirst, we substitute these values into the derived general expression. Since the dilation is $d=1$, the effective kernel length $k_{\\mathrm{eff}}$ is simply $k$:\n$$k_{\\mathrm{eff}} = 15 + (15-1)(1-1) = 15 + (14)(0) = 15$$\nThe formula for $L_{\\mathrm{out}}$ simplifies to:\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{L_{\\mathrm{in}} + 2p - k}{s} \\right\\rfloor + 1$$\nSubstituting the numerical values:\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{1{,}000 + 2(2) - 15}{3} \\right\\rfloor + 1$$\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{1{,}000 + 4 - 15}{3} \\right\\rfloor + 1$$\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{989}{3} \\right\\rfloor + 1$$\nThe division yields a non-integer value:\n$$\\frac{989}{3} \\approx 329.667$$\nApplying the floor function gives the greatest integer less than or equal to this value:\n$$\\lfloor 329.667 \\rfloor = 329$$\nFinally, adding $1$ yields the output length:\n$$L_{\\mathrm{out}} = 329 + 1 = 330$$\n\n**Qualitative Analysis of the Configuration**\n\nThe problem further requires an analysis of this configuration's ability to detect motifs of length $M = 20$ bases.\n\n1.  **Full Coverage and Receptive Field**: The receptive field of any single neuron in this convolutional layer is its effective kernel size, $k_{\\mathrm{eff}} = 15$ bases. The motif of interest is $M=20$ bases long. Since $M > k_{\\mathrm{eff}}$, **no single filter application can see the entire motif**. The filter can, at best, learn to recognize fragments of the motif that are no longer than $15$ bases. To identify the full $20$-base motif, the network must learn to combine information from spatially adjacent neurons in the output feature map, a task for subsequent layers (e.g., another convolutional layer or a fully-connected layer).\n\n2.  **Positional Sensitivity and Stride**: The stride of $s=3$ introduces a downsampling of the input. This means the kernel \"jumps\" over $2$ bases for each step it takes. Consequently, the layer is not translationally equivariant to single-base shifts. A motif that starts at input position $i$ will be viewed differently by the kernel than one starting at position $i+1$ or $i+2$. This high sensitivity to positional shifts modulo $s$ can be detrimental, as a small shift in a motif's location might cause it to be misaligned with the kernel's stride, leading to a diminished activation and a possible failed detection.\n\n3.  **Detection Near Sequence Boundaries**: The layer uses symmetric padding of $p=2$. With a kernel of size $k=15$, this padding is insufficient to allow the kernel to be centered over motifs at the extreme ends of the sequence. To center a kernel of size $k$ over the very first base (index $0$), a padding of $p_{\\mathrm{center}} = \\lfloor k/2 \\rfloor = \\lfloor 15/2 \\rfloor = 7$ would be required. With $p=2$, the receptive field of the very first output neuron is centered on original sequence position $7-2 = 5$. This implies that motifs within the first $\\approx 5$ and last $\\approx 5$ bases of the sequence can never be centered in the kernel's view, which can impair the network's ability to detect them.\n\nIn conclusion, this specific layer configuration is structurally limited for the task of detecting a $20$-base motif due to its small kernel size, and its performance is further compromised by a large stride and insufficient padding, which affect positional sensitivity and boundary detection.", "answer": "$$\\boxed{330}$$", "id": "4553876"}, {"introduction": "While convolutional layers extract features, pooling layers are often used to build hierarchical representations and achieve invariance. However, this comes at a cost, especially in genomics where predicting the exact location of an event, such as a nuclease cut-site, is crucial. This exercise asks you to model this process and derive the expected localization error introduced by downsampling. By quantifying this trade-off, you will gain a deeper appreciation for designing architectures that balance the need for abstract features with the demand for high spatial precision.", "problem": "In systems biomedicine, convolutional neural networks (CNNs) are used to predict nuclease cut-sites from Deoxyribonucleic Acid (DNA) sequences. Consider a one-dimensional CNN that ingests a DNA sequence sampled at $1$ sample per base pair, applies convolutional layers with stride $1$, and then applies $K$ max-pooling layers, where the $i$-th pooling layer uses window size $s_i$ base pairs and stride $s_i$ (non-overlapping pooling). The network outputs a coarse spatial map from which a single grid location is chosen (for example, via an $\\arg\\max$ over the coarse map), and the predicted cut-site position $\\hat{x}$ is taken to be the center of the selected coarse bin when mapped back to input coordinates.\n\nAssume the following:\n- The true cut-site position $x^{\\ast}$ (relative to the coarse grid induced by pooling) is uniformly distributed within any coarse bin due to unknown alignment between the true position and the pooling grid.\n- The upstream convolutions preserve translation equivariance at the input resolution, and no sub-bin interpolation is performed at the output; the prediction is restricted to the coarse grid induced by pooling.\n- You may appeal to the Nyquist–Shannon sampling theorem and the definition of max-pooling to formalize how stacking pooling layers changes the effective sampling resolution. Model the consequence as a uniform quantization of the input coordinate with some step size that is determined by $\\{s_i\\}_{i=1}^{K}$.\n\nStarting from these bases, derive the expected squared localization error\n$$\nE \\;=\\; \\mathbb{E}\\big[(\\hat{x} - x^{\\ast})^{2}\\big]\n$$\nas a closed-form analytic expression in terms of the pooling strides $\\{s_i\\}_{i=1}^{K}$. Explain why increasing $K$ or any $s_i$ increases $E$ by reducing positional information. Express your final $E$ in base pairs squared, and provide your answer as a single closed-form analytic expression. No rounding is required, and do not include units in the final boxed answer.", "solution": "The problem is deemed valid as it is scientifically grounded, well-posed, and objective. It presents a simplified but standard model of quantization error resulting from downsampling operations (max-pooling) in a convolutional neural network, a common topic in signal processing and machine learning applied to genomics. All necessary parameters and assumptions are provided to derive a unique, analytical solution.\n\nThe problem asks for the expected squared localization error, $E = \\mathbb{E}[(\\hat{x} - x^{\\ast})^{2}]$, where $\\hat{x}$ is the predicted cut-site position and $x^{\\ast}$ is the true position. The core of this problem lies in understanding how the sequential application of max-pooling layers affects the spatial resolution of the input signal.\n\nFirst, let us determine the size of a single resolution element (a \"coarse bin\") in the final output map when referred back to the input coordinate system. The input DNA sequence is sampled at $1$ sample per base pair. The network architecture consists of convolutional layers with stride $1$, which preserve the spatial resolution, followed by $K$ max-pooling layers. The $i$-th pooling layer has a window size $s_i$ and a stride of $s_i$. This non-overlapping pooling operation reduces the number of data points by a factor of $s_i$. When these $K$ layers are stacked, the total downsampling factor is the product of the individual stride factors.\n\nLet $S$ be the size of a single coarse bin in the original input coordinates, measured in base pairs. This size is the cumulative effect of all $K$ pooling operations.\n$$\nS = \\prod_{i=1}^{K} s_i\n$$\nA single grid location in the final coarse map corresponds to a segment of the input DNA of length $S$ base pairs. Let us represent such a bin as the interval $[a, a+S)$ for some starting position $a$.\n\nThe problem states that the predicted cut-site position, $\\hat{x}$, is the center of the selected coarse bin. Therefore, for a bin spanning $[a, a+S)$, the prediction is:\n$$\n\\hat{x} = a + \\frac{S}{2}\n$$\nThe true cut-site position, $x^{\\ast}$, is assumed to be uniformly distributed within this coarse bin due to unknown alignment. This can be expressed as:\n$$\nx^{\\ast} \\sim U[a, a+S)\n$$\nwhere $U[c, d)$ denotes a continuous uniform distribution over the interval $[c, d)$.\n\nWe are interested in the localization error, which is the difference $\\hat{x} - x^{\\ast}$. Let us define a random variable $\\epsilon$ representing this error:\n$$\n\\epsilon = \\hat{x} - x^{\\ast} = \\left(a + \\frac{S}{2}\\right) - x^{\\ast}\n$$\nSince $x^{\\ast}$ is uniformly distributed on $[a, a+S)$, the error variable $\\epsilon$ is uniformly distributed on the interval:\n$$\n\\left[ \\left(a + \\frac{S}{2}\\right) - (a+S), \\left(a + \\frac{S}{2}\\right) - a \\right] = \\left[ -\\frac{S}{2}, \\frac{S}{2} \\right]\n$$\nSo, $\\epsilon \\sim U[-S/2, S/2]$. The probability density function (PDF) of $\\epsilon$, denoted $p(\\epsilon)$, is:\n$$\np(\\epsilon) = \\begin{cases} \\frac{1}{(S/2) - (-S/2)} = \\frac{1}{S} & \\text{for } \\epsilon \\in [-S/2, S/2] \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nThe quantity to be derived is the expected squared localization error, $E$:\n$$\nE = \\mathbb{E}[(\\hat{x} - x^{\\ast})^{2}] = \\mathbb{E}[\\epsilon^2]\n$$\nThis expectation can be calculated by integrating $\\epsilon^2$ against its PDF over its support:\n$$\nE = \\int_{-\\infty}^{\\infty} \\epsilon^2 p(\\epsilon) \\, d\\epsilon = \\int_{-S/2}^{S/2} \\epsilon^2 \\left(\\frac{1}{S}\\right) \\, d\\epsilon\n$$\nWe can now evaluate the integral:\n$$\nE = \\frac{1}{S} \\left[ \\frac{\\epsilon^3}{3} \\right]_{-S/2}^{S/2} = \\frac{1}{3S} \\left[ \\left(\\frac{S}{2}\\right)^3 - \\left(-\\frac{S}{2}\\right)^3 \\right]\n$$\n$$\nE = \\frac{1}{3S} \\left[ \\frac{S^3}{8} - \\left(-\\frac{S^3}{8}\\right) \\right] = \\frac{1}{3S} \\left[ \\frac{S^3}{8} + \\frac{S^3}{8} \\right] = \\frac{1}{3S} \\left[ \\frac{2S^3}{8} \\right] = \\frac{1}{3S} \\left[ \\frac{S^3}{4} \\right]\n$$\n$$\nE = \\frac{S^2}{12}\n$$\nThis result is the variance of a uniform distribution centered at $0$, as the mean of $\\epsilon$ is $\\mathbb{E}[\\epsilon]=0$.\n\nFinally, we substitute the expression for the total bin size $S$ in terms of the individual pooling strides $\\{s_i\\}_{i=1}^{K}$:\n$$\nE = \\frac{\\left( \\prod_{i=1}^{K} s_i \\right)^2}{12}\n$$\nThis is the closed-form analytic expression for the expected squared localization error in units of base pairs squared.\n\nThis expression demonstrates why increasing the number of pooling layers, $K$, or the stride of any given pooling layer, $s_i$, leads to an increase in the expected error $E$. The total downsampling factor $S = \\prod s_i$ represents the width of the spatial quantization interval. Any increase in $K$ introduces a new multiplicative factor $s_K > 1$ (assuming non-trivial pooling), and increasing any $s_i$ directly enlarges the product $S$. The expected squared error $E$ is proportional to $S^2$. A larger bin size $S$ implies greater uncertainty about the true position $x^{\\ast}$ within that bin. This loss of positional information is mathematically captured by the larger variance of the uniform distribution over a wider interval, resulting in a quadratically larger expected localization error. This illustrates a fundamental trade-off in network design between feature invariance/abstraction and spatial precision.", "answer": "$$\n\\boxed{\\frac{\\left( \\prod_{i=1}^{K} s_i \\right)^2}{12}}\n$$", "id": "4331413"}, {"introduction": "A successful architecture is only as good as its training objective. A pervasive challenge in genomics is severe class imbalance, where functional elements are rare needles in a vast haystack of non-functional sequence. This practice delves into a powerful solution: the focal loss. By deriving and comparing its gradient to that of the standard cross-entropy loss, you will uncover the elegant mechanism by which it dynamically down-weights easy examples, forcing the model to focus on the rare positive cases it struggles to classify.", "problem": "A Convolutional Neural Network (CNN) is trained to predict transcription factor binding sites from Deoxyribonucleic Acid (DNA) sequences in systems biomedicine, where the positive class (binding) is rare relative to the negative class (non-binding). Each sequence is one-hot encoded and processed by convolutional layers followed by a final logistic (sigmoid) output. Let the model’s logit be $z \\in \\mathbb{R}$ and its predicted probability be $p = \\sigma(z) = \\frac{1}{1 + \\exp(-z)}$. For a sequence with true label $y \\in \\{0,1\\}$, the binary cross-entropy (logistic loss) is defined as $L_{\\mathrm{log}}(p,y) = - y \\ln(p) - (1-y) \\ln(1-p)$, and the focal loss is defined as $L_{\\mathrm{foc}}(p,y) = - \\alpha \\,(1-p)^{\\gamma} y \\ln(p) - \\alpha\\, p^{\\gamma} (1-y) \\ln(1-p)$, where $\\alpha \\in (0,1]$ is a balancing factor and $\\gamma \\geq 0$ is the focusing parameter. Consider the positive class $y = 1$ and set $\\alpha = 1$.\n\nStarting from the above definitions and the chain rule, derive the gradient with respect to the logit, $\\frac{\\partial L}{\\partial z}$, for the binary cross-entropy and the focal loss under $y=1$. Then, define the gradient magnitude scaling ratio\n$$S(p,\\gamma) = \\frac{\\left|\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z}\\right|}{\\left|\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial z}\\right|}.$$\nEvaluate $S(p,\\gamma)$ at two operating points of interest for rare positives in genomic sequence classification:\n- an “easy” positive example with $p = 0.99$,\n- a “hard” positive example with $p = 0.01$,\nusing $\\gamma = 2$.\n\nReport your two numerical values for $S(0.99,2)$ and $S(0.01,2)$ as a row matrix, in the order corresponding to the easy and hard cases. Round each entry to four significant figures. No units are required.", "solution": "The user has provided a well-defined problem in the domain of machine learning for computational biology. I will first validate the problem statement according to the specified protocol.\n\n### Step 1: Extract Givens\n- Model logit: $z \\in \\mathbb{R}$\n- Predicted probability: $p = \\sigma(z) = \\frac{1}{1 + \\exp(-z)}$\n- True label: $y \\in \\{0,1\\}$\n- Binary cross-entropy (logistic loss): $L_{\\mathrm{log}}(p,y) = - y \\ln(p) - (1-y) \\ln(1-p)$\n- Focal loss: $L_{\\mathrm{foc}}(p,y) = - \\alpha \\,(1-p)^{\\gamma} y \\ln(p) - \\alpha\\, p^{\\gamma} (1-y) \\ln(1-p)$\n- Balancing factor: $\\alpha \\in (0,1]$\n- Focusing parameter: $\\gamma \\geq 0$\n- Condition under analysis: positive class, $y = 1$\n- Parameter value assumption: $\\alpha = 1$\n- Gradient magnitude scaling ratio definition: $S(p,\\gamma) = \\frac{\\left|\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z}\\right|}{\\left|\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial z}\\right|}$\n- Parameter value for evaluation: $\\gamma = 2$\n- Evaluation points:\n  - \"Easy\" positive example: $p = 0.99$\n  - \"Hard\" positive example: $p = 0.01$\n- Required output: Numerical values for $S(0.99,2)$ and $S(0.01,2)$ rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is well-grounded in machine learning theory. The definitions for the sigmoid function, binary cross-entropy, and focal loss are standard. Their application to classifying genomic sequences is a mainstream research area in systems biomedicine.\n2.  **Well-Posed**: The problem is structured to have a unique solution. It provides all necessary functions, parameters, and evaluation points. The request to derive gradients and compute a specific ratio is unambiguous.\n3.  **Objective**: The problem is stated using precise mathematical definitions. The terms \"easy\" and \"hard\" examples are objectively defined by the predicted probability values ($p=0.99$ and $p=0.01$ for a true positive, $y=1$), which is a standard convention.\n4.  **Completeness and Consistency**: The problem is self-contained and internally consistent. All required parameters ($\\alpha$, $\\gamma$) and variables ($y$, $p$) are assigned values for the final calculation.\n5.  **Realism**: The premise of using such models for identifying transcription factor binding sites and dealing with class imbalance is highly realistic in modern bioinformatics. The parameter values are typical.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with a full derivation and solution.\n\nThe task is to derive the gradients of the binary cross-entropy loss and the focal loss with respect to the logit $z$ for a positive sample ($y=1$), and then to compute the ratio of their magnitudes at specific points.\n\nFirst, we need the derivative of the predicted probability $p$ with respect to the logit $z$. The function $p = \\sigma(z)$ is the sigmoid function.\n$$p = \\sigma(z) = \\frac{1}{1 + \\exp(-z)} = (1 + \\exp(-z))^{-1}$$\nUsing the chain rule, we find its derivative with respect to $z$:\n$$\\frac{dp}{dz} = \\frac{d\\sigma(z)}{dz} = -1 (1 + \\exp(-z))^{-2} \\cdot (-\\exp(-z)) = \\frac{\\exp(-z)}{(1 + \\exp(-z))^2}$$\nWe can express this in terms of $p$:\n$$\\frac{dp}{dz} = \\frac{1}{1 + \\exp(-z)} \\cdot \\frac{\\exp(-z)}{1 + \\exp(-z)} = p \\cdot \\left(1 - \\frac{1}{1 + \\exp(-z)}\\right) = p(1-p)$$\nThis is a standard result. The derivatives of the losses with respect to $z$ can now be found using the chain rule: $\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial p} \\frac{dp}{dz}$.\n\n**1. Gradient of Binary Cross-Entropy Loss ($L_{\\mathrm{log}}$)**\n\nFor a positive sample, $y=1$. The binary cross-entropy loss simplifies to:\n$$L_{\\mathrm{log}}(p,1) = - (1) \\ln(p) - (1-1) \\ln(1-p) = -\\ln(p)$$\nThe derivative with respect to $p$ is:\n$$\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial p} = -\\frac{1}{p}$$\nApplying the chain rule to find the gradient with respect to $z$:\n$$\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial z} = \\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial p} \\frac{dp}{dz} = \\left(-\\frac{1}{p}\\right) \\cdot (p(1-p)) = -(1-p) = p-1$$\n\n**2. Gradient of Focal Loss ($L_{\\mathrm{foc}}$)**\n\nFor a positive sample ($y=1$) and with the balancing factor $\\alpha=1$, the focal loss simplifies to:\n$$L_{\\mathrm{foc}}(p,1) = - (1) (1-p)^{\\gamma} (1) \\ln(p) - (1) p^{\\gamma} (1-1) \\ln(1-p) = -(1-p)^{\\gamma} \\ln(p)$$\nWe differentiate this with respect to $p$ using the product rule:\n$$\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial p} = \\frac{\\partial}{\\partial p} \\left[ -(1-p)^{\\gamma} \\ln(p) \\right]$$\n$$= -\\left( (\\gamma(1-p)^{\\gamma-1}(-1)) \\ln(p) + (1-p)^{\\gamma} \\frac{1}{p} \\right)$$\n$$= -\\left( -\\gamma(1-p)^{\\gamma-1} \\ln(p) + \\frac{(1-p)^{\\gamma}}{p} \\right)$$\n$$= \\gamma(1-p)^{\\gamma-1} \\ln(p) - \\frac{(1-p)^{\\gamma}}{p}$$\nNow, we apply the chain rule to find the gradient with respect to $z$:\n$$\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z} = \\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial p} \\frac{dp}{dz} = \\left( \\gamma(1-p)^{\\gamma-1} \\ln(p) - \\frac{(1-p)^{\\gamma}}{p} \\right) \\cdot (p(1-p))$$\nDistributing the $p(1-p)$ term:\n$$\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z} = \\gamma p (1-p)^{\\gamma} \\ln(p) - (1-p)^{\\gamma+1}$$\n$$= (1-p)^{\\gamma} \\left[ \\gamma p \\ln(p) - (1-p) \\right] = (1-p)^{\\gamma} \\left[ \\gamma p \\ln(p) + p - 1 \\right]$$\n\n**3. Gradient Magnitude Scaling Ratio ($S(p,\\gamma)$)**\n\nThe ratio is defined as:\n$$S(p,\\gamma) = \\frac{\\left|\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z}\\right|}{\\left|\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial z}\\right|}$$\nWe have the gradients:\n$$\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial z} = p-1$$\n$$\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z} = (1-p)^{\\gamma} (\\gamma p \\ln(p) + p-1)$$\nFor $p \\in (0,1)$, we have $p-1 < 0$ and $\\ln(p) < 0$. Since $\\gamma \\ge 0$, the term $\\gamma p \\ln(p)$ is non-positive. Thus, the entire bracketed term $(\\gamma p \\ln(p) + p-1)$ is negative. The term $(1-p)^{\\gamma}$ is positive. Therefore, both gradients are negative for $p \\in (0,1)$.\nTheir absolute values are their negations:\n$$ \\left|\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial z}\\right| = -(p-1) = 1-p $$\n$$ \\left|\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z}\\right| = -(1-p)^{\\gamma} (\\gamma p \\ln(p) + p-1) = (1-p)^{\\gamma} (1-p - \\gamma p \\ln(p)) $$\nThe ratio is:\n$$S(p,\\gamma) = \\frac{(1-p)^{\\gamma} (1-p - \\gamma p \\ln(p))}{1-p} = (1-p)^{\\gamma-1} (1-p - \\gamma p \\ln(p))$$\n\n**4. Numerical Evaluation**\n\nWe must evaluate $S(p,\\gamma)$ for $\\gamma=2$ at $p=0.99$ and $p=0.01$. The specific formula for $\\gamma=2$ is:\n$$S(p,2) = (1-p)^{2-1} (1-p - 2p \\ln(p)) = (1-p)(1-p - 2p \\ln(p))$$\n\nCase 1: \"Easy\" positive example, $p = 0.99$.\n$$S(0.99,2) = (1-0.99)(1 - 0.99 - 2(0.99)\\ln(0.99))$$\n$$S(0.99,2) = (0.01)(0.01 - 1.98 \\ln(0.99))$$\nUsing $\\ln(0.99) \\approx -0.01005033585$:\n$$S(0.99,2) \\approx (0.01)(0.01 - 1.98(-0.01005033585))$$\n$$S(0.99,2) \\approx (0.01)(0.01 + 0.019899665)$$\n$$S(0.99,2) \\approx (0.01)(0.029899665) = 0.00029899665$$\nRounding to four significant figures, we get $0.0002990$.\n\nCase 2: \"Hard\" positive example, $p = 0.01$.\n$$S(0.01,2) = (1-0.01)(1 - 0.01 - 2(0.01)\\ln(0.01))$$\n$$S(0.01,2) = (0.99)(0.99 - 0.02 \\ln(0.01))$$\nUsing $\\ln(0.01) = \\ln(10^{-2}) = -2\\ln(10) \\approx -4.605170186$:\n$$S(0.01,2) \\approx (0.99)(0.99 - 0.02(-4.605170186))$$\n$$S(0.01,2) \\approx (0.99)(0.99 + 0.0921034037)$$\n$$S(0.01,2) \\approx (0.99)(1.0821034037) = 1.07128237$$\nRounding to four significant figures, we get $1.071$.\n\nThe results demonstrate the \"focusing\" property of the focal loss. For an easy positive ($p=0.99$), the gradient magnitude is scaled down by a factor of approximately $3 \\times 10^{-4}$, effectively reducing its contribution to the weight update. For a hard positive ($p=0.01$), the gradient magnitude is comparable to (and slightly greater than) that of the standard cross-entropy loss, ensuring the model focuses its learning on this misclassified example.\n\nThe requested values are $S(0.99,2) = 0.0002990$ and $S(0.01,2) = 1.071$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.0002990 & 1.071\n\\end{pmatrix}\n}\n$$", "id": "4331425"}]}