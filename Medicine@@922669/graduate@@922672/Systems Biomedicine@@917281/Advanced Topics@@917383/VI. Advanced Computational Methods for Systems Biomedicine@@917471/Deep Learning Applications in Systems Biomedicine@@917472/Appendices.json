{"hands_on_practices": [{"introduction": "Understanding how data flows through a network is a fundamental skill for designing and troubleshooting deep learning models. This exercise deconstructs the U-Net, a cornerstone architecture in biomedical image segmentation, forcing you to track feature map dimensions layer by layer. By working through the mathematics of convolutions, pooling, and skip connections, you will gain a concrete understanding of how architectural choices impact the model's final output, a critical consideration for applications like analyzing histopathology slides [@problem_id:4332691].", "problem": "In a systems biomedicine pipeline for whole-slide histopathology analysis, a convolutional encoder–decoder network of the U-Net family is used to segment cellular compartments in multiplexed immunofluorescence images. Consider a $2$-dimensional U-Net with $3$ down-sampling stages and $3$ up-sampling stages. The spatial dimensions only are of interest. The input image has spatial size $(H, W)$, where $H$ and $W$ are positive integers. The architecture is as follows, with spatial operations specified exactly:\n\n- Encoder stage $1$: two convolutional layers with kernel size $3 \\times 3$, stride $1$, dilation $1$, and zero-padding chosen to preserve spatial size at each convolution (“same” padding); then a max-pooling layer with kernel size $2 \\times 2$, stride $2$, and zero-padding $0$.\n- Encoder stage $2$: two convolutional layers with kernel size $3 \\times 3$, stride $1$, dilation $1$, and “same” padding; then a max-pooling layer with kernel size $2 \\times 2$, stride $2$, and zero-padding $0$.\n- Encoder stage $3$: two convolutional layers with kernel size $3 \\times 3$, stride $1$, dilation $1$, and “same” padding; then a max-pooling layer with kernel size $2 \\times 2$, stride $2$, and zero-padding $0$.\n- Bottleneck: two convolutional layers with kernel size $3 \\times 3$, stride $1$, dilation $1$, and “same” padding.\n- Decoder stage $3$: one transposed convolution (“up-convolution”) with kernel size $2 \\times 2$, stride $2$, zero-padding $0$, and output padding $0$; concatenate with the encoder stage $3$ feature map taken immediately after its two “same”-padded convolutions and before its max-pooling (use symmetric center-cropping of the skip feature map if the spatial sizes do not match exactly); then two convolutional layers with kernel size $3 \\times 3$, stride $1$, dilation $1$, and “same” padding.\n- Decoder stage $2$: one transposed convolution with kernel size $2 \\times 2$, stride $2$, zero-padding $0$, and output padding $0$; concatenate with the encoder stage $2$ feature map taken immediately after its two “same”-padded convolutions and before its max-pooling (use symmetric center-cropping if needed); then two convolutional layers with kernel size $3 \\times 3$, stride $1$, dilation $1$, and “same” padding.\n- Decoder stage $1$: one transposed convolution with kernel size $2 \\times 2$, stride $2$, zero-padding $0$, and output padding $0$; concatenate with the encoder stage $1$ feature map taken immediately after its two “same”-padded convolutions and before its max-pooling (use symmetric center-cropping if needed); then two convolutional layers with kernel size $3 \\times 3$, stride $1$, dilation $1$, and “same” padding.\n- Final prediction layer: a $1 \\times 1$ convolution with stride $1$.\n\nAssume all operations are applied identically and independently along the $H$ and $W$ dimensions; channel counts do not affect spatial sizes and may be ignored. Concatenation does not change spatial sizes and is performed only after exactly matching the spatial sizes by symmetric cropping of the skip feature maps as described.\n\nTasks:\n- Using only fundamental definitions of discrete convolution, striding, padding, and pooling, compute the spatial size at the output of every spatial operation in the encoder and decoder as a function of $(H, W)$, showing how rounding arises where applicable. Explicitly identify any points where symmetric cropping of skip connections is required and quantify the mismatch at each skip as a function of $H$ and $W$.\n- Derive a closed-form analytical expression for the final segmentation map spatial size as a function of $(H, W)$ under the above architecture and cropping rule.\n- Briefly discuss how the choice of padding (“same” versus “valid”) in the convolutional layers would alter the layer-by-layer sizes and the final output size, and argue which padding strategy is preferable for tile-based histopathology inference to mitigate border artifacts, justifying your answer from first principles of discrete convolution and sampling theory.\n\nProvide your final answer for the closed-form final spatial size as a row matrix containing the two expressions for the height and width, respectively. No units are required. If you choose to express intermediate rounding, use the floor function notation. Do not include any text in the final answer box.", "solution": "The problem statement is well-defined, scientifically grounded in the principles of deep learning architectures, and internally consistent. It describes a standard U-Net architecture with specific layer parameters, providing a clear basis for calculation and analysis. The tasks are objective and solvable using the provided information. Therefore, the problem is valid.\n\nWe will analyze the spatial dimensions, denoted by $(H, W)$, as they are transformed by the layers of the specified U-Net architecture. The operations on the height dimension $H$ and the width dimension $W$ are identical and independent. We will derive the expressions for $H$, and the results for $W$ follow by analogy.\n\nThe fundamental formulae for the output size $S_{out}$ given an input size $S_{in}$, kernel size $k$, stride $s$, padding $p$, and dilation $d$ are:\n- For a standard 2D convolution: $S_{out} = \\lfloor \\frac{S_{in} + 2p - d(k-1) - 1}{s} \\rfloor + 1$.\n- For a max-pooling layer: $S_{out} = \\lfloor \\frac{S_{in} + 2p - k}{s} \\rfloor + 1$.\n- For a transposed convolution: $S_{out} = (S_{in} - 1)s - 2p + k + p_{out}$, where $p_{out}$ is the output padding.\n\nLet's apply these to the specified architecture.\n\n**1. Layer-by-Layer Spatial Size Calculation**\n\nLet the input image size be $(H, W)$. Let $f_{down}(x) = \\lfloor x/2 \\rfloor$.\n\n**Encoder Path**\n- **Input:** The initial spatial size is $(H, W)$.\n\n- **Encoder stage 1:**\n  - Two convolutions with kernel size $k=3$, stride $s=1$, dilation $d=1$, and \"same\" padding. \"Same\" padding is defined to preserve spatial size. For $k=3, s=1, d=1$, this requires $p=1$. The size after these two convolutions remains $(H, W)$. This feature map, denoted $S_1$, is passed to the skip connection.\n  - One max-pooling layer with $k=2, s=2, p=0$. The output size is $S_{out} = \\lfloor \\frac{S_{in} - 2}{2} \\rfloor + 1 = \\lfloor S_{in}/2 - 1 \\rfloor + 1 = \\lfloor S_{in}/2 \\rfloor = f_{down}(S_{in})$.\n  - Output size of stage 1: $(f_{down}(H), f_{down}(W))$.\n\n- **Encoder stage 2:**\n  - Input size: $(f_{down}(H), f_{down}(W))$.\n  - Two \"same\" padded convolutions preserve the size. The feature map $S_2$ for the skip connection has size $(f_{down}(H), f_{down}(W))$.\n  - One max-pooling layer reduces the size.\n  - Output size of stage 2: $(f_{down}(f_{down}(H)), f_{down}(f_{down}(W)))$.\n\n- **Encoder stage 3:**\n  - Input size: $(f_{down}(f_{down}(H)), f_{down}(f_{down}(W)))$.\n  - Two \"same\" padded convolutions preserve the size. The feature map $S_3$ for the skip connection has size $(f_{down}(f_{down}(H)), f_{down}(f_{down}(W)))$.\n  - One max-pooling layer reduces the size.\n  - Output size of stage 3: $(f_{down}(f_{down}(f_{down}(H))), f_{down}(f_{down}(f_{down}(W))))$.\n\n**Bottleneck**\n- Input size: $(f_{down}(f_{down}(f_{down}(H))), f_{down}(f_{down}(f_{down}(W))))$.\n- Two \"same\" padded convolutions preserve the size.\n- Output size of bottleneck: $(f_{down}(f_{down}(f_{down}(H))), f_{down}(f_{down}(f_{down}(W))))$. Let's denote this $(H_b, W_b)$.\n\n**Decoder Path and Skip Connection Mismatches**\nLet $f_{up}(x) = 2x$.\n\n- **Decoder stage 3:**\n  - Input from bottleneck: $(H_b, W_b) = (f_{down}(f_{down}(f_{down}(H))), f_{down}(f_{down}(f_{down}(W))))$.\n  - One transposed convolution with $k=2, s=2, p=0, p_{out}=0$. The output size is $S_{out} = (S_{in}-1) \\cdot 2 - 0 + 2 + 0 = 2S_{in}$. The size becomes $(2H_b, 2W_b)$. Let's call this $(H_{u3}, W_{u3})$.\n  - Skip connection $S_3$ size: $(f_{down}(f_{down}(H)), f_{down}(f_{down}(W)))$.\n  - **Mismatch for $H$ dimension:** The mismatch is $\\Delta H_3 = f_{down}(f_{down}(H)) - 2H_b = f_{down}(f_{down}(H)) - 2 f_{down}(f_{down}(f_{down}(H)))$.\n    This difference is $f_{down}(f_{down}(H)) \\pmod 2$, which is $0$ if $f_{down}(f_{down}(H))$ is even, and $1$ if it is odd. Symmetric cropping of $S_3$ is required if this mismatch is non-zero. The width mismatch $\\Delta W_3$ is analogous.\n  - After cropping and concatenation, the size is $(2H_b, 2W_b)$.\n  - Two \"same\" convolutions preserve the size.\n  - Output size of decoder stage 3: $(H_{d3}, W_{d3}) = (2H_b, 2W_b)$.\n\n- **Decoder stage 2:**\n  - Input from decoder stage 3: $(H_{d3}, W_{d3}) = (2H_b, 2W_b)$.\n  - One transposed convolution doubles the size to $(2 H_{d3}, 2 W_{d3}) = (4H_b, 4W_b)$. Let's call this $(H_{u2}, W_{u2})$.\n  - Skip connection $S_2$ size: $(f_{down}(H), f_{down}(W))$.\n  - **Mismatch for $H$ dimension:** $\\Delta H_2 = f_{down}(H) - 4H_b = f_{down}(H) - 4 f_{down}(f_{down}(f_{down}(H)))$. Symmetric cropping of $S_2$ is required to match the size $(4H_b, 4W_b)$.\n  - After cropping and concatenation, the size is $(4H_b, 4W_b)$.\n  - Two \"same\" convolutions preserve the size.\n  - Output size of decoder stage 2: $(H_{d2}, W_{d2}) = (4H_b, 4W_b)$.\n\n- **Decoder stage 1:**\n  - Input from decoder stage 2: $(H_{d2}, W_{d2}) = (4H_b, 4W_b)$.\n  - One transposed convolution doubles the size to $(2 H_{d2}, 2 W_{d2}) = (8H_b, 8W_b)$. Let's call this $(H_{u1}, W_{u1})$.\n  - Skip connection $S_1$ size: $(H, W)$.\n  - **Mismatch for $H$ dimension:** $\\Delta H_1 = H - 8H_b = H - 8 f_{down}(f_{down}(f_{down}(H)))$. Symmetric cropping of $S_1$ is required to match the size $(8H_b, 8W_b)$.\n  - After cropping and concatenation, the size is $(8H_b, 8W_b)$.\n  - Two \"same\" convolutions preserve the size.\n  - Output size of decoder stage 1: $(H_{d1}, W_{d1}) = (8H_b, 8W_b)$.\n\n**Final Prediction Layer**\n- Input size: $(8H_b, 8W_b)$.\n- A $1 \\times 1$ convolution with stride $1$ preserves the spatial size.\n- Final output size: $(H_{out}, W_{out}) = (8H_b, 8W_b)$.\n\n**2. Closed-Form Expression for Final Size**\n\nThe final output height is given by:\n$$ H_{out} = 8 H_b = 8 \\cdot f_{down}(f_{down}(f_{down}(H))) = 8 \\left\\lfloor \\frac{\\lfloor \\frac{\\lfloor H/2 \\rfloor}{2} \\rfloor}{2} \\right\\rfloor $$\nThis expression can be simplified. A property of the floor function is that $\\lfloor \\lfloor x/n \\rfloor / m \\rfloor = \\lfloor x/(nm) \\rfloor$. Applying this property iteratively:\n$$ H_{out} = 8 \\left\\lfloor \\frac{\\lfloor H/4 \\rfloor}{2} \\right\\rfloor = 8 \\left\\lfloor \\frac{H}{8} \\right\\rfloor $$\nThis expression calculates the largest multiple of $8$ that is less than or equal to $H$.\nThe same logic applies to the width dimension $W$. Thus, the final spatial size of the segmentation map is:\n$$ (H_{out}, W_{out}) = \\left( 8 \\left\\lfloor \\frac{H}{8} \\right\\rfloor, 8 \\left\\lfloor \\frac{W}{8} \\right\\rfloor \\right) $$\n\n**3. Discussion of Padding Strategy**\n\nThe choice of padding in the convolutional layers significantly impacts the network's behavior, particularly for tile-based analysis.\n\n- **\"Same\" Padding (as specified):** With \"same\" padding, the convolutions within each encoder and decoder block do not shrink the feature maps. Size reduction only occurs at max-pooling layers. This simplifies the architecture but, as shown, leads to an asymmetry between the down-sampling path ($S_{out} = \\lfloor S_{in}/2 \\rfloor$) and the up-sampling path ($S_{out} = 2S_{in}$). This asymmetry necessitates cropping of the high-resolution feature maps from the skip connections before concatenation. The final output has a smaller size than the input, unless input dimensions are multiples of $2^D$, where $D=3$ is the number of down-sampling stages. When processing tiles, the output tiles have the same dimension issues. While this makes stitching seem conceptually simple if input sizes are chosen carefully (e.g., multiples of $8$), the predictions at the borders of each output tile are inherently less reliable. This is because the receptive fields of these output pixels extend into the zero-padded regions of the original input tile, which is artificial data. Simply stitching these tiles would create a grid of artifact-laden seams in the final whole-slide segmentation. An explicit post-processing step to crop the unreliable borders from each output tile would be necessary.\n\n- **\"Valid\" Padding:** With \"valid\" padding ($p=0$), each convolution would reduce the spatial size. For a $3 \\times 3$ kernel, the size shrinks by $2$ at each layer ($S_{out} = S_{in} - 2$). A block of two convolutions would shrink the size by $4$. This would create a much larger size discrepancy at the skip connections, requiring more aggressive cropping. The final output of the network would be significantly smaller than the input.\n  In the context of tile-based histopathology inference, this is often the preferable strategy. The \"overlap-tile\" method is a standard technique to mitigate border artifacts. One extracts overlapping input tiles from the whole-slide image, processes each one through the network, and then discards the unreliable border region of each output prediction map before stitching the valid central parts together. A U-Net with \"valid\" convolutions naturally implements the discarding of borders. The shrinking of the feature map means that every pixel in the final, smaller output map is calculated based on a receptive field that was fully contained within the valid (non-padded) region of the input tile. From a sampling theory perspective, this ensures that every output is a function only of true image signal, not artificial padding. This increases the fidelity of the results and simplifies the stitching logic, as the network's architecture itself defines the \"valid\" region of the output, obviating the need for a separate manual cropping parameter in post-processing. Therefore, for producing a high-quality, seamless segmentation of a large image from tiles, \"valid\" padding is the more principled and robust choice.", "answer": "$$ \\boxed{ \\begin{pmatrix} 8 \\left\\lfloor \\frac{H}{8} \\right\\rfloor & 8 \\left\\lfloor \\frac{W}{8} \\right\\rfloor \\end{pmatrix} } $$", "id": "4332691"}, {"introduction": "Effective deep learning in systems biomedicine goes beyond architecture; it requires a principled statistical approach to data modeling. This practice delves into the probabilistic heart of deep generative models for single-cell RNA sequencing (scRNA-seq), a technology that generates sparse and noisy count data. By deriving the Negative Binomial likelihood from a Poisson-Gamma mixture and computing its gradient, you will uncover the mathematical engine that allows Variational Autoencoders to learn meaningful biological representations from complex genomic datasets [@problem_id:4332692].", "problem": "A deep generative model for single-cell RNA sequencing (scRNA-seq) with Unique Molecular Identifier (UMI) counts often assumes that per-gene counts in each cell arise from a Negative Binomial distribution, motivated by a Poisson process with a Gamma-distributed rate. Consider a single gene in a single cell with observed UMI count $y \\in \\{0,1,2,\\ldots\\}$ and model parameters: a mean parameter $\\mu > 0$ produced by the decoder of a Variational Autoencoder (VAE), and an inverse-dispersion parameter $\\theta > 0$ that controls overdispersion. The standard mixture construction is: draw a latent rate $\\lambda \\sim \\operatorname{Gamma}(\\text{shape}=\\theta,\\ \\text{rate}=\\theta/\\mu)$ and then sample the observed count $y \\mid \\lambda \\sim \\operatorname{Poisson}(\\lambda)$. This construction ensures $\\mathbb{E}[\\lambda]=\\mu$ and yields a Negative Binomial marginal for $y$.\n\nStarting from the definitions of the Poisson probability mass function and the Gamma probability density function, and using only well-tested integral identities for the Gamma function, derive the marginal log-likelihood $\\log p(y \\mid \\mu,\\theta)$ for the UMI count $y$ implied by the mixture construction. Then, compute the gradient $\\frac{\\partial}{\\partial \\mu}\\log p(y \\mid \\mu,\\theta)$ while holding $\\theta$ fixed. Express your final answer as a single closed-form expression for $\\frac{\\partial}{\\partial \\mu}\\log p(y \\mid \\mu,\\theta)$. No rounding is required, and no physical units apply.", "solution": "The problem requires the derivation of the marginal log-likelihood for a count variable $y$ that follows a Poisson-Gamma mixture distribution, and subsequently, the computation of its gradient with respect to the mean parameter $\\mu$.\n\nThe hierarchical model is defined as:\n$1$. The latent rate $\\lambda$ is drawn from a Gamma distribution with a shape parameter $\\theta > 0$ and a rate parameter $\\theta/\\mu$, where $\\mu > 0$. The probability density function (PDF) for $\\lambda$ is given by:\n$$p(\\lambda \\mid \\mu, \\theta) = \\frac{(\\theta/\\mu)^\\theta}{\\Gamma(\\theta)} \\lambda^{\\theta-1} \\exp\\left(-\\frac{\\theta}{\\mu}\\lambda\\right)$$\nfor $\\lambda > 0$. Here, $\\Gamma(\\cdot)$ denotes the Gamma function.\n\n$2$. The observed UMI count $y \\in \\{0, 1, 2, \\ldots\\}$ is drawn from a Poisson distribution with rate $\\lambda$. The probability mass function (PMF) for $y$ given $\\lambda$ is:\n$$p(y \\mid \\lambda) = \\frac{\\lambda^y \\exp(-\\lambda)}{y!}$$\n\nThe marginal probability of observing the count $y$ given the parameters $\\mu$ and $\\theta$, denoted $p(y \\mid \\mu, \\theta)$, is found by integrating the joint probability $p(y, \\lambda \\mid \\mu, \\theta) = p(y \\mid \\lambda) p(\\lambda \\mid \\mu, \\theta)$ over all possible values of the latent variable $\\lambda$.\n$$p(y \\mid \\mu, \\theta) = \\int_0^\\infty p(y \\mid \\lambda) p(\\lambda \\mid \\mu, \\theta) \\, d\\lambda$$\nSubstituting the expressions for the Poisson PMF and the Gamma PDF:\n$$p(y \\mid \\mu, \\theta) = \\int_0^\\infty \\left( \\frac{\\lambda^y \\exp(-\\lambda)}{y!} \\right) \\left( \\frac{(\\theta/\\mu)^\\theta}{\\Gamma(\\theta)} \\lambda^{\\theta-1} \\exp\\left(-\\frac{\\theta}{\\mu}\\lambda\\right) \\right) \\, d\\lambda$$\nWe can rearrange the terms by collecting constants with respect to $\\lambda$ outside the integral:\n$$p(y \\mid \\mu, \\theta) = \\frac{(\\theta/\\mu)^\\theta}{y! \\Gamma(\\theta)} \\int_0^\\infty \\lambda^y \\lambda^{\\theta-1} \\exp(-\\lambda) \\exp\\left(-\\frac{\\theta}{\\mu}\\lambda\\right) \\, d\\lambda$$\n$$p(y \\mid \\mu, \\theta) = \\frac{\\theta^\\theta}{\\mu^\\theta y! \\Gamma(\\theta)} \\int_0^\\infty \\lambda^{y+\\theta-1} \\exp\\left(-\\left(1 + \\frac{\\theta}{\\mu}\\right)\\lambda\\right) \\, d\\lambda$$\nThe integral has the form of a Gamma function. Specifically, we use the identity $\\int_0^\\infty x^{k-1} \\exp(-\\beta x) \\, dx = \\frac{\\Gamma(k)}{\\beta^k}$. In our expression, the shape is $k = y+\\theta$ and the rate is $\\beta = 1 + \\frac{\\theta}{\\mu} = \\frac{\\mu+\\theta}{\\mu}$.\nApplying this identity, the integral evaluates to:\n$$\\int_0^\\infty \\lambda^{y+\\theta-1} \\exp\\left(-\\left(\\frac{\\mu+\\theta}{\\mu}\\right)\\lambda\\right) \\, d\\lambda = \\frac{\\Gamma(y+\\theta)}{\\left(\\frac{\\mu+\\theta}{\\mu}\\right)^{y+\\theta}}$$\nSubstituting this back into the expression for $p(y \\mid \\mu, \\theta)$:\n$$p(y \\mid \\mu, \\theta) = \\frac{\\theta^\\theta}{\\mu^\\theta y! \\Gamma(\\theta)} \\cdot \\frac{\\Gamma(y+\\theta)}{\\left(\\frac{\\mu+\\theta}{\\mu}\\right)^{y+\\theta}}$$\nWe can simplify this expression:\n$$p(y \\mid \\mu, \\theta) = \\frac{\\Gamma(y+\\theta)}{y! \\Gamma(\\theta)} \\frac{\\theta^\\theta}{\\mu^\\theta} \\frac{\\mu^{y+\\theta}}{(\\mu+\\theta)^{y+\\theta}}$$\n$$p(y \\mid \\mu, \\theta) = \\frac{\\Gamma(y+\\theta)}{y! \\Gamma(\\theta)} \\frac{\\theta^\\theta \\mu^y}{(\\mu+\\theta)^{y+\\theta}}$$\nThis can be written in the recognizable form of the Negative Binomial distribution PMF:\n$$p(y \\mid \\mu, \\theta) = \\frac{\\Gamma(y+\\theta)}{y! \\Gamma(\\theta)} \\left(\\frac{\\theta}{\\mu+\\theta}\\right)^\\theta \\left(\\frac{\\mu}{\\mu+\\theta}\\right)^y$$\n\nThe next step is to find the marginal log-likelihood, $\\log p(y \\mid \\mu, \\theta)$. Taking the natural logarithm of the expression derived above:\n$$\\log p(y \\mid \\mu, \\theta) = \\log\\left( \\frac{\\Gamma(y+\\theta)}{y! \\Gamma(\\theta)} \\right) + \\log\\left( \\left(\\frac{\\theta}{\\mu+\\theta}\\right)^\\theta \\right) + \\log\\left( \\left(\\frac{\\mu}{\\mu+\\theta}\\right)^y \\right)$$\nUsing the properties of logarithms, $\\log(a^b) = b\\log(a)$ and $\\log(a/b) = \\log(a) - \\log(b)$:\n$$\\log p(y \\mid \\mu, \\theta) = \\log(\\Gamma(y+\\theta)) - \\log(y!) - \\log(\\Gamma(\\theta)) + \\theta \\log\\left(\\frac{\\theta}{\\mu+\\theta}\\right) + y \\log\\left(\\frac{\\mu}{\\mu+\\theta}\\right)$$\n$$\\log p(y \\mid \\mu, \\theta) = \\log(\\Gamma(y+\\theta)) - \\log(y!) - \\log(\\Gamma(\\theta)) + \\theta(\\log\\theta - \\log(\\mu+\\theta)) + y(\\log\\mu - \\log(\\mu+\\theta))$$\nCombining terms:\n$$\\log p(y \\mid \\mu, \\theta) = \\log(\\Gamma(y+\\theta)) - \\log(y!) - \\log(\\Gamma(\\theta)) + \\theta\\log\\theta + y\\log\\mu - (y+\\theta)\\log(\\mu+\\theta)$$\n\nFinally, we compute the gradient of the log-likelihood with respect to $\\mu$, holding $\\theta$ fixed. We differentiate the expression for $\\log p(y \\mid \\mu, \\theta)$ with respect to $\\mu$. The terms $\\log(\\Gamma(y+\\theta))$, $\\log(y!)$, $\\log(\\Gamma(\\theta))$, and $\\theta\\log\\theta$ do not depend on $\\mu$, so their derivatives are $0$.\n$$\\frac{\\partial}{\\partial \\mu}\\log p(y \\mid \\mu,\\theta) = \\frac{\\partial}{\\partial \\mu} \\left[ y\\log\\mu - (y+\\theta)\\log(\\mu+\\theta) \\right]$$\nApplying the rules of differentiation, $\\frac{d}{dx}\\log(x) = \\frac{1}{x}$ and the chain rule:\n$$\\frac{\\partial}{\\partial \\mu} (y\\log\\mu) = \\frac{y}{\\mu}$$\n$$\\frac{\\partial}{\\partial \\mu} (-(y+\\theta)\\log(\\mu+\\theta)) = -(y+\\theta) \\cdot \\frac{1}{\\mu+\\theta} \\cdot \\frac{\\partial}{\\partial \\mu}(\\mu+\\theta) = -(y+\\theta) \\cdot \\frac{1}{\\mu+\\theta} \\cdot 1 = -\\frac{y+\\theta}{\\mu+\\theta}$$\nCombining these results gives the gradient:\n$$\\frac{\\partial}{\\partial \\mu}\\log p(y \\mid \\mu,\\theta) = \\frac{y}{\\mu} - \\frac{y+\\theta}{\\mu+\\theta}$$\nTo express this as a single fraction, we find a common denominator, which is $\\mu(\\mu+\\theta)$:\n$$\\frac{\\partial}{\\partial \\mu}\\log p(y \\mid \\mu,\\theta) = \\frac{y(\\mu+\\theta) - \\mu(y+\\theta)}{\\mu(\\mu+\\theta)}$$\n$$= \\frac{y\\mu + y\\theta - \\mu y - \\mu\\theta}{\\mu(\\mu+\\theta)}$$\n$$= \\frac{y\\theta - \\mu\\theta}{\\mu(\\mu+\\theta)}$$\nFactoring out $\\theta$ from the numerator yields the final closed-form expression:\n$$\\frac{\\partial}{\\partial \\mu}\\log p(y \\mid \\mu,\\theta) = \\frac{\\theta(y-\\mu)}{\\mu(\\mu+\\theta)}$$\nThis expression represents the rate of change of the model's log-likelihood with respect to its mean parameter $\\mu$, a quantity essential for gradient-based optimization of the VAE decoder.", "answer": "$$\\boxed{\\frac{\\theta(y-\\mu)}{\\mu(\\mu+\\theta)}}$$", "id": "4332692"}, {"introduction": "A model's prediction is often just the beginning; in biomedicine, we must also ask *why* a prediction was made. This exercise tackles the \"black box\" problem by exploring Integrated Gradients, a powerful and theoretically grounded method for attributing a model's output to its input features. Deriving the attribution formula from first principles and applying it to a simple model demystifies the process, equipping you with the foundational knowledge to implement and interpret feature importance in your own deep learning applications [@problem_id:4332698].", "problem": "A systems biomedicine pipeline uses a differentiable scalar predictive model $F:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ to map a vector of standardized molecular features (for example, transcript abundances or protein markers) $x\\in\\mathbb{R}^{n}$ to a disease activity score. To interpret feature contributions, consider Integrated Gradients (IG), defined for a baseline representing a healthy homeostatic state $x'\\in\\mathbb{R}^{n}$ and a straight-line path from $x'$ to $x$. Starting only from the gradient definition, the chain rule, and the fundamental theorem of line integrals for conservative vector fields (namely, that for a differentiable scalar field $F$, the integral of $\\nabla F$ along any smooth path between two points equals the difference in $F$ evaluated at the endpoints), derive an analytic expression for the attribution assigned to feature $i$ along the straight-line path from $x'$ to $x$. Then, specialize your expression to the linear model $F(x)=w^{\\top}x+b$ and compute the attribution for feature $i=2$ for the following values:\n- $w=\\left(0.92,\\,-0.68,\\,0.11,\\,0.35\\right)$,\n- $x=\\left(0.50,\\,1.25,\\,-0.30,\\,0.80\\right)$,\n- $x'=\\left(0.10,\\,0.15,\\,-0.30,\\,0.20\\right)$.\nProvide the final numerical value for the feature $i=2$ attribution as an exact number; no rounding is required. Do not include units.", "solution": "The problem requires the derivation of an analytic expression for the Integrated Gradients (IG) attribution for a feature $i$, starting from fundamental principles. This expression must then be specialized for a linear model and used to compute a specific numerical value.\n\nFirst, we derive the general expression for the attribution. The problem states that we should use the fundamental theorem of line integrals for conservative vector fields. For a differentiable scalar field $F:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$, its gradient $\\nabla F$ is a conservative vector field. The theorem states that the integral of $\\nabla F$ along any smooth path $\\gamma$ from a point $x'$ to a point $x$ is equal to the difference in the scalar field's value at the endpoints:\n$$F(x) - F(x') = \\int_{\\gamma} \\nabla F(\\mathbf{r}) \\cdot d\\mathbf{r}$$\nThe problem specifies a straight-line path from the baseline $x'$ to the input $x$. We can parameterize this path as $\\gamma(\\alpha) = x' + \\alpha(x - x')$ for a parameter $\\alpha \\in [0, 1]$. At $\\alpha=0$, we have $\\gamma(0) = x'$, and at $\\alpha=1$, we have $\\gamma(1) = x' + (x - x') = x$. The differential path element is $d\\mathbf{r} = \\frac{d\\gamma}{d\\alpha}d\\alpha = (x - x')d\\alpha$.\n\nSubstituting this parameterization into the line integral gives:\n$$F(x) - F(x') = \\int_{0}^{1} \\nabla F(\\gamma(\\alpha)) \\cdot \\frac{d\\gamma}{d\\alpha} d\\alpha$$\n$$F(x) - F(x') = \\int_{0}^{1} \\nabla F(x' + \\alpha(x - x')) \\cdot (x - x') d\\alpha$$\nThe gradient vector is $\\nabla F(\\mathbf{r}) = \\left(\\frac{\\partial F}{\\partial x_1}, \\frac{\\partial F}{\\partial x_2}, \\dots, \\frac{\\partial F}{\\partial x_n}\\right)$, and the vector $(x - x')$ is $(x_1 - x'_1, x_2 - x'_2, \\dots, x_n - x'_n)$. The dot product in the integrand is:\n$$\\nabla F(x' + \\alpha(x - x')) \\cdot (x - x') = \\sum_{i=1}^{n} \\frac{\\partial F}{\\partial x_i}\\bigg|_{\\mathbf{r}=x' + \\alpha(x - x')} (x_i - x'_i)$$\nwhere $\\frac{\\partial F}{\\partial x_i}\\big|_{\\mathbf{r}=\\dots}$ denotes the partial derivative evaluated at the point $\\mathbf{r}$ on the path.\n\nSubstituting this sum into the integral expression for the total change in $F$:\n$$F(x) - F(x') = \\int_{0}^{1} \\sum_{i=1}^{n} \\frac{\\partial F}{\\partial x_i}\\bigg|_{x' + \\alpha(x - x')} (x_i - x'_i) d\\alpha$$\nSince the sum is finite, we can interchange the summation and integration operators:\n$$F(x) - F(x') = \\sum_{i=1}^{n} \\int_{0}^{1} \\frac{\\partial F}{\\partial x_i}\\bigg|_{x' + \\alpha(x - x')} (x_i - x'_i) d\\alpha$$\nThe term $(x_i - x'_i)$ is constant with respect to the integration variable $\\alpha$, so it can be moved outside the integral:\n$$F(x) - F(x') = \\sum_{i=1}^{n} (x_i - x'_i) \\int_{0}^{1} \\frac{\\partial F}{\\partial x_i}\\bigg|_{x' + \\alpha(x - x')} d\\alpha$$\nThe Integrated Gradients framework defines the attribution for feature $i$, denoted as $IG_i(x, x')$, as the $i$-th term in this summation. This ensures that the sum of attributions over all features equals the total change $F(x) - F(x')$. Thus, the analytic expression for the attribution assigned to feature $i$ is:\n$$IG_i(x, x') = (x_i - x'_i) \\int_{0}^{1} \\frac{\\partial F}{\\partial x_i}\\bigg|_{x' + \\alpha(x - x')} d\\alpha$$\nThis completes the first part of the problem.\n\nNext, we specialize this expression for the linear model $F(x) = w^{\\top}x + b$, which can be written as $F(x) = \\sum_{j=1}^{n} w_j x_j + b$. We need to find the partial derivative of $F$ with respect to a feature $x_i$:\n$$\\frac{\\partial F}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left(\\sum_{j=1}^{n} w_j x_j + b\\right) = w_i$$\nNotably, for a linear model, the partial derivative $\\frac{\\partial F}{\\partial x_i}$ is a constant, $w_i$, and does not depend on the input vector $x$. Therefore, its value is constant along the entire integration path.\n\nSubstituting this into the general attribution formula:\n$$IG_i(x, x') = (x_i - x'_i) \\int_{0}^{1} w_i d\\alpha$$\nThe integral of the constant $w_i$ over the interval $[0, 1]$ is:\n$$\\int_{0}^{1} w_i d\\alpha = w_i [\\alpha]_{0}^{1} = w_i (1 - 0) = w_i$$\nSo, for a linear model, the attribution for feature $i$ simplifies to:\n$$IG_i(x, x') = w_i (x_i - x'_i)$$\n\nFinally, we compute the attribution for feature $i=2$ using the given numerical values:\n- Weight vector: $w=\\left(0.92,\\,-0.68,\\,0.11,\\,0.35\\right)$, so the weight for feature $2$ is $w_2 = -0.68$.\n- Input feature vector: $x=\\left(0.50,\\,1.25,\\,-0.30,\\,0.80\\right)$, so the value for feature $2$ is $x_2 = 1.25$.\n- Baseline feature vector: $x'=\\left(0.10,\\,0.15,\\,-0.30,\\,0.20\\right)$, so the baseline value for feature $2$ is $x'_2 = 0.15$.\n\nUsing the specialized formula for the linear model:\n$$IG_2(x, x') = w_2 (x_2 - x'_2)$$\nSubstituting the values:\n$$IG_2(x, x') = (-0.68) \\times (1.25 - 0.15)$$\n$$IG_2(x, x') = (-0.68) \\times (1.10)$$\n$$IG_2(x, x') = -0.748$$\nThe attribution for feature $i=2$ is $-0.748$.", "answer": "$$\\boxed{-0.748}$$", "id": "4332698"}]}