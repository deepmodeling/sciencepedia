{"hands_on_practices": [{"introduction": "A successful Genome-Wide Association Study (GWAS) relies not only on powerful statistical tests but also on rigorous study design to prevent confounding. This exercise explores a critical pitfall known as a \"batch effect,\" where technical variations are systematically correlated with the phenotype of interest, leading to widespread false-positive associations. By analyzing a hypothetical but realistic scenario, you will develop the crucial skill of diagnosing systematic bias and distinguishing it from true biological signals or other sources of statistical inflation [@problem_id:1494331].", "problem": "A large research consortium is conducting a Genome-Wide Association Study (GWAS) to identify genetic variants associated with \"Aero-Metabolic Syndrome\" (AMS), a complex respiratory disease. The study design involves 10,000 cases recruited from a network of specialized respiratory clinics and 10,000 controls selected from a general population health registry. To manage the large sample size efficiently, the genotyping process was distributed. DNA samples from all 10,000 cases were processed and genotyped for millions of Single Nucleotide Polymorphisms (SNPs) using the \"BioArray v2.0\" platform at a central laboratory. Concurrently, DNA samples from all 10,000 controls were processed and genotyped using the \"GenoChip v3.5\" platform at a different facility.\n\nAfter the data from both platforms were merged and subjected to standard quality control, a statistical association test was performed for each SNP. The analysis revealed a striking result: tens of thousands of SNPs, distributed across every chromosome, showed a statistically significant association with AMS, far exceeding the typical findings for a complex disease GWAS.\n\nWhich of the following options provides the most plausible explanation for this genome-wide excess of significant associations?\n\nA. Population stratification: The case and control groups were drawn from populations with systematically different genetic ancestries, and these ancestry-related allele frequency differences are being misinterpreted as disease associations.\n\nB. Polygenic architecture: AMS is a highly polygenic disease, where its genetic risk is genuinely determined by the cumulative small effects of tens of thousands of common variants spread throughout the genome.\n\nC. Selection bias: Recruiting cases from specialized clinics resulted in a cohort with a more severe form of the disease, enriching for a large number of rare, high-impact variants not found in the general population controls.\n\nD. Systematic genotyping bias: The two different genotyping platforms have small, consistent, and platform-specific inaccuracies in allele measurement that, because they are perfectly correlated with the case-control labels, are creating widespread false-positive signals.\n\nE. Cryptic relatedness: The case group unknowingly contains a large number of closely related individuals, which violates the statistical assumption of sample independence and inflates association signals at loci shared within families.", "solution": "We formalize the per-SNP association test as either an allelic chi-square test or a logistic regression of case status on genotype. Let $Y \\in \\{0,1\\}$ denote case status, $G \\in \\{0,1,2\\}$ the coded genotype, and consider for each SNP the null hypothesis $H_{0}: \\beta_{G} = 0$ in the logistic model $\\ln\\left(\\frac{\\Pr(Y=1 \\mid G)}{\\Pr(Y=0 \\mid G)}\\right) = \\alpha + \\beta_{G} G$. Under correct model specification and no confounding, the score or Wald test statistic has an asymptotic $\\chi^{2}_{1}$ distribution under $H_{0}$, so $p$-values across the genome should be approximately uniform under the null aside from a small number of true associations in a complex disease.\n\nThe observed pattern is a genome-wide excess of significant associations across every chromosome with tens of thousands of SNPs reaching significance. Such a pervasive inflation indicates a systematic deviation from the null that affects a very large fraction of SNPs. We examine each option by linking it to the test behavior.\n\nOption B (polygenic architecture): In a highly polygenic trait, many variants have small true effects. The power to detect a SNP at a stringent genome-wide threshold $\\alpha_{GW}$ (e.g., $\\alpha_{GW} \\approx 5 \\times 10^{-8}$) depends on the noncentrality parameter $\\Lambda$ of the association test. For an additive test, a standard approximation gives $\\Lambda \\approx N_{\\text{eff}} \\cdot 2 p (1 - p) \\cdot (\\ln \\theta)^{2}$, where $p$ is the minor allele frequency, $\\theta$ is the per-allele odds ratio, and $N_{\\text{eff}}$ is an effective sample size function of the case and control counts. For realistic complex traits with small $\\ln \\theta$, even large $N_{\\text{eff}}$ yields limited power at $\\alpha_{GW}$. Therefore, polygenicity alone does not generate tens of thousands of genome-wide significant hits with this sample size; it yields a polygenic signal detectable by methods like LD score regression or polygenic scores, not ubiquitous single-SNP significance.\n\nOption C (selection bias toward severity): Recruiting more severe cases might enrich for rare, high-impact variants. Arrays primarily target common variants; rare, severe alleles are often poorly tagged. Moreover, such enrichment does not create consistent allele frequency differences at common SNPs across the entire genome; it would not yield widespread false positives at common variants.\n\nOption E (cryptic relatedness): Relatedness violates independence, affecting variance estimates and inflating the test statistic if unmodeled. However, this inflation typically depends on sharing patterns and is not uniform across all SNPs; in practice it produces a modest, genome-wide inflation factor and sometimes localized signals, not tens of thousands of significant associations across all chromosomes. Furthermore, relatedness reduces the effective sample size, which usually decreases, rather than massively increases, the number of genome-wide significant hits after proper standard errors are used.\n\nOption A (population stratification): Systematic ancestry differences between cases and controls induce genome-wide allele frequency differences due to demographic history rather than disease, inflating association statistics across many SNPs. If uncorrected, this can plausibly create widespread significance. However, standard GWAS pipelines include adjustment for principal components of ancestry, which typically mitigates most stratification. While A could explain broad inflation, the study design provides a more direct and stronger source of confounding.\n\nOption D (systematic genotyping bias from different platforms): Here, genotyping platform is perfectly confounded with phenotype: all cases on BioArray v2.0 and all controls on GenoChip v3.5. Let $B \\in \\{0,1\\}$ denote platform, with $B=1$ for cases and $B=0$ for controls. Let the true additive genotype be $G$, but the measured genotype be $\\tilde{G} = G + \\epsilon_{B}$, where $E[\\epsilon_{1}] \\neq E[\\epsilon_{0}]$ or the error distribution differs by platform due to differential calling, differential missingness, or probe chemistry. The expected difference in measured mean genotype between cases and controls is\n$$\nE[\\tilde{G} \\mid Y=1] - E[\\tilde{G} \\mid Y=0] = \\underbrace{E[G \\mid Y=1] - E[G \\mid Y=0]}_{\\Delta \\text{ (true effect)}} + \\underbrace{E[\\epsilon_{1}] - E[\\epsilon_{0}]}_{\\delta \\text{ (platform bias)}}.\n$$\nUnder the null for most SNPs, $\\Delta = 0$, but $\\delta \\neq 0$ whenever platform-specific calling bias exists. Because $B$ is perfectly collinear with $Y$, one cannot adjust for $B$ in the association model; omitted variable bias assigns platform-induced differences to the genotype coefficient, producing spurious associations. The test statistic for an allelic chi-square test is approximately\n$$\nT \\approx \\frac{\\left(\\hat{p}_{\\text{case}} - \\hat{p}_{\\text{control}}\\right)^{2}}{\\hat{V}},\n$$\nwhere $\\hat{p}_{\\text{case}}$ and $\\hat{p}_{\\text{control}}$ are observed allele frequencies and $\\hat{V}$ is the estimated variance under the null. If $\\hat{p}_{\\text{case}} - \\hat{p}_{\\text{control}} \\approx \\delta$ across many SNPs due to platform bias, then $T$ is inflated genome-wide. Unlike random non-differential measurement error, which attenuates associations, this is differential misclassification correlated with outcome status, which biases away from the null at many SNPs simultaneously, exactly matching the observed pattern of tens of thousands of significant hits across all chromosomes.\n\nGiven the specific design detail that all cases and controls were genotyped on different platforms at different facilities, systematic platform-specific genotyping bias that is perfectly correlated with case-control status is the most plausible and direct explanation for the genome-wide excess of significant associations.\n\nTherefore, the correct choice is D.", "answer": "$$\\boxed{D}$$", "id": "1494331"}, {"introduction": "Hardy-Weinberg Equilibrium (HWE) is a fundamental principle of population genetics that serves as a powerful quality control tool in GWAS. While a significant deviation from HWE often points to genotyping errors, its interpretation is nuanced and can also reveal important biological or demographic phenomena. This advanced practice challenges you to connect specific causes, from population stratification to different disease models, with their unique signatures in the genotype data of cases and controls [@problem_id:4347885].", "problem": "A central diagnostic in genome-wide association studies (GWAS; Genome-Wide Association Studies) is the Hardy–Weinberg Equilibrium (HWE; Hardy–Weinberg Equilibrium) test within cases and within controls at a biallelic single-nucleotide polymorphism (SNP; single-nucleotide polymorphism). For alleles $A$ and $B$ with allele frequencies $p$ and $q=1-p$ in a panmictic, non-overlapping, randomly mating population without selection, mutation, migration, or technical error, the expected genotype frequencies are $p^2$ for $AA$, $2pq$ for $AB$, and $q^2$ for $BB$. A deviation from these expectations (Hardy–Weinberg disequilibrium) can arise from biological or technical causes. Consider a large case-control GWAS where sampling variance is negligible and assume autosomal SNPs. In each option below, a set of statements proposes a cause and a qualitative manifestation for how genotype counts would differ between cases and controls. Select the option in which all proposed causes and manifestations are simultaneously correct and consistent with first-principles reasoning about HWE.\n\nDefinitions for this problem: within a group (cases or controls), “heterozygote deficit” means the observed count of $AB$ is less than the group’s $2\\hat{p}\\hat{q}\\hat{N}$, where $\\hat{p}$ and $\\hat{q}$ are the group’s allele-frequency estimates and $\\hat{N}$ is its sample size; “heterozygote excess” means the observed $AB$ exceeds $2\\hat{p}\\hat{q}\\hat{N}$.\n\nA. \n- Population stratification limited to controls (controls are a mixture of $2$ ancestries with different allele frequencies, cases are ancestry-homogeneous): the pooled controls exhibit a heterozygote deficit relative to $2\\hat{p}\\hat{q}$ (Wahlund effect), while cases remain consistent with HWE.\n- Case-batch genotyping error that undercalls true $AB$ as $AA$ or $BB$ with nonzero symmetric probability: cases show a heterozygote deficit at essentially unchanged case allele frequency, while controls remain consistent with HWE.\n- A recessive disease model at the typed SNP (increased penetrance only for $BB$): cases show an excess of $BB$ and a corresponding heterozygote deficit relative to $2\\hat{p}\\hat{q}$ computed from the case allele frequency, while controls remain consistent with HWE.\n\nB. \n- Population stratification limited to controls produces a heterozygote excess in the pooled controls relative to $2\\hat{p}\\hat{q}$, while cases remain consistent with HWE.\n- Case-batch genotyping error that undercalls true $AB$ as $AA$ or $BB$ with nonzero symmetric probability produces a heterozygote excess in cases, while controls remain consistent with HWE.\n- A multiplicative per-allele disease model ($AA:AB:BB$ risks proportional to $1:R:R^2$ with $R1$) necessarily causes heterozygote deficit in cases relative to $2\\hat{p}\\hat{q}$ computed from the case allele frequency, while controls remain consistent with HWE.\n\nC. \n- Allele-specific missingness in controls (true $BB$ more likely to be missing) yields a heterozygote deficit among observed control genotypes; cases remain consistent with HWE.\n- Outbreeding among cases (negative assortative mating) yields a heterozygote deficit in cases; controls remain consistent with HWE.\n- A dominant disease model at the typed SNP (increased penetrance for $AB$ and $BB$) yields heterozygote deficit in cases relative to $2\\hat{p}\\hat{q}$ computed from the case allele frequency; controls remain consistent with HWE.\n\nD. \n- Cryptic relatedness concentrated among cases (elevated inbreeding coefficient) causes a heterozygote excess in cases; controls remain consistent with HWE.\n- Population stratification present equally in cases and controls produces identical heterozygote deficits in both groups; when the two groups are pooled, the combined sample recovers HWE.\n- Uniform low-level DNA contamination (a fraction of case samples’ DNA includes a minor proportion of other individuals’ DNA) produces a heterozygote deficit in cases; controls remain consistent with HWE.", "solution": "The problem statement is evaluated as scientifically grounded, well-posed, objective, and complete. It is based on established principles of population genetics and their application to genome-wide association studies (GWAS). The terms are clearly defined, and the scenarios presented are standard topics in the analysis of genetic data. Therefore, the problem is valid, and a full solution can be derived.\n\nThe task is to identify the option (A, B, C, or D) in which all three statements about causes of Hardy-Weinberg disequilibrium (HWD) are correct. We will analyze each statement in each option based on first principles. In all scenarios, we assume the control group is sampled from a population in Hardy-Weinberg Equilibrium (HWE) unless a confounding factor is stated to affect controls. We denote the alleles as $A$ and $B$, with population frequencies $p$ and $q$ respectively. The expected HWE genotype frequencies are $p^2$ ($AA$), $2pq$ ($AB$), and $q^2$ ($BB$). For a given group (e.g., cases or controls) with sample size $\\hat{N}$ and estimated allele frequencies $\\hat{p}$ and $\\hat{q}$, HWD is assessed by comparing the observed count of heterozygotes, $N_{AB}$, to the expected count, $2\\hat{p}\\hat{q}\\hat{N}$. A \"heterozygote deficit\" means $N_{AB}  2\\hat{p}\\hat{q}\\hat{N}$, and a \"heterozygote excess\" means $N_{AB}  2\\hat{p}\\hat{q}\\hat{N}$.\n\n### Option A Analysis\n\n1.  **Population stratification limited to controls:**\n    -   **Cause:** Controls are a mixture of two ancestry-homogeneous subpopulations with different allele frequencies, while cases are from a single homogeneous population.\n    -   **Manifestation:** Pooled controls exhibit a heterozygote deficit (Wahlund effect), while cases remain in HWE.\n    -   **Analysis:** This is a description of the Wahlund effect. Let the two control subpopulations be 1 and 2, mixed in proportions $m_1$ and $m_2 = 1 - m_1$. Let their respective frequencies for allele $A$ be $p_1$ and $p_2$. Within each subpopulation, HWE holds, so the heterozygote frequencies are $2p_1q_1$ and $2p_2q_2$.\n    -   The allele frequency $\\hat{p}$ in the pooled control sample is the weighted average: $\\hat{p} = m_1p_1 + m_2p_2$.\n    -   The observed heterozygote frequency in the pooled sample is also a weighted average: $F_{obs}(AB) = m_1(2p_1q_1) + m_2(2p_2q_2)$.\n    -   The expected heterozygote frequency under HWE, based on the pooled allele frequency, is $F_{exp}(AB) = 2\\hat{p}\\hat{q} = 2(m_1p_1 + m_2p_2)(m_1q_1 + m_2q_2)$.\n    -   The difference is $F_{exp}(AB) - F_{obs}(AB) = 2m_1m_2(p_1-p_2)^2$. Since $m_1, m_2 \\in (0,1)$ and we assume $p_1 \\neq p_2$, this difference is positive. Thus, $F_{obs}(AB)  F_{exp}(AB)$, indicating a heterozygote deficit.\n    -   The cases are ancestry-homogeneous and thus, in the absence of other factors, will conform to HWE.\n    -   **Verdict:** This statement is **Correct**.\n\n2.  **Case-batch genotyping error:**\n    -   **Cause:** A symmetric error in the case batch where true heterozygotes ($AB$) are misclassified as either $AA$ or $BB$.\n    -   **Manifestation:** Cases show a heterozygote deficit at an essentially unchanged case allele frequency.\n    -   **Analysis:** Let the true genotype counts in cases be $N_{AA}$, $N_{AB}$, and $N_{BB}$, conforming to HWE. Let a fraction $\\epsilon  0$ of heterozygotes be miscalled, with half going to $AA$ and half to $BB$.\n    -   The observed counts become: $N'_{AA} = N_{AA} + \\frac{\\epsilon}{2}N_{AB}$, $N'_{AB} = (1-\\epsilon)N_{AB}$, $N'_{BB} = N_{BB} + \\frac{\\epsilon}{2}N_{AB}$.\n    -   The total sample size $N' = N'_{AA} + N'_{AB} + N'_{BB} = N_{AA} + N_{AB} + N_{BB} = N$ remains the same.\n    -   The new count of allele $A$ is $2N'_{AA} + N'_{AB} = 2(N_{AA} + \\frac{\\epsilon}{2}N_{AB}) + (1-\\epsilon)N_{AB} = 2N_{AA} + \\epsilon N_{AB} + N_{AB} - \\epsilon N_{AB} = 2N_{AA} + N_{AB}$. This is the original allele $A$ count. Thus, the allele frequencies $\\hat{p}$ and $\\hat{q}$ are unchanged.\n    -   The expected heterozygote count is based on the original (and current) allele frequencies: $E(N'_{AB}) = 2\\hat{p}\\hat{q}N = N_{AB}$.\n    -   The observed heterozygote count is $N'_{AB} = (1-\\epsilon)N_{AB}$.\n    -   Since $\\epsilon0$, we have $N'_{AB}  E(N'_{AB})$. This is a heterozygote deficit.\n    -   **Verdict:** This statement is **Correct**.\n\n3.  **Recessive disease model:**\n    -   **Cause:** Increased penetrance only for the $BB$ genotype at the typed SNP.\n    -   **Manifestation:** Cases show an excess of $BB$ and a corresponding heterozygote deficit relative to $2\\hat{p}\\hat{q}$ calculated from the case allele frequency.\n    -   **Analysis:** Let the penetrances be $f_{AA}=f_0$, $f_{AB}=f_0$, and $f_{BB}=f_1$, with $f_1  f_0$. The frequencies of genotypes in the general population are $p^2, 2pq, q^2$. The frequencies among cases are proportional to $f_{AA}p^2$, $f_{AB}(2pq)$, and $f_{BB}q^2$, respectively.\n    -   Let $P(\\text{case}) = f_0 p^2 + f_0(2pq) + f_1 q^2$. The genotype frequencies in cases are $P(AA|\\text{case}) = f_0 p^2 / P(\\text{case})$, $P(AB|\\text{case}) = 2f_0 pq / P(\\text{case})$, $P(BB|\\text{case}) = f_1 q^2 / P(\\text{case})$.\n    -   The allele frequencies in cases are $p_{case} = P(AA|\\text{case}) + \\frac{1}{2}P(AB|\\text{case}) = (f_0 p^2 + f_0 pq)/P(\\text{case})$ and $q_{case} = (f_0 pq + f_1 q^2)/P(\\text{case})$.\n    -   We must compare the observed heterozygote frequency in cases, $F(AB)_{case} = 2f_0 pq / P(\\text{case})$, with the expected frequency, $2p_{case}q_{case} = 2 \\frac{(f_0 p^2 + f_0 pq)(f_0 pq + f_1 q^2)}{P(\\text{case})^2}$.\n    -   A more careful derivation shows that this leads to a heterozygote deficit. The enrichment for genotype $BB$ is definitional to the model.\n    -   **Verdict:** This statement is **Correct**.\n\nSince all three statements in Option A are correct, Option A is a candidate for the correct answer.\n\n### Option B Analysis\n\n1.  **Population stratification:** The statement claims this produces a heterozygote *excess*. As proven in A.1, the Wahlund effect causes a heterozygote *deficit*. **Verdict: Incorrect**.\n2.  **Case-batch genotyping error (under-calling AB):** The statement claims this produces a heterozygote *excess*. As proven in A.2, under-calling heterozygotes causes a heterozygote *deficit*. **Verdict: Incorrect**.\n3.  **Multiplicative per-allele disease model:** The statement claims this *necessarily causes a heterozygote deficit*. Let risks be proportional to $1, R, R^2$ for genotypes $AA, AB, BB$ with $R1$. The genotype frequencies in cases are proportional to $p^2, 2Rpq, R^2q^2$. The allele frequencies in cases become $p_{case} = p/(p+Rq)$ and $q_{case} = Rq/(p+Rq)$. The expected genotype frequencies in cases under HWE are $p_{case}^2 = p^2/(p+Rq)^2$, $2p_{case}q_{case}=2Rpq/(p+Rq)^2$, and $q_{case}^2=R^2q^2/(p+Rq)^2$. The observed frequencies are $\\frac{p^2}{D}, \\frac{2Rpq}{D}, \\frac{R^2q^2}{D}$ where $D=p^2+2Rpq+R^2q^2$. Since $D=(p+Rq)^2$, the observed frequencies equal the expected frequencies. The case population remains in HWE. **Verdict: Incorrect**.\n\n### Option C Analysis\n\n1.  **Allele-specific missingness in controls (true BB missing):** The statement claims a heterozygote *deficit*. If $BB$ genotypes are preferentially missing, the relative proportions of $AA$ and $AB$ genotypes increase among the observed samples. This leads to a heterozygote *excess*. **Verdict: Incorrect**.\n2.  **Outbreeding (negative assortative mating):** The statement claims this yields a heterozygote *deficit*. Negative assortative mating is the preferential mating of dissimilar individuals (e.g., $AA \\times BB$). This process increases heterozygosity in the offspring generation. It causes a heterozygote *excess*. **Verdict: Incorrect**.\n3.  **Dominant disease model:** The statement claims this yields a heterozygote *deficit*. A dominant model enriches for both $AB$ and $BB$ genotypes. This leads to a heterozygote *excess* in cases. **Verdict: Incorrect**.\n\n### Option D Analysis\n\n1.  **Cryptic relatedness (inbreeding):** The statement claims this causes a heterozygote *excess*. Inbreeding increases homozygosity at the expense of heterozygosity. With an inbreeding coefficient $F0$, the heterozygote frequency is $2pq(1-F)$, which is less than the HWE expectation of $2pq$. This is a heterozygote *deficit*. **Verdict: Incorrect**.\n2.  **Population stratification in cases and controls:** The statement claims that when two equally stratified groups are pooled, the combined sample recovers HWE. If both the case and control groups have identical substructure and allele frequencies, they will both exhibit the same Wahlund effect (heterozygote deficit). Pooling them creates a larger sample with the exact same substructure and allele frequencies. It will therefore exhibit the exact same HWE deficit, not recover HWE. **Verdict: Incorrect**.\n3.  **Uniform low-level DNA contamination:** The statement claims this produces a heterozygote *deficit*. The most direct consequence of DNA contamination is that a sample from a true homozygote (e.g., $AA$) gets contaminated with DNA carrying the other allele (e.g., from a $BB$ or $AB$ individual). This causes the combined sample to contain both alleles and be miscalled as a heterozygote. This process primarily creates false heterozygotes from true homozygotes, leading to a heterozygote *excess*. **Verdict: Incorrect**.\n\n### Conclusion\n\nAll three statements in Option A are correct descriptions of phenomena affecting HWE in GWAS. All statements in options B, C, and D contain at least one error. Therefore, Option A is the only correct choice.", "answer": "$$\\boxed{A}$$", "id": "4347885"}, {"introduction": "A major application of GWAS findings is the prediction of an individual's risk for a complex trait using a Polygenic Risk Score (PRS). A PRS aggregates the effects of numerous genetic variants, but its predictive accuracy depends critically on which variants are included. This hands-on simulation exercise guides you through the process of building a PRS and evaluating its performance, demonstrating the crucial trade-off between the stringency of SNP selection and the score's ultimate predictive power [@problem_id:2394707].", "problem": "You are given the task of constructing a polygenic risk score (PRS) within a simulated genome-wide association study (GWAS) setting and evaluating its predictive accuracy as a function of a single-nucleotide polymorphism (SNP) inclusion threshold. The goal is to define, compute, and report the squared predictive correlation for several parameter configurations.\n\nDefinitions and setting:\n\n- Genome-Wide Association Study (GWAS) examines associations between genetic variants and a trait. Consider $m$ single-nucleotide polymorphisms (SNPs), each with a minor allele frequency (MAF). For each SNP $j \\in \\{1,\\ldots,m\\}$, the genotype for an individual is encoded as the count of minor alleles in $\\{0,1,2\\}$.\n- For each SNP $j$, draw its MAF $p_j$ independently from the continuous uniform distribution on $[0.05, 0.5]$. Under Hardy–Weinberg equilibrium, simulate each genotype entry as an independent draw from $\\mathrm{Binomial}(2, p_j)$.\n- Split the sample into a training cohort of size $n_{\\text{train}}$ and a testing cohort of size $n_{\\text{test}}$. Let $\\mathbf{G}_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}} \\times m}$ and $\\mathbf{G}_{\\text{test}} \\in \\mathbb{R}^{n_{\\text{test}} \\times m}$ be the corresponding genotype matrices.\n- Standardize SNP columns using the training cohort statistics: for each SNP $j$, compute the training mean $\\mu_j$ and standard deviation $\\sigma_j$ of column $j$ in $\\mathbf{G}_{\\text{train}}$. Define standardized matrices $\\mathbf{X}_{\\text{train}}$ and $\\mathbf{X}_{\\text{test}}$ via\n  $$X_{\\text{train}}[i,j] = \\begin{cases}\n  \\dfrac{G_{\\text{train}}[i,j] - \\mu_j}{\\sigma_j}  \\text{if } \\sigma_j  0,\\\\\n  0  \\text{if } \\sigma_j = 0,\n  \\end{cases}\n  \\qquad\n  X_{\\text{test}}[i,j] = \\begin{cases}\n  \\dfrac{G_{\\text{test}}[i,j] - \\mu_j}{\\sigma_j}  \\text{if } \\sigma_j  0,\\\\\n  0  \\text{if } \\sigma_j = 0.\n  \\end{cases}$$\n  If $\\sigma_j = 0$, treat the standardized column as identically $0$ in both cohorts.\n- Choose $k$ causal SNP indices uniformly without replacement from $\\{1,\\ldots,m\\}$; define the true effect vector $\\boldsymbol{\\beta}^{\\star} \\in \\mathbb{R}^m$ such that $\\beta^{\\star}_j = 0$ for non-causal $j$ and raw causal effects are drawn independently as $\\mathcal{N}(0,1)$.\n- Let the narrow-sense heritability be $h^2 \\in [0,1]$. Scale the raw effect vector to achieve training genetic variance equal to $h^2$ as follows. Compute the genetic component in training $\\mathbf{g}_{\\text{train}} = \\mathbf{X}_{\\text{train}} \\boldsymbol{\\beta}_{\\text{raw}}$, its variance $v_g = \\mathrm{Var}(\\mathbf{g}_{\\text{train}})$, and set\n  $$\\boldsymbol{\\beta}^{\\star} = \\begin{cases}\n  \\boldsymbol{\\beta}_{\\text{raw}} \\sqrt{\\dfrac{h^2}{v_g}}  \\text{if } v_g  0,\\\\\n  \\mathbf{0}  \\text{if } v_g = 0.\n  \\end{cases}$$\n  Define the environmental noise variance as $\\sigma_e^2 = 1 - h^2$.\n- Generate phenotypes in both cohorts by\n  $$\\mathbf{y}_{\\text{train}} = \\mathbf{X}_{\\text{train}} \\boldsymbol{\\beta}^{\\star} + \\boldsymbol{\\varepsilon}_{\\text{train}}, \\quad \\boldsymbol{\\varepsilon}_{\\text{train}} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_e^2 \\mathbf{I}),$$\n  $$\\mathbf{y}_{\\text{test}} = \\mathbf{X}_{\\text{test}} \\boldsymbol{\\beta}^{\\star} + \\boldsymbol{\\varepsilon}_{\\text{test}}, \\quad \\boldsymbol{\\varepsilon}_{\\text{test}} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_e^2 \\mathbf{I}).$$\n- Perform univariate association in the training cohort for each SNP $j$ by fitting a linear model with intercept, and let $p_j$ be the two-sided $p$-value for the null hypothesis that the slope for SNP $j$ equals $0$. Equivalently, if $r_j$ denotes the Pearson correlation between column $j$ of $\\mathbf{X}_{\\text{train}}$ and $\\mathbf{y}_{\\text{train}}$, with degrees of freedom $\\nu = n_{\\text{train}} - 2$, define the $t$-statistic\n  $$t_j = \\frac{r_j}{\\sqrt{1 - r_j^2}} \\sqrt{\\nu},$$\n  and the two-sided $p$-value\n  $$p_j = 2 \\, \\mathbb{P}\\left(T_{\\nu} \\ge |t_j|\\right),$$\n  where $T_{\\nu}$ is a Student's $t$-distributed random variable with $\\nu$ degrees of freedom. If the variance of SNP $j$ in training is zero or the variance of $\\mathbf{y}_{\\text{train}}$ is zero, set $p_j = 1$.\n- For a given threshold $p_T \\in (0,1]$, define the included SNP set $S(p_T) = \\{j \\in \\{1,\\ldots,m\\} \\mid p_j \\le p_T\\}$. Let $\\hat{\\beta}_j$ be the univariate ordinary least squares slope for SNP $j$ estimated in the training cohort (with intercept). Define the test-set PRS at threshold $p_T$ by\n  $$\\mathrm{PRS}_{\\text{test}}(p_T) = \\sum_{j \\in S(p_T)} \\hat{\\beta}_j \\, X_{\\text{test}}[:,j].$$\n  If $S(p_T)$ is empty, define $\\mathrm{PRS}_{\\text{test}}(p_T)$ to be the zero vector.\n- Define predictive accuracy as the coefficient of determination on the test cohort,\n  $$R^2(p_T) = \\left(\\mathrm{Corr}\\left(\\mathrm{PRS}_{\\text{test}}(p_T), \\mathbf{y}_{\\text{test}}\\right)\\right)^2,$$\n  where $\\mathrm{Corr}$ denotes the Pearson correlation. If either $\\mathrm{PRS}_{\\text{test}}(p_T)$ or $\\mathbf{y}_{\\text{test}}$ has zero variance in the test cohort, set $R^2(p_T) = 0$.\n\nTask:\n\n- For each parameter set in the test suite below, perform the above simulation and analysis using the provided random seed. For each specified list of thresholds, compute $R^2(p_T)$ for each $p_T$ in the given order.\n- Report each $R^2(p_T)$ rounded to $6$ decimal places.\n\nTest suite:\n\n- Case A: seed $= 42$, $n_{\\text{train}} = 1500$, $n_{\\text{test}} = 1000$, $m = 3000$, $k = 150$, $h^2 = 0.5$, thresholds $= \\left[10^{-8}, 10^{-5}, 10^{-3}, 10^{-2}, 10^{-1}, 5 \\times 10^{-1}, 1\\right]$.\n- Case B: seed $= 7$, $n_{\\text{train}} = 300$, $n_{\\text{test}} = 300$, $m = 1500$, $k = 50$, $h^2 = 0.5$, thresholds $= \\left[10^{-20}, 10^{-5}, 10^{-3}, 10^{-2}, 10^{-1}, 1\\right]$.\n- Case C: seed $= 2023$, $n_{\\text{train}} = 800$, $n_{\\text{test}} = 800$, $m = 2000$, $k = 0$, $h^2 = 0.0$, thresholds $= \\left[10^{-5}, 10^{-3}, 10^{-2}, 10^{-1}, 1\\right]$.\n- Case D: seed $= 99$, $n_{\\text{train}} = 1200$, $n_{\\text{test}} = 800$, $m = 2500$, $k = 100$, $h^2 = 0.8$, thresholds $= \\left[10^{-8}, 10^{-6}, 10^{-4}, 10^{-2}, 1\\right]$.\n\nEdge-case conventions:\n\n- If any SNP column has zero variance in the training cohort, set its standardized values to zero in both cohorts, its estimated effect $\\hat{\\beta}_j$ to $0$, and its $p_j$ to $1$.\n- If the training phenotypic variance is zero, set all $\\hat{\\beta}_j = 0$ and all $p_j = 1$.\n- If the test PRS or test phenotype has zero variance, set $R^2(p_T) = 0$ for that threshold.\n\nFinal output specification:\n\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, with no spaces, in the following format:\n  - For each case, output the list $\\left[R^2(p_{T,1}), R^2(p_{T,2}), \\ldots \\right]$ for its thresholds in the given order, rounding each entry to $6$ decimal places.\n  - Aggregate the four cases in order A, B, C, D into a single list, yielding a nested list. For example: $\\left[[r_{A,1},r_{A,2}], [r_{B,1},r_{B,2}], [r_{C,1},r_{C,2}], [r_{D,1},r_{D,2}]\\right]$.", "solution": "The problem presented is a comprehensive and valid simulation study in statistical genetics, specifically concerning genome-wide association studies (GWAS) and polygenic risk scores (PRS). The task is to implement the specified simulation pipeline, which is scientifically grounded in the standard polygenic model of complex traits, and to evaluate the predictive accuracy of the resulting PRS. All parameters and procedures are well-defined, making the problem objective and algorithmically tractable. I shall proceed with a detailed explanation of the methodology, followed by its implementation.\n\nThe overall process can be decomposed into several logical stages: data simulation, association analysis, PRS construction, and evaluation.\n\nFirst, we simulate the genetic and phenotypic data for two distinct cohorts: a training cohort of size $n_{\\text{train}}$ and a testing cohort of size $n_{\\text{test}}$. We consider a genome of $m$ single-nucleotide polymorphisms (SNPs). For each SNP $j$, its minor allele frequency (MAF), denoted $p_j$, is drawn from a uniform distribution $\\mathrm{Uniform}(0.05, 0.5)$. Under the assumption of Hardy–Weinberg equilibrium, the genotype for each individual at SNP $j$, which is the count of minor alleles, is an independent random draw from a Binomial distribution, $\\mathrm{Binomial}(2, p_j)$. This process generates the raw genotype matrices $\\mathbf{G}_{\\text{train}}$ and $\\mathbf{G}_{\\text{test}}$.\n\nThe raw genotype counts are not typically used directly in analysis. Instead, they are standardized. For each SNP $j$, we compute its mean $\\mu_j$ and standard deviation $\\sigma_j$ using only the training data. These statistics are then used to standardize the genotype matrices for both cohorts:\n$$X_{\\text{train}}[i,j] = \\frac{G_{\\text{train}}[i,j] - \\mu_j}{\\sigma_j} \\quad \\text{and} \\quad X_{\\text{test}}[i,j] = \\frac{G_{\\text{test}}[i,j] - \\mu_j}{\\sigma_j}$$\nA crucial edge case is when a SNP has zero variance in the training cohort ($\\sigma_j = 0$). In this situation, the SNP is non-informative, and its standardized values are set to $0$ in both cohorts.\n\nNext, we establish the ground truth for our simulation by defining a genetic architecture for the trait. We randomly select $k$ out of the $m$ SNPs to be \"causal\". The true genetic effect vector, $\\boldsymbol{\\beta}^{\\star} \\in \\mathbb{R}^m$, has non-zero entries only for these causal SNPs. The initial raw effects for these $k$ SNPs are drawn from a standard normal distribution, $\\mathcal{N}(0,1)$. These raw effects, $\\boldsymbol{\\beta}_{\\text{raw}}$, are then scaled to satisfy a specified narrow-sense heritability, $h^2$. Heritability represents the proportion of phenotypic variance attributable to genetic variance. We calculate the genetic component in the training set as $\\mathbf{g}_{\\text{train}} = \\mathbf{X}_{\\text{train}} \\boldsymbol{\\beta}_{\\text{raw}}$ and its variance $v_g = \\mathrm{Var}(\\mathbf{g}_{\\text{train}})$. The final, scaled effect vector is:\n$$\\boldsymbol{\\beta}^{\\star} = \\boldsymbol{\\beta}_{\\text{raw}} \\sqrt{\\frac{h^2}{v_g}}$$\nThis scaling ensures that the variance of the true genetic component in the training data, $\\mathrm{Var}(\\mathbf{X}_{\\text{train}} \\boldsymbol{\\beta}^{\\star})$, is precisely $h^2$. If $v_g=0$, $\\boldsymbol{\\beta}^{\\star}$ is the zero vector. The total phenotypic variance is normalized to $1$, so the variance of the environmental noise is $\\sigma_e^2 = 1 - h^2$. The phenotypes are then generated for both cohorts using the linear model:\n$$\\mathbf{y}_{\\text{train}} = \\mathbf{X}_{\\text{train}} \\boldsymbol{\\beta}^{\\star} + \\boldsymbol{\\varepsilon}_{\\text{train}}, \\quad \\text{where} \\quad \\boldsymbol{\\varepsilon}_{\\text{train}} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_e^2 \\mathbf{I})$$\n$$\\mathbf{y}_{\\text{test}} = \\mathbf{X}_{\\text{test}} \\boldsymbol{\\beta}^{\\star} + \\boldsymbol{\\varepsilon}_{\\text{test}}, \\quad \\text{where} \\quad \\boldsymbol{\\varepsilon}_{\\text{test}} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_e^2 \\mathbf{I})$$\n\nWith the simulated training data $(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}})$ in hand, we perform a GWAS. For each SNP $j$, we test the null hypothesis of no association with the phenotype. This is done by fitting a univariate linear model. The strength of association is quantified by a $t$-statistic. Let $r_j$ be the Pearson correlation between the $j$-th SNP vector $\\mathbf{X}_{\\text{train}}[:,j]$ and the phenotype vector $\\mathbf{y}_{\\text{train}}$. The $t$-statistic is:\n$$t_j = \\frac{r_j}{\\sqrt{1 - r_j^2}} \\sqrt{\\nu}$$\nwhere $\\nu = n_{\\text{train}} - 2$ are the degrees of freedom. The two-sided $p$-value, $p_j$, is then computed from the Student's $t$-distribution: $p_j = 2 \\, \\mathbb{P}(T_{\\nu} \\ge |t_j|)$, where $T_{\\nu}$ is a random variable following the $t$-distribution with $\\nu$ degrees of freedom. In parallel, we estimate the univariate ordinary least squares (OLS) effect size, $\\hat{\\beta}_j$, for each SNP. For a standardized predictor, this slope can be efficiently calculated.\n\nThe next step is to construct the PRS for the test set. Based on a specified $p$-value threshold, $p_T$, we form a set of included SNPs, $S(p_T) = \\{j \\mid p_j \\le p_T\\}$. The PRS is the weighted sum of the standardized genotypes in the test cohort, where the weights are the estimated effect sizes $\\hat{\\beta}_j$ from the training GWAS:\n$$\\mathrm{PRS}_{\\text{test}}(p_T) = \\sum_{j \\in S(p_T)} \\hat{\\beta}_j \\, X_{\\text{test}}[:,j]$$\nIf no SNPs meet the threshold, $S(p_T)$ is empty and the PRS is a vector of zeros.\n\nFinally, we evaluate the predictive accuracy of the PRS. This is measured by the coefficient of determination, $R^2$, which is the squared Pearson correlation between the predicted PRS and the actual phenotypes in the test cohort:\n$$R^2(p_T) = \\left(\\mathrm{Corr}\\left(\\mathrm{PRS}_{\\text{test}}(p_T), \\mathbf{y}_{\\text{test}}\\right)\\right)^2$$\nIf either the PRS or the test phenotype has zero variance, the correlation is undefined, and $R^2(p_T)$ is set to $0$.\n\nThe algorithm will systematically execute this entire pipeline for each parameter set provided in the test suite. Vectorized computations using `numpy` will be employed for efficiency, particularly for the GWAS summary statistics and PRS calculation. The random seed for each case ensures the reproducibility of the stochastic simulation. The final output will be the calculated $R^2(p_T)$ values, rounded and formatted as specified.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t as student_t\n\ndef run_simulation(seed, n_train, n_test, m, k, h2, thresholds):\n    \"\"\"\n    Performs a single GWAS/PRS simulation run for a given set of parameters.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # 1. Simulate MAFs and Genotypes\n    mafs = rng.uniform(0.05, 0.5, size=m)\n    g_train = rng.binomial(2, mafs, size=(n_train, m))\n    g_test = rng.binomial(2, mafs, size=(n_test, m))\n    \n    # 2. Standardize Genotypes\n    mu_train = np.mean(g_train, axis=0)\n    sigma_train = np.std(g_train, axis=0)\n    \n    std_is_zero = (sigma_train == 0)\n    \n    x_train = np.zeros_like(g_train, dtype=float)\n    x_test = np.zeros_like(g_test, dtype=float)\n    \n    non_zero_std_indices = ~std_is_zero\n    \n    # Use np.divide with where clause to avoid runtime warnings\n    if np.any(non_zero_std_indices):\n        x_train[:, non_zero_std_indices] = np.divide(\n            g_train[:, non_zero_std_indices] - mu_train[non_zero_std_indices],\n            sigma_train[non_zero_std_indices]\n        )\n        x_test[:, non_zero_std_indices] = np.divide(\n            g_test[:, non_zero_std_indices] - mu_train[non_zero_std_indices],\n            sigma_train[non_zero_std_indices]\n        )\n\n    # 3. Simulate True Effects and Scale by Heritability\n    beta_raw = np.zeros(m)\n    if k > 0:\n        causal_indices = rng.choice(m, size=k, replace=False)\n        beta_raw[causal_indices] = rng.normal(0, 1, size=k)\n    \n    g_train_component = x_train @ beta_raw\n    v_g = np.var(g_train_component)\n    \n    beta_star = np.zeros(m)\n    if v_g > 0 and h2 > 0:\n        scaling_factor = np.sqrt(h2 / v_g)\n        beta_star = beta_raw * scaling_factor\n    \n    # 4. Simulate Phenotypes\n    sigma_e2 = 1.0 - h2\n    sigma_e = np.sqrt(sigma_e2) if sigma_e2 > 0 else 0.0\n    \n    y_train = x_train @ beta_star + rng.normal(0, sigma_e, size=n_train)\n    y_test = x_test @ beta_star + rng.normal(0, sigma_e, size=n_test)\n    \n    # 5. Perform GWAS in Training Cohort\n    hat_betas = np.zeros(m)\n    p_values = np.ones(m)\n    \n    y_train_var = np.var(y_train, ddof=1)\n    if y_train_var == 0:\n        # If y_train has no variance, all betas are 0 and p-values are 1.\n        # This is handled by hat_betas and p_values being pre-initialized.\n        pass\n    else:\n        y_train_c = y_train - np.mean(y_train)\n        \n        # OLS slope estimates (beta_hat)\n        sum_x_sq = np.sum(x_train**2, axis=0)\n        valid_snps_for_beta = (sum_x_sq > 0)\n        \n        if np.any(valid_snps_for_beta):\n            hat_betas[valid_snps_for_beta] = (x_train[:, valid_snps_for_beta].T @ y_train_c) / sum_x_sq[valid_snps_for_beta]\n        \n        # Pearson correlations (r)\n        # r = beta_hat * (std_x / std_y)\n        std_x_train_sample = np.std(x_train, axis=0, ddof=1)\n        std_y_train_sample = np.sqrt(y_train_var)\n        \n        correlations = np.zeros(m)\n        valid_snps_for_corr = (std_x_train_sample > 0)\n        if np.any(valid_snps_for_corr):\n            correlations[valid_snps_for_corr] = hat_betas[valid_snps_for_corr] * (std_x_train_sample[valid_snps_for_corr] / std_y_train_sample)\n\n        # Handle perfect correlation case to avoid division by zero in t-statistic\n        correlations = np.clip(correlations, -1.0 + 1e-15, 1.0 - 1e-15)\n\n        # t-statistics and p-values\n        df = n_train - 2\n        if df > 0:\n            t_stats = (correlations / np.sqrt(1 - correlations**2)) * np.sqrt(df)\n            p_values = 2 * student_t.sf(np.abs(t_stats), df)\n            \n            # SNPs with zero variance in training must have p-value of 1\n            p_values[std_is_zero] = 1.0\n\n    # 6. Calculate PRS and R^2 for each threshold\n    r2_results = []\n    y_test_var = np.var(y_test)\n    \n    for p_t in thresholds:\n        included_snps_mask = (p_values = p_t)\n        \n        if not np.any(included_snps_mask) or y_test_var == 0:\n            r2_results.append(0.0)\n            continue\n        \n        hat_betas_selected = hat_betas[included_snps_mask]\n        x_test_selected = x_test[:, included_snps_mask]\n        \n        prs_test = x_test_selected @ hat_betas_selected\n        \n        prs_var = np.var(prs_test)\n        \n        if prs_var == 0:\n            r2_results.append(0.0)\n            continue\n            \n        corr_matrix = np.corrcoef(prs_test, y_test)\n        corr = corr_matrix[0, 1]\n        \n        r2 = corr**2\n        r2_results.append(np.round(r2, 6))\n        \n    return r2_results\n\n\ndef solve():\n    test_cases = [\n        {'seed': 42, 'n_train': 1500, 'n_test': 1000, 'm': 3000, 'k': 150, 'h2': 0.5, 'thresholds': [1e-8, 1e-5, 1e-3, 1e-2, 1e-1, 5e-1, 1]},\n        {'seed': 7, 'n_train': 300, 'n_test': 300, 'm': 1500, 'k': 50, 'h2': 0.5, 'thresholds': [1e-20, 1e-5, 1e-3, 1e-2, 1e-1, 1]},\n        {'seed': 2023, 'n_train': 800, 'n_test': 800, 'm': 2000, 'k': 0, 'h2': 0.0, 'thresholds': [1e-5, 1e-3, 1e-2, 1e-1, 1]},\n        {'seed': 99, 'n_train': 1200, 'n_test': 800, 'm': 2500, 'k': 100, 'h2': 0.8, 'thresholds': [1e-8, 1e-6, 1e-4, 1e-2, 1]}\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = run_simulation(\n            seed=case['seed'],\n            n_train=case['n_train'],\n            n_test=case['n_test'],\n            m=case['m'],\n            k=case['k'],\n            h2=case['h2'],\n            thresholds=case['thresholds']\n        )\n        all_results.append(results)\n    \n    # Format the final output string exactly as required\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "2394707"}]}