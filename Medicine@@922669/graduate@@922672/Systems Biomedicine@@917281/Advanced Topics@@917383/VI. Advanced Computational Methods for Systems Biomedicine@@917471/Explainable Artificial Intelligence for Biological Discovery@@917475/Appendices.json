{"hands_on_practices": [{"introduction": "In the field of explainable AI, numerous methods exist for generating feature attributions, but they don't all perform equally well. A crucial first step is to assess the *faithfulness* of an explanation method: does it accurately reflect the model's internal reasoning? This exercise introduces a powerful technique for this evaluation, the Area Over the Perturbation Curve (AOPC), which involves systematically removing the features deemed most important by an XAI method and measuring the impact on the model's output. By completing this practice [@problem_id:4340491], you will learn to quantitatively compare the faithfulness of different attribution methods, a fundamental skill for selecting a reliable XAI tool for biological discovery.", "problem": "You are given a toy genomics classifier in a systems biomedicine setting that maps a vector of gene expression levels to a scalar score interpreted as a decision score. The classifier is linear and defined as follows: for a feature vector $\\mathbf{x} \\in \\mathbb{R}^d$, a weight vector $\\mathbf{w} \\in \\mathbb{R}^d$, a bias $b \\in \\mathbb{R}$, and a baseline vector $\\mathbf{z} \\in \\mathbb{R}^d$ representing the \"no-expression\" background, the model output is $f(\\mathbf{x}) = b + \\sum_{i=1}^{d} w_i x_i$. Consider two attribution methods: gradient saliency and Shapley Additive exPlanations (SHAP). The goal is to compute the Area Over the Perturbation Curve (AOPC) for each method to compare the faithfulness of their attributions.\n\nBase principles and definitions to be used:\n- From multivariable calculus, the gradient of a scalar function $f(\\mathbf{x})$ with respect to $\\mathbf{x}$ is $\\nabla f(\\mathbf{x})$, whose $i$-th component is $\\frac{\\partial f}{\\partial x_i}$.\n- From cooperative game theory, the Shapley value for feature $i$ is the average marginal contribution of feature $i$ over all coalitions, computed by enumerating or integrating over subsets $S \\subseteq \\{1,\\dots,d\\} \\setminus \\{i\\}$ and comparing $f$ when feature $i$ is present versus absent under a well-defined missingness model that replaces missing features by the baseline $\\mathbf{z}$.\n- The perturbation curve is defined by the sequence of outputs $f(\\mathbf{x}^{(k)})$ where $\\mathbf{x}^{(k)}$ is formed by ablating (replacing with $\\mathbf{z}$) the top $k$ features according to the chosen attribution ranking. The Area Over the Perturbation Curve (AOPC) is the average drop in the model output across $k = 1,\\dots,K$, where $K$ is the number of ablation steps you choose (here, set $K=d$).\n\nYour task:\n- Using only the given base principles and definitions, derive the attribution scores for gradient saliency and for Shapley Additive exPlanations (SHAP) under the specified linear model and baseline replacement, and then implement the AOPC computation procedure as follows:\n    1. Compute the attribution scores $a_i$ for each feature $i$ according to each method.\n    2. Rank features by descending absolute attribution value $\\lvert a_i \\rvert$. In case of ties, break ties by ascending feature index $i$.\n    3. For $k \\in \\{1,\\dots,K\\}$ with $K=d$, define $\\mathbf{x}^{(k)}$ by replacing the top $k$ features in the ranking with their baseline values $\\mathbf{z}$ (feature-wise ablation to $\\mathbf{z}$). Let $\\Delta_k = f(\\mathbf{x}) - f(\\mathbf{x}^{(k)})$.\n    4. Define $\\mathrm{AOPC} = \\frac{1}{K} \\sum_{k=1}^{K} \\Delta_k$.\n- All computations use the linear score $f(\\mathbf{x})$ and the baseline $\\mathbf{z}$ as specified. No probabilistic or stochastic assumptions are required. There are no physical units involved in this computation, and the outputs are unitless.\n\nTest suite and parameters:\n- Use $d = 6$ features in all cases. The baseline is the zero vector $\\mathbf{z} = (0,0,0,0,0,0)$ for all cases. Implement exactly the following four test cases, each defined by $(\\mathbf{w}, b, \\mathbf{x})$:\n    1. Case A (general, mixed signs): $\\mathbf{w} = (1.5, -0.9, 0.4, 0.0, 1.0, -0.2)$, $b = 0.3$, $\\mathbf{x} = (2.0, 1.0, 0.0, 5.0, 0.1, 3.0)$.\n    2. Case B (boundary, all-zero input): $\\mathbf{w} = (1.5, -0.9, 0.4, 0.0, 1.0, -0.2)$, $b = 0.3$, $\\mathbf{x} = (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)$.\n    3. Case C (edge, attribution ties): $\\mathbf{w} = (0.5, -0.5, 0.5, -0.5, 0.0, 0.5)$, $b = 0.0$, $\\mathbf{x} = (1.0, 2.0, 3.0, 0.0, 10.0, 0.0)$.\n    4. Case D (edge, large weight on a zero-valued feature): $\\mathbf{w} = (2.0, 0.1, 0.1, 0.1, 0.1, 0.1)$, $b = -0.5$, $\\mathbf{x} = (0.0, 10.0, 10.0, 10.0, 10.0, 10.0)$.\n- For gradient saliency, compute the saliency from first principles for the given $f(\\mathbf{x})$ and rank by the magnitude of these saliencies. For SHAP, compute the Shapley values under the missingness model that replaces missing features with $\\mathbf{z}$, using the cooperative game theory definition specialized to this linear model, then rank by the magnitude of these values.\n- Set $K = d$ in all cases.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one of the four cases and is itself a two-element list of the AOPC values for gradient saliency and SHAP, in that order. For example, the final output format must be of the form $[ [\\mathrm{AOPC}_{\\mathrm{grad},A}, \\mathrm{AOPC}_{\\mathrm{shap},A}], [\\mathrm{AOPC}_{\\mathrm{grad},B}, \\mathrm{AOPC}_{\\mathrm{shap},B}], [\\mathrm{AOPC}_{\\mathrm{grad},C}, \\mathrm{AOPC}_{\\mathrm{shap},C}], [\\mathrm{AOPC}_{\\mathrm{grad},D}, \\mathrm{AOPC}_{\\mathrm{shap},D}] ]$ with the actual numeric values.", "solution": "The problem requires the computation and comparison of the Area Over the Perturbation Curve (AOPC) for two attribution methods, gradient saliency and Shapley Additive exPlanations (SHAP), applied to a linear classifier in a systems biomedicine context. The validation of the problem statement confirms that it is scientifically grounded, well-posed, and objective, with all necessary definitions and data provided. We can therefore proceed with a principled derivation and solution.\n\nThe classifier is a linear model defined by the function $f(\\mathbf{x}): \\mathbb{R}^d \\to \\mathbb{R}$, given by:\n$$f(\\mathbf{x}) = b + \\sum_{i=1}^{d} w_i x_i = b + \\mathbf{w}^T \\mathbf{x}$$\nwhere $\\mathbf{x}$ is the feature vector of gene expression levels, $\\mathbf{w}$ is the weight vector, and $b$ is a bias term. The baseline vector, representing a \"no-expression\" state, is given as $\\mathbf{z}$. For all test cases, $\\mathbf{z}$ is the zero vector, $\\mathbf{z} = \\mathbf{0}$.\n\nThe AOPC is defined as $\\frac{1}{K} \\sum_{k=1}^{K} \\Delta_k$, where $K=d$ and $\\Delta_k = f(\\mathbf{x}) - f(\\mathbf{x}^{(k)})$. The perturbed vector $\\mathbf{x}^{(k)}$ is constructed by ablating (setting to the baseline value $z_i$) the top $k$ features, as ranked by the absolute value of their attribution scores.\n\nLet's derive the attribution scores for each method and then outline the AOPC computation procedure.\n\n**1. Gradient Saliency Attribution**\n\nThe gradient saliency attribution for feature $i$, denoted $a_i^{\\text{grad}}$, is defined as the partial derivative of the model output with respect to the feature input $x_i$:\n$$a_i^{\\text{grad}} = \\frac{\\partial f(\\mathbf{x})}{\\partial x_i}$$\nFor the given linear model $f(\\mathbf{x}) = b + w_1 x_1 + w_2 x_2 + \\dots + w_d x_d$, the partial derivative with respect to $x_i$ is:\n$$\\frac{\\partial}{\\partial x_i} \\left( b + \\sum_{j=1}^{d} w_j x_j \\right) = w_i$$\nThus, the gradient attribution for feature $i$ is simply its corresponding weight, $a_i^{\\text{grad}} = w_i$. Note that for a linear model, this attribution is independent of the input vector $\\mathbf{x}$. The attribution vector is $\\mathbf{a}^{\\text{grad}} = \\mathbf{w}$.\n\n**2. SHAP (Shapley Additive exPlanations) Attribution**\n\nThe SHAP attribution for feature $i$, $a_i^{\\text{shap}}$, is its Shapley value from cooperative game theory. It is the average marginal contribution of feature $i$ across all possible coalitions (subsets) of other features. The \"value\" of a coalition of features $S \\subseteq \\{1, \\dots, d\\}$ is the model output when only those features have their values from $\\mathbf{x}$, and the rest have their values from the baseline $\\mathbf{z}$. Let us denote this value function as $v(S)$.\n$$v(S) = f(\\mathbf{x}_S, \\mathbf{z}_{\\bar{S}}) = b + \\sum_{j \\in S} w_j x_j + \\sum_{j \\notin S} w_j z_j$$\nwhere $\\mathbf{x}_S$ indicates that for indices in $S$, values from $\\mathbf{x}$ are used, and $\\mathbf{z}_{\\bar{S}}$ indicates that for indices not in $S$ (the complement set $\\bar{S}$), values from $\\mathbf{z}$ are used.\n\nThe marginal contribution of feature $i$ to a coalition $S$ (where $i \\notin S$) is $v(S \\cup \\{i\\}) - v(S)$. Let us compute this quantity:\n$$v(S \\cup \\{i\\}) = b + w_i x_i + \\sum_{j \\in S} w_j x_j + \\sum_{j \\notin S \\cup \\{i\\}} w_j z_j$$\n$$v(S) = b + w_i z_i + \\sum_{j \\in S} w_j x_j + \\sum_{j \\notin S \\cup \\{i\\}} w_j z_j$$\nThe difference is:\n$$v(S \\cup \\{i\\}) - v(S) = (w_i x_i) - (w_i z_i) = w_i(x_i - z_i)$$\nNotably, for a linear model, this marginal contribution is independent of the coalition $S$. The Shapley value is the weighted average of these marginal contributions over all coalitions. Since the contribution is constant, its average is simply the constant value itself.\nTherefore, the SHAP attribution for feature $i$ is:\n$$a_i^{\\text{shap}} = w_i(x_i - z_i)$$\nSince the problem specifies the baseline is the zero vector, $\\mathbf{z} = \\mathbf{0}$, this simplifies to:\n$$a_i^{\\text{shap}} = w_i x_i$$\nThe attribution for each feature is its exact contribution to the model's output score relative to the bias. The attribution vector is $\\mathbf{a}^{\\text{shap}} = \\mathbf{w} \\odot \\mathbf{x}$, where $\\odot$ denotes the element-wise product.\n\n**3. AOPC Computation Procedure**\n\nThe procedure is identical for both attribution methods, differing only in the initial scores used for ranking.\n1.  **Compute Attributions**: For a given test case $(\\mathbf{w}, b, \\mathbf{x})$, calculate the attribution vector $\\mathbf{a}$:\n    -   For Gradient Saliency: $\\mathbf{a} = \\mathbf{w}$.\n    -   For SHAP: $\\mathbf{a} = \\mathbf{w} \\odot \\mathbf{x}$.\n2.  **Rank Features**: Create a list of tuples $(|a_i|, i)$ for $i \\in \\{0, \\dots, d-1\\}$. Sort this list in descending order of $|a_i|$, breaking ties by sorting in ascending order of the feature index $i$. Let the resulting sequence of ranked indices be $I = [i_1, i_2, \\dots, i_d]$.\n3.  **Calculate Perturbation Drops ($\\Delta_k$)**: We need to compute $\\Delta_k = f(\\mathbf{x}) - f(\\mathbf{x}^{(k)})$. Let's simplify this expression. Let $I_k = \\{i_1, \\dots, i_k\\}$ be the set of the top $k$ ablated feature indices.\n    $$f(\\mathbf{x}) = b + \\sum_{j=1}^{d} w_j x_j$$\n    $$f(\\mathbf{x}^{(k)}) = b + \\sum_{j \\notin I_k} w_j x_j + \\sum_{j \\in I_k} w_j z_j$$\n    Since $\\mathbf{z} = \\mathbf{0}$, $f(\\mathbf{x}^{(k)}) = b + \\sum_{j \\notin I_k} w_j x_j$. The drop is then:\n    $$\\Delta_k = \\left(b + \\sum_{j=1}^{d} w_j x_j\\right) - \\left(b + \\sum_{j \\notin I_k} w_j x_j\\right) = \\sum_{j \\in I_k} w_j x_j$$\n    This is a significant simplification: $\\Delta_k$ is the sum of the individual contributions $w_j x_j$ for the features that have been ablated.\n4.  **Compute AOPC**: Calculate the average of the drops:\n    $$\\mathrm{AOPC} = \\frac{1}{d} \\sum_{k=1}^{d} \\Delta_k$$\nThis complete, principled procedure will now be implemented and applied to the four specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple of (w, b, x)\n    test_cases = [\n        # Case A (general, mixed signs)\n        (np.array([1.5, -0.9, 0.4, 0.0, 1.0, -0.2]), 0.3, np.array([2.0, 1.0, 0.0, 5.0, 0.1, 3.0])),\n        # Case B (boundary, all-zero input)\n        (np.array([1.5, -0.9, 0.4, 0.0, 1.0, -0.2]), 0.3, np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n        # Case C (edge, attribution ties)\n        (np.array([0.5, -0.5, 0.5, -0.5, 0.0, 0.5]), 0.0, np.array([1.0, 2.0, 3.0, 0.0, 10.0, 0.0])),\n        # Case D (edge, large weight on a zero-valued feature)\n        (np.array([2.0, 0.1, 0.1, 0.1, 0.1, 0.1]), -0.5, np.array([0.0, 10.0, 10.0, 10.0, 10.0, 10.0])),\n    ]\n\n    results = []\n    for w, b, x in test_cases:\n        d = len(w)\n        case_results = []\n        \n        # Calculate attributions for both methods\n        grad_attributions = w\n        shap_attributions = w * x\n        \n        attributions_list = [grad_attributions, shap_attributions]\n        \n        for attributions in attributions_list:\n            # Step 1: Rank features based on attributions\n            # Create a list of tuples: (absolute attribution, index)\n            # The sorting key uses -abs(attribution) for descending order of value,\n            # and index for ascending order to break ties.\n            indexed_attributions = [(abs(val), i) for i, val in enumerate(attributions)]\n            indexed_attributions.sort(key=lambda item: (-item[0], item[1]))\n            ranked_indices = [item[1] for item in indexed_attributions]\n\n            # Step 2: Calculate perturbation drops and AOPC\n            # Pre-compute the true contributions of each feature\n            contributions = w * x\n            \n            delta_k_sum = 0.0\n            current_delta_k = 0.0\n            \n            # The problem sets K=d, so we iterate d times\n            for k in range(d):\n                # The index of the feature to ablate at this step\n                idx_to_ablate = ranked_indices[k]\n                \n                # Delta_k is the sum of contributions of the top k ablated features\n                current_delta_k += contributions[idx_to_ablate]\n                \n                # Add the current Delta_k to the total sum for AOPC\n                delta_k_sum += current_delta_k\n            \n            # AOPC is the average of the Delta_k values\n            if d == 0:\n                aopc = 0.0\n            else:\n                aopc = delta_k_sum / d\n                \n            case_results.append(aopc)\n            \n        results.append(case_results)\n\n    # Final print statement in the exact required format without extra spaces\n    # [[AOPC_grad_A,AOPC_shap_A],[AOPC_grad_B,AOPC_shap_B],...]\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4340491"}, {"introduction": "Once we have a trustworthy XAI method, we can apply it to a cohort of samples to generate feature attributions. However, the attribution for a single feature may vary from sample to sample. To move from individual explanations to general biological principles, we must ask: which features have a consistently high importance across the population? This practice [@problem_id:4340397] walks you through the essential statistical procedure for answering this question by using a one-sample $t$-test to identify features with a non-zero mean attribution, while carefully controlling for the risk of false discoveries using the Bonferroni correction. This bridges the gap between raw XAI outputs and statistically robust biological insights.", "problem": "Consider a setting in systems biomedicine where Explainable Artificial Intelligence (XAI) is used to generate per-sample feature attributions from a predictive model mapping multi-omics features to a phenotype. Let there be an attribution matrix with $n$ samples and $d$ features, denoted by $X \\in \\mathbb{R}^{n \\times d}$, where $X_{i,j}$ is the attribution of feature $j$ for sample $i$. We wish to determine which features have nonzero mean attribution across samples, subject to control of the family-wise error rate using the Bonferroni procedure.\n\nYour task is to write a complete program that, for each provided test case, performs the following steps grounded in fundamental statistical principles:\n- For each feature $j \\in \\{0,\\dots,d-1\\}$, test the null hypothesis $H_0: \\mu_j = 0$ against the two-sided alternative $H_1: \\mu_j \\neq 0$, where $\\mu_j$ is the population mean attribution for feature $j$.\n- Use the classical one-sample Student’s $t$-test: compute the sample mean $\\hat{\\mu}_j = \\frac{1}{n}\\sum_{i=1}^{n} X_{i,j}$ and the unbiased sample standard deviation $s_j = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} \\left(X_{i,j}-\\hat{\\mu}_j\\right)^2}$. The $t$-statistic is $t_j = \\frac{\\hat{\\mu}_j}{s_j / \\sqrt{n}}$ with degrees of freedom $\\nu = n-1$, provided $s_j \\neq 0$.\n- When $s_j = 0$, use the limiting decision implied by the $t$-statistic: if $\\hat{\\mu}_j = 0$ then set the two-sided $p$-value $p_j = 1$, otherwise set $p_j = 0$.\n- Otherwise, when $s_j \\neq 0$, compute the two-sided $p$-value $p_j = 2\\left(1 - F_{\\nu}\\left(|t_j|\\right)\\right)$, where $F_{\\nu}$ is the cumulative distribution function of the Student’s $t$ distribution with $\\nu = n-1$ degrees of freedom.\n- Apply Bonferroni correction across the $d$ hypotheses: define the adjusted $p$-value $q_j = \\min\\{1, d \\cdot p_j\\}$. A feature $j$ is called significant if $q_j < \\alpha$, where $\\alpha$ is the target family-wise error rate.\n- Return the zero-based indices of all significant features in ascending order.\n\nFundamental base to use:\n- The Central Limit Theorem justifies the use of the Student’s $t$-test for the mean when the sample size $n$ is moderate and the population variance is unknown.\n- The one-sample $t$-test relies on the sampling distribution of $\\frac{\\bar{X}-\\mu}{S/\\sqrt{n}}$ being Student’s $t$ with $\\nu = n-1$ degrees of freedom under $H_0$, where $\\bar{X}$ and $S$ are the sample mean and sample standard deviation.\n- The Bonferroni procedure controls the family-wise error rate by testing each hypothesis at level $\\alpha/d$, or equivalently by using adjusted $p$-values $q_j = \\min\\{1, d \\cdot p_j\\}$ and comparing to $\\alpha$.\n\nInput specification is implicit: your program must internally evaluate the following test suite. For each case, you are given an attribution matrix $X$ and a significance level $\\alpha$:\n\n- Test case 1:\n  - $X \\in \\mathbb{R}^{8 \\times 5}$ with columns (features) defined as:\n    - Feature 0: $[0.65, 0.52, 0.48, 0.51, 0.62, 0.55, 0.46, 0.57]$\n    - Feature 1: $[-0.12, 0.07, -0.04, 0.01, 0.02, -0.03, 0.05, 0.04]$\n    - Feature 2: $[-0.41, -0.38, -0.43, -0.47, -0.44, -0.39, -0.42, -0.45]$\n    - Feature 3: $[0.10, 0.02, -0.10, 0.05, -0.02, 0.01, -0.01, -0.03]$\n    - Feature 4: $[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]$\n  - $\\alpha = 0.05$.\n- Test case 2:\n  - $X \\in \\mathbb{R}^{5 \\times 3}$ with columns:\n    - Feature 0: $[0.10, 0.05, -0.02, 0.03, -0.04]$\n    - Feature 1: $[0.30, -0.30, 0.30, -0.30, 0.30]$\n    - Feature 2: $[0.00, 0.00, 0.00, 0.00, 0.00]$\n  - $\\alpha = 0.01$.\n- Test case 3:\n  - $X \\in \\mathbb{R}^{6 \\times 2}$ with columns:\n    - Feature 0: $[0.20, 0.20, 0.20, 0.20, 0.20, 0.20]$\n    - Feature 1: $[0.00, 0.00, 0.00, 0.00, 0.00, 0.00]$\n  - $\\alpha = 0.05$.\n- Test case 4:\n  - $X \\in \\mathbb{R}^{10 \\times 4}$ with columns:\n    - Feature 0: $[0.15, 0.18, 0.10, 0.12, 0.20, 0.14, 0.16, 0.13, 0.19, 0.11]$\n    - Feature 1: $[0.05, 0.02, 0.01, 0.00, 0.04, -0.01, 0.03, 0.00, 0.02, 0.01]$\n    - Feature 2: $[-0.12, -0.15, -0.11, -0.13, -0.14, -0.16, -0.10, -0.14, -0.13, -0.12]$\n    - Feature 3: $[-0.01, 0.00, 0.01, -0.02, 0.02, 0.00, 0.01, -0.01, 0.00, 0.00]$\n  - $\\alpha = 0.05$.\n\nFor each test case, your program must output the indices of significant features as a list of integers in ascending order. Aggregate the results for all test cases into a single line of output containing the results as a comma-separated list enclosed in square brackets. For example, if the four per-case results were lists $L_1, L_2, L_3, L_4$, the output must be of the form $[L_1,L_2,L_3,L_4]$ on a single line.\n\nNo physical units or angle units are involved. All results are purely numerical. The final answers per test case are lists of integers. The final printed line must be exactly one line and match the format described.", "solution": "The problem presents a well-defined and statistically grounded task. It is self-contained, consistent, and scientifically sound. The objective is to identify features with a statistically significant non-zero mean attribution from a given attribution matrix $X \\in \\mathbb{R}^{n \\times d}$, while controlling the family-wise error rate (FWER). The specified methodology, employing a one-sample Student's $t$-test for each feature followed by a Bonferroni correction for multiple hypothesis testing, is a classical and appropriate approach for this problem. The problem is therefore deemed valid and a formal solution is provided below.\n\nThe core of the problem lies in performing $d$ parallel hypothesis tests, one for each feature. For each feature $j \\in \\{0, 1, \\dots, d-1\\}$, we test the null hypothesis $H_0: \\mu_j = 0$ against the two-sided alternative $H_1: \\mu_j \\neq 0$, where $\\mu_j$ represents the true, unknown population mean of the attributions for feature $j$. The significance level for the entire family of $d$ tests is denoted by $\\alpha$.\n\nThe procedure for each feature $j$ is as follows:\n\n1.  **Compute Sample Statistics**: Given the sample of $n$ attribution values for feature $j$, $\\{X_{1,j}, X_{2,j}, \\dots, X_{n,j}\\}$, we first calculate the sample mean $\\hat{\\mu}_j$ and the unbiased sample standard deviation $s_j$.\n    $$ \\hat{\\mu}_j = \\frac{1}{n} \\sum_{i=1}^{n} X_{i,j} $$\n    $$ s_j = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (X_{i,j} - \\hat{\\mu}_j)^2} $$\n    The quantity $s_j / \\sqrt{n}$ is the standard error of the mean.\n\n2.  **Calculate the Test Statistic and p-value**: The choice of test depends on the sample standard deviation $s_j$.\n    *   **Case 1: $s_j > 0$**. When there is variation in the sample, the Student's $t$-test is appropriate. The $t$-statistic measures how many standard errors the sample mean is away from the hypothesized mean of $0$. It is calculated as:\n        $$ t_j = \\frac{\\hat{\\mu}_j - 0}{s_j / \\sqrt{n}} = \\frac{\\hat{\\mu}_j \\sqrt{n}}{s_j} $$\n        Under the null hypothesis $H_0$, this statistic follows a Student's $t$-distribution with $\\nu = n-1$ degrees of freedom. The two-sided $p$-value, $p_j$, is the probability of observing a test statistic at least as extreme as $|t_j|$ if the null hypothesis were true.\n        $$ p_j = 2 \\cdot P(T_{\\nu} \\ge |t_j|) = 2 \\left(1 - F_{\\nu}(|t_j|)\\right) $$\n        where $F_{\\nu}$ is the cumulative distribution function (CDF) of the Student's $t$-distribution with $\\nu$ degrees of freedom.\n\n    *   **Case 2: $s_j = 0$**. This occurs if and only if all sample attributions for feature $j$ are identical, i.e., $X_{1,j} = X_{2,j} = \\dots = X_{n,j} = c$. In this scenario, $\\hat{\\mu}_j = c$. The standard formula for the $t$-statistic is undefined due to division by zero. We must use the specified limiting logic:\n        *   If $\\hat{\\mu}_j \\neq 0$, the sample provides perfect evidence against the null hypothesis that $\\mu_j=0$. The probability of observing this result if $H_0$ were true is zero. Thus, we set $p_j = 0$.\n        *   If $\\hat{\\mu}_j = 0$, all sample values are exactly $0$. The data are perfectly consistent with the null hypothesis. The test is indeterminate, and by convention, we adopt a conservative stance and fail to reject the null hypothesis. This is achieved by setting the $p$-value to its maximum possible value, $p_j = 1$.\n\n3.  **Correct for Multiple Comparisons**: Since we are performing $d$ independent or dependent tests, there is an inflated probability of making at least one Type I error (a false positive). To control the FWER—the probability of making one or more Type I errors—at level $\\alpha$, we apply the Bonferroni correction. This is achieved by adjusting each individual $p$-value. The adjusted $p$-value, $q_j$, for feature $j$ is:\n    $$ q_j = \\min\\{1, d \\cdot p_j\\} $$\n\n4.  **Decision Rule**: A feature $j$ is declared to have a statistically significant non-zero mean attribution if its adjusted $p$-value is less than the pre-specified FWER level $\\alpha$.\n    $$ \\text{Feature } j \\text{ is significant if } q_j < \\alpha $$\n\nThis complete procedure is executed for all $d$ features in each test case. The final output for each test case is the set of zero-based indices corresponding to the features identified as significant, sorted in ascending order.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Performs one-sample t-tests with Bonferroni correction on multiple features\n    to identify those with a non-zero mean.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"X\": np.array([\n                [0.65, -0.12, -0.41, 0.10, 0.00],\n                [0.52, 0.07, -0.38, 0.02, 0.00],\n                [0.48, -0.04, -0.43, -0.10, 0.00],\n                [0.51, 0.01, -0.47, 0.05, 0.00],\n                [0.62, 0.02, -0.44, -0.02, 0.00],\n                [0.55, -0.03, -0.39, 0.01, 0.00],\n                [0.46, 0.05, -0.42, -0.01, 0.00],\n                [0.57, 0.04, -0.45, -0.03, 0.00]\n            ]),\n            \"alpha\": 0.05\n        },\n        {\n            \"X\": np.array([\n                [0.10, 0.30, 0.00],\n                [0.05, -0.30, 0.00],\n                [-0.02, 0.30, 0.00],\n                [0.03, -0.30, 0.00],\n                [-0.04, 0.30, 0.00]\n            ]),\n            \"alpha\": 0.01\n        },\n        {\n            \"X\": np.array([\n                [0.20, 0.00],\n                [0.20, 0.00],\n                [0.20, 0.00],\n                [0.20, 0.00],\n                [0.20, 0.00],\n                [0.20, 0.00]\n            ]),\n            \"alpha\": 0.05\n        },\n        {\n            \"X\": np.array([\n                [0.15, 0.05, -0.12, -0.01],\n                [0.18, 0.02, -0.15, 0.00],\n                [0.10, 0.01, -0.11, 0.01],\n                [0.12, 0.00, -0.13, -0.02],\n                [0.20, 0.04, -0.14, 0.02],\n                [0.14, -0.01, -0.16, 0.00],\n                [0.16, 0.03, -0.10, 0.01],\n                [0.13, 0.00, -0.14, -0.01],\n                [0.19, 0.02, -0.13, 0.00],\n                [0.11, 0.01, -0.12, 0.00]\n            ]),\n            \"alpha\": 0.05\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        X = case[\"X\"]\n        alpha = case[\"alpha\"]\n        \n        n, d = X.shape\n        significant_indices = []\n\n        for j in range(d):\n            feature_data = X[:, j]\n            \n            mu_hat = np.mean(feature_data)\n            # Use ddof=1 for unbiased sample standard deviation\n            s_j = np.std(feature_data, ddof=1)\n            \n            p_value = 0.0\n\n            if s_j == 0:\n                if mu_hat == 0:\n                    p_value = 1.0\n                else: # mu_hat != 0\n                    p_value = 0.0\n            else:\n                nu = n - 1\n                t_statistic = mu_hat / (s_j / np.sqrt(n))\n                # Two-sided p-value\n                # Using survival function (1-cdf) is more numerically stable for large t\n                p_value = 2 * t.sf(np.abs(t_statistic), df=nu)\n\n            # Bonferroni correction\n            q_value = min(1.0, d * p_value)\n\n            if q_value  alpha:\n                significant_indices.append(j)\n        \n        all_results.append(significant_indices)\n\n    # Format the output exactly as specified.\n    # The str() of a list, e.g., [0, 2], will be used directly.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "4340397"}, {"introduction": "The ultimate goal of XAI in systems biomedicine is often to inform real-world interventions, such as designing a new therapy. This leads to the concept of *counterfactual explanations*, which seek to find the minimal change to an input (e.g., gene expression profile) that would alter a predicted outcome (e.g., from \"diseased\" to \"healthy\"). In this exercise [@problem_id:4340444], you will formalize this concept as a constrained optimization problem. By deriving the minimal-norm perturbation that flips a model's prediction while respecting biological feasibility constraints, you will gain hands-on experience in translating computational insights into concrete, actionable hypotheses for experimental validation.", "problem": "A systems biomedicine laboratory is using Explainable Artificial Intelligence (XAI) to design counterfactual interventions on a gene expression profile to flip a predicted phenotype while preserving biological feasibility. Consider a linear phenotype predictor trained on log-transformed gene expression features, represented by a weight vector $w \\in \\mathbb{R}^{3}$ and an intercept $b \\in \\mathbb{R}$. The current sample has feature vector $x \\in \\mathbb{R}^{3}$. The classifier’s score is $s(x) = w^{\\top} x + b$, and the predicted phenotype label is $\\operatorname{sign}(s(x))$. A counterfactual intervention modifies the state to $x' = x + \\delta$, where $\\delta \\in \\mathbb{R}^{3}$ is the perturbation. The intervention must flip the phenotype prediction, that is, it must satisfy $w^{\\top} (x + \\delta) + b \\leq 0$, and must respect biological feasibility constraints encoded as linear inequalities.\n\nSuppose:\n- The current state is $x = (1.2,\\, 0.3,\\, -0.1)$.\n- The predictor parameters are $w = (2,\\, -1,\\, 1)$ and $b = -0.5$, so the current score is $s(x) = w^{\\top} x + b > 0$.\n- Actionability is restricted to two features: only the second and third coordinates may be changed. This is encoded by the constraint $\\delta_{1} = 0$.\n- Biological feasibility is encoded by the following linear inequalities:\n  1. $-\\ln(3) \\leq \\delta_{2} \\leq \\ln(3)$,\n  2. $-\\ln(3) \\leq \\delta_{3} \\leq \\ln(3)$,\n  3. $\\delta_{3} - 0.5\\,\\delta_{2} \\leq 0.2$,\n  4. $0.2\\,(x_{2} + \\delta_{2}) + 0.1\\,(x_{3} + \\delta_{3}) \\leq 0.5$.\n\nDerive from first principles the minimal-norm actionable perturbation $\\delta^{\\star}$ that flips the phenotype prediction and satisfies all feasibility constraints. Formulate the optimization problem clearly and solve it exactly. Express your final answer as a single row vector using the $\\mathrm{pmatrix}$ environment. No rounding is required and no units are used.", "solution": "The problem is assessed to be valid. It is a well-posed constrained optimization problem, grounded in the formalization of counterfactual explanations for machine learning models, a legitimate topic in systems biomedicine. The problem is self-contained, mathematically consistent, and all terms are clearly defined.\n\nThe objective is to find the minimal-norm actionable perturbation $\\delta^{\\star} \\in \\mathbb{R}^{3}$. The norm is assumed to be the Euclidean norm ($L_2$-norm). Minimizing the norm $\\|\\delta\\|_2$ is equivalent to minimizing its square, $\\|\\delta\\|_2^2 = \\delta_1^2 + \\delta_2^2 + \\delta_3^2$. This is a quadratic programming problem.\n\nFirst, we formulate the optimization problem by stating the objective function and all constraints.\n\nObjective function to minimize:\n$$ f(\\delta) = \\delta_1^2 + \\delta_2^2 + \\delta_3^2 $$\n\nSubject to the following constraints:\n1.  Actionability constraint: $\\delta_1 = 0$.\n2.  Phenotype flip constraint: $w^{\\top}(x + \\delta) + b \\leq 0$.\n3.  Biological feasibility constraints:\n    a. $-\\ln(3) \\leq \\delta_2 \\leq \\ln(3)$\n    b. $-\\ln(3) \\leq \\delta_3 \\leq \\ln(3)$\n    c. $\\delta_3 - 0.5\\,\\delta_2 \\leq 0.2$\n    d. $0.2\\,(x_2 + \\delta_2) + 0.1\\,(x_3 + \\delta_3) \\leq 0.5$\n\nNext, we substitute the given values into these expressions.\nThe actionability constraint $\\delta_1 = 0$ simplifies the objective function to minimizing $f(\\delta_2, \\delta_3) = \\delta_2^2 + \\delta_3^2$. This is the squared distance from the origin in the $(\\delta_2, \\delta_3)$ plane.\n\nLet's simplify the inequality constraints:\nThe phenotype flip constraint is $w^{\\top}x + b + w^{\\top}\\delta \\leq 0$.\nThe initial score is $s(x) = w^{\\top}x+b = (2)(1.2) + (-1)(0.3) + (1)(-0.1) - 0.5 = 2.4 - 0.3 - 0.1 - 0.5 = 1.5$.\nThe perturbation term is $w^{\\top}\\delta = w_1\\delta_1 + w_2\\delta_2 + w_3\\delta_3 = (2)(0) + (-1)\\delta_2 + (1)\\delta_3 = \\delta_3 - \\delta_2$.\nSo, the constraint becomes $1.5 + \\delta_3 - \\delta_2 \\leq 0$, or:\n$$ \\delta_3 - \\delta_2 \\leq -1.5 $$\n\nNow, we simplify the last feasibility constraint (3d) using $x_2 = 0.3$ and $x_3 = -0.1$:\n$0.2\\,(0.3 + \\delta_2) + 0.1\\,(-0.1 + \\delta_3) \\leq 0.5$\n$0.06 + 0.2\\delta_2 - 0.01 + 0.1\\delta_3 \\leq 0.5$\n$0.2\\delta_2 + 0.1\\delta_3 \\leq 0.5 - 0.05$\n$0.2\\delta_2 + 0.1\\delta_3 \\leq 0.45$\nMultiplying by $10$ for clarity:\n$$ 2\\delta_2 + \\delta_3 \\leq 4.5 $$\n\nThe complete, simplified optimization problem is:\nMinimize $f(\\delta_2, \\delta_3) = \\delta_2^2 + \\delta_3^2$\nSubject to:\n1.  $\\delta_3 - \\delta_2 \\leq -1.5$\n2.  $-\\ln(3) \\leq \\delta_2 \\leq \\ln(3)$\n3.  $-\\ln(3) \\leq \\delta_3 \\leq \\ln(3)$\n4.  $\\delta_3 - 0.5\\,\\delta_2 \\leq 0.2$\n5.  $2\\delta_2 + \\delta_3 \\leq 4.5$\n\nThis problem is to find the point in the feasible region (a convex polygon in the $\\delta_2\\delta_3$-plane) that is closest to the origin $(0,0)$. The unconstrained minimum is at $(\\delta_2, \\delta_3)=(0,0)$. However, this point violates constraint (1), as $0-0=0 \\not\\leq -1.5$. Therefore, the solution must lie on the boundary of the feasible region.\n\nThe solution to this problem is the geometric projection of the unconstrained minimum (the origin) onto the feasible set. Since the feasible set is a convex polygon and the origin is outside it, the solution is either a vertex of the polygon or the projection of the origin onto one of its edges.\n\nLet's find the projection of the origin onto the line defined by the first constraint, $\\delta_3 - \\delta_2 = -1.5$, or $\\delta_2 - \\delta_3 - 1.5 = 0$. The vector normal to this line is $n = (1, -1)$. The projection of the origin will be a point $(k, -k)$ for some scalar $k$. Substituting this into the line equation: $k - (-k) = -1.5 \\implies 2k = -1.5 \\implies k = -0.75$. This is incorrect. The point on the line is of the form $\\lambda n = (\\lambda, -\\lambda)$. Substituting into $\\delta_2-\\delta_3-1.5=0$: $\\lambda-(-\\lambda)-1.5=0 \\implies 2\\lambda = 1.5 \\implies \\lambda = 0.75$.\nSo the projected point is $(\\delta_2, \\delta_3) = (0.75, -0.75)$.\n\nAlternatively, we can minimize the squared distance $d^2 = \\delta_2^2 + \\delta_3^2$ subject to $\\delta_3 = \\delta_2 - 1.5$.\n$d^2 = \\delta_2^2 + (\\delta_2 - 1.5)^2$.\nTo find the minimum, we set the derivative with respect to $\\delta_2$ to zero:\n$\\frac{d(d^2)}{d\\delta_2} = 2\\delta_2 + 2(\\delta_2 - 1.5) = 4\\delta_2 - 3 = 0$.\nThis gives $\\delta_2 = \\frac{3}{4} = 0.75$.\nThen, $\\delta_3 = \\delta_2 - 1.5 = 0.75 - 1.5 = -0.75$.\nThe candidate point is $(\\delta_2, \\delta_3) = (0.75, -0.75)$.\n\nNow, we must verify if this point satisfies all other constraints.\nWe use $\\ln(3) \\approx 1.0986$.\n1.  $\\delta_3 - \\delta_2 \\leq -1.5 \\implies -0.75 - 0.75 = -1.5 \\leq -1.5$. (Satisfied, as it is on the boundary).\n2.  $-\\ln(3) \\leq \\delta_2 \\leq \\ln(3) \\implies -1.0986 \\leq 0.75 \\leq 1.0986$. (Satisfied).\n3.  $-\\ln(3) \\leq \\delta_3 \\leq \\ln(3) \\implies -1.0986 \\leq -0.75 \\leq 1.0986$. (Satisfied).\n4.  $\\delta_3 - 0.5\\,\\delta_2 \\leq 0.2 \\implies -0.75 - 0.5(0.75) = -0.75 - 0.375 = -1.125 \\leq 0.2$. (Satisfied).\n5.  $2\\delta_2 + \\delta_3 \\leq 4.5 \\implies 2(0.75) + (-0.75) = 1.5 - 0.75 = 0.75 \\leq 4.5$. (Satisfied).\n\nSince the point $(0.75, -0.75)$ is the projection of the origin onto the line $\\delta_3 - \\delta_2 = -1.5$, and this point lies within the feasible region defined by all other constraints, it is the unique point in the feasible region closest to the origin. Therefore, it is the solution to our optimization problem.\n\nThe minimal-norm perturbation vector $\\delta^{\\star}$ has components:\n$\\delta_1^{\\star} = 0$\n$\\delta_2^{\\star} = 0.75$\n$\\delta_3^{\\star} = -0.75$\n\nSo, the solution is $\\delta^{\\star} = (0, 0.75, -0.75)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  0.75  -0.75\n\\end{pmatrix}\n}\n$$", "id": "4340444"}]}