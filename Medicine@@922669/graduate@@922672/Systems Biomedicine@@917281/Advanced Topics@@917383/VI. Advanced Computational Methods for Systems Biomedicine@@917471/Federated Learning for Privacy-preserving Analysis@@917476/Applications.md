## Applications and Interdisciplinary Connections

Having established the foundational principles and mechanisms of Federated Learning (FL) and Differential Privacy (DP), we now turn our attention to their practical application in the complex landscape of systems biomedicine. This chapter explores how these theoretical tools are operationalized to address real-world scientific, clinical, and ethical challenges. Moving beyond abstract algorithms, we will examine how federated, privacy-preserving analysis enables collaborative research across a spectrum of tasks, from training sophisticated predictive models and performing statistical inference to navigating regulatory frameworks and advancing data justice. The goal is not to reiterate the core mechanics, but to demonstrate their utility, versatility, and integration into the full lifecycle of biomedical data science.

### Core Biomedical Modeling and Inference

Federated learning provides a powerful paradigm for training sophisticated models on distributed biomedical data without centralizing sensitive records. This enables the creation of more robust and generalizable models by leveraging larger and more diverse patient populations than any single institution could provide.

#### End-to-End Predictive Modeling

A quintessential application of [federated learning](@entry_id:637118) in systems biomedicine is the development of clinical risk prediction models. Consider the design of a multi-institutional study to train a sepsis risk prediction model from Electronic Health Record (EHR) data. A robust study design requires a comprehensive, end-to-end protocol that encompasses not only the core learning algorithm but also the threat model, privacy accounting, optimization strategy, and a rigorous evaluation plan.

In such a cross-silo setting, the threat model typically assumes an honest-but-curious central server and passive network adversaries. To mitigate these threats, communications are protected by transport-layer encryption, and Secure Aggregation (SA) is employed to ensure the server only observes the sum of model updates from participating hospitals, never an individual hospital's contribution. Patient-level privacy is formally guaranteed using Differentially Private Stochastic Gradient Descent (DP-SGD). This involves each hospital clipping the per-example gradients to a fixed norm—bounding the influence of any single patient—and adding calibrated Gaussian noise before aggregation. The cumulative privacy loss across all training rounds is meticulously tracked using advanced composition techniques like the moments accountant or Rényi Differential Privacy (RDP), which provide tight bounds on the final $(\epsilon, \delta)$ guarantee.

Because data across hospitals are inherently non-identically distributed (non-IID), specialized optimization strategies are required. Standard Federated Averaging (FedAvg), which weights client updates by dataset size, can be augmented with techniques like FedProx, which adds a proximal term to the local objective function to mitigate [client drift](@entry_id:634167) and improve convergence. The evaluation of the final model must be equally rigorous, including assessments of discrimination (e.g., AUROC, AUPRC), calibration, and clinical utility, often performed on held-out data at each site. Crucially, this evaluation must also examine model performance across different sites to assess transportability and across demographic subgroups to check for fairness, ensuring the model does not perpetuate or amplify existing health disparities [@problem_id:4341010].

#### Survival Analysis

Many critical questions in clinical research involve time-to-event outcomes, which are analyzed using survival models. The Cox [proportional hazards model](@entry_id:171806) is a cornerstone of this field, yet its traditional implementation requires centralized access to patient-level event times and covariates. Federated learning makes it possible to fit such models without sharing raw data. The key insight is that the derivatives of the Cox partial [log-likelihood](@entry_id:273783)—the gradient and the Hessian matrix—can be expressed as sums over distinct event times. Each term in these sums depends on the covariates of the individual experiencing the event and on weighted moments (sums of powers of covariates, weighted by risk scores) over the set of individuals still at risk at that event time.

This structure is perfectly suited for federated execution. At each event time, every participating hospital can privately compute its local contribution to these weighted sums ($S^{(0)}$, $S^{(1)}$, $S^{(2)}$). These local aggregates can then be securely combined by a central server using Secure Aggregation. The server reconstructs only the global sums, from which it can compute the global gradient and Hessian of the partial log-likelihood. This enables iterative optimization algorithms, such as Newton-Raphson, to be performed on the global model without any hospital ever revealing its patient-level data. This application demonstrates how classical, likelihood-based statistical models can be seamlessly adapted to a privacy-preserving, federated framework [@problem_id:4341225].

#### Genome-Wide Association Studies (GWAS)

Federated analysis extends beyond [predictive modeling](@entry_id:166398) to large-scale [statistical inference](@entry_id:172747), such as in Genome-Wide Association Studies (GWAS). A federated GWAS can test for associations between millions of genetic variants (SNPs) and a disease outcome across distributed hospital cohorts. A common approach is to use a [score test](@entry_id:171353), which only requires fitting a single null model based on covariates (e.g., age, sex, ancestry components) and then computing a test statistic for each SNP.

This process can be made private and federated. First, the null [logistic regression model](@entry_id:637047) is fitted using a federated, differentially private optimization algorithm. Then, for each SNP, the score statistic $U_j$ and its variance $V_j$ can be expressed as sums of per-patient contributions. Hospitals compute their local sums of these contributions. To ensure patient privacy, per-patient contributions to the score are clipped to a bounded range, and calibrated Gaussian noise is added to the local sums before they are securely aggregated at the server. A critical step for maintaining valid [statistical inference](@entry_id:172747) is to account for this added noise. The variance of the final test statistic must be adjusted to include the variance of the aggregated noise. A conservative approach is to use a data-independent upper bound for the statistical variance and add the known noise variance to the denominator of the [test statistic](@entry_id:167372). This ensures that the resulting p-values are not anti-conservative and that the [family-wise error rate](@entry_id:175741) can be controlled using standard procedures like the Holm-Bonferroni correction, facilitating valid scientific discovery without data centralization [@problem_id:4341028].

### Advanced Data Types and Architectures

Biomedical data is not limited to simple tabular formats. It encompasses complex data types like time series, graphs, and multi-modal records. The principles of [federated learning](@entry_id:637118) can be extended to these domains, enabling collaborative analysis of rich, structured data.

#### Irregularly Sampled Time Series in EHR

Electronic Health Records (EHR) consist of patient data collected at irregular intervals, forming sparse and unevenly sampled time series. Standard [recurrent neural networks](@entry_id:171248) (RNNs) that assume uniform time steps are ill-suited for this data. More advanced architectures, such as Gated Recurrent Units with Decay (GRU-D) or Ordinary Differential Equation RNNs (ODE-RNNs), are designed to explicitly model the information contained in the time gaps between observations.

In a federated setting, these models can be trained privately, but this introduces a new privacy challenge: the absolute timestamps of clinical events can be highly sensitive. A robust privacy-preserving protocol must protect not only the patient's identity and feature values but also the timing of their care. This can be achieved by processing all time-related information locally on the client device. Instead of sharing absolute timestamps, each hospital converts them into relative time gaps ($\Delta t_{j} = t_{j} - t_{j-1}$). These time gaps, which are necessary inputs for models like GRU-D, are then used to train the local model. The standard DP-SGD protocol (per-example [gradient clipping](@entry_id:634808) and noise addition) is applied to the model updates to protect the underlying feature values. By never transmitting [absolute time](@entry_id:265046) information, the privacy of the patient's timeline is preserved with respect to the central server [@problem_id:4341030].

#### Patient Similarity Graphs

Graph-structured data, such as patient similarity networks, are increasingly used in systems biomedicine to model complex relationships. Graph Neural Networks (GNNs) learn from such data using a [message-passing](@entry_id:751915) mechanism, where each node iteratively aggregates information from its neighbors. In a federated setting where the graph is partitioned across multiple hospitals, this [message passing](@entry_id:276725) must occur without revealing the graph structure (i.e., the edges) or the node features to a central party.

This can be accomplished using a combination of cryptographic techniques. To identify neighbors that exist across different hospitals, parties can engage in Private Set Intersection (PSI) to discover shared nodes without revealing non-matching nodes. During the [message-passing](@entry_id:751915) step of GNN training, each hospital computes the partial sum of messages from its local neighbors. These [partial sums](@entry_id:162077) are then masked with data-independent random values that sum to zero across all hospitals. A Secure Aggregation protocol allows the server to learn only the unmasked global sum of messages for each node, which is identical to what would be computed in a centralized setting. This process perfectly hides each hospital's contribution, thereby protecting the privacy of cross-site edges. This cryptographic protection of the [forward pass](@entry_id:193086) is then combined with differential privacy on the [backward pass](@entry_id:199535) (i.e., on the gradients) to protect against inference attacks from the trained model itself [@problem_id:4341140].

#### Vertical Federated Learning for Multi-modal Data

The applications discussed so far assume a "horizontal" federation, where different hospitals hold the same types of features for different sets of patients. An alternative and equally important scenario is Vertical Federated Learning (VFL), where different parties hold different feature sets for the same set of patients. For example, one hospital may have genomic data for a patient cohort, while another has their clinical imaging data.

Training a unified model in a VFL setting requires a different set of privacy-enhancing technologies. First, the parties must securely identify their overlapping patient cohort without revealing the identities of non-overlapping patients; this is a canonical use case for Private Set Intersection (PSI). Once aligned, training a model like [logistic regression](@entry_id:136386) requires computing intermediate values that depend on features from both parties. For instance, to compute the logit $z_i = x_{A,i}^{\top} w_A + x_{B,i}^{\top} w_B$, one party can compute its local part, encrypt it using Additive Homomorphic Encryption (AHE), and send it to the other party. The second party, which holds the labels, can homomorphically add its local contribution and then decrypt the final sum. The exchange of gradient information, which depends on the private labels, must then be protected using Differential Privacy. This VFL architecture enables the integration of multi-modal data from different sources to build more powerful models, a task that is often organizationally and legally intractable without such privacy-preserving techniques [@problem_id:4341202].

### The Full Analytical Lifecycle

The utility of federated, privacy-preserving analysis extends beyond the model training phase. It provides a toolkit for conducting nearly every stage of a collaborative research project, from initial data harmonization to final [model evaluation](@entry_id:164873) and post-marketing surveillance.

#### Data Pre-processing and Harmonization

Before any modeling can begin, data from different sites must be harmonized. A pervasive challenge in multi-site biomedical studies, particularly with omics data, is the presence of "[batch effects](@entry_id:265859)"—technical variations arising from differences in lab protocols, equipment, or reagents. These site-specific shifts can confound biological signals if not properly addressed.

Privacy-preserving normalization techniques can mitigate these effects. For simple location-scale shifts, a federated moment-matching approach can be used. Each hospital computes its local feature mean and variance privately. Concurrently, the server uses Secure Aggregation to compute the global mean and variance from the securely summed first and second moments across all sites. The server then broadcasts these global target statistics to all hospitals, which use them to standardize their local data. This aligns the first two moments of the distribution across sites without revealing any site-specific statistics. For more complex, non-linear batch effects, or for data with outliers, more robust methods can be employed. For example, by securely aggregating binned counts to form a global [histogram](@entry_id:178776), the server can approximate the global data distribution and derive robust statistics like the global median and [median absolute deviation](@entry_id:167991) (MAD), enabling a robust federated normalization [@problem_id:4341009].

#### Federated Descriptive Statistics and Evaluation

A fundamental first step in any analysis is to compute basic descriptive statistics of the population. Even simple queries for counts, means, or variances require privacy protection in a federated setting. A robust method is to release noisy "[sufficient statistics](@entry_id:164717)." For instance, to compute a mean and variance, the server can use Secure Aggregation to obtain the global sum of counts ($n$), values ($\sum x_i$), and squared values ($\sum x_i^2$). To ensure Differential Privacy, these three values can be released as a single vector-valued query. The Gaussian mechanism is applied by adding noise calibrated to the joint $\ell_2$-sensitivity of this vector, which efficiently uses the [privacy budget](@entry_id:276909) by avoiding composition. The final mean and variance are then computed from these noisy aggregates as a post-processing step, which does not consume additional [privacy budget](@entry_id:276909). A similar approach can be used for [contingency tables](@entry_id:162738), which are fundamental to epidemiological studies and pharmacovigilance [@problem_id:4341180] [@problem_id:4581838].

Model evaluation can also be performed in a federated and private manner. For example, the Area Under the ROC Curve (AUC), a key metric for binary classifiers, can be computed without sharing individual-level predictions or labels. The AUC has a probabilistic interpretation related to the probability that a randomly chosen positive example scores higher than a randomly chosen negative one. This can be calculated exactly if the model's continuous scores are first quantized into a set of discrete bins. Each hospital then simply counts the number of true positives and true negatives that fall into each score bin. These counts can be securely aggregated, and the global AUC can be computed directly from these aggregated counts, providing a perfect evaluation of the global model's performance without revealing any patient-level information [@problem_id:4341080].

#### Addressing Data Heterogeneity and Imbalance

Real-world federated datasets are rarely identically distributed. A particularly challenging form of heterogeneity is class imbalance, a common problem in rare disease detection where positive examples are scarce. Standard federated training can lead to models that are biased toward the majority (negative) class, as its contribution to the global gradient dominates.

A principled way to address this is to re-weight the loss function to give more importance to the rare class. In a federated setting, this can be done by first obtaining a global estimate of the class prevalence. Each hospital counts its number of positive and negative examples. These counts are then securely aggregated and perturbed with DP-calibrated noise to produce a noisy-but-consistent estimate of the true global class prevalence. This global estimate is broadcast back to the clients, who use it to compute class weights (e.g., inverse-prevalence weights) for their local training. This approach allows the federated system to approximate a globally balanced training objective, significantly improving model performance on the rare class, while respecting all privacy constraints [@problem_id:4341031].

### Interdisciplinary Connections: Ethics, Law, and Governance

Federated learning does not exist in a vacuum; it is a sociotechnical system that intersects with critical ethical, legal, and governance frameworks. Its design and deployment must be informed by these broader contexts to be responsible and effective.

#### Regulatory Compliance: HIPAA and GDPR

Global health research is governed by regulations like the Health Insurance Portability and Accountability Act (HIPAA) in the United States and the General Data Protection Regulation (GDPR) in Europe. These frameworks impose strict requirements, including principles of data minimization and the need for auditable security controls. Federated learning systems can be designed to be compliant with these regulations.

Data minimization, a core GDPR principle, is inherently supported by FL architectures that keep raw data localized. This principle is further strengthened by client-side Differential Privacy, where noise is added before data ever leaves the client, and by the use of Secure Aggregation. For auditability, as required by HIPAA, a federated system can implement append-only, tamper-evident logs. These logs must not contain any Protected Health Information (PHI). Instead, they can record essential [metadata](@entry_id:275500), such as pseudonymous client identifiers, timestamps, and cryptographic hashes of model parameters and aggregated updates. This creates a verifiable trail of participation and protocol execution. Advanced cryptographic methods like Zero-Knowledge Proofs (ZKPs) can even allow a client to prove to an auditor that it correctly followed the protocol (e.g., applied the correct amount of DP noise) without revealing any of its private data, providing an exceptionally strong form of auditable compliance [@problem_id:4341022].

#### Algorithmic Fairness

A model that is accurate on average can still be biased and perform poorly for specific demographic subgroups. Ensuring fairness is a critical ethical and clinical requirement. Standard [fairness metrics](@entry_id:634499), such as [demographic parity](@entry_id:635293) and [equalized odds](@entry_id:637744), can be evaluated in a federated setting.

Demographic parity requires that the rate of positive predictions be equal across all protected groups, while [equalized odds](@entry_id:637744) requires that the true positive and false positive rates be equal across groups. To evaluate these metrics on the global population, each hospital first computes local counts of the relevant events (e.g., true positives, false positives, total individuals) stratified by the protected attribute (e.g., race, sex). These counts are then securely aggregated using SMPC. To protect individual privacy, the final aggregated counts are perturbed with noise calibrated for Differential Privacy before being used to compute the global [fairness metrics](@entry_id:634499). This allows the consortium to audit the model for bias across the entire federated network without centralizing sensitive demographic and outcome data [@problem_id:4341131].

#### Data Sovereignty and Epistemic Justice

Beyond regulatory compliance and fairness, a paramount consideration in modern genomics and health research is the concept of data sovereignty, particularly for Indigenous and other historically marginalized populations. Data sovereignty is not merely about privacy or ownership; it is the inherent right of a people to govern the collection, use, and application of their data according to their own laws, values, and priorities. It is a manifestation of collective self-determination.

This concept is deeply tied to epistemic justice—righting the historical wrongs where knowledge was extracted from communities without their consent, control, or benefit. Federated learning and [differential privacy](@entry_id:261539) can be powerful technical *tools* to help realize data sovereignty. By keeping data localized, FL respects the physical control of data. However, technology alone is insufficient. True data sovereignty requires a governance framework, such as one guided by the CARE Principles (Collective Benefit, Authority to Control, Responsibility, Ethics), where the community has ultimate authority over the research agenda. This includes co-governance of research questions, study design, data interpretation, and the sharing of benefits. In this context, FL is not the solution itself, but an enabling infrastructure that can be deployed within a sovereignty-respecting framework to facilitate community-led, collaborative research [@problem_id:4330114].

In conclusion, [federated learning](@entry_id:637118) and its associated privacy technologies offer a transformative toolkit for systems biomedicine. They enable the development of powerful models and the execution of rigorous statistical analyses on distributed, sensitive data. Yet their greatest potential is realized when they are thoughtfully integrated into the full scientific lifecycle and deployed within robust ethical and legal governance frameworks, fostering a new paradigm of responsible, collaborative, and just data science.