{"hands_on_practices": [{"introduction": "Federated Learning's promise of privacy comes with significant engineering hurdles, chief among them being communication overhead. As models grow in size, the process of repeatedly transmitting parameter updates between clients and a central server can become a major bottleneck. This exercise provides a tangible feel for this challenge by guiding you through the calculation of communication costs and exploring how simple compression techniques like quantization can dramatically improve the feasibility of large-scale federated training [@problem_id:4840332].", "problem": "A consortium of hospitals is training a shared diagnostic model using Federated Averaging (FedAvg), a standard algorithm in federated learning, to preserve patient privacy by keeping data on-premise. In each communication round, every hospital transmits its local model update to the coordinator and then receives the aggregated global model. Assume the model has $d=10^7$ trainable parameters and that the size of a transmitted update equals the size of the parameter vector. Consider two encoding regimes for parameters: single-precision floating point with $32$ bits per parameter and uniform quantization with $8$ bits per parameter. The network link available to each hospital is $100$ megabits per second, where one megabit is $10^{6}$ bits.\n\nStarting from the core definitions of data size and throughput (data size in bits, throughput in bits per second, and time as size divided by rate), derive the total per-round communication time per hospital (upload plus download) under both encoding regimes. Assume no compression, no protocol overhead, and that the coordinator sends back a model of the same size as the update sent by the hospital.\n\nState whether each regime is feasible under a per-round wall-clock budget of $2\\,\\mathrm{s}$, where feasibility is defined as total per-round communication time less than or equal to $2\\,\\mathrm{s}$.\n\nRound your final numerical times to four significant figures, and express the final times in seconds. Provide your final answer as a row matrix, with the first entry equal to the $32$-bit time and the second entry equal to the $8$-bit time.", "solution": "The problem is first validated against the required criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Number of model parameters: $d = 10^7$\n-   Encoding regime 1 (single-precision): $b_{32} = 32$ bits per parameter\n-   Encoding regime 2 (quantized): $b_8 = 8$ bits per parameter\n-   Network link speed (throughput): $R = 100$ megabits per second, where $1$ megabit $= 10^6$ bits.\n-   Communication round structure: each hospital performs one upload and one download.\n-   Size of downloaded model: same as the uploaded model update.\n-   Assumptions: no compression, no protocol overhead.\n-   Feasibility budget: total per-round communication time $\\le T_{budget} = 2\\,\\mathrm{s}$.\n-   Calculation basis: time = data size / throughput.\n-   Rounding requirement: final times to be rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in principles of computer networking and federated learning. The scenario is a standard application of federated learning in medical informatics. The parameters provided ($10^7$ parameters, $100\\,\\mathrm{Mbps}$ link, $32$-bit and $8$-bit encodings) are realistic values in modern machine learning and network infrastructure. The problem is well-posed, providing all necessary information and clear, objective definitions (e.g., $1$ megabit $= 10^6$ bits) to calculate a unique solution. It is free of scientific flaws, ambiguities, or contradictions.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A solution will be provided.\n\n### Solution Derivation\n\nThe analysis proceeds by first defining the total data size to be transmitted in a single communication round for each encoding regime. Then, using the given network throughput, the time required for this transmission is calculated. Finally, this time is compared against the specified feasibility budget.\n\nLet $d$ be the number of trainable parameters in the model.\nLet $b$ be the number of bits used to encode each parameter.\nThe size of a model update, $S$, in bits is given by the product of the number of parameters and the number of bits per parameter:\n$$S = d \\times b$$\n\nThe network link speed, $R$, is given as $100$ megabits per second. Using the provided definition, this is:\n$$R = 100 \\times 10^6 \\frac{\\mathrm{bits}}{\\mathrm{second}} = 10^8 \\frac{\\mathrm{bits}}{\\mathrm{second}}$$\n\nA single communication round for one hospital consists of one upload of the model update to the coordinator and one download of the aggregated global model from the coordinator. The problem states that the downloaded model has the same size as the uploaded update. Therefore, the total data transmitted per hospital per round, $S_{total}$, is twice the size of a single update:\n$$S_{total} = S_{upload} + S_{download} = S + S = 2S$$\n\nThe total time for one round, $T_{round}$, is the total data size divided by the network link speed:\n$$T_{round} = \\frac{S_{total}}{R} = \\frac{2S}{R} = \\frac{2db}{R}$$\n\nWe now calculate this time for both encoding regimes.\n\n**Regime 1: Single-Precision Floating Point ($32$ bits)**\n\nIn this case, the number of bits per parameter is $b_{32} = 32$.\nThe number of parameters is $d = 10^7$.\nThe size of a single model update is:\n$$S_{32} = d \\times b_{32} = 10^7 \\times 32 = 3.2 \\times 10^8 \\text{ bits}$$\n\nThe total per-round communication time, $T_{round, 32}$, is:\n$$T_{round, 32} = \\frac{2 S_{32}}{R} = \\frac{2 \\times (3.2 \\times 10^8 \\text{ bits})}{10^8 \\text{ bits/s}} = 2 \\times 3.2\\,\\mathrm{s} = 6.4\\,\\mathrm{s}$$\n\nThe problem requires rounding to four significant figures. Thus, $T_{round, 32} = 6.400\\,\\mathrm{s}$.\n\n**Feasibility Check for Regime 1:**\nThe feasibility budget is $T_{budget} = 2\\,\\mathrm{s}$.\nWe compare the calculated time: $6.400\\,\\mathrm{s} > 2\\,\\mathrm{s}$.\nTherefore, the $32$-bit encoding regime is **not feasible** under the given constraints.\n\n**Regime 2: Uniform Quantization ($8$ bits)**\n\nIn this case, the number of bits per parameter is $b_8 = 8$.\nThe number of parameters is $d = 10^7$.\nThe size of a single model update is:\n$$S_8 = d \\times b_8 = 10^7 \\times 8 = 8 \\times 10^7 \\text{ bits}$$\n\nThe total per-round communication time, $T_{round, 8}$, is:\n$$T_{round, 8} = \\frac{2 S_8}{R} = \\frac{2 \\times (8 \\times 10^7 \\text{ bits})}{10^8 \\text{ bits/s}} = 2 \\times 0.8\\,\\mathrm{s} = 1.6\\,\\mathrm{s}$$\n\nThe problem requires rounding to four significant figures. Thus, $T_{round, 8} = 1.600\\,\\mathrm{s}$.\n\n**Feasibility Check for Regime 2:**\nThe feasibility budget is $T_{budget} = 2\\,\\mathrm{s}$.\nWe compare the calculated time: $1.600\\,\\mathrm{s} \\le 2\\,\\mathrm{s}$.\nTherefore, the $8$-bit encoding regime is **feasible** under the given constraints.\n\nThe final answer requires the two calculated times, in seconds and rounded to four significant figures, presented as a row matrix.\nThe time for the $32$-bit regime is $6.400\\,\\mathrm{s}$.\nThe time for the $8$-bit regime is $1.600\\,\\mathrm{s}$.\nThe row matrix is $\\begin{pmatrix} 6.400 & 1.600 \\end{pmatrix}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n6.400 & 1.600\n\\end{pmatrix}\n}\n$$", "id": "4840332"}, {"introduction": "While federated learning prevents the direct sharing of raw data, it is not inherently immune to privacy breaches. An adversary in control of the central server can, under certain conditions, inspect the received model gradients and reverse-engineer the private training data used to compute them. This practice makes the abstract threat of such gradient inversion attacks concrete, guiding you to derive an exact data reconstruction formula for a simple linear model and encouraging you to think about potential mitigations [@problem_id:4341228].", "problem": "A consortium of hospitals collaborates under Federated Learning (FL) to build a linear predictor for a continuous biomarker in Systems Biomedicine. At each round, a client computes the gradient of a regularized squared loss using a single, randomly sampled patient record and sends this gradient to the coordinator. An adversarial coordinator knows the current model parameters and the patient’s label from side information. Consider a single sample with features $x \\in \\mathbb{R}^{d}$, nonnegative label $y \\in \\mathbb{R}_{\\ge 0}$, and model parameter $w \\in \\mathbb{R}^{d}$ where all entries of $x$ and $w$ are nonnegative due to domain constraints (e.g., nonnegative expression features and nonnegative weights imposed by interpretability constraints). The client uses the regularized squared loss\n$$\n\\mathcal{L}(w;x,y) = \\frac{1}{2}\\left(w^{\\top} x - y\\right)^{2} + \\frac{\\lambda}{2}\\|w\\|^{2},\n$$\nwith regularization coefficient $\\lambda \\ge 0$, and transmits the gradient $\\nabla_{w}\\mathcal{L}$ to the server. Assume the adversary observes $g = \\nabla_{w}\\mathcal{L}(w;x,y)$, and also knows $w$, $y$, and $\\lambda$. You may further assume $w^{\\top}\\left(g - \\lambda w\\right) > 0$, which is a sufficient condition for uniqueness under the aforementioned nonnegativity assumptions.\n\nUsing only fundamental definitions, derive a closed-form analytic expression $x^{\\star}(g,w,y,\\lambda)$ that exactly reconstructs the input features $x$ from the observed gradient $g$, the known label $y$, parameter $w$, and regularization $\\lambda$. As a privacy-preserving mitigation, propose a linear disclosure mapping that is applied client-side to alter the communicated quantity so that exact reconstruction of $x$ is no longer possible from a single round, while retaining utility for aggregation. Your derivation must start from the definition of the gradient of the given loss and standard linear algebra identities, and must not assume or state the target reconstruction formula in advance.\n\nExpress your final reconstructed $x^{\\star}$ as a single closed-form analytic expression. No numerical evaluation is required, and no rounding is needed. If you propose any angles or physical units in your mitigation, specify radians or appropriate units, respectively; otherwise, keep the mitigation symbolic.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the domain of machine learning and privacy, well-posed with sufficient information and a specified condition to ensure uniqueness, and objectively formulated using standard mathematical notation.\n\nThe solution proceeds in two parts. First, we derive the closed-form expression for the feature vector $x$. Second, we propose a linear disclosure mapping to mitigate this privacy leak.\n\n**Part 1: Derivation of the Reconstruction Formula for $x^{\\star}$**\n\nThe client computes the gradient of the regularized squared loss function:\n$$\n\\mathcal{L}(w;x,y) = \\frac{1}{2}\\left(w^{\\top} x - y\\right)^{2} + \\frac{\\lambda}{2}\\|w\\|^{2}\n$$\nwhere $\\|w\\|^2 = w^\\top w$. The gradient $g$ with respect to the model parameters $w$ is given by:\n$$\ng = \\nabla_{w}\\mathcal{L} = \\frac{\\partial}{\\partial w} \\left( \\frac{1}{2}\\left(w^{\\top} x - y\\right)^{2} + \\frac{\\lambda}{2}w^{\\top}w \\right)\n$$\nUsing the chain rule for the first term and the standard gradient of a quadratic form for the second, we have:\n$$\n\\nabla_{w} \\left( \\frac{1}{2}\\left(w^{\\top} x - y\\right)^{2} \\right) = \\frac{1}{2} \\cdot 2 \\left(w^{\\top} x - y\\right) \\cdot \\nabla_{w}(w^{\\top}x - y) = \\left(w^{\\top} x - y\\right)x\n$$\n$$\n\\nabla_{w} \\left( \\frac{\\lambda}{2}w^{\\top}w \\right) = \\lambda w\n$$\nCombining these terms yields the expression for the observed gradient $g$:\n$$\ng = (w^{\\top}x - y)x + \\lambda w\n$$\nThe adversary's goal is to solve this equation for the unknown feature vector $x$, given their knowledge of $g$, $w$, $y$, and $\\lambda$. We can rearrange the equation to isolate the terms involving $x$:\n$$\ng - \\lambda w = (w^{\\top}x - y)x\n$$\nThis equation reveals that the known vector on the left-hand side, $g - \\lambda w$, is a scalar multiple of the unknown vector $x$. This implies that $x$ must be parallel to $g - \\lambda w$. However, solving for $x$ directly is complicated by its presence inside the scalar term $w^{\\top}x - y$.\n\nTo proceed, we can first find the value of the scalar quantity $S = w^{\\top}x$. We take the inner product of both sides of the rearranged equation with $w$:\n$$\nw^{\\top}(g - \\lambda w) = w^{\\top}((w^{\\top}x - y)x)\n$$\nUsing the linearity of the inner product, we get:\n$$\nw^{\\top}(g - \\lambda w) = (w^{\\top}x - y)(w^{\\top}x)\n$$\nLet's substitute $S = w^{\\top}x$ and define the known constant $C = w^{\\top}(g - \\lambda w)$. The equation becomes a quadratic equation for $S$:\n$$\nC = (S - y)S\n$$\n$$\nS^2 - yS - C = 0\n$$\nSolving for $S$ using the quadratic formula:\n$$\nS = \\frac{-(-y) \\pm \\sqrt{(-y)^2 - 4(1)(-C)}}{2(1)} = \\frac{y \\pm \\sqrt{y^2 + 4C}}{2}\n$$\nSubstituting $C = w^{\\top}(g - \\lambda w)$ back:\n$$\nS = w^{\\top}x = \\frac{y \\pm \\sqrt{y^2 + 4w^{\\top}(g - \\lambda w)}}{2}\n$$\nThe problem specifies that the features $x \\in \\mathbb{R}^d$ and weights $w \\in \\mathbb{R}^d$ have all nonnegative entries. Consequently, their inner product $w^{\\top}x = \\sum_{i=1}^d w_i x_i$ must be nonnegative, i.e., $w^{\\top}x \\ge 0$.\nThe problem also provides the condition $C = w^{\\top}(g - \\lambda w) > 0$. Since $y \\ge 0$, we have $y^2 \\ge 0$. Therefore, the term under the square root, $y^2 + 4C$, is strictly positive. Let $D = \\sqrt{y^2 + 4C}$. Since $C>0$, we have $D > \\sqrt{y^2} = |y| = y$.\n\nThe two possible solutions for $S$ are $S_1 = \\frac{y+D}{2}$ and $S_2 = \\frac{y-D}{2}$.\nSince $D > y$, the second solution $S_2$ is negative. Given the constraint $S = w^{\\top}x \\ge 0$, this solution is invalid.\nThe first solution $S_1$ is positive, as $y \\ge 0$ and $D > 0$. Thus, we have a unique solution for $S$:\n$$\nw^{\\top}x = \\frac{y + \\sqrt{y^2 + 4w^{\\top}(g - \\lambda w)}}{2}\n$$\nNow that we have determined the value of $w^{\\top}x$, we can find the value of the scalar multiplier $(w^{\\top}x - y)$.\n$$\nw^{\\top}x - y = \\left( \\frac{y + \\sqrt{y^2 + 4w^{\\top}(g - \\lambda w)}}{2} \\right) - y = \\frac{-y + \\sqrt{y^2 + 4w^{\\top}(g - \\lambda w)}}{2}\n$$\nSince $\\sqrt{y^2 + 4w^{\\top}(g - \\lambda w)} > y$, this scalar term is strictly positive. This means we can divide by it. Returning to the equation $g - \\lambda w = (w^{\\top}x - y)x$, we can now solve for $x$:\n$$\nx^{\\star} = \\frac{g - \\lambda w}{w^{\\top}x - y}\n$$\nSubstituting the expression for the denominator, we obtain the closed-form reconstruction formula:\n$$\nx^{\\star} = \\frac{g - \\lambda w}{\\frac{-y + \\sqrt{y^2 + 4w^{\\top}(g - \\lambda w)}}{2}} = \\frac{2(g - \\lambda w)}{\\sqrt{y^2 + 4w^{\\top}(g - \\lambda w)} - y}\n$$\nThis is the exact reconstruction of the feature vector $x$ from the information available to the adversary.\n\n**Part 2: Privacy-Preserving Mitigation**\n\nThe derived reconstruction formula demonstrates a critical privacy leak. To mitigate this, we propose a linear disclosure mapping that alters the communicated gradient $g$ to make exact reconstruction impossible, while retaining utility for the federated aggregation process.\n\nThe proposed mitigation is **Gradient Projection**. Instead of transmitting the raw gradient $g$, the client transmits a projected version, $g'$.\n\n**Mechanism:** The client computes the gradient $g=\\nabla_w \\mathcal{L}$ as before. Then, it computes the projection of $g$ onto the subspace orthogonal to the current weight vector $w$. This is a linear mapping defined by the projection matrix $P_w = I - \\frac{w w^{\\top}}{\\|w\\|^2}$. The client sends the altered quantity $g'$ to the server:\n$$\ng' = P_w g = \\left(I - \\frac{w w^{\\top}}{\\|w\\|^2}\\right) g\n$$\n\n**Prevention of Reconstruction:** An adversary observing $g'$ would attempt the same reconstruction attack. They know $g'$, $w$, $y$, and $\\lambda$. The attack fundamentally relied on the fact that $g - \\lambda w$ is proportional to $x$. When the adversary receives $g'$, they can no longer make this assumption. If they try to form an equation for $x$, they would start from the effect of the projection on the original gradient equation:\n$$\ng' = P_w g = P_w \\left( (w^{\\top}x - y)x + \\lambda w \\right)\n$$\nSince $P_w w = 0$, the regularization term vanishes under projection:\n$$\ng' = (w^{\\top}x - y) P_w x\n$$\nThe adversary observes $g'$ and wishes to solve for $x$. The vector $x$ can be decomposed into two orthogonal components: $x = x_{||w} + x_{\\perp w}$, where $x_{||w}$ is parallel to $w$ and $x_{\\perp w}$ is orthogonal to $w$. The projection $P_w x$ is exactly $x_{\\perp w}$. The equation becomes:\n$$\ng' = (w^{\\top}x - y) x_{\\perp w}\n$$\nThis equation only contains information about the component of $x$ that is orthogonal to $w$. The component of $x$ that is parallel to $w$, $x_{||w} = \\frac{w^\\top x}{\\|w\\|^2}w$, has been completely eliminated from the equation. An adversary can determine $x_{\\perp w}$ (up to a scalar ambiguity similar to the original attack), but has no information from $g'$ about $x_{||w}$. Since any vector of the form $x' = x_{\\perp w} + \\alpha w$ for an arbitrary scalar $\\alpha$ has the same projection $P_w x' = x_{\\perp w}$, the reconstruction problem becomes underdetermined. Thus, exact reconstruction of $x$ is impossible.\n\n**Utility for Aggregation:** The server's goal is to aggregate gradients, e.g., $\\frac{1}{N} \\sum_i g_i$, to perform a model update. With this mitigation, the server aggregates the projected gradients:\n$$\n\\frac{1}{N} \\sum_i g'_i = \\frac{1}{N} \\sum_i P_w g_i = P_w \\left( \\frac{1}{N} \\sum_i g_i \\right)\n$$\nThe server obtains the projection of the true average gradient. A model update of the form $w_{t+1} \\leftarrow w_t - \\eta \\left( \\sum_i g'_i \\right)$ is now an update in a direction orthogonal to $w_t$. This corresponds to a projected gradient descent algorithm, which constrains the optimization trajectory to a hypersphere of radius $\\|w_t\\|$. While this alters the optimization dynamics from standard gradient descent, it is a well-defined optimization procedure that still seeks to minimize the loss function under this new constraint, thus retaining a clear form of utility.", "answer": "$$\n\\boxed{\\frac{2(g - \\lambda w)}{\\sqrt{y^2 + 4w^{\\top}(g - \\lambda w)} - y}}\n$$", "id": "4341228"}, {"introduction": "To formally protect against the privacy risks demonstrated by gradient inversion, we can employ the rigorous mathematical framework of Differential Privacy (DP). DP ensures that the output of an algorithm does not reveal significant information about any single individual's data by introducing carefully calibrated statistical noise. This fundamental exercise challenges you to derive the proper noise levels for two cornerstone mechanisms—the Gaussian and Laplace mechanisms—by starting from the first principles of DP, including the critical concept of global sensitivity [@problem_id:4341061].", "problem": "A federated average (FedAvg) server aggregates per-round client gradients in systems biomedicine by computing the average of $m$ client gradients, each clipped in the Euclidean norm to a bound $C>0$, and then adding noise for privacy. Consider a single round in which the server releases the noisy average of a fixed set of $m$ clients. Let the gradient dimension be $d$, and define neighboring datasets under the substitution adjacency: two client sets are neighbors if they differ by replacing one clipped gradient vector with another clipped gradient vector. The randomized mechanism must satisfy Differential Privacy (DP), defined as follows: a mechanism $\\mathcal{M}$ is $(\\epsilon,\\delta)$-DP if for all neighboring datasets $D$ and $D'$ and all measurable sets $S$, $\\Pr[\\mathcal{M}(D)\\in S]\\leq \\exp(\\epsilon)\\Pr[\\mathcal{M}(D')\\in S]+\\delta$. For the Laplace mechanism, consider the pure $\\epsilon$-DP case, where $\\delta=0$.\n\nStarting from the formal DP definition and the definition of global sensitivity for a vector-valued query, derive from first principles the calibration of the noise distributions needed to privatize the averaged clipped gradient under the substitution adjacency:\n1. For the Gaussian mechanism, derive the per-coordinate noise standard deviation $s_G$ required to achieve $(\\epsilon,\\delta)$-DP for the average of $m$ clipped gradients with Euclidean norm bound $C$.\n2. For the Laplace mechanism, derive the per-coordinate Laplace scale $b_L$ required to achieve $\\epsilon$-DP for the same average when noise is added independently to each coordinate.\n\nAssume the following scientifically realistic parameters for a single release in systems biomedicine model training:\n- Gradient dimension $d=100$,\n- Number of participating clients $m=50$,\n- Clipping bound $C=0.5$ (Euclidean norm),\n- Target $\\epsilon=0.8$ and $\\delta=1.0\\times 10^{-6}$ for the Gaussian mechanism,\n- Target $\\epsilon=0.8$ for the Laplace mechanism.\n\nCompute the numerical values of $s_G$ and $b_L$ under these parameters. Round your answers to four significant figures. Provide the two values as a single row vector in the order $\\big(s_G,\\, b_L\\big)$. No units are required.", "solution": "The problem requires the derivation and computation of noise parameters for the Gaussian and Laplace mechanisms to ensure differential privacy (DP) for the federated averaging of clipped gradients. We begin by formalizing the query and computing its global sensitivity under the specified adjacency definition.\n\nLet the set of $m$ client gradients be $D = \\{g_1, g_2, \\ldots, g_m\\}$, where each gradient $g_i \\in \\mathbb{R}^d$ is a vector in a $d$-dimensional space. Each gradient has been clipped such that its Euclidean norm (or $L_2$ norm) is bounded by a constant $C > 0$, i.e., $\\|g_i\\|_2 \\leq C$ for all $i \\in \\{1, \\ldots, m\\}$.\n\nThe query function $f$ computes the average of these gradients:\n$$\nf(D) = \\frac{1}{m} \\sum_{i=1}^{m} g_i\n$$\nThe problem specifies a substitution adjacency: two datasets $D$ and $D'$ are considered neighbors if $D'$ can be obtained from $D$ by replacing a single client's gradient vector, say $g_i$, with another valid gradient vector $g'_i$. Both $g_i$ and $g'_i$ must satisfy the clipping constraint, i.e., $\\|g_i\\|_2 \\leq C$ and $\\|g'_i\\|_2 \\leq C$.\n\nTo calibrate the noise for a DP mechanism, we must first compute the global sensitivity of the query function $f$. Global sensitivity is defined as the maximum change in the output of $f$ over all possible neighboring datasets, measured in a specific norm.\n\n**1. Gaussian Mechanism and $L_2$ Sensitivity**\n\nThe Gaussian mechanism adds noise calibrated to the $L_2$ sensitivity of the query function, defined as $\\Delta_2(f) = \\max_{D, D'} \\|f(D) - f(D')\\|_2$, where the maximum is taken over all neighboring datasets $D$ and $D'$.\n\nLet $D = \\{g_1, \\ldots, g_i, \\ldots, g_m\\}$ and its neighbor $D' = \\{g_1, \\ldots, g'_i, \\ldots, g_m\\}$. The difference in the query output is:\n$$\nf(D) - f(D') = \\left(\\frac{1}{m} \\sum_{j=1}^{m} g_j\\right) - \\left(\\frac{1}{m} \\left( \\sum_{j \\neq i} g_j + g'_i \\right)\\right) = \\frac{1}{m}(g_i - g'_i)\n$$\nWe need to find the maximum $L_2$ norm of this difference:\n$$\n\\|f(D) - f(D')\\|_2 = \\left\\|\\frac{1}{m}(g_i - g'_i)\\right\\|_2 = \\frac{1}{m}\\|g_i - g'_i\\|_2\n$$\nTo find the global sensitivity, we must maximize this quantity over all valid choices for $g_i$ and $g'_i$. By the triangle inequality:\n$$\n\\|g_i - g'_i\\|_2 \\leq \\|g_i\\|_2 + \\|-g'_i\\|_2 = \\|g_i\\|_2 + \\|g'_i\\|_2\n$$\nSince $\\|g_i\\|_2 \\leq C$ and $\\|g'_i\\|_2 \\leq C$, the maximum possible value is $C + C = 2C$. This maximum is attained, for instance, by choosing $g'_i = -g_i$ where $\\|g_i\\|_2 = C$. Both vectors satisfy the clipping constraint.\n\nTherefore, the $L_2$ global sensitivity of the averaging query is:\n$$\n\\Delta_2(f) = \\frac{2C}{m}\n$$\nThe Gaussian mechanism adds isotropic noise $Z \\sim \\mathcal{N}(0, s_G^2 I_d)$ to the query result $f(D)$, where $I_d$ is the $d \\times d$ identity matrix and $s_G$ is the per-coordinate standard deviation. For a given $(\\epsilon, \\delta)$-DP guarantee where $\\epsilon \\in (0,1)$, a sufficient condition for the standard deviation $s_G$ is:\n$$\ns_G \\geq \\frac{\\Delta_2(f) \\sqrt{2 \\ln(1.25/\\delta)}}{\\epsilon}\n$$\nTo minimize the added noise, we select the equality:\n$$\ns_G = \\frac{2C}{m\\epsilon} \\sqrt{2 \\ln\\left(\\frac{1.25}{\\delta}\\right)}\n$$\n\n**2. Laplace Mechanism and $L_1$ Sensitivity**\n\nThe Laplace mechanism, when applied to a vector-valued query by adding independent noise to each coordinate, is calibrated using the $L_1$ global sensitivity of the query function, defined as $\\Delta_1(f) = \\max_{D, D'} \\|f(D) - f(D')\\|_1$.\nThe density of the added noise vector $Y=(Y_1, \\ldots, Y_d)$ with i.i.d. components $Y_j \\sim \\text{Laplace}(b_L)$ is proportional to $\\exp(-\\|y\\|_1/b_L)$. To achieve $\\epsilon$-DP, the scale parameter $b_L$ must satisfy $b_L \\ge \\Delta_1(f) / \\epsilon$.\n\nWe proceed to compute $\\Delta_1(f)$:\n$$\n\\|f(D) - f(D')\\|_1 = \\left\\|\\frac{1}{m}(g_i - g'_i)\\right\\|_1 = \\frac{1}{m}\\|g_i - g'_i\\|_1\n$$\nWe must maximize $\\|g_i - g'_i\\|_1$ subject to $\\|g_i\\|_2 \\leq C$ and $\\|g'_i\\|_2 \\leq C$. Let $v = g_i - g'_i$. By the triangle inequality, $\\|v\\|_2 = \\|g_i - g'_i\\|_2 \\leq \\|g_i\\|_2 + \\|g'_i\\|_2 \\leq 2C$. We seek to maximize $\\|v\\|_1$ subject to the constraint $\\|v\\|_2 \\leq 2C$.\n\nUsing the Cauchy-Schwarz inequality, for any vector $v \\in \\mathbb{R}^d$:\n$$\n\\|v\\|_1 = \\sum_{j=1}^{d} |v_j| = \\sum_{j=1}^{d} 1 \\cdot |v_j| \\leq \\left(\\sum_{j=1}^{d} 1^2\\right)^{1/2} \\left(\\sum_{j=1}^{d} v_j^2\\right)^{1/2} = \\sqrt{d} \\|v\\|_2\n$$\nThus, $\\|v\\|_1 \\leq \\sqrt{d}(2C)$. This upper bound is tight. It is achieved when $|v_1| = |v_2| = \\ldots = |v_d|$, for example, if $v_j = \\frac{2C}{\\sqrt{d}}$ for all $j$. For such a vector $v$, $\\|v\\|_2 = \\sqrt{\\sum_{j=1}^d (\\frac{2C}{\\sqrt{d}})^2} = \\sqrt{d \\cdot \\frac{4C^2}{d}} = 2C$. We can construct $g_i = v/2$ and $g'_i = -v/2$. Then $\\|g_i\\|_2 = \\|v/2\\|_2 = C$ and $\\|g'_i\\|_2 = \\|-v/2\\|_2 = C$, and $g_i - g'_i = v$. Hence, $\\max \\|g_i - g'_i\\|_1 = 2C\\sqrt{d}$.\n\nThe $L_1$ global sensitivity of the averaging query is:\n$$\n\\Delta_1(f) = \\frac{2C\\sqrt{d}}{m}\n$$\nThe per-coordinate Laplace scale $b_L$ required to achieve $\\epsilon$-DP is given by:\n$$\nb_L = \\frac{\\Delta_1(f)}{\\epsilon} = \\frac{2C\\sqrt{d}}{m\\epsilon}\n$$\n\n**Numerical Computation**\n\nWe are given the following parameters:\n- Gradient dimension: $d = 100$\n- Number of clients: $m = 50$\n- Clipping bound: $C = 0.5$\n- Gaussian DP parameters: $\\epsilon = 0.8$, $\\delta = 1.0 \\times 10^{-6}$\n- Laplace DP parameter: $\\epsilon = 0.8$\n\n**For the Gaussian mechanism standard deviation $s_G$:**\n$$\ns_G = \\frac{2(0.5)}{50(0.8)} \\sqrt{2 \\ln\\left(\\frac{1.25}{1.0 \\times 10^{-6}}\\right)} = \\frac{1}{40} \\sqrt{2 \\ln(1.25 \\times 10^6)}\n$$\n$$\ns_G = 0.025 \\sqrt{2(14.038681)} = 0.025 \\sqrt{28.077362} \\approx 0.025 \\times 5.2988076 \\approx 0.13247019\n$$\nRounding to four significant figures, $s_G \\approx 0.1325$.\n\n**For the Laplace mechanism scale $b_L$:**\n$$\nb_L = \\frac{2(0.5)\\sqrt{100}}{50(0.8)} = \\frac{1 \\times 10}{40} = \\frac{10}{40} = 0.25\n$$\nThe value is exact. To four significant figures, $b_L = 0.2500$.\n\nThe computed values are $(s_G, b_L) = (0.1325, 0.2500)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.1325 & 0.2500 \\end{pmatrix}}\n$$", "id": "4341061"}]}