{"hands_on_practices": [{"introduction": "The foundation of any robust propensity score analysis is a well-specified model for the propensity score, $e(X)$. A common pitfall is to approach this as a standard prediction task, aiming to maximize the model's ability to predict treatment assignment. This exercise challenges that notion, guiding you to consider why the true goal is achieving covariate balance between treatment groups. By evaluating different model-building strategies, you will learn to distinguish between principled, balance-focused approaches and misguided, prediction-focused ones, a critical skill for any practitioner [@problem_id:4980943].", "problem": "In a large observational cohort study of antihypertensive therapy, let $i \\in \\{1,\\dots,n\\}$ index patients, $A_i \\in \\{0,1\\}$ denote receipt of the therapy, $X_i \\in \\mathbb{R}^p$ denote pre-treatment covariates (including continuous variables such as age, systolic blood pressure, and body mass index, and categorical variables such as sex and hospital), and $Y_i$ denote a clinical outcome measured after treatment initiation. Suppose the causal estimand is the average treatment effect, and the following standard causal identifiability conditions are assumed: consistency, exchangeability (i.e., $Y_i(a) \\perp A_i \\mid X_i$ for $a \\in \\{0,1\\}$), and positivity (i.e., $0  \\Pr(A_i=1 \\mid X_i=x)  1$ for all $x$ in the support of $X_i$). Define the propensity score $e(X) = \\Pr(A=1 \\mid X)$ and consider estimating $e(X)$ by a logistic regression model\n$$\n\\mathrm{logit}(e(X)) = \\beta_0 + \\beta^\\top f(X),\n$$\nwhere $f(X)$ is a prespecified vector of basis functions of the original covariates $X$ (e.g., transformations, restricted cubic splines for continuous covariates, and selected interaction terms). Inverse probability of treatment weighting (IPTW) with weights $w_i = \\dfrac{A_i}{\\hat{e}(X_i)} + \\dfrac{1-A_i}{1 - \\hat{e}(X_i)}$ and propensity score stratification by bins of $\\hat{e}(X)$ are planned for covariate adjustment.\n\nStarting from the definition of the propensity score and the properties of IPTW and stratification under the identifiability conditions above, reason about how the choice of $f(X)$ should be made to target covariate balance between treated and untreated groups, rather than merely maximizing predictive fit of $A$ given $X$. Which of the following model-building and selection strategies for $f(X)$ are principled ways to target covariate balance?\n\nSelect all that apply.\n\nA. Choose $f(X)$ to maximize the area under the receiver operating characteristic curve (AUC) for classifying $A$ using standard $K$-fold cross-validation on predictive log-likelihood or misclassification error, without reference to post-weighting imbalance.\n\nB. Specify $f(X)$ to include restricted cubic splines for continuous covariates with knots at empirical quantiles and clinically motivated interactions, and perform $K$-fold cross-validation in which the loss is the mean absolute standardized difference of covariates between $A=1$ and $A=0$ groups after IPTW using $\\hat{e}(X)$; select the $f(X)$ that minimizes this imbalance-focused loss.\n\nC. Use the Covariate Balancing Propensity Score (CBPS) approach, which estimates $\\beta$ by solving estimating equations that augment the logistic score with moment conditions enforcing equality of weighted covariate means between $A=1$ and $A=0$; choose $f(X)$ rich enough (e.g., splines and interactions) to allow these balance constraints to be satisfied.\n\nD. Construct a Super Learner ensemble for $\\hat{e}(X)$ over a library of flexible learners (e.g., penalized logistic regression with splines, gradient boosting machines, and generalized additive models), and perform $V$-fold cross-validation with a loss defined as the maximum standardized difference across covariates after stratifying subjects into deciles of $\\hat{e}(X)$; select the ensemble and its $f(X)$ components that minimize this stratification-based imbalance.\n\nE. Select $f(X)$ by minimizing the Bayesian Information Criterion (BIC) in the logistic regression for $A$ and then discard subjects with stabilized IPTW weights exceeding $10$ to improve overlap; accept the chosen $f(X)$ because trimming reduces imbalance and extreme weights even if predictive criteria alone were used.", "solution": "The problem statement is evaluated as valid. It presents a standard scenario in causal inference using observational data and asks a well-posed, objective, and scientifically grounded question about the principles of propensity score model specification. The concepts and terminology used are standard in the field of statistics and epidemiology.\n\nThe core principle of propensity score methods, such as Inverse Probability of Treatment Weighting (IPTW) and stratification, is to achieve covariate balance between the treated and untreated groups. The propensity score, $e(X) = \\Pr(A=1 \\mid X)$, is a balancing score, meaning that conditional on the true $e(X)$, the distribution of covariates $X$ is independent of the treatment status $A$. That is, $X \\perp A \\mid e(X)$.\n\nIn practice, the true propensity score is unknown and must be estimated, typically via a model, yielding $\\hat{e}(X)$. The utility of $\\hat{e}(X)$ for causal inference depends not on how well it predicts the treatment $A$, but on how well it balances the measured covariates. A model for $e(X)$ that is an excellent predictor of $A$ can lead to estimated propensity scores near $0$ or $1$, which violates the positivity assumption in practice, results in extreme weights for IPTW, and yields unstable and biased treatment effect estimates. Therefore, principled model-building strategies for the propensity score must prioritize the goal of achieving covariate balance. This is often described as a shift in focus from \"prediction\" to \"balance\".\n\nWe will now evaluate each proposed strategy based on this principle.\n\nA. Choose $f(X)$ to maximize the area under the receiver operating characteristic curve (AUC) for classifying $A$ using standard $K$-fold cross-validation on predictive log-likelihood or misclassification error, without reference to post-weighting imbalance.\n\nThis strategy is explicitly focused on maximizing the predictive performance of the propensity score model for treatment assignment $A$. Metrics like AUC, log-likelihood, and misclassification error are measures of discrimination or calibration for the outcome $A$. As established, the primary goal of propensity score estimation for causal inference is not to predict the treatment but to balance covariates. A model that is \"too good\" at predicting treatment can result in poor overlap (practical violations of positivity) and extreme weights, leading to increased variance and potential bias in the effect estimate. This approach completely ignores the direct assessment of covariate balance, which is the key diagnostic for a useful propensity score model. The modern consensus in causal inference methodology strongly advises against using prediction-oriented metrics for propensity score model selection.\n\nVerdict: **Incorrect**.\n\nB. Specify $f(X)$ to include restricted cubic splines for continuous covariates with knots at empirical quantiles and clinically motivated interactions, and perform $K$-fold cross-validation in which the loss is the mean absolute standardized difference of covariates between $A=1$ and $A=0$ groups after IPTW using $\\hat{e}(X)$; select the $f(X)$ that minimizes this imbalance-focused loss.\n\nThis strategy directly operationalizes the principle of targeting covariate balance. It begins by proposing a flexible model structure ($f(X)$ with splines and interactions) capable of capturing complex relationships. Crucially, the model selection criterion used in the cross-validation procedure is a direct measure of post-weighting covariate imbalance, the mean absolute standardized difference (ASMD). The ASMD for a single covariate $X_j$ is given by\n$$\n\\left| \\frac{\\sum_{i:A_i=1} w_i X_{ij}}{\\sum_{i:A_i=1} w_i} - \\frac{\\sum_{i:A_i=0} w_i X_{ij}}{\\sum_{i:A_i=0} w_i} \\right| / \\sqrt{\\frac{s_{j,1}^2 + s_{j,0}^2}{2}},\n$$\nwhere $s_{j,1}^2$ and $s_{j,0}^2$ are the unweighted sample variances in the treated and untreated groups. By selecting the model specification that minimizes this balance-focused loss function, the procedure is explicitly and directly optimized for the property required for valid causal estimation. This is a sound and principled approach.\n\nVerdict: **Correct**.\n\nC. Use the Covariate Balancing Propensity Score (CBPS) approach, which estimates $\\beta$ by solving estimating equations that augment the logistic score with moment conditions enforcing equality of weighted covariate means between $A=1$ and $A=0$; choose $f(X)$ rich enough (e.g., splines and interactions) to allow these balance constraints to be satisfied.\n\nThe Covariate Balancing Propensity Score (CBPS) is a method specifically designed to bridge the gap between propensity score model fitting and covariate balancing. Standard two-step approaches first estimate the propensity score parameters $\\beta$ by maximizing the likelihood of the treatment assignment model, and then check for balance. CBPS, in contrast, estimates $\\beta$ by solving a set of estimating equations that simultaneously includes the score from the logistic regression model (related to prediction) and a set of moment conditions that directly enforce covariate balance. For a set of balancing covariates $g(X)$, these moment conditions are typically of the form\n$$\n\\sum_{i=1}^n \\left( \\frac{A_i g(X_i)}{\\hat{e}(X_i)} - \\frac{(1-A_i) g(X_i)}{1-\\hat{e}(X_i)} \\right) = 0.\n$$\nBy building the balancing criterion directly into the parameter estimation step, CBPS ensures that the resulting propensity scores are optimized for balance. Choosing a sufficiently rich basis $f(X)$ allows these constraints to be met for a wide range of covariate functions. This represents a highly principled, integrated approach to achieving covariate balance.\n\nVerdict: **Correct**.\n\nD. Construct a Super Learner ensemble for $\\hat{e}(X)$ over a library of flexible learners (e.g., penalized logistic regression with splines, gradient boosting machines, and generalized additive models), and perform $V$-fold cross-validation with a loss defined as the maximum standardized difference across covariates after stratifying subjects into deciles of $\\hat{e}(X)$; select the ensemble and its $f(X)$ components that minimize this stratification-based imbalance.\n\nThis strategy is a sophisticated application of the same core principle as in option B. It uses Super Learner, an ensemble machine learning method, to create a flexible estimate of the propensity score from a library of candidate algorithms. This mitigates the risk of misspecifying a single parametric model. The critical feature is the choice of the loss function for the cross-validation that determines the optimal combination of learners in the ensemble. The loss is defined as a metric of covariate imbalanceâ€”in this case, the maximum standardized difference across covariates within strata defined by $\\hat{e}(X)$. Just like the ASMD in option B, this is a direct measure of how well the estimated propensity score is performing its function of balancing covariates. By optimizing the ensemble to minimize this imbalance metric, the procedure is principled and focused on the correct goal.\n\nVerdict: **Correct**.\n\nE. Select $f(X)$ by minimizing the Bayesian Information Criterion (BIC) in the logistic regression for $A$ and then discard subjects with stabilized IPTW weights exceeding $10$ to improve overlap; accept the chosen $f(X)$ because trimming reduces imbalance and extreme weights even if predictive criteria alone were used.\n\nThis strategy is flawed. The initial model selection for $f(X)$ is based on minimizing BIC. Like AUC and AIC, BIC is a criterion for selecting a good predictive model for the outcome $A$, balancing fit against model complexity. It does not target covariate balance. This is the same fundamental error as in option A. The second step, trimming subjects with large weights (e.g., stabilized weight $ 10$), is a post-hoc intervention to deal with the consequences of a potentially poor propensity score model (i.e., practical positivity violations). While trimming can sometimes improve the stability of the estimator, it is not a substitute for building the model correctly in the first place. Relying on trimming to fix a model that was selected based on the wrong criteria is an ad-hoc and unprincipled approach. Furthermore, trimming changes the estimand by restricting the analysis to a subset of the original population, which can introduce its own biases. The core model-building step is not directed at achieving balance.\n\nVerdict: **Incorrect**.", "answer": "$$\\boxed{BCD}$$", "id": "4980943"}, {"introduction": "After fitting a propensity score model, the essential next step is to verify its performance by checking for covariate balance. This is not optional; it is the primary diagnostic that validates the entire analysis. This practice guides you through the derivation of the inverse probability of treatment weighted (IPTW) standardized mean difference, a cornerstone metric for assessing balance. Working through this derivation solidifies the concept of how weighting creates a pseudo-population and provides the analytical tool to check if confounding has been successfully addressed [@problem_id:4980935].", "problem": "An observational cohort study evaluates the effect of a binary treatment on a clinical outcome. For each individual $i \\in \\{1,\\dots,n\\}$, let $A_i \\in \\{0,1\\}$ denote the observed treatment assignment ($A_i=1$ for treated, $A_i=0$ for control), and let $X_{ij}$ denote the $j$-th baseline covariate. Assume a correctly specified propensity score model yields an estimated propensity score $e_i = \\Pr(A_i=1 \\mid X_i)$ for each individual, where $X_i$ is the vector of baseline covariates. Consider the inverse probability of treatment weighting (IPTW) construction of the pseudo-population using the unstabilized weights given by $w_i^{(1)} = A_i / e_i$ for the treated and $w_i^{(0)} = (1-A_i)/(1-e_i)$ for the controls. \n\nUse the following fundamental bases:\n- The propensity score is the conditional probability $e_i = \\Pr(A_i=1 \\mid X_i)$, and inverse probability of treatment weighting reweights individuals by the inverse of their probability of receiving the treatment actually observed.\n- For any nonnegative weights $\\{v_i\\}_{i=1}^n$, the weighted mean of a scalar quantity $Z_i$ is $\\sum_{i=1}^n v_i Z_i \\big/ \\sum_{i=1}^n v_i$, and the corresponding within-weight weighted variance is $\\sum_{i=1}^n v_i (Z_i - \\bar{Z}_v)^2 \\big/ \\sum_{i=1}^n v_i$, where $\\bar{Z}_v = \\sum_{i=1}^n v_i Z_i \\big/ \\sum_{i=1}^n v_i$.\n\nDefine the standardized mean difference for a covariate as the difference between the treated and control group means of $X_{ij}$, divided by the pooled within-group standard deviation, computed using the same weighting scheme within each group. Using inverse probability of treatment weighting within groups as specified above, derive a single, closed-form analytic expression for the IPTW-weighted standardized mean difference of covariate $X_j$ that depends only on $\\{A_i, X_{ij}, e_i\\}_{i=1}^n$ and finite sums over indices. Your final answer must be one expression (not an equation to solve) and must not introduce any undefined intermediate symbols. Express the final answer as an analytic expression; no numerical evaluation is required and no units are involved.", "solution": "The problem requires the derivation of a single, closed-form analytic expression for the inverse probability of treatment weighting (IPTW) weighted standardized mean difference (SMD) of a covariate $X_j$. The derivation must be based on the provided definitions.\n\nThe standardized mean difference for covariate $j$, denoted $SMD_j$, is defined as the difference in the weighted means of the covariate between the treated and control groups, divided by a pooled weighted standard deviation. Symbolically,\n$$SMD_j = \\frac{\\bar{X}_{j,A=1,w} - \\bar{X}_{j,A=0,w}}{s_{p,j,w}}$$\nwhere $\\bar{X}_{j,A=1,w}$ and $\\bar{X}_{j,A=0,w}$ are the weighted means of covariate $X_j$ in the treated ($A=1$) and control ($A=0$) groups, respectively, and $s_{p,j,w}$ is the pooled weighted standard deviation.\n\nWe will derive each of these three components sequentially.\n\n_1. Weighted Mean in the Treated Group ($\\bar{X}_{j,A=1,w}$)_\n\nThe problem specifies the unstabilized IPTW weight for a treated individual $i$ (where $A_i=1$) is $w_i^{(1)} = A_i/e_i = 1/e_i$. The general formula for a weighted mean is given as $\\bar{Z}_v = \\sum_{k=1}^n v_k Z_k / \\sum_{k=1}^n v_k$. To calculate the mean for the treated group, we apply this formula to the subset of individuals with $A_k=1$, using their specified weights. This is equivalent to summing over all individuals from $k=1$ to $n$ and using the term $A_k/e_k$ as the weight, which is non-zero only for the treated.\nLetting $Z_k = X_{kj}$ and $v_k = A_k/e_k$, the weighted mean for the treated group is:\n$$\\bar{X}_{j,A=1,w} = \\frac{\\sum_{k=1}^{n} \\frac{A_k}{e_k} X_{kj}}{\\sum_{k=1}^{n} \\frac{A_k}{e_k}}$$\n\n_2. Weighted Mean in the Control Group ($\\bar{X}_{j,A=0,w}$)_\n\nSimilarly, the weight for a control individual $i$ (where $A_i=0$) is $w_i^{(0)} = (1-A_i)/(1-e_i) = 1/(1-e_i)$. The indicator for being in the control group is $(1-A_k)$. Applying the weighted mean formula with $Z_k = X_{kj}$ and $v_k = (1-A_k)/(1-e_k)$:\n$$\\bar{X}_{j,A=0,w} = \\frac{\\sum_{k=1}^{n} \\frac{1-A_k}{1-e_k} X_{kj}}{\\sum_{k=1}^{n} \\frac{1-A_k}{1-e_k}}$$\n\n_3. Pooled Weighted Standard Deviation ($s_{p,j,w}$)_\n\nThe pooled weighted standard deviation is the square root of the pooled weighted variance, $s_{p,j,w} = \\sqrt{s_{p,j,w}^2}$. The pooled weighted variance is a weighted average of the individual within-group weighted variances. First, we must calculate the variance for each group. The problem provides the formula for a within-weight weighted variance: $\\sigma_v^2 = \\sum_{i=1}^n v_i (Z_i - \\bar{Z}_v)^2 / \\sum_{i=1}^n v_i$.\n\nFor the treated group, $v_i = A_i/e_i$, $Z_i = X_{ij}$, and $\\bar{Z}_v = \\bar{X}_{j,A=1,w}$. The weighted variance is:\n$$s_{j,A=1,w}^2 = \\frac{\\sum_{i=1}^{n} \\frac{A_i}{e_i} (X_{ij} - \\bar{X}_{j,A=1,w})^2}{\\sum_{i=1}^{n} \\frac{A_i}{e_i}}$$\n\nFor the control group, $v_i = (1-A_i)/(1-e_i)$, $Z_i = X_{ij}$, and $\\bar{Z}_v = \\bar{X}_{j,A=0,w}$. The weighted variance is:\n$$s_{j,A=0,w}^2 = \\frac{\\sum_{i=1}^{n} \\frac{1-A_i}{1-e_i} (X_{ij} - \\bar{X}_{j,A=0,w})^2}{\\sum_{i=1}^{n} \\frac{1-A_i}{1-e_i}}$$\n\nThe concept of a pooled variance extends to the weighted case by combining the weighted sum of squared deviations from each group and dividing by the total sum of weights from both groups.\nLet $N_1^w = \\sum_{i=1}^{n} A_i/e_i$ be the sum of weights for the treated group, and $N_0^w = \\sum_{i=1}^{n} (1-A_i)/(1-e_i)$ be the sum of weights for the control group.\nThe weighted sum of squares (WSS) for the treated group is $WSS_1 = N_1^w s_{j,A=1,w}^2 = \\sum_{i=1}^{n} \\frac{A_i}{e_i} (X_{ij} - \\bar{X}_{j,A=1,w})^2$.\nThe WSS for the control group is $WSS_0 = N_0^w s_{j,A=0,w}^2 = \\sum_{i=1}^{n} \\frac{1-A_i}{1-e_i} (X_{ij} - \\bar{X}_{j,A=0,w})^2$.\nThe pooled weighted variance is $s_{p,j,w}^2 = \\frac{WSS_1 + WSS_0}{N_1^w + N_0^w}$. Substituting the expressions:\n$$s_{p,j,w}^2 = \\frac{\\sum_{i=1}^{n} \\frac{A_i}{e_i} (X_{ij} - \\bar{X}_{j,A=1,w})^2 + \\sum_{i=1}^{n} \\frac{1-A_i}{1-e_i} (X_{ij} - \\bar{X}_{j,A=0,w})^2}{\\sum_{i=1}^{n} \\frac{A_i}{e_i} + \\sum_{i=1}^{n} \\frac{1-A_i}{1-e_i}}$$\nThis can be written more compactly as:\n$$s_{p,j,w}^2 = \\frac{\\sum_{i=1}^{n} \\left[ \\frac{A_i}{e_i} (X_{ij} - \\bar{X}_{j,A=1,w})^2 + \\frac{1-A_i}{1-e_i} (X_{ij} - \\bar{X}_{j,A=0,w})^2 \\right]}{\\sum_{i=1}^{n} \\left( \\frac{A_i}{e_i} + \\frac{1-A_i}{1-e_i} \\right)}$$\nThe pooled weighted standard deviation is the square root of this expression.\n\n_4. Assembling the Final Expression_\n\nThe final expression for the IPTW-weighted SMD is obtained by substituting the derived expressions for the means and the pooled standard deviation into the initial formula. The problem requires a single expression without intermediate symbols. Therefore, the expressions for $\\bar{X}_{j,A=1,w}$ and $\\bar{X}_{j,A=0,w}$ must be substituted into the formula for $s_{p,j,w}$ and into the numerator of the $SMD_j$. Distinct summation indices ($i$ and $k$) are used to avoid ambiguity in nested sums.\n\nThe numerator is $\\left(\\frac{\\sum_{k=1}^{n} \\frac{A_k}{e_k} X_{kj}}{\\sum_{k=1}^{n} \\frac{A_k}{e_k}}\\right) - \\left(\\frac{\\sum_{k=1}^{n} \\frac{1-A_k}{1-e_k} X_{kj}}{\\sum_{k=1}^{n} \\frac{1-A_k}{1-e_k}}\\right)$.\n\nThe denominator is $\\sqrt{s_{p,j,w}^2}$, with the full expressions for the means substituted:\n$$\\sqrt{\\frac{\\sum_{i=1}^{n} \\left[ \\frac{A_i}{e_i} \\left(X_{ij} - \\frac{\\sum_{k=1}^{n} \\frac{A_k}{e_k} X_{kj}}{\\sum_{k=1}^{n} \\frac{A_k}{e_k}}\\right)^2 + \\frac{1-A_i}{1-e_i} \\left(X_{ij} - \\frac{\\sum_{k=1}^{n} \\frac{1-A_k}{1-e_k} X_{kj}}{\\sum_{k=1}^{n} \\frac{1-A_k}{1-e_k}}\\right)^2 \\right]}{\\sum_{i=1}^{n} \\left( \\frac{A_i}{e_i} + \\frac{1-A_i}{1-e_i} \\right)}}$$\n\nCombining these gives the final, single, closed-form expression.", "answer": "$$\\boxed{\\frac{\\frac{\\sum_{k=1}^{n} \\frac{A_k}{e_k} X_{kj}}{\\sum_{k=1}^{n} \\frac{A_k}{e_k}} - \\frac{\\sum_{k=1}^{n} \\frac{1-A_k}{1-e_k} X_{kj}}{\\sum_{k=1}^{n} \\frac{1-A_k}{1-e_k}}}{\\sqrt{\\frac{\\sum_{i=1}^{n} \\left[ \\frac{A_i}{e_i} \\left(X_{ij} - \\frac{\\sum_{k=1}^{n} \\frac{A_k}{e_k} X_{kj}}{\\sum_{k=1}^{n} \\frac{A_k}{e_k}}\\right)^2 + \\frac{1-A_i}{1-e_i} \\left(X_{ij} - \\frac{\\sum_{k=1}^{n} \\frac{1-A_k}{1-e_k} X_{kj}}{\\sum_{k=1}^{n} \\frac{1-A_k}{1-e_k}}\\right)^2 \\right]}{\\sum_{i=1}^{n} \\left( \\frac{A_i}{e_i} + \\frac{1-A_i}{1-e_i} \\right)}}}}$$", "id": "4980935"}, {"introduction": "With a propensity score model that successfully balances covariates, we can proceed to estimate the causal treatment effect. This exercise demonstrates propensity score stratification, a method that approximates conditioning on the full set of covariates by grouping subjects into strata of similar propensity scores. You will first derive the stratified estimator from causal principles and then apply it to a hypothetical dataset, bridging the gap between abstract theory and a concrete numerical estimate of the average treatment effect [@problem_id:4980878].", "problem": "Consider an observational study of a new antihypertensive therapy where the outcome is the change in systolic blood pressure at $30$ days, measured in millimeters of mercury (mmHg). Let $A \\in \\{0,1\\}$ denote treatment assignment, $Y(a)$ denote the potential outcome under treatment level $a$, and $X$ denote baseline covariates. The target estimand is the average treatment effect $\\tau = \\mathbb{E}[Y(1) - Y(0)]$. Assume the following are satisfied: the Stable Unit Treatment Value Assumption (SUTVA), consistency, positivity, and conditional exchangeability given $X$ (i.e., $\\{Y(1), Y(0)\\} \\perp A \\mid X$). Let the propensity score be $e(X) = \\Pr(A = 1 \\mid X)$. Suppose patients are partitioned into $K$ strata by the estimated propensity score (e.g., quintiles), forming a discrete stratum indicator $S \\in \\{1,\\dots,K\\}$.\n\nStarting from the above assumptions and the law of total expectation, derive a sample estimator for $\\tau$ expressed as a weighted average of within-stratum differences in observed outcomes, where the weights are proportional to the stratum sizes. Then, using the following data from $K=5$ propensity score strata, compute the numerical value of this estimator. Express your final effect magnitude in mmHg and round your answer to four significant figures.\n\nFor each stratum $k$, you are given the total stratum size $n_k$, the treated count $n_{1k}$, the control count $n_{0k}$, and the empirical mean outcome in the treated and control groups, denoted $\\bar{Y}_{1k}$ and $\\bar{Y}_{0k}$, respectively:\n- Stratum $k=1$: $n_1 = 120$, $n_{1,1} = 70$, $n_{0,1} = 50$, $\\bar{Y}_{1,1} = 8.2$, $\\bar{Y}_{0,1} = 5.1$.\n- Stratum $k=2$: $n_2 = 160$, $n_{1,2} = 90$, $n_{0,2} = 70$, $\\bar{Y}_{1,2} = 7.5$, $\\bar{Y}_{0,2} = 6.0$.\n- Stratum $k=3$: $n_3 = 200$, $n_{1,3} = 110$, $n_{0,3} = 90$, $\\bar{Y}_{1,3} = 6.3$, $\\bar{Y}_{0,3} = 5.8$.\n- Stratum $k=4$: $n_4 = 180$, $n_{1,4} = 95$, $n_{0,4} = 85$, $\\bar{Y}_{1,4} = 5.1$, $\\bar{Y}_{0,4} = 4.9$.\n- Stratum $k=5$: $n_5 = 140$, $n_{1,5} = 75$, $n_{0,5} = 65$, $\\bar{Y}_{1,5} = 4.0$, $\\bar{Y}_{0,5} = 3.7$.\n\nLet $N = \\sum_{k=1}^{5} n_k$ denote the total sample size. Use weights proportional to $n_k$ and compute the stratified estimator value. Your final answer must be a single real number.", "solution": "The target estimand is the average treatment effect $\\tau = \\mathbb{E}[Y(1) - Y(0)]$. Under the stated assumptions, we proceed from first principles.\n\nBy the law of total expectation,\n$$\n\\tau \\;=\\; \\mathbb{E}\\!\\left[\\,Y(1) - Y(0)\\,\\right]\n\\;=\\; \\mathbb{E}\\!\\left[\\,\\mathbb{E}\\!\\left[\\,Y(1) - Y(0) \\mid S\\,\\right]\\,\\right]\n\\;=\\; \\sum_{k=1}^{K} \\Pr(S=k)\\,\\mathbb{E}\\!\\left[\\,Y(1) - Y(0) \\mid S=k\\,\\right].\n$$\nWithin each stratum $S=k$, conditional exchangeability (as induced by stratification on the propensity score) implies $\\{Y(1),Y(0)\\} \\perp A \\mid S=k$ and positivity holds (both treatment arms are represented). Consistency links observed outcomes $Y$ to potential outcomes: if $A=a$, then $Y = Y(a)$. Therefore,\n$$\n\\mathbb{E}\\!\\left[\\,Y(1) \\mid S=k\\,\\right]\n\\;=\\; \\mathbb{E}\\!\\left[\\,\\mathbb{E}\\!\\left[\\,Y(1) \\mid A=1, S=k\\,\\right]\\,\\right]\n\\;=\\; \\mathbb{E}\\!\\left[\\,\\mathbb{E}\\!\\left[\\,Y \\mid A=1, S=k\\,\\right]\\,\\right]\n\\;=\\; \\mathbb{E}\\!\\left[\\,Y \\mid A=1, S=k\\,\\right],\n$$\nand similarly\n$$\n\\mathbb{E}\\!\\left[\\,Y(0) \\mid S=k\\,\\right]\n\\;=\\; \\mathbb{E}\\!\\left[\\,Y \\mid A=0, S=k\\,\\right].\n$$\nHence,\n$$\n\\mathbb{E}\\!\\left[\\,Y(1) - Y(0) \\mid S=k\\,\\right]\n\\;=\\; \\mathbb{E}\\!\\left[\\,Y \\mid A=1, S=k\\,\\right] - \\mathbb{E}\\!\\left[\\,Y \\mid A=0, S=k\\,\\right].\n$$\nSubstituting back,\n$$\n\\tau \\;=\\; \\sum_{k=1}^{K} \\Pr(S=k)\\,\\Big(\\mathbb{E}\\!\\left[\\,Y \\mid A=1, S=k\\,\\right] - \\mathbb{E}\\!\\left[\\,Y \\mid A=0, S=k\\,\\right]\\Big).\n$$\nIn a finite sample, we approximate $\\Pr(S=k)$ by the empirical stratum proportion $n_k/N$, and the conditional expectations by stratum-specific sample means,\n$$\n\\bar{Y}_{1k} \\;=\\; \\frac{1}{n_{1k}} \\sum_{i: A_i=1, S_i=k} Y_i,\n\\qquad\n\\bar{Y}_{0k} \\;=\\; \\frac{1}{n_{0k}} \\sum_{i: A_i=0, S_i=k} Y_i,\n$$\nyielding the stratified estimator with weights proportional to stratum sizes:\n$$\n\\hat{\\tau} \\;=\\; \\sum_{k=1}^{K} \\frac{n_k}{N}\\,\\big(\\bar{Y}_{1k} - \\bar{Y}_{0k}\\big).\n$$\nThese weights are proportional to stratum size and sum to $1$. This estimator can be understood as a discrete approximation to averaging conditional mean differences across the distribution of $X$; as the strata become arbitrarily fine, the approximation connects to inverse probability of treatment weighting (IPTW), where $\\mathbb{E}[Y(1)]$ and $\\mathbb{E}[Y(0)]$ are estimated by weighting observed outcomes by the inverse propensity score and its complement, respectively.\n\nWe now compute the numerical value using the provided data. First, compute the total sample size:\n$$\nN \\;=\\; n_1 + n_2 + n_3 + n_4 + n_5 \\;=\\; 120 + 160 + 200 + 180 + 140 \\;=\\; 800.\n$$\nCompute the stratum weights $w_k = n_k/N$:\n$$\nw_1 = \\frac{120}{800} = 0.15,\\quad\nw_2 = \\frac{160}{800} = 0.20,\\quad\nw_3 = \\frac{200}{800} = 0.25,\\quad\nw_4 = \\frac{180}{800} = 0.225,\\quad\nw_5 = \\frac{140}{800} = 0.175.\n$$\nCompute the within-stratum differences $d_k = \\bar{Y}_{1k} - \\bar{Y}_{0k}$:\n$$\nd_1 = 8.2 - 5.1 = 3.1,\\quad\nd_2 = 7.5 - 6.0 = 1.5,\\quad\nd_3 = 6.3 - 5.8 = 0.5,\\quad\nd_4 = 5.1 - 4.9 = 0.2,\\quad\nd_5 = 4.0 - 3.7 = 0.3.\n$$\nForm the weighted sum:\n\n$$\n\\hat{\\tau} \\;=\\; \\sum_{k=1}^{5} w_k d_k\n\\;=\\; 0.15 \\cdot 3.1 \\;+\\; 0.20 \\cdot 1.5 \\;+\\; 0.25 \\cdot 0.5 \\;+\\; 0.225 \\cdot 0.2 \\;+\\; 0.175 \\cdot 0.3.\n$$\n\nCompute each product:\n\n$$\n0.15 \\cdot 3.1 = 0.465,\\quad\n0.20 \\cdot 1.5 = 0.300,\\quad\n0.25 \\cdot 0.5 = 0.125,\\quad\n0.225 \\cdot 0.2 = 0.045,\\quad\n0.175 \\cdot 0.3 = 0.0525.\n$$\n\nSum them:\n\n$$\n\\hat{\\tau} = 0.465 + 0.300 + 0.125 + 0.045 + 0.0525 = 0.9875.\n$$\n\nThus, the stratified estimator of the average treatment effect on systolic blood pressure reduction is $0.9875$ mmHg. Rounded to four significant figures, it remains $0.9875$ mmHg.", "answer": "$$\\boxed{0.9875}$$", "id": "4980878"}]}