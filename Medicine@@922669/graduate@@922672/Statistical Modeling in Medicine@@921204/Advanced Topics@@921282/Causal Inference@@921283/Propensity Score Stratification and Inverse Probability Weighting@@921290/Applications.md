## Applications and Interdisciplinary Connections

Having established the theoretical foundations of [propensity score](@entry_id:635864) stratification and [inverse probability](@entry_id:196307) weighting (IPTW) in the preceding chapters, we now turn to their application in diverse and complex real-world settings. The true utility of these methods is revealed when they are employed to navigate the challenges inherent in observational data across various scientific disciplines. This chapter will demonstrate how the core principles of [propensity score](@entry_id:635864) analysis are extended and adapted to address specific research questions, from clinical trial emulation in medicine to the analysis of high-dimensional genomic data and complex survey designs. Our focus will be not on re-teaching the mechanics, but on showcasing the versatility and power of these tools in generating robust causal inferences from observational data.

### The Core Application: Emulating Target Trials in Pharmacoepidemiology

Observational studies using large healthcare databases, such as those derived from Electronic Health Records (EHR), are a cornerstone of modern medical research. However, they are fraught with a fundamental challenge known as **confounding by indication**. This bias arises when the clinical factors that prompt a physician to prescribe a treatment are also independent predictors of the patient's outcome. For instance, in an intensive care setting, patients with greater severity of illness are more likely to receive aggressive interventions like early vasopressors, but they are also at a higher baseline risk of mortality regardless of the treatment they receive. Similarly, physicians are more likely to prescribe statins to patients who already have a high underlying risk of cardiovascular events. In both scenarios, a naive comparison between treated and untreated patients would be misleading, as the treated group is systematically sicker or at higher risk from the outset. Formally, this violates the assumption of marginal exchangeability, as the potential outcomes are not independent of treatment assignment ($Y(a) \not\perp A$). The goal of adjustment is to restore exchangeability conditional on a sufficient set of measured covariates $X$, i.e., to achieve $(Y(0),Y(1)) \perp A \mid X$ [@problem_id:5221159] [@problem_id:4612511].

Propensity score methods provide a powerful framework for emulating a randomized controlled trial—a "target trial"—using observational data. A rigorous workflow for applying IPTW in a clinical study involves several critical steps: clearly defining the scientific estimand (e.g., the Average Treatment Effect, ATE), meticulously specifying the emulated trial components such as eligibility criteria and time zero, handling missing covariate data, and then proceeding through a multi-stage process of propensity score estimation, diagnostics, and outcome analysis. The [propensity score](@entry_id:635864) model itself should be developed using only pre-treatment baseline covariates, and flexible machine learning methods are often preferred to capture the complex relationships between patient characteristics and treatment assignment. After estimating the propensity scores, a series of diagnostic checks is essential before proceeding to outcome analysis. These checks ensure that the [propensity score](@entry_id:635864) model has succeeded in its primary goal: balancing the distribution of confounders between treatment groups [@problem_id:4980961].

#### Covariate Balance Assessment

The cornerstone of [propensity score](@entry_id:635864) diagnostics is the assessment of covariate balance. The key question is whether the weighting has created a pseudo-population in which the measured covariates are no longer associated with treatment assignment. While many metrics exist, the **Standardized Mean Difference (SMD)** is the most widely used because it is a unitless measure that is not sensitive to sample size. For a continuous covariate $X$, the unweighted SMD is the difference in the means between the treated and control groups, divided by a [pooled standard deviation](@entry_id:198759). To assess balance after IPTW, one calculates a *weighted* SMD, where the group means and variances are replaced by their weighted counterparts.

Specifically, the weighted mean for a treatment group $t$ is $\bar{x}_{t,w} = (\sum_{i \in t} w_i x_i) / (\sum_{i \in t} w_i)$, and the weighted variance is $s_{t,w}^2 = (\sum_{i \in t} w_i (x_i - \bar{x}_{t,w})^2) / (\sum_{i \in t} w_i)$. The pooled weighted standard deviation can then be formed, for example, by $s_{p,w} = \sqrt{(s_{1,w}^2 + s_{0,w}^2)/2}$. The weighted SMD is then the difference in weighted means divided by this pooled standardizer. A common heuristic for adequate balance is an absolute weighted SMD of less than $0.1$. This assessment must be performed for all measured covariates, not just those included in the propensity score model, to ensure comprehensive balance has been achieved [@problem_id:4980939] [@problem_id:4980925].

#### Weight Stability and Precision

A successful IPTW analysis balances confounders without unduly compromising statistical precision. The distribution of the inverse probability weights is a crucial diagnostic for this trade-off. Propensity scores close to $0$ or $1$ can lead to extremely large weights for a small number of individuals. These extreme weights can make the final effect estimate highly unstable and inflate its variance. To quantify this loss of precision, we can calculate the **Effective Sample Size (ESS)**. The ESS, denoted $n^{\star}$, represents the size of an unweighted sample that would yield an estimator with the same variance as our weighted estimator. It is derived by considering the variance of a weighted mean $\bar{Y}_w = (\sum w_i Y_i) / (\sum w_i)$, which, under the assumption of independent observations with common variance $\sigma^2$, is $\operatorname{Var}(\bar{Y}_w) = \sigma^2 (\sum w_i^2) / (\sum w_i)^2$. By equating this to the variance of an unweighted mean from a sample of size $n^{\star}$, $\sigma^2/n^{\star}$, we arrive at the expression for the [effective sample size](@entry_id:271661):
$$
n^{\star} = \frac{\left(\sum_{i=1}^{N} w_i\right)^2}{\sum_{i=1}^{N} w_i^2}
$$
This formula can also be expressed as $n^{\star} = N / (1 + \text{CV}_w^2)$, where $N$ is the original sample size and $\text{CV}_w$ is the [coefficient of variation](@entry_id:272423) of the weights. This form clearly illustrates that as the variability of the weights increases, the ESS decreases. A substantial drop from the original sample size $N$ to the effective sample size $n^{\star}$ indicates that a few highly weighted individuals are dominating the analysis, and the resulting effect estimate may be unstable. In such cases, strategies like weight truncation (capping weights at a certain percentile) may be considered, though this introduces a [bias-variance trade-off](@entry_id:141977) [@problem_id:4980912].

### Extending Propensity Score Methods to Diverse Data Structures

The utility of propensity score weighting extends far beyond the basic case of a binary treatment and a single post-treatment outcome. The framework can be adapted to handle complex data structures common in medical research, including time-to-event data, longitudinal studies with time-varying exposures, and non-binary treatments.

#### Time-to-Event Outcomes and Informative Censoring

In many studies, the outcome of interest is the time until an event occurs, such as death or disease recurrence. A common complication is that patients may be lost to follow-up, an event known as [right censoring](@entry_id:634946). If the reasons for censoring are related to the outcome (e.g., sicker patients are more likely to drop out of a study), this is termed **informative censoring**, which can introduce selection bias. IPTW can be adapted to handle both confounding of treatment and informative censoring simultaneously. This is achieved by constructing a combined, time-varying weight for each individual, which is the product of two components:
1.  **Inverse Probability of Treatment Weight (IPTW):** A time-fixed weight, typically stabilized, to account for baseline confounding of the treatment assignment.
2.  **Inverse Probability of Censoring Weight (IPCW):** A time-varying weight that, at each time point $t$, accounts for the probability of remaining uncensored up to that time, conditional on past treatment and covariate history.

The final weight for individual $i$ at time $t$ is $w_i(t) = w_i^{\text{IPTW}} \times w_i^{\text{IPCW}}(t)$. By using these combined weights in a weighted Cox proportional hazards model, one can estimate a **marginal hazard ratio**, which represents the population-averaged effect of the treatment, free from the biases of both confounding and informative censoring. This approach allows for the estimation of parameters of a Marginal Structural Cox Model, such as $\lambda^{\mathrm{MSM}}(t \mid A) = \lambda_0(t)\exp\{\beta A\}$, where $\exp(\beta)$ is the target marginal hazard ratio [@problem_id:4980948] [@problem_id:4980921].

#### Longitudinal Data and Time-Dependent Confounding

A particularly challenging scenario in observational research involves time-varying treatments and confounders. Consider a situation where a patient's clinical status at time $t$ (e.g., a lab value $L_t$) influences the treatment decision at that time ($A_t$), but the clinical status $L_t$ was itself affected by a prior treatment ($A_{t-1}$). This creates a feedback loop known as **time-dependent confounding affected by prior treatment**. In this case, standard regression adjustment for $L_t$ is inappropriate, because $L_t$ is both a confounder for the effect of $A_t$ and a mediator on the causal pathway from $A_{t-1}$ to the final outcome. Conditioning on $L_t$ would block part of the causal effect of the prior treatment.

Longitudinal IPTW for Marginal Structural Models (MSMs) was developed specifically to handle this problem. The approach involves calculating time-varying stabilized weights for each patient at each time point. The weight for patient $i$ at time $t$ is the ratio of the probability of receiving their observed treatment $A_{it}$ given only their past treatment history, to the probability of receiving their treatment given their full past treatment and covariate history:
$$
w_{it} = \frac{\Pr(A_{it} \mid \bar{A}_{i,t-1})}{\Pr(A_{it} \mid \bar{A}_{i,t-1}, \bar{X}_{it})}
$$
The final weight for a patient is the cumulative product of these time-specific weights over the follow-up period, $w_i = \prod_{t=1}^T w_{it}$. This weighting scheme creates a pseudo-population in which, at every time point $t$, the treatment $A_t$ is independent of the covariate history $\bar{X}_t$, conditional on the past treatment history $\bar{A}_{t-1}$. This breaks the confounding feedback loop without blocking causal pathways, allowing for consistent estimation of the marginal effect of a sustained treatment strategy in a subsequent weighted outcome model [@problem_id:4980960] [@problem_id:4980947].

#### Complex Exposure Types: Multi-Category and Continuous Treatments

The concept of the propensity score can be generalized beyond binary exposures.

For a treatment with multiple categories (e.g., comparing three different stroke prevention regimens), we define a **Generalized Propensity Score (GPS)** for each treatment level $a$ as the [conditional probability](@entry_id:151013) of receiving that treatment: $p_a(X) = \Pr(A=a \mid X)$. These probabilities are typically estimated using a [multinomial logistic regression](@entry_id:275878) model. The IPW estimator for the marginal mean outcome under treatment $a$, denoted $\mu_a = E[Y^a]$, is then a [simple extension](@entry_id:152948) of the binary case:
$$
\hat{\mu}_a = \frac{1}{n} \sum_{i=1}^n \frac{I(A_i=a)Y_i}{\hat{p}_a(X_i)}
$$
This provides a way to estimate the average outcome for each treatment arm, allowing for [pairwise comparisons](@entry_id:173821) in a population free of measured confounding [@problem_id:4980944].

The GPS framework can be further extended to **continuous exposures**, such as drug dosage. In this context, the GPS, denoted $r(a,X)$, is defined as the [conditional probability density](@entry_id:265457) of the exposure level $A=a$ given covariates $X$, i.e., $r(a,X) = f_{A|X}(a|X)$. This score has a balancing property analogous to the standard [propensity score](@entry_id:635864): for any given exposure level $a$, conditioning on the value of the GPS $r(a,X)$ is sufficient to balance the distribution of covariates $X$. To estimate a [dose-response curve](@entry_id:265216), one can use weighting to create a pseudo-population in which the exposure level $A$ is independent of the covariates $X$. This is achieved using stabilized weights of the form $w(X,A) = f_A(A) / f_{A|X}(A|X)$, where $f_A(A)$ is the [marginal density](@entry_id:276750) of the exposure. This approach allows researchers to estimate the average causal effect of the exposure across its entire continuous range [@problem_id:4980922].

### Interdisciplinary Connections and Advanced Topics

Propensity score methods are not confined to a single discipline but serve as a bridge connecting statistical theory with applied questions in fields like public health, bioinformatics, and economics. Their application often involves navigating specialized methodological challenges.

#### Integration with Survey Sampling

Much of our population health knowledge comes from large national surveys that employ complex sampling designs (e.g., stratification and multi-stage clustering) to ensure [representative sampling](@entry_id:186533). When estimating a causal effect from such data, one must simultaneously account for confounding and the sampling design. A common approach is to construct a **composite weight** by multiplying the survey weight with the IPTW weight: $w_i = w_i^{\text{survey}} \cdot w_i^{\text{IPTW}}$. The survey weight generalizes the findings from the sample to the target finite population, while the IPTW weight adjusts for confounding within that population.

While the point estimate is straightforward, variance estimation is highly complex. It must account for variability from both the sampling design (e.g., correlation of observations within clusters) and the estimation of the propensity score. Standard variance formulas that assume [independent and identically distributed](@entry_id:169067) data are invalid and will typically underestimate the true variance. Two principled approaches are used: (1) **Replication methods**, such as the jackknife or bootstrap, where the entire estimation process (including re-fitting the [propensity score](@entry_id:635864) model and re-calculating composite weights) is repeated for numerous subsamples (replicates) to empirically derive the variance; and (2) **Taylor series linearization**, an analytical method that uses stacked estimating equations for both the causal effect and the [propensity score](@entry_id:635864) parameters to derive an asymptotically valid variance estimate that properly incorporates the design features like strata and clusters [@problem_id:4980883].

#### High-Dimensional Data in Bioinformatics and Genomics

The advent of high-throughput technologies in genomics and bioinformatics presents a "large $p$, small $n$" problem, where the number of potential covariates $p$ (e.g., tens of thousands of gene expression levels) vastly exceeds the number of subjects $n$. In this setting, traditional logistic regression for the [propensity score](@entry_id:635864) is not feasible. The challenge is to select the right variables for the propensity score model from a massive pool. Different strategies exist, but they are not equally valid.

A principled strategy is **outcome-free selection**. This involves modeling the treatment assignment $T$ as a function of the high-dimensional covariates $X$ without using any information from the outcome $Y$. Penalized regression methods like the **LASSO (Least Absolute Shrinkage and Selection Operator)** are well-suited for this task, as they can select a sparse set of predictors from a large number of candidates. In contrast, strategies that pre-screen variables based on their association with the outcome are fundamentally flawed, as they can fail to select true confounders that have weak marginal associations with the outcome, leading to residual bias. Furthermore, it is critical to recognize the role of different variable types. Including strong *instruments* (variables that predict treatment but not outcome) in the [propensity score](@entry_id:635864) model, while not introducing bias, can severely inflate the variance of IPTW estimators by creating extreme weights. A well-designed high-dimensional [propensity score](@entry_id:635864) strategy focuses on identifying true confounders while judiciously handling other variable types [@problem_id:4599481].

#### Fitting Marginal Structural Models (MSMs)

At a more formal level, IPTW is the primary tool for fitting **Marginal Structural Models (MSMs)**. An MSM is a model for the marginal mean of a potential outcome, unconditional on covariates. For example, a logistic MSM might model the marginal log-odds of a binary potential outcome as a linear function of treatment: $\operatorname{logit}\{\Pr(Y(a)=1)\} = \beta_{0} + \beta_{1} a$. The parameter $\beta_1$ is the marginal causal log-odds ratio, a population-averaged measure of effect. This is fundamentally different from the *conditional* effect estimated by a standard multivariable [logistic regression model](@entry_id:637047) of the form $\operatorname{logit}\{\Pr(Y=1 \mid A, X)\}$. Due to the non-collapsibility of the odds ratio, the conditional effect is not equal to the marginal effect. IPTW provides the bridge: by fitting a simple weighted [logistic regression](@entry_id:136386) of the observed outcome $Y$ on the treatment $A$ (with no other covariates) using the IPTW weights, the resulting coefficient for $A$ is a [consistent estimator](@entry_id:266642) of the marginal parameter $\beta_1$ from the MSM [@problem_id:4980890]. This highlights the unique ability of IPTW to target marginal causal effects, which are often the quantities of greatest interest for public health and policy decisions.