{"hands_on_practices": [{"introduction": "Confounding is a central challenge in observational research, where the observed association between an exposure and an outcome may be distorted by a third variable. This exercise uses a simple linear structural model to dissect precisely how a common cause induces bias in an unadjusted effect estimate. By deriving the analytical form of this bias, you will make the abstract concept of confounding tangible and see exactly how statistical adjustment can, in principle, remove it [@problem_id:4954330].", "problem": "Consider an observational cohort study in which a continuous exposure $A$ (e.g., a biomarker level) and a continuous outcome $Y$ (e.g., a clinical severity index) are measured for $n$ patients. Let $U$ denote a patient-level latent factor (e.g., a baseline frailty score) that is a common cause of both $A$ and $Y$. Suppose the data-generating process is given by the linear structural equations\n$$\nA \\;=\\; \\alpha_U\\,U \\;+\\; \\epsilon_A,\\qquad\nY \\;=\\; \\beta_A\\,A \\;+\\; \\beta_U\\,U \\;+\\; \\epsilon_Y,\n$$\nwhere $U$, $\\epsilon_A$, and $\\epsilon_Y$ are independent and identically distributed (i.i.d.) across patients, with $E[U]=E[\\epsilon_A]=E[\\epsilon_Y]=0$, $\\operatorname{Var}(U)=\\sigma_U^2$, $\\operatorname{Var}(\\epsilon_A)=\\sigma_A^2$, and $\\operatorname{Var}(\\epsilon_Y)=\\sigma_Y^2$. Assume further that $U$, $\\epsilon_A$, and $\\epsilon_Y$ are mutually independent for each patient, and that observations are i.i.d. across patients. No other causes of $A$ or $Y$ are present.\n\nDefine the unadjusted estimator of $\\beta_A$ as the ordinary least squares (OLS) slope from the simple linear regression of $Y$ on $A$ (with an intercept), ignoring $U$. Under the stated assumptions and as $n \\to \\infty$, derive from first principles the probability limit of the unadjusted estimator and hence its asymptotic bias, defined as the large-sample limit of the estimator minus the true $\\beta_A$. Then, assuming $U$ were observed, use the same principles to argue whether the OLS coefficient on $A$ from the multiple linear regression of $Y$ on $(A,U)$ is asymptotically biased for $\\beta_A$ under the data-generating process above.\n\nProvide your final answer as a single closed-form analytic expression for the asymptotic bias of the unadjusted estimator in terms of $\\alpha_U$, $\\beta_U$, $\\sigma_U^2$, and $\\sigma_A^2$. Do not include any other symbols in your final expression. No numerical approximation is required; do not round.", "solution": "The problem statement has been critically validated and is deemed to be scientifically grounded, well-posed, and objective. It presents a standard, canonical problem in statistical modeling concerning omitted variable bias (a form of confounding) and is free of any invalidating flaws. The solution process may therefore proceed.\n\nThe problem asks for the asymptotic bias of an unadjusted estimator for a regression coefficient and an argument concerning the bias of an adjusted estimator. We will address these two parts in sequence.\n\nFirst, consider the unadjusted estimator of $\\beta_A$, which is the ordinary least squares (OLS) slope from the simple linear regression of $Y$ on $A$. Let this unadjusted estimator be denoted by $\\hat{\\beta}_{A, \\text{unadj}}$. For a sample of size $n$, the formula for this estimator is:\n$$\n\\hat{\\beta}_{A, \\text{unadj}} = \\frac{\\sum_{i=1}^n (A_i - \\bar{A})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (A_i - \\bar{A})^2} \\equiv \\frac{\\widehat{\\operatorname{Cov}}(A,Y)}{\\widehat{\\operatorname{Var}}(A)}\n$$\nwhere $\\bar{A}$ and $\\bar{Y}$ are the sample means of $A$ and $Y$, respectively, and $\\widehat{\\operatorname{Cov}}$ and $\\widehat{\\operatorname{Var}}$ denote the sample covariance and variance.\n\nTo find the asymptotic bias, we first need to determine the probability limit ($\\text{plim}$) of this estimator as the sample size $n \\to \\infty$. Since the observations $(A_i, Y_i)$ are i.i.d., by the Law of Large Numbers, the sample moments converge in probability to the corresponding population moments. By the continuous mapping theorem (invoking Slutsky's theorem), the ratio of these sample moments converges in probability to the ratio of the population moments:\n$$\n\\operatorname{plim}_{n \\to \\infty} \\hat{\\beta}_{A, \\text{unadj}} = \\frac{\\operatorname{Cov}(A,Y)}{\\operatorname{Var}(A)}\n$$\nWe must now derive expressions for $\\operatorname{Var}(A)$ and $\\operatorname{Cov}(A,Y)$ from first principles, using the given structural equations and statistical assumptions.\n\nThe structural equation for $A$ is $A = \\alpha_U U + \\epsilon_A$. The variables $U$ and $\\epsilon_A$ are assumed to be independent, with variances $\\operatorname{Var}(U) = \\sigma_U^2$ and $\\operatorname{Var}(\\epsilon_A) = \\sigma_A^2$. The variance of $A$ is therefore:\n$$\n\\operatorname{Var}(A) = \\operatorname{Var}(\\alpha_U U + \\epsilon_A)\n$$\nDue to the independence of $U$ and $\\epsilon_A$, the variance of the sum is the sum of the variances:\n$$\n\\operatorname{Var}(A) = \\operatorname{Var}(\\alpha_U U) + \\operatorname{Var}(\\epsilon_A) = \\alpha_U^2 \\operatorname{Var}(U) + \\operatorname{Var}(\\epsilon_A) = \\alpha_U^2 \\sigma_U^2 + \\sigma_A^2\n$$\n\nNext, we derive the covariance between $A$ and $Y$. We start with the structural equation for $Y$, $Y = \\beta_A A + \\beta_U U + \\epsilon_Y$. Using the property of covariance, we have:\n$$\n\\operatorname{Cov}(A,Y) = \\operatorname{Cov}(A, \\beta_A A + \\beta_U U + \\epsilon_Y) = \\beta_A \\operatorname{Cov}(A,A) + \\beta_U \\operatorname{Cov}(A,U) + \\operatorname{Cov}(A,\\epsilon_Y)\n$$\nThis simplifies to:\n$$\n\\operatorname{Cov}(A,Y) = \\beta_A \\operatorname{Var}(A) + \\beta_U \\operatorname{Cov}(A,U) + \\operatorname{Cov}(A,\\epsilon_Y)\n$$\nWe need to evaluate the two covariance terms. For $\\operatorname{Cov}(A,U)$, we substitute the expression for $A$:\n$$\n\\operatorname{Cov}(A,U) = \\operatorname{Cov}(\\alpha_U U + \\epsilon_A, U) = \\alpha_U \\operatorname{Cov}(U,U) + \\operatorname{Cov}(\\epsilon_A, U)\n$$\nSince $U$ and $\\epsilon_A$ are independent, $\\operatorname{Cov}(\\epsilon_A, U) = 0$. Thus:\n$$\n\\operatorname{Cov}(A,U) = \\alpha_U \\operatorname{Var}(U) = \\alpha_U \\sigma_U^2\n$$\nFor $\\operatorname{Cov}(A,\\epsilon_Y)$, we again substitute the expression for $A$:\n$$\n\\operatorname{Cov}(A,\\epsilon_Y) = \\operatorname{Cov}(\\alpha_U U + \\epsilon_A, \\epsilon_Y) = \\alpha_U \\operatorname{Cov}(U, \\epsilon_Y) + \\operatorname{Cov}(\\epsilon_A, \\epsilon_Y)\n$$\nBy assumption, $U$, $\\epsilon_A$, and $\\epsilon_Y$ are mutually independent. This implies $\\operatorname{Cov}(U, \\epsilon_Y) = 0$ and $\\operatorname{Cov}(\\epsilon_A, \\epsilon_Y) = 0$. Therefore:\n$$\n\\operatorname{Cov}(A,\\epsilon_Y) = 0\n$$\nSubstituting these covariance results back into the expression for $\\operatorname{Cov}(A,Y)$:\n$$\n\\operatorname{Cov}(A,Y) = \\beta_A \\operatorname{Var}(A) + \\beta_U (\\alpha_U \\sigma_U^2) + 0 = \\beta_A (\\alpha_U^2 \\sigma_U^2 + \\sigma_A^2) + \\alpha_U \\beta_U \\sigma_U^2\n$$\n\nNow we can find the probability limit of the unadjusted estimator:\n$$\n\\operatorname{plim}_{n \\to \\infty} \\hat{\\beta}_{A, \\text{unadj}} = \\frac{\\operatorname{Cov}(A,Y)}{\\operatorname{Var}(A)} = \\frac{\\beta_A (\\alpha_U^2 \\sigma_U^2 + \\sigma_A^2) + \\alpha_U \\beta_U \\sigma_U^2}{\\alpha_U^2 \\sigma_U^2 + \\sigma_A^2}\n$$\nSeparating the terms gives:\n$$\n\\operatorname{plim}_{n \\to \\infty} \\hat{\\beta}_{A, \\text{unadj}} = \\frac{\\beta_A (\\alpha_U^2 \\sigma_U^2 + \\sigma_A^2)}{\\alpha_U^2 \\sigma_U^2 + \\sigma_A^2} + \\frac{\\alpha_U \\beta_U \\sigma_U^2}{\\alpha_U^2 \\sigma_U^2 + \\sigma_A^2} = \\beta_A + \\frac{\\alpha_U \\beta_U \\sigma_U^2}{\\alpha_U^2 \\sigma_U^2 + \\sigma_A^2}\n$$\nThe asymptotic bias is defined as the probability limit of the estimator minus the true parameter value, $\\beta_A$.\n$$\n\\text{Asymptotic Bias} = \\left( \\operatorname{plim}_{n \\to \\infty} \\hat{\\beta}_{A, \\text{unadj}} \\right) - \\beta_A = \\frac{\\alpha_U \\beta_U \\sigma_U^2}{\\alpha_U^2 \\sigma_U^2 + \\sigma_A^2}\n$$\nThis bias arises because the unobserved confounder $U$ induces a correlation between the regressor $A$ and the error term of the simple regression model. The error term in the naive model $Y = \\beta_0 + \\beta_A A + \\text{error}$ is $\\beta_U U + \\epsilon_Y$. The regressor $A = \\alpha_U U + \\epsilon_A$ is correlated with this error term through the common factor $U$, violating the fundamental OLS assumption of exogeneity. The bias is non-zero if and only if $\\alpha_U \\neq 0$ (the confounder affects the exposure) and $\\beta_U \\neq 0$ (the confounder affects the outcome).\n\nSecond, consider the multiple linear regression of $Y$ on both $A$ and $U$, assuming $U$ were observed. The model being fitted is:\n$$\nY = \\gamma_0 + \\gamma_A A + \\gamma_U U + \\nu\n$$\nThe OLS estimators for $\\gamma_A$ and $\\gamma_U$ are consistent for the true parameters if the regressors are uncorrelated with the error term $\\nu$ in the population model. The true data-generating process is given by the structural equation:\n$$\nY = \\beta_A A + \\beta_U U + \\epsilon_Y\n$$\nWhen we fit the multiple regression model, we are attempting to estimate the parameters of this true structural relationship. By comparing the regression model to the true data-generating process, we see that the true coefficients are $\\beta_A$ and $\\beta_U$ (with a true intercept of $0$), and the error term $\\nu$ corresponds to $\\epsilon_Y$.\n\nFor the OLS estimator of the coefficient on $A$, let us call it $\\hat{\\beta}_{A, \\text{adj}}$, to be consistent for the true parameter $\\beta_A$, the regressors in the model ($A$ and $U$) must be uncorrelated with the error term ($\\epsilon_Y$). We verify these conditions:\n$1.$ $\\operatorname{Cov}(A, \\epsilon_Y)$: As derived earlier, $\\operatorname{Cov}(A, \\epsilon_Y) = \\operatorname{Cov}(\\alpha_U U + \\epsilon_A, \\epsilon_Y) = \\alpha_U \\operatorname{Cov}(U, \\epsilon_Y) + \\operatorname{Cov}(\\epsilon_A, \\epsilon_Y) = 0$, due to the mutual independence of $U$, $\\epsilon_A$, and $\\epsilon_Y$.\n$2.$ $\\operatorname{Cov}(U, \\epsilon_Y)$: This covariance is $0$ directly from the assumption that $U$ and $\\epsilon_Y$ are independent.\n\nSince all regressors in the multiple regression model are uncorrelated with the error term, the fundamental condition for OLS consistency is satisfied. Therefore, the probability limit of the OLS estimator for the coefficient on $A$ from the multiple regression of $Y$ on $(A, U)$ is the true parameter $\\beta_A$.\n$$\n\\operatorname{plim}_{n \\to \\infty} \\hat{\\beta}_{A, \\text{adj}} = \\beta_A\n$$\nThe asymptotic bias of this adjusted estimator is $\\beta_A - \\beta_A = 0$. Thus, a correctly specified multiple regression model that includes the confounder $U$ provides an asymptotically unbiased estimate of the causal effect $\\beta_A$.\n\nThe final answer required is the asymptotic bias of the unadjusted estimator.", "answer": "$$\n\\boxed{\\frac{\\alpha_U \\beta_U \\sigma_U^2}{\\alpha_U^2 \\sigma_U^2 + \\sigma_A^2}}\n$$", "id": "4954330"}, {"introduction": "Beyond confounding by common causes, bias can also arise when we condition on a common *effect* of the exposure and other outcome causes, a phenomenon known as collider bias. This practice provides a stark, quantitative example of how restricting an analysis to a selected subpopulation can create a spurious association that reverses the true effect's direction [@problem_id:4954359]. Working through the probabilistic calculations is a critical lesson in recognizing and avoiding this common pitfall in studies with non-random selection.", "problem": "Consider a binary exposure $X \\in \\{0,1\\}$, a binary unmeasured cause of the outcome $U \\in \\{0,1\\}$, a binary outcome $Y \\in \\{0,1\\}$, and a binary selection indicator $S \\in \\{0,1\\}$ representing entry into a clinical dataset (for example, emergency admission). Suppose $S$ is a collider that is a common effect of $X$ and $U$. Assume that, in the source population, $X$ and $U$ are independent. You are given the following data-generating mechanisms:\n- Marginal distributions: $P(X=1)=0.5$ and $P(U=1)=0.2$.\n- Outcome mechanism (risk model): \n  - $P(Y=1 \\mid X=0, U=0)=0.05$, \n  - $P(Y=1 \\mid X=1, U=0)=0.20$, \n  - $P(Y=1 \\mid X=0, U=1)=0.90$, \n  - $P(Y=1 \\mid X=1, U=1)=0.95$.\n- Selection mechanism (collider): \n  - $P(S=1 \\mid X=0, U=0)=0.01$, \n  - $P(S=1 \\mid X=0, U=1)=0.99$, \n  - $P(S=1 \\mid X=1, U=0)=0.50$, \n  - $P(S=1 \\mid X=1, U=1)=0.99$.\n\nUsing only the axioms of probability, the law of total probability, Bayes rule, and the definitions of odds, odds ratio, and conditional probability, derive the conditional odds ratio for $Y$ comparing $X=1$ to $X=0$ among the selected individuals, that is, compute\n$$\n\\text{OR}_{Y,X \\mid S=1} \\equiv \\frac{\\displaystyle \\frac{P(Y=1 \\mid X=1, S=1)}{P(Y=0 \\mid X=1, S=1)}}{\\displaystyle \\frac{P(Y=1 \\mid X=0, S=1)}{P(Y=0 \\mid X=0, S=1)}}.\n$$\nTo justify that conditioning on the collider $S$ can reverse the estimated effect direction, you may verify within your derivation that the marginal odds ratio $\\text{OR}_{Y,X}$ (without conditioning on $S$) is greater than $1$, while the conditional odds ratio $\\text{OR}_{Y,X \\mid S=1}$ is less than $1$. However, the final numerical answer you must report is the value of $\\text{OR}_{Y,X \\mid S=1}$.\n\nRound your final reported value of $\\text{OR}_{Y,X \\mid S=1}$ to four significant figures. No units are required.", "solution": "The problem is valid as it presents a well-posed, scientifically grounded scenario in statistical modeling, specifically demonstrating the phenomenon of collider stratification bias. All required data and definitions are provided, and the problem is free of contradictions or ambiguities.\n\nThe objective is to compute the conditional odds ratio of the outcome $Y$ for the exposure $X$, given selection into the dataset, $\\text{OR}_{Y,X \\mid S=1}$. This quantity is defined as:\n$$\n\\text{OR}_{Y,X \\mid S=1} = \\frac{\\text{Odds}(Y=1 \\mid X=1, S=1)}{\\text{Odds}(Y=1 \\mid X=0, S=1)} = \\frac{\\displaystyle \\frac{P(Y=1 \\mid X=1, S=1)}{P(Y=0 \\mid X=1, S=1)}}{\\displaystyle \\frac{P(Y=1 \\mid X=0, S=1)}{P(Y=0 \\mid X=0, S=1)}}\n$$\nThe core task is to determine the conditional probabilities $P(Y=1 \\mid X=x, S=1)$ for $x \\in \\{0, 1\\}$. Since the outcome $Y$ is causally determined by $X$ and the unmeasured variable $U$, but not directly by the selection variable $S$, we can state that $Y$ is conditionally independent of $S$ given $X$ and $U$. This is a standard assumption in such causal models, written as $Y \\perp S \\mid (X, U)$. We can therefore apply the law of total probability, marginalizing over the unmeasured cause $U$:\n$$\nP(Y=1 \\mid X=x, S=1) = \\sum_{u \\in \\{0,1\\}} P(Y=1 \\mid X=x, S=1, U=u) \\, P(U=u \\mid X=x, S=1)\n$$\nUsing the conditional independence assumption $P(Y=1 \\mid X=x, S=1, U=u) = P(Y=1 \\mid X=x, U=u)$, the expression simplifies to:\n$$\nP(Y=1 \\mid X=x, S=1) = \\sum_{u \\in \\{0,1\\}} P(Y=1 \\mid X=x, U=u) \\, P(U=u \\mid X=x, S=1)\n$$\nThe terms $P(Y=1 \\mid X=x, U=u)$ are given. We must compute $P(U=u \\mid X=x, S=1)$. This term represents the probability of the unmeasured cause $U$ within the stratum of selected individuals, which is not the same as the marginal probability $P(U=u)$ due to the collider effect. We use Bayes' rule:\n$$\nP(U=u \\mid X=x, S=1) = \\frac{P(S=1 \\mid X=x, U=u) \\, P(U=u \\mid X=x)}{P(S=1 \\mid X=x)}\n$$\nThe problem states that $X$ and $U$ are independent in the source population, so $P(U=u \\mid X=x) = P(U=u)$. The denominator is found by marginalizing over $U$:\n$$\nP(S=1 \\mid X=x) = \\sum_{u \\in \\{0,1\\}} P(S=1 \\mid X=x, U=u) \\, P(U=u)\n$$\nWe proceed with the calculations based on the given probabilities: $P(X=1)=0.5$, $P(U=1)=0.2$ (so $P(U=0)=0.8$).\n\nFirst, we compute $P(S=1 \\mid X=x)$ for $x \\in \\{0, 1\\}$.\nFor $x=0$:\n$$\nP(S=1 \\mid X=0) = P(S=1 \\mid X=0, U=0)P(U=0) + P(S=1 \\mid X=0, U=1)P(U=1)\n$$\n$$\nP(S=1 \\mid X=0) = (0.01)(0.8) + (0.99)(0.2) = 0.008 + 0.198 = 0.206\n$$\nFor $x=1$:\n$$\nP(S=1 \\mid X=1) = P(S=1 \\mid X=1, U=0)P(U=0) + P(S=1 \\mid X=1, U=1)P(U=1)\n$$\n$$\nP(S=1 \\mid X=1) = (0.50)(0.8) + (0.99)(0.2) = 0.400 + 0.198 = 0.598\n$$\nNext, we compute $P(U=u \\mid X=x, S=1)$.\nFor $x=0$:\n$$\nP(U=0 \\mid X=0, S=1) = \\frac{P(S=1 \\mid X=0, U=0)P(U=0)}{P(S=1 \\mid X=0)} = \\frac{(0.01)(0.8)}{0.206} = \\frac{0.008}{0.206} = \\frac{4}{103}\n$$\n$$\nP(U=1 \\mid X=0, S=1) = \\frac{P(S=1 \\mid X=0, U=1)P(U=1)}{P(S=1 \\mid X=0)} = \\frac{(0.99)(0.2)}{0.206} = \\frac{0.198}{0.206} = \\frac{99}{103}\n$$\nFor $x=1$:\n$$\nP(U=0 \\mid X=1, S=1) = \\frac{P(S=1 \\mid X=1, U=0)P(U=0)}{P(S=1 \\mid X=1)} = \\frac{(0.50)(0.8)}{0.598} = \\frac{0.4}{0.598} = \\frac{200}{299}\n$$\n$$\nP(U=1 \\mid X=1, S=1) = \\frac{P(S=1 \\mid X=1, U=1)P(U=1)}{P(S=1 \\mid X=1)} = \\frac{(0.99)(0.2)}{0.598} = \\frac{0.198}{0.598} = \\frac{99}{299}\n$$\nNow we compute $P(Y=1 \\mid X=x, S=1)$.\nFor $x=0$:\n$$\nP(Y=1 \\mid X=0, S=1) = P(Y=1 \\mid X=0, U=0)P(U=0 \\mid X=0, S=1) + P(Y=1 \\mid X=0, U=1)P(U=1 \\mid X=0, S=1)\n$$\n$$\nP(Y=1 \\mid X=0, S=1) = (0.05)\\left(\\frac{4}{103}\\right) + (0.90)\\left(\\frac{99}{103}\\right) = \\frac{0.20 + 89.1}{103} = \\frac{89.3}{103}\n$$\nFor $x=1$:\n$$\nP(Y=1 \\mid X=1, S=1) = P(Y=1 \\mid X=1, U=0)P(U=0 \\mid X=1, S=1) + P(Y=1 \\mid X=1, U=1)P(U=1 \\mid X=1, S=1)\n$$\n$$\nP(Y=1 \\mid X=1, S=1) = (0.20)\\left(\\frac{200}{299}\\right) + (0.95)\\left(\\frac{99}{299}\\right) = \\frac{40 + 94.05}{299} = \\frac{134.05}{299}\n$$\nWe can now compute the conditional odds.\nFor $x=0$, $S=1$:\n$$\nP(Y=0 \\mid X=0, S=1) = 1 - \\frac{89.3}{103} = \\frac{13.7}{103}\n$$\n$$\n\\text{Odds}(Y=1 \\mid X=0, S=1) = \\frac{89.3/103}{13.7/103} = \\frac{89.3}{13.7}\n$$\nFor $x=1$, $S=1$:\n$$\nP(Y=0 \\mid X=1, S=1) = 1 - \\frac{134.05}{299} = \\frac{164.95}{299}\n$$\n$$\n\\text{Odds}(Y=1 \\mid X=1, S=1) = \\frac{134.05/299}{164.95/299} = \\frac{134.05}{164.95}\n$$\nFinally, the conditional odds ratio is:\n$$\n\\text{OR}_{Y,X \\mid S=1} = \\frac{\\text{Odds}(Y=1 \\mid X=1, S=1)}{\\text{Odds}(Y=1 \\mid X=0, S=1)} = \\frac{134.05 / 164.95}{89.3 / 13.7} = \\frac{134.05 \\times 13.7}{164.95 \\times 89.3} \\approx 0.124676\n$$\nAs required for justification, we verify the effect reversal by computing the marginal odds ratio, $\\text{OR}_{Y,X}$.\nFirst, find $P(Y=1 \\mid X=x)$ by marginalizing over $U$ in the source population where $X \\perp U$.\nFor $x=0$:\n$$\nP(Y=1 \\mid X=0) = P(Y=1 \\mid X=0, U=0)P(U=0) + P(Y=1 \\mid X=0, U=1)P(U=1)\n$$\n$$\nP(Y=1 \\mid X=0) = (0.05)(0.8) + (0.90)(0.2) = 0.04 + 0.18 = 0.22\n$$\nFor $x=1$:\n$$\nP(Y=1 \\mid X=1) = P(Y=1 \\mid X=1, U=0)P(U=0) + P(Y=1 \\mid X=1, U=1)P(U=1)\n$$\n$$\nP(Y=1 \\mid X=1) = (0.20)(0.8) + (0.95)(0.2) = 0.16 + 0.19 = 0.35\n$$\nThe marginal odds are:\n$$\n\\text{Odds}(Y=1 \\mid X=0) = \\frac{0.22}{1-0.22} = \\frac{0.22}{0.78} = \\frac{11}{39}\n$$\n$$\n\\text{Odds}(Y=1 \\mid X=1) = \\frac{0.35}{1-0.35} = \\frac{0.35}{0.65} = \\frac{7}{13}\n$$\nThe marginal odds ratio is:\n$$\n\\text{OR}_{Y,X} = \\frac{7/13}{11/39} = \\frac{7}{13} \\times \\frac{39}{11} = \\frac{7 \\times 3}{11} = \\frac{21}{11} \\approx 1.909\n$$\nWe have verified that the marginal odds ratio $\\text{OR}_{Y,X} \\approx 1.909 > 1$, indicating that $X$ is a risk factor for $Y$ in the source population. However, the conditional odds ratio $\\text{OR}_{Y,X \\mid S=1} \\approx 0.1247 < 1$, indicating that $X$ appears to be a protective factor among the selected individuals. This reversal is a hallmark of collider stratification bias.\n\nThe final numerical answer required is $\\text{OR}_{Y,X \\mid S=1}$, rounded to four significant figures.\nValue: $0.124676...$\nRounded value: $0.1247$.", "answer": "$$\n\\boxed{0.1247}\n$$", "id": "4954359"}, {"introduction": "While we can adjust for measured confounders, the threat of unmeasured confounding always looms over observational findings. This exercise introduces the E-value, a powerful and widely used sensitivity analysis tool designed to quantify the robustness of an observed association to such unmeasured confounding [@problem_id:4954364]. By deriving the E-value formula from first principles, you will gain a deep understanding of this essential metric for critically appraising and reporting evidence from non-randomized studies.", "problem": "An observational cohort study evaluates whether preoperative elevation of a novel inflammatory biomarker is associated with increased $30$-day all-cause mortality among adult surgical patients. After adjustment for measured confounders, the fitted log-binomial model yields a risk ratio (RR) for death comparing high versus low biomarker of $RR_{\\text{obs}}=2.35$. You are concerned about a single, unmeasured binary confounder $U$ that may be associated with both the exposure $E$ (high biomarker) and the outcome $D$ (mortality), conditional on the measured covariates.\n\nBy definition, the E-value for a risk ratio is the minimum strength of association, on the risk ratio scale, that an unmeasured confounder would need to have with both the exposure and the outcome, conditional on the measured covariates, to reduce the true causal risk ratio to $1$ (i.e., to fully explain away the observed association).\n\nAdopt the following well-tested bias-bounding result: under standard conditions for multiplicative confounding with a single unmeasured binary confounder, the maximum multiplicative bias factor $B$ that such a confounder can induce satisfies\n\n$$\nB \\le \\frac{RR_{EU}\\times RR_{UD}}{RR_{EU} + RR_{UD} - 1},\n$$\n\nwhere $RR_{EU}$ is the risk ratio association of $U$ with $E$ conditional on measured covariates, and $RR_{UD}$ is the risk ratio association of $U$ with $D$ conditional on measured covariates.\n\nStarting only from the definition of the E-value and the bias-bounding result above, derive a closed-form expression for the E-value as a function of $RR_{\\text{obs}}$ when $RR_{\\text{obs}}>1$. Then evaluate your expression at $RR_{\\text{obs}}=2.35$. Round your numerical answer to four significant figures and report it as a pure number (no units).", "solution": "This problem requires us to derive the formula for the E-value from first principles and then apply it. The problem is well-posed and scientifically valid.\n\nThe relationship between the observed risk ratio ($RR_{\\text{obs}}$), the true causal risk ratio ($RR_{\\text{true}}$), and the bias factor due to unmeasured confounding ($B$) is multiplicative:\n$$RR_{\\text{obs}} = RR_{\\text{true}} \\times B$$\nTo \"fully explain away the observed association\" means to show that the observed association is entirely due to confounding, which implies the true causal risk ratio is null, i.e., $RR_{\\text{true}} = 1$. In this case, the bias factor must be equal to the observed risk ratio:\n$$B = RR_{\\text{obs}}$$\nThe problem provides an upper bound for the bias factor that can be generated by a single binary confounder $U$:\n$$B \\le \\frac{RR_{EU} \\times RR_{UD}}{RR_{EU} + RR_{UD} - 1}$$\nwhere $RR_{EU}$ and $RR_{UD}$ are the strengths of association between the confounder and the exposure and outcome, respectively.\n\nFor an unmeasured confounder to be capable of explaining away the observed effect, its maximum possible biasing effect must be at least as large as the observed risk ratio. This leads to the necessary condition:\n$$RR_{\\text{obs}} \\le \\frac{RR_{EU} \\times RR_{UD}}{RR_{EU} + RR_{UD} - 1}$$\nThe E-value is defined as the *minimum* strength of association, $E$, that the confounder would need to have with *both* the exposure and the outcome to satisfy this condition. The right-hand side of the inequality is an increasing function of both $RR_{EU}$ and $RR_{UD}$ (for values $\\ge 1$). Therefore, the minimum requirement occurs when we set $RR_{EU} = RR_{UD} = E$. The condition for the E-value is the boundary case where the inequality becomes an equality:\n$$RR_{\\text{obs}} = \\frac{E \\times E}{E + E - 1} = \\frac{E^2}{2E - 1}$$\nWe now solve this equation for $E$ in terms of $RR_{\\text{obs}}$:\n$$RR_{\\text{obs}}(2E - 1) = E^2$$\n$$E^2 - (2 \\cdot RR_{\\text{obs}})E + RR_{\\text{obs}} = 0$$\nThis is a quadratic equation in $E$. We use the quadratic formula $E = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$ with $a=1$, $b = -2 \\cdot RR_{\\text{obs}}$, and $c = RR_{\\text{obs}}$:\n$$E = \\frac{2 \\cdot RR_{\\text{obs}} \\pm \\sqrt{(-2 \\cdot RR_{\\text{obs}})^2 - 4(1)(RR_{\\text{obs}})}}{2}$$\n$$E = \\frac{2 \\cdot RR_{\\text{obs}} \\pm \\sqrt{4 \\cdot RR_{\\text{obs}}^2 - 4 \\cdot RR_{\\text{obs}}}}{2}$$\n$$E = RR_{\\text{obs}} \\pm \\sqrt{RR_{\\text{obs}}^2 - RR_{\\text{obs}}}$$\n$$E = RR_{\\text{obs}} \\pm \\sqrt{RR_{\\text{obs}}(RR_{\\text{obs}} - 1)}$$\nSince $RR_{\\text{obs}} > 1$ and the E-value must be a risk ratio $\\ge 1$, we must choose the larger root. The smaller root, $RR_{\\text{obs}} - \\sqrt{RR_{\\text{obs}}(RR_{\\text{obs}}-1)}$, can be shown to be less than 1 for any $RR_{\\text{obs}} > 1$, so it is not a valid solution.\nThe correct closed-form expression for the E-value is:\n$$E = RR_{\\text{obs}} + \\sqrt{RR_{\\text{obs}}(RR_{\\text{obs}} - 1)}$$\nNow, we evaluate this for the given value $RR_{\\text{obs}} = 2.35$:\n$$E = 2.35 + \\sqrt{2.35 \\times (2.35 - 1)}$$\n$$E = 2.35 + \\sqrt{2.35 \\times 1.35}$$\n$$E = 2.35 + \\sqrt{3.1725}$$\n$$E \\approx 2.35 + 1.78115$$\n$$E \\approx 4.13115$$\nRounding the result to four significant figures gives $4.131$. This is the minimum strength of association (on the RR scale) that an unmeasured confounder must have with both the exposure and the outcome to fully explain the observed association.", "answer": "$$ \\boxed{4.131} $$", "id": "4954364"}]}