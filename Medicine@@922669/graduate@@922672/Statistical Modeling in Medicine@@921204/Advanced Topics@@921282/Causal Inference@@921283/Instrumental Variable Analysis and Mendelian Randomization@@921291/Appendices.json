{"hands_on_practices": [{"introduction": "This practice focuses on a crucial first step in any instrumental variable analysis: assessing the strength of your instrument. A weak instrument, one that is only weakly associated with the exposure, can lead to biased and unreliable causal estimates. This exercise guides you through calculating the first-stage $F$ statistic, a standard metric for quantifying instrument strength, and interpreting its value using a common rule-of-thumb [@problem_id:4966489].", "problem": "A Mendelian randomization (MR) study uses a single genetic variant, a Single Nucleotide Polymorphism (SNP), as an Instrumental Variable (IV) to estimate the causal effect of an exposure $X$ on an outcome $Y$. The instrument relevance is assessed through the first-stage regression of $X$ on the instrument $Z$ using Ordinary Least Squares (OLS) in a large sample, specified as $X = \\pi Z + \\eta$, where $\\pi$ is the instrument-exposure effect and $\\eta$ is the disturbance term.\n\nYou are given the estimated first-stage coefficient $\\hat{\\pi} = 0.05$ and its standard error $SE(\\hat{\\pi}) = 0.02$. Assume the sample size is sufficiently large so that large-sample approximations apply.\n\nUsing the standard framework of hypothesis testing for linear models and the definition of the $F$ statistic for testing instrument relevance under the null hypothesis $H_{0} : \\pi = 0$, derive from first principles the appropriate test statistic for a single linear restriction and compute its value based on the provided estimates. Then, using accepted practice in instrumental variable analysis, determine whether the instrument would be considered strong under the rule-of-thumb threshold. No rounding is required; report the $F$ statistic as an exact value. The final answer must be the single numeric value of the $F$ statistic (unitless).", "solution": "The user has provided a problem in the field of statistical modeling, specifically concerning instrumental variable (IV) analysis and Mendelian randomization (MR). The problem statement must first be validated for scientific soundness, completeness, and objectivity.\n\n### Step 1: Extract Givens\n- **Model**: A first-stage regression model for an exposure $X$ on an instrumental variable $Z$ is specified as $X = \\pi Z + \\eta$.\n- **Instrument**: $Z$ is a single genetic variant, a Single Nucleotide Polymorphism (SNP).\n- **Coefficient of Interest**: $\\pi$ is the instrument-exposure effect.\n- **Hypothesis Test**: The null hypothesis for instrument relevance is $H_{0} : \\pi = 0$.\n- **Estimates**: The Ordinary Least Squares (OLS) estimate of the coefficient is $\\hat{\\pi} = 0.05$.\n- **Standard Error**: The standard error of the estimate is $SE(\\hat{\\pi}) = 0.02$.\n- **Sample Size**: Assumed to be \"sufficiently large\" for large-sample approximations to be valid.\n- **Task**: Derive the appropriate test statistic ($F$-statistic) for a single linear restriction, compute its value, and evaluate the instrument's strength against the conventional rule-of-thumb.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is firmly grounded in the established principles of econometrics and biostatistics. Mendelian randomization is a standard application of instrumental variable methods. The first-stage regression, the null hypothesis of no instrument-exposure association ($H_0: \\pi = 0$), and the use of a test statistic to assess instrument relevance are all fundamental and correct concepts within this framework.\n2.  **Well-Posed**: The problem is well-posed. It provides all necessary information ($\\hat{\\pi}$ and $SE(\\hat{\\pi})$) to calculate the required test statistic. The question has a unique and meaningful solution.\n3.  **Objective**: The problem is stated using precise, objective statistical terminology. The \"rule-of-thumb threshold\" refers to a widely recognized convention in the applied IV literature, not a subjective opinion.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically sound, well-posed, and objective. I will now proceed with the solution.\n\n### Solution Derivation\n\nThe problem requires the derivation and calculation of the $F$-statistic for testing the relevance of the instrumental variable $Z$. The null hypothesis for this test is that the instrument has no association with the exposure, which is specified as $H_{0} : \\pi = 0$ in the first-stage regression model $X = \\pi Z + \\eta$.\n\nIn a linear regression model, the test for a single linear restriction on a single coefficient is most directly conducted using a $t$-statistic. The $t$-statistic is defined as the ratio of the estimated coefficient to its standard error, centered around the value of the coefficient under the null hypothesis.\n\nThe general form of the $t$-statistic for testing $H_{0} : \\pi = \\pi_{0}$ is:\n$$\nt = \\frac{\\hat{\\pi} - \\pi_{0}}{SE(\\hat{\\pi})}\n$$\nIn this specific problem, the null hypothesis is $H_{0} : \\pi = 0$, so $\\pi_{0} = 0$. The $t$-statistic simplifies to:\n$$\nt = \\frac{\\hat{\\pi}}{SE(\\hat{\\pi})}\n$$\nA fundamental principle in the theory of linear models is the relationship between the $t$-statistic and the $F$-statistic. For a test involving a single linear restriction (in this case, $k=1$ restriction, which is $\\pi=0$), the $F$-statistic is exactly the square of the corresponding $t$-statistic. This holds true for large samples, an assumption given in the problem statement.\n$$\nF = t^2\n$$\nThis relationship constitutes the derivation from first principles requested by the problem, as it links the fundamental test for a single parameter ($t$-test) to the more general framework of the $F$-test for linear restrictions.\n\nNow, we can compute the value of the test statistic using the provided estimates.\nGiven:\n- Estimated coefficient: $\\hat{\\pi} = 0.05$\n- Standard error of the coefficient: $SE(\\hat{\\pi}) = 0.02$\n\nFirst, we calculate the value of the $t$-statistic:\n$$\nt = \\frac{0.05}{0.02} = 2.5\n$$\nNext, we use the relationship $F = t^2$ to find the $F$-statistic:\n$$\nF = (2.5)^2 = 6.25\n$$\nThe calculated $F$-statistic for testing instrument relevance is $6.25$.\n\nFinally, the problem asks to determine if the instrument would be considered strong. In instrumental variable analysis, a common rule-of-thumb, originating from the work of Staiger and Stock (1997), is used to assess instrument strength. According to this convention, an instrument is considered \"weak\" if the first-stage $F$-statistic is less than $10$. Weak instruments can lead to biased and imprecise causal estimates in the second stage of the analysis.\n\nIn this case, the computed $F$-statistic is $6.25$. Since $6.25 < 10$, the single SNP instrument would be classified as a weak instrument under this widely accepted threshold. This suggests that the study may suffer from weak instrument bias.\n\nThe final answer required is the numerical value of the $F$-statistic.", "answer": "$$\\boxed{6.25}$$", "id": "4966489"}, {"introduction": "Once an instrument is confirmed to be sufficiently strong, the next step is to estimate the causal effect itself. This exercise demonstrates how to calculate the causal effect using the Wald ratio in a two-sample Mendelian randomization context, a highly common study design. You will also apply the delta method to derive the standard error, a vital step for constructing confidence intervals and performing hypothesis tests on your causal estimate [@problem_id:4966448].", "problem": "Consider a two-sample Mendelian randomization setting in statistical modeling in medicine, where a single biallelic genetic variant $Z$ is proposed as an instrumental variable (IV) for an exposure $X$ that may causally influence an outcome $Y$. Assume the standard instrumental variable assumptions hold: relevance ($\\mathrm{Cov}(Z,X)\\neq 0$), independence ($Z$ is independent of all unmeasured confounders of $X$ and $Y$), and exclusion restriction ($Z$ affects $Y$ only through $X$), together with a linear causal model $Y=\\beta X+\\varepsilon$ and $\\mathrm{E}[Z\\varepsilon]=0$. In a two-sample design, the associations of $Z$ with $X$ and $Z$ with $Y$ are estimated in independent cohorts, yielding summary regression coefficients $\\hat{\\gamma}_{ZX}$ for the $Z$–$X$ association and $\\hat{\\gamma}_{ZY}$ for the $Z$–$Y$ association, with corresponding Standard Errors (SEs) $\\mathrm{SE}(\\hat{\\gamma}_{ZX})$ and $\\mathrm{SE}(\\hat{\\gamma}_{ZY})$.\n\nYou are given $\\hat{\\gamma}_{ZX}=0.12$ with $\\mathrm{SE}(\\hat{\\gamma}_{ZX})=0.03$ and $\\hat{\\gamma}_{ZY}=0.06$ with $\\mathrm{SE}(\\hat{\\gamma}_{ZY})=0.02$. Using only the fundamental IV moment condition $\\mathrm{E}[Z(Y-\\beta X)]=0$ and standard large-sample approximations, do the following:\n\n1. Derive an estimator of the causal effect $\\beta$ in terms of the observable genetic associations.\n2. Using the first-order multivariate delta method, and assuming the two samples are independent so that the covariance between $\\hat{\\gamma}_{ZX}$ and $\\hat{\\gamma}_{ZY}$ is $0$, derive an approximate expression for the asymptotic variance of your estimator in terms of $\\hat{\\gamma}_{ZX}, \\hat{\\gamma}_{ZY}, \\mathrm{SE}(\\hat{\\gamma}_{ZX}), \\mathrm{SE}(\\hat{\\gamma}_{ZY})$, and compute the corresponding approximate SE.\n\nReport the numerical values for both the point estimate and its approximate SE, rounding each to four significant figures. No units are required.", "solution": "The problem asks for the derivation and calculation of a causal effect estimate and its standard error in a two-sample Mendelian randomization (MR) context. The process involves two main parts: deriving the estimator for the causal effect $\\beta$ and then deriving the expression for its standard error using the multivariate delta method.\n\n### Part 1: Derivation of the Causal Effect Estimator\n\nWe are given a linear causal model for the outcome $Y$ as a function of the exposure $X$:\n$$Y = \\beta_0 + \\beta X + \\varepsilon$$\nHere, $\\beta$ is the causal effect of interest, and $\\varepsilon$ is an error term that includes all other influences on $Y$, including unmeasured confounders. We are also given that the instrumental variable (IV) $Z$ satisfies the fundamental moment condition $\\mathrm{E}[Z(Y-\\beta X)]=0$. While the problem provides a simplified model $Y=\\beta X+\\varepsilon$, we include an intercept $\\beta_0$ for generality; it will not affect the result for the slope coefficient $\\beta$. The condition $\\mathrm{E}[Z(Y-\\beta X)]=0$ is equivalent to $\\mathrm{E}[Z\\varepsilon]=0$ when substituting the structural model for $Y$.\n\nFrom the exogeneity condition $\\mathrm{E}[Z\\varepsilon]=0$, and assuming without loss of generality that the variables are centered or that an intercept is fitted, we have $\\mathrm{Cov}(Z, \\varepsilon) = 0$.\nLet's take the covariance of the instrumental variable $Z$ with the structural equation for $Y$:\n$$ \\mathrm{Cov}(Z, Y) = \\mathrm{Cov}(Z, \\beta_0 + \\beta X + \\varepsilon) $$\nUsing the properties of covariance, this becomes:\n$$ \\mathrm{Cov}(Z, Y) = \\mathrm{Cov}(Z, \\beta_0) + \\mathrm{Cov}(Z, \\beta X) + \\mathrm{Cov}(Z, \\varepsilon) $$\n$$ \\mathrm{Cov}(Z, Y) = 0 + \\beta \\mathrm{Cov}(Z, X) + \\mathrm{Cov}(Z, \\varepsilon) $$\nThe IV independence assumption implies that $Z$ is independent of all unmeasured confounders. These confounders are components of the error term $\\varepsilon$. Therefore, $Z$ and $\\varepsilon$ are uncorrelated, which means $\\mathrm{Cov}(Z, \\varepsilon) = 0$. The equation simplifies to:\n$$ \\mathrm{Cov}(Z, Y) = \\beta \\mathrm{Cov}(Z, X) $$\nUnder the IV relevance assumption ($\\mathrm{Cov}(Z, X) \\neq 0$), we can solve for the causal effect $\\beta$:\n$$ \\beta = \\frac{\\mathrm{Cov}(Z, Y)}{\\mathrm{Cov}(Z, X)} $$\nThe problem provides summary statistics from two independent samples in the form of regression coefficients. The coefficient from a linear regression of an outcome on a predictor is an estimate of the ratio of their covariance to the variance of the predictor.\nThe association of $Z$ with $Y$, $\\gamma_{ZY}$, is given by:\n$$ \\gamma_{ZY} = \\frac{\\mathrm{Cov}(Z, Y)}{\\mathrm{Var}(Z)} $$\nSimilarly, the association of $Z$ with $X$, $\\gamma_{ZX}$, is given by:\n$$ \\gamma_{ZX} = \\frac{\\mathrm{Cov}(Z, X)}{\\mathrm{Var}(Z)} $$\nSubstituting these into the expression for $\\beta$:\n$$ \\beta = \\frac{\\gamma_{ZY} \\mathrm{Var}(Z)}{\\gamma_{ZX} \\mathrm{Var}(Z)} = \\frac{\\gamma_{ZY}}{\\gamma_{ZX}} $$\nThis relationship holds for the true population parameters. The natural estimator for $\\beta$ is the ratio of the sample estimates of these coefficients, known as the ratio estimator or, in this context, the Wald estimator:\n$$ \\hat{\\beta} = \\frac{\\hat{\\gamma}_{ZY}}{\\hat{\\gamma}_{ZX}} $$\nUsing the provided numerical values:\n$$ \\hat{\\beta} = \\frac{0.06}{0.12} = 0.5 $$\n\n### Part 2: Derivation of the Approximate Standard Error\n\nTo find the variance of the estimator $\\hat{\\beta}$, we use the first-order multivariate delta method. Our estimator $\\hat{\\beta}$ is a function of two random variables, $\\hat{\\gamma}_{ZX}$ and $\\hat{\\gamma}_{ZY}$. Let $f(\\hat{\\gamma}_{ZX}, \\hat{\\gamma}_{ZY}) = \\frac{\\hat{\\gamma}_{ZY}}{\\hat{\\gamma}_{ZX}}$.\n\nThe delta method approximates the variance of $f(\\hat{\\gamma}_{ZX}, \\hat{\\gamma}_{ZY})$ as:\n$$ \\mathrm{Var}(\\hat{\\beta}) \\approx \\nabla f^T \\Sigma \\nabla f $$\nwhere $\\nabla f$ is the gradient of $f$ evaluated at the true parameter values $(\\gamma_{ZX}, \\gamma_{ZY})$, and $\\Sigma$ is the covariance matrix of $(\\hat{\\gamma}_{ZX}, \\hat{\\gamma}_{ZY})$.\n\nFirst, we compute the gradient vector $\\nabla f$:\n$$ \\nabla f = \\begin{pmatrix} \\frac{\\partial f}{\\partial \\gamma_{ZX}} \\\\ \\frac{\\partial f}{\\partial \\gamma_{ZY}} \\end{pmatrix} = \\begin{pmatrix} -\\frac{\\gamma_{ZY}}{\\gamma_{ZX}^2} \\\\ \\frac{1}{\\gamma_{ZX}} \\end{pmatrix} $$\nThe covariance matrix $\\Sigma$ of the estimators is:\n$$ \\Sigma = \\begin{pmatrix} \\mathrm{Var}(\\hat{\\gamma}_{ZX}) & \\mathrm{Cov}(\\hat{\\gamma}_{ZX}, \\hat{\\gamma}_{ZY}) \\\\ \\mathrm{Cov}(\\hat{\\gamma}_{ZY}, \\hat{\\gamma}_{ZX}) & \\mathrm{Var}(\\hat{\\gamma}_{ZY}) \\end{pmatrix} $$\nThe problem states that the two samples are independent, so the covariance term is zero: $\\mathrm{Cov}(\\hat{\\gamma}_{ZX}, \\hat{\\gamma}_{ZY}) = 0$. The variances are the squares of the given standard errors, $\\mathrm{Var}(\\hat{\\gamma}_{ZX}) = \\mathrm{SE}(\\hat{\\gamma}_{ZX})^2$ and $\\mathrm{Var}(\\hat{\\gamma}_{ZY}) = \\mathrm{SE}(\\hat{\\gamma}_{ZY})^2$.\n$$ \\Sigma = \\begin{pmatrix} \\mathrm{SE}(\\hat{\\gamma}_{ZX})^2 & 0 \\\\ 0 & \\mathrm{SE}(\\hat{\\gamma}_{ZY})^2 \\end{pmatrix} $$\nNow we apply the delta method formula. For the calculation, we substitute the sample estimates for the true parameters in the gradient:\n$$ \\widehat{\\mathrm{Var}}(\\hat{\\beta}) \\approx \\begin{pmatrix} -\\frac{\\hat{\\gamma}_{ZY}}{\\hat{\\gamma}_{ZX}^2} & \\frac{1}{\\hat{\\gamma}_{ZX}} \\end{pmatrix} \\begin{pmatrix} \\mathrm{SE}(\\hat{\\gamma}_{ZX})^2 & 0 \\\\ 0 & \\mathrm{SE}(\\hat{\\gamma}_{ZY})^2 \\end{pmatrix} \\begin{pmatrix} -\\frac{\\hat{\\gamma}_{ZY}}{\\hat{\\gamma}_{ZX}^2} \\\\ \\frac{1}{\\hat{\\gamma}_{ZX}} \\end{pmatrix} $$\nPerforming the matrix multiplication:\n$$ \\widehat{\\mathrm{Var}}(\\hat{\\beta}) \\approx \\left(-\\frac{\\hat{\\gamma}_{ZY}}{\\hat{\\gamma}_{ZX}^2}\\right)^2 \\mathrm{SE}(\\hat{\\gamma}_{ZX})^2 + \\left(\\frac{1}{\\hat{\\gamma}_{ZX}}\\right)^2 \\mathrm{SE}(\\hat{\\gamma}_{ZY})^2 $$\n$$ \\widehat{\\mathrm{Var}}(\\hat{\\beta}) \\approx \\frac{\\hat{\\gamma}_{ZY}^2}{\\hat{\\gamma}_{ZX}^4} \\mathrm{SE}(\\hat{\\gamma}_{ZX})^2 + \\frac{1}{\\hat{\\gamma}_{ZX}^2} \\mathrm{SE}(\\hat{\\gamma}_{ZY})^2 $$\nThis can be rearranged to a more intuitive form:\n$$ \\widehat{\\mathrm{Var}}(\\hat{\\beta}) \\approx \\frac{1}{\\hat{\\gamma}_{ZX}^2} \\left( \\frac{\\hat{\\gamma}_{ZY}^2}{\\hat{\\gamma}_{ZX}^2} \\mathrm{SE}(\\hat{\\gamma}_{ZX})^2 + \\mathrm{SE}(\\hat{\\gamma}_{ZY})^2 \\right) = \\frac{1}{\\hat{\\gamma}_{ZX}^2} \\left( \\hat{\\beta}^2 \\mathrm{SE}(\\hat{\\gamma}_{ZX})^2 + \\mathrm{SE}(\\hat{\\gamma}_{ZY})^2 \\right) $$\nThe approximate standard error, $\\mathrm{SE}(\\hat{\\beta})$, is the square root of this variance.\n\nNow, we compute the numerical value using the given data:\n$\\hat{\\gamma}_{ZX}=0.12$, $\\mathrm{SE}(\\hat{\\gamma}_{ZX})=0.03$\n$\\hat{\\gamma}_{ZY}=0.06$, $\\mathrm{SE}(\\hat{\\gamma}_{ZY})=0.02$\n\nUsing the derived formula for the variance:\n$$ \\widehat{\\mathrm{Var}}(\\hat{\\beta}) \\approx \\frac{(0.06)^2}{(0.12)^4} (0.03)^2 + \\frac{1}{(0.12)^2} (0.02)^2 $$\n$$ \\widehat{\\mathrm{Var}}(\\hat{\\beta}) \\approx \\frac{0.0036}{0.00020736} (0.0009) + \\frac{1}{0.0144} (0.0004) $$\n$$ \\widehat{\\mathrm{Var}}(\\hat{\\beta}) \\approx (17.3611...) \\times (0.0009) + (69.4444...) \\times (0.0004) $$\n$$ \\widehat{\\mathrm{Var}}(\\hat{\\beta}) \\approx 0.015625 + 0.027777... = 0.04340277... $$\nAlternatively, using fractions provides an exact result:\n$$ \\widehat{\\mathrm{Var}}(\\hat{\\beta}) \\approx \\left(\\frac{0.5}{0.12}\\right)^2 \\left( (0.03)^2 + \\frac{(0.02)^2}{(0.5)^2} \\right) = \\frac{(0.5)^2}{(0.12)^2} \\left( (0.03)^2 + \\frac{(0.02)^2}{(0.5)^2} \\right) = \\hat{\\beta}^2 \\left( \\frac{\\mathrm{SE}(\\hat{\\gamma}_{ZX})^2}{\\hat{\\gamma}_{ZX}^2} + \\frac{\\mathrm{SE}(\\hat{\\gamma}_{ZY})^2}{\\hat{\\gamma}_{ZY}^2} \\right) $$\n$$ \\widehat{\\mathrm{Var}}(\\hat{\\beta}) \\approx (0.5)^2 \\left( \\left(\\frac{0.03}{0.12}\\right)^2 + \\left(\\frac{0.02}{0.06}\\right)^2 \\right) = (0.5)^2 \\left( \\left(\\frac{1}{4}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 \\right) $$\n$$ \\widehat{\\mathrm{Var}}(\\hat{\\beta}) = \\frac{1}{4} \\left( \\frac{1}{16} + \\frac{1}{9} \\right) = \\frac{1}{4} \\left( \\frac{9+16}{144} \\right) = \\frac{1}{4} \\left( \\frac{25}{144} \\right) = \\frac{25}{576} $$\nThe standard error is the square root of the variance:\n$$ \\mathrm{SE}(\\hat{\\beta}) = \\sqrt{\\frac{25}{576}} = \\frac{5}{24} $$\nIn decimal form, this is $5 \\div 24 \\approx 0.208333...$.\n\nThe problem requires reporting the numerical values for the point estimate and its approximate SE, rounded to four significant figures.\nThe point estimate is $\\hat{\\beta} = 0.5000$.\nThe standard error is $\\mathrm{SE}(\\hat{\\beta}) \\approx 0.2083$.", "answer": "$$\\boxed{\\begin{pmatrix} 0.5000 & 0.2083 \\end{pmatrix}}$$", "id": "4966448"}, {"introduction": "A core assumption of Mendelian Randomization is the \"exclusion restriction,\" which states that the genetic instrument affects the outcome *only* through the exposure. When this assumption is violated by a phenomenon called horizontal pleiotropy, the causal estimate can be biased. This practice introduces the MR-Egger regression intercept test, a key sensitivity analysis used to detect a specific, biasing pattern of pleiotropy and assess the robustness of your findings [@problem_id:4966512].", "problem": "You conduct a two-sample Mendelian randomization (MR) analysis to estimate the causal effect of a cardiometabolic exposure on a binary disease outcome using many independent genome-wide significant single nucleotide polymorphisms (SNPs) as instruments. To probe the Instrument Strength Independent of Direct Effect (InSIDE) assumption and assess directional pleiotropy, you fit an MR-Egger regression across SNP-specific associations. The MR-Egger intercept estimate is reported as $\\hat{\\beta}_{0}=0.02$ with standard error $\\mathrm{SE}(\\hat{\\beta}_{0})=0.01$. Assume the usual large-sample conditions under which the MR-Egger intercept estimator is asymptotically normal and inference may proceed via a two-sided Wald test for the null hypothesis $H_{0}:\\beta_{0}=0$. Use a significance level of $\\alpha=0.05$. \n\nCompute the two-sided $p$ value for testing $H_{0}:\\beta_{0}=0$ and, based solely on this $p$ value and $\\alpha$, determine whether there is statistical evidence of directional pleiotropy. Round your $p$ value to four significant figures and express it as a decimal. The final answer should be the $p$ value only.", "solution": "The scientific objective is to test whether the average direct (pleiotropic) effect across instruments is zero, which in the Mendelian randomization (MR) framework can be assessed using the intercept from MR-Egger regression. Under standard regularity conditions and a large number of instruments, the MR-Egger intercept estimator is approximately normally distributed:\n$$\n\\hat{\\beta}_{0} \\sim \\mathcal{N}\\!\\left(\\beta_{0}, \\ \\mathrm{SE}(\\hat{\\beta}_{0})^{2}\\right).\n$$\nTo test the null hypothesis $H_{0}:\\beta_{0}=0$ against the two-sided alternative $H_{1}:\\beta_{0}\\neq 0$, we use the Wald statistic\n$$\nZ \\;=\\; \\frac{\\hat{\\beta}_{0}-0}{\\mathrm{SE}(\\hat{\\beta}_{0})}.\n$$\nGiven $\\hat{\\beta}_{0}=0.02$ and $\\mathrm{SE}(\\hat{\\beta}_{0})=0.01$, we compute\n$$\nZ \\;=\\; \\frac{0.02}{0.01} \\;=\\; 2.\n$$\nUnder $H_{0}$ and the large-sample normal approximation, $Z$ is approximately standard normal, so the two-sided $p$ value is\n$$\np \\;=\\; 2\\left[1-\\Phi\\!\\left(|Z|\\right)\\right] \\;=\\; 2\\left[1-\\Phi(2)\\right],\n$$\nwhere $\\Phi(\\cdot)$ is the cumulative distribution function of the standard normal distribution. Using the well-known value $\\Phi(2)\\approx 0.977249868$, we obtain\n$$\np \\;\\approx\\; 2\\left(1-0.977249868\\right) \\;=\\; 2\\cdot 0.022750132 \\;=\\; 0.045500264.\n$$\nRounded to four significant figures, the $p$ value is $0.04550$.\n\nFor interpretation at significance level $\\alpha=0.05$, since $p \\approx 0.04550  0.05$, there is statistical evidence against $H_{0}$, indicating the presence of directional pleiotropy according to the MR-Egger intercept test. The required final answer, however, is the $p$ value only.", "answer": "$$\\boxed{0.04550}$$", "id": "4966512"}]}