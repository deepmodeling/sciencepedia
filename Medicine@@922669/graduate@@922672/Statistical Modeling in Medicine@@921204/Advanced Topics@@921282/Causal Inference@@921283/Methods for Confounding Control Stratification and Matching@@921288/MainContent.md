## Introduction
In medical research, distinguishing correlation from causation is a paramount challenge. While randomized controlled trials (RCTs) represent the gold standard for causal inference, much of our evidence comes from observational studies where treatment assignment is not random. This lack of randomization introduces the risk of **confounding**, where underlying differences between treated and untreated groups distort the true effect of an intervention, sometimes leading to paradoxical and misleading conclusions. This article provides a graduate-level guide to two foundational methods for addressing this problem: stratification and matching.

This article will equip you with the theoretical knowledge and practical skills to control for confounding in your own research. The first chapter, **Principles and Mechanisms**, establishes the formal language of causal inference using the potential outcomes framework, defining the core assumptions of exchangeability and positivity that make causal claims possible. It then delves into the mechanics of how stratification and matching, including the use of the powerful propensity score, work to block confounding. The second chapter, **Applications and Interdisciplinary Connections**, bridges theory and practice, demonstrating how these techniques are applied in clinical epidemiology and health services research, with a focus on the essential workflow of covariate selection, balance assessment, and [sensitivity analysis](@entry_id:147555). Finally, the **Hands-On Practices** chapter will challenge you to apply these principles to solve realistic data analysis problems. We begin by exploring the fundamental challenge that confounding poses and the theoretical framework designed to overcome it.

## Principles and Mechanisms

### The Challenge of Confounding: A Case of Paradox

In an ideal randomized controlled trial, the comparison of treatment arms yields an unbiased estimate of the causal effect of an intervention. This is because randomization ensures that the groups being compared are, on average, identical with respect to all baseline characteristics, both measured and unmeasured. In observational studies, however, this fundamental exchangeability is lost. Treatment is not assigned at random but is often chosen based on patient characteristics that also influence the outcome. This lack of comparability can lead to **confounding**, a [systematic error](@entry_id:142393) that can distort the true relationship between an exposure and an outcome, sometimes leading to paradoxical and entirely misleading conclusions.

Consider a common scenario in clinical research: an observational cohort study is conducted to evaluate a new anticoagulant therapy ($A$) on the risk of stroke ($Y$) in patients with atrial fibrillation. Investigators collect data and perform a crude analysis, comparing the overall risk of stroke in the treated group ($A=1$) to that in the untreated group ($A=0$). To their surprise, they find that the risk of stroke is higher among patients who received the new drug, suggesting it is harmful. This crude risk difference, $E(Y \mid A=1) - E(Y \mid A=0)$, is positive.

However, a more careful analysis takes into account a baseline severity score ($L$), which quantifies the patient's underlying risk of stroke before treatment initiation. When the data is stratified by this score—separating patients into low-severity ($L=0$) and high-severity ($L=1$) groups—a completely different picture emerges. Within the low-severity group, the treatment is associated with a *lower* risk of stroke. Similarly, within the high-severity group, the treatment is also associated with a *lower* risk of stroke. The association has reversed its sign upon stratification.

This phenomenon, where a marginal association is different from, and often opposite to, the conditional associations within strata of a third variable, is known as **Simpson's Paradox**. As illustrated by this hypothetical but realistic example [@problem_id:4973486], the paradox is not a mathematical curiosity but a manifestation of confounding. In this case, clinicians, acting on their best judgment, were more likely to prescribe the new anticoagulant to patients with higher baseline severity—a practice known as **confounding by indication**. The treated group was therefore composed predominantly of sicker, higher-risk individuals, while the untreated group was composed of healthier, lower-risk individuals. The crude analysis was not a fair comparison; it was comparing high-risk treated patients to low-risk untreated patients. The baseline severity score $L$ is a classic **confounder**: it is a common cause of both the treatment decision ($L \rightarrow A$) and the outcome ($L \rightarrow Y$). This creates a non-causal [statistical association](@entry_id:172897) between $A$ and $Y$ through the "backdoor path" $A \leftarrow L \rightarrow Y$. The goal of confounding control methods, such as stratification, is to block this backdoor path and isolate the true causal effect of $A$ on $Y$ (the path $A \rightarrow Y$).

### Foundational Framework for Identification

To formalize the task of confounding control, we turn to the **potential outcomes framework**. For each individual, we imagine two potential outcomes: $Y(1)$, the outcome that would be observed if the individual received the treatment ($A=1$), and $Y(0)$, the outcome that would be observed if the individual did not receive the treatment ($A=0$). The causal effect for an individual is the difference $Y(1) - Y(0)$. The primary goal in many studies is to estimate the **Average Treatment Effect (ATE)** in the population, defined as $\tau = E[Y(1) - Y(0)]$.

The fundamental problem of causal inference is that for any given individual, we can only observe one of these potential outcomes. To identify the ATE from observational data, we must rely on a set of three core assumptions, which together allow us to connect the unobservable world of potential outcomes to the observable world of data $(A, Y, L)$, where $L$ is a set of measured pre-treatment covariates [@problem_id:4973446].

1.  **Consistency**: The observed outcome for an individual is the potential outcome corresponding to the treatment they actually received. That is, if an individual has $A=a$, their observed outcome $Y$ is equal to $Y(a)$. This is often written as $Y = Y(A)$. This assumption links the data we have to the causal quantities we want. It implicitly requires that treatments are well-defined and that one person's treatment does not affect another's outcome (part of the **Stable Unit Treatment Value Assumption**, or SUTVA).

2.  **Conditional Exchangeability** (or **Ignorability**): Conditional on the set of measured covariates $L$, treatment assignment is independent of the potential outcomes. Formally, $(Y(1), Y(0)) \perp A \mid L$. This is the "no unmeasured confounding" assumption. It states that within any stratum defined by the covariates in $L$, the individuals who were treated and untreated are comparable, as if treatment had been randomly assigned within that stratum. This is the key assumption that allows us to use observational data to emulate a randomized trial.

3.  **Positivity** (or **Overlap**): For every combination of covariates $l$ present in the population, there is a non-zero probability of receiving any level of the treatment. Formally, for a binary treatment, $0  P(A=1 \mid L=l)  1$ for all $l$ with $P(L=l) > 0$. This assumption ensures that for any type of subject we want to make inferences about, we actually have some individuals who were treated and some who were untreated, allowing for a direct comparison.

Under these three assumptions, we can identify the ATE. The derivation begins with the law of total expectation:
$E[Y(a)] = E_L[E[Y(a) \mid L]]$
By conditional exchangeability, within strata of $L$, the potential outcome is independent of the treatment received, so $E[Y(a) \mid L] = E[Y(a) \mid L, A=a]$.
By consistency, among those who received treatment $a$, their observed outcome is their potential outcome, so $E[Y(a) \mid L, A=a] = E[Y \mid L, A=a]$.

Combining these steps yields the fundamental **identification formula**, also known as the **g-formula** or **standardization formula**:
$E[Y(a)] = E_L[E[Y \mid L, A=a]]$
In practice, for a discrete confounder $L$, this is calculated as:
$E[Y(a)] = \sum_{l} E[Y \mid A=a, L=l] P(L=l)$

This formula provides a recipe for estimating the marginal mean of a potential outcome: first, compute the mean outcome for treatment level $a$ within each stratum of $L$; then, average these stratum-specific means, weighting by the distribution of $L$ in the target population. The ATE is then identified as $E_L[E[Y \mid A=1, L]] - E_L[E[Y \mid A=0, L]]$. This procedure is known as **stratification** or **standardization**.

### Stratification: The Method in Practice

Stratification is the direct implementation of the identification strategy derived above. The process involves two main steps: stratification and pooling.

#### Stratum-Specific Analysis

The first step is to partition the study population into mutually exclusive and exhaustive strata based on the levels of the confounder(s) $L$. For a discrete confounder set, this is straightforward. Within each stratum $i$ (defined by $L=l_i$), the data can be organized into a [contingency table](@entry_id:164487). For a binary exposure $E$ and binary outcome $Y$, this is a $2 \times 2$ table with counts $a_i, b_i, c_i, d_i$ representing the number of individuals for each combination of exposure and outcome status within that stratum [@problem_id:4973468]. From this table, we can compute a **stratum-specific effect measure**, such as the stratum-specific risk difference $RD_i = P(Y=1|E=1,L=l_i) - P(Y=1|E=0,L=l_i)$ or the stratum-specific odds ratio $OR_i$.

Under the foundational assumptions, this stratum-specific observed association is an unbiased estimate of the stratum-specific causal effect, $CATE(l_i) = E[Y(1) - Y(0) \mid L=l_i]$. For example, returning to the anticoagulant paradox, we found the stratum-specific risk differences were $-0.03$ in the low-severity group and $-0.10$ in the high-severity group, both indicating benefit [@problem_id:4973486].

#### Pooling, Standardization, and the Perils of Non-Collapsibility

To obtain a single summary measure of effect for the entire population, these stratum-specific estimates must be combined. This can be done in several ways. One approach is to assume the effect is constant across strata (**homogeneity of effect**) and compute a pooled estimate, such as the Mantel-Haenszel odds ratio. Another, more flexible approach is to perform standardization as dictated by the g-formula, which allows the effect to vary across strata (**effect modification**). This involves weighting each stratum-specific effect by the proportion of the target population in that stratum.

A critical and often misunderstood aspect of this process is the property of **collapsibility**. An effect measure is collapsible if the marginal (crude) measure is equal to the common conditional (stratum-specific) measure in the absence of confounding. The risk difference and risk ratio are collapsible under the condition of no effect modification. The **odds ratio**, however, is famously **non-collapsible**.

This means that the marginal odds ratio can differ from the common conditional odds ratio even when there is no confounding at all, such as in a randomized trial [@problem_id:4973471]. This is not a sign of bias but a mathematical property stemming from the [non-linearity](@entry_id:637147) of the logit function (by Jensen's inequality, the average of the odds is not the odds of the average probability). Consequently, an adjusted odds ratio from a logistic regression may differ from the unadjusted odds ratio simply because the model includes a prognostic covariate, even if that covariate is perfectly balanced between treatment arms and is not a confounder. This highlights the importance of specifying the target estimand—marginal or conditional—in advance and interpreting adjusted and unadjusted odds ratios with caution.

### Matching: Creating Comparable Groups

While stratification is conceptually straightforward, it becomes impractical when the set of confounders $L$ is large or contains continuous variables. This is due to the **[curse of dimensionality](@entry_id:143920)**: as the number of confounders increases, the number of strata grows exponentially, leading to strata that are sparsely populated or even empty, violating the positivity assumption.

**Matching** is an alternative design-stage strategy to control for confounding. The goal is to select a subset of the unexposed (control) group that is comparable to the exposed (treated) group with respect to the distribution of confounders $L$. In essence, matching aims to approximate a randomized trial by creating balanced exposure groups. Each matched set can be thought of as a single, finely-grained stratum.

#### Types of Matching and Their Analytical Consequences

There are two primary approaches to matching, which carry different implications for the subsequent analysis [@problem_id:4973505].

1.  **Individual Matching**: In this approach, each treated subject is matched to one or more control subjects who share the exact same (or very similar) values of the [confounding variables](@entry_id:199777) $L$. This creates explicit matched sets (e.g., pairs, triplets). A critical consequence is that the analysis must account for this matched structure. Ignoring the matching and performing a standard analysis leads to bias. The correct approach is a **conditional analysis**, such as conditional logistic regression, which effectively treats each matched set as a separate stratum. This method is necessary because the matching process introduces a unique "nuisance parameter" (a baseline risk) for each set, which must be conditioned out to obtain an unbiased estimate of the exposure effect. In the context of 1:1 pair-matched case-control studies, this conditional analysis famously relies only on **[discordant pairs](@entry_id:166371)**—pairs where the case and control have different exposure statuses [@problem_id:4973468].

2.  **Frequency Matching**: This method is less stringent. Controls are sampled to ensure that the overall [marginal distribution](@entry_id:264862) of the confounders $L$ in the control group is the same as in the case (or treated) group. For instance, if 25% of cases are in a certain age group, controls are sampled until 25% of the control group falls in that same age group. This balances the confounder distribution at the aggregate level but does not create explicit matched sets. Because confounding is not eliminated at the individual level, it is still essential to control for $L$ in the analysis stage, for example by including $L$ as a covariate in an unconditional [regression model](@entry_id:163386) or by performing a stratified analysis (e.g., Mantel-Haenszel).

#### Practical Implementation of Matching

In **nearest-neighbor matching**, for each treated unit, one finds the [control unit](@entry_id:165199)(s) "closest" in terms of their covariate values. Key implementation choices involve the with/without replacement decision and the use of calipers [@problem_id:4973436].

*   **Matching with replacement** allows a single [control unit](@entry_id:165199) to be matched to multiple treated units. This can improve the quality of matches (and thus reduce bias) because every treated unit can be paired with its best possible match from the entire control pool. However, it typically increases the variance of the effect estimate because the [effective sample size](@entry_id:271661) of controls is reduced. The reuse of controls also induces correlation, which must be accounted for in variance estimation using methods that handle clustered data [@problem_id:4973436].
*   **Matching without replacement** stipulates that each control can be used only once. This avoids the variance inflation from reuse but may increase bias, as the order of matching matters and some treated units may be forced to accept poorer matches after the best controls have been taken.
*   A **caliper** is a tolerance imposed on the maximum acceptable distance between matched units. It is a pragmatic tool to prevent poor matches that could introduce significant bias, at the cost of potentially being unable to match all treated units.

### The Propensity Score: A Unifying Tool for High Dimensions

The [curse of dimensionality](@entry_id:143920) affects direct covariate matching just as it does stratification. Finding matches on a large vector of covariates $L$ is difficult, as the "nearest" neighbor in a high-dimensional space can still be very far away. The expected distance to a nearest neighbor scales as $n^{-1/p}$ where $p$ is the number of dimensions, meaning distances shrink very slowly for large $p$ [@problem_id:4973440].

The **[propensity score](@entry_id:635864)**, defined as the [conditional probability](@entry_id:151013) of receiving treatment given the covariates, $e(L) = P(A=1 \mid L)$, provides a powerful solution. Rosenbaum and Rubin showed that the [propensity score](@entry_id:635864) is a **balancing score**: conditional on the [propensity score](@entry_id:635864), the distribution of the entire covariate vector $L$ is independent of treatment assignment ($L \perp A \mid e(L)$).

This remarkable property means that instead of having to control for the entire high-dimensional vector $L$, we only need to control for the one-dimensional scalar $e(L)$. By matching or stratifying on the [propensity score](@entry_id:635864), we can achieve balance across all observed confounders simultaneously, thus reducing a $p$-dimensional problem to a one-dimensional one and mitigating the [curse of dimensionality](@entry_id:143920) [@problem_id:4973440].

In practice, the true [propensity score](@entry_id:635864) is unknown and must be estimated, typically using logistic regression or more flexible machine learning models. This introduces new complexities. If the [propensity score](@entry_id:635864) model is misspecified, it will not be a true balancing score, and confounding will not be adequately controlled. Furthermore, the act of estimating the score affects the properties of the final effect estimator. For matching estimators, using an estimated score can lead to a larger [asymptotic variance](@entry_id:269933) than if one had used the true score [@problem_id:4973440]. When using modern machine learning methods to estimate the propensity score, which may have slower convergence rates, special techniques like **cross-fitting** or **sample splitting** are often necessary to prevent the error from the estimation stage from corrupting the final causal estimate [@problem_id:4973440].

### Advanced Considerations: Rigor in Application

Effective confounding control requires attention to several critical details, from verifying assumptions to correctly interpreting results.

#### The Indispensable Role of Positivity

The positivity assumption—that all types of subjects have a non-zero chance of receiving any treatment level—is essential for identification. A **structural violation** occurs when a subgroup is deterministically assigned to one treatment level. For example, if a drug is contraindicated for patients with severe renal impairment ($L=l^*$), then $P(A=1 \mid L=l^*) = 0$ [@problem_id:4973493]. In this case, we have no data on what would happen to these patients if they were treated, so the stratum-specific causal effect is unidentifiable. The overall ATE for the entire population is therefore also unidentifiable. A common strategy in this situation is to **change the estimand**: restrict the analysis to the subpopulation where positivity holds (e.g., patients without severe renal impairment) and estimate the ATE for this restricted group [@problem_id:4973493]. Even without structural violations, **practical violations**—where propensities are very close to 0 or 1—can lead to highly unstable and high-variance estimates due to a lack of effective overlap [@problem_id:4973493].

#### Choosing Covariates: The Backdoor Criterion

The choice of which covariates to include in the set $L$ is paramount. Causal Directed Acyclic Graphs (DAGs) provide a formal framework for this decision. The **[backdoor criterion](@entry_id:637856)** states that a set of covariates $L$ is sufficient to control for confounding if it blocks all non-causal paths between the exposure $A$ and outcome $Y$, and does not include any variables that are descendants of the exposure. This implies three rules for a set $L$ aiming to identify the total effect of $A$ on $Y$ [@problem_id:4973487]:
1.  **Adjust for Confounders**: Variables that are common causes of $A$ and $Y$ must be included in $L$.
2.  **Do Not Adjust for Mediators**: Variables that lie on the causal pathway between $A$ and $Y$ should not be included, as this would block the very effect we wish to measure.
3.  **Do Not Adjust for Colliders**: Variables that are common effects of $A$ and another cause of $Y$ must not be included, as conditioning on them can open a non-causal path and induce bias.

Furthermore, not all valid adjustment sets are equal. Adjusting for a variable that is a strong predictor of treatment but not a confounder (an **instrumental variable**) does not reduce bias but can substantially increase the variance of the effect estimate by removing "good" variation in the exposure [@problem_id:4973487]. Therefore, the goal is often to find a **minimal sufficient adjustment set**.

#### Confounding vs. Effect Modification: A Final Distinction

Finally, it is crucial to distinguish confounding from **effect modification**. A confounder causes bias; an effect modifier causes the magnitude of the causal effect to vary across subgroups. An RCT, by design, eliminates confounding but does not eliminate effect modification. If one were to match on a variable $L$ in an RCT, this would not reduce bias (as there is none to begin with). However, if $L$ is also an effect modifier, this action has a profound consequence: it changes the distribution of the effect modifier in the analysis sample. Since the overall ATE is an average of the stratum-specific effects weighted by the distribution of the modifier, changing this distribution **changes the target estimand** [@problem_id:4973423]. The analysis would no longer be estimating the ATE in the original trial population, but the ATE in a new, synthetic population defined by the matching procedure. This underscores a fundamental principle: our analytical choices do not just affect the accuracy of our answer; they can change the very question we are asking.