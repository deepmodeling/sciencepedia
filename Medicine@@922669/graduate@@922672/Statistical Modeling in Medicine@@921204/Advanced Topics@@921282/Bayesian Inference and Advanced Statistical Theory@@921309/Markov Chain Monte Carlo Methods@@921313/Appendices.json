{"hands_on_practices": [{"introduction": "The heart of the Metropolis-Hastings algorithm is the acceptance probability, which ensures the resulting chain correctly explores the target distribution. This first practice challenges you to derive this probability from its foundational formula in the context of sampling probability vectors, which are common in many statistical models. By working through the derivation [@problem_id:791624], you will gain a deep, mechanical understanding of how the target density and proposal density interact to guide the sampler.", "problem": "Consider the task of sampling from a target probability distribution over the 2-simplex, $\\mathcal{S}_2$, using the Metropolis-Hastings algorithm. The 2-simplex is the set of 3-dimensional probability vectors, defined as $\\mathcal{S}_2 = \\{ p = (p_1, p_2, p_3) \\in \\mathbb{R}^3 \\mid p_i \\ge 0 \\text{ for } i=1,2,3, \\text{ and } \\sum_{i=1}^3 p_i = 1 \\}$.\n\nThe target distribution, $\\pi(p)$, is a Dirichlet distribution with a vector of positive real parameters $\\beta = (\\beta_1, \\beta_2, \\beta_3)$. The probability density function is given by:\n$$\n\\pi(p) \\propto \\prod_{i=1}^3 p_i^{\\beta_i - 1}\n$$\n\nThe proposal mechanism for moving from a current state $p$ to a proposed state $p'$ is also based on the Dirichlet distribution. A new state $p'$ is drawn from a Dirichlet distribution whose parameters are determined by the current state $p$ and a positive concentration parameter $\\lambda$. Specifically, the proposal distribution $q(p'|p)$ is given by:\n$$\np' \\sim \\text{Dir}(\\lambda p_1, \\lambda p_2, \\lambda p_3)\n$$\nThe probability density function for the Dirichlet distribution, $\\text{Dir}(x | \\alpha)$, is given by:\n$$\nf(x_1, \\dots, x_K; \\alpha_1, \\dots, \\alpha_K) = \\frac{\\Gamma(\\sum_{i=1}^K \\alpha_i)}{\\prod_{i=1}^K \\Gamma(\\alpha_i)} \\prod_{i=1}^K x_i^{\\alpha_i - 1}\n$$\nwhere $\\Gamma(\\cdot)$ is the gamma function.\n\nYour task is to derive the Metropolis-Hastings acceptance probability, $\\alpha(p' | p)$, for a transition from a state $p = (p_1, p_2, p_3)$ to a proposed new state $p' = (p'_1, p'_2, p'_3)$. Express your final answer in terms of the components of $p$, $p'$, and the parameters $\\beta_i$ and $\\lambda$.", "solution": "1. The Metropolis–Hastings acceptance probability is\n$$\n\\alpha(p'|p)=\\min\\!\\Bigl(1,\\frac{\\pi(p')\\,q(p\\mid p')}{\\pi(p)\\,q(p'\\mid p)}\\Bigr).\n$$\n2. Using \n$$\\pi(p)\\propto\\prod_{i=1}^3p_i^{\\beta_i-1},\\quad\nq(p'\\mid p)\n=\\frac{\\Gamma(\\sum_i\\lambda p_i)}{\\prod_i\\Gamma(\\lambda p_i)}\\prod_{i=1}^3p'_i{}^{\\,\\lambda p_i-1},\n$$\nand similarly for $q(p\\mid p')$, we get\n$$\n\\frac{\\pi(p')}{\\pi(p)}\n=\\prod_{i=1}^3\\frac{p'_i{}^{\\beta_i-1}}{p_i^{\\beta_i-1}},\\quad\n\\frac{q(p\\mid p')}{q(p'\\mid p)}\n=\\frac{\\prod_{i=1}^3\\Gamma(\\lambda p_i)}{\\prod_{i=1}^3\\Gamma(\\lambda p'_i)}\n\\prod_{i=1}^3\\frac{p_i^{\\lambda p'_i-1}}{p'_i^{\\lambda p_i-1}}.\n$$\n3. Thus\n$$\n\\alpha(p'|p)\n=\\min\\!\\Bigl(1,\\;\\prod_{i=1}^3\\frac{p'_i^{\\beta_i-1}}{p_i^{\\beta_i-1}}\n\\frac{\\prod_{i=1}^3\\Gamma(\\lambda p_i)}{\\prod_{i=1}^3\\Gamma(\\lambda p'_i)}\n\\prod_{i=1}^3\\frac{p_i^{\\lambda p'_i-1}}{p'_i^{\\lambda p_i-1}}\n\\Bigr).\n$$", "answer": "$$\\boxed{\\min\\Bigl(1,\\;\\prod_{i=1}^3\\frac{p'_i^{\\beta_i-1}}{p_i^{\\beta_i-1}}\\frac{\\prod_{i=1}^3\\Gamma(\\lambda p_i)}{\\prod_{i=1}^3\\Gamma(\\lambda p'_i)}\\prod_{i=1}^3\\frac{p_i^{\\lambda p'_i-1}}{p'_i^{\\lambda p_i-1}}\\Bigr)}$$", "id": "791624"}, {"introduction": "With the theoretical foundation in place, the next step is to translate the mathematics into working code. This practice focuses on implementing a single Metropolis-Hastings step for a simple but widely used Normal-Normal model, which is often used for analyzing continuous clinical outcomes. By numerically calculating the acceptance probability for different scenarios [@problem_id:4971664], you will confront practical issues like numerical stability and directly see how the choice of proposal distribution affects the sampler's behavior.", "problem": "Consider a single-parameter Bayesian model commonly used for continuous clinical outcomes where the observed responses are conditionally independent and Normally distributed around a patient-level treatment effect. Let the data be summarized by the sample mean $\\,\\bar{y}\\,$ and the sample size $\\,n\\,$, with known observation variance $\\,\\sigma^2\\,$. Assume a Normal prior for the treatment effect parameter $\\,\\theta\\,$. Specifically:\n- The likelihood is $\\,y_i \\mid \\theta \\sim \\mathcal{N}(\\theta,\\sigma^2)\\,$ independently for $\\,i=1,\\dots,n\\,$.\n- The prior is $\\,\\theta \\sim \\mathcal{N}(\\mu_0,\\tau_0^2)\\,$.\n\nYou will implement a single Metropolis–Hastings (MH) step within Markov chain Monte Carlo (MCMC) for this Normal–Normal model. The algorithm must compute the MH acceptance probability for a given current state $\\,\\theta\\,$ and a proposed state $\\,\\theta'\\,$ under two proposal families:\n- Random-walk Normal proposal: $\\,q(\\theta' \\mid \\theta)=\\mathcal{N}(\\theta, s^2)\\,$.\n- Independence Normal proposal: $\\,q(\\theta' \\mid \\theta)=\\mathcal{N}(\\mu_q,\\sigma_q^2)\\,$.\n\nYour program must:\n1. Start from the fundamental definition of the Bayesian posterior proportional to likelihood times prior, and from the definition of the Metropolis–Hastings acceptance probability, and implement the numerical computation of the acceptance probability using only sufficient statistics $\\,\\bar{y}\\,$ and $\\,n\\,$ for the likelihood.\n2. Compute the log-acceptance ratio to maintain numerical stability, taking careful account of the Hastings correction $\\,q(\\theta \\mid \\theta')/q(\\theta' \\mid \\theta)\\,$ appropriate to the specified proposal family.\n3. For each test case, output the acceptance probability $\\,\\alpha=\\min\\{1,\\exp(\\log \\alpha)\\}\\,$ as a floating-point number.\n\nUse the following test suite of parameter sets, each fully specifying $\\,\\bar{y}\\,$, $\\,n\\,$, $\\,\\sigma^2\\,$, $\\,\\mu_0\\,$, $\\,\\tau_0^2\\,$, the current state $\\,\\theta\\,$, the proposed state $\\,\\theta'\\,$, and the proposal details:\n- Case $\\,1\\,$ (random-walk Normal proposal): $\\,\\bar{y}=1.2\\,$, $\\,n=50\\,$, $\\,\\sigma^2=16\\,$, $\\,\\mu_0=0\\,$, $\\,\\tau_0^2=100\\,$, $\\,\\theta=1.5\\,$, $\\,\\theta'=1.8\\,$, $\\,s^2=0.25\\,$.\n- Case $\\,2\\,$ (independence Normal proposal): $\\,\\bar{y}=8.4\\,$, $\\,n=5\\,$, $\\,\\sigma^2=4\\,$, $\\,\\mu_0=10\\,$, $\\,\\tau_0^2=1\\,$, $\\,\\theta=8\\,$, $\\,\\theta'=6\\,$, $\\,\\mu_q=9\\,$, $\\,\\sigma_q^2=9\\,$.\n- Case $\\,3\\,$ (independence Normal proposal, identical states): $\\,\\bar{y}=0.3\\,$, $\\,n=10\\,$, $\\,\\sigma^2=1\\,$, $\\,\\mu_0=0\\,$, $\\,\\tau_0^2=4\\,$, $\\,\\theta=0.7\\,$, $\\,\\theta'=0.7\\,$, $\\,\\mu_q=0\\,$, $\\,\\sigma_q^2=9\\,$.\n- Case $\\,4\\,$ (random-walk Normal proposal, far-out proposal): $\\,\\bar{y}=0.2\\,$, $\\,n=200\\,$, $\\,\\sigma^2=9\\,$, $\\,\\mu_0=0\\,$, $\\,\\tau_0^2=1000\\,$, $\\,\\theta=0.1\\,$, $\\,\\theta'=5.0\\,$, $\\,s^2=1.0\\,$.\n- Case $\\,5\\,$ (independence Normal proposal with nontrivial Hastings correction): $\\,\\bar{y}=0.6\\,$, $\\,n=5\\,$, $\\,\\sigma^2=1\\,$, $\\,\\mu_0=0\\,$, $\\,\\tau_0^2=10\\,$, $\\,\\theta=0.0\\,$, $\\,\\theta'=0.8\\,$, $\\,\\mu_q=1.0\\,$, $\\,\\sigma_q^2=0.25\\,$.\n\nProgram requirements:\n- Implement the computation entirely in terms of $\\,\\bar{y}\\,$, $\\,n\\,$, $\\,\\sigma^2\\,$, $\\,\\mu_0\\,$, $\\,\\tau_0^2\\,$, $\\,\\theta\\,$, $\\,\\theta'\\,$, and the proposal parameters. Do not use any closed-form posterior beyond the foundational definitions.\n- Use logarithms to compute the acceptance ratio stably, and then convert to the acceptance probability $\\,\\alpha\\,$.\n- For the random-walk Normal proposal, correctly realize that the Hastings correction simplifies due to symmetry; for the independence Normal proposal, compute the correction based on the specified $\\,\\mu_q\\,$ and $\\,\\sigma_q^2\\,$.\n\nFinal output format:\n- Your program should produce a single line of output containing the acceptance probabilities for the five cases as a left square bracket, followed by the five values formatted to six digits after the decimal point and separated by commas, followed by a right square bracket.\n- The outputs are unitless floating-point numbers in $[0,1]$.", "solution": "The problem has been validated and is deemed a well-posed, scientifically sound, and objective problem in computational statistics. It requires the implementation of a single Metropolis-Hastings (MH) step for a Normal-Normal Bayesian model.\n\nThe Metropolis-Hastings acceptance probability, $\\alpha$, for a transition from a current state $\\theta$ to a proposed state $\\theta'$ is given by:\n$$ \\alpha(\\theta', \\theta) = \\min\\left(1, \\frac{\\pi(\\theta')}{\\pi(\\theta)} \\frac{q(\\theta \\mid \\theta')}{q(\\theta' \\mid \\theta)}\\right) $$\nwhere $\\pi(\\cdot)$ is the target posterior probability density function and $q(\\cdot \\mid \\cdot)$ is the proposal probability density function. For numerical stability, computations are performed on the logarithmic scale. The log-acceptance ratio, $\\log R$, is:\n$$ \\log R = \\log(\\pi(\\theta')) - \\log(\\pi(\\theta)) + \\log(q(\\theta \\mid \\theta')) - \\log(q(\\theta' \\mid \\theta)) $$\nThe acceptance probability is then $\\alpha = \\min(1, \\exp(\\log R))$.\n\nThe target posterior density $\\pi(\\theta)$ is proportional to the product of the likelihood $L(\\theta \\mid \\mathbf{y})$ and the prior $p(\\theta)$.\n$$ \\pi(\\theta) \\propto L(\\theta \\mid \\mathbf{y}) \\times p(\\theta) $$\n\n**1. Likelihood and Prior Specification**\n\nThe likelihood for $n$ independent observations $y_i \\sim \\mathcal{N}(\\theta, \\sigma^2)$ is:\n$$ L(\\theta \\mid \\mathbf{y}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\theta)^2}{2\\sigma^2}\\right) $$\nAs requested, we use the sufficient statistics $\\bar{y}$ and $n$. The sum of squares in the exponent can be expanded as $\\sum_{i=1}^{n} (y_i - \\theta)^2 = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 + n(\\bar{y} - \\theta)^2$. Since the term $\\sum (y_i - \\bar{y})^2$ does not depend on $\\theta$, it is a constant of proportionality. Thus, the likelihood, as a function of $\\theta$, is proportional to:\n$$ L(\\theta \\mid \\bar{y}, n) \\propto \\exp\\left(-\\frac{n(\\bar{y} - \\theta)^2}{2\\sigma^2}\\right) $$\nThe prior on $\\theta$ is $\\theta \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$, so the prior density is proportional to:\n$$ p(\\theta) \\propto \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right) $$\nThe unnormalized posterior $\\pi(\\theta)$ is proportional to the product of these two expressions. The log-posterior, up to an additive constant, is:\n$$ \\log \\pi(\\theta) \\stackrel{c}{=} -\\frac{n(\\bar{y} - \\theta)^2}{2\\sigma^2} - \\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2} $$\nLet this function be denoted as $\\text{log_target}(\\theta)$. The term $\\log(\\pi(\\theta')) - \\log(\\pi(\\theta))$ in the log-acceptance ratio becomes $\\text{log_target}(\\theta') - \\text{log_target}(\\theta)$.\n\n**2. Proposal Densities and Hastings Correction**\n\nWe analyze the two specified proposal families to determine the Hastings correction term, $\\log(q(\\theta \\mid \\theta')) - \\log(q(\\theta' \\mid \\theta))$.\n\n**a. Random-walk Normal Proposal**\nThe proposal is $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta, s^2)$. The log-proposal density, ignoring constants, is:\n$$ \\log q(\\theta' \\mid \\theta) \\stackrel{c}{=} -\\frac{(\\theta' - \\theta)^2}{2s^2} $$\nThe density for the reverse move, from $\\theta'$ to $\\theta$, is $q(\\theta \\mid \\theta') = \\mathcal{N}(\\theta', s^2)$. Its log-density is:\n$$ \\log q(\\theta \\mid \\theta') \\stackrel{c}{=} -\\frac{(\\theta - \\theta')^2}{2s^2} $$\nSince $(\\theta' - \\theta)^2 = (\\theta - \\theta')^2$, the proposal distribution is symmetric, i.e., $q(\\theta' \\mid \\theta) = q(\\theta \\mid \\theta')$. The Hastings correction term is $\\log(1) = 0$.\nThe log-acceptance ratio simplifies to:\n$$ \\log R_{\\text{RW}} = \\text{log_target}(\\theta') - \\text{log_target}(\\theta) $$\n\n**b. Independence Normal Proposal**\nThe proposal is $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\mu_q, \\sigma_q^2)$, which does not depend on the current state $\\theta$. We can write $q(\\theta' \\mid \\theta) = q(\\theta')$. The log-proposal density is:\n$$ \\log q(\\theta' \\mid \\theta) = \\log q(\\theta') \\stackrel{c}{=} -\\frac{(\\theta' - \\mu_q)^2}{2\\sigma_q^2} $$\nThe reverse proposal density is $q(\\theta \\mid \\theta') = q(\\theta) = \\mathcal{N}(\\mu_q, \\sigma_q^2)$:\n$$ \\log q(\\theta \\mid \\theta') = \\log q(\\theta) \\stackrel{c}{=} -\\frac{(\\theta - \\mu_q)^2}{2\\sigma_q^2} $$\nThe log-Hastings correction is non-zero:\n$$ \\log\\left(\\frac{q(\\theta \\mid \\theta')}{q(\\theta' \\mid \\theta)}\\right) = \\log q(\\theta) - \\log q(\\theta') = \\left(-\\frac{(\\theta - \\mu_q)^2}{2\\sigma_q^2}\\right) - \\left(-\\frac{(\\theta' - \\mu_q)^2}{2\\sigma_q^2}\\right) = \\frac{(\\theta' - \\mu_q)^2 - (\\theta - \\mu_q)^2}{2\\sigma_q^2} $$\nThe full log-acceptance ratio is:\n$$ \\log R_{\\text{IND}} = (\\text{log_target}(\\theta') - \\text{log_target}(\\theta)) + \\frac{(\\theta' - \\mu_q)^2 - (\\theta - \\mu_q)^2}{2\\sigma_q^2} $$\n\n**3. Implementation Strategy**\n\nThe Python implementation will define a function to compute $\\text{log_target}(\\theta)$ given the model parameters. It will then iterate through the test cases. For each case, it will identify the proposal type and compute the log-acceptance ratio $\\log R$ using the appropriate formula derived above. Finally, it will compute $\\alpha = \\min(1, \\exp(\\log R))$ and store the result.\n\nSpecial case: For Case $3$, $\\theta = \\theta' = 0.7$. In this situation, $\\text{log_target}(\\theta') = \\text{log_target}(\\theta)$, and the Hastings correction term also evaluates to $0$, irrespective of the proposal family. Thus, $\\log R = 0$, and $\\alpha = \\min(1, \\exp(0)) = 1$. The program should correctly handle this.\nFor Case $4$, the proposed state $\\theta'=5.0$ is far in the tail of the posterior, which is concentrated around $\\bar{y}=0.2$. This will lead to a large negative $\\log R$ and an acceptance probability that is computationally zero.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Metropolis-Hastings acceptance probability for a Normal-Normal model\n    under two different proposal families for a series of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            'case': 1, 'proposal_type': 'random_walk',\n            'params': {'y_bar': 1.2, 'n': 50, 'sigma2': 16, 'mu0': 0, 'tau02': 100},\n            'states': {'theta': 1.5, 'theta_prime': 1.8},\n            'proposal_params': {'s2': 0.25}\n        },\n        {\n            'case': 2, 'proposal_type': 'independence',\n            'params': {'y_bar': 8.4, 'n': 5, 'sigma2': 4, 'mu0': 10, 'tau02': 1},\n            'states': {'theta': 8, 'theta_prime': 6},\n            'proposal_params': {'mu_q': 9, 'sigma_q2': 9}\n        },\n        {\n            'case': 3, 'proposal_type': 'independence',\n            'params': {'y_bar': 0.3, 'n': 10, 'sigma2': 1, 'mu0': 0, 'tau02': 4},\n            'states': {'theta': 0.7, 'theta_prime': 0.7},\n            'proposal_params': {'mu_q': 0, 'sigma_q2': 9}\n        },\n        {\n            'case': 4, 'proposal_type': 'random_walk',\n            'params': {'y_bar': 0.2, 'n': 200, 'sigma2': 9, 'mu0': 0, 'tau02': 1000},\n            'states': {'theta': 0.1, 'theta_prime': 5.0},\n            'proposal_params': {'s2': 1.0}\n        },\n        {\n            'case': 5, 'proposal_type': 'independence',\n            'params': {'y_bar': 0.6, 'n': 5, 'sigma2': 1, 'mu0': 0, 'tau02': 10},\n            'states': {'theta': 0.0, 'theta_prime': 0.8},\n            'proposal_params': {'mu_q': 1.0, 'sigma_q2': 0.25}\n        }\n    ]\n\n    results = []\n\n    def log_target_density(theta, y_bar, n, sigma2, mu0, tau02):\n        \"\"\"\n        Computes the log of the unnormalized posterior density (likelihood * prior).\n        This is proportional to log(pi(theta)).\n        \"\"\"\n        log_likelihood = -n * (y_bar - theta)**2 / (2 * sigma2)\n        log_prior = -(theta - mu0)**2 / (2 * tau02)\n        return log_likelihood + log_prior\n\n    for case in test_cases:\n        params = case['params']\n        states = case['states']\n        proposal_params = case['proposal_params']\n        \n        theta = states['theta']\n        theta_prime = states['theta_prime']\n\n        # If current and proposed states are identical, acceptance probability is 1.\n        if theta == theta_prime:\n            results.append(1.0)\n            continue\n\n        # Calculate the log of the posterior ratio\n        log_pi_ratio = log_target_density(theta_prime, **params) - log_target_density(theta, **params)\n\n        log_hastings_correction = 0.0\n\n        if case['proposal_type'] == 'random_walk':\n            # For a symmetric random-walk proposal, q(theta|theta') = q(theta'|theta),\n            # so the Hastings correction is 1, and its log is 0.\n            pass  # log_hastings_correction is already 0.0\n\n        elif case['proposal_type'] == 'independence':\n            # For an independence sampler, q(theta'|theta) = q(theta'),\n            # so the Hastings correction is q(theta)/q(theta').\n            mu_q = proposal_params['mu_q']\n            sigma_q2 = proposal_params['sigma_q2']\n            \n            # log(q(theta)) - log(q(theta'))\n            # The constant terms cancel.\n            log_q_theta = -(theta - mu_q)**2 / (2 * sigma_q2)\n            log_q_theta_prime = -(theta_prime - mu_q)**2 / (2 * sigma_q2)\n\n            log_hastings_correction = log_q_theta - log_q_theta_prime\n\n        # Total log acceptance ratio\n        log_acceptance_ratio = log_pi_ratio + log_hastings_correction\n        \n        # Acceptance probability\n        alpha = min(1.0, np.exp(log_acceptance_ratio))\n        results.append(alpha)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "4971664"}, {"introduction": "Real-world statistical models, especially in medicine, are often hierarchical. This final practice moves into this more complex and realistic setting, addressing a critical issue in applied MCMC: sampler efficiency. You will explore how the mathematical formulation of a model—specifically, a centered versus a non-centered parameterization—can drastically impact the performance of a Gibbs sampler. By implementing both samplers and quantifying the difference in mixing using autocorrelation [@problem_id:3144797], you will gain hands-on experience with a powerful technique for making MCMC methods practical for complex problems.", "problem": "Consider a hierarchical normal model where observed group-level data are modeled with latent group means and a population-level mean. The objective is to analyze the effect of reparameterization on Markov chain mixing and quantify differences in autocorrelation for the population mean parameter by comparing a centered parameterization against a non-centered parameterization. The comparison must be performed using Markov Chain Monte Carlo (MCMC), specifically Gibbs sampling, for a scenario with known observational variance and known group-level variance.\n\nFundamental base:\n- Use Bayes' theorem and normal-normal conjugacy to derive conditional distributions.\n- Use the definition of a Markov chain and the concept of Markov Chain Monte Carlo (MCMC), where the chain transitions are constructed to have a target posterior distribution as the stationary distribution.\n- Gibbs sampling is a special case of MCMC where one samples each variable from its full conditional distribution in sequence.\n- Autocorrelation at lag one is defined as the correlation between successive samples of a stationary time series.\n\nModel specification:\n- For each group index $j \\in \\{1, \\dots, J\\}$, the observed data $y_j$ are modeled as $y_j \\mid \\mu_j \\sim \\mathcal{N}(\\mu_j, \\sigma^2)$, where $\\sigma^2$ is known.\n- The latent group mean has the hierarchical prior $\\mu_j \\mid \\mu \\sim \\mathcal{N}(\\mu, \\tau^2)$, where $\\tau^2$ is known.\n- The population mean has a prior $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$, with $m_0$ and $s_0^2$ known.\n\nTwo parameterizations:\n- Centered parameterization: directly parameterize with $\\mu$ and $\\{\\mu_j\\}_{j=1}^J$.\n- Non-centered parameterization: reparameterize by introducing $\\eta_j \\sim \\mathcal{N}(0,1)$ and set $\\mu_j = \\mu + \\tau \\eta_j$, thereby parameterizing with $\\mu$ and $\\{\\eta_j\\}_{j=1}^J$.\n\nTask:\n- Derive the full conditional distributions required for Gibbs sampling under both parameterizations from Bayes' theorem and the properties of normal distributions. Do not use shortcut formulas; start from the fundamental definitions of the likelihood and priors.\n- Implement two Gibbs samplers corresponding to the centered and the non-centered parameterizations.\n- For each sampler, collect samples of the population mean $\\mu$ and compute the lag-$1$ autocorrelation coefficient for the post-burn-in $\\mu$ chain. For a time series $\\{x_t\\}_{t=1}^N$, the lag-$1$ autocorrelation is defined as\n$$\n\\rho_1 = \\frac{\\sum_{t=1}^{N-1} (x_t - \\bar{x})(x_{t+1} - \\bar{x})}{\\sum_{t=1}^{N} (x_t - \\bar{x})^2},\n$$\nwhere $\\bar{x}$ is the sample mean of $\\{x_t\\}_{t=1}^N$.\n- Quantify the difference in autocorrelation by computing $\\Delta = \\rho_{1,\\text{centered}} - \\rho_{1,\\text{non-centered}}$ for each test case. A positive $\\Delta$ indicates higher autocorrelation in the centered parameterization compared to the non-centered parameterization, and a negative $\\Delta$ indicates the reverse.\n\nTest suite:\nUse the following cases, each specified by $(J, \\{y_j\\}_{j=1}^J, \\sigma, \\tau, m_0, s_0, N_{\\text{iter}}, N_{\\text{burn}}, \\text{seed})$, with all quantities given explicitly as numbers.\n\n- Case $1$ (weak data, expect non-centered to mix better): $J = 8$, $\\{y_j\\} = [5, 7, 3, -2, 0, 4, 6, 8]$, $\\sigma = 5$, $\\tau = 1$, $m_0 = 0$, $s_0 = 10$, $N_{\\text{iter}} = 10000$, $N_{\\text{burn}} = 2000$, $\\text{seed} = 123$.\n- Case $2$ (strong data, expect centered to mix better): $J = 8$, $\\{y_j\\} = [5, 7, 3, -2, 0, 4, 6, 8]$, $\\sigma = 1$, $\\tau = 5$, $m_0 = 0$, $s_0 = 10$, $N_{\\text{iter}} = 10000$, $N_{\\text{burn}} = 2000$, $\\text{seed} = 456$.\n- Case $3$ (boundary case with a single group): $J = 1$, $\\{y_j\\} = [3]$, $\\sigma = 5$, $\\tau = 2$, $m_0 = 0$, $s_0 = 10$, $N_{\\text{iter}} = 10000$, $N_{\\text{burn}} = 2000$, $\\text{seed} = 789$.\n- Case $4$ (many groups, moderate information): $J = 50$, $\\{y_j\\}$ defined deterministically by $y_j = 0.3 \\times (j - 25.5)$ for $j = 1, \\dots, 50$, $\\sigma = 2$, $\\tau = 2$, $m_0 = 0$, $s_0 = 10$, $N_{\\text{iter}} = 10000$, $N_{\\text{burn}} = 2000$, $\\text{seed} = 987$.\n\nRequirements:\n- Implement both Gibbs samplers and compute $\\Delta$ for each case.\n- Use the seeds provided to ensure deterministic outputs.\n- There are no physical units involved in this problem.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the four cases, for example, $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$.", "solution": "The user requests a comparison of centered and non-centered parameterizations for a hierarchical normal model using Gibbs sampling. The primary goal is to analyze the difference in Markov chain Monte Carlo (MCMC) mixing properties by computing the lag-$1$ autocorrelation of the chain for the population mean parameter, $\\mu$.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Model:**\n    -   Data likelihood: $y_j \\mid \\mu_j \\sim \\mathcal{N}(\\mu_j, \\sigma^2)$ for $j \\in \\{1, \\dots, J\\}$, with $\\sigma^2$ known.\n    -   Hierarchical prior for group means: $\\mu_j \\mid \\mu \\sim \\mathcal{N}(\\mu, \\tau^2)$, with $\\tau^2$ known.\n    -   Prior for population mean: $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$, with $m_0$ and $s_0^2$ known.\n-   **Parameterizations:**\n    -   Centered: Parameters are $\\mu$ and $\\{\\mu_j\\}_{j=1}^J$.\n    -   Non-centered: Reparameterize with $\\eta_j \\sim \\mathcal{N}(0,1)$ such that $\\mu_j = \\mu + \\tau \\eta_j$. Parameters are $\\mu$ and $\\{\\eta_j\\}_{j=1}^J$.\n-   **Task:**\n    1.  Derive full conditional distributions for Gibbs sampling under both parameterizations.\n    2.  Implement two Gibbs samplers.\n    3.  Compute the lag-$1$ autocorrelation $\\rho_1$ for the post-burn-in chain of $\\mu$ for each sampler. The formula is $\\rho_1 = \\frac{\\sum_{t=1}^{N-1} (x_t - \\bar{x})(x_{t+1} - \\bar{x})}{\\sum_{t=1}^{N} (x_t - \\bar{x})^2}$.\n    4.  Compute the difference $\\Delta = \\rho_{1,\\text{centered}} - \\rho_{1,\\text{non-centered}}$ for each test case.\n-   **Test Suite:**\n    -   Case $1$: $(J, \\{y_j\\}, \\sigma, \\tau, m_0, s_0, N_{\\text{iter}}, N_{\\text{burn}}, \\text{seed}) = (8, [5, 7, 3, -2, 0, 4, 6, 8], 5, 1, 0, 10, 10000, 2000, 123)$.\n    -   Case $2$: $(J, \\{y_j\\}, \\sigma, \\tau, m_0, s_0, N_{\\text{iter}}, N_{\\text{burn}}, \\text{seed}) = (8, [5, 7, 3, -2, 0, 4, 6, 8], 1, 5, 0, 10, 10000, 2000, 456)$.\n    -   Case $3$: $(J, \\{y_j\\}, \\sigma, \\tau, m_0, s_0, N_{\\text{iter}}, N_{\\text{burn}}, \\text{seed}) = (1, [3], 5, 2, 0, 10, 10000, 2000, 789)$.\n    -   Case $4$: $(J, \\{y_j\\}, \\sigma, \\tau, m_0, s_0, N_{\\text{iter}}, N_{\\text{burn}}, \\text{seed}) = (50, \\{0.3 \\times (j - 25.5)\\}_{j=1}^{50}, 2, 2, 0, 10, 10000, 2000, 987)$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is a standard exercise in computational Bayesian statistics, focused on MCMC efficiency.\n-   **Scientific Grounding**: The hierarchical normal model, Gibbs sampling, and the concept of reparameterization to improve mixing are fundamental topics in Bayesian statistics and statistical learning. All models and formulas are based on established probability theory and statistics.\n-   **Well-Posedness**: The problem provides all necessary data, parameters, and a deterministic procedure (including random seeds) to arrive at a unique, meaningful numerical result for each test case.\n-   **Objectivity**: The language is precise and technical. There are no subjective elements.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of Full Conditional Distributions\n\nThe core of a Gibbs sampler is to iteratively draw samples for each parameter from its full conditional distribution, which is the distribution of that parameter conditioned on all other parameters and the data. We derive these distributions for both parameterizations using Bayes' theorem, which states that the posterior is proportional to the likelihood times the prior, $p(\\theta|D) \\propto p(D|\\theta)p(\\theta)$.\n\n#### 1. Centered Parameterization\n\nThe parameters are $\\theta_C = (\\mu, \\mu_1, \\dots, \\mu_J)$. The joint posterior distribution is:\n$$p(\\mu, \\{\\mu_j\\}_{j=1}^J | \\{y_j\\}_{j=1}^J) \\propto p(\\{y_j\\} | \\{\\mu_j\\}) p(\\{\\mu_j\\} | \\mu) p(\\mu)$$\n$$p(\\mu, \\{\\mu_j\\} | \\{y_j\\}) \\propto \\left( \\prod_{j=1}^J \\mathcal{N}(y_j | \\mu_j, \\sigma^2) \\right) \\left( \\prod_{j=1}^J \\mathcal{N}(\\mu_j | \\mu, \\tau^2) \\right) \\mathcal{N}(\\mu | m_0, s_0^2)$$\n\n**Full Conditional for $\\mu$:**\nThe distribution of $\\mu$ conditional on all other parameters only depends on those parameters on which it is a parent in the graphical model, which are the $\\{\\mu_j\\}$.\n$$p(\\mu | \\{\\mu_j\\}, \\{y_j\\}) \\propto p(\\mu | \\{\\mu_j\\}) \\propto p(\\{\\mu_j\\} | \\mu) p(\\mu)$$\n$$p(\\mu | \\{\\mu_j\\}) \\propto \\left( \\prod_{j=1}^J \\mathcal{N}(\\mu_j | \\mu, \\tau^2) \\right) \\mathcal{N}(\\mu | m_0, s_0^2)$$\n$$p(\\mu | \\{\\mu_j\\}) \\propto \\exp\\left( -\\frac{1}{2\\tau^2} \\sum_{j=1}^J (\\mu_j - \\mu)^2 \\right) \\exp\\left( -\\frac{1}{2s_0^2} (\\mu - m_0)^2 \\right)$$\nExpanding the quadratic terms in $\\mu$ in the exponent:\n$$-\\frac{1}{2}\\left[ \\mu^2 \\left(\\frac{J}{\\tau^2} + \\frac{1}{s_0^2}\\right) - 2\\mu \\left(\\frac{1}{\\tau^2}\\sum_{j=1}^J \\mu_j + \\frac{m_0}{s_0^2}\\right) + \\text{const} \\right]$$\nThis is the kernel of a normal distribution $\\mathcal{N}(\\mu | M_{\\mu}, V_{\\mu})$. By completing the square or matching terms, we find the posterior variance $V_{\\mu}$ and mean $M_{\\mu}$:\n$$V_{\\mu} = \\left(\\frac{J}{\\tau^2} + \\frac{1}{s_0^2}\\right)^{-1}$$\n$$M_{\\mu} = V_{\\mu} \\left(\\frac{\\sum_{j=1}^J \\mu_j}{\\tau^2} + \\frac{m_0}{s_0^2}\\right)$$\nSo, the full conditional is $\\mu | \\{\\mu_j\\} \\sim \\mathcal{N}(M_{\\mu}, V_{\\mu})$.\n\n**Full Conditional for $\\mu_j$:**\nDue to conditional independence in the model, the full conditional for $\\mu_j$ only depends on $y_j$ and $\\mu$.\n$$p(\\mu_j | \\mu, \\{\\mu_{k \\neq j}\\}, \\{y_k\\}) \\propto p(y_j | \\mu_j) p(\\mu_j | \\mu)$$\n$$p(\\mu_j | \\mu, y_j) \\propto \\mathcal{N}(y_j|\\mu_j, \\sigma^2) \\mathcal{N}(\\mu_j|\\mu, \\tau^2)$$\n$$p(\\mu_j | \\mu, y_j) \\propto \\exp\\left( -\\frac{(y_j - \\mu_j)^2}{2\\sigma^2} \\right) \\exp\\left( -\\frac{(\\mu_j - \\mu)^2}{2\\tau^2} \\right)$$\nExpanding the quadratic terms in $\\mu_j$ in the exponent:\n$$-\\frac{1}{2}\\left[ \\mu_j^2 \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2}\\right) - 2\\mu_j \\left(\\frac{y_j}{\\sigma^2} + \\frac{\\mu}{\\tau^2}\\right) + \\text{const} \\right]$$\nThis is the kernel of a normal distribution $\\mathcal{N}(\\mu_j | M_j, V_j)$ with variance $V_j$ and mean $M_j$:\n$$V_j = \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)^{-1}$$\n$$M_j = V_j \\left(\\frac{y_j}{\\sigma^2} + \\frac{\\mu}{\\tau^2}\\right)$$\nThe full conditional is $\\mu_j | \\mu, y_j \\sim \\mathcal{N}(M_j, V_j)$ for each $j \\in \\{1, \\dots, J\\}$.\n\n#### 2. Non-Centered Parameterization\n\nThe parameters are $\\theta_{NC} = (\\mu, \\eta_1, \\dots, \\eta_J)$, with the mapping $\\mu_j = \\mu + \\tau \\eta_j$. The likelihood is now $y_j | \\mu, \\eta_j \\sim \\mathcal{N}(\\mu + \\tau\\eta_j, \\sigma^2)$. The priors are $\\eta_j \\sim \\mathcal{N}(0, 1)$ and $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$. The joint posterior is:\n$$p(\\mu, \\{\\eta_j\\} | \\{y_j\\}) \\propto \\left( \\prod_{j=1}^J \\mathcal{N}(y_j | \\mu + \\tau\\eta_j, \\sigma^2) \\right) \\left( \\prod_{j=1}^J \\mathcal{N}(\\eta_j | 0, 1) \\right) \\mathcal{N}(\\mu | m_0, s_0^2)$$\n\n**Full Conditional for $\\mu$:**\n$$p(\\mu | \\{\\eta_j\\}, \\{y_j\\}) \\propto \\left( \\prod_{j=1}^J \\mathcal{N}(y_j | \\mu + \\tau\\eta_j, \\sigma^2) \\right) \\mathcal{N}(\\mu | m_0, s_0^2)$$\n$$p(\\mu | \\{\\eta_j\\}, \\{y_j\\}) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{j=1}^J (y_j - \\tau\\eta_j - \\mu)^2 \\right) \\exp\\left( -\\frac{(\\mu - m_0)^2}{2s_0^2} \\right)$$\nThis is a standard normal-normal conjugate update where we have $J$ observations $y'_j = y_j - \\tau\\eta_j$, each with mean $\\mu$ and known variance $\\sigma^2$. The full conditional for $\\mu$ is $\\mathcal{N}(\\mu | M_{\\mu, nc}, V_{\\mu, nc})$ with:\n$$V_{\\mu, nc} = \\left(\\frac{J}{\\sigma^2} + \\frac{1}{s_0^2}\\right)^{-1}$$\n$$M_{\\mu, nc} = V_{\\mu, nc} \\left(\\frac{\\sum_{j=1}^J (y_j - \\tau\\eta_j)}{\\sigma^2} + \\frac{m_0}{s_0^2}\\right)$$\n\n**Full Conditional for $\\eta_j$:**\n$$p(\\eta_j | \\mu, \\{\\eta_{k \\neq j}\\}, \\{y_k\\}) \\propto p(y_j | \\mu, \\eta_j) p(\\eta_j)$$\n$$p(\\eta_j | \\mu, y_j) \\propto \\mathcal{N}(y_j|\\mu + \\tau\\eta_j, \\sigma^2) \\mathcal{N}(\\eta_j|0, 1)$$\n$$p(\\eta_j | \\mu, y_j) \\propto \\exp\\left( -\\frac{(y_j - \\mu - \\tau\\eta_j)^2}{2\\sigma^2} \\right) \\exp\\left( -\\frac{\\eta_j^2}{2} \\right)$$\nExpanding the quadratic terms in $\\eta_j$ in the exponent:\n$$-\\frac{1}{2}\\left[ \\eta_j^2 \\left(\\frac{\\tau^2}{\\sigma^2} + 1\\right) - 2\\eta_j \\frac{\\tau(y_j - \\mu)}{\\sigma^2} + \\text{const} \\right]$$\nThis is the kernel of a normal distribution $\\mathcal{N}(\\eta_j | M_{\\eta_j}, V_{\\eta_j})$ with:\n$$V_{\\eta_j} = \\left(\\frac{\\tau^2}{\\sigma^2} + 1\\right)^{-1}$$\n$$M_{\\eta_j} = V_{\\eta_j} \\left(\\frac{\\tau(y_j - \\mu)}{\\sigma^2}\\right)$$\nThe full conditional is $\\eta_j | \\mu, y_j \\sim \\mathcal{N}(M_{\\eta_j}, V_{\\eta_j})$ for each $j \\in \\{1, \\dots, J\\}$.\n\n### MCMC Performance and Implementation\n\nThe two parameterizations are mathematically equivalent but can have vastly different computational properties.\n-   **Centered Parameterization:** The parameters $\\mu$ and $\\{\\mu_j\\}$ are often highly correlated in the posterior, especially when data is sparse ($\\sigma^2$ is large) and the hierarchical variance is small ($\\tau^2$ is small). This dependency means a draw for $\\mu$ strongly constrains the subsequent draws for $\\{\\mu_j\\}$, and vice-versa. This \"stickiness\" causes the Gibbs sampler to move slowly through the parameter space, resulting in high autocorrelation between successive samples.\n-   **Non-Centered Parameterization:** This approach aims to break the posterior dependency by sampling $\\mu$ and a set of independent standard normal parameters $\\{\\eta_j\\}$. The dependency is moved from the priors into the likelihood. This often reduces posterior correlation between the blocks of parameters being sampled, leading to faster mixing and lower autocorrelation, particularly in the sparse data scenarios mentioned above.\n-   Conversely, when data is informative ($\\sigma^2$ is small), the centered parameterization can perform well because the likelihood for each $y_j$ identifies $\\mu_j$ precisely, reducing its dependence on $\\mu$. The non-centered parameterization can struggle in this regime because a strong constraint $y_j \\approx \\mu + \\tau\\eta_j$ can induce high posterior correlation between $\\mu$ and $\\eta_j$.\n\nThe implementation will consist of two Gibbs sampler functions, one for each parameterization. Each function will iteratively sample from the derived full conditional distributions for $N_{\\text{iter}}$ steps. After discarding the first $N_{\\text{burn}}$ samples (the burn-in period), the lag-$1$ autocorrelation of the chain for $\\mu$ will be computed. The final result for each test case is the difference $\\Delta$ between the autocorrelations from the two samplers.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_autocorr_lag1(chain):\n    \"\"\"\n    Computes the lag-1 autocorrelation for a given MCMC chain.\n    \"\"\"\n    if len(chain)  2:\n        return np.nan\n    \n    x_bar = np.mean(chain)\n    dev = chain - x_bar\n    denominator = np.dot(dev, dev)\n    \n    if denominator == 0:\n        return 0.0\n        \n    numerator = np.dot(dev[:-1], dev[1:])\n    return numerator / denominator\n\ndef gibbs_centered(J, y, sigma, tau, m0, s0, N_iter, N_burn, seed):\n    \"\"\"\n    Gibbs sampler for the centered parameterization.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    y = np.array(y)\n    \n    sigma2 = sigma**2\n    tau2 = tau**2\n    s02 = s0**2\n    \n    # Initialize parameters\n    mu = float(m0)\n    mu_j = np.copy(y)\n    \n    mu_samples = np.zeros(N_iter)\n    \n    # Pre-compute conditional posterior variances (constant throughout sampling)\n    V_j = 1.0 / (1.0/sigma2 + 1.0/tau2)\n    V_mu = 1.0 / (J/tau2 + 1.0/s02)\n    \n    std_j = np.sqrt(V_j)\n    std_mu = np.sqrt(V_mu)\n\n    # Gibbs sampling loop\n    for i in range(N_iter):\n        # Update mu (population mean)\n        M_mu = V_mu * (np.sum(mu_j)/tau2 + m0/s02)\n        mu = rng.normal(loc=M_mu, scale=std_mu)\n        mu_samples[i] = mu\n\n        # Update mu_j (group means)\n        M_j = V_j * (y/sigma2 + mu/tau2)\n        mu_j = rng.normal(loc=M_j, scale=std_j)\n        \n    post_burn_in_samples = mu_samples[N_burn:]\n    return calculate_autocorr_lag1(post_burn_in_samples)\n\ndef gibbs_non_centered(J, y, sigma, tau, m0, s0, N_iter, N_burn, seed):\n    \"\"\"\n    Gibbs sampler for the non-centered parameterization.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    y = np.array(y)\n    \n    sigma2 = sigma**2\n    tau2 = tau**2\n    s02 = s0**2\n    \n    # Initialize parameters\n    mu = float(m0)\n    eta_j = np.zeros(J)\n    \n    mu_samples = np.zeros(N_iter)\n\n    # Pre-compute conditional posterior variances (constant throughout sampling)\n    V_mu_nc = 1.0 / (J/sigma2 + 1.0/s02)\n    V_eta_j = 1.0 / (tau2/sigma2 + 1.0)\n    \n    std_mu_nc = np.sqrt(V_mu_nc)\n    std_eta_j = np.sqrt(V_eta_j)\n\n    # Gibbs sampling loop\n    for i in range(N_iter):\n        # Update mu\n        M_mu_nc = V_mu_nc * (np.sum(y - tau * eta_j)/sigma2 + m0/s02)\n        mu = rng.normal(loc=M_mu_nc, scale=std_mu_nc)\n        mu_samples[i] = mu\n\n        # Update eta_j\n        M_eta_j = V_eta_j * (tau * (y - mu) / sigma2)\n        eta_j = rng.normal(loc=M_eta_j, scale=std_eta_j)\n        \n    post_burn_in_samples = mu_samples[N_burn:]\n    return calculate_autocorr_lag1(post_burn_in_samples)\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and compute the difference in autocorrelation.\n    \"\"\"\n    y_case4 = [0.3 * (j - 25.5) for j in range(1, 51)]\n\n    test_cases = [\n        # (J, {y_j}, sigma, tau, m0, s0, N_iter, N_burn, seed)\n        (8, [5, 7, 3, -2, 0, 4, 6, 8], 5, 1, 0, 10, 10000, 2000, 123),\n        (8, [5, 7, 3, -2, 0, 4, 6, 8], 1, 5, 0, 10, 10000, 2000, 456),\n        (1, [3], 5, 2, 0, 10, 10000, 2000, 789),\n        (50, y_case4, 2, 2, 0, 10, 10000, 2000, 987),\n    ]\n\n    results = []\n    for case in test_cases:\n        J, y, sigma, tau, m0, s0, N_iter, N_burn, seed = case\n        \n        # Run centered sampler\n        rho_centered = gibbs_centered(J, y, sigma, tau, m0, s0, N_iter, N_burn, seed)\n        \n        # Run non-centered sampler\n        rho_non_centered = gibbs_non_centered(J, y, sigma, tau, m0, s0, N_iter, N_burn, seed)\n        \n        # Compute the difference\n        delta_rho = rho_centered - rho_non_centered\n        results.append(delta_rho)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3144797"}]}