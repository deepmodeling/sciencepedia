{"hands_on_practices": [{"introduction": "Understanding the properties of estimators often begins with the fundamental task of estimating the variance of a normal distribution. This exercise [@problem_id:4831014] provides a hands-on derivation of the bias for the Maximum Likelihood Estimator of variance. It concretely illustrates the classic trade-off between bias and variance and clarifies why a biased estimator can sometimes be preferred in terms of mean squared error.", "problem": "A biostatistics team analyzing variability in a continuous biomarker across independent patients assumes that repeated measurements $X_{1},\\dots,X_{n}$ are independent and identically distributed draws from a normal distribution $N(\\mu,\\sigma^{2})$ with unknown mean $\\mu$ and variance $\\sigma^{2}$. Consider two estimators of the variance: the Maximum Likelihood Estimator (MLE), defined by $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}=\\frac{1}{n}\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}$, and the unbiased sample variance with denominator $n-1$, defined by $\\hat{\\sigma}^{2}_{U}=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}$, where $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. Starting from the definitions of bias, variance, and mean squared error, and from standard distributional properties of the normal model (in particular, the chi-square distribution of scaled sums of squared centered normals), derive the exact finite-sample bias of $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ as a function of $n$ and $\\sigma^{2}$. Then, compare the mean squared error of $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ to that of $\\hat{\\sigma}^{2}_{U}$ for finite $n$, and discuss which estimator is more efficient in the mean squared error sense and whether both estimators are consistent as $n\\to\\infty$. Report only the exact finite-sample bias of $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ as your final answer. No rounding is required.", "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and complete. It is a standard problem in mathematical statistics concerning the properties of estimators for the variance of a normal distribution. We proceed with the solution.\n\nLet the repeated measurements be $X_{1}, \\dots, X_{n}$, which are independent and identically distributed (i.i.d.) random variables from a normal distribution $N(\\mu, \\sigma^{2})$. The sample mean is $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n}X_{i}$.\n\nThe two estimators for the variance $\\sigma^{2}$ are given as:\n$1$. The Maximum Likelihood Estimator (MLE): $\\hat{\\sigma}^{2}_{\\mathrm{MLE}} = \\frac{1}{n}\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}$.\n$2$. The unbiased sample variance: $\\hat{\\sigma}^{2}_{U} = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}$.\n\nWe will analyze these estimators based on their bias, variance, mean squared error (MSE), and consistency. The bias of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $\\mathrm{Bias}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$. The MSE is defined as $\\mathrm{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^{2}]$, which can be decomposed as $\\mathrm{MSE}(\\hat{\\theta}) = \\mathrm{Var}(\\hat{\\theta}) + (\\mathrm{Bias}(\\hat{\\theta}))^{2}$.\n\nA fundamental result from statistical theory (specifically, a consequence of Cochran's theorem) for samples from a normal distribution is that the quantity $\\frac{1}{\\sigma^{2}}\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}$ follows a chi-square distribution with $n-1$ degrees of freedom. We denote this as:\n$$ \\frac{\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}}{\\sigma^{2}} \\sim \\chi^{2}_{n-1} $$\nA random variable $Y$ that follows a chi-square distribution with $k$ degrees of freedom, $Y \\sim \\chi^{2}_{k}$, has an expected value $E[Y] = k$ and a variance $\\mathrm{Var}(Y) = 2k$.\n\nFirst, we derive the exact finite-sample bias of $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$. To do this, we first calculate its expected value.\n$$ E[\\hat{\\sigma}^{2}_{\\mathrm{MLE}}] = E\\left[\\frac{1}{n}\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}\\right] = \\frac{1}{n}E\\left[\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}\\right] $$\nWe can rewrite the sum of squares in terms of the $\\chi^{2}_{n-1}$ distributed variable:\n$$ \\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2} = \\sigma^{2} \\left( \\frac{\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}}{\\sigma^{2}} \\right) $$\nThe expected value of this sum is:\n$$ E\\left[\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}\\right] = \\sigma^{2} E\\left[ \\frac{\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}}{\\sigma^{2}} \\right] = \\sigma^{2} (n-1) $$\nsince the expected value of a $\\chi^{2}_{n-1}$ variable is its degrees of freedom, $n-1$.\nSubstituting this result back into the expression for $E[\\hat{\\sigma}^{2}_{\\mathrm{MLE}}]$:\n$$ E[\\hat{\\sigma}^{2}_{\\mathrm{MLE}}] = \\frac{1}{n} (n-1)\\sigma^{2} = \\frac{n-1}{n}\\sigma^{2} $$\nThe bias of $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ is therefore:\n$$ \\mathrm{Bias}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}}) = E[\\hat{\\sigma}^{2}_{\\mathrm{MLE}}] - \\sigma^{2} = \\frac{n-1}{n}\\sigma^{2} - \\sigma^{2} = \\left(\\frac{n-1}{n} - 1\\right)\\sigma^{2} = -\\frac{1}{n}\\sigma^{2} $$\nThis is the required finite-sample bias of the MLE. It is a biased estimator, underestimating the true variance on average.\n\nNext, we compare the mean squared error (MSE) of $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ and $\\hat{\\sigma}^{2}_{U}$.\nFirst, let's confirm the unbiasedness of $\\hat{\\sigma}^{2}_{U}$:\n$$ E[\\hat{\\sigma}^{2}_{U}] = E\\left[\\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}\\right] = \\frac{1}{n-1}E\\left[\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}\\right] = \\frac{1}{n-1}(n-1)\\sigma^{2} = \\sigma^{2} $$\nSince $E[\\hat{\\sigma}^{2}_{U}] = \\sigma^{2}$, its bias is $\\mathrm{Bias}(\\hat{\\sigma}^{2}_{U}) = 0$.\n\nTo calculate the MSEs, we need the variances of the estimators. We use the fact that $\\mathrm{Var}(\\chi^{2}_{k}) = 2k$.\nThe variance of $\\hat{\\sigma}^{2}_{U}$ is:\n$$ \\mathrm{Var}(\\hat{\\sigma}^{2}_{U}) = \\mathrm{Var}\\left(\\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}\\right) = \\mathrm{Var}\\left(\\frac{\\sigma^{2}}{n-1} \\cdot \\frac{\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}}{\\sigma^{2}}\\right) $$\n$$ \\mathrm{Var}(\\hat{\\sigma}^{2}_{U}) = \\left(\\frac{\\sigma^{2}}{n-1}\\right)^{2} \\mathrm{Var}(\\chi^{2}_{n-1}) = \\frac{\\sigma^{4}}{(n-1)^{2}} \\cdot 2(n-1) = \\frac{2\\sigma^{4}}{n-1} $$\nSince $\\hat{\\sigma}^{2}_{U}$ is unbiased, its MSE is equal to its variance:\n$$ \\mathrm{MSE}(\\hat{\\sigma}^{2}_{U}) = \\mathrm{Var}(\\hat{\\sigma}^{2}_{U}) + (\\mathrm{Bias}(\\hat{\\sigma}^{2}_{U}))^{2} = \\frac{2\\sigma^{4}}{n-1} + 0^{2} = \\frac{2\\sigma^{4}}{n-1} $$\n\nNow for $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$. Note that $\\hat{\\sigma}^{2}_{\\mathrm{MLE}} = \\frac{n-1}{n}\\hat{\\sigma}^{2}_{U}$.\nThe variance of $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ is:\n$$ \\mathrm{Var}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}}) = \\mathrm{Var}\\left(\\frac{n-1}{n}\\hat{\\sigma}^{2}_{U}\\right) = \\left(\\frac{n-1}{n}\\right)^{2} \\mathrm{Var}(\\hat{\\sigma}^{2}_{U}) = \\frac{(n-1)^{2}}{n^{2}} \\cdot \\frac{2\\sigma^{4}}{n-1} = \\frac{2(n-1)\\sigma^{4}}{n^{2}} $$\nThe MSE of $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ is the sum of its variance and squared bias:\n$$ \\mathrm{MSE}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}}) = \\mathrm{Var}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}}) + (\\mathrm{Bias}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}}))^{2} = \\frac{2(n-1)\\sigma^{4}}{n^{2}} + \\left(-\\frac{\\sigma^{2}}{n}\\right)^{2} $$\n$$ \\mathrm{MSE}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}}) = \\frac{2(n-1)\\sigma^{4}}{n^{2}} + \\frac{\\sigma^{4}}{n^{2}} = \\frac{(2n - 2 + 1)\\sigma^{4}}{n^{2}} = \\frac{(2n-1)\\sigma^{4}}{n^{2}} $$\nTo compare the MSEs, we examine the difference $\\mathrm{MSE}(\\hat{\\sigma}^{2}_{U}) - \\mathrm{MSE}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}})$ for $n \\ge 2$:\n$$ \\frac{2\\sigma^{4}}{n-1} - \\frac{(2n-1)\\sigma^{4}}{n^{2}} = \\sigma^{4}\\left(\\frac{2}{n-1} - \\frac{2n-1}{n^{2}}\\right) = \\sigma^{4}\\left(\\frac{2n^{2} - (2n-1)(n-1)}{n^{2}(n-1)}\\right) $$\n$$ = \\sigma^{4}\\left(\\frac{2n^{2} - (2n^{2} - 3n + 1)}{n^{2}(n-1)}\\right) = \\sigma^{4}\\left(\\frac{3n - 1}{n^{2}(n-1)}\\right) $$\nFor any sample size $n \\ge 2$, the term $\\frac{3n-1}{n^{2}(n-1)}$ is strictly positive. Therefore, $\\mathrm{MSE}(\\hat{\\sigma}^{2}_{U}) > \\mathrm{MSE}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}})$. This shows that $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ has a smaller mean squared error than $\\hat{\\sigma}^{2}_{U}$ for any finite sample size. In the MSE sense, $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ is the more efficient estimator. This illustrates the classic bias-variance tradeoff: by accepting a small bias, the MLE achieves a lower variance, resulting in an overall smaller MSE.\n\nFinally, we discuss consistency. An estimator $\\hat{\\theta}_{n}$ is consistent for $\\theta$ if it converges in probability to $\\theta$ as $n \\to \\infty$. A sufficient condition for consistency is that both the bias and the variance tend to zero as $n \\to \\infty$.\nFor $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$:\n$$ \\lim_{n\\to\\infty} \\mathrm{Bias}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}}) = \\lim_{n\\to\\infty} \\left(-\\frac{\\sigma^{2}}{n}\\right) = 0 $$\n$$ \\lim_{n\\to\\infty} \\mathrm{Var}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}}) = \\lim_{n\\to\\infty} \\frac{2(n-1)\\sigma^{4}}{n^{2}} = \\lim_{n\\to\\infty} \\left(\\frac{2\\sigma^{4}}{n} - \\frac{2\\sigma^{4}}{n^{2}}\\right) = 0 $$\nSince both its bias and variance go to zero, $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ is a consistent estimator for $\\sigma^{2}$.\n\nFor $\\hat{\\sigma}^{2}_{U}$:\n$$ \\lim_{n\\to\\infty} \\mathrm{Bias}(\\hat{\\sigma}^{2}_{U}) = \\lim_{n\\to\\infty} 0 = 0 $$\n$$ \\lim_{n\\to\\infty} \\mathrm{Var}(\\hat{\\sigma}^{2}_{U}) = \\lim_{n\\to\\infty} \\frac{2\\sigma^{4}}{n-1} = 0 $$\nSince its bias is always zero and its variance goes to zero, $\\hat{\\sigma}^{2}_{U}$ is also a consistent estimator for $\\sigma^{2}$.\n\nThus, both estimators are consistent. Asymptotically, as $n \\to \\infty$, the factor $\\frac{n-1}{n}$ relating the two estimators approaches $1$, so they become equivalent.", "answer": "$$\\boxed{-\\frac{\\sigma^{2}}{n}}$$", "id": "4831014"}, {"introduction": "While an estimator for a parameter like a probability $p$ might be unbiased, applying a non-linear function, such as the log-odds transformation common in medical statistics, typically introduces bias due to Jensen's inequality. This practice [@problem_id:4981389] explores this important phenomenon by using a Taylor series expansion to derive the approximate bias of the log-odds estimator. This is a crucial skill for evaluating the properties of \"plug-in\" estimators in more complex models.", "problem": "A medical safety monitoring study tracks whether each of $n$ independent patients experiences a binary adverse event during a fixed follow-up window. Let $X_1,\\dots,X_n$ be independent and identically distributed with $X_i \\sim \\text{Bernoulli}(p)$, where $p \\in (0,1)$ is the true probability of the adverse event. The parameter of interest for downstream risk communication is the log-odds $\\theta = \\ln\\!\\big(p/(1-p)\\big)$. The Maximum Likelihood Estimator (MLE) is $\\hat{p} = \\bar{X} = n^{-1} \\sum_{i=1}^{n} X_i$, and the plug-in estimator for the log-odds is $\\hat{\\theta} = \\ln\\!\\big(\\hat{p}/(1-\\hat{p})\\big)$.\n\nTasks:\n1. Starting from the definitions of expectation and independence, show that $\\hat{p}$ is unbiased for $p$.\n2. Assume $p \\in (0,1)$ and consider only the event $\\{0\\hat{p}1\\}$ so that $\\hat{\\theta}$ is well-defined; note that $\\mathbb{P}(0\\hat{p}1) \\to 1$ as $n \\to \\infty$ for fixed $p \\in (0,1)$. By expanding the transformation $g(x) = \\ln\\!\\big(x/(1-x)\\big)$ around $x=p$ to second order and using the leading nonvanishing terms in the expectation, derive the leading-order asymptotic bias $\\mathbb{E}[\\hat{\\theta}] - \\theta$ up to order $1/n$, expressed explicitly as a symbolic function of $p$ and $n$.\n\nProvide as your final answer a single closed-form analytic expression in $p$ and $n$ for the leading-order asymptotic bias. No numerical rounding is required, and no units are involved.", "solution": "The problem statement has been validated and is deemed a valid, well-posed problem in statistical theory. It is scientifically grounded, self-contained, and objective. We may therefore proceed with a full solution.\n\nThe problem is divided into two tasks. We will address them sequentially.\n\n**Task 1: Show that $\\hat{p}$ is an unbiased estimator for $p$.**\n\nAn estimator is unbiased if its expected value is equal to the true parameter value. Here, we must show that $\\mathbb{E}[\\hat{p}] = p$.\n\nThe estimator for $p$ is given as the sample mean, $\\hat{p} = \\bar{X} = n^{-1} \\sum_{i=1}^{n} X_i$.\nThe expectation of $\\hat{p}$ is:\n$$\n\\mathbb{E}[\\hat{p}] = \\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right]\n$$\nBy the linearity of the expectation operator, we can move the constant factor $1/n$ and the summation outside the expectation:\n$$\n\\mathbb{E}[\\hat{p}] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}[X_i]\n$$\nThe problem states that $X_1, \\dots, X_n$ are independent and identically distributed (i.i.d.) random variables from a Bernoulli distribution with parameter $p$, i.e., $X_i \\sim \\text{Bernoulli}(p)$. For a Bernoulli random variable, the probability mass function is $\\mathbb{P}(X_i = 1) = p$ and $\\mathbb{P}(X_i = 0) = 1-p$. The expected value of each $X_i$ is, by definition:\n$$\n\\mathbb{E}[X_i] = 1 \\cdot \\mathbb{P}(X_i = 1) + 0 \\cdot \\mathbb{P}(X_i = 0) = 1 \\cdot p + 0 \\cdot (1-p) = p\n$$\nSince all $X_i$ are identically distributed, $\\mathbb{E}[X_i] = p$ for all $i \\in \\{1, \\dots, n\\}$. Substituting this result back into the expression for $\\mathbb{E}[\\hat{p}]$:\n$$\n\\mathbb{E}[\\hat{p}] = \\frac{1}{n} \\sum_{i=1}^{n} p\n$$\nThe sum consists of $n$ identical terms of $p$, so $\\sum_{i=1}^{n} p = n p$.\n$$\n\\mathbb{E}[\\hat{p}] = \\frac{1}{n} (n p) = p\n$$\nSince $\\mathbb{E}[\\hat{p}] = p$, the estimator $\\hat{p}$ is, by definition, an unbiased estimator for the parameter $p$. This completes the first task.\n\n**Task 2: Derive the leading-order asymptotic bias of $\\hat{\\theta}$.**\n\nThe parameter of interest is the log-odds, $\\theta = \\ln(p/(1-p))$, and its estimator is $\\hat{\\theta} = \\ln(\\hat{p}/(1-\\hat{p}))$. This can be written as $\\hat{\\theta} = g(\\hat{p})$ where the transformation function is $g(x) = \\ln(x/(1-x))$.\n\nThe bias of $\\hat{\\theta}$ is defined as $\\text{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$. To find an asymptotic expression for this bias, we use a Taylor series expansion of the function $g(\\hat{p})$ around the point $\\hat{p} = p$. The problem specifies expanding to the second order.\n$$\ng(\\hat{p}) \\approx g(p) + g'(p)(\\hat{p}-p) + \\frac{1}{2}g''(p)(\\hat{p}-p)^2\n$$\nFirst, we must find the first and second derivatives of $g(x) = \\ln(x) - \\ln(1-x)$.\nThe first derivative is:\n$$\ng'(x) = \\frac{d}{dx} [\\ln(x) - \\ln(1-x)] = \\frac{1}{x} - \\frac{1}{1-x}(-1) = \\frac{1}{x} + \\frac{1}{1-x} = \\frac{(1-x)+x}{x(1-x)} = \\frac{1}{x(1-x)}\n$$\nThe second derivative is:\n$$\ng''(x) = \\frac{d}{dx} [x^{-1} + (1-x)^{-1}] = -x^{-2} + (-1)(1-x)^{-2}(-1) = -x^{-2} + (1-x)^{-2} = \\frac{1}{(1-x)^2} - \\frac{1}{x^2}\n$$\nCombining the terms for $g''(x)$:\n$$\ng''(x) = \\frac{x^2 - (1-x)^2}{x^2(1-x)^2} = \\frac{x^2 - (1 - 2x + x^2)}{x^2(1-x)^2} = \\frac{2x-1}{x^2(1-x)^2}\n$$\nNow, we evaluate these derivatives at $x=p$:\n- $g(p) = \\ln(p/(1-p)) = \\theta$\n- $g'(p) = \\frac{1}{p(1-p)}$\n- $g''(p) = \\frac{2p-1}{p^2(1-p)^2}$\n\nSubstituting these into the Taylor expansion gives:\n$$\n\\hat{\\theta} = g(\\hat{p}) \\approx \\theta + \\left(\\frac{1}{p(1-p)}\\right)(\\hat{p}-p) + \\frac{1}{2}\\left(\\frac{2p-1}{p^2(1-p)^2}\\right)(\\hat{p}-p)^2\n$$\nTo find the approximate expected value of $\\hat{\\theta}$, we take the expectation of this expression. The problem assumes we are on the event $\\{0  \\hat{p}  1\\}$, where $\\hat{\\theta}$ is well-defined.\n$$\n\\mathbb{E}[\\hat{\\theta}] \\approx \\mathbb{E}\\left[\\theta + \\frac{1}{p(1-p)}(\\hat{p}-p) + \\frac{2p-1}{2p^2(1-p)^2}(\\hat{p}-p)^2\\right]\n$$\nUsing the linearity of expectation:\n$$\n\\mathbb{E}[\\hat{\\theta}] \\approx \\theta + \\frac{1}{p(1-p)}\\mathbb{E}[\\hat{p}-p] + \\frac{2p-1}{2p^2(1-p)^2}\\mathbb{E}[(\\hat{p}-p)^2]\n$$\nWe need to evaluate the two expectation terms:\n- $\\mathbb{E}[\\hat{p}-p]$ is the bias of $\\hat{p}$. As shown in Task 1, $\\hat{p}$ is unbiased, so $\\mathbb{E}[\\hat{p}] = p$. Therefore, $\\mathbb{E}[\\hat{p}-p] = \\mathbb{E}[\\hat{p}] - p = p - p = 0$.\n- $\\mathbb{E}[(\\hat{p}-p)^2]$ is, by definition, the variance of $\\hat{p}$, denoted $\\text{Var}(\\hat{p})$, since its mean is $p$.\n\nWe now calculate $\\text{Var}(\\hat{p})$:\n$$\n\\text{Var}(\\hat{p}) = \\text{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n^2} \\text{Var}\\left(\\sum_{i=1}^{n} X_i\\right)\n$$\nSince the $X_i$ are independent, the variance of the sum is the sum of the variances:\n$$\n\\text{Var}(\\hat{p}) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\text{Var}(X_i)\n$$\nFor a Bernoulli($p$) variable, the variance is $\\text{Var}(X_i) = p(1-p)$. Since the variables are identically distributed:\n$$\n\\text{Var}(\\hat{p}) = \\frac{1}{n^2} \\sum_{i=1}^{n} p(1-p) = \\frac{1}{n^2} [n p(1-p)] = \\frac{p(1-p)}{n}\n$$\nSubstituting these results back into the approximation for $\\mathbb{E}[\\hat{\\theta}]$:\n$$\n\\mathbb{E}[\\hat{\\theta}] \\approx \\theta + \\frac{1}{p(1-p)}(0) + \\frac{2p-1}{2p^2(1-p)^2}\\left(\\frac{p(1-p)}{n}\\right)\n$$\nSimplifying the second term:\n$$\n\\mathbb{E}[\\hat{\\theta}] \\approx \\theta + \\frac{(2p-1)p(1-p)}{2n p^2(1-p)^2} = \\theta + \\frac{2p-1}{2n p(1-p)}\n$$\nThe asymptotic bias is $\\mathbb{E}[\\hat{\\theta}] - \\theta$. From the expression above, we find the leading-order term for the bias:\n$$\n\\text{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta \\approx \\frac{2p-1}{2np(1-p)}\n$$\nThis expression is of order $1/n$. Higher-order terms in the Taylor expansion, such as the term involving $\\mathbb{E}[(\\hat{p}-p)^3]$, would contribute terms of order $1/n^2$ or smaller, confirming that this is the leading-order asymptotic bias.\n\nThe required final answer is the closed-form analytic expression for this leading-order asymptotic bias.", "answer": "$$\n\\boxed{\\frac{2p-1}{2np(1-p)}}\n$$", "id": "4981389"}, {"introduction": "In real-world medical research, especially with small sample sizes or rare events, standard Maximum Likelihood Estimators can behave poorly, leading to infinite estimates and extreme bias. This problem [@problem_id:4981382] delves into the practical issue of data separation in logistic regression. It provides a conceptual evaluation of Firth’s penalized likelihood, a powerful method designed to yield finite, less-biased estimates in situations where traditional methods fail.", "problem": "A case-control study assesses the effect of an exposure on a binary adverse outcome using logistic regression with a single binary covariate $x \\in \\{0,1\\}$ denoting exposure. The parameter of interest is the log-odds ratio $\\beta$, defined as the difference in log-odds of the outcome between exposed and unexposed. Consider the following two $2\\times 2$ data configurations, with cell counts $(a,b,c,d)$ denoting respectively exposed cases, exposed controls, unexposed cases, and unexposed controls.\n\n- Sparse configuration $\\mathsf{S}$: $(a,b,c,d)=(3,0,5,42)$.\n- Non-sparse configuration $\\mathsf{NS}$: $(a,b,c,d)=(3,2,5,40)$.\n\nLet $\\hat{\\beta}_{\\mathrm{MLE}}$ denote the Maximum Likelihood Estimator (MLE) of $\\beta$ from the standard logistic regression and let $\\hat{\\beta}_{\\mathrm{F}}$ denote the estimator obtained from Firth’s penalized likelihood (Jeffreys-prior-based) bias-reduction method. Bias is defined as $\\mathbb{E}[\\hat{\\beta}]-\\beta$, variance as $\\mathrm{Var}(\\hat{\\beta})$, and efficiency is to be interpreted through mean squared error (MSE), defined as $\\mathbb{E}\\big[(\\hat{\\beta}-\\beta)^2\\big]=\\mathrm{Bias}(\\hat{\\beta})^2+\\mathrm{Var}(\\hat{\\beta})$.\n\nUsing first principles about maximum likelihood, finite-sample bias expansions, and the qualitative behavior of estimators under separation and sparsity in logistic models, evaluate the following statements about the bias of $\\hat{\\beta}_{\\mathrm{MLE}}$ and the effect of Firth’s bias reduction, including its impact on variance. Select all statements that are correct.\n\nA. In sparse $2\\times 2$ data, $\\hat{\\beta}_{\\mathrm{MLE}}$ is unbiased for $\\beta$ to first order, and Firth’s method primarily decreases variance by shrinking estimates toward zero.\n\nB. With zero cells or quasi-complete separation as in configuration $\\mathsf{S}$, $\\hat{\\beta}_{\\mathrm{MLE}}$ can be infinite, implying extreme bias for any finite $\\beta$. Firth’s penalized likelihood returns a finite estimate closer to $0$ on average, thereby reducing bias and often reducing mean squared error.\n\nC. In $2\\times 2$ tables, Firth’s estimator $\\hat{\\beta}_{\\mathrm{F}}$ coincides with $\\hat{\\beta}_{\\mathrm{MLE}}$ but with inflated standard errors, so it cannot change bias.\n\nD. The variance of $\\hat{\\beta}_{\\mathrm{F}}$ is uniformly larger than that of $\\hat{\\beta}_{\\mathrm{MLE}}$ in all sparse logistic regression settings.\n\nE. Firth’s adjustment removes the $\\mathcal{O}(n^{-1})$ term in the asymptotic bias expansion of $\\hat{\\beta}_{\\mathrm{MLE}}$ without altering the leading $\\mathcal{O}(n^{-1})$ variance term, so asymptotically both estimators have the same variance while the penalized estimator has smaller bias.", "solution": "We begin from core definitions and well-tested facts.\n\n- Bias is $\\mathbb{E}[\\hat{\\beta}]-\\beta$.\n- Variance is $\\mathrm{Var}(\\hat{\\beta})$.\n- Mean squared error (MSE) equals $\\mathrm{Bias}(\\hat{\\beta})^2+\\mathrm{Var}(\\hat{\\beta})$.\n- In logistic regression with canonical link, the MLE $\\hat{\\beta}_{\\mathrm{MLE}}$ solves the score equations. In finite samples, the MLE admits an asymptotic expansion $\\mathbb{E}[\\hat{\\beta}_{\\mathrm{MLE}}]=\\beta+n^{-1}b(\\beta)+o(n^{-1})$, i.e., generally it has a nonzero first-order bias of order $\\mathcal{O}(n^{-1})$.\n- Firth’s method adds the Jeffreys invariant prior, leading to a penalized log-likelihood that adjusts the score to remove the $\\mathcal{O}(n^{-1})$ term in the asymptotic bias. The resulting estimator $\\hat{\\beta}_{\\mathrm{F}}$ has $\\mathbb{E}[\\hat{\\beta}_{\\mathrm{F}}]=\\beta+o(n^{-1})$, i.e., reduced first-order bias, while maintaining the same leading-order variance $\\mathcal{O}(n^{-1})$ as the MLE.\n\nBehavior under sparsity and separation. In a $2\\times 2$ table, with $(a,b,c,d)$ as defined, the sample odds ratio is $(ad)/(bc)$ and the MLE $\\hat{\\beta}_{\\mathrm{MLE}}$ for the single log-odds ratio equals the log of this sample odds ratio when all cells are positive. If any of $b$ or $c$ is zero, the sample odds ratio is infinite and the standard logistic MLE diverges, a manifestation of complete or quasi-complete separation. This implies that for any finite true $\\beta$, the estimator is effectively biased in the sense that $\\mathbb{E}[\\hat{\\beta}_{\\mathrm{MLE}}]$ is not finite or is far from $\\beta$ in such sparse regimes.\n\nConfiguration $\\mathsf{S}$ illustrates this: $(a,b,c,d)=(3,0,5,42)$. Here $b=0$, so there are no exposed controls. In the simple logistic regression with intercept $\\alpha$ and slope $\\beta$ for $x$, the log-likelihood is\n$$\n\\ell(\\alpha,\\beta)=\\sum_{i=1}^{n}\\bigl\\{y_i(\\alpha+\\beta x_i)-\\log\\bigl(1+\\exp(\\alpha+\\beta x_i)\\bigr)\\bigr\\},\n$$\nand when all subjects with $x=1$ are cases (no exposed controls), the likelihood is monotonically increasing in $\\beta$ and $\\hat{\\beta}_{\\mathrm{MLE}}\\to+\\infty$. Thus the ordinary MLE can be infinite, indicating extreme bias whenever the true $\\beta$ is finite.\n\nFirth’s bias-reduction method. Firth’s estimator can be derived by maximizing a penalized log-likelihood\n$$\n\\ell^{\\ast}(\\theta)=\\ell(\\theta)+\\frac{1}{2}\\log\\bigl|I(\\theta)\\bigr|,\n$$\nwhere $I(\\theta)$ is the Fisher information and $\\theta$ collects the regression coefficients. This choice corresponds to the Jeffreys prior and adjusts the score to remove the $\\mathcal{O}(n^{-1})$ term in the asymptotic bias of the MLE. In the $2\\times 2$ case with one binary covariate, a well-tested and widely used fact is that the bias-reduced estimator for the log-odds ratio coincides with taking the log of the odds ratio after adding a continuity correction of $+1/2$ to each cell. That is,\n$$\n\\hat{\\beta}_{\\mathrm{F}}=\\log\\left(\\frac{(a+\\tfrac{1}{2})(d+\\tfrac{1}{2})}{(b+\\tfrac{1}{2})(c+\\tfrac{1}{2})}\\right).\n$$\nThis yields a finite estimate even under separation. For $\\mathsf{S}$, this is\n$$\n\\hat{\\beta}_{\\mathrm{F}}=\\log\\left(\\frac{(3+\\tfrac{1}{2})(42+\\tfrac{1}{2})}{(0+\\tfrac{1}{2})(5+\\tfrac{1}{2})}\\right)\n=\\log\\left(\\frac{3.5\\times 42.5}{0.5\\times 5.5}\\right)\n=\\log\\left(\\frac{148.75}{2.75}\\right)\\approx \\log(54.0909)\\approx 3.989,\n$$\nfinite and plausibly closer to $0$ than $+\\infty$ on average when $\\beta$ is finite.\n\nImpact on variance. To leading order, both $\\hat{\\beta}_{\\mathrm{MLE}}$ and $\\hat{\\beta}_{\\mathrm{F}}$ have variance of order $\\mathcal{O}(n^{-1})$. In small samples for a $2\\times 2$ table, a commonly used large-sample variance approximation for the log-odds ratio is the sum of reciprocals of the cell counts. For the Firth-adjusted estimator, the analogous approximation uses the adjusted counts $(a+\\tfrac{1}{2}, b+\\tfrac{1}{2}, c+\\tfrac{1}{2}, d+\\tfrac{1}{2})$. Thus,\n$$\n\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{MLE}})\\approx \\frac{1}{a}+\\frac{1}{b}+\\frac{1}{c}+\\frac{1}{d},\\quad\n\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{F}})\\approx \\frac{1}{a+\\tfrac{1}{2}}+\\frac{1}{b+\\tfrac{1}{2}}+\\frac{1}{c+\\tfrac{1}{2}}+\\frac{1}{d+\\tfrac{1}{2}},\n$$\nwhen these approximations apply. In sparse settings with zero cells, $\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{MLE}})$ is not well-defined because $\\hat{\\beta}_{\\mathrm{MLE}}$ is infinite, whereas $\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{F}})$ is finite. In non-sparse small samples, Firth’s estimator often yields variance of similar order and sometimes smaller magnitude numerically due to stabilization of extreme cell-wise reciprocals, but asymptotically the leading variance term is the same as that of the MLE.\n\nFor illustration in $\\mathsf{NS}$, $(a,b,c,d)=(3,2,5,40)$,\n$$\n\\hat{\\beta}_{\\mathrm{MLE}}=\\log\\left(\\frac{3\\times 40}{2\\times 5}\\right)=\\log(12)\\approx 2.485,\\quad\n\\hat{\\beta}_{\\mathrm{F}}=\\log\\left(\\frac{3.5\\times 40.5}{2.5\\times 5.5}\\right)=\\log\\left(\\frac{141.75}{13.75}\\right)\\approx \\log(10.309)\\approx 2.333,\n$$\nshowing shrinkage toward $0$ (bias reduction). The variance approximations are\n$$\n\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{MLE}})\\approx \\frac{1}{3}+\\frac{1}{2}+\\frac{1}{5}+\\frac{1}{40}\\approx 1.058,\\quad\n\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{F}})\\approx \\frac{1}{3.5}+\\frac{1}{2.5}+\\frac{1}{5.5}+\\frac{1}{40.5}\\approx 0.893,\n$$\nindicating that, in this configuration, the penalized estimator also has a smaller approximate variance, and thus smaller MSE, while asymptotically both variances are of the same order $\\mathcal{O}(n^{-1})$.\n\nOption-by-option analysis.\n\n- Option A: Claims $\\hat{\\beta}_{\\mathrm{MLE}}$ is unbiased to first order in sparse data and that Firth’s method primarily decreases variance. This is incorrect. The MLE for logistic regression generally has an $\\mathcal{O}(n^{-1})$ bias term; Firth’s method is designed to remove this first-order bias. Although penalization may stabilize variance in small samples, the primary goal and guaranteed property of Firth’s adjustment is mean-bias reduction, not variance reduction.\n\nVerdict: Incorrect.\n\n- Option B: Notes that in $\\mathsf{S}$ with a zero cell, $\\hat{\\beta}_{\\mathrm{MLE}}$ can be infinite (quasi-complete separation), implying extreme bias for finite $\\beta$, while Firth yields a finite estimate that is closer to $0$ on average, reducing bias and often reducing MSE. This aligns with the separation pathology and the bias-reduction property of Firth’s method. The numerical illustration corroborates the finite and shrunken estimate, and small-sample studies typically show reduced MSE.\n\nVerdict: Correct.\n\n- Option C: States that $\\hat{\\beta}_{\\mathrm{F}}$ coincides with $\\hat{\\beta}_{\\mathrm{MLE}}$ but with inflated standard errors. This is false. In general, $\\hat{\\beta}_{\\mathrm{F}}\\neq \\hat{\\beta}_{\\mathrm{MLE}}$; indeed, Firth’s method alters the estimating equations and the point estimates, reducing first-order bias. In $2\\times 2$, $\\hat{\\beta}_{\\mathrm{F}}$ corresponds to a continuity-corrected odds ratio, distinct from the MLE unless cell counts are very large and balanced.\n\nVerdict: Incorrect.\n\n- Option D: Claims that the variance of $\\hat{\\beta}_{\\mathrm{F}}$ is uniformly larger than that of $\\hat{\\beta}_{\\mathrm{MLE}}$ in all sparse settings. This is not generally true. As shown in $\\mathsf{NS}$, the approximate variance can be smaller for the penalized estimator; in $\\mathsf{S}$, the MLE variance is not even finite. Asymptotically, leading variances coincide; there is no uniform inflation.\n\nVerdict: Incorrect.\n\n- Option E: States that Firth removes the $\\mathcal{O}(n^{-1})$ bias term without changing the leading $\\mathcal{O}(n^{-1})$ variance term, hence asymptotically equal variance but smaller bias. This is a standard and correct statement about Firth’s mean-bias-reducing penalized likelihood based on the Jeffreys prior; the variance to first order is the same as the MLE’s, while the first-order bias is removed.\n\nVerdict: Correct.\n\nTherefore, the correct statements are B and E.", "answer": "$$\\boxed{BE}$$", "id": "4981382"}]}