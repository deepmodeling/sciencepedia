{"hands_on_practices": [{"introduction": "Maximum Likelihood Estimators (MLEs) are celebrated for their excellent asymptotic properties, such as consistency and efficiency. However, in finite samples, they may exhibit bias. This practice provides a foundational, hands-on derivation to illustrate this key concept, guiding you to derive the MLE for the variance, $\\sigma^2$, of a normal distribution and calculate its exact bias. By contrasting the MLE with the familiar unbiased sample variance, you will gain a deeper appreciation for the nuances of estimator properties and the importance of concepts like Bessel's correction [@problem_id:4922788].", "problem": "A biostatistics team measures a continuous biomarker for $n$ subjects under standardized conditions. Suppose the measurements $X_{1}, X_{2}, \\dots, X_{n}$ are independent and identically distributed from a normal distribution $\\mathcal{N}(\\mu, \\sigma^{2})$ with both $\\mu$ and $\\sigma^{2}$ unknown. Using the principles of Maximum Likelihood Estimation (MLE), derive the MLE for $\\sigma^{2}$ from the likelihood function of the sample, and then determine its expectation under the model. Use the decomposition of total variation relative to the population mean as your fundamental base, together with core facts about expectations and variances of sums and averages of independent and identically distributed normal random variables. Show explicitly whether the MLE of $\\sigma^{2}$ is biased and quantify the bias as a function of $n$ and $\\sigma^{2}$. Finally, contrast this MLE with the estimator based on dividing by $n-1$ in terms of bias. Report the bias of the MLE of $\\sigma^{2}$ as a single closed-form expression in terms of $n$ and $\\sigma^{2}$. No numerical evaluation is required, and no rounding is needed. Your final answer must be a single analytic expression.", "solution": "The problem requires the derivation of the Maximum Likelihood Estimator (MLE) for the variance $\\sigma^2$ of a normal distribution, the calculation of its expectation and bias, and a comparison with the standard unbiased estimator. The sample consists of $n$ independent and identically distributed (i.i.d.) random variables $X_1, X_2, \\dots, X_n$ from a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ where both the mean $\\mu$ and the variance $\\sigma^2$ are unknown.\n\nFirst, we establish the likelihood function. The probability density function (PDF) for a single observation $X_i$ is given by:\n$$f(x_i | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$$\nSince the observations are i.i.d., the joint PDF, which is the likelihood function $L(\\mu, \\sigma^2)$, is the product of the individual PDFs:\n$$L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} f(x_i | \\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{(2\\pi\\sigma^2)^{1/2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$$\nThis can be simplified to:\n$$L(\\mu, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2\\right)$$\nTo find the MLEs, it is more convenient to work with the natural logarithm of the likelihood function, the log-likelihood function $\\ell(\\mu, \\sigma^2)$:\n$$\\ell(\\mu, \\sigma^2) = \\ln L(\\mu, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2$$\nThe MLEs are the values of $\\mu$ and $\\sigma^2$ that maximize $\\ell(\\mu, \\sigma^2)$. We find these by taking the partial derivatives with respect to $\\mu$ and $\\sigma^2$, setting them to $0$, and solving the resulting system of equations.\n\nThe partial derivative with respect to $\\mu$ is:\n$$\\frac{\\partial \\ell}{\\partial \\mu} = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} 2(x_i - \\mu)(-1) = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)$$\nSetting this to $0$ yields $\\sum_{i=1}^{n}(x_i - \\mu) = 0$, which implies $n\\mu = \\sum_{i=1}^{n}x_i$. Thus, the MLE for $\\mu$ is the sample mean, $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n}X_i = \\bar{X}$.\n\nNext, we find the partial derivative with respect to $\\sigma^2$. For simplicity, let $\\theta = \\sigma^2$.\n$$\\ell(\\mu, \\theta) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\theta) - \\frac{1}{2\\theta}\\sum_{i=1}^{n}(x_i - \\mu)^2$$\n$$\\frac{\\partial \\ell}{\\partial \\theta} = -\\frac{n}{2\\theta} + \\frac{1}{2\\theta^2}\\sum_{i=1}^{n}(x_i - \\mu)^2$$\nSetting the partial derivative to $0$ and substituting the MLE for $\\mu$, $\\hat{\\mu} = \\bar{X}$, we get:\n$$-\\frac{n}{2\\hat{\\theta}} + \\frac{1}{2\\hat{\\theta}^2}\\sum_{i=1}^{n}(x_i - \\bar{X})^2 = 0$$\nMultiplying by $2\\hat{\\theta}^2$ (assuming $\\hat{\\theta} > 0$) gives:\n$$-n\\hat{\\theta} + \\sum_{i=1}^{n}(x_i - \\bar{X})^2 = 0$$\nSolving for $\\hat{\\theta}$, we find the MLE for $\\sigma^2$:\n$$\\hat{\\sigma}^2_{MLE} = \\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\bar{X})^2$$\n\nNow, we must determine the expectation of this estimator, $E[\\hat{\\sigma}^2_{MLE}]$, to evaluate its bias. The prompt suggests using a decomposition of the total sum of squares. We can rewrite the sum of squared deviations from the sample mean as follows:\n$$\\sum_{i=1}^{n}(X_i - \\bar{X})^2 = \\sum_{i=1}^{n}((X_i - \\mu) - (\\bar{X} - \\mu))^2$$\nExpanding this expression gives:\n$$= \\sum_{i=1}^{n}\\left[(X_i - \\mu)^2 - 2(X_i - \\mu)(\\bar{X} - \\mu) + (\\bar{X} - \\mu)^2\\right]$$\n$$= \\sum_{i=1}^{n}(X_i - \\mu)^2 - 2(\\bar{X} - \\mu)\\sum_{i=1}^{n}(X_i - \\mu) + \\sum_{i=1}^{n}(\\bar{X} - \\mu)^2$$\nRecognizing that $\\sum_{i=1}^{n}(X_i - \\mu) = n(\\bar{X} - \\mu)$, we substitute this into the middle term:\n$$= \\sum_{i=1}^{n}(X_i - \\mu)^2 - 2(\\bar{X} - \\mu) \\cdot n(\\bar{X} - \\mu) + n(\\bar{X} - \\mu)^2$$\n$$= \\sum_{i=1}^{n}(X_i - \\mu)^2 - 2n(\\bar{X} - \\mu)^2 + n(\\bar{X} - \\mu)^2$$\n$$= \\sum_{i=1}^{n}(X_i - \\mu)^2 - n(\\bar{X} - \\mu)^2$$\nNow we take the expectation of this expression:\n$$E\\left[\\sum_{i=1}^{n}(X_i - \\bar{X})^2\\right] = E\\left[\\sum_{i=1}^{n}(X_i - \\mu)^2\\right] - E\\left[n(\\bar{X} - \\mu)^2\\right]$$\nWe evaluate the expectation of each term separately. For the first term, by definition of variance, $E[(X_i - \\mu)^2] = \\text{Var}(X_i) = \\sigma^2$. By linearity of expectation:\n$$E\\left[\\sum_{i=1}^{n}(X_i - \\mu)^2\\right] = \\sum_{i=1}^{n}E[(X_i - \\mu)^2] = \\sum_{i=1}^{n}\\sigma^2 = n\\sigma^2$$\nFor the second term, we first find the variance of the sample mean, $\\bar{X}$. Since the $X_i$ are independent, $\\text{Var}(\\bar{X}) = \\text{Var}(\\frac{1}{n}\\sum X_i) = \\frac{1}{n^2}\\sum\\text{Var}(X_i) = \\frac{1}{n^2}(n\\sigma^2) = \\frac{\\sigma^2}{n}$. The expectation of the second term is:\n$$E\\left[n(\\bar{X} - \\mu)^2\\right] = nE[(\\bar{X} - E[\\bar{X}])^2] = n\\text{Var}(\\bar{X}) = n\\left(\\frac{\\sigma^2}{n}\\right) = \\sigma^2$$\nSubstituting these results back into the equation for the expected sum of squares:\n$$E\\left[\\sum_{i=1}^{n}(X_i - \\bar{X})^2\\right] = n\\sigma^2 - \\sigma^2 = (n-1)\\sigma^2$$\nNow we can compute the expectation of the MLE for $\\sigma^2$:\n$$E[\\hat{\\sigma}^2_{MLE}] = E\\left[\\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\bar{X})^2\\right] = \\frac{1}{n}E\\left[\\sum_{i=1}^{n}(X_i - \\bar{X})^2\\right] = \\frac{1}{n}(n-1)\\sigma^2 = \\frac{n-1}{n}\\sigma^2$$\nThe bias of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $\\text{Bias}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$. For $\\hat{\\sigma}^2_{MLE}$, the bias is:\n$$\\text{Bias}(\\hat{\\sigma}^2_{MLE}) = E[\\hat{\\sigma}^2_{MLE}] - \\sigma^2 = \\frac{n-1}{n}\\sigma^2 - \\sigma^2 = \\left(\\frac{n-1}{n} - 1\\right)\\sigma^2 = \\left(\\frac{n-1-n}{n}\\right)\\sigma^2 = -\\frac{1}{n}\\sigma^2$$\nSince the bias is non-zero for any finite $n$, the MLE for $\\sigma^2$ is a biased estimator. It systematically underestimates the true variance.\n\nIn contrast, the sample variance, often denoted by $S^2$, uses a denominator of $n-1$:\n$$S^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X})^2$$\nLet us compute its expectation:\n$$E[S^2] = E\\left[\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X})^2\\right] = \\frac{1}{n-1}E\\left[\\sum_{i=1}^{n}(X_i - \\bar{X})^2\\right]$$\nUsing our previous result that $E[\\sum_{i=1}^{n}(X_i - \\bar{X})^2] = (n-1)\\sigma^2$:\n$$E[S^2] = \\frac{1}{n-1}(n-1)\\sigma^2 = \\sigma^2$$\nThe bias of $S^2$ is $\\text{Bias}(S^2) = E[S^2] - \\sigma^2 = \\sigma^2 - \\sigma^2 = 0$. Thus, $S^2$ is an unbiased estimator of $\\sigma^2$. The factor $\\frac{n}{n-1}$ that relates $S^2$ to $\\hat{\\sigma}^2_{MLE}$ is known as Bessel's correction.\n\nThe problem asks for the bias of the MLE of $\\sigma^2$, which we have derived to be $-\\frac{1}{n}\\sigma^2$.", "answer": "$$\\boxed{-\\frac{\\sigma^2}{n}}$$", "id": "4922788"}, {"introduction": "While some MLEs can be solved for algebraically, many essential statistical models, including logistic regression, do not have a closed-form solution for their parameters. In these cases, we rely on iterative numerical algorithms to find the maximum of the likelihood function. This practice introduces the Newton-Raphson method, a powerful optimization technique that uses information about the curvature of the log-likelihood surface to converge quickly to the MLE. You will derive the score vector and Jacobian matrix from first principles and apply one iterative step to a dataset, providing concrete insight into how statistical software computes these estimates [@problem_id:4969348].", "problem": "A clinical study investigates the probability of a postoperative complication as a function of a standardized biomarker. Let $y_i \\in \\{0,1\\}$ indicate whether patient $i$ experienced the complication and let $x_i \\in \\mathbb{R}$ denote the biomarker value. Suppose a parametric model with parameter vector $\\theta \\in \\mathbb{R}^{p}$ is fit by the principle of Maximum Likelihood Estimation (MLE), which selects $\\theta$ to maximize the log-likelihood $l(\\theta)$ of the observed data. Starting from the definitions of the score function $U(\\theta)$ as the gradient of $l(\\theta)$ and the Jacobian matrix $J(\\theta)$ of $U(\\theta)$, derive the Newton–Raphson update rule for iteratively solving the score equation $U(\\theta)=0$ to obtain the MLE.\n\nThen, specialize to binary logistic regression with a single predictor, where for each patient\n$$\np_i(\\beta_0,\\beta_1) \\equiv \\Pr(y_i=1 \\mid x_i) = \\frac{1}{1+\\exp\\!\\big(-\\eta_i\\big)}, \\quad \\text{with } \\eta_i = \\beta_0 + \\beta_1 x_i,\n$$\nand the log-likelihood of independent observations is\n$$\nl(\\beta_0,\\beta_1) = \\sum_{i=1}^{n} \\left[ y_i \\ln p_i + (1-y_i)\\ln(1-p_i) \\right].\n$$\nUsing first principles and without introducing any shortcut formulas, derive the score vector $U(\\beta_0,\\beta_1)$ and the Jacobian $J(\\beta_0,\\beta_1)$ for this model.\n\nFinally, apply one Newton–Raphson iteration starting from the initial guess $\\beta^{(0)} = (\\beta_0^{(0)}, \\beta_1^{(0)})^{\\top} = (0,0)^{\\top}$ to the following dataset of $n=6$ patients:\n- biomarker values $x = (-2,\\, -1,\\, 0,\\, 1,\\, 2,\\, 0)$,\n- outcomes $y = (0,\\, 0,\\, 0,\\, 1,\\, 1,\\, 0)$.\n\nCompute the updated parameter vector $\\beta^{(1)}$ produced by one Newton–Raphson step. Express your final answer as a $1 \\times 2$ row matrix and round each entry to four significant figures.", "solution": "The problem is divided into three parts: first, a general derivation of the Newton–Raphson update rule for Maximum Likelihood Estimation (MLE); second, a specific derivation of the score vector and Jacobian matrix for a binary logistic regression model; and third, a numerical application of one Newton–Raphson step to a given dataset.\n\n### Part 1: Derivation of the Newton–Raphson Update Rule\n\nThe goal of Maximum Likelihood Estimation (MLE) is to find the parameter vector $\\theta$ that maximizes the log-likelihood function $l(\\theta)$. This maximum is found at a critical point where the gradient of the log-likelihood is zero. The score function, $U(\\theta)$, is defined as this gradient:\n$$\nU(\\theta) = \\nabla l(\\theta)\n$$\nThus, finding the MLE $\\hat\\theta$ is equivalent to solving the score equation $U(\\hat\\theta) = 0$.\n\nThe Newton–Raphson method is an iterative root-finding algorithm. To solve $U(\\theta) = 0$, we start with an initial guess $\\theta^{(k)}$ and aim to find a better approximation $\\theta^{(k+1)}$. We approximate $U(\\theta)$ by its first-order Taylor series expansion around $\\theta^{(k)}$:\n$$\nU(\\theta) \\approx U(\\theta^{(k)}) + J(\\theta^{(k)}) (\\theta - \\theta^{(k)})\n$$\nwhere $J(\\theta^{(k)})$ is the Jacobian matrix of the score function $U(\\theta)$ evaluated at $\\theta^{(k)}$. The elements of the Jacobian are given by $J_{ij} = \\frac{\\partial U_i}{\\partial \\theta_j} = \\frac{\\partial^2 l}{\\partial \\theta_j \\partial \\theta_i}$. Note that this is the Hessian matrix of the log-likelihood function $l(\\theta)$.\n\nTo find the next iterate $\\theta^{(k+1)}$, we set $U(\\theta^{(k+1)}) = 0$ in the approximation:\n$$\n0 \\approx U(\\theta^{(k)}) + J(\\theta^{(k)}) (\\theta^{(k+1)} - \\theta^{(k)})\n$$\nRearranging this equation to solve for the update step $(\\theta^{(k+1)} - \\theta^{(k)})$, we get:\n$$\nJ(\\theta^{(k)}) (\\theta^{(k+1)} - \\theta^{(k)}) = -U(\\theta^{(k)})\n$$\nAssuming that the Jacobian matrix $J(\\theta^{(k)})$ is invertible, we can multiply by its inverse, $[J(\\theta^{(k)})]^{-1}$:\n$$\n\\theta^{(k+1)} - \\theta^{(k)} = -[J(\\theta^{(k)})]^{-1} U(\\theta^{(k)})\n$$\nThis leads to the Newton–Raphson update rule for iteratively finding the MLE:\n$$\n\\theta^{(k+1)} = \\theta^{(k)} - [J(\\theta^{(k)})]^{-1} U(\\theta^{(k)})\n$$\n\n### Part 2: Score and Jacobian for Logistic Regression\n\nWe now specialize to the binary logistic regression model with parameters $\\beta = (\\beta_0, \\beta_1)^{\\top}$. The log-likelihood for $n$ independent observations is given by:\n$$\nl(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} \\left[ y_i \\ln p_i + (1-y_i)\\ln(1-p_i) \\right]\n$$\nwhere $p_i = \\Pr(y_i=1 \\mid x_i) = \\frac{1}{1+\\exp(-\\eta_i)}$ and $\\eta_i = \\beta_0 + \\beta_1 x_i$.\n\nFirst, we find a crucial derivative of $p_i$ with respect to $\\eta_i$:\n$$\n\\frac{\\partial p_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i} \\left( (1+\\exp(-\\eta_i))^{-1} \\right) = -1 \\cdot (1+\\exp(-\\eta_i))^{-2} \\cdot (-\\exp(-\\eta_i)) = \\frac{\\exp(-\\eta_i)}{(1+\\exp(-\\eta_i))^2}\n$$\nThis can be rewritten as:\n$$\n\\frac{\\partial p_i}{\\partial \\eta_i} = \\frac{1}{1+\\exp(-\\eta_i)} \\cdot \\frac{\\exp(-\\eta_i)}{1+\\exp(-\\eta_i)} = p_i \\cdot \\left( \\frac{1+\\exp(-\\eta_i)-1}{1+\\exp(-\\eta_i)} \\right) = p_i \\cdot \\left( 1 - \\frac{1}{1+\\exp(-\\eta_i)} \\right) = p_i(1-p_i)\n$$\n\nThe score vector $U(\\beta_0, \\beta_1)$ has components $\\frac{\\partial l}{\\partial \\beta_0}$ and $\\frac{\\partial l}{\\partial \\beta_1}$. We use the chain rule:\n$$\n\\frac{\\partial l}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial p_i} \\frac{\\partial p_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\nThe derivative of the $i$-th log-likelihood term with respect to $p_i$ is:\n$$\n\\frac{\\partial l_i}{\\partial p_i} = \\frac{y_i}{p_i} - \\frac{1-y_i}{1-p_i} = \\frac{y_i(1-p_i) - (1-y_i)p_i}{p_i(1-p_i)} = \\frac{y_i - p_i}{p_i(1-p_i)}\n$$\nCombining these results:\n$$\n\\frac{\\partial l_i}{\\partial \\beta_j} = \\left( \\frac{y_i - p_i}{p_i(1-p_i)} \\right) \\cdot (p_i(1-p_i)) \\cdot \\frac{\\partial \\eta_i}{\\partial \\beta_j} = (y_i - p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\nThe derivatives of $\\eta_i$ are $\\frac{\\partial \\eta_i}{\\partial \\beta_0} = 1$ and $\\frac{\\partial \\eta_i}{\\partial \\beta_1} = x_i$.\nThe components of the score vector are:\n$$\nU_0 = \\frac{\\partial l}{\\partial \\beta_0} = \\sum_{i=1}^{n} (y_i - p_i) \\cdot 1 = \\sum_{i=1}^{n} (y_i - p_i)\n$$\n$$\nU_1 = \\frac{\\partial l}{\\partial \\beta_1} = \\sum_{i=1}^{n} (y_i - p_i) \\cdot x_i = \\sum_{i=1}^{n} x_i(y_i - p_i)\n$$\nSo the score vector is $U(\\beta_0, \\beta_1) = \\begin{pmatrix} \\sum_{i=1}^{n} (y_i - p_i) \\\\ \\sum_{i=1}^{n} x_i(y_i - p_i) \\end{pmatrix}$.\n\nNext, we derive the Jacobian matrix $J(\\beta_0, \\beta_1)$, whose elements are $J_{jk} = \\frac{\\partial U_j}{\\partial \\beta_k} = \\frac{\\partial^2 l}{\\partial \\beta_k \\partial \\beta_j}$.\n$$\n\\frac{\\partial^2 l}{\\partial \\beta_k \\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^{n} (y_i - p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_j} \\right) = \\sum_{i=1}^{n} \\left( -\\frac{\\partial p_i}{\\partial \\beta_k} \\right) \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\nUsing the chain rule again, $\\frac{\\partial p_i}{\\partial \\beta_k} = \\frac{\\partial p_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_k} = p_i(1-p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_k}$.\nSubstituting this gives the general form of the Jacobian elements:\n$$\nJ_{jk} = -\\sum_{i=1}^{n} p_i(1-p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_k} \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\nNow we compute the specific elements:\n$$\nJ_{00} = \\frac{\\partial^2 l}{\\partial \\beta_0^2} = -\\sum_{i=1}^{n} p_i(1-p_i) (1)(1) = -\\sum_{i=1}^{n} p_i(1-p_i)\n$$\n$$\nJ_{01} = \\frac{\\partial^2 l}{\\partial \\beta_1 \\partial \\beta_0} = -\\sum_{i=1}^{n} p_i(1-p_i) (x_i)(1) = -\\sum_{i=1}^{n} x_i p_i(1-p_i)\n$$\nBy symmetry (Clairaut's theorem), $J_{10} = J_{01}$.\n$$\nJ_{11} = \\frac{\\partial^2 l}{\\partial \\beta_1^2} = -\\sum_{i=1}^{n} p_i(1-p_i) (x_i)(x_i) = -\\sum_{i=1}^{n} x_i^2 p_i(1-p_i)\n$$\nThus, the Jacobian matrix is $J(\\beta_0, \\beta_1) = \\begin{pmatrix} -\\sum p_i(1-p_i)  -\\sum x_i p_i(1-p_i) \\\\ -\\sum x_i p_i(1-p_i)  -\\sum x_i^2 p_i(1-p_i) \\end{pmatrix}$.\n\n### Part 3: Numerical Application\n\nWe apply one Newton–Raphson step starting from $\\beta^{(0)} = (\\beta_0^{(0)}, \\beta_1^{(0)})^{\\top} = (0,0)^{\\top}$.\nThe dataset is $x = (-2, -1, 0, 1, 2, 0)$ and $y = (0, 0, 0, 1, 1, 0)$ for $n=6$.\n\nFirst, evaluate probabilities at $\\beta^{(0)}$. For any $x_i$, $\\eta_i^{(0)} = \\beta_0^{(0)} + \\beta_1^{(0)} x_i = 0 + 0 \\cdot x_i = 0$.\nThe probability for each patient is $p_i^{(0)} = \\frac{1}{1+\\exp(-0)} = \\frac{1}{1+1} = 0.5$.\n\nNext, calculate the score vector $U(\\beta^{(0)})$:\nWe need the sums:\n$\\sum_{i=1}^6 y_i = 0+0+0+1+1+0 = 2$.\n$\\sum_{i=1}^6 x_i = -2-1+0+1+2+0 = 0$.\n$\\sum_{i=1}^6 x_i y_i = (-2)(0) + (-1)(0) + (0)(0) + (1)(1) + (2)(1) + (0)(0) = 3$.\nThe components of the score vector are:\n$U_0^{(0)} = \\sum_{i=1}^6 (y_i - p_i^{(0)}) = \\sum y_i - \\sum p_i^{(0)} = 2 - 6 \\times 0.5 = 2 - 3 = -1$.\n$U_1^{(0)} = \\sum_{i=1}^6 x_i (y_i - p_i^{(0)}) = \\sum x_i y_i - p_i^{(0)} \\sum x_i = 3 - 0.5 \\times 0 = 3$.\nSo, $U(\\beta^{(0)}) = \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix}$.\n\nNext, calculate the Jacobian matrix $J(\\beta^{(0)})$:\nFor all $i$, $p_i^{(0)}(1-p_i^{(0)}) = 0.5 \\times (1-0.5) = 0.25$.\nWe need the sum of squares of $x_i$:\n$\\sum_{i=1}^6 x_i^2 = (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 + 0^2 = 4+1+0+1+4+0 = 10$.\nThe elements of the Jacobian are:\n$J_{00}^{(0)} = -\\sum p_i^{(0)}(1-p_i^{(0)}) = -6 \\times 0.25 = -1.5$.\n$J_{01}^{(0)} = -\\sum x_i p_i^{(0)}(1-p_i^{(0)}) = -0.25 \\sum x_i = -0.25 \\times 0 = 0$.\n$J_{11}^{(0)} = -\\sum x_i^2 p_i^{(0)}(1-p_i^{(0)}) = -0.25 \\sum x_i^2 = -0.25 \\times 10 = -2.5$.\nSo, $J(\\beta^{(0)}) = \\begin{pmatrix} -1.5  0 \\\\ 0  -2.5 \\end{pmatrix}$.\n\nFinally, perform the update to find $\\beta^{(1)} = (\\beta_0^{(1)}, \\beta_1^{(1)})^{\\top}$:\n$$\n\\beta^{(1)} = \\beta^{(0)} - [J(\\beta^{(0)})]^{-1} U(\\beta^{(0)})\n$$\nThe inverse of the diagonal Jacobian matrix is:\n$$\n[J(\\beta^{(0)})]^{-1} = \\begin{pmatrix} 1/(-1.5)  0 \\\\ 0  1/(-2.5) \\end{pmatrix} = \\begin{pmatrix} -2/3  0 \\\\ 0  -2/5 \\end{pmatrix}\n$$\nNow, we compute $\\beta^{(1)}$:\n$$\n\\beta^{(1)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} -2/3  0 \\\\ 0  -2/5 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix} = -\\begin{pmatrix} (-2/3)(-1) + (0)(3) \\\\ (0)(-1) + (-2/5)(3) \\end{pmatrix} = -\\begin{pmatrix} 2/3 \\\\ -6/5 \\end{pmatrix} = \\begin{pmatrix} -2/3 \\\\ 6/5 \\end{pmatrix}\n$$\nIn decimal form, this is $\\beta^{(1)} = \\begin{pmatrix} -0.6666... \\\\ 1.2 \\end{pmatrix}$.\nRounding each entry to four significant figures, we get:\n$\\beta_0^{(1)} \\approx -0.6667$\n$\\beta_1^{(1)} = 1.200$\n\nThe updated parameter vector is $\\beta^{(1)} \\approx (-0.6667, 1.200)^{\\top}$. Expressed as a $1 \\times 2$ row matrix as requested:\n$\\begin{pmatrix} -0.6667  1.200 \\end{pmatrix}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} -0.6667  1.200 \\end{pmatrix}}\n$$", "id": "4969348"}, {"introduction": "Beyond point estimation, a primary use of likelihood is to perform hypothesis tests. The Likelihood Ratio Test (LRT) offers a general and powerful framework for comparing nested models, forming one of the three pillars of likelihood-based inference. This practice will guide you through deriving the LRT statistic, which elegantly compares the maximized likelihoods under the null and alternative hypotheses. By applying this principle to test the rate parameter, $\\lambda$, of a Poisson distribution and invoking Wilks' theorem, you will see how the theory of maximum likelihood directly translates into a practical tool for statistical inference [@problem_id:4922734].", "problem": "A hospital infection-control unit monitors the weekly counts of new hospital-acquired infections over $15$ consecutive weeks. Assume the weekly counts $X_{1}, \\ldots, X_{n}$ are independent and identically distributed as a Poisson distribution with rate parameter $\\lambda \\in (0,\\infty)$. The observed counts (in chronological order) are $2, 0, 3, 1, 4, 2, 5, 3, 1, 2, 2, 4, 3, 0, 1$. The unit wishes to assess whether the baseline rate equals a reference value $\\lambda_{0} = 2$ infections per week.\n\nTasks:\n1) Starting only from the definitions of the likelihood function $L(\\theta)$, the log-likelihood function $\\ell(\\theta) = \\ln L(\\theta)$, the maximum likelihood estimator $\\hat{\\theta}$, the constrained maximum likelihood estimator $\\hat{\\theta}_{0}$ under a null hypothesis parameter subspace $\\Theta_{0} \\subset \\Theta$, and the likelihood ratio $\\Lambda = \\dfrac{\\sup_{\\theta \\in \\Theta_{0}} L(\\theta)}{\\sup_{\\theta \\in \\Theta} L(\\theta)}$, derive the likelihood ratio test statistic in terms of the difference between the maximized log-likelihoods under the alternative and under the null.\n2) State Wilks’ theorem, including the asymptotic distribution of the likelihood ratio statistic under the null hypothesis and how its degrees of freedom are determined.\n3) Specialize to the Poisson model for testing $H_{0}: \\lambda = \\lambda_{0}$ against $H_{1}: \\lambda \\neq \\lambda_{0}$. Derive the closed-form expression of the likelihood ratio statistic in terms of $n$, the sample mean $\\bar{X}$, and $\\lambda_{0}$. Then compute its numerical value for the given data using $\\lambda_{0} = 2$ and $n = 15$. Use the natural logarithm and round your final numerical answer to four significant figures. No units are required in the final numerical answer.", "solution": "### Task 1: Derivation of the Likelihood Ratio Test Statistic\n\nWe are asked to derive the form of the likelihood ratio test statistic, starting from its fundamental definitions.\n\nLet $\\theta$ be a parameter in a parameter space $\\Theta$. Let $L(\\theta)$ be the likelihood function for a given set of data. The log-likelihood function is defined as $\\ell(\\theta) = \\ln L(\\theta)$.\n\nThe likelihood ratio for a null hypothesis $H_0: \\theta \\in \\Theta_0$ against an alternative $H_1: \\theta \\in \\Theta \\setminus \\Theta_0$, where $\\Theta_0$ is a subset of $\\Theta$, is defined as:\n$$\n\\Lambda = \\frac{\\sup_{\\theta \\in \\Theta_{0}} L(\\theta)}{\\sup_{\\theta \\in \\Theta} L(\\theta)}\n$$\nThe value of the parameter that maximizes the likelihood over the entire parameter space $\\Theta$ is the maximum likelihood estimator (MLE), denoted $\\hat{\\theta}$. Thus, the denominator is the likelihood evaluated at the MLE:\n$$\n\\sup_{\\theta \\in \\Theta} L(\\theta) = L(\\hat{\\theta})\n$$\nThe value of the parameter that maximizes the likelihood over the constrained parameter space $\\Theta_0$ is the constrained maximum likelihood estimator, denoted $\\hat{\\theta}_0$. Thus, the numerator is the likelihood evaluated at the constrained MLE:\n$$\n\\sup_{\\theta \\in \\Theta_{0}} L(\\theta) = L(\\hat{\\theta}_{0})\n$$\nSubstituting these into the definition of the likelihood ratio gives:\n$$\n\\Lambda = \\frac{L(\\hat{\\theta}_0)}{L(\\hat{\\theta})}\n$$\nThe likelihood ratio test statistic, often denoted by $D$ or $-2 \\ln \\Lambda$, is defined as:\n$$\nD = -2 \\ln \\Lambda\n$$\nSubstituting the expression for $\\Lambda$:\n$$\nD = -2 \\ln \\left( \\frac{L(\\hat{\\theta}_0)}{L(\\hat{\\theta})} \\right)\n$$\nUsing the property of logarithms that $\\ln(a/b) = \\ln(a) - \\ln(b)$, we get:\n$$\nD = -2 \\left[ \\ln(L(\\hat{\\theta}_0)) - \\ln(L(\\hat{\\theta})) \\right]\n$$\nRecalling the definition of the log-likelihood function, $\\ell(\\theta) = \\ln(L(\\theta))$, we can write this as:\n$$\nD = -2 \\left[ \\ell(\\hat{\\theta}_0) - \\ell(\\hat{\\theta}) \\right]\n$$\nDistributing the $-2$ factor, we arrive at the desired expression:\n$$\nD = 2 \\left[ \\ell(\\hat{\\theta}) - \\ell(\\hat{\\theta}_0) \\right]\n$$\nThis shows that the likelihood ratio test statistic is twice the difference between the maximized log-likelihood under the alternative hypothesis (i.e., over the full parameter space) and the maximized log-likelihood under the null hypothesis (i.e., over the constrained parameter space).\n\n### Task 2: Statement of Wilks’ Theorem\n\nWilks’ theorem describes the asymptotic distribution of the likelihood ratio test statistic.\n\nLet $X_1, \\ldots, X_n$ be an independent and identically distributed (i.i.d.) random sample from a probability distribution with a parameter vector $\\theta$ belonging to a parameter space $\\Theta \\subseteq \\mathbb{R}^k$. Consider testing the null hypothesis $H_0: \\theta \\in \\Theta_0$ against the alternative hypothesis $H_1: \\theta \\in \\Theta \\setminus \\Theta_0$. Assume that the null parameter space $\\Theta_0$ is a subspace of $\\Theta$ defined by $r$ independent constraints on the parameters of $\\theta$. This implies that the dimension of $\\Theta$ is $k$ and the dimension of $\\Theta_0$ is $k-r$.\n\nLet $D = -2 \\ln \\Lambda = 2(\\ell(\\hat{\\theta}) - \\ell(\\hat{\\theta}_0))$ be the likelihood ratio test statistic, where $\\hat{\\theta}$ is the maximum likelihood estimator of $\\theta$ in $\\Theta$ and $\\hat{\\theta}_0$ is the maximum likelihood estimator of $\\theta$ in $\\Theta_0$.\n\nUnder certain regularity conditions (which hold for the Poisson model and many other standard distributions), Wilks' theorem states that as the sample size $n$ approaches infinity, the distribution of the statistic $D$ under the null hypothesis $H_0$ converges to a chi-squared ($\\chi^2$) distribution. The degrees of freedom of this limiting chi-squared distribution are equal to the difference in the dimensionality of the parameter spaces $\\Theta$ and $\\Theta_0$.\n$$\n\\text{Degrees of freedom, } \\nu = \\dim(\\Theta) - \\dim(\\Theta_0) = k - (k-r) = r\n$$\nThus, under $H_0$, as $n \\to \\infty$, we have the convergence in distribution:\n$$\nD \\xrightarrow{d} \\chi^2_r\n$$\n\n### Task 3: Specialization to Poisson Model and Numerical Computation\n\nWe are given a sample $X_1, \\ldots, X_n$ of i.i.d. observations from a Poisson($\\lambda$) distribution. The probability mass function (PMF) for a single observation $X_i=x_i$ is:\n$$\nP(X_i = x_i; \\lambda) = \\frac{\\lambda^{x_i} \\exp(-\\lambda)}{x_i!}\n$$\nThe likelihood function for the entire sample $x = (x_1, \\ldots, x_n)$ is the product of the individual PMFs:\n$$\nL(\\lambda; x) = \\prod_{i=1}^{n} \\frac{\\lambda^{x_i} \\exp(-\\lambda)}{x_i!} = \\frac{\\lambda^{\\sum_{i=1}^{n} x_i} \\exp(-n\\lambda)}{\\prod_{i=1}^{n} x_i!}\n$$\nThe log-likelihood function is:\n$$\n\\ell(\\lambda) = \\ln L(\\lambda) = \\left(\\sum_{i=1}^{n} x_i\\right) \\ln(\\lambda) - n\\lambda - \\sum_{i=1}^{n} \\ln(x_i!)\n$$\nWe are testing $H_0: \\lambda = \\lambda_0$ against $H_1: \\lambda \\neq \\lambda_0$.\nThe full parameter space is $\\Theta = (0, \\infty)$, which is $1$-dimensional. The null parameter space is $\\Theta_0 = \\{\\lambda_0\\}$, which is $0$-dimensional. The difference in dimensions is $1-0=1$, so the test statistic will asymptotically follow a $\\chi^2_1$ distribution.\n\nFirst, we find the unconstrained MLE, $\\hat{\\lambda}$. We differentiate $\\ell(\\lambda)$ with respect to $\\lambda$ and set the result to zero:\n$$\n\\frac{d\\ell}{d\\lambda} = \\frac{\\sum x_i}{\\lambda} - n = 0 \\implies \\hat{\\lambda} = \\frac{\\sum x_i}{n} = \\bar{X}\n$$\nThe maximized log-likelihood under the alternative is $\\ell(\\hat{\\lambda}) = \\ell(\\bar{X})$.\n\nNext, we find the constrained MLE, $\\hat{\\lambda}_0$. Under $H_0$, the parameter space $\\Theta_0$ contains only one point, $\\lambda_0$. Thus, the likelihood is maximized at this value:\n$$\n\\hat{\\lambda}_0 = \\lambda_0\n$$\nThe maximized log-likelihood under the null is $\\ell(\\hat{\\lambda}_0) = \\ell(\\lambda_0)$.\n\nNow, we construct the likelihood ratio statistic $D = 2[\\ell(\\hat{\\lambda}) - \\ell(\\hat{\\lambda}_0)]$:\n$$\n\\ell(\\hat{\\lambda}) = \\left(\\sum X_i\\right) \\ln(\\bar{X}) - n\\bar{X} - \\sum \\ln(X_i!)\n$$\n$$\n\\ell(\\hat{\\lambda}_0) = \\left(\\sum X_i\\right) \\ln(\\lambda_0) - n\\lambda_0 - \\sum \\ln(X_i!)\n$$\nSubstituting $\\sum X_i = n\\bar{X}$:\n$$\n\\ell(\\hat{\\lambda}) = n\\bar{X} \\ln(\\bar{X}) - n\\bar{X} - \\sum \\ln(X_i!)\n$$\n$$\n\\ell(\\hat{\\lambda}_0) = n\\bar{X} \\ln(\\lambda_0) - n\\lambda_0 - \\sum \\ln(X_i!)\n$$\nThe difference is:\n$$\n\\ell(\\hat{\\lambda}) - \\ell(\\hat{\\lambda}_0) = (n\\bar{X} \\ln(\\bar{X}) - n\\bar{X}) - (n\\bar{X} \\ln(\\lambda_0) - n\\lambda_0)\n$$\n$$\n= n\\bar{X} \\ln(\\bar{X}) - n\\bar{X} - n\\bar{X} \\ln(\\lambda_0) + n\\lambda_0\n$$\n$$\n= n \\left[ \\bar{X} (\\ln(\\bar{X}) - \\ln(\\lambda_0)) - (\\bar{X} - \\lambda_0) \\right]\n$$\n$$\n= n \\left[ \\bar{X} \\ln\\left(\\frac{\\bar{X}}{\\lambda_0}\\right) - \\bar{X} + \\lambda_0 \\right]\n$$\nThe likelihood ratio statistic is $D = 2(\\ell(\\hat{\\lambda}) - \\ell(\\hat{\\lambda}_0))$:\n$$\nD = 2n \\left[ \\lambda_0 - \\bar{X} + \\bar{X} \\ln\\left(\\frac{\\bar{X}}{\\lambda_0}\\right) \\right]\n$$\nThis is the closed-form expression of the statistic in terms of $n$, $\\bar{X}$, and $\\lambda_0$.\n\nFinally, we compute its numerical value for the given data.\nThe data are: $2, 0, 3, 1, 4, 2, 5, 3, 1, 2, 2, 4, 3, 0, 1$.\nThe sample size is $n=15$.\nThe sum of observations is $\\sum x_i = 2+0+3+1+4+2+5+3+1+2+2+4+3+0+1 = 33$.\nThe sample mean is $\\bar{x} = \\frac{33}{15} = 2.2$.\nThe null hypothesis value is $\\lambda_0 = 2$.\n\nSubstituting these values into the expression for $D$:\n$$\nD = 2(15) \\left[ 2 - 2.2 + 2.2 \\ln\\left(\\frac{2.2}{2}\\right) \\right]\n$$\n$$\nD = 30 \\left[ -0.2 + 2.2 \\ln(1.1) \\right]\n$$\nUsing the natural logarithm:\n$$\n\\ln(1.1) \\approx 0.0953101798\n$$\n$$\nD \\approx 30 \\left[ -0.2 + 2.2 \\times 0.0953101798 \\right]\n$$\n$$\nD \\approx 30 \\left[ -0.2 + 0.20968239556 \\right]\n$$\n$$\nD \\approx 30 \\left[ 0.00968239556 \\right]\n$$\n$$\nD \\approx 0.2904718668\n$$\nRounding the final numerical answer to four significant figures gives $0.2905$.", "answer": "$$\\boxed{0.2905}$$", "id": "4922734"}]}