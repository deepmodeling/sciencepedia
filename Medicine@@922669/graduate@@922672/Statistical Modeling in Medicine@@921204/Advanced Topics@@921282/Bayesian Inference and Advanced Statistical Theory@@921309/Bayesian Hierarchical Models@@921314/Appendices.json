{"hands_on_practices": [{"introduction": "Hierarchical models are essential when data possesses a grouped structure, such as patients nested within hospitals in a multi-center clinical trial. Before building a complex model, it is crucial to quantify how much of the total variation is due to differences *between* groups versus *within* them. By deriving the Intraclass Correlation Coefficient (ICC) for a random-intercept model, you will gain a first-principles understanding of how variance is partitioned and what it means for observations from the same group to be correlated [@problem_id:4953506].", "problem": "Consider a multi-center clinical study of a continuous biomarker, where patients are nested within centers (e.g., hospitals). Suppose the response for patient $i$ in center $j$ is modeled as a Bayesian hierarchical random-intercept model\n$$\ny_{ij} \\,=\\, \\mu \\,+\\, u_{j} \\,+\\, \\epsilon_{ij},\n$$\nwith center-specific random intercepts $u_{j} \\sim \\mathcal{N}(0,\\tau^{2})$ and patient-level errors $\\epsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^{2})$. Assume that $u_{j}$ and $\\epsilon_{ij}$ are mutually independent across all indices and independent of $\\mu$ (treated as a constant for the purpose of variance calculations). Using only fundamental definitions of variance, covariance, and correlation, derive the intraclass correlation coefficient (ICC) defined as\n$$\n\\operatorname{Corr}\\!\\big(y_{ij},\\,y_{i' j}\\big)\n$$\nfor $i \\neq i'$ within the same center $j$, under the marginal distribution that integrates over $u_{j}$. Then, briefly interpret the ICC in clinical terms, focusing on what it quantifies about within-center similarity and between-center heterogeneity in this medical context. Express your final answer as a simplified symbolic expression in terms of $\\tau^{2}$ and $\\sigma^{2}$. No rounding is required.", "solution": "The problem statement is evaluated as valid. It is a well-posed, scientifically grounded problem in statistical modeling, with all necessary information provided and no internal contradictions. It requests the derivation of a standard statistical quantity, the intraclass correlation coefficient (ICC), from a clearly defined hierarchical model.\n\nThe problem requires the derivation of the intraclass correlation coefficient (ICC), defined as $\\operatorname{Corr}(y_{ij}, y_{i'j})$ for two distinct patients ($i \\neq i'$) within the same center $j$. The model for the response variable $y_{ij}$ is given by:\n$$\ny_{ij} = \\mu + u_j + \\epsilon_{ij}\n$$\nwhere $\\mu$ is a constant overall mean, $u_j$ is the random intercept for center $j$, and $\\epsilon_{ij}$ is the random error for patient $i$ in center $j$.\n\nThe distributional assumptions are:\n1.  Center-specific random intercepts: $u_j \\sim \\mathcal{N}(0, \\tau^2)$, which implies $\\operatorname{E}[u_j] = 0$ and $\\operatorname{Var}(u_j) = \\tau^2$.\n2.  Patient-level errors: $\\epsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$, which implies $\\operatorname{E}[\\epsilon_{ij}] = 0$ and $\\operatorname{Var}(\\epsilon_{ij}) = \\sigma^2$.\n3.  The random variables $u_j$ and $\\epsilon_{ij}$ are mutually independent across all indices. This implies that for any $i, i', j, j'$, $\\operatorname{Cov}(u_j, \\epsilon_{i'j'}) = 0$. It also implies that for $i \\neq i'$, $\\operatorname{Cov}(\\epsilon_{ij}, \\epsilon_{i'j}) = 0$.\n\nThe definition of the correlation between two random variables $X$ and $Y$ is:\n$$\n\\operatorname{Corr}(X, Y) = \\frac{\\operatorname{Cov}(X, Y)}{\\sqrt{\\operatorname{Var}(X) \\operatorname{Var}(Y)}}\n$$\nIn this problem, we are interested in $\\operatorname{Corr}(y_{ij}, y_{i'j})$. We must first compute the variance of an observation, $\\operatorname{Var}(y_{ij})$, and the covariance between two observations from the same center, $\\operatorname{Cov}(y_{ij}, y_{i'j})$.\n\nFirst, let's compute the variance of $y_{ij}$. Since $\\mu$ is treated as a constant, its variance is zero.\n$$\n\\operatorname{Var}(y_{ij}) = \\operatorname{Var}(\\mu + u_j + \\epsilon_{ij}) = \\operatorname{Var}(u_j + \\epsilon_{ij})\n$$\nUsing the property that the variance of a sum of independent random variables is the sum of their variances:\n$$\n\\operatorname{Var}(y_{ij}) = \\operatorname{Var}(u_j) + \\operatorname{Var}(\\epsilon_{ij})\n$$\nSubstituting the given variances:\n$$\n\\operatorname{Var}(y_{ij}) = \\tau^2 + \\sigma^2\n$$\nBy the same logic, the variance for observation $y_{i'j}$ is identical, as the underlying distributions for $u_j$ and $\\epsilon_{i'j}$ are the same:\n$$\n\\operatorname{Var}(y_{i'j}) = \\tau^2 + \\sigma^2\n$$\n\nNext, we compute the covariance between $y_{ij}$ and $y_{i'j}$ for $i \\neq i'$.\n$$\n\\operatorname{Cov}(y_{ij}, y_{i'j}) = \\operatorname{Cov}(\\mu + u_j + \\epsilon_{ij}, \\mu + u_j + \\epsilon_{i'j})\n$$\nUsing the properties of covariance, specifically its bilinearity and the fact that constants do not contribute to covariance:\n$$\n\\operatorname{Cov}(y_{ij}, y_{i'j}) = \\operatorname{Cov}(u_j + \\epsilon_{ij}, u_j + \\epsilon_{i'j})\n$$\nExpanding this expression:\n$$\n\\operatorname{Cov}(y_{ij}, y_{i'j}) = \\operatorname{Cov}(u_j, u_j) + \\operatorname{Cov}(u_j, \\epsilon_{i'j}) + \\operatorname{Cov}(\\epsilon_{ij}, u_j) + \\operatorname{Cov}(\\epsilon_{ij}, \\epsilon_{i'j})\n$$\nWe evaluate each term based on the problem's assumptions:\n1.  $\\operatorname{Cov}(u_j, u_j) = \\operatorname{Var}(u_j) = \\tau^2$.\n2.  $\\operatorname{Cov}(u_j, \\epsilon_{i'j}) = 0$, because $u_j$ and $\\epsilon_{i'j}$ are independent.\n3.  $\\operatorname{Cov}(\\epsilon_{ij}, u_j) = 0$, for the same reason.\n4.  $\\operatorname{Cov}(\\epsilon_{ij}, \\epsilon_{i'j}) = 0$, because patient-level errors are independent for different patients ($i \\neq i'$).\n\nSumming these terms gives the covariance:\n$$\n\\operatorname{Cov}(y_{ij}, y_{i'j}) = \\tau^2 + 0 + 0 + 0 = \\tau^2\n$$\nThe covariance between two different patients' outcomes in the same center is equal to the between-center variance, $\\tau^2$. This is because the shared random effect $u_j$ is the only source of statistical dependence between their outcomes.\n\nNow we can assemble the ICC using the derived variance and covariance:\n$$\n\\operatorname{ICC} = \\operatorname{Corr}(y_{ij}, y_{i'j}) = \\frac{\\operatorname{Cov}(y_{ij}, y_{i'j})}{\\sqrt{\\operatorname{Var}(y_{ij}) \\operatorname{Var}(y_{i'j})}}\n$$\n$$\n\\operatorname{ICC} = \\frac{\\tau^2}{\\sqrt{(\\tau^2 + \\sigma^2)(\\tau^2 + \\sigma^2)}} = \\frac{\\tau^2}{\\tau^2 + \\sigma^2}\n$$\n\nThe interpretation of the ICC in this clinical context follows directly from its formula. The total variance of the biomarker measurements, $\\operatorname{Var}(y_{ij}) = \\tau^2 + \\sigma^2$, is partitioned into two components:\n- $\\tau^2$: The variance of the center-specific effects, representing the heterogeneity *between* centers.\n- $\\sigma^2$: The variance of patient-specific errors, representing the heterogeneity *within* centers.\n\nThe ICC, $\\frac{\\tau^2}{\\tau^2 + \\sigma^2}$, is thus the proportion of the total variance in the biomarker that is attributable to systematic differences between the clinical centers.\n- A **high ICC** (close to $1$) indicates that $\\tau^2$ is large relative to $\\sigma^2$. This implies that patients within the same center are highly similar to one another, and there are large, systematic differences between centers. In clinical terms, the choice of hospital significantly influences the patient's biomarker level. This could be due to differences in measurement protocols, local environmental factors, or the specific sub-population each center serves.\n- A **low ICC** (close to $0$) indicates that $\\tau^2$ is small relative to $\\sigma^2$. This implies that most of the variation is due to individual patient differences, not the center they belong to. The outcomes of patients within a center are not strongly correlated, and the centers themselves are relatively homogeneous. The specific hospital has little bearing on the biomarker outcome.\nIn summary, the ICC quantifies the degree of clustering or non-independence of observations within the same center, providing a measure of how much the between-center heterogeneity contributes to the total observed variation in the clinical study.", "answer": "$$\n\\boxed{\\frac{\\tau^{2}}{\\tau^{2} + \\sigma^{2}}}\n$$", "id": "4953506"}, {"introduction": "Once we identify group-level effects, a key challenge is estimating them reliably, especially for groups with sparse data. Bayesian hierarchical models address this through \"partial pooling,\" where a group-specific estimate is informed by both its own data and the overall distribution of effects across all groups—a phenomenon known as shrinkage. This exercise asks you to derive the shrinkage factor in a classic Beta-Binomial model, revealing precisely how the posterior estimate becomes a weighted average of the local data and the global, pooled mean [@problem_id:4800178].", "problem": "Consider a multicenter surgical cohort where $J$ hospitals independently report counts of post-operative complications. For hospital $j \\in \\{1,\\dots,J\\}$, let $y_j$ denote the number of patients with a complication out of $n_j$ operated patients, modeled as $y_j \\sim \\text{Binomial}(n_j,\\theta_j)$ with a hospital-specific complication probability $0 < \\theta_j < 1$. Assume an exchangeable hierarchical Bayesian model with the prior $\\theta_j \\mid \\alpha,\\beta \\sim \\text{Beta}(\\alpha,\\beta)$, where $(\\alpha,\\beta)$ have proper hyperpriors that encode uncertainty about the across-hospital complication profile. Let the pooled mean under the exchangeable prior be $\\mu_0 = \\frac{\\alpha}{\\alpha+\\beta}$ and the empirical hospital complication rate be $\\hat{p}_j = \\frac{y_j}{n_j}$.\n\nDefine the posterior mean shrinkage of $\\theta_j$ toward the pooled mean as the unique scalar $\\kappa_j \\in (0,1)$ such that the posterior mean of $\\theta_j$ given $(y_j,\\alpha,\\beta)$ can be written as the convex combination $E[\\theta_j \\mid y_j,\\alpha,\\beta] = \\kappa_j \\mu_0 + (1-\\kappa_j) \\hat{p}_j$. Starting from Bayes’ theorem and the Binomial likelihood with a Beta prior, derive a closed-form expression for $\\kappa_j$ as a function of $n_j$, $\\alpha$, and $\\beta$. Express your final answer as a single analytic expression. No numerical rounding is required.", "solution": "The user wants me to solve a problem based on the provided problem statement.\n\n### Step 1: Extract Givens\n-   **Model:** For hospital $j \\in \\{1, \\dots, J\\}$, the number of complications $y_j$ out of $n_j$ patients is modeled as $y_j \\sim \\text{Binomial}(n_j, \\theta_j)$.\n-   **Parameter of Interest:** $\\theta_j$, the hospital-specific complication probability, where $0 < \\theta_j < 1$.\n-   **Prior:** The parameters $\\theta_j$ are given an exchangeable prior from a Beta distribution: $\\theta_j \\mid \\alpha, \\beta \\sim \\text{Beta}(\\alpha, \\beta)$. The hyperparameters $(\\alpha, \\beta)$ have their own proper hyperpriors.\n-   **Definitions:**\n    -   Pooled mean (prior mean): $\\mu_0 = \\frac{\\alpha}{\\alpha+\\beta}$.\n    -   Empirical hospital complication rate: $\\hat{p}_j = \\frac{y_j}{n_j}$.\n-   **Target Expression:** The posterior mean of $\\theta_j$ given the data $y_j$ and hyperparameters $\\alpha, \\beta$ is expressed as a convex combination: $E[\\theta_j \\mid y_j, \\alpha, \\beta] = \\kappa_j \\mu_0 + (1-\\kappa_j) \\hat{p}_j$.\n-   **Objective:** Derive a closed-form expression for the shrinkage factor $\\kappa_j \\in (0,1)$ as a function of $n_j$, $\\alpha$, and $\\beta$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically and mathematically sound. It describes a standard and widely used hierarchical Bayesian model, specifically the Beta-Binomial model, which is a classic example of Bayesian inference with conjugate priors.\n-   **Scientifically Grounded:** The model is a cornerstone of statistical analysis in fields like medicine and public health for modeling rates and proportions across different groups. There are no violations of scientific principles.\n-   **Well-Posed:** The use of a Beta prior for a Binomial proportion is a conjugate pairing, which guarantees a closed-form posterior distribution (also a Beta distribution). This ensures that the posterior mean is well-defined and can be calculated analytically, leading to a unique solution for $\\kappa_j$.\n-   **Objective:** The language is precise and all terms are formally defined within the context of probability theory and Bayesian statistics.\n-   **Completeness:** The problem provides all necessary information (likelihood, prior, definitions) to perform the derivation.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the derivation.\n\nTo find the expression for the shrinkage factor $\\kappa_j$, we must first derive the posterior mean $E[\\theta_j \\mid y_j, \\alpha, \\beta]$ from first principles using Bayes' theorem and then equate it to the given convex combination form.\n\nThe derivation proceeds in three main steps:\n1.  Determine the posterior distribution of $\\theta_j$.\n2.  Calculate the mean of this posterior distribution.\n3.  Rearrange the expression for the posterior mean to match the specified structure and identify $\\kappa_j$.\n\n**1. Posterior Distribution of $\\theta_j$**\n\nAccording to Bayes' theorem, the posterior probability density function (PDF) for $\\theta_j$ is proportional to the product of the likelihood and the prior PDF.\n$p(\\theta_j \\mid y_j, \\alpha, \\beta) \\propto p(y_j \\mid \\theta_j, n_j) \\cdot p(\\theta_j \\mid \\alpha, \\beta)$\n\nThe likelihood function, from the Binomial model $y_j \\sim \\text{Binomial}(n_j, \\theta_j)$, is:\n$p(y_j \\mid \\theta_j, n_j) = \\binom{n_j}{y_j} \\theta_j^{y_j} (1 - \\theta_j)^{n_j - y_j}$\nFor deriving the posterior kernel, we can drop the constant binomial coefficient $\\binom{n_j}{y_j}$:\n$L(\\theta_j \\mid y_j) \\propto \\theta_j^{y_j} (1 - \\theta_j)^{n_j - y_j}$\n\nThe prior distribution for $\\theta_j$ is $\\text{Beta}(\\alpha, \\beta)$, with the PDF:\n$p(\\theta_j \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta_j^{\\alpha-1} (1 - \\theta_j)^{\\beta-1}$\nThe kernel of the prior is:\n$p(\\theta_j \\mid \\alpha, \\beta) \\propto \\theta_j^{\\alpha-1} (1 - \\theta_j)^{\\beta-1}$\n\nNow, we multiply the kernels of the likelihood and the prior:\n$p(\\theta_j \\mid y_j, \\alpha, \\beta) \\propto \\left( \\theta_j^{y_j} (1 - \\theta_j)^{n_j - y_j} \\right) \\cdot \\left( \\theta_j^{\\alpha-1} (1 - \\theta_j)^{\\beta-1} \\right)$\nCombining the terms with the same base:\n$p(\\theta_j \\mid y_j, \\alpha, \\beta) \\propto \\theta_j^{y_j + \\alpha - 1} (1 - \\theta_j)^{n_j - y_j + \\beta - 1}$\n\nThis resulting kernel is recognizable as the kernel of a Beta distribution. The posterior distribution of $\\theta_j$ is therefore:\n$\\theta_j \\mid y_j, \\alpha, \\beta \\sim \\text{Beta}(\\alpha', \\beta')$\nwith posterior parameters:\n$\\alpha' = y_j + \\alpha$\n$\\beta' = n_j - y_j + \\beta$\n\n**2. Posterior Mean $E[\\theta_j \\mid y_j, \\alpha, \\beta]$**\n\nThe expected value of a random variable following a Beta distribution $\\text{Beta}(a, b)$ is $\\frac{a}{a+b}$.\nUsing the posterior parameters $\\alpha'$ and $\\beta'$, the posterior mean of $\\theta_j$ is:\n$E[\\theta_j \\mid y_j, \\alpha, \\beta] = \\frac{\\alpha'}{\\alpha' + \\beta'} = \\frac{y_j + \\alpha}{(y_j + \\alpha) + (n_j - y_j + \\beta)}$\nSimplifying the denominator:\n$E[\\theta_j \\mid y_j, \\alpha, \\beta] = \\frac{y_j + \\alpha}{n_j + \\alpha + \\beta}$\n\n**3. Derivation of the Shrinkage Factor $\\kappa_j$**\n\nThe final step is to equate this derived expression for the posterior mean with the form given in the problem statement and solve for $\\kappa_j$. The problem states:\n$E[\\theta_j \\mid y_j, \\alpha, \\beta] = \\kappa_j \\mu_0 + (1-\\kappa_j) \\hat{p}_j$\n\nWe will rewrite our derived posterior mean to match this structure. The term $\\alpha + \\beta$ can be interpreted as a \"prior sample size\" or the strength of the prior information. The posterior mean is a weighted average of the empirical rate and the prior mean. Let's make this explicit.\n$E[\\theta_j \\mid y_j, \\alpha, \\beta] = \\frac{y_j + \\alpha}{n_j + \\alpha + \\beta}$\n\nWe can rewrite the numerator by relating $y_j$ to $\\hat{p}_j = \\frac{y_j}{n_j}$ and $\\alpha$ to $\\mu_0 = \\frac{\\alpha}{\\alpha+\\beta}$:\n$E[\\theta_j \\mid y_j, \\alpha, \\beta] = \\frac{n_j (\\frac{y_j}{n_j}) + \\alpha}{n_j + \\alpha + \\beta} = \\frac{n_j \\hat{p}_j + \\alpha}{n_j + \\alpha + \\beta}$\n\nTo cast this as a convex combination of $\\hat{p}_j$ and $\\mu_0$, we can rewrite the expression as follows:\n$E[\\theta_j \\mid y_j, \\alpha, \\beta] = \\left( \\frac{n_j}{n_j + \\alpha + \\beta} \\right) \\hat{p}_j + \\frac{\\alpha}{n_j + \\alpha + \\beta}$\nNow, we manipulate the second term to introduce $\\mu_0 = \\frac{\\alpha}{\\alpha+\\beta}$:\n$\\frac{\\alpha}{n_j + \\alpha + \\beta} = \\left( \\frac{\\alpha+\\beta}{n_j + \\alpha + \\beta} \\right) \\frac{\\alpha}{\\alpha+\\beta} = \\left( \\frac{\\alpha+\\beta}{n_j + \\alpha + \\beta} \\right) \\mu_0$\n\nSubstituting this back, we obtain the posterior mean expressed as a weighted average of $\\hat{p}_j$ and $\\mu_0$:\n$E[\\theta_j \\mid y_j, \\alpha, \\beta] = \\left( \\frac{n_j}{n_j + \\alpha + \\beta} \\right) \\hat{p}_j + \\left( \\frac{\\alpha + \\beta}{n_j + \\alpha + \\beta} \\right) \\mu_0$\n\nWe now compare this expression with the form given in the problem:\n$E[\\theta_j \\mid y_j, \\alpha, \\beta] = (1 - \\kappa_j) \\hat{p}_j + \\kappa_j \\mu_0$\n\nBy equating the coefficients of $\\mu_0$, we directly identify the shrinkage factor $\\kappa_j$:\n$\\kappa_j = \\frac{\\alpha + \\beta}{n_j + \\alpha + \\beta}$\n\nTo be thorough, we verify this is consistent with the coefficient of $\\hat{p}_j$:\n$1 - \\kappa_j = 1 - \\frac{\\alpha + \\beta}{n_j + \\alpha + \\beta} = \\frac{(n_j + \\alpha + \\beta) - (\\alpha + \\beta)}{n_j + \\alpha + \\beta} = \\frac{n_j}{n_j + \\alpha + \\beta}$\nThis matches the coefficient of $\\hat{p}_j$ we derived. Thus, the expression for $\\kappa_j$ is correct.\n\nThis factor $\\kappa_j$ represents the weight given to the prior mean $\\mu_0$. It is the ratio of the \"prior strength\" ($\\alpha + \\beta$) to the total effective sample size, which is the sum of the data sample size ($n_j$) and the prior strength. As $n_j$ increases, $\\kappa_j$ approaches $0$, and the estimate relies more heavily on the data $\\hat{p}_j$. Conversely, for small $n_j$, $\\kappa_j$ is larger, and the estimate is \"shrunk\" more strongly toward the pooled mean $\\mu_0$.", "answer": "$$\n\\boxed{\\frac{\\alpha + \\beta}{n_j + \\alpha + \\beta}}\n$$", "id": "4800178"}, {"introduction": "A primary goal of modeling is to make predictions for new observations. In a hierarchical context, the uncertainty of a prediction depends critically on whether the new observation belongs to one of the existing, observed groups or an entirely new one. This practice challenges you to formalize the posterior predictive distributions for these two distinct scenarios, using the law of total variance to dissect the sources of uncertainty and understand why predicting for a new, unseen group inherently carries greater uncertainty [@problem_id:4953491].", "problem": "Consider a continuous clinical outcome modeled with a Bayesian hierarchical random-intercept structure across hospitals. For hospital index $j \\in \\{1,\\dots,J\\}$ and patient index $i \\in \\{1,\\dots,n_j\\}$, the model is\n$$\ny_{ij} \\mid \\boldsymbol{\\beta}, b_j, \\sigma^2 \\sim \\mathcal{N}\\!\\big(x_{ij}^\\top \\boldsymbol{\\beta} + b_j,\\ \\sigma^2\\big),\n$$\nwith hospital-specific random intercepts\n$$\nb_j \\mid \\tau^2 \\sim \\mathcal{N}\\!\\big(0,\\ \\tau^2\\big),\n$$\nand independent priors\n$$\n\\boldsymbol{\\beta} \\sim \\mathcal{N}\\!\\big(\\boldsymbol{m}_0,\\ \\mathbf{V}_0\\big),\\quad \\sigma^2 \\sim \\text{Inverse-Gamma (IG)}\\!\\big(a_0,\\ b_0\\big),\\quad \\tau^2 \\sim \\text{Inverse-Gamma (IG)}\\!\\big(c_0,\\ d_0\\big).\n$$\nAssume data $\\mathcal{D}=\\{(y_{ij}, x_{ij}, j): j=1,\\dots,J;\\ i=1,\\dots,n_j\\}$ have been observed from $J$ existing hospitals. Let $x_\\star$ denote the covariate vector for a single new patient. Using the definition of posterior predictive distributions from Bayes’ rule and conditional independence in hierarchical models, select the option that correctly specifies:\n\n(i) The posterior predictive distribution $p(y_\\star \\mid x_\\star, \\mathcal{D}, j_\\star)$ for a new patient in an existing hospital $j_\\star \\in \\{1,\\dots,J\\}$; and\n\n(ii) The posterior predictive distribution $p(y_\\star \\mid x_\\star, \\mathcal{D}, \\text{new})$ for a new patient in a previously unseen hospital, together with a correct characterization of how the uncertainty (variance) differs across these two cases, using the law of total variance as the fundamental base.\n\nOptions:\n\nA. \n(i) \n$$\np(y_\\star \\mid x_\\star, \\mathcal{D}, j_\\star)\\;=\\;\\int\\!\\!\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta} + b_{j_\\star},\\ \\sigma^2\\big)\\,p(\\boldsymbol{\\beta}, b_{j_\\star}, \\sigma^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,db_{j_\\star}\\,d\\sigma^2.\n$$\n(ii) \n$$\np(y_\\star \\mid x_\\star, \\mathcal{D}, \\text{new})\\;=\\;\\int\\!\\!\\int\\!\\!\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta} + b_{\\text{new}},\\ \\sigma^2\\big)\\,\\mathcal{N}\\!\\big(b_{\\text{new}} \\mid 0,\\ \\tau^2\\big)\\,p(\\boldsymbol{\\beta}, \\sigma^2, \\tau^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,d\\sigma^2\\,d\\tau^2\\,db_{\\text{new}}.\n$$\nUnder the law of total variance,\n$$\n\\operatorname{Var}\\!\\big(y_\\star \\mid x_\\star, \\mathcal{D}, j_\\star\\big)\\;=\\;\\mathbb{E}\\!\\big[\\sigma^2 \\mid \\mathcal{D}\\big]\\;+\\;\\operatorname{Var}\\!\\big(x_\\star^\\top \\boldsymbol{\\beta} + b_{j_\\star} \\mid \\mathcal{D}\\big),\n$$\nwhereas\n$$\n\\operatorname{Var}\\!\\big(y_\\star \\mid x_\\star, \\mathcal{D}, \\text{new}\\big)\\;=\\;\\mathbb{E}\\!\\big[\\sigma^2 \\mid \\mathcal{D}\\big]\\;+\\;\\operatorname{Var}\\!\\big(x_\\star^\\top \\boldsymbol{\\beta} \\mid \\mathcal{D}\\big)\\;+\\;\\mathbb{E}\\!\\big[\\tau^2 \\mid \\mathcal{D}\\big].\n$$\nBecause $b_{j_\\star}$ is informed by hospital-$j_\\star$ data, $\\operatorname{Var}\\!\\big(b_{j_\\star} \\mid \\mathcal{D}\\big)$ is typically less than $\\mathbb{E}\\!\\big[\\tau^2 \\mid \\mathcal{D}\\big]$, so the new-hospital predictive variance is generally larger.\n\nB.\n(i)\n$$\np(y_\\star \\mid x_\\star, \\mathcal{D}, j_\\star)\\;=\\;\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta},\\ \\sigma^2\\big)\\,p(\\boldsymbol{\\beta}, \\sigma^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,d\\sigma^2,\n$$\nand\n(ii)\n$$\np(y_\\star \\mid x_\\star, \\mathcal{D}, \\text{new})\\;=\\;\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta},\\ \\sigma^2\\big)\\,p(\\boldsymbol{\\beta}, \\sigma^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,d\\sigma^2,\n$$\nwith the uncertainty being identical across the two cases because the random intercepts integrate out automatically.\n\nC.\n(i)\n$$\np(y_\\star \\mid x_\\star, \\mathcal{D}, j_\\star)\\;=\\;\\int\\!\\!\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta} + b_{j_\\star},\\ \\sigma^2\\big)\\,p(\\boldsymbol{\\beta}, b_{j_\\star}, \\sigma^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,db_{j_\\star}\\,d\\sigma^2,\n$$\nand\n(ii)\n$$\np(y_\\star \\mid x_\\star, \\mathcal{D}, \\text{new})\\;=\\;\\int\\!\\!\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta} + b_{\\text{new}},\\ \\sigma^2\\big)\\,p(\\boldsymbol{\\beta}, b_{\\text{new}}, \\sigma^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,db_{\\text{new}}\\,d\\sigma^2,\n$$\nwith uncertainty smaller for the new hospital because $b_{\\text{new}}$ has mean zero, so it does not contribute variability.\n\nD.\n(i)\n$$\np(y_\\star \\mid x_\\star, \\mathcal{D}, j_\\star)\\;=\\;\\int\\!\\!\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta} + b_{j_\\star},\\ \\sigma^2\\big)\\,p(\\boldsymbol{\\beta}, b_{j_\\star}, \\sigma^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,db_{j_\\star}\\,d\\sigma^2,\n$$\nand\n(ii)\n$$\np(y_\\star \\mid x_\\star, \\mathcal{D}, \\text{new})\\;=\\;\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta},\\ \\sigma^2\\big)\\,p(\\boldsymbol{\\beta}, \\sigma^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,d\\sigma^2,\n$$\nwith uncertainties equal provided that $\\mathbb{E}\\!\\big[\\tau^2 \\mid \\mathcal{D}\\big]=\\operatorname{Var}\\!\\big(b_{j_\\star} \\mid \\mathcal{D}\\big)$, which holds under exchangeability of hospitals.", "solution": "The fundamental base is Bayes’ rule and the definition of posterior predictive distributions in hierarchical models. For generic parameters $\\boldsymbol{\\theta}$ and data $\\mathcal{D}$, the posterior predictive distribution for a new observation $y_\\star$ is\n$$\np(y_\\star \\mid \\mathcal{D}) \\;=\\; \\int p(y_\\star \\mid \\boldsymbol{\\theta})\\,p(\\boldsymbol{\\theta} \\mid \\mathcal{D})\\,d\\boldsymbol{\\theta}.\n$$\nIn hierarchical models with latent random effects, the parameter vector splits into structural parameters and random effects, and the predictive distribution integrates over both the posterior of parameters informed by $\\mathcal{D}$ and any new random effect drawn from its conditional distribution.\n\nIn the given random-intercept model,\n$$\ny_{ij} \\mid \\boldsymbol{\\beta}, b_j, \\sigma^2 \\sim \\mathcal{N}\\!\\big(x_{ij}^\\top \\boldsymbol{\\beta} + b_j,\\ \\sigma^2\\big),\\quad b_j \\mid \\tau^2 \\sim \\mathcal{N}\\!\\big(0,\\ \\tau^2\\big),\n$$\nwith independent priors for $\\boldsymbol{\\beta}$, $\\sigma^2$, and $\\tau^2$. For an existing hospital $j_\\star$, the random intercept $b_{j_\\star}$ has a posterior distribution $p(b_{j_\\star} \\mid \\mathcal{D})$ informed by the data from hospital $j_\\star$; for a new hospital, the new random intercept $b_{\\text{new}}$ is not informed by $\\mathcal{D}$ except through $\\tau^2$, and is drawn from $\\mathcal{N}(0,\\tau^2)$ with $\\tau^2$ integrated against its posterior $p(\\tau^2 \\mid \\mathcal{D})$.\n\nTherefore, by direct application of the posterior predictive definition:\n\n- Existing hospital $j_\\star$:\n$$\np(y_\\star \\mid x_\\star, \\mathcal{D}, j_\\star)\\;=\\;\\int\\!\\!\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta} + b_{j_\\star},\\ \\sigma^2\\big)\\,p(\\boldsymbol{\\beta}, b_{j_\\star}, \\sigma^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,db_{j_\\star}\\,d\\sigma^2,\n$$\nwhich integrates over the hospital-specific posterior $p(b_{j_\\star} \\mid \\mathcal{D})$ as well as the posterior $p(\\boldsymbol{\\beta}, \\sigma^2 \\mid \\mathcal{D})$.\n\n- New hospital:\n$$\np(y_\\star \\mid x_\\star, \\mathcal{D}, \\text{new})\\;=\\;\\int\\!\\!\\int\\!\\!\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta} + b_{\\text{new}},\\ \\sigma^2\\big)\\,\\mathcal{N}\\!\\big(b_{\\text{new}} \\mid 0,\\ \\tau^2\\big)\\,p(\\boldsymbol{\\beta}, \\sigma^2, \\tau^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,d\\sigma^2\\,d\\tau^2\\,db_{\\text{new}}.\n$$\nHere, $b_{\\text{new}}$ is integrated against its conditional distribution given $\\tau^2$, and $\\tau^2$ itself is integrated against its posterior.\n\nTo highlight the difference in uncertainty, we invoke the law of total variance. For any random variable $Y$ and latent variables $\\boldsymbol{\\Theta}$,\n$$\n\\operatorname{Var}(Y) \\;=\\; \\mathbb{E}\\!\\big[\\operatorname{Var}(Y \\mid \\boldsymbol{\\Theta})\\big] \\;+\\; \\operatorname{Var}\\!\\big(\\mathbb{E}[Y \\mid \\boldsymbol{\\Theta}]\\big).\n$$\nApply this to the two predictive settings:\n\n- Existing hospital $j_\\star$: Conditional on $(\\boldsymbol{\\beta}, b_{j_\\star}, \\sigma^2)$, we have $y_\\star \\sim \\mathcal{N}(x_\\star^\\top \\boldsymbol{\\beta} + b_{j_\\star}, \\sigma^2)$, so $\\operatorname{Var}(y_\\star \\mid \\boldsymbol{\\beta}, b_{j_\\star}, \\sigma^2) = \\sigma^2$ and $\\mathbb{E}(y_\\star \\mid \\boldsymbol{\\beta}, b_{j_\\star}, \\sigma^2) = x_\\star^\\top \\boldsymbol{\\beta} + b_{j_\\star}$. Thus,\n$$\n\\operatorname{Var}\\!\\big(y_\\star \\mid x_\\star, \\mathcal{D}, j_\\star\\big)\\;=\\;\\mathbb{E}\\!\\big[\\sigma^2 \\mid \\mathcal{D}\\big]\\;+\\;\\operatorname{Var}\\!\\big(x_\\star^\\top \\boldsymbol{\\beta} + b_{j_\\star} \\mid \\mathcal{D}\\big).\n$$\nThe second term accounts for uncertainty in both $\\boldsymbol{\\beta}$ and $b_{j_\\star}$ under the posterior, including any covariance between them.\n\n- New hospital: Conditional on $(\\boldsymbol{\\beta}, \\sigma^2, \\tau^2)$ and $b_{\\text{new}}$, we again have $y_\\star \\sim \\mathcal{N}(x_\\star^\\top \\boldsymbol{\\beta} + b_{\\text{new}}, \\sigma^2)$. Integrating $b_{\\text{new}} \\sim \\mathcal{N}(0,\\tau^2)$ yields $\\mathbb{E}(y_\\star \\mid \\boldsymbol{\\beta}, \\sigma^2, \\tau^2) = x_\\star^\\top \\boldsymbol{\\beta}$ and\n$$\n\\operatorname{Var}\\!\\big(y_\\star \\mid \\boldsymbol{\\beta}, \\sigma^2, \\tau^2\\big)\\;=\\;\\sigma^2 + \\tau^2,\n$$\nbecause the sum of independent normal terms has additive variances. Applying total variance with $(\\boldsymbol{\\beta}, \\sigma^2, \\tau^2)$ as latent variables,\n$$\n\\operatorname{Var}\\!\\big(y_\\star \\mid x_\\star, \\mathcal{D}, \\text{new}\\big)\\;=\\;\\mathbb{E}\\!\\big[\\sigma^2 + \\tau^2 \\mid \\mathcal{D}\\big]\\;+\\;\\operatorname{Var}\\!\\big(x_\\star^\\top \\boldsymbol{\\beta} \\mid \\mathcal{D}\\big)\n\\;=\\;\\mathbb{E}\\!\\big[\\sigma^2 \\mid \\mathcal{D}\\big]\\;+\\;\\operatorname{Var}\\!\\big(x_\\star^\\top \\boldsymbol{\\beta} \\mid \\mathcal{D}\\big)\\;+\\;\\mathbb{E}\\!\\big[\\tau^2 \\mid \\mathcal{D}\\big].\n$$\nComparing the two, the new-hospital predictive variance contains the term $\\mathbb{E}[\\tau^2 \\mid \\mathcal{D}]$, whereas the existing-hospital variance contains $\\operatorname{Var}(b_{j_\\star} \\mid \\mathcal{D})$ as part of $\\operatorname{Var}(x_\\star^\\top \\boldsymbol{\\beta} + b_{j_\\star} \\mid \\mathcal{D})$. Because $b_{j_\\star}$ is informed by hospital-$j_\\star$ data, shrinkage reduces $\\operatorname{Var}(b_{j_\\star} \\mid \\mathcal{D})$ relative to the population-level dispersion $\\mathbb{E}[\\tau^2 \\mid \\mathcal{D}]$. Hence, all else equal, the new-hospital predictive variance is typically larger, reflecting greater uncertainty about the random intercept for an unseen hospital.\n\nOption-by-option analysis:\n\n- Option A: The existing-hospital predictive distribution correctly integrates over $(\\boldsymbol{\\beta}, b_{j_\\star}, \\sigma^2)$ under their posterior. The new-hospital predictive distribution correctly introduces $b_{\\text{new}} \\sim \\mathcal{N}(0,\\tau^2)$ and integrates over $(\\boldsymbol{\\beta}, \\sigma^2, \\tau^2)$ under their posterior. The variance characterizations follow directly from the law of total variance and the independence structure, yielding $\\mathbb{E}[\\sigma^2 \\mid \\mathcal{D}] + \\operatorname{Var}(x_\\star^\\top \\boldsymbol{\\beta} + b_{j_\\star} \\mid \\mathcal{D})$ for the existing hospital and $\\mathbb{E}[\\sigma^2 \\mid \\mathcal{D}] + \\operatorname{Var}(x_\\star^\\top \\boldsymbol{\\beta} \\mid \\mathcal{D}) + \\mathbb{E}[\\tau^2 \\mid \\mathcal{D}]$ for the new hospital. The explanation that the new-hospital predictive variance is larger is correct. Verdict: Correct.\n\n- Option B: Both predictive distributions drop the random intercept entirely and integrate only over $(\\boldsymbol{\\beta}, \\sigma^2)$, which is inconsistent with the hierarchical model. Further, it claims identical uncertainty because random intercepts “integrate out automatically,” which is false: the random intercept contributes additional variance $\\tau^2$ for a new hospital and posterior uncertainty in $b_{j_\\star}$ for an existing hospital. Verdict: Incorrect.\n\n- Option C: The existing-hospital expression is correct, but the new-hospital expression incorrectly uses $p(\\boldsymbol{\\beta}, b_{\\text{new}}, \\sigma^2 \\mid \\mathcal{D})$ as though $b_{\\text{new}}$ had a posterior informed by $\\mathcal{D}$. In truth, $b_{\\text{new}}$ is drawn from its conditional distribution given $\\tau^2$, and one must integrate over $\\tau^2$ via $p(\\tau^2 \\mid \\mathcal{D})$. The claim that uncertainty is smaller for the new hospital because $b_{\\text{new}}$ has mean zero is also incorrect: mean zero does not imply zero variance, and the variance contribution $\\tau^2$ increases predictive uncertainty. Verdict: Incorrect.\n\n- Option D: The existing-hospital expression is correct, but the new-hospital expression omits $b_{\\text{new}}$ entirely, treating the new hospital as if it had no random intercept. The statement that uncertainties are equal “provided” $\\mathbb{E}[\\tau^2 \\mid \\mathcal{D}] = \\operatorname{Var}(b_{j_\\star} \\mid \\mathcal{D})$ is misleading: even if these terms happened to match numerically, the omission of $b_{\\text{new}}$ in the predictive integral is conceptually wrong, and equality of uncertainties is not guaranteed because $\\operatorname{Var}(x_\\star^\\top \\boldsymbol{\\beta} + b_{j_\\star} \\mid \\mathcal{D})$ need not equal $\\operatorname{Var}(x_\\star^\\top \\boldsymbol{\\beta} \\mid \\mathcal{D})$. Verdict: Incorrect.", "answer": "$$\\boxed{A}$$", "id": "4953491"}]}