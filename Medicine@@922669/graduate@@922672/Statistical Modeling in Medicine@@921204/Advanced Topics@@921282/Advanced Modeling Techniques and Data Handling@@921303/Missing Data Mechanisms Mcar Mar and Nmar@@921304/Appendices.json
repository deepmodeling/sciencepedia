{"hands_on_practices": [{"introduction": "Understanding the consequences of missing data begins with the simplest case: Missing Completely At Random (MCAR). While analyzing only the complete cases is an unbiased approach under MCAR, it comes at the cost of statistical efficiency. This foundational exercise guides you through a first-principles derivation to precisely quantify this loss of efficiency, revealing the mathematical relationship between the fraction of missing data and the resulting increase in variance [@problem_id:4973778].", "problem": "In a longitudinal observational study of a continuous biomarker, let $\\{Y_{i}\\}_{i=1}^{n}$ be independent and identically distributed with mean $\\mu=\\mathbb{E}[Y]$ and variance $\\sigma^{2}=\\operatorname{Var}(Y)$, and let $\\{R_{i}\\}_{i=1}^{n}$ be independent and identically distributed missingness indicators with $R_{i}\\in\\{0,1\\}$ denoting whether $Y_{i}$ is observed. Define $\\pi=\\mathbb{E}[R]\\in(0,1)$ as the fraction observed. Assume the Missing Completely At Random (MCAR) mechanism holds, meaning $R_{i}$ is independent of $Y_{i}$ and any covariates; for context, Missing At Random (MAR) allows dependence on observed covariates but not on the missing values themselves, and Not Missing At Random (NMAR) allows dependence on the unobserved outcomes.\n\nConsider the complete-case estimator\n$$\n\\hat{\\mu}_{\\text{cc}}=\\frac{\\sum_{i=1}^{n}R_{i}Y_{i}}{\\sum_{i=1}^{n}R_{i}}.\n$$\nStarting from fundamental definitions and standard limit theorems, derive the large-sample variance of $\\hat{\\mu}_{\\text{cc}}$ under MCAR and then quantify the loss of efficiency relative to the full-data sample mean $\\bar{Y}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}$ by computing the variance inflation factor, defined as the ratio of the large-sample variance of $\\hat{\\mu}_{\\text{cc}}$ to that of $\\bar{Y}_{n}$. Express your final answer as a closed-form analytic expression in $\\pi$. No numerical approximation or rounding is required, and no physical units apply.", "solution": "The problem as stated is valid. It is a well-posed question in statistical theory, grounded in established principles of handling missing data. All terms are formally defined, and the premises are self-consistent and sufficient to derive a unique solution.\n\nThe objective is to derive the variance inflation factor of the complete-case estimator $\\hat{\\mu}_{\\text{cc}}$ relative to the full-data sample mean $\\bar{Y}_{n}$. This factor is defined as the ratio of their large-sample variances.\n\nFirst, we determine the large-sample variance of the full-data sample mean, $\\bar{Y}_{n}$.\nThe full-data estimator is defined as:\n$$\n\\bar{Y}_{n} = \\frac{1}{n} \\sum_{i=1}^{n} Y_{i}\n$$\nThe variables $\\{Y_i\\}_{i=1}^n$ are independent and identically distributed (i.i.d.) with variance $\\operatorname{Var}(Y) = \\sigma^2$. The variance of the sample mean is therefore:\n$$\n\\operatorname{Var}(\\bar{Y}_{n}) = \\operatorname{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\operatorname{Var}(Y_{i}) = \\frac{1}{n^2} (n \\sigma^2) = \\frac{\\sigma^2}{n}\n$$\nThis exact variance serves as the large-sample variance for $\\bar{Y}_{n}$.\n\nNext, we derive the large-sample variance of the complete-case estimator, $\\hat{\\mu}_{\\text{cc}}$.\nThe estimator is defined as the ratio:\n$$\n\\hat{\\mu}_{\\text{cc}} = \\frac{\\sum_{i=1}^{n} R_{i} Y_{i}}{\\sum_{i=1}^{n} R_{i}} = \\frac{\\frac{1}{n} \\sum_{i=1}^{n} R_{i} Y_{i}}{\\frac{1}{n} \\sum_{i=1}^{n} R_{i}}\n$$\nThis is a ratio of two sample means. We can find the asymptotic distribution of $\\hat{\\mu}_{\\text{cc}}$ using the multivariate Central Limit Theorem (CLT) and the Delta Method.\nLet us define a vector of random variables $W_i$. Since $\\{Y_i\\}_{i=1}^n$ and $\\{R_i\\}_{i=1}^n$ are i.i.d. sequences and are independent of each other (due to MCAR), the sequence of vectors $\\{W_i\\}_{i=1}^n$ is also i.i.d.\n$$W_i = \\begin{pmatrix} R_i Y_i \\\\ R_i \\end{pmatrix}$$\n\nWe first compute the expectation of $W_i$.\n$\\mathbb{E}[R_i] = \\pi$.\nUnder the Missing Completely At Random (MCAR) assumption, $R_i$ is independent of $Y_i$. Thus:\n$\\mathbb{E}[R_i Y_i] = \\mathbb{E}[R_i] \\mathbb{E}[Y_i] = \\pi \\mu$.\nThe expectation vector is:\n$$\n\\mathbb{E}[W_i] = \\begin{pmatrix} \\mathbb{E}[R_i Y_i] \\\\ \\mathbb{E}[R_i] \\end{pmatrix} = \\begin{pmatrix} \\pi \\mu \\\\ \\pi \\end{pmatrix}\n$$\nNext, we compute the covariance matrix of $W_i$, denoted $\\Sigma_W = \\operatorname{Cov}(W_i)$.\n$$\n\\Sigma_W = \\begin{pmatrix} \\operatorname{Var}(R_i Y_i) & \\operatorname{Cov}(R_i Y_i, R_i) \\\\ \\operatorname{Cov}(R_i Y_i, R_i) & \\operatorname{Var}(R_i) \\end{pmatrix}\n$$\nThe components are calculated as follows:\n1.  $\\operatorname{Var}(R_i)$: Since $R_i$ is a Bernoulli random variable with parameter $\\pi$, its variance is $\\operatorname{Var}(R_i) = \\pi(1-\\pi)$.\n2.  $\\operatorname{Var}(R_i Y_i) = \\mathbb{E}[(R_i Y_i)^2] - (\\mathbb{E}[R_i Y_i])^2$.\n    Since $R_i \\in \\{0, 1\\}$, we have $R_i^2 = R_i$.\n    $\\mathbb{E}[(R_i Y_i)^2] = \\mathbb{E}[R_i^2 Y_i^2] = \\mathbb{E}[R_i Y_i^2]$. By MCAR, this is $\\mathbb{E}[R_i]\\mathbb{E}[Y_i^2]$.\n    We know $\\mathbb{E}[Y_i^2] = \\operatorname{Var}(Y_i) + (\\mathbb{E}[Y_i])^2 = \\sigma^2 + \\mu^2$.\n    So, $\\mathbb{E}[R_i Y_i^2] = \\pi (\\sigma^2 + \\mu^2)$.\n    Therefore, $\\operatorname{Var}(R_i Y_i) = \\pi(\\sigma^2 + \\mu^2) - (\\pi \\mu)^2 = \\pi \\sigma^2 + \\pi \\mu^2 - \\pi^2 \\mu^2 = \\pi \\sigma^2 + \\mu^2 \\pi(1-\\pi)$.\n3.  $\\operatorname{Cov}(R_i Y_i, R_i) = \\mathbb{E}[(R_i Y_i)R_i] - \\mathbb{E}[R_i Y_i]\\mathbb{E}[R_i]$.\n    $\\mathbb{E}[(R_i Y_i)R_i] = \\mathbb{E}[R_i^2 Y_i] = \\mathbb{E}[R_i Y_i] = \\pi \\mu$.\n    So, $\\operatorname{Cov}(R_i Y_i, R_i) = \\pi \\mu - (\\pi \\mu)\\pi = \\pi \\mu (1-\\pi)$.\n\nThe covariance matrix is:\n$$\n\\Sigma_W = \\begin{pmatrix} \\pi \\sigma^2 + \\mu^2 \\pi(1-\\pi) & \\mu \\pi(1-\\pi) \\\\ \\mu \\pi(1-\\pi) & \\pi(1-\\pi) \\end{pmatrix}\n$$\nThe estimator $\\hat{\\mu}_{\\text{cc}}$ is a function $g$ of the sample mean of $W_i$, where $g(a,b) = a/b$. Let $\\bar{W}_n = \\frac{1}{n} \\sum_{i=1}^n W_i$. By the multivariate CLT, $\\sqrt{n}(\\bar{W}_n - \\mathbb{E}[W_i])$ converges in distribution to a multivariate normal distribution with mean $0$ and covariance matrix $\\Sigma_W$.\nTo find the asymptotic variance of $\\hat{\\mu}_{\\text{cc}} = g(\\bar{W}_n)$, we use the Delta Method. The asymptotic variance of $\\sqrt{n}(\\hat{\\mu}_{\\text{cc}} - \\mu)$ is given by $\\nabla g^T \\Sigma_W \\nabla g$, where the gradient $\\nabla g$ is evaluated at $\\mathbb{E}[W_i]$.\nThe gradient of $g(a,b)$ is:\n$$\\nabla g(a,b) = \\begin{pmatrix} 1/b \\\\ -a/b^2 \\end{pmatrix}$$\nEvaluating at $\\mathbb{E}[W_i] = (\\pi\\mu, \\pi)^T$:\n$$\n\\nabla g(\\pi\\mu, \\pi) = \\begin{pmatrix} 1/\\pi \\\\ -\\pi\\mu/\\pi^2 \\end{pmatrix} = \\begin{pmatrix} 1/\\pi \\\\ -\\mu/\\pi \\end{pmatrix}\n$$\nNow we compute the quadratic form $\\nabla g^T \\Sigma_W \\nabla g$:\n$$\n\\begin{pmatrix} \\frac{1}{\\pi} & -\\frac{\\mu}{\\pi} \\end{pmatrix} \\begin{pmatrix} \\pi \\sigma^2 + \\mu^2 \\pi(1-\\pi) & \\mu \\pi(1-\\pi) \\\\ \\mu \\pi(1-\\pi) & \\pi(1-\\pi) \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\pi} \\\\ -\\frac{\\mu}{\\pi} \\end{pmatrix}\n$$\nFirst, multiply the row vector by the matrix:\n$$\n\\begin{pmatrix} \\frac{1}{\\pi}(\\pi \\sigma^2 + \\mu^2 \\pi(1-\\pi)) - \\frac{\\mu}{\\pi}(\\mu \\pi(1-\\pi)) & \\frac{1}{\\pi}(\\mu \\pi(1-\\pi)) - \\frac{\\mu}{\\pi}(\\pi(1-\\pi)) \\end{pmatrix}\n$$\nThe first component simplifies to:\n$$\n(\\sigma^2 + \\mu^2(1-\\pi)) - \\mu^2(1-\\pi) = \\sigma^2\n$$\nThe second component simplifies to:\n$$\n\\mu(1-\\pi) - \\mu(1-\\pi) = 0\n$$\nSo the intermediate result is the row vector $\\begin{pmatrix} \\sigma^2 & 0 \\end{pmatrix}$.\nFinally, we multiply this by the column vector:\n$$\n\\begin{pmatrix} \\sigma^2 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\pi} \\\\ -\\frac{\\mu}{\\pi} \\end{pmatrix} = \\sigma^2 \\cdot \\frac{1}{\\pi} + 0 \\cdot \\left(-\\frac{\\mu}{\\pi}\\right) = \\frac{\\sigma^2}{\\pi}\n$$\nThis is the variance of the limiting distribution of $\\sqrt{n}(\\hat{\\mu}_{\\text{cc}} - \\mu)$. The large-sample variance of the estimator $\\hat{\\mu}_{\\text{cc}}$ is this quantity divided by $n$:\n$$\n\\text{Large-Sample Var}(\\hat{\\mu}_{\\text{cc}}) = \\frac{\\sigma^2}{n\\pi}\n$$\nFinally, we compute the variance inflation factor (VIF), which is the ratio of the large-sample variance of $\\hat{\\mu}_{\\text{cc}}$ to that of $\\bar{Y}_{n}$:\n$$\n\\text{VIF} = \\frac{\\text{Large-Sample Var}(\\hat{\\mu}_{\\text{cc}})}{\\text{Large-Sample Var}(\\bar{Y}_{n})} = \\frac{\\frac{\\sigma^2}{n\\pi}}{\\frac{\\sigma^2}{n}}\n$$\nThe terms $\\sigma^2$ and $n$ cancel out, yielding:\n$$\n\\text{VIF} = \\frac{1}{\\pi}\n$$\nThis result is intuitive: the variance of the complete-case estimator is inflated by a factor equal to the reciprocal of the observation probability. If all data are observed ($\\pi=1$), the inflation factor is $1$, as expected. If only half the data are observed ($\\pi=0.5$), the variance is doubled.", "answer": "$$\n\\boxed{\\frac{1}{\\pi}}\n$$", "id": "4973778"}, {"introduction": "When data are Missing At Random (MAR), sophisticated likelihood-based methods can provide valid inferences without discarding incomplete records. The Expectation-Maximization (EM) algorithm is a cornerstone of this approach, but its iterative nature can feel abstract. This hands-on practice demystifies the process by tasking you with performing one full iteration for a bivariate normal dataset, from computing conditional expectations in the E-step to updating parameters in the M-step [@problem_id:4973852].", "problem": "A clinical study measures a pair of continuous outcomes for each patient: a blood biomarker $X$ and a physiological index $Y$. The joint distribution of $(X,Y)$ is assumed to be bivariate normal with mean vector $\\mu = (\\mu_{x}, \\mu_{y})^{\\top}$ and covariance matrix $\\Sigma = \\begin{pmatrix} \\sigma_{xx} & \\sigma_{xy} \\\\ \\sigma_{xy} & \\sigma_{yy} \\end{pmatrix}$. For some patients, $X$ is missing, and the missingness mechanism is Missing At Random (MAR), meaning that the probability that $X$ is missing may depend on $Y$ but not on $X$ after conditioning on $Y$. Under MAR with parameter distinctness, the missingness mechanism is ignorable for likelihood-based inference.\n\nYou are given $N=4$ independent observations with $Y$ fully observed and $X$ possibly missing: $(x_{1}, y_{1}) = (-1, 0)$, $(x_{2} \\text{ missing}, y_{2} = 2)$, $(x_{3}, y_{3}) = (1,4)$, $(x_{4} \\text{ missing}, y_{4} = 6)$. Suppose the current parameter iterate for the Expectation-Maximization (EM) algorithm is\n$$\n\\mu^{(0)} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\qquad \n\\Sigma^{(0)} = \\begin{pmatrix} 9 & 2 \\\\ 2 & 4 \\end{pmatrix}.\n$$\n\nStarting from first principles appropriate for this context, namely: (i) the definition of the Missing At Random (MAR) mechanism and ignorability under parameter distinctness, (ii) the joint and conditional distribution properties of the multivariate normal family, and (iii) the definition of the Expectation-Maximization (EM) algorithm through maximization of the expected complete-data log-likelihood, perform one full EM iteration. In the E-step, compute the conditional expectations of the sufficient statistics for the complete-data log-likelihood given the observed data and the current parameters $\\mu^{(0)}$ and $\\Sigma^{(0)}$. In the M-step, maximize the expected complete-data log-likelihood to obtain the updated mean vector $\\mu^{(1)}$ and covariance matrix $\\Sigma^{(1)}$ under the maximum likelihood parameterization, where covariance uses the factor $1/N$ (not the unbiased factor).\n\nProvide the explicit numerical values for $\\mu_{x}^{(1)}$, $\\mu_{y}^{(1)}$, $\\sigma_{xx}^{(1)}$, $\\sigma_{xy}^{(1)}$, and $\\sigma_{yy}^{(1)}$. Do not round. Express your final answer as a single row matrix in the order $\\mu_{x}^{(1)}$, $\\mu_{y}^{(1)}$, $\\sigma_{xx}^{(1)}$, $\\sigma_{xy}^{(1)}$, $\\sigma_{yy}^{(1)}$.", "solution": "The problem is valid as it is scientifically grounded in statistical theory, well-posed, objective, and contains all necessary information to perform the requested calculation.\n\nThe Expectation-Maximization (EM) algorithm is an iterative method for finding maximum likelihood estimates of parameters in statistical models, where the model depends on unobserved latent variables. In this case, the unobserved variables are the missing values of the blood biomarker $X$. The algorithm alternates between two steps: the Expectation (E) step and the Maximization (M) step.\n\nLet the complete data be $Z = (X, Y)$, where $Z_i = (X_i, Y_i)^\\top$ for $i=1, \\dots, N$. The data is assumed to be drawn from a bivariate normal distribution $\\mathcal{N}(\\mu, \\Sigma)$. We are given $N=4$ observations. The observed data consists of $Z_{obs} = \\{(x_1, y_1), y_2, (x_3, y_3), y_4\\}$ and the missing data is $Z_{mis} = \\{x_2, x_4\\}$. The initial parameter estimates at iteration $t=0$ are:\n$$\n\\mu^{(0)} = \\begin{pmatrix} \\mu_{x}^{(0)} \\\\ \\mu_{y}^{(0)} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\qquad \n\\Sigma^{(0)} = \\begin{pmatrix} \\sigma_{xx}^{(0)} & \\sigma_{xy}^{(0)} \\\\ \\sigma_{xy}^{(0)} & \\sigma_{yy}^{(0)} \\end{pmatrix} = \\begin{pmatrix} 9 & 2 \\\\ 2 & 4 \\end{pmatrix}.\n$$\n\nThe complete-data log-likelihood for $N$ observations from a bivariate normal distribution is, up to a constant, proportional to:\n$$\nl(\\mu, \\Sigma; Z) \\propto - \\frac{N}{2} \\ln \\det(\\Sigma) - \\frac{1}{2} \\sum_{i=1}^{N} (Z_i - \\mu)^\\top \\Sigma^{-1} (Z_i - \\mu)\n$$\nThis likelihood is a function of the sufficient statistics of the normal distribution, which are $\\sum_{i=1}^N Z_i$ and $\\sum_{i=1}^N Z_i Z_i^\\top$. In component form, the sufficient statistics are $\\sum X_i$, $\\sum Y_i$, $\\sum X_i^2$, $\\sum Y_i^2$, and $\\sum X_i Y_i$.\n\n**E-Step: Compute conditional expectations of sufficient statistics**\n\nIn the E-step, we compute the expectation of the complete-data log-likelihood given the observed data $Z_{obs}$ and the current parameter estimates $\\theta^{(0)} = (\\mu^{(0)}, \\Sigma^{(0)})$. This is equivalent to computing the conditional expectations of the sufficient statistics.\nFor observations where both $X_i$ and $Y_i$ are observed (i.e., $i=1, 3$), the statistics are just their observed values.\nFor observations where $X_i$ is missing (i.e., $i=2, 4$), we need to compute the conditional expectation of $X_i$ and its functions, given $Y_i=y_i$ and $\\theta^{(0)}$.\n\nThe conditional distribution of $X$ given $Y=y$ for a bivariate normal is a univariate normal distribution, $X | Y=y \\sim \\mathcal{N}(\\mu_{x|y}, \\sigma^2_{x|y})$, with mean and variance:\n$$\n\\mu_{x|y} = \\mu_x + \\frac{\\sigma_{xy}}{\\sigma_{yy}}(y - \\mu_y)\n$$\n$$\n\\sigma^2_{x|y} = \\sigma_{xx} - \\frac{\\sigma_{xy}^2}{\\sigma_{yy}}\n$$\nUsing the current parameters $\\theta^{(0)}$:\n$$\n\\sigma^{2, (0)}_{x|y} = \\sigma_{xx}^{(0)} - \\frac{(\\sigma_{xy}^{(0)})^2}{\\sigma_{yy}^{(0)}} = 9 - \\frac{2^2}{4} = 9 - 1 = 8\n$$\nThe conditional mean for an observation $i$ is:\n$$\n\\mu_{x|y_i}^{(0)} = \\mu_x^{(0)} + \\frac{\\sigma_{xy}^{(0)}}{\\sigma_{yy}^{(0)}}(y_i - \\mu_y^{(0)}) = 1 + \\frac{2}{4}(y_i - 2) = 1 + 0.5(y_i - 2)\n$$\nWe need the conditional expectations $E[X_i | y_i, \\theta^{(0)}]$ and $E[X_i^2 | y_i, \\theta^{(0)}]$ for $i=2, 4$. For a normal variable $W \\sim \\mathcal{N}(\\mu_W, \\sigma_W^2)$, we have $E[W] = \\mu_W$ and $E[W^2] = \\text{Var}(W) + (E[W])^2 = \\sigma_W^2 + \\mu_W^2$.\n\nFor observation $i=2$, with $y_2=2$:\n$$\nE[X_2 | y_2=2, \\theta^{(0)}] = 1 + 0.5(2-2) = 1\n$$\n$$\nE[X_2^2 | y_2=2, \\theta^{(0)}] = \\sigma^{2, (0)}_{x|y} + (E[X_2 | y_2=2, \\theta^{(0)}])^2 = 8 + 1^2 = 9\n$$\n$$\nE[X_2 Y_2 | y_2=2, \\theta^{(0)}] = y_2 E[X_2 | y_2=2, \\theta^{(0)}] = 2 \\times 1 = 2\n$$\n\nFor observation $i=4$, with $y_4=6$:\n$$\nE[X_4 | y_4=6, \\theta^{(0)}] = 1 + 0.5(6-2) = 1 + 0.5(4) = 3\n$$\n$$\nE[X_4^2 | y_4=6, \\theta^{(0)}] = \\sigma^{2, (0)}_{x|y} + (E[X_4 | y_4=6, \\theta^{(0)}])^2 = 8 + 3^2 = 8 + 9 = 17\n$$\n$$\nE[X_4 Y_4 | y_4=6, \\theta^{(0)}] = y_4 E[X_4 | y_4=6, \\theta^{(0)}] = 6 \\times 3 = 18\n$$\n\nNow we compute the total expected sufficient statistics over all $N=4$ observations. We denote $E^{(0)}[\\cdot]$ as $E[\\cdot | Z_{obs}, \\theta^{(0)}]$.\n$$\nE^{(0)}\\left[\\sum_{i=1}^4 X_i\\right] = x_1 + E[X_2 | \\dots] + x_3 + E[X_4 | \\dots] = -1 + 1 + 1 + 3 = 4\n$$\n$$\nE^{(0)}\\left[\\sum_{i=1}^4 Y_i\\right] = \\sum_{i=1}^4 y_i = 0 + 2 + 4 + 6 = 12 \\quad (\\text{since } Y \\text{ is fully observed})\n$$\n$$\nE^{(0)}\\left[\\sum_{i=1}^4 X_i^2\\right] = x_1^2 + E[X_2^2 | \\dots] + x_3^2 + E[X_4^2 | \\dots] = (-1)^2 + 9 + 1^2 + 17 = 1 + 9 + 1 + 17 = 28\n$$\n$$\nE^{(0)}\\left[\\sum_{i=1}^4 Y_i^2\\right] = \\sum_{i=1}^4 y_i^2 = 0^2 + 2^2 + 4^2 + 6^2 = 0 + 4 + 16 + 36 = 56\n$$\n$$\nE^{(0)}\\left[\\sum_{i=1}^4 X_i Y_i\\right] = x_1 y_1 + E[X_2 Y_2 | \\dots] + x_3 y_3 + E[X_4 Y_4 | \\dots] = (-1)(0) + 2 + (1)(4) + 18 = 0 + 2 + 4 + 18 = 24\n$$\n\n**M-Step: Maximize the expected log-likelihood**\n\nIn the M-step, we find the parameters $\\theta^{(1)}=(\\mu^{(1)}, \\Sigma^{(1)})$ that maximize the expected complete-data log-likelihood. This is done by using the standard maximum likelihood estimators for a bivariate normal distribution, with the sufficient statistics replaced by their conditional expectations computed in the E-step.\n\nThe updated mean vector $\\mu^{(1)}$ is:\n$$\n\\mu_x^{(1)} = \\frac{1}{N} E^{(0)}\\left[\\sum_{i=1}^4 X_i\\right] = \\frac{4}{4} = 1\n$$\n$$\n\\mu_y^{(1)} = \\frac{1}{N} E^{(0)}\\left[\\sum_{i=1}^4 Y_i\\right] = \\frac{12}{4} = 3\n$$\nSo, $\\mu^{(1)} = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix}$.\n\nThe updated covariance matrix $\\Sigma^{(1)}$ (using the $1/N$ factor for MLE) is:\n$$\n\\Sigma^{(1)} = \\frac{1}{N} E^{(0)}\\left[\\sum_{i=1}^N (Z_i - \\mu^{(1)})(Z_i - \\mu^{(1)})^\\top\\right] = \\frac{1}{N} E^{(0)}\\left[\\sum_{i=1}^N Z_i Z_i^\\top\\right] - \\mu^{(1)}(\\mu^{(1)})^\\top\n$$\nFirst, we compute the matrix of expected second moments:\n$$\n\\frac{1}{N} E^{(0)}\\left[\\sum_{i=1}^N Z_i Z_i^\\top\\right] = \\frac{1}{4} \\begin{pmatrix} E^{(0)}[\\sum X_i^2] & E^{(0)}[\\sum X_i Y_i] \\\\ E^{(0)}[\\sum X_i Y_i] & E^{(0)}[\\sum Y_i^2] \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 28 & 24 \\\\ 24 & 56 \\end{pmatrix} = \\begin{pmatrix} 7 & 6 \\\\ 6 & 14 \\end{pmatrix}\n$$\nNext, we compute the outer product of the new mean:\n$$\n\\mu^{(1)}(\\mu^{(1)})^\\top = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} \\begin{pmatrix} 1 & 3 \\end{pmatrix} = \\begin{pmatrix} 1 & 3 \\\\ 3 & 9 \\end{pmatrix}\n$$\nFinally, we compute $\\Sigma^{(1)}$:\n$$\n\\Sigma^{(1)} = \\begin{pmatrix} 7 & 6 \\\\ 6 & 14 \\end{pmatrix} - \\begin{pmatrix} 1 & 3 \\\\ 3 & 9 \\end{pmatrix} = \\begin{pmatrix} 7-1 & 6-3 \\\\ 6-3 & 14-9 \\end{pmatrix} = \\begin{pmatrix} 6 & 3 \\\\ 3 & 5 \\end{pmatrix}\n$$\nThe elements of the updated covariance matrix are $\\sigma_{xx}^{(1)} = 6$, $\\sigma_{xy}^{(1)} = 3$, and $\\sigma_{yy}^{(1)} = 5$.\n\nThe results of one full EM iteration are the updated parameters:\n$\\mu_{x}^{(1)} = 1$\n$\\mu_{y}^{(1)} = 3$\n$\\sigma_{xx}^{(1)} = 6$\n$\\sigma_{xy}^{(1)} = 3$\n$\\sigma_{yy}^{(1)} = 5$\nThese are the required numerical values.", "answer": "$$\n\\boxed{\n\\begin{pmatrix} 1 & 3 & 6 & 3 & 5 \\end{pmatrix}\n}\n$$", "id": "4973852"}, {"introduction": "Multiple Imputation (MI) is a powerful and widely used technique for handling missing data, but its successful application hinges on the subtle concept of \"congeniality.\" This principle requires that the model used to create imputations is compatible with the final analysis model that addresses the scientific question. This exercise explores a common pitfall where violating congeniality—by omitting a key interaction term from the imputation model—introduces bias, highlighting the importance of careful model specification in MI workflows [@problem_id:4973789].", "problem": "A randomized comparative effectiveness study in oncology collects a continuous biomarker $X$, a randomized treatment indicator $Z \\in \\{0,1\\}$, and a continuous progression score $Y$ at $6$ months. Some $Y$ values are missing. Let $R$ be the missingness indicator for $Y$, with $R=1$ if $Y$ is observed and $R=0$ if $Y$ is missing. Suppose the missingness mechanism satisfies Missing At Random (MAR): $\\mathbb{P}(R=1 \\mid Y,X,Z)=\\mathbb{P}(R=1 \\mid X)$, and the scientific estimand is the degree of treatment effect modification by the biomarker, operationalized as the coefficient of the $XZ$ interaction in a linear model for $Y$.\n\nAn analyst plans to use Multiple Imputation (MI) with Rubin’s rules for pooling to handle missing $Y$, and then fit an analysis model that includes $1$, $X$, $Z$, and $XZ$ as regressors. The imputer considers several candidate imputation models for $Y \\mid X,Z$.\n\nWhich option correctly explains the notion of congeniality between the imputation and analysis models and gives a scientifically sound example in which violating congeniality leads to biased pooled estimates of the $XZ$ interaction under MAR?\n\nA. Congeniality requires that the imputation model be compatible with, and at least as rich as, the analysis model for the joint distribution relevant to the estimand, including the same variables and their functional forms (for example, interactions and nonlinear transformations) so that the target estimand is a functional of the imputation model’s posterior predictive distribution. For example, if the true data-generating model is $Y=\\beta_{0}+\\beta_{1}X+\\beta_{2}Z+\\beta_{3}XZ+\\varepsilon$ with $\\mathbb{E}[\\varepsilon \\mid X,Z]=0$ and $R \\perp Y \\mid X$ (MAR), but the imputation model for missing $Y$ omits $XZ$ and uses a linear additive mean $\\mathbb{E}[Y \\mid X,Z]=\\alpha_{0}+\\alpha_{1}X+\\alpha_{2}Z$, then completed datasets will systematically underrepresent the interaction signal in the subset with $R=0$. As a result, the pooled MI estimate for the $XZ$ coefficient is biased toward $0$ even as the number of imputations $m \\to \\infty$, because the imputation model is uncongenial to the analysis model.\n\nB. Under MAR, MI delivers unbiased pooled point estimates for any analysis model as long as the imputation model includes the same set of variables as predictors, regardless of whether it includes the same transformations (for example, interactions or nonlinear terms). Therefore, omitting $XZ$ from the imputation model when the analysis model includes it cannot bias the MI estimate of the $XZ$ coefficient.\n\nC. Congeniality concerns only variance estimation; as long as MAR holds, an imputation model that omits interaction or nonlinear terms used in the analysis model will produce correct pooled point estimates, with at worst conservative standard errors due to model mismatch.\n\nD. For a count outcome analyzed with a log-linear Poisson regression including an $XZ$ interaction, imputing missing counts from a homoscedastic normal linear regression on $X$ and $Z$ (excluding $XZ$) is congenial under MAR because both models are linear in parameters; therefore, the pooled estimate of the interaction is unbiased as $m \\to \\infty$.\n\nSelect the single best answer.", "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n- A randomized comparative effectiveness study in oncology.\n- Variables collected:\n    - $X$: a continuous biomarker.\n    - $Z$: a randomized treatment indicator, $Z \\in \\{0,1\\}$.\n    - $Y$: a continuous progression score at $6$ months. Some values of $Y$ are missing.\n- Missingness indicator for $Y$: $R$, where $R=1$ if $Y$ is observed and $R=0$ if $Y$ is missing.\n- Assumed missingness mechanism: Missing At Random (MAR), specified as $\\mathbb{P}(R=1 \\mid Y,X,Z)=\\mathbb{P}(R=1 \\mid X)$.\n- Scientific estimand: The degree of treatment effect modification by the biomarker.\n- Operationalization of the estimand: The coefficient of the $XZ$ interaction term in a linear model for $Y$.\n- Proposed analysis method:\n    1. Multiple Imputation (MI) with Rubin’s rules for pooling to handle missing $Y$.\n    2. Fit an analysis model (substantive model) with regressors $1$, $X$, $Z$, and $XZ$.\n- Imputation model: Candidate models are considered for the conditional distribution of $Y$ given $X$ and $Z$, i.e., $Y \\mid X,Z$.\n- Question: Identify the option that correctly explains the concept of congeniality between imputation and analysis models and provides a scientifically sound example of bias resulting from a violation of congeniality under the specified MAR mechanism.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is set within the standard framework of statistical inference for clinical trials with missing data. The concepts of MAR, multiple imputation, linear regression, and interaction effects are fundamental and well-established in biostatistics and related fields. The specified MAR mechanism, $\\mathbb{P}(R=1 \\mid Y,X,Z)=\\mathbb{P}(R=1 \\mid X)$, is a valid, though specific, instance of MAR.\n- **Well-Posed:** The problem asks for the correct conceptual explanation and a valid example concerning \"congeniality\" in multiple imputation. The question is structured to have a unique correct answer among the choices, based on established statistical theory.\n- **Objective:** The problem is stated using precise, objective statistical terminology. There is no ambiguity or subjectivity.\n- **Completeness and Consistency:** The problem provides sufficient information to evaluate the options. It defines all variables, the estimand, the missing data mechanism, and the analytical approach. There are no internal contradictions.\n- **Feasibility:** The scenario described is a realistic and common situation in medical research.\n\nThe problem statement passes all validation criteria. It is a valid, well-posed, and scientifically grounded problem in applied statistics.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution will proceed by analyzing the principles of multiple imputation and congeniality, followed by an evaluation of each option.\n\n### Derivation and Analysis\n\nThe core of this problem lies in understanding the concept of \"congeniality\" (or compatibility) in the context of multiple imputation (MI), as first formally described by Meng (1994). MI is a two-stage process:\n1.  **Imputation Stage:** Missing values are filled in $m > 1$ times, creating $m$ complete datasets. These imputations are draws from the posterior predictive distribution of the missing data given the observed data, under a specified imputation model.\n2.  **Analysis Stage:** The desired analysis (e.g., fitting a regression model) is performed on each of the $m$ complete datasets.\n3.  **Pooling Stage:** The results from the $m$ analyses are combined using Rubin's rules to obtain a single point estimate and a variance estimate that accounts for both within-imputation and between-imputation variability.\n\nFor the pooled estimates to have good statistical properties (e.g., consistency for the target estimand), the imputation model and the analysis model must be \"congenial.\" A primary condition for congeniality is that the imputation model must be at least as complex or general as the analysis model. This means that any variables, interaction terms, or non-linear transformations of variables that are part of the scientific question (i.e., included in the analysis model) must also be included in the imputation model.\n\nIf the models are uncongenial, bias can be introduced. Let's consider the specific scenario.\nThe **analysis model** is a linear model for $Y$:\n$$\n\\mathbb{E}[Y \\mid X,Z] = \\beta_{0} + \\beta_{1}X + \\beta_{2}Z + \\beta_{3}XZ\n$$\nThe **estimand** of interest is $\\beta_{3}$, the coefficient of the $XZ$ interaction.\n\nSuppose an **uncongenial imputation model** is used, one that omits the interaction term:\n$$\n\\mathbb{E}[Y \\mid X,Z] = \\alpha_{0} + \\alpha_{1}X + \\alpha_{2}Z\n$$\nThis imputation model implicitly assumes that the true interaction effect is zero ($\\beta_{3}=0$). When this model is used to generate imputed values for the missing $Y$s (i.e., for subjects with $R=0$), the imputed values $Y_{imp}$ will be drawn from a distribution whose mean structure is additive in $X$ and $Z$. Consequently, within the imputed data points, there is no evidence of an $XZ$ interaction.\n\nEach of the $m$ completed datasets consists of two parts:\n1.  The observed data $(Y_{obs}, X, Z)$ for subjects with $R=1$.\n2.  The imputed data $(Y_{imp}, X, Z)$ for subjects with $R=0$.\n\nWhen the analyst fits the analysis model (including the $XZ$ term) to a completed dataset, the resulting estimate $\\hat{\\beta_3}$ will be a blend of the interaction effect present in the observed data and the zero-interaction effect built into the imputed data. The estimate will be biased towards the assumption of the imputation model, i.e., biased towards $0$. This bias is not a consequence of finite sample size or a finite number of imputations ($m$); it is a systematic modeling error. It persists even as the sample size and $m$ approach infinity. The MAR assumption, $\\mathbb{P}(R=1 \\mid Y,X,Z)=\\mathbb{P}(R=1 \\mid X)$, makes imputation possible but does not protect against this model misspecification bias.\n\n### Option-by-Option Evaluation\n\n**A. Congeniality requires that the imputation model be compatible with, and at least as rich as, the analysis model for the joint distribution relevant to the estimand, including the same variables and their functional forms (for example, interactions and nonlinear transformations) so that the target estimand is a functional of the imputation model’s posterior predictive distribution. For example, if the true data-generating model is $Y=\\beta_{0}+\\beta_{1}X+\\beta_{2}Z+\\beta_{3}XZ+\\varepsilon$ with $\\mathbb{E}[\\varepsilon \\mid X,Z]=0$ and $R \\perp Y \\mid X$ (MAR), but the imputation model for missing $Y$ omits $XZ$ and uses a linear additive mean $\\mathbb{E}[Y \\mid X,Z]=\\alpha_{0}+\\alpha_{1}X+\\alpha_{2}Z$, then completed datasets will systematically underrepresent the interaction signal in the subset with $R=0$. As a result, the pooled MI estimate for the $XZ$ coefficient is biased toward $0$ even as the number of imputations $m \\to \\infty$, because the imputation model is uncongenial to the analysis model.**\n\nThis option provides a precise and correct definition of congeniality. The example perfectly illustrates the consequence of violating it: using an imputation model that is simpler (omitting the $XZ$ term) than the analysis model. The explanation that the imputed values will lack the interaction signal, leading to a pooled estimate biased towards $0$, is exactly correct. The statement that this bias persists for $m \\to \\infty$ is also correct. The notation $R \\perp Y \\mid X$ is a valid statement of the given MAR assumption $\\mathbb{P}(R=1 \\mid Y,X,Z)=\\mathbb{P}(R=1 \\mid X)$.\n\n**Verdict: Correct.**\n\n**B. Under MAR, MI delivers unbiased pooled point estimates for any analysis model as long as the imputation model includes the same set of variables as predictors, regardless of whether it includes the same transformations (for example, interactions or nonlinear terms). Therefore, omitting $XZ$ from the imputation model when the analysis model includes it cannot bias the MI estimate of the $XZ$ coefficient.**\n\nThis statement is false. It incorrectly claims that including the base variables ($X$ and $Z$) is sufficient for unbiasedness. As derived above, the functional form of the relationship, including interactions like $XZ$, is critical. Omitting a term from the imputation model that is present in the analysis model is a classic cause of bias in MI.\n\n**Verdict: Incorrect.**\n\n**C. Congeniality concerns only variance estimation; as long as MAR holds, an imputation model that omits interaction or nonlinear terms used in the analysis model will produce correct pooled point estimates, with at worst conservative standard errors due to model mismatch.**\n\nThis statement is false. Uncongeniality, particularly when the imputation model is misspecified to be simpler than the analysis model, primarily leads to biased point estimates. While it also affects variance estimation, often leading to underestimation (i.e., anti-conservative standard errors) because the imputation model fails to propagate uncertainty about the more complex structure, the main issue is bias in the point estimate. The claim that the point estimate is correct is fundamentally wrong.\n\n**Verdict: Incorrect.**\n\n**D. For a count outcome analyzed with a log-linear Poisson regression including an $XZ$ interaction, imputing missing counts from a homoscedastic normal linear regression on $X$ and $Z$ (excluding $XZ$) is congenial under MAR because both models are linear in parameters; therefore, the pooled estimate of the interaction is unbiased as $m \\to \\infty$.**\n\nThis statement is false on multiple grounds.\n1.  **Omission of Interaction:** It proposes an imputation model that excludes the $XZ$ interaction, while the analysis model includes it. As established, this is a form of uncongeniality that leads to bias.\n2.  **Distributional Mismatch:** It suggests imputing a count variable (which is discrete and non-negative) using a normal linear regression model (which assumes a continuous, homoscedastic variable and can generate negative or non-integer values). This mismatch between the distributional assumptions of the imputation and analysis models is another form of uncongeniality that can cause bias and other problems.\n3.  **Invalid Justification:** The claim of congeniality \"because both models are linear in parameters\" is irrelevant and misleading. This property of Generalised Linear Models does not override the fundamental requirements for congeniality concerning the included variables/terms and the appropriateness of the distributional assumptions.\nThe conclusion that the estimate is unbiased is therefore incorrect.\n\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "4973789"}]}