{"hands_on_practices": [{"introduction": "A primary motivation for using multiple imputation is to obtain valid statistical inferences by correctly accounting for the uncertainty of the missing data. This exercise demonstrates the pitfalls of simpler, 'improper' methods by asking you to quantify the underestimation of variance that occurs with a single imputation compared to a proper MI approach [@problem_id:4976456]. By working through this comparison, you will gain a concrete understanding of why MI's pooling rules, which combine within- and between-imputation variance, are necessary for accurate inference.", "problem": "A clinical trial in hypertensive patients measures change in Systolic Blood Pressure (SBP) from baseline to $3$ months, denoted by $Y_{i}$, for $i=1,\\dots,n$. Assume that the outcome model is $Y_{i} \\sim \\mathcal{N}(\\mu, \\sigma^{2})$ with a known residual variance $\\sigma^{2} = 100 \\, \\mathrm{mmHg}^2$ based on prior external validation, and Missing Completely At Random (MCAR) for a subset of outcomes, independently of $Y_{i}$. The total sample size is $n = 200$, with $n_{\\mathrm{obs}} = 120$ observed outcomes and $n_{\\mathrm{miss}} = 80$ missing outcomes.\n\nConsider two imputation-and-analysis strategies for estimating the population mean $\\mu$:\n1. Improper single imputation: perform a single imputation by drawing each missing $Y_{i}$ independently from $\\mathcal{N}(\\bar{Y}_{\\mathrm{obs}}, \\sigma^{2})$, where $\\bar{Y}_{\\mathrm{obs}}$ is the sample mean of the observed outcomes. Analyze the completed data as if fully observed, using the complete-data variance formula for the sample mean, which, under the known residual variance assumption, is $\\sigma^{2}/n$.\n2. Proper Multiple Imputation (MI): perform $m = 20$ imputations. For each imputation $j$, draw $\\mu^{(j)}$ from its posterior distribution given the observed data, and then draw each missing $Y_{i}$ independently from $\\mathcal{N}(\\mu^{(j)}, \\sigma^{2})$. Combine the point estimates and the variances across the $m$ completed datasets in a manner that correctly aggregates within-imputation uncertainty and between-imputation uncertainty. Multiple Imputation (MI) refers to repeated imputation with appropriate combination of estimates to reflect all sources of uncertainty.\n\nUsing fundamental probability laws and the MCAR assumption, derive the expected total variance of the estimator of $\\mu$ under the proper MI strategy and the expected variance under the improper single imputation strategy. Define the bias of the improper strategy relative to the proper MI strategy as $\\mathrm{Bias} = \\mathbb{E}[\\widehat{V}_{\\mathrm{improper}}] - \\mathbb{E}[\\widehat{V}_{\\mathrm{proper}}]$. Compute this bias numerically for the specified $n$, $n_{\\mathrm{obs}}$, $n_{\\mathrm{miss}}$, $\\sigma^{2}$, and $m$.\n\nExpress the final bias in $\\mathrm{mmHg}^2$ and round your answer to four significant figures.", "solution": "The problem requires the derivation and computation of the bias of an improper single imputation variance estimator relative to a proper multiple imputation (MI) variance estimator for the population mean $\\mu$. The bias is defined as $\\mathrm{Bias} = \\mathbb{E}[\\widehat{V}_{\\mathrm{improper}}] - \\mathbb{E}[\\widehat{V}_{\\mathrm{proper}}]$.\n\nFirst, we analyze the improper single imputation strategy (Strategy 1).\nThe problem states that after a single imputation, the completed data is analyzed as if fully observed, and the variance of the sample mean is taken to be the complete-data variance formula for a known residual variance, which is $\\sigma^{2}/n$. Therefore, the variance estimator under this strategy is a constant value determined by the given parameters:\n$$\n\\widehat{V}_{\\mathrm{improper}} = \\frac{\\sigma^{2}}{n}\n$$\nThe expectation of a constant is the constant itself. Thus, the expected variance under the improper strategy is:\n$$\n\\mathbb{E}[\\widehat{V}_{\\mathrm{improper}}] = \\frac{\\sigma^{2}}{n}\n$$\n\nNext, we analyze the proper multiple imputation strategy (Strategy 2).\nThe total variance of the MI estimator for $\\mu$ is estimated using Rubin's rules. The formula for the total variance, $\\widehat{V}_{\\mathrm{proper}}$, is the sum of the within-imputation variance, $W$, and the between-imputation variance, $B$, adjusted for a finite number of imputations, $m$:\n$$\n\\widehat{V}_{\\mathrm{proper}} = W + \\left(1 + \\frac{1}{m}\\right)B\n$$\nWe need to find the expectation of this estimator, $\\mathbb{E}[\\widehat{V}_{\\mathrm{proper}}] = \\mathbb{E}[W] + \\left(1 + \\frac{1}{m}\\right)\\mathbb{E}[B]$.\n\nThe within-imputation variance, $W$, is the average of the variance estimates from each of the $m$ completed datasets. For each completed dataset $j$, the estimate of the variance of the mean $\\hat{\\mu}^{(j)}$ is $\\widehat{V}^{(j)}$. Since the data is considered complete and the residual variance $\\sigma^2$ is known, this variance is $\\widehat{V}^{(j)} = \\sigma^2/n$. This value is the same for all $j=1, \\dots, m$.\n$$\nW = \\frac{1}{m}\\sum_{j=1}^{m} \\widehat{V}^{(j)} = \\frac{1}{m}\\sum_{j=1}^{m} \\frac{\\sigma^{2}}{n} = \\frac{\\sigma^{2}}{n}\n$$\nSince $W$ is a constant, its expectation is $\\mathbb{E}[W] = \\sigma^2/n$.\n\nThe between-imputation variance, $B$, is the sample variance of the point estimates of $\\mu$ across the $m$ imputations:\n$$\nB = \\frac{1}{m-1}\\sum_{j=1}^{m} (\\hat{\\mu}^{(j)} - \\bar{\\mu}_{\\mathrm{MI}})^{2}\n$$\nwhere $\\hat{\\mu}^{(j)}$ is the sample mean from the $j$-th imputed dataset and $\\bar{\\mu}_{\\mathrm{MI}} = \\frac{1}{m}\\sum_{j=1}^{m} \\hat{\\mu}^{(j)}$.\n$B$ is an unbiased estimator of the true variance between imputations, which arises because the missing data are imputed from a distribution that reflects the uncertainty about the true parameters. Let this true variance be $B_{\\mathrm{true}}$. Thus, $\\mathbb{E}[B] = B_{\\mathrm{true}}$.\nIn the Bayesian framework underlying MI, the total variance of the parameter $\\mu$ given the observed data $Y_{\\mathrm{obs}}$ is $T = \\text{Var}(\\mu|Y_{\\mathrm{obs}})$. This total variance can be decomposed into the complete-data variance $U = \\text{Var}(\\mu|Y_{\\mathrm{obs}}, Y_{\\mathrm{miss}})$ and the variance due to missing data, $B_{\\mathrm{true}}$. That is, $T = U + B_{\\mathrm{true}}$. Under the assumption of a non-informative prior for $\\mu$ and known $\\sigma^2$:\nThe complete-data posterior variance (what we would have with the full dataset of size $n$) is $U = \\sigma^2/n$.\nThe posterior variance based only on observed data (of size $n_{\\mathrm{obs}}$) is $T = \\sigma^2/n_{\\mathrm{obs}}$.\nTherefore, the true between-imputation variance is:\n$$\nB_{\\mathrm{true}} = T - U = \\frac{\\sigma^{2}}{n_{\\mathrm{obs}}} - \\frac{\\sigma^{2}}{n} = \\sigma^{2}\\left(\\frac{1}{n_{\\mathrm{obs}}} - \\frac{1}{n}\\right) = \\sigma^{2}\\frac{n - n_{\\mathrm{obs}}}{n \\cdot n_{\\mathrm{obs}}} = \\sigma^{2}\\frac{n_{\\mathrm{miss}}}{n \\cdot n_{\\mathrm{obs}}}\n$$\nSince $\\mathbb{E}[B] = B_{\\mathrm{true}}$, we have:\n$$\n\\mathbb{E}[B] = \\sigma^{2}\\frac{n_{\\mathrm{miss}}}{n \\cdot n_{\\mathrm{obs}}}\n$$\nNow we can compute the expected total variance for the proper MI strategy:\n$$\n\\mathbb{E}[\\widehat{V}_{\\mathrm{proper}}] = \\mathbb{E}[W] + \\left(1 + \\frac{1}{m}\\right)\\mathbb{E}[B] = \\frac{\\sigma^{2}}{n} + \\left(1 + \\frac{1}{m}\\right)\\sigma^{2}\\frac{n_{\\mathrm{miss}}}{n \\cdot n_{\\mathrm{obs}}}\n$$\n\nWith the expected variances for both strategies, we can calculate the bias:\n$$\n\\mathrm{Bias} = \\mathbb{E}[\\widehat{V}_{\\mathrm{improper}}] - \\mathbb{E}[\\widehat{V}_{\\mathrm{proper}}]\n$$\n$$\n\\mathrm{Bias} = \\frac{\\sigma^{2}}{n} - \\left[\\frac{\\sigma^{2}}{n} + \\left(1 + \\frac{1}{m}\\right)\\sigma^{2}\\frac{n_{\\mathrm{miss}}}{n \\cdot n_{\\mathrm{obs}}}\\right]\n$$\n$$\n\\mathrm{Bias} = -\\left(1 + \\frac{1}{m}\\right)\\sigma^{2}\\frac{n_{\\mathrm{miss}}}{n \\cdot n_{\\mathrm{obs}}}\n$$\nThe negative sign indicates that the improper single imputation strategy systematically underestimates the variance.\n\nNow, we substitute the given numerical values:\n$n = 200$, $n_{\\mathrm{obs}} = 120$, $n_{\\mathrm{miss}} = 80$, $\\sigma^{2} = 100$, and $m=20$.\n$$\n\\mathrm{Bias} = -\\left(1 + \\frac{1}{20}\\right) \\times 100 \\times \\frac{80}{200 \\times 120}\n$$\n$$\n\\mathrm{Bias} = -\\left(\\frac{21}{20}\\right) \\times 100 \\times \\frac{80}{24000}\n$$\n$$\n\\mathrm{Bias} = -1.05 \\times 100 \\times \\frac{1}{300}\n$$\n$$\n\\mathrm{Bias} = -1.05 \\times \\frac{100}{300} = -1.05 \\times \\frac{1}{3}\n$$\n$$\n\\mathrm{Bias} = -0.35\n$$\nThe problem asks for the answer to be rounded to four significant figures.\nThe bias is $-0.3500 \\, \\mathrm{mmHg}^2$.", "answer": "$$\\boxed{-0.3500}$$", "id": "4976456"}, {"introduction": "A valid multiple imputation procedure requires the imputation model to be 'congenial' with the analysis model, meaning it must preserve the relationships you intend to study. This practice problem explores the consequences of violating this principle in the context of a randomized trial [@problem_id:4976459]. You will derive the asymptotic bias in a treatment effect estimate when the treatment indicator itself is omitted from the imputation model, providing a crucial lesson on the importance of careful model specification.", "problem": "A randomized controlled trial in cardiovascular medicine studies the effect of an antihypertensive treatment on systolic blood pressure. Let $T \\in \\{0,1\\}$ denote treatment assignment with $\\Pr(T=1)=1/2$, and let $X \\in \\mathbb{R}$ denote a baseline covariate (for example, baseline risk score) with $X \\sim \\mathcal{N}(0,\\sigma_{X}^{2})$, independent of $T$. The continuous outcome is modeled by a linear Gaussian data-generating process\n$$\nY \\;=\\; \\alpha \\;+\\; \\tau\\, T \\;+\\; \\beta\\, X \\;+\\; \\varepsilon,\\qquad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2}),\\quad \\varepsilon \\perp (X,T).\n$$\nOutcomes are subject to missingness indicated by $R \\in \\{0,1\\}$, where $R=1$ means $Y$ is observed. The missingness mechanism is Missing At Random (MAR) with\n$$\n\\Pr(R=1 \\mid X,T,Y) \\;=\\; s(X),\n$$\nfor some measurable function $s$ with $0 < s(X) < 1$, and $s(X)$ does not depend on $T$ or $Y$ given $X$. Owing to randomization and the MAR mechanism depending only on $X$, the marginal missingness fraction is the same in both arms; let\n$$\nr \\;=\\; \\Pr(R=0 \\mid T=1) \\;=\\; \\Pr(R=0 \\mid T=0) \\;=\\; 1 - \\mathbb{E}\\big[s(X)\\big].\n$$\nAn analyst performs Multiple Imputation (MI) but uses a mis-specified imputation model that omits the treatment indicator. Specifically, for each imputation, the analyst fits a linear regression of $Y$ on $X$ using only the observed cases $R=1$, pooling both treatment arms, and imputes missing $Y$ values from a Normal distribution with mean $a + b X$ and some variance estimated from the observed data (the exact variance used for the imputation is arbitrary but finite and positive). After creating many imputations and completing the data, the analyst estimates the treatment effect by the unadjusted difference in the completed-data arm means and averages this point estimate across imputations.\n\nUsing only fundamental definitions of Missing At Random, the law of total expectation, and the definition of the Multiple Imputation point estimator as the average of completed-data estimators, derive the large-sample limit of the MI point estimate of the treatment effect and compute its asymptotic bias relative to the true $\\tau$. Express your final answer as a single closed-form analytic expression for the asymptotic bias in terms of $\\tau$ and $r$. Do not provide a derivation of variance components. Do not plug in numbers. The final answer must be a single expression with no units.", "solution": "The user wants to find the asymptotic bias of a Multiple Imputation (MI) estimator for a treatment effect when the imputation model is mis-specified.\n\n### 1. Preliminaries and Definitions\n\nThe true data-generating process is given by the linear model:\n$$\nY \\;=\\; \\alpha \\;+\\; \\tau\\, T \\;+\\; \\beta\\, X \\;+\\; \\varepsilon\n$$\nwhere $T \\in \\{0,1\\}$ with $\\Pr(T=1) = 1/2$, $X \\sim \\mathcal{N}(0, \\sigma_X^2)$, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$. We are given that $X \\perp T$ and $\\varepsilon \\perp (X,T)$.\n\nThe true average treatment effect, $\\tau$, is the difference in the expected outcomes under treatment and control:\n$$\n\\text{ATE} \\;=\\; \\mathbb{E}[Y \\mid T=1] - \\mathbb{E}[Y \\mid T=0]\n$$\nUsing the linearity of expectation and the fact that $\\mathbb{E}[X \\mid T=t] = \\mathbb{E}[X] = 0$ (due to $X \\perp T$):\n$$\n\\mathbb{E}[Y \\mid T=t] \\;=\\; \\mathbb{E}[\\alpha + \\tau t + \\beta X + \\varepsilon \\mid T=t] \\;=\\; \\alpha + \\tau t + \\beta \\mathbb{E}[X] \\;=\\; \\alpha + \\tau t\n$$\nThus, the true treatment effect is:\n$$\n\\tau \\;=\\; (\\alpha + \\tau) - \\alpha \\;=\\; \\tau\n$$\n\nThe MI point estimator for the treatment effect, $\\hat{\\tau}_{MI}$, is the average of the completed-data estimators. In the large-sample limit (as the number of subjects and the number of imputations go to infinity), this converges to:\n$$\n\\hat{\\tau}_{MI}^{\\infty} \\;=\\; \\mathbb{E}[Y_{\\text{comp}} \\mid T=1] - \\mathbb{E}[Y_{\\text{comp}} \\mid T=0]\n$$\nwhere $Y_{\\text{comp}}$ is a random variable representing a value in the completed dataset.\n\nBy definition, $Y_{\\text{comp}} = Y$ if the outcome is observed ($R=1$), and $Y_{\\text{comp}} = Y_{\\text{imp}}$ if the outcome is missing ($R=0$), where $Y_{\\text{imp}}$ is the imputed value.\n\n### 2. Large-Sample Expectation of Completed Data\n\nWe can express the expected value of the completed data in each treatment arm using the law of total expectation, conditioning on the missingness indicator $R$:\n$$\n\\mathbb{E}[Y_{\\text{comp}} \\mid T=t] \\;=\\; \\mathbb{E}[Y_{\\text{comp}} \\mid T=t, R=1]\\Pr(R=1 \\mid T=t) \\;+\\; \\mathbb{E}[Y_{\\text{comp}} \\mid T=t, R=0]\\Pr(R=0 \\mid T=t)\n$$\nWe are given that the marginal missingness probability is $r = \\Pr(R=0 \\mid T=t)$, so $\\Pr(R=1 \\mid T=t) = 1-r$.\n$$\n\\mathbb{E}[Y_{\\text{comp}} \\mid T=t] \\;=\\; (1-r)\\mathbb{E}[Y \\mid T=t, R=1] \\;+\\; r\\mathbb{E}[Y_{\\text{imp}} \\mid T=t, R=0]\n$$\n\n### 3. Characterizing the Imputation Model\n\nThe analyst uses a mis-specified imputation model by regressing $Y$ on $X$ using only observed cases ($R=1$) from both treatment arms pooled together. The imputed values are drawn from a distribution with a mean given by this regression model. In the large-sample limit, the coefficients of this regression converge to the population parameters of the conditional expectation $\\mathbb{E}[Y \\mid X, R=1]$.\n\nThe missingness mechanism is MAR of the form $\\Pr(R=1 \\mid X,T,Y) = s(X)$, which means that conditional on $X$, missingness is independent of $Y$ and $T$. This implies:\n$$\n\\mathbb{E}[Y \\mid X, R=1] \\;=\\; \\mathbb{E}[Y \\mid X]\n$$\nWe compute $\\mathbb{E}[Y \\mid X]$ by averaging over the treatment assignment $T$:\n$$\n\\mathbb{E}[Y \\mid X] \\;=\\; \\mathbb{E}[\\alpha + \\tau T + \\beta X + \\varepsilon \\mid X] \\;=\\; \\alpha + \\tau \\mathbb{E}[T \\mid X] + \\beta X + \\mathbb{E}[\\varepsilon \\mid X]\n$$\nSince $T \\perp X$ and $\\varepsilon \\perp X$, and $\\mathbb{E}[T] = 1/2$, this simplifies to:\n$$\n\\mathbb{E}[Y \\mid X] \\;=\\; \\alpha + \\frac{\\tau}{2} + \\beta X\n$$\nThis is the large-sample limit of the analyst's imputation model. Thus, the expected value of an imputed outcome, conditional on $X$, is:\n$$\n\\mathbb{E}[Y_{\\text{imp}} \\mid X] \\;=\\; \\alpha + \\frac{\\tau}{2} + \\beta X\n$$\nThis expected value does not depend on the treatment assignment $T$ because $T$ was omitted from the imputation model.\n\n### 4. Calculating Completed-Data Arm Means\n\nNow we can evaluate the terms in the expression for $\\mathbb{E}[Y_{\\text{comp}} \\mid T=t]$.\n\nFor the observed part:\n$$\n\\mathbb{E}[Y \\mid T=t, R=1] \\;=\\; \\mathbb{E}[\\alpha + \\tau T + \\beta X + \\varepsilon \\mid T=t, R=1] \\;=\\; \\alpha + \\tau t + \\beta \\mathbb{E}[X \\mid T=t, R=1]\n$$\nFor the imputed part:\n$$\n\\mathbb{E}[Y_{\\text{imp}} \\mid T=t, R=0] \\;=\\; \\mathbb{E}[\\mathbb{E}[Y_{\\text{imp}} \\mid X] \\mid T=t, R=0] \\;=\\; \\mathbb{E}\\left[\\alpha + \\frac{\\tau}{2} + \\beta X \\mid T=t, R=0\\right]\n$$\n$$\n\\;=\\; \\alpha + \\frac{\\tau}{2} + \\beta \\mathbb{E}[X \\mid T=t, R=0]\n$$\nSubstituting these back into the expression for $\\mathbb{E}[Y_{\\text{comp}} \\mid T=t]$:\n$$\n\\mathbb{E}[Y_{\\text{comp}} \\mid T=t] \\;=\\; (1-r)(\\alpha + \\tau t + \\beta \\mathbb{E}[X \\mid T=t, R=1]) \\;+\\; r\\left(\\alpha + \\frac{\\tau}{2} + \\beta \\mathbb{E}[X \\mid T=t, R=0]\\right)\n$$\nCollecting terms:\n$$\n\\mathbb{E}[Y_{\\text{comp}} \\mid T=t] \\;=\\; (1-r)\\alpha + r\\alpha \\;+\\; (1-r)\\tau t + r\\frac{\\tau}{2} \\;+\\; \\beta \\left( (1-r)\\mathbb{E}[X \\mid T=t, R=1] + r\\mathbb{E}[X \\mid T=t, R=0] \\right)\n$$\nThe term in the parenthesis multiplied by $\\beta$ is an application of the law of total expectation for $\\mathbb{E}[X \\mid T=t]$:\n$$\n(1-r)\\mathbb{E}[X \\mid T=t, R=1] + r\\mathbb{E}[X \\mid T=t, R=0] \\;=\\; \\mathbb{E}[X \\mid T=t]\n$$\nSince $X \\perp T$, we have $\\mathbb{E}[X \\mid T=t]=\\mathbb{E}[X]=0$. Therefore, the entire term involving $\\beta$ is zero.\nThe expression for the completed-data mean simplifies to:\n$$\n\\mathbb{E}[Y_{\\text{comp}} \\mid T=t] \\;=\\; \\alpha + (1-r)\\tau t + \\frac{r\\tau}{2}\n$$\n\nNow, we evaluate this for the treatment ($t=1$) and control ($t=0$) arms:\nFor $T=1$:\n$$\n\\mathbb{E}[Y_{\\text{comp}} \\mid T=1] \\;=\\; \\alpha + (1-r)\\tau + \\frac{r\\tau}{2} \\;=\\; \\alpha + \\tau - r\\tau + \\frac{r\\tau}{2} \\;=\\; \\alpha + \\tau\\left(1-\\frac{r}{2}\\right)\n$$\nFor $T=0$:\n$$\n\\mathbb{E}[Y_{\\text{comp}} \\mid T=0] \\;=\\; \\alpha + (1-r)\\tau(0) + \\frac{r\\tau}{2} \\;=\\; \\alpha + \\frac{r\\tau}{2}\n$$\n\n### 5. Calculating the Bias\n\nThe large-sample limit of the MI estimator is the difference between these two expectations:\n$$\n\\hat{\\tau}_{MI}^{\\infty} \\;=\\; \\mathbb{E}[Y_{\\text{comp}} \\mid T=1] - \\mathbb{E}[Y_{\\text{comp}} \\mid T=0]\n$$\n$$\n\\hat{\\tau}_{MI}^{\\infty} \\;=\\; \\left[ \\alpha + \\tau\\left(1-\\frac{r}{2}\\right) \\right] - \\left[ \\alpha + \\frac{r\\tau}{2} \\right] \\;=\\; \\tau - \\frac{r\\tau}{2} - \\frac{r\\tau}{2} \\;=\\; \\tau - r\\tau \\;=\\; \\tau(1-r)\n$$\nThe asymptotic bias is the difference between the large-sample limit of the estimator and the true parameter $\\tau$:\n$$\n\\text{Bias} \\;=\\; \\hat{\\tau}_{MI}^{\\infty} - \\tau \\;=\\; \\tau(1-r) - \\tau \\;=\\; -\\tau r\n$$\nThis result demonstrates that omitting a predictor of the outcome that is also associated with treatment (in this case, the treatment indicator $T$ itself) from the imputation model leads to a bias that is proportional to the true effect size $\\tau$ and the fraction of missing data $r$. The bias is toward the null, as the imputation procedure pulls the means of the two groups toward the grand mean.", "answer": "$$\\boxed{-r\\tau}$$", "id": "4976459"}, {"introduction": "Executing a multiple imputation analysis in practice requires more than just understanding the theory; it demands a systematic and reproducible workflow. This exercise presents a realistic medical research scenario and challenges you to identify the elements of a high-quality, end-to-end imputation strategy, from managing stochasticity to specifying congenial models and performing diagnostics [@problem_id:4976551]. Successfully navigating this problem will equip you with a practical checklist for conducting rigorous and reproducible research with missing data.", "problem": "A multi-center clinical registry is analyzing a binary outcome $Y$ (death within $30$ days) with covariates age (continuous), sex (binary), creatinine (continuous, approximately log-normal, $28\\%$ missing), systolic blood pressure (continuous, $12\\%$ missing), and center (categorical with $18$ levels). The investigators plan to fit a logistic regression with center fixed effects and prespecified interactions age $\\times$ sex and log-creatinine $\\times$ sex. Missingness is plausibly Missing At Random (MAR) conditional on the listed covariates and center. They will use Multiple Imputation (MI) by Fully Conditional Specification (FCS; also known as Multiple Imputation by Chained Equations) with Predictive Mean Matching (PMM) for skewed continuous variables and logistic models for binary variables.\n\nFrom first principles, recall that MI draws $m$ completed datasets by sampling from the posterior predictive distribution of the missing data under a fully specified imputation model, then fits the analysis model to each completed dataset and combines estimates using established pooling rules. Reproducibility requires that the entire process is a deterministic function of the observed data and the pseudo-random number generator state. Cross-software consistency requires that the same stochastic data-generating mechanism and analysis conventions are implemented across platforms.\n\nConsider the following candidate workflows. Which option best outlines a reproducible MI workflow that includes seed management, diagnostic plots, and pooling scripts, and also explains how to ensure consistent results across software platforms (for example, across R and Stata), under the stated modeling goals?\n\nA. Set a single global seed once, run $m$ imputations in parallel using software defaults for predictor matrices and PMM donor counts, let each worker draw from the same random number stream, visually check convergence by plotting only the overall proportion missing per iteration, and pool using Rubin’s rules. Accept that results should be identical across software if the same seed is used.\n\nB. Choose $m$ and fix a master seed $s_0$; generate $m$ independent random number generator (RNG) streams deterministically from $s_0$ (for example, using L’Ecuyer–Combined Multiple Recursive Generator streams), assign one stream per imputation to avoid dependence on parallelization order, and record $s_0$ and stream indices. Predefine and document the imputation models to mirror the analysis model structure: specify the same transformations (for example, log-creatinine), interactions (for example, age $\\times$ sex, log-creatinine $\\times$ sex), and passive imputation rules for derived variables; standardize continuous predictors outside the imputation engine using identical centering and scaling; fix PMM donor count $k$; fix the burn-in $b$ and iteration count per cycle $t$; and set bounds and type constraints identically. Produce diagnostic plots for each incomplete variable: trace plots of within-chain means and variances across iterations and imputations, autocorrelation functions, density overlays of observed versus imputed values by center, and overimputation or posterior predictive checks. Implement pooling using scripts that compute, for each parameter, the within-imputation variance $\\bar{U}$, between-imputation variance $B$, total variance $T=\\bar{U} + (1+\\frac{1}{m})B$, the fraction of missing information, and small-sample degrees-of-freedom adjustments (for example, Barnard–Rubin), and report Monte Carlo error. For cross-software consistency, enforce identical factor levels and contrasts for center and sex, identical link and family for models, identical priors or penalty equivalents where relevant, identical PMM donor $k$, equal iteration schedules, and identical preprocessing pipelines; document software and package versions. Expect stochastic equivalence (matching within Monte Carlo error), not bitwise identity, because RNGs and numerical solvers differ.\n\nC. To guarantee cross-platform identity and reproducibility, replace stochastic draws by imputing conditional means (deterministic regression imputation) for all continuous variables and modal classes for categorical variables, then analyze the single completed dataset and use complete-data variance formulas, which remain unbiased because no randomness is introduced.\n\nD. To emulate MI with minimal computation, generate a single imputed dataset ($m=1$) but refit the analysis model $m$ times with different seeds to produce $m$ estimates, then pool them using Rubin’s rules as if they arose from distinct imputations; since the analysis model is the same, this reproduces MI behavior.\n\nE. Ensure reproducibility by sorting the dataset by subject identifier before imputation, leaving all other settings at software defaults including rounding imputed values to the nearest observed value, producing only density overlays of observed versus imputed values, and expecting identical results across software so long as file order and a common seed are shared.\n\nF. For multilevel structure, always treat center as a random effect in imputation regardless of the analysis model, rely on each software’s default priors, set $m=5$ because increases beyond $5$ rarely matter, and pool by averaging point estimates and ignoring between-imputation variance when $m$ is large, which improves reproducibility by reducing sensitivity to random seeds.\n\nSelect all that apply.", "solution": "The user has provided a problem statement regarding a multiple imputation (MI) workflow for a clinical registry dataset. The task is to validate the problem statement and then identify the option that best outlines a reproducible and statistically sound MI workflow.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Outcome:** Binary variable $Y$ (death within $30$ days).\n-   **Covariates:**\n    -   `age`: continuous.\n    -   `sex`: binary.\n    -   `creatinine`: continuous, approximately log-normal, $28\\%$ missing.\n    -   `systolic blood pressure`: continuous, $12\\%$ missing.\n    -   `center`: categorical with $18$ levels.\n-   **Proposed Analysis Model:** A logistic regression for $Y$ with the following terms: `age`, `sex`, `log(creatinine)`, `systolic blood pressure`, `center` (as fixed effects), `age` $\\times$ `sex` interaction, and `log(creatinine)` $\\times$ `sex` interaction.\n-   **Missing Data Assumption:** Missing At Random (MAR) conditional on the listed covariates and `center`.\n-   **Proposed Imputation Method:** Multiple Imputation (MI) by Fully Conditional Specification (FCS), also known as Multiple Imputation by Chained Equations.\n-   **Proposed Imputation Models within FCS:**\n    -   Predictive Mean Matching (PMM) for skewed continuous variables.\n    -   Logistic models for binary variables.\n-   **First Principles Summary:**\n    1.  MI draws $m$ completed datasets by sampling from the posterior predictive distribution of the missing data.\n    2.  The analysis model is fitted to each of the $m$ datasets.\n    3.  Estimates are combined using established pooling rules.\n    4.  Reproducibility requires the process to be a deterministic function of observed data and the pseudo-random number generator (RNG) state.\n    5.  Cross-software consistency requires equivalent stochastic data-generating mechanisms and analysis conventions.\n-   **Question:** Identify the option that best outlines a reproducible MI workflow, including seed management, diagnostics, pooling, and cross-software consistency, for the stated modeling goals.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is firmly based on established principles of modern biostatistics. Multiple imputation, FCS, PMM, logistic regression, MAR, and Rubin's pooling rules are all standard and well-accepted methods. The scenario described is a realistic and common challenge in multi-center clinical studies. The description of first principles is accurate and consistent with statistical theory.\n-   **Well-Posed:** The problem is well-posed. It provides a detailed, concrete statistical scenario and asks for an evaluation of different procedural workflows against the established best practices summarized in the problem itself. It requests the \"best\" option, which implies a rank-ordering based on correctness and completeness, a solvable task.\n-   **Objective:** The language is technical, precise, and free of subjective or opinion-based claims. The scenario and the principles are described objectively.\n\n**Step 3: Verdict and Action**\nThe problem statement is scientifically sound, well-posed, and objective. It contains no contradictions, ambiguities, or factual errors. The problem is **valid**. I will proceed with the analysis of the options.\n\n### Solution Derivation and Option Analysis\n\nA correct and reproducible MI workflow must address several key areas: (1) control of stochasticity for reproducibility, especially in parallel environments; (2) specification of imputation models that are \"congenial\" with the analysis model (i.e., include all relationships of interest); (3) thorough diagnostics to check the plausibility of imputed values and convergence of the algorithm; (4) correct application of pooling rules to account for missing data uncertainty; and (5) a realistic understanding of cross-platform consistency.\n\n**Option A Evaluation**\nThis option suggests setting a single global seed and letting parallel workers draw from the same stream. This is a flawed approach to parallel random number generation. If multiple processes draw from the same stream without coordination, the sequence of numbers they receive depends on scheduling and execution speed, making the result non-reproducible. A proper method uses independent streams for each parallel process. The suggestion to use \"software defaults for predictor matrices\" is poor practice; the imputation model must be carefully specified to include all analysis model variables (including the outcome $Y$) and relevant interactions to ensure congeniality. Diagnostics are insufficient (\"only the overall proportion missing\"). Finally, the claim that results \"should be identical across software if the same seed is used\" is false. Different software packages use different RNG algorithms, numerical solvers, and default settings, so bitwise identity is not expected; the goal is stochastic equivalence.\n**Verdict: Incorrect.**\n\n**Option B Evaluation**\nThis option outlines a comprehensive and rigorous workflow.\n1.  **Seed Management:** It correctly specifies using a master seed $s_0$ to deterministically generate $m$ independent RNG streams (e.g., L’Ecuyer streams), one for each imputation chain. This is the state-of-the-art method for ensuring reproducibility in parallel computation.\n2.  **Imputation Model Specification:** It correctly emphasizes predefining and documenting the imputation models to mirror the analysis model, including transformations (`log-creatinine`), interactions (`age` $\\times$ `sex`, `log-creatinine` $\\times$ `sex`), and using passive imputation. It also correctly notes the need to fix hyperparameters like the PMM donor count $k$, burn-in $b$, and thinning interval $t$.\n3.  **Diagnostics:** It proposes a thorough suite of diagnostic checks: trace plots of chain means and variances to assess convergence, and density overlays to check the plausibility of imputed distributions. These are standard best practices.\n4.  **Pooling:** It correctly states Rubin's rules for pooling, including the formulas for within-imputation variance ($\\bar U$), between-imputation variance ($B$), and total variance ($T=\\bar{U} + (1+\\frac{1}{m})B$). It also mentions advanced but important topics like small-sample adjustments for degrees of freedom and reporting Monte Carlo error.\n5.  **Cross-Software Consistency:** It correctly states that the goal is \"stochastic equivalence,\" not \"bitwise identity,\" and lists the numerous technical details that must be aligned to achieve it (factor contrasts, model families, parameter choices, preprocessing).\nThis option represents a gold-standard approach.\n**Verdict: Correct.**\n\n**Option C Evaluation**\nThis option proposes deterministic imputation (imputing the conditional mean or mode) and analyzing the resulting single dataset with complete-data variance formulas. This is not multiple imputation. It is single imputation, a method known to be statistically invalid because it fails to account for the uncertainty inherent in the imputation process. By treating imputed values as if they were observed, this method systematically underestimates standard errors and produces confidence intervals that are too narrow and p-values that are too small. The claim that variance formulas \"remain unbiased\" is false. The core principle of MI is to propagate the uncertainty about missing values into the final variance estimate, which this method explicitly discards.\n**Verdict: Incorrect.**\n\n**Option D Evaluation**\nThis option demonstrates a fundamental misunderstanding of MI. It suggests creating a single imputed dataset ($m=1$) and then refitting the *analysis model* $m$ times. If the analysis model fitting algorithm is deterministic (which is standard for logistic regression), this will produce the exact same parameter estimates $m$ times. Applying Rubin's rules would result in a between-imputation variance $B=0$, leading to the same underestimation of total variance as single imputation. This procedure does not \"reproduce MI behavior\" because it generates no variability between \"imputations\" and thus fails to capture missing data uncertainty.\n**Verdict: Incorrect.**\n\n**Option E Evaluation**\nThis option focuses on superficial details while ignoring core principles. Sorting the dataset might affect some naive algorithms but is not a robust method for ensuring reproducibility. Relying on \"software defaults\" is poor practice, as defaults are often suboptimal and not congenial with the specific analysis model. Rounding imputed values can introduce bias. The proposed diagnostics (\"only density overlays\") are insufficient as they do not assess convergence of the chained equations algorithm. The expectation of \"identical results across software\" is, as explained previously, incorrect. This workflow is an assemblage of weak or misguided practices.\n**Verdict: Incorrect.**\n\n**Option F Evaluation**\nThis option presents a mix of reasonable but overly strong advice with statistically incorrect procedures. Using a random effect for `center` in the imputation model can be a good strategy to handle clustering, even if `center` is a fixed effect in the analysis model. However, the advice to set $m=5$ is outdated; for the stated $28\\%$ missingness, a much larger $m$ (e.g., $m \\ge 28$) would be recommended by modern standards for stable variance estimation. The most significant error is the suggestion to \"pool by averaging point estimates and ignoring between-imputation variance.\" This negates the entire purpose of multiple imputation, which is to calculate and incorporate this variance component ($B$) to obtain valid inferences. Ignoring $B$ is equivalent to performing single imputation. The claim that this \"improves reproducibility\" is misleading; it reproducibly obtains a wrong answer.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{B}$$", "id": "4976551"}]}