{"hands_on_practices": [{"introduction": "The bootstrap provides a powerful computational framework for understanding the uncertainty of a statistic when its theoretical sampling distribution is difficult or impossible to derive analytically. The sample median is a classic example of a robust statistic whose standard error does not have a simple closed-form expression. This first exercise [@problem_id:4954777] guides you through the fundamental mechanics of the bootstrap, challenging you to implement the procedure from first principles to estimate the standard error of the sample median by resampling directly from the empirical distribution function.", "problem": "You are given independent and identically distributed patient biomarker measurements modeled as realizations $\\{x_1,\\dots,x_n\\}$ from an unknown continuous distribution $F$. Your task is to implement a nonparametric resampling procedure grounded in core definitions to approximate the sampling distribution of the sample median and to compute the Empirical Cumulative Distribution Function (ECDF). Specifically, you must use the definition-based empirical distribution $\\hat F_n$ as the resampling mechanism and simulate the sampling distribution of the sample median by drawing bootstrap samples with replacement from $\\hat F_n$. For each test case, you must compute two quantities: the values of $\\hat F_n(x)$ evaluated at specified query points and the bootstrap estimate of the standard error of the sample median.\n\nFundamental base for the derivation and algorithm design must explicitly rely on core definitions and facts: the definition of the Empirical Cumulative Distribution Function (ECDF), the definition of the sample median as a minimizer of the sum of absolute deviations and as an order statistic, and the bootstrap principle that approximates the sampling distribution by resampling from $\\hat F_n$.\n\nImplement the following for each test case:\n- Compute the ECDF $\\hat F_n(x)$ values at the list of query points. Without invoking any shortcut formulas, obtain $\\hat F_n(x)$ by applying its definition that counts the fraction of observed samples not exceeding $x$.\n- Simulate the bootstrap sampling distribution of the sample median by repeatedly drawing $n$ values with replacement from the observed sample (equivalently, sampling from $\\hat F_n$), computing the median for each resample, and then estimating the standard error as the sample standard deviation of these bootstrap medians using the denominator $B-1$, where $B$ is the number of bootstrap replicates. The sample median for even $n$ must be defined as the average of the two central order statistics.\n- Use the specified pseudo-random number generator seeds to ensure reproducibility.\n\nYour program must use the following test suite, with each test case described by $(\\text{sample}, \\text{queries}, B, \\text{seed})$:\n- Case $1$ (general skewed, positive biomarker values): sample $[1.8, 2.5, 3.0, 3.2, 4.1, 5.5, 7.0, 9.2, 14.8, 26.3]$, queries $[2.0, 4.0, 10.0, 20.0]$, $B=5000$, seed $314159$.\n- Case $2$ (even sample size with ties near zero): sample $[0.12, 0.15, 0.15, 0.20, 0.22, 0.35]$, queries $[0.15, 0.21, 0.40]$, $B=7000$, seed $271828$.\n- Case $3$ (degenerate constant values): sample $[5.0, 5.0, 5.0, 5.0, 5.0]$, queries $[4.9, 5.0, 5.1]$, $B=4000$, seed $444$.\n- Case $4$ (small sample with a severe outlier): sample $[1.0, 2.0, 100.0]$, queries $[1.0, 50.0, 100.0]$, $B=8000$, seed $1234567$.\n\nAll biomarker values and derived quantities are unitless real numbers. For each test case, produce two outputs:\n- A list of floats containing the ECDF values $\\hat F_n(x)$ evaluated at the corresponding query points in the given order.\n- A single float which is the bootstrap estimate of the standard error of the sample median based on $B$ replicates.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a two-element list of the form $[\\text{ecdf\\_values\\_list}, \\text{standard\\_error\\_float}]$. For example, the final output format must be $[[\\text{ecdf\\_case1}, \\text{se\\_case1}],[\\text{ecdf\\_case2}, \\text{se\\_case2}],\\dots]$ with no additional text.", "solution": "The problem requires the implementation of two fundamental nonparametric statistical procedures: the computation of the Empirical Cumulative Distribution Function (ECDF) and the estimation of the standard error of the sample median using the bootstrap method. The solution is designed to strictly adhere to the first principles and definitions provided.\n\n**Part 1: Computation of the Empirical Cumulative Distribution Function (ECDF)**\n\nThe ECDF, denoted $\\hat{F}_n(x)$, is a nonparametric estimate of the true underlying cumulative distribution function $F(x)$. By definition, for a given sample $\\{x_1, x_2, \\dots, x_n\\}$ of size $n$, the ECDF at a point $x$ is the proportion of sample observations that are less than or equal to $x$. This is formally expressed as:\n$$\n\\hat{F}_n(x) = \\frac{1}{n} \\sum_{i=1}^{n} I(x_i \\le x)\n$$\nwhere $I(\\cdot)$ is the indicator function, which evaluates to $1$ if its argument is true and $0$ otherwise.\n\nThe algorithm to compute $\\hat{F}_n(x)$ for a list of query points follows this definition directly, as required. For each query point $q$ in the provided list, the procedure iterates through all data points $x_i$ in the sample. A counter is maintained and incremented for every $x_i$ that satisfies the condition $x_i \\le q$. The final count is then divided by the total sample size $n$ to yield the value of $\\hat{F}_n(q)$. This process is repeated for all query points. This method avoids any library-specific \"shortcut\" functions and implements the counting definition from first principles.\n\n**Part 2: Bootstrap Estimation of the Standard Error of the Sample Median**\n\nThe second task is to estimate the standard error of the sample median, a measure of the variability of the sample median as an estimator of the population median. The bootstrap is a powerful resampling technique for approximating the sampling distribution of a statistic when the underlying population distribution $F$ is unknown.\n\n**The Sample Median**\nThe statistic of interest is the sample median, denoted $\\hat{m}$. For a sample sorted in non-decreasing order, $x_{(1)} \\le x_{(2)} \\le \\dots \\le x_{(n)}$, the sample median is defined based on the sample size $n$.\n- If $n$ is odd, the median is the central value, $\\hat{m} = x_{(\\lceil n/2 \\rceil)}$. In $0$-based indexing, this is $x_{( (n-1)/2 )}$.\n- If $n$ is even, the median is the average of the two central values, $\\hat{m} = \\frac{1}{2} (x_{(n/2)} + x_{(n/2+1)})$. In $0$-based indexing, this corresponds to $\\frac{1}{2}(x_{(n/2 - 1)} + x_{(n/2)})$.\nThis definition is standard and is implemented by functions such as `numpy.median`.\n\n**The Bootstrap Principle and Algorithm**\nThe bootstrap procedure approximates the unknown sampling distribution by leveraging the empirical distribution $\\hat{F}_n$. The core assumption is that resampling from the sample is analogous to drawing new samples from the population. The algorithm proceeds as follows:\n$1$. **Resampling**: A large number of bootstrap samples, $B$, are generated. Each bootstrap sample, denoted $\\{x_1^*, \\dots, x_n^*\\}$, is created by drawing $n$ elements from the original sample $\\{x_1, \\dots, x_n\\}$ *with replacement*. This process is equivalent to sampling from the discrete uniform distribution on the original sample points, which is the mechanism defined by $\\hat{F}_n$.\n$2$. **Statistic Calculation**: For each of the $B$ bootstrap samples, the sample median is calculated. This results in a collection of $B$ bootstrap medians: $\\{m_1^*, m_2^*, \\dots, m_B^*\\}$. This collection empirically approximates the sampling distribution of the sample median.\n$3$. **Standard Error Estimation**: The standard error of the sample median is the standard deviation of its sampling distribution. The bootstrap estimate of this standard error, $SE_{boot}(\\hat{m})$, is computed as the sample standard deviation of the $B$ bootstrap medians. The problem specifies using a denominator of $B-1$, which corresponds to the formula for an unbiased estimate of the variance:\n    $$\n    SE_{boot}(\\hat{m}) = \\sqrt{ \\frac{1}{B-1} \\sum_{j=1}^{B} (m_j^* - \\bar{m}^*)^2 }\n    $$\n    where $\\bar{m}^* = \\frac{1}{B} \\sum_{j=1}^{B} m_j^*$ is the average of the bootstrap medians.\n\n**Implementation Details**\nThe implementation uses the `numpy` library for efficient numerical operations. A pseudo-random number generator, initialized with the specified seed for each test case using `numpy.random.default_rng(seed)`, ensures the reproducibility of the bootstrap resampling process. The function `rng.choice` is used for drawing samples with replacement. The `numpy.median` function is used for calculating the median of each bootstrap sample, and `numpy.std` with the parameter `ddof=1` is used to compute the final standard error estimate, correctly implementing the $B-1$ denominator. This approach integrates the theoretical principles of bootstrapping with a robust and reproducible computational algorithm.", "answer": "```python\nimport numpy as np\n# No other libraries are permitted.\n# scipy is listed in the problem preamble but not needed for this solution.\n\ndef compute_ecdf_values(sample, queries):\n    \"\"\"\n    Computes the ECDF for a set of query points based on a sample,\n    adhering to the definition of counting observations.\n\n    Args:\n        sample (np.ndarray): The observed data points.\n        queries (np.ndarray): The points at which to evaluate the ECDF.\n\n    Returns:\n        list: A list of floats representing the ECDF values at the query points.\n    \"\"\"\n    n = sample.shape[0]\n    if n == 0:\n        return [0.0] * len(queries)\n    \n    ecdf_vals = []\n    for x in queries:\n        # Count how many samples are less than or equal to x\n        count = np.sum(sample = x)\n        ecdf_vals.append(count / n)\n    return ecdf_vals\n\ndef compute_bootstrap_se_median(sample, B, seed):\n    \"\"\"\n    Computes the bootstrap standard error of the sample median.\n\n    Args:\n        sample (np.ndarray): The observed data points.\n        B (int): The number of bootstrap replicates.\n        seed (int): The seed for the pseudo-random number generator.\n\n    Returns:\n        float: The bootstrap estimate of the standard error of the sample median.\n    \"\"\"\n    n = sample.shape[0]\n    if n == 0:\n        return 0.0\n\n    # Initialize the random number generator for reproducibility\n    rng = np.random.default_rng(seed)\n    \n    # Store bootstrap medians\n    bootstrap_medians = np.zeros(B)\n    \n    # Perform B bootstrap resamples\n    for i in range(B):\n        # Draw a sample of size n with replacement from the original sample\n        bootstrap_sample = rng.choice(sample, size=n, replace=True)\n        # Compute and store the median of the bootstrap sample\n        bootstrap_medians[i] = np.median(bootstrap_sample)\n        \n    # The standard error is the sample standard deviation of the bootstrap medians.\n    # The parameter ddof=1 ensures the denominator in the variance calculation is B-1.\n    se = np.std(bootstrap_medians, ddof=1)\n    \n    return se\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        {'sample': [1.8, 2.5, 3.0, 3.2, 4.1, 5.5, 7.0, 9.2, 14.8, 26.3], 'queries': [2.0, 4.0, 10.0, 20.0], 'B': 5000, 'seed': 314159},\n        {'sample': [0.12, 0.15, 0.15, 0.20, 0.22, 0.35], 'queries': [0.15, 0.21, 0.40], 'B': 7000, 'seed': 271828},\n        {'sample': [5.0, 5.0, 5.0, 5.0, 5.0], 'queries': [4.9, 5.0, 5.1], 'B': 4000, 'seed': 444},\n        {'sample': [1.0, 2.0, 100.0], 'queries': [1.0, 50.0, 100.0], 'B': 8000, 'seed': 1234567},\n    ]\n\n    results = []\n    for case in test_cases:\n        # It's good practice to convert lists to numpy arrays for numerical processing\n        sample = np.array(case['sample'])\n        queries = np.array(case['queries'])\n        B = case['B']\n        seed = case['seed']\n        \n        # Task 1: Compute ECDF values\n        ecdf_results = compute_ecdf_values(sample, queries)\n        \n        # Task 2: Compute bootstrap standard error of the median\n        se_median = compute_bootstrap_se_median(sample, B, seed)\n        \n        results.append([ecdf_results, se_median])\n\n    # The required output format is a single line, comma-separated list of lists.\n    # str() on a list produces the desired representation '[...]'\n    # We join these string representations with commas.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4954777"}, {"introduction": "Building on the fundamentals, we now move to a practical and critical application in medical research: assessing the performance of a diagnostic test. A key metric for this is the Area Under the Receiver Operating Characteristic Curve (AUC), but to construct a reliable confidence interval for it from case-control data, we must use a more nuanced approach called stratified resampling. This exercise [@problem_id:4954664] provides hands-on practice in applying this essential technique to compute a percentile bootstrap confidence interval for the AUC, a common task in model validation.", "problem": "You are given a two-class diagnostic setting common in medical studies, with continuous test scores for individuals with disease and individuals without disease. The goal is to construct a bootstrap confidence interval for the Area Under the Receiver Operating Characteristic Curve (AUC) using nonparametric, stratified bootstrap resampling. The Area Under the Receiver Operating Characteristic Curve (AUC) is defined as the probability that a randomly selected diseased individual's score exceeds that of a randomly selected non-diseased individual, with ties contributing half-credit. Use the percentile bootstrap method to estimate a two-sided confidence interval for the AUC, based on resampling at the patient level.\n\nBase your derivation on the following well-tested definitions and facts:\n- The Receiver Operating Characteristic (ROC) curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) as a decision threshold varies; the Area Under the Receiver Operating Characteristic curve (AUC) is a functional of the joint distribution of test scores for diseased and non-diseased individuals and can be interpreted as an integral over the ROC curve or as a probability statement about pairwise comparisons.\n- Nonparametric bootstrap resampling approximates the sampling distribution of a statistic by repeated resampling with replacement from the empirical distribution. In case-control medical studies, resampling should be stratified by class to preserve the class-conditional empirical distributions.\n\nImplement the following tasks:\n1. For each test case, simulate test scores for diseased and non-diseased individuals. For diseased individuals, draw independent scores from a normal distribution with mean $\\mu_1$ and variance $\\sigma_1^2$. For non-diseased individuals, draw independent scores from a normal distribution with mean $\\mu_0$ and variance $\\sigma_0^2$. Use a fixed random seed $s=12345$ so that the results are reproducible. If a rounding resolution $r$ is specified, round each simulated score to the nearest multiple of $r$ (this will induce ties).\n2. Compute the empirical AUC using the pairwise-comparison interpretation: it is the proportion of all diseased versus non-diseased pairs for which the diseased score is greater than the non-diseased score, with ties contributing half-credit. The empirical estimator must be consistent with the rank-based Wilcoxon-Mann-Whitney formulation and must handle ties appropriately.\n3. Construct a $(1-\\alpha)$ bootstrap confidence interval using the percentile method:\n   - Perform $B$ bootstrap replicates. In each replicate, resample with replacement within the diseased group to size $n_1$ and within the non-diseased group to size $n_0$ (stratified resampling), then compute the AUC for the resampled data.\n   - Let $\\hat{F}_B$ be the empirical distribution of the $B$ bootstrap AUC values. The lower bound is the $\\alpha/2$ quantile of $\\hat{F}_B$, and the upper bound is the $(1-\\alpha/2)$ quantile of $\\hat{F}_B$.\n4. For each test case, return the two numbers constituting the lower and upper confidence bounds. Express the two numbers as decimal floats rounded to $6$ decimal places.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a two-element list of the lower and upper bounds. For example: $[ [\\text{lower}_1,\\text{upper}_1],[\\text{lower}_2,\\text{upper}_2],\\dots ]$.\n\nUse the following test suite (all numbers are specified for reproducibility and coverage of different scenarios):\n- Test case $1$ (general case with moderate class balance and separation): $n_1=120$, $n_0=150$, $\\mu_1=1.0$, $\\sigma_1=1.0$, $\\mu_0=0.0$, $\\sigma_0=1.0$, $B=4000$, $\\alpha=0.05$, no rounding.\n- Test case $2$ (high class imbalance, modest separation): $n_1=40$, $n_0=400$, $\\mu_1=0.8$, $\\sigma_1=1.2$, $\\mu_0=0.2$, $\\sigma_0=1.2$, $B=3000$, $\\alpha=0.05$, no rounding.\n- Test case $3$ (small sample size with ties induced by rounding): $n_1=12$, $n_0=10$, $\\mu_1=0.0$, $\\sigma_1=1.0$, $\\mu_0=0.0$, $\\sigma_0=1.0$, rounding resolution $r=0.1$, $B=5000$, $\\alpha=0.10$.\n- Test case $4$ (near perfect separation): $n_1=60$, $n_0=60$, $\\mu_1=2.5$, $\\sigma_1=0.5$, $\\mu_0=-0.5$, $\\sigma_0=0.5$, $B=3000$, $\\alpha=0.05$, no rounding.\n\nAll floating-point outputs must be rounded to $6$ decimal places and expressed as decimals (not fractions), and no physical units are involved. Angles are not involved. The final printed line must be exactly a single bracketed list as specified above.", "solution": "The problem statement has been meticulously validated and is determined to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to proceed with a solution. No scientific, logical, or formal flaws were identified.\n\nThe objective is to compute a two-sided $(1-\\alpha)$ percentile bootstrap confidence interval for the Area Under the Receiver Operating Characteristic Curve (AUC). This task is common in the evaluation of diagnostic tests in medicine, where test scores are collected from a group of individuals with a disease (cases) and a group without the disease (controls).\n\n### Principle 1: The Area Under the ROC Curve (AUC)\n\nThe AUC is a summary measure of the performance of a diagnostic test. A test score $X$ from a diseased individual is drawn from a distribution $F_1$, while a score $Y$ from a non-diseased individual is drawn from a distribution $F_0$. The AUC is formally defined as the probability that the score of a randomly chosen diseased individual is higher than that of a randomly chosen non-diseased individual. When ties can occur, they are typically given a half-credit.\n\n$$\nAUC = P(X > Y) + \\frac{1}{2} P(X = Y)\n$$\n\nGiven a sample of $n_1$ scores $\\{x_i\\}_{i=1}^{n_1}$ from diseased individuals and $n_0$ scores $\\{y_j\\}_{j=1}^{n_0}$ from non-diseased individuals, a non-parametric estimate of the AUC, denoted $\\widehat{AUC}$, is the proportion of all possible $(x_i, y_j)$ pairs for which $x_i$ is greater than $y_j$, plus half the proportion of pairs for which they are equal.\n\n$$\n\\widehat{AUC} = \\frac{1}{n_1 n_0} \\sum_{i=1}^{n_1} \\sum_{j=1}^{n_0} \\Psi(x_i, y_j)\n$$\n\nwhere the scoring function $\\Psi(a,b)$ is defined as:\n\n$$\n\\Psi(a,b) = \\begin{cases} 1  \\text{if } a > b \\\\ \\frac{1}{2}  \\text{if } a = b \\\\ 0  \\text{if } a  b \\end{cases}\n$$\n\nThis estimator is equivalent to the normalized Wilcoxon-Mann-Whitney U-statistic. A computationally efficient method to calculate $\\widehat{AUC}$ uses ranks. First, all $N = n_1 + n_0$ scores are combined and ranked from $1$ to $N$. In case of ties, the average rank (mid-rank) is assigned to each tied score. Let $R_i$ be the rank of the $i$-th diseased score $x_i$ in the combined sample. The U-statistic for the diseased group, $U_1$, is the sum of these ranks, adjusted for the minimum possible rank sum:\n\n$$\nU_1 = \\left( \\sum_{i=1}^{n_1} R_i \\right) - \\frac{n_1(n_1+1)}{2}\n$$\n\nThe AUC estimator is then given by:\n\n$$\n\\widehat{AUC} = \\frac{U_1}{n_1 n_0}\n$$\n\nThis rank-based formulation correctly handles ties and is computationally superior to the naive $O(n_1 n_0)$ pairwise comparison, typically having a complexity of $O(N \\log N)$ due to the sorting involved in ranking.\n\n### Principle 2: Stratified Bootstrap Resampling\n\nThe bootstrap is a powerful resampling technique used to approximate the sampling distribution of a statistic. To construct a confidence interval for the AUC, we need to understand how $\\widehat{AUC}$ would vary across different samples drawn from the underlying populations $F_1$ and $F_0$. Since we only have access to the empirical distributions $\\hat{F}_1$ (from $\\{x_i\\}$) and $\\hat{F}_0$ (from $\\{y_j\\}$), we use these as proxies for the true distributions.\n\nFor case-control data, it is crucial to use **stratified resampling**. This means we resample independently from the diseased and non-diseased groups, preserving the original sample size of each group. This respects the study design and ensures that our bootstrap samples mimic the structure of the original data acquisition process.\n\nThe stratified bootstrap procedure for the AUC is as follows:\nFor $b = 1, 2, \\ldots, B$:\n1.  Generate a bootstrap sample of diseased scores, $X_b^*$, by drawing $n_1$ scores with replacement from the original sample $\\{x_i\\}_{i=1}^{n_1}$.\n2.  Generate a bootstrap sample of non-diseased scores, $Y_b^*$, by drawing $n_0$ scores with replacement from the original sample $\\{y_j\\}_{j=1}^{n_0}$.\n3.  Compute the AUC for this bootstrap replicate, $\\widehat{AUC}_b^*$, using the rank-based method on $X_b^*$ and $Y_b^*$.\n\nThis process yields a collection of $B$ bootstrap AUC values, $\\{\\widehat{AUC}_1^*, \\widehat{AUC}_2^*, \\ldots, \\widehat{AUC}_B^*\\}$, which serves as an empirical approximation of the sampling distribution of $\\widehat{AUC}$.\n\n### Principle 3: The Percentile Confidence Interval\n\nThe percentile method is a direct way to construct a confidence interval from the bootstrap distribution. A $(1-\\alpha)$ confidence interval is formed by taking the quantiles of the sorted bootstrap replicates.\n\n-   The lower bound of the confidence interval is the $(\\alpha/2)$-th quantile of the bootstrap distribution $\\{\\widehat{AUC}_b^*\\}$.\n-   The upper bound is the $(1-\\alpha/2)$-th quantile.\n\nFor instance, for a $95\\%$ confidence interval ($\\alpha=0.05$), the bounds are the $2.5$-th and $97.5$-th percentiles of the ordered list of bootstrap AUCs.\n\n### Algorithmic Solution\nThe overall algorithm proceeds as follows for each test case:\n1.  **Data Generation**: Using a fixed random seed $s=12345$ for reproducibility, simulate $n_1$ scores from $N(\\mu_1, \\sigma_1^2)$ for the diseased group and $n_0$ scores from $N(\\mu_0, \\sigma_0^2)$ for the non-diseased group. If a rounding resolution $r$ is specified, each score is rounded to the nearest multiple of $r$. This step explicitly introduces ties in the data, testing the robustness of the AUC calculation.\n2.  **Bootstrap Loop**: Perform $B$ iterations. In each iteration:\n    a.  Perform stratified resampling with replacement to create bootstrap samples for the diseased and non-diseased groups.\n    b.  Calculate the $\\widehat{AUC}$ on the resampled data using the efficient rank-based method.\n    c.  Store the calculated $\\widehat{AUC}_b^*$.\n3.  **Confidence Interval Calculation**: After $B$ iterations, compute the $\\alpha/2$ and $(1-\\alpha/2)$ quantiles of the collected bootstrap AUC values. These quantiles form the lower and upper bounds of the confidence interval.\n4.  **Formatting**: The resulting bounds are rounded to $6$ decimal places and formatted as specified.\n\nThis procedure robustly estimates the confidence interval for the AUC, correctly handling stratified data and the presence of ties.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Solves the problem of constructing bootstrap confidence intervals for AUC\n    for a series of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1 (general case with moderate class balance and separation)\n        {'n1': 120, 'n0': 150, 'mu1': 1.0, 'sigma1': 1.0, 'mu0': 0.0, 'sigma0': 1.0, 'B': 4000, 'alpha': 0.05, 'r': None},\n        # Test case 2 (high class imbalance, modest separation)\n        {'n1': 40, 'n0': 400, 'mu1': 0.8, 'sigma1': 1.2, 'mu0': 0.2, 'sigma0': 1.2, 'B': 3000, 'alpha': 0.05, 'r': None},\n        # Test case 3 (small sample size with ties induced by rounding)\n        {'n1': 12, 'n0': 10, 'mu1': 0.0, 'sigma1': 1.0, 'mu0': 0.0, 'sigma0': 1.0, 'r': 0.1, 'B': 5000, 'alpha': 0.10},\n        # Test case 4 (near perfect separation)\n        {'n1': 60, 'n0': 60, 'mu1': 2.5, 'sigma1': 0.5, 'mu0': -0.5, 'sigma0': 0.5, 'B': 3000, 'alpha': 0.05, 'r': None},\n    ]\n\n    # Fixed random seed for reproducibility\n    seed = 12345\n    rng = np.random.default_rng(seed)\n\n    def calculate_auc(diseased_scores, non_diseased_scores):\n        \"\"\"\n        Calculates the AUC using the Wilcoxon-Mann-Whitney U-statistic formulation,\n        which is efficient and correctly handles ties.\n        \"\"\"\n        n1 = len(diseased_scores)\n        n0 = len(non_diseased_scores)\n\n        if n1 == 0 or n0 == 0:\n            return 0.5\n\n        # Combine scores and calculate ranks\n        all_scores = np.concatenate((diseased_scores, non_diseased_scores))\n        ranks = stats.rankdata(all_scores, method='average')\n\n        # Sum of ranks for the diseased group\n        sum_ranks_diseased = np.sum(ranks[:n1])\n\n        # Calculate U statistic for the diseased group\n        u_stat = sum_ranks_diseased - (n1 * (n1 + 1)) / 2\n        \n        # AUC is the normalized U statistic\n        auc = u_stat / (n1 * n0)\n        return auc\n\n    results = []\n    for case in test_cases:\n        n1, n0 = case['n1'], case['n0']\n        mu1, sigma1 = case['mu1'], case['sigma1']\n        mu0, sigma0 = case['mu0'], case['sigma0']\n        B, alpha, r = case['B'], case['alpha'], case['r']\n        \n        # 1. Simulate test scores\n        diseased_scores = rng.normal(loc=mu1, scale=sigma1, size=n1)\n        non_diseased_scores = rng.normal(loc=mu0, scale=sigma0, size=n0)\n\n        # Apply rounding if resolution 'r' is specified\n        if r is not None and r > 0:\n            diseased_scores = np.round(diseased_scores / r) * r\n            non_diseased_scores = np.round(non_diseased_scores / r) * r\n\n        # 3. Construct bootstrap confidence interval\n        bootstrap_aucs = np.zeros(B)\n        for i in range(B):\n            # Stratified resampling with replacement\n            resampled_diseased = rng.choice(diseased_scores, size=n1, replace=True)\n            resampled_non_diseased = rng.choice(non_diseased_scores, size=n0, replace=True)\n            \n            # Compute AUC for the resampled data\n            bootstrap_aucs[i] = calculate_auc(resampled_diseased, resampled_non_diseased)\n            \n        # Calculate percentile confidence interval\n        lower_quantile = alpha / 2\n        upper_quantile = 1 - alpha / 2\n        \n        ci_lower = np.quantile(bootstrap_aucs, lower_quantile)\n        ci_upper = np.quantile(bootstrap_aucs, upper_quantile)\n        \n        results.append([ci_lower, ci_upper])\n\n    # Format the final output string to precisely 6 decimal places.\n    formatted_results = []\n    for lower, upper in results:\n        formatted_results.append(f\"[{lower:.6f},{upper:.6f}]\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4954664"}, {"introduction": "A mature understanding of any statistical method requires moving beyond its application to a critical evaluation of its performance. This final practice shifts our perspective from simply *using* the bootstrap to *validating* it. You will investigate the concept of coverage probability—the frequency with which a confidence interval contains the true parameter value—and see how it is affected by properties of the data, such as skewness. Through the Monte Carlo simulation in this exercise [@problem_id:4954794], you will gain a deeper intuition for the conditions under which the bootstrap percentile interval performs as expected and when it may fall short.", "problem": "You are to write a complete, runnable program that uses Monte Carlo simulation to assess whether the bootstrap percentile interval attains nominal coverage for the sample median when the underlying distribution is symmetric versus skewed. The program must implement the following from first principles, using only definitions and well-tested facts from probability and statistics.\n\nLet a sample be drawn as independent and identically distributed (i.i.d.) observations from an unknown distribution function $F$. For a given sample size $n$, denote the sample by $\\{X_1,\\dots,X_n\\}$ and the sample median by $\\hat{m}$. The bootstrap principle approximates the sampling distribution of $\\hat{m}$ by resampling with replacement from the observed sample. Specifically, given an observed sample, generate $B$ i.i.d. bootstrap resamples, each of size $n$, compute the sample median for each resample to obtain $\\{\\hat{m}^{\\ast}_1,\\dots,\\hat{m}^{\\ast}_B\\}$, and then form the percentile confidence interval (Confidence Interval (CI)) at nominal level $1-\\alpha$ by taking the empirical quantiles:\n$$\n[\\hat{q}_{\\alpha/2},\\hat{q}_{1-\\alpha/2}],\n$$\nwhere $\\hat{q}_p$ is the empirical $p$-quantile of $\\{\\hat{m}^{\\ast}_b\\}_{b=1}^B$. The empirical quantile is to be computed by the standard linear interpolation between order statistics: if $\\tilde{m}^{\\ast}_{(1)} \\le \\cdots \\le \\tilde{m}^{\\ast}_{(B)}$ are the ordered bootstrap medians and $p \\in (0,1)$, define $h=(B-1)p+1$, let $k=\\lfloor h \\rfloor$, $t = h-k$, and set\n$$\n\\hat{q}_p = \\tilde{m}^{\\ast}_{(k)} + t\\left(\\tilde{m}^{\\ast}_{(k+1)} - \\tilde{m}^{\\ast}_{(k)}\\right),\n$$\nwith the convention that $\\tilde{m}^{\\ast}_{(B+1)}=\\tilde{m}^{\\ast}_{(B)}$. Coverage at level $1-\\alpha$ is the probability that the true population median $m$ lies in the random interval $[\\hat{q}_{\\alpha/2},\\hat{q}_{1-\\alpha/2}]$.\n\nYour task is to approximate this coverage probability via Monte Carlo simulation for several distributions where the true median $m$ is known. For each test case, do the following:\n- Repeat $R$ times:\n  - Generate an i.i.d. sample of size $n$ from the specified distribution.\n  - Compute the bootstrap percentile interval for the median using $B$ bootstrap resamples as defined above.\n  - Record whether $m \\in [\\hat{q}_{\\alpha/2},\\hat{q}_{1-\\alpha/2}]$ using a closed interval, i.e., count as covered if $m \\ge \\hat{q}_{\\alpha/2}$ and $m \\le \\hat{q}_{1-\\alpha/2}$.\n- The empirical coverage is then the fraction of the $R$ repetitions for which coverage occurred.\n\nUse the following fundamental base:\n- Independent and identically distributed (i.i.d.) sampling from a fixed distribution $F$.\n- The definition of the sample median.\n- The bootstrap resampling scheme as an empirical approximation to the sampling distribution of a statistic.\n- The definition of confidence intervals via quantiles.\n- Basic limit laws such as the Law of Large Numbers to justify Monte Carlo approximation quality at large $R$ (you do not need to prove these).\n\nImplement the simulation for the following test suite of parameter values, which covers symmetric light-tailed, symmetric heavy-tailed, and skewed distributions, as well as a larger-sample scenario:\n- Test case $1$ (symmetric, light-tailed): Normal with mean $0$ and standard deviation $1$, denoted $\\mathcal{N}(0,1)$. Use $n=40$, $\\alpha=0.05$, $B=300$, $R=400$. The true median is $m=0$.\n- Test case $2$ (symmetric, heavy-tailed): Laplace with location $0$ and scale $1$. Use $n=40$, $\\alpha=0.05$, $B=300$, $R=400$. The true median is $m=0$.\n- Test case $3$ (skewed, small-to-moderate sample): Exponential with rate $\\lambda=1$. Use $n=40$, $\\alpha=0.05$, $B=300$, $R=400$. The true median is $m=(\\log 2)/\\lambda$.\n- Test case $4$ (skewed, larger sample): Exponential with rate $\\lambda=1$. Use $n=120$, $\\alpha=0.05$, $B=200$, $R=300$. The true median is $m=(\\log 2)/\\lambda$.\n\nAll random variate generation must be performed using a fixed seed $s=1729$ to ensure replicability. Angles are not involved. No physical units apply. Express all coverage results as decimals in $[0,1]$.\n\nProgram input: none. Program output: a single line consisting of a list of four floating-point numbers representing the empirical coverage for test cases $1$ through $4$, in that order, each rounded to exactly three decimal places. The format must be a single line:\n\"[c1,c2,c3,c4]\"\nwhere $c1$, $c2$, $c3$, and $c4$ are the rounded empirical coverages for the four test cases, respectively, with no additional text or spaces.", "solution": "The user has provided a valid, well-posed problem statement from the field of computational statistics. The problem is scientifically grounded in established statistical theory, namely the use of bootstrap resampling to construct confidence intervals and the use of Monte Carlo simulation to evaluate their performance. All parameters, definitions, and procedures are specified with sufficient precision to permit a unique and reproducible solution. The task is to implement this simulation to assess the coverage probability of the bootstrap percentile interval for the sample median under different underlying distributions.\n\nThe solution is developed by first principles, as requested. The algorithm is constructed by directly translating the statistical definitions into a computational procedure.\n\nThe primary objective is to estimate the coverage probability of a confidence interval. The coverage probability of a procedure for constructing a $(1-\\alpha)$ confidence interval for a parameter $\\theta$ is the probability, over repeated sampling, that the computed interval contains the true value of $\\theta$. A procedure is said to attain the nominal coverage level if this probability is equal to $1-\\alpha$.\n\nThe parameter of interest is the population median, denoted by $m$. The estimator for this parameter is the sample median, $\\hat{m}$, calculated from an independent and identically distributed (i.i.d.) sample $\\{X_1, \\dots, X_n\\}$ drawn from a distribution $F$.\n\nThe confidence interval is constructed using the bootstrap percentile method. The fundamental idea of the bootstrap is to approximate the sampling distribution of a statistic (e.g., $\\hat{m}$) by simulating it. Since the true population distribution $F$ is unknown, we use the empirical distribution function derived from the observed sample as a proxy. The procedure is as follows:\n1.  From the original sample $\\{X_1, \\dots, X_n\\}$, draw a \"bootstrap resample\" $\\{X_1^\\ast, \\dots, X_n^\\ast\\}$ of size $n$ by sampling *with replacement*.\n2.  Calculate the statistic of interest for this resample. Here, we compute the sample median, denoted $\\hat{m}^\\ast$.\n3.  Repeat steps 1 and 2 a large number of times, $B$, to obtain a collection of bootstrap statistics, $\\{\\hat{m}^\\ast_1, \\dots, \\hat{m}^\\ast_B\\}$. This collection serves as an empirical approximation to the sampling distribution of $\\hat{m}$.\n\nThe $(1-\\alpha)$ percentile confidence interval is formed by taking the empirical $\\alpha/2$ and $1-\\alpha/2$ quantiles of the bootstrap distribution. Let the ordered bootstrap medians be $\\tilde{m}^\\ast_{(1)} \\leq \\tilde{m}^\\ast_{(2)} \\leq \\dots \\leq \\tilde{m}^\\ast_{(B)}$. The problem specifies a precise method for calculating the empirical $p$-quantile, $\\hat{q}_p$, via linear interpolation. Given a probability $p \\in (0,1)$, we first find a position index $h = (B-1)p+1$. Let $k = \\lfloor h \\rfloor$ be the integer part and $t = h - k$ be the fractional part. The quantile is then given by:\n$$\n\\hat{q}_p = \\tilde{m}^\\ast_{(k)} + t(\\tilde{m}^\\ast_{(k+1)} - \\tilde{m}^\\ast_{(k)})\n$$\nThe problem specifies the convention $\\tilde{m}^\\ast_{(B+1)} = \\tilde{m}^\\ast_{(B)}$, which ensures the formula is well-defined for $k=B$. The confidence interval is then given by $[\\hat{q}_{\\alpha/2}, \\hat{q}_{1-\\alpha/2}]$.\n\nTo assess the performance of this interval, we estimate its coverage probability using a Monte Carlo simulation. This introduces a second layer of simulation. The overall algorithm for a single test case is:\n1.  Initialize a coverage counter to $0$. Fix the parameters: sample size $n$, significance level $\\alpha$, number of bootstrap resamples $B$, number of Monte Carlo repetitions $R$, the underlying distribution $F$, and its true median $m$.\n2.  Begin the outer loop, which will run $R$ times. Each iteration represents one complete experiment.\n    a.  Generate an original sample $\\{X_1, \\dots, X_n\\}$ of size $n$ from the true distribution $F$.\n    b.  Begin the inner loop (the bootstrap procedure), which will run $B$ times.\n        i. Create a bootstrap resample by drawing $n$ elements from the original sample with replacement.\n        ii. Compute the median of this resample.\n        iii. Store this bootstrap median.\n    c. After the inner loop, all $B$ bootstrap medians $\\{\\hat{m}^\\ast_1, \\dots, \\hat{m}^\\ast_B\\}$ have been collected.\n    d. Sort the bootstrap medians to get the ordered statistics $\\tilde{m}^\\ast_{(1)}, \\dots, \\tilde{m}^\\ast_{(B)}$.\n    e. Calculate the lower and upper bounds of the confidence interval, $\\hat{q}_{\\alpha/2}$ and $\\hat{q}_{1-\\alpha/2}$, using the specified linear interpolation formula.\n    f. Check if the true median $m$ is contained within this computed interval, i.e., if $m \\in [\\hat{q}_{\\alpha/2}, \\hat{q}_{1-\\alpha/2}]$.\n    g. If coverage occurs, increment the coverage counter.\n3.  After the outer loop completes, the estimated coverage probability is the total count from the coverage counter divided by the number of Monte Carlo repetitions, $R$.\n\nThis simulation is performed for four test cases designed to reveal the properties of the bootstrap percentile interval:\n-   **Case 1: Normal distribution $\\mathcal{N}(0,1)$**. This is a symmetric, light-tailed distribution. The sampling distribution of the median is symmetric, and bootstrap methods are expected to perform well. The coverage should be close to the nominal level of $1-\\alpha = 0.95$.\n-   **Case 2: Laplace distribution**. This is a symmetric, heavy-tailed distribution. The sample median is a particularly efficient estimator for the center of a Laplace distribution. Performance is expected to be good.\n-   **Case 3: Exponential distribution, $n=40$**. This distribution is skewed. The sampling distribution of the median will also be skewed. Standard percentile bootstrap intervals often exhibit systematic bias and under-coverage for skewed distributions, especially with smaller sample sizes. We expect the coverage to be notably less than $0.95$.\n-   **Case 4: Exponential distribution, $n=120$**. This is the same skewed distribution but with a larger sample size. As $n$ increases, the sampling distribution of the median becomes more symmetric due to the Central Limit Theorem. Therefore, we expect the performance of the bootstrap to improve, and the coverage should be closer to $0.95$ than in Case 3.\n\nA fixed random seed is used to ensure the reproducibility of the pseudo-random numbers generated for the samples and resamples, leading to a deterministic final output. The implementation will use the `numpy` library for numerical operations and random number generation. The quantile calculation will be implemented explicitly as per the problem's definition to ensure complete fidelity to the specification.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a Monte Carlo simulation to assess the coverage of bootstrap \n    percentile intervals for the sample median across different distributions.\n    \"\"\"\n\n    # Set the global random seed for reproducibility.\n    SEED = 1729\n    rng = np.random.default_rng(SEED)\n\n    def _calculate_quantile(sorted_data, p):\n        \"\"\"\n        Computes the empirical p-quantile using linear interpolation as specified.\n\n        Args:\n            sorted_data (np.ndarray): A 1D numpy array of data, already sorted.\n            p (float): The probability for the quantile, in (0, 1).\n\n        Returns:\n            float: The calculated quantile.\n        \"\"\"\n        B = len(sorted_data)\n        \n        # Calculate the position index h = (B-1)p + 1.\n        h = (B - 1) * p + 1.0\n        \n        # Get integer and fractional parts of h.\n        k = int(h)\n        t = h - k\n        \n        # Convert 1-based k to 0-based index.\n        idx_k = k - 1\n        \n        # Get the value at the k-th order statistic.\n        val_k = sorted_data[idx_k]\n        \n        # Handle the edge case where k is the last element index.\n        # The convention is m*(B+1) = m*(B), making the difference term zero.\n        if k == B:\n            return val_k\n        else:\n            # Get the value at the (k+1)-th order statistic.\n            idx_k_plus_1 = k\n            val_k_plus_1 = sorted_data[idx_k_plus_1]\n            \n            # Apply the linear interpolation formula.\n            return val_k + t * (val_k_plus_1 - val_k)\n\n    def run_simulation(dist_func, true_median, n, alpha, B, R, local_rng):\n        \"\"\"\n        Runs the full simulation for one test case.\n\n        Args:\n            dist_func (callable): A function that takes size n and returns a random sample.\n            true_median (float): The true population median.\n            n (int): The sample size.\n            alpha (float): The significance level.\n            B (int): The number of bootstrap resamples.\n            R (int): The number of Monte Carlo repetitions.\n            local_rng (np.random.Generator): The random number generator instance.\n\n        Returns:\n            float: The empirical coverage probability.\n        \"\"\"\n        coverage_count = 0\n        \n        # Outer loop for Monte Carlo replications.\n        for _ in range(R):\n            # 1. Generate an i.i.d. sample of size n.\n            sample = dist_func(n, local_rng)\n            \n            bootstrap_medians = np.empty(B)\n            \n            # Inner loop for bootstrap resampling.\n            for i in range(B):\n                # 2. Generate a bootstrap resample.\n                resample = local_rng.choice(sample, size=n, replace=True)\n                \n                # 3. Compute and store the median of the resample.\n                bootstrap_medians[i] = np.median(resample)\n            \n            # 4. Sort the bootstrap medians to get order statistics.\n            bootstrap_medians.sort()\n            \n            # 5. Compute the percentile confidence interval.\n            lower_quantile = alpha / 2.0\n            upper_quantile = 1.0 - (alpha / 2.0)\n            \n            ci_lower = _calculate_quantile(bootstrap_medians, lower_quantile)\n            ci_upper = _calculate_quantile(bootstrap_medians, upper_quantile)\n            \n            # 6. Check for coverage.\n            if ci_lower = true_median = ci_upper:\n                coverage_count += 1\n                \n        # 7. Calculate empirical coverage.\n        return coverage_count / R\n\n    # Define random variate generation functions\n    dist_normal = lambda size, r: r.normal(loc=0, scale=1, size=size)\n    dist_laplace = lambda size, r: r.laplace(loc=0, scale=1, size=size)\n    # Numpy's exponential uses scale = 1/lambda. Rate lambda=1 means scale=1.\n    dist_exponential = lambda size, r: r.exponential(scale=1, size=size)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Normal(0, 1)\n        {'name': 'Normal', 'dist': dist_normal, 'median': 0.0, 'n': 40, 'alpha': 0.05, 'B': 300, 'R': 400},\n        # Case 2: Laplace(0, 1)\n        {'name': 'Laplace', 'dist': dist_laplace, 'median': 0.0, 'n': 40, 'alpha': 0.05, 'B': 300, 'R': 400},\n        # Case 3: Exponential(1), small n\n        {'name': 'Exponential_40', 'dist': dist_exponential, 'median': np.log(2), 'n': 40, 'alpha': 0.05, 'B': 300, 'R': 400},\n        # Case 4: Exponential(1), large n\n        {'name': 'Exponential_120', 'dist': dist_exponential, 'median': np.log(2), 'n': 120, 'alpha': 0.05, 'B': 200, 'R': 300},\n    ]\n\n    results = []\n    for case in test_cases:\n        coverage = run_simulation(\n            dist_func=case['dist'],\n            true_median=case['median'],\n            n=case['n'],\n            alpha=case['alpha'],\n            B=case['B'],\n            R=case['R'],\n            local_rng=rng\n        )\n        results.append(coverage)\n\n    # Format the results as specified: rounded to exactly three decimal places.\n    formatted_results = [f\"{res:.3f}\" for res in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4954794"}]}