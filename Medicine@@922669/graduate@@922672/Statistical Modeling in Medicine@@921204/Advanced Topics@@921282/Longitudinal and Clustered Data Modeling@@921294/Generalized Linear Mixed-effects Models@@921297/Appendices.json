{"hands_on_practices": [{"introduction": "To understand how Generalized Linear Mixed-effects Models are estimated, we must first construct the objective function that statistical software aims to maximize: the marginal log-likelihood. This foundational exercise [@problem_id:4965252] guides you through deriving the log-likelihood contribution from a single cluster by integrating out the unobserved random effect. Mastering this derivation is key to understanding why GLMMs typically lack closed-form solutions and require specialized numerical methods for estimation.", "problem": "In a multi-center clinical study of postoperative surgical-site infection, the binary outcome infection status for patient $j$ in hospital (cluster) $i$ is recorded as $y_{ij} \\in \\{0,1\\}$. The study investigators model infection risk using a Generalized Linear Mixed-Effects Model (GLMM) with a logistic link and a hospital-specific random intercept to capture latent heterogeneity across hospitals. Let $x_{ij} \\in \\mathbb{R}^{p}$ denote the patient-level covariates (e.g., age, operative urgency, comorbidity indices), and let $\\beta \\in \\mathbb{R}^{p}$ denote the fixed-effect coefficients. The linear predictor for patient $j$ in hospital $i$ is given by $\\eta_{ij} = x_{ij}^{\\top}\\beta$, and the hospital-specific random intercept is $b_{i} \\sim \\mathcal{N}(0,\\sigma_{b}^{2})$ with $\\sigma_{b}^{2} > 0$. Conditional on $b_{i}$, the outcomes $\\{y_{ij}\\}_{j=1}^{m_{i}}$ are independent Bernoulli with success probability $\\pi_{ij}(b_{i}) = \\mathrm{logit}^{-1}(\\eta_{ij} + b_{i}) = \\frac{1}{1+\\exp(-\\eta_{ij}-b_{i})}$.\n\nStarting from fundamental definitions of the Bernoulli probability mass function, conditional independence given random effects, and the law of total probability for marginalization over latent variables, derive the contribution to the marginal log-likelihood from a single hospital $i$ as an explicit one-dimensional integral with respect to $b_{i}$. Express your final answer as a closed-form analytic expression for the hospital-specific log-likelihood contribution $\\ell_{i}(\\beta,\\sigma_{b}^{2};y_{i},X_{i})$, where $y_{i} = (y_{i1},\\dots,y_{im_{i}})$ and $X_{i} = (x_{i1},\\dots,x_{im_{i}})$. Your final expression must make the need for numerical methods apparent by exhibiting a one-dimensional integral that cannot be simplified to elementary functions. No numerical approximation is required. Provide the final answer as a single symbolic expression; do not include any units.", "solution": "The problem requires the derivation of the marginal log-likelihood contribution for a single hospital (cluster) in a Generalized Linear Mixed-Effects Model (GLMM). The process begins with the fundamental definition of the model components and proceeds by applying principles of probability theory to integrate out the latent random effect.\n\nThe model specification for a patient $j$ in hospital $i$ is as follows:\nThe outcome $y_{ij} \\in \\{0, 1\\}$ is a binary variable.\nThe linear predictor for the fixed effects is $\\eta_{ij} = x_{ij}^{\\top}\\beta$.\nThe hospital-specific random intercept is $b_i \\sim \\mathcal{N}(0, \\sigma_b^2)$.\nConditional on $b_i$, the outcome $y_{ij}$ follows a Bernoulli distribution, $y_{ij} | b_i \\sim \\mathrm{Bernoulli}(\\pi_{ij}(b_i))$.\nThe conditional probability of infection (success, $y_{ij}=1$) is given by the inverse-logit function:\n$$\n\\pi_{ij}(b_i) = \\mathrm{logit}^{-1}(\\eta_{ij} + b_i) = \\frac{\\exp(\\eta_{ij} + b_i)}{1 + \\exp(\\eta_{ij} + b_i)} = \\frac{1}{1 + \\exp(-\\eta_{ij} - b_i)}\n$$\n\nThe derivation proceeds in three main steps:\n$1$. Formulate the conditional likelihood for all patients within a single hospital $i$, given the random effect $b_i$.\n$2$. Obtain the marginal likelihood for hospital $i$ by integrating the conditional likelihood over the distribution of the random effect $b_i$.\n$3$. Take the natural logarithm of the marginal likelihood to find the log-likelihood contribution $\\ell_i$.\n\n**Step 1: Conditional Likelihood for Hospital $i$**\n\nThe probability mass function (PMF) for a single Bernoulli observation $y_{ij}$ conditional on $b_i$ is given by:\n$$\nP(Y_{ij} = y_{ij} | b_i; \\beta) = \\pi_{ij}(b_i)^{y_{ij}} (1 - \\pi_{ij}(b_i))^{1 - y_{ij}}\n$$\nwhere $y_{ij} \\in \\{0, 1\\}$. A more compact and computationally convenient form for this PMF is:\n$$\nP(Y_{ij} = y_{ij} | b_i; \\beta) = \\frac{\\exp(y_{ij}(\\eta_{ij} + b_i))}{1 + \\exp(\\eta_{ij} + b_i)}\n$$\nThe problem states that, conditional on $b_i$, the outcomes $\\{y_{ij}\\}_{j=1}^{m_i}$ for all $m_i$ patients in hospital $i$ are independent. Therefore, the conditional likelihood for the vector of outcomes $y_i = (y_{i1}, \\dots, y_{im_i})$ for hospital $i$, given $b_i$, is the product of the individual PMFs:\n$$\nL(y_i | b_i; \\beta) = P(Y_{i1}=y_{i1}, \\dots, Y_{im_i}=y_{im_i} | b_i; \\beta) = \\prod_{j=1}^{m_i} P(Y_{ij}=y_{ij} | b_i; \\beta)\n$$\nSubstituting the compact form of the PMF, we have:\n$$\nL(y_i | b_i; \\beta) = \\prod_{j=1}^{m_i} \\frac{\\exp(y_{ij}(\\eta_{ij} + b_i))}{1 + \\exp(\\eta_{ij} + b_i)}\n$$\nThis expression represents the joint probability of observing the specific outcomes in hospital $i$ if the value of the hospital's latent characteristic $b_i$ were known.\n\n**Step 2: Marginal Likelihood for Hospital $i$**\n\nThe random effect $b_i$ is an unobserved latent variable. To obtain the marginal likelihood of the data for hospital $i$, we must average the conditional likelihood over all possible values of $b_i$, weighted by the probability of each value occurring. This is achieved by integrating the conditional likelihood with respect to the probability distribution of $b_i$. This procedure is an application of the law of total probability for continuous random variables.\n\nThe random intercept $b_i$ is distributed according to a normal distribution with mean $0$ and variance $\\sigma_b^2$, denoted $b_i \\sim \\mathcal{N}(0, \\sigma_b^2)$. The probability density function (PDF) for $b_i$ is:\n$$\nf(b_i; \\sigma_b^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_b^2}} \\exp\\left(-\\frac{b_i^2}{2\\sigma_b^2}\\right)\n$$\nThe marginal likelihood for hospital $i$, denoted $L_i(\\beta, \\sigma_b^2; y_i, X_i)$, is the integral of the product of the conditional likelihood and the PDF of the random effect over the entire support of $b_i$, which is $(-\\infty, \\infty)$:\n$$\nL_i(\\beta, \\sigma_b^2; y_i, X_i) = \\int_{-\\infty}^{\\infty} L(y_i | b_i; \\beta) f(b_i; \\sigma_b^2) \\, db_i\n$$\nSubstituting the expressions for $L(y_i | b_i; \\beta)$ and $f(b_i; \\sigma_b^2)$, and recalling that $\\eta_{ij} = x_{ij}^{\\top}\\beta$:\n$$\nL_i(\\beta, \\sigma_b^2; y_i, X_i) = \\int_{-\\infty}^{\\infty} \\left( \\prod_{j=1}^{m_i} \\frac{\\exp(y_{ij}(x_{ij}^{\\top}\\beta + b_i))}{1 + \\exp(x_{ij}^{\\top}\\beta + b_i)} \\right) \\frac{1}{\\sqrt{2\\pi}\\sigma_b} \\exp\\left(-\\frac{b_i^2}{2\\sigma_b^2}\\right) \\, db_i\n$$\n\n**Step 3: Marginal Log-Likelihood Contribution**\n\nThe final step is to take the natural logarithm of the marginal likelihood to obtain the log-likelihood contribution from hospital $i$, denoted $\\ell_i(\\beta, \\sigma_b^2; y_i, X_i)$. The total log-likelihood for the entire dataset would be the sum of these contributions over all hospitals, $\\ell = \\sum_i \\ell_i$.\n$$\n\\ell_i(\\beta, \\sigma_b^2; y_i, X_i) = \\ln \\left( L_i(\\beta, \\sigma_b^2; y_i, X_i) \\right)\n$$\nSubstituting the integral expression for $L_i$:\n$$\n\\ell_i(\\beta, \\sigma_b^2; y_i, X_i) = \\ln \\left( \\int_{-\\infty}^{\\infty} \\left( \\prod_{j=1}^{m_i} \\frac{\\exp(y_{ij}(x_{ij}^{\\top}\\beta + b_i))}{1 + \\exp(x_{ij}^{\\top}\\beta + b_i)} \\right) \\frac{1}{\\sqrt{2\\pi}\\sigma_b} \\exp\\left(-\\frac{b_i^2}{2\\sigma_b^2}\\right) \\, db_i \\right)\n$$\nThis expression represents the complete and exact analytical form of the log-likelihood contribution from a single cluster. The integrand is a product of a series of logistic functions and a Gaussian density function. There is no general closed-form analytical solution for this integral in terms of elementary functions. The presence of this intractable integral makes it explicit that numerical methods, such as adaptive Gaussian quadrature or Markov Chain Monte Carlo (MCMC) techniques, are required to evaluate the likelihood and consequently to estimate the model parameters $\\beta$ and $\\sigma_b^2$.", "answer": "$$\n\\boxed{\\ln \\left( \\int_{-\\infty}^{\\infty} \\left( \\prod_{j=1}^{m_i} \\frac{\\exp(y_{ij}(x_{ij}^{\\top}\\beta + b_i))}{1 + \\exp(x_{ij}^{\\top}\\beta + b_i)} \\right) \\frac{1}{\\sqrt{2\\pi}\\sigma_b} \\exp\\left(-\\frac{b_i^2}{2\\sigma_b^2}\\right) \\, db_i \\right)}\n$$", "id": "4965252"}, {"introduction": "Once a GLMM is fitted, we obtain an estimate for the random-effect variance, $\\sigma_{b}^{2}$, but this parameter can be difficult to interpret on its own. This practice [@problem_id:4965259] bridges the gap between a statistical parameter and an intuitive concept by deriving the Intraclass Correlation Coefficient (ICC) for a logistic GLMM. This exercise will show you how to quantify the degree of within-cluster correlation, translating the abstract variance component into a meaningful measure of dependency.", "problem": "A nationwide observational study evaluates binary remission outcomes for patients with chronic autoimmune disease treated in $J$ hospital wards. Patients are indexed by $i=1,\\dots,n_{j}$ within ward $j=1,\\dots,J$. Investigators fit a Generalized Linear Mixed-Effects Model (GLMM) with a logit link and a ward-specific random intercept to account for clustering. Specifically, patient outcomes $Y_{ij} \\in \\{0,1\\}$ satisfy $Y_{ij} \\sim \\text{Bernoulli}(p_{ij})$, and the linear predictor on the logit scale is $\\log\\!\\left(\\frac{p_{ij}}{1-p_{ij}}\\right)=\\mathbf{x}_{ij}^{\\top}\\boldsymbol{\\beta}+b_{j}$, where $b_{j}$ is a ward-level random intercept satisfying $b_{j} \\sim \\mathcal{N}(0,\\sigma_{b}^{2})$, independent across wards.\n\nUnder the latent-variable approach consistent with the logit link, one posits an underlying continuous propensity $Y^{\\ast}_{ij}=\\mathbf{x}_{ij}^{\\top}\\boldsymbol{\\beta}+b_{j}+\\varepsilon_{ij}$, where $\\varepsilon_{ij}$ has a standard logistic distribution with variance $\\frac{\\pi^{2}}{3}$, independent of $b_{j}$. The intraclass correlation coefficient (ICC) is defined as the correlation, on the latent propensity scale, between two exchangeable patients from the same ward.\n\nStarting from variance decomposition principles and the latent-variable representation above, derive a closed-form analytical expression for the intraclass correlation coefficient (ICC) as a function of the between-ward variance parameter $\\sigma_{b}^{2}$, treating the latent residual variance as $\\frac{\\pi^{2}}{3}$. Express your final answer as a single simplified expression in terms of $\\sigma_{b}^{2}$ and $\\pi$. No numerical approximation is required.", "solution": "The problem asks for the derivation of the intraclass correlation coefficient (ICC) on the scale of the latent propensity variable, $Y^{\\ast}$. The ICC is defined as the correlation between the latent propensities of two distinct, exchangeable patients, indexed by $i$ and $k$ (where $i \\neq k$), from the same ward $j$.\nMathematically, this is expressed as:\n$$ \\text{ICC} = \\text{Corr}(Y^{\\ast}_{ij}, Y^{\\ast}_{kj}) $$\nThe general formula for the correlation between two random variables $A$ and $B$ is:\n$$ \\text{Corr}(A, B) = \\frac{\\text{Cov}(A, B)}{\\sqrt{\\text{Var}(A)\\text{Var}(B)}} $$\nThe latent variable model for the two patients is:\n$$ Y^{\\ast}_{ij} = \\mathbf{x}_{ij}^{\\top}\\boldsymbol{\\beta} + b_{j} + \\varepsilon_{ij} $$\n$$ Y^{\\ast}_{kj} = \\mathbf{x}_{kj}^{\\top}\\boldsymbol{\\beta} + b_{j} + \\varepsilon_{kj} $$\nNotice that both patients are in the same ward $j$ and thus share the same random intercept $b_{j}$.\n\nFirst, we determine the total variance of a single latent propensity, $\\text{Var}(Y^{\\ast}_{ij})$. The total variance is the variance of the sum of the random components of the model. The term $\\mathbf{x}_{ij}^{\\top}\\boldsymbol{\\beta}$ is the fixed part and does not contribute to the variance. The random components are the ward-level random intercept $b_{j}$ and the patient-level residual error $\\varepsilon_{ij}$.\n$$ \\text{Var}(Y^{\\ast}_{ij}) = \\text{Var}(\\mathbf{x}_{ij}^{\\top}\\boldsymbol{\\beta} + b_{j} + \\varepsilon_{ij}) = \\text{Var}(b_{j} + \\varepsilon_{ij}) $$\nAccording to the problem statement, $b_{j}$ and $\\varepsilon_{ij}$ are independent. Therefore, the variance of their sum is the sum of their variances:\n$$ \\text{Var}(Y^{\\ast}_{ij}) = \\text{Var}(b_{j}) + \\text{Var}(\\varepsilon_{ij}) $$\nWe are given the variances of these components:\n- The between-ward variance: $\\text{Var}(b_{j}) = \\sigma_{b}^{2}$\n- The within-ward (residual) variance from the standard logistic distribution: $\\text{Var}(\\varepsilon_{ij}) = \\frac{\\pi^{2}}{3}$\nThus, the total variance of the latent propensity for any patient is:\n$$ \\text{Var}(Y^{\\ast}_{ij}) = \\sigma_{b}^{2} + \\frac{\\pi^{2}}{3} $$\nBecause the patients are exchangeable, $\\text{Var}(Y^{\\ast}_{kj}) = \\text{Var}(Y^{\\ast}_{ij})$.\n\nNext, we determine the covariance between the latent propensities of the two patients, $\\text{Cov}(Y^{\\ast}_{ij}, Y^{\\ast}_{kj})$.\n$$ \\text{Cov}(Y^{\\ast}_{ij}, Y^{\\ast}_{kj}) = \\text{Cov}(\\mathbf{x}_{ij}^{\\top}\\boldsymbol{\\beta} + b_{j} + \\varepsilon_{ij}, \\mathbf{x}_{kj}^{\\top}\\boldsymbol{\\beta} + b_{j} + \\varepsilon_{kj}) $$\nUsing the property of bilinearity for covariance and noting that fixed terms do not contribute:\n$$ \\text{Cov}(Y^{\\ast}_{ij}, Y^{\\ast}_{kj}) = \\text{Cov}(b_{j} + \\varepsilon_{ij}, b_{j} + \\varepsilon_{kj}) = \\text{Cov}(b_{j}, b_{j}) + \\text{Cov}(b_{j}, \\varepsilon_{kj}) + \\text{Cov}(\\varepsilon_{ij}, b_{j}) + \\text{Cov}(\\varepsilon_{ij}, \\varepsilon_{kj}) $$\nWe evaluate each covariance term based on the model's independence assumptions:\n- $\\text{Cov}(b_{j}, b_{j}) = \\text{Var}(b_{j}) = \\sigma_{b}^{2}$.\n- $b_{j}$ is independent of the residual terms, so $\\text{Cov}(b_{j}, \\varepsilon_{kj}) = 0$ and $\\text{Cov}(\\varepsilon_{ij}, b_{j}) = 0$.\n- The residual terms for two different patients are independent, even within the same ward ($i \\neq k$), so $\\text{Cov}(\\varepsilon_{ij}, \\varepsilon_{kj}) = 0$.\nSubstituting these results back into the covariance expression:\n$$ \\text{Cov}(Y^{\\ast}_{ij}, Y^{\\ast}_{kj}) = \\sigma_{b}^{2} + 0 + 0 + 0 = \\sigma_{b}^{2} $$\nThe covariance between the latent propensities of two patients in the same ward is simply the between-ward variance component, $\\sigma_{b}^{2}$. This is because the shared random intercept $b_{j}$ is the only source of correlation.\n\nFinally, we assemble the ICC by substituting the expressions for the covariance and the total variance into the correlation formula:\n$$ \\text{ICC} = \\frac{\\text{Cov}(Y^{\\ast}_{ij}, Y^{\\ast}_{kj})}{\\text{Var}(Y^{\\ast}_{ij})} = \\frac{\\sigma_{b}^{2}}{\\sigma_{b}^{2} + \\frac{\\pi^{2}}{3}} $$\nThis expression represents the proportion of the total variance in the latent propensity that is attributable to the variation between wards.", "answer": "$$\n\\boxed{\\frac{\\sigma_{b}^{2}}{\\sigma_{b}^{2} + \\frac{\\pi^{2}}{3}}}\n$$", "id": "4965259"}, {"introduction": "A primary goal of statistical modeling is to make predictions for new individuals. In a GLMM, this is more complex than in a standard GLM because we must account for the distribution of the random effects. This hands-on coding exercise [@problem_id:4965333] challenges you to compute a marginal (population-average) predicted probability by numerically integrating over the random-effect distribution. By implementing the Gauss-Hermite quadrature, you will gain practical insight into the computational machinery that powers prediction in GLMMs.", "problem": "Consider binary outcomes in a clinical risk model where the probability of an event for patient covariates $x \\in \\mathbb{R}^p$ is modeled by a Generalized Linear Mixed-Effects Model (GLMM) with a random intercept. The conditional distribution of the outcome given a random intercept is Bernoulli, and the canonical logit link is used. The random intercept represents unobserved cluster-level heterogeneity and follows a normal distribution. Using the law of total probability and the definition of the logistic link, the predicted probability for a new patient must be computed by integrating over the random intercept. Your task is to construct a program that carries out this computation numerically.\n\nFundamental base for the derivation:\n- The conditional model is Bernoulli with probability specified by a logistic link.\n- The random intercept is normally distributed.\n- The marginal prediction is the conditional prediction averaged over the distribution of the random intercept.\n\nDefine the model components precisely:\n- Let $y \\in \\{0,1\\}$ be the binary outcome.\n- Let $x \\in \\mathbb{R}^p$ be a covariate vector including an intercept term (i.e., the first element of $x$ is $1$).\n- Let $\\beta \\in \\mathbb{R}^p$ be the fixed-effect coefficient vector.\n- Let $b \\in \\mathbb{R}$ be a random intercept, distributed as a normal random variable with mean $0$ and variance $\\sigma^2$.\n- The conditional event probability given $b$ is specified by the logistic link as a function of $x^\\top \\beta + b$.\n\nYour program must:\n- Compute the predicted marginal event probability for each provided test case by numerically evaluating the integral over all possible values of the random intercept $b$.\n- Use a numerically stable strategy rooted in well-tested rules of numerical integration that is appropriate for functions involving the normal distribution and logistic link.\n- For each test case, produce the predicted probability as a decimal number.\n\nThe test suite to evaluate the program comprises a fixed-effect vector and four distinct patient covariate scenarios with varying random intercept standard deviations:\n- Fixed-effect coefficient vector: $\\beta = [-1.5, 0.3, -0.8, 0.6]$.\n- Test case $1$: $x = [1, 0.0, 0.0, 0.0]$, $\\sigma = 0.0$.\n- Test case $2$: $x = [1, 2.0, -1.5, 0.8]$, $\\sigma = 0.2$.\n- Test case $3$: $x = [1, -3.0, 2.5, -0.5]$, $\\sigma = 1.5$.\n- Test case $4$: $x = [1, 15.0, 8.0, 6.0]$, $\\sigma = 2.0$.\n\nDesign for coverage:\n- Test case $1$ is a boundary condition where the random intercept variance is $0$, so no random effect heterogeneity is present.\n- Test case $2$ is a scenario with small heterogeneity where the marginal prediction should be close to the conditional prediction.\n- Test case $3$ is a scenario with substantial heterogeneity and a strongly negative linear predictor.\n- Test case $4$ is an extreme scenario with a very large positive linear predictor and large heterogeneity, challenging numerical stability.\n\nAnswer specification:\n- Your program must produce a single line of output containing the predicted probabilities for the four test cases as a comma-separated list enclosed in square brackets.\n- Each probability must be rounded to $6$ decimal places.\n- The output must be in the format $[p_1,p_2,p_3,p_4]$ where $p_i$ are decimal numbers.\n\nNo physical units are involved. Probabilities must be expressed as decimals. The program must be complete and runnable without external inputs. It must only use the specified Python, NumPy, and SciPy environments.", "solution": "The problem requires the computation of the marginal probability of an event in a Generalized Linear Mixed-Effects Model (GLMM). The model specifies a binary outcome $y \\in \\{0, 1\\}$, a vector of fixed-effect covariates $x \\in \\mathbb{R}^p$, and a random intercept $b \\in \\mathbb{R}$.\n\nThe model's components are defined as follows:\n1. The conditional probability of an event ($y=1$) given the covariates $x$ and the random intercept $b$ is modeled using a logistic link function:\n$$P(y=1 | x, b) = \\text{logit}^{-1}(x^\\top \\beta + b) = \\frac{1}{1 + e^{-(x^\\top \\beta + b)}}$$\nHere, $\\beta \\in \\mathbb{R}^p$ is the vector of fixed-effect coefficients.\n\n2. The random intercept $b$, which captures unobserved heterogeneity, is assumed to follow a normal distribution with mean $0$ and variance $\\sigma^2$:\n$$b \\sim \\mathcal{N}(0, \\sigma^2)$$\nThe probability density function (PDF) of $b$ is given by:\n$$f(b; \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{b^2}{2\\sigma^2}}$$\n\nTo find the marginal probability of an event for a new observation with covariates $x$, we must integrate (or \"average out\") the conditional probability over all possible values of the random intercept $b$, weighted by its probability density. This is an application of the law of total probability:\n$$P(y=1 | x) = \\int_{-\\infty}^{\\infty} P(y=1 | x, b) f(b; \\sigma^2) db$$\nSubstituting the specific forms for the logistic function and the normal PDF, we get:\n$$P(y=1 | x) = \\int_{-\\infty}^{\\infty} \\frac{1}{1 + e^{-(x^\\top \\beta + b)}} \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{b^2}{2\\sigma^2}} db$$\n\nThis integral does not have a closed-form analytical solution and must be approximated numerically. A highly effective and appropriate method for this type of integral, which involves a normal distribution kernel, is Gauss-Hermite quadrature. This method is designed to approximate integrals of the form $\\int_{-\\infty}^{\\infty} g(t) e^{-t^2} dt$.\n\nTo apply Gauss-Hermite quadrature, we must transform our integral into the required form via a change of variables. Let the fixed linear predictor be $\\eta = x^\\top \\beta$. Our integral is $\\int_{-\\infty}^{\\infty} \\frac{1}{1 + e^{-(\\eta + b)}} f(b; \\sigma^2) db$. We introduce a new variable $t$ such that $b = \\sqrt{2}\\sigma t$. This implies $db = \\sqrt{2}\\sigma dt$. The integration limits remain $(-\\infty, \\infty)$. Substituting this into the integral gives:\n$$P(y=1 | x) = \\int_{-\\infty}^{\\infty} \\frac{1}{1 + e^{-(\\eta + \\sqrt{2}\\sigma t)}} \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(\\sqrt{2}\\sigma t)^2}{2\\sigma^2}} \\cdot (\\sqrt{2}\\sigma dt)$$\nSimplifying the expression within the integral:\n$$P(y=1 | x) = \\int_{-\\infty}^{\\infty} \\frac{1}{1 + e^{-(\\eta + \\sqrt{2}\\sigma t)}} \\cdot \\frac{1}{\\sqrt{\\pi}} \\frac{1}{\\sqrt{2}\\sigma} e^{-t^2} \\cdot (\\sqrt{2}\\sigma dt)$$\n$$P(y=1 | x) = \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^{\\infty} \\underbrace{\\frac{1}{1 + e^{-(\\eta + \\sqrt{2}\\sigma t)}}}_{g(t)} e^{-t^2} dt$$\nThis integral is now in the standard form for Gauss-Hermite quadrature. The $N$-point quadrature rule approximates the integral as a weighted sum:\n$$\\int_{-\\infty}^{\\infty} g(t) e^{-t^2} dt \\approx \\sum_{k=1}^{N} w_k g(t_k)$$\nwhere $t_k$ are the roots (nodes) of the $N$-th degree Hermite polynomial $H_N(t)$, and $w_k$ are the corresponding weights. The final approximation for the marginal probability is:\n$$P(y=1 | x) \\approx \\frac{1}{\\sqrt{\\pi}} \\sum_{k=1}^{N} w_k \\frac{1}{1+e^{-(\\eta + \\sqrt{2}\\sigma t_k)}}$$\nThe nodes and weights can be obtained from standard numerical libraries.\n\nA special case arises when $\\sigma = 0$. In this scenario, the random intercept $b$ is a constant equal to its mean, $0$. There is no random variation, so the model simplifies to a standard logistic regression model. The integral is no longer necessary, and the probability is simply the conditional probability evaluated at $b=0$:\n$$P(y=1 | x) = \\frac{1}{1 + e^{-x^\\top \\beta}}$$\nThis case corresponds to Test Case 1 and is handled separately for both conceptual correctness and to avoid numerical issues.\n\nFor the implementation, we will use a sufficient number of quadrature points (e.g., $N=32$) to ensure high accuracy across all test cases, including those with large $\\sigma$.", "answer": "```python\nimport numpy as np\nfrom scipy.special import expit\n\ndef compute_marginal_probability(x, beta, sigma, n_points=32):\n    \"\"\"\n    Computes the marginal event probability for a GLMM with a random intercept.\n\n    Args:\n        x (np.ndarray): Covariate vector for a single patient.\n        beta (np.ndarray): Fixed-effect coefficient vector.\n        sigma (float): Standard deviation of the random intercept.\n        n_points (int): Number of quadrature points for Gauss-Hermite integration.\n\n    Returns:\n        float: The computed marginal probability.\n    \"\"\"\n    # Calculate the fixed part of the linear predictor\n    eta = np.dot(x, beta)\n\n    # Handle the special case where there is no random effect.\n    # The model reduces to a standard logistic regression.\n    if sigma == 0.0:\n        return expit(eta)\n\n    # For sigma > 0, use Gauss-Hermite quadrature to integrate out the random effect.\n    # The method approximates integral[g(t) * exp(-t^2), -inf, inf].\n    # We performed a change of variables: b = sqrt(2)*sigma*t.\n    # The integral becomes (1/sqrt(pi)) * integral[g(t) * exp(-t^2), -inf, inf],\n    # where g(t) is the logistic function of the full linear predictor.\n    \n    # Get Gauss-Hermite quadrature nodes (roots) and weights.\n    nodes, weights = np.polynomial.hermite.hermgauss(n_points)\n\n    # Calculate the full linear predictor at each quadrature node.\n    # The random effect 'b' is evaluated at sqrt(2) * sigma * node.\n    full_linear_predictor = eta + np.sqrt(2.0) * sigma * nodes\n\n    # Evaluate the conditional probability (g(t_k) in the formula) at each node\n    # using the numerically stable logistic sigmoid function (expit).\n    prob_at_nodes = expit(full_linear_predictor)\n\n    # The integral is approximated by the weighted sum of probabilities.\n    integral_approximation = np.sum(weights * prob_at_nodes)\n\n    # Scale the result according to the change of variables formula.\n    marginal_prob = (1.0 / np.sqrt(np.pi)) * integral_approximation\n\n    return marginal_prob\n\ndef solve():\n    \"\"\"\n    Executes the problem by calculating marginal probabilities for all test cases.\n    \"\"\"\n\n    # Fixed-effect coefficient vector from the problem statement.\n    beta = np.array([-1.5, 0.3, -0.8, 0.6])\n\n    # Test cases comprising covariate vectors (x) and random effect std. dev. (sigma).\n    test_cases = [\n        (np.array([1.0, 0.0, 0.0, 0.0]), 0.0),   # Case 1: No random effect\n        (np.array([1.0, 2.0, -1.5, 0.8]), 0.2), # Case 2: Small heterogeneity\n        (np.array([1.0, -3.0, 2.5, -0.5]), 1.5),# Case 3: Substantial heterogeneity\n        (np.array([1.0, 15.0, 8.0, 6.0]), 2.0),  # Case 4: Extreme predictor, large heterogeneity\n    ]\n\n    results = []\n    for x_patient, sigma_patient in test_cases:\n        # Compute the probability for the current test case.\n        prob = compute_marginal_probability(x_patient, beta, sigma_patient)\n        \n        # Format the result to 6 decimal places.\n        results.append(f\"{prob:.6f}\")\n\n    # Print the final output in the required format.\n    print(f\"[{','.join(results)}]\")\n\n# Run the solution.\nsolve()\n```", "id": "4965333"}]}