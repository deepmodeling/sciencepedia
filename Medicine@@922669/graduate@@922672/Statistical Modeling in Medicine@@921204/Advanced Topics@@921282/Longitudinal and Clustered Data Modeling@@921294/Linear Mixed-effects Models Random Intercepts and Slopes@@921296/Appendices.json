{"hands_on_practices": [{"introduction": "This exercise goes back to first principles to reveal the mathematical heart of linear mixed-effects models. By deriving the marginal expectation, variance, and covariance, you will gain a deep understanding of how random effects model the unique structure of longitudinal data without biasing the overall population trend. This foundational work is crucial for correctly interpreting model parameters and understanding how individual heterogeneity is partitioned from residual error. [@problem_id:4970090]", "problem": "A multicenter longitudinal study of an antihypertensive therapy records repeated systolic blood pressure measurements $y_{ij}$ for patient $i$ at visit $j$, together with a time-varying covariate $x_{ij}$ representing the time since treatment initiation in months. Assume the repeated measurements within a patient follow a Linear Mixed-Effects (LME) model with a random intercept and a random slope:\n$$\ny_{ij} \\;=\\; \\beta_{0} \\;+\\; \\beta_{1} x_{ij} \\;+\\; b_{0i} \\;+\\; b_{1i} x_{ij} \\;+\\; \\varepsilon_{ij},\n$$\nwhere the random effects vector $b_{i} = \\begin{pmatrix} b_{0i} \\\\ b_{1i} \\end{pmatrix}$ is distributed as a mean-zero bivariate normal $b_{i} \\sim \\mathcal{N}\\!\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\Sigma_{b}\\right)$ with a positive-definite covariance matrix\n$$\n\\Sigma_{b} \\;=\\; \\begin{pmatrix} \\sigma_{b0}^{2}  \\sigma_{b01} \\\\ \\sigma_{b01}  \\sigma_{b1}^{2} \\end{pmatrix},\n$$\nand the measurement errors $\\varepsilon_{ij} \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^{2})$ are independent across $j$, independent of $b_{i}$, and have mean zero. Treat $x_{ij}$ as observed and nonrandom (or condition on $x_{ij}$).\n\nStarting only from the definition of conditional and marginal expectations, the law of iterated expectations, and the properties of variance and covariance under independence, perform the following:\n\n- Derive the closed-form expression for the marginal expectation $\\operatorname{E}(y_{ij} \\mid x_{ij})$ by integrating out the random effects $b_{i}$ and the measurement error $\\varepsilon_{ij}$.\n- Using the law of total variance and the law of total covariance, show in symbolic form how the random intercept and slope affect the marginal variance $\\operatorname{Var}(y_{ij} \\mid x_{ij})$ and the within-patient marginal covariance $\\operatorname{Cov}(y_{ij}, y_{ik} \\mid x_{ij}, x_{ik})$ for $j \\neq k$, and explain why they do not alter the marginal expectation.\n\nProvide your final reported answer as the single closed-form analytic expression for $\\operatorname{E}(y_{ij} \\mid x_{ij})$. No rounding is required and no units should be included in the final reported expression.", "solution": "### Step 1: Extract Givens\nThe problem provides the following model and definitions:\n- The Linear Mixed-Effects (LME) model for repeated measurements $y_{ij}$ for patient $i$ at visit $j$:\n$$y_{ij} = \\beta_{0} + \\beta_{1} x_{ij} + b_{0i} + b_{1i} x_{ij} + \\varepsilon_{ij}$$\n- $x_{ij}$ is a time-varying covariate, treated as nonrandom.\n- $\\beta_{0}$ and $\\beta_{1}$ are fixed-effect parameters.\n- The random effects vector for patient $i$ is $b_{i} = \\begin{pmatrix} b_{0i} \\\\ b_{1i} \\end{pmatrix}$.\n- The distribution of the random effects is a mean-zero bivariate normal:\n$b_{i} \\sim \\mathcal{N}\\!\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\Sigma_{b}\\right)$\n- The covariance matrix of the random effects is $\\Sigma_{b} = \\begin{pmatrix} \\sigma_{b0}^{2}  \\sigma_{b01} \\\\ \\sigma_{b01}  \\sigma_{b1}^{2} \\end{pmatrix}$, which is positive-definite.\n- The measurement errors are $\\varepsilon_{ij} \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^{2})$.\n- The measurement errors $\\varepsilon_{ij}$ are independent across $j$ and independent of the random effects $b_{i}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n- **Scientifically Grounded**: The problem describes a standard random intercept and random slope model, a cornerstone of statistical modeling for longitudinal data in medicine and other STEM fields. All definitions and assumptions are standard and consistent with established statistical theory.\n- **Well-Posed**: The problem asks for the derivation of fundamental properties (expectation, variance, covariance) of the specified model. The setup is complete and internally consistent, allowing for unique and meaningful solutions to be derived from first principles.\n- **Objective**: The language is precise, mathematical, and free of any subjective or ambiguous terminology.\nThe problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\n### Derivation of Marginal Expectation, Variance, and Covariance\n\nThe task is to derive the marginal expectation $\\operatorname{E}(y_{ij} \\mid x_{ij})$, and to show how the random effects influence the marginal variance $\\operatorname{Var}(y_{ij} \\mid x_{ij})$ and within-patient marginal covariance $\\operatorname{Cov}(y_{ij}, y_{ik} \\mid x_{ij}, x_{ik})$ for $j \\neq k$.\n\n#### Marginal Expectation $\\operatorname{E}(y_{ij} \\mid x_{ij})$\nWe derive the marginal expectation by integrating out the random components, namely the random effects $b_i = (b_{0i}, b_{1i})^\\top$ and the measurement error $\\varepsilon_{ij}$. We use the law of iterated expectations, conditioning on the random effects vector $b_i$. The covariate $x_{ij}$ is treated as a fixed quantity.\n\nThe law of iterated expectations states that for random variables $Y$ and $Z$, $\\operatorname{E}[Y] = \\operatorname{E}[\\operatorname{E}[Y \\mid Z]]$. Here, our outcome is $y_{ij}$ and we will condition on $b_i$.\n\n$$\n\\operatorname{E}(y_{ij} \\mid x_{ij}) = \\operatorname{E}_{b_i} \\left[ \\operatorname{E}_{\\varepsilon_{ij}} [ y_{ij} \\mid b_i, x_{ij} ] \\right]\n$$\n\nFirst, we compute the inner expectation, conditional on the random effects $b_i$:\n$$\n\\operatorname{E}_{\\varepsilon_{ij}} [ y_{ij} \\mid b_i, x_{ij} ] = \\operatorname{E}_{\\varepsilon_{ij}} [ \\beta_{0} + \\beta_{1} x_{ij} + b_{0i} + b_{1i} x_{ij} + \\varepsilon_{ij} \\mid b_i, x_{ij} ]\n$$\nGiven $b_i$, the terms $\\beta_{0}$, $\\beta_{1} x_{ij}$, $b_{0i}$, and $b_{1i} x_{ij}$ are all constants. Using the linearity of expectation:\n$$\n\\operatorname{E}_{\\varepsilon_{ij}} [ y_{ij} \\mid b_i, x_{ij} ] = \\beta_{0} + \\beta_{1} x_{ij} + b_{0i} + b_{1i} x_{ij} + \\operatorname{E}[\\varepsilon_{ij}]\n$$\nFrom the problem statement, $\\varepsilon_{ij} \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^{2})$, so $\\operatorname{E}[\\varepsilon_{ij}] = 0$. This gives:\n$$\n\\operatorname{E}_{\\varepsilon_{ij}} [ y_{ij} \\mid b_i, x_{ij} ] = \\beta_{0} + \\beta_{1} x_{ij} + b_{0i} + b_{1i} x_{ij}\n$$\nThis expression represents the expected value of $y_{ij}$ for a specific patient $i$ with known random effects $b_{0i}$ and $b_{1i}$.\n\nNext, we compute the outer expectation, which averages over the distribution of the random effects $b_i$:\n$$\n\\operatorname{E}(y_{ij} \\mid x_{ij}) = \\operatorname{E}_{b_i} \\left[ \\beta_{0} + \\beta_{1} x_{ij} + b_{0i} + b_{1i} x_{ij} \\right]\n$$\nAgain, by linearity of expectation:\n$$\n\\operatorname{E}(y_{ij} \\mid x_{ij}) = \\beta_{0} + \\beta_{1} x_{ij} + \\operatorname{E}[b_{0i}] + \\operatorname{E}[b_{1i} x_{ij}] = \\beta_{0} + \\beta_{1} x_{ij} + \\operatorname{E}[b_{0i}] + x_{ij}\\operatorname{E}[b_{1i}]\n$$\nFrom the problem statement, $b_i \\sim \\mathcal{N}(0, \\Sigma_b)$, which implies $\\operatorname{E}[b_{0i}] = 0$ and $\\operatorname{E}[b_{1i}] = 0$. Substituting these values:\n$$\n\\operatorname{E}(y_{ij} \\mid x_{ij}) = \\beta_{0} + \\beta_{1} x_{ij} + 0 + x_{ij}(0) = \\beta_{0} + \\beta_{1} x_{ij}\n$$\n\n#### Marginal Variance $\\operatorname{Var}(y_{ij} \\mid x_{ij})$\nWe use the law of total variance: $\\operatorname{Var}(Y) = \\operatorname{E}[\\operatorname{Var}(Y \\mid Z)] + \\operatorname{Var}[\\operatorname{E}(Y \\mid Z)]$.\n$$\n\\operatorname{Var}(y_{ij} \\mid x_{ij}) = \\operatorname{E}_{b_i}[\\operatorname{Var}(y_{ij} \\mid b_i, x_{ij})] + \\operatorname{Var}_{b_i}[\\operatorname{E}(y_{ij} \\mid b_i, x_{ij})]\n$$\nThe two components are calculated as follows:\n1.  The inner conditional variance, $\\operatorname{Var}(y_{ij} \\mid b_i, x_{ij})$:\n    $$\n    \\operatorname{Var}(y_{ij} \\mid b_i, x_{ij}) = \\operatorname{Var}(\\beta_{0} + \\beta_{1} x_{ij} + b_{0i} + b_{1i} x_{ij} + \\varepsilon_{ij} \\mid b_i, x_{ij})\n    $$\n    Conditional on $b_i$, all terms are constant except for the measurement error $\\varepsilon_{ij}$.\n    $$\n    \\operatorname{Var}(y_{ij} \\mid b_i, x_{ij}) = \\operatorname{Var}(\\varepsilon_{ij}) = \\sigma_{\\varepsilon}^{2}\n    $$\n    Taking the expectation of this quantity over $b_i$ yields $\\operatorname{E}_{b_i}[\\sigma_{\\varepsilon}^{2}] = \\sigma_{\\varepsilon}^{2}$.\n\n2.  The variance of the conditional expectation, $\\operatorname{Var}_{b_i}[\\operatorname{E}(y_{ij} \\mid b_i, x_{ij})]$:\n    We use the previously derived expression for $\\operatorname{E}(y_{ij} \\mid b_i, x_{ij})$:\n    $$\n    \\operatorname{Var}_{b_i}[\\operatorname{E}(y_{ij} \\mid b_i, x_{ij})] = \\operatorname{Var}_{b_i}[\\beta_{0} + \\beta_{1} x_{ij} + b_{0i} + b_{1i} x_{ij}]\n    $$\n    The fixed-effect terms are constants and do not contribute to the variance.\n    $$\n    \\operatorname{Var}_{b_i}[b_{0i} + b_{1i} x_{ij}]\n    $$\n    Using the property $\\operatorname{Var}(aX + cY) = a^2\\operatorname{Var}(X) + c^2\\operatorname{Var}(Y) + 2ac\\operatorname{Cov}(X, Y)$ with $a=1$, $c=x_{ij}$, $X=b_{0i}$, and $Y=b_{1i}$:\n    $$\n    \\operatorname{Var}_{b_i}[b_{0i} + x_{ij}b_{1i}] = 1^2 \\operatorname{Var}(b_{0i}) + x_{ij}^2 \\operatorname{Var}(b_{1i}) + 2(1)(x_{ij})\\operatorname{Cov}(b_{0i}, b_{1i})\n    $$\n    From the covariance matrix $\\Sigma_b$, we have $\\operatorname{Var}(b_{0i}) = \\sigma_{b0}^2$, $\\operatorname{Var}(b_{1i}) = \\sigma_{b1}^2$, and $\\operatorname{Cov}(b_{0i}, b_{1i}) = \\sigma_{b01}$.\n    $$\n    \\operatorname{Var}_{b_i}[\\operatorname{E}(y_{ij} \\mid b_i, x_{ij})] = \\sigma_{b0}^2 + x_{ij}^2 \\sigma_{b1}^2 + 2x_{ij}\\sigma_{b01}\n    $$\n\nCombining the two components, the marginal variance is:\n$$\n\\operatorname{Var}(y_{ij} \\mid x_{ij}) = \\sigma_{\\varepsilon}^{2} + \\sigma_{b0}^{2} + 2x_{ij}\\sigma_{b01} + x_{ij}^{2}\\sigma_{b1}^{2}\n$$\n\n#### Marginal Covariance $\\operatorname{Cov}(y_{ij}, y_{ik} \\mid x_{ij}, x_{ik})$ for $j \\neq k$\nWe use the law of total covariance: $\\operatorname{Cov}(X, Y) = \\operatorname{E}[\\operatorname{Cov}(X, Y \\mid Z)] + \\operatorname{Cov}[\\operatorname{E}(X \\mid Z), \\operatorname{E}(Y \\mid Z)]$.\n$$\n\\operatorname{Cov}(y_{ij}, y_{ik} \\mid x_{ij}, x_{ik}) = \\operatorname{E}_{b_i}[\\operatorname{Cov}(y_{ij}, y_{ik} \\mid b_i, x_{ij}, x_{ik})] + \\operatorname{Cov}_{b_i}[\\operatorname{E}(y_{ij} \\mid b_i, x_{ij}), \\operatorname{E}(y_{ik} \\mid b_i, x_{ik})]\n$$\n1.  The inner conditional covariance:\n    $$\n    \\operatorname{Cov}(y_{ij}, y_{ik} \\mid b_i, x_{ij}, x_{ik}) = \\operatorname{Cov}(\\beta_{0} + \\dots + \\varepsilon_{ij}, \\beta_{0} + \\dots + \\varepsilon_{ik} \\mid b_i, \\dots)\n    $$\n    Given $b_i$, the only random components are $\\varepsilon_{ij}$ and $\\varepsilon_{ik}$. The problem states these are independent for $j \\neq k$. Thus, their covariance is zero.\n    $$\n    \\operatorname{Cov}(y_{ij}, y_{ik} \\mid b_i, x_{ij}, x_{ik}) = \\operatorname{Cov}(\\varepsilon_{ij}, \\varepsilon_{ik}) = 0\n    $$\n    The expectation of this is $\\operatorname{E}_{b_i}[0] = 0$.\n\n2.  The covariance of the conditional expectations:\n    $$\n    \\operatorname{Cov}_{b_i}[\\operatorname{E}(y_{ij} \\mid b_i, x_{ij}), \\operatorname{E}(y_{ik} \\mid b_i, x_{ik})] = \\operatorname{Cov}_{b_i}[\\beta_0 + \\beta_1 x_{ij} + b_{0i} + b_{1i}x_{ij}, \\beta_0 + \\beta_1 x_{ik} + b_{0i} + b_{1i}x_{ik}]\n    $$\n    The fixed terms drop out, leaving:\n    $$\n    \\operatorname{Cov}_{b_i}[b_{0i} + b_{1i}x_{ij}, b_{0i} + b_{1i}x_{ik}]\n    $$\n    Using the bilinearity of covariance:\n    $$\n    = \\operatorname{Cov}(b_{0i}, b_{0i}) + \\operatorname{Cov}(b_{0i}, b_{1i}x_{ik}) + \\operatorname{Cov}(b_{1i}x_{ij}, b_{0i}) + \\operatorname{Cov}(b_{1i}x_{ij}, b_{1i}x_{ik})\n    $$\n    $$\n    = \\operatorname{Var}(b_{0i}) + x_{ik}\\operatorname{Cov}(b_{0i}, b_{1i}) + x_{ij}\\operatorname{Cov}(b_{1i}, b_{0i}) + x_{ij}x_{ik}\\operatorname{Var}(b_{1i})\n    $$\n    $$\n    = \\sigma_{b0}^2 + x_{ik}\\sigma_{b01} + x_{ij}\\sigma_{b01} + x_{ij}x_{ik}\\sigma_{b1}^2 = \\sigma_{b0}^{2} + (x_{ij} + x_{ik})\\sigma_{b01} + x_{ij}x_{ik}\\sigma_{b1}^{2}\n    $$\n\nCombining the two components, the marginal covariance is:\n$$\n\\operatorname{Cov}(y_{ij}, y_{ik} \\mid x_{ij}, x_{ik}) = \\sigma_{b0}^{2} + (x_{ij} + x_{ik})\\sigma_{b01} + x_{ij}x_{ik}\\sigma_{b1}^{2}\n$$\n\n### Explanation of the Role of Random Effects\n\n- **Marginal Expectation**: The random effects $b_{0i}$ and $b_{1i}$ do not alter the marginal expectation because they are defined to have a mean of zero. When averaging over the entire population of patients, these patient-specific deviations from the mean trajectory cancel out. The marginal expectation $\\operatorname{E}(y_{ij} \\mid x_{ij}) = \\beta_0 + \\beta_1 x_{ij}$ thus describes the average trajectory for the entire population, determined solely by the fixed effects.\n\n- **Marginal Variance and Covariance**: The random effects are crucial for describing the variability and correlation structure of the data.\n  - **Variance**: The random effects introduce an additional source of variability between subjects. The term $\\sigma_{b0}^{2} + 2x_{ij}\\sigma_{b01} + x_{ij}^{2}\\sigma_{b1}^{2}$ in the marginal variance expression accounts for the fact that each patient has their own unique intercept and slope, which deviate from the population average. This between-subject variance is added to the within-subject measurement error variance $\\sigma_{\\varepsilon}^2$. The marginal variance is thus heteroscedastic, depending on the covariate $x_{ij}$.\n  - **Covariance**: Repeated measurements $y_{ij}$ and $y_{ik}$ from the same patient $i$ are not independent. They are correlated because they share the same patient-specific random effects $b_{0i}$ and $b_{1i}$. This shared influence induces a positive covariance between observations within the same patient, captured by the derived expression $\\operatorname{Cov}(y_{ij}, y_{ik})$. This within-patient correlation is a key feature of longitudinal data that LME models are designed to capture. Without random effects, this covariance would be zero (due to independent errors), which is a completely unrealistic assumption for such data.", "answer": "$$\n\\boxed{\\beta_{0} + \\beta_{1} x_{ij}}\n$$", "id": "4970090"}, {"introduction": "A fitted model is only as good as its assumptions, and residuals are our primary tool for checking them. This practice explores the crucial distinction between conditional and marginal residuals, demonstrating how each type provides a unique window into different aspects of the model's performance. By calculating and interpreting both, you will learn to diagnose assumptions about the within-patient error distribution versus misspecification of the fixed-effects mean structure. [@problem_id:4970134]", "problem": "A longitudinal cohort study in rheumatology tracks a continuous biomarker outcome for each patient over repeated visits. Suppose the analysis uses a Linear Mixed-Effects (LME) model with random intercepts and random slopes for time. For patient-level index $i$ and visit index $j$, the model is\n$$\ny_{ij} \\;=\\; \\beta_{0} \\;+\\; \\beta_{1}\\, t_{ij} \\;+\\; u_{0i} \\;+\\; u_{1i}\\, t_{ij} \\;+\\; \\varepsilon_{ij},\n$$\nwhere $u_{0i}$ and $u_{1i}$ are the patient-specific random intercept and random slope, respectively, and $\\varepsilon_{ij}$ is the within-patient error. Assume $(u_{0i},u_{1i})^{\\top}$ has mean $\\mathbf{0}$ and finite covariance matrix, and $\\varepsilon_{ij}$ are independent across $i$ and $j$ with mean $0$ and finite variance, independent of the random effects. The marginal mean function is therefore $E[y_{ij}\\mid t_{ij}] = \\beta_{0} + \\beta_{1} t_{ij}$, and the conditional mean given the random effects is $E[y_{ij}\\mid t_{ij},u_{0i},u_{1i}] = \\beta_{0} + \\beta_{1} t_{ij} + u_{0i} + u_{1i} t_{ij}$.\n\nYou are given the fitted fixed effects estimates $\\hat{\\beta}_{0} = 1.74$ and $\\hat{\\beta}_{1} = 0.58$. For a particular patient $i^{\\ast}$, the empirical best linear unbiased predictors (also called empirical Bayes estimates) of the random effects are $\\hat{u}_{0\\,i^{\\ast}} = -0.47$ and $\\hat{u}_{1\\,i^{\\ast}} = 0.13$. At a specific visit $j^{\\ast}$ for that patient, the observed time is $t_{i^{\\ast} j^{\\ast}} = 2.50$ and the observed outcome is $y_{i^{\\ast} j^{\\ast}} = 4.81$.\n\nUsing only the model definition and these estimates:\n1. Compute the conditional fitted value $\\hat{y}_{i^{\\ast} j^{\\ast}}^{\\text{cond}}$ and the marginal fitted value $\\hat{y}_{i^{\\ast} j^{\\ast}}^{\\text{marg}}$.\n2. Compute the conditional residual $r_{i^{\\ast} j^{\\ast}} = y_{i^{\\ast} j^{\\ast}} - \\hat{y}_{i^{\\ast} j^{\\ast}}^{\\text{cond}}$ and the marginal residual $e_{i^{\\ast} j^{\\ast}} = y_{i^{\\ast} j^{\\ast}} - \\hat{y}_{i^{\\ast} j^{\\ast}}^{\\text{marg}}$.\n3. From first principles, explain which of the two residuals is most appropriate for diagnosing assumptions about the within-patient error distribution and variance structure, and which is more appropriate for diagnosing misspecification of the fixed-effects mean structure.\n\nReport the single scalar $r_{i^{\\ast} j^{\\ast}} - e_{i^{\\ast} j^{\\ast}}$ rounded to four significant figures. Do not include units in your final report.", "solution": "We begin from the Linear Mixed-Effects (LME) model definition:\n$$\ny_{ij} \\;=\\; \\beta_{0} \\;+\\; \\beta_{1}\\, t_{ij} \\;+\\; u_{0i} \\;+\\; u_{1i}\\, t_{ij} \\;+\\; \\varepsilon_{ij},\n$$\nwith $(u_{0i},u_{1i})^{\\top}$ centered at $\\mathbf{0}$ and $\\varepsilon_{ij}$ centered at $0$, independent of the random effects. It follows that the marginal mean function is\n$$\nE[y_{ij}\\mid t_{ij}] \\;=\\; \\beta_{0} \\;+\\; \\beta_{1}\\, t_{ij},\n$$\nand the conditional mean given the random effects is\n$$\nE[y_{ij}\\mid t_{ij},u_{0i},u_{1i}] \\;=\\; \\beta_{0} \\;+\\; \\beta_{1}\\, t_{ij} \\;+\\; u_{0i} \\;+\\; u_{1i}\\, t_{ij}.\n$$\nIn matrix notation, with design vectors $x_{ij} = (1,\\, t_{ij})^{\\top}$ for fixed effects and $z_{ij} = (1,\\, t_{ij})^{\\top}$ for random effects, the marginal fitted value uses only the fixed effects,\n$$\n\\hat{y}_{ij}^{\\text{marg}} \\;=\\; x_{ij}^{\\top}\\,\\hat{\\beta} \\;=\\; \\hat{\\beta}_{0} \\;+\\; \\hat{\\beta}_{1}\\, t_{ij},\n$$\nand the conditional fitted value additionally incorporates the empirical best linear unbiased predictors for that subject,\n$$\n\\hat{y}_{ij}^{\\text{cond}} \\;=\\; x_{ij}^{\\top}\\,\\hat{\\beta} \\;+\\; z_{ij}^{\\top}\\,\\hat{u}_{i} \\;=\\; \\hat{\\beta}_{0} \\;+\\; \\hat{\\beta}_{1}\\, t_{ij} \\;+\\; \\hat{u}_{0i} \\;+\\; \\hat{u}_{1i}\\, t_{ij}.\n$$\n\nWe apply these to the given numerical values for patient $i^{\\ast}$ and visit $j^{\\ast}$:\n- $\\hat{\\beta}_{0} = 1.74$,\n- $\\hat{\\beta}_{1} = 0.58$,\n- $\\hat{u}_{0\\,i^{\\ast}} = -0.47$,\n- $\\hat{u}_{1\\,i^{\\ast}} = 0.13$,\n- $t_{i^{\\ast} j^{\\ast}} = 2.50$,\n- $y_{i^{\\ast} j^{\\ast}} = 4.81$.\n\nFirst, compute the marginal fitted value:\n$$\n\\hat{y}_{i^{\\ast} j^{\\ast}}^{\\text{marg}} \\;=\\; \\hat{\\beta}_{0} \\;+\\; \\hat{\\beta}_{1}\\, t_{i^{\\ast} j^{\\ast}} \\;=\\; 1.74 \\;+\\; 0.58 \\times 2.50 \\;=\\; 1.74 \\;+\\; 1.45 \\;=\\; 3.19.\n$$\nNext, compute the conditional fitted value:\n$$\n\\hat{y}_{i^{\\ast} j^{\\ast}}^{\\text{cond}} \\;=\\; \\hat{\\beta}_{0} \\;+\\; \\hat{\\beta}_{1}\\, t_{i^{\\ast} j^{\\ast}} \\;+\\; \\hat{u}_{0\\,i^{\\ast}} \\;+\\; \\hat{u}_{1\\,i^{\\ast}}\\, t_{i^{\\ast} j^{\\ast}} \\;=\\; 3.19 \\;+\\; \\big(-0.47 \\;+\\; 0.13 \\times 2.50\\big).\n$$\nCompute the random-effect contribution:\n$$\n-0.47 \\;+\\; 0.13 \\times 2.50 \\;=\\; -0.47 \\;+\\; 0.325 \\;=\\; -0.145,\n$$\nhence\n$$\n\\hat{y}_{i^{\\ast} j^{\\ast}}^{\\text{cond}} \\;=\\; 3.19 \\;-\\; 0.145 \\;=\\; 3.045.\n$$\n\nNow compute the residuals:\n- Conditional residual:\n$$\nr_{i^{\\ast} j^{\\ast}} \\;=\\; y_{i^{\\ast} j^{\\ast}} \\;-\\; \\hat{y}_{i^{\\ast} j^{\\ast}}^{\\text{cond}} \\;=\\; 4.81 \\;-\\; 3.045 \\;=\\; 1.765.\n$$\n- Marginal residual:\n$$\ne_{i^{\\ast} j^{\\ast}} \\;=\\; y_{i^{\\ast} j^{\\ast}} \\;-\\; \\hat{y}_{i^{\\ast} j^{\\ast}}^{\\text{marg}} \\;=\\; 4.81 \\;-\\; 3.19 \\;=\\; 1.62.\n$$\n\nThe requested scalar difference is\n$$\nr_{i^{\\ast} j^{\\ast}} \\;-\\; e_{i^{\\ast} j^{\\ast}} \\;=\\; 1.765 \\;-\\; 1.62 \\;=\\; 0.145.\n$$\nEquivalently, by algebra,\n$$\nr_{ij} - e_{ij} \\;=\\; \\big(y_{ij} - \\hat{y}_{ij}^{\\text{cond}}\\big) \\;-\\; \\big(y_{ij} - \\hat{y}_{ij}^{\\text{marg}}\\big) \\;=\\; \\hat{y}_{ij}^{\\text{marg}} - \\hat{y}_{ij}^{\\text{cond}} \\;=\\; -\\,z_{ij}^{\\top}\\,\\hat{u}_{i},\n$$\nwhich agrees numerically with $0.145$ because $z_{i^{\\ast} j^{\\ast}}^{\\top}\\,\\hat{u}_{i^{\\ast}} = \\hat{u}_{0\\,i^{\\ast}} + \\hat{u}_{1\\,i^{\\ast}} t_{i^{\\ast} j^{\\ast}} = -0.145$.\n\nInterpretation for diagnostics from first principles:\n- Conditional residuals $r_{ij}$ are estimates of the within-patient errors $\\varepsilon_{ij}$ because $\\hat{y}_{ij}^{\\text{cond}}$ conditions on the subject-specific random effects. Therefore, plots and tests based on $r_{ij}$ are appropriate for diagnosing distributional assumptions and variance structure of the level-$1$ errors, such as normality of $\\varepsilon_{ij}$, homoscedasticity over time or covariates, and independence of errors within subject after accounting for the random effects.\n- Marginal residuals $e_{ij}$ compare observations to the population-mean trajectory $E[y_{ij}\\mid t_{ij}]$, hence they conflate random-effect variation and error. They are more sensitive to misspecification of the fixed-effects mean structure (for example, omitted nonlinear terms in $t_{ij}$ or omitted fixed covariates), because any systematic lack of fit in the fixed part will manifest as structured patterns in $e_{ij}$ across covariates or time. However, because $e_{ij}$ includes between-subject variation via $u_{i}$, it is not appropriate for diagnosing the distribution of $\\varepsilon_{ij}$.\n\nFinally, rounding $r_{i^{\\ast} j^{\\ast}} - e_{i^{\\ast} j^{\\ast}} = 0.145$ to four significant figures yields $0.1450$.", "answer": "$$\\boxed{0.1450}$$", "id": "4970134"}, {"introduction": "Deciding on the complexity of a mixed-effects model—for instance, whether to include a random slope—is a critical modeling step. This exercise guides you through the formal process using a Likelihood Ratio Test, highlighting a subtle but vital point: testing a variance component requires a special non-standard null distribution because the test is performed on the boundary of the parameter space. Mastering this concept is key to robust model selection and avoiding incorrect conclusions about the sources of variability in your data. [@problem_id:4970104]", "problem": "A longitudinal study in cardiology examines systolic blood pressure trajectories during antihypertensive therapy. Patients $i=1,\\dots,n$ are measured at visit times $t_{ij}$, $j=1,\\dots,m_i$. A linear mixed-effects model with Gaussian errors is posited:\n$$\ny_{ij} \\;=\\; \\beta_0 \\;+\\; \\beta_1 t_{ij} \\;+\\; b_{0i} \\;+\\; b_{1i} t_{ij} \\;+\\; \\varepsilon_{ij},\n$$\nwhere $(b_{0i}, b_{1i})^\\top \\sim \\mathcal{N}\\!\\left( \\mathbf{0}, \\mathbf{D} \\right)$ with\n$$\n\\mathbf{D} \\;=\\; \\begin{pmatrix} \\tau_0^2  \\tau_{01} \\\\ \\tau_{01}  \\tau_1^2 \\end{pmatrix},\n$$\nand $\\varepsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^2)$ independently of $(b_{0i}, b_{1i})$. The time covariate $t_{ij}$ is centered to improve numerical stability. To assess whether a patient-specific slope is needed, consider testing the variance component via the one-sided hypothesis\n$$\nH_0: \\;\\tau_1^2 = 0 \\quad \\text{versus} \\quad H_1: \\;\\tau_1^2  0,\n$$\nwith the random intercept variance $\\tau_0^2$ and the random intercept–slope covariance $\\tau_{01}$ unconstrained under $H_1$. Note that positive semidefiniteness of $\\mathbf{D}$ implies $\\tau_{01}=0$ whenever $\\tau_1^2=0$.\n\nSuppose the model is fit by Restricted Maximum Likelihood (REML), yielding the maximized restricted log-likelihoods $\\ell^{(0)} = -1240.6$ under $H_0$ and $\\ell^{(1)} = -1236.7$ under $H_1$.\n\nWhich option correctly constructs the Likelihood Ratio Test (LRT) statistic for this variance-component test, specifies the appropriate asymptotic null distribution that accounts for boundary constraints, and gives the resulting approximate $p$-value and conclusion at significance level $\\alpha=0.05$?\n\nA. Use the REML-based LRT statistic $2(\\ell^{(1)}-\\ell^{(0)})$ with null distribution a $50:50$ mixture of $\\chi^2$ with $0$ and $1$ degrees of freedom, i.e., $\\tfrac{1}{2}\\chi^2_{0} + \\tfrac{1}{2}\\chi^2_{1}$. For the given values, the $p$-value is approximately $0.0027$, so reject $H_0$.\n\nB. Use the REML-based LRT statistic $2(\\ell^{(1)}-\\ell^{(0)})$ with null distribution a $50:50$ mixture of $\\chi^2$ with $1$ and $2$ degrees of freedom, i.e., $\\tfrac{1}{2}\\chi^2_{1} + \\tfrac{1}{2}\\chi^2_{2}$. For the given values, the $p$-value is approximately $0.0127$, so reject $H_0$.\n\nC. Use the REML-based LRT statistic $2(\\ell^{(1)}-\\ell^{(0)})$ with standard null distribution $\\chi^2$ with $1$ degree of freedom. For the given values, the $p$-value is approximately $0.0053$, so reject $H_0$.\n\nD. Use the REML-based LRT statistic $2(\\ell^{(1)}-\\ell^{(0)})$ with standard null distribution $\\chi^2$ with $2$ degrees of freedom. For the given values, the $p$-value is approximately $0.0202$, so reject $H_0$.", "solution": "The user wants me to validate and solve a problem concerning a hypothesis test for a variance component in a linear mixed-effects model.\n\n### Step 1: Extract Givens\n\n-   **Model:** A linear mixed-effects model is specified for the systolic blood pressure $y_{ij}$ of patient $i$ at visit time $t_{ij}$:\n    $$\n    y_{ij} \\;=\\; \\beta_0 \\;+\\; \\beta_1 t_{ij} \\;+\\; b_{0i} \\;+\\; b_{1i} t_{ij} \\;+\\; \\varepsilon_{ij}\n    $$\n-   **Random Effects Distribution:** The patient-specific random intercept $b_{0i}$ and random slope $b_{1i}$ follow a bivariate normal distribution:\n    $$\n    (b_{0i}, b_{1i})^\\top \\sim \\mathcal{N}\\!\\left( \\mathbf{0}, \\mathbf{D} \\right), \\quad \\text{with} \\quad \\mathbf{D} \\;=\\; \\begin{pmatrix} \\tau_0^2  \\tau_{01} \\\\ \\tau_{01}  \\tau_1^2 \\end{pmatrix}\n    $$\n-   **Error Distribution:** The within-subject errors $\\varepsilon_{ij}$ are independent and identically distributed as $\\mathcal{N}(0,\\sigma^2)$, and are independent of the random effects.\n-   **Hypothesis Test:** The hypothesis test is one-sided for the random slope variance:\n    $$\n    H_0: \\;\\tau_1^2 = 0 \\quad \\text{versus} \\quad H_1: \\;\\tau_1^2  0\n    $$\n-   **Parameter Constraints:** Under $H_1$, the random intercept variance $\\tau_0^2$ and the covariance $\\tau_{01}$ are unconstrained (subject to $\\mathbf{D}$ being positive semidefinite). The problem notes that under $H_0$, the positive semidefiniteness of $\\mathbf{D}$ implies that $\\tau_{01}$ must also be $0$.\n-   **Estimation and Data:** The model is fitted using Restricted Maximum Likelihood (REML). The maximized restricted log-likelihood under the null hypothesis ($H_0$) is $\\ell^{(0)} = -1240.6$. The maximized restricted log-likelihood under the alternative hypothesis ($H_1$) is $\\ell^{(1)} = -1236.7$.\n-   **Significance Level:** $\\alpha = 0.05$.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded:** The problem describes a standard application of linear mixed-effects models to analyze longitudinal data (systolic blood pressure trajectories), a canonical use case in biostatistics and medical research. The specified model, a random intercept and random slope model, is fundamental. The hypothesis test for the necessity of a random slope variance component is a critical step in model building. The use of REML is a standard and often preferred method for estimating variance components. The note about $\\tau_{01}=0$ when $\\tau_1^2=0$ is a correct mathematical consequence of the properties of covariance matrices. The problem is scientifically and mathematically sound.\n-   **Well-Posed:** The problem is well-defined. It provides all necessary information (model structure, hypotheses, log-likelihood values) to construct the test statistic, determine its null distribution, compute the p-value, and draw a conclusion.\n-   **Objective:** The language is formal, precise, and free of any subjective or ambiguous terminology.\n-   **Completeness and Consistency:** The problem is self-contained. The provided data ($\\ell^{(0)}$, $\\ell^{(1)}$) are sufficient for the required calculations. There are no contradictions in the setup.\n-   **Other Checks:** The problem is not trivial; it assesses a subtle but crucial point in statistical theory—hypothesis testing on the boundary of the parameter space. It is not ill-posed, unrealistic, or un-verifiable.\n\n### Step 3: Verdict and Action\n\n-   **Verdict:** The problem is **valid**.\n-   **Action:** Proceed with solving the problem.\n\n### Solution Derivation\n\nThe problem asks to perform a Likelihood Ratio Test (LRT) for a variance component, specifically to determine if a patient-specific random slope is necessary in the model.\n\n1.  **Construct the Test Statistic:**\n    The LRT statistic, based on REML estimates, is given by twice the difference in the maximized log-likelihoods of the alternative and null models:\n    $$\n    T = 2(\\ell^{(1)} - \\ell^{(0)})\n    $$\n    Using the provided values:\n    $$\n    T = 2\\left( (-1236.7) - (-1240.6) \\right) = 2(3.9) = 7.8\n    $$\n\n2.  **Determine the Asymptotic Null Distribution:**\n    The null hypothesis is $H_0: \\tau_1^2 = 0$. Since variances cannot be negative, this value lies on the boundary of the parameter space for $\\tau_1^2 \\in [0, \\infty)$. Therefore, the standard asymptotic theory for LRTs (Wilks' theorem), which assumes the null value is in the interior of the parameter space, does not apply. In such boundary cases, the asymptotic distribution of the LRT statistic is a mixture of chi-squared ($\\chi^2$) distributions, often denoted as a chi-bar-squared ($\\bar{\\chi}^2$) distribution.\n\n    The specific mixture depends on the number of parameters constrained to the boundary. Under $H_0$, we set $\\tau_1^2=0$. As correctly stated in the problem, for the covariance matrix $\\mathbf{D}$ to remain positive semidefinite, if a diagonal element is zero, the entire corresponding row and column must also be zero. Thus, $\\tau_1^2=0$ implies $\\tau_{01}=0$. The null model (random intercept only) is nested within the alternative model (random intercept and slope). The alternative model adds two parameters to the random part of the model: the variance $\\tau_1^2$ and the covariance $\\tau_{01}$.\n\n    The theoretical work by Self and Liang (1987) and Stram and Lee (1994, 1995) addresses this exact situation. For a test comparing a model with $p$ random effects to a model with an additional $q=1$ random effect (i.e., adding a random slope to a random intercept model, where $p=1$), the asymptotic null distribution of the LRT statistic (using either ML or REML likelihoods) is a $50:50$ mixture of a $\\chi^2_p$ and a $\\chi^2_{p+1}$ distribution. In our case, the null model has one random effect (the intercept), so $p=1$. The distribution is therefore:\n    $$\n    T \\sim \\frac{1}{2}\\chi^2_1 + \\frac{1}{2}\\chi^2_2\n    $$\n    This distribution reflects that, under the null, the data can either suggest a maximum likelihood estimate on the boundary ($\\hat{\\tau}_1^2=0$) with probability $0.5$, or in the interior ($\\hat{\\tau}_1^20$) with probability $0.5$. The degrees of freedom correspond to testing one parameter ($\\chi^2_1$) or two parameters ($\\chi^2_2$), respectively.\n\n3.  **Calculate the p-value:**\n    The p-value is the probability of observing a test statistic as extreme as or more extreme than the observed value $T=7.8$, under the null distribution.\n    $$\n    p\\text{-value} = P(T_{obs} \\ge 7.8) = \\frac{1}{2} P(\\chi^2_1 \\ge 7.8) + \\frac{1}{2} P(\\chi^2_2 \\ge 7.8)\n    $$\n    We find the probabilities from standard $\\chi^2$ distributions:\n    -   Using statistical software or tables for a $\\chi^2$ distribution with $1$ degree of freedom: $P(\\chi^2_1 \\ge 7.8) \\approx 0.00522$.\n    -   Using statistical software or tables for a $\\chi^2$ distribution with $2$ degrees of freedom: $P(\\chi^2_2 \\ge 7.8) \\approx 0.02024$.\n\n    Substituting these values into the p-value formula:\n    $$\n    p\\text{-value} \\approx \\frac{1}{2}(0.00522) + \\frac{1}{2}(0.02024) = 0.00261 + 0.01012 = 0.01273\n    $$\n    The approximate p-value is $0.0127$.\n\n4.  **Conclusion:**\n    The significance level is given as $\\alpha=0.05$. Since the calculated p-value ($0.0127$) is less than $\\alpha$, we reject the null hypothesis $H_0$. This indicates that there is statistically significant evidence to support the inclusion of a patient-specific random slope in the model. The variance component $\\tau_1^2$ is significantly greater than zero.\n\n### Option-by-Option Analysis\n\n-   **A. Use the REML-based LRT statistic $2(\\ell^{(1)}-\\ell^{(0)})$ with null distribution a $50:50$ mixture of $\\chi^2$ with $0$ and $1$ degrees of freedom, i.e., $\\tfrac{1}{2}\\chi^2_{0} + \\tfrac{1}{2}\\chi^2_{1}$. For the given values, the $p$-value is approximately $0.0027$, so reject $H_0$.**\n    -   **Verdict:** **Incorrect**. The test statistic calculation is correct. However, the null distribution $\\tfrac{1}{2}\\chi^2_{0} + \\tfrac{1}{2}\\chi^2_{1}$ is incorrect. This distribution applies when testing a single variance component in a model with no associated covariance parameters (e.g., testing $\\tau_0^2=0$ in a model with no other random effects, or testing $\\tau_1^2=0$ under the assumption that $\\tau_{01}$ is fixed at $0$). The problem involves testing for a random slope whose covariance with the random intercept is also estimated, which requires a different mixture distribution. While the p-value calculation is consistent with the stated incorrect distribution ($p = 0.5 \\times P(\\chi^2_1 \\ge 7.8) \\approx 0.0026$), the premise is flawed.\n\n-   **B. Use the REML-based LRT statistic $2(\\ell^{(1)}-\\ell^{(0)})$ with null distribution a $50:50$ mixture of $\\chi^2$ with $1$ and $2$ degrees of freedom, i.e., $\\tfrac{1}{2}\\chi^2_{1} + \\tfrac{1}{2}\\chi^2_{2}$. For the given values, the $p$-value is approximately $0.0127$, so reject $H_0$.**\n    -   **Verdict:** **Correct**. The test statistic $T=7.8$ is correct. The specified null distribution, $\\tfrac{1}{2}\\chi^2_{1} + \\tfrac{1}{2}\\chi^2_{2}$, is the correct asymptotic distribution for this specific variance component test, as established by Stram and Lee (1994). The calculated p-value, $\\approx 0.0127$, is correct for this statistic and distribution. The conclusion to reject $H_0$ is also correct as $0.0127  0.05$.\n\n-   **C. Use the REML-based LRT statistic $2(\\ell^{(1)}-\\ell^{(0)})$ with standard null distribution $\\chi^2$ with $1$ degree of freedom. For the given values, the $p$-value is approximately $0.0053$, so reject $H_0$.**\n    -   **Verdict:** **Incorrect**. This approach wrongly applies the standard Wilks' theorem, ignoring both the boundary nature of the hypothesis and the fact that two parameters ($\\tau_1^2$, $\\tau_{01}$) are simultaneously constrained to zero under $H_0$. A $\\chi^2_1$ distribution is not appropriate. The p-value calculation is consistent with this incorrect distribution ($p = P(\\chi^2_1 \\ge 7.8) \\approx 0.00522$), but the distribution itself is wrong.\n\n-   **D. Use the REML-based LRT statistic $2(\\ell^{(1)}-\\ell^{(0)})$ with standard null distribution $\\chi^2$ with $2$ degrees of freedom. For the given values, the $p$-value is approximately $0.0202$, so reject $H_0$.**\n    -   **Verdict:** **Incorrect**. This approach correctly counts the number of parameters fixed under the null hypothesis ($2$, namely $\\tau_1^2$ and $\\tau_{01}$) but incorrectly applies a standard $\\chi^2_2$ distribution. It fails to account for the fact that the null hypothesis lies on the boundary of the parameter space. The p-value calculation is consistent with this incorrect distribution ($p = P(\\chi^2_2 \\ge 7.8) \\approx 0.0202$), but the distribution is wrong.", "answer": "$$\\boxed{B}$$", "id": "4970104"}]}