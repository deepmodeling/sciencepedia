{"hands_on_practices": [{"introduction": "At the heart of joint modeling lies the ability to capture individual-specific trajectories using random effects. This first exercise provides a focused look at the longitudinal submodel to build intuition about Bayesian shrinkage—the mechanism by which random effects are estimated [@problem_id:4968602]. By deriving the posterior distribution of a random intercept, you will see how the model intelligently weighs information from an individual's measurements against the population's overall trend, a core principle for borrowing strength across subjects.", "problem": "In a joint model used in a medical study, the longitudinal submodel for a subject-specific biomarker is given by the random-intercept Gaussian model\n$$\ny_{ij} \\mid b_i \\sim \\mathcal{N}\\!\\left(\\mu + b_i,\\, \\sigma^{2}\\right), \\quad j = 1,\\dots,n_i,\n$$\nwith independent measurement errors across visits, and the subject-specific random intercept has a normal prior\n$$\nb_i \\sim \\mathcal{N}\\!\\left(0,\\, \\tau^{2}\\right).\n$$\nThe time-to-event submodel is a proportional hazards model with hazard\n$$\nh_i(t \\mid b_i) = h_0(t)\\,\\exp\\!\\left(\\alpha\\, b_i\\right).\n$$\nTo isolate the role of the longitudinal measurement error variance, suppose that the event-time contribution is conditionally uninformative for the random effect by taking $\\alpha = 0$, so that the likelihood factor from the event-time submodel does not involve $b_i$.\n\nStarting only from the definitions of the normal density and Bayes’s rule, derive the conditional posterior distribution of $b_i$ given the longitudinal measurements $\\{y_{ij}\\}_{j=1}^{n_i}$ and the hyperparameters $\\mu$, $\\sigma^{2}$, and $\\tau^{2}$. In particular, compute the posterior variance $\\operatorname{Var}(b_i \\mid y_{i1},\\dots,y_{in_i})$ in closed form as a function of $n_i$, $\\sigma^{2}$, and $\\tau^{2}$. Then, using the same derivation, identify how increasing $\\sigma^{2}$ affects the shrinkage of the posterior mean of $b_i$ toward zero, and explain why this occurs in terms of the posterior precision. Do not substitute numerical values. Your final reported answer must be the single closed-form analytic expression for the posterior variance $\\operatorname{Var}(b_i \\mid y_{i1},\\dots,y_{in_i})$. No rounding is required and no units are to be reported.", "solution": "The problem is valid as it presents a standard, well-posed problem in Bayesian statistical modeling, grounded in established probability theory. All necessary components (likelihood, prior, simplifying assumptions) are provided, and there are no scientific or logical contradictions. We will proceed with the derivation.\n\nThe objective is to derive the conditional posterior distribution of the random intercept $b_i$ for subject $i$, given the longitudinal measurements $\\{y_{ij}\\}_{j=1}^{n_i}$ and the hyperparameters $\\mu$, $\\sigma^{2}$, and $\\tau^{2}$. The problem specifies that the event-time submodel is uninformative for $b_i$ by setting the association parameter $\\alpha$ to $0$.\n\nAccording to Bayes’s rule, the posterior probability density function (PDF) for $b_i$ is proportional to the product of the likelihood of the data given $b_i$ and the prior PDF of $b_i$. We denote the set of measurements for subject $i$ as $Y_i = \\{y_{ij}\\}_{j=1}^{n_i}$.\n\n$$\np(b_i \\mid Y_i, \\mu, \\sigma^2, \\tau^2) \\propto p(Y_i \\mid b_i, \\mu, \\sigma^2) \\cdot p(b_i \\mid \\tau^2)\n$$\n\nThe hyperparameters are treated as fixed, known quantities. First, we define the likelihood function, $p(Y_i \\mid b_i, \\mu, \\sigma^2)$. The model states that, conditional on $b_i$, the measurements $y_{ij}$ are independent and identically distributed as $y_{ij} \\mid b_i \\sim \\mathcal{N}(\\mu + b_i, \\sigma^2)$. The PDF for a single measurement is:\n$$\np(y_{ij} \\mid b_i, \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_{ij} - (\\mu + b_i))^2}{2\\sigma^2}\\right)\n$$\nDue to independence, the likelihood for the set of $n_i$ measurements is the product of the individual PDFs:\n$$\np(Y_i \\mid b_i, \\mu, \\sigma^2) = \\prod_{j=1}^{n_i} p(y_{ij} \\mid b_i, \\mu, \\sigma^2) = (2\\pi\\sigma^2)^{-n_i/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{j=1}^{n_i} (y_{ij} - \\mu - b_i)^2 \\right)\n$$\n\nNext, we define the prior distribution for $b_i$, which is given as $b_i \\sim \\mathcal{N}(0, \\tau^2)$. The prior PDF is:\n$$\np(b_i \\mid \\tau^2) = \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left(-\\frac{b_i^2}{2\\tau^2}\\right)\n$$\n\nNow, we combine these to form the posterior, dropping any constant factors that do not depend on $b_i$:\n$$\np(b_i \\mid Y_i, \\dots) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{j=1}^{n_i} (y_{ij} - \\mu - b_i)^2 - \\frac{b_i^2}{2\\tau^2} \\right)\n$$\nTo identify the form of this distribution, we analyze the expression in the exponent, focusing on terms involving $b_i$. Let the exponent be denoted $Q(b_i)$:\n$$\nQ(b_i) = -\\frac{1}{2\\sigma^2} \\sum_{j=1}^{n_i} ( (y_{ij} - \\mu) - b_i )^2 - \\frac{b_i^2}{2\\tau^2}\n$$\nExpanding the squared term:\n$$\n\\sum_{j=1}^{n_i} ((y_{ij} - \\mu) - b_i)^2 = \\sum_{j=1}^{n_i} ( (y_{ij} - \\mu)^2 - 2b_i(y_{ij}-\\mu) + b_i^2 )\n$$\n$$\n= \\sum_{j=1}^{n_i} (y_{ij} - \\mu)^2 - 2b_i \\sum_{j=1}^{n_i} (y_{ij}-\\mu) + n_i b_i^2\n$$\nThe first term, $\\sum (y_{ij} - \\mu)^2$, does not depend on $b_i$ and can be absorbed into the proportionality constant. Let $\\bar{y}_i = \\frac{1}{n_i}\\sum_{j=1}^{n_i} y_{ij}$. Then $\\sum (y_{ij}-\\mu) = n_i(\\bar{y}_i - \\mu)$.\nSubstituting this back into $Q(b_i)$ and ignoring constant terms, we get:\n$$\nQ(b_i) \\propto -\\frac{1}{2\\sigma^2} (-2b_i n_i(\\bar{y}_i - \\mu) + n_i b_i^2) - \\frac{b_i^2}{2\\tau^2}\n$$\n$$\n= \\frac{n_i(\\bar{y}_i - \\mu)}{\\sigma^2} b_i - \\frac{n_i}{2\\sigma^2} b_i^2 - \\frac{1}{2\\tau^2} b_i^2\n$$\n$$\n= -\\frac{1}{2}\\left( \\left(\\frac{n_i}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)b_i^2 - 2\\frac{n_i(\\bar{y}_i - \\mu)}{\\sigma^2} b_i \\right)\n$$\nThe expression for the posterior PDF is of the form $p(b_i \\mid \\dots) \\propto \\exp(-\\frac{1}{2}(\\cdot))$. The quadratic form in $b_i$ indicates that the posterior distribution is also Gaussian. A general normal PDF for a variable $x$ with mean $\\mu_{\\text{post}}$ and variance $\\sigma^2_{\\text{post}}$ has a kernel proportional to $\\exp(-\\frac{(x-\\mu_{\\text{post}})^2}{2\\sigma^2_{\\text{post}}}) \\propto \\exp(-\\frac{1}{2}(\\frac{1}{\\sigma^2_{\\text{post}}}x^2 - \\frac{2\\mu_{\\text{post}}}{\\sigma^2_{\\text{post}}}x))$.\n\nBy matching coefficients with our expression for $Q(b_i)$, we can identify the parameters of the posterior distribution of $b_i$.\nThe coefficient of $b_i^2$ gives the posterior precision (inverse variance):\n$$\n\\frac{1}{\\sigma^2_{\\text{post}}} = \\frac{1}{\\operatorname{Var}(b_i \\mid Y_i)} = \\frac{n_i}{\\sigma^2} + \\frac{1}{\\tau^2}\n$$\nFrom this, we directly compute the posterior variance as its reciprocal:\n$$\n\\operatorname{Var}(b_i \\mid Y_i) = \\left(\\frac{n_i}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)^{-1} = \\frac{1}{\\frac{n_i\\tau^2 + \\sigma^2}{\\sigma^2\\tau^2}} = \\frac{\\sigma^2\\tau^2}{n_i\\tau^2 + \\sigma^2}\n$$\nThis provides the closed-form expression for the posterior variance as required.\n\nFor completeness and to address the second part of the question, we also derive the posterior mean. Matching the coefficient of the linear term in $b_i$:\n$$\n\\frac{\\mu_{\\text{post}}}{\\sigma^2_{\\text{post}}} = \\frac{n_i(\\bar{y}_i - \\mu)}{\\sigma^2}\n$$\n$$\n\\mathbb{E}[b_i \\mid Y_i] = \\mu_{\\text{post}} = \\sigma^2_{\\text{post}} \\left(\\frac{n_i(\\bar{y}_i - \\mu)}{\\sigma^2}\\right) = \\left(\\frac{\\sigma^2\\tau^2}{n_i\\tau^2 + \\sigma^2}\\right) \\left(\\frac{n_i(\\bar{y}_i - \\mu)}{\\sigma^2}\\right) = \\frac{n_i\\tau^2}{n_i\\tau^2 + \\sigma^2}(\\bar{y}_i - \\mu)\n$$\nThe posterior distribution is therefore $b_i \\mid Y_i \\sim \\mathcal{N}(\\mu_{\\text{post}}, \\sigma^2_{\\text{post}})$.\n\nNow we analyze the effect of increasing the measurement error variance, $\\sigma^2$, on the shrinkage of the posterior mean of $b_i$ toward its prior mean of zero. The posterior mean is:\n$$\n\\mathbb{E}[b_i \\mid Y_i] = \\left(\\frac{n_i\\tau^2}{n_i\\tau^2 + \\sigma^2}\\right)(\\bar{y}_i - \\mu)\n$$\nThe term $(\\bar{y}_i - \\mu)$ is an unbiased estimate of $b_i$ based only on the data for subject $i$. The term $S = \\frac{n_i\\tau^2}{n_i\\tau^2 + \\sigma^2}$ is a shrinkage factor, which is between $0$ and $1$. The posterior mean is a \"shrunken\" version of the data-based estimate, pulled towards the prior mean of $0$.\n\nTo see how increasing $\\sigma^2$ affects this shrinkage, we examine the derivative of $S$ with respect to $\\sigma^2$:\n$$\n\\frac{\\partial S}{\\partial(\\sigma^2)} = \\frac{\\partial}{\\partial(\\sigma^2)}\\left(\\frac{n_i\\tau^2}{n_i\\tau^2 + \\sigma^2}\\right) = -\\frac{n_i\\tau^2}{(n_i\\tau^2 + \\sigma^2)^2}\n$$\nSince $n_i > 0$ and $\\tau^2 > 0$, this derivative is strictly negative. This means that as $\\sigma^2$ increases, the shrinkage factor $S$ decreases. A smaller $S$ implies that the posterior mean $\\mathbb{E}[b_i \\mid Y_i]$ is pulled closer to $0$, which signifies stronger shrinkage.\n\nThis behavior is explained by considering the relative contributions of the data and the prior to the posterior precision. The posterior precision is the sum of the data precision and the prior precision:\n$$\n\\text{Posterior Precision} = \\underbrace{\\frac{n_i}{\\sigma^2}}_{\\text{Data Precision}} + \\underbrace{\\frac{1}{\\tau^2}}_{\\text{Prior Precision}}\n$$\nWhen $\\sigma^2$ increases, the data measurements become noisier and less reliable. This is reflected in the decrease of the data precision term, $n_i/\\sigma^2$. The Bayesian framework automatically accounts for this reduced information content from the data by giving more relative weight to the prior information. The posterior mean can be expressed as a precision-weighted average of the prior mean ($0$) and the data-based estimate ($\\bar{y}_i - \\mu$):\n$$\n\\mathbb{E}[b_i \\mid Y_i] = \\frac{n_i/\\sigma^2}{n_i/\\sigma^2 + 1/\\tau^2}(\\bar{y}_i - \\mu) + \\frac{1/\\tau^2}{n_i/\\sigma^2 + 1/\\tau^2}(0)\n$$\nAs $\\sigma^2$ increases, the weight on the data-based estimate, $\\frac{n_i/\\sigma^2}{n_i/\\sigma^2 + 1/\\tau^2}$, decreases, while the weight on the prior mean increases. Consequently, the posterior estimate of $b_i$ is shrunken more strongly toward the prior mean of $0$.", "answer": "$$\\boxed{\\frac{\\sigma^{2}\\tau^{2}}{n_i\\tau^{2} + \\sigma^{2}}}$$", "id": "4968602"}, {"introduction": "With an understanding of random effects, we now turn to the practical challenge of estimating the parameters of a full joint model. The Expectation-Maximization (EM) algorithm is a powerful engine for this task, and this hands-on coding practice guides you through a single, crucial iteration [@problem_id:4968553]. By implementing the E-step (estimating random effects) and M-step (updating fixed effects), you will gain a concrete understanding of how the model leverages both the longitudinal and survival data to refine its estimates.", "problem": "Consider a joint model for a medical longitudinal marker and time-to-event outcome with the following components, chosen to reflect widely used modeling strategies in biostatistics and epidemiology. The longitudinal submodel is a linear mixed-effects model: for subject $i$ measured at times $t_{ij}$ with responses $y_{ij}$,\n$$\ny_{ij} = x_{ij}^{\\top}\\beta + z_{ij}^{\\top} b_i + \\varepsilon_{ij}, \\quad \\varepsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^2), \\quad b_i \\sim \\mathcal{N}(0, D),\n$$\nwhere $x_{ij}$ and $z_{ij}$ are design vectors, $\\beta$ is the fixed-effect vector, $b_i$ is the subject-specific random-effect vector, $\\sigma^2$ is the residual variance, and $D$ is the random-effects covariance matrix. The time-to-event submodel adopts a proportional hazards formulation with a constant baseline hazard and a current-value association:\n$$\nh_i(t) = \\lambda_0 \\exp\\left(\\alpha \\, m_i(t)\\right), \\quad m_i(t) = x(t)^{\\top}\\beta + z(t)^{\\top} b_i,\n$$\nwhere $\\lambda_0$ is the baseline hazard, $\\alpha$ is the association parameter linking the marker to the hazard, and $m_i(t)$ is the current value of the longitudinal trajectory. Let $T_i$ denote the observed event or censoring time and $\\delta_i \\in \\{0,1\\}$ the event indicator (with $1$ meaning an observed event).\n\nStarting from the foundational definitions above and standard likelihood theory, the posterior distribution of $b_i$ given $(y_i, T_i, \\delta_i)$ is proportional to the product of the longitudinal likelihood, the survival likelihood, and the random-effects prior. In an Expectation-Maximization (EM) algorithm, the E-step requires computing the conditional expectation $E[b_i \\mid y_i, T_i, \\delta_i]$. A Laplace approximation evaluates this expectation by locating the mode of the log-posterior and using a local quadratic approximation. The M-step then updates $\\beta$ via weighted least squares (weighted by $1/\\sigma^2$) using the conditional expectations of $b_i$.\n\nImplement a single EM iteration according to the following specification:\n\n1. For each subject $i$, compute $E[b_i \\mid y_i, T_i, \\delta_i]$ using a Laplace approximation centered at the mode of the log-posterior of $b_i$, obtained by Newton's method with a numerically approximated Hessian of the log-posterior. The log-posterior is the sum of:\n   - The longitudinal log-likelihood under a normal model with residual variance $\\sigma^2$.\n   - The survival log-likelihood for constant baseline hazard with current-value association:\n     $$\n     \\ell_{S,i}(b_i) = \\delta_i \\left(\\log \\lambda_0 + \\alpha \\, m_i(T_i)\\right) - \\int_0^{T_i} \\lambda_0 \\exp\\left(\\alpha \\, m_i(u)\\right) \\, du,\n     $$\n     with $m_i(u) = x(u)^{\\top}\\beta + z(u)^{\\top} b_i$ and $x(u) = z(u)$ defined below.\n   - The Gaussian prior log-density for $b_i$ with covariance $D$.\n\n2. Update $\\beta$ via weighted least squares using weights $w_{ij} = 1/\\sigma^2$, treating $E[b_i \\mid y_i, T_i, \\delta_i]$ as known offsets in the mixed model.\n\n3. In addition, compute the \"tilt\" induced by the survival information compared to a pure linear mixed-effects (LME) analysis. Define the LME posterior mean $E_{\\text{LME}}[b_i \\mid y_i]$ as the Gaussian posterior mean under the longitudinal model alone. Define the tilt for subject $i$ as the difference in current value at $T_i$ implied by the two posterior means:\n   $$\n   \\Delta m_i(T_i) = \\left[1, \\, T_i\\right]^{\\top} \\left(E[b_i \\mid y_i, T_i, \\delta_i] - E_{\\text{LME}}[b_i \\mid y_i]\\right).\n   $$\n   Report the average tilt across subjects,\n   $$\n   \\text{Tilt} = \\frac{1}{N} \\sum_{i=1}^N \\Delta m_i(T_i),\n   $$\n   which quantifies how survival information tilts $E[b_i \\mid \\cdot]$ relative to the pure LME posterior mean in the direction dictated by $\\alpha$ and the observed $(T_i, \\delta_i)$.\n\nUse the following scientifically plausible and self-consistent data and parameters, with units interpreted implicitly as dimensionless markers and times measured in the same unit for both longitudinal visits and survival times:\n\n- Number of subjects: $N = 3$.\n- Visit times per subject: $t_{ij} \\in \\{0, 1, 2\\}$ for $j = 1, 2, 3$.\n- Design vectors: $x_{ij} = z_{ij} = \\begin{bmatrix} 1 \\\\ t_{ij} \\end{bmatrix}$ for the longitudinal model; $x(u) = z(u) = \\begin{bmatrix} 1 \\\\ u \\end{bmatrix}$ for the survival model current value.\n- Longitudinal responses:\n  - Subject $1$: $y_{1\\cdot} = [\\,2.4, \\,3.0, \\,3.2\\,]$.\n  - Subject $2$: $y_{2\\cdot} = [\\,1.8, \\,2.1, \\,2.5\\,]$.\n  - Subject $3$: $y_{3\\cdot} = [\\,2.1, \\,2.9, \\,3.6\\,]$.\n- Survival times and indicators:\n  - Subject $1$: $T_1 = 0.8$, $\\delta_1 = 1$.\n  - Subject $2$: $T_2 = 3.0$, $\\delta_2 = 0$.\n  - Subject $3$: $T_3 = 2.0$, $\\delta_3 = 1$.\n- Baseline hazard: $\\lambda_0 = 0.15$ (constant).\n- Association parameter test suite (three cases to test tilt direction and boundary behavior):\n  - Case A: $\\alpha = 0.8$ (positive association).\n  - Case B: $\\alpha = -0.8$ (negative association).\n  - Case C: $\\alpha = 0.0$ (no association).\n- Residual variance: $\\sigma^2 = 0.25$.\n- Random-effects covariance: $D = \\mathrm{diag}(0.4, 0.05)$.\n- Initial fixed effects for the single EM iteration: $\\beta^{(0)} = [\\,1.5, \\,0.3\\,]^{\\top}$.\n\nImplementation requirements:\n\n- For each $\\alpha$ in the test suite, perform exactly one EM iteration starting at $\\beta^{(0)}$:\n  - E-step: compute $E[b_i \\mid y_i, T_i, \\delta_i]$ using a Laplace approximation centered at the posterior mode found by Newton's method, with the Hessian of the log-posterior approximated numerically via central differences.\n  - M-step: update $\\beta$ by weighted least squares with weights $1/\\sigma^2$ across all subjects and visits, treating $E[b_i \\mid y_i, T_i, \\delta_i]$ as known offsets.\n\n- Define the pure LME posterior mean $E_{\\text{LME}}[b_i \\mid y_i]$ using the Gaussian posterior formula under the longitudinal submodel only.\n\n- For each $\\alpha$ case, compute and return the result list $[\\,\\beta_0^{(1)}, \\,\\beta_1^{(1)}, \\,\\text{Tilt}\\,]$, where $\\beta^{(1)}$ is the updated fixed-effect vector after the single EM iteration.\n\nYour program should produce a single line of output containing the results as a comma-separated list of the three case results, each itself a list of three floats, enclosed in square brackets, exactly in the format `[[β₀,A,β₁,A,Tilt_A],[β₀,B,β₁,B,Tilt_B],[β₀,C,β₁,C,Tilt_C]]`. No other text should be printed. The answer must consist of pure numbers with no units appended. The outputs are floats and must be computed by the program based on the specified data and parameters; no user input is required.\n\nTest suite coverage:\n\n- Case A ($\\alpha = 0.8$) exercises the standard positive association in joint modeling.\n- Case B ($\\alpha = -0.8$) tests the directionality reversal, ensuring the tilt reflects negative association.\n- Case C ($\\alpha = 0.0$) serves as a boundary case where survival information does not affect the posterior of $b_i$, and the tilt should be numerically close to zero.\n\nScientific realism is ensured by using a constant baseline hazard with current-value association, Gaussian longitudinal errors, normal random effects, and time grids consistent between longitudinal visits and survival times. All numerical values are modest and consistent with typical clinical longitudinal markers and follow-up times.", "solution": "The problem requires the implementation of a single Expectation-Maximization (EM) iteration for a joint model of longitudinal and time-to-event data. The goal is to update the fixed-effects parameter vector $\\beta$ and to compute a \"tilt\" metric that quantifies the influence of the survival data on the random effects' posterior estimates. The process is performed for three different values of the association parameter $\\alpha$.\n\nThe longitudinal submodel for subject $i$ at time $t_{ij}$ is a linear mixed-effects model:\n$$\ny_{ij} = x_{ij}^{\\top}\\beta + z_{ij}^{\\top} b_i + \\varepsilon_{ij}, \\quad \\varepsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^2), \\quad b_i \\sim \\mathcal{N}(0, D)\n$$\nThe time-to-event submodel for subject $i$ follows a proportional hazards formulation with a constant baseline hazard $\\lambda_0$ and a current-value association structure:\n$$\nh_i(t) = \\lambda_0 \\exp\\left(\\alpha \\, m_i(t)\\right), \\quad \\text{where} \\quad m_i(t) = x(t)^{\\top}\\beta + z(t)^{\\top} b_i\n$$\nThe problem specifies the design vectors as $x_{ij} = z_{ij} = [1, t_{ij}]^{\\top}$ and $x(t) = z(t) = [1, t]^{\\top}$, corresponding to a random intercept and random slope model. We are given initial parameters $\\beta^{(0)}$, $\\sigma^2$, and $D$, and data for $N=3$ subjects.\n\nThe single EM iteration consists of an Expectation (E) step and a Maximization (M) step.\n\n**1. E-Step: Conditional Expectation of Random Effects**\n\nThe E-step requires computing the expectation of the random effects $b_i$ conditional on the observed data $(y_i, T_i, \\delta_i)$ and the current parameter estimates $\\theta^{(0)} = (\\beta^{(0)}, \\sigma^2, D, \\lambda_0, \\alpha)$. This expectation is $E[b_i \\mid y_i, T_i, \\delta_i, \\theta^{(0)}]$.\n\nA Laplace approximation is used to evaluate this expectation. The approximation states that the conditional expectation is well-approximated by the mode of the posterior distribution of $b_i$.\n$$\n\\hat{b}_i^{\\text{joint}} = E[b_i \\mid y_i, T_i, \\delta_i, \\theta^{(0)}] \\approx \\arg\\max_{b_i} p(b_i \\mid y_i, T_i, \\delta_i, \\theta^{(0)})\n$$\nFinding the mode is equivalent to maximizing the log-posterior, or minimizing its negative. The log-posterior is proportional to the sum of three components: the longitudinal log-likelihood, the survival log-likelihood, and the prior log-density for the random effects.\n$$\n\\log p(b_i \\mid \\cdot) \\propto \\ell_{L,i}(b_i) + \\ell_{S,i}(b_i) + \\log p(b_i)\n$$\nThe individual components, with respect to $b_i$, are:\n-   **Longitudinal Log-Likelihood**: Based on the normal distribution of measurement errors, $\\varepsilon_i = y_i - X_i\\beta - Z_ib_i$.\n    $$\n    \\ell_{L,i}(b_i) = -\\frac{1}{2\\sigma^2} (y_i - X_i\\beta^{(0)} - Z_ib_i)^{\\top}(y_i - X_i\\beta^{(0)} - Z_ib_i)\n    $$\n-   **Survival Log-Likelihood**: For a constant baseline hazard and current-value association.\n    $$\n    \\ell_{S,i}(b_i) = \\delta_i \\left(\\log \\lambda_0 + \\alpha \\, m_i(T_i)\\right) - \\int_0^{T_i} \\lambda_0 \\exp\\left(\\alpha \\, m_i(u)\\right) \\, du\n    $$\n    where $m_i(u) = [1, u]^{\\top}\\beta^{(0)} + [1, u]^{\\top}b_i$. The integral can be computed analytically. Let $C_0 = \\alpha(\\beta_0^{(0)} + b_{i0})$ and $C_1 = \\alpha(\\beta_1^{(0)} + b_{i1})$. The integral is $\\frac{\\exp(C_0)}{C_1}(\\exp(C_1 T_i) - 1)$ if $C_1 \\neq 0$, and $T_i\\exp(C_0)$ if $C_1 = 0$.\n-   **Prior Log-Density**: From the assumption $b_i \\sim \\mathcal{N}(0,D)$.\n    $$\n    \\log p(b_i) = -\\frac{1}{2} b_i^{\\top}D^{-1}b_i - \\frac{1}{2}\\log|2\\pi D|\n    $$\nThe mode of the posterior is found by minimizing the negative log-posterior using Newton's method. Let $F(b_i)$ be the negative log-posterior function (objective function). The iterative update for $b_i$ is:\n$$\nb_i^{(k+1)} = b_i^{(k)} - [H_F(b_i^{(k)})]^{-1} g_F(b_i^{(k)})\n$$\nwhere $g_F(b_i)$ and $H_F(b_i)$ are the gradient and Hessian of $F(b_i)$, respectively. As specified, the Hessian is approximated numerically. For robustness, the gradient is also computed numerically using central differences. The iteration starts from $b_i^{(0)} = [0, 0]^{\\top}$.\n\n**2. M-Step: Update of Fixed Effects**\n\nIn the M-step, the fixed-effects vector $\\beta$ is updated by maximizing the expected complete-data log-likelihood, which simplifies to a weighted least squares problem. We find $\\beta^{(1)}$ that minimizes:\n$$\n\\sum_{i=1}^N \\sum_{j=1}^{n_i} w_{ij} (y_{ij} - x_{ij}^{\\top}\\beta - z_{ij}^{\\top}\\hat{b}_i^{\\text{joint}})^2\n$$\nwhere $\\hat{b}_i^{\\text{joint}}$ are the conditional expectations computed in the E-step, and weights are $w_{ij}=1/\\sigma^2$. Letting $y_{ij}^* = y_{ij} - z_{ij}^{\\top}\\hat{b}_i^{\\text{joint}}$, the problem becomes minimizing $\\sum_{i,j} (y_{ij}^* - x_{ij}^{\\top}\\beta)^2 / \\sigma^2$. The constant factor $1/\\sigma^2$ can be ignored, resulting in an ordinary least squares problem. In matrix form for all subjects' data stacked together:\n$$\n\\beta^{(1)} = (X_{\\text{stacked}}^{\\top}X_{\\text{stacked}})^{-1}X_{\\text{stacked}}^{\\top}Y_{\\text{stacked}}^*\n$$\nwhere $Y_{\\text{stacked}}^*$ is the vector of all $y_{ij}^*$ values and $X_{\\text{stacked}}$ is the stacked design matrix for fixed effects.\n\n**3. Tilt Calculation**\n\nThe \"tilt\" quantifies the change in the estimated trajectory caused by incorporating survival information. It compares the posterior mean from the joint model, $\\hat{b}_i^{\\text{joint}}$, with the posterior mean from a pure Linear Mixed Effects (LME) model, $\\hat{b}_i^{\\text{LME}} = E_{\\text{LME}}[b_i \\mid y_i, \\theta^{(0)}]$.\nThe LME posterior mean is a standard result from Bayesian linear models, given by:\n$$\n\\hat{b}_i^{\\text{LME}} = \\left(\\frac{1}{\\sigma^2}Z_i^\\top Z_i + D^{-1}\\right)^{-1} \\frac{1}{\\sigma^2} Z_i^\\top (y_i - X_i\\beta^{(0)})\n$$\nThe tilt for subject $i$ is the difference in the current value of the trajectory at their survival time $T_i$:\n$$\n\\Delta m_i(T_i) = [1, \\, T_i]^{\\top} \\left(\\hat{b}_i^{\\text{joint}} - \\hat{b}_i^{\\text{LME}}\\right)\n$$\nThe final reported value is the average tilt across all $N$ subjects:\n$$\n\\text{Tilt} = \\frac{1}{N} \\sum_{i=1}^N \\Delta m_i(T_i)\n$$\nFor $\\alpha=0$, survival data provides no information on $b_i$, so we expect $\\hat{b}_i^{\\text{joint}} \\approx \\hat{b}_i^{\\text{LME}}$ and thus $\\text{Tilt} \\approx 0$. For $\\alpha > 0$, events ($\\delta_i=1$) suggest higher trajectories, tilting $\\hat{b}_i^{\\text{joint}}$ upwards, while for $\\alpha < 0$, events suggest lower trajectories. The reverse logic applies to censored subjects ($\\delta_i=0$).\n\nThe procedure is executed for each value of $\\alpha$ in $\\{0.8, -0.8, 0.0\\}$ using the provided data and initial parameters $\\beta^{(0)} = [1.5, 0.3]^{\\top}$, $\\sigma^2=0.25$, $D=\\mathrm{diag}(0.4, 0.05)$, and $\\lambda_0=0.15$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a single EM iteration for a joint model of longitudinal and time-to-event data.\n    \"\"\"\n\n    # --- 1. Define Data and Parameters ---\n    # Number of subjects\n    N = 3\n\n    # Longitudinal and survival data for N=3 subjects\n    y_data = [\n        np.array([2.4, 3.0, 3.2]),\n        np.array([1.8, 2.1, 2.5]),\n        np.array([2.1, 2.9, 3.6])\n    ]\n    t_visits = np.array([0.0, 1.0, 2.0])\n    T_survival = np.array([0.8, 3.0, 2.0])\n    delta_event = np.array([1, 0, 1])\n\n    # Design matrices (common for all subjects)\n    Z = np.vstack((np.ones_like(t_visits), t_visits)).T\n    X = Z\n\n    # Model parameters\n    beta0 = np.array([1.5, 0.3])\n    sigma2 = 0.25\n    D = np.diag([0.4, 0.05])\n    D_inv = np.linalg.inv(D)\n    lambda0 = 0.15\n\n    # Test suite for alpha\n    alpha_suite = [0.8, -0.8, 0.0]\n\n    # --- 2. Helper Functions ---\n    def neg_log_posterior(b, y, T, delta, alpha, X, Z, beta, sigma2, D_inv, lambda0):\n        \"\"\"\n        Computes the negative of the log-posterior of b_i. This is the objective function to minimize.\n        \"\"\"\n        # Longitudinal log-likelihood contribution\n        residuals = y - X @ beta - Z @ b\n        log_lik_long = (1 / (2 * sigma2)) * (residuals.T @ residuals)\n\n        # Survival log-likelihood contribution\n        m_T = np.array([1, T]) @ beta + np.array([1, T]) @ b\n        term1_surv = delta * (np.log(lambda0) + alpha * m_T)\n\n        # Integral part of survival likelihood\n        c0 = alpha * (beta[0] + b[0])\n        c1 = alpha * (beta[1] + b[1])\n        \n        integral_val = 0\n        if np.abs(c1) < 1e-9: # Handle case where slope is near zero\n            integral_val = T * np.exp(c0)\n        else:\n            integral_val = (np.exp(c0) / c1) * (np.exp(c1 * T) - 1)\n        \n        term2_surv = -lambda0 * integral_val\n        log_lik_surv = term1_surv + term2_surv\n\n        # Prior log-density contribution for b_i\n        log_prior_b = 0.5 * (b.T @ D_inv @ b)\n\n        # Negative log-posterior (ignoring constants)\n        return log_lik_long + log_prior_b - log_lik_surv\n\n    def numerical_gradient(f, x, args, h=1e-5):\n        \"\"\"Numerically compute gradient of f at x using central differences.\"\"\"\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_plus_h = x.copy()\n            x_plus_h[i] += h\n            x_minus_h = x.copy()\n            x_minus_h[i] -= h\n            grad[i] = (f(x_plus_h, *args) - f(x_minus_h, *args)) / (2 * h)\n        return grad\n\n    def numerical_hessian(f, x, args, h=1e-5):\n        \"\"\"Numerically compute Hessian of f at x using central differences.\"\"\"\n        n = len(x)\n        hess = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                x_pp = x.copy(); x_pp[i] += h; x_pp[j] += h\n                x_pm = x.copy(); x_pm[i] += h; x_pm[j] -= h\n                x_mp = x.copy(); x_mp[i] -= h; x_mp[j] += h\n                x_mm = x.copy(); x_mm[i] -= h; x_mm[j] -= h\n                \n                val = (f(x_pp, *args) - f(x_pm, *args) - f(x_mp, *args) + f(x_mm, *args)) / (4 * h**2)\n                hess[i, j] = val\n        return hess\n\n    def newton_method(f, x0, args, max_iter=10, tol=1e-6):\n        \"\"\"Newton's method for finding the minimum of f.\"\"\"\n        x = x0.copy()\n        for _ in range(max_iter):\n            grad = numerical_gradient(f, x, args)\n            hess = numerical_hessian(f, x, args)\n            try:\n                step = np.linalg.solve(hess, grad)\n            except np.linalg.LinAlgError:\n                # Use psuedo-inverse if Hessian is singular\n                step = np.linalg.pinv(hess) @ grad\n            \n            x_new = x - step\n            if np.linalg.norm(x_new - x) < tol:\n                break\n            x = x_new\n        return x\n\n    # --- 3. Main Loop for Each Alpha Case ---\n    results = []\n    \n    # Pre-calculate LME posterior means (independent of alpha)\n    b_lme_means = []\n    P_inv = np.linalg.inv(Z.T @ Z / sigma2 + D_inv)\n    for i in range(N):\n        y_i = y_data[i]\n        term2 = (Z.T @ (y_i - X @ beta0)) / sigma2\n        b_lme_i = P_inv @ term2\n        b_lme_means.append(b_lme_i)\n\n    # Pre-calculate terms for M-step\n    X_stacked = np.vstack([X] * N)\n    XTX_inv = np.linalg.inv(X_stacked.T @ X_stacked)\n\n    for alpha in alpha_suite:\n        # --- E-Step ---\n        b_joint_means = []\n        for i in range(N):\n            b0_i = np.array([0.0, 0.0])\n            args = (y_data[i], T_survival[i], delta_event[i], alpha, X, Z, beta0, sigma2, D_inv, lambda0)\n            b_joint_i = newton_method(neg_log_posterior, b0_i, args)\n            b_joint_means.append(b_joint_i)\n\n        # --- M-Step ---\n        y_star_stacked = np.zeros(N * len(t_visits))\n        for i in range(N):\n            y_i_star = y_data[i] - Z @ b_joint_means[i]\n            y_star_stacked[i*len(t_visits):(i+1)*len(t_visits)] = y_i_star\n        \n        beta1 = XTX_inv @ X_stacked.T @ y_star_stacked\n\n        # --- Tilt Calculation ---\n        tilts = []\n        for i in range(N):\n            zt_i = np.array([1, T_survival[i]])\n            delta_m_i = zt_i.T @ (b_joint_means[i] - b_lme_means[i])\n            tilts.append(delta_m_i)\n        \n        avg_tilt = np.mean(tilts)\n        \n        results.append([beta1[0], beta1[1], avg_tilt])\n\n    # --- 4. Format and Print Output ---\n    # Convert lists to string representation for printing\n    result_strings = [f\"[{r[0]:.8f},{r[1]:.8f},{r[2]:.8f}]\" for r in results]\n    print(f\"[{','.join(result_strings)}]\")\n\n\nsolve()\n```", "id": "4968553"}, {"introduction": "The ultimate goal of building a joint model is often to make personalized, up-to-date predictions. This final practice focuses on this key application: dynamic prediction of survival probabilities [@problem_id:4968550]. Using the outputs of a fitted model, you will calculate a patient's conditional survival probability and, just as importantly, quantify the uncertainty in that prediction using the Delta method, a versatile tool for error propagation in complex models.", "problem": "A clinical study monitors a continuously observed biomarker through a Random Intercept and Random Slope longitudinal model and relates the instantaneous risk of a primary event to the current underlying biomarker level via a proportional hazards structure. Consider subject $i$ with history up to time $t^{\\star} = 2$ (years). The longitudinal submodel specifies the subject-specific latent biomarker trajectory as $m_i(t) = \\beta_0 + \\beta_1 t + b_{0i} + b_{1i} t$, where $b_{0i}$ and $b_{1i}$ are the random intercept and random slope, respectively. The time-to-event submodel specifies the hazard as $h_i(t \\mid b_i) = \\lambda_0 \\exp\\{\\alpha \\, m_i(t)\\}$, where $\\lambda_0 > 0$ is a constant baseline hazard and $\\alpha$ is a biomarker–event association parameter. Assume no additional baseline covariates in the hazard beyond $m_i(t)$.\n\nSuppose a joint model has been fit and yields the Empirical Bayes (EB) estimate $\\hat b_i = (\\hat b_{0i}, \\hat b_{1i})^{\\top}$ and a normal approximation to the posterior of $b_i$ given the subject’s data up to $t^{\\star}$: $b_i \\mid \\text{data up to } t^{\\star} \\approx \\mathcal{N}_2(\\hat b_i, V_i)$, where $V_i$ is the $2 \\times 2$ covariance matrix. For this subject, use the following parameter values estimated from the joint model fit:\n- $\\lambda_0 = 0.05$ (per year), $\\alpha = 0.4$, $\\beta_0 = 1.0$, $\\beta_1 = 0.3$,\n- $\\hat b_i = (\\hat b_{0i}, \\hat b_{1i})^{\\top} = (0.2, -0.05)^{\\top}$,\n- $V_i = \\begin{pmatrix} 0.04 & 0.006 \\\\ 0.006 & 0.01 \\end{pmatrix}$.\n\nStarting from the fundamental relationship between hazard and survival, $S_i(t) = \\exp\\!\\left(-\\int_0^t h_i(s \\mid b_i)\\, ds\\right)$, and conditioning on the subject’s information at $t^{\\star}$, derive from first principles the plug-in one-year dynamic survival prediction $S_i(1 \\mid \\hat b_i)$ for the window $[t^{\\star}, t^{\\star}+1]$ using the EB estimate $\\hat b_i$, and then quantify the uncertainty in $S_i(1 \\mid \\hat b_i)$ due solely to the estimation of $b_i$ via a first-order Delta method approximation based on $V_i$. Explicitly compute:\n1. The numerical value of $S_i(1 \\mid \\hat b_i)$.\n2. The numerical value of the first-order Delta method standard error for $S_i(1 \\mid \\hat b_i)$ induced by the uncertainty in $b_i$.\n\nExpress both the survival prediction and its standard error as unitless quantities. Round each to four significant figures. Your final answer must be the ordered pair of these two numbers.", "solution": "The problem requires the computation of a dynamic survival prediction and its associated standard error within the framework of a joint model for longitudinal and time-to-event data. The solution proceeds in two parts: first, the calculation of the plug-in survival estimate, and second, the quantification of its uncertainty using the first-order Delta method.\n\nThe problem notation $S_i(1 \\mid \\hat b_i)$ for the \"one-year dynamic survival prediction for the window $[t^{\\star}, t^{\\star}+1]$\" refers to the conditional probability of surviving past time $t^{\\star}+1$, given survival up to time $t^{\\star}$, evaluated using the Empirical Bayes estimate $\\hat b_i$. This conditional survival probability, which we denote as $S_{dyn}(1 \\mid b_i)$, is formally given by $P(T_i > t^{\\star}+1 \\mid T_i > t^{\\star}, b_i)$. Based on the fundamental relationship between the hazard function $h_i(t)$ and the survival function $S_i(t) = \\exp(-\\int_0^t h_i(s) ds)$, this conditional probability is:\n$$\nS_{dyn}(1 \\mid b_i) = \\frac{S_i(t^{\\star}+1 \\mid b_i)}{S_i(t^{\\star} \\mid b_i)} = \\frac{\\exp\\left(-\\int_0^{t^{\\star}+1} h_i(s \\mid b_i) \\, ds\\right)}{\\exp\\left(-\\int_0^{t^{\\star}} h_i(s \\mid b_i) \\, ds\\right)} = \\exp\\left(-\\int_{t^{\\star}}^{t^{\\star}+1} h_i(s \\mid b_i) \\, ds\\right)\n$$\nThe problem asks for the \"plug-in\" estimate, which means we substitute the random effects vector $b_i = (b_{0i}, b_{1i})^{\\top}$ with its estimate $\\hat b_i = (\\hat b_{0i}, \\hat b_{1i})^{\\top}$.\n\nThe subject-specific longitudinal trajectory, evaluated at the estimate $\\hat b_i$, is:\n$$\nm_i(t \\mid \\hat b_i) = \\beta_0 + \\beta_1 t + \\hat b_{0i} + \\hat b_{1i} t = (\\beta_0 + \\hat b_{0i}) + (\\beta_1 + \\hat b_{1i}) t\n$$\nLet's define the fixed and random components for subject $i$ as $B_{0i} = \\beta_0 + \\hat b_{0i}$ and $B_{1i} = \\beta_1 + \\hat b_{1i}$.\nThe hazard function is then $h_i(t \\mid \\hat b_i) = \\lambda_0 \\exp\\{\\alpha \\, m_i(t \\mid \\hat b_i)\\} = \\lambda_0 \\exp\\{\\alpha(B_{0i} + B_{1i} t)\\}$.\n\nFirst, we compute the cumulative hazard over the interval $[t^{\\star}, t^{\\star}+1]$:\n$$\n\\Lambda_i(t^{\\star}, t^{\\star}+1 \\mid \\hat b_i) = \\int_{t^{\\star}}^{t^{\\star}+1} h_i(s \\mid \\hat b_i) \\, ds = \\int_{t^{\\star}}^{t^{\\star}+1} \\lambda_0 \\exp(\\alpha B_{0i} + \\alpha B_{1i} s) \\, ds\n$$\n$$\n= \\lambda_0 \\exp(\\alpha B_{0i}) \\int_{t^{\\star}}^{t^{\\star}+1} \\exp(\\alpha B_{1i} s) \\, ds\n$$\nAssuming $B_{1i} \\neq 0$, the integral evaluates to:\n$$\n\\Lambda_i(t^{\\star}, t^{\\star}+1 \\mid \\hat b_i) = \\lambda_0 \\exp(\\alpha B_{0i}) \\left[ \\frac{\\exp(\\alpha B_{1i} s)}{\\alpha B_{1i}} \\right]_{s=t^{\\star}}^{t^{\\star}+1} = \\frac{\\lambda_0 \\exp(\\alpha B_{0i})}{\\alpha B_{1i}} \\left( \\exp(\\alpha B_{1i}(t^{\\star}+1)) - \\exp(\\alpha B_{1i} t^{\\star}) \\right)\n$$\nThe plug-in dynamic survival prediction is $S_{dyn}(1 \\mid \\hat b_i) = \\exp(-\\Lambda_i(t^{\\star}, t^{\\star}+1 \\mid \\hat b_i))$.\n\nUsing the given values:\n$t^{\\star} = 2$, $\\lambda_0 = 0.05$, $\\alpha = 0.4$, $\\beta_0 = 1.0$, $\\beta_1 = 0.3$, $\\hat b_{0i} = 0.2$, $\\hat b_{1i} = -0.05$.\n$B_{0i} = 1.0 + 0.2 = 1.2$\n$B_{1i} = 0.3 + (-0.05) = 0.25$\nSince $B_{1i} = 0.25 \\neq 0$, we can use the derived formula.\n$$\n\\Lambda_i(2, 3 \\mid \\hat b_i) = \\frac{0.05 \\exp(0.4 \\times 1.2)}{0.4 \\times 0.25} \\left( \\exp(0.4 \\times 0.25 \\times 3) - \\exp(0.4 \\times 0.25 \\times 2) \\right)\n$$\n$$\n= \\frac{0.05 \\exp(0.48)}{0.1} \\left( \\exp(0.3) - \\exp(0.2) \\right) = 0.5 \\exp(0.48) \\left( \\exp(0.3) - \\exp(0.2) \\right)\n$$\nNumerically, this is:\n$$\n\\Lambda_i(2, 3 \\mid \\hat b_i) \\approx 0.5 \\times 1.616074 \\times (1.349859 - 1.221403) \\approx 0.5 \\times 1.616074 \\times 0.128456 \\approx 0.10380\n$$\nThe survival prediction is:\n$$\nS_{dyn}(1 \\mid \\hat b_i) = \\exp(-0.10380) \\approx 0.90141\n$$\nRounding to four significant figures, the survival prediction is $0.9014$.\n\nNext, we compute the standard error using the first-order Delta method. Let $g(b_i) = S_{dyn}(1 \\mid b_i)$. The variance of $g(b_i)$ is approximated by:\n$$\n\\text{Var}(g(b_i)) \\approx \\left[ \\nabla_b g(b_i) \\right]^{\\top} V_i \\left[ \\nabla_b g(b_i) \\right] \\Big|_{b_i=\\hat b_i}\n$$\nwhere $\\nabla_b g(b_i)$ is the gradient of $g(b_i)$ with respect to $b_i = (b_{0i}, b_{1i})^{\\top}$.\nLet $\\Lambda(b_i) = \\int_{t^{\\star}}^{t^{\\star}+1} h_i(s \\mid b_i) \\, ds$. Then $g(b_i) = \\exp(-\\Lambda(b_i))$.\nUsing the chain rule, the gradient is:\n$$\n\\nabla_b g(b_i) = -\\exp(-\\Lambda(b_i)) \\nabla_b \\Lambda(b_i) = -g(b_i) \\nabla_b \\Lambda(b_i)\n$$\nWe need to compute the gradient of the cumulative hazard, $\\nabla_b \\Lambda(b_i) = (\\frac{\\partial \\Lambda}{\\partial b_{0i}}, \\frac{\\partial \\Lambda}{\\partial b_{1i}})^{\\top}$.\nThe first partial derivative is:\n$$\n\\frac{\\partial \\Lambda}{\\partial b_{0i}} = \\int_{t^{\\star}}^{t^{\\star}+1} \\frac{\\partial}{\\partial b_{0i}} h_i(s \\mid b_i) \\, ds = \\int_{t^{\\star}}^{t^{\\star}+1} h_i(s \\mid b_i) \\cdot \\alpha \\frac{\\partial m_i(s \\mid b_i)}{\\partial b_{0i}} \\, ds = \\int_{t^{\\star}}^{t^{\\star}+1} \\alpha h_i(s \\mid b_i) \\, ds = \\alpha \\Lambda(b_i)\n$$\nThe second partial derivative is:\n$$\n\\frac{\\partial \\Lambda}{\\partial b_{1i}} = \\int_{t^{\\star}}^{t^{\\star}+1} \\frac{\\partial}{\\partial b_{1i}} h_i(s \\mid b_i) \\, ds = \\int_{t^{\\star}}^{t^{\\star}+1} h_i(s \\mid b_i) \\cdot \\alpha \\frac{\\partial m_i(s \\mid b_i)}{\\partial b_{1i}} \\, ds = \\int_{t^{\\star}}^{t^{\\star}+1} \\alpha s h_i(s \\mid b_i) \\, ds\n$$\nWe evaluate these derivatives at $b_i = \\hat b_i$.\n$\\frac{\\partial \\Lambda}{\\partial b_{0i}} \\Big|_{\\hat b_i} = \\alpha \\Lambda(\\hat b_i) \\approx 0.4 \\times 0.10380 = 0.041520$.\nFor the second derivative:\n$$\n\\frac{\\partial \\Lambda}{\\partial b_{1i}} \\Big|_{\\hat b_i} = \\int_{2}^{3} \\alpha s \\, \\lambda_0 \\exp(\\alpha(B_{0i} + B_{1i} s)) \\, ds = \\alpha \\lambda_0 \\exp(\\alpha B_{0i}) \\int_{2}^{3} s \\exp(\\alpha B_{1i} s) \\, ds\n$$\nThe integral $\\int s \\exp(as) \\, ds$ is solved using integration by parts: $\\left(\\frac{s}{a} - \\frac{1}{a^2}\\right)\\exp(as)$.\nWith $a = \\alpha B_{1i} = 0.4 \\times 0.25 = 0.1$:\n$$\n\\int_{2}^{3} s \\exp(0.1 s) \\, ds = \\left[ \\left(\\frac{s}{0.1} - \\frac{1}{0.1^2}\\right) \\exp(0.1s) \\right]_{2}^{3} = \\left[ (10s - 100) \\exp(0.1s) \\right]_{2}^{3}\n$$\n$$\n= (10 \\times 3 - 100)\\exp(0.3) - (10 \\times 2 - 100)\\exp(0.2) = 80\\exp(0.2) - 70\\exp(0.3)\n$$\nNumerically, this is $80 \\times 1.221403 - 70 \\times 1.349859 \\approx 97.71224 - 94.49013 = 3.22211$.\nThen,\n$$\n\\frac{\\partial \\Lambda}{\\partial b_{1i}} \\Big|_{\\hat b_i} = (0.4)(0.05) \\exp(0.48) \\times 3.22211 = 0.02 \\exp(0.48) \\times 3.22211 \\approx 0.02 \\times 1.616074 \\times 3.22211 \\approx 0.10411\n$$\nSo, $\\nabla_b \\Lambda(\\hat b_i) \\approx (0.041520, 0.10411)^{\\top}$.\nThe gradient of $g$ is:\n$$\n\\nabla_b g(\\hat b_i) \\approx -0.90141 \\begin{pmatrix} 0.041520 \\\\ 0.10411 \\end{pmatrix} = \\begin{pmatrix} -0.037422 \\\\ -0.093845 \\end{pmatrix}\n$$\nNow we compute the variance using $V_i = \\begin{pmatrix} 0.04 & 0.006 \\\\ 0.006 & 0.01 \\end{pmatrix}$:\n$$\n\\text{Var}(g(\\hat b_i)) \\approx \\begin{pmatrix} -0.037422 & -0.093845 \\end{pmatrix} \\begin{pmatrix} 0.04 & 0.006 \\\\ 0.006 & 0.01 \\end{pmatrix} \\begin{pmatrix} -0.037422 \\\\ -0.093845 \\end{pmatrix}\n$$\nFirst, the row-vector times matrix product:\n$$\n\\begin{pmatrix} -0.037422 & -0.093845 \\end{pmatrix} \\begin{pmatrix} 0.04 & 0.006 \\\\ 0.006 & 0.01 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} (-0.037422)(0.04) + (-0.093845)(0.006) & (-0.037422)(0.006) + (-0.093845)(0.01) \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} -0.00149688 - 0.00056307 & -0.00022453 - 0.00093845 \\end{pmatrix} = \\begin{pmatrix} -0.00205995 & -0.00116298 \\end{pmatrix}\n$$\nFinally, multiplying by the column vector:\n$$\n\\text{Var}(g(\\hat b_i)) \\approx \\begin{pmatrix} -0.00205995 & -0.00116298 \\end{pmatrix} \\begin{pmatrix} -0.037422 \\\\ -0.093845 \\end{pmatrix}\n$$\n$$\n= (-0.00205995)(-0.037422) + (-0.00116298)(-0.093845) \\approx 0.00007709 + 0.00010915 \\approx 0.00018624\n$$\nThe standard error is the square root of the variance:\n$$\n\\text{SE}(g(\\hat b_i)) = \\sqrt{\\text{Var}(g(\\hat b_i))} \\approx \\sqrt{0.00018624} \\approx 0.013647\n$$\nRounding to four significant figures, the standard error is $0.01365$.\n\nThe required values are the survival prediction and its standard error.\n1. Survival prediction: $0.9014$\n2. Standard error: $0.01365$", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.9014 & 0.01365 \\end{pmatrix}}\n$$", "id": "4968550"}]}