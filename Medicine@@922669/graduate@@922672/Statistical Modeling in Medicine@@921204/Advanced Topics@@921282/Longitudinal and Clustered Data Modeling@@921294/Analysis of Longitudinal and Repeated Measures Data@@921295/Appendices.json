{"hands_on_practices": [{"introduction": "In analyzing data from pre-post study designs, a common question is how to best utilize baseline measurements. This exercise delves into a foundational concept in longitudinal analysis: statistical efficiency. By mathematically comparing the variance of estimators from two common approaches—change-score analysis and Analysis of Covariance (ANCOVA)—you will derive the conditions under which one method provides more statistical power than the other, highlighting the importance of covariate adjustment. [@problem_id:4951173]", "problem": "Consider a randomized, parallel-group clinical trial with $2$ arms, each of size $n$, measuring a continuous biomarker at baseline ($Y_{0}$) and at a single post-treatment follow-up ($Y_{1}$) for each participant. Assume the following:\n\n- Subjects are independently and identically distributed and randomized, so the treatment indicator $X \\in \\{0,1\\}$ is independent of all baseline covariates, including $Y_{0}$.\n- The pair $(Y_{0}, Y_{1})$ follows a bivariate normal distribution with group-invariant marginal variances $\\operatorname{Var}(Y_{0}) = \\sigma_{0}^{2}$, $\\operatorname{Var}(Y_{1}) = \\sigma_{1}^{2}$, and correlation $\\rho = \\operatorname{Corr}(Y_{0}, Y_{1})$, and the treatment has an additive effect on $Y_{1}$ that does not alter these variances or $\\rho$.\n- Homoscedasticity holds within arms, and all quantities are identical across the two arms except for the mean of $Y_{1}$ due to treatment.\n\nTwo analysis strategies are considered for estimating the average treatment effect on $Y_{1}$:\n\n$1.$ The change-score approach: compute $D = Y_{1} - Y_{0}$ for each participant, and estimate the treatment effect as the difference in the group means of $D$.\n\n$2.$ Analysis of Covariance (ANCOVA): regress $Y_{1}$ on the treatment indicator $X$ and the baseline $Y_{0}$, and use the coefficient of $X$ as the treatment effect estimator.\n\nDefine the efficiency gain of ANCOVA relative to the change-score analysis as the ratio of the large-sample variance of the change-score estimator to the large-sample variance of the ANCOVA estimator. Derive this efficiency gain under the assumptions above as a closed-form analytic expression in terms of $\\sigma_{0}^{2}$, $\\sigma_{1}^{2}$, and $\\rho$. Express your final answer as a single analytical expression. No numerical approximation or rounding is required, and no units should be included in the final answer.", "solution": "The user wants to derive the efficiency gain of an Analysis of Covariance (ANCOVA) model relative to a change-score analysis for estimating a treatment effect in a randomized clinical trial. The efficiency gain is defined as the ratio of the large-sample variance of the change-score estimator to that of the ANCOVA estimator.\n\nThe problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- A randomized, parallel-group clinical trial with $2$ arms, each of size $n$.\n- A continuous biomarker is measured at baseline ($Y_{0}$) and at one post-treatment follow-up ($Y_{1}$).\n- The treatment indicator is $X \\in \\{0,1\\}$. Randomization implies $X$ is independent of $Y_{0}$.\n- The pair $(Y_{0}, Y_{1})$ follows a bivariate normal distribution.\n- The marginal variances are group-invariant: $\\operatorname{Var}(Y_{0}) = \\sigma_{0}^{2}$ and $\\operatorname{Var}(Y_{1}) = \\sigma_{1}^{2}$.\n- The correlation is group-invariant: $\\rho = \\operatorname{Corr}(Y_{0}, Y_{1})$.\n- The treatment has an additive effect on $Y_{1}$ and does not alter variances or the correlation.\n- Strategy $1$: Change-score analysis on $D = Y_{1} - Y_{0}$. The estimator is the difference in group means of $D$.\n- Strategy $2$: ANCOVA, regressing $Y_{1}$ on $X$ and $Y_{0}$. The estimator is the coefficient of $X$.\n- Goal: Derive the efficiency gain, defined as $\\operatorname{Var}(\\hat{\\tau}_{\\text{CS}}) / \\operatorname{Var}(\\hat{\\tau}_{\\text{ANCOVA}})$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard, classic question in the statistical analysis of clinical trials. The assumptions, such as bivariate normality, homoscedasticity, and an additive treatment effect, are common idealizations used to compare the statistical properties of different estimators. The problem is self-contained, with all necessary information provided to derive the requested expression. The language is precise and objective. There are no scientific or logical flaws, contradictions, or ambiguities.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full, reasoned solution will be provided.\n\n### Derivation of the Efficiency Gain\n\nThe derivation proceeds in three parts: first, we find the large-sample variance of the change-score estimator; second, we find the large-sample variance of the ANCOVA estimator; and third, we compute their ratio.\n\n**1. Variance of the Change-Score Estimator ($\\hat{\\tau}_{\\text{CS}}$)**\n\nThe change score for each participant $i$ is defined as $D_i = Y_{1i} - Y_{0i}$. The treatment effect is estimated by the difference in the mean change scores between the treatment group ($X=1$) and the control group ($X=0$):\n$$ \\hat{\\tau}_{\\text{CS}} = \\bar{D}_1 - \\bar{D}_0 = \\frac{1}{n} \\sum_{i:X_i=1} D_i - \\frac{1}{n} \\sum_{i:X_i=0} D_i $$\nSince the two groups are independent due to randomization, the variance of the estimator is the sum of the variances of the two sample means:\n$$ \\operatorname{Var}(\\hat{\\tau}_{\\text{CS}}) = \\operatorname{Var}(\\bar{D}_1) + \\operatorname{Var}(\\bar{D}_0) $$\nGiven that subjects are i.i.d. within each arm of size $n$, we have:\n$$ \\operatorname{Var}(\\hat{\\tau}_{\\text{CS}}) = \\frac{\\operatorname{Var}(D | X=1)}{n} + \\frac{\\operatorname{Var}(D | X=0)}{n} $$\nThe problem states that variances are group-invariant. This implies that the variance of the change score $D$ is the same in both groups: $\\operatorname{Var}(D | X=1) = \\operatorname{Var}(D | X=0) = \\operatorname{Var}(D)$. Therefore:\n$$ \\operatorname{Var}(\\hat{\\tau}_{\\text{CS}}) = \\frac{2}{n} \\operatorname{Var}(D) $$\nThe variance of the change score $D$ is:\n$$ \\operatorname{Var}(D) = \\operatorname{Var}(Y_1 - Y_0) = \\operatorname{Var}(Y_1) + \\operatorname{Var}(Y_0) - 2 \\operatorname{Cov}(Y_1, Y_0) $$\nUsing the given parameters $\\operatorname{Var}(Y_0) = \\sigma_0^2$, $\\operatorname{Var}(Y_1) = \\sigma_1^2$, and the definition of correlation $\\operatorname{Cov}(Y_1, Y_0) = \\rho \\sqrt{\\operatorname{Var}(Y_0)\\operatorname{Var}(Y_1)} = \\rho \\sigma_0 \\sigma_1$, we find:\n$$ \\operatorname{Var}(D) = \\sigma_1^2 + \\sigma_0^2 - 2 \\rho \\sigma_0 \\sigma_1 $$\nSubstituting this back, the large-sample variance of the change-score estimator is:\n$$ \\operatorname{Var}(\\hat{\\tau}_{\\text{CS}}) = \\frac{2}{n} (\\sigma_0^2 + \\sigma_1^2 - 2 \\rho \\sigma_0 \\sigma_1) $$\n\n**2. Variance of the ANCOVA Estimator ($\\hat{\\tau}_{\\text{ANCOVA}}$)**\n\nThe ANCOVA model is a multiple linear regression:\n$$ Y_{1i} = \\beta_0 + \\tau_{\\text{ANCOVA}} X_i + \\beta_1 Y_{0i} + \\epsilon_i $$\nThe estimator $\\hat{\\tau}_{\\text{ANCOVA}}$ is the ordinary least squares (OLS) estimate for the coefficient of the treatment indicator $X_i$. The large-sample variance of a regression coefficient can be found using the Frisch-Waugh-Lovell theorem. The variance of $\\hat{\\tau}_{\\text{ANCOVA}}$ is given by:\n$$ \\operatorname{Var}(\\hat{\\tau}_{\\text{ANCOVA}}) = \\frac{\\sigma_\\epsilon^2}{\\sum_{i=1}^{2n} \\tilde{X}_i^2} $$\nwhere $\\sigma_\\epsilon^2 = \\operatorname{Var}(\\epsilon_i) = \\operatorname{Var}(Y_1 | X, Y_0)$ is the residual variance, and $\\tilde{X}_i$ are the residuals from regressing $X_i$ on the other covariates in the model (in this case, the intercept and $Y_{0i}$).\n\nFirst, we determine the denominator, $\\sum \\tilde{X}_i^2$. Because subjects are randomized to treatment arms, $X_i$ is independent of the baseline measurement $Y_{0i}$. Thus, the regression of $X_i$ on $Y_{0i}$ yields a slope that tends to zero for large $n$. The residuals are simply $\\tilde{X}_i \\approx X_i - \\bar{X}$.\nThe total sample size is $2n$, with $n$ subjects in each arm. The mean of the treatment indicator is $\\bar{X} = \\frac{n \\cdot 1 + n \\cdot 0}{2n} = \\frac{1}{2}$. The sum of squared residuals is:\n$$ \\sum_{i=1}^{2n} \\tilde{X}_i^2 \\approx \\sum_{i=1}^{2n} (X_i - \\bar{X})^2 = n \\left(1 - \\frac{1}{2}\\right)^2 + n \\left(0 - \\frac{1}{2}\\right)^2 = n \\left(\\frac{1}{4}\\right) + n \\left(\\frac{1}{4}\\right) = \\frac{n}{2} $$\nNext, we determine the numerator, $\\sigma_\\epsilon^2$. This is the variance of $Y_1$ conditional on $X$ and $Y_0$. Since the treatment effect is purely additive, conditioning on $X$ does not reduce the variance beyond conditioning on $Y_0$. Therefore, $\\sigma_\\epsilon^2 = \\operatorname{Var}(Y_1 | Y_0)$. For a bivariate normal distribution, the conditional variance is given by:\n$$ \\operatorname{Var}(Y_1 | Y_0) = \\operatorname{Var}(Y_1) (1 - \\operatorname{Corr}(Y_0, Y_1)^2) $$\nSubstituting the given parameters $\\operatorname{Var}(Y_1) = \\sigma_1^2$ and $\\operatorname{Corr}(Y_0, Y_1) = \\rho$:\n$$ \\sigma_\\epsilon^2 = \\sigma_1^2 (1 - \\rho^2) $$\nCombining the numerator and denominator, the large-sample variance of the ANCOVA estimator is:\n$$ \\operatorname{Var}(\\hat{\\tau}_{\\text{ANCOVA}}) = \\frac{\\sigma_1^2 (1 - \\rho^2)}{n/2} = \\frac{2 \\sigma_1^2 (1 - \\rho^2)}{n} $$\n\n**3. Efficiency Gain**\n\nThe efficiency gain of ANCOVA relative to the change-score analysis is the ratio of their variances:\n$$ \\text{Efficiency Gain} = \\frac{\\operatorname{Var}(\\hat{\\tau}_{\\text{CS}})}{\\operatorname{Var}(\\hat{\\tau}_{\\text{ANCOVA}})} $$\nSubstituting the expressions derived above:\n$$ \\text{Efficiency Gain} = \\frac{\\frac{2}{n} (\\sigma_0^2 + \\sigma_1^2 - 2 \\rho \\sigma_0 \\sigma_1)}{\\frac{2}{n} \\sigma_1^2 (1 - \\rho^2)} $$\nCanceling the common factor of $\\frac{2}{n}$ yields the final expression:\n$$ \\text{Efficiency Gain} = \\frac{\\sigma_0^2 + \\sigma_1^2 - 2 \\rho \\sigma_0 \\sigma_1}{\\sigma_1^2 (1 - \\rho^2)} $$\nThis expression represents the factor by which the variance of the treatment effect estimator is reduced when using ANCOVA compared to a change-score analysis under the specified assumptions.", "answer": "$$\n\\boxed{\\frac{\\sigma_{0}^{2} + \\sigma_{1}^{2} - 2 \\rho \\sigma_{0} \\sigma_{1}}{\\sigma_{1}^{2} (1 - \\rho^{2})}}\n$$", "id": "4951173"}, {"introduction": "Once a Linear Mixed Model (LMM) is fitted to longitudinal data, the next step is to interpret the results and answer specific research questions. This practice provides a hands-on opportunity to test joint hypotheses about the model's fixed effects using the estimated parameters and their covariance matrix. You will construct a contrast matrix to precisely formulate questions about changes over time and then compute the corresponding Wald statistic, a fundamental skill for drawing conclusions from LMMs. [@problem_id:4951162]", "problem": "Consider a longitudinal study of adults with primary hypertension, where systolic blood pressure is measured at four clinic visits: baseline ($t_{0}$), $4$ weeks ($t_{1}$), $12$ weeks ($t_{2}$), and $24$ weeks ($t_{3}$). A Linear Mixed Model (LMM) with a subject-specific random intercept and visit-specific fixed effects is fit to the repeated measures data. Let the vector of visit-specific fixed-effect means be $\\beta = (\\mu_{0}, \\mu_{1}, \\mu_{2}, \\mu_{3})^{T}$, representing the population mean systolic blood pressure at each visit. Under standard regularity conditions for maximum likelihood estimation in LMMs, the Generalized Least Squares (GLS) estimator $\\hat{\\beta}$ is approximately multivariate normal with mean $\\beta$ and covariance matrix $\\widehat{\\text{Var}}(\\hat{\\beta})$.\n\nSuppose the fitted model yields the fixed-effect estimates\n$$\n\\hat{\\beta} = \\begin{pmatrix} 132.0 \\\\ 128.5 \\\\ 127.0 \\\\ 125.5 \\end{pmatrix},\n$$\nand an estimated covariance matrix for $\\hat{\\beta}$ with a plausible compound-symmetric structure,\n$$\n\\widehat{\\text{Var}}(\\hat{\\beta}) = \\begin{pmatrix}\n0.49  0.22  0.22  0.22 \\\\\n0.22  0.49  0.22  0.22 \\\\\n0.22  0.22  0.49  0.22 \\\\\n0.22  0.22  0.22  0.49\n\\end{pmatrix}.\n$$\n\nYou are interested in simultaneously testing whether the early change from baseline is zero and whether the late change from $12$ to $24$ weeks is zero, that is, the joint null hypothesis $H_{0}: \\mu_{1} - \\mu_{0} = 0$ and $\\mu_{3} - \\mu_{2} = 0$.\n\nUsing only the foundational facts that $\\hat{\\beta}$ is approximately multivariate normal and that linear functions of multivariate normal vectors are also multivariate normal, perform the following:\n\n- Construct the contrast matrix $C$ that maps $\\beta$ to the two differences $(\\mu_{1} - \\mu_{0}, \\mu_{3} - \\mu_{2})^{T}$.\n- From first principles, derive the corresponding Wald test statistic for the general linear hypothesis $H_{0}$.\n- Compute the numerical value of this Wald test statistic using the provided $\\hat{\\beta}$ and $\\widehat{\\text{Var}}(\\hat{\\beta})$.\n\nRound your final numeric answer to four significant figures. Do not report any units with your answer.", "solution": "The problem is evaluated as scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. It is therefore deemed valid.\n\nThe problem asks for the construction of a contrast matrix, the derivation of a Wald test statistic, and its computation for a joint null hypothesis on the fixed effects of a Linear Mixed Model (LMM).\n\nThe vector of fixed-effect parameter estimates is given as $\\hat{\\beta}$, which is approximately multivariate normal with mean $\\beta$ and estimated covariance matrix $\\widehat{\\text{Var}}(\\hat{\\beta}) = \\hat{V}$.\n$$\n\\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\hat{V})\n$$\nwhere\n$$\n\\hat{\\beta} = \\begin{pmatrix} 132.0 \\\\ 128.5 \\\\ 127.0 \\\\ 125.5 \\end{pmatrix}, \\quad \\hat{V} = \\widehat{\\text{Var}}(\\hat{\\beta}) = \\begin{pmatrix}\n0.49  0.22  0.22  0.22 \\\\\n0.22  0.49  0.22  0.22 \\\\\n0.22  0.22  0.49  0.22 \\\\\n0.22  0.22  0.22  0.49\n\\end{pmatrix}\n$$\nThe parameter vector is $\\beta = (\\mu_0, \\mu_1, \\mu_2, \\mu_3)^T$. The joint null hypothesis of interest is:\n$$\nH_0: \\mu_1 - \\mu_0 = 0 \\quad \\text{and} \\quad \\mu_3 - \\mu_2 = 0\n$$\n\nThis is a general linear hypothesis of the form $H_0: C\\beta = \\mathbf{0}$, where $C$ is a contrast matrix and $\\mathbf{0}$ is a zero vector.\n\n**1. Construct the Contrast Matrix $C$**\n\nThe goal is to define a matrix $C$ that transforms the parameter vector $\\beta$ into a vector of the two specified differences.\n$$\nC\\beta = \\begin{pmatrix} \\mu_1 - \\mu_0 \\\\ \\mu_3 - \\mu_2 \\end{pmatrix}\n$$\nWe can write this transformation explicitly:\n$$\n\\begin{pmatrix} \\mu_1 - \\mu_0 \\\\ \\mu_3 - \\mu_2 \\end{pmatrix} = \\begin{pmatrix} (-1)\\mu_0 + (1)\\mu_1 + (0)\\mu_2 + (0)\\mu_3 \\\\ (0)\\mu_0 + (0)\\mu_1 + (-1)\\mu_2 + (1)\\mu_3 \\end{pmatrix}\n$$\nThis directly gives the rows of the contrast matrix $C$. The first row corresponds to the coefficients of $(\\mu_0, \\mu_1, \\mu_2, \\mu_3)$ for the first contrast, and the second row corresponds to the coefficients for the second contrast.\n$$\nC = \\begin{pmatrix}\n-1  1  0  0 \\\\\n0  0  -1  1\n\\end{pmatrix}\n$$\nThis is a $2 \\times 4$ matrix, which correctly maps the $4 \\times 1$ vector $\\beta$ to a $2 \\times 1$ vector of contrasts.\n\n**2. Derive the Wald Test Statistic**\n\nThe derivation proceeds from the foundational fact that a linear transformation of a multivariate normal random vector is also a multivariate normal random vector.\nWe start with the approximate distribution of the estimator $\\hat{\\beta}$:\n$$\n\\hat{\\beta} \\sim \\mathcal{N}(\\beta, V)\n$$\nwhere $V = \\text{Var}(\\hat{\\beta})$.\nThe linear transformation of interest is defined by the contrast matrix $C$. Let us consider the distribution of the random vector $C\\hat{\\beta}$.\nThe expectation of $C\\hat{\\beta}$ is:\n$$\nE[C\\hat{\\beta}] = C E[\\hat{\\beta}] = C\\beta\n$$\nThe covariance matrix of $C\\hat{\\beta}$ is given by the sandwich formula:\n$$\n\\text{Var}(C\\hat{\\beta}) = C \\text{Var}(\\hat{\\beta}) C^T = CVC^T\n$$\nTherefore, the distribution of the estimated contrasts is:\n$$\nC\\hat{\\beta} \\sim \\mathcal{N}(C\\beta, CVC^T)\n$$\nUnder the null hypothesis $H_0: C\\beta = \\mathbf{0}$, the distribution simplifies to:\n$$\nC\\hat{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, CVC^T)\n$$\nFor a $q$-dimensional random vector $X \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$, the quadratic form $X^T \\Sigma^{-1} X$ follows a chi-squared distribution with $q$ degrees of freedom, $X^T \\Sigma^{-1} X \\sim \\chi^2_q$.\nApplying this theorem to our case, we set $X = C\\hat{\\beta}$ and $\\Sigma = CVC^T$. The dimension $q$ is the number of contrasts (rows in $C$), which is $q=2$.\nThe Wald test statistic $W$ is therefore the quadratic form:\n$$\nW = (C\\hat{\\beta})^T (CVC^T)^{-1} (C\\hat{\\beta})\n$$\nIn practice, the true covariance matrix $V$ is unknown and is replaced by its consistent estimate $\\hat{V} = \\widehat{\\text{Var}}(\\hat{\\beta})$. The resulting test statistic is:\n$$\nW = (C\\hat{\\beta})^T (C\\hat{V}C^T)^{-1} (C\\hat{\\beta}) \\sim \\chi^2_q\n$$\nThis is the general formula for the Wald statistic for the linear hypothesis $H_0: C\\beta = \\mathbf{0}$.\n\n**3. Compute the Numerical Value of the Wald Test Statistic**\n\nWe now compute the value of $W$ using the provided data.\n\nFirst, compute the vector of estimated contrasts, $C\\hat{\\beta}$:\n$$\nC\\hat{\\beta} = \\begin{pmatrix} -1  1  0  0 \\\\ 0  0  -1  1 \\end{pmatrix} \\begin{pmatrix} 132.0 \\\\ 128.5 \\\\ 127.0 \\\\ 125.5 \\end{pmatrix} = \\begin{pmatrix} 128.5 - 132.0 \\\\ 125.5 - 127.0 \\end{pmatrix} = \\begin{pmatrix} -3.5 \\\\ -1.5 \\end{pmatrix}\n$$\n\nNext, compute the covariance matrix of the estimated contrasts, $\\Sigma_C = C\\hat{V}C^T$:\n$$\n\\Sigma_C = \\begin{pmatrix} -1  1  0  0 \\\\ 0  0  -1  1 \\end{pmatrix} \\begin{pmatrix}\n0.49  0.22  0.22  0.22 \\\\\n0.22  0.49  0.22  0.22 \\\\\n0.22  0.22  0.49  0.22 \\\\\n0.22  0.22  0.22  0.49\n\\end{pmatrix} \\begin{pmatrix} -1  0 \\\\ 1  0 \\\\ 0  -1 \\\\ 0  1 \\end{pmatrix}\n$$\nThe variance of the first contrast $(\\mu_1 - \\mu_0)$ is the $(1,1)$ element of $\\Sigma_C$:\n$$\n\\text{Var}(\\hat{\\mu}_1 - \\hat{\\mu}_0) = \\text{Var}(\\hat{\\mu}_1) + \\text{Var}(\\hat{\\mu}_0) - 2\\text{Cov}(\\hat{\\mu}_1, \\hat{\\mu}_0) = 0.49 + 0.49 - 2(0.22) = 0.98 - 0.44 = 0.54\n$$\nThe variance of the second contrast $(\\mu_3 - \\mu_2)$ is the $(2,2)$ element of $\\Sigma_C$:\n$$\n\\text{Var}(\\hat{\\mu}_3 - \\hat{\\mu}_2) = \\text{Var}(\\hat{\\mu}_3) + \\text{Var}(\\hat{\\mu}_2) - 2\\text{Cov}(\\hat{\\mu}_3, \\hat{\\mu}_2) = 0.49 + 0.49 - 2(0.22) = 0.98 - 0.44 = 0.54\n$$\nThe covariance between the two contrasts is the $(1,2)$ and $(2,1)$ element of $\\Sigma_C$:\n$$\n\\text{Cov}(\\hat{\\mu}_1 - \\hat{\\mu}_0, \\hat{\\mu}_3 - \\hat{\\mu}_2) = \\text{Cov}(\\hat{\\mu}_1, \\hat{\\mu}_3) - \\text{Cov}(\\hat{\\mu}_1, \\hat{\\mu}_2) - \\text{Cov}(\\hat{\\mu}_0, \\hat{\\mu}_3) + \\text{Cov}(\\hat{\\mu}_0, \\hat{\\mu}_2)\n$$\nFrom the matrix $\\hat{V}$, all these covariance terms are equal to $0.22$.\n$$\n\\text{Cov}(\\dots) = 0.22 - 0.22 - 0.22 + 0.22 = 0\n$$\nSo, the covariance matrix of the contrasts is:\n$$\n\\Sigma_C = \\begin{pmatrix} 0.54  0 \\\\ 0  0.54 \\end{pmatrix}\n$$\nNow, compute the inverse of $\\Sigma_C$:\n$$\n\\Sigma_C^{-1} = \\begin{pmatrix} 0.54  0 \\\\ 0  0.54 \\end{pmatrix}^{-1} = \\begin{pmatrix} \\frac{1}{0.54}  0 \\\\ 0  \\frac{1}{0.54} \\end{pmatrix}\n$$\nFinally, compute the Wald statistic $W$:\n$$\nW = (C\\hat{\\beta})^T \\Sigma_C^{-1} (C\\hat{\\beta}) = \\begin{pmatrix} -3.5  -1.5 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{0.54}  0 \\\\ 0  \\frac{1}{0.54} \\end{pmatrix} \\begin{pmatrix} -3.5 \\\\ -1.5 \\end{pmatrix}\n$$\n$$\nW = \\frac{(-3.5)^2}{0.54} + \\frac{(-1.5)^2}{0.54} = \\frac{12.25 + 2.25}{0.54} = \\frac{14.5}{0.54}\n$$\nCalculating the numerical value:\n$$\nW = \\frac{14.5}{0.54} \\approx 26.85185...\n$$\nRounding to four significant figures, the value of the Wald test statistic is $26.85$.\nThis value would be compared to a chi-squared distribution with $q=2$ degrees of freedom to obtain a p-value for the joint hypothesis test. For instance, the critical value for $\\alpha=0.05$ is $\\chi^2_{2, 0.95} \\approx 5.99$. Since $26.85  5.99$, the null hypothesis would be rejected at this significance level.", "answer": "$$\n\\boxed{26.85}\n$$", "id": "4951162"}, {"introduction": "A crucial part of building a mixed model is justifying the inclusion of random effects, which involves testing if their variance is significantly different from zero. This seemingly straightforward test is complicated by the fact that variance components cannot be negative, placing the null hypothesis on the boundary of the parameter space. This exercise challenges you to derive the correct asymptotic distribution for the likelihood ratio test in this special case and apply it to determine if between-subject variability is a necessary component of the model. [@problem_id:4951121]", "problem": "Consider a longitudinal study of systolic blood pressure in adults receiving antihypertensive therapy, with repeated measurements across clinical visits. For patient $i \\in \\{1,\\dots,n\\}$ and visit $j \\in \\{1,\\dots,m_i\\}$, let the observed systolic blood pressure be $Y_{ij}$. A commonly used linear mixed-effects model for repeated measures is the random-intercept model\n$$\nY_{ij} \\;=\\; \\beta_0 \\;+\\; \\beta_1 t_{ij} \\;+\\; b_i \\;+\\; \\varepsilon_{ij},\n$$\nwhere $t_{ij}$ is the visit time (in months) centered at baseline, $b_i \\sim \\mathcal{N}(0,\\sigma_b^2)$ is the subject-specific random intercept and $\\varepsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^2)$ is the residual error, independent of $b_i$. Stack the observations into a vector $Y$ with fixed-effects design matrix $X$ and random-effects design matrix $Z$. Under standard assumptions, $Y \\sim \\mathcal{N}(X\\beta, V)$ with $V = ZGZ^{\\top} + R$, $G = \\sigma_b^2 I_n$, and $R = \\sigma^2 I_{N}$, where $N = \\sum_{i=1}^{n} m_i$.\n\nYou are interested in testing whether between-subject heterogeneity is present. Formally, consider\n$$\nH_0: \\sigma_b^2 = 0 \\quad \\text{versus} \\quad H_1: \\sigma_b^2  0,\n$$\nwhere $\\sigma_b^2$ is a variance component that is constrained to be nonnegative. Starting from the general maximum likelihood framework and the properties of the multivariate normal likelihood, derive the Likelihood Ratio Test (LRT) statistic for testing $H_0$ versus $H_1$ in this boundary case and, using first-principles asymptotics based on the score function and Fisher information, justify the correct reference distribution for the LRT under $H_0$.\n\nSuppose a maximum likelihood fit on a dataset with $n$ patients and balanced visits yields an optimized log-likelihood under the full model (with $\\sigma_b^2 \\ge 0$) of $l_1 = -838.015$ and an optimized log-likelihood under the null model (with $\\sigma_b^2 = 0$) of $l_0 = -840.015$. Compute the observed LRT statistic and the corresponding $p$-value under the appropriate reference distribution you derive. Round your final $p$-value to four significant figures and express it as a decimal.", "solution": "The user wants me to solve a problem related to hypothesis testing in linear mixed-effects models.\n\n**Part 1: Problem Validation**\n\n1.  **Extract Givens:**\n    *   Model: $Y_{ij} = \\beta_0 + \\beta_1 t_{ij} + b_i + \\varepsilon_{ij}$\n    *   $i$: patient index, $i \\in \\{1,\\dots,n\\}$\n    *   $j$: visit index, $j \\in \\{1,\\dots,m_i\\}$\n    *   $Y_{ij}$: observed systolic blood pressure\n    *   $t_{ij}$: visit time, centered\n    *   $b_i$: random intercept, $b_i \\sim \\mathcal{N}(0,\\sigma_b^2)$\n    *   $\\varepsilon_{ij}$: residual error, $\\varepsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^2)$\n    *   $b_i$ and $\\varepsilon_{ij}$ are independent.\n    *   Hypothesis test: $H_0: \\sigma_b^2 = 0$ versus $H_1: \\sigma_b^2  0$.\n    *   The parameter $\\sigma_b^2$ is constrained to be non-negative.\n    *   Data: Maximized log-likelihood under the full model ($l_1$) is $-838.015$. Maximized log-likelihood under the null model ($l_0$) is $-840.015$.\n\n2.  **Validate Using Extracted Givens:**\n    *   **Scientific Grounding:** The problem is grounded in the well-established statistical theory of linear mixed-effects models and likelihood ratio tests. This framework is a standard method for analyzing longitudinal data in medical research and other fields.\n    *   **Well-Posed:** The problem is well-posed. It clearly defines the statistical model, the parameters, the distributions, and the specific hypothesis to be tested. The request to derive the asymptotic distribution of the test statistic under a boundary condition is a standard, albeit advanced, problem in statistical theory with a unique, well-known solution. The provided data are sufficient for the requested calculation.\n    *   **Objectivity:** The problem is stated using precise, objective, and standard statistical terminology.\n    *   **Completeness and Consistency:** The problem is self-contained and consistent. It provides all necessary information and definitions.\n    *   **Realism and Feasibility:** The scenario (monitoring blood pressure) and the statistical model are entirely realistic. The provided log-likelihood values are plausible.\n    *   **Structure and Complexity:** The problem is not ill-posed, trivial, or pseudo-profound. The need to address the boundary condition ($\\sigma_b^2 = 0$) violates the standard regularity conditions of Wilks' theorem, requiring a more fundamental derivation of the reference distribution. This represents a genuine conceptual challenge.\n    *   **Verifiability:** The derivation and the final result are mathematically verifiable against established statistical literature (e.g., Self  Liang, 1987; Stram  Lee, 1994).\n\n3.  **Verdict and Action:**\n    The problem is **valid**. I will proceed with a full solution.\n\n**Part 2: Solution Derivation**\n\nThe problem requires the derivation of the Likelihood Ratio Test (LRT) statistic and its asymptotic distribution for testing a variance component on the boundary of its parameter space, followed by a calculation using provided data.\n\nThe general form of the LRT statistic is given by\n$$\n\\lambda = 2(l_1 - l_0)\n$$\nwhere $l_1$ is the maximized log-likelihood under the full model (which includes the parameter of interest) and $l_0$ is the maximized log-likelihood under the null hypothesis. In this problem, the null hypothesis is $H_0: \\sigma_b^2 = 0$ and the alternative is $H_1: \\sigma_b^2  0$. The parameter space for the variance component $\\sigma_b^2$ is $[0, \\infty)$.\n\nAccording to standard asymptotic theory (Wilks' theorem), under certain regularity conditions, the LRT statistic $\\lambda$ follows a chi-squared distribution under $H_0$ with degrees of freedom equal to the difference in the number of free parameters between the full and null models. Here, the full model has parameters $\\{\\beta_0, \\beta_1, \\sigma^2, \\sigma_b^2\\}$ and the null model has $\\{\\beta_0, \\beta_1, \\sigma^2\\}$. The difference is one parameter, $\\sigma_b^2$. A naive application of Wilks' theorem would suggest $\\lambda \\sim \\chi_1^2$. However, this is incorrect because a critical regularity condition is violated: the null value of the parameter ($\\sigma_b^2 = 0$) lies on the boundary of the parameter space $[0, \\infty)$, not in its interior.\n\nTo find the correct asymptotic distribution, we must use a more fundamental approach based on the properties of the score function and the behavior of the maximum likelihood estimator (MLE) at the boundary. Let $\\theta = (\\boldsymbol{\\beta}, \\sigma^2, \\sigma_b^2)$ be the vector of all parameters. Let $l(\\theta)$ be the log-likelihood function. The MLE under the full model, $\\hat{\\theta}_1$, is found by maximizing $l(\\theta)$ subject to the constraint $\\sigma_b^2 \\ge 0$. The MLE under the null, $\\hat{\\theta}_0$, is found by maximizing $l(\\theta)$ with $\\sigma_b^2$ fixed at $0$.\n\nConsider the score function for $\\sigma_b^2$, which is the partial derivative of the log-likelihood with respect to $\\sigma_b^2$, evaluated under the null hypothesis at the null MLEs for the other parameters: $S_b = \\frac{\\partial l}{\\partial \\sigma_b^2}|_{\\sigma_b^2=0, \\boldsymbol{\\beta}=\\hat{\\boldsymbol{\\beta}}_0, \\sigma^2=\\hat{\\sigma}_0^2}$. Under $H_0$, the score function $S_b$ is asymptotically normal with a mean of $0$ and a variance given by the corresponding element of the Fisher information matrix.\n\nThe behavior of the LRT depends on the sign of $S_b$. If $S_b  0$, the likelihood increases as we move away from $\\sigma_b^2=0$ into the permissible region $\\sigma_b^2  0$. In this situation, the constrained MLE under the full model, $\\hat{\\sigma}_{b,1}^2$, will be positive ($\\hat{\\sigma}_{b,1}^2  0$). When the MLE is in the interior of the parameter space, the standard asymptotic arguments hold. The LRT statistic can be approximated by a quadratic form of the score statistic, $\\lambda \\approx S_b^2 / I_{bb}$, where $I_{bb}$ is the Fisher information for $\\sigma_b^2$. This quantity follows a $\\chi_1^2$ distribution.\n\nConversely, if $S_b \\le 0$, the likelihood function is maximized (or at least does not increase) by moving from $\\sigma_b^2=0$ toward negative values. Since the parameter space is constrained to $\\sigma_b^2 \\ge 0$, the MLE is forced to be at the boundary: $\\hat{\\sigma}_{b,1}^2 = 0$. In this case, the MLEs for all parameters under the full model are identical to those under the null model. Consequently, the maximized log-likelihoods are identical, $l_1 = l_0$, and the LRT statistic is $\\lambda = 2(l_1 - l_0) = 0$.\n\nSince the score function $S_b$ is asymptotically centered at $0$ under $H_0$, there is a probability of approximately $0.5$ that $S_b  0$ and a probability of approximately $0.5$ that $S_b \\le 0$. This leads to the conclusion that the asymptotic distribution of the LRT statistic $\\lambda$ is a mixture of two distributions:\n1.  With probability $0.5$, $\\lambda = 0$ (a point mass at zero, which can be denoted as a $\\chi_0^2$ distribution).\n2.  With probability $0.5$, $\\lambda$ follows a $\\chi_1^2$ distribution.\n\nTherefore, the asymptotic distribution of $\\lambda$ under $H_0$ is a $50:50$ mixture of a $\\chi_0^2$ and a $\\chi_1^2$ distribution:\n$$\n\\lambda \\sim \\frac{1}{2}\\chi_0^2 + \\frac{1}{2}\\chi_1^2\n$$\nThe $p$-value for an observed test statistic $\\lambda_{obs}$ is the probability of observing a value as extreme or more extreme under the null distribution. For a non-zero observed value $\\lambda_{obs}  0$:\n$$\np\\text{-value} = P(\\lambda \\ge \\lambda_{obs})\n$$\nUsing the mixture distribution:\n$$\nP(\\lambda \\ge \\lambda_{obs}) = \\frac{1}{2} P(\\chi_0^2 \\ge \\lambda_{obs}) + \\frac{1}{2} P(\\chi_1^2 \\ge \\lambda_{obs})\n$$\nSince $\\lambda_{obs}  0$, the probability $P(\\chi_0^2 \\ge \\lambda_{obs})$ is $0$. The expression simplifies to:\n$$\np\\text{-value} = \\frac{1}{2} P(\\chi_1^2 \\ge \\lambda_{obs})\n$$\nThis means the correct $p$-value is half of what would be calculated using a standard (and incorrect) $\\chi_1^2$ reference distribution.\n\nNow, we apply this theory to the given data.\nThe maximized log-likelihood under the full model is $l_1 = -838.015$.\nThe maximized log-likelihood under the null model is $l_0 = -840.015$.\n\nThe observed LRT statistic is:\n$$\n\\lambda_{obs} = 2(l_1 - l_0) = 2(-838.015 - (-840.015)) = 2(2.0) = 4.0\n$$\nSince $\\lambda_{obs}=4.0  0$, the MLE for $\\sigma_b^2$ was positive, and we are in the non-trivial case. The $p$-value is calculated as:\n$$\np\\text{-value} = \\frac{1}{2} P(\\chi_1^2 \\ge 4.0)\n$$\nA random variable $W$ follows a $\\chi_1^2$ distribution if $W = Z^2$ where $Z \\sim \\mathcal{N}(0,1)$. Thus,\n$$\nP(\\chi_1^2 \\ge 4.0) = P(Z^2 \\ge 4.0) = P(|Z| \\ge \\sqrt{4.0}) = P(|Z| \\ge 2.0)\n$$\nThis probability corresponds to the area in the two tails of a standard normal distribution beyond $Z=-2$ and $Z=2$. From standard normal tables or a calculator, $P(Z  2) \\approx 0.02275$. By symmetry, $P(Z  -2) \\approx 0.02275$.\nTherefore,\n$$\nP(\\chi_1^2 \\ge 4.0) = P(Z  2) + P(Z  -2) \\approx 0.02275 + 0.02275 = 0.0455\n$$\nThe final $p$-value is:\n$$\np\\text{-value} = \\frac{1}{2} \\times 0.0455 = 0.02275\n$$\nThe problem asks for the $p$-value to be rounded to four significant figures. The calculated value is $0.02275$. The first four significant figures are $2$, $2$, $7$, and $5$. The next digit is a $0$ (from a more precise calculation yielding $0.02275013...$), so no further rounding is needed. The final answer is $0.02275$.\nThis $p$-value is typically considered statistically significant (e.g., at an $\\alpha=0.05$ level), leading to the rejection of the null hypothesis $H_0: \\sigma_b^2 = 0$. This suggests that there is significant between-subject heterogeneity in systolic blood pressure that is captured by the random intercept.", "answer": "$$\n\\boxed{0.02275}\n$$", "id": "4951121"}]}