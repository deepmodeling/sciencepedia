{"hands_on_practices": [{"introduction": "Confidence intervals are fundamental to statistical inference, yet the theory underlying their calculation is often taken for granted. This first exercise tasks you with deriving the large-sample variance estimators for the logarithm of the three primary measures of association: the risk ratio ($RR$), odds ratio ($OR$), and incidence rate ratio ($IRR$) [@problem_id:4972058]. By applying the delta method from first principles, you will uncover the statistical foundation for the standard errors that are routinely produced by statistical software, solidifying your understanding of asymptotic theory for binomial and Poisson models.", "problem": "A prospective cohort study compares an exposed group and an unexposed group on a binary health outcome and, in a parallel surveillance system, compares incidence rates based on person-time. Let $a$ denote the number of outcomes in the exposed group and $b$ denote the number of non-outcomes in the exposed group, so that the exposed group size is $n_{1} = a + b$. Let $c$ denote the number of outcomes in the unexposed group and $d$ denote the number of non-outcomes in the unexposed group, so that the unexposed group size is $n_{0} = c + d$. Define the empirical risk ratio (RR), odds ratio (OR), and incidence rate ratio (IRR) by\n$$\\widehat{\\text{RR}} = \\frac{a / n_{1}}{c / n_{0}}, \\quad \\widehat{\\text{OR}} = \\frac{a / b}{c / d} = \\frac{a d}{b c}, \\quad \\widehat{\\text{IRR}} = \\frac{X_{1} / T_{1}}{X_{0} / T_{0}},$$\nwhere $X_{1}$ and $X_{0}$ denote independent Poisson event counts in the exposed and unexposed groups, respectively, and $T_{1}$ and $T_{0}$ denote the corresponding fixed person-time totals.\n\nAssume the following data-generating mechanisms:\n- In the cohort component, $a \\sim \\text{Binomial}(n_{1}, p_{1})$ and $c \\sim \\text{Binomial}(n_{0}, p_{0})$, independently, where $p_{1}$ and $p_{0}$ are the true risks in the exposed and unexposed groups.\n- In the rate component, $X_{1} \\sim \\text{Poisson}(\\lambda_{1} T_{1})$ and $X_{0} \\sim \\text{Poisson}(\\lambda_{0} T_{0})$, independently, where $\\lambda_{1}$ and $\\lambda_{0}$ are the true incidence rates.\n\nUsing only first principles of large-sample theory for Binomial and Poisson models and the delta method, derive first-order large-sample estimators for $\\widehat{\\operatorname{Var}}\\!\\left[\\ln\\!\\left(\\widehat{\\text{RR}}\\right)\\right]$, $\\widehat{\\operatorname{Var}}\\!\\left[\\ln\\!\\left(\\widehat{\\text{OR}}\\right)\\right]$, and $\\widehat{\\operatorname{Var}}\\!\\left[\\ln\\!\\left(\\widehat{\\text{IRR}}\\right)\\right]$ expressed purely in terms of $a, b, c, d, X_{1}, X_{0}, T_{1}, T_{0}$.\n\nYour final answer must be a single closed-form analytic expression, presented as a row matrix with the three entries in the order $\\widehat{\\operatorname{Var}}\\!\\left[\\ln\\!\\left(\\widehat{\\text{RR}}\\right)\\right]$, $\\widehat{\\operatorname{Var}}\\!\\left[\\ln\\!\\left(\\widehat{\\text{OR}}\\right)\\right]$, $\\widehat{\\operatorname{Var}}\\!\\left[\\ln\\!\\left(\\widehat{\\text{IRR}}\\right)\\right]$. Use the natural logarithm $\\ln$. No numerical evaluation, rounding, or units are required.", "solution": "The problem requires the derivation of large-sample estimators for the variance of the natural logarithm of the risk ratio ($\\widehat{\\text{RR}}$), odds ratio ($\\widehat{\\text{OR}}$), and incidence rate ratio ($\\widehat{\\text{IRR}}$). The core theoretical tool for this derivation is the delta method.\n\nThe delta method provides a first-order Taylor series approximation for the variance of a function of one or more random variables. For a single random variable $Y$ and a differentiable function $g$, the variance of $g(Y)$ is approximated as:\n$$ \\operatorname{Var}[g(Y)] \\approx \\left( g'(E[Y]) \\right)^2 \\operatorname{Var}[Y] $$\nFor a function of two independent random variables, $Y_1$ and $Y_2$, the variance of $g(Y_1, Y_2)$ is approximated as:\n$$ \\operatorname{Var}[g(Y_1, Y_2)] \\approx \\left( \\frac{\\partial g}{\\partial y_1} \\right)^2 \\operatorname{Var}[Y_1] + \\left( \\frac{\\partial g}{\\partial y_2} \\right)^2 \\operatorname{Var}[Y_2] $$\nwhere the partial derivatives are evaluated at the expected values of $Y_1$ and $Y_2$. A large-sample estimator for this variance, denoted $\\widehat{\\operatorname{Var}}$, is obtained by substituting the population parameters (including expected values and variances) with their sample-based estimates.\n\nThe three requested variance estimators are derived separately.\n\n1.  **Derivation of $\\widehat{\\operatorname{Var}}\\!\\left[\\ln\\!\\left(\\widehat{\\text{RR}}\\right)\\right]$**\n\nThe empirical risk ratio is defined as $\\widehat{\\text{RR}} = \\frac{\\hat{p}_{1}}{\\hat{p}_{0}}$, where $\\hat{p}_{1} = a/n_{1}$ and $\\hat{p}_{0} = c/n_{0}$ are the estimated risks in the exposed and unexposed groups, respectively. The natural logarithm is $\\ln(\\widehat{\\text{RR}}) = \\ln(\\hat{p}_{1}) - \\ln(\\hat{p}_{0})$.\nThe random variables are $a \\sim \\text{Binomial}(n_{1}, p_{1})$ and $c \\sim \\text{Binomial}(n_{0}, p_{0})$, which are independent. Consequently, the estimators $\\hat{p}_{1} = a/n_{1}$ and $\\hat{p}_{0} = c/n_{0}$ are also independent.\nDue to independence, the variance of the difference is the sum of the variances:\n$$ \\operatorname{Var}[\\ln(\\widehat{\\text{RR}})] = \\operatorname{Var}[\\ln(\\hat{p}_{1})] + \\operatorname{Var}[\\ln(\\hat{p}_{0})] $$\nWe apply the delta method to each term. Let $g(p) = \\ln(p)$, so $g'(p) = 1/p$.\nThe variance of $\\hat{p}_{1}$ is $\\operatorname{Var}[\\hat{p}_{1}] = \\operatorname{Var}[a/n_{1}] = \\frac{1}{n_{1}^2}\\operatorname{Var}[a] = \\frac{n_{1} p_{1}(1-p_{1})}{n_{1}^2} = \\frac{p_{1}(1-p_{1})}{n_{1}}$.\nApplying the delta method for $\\ln(\\hat{p}_{1})$:\n$$ \\operatorname{Var}[\\ln(\\hat{p}_{1})] \\approx \\left( g'(p_1) \\right)^2 \\operatorname{Var}[\\hat{p}_{1}] = \\left(\\frac{1}{p_{1}}\\right)^2 \\frac{p_{1}(1-p_{1})}{n_{1}} = \\frac{1-p_{1}}{n_{1} p_{1}} $$\nSimilarly, for the unexposed group:\n$$ \\operatorname{Var}[\\ln(\\hat{p}_{0})] \\approx \\frac{1-p_{0}}{n_{0} p_{0}} $$\nCombining these gives the approximate variance of $\\ln(\\widehat{\\text{RR}})$:\n$$ \\operatorname{Var}[\\ln(\\widehat{\\text{RR}})] \\approx \\frac{1-p_{1}}{n_{1} p_{1}} + \\frac{1-p_{0}}{n_{0} p_{0}} $$\nTo obtain the estimator $\\widehat{\\operatorname{Var}}\\!\\left[\\ln\\!\\left(\\widehat{\\text{RR}}\\right)\\right]$, we substitute the true probabilities $p_{1}$ and $p_{0}$ with their sample estimates $\\hat{p}_{1} = a/n_{1}$ and $\\hat{p}_{0} = c/n_{0}$.\n$$ \\widehat{\\operatorname{Var}}\\!\\left[\\ln\\!\\left(\\widehat{\\text{RR}}\\right)\\right] = \\frac{1-\\hat{p}_{1}}{n_{1}\\hat{p}_{1}} + \\frac{1-\\hat{p}_{0}}{n_{0}\\hat{p}_{0}} = \\frac{1 - a/n_{1}}{n_{1}(a/n_{1})} + \\frac{1 - c/n_{0}}{n_{0}(c/n_{0})} $$\n$$ = \\frac{(n_{1}-a)/n_{1}}{a} + \\frac{(n_{0}-c)/n_{0}}{c} = \\frac{b/n_{1}}{a} + \\frac{d/n_{0}}{c} = \\frac{b}{an_{1}} + \\frac{d}{cn_{0}} $$\nSince $n_{1} = a+b$ and $n_{0} = c+d$, this can be written as:\n$$ \\frac{b}{a(a+b)} + \\frac{d}{c(c+d)} = \\left(\\frac{1}{a} - \\frac{1}{a+b}\\right) + \\left(\\frac{1}{c} - \\frac{1}{c+d}\\right) $$\nThus, the final estimator is $\\frac{1}{a} - \\frac{1}{a+b} + \\frac{1}{c} - \\frac{1}{c+d}$.\n\n2.  **Derivation of $\\widehat{\\operatorname{Var}}\\!\\left[\\ln\\!\\left(\\widehat{\\text{OR}}\\right)\\right]$**\n\nThe empirical odds ratio is $\\widehat{\\text{OR}} = \\frac{a/b}{c/d}$. It is more convenient to express this in terms of proportions: $\\widehat{\\text{OR}} = \\frac{\\hat{p}_{1}/(1-\\hat{p}_{1})}{\\hat{p}_{0}/(1-\\hat{p}_{0})}$.\nThe natural logarithm of the odds ratio, known as the log-odds ratio, is:\n$$ \\ln(\\widehat{\\text{OR}}) = \\ln\\left(\\frac{\\hat{p}_{1}}{1-\\hat{p}_{1}}\\right) - \\ln\\left(\\frac{\\hat{p}_{0}}{1-\\hat{p}_{0}}\\right) = \\text{logit}(\\hat{p}_{1}) - \\text{logit}(\\hat{p}_{0}) $$\nAs before, $\\hat{p}_{1}$ and $\\hat{p}_{0}$ are independent. We apply the delta method to the logit function, $g(p) = \\text{logit}(p) = \\ln(p) - \\ln(1-p)$.\nThe derivative is $g'(p) = \\frac{1}{p} + \\frac{1}{1-p} = \\frac{1}{p(1-p)}$.\nThe variance of $\\text{logit}(\\hat{p}_{1})$ is:\n$$ \\operatorname{Var}[\\text{logit}(\\hat{p}_{1})] \\approx \\left( g'(p_1) \\right)^2 \\operatorname{Var}[\\hat{p}_{1}] = \\left(\\frac{1}{p_{1}(1-p_{1})}\\right)^2 \\frac{p_{1}(1-p_{1})}{n_{1}} = \\frac{1}{n_{1}p_{1}(1-p_{1})} $$\nBy independence, the variance of $\\ln(\\widehat{\\text{OR}})$ is:\n$$ \\operatorname{Var}[\\ln(\\widehat{\\text{OR}})] \\approx \\frac{1}{n_{1}p_{1}(1-p_{1})} + \\frac{1}{n_{0}p_{0}(1-p_{0})} $$\nThe estimator is found by substituting sample estimates for the parameters:\n$$ \\widehat{\\operatorname{Var}}\\!\\left[\\ln\\!\\left(\\widehat{\\text{OR}}\\right)\\right] = \\frac{1}{n_{1}\\hat{p}_{1}(1-\\hat{p}_{1})} + \\frac{1}{n_{0}\\hat{p}_{0}(1-\\hat{p}_{0})} $$\nNow, substitute $\\hat{p}_{1} = a/n_{1}$, $1-\\hat{p}_{1} = b/n_{1}$, $\\hat{p}_{0} = c/n_{0}$, and $1-\\hat{p}_{0} = d/n_{0}$:\n$$ \\widehat{\\operatorname{Var}}\\!\\left[\\ln\\!\\left(\\widehat{\\text{OR}}\\right)\\right] = \\frac{1}{n_{1}\\frac{a}{n_{1}}\\frac{b}{n_{1}}} + \\frac{1}{n_{0}\\frac{c}{n_{0}}\\frac{d}{n_{0}}} = \\frac{n_{1}}{ab} + \\frac{n_{0}}{cd} $$\nSubstituting $n_{1} = a+b$ and $n_{0} = c+d$:\n$$ \\frac{a+b}{ab} + \\frac{c+d}{cd} = \\left(\\frac{a}{ab} + \\frac{b}{ab}\\right) + \\left(\\frac{c}{cd} + \\frac{d}{cd}\\right) = \\frac{1}{b} + \\frac{1}{a} + \\frac{1}{d} + \\frac{1}{c} $$\nThus, the final estimator is $\\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}$.\n\n3.  **Derivation of $\\widehat{\\operatorname{Var}}\\!\\left[\\ln\\!\\left(\\widehat{\\text{IRR}}\\right)\\right]$**\n\nThe empirical incidence rate ratio is $\\widehat{\\text{IRR}} = \\frac{X_{1}/T_{1}}{X_{0}/T_{0}}$. Its natural logarithm is:\n$$ \\ln(\\widehat{\\text{IRR}}) = \\ln(X_{1}) - \\ln(X_{0}) - \\ln(T_{1}) + \\ln(T_{0}) $$\nThe random variables are the event counts $X_{1} \\sim \\text{Poisson}(\\lambda_{1} T_{1})$ and $X_{0} \\sim \\text{Poisson}(\\lambda_{0} T_{0})$, which are independent. The person-time totals $T_{1}$ and $T_{0}$ are fixed constants.\nThe variance of the sum is the sum of the variances (constants have zero variance):\n$$ \\operatorname{Var}[\\ln(\\widehat{\\text{IRR}})] = \\operatorname{Var}[\\ln(X_{1})] + \\operatorname{Var}[\\ln(X_{0})] $$\nWe apply the delta method to $g(X) = \\ln(X)$, for which $g'(X) = 1/X$. For the Poisson distribution, the variance is equal to the mean, so $\\operatorname{Var}[X_{1}] = E[X_{1}] = \\lambda_{1} T_{1}$.\nApplying the delta method for $\\ln(X_{1})$:\n$$ \\operatorname{Var}[\\ln(X_{1})] \\approx \\left( g'(E[X_{1}]) \\right)^2 \\operatorname{Var}[X_{1}] = \\left(\\frac{1}{E[X_{1}]}\\right)^2 \\operatorname{Var}[X_{1}] = \\frac{\\operatorname{Var}[X_{1}]}{(E[X_{1}])^2} = \\frac{\\lambda_{1} T_{1}}{(\\lambda_{1} T_{1})^2} = \\frac{1}{\\lambda_{1} T_{1}} $$\nSimilarly, for the unexposed group, $\\operatorname{Var}[\\ln(X_{0})] \\approx \\frac{1}{\\lambda_{0} T_{0}}$.\nThe approximate variance of $\\ln(\\widehat{\\text{IRR}})$ is:\n$$ \\operatorname{Var}[\\ln(\\widehat{\\text{IRR}})] \\approx \\frac{1}{\\lambda_{1} T_{1}} + \\frac{1}{\\lambda_{0} T_{0}} $$\nTo form the estimator $\\widehat{\\operatorname{Var}}\\!\\left[\\ln\\!\\left(\\widehat{\\text{IRR}}\\right)\\right]$, we must estimate the parameters $\\mu_{1}=\\lambda_{1}T_{1}$ and $\\mu_{0}=\\lambda_{0}T_{0}$. For a Poisson distribution, the maximum likelihood estimate of the mean parameter $\\mu$ is the observed count. Therefore, we substitute $\\lambda_{1}T_{1}$ with $X_{1}$ and $\\lambda_{0}T_{0}$ with $X_{0}$.\n$$ \\widehat{\\operatorname{Var}}\\!\\left[\\ln\\!\\left(\\widehat{\\text{IRR}}\\right)\\right] = \\frac{1}{X_{1}} + \\frac{1}{X_{0}} $$\nThis is the final estimator for the variance of the log-incidence rate ratio.\n\nThe three derived large-sample estimators are assembled into a row matrix as requested.", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{1}{a} - \\frac{1}{a+b} + \\frac{1}{c} - \\frac{1}{c+d}  \\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}  \\frac{1}{X_{1}} + \\frac{1}{X_{0}} \\end{pmatrix} } $$", "id": "4972058"}, {"introduction": "When analyzing data from matched-pair studies, standard logistic regression is biased due to the large number of nuisance parameters introduced by the matching. This practice explores the elegant solution provided by conditional logistic regression, a cornerstone of modern epidemiology [@problem_id:4972042]. You will derive the conditional likelihood from scratch, seeing precisely how conditioning on the number of cases per pair eliminates the nuisance parameters and leads to an intuitive estimator for the matched odds ratio based solely on discordant pairs.", "problem": "A $1{:}1$ matched case-control study evaluates the association between a binary exposure $X \\in \\{0,1\\}$ and a binary outcome $Y \\in \\{0,1\\}$, where $Y=1$ indicates a case and $Y=0$ indicates a control. Each matched pair $i$ consists of two individuals who are comparable on measured matching factors (for example, age, clinic, and calendar time), and within-pair case-control status is determined by $Y$. Assume the following logistic regression model with a stratum-specific intercept for each pair:\n$$\n\\operatorname{logit}\\big(\\Pr(Y=1 \\mid X=x, \\text{pair } i)\\big) \\;=\\; \\alpha_i + \\beta x,\n$$\nwhere $\\alpha_i$ is an unknown nuisance parameter specific to pair $i$, and $\\beta$ is the log-odds ratio parameter of interest associated with $X$.\n\nUsing only the definitions of the logistic function, odds, and conditional probability, and without appealing to any pre-derived conditional likelihood shortcuts:\n\n- Derive the conditional likelihood contribution for a single matched pair $i$ by conditioning on the event that exactly one subject in the pair is a case (that is, $\\sum_{j=1}^{2} Y_{ij} = 1$), and show explicitly how the nuisance parameter $\\alpha_i$ is eliminated by this conditioning.\n- Combine the pairwise conditional contributions across independent pairs to express the full conditional likelihood in terms of the counts of discordant pairs, where a discordant pair is one in which the case and the control have different exposure values. Let $m_{10}$ denote the number of pairs where the case is exposed ($X=1$) and the control is unexposed ($X=0$), and let $m_{01}$ denote the number of pairs where the case is unexposed ($X=0$) and the control is exposed.\n- Using your derived conditional likelihood, obtain the maximum likelihood estimator for $\\exp(\\beta)$ in terms of $m_{10}$ and $m_{01}$. Interpret $\\exp(\\beta)$ as a matched Odds Ratio (OR), defining Odds Ratio (OR) on its first use, and explain how conditioning controls for the matching factors.\n- For a dataset with $m_{10} = 18$, $m_{01} = 12$, and the remaining pairs concordant, compute the numerical value of $\\exp(\\hat{\\beta})$. Provide your final answer as a single number with no units; no rounding is required.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- Study Design: A $1{:}1$ matched case-control study.\n- Pair Structure: Each pair $i$ consists of two individuals, one case ($Y=1$) and one control ($Y=0$). This implies for each pair $i$ with individuals $j=1,2$, we have $\\sum_{j=1}^{2} Y_{ij} = 1$.\n- Exposure Variable: A binary variable $X \\in \\{0, 1\\}$.\n- Outcome Variable: A binary variable $Y \\in \\{0, 1\\}$.\n- Logistic Regression Model: $\\operatorname{logit}\\big(\\Pr(Y=1 \\mid X=x, \\text{pair } i)\\big) \\;=\\; \\alpha_i + \\beta x$.\n- Parameters: $\\alpha_i$ is a pair-specific nuisance parameter. $\\beta$ is the log-odds ratio parameter of interest.\n- Definitions: A discordant pair is one where the case and control have different exposure values. $m_{10}$ is the count of pairs where the case is exposed ($X=1$) and the control is unexposed ($X=0$). $m_{01}$ is the count of pairs where the case is unexposed ($X=0$) and the control is exposed ($X=1$).\n- Data: For a specific dataset, $m_{10} = 18$ and $m_{01} = 12$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem describes conditional logistic regression for matched pairs, a standard and fundamental technique in epidemiology and biostatistics for analyzing case-control studies. The model and the derivation requested are central to this field of study. The problem is scientifically and mathematically sound.\n- **Well-Posed:** The problem provides all necessary information and definitions to proceed through a logical sequence of derivations to a unique solution. The question is clearly structured.\n- **Objective:** The problem is stated in precise, formal mathematical and statistical language, free of ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation and Solution\n\nThe problem requires a four-part solution: deriving the conditional likelihood for a single pair, extending it to the full likelihood, finding the maximum likelihood estimator (MLE), and calculating its value for the given data.\n\n**Part 1: Conditional Likelihood for a Single Pair**\n\nLet the two individuals in matched pair $i$ be indexed by $j=1$ and $j=2$. Their respective exposures are $X_{i1}$ and $X_{i2}$. The logistic model is given by $\\operatorname{logit}\\big(\\Pr(Y_{ij}=1 \\mid X_{ij}, i)\\big) = \\alpha_i + \\beta X_{ij}$. The logistic function is $p = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)}$, where $\\eta$ is the linear predictor. Thus, the probability of an individual $j$ in pair $i$ being a case ($Y_{ij}=1$) is:\n$$\nP_{ij} = \\Pr(Y_{ij}=1 \\mid X_{ij}, i) \\;=\\; \\frac{\\exp(\\alpha_i + \\beta X_{ij})}{1 + \\exp(\\alpha_i + \\beta X_{ij})}\n$$\nAnd the probability of being a control ($Y_{ij}=0$) is:\n$$\n1 - P_{ij} = \\Pr(Y_{ij}=0 \\mid X_{ij}, i) \\;=\\; \\frac{1}{1 + \\exp(\\alpha_i + \\beta X_{ij})}\n$$\nBy design, each pair has exactly one case and one control, so we condition on the event $\\sum_{j=1}^{2} Y_{ij} = 1$. Let's denote the observed exposures for the two individuals as $(x_{i1}, x_{i2})$ and their outcomes as $(y_{i1}, y_{i2})$. Without loss of generality, let's say individual $1$ is the case ($y_{i1}=1$) and individual $2$ is the control ($y_{i2}=0$). The contribution of this pair to the conditional likelihood is the probability of this specific outcome configuration, given that one of them is a case and the other is a control.\n$$\nL_i(\\beta) = \\Pr(Y_{i1}=1, Y_{i2}=0 \\mid \\sum_{j=1}^{2} Y_{ij} = 1, X_{i1}=x_{i1}, X_{i2}=x_{i2})\n$$\nUsing the definition of conditional probability, $P(A \\mid B) = P(A \\cap B) / P(B)$:\n$$\nL_i(\\beta) = \\frac{\\Pr(Y_{i1}=1, Y_{i2}=0 \\mid x_{i1}, x_{i2})}{\\Pr(\\sum_{j=1}^{2} Y_{ij} = 1 \\mid x_{i1}, x_{i2})}\n$$\nAssuming the outcomes of the two individuals are independent conditional on their covariates (which includes the stratum $\\alpha_i$), the numerator is:\n$$\n\\Pr(Y_{i1}=1, Y_{i2}=0 \\mid x_{i1}, x_{i2}) = P_{i1}(1-P_{i2}) = \\frac{\\exp(\\alpha_i + \\beta x_{i1})}{1 + \\exp(\\alpha_i + \\beta x_{i1})} \\cdot \\frac{1}{1 + \\exp(\\alpha_i + \\beta x_{i2})}\n$$\nThe denominator is the total probability of the conditioning event, which has two possibilities: individual $1$ is the case and $2$ is the control, or individual $1$ is the control and $2$ is the case.\n$$\n\\Pr(\\sum_{j=1}^{2} Y_{ij} = 1 \\mid x_{i1}, x_{i2}) = P_{i1}(1-P_{i2}) + (1-P_{i1})P_{i2}\n$$\n$$\n= \\frac{\\exp(\\alpha_i + \\beta x_{i1})}{(1+\\exp(\\alpha_i + \\beta x_{i1}))(1+\\exp(\\alpha_i + \\beta x_{i2}))} + \\frac{\\exp(\\alpha_i + \\beta x_{i2})}{(1+\\exp(\\alpha_i + \\beta x_{i1}))(1+\\exp(\\alpha_i + \\beta x_{i2}))}\n$$\n$$\n= \\frac{\\exp(\\alpha_i + \\beta x_{i1}) + \\exp(\\alpha_i + \\beta x_{i2})}{(1+\\exp(\\alpha_i + \\beta x_{i1}))(1+\\exp(\\alpha_i + \\beta x_{i2}))}\n$$\nDividing the numerator by the denominator, the common factor in the denominators cancels out:\n$$\nL_i(\\beta) = \\frac{\\exp(\\alpha_i + \\beta x_{i1})}{\\exp(\\alpha_i + \\beta x_{i1}) + \\exp(\\alpha_i + \\beta x_{i2})}\n$$\nTo show the elimination of the nuisance parameter $\\alpha_i$, we factor $\\exp(\\alpha_i)$ from the numerator and denominator:\n$$\nL_i(\\beta) = \\frac{\\exp(\\alpha_i) \\exp(\\beta x_{i1})}{\\exp(\\alpha_i) \\exp(\\beta x_{i1}) + \\exp(\\alpha_i) \\exp(\\beta x_{i2})} = \\frac{\\exp(\\alpha_i) \\exp(\\beta x_{i1})}{\\exp(\\alpha_i) (\\exp(\\beta x_{i1}) + \\exp(\\beta x_{i2}))}\n$$\nCanceling $\\exp(\\alpha_i)$, we get the conditional likelihood contribution for pair $i$, where individual $1$ is the case and individual $2$ is the control:\n$$\nL_i(\\beta) = \\frac{\\exp(\\beta x_{i1})}{\\exp(\\beta x_{i1}) + \\exp(\\beta x_{i2})}\n$$\nThis expression depends only on the exposures of the two individuals in the pair and the parameter of interest $\\beta$, thus successfully eliminating the nuisance parameter $\\alpha_i$.\n\n**Part 2: Full Conditional Likelihood**\n\nLet's denote the exposure of the case in pair $i$ as $x_{ic}$ and the exposure of the control as $x_{ik}$. The conditional likelihood for pair $i$ is the probability that the individual with exposure $x_{ic}$ is the case, given the set of exposures $\\{x_{ic}, x_{ik}\\}$:\n$$\nL_i(\\beta) = \\frac{\\exp(\\beta x_{ic})}{\\exp(\\beta x_{ic}) + \\exp(\\beta x_{ik})}\n$$\nThe full conditional likelihood, $L_C(\\beta)$, is the product of the contributions from all $N$ independent pairs:\n$$\nL_C(\\beta) = \\prod_{i=1}^{N} \\frac{\\exp(\\beta x_{ic})}{\\exp(\\beta x_{ic}) + \\exp(\\beta x_{ik})}\n$$\nWe can group the pairs into four types:\n1.  Concordant exposed pairs ($m_{11}$ pairs): $x_{ic}=1, x_{ik}=1$. Contribution is $\\frac{\\exp(\\beta \\cdot 1)}{\\exp(\\beta \\cdot 1) + \\exp(\\beta \\cdot 1)} = \\frac{1}{2}$.\n2.  Concordant unexposed pairs ($m_{00}$ pairs): $x_{ic}=0, x_{ik}=0$. Contribution is $\\frac{\\exp(\\beta \\cdot 0)}{\\exp(\\beta \\cdot 0) + \\exp(\\beta \\cdot 0)} = \\frac{1}{2}$.\n3.  Discordant pairs ($m_{10}$ pairs): $x_{ic}=1, x_{ik}=0$. Contribution is $\\frac{\\exp(\\beta \\cdot 1)}{\\exp(\\beta \\cdot 1) + \\exp(\\beta \\cdot 0)} = \\frac{\\exp(\\beta)}{1+\\exp(\\beta)}$.\n4.  Discordant pairs ($m_{01}$ pairs): $x_{ic}=0, x_{ik}=1$. Contribution is $\\frac{\\exp(\\beta \\cdot 0)}{\\exp(\\beta \\cdot 0) + \\exp(\\beta \\cdot 1)} = \\frac{1}{1+\\exp(\\beta)}$.\n\nThe full conditional likelihood is the product of these terms raised to the power of their respective counts:\n$$\nL_C(\\beta) = \\left(\\frac{1}{2}\\right)^{m_{11}} \\left(\\frac{1}{2}\\right)^{m_{00}} \\left(\\frac{\\exp(\\beta)}{1+\\exp(\\beta)}\\right)^{m_{10}} \\left(\\frac{1}{1+\\exp(\\beta)}\\right)^{m_{01}}\n$$\n$$\nL_C(\\beta) = \\left(\\frac{1}{2}\\right)^{m_{11}+m_{00}} \\frac{[\\exp(\\beta)]^{m_{10}}}{[1+\\exp(\\beta)]^{m_{10}}[1+\\exp(\\beta)]^{m_{01}}}\n$$\n$$\nL_C(\\beta) = \\left(\\frac{1}{2}\\right)^{m_{11}+m_{00}} \\frac{\\exp(\\beta m_{10})}{(1+\\exp(\\beta))^{m_{10}+m_{01}}}\n$$\nAs shown, only the discordant pairs ($m_{10}$ and $m_{01}$) contribute information for the estimation of $\\beta$.\n\n**Part 3: Maximum Likelihood Estimator and Interpretation**\n\nTo find the MLE for $\\beta$, we maximize the log-likelihood $\\ell_C(\\beta) = \\ln(L_C(\\beta))$:\n$$\n\\ell_C(\\beta) = (m_{11}+m_{00})\\ln(1/2) + m_{10}\\beta - (m_{10}+m_{01})\\ln(1+\\exp(\\beta))\n$$\nWe take the derivative with respect to $\\beta$ and set it to zero:\n$$\n\\frac{d\\ell_C(\\beta)}{d\\beta} = m_{10} - (m_{10}+m_{01}) \\frac{\\exp(\\beta)}{1+\\exp(\\beta)}\n$$\nSetting the derivative to zero to find the MLE $\\hat{\\beta}$:\n$$\nm_{10} - (m_{10}+m_{01}) \\frac{\\exp(\\hat{\\beta})}{1+\\exp(\\hat{\\beta})} = 0\n$$\n$$\nm_{10} = (m_{10}+m_{01}) \\frac{\\exp(\\hat{\\beta})}{1+\\exp(\\hat{\\beta})}\n$$\n$$\nm_{10}(1+\\exp(\\hat{\\beta})) = (m_{10}+m_{01})\\exp(\\hat{\\beta})\n$$\n$$\nm_{10} + m_{10}\\exp(\\hat{\\beta}) = m_{10}\\exp(\\hat{\\beta}) + m_{01}\\exp(\\hat{\\beta})\n$$\n$$\nm_{10} = m_{01}\\exp(\\hat{\\beta})\n$$\nSolving for $\\exp(\\hat{\\beta})$, the MLE of the odds ratio, yields:\n$$\n\\exp(\\hat{\\beta}) = \\frac{m_{10}}{m_{01}}\n$$\nInterpretation: An Odds Ratio (OR) is a measure of association between an exposure and an outcome. It represents the odds that an outcome will occur given a particular exposure, compared to the odds of the outcome occurring in the absence of that exposure. From our model, $\\ln(\\text{Odds}(Y=1)) = \\alpha_i + \\beta x$. The odds ratio for exposure ($X=1$) versus non-exposure ($X=0$) within a stratum $i$ is:\n$$\nOR = \\frac{\\text{Odds}(Y=1 \\mid X=1, i)}{\\text{Odds}(Y=1 \\mid X=0, i)} = \\frac{\\exp(\\alpha_i + \\beta \\cdot 1)}{\\exp(\\alpha_i + \\beta \\cdot 0)} = \\frac{\\exp(\\alpha_i)\\exp(\\beta)}{\\exp(\\alpha_i)} = \\exp(\\beta)\n$$\nThus, $\\exp(\\beta)$ is the odds ratio, constant across all pairs. Its estimator, $\\exp(\\hat{\\beta}) = m_{10}/m_{01}$, is known as the matched Odds Ratio. It is the ratio of discordant pairs where the case was exposed to discordant pairs where the control was exposed.\n\nThe conditioning approach controls for matching factors such as age, sex, or clinical center. These factors are assumed to have a common effect on the baseline risk of the outcome for both individuals within a matched pair. This common effect is captured by the stratum-specific intercept $\\alpha_i$. By conditioning on the fact that each pair contains exactly one case and one control, we derive a likelihood that is independent of $\\alpha_i$. Therefore, the resulting estimate for $\\beta$ is not confounded by the factors that were used for matching, as their influence has been mathematically eliminated from the estimation process.\n\n**Part 4: Numerical Calculation**\n\nGiven the data: $m_{10} = 18$ and $m_{01} = 12$. The number of concordant pairs is not needed for the point estimate.\nThe numerical value of the MLE for the odds ratio $\\exp(\\beta)$ is:\n$$\n\\exp(\\hat{\\beta}) = \\frac{m_{10}}{m_{01}} = \\frac{18}{12} = \\frac{3}{2} = 1.5\n$$", "answer": "$$\n\\boxed{1.5}\n$$", "id": "4972042"}, {"introduction": "Shifting from the frequentist to the Bayesian paradigm offers a different and powerful approach to statistical inference, particularly in handling uncertainty and incorporating prior knowledge. This computational exercise guides you through a Bayesian analysis of the odds ratio using a Laplace approximation to the posterior distribution [@problem_id:4972024]. By comparing the results from a weakly informative prior to those from a strongly informative one, you will directly investigate the concept of prior sensitivity and appreciate how Bayesian methods can provide stable estimates even in challenging data scenarios, such as quasi-complete separation.", "problem": "You are given a binary-outcome, two-group study setting where a logistic regression with an intercept and a single binary exposure is used to model the event probability. The exposure coefficient represents the logarithm of the odds ratio (OR) between exposed and unexposed groups. Your task is to implement a principled, likelihood-based Bayesian computation to assess prior sensitivity: compare the posterior for the exposure coefficient (which equals the log-odds ratio, denoted as $ \\log(\\mathrm{OR}) $) under a weakly informative prior versus a strongly informative prior, and quantify how the $ 0.95 $ credible interval changes.\n\nFundamental base:\n- The logistic regression model with a binary predictor $ x \\in \\{0,1\\} $ and an intercept $ \\alpha $ posits $ \\mathrm{logit}(p_g) = \\alpha + \\beta x_g $ for group $ g \\in \\{0,1\\} $, where $ p_g $ is the event probability in group $ g $ and $ \\beta $ is the exposure coefficient. The odds ratio (OR) is defined as the ratio of odds of the event in exposed versus unexposed groups. In this model, the coefficient $ \\beta $ equals $ \\log(\\mathrm{OR}) $.\n- For grouped binomial data with $ y_g $ events out of $ n_g $ trials, the binomial likelihood is a well-tested model for binary outcomes. The Bernoulli-logit link is the core of logistic regression.\n- Bayesian inference combines the likelihood with prior distributions via Bayes’ theorem to yield the posterior distribution. A Laplace approximation uses a second-order Taylor expansion of the log-posterior around its mode (maximum a posteriori) to yield a multivariate normal approximation whose marginal for $ \\beta $ gives an approximate posterior for $ \\log(\\mathrm{OR}) $.\n- A $ 0.95 $ central credible interval is obtained by the $ 0.025 $ and $ 0.975 $ posterior quantiles.\n\nYour program must:\n- Implement the grouped-binomial logistic regression with parameter vector $ (\\alpha,\\beta) $ and compute the posterior under two priors for $ \\beta $:\n  - Weakly informative prior: $ \\beta \\sim \\mathcal{N}(0, 10^2) $.\n  - Strongly informative prior: $ \\beta \\sim \\mathcal{N}(m_\\beta, s_\\beta^2) $ with specified $ m_\\beta $ and $ s_\\beta $ per test case.\n- Use the same prior for the intercept in all cases: $ \\alpha \\sim \\mathcal{N}(0, 10^2) $.\n- For each prior, compute the maximum a posteriori estimate and the negative Hessian of the log-posterior at the mode, and use the Laplace approximation to obtain an approximate marginal posterior for $ \\beta $ that is normal with mean equal to the mode’s $ \\beta $ component and variance equal to the $ (\\beta,\\beta) $ element of the inverse of the observed information (negative Hessian).\n- From this approximate marginal posterior, compute the $ 0.95 $ central credible interval for $ \\beta $ as the interval between the $ 0.025 $ and $ 0.975 $ quantiles, expressed on the logarithmic scale. Do not convert to an odds ratio; remain on the $ \\log(\\mathrm{OR}) $ scale.\n- Quantify prior sensitivity by computing, for each test case, the following three quantities (weak minus strong):\n  - The difference in interval widths: $ \\text{width}_{\\text{weak}} - \\text{width}_{\\text{strong}} $.\n  - The difference in lower bounds: $ \\text{lower}_{\\text{weak}} - \\text{lower}_{\\text{strong}} $.\n  - The difference in upper bounds: $ \\text{upper}_{\\text{weak}} - \\text{upper}_{\\text{strong}} $.\n- Report each difference as a float rounded to $ 6 $ decimal places.\n\nTest suite:\nFor each case, data are given as counts $ (n_0, y_0) $ for the unexposed group and $ (n_1, y_1) $ for the exposed group, with strongly informative prior hyperparameters $ (m_\\beta, s_\\beta) $ for $ \\beta $. Use $ \\alpha \\sim \\mathcal{N}(0, 10^2) $ in all cases and a $ 0.95 $ credible level.\n\n- Case $ 1 $ (moderate information):\n  - Data: $ n_0 = 200 $, $ y_0 = 30 $, $ n_1 = 200 $, $ y_1 = 50 $.\n  - Strong prior: $ m_\\beta = \\log(1.3) $, $ s_\\beta = 0.35 $.\n- Case $ 2 $ (rare events, small sample):\n  - Data: $ n_0 = 30 $, $ y_0 = 1 $, $ n_1 = 30 $, $ y_1 = 6 $.\n  - Strong prior: $ m_\\beta = \\log(2.0) $, $ s_\\beta = 0.2 $.\n- Case $ 3 $ (quasi-separation):\n  - Data: $ n_0 = 50 $, $ y_0 = 0 $, $ n_1 = 50 $, $ y_1 = 15 $.\n  - Strong prior: $ m_\\beta = \\log(3.0) $, $ s_\\beta = 0.35 $.\n\nImplementation constraints:\n- Derive and use the exact gradient and Hessian of the grouped-binomial log-posterior for $ (\\alpha,\\beta) $. Use a numerically stable logistic and log-sum-exp computation. Use a robust Newton method with backtracking line search to find the mode. If necessary, apply a tiny ridge to the Hessian to ensure numerical invertibility, but keep it negligible relative to the posterior curvature so as not to distort results.\n- Use a $ 0.95 $ credible level and express it as a decimal. No percentage sign is allowed anywhere in the computations or outputs.\n\nFinal output specification:\n- For each test case, output the three differences in the order: width difference, lower bound difference, upper bound difference, each rounded to $ 6 $ decimals.\n- Aggregate all test cases’ results into a single flat list in the order of the cases, and print a single line containing that list in the exact format $ [r_1,r_2,\\dots,r_{9}] $, where the values are the $ 9 $ floats corresponding to the $ 3 $ differences for each of the $ 3 $ cases.", "solution": "The user-defined task is to conduct a Bayesian prior sensitivity analysis for the exposure coefficient in a two-group logistic regression model. The analysis compares the posterior distribution of the log-odds ratio, $\\beta$, under a weakly informative prior versus a strongly informative prior. The posterior is characterized using a Laplace approximation, and the sensitivity is quantified by the differences in the resulting $0.95$ central credible intervals.\n\n### 1. The Statistical Model\n\nThe problem involves data from two groups: an unexposed group ($g=0$) and an exposed group ($g=1$). The outcome is binary (event or non-event).\n\n**Likelihood:**\nThe number of events $y_g$ in group $g$ out of $n_g$ individuals is modeled by a binomial distribution:\n$$ y_g \\sim \\mathrm{Binomial}(n_g, p_g) $$\nwhere $p_g$ is the probability of an event in group $g$. The total log-likelihood for the observed data $(y_0, n_0)$ and $(y_1, n_1)$, ignoring constants, is:\n$$ L(p_0, p_1) = y_0 \\log(p_0) + (n_0 - y_0) \\log(1 - p_0) + y_1 \\log(p_1) + (n_1 - y_1) \\log(1 - p_1) $$\n\n**Logistic Regression Link:**\nThe event probabilities $p_g$ are linked to a linear model via the logit function, $\\mathrm{logit}(p) = \\log(p/(1-p))$. The model is specified as:\n$$ \\mathrm{logit}(p_g) = \\alpha + \\beta x_g $$\nwhere $x_g$ is the binary exposure indicator ($x_0 = 0$ for unexposed, $x_1 = 1$ for exposed).\nThis implies:\n-   For the unexposed group ($x_0=0$): $\\mathrm{logit}(p_0) = \\alpha$, so $p_0 = \\frac{e^\\alpha}{1+e^\\alpha} = \\sigma(\\alpha)$.\n-   For the exposed group ($x_1=1$): $\\mathrm{logit}(p_1) = \\alpha + \\beta$, so $p_1 = \\frac{e^{\\alpha+\\beta}}{1+e^{\\alpha+\\beta}} = \\sigma(\\alpha+\\beta)$.\n\nHere, $\\sigma(z) = (1+e^{-z})^{-1}$ is the logistic sigmoid function. The parameter $\\beta$ is the log-odds ratio: $\\beta = \\mathrm{logit}(p_1) - \\mathrm{logit}(p_0) = \\log(\\mathrm{OR})$.\n\nSubstituting these into the log-likelihood, we obtain the log-likelihood as a function of the parameters $\\theta = (\\alpha, \\beta)^T$:\n$$ L(\\alpha, \\beta) = y_0 \\alpha - n_0 \\log(1+e^\\alpha) + y_1(\\alpha+\\beta) - n_1 \\log(1+e^{\\alpha+\\beta}) $$\n\n### 2. Bayesian Inference Framework\n\nIn the Bayesian framework, we combine the likelihood with prior distributions for the parameters to obtain the posterior distribution, according to Bayes' theorem: $p(\\theta | \\text{data}) \\propto p(\\text{data} | \\theta) p(\\theta)$. On the log scale, this becomes:\n$$ \\ell_p(\\alpha, \\beta) = \\log p(\\alpha, \\beta | \\text{data}) = L(\\alpha, \\beta) + \\log p(\\alpha) + \\log p(\\beta) + \\mathrm{const} $$\n\n**Priors:**\nThe problem specifies normal priors for $\\alpha$ and $\\beta$:\n-   Intercept prior (fixed for all cases): $\\alpha \\sim \\mathcal{N}(0, \\sigma_\\alpha^2)$ with $\\sigma_\\alpha = 10$.\n-   Weakly informative prior for $\\beta$: $\\beta \\sim \\mathcal{N}(0, \\sigma_{\\beta,w}^2)$ with $\\sigma_{\\beta,w} = 10$.\n-   Strongly informative prior for $\\beta$: $\\beta \\sim \\mathcal{N}(m_\\beta, s_\\beta^2)$ with $m_\\beta$ and $s_\\beta$ specified per test case.\n\nThe log-prior density for generic parameters $\\alpha \\sim \\mathcal{N}(m_\\alpha, s_\\alpha^2)$ and $\\beta \\sim \\mathcal{N}(m_\\beta, s_\\beta^2)$ is:\n$$ \\log p(\\alpha, \\beta) = -\\frac{(\\alpha - m_\\alpha)^2}{2s_\\alpha^2} - \\frac{(\\beta - m_\\beta)^2}{2s_\\beta^2} + \\mathrm{const} $$\n\n### 3. Laplace Approximation and Numerical Computation\n\nThe Laplace approximation provides a Gaussian approximation to the posterior distribution, centered at its mode (the Maximum a Posteriori or MAP estimate). The MAP estimate $\\hat{\\theta} = (\\hat{\\alpha}, \\hat{\\beta})^T$ is found by maximizing the log-posterior $\\ell_p(\\theta)$. This is a numerical optimization problem.\n\n**Newton's Method:**\nWe use Newton's method to find the maximum of $\\ell_p(\\theta)$. This iterative algorithm uses the gradient and Hessian of the log-posterior. The update rule at iteration $k$ is:\n$$ \\theta_{k+1} = \\theta_k - \\gamma_k [\\mathbf{H}_p(\\theta_k)]^{-1} \\nabla \\ell_p(\\theta_k) $$\nwhere $\\nabla \\ell_p$ is the gradient vector, $\\mathbf{H}_p$ is the Hessian matrix of the log-posterior, and $\\gamma_k$ is a step size determined by a backtracking line search to ensure convergence.\n\n**Gradient of Log-Posterior $\\nabla \\ell_p(\\theta)$:**\nThe components of the gradient are:\n$$ \\frac{\\partial \\ell_p}{\\partial \\alpha} = (y_0 - n_0 p_0) + (y_1 - n_1 p_1) - \\frac{\\alpha}{\\sigma_\\alpha^2} = (y_0 + y_1) - n_0 \\sigma(\\alpha) - n_1 \\sigma(\\alpha+\\beta) - \\frac{\\alpha}{\\sigma_\\alpha^2} $$\n$$ \\frac{\\partial \\ell_p}{\\partial \\beta} = (y_1 - n_1 p_1) - \\frac{\\beta - m_\\beta}{s_\\beta^2} = y_1 - n_1 \\sigma(\\alpha+\\beta) - \\frac{\\beta - m_\\beta}{s_\\beta^2} $$\n\n**Hessian of Log-Posterior $\\mathbf{H}_p(\\theta)$:**\nThe Hessian matrix is the matrix of second partial derivatives. Using the property $\\frac{d\\sigma(z)}{dz} = \\sigma(z)(1-\\sigma(z))$, we get:\n$$ \\mathbf{H}_p(\\alpha, \\beta) = \\begin{pmatrix}\n-n_0 p_0(1-p_0) - n_1 p_1(1-p_1) - \\frac{1}{\\sigma_\\alpha^2}  -n_1 p_1(1-p_1) \\\\\n-n_1 p_1(1-p_1)  -n_1 p_1(1-p_1) - \\frac{1}{s_\\beta^2}\n\\end{pmatrix} $$\nwhere $p_0 = \\sigma(\\alpha)$ and $p_1 = \\sigma(\\alpha+\\beta)$. The priors add diagonal terms to the Hessian, which regularizes the problem and ensures the Hessian is non-singular even in cases of data separation (e.g., when $y_0=0$).\n\n### 4. Credible Interval Calculation\n\nAfter Newton's method converges to the MAP estimate $\\hat{\\theta}$, the posterior is approximated as:\n$$ p(\\theta | \\text{data}) \\approx \\mathcal{N}(\\hat{\\theta}, \\mathbf{V}) $$\nwhere the covariance matrix $\\mathbf{V}$ is the inverse of the observed information matrix, which is the negative of the Hessian evaluated at the MAP:\n$$ \\mathbf{V} = [-\\mathbf{H}_p(\\hat{\\theta})]^{-1} $$\n\nThe marginal posterior distribution for $\\beta$ is then approximated by a normal distribution:\n$$ \\beta | \\text{data} \\sim \\mathcal{N}(\\hat{\\beta}, V_{\\beta\\beta}) $$\nwhere $V_{\\beta\\beta}$ is the diagonal element of $\\mathbf{V}$ corresponding to $\\beta$.\n\nThe $100(1-\\delta)\\%$ central credible interval for $\\beta$ is calculated as:\n$$ [\\hat{\\beta} - z_{1-\\delta/2} \\sqrt{V_{\\beta\\beta}}, \\quad \\hat{\\beta} + z_{1-\\delta/2} \\sqrt{V_{\\beta\\beta}}] $$\nFor a $0.95$ credible interval, $\\delta=0.05$, and $z_{0.975} \\approx 1.96$ is the $0.975$-quantile of the standard normal distribution.\n\n### 5. Quantifying Prior Sensitivity\n\nThe entire procedure—from MAP estimation to credible interval calculation—is performed twice for each test case:\n1.  Using the weakly informative prior: $\\beta \\sim \\mathcal{N}(0, 10^2)$.\n2.  Using the specified strongly informative prior: $\\beta \\sim \\mathcal{N}(m_\\beta, s_\\beta^2)$.\n\nThis yields two credible intervals, $(\\text{lower}_{\\text{weak}}, \\text{upper}_{\\text{weak}})$ and $(\\text{lower}_{\\text{strong}}, \\text{upper}_{\\text{strong}})$, with corresponding widths $\\text{width}_{\\text{weak}}$ and $\\text{width}_{\\text{strong}}$. The sensitivity to the prior choice is measured by the differences:\n-   $\\Delta_{\\text{width}} = \\text{width}_{\\text{weak}} - \\text{width}_{\\text{strong}}$\n-   $\\Delta_{\\text{lower}} = \\text{lower}_{\\text{weak}} - \\text{lower}_{\\text{strong}}$\n-   $\\Delta_{\\text{upper}} = \\text{upper}_{\\text{weak}} - \\text{upper}_{\\text{strong}}$\n\nThese three differences are computed for each test case and reported as the final result.", "answer": "[0.114178, 0.126487, 0.240665, 3.404391, -0.588597, 2.815794, 2.220793, 0.581907, 2.8027]", "id": "4972024"}]}