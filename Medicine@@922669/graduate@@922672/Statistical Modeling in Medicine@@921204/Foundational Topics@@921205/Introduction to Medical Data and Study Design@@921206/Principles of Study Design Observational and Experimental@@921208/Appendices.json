{"hands_on_practices": [{"introduction": "The randomized controlled trial (RCT) is the cornerstone of evidence-based medicine, primarily because its design allows for robust causal inference. This first exercise bridges the gap between the theoretical potential outcomes framework and the analysis of a real-world trial. By leveraging the principle of exchangeability guaranteed by randomization, you will see how the causal Average Treatment Effect ($ATE$) can be directly estimated from observed data, providing a clear, hands-on understanding of the fundamental power of experimental design [@problem_id:4980051].", "problem": "A two-arm, parallel-group randomized controlled trial compares an investigational therapy, indexed by $T=1$, to standard care, indexed by $T=0$, for reducing the occurrence of a binary clinical outcome $Y$ observed within $30$ days, where $Y=1$ indicates the event occurred and $Y=0$ indicates the event did not occur. Let the potential outcomes be $Y(1)$ and $Y(0)$, defined as the values $Y$ would take under treatment $T=1$ and $T=0$, respectively. The observed arm-specific mean outcomes are $E[Y\\mid T=1]=0.62$ and $E[Y\\mid T=0]=0.50$. Using the potential outcomes framework and principles of experimental design, including exchangeability induced by randomization, consistency under the Stable Unit Treatment Value Assumption (SUTVA), and positivity, derive an expression for the causal Average Treatment Effect (ATE) $E[Y(1)-Y(0)]$ in terms of observable quantities in this trial, then compute its numerical value from the provided data. Interpret this value as an absolute risk difference for the $30$-day outcome. Express your final answer as a single decimal number (no percentage sign). Use exact arithmetic; no rounding is required.", "solution": "The goal is to express the Average Treatment Effect (ATE), $E[Y(1) - Y(0)]$, in terms of observable quantities and then compute its value.\n\n**1. Derivation from First Principles**\n\nThe ATE is defined as the difference in the expected potential outcomes:\n$$\n\\text{ATE} = E[Y(1) - Y(0)] = E[Y(1)] - E[Y(0)]\n$$\n\nTo estimate this from observed data, we rely on three key assumptions for a randomized controlled trial:\n\n*   **Exchangeability**: Due to randomization, the treatment assignment $T$ is independent of the potential outcomes $(Y(1), Y(0))$. Formally, $(Y(1), Y(0)) \\perp T$. This implies that the expected potential outcomes are the same regardless of the treatment group assigned:\n    *   $E[Y(1) \\mid T=1] = E[Y(1) \\mid T=0] = E[Y(1)]$\n    *   $E[Y(0) \\mid T=1] = E[Y(0) \\mid T=0] = E[Y(0)]$\n\n*   **Consistency**: An individual's observed outcome $Y$ is their potential outcome under the treatment they actually received. Formally, if $T=t$, then $Y = Y(t)$. This allows us to connect observed outcomes to potential outcomes:\n    *   For the treated group ($T=1$): $E[Y \\mid T=1] = E[Y(1) \\mid T=1]$\n    *   For the control group ($T=0$): $E[Y \\mid T=0] = E[Y(0) \\mid T=0]$\n\n*   **Positivity**: Every participant has a non-zero probability of being assigned to either treatment group. This is guaranteed by the design of a standard RCT.\n\nCombining these principles, we can identify the ATE.\nUsing consistency, we can write the observed mean in the treated group as $E[Y \\mid T=1] = E[Y(1) \\mid T=1]$.\nBy exchangeability, $E[Y(1) \\mid T=1] = E[Y(1)]$.\nTherefore, the unobservable mean potential outcome $E[Y(1)]$ is equal to the observable mean outcome in the treated group, $E[Y \\mid T=1]$.\n\nSimilarly, for the control group:\n$E[Y \\mid T=0] = E[Y(0) \\mid T=0]$ (by consistency).\n$E[Y(0) \\mid T=0] = E[Y(0)]$ (by exchangeability).\nTherefore, $E[Y(0)]$ is equal to the observable mean outcome in the control group, $E[Y \\mid T=0]$.\n\nSubstituting these into the definition of the ATE:\n$$\n\\text{ATE} = E[Y(1)] - E[Y(0)] = E[Y \\mid T=1] - E[Y \\mid T=0]\n$$\nThis shows that in an ideal RCT, the ATE is simply the difference in the mean outcomes observed in the two arms of the trial.\n\n**2. Computation and Interpretation**\n\nGiven the trial data:\n*   Mean outcome in the investigational therapy arm: $E[Y \\mid T=1] = 0.62$\n*   Mean outcome in the standard care arm: $E[Y \\mid T=0] = 0.50$\n\nThe estimated ATE is:\n$$\n\\widehat{\\text{ATE}} = 0.62 - 0.50 = 0.12\n$$\n\n**Interpretation:** The ATE of 0.12 is an absolute risk difference. It indicates that, on average, the investigational therapy causes an increase in the 30-day risk of the clinical outcome by 12 percentage points compared to standard care.", "answer": "$$\n\\boxed{0.12}\n$$", "id": "4980051"}, {"introduction": "A well-analyzed trial begins with a well-designed one, and a critical component of design is ensuring adequate statistical power. This next practice moves from analysis to planning, tasking you with deriving the sample size formula for a superiority trial from fundamental statistical principles. Mastering this skill is essential for designing efficient and ethical studies that have a high probability of detecting a true treatment effect if one exists [@problem_id:4980114].", "problem": "A clinical research team plans an individually randomized, parallel-group superiority trial (an experimental study) to compare a new intervention versus usual care on a binary clinical endpoint observed at a fixed follow-up time. Let the true event probability in the intervention arm be $p_{1}$ and in the control arm be $p_{0}$. The primary analysis tests the null hypothesis $H_{0}: p_{1} = p_{0}$ against the two-sided alternative $H_{1}: p_{1} \\neq p_{0}$ at type I error $\\alpha$, using a large-sample score test under equal allocation with $n$ patients per group. The study is powered to detect a prespecified difference $\\Delta = p_{1} - p_{0}  0$ with power $1 - \\beta$.\n\nStarting from the binomial model for independent outcomes in each arm, the Central Limit Theorem (CLT) implies that the difference in sample proportions is approximately normal. Using this as the fundamental base, derive the per-group sample size $n$ that ensures the two-sided score test has power at least $1 - \\beta$ to detect $\\Delta$ at type I error $\\alpha$. Your derivation must begin from the distributional approximations implied by the CLT and the null and alternative variances appropriate to the score test, and then algebraically solve for $n$ in terms of $p_{0}$, $p_{1}$, $\\alpha$, and $\\beta$. Denote by $\\Phi$ the cumulative distribution function of the Standard Normal Distribution (SND), and by $\\Phi^{-1}$ its quantile function.\n\nThen, compute the required per-group sample size for $p_{1} = 0.30$, $p_{0} = 0.20$, $\\alpha = 0.05$ (two-sided), and power $0.80$ using your derived expression. Report the smallest integer total sample size $N_{\\text{total}} = 2n$ that achieves the target power. Do not use percentage signs; express all probabilities as decimals.", "solution": "We begin by formally defining the components of the problem. Let $X_1$ and $X_0$ be the number of events in the intervention and control arms, respectively. Under the specified model, these are independent random variables with binomial distributions: $X_1 \\sim \\text{Binomial}(n, p_1)$ and $X_0 \\sim \\text{Binomial}(n, p_0)$. The corresponding sample proportions are $\\hat{p}_1 = X_1/n$ and $\\hat{p}_0 = X_0/n$. The null and alternative hypotheses are $H_0: p_1 = p_0$ and $H_1: p_1 \\neq p_0$.\n\nThe score test is based on a test statistic evaluated under the null hypothesis. Under $H_0$, there is a common event probability, say $p$, which is estimated by the pooled proportion:\n$$\n\\hat{p}_{\\text{pool}} = \\frac{X_1 + X_0}{n + n} = \\frac{\\hat{p}_1 + \\hat{p}_0}{2}\n$$\nThe variance of the difference in proportions, $\\hat{p}_1 - \\hat{p}_0$, under the assumption that $H_0$ is true, is $\\text{Var}(\\hat{p}_1 - \\hat{p}_0 | H_0) = \\frac{p(1-p)}{n} + \\frac{p(1-p)}{n} = \\frac{2p(1-p)}{n}$. The score test statistic standardizes the observed difference using an estimate of this variance based on $\\hat{p}_{\\text{pool}}$:\n$$\nZ_{\\text{score}} = \\frac{\\hat{p}_1 - \\hat{p}_0}{\\sqrt{\\frac{2\\hat{p}_{\\text{pool}}(1-\\hat{p}_{\\text{pool}})}{n}}}\n$$\nUnder $H_0$, $Z_{\\text{score}}$ follows approximately a Standard Normal Distribution, $N(0,1)$. For a two-sided test with a type I error rate of $\\alpha$, we reject $H_0$ if $|Z_{\\text{score}}|  \\Phi^{-1}(1 - \\alpha/2)$, where $\\Phi^{-1}$ is the quantile function of the Standard Normal Distribution.\n\nPower is the probability of correctly rejecting $H_0$ when the alternative hypothesis $H_1$ is true. For sample size planning, we consider the specific alternative where the true probabilities are $p_1$ and $p_0$. We require the power to be at least $1-\\beta$. Under $H_1$, the expected value of $\\hat{p}_{\\text{pool}}$ is $E[\\hat{p}_{\\text{pool}}] = \\frac{p_1+p_0}{2}$. Let's denote this average probability as $\\bar{p} = \\frac{p_1+p_0}{2}$. For large $n$, $\\hat{p}_{\\text{pool}}$ will be close to $\\bar{p}$. Therefore, for the purpose of power calculation, we approximate the denominator of the score statistic using $\\bar{p}$ instead of the random $\\hat{p}_{\\text{pool}}$.\n\nThe rejection rule is thus approximated as:\n$$\n\\left| \\frac{\\hat{p}_1 - \\hat{p}_0}{\\sqrt{\\frac{2\\bar{p}(1-\\bar{p})}{n}}} \\right|  \\Phi^{-1}(1 - \\alpha/2)\n$$\nWe are designing the study to detect a difference $\\Delta = p_1 - p_0  0$. In this case, the power is dominated by the upper tail of the rejection region. The power is approximately:\n$$\n\\text{Power} = P\\left( \\frac{\\hat{p}_1 - \\hat{p}_0}{\\sqrt{\\frac{2\\bar{p}(1-\\bar{p})}{n}}}  \\Phi^{-1}(1 - \\alpha/2) \\;\\middle|\\; H_1 \\right) = 1-\\beta\n$$\nThis is equivalent to:\n$$\nP\\left( \\hat{p}_1 - \\hat{p}_0  \\Phi^{-1}(1 - \\alpha/2) \\sqrt{\\frac{2\\bar{p}(1-\\bar{p})}{n}} \\;\\middle|\\; H_1 \\right) = 1-\\beta\n$$\nTo evaluate this probability, we must standardize the random variable $\\hat{p}_1 - \\hat{p}_0$ using its true distribution under $H_1$. By the Central Limit Theorem, $\\hat{p}_1 - \\hat{p}_0$ is approximately normally distributed with mean $E[\\hat{p}_1 - \\hat{p}_0] = p_1 - p_0$ and variance $\\text{Var}(\\hat{p}_1 - \\hat{p}_0) = \\text{Var}(\\hat{p}_1) + \\text{Var}(\\hat{p}_0) = \\frac{p_1(1-p_1)}{n} + \\frac{p_0(1-p_0)}{n}$.\n\nLet $Z$ be a standard normal random variable. Standardizing the inequality gives:\n$$\nP\\left( \\frac{(\\hat{p}_1 - \\hat{p}_0) - (p_1-p_0)}{\\sqrt{\\frac{p_1(1-p_1) + p_0(1-p_0)}{n}}}  \\frac{\\Phi^{-1}(1 - \\alpha/2)\\sqrt{\\frac{2\\bar{p}(1-\\bar{p})}{n}} - (p_1-p_0)}{\\sqrt{\\frac{p_1(1-p_1) + p_0(1-p_0)}{n}}} \\right) = 1-\\beta\n$$\nThe left side is $P(Z  \\text{some value})$. For this probability to equal $1-\\beta$, the argument must be equal to $\\Phi^{-1}(\\beta) = -\\Phi^{-1}(1-\\beta)$.\n$$\n\\frac{\\Phi^{-1}(1 - \\alpha/2)\\sqrt{2\\bar{p}(1-\\bar{p})}\\frac{1}{\\sqrt{n}} - (p_1-p_0)}{\\sqrt{p_1(1-p_1) + p_0(1-p_0)}\\frac{1}{\\sqrt{n}}} = -\\Phi^{-1}(1-\\beta)\n$$\nMultiplying the terms by $\\sqrt{n}$ and rearranging yields:\n$$\n\\Phi^{-1}(1 - \\alpha/2)\\sqrt{2\\bar{p}(1-\\bar{p})} - \\sqrt{n}(p_1-p_0) = -\\Phi^{-1}(1-\\beta)\\sqrt{p_1(1-p_1) + p_0(1-p_0)}\n$$\nNow, we solve for $\\sqrt{n}$:\n$$\n\\sqrt{n}(p_1-p_0) = \\Phi^{-1}(1 - \\alpha/2)\\sqrt{2\\bar{p}(1-\\bar{p})} + \\Phi^{-1}(1-\\beta)\\sqrt{p_1(1-p_1) + p_0(1-p_0)}\n$$\n$$\n\\sqrt{n} = \\frac{\\Phi^{-1}(1 - \\alpha/2)\\sqrt{2\\bar{p}(1-\\bar{p})} + \\Phi^{-1}(1-\\beta)\\sqrt{p_1(1-p_1) + p_0(1-p_0)}}{p_1-p_0}\n$$\nSquaring both sides gives the final expression for the per-group sample size $n$:\n$$\nn = \\frac{\\left( \\Phi^{-1}(1 - \\alpha/2)\\sqrt{2\\bar{p}(1-\\bar{p})} + \\Phi^{-1}(1-\\beta)\\sqrt{p_1(1-p_1) + p_0(1-p_0)} \\right)^2}{(p_1-p_0)^2}\n$$\nwhere $\\bar{p} = (p_0+p_1)/2$.\n\nWe now compute the required sample size for the given parameters: $p_1 = 0.30$, $p_0 = 0.20$, $\\alpha = 0.05$, and power $1-\\beta = 0.80$.\nThis implies $\\beta = 0.20$. The effect size is $\\Delta = p_1 - p_0 = 0.10$.\nThe required quantiles from the Standard Normal Distribution are:\n$$\n\\Phi^{-1}(1 - \\alpha/2) = \\Phi^{-1}(1 - 0.05/2) = \\Phi^{-1}(0.975) \\approx 1.95996\n$$\n$$\n\\Phi^{-1}(1-\\beta) = \\Phi^{-1}(1-0.20) = \\Phi^{-1}(0.80) \\approx 0.84162\n$$\nNext, we compute the variance components. The average proportion $\\bar{p}$ is:\n$$\n\\bar{p} = \\frac{0.20 + 0.30}{2} = 0.25\n$$\nThe variance term corresponding to the score test's null hypothesis structure is:\n$$\n2\\bar{p}(1-\\bar{p}) = 2(0.25)(1-0.25) = 2(0.25)(0.75) = 0.375\n$$\nThe variance term corresponding to the true distribution under the alternative hypothesis is:\n$$\np_1(1-p_1) + p_0(1-p_0) = 0.30(0.70) + 0.20(0.80) = 0.21 + 0.16 = 0.37\n$$\nSubstituting these values into the derived formula for $n$:\n$$\nn = \\frac{\\left( 1.95996 \\sqrt{0.375} + 0.84162 \\sqrt{0.37} \\right)^2}{(0.10)^2}\n$$\n$$\nn \\approx \\frac{\\left( 1.95996 \\times 0.61237 + 0.84162 \\times 0.60828 \\right)^2}{0.01}\n$$\n$$\nn \\approx \\frac{\\left( 1.20023 + 0.51194 \\right)^2}{0.01} = \\frac{(1.71217)^2}{0.01} \\approx \\frac{2.93153}{0.01} \\approx 293.153\n$$\nSince the sample size must be an integer, we take the ceiling of this value to ensure the power is at least $0.80$.\n$$\nn = \\lceil 293.153 \\rceil = 294\n$$\nThis is the required sample size per group. The total sample size is $N_{\\text{total}} = 2n$.\n$$\nN_{\\text{total}} = 2 \\times 294 = 588\n$$\nThus, the smallest total sample size that achieves the target power is $588$.", "answer": "$$\\boxed{588}$$", "id": "4980114"}, {"introduction": "While RCTs are the gold standard, many research questions can only be addressed through observational studies. This exercise shifts our focus to the prospective cohort study, a powerful observational design for investigating the incidence of outcomes over time. You will derive and calculate the Incidence Rate Ratio ($IRR$), a key metric that accounts for variable follow-up by using person-time, providing a robust method for measuring association when randomization is not possible [@problem_id:4980097].", "problem": "A prospective cohort study in cardiovascular epidemiology follows two groups of adults over time to compare the incidence of a sudden-onset event under routine clinical care. The exposed group comprises patients with a documented high-inflammatory biomarker at baseline, while the unexposed group comprises patients without that biomarker. Over the follow-up, the exposed group contributes 3,000 person-years with 90 incident events, and the unexposed group contributes 5,000 person-years with 100 incident events. Assume that within each exposure group the event-generating mechanism can be modeled as a counting process with a constant intensity (rate) over the observed follow-up, and that event counts are well approximated by a Poisson distribution with mean equal to the product of the underlying rate and the accrued person-time.\n\nStarting from the fundamental definition of the incidence rate in a group as the expected number of incident events per unit person-time, and defining the incidence rate ratio (IRR) as the ratio of the exposed-group rate to the unexposed-group rate, derive the estimator of the IRR based on observed event counts and person-time in each group. Then, using the study data given above, compute the value of the IRR and briefly interpret its clinical meaning in terms of relative instantaneous event occurrence under the stated assumptions.\n\nProvide the final numeric value of the IRR as a single number with no units. Provide the exact value; do not round.", "solution": "**Derivation of the IRR Estimator**\nLet $\\lambda_1$ and $\\lambda_0$ be the true, constant incidence rates (events per person-time) in the exposed and unexposed groups, respectively. The Incidence Rate Ratio (IRR) is defined as $IRR = \\frac{\\lambda_1}{\\lambda_0}$.\n\nLet $E_1$ and $E_0$ be the observed event counts over $T_1$ and $T_0$ person-years of follow-up for the exposed and unexposed groups. Under the assumption that event counts follow a Poisson distribution, we have:\n$$\nE_1 \\sim \\text{Poisson}(\\lambda_1 T_1) \\quad \\text{and} \\quad E_0 \\sim \\text{Poisson}(\\lambda_0 T_0)\n$$\nThe maximum likelihood estimator for the rate $\\lambda_i$ in each group is the empirical rate, which is the number of observed events divided by the total person-time at risk:\n$$\n\\hat{\\lambda}_1 = \\frac{E_1}{T_1} \\quad \\text{and} \\quad \\hat{\\lambda}_0 = \\frac{E_0}{T_0}\n$$\nBy the plug-in principle, we estimate the IRR by substituting these estimators for the true rates:\n$$\n\\widehat{IRR} = \\frac{\\hat{\\lambda}_1}{\\hat{\\lambda}_0} = \\frac{E_1 / T_1}{E_0 / T_0}\n$$\nThis is the estimator for the IRR based on observed event counts and person-time.\n\n**Computation of the IRR and Interpretation**\nUsing the provided study data:\n- Exposed group: $E_1 = 90$ events, $T_1 = 3,000$ person-years\n- Unexposed group: $E_0 = 100$ events, $T_0 = 5,000$ person-years\n\nFirst, we compute the estimated incidence rate for each group:\n$$ \\hat{\\lambda}_1 = \\frac{90}{3,000} = 0.03 \\text{ events per person-year} $$\n$$ \\hat{\\lambda}_0 = \\frac{100}{5,000} = 0.02 \\text{ events per person-year} $$\nNext, we compute the estimated IRR:\n$$ \\widehat{IRR} = \\frac{0.03}{0.02} = 1.5 $$\n\n**Interpretation:** An IRR of 1.5 indicates that the incidence rate of the event in the exposed group (those with the high-inflammatory biomarker) is 1.5 times the incidence rate in the unexposed group. In other words, at any given point in time, an individual in the exposed group has a 50% higher instantaneous risk (hazard) of experiencing the event compared to an individual in the unexposed group, assuming the association is causal.", "answer": "$$\\boxed{1.5}$$", "id": "4980097"}]}