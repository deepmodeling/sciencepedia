{"hands_on_practices": [{"introduction": "Understanding p-values begins with mastering their fundamental definition and calculation. This exercise grounds you in a classic epidemiological scenario: testing for an association between an exposure and a health outcome. By working through the derivation and calculation of a two-sided p-value for a Z-test, you will solidify your understanding of what a p-value representsâ€”the probability of observing a result at least as extreme as the one found, purely by chance, if the null hypothesis were true [@problem_id:4617807].", "problem": "A cohort study investigates whether high long-term exposure to fine particulate matter in air pollution is associated with incident asthma. Let the logarithm of the relative risk be denoted by $\\beta$. The estimator $\\hat{\\beta}$ is approximately normal with mean $0$ and variance $s_{\\hat{\\beta}}^{2}$ under the null hypothesis $H_{0}:\\beta=0$, by the Central Limit Theorem (CLT). Consider the standardized test statistic $Z=\\hat{\\beta}/s_{\\hat{\\beta}}$, which under $H_{0}$ has a standard normal distribution with mean $0$ and variance $1$. In the study, the observed value is $Z_{\\text{obs}}=2.1$. Using the definition of a two-sided $p$-value as the probability, computed under $H_{0}$, of obtaining a test statistic at least as extreme as the observed value in either tail, derive an expression for the two-sided $p$-value in terms of the standard normal cumulative distribution function $\\Phi(\\cdot)$ and compute its numerical value for $Z_{\\text{obs}}=2.1$. Round your numerical answer to four significant figures. Finally, briefly interpret the meaning of this $p$-value in the epidemiological context given.", "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- A cohort study investigates the association between long-term exposure to fine particulate matter and incident asthma.\n- The logarithm of the relative risk is denoted by $\\beta$.\n- The null hypothesis is $H_{0}: \\beta=0$.\n- The estimator for $\\beta$ is $\\hat{\\beta}$.\n- Under $H_{0}$, the distribution of the estimator is approximately normal: $\\hat{\\beta} \\sim N(0, s_{\\hat{\\beta}}^{2})$.\n- The standardized test statistic is $Z=\\hat{\\beta}/s_{\\hat{\\beta}}$.\n- Under $H_{0}$, the distribution of the test statistic is standard normal: $Z \\sim N(0, 1)$.\n- The observed value of the test statistic is $Z_{\\text{obs}}=2.1$.\n- The definition of a two-sided $p$-value is the probability, computed under $H_{0}$, of obtaining a test statistic at least as extreme as the observed value in either tail.\n- The standard normal cumulative distribution function (CDF) is denoted by $\\Phi(\\cdot)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It presents a standard scenario in biostatistics and epidemiology: hypothesis testing for an association parameter (log relative risk) derived from a cohort study.\n- **Scientifically Grounded:** The problem uses established statistical principles (Central Limit Theorem, hypothesis testing, Z-statistic, p-value) within a realistic epidemiological context. The relationship between air pollution and asthma is a valid area of scientific inquiry.\n- **Well-Posed:** All necessary information is provided to derive the expression for the $p$-value, calculate its value, and interpret the result. The task is clear and unambiguous.\n- **Objective:** The language is formal, precise, and free of subjective claims.\n\nThe problem does not violate any of the invalidity criteria. It is complete, consistent, realistic, and well-structured.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\nThe solution consists of three parts as requested: deriving an expression for the two-sided $p$-value, computing its numerical value, and providing an interpretation in the given context.\n\n**Part 1: Derivation of the $p$-value expression**\n\nThe two-sided $p$-value is defined as the probability, under the null hypothesis $H_{0}$, of observing a test statistic at least as extreme as the one observed. The test statistic is $Z$, which follows a standard normal distribution, $Z \\sim N(0, 1)$, under $H_{0}$. The observed value is $Z_{\\text{obs}} = 2.1$.\n\nAn outcome \"at least as extreme\" as $Z_{\\text{obs}}$ means the value of the test statistic $Z$ is either greater than or equal to $Z_{\\text{obs}}$ or less than or equal to $-Z_{\\text{obs}}$. Since the observed value is positive, this is equivalent to the test statistic's absolute value being greater than or equal to the observed value's absolute value.\nThe $p$-value is therefore given by the probability:\n$$p = \\Pr(|Z| \\ge |Z_{\\text{obs}}| \\mid H_{0})$$\nSubstituting the observed value $Z_{\\text{obs}} = 2.1$:\n$$p = \\Pr(|Z| \\ge 2.1)$$\nThis inequality can be split into two disjoint events, representing the two tails of the distribution:\n$$p = \\Pr(Z \\ge 2.1) + \\Pr(Z \\le -2.1)$$\nThe standard normal distribution is symmetric about its mean of $0$. Therefore, the probability in the left tail is equal to the probability in the right tail:\n$$\\Pr(Z \\le -2.1) = \\Pr(Z \\ge 2.1)$$\nThe expression for the $p$-value can thus be simplified to:\n$$p = 2 \\times \\Pr(Z \\ge 2.1)$$\nThe standard normal cumulative distribution function (CDF), $\\Phi(z)$, is defined as $\\Phi(z) = \\Pr(Z \\le z)$. The probability in the right tail can be expressed in terms of the CDF as:\n$$\\Pr(Z \\ge z) = 1 - \\Pr(Z  z)$$\nSince the normal distribution is a continuous distribution, $\\Pr(Z  z) = \\Pr(Z \\le z) = \\Phi(z)$. Thus:\n$$\\Pr(Z \\ge 2.1) = 1 - \\Phi(2.1)$$\nSubstituting this back into the expression for the $p$-value, we get the desired expression in terms of the standard normal CDF:\n$$p = 2 \\times (1 - \\Phi(2.1))$$\nFor a general observed test statistic $Z_{\\text{obs}}$, the expression is $p = 2(1 - \\Phi(|Z_{\\text{obs}}|))$.\n\n**Part 2: Numerical computation of the $p$-value**\n\nWe need to compute the value of $p = 2 \\times (1 - \\Phi(2.1))$. Using standard statistical tables or software, the value of the standard normal CDF at $z=2.1$ is approximately:\n$$\\Phi(2.1) \\approx 0.98213558$$\nNow, we compute the $p$-value:\n$$p \\approx 2 \\times (1 - 0.98213558)$$\n$$p \\approx 2 \\times 0.01786442$$\n$$p \\approx 0.03572884$$\nThe problem requires the answer to be rounded to four significant figures. The first significant figure is $3$. The next three are $5$, $7$, and $2$. The fifth significant figure is $8$, which is $5$ or greater, so we round up the fourth significant figure ($2$) to $3$.\n$$p \\approx 0.03573$$\n\n**Part 3: Interpretation of the $p$-value**\n\nThe calculated $p$-value is approximately $0.03573$. In the context of the epidemiological study, this value has a precise meaning. The null hypothesis, $H_{0}: \\beta=0$, corresponds to no association between high long-term exposure to fine particulate matter and the incidence of asthma (i.e., the relative risk is $1$). The $p$-value is the probability of observing data that suggest an association at least as strong as the one found in this study (represented by $Z_{\\text{obs}}=2.1$), *assuming that the null hypothesis is true*.\n\nTherefore, the interpretation is: If there were truly no association between long-term exposure to fine particulate matter and the incidence of asthma, there would be a $3.573\\%$ probability of observing an association as strong as, or stronger than, the one detected in this study purely due to random chance or sampling variability. A small $p$-value (typically less than a pre-specified significance level such as $0.05$) is often taken as evidence to reject the null hypothesis in favor of the alternative hypothesis that an association does exist.", "answer": "$$\n\\boxed{0.03573}\n$$", "id": "4617807"}, {"introduction": "While calculating a p-value and comparing it to a significance threshold like $\\alpha = 0.05$ is a standard procedure, a critical interpretation demands that we assess the robustness of this conclusion. This practice introduces the 'fragility index', a powerful metric that quantifies how many participant outcomes would need to change to flip a 'significant' result to 'non-significant'. Calculating this index for a clinical trial scenario using Fisher's exact test will help you move beyond a binary view of statistical significance and learn to evaluate the stability of research findings [@problem_id:4617811].", "problem": "A randomized, parallel-group clinical trial in an outbreak setting compares a candidate prophylactic intervention to standard care. In the intervention arm, $n_{1} = 6$ participants are enrolled; in the control arm, $n_{2} = 6$ participants are enrolled. The primary endpoint is a binary outcome: confirmed infection within $7$ days post-exposure (event) versus no confirmed infection (non-event). The observed $2 \\times 2$ table is:\n- Intervention arm: $0$ events and $6$ non-events.\n- Control arm: $5$ events and $1$ non-event.\n\nAssume the null hypothesis that the probability of event is equal in both arms and the trial design justifies conditioning on the fixed margins (total events and total allocation) when computing the p-value. Use the two-sided Fisher's exact test at significance level $\\alpha = 0.05$, where the two-sided p-value is defined as the sum of hypergeometric probabilities of all tables with the same margins whose probability under the null is less than or equal to that of the observed table.\n\nTasks:\n1. Starting from the definition of the p-value under the null with fixed margins and random allocation, derive and compute the exact two-sided Fisher's p-value for the observed table. Express intermediate probabilities as exact rational numbers and you may also provide decimal expansions for interpretation.\n2. Define the fragility index for this dichotomous comparison as the smallest integer $f \\geq 0$ such that, after reclassifying $f$ outcomes in the arm with fewer events (here, the intervention arm) from non-event to event and keeping all else unchanged, the recomputed exact two-sided Fisher's p-value is greater than or equal to $\\alpha$. Compute this fragility index by incrementally changing one outcome at a time and recomputing the two-sided p-value exactly at each step.\n3. Briefly state, in one sentence, the epidemiological interpretation of a small fragility index in terms of the stability of p-value-based conclusions.\n\nReport the fragility index as an integer. No rounding is required for the final answer.", "solution": "The problem statement provides a complete and scientifically grounded scenario for statistical analysis. It is based on established principles of clinical trial design and biostatistics, specifically the use of Fisher's exact test for 2x2 contingency tables. All necessary data, definitions, and conditions are provided, and there are no contradictions or ambiguities. The problem is valid.\n\nLet the $2 \\times 2$ contingency table be structured as follows, where $a$ and $c$ are the number of events, and $b$ and $d$ are the number of non-events.\n\n| Arm          | Events | Non-Events | Total |\n|--------------|--------|------------|-------|\n| Intervention | $a$    | $b$        | $n_1$ |\n| Control      | $c$    | $d$        | $n_2$ |\n| Total        | $K$    | $N-K$      | $N$   |\n\nThe problem provides the following observed data:\n- Intervention arm: $a=0$ events, $b=6$ non-events.\n- Control arm: $c=5$ events, $d=1$ non-event.\n\nThe resulting table and its margins are:\n- Row totals: $n_1 = a+b = 0+6 = 6$, $n_2 = c+d = 5+1 = 6$.\n- Column totals: $K = a+c = 0+5 = 5$, $N-K = b+d = 6+1 = 7$.\n- Grand total: $N = n_1+n_2 = 6+6 = 12$.\n\nUnder the null hypothesis of no difference in event probability between the arms, and conditioning on the fixed margins, the probability of observing a table with $a$ events in the intervention arm is given by the hypergeometric probability mass function:\n$$ P(a) = \\frac{\\binom{K}{a} \\binom{N-K}{n_1-a}}{\\binom{N}{n_1}} $$\nFor the observed table, the parameters are $N=12$, $n_1=6$, and $K=5$. The possible values for $a$ are integers satisfying $ \\max(0, n_1+K-N) \\leq a \\leq \\min(n_1, K) $. Here, $ \\max(0, 6+5-12) \\leq a \\leq \\min(6, 5) $, which simplifies to $0 \\leq a \\leq 5$.\n\n**1. Computation of the two-sided p-value**\n\nFirst, we calculate the probabilities for all possible values of $a$ from $0$ to $5$. The denominator is $\\binom{12}{6} = \\frac{12 \\cdot 11 \\cdot 10 \\cdot 9 \\cdot 8 \\cdot 7}{6 \\cdot 5 \\cdot 4 \\cdot 3 \\cdot 2 \\cdot 1} = 924$.\nThe probabilities are:\n- For $a=0$: $P(0) = \\frac{\\binom{5}{0} \\binom{7}{6}}{\\binom{12}{6}} = \\frac{1 \\cdot 7}{924} = \\frac{7}{924}$\n- For $a=1$: $P(1) = \\frac{\\binom{5}{1} \\binom{7}{5}}{\\binom{12}{6}} = \\frac{5 \\cdot 21}{924} = \\frac{105}{924}$\n- For $a=2$: $P(2) = \\frac{\\binom{5}{2} \\binom{7}{4}}{\\binom{12}{6}} = \\frac{10 \\cdot 35}{924} = \\frac{350}{924}$\n- For $a=3$: $P(3) = \\frac{\\binom{5}{3} \\binom{7}{3}}{\\binom{12}{6}} = \\frac{10 \\cdot 35}{924} = \\frac{350}{924}$\n- For $a=4$: $P(4) = \\frac{\\binom{5}{4} \\binom{7}{2}}{\\binom{12}{6}} = \\frac{5 \\cdot 21}{924} = \\frac{105}{924}$\n- For $a=5$: $P(5) = \\frac{\\binom{5}{5} \\binom{7}{1}}{\\binom{12}{6}} = \\frac{1 \\cdot 7}{924} = \\frac{7}{924}$\n\nThe observed table corresponds to $a=0$, with probability $P(0) = 7/924$. The two-sided p-value is the sum of probabilities of all tables that are equally or less probable than the observed table. We identify all $a$ for which $P(a) \\leq P(0)$. From the list above, these are $a=0$ and $a=5$.\n\nThe two-sided p-value is therefore:\n$$ p = P(0) + P(5) = \\frac{7}{924} + \\frac{7}{924} = \\frac{14}{924} $$\nSimplifying the fraction gives:\n$$ p = \\frac{14}{14 \\cdot 66} = \\frac{1}{66} $$\nAs a decimal, this is $p \\approx 0.01515...$. Since $p  \\alpha = 0.05$, the result is statistically significant.\n\n**2. Computation of the fragility index**\n\nThe fragility index, $f$, is the smallest non-negative integer of outcomes to reclassify from non-event to event in the intervention arm that makes the p-value no longer statistically significant (i.e., $p \\geq \\alpha = 0.05$).\n\n- **Case $f=0$**: No outcomes are changed. The p-value is $p = 1/66 \\approx 0.015$, which is less than $0.05$. The result remains significant.\n\n- **Case $f=1$**: We change one non-event to an event in the intervention arm. The new observed table is:\n  - Intervention arm: $a' = 0+1=1$ event, $b' = 6-1=5$ non-events.\n  - Control arm: $c' = 5$ events, $d' = 1$ non-event.\n\nThis modification changes the column margins. The new table for analysis is:\n| Arm          | Events | Non-Events | Total |\n|--------------|--------|------------|-------|\n| Intervention | $1$    | $5$        | $6$   |\n| Control      | $5$    | $1$        | $6$   |\n| Total        | $6$    | $6$        | $12$  |\n\nThe new margins are $n_1=6$, $n_2=6$, $K'=6$, and $N-K'=6$. We must re-calculate the p-value with this new hypergeometric distribution, where the observed cell is $a=1$. The probability distribution is $P(a) = \\frac{\\binom{K'}{a} \\binom{N-K'}{n_1-a}}{\\binom{N}{n_1}} = \\frac{\\binom{6}{a} \\binom{6}{6-a}}{\\binom{12}{6}}$.\nThe denominator remains $924$. The possible values for $a$ are now $0, 1, ..., 6$.\n\nThe probabilities for this new distribution are:\n- $P(0) = \\frac{\\binom{6}{0}\\binom{6}{6}}{924} = \\frac{1 \\cdot 1}{924} = \\frac{1}{924}$\n- $P(1) = \\frac{\\binom{6}{1}\\binom{6}{5}}{924} = \\frac{6 \\cdot 6}{924} = \\frac{36}{924}$\n- $P(2) = \\frac{\\binom{6}{2}\\binom{6}{4}}{924} = \\frac{15 \\cdot 15}{924} = \\frac{225}{924}$\n- $P(3) = \\frac{\\binom{6}{3}\\binom{6}{3}}{924} = \\frac{20 \\cdot 20}{924} = \\frac{400}{924}$\n- $P(4) = P(2) = \\frac{225}{924}$\n- $P(5) = P(1) = \\frac{36}{924}$\n- $P(6) = P(0) = \\frac{1}{924}$\n\nThe observed table for the $f=1$ case corresponds to $a=1$, with probability $P(1) = 36/924$. The new two-sided p-value is the sum of probabilities for tables with $P(a) \\leq P(1)$. These are the tables for $a=0, 1, 5, 6$.\nThe new p-value, $p'$, is:\n$$ p' = P(0) + P(1) + P(5) + P(6) = \\frac{1}{924} + \\frac{36}{924} + \\frac{36}{924} + \\frac{1}{924} = \\frac{74}{924} $$\nSimplifying gives $p' = \\frac{37}{462}$.\nAs a decimal, $p' \\approx 0.08008...$. We check this against the significance level:\n$$ p' \\approx 0.08008... \\geq 0.05 $$\nThe condition is met for $f=1$. Since the condition was not met for $f=0$, the smallest integer is $1$. Thus, the fragility index is $1$.\n\n**3. Epidemiological interpretation**\n\nA small fragility index implies that the statistical significance of the trial's finding is fragile, as it depends on the outcomes of only a few participants.", "answer": "$$\\boxed{1}$$", "id": "4617811"}, {"introduction": "Classical statistical tests often rely on assumptions about the underlying data distribution, such as normality, which may not always be justifiable. This hands-on coding exercise introduces the permutation test, a robust non-parametric method that generates a valid p-value directly from the data under the intuitive principle of exchangeability. By implementing a permutation test for the difference in means, you will gain a profound, operational understanding of the null hypothesis and learn how to perform valid inference when parametric assumptions are in doubt [@problem_id:4617824].", "problem": "Consider a two-group comparison commonly encountered in epidemiology, where a continuous outcome is measured in an exposed group and an unexposed group. When the assumption of normality is doubtful, a principled approach to obtaining a valid $p$-value is to use a permutation test under the assumption of exchangeability. Exchangeability asserts that, under the null hypothesis of no effect, the joint distribution of outcomes is invariant to permutations of exposure labels. \n\nYou are required to formalize the permutation $p$-value based on exchangeability and implement a program that computes two-sided permutation $p$-values for the difference in means between two groups. The test statistic must be the difference in sample means, defined as $T = \\bar{X}_E - \\bar{X}_U$, where $\\bar{X}_E$ and $\\bar{X}_U$ denote the sample means of the exposed and unexposed groups, respectively. The two-sided $p$-value is to be computed using the absolute value $|T|$ as the measure of extremeness. Under exchangeability, the null distribution of $T$ is generated by relabeling the observed outcomes across all assignments of $n_E$ exposed labels and $n_U$ unexposed labels across the $n = n_E + n_U$ observations, or approximated via Monte Carlo (MC) sampling when exact enumeration is computationally infeasible.\n\nYour program must:\n- Accept a hard-coded test suite of cases, each case consisting of two lists of observed outcomes (real numbers) corresponding to exposed and unexposed groups, and an integer $B$ indicating the number of Monte Carlo permutations. If $B = 0$, use exact enumeration over all $\\binom{n}{n_E}$ relabelings; if $B  0$, use $B$ random permutations to approximate the $p$-value under the exchangeability assumption. Use a fixed random seed for Monte Carlo to ensure reproducibility.\n- For each case, compute the two-sided permutation $p$-value for $T = \\bar{X}_E - \\bar{X}_U$, defined as the proportion of permutations (exact or MC) for which $|T^{\\pi}| \\ge |T_{\\text{obs}}|$, where $T^{\\pi}$ is the test statistic computed under a given label permutation $\\pi$, and $T_{\\text{obs}}$ is the observed test statistic computed from the original labeling.\n- Express each $p$-value as a decimal rounded to $6$ decimal places.\n\nFundamental base to use:\n- The definition of exchangeability under the null of no effect in epidemiology: the joint distribution of outcomes is invariant under permutations of exposure labels.\n- The definition of a $p$-value as the probability, under the null hypothesis, of observing a test statistic at least as extreme as the realized one.\n\nTest suite:\n- Case $1$ (balanced, exact computation): exposed $[2.1, 1.9, 2.3, 2.0, 2.2]$, unexposed $[1.8, 1.7, 1.6, 1.9, 1.5]$, $B = 0$.\n- Case $2$ (boundary, exact computation with identical outcomes): exposed $[0.0, 0.0]$, unexposed $[0.0, 0.0]$, $B = 0$.\n- Case $3$ (unbalanced, Monte Carlo approximation): exposed $[1.04, 0.92, 1.10, 0.95, 1.00, 1.08, 0.97, 1.02]$, unexposed $[0.88, 0.79, 0.85, 0.80, 0.90, 0.84, 0.76, 0.83, 0.88, 0.82, 0.91, 0.86]$, $B = 50000$, fixed seed $42$.\n- Case $4$ (extreme difference, exact computation): exposed $[5.0, 5.1, 5.2, 4.9]$, unexposed $[1.0, 1.1, 0.9, 1.2]$, $B = 0$.\n\nFinal output format:\nYour program should produce a single line of output containing the permutation $p$-values for the test suite, rounded to $6$ decimal places, as a comma-separated list enclosed in square brackets. For example, the output should look like $[p_1,p_2,p_3,p_4]$, where each $p_i$ corresponds to the $i$-th case in the order given above.", "solution": "The problem requires the computation of permutation test $p$-values for a two-group comparison, a non-parametric method of fundamental importance in epidemiology and biostatistics when distributional assumptions like normality are not met. The cornerstone of this method is the principle of exchangeability under the null hypothesis.\n\n### Step 1: Problem Validation\n\n**1.1. Extraction of Givens**\n-   **Context**: Two-group comparison (exposed vs. unexposed) with a continuous outcome in epidemiology.\n-   **Null Hypothesis ($H_0$) Assumption**: Exchangeability, which posits that the joint distribution of outcomes is invariant to permutations of exposure labels.\n-   **Test Statistic**: The difference in sample means, $T = \\bar{X}_E - \\bar{X}_U$, where $\\bar{X}_E$ and $\\bar{X}_U$ are the sample means of the exposed and unexposed groups, respectively.\n-   **Measure of Extremeness**: For a two-sided test, the absolute value of the test statistic, $|T|$.\n-   **Null Distribution Generation**:\n    -   If an integer $B=0$, use exact enumeration of all $\\binom{n}{n_E}$ possible relabelings, where $n=n_E+n_U$.\n    -   If $B0$, use a Monte Carlo (MC) approximation with $B$ random permutations. A fixed random seed must be used for reproducibility in MC cases.\n-   **$p$-value Definition**: The two-sided permutation $p$-value is the proportion of permutations, $\\pi$, for which the permuted test statistic, $T^{\\pi}$, is at least as extreme as the observed test statistic, $T_{\\text{obs}}$. This is expressed as the proportion of permutations where $|T^{\\pi}| \\ge |T_{\\text{obs}}|$.\n-   **Output Requirement**: $p$-values must be rounded to $6$ decimal places.\n-   **Test Suite**:\n    -   Case $1$: exposed $[2.1, 1.9, 2.3, 2.0, 2.2]$, unexposed $[1.8, 1.7, 1.6, 1.9, 1.5]$, $B = 0$.\n    -   Case $2$: exposed $[0.0, 0.0]$, unexposed $[0.0, 0.0]$, $B = 0$.\n    -   Case $3$: exposed $[1.04, 0.92, 1.10, 0.95, 1.00, 1.08, 0.97, 1.02]$, unexposed $[0.88, 0.79, 0.85, 0.80, 0.90, 0.84, 0.76, 0.83, 0.88, 0.82, 0.91, 0.86]$, $B = 50000$, seed $42$.\n    -   Case $4$: exposed $[5.0, 5.1, 5.2, 4.9]$, unexposed $[1.0, 1.1, 0.9, 1.2]$, $B = 0$.\n\n**1.2. Validation against Criteria**\nThe problem statement is evaluated based on the provided criteria.\n-   **Scientifically Grounded**: The problem is well-grounded in established principles of non-parametric statistics. Permutation testing under the assumption of exchangeability is a standard, robust method for hypothesis testing. The choice of the difference in means as the test statistic is common and valid.\n-   **Well-Posed**: The problem is well-posed. It provides all necessary data (outcomes for two groups, number of permutations $B$), a clear definition of the test statistic, a precise rule for generating the null distribution (exact vs. MC), and an unambiguous definition of the two-sided $p$-value. A unique, stable, and meaningful solution exists for each test case.\n-   **Objective**: The problem is stated in precise, objective language, free from subjectivity or ambiguity.\n\nThe problem does not violate any of the specified invalidity conditions. It is scientifically sound, formalizable, complete, and computationally feasible. The test cases include a standard comparison, a boundary condition (identical groups), a larger sample size requiring approximation, and a case with a very clear separation between groups, providing a comprehensive test of the required implementation.\n\n**1.3. Verdict**\nThe problem is **valid**. A solution will be provided.\n\n### Step 2: Solution Formulation\n\nThe solution proceeds by formalizing the statistical procedure and then designing an algorithm to implement it.\n\n**2.1. Theoretical Framework**\nLet the observed outcomes for the exposed group be the set $Y_E = \\{y_{E,1}, \\dots, y_{E,n_E}\\}$ and for the unexposed group be $Y_U = \\{y_{U,1}, \\dots, y_{U,n_U}\\}$. The respective sample sizes are $n_E$ and $n_U$. The total number of observations is $n = n_E + n_U$.\n\nThe null hypothesis, $H_0$, is that the exposure has no effect on the outcome. Under $H_0$, the assumption of exchangeability implies that any of the $n$ subjects was equally likely to have been in the exposed or unexposed group. Therefore, the partition of the pooled outcomes $Y = Y_E \\cup Y_U$ into groups of size $n_E$ and $n_U$ is arbitrary.\n\nThe observed test statistic is the difference between the sample means:\n$$T_{\\text{obs}} = \\bar{y}_E - \\bar{y}_U = \\frac{1}{n_E}\\sum_{i=1}^{n_E} y_{E,i} - \\frac{1}{n_U}\\sum_{j=1}^{n_U} y_{U,j}$$\nTo assess the significance of $T_{\\text{obs}}$, we compare it to a reference distribution of test statistics generated under the null hypothesis. This null distribution is created by considering all possible ways to relabel the $n$ observations in the pooled set $Y$.\n\nA permutation $\\pi$ corresponds to a re-partitioning of $Y$ into a new \"exposed\" group $Y_E^{\\pi}$ of size $n_E$ and a new \"unexposed\" group $Y_U^{\\pi}$ of size $n_U$. For each such permutation, we compute a test statistic:\n$$T^{\\pi} = \\bar{y}_E^{\\pi} - \\bar{y}_U^{\\pi}$$\nThe collection of all $T^{\\pi}$ values forms the exact null distribution. The total number of unique permutations (i.e., unique partitions) is $N_{total} = \\binom{n}{n_E}$.\n\nThe two-sided $p$-value is the probability of observing a test statistic at least as extreme as what was actually observed. Using the absolute value as the measure of extremeness, the $p$-value is:\n$$p = \\frac{\\text{count}(\\{ \\pi : |T^{\\pi}| \\ge |T_{\\text{obs}}|\\})}{N_{total}}$$\n\n**2.2. Computational Strategy**\nThe implementation requires two distinct procedures based on the value of $B$.\n\n**Case 1: Exact Enumeration ($B = 0$)**\nThis approach is used when $N_{total} = \\binom{n}{n_E}$ is computationally manageable.\n1.  Combine all observations into a single pooled array, $Y$.\n2.  Calculate and store the absolute value of the observed test statistic, $|T_{\\text{obs}}|$.\n3.  Generate all possible combinations of $n_E$ indices from the set $\\{0, 1, \\dots, n-1\\}$. Each combination represents the indices of observations forming a permuted exposed group.\n4.  Initialize a counter, $N_{\\text{extreme}}$, to $0$.\n5.  Iterate through each combination of indices:\n    a. Form the permuted exposed group, $Y_E^{\\pi}$, and the permuted unexposed group, $Y_U^{\\pi}$.\n    b. Calculate the permuted test statistic, $T^{\\pi}$.\n    c. If $|T^{\\pi}| \\ge |T_{\\text{obs}}|$, increment $N_{\\text{extreme}}$.\n6.  The exact $p$-value is $p = N_{\\text{extreme}} / N_{total}$, where $N_{total} = \\binom{n}{n_E}$.\n\n**Case 2: Monte Carlo Approximation ($B  0$)**\nThis approach is used when $N_{total}$ is too large for exact enumeration.\n1.  Combine all observations into a single pooled array, $Y$.\n2.  Calculate and store $|T_{\\text{obs}}|$.\n3.  Set the random number generator seed to ensure reproducibility.\n4.  Initialize a counter, $N_{\\text{extreme}}$, to $0$.\n5.  Repeat $B$ times:\n    a. Randomly shuffle the pooled array $Y$.\n    b. The first $n_E$ elements form the permuted exposed group, $Y_E^{\\pi}$, and the remaining $n_U$ elements form the unexposed group, $Y_U^{\\pi}$.\n    c. Calculate the permuted test statistic, $T^{\\pi}$.\n    d. If $|T^{\\pi}| \\ge |T_{\\text{obs}}|$, increment $N_{\\text{extreme}}$.\n6.  The approximate $p$-value is $p \\approx N_{\\text{extreme}} / B$.\n\nThis computational framework directly implements the theoretical principles and adheres to the specifications of the problem statement. The final result for each case is rounded to $6$ decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import comb\nimport itertools\n\ndef calculate_p_value(exposed_obs, unexposed_obs, B, seed=None):\n    \"\"\"\n    Computes the two-sided permutation p-value for the difference in means.\n\n    Args:\n        exposed_obs (list): Observed outcomes for the exposed group.\n        unexposed_obs (list): Observed outcomes for the unexposed group.\n        B (int): Number of Monte Carlo permutations. If 0, performs exact enumeration.\n        seed (int, optional): Random seed for Monte Carlo simulation. Defaults to None.\n\n    Returns:\n        float: The computed p-value, rounded to 6 decimal places.\n    \"\"\"\n    exposed_arr = np.array(exposed_obs)\n    unexposed_arr = np.array(unexposed_obs)\n\n    n_e = len(exposed_arr)\n    n_u = len(unexposed_arr)\n    n = n_e + n_u\n\n    all_data = np.concatenate((exposed_arr, unexposed_arr))\n\n    # Calculate the observed test statistic\n    t_obs = np.mean(exposed_arr) - np.mean(unexposed_arr)\n    abs_t_obs = np.abs(t_obs)\n\n    extreme_count = 0\n\n    if B == 0:  # Exact enumeration\n        total_permutations = comb(n, n_e, exact=True)\n        indices = range(n)\n        \n        # Iterate over all combinations of indices for the exposed group\n        for exposed_indices in itertools.combinations(indices, n_e):\n            exposed_indices = np.array(exposed_indices)\n            unexposed_indices = np.array(list(set(indices) - set(exposed_indices)))\n            \n            perm_exposed = all_data[exposed_indices]\n            perm_unexposed = all_data[unexposed_indices]\n            \n            t_perm = np.mean(perm_exposed) - np.mean(perm_unexposed)\n            \n            if np.abs(t_perm) = abs_t_obs:\n                extreme_count += 1\n        \n        p_value = extreme_count / total_permutations\n\n    else:  # Monte Carlo approximation\n        if seed is not None:\n            rng = np.random.default_rng(seed)\n        else:\n            rng = np.random.default_rng()\n\n        for _ in range(B):\n            permuted_data = rng.permutation(all_data)\n            \n            perm_exposed = permuted_data[:n_e]\n            perm_unexposed = permuted_data[n_e:]\n            \n            t_perm = np.mean(perm_exposed) - np.mean(perm_unexposed)\n            \n            if np.abs(t_perm) = abs_t_obs:\n                extreme_count += 1\n        \n        p_value = extreme_count / B\n    \n    return round(p_value, 6)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (balanced, exact computation)\n        {'exposed': [2.1, 1.9, 2.3, 2.0, 2.2], 'unexposed': [1.8, 1.7, 1.6, 1.9, 1.5], 'B': 0, 'seed': None},\n        # Case 2 (boundary, exact computation with identical outcomes)\n        {'exposed': [0.0, 0.0], 'unexposed': [0.0, 0.0], 'B': 0, 'seed': None},\n        # Case 3 (unbalanced, Monte Carlo approximation)\n        {'exposed': [1.04, 0.92, 1.10, 0.95, 1.00, 1.08, 0.97, 1.02], 'unexposed': [0.88, 0.79, 0.85, 0.80, 0.90, 0.84, 0.76, 0.83, 0.88, 0.82, 0.91, 0.86], 'B': 50000, 'seed': 42},\n        # Case 4 (extreme difference, exact computation)\n        {'exposed': [5.0, 5.1, 5.2, 4.9], 'unexposed': [1.0, 1.1, 0.9, 1.2], 'B': 0, 'seed': None}\n    ]\n\n    results = []\n    for case in test_cases:\n        p_val = calculate_p_value(case['exposed'], case['unexposed'], case['B'], case['seed'])\n        results.append(p_val)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "4617824"}]}