{"hands_on_practices": [{"introduction": "The negative binomial distribution is fundamental in modeling \"waiting time\" processes, such as counting the number of negative outcomes before a target number of successes is achieved. While its mean and variance formulas are standard results, deriving them from first principles is an essential exercise. This practice will guide you through a powerful technique: decomposing the total count into a sum of simpler, independent geometric random variables to find the moments, reinforcing the utility of linearity of expectation and variance. [@problem_id:4962576]", "problem": "A clinical microbiology laboratory monitors whether a patient’s blood culture draws are contaminated. Each draw either results in a contamination event (failure) with probability $p \\in (0,1)$ or a clean result (success) with probability $1-p$. Assume draws are conducted sequentially and independently under stable conditions. The protocol continues until $r \\in \\mathbb{N}$ clean results (successes) have been observed, at which point the sampling stops. Define the random variable $X$ to be the total number of contamination events (failures) that occur before the $r$-th clean result is observed.\n\nStarting from the definitions of independent Bernoulli trials and the expectation and variance of random variables, and without appealing to any pre-derived formulas for the negative binomial distribution, derive closed-form analytic expressions for the expectation $E[X]$ and the variance $\\mathrm{Var}(X)$ in terms of $r$ and $p$ under this failures-until-$r$ stopping rule. Express your final answer as exact symbolic expressions; no numerical rounding is required.", "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n- A sequence of independent Bernoulli trials is conducted.\n- Each trial results in a \"contamination event\" (failure) with probability $p$ or a \"clean result\" (success) with probability $1-p$.\n- The probability of failure is specified as $p \\in (0,1)$.\n- The process terminates upon the observation of the $r$-th success, where $r \\in \\mathbb{N}$.\n- The random variable $X$ is defined as the total number of failures that occur before the $r$-th success.\n- The objective is to derive the expectation $E[X]$ and variance $\\mathrm{Var}(X)$ in terms of $r$ and $p$.\n- The derivation must not rely on pre-derived formulas for the negative binomial distribution but must start from the definitions of independent Bernoulli trials and the properties of expectation and variance.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It describes a classic scenario in probability theory modeled by the negative binomial distribution. The setup of sequential, independent Bernoulli trials is a fundamental concept in statistics. The parameters $p$ and $r$ are clearly defined with appropriate domains, ensuring the problem is self-contained and mathematically consistent. The task is to perform a derivation from first principles, which is a standard and non-trivial exercise in probability theory. The problem does not violate any scientific principles, is not ambiguous, and contains all necessary information for a unique solution.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full, reasoned solution will be provided.\n\n### Derivation\nThe problem describes a sequence of independent Bernoulli trials where the probability of success is $1-p$ and the probability of failure is $p$. The random variable $X$ counts the total number of failures until $r$ successes are accumulated.\n\nTo derive the expectation and variance of $X$ from first principles, we can decompose $X$ into a sum of simpler random variables. Let $Y_i$ be the number of failures between the $(i-1)$-th success and the $i$-th success, for $i=1, 2, \\dots, r$. We define the $0$-th success as the start of the experiment.\n\nThe total number of failures $X$ is the sum of these intermediate failure counts:\n$$X = Y_1 + Y_2 + \\dots + Y_r$$\nBecause the trials are independent and the process conditions are stable, the number of failures after one success does not influence the number of failures until the next. Therefore, the random variables $Y_1, Y_2, \\dots, Y_r$ are independent and identically distributed (i.i.d.).\n\nLet us determine the probability distribution of any one of these variables, say $Y_i$. The event $Y_i=k$ (where $k \\in \\{0, 1, 2, \\dots\\}$) signifies that, after the $(i-1)$-th success, there are exactly $k$ failures followed by one success. The probability of this specific sequence of $k+1$ trials is $p^k(1-p)$. Thus, the probability mass function (PMF) of $Y_i$ is:\n$$P(Y_i=k) = p^k(1-p) \\quad \\text{for } k = 0, 1, 2, \\dots$$\nThis defines a geometric distribution on the non-negative integers.\n\n**Derivation of Expectation $E[X]$**\n\nBy the linearity of expectation, the expectation of a sum of random variables is the sum of their expectations:\n$$E[X] = E\\left[\\sum_{i=1}^{r} Y_i\\right] = \\sum_{i=1}^{r} E[Y_i]$$\nSince the $Y_i$ are identically distributed, $E[Y_i] = E[Y_1]$ for all $i$. Therefore:\n$$E[X] = r E[Y_1]$$\nWe compute $E[Y_1]$ directly from its definition:\n$$E[Y_1] = \\sum_{k=0}^{\\infty} k \\cdot P(Y_1=k) = \\sum_{k=0}^{\\infty} k p^k (1-p) = (1-p) \\sum_{k=0}^{\\infty} k p^k$$\nTo evaluate the summation, we use the formula for a geometric series and its derivative. For $|z|1$, we have:\n$$\\sum_{k=0}^{\\infty} z^k = \\frac{1}{1-z}$$\nDifferentiating both sides with respect to $z$ gives:\n$$\\frac{d}{dz} \\sum_{k=0}^{\\infty} z^k = \\sum_{k=1}^{\\infty} k z^{k-1} = \\frac{d}{dz} \\left(\\frac{1}{1-z}\\right) = \\frac{1}{(1-z)^2}$$\nMultiplying by $z$:\n$$\\sum_{k=1}^{\\infty} k z^k = \\frac{z}{(1-z)^2}$$\nSince the $k=0$ term in $\\sum k z^k$ is zero, this is also equal to $\\sum_{k=0}^{\\infty} k z^k$. Substituting $z=p$ (since $p \\in (0,1)$, we have $|p|1$):\n$$\\sum_{k=0}^{\\infty} k p^k = \\frac{p}{(1-p)^2}$$\nNow, we substitute this back into the expression for $E[Y_1]$:\n$$E[Y_1] = (1-p) \\left( \\frac{p}{(1-p)^2} \\right) = \\frac{p}{1-p}$$\nFinally, the expectation of $X$ is:\n$$E[X] = r E[Y_1] = \\frac{rp}{1-p}$$\n\n**Derivation of Variance $\\mathrm{Var}(X)$**\n\nSince the random variables $Y_1, Y_2, \\dots, Y_r$ are independent, the variance of their sum is the sum of their variances:\n$$\\mathrm{Var}(X) = \\mathrm{Var}\\left(\\sum_{i=1}^{r} Y_i\\right) = \\sum_{i=1}^{r} \\mathrm{Var}(Y_i)$$\nAs the $Y_i$ are identically distributed, $\\mathrm{Var}(Y_i) = \\mathrm{Var}(Y_1)$ for all $i$. Thus:\n$$\\mathrm{Var}(X) = r \\mathrm{Var}(Y_1)$$\nThe variance of $Y_1$ is given by $\\mathrm{Var}(Y_1) = E[Y_1^2] - (E[Y_1])^2$. We already found $E[Y_1]$. We now need to compute $E[Y_1^2]$:\n$$E[Y_1^2] = \\sum_{k=0}^{\\infty} k^2 \\cdot P(Y_1=k) = \\sum_{k=0}^{\\infty} k^2 p^k (1-p) = (1-p) \\sum_{k=0}^{\\infty} k^2 p^k$$\nTo evaluate $\\sum k^2 p^k$, we differentiate the series $\\sum k z^k = \\frac{z}{(1-z)^2}$ with respect to $z$:\n$$\\frac{d}{dz} \\sum_{k=1}^{\\infty} k z^k = \\sum_{k=1}^{\\infty} k^2 z^{k-1} = \\frac{d}{dz} \\left( \\frac{z}{(1-z)^2} \\right)$$\nUsing the quotient rule:\n$$\\frac{d}{dz} \\left( \\frac{z}{(1-z)^2} \\right) = \\frac{(1)(1-z)^2 - z(2(1-z)(-1))}{((1-z)^2)^2} = \\frac{(1-z)^2 + 2z(1-z)}{(1-z)^4} = \\frac{1-z+2z}{(1-z)^3} = \\frac{1+z}{(1-z)^3}$$\nMultiplying by $z$:\n$$\\sum_{k=1}^{\\infty} k^2 z^k = \\frac{z(1+z)}{(1-z)^3}$$\nThis is equivalent to $\\sum_{k=0}^{\\infty} k^2 z^k$. We substitute $z=p$:\n$$\\sum_{k=0}^{\\infty} k^2 p^k = \\frac{p(1+p)}{(1-p)^3}$$\nNow we compute $E[Y_1^2]$:\n$$E[Y_1^2] = (1-p) \\left( \\frac{p(1+p)}{(1-p)^3} \\right) = \\frac{p(1+p)}{(1-p)^2}$$\nWe can now calculate the variance of $Y_1$:\n$$\\mathrm{Var}(Y_1) = E[Y_1^2] - (E[Y_1])^2 = \\frac{p(1+p)}{(1-p)^2} - \\left(\\frac{p}{1-p}\\right)^2$$\n$$\\mathrm{Var}(Y_1) = \\frac{p+p^2}{(1-p)^2} - \\frac{p^2}{(1-p)^2} = \\frac{p+p^2-p^2}{(1-p)^2} = \\frac{p}{(1-p)^2}$$\nFinally, the variance of $X$ is:\n$$\\mathrm{Var}(X) = r \\mathrm{Var}(Y_1) = \\frac{rp}{(1-p)^2}$$\n\nThe derived expressions for the expectation and variance are $E[X] = \\frac{rp}{1-p}$ and $\\mathrm{Var}(X) = \\frac{rp}{(1-p)^2}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{rp}{1-p}  \\frac{rp}{(1-p)^2} \\end{pmatrix}}$$", "id": "4962576"}, {"introduction": "In medical research, measurement instruments often have a limited range, leading to truncated data where observations outside certain bounds are not recorded. This scenario requires us to work with conditional distributions. This practice explores how to derive the expectation and variance of a normally distributed variable given that it falls within a specific interval, a crucial skill for accurately modeling data from assays with defined quantification limits. [@problem_id:4962616]", "problem": "In a clinical pharmacology study, a laboratory quantifies a circulating biomarker using an assay with a finite dynamic range. True biomarker concentrations in the target population are well described by a Gaussian model: $X \\sim \\mathcal{N}(\\mu,\\sigma^{2})$ with unknown parameters $\\,\\mu \\in \\mathbb{R}\\,$ and $\\,\\sigma  0\\,$. However, the assay reports a numeric value only when the true concentration falls strictly between the lower and upper quantification limits, denoted $\\,a\\,$ and $\\,b\\,$ with $\\,a  b\\,$. Outside this interval, the measurement is censored and no numeric value is reported. In an estimation pipeline that uses conditional moments (for example, as part of an Expectation–Maximization procedure), you need the conditional mean and conditional variance of $\\,X\\,$ given that it lies within the quantifiable interval.\n\nStarting only from the core definitions of conditional expectation for continuous random variables and the standard change-of-variables mapping between a general normal distribution and the standard normal distribution, derive closed-form expressions for $\\,E[X \\mid a  X  b]\\,$ and $\\,\\mathrm{Var}(X \\mid a  X  b]\\,$ in terms of the standard normal probability density function (pdf) and cumulative distribution function (cdf). Denote the standard normal pdf by $\\,\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^{2}/2)\\,$ and the standard normal cdf by $\\,\\Phi(z) = \\int_{-\\infty}^{z} \\phi(t)\\,\\mathrm{d}t$.\n\nYour final expressions must depend only on $\\,\\mu\\,$, $\\,\\sigma\\,$, $\\,a\\,$, $\\,b\\,$, $\\,\\phi(\\cdot)\\,$, and $\\,\\Phi(\\cdot)\\,$. Express the final answer as closed-form analytic expressions. No numerical evaluation or rounding is required. The final answer must contain only the two expressions requested, in a single row matrix, in the order $\\,E[X \\mid aXb]\\,$ then $\\,\\mathrm{Var}(X \\mid aXb]$.", "solution": "The problem as stated constitutes a valid and well-posed question in the field of mathematical statistics. It requests the derivation of the conditional mean and variance of a normally distributed random variable, given that it falls within a specified interval. This is a standard problem concerning the truncated normal distribution, which has direct applications in fields such as econometrics, biostatistics, and engineering, including the clinical pharmacology context provided. The problem is scientifically grounded, objective, and contains all necessary information for a unique solution.\n\nLet the random variable $X$ follow a normal distribution with mean $\\mu$ and variance $\\sigma^2$, denoted as $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The probability density function (pdf) of $X$ is given by:\n$$f_X(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\nWe are interested in the moments of $X$ conditional on the event $C = \\{x \\in \\mathbb{R} \\mid a  x  b\\}$. The probability of this event, $P(C)$, is the integral of the pdf over the interval $(a, b)$. To compute this, we standardize the variable. Let $Z = \\frac{X-\\mu}{\\sigma}$. $Z$ is a standard normal random variable, $Z \\sim \\mathcal{N}(0, 1)$, with pdf $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2)$ and cumulative distribution function (cdf) $\\Phi(z) = \\int_{-\\infty}^{z} \\phi(t)\\,\\mathrm{d}t$.\n\nThe event $a  X  b$ is equivalent to $\\frac{a-\\mu}{\\sigma}  Z  \\frac{b-\\mu}{\\sigma}$. For notational simplicity, let $\\alpha = \\frac{a-\\mu}{\\sigma}$ and $\\beta = \\frac{b-\\mu}{\\sigma}$. Since $a  b$ and $\\sigma  0$, it follows that $\\alpha  \\beta$. The probability of the conditioning event is:\n$$P(C) = P(a  X  b) = P(\\alpha  Z  \\beta) = \\int_{\\alpha}^{\\beta} \\phi(z) \\, \\mathrm{d}z = \\Phi(\\beta) - \\Phi(\\alpha)$$\nThe conditional pdf of $X$ given $C$ is $f_{X|C}(x) = \\frac{f_X(x)}{P(C)}$ for $x \\in (a, b)$ and $0$ otherwise.\n\nFirst, we derive the conditional expectation, $E[X \\mid a  X  b]$.\nBy definition, the conditional expectation is:\n$$E[X \\mid a  X  b] = \\int_a^b x \\cdot f_{X|C}(x) \\, \\mathrm{d}x = \\frac{1}{P(C)} \\int_a^b x f_X(x) \\, \\mathrm{d}x$$\nWe evaluate the integral by changing the variable to $z = \\frac{x-\\mu}{\\sigma}$, which implies $x = \\mu + \\sigma z$ and $\\mathrm{d}x = \\sigma \\, \\mathrm{d}z$. The expression $f_X(x) \\, \\mathrm{d}x$ transforms to $\\phi(z) \\, \\mathrm{d}z$. The limits of integration change from $(a, b)$ to $(\\alpha, \\beta)$.\n$$\\int_a^b x f_X(x) \\, \\mathrm{d}x = \\int_{\\alpha}^{\\beta} (\\mu + \\sigma z) \\phi(z) \\, \\mathrm{d}z = \\mu \\int_{\\alpha}^{\\beta} \\phi(z) \\, \\mathrm{d}z + \\sigma \\int_{\\alpha}^{\\beta} z \\phi(z) \\, \\mathrm{d}z$$\nThe first integral is $\\mu P(C)$. For the second integral, we use the fact that $\\frac{\\mathrm{d}}{\\mathrm{d}z}\\phi(z) = -z\\phi(z)$.\n$$\\int_{\\alpha}^{\\beta} z \\phi(z) \\, \\mathrm{d}z = \\int_{\\alpha}^{\\beta} \\left(-\\frac{\\mathrm{d}}{\\mathrm{d}z}\\phi(z)\\right) \\, \\mathrm{d}z = [-\\phi(z)]_{\\alpha}^{\\beta} = -\\phi(\\beta) + \\phi(\\alpha) = \\phi(\\alpha) - \\phi(\\beta)$$\nSubstituting back, we get:\n$$\\int_a^b x f_X(x) \\, \\mathrm{d}x = \\mu P(C) + \\sigma(\\phi(\\alpha) - \\phi(\\beta))$$\nThus, the conditional expectation is:\n$$E[X \\mid a  X  b] = \\frac{\\mu P(C) + \\sigma(\\phi(\\alpha) - \\phi(\\beta))}{P(C)} = \\mu + \\sigma \\frac{\\phi(\\alpha) - \\phi(\\beta)}{\\Phi(\\beta) - \\Phi(\\alpha)}$$\n\nNext, we derive the conditional variance, $\\mathrm{Var}(X \\mid a  X  b) = E[X^2 \\mid a  X  b] - (E[X \\mid a  X  b])^2$. We first compute the second conditional moment $E[X^2 \\mid a  X  b]$.\n$$E[X^2 \\mid a  X  b] = \\frac{1}{P(C)} \\int_a^b x^2 f_X(x) \\, \\mathrm{d}x$$\nUsing the same change of variables:\n$$\\int_a^b x^2 f_X(x) \\, \\mathrm{d}x = \\int_{\\alpha}^{\\beta} (\\mu + \\sigma z)^2 \\phi(z) \\, \\mathrm{d}z = \\int_{\\alpha}^{\\beta} (\\mu^2 + 2\\mu\\sigma z + \\sigma^2 z^2) \\phi(z) \\, \\mathrm{d}z$$\n$$= \\mu^2 \\int_{\\alpha}^{\\beta} \\phi(z) \\, \\mathrm{d}z + 2\\mu\\sigma \\int_{\\alpha}^{\\beta} z \\phi(z) \\, \\mathrm{d}z + \\sigma^2 \\int_{\\alpha}^{\\beta} z^2 \\phi(z) \\, \\mathrm{d}z$$\nThe first two terms are $\\mu^2 P(C)$ and $2\\mu\\sigma(\\phi(\\alpha) - \\phi(\\beta))$. For the third integral, $\\int z^2 \\phi(z) \\, \\mathrm{d}z$, we use integration by parts with $u=z$ and $\\mathrm{d}v = z\\phi(z)\\,\\mathrm{d}z$, so that $\\mathrm{d}u = \\mathrm{d}z$ and $v = -\\phi(z)$.\n$$\\int_{\\alpha}^{\\beta} z^2 \\phi(z) \\, \\mathrm{d}z = [-z\\phi(z)]_{\\alpha}^{\\beta} - \\int_{\\alpha}^{\\beta} (-\\phi(z)) \\, \\mathrm{d}z = [-\\beta\\phi(\\beta) - (-\\alpha\\phi(\\alpha))] + [\\Phi(z)]_{\\alpha}^{\\beta}$$\n$$= \\alpha\\phi(\\alpha) - \\beta\\phi(\\beta) + \\Phi(\\beta) - \\Phi(\\alpha) = \\alpha\\phi(\\alpha) - \\beta\\phi(\\beta) + P(C)$$\nThe integral for the second moment is:\n$$\\int_a^b x^2 f_X(x) \\, \\mathrm{d}x = \\mu^2 P(C) + 2\\mu\\sigma(\\phi(\\alpha) - \\phi(\\beta)) + \\sigma^2(\\alpha\\phi(\\alpha) - \\beta\\phi(\\beta) + P(C))$$\nDividing by $P(C)$ gives $E[X^2 \\mid a  X  b]$:\n$$E[X^2 \\mid a  X  b] = \\mu^2 + 2\\mu\\sigma\\frac{\\phi(\\alpha) - \\phi(\\beta)}{P(C)} + \\sigma^2\\left(1 + \\frac{\\alpha\\phi(\\alpha) - \\beta\\phi(\\beta)}{P(C)}\\right)$$\nNow we compute the variance. Let $K = \\frac{\\phi(\\alpha) - \\phi(\\beta)}{P(C)}$. Then $E[X \\mid a  X  b] = \\mu + \\sigma K$ and $(E[X \\mid a  X  b])^2 = \\mu^2 + 2\\mu\\sigma K + \\sigma^2 K^2$.\n$$\\mathrm{Var}(X \\mid a  X  b) = E[X^2 \\mid a  X  b] - (E[X \\mid a  X  b])^2$$\n$$= \\left(\\mu^2 + 2\\mu\\sigma K + \\sigma^2\\left(1 + \\frac{\\alpha\\phi(\\alpha) - \\beta\\phi(\\beta)}{P(C)}\\right)\\right) - (\\mu^2 + 2\\mu\\sigma K + \\sigma^2 K^2)$$\n$$= \\sigma^2\\left(1 + \\frac{\\alpha\\phi(\\alpha) - \\beta\\phi(\\beta)}{P(C)} - K^2\\right)$$\nSubstituting for $P(C)$ and $K$:\n$$\\mathrm{Var}(X \\mid a  X  b) = \\sigma^2 \\left[ 1 + \\frac{\\alpha\\phi(\\alpha) - \\beta\\phi(\\beta)}{\\Phi(\\beta) - \\Phi(\\alpha)} - \\left(\\frac{\\phi(\\alpha) - \\phi(\\beta)}{\\Phi(\\beta) - \\Phi(\\alpha)}\\right)^2 \\right]$$\nFinally, replacing $\\alpha$ and $\\beta$ with their definitions in terms of $a$, $b$, $\\mu$, and $\\sigma$ gives the complete expressions.\n\nThe conditional expectation is:\n$$E[X \\mid a  X  b] = \\mu + \\sigma \\frac{\\phi\\left(\\frac{a-\\mu}{\\sigma}\\right) - \\phi\\left(\\frac{b-\\mu}{\\sigma}\\right)}{\\Phi\\left(\\frac{b-\\mu}{\\sigma}\\right) - \\Phi\\left(\\frac{a-\\mu}{\\sigma}\\right)}$$\nThe conditional variance is:\n$$\\mathrm{Var}(X \\mid a  X  b) = \\sigma^2 \\left[ 1 + \\frac{\\frac{a-\\mu}{\\sigma}\\phi\\left(\\frac{a-\\mu}{\\sigma}\\right) - \\frac{b-\\mu}{\\sigma}\\phi\\left(\\frac{b-\\mu}{\\sigma}\\right)}{\\Phi\\left(\\frac{b-\\mu}{\\sigma}\\right) - \\Phi\\left(\\fraca-\\mu}{\\sigma}\\right)} - \\left( \\frac{\\phi\\left(\\frac{a-\\mu}{\\sigma}\\right) - \\phi\\left(\\frac{b-\\mu}{\\sigma}\\right)}{\\Phi\\left(\\frac{b-\\mu}{\\sigma}\\right) - \\Phi\\left(\\frac{a-\\mu}{\\sigma}\\right)} \\right)^2 \\right]$$\nThese are the final closed-form expressions.", "answer": "$$ \\boxed{\n\\begin{pmatrix}\n\\mu + \\sigma \\frac{\\phi\\left(\\frac{a-\\mu}{\\sigma}\\right) - \\phi\\left(\\frac{b-\\mu}{\\sigma}\\right)}{\\Phi\\left(\\frac{b-\\mu}{\\sigma}\\right) - \\Phi\\left(\\frac{a-\\mu}{\\sigma}\\right)}  \\sigma^2 \\left[ 1 + \\frac{\\frac{a-\\mu}{\\sigma}\\phi\\left(\\frac{a-\\mu}{\\sigma}\\right) - \\frac{b-\\mu}{\\sigma}\\phi\\left(\\frac{b-\\mu}{\\sigma}\\right)}{\\Phi\\left(\\frac{b-\\mu}{\\sigma}\\right) - \\Phi\\left(\\frac{a-\\mu}{\\sigma}\\right)} - \\left( \\frac{\\phi\\left(\\frac{a-\\mu}{\\sigma}\\right) - \\phi\\left(\\frac{b-\\mu}{\\sigma}\\right)}{\\Phi\\left(\\frac{b-\\mu}{\\sigma}\\right) - \\Phi\\left(\\frac{a-\\mu}{\\sigma}\\right)} \\right)^2 \\right]\n\\end{pmatrix}\n} $$", "id": "4962616"}, {"introduction": "A deep understanding of expectation is critical when analyzing transformed data, a common practice in medical statistics. Biomarker data are often log-transformed to achieve normality, but the ultimate interest lies in the mean on the original scale. This practice demonstrates a crucial concept related to Jensen's inequality: the exponential of the sample mean, $\\exp(\\hat{\\mu})$, is a biased estimator for the true mean of a log-normal distribution. By deriving the magnitude of this bias, you will learn how to construct a corrected, more accurate estimator. [@problem_id:4962594]", "problem": "In a longitudinal clinical study of an anti-inflammatory therapy, the within-patient plasma concentration of a biomarker is modeled as log-normal due to multiplicative biological variability. Specifically, for a given time point, suppose $Y$ denotes the biomarker concentration (measured on the original scale), and let $X = \\ln(Y)$. Assume a parametric model in which $X$ is independently and identically distributed across patients as a normal random variable with mean $\\mu$ and variance $\\sigma^{2}$, that is, $X \\sim N(\\mu,\\sigma^{2})$. From an observed sample $\\{Y_{1},\\dots,Y_{n}\\}$, define $X_{i} = \\ln(Y_{i})$ and denote the maximum likelihood estimators (MLE) for the normal parameters by $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$ and $\\hat{\\sigma}^{2} = \\frac{1}{n}\\sum_{i=1}^{n}(X_{i} - \\hat{\\mu})^{2}$. A widely used “naive” estimator of the expected concentration $E[Y]$ is $\\exp(\\hat{\\mu})$. \n\nUsing only definitions of expectation and well-established properties of the normal distribution, provide a first-principles derivation of $E[Y]$ in terms of $\\mu$ and $\\sigma^{2}$, and then show that the naive estimator $\\exp(\\hat{\\mu})$ is biased for $E[Y]$ under the specified model. Based on these derivations, propose and simplify a closed-form, model-based plug-in estimator that corrects for the transformation-induced bias when estimating $E[Y]$ from the observed sample, expressed solely in terms of $\\hat{\\mu}$ and $\\hat{\\sigma}^{2}$. Your final answer must be a single closed-form analytic expression. No rounding is required, and no units should be included in your final expression.", "solution": "The problem asks for a derivation of the expected value of a log-normally distributed random variable, an analysis of the bias of a naive estimator for this expected value, and the formulation of a corrected plug-in estimator. The validation of the problem statement confirms that it is scientifically grounded, well-posed, and objective. We can proceed with the solution.\n\nThe problem defines a random variable $Y$ representing a biomarker concentration, such that $X = \\ln(Y)$ follows a normal distribution with mean $\\mu$ and variance $\\sigma^2$. We write this as $X \\sim N(\\mu, \\sigma^2)$. The random variable $Y$ is therefore log-normally distributed.\n\nFirst, we derive the expected value of $Y$, denoted $E[Y]$, from first principles. Since $Y = \\exp(X)$, its expectation is given by the integral of $\\exp(x)$ weighted by the probability density function (PDF) of $X$. The PDF of $X \\sim N(\\mu, \\sigma^2)$ is:\n$$f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\nThe expectation of $Y$ is then:\n$$E[Y] = E[\\exp(X)] = \\int_{-\\infty}^{\\infty} \\exp(x) f_X(x) dx = \\int_{-\\infty}^{\\infty} \\exp(x) \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) dx$$\nWe can combine the arguments of the exponential functions:\n$$E[Y] = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\int_{-\\infty}^{\\infty} \\exp\\left(x - \\frac{(x-\\mu)^2}{2\\sigma^2}\\right) dx$$\nTo solve this integral, we complete the square for the term in the exponent:\n$$x - \\frac{(x-\\mu)^2}{2\\sigma^2} = x - \\frac{x^2 - 2\\mu x + \\mu^2}{2\\sigma^2} = \\frac{2\\sigma^2 x - (x^2 - 2\\mu x + \\mu^2)}{2\\sigma^2}$$\n$$= -\\frac{1}{2\\sigma^2} [x^2 - 2\\mu x - 2\\sigma^2 x + \\mu^2] = -\\frac{1}{2\\sigma^2} [x^2 - 2(\\mu+\\sigma^2)x + \\mu^2]$$\nWe complete the square for the polynomial in $x$: $x^2 - 2(\\mu+\\sigma^2)x + \\mu^2 = [x - (\\mu+\\sigma^2)]^2 - (\\mu+\\sigma^2)^2 + \\mu^2 = [x - (\\mu+\\sigma^2)]^2 - (\\mu^2 + 2\\mu\\sigma^2 + \\sigma^4) + \\mu^2 = [x - (\\mu+\\sigma^2)]^2 - 2\\mu\\sigma^2 - \\sigma^4$.\nSubstituting this back into the exponent:\n$$-\\frac{1}{2\\sigma^2} \\left( [x - (\\mu+\\sigma^2)]^2 - 2\\mu\\sigma^2 - \\sigma^4 \\right) = -\\frac{[x - (\\mu+\\sigma^2)]^2}{2\\sigma^2} + \\frac{2\\mu\\sigma^2 + \\sigma^4}{2\\sigma^2} = -\\frac{[x - (\\mu+\\sigma^2)]^2}{2\\sigma^2} + \\mu + \\frac{\\sigma^2}{2}$$\nWe can now rewrite the integral for $E[Y]$:\n$$E[Y] = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{[x - (\\mu+\\sigma^2)]^2}{2\\sigma^2} + \\mu + \\frac{\\sigma^2}{2}\\right) dx$$\nThe term $\\exp(\\mu + \\frac{\\sigma^2}{2})$ is a constant with respect to $x$ and can be factored out:\n$$E[Y] = \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right) \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{[x - (\\mu+\\sigma^2)]^2}{2\\sigma^2}\\right) dx$$\nThe integral is the total area under the PDF of a normal distribution with mean $\\mu' = \\mu+\\sigma^2$ and variance $\\sigma^2$. This integral is equal to $1$.\nTherefore, the expected value of $Y$ is:\n$$E[Y] = \\exp\\left(\\mu + \\frac{1}{2}\\sigma^2\\right)$$\nThis result can also be obtained by recognizing that $E[\\exp(X)]$ is the moment-generating function (MGF) of $X$, $M_X(t) = E[\\exp(tX)]$, evaluated at $t=1$. For $X \\sim N(\\mu, \\sigma^2)$, the MGF is $M_X(t) = \\exp(\\mu t + \\frac{1}{2}\\sigma^2 t^2)$. Evaluating at $t=1$ yields the same result.\n\nNext, we address the bias of the naive estimator $\\exp(\\hat{\\mu})$. The estimator for $\\mu$ is the sample mean of the log-transformed data, $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$. Since each $X_i$ is an independent random variable from $N(\\mu, \\sigma^2)$, $\\hat{\\mu}$ is also normally distributed. Its mean is $E[\\hat{\\mu}] = E[\\frac{1}{n}\\sum X_i] = \\frac{1}{n}\\sum E[X_i] = \\frac{1}{n}(n\\mu) = \\mu$. Its variance is $Var(\\hat{\\mu}) = Var(\\frac{1}{n}\\sum X_i) = \\frac{1}{n^2}\\sum Var(X_i) = \\frac{1}{n^2}(n\\sigma^2) = \\frac{\\sigma^2}{n}$.\nThus, $\\hat{\\mu} \\sim N(\\mu, \\frac{\\sigma^2}{n})$.\n\nTo evaluate the bias of the estimator $\\exp(\\hat{\\mu})$, we calculate its expectation, $E[\\exp(\\hat{\\mu})]$. This is the MGF of $\\hat{\\mu}$ evaluated at $t=1$. Using the MGF formula for a normal variable with mean $\\mu$ and variance $\\sigma^2/n$:\n$$E[\\exp(\\hat{\\mu})] = \\exp\\left(\\mu \\cdot 1 + \\frac{1}{2}\\left(\\frac{\\sigma^2}{n}\\right) \\cdot 1^2\\right) = \\exp\\left(\\mu + \\frac{\\sigma^2}{2n}\\right)$$\nThe bias of the estimator is the difference between its expectation and the true value of the quantity being estimated:\n$$B(\\exp(\\hat{\\mu})) = E[\\exp(\\hat{\\mu})] - E[Y] = \\exp\\left(\\mu + \\frac{\\sigma^2}{2n}\\right) - \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)$$\nFor any finite sample size $n > 1$ and non-zero biological variability $\\sigma^2 > 0$, we have $\\frac{\\sigma^2}{2n}  \\frac{\\sigma^2}{2}$. Since the exponential function is strictly increasing, $\\exp(\\mu + \\frac{\\sigma^2}{2n})  \\exp(\\mu + \\frac{\\sigma^2}{2})$. Therefore, the bias is non-zero (specifically, it is negative), and the estimator $\\exp(\\hat{\\mu})$ is biased for $E[Y]$. It systematically underestimates the true mean concentration. This bias arises from applying the non-linear exponential transformation to an estimator of the mean on the log scale, while ignoring the contribution of the variance (a consequence of Jensen's inequality). The \"naive\" estimator effectively targets $\\exp(\\mu)$ rather than the correct quantity $\\exp(\\mu + \\frac{1}{2}\\sigma^2)$.\n\nFinally, we propose a model-based plug-in estimator that corrects for this transformation-induced bias. The derivation of $E[Y]$ shows that a correct estimation of the mean requires accounting for both $\\mu$ and $\\sigma^2$. The \"correction\" for the bias of the naive estimator involves incorporating the variance term $\\frac{1}{2}\\sigma^2$ into the expression. The corrected target expression is $\\exp(\\mu + \\frac{1}{2}\\sigma^2)$.\nA \"plug-in\" estimator is formed by substituting parameter estimators for the true parameters in the expression for the quantity of interest. The problem provides the maximum likelihood estimators (MLEs) for $\\mu$ and $\\sigma^2$:\n$$\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$$\n$$\\hat{\\sigma}^{2} = \\frac{1}{n}\\sum_{i=1}^{n}(X_{i} - \\hat{\\mu})^{2}$$\nSubstituting these estimators into the expression for $E[Y]$ gives the model-based plug-in estimator, which we denote as $\\hat{E}[Y]$:\n$$\\hat{E}[Y] = \\exp\\left(\\hat{\\mu} + \\frac{1}{2}\\hat{\\sigma}^2\\right)$$\nThis estimator explicitly uses the model structure ($Y$ is log-normal) and corrects the primary deficiency of the naive estimator by including an estimate of the variance term. The expression is already in its simplest closed form in terms of the specified estimators $\\hat{\\mu}$ and $\\hat{\\sigma}^2$.", "answer": "$$\\boxed{\\exp\\left(\\hat{\\mu} + \\frac{1}{2}\\hat{\\sigma}^{2}\\right)}$$", "id": "4962594"}]}