{"hands_on_practices": [{"introduction": "Understanding the Central Limit Theorem begins with its core subject: the sample mean. This foundational exercise guides you to derive the variance of the sample mean from first principles, revealing why its variability decreases as the sample size $n$ grows. Mastering this derivation is key to grasping the concept of the standard error and its role in constructing the confidence intervals that are ubiquitous in medical research. [@problem_id:4986781]", "problem": "A clinical laboratory is validating a high-throughput assay for measuring a biomarker of systemic inflammation in hospitalized patients. Let $X_1, X_2, \\ldots, X_n$ denote the biomarker measurements on $n$ distinct patients, assumed to be independent and identically distributed (i.i.d.) with finite mean $E[X_i] = \\mu$ and finite variance $\\operatorname{Var}(X_i) = \\sigma^2$. The study team will report the sample mean $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ and a large-sample confidence interval for $\\mu$ based on the Central Limit Theorem (CLT).\n\nStarting only from core definitions of expectation and variance, the property that independence implies factorization of expectations, and the well-tested fact that the Central Limit Theorem (CLT) provides a normal approximation for appropriately standardized averages when the second moment is finite, derive the variance of the sample mean $\\bar{X}_n$ from first principles without invoking any pre-memorized variance formulas. Then connect this derivation to the standard error that underlies large-sample confidence intervals for the mean in this medical context, explaining why the standard error scales with $n$ as it does under the CLT.\n\nProvide your final answer as a closed-form analytic expression for $\\operatorname{Var}(\\bar{X}_n)$ in terms of $\\sigma$ and $n$. Do not include any units in your final boxed expression. No numerical rounding is required.", "solution": "The problem requires the derivation of the variance of the sample mean, $\\operatorname{Var}(\\bar{X}_n)$, from first principles for a set of independent and identically distributed (i.i.d.) random variables $X_1, X_2, \\ldots, X_n$. We are given that $E[X_i] = \\mu$ and $\\operatorname{Var}(X_i) = \\sigma^2$ are finite for all $i \\in \\{1, \\ldots, n\\}$. The derivation must rely only on the core definitions of expectation and variance, and the property of independence.\n\nFirst, let us state the definition of the sample mean, $\\bar{X}_n$:\n$$\n\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i\n$$\nThe core definition of the variance of any random variable $Y$ is given by $\\operatorname{Var}(Y) = E[(Y - E[Y])^2]$. To apply this definition to $\\bar{X}_n$, we must first determine its expectation, $E[\\bar{X}_n]$.\n\nUsing the linearity property of the expectation operator, which states that $E[aY + bZ] = aE[Y] + bE[Z]$ for constants $a, b$ and random variables $Y, Z$, we can compute $E[\\bar{X}_n]$ as follows:\n$$\nE[\\bar{X}_n] = E\\left[\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right]\n$$\nThe term $\\frac{1}{n}$ is a constant and can be factored out of the expectation:\n$$\nE[\\bar{X}_n] = \\frac{1}{n} E\\left[\\sum_{i=1}^{n} X_i\\right]\n$$\nThe expectation of a sum is the sum of the expectations:\n$$\nE[\\bar{X}_n] = \\frac{1}{n} \\sum_{i=1}^{n} E[X_i]\n$$\nSince the random variables are identically distributed with mean $\\mu$, we have $E[X_i] = \\mu$ for all $i$. Substituting this into the expression gives:\n$$\nE[\\bar{X}_n] = \\frac{1}{n} \\sum_{i=1}^{n} \\mu = \\frac{1}{n} (n\\mu) = \\mu\n$$\nThus, the sample mean $\\bar{X}_n$ is an unbiased estimator of the population mean $\\mu$.\n\nNow we can proceed to derive the variance, $\\operatorname{Var}(\\bar{X}_n)$. Using the definition of variance with $Y = \\bar{X}_n$ and $E[Y] = \\mu$:\n$$\n\\operatorname{Var}(\\bar{X}_n) = E\\left[ (\\bar{X}_n - E[\\bar{X}_n])^2 \\right] = E\\left[ (\\bar{X}_n - \\mu)^2 \\right]\n$$\nSubstitute the definition of $\\bar{X}_n$:\n$$\n\\bar{X}_n - \\mu = \\left(\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right) - \\mu = \\frac{1}{n}\\left(\\sum_{i=1}^{n} X_i - n\\mu\\right) = \\frac{1}{n}\\sum_{i=1}^{n} (X_i - \\mu)\n$$\nSquaring this expression, we obtain:\n$$\n(\\bar{X}_n - \\mu)^2 = \\left( \\frac{1}{n}\\sum_{i=1}^{n} (X_i - \\mu) \\right)^2 = \\frac{1}{n^2} \\left( \\sum_{i=1}^{n} (X_i - \\mu) \\right)^2\n$$\nThe squared sum can be expanded into a double summation, separating the terms where indices are equal from those where they are not:\n$$\n\\left( \\sum_{i=1}^{n} (X_i - \\mu) \\right)^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_i - \\mu)(X_j - \\mu) = \\sum_{i=1}^{n} (X_i - \\mu)^2 + \\sum_{i \\neq j} (X_i - \\mu)(X_j - \\mu)\n$$\nNow, we substitute this back into the expression for variance and apply the expectation operator:\n$$\n\\operatorname{Var}(\\bar{X}_n) = E\\left[ \\frac{1}{n^2} \\left( \\sum_{i=1}^{n} (X_i - \\mu)^2 + \\sum_{i \\neq j} (X_i - \\mu)(X_j - \\mu) \\right) \\right]\n$$\nUsing the linearity of expectation, we get:\n$$\n\\operatorname{Var}(\\bar{X}_n) = \\frac{1}{n^2} \\left( E\\left[\\sum_{i=1}^{n} (X_i - \\mu)^2\\right] + E\\left[\\sum_{i \\neq j} (X_i - \\mu)(X_j - \\mu)\\right] \\right)\n$$\n$$\n\\operatorname{Var}(\\bar{X}_n) = \\frac{1}{n^2} \\left( \\sum_{i=1}^{n} E[(X_i - \\mu)^2] + \\sum_{i \\neq j} E[(X_i - \\mu)(X_j - \\mu)] \\right)\n$$\nLet's evaluate the two expectation terms separately.\nFor the first term, by the definition of variance, $E[(X_i - \\mu)^2] = \\operatorname{Var}(X_i) = \\sigma^2$. Since this holds for all $i$:\n$$\n\\sum_{i=1}^{n} E[(X_i - \\mu)^2] = \\sum_{i=1}^{n} \\sigma^2 = n\\sigma^2\n$$\nFor the second term, which contains the cross-products where $i \\neq j$, we use the given property that the random variables $X_i$ and $X_j$ are independent. This implies that the random variables $(X_i - \\mu)$ and $(X_j - \\mu)$ are also independent. For independent random variables, the expectation of their product factorizes into the product of their individual expectations:\n$$\nE[(X_i - \\mu)(X_j - \\mu)] = E[X_i - \\mu] \\cdot E[X_j - \\mu] \\quad \\text{for } i \\neq j\n$$\nWe know that for any $k$, $E[X_k - \\mu] = E[X_k] - E[\\mu] = \\mu - \\mu = 0$.\nTherefore, for any pair with $i \\neq j$:\n$$\nE[(X_i - \\mu)(X_j - \\mu)] = 0 \\cdot 0 = 0\n$$\nSince every cross-product term in the sum $\\sum_{i \\neq j}$ has an expectation of zero, the entire sum is zero.\n\nSubstituting these results back into the equation for $\\operatorname{Var}(\\bar{X}_n)$:\n$$\n\\operatorname{Var}(\\bar{X}_n) = \\frac{1}{n^2} (n\\sigma^2 + 0) = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}\n$$\nThis completes the derivation of the variance of the sample mean from first principles.\n\nThe connection to the Central Limit Theorem (CLT) and the standard error is now direct. The CLT states that for a sufficiently large sample size $n$, the distribution of the standardized sample mean is approximately standard normal:\n$$\n\\frac{\\bar{X}_n - \\mu}{\\sqrt{\\operatorname{Var}(\\bar{X}_n)}} \\xrightarrow{d} N(0, 1)\n$$\nThe denominator, $\\sqrt{\\operatorname{Var}(\\bar{X}_n)}$, is the standard deviation of the sampling distribution of $\\bar{X}_n$, which is defined as the **standard error** of the mean, denoted $\\text{SE}(\\bar{X}_n)$. From our derivation, we have $\\operatorname{Var}(\\bar{X}_n) = \\frac{\\sigma^2}{n}$, so the standard error is:\n$$\n\\text{SE}(\\bar{X}_n) = \\sqrt{\\frac{\\sigma^2}{n}} = \\frac{\\sigma}{\\sqrt{n}}\n$$\nThis quantity is fundamental for constructing large-sample confidence intervals for $\\mu$. For example, a $95\\%$ confidence interval is approximately $\\bar{X}_n \\pm 1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}}$ (if $\\sigma$ is known) or $\\bar{X}_n \\pm 1.96 \\cdot \\frac{s}{\\sqrt{n}}$ (using the sample standard deviation $s$ as an estimate for $\\sigma$).\n\nThe derivation explains the scaling of the standard error with the sample size $n$. The variance $\\operatorname{Var}(\\bar{X}_n)$ is inversely proportional to $n$. This occurs because averaging over $n$ variables introduces a scaling factor of $\\frac{1}{n}$, which becomes $\\frac{1}{n^2}$ in the variance calculation. Meanwhile, the sum of the variances of the individual, independent measurements contributes a term proportional to $n$ (specifically, $n\\sigma^2$). The ratio of these effects, $\\frac{n\\sigma^2}{n^2}$, results in the $\\frac{1}{n}$ dependence. Consequently, the standard error, being the square root of the variance, scales as $\\frac{1}{\\sqrt{n}}$. This scaling reflects the fact that as more independent measurements are averaged, random errors tend to cancel out, leading to a more precise estimate of the true mean $\\mu$. To double the precision of the estimate (i.e., to halve the standard error), the sample size $n$ must be quadrupled, a critical consideration in designing clinical studies and other experiments.", "answer": "$$\\boxed{\\frac{\\sigma^2}{n}}$$", "id": "4986781"}, {"introduction": "Medical studies often involve paired data, such as measurements taken before and after an intervention. This practice demonstrates a powerful strategy: by analyzing the differences between pairs, we can apply the CLT to assess the intervention's effect. You will also rigorously justify \"studentization,\" the crucial step of substituting the unknown population variance with a sample estimate, a procedure that underpins the widely used paired t-test. [@problem_id:4986800]", "problem": "A longitudinal clinical study measures a continuous biomarker before and after an intervention in a cohort of $n$ patients. For patient $i$, denote the pre-intervention measurement by $X_{i}$ and the post-intervention measurement by $Y_{i}$. Assume the pairs $\\{(X_{i},Y_{i})\\}_{i=1}^{n}$ are independent and identically distributed (i.i.d.) across patients, with arbitrary within-pair dependence, and that there exists $\\delta0$ such that $\\mathbb{E}(|X_{1}|^{2+\\delta})\\infty$ and $\\mathbb{E}(|Y_{1}|^{2+\\delta})\\infty$. Define the paired difference $D_{i}=X_{i}-Y_{i}$ and let $\\bar{D}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}D_{i}$ denote the sample mean of differences. Let $\\mu_{D}=\\mathbb{E}(D_{1})$ and let $S_{D}^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}(D_{i}-\\bar{D}_{n})^{2}$ denote the sample variance of the differences.\n\nStarting from core definitions of independence, expectation, and variance, and using only well-tested probabilistic facts such as the Lindeberg–Feller Central Limit Theorem (CLT) and Slutsky’s theorem, do the following:\n\n1. Establish the asymptotic normality of $\\sqrt{n}(\\bar{D}_{n}-\\mu_{D})$ under the stated moment assumptions, making explicit the role of the moment condition in verifying the requisite CLT condition for $\\{D_{i}\\}$.\n2. Justify studentization by proving that $T_{n}=\\sqrt{n}(\\bar{D}_{n}-\\mu_{D})/S_{D}$ converges in distribution to a standard normal random variable under the same assumptions, and explain why the consistency of $S_{D}^{2}$ is sufficient for this conclusion.\n3. Suppose additionally that $\\operatorname{Var}(X_{1})=\\sigma_{X}^{2}$, $\\operatorname{Var}(Y_{1})=\\sigma_{Y}^{2}$, and $\\operatorname{Cov}(X_{1},Y_{1})=\\sigma_{XY}$. Derive the explicit expression, in closed form, for the asymptotic variance of $\\sqrt{n}(\\bar{D}_{n}-\\mu_{D})$ in terms of $\\sigma_{X}^{2}$, $\\sigma_{Y}^{2}$, and $\\sigma_{XY}$.\n\nProvide your final answer as the single closed-form analytic expression for the asymptotic variance requested in item 3. No numerical approximation is needed; do not round. No units are required.", "solution": "The problem addresses the asymptotic properties of the sample mean of paired differences, a cornerstone of statistical inference for two-sample paired data. We will address the three parts sequentially.\n\nFirst, we define properties of the random variables $D_{i}$. Since the pairs $(X_{i}, Y_{i})$ are i.i.d. for $i=1, \\dots, n$, the differences $D_{i}=X_{i}-Y_{i}$ also form an i.i.d. sequence of random variables.\n\nThe mean of $D_i$ is $\\mathbb{E}(D_{i}) = \\mathbb{E}(X_{i} - Y_{i})$. By linearity of expectation, $\\mathbb{E}(D_{i}) = \\mathbb{E}(X_{i}) - \\mathbb{E}(Y_{i})$. Since the pairs are identically distributed, this is constant for all $i$, and we denote it $\\mu_D = \\mathbb{E}(D_{1})$. The existence of moments of order $2+\\delta$ implies that all lower-order moments, including the first, are finite.\n\nThe variance of $D_i$ is $\\operatorname{Var}(D_i) = \\operatorname{Var}(X_i - Y_i)$. Let's denote $\\sigma_{D}^{2} = \\operatorname{Var}(D_1)$. We must verify that this variance is finite. The condition $\\mathbb{E}(|X_{1}|^{2+\\delta})  \\infty$ implies $\\mathbb{E}(X_{1}^{2})  \\infty$, and similarly $\\mathbb{E}(Y_{1}^{2})  \\infty$. Using the triangle inequality and the $c_r$-inequality ($|a+b|^p \\le 2^{p-1}(|a|^p+|b|^p)$), we have $\\mathbb{E}(D_1^2) = \\mathbb{E}((X_1-Y_1)^2) \\le \\mathbb{E}((|X_1|+|Y_1|)^2) = \\mathbb{E}(X_1^2 + Y_1^2 + 2|X_1 Y_1|)$. By the Cauchy-Schwarz inequality, $\\mathbb{E}(|X_1 Y_1|) \\le \\sqrt{\\mathbb{E}(X_1^2)\\mathbb{E}(Y_1^2)}$, which is finite. Therefore, $\\mathbb{E}(D_1^2)$ is finite, and so is the variance $\\sigma_{D}^{2} = \\mathbb{E}(D_1^2) - \\mu_D^2$.\n\n**1. Asymptotic Normality of $\\sqrt{n}(\\bar{D}_{n}-\\mu_{D})$**\n\nWe aim to apply the Central Limit Theorem to the i.i.d. sequence $\\{D_i\\}_{i=1}^n$. The classical (Lindeberg-Lévy) CLT requires the variables to be i.i.d. with finite mean and variance. We have already established these properties for $\\{D_i\\}$. Thus, the CLT directly applies.\nThe theorem states that $\\frac{\\sum_{i=1}^{n}(D_i - \\mu_D)}{\\sqrt{n\\sigma_D^2}} \\xrightarrow{d} N(0,1)$, where $\\xrightarrow{d}$ denotes convergence in distribution.\nRearranging the term, we get $\\frac{\\sqrt{n}(\\frac{1}{n}\\sum_{i=1}^{n}D_i - \\mu_D)}{\\sigma_D} = \\frac{\\sqrt{n}(\\bar{D}_n - \\mu_D)}{\\sigma_D} \\xrightarrow{d} N(0,1)$.\nThis is equivalent to stating that $\\sqrt{n}(\\bar{D}_n - \\mu_D) \\xrightarrow{d} N(0, \\sigma_D^2)$. Thus, the sequence $\\sqrt{n}(\\bar{D}_n - \\mu_D)$ is asymptotically normal with mean $0$ and asymptotic variance $\\sigma_D^2$.\n\nThe problem specifically requests an explanation using the Lindeberg-Feller CLT and the role of the moment condition $\\mathbb{E}(|D_1|^{2+\\delta})  \\infty$. The Lindeberg condition for a sequence of independent random variables $\\{Z_i\\}$ with $\\mathbb{E}(Z_i)=0$ and finite variances $\\sigma_i^2$ is:\nFor any $\\epsilon  0$, $\\lim_{n \\to \\infty} \\frac{1}{s_n^2} \\sum_{i=1}^n \\mathbb{E} \\left[ Z_i^2 \\cdot \\mathbb{I}(|Z_i|  \\epsilon s_n) \\right] = 0$, where $s_n^2 = \\sum_{i=1}^n \\sigma_i^2$.\n\nIn our case, let $Z_i = D_i - \\mu_D$. These are i.i.d. with $\\mathbb{E}(Z_i)=0$ and $\\operatorname{Var}(Z_i) = \\sigma_D^2$. So, $s_n^2 = n\\sigma_D^2$. The Lindeberg condition becomes:\n$$ \\lim_{n \\to \\infty} \\frac{1}{n\\sigma_D^2} \\sum_{i=1}^n \\mathbb{E} \\left[ (D_i - \\mu_D)^2 \\cdot \\mathbb{I}(|D_i - \\mu_D|  \\epsilon \\sqrt{n}\\sigma_D) \\right] = 0 $$\nSince the terms are identically distributed, this simplifies to:\n$$ \\lim_{n \\to \\infty} \\frac{1}{\\sigma_D^2} \\mathbb{E} \\left[ (D_1 - \\mu_D)^2 \\cdot \\mathbb{I}(|D_1 - \\mu_D|  \\epsilon \\sqrt{n}\\sigma_D) \\right] = 0 $$\nThe stronger moment condition, $\\mathbb{E}(|X_{1}|^{2+\\delta})  \\infty$ and $\\mathbb{E}(|Y_{1}|^{2+\\delta})  \\infty$, implies $\\mathbb{E}(|D_1|^{2+\\delta})  \\infty$ by the $c_r$-inequality. Let $W = D_1 - \\mu_D$. Then $\\mathbb{E}(|W|^{2+\\delta})$ is also finite. We can use this to bound the expectation:\n$$ \\mathbb{E} \\left[ W^2 \\cdot \\mathbb{I}(|W|  C_n) \\right] \\text{ where } C_n = \\epsilon \\sqrt{n}\\sigma_D \\to \\infty $$\n$$ \\mathbb{E} \\left[ W^2 \\cdot \\frac{|W|^\\delta}{|W|^\\delta} \\cdot \\mathbb{I}(|W|  C_n) \\right] \\le \\mathbb{E} \\left[ W^2 \\cdot \\frac{|W|^\\delta}{C_n^\\delta} \\cdot \\mathbb{I}(|W|  C_n) \\right] = \\frac{1}{C_n^\\delta} \\mathbb{E} \\left[ |W|^{2+\\delta} \\cdot \\mathbb{I}(|W|  C_n) \\right] $$\nThis can be further bounded by $\\frac{1}{C_n^\\delta} \\mathbb{E} \\left[ |W|^{2+\\delta} \\right]$.\nSince $\\mathbb{E} \\left[ |W|^{2+\\delta} \\right]  \\infty$ and $C_n^\\delta = (\\epsilon\\sigma_D)^\\delta n^{\\delta/2} \\to \\infty$ as $n \\to \\infty$, the entire expression converges to $0$. This verifies the Lindeberg condition. The existence of a moment of order $2+\\delta$ (for $\\delta0$) provides a mechanism to prove that the tail integral vanishes, satisfying the CLT's requirements.\n\n**2. Justification of Studentization**\n\nWe need to show that $T_n = \\frac{\\sqrt{n}(\\bar{D}_{n}-\\mu_{D})}{S_{D}}$ converges in distribution to a standard normal variable, $N(0,1)$. This process, called studentization, involves replacing the unknown population standard deviation $\\sigma_D$ in the normalizing factor with its sample estimate $S_D$. The validity of this replacement hinges on Slutsky's theorem.\n\nSlutsky's theorem states that if $A_n \\xrightarrow{d} A$ and $B_n \\xrightarrow{p} c$ (where $c$ is a constant and $\\xrightarrow{p}$ denotes convergence in probability), then $A_n / B_n \\xrightarrow{d} A / c$.\n\nFrom part 1, we identify $A_n = \\sqrt{n}(\\bar{D}_n - \\mu_D)$, and we know that $A_n \\xrightarrow{d} A \\sim N(0, \\sigma_D^2)$.\nWe identify $B_n = S_D$. To apply Slutsky's theorem, we must show that $S_D$ converges in probability to the constant $\\sigma_D$. This is equivalent to showing that its square, $S_D^2$, converges in probability to $\\sigma_D^2$. That is, we must prove that $S_D^2$ is a consistent estimator of $\\sigma_D^2$.\n\nThe sample variance is $S_D^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(D_{i}-\\bar{D}_{n})^{2}$. This can be rewritten as:\n$$ S_D^2 = \\frac{n}{n-1} \\left( \\frac{1}{n}\\sum_{i=1}^n D_i^2 - \\bar{D}_n^2 \\right) $$\nAs $n \\to \\infty$, the factor $\\frac{n}{n-1} \\to 1$.\nThe variables $D_i$ are i.i.d. with finite mean $\\mu_D$. Thus, by the Weak Law of Large Numbers (WLLN), the sample mean converges in probability to the population mean: $\\bar{D}_n \\xrightarrow{p} \\mu_D$.\nThe variables $D_i^2$ are also i.i.d. and we have shown that $\\mathbb{E}(D_1^2)  \\infty$. Therefore, by the WLLN, their sample mean also converges in probability to the population mean: $\\frac{1}{n}\\sum_{i=1}^n D_i^2 \\xrightarrow{p} \\mathbb{E}(D_1^2)$.\n\nBy the continuous mapping theorem, if $g$ is a continuous function, $Z_n \\xrightarrow{p} c \\implies g(Z_n) \\xrightarrow{p} g(c)$. Applying this, $\\bar{D}_n^2 \\xrightarrow{p} \\mu_D^2$.\nCombining these results using properties of convergence in probability:\n$$ S_D^2 = \\left(\\frac{n}{n-1}\\right) \\left( \\frac{1}{n}\\sum_{i=1}^n D_i^2 - \\bar{D}_n^2 \\right) \\xrightarrow{p} (1) \\cdot (\\mathbb{E}(D_1^2) - \\mu_D^2) = \\sigma_D^2 $$\nThus, $S_D^2$ is a consistent estimator of $\\sigma_D^2$. This is precisely what was needed. The consistency of $S_D^2$ is sufficient.\nAgain, by the continuous mapping theorem (with the square root function, which is continuous for non-negative values), $S_D = \\sqrt{S_D^2} \\xrightarrow{p} \\sqrt{\\sigma_D^2} = \\sigma_D$ (assuming $\\sigma_D^2  0$).\n\nNow we apply Slutsky's theorem:\n$$ T_n = \\frac{A_n}{B_n} = \\frac{\\sqrt{n}(\\bar{D}_{n}-\\mu_{D})}{S_{D}} \\xrightarrow{d} \\frac{N(0, \\sigma_D^2)}{\\sigma_D} \\sim N\\left(\\frac{0}{\\sigma_D}, \\frac{\\sigma_D^2}{\\sigma_D^2}\\right) \\sim N(0,1) $$\nThis completes the justification. The consistency of $S_D^2$ (and thus $S_D$) allows it to be treated as a constant in the limit, ensuring the resulting statistic has a parameter-free standard normal distribution.\n\n**3. Explicit Expression for Asymptotic Variance**\n\nThe asymptotic variance of $\\sqrt{n}(\\bar{D}_n - \\mu_D)$ is, by definition, the variance of the limiting normal distribution established in part 1. This variance is $\\sigma_D^2 = \\operatorname{Var}(D_1)$. We need to express this in terms of $\\sigma_{X}^{2} = \\operatorname{Var}(X_{1})$, $\\sigma_{Y}^{2} = \\operatorname{Var}(Y_{1})$, and $\\sigma_{XY} = \\operatorname{Cov}(X_{1},Y_{1})$.\n\nUsing the properties of variance, we have:\n$$ \\sigma_D^2 = \\operatorname{Var}(D_1) = \\operatorname{Var}(X_1 - Y_1) $$\nThe formula for the variance of a difference of two random variables is $\\operatorname{Var}(A-B) = \\operatorname{Var}(A) + \\operatorname{Var}(B) - 2\\operatorname{Cov}(A,B)$. Applying this formula with $A=X_1$ and $B=Y_1$:\n$$ \\operatorname{Var}(X_1 - Y_1) = \\operatorname{Var}(X_1) + \\operatorname{Var}(Y_1) - 2\\operatorname{Cov}(X_1, Y_1) $$\nSubstituting the given notation:\n$$ \\sigma_D^2 = \\sigma_{X}^{2} + \\sigma_{Y}^{2} - 2\\sigma_{XY} $$\nThis expression represents the explicit closed form for the asymptotic variance of $\\sqrt{n}(\\bar{D}_n - \\mu_D)$.", "answer": "$$\\boxed{\\sigma_{X}^{2} + \\sigma_{Y}^{2} - 2\\sigma_{XY}}$$", "id": "4986800"}, {"introduction": "Real-world data analysis often moves beyond simple averages to weighted summaries, for instance, in meta-analyses or surveys with complex sampling designs. This advanced exercise challenges you to derive a Central Limit Theorem for a weighted average, extending its application beyond the standard i.i.d. case. By working through this problem, you will engage with the more general conditions under which the normal approximation holds, a key insight for sophisticated statistical modeling. [@problem_id:4986792]", "problem": "Consider a multi-center clinical registry where a biomarker measurement is recorded from $n$ distinct patients across different clinics. Let $\\{X_{i}\\}_{i=1}^{n}$ denote independent and identically distributed measurements with $\\mathbb{E}[X_{i}]=\\mu$ and $\\operatorname{Var}(X_{i})=\\sigma^{2}$, and assume $\\mathbb{E}\\big[(X_{i}-\\mu)^{2}\\big]\\infty$. To build a population-level summary that accounts for clinic-level audit weights, consider the weighted average\n$$\n\\widehat{\\mu}_{w,n} \\;=\\; \\frac{\\sum_{i=1}^{n} w_{i} X_{i}}{\\sum_{i=1}^{n} w_{i}},\n$$\nwhere the weights $\\{w_{i}\\}_{i=1}^{n}$ are nonrandom, nonnegative, and uniformly bounded, i.e., there exists a constant $M\\infty$ such that $\\sup_{i\\geq 1} w_{i} \\leq M$. Suppose additionally that the Cesàro limits of the first and second empirical moments of the weights exist and are strictly positive and finite:\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} w_{i} \\;\\longrightarrow\\; m_{1} \\in (0,\\infty)\n\\quad\\text{and}\\quad\n\\frac{1}{n}\\sum_{i=1}^{n} w_{i}^{2} \\;\\longrightarrow\\; m_{2} \\in (0,\\infty).\n$$\nStarting only from the independence structure, finite second moments, and the stated boundedness and moment assumptions on $\\{w_{i}\\}$, derive an appropriate Central Limit Theorem (CLT) for the weighted average that characterizes the large-sample distribution of $\\sqrt{n}\\big(\\widehat{\\mu}_{w,n}-\\mu\\big)$ under these conditions. As part of your derivation, justify any normalization via a triangular-array argument grounded in verifiable moment and boundedness conditions, and identify the asymptotic variance constant explicitly in terms of $\\sigma^{2}$, $m_{1}$, and $m_{2}$. Provide your final answer as a single closed-form analytic expression for the asymptotic variance of $\\sqrt{n}\\big(\\widehat{\\mu}_{w,n}-\\mu\\big)$. No numerical rounding is required.", "solution": "Let the quantity of interest be $T_n = \\sqrt{n}\\big(\\widehat{\\mu}_{w,n}-\\mu\\big)$. We begin by rewriting this expression. The weighted average is given by $\\widehat{\\mu}_{w,n} = \\frac{\\sum_{i=1}^{n} w_{i} X_{i}}{\\sum_{i=1}^{n} w_{i}}$.\nSubstituting this into the expression for $T_n$ yields:\n$$\nT_n = \\sqrt{n}\\left( \\frac{\\sum_{i=1}^{n} w_{i} X_{i}}{\\sum_{i=1}^{n} w_{i}} - \\mu \\right) = \\sqrt{n}\\left( \\frac{\\sum_{i=1}^{n} w_{i} X_{i} - \\mu \\sum_{i=1}^{n} w_{i}}{\\sum_{i=1}^{n} w_{i}} \\right) = \\frac{\\sqrt{n} \\sum_{i=1}^{n} w_{i} (X_{i} - \\mu)}{\\sum_{i=1}^{n} w_{i}}.\n$$\nLet $Y_{i} = X_{i} - \\mu$. The variables $\\{Y_{i}\\}_{i=1}^{n}$ are independent and identically distributed with $\\mathbb{E}[Y_{i}] = 0$ and $\\operatorname{Var}(Y_{i}) = \\operatorname{Var}(X_{i}) = \\sigma^{2}$, where $\\sigma^2  \\infty$. The expression for $T_n$ becomes:\n$$\nT_n = \\frac{\\sqrt{n}}{\\sum_{i=1}^{n} w_{i}} \\sum_{i=1}^{n} w_{i} Y_{i}.\n$$\nTo determine the asymptotic distribution of $T_n$, we can use Slutsky's theorem. Let us rewrite $T_n$ as a product of two terms:\n$$\nT_n = \\left( \\frac{n}{\\sum_{i=1}^{n} w_{i}} \\right) \\left( \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} w_{i} Y_{i} \\right).\n$$\nLet's analyze the first term. The problem states that the weights $\\{w_i\\}$ are nonrandom and that $\\frac{1}{n}\\sum_{i=1}^{n} w_{i} \\to m_{1}$ as $n \\to \\infty$, where $m_1 \\in (0, \\infty)$. Therefore, the first term is a deterministic sequence that converges to a constant:\n$$\n\\lim_{n\\to\\infty} \\left( \\frac{n}{\\sum_{i=1}^{n} w_{i}} \\right) = \\lim_{n\\to\\infty} \\left( \\frac{1}{\\frac{1}{n}\\sum_{i=1}^{n} w_{i}} \\right) = \\frac{1}{m_{1}}.\n$$\nNow, we must find the limiting distribution of the second term, $S_n = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} w_{i} Y_{i}$. This is a scaled sum of independent but not identically distributed random variables, $w_i Y_i$. We can establish its asymptotic normality by applying the Lindeberg-Feller Central Limit Theorem.\n\nConsider the triangular array of random variables $V_{n,i} = w_{i}Y_{i}$ for $i=1, \\dots, n$. These variables are independent for each $n$, with $\\mathbb{E}[V_{n,i}] = w_i \\mathbb{E}[Y_i] = 0$. The variance of their sum is given by\n$$\ns_{n}^{2} = \\operatorname{Var}\\left(\\sum_{i=1}^{n} V_{n,i}\\right) = \\sum_{i=1}^{n} \\operatorname{Var}(V_{n,i}) = \\sum_{i=1}^{n} w_{i}^{2} \\operatorname{Var}(Y_{i}) = \\sigma^{2} \\sum_{i=1}^{n} w_{i}^{2}.\n$$\nThe Lindeberg-Feller CLT states that if the Lindeberg condition holds, then $\\frac{1}{s_n}\\sum_{i=1}^{n} V_{n,i} \\xrightarrow{d} N(0,1)$. The Lindeberg condition requires that for any $\\epsilon  0$:\n$$\n\\lim_{n\\to\\infty} \\frac{1}{s_{n}^{2}} \\sum_{i=1}^{n} \\mathbb{E}\\left[V_{n,i}^{2} \\cdot \\mathbb{I}\\left(|V_{n,i}|  \\epsilon s_{n}\\right)\\right] = 0.\n$$\nSubstituting $V_{n,i} = w_{i}Y_{i}$ and $s_n^2 = \\sigma^2 \\sum_{j=1}^{n} w_{j}^{2}$, the condition is:\n$$\n\\lim_{n\\to\\infty} \\frac{1}{\\sigma^{2} \\sum_{j=1}^{n} w_{j}^{2}} \\sum_{i=1}^{n} \\mathbb{E}\\left[w_{i}^{2}Y_{i}^{2} \\cdot \\mathbb{I}\\left(|w_{i}Y_{i}|  \\epsilon \\sigma \\sqrt{\\sum_{j=1}^{n} w_{j}^{2}}\\right)\\right] = 0.\n$$\nWe are given that the weights are uniformly bounded, i.e., there exists $M  \\infty$ such that $w_{i} \\leq M$ for all $i$. Thus, $|w_{i}Y_{i}| \\leq M|Y_{i}|$. We can use this to bound the indicator function: $\\mathbb{I}(|w_i Y_i|  \\epsilon s_n) \\leq \\mathbb{I}(M|Y_i| \\epsilon s_n)$.\nThe expression to be evaluated, let's call it $L_n$, can be bounded as follows:\n$$\nL_n = \\frac{1}{\\sigma^{2} \\sum_{j=1}^{n} w_{j}^{2}} \\sum_{i=1}^{n} w_{i}^{2} \\mathbb{E}\\left[Y_{i}^{2} \\cdot \\mathbb{I}\\left(|w_{i}Y_{i}|  \\epsilon s_{n}\\right)\\right] \\leq \\frac{1}{\\sigma^{2} \\sum_{j=1}^{n} w_{j}^{2}} \\sum_{i=1}^{n} w_{i}^{2} \\mathbb{E}\\left[Y_{i}^{2} \\cdot \\mathbb{I}\\left(M|Y_{i}|  \\epsilon s_{n}\\right)\\right].\n$$\nSince the $Y_i$ are identically distributed, let $Y$ be a random variable with this common distribution. The expectation term is the same for all $i$.\n$$\nL_n \\leq \\frac{\\sum_{i=1}^{n} w_{i}^{2}}{\\sigma^{2} \\sum_{j=1}^{n} w_{j}^{2}} \\mathbb{E}\\left[Y^{2} \\cdot \\mathbb{I}\\left(|Y|  \\frac{\\epsilon s_{n}}{M}\\right) \\right] = \\frac{1}{\\sigma^{2}} \\mathbb{E}\\left[Y^{2} \\cdot \\mathbb{I}\\left(|Y|  \\frac{\\epsilon s_{n}}{M}\\right) \\right].\n$$\nAs $n \\to \\infty$, we have $\\frac{1}{n} \\sum_{i=1}^n w_i^2 \\to m_2 \\in (0, \\infty)$, which implies $\\sum_{i=1}^n w_i^2 \\sim n m_2$. Thus, $s_n^2 = \\sigma^2 \\sum_{i=1}^n w_i^2 \\to \\infty$, and so $s_n \\to \\infty$. Consequently, the threshold $\\frac{\\epsilon s_n}{M} \\to \\infty$.\nWe are given that $\\mathbb{E}[X_i^2]\\infty$, which implies $\\mathbb{E}[Y^2] = \\sigma^2  \\infty$. A fundamental property of integrable random variables is that the contribution to the expectation from the tail of the distribution vanishes. Formally, since $\\mathbb{E}[Y^2]  \\infty$, it follows from the Dominated Convergence Theorem that $\\lim_{K \\to \\infty} \\mathbb{E}[Y^2 \\cdot \\mathbb{I}(|Y|  K)] = 0$.\nSince $\\frac{\\epsilon s_n}{M} \\to \\infty$ as $n \\to \\infty$, we must have $\\lim_{n \\to \\infty} \\mathbb{E}\\left[Y^{2} \\cdot \\mathbb{I}\\left(|Y|  \\frac{\\epsilon s_{n}}{M}\\right) \\right] = 0$.\nThis implies $\\lim_{n \\to \\infty} L_n = 0$, satisfying the Lindeberg condition.\n\nTherefore, by the Lindeberg-Feller CLT, we have the convergence in distribution:\n$$\n\\frac{\\sum_{i=1}^{n} w_{i} Y_{i}}{s_{n}} = \\frac{\\sum_{i=1}^{n} w_{i} Y_{i}}{\\sigma\\sqrt{\\sum_{i=1}^{n} w_{i}^{2}}} \\xrightarrow{d} N(0,1).\n$$\nNow we return to the second term of our original product, $S_n = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} w_{i} Y_{i}$. We can rewrite it as:\n$$\nS_n = \\left(\\frac{\\sigma\\sqrt{\\sum_{i=1}^{n} w_{i}^{2}}}{\\sqrt{n}}\\right) \\left(\\frac{\\sum_{i=1}^{n} w_{i} Y_{i}}{\\sigma\\sqrt{\\sum_{i=1}^{n} w_{i}^{2}}}\\right).\n$$\nThe first part is a deterministic sequence. Let's find its limit:\n$$\n\\lim_{n\\to\\infty} \\frac{\\sigma^{2}\\left(\\sum_{i=1}^{n} w_{i}^{2}\\right)}{n} = \\lim_{n\\to\\infty} \\sigma^{2} \\left(\\frac{1}{n}\\sum_{i=1}^{n} w_{i}^{2}\\right) = \\sigma^{2} m_{2}.\n$$\nThus, $\\lim_{n\\to\\infty} \\frac{\\sigma\\sqrt{\\sum_{i=1}^{n} w_{i}^{2}}}{\\sqrt{n}} = \\sqrt{\\sigma^{2} m_{2}} = \\sigma \\sqrt{m_{2}}$.\nBy Slutsky's theorem, since the first part converges to a constant $\\sigma\\sqrt{m_2}$ and the second part converges in distribution to $N(0,1)$, their product converges in distribution:\n$$\nS_n = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} w_{i} Y_{i} \\xrightarrow{d} (\\sigma \\sqrt{m_{2}}) \\cdot N(0,1) \\sim N(0, \\sigma^{2}m_{2}).\n$$\nFinally, we combine the results for the two terms of $T_n$:\n$$\nT_n = \\underbrace{\\left( \\frac{n}{\\sum_{i=1}^{n} w_{i}} \\right)}_{\\to \\frac{1}{m_{1}}} \\underbrace{\\left( \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} w_{i} Y_{i} \\right)}_{\\xrightarrow{d} N(0, \\sigma^{2}m_{2})}.\n$$\nApplying Slutsky's theorem again, the product converges in distribution to the product of the limits:\n$$\n\\sqrt{n}\\big(\\widehat{\\mu}_{w,n}-\\mu\\big) \\xrightarrow{d} \\frac{1}{m_{1}} \\cdot N(0, \\sigma^{2}m_{2}) \\sim N\\left(0, \\left(\\frac{1}{m_1}\\right)^2 \\sigma^2 m_2\\right).\n$$\nThe limiting distribution is a normal distribution with mean $0$ and variance $\\frac{\\sigma^{2}m_{2}}{m_{1}^{2}}$.\nThe asymptotic variance of $\\sqrt{n}\\big(\\widehat{\\mu}_{w,n}-\\mu\\big)$ is the variance of this limiting distribution.\n\nThe asymptotic variance is $\\frac{\\sigma^{2}m_{2}}{m_{1}^{2}}$.", "answer": "$$\\boxed{\\frac{\\sigma^{2}m_{2}}{m_{1}^{2}}}$$", "id": "4986792"}]}