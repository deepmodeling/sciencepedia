{"hands_on_practices": [{"introduction": "Beyond simple formulas, practical sample size determination for studies with unknown variance requires a computational approach. This exercise guides you through the foundational process of calculating power using the exact noncentral $t$-distribution, a cornerstone for analyses involving normally distributed data where the variance is estimated. By implementing an algorithm to numerically invert the power function, you will develop a core, reusable skill for accurately sizing studies when relying on small-sample theory [@problem_id:4979726].", "problem": "A one-sample mean comparison in a clinical pharmacology study is to be analyzed under the Gaussian sampling model with unknown variance. Let $\\{X_i\\}_{i=1}^n$ be independent and identically distributed with $X_i \\sim \\mathcal{N}(\\mu,\\sigma^2)$, where $\\sigma^2$ is unknown. Consider testing the null hypothesis $H_0:\\mu=\\mu_0$ versus the two-sided alternative $H_1:\\mu\\neq\\mu_0$ using the usual Student $t$ statistic constructed from the sample mean and the unbiased sample variance. Define the standardized effect size $d=(\\mu_1-\\mu_0)/\\sigma$, where $\\mu_1$ is the true mean under the alternative, and the target power $\\pi \\in (0,1)$ at type I error level $\\alpha \\in (0,1)$.\n\nStarting only from the following fundamental facts: (i) if $Z\\sim\\mathcal{N}(0,1)$ and $U\\sim\\chi^2_\\nu$ are independent, then the central Student $t$ variable is $T=Z/\\sqrt{U/\\nu}$; (ii) under a nonzero mean shift, the ratio $T_\\text{alt}=(Z+\\delta)/\\sqrt{U/\\nu}$ has a noncentral Student $t$ distribution with degrees of freedom $\\nu$ and noncentrality parameter $\\delta$; (iii) for the one-sample $t$ statistic with $n$ observations from $\\mathcal{N}(\\mu,\\sigma^2)$, the degrees of freedom are $\\nu=n-1$ and, under $H_1:\\mu=\\mu_1$, the noncentrality parameter equals $\\delta(n)=\\sqrt{n}\\,d$, derive an expression for the power `Power(n)` of the two-sided $t$-test as a function of $n$, $\\alpha$, and $d$ in terms of the cumulative distribution function of the noncentral Student $t$ distribution. Then, design and implement a numerical algorithm that finds the smallest integer $n\\geq 2$ such that $\\mathsf{Power}(n)\\geq \\pi$.\n\nYour implementation must adhere to the following computational and output requirements:\n- You must use the exact noncentral Student $t$ distribution for finite $n$; no asymptotic or normal approximations are permitted.\n- All angles are irrelevant. No physical units are involved. All probabilities must be specified and interpreted as decimals in $(0,1)$.\n- If $d=0$, conclude what happens to `Power(n)` for any finite $n$ and handle this edge case consistently in your algorithm: if $\\pi>\\alpha$, declare that no finite $n$ can achieve the target power; if $\\pi\\leq \\alpha$, return the smallest valid $n$.\n- Your algorithm must be monotonicity-aware and must return the minimal $n$ that achieves the target power. It should bracket and then invert `Power(n)` over the integers by a provably terminating search procedure.\n\nTest suite. Your program must compute the minimal $n$ for each of the following parameter sets $(d,\\alpha,\\pi)$ for a two-sided test:\n- Case $1$: $(d,\\alpha,\\pi)=(0.5,0.05,0.8)$.\n- Case $2$: $(d,\\alpha,\\pi)=(0.2,0.05,0.8)$.\n- Case $3$: $(d,\\alpha,\\pi)=(1.0,0.05,0.9)$.\n- Case $4$: $(d,\\alpha,\\pi)=(0.0,0.05,0.8)$.\n- Case $5$: $(d,\\alpha,\\pi)=(0.5,0.10,0.5)$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For the five cases above, output $[n_1,n_2,n_3,n_4,n_5]$, where $n_j$ is the minimal integer sample size for case $j$, and where $n_4$ must be a floating-point infinity if no finite sample size can achieve the requested power, written as a floating-point value so that it appears as $inf$ when printed by the programming language.", "solution": "The problem is valid as it is scientifically grounded in established statistical theory, well-posed, and objective. It provides a complete and consistent set of definitions and constraints necessary for deriving a unique and meaningful solution. The task is a standard problem in statistical power analysis that requires a rigorous application of statistical distribution theory and numerical methods.\n\nWe begin by deriving the power function for the specified one-sample, two-sided Student's $t$-test.\n\nThe test statistic is given by $T = \\frac{\\bar{X} - \\mu_0}{S/\\sqrt{n}}$, where $\\bar{X}$ is the sample mean and $S^2$ is the unbiased sample variance from a sample of size $n$.\n\nUnder the null hypothesis, $H_0: \\mu = \\mu_0$, the sample is drawn from $\\mathcal{N}(\\mu_0, \\sigma^2)$. The test statistic $T$ follows a central Student's $t$-distribution with $\\nu = n-1$ degrees of freedom, denoted $T \\sim t_{n-1}$.\n\nThe test is two-sided with a type I error rate of $\\alpha$. We reject $H_0$ if $|T| > c$, where $c$ is the critical value. This critical value is the upper $\\alpha/2$ quantile of the central $t$-distribution with $n-1$ degrees of freedom. We define $c(n) \\equiv t_{1-\\alpha/2, n-1}$, where $t_{q, \\nu}$ is the $q$-th quantile of the $t_\\nu$ distribution. The rejection region is thus $T  -c(n)$ or $T > c(n)$.\n\nThe power of the test, $\\mathsf{Power}(n)$, is the probability of rejecting $H_0$ when the alternative hypothesis, $H_1: \\mu = \\mu_1$, is true.\n$$\n\\mathsf{Power}(n) = P\\left( |T| > c(n) \\mid H_1 \\right) = P\\left( T > c(n) \\mid H_1 \\right) + P\\left( T  -c(n) \\mid H_1 \\right)\n$$\nTo evaluate this probability, we must determine the distribution of $T$ under $H_1$. We can rewrite the statistic as:\n$$\nT = \\frac{\\bar{X} - \\mu_0}{S/\\sqrt{n}} = \\frac{(\\bar{X} - \\mu_1) + (\\mu_1 - \\mu_0)}{S/\\sqrt{n}}\n$$\nDividing the numerator and denominator by $\\sigma/\\sqrt{n}$ gives:\n$$\nT = \\frac{\\frac{\\bar{X} - \\mu_1}{\\sigma/\\sqrt{n}} + \\frac{\\mu_1 - \\mu_0}{\\sigma/\\sqrt{n}}}{S/\\sigma} = \\frac{\\frac{\\bar{X} - \\mu_1}{\\sigma/\\sqrt{n}} + \\sqrt{n} \\frac{\\mu_1 - \\mu_0}{\\sigma}}{S/\\sigma}\n$$\nFollowing the provided facts:\n(i) $Z = \\frac{\\bar{X} - \\mu_1}{\\sigma/\\sqrt{n}} \\sim \\mathcal{N}(0,1)$ under $H_1$.\n(ii) By Cochran's theorem, $U = \\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1}$ and is independent of $Z$. Thus, $S/\\sigma = \\sqrt{U/(n-1)}$.\n(iii) The standardized effect size is $d = (\\mu_1 - \\mu_0)/\\sigma$. The noncentrality parameter is $\\delta(n) = \\sqrt{n} d$.\n\nSubstituting these into the expression for $T$, we have:\n$$\nT = \\frac{Z + \\delta(n)}{\\sqrt{U/\\nu}} \\quad \\text{where } \\nu = n-1\n$$\nThis is the canonical form of a noncentral Student's $t$-distributed random variable. Thus, under $H_1$, $T$ follows a noncentral Student's $t$-distribution with $\\nu = n-1$ degrees of freedom and noncentrality parameter $\\delta(n) = \\sqrt{n} d$. We denote this distribution $t_{n-1, \\delta(n)}$.\n\nLet $F_{t, \\nu, \\delta}(\\cdot)$ be the cumulative distribution function (CDF) of the noncentral $t$-distribution with $\\nu$ degrees of freedom and noncentrality parameter $\\delta$. The power is then:\n$$\n\\mathsf{Power}(n) = P(T_{alt} > c(n)) + P(T_{alt}  -c(n))\n$$\nwhere $T_{alt} \\sim t_{n-1, \\sqrt{n}d}$. This can be expressed using the CDF as:\n$$\n\\mathsf{Power}(n) = (1 - F_{t, n-1, \\sqrt{n}d}(c(n))) + F_{t, n-1, \\sqrt{n}d}(-c(n))\n$$\nwhere $c(n) = t_{1-\\alpha/2, n-1}$. This is the desired expression for power as a function of $n$, $\\alpha$, and $d$.\n\nFor the edge case where the effect size $d=0$, the noncentrality parameter $\\delta(n) = \\sqrt{n} \\cdot 0 = 0$. A noncentral $t$-distribution with a noncentrality parameter of zero is identical to the central $t$-distribution. Therefore, under $H_1$ with $d=0$ (which is equivalent to $H_0$), the statistic $T \\sim t_{n-1}$. The power calculation becomes:\n$$\n\\mathsf{Power}(n) = P(|T| > t_{1-\\alpha/2, n-1} \\mid T \\sim t_{n-1})\n$$\nBy definition of the critical value, this probability is exactly $\\alpha$. So, for $d=0$, $\\mathsf{Power}(n) = \\alpha$ for any $n \\ge 2$. To achieve a target power $\\pi$, we require $\\alpha \\ge \\pi$.\n- If $\\pi > \\alpha$, no finite sample size $n$ can satisfy the condition. The required sample size is infinite.\n- If $\\pi \\le \\alpha$, the condition is met for all valid sample sizes. The problem asks for the smallest integer $n \\ge 2$, so the answer is $n=2$.\n\nTo find the smallest integer $n \\ge 2$ such that $\\mathsf{Power}(n) \\ge \\pi$, we design a numerical algorithm. For any fixed $d \\neq 0$ and $\\alpha$, $\\mathsf{Power}(n)$ is a monotonically increasing function of $n$. This monotonicity allows for an efficient search. Our algorithm consists of two phases:\n\n1.  **Bracketing Phase**: We must find an interval $[n_{low}, n_{high}]$ that is guaranteed to contain the solution. We start with $n_{low} = 2$. If $\\mathsf{Power}(2) \\ge \\pi$, then $n=2$ is the solution. Otherwise, we establish an upper bound by starting with $n_{high} = 4$ and repeatedly doubling it ($n_{high} \\leftarrow 2 \\cdot n_{high}$) until we find an $n_{high}$ such that $\\mathsf{Power}(n_{high}) \\ge \\pi$. At each step, we update $n_{low}$ with the previous value of $n_{high}$. This process efficiently finds a bracket $(n_{low}, n_{high}]$ containing the smallest solution.\n\n2.  **Search Phase**: Once the bracket is established, we use a binary search over the integers in the range $[n_{low}, n_{high}]$ to find the minimum $n$ satisfying the power requirement. Let the search range be `[low, high]`. In each step, we compute the power at the midpoint, `mid = (low + high) // 2`.\n    - If $\\mathsf{Power}(mid) \\ge \\pi$, then `mid` is a potential solution, and we try to find a smaller one by setting `high = mid`.\n    - If $\\mathsf{Power}(mid)  \\pi$, then `mid` is too small, and the solution must be larger. We set `low = mid + 1`.\n    The search terminates when `low` equals `high`, which will be the smallest integer $n$ in the bracket that achieves the target power $\\pi$. Since the bracket was constructed to bound the true minimum $n$, this value is the overall smallest required sample size. This two-phase search is provably terminating and correct due to the monotonicity of the power function.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t, nct\n\ndef calculate_power(n, d, alpha):\n    \"\"\"\n    Calculates the exact power of a two-sided one-sample t-test.\n\n    Args:\n        n (int): The sample size.\n        d (float): The standardized effect size, (mu1 - mu0) / sigma.\n        alpha (float): The significance level (type I error rate).\n\n    Returns:\n        float: The power of the test.\n    \"\"\"\n    if n  2:\n        return 0.0\n\n    # Degrees of freedom for the t-test\n    nu = n - 1\n\n    # Critical value from the central t-distribution for a two-sided test.\n    # ppf is the percent point function (inverse of CDF, or quantile function).\n    critical_value = t.ppf(1 - alpha / 2.0, df=nu)\n\n    # Noncentrality parameter under the alternative hypothesis\n    delta = d * np.sqrt(n)\n\n    # Power is the probability of rejecting H0 when H1 is true.\n    # Under H1, the test statistic follows a noncentral t-distribution.\n    # Power = P(T_alt > c) + P(T_alt  -c)\n    # where c is the critical_value and T_alt has a noncentral t-distribution.\n    # nct.cdf(x, df, nc) is the CDF of the noncentral t-distribution.\n    power = 1.0 - nct.cdf(critical_value, df=nu, nc=delta) + nct.cdf(-critical_value, df=nu, nc=delta)\n\n    return power\n\ndef find_min_sample_size(d, alpha, pi):\n    \"\"\"\n    Finds the smallest integer sample size n >= 2 that achieves the target power.\n\n    This function implements a monotonicity-aware search algorithm, first bracketing\n    the solution and then using binary search to find the exact integer.\n\n    Args:\n        d (float): The standardized effect size.\n        alpha (float): The significance level.\n        pi (float): The target power.\n\n    Returns:\n        int or float: The minimal sample size, or np.inf if no finite size exists.\n    \"\"\"\n    # Handle the edge case where the effect size is zero.\n    if d == 0:\n        # If d=0, the power is always equal to alpha for any n >= 2.\n        # A solution exists only if the target power is not greater than alpha.\n        if pi > alpha:\n            return np.inf\n        else:\n            # If power >= pi is achievable, the smallest valid n is 2.\n            return 2\n\n    # A minimal sample size of n=2 is required for variance calculation.\n    # Check if n=2 already meets the power requirement.\n    if calculate_power(2, d, alpha) >= pi:\n        return 2\n\n    # Bracketing phase: Find an interval [n_low, n_high] that contains the solution.\n    # We expand the interval exponentially until power(n_high) >= pi.\n    n_low = 2\n    n_high = 4\n    while calculate_power(n_high, d, alpha)  pi:\n        n_low = n_high\n        n_high *= 2\n        # Safety stop for unreasonable sample sizes\n        if n_high > 10**7:\n            raise OverflowError(\"Sample size search exceeded 10,000,000.\")\n\n    # Search phase: Use binary search to find the smallest integer n in the bracket.\n    # The solution is known to be in the range [n_low, n_high].\n    low = n_low\n    high = n_high\n    \n    # This binary search variant finds the first element in a sorted sequence\n    # that satisfies the condition (i.e., power >= pi).\n    while low  high:\n        mid = (low + high) // 2\n        if calculate_power(mid, d, alpha)  pi:\n            low = mid + 1\n        else:\n            high = mid\n            \n    # The loop terminates when low == high, which is the smallest n satisfying the condition.\n    return low\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (d, alpha, pi)\n    test_cases = [\n        (0.5, 0.05, 0.8),  # Case 1\n        (0.2, 0.05, 0.8),  # Case 2\n        (1.0, 0.05, 0.9),  # Case 3\n        (0.0, 0.05, 0.8),  # Case 4\n        (0.5, 0.10, 0.5),  # Case 5\n    ]\n\n    results = []\n    for d, alpha, pi in test_cases:\n        result = find_min_sample_size(d, alpha, pi)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4979726"}, {"introduction": "A critical decision in study design is the choice of statistical test, as this directly impacts the required sample size. This practice explores the concept of Asymptotic Relative Efficiency (ARE) to formally compare the performance of the parametric $t$-test against its non-parametric counterpart, the Wilcoxon rank-sum test. By deriving the ARE from first principles, you will gain a deeper understanding of how the underlying distribution of the data dictates the most powerful and efficient analytical approach [@problem_id:4979668].", "problem": "A biostatistics team is planning a balanced two-arm randomized clinical trial to compare a continuous biomarker between a treatment arm and a control arm. Let each arm have $n$ participants. Under standard modeling assumptions, outcomes in the control arm follow a continuous symmetric distribution with probability density function $f$ and finite variance $\\sigma^{2}$, and the treatment arm differs by a small location shift. For principled sample size determination under two competing tests—the Wilcoxon rank-sum test (also known as the Mann–Whitney test) and the two-sample Student’s $t$-test—the team decides to use the concept of Pitman Asymptotic Relative Efficiency (ARE), which compares local asymptotic power under contiguous location alternatives.\n\nStarting from first principles—namely, the Central Limit Theorem for sample means and the large-sample behavior of rank-based statistics under contiguous alternatives—derive an analytic expression for the Pitman Asymptotic Relative Efficiency (ARE) of the Wilcoxon rank-sum test relative to the two-sample $t$-test in terms of $\\sigma^{2}$ and the integral $\\int_{-\\infty}^{\\infty} f(x)^{2}\\,dx$. Then, specialize this expression to two data-generating distributions relevant in medical settings:\n1. The normal distribution $\\mathcal{N}(0,\\sigma^{2})$.\n2. The symmetric double exponential (Laplace) distribution with density $f(x)=\\frac{1}{2b}\\exp\\!\\left(-\\frac{|x|}{b}\\right)$ and variance $\\sigma^{2}=2b^{2}$.\n\nUsing the derived ARE as a sample size conversion factor, compute the ratio of required sample sizes $n_{\\mathrm{W}}/n_{t}$ (Wilcoxon rank-sum over $t$-test) to achieve the same local asymptotic power at a fixed significance level under the contiguous alternative model in each distributional case above. Express your final answer as a row matrix with the first entry corresponding to the normal case and the second entry to the Laplace case, and round your numerical values to four significant figures. No units are required in the final answer.", "solution": "### Derivation of the Pitman ARE\n\nThe Pitman Asymptotic Relative Efficiency of a test $T_1$ with respect to a test $T_2$, denoted $\\text{ARE}(T_1, T_2)$, is defined as the inverse ratio of a quantity called efficacy. For a test statistic $S_N$ based on a total sample size of $N=2n$ for testing $H_0: \\Delta=0$ against $H_1: \\Delta \\ne 0$, its efficacy is related to the rate at which its standardized version detects a small deviation from the null. A common way to compute the ARE is via the formula:\n$$ \\text{ARE}(T_1, T_2) = \\lim_{n \\to \\infty} \\frac{\\mathcal{Q}(T_1)}{\\mathcal{Q}(T_2)} $$\nwhere for a test statistic $S$, $\\mathcal{Q}(S) = \\frac{\\left( \\left. \\frac{d}{d\\Delta} E_\\Delta[S] \\right|_{\\Delta=0} \\right)^2}{\\text{Var}_0[S]}$. Here $E_\\Delta[S]$ is the expectation of $S$ under the location-shift alternative $\\Delta$, and $\\text{Var}_0[S]$ is the variance of $S$ under the null hypothesis $H_0: \\Delta=0$. We will choose appropriate (unscaled) statistics for each test.\n\n**Efficacy component for the two-sample $t$-test ($T_t$)**\n\nThe two-sample $t$-test is based on the difference in sample means. We choose the statistic $S_t = \\bar{Y} - \\bar{X} = \\frac{1}{n}\\sum_{j=1}^n Y_j - \\frac{1}{n}\\sum_{i=1}^n X_i$.\nThe expectation of $S_t$ under the alternative is $E_\\Delta[S_t] = E[Y_j] - E[X_i] = \\Delta - 0 = \\Delta$.\nThe derivative of the expectation with respect to $\\Delta$ at $\\Delta=0$ is:\n$$ \\left. \\frac{d}{d\\Delta} E_\\Delta[S_t] \\right|_{\\Delta=0} = \\left. \\frac{d}{d\\Delta} (\\Delta) \\right|_{\\Delta=0} = 1 $$\nThe variance of $S_t$ under the null hypothesis is:\n$$ \\text{Var}_0[S_t] = \\text{Var}_0[\\bar{Y}] + \\text{Var}_0[\\bar{X}] = \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{n} = \\frac{2\\sigma^2}{n} $$\nThus, the efficacy-related quantity for the $t$-test is:\n$$ \\mathcal{Q}(T_t) = \\frac{1^2}{\\frac{2\\sigma^2}{n}} = \\frac{n}{2\\sigma^2} $$\n\n**Efficacy component for the Wilcoxon rank-sum test ($T_W$)**\n\nThe Wilcoxon rank-sum test is equivalent to the Mann-Whitney $U$-test. We use the Mann-Whitney statistic $S_W = U = \\sum_{i=1}^n \\sum_{j=1}^n I(Y_j > X_i)$, where $I(\\cdot)$ is the indicator function.\nThe expectation of $S_W$ is $E_\\Delta[S_W] = n^2 P_\\Delta(Y > X)$.\nLet $X \\sim f(x)$ and $Y \\sim f(x-\\Delta)$. Let $Y = Y'+\\Delta$ where $Y' \\sim f(y)$. Then $P_\\Delta(Y > X) = P(Y'+\\Delta > X) = P(X-Y'  \\Delta)$.\nThe PDF of $Z = X-Y'$ is $g(z) = \\int_{-\\infty}^\\infty f(x) f(x-z) \\,dx$.\nSo, $P(X-Y'  \\Delta) = \\int_{-\\infty}^\\Delta g(z) \\,dz$.\nThe derivative of the expectation of $S_W$ is:\n$$ \\left. \\frac{d}{d\\Delta} E_\\Delta[S_W] \\right|_{\\Delta=0} = n^2 \\left. \\frac{d}{d\\Delta} \\int_{-\\infty}^\\Delta g(z) \\,dz \\right|_{\\Delta=0} = n^2 g(0) $$\nby the Fundamental Theorem of Calculus.\nThe value $g(0)$ is $g(0) = \\int_{-\\infty}^\\infty f(x)f(x-0)\\,dx = \\int_{-\\infty}^\\infty f(x)^2 \\,dx$.\nSo, $\\left. \\frac{d}{d\\Delta} E_\\Delta[S_W] \\right|_{\\Delta=0} = n^2 \\int_{-\\infty}^\\infty f(x)^2 \\,dx$.\nThe variance of $U$ under the null hypothesis is given by the standard formula:\n$$ \\text{Var}_0[S_W] = \\text{Var}_0[U] = \\frac{n \\cdot n \\cdot (n+n+1)}{12} = \\frac{n^2(2n+1)}{12} $$\nThe efficacy-related quantity for the Wilcoxon test is:\n$$ \\mathcal{Q}(T_W) = \\frac{\\left( n^2 \\int_{-\\infty}^\\infty f(x)^2 \\,dx \\right)^2}{\\frac{n^2(2n+1)}{12}} = \\frac{12 n^4 \\left( \\int_{-\\infty}^\\infty f(x)^2 \\,dx \\right)^2}{n^2(2n+1)} = \\frac{12 n^2}{2n+1} \\left( \\int_{-\\infty}^\\infty f(x)^2 \\,dx \\right)^2 $$\nIn the limit as $n \\to \\infty$, we have $\\frac{12n^2}{2n+1} \\approx \\frac{12n^2}{2n} = 6n$.\nSo, $\\lim_{n \\to \\infty} \\frac{\\mathcal{Q}(T_W)}{n} = 6 \\left( \\int_{-\\infty}^\\infty f(x)^2 \\,dx \\right)^2$.\n\n**General Expression for ARE($W,t$)**\n\nNow we compute the ARE as the ratio of these asymptotic efficacy-related quantities:\n$$ \\text{ARE}(W,t) = \\lim_{n \\to \\infty} \\frac{\\mathcal{Q}(T_W)}{\\mathcal{Q}(T_t)} = \\frac{ \\frac{12 n^2}{2n+1} \\left( \\int_{-\\infty}^\\infty f(x)^2 \\,dx \\right)^2 }{ \\frac{n}{2\\sigma^2} } $$\n$$ = \\lim_{n \\to \\infty} \\frac{12 n^2}{2n+1} \\cdot \\frac{2\\sigma^2}{n} \\left( \\int_{-\\infty}^\\infty f(x)^2 \\,dx \\right)^2 = \\lim_{n \\to \\infty} \\frac{24 n}{2n+1} \\sigma^2 \\left( \\int_{-\\infty}^\\infty f(x)^2 \\,dx \\right)^2 $$\n$$ = 12 \\sigma^2 \\left( \\int_{-\\infty}^\\infty f(x)^2 \\,dx \\right)^2 $$\nThis is the general analytic expression for the Pitman ARE of the Wilcoxon rank-sum test relative to the two-sample $t$-test.\n\n### Specialization to Specific Distributions\n\n**1. Normal Distribution**\nThe PDF is $f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)$. We compute the integral:\n$$ \\int_{-\\infty}^\\infty f(x)^2 \\,dx = \\int_{-\\infty}^\\infty \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) \\right)^2 \\,dx = \\frac{1}{2\\pi\\sigma^2} \\int_{-\\infty}^\\infty \\exp\\left(-\\frac{x^2}{\\sigma^2}\\right) \\,dx $$\nThe integral is a standard Gaussian integral, $\\int_{-\\infty}^\\infty \\exp(-ax^2)dx = \\sqrt{\\pi/a}$. Here $a=1/\\sigma^2$.\n$$ \\int_{-\\infty}^\\infty \\exp\\left(-\\frac{x^2}{\\sigma^2}\\right) \\,dx = \\sqrt{\\pi / (1/\\sigma^2)} = \\sigma\\sqrt{\\pi} $$\nSo, $\\int_{-\\infty}^\\infty f(x)^2 \\,dx = \\frac{1}{2\\pi\\sigma^2} (\\sigma\\sqrt{\\pi}) = \\frac{1}{2\\sigma\\sqrt{\\pi}}$.\nSubstituting into the ARE formula:\n$$ \\text{ARE}_{Normal} = 12\\sigma^2 \\left( \\frac{1}{2\\sigma\\sqrt{\\pi}} \\right)^2 = 12\\sigma^2 \\left( \\frac{1}{4\\sigma^2\\pi} \\right) = \\frac{3}{\\pi} $$\n\n**2. Laplace (Double Exponential) Distribution**\nThe PDF is $f(x) = \\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right)$ with variance $\\sigma^2 = 2b^2$.\n$$ \\int_{-\\infty}^\\infty f(x)^2 \\,dx = \\int_{-\\infty}^\\infty \\left( \\frac{1}{2b} \\exp\\left(-\\frac{|x|}{b}\\right) \\right)^2 \\,dx = \\frac{1}{4b^2} \\int_{-\\infty}^\\infty \\exp\\left(-\\frac{2|x|}{b}\\right) \\,dx $$\nSince the integrand is an even function:\n$$ = \\frac{2}{4b^2} \\int_0^\\infty \\exp\\left(-\\frac{2x}{b}\\right) \\,dx = \\frac{1}{2b^2} \\left[ -\\frac{b}{2} \\exp\\left(-\\frac{2x}{b}\\right) \\right]_0^\\infty = \\frac{1}{2b^2} \\left( 0 - \\left(-\\frac{b}{2}\\right) \\right) = \\frac{b}{4b^2} = \\frac{1}{4b} $$\nFrom $\\sigma^2 = 2b^2$, we have $b = \\sigma/\\sqrt{2}$. Substituting this:\n$$ \\int_{-\\infty}^\\infty f(x)^2 \\,dx = \\frac{1}{4(\\sigma/\\sqrt{2})} = \\frac{\\sqrt{2}}{4\\sigma} $$\nSubstituting into the ARE formula:\n$$ \\text{ARE}_{Laplace} = 12\\sigma^2 \\left( \\frac{\\sqrt{2}}{4\\sigma} \\right)^2 = 12\\sigma^2 \\left( \\frac{2}{16\\sigma^2} \\right) = \\frac{24}{16} = \\frac{3}{2} $$\n\n### Ratio of Required Sample Sizes\n\nThe Pitman ARE is the limiting ratio of sample sizes required by two tests to achieve the same power against the same sequence of local alternatives. Specifically, ARE$(W, t) = \\lim \\frac{n_t}{n_W}$.\nTherefore, the ratio of sample sizes required by the Wilcoxon test ($n_W$) versus the $t$-test ($n_t$) is the reciprocal of the ARE:\n$$ \\frac{n_W}{n_t} = \\frac{1}{\\text{ARE}(W,t)} $$\n**1. Normal Case:**\n$$ \\frac{n_W}{n_t} = \\frac{1}{3/\\pi} = \\frac{\\pi}{3} \\approx 1.047197... $$\nRounding to four significant figures, this is $1.047$.\n\n**2. Laplace Case:**\n$$ \\frac{n_W}{n_t} = \\frac{1}{3/2} = \\frac{2}{3} \\approx 0.666666... $$\nRounding to four significant figures, this is $0.6667$.\n\nThese results indicate that for normally distributed data, the Wilcoxon test requires approximately $4.7\\%$ more samples than the $t$-test to achieve the same power. For Laplace distributed (heavier-tailed) data, the Wilcoxon test is more efficient, requiring only about two-thirds of the sample size of the $t$-test.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.047  0.6667\n\\end{pmatrix}\n}\n$$", "id": "4979668"}, {"introduction": "Efficient study designs, such as crossover trials, can dramatically reduce the required sample size by allowing each subject to serve as their own control. This advanced exercise challenges you to derive a sample size formula from first principles for a crossover study with a recurrent event (count) outcome, a common scenario in clinical research. You will apply the powerful technique of conditional likelihood to eliminate nuisance parameters and derive the Fisher information, providing a blueprint for tackling bespoke power calculation problems [@problem_id:4979687].", "problem": "A randomized two-period crossover study will compare a new prophylactic regimen against standard care on the rate of recurrent acute infection episodes in a chronic-disease population. Each subject will contribute two observation periods of equal length $T$, one on new prophylaxis and one on standard care, with the order randomized. Let $Y_{i1}$ and $Y_{i2}$ denote the event counts for subject $i$ in the prophylaxis and standard-care periods, respectively. Assume the following generative model rooted in first principles:\n\n- Conditional on an unobserved subject-specific baseline intensity $\\lambda_i$, events within each period arise from a Poisson process, and the two periods are conditionally independent.\n- The mean counts satisfy $\\mathbb{E}[Y_{i1} \\mid \\lambda_i] = \\lambda_i T \\exp(\\beta/2)$ and $\\mathbb{E}[Y_{i2} \\mid \\lambda_i] = \\lambda_i T \\exp(-\\beta/2)$, where $\\beta = \\ln(\\theta)$ is the log rate ratio comparing prophylaxis to standard care.\n\nPlanning values from prior data suggest $\\mathbb{E}[\\lambda_i] = m = 0.04$ events per day, with each period of length $T = 60$ days. The study aims to detect a clinically meaningful rate ratio $\\theta = 0.75$ at a two-sided type I error $\\alpha = 0.05$ with power $1 - \\beta_{\\text{power}} = 0.90$.\n\nUsing the conditional Poisson approach that conditions on the within-subject total $S_i = Y_{i1} + Y_{i2}$ to eliminate the nuisance $\\lambda_i$, derive from first principles a Wald test for $H_0\\!:\\, \\beta = 0$ and obtain the expected Fisher information per pair under the alternative. Then compute the minimum integer number of matched pairs $n$ required to achieve the stated power.\n\nFinally, for design insight, suppose that in an unpaired two-arm parallel design with equal allocation, subject-level heterogeneity induces negative binomial variation with variance function $\\operatorname{Var}(Y) = \\mu \\{1 + \\phi \\mu\\}$ and overdispersion parameter $\\phi = 0.5$. Using the same $m$, $T$, and $\\theta$, compute the ratio of per-total-subject information (one matched pair in the paired design versus two independent subjects, one per arm, in the unpaired design). Explain the efficiency gain or loss implied by this ratio.\n\nReport as your final numeric answer only the smallest integer $n$ that achieves the target power. Do not include any additional quantities in your final answer.", "solution": "The first step is to derive the statistical model for the conditional analysis. Let $Y_{i1}$ and $Y_{i2}$ be the event counts for subject $i$ in the prophylaxis and standard care periods, respectively. The period length is $T$. Conditional on a subject-specific baseline rate $\\lambda_i$, the counts are independent Poisson variables:\n$Y_{i1} \\mid \\lambda_i \\sim \\text{Poisson}(\\mu_{i1})$ with $\\mu_{i1} = \\lambda_i T \\exp(\\beta/2)$\n$Y_{i2} \\mid \\lambda_i \\sim \\text{Poisson}(\\mu_{i2})$ with $\\mu_{i2} = \\lambda_i T \\exp(-\\beta/2)$\nwhere $\\beta = \\ln(\\theta)$ is the log rate ratio.\n\nTo eliminate the nuisance parameter $\\lambda_i$, we condition on the total count for subject $i$, $S_i = Y_{i1} + Y_{i2}$. The distribution of $S_i$ conditional on $\\lambda_i$ is also Poisson, as it is the sum of two independent Poisson variables:\n$S_i \\mid \\lambda_i \\sim \\text{Poisson}(\\mu_{i1} + \\mu_{i2})$.\nThe conditional distribution of $Y_{i1}$ given $S_i=s_i$ is:\n$$ P(Y_{i1}=y_{i1} \\mid S_i=s_i) = \\frac{P(Y_{i1}=y_{i1}, Y_{i2}=s_i-y_{i1})}{P(S_i=s_i)} $$\nConditional on $\\lambda_i$, this probability becomes:\n$$ P(Y_{i1}=y_{i1} \\mid S_i=s_i, \\lambda_i) = \\frac{P(Y_{i1}=y_{i1} \\mid \\lambda_i) P(Y_{i2}=s_i-y_{i1} \\mid \\lambda_i)}{P(S_i=s_i \\mid \\lambda_i)} $$\n$$ = \\frac{ \\frac{\\exp(-\\mu_{i1})\\mu_{i1}^{y_{i1}}}{y_{i1}!} \\frac{\\exp(-\\mu_{i2})\\mu_{i2}^{s_i-y_{i1}}}{(s_i-y_{i1})!} }{ \\frac{\\exp(-(\\mu_{i1}+\\mu_{i2}))(\\mu_{i1}+\\mu_{i2})^{s_i}}{s_i!} } $$\n$$ = \\frac{s_i!}{y_{i1}!(s_i-y_{i1})!} \\frac{\\mu_{i1}^{y_{i1}}\\mu_{i2}^{s_i-y_{i1}}}{(\\mu_{i1}+\\mu_{i2})^{s_i}} = \\binom{s_i}{y_{i1}} p^{y_{i1}} (1-p)^{s_i-y_{i1}} $$\nwhere the probability $p$ is:\n$$ p = \\frac{\\mu_{i1}}{\\mu_{i1}+\\mu_{i2}} = \\frac{\\lambda_i T \\exp(\\beta/2)}{\\lambda_i T \\exp(\\beta/2) + \\lambda_i T \\exp(-\\beta/2)} = \\frac{\\exp(\\beta/2)}{\\exp(\\beta/2) + \\exp(-\\beta/2)} $$\nThis simplifies to $p = \\frac{1}{1 + \\exp(-\\beta)}$, which is the logistic function. Crucially, $p$ does not depend on $\\lambda_i$, so the conditional distribution is free of the nuisance parameter. Thus, for each subject $i$ with total count $S_i > 0$, the conditional distribution is Binomial: $Y_{i1} \\mid S_i=s_i \\sim \\text{Binomial}(s_i, p)$.\n\nThe log-likelihood for subject $i$ based on this conditional distribution is:\n$$ l_i(\\beta) = \\ln\\binom{S_i}{Y_{i1}} + Y_{i1}\\ln(p) + (S_i-Y_{i1})\\ln(1-p) $$\n$$ l_i(\\beta) = \\text{const} + Y_{i1}\\beta - S_i\\ln(1+\\exp(\\beta)) $$\nThe score function (first derivative) for subject $i$ is:\n$$ U_i(\\beta) = \\frac{\\partial l_i}{\\partial \\beta} = Y_{i1} - S_i \\frac{\\exp(\\beta)}{1+\\exp(\\beta)} = Y_{i1} - S_i p $$\nThe Fisher information for subject $i$, conditional on $S_i$, is:\n$$ I_i(\\beta \\mid S_i) = -\\frac{\\partial^2 l_i}{\\partial \\beta^2} = S_i \\frac{\\partial p}{\\partial \\beta} = S_i p(1-p) $$\nA Wald test for $H_0: \\beta=0$ is based on the test statistic $W = \\frac{\\hat{\\beta}}{\\text{se}(\\hat{\\beta})}$, where $\\hat{\\beta}$ is the maximum likelihood estimate of $\\beta$ from the full conditional likelihood $L(\\beta) = \\sum_i l_i(\\beta)$, and $\\text{se}(\\hat{\\beta}) = \\left(\\sum_i I_i(\\hat{\\beta} \\mid S_i)\\right)^{-1/2}$. The test rejects $H_0$ if $|W| > z_{1-\\alpha/2}$.\n\nNext, we derive the expected Fisher information per matched pair, $I_1(\\beta)$. This is the expectation of $I_i(\\beta \\mid S_i)$ over the distribution of $S_i$:\n$$ I_1(\\beta) = \\mathbb{E}[I_i(\\beta \\mid S_i)] = \\mathbb{E}[S_i p(1-p)] = \\mathbb{E}[S_i] p(1-p) $$\nThe expectation of $S_i$ is found by first conditioning on $\\lambda_i$:\n$$ \\mathbb{E}[S_i \\mid \\lambda_i] = \\mu_{i1} + \\mu_{i2} = \\lambda_i T (\\exp(\\beta/2) + \\exp(-\\beta/2)) = 2\\lambda_i T \\cosh(\\beta/2) $$\nTaking the expectation over $\\lambda_i$, and using $\\mathbb{E}[\\lambda_i] = m$:\n$$ \\mathbb{E}[S_i] = 2mT\\cosh(\\beta/2) $$\nThe term $p(1-p)$ simplifies to:\n$$ p(1-p) = \\frac{\\exp(\\beta/2)}{\\exp(\\beta/2) + \\exp(-\\beta/2)} \\frac{\\exp(-\\beta/2)}{\\exp(\\beta/2) + \\exp(-\\beta/2)} = \\frac{1}{(\\exp(\\beta/2) + \\exp(-\\beta/2))^2} = \\frac{1}{4\\cosh^2(\\beta/2)} $$\nCombining these gives the expected information per pair:\n$$ I_1(\\beta) = 2mT\\cosh(\\beta/2) \\cdot \\frac{1}{4\\cosh^2(\\beta/2)} = \\frac{mT}{2\\cosh(\\beta/2)} $$\nUnder the alternative hypothesis, $\\theta_A = 0.75$, so $\\beta_A = \\ln(0.75)$.\n$$ \\cosh(\\beta_A/2) = \\frac{\\exp(\\ln(0.75)/2) + \\exp(-\\ln(0.75)/2)}{2} = \\frac{\\sqrt{0.75} + 1/\\sqrt{0.75}}{2} = \\frac{\\sqrt{3}/2 + 2/\\sqrt{3}}{2} = \\frac{7}{4\\sqrt{3}} $$\nThe information under the alternative is:\n$$ I_1(\\beta_A) = \\frac{mT}{2(7/(4\\sqrt{3}))} = \\frac{2\\sqrt{3}mT}{7} $$\n\nThe sample size $n$ required to achieve power $1-\\beta_{\\text{power}}$ for a two-sided test at level $\\alpha$ is given by the standard formula for a Wald test:\n$$ n = \\frac{(z_{1-\\alpha/2} + z_{1-\\beta_{\\text{power}}})^2}{\\beta_A^2 I_1(\\beta_A)} $$\nWe are given $\\alpha=0.05$ and $1-\\beta_{\\text{power}}=0.90$, which yields $z_{1-\\alpha/2}=z_{0.975} \\approx 1.96$ and $z_{1-\\beta_{\\text{power}}}=z_{0.90} \\approx 1.2816$.\nThe planning values are $m=0.04$ events/day and $T=60$ days. The effect size is $\\beta_A = \\ln(0.75) \\approx -0.28768$.\nThe information per pair under the alternative is:\n$$ I_1(\\beta_A) = \\frac{2\\sqrt{3}(0.04)(60)}{7} = \\frac{4.8\\sqrt{3}}{7} \\approx 1.18765 $$\nPlugging these values into the sample size formula:\n$$ n = \\frac{(1.96 + 1.2816)^2}{(\\ln(0.75))^2 \\left(\\frac{4.8\\sqrt{3}}{7}\\right)} \\approx \\frac{(3.2416)^2}{(-0.28768)^2(1.18765)} \\approx \\frac{10.508}{0.08276 \\times 1.18765} \\approx \\frac{10.508}{0.09827} \\approx 106.93 $$\nSince the number of pairs must be an integer, we take the ceiling, which gives $n=107$.\n\nFinally, for design insight, we compute the ratio of information. The information from one matched pair in the paired design is $I_{\\text{paired}} = I_1(\\beta_A) \\approx 1.1877$.\nFor the unpaired design, we have two independent subjects, one in each arm, with counts following a negative binomial distribution with variance $\\operatorname{Var}(Y) = \\mu(1+\\phi\\mu)$ and $\\phi=0.5$. The means are $\\mu_1 = mT\\exp(\\beta/2)$ and $\\mu_2 = mT\\exp(-\\beta/2)$. The Fisher information for $\\beta$ from these two subjects is:\n$$ I_{\\text{unpaired}}(\\beta) = \\left(\\frac{d\\mu_1}{d\\beta}\\right)^2 \\frac{1}{\\operatorname{Var}(Y_1)} + \\left(\\frac{d\\mu_2}{d\\beta}\\right)^2 \\frac{1}{\\operatorname{Var}(Y_2)} $$\nWith $\\frac{d\\mu_1}{d\\beta} = \\frac{1}{2}\\mu_1$ and $\\frac{d\\mu_2}{d\\beta} = -\\frac{1}{2}\\mu_2$, this becomes:\n$$ I_{\\text{unpaired}}(\\beta) = \\frac{1}{4} \\left( \\frac{\\mu_1}{1+\\phi\\mu_1} + \\frac{\\mu_2}{1+\\phi\\mu_2} \\right) $$\nUnder the alternative, $mT=2.4$, $\\beta_A=\\ln(0.75)$, $\\phi=0.5$.\n$\\mu_1 = 2.4 \\sqrt{0.75} = 1.2\\sqrt{3} \\approx 2.0785$.\n$\\mu_2 = 2.4 / \\sqrt{0.75} = 1.6\\sqrt{3} \\approx 2.7713$.\n$$ I_{\\text{unpaired}}(\\beta_A) \\approx \\frac{1}{4} \\left( \\frac{2.0785}{1+0.5(2.0785)} + \\frac{2.7713}{1+0.5(2.7713)} \\right) \\approx \\frac{1}{4} \\left( \\frac{2.0785}{2.03925} + \\frac{2.7713}{2.38565} \\right) \\approx 0.5452 $$\nThe ratio of per-total-subject information is:\n$$ \\text{Ratio} = \\frac{I_{\\text{paired}}}{I_{\\text{unpaired}}} \\approx \\frac{1.1877}{0.5452} \\approx 2.18 $$\nThis ratio greater than $2$ indicates a substantial efficiency gain for the crossover design. For the same number of total observations (e.g., $1$ pair vs $2$ individuals), the crossover study provides more than twice the statistical information about the treatment effect. This gain is due to the paired design's ability to eliminate between-subject variability ($\\lambda_i$) by making within-subject comparisons. The large overdispersion parameter $\\phi=0.5$ in the parallel design model signifies high between-subject variability, which inflates the variance of estimates and reduces information, making the crossover design comparatively much more efficient.", "answer": "$$\n\\boxed{107}\n$$", "id": "4979687"}]}