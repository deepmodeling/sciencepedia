{"hands_on_practices": [{"introduction": "A cornerstone of frequentist inference is the principle of maximum likelihood, which provides a general and powerful method for estimating model parameters. This practice guides you through deriving a Maximum Likelihood Estimator (MLE) for a binomial proportion, a common task in medical studies with binary outcomes. Beyond finding the single best estimate, you will also compute the Fisher information, a key concept that measures the precision of the MLE and forms the basis for constructing confidence intervals and conducting hypothesis tests [@problem_id:4988042].", "problem": "A single-arm pharmacovigilance study monitors the occurrence of a specific adverse event among $n$ independent patients receiving a new therapy. Let $X$ denote the number of patients who experience the event, and model $X$ as a binomial random variable $X \\sim \\mathrm{Bin}(n,p)$, where $p$ is the per-patient event probability. You observe the count $x \\in \\{0,1,\\dots,n\\}$. Under the frequentist approach to inference, and starting from the binomial probability mass function and the definition of a maximum likelihood estimator (MLE), perform the following:\n\n1. Derive the maximum likelihood estimator (MLE) $\\hat{p}$ for $p$.\n2. Starting from the log-likelihood for $p$ based on the observed count $x$, compute the observed Fisher information for $p$. Use it to obtain a closed-form expression for the asymptotic variance of $\\hat{p}$ evaluated at $\\hat{p}$, expressed only in terms of $x$ and $n$.\n\nProvide your final answer as a single analytic expression in terms of $x$ and $n$. No rounding is necessary. Do not include any units in your final expression.", "solution": "The problem as stated is valid. It is a well-posed problem in frequentist statistical inference, grounded in fundamental principles of maximum likelihood estimation and Fisher information for a binomial model, a standard application in medical statistics. All necessary information is provided, and the problem is self-contained and scientifically sound.\n\nThe model for the number of adverse events, $X$, among $n$ patients is a binomial distribution, $X \\sim \\mathrm{Bin}(n, p)$, where $p$ is the unknown per-patient event probability. We are given an observed count $x$.\n\nThe probability mass function (PMF) for $X=x$ is:\n$$\nP(X=x | n, p) = \\binom{n}{x} p^x (1-p)^{n-x}\n$$\nThis function, when viewed as a function of the parameter $p$ for a fixed observation $x$, is the likelihood function, $L(p|x)$.\n\n**Part 1: Derivation of the Maximum Likelihood Estimator (MLE) for $p$**\n\nTo find the MLE, we first define the log-likelihood function, $\\ell(p|x)$, which is the natural logarithm of the likelihood function. Maximizing the log-likelihood is equivalent to maximizing the likelihood.\n$$\n\\ell(p|x) = \\ln[L(p|x)] = \\ln\\left[\\binom{n}{x} p^x (1-p)^{n-x}\\right]\n$$\nUsing the properties of logarithms, we can write:\n$$\n\\ell(p|x) = \\ln\\binom{n}{x} + x\\ln(p) + (n-x)\\ln(1-p)\n$$\nTo find the value of $p$ that maximizes this function, we take the first derivative with respect to $p$ (the score function) and set it to zero.\n$$\n\\frac{d\\ell}{dp} = \\frac{d}{dp}\\left[ \\ln\\binom{n}{x} + x\\ln(p) + (n-x)\\ln(1-p) \\right] = \\frac{x}{p} - \\frac{n-x}{1-p}\n$$\nSetting the derivative to zero yields the equation:\n$$\n\\frac{x}{p} = \\frac{n-x}{1-p}\n$$\nSolving for $p$:\n$$\nx(1-p) = p(n-x)\n$$\n$$\nx - xp = np - xp\n$$\n$$\nx = np\n$$\nThe solution for $p$ is the maximum likelihood estimator, denoted as $\\hat{p}$:\n$$\n\\hat{p} = \\frac{x}{n}\n$$\nTo confirm this point is a maximum, we can check the second derivative of the log-likelihood:\n$$\n\\frac{d^2\\ell}{dp^2} = -\\frac{x}{p^2} - \\frac{n-x}{(1-p)^2}\n$$\nFor $p \\in (0, 1)$ and observed $x \\in \\{1, \\dots, n-1\\}$, this second derivative is always negative, confirming that $\\hat{p}$ is indeed a maximum. The formula also holds for the boundary cases $x=0$ and $x=n$.\n\n**Part 2: Asymptotic Variance of $\\hat{p}$**\n\nThe problem requires calculating the observed Fisher information, which is defined as the negative of the second derivative of the log-likelihood function:\n$$\nI_O(p) = -\\frac{d^2\\ell}{dp^2} = -\\left(-\\frac{x}{p^2} - \\frac{n-x}{(1-p)^2}\\right) = \\frac{x}{p^2} + \\frac{n-x}{(1-p)^2}\n$$\nThe asymptotic variance of an MLE can be estimated by the inverse of the observed Fisher information evaluated at the MLE, $\\hat{p}$. So, we substitute $p = \\hat{p} = \\frac{x}{n}$ into the expression for $I_O(p)$. This step is strictly valid for $x \\in \\{1, \\dots, n-1\\}$ to avoid division by zero.\n$$\nI_O(\\hat{p}) = \\frac{x}{(\\frac{x}{n})^2} + \\frac{n-x}{(1-\\frac{x}{n})^2}\n$$\nSimplifying the expression:\n$$\nI_O(\\hat{p}) = \\frac{x}{x^2/n^2} + \\frac{n-x}{((n-x)/n)^2} = \\frac{xn^2}{x^2} + \\frac{(n-x)n^2}{(n-x)^2}\n$$\n$$\nI_O(\\hat{p}) = \\frac{n^2}{x} + \\frac{n^2}{n-x}\n$$\nTo combine these terms, we find a common denominator:\n$$\nI_O(\\hat{p}) = n^2 \\left( \\frac{1}{x} + \\frac{1}{n-x} \\right) = n^2 \\left( \\frac{n-x+x}{x(n-x)} \\right) = \\frac{n^3}{x(n-x)}\n$$\nThe estimated asymptotic variance of $\\hat{p}$, denoted $\\widehat{\\mathrm{Var}(\\hat{p})}$, is the inverse of the observed Fisher information evaluated at $\\hat{p}$:\n$$\n\\widehat{\\mathrm{Var}(\\hat{p})} = [I_O(\\hat{p})]^{-1} = \\left( \\frac{n^3}{x(n-x)} \\right)^{-1}\n$$\n$$\n\\widehat{\\mathrm{Var}(\\hat{p})} = \\frac{x(n-x)}{n^3}\n$$\nThis is the required closed-form expression for the asymptotic variance of $\\hat{p}$, evaluated at $\\hat{p}$, expressed in terms of $x$ and $n$. Although the intermediate steps in the derivation required $x$ to be strictly between $0$ and $n$, this final expression is well-defined for $x=0$ and $x=n$, where it correctly evaluates to $0$.", "answer": "$$\n\\boxed{\\frac{x(n-x)}{n^3}}\n$$", "id": "4988042"}, {"introduction": "While a point estimate provides a single value for a parameter, a confidence interval communicates the associated uncertainty. This exercise demonstrates how to construct an exact confidence interval for a population mean in the common and realistic scenario where the population variance is unknown. You will derive this interval from first principles by defining a pivotal quantity—the t-statistic—which follows a known distribution regardless of the true parameter values, a fundamental technique in building frequentist interval estimators [@problem_id:4988052].", "problem": "A clinical pharmacology team is evaluating the mean change in low-density lipoprotein cholesterol across a cohort of patients initiating a high-intensity statin. For a simple yet scientifically realistic model, assume the patient-level change (post-therapy minus pre-therapy) in low-density lipoprotein cholesterol, denoted $X_{i}$, is independent and identically distributed as a normal random variable with unknown mean $\\mu$ and unknown variance $\\sigma^2$. The sample size is $n$, the sample mean is $\\bar{X}$, and the unbiased sample variance is $S^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}$. Using the frequentist paradigm, treat $\\mu$ and $\\sigma^2$ as fixed but unknown parameters and construct an interval estimator that has exact coverage under the stated model.\n\nStarting only from fundamental definitions and well-tested distributional facts for the normal model (for example, the standard normal distribution of a standardized mean under known variance, the chi-square distribution of the scaled unbiased sample variance, and the independence of the sample mean and sample variance under normality), do the following:\n\n1. Define a pivotal quantity for $\\mu$ when $\\sigma^2$ is unknown and justify why its distribution does not depend on any unknown parameters.\n2. Use the pivotal quantity to derive a two-sided $(1-\\alpha)$ Confidence Interval (CI) for $\\mu$, written explicitly in terms of $\\bar{X}$, $S$, $n$, and appropriate quantiles of a fully specified reference distribution.\n3. In a prospective study of $n=23$ patients, the observed summary statistics are $\\bar{X}=-32.5$ and $S=12.0$, where low-density lipoprotein cholesterol changes are measured in $\\mathrm{mg/dL}$. Using your exact CI from part 2 with $\\alpha=0.05$, compute the numerical endpoints. Round your two endpoints to four significant figures. Express both endpoints in $\\mathrm{mg/dL}$.", "solution": "The problem requires the derivation and subsequent application of a frequentist confidence interval for the population mean $\\mu$ of a normal distribution where the population variance $\\sigma^2$ is unknown. The solution will be constructed by first identifying a suitable pivotal quantity, then deriving the general form of the confidence interval, and finally computing the numerical interval for the given data.\n\n1.  Definition and Justification of the Pivotal Quantity\n\nThe foundation of constructing this confidence interval is a pivotal quantity, which is a function of the sample data and the unknown parameter whose probability distribution does not depend on any unknown parameters.\n\nWe are given that the patient-level changes, $X_{i}$, are independent and identically distributed as normal random variables, $X_{i} \\sim \\mathcal{N}(\\mu, \\sigma^2)$. Based on this, we use the following fundamental distributional facts:\n- The sample mean, $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$, is normally distributed with mean $\\mu$ and variance $\\frac{\\sigma^2}{n}$. Standardizing $\\bar{X}$ yields a standard normal random variable, $Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim \\mathcal{N}(0, 1)$.\n- The scaled unbiased sample variance, where $S^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}$, follows a chi-square distribution: $\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi_{n-1}^{2}$.\n- For samples drawn from a normal distribution, the sample mean $\\bar{X}$ and the sample variance $S^2$ are statistically independent.\n\nThe standardized mean, $Z$, is not a pivotal quantity because its formula involves the unknown parameter $\\sigma$. To create a pivotal quantity, we must replace $\\sigma$ with an estimate. The natural estimator for $\\sigma$ is the sample standard deviation $S$.\n\nWe define a Student's t-distributed random variable as the ratio of a standard normal random variable to the square root of an independent chi-square random variable divided by its degrees of freedom. Let $Z \\sim \\mathcal{N}(0, 1)$, $V \\sim \\chi_{k}^{2}$, and let $Z$ and $V$ be independent. Then the random variable $T = \\frac{Z}{\\sqrt{V/k}}$ follows a Student's t-distribution with $k$ degrees of freedom, denoted $t_{k}$.\n\nIn our case, we set:\n- $Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}}$\n- $V = \\frac{(n-1)S^2}{\\sigma^2}$\n- $k = n-1$\n\nDue to the independence of $\\bar{X}$ and $S^2$, $Z$ and $V$ are independent. We can now construct the pivotal quantity $T$:\n$$T = \\frac{Z}{\\sqrt{V/k}} = \\frac{\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{\\frac{(n-1)S^2/\\sigma^2}{n-1}}}$$\nSimplifying the denominator gives:\n$$\\sqrt{\\frac{S^2}{\\sigma^2}} = \\frac{S}{\\sigma} \\quad (\\text{since } S > 0)$$\nSubstituting this back into the expression for $T$:\n$$T = \\frac{\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}}}{\\frac{S}{\\sigma}} = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\cdot \\frac{\\sigma}{S} = \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}}$$\nThis quantity $T$ follows a Student's t-distribution with $n-1$ degrees of freedom, $T \\sim t_{n-1}$. Its distribution depends only on the sample size $n$, which is known, and not on the unknown parameters $\\mu$ or $\\sigma^2$. Therefore, $T = \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}}$ is a pivotal quantity for $\\mu$.\n\n2.  Derivation of the $(1-\\alpha)$ Confidence Interval for $\\mu$\n\nUsing the pivotal quantity $T \\sim t_{n-1}$, we can construct a probability statement to form the basis of the confidence interval. For a two-sided confidence level of $1-\\alpha$, we seek values that bound the central $1-\\alpha$ portion of the $t_{n-1}$ distribution. Let $t_{n-1, \\alpha/2}$ denote the critical value from the $t_{n-1}$ distribution such that the area in the upper tail is $\\alpha/2$; that is, $P(T > t_{n-1, \\alpha/2}) = \\alpha/2$. Due to the symmetry of the t-distribution about $0$, we also have $P(T < -t_{n-1, \\alpha/2}) = \\alpha/2$.\n\nThe central probability is therefore:\n$$P(-t_{n-1, \\alpha/2} \\le T \\le t_{n-1, \\alpha/2}) = 1 - \\alpha$$\nSubstituting the expression for the pivotal quantity $T$:\n$$P\\left(-t_{n-1, \\alpha/2} \\le \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} \\le t_{n-1, \\alpha/2}\\right) = 1 - \\alpha$$\nTo derive the confidence interval for $\\mu$, we algebraically isolate $\\mu$ in the center of the inequalities:\n$$-t_{n-1, \\alpha/2} \\cdot \\frac{S}{\\sqrt{n}} \\le \\bar{X} - \\mu \\le t_{n-1, \\alpha/2} \\cdot \\frac{S}{\\sqrt{n}}$$\nSubtract $\\bar{X}$ from all parts:\n$$-\\bar{X} -t_{n-1, \\alpha/2} \\cdot \\frac{S}{\\sqrt{n}} \\le - \\mu \\le -\\bar{X} + t_{n-1, \\alpha/2} \\cdot \\frac{S}{\\sqrt{n}}$$\nMultiply all parts by $-1$ and reverse the direction of the inequalities:\n$$\\bar{X} + t_{n-1, \\alpha/2} \\cdot \\frac{S}{\\sqrt{n}} \\ge \\mu \\ge \\bar{X} - t_{n-1, \\alpha/2} \\cdot \\frac{S}{\\sqrt{n}}$$\nThis gives the two-sided $(1-\\alpha)$ confidence interval for $\\mu$:\n$$\\left[ \\bar{X} - t_{n-1, \\alpha/2} \\frac{S}{\\sqrt{n}}, \\quad \\bar{X} + t_{n-1, \\alpha/2} \\frac{S}{\\sqrt{n}} \\right]$$\n\n3.  Numerical Computation\n\nThe problem provides the following summary statistics and parameters:\n- Sample size: $n=23$\n- Sample mean: $\\bar{X}=-32.5$\n- Sample standard deviation: $S=12.0$\n- Significance level: $\\alpha=0.05$\n\nThe degrees of freedom for the t-distribution are $df = n-1 = 23-1 = 22$.\nThe desired confidence level is $1-\\alpha = 1-0.05 = 0.95$.\nWe need the critical value $t_{df, \\alpha/2} = t_{22, 0.025}$. From standard statistical tables or computational software, this value is approximately $2.0739$.\n\nThe standard error of the mean (SE) is:\n$$SE = \\frac{S}{\\sqrt{n}} = \\frac{12.0}{\\sqrt{23}}$$\nThe margin of error (ME) is the product of the critical value and the standard error:\n$$ME = t_{22, 0.025} \\cdot SE = 2.0739 \\cdot \\frac{12.0}{\\sqrt{23}} \\approx 2.0739 \\cdot \\frac{12.0}{4.79583} \\approx 2.0739 \\cdot 2.50216 \\approx 5.1893$$\nThe confidence interval is computed as $\\bar{X} \\pm ME$:\n- Lower Endpoint: $L = \\bar{X} - ME = -32.5 - 5.1893 = -37.6893$\n- Upper Endpoint: $U = \\bar{X} + ME = -32.5 + 5.1893 = -27.3107$\n\nThe problem requires rounding the endpoints to four significant figures.\n- Rounded Lower Endpoint: $-37.69$\n- Rounded Upper Endpoint: $-27.31$\n\nThe $95\\%$ confidence interval for the mean change in low-density lipoprotein cholesterol is approximately $(-37.69, -27.31) \\ \\mathrm{mg/dL}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix} -37.69 & -27.31 \\end{pmatrix}\n}\n$$", "id": "4988052"}, {"introduction": "Parametric methods, like the t-interval, are powerful but rely on specific assumptions about the underlying data distribution. This practice introduces the nonparametric bootstrap, a computationally intensive yet highly flexible alternative that circumvents strong distributional assumptions. By resampling directly from the observed data, you will empirically approximate the sampling distribution of the mean and construct a percentile confidence interval, gaining hands-on experience with a modern, robust inferential tool that is essential for today's data analysis challenges [@problem_id:4988031].", "problem": "Consider a medical study in which a continuous biomarker concentration is measured in nanograms per milliliter (ng/mL). In the frequentist framework, a parameter is defined as a functional of the true data-generating distribution. Let $X$ denote a biomarker measurement, let $F$ denote the unknown distribution of $X$ in the target clinical population, and let the estimand be the population mean $\\theta = \\mathbb{E}_F[X]$. You observe $n$ independent and identically distributed (IID) realizations $\\{x_1,\\dots,x_n\\}$ from $F$ and use the sample mean $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n x_i$ as the estimator of $\\theta$. The sampling distribution of $\\bar{X}$ is unknown because $F$ is unknown.\n\nDefine the nonparametric bootstrap: replace $F$ by the empirical distribution $\\hat{F}_n$ that places mass $1/n$ at each observed $x_i$, and approximate the sampling distribution of an estimator by repeatedly resampling with replacement from $\\{x_1,\\dots,x_n\\}$ and recomputing the estimator on each resample. Describe how to obtain a percentile confidence interval for $\\theta$ by using the empirical quantiles of the bootstrap sampling distribution of $\\bar{X}$.\n\nYour task is to implement a program that, given observed biomarker data, a number of bootstrap replicates $B$, and a confidence level $(1-\\alpha)$ (with $\\alpha$ specified as a decimal), computes the nonparametric bootstrap percentile confidence interval for the mean biomarker level $\\theta$ via resampling with replacement from the observed data. For each test case below, generate $B$ bootstrap samples of size $n$ by sampling with replacement from the observed values, compute the bootstrap sample mean for each resample, and return the lower and upper endpoints given by the empirical quantiles at levels $\\alpha/2$ and $1 - \\alpha/2$, respectively. Express all biomarker concentrations and confidence interval endpoints in ng/mL, rounded to three decimal places. Use the provided random seeds to ensure reproducibility.\n\nTest suite (each test case is a tuple of $(\\text{data}, B, \\alpha, \\text{seed})$):\n- Case $1$: data $[2.1, 3.4, 3.0, 2.7, 4.2, 5.1, 3.8, 2.9, 3.2, 4.0]$, $B = 5000$, $\\alpha = 0.05$, seed $= 11$.\n- Case $2$: data $[2.4, 2.5, 2.6, 2.7, 2.8]$, $B = 10000$, $\\alpha = 0.10$, seed $= 123$.\n- Case $3$: data $[3.0, 3.0, 3.0, 3.0, 3.0]$, $B = 3000$, $\\alpha = 0.05$, seed $= 7$.\n- Case $4$: data $[2.0, 2.1, 2.2, 2.3, 10.0, 2.4, 2.5]$, $B = 15000$, $\\alpha = 0.05$, seed $= 2023$.\n\nDesign for coverage rationale:\n- Case $1$ exercises a typical moderate sample with moderate variability.\n- Case $2$ varies $\\alpha$ to test a different confidence level and uses a small sample size.\n- Case $3$ is a degenerate case in which all observations are identical, testing the boundary where the bootstrap distribution is a point mass.\n- Case $4$ includes a notable outlier to assess robustness of the percentile interval under skew and heavy tails.\n\nFinal output format specification:\nYour program should produce a single line of output containing the list of confidence intervals for all test cases, as a comma-separated list of two-element lists (each interval is $[\\text{lower}, \\text{upper}]$) enclosed in square brackets. For example, the output should look like $[[l_1,u_1],[l_2,u_2],[l_3,u_3],[l_4,u_4]]$ with all numerical entries rounded to three decimal places. Do not print units in the output; interpret all numerical values as ng/mL.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the established theory of nonparametric bootstrapping, a fundamental technique in frequentist statistics. It is well-posed, providing all necessary inputs—data, parameters, and random seeds—to ensure a unique, reproducible result. The problem is objective, using precise statistical terminology without ambiguity.\n\nThe objective is to compute a $(1-\\alpha)$ nonparametric bootstrap percentile confidence interval for the population mean, $\\theta = \\mathbb{E}_F[X]$, of a continuous biomarker. We are given a set of $n$ independent and identically distributed (IID) observations $\\{x_1, \\dots, x_n\\}$ from the unknown true distribution $F$.\n\nThe core principle of the bootstrap is to approximate the sampling distribution of an estimator by resampling from the observed data. The true distribution $F$ is unknown, so we use its best available estimate, the empirical distribution function $\\hat{F}_n$. The distribution $\\hat{F}_n$ is a discrete distribution that assigns a probability mass of $1/n$ to each observed data point $x_i, i = 1, \\dots, n$.\n\nThe procedure for constructing a bootstrap percentile confidence interval is as follows:\n\n1.  **Bootstrap Resampling**: We generate a large number, $B$, of \"bootstrap samples\". Each bootstrap sample, denoted $\\{x_1^*, \\dots, x_n^*\\}$, is created by drawing $n$ observations with replacement from the original data set $\\{x_1, \\dots, x_n\\}$. This process is equivalent to drawing an IID sample of size $n$ from the empirical distribution $\\hat{F}_n$.\n\n2.  **Bootstrap Replicates of the Estimator**: For each of the $B$ bootstrap samples, we compute the statistic of interest. In this case, the statistic is the sample mean. Let the $b$-th bootstrap sample be $\\{x_{b,1}^*, \\dots, x_{b,n}^*\\}$ for $b=1, \\dots, B$. The corresponding bootstrap replicate of the sample mean is:\n    $$\n    \\bar{X}_b^* = \\frac{1}{n} \\sum_{i=1}^n x_{b,i}^*\n    $$\n    This process yields a collection of $B$ bootstrap means, $\\{\\bar{X}_1^*, \\bar{X}_2^*, \\dots, \\bar{X}_B^*\\}$. This collection serves as an empirical approximation to the sampling distribution of the sample mean estimator $\\bar{X}$.\n\n3.  **Percentile Confidence Interval Construction**: The percentile method constructs the confidence interval directly from the quantiles of the empirical distribution of the bootstrap replicates. For a $(1-\\alpha)$ confidence level, the confidence interval is defined by the $\\alpha/2$ and $1 - \\alpha/2$ quantiles of the sorted bootstrap means. Let $\\bar{X}_{(j)}^*$ denote the $j$-th ordered value in the collection of bootstrap means. The $(1-\\alpha)$ percentile confidence interval for $\\theta$ is given by:\n    $$\n    \\left[ \\text{Quantile}_{\\bar{X}^*}\\left(\\frac{\\alpha}{2}\\right), \\text{Quantile}_{\\bar{X}^*}\\left(1 - \\frac{\\alpha}{2}\\right) \\right]\n    $$\n    In practice, to find these quantiles from the $B$ sorted replicates, we find the values at ranks corresponding to the desired probabilities. For instance, the lower bound is the value at approximately rank $(\\alpha/2) \\times B$ and the upper bound is the value at approximately rank $(1 - \\alpha/2) \\times B$. Standard statistical software uses interpolation methods to provide more accurate quantile estimates for finite $B$.\n\nThe implementation will follow this algorithm for each test case. A separate random number generator will be initialized for each case using the specified seed to ensure reproducibility. The calculations will be vectorized for efficiency. For each case, we will generate $B \\times n$ random choices from the data to form $B$ samples of size $n$. We then compute the mean of each of the $B$ samples. Finally, we determine the lower and upper quantiles of this distribution of means to form the confidence interval, rounding the endpoints to three decimal places as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes nonparametric bootstrap percentile confidence intervals for the mean\n    for a series of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple of (data, B, alpha, seed).\n    test_cases = [\n        ([2.1, 3.4, 3.0, 2.7, 4.2, 5.1, 3.8, 2.9, 3.2, 4.0], 5000, 0.05, 11),\n        ([2.4, 2.5, 2.6, 2.7, 2.8], 10000, 0.10, 123),\n        ([3.0, 3.0, 3.0, 3.0, 3.0], 3000, 0.05, 7),\n        ([2.0, 2.1, 2.2, 2.3, 10.0, 2.4, 2.5], 15000, 0.05, 2023)\n    ]\n\n    results = []\n    for data, B, alpha, seed in test_cases:\n        # Convert data to a numpy array for efficient computation.\n        data_array = np.array(data)\n        n = len(data_array)\n\n        # Initialize a random number generator with the specified seed for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Generate B bootstrap samples of size n.\n        # This creates a (B, n) array where each row is a bootstrap sample.\n        bootstrap_samples = rng.choice(data_array, size=(B, n), replace=True)\n\n        # Compute the mean for each of the B bootstrap samples along the appropriate axis.\n        bootstrap_means = np.mean(bootstrap_samples, axis=1)\n\n        # Determine the quantile levels for the confidence interval.\n        q_lower = alpha / 2\n        q_upper = 1 - alpha / 2\n\n        # Calculate the lower and upper bounds of the confidence interval\n        # using the empirical quantiles of the bootstrap means.\n        ci_lower = np.quantile(bootstrap_means, q_lower)\n        ci_upper = np.quantile(bootstrap_means, q_upper)\n\n        # Round the endpoints to three decimal places as required.\n        interval = [round(ci_lower, 3), round(ci_upper, 3)]\n        results.append(interval)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists is converted to a string\n    # and spaces are removed to match the specified output format '[[l1,u1],[l2,u2],...]'.\n    print(str(results).replace(' ', ''))\n\nsolve()\n```", "id": "4988031"}]}