{"hands_on_practices": [{"introduction": "Mastering the hypothesis testing framework begins with a firm grasp of its fundamental trade-offs. This exercise provides a concrete scenario to calculate the probabilities of making Type I and Type II errors, denoted by $\\alpha$ and $\\beta$ respectively. By working through this clinical trial example, you will directly compute the power ($1-\\beta$) of a test and assess its adequacy, translating abstract statistical concepts into the practical language of risk and diagnostic accuracy in medical research [@problem_id:4989124].", "problem": "A clinical trial in hypertension targets detection of a clinically meaningful reduction in population mean systolic blood pressure of $5$ millimeters of mercury (mmHg). Assume individual systolic blood pressure measurements after treatment are independent and identically distributed as a normal random variable with known standard deviation $\\sigma = 12$ mmHg and unknown mean $\\mu$. Baseline mean systolic blood pressure prior to treatment is $\\mu_{0} = 140$ mmHg. A decision rule is specified prospectively: declare a mean reduction as detected and reject the null hypothesis if the sample mean $\\bar{X}$ from a trial with sample size $n = 64$ satisfies $\\bar{X} \\le \\mu_{0} - 5$ mmHg.\n\nWithin the hypothesis testing framework, consider the simple null and alternative hypotheses $H_{0}: \\mu = \\mu_{0}$ and $H_{1}: \\mu = \\mu_{0} - 5$, while recognizing that the one-sided scientific alternative is $H_{1}: \\mu  \\mu_{0}$. Starting from fundamental distributional properties of the sample mean under normal sampling with known variance and the definitions of Type I and Type II errors, derive the following for the specified $n$ and decision rule:\n\n- The Type I error probability $\\alpha$ under $H_{0}$.\n- The Type II error probability $\\beta$ under the point alternative $\\mu = \\mu_{0} - 5$.\n- The power $1 - \\beta$ to detect a $5$ mmHg reduction.\n\nThen assess whether the procedure is adequate relative to conventional design targets $\\alpha^{\\star} = 0.05$ and power target $\\pi^{\\star} = 0.80$ for detecting a $5$ mmHg mean reduction.\n\nRound your final numerical answer (the achieved power) to four significant figures. Express the final answer as a decimal without a percent sign.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- Clinically meaningful reduction in population mean systolic blood pressure: $5$ mmHg.\n- Individual measurements are independent and identically distributed (i.i.d.) as a Normal random variable.\n- Known population standard deviation: $\\sigma = 12$ mmHg.\n- Unknown population mean: $\\mu$.\n- Baseline mean (null hypothesis): $\\mu_{0} = 140$ mmHg.\n- Sample size: $n = 64$.\n- Decision rule: Reject the null hypothesis if the sample mean $\\bar{X} \\le \\mu_{0} - 5$ mmHg.\n- Null hypothesis: $H_{0}: \\mu = \\mu_{0}$.\n- Simple alternative hypothesis: $H_{1}: \\mu = \\mu_{0} - 5$.\n- Conventional target Type I error: $\\alpha^{\\star} = 0.05$.\n- Conventional target power: $\\pi^{\\star} = 0.80$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard application of hypothesis testing concerning the mean of a normally distributed population with known variance.\n- **Scientifically Grounded:** The context and parameters are typical for a biostatistics problem modeling a clinical trial. The use of a normal distribution for blood pressure is a common and reasonable approximation. The values are realistic.\n- **Well-Posed:** All necessary information ($\\mu_0$, $\\sigma$, $n$, and a precise decision rule) is provided to calculate the required probabilities. The questions are unambiguous and lead to a unique solution.\n- **Objective:** The problem is stated in precise, quantitative terms, free from subjectivity.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically sound, well-posed, and internally consistent. A solution will be derived.\n\n### Derivation\nThe fundamental principle underlying this problem is the distribution of the sample mean, $\\bar{X}$. Given a random sample of size $n$ from a normal population with mean $\\mu$ and known standard deviation $\\sigma$, the sample mean $\\bar{X}$ is also normally distributed with mean $\\mu$ and standard deviation $\\frac{\\sigma}{\\sqrt{n}}$. This standard deviation of the sample mean is known as the standard error ($SE$).\nThus, the distribution of $\\bar{X}$ is specified as:\n$$ \\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) $$\nWith the given values, $n=64$ and $\\sigma=12$ mmHg, the standard error is:\n$$ SE = \\frac{\\sigma}{\\sqrt{n}} = \\frac{12}{\\sqrt{64}} = \\frac{12}{8} = 1.5 \\text{ mmHg} $$\n\nTo calculate probabilities, we standardize $\\bar{X}$ to the standard normal random variable $Z \\sim N(0, 1)$ using the transformation:\n$$ Z = \\frac{\\bar{X} - \\mu}{SE} = \\frac{\\bar{X} - \\mu}{1.5} $$\n\n**1. Type I Error Probability ($\\alpha$)**\nThe Type I error is the probability of rejecting the null hypothesis $H_0$ when it is actually true.\n$$ \\alpha = P(\\text{Reject } H_0 \\mid H_0 \\text{ is true}) $$\nThe decision rule is to reject $H_0$ if $\\bar{X} \\le \\mu_0 - 5$. The condition \"$H_0$ is true\" implies that the true population mean is $\\mu = \\mu_0$.\n$$ \\alpha = P(\\bar{X} \\le \\mu_0 - 5 \\mid \\mu = \\mu_0) $$\nWe standardize the inequality by subtracting the mean under $H_0$ (which is $\\mu_0$) and dividing by the standard error:\n$$ \\alpha = P\\left(\\frac{\\bar{X} - \\mu_0}{SE} \\le \\frac{(\\mu_0 - 5) - \\mu_0}{SE}\\right) $$\n$$ \\alpha = P\\left(Z \\le \\frac{-5}{1.5}\\right) = P(Z \\le -3.333...) = P(Z \\le -10/3) $$\nLet $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution. Then, $\\alpha = \\Phi(-10/3)$. The value is approximately $\\alpha \\approx 0.000429$.\n\n**2. Type II Error Probability ($\\beta$)**\nThe Type II error is the probability of failing to reject the null hypothesis $H_0$ when the alternative hypothesis $H_1$ is true.\n$$ \\beta = P(\\text{Fail to reject } H_0 \\mid H_1 \\text{ is true}) $$\n\"Failing to reject $H_0$\" is the complement of the rejection rule, so it corresponds to the event $\\bar{X}  \\mu_0 - 5$. The simple alternative hypothesis $H_1$ specifies that the true population mean is $\\mu = \\mu_0 - 5$. Let's denote this alternative mean as $\\mu_1 = \\mu_0 - 5$.\n$$ \\beta = P(\\bar{X}  \\mu_0 - 5 \\mid \\mu = \\mu_1) $$\nSubstituting $\\mu_1 = \\mu_0 - 5$, we get:\n$$ \\beta = P(\\bar{X}  \\mu_1 \\mid \\mu = \\mu_1) $$\nUnder $H_1$, the sampling distribution of $\\bar{X}$ is centered at $\\mu_1$. We standardize the inequality by subtracting the mean under $H_1$ (which is $\\mu_1$) and dividing by the standard error:\n$$ \\beta = P\\left(\\frac{\\bar{X} - \\mu_1}{SE}  \\frac{\\mu_1 - \\mu_1}{SE}\\right) $$\n$$ \\beta = P(Z  0) $$\nFor the standard normal distribution, which is symmetric about $0$, the probability of a value being greater than the mean is exactly $0.5$.\n$$ \\beta = 0.5 $$\n\n**3. Power to Detect a $5$ mmHg Reduction ($1-\\beta$)**\nThe power of a test is the probability of correctly rejecting a false null hypothesis. It is defined as $1 - \\beta$.\n$$ \\text{Power} = 1 - \\beta = 1 - 0.5 = 0.5 $$\n\n**4. Assessment of Adequacy**\nWe compare the calculated test characteristics to the conventional design targets.\n- **Type I Error Comparison:** The calculated Type I error is $\\alpha \\approx 0.000429$. This is much smaller than the conventional target of $\\alpha^{\\star} = 0.05$. The test is therefore very conservative, meaning it is highly unlikely to produce a false positive.\n- **Power Comparison:** The calculated power is $1 - \\beta = 0.5$. This is substantially lower than the conventional target power of $\\pi^{\\star} = 0.80$. This indicates that the study, as designed with this specific decision rule, has only a $50\\%$ chance of detecting the specified $5$ mmHg mean reduction in blood pressure.\n\n**Conclusion on Adequacy:** The procedure is not adequate. While its control of Type I error is exceptionally stringent, it is severely underpowered relative to conventional standards. The low power means the study has a high risk of failing to detect a true effect of the specified magnitude (a Type II error), rendering its results potentially inconclusive even if the treatment is effective. The specific choice of the critical value at $\\mu_0 - 5$, which is precisely the mean under the alternative, leads directly to this power of $0.5$. A properly designed trial would set a critical value based on the desired $\\alpha$ level (e.g., $\\alpha=0.05$), which would typically yield a higher power for the given sample size.\n\nThe problem asks for the calculated power, rounded to four significant figures.\nPower $= 0.5$. To four significant figures, this is $0.5000$.", "answer": "$$\\boxed{0.5000}$$", "id": "4989124"}, {"introduction": "Moving beyond tests that rely on distributional assumptions like normality, this practice explores the elegant and powerful method of permutation testing. By implementing a permutation test from first principles, you will see how statistical inference can be based directly on the physical act of randomization in an experiment. This exercise reinforces the meaning of the sharp null hypothesis and demonstrates how to construct an exact reference distribution to calculate a p-value without resorting to parametric theory [@problem_id:4989003].", "problem": "Consider a completely randomized two-arm clinical experiment within statistical modeling in medicine, with a fixed number of subjects $n$, among which $n_1$ subjects are assigned to treatment and $n_0 = n - n_1$ subjects are assigned to control. Let the observed outcome vector be $\\mathbf{y} = (y_1,\\dots,y_n)$ and the observed assignment vector be $\\mathbf{z} = (z_1,\\dots,z_n)$ where $z_i \\in \\{0,1\\}$ indicates control ($0$) or treatment ($1$) for subject $i$. Assume complete randomization: conditional on $n_1$, all assignments with exactly $n_1$ treated are equally likely. The null hypothesis is the sharp null of no treatment effect, $H_0: y_i(1) = y_i(0)$ for all $i$, so that under $H_0$ the observed outcomes are invariant to label permutations. Define the test statistic as the difference in sample means,\n$$\nT(\\mathbf{z}, \\mathbf{y}) = \\bar{y}_{\\text{treat}} - \\bar{y}_{\\text{control}} = \\frac{1}{n_1}\\sum_{i:z_i=1} y_i - \\frac{1}{n_0}\\sum_{i:z_i=0} y_i.\n$$\nUnder complete randomization and $H_0$, the reference distribution of $T(\\mathbf{z}, \\mathbf{y})$ is obtained by enumerating all possible label permutations $\\mathbf{z}'$ with exactly $n_1$ ones and computing $T(\\mathbf{z}', \\mathbf{y})$ for each. The two-sided permutation $p$-value is defined as the probability, under the uniform distribution over assignments with $n_1$ treated, that the absolute test statistic is at least as extreme as the observed statistic,\n$$\np = \\Pr\\left( \\left|T(\\mathbf{Z}, \\mathbf{y})\\right| \\ge \\left|T(\\mathbf{z}, \\mathbf{y})\\right| \\,\\middle|\\, \\sum_i Z_i = n_1 \\right),\n$$\nwhich is computed exactly by counting the fraction of enumerated assignments with $\\left|T(\\mathbf{z}', \\mathbf{y})\\right| \\ge \\left|T(\\mathbf{z}, \\mathbf{y})\\right|$.\n\nYour task is to write a complete, runnable program that:\n- Implements the exact two-sided permutation test for $T(\\mathbf{z}, \\mathbf{y})$ by enumerating all assignments with exactly $n_1$ treated subjects and computing the exact $p$-value as described above.\n- Uses the significance level $\\alpha$ to make a decision to reject $H_0$ if $p \\le \\alpha$.\n\nDesign and implement your solution from first principles of the hypothesis testing framework, starting from the definition of complete randomization, the sharp null hypothesis, and the construction of the randomization distribution. Do not use asymptotic approximations or parametric distributional assumptions.\n\nTest Suite:\nUse the following four test cases to validate your program. For each case, the outcome vector $\\mathbf{y}$ is dimensionless, the assignment vector $\\mathbf{z}$ has exactly $n_1$ ones, and the significance level is $\\alpha = 0.05$.\n\n1. Happy path, balanced groups:\n   - $\\mathbf{y} = [8.1, 7.9, 9.5, 6.7, 7.2, 12.3, 11.8, 10.9, 13.1, 9.7]$\n   - $\\mathbf{z} = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]$ (so $n = 10$, $n_1 = 5$)\n   - $\\alpha = 0.05$\n\n2. Boundary case, zero observed difference:\n   - $\\mathbf{y} = [10.0, 11.0, 12.0, 10.0, 11.0, 12.0]$\n   - $\\mathbf{z} = [1, 1, 1, 0, 0, 0]$ (so $n = 6$, $n_1 = 3$)\n   - $\\alpha = 0.05$\n\n3. Unbalanced groups with pronounced separation:\n   - $\\mathbf{y} = [5.2, 4.8, 5.0, 5.1, 4.9, 9.0, 8.8, 9.2, 8.9]$\n   - $\\mathbf{z} = [0, 0, 0, 0, 0, 1, 1, 1, 0]$ (so $n = 9$, $n_1 = 3$)\n   - $\\alpha = 0.05$\n\n4. Extreme assignment with ties and repeated values:\n   - $\\mathbf{y} = [8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0]$\n   - $\\mathbf{z} = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]$ (so $n = 12$, $n_1 = 6$)\n   - $\\alpha = 0.05$\n\nRequired Final Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case should contribute a two-element list $[p, d]$ where $p$ is the exact two-sided permutation $p$-value rounded to six decimal places and $d$ is a boolean indicating whether $H_0$ is rejected at level $\\alpha$. For example, the output should look like\n$$\n[[p_1, d_1],[p_2, d_2],[p_3, d_3],[p_4, d_4]]\n$$\nprinted as a single line.", "solution": "The problem statement is critically validated and found to be valid. It is scientifically grounded, well-posed, and objective. It presents a standard, well-defined task in statistical inference—the implementation of Fisher's exact permutation test for a difference in means under the sharp null hypothesis of no effect. All necessary data and definitions are provided, and the computational task is feasible for the given parameters.\n\nThe solution is predicated on the fundamental principles of the hypothesis testing framework within the context of a randomized experiment.\n\n**1. The Sharp Null Hypothesis ($H_0$)**\n\nThe cornerstone of this test is the sharp null hypothesis, which posits that the treatment has absolutely no effect on any individual. Formally, for each subject $i$, their potential outcome under treatment, $y_i(1)$, is identical to their potential outcome under control, $y_i(0)$.\n$$\nH_0: y_i(1) = y_i(0) \\quad \\text{for all } i=1, \\dots, n\n$$\nA profound consequence of this hypothesis is that the observed outcome $y_i$ for any subject is invariant to the treatment they were assigned. That is, $y_i = z_i y_i(1) + (1-z_i)y_i(0)$. Under $H_0$, this simplifies to $y_i = z_i y_i(0) + (1-z_i)y_i(0) = y_i(0)$. This implies that the entire vector of observed outcomes $\\mathbf{y}$ is fixed, regardless of the assignment vector $\\mathbf{z}$. The only source of randomness in the experiment, under $H_0$, is the assignment mechanism itself.\n\n**2. Complete Randomization and the Reference Distribution**\n\nThe problem specifies complete randomization, meaning that for a fixed number of treated subjects, $n_1$, every possible assignment vector $\\mathbf{z}'$ containing exactly $n_1$ ones is equally likely. The total number of such assignments is given by the binomial coefficient $\\binom{n}{n_1}$. The probability of observing any specific assignment is $1/\\binom{n}{n_1}$.\n\nUnder the sharp null hypothesis, we can construct the exact distribution of any test statistic by considering its value under every possible randomization. For the given test statistic $T(\\mathbf{z}, \\mathbf{y})$, we can compute its value for all $\\binom{n}{n_1}$ possible assignment vectors $\\mathbf{z}'$, while holding the outcome vector $\\mathbf{y}$ constant. The set of these computed values, $\\{T(\\mathbf{z}', \\mathbf{y})\\}_{\\mathbf{z}'}$, forms the exact reference distribution for the test.\n\n**3. The Test Statistic**\n\nThe test statistic is defined as the difference in sample means between the treatment and control groups:\n$$\nT(\\mathbf{z}, \\mathbf{y}) = \\bar{y}_{\\text{treat}} - \\bar{y}_{\\text{control}} = \\frac{\\sum_{i=1}^n z_i y_i}{\\sum_{i=1}^n z_i} - \\frac{\\sum_{i=1}^n (1-z_i) y_i}{\\sum_{i=1}^n (1-z_i)} = \\frac{1}{n_1}\\sum_{i:z_i=1} y_i - \\frac{1}{n_0}\\sum_{i:z_i=0} y_i\n$$\nwhere $n_1$ is the number of treated subjects and $n_0 = n - n_1$ is the number of control subjects. We first compute the observed value of this statistic, $T_{obs} = T(\\mathbf{z}, \\mathbf{y})$, using the actual assignment vector $\\mathbf{z}$ from the experiment.\n\n**4. The Two-Sided $p$-Value and Decision Rule**\n\nThe two-sided $p$-value measures the probability of observing a test statistic at least as extreme as the one actually observed, assuming the null hypothesis is true. Extremeness is measured by the absolute value of the statistic. The $p$-value is the proportion of possible assignments $\\mathbf{z}'$ that would lead to a test statistic $T(\\mathbf{z}', \\mathbf{y})$ whose absolute value is greater than or equal to the absolute value of the observed statistic, $|T_{obs}|$.\n$$\np = \\frac{\\text{Number of assignments } \\mathbf{z}' \\text{ such that } |T(\\mathbf{z}', \\mathbf{y})| \\ge |T_{obs}|}{\\text{Total number of assignments}} = \\frac{\\left| \\left\\{ \\mathbf{z}' \\,:\\, |\\mathbf{z}'|_1=n_1, |T(\\mathbf{z}', \\mathbf{y})| \\ge |T_{obs}| \\right\\} \\right|}{\\binom{n}{n_1}}\n$$\nTo account for potential floating-point inaccuracies in computation, the comparison $|T(\\mathbf{z}', \\mathbf{y})| \\ge |T_{obs}|$ is implemented robustly.\n\nFinally, a decision is made by comparing the calculated $p$-value to a pre-specified significance level, $\\alpha$. If $p \\le \\alpha$, we reject the null hypothesis $H_0$, concluding there is statistically significant evidence of a treatment effect. Otherwise, we fail to reject $H_0$.\n\n**Algorithmic Steps:**\n\nFor each test case $(\\mathbf{y}, \\mathbf{z}, \\alpha)$:\n1.  Identify the total number of subjects $n$, the number of treated subjects $n_1 = \\sum z_i$, and the number of control subjects $n_0 = n - n_1$.\n2.  Calculate the observed test statistic $T_{obs}$ using the given vectors $\\mathbf{y}$ and $\\mathbf{z}$.\n3.  Generate all unique combinations of $n_1$ indices from the set $\\{0, 1, \\dots, n-1\\}$. Each combination represents the set of indices for subjects in a hypothetical treatment group.\n4.  Initialize a counter for extreme statistics to $0$.\n5.  Iterate through each combination of treatment indices:\n    a. Construct the hypothetical treatment and control groups from the fixed vector $\\mathbf{y}$.\n    b. Calculate the corresponding test statistic for this permutation, $T_{perm}$.\n    c. If $|T_{perm}| \\ge |T_{obs}|$, increment the counter.\n6.  The total number of permutations is $N_{total} = \\binom{n}{n_1}$.\n7.  The exact $p$-value is the final count divided by $N_{total}$.\n8.  The decision $d$ is the boolean result of the comparison $p \\le \\alpha$.\n9.  The result for the test case is the pair $[p, d]$, with $p$ rounded to six decimal places.\n\nThis procedure provides an exact, non-parametric test of the sharp null hypothesis, relying solely on the randomization of assignments as the basis for inference.", "answer": "```python\nimport numpy as np\nfrom itertools import combinations\nfrom scipy.special import comb\n\ndef solve():\n    \"\"\"\n    Solves the permutation test problem for the given test suite.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1: Happy path, balanced groups\n        {\n            \"y\": np.array([8.1, 7.9, 9.5, 6.7, 7.2, 12.3, 11.8, 10.9, 13.1, 9.7]),\n            \"z\": np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1]),\n            \"alpha\": 0.05\n        },\n        # Case 2: Boundary case, zero observed difference\n        {\n            \"y\": np.array([10.0, 11.0, 12.0, 10.0, 11.0, 12.0]),\n            \"z\": np.array([1, 1, 1, 0, 0, 0]),\n            \"alpha\": 0.05\n        },\n        # Case 3: Unbalanced groups with pronounced separation\n        {\n            \"y\": np.array([5.2, 4.8, 5.0, 5.1, 4.9, 9.0, 8.8, 9.2, 8.9]),\n            \"z\": np.array([0, 0, 0, 0, 0, 1, 1, 1, 0]),\n            \"alpha\": 0.05\n        },\n        # Case 4: Extreme assignment with ties and repeated values\n        {\n            \"y\": np.array([8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0]),\n            \"z\": np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]),\n            \"alpha\": 0.05\n        }\n    ]\n\n    results = []\n    \n    # Small tolerance for floating-point comparisons\n    TOLERANCE = 1e-9\n\n    def calculate_test_statistic(y_outcomes, treat_indices, n_total, n_treat):\n        \"\"\"\n        Calculates the difference in means statistic for a given treatment assignment.\n        \"\"\"\n        n_control = n_total - n_treat\n        if n_treat == 0 or n_control == 0:\n            return 0.0 # Or handle as an error, though not expected here.\n\n        all_indices = set(range(n_total))\n        control_indices = all_indices - set(treat_indices)\n\n        y_treat = y_outcomes[list(treat_indices)]\n        y_control = y_outcomes[list(control_indices)]\n\n        mean_treat = np.mean(y_treat)\n        mean_control = np.mean(y_control)\n\n        return mean_treat - mean_control\n\n    for case in test_cases:\n        y, z, alpha = case[\"y\"], case[\"z\"], case[\"alpha\"]\n\n        n = len(y)\n        n_1 = int(np.sum(z))\n        \n        # Step 1: Calculate the observed test statistic\n        observed_treat_indices = np.where(z == 1)[0]\n        t_obs = calculate_test_statistic(y, observed_treat_indices, n, n_1)\n        abs_t_obs = np.abs(t_obs)\n\n        # Step 2: Enumerate all possible assignments and build the reference distribution\n        all_possible_indices = range(n)\n        \n        # Generate all combinations of n_1 indices for the treatment group\n        all_treat_assignments = combinations(all_possible_indices, n_1)\n\n        num_permutations = comb(n, n_1, exact=True)\n        extreme_count = 0\n\n        for permuted_treat_indices in all_treat_assignments:\n            t_perm = calculate_test_statistic(y, permuted_treat_indices, n, n_1)\n            \n            # Step 3: Compare each permuted statistic to the observed one\n            # The comparison includes a tolerance for floating point arithmetic\n            if np.abs(t_perm) >= abs_t_obs - TOLERANCE:\n                extreme_count += 1\n        \n        # Step 4: Calculate the p-value\n        if num_permutations == 0:\n            p_value = 1.0 # Or handle as an undefined case\n        else:\n            p_value = extreme_count / num_permutations\n\n        # Step 5: Make a decision\n        decision = p_value = alpha\n        \n        # Format the result according to the problem specification\n        results.append(f\"[{p_value:.6f}, {str(decision).lower()}]\")\n        \n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "4989003"}, {"introduction": "A statistically significant result does not always imply a practically meaningful one, a critical distinction in the age of \"big data\". This hands-on problem challenges you to explore the relationship between statistical significance (the p-value) and practical importance (the effect size). By analyzing gene expression data with massive sample sizes, you will see how a trivially small effect can become highly significant, learning to critically evaluate results beyond the simple threshold of a p-value [@problem_id:2398939].", "problem": "You are analyzing a single gene’s expression under two experimental conditions, denoted as condition $A$ and condition $B$. Measurements are on a positive scale and are to be treated as independent and identically distributed (i.i.d.) within each condition, drawn from normal distributions with finite, possibly unequal variances. Let the population means be $\\mu_A$ and $\\mu_B$, respectively. For each condition you are given the sample size, sample mean, and sample standard deviation. You must assess the null hypothesis $H_0: \\mu_A = \\mu_B$ against the two-sided alternative $H_1: \\mu_A \\neq \\mu_B$ using the exact two-sided $p$-value under this model. Separately, define the fold-change as $\\mathrm{FC} = \\bar{x}_B / \\bar{x}_A$, where $\\bar{x}_A$ and $\\bar{x}_B$ are the sample means for conditions $A$ and $B$.\n\nUse the following thresholds:\n- Significance threshold $\\alpha = 10^{-12}$.\n- Biological negligibility tolerance $\\tau = 10^{-2}$ (that is, a deviation from $1$ not exceeding $0.01$ is considered biologically negligible).\n\nFor each test case below, decide whether both of the following criteria hold simultaneously: (i) the two-sided $p$-value is strictly less than $\\alpha$, and (ii) the fold-change is biologically negligible, meaning $\\lvert \\mathrm{FC} - 1 \\rvert \\le \\tau$. Report a boolean for each test case: $\\,\\text{True}\\,$ if both criteria hold, and $\\,\\text{False}\\,$ otherwise.\n\nTest suite (each case is $(n_A, n_B, \\bar{x}_A, \\bar{x}_B, s_A, s_B)$):\n- Case $1$: $(200000, 200000, 100.0, 100.14, 1.0, 1.0)$.\n- Case $2$: $(50000, 50000, 50.0, 50.5, 2.0, 2.0)$.\n- Case $3$: $(6, 6, 100.0, 100.3, 1.5, 1.5)$.\n- Case $4$: $(1000, 1000, 100.0, 100.0, 5.0, 5.0)$.\n- Case $5$: $(100000, 100000, 80.0, 81.6, 1.5, 1.5)$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\,\\text{True},\\text{False},\\text{True}\\,]$), ordered as Cases $1$ through $5$.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Experimental setup**: Two conditions, $A$ and $B$.\n- **Data per condition**: Measurements are independent and identically distributed (i.i.d.) from normal distributions with population means $\\mu_A$ and $\\mu_B$, and finite, possibly unequal variances.\n- **Provided statistics for each case**: Sample size for condition $A$ ($n_A$), sample size for condition $B$ ($n_B$), sample mean for condition $A$ ($\\bar{x}_A$), sample mean for condition $B$ ($\\bar{x}_B$), sample standard deviation for condition $A$ ($s_A$), and sample standard deviation for condition $B$ ($s_B$).\n- **Hypothesis test**: Null hypothesis $H_0: \\mu_A = \\mu_B$ against the two-sided alternative $H_1: \\mu_A \\neq \\mu_B$.\n- **Fold-change definition**: $\\mathrm{FC} = \\bar{x}_B / \\bar{x}_A$.\n- **Thresholds**:\n    - Significance threshold: $\\alpha = 10^{-12}$.\n    - Biological negligibility tolerance: $\\tau = 10^{-2}$.\n- **Decision criteria**: Determine if both of the following conditions are met:\n    1. The two-sided $p$-value is strictly less than $\\alpha$.\n    2. The fold-change is biologically negligible, defined as $\\lvert \\mathrm{FC} - 1 \\rvert \\le \\tau$.\n- **Test suite**:\n    - Case $1$: $(200000, 200000, 100.0, 100.14, 1.0, 1.0)$.\n    - Case $2$: $(50000, 50000, 50.0, 50.5, 2.0, 2.0)$.\n    - Case $3$: $(6, 6, 100.0, 100.3, 1.5, 1.5)$.\n    - Case $4$: $(1000, 1000, 100.0, 100.0, 5.0, 5.0)$.\n    - Case $5$: $(100000, 100000, 80.0, 81.6, 1.5, 1.5)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It presents a standard problem in biostatistics: comparing the means of two independent groups.\n- **Scientific Grounding**: The problem is based on the established statistical framework of hypothesis testing. The use of a t-test for comparing means of normally distributed populations is a fundamental concept. The problem correctly specifies that variances may be unequal, which requires the use of Welch's t-test, a standard and robust procedure.\n- **Well-Posedness**: All necessary parameters ($n_A, n_B, \\bar{x}_A, \\bar{x}_B, s_A, s_B$) are provided for each case. The decision criteria are clear and unambiguous. A unique, stable solution exists for each test case.\n- **Objectivity**: The problem is stated in precise mathematical and statistical language without subjective claims or ambiguity.\n\nThe problem does not exhibit any of the flaws listed in the invalidity criteria. It is a formalizable, complete, and realistic statistical task.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe task is to evaluate, for each test case, whether two distinct criteria are met simultaneously. The first criterion is related to statistical significance, and the second to practical or biological significance.\n\n#### Criterion 1: Statistical Significance ($p$-value)\nWe must test the null hypothesis $H_0: \\mu_A = \\mu_B$ against the two-sided alternative $H_1: \\mu_A \\neq \\mu_B$. The problem states that the underlying populations are normal but their variances might be unequal. This is the Behrens-Fisher problem. The appropriate statistical test is Welch's t-test, which does not assume equality of variances.\n\nThe test statistic, $t$, is calculated as:\n$$\nt = \\frac{\\bar{x}_A - \\bar{x}_B}{\\sqrt{\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}}}\n$$\nwhere $\\bar{x}_A$ and $\\bar{x}_B$ are the sample means, $s_A^2$ and $s_B^2$ are the sample variances, and $n_A$ and $n_B$ are the sample sizes.\n\nUnder the null hypothesis, this statistic approximately follows a Student's t-distribution. The degrees of freedom, $\\nu$, are approximated using the Welch-Satterthwaite equation:\n$$\n\\nu \\approx \\frac{\\left(\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}\\right)^2}{\\frac{\\left(\\frac{s_A^2}{n_A}\\right)^2}{n_A - 1} + \\frac{\\left(\\frac{s_B^2}{n_B}\\right)^2}{n_B - 1}}\n$$\n\nThe two-sided $p$-value is the probability of observing a test statistic at least as extreme as the one computed, assuming the null hypothesis is true. This is given by:\n$$\np = 2 \\cdot \\mathbb{P}(T_\\nu \\ge |t|)\n$$\nwhere $T_\\nu$ is a random variable following the t-distribution with $\\nu$ degrees of freedom.\n\nThe first criterion is met if this $p$-value is strictly less than the given significance threshold $\\alpha = 10^{-12}$.\n$$\np  10^{-12}\n$$\n\n#### Criterion 2: Biological Negligibility (Fold-Change)\nThe fold-change, $\\mathrm{FC}$, is defined as the ratio of the sample means:\n$$\n\\mathrm{FC} = \\frac{\\bar{x}_B}{\\bar{x}_A}\n$$\nThe problem defines a change as biologically negligible if the absolute deviation of the fold-change from $1$ does not exceed a tolerance $\\tau = 10^{-2}$. Mathematically, this is:\n$$\n|\\mathrm{FC} - 1| \\le \\tau\n$$\nSubstituting the value of $\\tau$:\n$$\n\\left|\\frac{\\bar{x}_B}{\\bar{x}_A} - 1\\right| \\le 0.01\n$$\n\n#### Final Decision Logic\nFor each test case, we compute the $p$-value from Welch's t-test and the fold-change. We then evaluate the logical conjunction of the two conditions:\n$$\n(\\text{result is True}) \\iff (p  10^{-12}) \\land \\left( \\left|\\frac{\\bar{x}_B}{\\bar{x}_A} - 1\\right| \\le 0.01 \\right)\n$$\nThis evaluation will be performed for each of the five provided test cases.\n\nIt is important to note the potential for a small effect size (biologically negligible) to be highly statistically significant if the sample size is very large. The standard error of the difference, $\\sqrt{s_A^2/n_A + s_B^2/n_B}$, decreases as $n_A$ and $n_B$ increase, which can lead to a large magnitude for the $t$-statistic even for a very small difference in sample means $(\\bar{x}_A - \\bar{x}_B)$. This problem is designed to illustrate this fundamental concept in statistical inference.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t as t_dist\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing gene expression data under two conditions\n    using Welch's t-test and a fold-change criterion.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (n_A, n_B, x_bar_A, x_bar_B, s_A, s_B)\n    test_cases = [\n        (200000, 200000, 100.0, 100.14, 1.0, 1.0),   # Case 1\n        (50000, 50000, 50.0, 50.5, 2.0, 2.0),       # Case 2\n        (6, 6, 100.0, 100.3, 1.5, 1.5),             # Case 3\n        (1000, 1000, 100.0, 100.0, 5.0, 5.0),       # Case 4\n        (100000, 100000, 80.0, 81.6, 1.5, 1.5),     # Case 5\n    ]\n\n    # Define thresholds\n    alpha = 1e-12\n    tau = 1e-2\n\n    results = []\n    for case in test_cases:\n        n_A, n_B, x_bar_A, x_bar_B, s_A, s_B = case\n\n        # --- Criterion 2: Biological Negligibility ---\n        # Handle division by zero for x_bar_A, though not present in tests.\n        if x_bar_A == 0:\n            # If x_bar_A is 0, FC is undefined or infinite if x_bar_B is not 0.\n            # This would not be negligible. If both are 0, FC is ambiguous.\n            # We assume non-zero means as is typical in gene expression.\n            # Here we treat FC as non-negligible.\n            is_negligible = False\n        else:\n            fold_change = x_bar_B / x_bar_A\n            is_negligible = np.abs(fold_change - 1) = tau\n\n        # --- Criterion 1: Statistical Significance ---\n        # Welch's t-test for two independent samples.\n        var_A = s_A**2\n        var_B = s_B**2\n\n        # Handle edge case where difference is exactly zero\n        if x_bar_A == x_bar_B:\n            p_value = 1.0\n        else:\n            # Standard error of the difference\n            se_diff = np.sqrt(var_A / n_A + var_B / n_B)\n\n            # Welch's t-statistic\n            t_statistic = (x_bar_A - x_bar_B) / se_diff\n        \n            # Welch-Satterthwaite equation for degrees of freedom (nu)\n            term_A = (var_A / n_A)\n            term_B = (var_B / n_B)\n            \n            # Handle cases where sample size is too small (e.g., n=1)\n            # The tests have n > 1, so no division by zero here.\n            dof_numerator = (term_A + term_B)**2\n            dof_denominator = (term_A**2 / (n_A - 1)) + (term_B**2 / (n_B - 1))\n            \n            if dof_denominator == 0:\n                # This would only happen in pathological cases not relevant here\n                # (e.g., zero variance and n>1 which implies all points are same)\n                # Assign a large DoF as a fallback.\n                dof = n_A + n_B - 2\n            else:\n                dof = dof_numerator / dof_denominator\n\n            # Two-sided p-value from the t-distribution survival function (1-CDF)\n            # Using sf is more accurate for very small p-values (in the tail)\n            p_value = 2 * t_dist.sf(np.abs(t_statistic), df=dof)\n\n        is_significant = p_value  alpha\n\n        # --- Final Decision ---\n        # The result is True if and only if both criteria are met.\n        final_decision = is_significant and is_negligible\n        results.append(final_decision)\n\n    # Format the final output as specified\n    output_str = '[' + ','.join(str(res).lower() for res in results) + ']'\n    print(output_str)\n\n```", "id": "2398939"}]}