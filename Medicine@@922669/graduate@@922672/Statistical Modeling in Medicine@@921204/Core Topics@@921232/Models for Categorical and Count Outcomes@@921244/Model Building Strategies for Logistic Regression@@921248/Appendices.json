{"hands_on_practices": [{"introduction": "A critical step in model building is correctly specifying the relationship between a continuous predictor and the outcome. Assuming a simple linear effect on the log-odds scale is often biologically implausible and can lead to a poorly fitting model. This practice introduces the Multivariable Fractional Polynomials (MFP) procedure, a powerful and flexible strategy for identifying a parsimonious yet effective functional form for a continuous variable by searching through a predefined set of power transformations [@problem_id:4974006]. By implementing the MFP search algorithm, you will gain hands-on experience in moving beyond the restrictive linearity assumption to build more accurate predictive models.", "problem": "You are tasked with implementing a search procedure for Multivariable Fractional Polynomials (MFP) applied to a single continuous medical predictor, serum creatinine, in a binary outcome logistic regression. The goal is to select the best Fractional Polynomial (FP) transformation by the deviance criterion and present the resulting functional form quantifiably. The modeling context is advanced statistical modeling in medicine. You must produce a complete, runnable program that implements the following specification.\n\nFundamental base and definitions to use:\n- Logistic regression with binary outcomes: given observations $\\{(y_i, x_i)\\}_{i=1}^n$ where $y_i \\in \\{0,1\\}$ and $x_i \\in \\mathbb{R}_{0}$ represents serum creatinine measured in $\\mathrm{mg}/\\mathrm{dL}$, assume the model\n$$\\operatorname{logit}(\\pi_i) = \\eta_i = \\beta_0 + \\beta_1 f_1(x_i) + \\beta_2 f_2(x_i),$$\nwhere for degree $1$ models the coefficient $\\beta_2$ and the term $f_2(x_i)$ are omitted, $\\pi_i = \\Pr(y_i=1 \\mid x_i)$, and $\\operatorname{logit}(\\pi) = \\log\\left(\\frac{\\pi}{1-\\pi}\\right)$.\n- Maximum Likelihood Estimation (MLE): given a design matrix for a chosen transformation, estimate the coefficient vector $\\boldsymbol{\\beta}$ by maximizing the log-likelihood\n$$\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left(y_i \\eta_i - \\log\\left(1 + e^{\\eta_i}\\right)\\right),$$\nwith $\\eta_i$ as above.\n- Deviance: for a fitted model with MLE $\\hat{\\boldsymbol{\\beta}}$, compute the deviance\n$$D = -2\\,\\ell(\\hat{\\boldsymbol{\\beta}}).$$\nSelect the FP transformation that minimizes $D$.\n\nFractional Polynomial (FP) transformation family:\n- Candidate powers set\n$$\\mathcal{P} = \\{-2,-1,-0.5,0,0.5,1,2,3\\}.$$\n- Degree $1$ FP: for $p \\in \\mathcal{P}$,\n$$f_1(x;p) = \\begin{cases}\nx^p,  p \\neq 0, \\\\\n\\log(x),  p = 0.\n\\end{cases}$$\n- Degree $2$ FP: for $(p_1, p_2) \\in \\mathcal{P} \\times \\mathcal{P}$,\n$$\n(f_1, f_2) =\n\\begin{cases}\n\\left(x^{p_1}, x^{p_2}\\right),  p_1 \\neq 0, p_2 \\neq 0, p_1 \\neq p_2, \\\\\n\\left(\\log(x), x^{p_2}\\right),  p_1 = 0, p_2 \\neq 0, \\\\\n\\left(x^{p_1}, \\log(x)\\right),  p_1 \\neq 0, p_2 = 0, \\\\\n\\left(\\log(x), \\log(x)^2\\right),  p_1 = p_2 = 0, \\\\\n\\left(x^p, x^p \\log(x)\\right),  p_1 = p_2 = p \\neq 0.\n\\end{cases}\n$$\nTo improve numerical stability, center each transformation column before fitting: for any column $g(x_i)$ in the design, use the centered version $\\tilde{g}(x_i) = g(x_i) - \\frac{1}{n}\\sum_{j=1}^n g(x_j)$.\n\nYour program must:\n- For each test case, enumerate all candidate FP degree $1$ and degree $2$ transformations defined above, fit a logistic regression by MLE, compute the deviance $D$, and select the transformation that minimizes $D$.\n- Report the selected FP degree $d \\in \\{1,2\\}$, the selected powers $(p_1, p_2)$ where $p_2$ is set to the special floating value $\\mathrm{NaN}$ when $d=1$, and the minimized deviance $D$.\n\nTest suite specification (all random generation must be deterministic and reproducible using the stated seeds):\n- Test case $1$ (happy path, repeated power optimal): number of patients $n = 200$; generate $x_i \\sim \\mathrm{Uniform}(0.6, 2.0)$ in $\\mathrm{mg}/\\mathrm{dL}$; true data-generating logit\n$$\\operatorname{logit}(\\pi_i) = -1 + 2\\,\\log(x_i) - 1\\,\\log(x_i)^2;$$\nthen $y_i \\sim \\mathrm{Bernoulli}(\\pi_i)$. Use a pseudo-random number generator seed $123$.\n- Test case $2$ (linear optimal): number of patients $n = 150$; generate $x_i \\sim \\mathrm{Uniform}(0.5, 1.5)$ in $\\mathrm{mg}/\\mathrm{dL}$; true data-generating logit\n$$\\operatorname{logit}(\\pi_i) = -5 + 4\\,x_i;$$\nthen $y_i \\sim \\mathrm{Bernoulli}(\\pi_i)$. Use a pseudo-random number generator seed $321$.\n- Test case $3$ (inverse optimal, boundary of negative power): number of patients $n = 120$; generate $x_i \\sim \\mathrm{Uniform}(0.8, 2.5)$ in $\\mathrm{mg}/\\mathrm{dL}$; true data-generating logit\n$$\\operatorname{logit}(\\pi_i) = -2 + \\frac{3}{x_i};$$\nthen $y_i \\sim \\mathrm{Bernoulli}(\\pi_i)$. Use a pseudo-random number generator seed $222$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a sublist of the form $[d, p_1, p_2, D]$, where $d$ is an integer, $p_1$ and $p_2$ are real numbers drawn from $\\mathcal{P}$ (with $p_2$ equal to the floating $\\mathrm{NaN}$ when $d=1$), and $D$ is a real number. For example,\n$$\\texttt{[[2,0.0,0.0,123.456789],[1,1.0,NaN,234.567890],[1,-1.0,NaN,345.678901]]}.$$\n\nConstraints and scientific realism:\n- All $x_i$ must remain strictly positive due to the $\\log(x)$ terms; use the specified uniform ranges.\n- The predictor unit is $\\mathrm{mg}/\\mathrm{dL}$; no unit conversion is required in the output because the deviance $D$ is unitless.\n- No external input or files are permitted; the program must be fully self-contained.", "solution": "The user-provided problem statement has been critically validated and is determined to be **valid**. It is scientifically grounded, well-posed, objective, and presents a non-trivial computational statistics task that is representative of genuine research problems in biostatistics. The problem consists of implementing the Multivariable Fractional Polynomials (MFP) method to select the optimal functional form for a single continuous predictor in a logistic regression model. This will be accomplished by performing an exhaustive search over a predefined set of candidate transformations and selecting the one that minimizes the deviance.\n\n### 1. Theoretical Framework: Logistic Regression and Model Selection\n\nThe problem is situated within the framework of generalized linear models, specifically binary logistic regression. We are given a set of observations $\\{(y_i, x_i)\\}_{i=1}^n$, where $y_i \\in \\{0, 1\\}$ is a binary outcome and $x_i \\in \\mathbb{R}_{0}$ is a continuous predictor (serum creatinine). The logistic regression model links the predictor to the probability of the outcome, $\\pi_i = \\Pr(y_i=1 \\mid x_i)$, via the logit link function:\n$$ \\operatorname{logit}(\\pi_i) = \\log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = \\eta_i $$\nHere, $\\eta_i$ is the linear predictor. The core of the MFP procedure is to find the best-fitting functional form for $x_i$ in this linear predictor. The general form of the model we will fit is:\n$$ \\eta_i = \\beta_0 + \\beta_1 f_1(x_i) + \\beta_2 f_2(x_i) $$\nwhere $f_1$ and $f_2$ are transformations of $x_i$ drawn from the Fractional Polynomial family. For degree $d=1$ models, the term $\\beta_2 f_2(x_i)$ is omitted.\n\nThe model parameters, $\\boldsymbol{\\beta}$, are estimated using Maximum Likelihood Estimation (MLE). This involves maximizing the log-likelihood function for Bernoulli-distributed outcomes:\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left( y_i \\eta_i - \\log(1 + e^{\\eta_i}) \\right) $$\nThis function is globally concave, guaranteeing that numerical optimization methods can reliably find the unique maximum. The model's goodness-of-fit is assessed using the deviance, defined as $D = -2\\ell(\\hat{\\boldsymbol{\\beta}})$, where $\\hat{\\boldsymbol{\\beta}}$ is the maximum likelihood estimate of $\\boldsymbol{\\beta}$. The goal of the MFP search is to identify the functional form $(f_1, f_2)$ that results in the minimum deviance $D$.\n\n### 2. The Candidate Model Space: Fractional Polynomials\n\nFractional Polynomials provide a rich, yet parsimonious, class of functions for modeling nonlinear relationships. The search is conducted over a pre-specified set of candidate powers, $\\mathcal{P} = \\{-2, -1, -0.5, 0, 0.5, 1, 2, 3\\}$. The function associated with power $p=0$ is defined as $\\log(x)$ as it arises from the limit of $(x^p - 1)/p$ as $p \\to 0$.\n\nThe candidate models are categorized by their degree, $d$:\n\n**Degree 1 (FP1):** These models use a single transformation.\n$$ \\eta_i = \\beta_0 + \\beta_1 f_1(x_i; p) \\quad \\text{where} \\quad f_1(x;p) = \\begin{cases} x^p  p \\neq 0 \\\\ \\log(x)  p = 0 \\end{cases} $$\nWith $|\\mathcal{P}|=8$ powers, there are $8$ candidate FP1 models.\n\n**Degree 2 (FP2):** These models use two transformations, with powers $(p_1, p_2)$ from $\\mathcal{P}$. By convention, we take $p_1 \\le p_2$.\n$$ \\eta_i = \\beta_0 + \\beta_1 f_1(x_i; p_1, p_2) + \\beta_2 f_2(x_i; p_1, p_2) $$\nThe functional forms depend on whether the powers are distinct or repeated:\n- **Distinct Powers ($p_1  p_2$):** The transformations are $(f_1, f_2) = (x^{p_1}, x^{p_2})$, with the $\\log(x)$ modification if a power is $0$. There are $\\binom{8}{2} = 28$ such models.\n- **Repeated Powers ($p_1 = p_2 = p$):** To avoid collinearity, the second term is modified. This is known as a \"repeated powers\" model.\n$$ (f_1, f_2) = \\begin{cases} (x^p, x^p \\log(x))  p \\neq 0 \\\\ (\\log(x), \\log(x)^2)  p = 0 \\end{cases} $$\nThere are $8$ such models.\n\nIn total, the search space consists of $8$ (FP1) + $28$ (distinct FP2) + $8$ (repeated FP2) = $44$ candidate models for each test-case dataset.\n\n### 3. Algorithmic Procedure\n\nThe implementation will follow a systematic, exhaustive search algorithm for each test case.\n\n**Step 1: Data Generation:** For each test case, generate $n$ observations $(x_i, y_i)$ according to the specified data-generating process. The predictor $x_i$ is drawn from a uniform distribution, the linear predictor $\\eta_i$ is calculated using the true model, the probability $\\pi_i$ is found via the inverse logit function $\\pi_i = (1 + e^{-\\eta_i})^{-1}$, and the outcome $y_i$ is drawn from a Bernoulli distribution with parameter $\\pi_i$.\n\n**Step 2: Model Search and Evaluation:**\nThe core of the algorithm is a loop over all $44$ candidate FP models.\n- For each model, defined by its degree $d$ and power(s) $(p_1, p_2)$:\n    a. **Construct Design Matrix:** Generate the transformed predictor columns (e.g., $x_i^{-0.5}$, $\\log(x_i)$).\n    b. **Center Predictors:** For numerical stability, center each transformed column by subtracting its mean. This creates $\\tilde{f}_j(x_i) = f_j(x_i) - \\bar{f}_j$.\n    c. **Form Full Design Matrix:** Combine a column of ones (for the intercept $\\beta_0$) with the one or two centered transformed predictor columns to form the design matrix $X$.\n    d. **Fit Model:** Perform MLE to find the coefficients $\\hat{\\boldsymbol{\\beta}}$ that maximize the log-likelihood. This is a numerical optimization problem. We will minimize the negative log-likelihood function, $F(\\boldsymbol{\\beta}) = -\\ell(\\boldsymbol{\\beta})$, using a quasi-Newton method (L-BFGS-B) from `scipy.optimize`. This requires providing the objective function $F(\\boldsymbol{\\beta})$ and its gradient (Jacobian):\n    $$ \\nabla_{\\boldsymbol{\\beta}} F(\\boldsymbol{\\beta}) = X^T(\\boldsymbol{\\pi} - \\mathbf{y}), \\quad \\text{where } \\boldsymbol{\\pi} = (1 + e^{-X\\boldsymbol{\\beta}})^{-1} $$\n    e. **Compute Deviance:** The minimized value of the objective function is $-\\ell(\\hat{\\boldsymbol{\\beta}})$. The deviance is $D = -2\\ell(\\hat{\\boldsymbol{\\beta}}) = 2 \\times \\min(F(\\boldsymbol{\\beta}))$.\n    f. **Compare and Store:** Keep track of the model (degree, powers) that yields the minimum deviance found so far.\n\n**Step 3: Report Results:** After iterating through all $44$ models, the one with the lowest deviance is the \"best\" model according to the MFP criterion. For each test case, report its degree $d$, powers $(p_1, p_2)$ (with $p_2$ as NaN for $d=1$), and its minimized deviance $D$.\n\nThis comprehensive procedure ensures that we correctly implement the specified statistical methodology and arrive at a reproducible, verifiable result for each defined test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy\nfrom scipy.optimize import minimize\nfrom scipy.special import expit\nimport itertools\n\ndef solve():\n    \"\"\"\n    Main function to run the MFP search procedure on all test cases and print the results.\n    \"\"\"\n    \n    POWERS = [-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0, 3.0]\n\n    test_cases = [\n        {\n            \"n\": 200, \"x_range\": (0.6, 2.0), \"seed\": 123,\n            \"true_logit_func\": lambda x: -1 + 2 * np.log(x) - 1 * (np.log(x)**2)\n        },\n        {\n            \"n\": 150, \"x_range\": (0.5, 1.5), \"seed\": 321,\n            \"true_logit_func\": lambda x: -5 + 4 * x\n        },\n        {\n            \"n\": 120, \"x_range\": (0.8, 2.5), \"seed\": 222,\n            \"true_logit_func\": lambda x: -2 + 3 / x\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        # 1. Generate data\n        rng = np.random.default_rng(case[\"seed\"])\n        x = rng.uniform(case[\"x_range\"][0], case[\"x_range\"][1], case[\"n\"])\n        \n        true_eta = case[\"true_logit_func\"](x)\n        pi = expit(true_eta)\n        y = rng.binomial(1, pi, size=case[\"n\"])\n\n        best_model = {\n            \"d\": 0, \"p1\": np.nan, \"p2\": np.nan, \"deviance\": np.inf\n        }\n        \n        # 2. MFP Search\n        \n        # Null model (intercept only) for initial beta guess\n        p_avg = np.mean(y)\n        beta_init_null = [np.log(p_avg / (1 - p_avg))]\n\n        # Degree 1 models\n        for p1 in POWERS:\n            # Create transformed predictor\n            if p1 == 0:\n                t1 = np.log(x)\n            else:\n                t1 = x**p1\n            \n            # Center and form design matrix\n            t1_centered = t1 - np.mean(t1)\n            X = np.c_[np.ones(case[\"n\"]), t1_centered]\n            \n            # Fit model and get deviance\n            beta_init = np.append(beta_init_null, 0)\n            res = fit_logistic_mle(X, y, beta_init)\n            \n            if res is not None and res['deviance']  best_model[\"deviance\"]:\n                best_model = {\"d\": 1, \"p1\": p1, \"p2\": np.nan, \"deviance\": res['deviance']}\n\n        # Degree 2 models\n        for p1, p2 in itertools.combinations_with_replacement(POWERS, 2):\n            # Create transformed predictors\n            if p1 == p2:\n                if p1 == 0: # (log(x), log(x)^2)\n                    t1 = np.log(x)\n                    t2 = np.log(x)**2\n                else: # (x^p, x^p*log(x))\n                    t1 = x**p1\n                    t2 = t1 * np.log(x)\n            else: # p1  p2\n                if p1 == 0:\n                    t1 = np.log(x)\n                else:\n                    t1 = x**p1\n                \n                if p2 == 0: # This won't be reached because p1p2 and p1=0 is handled above\n                    t2 = np.log(x)\n                else:\n                    t2 = x**p2\n            \n            # Center and form design matrix\n            t1_centered = t1 - np.mean(t1)\n            t2_centered = t2 - np.mean(t2)\n            X = np.c_[np.ones(case[\"n\"]), t1_centered, t2_centered]\n\n            # Fit model and get deviance\n            beta_init = np.append(beta_init_null, [0, 0])\n            res = fit_logistic_mle(X, y, beta_init)\n            \n            if res is not None and res['deviance']  best_model[\"deviance\"]:\n                best_model = {\"d\": 2, \"p1\": p1, \"p2\": p2, \"deviance\": res['deviance']}\n        \n        all_results.append([best_model['d'], best_model['p1'], best_model['p2'], best_model['deviance']])\n\n    # 3. Format and print output\n    print(format_results(all_results))\n\ndef fit_logistic_mle(X, y, beta_init):\n    \"\"\"\n    Fits a logistic regression model using MLE.\n    \n    Args:\n        X (np.ndarray): Design matrix (n_samples, n_features).\n        y (np.ndarray): Response vector (n_samples,).\n        beta_init (np.ndarray): Initial guess for coefficients.\n    \n    Returns:\n        A dictionary with 'beta' and 'deviance', or None on failure.\n    \"\"\"\n    def neg_log_likelihood(beta, X, y):\n        eta = X @ beta\n        # Add clipping for numerical stability to avoid overflow in exp\n        eta = np.clip(eta, -30, 30)\n        p = expit(eta)\n        # Add clipping for stability to avoid log(0)\n        p = np.clip(p, 1e-9, 1 - 1e-9)\n        return -np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n\n    def jacobian(beta, X, y):\n        eta = X @ beta\n        eta = np.clip(eta, -30, 30)\n        p = expit(eta)\n        return X.T @ (p - y)\n\n    try:\n        opt_res = minimize(\n            fun=neg_log_likelihood,\n            x0=beta_init,\n            args=(X, y),\n            method='L-BFGS-B',\n            jac=jacobian,\n            options={'maxiter': 1000}\n        )\n        if opt_res.success:\n            deviance = 2 * opt_res.fun\n            return {\"beta\": opt_res.x, \"deviance\": deviance}\n        return None\n    except (ValueError, np.linalg.LinAlgError):\n        return None\n\ndef format_results(results_list):\n    \"\"\"\n    Formats the list of results into the required string format.\n    Example: [[2,0.0,0.0,123.456789],[1,1.0,NaN,234.567890]]\n    \"\"\"\n    def format_single_result(res):\n        d, p1, p2, D = res\n        p1_str = str(float(p1))\n        p2_str = 'NaN' if np.isnan(p2) else str(float(p2))\n        d_str = f\"{D:.15g}\" # Use 'g' for flexible precision without trailing zeros\n        return f\"[{d},{p1_str},{p2_str},{d_str}]\"\n    \n    return f\"[{','.join(format_single_result(r) for r in results_list)}]\"\n\n# Run the solver\nsolve()\n\n```", "id": "4974006"}, {"introduction": "Once a potential predictor and its functional form are proposed, a key model-building task is to determine if its inclusion is statistically justified. The Likelihood Ratio Test (LRT) offers a principled framework for this decision by formally comparing the fit of two nested models: a simpler (reduced) model and a more complex (full) model that includes the new predictor [@problem_id:4974063]. This exercise will guide you through calculating the LRT statistic and its corresponding $p$-value, providing a foundational skill for evidence-based variable selection strategies like forward selection or backward elimination.", "problem": "In a multicenter cohort of $n = 2{,}200$ adults hospitalized with Coronavirus Disease 2019 (COVID-19), investigators model the probability of $30$-day in-hospital mortality using logistic regression. Let $Y_{i} \\in \\{0,1\\}$ denote mortality for patient $i$, and let $\\boldsymbol{X}_{i}$ denote a vector of baseline predictors (age, sex, comorbidity index, and a composite severity score). The reduced model uses $\\boldsymbol{X}_{i}$ only. A proposed full model augments the reduced model with an additional continuous predictor $x_{j}$ representing admission serum lactate (in $\\text{mmol}/\\text{L}$), centered at the cohort mean.\n\nBoth models are fit by maximum likelihood under standard regularity conditions (independent observations, correctly specified link and mean structure, no separation, and finite Fisher information). The maximized log-likelihood of the reduced model is $\\ell_{0} = -1{,}248.30$, and the maximized log-likelihood of the full model is $\\ell_{1} = -1{,}241.80$. Assume that the full model differs from the reduced model by exactly one free parameter (the coefficient of $x_{j}$), and that asymptotic likelihood theory applies.\n\nUsing the likelihood principle for nested models and an appropriate asymptotic reference distribution, compute the test statistic comparing the full and reduced models, and then compute the corresponding $p$-value for testing the null hypothesis that the coefficient of $x_{j}$ is $0$. Briefly justify the asymptotic distribution used in the comparison and interpret the meaning of the resulting $p$-value in the context of principled model building strategies for logistic regression in medicine.\n\nReport as your final answer the $p$-value as a decimal rounded to three significant figures. Do not report an inequality; provide a single number.", "solution": "The problem requires a comparison of two nested logistic regression models using the likelihood ratio test (LRT). The reduced model, which we will denote as $M_0$, predicts mortality using a set of baseline predictors $\\boldsymbol{X}_{i}$. The full model, $M_1$, includes all predictors from $M_0$ plus an additional predictor, admission serum lactate $x_{j}$. The models are nested because $M_0$ is a special case of $M_1$ where the coefficient for $x_{j}$ is constrained to be zero.\n\nThe comparison is based on the likelihood principle, which states that all evidence in the data about the parameters is contained in the likelihood function. For nested models, the improvement in fit provided by the additional parameters in the larger model can be quantified by comparing their maximized log-likelihoods.\n\nThe test statistic for the likelihood ratio test is denoted by $D$ and is calculated as twice the difference between the maximized log-likelihood of the full model ($\\ell_1$) and the maximized log-likelihood of the reduced model ($\\ell_0$).\n$$\nD = 2(\\ell_{1} - \\ell_{0})\n$$\nThe problem provides the necessary values:\n- Maximized log-likelihood of the reduced model ($M_0$): $\\ell_{0} = -1{,}248.30$\n- Maximized log-likelihood of the full model ($M_1$): $\\ell_{1} = -1{,}241.80$\n\nSubstituting these values into the formula for the test statistic:\n$$\nD = 2(-1{,}241.80 - (-1{,}248.30)) = 2(-1{,}241.80 + 1{,}248.30) = 2(6.50) = 13.00\n$$\nThe calculated test statistic is $D = 13.00$.\n\nThe next step is to determine the statistical significance of this value by comparing it to an appropriate reference distribution. The null hypothesis, $H_0$, is that the coefficient of the additional predictor $x_{j}$ is zero. This is equivalent to stating that the reduced model $M_0$ is the correct model. According to Wilks' Theorem, under the null hypothesis and for a sufficiently large sample size ($n=2{,}200$ is large), the likelihood ratio test statistic $D$ asymptotically follows a chi-squared ($\\chi^2$) distribution.\n\nThe degrees of freedom ($df$) for this $\\chi^2$ distribution are equal to the difference in the number of freely estimated parameters between the full model and the reduced model. The problem states that the full model differs from the reduced model by exactly one free parameter (the coefficient of $x_{j}$). Therefore, the degrees of freedom are $df = 1$.\nSo, under $H_0$, the test statistic $D$ follows a $\\chi^2_1$ distribution:\n$$\nD \\sim \\chi^2_1\n$$\nThe $p$-value is the probability of observing a test statistic as extreme as, or more extreme than, the one calculated, assuming the null hypothesis is true. This corresponds to the upper tail probability of the $\\chi^2_1$ distribution.\n$$\np = P(\\chi^2_1 \\ge 13.00)\n$$\nUsing a standard statistical calculator or software to find the cumulative probability for a $\\chi^2$ distribution with $1$ degree of freedom, we find that the probability of a value being greater than or equal to $13.00$ is approximately $0.0003115$. Rounding this value to three significant figures, as requested, gives $0.000312$.\n\nThe justification for using the $\\chi^2_1$ distribution is Wilks' Theorem, which establishes the asymptotic distribution of the likelihood ratio statistic for nested models under standard regularity conditions. These conditions (large sample size, independent observations, correctly specified models, etc.) are stated to be met in the problem description.\n\nThe interpretation of this $p$-value in the context of medical model building is as follows. The very small $p$-value ($p \\approx 0.000312$) is well below conventional significance thresholds (e.g., $\\alpha = 0.05$ or $\\alpha = 0.01$). This provides strong statistical evidence to reject the null hypothesis that the coefficient for serum lactate is zero. In other words, adding serum lactate to the model containing age, sex, comorbidity index, and a severity score leads to a statistically significant improvement in the model's fit to the data. From a principled model-building perspective, this result suggests that serum lactate is an important predictor of $30$-day mortality in this patient population, and its inclusion in the final predictive model is strongly supported by the data. The decision to retain it should also consider clinical plausibility and the principle of parsimony, but the statistical evidence here is compelling.", "answer": "$$\\boxed{0.000312}$$", "id": "4974063"}, {"introduction": "A significant challenge in fitting logistic regression models, particularly with medical data, is the issue of separation, where a predictor or a combination of them perfectly or nearly-perfectly predicts the outcome. This phenomenon causes standard maximum likelihood estimates of the coefficients to become infinite or highly unstable, rendering the model useless. This practice demonstrates how to overcome this problem by implementing a penalized logistic regression model, which adds a penalty term to the likelihood to shrink the coefficients and ensure finite, stable estimates [@problem_id:4974092]. Mastering this technique is essential for building robust models in real-world scenarios where separation is a common occurrence.", "problem": "Consider a binary outcome model for medical event occurrence using logistic regression, where each individual $i$ provides a feature vector $\\boldsymbol{x}_i \\in \\mathbb{R}^p$ and an outcome $y_i \\in \\{0,1\\}$. The fundamental base consists of: (i) the Bernoulli data model with probability $\\mathbb{P}(Y_i = 1 \\mid \\boldsymbol{x}_i) = \\mu_i$, (ii) the logistic link $\\mu_i = \\left(1 + \\exp\\left(-\\eta_i\\right)\\right)^{-1}$ with linear predictor $\\eta_i = \\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}$, and (iii) maximum likelihood estimation via the log-likelihood of independent Bernoulli observations. In medical datasets, complete or quasi separation occurs when there exists a linear combination of predictors that perfectly (or almost perfectly) classifies outcomes, making unpenalized maximum likelihood estimates unstable or non-finite.\n\nYour task is to implement a model building strategy for logistic regression that yields finite, stable estimates in the presence of separation by adding a Euclidean norm (L2) penalty to the coefficients. Specifically, estimate $\\boldsymbol{\\beta}$ by maximizing the penalized objective formed by adding a quadratic penalty in the coefficients to the Bernoulli log-likelihood, while not penalizing the intercept. Derive the algorithmic steps from the above fundamental base using the canonical iterative weighted framework for generalized linear models and solve the penalized normal equations at each iteration. Ensure numerical stability by appropriately handling extreme fitted values. Standardize the continuous predictor $x_{\\text{age}}$ to improve conditioning using $x_{\\text{age}}^{*} = \\left(x_{\\text{age}} - \\bar{x}_{\\text{age}}\\right)/10$, where $10$ is a scaling factor measured in years.\n\nImplement a complete, runnable program that, for the following test suite, fits the penalized logistic regression model and reports the coefficient estimates and fitted probabilities. The intercept must not be penalized. For each test case, output a list containing the estimated intercept and coefficients, followed by the minimum and maximum fitted probabilities on the training data, and finally the fitted probabilities for two specified new patients. All outputs must be real numbers. The final program output must be a single line containing a list of the per-test-case lists, enclosed in square brackets, with elements comma-separated.\n\nTest suite specification (each case uses an intercept and two predictors: $x_{\\text{age}}$ measured in years and $x_{\\text{bio}} \\in \\{0,1\\}$ indicating a binary biomarker):\n\n- Case $1$ (quasi separation, moderate penalty):\n  - Ages (years): $[55,64,47,38,70,59,53,61,49,45,72,50]$.\n  - Biomarker: $[1,1,1,0,1,0,1,1,0,0,1,0]$.\n  - Outcomes: $[1,1,1,0,1,0,1,0,0,0,1,0]$.\n  - Penalty parameter: $\\lambda = 1$.\n  - New patients to predict:\n    - Patient A: $x_{\\text{age}} = 60$, $x_{\\text{bio}} = 1$.\n    - Patient B: $x_{\\text{age}} = 50$, $x_{\\text{bio}} = 0$.\n\n- Case $2$ (complete separation, moderate penalty):\n  - Ages (years): $[40,42,45,50,52,55,60,62,65,70]$.\n  - Biomarker: $[0,0,0,0,1,1,1,1,1,1]$.\n  - Outcomes: $[0,0,0,0,1,1,1,1,1,1]$.\n  - Penalty parameter: $\\lambda = 1$.\n  - New patients to predict:\n    - Patient A: $x_{\\text{age}} = 60$, $x_{\\text{bio}} = 1$.\n    - Patient B: $x_{\\text{age}} = 50$, $x_{\\text{bio}} = 0$.\n\n- Case $3$ (extreme imbalance, near separation, light penalty):\n  - Ages (years): $[30,35,40,45,50,55,60,65,70,75,80,85]$.\n  - Biomarker: $[0,0,0,0,0,1,0,1,0,1,0,1]$.\n  - Outcomes: $[0,0,0,0,0,0,0,0,0,1,0,0]$.\n  - Penalty parameter: $\\lambda = 0.1$.\n  - New patients to predict:\n    - Patient A: $x_{\\text{age}} = 60$, $x_{\\text{bio}} = 1$.\n    - Patient B: $x_{\\text{age}} = 50$, $x_{\\text{bio}} = 0$.\n\n- Case $4$ (high shrinkage to assess stability under strong penalty):\n  - Use the same ages, biomarker, and outcomes as Case $1$.\n  - Penalty parameter: $\\lambda = 100$.\n  - New patients to predict:\n    - Patient A: $x_{\\text{age}} = 60$, $x_{\\text{bio}} = 1$.\n    - Patient B: $x_{\\text{age}} = 50$, $x_{\\text{bio}} = 0$.\n\nFor each case, build the design matrix with an intercept term $x_0 = 1$, the standardized age predictor $x_{\\text{age}}^{*}$, and the biomarker $x_{\\text{bio}}$. Fit the penalized model and compute:\n- The estimated intercept and coefficients $(\\hat{\\beta}_0, \\hat{\\beta}_{\\text{age}}, \\hat{\\beta}_{\\text{bio}})$ as finite real numbers.\n- The minimum and maximum fitted probabilities on the training data, each expressed as a decimal in $[0,1]$.\n- The fitted probabilities for Patient A and Patient B, each expressed as a decimal in $[0,1]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of eight floats: $[\\hat{\\beta}_0,\\hat{\\beta}_{\\text{age}},\\hat{\\beta}_{\\text{bio}},\\min(\\hat{\\mu}),\\max(\\hat{\\mu}),\\hat{\\mu}_{\\text{A}},\\hat{\\mu}_{\\text{B}}]$.", "solution": "The problem requires the implementation of a penalized logistic regression model to ensure stable coefficient estimation in the presence of data separation. The solution is developed from first principles using the Iteratively Reweighted Least Squares (IRLS) algorithm, adapted for an L2 penalty (ridge regression).\n\n### 1. Model and Objective Function\n\nThe model for a binary outcome $y_i \\in \\{0, 1\\}$ given a predictor vector $\\boldsymbol{x}_i \\in \\mathbb{R}^p$ is specified by the logistic regression framework:\n1.  **Data Model**: The outcome $Y_i$ follows a Bernoulli distribution with success probability $\\mu_i = \\mathbb{P}(Y_i = 1 \\mid \\boldsymbol{x}_i)$.\n2.  **Systematic Component**: The predictors are linearly related to the log-odds of the outcome via a linear predictor $\\eta_i = \\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}$, where $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is the vector of coefficients. The design matrix $\\boldsymbol{X}$ is an $N \\times p$ matrix where row $i$ is $\\boldsymbol{x}_i^\\top$. For this problem, $p=3$ and $\\boldsymbol{x}_i = [1, x_{\\text{age}, i}^{*}, x_{\\text{bio}, i}]^\\top$.\n3.  **Link Function**: The logistic link function connects the probability $\\mu_i$ to the linear predictor $\\eta_i$: $\\eta_i = \\log\\left(\\frac{\\mu_i}{1 - \\mu_i}\\right)$. The inverse link function is $\\mu_i = \\frac{1}{1 + \\exp(-\\eta_i)}$.\n\nThe log-likelihood for $N$ independent observations is the sum of individual Bernoulli log-likelihoods:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\left[ y_i \\log(\\mu_i) + (1-y_i) \\log(1-\\mu_i) \\right]\n$$\nSubstituting $\\eta_i = \\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}$ and the relationship between $\\mu_i$ and $\\eta_i$, this can be expressed as:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\left[ y_i (\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}) - \\log(1 + \\exp(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta})) \\right]\n$$\nTo address estimation instability from separation, we add an L2 penalty to the log-likelihood for all coefficients except the intercept $\\beta_0$. The penalized log-likelihood objective function to be maximized is:\n$$\nQ(\\boldsymbol{\\beta}) = \\ell(\\boldsymbol{\\beta}) - \\frac{\\lambda}{2} \\sum_{j=1}^{p-1} \\beta_j^2\n$$\nwhere $\\lambda  0$ is the penalty parameter. The term $\\frac{1}{2}$ is a convention for mathematical convenience.\n\n### 2. Optimization via Penalized IRLS\n\nWe use a Newton-Raphson-based method to find the $\\boldsymbol{\\beta}$ that maximizes $Q(\\boldsymbol{\\beta})$. The update step is $\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [\\nabla^2 Q(\\boldsymbol{\\beta}^{(t)})]^{-1} \\nabla Q(\\boldsymbol{\\beta}^{(t)})$.\n\n**Gradient Vector (Score):**\nThe gradient of the log-likelihood is $\\nabla \\ell(\\boldsymbol{\\beta}) = \\boldsymbol{X}^\\top (\\boldsymbol{y} - \\boldsymbol{\\mu})$, where $\\boldsymbol{y}$ is the vector of outcomes and $\\boldsymbol{\\mu}$ is the vector of probabilities. The gradient of the penalty term is $\\lambda \\boldsymbol{P} \\boldsymbol{\\beta}$, where $\\boldsymbol{P}$ is a diagonal matrix with $P_{00}=0$ and $P_{jj}=1$ for $j \\ge 1$.\nThe gradient of the penalized objective is:\n$$\n\\nabla Q(\\boldsymbol{\\beta}) = \\boldsymbol{X}^\\top (\\boldsymbol{y} - \\boldsymbol{\\mu}) - \\lambda \\boldsymbol{P} \\boldsymbol{\\beta}\n$$\n\n**Hessian Matrix:**\nThe Hessian of the log-likelihood is $\\nabla^2 \\ell(\\boldsymbol{\\beta}) = -\\boldsymbol{X}^\\top \\boldsymbol{W} \\boldsymbol{X}$, where $\\boldsymbol{W}$ is a diagonal matrix with elements $W_{ii} = \\mu_i(1-\\mu_i)$, the variance of the Bernoulli response. The Hessian of the penalty term is $-\\lambda \\boldsymbol{P}$.\nThe Hessian of the penalized objective is:\n$$\n\\nabla^2 Q(\\boldsymbol{\\beta}) = -\\boldsymbol{X}^\\top \\boldsymbol{W} \\boldsymbol{X} - \\lambda \\boldsymbol{P}\n$$\n\n**Newton-Raphson Update:**\nSubstituting the gradient and Hessian into the update rule gives:\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [-\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} - \\lambda \\boldsymbol{P}]^{-1} [\\boldsymbol{X}^\\top (\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)}) - \\lambda \\boldsymbol{P} \\boldsymbol{\\beta}^{(t)}]\n$$\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + [\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} + \\lambda \\boldsymbol{P}]^{-1} [\\boldsymbol{X}^\\top (\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)}) - \\lambda \\boldsymbol{P} \\boldsymbol{\\beta}^{(t)}]\n$$\nRearranging this equation reveals the IRLS structure. Let's solve for $\\boldsymbol{\\beta}^{(t+1)}$:\n$$\n(\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} + \\lambda \\boldsymbol{P}) \\boldsymbol{\\beta}^{(t+1)} = (\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} + \\lambda \\boldsymbol{P}) \\boldsymbol{\\beta}^{(t)} + \\boldsymbol{X}^\\top (\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)}) - \\lambda \\boldsymbol{P} \\boldsymbol{\\beta}^{(t)}\n$$\n$$\n(\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} + \\lambda \\boldsymbol{P}) \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} \\boldsymbol{\\beta}^{(t)} + \\boldsymbol{X}^\\top (\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)})\n$$\nThe right-hand side can be written as $\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{z}^{(t)}$, where $\\boldsymbol{z}^{(t)}$ is the working response vector:\n$$\n\\boldsymbol{z}^{(t)} = \\boldsymbol{X} \\boldsymbol{\\beta}^{(t)} + (\\boldsymbol{W}^{(t)})^{-1} (\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)}) = \\boldsymbol{\\eta}^{(t)} + \\frac{\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)}}{\\boldsymbol{\\mu}^{(t)}(1-\\boldsymbol{\\mu}^{(t)})}\n$$\nThe update for $\\boldsymbol{\\beta}^{(t+1)}$ is obtained by solving the following system of linear equations, which are the normal equations for a weighted ridge regression:\n$$\n(\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} + \\lambda \\boldsymbol{P}) \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{z}^{(t)}\n$$\n\n### 3. Algorithmic Implementation\n\nThe penalized logistic regression model is fitted using the following iterative algorithm:\n\n1.  **Data Standardization**: Standardize the continuous predictor $x_{\\text{age}}$ using the training data mean $\\bar{x}_{\\text{age}}$: $x_{\\text{age}}^{*} = (x_{\\text{age}} - \\bar{x}_{\\text{age}}) / 10$. Construct the design matrix $\\boldsymbol{X}$ with columns for the intercept ($1$), standardized age ($x_{\\text{age}}^{*}$), and biomarker ($x_{\\text{bio}}$).\n\n2.  **Initialization**: Initialize the coefficient vector $\\boldsymbol{\\beta}^{(0)} = \\boldsymbol{0}$ and set the penalty matrix $\\boldsymbol{P} = \\text{diag}(0, 1, ..., 1)$. Choose a convergence tolerance $\\epsilon$ (e.g., $10^{-8}$) and a maximum number of iterations.\n\n3.  **Iteration**: For $t=0, 1, 2, ...$ until convergence:\n    a.  Compute the linear predictor: $\\boldsymbol{\\eta}^{(t)} = \\boldsymbol{X} \\boldsymbol{\\beta}^{(t)}$.\n    b.  Compute the fitted probabilities: $\\boldsymbol{\\mu}^{(t)} = (1 + \\exp(-\\boldsymbol{\\eta}^{(t)}))^{-1}$. To ensure numerical stability and prevent weights from becoming zero, probabilities are clipped to a small distance from $0$ and $1$, e.g., $\\mu_i \\in [10^{-10}, 1 - 10^{-10}]$.\n    c.  Compute the IRLS weights: Construct the diagonal matrix $\\boldsymbol{W}^{(t)}$ with $W_{ii}^{(t)} = \\mu_i^{(t)}(1-\\mu_i^{(t)})$.\n    d.  Compute the working response: $\\boldsymbol{z}^{(t)} = \\boldsymbol{\\eta}^{(t)} + (\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)}) / (\\mu_i^{(t)}(1-\\mu_i^{(t)}))$.\n    e.  Solve for the updated coefficients $\\boldsymbol{\\beta}^{(t+1)}$ from the penalized normal equations:\n        $$\n        \\boldsymbol{\\beta}^{(t+1)} = (\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} + \\lambda \\boldsymbol{P})^{-1} (\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{z}^{(t)})\n        $$\n        This linear system is solved efficiently without explicit matrix inversion, for instance, using `numpy.linalg.solve`.\n    f.  Check for convergence: The loop terminates if the change in coefficients is below the tolerance, i.e., $\\sum_j |\\beta_j^{(t+1)} - \\beta_j^{(t)}|  \\epsilon$.\n\n4.  **Final Estimates**: Upon convergence, the final coefficient vector is $\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta}^{(t+1)}$. The final fitted probabilities for the training data and new patients are calculated using $\\hat{\\boldsymbol{\\mu}} = (1 + \\exp(-\\boldsymbol{x}^\\top \\hat{\\boldsymbol{\\beta}}))^{-1}$, where for new patients, the predictor $x_{\\text{age}}$ is standardized using the mean from the original training data.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fit_penalized_logistic(ages, biomarker, outcomes, lambda_val, new_patients):\n    \"\"\"\n    Fits a penalized logistic regression model using Iteratively Reweighted Least Squares (IRLS).\n\n    Args:\n        ages (list): List of patient ages.\n        biomarker (list): List of binary biomarker statuses.\n        outcomes (list): List of binary outcomes.\n        lambda_val (float): The L2 penalty parameter.\n        new_patients (list of tuples): Data for new patients to predict on.\n\n    Returns:\n        list: A list of 8 floats: [beta_0, beta_age, beta_bio, min_mu, max_mu, mu_A, mu_B].\n    \"\"\"\n    # 1. Preprocessing and design matrix construction\n    ages_arr = np.array(ages, dtype=float)\n    biomarker_arr = np.array(biomarker, dtype=float)\n    y = np.array(outcomes, dtype=float)\n    n_obs = len(ages_arr)\n\n    mean_age = np.mean(ages_arr)\n    # Standardize age predictor as specified\n    scaled_age = (ages_arr - mean_age) / 10.0\n\n    # Design matrix X with intercept, standardized age, and biomarker\n    X = np.c_[np.ones(n_obs), scaled_age, biomarker_arr]\n    p = X.shape[1]\n\n    # 2. IRLS algorithm\n    # Initialization\n    beta = np.zeros(p)\n    # Penalty matrix P (does not penalize intercept)\n    P = np.diag([0.0] + [1.0] * (p - 1))\n    \n    # Convergence parameters\n    max_iter = 50\n    tol = 1e-8\n    epsilon = 1e-10\n\n    for i in range(max_iter):\n        # Calculate linear predictor and probabilities\n        eta = X @ beta\n        mu = 1.0 / (1.0 + np.exp(-eta))\n        \n        # Clip probabilities for numerical stability\n        mu = np.clip(mu, epsilon, 1.0 - epsilon)\n        \n        # Calculate weights and working response\n        W_diag = mu * (1.0 - mu)\n        W = np.diag(W_diag)\n        z = eta + (y - mu) / W_diag\n        \n        # Solve the penalized normal equations\n        # (X^T W X + lambda * P) beta = X^T W z\n        A = X.T @ W @ X + lambda_val * P\n        b = X.T @ W @ z\n        \n        try:\n            beta_new = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudo-inverse if solve fails, though unlikely with penalty\n            beta_new = np.linalg.pinv(A) @ b\n\n        # Check for convergence\n        if np.sum(np.abs(beta_new - beta))  tol:\n            beta = beta_new\n            break\n        \n        beta = beta_new\n\n    # 3. Final calculations\n    beta_hat = beta\n    \n    # Fitted probabilities on training data\n    final_eta = X @ beta_hat\n    final_mu = 1.0 / (1.0 + np.exp(-final_eta))\n    min_mu = np.min(final_mu)\n    max_mu = np.max(final_mu)\n\n    # Predictions for new patients\n    mu_preds = []\n    for patient_data in new_patients:\n        age_new, bio_new = patient_data\n        # Standardize age using the training data mean\n        scaled_age_new = (age_new - mean_age) / 10.0\n        x_new = np.array([1.0, scaled_age_new, float(bio_new)])\n        eta_new = x_new @ beta_hat\n        mu_new = 1.0 / (1.0 + np.exp(-eta_new))\n        mu_preds.append(mu_new)\n        \n    # 4. Assemble results in the required format\n    result_list = list(beta_hat) + [min_mu, max_mu] + mu_preds\n    return result_list\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"ages\": [55, 64, 47, 38, 70, 59, 53, 61, 49, 45, 72, 50],\n            \"biomarker\": [1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0],\n            \"outcomes\": [1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0],\n            \"lambda\": 1.0,\n            \"new_patients\": [(60, 1), (50, 0)],\n        },\n        {\n            \"ages\": [40, 42, 45, 50, 52, 55, 60, 62, 65, 70],\n            \"biomarker\": [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n            \"outcomes\": [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n            \"lambda\": 1.0,\n            \"new_patients\": [(60, 1), (50, 0)],\n        },\n        {\n            \"ages\": [30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85],\n            \"biomarker\": [0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1],\n            \"outcomes\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n            \"lambda\": 0.1,\n            \"new_patients\": [(60, 1), (50, 0)],\n        },\n        {\n            \"ages\": [55, 64, 47, 38, 70, 59, 53, 61, 49, 45, 72, 50],\n            \"biomarker\": [1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0],\n            \"outcomes\": [1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0],\n            \"lambda\": 100.0,\n            \"new_patients\": [(60, 1), (50, 0)],\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = fit_penalized_logistic(\n            ages=case[\"ages\"],\n            biomarker=case[\"biomarker\"],\n            outcomes=case[\"outcomes\"],\n            lambda_val=case[\"lambda\"],\n            new_patients=case[\"new_patients\"]\n        )\n        all_results.append(results)\n\n    # Format the final output string exactly as required\n    # Create string representations of inner lists, then join them\n    inner_lists_str = [f\"[{','.join(f'{x:.8f}' for x in res)}]\" for res in all_results]\n    print(f\"[{','.join(inner_lists_str)}]\")\n\nsolve()\n```", "id": "4974092"}]}