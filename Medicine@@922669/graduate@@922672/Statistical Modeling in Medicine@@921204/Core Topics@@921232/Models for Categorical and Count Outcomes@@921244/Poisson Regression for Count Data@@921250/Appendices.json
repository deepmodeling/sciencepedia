{"hands_on_practices": [{"introduction": "The foundation of Poisson regression lies in its ability to model event rates. This first exercise takes you back to first principles, tasking you with estimating a rate ratio directly from raw count data and follow-up times [@problem_id:4826677]. By working through the maximum likelihood estimation for a simple two-group comparison, you will uncover an elegant and intuitive result that directly connects the formal statistical model to the empirical rates observed in the data.", "problem": "You are given independent count outcomes from patients in two exposure groups, along with their follow-up times. Assume that, conditional on exposure and follow-up time, each patient’s count arises from a Poisson distribution with mean equal to a rate times follow-up time. Model the expected count for patient $i$ as\n$$\nE[Y_i \\mid X_i, T_i] \\;=\\; \\mu_i \\;=\\; T_i \\, \\exp\\big(\\beta_0 + \\beta_1 X_i\\big),\n$$\nwhere $Y_i$ is the observed count, $X_i \\in \\{0,1\\}$ is the exposure indicator (with $X_i=0$ for unexposed and $X_i=1$ for exposed), and $T_i > 0$ is the follow-up time. This is a Poisson log-linear model with an offset equal to the natural logarithm $\\ln(T_i)$. The parameter $\\exp(\\beta_1)$ is the rate ratio comparing the exposed group to the unexposed group.\n\nStarting from the fundamental definition of the Poisson likelihood for independent observations and the log-linear model with an offset, compute the maximum likelihood estimate of the rate ratio $\\exp(\\beta_1)$ for each of the following test cases. You must use the Poisson model and the offset exactly as specified. The natural logarithm $\\ln$ is base $e$.\n\nTest suite. For each case, the data are given as three aligned lists: counts $Y$, exposure indicators $X$, and follow-up times $T$ (in person-years). All $T_i$ are strictly positive; all $Y_i$ are nonnegative integers.\n\n- Case A:\n  - $Y = [2, 1, 0, 3, 4, 2, 1, 6]$\n  - $X = [0, 0, 0, 0, 1, 1, 1, 1]$\n  - $T = [1.2, 0.8, 1.0, 2.0, 1.0, 1.5, 0.5, 2.0]$\n\n- Case B:\n  - $Y = [0, 1, 8, 0, 0, 15]$\n  - $X = [0, 0, 0, 1, 1, 1]$\n  - $T = [0.2, 0.3, 5.0, 0.1, 0.4, 4.0]$\n\n- Case C:\n  - $Y = [0, 1, 0, 0, 1, 0, 1, 0]$\n  - $X = [0, 0, 0, 0, 1, 1, 1, 1]$\n  - $T = [5.0, 10.0, 8.0, 7.0, 4.0, 9.0, 6.0, 8.0]$\n\n- Case D:\n  - $Y = [40, 60, 70, 50]$\n  - $X = [0, 0, 1, 1]$\n  - $T = [2.0, 3.0, 1.0, 4.0]$\n\nScientific and modeling assumptions. Assume independent Poisson counts with means as specified and use the canonical log link with an offset equal to $\\ln(T_i)$. The solution must be based on maximizing the Poisson log-likelihood under this model.\n\nTask. Write a complete, runnable program that, for each case, fits the described Poisson regression model with an intercept and a single exposure indicator using the offset $\\ln(T_i)$, and returns the maximum likelihood estimate of the rate ratio $\\exp(\\beta_1)$.\n\nRequired final output format. Your program should produce a single line of output containing the four estimated rate ratios, in the order Case A, Case B, Case C, Case D, as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places (for example, $[1.234567, 2.345678, 3.456789, 4.567890]$).", "solution": "The problem is valid. It is a standard, well-posed problem in statistical inference, specifically a maximum likelihood estimation for a Poisson log-linear model, a type of Generalized Linear Model (GLM). The problem is scientifically grounded, objective, and provides all necessary information for a unique solution.\n\nThe objective is to find the maximum likelihood estimate (MLE) of the rate ratio, $\\exp(\\beta_1)$, for a Poisson regression model. The model specifies that for patient $i$, the observed count $Y_i$ follows a Poisson distribution with mean $\\mu_i$, where the mean is related to covariates by:\n$$\n\\mu_i = E[Y_i \\mid X_i, T_i] = T_i \\exp(\\beta_0 + \\beta_1 X_i)\n$$\nHere, $Y_i$ is the count, $X_i \\in \\{0, 1\\}$ is the exposure status, and $T_i > 0$ is the follow-up time. Taking the natural logarithm of both sides reveals the underlying linear model with an offset term:\n$$\n\\ln(\\mu_i) = \\beta_0 + \\beta_1 X_i + \\ln(T_i)\n$$\nwhere $\\ln(T_i)$ is the offset.\n\nThe MLE is found by maximizing the log-likelihood function. For a set of $N$ independent observations $(y_i, x_i, t_i)$, the probability mass function for a single observation is $P(Y_i = y_i) = \\mu_i^{y_i} e^{-\\mu_i} / y_i!$. The likelihood function is the product of these probabilities:\n$$\nL(\\beta_0, \\beta_1) = \\prod_{i=1}^{N} \\frac{\\mu_i^{y_i} e^{-\\mu_i}}{y_i!}\n$$\nThe log-likelihood function, $\\ell(\\beta_0, \\beta_1) = \\log L(\\beta_0, \\beta_1)$, is:\n$$\n\\ell(\\beta_0, \\beta_1) = \\sum_{i=1}^{N} \\left( y_i \\ln(\\mu_i) - \\mu_i - \\ln(y_i!) \\right)\n$$\nTo maximize this function with respect to the parameters $\\beta_0$ and $\\beta_1$, we can ignore the term $\\ln(y_i!)$ as it does not depend on them. Substituting $\\ln(\\mu_i) = \\ln(t_i) + \\beta_0 + \\beta_1 x_i$, the relevant part of the log-likelihood, $\\ell^*$, becomes:\n$$\n\\ell^*(\\beta_0, \\beta_1) = \\sum_{i=1}^{N} \\left[ y_i (\\ln(t_i) + \\beta_0 + \\beta_1 x_i) - t_i \\exp(\\beta_0 + \\beta_1 x_i) \\right]\n$$\nTo find the maximum, we compute the partial derivatives of $\\ell^*$ with respect to $\\beta_0$ and $\\beta_1$ and set them to zero. These are the score equations.\n\n1.  Derivative with respect to $\\beta_0$:\n    $$\n    \\frac{\\partial \\ell^*}{\\partial \\beta_0} = \\sum_{i=1}^{N} \\left( y_i - t_i \\exp(\\beta_0 + \\beta_1 x_i) \\right) = \\sum_{i=1}^{N} (y_i - \\mu_i)\n    $$\n2.  Derivative with respect to $\\beta_1$:\n    $$\n    \\frac{\\partial \\ell^*}{\\partial \\beta_1} = \\sum_{i=1}^{N} \\left( y_i x_i - x_i t_i \\exp(\\beta_0 + \\beta_1 x_i) \\right) = \\sum_{i=1}^{N} x_i(y_i - \\mu_i)\n    $$\nSetting these to zero gives the system of equations:\n$$\n\\sum_{i=1}^{N} (y_i - \\mu_i) = 0 \\quad \\text{and} \\quad \\sum_{i=1}^{N} x_i(y_i - \\mu_i) = 0\n$$\nThe second equation involves a sum over terms where $x_i=1$, as terms are zero when $x_i=0$. This gives:\n$$\n\\sum_{i: x_i=1} (y_i - t_i \\exp(\\beta_0 + \\beta_1 \\cdot 1)) = 0\n$$\n$$\n\\sum_{i: x_i=1} y_i = \\exp(\\beta_0 + \\beta_1) \\sum_{i: x_i=1} t_i\n$$\nLet $Y_1 = \\sum_{i: x_i=1} y_i$ be the total count in the exposed group and $T_1 = \\sum_{i: x_i=1} t_i$ be the total person-time in the exposed group. Then the MLE for the rate in the exposed group, $\\hat{\\lambda}_1 = \\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1)$, is:\n$$\n\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1) = \\frac{Y_1}{T_1}\n$$\nNow, let's use the first score equation. Let $Y_0 = \\sum_{i: x_i=0} y_i$ and $T_0 = \\sum_{i: x_i=0} t_i$.\n$$\n\\sum_{i=1}^N y_i = \\sum_{i=1}^N \\mu_i = \\sum_{i:x_i=0} t_i \\exp(\\beta_0) + \\sum_{i:x_i=1} t_i \\exp(\\beta_0 + \\beta_1)\n$$\n$$\nY_0 + Y_1 = T_0 \\exp(\\beta_0) + T_1 \\exp(\\beta_0 + \\beta_1)\n$$\nSubstituting the result from the second score equation, $T_1 \\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1) = Y_1$, we get:\n$$\nY_0 + Y_1 = T_0 \\exp(\\hat{\\beta}_0) + Y_1 \\implies Y_0 = T_0 \\exp(\\hat{\\beta}_0)\n$$\nThus, the MLE for the rate in the unexposed group, $\\hat{\\lambda}_0 = \\exp(\\hat{\\beta}_0)$, is:\n$$\n\\exp(\\hat{\\beta}_0) = \\frac{Y_0}{T_0}\n$$\nThis simplification into a closed-form solution is a special case that arises because the model's only predictor is a single binary variable. The analysis of the Hessian matrix confirms that these solutions correspond to a unique maximum.\n\nThe quantity of interest is the rate ratio, $RR = \\exp(\\beta_1)$. The MLE for the rate ratio is:\n$$\n\\exp(\\hat{\\beta}_1) = \\frac{\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1)}{\\exp(\\hat{\\beta}_0)} = \\frac{Y_1/T_1}{Y_0/T_0}\n$$\nThis elegant result shows that the MLE of the rate ratio is simply the ratio of the empirical rates in the two exposure groups. This formula will be used to compute the result for each test case. As all test cases have $Y_0 > 0$, $Y_1 > 0$, $T_0 > 0$, and $T_1 > 0$, the MLE is well-defined and positive.\n\nThe calculation for each case proceeds by:\n1.  Summing the counts ($Y$) and person-times ($T$) separately for the unexposed group ($X=0$) to get $Y_0$ and $T_0$.\n2.  Summing the counts ($Y$) and person-times ($T$) separately for the exposed group ($X=1$) to get $Y_1$ and $T_1$.\n3.  Computing the rate ratio using the derived formula $\\exp(\\hat{\\beta}_1) = (Y_1/T_1) / (Y_0/T_0)$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the maximum likelihood estimate of the rate ratio for a Poisson\n    log-linear model for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        (\n            np.array([2, 1, 0, 3, 4, 2, 1, 6], dtype=np.float64),\n            np.array([0, 0, 0, 0, 1, 1, 1, 1], dtype=np.int64),\n            np.array([1.2, 0.8, 1.0, 2.0, 1.0, 1.5, 0.5, 2.0], dtype=np.float64),\n        ),\n        # Case B\n        (\n            np.array([0, 1, 8, 0, 0, 15], dtype=np.float64),\n            np.array([0, 0, 0, 1, 1, 1], dtype=np.int64),\n            np.array([0.2, 0.3, 5.0, 0.1, 0.4, 4.0], dtype=np.float64),\n        ),\n        # Case C\n        (\n            np.array([0, 1, 0, 0, 1, 0, 1, 0], dtype=np.float64),\n            np.array([0, 0, 0, 0, 1, 1, 1, 1], dtype=np.int64),\n            np.array([5.0, 10.0, 8.0, 7.0, 4.0, 9.0, 6.0, 8.0], dtype=np.float64),\n        ),\n        # Case D\n        (\n            np.array([40, 60, 70, 50], dtype=np.float64),\n            np.array([0, 0, 1, 1], dtype=np.int64),\n            np.array([2.0, 3.0, 1.0, 4.0], dtype=np.float64),\n        ),\n    ]\n\n    results = []\n    for Y, X, T in test_cases:\n        # The MLE of the rate ratio for a simple binary predictor X is\n        # the ratio of the empirical rates in the two groups (X=1 vs X=0).\n        # Rate = Total Counts / Total Person-Time\n\n        # Select data for the unexposed group (X=0)\n        unexposed_mask = (X == 0)\n        Y_0 = np.sum(Y[unexposed_mask])\n        T_0 = np.sum(T[unexposed_mask])\n        \n        # Select data for the exposed group (X=1)\n        exposed_mask = (X == 1)\n        Y_1 = np.sum(Y[exposed_mask])\n        T_1 = np.sum(T[exposed_mask])\n\n        # Calculate the empirical rates for each group\n        # The problem statement ensures T_0, T_1, and Y_0 are non-zero.\n        rate_0 = Y_0 / T_0\n        rate_1 = Y_1 / T_1\n        \n        # The MLE for the rate ratio is the ratio of the empirical rates.\n        rate_ratio = rate_1 / rate_0\n        results.append(rate_ratio)\n\n    # Final print statement in the exact required format.\n    # Each value is rounded to six decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "4826677"}, {"introduction": "Once a model is fitted, the next step is to translate its parameters into clinically meaningful predictions. This practice focuses on interpreting the output of a Poisson regression model to predict event rates for a specific patient profile [@problem_id:4978314]. You will go beyond simple point estimates to construct a prediction interval, a crucial skill that involves quantifying both the uncertainty in the model's parameters and the inherent randomness of the event-generating process.", "problem": "A multicenter cohort study in cardiovascular medicine recorded counts of myocardial infarctions over varying person-time exposure. Investigators fit a Generalized Linear Model (GLM) with Poisson likelihood and a log link to model event counts. For individual or stratum $i$ with exposure $E_{i}$, covariates $\\mathbf{x}_{i}$, and count $Y_{i}$, the model is specified by the intensity parameter $\\mu_{i}$ satisfying\n$$\nY_{i} \\sim \\mathrm{Poisson}(\\mu_{i}), \\quad \\ln(\\mu_{i}) \\;=\\; \\ln(E_{i}) \\;+\\; \\mathbf{x}_{i}^{\\top}\\boldsymbol{\\beta},\n$$\nso that the rate per person-year is $\\lambda_{i} \\equiv \\mu_{i}/E_{i} = \\exp(\\mathbf{x}_{i}^{\\top}\\boldsymbol{\\beta})$.\n\nA fitted model on aggregated data provides the maximum likelihood estimate $\\widehat{\\boldsymbol{\\beta}}$ and its covariance matrix $\\widehat{\\mathrm{Var}}(\\widehat{\\boldsymbol{\\beta}})$. Consider the covariate vector $\\mathbf{x}^{\\star} = (1,\\,\\text{age10},\\,\\text{male},\\,\\text{treat})^{\\top}$ with the following profile: age $50$ years so that $\\text{age10} = 5$, male so that $\\text{male} = 1$, and untreated so that $\\text{treat} = 0$. The fitted coefficients and covariance matrix (in the order $(\\beta_{0}, \\beta_{1}, \\beta_{2}, \\beta_{3})$ corresponding to $(\\text{Intercept}, \\text{age10}, \\text{male}, \\text{treat})$) are:\n$$\n\\widehat{\\boldsymbol{\\beta}} \\;=\\; \\begin{pmatrix} -4.6 \\\\ 0.15 \\\\ 0.25 \\\\ -0.30 \\end{pmatrix},\n\\qquad\n\\widehat{\\mathrm{Var}}(\\widehat{\\boldsymbol{\\beta}}) \\;=\\;\n\\begin{bmatrix}\n0.0225 & -0.0010 & 0.0005 & 0.0002 \\\\\n-0.0010 & 0.0025 & -0.0003 & 0.0001 \\\\\n0.0005 & -0.0003 & 0.0100 & 0.0002 \\\\\n0.0002 & 0.0001 & 0.0002 & 0.0100\n\\end{bmatrix}.\n$$\n\nTasks:\n1. Using the model definition and the given estimates, translate the fitted model into the predicted rate per $1000$ person-years for the specified covariate profile by explicitly identifying the linear predictor and the offset. Justify the transformation from the rate per person-year to counts per $1000$ person-years.\n2. Starting from the Poisson variance property $\\mathrm{Var}(Y_{i}\\mid \\mu_{i})=\\mu_{i}$ and the large-sample normality of $\\widehat{\\boldsymbol{\\beta}}$, derive an approximation for a $95\\%$ prediction interval on the rate scale for a new observation with exposure $E^{\\star}$ under covariate profile $\\mathbf{x}^{\\star}$. Your derivation must proceed on the log-rate scale, clearly showing the contribution of parameter uncertainty and the contribution of Poisson sampling variability, and must use the delta method to justify the variance term attributable to the new observation.\n3. Take $E^{\\star} = 1000$ person-years so that the “rate per $1000$ person-years” equals the count over $1000$ person-years. Compute the upper endpoint of the $95\\%$ prediction interval for the observed number of events per $1000$ person-years under the specified covariate profile. Round your final numeric answer to $4$ significant figures. Express the final result in “counts per $1000$ person-years.”", "solution": "The user wants to solve a problem based on a statistical model.\n\nThe problem is a standard application of Poisson regression, a widely used statistical model for count data, with well-defined tasks. It requires the calculation of a predicted rate, the derivation of a prediction interval, and a final numerical computation. The provided data (coefficients, covariance matrix) are consistent and sufficient for solving the problem. The question is scientifically grounded in statistical theory, well-posed, and objective. Therefore, the problem is valid.\n\nThe solution is organized into three parts, corresponding to the three tasks in the problem statement.\n\n### 1. Predicted Rate for the Specified Covariate Profile\n\nThe problem specifies a Poisson Generalized Linear Model (GLM) for the count of events $Y_{i}$ with exposure $E_{i}$ and covariates $\\mathbf{x}_{i}$. The model is:\n$$\nY_{i} \\sim \\mathrm{Poisson}(\\mu_{i})\n$$\n$$\n\\ln(\\mu_{i}) = \\ln(E_{i}) + \\mathbf{x}_{i}^{\\top}\\boldsymbol{\\beta}\n$$\nThe term $\\ln(E_{i})$ is an **offset**, which is a known predictor with a coefficient fixed to $1$. The term $\\eta_{i} = \\mathbf{x}_{i}^{\\top}\\boldsymbol{\\beta}$ is the **linear predictor**. The link function connecting the mean $\\mu_{i}$ to the linear predictor and offset is the logarithm, i.e., a log link.\n\nThe rate of events per unit of exposure (person-year) is $\\lambda_{i} = \\mu_{i}/E_{i}$. Taking the logarithm of this definition and substituting the model equation gives:\n$$\n\\ln(\\lambda_{i}) = \\ln(\\mu_{i}) - \\ln(E_{i}) = (\\ln(E_{i}) + \\mathbf{x}_{i}^{\\top}\\boldsymbol{\\beta}) - \\ln(E_{i}) = \\mathbf{x}_{i}^{\\top}\\boldsymbol{\\beta}\n$$\nThus, the predicted log-rate is equal to the linear predictor, and the predicted rate is $\\widehat{\\lambda}_{i} = \\exp(\\mathbf{x}_{i}^{\\top}\\widehat{\\boldsymbol{\\beta}})$.\n\nThe specified covariate profile is for a subject of age $50$ years, who is male and untreated. This corresponds to:\n- $\\text{age10} = 50/10 = 5$\n- $\\text{male} = 1$\n- $\\text{treat} = 0$\nThe covariate vector, including a $1$ for the intercept, is $\\mathbf{x}^{\\star} = (1, 5, 1, 0)^{\\top}$.\n\nThe estimated coefficient vector is $\\widehat{\\boldsymbol{\\beta}} = (-4.6, 0.15, 0.25, -0.30)^{\\top}$.\n\nThe predicted linear predictor for this profile is:\n$$\n\\widehat{\\eta}^{\\star} = {\\mathbf{x}^{\\star}}^{\\top}\\widehat{\\boldsymbol{\\beta}} = \\begin{pmatrix} 1 & 5 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} -4.6 \\\\ 0.15 \\\\ 0.25 \\\\ -0.30 \\end{pmatrix}\n$$\n$$\n\\widehat{\\eta}^{\\star} = (1)(-4.6) + (5)(0.15) + (1)(0.25) + (0)(-0.30) = -4.6 + 0.75 + 0.25 = -3.6\n$$\nThis is the predicted log-rate per person-year. The predicted rate per person-year is:\n$$\n\\widehat{\\lambda}^{\\star} = \\exp(\\widehat{\\eta}^{\\star}) = \\exp(-3.6)\n$$\nThe question asks for the predicted rate per $1000$ person-years. If the rate is $\\widehat{\\lambda}^{\\star}$ events per person-year, then over an exposure of $1000$ person-years, the expected number of events is $1000 \\times \\widehat{\\lambda}^{\\star}$. Thus, the rate per $1000$ person-years is $1000$ times the rate per person-year.\n$$\n\\text{Predicted Rate per 1000 Person-Years} = 1000 \\widehat{\\lambda}^{\\star} = 1000 \\exp(-3.6) \\approx 27.253\n$$\n\n### 2. Derivation of the 95% Prediction Interval\n\nA prediction interval for a new observation must account for two sources of uncertainty:\n1.  **Parameter Uncertainty**: The uncertainty in the estimation of the model parameters $\\boldsymbol{\\beta}$, captured by $\\widehat{\\mathrm{Var}}(\\widehat{\\boldsymbol{\\beta}})$.\n2.  **Sampling Variability**: The inherent randomness of a new observation, which for a Poisson model is described by the Poisson distribution.\n\nThe derivation proceeds on the log-rate scale, as suggested. Let $Y^{\\star}$ be a new observation with covariate profile $\\mathbf{x}^{\\star}$ and exposure $E^{\\star}$. We model $Y^{\\star} \\sim \\mathrm{Poisson}(\\mu^{\\star})$, where $\\mu^{\\star} = E^{\\star} \\lambda^{\\star}$ and $\\ln(\\lambda^{\\star}) = {\\mathbf{x}^{\\star}}^{\\top}\\boldsymbol{\\beta}$.\n\nOur point prediction for the log-rate is $\\ln(\\widehat{\\lambda}^{\\star}) = {\\mathbf{x}^{\\star}}^{\\top}\\widehat{\\boldsymbol{\\beta}}$. Due to the large-sample normality of the maximum likelihood estimate $\\widehat{\\boldsymbol{\\beta}}$, $\\ln(\\widehat{\\lambda}^{\\star})$ is approximately normal. The variance of this prediction, arising from parameter uncertainty, is:\n$$\n\\mathrm{Var}(\\ln(\\widehat{\\lambda}^{\\star})) = \\mathrm{Var}({\\mathbf{x}^{\\star}}^{\\top}\\widehat{\\boldsymbol{\\beta}}) = {\\mathbf{x}^{\\star}}^{\\top} \\mathrm{Var}(\\widehat{\\boldsymbol{\\beta}}) \\mathbf{x}^{\\star}\n$$\nThis is the **contribution of parameter uncertainty**.\n\nNow, consider the new observation $Y^{\\star}$. We analyze its variability on the log scale. Let's define an \"observed\" log-rate from this single new observation as $\\ln(Y^{\\star}/E^{\\star})$. The randomness here comes from $Y^{\\star}$. We need to find the variance of $\\ln(Y^{\\star})$. We use the delta method, which approximates the variance of a function $g(X)$ of a random variable $X$ as $\\mathrm{Var}(g(X)) \\approx [g'(\\mathrm{E}[X])]^2 \\mathrm{Var}(X)$.\n\nHere, the random variable is $Y^{\\star}$, $g(Y^{\\star})=\\ln(Y^{\\star})$, $\\mathrm{E}[Y^{\\star}] = \\mu^{\\star}$, and from the Poisson property, $\\mathrm{Var}(Y^{\\star}) = \\mu^{\\star}$. The derivative is $g'(y) = 1/y$. Applying the delta method:\n$$\n\\mathrm{Var}(\\ln(Y^{\\star})) \\approx \\left(\\frac{1}{\\mathrm{E}[Y^{\\star}]}\\right)^2 \\mathrm{Var}(Y^{\\star}) = \\left(\\frac{1}{\\mu^{\\star}}\\right)^2 \\mu^{\\star} = \\frac{1}{\\mu^{\\star}}\n$$\nSince $\\ln(Y^{\\star}/E^{\\star}) = \\ln(Y^{\\star}) - \\ln(E^{\\star})$ and $\\ln(E^{\\star})$ is a constant, $\\mathrm{Var}(\\ln(Y^{\\star}/E^{\\star})) = \\mathrm{Var}(\\ln(Y^{\\star})) \\approx 1/\\mu^{\\star}$. This is the **contribution of Poisson sampling variability**, transformed to the log-rate scale.\n\nThe prediction error on the log-rate scale is the difference between the new (random) log-rate and our predicted log-rate. Assuming the new observation is independent of the data used to fit the model, the variance of the prediction error is the sum of the two variance components:\n$$\n\\sigma^2_{\\text{pred}} = \\mathrm{Var}(\\ln(Y^{\\star}/E^{\\star}) - \\ln(\\widehat{\\lambda}^{\\star})) = \\mathrm{Var}(\\ln(Y^{\\star}/E^{\\star})) + \\mathrm{Var}(\\ln(\\widehat{\\lambda}^{\\star})) \\approx \\frac{1}{\\mu^{\\star}} + {\\mathbf{x}^{\\star}}^{\\top} \\mathrm{Var}(\\widehat{\\boldsymbol{\\beta}}) \\mathbf{x}^{\\star}\n$$\nIn practice, we must estimate this variance by plugging in our estimates for the unknown parameters:\n$$\n\\widehat{\\sigma}^2_{\\text{pred}} = \\frac{1}{\\widehat{\\mu}^{\\star}} + {\\mathbf{x}^{\\star}}^{\\top} \\widehat{\\mathrm{Var}}(\\widehat{\\boldsymbol{\\beta}}) \\mathbf{x}^{\\star} \\quad \\text{where} \\quad \\widehat{\\mu}^{\\star} = E^{\\star}\\exp({\\mathbf{x}^{\\star}}^{\\top}\\widehat{\\boldsymbol{\\beta}})\n$$\nAn approximate $95\\%$ prediction interval for the log-rate of the new observation is formed by taking the point estimate $\\pm z_{0.975}$ times the standard error of prediction:\n$$\n\\text{PI for } \\ln(Y^{\\star}/E^{\\star}): \\quad {\\mathbf{x}^{\\star}}^{\\top}\\widehat{\\boldsymbol{\\beta}} \\pm z_{0.975} \\sqrt{\\widehat{\\sigma}^2_{\\text{pred}}}\n$$\nTo obtain the prediction interval for the count $Y^{\\star}$, we exponentiate the endpoints to get the interval for the rate $\\lambda^{\\star} = Y^{\\star}/E^{\\star}$, and then multiply by the exposure $E^{\\star}$:\n$$\n\\text{PI for } Y^{\\star}: \\quad E^{\\star} \\exp \\left( {\\mathbf{x}^{\\star}}^{\\top}\\widehat{\\boldsymbol{\\beta}} \\pm z_{0.975} \\sqrt{{\\mathbf{x}^{\\star}}^{\\top} \\widehat{\\mathrm{Var}}(\\widehat{\\boldsymbol{\\beta}}) \\mathbf{x}^{\\star} + \\frac{1}{E^{\\star}\\exp({\\mathbf{x}^{\\star}}^{\\top}\\widehat{\\boldsymbol{\\beta}})}} \\right)\n$$\n\n### 3. Calculation of the Upper Endpoint of the Prediction Interval\n\nWe are asked to compute the upper endpoint of the $95\\%$ prediction interval for the number of events for the specified profile, with an exposure of $E^{\\star} = 1000$ person-years. We use a $z$-score of $z_{0.975} \\approx 1.96$.\n\n**Step 1: Calculate the linear predictor and predicted mean count.**\nFrom Part 1, the linear predictor is $\\widehat{\\eta}^{\\star} = {\\mathbf{x}^{\\star}}^{\\top}\\widehat{\\boldsymbol{\\beta}} = -3.6$.\nThe predicted mean count is $\\widehat{\\mu}^{\\star} = E^{\\star}\\exp(\\widehat{\\eta}^{\\star}) = 1000\\exp(-3.6)$.\n\n**Step 2: Calculate the variance from parameter uncertainty.**\nThis is the quadratic form ${\\mathbf{x}^{\\star}}^{\\top} \\widehat{\\mathrm{Var}}(\\widehat{\\boldsymbol{\\beta}}) \\mathbf{x}^{\\star}$ with $\\mathbf{x}^{\\star} = (1, 5, 1, 0)^{\\top}$ and $V = \\widehat{\\mathrm{Var}}(\\widehat{\\boldsymbol{\\beta}})$.\n$$\n{\\mathbf{x}^{\\star}}^{\\top} V \\mathbf{x}^{\\star} = \\sum_{j=0}^{3}\\sum_{k=0}^{3} x_j x_k V_{jk}\n$$\nSince $x_3=0$, we only need the upper-left $3 \\times 3$ submatrix of $V$.\n\\begin{align*}\n{\\mathbf{x}^{\\star}}^{\\top} V \\mathbf{x}^{\\star} &= (1)^2 V_{00} + (5)^2 V_{11} + (1)^2 V_{22} + 2(1)(5)V_{01} + 2(1)(1)V_{02} + 2(5)(1)V_{12} \\\\\n&= (1)(0.0225) + (25)(0.0025) + (1)(0.0100) + (10)(-0.0010) + (2)(0.0005) + (10)(-0.0003) \\\\\n&= 0.0225 + 0.0625 + 0.0100 - 0.0100 + 0.0010 - 0.0030 \\\\\n&= 0.0850 - 0.0020 = 0.0830\n\\end{align*}\n\n**Step 3: Calculate the total prediction variance.**\nThe variance component from sampling variability is $1/\\widehat{\\mu}^{\\star} = 1/(1000\\exp(-3.6)) = \\exp(3.6)/1000$.\n$$\n\\widehat{\\sigma}^2_{\\text{pred}} = {\\mathbf{x}^{\\star}}^{\\top} \\widehat{\\mathrm{Var}}(\\widehat{\\boldsymbol{\\beta}}) \\mathbf{x}^{\\star} + \\frac{1}{\\widehat{\\mu}^{\\star}} = 0.0830 + \\frac{\\exp(3.6)}{1000}\n$$\nNumerically, $\\exp(3.6) \\approx 36.59823$.\n$$\n\\widehat{\\sigma}^2_{\\text{pred}} \\approx 0.0830 + \\frac{36.59823}{1000} = 0.0830 + 0.03659823 = 0.11959823\n$$\nThe standard error of prediction is $\\widehat{\\sigma}_{\\text{pred}} = \\sqrt{0.11959823} \\approx 0.3458298$.\n\n**Step 4: Calculate the upper endpoint of the prediction interval.**\nThe upper endpoint for the count $Y^{\\star}$ is given by:\n$$\nY^{\\star}_{\\text{upper}} = E^{\\star} \\exp(\\widehat{\\eta}^{\\star} + z_{0.975} \\widehat{\\sigma}_{\\text{pred}}) = 1000 \\exp(-3.6 + 1.96 \\times 0.3458298)\n$$\n$$\nY^{\\star}_{\\text{upper}} \\approx 1000 \\exp(-3.6 + 0.6778264) = 1000 \\exp(-2.9221736)\n$$\n$$\nY^{\\star}_{\\text{upper}} \\approx 1000 \\times 0.053818 = 53.818\n$$\nRounding to $4$ significant figures, the upper endpoint is $53.82$. This represents the upper bound for the predicted number of events per $1000$ person-years.", "answer": "$$\n\\boxed{53.82}\n$$", "id": "4978314"}, {"introduction": "A critical step in any statistical analysis is to assess how well the model fits the data. The Poisson model makes a strong assumption that the variance of the counts is equal to their mean, a condition often violated in practice, leading to a phenomenon called overdispersion. This exercise challenges you to use a standard goodness-of-fit metric, the Pearson chi-square statistic, to diagnose overdispersion from a model's output and to understand its critical implications for the validity of your statistical inferences [@problem_id:4826692].", "problem": "A hospital quality improvement study monitors the number of hospital-onset bloodstream infections in adult wards over $n = 130$ ward-weeks. For each ward-week $i$, the count $Y_i$ is assumed conditionally independent given covariates, with a Poisson working model motivated by the empirical fact that, for a Poisson random variable, the mean equals the variance. The investigators fit a Poisson generalized linear model (GLM) with a log link and an offset for exposure (patient-days), specified as $\\log(\\mu_i) = \\log(\\mathrm{exposure}_i) + \\beta_0 + \\beta_1 \\,\\mathrm{Intervention}_i + \\cdots$, estimating $p = 7$ regression coefficients by maximum likelihood. The software reports a Pearson chi-square statistic, defined by the sum of squared Pearson residuals, with value $X^2 = 165.4$, and also reports the model deviance $D = 171.2$.\n\nFrom first principles, consider how the Pearson chi-square statistic arises from standardizing residuals by the model-implied variance when the Poisson mean-variance relation holds, and how the sampling distribution of $X^2$ is approximated after fitting $p$ parameters. Then interpret $X^2$ relative to its degrees of freedom in this study and state the implications for assessing dispersion and inference on regression coefficients.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. Under correct Poisson mean-variance specification and correct mean structure, $X^2$ is approximately chi-square distributed with $\\mathrm{df} = n - p$; equivalently, $E[X^2] \\approx n - p$, so $X^2/\\mathrm{df} \\approx 1$ in large samples. With $n = 130$ and $p = 7$, $\\mathrm{df} = 123$ and $X^2/\\mathrm{df} \\approx 165.4/123 \\approx 1.34$, suggesting overdispersion; a quasi-Poisson analysis would inflate standard errors by a factor of $\\sqrt{1.34}$.\n\nB. The Pearson chi-square statistic is exactly chi-square with $\\mathrm{df} = n$ regardless of how many parameters are estimated, so $E[X^2/\\mathrm{df}] = 1$ with $\\mathrm{df} = n$. With $n = 130$, $165.4/130 \\approx 1.27$ indicates no material overdispersion.\n\nC. The Pearson chi-square statistic is a likelihood ratio comparing the fitted model to a saturated model, so it has an exact chi-square distribution with $\\mathrm{df} = n - p$. Consequently, only the deviance $D$ should be used to assess dispersion; $X^2$ is not informative about dispersion.\n\nD. Because Poisson counts are not normally distributed, the sampling distribution of $X^2$ is unknown; therefore, values of $X^2/\\mathrm{df} > 1$ indicate underdispersion. In such cases, a zero-inflated Poisson model should always be preferred.\n\nE. If the mean structure is misspecified (for example, important covariates are omitted), then $X^2/\\mathrm{df} > 1$ can occur even without extra-Poisson variation; practical remedies include adopting a negative binomial mean-variance relationship or retaining the Poisson mean with a scale parameter to adjust standard errors without changing the mean structure.", "solution": "## Problem Validation\n\n### Step 1: Extract Givens\nThe problem statement provides the following information:\n-   Sample size: $n = 130$ ward-weeks.\n-   Response variable: $Y_i$, a count of infections, assumed to follow a Poisson working model.\n-   Model: A Poisson generalized linear model (GLM) with a log link and an offset.\n-   Model structure: $\\log(\\mu_i) = \\log(\\mathrm{exposure}_i) + \\beta_0 + \\beta_1 \\,\\mathrm{Intervention}_i + \\cdots$\n-   Number of estimated regression coefficients: $p = 7$.\n-   Pearson chi-square statistic: $X^2 = 165.4$.\n-   Definition of Pearson chi-square statistic: The sum of squared Pearson residuals, $\\sum_i (y_i - \\hat{\\mu}_i)^2 / \\hat{\\mu}_i$.\n-   Model deviance: $D = 171.2$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to validation against the required criteria.\n-   **Scientifically Grounded**: The problem describes a standard biostatistical analysis using a Poisson GLM for count data, a fundamental and widely-used technique in medical research and epidemiology. The concepts of Pearson chi-square, deviance, overdispersion, quasi-Poisson models, and negative binomial models are all established pillars of statistical theory and practice. The problem is free of pseudoscience and is factually sound.\n-   **Well-Posed**: The problem is clearly stated and provides all necessary numerical values ($n$, $p$, $X^2$, $D$) to perform the requested interpretation and evaluation. A unique and meaningful statistical assessment is possible.\n-   **Objective**: The language is precise, unbiased, and objective, describing a hypothetical but realistic statistical scenario.\n-   **Completeness and Consistency**: The problem is self-contained and the provided data are internally consistent. The values for $n$, $p$, $X^2$, and $D$ are plausible for a real-world dataset.\n-   **No other flaws**: The problem is not trivial, unrealistic, ill-posed, or metaphorical. It poses a substantive question about the interpretation of goodness-of-fit statistics in the context of GLMs.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. The solution will now be derived.\n\n## Solution Derivation\n\nThis problem requires an assessment of goodness-of-fit for a Poisson generalized linear model (GLM), focusing on the Pearson chi-square statistic, $X^2$.\n\n### First Principles of Poisson GLMs and Goodness-of-Fit\n\n1.  **The Poisson Model:** A random variable $Y$ is said to follow a Poisson distribution with mean $\\mu$ if its probability mass function is $P(Y=y) = (\\mu^y e^{-\\mu}) / y!$ for $y = 0, 1, 2, \\dots$. A key property of the Poisson distribution is that its variance is equal to its mean: $\\mathrm{E}[Y] = \\mathrm{Var}(Y) = \\mu$. In a Poisson GLM, we model the mean $\\mu_i$ for each observation $i$ as a function of covariates. The problem specifies a log link function and an offset term:\n    $$ \\log(\\mu_i) = \\log(\\text{exposure}_i) + \\eta_i $$\n    where $\\eta_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}$ is the linear predictor. This is equivalent to modeling the rate $\\lambda_i = \\mu_i / \\text{exposure}_i$ as $\\log(\\lambda_i) = \\mathbf{x}_i^T \\boldsymbol{\\beta}$.\n\n2.  **Pearson Residuals and the Chi-Square Statistic:** After fitting the model to obtain estimates $\\hat{\\boldsymbol{\\beta}}$ and fitted means $\\hat{\\mu}_i$, we can define residuals. The Pearson residual for observation $i$ standardizes the raw residual ($y_i - \\hat{\\mu}_i$) by the estimated standard deviation of the observation under the model's variance assumption:\n    $$ r_{P,i} = \\frac{y_i - \\hat{\\mu}_i}{\\sqrt{\\mathrm{Var}(y_i)}} = \\frac{y_i - \\hat{\\mu}_i}{\\sqrt{\\hat{\\mu}_i}} $$\n    The Pearson chi-square statistic, $X^2$, is the sum of the squares of these residuals:\n    $$ X^2 = \\sum_{i=1}^{n} r_{P,i}^2 = \\sum_{i=1}^{n} \\frac{(y_i - \\hat{\\mu}_i)^2}{\\hat{\\mu}_i} $$\n\n3.  **Sampling Distribution of $X^2$ and Dispersion:** According to large-sample asymptotic theory, if the Poisson model is correctly specified (both the mean structure and the mean-variance relationship are correct), then the statistic $X^2$ follows an approximate chi-square distribution with degrees of freedom equal to the number of observations ($n$) minus the number of estimated parameters ($p$).\n    $$ X^2 \\approx \\chi^2_{n-p} $$\n    The expected value of a $\\chi^2_k$ random variable is $k$. Therefore, if the model is correct, we expect $E[X^2] \\approx n-p$. This leads to the definition of the dispersion parameter, $\\phi$, which is estimated by:\n    $$ \\hat{\\phi} = \\frac{X^2}{n-p} $$\n    If the model holds, $\\hat{\\phi}$ should be close to $1$.\n    -   $\\hat{\\phi} > 1$ indicates **overdispersion**, meaning the observed variance is greater than the mean.\n    -   $\\hat{\\phi} < 1$ indicates **underdispersion**, meaning the observed variance is less than the mean.\n\n4.  **Applying to the Study Data:**\n    -   $n = 130$\n    -   $p = 7$\n    -   $X^2 = 165.4$\n    The degrees of freedom are $\\mathrm{df} = n-p = 130 - 7 = 123$.\n    The estimated dispersion is $\\hat{\\phi} = \\frac{165.4}{123} \\approx 1.3447$.\n    Since $\\hat{\\phi} \\approx 1.34$ is substantially greater than $1$, there is evidence of overdispersion.\n\n5.  **Implications of Overdispersion:** Unaccounted-for overdispersion means that the model's assumption of $\\mathrm{Var}(Y_i) = \\mu_i$ is incorrect; the true variance is larger. While the maximum likelihood estimates of $\\beta_j$ remain consistent, their standard errors are underestimated. This leads to artificially small p-values and overly narrow confidence intervals, increasing the risk of Type I errors.\n\n6.  **Remedies for Overdispersion:**\n    -   **Quasi-Poisson Model:** This approach assumes $\\mathrm{Var}(Y_i) = \\phi \\mu_i$. It uses the same coefficient estimates as the Poisson model but adjusts the variance-covariance matrix of the estimators by multiplying it by $\\hat{\\phi}$. Consequently, the standard errors of the coefficients are inflated by a factor of $\\sqrt{\\hat{\\phi}}$.\n    -   **Negative Binomial Model:** This is a fully parametric approach that replaces the Poisson likelihood with the negative binomial likelihood. The negative binomial distribution has the variance function $\\mathrm{Var}(Y_i) = \\mu_i + \\alpha \\mu_i^2$, which explicitly models overdispersion. This will generally lead to different coefficient estimates and standard errors.\n    -   **Model Respecification:** Overdispersion can also be a symptom of a misspecified mean structure (e.g., omitted covariates, incorrect functional form for a predictor, failure to include interactions). The most fundamental solution, if possible, is to improve the model for the mean.\n\n## Option-by-Option Analysis\n\n**A. Under correct Poisson mean-variance specification and correct mean structure, $X^2$ is approximately chi-square distributed with $\\mathrm{df} = n - p$; equivalently, $E[X^2] \\approx n - p$, so $X^2/\\mathrm{df} \\approx 1$ in large samples. With $n = 130$ and $p = 7$, $\\mathrm{df} = 123$ and $X^2/\\mathrm{df} \\approx 165.4/123 \\approx 1.34$, suggesting overdispersion; a quasi-Poisson analysis would inflate standard errors by a factor of $\\sqrt{1.34}$.**\nThis statement is fully consistent with the first principles outlined above. It correctly states the asymptotic distribution of $X^2$, the calculation of the degrees of freedom $\\mathrm{df} = n-p = 123$, the correct calculation of the dispersion parameter $\\hat{\\phi} \\approx 1.34$, the correct interpretation of this value as indicating overdispersion, and the correct procedure for adjusting standard errors in a quasi-Poisson framework (multiplying by $\\sqrt{\\hat{\\phi}}$).\n**Verdict: Correct.**\n\n**B. The Pearson chi-square statistic is exactly chi-square with $\\mathrm{df} = n$ regardless of how many parameters are estimated, so $E[X^2/\\mathrm{df}] = 1$ with $\\mathrm{df} = n$. With $n = 130$, $165.4/130 \\approx 1.27$ indicates no material overdispersion.**\nThis statement contains multiple fundamental errors. First, the chi-square distribution is an *approximation*, not an *exact* result. Second, the degrees of freedom must be adjusted for the number of estimated parameters, so the correct value is $\\mathrm{df} = n-p$, not $\\mathrm{df} = n$. Failing to subtract $p$ is a critical mistake. The conclusion that there is no material overdispersion is based on this incorrect $\\mathrm{df}$.\n**Verdict: Incorrect.**\n\n**C. The Pearson chi-square statistic is a likelihood ratio comparing the fitted model to a saturated model, so it has an exact chi-square distribution with $\\mathrm{df} = n - p$. Consequently, only the deviance $D$ should be used to assess dispersion; $X^2$ is not informative about dispersion.**\nThis statement incorrectly defines the Pearson chi-square statistic. The statistic defined as the likelihood ratio test against the saturated model is the **deviance**, $D$. The deviance formula is $D = 2 \\sum_i [y_i \\log(y_i/\\hat{\\mu}_i) - (y_i - \\hat{\\mu}_i)]$. The Pearson statistic $X^2$ is based on squared standardized residuals. Furthermore, the claim that $X^2$ is not informative about dispersion is false; in fact, the dispersion parameter $\\hat{\\phi}$ in quasi-likelihood methods is prototypically estimated using $X^2/(n-p)$. Both $X^2$ and $D$ are informative. Also, the distribution is approximate, not exact.\n**Verdict: Incorrect.**\n\n**D. Because Poisson counts are not normally distributed, the sampling distribution of $X^2$ is unknown; therefore, values of $X^2/\\mathrm{df} > 1$ indicate underdispersion. In such cases, a zero-inflated Poisson model should always be preferred.**\nThis statement is flawed in several ways. While it is true that Poisson counts are not normal, the asymptotic sampling distribution of $X^2$ is well-established as approximately chi-square; it is not \"unknown.\" The interpretation of the dispersion parameter is inverted: $X^2/\\mathrm{df} > 1$ indicates **overdispersion**, not underdispersion. Finally, while zero-inflation is one possible cause of overdispersion, it is not the only cause, and a zero-inflated model should not \"always\" be preferred without further diagnostic checks for an excess of zero counts.\n**Verdict: Incorrect.**\n\n**E. If the mean structure is misspecified (for example, important covariates are omitted), then $X^2/\\mathrm{df} > 1$ can occur even without extra-Poisson variation; practical remedies include adopting a negative binomial mean-variance relationship or retaining the Poisson mean with a scale parameter to adjust standard errors without changing the mean structure.**\nThis statement correctly identifies a crucial point in model diagnostics: misspecification of the systematic part of the model (the mean structure) can produce apparent overdispersion, even if the true conditional distribution is Poisson. The statement then lists two standard and valid \"practical remedies\" for handling the resulting overdispersion: fitting a negative binomial model (a parametric approach) or using a quasi-Poisson model to scale the standard errors (a semi-parametric approach). Both are common strategies employed in practice when $\\hat{\\phi} > 1$ is observed, regardless of its ultimate cause.\n**Verdict: Correct.**", "answer": "$$\\boxed{AE}$$", "id": "4826692"}]}