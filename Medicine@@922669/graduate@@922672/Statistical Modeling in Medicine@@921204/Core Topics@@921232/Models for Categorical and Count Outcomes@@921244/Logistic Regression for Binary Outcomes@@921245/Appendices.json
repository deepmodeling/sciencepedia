{"hands_on_practices": [{"introduction": "Understanding logistic regression begins with its core estimation principles. This first practice grounds you in the theory of maximum likelihood estimation (MLE) by stripping the model down to its simplest form: an intercept-only model. By manually deriving the MLE and its corresponding Wald confidence interval from first principles for a small, hypothetical dataset [@problem_id:4807790], you will gain a deep appreciation for the statistical machinery that underpins the automated outputs of statistical software.", "problem": "A phase II single-arm pilot study in oncology evaluates an investigational agent where the binary outcome is clinical response within six weeks, recorded as $Y_{i} \\in \\{0,1\\}$ for participant $i$. Investigators wish to report the baseline log-odds of response, assuming an intercept-only logistic regression model, which is appropriate when no covariates are included and the probability of response is constant across participants. The sampling model assumes independent Bernoulli trials with a common response probability $p$ and the link function satisfies $\\operatorname{logit}(p)=\\beta_{0}$, where $\\beta_{0}$ is the intercept.\n\nIn the study, $n=10$ participants were enrolled and $k=3$ responses were observed. Under standard regularity conditions for maximum likelihood estimation in logistic regression, derive from first principles the maximum likelihood estimate (MLE) $\\hat{\\beta}_{0}$ and its large-sample Wald $95\\%$ confidence interval (CI) on the log-odds scale, using the Bernoulli likelihood and the Fisher information for $\\beta_{0}$. Use the standard normal two-sided critical value appropriate for a $95\\%$ CI.\n\nReport three numbers: the point estimate $\\hat{\\beta}_{0}$, the lower bound, and the upper bound of the Wald $95\\%$ CI, all on the log-odds scale. Round each reported value to four significant figures.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of statistical inference, specifically maximum likelihood estimation for a logistic regression model. The problem is well-posed, providing all necessary data ($n=10$, $k=3$) and a clear objective: to derive the maximum likelihood estimate and a Wald confidence interval for the log-odds parameter $\\beta_0$. The language is objective and unambiguous. The use of a large-sample approximation for a relatively small sample size ($n=10$) is a standard feature of textbook problems designed to teach the methodology and does not render the problem invalid.\n\nThe derivation proceeds from first principles as follows.\n\nLet the binary outcome for participant $i$ be $Y_i \\in \\{0, 1\\}$, where $Y_i=1$ denotes a clinical response. The outcomes are modeled as independent and identically distributed Bernoulli random variables, $Y_i \\sim \\text{Bernoulli}(p)$, where $p$ is the common probability of response. The sample size is $n=10$, and the total number of observed responses is $k=3$.\n\nThe likelihood function for the observed data, which consists of $k$ responses and $n-k$ non-responses, is given by the product of the individual Bernoulli probabilities:\n$$L(p) = \\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i} = p^{\\sum y_i} (1-p)^{n-\\sum y_i} = p^k (1-p)^{n-k}$$\n\nThe problem specifies an intercept-only logistic regression model, where the log-odds of response is related to the intercept $\\beta_0$ via the logit link function:\n$$\\operatorname{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0$$\nFrom this relationship, we can express the probability $p$ as a function of $\\beta_0$:\n$$p(\\beta_0) = \\frac{\\exp(\\beta_0)}{1+\\exp(\\beta_0)} = (1 + \\exp(-\\beta_0))^{-1}$$\nConsequently, $1-p$ can be expressed as:\n$$1-p(\\beta_0) = 1 - \\frac{\\exp(\\beta_0)}{1+\\exp(\\beta_0)} = \\frac{1}{1+\\exp(\\beta_0)}$$\n\nSubstituting these expressions into the likelihood function gives the likelihood in terms of $\\beta_0$:\n$$L(\\beta_0) = \\left(\\frac{\\exp(\\beta_0)}{1+\\exp(\\beta_0)}\\right)^k \\left(\\frac{1}{1+\\exp(\\beta_0)}\\right)^{n-k} = \\frac{\\exp(k\\beta_0)}{(1+\\exp(\\beta_0))^n}$$\n\nTo find the maximum likelihood estimate (MLE) of $\\beta_0$, we work with the log-likelihood function, $\\ell(\\beta_0) = \\ln(L(\\beta_0))$:\n$$\\ell(\\beta_0) = k\\beta_0 - n\\ln(1+\\exp(\\beta_0))$$\n\nWe find the maximum by differentiating $\\ell(\\beta_0)$ with respect to $\\beta_0$ and setting the result (the score function, $U(\\beta_0)$) to zero:\n$$\\frac{d\\ell}{d\\beta_0} = U(\\beta_0) = k - n \\frac{\\exp(\\beta_0)}{1+\\exp(\\beta_0)} = k - np$$\nSetting $U(\\hat{\\beta}_0)=0$ to find the MLE implies $k - n\\hat{p} = 0$, which yields the MLE for $p$ as $\\hat{p} = \\frac{k}{n}$. By the invariance property of MLEs, the MLE for $\\beta_0$ is:\n$$\\hat{\\beta}_0 = \\operatorname{logit}(\\hat{p}) = \\ln\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right) = \\ln\\left(\\frac{k/n}{1-k/n}\\right) = \\ln\\left(\\frac{k}{n-k}\\right)$$\nSubstituting the given values $n=10$ and $k=3$:\n$$\\hat{\\beta}_0 = \\ln\\left(\\frac{3}{10-3}\\right) = \\ln\\left(\\frac{3}{7}\\right) \\approx -0.84729786$$\n\nNext, we derive the standard error for $\\hat{\\beta}_0$ using the Fisher information. The Fisher information, $I(\\beta_0)$, is the negative of the expected value of the second derivative of the log-likelihood function.\nThe second derivative is:\n$$\\frac{d^2\\ell}{d\\beta_0^2} = \\frac{d}{d\\beta_0}(k - np) = -n\\frac{dp}{d\\beta_0}$$\nSince $p = (1+\\exp(-\\beta_0))^{-1}$, its derivative is $\\frac{dp}{d\\beta_0} = -(1+\\exp(-\\beta_0))^{-2} \\cdot (-\\exp(-\\beta_0)) = \\frac{\\exp(-\\beta_0)}{(1+\\exp(-\\beta_0))^2} = \\frac{1}{1+\\exp(\\beta_0)} \\cdot \\frac{\\exp(\\beta_0)}{1+\\exp(\\beta_0)} = p(1-p)$.\nThus, the second derivative of the log-likelihood is:\n$$\\frac{d^2\\ell}{d\\beta_0^2} = -np(1-p)$$\nSince this expression does not depend on the data $y_i$ (only on the parameter $\\beta_0$ via $p$), its expectation is the expression itself. The Fisher Information for $n$ observations is:\n$$I(\\beta_0) = -E\\left[\\frac{d^2\\ell}{d\\beta_0^2}\\right] = -(-np(1-p)) = np(1-p)$$\nThe asymptotic variance of $\\hat{\\beta}_0$ is the inverse of the Fisher information, evaluated at the true parameter value. We estimate this variance by plugging in the MLE, $\\hat{p}$:\n$$\\widehat{\\operatorname{Var}}(\\hat{\\beta}_0) = [I(\\hat{\\beta}_0)]^{-1} = \\frac{1}{n\\hat{p}(1-\\hat{p})}$$\nThe standard error (SE) is the square root of this estimated variance:\n$$\\operatorname{SE}(\\hat{\\beta}_0) = \\sqrt{\\frac{1}{n\\hat{p}(1-\\hat{p})}} = \\sqrt{\\frac{1}{n\\frac{k}{n}(1-\\frac{k}{n})}} = \\sqrt{\\frac{n}{k(n-k)}}$$\nSubstituting the given values:\n$$\\operatorname{SE}(\\hat{\\beta}_0) = \\sqrt{\\frac{10}{3(10-3)}} = \\sqrt{\\frac{10}{21}} \\approx 0.69006556$$\n\nThe large-sample Wald $95\\%$ confidence interval (CI) for $\\beta_0$ is given by:\n$$\\hat{\\beta}_0 \\pm z_{1-\\alpha/2} \\operatorname{SE}(\\hat{\\beta}_0)$$\nFor a $95\\%$ CI, $\\alpha=0.05$, so we need the critical value $z_{1-0.05/2} = z_{0.975}$ from the standard normal distribution. The standard value is $z_{0.975} \\approx 1.96$.\nThe margin of error (ME) is:\n$$\\text{ME} = 1.96 \\times \\operatorname{SE}(\\hat{\\beta}_0) \\approx 1.96 \\times 0.69006556 \\approx 1.35252849$$\nThe lower bound (LB) of the CI is:\n$$\\text{LB} = \\hat{\\beta}_0 - \\text{ME} \\approx -0.84729786 - 1.35252849 = -2.19982635$$\nThe upper bound (UB) of the CI is:\n$$\\text{UB} = \\hat{\\beta}_0 + \\text{ME} \\approx -0.84729786 + 1.35252849 = 0.50523063$$\n\nFinally, we round the three required values to four significant figures:\nPoint estimate $\\hat{\\beta}_0$: $-0.8473$\nLower bound: $-2.200$\nUpper bound: $0.5052$", "answer": "$$\\boxed{\\begin{pmatrix} -0.8473  -2.200  0.5052 \\end{pmatrix}}$$", "id": "4807790"}, {"introduction": "While the previous exercise established the theoretical target for our estimates, logistic regression models rarely have a simple closed-form solution for their parameters. This next exercise peels back the curtain on the computational engine used to fit these models: the Iteratively Reweighted Least Squares (IRLS) algorithm. By performing a single, complete iteration by hand [@problem_id:4970685], you will see how a complex optimization problem is elegantly solved through a sequence of weighted least squares steps, giving you insight into the mechanics of your statistical software.", "problem": "Consider a binary outcome model for a short perioperative study where each patient $i \\in \\{1,2,3\\}$ contributes a single Bernoulli trial $y_{i} \\in \\{0,1\\}$ indicating occurrence of an acute postoperative complication. Let the covariate vector be $x_{i} \\in \\mathbb{R}^{2}$ with an intercept and a standardized operative complexity score, and denote the parameter vector by $\\beta \\in \\mathbb{R}^{2}$. The generalized linear model uses the logistic link so that the conditional mean is $p_{i} = \\mathbb{E}(y_{i} \\mid x_{i}, \\beta)$ satisfying $\\operatorname{logit}(p_{i}) = x_{i}^{\\top} \\beta$. The data are given by the design matrix $X = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 1  2 \\end{pmatrix}$ and the outcome vector $y = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$. Starting at the initial parameter $\\beta^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, perform one iteration of Iteratively Reweighted Least Squares (IRLS) (first defining the score and observed information from the Bernoulli log-likelihood with the logistic link and then deriving the IRLS quantities from these first principles), and compute the following objects at the initial iterate: the vector of model-predicted probabilities $p^{(0)}$, the diagonal weight matrix $W^{(0)}$, the working response $z^{(0)}$, and the updated parameter vector $\\beta^{(1)}$. Express all computed quantities exactly as rational numbers or integers; no rounding is required. Provide your final answer as a single composite expression containing all four results in the order $\\left(p^{(0)}, W^{(0)}, z^{(0)}, \\beta^{(1)}\\right)$.", "solution": "The user wants to find the predicted probabilities $p^{(0)}$, the weight matrix $W^{(0)}$, the working response $z^{(0)}$, and the updated parameter vector $\\beta^{(1)}$ after one iteration of the Iteratively Reweighted Least Squares (IRLS) algorithm for a logistic regression model.\n\n### Step 1: Problem Validation\n\nThe problem is a standard, well-defined exercise in computational statistics for generalized linear models (GLMs).\n\n*   **Extracted Givens**:\n    *   Model: Logistic regression for a binary outcome $y_i \\in \\{0, 1\\}$.\n    *   Number of observations: $N=3$.\n    *   Link function: $\\operatorname{logit}(p_i) = \\ln\\left(\\frac{p_i}{1-p_i}\\right) = x_i^\\top \\beta$.\n    *   Design matrix: $X = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 1  2 \\end{pmatrix}$.\n    *   Outcome vector: $y = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\n    *   Initial parameter vector: $\\beta^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n    *   Task: Perform one IRLS iteration starting from $\\beta^{(0)}$ and compute $p^{(0)}$, $W^{(0)}$, $z^{(0)}$, and $\\beta^{(1)}$.\n\n*   **Validation Verdict**:\n    *   The problem is **scientifically grounded** in the theory of GLMs.\n    *   It is **well-posed**, providing all necessary information for a single, unique iteration of the IRLS algorithm.\n    *   The language is **objective** and mathematically precise.\n    *   The problem is **valid**.\n\n### Step 2: Derivation of the IRLS Algorithm from First Principles\n\nThe IRLS algorithm for GLMs can be derived as a Newton-Raphson method for maximizing the log-likelihood. For a binary outcome $y_i \\sim \\text{Bernoulli}(p_i)$, the log-likelihood for a single observation $i$ is:\n$$\n\\ell_i(\\beta) = y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i)\n$$\nThe total log-likelihood is $\\ell(\\beta) = \\sum_{i=1}^{N} \\ell_i(\\beta)$. The link function relates the probability $p_i$ to the linear predictor $\\eta_i = x_i^\\top \\beta$:\n$$\n\\eta_i = \\ln\\left(\\frac{p_i}{1-p_i}\\right) \\implies p_i = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = \\frac{1}{1+\\exp(-\\eta_i)}\n$$\nThis is the logistic sigmoid function, $\\sigma(\\eta_i)$. Its derivative is $\\frac{d p_i}{d \\eta_i} = p_i(1-p_i)$.\n\n**1. Score Function (Gradient of the Log-Likelihood)**\nWe find the gradient of $\\ell(\\beta)$ with respect to the parameter vector $\\beta$. Using the chain rule for a single observation's contribution:\n$$\n\\frac{\\partial \\ell_i}{\\partial \\beta_j} = \\frac{\\partial \\ell_i}{\\partial p_i} \\frac{\\partial p_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\nThe components are:\n$$\n\\frac{\\partial \\ell_i}{\\partial p_i} = \\frac{y_i}{p_i} - \\frac{1-y_i}{1-p_i} = \\frac{y_i - p_i}{p_i(1-p_i)}\n$$\n$$\n\\frac{\\partial p_i}{\\partial \\eta_i} = p_i(1-p_i)\n$$\n$$\n\\frac{\\partial \\eta_i}{\\partial \\beta_j} = \\frac{\\partial (x_i^\\top \\beta)}{\\partial \\beta_j} = x_{ij}\n$$\nCombining these gives:\n$$\n\\frac{\\partial \\ell_i}{\\partial \\beta_j} = \\frac{y_i - p_i}{p_i(1-p_i)} \\cdot p_i(1-p_i) \\cdot x_{ij} = (y_i - p_i) x_{ij}\n$$\nThe score vector for the full dataset is the sum over all observations, which can be written in matrix form:\n$$\nS(\\beta) = \\nabla_\\beta \\ell(\\beta) = \\sum_{i=1}^{N} (y_i - p_i) x_i = X^\\top (y - p)\n$$\n\n**2. Observed Information Matrix (Negative Hessian)**\nNext, we compute the Hessian matrix $\\mathcal{H}(\\beta) = \\frac{\\partial^2 \\ell(\\beta)}{\\partial \\beta \\partial \\beta^\\top}$. The entry $(\\mathcal{H})_{jk}$ is:\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\beta_k \\partial \\beta_j} = \\sum_{i=1}^{N} \\frac{\\partial}{\\partial \\beta_k} \\left[ (y_i - p_i) x_{ij} \\right] = \\sum_{i=1}^{N} - \\frac{\\partial p_i}{\\partial \\beta_k} x_{ij}\n$$\nUsing the chain rule again, $\\frac{\\partial p_i}{\\partial \\beta_k} = \\frac{\\partial p_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_k} = p_i(1-p_i) x_{ik}$. Substituting this in:\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\beta_k \\partial \\beta_j} = \\sum_{i=1}^{N} -p_i(1-p_i) x_{ik} x_{ij}\n$$\nIn matrix form, the Hessian is $\\mathcal{H}(\\beta) = -X^\\top W X$, where $W$ is a diagonal matrix with diagonal entries $W_{ii} = p_i(1-p_i)$. The observed information matrix is $J(\\beta) = -\\mathcal{H}(\\beta) = X^\\top W X$.\n\n**3. IRLS Update Step**\nThe Newton-Raphson update to find the maximum of $\\ell(\\beta)$ is:\n$$\n\\beta^{(t+1)} = \\beta^{(t)} - [\\mathcal{H}(\\beta^{(t)})]^{-1} S(\\beta^{(t)})\n$$\nSubstituting our expressions for the score $S$ and Hessian $\\mathcal{H}$:\n$$\n\\beta^{(t+1)} = \\beta^{(t)} - [-X^\\top W^{(t)} X]^{-1} [X^\\top (y - p^{(t)})] = \\beta^{(t)} + (X^\\top W^{(t)} X)^{-1} X^\\top (y - p^{(t)})\n$$\nTo see this as a weighted least squares problem, we rearrange the terms:\n$$\n(X^\\top W^{(t)} X) \\beta^{(t+1)} = (X^\\top W^{(t)} X) \\beta^{(t)} + X^\\top (y - p^{(t)})\n$$\nLet us define a \"working response\" vector $z^{(t)}$ such that the update is the solution to the weighted normal equations: $\\beta^{(t+1)} = (X^\\top W^{(t)} X)^{-1} X^\\top W^{(t)} z^{(t)}$. This implies $X^\\top W^{(t)} z^{(t)} = (X^\\top W^{(t)} X) \\beta^{(t)} + X^\\top (y - p^{(t)})$. We can solve for $z^{(t)}$:\n$$\nW^{(t)} z^{(t)} = W^{(t)} X \\beta^{(t)} + (y - p^{(t)}) \\implies z^{(t)} = X \\beta^{(t)} + (W^{(t)})^{-1} (y - p^{(t)})\n$$\nRecalling that $\\eta^{(t)} = X \\beta^{(t)}$, the working response is:\n$$\nz^{(t)} = \\eta^{(t)} + (W^{(t)})^{-1} (y - p^{(t)})\n$$\n\n### Step 3: Calculation for the First Iteration\n\nGiven the initial parameter vector $\\beta^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\n**1. Compute Probabilities $p^{(0)}$**\nFirst, calculate the linear predictors $\\eta^{(0)} = X \\beta^{(0)}$:\n$$\n\\eta^{(0)} = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nNow, compute the probabilities $p_i^{(0)}$ using the inverse link function $p_i = \\sigma(\\eta_i)$:\n$$\np_i^{(0)} = \\frac{1}{1+\\exp(-\\eta_i^{(0)})} = \\frac{1}{1+\\exp(0)} = \\frac{1}{1+1} = \\frac{1}{2} \\quad \\text{for } i=1,2,3\n$$\nThus, the vector of probabilities is:\n$$\np^{(0)} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix}\n$$\n\n**2. Compute Weight Matrix $W^{(0)}$**\nThe diagonal weights are $W_{ii}^{(0)} = p_i^{(0)}(1 - p_i^{(0)})$:\n$$\nW_{ii}^{(0)} = \\frac{1}{2} \\left(1 - \\frac{1}{2}\\right) = \\frac{1}{4} \\quad \\text{for } i=1,2,3\n$$\nThe weight matrix is:\n$$\nW^{(0)} = \\begin{pmatrix} 1/4  0  0 \\\\ 0  1/4  0 \\\\ 0  0  1/4 \\end{pmatrix}\n$$\n\n**3. Compute Working Response $z^{(0)}$**\nThe working response is $z^{(0)} = \\eta^{(0)} + (W^{(0)})^{-1} (y - p^{(0)})$:\n$$\ny - p^{(0)} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} -1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix}\n$$\nThe inverse of the weight matrix is $(W^{(0)})^{-1} = \\text{diag}(4, 4, 4)$.\n$$\nz^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 4  0  0 \\\\ 0  4  0 \\\\ 0  0  4 \\end{pmatrix} \\begin{pmatrix} -1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} -2 \\\\ 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 2 \\\\ 2 \\end{pmatrix}\n$$\n\n**4. Compute Updated Parameters $\\beta^{(1)}$**\nThe update is $\\beta^{(1)} = (X^\\top W^{(0)} X)^{-1} X^\\top W^{(0)} z^{(0)}$.\nFirst, compute $X^\\top W^{(0)} X$:\n$$\nX^\\top W^{(0)} X = \\begin{pmatrix} 1  1  1 \\\\ 0  1  2 \\end{pmatrix} \\begin{pmatrix} 1/4  0  0 \\\\ 0  1/4  0 \\\\ 0  0  1/4 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 1  2 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 1  1  1 \\\\ 0  1  2 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 1  2 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 3  3 \\\\ 3  5 \\end{pmatrix}\n$$\nNext, find the inverse of this matrix:\n$$\n(X^\\top W^{(0)} X)^{-1} = \\left(\\frac{1}{4} \\begin{pmatrix} 3  3 \\\\ 3  5 \\end{pmatrix}\\right)^{-1} = 4 \\cdot \\frac{1}{3(5)-3(3)} \\begin{pmatrix} 5  -3 \\\\ -3  3 \\end{pmatrix} = \\frac{4}{6} \\begin{pmatrix} 5  -3 \\\\ -3  3 \\end{pmatrix} = \\frac{2}{3} \\begin{pmatrix} 5  -3 \\\\ -3  3 \\end{pmatrix}\n$$\nNow, compute $X^\\top W^{(0)} z^{(0)}$:\n$$\nX^\\top W^{(0)} z^{(0)} = \\frac{1}{4} \\begin{pmatrix} 1  1  1 \\\\ 0  1  2 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ 2 \\\\ 2 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} -2+2+2 \\\\ 0+2+4 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 2 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 3/2 \\end{pmatrix}\n$$\nFinally, compute $\\beta^{(1)}$:\n$$\n\\beta^{(1)} = \\left(\\frac{2}{3} \\begin{pmatrix} 5  -3 \\\\ -3  3 \\end{pmatrix}\\right) \\begin{pmatrix} 1/2 \\\\ 3/2 \\end{pmatrix} = \\frac{2}{3} \\begin{pmatrix} 5(1/2) - 3(3/2) \\\\ -3(1/2) + 3(3/2) \\end{pmatrix} = \\frac{2}{3} \\begin{pmatrix} 5/2 - 9/2 \\\\ -3/2 + 9/2 \\end{pmatrix} = \\frac{2}{3} \\begin{pmatrix} -4/2 \\\\ 6/2 \\end{pmatrix} = \\frac{2}{3} \\begin{pmatrix} -2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} -4/3 \\\\ 2 \\end{pmatrix}\n$$\n\nThe four requested quantities at the initial iterate are:\n$p^{(0)} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix}$, $W^{(0)} = \\begin{pmatrix} 1/4  0  0 \\\\ 0  1/4  0 \\\\ 0  0  1/4 \\end{pmatrix}$, $z^{(0)} = \\begin{pmatrix} -2 \\\\ 2 \\\\ 2 \\end{pmatrix}$, and $\\beta^{(1)} = \\begin{pmatrix} -4/3 \\\\ 2 \\end{pmatrix}$.", "answer": "$$\n\\boxed{\n\\begin{aligned}\np^{(0)} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix} \\\\\nW^{(0)} = \\begin{pmatrix} 1/4  0  0 \\\\ 0  1/4  0 \\\\ 0  0  1/4 \\end{pmatrix} \\\\\nz^{(0)} = \\begin{pmatrix} -2 \\\\ 2 \\\\ 2 \\end{pmatrix} \\\\\n\\beta^{(1)} = \\begin{pmatrix} -4/3 \\\\ 2 \\end{pmatrix}\n\\end{aligned}\n}\n$$", "id": "4970685"}, {"introduction": "With a fitted model in hand, the next critical task is interpretation. Coefficients in a logistic regression model represent the change in log-odds, a scale that is not always intuitive for clinical or practical application. This final practice focuses on translating these coefficients into a more meaningful quantity: the marginal effect on probability [@problem_id:4970705]. Deriving and calculating this effect reveals a key feature of logistic regression—that the impact of a one-unit change in a predictor on the outcome probability is not constant, but depends on the individual's baseline risk.", "problem": "A clinical study models the probability of post-operative acute kidney injury as a binary outcome using logistic regression. Let $Y \\in \\{0,1\\}$ denote whether acute kidney injury occurs ($Y=1$) or not ($Y=0$). For a patient with covariate vector $\\boldsymbol{x}$, the model specifies the conditional probability $p(\\boldsymbol{x}) = \\mathbb{P}(Y=1 \\mid \\boldsymbol{x})$ through the log-odds (logit) link: the log-odds is linear in the covariates, with linear predictor $\\eta(\\boldsymbol{x}) = \\beta_{0} + \\sum_{k} \\beta_{k} x_{k}$, and probability $p(\\boldsymbol{x})$ obtained by mapping $\\eta(\\boldsymbol{x})$ to the unit interval via the logistic function. Consider a particular continuous covariate $x_{j}$ representing a standardized preoperative biomarker. At a specific patient’s covariate values, the model yields a predicted probability $p(\\boldsymbol{x}) = p = 0.3$, and the estimated coefficient for $x_{j}$ is $\\beta_{j} = 1.2$. Starting from the definition of the logistic regression model and the logistic function, derive the marginal effect of $x_{j}$ on the probability, $\\frac{\\partial p}{\\partial x_{j}}$, evaluated at this patient’s covariates, and compute its numerical value. Express your final answer as a pure number. No rounding is required.", "solution": "The problem is well-posed, scientifically grounded, and provides sufficient information to derive and compute the requested quantity. It is a standard application of differential calculus to the functional form of a logistic regression model. Therefore, the problem is valid.\n\nThe logistic regression model defines the probability of a binary outcome $Y=1$ for a given covariate vector $\\boldsymbol{x}$ as a function of a linear predictor $\\eta(\\boldsymbol{x})$. The relationship is given by the logistic function, often denoted $\\sigma(\\cdot)$:\n$$p(\\boldsymbol{x}) = \\sigma(\\eta(\\boldsymbol{x})) = \\frac{1}{1 + \\exp(-\\eta(\\boldsymbol{x}))}$$\nThe linear predictor $\\eta(\\boldsymbol{x})$ is a linear combination of the covariates:\n$$\\eta(\\boldsymbol{x}) = \\beta_{0} + \\sum_{k} \\beta_{k} x_{k}$$\nwhere $\\beta_0$ is the intercept and $\\beta_k$ are the coefficients for the covariates $x_k$.\n\nWe are asked to find the marginal effect of a specific continuous covariate $x_j$ on the probability $p(\\boldsymbol{x})$. This marginal effect is defined as the partial derivative $\\frac{\\partial p}{\\partial x_{j}}$. To find this derivative, we apply the chain rule of differentiation:\n$$\\frac{\\partial p}{\\partial x_{j}} = \\frac{d p}{d \\eta} \\cdot \\frac{\\partial \\eta}{\\partial x_{j}}$$\n\nFirst, we compute the derivative of the linear predictor $\\eta(\\boldsymbol{x})$ with respect to $x_j$:\n$$\\frac{\\partial \\eta}{\\partial x_{j}} = \\frac{\\partial}{\\partial x_{j}} \\left( \\beta_{0} + \\sum_{k} \\beta_{k} x_{k} \\right)$$\nAssuming the covariates are distinct variables, the derivative of $x_k$ with respect to $x_j$ is $1$ if $k=j$ and $0$ otherwise. Thus, only the term involving $\\beta_j x_j$ contributes to the derivative.\n$$\\frac{\\partial \\eta}{\\partial x_{j}} = \\beta_{j}$$\n\nNext, we compute the derivative of the logistic function $p(\\eta)$ with respect to its argument $\\eta$:\n$$p(\\eta) = (1 + \\exp(-\\eta))^{-1}$$\nUsing the power rule and the chain rule:\n$$\\frac{d p}{d \\eta} = -1 \\cdot (1 + \\exp(-\\eta))^{-2} \\cdot \\frac{d}{d\\eta}(\\exp(-\\eta))$$\n$$\\frac{d p}{d \\eta} = -1 \\cdot (1 + \\exp(-\\eta))^{-2} \\cdot (-\\exp(-\\eta))$$\n$$\\frac{d p}{d \\eta} = \\frac{\\exp(-\\eta)}{(1 + \\exp(-\\eta))^{2}}$$\nThis expression can be conveniently rewritten in terms of $p$ itself. We recognize that:\n$$p = \\frac{1}{1 + \\exp(-\\eta)}$$\nAnd we can write $1-p$ as:\n$$1 - p = 1 - \\frac{1}{1 + \\exp(-\\eta)} = \\frac{(1 + \\exp(-\\eta)) - 1}{1 + \\exp(-\\eta)} = \\frac{\\exp(-\\eta)}{1 + \\exp(-\\eta)}$$\nTherefore, the derivative $\\frac{dp}{d\\eta}$ is the product of these two terms:\n$$\\frac{d p}{d \\eta} = \\left( \\frac{1}{1 + \\exp(-\\eta)} \\right) \\cdot \\left( \\frac{\\exp(-\\eta)}{1 + \\exp(-\\eta)} \\right) = p \\cdot (1 - p)$$\nThis is a fundamental property of the logistic function.\n\nNow, we combine the two parts of the chain rule to obtain the final expression for the marginal effect:\n$$\\frac{\\partial p}{\\partial x_{j}} = \\frac{d p}{d \\eta} \\cdot \\frac{\\partial \\eta}{\\partial x_{j}} = p(\\boldsymbol{x}) \\cdot (1 - p(\\boldsymbol{x})) \\cdot \\beta_{j}$$\n\nThe problem provides the necessary values evaluated for a specific patient:\nThe predicted probability is $p(\\boldsymbol{x}) = p = 0.3$.\nThe estimated coefficient for the covariate $x_j$ is $\\beta_j = 1.2$.\n\nWe substitute these numerical values into our derived formula for the marginal effect:\n$$\\frac{\\partial p}{\\partial x_{j}} = 0.3 \\cdot (1 - 0.3) \\cdot 1.2$$\n$$\\frac{\\partial p}{\\partial x_{j}} = 0.3 \\cdot 0.7 \\cdot 1.2$$\n$$\\frac{\\partial p}{\\partial x_{j}} = 0.21 \\cdot 1.2$$\n$$\\frac{\\partial p}{\\partial x_{j}} = 0.252$$\nThus, at this patient's covariate values, a one-unit increase in the standardized biomarker $x_j$ is associated with an increase in the probability of acute kidney injury by approximately $0.252$.", "answer": "$$\n\\boxed{0.252}\n$$", "id": "4970705"}]}