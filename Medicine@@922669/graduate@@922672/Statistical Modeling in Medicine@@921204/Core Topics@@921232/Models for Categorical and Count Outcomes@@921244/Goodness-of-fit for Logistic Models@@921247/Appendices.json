{"hands_on_practices": [{"introduction": "Assessing how well a model fits the data is a cornerstone of statistical modeling. One of the most fundamental methods for this, especially with grouped data common in epidemiological or multi-center studies, is the Pearson chi-squared ($X^2$) goodness-of-fit test. This exercise [@problem_id:4965716] provides foundational practice in applying this test to a logistic model by guiding you through the calculation of Pearson residuals—the standardized difference between observed and model-expected counts—and aggregating them into the overall $X^2$ statistic. Mastering this calculation solidifies the core concept of quantifying model discrepancy.", "problem": "A multi-center surgical outcomes study evaluates the probability of a postoperative infection within $30$ days. At each center $j$, investigators record $m_j$ patients and $y_j$ infections. A Generalized Linear Model (GLM) with binomial variance function and a logistic link is fitted to the grouped-binomial data, using a standardized comorbidity index $x_j$ as the single covariate. The fitted linear predictor is $g(\\pi_j) = \\log\\!\\left(\\frac{\\pi_j}{1-\\pi_j}\\right) = \\beta_0 + \\beta_1 x_j$, where $\\pi_j$ is the probability of infection for center $j$. The maximum likelihood estimates are $\\hat{\\beta}_0 = \\ln\\!\\left(\\frac{0.2}{0.8}\\right)$ and $\\hat{\\beta}_1 = \\ln(2)$. The five centers have covariates, sample sizes, and observed infections:\n- $j=1$: $x_1 = 0$, $m_1 = 50$, $y_1 = 12$.\n- $j=2$: $x_2 = 1$, $m_2 = 60$, $y_2 = 18$.\n- $j=3$: $x_3 = 2$, $m_3 = 80$, $y_3 = 42$.\n- $j=4$: $x_4 = 3$, $m_4 = 90$, $y_4 = 55$.\n- $j=5$: $x_5 = 4$, $m_5 = 100$, $y_5 = 78$.\n\nStarting from the binomial model for grouped outcomes, derive the appropriate Pearson residual for center $j$ based on the mean and variance of the binomial distribution under the fitted model, and use it to construct the Pearson goodness-of-fit statistic by summing the squared residuals over centers. Compute this goodness-of-fit statistic using the data above. Express the final statistic as a real number. If any intermediate numerical quantities require approximation, carry them symbolically until the final step; no rounding of the final statistic is required in this problem.", "solution": "The problem is valid as it is scientifically grounded in statistical theory, well-posed with sufficient data, and objectively stated.\n\nThe problem asks for the computation of the Pearson goodness-of-fit statistic for a logistic regression model fitted to grouped binomial data from five surgical centers.\n\nThe model assumes that for each center $j$, the number of observed infections, $y_j$, follows a binomial distribution, $Y_j \\sim \\text{Binomial}(m_j, \\pi_j)$, where $m_j$ is the total number of patients and $\\pi_j$ is the probability of infection. The mean and variance of $Y_j$ are given by:\n$$\nE[Y_j] = m_j \\pi_j\n$$\n$$\n\\text{Var}(Y_j) = m_j \\pi_j (1 - \\pi_j)\n$$\nThe logistic regression model connects the probability $\\pi_j$ to a linear predictor $\\eta_j = \\beta_0 + \\beta_1 x_j$ via the logit link function:\n$$\n\\eta_j = \\text{logit}(\\pi_j) = \\ln\\left(\\frac{\\pi_j}{1-\\pi_j}\\right)\n$$\nThe Pearson residual for the $j$-th group is defined as the difference between the observed count $y_j$ and the fitted count $\\hat{y}_j$, standardized by the estimated standard deviation of the observed count under the model. The fitted count is $\\hat{y}_j = m_j \\hat{\\pi}_j$, where $\\hat{\\pi}_j$ is the fitted probability for center $j$. The estimated variance is $\\widehat{\\text{Var}}(Y_j) = m_j \\hat{\\pi}_j (1 - \\hat{\\pi}_j)$.\n\nThus, the Pearson residual for center $j$, denoted $r_{P,j}$, is:\n$$\nr_{P,j} = \\frac{y_j - E[Y_j]}{\\sqrt{\\widehat{\\text{Var}}(Y_j)}} = \\frac{y_j - m_j \\hat{\\pi}_j}{\\sqrt{m_j \\hat{\\pi}_j (1 - \\hat{\\pi}_j)}}\n$$\nThe Pearson goodness-of-fit statistic, often denoted as $X^2$, is the sum of the squared Pearson residuals over all $J$ centers:\n$$\nX^2 = \\sum_{j=1}^{J} r_{P,j}^2 = \\sum_{j=1}^{J} \\frac{(y_j - m_j \\hat{\\pi}_j)^2}{m_j \\hat{\\pi}_j (1 - \\hat{\\pi}_j)}\n$$\nIn this problem, we have $J=5$ centers. The first step is to calculate the fitted probabilities $\\hat{\\pi}_j$ for each center using the provided maximum likelihood estimates for the model parameters: $\\hat{\\beta}_0 = \\ln(0.2/0.8) = \\ln(0.25)$ and $\\hat{\\beta}_1 = \\ln(2)$.\n\nThe fitted linear predictor for center $j$ is $\\hat{\\eta}_j = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_j$.\n$$\n\\hat{\\eta}_j = \\ln(0.25) + x_j \\ln(2) = \\ln(0.25 \\cdot 2^{x_j}) = \\ln(2^{-2} \\cdot 2^{x_j}) = \\ln(2^{x_j-2})\n$$\nTo find the fitted probability $\\hat{\\pi}_j$, we apply the inverse logit function:\n$$\n\\hat{\\pi}_j = \\text{logit}^{-1}(\\hat{\\eta}_j) = \\frac{\\exp(\\hat{\\eta}_j)}{1 + \\exp(\\hat{\\eta}_j)} = \\frac{\\exp(\\ln(2^{x_j-2}))}{1 + \\exp(\\ln(2^{x_j-2}))} = \\frac{2^{x_j-2}}{1 + 2^{x_j-2}}\n$$\nMultiplying the numerator and denominator by $2^2=4$ gives a more convenient form for calculation:\n$$\n\\hat{\\pi}_j = \\frac{4 \\cdot 2^{x_j-2}}{4 \\cdot (1 + 2^{x_j-2})} = \\frac{2^2 \\cdot 2^{x_j-2}}{4 + 4 \\cdot 2^{x_j-2}} = \\frac{2^{x_j}}{4 + 2^{x_j}}\n$$\nWe now compute the components for the $X^2$ statistic for each of the five centers.\n\nFor center $j=1$:\n$x_1=0$, $m_1=50$, $y_1=12$.\n$\\hat{\\pi}_1 = \\frac{2^0}{4+2^0} = \\frac{1}{5} = 0.2$.\nFitted count: $\\hat{y}_1 = m_1 \\hat{\\pi}_1 = 50 \\times 0.2 = 10$.\nEstimated variance: $m_1 \\hat{\\pi}_1 (1-\\hat{\\pi}_1) = 50 \\times 0.2 \\times (1-0.2) = 10 \\times 0.8 = 8$.\nSquared residual: $\\frac{(y_1 - \\hat{y}_1)^2}{m_1 \\hat{\\pi}_1 (1-\\hat{\\pi}_1)} = \\frac{(12-10)^2}{8} = \\frac{2^2}{8} = \\frac{4}{8} = 0.5$.\n\nFor center $j=2$:\n$x_2=1$, $m_2=60$, $y_2=18$.\n$\\hat{\\pi}_2 = \\frac{2^1}{4+2^1} = \\frac{2}{6} = \\frac{1}{3}$.\nFitted count: $\\hat{y}_2 = m_2 \\hat{\\pi}_2 = 60 \\times \\frac{1}{3} = 20$.\nEstimated variance: $m_2 \\hat{\\pi}_2 (1-\\hat{\\pi}_2) = 60 \\times \\frac{1}{3} \\times (1-\\frac{1}{3}) = 20 \\times \\frac{2}{3} = \\frac{40}{3}$.\nSquared residual: $\\frac{(y_2 - \\hat{y}_2)^2}{m_2 \\hat{\\pi}_2 (1-\\hat{\\pi}_2)} = \\frac{(18-20)^2}{40/3} = \\frac{(-2)^2}{40/3} = \\frac{4 \\times 3}{40} = \\frac{12}{40} = 0.3$.\n\nFor center $j=3$:\n$x_3=2$, $m_3=80$, $y_3=42$.\n$\\hat{\\pi}_3 = \\frac{2^2}{4+2^2} = \\frac{4}{8} = \\frac{1}{2} = 0.5$.\nFitted count: $\\hat{y}_3 = m_3 \\hat{\\pi}_3 = 80 \\times 0.5 = 40$.\nEstimated variance: $m_3 \\hat{\\pi}_3 (1-\\hat{\\pi}_3) = 80 \\times 0.5 \\times (1-0.5) = 40 \\times 0.5 = 20$.\nSquared residual: $\\frac{(y_3 - \\hat{y}_3)^2}{m_3 \\hat{\\pi}_3 (1-\\hat{\\pi}_3)} = \\frac{(42-40)^2}{20} = \\frac{2^2}{20} = \\frac{4}{20} = 0.2$.\n\nFor center $j=4$:\n$x_4=3$, $m_4=90$, $y_4=55$.\n$\\hat{\\pi}_4 = \\frac{2^3}{4+2^3} = \\frac{8}{12} = \\frac{2}{3}$.\nFitted count: $\\hat{y}_4 = m_4 \\hat{\\pi}_4 = 90 \\times \\frac{2}{3} = 60$.\nEstimated variance: $m_4 \\hat{\\pi}_4 (1-\\hat{\\pi}_4) = 90 \\times \\frac{2}{3} \\times (1-\\frac{2}{3}) = 60 \\times \\frac{1}{3} = 20$.\nSquared residual: $\\frac{(y_4 - \\hat{y}_4)^2}{m_4 \\hat{\\pi}_4 (1-\\hat{\\pi}_4)} = \\frac{(55-60)^2}{20} = \\frac{(-5)^2}{20} = \\frac{25}{20} = 1.25$.\n\nFor center $j=5$:\n$x_5=4$, $m_5=100$, $y_5=78$.\n$\\hat{\\pi}_5 = \\frac{2^4}{4+2^4} = \\frac{16}{20} = \\frac{4}{5} = 0.8$.\nFitted count: $\\hat{y}_5 = m_5 \\hat{\\pi}_5 = 100 \\times 0.8 = 80$.\nEstimated variance: $m_5 \\hat{\\pi}_5 (1-\\hat{\\pi}_5) = 100 \\times 0.8 \\times (1-0.8) = 80 \\times 0.2 = 16$.\nSquared residual: $\\frac{(y_5 - \\hat{y}_5)^2}{m_5 \\hat{\\pi}_5 (1-\\hat{\\pi}_5)} = \\frac{(78-80)^2}{16} = \\frac{(-2)^2}{16} = \\frac{4}{16} = 0.25$.\n\nFinally, the Pearson goodness-of-fit statistic $X^2$ is the sum of these squared residuals:\n$$\nX^2 = 0.5 + 0.3 + 0.2 + 1.25 + 0.25\n$$\n$$\nX^2 = (0.5 + 0.3 + 0.2) + (1.25 + 0.25) = 1.0 + 1.5 = 2.5\n$$\nThe value of the Pearson goodness-of-fit statistic is $2.5$.", "answer": "$$\n\\boxed{2.5}\n$$", "id": "4965716"}, {"introduction": "While classical tests are useful for grouped data, modern clinical prediction models often provide individualized risk scores, demanding evaluation metrics that assess the accuracy of these specific probabilities. The Brier score is a proper scoring rule that measures the mean squared error between predicted probabilities and actual binary outcomes, directly rewarding well-calibrated models. This practice problem [@problem_id:4965723] challenges you to compute the Brier Skill Score, which contextualizes a model's performance by comparing its Brier score to that of a simple baseline model, providing a more intuitive measure of its practical value in a medical screening setting.", "problem": "A hospital deploys a risk model based on logistic regression (LR) to screen for occult bacteremia among adult patients presenting to the emergency department. The model outputs predicted probabilities for bacteremia, and decisions are made downstream using a threshold, but here the goal is to evaluate model goodness-of-fit using a proper scoring rule. Consider a held-out evaluation set of $N=20$ patients with out-of-sample predicted probabilities from the LR model and observed outcomes coded as $y_{i} \\in \\{0,1\\}$, where $y_{i}=1$ indicates a confirmed bacteremia case and $y_{i}=0$ indicates no bacteremia. The evaluation is framed using squared-error scoring for binary events, and skill is measured relative to a baseline model that predicts the observed prevalence in the evaluation set for every individual. This baseline reflects a constant-probability \"climatology\" model appropriate for screening contexts when individual-level features are ignored.\n\nThe $20$ patients have the following $(p_{i}, y_{i})$ pairs:\n\n- $4$ patients with $p_{i}=0.05$ and $y_{i}=0$.\n- $2$ patients with $p_{i}=0.10$ and $y_{i}=0$.\n- $3$ patients with $p_{i}=0.40$ and $y_{i}=0$.\n- $2$ patients with $p_{i}=0.60$ and $y_{i}=0$.\n- $4$ patients with $p_{i}=0.85$ and $y_{i}=1$.\n- $3$ patients with $p_{i}=0.70$ and $y_{i}=1$.\n- $2$ patients with $p_{i}=0.20$ and $y_{i}=1$.\n\nCompute the Brier skill score (BSS) of the LR model relative to the baseline that predicts the observed prevalence. Then, interpret the magnitude of the BSS in the medical screening context described, focusing on what it implies about improvement in squared-error risk relative to the baseline. Round your numerical answer for the Brier skill score to four significant figures. No units are required.", "solution": "The evaluation uses a proper scoring rule for probabilistic predictions in binary outcomes, specifically the squared-error score. For a single prediction $p_i$ and outcome $y_i \\in \\{0,1\\}$, the contribution to the Brier score is $(p_i - y_i)^2$. The Brier score for the model over $N$ observations is the average of these squared errors:\n$$\n\\text{BS}_{\\text{model}} = \\frac{1}{N} \\sum_{i=1}^{N} (p_i - y_i)^2.\n$$\nThe baseline model in this screening context predicts a constant probability equal to the observed prevalence $\\bar{y} = \\frac{1}{N} \\sum_{i=1}^{N} y_i$ for every individual. For this constant predictor, the Brier score can be derived from first principles by conditioning on the outcome:\n- When $y_i=1$, the squared error is $(1 - \\bar{y})^2$.\n- When $y_i=0$, the squared error is $\\bar{y}^2$.\nAveraging over the distribution of $y_i$ and using $\\mathbb{P}(y_i=1)=\\bar{y}$ and $\\mathbb{P}(y_i=0)=1-\\bar{y}$ yields\n$$\n\\text{BS}_{\\text{baseline}} = \\bar{y} \\cdot (1 - \\bar{y})^{2} + (1 - \\bar{y}) \\cdot \\bar{y}^{2} = \\bar{y}(1-\\bar{y}).\n$$\nThe Brier skill score (BSS) quantifies improvement relative to the baseline as\n$$\n\\text{BSS} = 1 - \\frac{\\text{BS}_{\\text{model}}}{\\text{BS}_{\\text{baseline}}}.\n$$\nA value of $\\text{BSS}  0$ indicates improvement over the baseline; $\\text{BSS}=0$ indicates performance equal to the baseline; $\\text{BSS}0$ indicates worse performance.\n\nWe now compute the quantities for the given data.\n\nStep 1: Compute the observed prevalence $\\bar{y}$.\nThe total number of positives is $4+3+2=9$ (from the three positive groups), and $N=20$, so\n$$\n\\bar{y} = \\frac{9}{20} = 0.45.\n$$\nTherefore,\n$$\n\\text{BS}_{\\text{baseline}} = \\bar{y}(1-\\bar{y}) = \\frac{9}{20}\\cdot\\frac{11}{20} = \\frac{99}{400} = 0.2475.\n$$\n\nStep 2: Compute $\\text{BS}_{\\text{model}}$ by summing $(p_i-y_i)^2$ across all patients and dividing by $N$.\n\nFor $y_i=0$, the squared errors are $p_i^2$:\n- $4$ patients with $p_i=0.05$: sum $= 4 \\cdot (0.05)^2 = 4 \\cdot 0.0025 = 0.01$.\n- $2$ patients with $p_i=0.10$: sum $= 2 \\cdot (0.10)^2 = 2 \\cdot 0.01 = 0.02$.\n- $3$ patients with $p_i=0.40$: sum $= 3 \\cdot (0.40)^2 = 3 \\cdot 0.16 = 0.48$.\n- $2$ patients with $p_i=0.60$: sum $= 2 \\cdot (0.60)^2 = 2 \\cdot 0.36 = 0.72$.\nTotal for negatives:\n$$\nS_0 = 0.01 + 0.02 + 0.48 + 0.72 = 1.23.\n$$\n\nFor $y_i=1$, the squared errors are $(1-p_i)^2$:\n- $4$ patients with $p_i=0.85$: sum $= 4 \\cdot (1 - 0.85)^2 = 4 \\cdot (0.15)^2 = 4 \\cdot 0.0225 = 0.09$.\n- $3$ patients with $p_i=0.70$: sum $= 3 \\cdot (1 - 0.70)^2 = 3 \\cdot (0.30)^2 = 3 \\cdot 0.09 = 0.27$.\n- $2$ patients with $p_i=0.20$: sum $= 2 \\cdot (1 - 0.20)^2 = 2 \\cdot (0.80)^2 = 2 \\cdot 0.64 = 1.28$.\nTotal for positives:\n$$\nS_1 = 0.09 + 0.27 + 1.28 = 1.64.\n$$\n\nSumming over all $N=20$ patients:\n$$\n\\sum_{i=1}^{N} (p_i - y_i)^2 = S_0 + S_1 = 1.23 + 1.64 = 2.87.\n$$\nThus,\n$$\n\\text{BS}_{\\text{model}} = \\frac{2.87}{20} = \\frac{287}{2000} = 0.1435.\n$$\n\nStep 3: Compute the Brier skill score.\nUsing exact fractions,\n$$\n\\frac{\\text{BS}_{\\text{model}}}{\\text{BS}_{\\text{baseline}}} = \\frac{\\frac{287}{2000}}{\\frac{99}{400}} = \\frac{287}{2000} \\cdot \\frac{400}{99} = \\frac{287}{5 \\cdot 99} = \\frac{287}{495}.\n$$\nTherefore,\n$$\n\\text{BSS} = 1 - \\frac{287}{495} = \\frac{495 - 287}{495} = \\frac{208}{495}.\n$$\nConverting to a decimal and rounding to four significant figures,\n$$\n\\text{BSS} = \\frac{208}{495} \\approx 0.4202 \\quad \\text{(four significant figures)}.\n$$\n\nInterpretation in the medical screening context: A Brier skill score of $0.4202$ indicates that, relative to the baseline that predicts the observed prevalence for every patient, the logistic regression model reduces the mean squared error of probabilistic predictions by a fraction of approximately $0.4202$. In screening terms, this magnitude reflects a substantial improvement in calibrated risk estimation over prevalence-only prediction, implying materially better individualized probability forecasts that can support more effective decision thresholds than those based on prevalence alone.", "answer": "$$\\boxed{0.4202}$$", "id": "4965723"}, {"introduction": "A high-performing risk model must excel in two distinct areas: discrimination (correctly ranking individuals by risk) and calibration (accurately estimating the probability of an outcome). A common mistake is to rely solely on metrics of discrimination, like the Area Under the ROC Curve (AUC). This advanced exercise [@problem_id:4965808] brilliantly illustrates the potential pitfall of this approach by demonstrating how a model's predictions can be transformed to severely degrade calibration without changing the AUC at all. By working through this derivation, you will gain a crucial, first-principles insight into why assessing calibration is an indispensable and separate step in model validation.", "problem": "A hospital develops a clinical decision support system to predict $30$-day mortality in patients admitted with acute decompensated heart failure using a logistic regression model. Let $Y \\in \\{0,1\\}$ denote mortality by $30$ days and let $S$ denote the model’s linear predictor so that the true conditional probability of death given $S$ is $P(Y=1 \\mid S=s) = \\sigma(s)$, where $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$ is the logistic function and $\\logit(p) = \\ln\\!\\left(\\frac{p}{1-p}\\right)$ is its inverse. The model’s predicted probability is $\\hat{\\pi} = \\sigma(S)$, which is therefore perfectly calibrated by construction.\n\nAn investigator wishes to “sharpen” risk stratification without changing ranking. They report a transformed prediction $\\tilde{\\pi} = g(\\hat{\\pi})$ defined by\n$$\n\\tilde{\\pi} \\equiv \\sigma\\!\\big(a + b\\,\\logit(\\hat{\\pi})\\big),\n$$\nwith constants $a \\in \\mathbb{R}$ and $b0$. Consider the specific choice $a = 0.4$ and $b = 2$.\n\nUsing only fundamental definitions, do the following:\n- Show that $g$ is strictly increasing on $(0,1)$ and therefore preserves the ranking of patients induced by $\\hat{\\pi}$.\n- Starting from the definition of the Area Under the Receiver Operating Characteristic curve (AUC) as $\\,\\mathrm{AUC} = P\\big(S_{1}  S_{0}\\big)$, where $S_{1}$ is the score for a randomly chosen case ($Y=1$) and $S_{0}$ is the score for a randomly chosen control ($Y=0$), argue that any strictly increasing transformation of the score preserves the AUC.\n- Derive, from first principles, the population calibration relationship between $Y$ and the transformed linear predictor $S' \\equiv \\logit(\\tilde{\\pi}) = a + b S$, that is, derive $\\logit\\!\\big(P(Y=1 \\mid S'=s')\\big)$ as a linear function of $s'$. Identify the calibration intercept and calibration slope.\n- Finally, compute the calibration slope numerically for the given $(a,b) = (0.4, 2)$.\n\nProvide the final answer as the single numerical value of the calibration slope. No rounding is required; give the exact value.", "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a complete derivation.\n\nThe problem requires a four-part analysis of a transformed logistic regression predictor. Let us address each part systematically. The initial, perfectly calibrated model has a linear predictor $S$, such that the true probability of the event $Y=1$ is given by $P(Y=1 \\mid S=s) = \\sigma(s)$, where $\\sigma(z) = (1+\\exp(-z))^{-1}$ is the logistic function. The model's predicted probability is $\\hat{\\pi} = \\sigma(S)$.\n\nThe investigator proposes a transformed prediction $\\tilde{\\pi} = g(\\hat{\\pi})$, where the transformation function is $g(p) = \\sigma(a + b\\,\\logit(p))$ for $p \\in (0,1)$, with constants $a \\in \\mathbb{R}$ and $b0$.\n\n**Part 1: Show that $g$ is strictly increasing on $(0,1)$**\n\nTo demonstrate that the function $g(p)$ is strictly increasing for $p \\in (0,1)$, we must show that its first derivative, $g'(p)$, is strictly positive on this interval. We use the chain rule for differentiation.\nLet $u(p) = a + b\\,\\logit(p)$. Then $g(p) = \\sigma(u(p))$.\nThe derivative is $g'(p) = \\sigma'(u(p)) \\cdot u'(p)$.\n\nFirst, let's find the derivatives of the component functions. The derivative of the logit function, $\\logit(p) = \\ln(p) - \\ln(1-p)$, is:\n$$\n\\frac{d}{dp}\\logit(p) = \\frac{1}{p} - \\frac{-1}{1-p} = \\frac{1}{p} + \\frac{1}{1-p} = \\frac{1-p+p}{p(1-p)} = \\frac{1}{p(1-p)}\n$$\nFor any $p \\in (0,1)$, both $p$ and $1-p$ are positive, so their product $p(1-p)$ is positive. Thus, $\\frac{d}{dp}\\logit(p)  0$ for $p \\in (0,1)$.\n\nThe derivative of the logistic function $\\sigma(z)$ is:\n$$\n\\sigma'(z) = \\frac{d}{dz}\\left(\\frac{1}{1+\\exp(-z)}\\right) = -\\frac{-\\exp(-z)}{(1+\\exp(-z))^2} = \\frac{\\exp(-z)}{(1+\\exp(-z))^2}\n$$\nThis can also be written as $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$. Since $\\sigma(z)$ maps $\\mathbb{R}$ to $(0,1)$, both $\\sigma(z)$ and $1-\\sigma(z)$ are strictly positive for any finite $z$. Therefore, $\\sigma'(z)  0$ for all $z \\in \\mathbb{R}$.\n\nNow, we compute $u'(p)$:\n$$\nu'(p) = \\frac{d}{dp}\\big(a + b\\,\\logit(p)\\big) = b \\cdot \\frac{d}{dp}\\logit(p) = \\frac{b}{p(1-p)}\n$$\nGiven that $b0$ and $p \\in (0,1)$, it follows that $u'(p)  0$.\n\nFinally, assembling the derivative $g'(p)$:\n$$\ng'(p) = \\sigma'(u(p)) \\cdot u'(p)\n$$\nAs established, $\\sigma'(u(p))$ is positive because the range of $u(p)$ is a subset of $\\mathbb{R}$, and $u'(p)$ is positive. The product of two positive numbers is positive.\n$$\ng'(p)  0 \\quad \\text{for all } p \\in (0,1)\n$$\nThis confirms that $g(p)$ is a strictly increasing function on its domain. Consequently, this transformation preserves the ranking of patients' risks as determined by $\\hat{\\pi}$.\n\n**Part 2: Argue that the AUC is preserved**\n\nThe Area Under the Receiver Operating Characteristic curve (AUC) is defined as the probability that the score assigned to a randomly chosen positive case ($Y=1$) is greater than the score assigned to a randomly chosen negative case ($Y=0$). Let $\\hat{\\pi}_1$ be the score for a random case and $\\hat{\\pi}_0$ be the score for a random control. The AUC of the original model is $\\mathrm{AUC}_{\\hat{\\pi}} = P(\\hat{\\pi}_1  \\hat{\\pi}_0)$. The problem statement uses the linear predictor $S$ for the definition, $\\mathrm{AUC} = P(S_1  S_0)$; since $\\hat{\\pi}=\\sigma(S)$ and $\\sigma$ is strictly increasing, the ranking is identical, so $P(S_1  S_0) = P(\\hat{\\pi}_1  \\hat{\\pi}_0)$.\n\nThe new scores are given by the transformation $\\tilde{\\pi} = g(\\hat{\\pi})$. The AUC for the new scores is $\\mathrm{AUC}_{\\tilde{\\pi}} = P(\\tilde{\\pi}_1  \\tilde{\\pi}_0)$.\nWe have shown that the function $g$ is strictly increasing. A strictly increasing function preserves order, meaning that for any two values $x_1, x_2$ in its domain, $x_1  x_2 \\iff g(x_1)  g(x_2)$.\nApplying this to our scores, the inequality $\\hat{\\pi}_1  \\hat{\\pi}_0$ holds if and only if the inequality $g(\\hat{\\pi}_1)  g(\\hat{\\pi}_0)$ holds. This is equivalent to $\\tilde{\\pi}_1  \\tilde{\\pi}_0$.\nThe events $\\{\\hat{\\pi}_1  \\hat{\\pi}_0\\}$ and $\\{\\tilde{\\pi}_1  \\tilde{\\pi}_0\\}$ are therefore identical. Because the events are identical, their probabilities must be equal:\n$$\nP(\\hat{\\pi}_1  \\hat{\\pi}_0) = P(\\tilde{\\pi}_1  \\tilde{\\pi}_0)\n$$\nThus, $\\mathrm{AUC}_{\\hat{\\pi}} = \\mathrm{AUC}_{\\tilde{\\pi}}$. Any strictly increasing transformation of a model's score preserves the AUC because the AUC depends only on the rank-ordering of scores, which is invariant under such transformations.\n\n**Part 3: Derive the population calibration relationship for $S'$**\n\nThe new linear predictor is defined as $S' \\equiv \\logit(\\tilde{\\pi})$. We first express $S'$ in terms of the original linear predictor $S$.\nWe are given $\\tilde{\\pi} = \\sigma(a + b\\,\\logit(\\hat{\\pi}))$.\nWe also know that $\\hat{\\pi} = \\sigma(S)$, and because $\\logit$ is the inverse of $\\sigma$, we have $\\logit(\\hat{\\pi}) = \\logit(\\sigma(S)) = S$.\nSubstituting this into the expression for $\\tilde{\\pi}$:\n$$\n\\tilde{\\pi} = \\sigma(a + bS)\n$$\nNow, we can find $S'$:\n$$\nS' = \\logit(\\tilde{\\pi}) = \\logit(\\sigma(a + bS)) = a + bS\n$$\nWe need to derive the calibration relationship, which is the functional form of $\\logit\\!\\big(P(Y=1 \\mid S'=s')\\big)$ in terms of $s'$.\nThe condition $S' = s'$ is equivalent to $a + bS = s'$, which can be solved for $S$:\n$$\nS = \\frac{s' - a}{b}\n$$\nTherefore, conditioning on $S'=s'$ is equivalent to conditioning on $S = \\frac{s' - a}{b}$.\nWe can write:\n$$\nP(Y=1 \\mid S'=s') = P\\left(Y=1 \\mid S = \\frac{s' - a}{b}\\right)\n$$\nThe problem states that the original model is perfectly calibrated, meaning $P(Y=1 \\mid S=s) = \\sigma(s)$ for any value $s$. We apply this fundamental relationship:\n$$\nP(Y=1 \\mid S'=s') = \\sigma\\left(\\frac{s' - a}{b}\\right)\n$$\nTo find the calibration relationship on the logit scale, we apply the $\\logit$ function to both sides:\n$$\n\\logit\\!\\big(P(Y=1 \\mid S'=s')\\big) = \\logit\\left(\\sigma\\left(\\frac{s' - a}{b}\\right)\\right)\n$$\nSince $\\logit$ and $\\sigma$ are inverse functions, $\\logit(\\sigma(z))=z$. Thus:\n$$\n\\logit\\!\\big(P(Y=1 \\mid S'=s')\\big) = \\frac{s' - a}{b}\n$$\nRewriting this in the standard linear form (intercept + slope $\\times$ predictor):\n$$\n\\logit\\!\\big(P(Y=1 \\mid S'=s')\\big) = -\\frac{a}{b} + \\frac{1}{b}s'\n$$\nThis is the desired calibration relationship. The relationship between the true log-odds and the new predictor $s'$ is linear.\n\nThe calibration intercept is the constant term, $-\\frac{a}{b}$.\nThe calibration slope is the coefficient of the predictor $s'$, which is $\\frac{1}{b}$.\n\n**Part 4: Compute the calibration slope**\n\nThe problem provides the specific parameter values $a=0.4$ and $b=2$.\nUsing the formula for the calibration slope derived above:\n$$\n\\text{Calibration Slope} = \\frac{1}{b}\n$$\nSubstituting $b=2$:\n$$\n\\text{Calibration Slope} = \\frac{1}{2}\n$$\nThe numerical value of the calibration slope is $0.5$.", "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$", "id": "4965808"}]}