{"hands_on_practices": [{"introduction": "A common pitfall in count data analysis is equating overdispersion—where variance exceeds the mean—with the presence of \"excess zeros.\" This exercise sharpens your diagnostic skills by demonstrating, from first principles, why this is not necessarily true. You will explore the theoretical distinction and then calculate the expected zero probability for the Negative Binomial distribution, a standard model for overdispersion, to see how it accommodates zeros without a separate inflation component [@problem_id:4993564].", "problem": "A medical center tracks the annual count of severe exacerbations per patient for a chronic respiratory condition. Let $Y$ denote the count per patient-year. The center observes overdispersion, in the sense that $\\mathrm{Var}(Y)>\\mathrm{E}(Y)$ when compared with a Poisson baseline. Analysts are considering whether a zero-inflated model is required to account for an apparently large fraction of zero counts.\n\nUsing only core definitions and well-tested facts, proceed as follows.\n\n1) Explain, from first principles, why overdispersion alone does not logically imply “excess zeros.” Here, “excess zeros” means that the probability mass at zero, $\\mathbb{P}(Y=0)$, is larger than what would be expected under a suitably chosen baseline count model with the same mean. Your explanation should make clear the distinction between a statement about the second moment (variance inflation) and a statement about the mass at a single point ($0$). Give a concrete construction of an overdispersed count distribution that has a zero deficit (i.e., $\\mathbb{P}(Y=0)$ smaller than a standard baseline such as the Poisson with the same mean), thereby demonstrating that overdispersion by itself does not guarantee excess zeros.\n\n2) Now assume that $Y$ follows a Negative Binomial (NB) model widely used in biostatistics for overdispersed counts, defined via the Poisson–Gamma mixture: conditional on a subject-specific frailty $\\,\\Theta\\,$, $Y \\mid \\Theta \\sim \\mathrm{Poisson}(\\lambda\\,\\Theta)$, and the frailty $\\Theta \\sim \\mathrm{Gamma}(k,\\text{rate}=k)$, so that $\\mathrm{E}[\\Theta]=1$ and $\\mathrm{Var}(\\Theta)=1/k$. Consequently, $\\mathrm{E}[Y]=\\lambda$ and $\\mathrm{Var}(Y)=\\lambda+\\lambda^{2}/k$. Starting from this mixture definition and the definition of probability for a Poisson random variable, derive the closed-form expression for the marginal zero probability $\\mathbb{P}(Y=0)$ in terms of $\\,\\lambda\\,$ and $\\,k\\,$.\n\nFor a cohort with mean count $\\lambda=2.4$ and dispersion parameter $k=1.5$, evaluate your derived expression numerically. Round your final numerical answer to four significant figures. Report the final quantity as a unitless decimal.", "solution": "The problem statement is scientifically grounded, well-posed, and free of contradictions. It presents a standard, albeit subtle, conceptual challenge in applied statistics. We may therefore proceed with the solution.\n\n1) The statement that \"overdispersion does not logically imply excess zeros\" concerns the relationship between a distribution's second moment and the probability mass at a single point. Overdispersion, in the context of count data, means that the variance of a random variable $Y$ is greater than its expectation, $\\mathrm{Var}(Y) > \\mathrm{E}(Y)$. This is a comparison to the Poisson distribution, for which $\\mathrm{Var}(Y) = \\mathrm{E}(Y)$. \"Excess zeros\" means that the observed zero-count probability, $\\mathbb{P}(Y=0)$, is greater than that predicted by a baseline model with the same mean. For a Poisson baseline with mean $\\mu = \\mathrm{E}(Y)$, this would mean $\\mathbb{P}(Y=0) > \\exp(-\\mu)$.\n\nThe variance, defined as $\\mathrm{Var}(Y) = \\mathrm{E}[(Y-\\mu)^2] = \\sum_{k=0}^{\\infty} (k-\\mu)^2 \\mathbb{P}(Y=k)$, is a global property of the distribution, as it is an average weighted over all possible outcomes. It measures the spread of the probability mass around the mean. A large variance can be caused by having more probability mass in the tails of the distribution (i.e., at values far from the mean) compared to the baseline.\n\nIn contrast, $\\mathbb{P}(Y=0)$ is a local property, referring to the probability mass at a single point, $k=0$. It is possible to inflate the variance without increasing, and even while decreasing, the probability mass at $k=0$. This can be achieved by shifting probability mass from near the mean to the upper tail of the distribution. Taking probability mass from $k=0$ and relocating it to a very large value of $k$ will simultaneously decrease $\\mathbb{P}(Y=0)$ and increase the variance, as the large squared deviation $(k-\\mu)^2$ for the new large outcome will heavily contribute to the sum.\n\nTo provide a concrete construction, let us define a baseline model $Y_P \\sim \\mathrm{Poisson}(\\lambda=1)$.\nFor this baseline:\n- Mean: $\\mathrm{E}[Y_P] = 1$.\n- Variance: $\\mathrm{Var}[Y_P] = 1$.\n- Zero Probability: $\\mathbb{P}(Y_P=0) = \\exp(-1) \\approx 0.3679$.\n\nNow, we construct a new discrete random variable $Y$ with the following probability mass function (PMF), designed to have the same mean, be overdispersed, but exhibit a zero deficit:\nLet $Y$ take values in $\\{0, 1, 11\\}$ with the PMF:\n- $\\mathbb{P}(Y=0) = 0.1$\n- $\\mathbb{P}(Y=1) = 0.89$\n- $\\mathbb{P}(Y=11) = 0.01$\nThe probabilities sum to $0.1 + 0.89 + 0.01 = 1$, so this is a valid PMF.\n\nLet's check its properties:\n- Mean: $\\mathrm{E}[Y] = (0 \\times 0.1) + (1 \\times 0.89) + (11 \\times 0.01) = 0 + 0.89 + 0.11 = 1$. The mean is identical to the Poisson baseline.\n- Zero Probability: $\\mathbb{P}(Y=0) = 0.1$. Comparing this to the baseline, $0.1 < \\exp(-1) \\approx 0.3679$. This distribution thus has a \"zero deficit,\" not an excess of zeros.\n- Variance: First, we calculate the second moment, $\\mathrm{E}[Y^2]$.\n$\\mathrm{E}[Y^2] = (0^2 \\times 0.1) + (1^2 \\times 0.89) + (11^2 \\times 0.01) = 0 + 0.89 + (121 \\times 0.01) = 0.89 + 1.21 = 2.1$.\nThe variance is $\\mathrm{Var}(Y) = \\mathrm{E}[Y^2] - (\\mathrm{E}[Y])^2 = 2.1 - 1^2 = 1.1$.\nSince $\\mathrm{Var}(Y) = 1.1 > \\mathrm{E}[Y] = 1$, the distribution is overdispersed.\n\nThis construction demonstrates a random variable $Y$ that is overdispersed relative to its mean but has a smaller zero-count probability than the corresponding Poisson distribution. Therefore, the observation of overdispersion alone is not sufficient to conclude the presence of excess zeros.\n\n2) We are asked to derive the marginal probability $\\mathbb{P}(Y=0)$ for a Negative Binomial model defined as a Poisson-Gamma mixture. The model is given by:\n$Y \\mid \\Theta \\sim \\mathrm{Poisson}(\\lambda\\Theta)$\n$\\Theta \\sim \\mathrm{Gamma}(\\text{shape}=k, \\text{rate}=k)$\n\nThe marginal probability $\\mathbb{P}(Y=0)$ is obtained by averaging the conditional probability $\\mathbb{P}(Y=0|\\Theta)$ over the distribution of the frailty parameter $\\Theta$.\nThe probability density function (PDF) of $\\Theta \\sim \\mathrm{Gamma}(\\text{shape}=\\alpha, \\text{rate}=\\beta)$ is $f(\\theta; \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\theta^{\\alpha-1}\\exp(-\\beta\\theta)$ for $\\theta > 0$. In our case, $\\alpha=k$ and $\\beta=k$, so the PDF of $\\Theta$ is:\n$$f_{\\Theta}(\\theta) = \\frac{k^k}{\\Gamma(k)}\\theta^{k-1}\\exp(-k\\theta)$$\nFor a Poisson distribution with mean $\\mu$, the probability of an outcome of zero is $\\exp(-\\mu)$. Here, the conditional mean is $\\lambda\\Theta$, so the conditional probability of zero is:\n$$\\mathbb{P}(Y=0|\\Theta=\\theta) = \\exp(-\\lambda\\theta)$$\nTo find the marginal probability, we compute the expectation of the conditional probability with respect to $\\Theta$:\n$$\\mathbb{P}(Y=0) = \\mathrm{E}_{\\Theta}[\\mathbb{P}(Y=0|\\Theta)] = \\int_0^\\infty \\mathbb{P}(Y=0|\\Theta=\\theta) f_{\\Theta}(\\theta) d\\theta$$\nSubstituting the expressions for the conditional probability and the PDF of $\\Theta$:\n$$\\mathbb{P}(Y=0) = \\int_0^\\infty \\exp(-\\lambda\\theta) \\left( \\frac{k^k}{\\Gamma(k)}\\theta^{k-1}\\exp(-k\\theta) \\right) d\\theta$$\nWe can combine the exponential terms and pull the constants outside the integral:\n$$\\mathbb{P}(Y=0) = \\frac{k^k}{\\Gamma(k)} \\int_0^\\infty \\theta^{k-1} \\exp(-k\\theta - \\lambda\\theta) d\\theta$$\n$$\\mathbb{P}(Y=0) = \\frac{k^k}{\\Gamma(k)} \\int_0^\\infty \\theta^{k-1} \\exp(-(\\lambda+k)\\theta) d\\theta$$\nThe integral is the kernel of a Gamma distribution's normalizing constant. We use the identity $\\int_0^\\infty x^{\\alpha-1}\\exp(-\\beta x)dx = \\frac{\\Gamma(\\alpha)}{\\beta^\\alpha}$. Here, $\\alpha=k$ and $\\beta=\\lambda+k$.\nApplying this identity, the integral evaluates to:\n$$\\int_0^\\infty \\theta^{k-1} \\exp(-(\\lambda+k)\\theta) d\\theta = \\frac{\\Gamma(k)}{(\\lambda+k)^k}$$\nSubstituting this result back into our expression for $\\mathbb{P}(Y=0)$:\n$$\\mathbb{P}(Y=0) = \\frac{k^k}{\\Gamma(k)} \\left( \\frac{\\Gamma(k)}{(\\lambda+k)^k} \\right) = \\frac{k^k}{(\\lambda+k)^k}$$\nThus, the closed-form expression for the marginal zero probability is:\n$$\\mathbb{P}(Y=0) = \\left(\\frac{k}{\\lambda+k}\\right)^k$$\nFor the given cohort, we have the mean count $\\lambda=2.4$ and the dispersion parameter $k=1.5$. We evaluate the derived expression numerically:\n$$\\mathbb{P}(Y=0) = \\left(\\frac{1.5}{2.4+1.5}\\right)^{1.5} = \\left(\\frac{1.5}{3.9}\\right)^{1.5} = \\left(\\frac{5}{13}\\right)^{1.5}$$\nCalculating the value:\n$$\\left(\\frac{5}{13}\\right)^{1.5} \\approx (0.38461538)^{1.5} \\approx 0.23853535$$\nRounding this result to four significant figures gives $0.2385$.", "answer": "$$\\boxed{0.2385}$$", "id": "4993564"}, {"introduction": "After fitting a zero-inflated model, a key task is to interpret its components in the context of the scientific question. The Zero-Inflated Poisson (ZIP) model, by its very structure, separates observations into a \"structural zero\" group and an \"at-risk\" group. This practice guides you through applying Bayes' rule to quantify the probability that a specific zero observation belongs to the structural-zero component, a crucial skill for translating model parameters into meaningful insights [@problem_id:4993558].", "problem": "A hospital system models monthly counts of emergency department visits due to asthma exacerbations using a Zero-Inflated Poisson (ZIP) model. The ZIP model assumes the following generative mechanism: for patient $i$, with probability $\\pi_i$ the patient is in a structural-zero state and the observed count $Y_i$ is deterministically $0$; otherwise, with probability $1 - \\pi_i$, the count arises from a Poisson distribution with mean $\\lambda_i$. The structural-zero probability $\\pi_i$ is linked to covariates via the logistic (logit) link, and the Poisson mean $\\lambda_i$ is linked to covariates via the logarithm (log) link, both being standard in generalized linear modeling.\n\nFor an individual patient $i$, suppose the fitted covariate models are\n$$\\mathrm{logit}(\\pi_i) = \\alpha_0 + \\alpha_1 \\,\\mathrm{Prophylaxis}_i + \\alpha_2 \\,\\mathrm{Immunocompromised}_i + \\alpha_3 \\,\\mathrm{PriorExacerbation}_i,$$\n$$\\ln(\\lambda_i) = \\beta_0 + \\beta_1 \\,\\mathrm{Smoking}_i + \\beta_2 \\,\\frac{\\mathrm{Age}_i}{10} + \\beta_3 \\,\\mathrm{Prophylaxis}_i + \\beta_4 \\,\\mathrm{ComorbidityIndex}_i.$$\n\nAssume the following estimated coefficients are given from a validated study:\n$$\\alpha_0 = -0.5,\\quad \\alpha_1 = 0.8,\\quad \\alpha_2 = 0.4,\\quad \\alpha_3 = -0.6,$$\n$$\\beta_0 = -0.6,\\quad \\beta_1 = 0.4,\\quad \\beta_2 = 0.05,\\quad \\beta_3 = -0.3,\\quad \\beta_4 = 0.1.$$\n\nFor the patient of interest, the covariate values are\n$$\\mathrm{Prophylaxis}_i = 1,\\quad \\mathrm{Immunocompromised}_i = 0,\\quad \\mathrm{PriorExacerbation}_i = 0,$$\n$$\\mathrm{Smoking}_i = 1,\\quad \\mathrm{Age}_i = 60,\\quad \\mathrm{ComorbidityIndex}_i = 2.$$\n\nIn a given month, the observed count is $y_i = 0$. Starting from the mixture definition of the ZIP model and Bayes’ rule, derive and compute the posterior probability that this observed zero came from the structural-zero component rather than the Poisson component, using the above $(\\pi_i,\\lambda_i)$ implied by the fitted models. Express your final answer as a decimal and round your answer to four significant figures.", "solution": "This problem requires the calculation of a posterior probability within the framework of a Zero-Inflated Poisson (ZIP) model. Specifically, given that an observed count is zero, we need to determine the probability that this zero originates from the \"structural-zero\" component of the model rather than the \"count\" (Poisson) component. This can be solved using Bayes' rule.\n\nLet $Y_i$ be the random variable representing the count of emergency department visits for patient $i$. The ZIP model postulates a mixture of two processes. We can introduce a latent binary variable, $S_i$, that indicates the component from which the observation is drawn.\nLet $S_i = 1$ if the patient is in the structural-zero group (an \"always-zero\" patient for the given context). The probability of this is $P(S_i=1) = \\pi_i$.\nLet $S_i = 0$ if the patient is in the \"at-risk\" group, whose counts follow a Poisson distribution. The probability of this is $P(S_i=0) = 1-\\pi_i$.\n\nThe model is defined as:\n- If $S_i=1$, then the observed count $Y_i$ is deterministically $0$. Thus, the conditional probability is $P(Y_i=0 | S_i=1) = 1$.\n- If $S_i=0$, then the count $Y_i$ follows a Poisson distribution with mean $\\lambda_i$, i.e., $Y_i | (S_i=0) \\sim \\mathrm{Poisson}(\\lambda_i)$. The probability mass function is $P(Y_i=k | S_i=0) = \\frac{\\exp(-\\lambda_i)\\lambda_i^k}{k!}$ for $k \\in \\{0, 1, 2, ...\\}$.\n\nThe problem states that for the patient of interest, the observed count is $y_i = 0$. We are asked to find the posterior probability that this zero is a structural zero, which is $P(S_i=1 | Y_i=0)$.\n\nUsing Bayes' rule, this posterior probability is given by:\n$$P(S_i=1 | Y_i=0) = \\frac{P(Y_i=0 | S_i=1) P(S_i=1)}{P(Y_i=0)}$$\nThe terms in this equation are:\n- $P(S_i=1) = \\pi_i$, the prior probability of being in the structural-zero group.\n- $P(Y_i=0 | S_i=1) = 1$, as defined by the model.\n- $P(Y_i=0)$ is the total probability of observing a zero, which can be found using the law of total probability:\n$$P(Y_i=0) = P(Y_i=0 | S_i=1)P(S_i=1) + P(Y_i=0 | S_i=0)P(S_i=0)$$\nThe term $P(Y_i=0 | S_i=0)$ is the probability of observing a zero from the Poisson component, which is:\n$$P(Y_i=0 | S_i=0) = \\frac{\\exp(-\\lambda_i)\\lambda_i^0}{0!} = \\exp(-\\lambda_i)$$\nSubstituting the components into the law of total probability gives:\n$$P(Y_i=0) = (1)(\\pi_i) + (\\exp(-\\lambda_i))(1-\\pi_i) = \\pi_i + (1-\\pi_i)\\exp(-\\lambda_i)$$\nNow, we can write the final expression for the desired posterior probability:\n$$P(S_i=1 | Y_i=0) = \\frac{1 \\cdot \\pi_i}{\\pi_i + (1-\\pi_i)\\exp(-\\lambda_i)}$$\n\nThe next step is to calculate the patient-specific parameters $\\pi_i$ and $\\lambda_i$ using the given covariate models and estimated coefficients.\n\nFirst, we calculate the linear predictor for the structural-zero probability, $\\pi_i$:\n$$\\mathrm{logit}(\\pi_i) = \\alpha_0 + \\alpha_1 \\,\\mathrm{Prophylaxis}_i + \\alpha_2 \\,\\mathrm{Immunocompromised}_i + \\alpha_3 \\,\\mathrm{PriorExacerbation}_i$$\nSubstituting the given coefficients and covariate values:\n$$\\mathrm{logit}(\\pi_i) = -0.5 + (0.8)(1) + (0.4)(0) + (-0.6)(0) = -0.5 + 0.8 = 0.3$$\nTo find $\\pi_i$, we apply the inverse logit (logistic) function:\n$$\\pi_i = \\mathrm{logit}^{-1}(0.3) = \\frac{\\exp(0.3)}{1 + \\exp(0.3)}$$\n\nNext, we calculate the linear predictor for the Poisson mean, $\\lambda_i$:\n$$\\ln(\\lambda_i) = \\beta_0 + \\beta_1 \\,\\mathrm{Smoking}_i + \\beta_2 \\,\\frac{\\mathrm{Age}_i}{10} + \\beta_3 \\,\\mathrm{Prophylaxis}_i + \\beta_4 \\,\\mathrm{ComorbidityIndex}_i$$\nSubstituting the given coefficients and covariate values:\n$$\\ln(\\lambda_i) = -0.6 + (0.4)(1) + (0.05)\\left(\\frac{60}{10}\\right) + (-0.3)(1) + (0.1)(2)$$\n$$\\ln(\\lambda_i) = -0.6 + 0.4 + (0.05)(6) - 0.3 + 0.2$$\n$$\\ln(\\lambda_i) = -0.6 + 0.4 + 0.3 - 0.3 + 0.2 = 0$$\nTo find $\\lambda_i$, we apply the inverse log (exponential) function:\n$$\\lambda_i = \\exp(0) = 1$$\n\nNow we substitute the expressions for $\\pi_i$ and the value of $\\lambda_i$ into our posterior probability formula. To simplify, we can multiply the numerator and denominator by $(1 + \\exp(0.3))$:\n$$P(S_i=1 | Y_i=0) = \\frac{\\frac{\\exp(0.3)}{1 + \\exp(0.3)}}{\\frac{\\exp(0.3)}{1 + \\exp(0.3)} + \\left(1 - \\frac{\\exp(0.3)}{1 + \\exp(0.3)}\\right)\\exp(-1)}$$\n$$P(S_i=1 | Y_i=0) = \\frac{\\frac{\\exp(0.3)}{1 + \\exp(0.3)}}{\\frac{\\exp(0.3)}{1 + \\exp(0.3)} + \\frac{1}{1 + \\exp(0.3)}\\exp(-1)}$$\n$$P(S_i=1 | Y_i=0) = \\frac{\\exp(0.3)}{\\exp(0.3) + \\exp(-1)}$$\nNow we compute the numerical value:\n$$\\exp(0.3) \\approx 1.3498588$$\n$$\\exp(-1) \\approx 0.3678794$$\n$$P(S_i=1 | Y_i=0) \\approx \\frac{1.3498588}{1.3498588 + 0.3678794} = \\frac{1.3498588}{1.7177382} \\approx 0.7858310$$\nRounding the result to four significant figures, as requested, we obtain $0.7858$.\n\nThis value represents the posterior probability that the observed zero count for this patient is due to the patient being in the structural-zero state, given the model and their specific covariates.", "answer": "$$\\boxed{0.7858}$$", "id": "4993558"}, {"introduction": "No model is a perfect representation of reality, making model diagnostics an indispensable step in any statistical analysis. This hands-on exercise delves into the computation and interpretation of residuals, the workhorses of model assessment, for both zero-inflated and hurdle models. By deriving and implementing these diagnostic tools from their foundational definitions, you will develop a nuanced understanding of how to evaluate model fit and identify discrepancies between your model and the data [@problem_id:4993527].", "problem": "Consider count outcomes generated from clinical events in which the data exhibit excess zeros beyond what a standard Poisson model would predict. Two widely used approaches are the Zero-Inflated Poisson (ZIP) model and the hurdle model. You are asked to construct a program that, for specified parameter values and observed counts, computes Pearson residuals and deviance residuals under both models and aggregates the results for a given test suite. Your derivations and implementation must start from the following foundational bases: mixture distribution definitions, the law of total expectation, the law of total variance, the definition of the probability mass function, and the definition of residuals from likelihood contributions.\n\nDefinitions and modeling setup:\n- The ZIP model assumes that an observation $Y$ arises from a mixture of a structural-zero process with probability $\\pi \\in (0,1)$ and a Poisson process with rate $\\lambda \\in (0,\\infty)$ with probability $1-\\pi$. The probability mass function (PMF) is defined by $P(Y=0)=\\pi+(1-\\pi)\\exp(-\\lambda)$ and $P(Y=y)= (1-\\pi)\\exp(-\\lambda)\\lambda^y/y!$ for $y \\in \\{1,2,3,\\dots\\}$. The expected value $E[Y]$ and variance $\\operatorname{Var}(Y)$ must be obtained using the law of total expectation and the law of total variance applied to the mixture specification, not by shortcut formulas.\n- The hurdle model assumes a two-part process: a structural-zero process with probability $\\pi_h \\in (0,1)$ and a positive-count process with probability $1-\\pi_h$. Conditional on being positive, $Y$ follows a Zero-Truncated Poisson (ZTP) distribution with rate $\\lambda \\in (0,\\infty)$, that is, the conditional PMF is $P(Y=y \\mid Y>0) = \\left[\\exp(-\\lambda)\\lambda^y/y!\\right]/\\left[1-\\exp(-\\lambda)\\right]$ for $y \\in \\{1,2,3,\\dots\\}$. The unconditional PMF of the hurdle model is $P(Y=0)=\\pi_h$ and $P(Y=y>0)=(1-\\pi_h)P(Y=y \\mid Y>0)$. The expected value $E[Y]$ and variance $\\operatorname{Var}(Y)$ must be derived from the mixture structure and the truncated positive-count distribution via the law of total expectation and the law of total variance.\n\nResiduals to compute:\n- The Pearson residual for a single observation $y$ is $r_P = \\dfrac{y - \\mu}{\\sqrt{\\sigma^2}}$, where $\\mu = E[Y]$ and $\\sigma^2 = \\operatorname{Var}(Y)$ are computed under the specified model and parameters.\n- The deviance residual for a single observation $y$ is $r_D = \\operatorname{sign}(y - \\mu)\\sqrt{2\\left(\\ell_\\text{sat}(y) - \\ell(y;\\theta)\\right)}$, where $\\ell(y;\\theta)$ is the log-likelihood contribution under the specified model with parameters $\\theta$ and $\\ell_\\text{sat}(y)$ is the saturated log-likelihood contribution. For mixture count models considered here, the saturated log-likelihood places a unit probability mass on the realized outcome, yielding $\\ell_\\text{sat}(y)=0$, so the deviance residual reduces to $r_D = \\operatorname{sign}(y - \\mu)\\sqrt{-2\\log p(y;\\theta)}$, where $p(y;\\theta)$ is the model PMF evaluated at $y$. Your computation must explicitly use the appropriate PMF for ZIP or hurdle and handle $y=0$ using the mixture mass and $y>0$ using the Poisson or Zero-Truncated Poisson terms, respectively, without employing any pre-simplified shortcuts.\n\nInterpretation requirements:\n- In the ZIP setting, explain how $r_P$ and $r_D$ reflect the influence of the zero-inflation parameter $\\pi$ on the dispersion and the likelihood contribution at $y=0$ versus $y>0$.\n- In the hurdle setting, explain how truncation in the positive component changes the reference mean and variance for $y>0$, and how this affects $r_P$ and $r_D$ compared to non-truncated models, especially for small $\\lambda$.\n\nYour program must implement the computations numerically in a stable way. When computing log-likelihood contributions, use sums and logarithms that avoid numerical underflow for small probabilities. For factorial terms, compute $\\log(y!)$ using the logarithm of the gamma function to avoid loss of precision. For mixture probabilities of zeros, compute $\\log\\left(\\pi + (1-\\pi)\\exp(-\\lambda)\\right)$ using a numerically stable summation.\n\nTest suite:\nCompute $(r_P,r_D)$ for each of the following parameter sets, expressed as ordered quadruples $(\\text{model}, y, \\pi\\text{ or }\\pi_h, \\lambda)$:\n- Case $1$: $($ZIP$,\\, y=$3$,\\, \\pi=$0.2$,\\, \\lambda=$2.5$)$\n- Case $2$: $($ZIP$,\\, y=$0$,\\, \\pi=$0.85$,\\, \\lambda=$0.7$)$\n- Case $3$: $($ZIP$,\\, y=$10$,\\, \\pi=$0.1$,\\, \\lambda=$1.2$)$\n- Case $4$: $($hurdle$,\\, y=$2$,\\, \\pi_h=$0.3$,\\, \\lambda=$1.8$)$\n- Case $5$: $($hurdle$,\\, y=$0$,\\, \\pi_h=$0.05$,\\, \\lambda=$0.2$)$\n- Case $6$: $($hurdle$,\\, y=$7$,\\, \\pi_h=$0.6$,\\, \\lambda=$3.0$)$\n\nOutput requirements:\n- For each case, return a list $[r_P, r_D]$ with both values rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the $[r_P, r_D]$ pair of a case, in the same order as the test suite (for example, $[[r_{P,1},r_{D,1}],[r_{P,2},r_{D,2}],\\dots]$)).\n- All probability parameters must be treated and reported internally as decimals or fractions, not with a percentage sign.\n\nYour solution must clearly justify, from the fundamental bases, each formula implemented in the code, and discuss the interpretation of residuals in the presence of excess zeros and, for hurdle models, truncation of the positive counts. No external data or user input is permitted, and no hints or shortcut formulas are to be used in the problem statement beyond the foundational bases listed above.", "solution": "The problem statement has been critically validated and is deemed to be scientifically grounded, well-posed, and objective. It presents a standard, albeit detailed, task in statistical modeling. All definitions for the Zero-Inflated Poisson (ZIP) and hurdle models are correct, and the required derivations are based on fundamental statistical principles. The parameters for the test suite are well-defined and within valid ranges. Therefore, I will proceed with a complete solution.\n\nThe solution is structured as follows: First, we derive the necessary properties (probability mass function, mean, variance) for the ZIP and hurdle models from the foundational principles specified. Second, we formalize the definitions of Pearson and deviance residuals. Third, we discuss the interpretation of these residuals in the context of the models. Finally, we apply these formulas to the provided test cases.\n\n**1. Theoretical Derivations**\n\n**1.1. Zero-Inflated Poisson (ZIP) Model**\n\nThe ZIP model conceptualizes an observation $Y$ as arising from a two-state mixture process. Let $Z$ be a Bernoulli random variable with $P(Z=1) = \\pi$ and $P(Z=0) = 1-\\pi$. If $Z=1$, the outcome is a \"structural zero,\" so $Y=0$. If $Z=0$, the outcome $Y$ is drawn from a Poisson distribution with rate $\\lambda$, denoted as $Y_P \\sim \\text{Poisson}(\\lambda)$.\n\n**Probability Mass Function (PMF):**\nThe PMF, $p(y;\\pi,\\lambda) = P(Y=y)$, is derived using the law of total probability:\n$P(Y=y) = P(Y=y|Z=1)P(Z=1) + P(Y=y|Z=0)P(Z=0)$.\nFor $y=0$:\n$$P(Y=0) = (1)\\cdot\\pi + P(Y_P=0)\\cdot(1-\\pi) = \\pi + (1-\\pi)e^{-\\lambda}$$\nFor integer $y > 0$:\n$$P(Y=y) = (0)\\cdot\\pi + P(Y_P=y)\\cdot(1-\\pi) = (1-\\pi)\\frac{e^{-\\lambda}\\lambda^y}{y!}$$\n\n**Expected Value ($E[Y]$):**\nUsing the law of total expectation, $\\mu = E[Y] = E[E[Y|Z]]$:\nThe conditional expectations are $E[Y|Z=1]=0$ and $E[Y|Z=0]=E[Y_P]=\\lambda$.\n$$ \\mu = E[Y|Z=1]P(Z=1) + E[Y|Z=0]P(Z=0) = 0 \\cdot \\pi + \\lambda \\cdot (1-\\pi) = (1-\\pi)\\lambda $$\n\n**Variance ($\\operatorname{Var}(Y)$):**\nUsing the law of total variance, $\\sigma^2 = \\operatorname{Var}(Y) = E[\\operatorname{Var}(Y|Z)] + \\operatorname{Var}(E[Y|Z])$:\nThe conditional variances are $\\operatorname{Var}(Y|Z=1)=0$ and $\\operatorname{Var}(Y|Z=0)=\\operatorname{Var}(Y_P)=\\lambda$.\nThe first term is $E[\\operatorname{Var}(Y|Z)] = 0 \\cdot \\pi + \\lambda \\cdot (1-\\pi) = (1-\\pi)\\lambda$.\nThe second term, $\\operatorname{Var}(E[Y|Z])$, is the variance of a random variable that takes value $E[Y|Z=1]=0$ with probability $\\pi$ and $E[Y|Z=0]=\\lambda$ with probability $1-\\pi$.\n$E[E[Y|Z]] = \\mu = (1-\\pi)\\lambda$.\n$E[(E[Y|Z])^2] = 0^2 \\cdot \\pi + \\lambda^2 \\cdot (1-\\pi) = (1-\\pi)\\lambda^2$.\n$\\operatorname{Var}(E[Y|Z]) = E[(E[Y|Z])^2] - (E[E[Y|Z]])^2 = (1-\\pi)\\lambda^2 - ((1-\\pi)\\lambda)^2 = (1-\\pi)\\lambda^2 - (1-\\pi)^2\\lambda^2 = \\lambda^2(1-\\pi)(1-(1-\\pi)) = \\pi(1-\\pi)\\lambda^2$.\nCombining the terms:\n$$ \\sigma^2 = (1-\\pi)\\lambda + \\pi(1-\\pi)\\lambda^2 = (1-\\pi)\\lambda(1+\\pi\\lambda) $$\nThis variance is greater than the mean, demonstrating the overdispersion characteristic of the ZIP model.\n\n**1.2. Hurdle Model**\n\nThe hurdle model is a two-part model. A binary process determines whether the count is zero or positive. Let this be governed by a Bernoulli trial with probability $\\pi_h$ for a zero outcome. If the outcome is positive (with probability $1-\\pi_h$), the count $y$ follows a Zero-Truncated Poisson (ZTP) distribution with rate $\\lambda$.\n\n**PMF of the ZTP distribution:**\nA random variable $Y^*$ follows a ZTP distribution if its PMF for $y \\in \\{1, 2, \\dots\\}$ is given by:\n$$ P(Y^*=y) = P(\\text{Poisson}=y | \\text{Poisson}>0) = \\frac{P(\\text{Poisson}=y)}{P(\\text{Poisson}>0)} = \\frac{e^{-\\lambda}\\lambda^y/y!}{1-e^{-\\lambda}} $$\n\n**Moments of the ZTP distribution:**\nThe expected value $E[Y^*]$ is:\n$$ E[Y^*] = \\frac{1}{1-e^{-\\lambda}} \\sum_{k=1}^{\\infty} k \\frac{e^{-\\lambda}\\lambda^k}{k!} = \\frac{e^{-\\lambda}}{1-e^{-\\lambda}} \\sum_{k=1}^{\\infty} \\frac{\\lambda^k}{(k-1)!} = \\frac{e^{-\\lambda}\\lambda}{1-e^{-\\lambda}} \\sum_{j=0}^{\\infty} \\frac{\\lambda^j}{j!} = \\frac{e^{-\\lambda}\\lambda e^{\\lambda}}{1-e^{-\\lambda}} = \\frac{\\lambda}{1-e^{-\\lambda}} $$\nSimilarly, the second raw moment is $E[(Y^*)^2] = \\frac{\\lambda+\\lambda^2}{1-e^{-\\lambda}}$.\nThe variance is $\\operatorname{Var}(Y^*) = E[(Y^*)^2] - (E[Y^*])^2 = \\frac{\\lambda+\\lambda^2}{1-e^{-\\lambda}} - \\left(\\frac{\\lambda}{1-e^{-\\lambda}}\\right)^2$.\n\n**Unconditional PMF of the Hurdle Model:**\nFor $y=0$:\n$$ P(Y=0) = \\pi_h $$\nFor integer $y > 0$:\n$$ P(Y=y) = (1-\\pi_h)P(Y^*=y) = (1-\\pi_h)\\frac{e^{-\\lambda}\\lambda^y/y!}{1-e^{-\\lambda}} $$\n\n**Expected Value ($E[Y]$):**\nUsing the law of total expectation, where the two states are \"zero count\" and \"positive count\":\n$$ \\mu = E[Y] = 0 \\cdot P(Y=0) + E[Y^*] \\cdot P(Y>0) = E[Y^*](1-\\pi_h) = (1-\\pi_h)\\frac{\\lambda}{1-e^{-\\lambda}} $$\n\n**Variance ($\\operatorname{Var}(Y)$):**\nUsing the law of total variance, $\\sigma^2 = E[\\operatorname{Var}(Y|\\text{state})] + \\operatorname{Var}(E[Y|\\text{state}])$.\n$E[Y|\\text{zero state}] = 0$ and $\\operatorname{Var}(Y|\\text{zero state}) = 0$.\n$E[Y|\\text{positive state}] = E[Y^*]$ and $\\operatorname{Var}(Y|\\text{positive state}) = \\operatorname{Var}(Y^*)$.\n$E[\\operatorname{Var}(Y|\\text{state})] = 0 \\cdot \\pi_h + \\operatorname{Var}(Y^*) \\cdot (1-\\pi_h) = (1-\\pi_h)\\operatorname{Var}(Y^*)$.\n$\\operatorname{Var}(E[Y|\\text{state}])$ is the variance of a variable taking value $0$ with probability $\\pi_h$ and $E[Y^*]$ with probability $1-\\pi_h$.\n$\\operatorname{Var}(E[Y|\\text{state}]) = E[(E[Y|\\text{state}])^2] - (E[E[Y|\\text{state}]])^2 = (1-\\pi_h)(E[Y^*])^2 - ((1-\\pi_h)E[Y^*])^2 = \\pi_h(1-\\pi_h)(E[Y^*])^2$.\nCombining the terms:\n$$ \\sigma^2 = (1-\\pi_h)\\operatorname{Var}(Y^*) + \\pi_h(1-\\pi_h)(E[Y^*])^2 $$\nSubstituting the moments of the ZTP gives the full expression for the variance.\n\n**1.3. Residual Definitions**\n\nA residual measures the discrepancy between an observation $y$ and its value predicted by a model.\n**Pearson Residual ($r_P$):**\nThis residual is the standardized difference between the observation and the model's expected value.\n$$ r_P = \\frac{y - \\mu}{\\sqrt{\\sigma^2}} $$\nwhere $\\mu=E[Y]$ and $\\sigma^2=\\operatorname{Var}(Y)$ are the mean and variance under the specified model (ZIP or hurdle).\n\n**Deviance Residual ($r_D$):**\nThis residual is based on the contribution of the observation to the log-likelihood function.\n$$ r_D = \\operatorname{sign}(y - \\mu)\\sqrt{2\\left(\\ell_\\text{sat}(y) - \\ell(y;\\theta)\\right)} $$\nHere, $\\theta$ represents the model parameters (e.g., $\\{\\pi, \\lambda\\}$). The saturated log-likelihood $\\ell_\\text{sat}(y)$ corresponds to a model that perfectly fits the data point, which for a discrete observation $y$ means assigning probability $1$ to that outcome. Thus, $p_\\text{sat}(y)=1$ and $\\ell_\\text{sat}(y) = \\log(1) = 0$. The formula simplifies to:\n$$ r_D = \\operatorname{sign}(y - \\mu)\\sqrt{-2\\ell(y;\\theta)} = \\operatorname{sign}(y - \\mu)\\sqrt{-2\\log p(y;\\theta)} $$\nwhere $p(y;\\theta)$ is the PMF of the specified model.\n\n**2. Interpretation of Residuals**\n\n**ZIP Model:** The zero-inflation parameter $\\pi$ explicitly models excess zeros.\n- Its influence on $r_P$: The variance $\\sigma^2=(1-\\pi)\\lambda(1+\\pi\\lambda)$ is a key component. As $\\pi$ increases, it generally increases the overdispersion, leading to a larger variance $\\sigma^2$ for a given Poisson rate $\\lambda$. This larger denominator in $r_P$ may shrink the residual's magnitude, reflecting the model's built-in expectation of higher variability.\n- Its influence on $r_D$: For an observation $y=0$, the PMF is $p(0) = \\pi+(1-\\pi)e^{-\\lambda}$. A larger $\\pi$ increases $p(0)$, bringing $\\log p(0)$ closer to $0$. This results in a smaller deviance residual, indicating that the model considers the zero to be a good fit. For an observation $y>0$, the PMF is $p(y)=(1-\\pi) \\frac{e^{-\\lambda}\\lambda^y}{y!}$. A larger $\\pi$ decreases this probability, making $\\log p(y)$ more negative and thus increasing the deviance residual, indicating a worse fit. The parameter $\\pi$ thus arbitrates the model's goodness-of-fit between zero and non-zero counts.\n\n**Hurdle Model:** The hurdle model separates the zero-generation process from the positive-count generation process.\n- Effect of truncation: The positive component follows a ZTP distribution, whose mean $E[Y^*] = \\lambda/(1-e^{-\\lambda})$ is strictly greater than the mean $\\lambda$ of the untruncated Poisson. This difference is most pronounced for small $\\lambda$. For example, as $\\lambda \\to 0$, $E[Y^*] \\to 1$, while $\\lambda \\to 0$. The ZTP model \"knows\" that a positive outcome must be at least $1$, so its expectation is shifted upwards. The overall model mean $\\mu = (1-\\pi_h)E[Y^*]$ reflects this.\n- Effect on residuals: When computing residuals for a positive observation $y>0$, the hurdle model uses a reference mean $\\mu$ and a log-likelihood $\\log p(y)$ that are conditioned on the count being positive. Compared to a non-truncated model (like a standard Poisson or even ZIP), the hurdle model's conditional probability for a small positive count (e.g., $y=1$) is higher, especially for small $\\lambda$. This leads to smaller deviance residuals for such counts. The Pearson residual is also affected as the numerator $y-\\mu$ is calculated with respect to a higher effective mean for the positive part of the process.\n\n**3. Numerical Implementation Notes**\n\nFor robust computation, the following are necessary:\n- The term $\\log(y!)$ is computed using the log-gamma function, `gammaln(y+1)`, to maintain precision and avoid overflow for large $y$.\n- The log-PMF for a ZIP model at $y=0$, $\\log(\\pi + (1-\\pi)e^{-\\lambda})$, is computed using a log-sum-exp operation to avoid underflow: $\\log(e^{\\log(\\pi)} + e^{\\log(1-\\pi)-\\lambda})$.\n- The log-PMF for a hurdle model for $y>0$ involves the term $\\log(1-e^{-\\lambda})$, which is computed as `log1p(-exp(-lambda))` for numerical stability when $\\lambda$ is small.\n\n**4. Application to Test Cases**\n\nThe derived formulas are now applied to the specified test suite. For each case, we compute the model mean $\\mu$, variance $\\sigma^2$, and the log-PMF $\\ell(y;\\theta) = \\log p(y;\\theta)$ to find the residuals $r_P$ and $r_D$.\n\n- **Case 1:** (ZIP, $y=3$, $\\pi=0.2$, $\\lambda=2.5$)\n  - $\\mu = 2.0$, $\\sigma^2 = 3.0$\n  - $\\log p(3) = -1.766033$\n  - $r_P = (3 - 2.0) / \\sqrt{3.0} \\approx 0.577350$\n  - $r_D = \\operatorname{sign}(3 - 2.0)\\sqrt{-2(-1.766033)} \\approx 1.879379$\n\n- **Case 2:** (ZIP, $y=0$, $\\pi=0.85$, $\\lambda=0.7$)\n  - $\\mu = 0.105$, $\\sigma^2 = 0.167475$\n  - $\\log p(0) = -0.078490$\n  - $r_P = (0 - 0.105) / \\sqrt{0.167475} \\approx -0.256575$\n  - $r_D = \\operatorname{sign}(0 - 0.105)\\sqrt{-2(-0.078490)} \\approx -0.396207$\n\n- **Case 3:** (ZIP, $y=10$, $\\pi=0.1$, $\\lambda=1.2$)\n  - $\\mu = 1.08$, $\\sigma^2 = 1.2096$\n  - $\\log p(10) = -14.586558$\n  - $r_P = (10 - 1.08) / \\sqrt{1.2096} \\approx 8.110452$\n  - $r_D = \\operatorname{sign}(10 - 1.08)\\sqrt{-2(-14.586558)} \\approx 5.401214$\n\n- **Case 4:** (hurdle, $y=2$, $\\pi_h=0.3$, $\\lambda=1.8$)\n  - $\\mu = 1.509529$, $\\sigma^2 = 1.947978$\n  - $\\log p(2) = -1.493617$\n  - $r_P = (2 - 1.509529) / \\sqrt{1.947978} \\approx 0.351412$\n  - $r_D = \\operatorname{sign}(2 - 1.509529)\\sqrt{-2(-1.493617)} \\approx 1.728362$\n\n- **Case 5:** (hurdle, $y=0$, $\\pi_h=0.05$, $\\lambda=0.2$)\n  - $\\mu = 1.048160$, $\\sigma^2 = 0.159135$\n  - $\\log p(0) = -2.995732$\n  - $r_P = (0 - 1.048160) / \\sqrt{0.159135} \\approx -2.627519$\n  - $r_D = \\operatorname{sign}(0 - 1.048160)\\sqrt{-2(-2.995732)} \\approx -2.447747$\n\n- **Case 6:** (hurdle, $y=7$, $\\pi_h=0.6$, $\\lambda=3.0$)\n  - $\\mu = 1.262869$, $\\sigma^2 = 3.456592$\n  - $\\log p(7) = -6.994715$\n  - $r_P = (7 - 1.262869) / \\sqrt{3.456592} \\approx 3.085827$\n  - $r_D = \\operatorname{sign}(7 - 1.262869)\\sqrt{-2(-6.994715)} \\approx 3.740244$", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Computes Pearson and deviance residuals for ZIP and Hurdle models for a given test suite.\n    \"\"\"\n    test_cases = [\n        (\"ZIP\", 3, 0.2, 2.5),\n        (\"ZIP\", 0, 0.85, 0.7),\n        (\"ZIP\", 10, 0.1, 1.2),\n        (\"hurdle\", 2, 0.3, 1.8),\n        (\"hurdle\", 0, 0.05, 0.2),\n        (\"hurdle\", 7, 0.6, 3.0),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        model, y, pi_param, lam = case\n        \n        mu = 0.0\n        var = 0.0\n        log_pmf = 0.0\n\n        if model == \"ZIP\":\n            pi = pi_param\n            # Expected value\n            mu = (1 - pi) * lam\n            \n            # Variance\n            var = (1 - pi) * lam * (1 + pi * lam)\n            \n            # Log-Probability Mass Function contribution\n            if y == 0:\n                # Use np.logaddexp for numerical stability: log(a+b) = log(exp(log(a)) + exp(log(b)))\n                log_p_zero_poisson = -lam\n                log_pmf = np.logaddexp(np.log(pi), np.log(1 - pi) + log_p_zero_poisson)\n            else:\n                log_pmf = np.log(1 - pi) - lam + y * np.log(lam) - gammaln(y + 1)\n        \n        elif model == \"hurdle\":\n            pi_h = pi_param\n            \n            if y == 0:\n                # For y=0, mean and variance of the whole distribution are needed for residuals\n                # but log_pmf is simple.\n                one_m_exp_lam = np.log1p(-np.exp(-lam))\n                mu_c = lam / (1 - np.exp(-lam)) # E[Y|Y>0]\n                mu = (1 - pi_h) * mu_c\n                \n                var_c = mu_c - (lam**2 * np.exp(-lam)) / (1 - np.exp(-lam))**2 # Var(Y|Y>0)\n                var = (1 - pi_h) * var_c + pi_h * (1 - pi_h) * mu_c**2\n                \n                log_pmf = np.log(pi_h)\n            else: # y > 0\n                # Mean and Variance for the whole distribution\n                # Note: log1p(-exp(-x)) is a stable way to compute log(1-exp(-x))\n                one_m_exp_lam = 1 - np.exp(-lam)\n                \n                mu_c = lam / one_m_exp_lam # E[Y|Y>0]\n                mu = (1 - pi_h) * mu_c\n                \n                var_c = mu_c - (lam**2 * np.exp(-lam)) / (one_m_exp_lam**2) # Var(Y|Y>0)\n                var = (1 - pi_h) * var_c + pi_h * (1 - pi_h) * mu_c**2\n\n                # Log-Probability Mass Function contribution\n                log_term_trunc = np.log1p(-np.exp(-lam))\n                log_pmf = np.log(1 - pi_h) - lam + y * np.log(lam) - gammaln(y + 1) - log_term_trunc\n\n        # Pearson residual\n        if var > 0:\n            r_p = (y - mu) / np.sqrt(var)\n        else: # Handle case of zero variance if it occurs\n            r_p = np.inf if y != mu else 0.0\n\n        # Deviance residual\n        r_d = np.sign(y - mu) * np.sqrt(-2 * log_pmf) if mu != y else 0.0\n        \n        # In the edge case y = mu, sign(0) is 0, so r_d becomes 0\n        if y == mu:\n            r_d = 0.0\n\n        results.append([round(r_p, 6), round(r_d, 6)])\n\n    # Format the final output string exactly as required\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4993527"}]}