{"hands_on_practices": [{"introduction": "Before drawing conclusions from a model, we must first determine whether our predictors have a genuine, statistically significant association with the outcome. The Likelihood Ratio Test ($LRT$) offers a fundamental framework for this by comparing the fit of a model that includes a predictor against a simpler, nested model that excludes it. This practice will guide you through the process of implementing an $LRT$ to test the overall significance of a biomarker in a multinomial logistic regression model, a core skill in model building and inference. [@problem_id:4976107]", "problem": "Consider a medical cohort where the categorical outcome represents disease status with $K$ mutually exclusive categories and a continuous biomarker measured for each patient. Assume the following baseline-category multinomial logistic regression representation: for categories indexed by $k \\in \\{1,\\dots,K-1\\}$ relative to a chosen baseline category $0$, the linear predictor for category $k$ is $\\eta_k = \\alpha_k + \\beta_k x$, where $x$ is the biomarker value for a patient, and the category probabilities are defined by the logistic link. The dataset consists of independent observations $\\{(x_i, y_i)\\}_{i=1}^N$ with $y_i \\in \\{0,1,\\dots,K-1\\}$ and biomarker values $x_i \\in \\mathbb{R}$. The modeling target is to assess whether the biomarker has any effect on the odds of assignment to any non-baseline category, that is, whether all slope parameters are zero, under the null hypothesis $H_0: \\beta_k = 0$ for all $k \\in \\{1,\\dots,K-1\\}$, versus the alternative $H_1: \\exists k$ such that $\\beta_k \\ne 0$.\n\nStarting exclusively from the following bases:\n- The definition of independent categorical outcomes with probabilities $(p_{0i}, p_{1i}, \\dots, p_{(K-1)i})$ for each observation $i$ with $p_{0i} + \\sum_{k=1}^{K-1} p_{ki} = 1$.\n- The principle of maximum likelihood estimation, whereby parameter estimates maximize the joint likelihood (equivalently minimize the negative log-likelihood) implied by the specified model.\n- The baseline-category multinomial logistic link that maps linear predictors to probabilities through the logistic transformation, ensuring valid probabilities that sum to one across categories.\n- The general likelihood ratio test principle, which compares nested models using their maximized likelihoods and evaluates statistical significance via the appropriate large-sample distribution determined by the difference in dimensionality between the models.\n\nYour task is to implement a program that:\n1. Constructs synthetic datasets under scientifically plausible medical scenarios using specified parameter sets and random seeds.\n2. Fits two models by maximizing the appropriate likelihoods: the unrestricted model allowing $\\beta_k$ to vary freely across $k \\in \\{1,\\dots,K-1\\}$, and the restricted model under $H_0$ where $\\beta_k = 0$ for all $k \\in \\{1,\\dots,K-1\\}$ (intercepts-only).\n3. Performs the likelihood ratio test for $H_0$ against $H_1$ for each dataset, reporting the test statistic, the degrees of freedom defined as the difference in the number of free parameters between the unrestricted and restricted models, and the corresponding large-sample $p$-value computed using the appropriate reference distribution. The $p$-value must be reported as a decimal, not a percentage.\n\nData generation and test suite:\n- For each test case, generate biomarker values $x_i$ independently from a normal distribution specified by a mean and standard deviation, and then generate outcomes $y_i$ independently according to the multinomial logistic model with the given true parameters. Use the baseline-category parameterization with the baseline category indexed by $0$.\n- Use the following test suite, where each tuple specifies $(N, K, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta}, \\mu_x, \\sigma_x, \\text{seed})$ with $\\boldsymbol{\\alpha} = (\\alpha_1,\\dots,\\alpha_{K-1})$ and $\\boldsymbol{\\beta} = (\\beta_1,\\dots,\\beta_{K-1})$:\n    1. $(800, 4, (-1.2, 0.5, -0.3), (0.8, -0.5, 0.4), 2.0, 1.0, 42)$, representing a four-category disease status with a biomarker showing heterogeneous effects across categories.\n    2. $(600, 3, (-0.2, -1.0), (0.0, 0.0), 1.0, 0.8, 123)$, representing a null-effect scenario where the biomarker has no impact on category odds.\n    3. $(50, 3, (-1.5, -1.0), (1.5, 1.2), 0.0, 1.2, 999)$, representing a small-sample boundary case with substantial biomarker effects.\n    4. $(1200, 5, (-2.0, -1.0, -1.5, -3.0), (0.7, 0.3, -0.4, 0.9), 1.5, 0.7, 2023)$, representing five-category outcomes including a rare category scenario with varied biomarker effects.\n\nAlgorithmic and numerical requirements:\n- Implement maximum likelihood estimation for both the unrestricted and restricted models using a numerically stable computation for the log-likelihood under the baseline-category multinomial logistic link. The optimizer should be based on a standard quasi-Newton method and must not rely on any closed-form solution.\n- Ensure reproducibility by using the specified seeds.\n- The program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets. For each test case, output a list of three values in the order $[T, \\text{df}, p]$, where $T$ is the likelihood ratio test statistic as a float, $\\text{df}$ is the degrees of freedom as an integer, and $p$ is the $p$-value as a float. The final output should be of the form $[[T_1,\\text{df}_1,p_1],[T_2,\\text{df}_2,p_2],[T_3,\\text{df}_3,p_3],[T_4,\\text{df}_4,p_4]]$.\n\nNo physical units or angle units are involved in this problem; all outputs are dimensionless quantities.", "solution": "The problem requires the implementation of a likelihood ratio test (LRT) for a multinomial logistic regression model. This involves generating synthetic data, fitting both a full (unrestricted) model and a reduced (restricted) model under a null hypothesis, and then comparing their maximized log-likelihoods to compute a test statistic.\n\n### Step 1: Problem Validation\nThe problem statement is critically evaluated according to the specified criteria.\n\n#### Step 1.1: Extracted Givens\n- **Model:** Baseline-category multinomial logistic regression with a continuous biomarker predictor $x$.\n- **Outcome:** A categorical variable with $K$ mutually exclusive categories, indexed $y_i \\in \\{0, 1, \\dots, K-1\\}$. Category $0$ is the baseline.\n- **Linear Predictor:** For each non-baseline category $k \\in \\{1,\\dots,K-1\\}$, the linear predictor for observation $i$ is $\\eta_{ik} = \\alpha_k + \\beta_k x_i$. The linear predictor for the baseline category is $\\eta_{i0} = 0$.\n- **Probabilities:** The probability of observation $i$ falling into category $k$ is given by the softmax function: $p_{ik} = \\frac{\\exp(\\eta_{ik})}{\\sum_{j=0}^{K-1} \\exp(\\eta_{ij})}$. The probabilities for each observation sum to one: $\\sum_{k=0}^{K-1} p_{ik} = 1$.\n- **Hypotheses:** The null hypothesis $H_0$ states that the biomarker has no effect, i.e., $\\beta_k = 0$ for all $k \\in \\{1,\\dots,K-1\\}$. The alternative hypothesis $H_1$ is that at least one $\\beta_k \\neq 0$.\n- **Core Principles:** The solution must be derived from (1) the model of independent categorical outcomes, (2) the principle of maximum likelihood estimation (MLE), (3) the specified baseline-category logistic link, and (4) the general likelihood ratio test principle.\n- **Task:** For several synthetic datasets, fit the unrestricted and restricted models, and perform the LRT to obtain the test statistic $T$, degrees of freedom $\\text{df}$, and the $p$-value.\n- **Data Generation:** For each test case, $N$ biomarker values $x_i$ are drawn from a normal distribution $\\mathcal{N}(\\mu_x, \\sigma_x^2)$. The outcomes $y_i$ are then drawn from a multinomial distribution with probabilities derived from the true model parameters $(\\boldsymbol{\\alpha}, \\boldsymbol{\\beta})$. Specific values for $(N, K, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta}, \\mu_x, \\sigma_x, \\text{seed})$ are provided for four test cases.\n- **Numerical Requirements:** MLE must be performed using a quasi-Newton optimization method. Calculations must be numerically stable.\n\n#### Step 1.2: Validation and Verdict\nThe problem is evaluated against the validation checklist:\n- **Scientifically Grounded:** The problem is firmly rooted in classical statistical theory. Multinomial logistic regression, MLE, and the LRT are fundamental and widely used methods in biostatistics and other scientific fields.\n- **Well-Posed:** The problem is self-contained and provides all necessary information to generate data and perform the specified analysis. The objective function (log-likelihood) for this model is globally concave, ensuring that numerical optimization is well-posed and will converge to a unique maximum.\n- **Objective:** The problem is stated using precise, formal, and unambiguous mathematical and statistical language.\n- **Flaw Analysis:** The problem exhibits none of the specified flaws. It is scientifically sound, formalizable, complete, realistic, and well-posed. It is a non-trivial computational task that requires a correct implementation of established statistical principles.\n\n**Verdict:** The problem is **valid**.\n\n### Step 2: Principled Solution Design\n\n#### 2.1: Multinomial Logistic Regression Model\nFor an observation $i$ with biomarker value $x_i$, the model defines $K-1$ linear predictors for the non-baseline categories:\n$$ \\eta_{ik} = \\alpha_k + \\beta_k x_i \\quad \\text{for } k \\in \\{1, \\dots, K-1\\} $$\nThe linear predictor for the baseline category $k=0$ is fixed at zero, $\\eta_{i0} = 0$, for model identifiability. The probability that observation $i$ belongs to category $k$ is given by the softmax transformation:\n$$ p_{ik} = P(y_i=k | x_i; \\boldsymbol{\\alpha}, \\boldsymbol{\\beta}) = \\frac{\\exp(\\eta_{ik})}{\\sum_{j=0}^{K-1} \\exp(\\eta_{ij})} $$\n\n#### 2.2: Maximum Likelihood Estimation (MLE)\nThe parameters of the model, $\\boldsymbol{\\theta} = (\\alpha_1, \\dots, \\alpha_{K-1}, \\beta_1, \\dots, \\beta_{K-1})$, are estimated by maximizing the log-likelihood of the observed data. Given $N$ independent observations $\\{(x_i, y_i)\\}_{i=1}^N$, the log-likelihood function is:\n$$ \\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^N \\log P(y_i | x_i; \\boldsymbol{\\theta}) $$\nTo facilitate computation, we use a one-hot encoding for the outcomes, where $y_{ik} = 1$ if the $i$-th observation is in category $k$, and $y_{ik} = 0$ otherwise. The log-likelihood can then be written as:\n$$ \\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\sum_{k=0}^{K-1} y_{ik} \\log(p_{ik}) $$\nSubstituting the expression for $p_{ik}$:\n$$ \\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left( \\sum_{k=0}^{K-1} y_{ik}\\eta_{ik} - \\log\\left(\\sum_{j=0}^{K-1} \\exp(\\eta_{ij})\\right) \\right) $$\nSince $\\eta_{i0}=0$ and $\\sum_{k=0}^{K-1} y_{ik}=1$, and noting that $y_{i0} \\eta_{i0} = 0$:\n$$ \\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left( \\sum_{k=1}^{K-1} y_{ik}\\eta_{ik} - \\log\\left(1 + \\sum_{j=1}^{K-1} \\exp(\\eta_{ij})\\right) \\right) $$\nDirect computation of $\\exp(\\eta_{ij})$ can lead to numerical overflow. The `log-sum-exp` trick is used for stable computation: $\\log(\\sum_j e^{z_j}) = M + \\log(\\sum_j e^{z_j-M})$ where $M=\\max_j(z_j)$. The optimization is performed by minimizing the negative log-likelihood, $-\\ell(\\boldsymbol{\\theta})$, using a quasi-Newton method (L-BFGS-B).\n\n#### 2.3: The Likelihood Ratio Test (LRT)\nThe LRT is used to compare two nested models.\n- **Unrestricted Model ($M_1$):** This is the full model described above, with parameters $\\boldsymbol{\\theta}_1 = (\\alpha_1, \\dots, \\alpha_{K-1}, \\beta_1, \\dots, \\beta_{K-1})$. The number of parameters is $d_1 = 2(K-1)$. Maximizing the likelihood for this model yields the maximized log-likelihood $\\ell_1^* = \\ell(\\hat{\\boldsymbol{\\theta}}_1)$.\n- **Restricted Model ($M_0$):** This model is specified by the null hypothesis $H_0: \\beta_k = 0$ for all $k$. Its linear predictors are $\\eta_{ik} = \\alpha_k$. The parameters are $\\boldsymbol{\\theta}_0 = (\\alpha_1, \\dots, \\alpha_{K-1})$. The number of parameters is $d_0 = K-1$. Maximizing the likelihood under this restriction yields $\\ell_0^* = \\ell(\\hat{\\boldsymbol{\\theta}}_0)$.\n\nThe likelihood ratio test statistic $T$ is defined as:\n$$ T = -2 (\\ell_0^* - \\ell_1^*) = 2(\\ell_1^* - \\ell_0^*) $$\nUnder the null hypothesis $H_0$, for a sufficiently large sample size $N$, $T$ asymptotically follows a chi-squared distribution ($\\chi^2$) with degrees of freedom equal to the difference in the number of parameters between the two models:\n$$ \\text{df} = d_1 - d_0 = 2(K-1) - (K-1) = K-1 $$\nThe $p$-value is the probability of observing a test statistic at least as large as the one computed, assuming $H_0$ is true:\n$$ p = P(\\chi^2_{\\text{df}} \\ge T) $$\nThis is calculated using the survival function of the $\\chi^2_{\\text{df}}$ distribution.\n\n#### 2.4: Implementation\nThe solution logic is implemented in a Python program. For each test case:\n1.  A synthetic dataset is created using `numpy.random`. The `default_rng` is seeded for reproducibility.\n2.  Two objective functions, `nll_m0` and `nll_m1`, are defined to compute the negative log-likelihood for the restricted and unrestricted models, respectively. These functions are designed for numerical stability.\n3.  The `scipy.optimize.minimize` function with the `L-BFGS-B` method is used to find the maximum likelihood estimates for both models by minimizing their respective negative log-likelihoods.\n4.  The maximized log-likelihoods $\\ell_0^*$ and $\\ell_1^*$ are extracted from the optimization results.\n5.  The LRT statistic $T$, degrees of freedom $\\text{df}$, and $p$-value are calculated as described above, with the $p$-value obtained from `scipy.stats.chi2.sf`.\n6.  The results for all test cases are collected and formatted into the required output string.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import softmax, logsumexp\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Main function to run the likelihood ratio test for multinomial logistic regression\n    on several synthetic datasets.\n    \"\"\"\n    test_cases = [\n        # (N, K, alpha_true, beta_true, mu_x, sigma_x, seed)\n        (800, 4, [-1.2, 0.5, -0.3], [0.8, -0.5, 0.4], 2.0, 1.0, 42),\n        (600, 3, [-0.2, -1.0], [0.0, 0.0], 1.0, 0.8, 123),\n        (50, 3, [-1.5, -1.0], [1.5, 1.2], 0.0, 1.2, 999),\n        (1200, 5, [-2.0, -1.0, -1.5, -3.0], [0.7, 0.3, -0.4, 0.9], 1.5, 0.7, 2023),\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        N, K, alphas_true, betas_true, mu_x, sigma_x, seed = case\n        \n        # 1. Generate synthetic data\n        rng = np.random.default_rng(seed)\n        x_data = rng.normal(loc=mu_x, scale=sigma_x, size=N)\n        \n        alphas_true_np = np.array(alphas_true)\n        betas_true_np = np.array(betas_true)\n\n        # Calculate true linear predictors (eta) for categories 1 to K-1\n        eta = alphas_true_np[None, :] + np.outer(x_data, betas_true_np)\n        \n        # Add baseline category's eta (which is 0)\n        eta_full = np.hstack([np.zeros((N, 1)), eta])\n        \n        # Convert to probabilities using softmax\n        probs = softmax(eta_full, axis=1)\n        \n        # Generate categorical outcomes\n        y_data = np.array([rng.choice(K, p=p_i) for p_i in probs])\n        \n        # One-hot encode the outcome variable for likelihood calculation\n        y_one_hot = np.zeros((N, K))\n        y_one_hot[np.arange(N), y_data] = 1\n\n        # 2. Fit models using MLE\n        \n        def nll_m0(params, x, y_oh, k_val):\n            \"\"\"Negative log-likelihood for the restricted model (M0: intercepts only).\"\"\"\n            n_obs = x.shape[0]\n            alphas = params\n            # Eta does not depend on x\n            eta_k = np.tile(alphas, (n_obs, 1))\n            eta_full = np.hstack([np.zeros((n_obs, 1)), eta_k])\n            \n            log_denominators = logsumexp(eta_full, axis=1)\n            log_probs = eta_full - log_denominators[:, None]\n            \n            nll = -np.sum(y_oh * log_probs)\n            return nll\n\n        def nll_m1(params, x, y_oh, k_val):\n            \"\"\"Negative log-likelihood for the unrestricted model (M1).\"\"\"\n            n_obs = x.shape[0]\n            alphas = params[:k_val - 1]\n            betas = params[k_val - 1:]\n            \n            eta_k = alphas[None, :] + np.outer(x, betas)\n            eta_full = np.hstack([np.zeros((n_obs, 1)), eta_k])\n\n            log_denominators = logsumexp(eta_full, axis=1)\n            log_probs = eta_full - log_denominators[:, None]\n\n            nll = -np.sum(y_oh * log_probs)\n            return nll\n\n        # Fit restricted model (M0)\n        initial_params_0 = np.zeros(K - 1)\n        res0 = minimize(nll_m0, initial_params_0, args=(x_data, y_one_hot, K), method='L-BFGS-B')\n        logL0 = -res0.fun\n\n        # Fit unrestricted model (M1)\n        initial_params_1 = np.zeros(2 * (K - 1))\n        res1 = minimize(nll_m1, initial_params_1, args=(x_data, y_one_hot, K), method='L-BFGS-B')\n        logL1 = -res1.fun\n\n        # 3. Perform Likelihood Ratio Test\n        # Test statistic T = 2 * (logL(M1) - logL(M0))\n        T = 2 * (logL1 - logL0)\n        \n        # Degrees of freedom = difference in number of parameters\n        df = (2 * (K - 1)) - (K - 1)\n        \n        # p-value from chi-squared distribution\n        p_value = chi2.sf(T, df)\n        \n        all_results.append([T, df, p_value])\n\n    # Format the final output string\n    # E.g., [[T1,df1,p1],[T2,df2,p2],...]\n    output_str = f\"[{','.join(f'[{t:.6f},{d},{p:.6f}]' for t, d, p in all_results)}]\"\n    print(output_str)\n\nsolve()\n```", "id": "4976107"}, {"introduction": "Once a model has been validated, its primary purpose is to make predictions for new observations, translating statistical parameters into actionable insights. For an ordinal model like the proportional odds model, this involves calculating the probability that an individual falls into each specific outcome category based on their unique characteristics. This hands-on calculation challenges you to apply a fitted model to a new patient profile, compute the predicted probabilities for each severity level, and synthesize these into an expected outcome, demonstrating a key application of ordinal regression in a clinical context. [@problem_id:4976153]", "problem": "A hospital has adopted a four-level ordinal severity scale for acute infection, coded as $Y \\in \\{1,2,3,4\\}$ with $1=$ none, $2=$ mild, $3=$ moderate, and $4=$ severe. A proportional odds (cumulative logit) model has been fitted to a cohort study, using covariates measured at admission: age in years, sex (male indicator), serum C-reactive protein in $\\mathrm{mg/L}$, and Charlson comorbidity index. The fitted model uses cutpoints (also called thresholds) $\\alpha_1$, $\\alpha_2$, $\\alpha_3$ and a common slope vector $\\boldsymbol{\\beta}$, reported below, with the covariate scaling indicated for interpretability:\n\n- Cutpoints: $\\alpha_1 = -1.5$, $\\alpha_2 = 0.0$, $\\alpha_3 = 1.5$.\n- Slopes: age per $10$ years: $\\beta_{\\text{age}} = 0.25$; male indicator: $\\beta_{\\text{male}} = 0.40$; C-reactive protein per $50$ $\\mathrm{mg/L}$: $\\beta_{\\text{CRP}} = 0.60$; Charlson comorbidity per point: $\\beta_{\\text{CCI}} = 0.15$.\n\nConsider a patient with covariates: age $45$ years, female (male indicator $0$), C-reactive protein $20$ $\\mathrm{mg/L}$, Charlson comorbidity index $0$.\n\nUsing the definitions of the logit transform and the logistic function, compute the predicted category probabilities $P(Y=1 \\mid \\mathbf{x})$, $P(Y=2 \\mid \\mathbf{x})$, $P(Y=3 \\mid \\mathbf{x})$, and $P(Y=4 \\mid \\mathbf{x})$ under the proportional odds model for this patient, and interpret the distribution across severity levels in terms of clinical plausibility. Then, compute the expected severity index $\\mathbb{E}[Y \\mid \\mathbf{x}]$ for this patient.\n\nProvide only the expected severity index as your final answer, rounded to four significant figures. No units are required.", "solution": "The proportional odds (cumulative logit) model for an ordinal outcome $Y$ with $K$ categories (here $K=4$) models the cumulative probabilities $P(Y \\le j \\mid \\mathbf{x})$ for $j=1, \\dots, K-1$. The model is defined as:\n$$\n\\text{logit}(P(Y \\le j \\mid \\mathbf{x})) = \\ln\\left(\\frac{P(Y \\le j \\mid \\mathbf{x})}{1 - P(Y \\le j \\mid \\mathbf{x})}\\right) = \\alpha_j - \\boldsymbol{\\beta}^T\\mathbf{x}\n$$\nHere, $\\alpha_j$ are the cutpoints (intercepts) for each cumulative probability, $\\boldsymbol{\\beta}$ is the vector of regression coefficients, and $\\mathbf{x}$ is the vector of covariates. The positive coefficients for risk factors (age, CRP, CCI) imply that an increase in these covariates leads to a decrease in the cumulative logit, which means a decrease in $P(Y \\le j \\mid \\mathbf{x})$, thus shifting the probability mass towards higher values of $Y$ (greater severity). This formulation is standard and appropriate.\n\nFirst, we calculate the linear predictor, $\\eta = \\boldsymbol{\\beta}^T\\mathbf{x}$, for the specified patient. The covariates must be scaled according to the model's specifications. The patient is a $45$-year-old female with C-reactive protein (CRP) of $20$ $\\mathrm{mg/L}$ and a Charlson comorbidity index (CCI) of $0$.\nThe covariate vector $\\mathbf{x}$ is constructed as follows:\n- Age: $x_{\\text{age}} = \\frac{45}{10} = 4.5$\n- Sex (male indicator): $x_{\\text{male}} = 0$\n- C-reactive protein: $x_{\\text{CRP}} = \\frac{20}{50} = 0.4$\n- Charlson comorbidity index: $x_{\\text{CCI}} = 0$\n\nThe linear predictor is:\n$$\n\\eta = \\boldsymbol{\\beta}^T\\mathbf{x} = \\beta_{\\text{age}}x_{\\text{age}} + \\beta_{\\text{male}}x_{\\text{male}} + \\beta_{\\text{CRP}}x_{\\text{CRP}} + \\beta_{\\text{CCI}}x_{\\text{CCI}}\n$$\n$$\n\\eta = (0.25)(4.5) + (0.40)(0) + (0.60)(0.4) + (0.15)(0)\n$$\n$$\n\\eta = 1.125 + 0 + 0.24 + 0 = 1.365\n$$\n\nNext, we compute the cumulative probabilities, $P(Y \\le j \\mid \\mathbf{x})$, by applying the inverse logit (logistic) function, which is $f(z) = \\frac{1}{1 + \\exp(-z)}$.\nFor $j=1$:\n$$\nP(Y \\le 1 \\mid \\mathbf{x}) = \\frac{1}{1 + \\exp(-(\\alpha_1 - \\eta))} = \\frac{1}{1 + \\exp(-(-1.5 - 1.365))} = \\frac{1}{1 + \\exp(2.865)}\n$$\nFor $j=2$:\n$$\nP(Y \\le 2 \\mid \\mathbf{x}) = \\frac{1}{1 + \\exp(-(\\alpha_2 - \\eta))} = \\frac{1}{1 + \\exp(-(0.0 - 1.365))} = \\frac{1}{1 + \\exp(1.365)}\n$$\nFor $j=3$:\n$$\nP(Y \\le 3 \\mid \\mathbf{x}) = \\frac{1}{1 + \\exp(-(\\alpha_3 - \\eta))} = \\frac{1}{1 + \\exp(-(1.5 - 1.365))} = \\frac{1}{1 + \\exp(-0.135)}\n$$\nAlso, by definition, $P(Y \\le 4 \\mid \\mathbf{x}) = 1$.\n\nNow, we compute the individual category probabilities $P(Y=j \\mid \\mathbf{x})$:\n$P(Y=1 \\mid \\mathbf{x}) = P(Y \\le 1 \\mid \\mathbf{x}) \\approx 0.053914$\n$P(Y=2 \\mid \\mathbf{x}) = P(Y \\le 2 \\mid \\mathbf{x}) - P(Y \\le 1 \\mid \\mathbf{x}) \\approx 0.203430 - 0.053914 = 0.149516$\n$P(Y=3 \\mid \\mathbf{x}) = P(Y \\le 3 \\mid \\mathbf{x}) - P(Y \\le 2 \\mid \\mathbf{x}) \\approx 0.533700 - 0.203430 = 0.330270$\n$P(Y=4 \\mid \\mathbf{x}) = 1 - P(Y \\le 3 \\mid \\mathbf{x}) \\approx 1 - 0.533700 = 0.466300$\n\nThe predicted probability distribution for this patient across the severity levels is approximately: $P(Y=1, \\text{none}) \\approx 5.4\\%$, $P(Y=2, \\text{mild}) \\approx 15.0\\%$, $P(Y=3, \\text{moderate}) \\approx 33.0\\%$, and $P(Y=4, \\text{severe}) \\approx 46.6\\%$. The model predicts that the most likely outcome is 'severe', which is clinically plausible. Although the patient is female and has no comorbidities (lower risk factors), her age ($45$) and elevated CRP level ($20$ $\\mathrm{mg/L}$) contribute to a positive linear predictor ($\\eta=1.365$), indicating a risk profile that is substantially higher than a baseline individual. This justifies the shift in probability mass towards the higher severity categories.\n\nFinally, we compute the expected severity index, $\\mathbb{E}[Y \\mid \\mathbf{x}]$. This is the weighted average of the category values, where the weights are the category probabilities:\n$$\n\\mathbb{E}[Y \\mid \\mathbf{x}] = \\sum_{j=1}^{4} j \\cdot P(Y=j \\mid \\mathbf{x})\n$$\nA more direct calculation uses the cumulative probabilities:\n$$\n\\mathbb{E}[Y \\mid \\mathbf{x}] = \\sum_{j=1}^{K} P(Y \\ge j \\mid \\mathbf{x}) = 1 + \\sum_{j=1}^{K-1} P(Y  j \\mid \\mathbf{x}) = 1 + \\sum_{j=1}^{K-1} (1 - P(Y \\le j \\mid \\mathbf{x})) = K - \\sum_{j=1}^{K-1} P(Y \\le j \\mid \\mathbf{x})\n$$\nWith $K=4$, we have:\n$$\n\\mathbb{E}[Y \\mid \\mathbf{x}] = 4 - [P(Y \\le 1 \\mid \\mathbf{x}) + P(Y \\le 2 \\mid \\mathbf{x}) + P(Y \\le 3 \\mid \\mathbf{x})]\n$$\nLet's substitute the numerical values of the cumulative probabilities:\n$P(Y \\le 1 \\mid \\mathbf{x}) \\approx 0.05391418$\n$P(Y \\le 2 \\mid \\mathbf{x}) \\approx 0.20343015$\n$P(Y \\le 3 \\mid \\mathbf{x}) \\approx 0.53369986$\n$$\n\\mathbb{E}[Y \\mid \\mathbf{x}] \\approx 4 - (0.05391418 + 0.20343015 + 0.53369986)\n$$\n$$\n\\mathbb{E}[Y \\mid \\mathbf{x}] \\approx 4 - 0.79104419 = 3.20895581\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\n\\mathbb{E}[Y \\mid \\mathbf{x}] \\approx 3.209\n$$", "answer": "$$\n\\boxed{3.209}\n$$", "id": "4976153"}, {"introduction": "A crucial aspect of evaluating any predictive model is assessing its discriminatory power—its ability to distinguish between individuals with different outcomes. For ordinal models, this means evaluating how well the model's predicted risk scores align with the observed order of outcome severity. This exercise introduces the generalized concordance index, a powerful metric for quantifying discrimination in an ordinal setting, and tasks you with its implementation to provide a robust measure of model performance. [@problem_id:4976101]", "problem": "Consider an ordinal outcome with $K$ ordered categories, where a higher category index indicates greater severity. An ordinal logistic regression under the proportional odds framework models the cumulative probabilities through the logistic function. Let $x_i \\in \\mathbb{R}^p$ be the predictors for individual $i$, $\\beta \\in \\mathbb{R}^p$ be the regression coefficients, and $\\theta_1, \\dots, \\theta_{K-1}$ be the ordered intercepts (thresholds). Define the linear predictor $\\eta_i = x_i^\\top \\beta$. The proportional odds model specifies, for each $k \\in \\{1,\\dots,K-1\\}$,\n$$ \\text{logit}\\left( \\Pr(Y_i \\le k \\mid x_i) \\right) = \\theta_k - \\eta_i, $$\nwhere $\\text{logit}(u) = \\log\\left(\\frac{u}{1-u}\\right)$.\n\nFrom this, the cumulative probabilities are \n$$ q_{ik} = \\Pr(Y_i \\le k \\mid x_i) = \\left(1 + \\exp\\left(-(\\theta_k - \\eta_i)\\right)\\right)^{-1}, $$\nand the category probabilities satisfy \n$$ p_{i1} = q_{i1}, \\quad p_{ik} = q_{ik} - q_{i,k-1} \\text{ for } k \\in \\{2,\\dots,K-1\\}, \\quad p_{iK} = 1 - q_{i,K-1}. $$\nA continuous severity score can be constructed as the conditional expectation \n$$ s_i = \\mathbb{E}[Y_i \\mid x_i] = \\sum_{k=1}^K k \\, p_{ik}. $$\n\nTo assess discrimination in the severity ordering context, define the generalized concordance index as a weighted fraction of concordant pairs across all individuals with different observed categories. For any pair $(i,j)$ with $y_i \\ne y_j$, define the severity difference $d_{ij} = |y_i - y_j|$ and the weight $w(d_{ij}) = d_{ij}$. A pair is concordant if $(s_i - s_j)(y_i - y_j)  0$, discordant if $(s_i - s_j)(y_i - y_j)  0$, and tied in prediction if $s_i = s_j$. Award full weight $w(d_{ij})$ to concordant pairs, zero to discordant pairs, and half-weight $\\tfrac{1}{2} w(d_{ij})$ to tied predictive pairs. The generalized concordance index is then\n$$ G = \\frac{\\sum_{ij, \\, y_i \\ne y_j} \\left[ w(d_{ij}) \\cdot \\mathbf{1}\\left((s_i - s_j)(y_i - y_j)  0\\right) + \\tfrac{1}{2} w(d_{ij}) \\cdot \\mathbf{1}\\left(s_i = s_j\\right) \\right]}{\\sum_{ij, \\, y_i \\ne y_j} w(d_{ij})}. $$\nInterpretation: $G$ is the weighted probability that the model’s predicted severity ordering agrees with the true ordering, with larger observed severity gaps receiving larger weights. Values close to $1$ indicate near-perfect discrimination, values near $0.5$ indicate no discrimination beyond chance, and values below $0.5$ indicate inversions of severity ranking.\n\nTask: Implement a program that computes $G$ for each of the following test cases using the proportional odds model specified above and the weight function $w(d) = d$, and also returns a boolean indicator that is `True` if $G > 0.5$ and `False` otherwise. You must use the severity score $s_i$ defined by the conditional expectation. The computations must be self-contained and reproducible.\n\nTest Suite:\n- Test Case $1$ (general case with repeated categories):\n    - $K = 4$\n    - $\\theta = (-1.2, 0.2, 1.0)$\n    - $\\beta = (1.0, -0.5)$\n    - $X$ rows: $(0.2, 0.1)$, $(0.5, -0.2)$, $(1.0, -0.3)$, $(1.5, -0.4)$, $(0.0, 0.2)$, $(0.7, -0.1)$, $(1.2, -0.2)$, $(1.8, -0.5)$, $(0.3, 0.0)$, $(0.9, -0.1)$\n    - $y = (1, 2, 3, 4, 1, 2, 3, 4, 1, 2)$\n- Test Case $2$ (perfect discrimination, strictly increasing severity):\n    - $K = 5$\n    - $\\theta = (-1.5, -0.5, 0.5, 1.5)$\n    - $\\beta = (1.2)$\n    - $X$ rows: $(-1.0)$, $(-0.5)$, $(0.0)$, $(0.5)$, $(1.0)$\n    - $y = (1, 2, 3, 4, 5)$\n- Test Case $3$ (inverted discrimination due to reversed coefficients):\n    - $K = 4$\n    - $\\theta = (-1.2, 0.2, 1.0)$\n    - $\\beta = (-1.0, 0.5)$\n    - $X$ rows: $(0.2, 0.1)$, $(0.5, -0.2)$, $(1.0, -0.3)$, $(1.5, -0.4)$, $(0.0, 0.2)$, $(0.7, -0.1)$, $(1.2, -0.2)$, $(1.8, -0.5)$, $(0.3, 0.0)$, $(0.9, -0.1)$\n    - $y = (1, 2, 3, 4, 1, 2, 3, 4, 1, 2)$\n- Test Case $4$ (ties in prediction: zero coefficients produce identical predictions):\n    - $K = 3$\n    - $\\theta = (-0.5, 0.5)$\n    - $\\beta = (0.0, 0.0)$\n    - $X$ rows: $(0.2, -0.1)$, $(1.0, 0.3)$, $(-0.5, 0.7)$, $(0.0, 0.0)$\n    - $y = (1, 2, 3, 1)$\n\nOutput Requirements:\n- For each test case, compute the generalized concordance index $G$ as a decimal number and the boolean indicator of better-than-chance discrimination, defined as $G > 0.5$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is represented as a two-element list $[G,\\text{indicator}]$. Express $G$ rounded to $6$ decimal places, and print booleans as either `True` or `False`. For example, a valid output for $3$ test cases would look like $[[0.731000,True],[0.500000,False],[0.120000,False]]$.", "solution": "The problem requires the computation of a generalized concordance index, $G$, for an ordinal logistic regression model under the proportional odds assumption. The solution is executed in two principal stages: first, the calculation of a continuous severity score $s_i$ for each individual $i$, and second, the pair-wise comparison of these scores against the observed outcomes to compute $G$.\n\n**Stage 1: Calculation of the Severity Score**\n\nThe severity score $s_i$ is defined as the conditional expectation of the ordinal outcome $Y_i$ given the predictor vector $x_i$, denoted $s_i = \\mathbb{E}[Y_i \\mid x_i]$. To compute this, we must first determine the probabilities $p_{ik} = \\Pr(Y_i=k \\mid x_i)$ for each category $k \\in \\{1, \\dots, K\\}$.\n\nThe proportional odds model provides a direct path to the cumulative probabilities $q_{ik} = \\Pr(Y_i \\le k \\mid x_i)$. The model is specified by the equation:\n$$\n\\text{logit}\\left( q_{ik} \\right) = \\theta_k - \\eta_i\n$$\nwhere $\\eta_i = x_i^\\top \\beta$ is the linear predictor, $\\beta$ is the vector of regression coefficients, and $\\theta_k$ are the ordered category-specific intercepts or thresholds. Inverting the logit link function, which is $\\text{logit}^{-1}(z) = \\sigma(z) = (1 + e^{-z})^{-1}$, yields the cumulative probabilities:\n$$\nq_{ik} = \\sigma(\\theta_k - \\eta_i) \\quad \\text{for } k \\in \\{1, \\dots, K-1\\}\n$$\nBy definition, $q_{iK} = \\Pr(Y_i \\le K \\mid x_i) = 1$. The individual category probabilities $p_{ik}$ are derived from the cumulative probabilities as follows:\n$$\np_{i1} = q_{i1} \\\\\np_{ik} = q_{ik} - q_{i,k-1} \\quad \\text{for } k \\in \\{2, \\dots, K-1\\} \\\\\np_{iK} = 1 - q_{i,K-1}\n$$\nWith the category probabilities established, the severity score $s_i$ is computed as their weighted sum, where the weights are the category indices:\n$$\ns_i = \\mathbb{E}[Y_i \\mid x_i] = \\sum_{k=1}^K k \\cdot p_{ik}\n$$\nA more computationally direct formula for $s_i$ can be derived from the definition of expectation for a discrete random variable on $\\{1, \\dots, K\\}$, which can be expressed in terms of the cumulative probabilities:\n$$\ns_i = K - \\sum_{k=1}^{K-1} q_{ik} = K - \\sum_{k=1}^{K-1} \\sigma(\\theta_k - \\eta_i)\n$$\nThis latter formulation is computationally efficient, particularly with vectorized operations. For a dataset of $N$ individuals, we first compute the $N$-dimensional vector of linear predictors $\\eta = X\\beta$. Then, we compute an $N \\times (K-1)$ matrix of cumulative probabilities $Q$, where $Q_{ik} = q_{i, k+1}$. The vector of severity scores $s$ is then obtained by summing across the rows of $Q$ and subtracting from $K$.\n\n**Stage 2: Computation of the Generalized Concordance Index ($G$)**\n\nThe generalized concordance index $G$ is a measure of the model's discriminatory power. It quantifies the agreement between the predicted severity ordering (given by the scores $s_i$) and the observed outcome ordering (given by the true categories $y_i$). It is calculated over all pairs of individuals $(i,j)$ with $ij$ for whom the observed outcomes differ, i.e., $y_i \\ne y_j$.\n\nThe formula for $G$ is:\n$$\nG = \\frac{N_c}{D_c} = \\frac{\\sum_{ij, \\, y_i \\ne y_j} \\left[ w_{ij} \\cdot \\mathbf{1}\\left((s_i - s_j)(y_i - y_j)  0\\right) + \\tfrac{1}{2} w_{ij} \\cdot \\mathbf{1}\\left(s_i = s_j\\right) \\right]}{\\sum_{ij, \\, y_i \\ne y_j} w_{ij}}\n$$\nwhere $w_{ij} = |y_i - y_j|$ is the weight for the pair, and $\\mathbf{1}(\\cdot)$ is the indicator function.\n\nThe algorithm proceeds as follows:\n1.  Initialize a numerator sum $N_c = 0$ and a denominator sum $D_c = 0$.\n2.  Iterate through all unique pairs of individuals $(i,j)$ where $i  j$.\n3.  For each pair, check if $y_i \\ne y_j$. If they are equal, the pair is ignored.\n4.  If $y_i \\ne y_j$, calculate the weight $w_{ij} = |y_i - y_j|$ and add it to the denominator $D_c$.\n5.  Evaluate the concordance status of the pair:\n    *   **Concordant**: If the predicted and observed orderings agree, i.e., $(s_i - s_j)(y_i - y_j)  0$. In this case, add the full weight $w_{ij}$ to the numerator $N_c$.\n    *   **Tied**: If the model predicts identical severity, i.e., $s_i = s_j$. In this case, add half the weight, $\\frac{1}{2} w_{ij}$, to the numerator $N_c$. In implementation, this equality check is performed using a tolerance (e.g., `numpy.isclose`) for numerical stability.\n    *   **Discordant**: If the predicted and observed orderings are opposite, i.e., $(s_i - s_j)(y_i - y_j)  0$. In this case, add zero to the numerator $N_c$.\n6.  After all pairs are evaluated, calculate the index $G = N_c / D_c$. If the denominator is zero (which occurs if all individuals have the same outcome), $G$ is undefined; a value of $0.5$ is a conventional choice in such cases, representing chance-level discrimination.\n7. Finally, a boolean indicator for better-than-chance discrimination is determined by evaluating the condition $G  0.5$.\n\nThis procedure is systematically applied to each test case to produce the required results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases specified.\n    Computes the generalized concordance index G and a boolean indicator for G  0.5.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"K\": 4,\n            \"theta\": np.array([-1.2, 0.2, 1.0]),\n            \"beta\": np.array([1.0, -0.5]),\n            \"X\": np.array([\n                [0.2, 0.1], [0.5, -0.2], [1.0, -0.3], [1.5, -0.4],\n                [0.0, 0.2], [0.7, -0.1], [1.2, -0.2], [1.8, -0.5],\n                [0.3, 0.0], [0.9, -0.1]\n            ]),\n            \"y\": np.array([1, 2, 3, 4, 1, 2, 3, 4, 1, 2])\n        },\n        {\n            \"K\": 5,\n            \"theta\": np.array([-1.5, -0.5, 0.5, 1.5]),\n            \"beta\": np.array([1.2]),\n            \"X\": np.array([[-1.0], [-0.5], [0.0], [0.5], [1.0]]),\n            \"y\": np.array([1, 2, 3, 4, 5])\n        },\n        {\n            \"K\": 4,\n            \"theta\": np.array([-1.2, 0.2, 1.0]),\n            \"beta\": np.array([-1.0, 0.5]),\n            \"X\": np.array([\n                [0.2, 0.1], [0.5, -0.2], [1.0, -0.3], [1.5, -0.4],\n                [0.0, 0.2], [0.7, -0.1], [1.2, -0.2], [1.8, -0.5],\n                [0.3, 0.0], [0.9, -0.1]\n            ]),\n            \"y\": np.array([1, 2, 3, 4, 1, 2, 3, 4, 1, 2])\n        },\n        {\n            \"K\": 3,\n            \"theta\": np.array([-0.5, 0.5]),\n            \"beta\": np.array([0.0, 0.0]),\n            \"X\": np.array([[0.2, -0.1], [1.0, 0.3], [-0.5, 0.7], [0.0, 0.0]]),\n            \"y\": np.array([1, 2, 3, 1])\n        }\n    ]\n\n    def compute_g_and_indicator(K, theta, beta, X, y):\n        \"\"\"\n        Computes the severity scores and the generalized concordance index G.\n\n        Parameters:\n            K (int): Number of ordered categories.\n            theta (np.ndarray): Intercepts/thresholds, shape (K-1,).\n            beta (np.ndarray): Regression coefficients, shape (p,).\n            X (np.ndarray): Predictor matrix, shape (N, p).\n            y (np.ndarray): Observed outcomes, shape (N,).\n\n        Returns:\n            tuple: A tuple containing the computed G index (float) and the\n                   boolean indicator (G  0.5).\n        \"\"\"\n        N = X.shape[0]\n\n        # Stage 1: Compute severity scores using a vectorized approach.\n        eta = X @ beta\n        \n        # Create a matrix of logit arguments: logits_mat[i, k] = theta[k] - eta[i]\n        # This is done using NumPy broadcasting.\n        logits_mat = theta.reshape(1, K - 1) - eta.reshape(N, 1)\n        \n        # Compute cumulative probabilities: q_mat[i, k] = Pr(Y_i = k+1)\n        # scipy.special.expit is a numerically stable sigmoid function.\n        q_mat = expit(logits_mat)\n        \n        # The expected severity score can be efficiently calculated as:\n        # s_i = K - sum_{k=1}^{K-1} q_ik\n        s = K - np.sum(q_mat, axis=1)\n\n        # Stage 2: Compute the Generalized Concordance Index G.\n        numerator = 0.0\n        denominator = 0.0\n\n        for i in range(N):\n            for j in range(i + 1, N):\n                if y[i] != y[j]:\n                    # Weight is the absolute difference in observed categories.\n                    weight = abs(y[i] - y[j])\n                    denominator += weight\n                    \n                    s_diff = s[i] - s[j]\n                    \n                    # Use np.isclose for robust floating-point comparison.\n                    if np.isclose(s_diff, 0):  # Tied prediction\n                        numerator += 0.5 * weight\n                    # Concordant if predicted and observed order agree.\n                    elif (s_diff * (y[i] - y[j]))  0:\n                        numerator += weight\n                    # Discordant pairs contribute 0 to the numerator.\n\n        if np.isclose(denominator, 0):\n            # If all y values are the same, no pairs are considered.\n            # G is undefined, but 0.5 represents chance-level performance.\n            g_index = 0.5\n        else:\n            g_index = numerator / denominator\n            \n        indicator = g_index  0.5\n        \n        return g_index, indicator\n\n    results = []\n    for case in test_cases:\n        g, ind = compute_g_and_indicator(**case)\n        results.append([g, ind])\n\n    # Format the final output string as per requirements.\n    result_strings = []\n    for g_val, indicator in results:\n        g_str = f\"{g_val:.6f}\"\n        ind_str = repr(indicator)\n        result_strings.append(f\"[{g_str},{ind_str}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "4976101"}]}