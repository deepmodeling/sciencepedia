{"hands_on_practices": [{"introduction": "Mastering multicollinearity requires moving from theory to practice. This exercise solidifies the link between predictor correlations and the stability of model coefficients by having you compute Variance Inflation Factors (VIFs) from first principles. By working with a given correlation matrix, you will gain a concrete understanding of how much each predictor's estimation variance is amplified by its relationship with others. [@problem_id:4952398]", "problem": "A biomedical research team is building a multiple linear regression model to study a continuous renal risk score $Y$ in a cohort of patients with chronic kidney disease. Three clinically relevant predictors are included: serum creatinine $X_1$, estimated glomerular filtration rate $X_2$, and age $X_3$. Each predictor has been standardized to zero mean and unit variance, yielding the design matrix with columns $(Z_1, Z_2, Z_3)$. The sample correlation matrix of $(Z_1, Z_2, Z_3)$ is\n$$\n\\mathbf{R} \\;=\\;\n\\begin{pmatrix}\n1  -0.82  0.45 \\\\\n-0.82  1  -0.60 \\\\\n0.45  -0.60  1\n\\end{pmatrix}.\n$$\nStarting from the ordinary least squares (OLS) estimator and the definition of the Variance Inflation Factor (VIF), derive from first principles how the variance inflation for each coefficient arises from the inverse of the predictor correlation structure. Then, explicitly invert the matrix $\\mathbf{R}$ and compute $\\text{VIF}_{j}$ for $j \\in \\{1,2,3\\}$. Finally, interpret each value in terms of redundancy of predictor $Z_j$ with respect to the other two predictors, based on the coefficient of determination $R_j^2$ when regressing $Z_j$ on the remaining predictors.\n\nRound your final numerical answers for $(\\text{VIF}_1, \\text{VIF}_2, \\text{VIF}_3)$ to four significant figures. Express the final vector of VIFs as a row matrix.", "solution": "The problem requires a derivation of the Variance Inflation Factor (VIF) from first principles, followed by its calculation and interpretation for a given set of standardized predictors.\n\n### Part 1: Derivation of VIF from First Principles\n\nThe standard multiple linear regression model is expressed in matrix form as:\n$$ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} $$\nwhere $\\mathbf{y}$ is an $n \\times 1$ vector of observations of the response variable, $\\mathbf{X}$ is an $n \\times (p+1)$ design matrix (including an intercept column), $\\boldsymbol{\\beta}$ is a $(p+1) \\times 1$ vector of coefficients, and $\\boldsymbol{\\epsilon}$ is an $n \\times 1$ vector of errors with $\\mathbb{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}$ and $\\text{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2 \\mathbf{I}_n$.\n\nThe Ordinary Least Squares (OLS) estimator for $\\boldsymbol{\\beta}$ is:\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $$\nThe covariance matrix of this estimator is:\n$$ \\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\text{Var}((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}) = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\text{Var}(\\mathbf{y}) \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} $$\nSince $\\text{Var}(\\mathbf{y}) = \\text{Var}(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) = \\text{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2 \\mathbf{I}_n$, this simplifies to:\n$$ \\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1} $$\nThe variance of a single coefficient estimator, $\\hat{\\beta}_j$, is the $j$-th diagonal element of this matrix:\n$$ \\text{Var}(\\hat{\\beta}_j) = \\sigma^2 [(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj} $$\nThe problem states that the predictors have been standardized to have a mean of zero and a variance of one. Let the $n \\times p$ matrix of these standardized predictors be $\\mathbf{Z} = [Z_1, Z_2, \\dots, Z_p]$. Because the predictors are centered, the intercept term is orthogonal to them. We can therefore focus on the coefficients of the predictors, and the relevant matrix is $\\mathbf{Z}^T\\mathbf{Z}$.\n\nThe $(i,j)$-th element of the $\\mathbf{Z}^T\\mathbf{Z}$ matrix is $\\sum_{k=1}^{n} Z_{ki}Z_{kj}$. By the definition of sample correlation for centered variables, the correlation between $Z_i$ and $Z_j$ is $r_{ij} = \\frac{\\sum_k Z_{ki}Z_{kj}}{\\sqrt{\\sum_k Z_{ki}^2 \\sum_k Z_{kj}^2}}$. Since the variables are standardized, their variance is $1$, and $\\sum_k Z_{ki}^2 = \\sum_k Z_{kj}^2 = n-1$. Thus, $r_{ij} = \\frac{\\sum_k Z_{ki}Z_{kj}}{n-1}$.\nThis implies that the predictor correlation matrix $\\mathbf{R}$ is related to $\\mathbf{Z}^T\\mathbf{Z}$ by:\n$$ \\mathbf{Z}^T\\mathbf{Z} = (n-1)\\mathbf{R} $$\nSubstituting this into the variance formula for the coefficients $\\hat{\\boldsymbol{\\beta}}_\\text{std}$ of the standardized model:\n$$ \\text{Var}(\\hat{\\boldsymbol{\\beta}}_\\text{std}) = \\sigma^2 (\\mathbf{Z}^T\\mathbf{Z})^{-1} = \\sigma^2 ((n-1)\\mathbf{R})^{-1} = \\frac{\\sigma^2}{n-1}\\mathbf{R}^{-1} $$\nThe variance of the $j$-th coefficient estimator is the $j$-th diagonal element:\n$$ \\text{Var}(\\hat{\\beta}_j) = \\frac{\\sigma^2}{n-1} [\\mathbf{R}^{-1}]_{jj} $$\nThe Variance Inflation Factor ($\\text{VIF}_j$) is defined as the factor by which $\\text{Var}(\\hat{\\beta}_j)$ is increased due to collinearity, compared to a baseline scenario where predictor $Z_j$ is orthogonal to all other predictors. In such an orthogonal case, the correlation matrix $\\mathbf{R}$ would be the identity matrix $\\mathbf{I}$, and its inverse $\\mathbf{R}^{-1}$ would also be $\\mathbf{I}$. The $j$-th diagonal element would be $[\\mathbf{I}]_{jj}=1$. The variance in this idealized case would be:\n$$ \\text{Var}(\\hat{\\beta}_j)_{\\text{ortho}} = \\frac{\\sigma^2}{n-1}(1) $$\nThe VIF is the ratio of the actual variance to this orthogonal-case variance:\n$$ \\text{VIF}_j = \\frac{\\text{Var}(\\hat{\\beta}_j)}{\\text{Var}(\\hat{\\beta}_j)_{\\text{ortho}}} = \\frac{\\frac{\\sigma^2}{n-1} [\\mathbf{R}^{-1}]_{jj}}{\\frac{\\sigma^2}{n-1}} = [\\mathbf{R}^{-1}]_{jj} $$\nThis derivation shows that the VIF for the $j$-th predictor is precisely the $j$-th diagonal element of the inverse of the predictor correlation matrix $\\mathbf{R}$.\n\nFurthermore, it is a known property of the inverse of a correlation matrix that its diagonal elements are related to the coefficient of determination, $R_j^2$, obtained from regressing the $j$-th predictor $Z_j$ on the remaining $p-1$ predictors. The relationship is:\n$$ \\text{VIF}_j = [\\mathbf{R}^{-1}]_{jj} = \\frac{1}{1 - R_j^2} $$\n\n### Part 2: Calculation of VIFs\n\nThe given correlation matrix is:\n$$ \\mathbf{R} = \\begin{pmatrix} 1  -0.82  0.45 \\\\ -0.82  1  -0.60 \\\\ 0.45  -0.60  1 \\end{pmatrix} $$\nTo find the VIFs, we must compute $\\mathbf{R}^{-1}$. We first calculate the determinant of $\\mathbf{R}$:\n$$ \\det(\\mathbf{R}) = 1(1 \\cdot 1 - (-0.60)^2) - (-0.82)((-0.82) \\cdot 1 - (-0.60) \\cdot 0.45) + 0.45((-0.82)(-0.60) - 1 \\cdot 0.45) $$\n$$ \\det(\\mathbf{R}) = 1(1 - 0.36) + 0.82(-0.82 + 0.27) + 0.45(0.492 - 0.45) $$\n$$ \\det(\\mathbf{R}) = 0.64 + 0.82(-0.55) + 0.45(0.042) $$\n$$ \\det(\\mathbf{R}) = 0.64 - 0.451 + 0.0189 = 0.2079 $$\nNext, we find the adjugate of $\\mathbf{R}$, which is the transpose of its cofactor matrix $\\mathbf{C}$.\n$$ C_{11} = 1 - (-0.60)^2 = 1 - 0.36 = 0.64 $$\n$$ C_{22} = 1 - (0.45)^2 = 1 - 0.2025 = 0.7975 $$\n$$ C_{33} = 1 - (-0.82)^2 = 1 - 0.6724 = 0.3276 $$\nSince $\\mathbf{R}$ is symmetric, its cofactor matrix is symmetric, and we only need the diagonal elements for the VIFs.\nThe inverse matrix is $\\mathbf{R}^{-1} = \\frac{1}{\\det(\\mathbf{R})} \\text{adj}(\\mathbf{R})$. The diagonal elements are:\n$$ [\\mathbf{R}^{-1}]_{11} = \\frac{C_{11}}{\\det(\\mathbf{R})} = \\frac{0.64}{0.2079} $$\n$$ [\\mathbf{R}^{-1}]_{22} = \\frac{C_{22}}{\\det(\\mathbf{R})} = \\frac{0.7975}{0.2079} $$\n$$ [\\mathbf{R}^{-1}]_{33} = \\frac{C_{33}}{\\det(\\mathbf{R})} = \\frac{0.3276}{0.2079} $$\nNow we compute the VIFs:\n$$ \\text{VIF}_1 = [\\mathbf{R}^{-1}]_{11} \\approx 3.07840307... $$\n$$ \\text{VIF}_2 = [\\mathbf{R}^{-1}]_{22} \\approx 3.83645983... $$\n$$ \\text{VIF}_3 = [\\mathbf{R}^{-1}]_{33} \\approx 1.57575757... $$\nRounding to four significant figures, we get:\n$$ \\text{VIF}_1 \\approx 3.078 $$\n$$ \\text{VIF}_2 \\approx 3.836 $$\n$$ \\text{VIF}_3 \\approx 1.576 $$\n\n### Part 3: Interpretation\n\nWe interpret these values using the relationship $\\text{VIF}_j = 1/(1 - R_j^2)$, which implies $R_j^2 = 1 - 1/\\text{VIF}_j$.\n\nFor predictor $Z_1$ (serum creatinine):\n- $\\text{VIF}_1 = 3.078$. This means the variance of the coefficient for serum creatinine is $3.078$ times larger than it would be if serum creatinine were uncorrelated with eGFR and age.\n- $R_1^2 = 1 - 1/3.0784... = 1 - 0.3248... = 0.6751...$. This indicates that about $67.5\\%$ of the variance in standardized serum creatinine ($Z_1$) is explained by a linear combination of standardized eGFR ($Z_2$) and age ($Z_3$). This level of collinearity is moderate.\n\nFor predictor $Z_2$ (eGFR):\n- $\\text{VIF}_2 = 3.836$. The variance of the coefficient for eGFR is $3.836$ times larger than it would be in the absence of collinearity.\n- $R_2^2 = 1 - 1/3.8364... = 1 - 0.2606... = 0.7393...$. This indicates that about $73.9\\%$ of the variance in standardized eGFR ($Z_2$) is explained by standardized serum creatinine ($Z_1$) and age ($Z_3$). This represents a moderate to high level of redundancy, which is clinically expected since eGFR is often calculated from serum creatinine.\n\nFor predictor $Z_3$ (age):\n- $\\text{VIF}_3 = 1.576$. The variance of the coefficient for age is $1.576$ times larger than it would be in the absence of collinearity.\n- $R_3^2 = 1 - 1/1.5757... = 1 - 0.6346... = 0.3653...$. This indicates that about $36.5\\%$ of the variance in standardized age ($Z_3$) is explained by standardized serum creatinine ($Z_1$) and eGFR ($Z_2$). This is a low level of collinearity, suggesting that age provides relatively unique information compared to the other two predictors.\n\nIn summary, the VIF values quantify the degree of multicollinearity. $Z_1$ and particularly $Z_2$ show moderate collinearity, with $Z_2$ (eGFR) being the most redundant predictor, largely because it is functionally related to $Z_1$ (serum creatinine). $Z_3$ (age) is the least redundant.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3.078  3.836  1.576\n\\end{pmatrix}\n}\n$$", "id": "4952398"}, {"introduction": "While VIFs assess each predictor individually, a more powerful approach examines the geometric properties of the entire predictor space. This practice introduces diagnostics based on the eigenvalues of the Gram matrix $X^{\\top}X$, allowing you to compute condition indices. Through this exercise, you will learn to identify the number and severity of near-linear dependencies that may not be obvious from simple pairwise correlations or VIFs alone. [@problem_id:4952344]", "problem": "A clinical researcher fits a multiple linear regression to model the logarithm of one-year hospitalization cost using $p=6$ predictors derived from a registry of patients with hypertension: age, body mass index, systolic blood pressure, diastolic blood pressure, estimated glomerular filtration rate, and an antihypertensive medication burden score. Each predictor is centered to zero mean and scaled to unit variance prior to constructing the design matrix $X \\in \\mathbb{R}^{n \\times p}$. Let $X^{\\top}X$ denote the $p \\times p$ Gram matrix. Suppose that an eigendecomposition of $X^{\\top}X$ yields the eigenvalues (ordered from largest to smallest)\n$$\n\\{\\lambda_{1},\\lambda_{2},\\lambda_{3},\\lambda_{4},\\lambda_{5},\\lambda_{6}\\}=\\{220,95,25,3.2,0.50,0.020\\}.\n$$\nNote that whether $X^{\\top}X$ is divided by $n$ does not affect any dimensionless ratios used below.\n\nStarting only from the properties of the spectral decomposition of a real symmetric positive definite matrix and the definition of singular values of $X$, do the following:\n- Derive a dimensionless diagnostic, computed from $\\{\\lambda_{i}\\}$, that assesses the degree to which $X^{\\top}X$ is nearly singular in specific directions, and explain why this diagnostic is relevant for the stability of ordinary least squares (OLS) estimates.\n- Compute this diagnostic for each $\\lambda_{i}$ above.\n- State explicit numeric thresholds you adopt to flag moderate and severe multicollinearity, and justify these thresholds based on how near-singularity of $X^{\\top}X$ inflates components of the OLS covariance.\n- Identify which of the computed diagnostics indicate problematic multicollinearity under your thresholds.\n\nFinally, report the largest value of this diagnostic for the given eigenvalues. Round your final reported number to four significant figures.", "solution": "The primary task is to derive and apply a diagnostic for multicollinearity using the eigenvalues of the Gram matrix $X^{\\top}X$. Multicollinearity refers to the near-linear dependence among predictor variables in a regression model. Its presence can severely undermine the stability and interpretability of the ordinary least squares (OLS) coefficient estimates.\n\nThe OLS estimator for the coefficient vector $\\beta$ is given by:\n$$\n\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y\n$$\nwhere $y$ is the vector of outcomes. The stability of $\\hat{\\beta}$ is directly related to the properties of the matrix $(X^{\\top}X)^{-1}$. The covariance matrix of the OLS estimator is:\n$$\n\\text{Cov}(\\hat{\\beta}) = \\sigma^2 (X^{\\top}X)^{-1}\n$$\nwhere $\\sigma^2$ is the variance of the model errors. The variance of the individual coefficient estimates, $\\text{Var}(\\hat{\\beta}_j)$, are the diagonal elements of this matrix. High variances imply that the estimates are imprecise and unstable.\n\nThe Gram matrix $X^{\\top}X$ is a real, symmetric, and positive semi-definite matrix. Assuming the columns of $X$ are linearly independent, $X^{\\top}X$ is positive definite. It admits a spectral decomposition:\n$$\nX^{\\top}X = V \\Lambda V^{\\top}\n$$\nwhere $V$ is a $p \\times p$ orthogonal matrix whose columns $v_k$ are the eigenvectors of $X^{\\top}X$, and $\\Lambda$ is a diagonal matrix containing the corresponding eigenvalues $\\lambda_k$, such that $\\Lambda = \\text{diag}(\\lambda_1, \\lambda_2, \\dots, \\lambda_p)$. The inverse is then:\n$$\n(X^{\\top}X)^{-1} = (V \\Lambda V^{\\top})^{-1} = V \\Lambda^{-1} V^{\\top}\n$$\nwhere $\\Lambda^{-1} = \\text{diag}(1/\\lambda_1, 1/\\lambda_2, \\dots, 1/\\lambda_p)$.\n\nThe total variance of the coefficient estimates is the trace of the covariance matrix:\n$$\n\\sum_{j=1}^{p} \\text{Var}(\\hat{\\beta}_j) = \\text{tr}(\\text{Cov}(\\hat{\\beta})) = \\text{tr}(\\sigma^2 V \\Lambda^{-1} V^{\\top}) = \\sigma^2 \\text{tr}(\\Lambda^{-1} V^{\\top}V) = \\sigma^2 \\text{tr}(\\Lambda^{-1}) = \\sigma^2 \\sum_{k=1}^{p} \\frac{1}{\\lambda_k}\n$$\nThis equation reveals that if any eigenvalue $\\lambda_k$ is close to zero, its reciprocal $1/\\lambda_k$ becomes very large, thus inflating the total variance of the coefficient estimates. This indicates that the OLS solution is unstable. A small eigenvalue $\\lambda_k$ signifies that the data exhibit very little variance in the direction of the corresponding eigenvector $v_k$, implying a near-linear relationship among the predictors.\n\nTo formalize a dimensionless diagnostic, we begin with the singular values of the design matrix $X$, which are defined as the square roots of the eigenvalues of $X^{\\top}X$. Let the singular values be $\\mu_k = \\sqrt{\\lambda_k}$. A standard measure of a matrix's conditioning and proximity to singularity is its condition number. For the design matrix $X$, the condition number is the ratio of its largest to smallest singular value, $\\kappa(X) = \\mu_{\\text{max}} / \\mu_{\\text{min}}$.\n\nA more detailed diagnostic, which assesses near-singularity in specific directions, is the set of **condition indices**. The $k$-th condition index, denoted $\\eta_k$, is defined as the ratio of the largest singular value to the $k$-th singular value:\n$$\n\\eta_k = \\frac{\\mu_{\\text{max}}}{\\mu_k} = \\frac{\\sqrt{\\lambda_{\\text{max}}}}{\\sqrt{\\lambda_k}}\n$$\nBy convention, the eigenvalues are ordered from largest to smallest, so $\\lambda_{\\text{max}} = \\lambda_1$. The diagnostic is therefore:\n$$\n\\eta_k = \\sqrt{\\frac{\\lambda_1}{\\lambda_k}} \\quad \\text{for } k = 1, 2, \\dots, p\n$$\nThis is a dimensionless diagnostic. A large value for $\\eta_k$ ($k  1$) indicates that $\\lambda_k$ is much smaller than $\\lambda_1$, signaling a potential collinearity problem associated with the $k$-th principal component of the predictor space. This is the diagnostic required by the problem.\n\nNow, we compute this diagnostic for the given eigenvalues. The set of ordered eigenvalues is:\n$$\n\\{\\lambda_{1},\\lambda_{2},\\lambda_{3},\\lambda_{4},\\lambda_{5},\\lambda_{6}\\}=\\{220, 95, 25, 3.2, 0.50, 0.020\\}\n$$\nThe largest eigenvalue is $\\lambda_1 = 220$. The condition indices are calculated as follows:\n$$\n\\eta_1 = \\sqrt{\\frac{220}{220}} = \\sqrt{1} = 1\n$$\n$$\n\\eta_2 = \\sqrt{\\frac{220}{95}} \\approx \\sqrt{2.315789} \\approx 1.52177\n$$\n$$\n\\eta_3 = \\sqrt{\\frac{220}{25}} = \\sqrt{8.8} \\approx 2.96648\n$$\n$$\n\\eta_4 = \\sqrt{\\frac{220}{3.2}} = \\sqrt{68.75} \\approx 8.29156\n$$\n$$\n\\eta_5 = \\sqrt{\\frac{220}{0.50}} = \\sqrt{440} \\approx 20.97618\n$$\n$$\n\\eta_6 = \\sqrt{\\frac{220}{0.020}} = \\sqrt{11000} \\approx 104.88088\n$$\n\nNext, we establish and justify thresholds for identifying problematic multicollinearity. The following rules of thumb, based on the work of Belsley, Kuh, and Welsch, are widely adopted in practice:\n- **Moderate multicollinearity**: A condition index $\\eta_k$ in the range $[10, 30)$ is taken to indicate a moderate to strong linear dependency.\n- **Severe multicollinearity**: A condition index $\\eta_k \\ge 30$ is taken to indicate a severe linear dependency, which can seriously degrade the regression estimates.\n\nThe justification for these thresholds relates to the numerical stability of the linear system. A condition index $\\eta_k = 10$ implies that $\\lambda_1/\\lambda_k = 100$, and $\\eta_k = 30$ implies $\\lambda_1/\\lambda_k = 900$. An eigenvalue that is two to three orders of magnitude smaller than the largest eigenvalue represents a dimension in the predictor space that is nearly degenerate (i.e., the data cloud is nearly flat in that direction). This near-degeneracy creates instability, as small changes in the input data can lead to large swings in the coefficient estimates associated with that dimension. The thresholds serve as practical markers for when the variance inflation caused by small eigenvalues becomes a concern for statistical inference.\n\nFinally, we identify which of the computed diagnostics indicate problematic multicollinearity using these thresholds:\n- $\\eta_1, \\eta_2, \\eta_3, \\eta_4$: These are all less than $10$ and do not indicate a problem.\n- $\\eta_5 \\approx 20.98$: This value is between $10$ and $30$, indicating **moderate multicollinearity**.\n- $\\eta_6 \\approx 104.88$: This value is greater than $30$, indicating **severe multicollinearity**.\n\nThe two smallest eigenvalues, $\\lambda_5 = 0.50$ and particularly $\\lambda_6 = 0.020$, are the sources of the multicollinearity in this dataset. The largest value of this diagnostic is $\\eta_6$.\n\nThe problem asks for the largest value of this diagnostic, rounded to four significant figures:\n$$\n\\eta_6 = \\sqrt{11000} \\approx 104.88088...\n$$\nRounding to four significant figures yields $104.9$.", "answer": "$$\n\\boxed{104.9}\n$$", "id": "4952344"}, {"introduction": "Effective statistical practice in medicine demands reproducible and robust data-processing pipelines. This advanced exercise moves from manual diagnosis to algorithmic handling of multicollinearity using the Singular Value Decomposition (SVD). You will build a program to not only detect rank deficiency with numerical precision but also to implement and contrast two common remediation strategies: systematic predictor removal and principal component projection. [@problem_id:4952418]", "problem": "You are tasked with building a reproducible algorithm to detect and handle multicollinearity in medical design matrices using Singular Value Decomposition (SVD) tolerance thresholds. Consider a linear modeling context with a design matrix $X \\in \\mathbb{R}^{n \\times p}$, where $n$ is the number of observations and $p$ is the number of predictors. Multicollinearity arises when columns of $X$ are linearly dependent or nearly dependent, causing instability in parameter estimation.\n\nStarting from the fundamental base:\n- The numerical rank of a matrix $X$ is the number of singular values of $X$ that are larger than a tolerance.\n- The Singular Value Decomposition (SVD) states that any real matrix $X$ admits a factorization $X = U \\Sigma V^\\top$, where $U$ and $V$ are orthogonal matrices and $\\Sigma$ is diagonal with nonnegative entries $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_{\\min(n,p)} \\ge 0$, the singular values.\n- Floating-point arithmetic has machine precision $\\epsilon$ (machine epsilon), which sets a scale for what is numerically negligible.\n\nYour program must implement the following, from first principles:\n1. Column centering for reproducibility across medical predictors measured in different units: replace each column $x_j$ with $x_j - \\bar{x}_j$, where $\\bar{x}_j$ is the mean of column $j$. Columns that become identically zero after centering are constants and must be dropped deterministically.\n2. Compute the SVD of the centered matrix and determine its numerical rank $r$ by counting singular values larger than a tolerance. The tolerance must be computed as a function of machine precision, problem size, and the largest singular value. Let $\\tau$ be a user-specified tolerance multiplier; you must set the threshold as a product of $\\tau$, a dimension-dependent factor, machine epsilon, and the largest singular value.\n3. If $r  p$, handle multicollinearity by one of two reproducible strategies:\n   - Drop strategy: Iteratively identify a null-space direction from the SVD (a right singular vector corresponding to a singular value below the tolerance), and drop the predictor whose absolute loading in that vector is largest. Perform tie-breaking deterministically by selecting the predictor with the largest index among those tied. Recompute until the remaining columns are full rank.\n   - Combine strategy: Construct orthogonal surrogate predictors by projecting onto the top $r$ right singular vectors of the centered matrix (principal components). Report the fraction of total squared singular values preserved by the top $r$ components, expressed as a decimal.\n\nDefinitions and constraints to be applied throughout:\n- The numerical rank $r$ is defined as the count of singular values strictly greater than the tolerance.\n- The null space basis vectors are the right singular vectors associated with singular values less than or equal to the tolerance.\n- All decisions must be deterministic and invariant to column permutation except for the specified tie-breaking rule.\n\nTest suite:\nFor each test case, you are given $(X, \\tau, \\text{strategy})$ and must return a structured result. Construct $X$ using the following parameter values; your program must reproduce these exactly.\n\n- Test case A (happy path, full rank under drop strategy):\n  - $n = 12$\n  - $t_i = \\frac{i}{11}$ for $i = 0, 1, \\dots, 11$\n  - Columns: $x_1 = t$, $x_2 = t^2$, $x_3 = \\sin(2\\pi t)$\n  - $\\tau = 1.0$\n  - Strategy: drop\n\n- Test case B (exact dependency among three predictors under drop strategy):\n  - $n = 50$\n  - $t_i = \\frac{i}{49}$ for $i = 0, 1, \\dots, 49$\n  - Columns: $x_1 = t$, $x_2 = \\sin(3\\pi t)$, $x_3 = x_1 + 2x_2$\n  - $\\tau = 1.0$\n  - Strategy: drop\n\n- Test case C (near collinearity treated as deficient due to large tolerance under drop strategy):\n  - $n = 50$\n  - $t_i = \\frac{i}{49}$ for $i = 0, 1, \\dots, 49$\n  - Columns: $x_1 = t$, $x_2 = x_1 + 10^{-8}\\cos(5\\pi t)$, $x_3 = \\sqrt{t + 10^{-6}}$\n  - $\\tau = 10.0$\n  - Strategy: drop\n\n- Test case D (same as Test case C but with small tolerance, so full rank under drop strategy):\n  - $n = 50$\n  - $t_i = \\frac{i}{49}$ for $i = 0, 1, \\dots, 49$\n  - Columns: $x_1 = t$, $x_2 = x_1 + 10^{-8}\\cos(5\\pi t)$, $x_3 = \\sqrt{t + 10^{-6}}$\n  - $\\tau = 0.01$\n  - Strategy: drop\n\n- Test case E (multiple exact dependencies under combine strategy; report preserved variance fraction):\n  - $n = 100$\n  - $t_i = \\frac{i}{99}$ for $i = 0, 1, \\dots, 99$\n  - Columns: $x_1 = t$, $x_2 = \\sin(2\\pi t)$, $x_3 = \\log(1 + 2t)$, $x_4 = x_1 + x_2$, $x_5 = 3x_2 - x_3$\n  - $\\tau = 1.0$\n  - Strategy: combine\n\nFinal output specification:\n- For a drop strategy test case, output a two-element list $[r, [k_1, k_2, \\dots]]$, where $r$ is the numerical rank after centering and $[k_1, k_2, \\dots]$ is the ascending list of kept column indices using $0$-based indexing from the original $X$.\n- For a combine strategy test case, output a two-element list $[r, f]$, where $r$ is the numerical rank after centering and $f$ is the fraction of total squared singular values preserved by the top $r$ singular values, expressed as a decimal (not a percentage), rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3,result4,result5]$). Each $resulti$ must follow the format above for the corresponding strategy.\n\nNo external input is permitted; the program must construct the test matrices internally and must be reproducible and deterministic.", "solution": "This problem requires implementing a function that processes each test case according to the specified strategy (`drop` or `combine`). The process can be broken down into modular steps.\n\n**1. Preprocessing: Matrix Centering and Constant Column Removal**\nGiven a design matrix $X \\in \\mathbb{R}^{n \\times p}$, the first step is to center it. For each column $x_j$, we compute its mean $\\bar{x}_j$ and subtract it from the column. After centering, any columns that become identically zero (i.e., were constant predictors) are identified and removed, as they have zero variance and contribute to rank deficiency. The original indices of the remaining columns must be tracked.\n\n**2. Numerical Rank Calculation**\nFor the preprocessed matrix $X_p$ (the centered matrix with constant columns removed), its Singular Value Decomposition (SVD) is computed: $X_p = U \\Sigma V^\\top$. The singular values are $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$. The tolerance for determining numerical rank is calculated as:\n$$ \\text{tol} = \\tau \\cdot \\max(n, p') \\cdot \\epsilon \\cdot \\sigma_1 $$\nwhere $p'$ is the number of non-constant columns, $\\epsilon$ is machine epsilon, $\\tau$ is the given multiplier, and $\\sigma_1$ is the largest singular value. The numerical rank $r$ is the number of singular values strictly greater than this tolerance. This initial rank is part of the output for both strategies.\n\n**3. `combine` Strategy**\nIf the strategy is `combine`, we use the initial numerical rank $r$ to compute the fraction of variance preserved by the top $r$ components. The total variance is proportional to the sum of the squares of all singular values, $\\sum_{j} \\sigma_j^2$. The variance captured by the top $r$ components is $\\sum_{j=1}^{r} \\sigma_j^2$. The required fraction $f$ is:\n$$ f = \\frac{\\sum_{j=1}^{r} \\sigma_j^2}{\\sum_{j=1}^{p'} \\sigma_j^2} $$\nThe result is the list $[r, f]$, with $f$ rounded to six decimal places.\n\n**4. `drop` Strategy**\nThis strategy is iterative. We start with the set of all non-constant predictors.\nThe procedure is as follows:\n1.  Construct the current data matrix $X_{current}$ from the columns of the original centered matrix corresponding to the set of currently kept indices.\n2.  Compute the SVD of $X_{current}$ and its numerical rank $r_{curr}$ using the tolerance formula.\n3.  If $r_{curr}$ equals the number of columns in $X_{current}$, the matrix has full numerical rank, and the process stops. The final set of kept original indices is the result.\n4.  If the matrix is rank-deficient, we identify the right singular vector corresponding to the smallest singular value.\n5.  We find the predictor that has the largest absolute loading (coefficient) in this vector. In case of a tie, the predictor with the largest original column index is chosen.\n6.  This predictor's index is removed from the set of kept indices, and the process repeats from step 1.\n\nThe loop terminates when the remaining set of columns represents a full-rank matrix. The output is a list containing the *initial* numerical rank $r$ and the sorted list of final kept original indices.", "answer": "[[3,[0,1,2]],[2,[0,1]],[2,[1,2]],[3,[0,1,2]],[3,1.000000]]", "id": "4952418"}]}