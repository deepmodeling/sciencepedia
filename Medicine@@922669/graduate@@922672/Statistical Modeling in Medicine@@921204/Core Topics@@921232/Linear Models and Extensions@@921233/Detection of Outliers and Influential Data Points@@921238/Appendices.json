{"hands_on_practices": [{"introduction": "In regression diagnostics, it is crucial to distinguish between a data point's leverage and its influence. This exercise explores the counterintuitive scenario where a high-leverage point can appear to improve a model's overall precision, as indicated by a $\\mathrm{COVRATIO}_i < 1$, while simultaneously exerting a strong, distorting influence on specific coefficient estimates. By working through this apparent paradox [@problem_id:4959084], you will develop a deeper understanding of how global and parameter-specific diagnostics can tell different stories and why a comprehensive assessment is essential.", "problem": "A prospective cohort study models a continuous inflammatory marker outcome $y$ (for example, $\\log$ C-Reactive Protein) using ordinary least squares (OLS) linear regression with predictors $x_1$ (age), $x_2$ (body mass index), $x_3$ (current smoking indicator), and $x_4$ (treatment assignment), along with an intercept. Let the design matrix be $X \\in \\mathbb{R}^{n \\times p}$ with $p=5$ including the intercept. Consider a specific patient $i$ whose covariate vector $x_i \\in \\mathbb{R}^p$ lies at the periphery of the observed predictor space (very advanced age and low body mass index), so that the leverage $h_{ii}$ from the hat matrix $H = X (X^\\top X)^{-1} X^\\top$ is high. Suppose the raw residual $e_i = y_i - x_i^\\top \\hat{\\beta}$ is small in magnitude, and the case-deletion precision diagnostic $\\mathrm{COVRATIO}_i$ satisfies $\\mathrm{COVRATIO}_i < 1$, where $\\mathrm{COVRATIO}_i$ is defined as the ratio of the determinant of the variance-covariance matrix of the full-sample OLS estimator $\\hat{\\beta}$ to the determinant of the variance-covariance matrix of the case-deleted estimator $\\hat{\\beta}_{(-i)}$. Empirically, one observes that the patient has a large coefficient-specific case-deletion effect on the age coefficient, with $|\\mathrm{DFBETAS}_{i,\\text{age}}|$ exceeding conventional thresholds, while the effects on the smoking and treatment coefficients are negligible.\n\nFrom the standpoint of medical statistical modeling, start from the fundamental definitions of the OLS estimator and its variance, and reason about how the inclusion of an extreme covariate vector $x_i$ changes both the global precision of $\\hat{\\beta}$ and the parameter-specific sensitivity of certain coefficients. Then, select the option(s) that correctly explain why a high-leverage point can concurrently yield $\\mathrm{COVRATIO}_i < 1$ (indicating improved global precision) yet be influential for particular coefficients, and recommend appropriate, scientifically coherent diagnostics that resolve this apparent tension in the context of clinical regression modeling.\n\nOptions:\nA. If $h_{ii}$ is high but $|e_i|$ is small, the observation cannot be influential on any coefficient because influence requires a large residual; therefore, a finding of $\\mathrm{COVRATIO}_i < 1$ proves the case is entirely benign and no coefficient-level diagnostics are needed.\nB. The inclusion of a high-leverage $x_i$ adds $x_i x_i^\\top$ to $X^\\top X$, which decreases $\\det((X^\\top X)^{-1})$ and can yield $\\mathrm{COVRATIO}_i < 1$ (improved global precision), while the geometric orientation of $x_i$ in predictor space can still reweight information in a way that shifts specific coefficients; to resolve this, examine coefficient-specific case-deletion effects such as $\\mathrm{DFBETAS}_{i,j}$, component-wise Cook’s distances, added-variable plots for $x_j$, and perform sensitivity analyses using robust regression.\nC. Because $\\mathrm{COVRATIO}_i$ compares full-sample to case-deleted precision globally, a value $<1$ guarantees that the observation improves all aspects of the model; consequently, coefficient influence cannot occur and any large $\\mathrm{DFBETAS}$ values must be spurious.\nD. Parameter influence is directional: even with $|e_i| \\approx 0$, a large $h_{ii}$ coupled with $x_i$ aligning to a poorly informed axis of $X$ can cause substantial shifts in some coefficients through case-deletion reorientation of $(X^\\top X)^{-1}$; appropriate diagnostics include $\\mathrm{DFBETAS}_{i,j}$ for each coefficient, examination of condition indices and variance-decomposition proportions to assess collinearity, local influence analyses, and targeted leave-one-out refits to quantify sensitivity.", "solution": "The problem statement describes a scenario in ordinary least squares (OLS) regression that is common in applied statistics, particularly in fields like medical research. It presents an apparent paradox: a single data point simultaneously improves the *global* precision of the estimated coefficient vector $\\hat{\\beta}$ while being highly *influential* for a specific coefficient. We must validate the problem's premise and then resolve this tension using fundamental principles of linear regression analysis.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Model:** OLS linear regression, $y = X\\beta + \\epsilon$.\n-   **Data:** A prospective cohort study.\n-   **Outcome $y$:** A continuous inflammatory marker.\n-   **Predictors:** $x_1$ (age), $x_2$ (body mass index), $x_3$ (smoking indicator), $x_4$ (treatment assignment), plus an intercept. The design matrix is $X \\in \\mathbb{R}^{n \\times p}$ with $p=5$.\n-   **Observation $i$:** A specific patient with covariate vector $x_i$.\n-   **Properties of observation $i$:**\n    1.  The covariate vector $x_i$ is at the periphery of the predictor space (e.g., very high age, low BMI).\n    2.  The leverage $h_{ii}$ is high. The leverage is the $i$-th diagonal element of the hat matrix $H = X(X^\\top X)^{-1}X^\\top$. Thus, $h_{ii} = x_i^\\top(X^\\top X)^{-1}x_i$.\n    3.  The raw residual $e_i = y_i - x_i^\\top \\hat{\\beta}$ is small in magnitude.\n    4.  The case-deletion precision diagnostic $\\mathrm{COVRATIO}_i < 1$. This is defined as the ratio of the determinant of the variance-covariance matrix of the full-sample OLS estimator $\\hat{\\beta}$ to that of the case-deleted estimator $\\hat{\\beta}_{(-i)}$.\n    5.  The observation has a large influence on the age coefficient, $|\\mathrm{DFBETAS}_{i,\\text{age}}|$, but negligible influence on other coefficients.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically and statistically sound. The scenario described is a classic case study in regression diagnostics.\n-   **Scientific Grounding:** The concepts of OLS, leverage, residuals, and influence diagnostics like $\\mathrm{COVRATIO}$ and $\\mathrm{DFBETAS}$ are foundational to statistical modeling. The application to a medical study with plausible variables is realistic.\n-   **Well-Posed:** The problem asks for a conceptual explanation and recommendation of further diagnostics based on a well-defined statistical situation. It is structured to have a correct, non-unique but coherent, set of explanations and recommendations grounded in statistical theory.\n-   **Objectivity:** The problem is stated in precise, objective, and standard statistical terminology.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It describes a non-trivial and important phenomenon in applied regression analysis. We will proceed to derive a solution.\n\n### Derivation from First Principles\n\nLet $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$ be the OLS estimator for the full dataset of size $n$. The estimated variance-covariance matrix of $\\hat{\\beta}$ is $\\widehat{\\mathrm{Var}}(\\hat{\\beta}) = s^2(X^\\top X)^{-1}$, where $s^2$ is the estimated error variance. Let $\\hat{\\beta}_{(-i)}$ and $s_{(-i)}^2$ be the corresponding quantities when observation $i$ is deleted. The design matrix without row $i$ is $X_{(-i)}$.\n\n**1. Analysis of $\\mathrm{COVRATIO}_i$**\n\nThe problem defines $\\mathrm{COVRATIO}_i$ as $\\frac{\\det(\\widehat{\\mathrm{Var}}(\\hat{\\beta}))}{\\det(\\widehat{\\mathrm{Var}}(\\hat{\\beta}_{(-i)}))}$. Substituting the formulas:\n$$ \\mathrm{COVRATIO}_i = \\frac{\\det(s^2(X^\\top X)^{-1})}{\\det(s_{(-i)}^2(X_{(-i)}^\\top X_{(-i)})^{-1})} = \\frac{s^{2p} \\det((X^\\top X)^{-1})}{s_{(-i)}^{2p} \\det((X_{(-i)}^\\top X_{(-i)})^{-1})} = \\left(\\frac{s}{s_{(-i)}}\\right)^{2p} \\frac{\\det(X_{(-i)}^\\top X_{(-i)})}{\\det(X^\\top X)} $$\nThe relationship between the full and case-deleted information matrices is $X^\\top X = X_{(-i)}^\\top X_{(-i)} + x_i x_i^\\top$. Using the matrix determinant lemma, $\\det(A + uv^\\top) = \\det(A)(1 + v^\\top A^{-1} u)$, we have:\n$$ \\det(X^\\top X) = \\det(X_{(-i)}^\\top X_{(-i)}) (1 + x_i^\\top (X_{(-i)}^\\top X_{(-i)})^{-1} x_i) $$\nA standard identity in regression diagnostics is $1 - h_{ii} = \\frac{1}{1 + x_i^\\top (X_{(-i)}^\\top X_{(-i)})^{-1} x_i}$. Therefore:\n$$ \\frac{\\det(X_{(-i)}^\\top X_{(-i)})}{\\det(X^\\top X)} = 1 - h_{ii} $$\nSubstituting this back into the expression for $\\mathrm{COVRATIO}_i$:\n$$ \\mathrm{COVRATIO}_i = \\left(\\frac{s}{s_{(-i)}}\\right)^{2p} (1 - h_{ii}) $$\nThe problem states that leverage $h_{ii}$ is high, so $h_{ii}$ is close to $1$. This makes the term $(1 - h_{ii})$ a small positive number. Furthermore, the residual $e_i$ is small. A small residual implies that observation $i$ fits the full model well, and its removal is unlikely to drastically change the estimated error variance. Thus, the ratio $s/s_{(-i)}$ is expected to be close to $1$. The dominant term is $(1-h_{ii})$, which drives the value of $\\mathrm{COVRATIO}_i$ below $1$.\n\nThe underlying reason is that the precision of $\\hat{\\beta}$ is related to the \"size\" of the information matrix $X^\\top X$. Adding a high-leverage point $x_i$ (which is far from the center of the data) adds the matrix $x_i x_i^\\top$ to $X_{(-i)}^\\top X_{(-i)}$, effectively expanding the convex hull of the data points. This generally increases the \"size\" of the information matrix (specifically, $\\det(X^\\top X) > \\det(X_{(-i)}^\\top X_{(-i)})$), which in turn decreases the \"size\" of the variance-covariance matrix $(X^\\top X)^{-1}$. The determinant of the variance-covariance matrix, known as the generalized variance, is a measure of the volume of the confidence ellipsoid for $\\hat{\\beta}$. A smaller volume implies higher *global* precision. Thus, a high-leverage point can improve the overall precision of the estimates, leading to $\\mathrm{COVRATIO}_i < 1$.\n\n**2. Analysis of $\\mathrm{DFBETAS}_{i,j}$**\n\n$\\mathrm{DFBETAS}_{i,j}$ measures the influence of deleting observation $i$ on the $j$-th coefficient, $\\hat{\\beta}_j$. The change in the coefficient vector is given by:\n$$ \\hat{\\beta} - \\hat{\\beta}_{(-i)} = \\frac{(X^\\top X)^{-1} x_i e_i}{1 - h_{ii}} $$\nThe numerator of $\\mathrm{DFBETAS}_{i,j}$ is the $j$-th component of this vector, $(\\hat{\\beta} - \\hat{\\beta}_{(-i)})_j$. The full expression is:\n$$ \\mathrm{DFBETAS}_{i,j} = \\frac{(\\hat{\\beta}_j - \\hat{\\beta}_{j,(-i)})}{s_{(-i)}\\sqrt{((X^\\top X)^{-1})_{jj}}} = \\frac{((X^\\top X)^{-1} x_i)_j}{s_{(-i)}\\sqrt{((X^\\top X)^{-1})_{jj}}} \\frac{e_i}{1-h_{ii}} $$\nHere, we see the critical combination of leverage and residual. Even though the raw residual $e_i$ is small, the high leverage $h_{ii}$ makes the denominator $(1-h_{ii})$ also small. Their ratio, $e_i / (1-h_{ii})$, which is the residual of point $i$ from the case-deleted regression, can be large. This inflated residual effect is then distributed to the coefficients via the vector $((X^\\top X)^{-1} x_i)_j$.\n\nThe influence is *directional*. The observation $x_i$ is a vector in the $p$-dimensional predictor space. Its influence on the coefficients depends on its orientation relative to the axes defined by the predictors and their correlations. For the given patient with very advanced age and low BMI, the vector $x_i$ is extreme along the 'age' and 'BMI' dimensions. If the 'age' predictor is a \"poorly informed axis\" of the design matrix $X$ (e.g., due to collinearity, or simply because there are few other points at that extreme age), the information matrix $X^\\top X$ is relatively weak in that direction. The inclusion/exclusion of point $i$ can then cause a significant \"reorientation\" of the inverse matrix $(X^\\top X)^{-1}$, leading to a large change in the corresponding coefficient $\\hat{\\beta}_{\\text{age}}$. The influence on other coefficients like 'smoking' or 'treatment' might be negligible if the extremity of $x_i$ is largely orthogonal to those predictor dimensions in the space spanned by $X$.\n\n**Conclusion of Derivation:** The apparent tension is resolved by recognizing the distinction between a scalar, global measure of precision ($\\det(\\widehat{\\mathrm{Var}}(\\hat{\\beta}))$) and vector-valued, component-specific measures of influence ($\\mathrm{DFBETAS}_i$). A high-leverage point can stabilize the regression plane overall (reducing the volume of the joint confidence ellipsoid), while simultaneously pivoting it in a way that dramatically alters specific slopes.\n\n### Option-by-Option Analysis\n\n**A. If $h_{ii}$ is high but $|e_i|$ is small, the observation cannot be influential on any coefficient because influence requires a large residual; therefore, a finding of $\\mathrm{COVRATIO}_i < 1$ proves the case is entirely benign and no coefficient-level diagnostics are needed.**\nThis statement is fundamentally flawed. Influence is a function of both leverage and residual size. The formula for the change in coefficients, $\\hat{\\beta} - \\hat{\\beta}_{(-i)}$, contains the term $e_i / (1 - h_{ii})$. For high leverage ($h_{ii} \\to 1$), this term can be large even for a small $e_i$. To claim a high-leverage point cannot be influential because its residual is small is a dangerous misconception. The conclusion to ignore coefficient-level diagnostics is incorrect and statistically naive.\n**Verdict: Incorrect.**\n\n**B. The inclusion of a high-leverage $x_i$ adds $x_i x_i^\\top$ to $X^\\top X$, which decreases $\\det((X^\\top X)^{-1})$ and can yield $\\mathrm{COVRATIO}_i < 1$ (improved global precision), while the geometric orientation of $x_i$ in predictor space can still reweight information in a way that shifts specific coefficients; to resolve this, examine coefficient-specific case-deletion effects such as $\\mathrm{DFBETAS}_{i,j}$, component-wise Cook’s distances, added-variable plots for $x_j$, and perform sensitivity analyses using robust regression.**\nThis option correctly provides the mechanism for the improved global precision ($\\mathrm{COVRATIO}_i < 1$) based on the addition of $x_i x_i^\\top$ to the information matrix and the resulting decrease in the determinant of its inverse. It also correctly recognizes that the geometric orientation of $x_i$ can lead to influence on specific coefficients. The list of recommended diagnostics—$\\mathrm{DFBETAS}$, component-wise influence measures, added-variable plots, and robust regression—is comprehensive and entirely appropriate for investigating and resolving the issue.\n**Verdict: Correct.**\n\n**C. Because $\\mathrm{COVRATIO}_i$ compares full-sample to case-deleted precision globally, a value $<1$ guarantees that the observation improves all aspects of the model; consequently, coefficient influence cannot occur and any large $\\mathrm{DFBETAS}$ values must be spurious.**\nThis statement makes an invalid generalization. A global measure of precision does not guarantee improvement in all aspects of the model, particularly the stability of individual parameter estimates. The problem statement itself is a direct counterexample to this claim. Declaring observed large $\\mathrm{DFBETAS}$ values as \"spurious\" based on a global metric is a serious error in statistical reasoning.\n**Verdict: Incorrect.**\n\n**D. Parameter influence is directional: even with $|e_i| \\approx 0$, a large $h_{ii}$ coupled with $x_i$ aligning to a poorly informed axis of $X$ can cause substantial shifts in some coefficients through case-deletion reorientation of $(X^\\top X)^{-1}$; appropriate diagnostics include $\\mathrm{DFBETAS}_{i,j}$ for each coefficient, examination of condition indices and variance-decomposition proportions to assess collinearity, local influence analyses, and targeted leave-one-out refits to quantify sensitivity.**\nThis option provides an excellent and deep explanation for the parameter-specific influence. It correctly identifies the directional nature of influence and precisely describes the underlying mechanism: the alignment of the extreme point with a \"poorly informed axis\" of the design matrix, leading to a \"reorientation\" of $(X^\\top X)^{-1}$. The recommended diagnostics are also highly appropriate and complementary to those in option B, focusing on diagnosing the structure of the design matrix (collinearity diagnostics) and formal sensitivity (local influence, leave-one-out). The explanation is insightful and technically precise.\n**Verdict: Correct.**\n\nSince both options B and D provide correct explanations and recommend appropriate actions, they are both valid answers. Option B gives a more balanced explanation covering both the $\\mathrm{COVRATIO}$ and $\\mathrm{DFBETAS}$ phenomena explicitly, while option D provides a deeper, more specialized explanation of the directional influence. Together, they form a comprehensive picture.", "answer": "$$\\boxed{BD}$$", "id": "4959084"}, {"introduction": "While convenient, textbook rules of thumb for flagging outliers often lack statistical rigor, as they don't account for the multiple comparisons problem or the specific geometry of the data. This hands-on exercise guides you through implementing a modern, computationally-intensive solution: a residual bootstrap procedure to derive empirical, data-specific thresholds for diagnostic statistics. This practice [@problem_id:4959122] demonstrates how to control the family-wise error rate, providing a principled foundation for deciding which points warrant closer investigation.", "problem": "Consider the standard linear model for a continuous medical outcome, where a response vector $y \\in \\mathbb{R}^{n}$ is modeled as $y = X\\beta + \\varepsilon$, with design matrix $X \\in \\mathbb{R}^{n \\times p}$ (including an intercept column), coefficient vector $\\beta \\in \\mathbb{R}^{p}$, and noise vector $\\varepsilon \\in \\mathbb{R}^{n}$. The ordinary least squares (OLS) estimator $\\hat{\\beta}$ minimizes the residual sum of squares and yields fitted values $\\hat{y} = X\\hat{\\beta}$ and residuals $e = y - \\hat{y}$. The OLS hat matrix is $H = X(X^{\\top}X)^{-1}X^{\\top}$, and the leverage values are the diagonal elements $h_{ii}$ of $H$. The residual variance estimator is $s^{2} = \\frac{1}{n - p}\\sum_{i=1}^{n} e_{i}^{2}$. \n\nDefine the externally studentized residual for observation $i$ by\n$$\nr_{i} = \\frac{e_{i}}{s_{(i)}\\sqrt{1 - h_{ii}}}, \n$$\nwhere $s_{(i)}^{2}$ is the residual variance estimator computed from the model refitted without observation $i$. Also define Cook’s distance for observation $i$ by\n$$\nD_{i} = \\frac{e_{i}^{2}}{p\\,s^{2}} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^{2}}.\n$$\n\nYou are to implement a principled thresholding approach for detecting outliers and influential data points using an empirical null distribution obtained via a residual bootstrap with refitting. The empirical null is defined under the model $y = X\\hat{\\beta} + \\varepsilon^{*}$, where $\\varepsilon^{*}$ is generated by resampling centered residuals. The test statistic families are the absolute externally studentized residuals $\\{|r_{i}|\\}_{i=1}^{n}$ for outlier detection and the Cook’s distances $\\{D_{i}\\}_{i=1}^{n}$ for influence detection. To control the family-wise error rate (FWER) at level $\\alpha$ for each family, use the empirical distribution of the maximum statistic across $i$ in bootstrap replicates:\n- For $b = 1, \\dots, B$, construct bootstrap responses $y^{*(b)} = X\\hat{\\beta} + e^{*(b)}$, where $e^{*(b)}$ is obtained by sampling with replacement from $\\{e_{i} - \\bar{e}\\}_{i=1}^{n}$, with $\\bar{e}$ the mean residual. Refit the model to $y^{*(b)}$ to compute both families of statistics, and record $M_{r}^{(b)} = \\max_{1 \\le i \\le n} |r_{i}^{*(b)}|$ and $M_{D}^{(b)} = \\max_{1 \\le i \\le n} D_{i}^{*(b)}$.\n- Define thresholds $t_{r}$ and $t_{D}$ as the empirical $(1 - \\alpha)$-quantiles of $\\{M_{r}^{(b)}\\}_{b=1}^{B}$ and $\\{M_{D}^{(b)}\\}_{b=1}^{B}$, respectively.\n- Flag observation $i$ as an outlier if $|r_{i}| > t_{r}$ and as influential if $D_{i} > t_{D}$.\n\nYour program must implement the above and return, for each test case, two sorted lists of zero-based indices: the set of detected outliers and the set of detected influential points.\n\nNo physical units are involved, so none are required in the answer. Angles are not involved. If a fraction or decimal is needed for $\\alpha$, express it as a decimal (e.g., $0.05$). The final outputs must be lists of integers.\n\nTest Suite:\nImplement the following $4$ test cases. In each case, construct $X$ by concatenating an intercept column (all ones) with a single predictor column $x$ as specified. For all cases, define $y$ deterministically from the given formulae, without any random draws.\n\n- Case $1$ (baseline, happy path):\n  - Sample size $n = 20$ and number of parameters $p = 2$.\n  - Predictor values $x_{i} = \\frac{i}{19}$ for $i = 0, 1, \\dots, 19$.\n  - Noise offsets $\\varepsilon_{i}$ given by the list \n    $[0.3, -0.4, 0.8, -0.2, 0.1, -0.7, 0.9, -0.6, 0.2, -1.1, 0.3, 0.4, -0.2, 0.7, -0.5, 0.6, -0.8, 0.2, -0.4, 0.1]$.\n  - Response values $y_{i} = 100 + 5 x_{i} + \\varepsilon_{i}$.\n  - Bootstrap replicates $B = 300$ and FWER level $\\alpha = 0.05$.\n\n- Case $2$ (single vertical outlier):\n  - Use the same $x_{i}$ and $\\varepsilon_{i}$ as Case $1$.\n  - Construct the baseline $y_{i}$ as in Case $1$, then add a single deviation of magnitude $+8.0$ at index $i = 10$; that is, set $y_{10} \\leftarrow y_{10} + 8.0$.\n  - Bootstrap replicates $B = 300$ and FWER level $\\alpha = 0.05$.\n\n- Case $3$ (high-leverage influential point):\n  - Start from Case $1$ data and append one additional observation, giving $n = 21$.\n  - Append $x_{20} = 5.0$.\n  - Define the appended response value by $y_{20} = 100 + 5 \\cdot x_{20} + 8.5 = 133.5$ (i.e., a moderate positive deviation at a high-leverage $x$).\n  - Bootstrap replicates $B = 300$ and FWER level $\\alpha = 0.05$.\n\n- Case $4$ (small sample, milder deviation, different level):\n  - Sample size $n = 8$ and number of parameters $p = 2$.\n  - Predictor values $x_{i} = \\frac{i}{7}$ for $i = 0, 1, \\dots, 7$.\n  - Noise offsets $\\varepsilon_{i}$ given by the list $[0.1, -0.2, 0.0, 0.3, -0.4, 0.2, -0.1, 0.0]$.\n  - Response values $y_{i} = 100 + 5 x_{i} + \\varepsilon_{i}$, then add a milder deviation of magnitude $+3.0$ at index $i = 3$: set $y_{3} \\leftarrow y_{3} + 3.0$.\n  - Bootstrap replicates $B = 300$ and FWER level $\\alpha = 0.10$.\n\nAlgorithmic requirements:\n- Implement OLS via matrix algebra to obtain $\\hat{\\beta}$, $\\hat{y}$, $e$, $H$, and $h_{ii}$.\n- Compute $s^{2}$ and $s_{(i)}^{2}$ without refitting $n$ times by using algebraic identities valid under OLS. Ensure numerical stability for cases where $1 - h_{ii}$ is very small by applying safe guards in denominators.\n- Implement the residual bootstrap with refitting for $B$ replicates to obtain empirical maxima $M_{r}^{(b)}$ and $M_{D}^{(b)}$ and thresholds $t_{r}$ and $t_{D}$ at the desired $\\alpha$.\n- For each test case, return two sorted lists: indices $i$ such that $|r_{i}| > t_{r}$ and indices $i$ such that $D_{i} > t_{D}$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this list corresponds to one test case and must itself be a two-element list, where the first element is the sorted list of detected outlier indices and the second element is the sorted list of detected influential indices. For example, the output must look like \n$[[[i\\_1,i\\_2], [j\\_1]], [[],[k\\_1,k\\_2]], \\dots]$ \nwith all indices expressed as integers and lists ordered increasingly. No additional text may be printed.", "solution": "The foundation for principled detection of outliers and influential observations in linear modeling begins with ordinary least squares (OLS) and the geometry of the hat matrix. Given $y = X\\beta + \\varepsilon$ with $X \\in \\mathbb{R}^{n \\times p}$, the OLS estimator is $\\hat{\\beta} = (X^{\\top}X)^{-1} X^{\\top} y$, the fitted values are $\\hat{y} = X \\hat{\\beta}$, and the residuals are $e = y - \\hat{y}$. The hat matrix $H = X (X^{\\top}X)^{-1} X^{\\top}$ projects observed responses onto the column space of $X$, and its diagonal entries $h_{ii}$ quantify leverage, i.e., the influence of the $i$-th covariate pattern on its own fitted value.\n\nTo quantify outlyingness in the response direction relative to the model, externally studentized residuals adjust for observation-specific variance and leave-one-out uncertainty. The externally studentized residual for observation $i$ is \n$$\nr_{i} = \\frac{e_{i}}{s_{(i)}\\sqrt{1 - h_{ii}}},\n$$\nwhere $s_{(i)}^{2}$ is the residual variance estimator from the model refitted without observation $i$. The identity\n$$\ns_{(i)}^{2} = \\frac{(n - p)s^{2} - \\frac{e_{i}^{2}}{1 - h_{ii}}}{n - p - 1}\n$$\nis obtained from the Sherman–Morrison–Woodbury lemma and the leave-one-out decomposition of the residual sum of squares. This identity enables computing $s_{(i)}^{2}$ efficiently without explicitly refitting $n$ models. It requires $n - p - 1 > 0$ and numerical safeguards if $1 - h_{ii}$ is near zero.\n\nInfluence on the fitted model is quantified by Cook’s distance,\n$$\nD_{i} = \\frac{e_{i}^{2}}{p\\,s^{2}} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^{2}},\n$$\nwhich measures the aggregate change in $\\hat{\\beta}$ when observation $i$ is deleted, normalized by the residual variance and dimensionality. Large values of $D_{i}$ indicate influential points, typically combining substantial leverage $h_{ii}$ and non-negligible residual magnitude.\n\nClassical parametric thresholds for $|r_{i}|$ or $D_{i}$ rely on exact Gaussian assumptions and multiple-comparison adjustments. However, medical data often exhibit deviations from idealized assumptions. To control the family-wise error rate (FWER) at level $\\alpha$ across all observations in a model-agnostic way, we use an empirical null via a residual bootstrap with refitting:\n- Fit the model to obtain $\\hat{\\beta}$, residuals $e$, and fitted values $\\hat{y}$.\n- Center the residuals to enforce mean zero, $\\tilde{e}_{i} = e_{i} - \\bar{e}$, with $\\bar{e} = \\frac{1}{n}\\sum_{i=1}^{n} e_{i}$. Under standard OLS with an intercept, $\\bar{e} = 0$, but centering improves numerical stability and maintains the null symmetry.\n- For $b = 1, \\dots, B$:\n  1. Sample $\\{e_{i}^{*(b)}\\}_{i=1}^{n}$ with replacement from $\\{\\tilde{e}_{i}\\}_{i=1}^{n}$.\n  2. Form bootstrap responses $y^{*(b)} = X\\hat{\\beta} + e^{*(b)}$.\n  3. Refit OLS to $(X, y^{*(b)})$ to obtain $e^{*(b)}$, $h_{ii}^{*(b)}$, $s^{2*(b)}$, and compute both externally studentized residuals $\\{r_{i}^{*(b)}\\}$ and Cook’s distances $\\{D_{i}^{*(b)}\\}$.\n  4. Record maxima $M_{r}^{(b)} = \\max_{i} |r_{i}^{*(b)}|$ and $M_{D}^{(b)} = \\max_{i} D_{i}^{*(b)}$.\n- After $B$ replicates, the empirical thresholds are the $(1 - \\alpha)$-quantiles $t_{r} = Q_{1 - \\alpha}\\left(\\{M_{r}^{(b)}\\}_{b=1}^{B}\\right)$ and $t_{D} = Q_{1 - \\alpha}\\left(\\{M_{D}^{(b)}\\}_{b=1}^{B}\\right)$.\n\nThis approach controls the FWER across all observations because the threshold is derived from the distribution of the maximum statistic under the fitted model’s residual structure, treating the design matrix $X$ as fixed. The thresholds adapt to the correlation induced by leverage and to the scale of residual variation. Observations are flagged if their observed statistics exceed these thresholds: observation $i$ is considered an outlier if $|r_{i}| > t_{r}$ and influential if $D_{i} > t_{D}$.\n\nAlgorithmic steps to implement:\n1. Build $X$ with an intercept and the specified predictor vector $x$.\n2. Compute $\\hat{\\beta}$, $\\hat{y}$, $e$, $H$, $h_{ii}$, and $s^{2}$ via OLS identities.\n3. Compute $s_{(i)}^{2}$ using the leave-one-out identity, apply non-negativity and small-denominator safeguards: when $(1 - h_{ii})$ is very small, use a minimal floor such as $10^{-12}$ to avoid division by zero; similarly, clamp $s_{(i)}^{2}$ to be at least $10^{-12}$ to avoid numerical instabilities.\n4. Compute $|r_{i}|$ and $D_{i}$ for the observed data.\n5. Perform $B$ residual bootstrap replicates with refitting and record maxima of both statistic families; compute the empirical $(1 - \\alpha)$-quantiles to get $t_{r}$ and $t_{D}$.\n6. Return two sorted lists of indices for each test case: those $i$ such that $|r_{i}| > t_{r}$ and those $i$ such that $D_{i} > t_{D}$.\n\nDiscussion of test suite coverage:\n- Case $1$ provides a baseline with moderate noise and no injected anomalies; empirical thresholds should accommodate the residual scale and design geometry, typically yielding no detections.\n- Case $2$ injects a single vertical outlier (large deviation in $y$ at a typical $x$), which should result in a large externally studentized residual and be detected by the $|r_{i}|$ criterion; Cook’s distance might be less sensitive if leverage remains typical.\n- Case $3$ appends a high-leverage point with a moderate deviation in $y$; Cook’s distance should detect influence because $D_{i}$ scales with both $h_{ii}$ and $e_{i}^{2}$, particularly penalizing large leverage via $(1 - h_{ii})^{-2}$.\n- Case $4$ uses a small sample and a milder deviation, testing sensitivity at a different FWER level $\\alpha = 0.10$ and ensuring the algorithm behaves correctly with smaller $n$ where $n - p - 1$ remains positive.\n\nThe final program must implement these steps exactly and print a single line containing a list of pairs of sorted index lists for the four cases, with zero-based indexing and no extra text.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef add_intercept(x):\n    \"\"\"\n    Construct design matrix X with intercept and single predictor column.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    ones = np.ones_like(x)\n    X = np.column_stack([ones, x])\n    return X\n\ndef ols_fit(X, y):\n    \"\"\"\n    Compute OLS estimates, fitted values, residuals, residual variance, and hat matrix diagonal.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float)\n    XtX = X.T @ X\n    XtX_inv = np.linalg.inv(XtX)\n    beta = XtX_inv @ (X.T @ y)\n    fitted = X @ beta\n    resid = y - fitted\n    n, p = X.shape\n    rss = float(resid.T @ resid)\n    s2 = rss / max(n - p, 1)  # guard against division by zero, though test cases ensure n > p\n    # Efficient computation of hat matrix diagonal: diag(X @ XtX_inv @ X^T)\n    # Each diagonal element h_i = x_i^T (X^T X)^{-1} x_i\n    H_diag = np.sum(X * (X @ XtX_inv), axis=1)\n    return beta, fitted, resid, s2, H_diag\n\ndef externally_studentized_residuals_abs(X, y):\n    \"\"\"\n    Compute absolute externally studentized residuals |r_i|.\n    \"\"\"\n    beta, fitted, resid, s2, h = ols_fit(X, y)\n    n, p = X.shape\n    # Safeguards for numerical stability\n    one_minus_h = np.maximum(1.0 - h, 1e-12)\n    # Leave-one-out residual variance using OLS identity:\n    # s_(i)^2 = ((n - p) * s^2 - e_i^2 / (1 - h_ii)) / (n - p - 1)\n    denom_df = max(n - p - 1, 1)  # guard; test cases ensure n - p - 1 > 0\n    s_loo_sq = ((n - p) * s2 - (resid ** 2) / one_minus_h) / denom_df\n    s_loo_sq = np.maximum(s_loo_sq, 1e-12)\n    r_ext = resid / (np.sqrt(one_minus_h) * np.sqrt(s_loo_sq))\n    return np.abs(r_ext)\n\ndef cooks_distance(X, y):\n    \"\"\"\n    Compute Cook's distance D_i for each observation.\n    \"\"\"\n    beta, fitted, resid, s2, h = ols_fit(X, y)\n    n, p = X.shape\n    one_minus_h = np.maximum(1.0 - h, 1e-12)\n    denom = p * s2 * (one_minus_h ** 2)\n    denom = np.maximum(denom, 1e-15)\n    D = (resid ** 2) * h / denom\n    return D\n\ndef bootstrap_thresholds(X, y, B=300, alpha=0.05, rng_seed=12345):\n    \"\"\"\n    Residual bootstrap with refitting to compute empirical (1 - alpha)-quantile thresholds\n    for the maxima of |r_i| and D_i across observations.\n    \"\"\"\n    # Fit on the original data\n    beta, fitted, resid, s2, h = ols_fit(X, y)\n    # Center residuals\n    resid_centered = resid - np.mean(resid)\n    n = len(y)\n    rng = np.random.default_rng(rng_seed)\n    max_r_values = np.empty(B, dtype=float)\n    max_D_values = np.empty(B, dtype=float)\n    for b in range(B):\n        resampled = rng.choice(resid_centered, size=n, replace=True)\n        y_star = fitted + resampled\n        # Recompute statistics on bootstrap sample\n        r_abs_star = externally_studentized_residuals_abs(X, y_star)\n        D_star = cooks_distance(X, y_star)\n        max_r_values[b] = np.max(r_abs_star)\n        max_D_values[b] = np.max(D_star)\n    thr_r = float(np.quantile(max_r_values, 1.0 - alpha))\n    thr_D = float(np.quantile(max_D_values, 1.0 - alpha))\n    return thr_r, thr_D\n\ndef case_1():\n    # n = 20, x_i = i/19\n    n = 20\n    x = np.linspace(0.0, 1.0, n)\n    eps = np.array([0.3, -0.4, 0.8, -0.2, 0.1, -0.7, 0.9, -0.6, 0.2, -1.1,\n                    0.3, 0.4, -0.2, 0.7, -0.5, 0.6, -0.8, 0.2, -0.4, 0.1], dtype=float)\n    y = 100.0 + 5.0 * x + eps\n    B = 300\n    alpha = 0.05\n    return x, y, B, alpha\n\ndef case_2():\n    # Same as case 1, but add +8 at index 10\n    x, y, B, alpha = case_1()\n    y2 = y.copy()\n    y2[10] = y2[10] + 8.0\n    return x, y2, B, alpha\n\ndef case_3():\n    # Case 1 appended with high-leverage x=5.0 and y=133.5\n    x, y, B, alpha = case_1()\n    x3 = np.concatenate([x, np.array([5.0])])\n    y3 = np.concatenate([y, np.array([133.5])])\n    return x3, y3, B, alpha\n\ndef case_4():\n    # n = 8, x_i = i/7, mild deviation +3 at index 3, alpha = 0.10\n    n = 8\n    x = np.linspace(0.0, 1.0, n)\n    eps = np.array([0.1, -0.2, 0.0, 0.3, -0.4, 0.2, -0.1, 0.0], dtype=float)\n    y = 100.0 + 5.0 * x + eps\n    y[3] = y[3] + 3.0\n    B = 300\n    alpha = 0.10\n    return x, y, B, alpha\n\ndef analyze_case(x, y, B, alpha):\n    \"\"\"\n    Perform bootstrap-based thresholding and return lists of outlier and influential indices.\n    \"\"\"\n    X = add_intercept(x)\n    thr_r, thr_D = bootstrap_thresholds(X, y, B=B, alpha=alpha, rng_seed=12345)\n    r_abs = externally_studentized_residuals_abs(X, y)\n    D = cooks_distance(X, y)\n    outliers = np.where(r_abs > thr_r)[0]\n    influentials = np.where(D > thr_D)[0]\n    return sorted(outliers.tolist()), sorted(influentials.tolist())\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        case_1(),\n        case_2(),\n        case_3(),\n        case_4(),\n    ]\n\n    results = []\n    for x, y, B, alpha in test_cases:\n        out_idx, inf_idx = analyze_case(x, y, B, alpha)\n        results.append([out_idx, inf_idx])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4959122"}, {"introduction": "An alternative to detecting and removing outliers is to build models that are inherently robust to their effects. This advanced practice introduces a powerful Bayesian approach where the assumption of normally distributed errors is relaxed in favor of a heavy-tailed distribution, which automatically down-weights extreme observations. By implementing an iterative algorithm to fit this model [@problem_id:4959208], you will gain hands-on experience with a framework that accommodates outliers within the model, shifting the focus from outlier detection to robust estimation.", "problem": "Consider the linear regression model for patient-level measurements in medicine: for each patient index $i \\in \\{1,\\dots,n\\}$, the scalar response $y_i$ relates to a scalar covariate $x_i$ via $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$. Assume $\\varepsilon_i$ are independent conditional on parameters. We compare two Bayesian specifications that differ only in how they regularize observation-specific deviations.\n\nSpecification $S_{\\mathrm{N}}$ (light-tailed): $\\varepsilon_i \\mid \\sigma^2 \\sim \\mathcal{N}(0,\\sigma^2)$, where $\\sigma^2$ has an unspecified proper prior and the coefficient vector $(\\beta_0,\\beta_1)$ has an unspecified proper prior; these choices guarantee a proper posterior under standard conditions but are otherwise not prescriptive.\n\nSpecification $S_{\\mathrm{HT}}$ (heavy-tailed via scale mixture): introduce latent scales $\\lambda_i$ and set $\\varepsilon_i \\mid \\lambda_i,\\sigma^2 \\sim \\mathcal{N}(0,\\lambda_i \\sigma^2)$, with independent latent scales $\\lambda_i \\sim \\mathrm{Inv\\text{-}Gamma}(\\alpha,\\beta)$ for fixed hyperparameters $(\\alpha,\\beta)$ and unspecified proper priors for $(\\beta_0,\\beta_1,\\sigma^2)$. This hierarchical prior induces heavy-tailed marginal errors for $y_i \\mid x_i$, and is widely used to mitigate the influence of outliers by allowing large residuals to be accommodated through large $\\lambda_i$ values rather than extreme parameter shifts.\n\nUsing Bayes’ theorem $p(\\theta \\mid y) \\propto p(y \\mid \\theta)\\,p(\\theta)$, where $\\theta$ collects all unknowns, and the scale-mixture representation $\\mathcal{N}(0,\\lambda_i \\sigma^2)$ with $\\lambda_i \\sim \\mathrm{Inv\\text{-}Gamma}(\\alpha,\\beta)$, derive algorithmic estimators that follow the Maximum A Posteriori (MAP) principle, starting from the fundamental definitions of conditional independence and the normal linear model, without assuming any closed-form posterior that shortcuts the derivation.\n\nYour program must implement, for each specification:\n- A principled iterative scheme grounded in Expectation-Maximization (EM) for $S_{\\mathrm{HT}}$ and standard normal equations for $S_{\\mathrm{N}}$, yielding MAP-equivalent fixed-point estimates for $(\\beta_0,\\beta_1)$ and $\\sigma^2$, and a set of observation-specific weights $w_i$ reflecting local downweighting. For $S_{\\mathrm{N}}$, the weights are flat, i.e., $w_i = 1$ for all $i$; for $S_{\\mathrm{HT}}}$, the EM produces $w_i$ as functions of residuals that decrease for large $|y_i - \\beta_0 - \\beta_1 x_i|$.\n- Influence diagnostics derived from the hat matrix $H$ of the fitted weighted least squares. Let $X$ denote the $n \\times 2$ design matrix with first column $1$ and second column $x_i$, and $W$ the diagonal matrix with entries $w_i$. Define the weighted hat matrix $H = W^{1/2} X (X^\\top W X)^{-1} X^\\top W^{1/2}$, the residuals $r_i = y_i - \\hat{y}_i$ with $\\hat{y} = X \\hat{\\beta}$, and use the Leave-One-Out (LOO) identity $r_i^{(-i)} = r_i / (1 - h_{ii})$ where $h_{ii}$ is the $i$-th diagonal element of $H$. For specification $S_{\\mathrm{N}}$, approximate the LOO predictive variance as $\\hat{\\sigma}^2/(1 - h_{ii})$. For specification $S_{\\mathrm{HT}}$, approximate the LOO predictive variance as $(\\hat{\\sigma}^2 / w_i)/(1 - h_{ii})$. These approximations must be derived from first principles of linear projection in weighted least squares and conditional Gaussianity.\n- Posterior Predictive Checks (PPCs): for each $i$, compute the two-sided tail probability under the approximate LOO predictive distribution that $|Y_i - \\mu_i^{(-i)}| \\ge |y_i - \\mu_i^{(-i)}|$, where $\\mu_i^{(-i)} = y_i - r_i^{(-i)}$ and the variance is as specified above. Under the normal approximation with variance $v_i^{(-i)}$, the two-sided tail probability is $p_i = 2 \\left( 1 - \\Phi\\left( \\left| r_i^{(-i)} \\right|/\\sqrt{v_i^{(-i)}} \\right) \\right)$, where $\\Phi$ is the standard normal cumulative distribution function. Flag observation $i$ as an outlier if $p_i < 0.05$.\n- Cook’s distance analogs: define the parameter dimension $p = 2$, and compute $D_i = \\frac{r_i^2}{p \\,\\hat{\\sigma}^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}$ for $S_{\\mathrm{N}}$. For $S_{\\mathrm{HT}}$, compute the weighted analog with $r_i^{(w)} = \\sqrt{w_i}\\,r_i$, namely $D_i^{(w)} = \\frac{(r_i^{(w)})^2}{p \\,\\hat{\\sigma}^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}$. Report the maximum Cook’s distance over $i$ for each specification.\n\nTest Suite:\nImplement the above for the following deterministic test cases, each providing $(n,\\{x_i\\},\\{y_i\\},\\alpha,\\beta)$:\n- Case $1$ (general case with a moderate outlier): $n = 24$, $x_i = i/(n-1)$ for $i = 0,\\dots,23$, base signal $y_i^{(0)} = 5 + 2 x_i + 0.1 \\sin(10 x_i)$, and an additive outlier at index $i = 20$ of magnitude $+8$. Thus $y_i = y_i^{(0)}$ for all $i \\ne 20$ and $y_{20} = y_{20}^{(0)} + 8$. Heavy-tailed hyperparameters: $(\\alpha,\\beta) = (2,2)$.\n- Case $2$ (edge case with a high-leverage strong outlier): the same $x_i$ and base signal, but an additive outlier at index $i = 23$ of magnitude $+12$. Heavy-tailed hyperparameters: $(\\alpha,\\beta) = (2,2)$.\n- Case $3$ (edge case with no outliers): the same $x_i$ and base signal, $y_i = y_i^{(0)}$ for all $i$, with $(\\alpha,\\beta) = (2,2)$.\n\nFor each case, your program must output a single line containing a list of results across the three cases. For each case, report a list of $4$ values in the order: number of outliers flagged under $S_{\\mathrm{N}}$ as an integer, number of outliers flagged under $S_{\\mathrm{HT}}$ as an integer, maximum Cook’s distance under $S_{\\mathrm{N}}$ as a float rounded to $6$ decimal places, and maximum Cook’s distance under $S_{\\mathrm{HT}}$ as a float rounded to $6$ decimal places.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list of case-wise lists, enclosed in square brackets. For example, an output with three cases should look like $[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3]]$ where each $a_k$ and $b_k$ are integers, and each $c_k$ and $d_k$ are floats rounded to $6$ decimal places.", "solution": "The problem requires the derivation and implementation of statistical methods for outlier detection in linear regression, comparing a standard Normal model ($S_{\\mathrm{N}}$) with a robust heavy-tailed model ($S_{\\mathrm{HT}}$). This involves deriving parameter estimation schemes and influence diagnostics from fundamental principles.\n\n### Problem Validation\n**Step 1: Extract Givens**\n- **Linear Model**: $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ for $i \\in \\{1, \\dots, n\\}$.\n- **Specification $S_{\\mathrm{N}}$**: $\\varepsilon_i \\mid \\sigma^2 \\sim \\mathcal{N}(0,\\sigma^2)$. Proper priors for $(\\beta_0, \\beta_1, \\sigma^2)$. Estimation via normal equations. Weights $w_i = 1$.\n- **Specification $S_{\\mathrm{HT}}$**: $\\varepsilon_i \\mid \\lambda_i, \\sigma^2 \\sim \\mathcal{N}(0,\\lambda_i \\sigma^2)$ with latent scales $\\lambda_i \\sim \\mathrm{Inv\\text{-}Gamma}(\\alpha,\\beta)$. Proper priors for $(\\beta_0, \\beta_1, \\sigma^2)$. Estimation via Expectation-Maximization (EM). Weights $w_i$ are functions of residuals.\n- **Influence Diagnostics**:\n    - Design matrix $X$ is $n \\times 2$ with columns of $1$s and $x_i$s.\n    - Weighted hat matrix $H = W^{1/2} X (X^\\top W X)^{-1} X^\\top W^{1/2}$, where $W = \\mathrm{diag}(w_i)$.\n    - LOO residual identity: $r_i^{(-i)} = r_i / (1 - h_{ii})$, where $r_i = y_i - x_i^\\top\\hat{\\beta}$.\n    - Approx. LOO predictive variance for $S_{\\mathrm{N}}$: $v_i^{(-i)} = \\hat{\\sigma}^2/(1 - h_{ii})$.\n    - Approx. LOO predictive variance for $S_{\\mathrm{HT}}$: $v_i^{(-i)} = (\\hat{\\sigma}^2 / w_i)/(1 - h_{ii})$.\n- **Posterior Predictive Checks (PPCs)**: Flag observation $i$ as an outlier if $p_i < 0.05$, where $p_i = 2 \\left( 1 - \\Phi\\left( |r_i^{(-i)}|/\\sqrt{v_i^{(-i)}} \\right) \\right)$ and $\\Phi$ is the standard Normal CDF.\n- **Cook's Distance Analogs**: Parameter count $p=2$.\n    - For $S_{\\mathrm{N}}$: $D_i = \\frac{r_i^2}{p \\hat{\\sigma}^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}$.\n    - For $S_{\\mathrm{HT}}$: $D_i^{(w)} = \\frac{w_i r_i^2}{p \\hat{\\sigma}^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}$.\n- **Test Cases**: Three deterministic cases are provided, specifying $n, \\{x_i\\}, \\{y_i\\}, \\alpha, \\beta$.\n- **Output**: For each case, report `[# outliers SN, # outliers SHT, max Cook's D SN, max Cook's D SHT]`.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is well-grounded in standard statistical theory. Robust regression using scale mixtures of Normals (leading to Student-t marginal errors) is a classic technique. The EM algorithm is the standard method for latent variable model estimation. All diagnostics (hat matrix, LOO cross-validation, Cook's distance) are fundamental to regression analysis.\n- **Well-Posedness**: The problem is well-posed. Although it mentions \"unspecified proper priors\" for a \"Maximum A Posteriori (MAP)\" estimate, it immediately clarifies the intended procedure by specifying the algorithms (normal equations and EM). This directs the solver towards what is effectively Maximum Likelihood Estimation (MLE), a common and valid approach that can be viewed as the MAP estimate under flat priors. The iterative schemes are guaranteed to converge under mild conditions.\n- **Objectivity**: The problem is stated with precise mathematical definitions and objective criteria. The test cases are deterministic.\n\n**Step 3: Verdict and Action**\nThe problem is scientifically sound, well-posed, and objective. It is deemed **valid**. We proceed to the solution.\n\n---\n\n### Derivation of Estimators and Diagnostics\n\nLet $\\beta = [\\beta_0, \\beta_1]^\\top$ be the coefficient vector and $X$ be the $n \\times 2$ design matrix whose $i$-th row is $x_i^\\top = [1, x_i]$.\n\n#### Specification $S_{\\mathrm{N}}$ (Standard Normal Model)\n\n**1. Parameter Estimation**\nThe problem calls for MAP estimation. With unspecified priors, we find the parameter values that maximize the likelihood of the data. The log-likelihood function for the Normal model is:\n$$ \\mathcal{L}(\\beta, \\sigma^2; y, X) = \\log p(y | X, \\beta, \\sigma^2) = \\sum_{i=1}^n \\log \\mathcal{N}(y_i; x_i^\\top\\beta, \\sigma^2) $$\n$$ \\mathcal{L} = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - x_i^\\top\\beta)^2 $$\nTo find the MAP/MLE estimate $\\hat{\\beta}$, we minimize the sum of squared errors term $\\sum_{i=1}^n (y_i - x_i^\\top\\beta)^2 = \\|y - X\\beta\\|^2$. This is the Ordinary Least Squares (OLS) criterion. The solution is given by the normal equations $X^\\top X \\hat{\\beta} = X^\\top y$, which yields:\n$$ \\hat{\\beta} = (X^\\top X)^{-1}X^\\top y $$\nTo find $\\hat{\\sigma}^2$, we differentiate $\\mathcal{L}$ with respect to $\\sigma^2$ and set the derivative to zero, substituting $\\hat{\\beta}$:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^n (y_i - x_i^\\top\\hat{\\beta})^2 = 0 \\implies \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (y_i - x_i^\\top\\hat{\\beta})^2 = \\frac{1}{n} \\|y - X\\hat{\\beta}\\|^2 $$\nFor this model, the weights are uniform, $w_i=1$ for all $i$, so $W=I_n$.\n\n**2. Diagnostics**\n- **Hat Matrix and LOO Residuals**: The hat matrix is $H = X(X^\\top X)^{-1}X^\\top$. The diagonal elements $h_{ii}$ are the leverages. The leave-one-out (LOO) residual identity $r_i^{(-i)} = r_i / (1 - h_{ii})$ is a standard result from the algebra of linear projections.\n- **LOO Predictive Variance**: The LOO prediction for $y_i$ is $\\mu_i^{(-i)} = x_i^\\top \\hat{\\beta}^{(-i)}$. The variance of the predictive error $Y_i - \\mu_i^{(-i)}$ is the sum of the observation variance and the prediction variance: $\\mathrm{Var}(Y_i - \\mu_i^{(-i)}) = \\mathrm{Var}(\\varepsilon_i) + \\mathrm{Var}(x_i^\\top \\hat{\\beta}^{(-i)})$. This evaluates to $\\sigma^2 + \\sigma^2 x_i^\\top(X_{(-i)}^\\top X_{(-i)})^{-1}x_i$. Using the Sherman-Morrison-Woodbury formula, one can show that $x_i^\\top(X_{(-i)}^\\top X_{(-i)})^{-1}x_i = h_{ii}/(1-h_{ii})$. The exact LOO predictive variance is thus:\n$$ \\mathrm{Var}(Y_i - \\mu_i^{(-i)}) = \\sigma^2 \\left(1 + \\frac{h_{ii}}{1-h_{ii}}\\right) = \\frac{\\sigma^2}{1-h_{ii}} $$\nUsing our estimate $\\hat{\\sigma}^2$, we get the specified approximation $v_i^{(-i)} = \\hat{\\sigma}^2 / (1-h_{ii})$.\n- **PPC p-value**: The test statistic is $Z_i = (Y_i - \\mu_i^{(-i)}) / \\sqrt{v_i^{(-i)}}$, which is approximately $\\mathcal{N}(0,1)$. The statistic becomes $|z_i| = |r_i^{(-i)}|/\\sqrt{v_i^{(-i)}} = |r_i| / \\sqrt{\\hat{\\sigma}^2(1-h_{ii})}$. The two-sided p-value is $p_i = 2(1 - \\Phi(|z_i|))$.\n- **Cook's Distance**: The formula $D_i = \\frac{r_i^2}{p \\hat{\\sigma}^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}$ with $p=2$ is used directly.\n\n#### Specification $S_{\\mathrm{HT}}$ (Heavy-Tailed Model)\n\n**1. Parameter Estimation via EM Algorithm**\nThis model introduces latent variables $\\lambda_i \\sim \\mathrm{Inv\\text{-}Gamma}(\\alpha, \\beta)$, making direct maximization of the likelihood difficult. We use the EM algorithm, which iteratively computes the expectation of the complete-data log-likelihood (E-step) and then maximizes it (M-step).\nThe complete-data log-likelihood (proportional to the log-posterior with flat priors for $\\beta, \\log \\sigma$) is:\n$$ \\log p(y, \\lambda | X, \\beta, \\sigma^2) \\propto \\sum_{i=1}^n \\left( \\log p(y_i|\\lambda_i, \\beta, \\sigma^2) + \\log p(\\lambda_i|\\alpha, \\beta) \\right) $$\n$$ \\propto \\sum_{i=1}^n \\left( -\\frac{1}{2}\\log(\\lambda_i \\sigma^2) - \\frac{(y_i - x_i^\\top\\beta)^2}{2\\lambda_i\\sigma^2} - (\\alpha+1)\\log\\lambda_i - \\frac{\\beta}{\\lambda_i} \\right) $$\n\n- **E-Step**: At iteration $t$, we compute the expectation of the sufficient statistics for $\\lambda_i$ (namely $1/\\lambda_i$ and $\\log\\lambda_i$) conditional on the data $y$ and current parameters $\\theta^{(t)} = \\{\\beta^{(t)}, (\\sigma^2)^{(t)}\\}$. The conditional posterior for $\\lambda_i$ is:\n$$ p(\\lambda_i | y_i, \\theta^{(t)}) \\propto p(y_i|\\lambda_i, \\theta^{(t)}) p(\\lambda_i) \\propto \\lambda_i^{-1/2} e^{-\\frac{(r_i^{(t)})^2}{2\\lambda_i(\\sigma^2)^{(t)}}} \\cdot \\lambda_i^{-(\\alpha+1)} e^{-\\frac{\\beta}{\\lambda_i}} $$\nwhere $r_i^{(t)} = y_i - x_i^\\top \\beta^{(t)}$. This is the kernel of an Inverse-Gamma distribution:\n$$ \\lambda_i | y_i, \\theta^{(t)} \\sim \\mathrm{Inv\\text{-}Gamma}\\left(\\alpha' = \\alpha + \\frac{1}{2}, \\beta' = \\beta + \\frac{(r_i^{(t)})^2}{2(\\sigma^2)^{(t)}}\\right) $$\nThe required expectation for the M-step is $w_i^{(t+1)} = E[1/\\lambda_i | y, \\theta^{(t)}]$. For an $\\mathrm{Inv\\text{-}Gamma}(a, b)$ distribution, $E[1/X] = a/b$. Thus:\n$$ w_i^{(t+1)} = \\frac{\\alpha + 1/2}{\\beta + \\frac{(r_i^{(t)})^2}{2(\\sigma^2)^{(t)}}} = \\frac{2\\alpha + 1}{2\\beta + (r_i^{(t)})^2/(\\sigma^2)^{(t)}} $$\nThese are the observation-specific weights. Large residuals $|r_i^{(t)}|$ lead to small weights $w_i^{(t+1)}$, down-weighting the influence of potential outliers.\n\n- **M-Step**: We maximize the expected complete-data log-likelihood $Q(\\theta|\\theta^{(t)}) = E_{\\lambda|y,\\theta^{(t)}}[\\log p(y, \\lambda|\\theta)]$:\n$$ Q(\\theta|\\theta^{(t)}) \\propto -\\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n w_i^{(t+1)} (y_i - x_i^\\top\\beta)^2 $$\nMaximizing with respect to $\\beta$ is a Weighted Least Squares (WLS) problem:\n$$ \\hat{\\beta}^{(t+1)} = (X^\\top W^{(t+1)} X)^{-1} X^\\top W^{(t+1)} y $$\nwhere $W^{(t+1)} = \\mathrm{diag}(w_i^{(t+1)})$. Maximizing with respect to $\\sigma^2$ yields:\n$$ (\\hat{\\sigma}^2)^{(t+1)} = \\frac{1}{n} \\sum_{i=1}^n w_i^{(t+1)} (y_i - x_i^\\top\\hat{\\beta}^{(t+1)})^2 $$\nThe E and M steps are iterated until convergence of the parameters.\n\n**2. Diagnostics**\n- **Hat Matrix and LOO Residuals**: The WLS fit is equivalent to an OLS fit on transformed variables $y^* = W^{1/2}y$ and $X^* = W^{1/2}X$. The relevant hat matrix is for this transformed problem, $H = X^*((X^*)^\\top X^*)^{-1}(X^*)^\\top = W^{1/2}X(X^\\top WX)^{-1}X^\\top W^{1/2}$, as specified. Its diagonal elements are $h_{ii} = w_i x_i^\\top (X^\\top WX)^{-1} x_i$. The LOO residual identity $r_i^{(-i)} = r_i / (1 - h_{ii})$ still holds for this definition of $h_{ii}$.\n- **LOO Predictive Variance**: In the $S_{HT}$ model, the WLS formulation implies an effective error variance for observation $i$ of $\\sigma^2/w_i$. The derivation for the LOO predictive variance is analogous to the OLS case, but incorporating this observation-specific variance. The total variance is $\\mathrm{Var}(Y_i) + \\mathrm{Var}(\\text{prediction}) = \\frac{\\sigma^2}{w_i} + \\mathrm{Var}(x_i^\\top \\hat{\\beta}^{(-i)})$. The second term evaluates to $\\frac{\\sigma^2}{w_i} \\frac{h_{ii}}{1-h_{ii}}$. Summing them gives:\n$$ \\mathrm{Var}(Y_i - \\mu_i^{(-i)}) = \\frac{\\sigma^2}{w_i} \\left(1 + \\frac{h_{ii}}{1-h_{ii}}\\right) = \\frac{\\sigma^2}{w_i(1-h_{ii})} $$\nUsing the estimates from the EM algorithm, we obtain the approximation $v_i^{(-i)} = (\\hat{\\sigma}^2/w_i)/(1 - h_{ii})$.\n- **PPC p-value**: The standardized LOO residual is $|z_i| = |r_i^{(-i)}|/\\sqrt{v_i^{(-i)}} = |r_i|\\sqrt{w_i} / \\sqrt{\\hat{\\sigma}^2(1-h_{ii})}$. The p-value is $p_i = 2(1 - \\Phi(|z_i|))$.\n- **Cook's Distance**: The formula $D_i^{(w)} = \\frac{w_i r_i^2}{p \\hat{\\sigma}^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}$ is the direct analog of the standard Cook's distance for the weighted regression.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases.\n    \"\"\"\n    \n    test_cases_params = [\n        {'n': 24, 'outlier_idx': 20, 'outlier_mag': 8, 'alpha': 2.0, 'beta_p': 2.0},\n        {'n': 24, 'outlier_idx': 23, 'outlier_mag': 12, 'alpha': 2.0, 'beta_p': 2.0},\n        {'n': 24, 'outlier_idx': None, 'outlier_mag': 0, 'alpha': 2.0, 'beta_p': 2.0},\n    ]\n\n    all_results = []\n    \n    for params in test_cases_params:\n        n = params['n']\n        x_vals = np.linspace(0, 1, n)\n        y_base = 5.0 + 2.0 * x_vals + 0.1 * np.sin(10.0 * x_vals)\n        y_vals = y_base.copy()\n        if params['outlier_idx'] is not None:\n            y_vals[params['outlier_idx']] += params['outlier_mag']\n        \n        alpha = params['alpha']\n        beta_p = params['beta_p']\n        \n        case_results = process_case(n, x_vals, y_vals, alpha, beta_p)\n        all_results.append(case_results)\n\n    # Format output\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]:.6f},{r[3]:.6f}]\" for r in all_results]) + \"]\"\n    print(output_str)\n\ndef process_case(n, x_vals, y_vals, alpha, beta_p):\n    \"\"\"\n    Processes a single test case, performing analysis for S_N and S_HT models.\n    \"\"\"\n    X = np.stack([np.ones(n), x_vals], axis=1)\n    y = y_vals\n    p_dim = 2.0  # float for calculations\n\n    # ----- Specification S_N (Normal Model) -----\n    w_n = np.ones(n)\n    W_n = np.diag(w_n)\n    \n    # Parameter estimation\n    try:\n        beta_n = np.linalg.solve(X.T @ X, X.T @ y)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudo-inverse if singular\n        beta_n = np.linalg.pinv(X.T @ X) @ X.T @ y\n        \n    r_n = y - X @ beta_n\n    sigma_sq_n = np.mean(r_n**2)\n    \n    # Hat matrix diagonals\n    try:\n        C_n = np.linalg.inv(X.T @ W_n @ X)\n        h_n = np.einsum('ij,jk,ki->i', X, C_n, X.T) * w_n\n    except np.linalg.LinAlgError:\n         h_n = np.full(n, 1.0/n) # Fallback for singular matrix\n    \n    # PPC outlier detection\n    # Prevent division by zero or sqrt of negative for h_ii near 1\n    h_n_safe = np.minimum(h_n, 1.0 - 1e-9)\n    z_n_denom = np.sqrt(sigma_sq_n * (1.0 - h_n_safe))\n    z_n = np.divide(np.abs(r_n), z_n_denom, out=np.zeros_like(z_n_denom), where=z_n_denom!=0)\n    p_vals_n = 2 * (1 - norm.cdf(z_n))\n    outliers_n = np.sum(p_vals_n < 0.05)\n    \n    # Cook's distance\n    cook_d_denom = (1.0 - h_n_safe)**2\n    cook_d_n_term = np.divide(h_n_safe, cook_d_denom, out=np.zeros_like(cook_d_denom), where=cook_d_denom!=0)\n    cook_d_n = (r_n**2 / (p_dim * sigma_sq_n)) * cook_d_n_term\n    max_cook_d_n = np.max(cook_d_n)\n    \n    # ----- Specification S_HT (Heavy-Tailed Model) -----\n    # Initialize with S_N results\n    beta_ht = beta_n.copy()\n    sigma_sq_ht = sigma_sq_n\n    \n    n_iter = 100\n    for _ in range(n_iter):\n        r_ht_iter = y - X @ beta_ht\n        \n        # E-step: calculate weights\n        w_denom = (2.0 * beta_p + r_ht_iter**2 / sigma_sq_ht)\n        w_ht = np.divide(2.0 * alpha + 1.0, w_denom, out=np.zeros_like(w_denom), where=w_denom!=0)\n        W_ht = np.diag(w_ht)\n        \n        # M-step: update parameters\n        X_T_W_ht = X.T @ W_ht\n        try:\n           beta_ht = np.linalg.solve(X_T_W_ht @ X, X_T_W_ht @ y)\n        except np.linalg.LinAlgError:\n           beta_ht = np.linalg.pinv(X_T_W_ht @ X) @ (X_T_W_ht @ y)\n\n        r_ht_updated = y - X @ beta_ht\n        sigma_sq_ht = np.mean(w_ht * r_ht_updated**2)\n        if sigma_sq_ht < 1e-12: sigma_sq_ht = 1e-12\n        \n    # Final values after convergence\n    r_ht = y - X @ beta_ht\n    w_ht_final = (2.0 * alpha + 1.0) / (2.0 * beta_p + r_ht**2 / sigma_sq_ht)\n    W_ht_final = np.diag(w_ht_final)\n    \n    # Hat matrix diagonals\n    try:\n        C_ht = np.linalg.inv(X.T @ W_ht_final @ X)\n        h_ht = w_ht_final * np.einsum('ij,jk,ki->i', X, C_ht, X.T)\n    except np.linalg.LinAlgError:\n        h_ht = np.full(n, 1.0/n)\n        \n    # PPC outlier detection\n    h_ht_safe = np.minimum(h_ht, 1.0 - 1e-9)\n    z_ht_denom = np.sqrt(sigma_sq_ht * (1.0 - h_ht_safe))\n    z_ht_num = np.abs(r_ht) * np.sqrt(w_ht_final)\n    z_ht = np.divide(z_ht_num, z_ht_denom, out=np.zeros_like(z_ht_denom), where=z_ht_denom!=0)\n    p_vals_ht = 2 * (1 - norm.cdf(z_ht))\n    outliers_ht = np.sum(p_vals_ht < 0.05)\n    \n    # Weighted Cook's distance\n    cook_d_ht_denom = (1.0 - h_ht_safe)**2\n    cook_d_ht_term = np.divide(h_ht_safe, cook_d_ht_denom, out=np.zeros_like(cook_d_ht_denom), where=cook_d_ht_denom!=0)\n    cook_d_ht_num = w_ht_final * r_ht**2\n    sigma_sq_ht_safe = max(sigma_sq_ht, 1e-12)\n    cook_d_ht = (cook_d_ht_num / (p_dim * sigma_sq_ht_safe)) * cook_d_ht_term\n    max_cook_d_ht = np.max(cook_d_ht)\n\n    return outliers_n, outliers_ht, max_cook_d_n, max_cook_d_ht\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "4959208"}]}