{"hands_on_practices": [{"introduction": "The stability of our regression coefficients is critically dependent on the relationships between the predictors. This exercise moves beyond a qualitative understanding of multicollinearity to a rigorous quantification, asking you to derive the link between the design matrix's condition number and the worst-case inflation of estimator variance. By working through this derivation, you will gain a deeper appreciation for how the geometry of your data directly impacts the precision of your statistical inferences [@problem_id:4952774].", "problem": "Consider a cohort study in hypertension with $n$ adult patients, where the outcome vector $y \\in \\mathbb{R}^{n}$ is modeled linearly by $p$ clinically relevant covariates collected in the design matrix $X \\in \\mathbb{R}^{n \\times p}$. Assume the Gauss–Markov conditions: the true model is $y = X \\beta + \\varepsilon$ with $\\mathbb{E}(\\varepsilon) = 0$ and $\\operatorname{Var}(\\varepsilon) = \\sigma^{2} I_{n}$, and the columns of $X$ are linearly independent. Let $\\hat{\\beta}$ denote the Ordinary Least Squares (OLS) estimator, defined as the minimizer of the sum of squared residuals.\n\nIn medical datasets, near collinearity among predictors (for example, among laboratory biomarkers that measure overlapping physiological processes) can inflate the uncertainty in estimated effects. To formalize this, let $s_{1} \\geq s_{2} \\geq \\dots \\geq s_{p} > 0$ be the singular values of $X$ from its Singular Value Decomposition (SVD), and define the Euclidean-norm condition number of $X$ as $\\kappa(X) = \\frac{s_{1}}{s_{p}}$. For any contrast vector $c \\in \\mathbb{R}^{p}$ with $\\|c\\|_{2} = 1$, the sampling variance of the contrast $c^\\top \\hat{\\beta}$ depends on the geometry of $X$.\n\nStarting strictly from the stated linear model assumptions and basic definitions (without invoking any pre-stated estimator variance formulas), derive the expression for the maximum-to-minimum ratio, over all unit-norm contrasts $c$, of the sampling variance $\\operatorname{Var}(c^\\top \\hat{\\beta})$. Express your final answer solely as a closed-form analytic expression in terms of $\\kappa(X)$, and provide no numerical approximation. No physical units are required for the final expression.", "solution": "The problem is subjected to validation against the provided criteria.\n\n### Problem Validation\n**Step 1: Extract Givens**\n- The model is a linear model for a cohort study in hypertension with $n$ patients.\n- The outcome vector is $y \\in \\mathbb{R}^{n}$.\n- The design matrix is $X \\in \\mathbb{R}^{n \\times p}$, containing $p$ covariates.\n- The Gauss–Markov conditions are assumed:\n    - The true model is $y = X \\beta + \\varepsilon$.\n    - The expected value of the error term is $\\mathbb{E}(\\varepsilon) = 0$.\n    - The variance-covariance matrix of the error term is $\\operatorname{Var}(\\varepsilon) = \\sigma^{2} I_{n}$, where $I_n$ is the $n \\times n$ identity matrix.\n    - The columns of the design matrix $X$ are linearly independent.\n- The Ordinary Least Squares (OLS) estimator, $\\hat{\\beta}$, is the minimizer of the sum of squared residuals.\n- The singular values of $X$ are $s_{1} \\geq s_{2} \\geq \\dots \\geq s_{p} > 0$.\n- The Euclidean-norm condition number of $X$ is defined as $\\kappa(X) = \\frac{s_{1}}{s_{p}}$.\n- The contrast vector is $c \\in \\mathbb{R}^{p}$ with the constraint $\\|c\\|_{2} = 1$.\n- The task is to derive the maximum-to-minimum ratio of the sampling variance $\\operatorname{Var}(c^\\top \\hat{\\beta})$ over all possible unit-norm contrast vectors $c$.\n- The final answer must be a closed-form expression in terms of $\\kappa(X)$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly rooted in the classical theory of linear statistical models, a cornerstone of biostatistics and numerous scientific disciplines. All concepts—OLS, Gauss-Markov assumptions, SVD, condition numbers, and sampling variance—are standard and mathematically rigorous.\n- **Well-Posed:** The problem is well-defined. It provides all necessary assumptions and definitions to derive the requested ratio. The condition that the columns of $X$ are linearly independent, coupled with $s_p > 0$, ensures that the matrix $X^\\top X$ is invertible, which guarantees a unique OLS estimator $\\hat{\\beta}$. The question asks for a specific, calculable quantity.\n- **Objective:** The problem is stated using formal, objective mathematical language, free of any subjectivity, ambiguity, or opinion.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid** as it is scientifically sound, well-posed, and objective. A solution will be derived.\n\n### Derivation\nThe objective is to find the ratio $\\frac{\\max_{\\|c\\|_{2}=1} \\operatorname{Var}(c^\\top \\hat{\\beta})}{\\min_{\\|c\\|_{2}=1} \\operatorname{Var}(c^\\top \\hat{\\beta})}$. We will derive this from first principles as requested.\n\nFirst, we determine the expression for the OLS estimator $\\hat{\\beta}$. The estimator $\\hat{\\beta}$ is defined as the value of $\\beta$ that minimizes the sum of squared residuals, $S(\\beta)$:\n$$ S(\\beta) = (y - X\\beta)^\\top(y - X\\beta) = y^\\top y - 2y^\\top X\\beta + \\beta^\\top X^\\top X\\beta $$\nTo find the minimum, we compute the gradient of $S(\\beta)$ with respect to $\\beta$ and set it to the zero vector:\n$$ \\nabla_{\\beta} S(\\beta) = -2X^\\top y + 2X^\\top X\\beta = 0 $$\nThis yields the normal equations:\n$$ X^\\top X\\beta = X^\\top y $$\nSince the columns of $X$ are linearly independent, the matrix $X^\\top X$ is a $p \\times p$ positive definite matrix and is therefore invertible. Solving for $\\beta$ gives the unique OLS estimator:\n$$ \\hat{\\beta} = (X^\\top X)^{-1}X^\\top y $$\n\nNext, we derive the sampling variance of $\\hat{\\beta}$. Using the properties of the variance operator and the fact that $\\beta$ is a fixed (non-random) vector:\n$$ \\operatorname{Var}(\\hat{\\beta}) = \\operatorname{Var}((X^\\top X)^{-1}X^\\top y) = (X^\\top X)^{-1}X^\\top \\operatorname{Var}(y) X(X^\\top X)^{-1} $$\nThe variance of $y$ is $\\operatorname{Var}(y) = \\operatorname{Var}(X\\beta + \\varepsilon) = \\operatorname{Var}(\\varepsilon)$. From the problem statement, $\\operatorname{Var}(\\varepsilon) = \\sigma^{2}I_{n}$. So, $\\operatorname{Var}(y) = \\sigma^{2}I_{n}$. Substituting this into the expression for $\\operatorname{Var}(\\hat{\\beta})$:\n$$ \\operatorname{Var}(\\hat{\\beta}) = (X^\\top X)^{-1}X^\\top (\\sigma^{2}I_{n}) X((X^\\top X)^{-1})^\\top $$\nSince $X^\\top X$ is symmetric, its inverse is also symmetric. Thus, $((X^\\top X)^{-1})^\\top = (X^\\top X)^{-1}$.\n$$ \\operatorname{Var}(\\hat{\\beta}) = \\sigma^{2} (X^\\top X)^{-1} X^\\top X (X^\\top X)^{-1} = \\sigma^{2} (X^\\top X)^{-1} $$\n\nNow, we find the variance of the linear combination (contrast) $c^\\top\\hat{\\beta}$. For a constant vector $c$ and a random vector $\\hat{\\beta}$, the variance is given by:\n$$ \\operatorname{Var}(c^\\top\\hat{\\beta}) = c^\\top \\operatorname{Var}(\\hat{\\beta}) c $$\nSubstituting the expression for $\\operatorname{Var}(\\hat{\\beta})$:\n$$ \\operatorname{Var}(c^\\top\\hat{\\beta}) = c^\\top (\\sigma^{2}(X^\\top X)^{-1}) c = \\sigma^{2} c^\\top(X^\\top X)^{-1}c $$\n\nThe problem reduces to finding the maximum and minimum values of the quadratic form $c^\\top(X^\\top X)^{-1}c$ subject to the constraint $\\|c\\|_{2} = 1$, which is equivalent to $c^\\top c=1$. This expression is a Rayleigh quotient for the matrix $(X^\\top X)^{-1}$. The extreme values of a Rayleigh quotient are the maximum and minimum eigenvalues of the matrix.\n\nLet the Singular Value Decomposition (SVD) of $X$ be $X = U S V^\\top$, where $U$ and $V$ are orthogonal matrices and $S$ is an $n \\times p$ rectangular diagonal matrix with the singular values $s_{1}, s_{2}, \\dots, s_{p}$ on its diagonal.\nThe matrix $X^\\top X$ can be expressed in terms of the SVD components:\n$$ X^\\top X = (U S V^\\top)^\\top(U S V^\\top) = V S^\\top U^\\top U S V^\\top $$\nSince $U^\\top U = I_{n}$, this simplifies to:\n$$ X^\\top X = V S^\\top S V^\\top $$\nThe matrix $S^\\top S$ is a $p \\times p$ diagonal matrix with entries $s_{1}^{2}, s_{2}^{2}, \\dots, s_{p}^{2}$. So, $X^\\top X = V \\operatorname{diag}(s_{1}^{2}, s_{2}^{2}, \\dots, s_{p}^{2}) V^\\top$. This is the eigenvalue decomposition of $X^\\top X$. The eigenvalues of $X^\\top X$ are $\\{s_{i}^{2}\\}_{i=1}^{p}$.\n\nThe matrix of interest is $(X^\\top X)^{-1}$. If a matrix has eigenvalues $\\lambda_i$, its inverse has eigenvalues $1/\\lambda_i$. Therefore, the eigenvalues of $(X^\\top X)^{-1}$ are $\\{\\frac{1}{s_{i}^{2}}\\}_{i=1}^{p}$.\n\nGiven the ordering of singular values $s_{1} \\geq s_{2} \\geq \\dots \\geq s_{p} > 0$, the ordering of their squares is $s_{1}^{2} \\geq s_{2}^{2} \\geq \\dots \\geq s_{p}^{2} > 0$. The ordering of their reciprocals is reversed:\n$$ \\frac{1}{s_{p}^{2}} \\geq \\frac{1}{s_{p-1}^{2}} \\geq \\dots \\geq \\frac{1}{s_{1}^{2}} > 0 $$\nThe maximum and minimum eigenvalues of $(X^\\top X)^{-1}$ are:\n$$ \\lambda_{\\max}((X^\\top X)^{-1}) = \\frac{1}{s_{p}^{2}} $$\n$$ \\lambda_{\\min}((X^\\top X)^{-1}) = \\frac{1}{s_{1}^{2}} $$\n\nThe maximum and minimum values of the variance of the contrast are:\n$$ \\max_{\\|c\\|_{2}=1} \\operatorname{Var}(c^\\top \\hat{\\beta}) = \\sigma^{2} \\left( \\max_{\\|c\\|_{2}=1} c^\\top(X^\\top X)^{-1}c \\right) = \\sigma^{2} \\lambda_{\\max}((X^\\top X)^{-1}) = \\frac{\\sigma^{2}}{s_{p}^{2}} $$\n$$ \\min_{\\|c\\|_{2}=1} \\operatorname{Var}(c^\\top \\hat{\\beta}) = \\sigma^{2} \\left( \\min_{\\|c\\|_{2}=1} c^\\top(X^\\top X)^{-1}c \\right) = \\sigma^{2} \\lambda_{\\min}((X^\\top X)^{-1}) = \\frac{\\sigma^{2}}{s_{1}^{2}} $$\n\nFinally, the ratio of the maximum to the minimum variance is:\n$$ \\frac{\\max_{\\|c\\|_{2}=1} \\operatorname{Var}(c^\\top \\hat{\\beta})}{\\min_{\\|c\\|_{2}=1} \\operatorname{Var}(c^\\top \\hat{\\beta})} = \\frac{\\sigma^{2}/s_{p}^{2}}{\\sigma^{2}/s_{1}^{2}} = \\frac{s_{1}^{2}}{s_{p}^{2}} = \\left(\\frac{s_{1}}{s_{p}}\\right)^{2} $$\nBy definition, the condition number is $\\kappa(X) = \\frac{s_{1}}{s_{p}}$. Therefore, the ratio is equal to the square of the condition number.\n$$ \\left(\\frac{s_{1}}{s_{p}}\\right)^{2} = (\\kappa(X))^{2} $$\nThis result quantifies how collinearity, as measured by the condition number of the design matrix, creates anisotropy in the sampling uncertainty of the estimated parameters. The variance of a standardized linear combination of regression coefficients can vary by a factor of $\\kappa(X)^{2}$ depending on the direction of the contrast in parameter space.", "answer": "$$\n\\boxed{\\kappa(X)^{2}}\n$$", "id": "4952774"}, {"introduction": "While multicollinearity describes a global property of the design matrix, influence diagnostics allow us to zoom in on the impact of individual observations. This exercise will guide you through a foundational derivation in regression diagnostics: calculating the exact change in the coefficient vector, $\\hat{\\beta}$, when a single data point is removed. This reveals the precise mathematical roles of an observation's residual and leverage in determining its influence, a concept central to building robust medical models [@problem_id:4952702].", "problem": "Consider a multicenter randomized controlled trial (RCT) in which systolic blood pressure is modeled with a linear predictor under the classical Gauss–Markov conditions: linearity of the conditional mean, full column rank of the design matrix, errors $\\varepsilon$ that are independent and identically distributed with mean $0$ and constant variance $\\sigma^{2}$, and covariates measured without error. Let the design matrix be $X \\in \\mathbb{R}^{n \\times p}$ with $p=3$ columns corresponding to an intercept, centered age, and a binary treatment indicator. The ordinary least squares estimator is defined by $\\hat{\\beta} = (X^\\top X)^{-1}X^\\top y$.\n\nYou delete a single patient $j$ with row vector $x_{j} \\in \\mathbb{R}^{3}$ and observe their residual $r_{j} = y_{j} - x_{j}^\\top\\hat{\\beta}$. Starting from the Gauss–Markov framework and the ordinary least squares normal equations, and using only the rank-one Sherman–Morrison identity to update matrix inverses, derive the exact analytic expression for the change in coefficients due to deleting patient $j$, namely $\\,\\hat{\\beta}_{(j)} - \\hat{\\beta}\\,$, in terms of $x_{j}$, $(X^\\top X)^{-1}$, $r_{j}$, and the leverage $h_{jj} = x_{j}^\\top(X^\\top X)^{-1}x_{j}$. Explain how this expression reveals the roles of leverage and residual in standard influence measures used in medical regression diagnostics.\n\nThen, for a specific site, suppose the following quantities have been estimated on the full sample:\n$$\n(X^\\top X)^{-1} \\;=\\;\n\\begin{pmatrix}\n0.052 & 0 & 0 \\\\\n0 & 0.00225 & 0 \\\\\n0 & 0 & 0.008\n\\end{pmatrix},\n\\qquad\nx_{j} \\;=\\; \\begin{pmatrix} 1 \\\\ -0.3 \\\\ 1 \\end{pmatrix},\n\\qquad\nr_{j} \\;=\\; 2.1.\n$$\nCompute the numerical value of the change in the treatment-effect coefficient (the third component of $\\hat{\\beta}$) caused by deleting patient $j$. The treatment-effect coefficient is measured in millimeters of mercury (mmHg). Round your final numerical answer to four significant figures. Express the final numerical value; do not include units in your boxed answer.", "solution": "The problem requires the derivation of the change in ordinary least squares (OLS) regression coefficients upon the deletion of a single observation, followed by a numerical calculation. The process begins with a formal validation of the problem statement.\n\n### Step 1: Extract Givens\n- **Model**: Linear regression model for systolic blood pressure.\n- **Assumptions**: Classical Gauss–Markov conditions hold.\n- **Design Matrix**: $X \\in \\mathbb{R}^{n \\times p}$ with $p=3$. Columns are intercept, centered age, and a binary treatment indicator.\n- **OLS Estimator**: $\\hat{\\beta} = (X^\\top X)^{-1}X^\\top y$.\n- **Observation deletion**: Patient $j$ with row vector $x_{j} \\in \\mathbb{R}^{3}$ is deleted.\n- **Residual**: $r_{j} = y_{j} - x_{j}^\\top\\hat{\\beta}$.\n- **Leverage**: $h_{jj} = x_{j}^\\top(X^\\top X)^{-1}x_{j}$.\n- **Task 1 (Derivation)**: Derive $\\hat{\\beta}_{(j)} - \\hat{\\beta}$ using the Sherman–Morrison identity, expressed in terms of $x_{j}$, $(X^\\top X)^{-1}$, $r_{j}$, and $h_{jj}$.\n- **Task 2 (Explanation)**: Explain how the derived expression reveals the roles of leverage and residual in influence measures.\n- **Task 3 (Calculation)**: Compute the numerical change in the third coefficient (treatment effect) given the following data:\n$$ (X^\\top X)^{-1} = \\begin{pmatrix} 0.052 & 0 & 0 \\\\ 0 & 0.00225 & 0 \\\\ 0 & 0 & 0.008 \\end{pmatrix}, \\qquad x_{j} = \\begin{pmatrix} 1 \\\\ -0.3 \\\\ 1 \\end{pmatrix}, \\qquad r_{j} = 2.1 $$\n- **Rounding**: Round the final numerical answer to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is set within the standard, well-established framework of linear regression theory and diagnostics. The concepts of OLS, Gauss-Markov conditions, residuals, leverage, and influence diagnostics (like DFBETAS, which this derivation produces) are fundamental to statistical modeling. The application context of a medical trial is appropriate.\n- **Well-Posed**: The problem is clearly stated. It asks for a specific, standard derivation and a subsequent numerical calculation. All necessary information for both parts is provided. A unique, stable, and meaningful solution exists.\n- **Objective**: The problem is expressed in precise mathematical and statistical language, free from ambiguity or subjective content.\n\n### Step 3: Verdict and Action\nThe problem is scientifically sound, well-posed, objective, and complete. It is therefore deemed **valid**. The solution will proceed.\n\n### Derivation of the Change in Coefficients\nLet $X$ and $y$ be the full data matrices. The OLS estimator is the solution to the normal equations: $X^\\top X \\hat{\\beta} = X^\\top y$.\nLet $X_{(j)}$ and $y_{(j)}$ denote the data with the $j$-th observation removed. The new estimator, $\\hat{\\beta}_{(j)}$, is given by:\n$$ \\hat{\\beta}_{(j)} = (X_{(j)}^\\top X_{(j)})^{-1} X_{(j)}^\\top y_{(j)} $$\nThe matrices for the reduced dataset can be expressed in terms of the full-dataset matrices and the deleted observation's data, $x_j$ and $y_j$:\n$$ X^\\top X = \\sum_{i=1}^{n} x_i x_i^\\top = X_{(j)}^\\top X_{(j)} + x_j x_j^\\top \\implies X_{(j)}^\\top X_{(j)} = X^\\top X - x_j x_j^\\top $$\n$$ X^\\top y = \\sum_{i=1}^{n} x_i y_i = X_{(j)}^\\top y_{(j)} + x_j y_j \\implies X_{(j)}^\\top y_{(j)} = X^\\top y - x_j y_j $$\nTo find the inverse of $X_{(j)}^\\top X_{(j)}$, we use the Sherman–Morrison identity for a rank-$1$ update: $(A - uv^\\top)^{-1} = A^{-1} + \\frac{A^{-1}uv^\\top A^{-1}}{1 - v^\\top A^{-1}u}$.\nLet $A = X^\\top X$, $u = x_j$, and $v = x_j$. The identity becomes:\n$$ (X^\\top X - x_j x_j^\\top)^{-1} = (X^\\top X)^{-1} + \\frac{(X^\\top X)^{-1}x_j x_j^\\top(X^\\top X)^{-1}}{1 - x_j^\\top(X^\\top X)^{-1}x_j} $$\nRecognizing the leverage $h_{jj} = x_j^\\top(X^\\top X)^{-1}x_j$, we have:\n$$ (X_{(j)}^\\top X_{(j)})^{-1} = (X^\\top X)^{-1} + \\frac{(X^\\top X)^{-1}x_j x_j^\\top(X^\\top X)^{-1}}{1 - h_{jj}} $$\nNow, we substitute these expressions into the formula for $\\hat{\\beta}_{(j)}$:\n$$ \\hat{\\beta}_{(j)} = \\left[ (X^\\top X)^{-1} + \\frac{(X^\\top X)^{-1}x_j x_j^\\top(X^\\top X)^{-1}}{1 - h_{jj}} \\right] (X^\\top y - x_j y_j) $$\nExpanding the product gives four terms:\n$$ \\hat{\\beta}_{(j)} = (X^\\top X)^{-1}(X^\\top y) - (X^\\top X)^{-1}x_j y_j + \\frac{(X^\\top X)^{-1}x_j x_j^\\top(X^\\top X)^{-1}(X^\\top y)}{1 - h_{jj}} - \\frac{(X^\\top X)^{-1}x_j x_j^\\top(X^\\top X)^{-1}x_j y_j}{1 - h_{jj}} $$\nWe simplify each term:\n1. $(X^\\top X)^{-1}(X^\\top y) = \\hat{\\beta}$\n2. $x_j^\\top(X^\\top X)^{-1}(X^\\top y) = x_j^\\top\\hat{\\beta} = \\hat{y}_j$ (the fitted value for observation $j$)\n3. $x_j^\\top(X^\\top X)^{-1}x_j = h_{jj}$ (the leverage of observation $j$)\n\nSubstituting these back:\n$$ \\hat{\\beta}_{(j)} = \\hat{\\beta} - (X^\\top X)^{-1}x_j y_j + \\frac{(X^\\top X)^{-1}x_j \\hat{y}_j}{1 - h_{jj}} - \\frac{(X^\\top X)^{-1}x_j h_{jj} y_j}{1 - h_{jj}} $$\nWe are interested in the change $\\hat{\\beta}_{(j)} - \\hat{\\beta}$:\n$$ \\hat{\\beta}_{(j)} - \\hat{\\beta} = - (X^\\top X)^{-1}x_j y_j + \\frac{(X^\\top X)^{-1}x_j (\\hat{y}_j - h_{jj} y_j)}{1 - h_{jj}} $$\nFactoring out the common vector $(X^\\top X)^{-1}x_j$:\n$$ \\hat{\\beta}_{(j)} - \\hat{\\beta} = (X^\\top X)^{-1}x_j \\left[ -y_j + \\frac{\\hat{y}_j - h_{jj} y_j}{1 - h_{jj}} \\right] $$\nCombining the terms inside the brackets over a common denominator:\n$$ \\left[ \\frac{-y_j(1 - h_{jj}) + \\hat{y}_j - h_{jj} y_j}{1 - h_{jj}} \\right] = \\left[ \\frac{-y_j + y_j h_{jj} + \\hat{y}_j - h_{jj} y_j}{1 - h_{jj}} \\right] = \\frac{\\hat{y}_j - y_j}{1 - h_{jj}} $$\nThe residual is defined as $r_j = y_j - \\hat{y}_j$, so $\\hat{y}_j - y_j = -r_j$. Thus, the expression in brackets simplifies to $\\frac{-r_j}{1 - h_{jj}}$.\nSubstituting this back yields the final expression for the change in coefficients:\n$$ \\hat{\\beta}_{(j)} - \\hat{\\beta} = -\\frac{(X^\\top X)^{-1}x_j r_j}{1 - h_{jj}} $$\n\n### Explanation of Influence\nThis exact expression, sometimes denoted DFBETA$_j$, quantifies the influence of observation $j$ on the estimated coefficient vector $\\hat{\\beta}$. It reveals that the influence of an observation is a function of two key quantities:\n1.  **The residual ($r_j$)**: The change is directly proportional to the residual $r_j$. An observation is influential only if it is poorly predicted by the model fitted to the full data. A point that lies far from the general trend of the data (large $|r_j|$) has the potential to be influential. If $r_j=0$, the observation has no influence on the coefficients, regardless of its other properties.\n2.  **The leverage ($h_{jj}$)**: The change is amplified by the factor $1/(1 - h_{jj})$. Leverage $h_{jj}$ measures how unusual or extreme an observation's covariate values ($x_j$) are. Since $0 \\le h_{jj} \\le 1$, the denominator $1 - h_{jj}$ is always non-negative. As $h_{jj} \\to 1$, the denominator approaches $0$, and the influence on the coefficients can become arbitrarily large. Therefore, points with high leverage (covariate outliers) have a much greater potential to be influential.\n\nIn summary, an observation is influential if it has a large residual (it is an outlier in the response dimension) and/or high leverage (it is an outlier in the predictor space). The derived formula shows that the greatest influence occurs when an observation has both high leverage and a large residual.\n\n### Numerical Calculation\nWe need to compute the change in the third coefficient (treatment effect), which we denote as $(\\hat{\\beta}_{(j)} - \\hat{\\beta})_3$. The full vector of changes is:\n$$ \\hat{\\beta}_{(j)} - \\hat{\\beta} = -\\frac{r_j}{1 - h_{jj}} (X^\\top X)^{-1}x_j $$\nFirst, we compute the leverage $h_{jj}$:\n$$ h_{jj} = x_{j}^\\top(X^\\top X)^{-1}x_{j} $$\n$$ h_{jj} = \\begin{pmatrix} 1 & -0.3 & 1 \\end{pmatrix} \\begin{pmatrix} 0.052 & 0 & 0 \\\\ 0 & 0.00225 & 0 \\\\ 0 & 0 & 0.008 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -0.3 \\\\ 1 \\end{pmatrix} $$\nSince $(X^\\top X)^{-1}$ is diagonal, the calculation simplifies:\n$$ h_{jj} = (1)^2(0.052) + (-0.3)^2(0.00225) + (1)^2(0.008) $$\n$$ h_{jj} = 0.052 + (0.09)(0.00225) + 0.008 $$\n$$ h_{jj} = 0.052 + 0.0002025 + 0.008 = 0.0602025 $$\nNext, we compute the vector $(X^\\top X)^{-1}x_j$:\n$$ (X^\\top X)^{-1}x_j = \\begin{pmatrix} 0.052 & 0 & 0 \\\\ 0 & 0.00225 & 0 \\\\ 0 & 0 & 0.008 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -0.3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0.052 \\times 1 \\\\ 0.00225 \\times (-0.3) \\\\ 0.008 \\times 1 \\end{pmatrix} = \\begin{pmatrix} 0.052 \\\\ -0.000675 \\\\ 0.008 \\end{pmatrix} $$\nThe third component of this vector is $0.008$.\nNow, we can compute the change in the third coefficient using the given $r_j = 2.1$:\n$$ (\\hat{\\beta}_{(j)} - \\hat{\\beta})_3 = -\\frac{r_j}{1 - h_{jj}} \\times \\left( (X^\\top X)^{-1}x_j \\right)_3 $$\n$$ (\\hat{\\beta}_{(j)} - \\hat{\\beta})_3 = -\\frac{2.1}{1 - 0.0602025} \\times 0.008 $$\n$$ (\\hat{\\beta}_{(j)} - \\hat{\\beta})_3 = -\\frac{2.1 \\times 0.008}{0.9397975} = -\\frac{0.0168}{0.9397975} $$\n$$ (\\hat{\\beta}_{(j)} - \\hat{\\beta})_3 \\approx -0.01787618... $$\nRounding to four significant figures, we get $-0.01788$.", "answer": "$$\\boxed{-0.01788}$$", "id": "4952702"}, {"introduction": "Identifying violations of model assumptions is only half the battle; the other half is implementing and validating a solution. This hands-on coding challenge addresses heteroscedasticity, a common issue where error variance is not constant. You will not only apply Weighted Least Squares (WLS) to correct for this but, more importantly, develop a comprehensive diagnostic suite to verify that your remedy has successfully restored homoscedasticity in the transformed model [@problem_id:4952713].", "problem": "You are asked to formalize a post-fit validation plan for assessing whether Weighted Least Squares (WLS) has achieved approximately homoscedastic residuals in the context of linear regression applied to medical data. Your task is to implement this plan as a complete program that generates synthetic data, fits WLS, computes post-fit diagnostics on the transformed residuals, and decides whether the homoscedasticity assumption is approximately satisfied. Your implementation must be based on fundamental definitions and well-tested formulas, without relying on external files or user input.\n\nFundamental base:\n- A linear model is defined as $y_i = \\mathbf{x}_i^\\top\\boldsymbol{\\beta} + \\varepsilon_i$ for $i = 1,\\dots,n$, where $\\mathbf{x}_i \\in \\mathbb{R}^p$ are known predictors, $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ are unknown coefficients, and $\\varepsilon_i$ are random errors.\n- Homoscedasticity is the assumption that $\\mathrm{Var}(\\varepsilon_i) = \\sigma^2$ is constant across $i$.\n- When errors are heteroscedastic with $\\mathrm{Var}(\\varepsilon_i)=\\sigma^2 v_i$ and $v_i$ known up to scale, Weighted Least Squares (WLS) rescales by $w_i = v_i^{-1/2}$ to obtain the transformed model $y_i^* = w_i y_i$, $\\mathbf{x}_i^* = w_i \\mathbf{x}_i$, for which $\\mathrm{Var}(y_i^* - \\mathbf{x}_i^{*\\top}\\boldsymbol{\\beta}) = \\sigma^2$ holds under correct specification. This is the core justification for WLS.\n\nYour program must:\n1. Generate data under a linear model with potentially heteroscedastic errors, using a specified random seed $s$, sample size $n$, coefficient vector $\\boldsymbol{\\beta} = (\\beta_0,\\beta_1)^\\top$, baseline scale $\\sigma_0$, and a heteroscedastic pattern $v_i = 1 + a |x_i|$ with $x_i \\sim \\mathcal{N}(0,1)$. Errors must be drawn as $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_0^2 v_i)$ independently across $i$. Outcomes are $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$.\n2. Fit a Weighted Least Squares (WLS) model using weights $w_i = (1 + a |x_i|)^{-p}$, where $p$ is specified per test case. Implement WLS by transforming $y_i$ and $\\mathbf{x}_i$ by $w_i$ and fitting Ordinary Least Squares (OLS) to the transformed data using linear algebra. Let $\\widehat{\\boldsymbol{\\beta}}$ denote the coefficient estimates, $\\widehat{y}_i = \\mathbf{x}_i^\\top\\widehat{\\boldsymbol{\\beta}}$ the fitted values, and $e_i = y_i - \\widehat{y}_i$ the raw residuals. Define the transformed residuals $r_i = w_i e_i$. All post-fit diagnostics below must be computed using $\\{r_i\\}$ and $\\{\\widehat{y}_i\\}$.\n3. Compute the following post-fit diagnostics to assess whether $r_i$ are approximately homoscedastic:\n   - Breusch–Pagan test: Regress $r_i^2$ on an intercept and the original predictor $x_i$ to obtain the coefficient of determination $R^2$. Form the Lagrange Multiplier statistic $LM = n R^2$ and compare it to the chi-square distribution with $k$ degrees of freedom, where $k$ is the number of non-intercept regressors (here, $k=1$). Compute the $p$-value using the chi-square distribution. Use significance level $\\alpha = 0.05$.\n   - Rank-based trend test: Compute the Spearman rank correlation between $|\\!r_i\\!|$ and $\\widehat{y}_i$ to test for monotonic scale trends in the residuals with respect to the fitted values. Record the correlation coefficient $\\rho_S$ and its $p$-value. Use a correlation magnitude threshold $\\rho_{\\max} = 0.1$ and significance level $\\alpha = 0.05$.\n   - Binned variance ratio: Partition the fitted values $\\{\\widehat{y}_i\\}$ into quartiles. Compute the mean of $r_i^2$ in the lowest-quartile bin and the highest-quartile bin, say $\\bar{v}_{\\mathrm{low}}$ and $\\bar{v}_{\\mathrm{high}}$, and form the ratio $Q = \\bar{v}_{\\mathrm{high}}/\\bar{v}_{\\mathrm{low}}$. Use acceptance bounds $Q \\in [0.7, 1.4]$.\n4. Decision rule: Declare that WLS has achieved approximately homoscedastic residuals if and only if all of the following hold simultaneously: the Breusch–Pagan $p$-value is at least $\\alpha$, the Spearman $p$-value is at least $\\alpha$ and $|\\rho_S| \\le \\rho_{\\max}$, and the binned variance ratio $Q$ lies in the interval $[0.7, 1.4]$.\n\nAngle units are not involved. No physical units are involved. All probabilities must be expressed as decimals. All thresholds are given explicitly above.\n\nTest suite:\nImplement your program to run the following four test cases, each defined by $(n, s, \\beta_0, \\beta_1, \\sigma_0, a, p)$:\n- Case $1$ (happy path, correctly specified WLS on strongly heteroscedastic data): $(n, s, \\beta_0, \\beta_1, \\sigma_0, a, p) = (800, 202311, 1.0, 2.0, 1.0, 4.0, 0.5)$.\n- Case $2$ (heteroscedastic data, unweighted OLS as a negative control): $(800, 202311, 1.0, 2.0, 1.0, 4.0, 0.0)$.\n- Case $3$ (heteroscedastic data, mis-specified overweighting): $(800, 202311, 1.0, 2.0, 1.0, 4.0, 1.0)$.\n- Case $4$ (homoscedastic data, OLS baseline): $(600, 909, 0.5, -1.0, 1.0, 0.0, 0.0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with lowercase boolean literals, in the order of the cases above. For example, if the first, second, and fourth cases satisfy the decision rule and the third does not, the output must be exactly of the form: [true,false,false,true].", "solution": "The problem is valid. It presents a well-defined task in statistical modeling: to implement a programmatic validation plan for assessing the homoscedasticity assumption in a linear model after applying Weighted Least Squares (WLS). The problem is scientifically grounded in established statistical theory, provides all necessary parameters and definitions, and outlines a clear, objective decision-making process.\n\nThe solution is a step-by-step implementation of the specified validation plan. The core principle of WLS is to counteract heteroscedasticity—non-constant variance of errors—by applying weights to each observation that are inversely proportional to the standard deviation of its error term. If the weights are correctly specified, the transformed model will exhibit homoscedastic residuals, a key assumption for the validity of subsequent statistical inference. The following procedure formalizes this process.\n\nStep 1: Data Generation\nFor each test case, we generate synthetic data according to the specified linear model, $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$.\n- A vector of $n$ predictors, $x_i$, is drawn from a standard normal distribution, $x_i \\sim \\mathcal{N}(0, 1)$. The design matrix is $\\mathbf{X} \\in \\mathbb{R}^{n \\times 2}$, with the first column being all $1$s (for the intercept $\\beta_0$) and the second column containing the $x_i$ values.\n- The error terms, $\\varepsilon_i$, are generated from a normal distribution with mean $0$ and non-constant variance, $\\mathrm{Var}(\\varepsilon_i) = \\sigma_0^2 v_i$, where the heteroscedastic pattern is given by $v_i = 1 + a |x_i|$. The errors are thus drawn as $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_0^2 (1 + a |x_i|))$.\n- The response variable, $y_i$, is computed as the sum of the deterministic part, $\\beta_0 + \\beta_1 x_i$, and the stochastic error, $\\varepsilon_i$.\n\nStep 2: Weighted Least Squares (WLS) Estimation\nThe WLS procedure aims to find the coefficient vector $\\widehat{\\boldsymbol{\\beta}}$ that minimizes the weighted sum of squared residuals, $\\sum_{i=1}^n w_i^2 (y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2$. This is equivalent to performing an Ordinary Least Squares (OLS) regression on a transformed model.\n- The weights are defined as $w_i = (1 + a |x_i|)^{-p}$. The parameter $p$ controls the strength of the weighting. The theoretically optimal choice to counteract the specified heteroscedasticity is $p=0.5$, since this makes $w_i \\propto v_i^{-1/2}$.\n- The data are transformed: the transformed predictors are $\\mathbf{x}_i^* = w_i \\mathbf{x}_i$ and the transformed response is $y_i^* = w_i y_i$. In matrix notation, $\\mathbf{X}^* = \\mathbf{W}\\mathbf{X}$ and $\\mathbf{y}^* = \\mathbf{W}\\mathbf{y}$, where $\\mathbf{W}$ is a diagonal matrix with entries $w_i$.\n- The WLS estimate $\\widehat{\\boldsymbol{\\beta}}$ is obtained by applying the OLS formula to the transformed data:\n$$ \\widehat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{*\\top} \\mathbf{X}^*)^{-1} \\mathbf{X}^{*\\top} \\mathbf{y}^* $$\n- From $\\widehat{\\boldsymbol{\\beta}}$, we compute the fitted values on the original scale, $\\widehat{y}_i = \\mathbf{x}_i^\\top\\widehat{\\boldsymbol{\\beta}}$, and the raw residuals, $e_i = y_i - \\widehat{y}_i$. The key objects for diagnostics are the transformed residuals, $r_i = w_i e_i$, which are the residuals from the transformed model fit. If WLS is successful, the set $\\{r_i\\}$ should be approximately homoscedastic.\n\nStep 3: Post-Fit Diagnostic Tests\nWe perform three tests on the transformed residuals $\\{r_i\\}$ to assess whether homoscedasticity has been achieved.\n\n- **Breusch–Pagan Test**: This test checks for a specific form of heteroscedasticity where the residual variance is linearly related to one or more predictors. We regress the squared transformed residuals, $r_i^2$, on an intercept and the original predictor, $x_i$: $r_i^2 = \\gamma_0 + \\gamma_1 x_i + u_i$. The coefficient of determination, $R^2$, from this auxiliary regression is used to form the Lagrange Multiplier statistic, $LM = n R^2$. Under the null hypothesis of homoscedasticity, $LM$ follows a chi-square distribution with $k=1$ degree of freedom (since there is one regressor, $x_i$). A $p$-value is computed from this $\\chi^2_1$ distribution. We conclude homoscedasticity is plausible if the $p$-value is greater than or equal to the significance level $\\alpha=0.05$.\n\n- **Rank-based Trend Test**: This test detects monotonic trends in the magnitude of residuals with respect to the fitted values, which is another sign of heteroscedasticity. We compute the Spearman rank correlation coefficient, $\\rho_S$, between the absolute transformed residuals, $|\\!r_i\\!|$, and the fitted values, $\\widehat{y}_i$. The test is considered passed if two conditions are met: the correlation is not statistically significant (i.e., its $p$-value is at least $\\alpha=0.05$) and its magnitude is small ($|\\rho_S| \\le \\rho_{\\max}=0.1$).\n\n- **Binned Variance Ratio**: This is a direct, heuristic check for variance trends. The data are sorted by their fitted values, $\\widehat{y}_i$, and partitioned into quartiles. We compute the mean of the squared transformed residuals, $\\mathrm{mean}(r_i^2)$, for observations in the lowest quartile ($\\bar{v}_{\\mathrm{low}}$) and the highest quartile ($\\bar{v}_{\\mathrm{high}}$). The ratio $Q = \\bar{v}_{\\mathrm{high}} / \\bar{v}_{\\mathrm{low}}$ is formed. If the residuals are homoscedastic, this ratio should be close to $1$. The test is passed if $Q$ falls within the acceptance interval $[0.7, 1.4]$.\n\nStep 4: Final Decision\nA test case is deemed to have successfully achieved approximately homoscedastic residuals if and only if all three diagnostic tests pass simultaneously. The final output is a boolean value (`true` or `false`) indicating this outcome for each case in the test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2, spearmanr\n\ndef run_wls_validation(n, s, beta0, beta1, sigma0, a, p):\n    \"\"\"\n    Runs the full WLS validation plan for a single test case.\n    \n    Args:\n        n (int): Sample size.\n        s (int): Random seed.\n        beta0 (float): True intercept.\n        beta1 (float): True slope.\n        sigma0 (float): Baseline error scale.\n        a (float): Heteroscedasticity parameter.\n        p (float): Weighting power for WLS.\n\n    Returns:\n        bool: True if all homoscedasticity checks pass, False otherwise.\n    \"\"\"\n    # 1. Generate Data\n    rng = np.random.default_rng(s)\n    \n    # Generate predictors x_i ~ N(0,1)\n    x = rng.normal(loc=0.0, scale=1.0, size=n)\n    \n    # Construct design matrix X = [1, x]\n    X = np.c_[np.ones(n), x]\n    \n    # Define true heteroscedastic variance pattern v_i = 1 + a*|x_i|\n    v = 1.0 + a * np.abs(x)\n    \n    # Generate heteroscedastic errors e_i ~ N(0, sigma0^2 * v_i)\n    error_std_devs = sigma0 * np.sqrt(v)\n    epsilon = rng.normal(loc=0.0, scale=error_std_devs, size=n)\n    \n    # Generate outcome y_i = beta0 + beta1*x_i + epsilon_i\n    beta_true = np.array([beta0, beta1])\n    y = X @ beta_true + epsilon\n    \n    # 2. Fit Weighted Least Squares (WLS)\n    # Define weights w_i = (1 + a*|x_i|)^(-p)\n    weights = (1.0 + a * np.abs(x)) ** (-p)\n    \n    # Transform data: X* = WX, y* = Wy\n    X_star = weights[:, np.newaxis] * X\n    y_star = weights * y\n    \n    # Compute WLS coefficients via OLS on transformed data\n    # beta_hat = (X*^T X*)^-1 X*^T y*\n    try:\n        beta_hat = np.linalg.inv(X_star.T @ X_star) @ X_star.T @ y_star\n    except np.linalg.LinAlgError:\n        # Failsafe for singular matrix, unlikely here.\n        return False\n\n    # Calculate fitted values on original scale and transformed residuals\n    y_hat = X @ beta_hat\n    raw_residuals = y - y_hat\n    transformed_residuals = weights * raw_residuals\n    \n    # 3. Perform Post-fit Diagnostics on transformed residuals\n    \n    # 3a. Breusch-Pagan Test\n    r_sq = transformed_residuals**2\n    # Auxiliary regression: r_sq on an intercept and x\n    Z_bp = np.c_[np.ones(n), x]\n    \n    try:\n        # beta_bp = (Z_bp^T Z_bp)^-1 Z_bp^T r_sq\n        gamma_hat = np.linalg.inv(Z_bp.T @ Z_bp) @ Z_bp.T @ r_sq\n    except np.linalg.LinAlgError:\n        return False\n        \n    r_sq_hat = Z_bp @ gamma_hat\n    ss_res_bp = np.sum((r_sq - r_sq_hat)**2)\n    ss_tot_bp = np.sum((r_sq - np.mean(r_sq))**2)\n    \n    R2_bp = 1.0 - ss_res_bp / ss_tot_bp if ss_tot_bp > 0 else 0.0\n        \n    LM_statistic = n * R2_bp\n    bp_p_value = 1.0 - chi2.cdf(LM_statistic, df=1)\n    bp_pass = bp_p_value >= 0.05\n\n    # 3b. Rank-based Trend Test (Spearman)\n    spearman_result = spearmanr(np.abs(transformed_residuals), y_hat)\n    rho_S = spearman_result.correlation\n    spearman_p_value = spearman_result.pvalue\n    spearman_pass = (spearman_p_value >= 0.05) and (np.abs(rho_S) <= 0.1)\n\n    # 3c. Binned Variance Ratio\n    q1 = np.percentile(y_hat, 25)\n    q3 = np.percentile(y_hat, 75)\n    \n    low_quartile_mask = y_hat <= q1\n    high_quartile_mask = y_hat >= q3\n    \n    # Failsafe for empty bins\n    if not np.any(low_quartile_mask) or not np.any(high_quartile_mask):\n        return False\n        \n    v_low = np.mean(r_sq[low_quartile_mask])\n\n    # Failsafe for division by zero\n    if v_low == 0:\n        return False\n\n    v_high = np.mean(r_sq[high_quartile_mask])\n    Q = v_high / v_low\n    binned_pass = (Q >= 0.7) and (Q <= 1.4)\n\n    # 4. Final Decision Rule\n    return bp_pass and spearman_pass and binned_pass\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Correctly specified WLS\n        (800, 202311, 1.0, 2.0, 1.0, 4.0, 0.5),\n        # Case 2: OLS on heteroscedastic data (negative control)\n        (800, 202311, 1.0, 2.0, 1.0, 4.0, 0.0),\n        # Case 3: Mis-specified WLS (overweighting)\n        (800, 202311, 1.0, 2.0, 1.0, 4.0, 1.0),\n        # Case 4: OLS on homoscedastic data (baseline)\n        (600, 909, 0.5, -1.0, 1.0, 0.0, 0.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, s, beta0, beta1, sigma0, a, p = case\n        result = run_wls_validation(n, s, beta0, beta1, sigma0, a, p)\n        results.append(str(result).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4952713"}]}