{"hands_on_practices": [{"introduction": "When assessing a linear model, raw residuals, the simple differences between observed and predicted values, can be misleading. Their variances are not equal, depending on the leverage of each data point. This practice [@problem_id:4982808] guides you through the process of calculating standardized residuals, which account for this non-constant variance. By scaling each residual by its estimated standard error, we create a set of diagnostics that are more comparable and whose magnitudes can be more reliably interpreted for identifying potential outliers.", "problem": "Consider a cohort study in clinical epidemiology where the biomarker C-reactive protein (CRP) is measured in milligrams per liter and then log-transformed to stabilize variance. Denote the log-transformed CRP by $y_i$, the age in years by $x_i$, and the smoking status indicator by $z_i$, where $z_i \\in \\{0,1\\}$ with $z_i=1$ for a current smoker and $z_i=0$ otherwise. We consider a linear model for $i=1,\\dots,n$ defined by $y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i + \\varepsilon_i$ where the errors $\\varepsilon_i$ are assumed to be independent and identically distributed as Gaussian with mean $0$ and variance $\\sigma^2$. Using ordinary least squares (OLS), the fitted values and residuals are obtained from the data and the diagonal elements $h_{ii}$ of the hat matrix quantify leverage. Standardized residuals $r_i$ are used in regression diagnostics to assess model adequacy and outliers, and numerical thresholds such as $|r_i|2$ are commonly interpreted in terms of approximate tail probabilities under a standard normal reference.\n\nFrom first principles of the Gauss–Markov assumptions and the properties of the OLS estimator, implement an algorithm that:\n- Fits the linear model to obtain $\\hat{\\beta}$ using the design matrix $X$ with columns for an intercept, age $x_i$, and smoking indicator $z_i$.\n- Computes residuals $e_i$ and the hat matrix $H$ to obtain $h_{ii}$.\n- Estimates $\\sigma^2$ by the residual mean square $s^2$ using $n-p$ degrees of freedom, where $p$ is the number of regression coefficients including the intercept (here $p=3$).\n- Computes the standardized residuals $r_i$ for all observations.\n- Interprets the threshold $|r_i|2$ by reporting the approximate two-sided tail probability under a standard normal reference, $2\\{1 - \\Phi(2)\\}$, where $\\Phi(\\cdot)$ is the standard normal cumulative distribution function. Additionally, report the approximate two-sided tail probabilities $p_i = 2\\Phi(-|r_i|)$ for each residual.\n\nAll ages $x_i$ must be treated in units of years. The outputs are unitless because $y_i$ is a logarithm.\n\nYour program must apply the above to the following test suite of three datasets, each specified as triplets $(\\{x_i\\},\\{z_i\\},\\{y_i\\})$:\n\n- Test case $1$ (typical variability, $n=10$, $p=3$):\n  - Ages $\\{x_i\\}$: $[22,34,47,51,36,63,41,55,29,68]$ (years)\n  - Smoking indicators $\\{z_i\\}$: $[0,1,0,1,0,1,0,1,0,1]$\n  - Log-CRP $\\{y_i\\}$: $[0.79,1.35,1.24,1.59,1.15,1.85,1.26,1.80,0.96,1.97]$\n\n- Test case $2$ (one high-leverage outlier, $n=8$, $p=3$):\n  - Ages $\\{x_i\\}$: $[25,40,45,50,55,60,65,95]$ (years)\n  - Smoking indicators $\\{z_i\\}$: $[0,0,1,0,1,0,1,1]$\n  - Log-CRP $\\{y_i\\}$: $[0.93,1.16,1.61,1.41,1.70,1.62,1.95,0.80]$\n\n- Test case $3$ (near-boundary small sample, $n=4$, $p=3$):\n  - Ages $\\{x_i\\}$: $[30,50,70,60]$ (years)\n  - Smoking indicators $\\{z_i\\}$: $[0,1,0,1]$\n  - Log-CRP $\\{y_i\\}$: $[1.01,1.64,1.78,1.86]$\n\nFor each test case, produce:\n- The list of standardized residuals $[r_1,\\dots,r_n]$ rounded to $6$ decimal places.\n- The list of approximate two-sided tail probabilities $[p_1,\\dots,p_n]$ where $p_i = 2\\Phi(-|r_i|)$, rounded to $6$ decimal places.\n- The integer count $c$ of residuals with $|r_i|2$.\n- The float $\\pi_2 = 2\\{1 - \\Phi(2)\\}$, rounded to $6$ decimal places.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list in the order specified above. For example, the overall format is $[[\\text{case1\\_residuals},\\text{case1\\_pvals},c_1,\\pi_2],[\\text{case2\\_residuals},\\text{case2\\_pvals},c_2,\\pi_2],[\\text{case3\\_residuals},\\text{case3\\_pvals},c_3,\\pi_2]]$. All probabilities must be expressed as decimals, not with a percentage sign.", "solution": "The user has provided a problem in the domain of statistical regression diagnostics, which requires the implementation of an algorithm to compute and interpret standardized residuals for a multiple linear regression model. The problem statement has been critically validated and is determined to be valid.\n\n### Validation Verdict:\nThe problem is **valid**.\n- **Scientifically Grounded**: The problem is based on the classical theory of linear models, ordinary least squares (OLS) estimation, and standard regression diagnostics. All concepts, including the Gauss-Markov model, hat matrix, leverage, and standardized residuals, are fundamental to statistics.\n- **Well-Posed**: The problem is clearly defined with all necessary data and mathematical formulas either provided or standard in the field. The OLS solution exists and is unique for the provided datasets as the design matrices have full column rank ($np$ and no perfect collinearity).\n- **Objective**: The problem is stated using precise and unambiguous mathematical language, free of subjective claims.\n- **Complete and Consistent**: All required data for the three test cases are provided, and the steps for the algorithm are explicitly outlined. The definition of the variance estimator $s^2$ and its degrees of freedom are consistent with standard OLS theory. The definition of the standardized residual is implicitly the standard one, $r_i = e_i / (s \\sqrt{1-h_{ii}})$, which is appropriate.\n\nThe problem is a straightforward application of established statistical methods and is fully specified, allowing for a unique and verifiable solution.\n\n### Solution Derivations\nThe solution will be implemented by following the algorithmic steps outlined in the problem statement, which are based on the principles of ordinary least squares (OLS) regression.\n\nFor each dataset comprising $n$ observations of $(y_i, x_i, z_i)$, we define the model as:\n$y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i + \\varepsilon_i$, with $\\varepsilon_i \\sim \\text{i.i.d. } N(0, \\sigma^2)$.\n\nThis can be expressed in matrix form as $y = X\\beta + \\varepsilon$, where:\n- $y$ is the $n \\times 1$ response vector $\\{y_i\\}$.\n- $X$ is the $n \\times p$ design matrix, with $p=3$. The columns of $X$ are a vector of ones (for the intercept $\\beta_0$), the vector of ages $\\{x_i\\}$, and the vector of smoking indicators $\\{z_i\\}$.\n- $\\beta = (\\beta_0, \\beta_1, \\beta_2)^T$ is the vector of coefficients.\n- $\\varepsilon$ is the vector of error terms.\n\nThe algorithm proceeds as follows:\n\n**1. OLS Estimation of $\\beta$**:\nThe OLS estimator $\\hat{\\beta}$ minimizes the residual sum of squares $RSS = \\sum e_i^2 = (y - X\\beta)^T(y - X\\beta)$. The solution is given by the normal equations:\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n$$\n\n**2. Computation of Residuals and Hat Matrix**:\n- The fitted values are calculated as $\\hat{y} = X\\hat{\\beta}$.\n- The raw residuals are the differences between observed and fitted values: $e = y - \\hat{y}$.\n- The hat matrix, $H$, projects the observed values $y$ onto the space spanned by the columns of $X$ to produce the fitted values: $\\hat{y} = Hy$. It is defined as:\n$$\nH = X(X^T X)^{-1} X^T\n$$\nThe diagonal elements of the hat matrix, $h_{ii}$, are the leverage scores for each observation. They satisfy $0 \\le h_{ii} \\le 1$ and $\\sum_{i=1}^n h_{ii} = p$.\n\n**3. Estimation of Error Variance $\\sigma^2$**:\nThe unknown error variance $\\sigma^2$ is estimated by the residual mean square, $s^2$:\n$$\ns^2 = \\frac{RSS}{n-p} = \\frac{e^T e}{n-p}\n$$\nwhere $n-p$ are the residual degrees of freedom. The estimator for the standard deviation of the error is $s = \\sqrt{s^2}$.\n\n**4. Computation of Standardized Residuals**:\nThe variance of the $i$-th raw residual $e_i$ is $\\text{Var}(e_i) = \\sigma^2(1-h_{ii})$. The standardized residual, $r_i$, is the raw residual scaled by its estimated standard deviation:\n$$\nr_i = \\frac{e_i}{s \\sqrt{1 - h_{ii}}}\n$$\nUnder the model assumptions, each $r_i$ has a mean of approximately $0$ and a variance of approximately $1$. For large $n$, their distribution is well-approximated by the standard normal distribution, $N(0, 1)$.\n\n**5. Calculation of Probabilities**:\n- The reference probability for the threshold $|r_i|  2$ is computed under the standard normal approximation:\n$$\n\\pi_2 = P(|Z|  2) = 2 \\cdot P(Z  -2) = 2 \\Phi(-2)\n$$\nwhere $Z \\sim N(0,1)$ and $\\Phi(\\cdot)$ is the standard normal cumulative distribution function (CDF).\n- For each standardized residual $r_i$, the corresponding approximate two-sided tail probability is:\n$$\np_i = P(|Z|  |r_i|) = 2 \\Phi(-|r_i|)\n$$\nThe standard normal CDF is computed using its relation to the error function, $\\text{erf}(x)$, available in `scipy.special`:\n$$\n\\Phi(z) = \\frac{1}{2} \\left(1 + \\text{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right)\n$$\nThis leads to the expression for the two-sided tail probability:\n$$\np_i = 2(1 - \\Phi(|r_i|)) = 2\\left(1 - \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{|r_i|}{\\sqrt{2}}\\right)\\right)\\right) = 1 - \\text{erf}\\left(\\frac{|r_i|}{\\sqrt{2}}\\right)\n$$\nA similar formula holds for $\\pi_2$. The implementation will use this `erf`-based calculation.\n\nThe final step is to count the number of residuals $c$ for which $|r_i|  2$ and assemble all results into the specified output format. This entire procedure will be applied to each of the three test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final result.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (typical variability, n=10, p=3)\n        (\n            [22, 34, 47, 51, 36, 63, 41, 55, 29, 68],  # Ages {x_i}\n            [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],          # Smoking indicators {z_i}\n            [0.79, 1.35, 1.24, 1.59, 1.15, 1.85, 1.26, 1.80, 0.96, 1.97] # Log-CRP {y_i}\n        ),\n        # Test case 2 (one high-leverage outlier, n=8, p=3)\n        (\n            [25, 40, 45, 50, 55, 60, 65, 95],         # Ages {x_i}\n            [0, 0, 1, 0, 1, 0, 1, 1],                 # Smoking indicators {z_i}\n            [0.93, 1.16, 1.61, 1.41, 1.70, 1.62, 1.95, 0.80] # Log-CRP {y_i}\n        ),\n        # Test case 3 (near-boundary small sample, n=4, p=3)\n        (\n            [30, 50, 70, 60],                         # Ages {x_i}\n            [0, 1, 0, 1],                             # Smoking indicators {z_i}\n            [1.01, 1.64, 1.78, 1.86]                  # Log-CRP {y_i}\n        )\n    ]\n\n    all_results = []\n    for case_data in test_cases:\n        result = process_case(case_data)\n        all_results.append(result)\n\n    # Format the final output string exactly as specified.\n    # The str() of a list automatically includes spaces, e.g., '[1, 2]'.\n    # To get a compact representation, we can build the string manually.\n    result_strings = []\n    for res in all_results:\n        r_list_str = f\"[{','.join(map(str, res[0]))}]\"\n        p_list_str = f\"[{','.join(map(str, res[1]))}]\"\n        c_str = str(res[2])\n        pi2_str = str(res[3])\n        result_strings.append(f\"[{r_list_str},{p_list_str},{c_str},{pi2_str}]\")\n    \n    # The problem example implies that str(list) is fine. Let's use the simpler approach.\n    print(str(all_results).replace(\" \", \"\"))\n\n\ndef process_case(case_data):\n    \"\"\"\n    Implements OLS regression and diagnostics for a single dataset.\n\n    Args:\n        case_data (tuple): A tuple containing lists for ages, smoking indicators,\n                           and log-CRP values.\n\n    Returns:\n        list: A list containing [standardized_residuals, tail_probabilities, count, pi_2].\n    \"\"\"\n    x_i, z_i, y_i = case_data\n    \n    # Convert input lists to numpy arrays for vector/matrix operations.\n    x_vec = np.array(x_i)\n    z_vec = np.array(z_i)\n    y_vec = np.array(y_i)\n    \n    n = len(y_vec)  # Number of observations\n    p = 3           # Number of parameters (beta_0, beta_1, beta_2)\n    \n    # Construct the n x p design matrix X.\n    X = np.ones((n, p))\n    X[:, 1] = x_vec\n    X[:, 2] = z_vec\n    \n    # Step 1: Fit the linear model using OLS to get beta_hat.\n    # beta_hat = (X'X)^-1 * X'y\n    XTX = X.T @ X\n    XTX_inv = np.linalg.inv(XTX)\n    XTY = X.T @ y_vec\n    beta_hat = XTX_inv @ XTY\n    \n    # Step 2: Compute residuals and hat matrix.\n    # Fitted values: y_hat = X * beta_hat\n    y_hat = X @ beta_hat\n    # Raw residuals: e = y - y_hat\n    residuals = y_vec - y_hat\n    # Hat matrix: H = X * (X'X)^-1 * X'\n    hat_matrix = X @ XTX_inv @ X.T\n    # Leverage values (diagonal of H): h_ii\n    leverages = np.diag(hat_matrix)\n    \n    # Step 3: Estimate error variance sigma^2.\n    # Residual Sum of Squares (RSS)\n    rss = residuals.T @ residuals\n    # Degrees of freedom for error\n    df = n - p\n    # Residual Mean Square (s^2)\n    s_squared = rss / df\n    # Residual standard error (s)\n    s = np.sqrt(s_squared)\n    \n    # Step 4: Compute standardized residuals.\n    # r_i = e_i / (s * sqrt(1 - h_ii))\n    denom = s * np.sqrt(1 - leverages)\n    # Avoid division by zero in case of h_ii=1 (not expected here)\n    # Adding a small epsilon would be robust, but not necessary for these test cases.\n    standardized_residuals = residuals / denom\n    \n    # Step 5  6: Compute tail probabilities and count.\n    # The two-sided tail probability is P(|Z|  |x|) = 2 * (1 - Phi(|x|))\n    # which simplifies to 1 - erf(|x|/sqrt(2)).\n    \n    # For the threshold |r_i|  2\n    pi_2 = 1.0 - erf(2.0 / np.sqrt(2.0))\n    \n    # For each standardized residual\n    tail_probabilities = 1.0 - erf(np.abs(standardized_residuals) / np.sqrt(2.0))\n    \n    # Count of residuals with absolute value greater than 2.\n    count_gt_2 = np.sum(np.abs(standardized_residuals)  2)\n\n    # Prepare the output lists and values, rounded to 6 decimal places.\n    r_list = np.round(standardized_residuals, 6).tolist()\n    p_vals_list = np.round(tail_probabilities, 6).tolist()\n    \n    return [r_list, p_vals_list, int(count_gt_2), round(pi_2, 6)]\n\n# Execute the solver\nsolve()\n```", "id": "4982808"}, {"introduction": "Moving from linear models to Generalized Linear Models (GLMs) like logistic regression requires a more sophisticated concept of residuals. The simple residual $y_i - \\hat{\\pi}_i$ is no longer ideal, as the outcome is binary and the variance depends on the mean. This exercise [@problem_id:4982785] introduces deviance residuals, which are derived from the log-likelihood function. Each deviance residual quantifies an observation's contribution to the model's total deviance, offering a powerful way to identify points that are poorly fitted by the model.", "problem": "You are modeling in-hospital mortality in sepsis as a binary outcome using logistic regression with predictors age and Sequential Organ Failure Assessment (SOFA) score. Let each observation be indexed by $i \\in \\{1,\\dots,n\\}$ with binary outcome $y_i \\in \\{0,1\\}$. The regression model assumes that $Y_i \\mid \\mathbf{x}_i \\sim \\mathrm{Bernoulli}(\\pi_i)$ with $\\pi_i = \\mathbb{P}(Y_i = 1 \\mid \\mathbf{x}_i)$ and the canonical link $\\mathrm{logit}(\\pi_i) = \\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$, where $\\mathbf{x}_i = (1, \\mathrm{age}_i, \\mathrm{sofa}_i)^\\top$ and $\\boldsymbol{\\beta} = (\\beta_0,\\beta_1,\\beta_2)^\\top$.\n\nStarting only from the following foundational elements:\n- The Bernoulli likelihood for a single observation: $L_i(\\pi_i \\mid y_i) = \\pi_i^{y_i} (1-\\pi_i)^{1-y_i}$ and its log-likelihood $\\ell_i(\\pi_i \\mid y_i) = y_i \\log(\\pi_i) + (1-y_i)\\log(1-\\pi_i)$.\n- The definition of the deviance for a generalized linear model (GLM) as twice the difference between the log-likelihood of the saturated model (a model that fits each $y_i$ exactly) and that of the fitted model, aggregated over observations.\n\nYour tasks are:\n1) Derive from first principles an explicit closed-form expression for the individual deviance residual $r_i$ for logistic regression, expressed as a signed square root of the per-observation deviance contribution in terms of $y_i$ and $\\hat{\\pi}_i$, where $\\hat{\\pi}_i = \\mathrm{logistic}(\\hat{\\eta}_i)$ and $\\hat{\\eta}_i = \\mathbf{x}_i^\\top \\hat{\\boldsymbol{\\beta}}$.\n2) Then, for a fixed and known coefficient vector $\\hat{\\boldsymbol{\\beta}} = (\\hat{\\beta}_0,\\hat{\\beta}_1,\\hat{\\beta}_2) = (-7.0, 0.04, 0.35)$, compute the deviance residuals for each observation in the test suites below. Use age measured in years and SOFA as a unitless score. The residuals are dimensionless; report the residuals rounded to six decimal places.\n\nImplementation instructions and numerical safety requirements:\n- Use the logistic function $\\mathrm{logistic}(z) = \\dfrac{1}{1+\\exp(-z)}$ to compute $\\hat{\\pi}_i$ from $\\hat{\\eta}_i$.\n- Ensure numerical stability so that no logarithm is evaluated at $0$. You may use any mathematically equivalent stable form for binary $y_i \\in \\{0,1\\}$ to avoid undefined terms while preserving the exact deviance residual definition.\n- For each test case, compute and return the list of residuals in the order the observations are listed and round each to six decimal places.\n\nTest suite:\n- Test case $1$ (general mixed-risk cohort, $5$ observations):\n  - Observation $1$: age $65$ years, SOFA $6$, outcome $y=1$.\n  - Observation $2$: age $72$ years, SOFA $10$, outcome $y=1$.\n  - Observation $3$: age $50$ years, SOFA $4$, outcome $y=0$.\n  - Observation $4$: age $34$ years, SOFA $2$, outcome $y=0$.\n  - Observation $5$: age $80$ years, SOFA $12$, outcome $y=1$.\n- Test case $2$ (near-boundary probabilities to probe stability, $5$ observations):\n  - Observation $1$: age $22$ years, SOFA $0$, outcome $y=1$.\n  - Observation $2$: age $90$ years, SOFA $18$, outcome $y=0$.\n  - Observation $3$: age $85$ years, SOFA $20$, outcome $y=1$.\n  - Observation $4$: age $25$ years, SOFA $1$, outcome $y=0$.\n  - Observation $5$: age $78$ years, SOFA $22$, outcome $y=1$.\n- Test case $3$ (duplicate covariates with discordant outcomes, $4$ observations):\n  - Observation $1$: age $60$ years, SOFA $8$, outcome $y=0$.\n  - Observation $2$: age $60$ years, SOFA $8$, outcome $y=1$.\n  - Observation $3$: age $40$ years, SOFA $5$, outcome $y=0$.\n  - Observation $4$: age $40$ years, SOFA $5$, outcome $y=1$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the three test-case result lists, enclosed in a single pair of square brackets. For example, the formatting should look like $[[a_{11},\\dots,a_{1m_1}],[a_{21},\\dots,a_{2m_2}],[a_{31},\\dots,a_{3m_3}]]$, where each $a_{jk}$ is a float rounded to six decimal places. No additional text should be printed.", "solution": "The user-provided problem is assessed as valid. It is scientifically grounded in the theory of generalized linear models (GLMs), well-posed with all necessary data and parameters provided, and objectively stated. The task involves a standard derivation and application of deviance residuals in logistic regression, a core concept in statistical modeling.\n\n### **Part 1: Derivation of the Deviance Residual for Logistic Regression**\n\nThe deviance of a GLM is defined as $D = 2 \\sum_{i=1}^n [\\ell(\\boldsymbol{\\beta}_{sat}; y_i) - \\ell(\\hat{\\boldsymbol{\\beta}}; y_i)]$, where $\\ell(\\cdot)$ is the log-likelihood function, $\\hat{\\boldsymbol{\\beta}}$ is the maximum likelihood estimate of the model parameters, and $\\boldsymbol{\\beta}_{sat}$ represents the parameters of a *saturated* model. A saturated model is one that fits the data perfectly, having one parameter for each observation. The total deviance $D$ is the sum of individual deviance contributions, $d_i = 2 [\\ell(\\boldsymbol{\\beta}_{sat}; y_i) - \\ell(\\hat{\\boldsymbol{\\beta}}; y_i)]$.\n\nThe deviance residual for observation $i$, denoted $r_i$, is defined as the signed square root of its contribution to the deviance:\n$$\nr_i = \\mathrm{sign}(y_i - \\hat{\\pi}_i) \\sqrt{d_i}\n$$\nwhere $y_i$ is the observed outcome and $\\hat{\\pi}_i$ is the fitted probability from the model.\n\nWe proceed by deriving the form of $d_i$ for logistic regression, which models a binary outcome $y_i \\in \\{0, 1\\}$ with a Bernoulli distribution.\n\n**1. Log-Likelihood of the Saturated Model**\n\nFor a Bernoulli trial, the log-likelihood for observation $i$ is given by $\\ell_i(\\pi_i \\mid y_i) = y_i \\log(\\pi_i) + (1-y_i)\\log(1-\\pi_i)$. The saturated model perfectly explains the data, meaning its fitted probability for observation $i$, let's call it $\\tilde{\\pi}_i$, is equal to the observed outcome $y_i$. Thus, $\\tilde{\\pi}_i = y_i$.\n\nThe log-likelihood of the saturated model for observation $i$ is the maximum possible value of the log-likelihood function, which occurs at $\\pi_i=y_i$.\nSubstituting $\\pi_i = y_i$ into the log-likelihood function yields:\n$$\n\\ell(\\boldsymbol{\\beta}_{sat}; y_i) = y_i \\log(y_i) + (1-y_i) \\log(1-y_i)\n$$\nIf $y_i = 1$, the expression becomes $1 \\log(1) + 0 \\log(0)$. If $y_i=0$, it is $0 \\log(0) + 1 \\log(1)$. Using the convention that $\\lim_{x\\to 0} x \\log x = 0$, the log-likelihood of the saturated model for any single Bernoulli observation is $0$.\n$$\n\\ell(\\boldsymbol{\\beta}_{sat}; y_i) = 0\n$$\n\n**2. Log-Likelihood of the Fitted Model**\n\nThe fitted logistic regression model provides an estimated probability $\\hat{\\pi}_i$ for each observation, where $\\hat{\\pi}_i = \\mathrm{logistic}(\\mathbf{x}_i^\\top \\hat{\\boldsymbol{\\beta}})$. The log-likelihood for observation $i$ under the fitted model is:\n$$\n\\ell(\\hat{\\boldsymbol{\\beta}}; y_i) = y_i \\log(\\hat{\\pi}_i) + (1-y_i)\\log(1-\\hat{\\pi}_i)\n$$\n\n**3. Individual Deviance Contribution ($d_i$)**\n\nCombining the log-likelihoods, the deviance contribution for observation $i$ is:\n$$\nd_i = 2 [0 - (y_i \\log(\\hat{\\pi}_i) + (1-y_i)\\log(1-\\hat{\\pi}_i))]\n$$\n$$\nd_i = -2 [y_i \\log(\\hat{\\pi}_i) + (1-y_i)\\log(1-\\hat{\\pi}_i)]\n$$\nThis is the general expression for the deviance component for a single observation in logistic regression.\n\n**4. Deviance Residual ($r_i$)**\n\nWe now construct the deviance residual $r_i = \\mathrm{sign}(y_i - \\hat{\\pi}_i) \\sqrt{d_i}$. To obtain an explicit form, we consider the two cases for the binary outcome $y_i$.\n\n- **Case 1: $y_i = 1$ (e.g., mortality occurs)**\n  In this case, the deviance contribution is $d_i = -2 [1 \\cdot \\log(\\hat{\\pi}_i) + 0 \\cdot \\log(1-\\hat{\\pi}_i)] = -2 \\log(\\hat{\\pi}_i)$.\n  The sign term is $\\mathrm{sign}(1 - \\hat{\\pi}_i)$. Since $\\hat{\\pi}_i \\in (0,1)$, this sign is always $+1$.\n  The residual is therefore:\n  $$\n  r_i = \\sqrt{-2 \\log(\\hat{\\pi}_i)} \\quad \\text{for } y_i = 1\n  $$\n\n- **Case 2: $y_i = 0$ (e.g., survival)**\n  In this case, the deviance contribution is $d_i = -2 [0 \\cdot \\log(\\hat{\\pi}_i) + 1 \\cdot \\log(1-\\hat{\\pi}_i)] = -2 \\log(1 - \\hat{\\pi}_i)$.\n  The sign term is $\\mathrm{sign}(0 - \\hat{\\pi}_i)$. Since $\\hat{\\pi}_i \\in (0,1)$, this sign is always $-1$.\n  The residual is therefore:\n  $$\n  r_i = -\\sqrt{-2 \\log(1 - \\hat{\\pi}_i)} \\quad \\text{for } y_i = 0\n  $$\n\nThese two expressions provide the explicit closed-form formula for the deviance residual in logistic regression, as required.\n\n### **Part 2: Calculation of Deviance Residuals**\n\nThe calculation for each observation $(\\mathrm{age}_i, \\mathrm{sofa}_i, y_i)$ proceeds as follows, using the given coefficient vector $\\hat{\\boldsymbol{\\beta}} = (-7.0, 0.04, 0.35)^\\top$.\n\n1.  **Compute the linear predictor $\\hat{\\eta}_i$**:\n    $$\n    \\hat{\\eta}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot \\mathrm{age}_i + \\hat{\\beta}_2 \\cdot \\mathrm{sofa}_i = -7.0 + 0.04 \\cdot \\mathrm{age}_i + 0.35 \\cdot \\mathrm{sofa}_i\n    $$\n2.  **Compute the fitted probability $\\hat{\\pi}_i$**:\n    $$\n    \\hat{\\pi}_i = \\mathrm{logistic}(\\hat{\\eta}_i) = \\frac{1}{1 + \\exp(-\\hat{\\eta}_i)}\n    $$\n3.  **Compute the deviance residual $r_i$** using the derived piecewise formula based on the value of $y_i$:\n    $$\n    r_i = \\begin{cases} \\sqrt{-2 \\log(\\hat{\\pi}_i)}  \\text{if } y_i=1 \\\\ -\\sqrt{-2 \\log(1-\\hat{\\pi}_i)}  \\text{if } y_i=0 \\end{cases}\n    $$\n4.  **Round** the result to six decimal places.\n\nThe provided Python code implements this procedure for each observation in the given test suites. Numerical stability is ensured by using standard `numpy` functions which handle floating-point arithmetic robustly for the range of inputs encountered in this problem. The expressions $\\log(\\hat{\\pi}_i)$ and $\\log(1-\\hat{\\pi}_i)$ are well-defined as $\\hat{\\pi}_i$ is strictly between $0$ and $1$ for any finite $\\hat{\\eta}_i$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes deviance residuals for logistic regression based on fixed coefficients.\n    \"\"\"\n    \n    # The given fixed coefficient vector beta_hat = (beta_0, beta_1, beta_2)\n    # corresponding to (intercept, age, sofa).\n    beta_hat = np.array([-7.0, 0.04, 0.35])\n\n    # Test suites as a list of lists of tuples. Each tuple is (age, sofa, y).\n    test_cases = [\n        # Test case 1: General mixed-risk cohort\n        [\n            (65, 6, 1),   # Observation 1\n            (72, 10, 1),  # Observation 2\n            (50, 4, 0),   # Observation 3\n            (34, 2, 0),   # Observation 4\n            (80, 12, 1)   # Observation 5\n        ],\n        # Test case 2: Near-boundary probabilities\n        [\n            (22, 0, 1),   # Observation 1\n            (90, 18, 0),  # Observation 2\n            (85, 20, 1),  # Observation 3\n            (25, 1, 0),   # Observation 4\n            (78, 22, 1)   # Observation 5\n        ],\n        # Test case 3: Duplicate covariates with discordant outcomes\n        [\n            (60, 8, 0),   # Observation 1\n            (60, 8, 1),   # Observation 2\n            (40, 5, 0),   # Observation 3\n            (40, 5, 1)    # Observation 4\n        ]\n    ]\n\n    all_results = []\n    for case_data in test_cases:\n        case_residuals = []\n        for age, sofa, y in case_data:\n            # Construct the design vector x_i with an intercept term\n            x_i = np.array([1, age, sofa])\n            \n            # Step 1: Compute the linear predictor eta_hat_i\n            eta_hat_i = x_i @ beta_hat\n            \n            # Step 2: Compute the fitted probability pi_hat_i\n            pi_hat_i = 1 / (1 + np.exp(-eta_hat_i))\n            \n            # Step 3: Compute the deviance residual r_i\n            # The piecewise formula derived in the solution is used.\n            if y == 1:\n                # Per-observation deviance for y=1\n                d_i = -2 * np.log(pi_hat_i)\n                # sign(y - pi_hat) is sign(1 - pi_hat) which is +1\n                residual = np.sqrt(d_i)\n            else: # y == 0\n                # Per-observation deviance for y=0.\n                # np.log1p(-pi_hat_i) is equivalent to np.log(1 - pi_hat_i) but\n                # can be more accurate for pi_hat_i close to 0. Here, 1-pi_hat_i\n                # is not extremely close to 1, so np.log is sufficient.\n                d_i = -2 * np.log(1 - pi_hat_i)\n                # sign(y - pi_hat) is sign(0 - pi_hat) which is -1\n                residual = -np.sqrt(d_i)\n\n            # Step 4: Round and store the result\n            case_residuals.append(round(residual, 6))\n        \n        all_results.append(case_residuals)\n\n    # Format the final output string as specified in the problem.\n    # e.g., [[-1.2,3.4],[5.6,7.8]]\n    output_str = f\"[{','.join([f'[{\",\".join(map(str, res))}]' for res in all_results])}]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "4982785"}, {"introduction": "For complex models such as the negative binomial for count data, traditional residuals can be difficult to interpret. This advanced practice [@problem_id:4982829] introduces a powerful, simulation-based approach: randomized quantile residuals (DHARMa). By transforming the residuals from any fitted model into a set of values that should be uniformly distributed if the model is correct, this technique provides a unified framework. It allows for robustly testing for global model misfit, as well as specific issues common in medical count data like zero-inflation and incorrect dispersion.", "problem": "You are tasked with implementing simulation-based regression diagnostics for count data in a medical context, drawing on the theoretical idea of Diagnostics for HierArchical Regression Models (DHARMa). The diagnostics rely on randomized quantile residuals constructed from a fitted negative binomial model for hospitalization counts. The setting and tasks are fully specified below.\n\nFundamental base:\n- A negative binomial distribution with mean $ \\mu $ and “size” (also known as inverse dispersion) $ \\theta $ satisfies $ \\operatorname{Var}(Y \\mid \\mu, \\theta) = \\mu + \\mu^2 / \\theta $. A generalized linear model with a log link encodes $ \\mu_i = \\exp(\\eta_i) $ with $ \\eta_i = \\beta_0 + \\beta_1 x_i + \\log(\\text{offset}_i) $, where $ \\beta_0 $ and $ \\beta_1 $ are regression coefficients and $ \\text{offset}_i $ is known exposure.\n- For a discrete response $ Y_i $ conditioned on $ \\mu_i $ and $ \\theta $, the randomized quantile residual (DHARMa residual) is defined by\n$$ U_i = F_{Y_i \\mid \\mu_i, \\theta}(Y_i - 1) + V_i \\cdot \\mathbb{P}(Y_i \\mid \\mu_i, \\theta), $$\nwhere $ V_i \\sim \\text{Uniform}(0, 1) $, $ F $ is the cumulative distribution function, and $ \\mathbb{P} $ denotes the probability mass function. Under correct model specification, $ U_i $ are independent and identically distributed Uniform on $ [0,1] $.\n- Uniformity can be assessed via a distributional test such as the Kolmogorov–Smirnov test of $ H_0: U_i \\sim \\text{Uniform}(0,1) $. Zero-inflation can be assessed by comparing the observed number of zeros to the model-implied distribution of zero counts constructed through simulation under the fitted model. Dispersion can be assessed by comparing a dispersion statistic (e.g., a sum of squared Pearson residuals) to its sampling distribution obtained via parametric simulation under the fitted model.\n\nYour implementation must:\n- Use the log-link generalized linear model definition $ \\mu_i = \\exp(\\beta_0 + \\beta_1 x_i + \\log(\\text{offset}_i)) $ to compute $ \\mu_i $ for each observation.\n- Use the negative binomial distribution parameterized by $(\\mu_i, \\theta)$, with the relationship to “number of successes” $ n = \\theta $ and success probability $ p_i = \\theta / (\\theta + \\mu_i) $ so that $ \\mathbb{E}[Y_i] = \\mu_i $ and $ \\operatorname{Var}(Y_i) = \\mu_i + \\mu_i^2 / \\theta $.\n- Construct DHARMa residuals $ U_i $ and compute a Kolmogorov–Smirnov test against $ \\text{Uniform}(0,1) $.\n- Implement a zero-inflation diagnostic by simulating replicated datasets under the fitted model (using the given $ \\mu_i $ and $ \\theta $) to obtain the empirical distribution of the total zero count. Compute a two-sided Monte Carlo p-value based on the observed zero count.\n- Implement a dispersion diagnostic by computing a Pearson-type statistic\n$$ R_{\\text{obs}} = \\sum_{i=1}^n \\frac{(Y_i - \\mu_i)^2}{\\mu_i + \\mu_i^2 / \\theta}, $$\nand comparing it to its empirical sampling distribution under the fitted model via parametric simulation. Report a two-sided Monte Carlo p-value.\n\nSignificance threshold and reporting:\n- Use significance level $ \\alpha = 0.05 $.\n- For each test case, return three boolean indicators $ [b_1, b_2, b_3] $ where $ b_1 $ is $ \\text{True} $ if the Kolmogorov–Smirnov test rejects at level $ \\alpha $, $ b_2 $ is $ \\text{True} $ if the zero-inflation test rejects at level $ \\alpha $, and $ b_3 $ is $ \\text{True} $ if the dispersion test rejects at level $ \\alpha $. Otherwise, return $ \\text{False} $.\n\nSimulation details and test suite:\n- Let $ S $ denote the number of parametric simulations to build the reference distributions for the zero-inflation and dispersion tests. Use $ S = 3000 $ for all cases.\n- Use the gamma–Poisson mixture representation to simulate negative binomial random variables: if $ \\Lambda_i \\sim \\text{Gamma}(\\text{shape}=\\theta, \\text{scale}=\\mu_i/\\theta) $ and $ Y_i \\mid \\Lambda_i \\sim \\text{Poisson}(\\Lambda_i) $, then $ Y_i \\sim \\text{NB}(\\mu_i, \\theta) $. This should be used for efficient vectorized simulation across $ S $ replicates and $ n $ observations.\n\nTest cases:\n- Case A (well-specified negative binomial):\n    - Seed for data generation: $ 12345 $.\n    - Sample size: $ n = 800 $.\n    - Covariate: $ x_i \\sim \\text{Uniform}(0,1) $ independently.\n    - Offset: $ \\text{offset}_i \\sim \\text{Uniform}(0.5, 2.0) $ independently.\n    - Fitted parameters: $ \\beta_0 = -1.0 $, $ \\beta_1 = 0.8 $, $ \\theta = 12.0 $.\n    - Generate observed counts: $ Y_i \\sim \\text{NB}(\\mu_i, \\theta) $ with $ \\mu_i = \\exp(\\beta_0 + \\beta_1 x_i + \\log(\\text{offset}_i)) $.\n- Case B (zero-inflated counts relative to the fitted negative binomial):\n    - Seed for data generation: $ 424242 $.\n    - Sample size: $ n = 800 $.\n    - Covariate: $ x_i \\sim \\text{Uniform}(0,1) $ independently.\n    - Offset: $ \\text{offset}_i \\sim \\text{Uniform}(0.5, 2.0) $ independently.\n    - Fitted parameters: $ \\beta_0 = -1.0 $, $ \\beta_1 = 0.8 $, $ \\theta = 12.0 $.\n    - Zero-inflation probability: $ \\pi = 0.5 $.\n    - Generate observed counts: with probability $ \\pi $, set $ Y_i = 0 $; otherwise sample $ Y_i \\sim \\text{NB}(\\mu_i, \\theta) $ as above.\n- Case C (underdispersed relative to the fitted negative binomial):\n    - Seed for data generation: $ 999 $.\n    - Sample size: $ n = 600 $.\n    - Covariate: $ x_i \\sim \\text{Uniform}(0,1) $ independently.\n    - Offset: $ \\text{offset}_i \\equiv 5.0 $ (constant).\n    - Fitted parameters: $ \\beta_0 = 1.0 $, $ \\beta_1 = 0.3 $, $ \\theta = 1.5 $.\n    - Generate observed counts: $ Y_i \\sim \\text{Poisson}(\\mu_i) $ with $ \\mu_i = \\exp(\\beta_0 + \\beta_1 x_i + \\log(\\text{offset}_i)) $.\n\nAngle units and physical units are not applicable. There are no percentage outputs; significance is determined by direct comparison of p-values to $ \\alpha $.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a three-element list of booleans $ [b_1,b_2,b_3] $ for Cases A, B, and C in that order. For example, the format should be like $ [[\\ldots],[\\ldots],[\\ldots]] $ with no spaces inserted by the program unless required by list formatting. The actual boolean values must be computed by your program from the specification above, using $ S = 3000 $ parametric simulations and $ \\alpha = 0.05 $.", "solution": "The user-provided problem statement is subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **Model Framework**: A negative binomial generalized linear model with a log link function is specified. The mean of the response $Y_i$ is $ \\mu_i = \\exp(\\beta_0 + \\beta_1 x_i + \\log(\\text{offset}_i)) $. The variance is $ \\operatorname{Var}(Y_i \\mid \\mu_i, \\theta) = \\mu_i + \\mu_i^2 / \\theta $, where $ \\theta $ is the inverse dispersion parameter.\n- **Negative Binomial Parameterization**: For compatibility with standard software libraries, the negative binomial distribution is parameterized by the number of successes $n = \\theta$ and the success probability $p_i = \\theta / (\\theta + \\mu_i)$.\n- **Randomized Quantile (DHARMa) Residuals**: The residual for observation $i$ is defined as $ U_i = F_{Y_i \\mid \\mu_i, \\theta}(Y_i - 1) + V_i \\cdot \\mathbb{P}(Y_i \\mid \\mu_i, \\theta) $, where $ F $ is the cumulative distribution function (CDF), $ \\mathbb{P} $ is the probability mass function (PMF), and $ V_i $ is a random variable drawn from a $ \\text{Uniform}(0, 1) $ distribution.\n- **Diagnostic Test 1 (Uniformity)**: A Kolmogorov-Smirnov (KS) test is used to assess if the calculated residuals $ U_i $ follow a $ \\text{Uniform}(0,1) $ distribution.\n- **Diagnostic Test 2 (Zero-Inflation)**: A Monte Carlo test comparing the observed count of zeros in the data to the distribution of zero counts obtained from parametric simulations of the fitted model. A two-sided p-value is to be computed.\n- **Diagnostic Test 3 (Dispersion)**: A Monte Carlo test comparing the observed Pearson-type dispersion statistic, $ R_{\\text{obs}} = \\sum_{i=1}^n \\frac{(Y_i - \\mu_i)^2}{\\mu_i + \\mu_i^2 / \\theta} $, to its sampling distribution derived from parametric simulations. A two-sided p-value is to be computed.\n- **Simulation Parameters**: The number of simulations for the Monte Carlo tests is $ S = 3000 $. The significance level for all tests is $ \\alpha = 0.05 $.\n- **Simulation Method**: Negative binomial random variables are to be generated using the Gamma-Poisson mixture formulation: If $ \\Lambda_i \\sim \\text{Gamma}(\\text{shape}=\\theta, \\text{scale}=\\mu_i/\\theta) $ and $ Y_i \\mid \\Lambda_i \\sim \\text{Poisson}(\\Lambda_i) $, then $ Y_i \\sim \\text{NB}(\\mu_i, \\theta) $.\n- **Test Cases**:\n    - **Case A (Well-specified)**: $ \\text{seed}=12345 $, $ n=800 $, $ x_i \\sim \\text{Uniform}(0,1) $, $ \\text{offset}_i \\sim \\text{Uniform}(0.5, 2.0) $, $ \\beta_0 = -1.0 $, $ \\beta_1 = 0.8 $, $ \\theta = 12.0 $. Data is generated from $ Y_i \\sim \\text{NB}(\\mu_i, \\theta) $.\n    - **Case B (Zero-inflated)**: $ \\text{seed}=424242 $, $ n=800 $, $ x_i \\sim \\text{Uniform}(0,1) $, $ \\text{offset}_i \\sim \\text{Uniform}(0.5, 2.0) $, $ \\beta_0 = -1.0 $, $ \\beta_1 = 0.8 $, $ \\theta = 12.0 $. Data is generated with probability $ \\pi = 0.5 $ as $ Y_i=0 $, and with probability $ 1-\\pi $ from $ Y_i \\sim \\text{NB}(\\mu_i, \\theta) $.\n    - **Case C (Underdispersed)**: $ \\text{seed}=999 $, $ n=600 $, $ x_i \\sim \\text{Uniform}(0,1) $, $ \\text{offset}_i = 5.0 $, $ \\beta_0 = 1.0 $, $ \\beta_1 = 0.3 $, $ \\theta = 1.5 $. Data is generated from $ Y_i \\sim \\text{Poisson}(\\mu_i) $.\n- **Output**: A list of lists of booleans, e.g., $ [[b_{A1}, b_{A2}, b_{A3}], [b_{B1}, b_{B2}, b_{B3}], [b_{C1}, b_{C2}, b_{C3}]] $, where $ b_{j} $ is true if the corresponding test rejects the null hypothesis.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the established validation criteria.\n1.  **Scientifically Grounded**: The problem is sound. It is based on established statistical principles of Generalized Linear Models (GLMs), the negative binomial distribution, and simulation-based diagnostics (specifically the DHARMa framework). These are standard, non-controversial methods in applied statistics.\n2.  **Well-Posed**: The problem is well-posed. All parameters, seeds, distributions, and diagnostic procedures are specified without ambiguity. The task is to implement a deterministic (given the seeds) algorithm and report the results, which leads to a unique solution.\n3.  **Objective**: The problem is stated in precise, objective, and formal mathematical language. It is free from subjective claims or opinions.\n4.  **Incomplete or Contradictory Setup**: The problem is complete and self-contained. All necessary information to generate the data and perform the diagnostics is provided. The distinction between the data-generating process and the \"fitted model\" used for diagnostics is a standard and intentional part of evaluating model misspecification. There are no contradictions.\n5.  **Unrealistic or Infeasible**: The problem is feasible. The models and parameters are realistic for count data analysis in fields like epidemiology. The computational load ($S=3000$ simulations for $n \\le 800$) is well within the capabilities of standard hardware.\n6.  **Other Flaws**: The problem is not trivial, metaphorical, or outside the scope of scientific verification. It represents a concrete and substantive task in computational statistics.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be developed and presented.\n\n### Principle-Based Design\nThe solution will be structured around a central function that executes the entire diagnostic pipeline for a single test case. This function will encapsulate the sequence of data generation and model checking, promoting code reuse and clarity. The three main diagnostic tests will be implemented sequentially.\n\n1.  **Data Generation**: For each case, a specific random number generator seed is used to ensure reproducibility. Covariates ($x_i$), offsets ($\\text{offset}_i$), and the true mean ($\\mu_i$) are generated as specified. The observed data ($Y_i$) are then sampled from the distribution defined for that case (Negative Binomial, Zero-Inflated NB, or Poisson).\n\n2.  **DHARMa Residual Calculation and KS Test**: The core of this step is the formula for the randomized quantile residual, $ U_i = F(Y_i-1) + V_i \\cdot \\mathbb{P}(Y_i) $. The PMF ($\\mathbb{P}$) and CDF ($F$) of the fitted negative binomial model are computed using `scipy.stats.nbinom`, which correctly handles the specified parameterization. After generating the vector of residuals $U$, the `scipy.stats.kstest` function is used to test for uniformity. The resulting p-value is compared against the significance level $ \\alpha $ to determine rejection.\n\n3.  **Parametric Simulation and Monte Carlo Tests**: To assess zero-inflation and dispersion, a reference distribution for each test statistic is needed. This is constructed via parametric simulation from the *fitted* negative binomial model. To do this efficiently, we leverage the specified Gamma-Poisson mixture property and `numpy`'s vectorized operations.\n    - A matrix of $S$ simulated datasets, each of size $n$, is generated. For each observation $i$, we draw a rate $ \\Lambda_{ji} \\sim \\text{Gamma}(\\theta, \\mu_i/\\theta) $ for each simulation $j \\in \\{1, \\dots, S\\}$, and then draw a count $Y_{ji} \\sim \\text{Poisson}(\\Lambda_{ji})$.\n    - **Zero-Inflation Test**: The number of zeros in the observed data is calculated. Then, the number of zeros is calculated for each of the $S$ simulated datasets, yielding an empirical distribution of zero counts under the null model. A two-sided Monte Carlo p-value is computed by comparing the rank of the observed statistic to this empirical distribution.\n    - **Dispersion Test**: The Pearson-type statistic $R_{\\text{obs}}$ is calculated for the observed data. The same statistic is then calculated for each of the $S$ simulated datasets, creating an empirical distribution for $R$ under the null. A two-sided Monte Carlo p-value is computed similarly to the zero-inflation test.\n\nFinally, the boolean outcomes (rejection or not) from the three tests are collected for each case and formatted into the required output string.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import nbinom, kstest\n\ndef _generate_data_nb(rng, mu, theta, **kwargs):\n    \"\"\"Generates data from a negative binomial distribution.\"\"\"\n    # The parameterization of numpy's negative_binomial (mean = n*(1-p)/p)\n    # matches the problem's definition for E[Y] = mu when n=theta and p=theta/(theta+mu).\n    p = theta / (theta + mu)\n    return rng.negative_binomial(n=theta, p=p)\n\ndef _generate_data_zinb(rng, mu, theta, n, pi, **kwargs):\n    \"\"\"Generates data from a zero-inflated negative binomial process.\"\"\"\n    # Generate the NB part of the data.\n    p = theta / (theta + mu)\n    nb_draws = rng.negative_binomial(n=theta, p=p, size=n)\n    \n    # Generate the zero-inflation mask. With probability pi, the value is 0.\n    zero_mask = rng.random(size=n)  pi\n    \n    # Where the mask is True, set value to 0; otherwise, use the NB draw.\n    return np.where(zero_mask, 0, nb_draws)\n\ndef _generate_data_poisson(rng, mu, **kwargs):\n    \"\"\"Generates data from a Poisson distribution.\"\"\"\n    return rng.poisson(lam=mu)\n\ndef _run_diagnostics(params):\n    \"\"\"\n    Runs the full diagnostic pipeline for a single test case.\n    \"\"\"\n    # 1. Setup RNG and extract parameters\n    rng = np.random.default_rng(params['seed'])\n    n, beta0, beta1, theta = params['n'], params['beta0'], params['beta1'], params['theta']\n    offset_params = params['offset_params']\n    S, alpha = params['S'], params['alpha']\n    \n    # 2. Generate predictors and mu based on fitted model parameters\n    x = rng.uniform(0, 1, size=n)\n    if isinstance(offset_params, tuple):\n        offset = rng.uniform(offset_params[0], offset_params[1], size=n)\n    else:\n        offset = np.full(n, offset_params)\n    mu = np.exp(beta0 + beta1 * x + np.log(offset))\n    \n    # 3. Generate observed data Y using the case-specific function\n    Y = params['data_gen_func'](rng, mu=mu, **params)\n\n    # --- Start Diagnostics against the fitted NB(mu, theta) model ---\n\n    # 4. DHARMa Residuals and KS Test (b1)\n    p_nb = theta / (theta + mu)\n    cdf_at_y_minus_1 = nbinom.cdf(Y - 1, n=theta, p=p_nb)\n    pmf_at_y = nbinom.pmf(Y, n=theta, p=p_nb)\n    v = rng.uniform(size=n)\n    U = cdf_at_y_minus_1 + v * pmf_at_y\n    _, ks_pvalue = kstest(U, 'uniform')\n    b1 = ks_pvalue  alpha\n\n    # 5. Parametric Simulations for reference distributions\n    shape = theta\n    scale = mu / theta\n    gamma_draws = rng.gamma(shape, scale, size=(S, n))\n    Y_sims = rng.poisson(lam=gamma_draws)\n\n    # 6. Zero-Inflation Test (b2)\n    observed_zeros = np.sum(Y == 0)\n    simulated_zeros = np.sum(Y_sims == 0, axis=1)\n    p_upper_zi = (np.sum(simulated_zeros = observed_zeros) + 1) / (S + 1)\n    p_lower_zi = (np.sum(simulated_zeros = observed_zeros) + 1) / (S + 1)\n    zi_pvalue = 2 * np.min([p_upper_zi, p_lower_zi])\n    b2 = zi_pvalue  alpha\n\n    # 7. Dispersion Test (b3)\n    nb_variance = mu + mu**2 / theta\n    R_obs = np.sum((Y - mu)**2 / nb_variance)\n    R_sims = np.sum((Y_sims - mu)**2 / nb_variance, axis=1)\n    p_upper_disp = (np.sum(R_sims = R_obs) + 1) / (S + 1)\n    p_lower_disp = (np.sum(R_sims = R_obs) + 1) / (S + 1)\n    disp_pvalue = 2 * np.min([p_upper_disp, p_lower_disp])\n    b3 = disp_pvalue  alpha\n\n    return [b1, b2, b3]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run diagnostics, and print results.\n    \"\"\"\n    # Define common simulation parameters\n    S = 3000\n    alpha = 0.05\n    \n    # Define parameters for each test case as a list of dictionaries\n    test_cases = [\n        # Case A: Well-specified Negative Binomial\n        {\n            \"seed\": 12345, \"n\": 800, \"beta0\": -1.0, \"beta1\": 0.8, \"theta\": 12.0,\n            \"offset_params\": (0.5, 2.0), \"data_gen_func\": _generate_data_nb,\n            \"S\": S, \"alpha\": alpha\n        },\n        # Case B: Zero-inflated relative to the fitted Negative Binomial\n        {\n            \"seed\": 424242, \"n\": 800, \"beta0\": -1.0, \"beta1\": 0.8, \"theta\": 12.0,\n            \"offset_params\": (0.5, 2.0), \"data_gen_func\": _generate_data_zinb,\n            \"S\": S, \"alpha\": alpha, \"pi\": 0.5\n        },\n        # Case C: Underdispersed (Poisson) relative to the fitted Negative Binomial\n        {\n            \"seed\": 999, \"n\": 600, \"beta0\": 1.0, \"beta1\": 0.3, \"theta\": 1.5,\n            \"offset_params\": 5.0, \"data_gen_func\": _generate_data_poisson,\n            \"S\": S, \"alpha\": alpha\n        }\n    ]\n    \n    results = []\n    for case_params in test_cases:\n        case_result = _run_diagnostics(case_params)\n        results.append(case_result)\n        \n    # Format the final output string to remove spaces, as per spec hint.\n    formatted_results = [f\"[{b1},{b2},{b3}]\" for b1, b2, b3 in results]\n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "4982829"}]}