{"hands_on_practices": [{"introduction": "Before we can master complex non-linear models, we must first build a solid foundation in the principles of polynomial regression. This exercise takes you back to first principles, guiding you through the construction of a design matrix and the derivation of the Ordinary Least Squares (OLS) estimator for a cubic polynomial model [@problem_id:4974743]. By working through a small, concrete example, you will gain a hands-on appreciation for the mathematical machinery that underpins the automated regression tools used in practice.", "problem": "In a clinical pharmacology study of antihypertensive therapy, investigators suspect a non-linear dose–response relationship between drug dose and reduction in systolic blood pressure. Let the predictor be the drug dose in milligrams, denoted by $x$, and the response be the reduction in systolic blood pressure in millimeters of mercury, denoted by $y$. Motivated by non-linear modeling with polynomials, consider approximating the dose–response function by a cubic polynomial in a scaled dose variable $z$ defined by $z = x / 50$ to improve numerical conditioning. Specifically, model the mean response as\n$$\n\\mu(z) = \\beta_{0} + \\beta_{1} z + \\beta_{2} z^{2} + \\beta_{3} z^{3},\n$$\nand assume the classical linear model $y_{i} = \\mu(z_{i}) + \\varepsilon_{i}$ with independent errors satisfying $\\mathbb{E}[\\varepsilon_{i}] = 0$ and $\\operatorname{Var}(\\varepsilon_{i}) = \\sigma^{2}$ for all $i$, where $\\sigma^{2}$ is a finite, positive constant.\n\nTasks:\n1) Construct the design matrix for the cubic polynomial model using the basis $\\{1, z, z^{2}, z^{3}\\}$ for $n$ observations $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$, and write the corresponding linear model in matrix form.\n2) Starting from the least squares criterion that minimizes the sum of squared residuals $\\sum_{i=1}^{n} (y_{i} - \\mu(z_{i}))^{2}$, derive the ordinary least squares (OLS) estimator for the coefficient vector $\\boldsymbol{\\beta} = (\\beta_{0}, \\beta_{1}, \\beta_{2}, \\beta_{3})^{\\top}$.\n3) For the following $n = 4$ observations, compute the OLS estimate $\\hat{\\boldsymbol{\\beta}}$ explicitly:\n$$\n(x_{1}, y_{1}) = (0, 10), \\quad (x_{2}, y_{2}) = (50, 10), \\quad (x_{3}, y_{3}) = (100, 20), \\quad (x_{4}, y_{4}) = (150, 46).\n$$\nUse the scaled variable $z_{i} = x_{i} / 50$ when constructing the design matrix. Report your final answer as the row vector of estimated coefficients corresponding to the polynomial in $z$, ordered as $(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}, \\hat{\\beta}_{2}, \\hat{\\beta}_{3})$. Do not include units in your final numerical expression. No rounding is required.", "solution": "The problem statement is first validated against the required criteria.\n\n### Step 1: Extract Givens\n- Predictor variable: drug dose, $x$, in milligrams.\n- Response variable: reduction in systolic blood pressure, $y$, in millimeters of mercury.\n- Scaled predictor variable: $z = x / 50$.\n- Model for mean response: $\\mu(z) = \\beta_{0} + \\beta_{1} z + \\beta_{2} z^{2} + \\beta_{3} z^{3}$.\n- Statistical model: $y_{i} = \\mu(z_{i}) + \\varepsilon_{i}$ for observations $i=1, \\dots, n$.\n- Error assumptions: The errors $\\varepsilon_{i}$ are independent with $\\mathbb{E}[\\varepsilon_{i}] = 0$ and $\\operatorname{Var}(\\varepsilon_{i}) = \\sigma^{2}$, where $\\sigma^{2}$ is a finite, positive constant.\n- Coefficient vector: $\\boldsymbol{\\beta} = (\\beta_{0}, \\beta_{1}, \\beta_{2}, \\beta_{3})^{\\top}$.\n- Task 1: Construct the design matrix for the basis $\\{1, z, z^{2}, z^{3}\\}$ and write the model in matrix form.\n- Task 2: Derive the ordinary least squares (OLS) estimator for $\\boldsymbol{\\beta}$ by minimizing $\\sum_{i=1}^{n} (y_{i} - \\mu(z_{i}))^{2}$.\n- Task 3: Compute the OLS estimate $\\hat{\\boldsymbol{\\beta}}$ for $n = 4$ observations: $(x_{1}, y_{1}) = (0, 10)$, $(x_{2}, y_{2}) = (50, 10)$, $(x_{3}, y_{3}) = (100, 20)$, and $(x_{4}, y_{4}) = (150, 46)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, posing a standard question in applied statistics (polynomial regression) within a plausible context (pharmacology). The assumptions (classical linear model) are standard. The problem is well-posed, providing all necessary information, definitions, and data to complete the requested tasks. The questions are objective and mathematically formalizable. There are no contradictions, ambiguities, or factual unsoundness. The use of a scaled variable is a common and sound practice for improving numerical stability in polynomial regression. The chosen data points for the final calculation are distinct, ensuring that the design matrix will be invertible.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n***\n\n**1) Design Matrix and Matrix Form of the Model**\n\nThe model for a single observation $y_i$ is given by:\n$$ y_{i} = \\beta_{0} + \\beta_{1} z_{i} + \\beta_{2} z_{i}^{2} + \\beta_{3} z_{i}^{3} + \\varepsilon_{i} $$\nThis can be written as a system of $n$ linear equations. For the entire dataset $\\{(z_{i}, y_{i})\\}_{i=1}^{n}$, we have:\n$$\n\\begin{cases}\n    y_1 = \\beta_0 \\cdot 1 + \\beta_1 \\cdot z_1 + \\beta_2 \\cdot z_1^2 + \\beta_3 \\cdot z_1^3 + \\varepsilon_1 \\\\\n    y_2 = \\beta_0 \\cdot 1 + \\beta_1 \\cdot z_2 + \\beta_2 \\cdot z_2^2 + \\beta_3 \\cdot z_2^3 + \\varepsilon_2 \\\\\n    \\vdots \\\\\n    y_n = \\beta_0 \\cdot 1 + \\beta_1 \\cdot z_n + \\beta_2 \\cdot z_n^2 + \\beta_3 \\cdot z_n^3 + \\varepsilon_n\n\\end{cases}\n$$\nThis system can be expressed in matrix form as $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where:\n- $\\mathbf{y} = (y_1, y_2, \\dots, y_n)^{\\top}$ is the $n \\times 1$ vector of observations.\n- $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\beta_2, \\beta_3)^{\\top}$ is the $4 \\times 1$ vector of coefficients.\n- $\\boldsymbol{\\varepsilon} = (\\varepsilon_1, \\varepsilon_2, \\dots, \\varepsilon_n)^{\\top}$ is the $n \\times 1$ vector of random errors.\n- $\\mathbf{X}$ is the $n \\times 4$ design matrix, whose columns correspond to the basis functions $\\{1, z, z^2, z^3\\}$ evaluated at each data point $z_i$. The $i$-th row of $\\mathbf{X}$ is $(1, z_i, z_i^2, z_i^3)$.\n\nThe design matrix is therefore:\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1 & z_1 & z_1^2 & z_1^3 \\\\\n1 & z_2 & z_2^2 & z_2^3 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & z_n & z_n^2 & z_n^3\n\\end{pmatrix}\n$$\nThe linear model in matrix form is:\n$$\n\\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} =\n\\begin{pmatrix}\n1 & z_1 & z_1^2 & z_1^3 \\\\\n1 & z_2 & z_2^2 & z_2^3 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & z_n & z_n^2 & z_n^3\n\\end{pmatrix}\n\\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{pmatrix} +\n\\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix}\n$$\n\n**2) Derivation of the OLS Estimator**\n\nThe ordinary least squares (OLS) estimator $\\hat{\\boldsymbol{\\beta}}$ is the vector of coefficients that minimizes the sum of squared residuals, $S(\\boldsymbol{\\beta})$. The sum of squared residuals is defined as:\n$$ S(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mu(z_i))^2 = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 z_i + \\beta_2 z_i^2 + \\beta_3 z_i^3))^2 $$\nIn matrix notation, the vector of residuals is $\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}$. The sum of squared residuals is the squared Euclidean norm of this vector:\n$$ S(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^{\\top}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) $$\nExpanding this expression gives:\n$$ S(\\boldsymbol{\\beta}) = (\\mathbf{y}^{\\top} - (\\mathbf{X}\\boldsymbol{\\beta})^{\\top})(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = \\mathbf{y}^{\\top}\\mathbf{y} - \\mathbf{y}^{\\top}\\mathbf{X}\\boldsymbol{\\beta} - (\\mathbf{X}\\boldsymbol{\\beta})^{\\top}\\mathbf{y} + (\\mathbf{X}\\boldsymbol{\\beta})^{\\top}(\\mathbf{X}\\boldsymbol{\\beta}) $$\nUsing the property $(\\mathbf{A}\\mathbf{B})^{\\top} = \\mathbf{B}^{\\top}\\mathbf{A}^{\\top}$, this becomes:\n$$ S(\\boldsymbol{\\beta}) = \\mathbf{y}^{\\top}\\mathbf{y} - \\mathbf{y}^{\\top}\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y} + \\boldsymbol{\\beta}^{\\top}\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\beta} $$\nSince $\\mathbf{y}^{\\top}\\mathbf{X}\\boldsymbol{\\beta}$ is a $1 \\times 1$ matrix (a scalar), it is equal to its transpose, $(\\mathbf{y}^{\\top}\\mathbf{X}\\boldsymbol{\\beta})^{\\top} = \\boldsymbol{\\beta}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y}$. Thus, the two middle terms are identical.\n$$ S(\\boldsymbol{\\beta}) = \\mathbf{y}^{\\top}\\mathbf{y} - 2\\boldsymbol{\\beta}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y} + \\boldsymbol{\\beta}^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X})\\boldsymbol{\\beta} $$\nTo find the minimum, we compute the gradient of $S(\\boldsymbol{\\beta})$ with respect to $\\boldsymbol{\\beta}$ and set it to the zero vector. Using the standard results for vector differentiation $\\frac{\\partial \\mathbf{a}^{\\top}\\mathbf{x}}{\\partial \\mathbf{x}} = \\mathbf{a}$ and $\\frac{\\partial \\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x}}{\\partial \\mathbf{x}} = 2\\mathbf{A}\\mathbf{x}$ for symmetric $\\mathbf{A}$ (note that $\\mathbf{X}^{\\top}\\mathbf{X}$ is symmetric):\n$$ \\frac{\\partial S(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^{\\top}\\mathbf{y} + 2(\\mathbf{X}^{\\top}\\mathbf{X})\\boldsymbol{\\beta} $$\nSetting the gradient to the zero vector to find the critical point, which corresponds to the OLS estimate $\\hat{\\boldsymbol{\\beta}}$:\n$$ -2\\mathbf{X}^{\\top}\\mathbf{y} + 2(\\mathbf{X}^{\\top}\\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{0} $$\n$$ (\\mathbf{X}^{\\top}\\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^{\\top}\\mathbf{y} $$\nThis set of equations is known as the normal equations. Assuming that the matrix $\\mathbf{X}^{\\top}\\mathbf{X}$ is invertible (which holds if the columns of $\\mathbf{X}$ are linearly independent), we can solve for $\\hat{\\boldsymbol{\\beta}}$ by pre-multiplying by $(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}$:\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y} $$\nThis is the general formula for the OLS estimator. The second derivative (Hessian) matrix of $S(\\boldsymbol{\\beta})$ is $2\\mathbf{X}^{\\top}\\mathbf{X}$, which is positive definite if $\\mathbf{X}$ has full column rank, confirming that this solution corresponds to a minimum.\n\n**3) Computation of the OLS Estimate**\n\nWe are given $n=4$ data points: $(0, 10)$, $(50, 10)$, $(100, 20)$, $(150, 46)$.\nFirst, we compute the scaled predictor variable $z_i = x_i / 50$:\n$$\nz_1 = 0 / 50 = 0 \\\\\nz_2 = 50 / 50 = 1 \\\\\nz_3 = 100 / 50 = 2 \\\\\nz_4 = 150 / 50 = 3\n$$\nThe vector of responses is $\\mathbf{y} = (10, 10, 20, 46)^{\\top}$.\nThe number of observations is $n=4$, and the number of parameters in the cubic model is $p=4$ (for $\\beta_0, \\beta_1, \\beta_2, \\beta_3$). Since $n=p$, the design matrix $\\mathbf{X}$ is a square matrix.\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1 & z_1 & z_1^2 & z_1^3 \\\\\n1 & z_2 & z_2^2 & z_2^3 \\\\\n1 & z_3 & z_3^2 & z_3^3 \\\\\n1 & z_4 & z_4^2 & z_4^3\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n1 & 1 & 1 & 1 \\\\\n1 & 2 & 4 & 8 \\\\\n1 & 3 & 9 & 27\n\\end{pmatrix}\n$$\nThis is a Vandermonde matrix with distinct points $z_i$, so it is invertible. In the special case where $n=p$ and $\\mathbf{X}$ is invertible, the OLS estimator formula simplifies:\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y} = \\mathbf{X}^{-1}(\\mathbf{X}^{\\top})^{-1}\\mathbf{X}^{\\top}\\mathbf{y} = \\mathbf{X}^{-1}\\mathbf{y} $$\nThis signifies that the fitted polynomial will pass exactly through all four data points (interpolation). We need to find the inverse of $\\mathbf{X}$. Using a method such as Gauss-Jordan elimination, the inverse is found to be:\n$$ \\mathbf{X}^{-1} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ -\\frac{11}{6} &  3 & -\\frac{3}{2} & \\frac{1}{3} \\\\ 1 & -\\frac{5}{2} & 2 & -\\frac{1}{2} \\\\ -\\frac{1}{6} & \\frac{1}{2} & -\\frac{1}{2} & \\frac{1}{6} \\end{pmatrix} $$\nNow we compute $\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^{-1}\\mathbf{y}$:\n$$\n\\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\\\ \\hat{\\beta}_3 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ -\\frac{11}{6} &  3 & -\\frac{3}{2} & \\frac{1}{3} \\\\ 1 & -\\frac{5}{2} & 2 & -\\frac{1}{2} \\\\ -\\frac{1}{6} & \\frac{1}{2} & -\\frac{1}{2} & \\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 10 \\\\ 10 \\\\ 20 \\\\ 46 \\end{pmatrix}\n$$\nPerforming the matrix-vector multiplication:\n$$ \\hat{\\beta}_0 = 1 \\cdot 10 + 0 \\cdot 10 + 0 \\cdot 20 + 0 \\cdot 46 = 10 $$\n$$ \\hat{\\beta}_1 = -\\frac{11}{6} \\cdot 10 + 3 \\cdot 10 - \\frac{3}{2} \\cdot 20 + \\frac{1}{3} \\cdot 46 = -\\frac{110}{6} + 30 - 30 + \\frac{46}{3} = -\\frac{55}{3} + \\frac{46}{3} = -\\frac{9}{3} = -3 $$\n$$ \\hat{\\beta}_2 = 1 \\cdot 10 - \\frac{5}{2} \\cdot 10 + 2 \\cdot 20 - \\frac{1}{2} \\cdot 46 = 10 - 25 + 40 - 23 = 50 - 48 = 2 $$\n$$ \\hat{\\beta}_3 = -\\frac{1}{6} \\cdot 10 + \\frac{1}{2} \\cdot 10 - \\frac{1}{2} \\cdot 20 + \\frac{1}{6} \\cdot 46 = -\\frac{10}{6} + 5 - 10 + \\frac{46}{6} = -5 + \\frac{36}{6} = -5 + 6 = 1 $$\nThe OLS estimate of the coefficient vector is $\\hat{\\boldsymbol{\\beta}} = (10, -3, 2, 1)^{\\top}$. The problem asks for the result as a row vector.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n10 & -3 & 2 & 1\n\\end{pmatrix}\n}\n$$", "id": "4974743"}, {"introduction": "While polynomials are a straightforward way to introduce non-linearity, higher-degree polynomials can be treacherous, often leading to overfitting and wild oscillations. This simulation exercise provides a vivid, hands-on demonstration of this issue, a classic pitfall known as Runge's phenomenon [@problem_id:4974750]. By fitting both a low-degree and a high-degree polynomial to data from a known smooth function, you will quantitatively measure how model complexity can introduce spurious artifacts, motivating the need for more stable modeling approaches.", "problem": "You are asked to design and implement a reproducible simulation that demonstrates, in the context of statistical modeling in medicine, how high-degree polynomial regression can exhibit Runge’s phenomenon on the unit interval $[0,1]$ when the design points are equispaced. You will simulate outcomes from a smooth saturating dose-response mean function commonly used in pharmacology and fit ordinary least squares (OLS) polynomial regressions of degree $d=3$ and $d=6$ to the same equispaced design points. You will then quantify the oscillations in the fitted predictions on a dense grid using principled curvature-based and edge-based metrics.\n\nYour implementation must adhere to the following definitions and requirements.\n\n1) Data-generating process. Let the true mean function be the pharmacological $E_{\\max}$ model\n$$\nm(x) = E_0 + \\frac{E_{\\max} x^{h}}{EC_{50}^{h} + x^{h}},\n$$\nwith fixed parameters $E_0=0$, $E_{\\max}=1$, $EC_{50}=0.3$, and $h=3$. For each simulated dataset, draw $n$ equispaced design points\n$$\nx_i = \\frac{i-1}{n-1},\\quad i\\in\\{1,2,\\dots,n\\},\n$$\nand generate outcomes\n$$\ny_i = m(x_i) + \\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2),\n$$\nindependently. All computations are unitless and require no physical unit conversions.\n\n2) Model fitting. For each dataset, fit two ordinary least squares (OLS) polynomial regressions with intercept using the monomial basis on $[0,1]$:\n$$\n\\hat{p}_d(x) = \\sum_{k=0}^{d} \\hat{\\beta}_{k} x^{k},\\quad d\\in\\{3,6\\},\n$$\nwhere the coefficient vector $\\hat{\\boldsymbol{\\beta}}$ minimizes the sum of squared residuals in the usual OLS sense.\n\n3) Grid-based evaluation and oscillation quantification. Let $G=\\{x^{\\ast}_j\\}_{j=1}^{M}$ be a dense, uniformly spaced grid on $[0,1]$ with $M=5001$ points, including both endpoints. For each fitted polynomial $\\hat{p}_d(x)$, compute the following quantities:\n- Roughness index (curvature-based). Define the second derivative\n$$\n\\hat{p}_d''(x) = \\sum_{k=2}^{d} k(k-1)\\hat{\\beta}_k x^{k-2}.\n$$\nApproximate the integral\n$$\nR_d = \\int_0^1 \\left(\\hat{p}_d''(x)\\right)^2\\,dx\n$$\nnumerically using the composite trapezoidal rule on $G$.\n- Edge oscillation amplitude (boundary-based). With $\\eta=0.05$, define\n$$\nL_d = \\operatorname{max}_{x\\in[0,\\eta]} \\hat{p}_d(x) - \\operatorname{min}_{x\\in[0,\\eta]} \\hat{p}_d(x),\\qquad\nU_d = \\operatorname{max}_{x\\in[1-\\eta,1]} \\hat{p}_d(x) - \\operatorname{min}_{x\\in[1-\\eta,1]} \\hat{p}_d(x),\n$$\nand $E_d = \\frac{L_d + U_d}{2}$.\n- Derivative sign-change count (monotonicity violations). Let the first derivative be\n$$\n\\hat{p}_d'(x) = \\sum_{k=1}^{d} k\\hat{\\beta}_k x^{k-1}.\n$$\nOn the grid $G$, count the number of sign changes in the sequence $\\{\\hat{p}_d'(x^{\\ast}_j)\\}_{j=1}^{M}$ after discarding values with absolute magnitude below a tolerance $\\tau=10^{-6}$ (these near-zero values should be treated as zero and not contribute spurious sign changes).\n\n4) Required outputs per dataset. For each dataset, compute the following three scalar outputs:\n- The roughness ratio $Q_R = R_6 / R_3$.\n- The edge-oscillation ratio $Q_E = E_6 / E_3$.\n- The change in derivative sign-change counts $\\Delta S = S_6 - S_3$, where $S_d$ is the sign-change count for degree $d$.\n\n5) Test suite. Your program must run the following four datasets (parameter triplets), where $n$ is the sample size, $\\sigma$ is the noise standard deviation, and $s$ is the pseudorandom seed used to generate the Gaussian noise:\n- Case A (happy path): $(n,\\sigma,s) = (11,\\,0.1,\\,202401)$.\n- Case B (near-interpolation boundary): $(n,\\sigma,s) = (7,\\,0.05,\\,202402)$.\n- Case C (high noise): $(n,\\sigma,s) = (11,\\,0.5,\\,202403)$.\n- Case D (larger sample): $(n,\\sigma,s) = (25,\\,0.05,\\,202404)$.\n\n6) Final output format. Your program should produce a single line of output containing a list of four entries, one for each test case in the order A, B, C, D. Each entry must itself be a list with three values in the order $[Q_R,\\,Q_E,\\,\\Delta S]$. The final printed line must have the exact format of a Python list of lists, for example:\n\"[[r11,r12,r13],[r21,r22,r23],[r31,r32,r33],[r41,r42,r43]]\"\nwhere each $r_{ij}$ is a real number or an integer as defined above. No additional text should be printed.\n\nHints are not permitted. All derivations must rely solely on core definitions such as the form of the OLS estimator, polynomial differentiation, and composite numerical integration. The answer must be expressed without physical units. Angles are not involved. Percentages must not be used; express any ratios as real-valued numbers. Your code must be a complete, runnable program that requires no user input and produces the specified output.", "solution": "The user has provided a computational problem in statistical modeling. The first and most critical step is to validate the problem statement.\n\n### Step 1: Extract Givens\n\nThe problem statement provides the following definitions, parameters, and procedures:\n\n*   **True Mean Function**: Pharmacological $E_{\\max}$ model:\n    $$\n    m(x) = E_0 + \\frac{E_{\\max} x^{h}}{EC_{50}^{h} + x^{h}}\n    $$\n*   **Model Parameters**:\n    -   $E_0=0$\n    -   $E_{\\max}=1$\n    -   $EC_{50}=0.3$\n    -   $h=3$\n*   **Data Generation**:\n    -   Design points: $n$ equispaced points on $[0, 1]$, $x_i = \\frac{i-1}{n-1},\\quad i\\in\\{1,2,\\dots,n\\}$.\n    -   Outcomes: $y_i = m(x_i) + \\varepsilon_i$, with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ independently.\n*   **Model Fitting**:\n    -   Ordinary Least Squares (OLS) polynomial regression with intercept.\n    -   Model form: $\\hat{p}_d(x) = \\sum_{k=0}^{d} \\hat{\\beta}_{k} x^{k}$.\n    -   Degrees to be fitted: $d \\in \\{3, 6\\}$.\n*   **Evaluation Grid**:\n    -   $G=\\{x^{\\ast}_j\\}_{j=1}^{M}$ is a dense, uniform grid on $[0,1]$.\n    -   $M=5001$ points.\n*   **Oscillation Metrics**:\n    -   **Roughness Index ($R_d$)**:\n        -   Second derivative: $\\hat{p}_d''(x) = \\sum_{k=2}^{d} k(k-1)\\hat{\\beta}_k x^{k-2}$.\n        -   Integral: $R_d = \\int_0^1 \\left(\\hat{p}_d''(x)\\right)^2\\,dx$, approximated using the composite trapezoidal rule on $G$.\n    -   **Edge Oscillation Amplitude ($E_d$)**:\n        -   Boundary region width: $\\eta=0.05$.\n        -   Left boundary amplitude: $L_d = \\operatorname{max}_{x\\in[0,\\eta]} \\hat{p}_d(x) - \\operatorname{min}_{x\\in[0,\\eta]} \\hat{p}_d(x)$.\n        -   Right boundary amplitude: $U_d = \\operatorname{max}_{x\\in[1-\\eta,1]} \\hat{p}_d(x) - \\operatorname{min}_{x\\in[1-\\eta,1]} \\hat{p}_d(x)$.\n        -   Average amplitude: $E_d = \\frac{L_d + U_d}{2}$.\n    -   **Derivative Sign-Change Count ($S_d$)**:\n        -   First derivative: $\\hat{p}_d'(x) = \\sum_{k=1}^{d} k\\hat{\\beta}_k x^{k-1}$.\n        -   Count sign changes in $\\{\\hat{p}_d'(x^{\\ast}_j)\\}_{j=1}^{M}$ after discarding values with magnitude below a tolerance $\\tau=10^{-6}$.\n*   **Required Outputs Per Dataset**:\n    -   Roughness ratio: $Q_R = R_6 / R_3$.\n    -   Edge-oscillation ratio: $Q_E = E_6 / E_3$.\n    -   Change in sign-change counts: $\\Delta S = S_6 - S_3$.\n*   **Test Cases**: $(n,\\sigma,s)$ where $n$ is sample size, $\\sigma$ is noise SD, $s$ is PRNG seed.\n    -   A: $(11,\\,0.1,\\,202401)$\n    -   B: $(7,\\,0.05,\\,202402)$\n    -   C: $(11,\\,0.5,\\,202403)$\n    -   D: $(25,\\,0.05,\\,202404)$\n*   **Final Output Format**: A string representation of a Python list of lists, e.g., `[[r11,r12,r13],[r21,r22,r23],[r31,r32,r33],[r41,r42,r43]]`.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is subjected to validation against the established criteria.\n\n*   **Scientifically Grounded**: The problem is impeccably grounded. It involves standard, well-established concepts from multiple STEM fields:\n    *   **Pharmacology**: The $E_{\\max}$ model is a cornerstone of dose-response modeling.\n    *   **Statistics**: Ordinary Least Squares (OLS) polynomial regression is a fundamental technique. The generation of data from a true model with additive Gaussian noise is a standard simulation paradigm.\n    *   **Numerical Analysis**: The use of polynomial fitting, numerical differentiation, and numerical integration (trapezoidal rule) are core topics. The emergence of oscillations (Runge's phenomenon) when fitting high-degree polynomials to equispaced points is a classic, well-documented result.\n    *   The problem does not violate any scientific principles or mathematical logic.\n\n*   **Well-Posed**: The problem is well-posed.\n    *   A unique OLS solution $\\hat{\\boldsymbol{\\beta}}$ exists and is computable, provided the design matrix is of full rank, which it will be for the given distinct design points $x_i$.\n    *   The metrics ($R_d, E_d, S_d$) are unambiguously defined.\n    *   The inputs (test cases) are fully specified, and the required outputs are explicitly defined. The problem is self-contained.\n\n*   **Objective**: The language is precise, quantitative, and free of subjectivity. All terms are formally defined (e.g., $E_{\\max}$ model, OLS, $R_d, E_d, S_d$). There are no metaphorical or ambiguous statements.\n\n*   **Other Flaws**:\n    1.  **Scientific Unsoundness**: None.\n    2.  **Non-Formalizable/Irrelevant**: The problem is fully formalizable and is centrally relevant to the specified topic of modeling non-linear relationships.\n    3.  **Incomplete/Contradictory**: All necessary parameters ($E_0, E_{\\max}, EC_{50}, h, n, \\sigma, d, M, \\eta, \\tau$) and seeds are provided. The setup is complete and consistent.\n    4.  **Unrealistic/Infeasible**: The parameters are well within a realistic range for both the simulation and numerical computation. The degrees of polynomials ($3$ and $6$) are low enough that standard double-precision arithmetic is sufficient to avoid catastrophic ill-conditioning of the Vandermonde matrix.\n    5.  **Ill-Posed/Poorly Structured**: None. The problem structure is clear and leads to a unique, meaningful set of outputs.\n    6.  **Pseudo-Profound/Trivial**: The problem is not trivial. It requires the correct application of several distinct concepts (data generation, regression, numerical calculus) and serves a clear pedagogical purpose: to demonstrate a known pitfall of a common modeling technique.\n    7.  **Outside Scientific Verifiability**: The results are computationally verifiable and reproducible given the specified seeds.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. It is a well-defined, scientifically sound, and reproducible computational task. I will now proceed to construct the solution.\n\n###\n### Solution Derivation\n\nThe objective is to simulate a common statistical modeling scenario in medicine and quantify the adverse effects of using overly complex models. Specifically, we will demonstrate Runge's phenomenon, where high-degree polynomial regression on equispaced data points leads to spurious oscillations, particularly near the interval boundaries.\n\n**1. Data Generation**\n\nFirst, we establish the ground truth. The data are generated from a smooth, sigmoidal dose-response curve, the $E_{\\max}$ model. Given the parameters $E_0=0$, $E_{\\max}=1$, $EC_{50}=0.3$, and $h=3$, the true mean function is:\n$$\nm(x) = \\frac{x^3}{0.3^3 + x^3} = \\frac{x^3}{0.027 + x^3}\n$$\nThis function is monotonic and smoothly saturates from $0$ to $1$ on the interval $[0, 1]$.\nFor a given sample size $n$, we generate $n$ equispaced design points $x_i$ on $[0,1]$ using the formula $x_i = (i-1)/(n-1)$, for $i=1, \\dots, n$. These are the \"doses\" in our simulation.\nThe observed outcomes $y_i$ are then generated by adding independent and identically distributed Gaussian noise to the true mean at each design point:\n$$\ny_i = m(x_i) + \\varepsilon_i, \\quad \\text{where } \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n$$\nThe standard deviation of the noise, $\\sigma$, is a key parameter controlling the difficulty of the estimation problem.\n\n**2. Polynomial Regression**\n\nWe fit the simulated data $(x_i, y_i)$ using polynomial models of degree $d=3$ and $d=6$. For a given degree $d$, the model is:\n$$\n\\hat{p}_d(x) = \\sum_{k=0}^d \\hat{\\beta}_k x^k\n$$\nThe coefficients $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_d]^T$ are determined by the method of Ordinary Least Squares (OLS). This involves finding the $\\hat{\\boldsymbol{\\beta}}$ that minimizes the sum of squared residuals, $\\sum_{i=1}^n (y_i - \\hat{p}_d(x_i))^2$.\nIn matrix notation, this is equivalent to solving the normal equations for the linear model $\\boldsymbol{y} = X_d \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\boldsymbol{y} = [y_1, \\dots, y_n]^T$ and $X_d$ is the $n \\times (d+1)$ design matrix (a Vandermonde matrix):\n$$\nX_d = \\begin{pmatrix}\n1 & x_1 & x_1^2 & \\cdots & x_1^d \\\\\n1 & x_2 & x_2^2 & \\cdots & x_2^d \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & x_n^2 & \\cdots & x_n^d\n\\end{pmatrix}\n$$\nThe OLS solution is formally given by $\\hat{\\boldsymbol{\\beta}} = (X_d^T X_d)^{-1} X_d^T \\boldsymbol{y}$. Computationally, this is solved using robust numerical linear algebra routines (such as those based on QR decomposition) to mitigate potential ill-conditioning of $X_d^T X_d$.\n\n**3. Quantification of Oscillation**\n\nTo objectively measure the \"wiggliness\" of the fitted polynomials, we use three metrics evaluated on a dense grid $G$ of $M=5001$ points in $[0,1]$. A higher degree polynomial ($d=6$) has more flexibility and is prone to overfitting the noise, which manifests as oscillations not present in the true underlying function $m(x)$.\n\n*   **Roughness Index ($R_d$):** The integral of the squared second derivative, $R_d = \\int_0^1 (\\hat{p}_d''(x))^2 dx$, is a classic measure of a function's roughness. A straight line has zero roughness. A function with high-frequency oscillations will have a large second derivative and thus a high roughness index.\n    The second derivative of our fitted polynomial is:\n    $$ \\hat{p}_d''(x) = \\frac{d^2}{dx^2} \\sum_{k=0}^d \\hat{\\beta}_k x^k = \\sum_{k=2}^d k(k-1) \\hat{\\beta}_k x^{k-2} $$\n    We approximate the integral for $R_d$ numerically using the composite trapezoidal rule on the grid $G=\\{x^{\\ast}_j\\}_{j=1}^{M}$:\n    $$ R_d \\approx \\sum_{j=1}^{M-1} \\frac{1}{2} \\left[ (\\hat{p}_d''(x^{\\ast}_j))^2 + (\\hat{p}_d''(x^{\\ast}_{j+1}))^2 \\right] (x^{\\ast}_{j+1} - x^{\\ast}_j) $$\n    Since the grid is uniform with spacing $\\Delta x = 1/(M-1)$, this simplifies to:\n    $$ R_d \\approx \\Delta x \\left( \\sum_{j=2}^{M-1} (\\hat{p}_d''(x^{\\ast}_j))^2 + \\frac{(\\hat{p}_d''(x^{\\ast}_1))^2 + (\\hat{p}_d''(x^{\\ast}_M))^2}{2} \\right) $$\n\n*   **Edge Oscillation ($E_d$):** Runge's phenomenon is characterized by large oscillations near the ends of the fitting interval. We quantify this directly. We define two boundary regions: a lower one $[0, \\eta]$ and an upper one $[1-\\eta, 1]$, with $\\eta=0.05$. In each region, we find the range of the fitted polynomial, i.e., its maximum value minus its minimum value.\n    $$ L_d = \\operatorname{max}_{x \\in [0, \\eta]} \\hat{p}_d(x) - \\operatorname{min}_{x \\in [0, \\eta]} \\hat{p}_d(x) $$\n    $$ U_d = \\operatorname{max}_{x \\in [1-\\eta, 1]} \\hat{p}_d(x) - \\operatorname{min}_{x \\in [1-\\eta, 1]} \\hat{p}_d(x) $$\n    The final metric, $E_d$, is the average of these two ranges, capturing the overall magnitude of boundary oscillations: $E_d = (L_d + U_d)/2$.\n\n*   **Derivative Sign Changes ($S_d$):** The true function $m(x)$ is strictly increasing on $[0,1]$, so its derivative is always positive. Spurious oscillations in the fitted polynomial $\\hat{p}_d(x)$ will introduce local maxima and minima, causing its derivative, $\\hat{p}_d'(x)$, to change sign.\n    The first derivative is:\n    $$ \\hat{p}_d'(x) = \\sum_{k=1}^d k \\hat{\\beta}_k x^{k-1} $$\n    We evaluate this derivative at all points on the grid $G$. To avoid counting spurious sign changes due to numerical noise, we first filter the sequence of derivative values, treating any value with absolute magnitude less than $\\tau=10^{-6}$ as zero. We then count the number of times the sign changes in the remaining non-zero sequence. Each such change corresponds to a violation of the monotonicity of the true function.\n\n**4. Final Algorithm and Output**\n\nThe entire process is executed for each of the four test cases. For each case, we perform the following steps:\n1.  Initialize the pseudo-random number generator with the given seed $s$.\n2.  Generate the data $(x_i, y_i)_{i=1}^n$ based on parameters $n$ and $\\sigma$.\n3.  Fit the polynomial $\\hat{p}_3(x)$ (degree $d=3$) and calculate its metrics $R_3, E_3, S_3$.\n4.  Fit the polynomial $\\hat{p}_6(x)$ (degree $d=6$) to the *same data* and calculate its metrics $R_6, E_6, S_6$.\n5.  Compute the final comparison outputs: the roughness ratio $Q_R = R_6/R_3$, the edge-oscillation ratio $Q_E = E_6/E_3$, and the difference in sign changes $\\Delta S = S_6 - S_3$.\n6.  These three values form the result for the test case. The results from all four cases are aggregated into a list of lists for the final output. The ratios $Q_R$ and $Q_E$ are expected to be greater than $1$, and $\\Delta S$ is expected to be non-negative, indicating that the degree $6$ polynomial is \"worse\" on all three measures of undesirable oscillation.", "answer": "```python\nimport numpy as np\n\ndef run_simulation(n: int, sigma: float, seed: int):\n    \"\"\"\n    Runs a single simulation for a given set of parameters.\n\n    Args:\n        n (int): Sample size.\n        sigma (float): Standard deviation of the noise.\n        seed (int): Seed for the pseudorandom number generator.\n\n    Returns:\n        list: A list containing [Q_R, Q_E, Delta_S].\n    \"\"\"\n    # 1) Data-generating process\n    E0 = 0.0\n    E_max = 1.0\n    EC50 = 0.3\n    h = 3.0\n\n    def m(x):\n        \"\"\"The true E_max mean function.\"\"\"\n        return E0 + (E_max * x**h) / (EC50**h + x**h)\n\n    # Set seed for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # Generate equispaced design points and noisy outcomes\n    x_i = np.linspace(0.0, 1.0, n)\n    epsilon_i = rng.normal(0, sigma, n)\n    y_i = m(x_i) + epsilon_i\n\n    # 3) Grid-based evaluation and oscillation quantification parameters\n    M = 5001\n    grid = np.linspace(0.0, 1.0, M)\n    eta = 0.05\n    tau = 1e-6\n\n    metrics = {}\n    degrees = [3, 6]\n\n    for d in degrees:\n        # 2) Model fitting\n        # Construct the Vandermonde matrix for polynomial regression\n        X_d = np.vander(x_i, d + 1, increasing=True)\n        \n        # Solve for OLS coefficients using np.linalg.lstsq\n        # Coeffs are returned in ascending order of power, matching our poly definition\n        coeffs, _, _, _ = np.linalg.lstsq(X_d, y_i, rcond=None)\n        \n        # Create a polynomial object from the coefficients\n        p_d = np.poly1d(np.flip(coeffs)) # np.poly1d expects descending power coeffs\n\n        # Evaluate metrics on the dense grid\n        \n        # Roughness Index (R_d)\n        p_d_dd = p_d.deriv(2)\n        p_d_dd_vals_on_grid = p_d_dd(grid)\n        integrand = p_d_dd_vals_on_grid**2\n        R_d = np.trapz(integrand, grid)\n        \n        # Edge Oscillation Amplitude (E_d)\n        p_d_vals_on_grid = p_d(grid)\n        \n        # Lower edge\n        lower_edge_mask = grid <= eta\n        lower_vals = p_d_vals_on_grid[lower_edge_mask]\n        L_d = np.max(lower_vals) - np.min(lower_vals)\n        \n        # Upper edge\n        upper_edge_mask = grid >= (1.0 - eta)\n        upper_vals = p_d_vals_on_grid[upper_edge_mask]\n        U_d = np.max(upper_vals) - np.min(upper_vals)\n        \n        E_d = (L_d + U_d) / 2.0\n        \n        # Derivative Sign-Change Count (S_d)\n        p_d_d = p_d.deriv(1)\n        p_d_d_vals_on_grid = p_d_d(grid)\n        \n        # Discard values with absolute magnitude below tolerance\n        filtered_derivs = p_d_d_vals_on_grid[np.abs(p_d_d_vals_on_grid) >= tau]\n        \n        # Count sign changes\n        if len(filtered_derivs) < 2:\n            S_d = 0\n        else:\n            signs = np.sign(filtered_derivs)\n            S_d = np.sum(np.diff(signs) != 0)\n\n        metrics[d] = {'R': R_d, 'E': E_d, 'S': S_d}\n\n    # 4) Required outputs per dataset\n    R3, E3, S3 = metrics[3]['R'], metrics[3]['E'], metrics[3]['S']\n    R6, E6, S6 = metrics[6]['R'], metrics[6]['E'], metrics[6]['S']\n\n    # Handle potential division by zero, although unlikely in this problem.\n    Q_R = R6 / R3 if R3 != 0 else np.inf\n    Q_E = E6 / E3 if E3 != 0 else np.inf\n    Delta_S = S6 - S3\n    \n    return [Q_R, Q_E, Delta_S]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    # 5) Test suite\n    test_cases = [\n        # (n, sigma, seed)\n        (11, 0.1, 202401),   # Case A\n        (7, 0.05, 202402),  # Case B\n        (11, 0.5, 202403),  # Case C\n        (25, 0.05, 202404),  # Case D\n    ]\n\n    results = []\n    for n, sigma, seed in test_cases:\n        result = run_simulation(n, sigma, seed)\n        results.append(result)\n\n    # 6) Final output format\n    # The format must be a string representation of a Python list of lists.\n    # We build this string manually to ensure exactness.\n    outer_list_str = []\n    for res in results:\n        inner_list_str = [f\"{val:.8g}\" for val in res] # Format numbers for consistency\n        outer_list_str.append(f\"[{','.join(inner_list_str)}]\")\n    final_output_str = f\"[{','.join(outer_list_str)}]\"\n\n    print(final_output_str)\n\nsolve()\n```", "id": "4974750"}, {"introduction": "Having seen the limitations of high-degree polynomials, we now turn to a more powerful and flexible alternative: regression splines. This practice challenges you to implement a natural cubic spline model, a popular method for capturing complex non-linear patterns without the erratic behavior of global polynomials [@problem_id:4974778]. You will construct a specialized basis that enforces the 'natural' linearity constraints at the data boundaries, providing a robust tool for modeling relationships like those between patient age and biomarkers.", "problem": "You are given paired observations of patient age and fasting low-density lipoprotein cholesterol, denoted by $\\{(x_i,y_i)\\}_{i=1}^n$. You will fit a natural cubic regression spline for cholesterol as a function of age using ordinary least squares and interior knots chosen as empirical age quantiles. The aim is to compute fitted cholesterol values at ages $40$, $60$, and $80$ years, in milligrams per deciliter (mg/dL), for multiple test datasets.\n\nFundamental base and model specification:\n- Let $f(x)$ be a regression function to be estimated from data by minimizing the sum of squared residuals $\\sum_{i=1}^n (y_i - f(x_i))^2$.\n- Consider the cubic regression spline space with interior knots $\\kappa_1 < \\kappa_2 < \\cdots < \\kappa_K$, where $\\kappa_j$ are the empirical quantiles of the observed ages. A natural cubic spline imposes the boundary conditions that the second derivative is zero outside the boundary knots.\n- Use the representation\n$$\nf(x) = \\beta_0 + \\beta_1 x + \\sum_{j=1}^{K} \\theta_j (x - \\kappa_j)_+^3,\n$$\nsubject to the linear constraints\n$$\n\\sum_{j=1}^{K} \\theta_j = 0, \\qquad \\sum_{j=1}^{K} \\theta_j \\kappa_j = 0,\n$$\nwhich enforce the natural boundary conditions (zero second derivative beyond the extreme knots). Here $(\\cdot)_+ = \\operatorname{max}\\{\\cdot,0\\}$.\n- Fit the parameters by ordinary least squares, minimizing $\\sum_{i=1}^n (y_i - f(x_i))^2$, using a basis construction that satisfies the constraints by parameterization. Specifically, denote $d_j(x) = (x-\\kappa_j)_+^3$ and let $\\mathbf{A}$ be the $2 \\times K$ matrix with first row all ones and second row $(\\kappa_1,\\dots,\\kappa_K)$. Let $\\mathbf{N}$ be any $K \\times (K-2)$ matrix whose columns form a basis for the null space of $\\mathbf{A}$; equivalently, $\\mathbf{A}\\mathbf{N} = \\mathbf{0}$. Then the natural cubic spline basis can be taken as $\\{1, x, \\sum_{j=1}^K N_{j1} d_j(x), \\dots, \\sum_{j=1}^K N_{j,K-2} d_j(x)\\}$, guaranteeing the constraints for any coefficients.\n\nQuantile-based knot selection:\n- Use interior knots at empirical quantiles of the ages with probabilities $0.2$, $0.4$, $0.6$, and $0.8$.\n- If duplicate quantiles occur, reduce to the strictly increasing set of unique knots. If after deduplication there are fewer than $2$ unique interior knots (i.e., $K < 2$), then the natural cubic spline degenerates to the linear model $f(x) = \\beta_0 + \\beta_1 x$.\n\nNumerical details:\n- Construct the design matrix using the above basis and estimate parameters by ordinary least squares.\n- Evaluate and report $f(40)$, $f(60)$, and $f(80)$ for each dataset.\n- All reported cholesterol values must be in milligrams per deciliter (mg/dL). Do not include units in the printed output; the unit specification here suffices.\n\nTest suite:\nImplement your program to evaluate the following three datasets, with ages in years and cholesterol in mg/dL. For each case, construct the specified $\\{x_i\\}$ and $y_i$ deterministically.\n\n- Test case A (general smooth nonlinearity, broad age coverage):\n  - Ages: $X_A = \\{30,31,32,\\dots,90\\}$ (inclusive), so $n = 61$.\n  - Cholesterol: for each $x \\in X_A$, define\n    $$\n    y = 180 + 0.6(x-60) - 0.02(x-60)^2 + 0.0003(x-60)^3.\n    $$\n\n- Test case B (strong ties at a few ages):\n  - Ages: $X_B$ contains $10$ replicates of $40$, $10$ replicates of $60$, and $10$ replicates of $80$, so $n = 30$.\n  - Cholesterol: for each $x \\in X_B$, define\n    $$\n    y = 195 + 0.8(x-60) - 0.015(x-60)^2.\n    $$\n\n- Test case C (ages clustered near extremes, potential knot degeneracy):\n  - Ages: $X_C$ consists of $25$ replicates of $40$, $10$ replicates of $41$, $5$ replicates of $42$, $5$ replicates of $78$, $10$ replicates of $79$, and $25$ replicates of $80$, so $n = 80$.\n  - Cholesterol: for each $x \\in X_C$, define\n    $$\n    y = 175 + 0.4(x-60) + 0.002(x-60)^3.\n    $$\n\nEvaluation targets:\n- For each test case, compute the fitted values $f(40)$, $f(60)$, and $f(80)$ using the fitted natural cubic spline with interior knots at the empirical quantiles $0.2$, $0.4$, $0.6$, and $0.8$ of the respective age sample, deduplicated as described. If fewer than $2$ unique interior knots remain, fit the linear model $f(x) = \\beta_0 + \\beta_1 x$.\n\nFinal output format:\n- Your program should produce a single line of output containing the nine results, ordered as $[f_A(40), f_A(60), f_A(80), f_B(40), f_B(60), f_B(80), f_C(40), f_C(60), f_C(80)]$, printed as a comma-separated list enclosed in square brackets, with plain numeric values. These are cholesterol values in mg/dL.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the theory of spline regression, a standard topic in statistical modeling. The problem is well-posed, with all data, model specifications, knot selection criteria, and estimation procedures explicitly and unambiguously defined. The test cases are deterministic and the overall problem is objective and verifiable.\n\nThe task is to fit a natural cubic regression spline to three distinct datasets of age and cholesterol, and then compute the fitted cholesterol values at ages $40$, $60$, and $80$. The solution involves several steps: data generation for each test case, selection of interior knots based on empirical quantiles, construction of a suitable basis for the natural cubic spline that incorporates the boundary constraints, estimation of model parameters via ordinary least squares (OLS), and finally, prediction at the target age points.\n\nFirst, for each test case, the paired observations $\\{(x_i, y_i)\\}_{i=1}^n$ are generated according to the provided formulas. Here, $x_i$ represents age and $y_i$ represents LDL cholesterol.\n\nThe second step is to determine the interior knots $\\{\\kappa_j\\}_{j=1}^K$. The problem specifies using the empirical quantiles of the observed ages $\\{x_i\\}$ for probabilities $0.2$, $0.4$, $0.6$, and $0.8$. After computing these quantiles, duplicates are removed to yield a strictly increasing set of $K$ unique knots. The problem mandates a degeneracy condition: if aƒter deduplication, fewer than $2$ knots remain (i.e., $K < 2$), a simple linear model $f(x) = \\beta_0 + \\beta_1 x$ must be fitted instead of a spline. For all three test cases provided, this condition is not met, and a spline model is appropriate.\n\nThe core of the problem lies in the construction of the basis for the natural cubic spline. A natural cubic spline is a cubic spline with the additional constraint that the function is linear beyond the boundary knots. This translates to the second derivative being zero at the boundaries. The model is represented as:\n$$\nf(x) = \\beta_0 + \\beta_1 x + \\sum_{j=1}^{K} \\theta_j d_j(x), \\quad \\text{where } d_j(x) = (x - \\kappa_j)_+^3\n$$\nThe notation $(u)_+$ stands for the positive part of $u$, i.e., $\\operatorname{max}\\{u, 0\\}$. The natural spline constraints on the parameters $\\theta_j$ are:\n$$\n\\sum_{j=1}^{K} \\theta_j = 0 \\quad \\text{and} \\quad \\sum_{j=1}^{K} \\theta_j \\kappa_j = 0\n$$\nTo incorporate these constraints directly into the basis, we use a reparameterization. Let $\\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_K)^T$. The constraints can be written as $\\mathbf{A}\\boldsymbol{\\theta} = \\mathbf{0}$, where $\\mathbf{A}$ is the $2 \\times K$ matrix:\n$$\n\\mathbf{A} = \\begin{pmatrix} 1 & 1 & \\cdots & 1 \\\\ \\kappa_1 & \\kappa_2 & \\cdots & \\kappa_K \\end{pmatrix}\n$$\nAny vector $\\boldsymbol{\\theta}$ satisfying these constraints must lie in the null space of $\\mathbf{A}$. Let $\\mathbf{N}$ be a $K \\times (K-2)$ matrix whose columns form a basis for this null space. Then we can write $\\boldsymbol{\\theta} = \\mathbf{N}\\boldsymbol{\\gamma}$ for some vector of new parameters $\\boldsymbol{\\gamma} = (\\gamma_1, \\dots, \\gamma_{K-2})^T$. Substituting this into the model equation gives:\n$$\nf(x) = \\beta_0 + \\beta_1 x + \\sum_{j=1}^{K} (\\mathbf{N}\\boldsymbol{\\gamma})_j d_j(x) = \\beta_0 + \\beta_1 x + \\sum_{k=1}^{K-2} \\gamma_k \\left( \\sum_{j=1}^K N_{jk} d_j(x) \\right)\n$$\nThis defines a new set of $K-2$ basis functions $B_k(x) = \\sum_{j=1}^K N_{jk} d_j(x)$. The full set of basis functions for the model is $\\{1, x, B_1(x), \\dots, B_{K-2}(x)\\}$. The total number of parameters to estimate is $2 + (K-2) = K$.\n\nWith this basis, we construct the $n \\times K$ design matrix $\\mathbf{X}$ for the OLS regression. The $i$-th row of $\\mathbf{X}$ corresponds to the observation $x_i$ and is given by $[1, x_i, B_1(x_i), \\dots, B_{K-2}(x_i)]$. Let $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\gamma_1, \\dots, \\gamma_{K-2})^T$ be the vector of coefficients. We find the OLS estimate $\\hat{\\boldsymbol{\\beta}}$ by minimizing the sum of squared residuals $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2$, which is solved by $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{\\dagger}\\mathbf{X}^T\\mathbf{y}$, where $(\\cdot)^{\\dagger}$ denotes the Moore-Penrose pseudoinverse, typically computed via a stable numerical method like singular value decomposition.\n\nFinally, to predict the cholesterol value at a new age $x_{new}$, we form a corresponding row vector $\\mathbf{x}_{new}^T = [1, x_{new}, B_1(x_{new}), \\dots, B_{K-2}(x_{new})]$ and compute the fitted value as $\\hat{f}(x_{new}) = \\mathbf{x}_{new}^T \\hat{\\boldsymbol{\\beta}}$. This procedure is applied for $x_{new} = 40, 60, 80$ for each test case.\n\n- **Test Case A**: Ages $X_A = \\{30, 31, \\dots, 90\\}$. Quantile calculation yields $K=4$ unique knots at $\\{42.0, 54.0, 66.0, 78.0\\}$. The model has $K=4$ parameters.\n- **Test Case B**: Ages $X_B$ consist of $10$ replicates each of $\\{40, 60, 80\\}$. Quantile calculation yields raw knots $\\{40.0, 60.0, 60.0, 80.0\\}$, which results in $K=3$ unique knots at $\\{40.0, 60.0, 80.0\\}$. The model has $K=3$ parameters. Since there are only $3$ unique age values in the data, the $3$-parameter model provides an exact fit to the mean cholesterol values at these ages.\n- **Test Case C**: Ages $X_C$ are clustered. Quantile calculation yields $K=4$ unique knots at $\\{40.0, 41.0, 79.0, 80.0\\}$. The model has $K=4$ parameters.\n\nThe implementation uses `numpy` for numerical computations and `scipy.linalg.null_space` to find the basis $\\mathbf{N}$ for the constraint matrix's null space.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import null_space\n\ndef fit_and_predict_spline(x_obs, y_obs, x_pred):\n    \"\"\"\n    Fits a natural cubic spline and predicts at new data points.\n    \n    Args:\n        x_obs (np.ndarray): Observed independent variable values (ages).\n        y_obs (np.ndarray): Observed dependent variable values (cholesterol).\n        x_pred (np.ndarray): Points at which to predict.\n\n    Returns:\n        np.ndarray: Predicted values at x_pred.\n    \"\"\"\n    knot_probs = [0.2, 0.4, 0.6, 0.8]\n    knots_raw = np.quantile(x_obs, knot_probs, method='linear')\n    unique_knots = sorted(list(set(knots_raw)))\n    K = len(unique_knots)\n\n    n_obs = len(x_obs)\n    n_pred = len(x_pred)\n\n    # Degeneracy condition: if K < 2, fit a simple linear model.\n    if K < 2:\n        # Design matrix for linear model (intercept and slope)\n        X_obs = np.vander(x_obs, 2, increasing=True)\n        # Solve for coefficients using least squares\n        coeffs, _, _, _ = np.linalg.lstsq(X_obs, y_obs, rcond=None)\n        # Prediction matrix\n        X_pred_mat = np.vander(x_pred, 2, increasing=True)\n        # Predict\n        y_pred = X_pred_mat @ coeffs\n        return y_pred\n\n    # Natural cubic spline model (K >= 2)\n    # The basis has K functions: {1, x, B_1(x), ..., B_{K-2}(x)}\n    \n    # Construct the D matrix from truncated power basis functions\n    # d_j(x) = (x - k_j)_+^3\n    knots_arr = np.array(unique_knots)\n    D_obs = np.maximum(0, x_obs[:, None] - knots_arr)**3\n    \n    # Construct constraint matrix A\n    A = np.ones((2, K))\n    A[1, :] = knots_arr\n    \n    # Find N, a basis for the null space of A.\n    # For K=2, N will be a 2x0 empty matrix.\n    if K > 2:\n        N = null_space(A)\n    else: # K=2\n        N = np.empty((K, 0))\n    \n    # Construct the full design matrix X\n    X_obs = np.zeros((n_obs, K))\n    X_obs[:, 0] = 1.0  # Intercept\n    X_obs[:, 1] = x_obs  # Linear term\n    if K > 2:\n        X_obs[:, 2:] = D_obs @ N  # Spline basis functions\n\n    # Solve for coefficients using least squares\n    coeffs, _, _, _ = np.linalg.lstsq(X_obs, y_obs, rcond=None)\n    \n    # Perform prediction\n    # Construct D matrix for prediction points\n    D_pred = np.maximum(0, x_pred[:, None] - knots_arr)**3\n    \n    # Construct prediction matrix\n    X_pred_mat = np.zeros((n_pred, K))\n    X_pred_mat[:, 0] = 1.0\n    X_pred_mat[:, 1] = x_pred\n    if K > 2:\n        X_pred_mat[:, 2:] = D_pred @ N\n\n    y_pred = X_pred_mat @ coeffs\n    return y_pred\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    eval_ages = np.array([40, 60, 80])\n    all_results = []\n\n    # Test Case A\n    x_a = np.arange(30, 91, dtype=float)\n    y_a = 180.0 + 0.6 * (x_a - 60.0) - 0.02 * (x_a - 60.0)**2 + 0.0003 * (x_a - 60.0)**3\n    results_a = fit_and_predict_spline(x_a, y_a, eval_ages)\n    all_results.extend(results_a)\n\n    # Test Case B\n    x_b = np.repeat([40.0, 60.0, 80.0], 10)\n    y_b = 195.0 + 0.8 * (x_b - 60.0) - 0.015 * (x_b - 60.0)**2\n    results_b = fit_and_predict_spline(x_b, y_b, eval_ages)\n    all_results.extend(results_b)\n\n    # Test Case C\n    x_c = np.concatenate([\n        np.repeat(40.0, 25),\n        np.repeat(41.0, 10),\n        np.repeat(42.0, 5),\n        np.repeat(78.0, 5),\n        np.repeat(79.0, 10),\n        np.repeat(80.0, 25)\n    ])\n    y_c = 175.0 + 0.4 * (x_c - 60.0) + 0.002 * (x_c - 60.0)**3\n    results_c = fit_and_predict_spline(x_c, y_c, eval_ages)\n    all_results.extend(results_c)\n\n    # Format and print the final output string\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "4974778"}]}