{"hands_on_practices": [{"introduction": "To truly master statistical modeling, we must look under the hood of the methods we use. This first practice takes you to the core of logistic regression, the workhorse for modeling binary outcomes in medicine. By deriving the log-likelihood function, its gradient (the score vector), and its Hessian matrix from first principles, you will build a foundational understanding of how model parameters are estimated via maximum likelihood [@problem_id:4988404]. This exercise reveals the mathematical machinery that optimization algorithms like Newton-Raphson employ to fit the model to data.", "problem": "A clinical data science team is developing a Generalized Linear Model (GLM) to estimate the probability that a postoperative patient develops sepsis within $48$ hours based on covariates that include age, surgical duration, baseline white blood cell count, and a comorbidity score. For patient $i$, let the binary outcome be $y_{i} \\in \\{0,1\\}$, where $y_{i}=1$ indicates sepsis and $y_{i}=0$ indicates no sepsis. Let the covariate vector be $x_{i} \\in \\mathbb{R}^{p}$ and the regression parameter be $\\beta \\in \\mathbb{R}^{p}$, with the linear predictor defined by $\\eta_{i} = x_{i}^{\\top}\\beta$. Assume conditional independence of outcomes given covariates and parameters and that, for each $i$, $Y_{i}$ follows a Bernoulli distribution with mean $\\mu_{i} = \\mathbb{E}[Y_{i} \\mid x_{i}]$.\n\nStarting from the following fundamental bases:\n- The Bernoulli probability mass function (pmf) for an independent observation is $p(y_{i} \\mid \\mu_{i}) = \\mu_{i}^{y_{i}} (1-\\mu_{i})^{1-y_{i}}$ for $y_{i} \\in \\{0,1\\}$.\n- The canonical link for the Bernoulli distribution in a Generalized Linear Model (GLM) is the logit link, defined by $\\ln\\!\\left(\\frac{\\mu_{i}}{1-\\mu_{i}}\\right) = \\eta_{i} = x_{i}^{\\top}\\beta$.\n\nUsing only these bases and the definition of the exponential family, derive the log-likelihood function $\\ell(\\beta)$ for the parameter vector $\\beta$ under the logit link. Then compute the gradient (score) vector and Hessian (observed information) matrix of $\\ell(\\beta)$ with respect to $\\beta$, expressing them in terms of $\\{(y_{i}, x_{i})\\}_{i=1}^{n}$ and $\\beta$. Provide simplified closed-form expressions; no numerical approximation is required. Express any logarithms using $\\ln$ and exponentials using $\\exp$. Your final answer must be the three derived expressions.", "solution": "The user wants me to act as a STEM professor and solve a problem related to Generalized Linear Models (GLMs).\n\nHere's my plan:\n1.  **Validate the Problem**:\n    -   **Step 1: Extract Givens**: I'll list all the provided information verbatim.\n        -   Outcome: $y_i \\in \\{0, 1\\}$ for patient $i$. $y_i=1$ is sepsis, $y_i=0$ is no sepsis.\n        -   Covariates: $x_i \\in \\mathbb{R}^p$.\n        -   Parameters: $\\beta \\in \\mathbb{R}^p$.\n        -   Linear predictor: $\\eta_i = x_i^{\\top}\\beta$.\n        -   Distribution: $Y_i \\sim \\text{Bernoulli}(\\mu_i)$.\n        -   Mean: $\\mu_i = \\mathbb{E}[Y_i \\mid x_i]$.\n        -   PMF: $p(y_i \\mid \\mu_i) = \\mu_i^{y_i} (1-\\mu_i)^{1-y_i}$.\n        -   Link function: Canonical logit link, $\\ln(\\frac{\\mu_i}{1-\\mu_i}) = \\eta_i = x_i^{\\top}\\beta$.\n        -   Assumption: Conditional independence of outcomes.\n        -   Task: Derive the log-likelihood $\\ell(\\beta)$, its gradient (score) $\\nabla_{\\beta} \\ell(\\beta)$, and its Hessian $\\nabla_{\\beta}^2 \\ell(\\beta)$.\n\n    -   **Step 2: Validate Using Extracted Givens**:\n        -   **Scientifically Grounded**: The problem is set within the well-established statistical framework of Generalized Linear Models. The Bernoulli distribution for binary outcomes and the logit link are standard for logistic regression, which is a type of GLM. All concepts are standard in statistics and machine learning. This is sound.\n        -   **Well-Posed**: The problem asks for the derivation of well-defined statistical quantities (log-likelihood, gradient, Hessian) from fundamental principles. A unique solution exists for these derivations. The problem is well-posed.\n        -   **Objective**: The language is precise and technical. No subjective claims.\n        -   **Completeness**: All necessary information is provided. The PMF, the link function, and the definition of the linear predictor are all explicitly given.\n        -   **Consistency**: The givens are consistent. The logit link is indeed the canonical link for the Bernoulli distribution.\n        -   **No other flaws**: The problem is not trivial, not metaphorical, not unfalsifiable. It's a standard and important derivation in statistical theory.\n\n    -   **Step 3: Verdict and Action**: The problem is **valid**. I will proceed to the solution.\n\n2.  **Solve the Problem**:\n    -   **Goal**: Derive $\\ell(\\beta)$, $\\nabla_{\\beta} \\ell(\\beta)$, and $\\nabla_{\\beta}^2 \\ell(\\beta)$.\n\n    -   **Part 1: Derive the log-likelihood $\\ell(\\beta)$**.\n        -   The likelihood for a single observation $y_i$ is $L_i(\\mu_i) = p(y_i \\mid \\mu_i) = \\mu_i^{y_i} (1-\\mu_i)^{1-y_i}$.\n        -   The log-likelihood for a single observation is $\\ell_i(\\mu_i) = \\ln(L_i(\\mu_i)) = y_i \\ln(\\mu_i) + (1-y_i) \\ln(1-\\mu_i)$.\n        -   I need to express this in terms of $\\beta$. The link is $\\eta_i = x_i^\\top \\beta = \\ln(\\frac{\\mu_i}{1-\\mu_i})$.\n        -   I need to solve for $\\mu_i$ in terms of $\\eta_i$.\n            -   $\\exp(\\eta_i) = \\frac{\\mu_i}{1-\\mu_i}$\n            -   $(1-\\mu_i)\\exp(\\eta_i) = \\mu_i$\n            -   $\\exp(\\eta_i) - \\mu_i \\exp(\\eta_i) = \\mu_i$\n            -   $\\exp(\\eta_i) = \\mu_i(1 + \\exp(\\eta_i))$\n            -   $\\mu_i = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} = \\frac{1}{1 + \\exp(-\\eta_i)}$. This is the logistic sigmoid function, often denoted $\\sigma(\\eta_i)$.\n        -   I also need $1-\\mu_i$:\n            -   $1 - \\mu_i = 1 - \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} = \\frac{1 + \\exp(\\eta_i) - \\exp(\\eta_i)}{1 + \\exp(\\eta_i)} = \\frac{1}{1 + \\exp(\\eta_i)} = \\sigma(-\\eta_i)$.\n        -   Now, substitute these into $\\ell_i$.\n            -   $\\ell_i(\\beta) = y_i \\ln\\left(\\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)}\\right) + (1-y_i) \\ln\\left(\\frac{1}{1 + \\exp(\\eta_i)}\\right)$.\n            -   Simplify using log properties:\n            -   $\\ell_i(\\beta) = y_i (\\ln(\\exp(\\eta_i)) - \\ln(1 + \\exp(\\eta_i))) + (1-y_i) (-\\ln(1 + \\exp(\\eta_i)))$.\n            -   $\\ell_i(\\beta) = y_i \\eta_i - y_i \\ln(1 + \\exp(\\eta_i)) - \\ln(1 + \\exp(\\eta_i)) + y_i \\ln(1 + \\exp(\\eta_i))$.\n            -   $\\ell_i(\\beta) = y_i \\eta_i - \\ln(1 + \\exp(\\eta_i))$.\n            -   This is the log-likelihood for one observation in terms of $\\eta_i$. Substitute $\\eta_i = x_i^\\top \\beta$:\n            -   $\\ell_i(\\beta) = y_i x_i^\\top \\beta - \\ln(1 + \\exp(x_i^\\top \\beta))$.\n        -   The total log-likelihood for $n$ independent observations is the sum:\n            -   $\\ell(\\beta) = \\sum_{i=1}^n \\ell_i(\\beta) = \\sum_{i=1}^n \\left( y_i x_i^\\top \\beta - \\ln(1 + \\exp(x_i^\\top \\beta)) \\right)$.\n        -   This is the first part of the answer.\n\n    -   **Part 2: Derive the gradient (score) vector $\\nabla_{\\beta} \\ell(\\beta)$**.\n        -   The gradient is a $p \\times 1$ vector of partial derivatives $\\frac{\\partial \\ell}{\\partial \\beta_j}$ for $j=1, ..., p$.\n        -   Let's find the gradient of the total log-likelihood: $\\nabla_{\\beta} \\ell(\\beta) = \\sum_{i=1}^n \\nabla_{\\beta} \\ell_i(\\beta)$.\n        -   We need to compute $\\nabla_{\\beta} \\ell_i(\\beta)$. Let's use the chain rule. $\\ell_i$ is a function of $\\eta_i$, which is a function of $\\beta$.\n        -   $\\nabla_{\\beta} \\ell_i(\\beta) = \\frac{\\partial \\ell_i}{\\partial \\eta_i} \\nabla_{\\beta} \\eta_i$.\n        -   $\\eta_i = x_i^\\top \\beta = \\sum_{j=1}^p x_{ij} \\beta_j$. So, $\\nabla_{\\beta} \\eta_i = x_i$. This is a $p \\times 1$ column vector.\n        -   Now compute the scalar derivative $\\frac{\\partial \\ell_i}{\\partial \\eta_i}$:\n            -   $\\ell_i(\\eta_i) = y_i \\eta_i - \\ln(1 + \\exp(\\eta_i))$.\n            -   $\\frac{\\partial \\ell_i}{\\partial \\eta_i} = y_i - \\frac{1}{1 + \\exp(\\eta_i)} \\cdot \\exp(\\eta_i) = y_i - \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} = y_i - \\mu_i$.\n        -   So, $\\nabla_{\\beta} \\ell_i(\\beta) = (y_i - \\mu_i) x_i$.\n        -   Summing over all observations:\n            -   $\\nabla_{\\beta} \\ell(\\beta) = \\sum_{i=1}^n (y_i - \\mu_i) x_i$.\n        -   Where $\\mu_i = \\frac{\\exp(x_i^\\top \\beta)}{1 + \\exp(x_i^\\top \\beta)}$.\n        -   Let's write this in matrix form. Let $X$ be the $n \\times p$ design matrix where row $i$ is $x_i^\\top$. Let $y$ be the $n \\times 1$ vector of outcomes and $\\mu$ be the $n \\times 1$ vector of means.\n        -   $\\nabla_{\\beta} \\ell(\\beta) = \\sum_{i=1}^n x_i (y_i - \\mu_i) = X^\\top (y - \\mu)$. This is a compact and standard form. The problem asks for the expression in terms of $\\{(y_i, x_i)\\}_{i=1}^n$ and $\\beta$, so the summation form is more direct.\n        -   $\\nabla_{\\beta} \\ell(\\beta) = \\sum_{i=1}^n \\left( y_i - \\frac{\\exp(x_i^\\top \\beta)}{1 + \\exp(x_i^\\top \\beta)} \\right) x_i$.\n        -   This is the second part of the answer.\n\n    -   **Part 3: Derive the Hessian matrix $\\nabla_{\\beta}^2 \\ell(\\beta)$**.\n        -   The Hessian is a $p \\times p$ matrix of second partial derivatives. $\\nabla_{\\beta}^2 \\ell(\\beta) = \\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\beta^\\top}$.\n        -   $\\nabla_{\\beta}^2 \\ell(\\beta) = \\sum_{i=1}^n \\nabla_{\\beta}^2 \\ell_i(\\beta)$.\n        -   Let's compute the Hessian for a single observation: $\\nabla_{\\beta}^2 \\ell_i(\\beta) = \\nabla_{\\beta} \\left( (y_i - \\mu_i) x_i \\right)$.\n        -   Using the product rule for vector calculus: $\\nabla_{\\beta} (f(\\beta) \\mathbf{v}(\\beta)) = (\\nabla_{\\beta} f(\\beta)) \\mathbf{v}(\\beta)^\\top + f(\\beta) \\nabla_{\\beta} \\mathbf{v}(\\beta)$.\n        -   Here, $f(\\beta) = y_i - \\mu_i$ and $\\mathbf{v}(\\beta) = x_i$ is a constant vector with respect to $\\beta$. So the second term is zero.\n        -   $\\nabla_{\\beta}^2 \\ell_i(\\beta) = \\nabla_{\\beta} (y_i - \\mu_i) x_i^\\top$.\n        -   The gradient is taken with respect to the row vector $\\beta^\\top$, so the result is a $p \\times p$ matrix.\n        -   We need $\\nabla_{\\beta} (y_i - \\mu_i) = \\nabla_{\\beta} (-\\mu_i) = - \\nabla_{\\beta} \\mu_i$.\n        -   Let's use the chain rule again. $\\mu_i$ is a function of $\\eta_i$, which is a function of $\\beta$.\n        -   $\\nabla_{\\beta} \\mu_i = \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\nabla_{\\beta} \\eta_i = \\frac{\\partial \\mu_i}{\\partial \\eta_i} x_i$.\n        -   Now compute the scalar derivative $\\frac{\\partial \\mu_i}{\\partial \\eta_i}$:\n            -   $\\mu_i = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)}$.\n            -   Using the quotient rule: $\\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\frac{\\exp(\\eta_i)(1 + \\exp(\\eta_i)) - \\exp(\\eta_i)\\exp(\\eta_i)}{(1 + \\exp(\\eta_i))^2}$.\n            -   $\\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\frac{\\exp(\\eta_i)}{(1 + \\exp(\\eta_i))^2} = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} \\cdot \\frac{1}{1 + \\exp(\\eta_i)} = \\mu_i (1-\\mu_i)$.\n            -   This is a general result for GLMs: the derivative of the mean function with respect to the linear predictor is the variance function (for the canonical link). For Bernoulli, $V(\\mu_i) = \\mu_i(1-\\mu_i)$.\n        -   So, $\\nabla_{\\beta} \\mu_i = \\mu_i(1-\\mu_i) x_i$.\n        -   Substituting back: $\\nabla_{\\beta}^2 \\ell_i(\\beta) = - (\\nabla_{\\beta} \\mu_i) x_i^\\top = - (\\mu_i(1-\\mu_i)x_i) x_i^\\top = - \\mu_i(1-\\mu_i) x_i x_i^\\top$.\n        -   Note that $x_i x_i^\\top$ is a $p \\times p$ outer product matrix.\n        -   The total Hessian is the sum over all observations:\n            -   $\\nabla_{\\beta}^2 \\ell(\\beta) = \\sum_{i=1}^n \\left( - \\mu_i(1-\\mu_i) x_i x_i^\\top \\right) = - \\sum_{i=1}^n \\mu_i(1-\\mu_i) x_i x_i^\\top$.\n        -   Again, substituting the expression for $\\mu_i$:\n            -   $\\mu_i(1-\\mu_i) = \\frac{\\exp(x_i^\\top \\beta)}{1 + \\exp(x_i^\\top \\beta)} \\cdot \\frac{1}{1 + \\exp(x_i^\\top \\beta)} = \\frac{\\exp(x_i^\\top \\beta)}{(1 + \\exp(x_i^\\top \\beta))^2}$.\n        -   So, $\\nabla_{\\beta}^2 \\ell(\\beta) = - \\sum_{i=1}^n \\frac{\\exp(x_i^\\top \\beta)}{(1 + \\exp(x_i^\\top \\beta))^2} x_i x_i^\\top$.\n        -   In matrix form, this is $\\nabla_{\\beta}^2 \\ell(\\beta) = - X^\\top W X$, where $W$ is a diagonal matrix with diagonal elements $W_{ii} = \\mu_i(1-\\mu_i)$. This is the standard form for iteratively reweighted least squares (IRLS) used to fit GLMs.\n        -   The expression with the summation is what the problem asks for.\n\n3.  **Final Review**:\n    -   Log-likelihood: $\\ell(\\beta) = \\sum_{i=1}^{n} \\left( y_i x_i^\\top \\beta - \\ln(1 + \\exp(x_i^\\top \\beta)) \\right)$. Looks correct.\n    -   Gradient (Score): $\\nabla_{\\beta} \\ell(\\beta) = \\sum_{i=1}^{n} (y_i - \\mu_i) x_i = \\sum_{i=1}^{n} \\left( y_i - \\frac{\\exp(x_i^\\top \\beta)}{1 + \\exp(x_i^\\top \\beta)} \\right) x_i$. Looks correct. This matches the general form $\\sum (y_i - \\mu_i) / V(\\mu_i) \\cdot (\\partial \\mu_i / \\partial \\eta_i) \\cdot (\\partial \\eta_i / \\partial \\beta_j)$. For canonical link, $\\partial \\mu_i / \\partial \\eta_i = V(\\mu_i)$, so it simplifies to $\\sum (y_i - \\mu_i) x_i$.\n    -   Hessian (Observed Information): $\\nabla_{\\beta}^{2} \\ell(\\beta) = - \\sum_{i=1}^{n} \\mu_i (1-\\mu_i) x_i x_i^\\top = - \\sum_{i=1}^{n} \\frac{\\exp(x_i^\\top \\beta)}{(1 + \\exp(x_i^\\top \\beta))^2} x_i x_i^\\top$. Looks correct. The Hessian is negative definite since $\\mu_i(1-\\mu_i) > 0$ and $x_i x_i^\\top$ is positive semi-definite, which means the log-likelihood function is concave, which is required for a unique maximum likelihood estimate. This confirms the derivation.\n\n4.  **Formatting**:\n    -   All math entities must be in LaTeX. I will be careful with this. Every variable, number, operator.\n    -   No markdown headers.\n    -   The final answer must be a single `pmatrix` with the three expressions. I'll need to use `\\begin{aligned}` or similar inside the matrix to format them nicely. The request is a row matrix. So `\\begin{pmatrix} \\text{expr1} & \\text{expr2} & \\text{expr3} \\end{pmatrix}`. This will be extremely wide and potentially unreadable. Maybe the intent is to list them. But the rule says \"row matrix\". I'll use `\\begin{array}` to stack them vertically inside a single `pmatrix` element if a row matrix is too wide. No, that would be a column matrix.\n    Okay, I will stick to the rule and make it a row matrix.\n\n    Let's re-read the rules. \"Composite Answer: Write multiple answers as a row matrix using the LaTeX pmatrix environment. For example, if the answer is a=1, b=2, write `1 & 2` inside a LaTeX pmatrix environment.\"\n    This suggests that if the answers are expressions, they should be listed similarly.\n    Let's define the final expressions:\n    $E_1 = \\sum_{i=1}^{n} \\left( y_i x_i^\\top \\beta - \\ln(1 + \\exp(x_i^\\top \\beta)) \\right)$\n    $E_2 = \\sum_{i=1}^{n} \\left( y_i - \\frac{\\exp(x_i^\\top \\beta)}{1 + \\exp(x_i^\\top \\beta)} \\right) x_i$\n    $E_3 = - \\sum_{i=1}^{n} \\frac{\\exp(x_i^\\top \\beta)}{(1 + \\exp(x_i^\\top \\beta))^2} x_i x_i^\\top$\n\n    The final answer box would contain:\n    `\\boxed{\\begin{pmatrix} \\sum_{i=1}^{n} \\left( y_i x_i^\\top \\beta - \\ln(1 + \\exp(x_i^\\top \\beta)) \\right) & \\sum_{i=1}^{n} \\left( y_i - \\frac{\\exp(x_i^\\top \\beta)}{1 + \\exp(x_i^\\top \\beta)} \\right) x_i & - \\sum_{i=1}^{n} \\frac{\\exp(x_i^\\top \\beta)}{(1 + \\exp(x_i^\\top \\beta))^2} x_i x_i^\\top \\end{pmatrix}}`\n\n    This seems to follow the instructions precisely, even if it results in a very wide matrix. I will write out the solution text and then format the final answer. The solution text needs to detail every step of the derivation clearly.\n\n    The solution should follow this structure:\n    1.  Start with the likelihood of a single observation, $L_i$.\n    2.  Take the logarithm to get $\\ell_i$.\n    3.  Express $\\mu_i$ in terms of $\\eta_i = x_i^\\top\\beta$ using the given logit link.\n    4.  Substitute this into $\\ell_i$ to get $\\ell_i(\\beta)$.\n    5.  Sum over all $n$ observations to get the total log-likelihood $\\ell(\\beta)$. This is the first result.\n    6.  Compute the gradient of $\\ell(\\beta)$ by summing the gradients of $\\ell_i(\\beta)$.\n    7.  Use the chain rule: $\\nabla_{\\beta}\\ell_i(\\beta) = \\frac{\\partial\\ell_i}{\\partial\\eta_i}\\nabla_{\\beta}\\eta_i$.\n    8.  Calculate $\\frac{\\partial\\ell_i}{\\partial\\eta_i}$ and $\\nabla_{\\beta}\\eta_i$.\n    9.  Combine to get $\\nabla_{\\beta}\\ell_i(\\beta)$ and sum to get $\\nabla_{\\beta}\\ell(\\beta)$. This is the second result.\n    10. Compute the Hessian of $\\ell(\\beta)$ by summing the Hessians of $\\ell_i(\\beta)$.\n    11. Use the chain/product rule: $\\nabla_{\\beta}^2\\ell_i(\\beta) = \\nabla_{\\beta} [(\\frac{\\partial\\ell_i}{\\partial\\eta_i}) x_i^\\top]$. Note, my previous derivation was slightly different. $\\nabla_{\\beta} \\ell_i(\\beta) = (\\frac{\\partial\\ell_i}{\\partial\\eta_i}) x_i$. So the Hessian is $\\nabla_{\\beta} ((\\frac{\\partial\\ell_i}{\\partial\\eta_i}) x_i)$. Using the product rule $\\nabla (f \\mathbf{v}) = (\\nabla f) \\mathbf{v}^\\top + f (\\nabla \\mathbf{v})$. Here $\\mathbf{v}=x_i$ is constant, so the second term is $0$. It should be $\\nabla_{\\beta} ((\\frac{\\partial\\ell_i}{\\partial\\eta_i})) x_i^\\top$. Yes, this is correct.\n    12. Calculate $\\nabla_{\\beta} (\\frac{\\partial\\ell_i}{\\partial\\eta_i})$. Again, chain rule: $\\nabla_{\\beta}(\\cdot) = \\frac{\\partial(\\cdot)}{\\partial\\eta_i} \\nabla_{\\beta}\\eta_i = \\frac{\\partial^2\\ell_i}{\\partial\\eta_i^2} x_i$.\n    13. So, $\\nabla_{\\beta}^2\\ell_i(\\beta) = (\\frac{\\partial^2\\ell_i}{\\partial\\eta_i^2} x_i) x_i^\\top = \\frac{\\partial^2\\ell_i}{\\partial\\eta_i^2} x_i x_i^\\top$.\n    14. Calculate the second derivative $\\frac{\\partial^2\\ell_i}{\\partial\\eta_i^2}$. We had $\\frac{\\partial\\ell_i}{\\partial\\eta_i} = y_i - \\mu_i$. So $\\frac{\\partial^2\\ell_i}{\\partial\\eta_i^2} = -\\frac{\\partial\\mu_i}{\\partial\\eta_i}$.\n    15. We already found $\\frac{\\partial\\mu_i}{\\partial\\eta_i} = \\mu_i(1-\\mu_i)$.\n    16. So $\\frac{\\partial^2\\ell_i}{\\partial\\eta_i^2} = -\\mu_i(1-\\mu_i)$.\n    17. Therefore, $\\nabla_{\\beta}^2\\ell_i(\\beta) = -\\mu_i(1-\\mu_i) x_i x_i^\\top$.\n    18. Sum over all $i$ to get the total Hessian $\\nabla_{\\beta}^2\\ell(\\beta)$. This is the third and final result. The derivation is consistent and complete.\n\nEverything seems correct. I will now write the full solution following the specified formatting. I will be extra careful with LaTeX syntax for every single mathematical element.\nThe problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, objective, and contains a complete and consistent setup for a standard derivation in statistical learning theory.\n\nWe are tasked with deriving the log-likelihood function $\\ell(\\beta)$, its gradient vector $\\nabla_{\\beta} \\ell(\\beta)$, and its Hessian matrix $\\nabla_{\\beta}^2 \\ell(\\beta)$ for a logistic regression model, which is a specific type of Generalized Linear Model (GLM).\n\nFirst, we derive the log-likelihood function $\\ell(\\beta)$.\nThe probability mass function (pmf) for a single Bernoulli observation $y_i$ is given as $p(y_i \\mid \\mu_i) = \\mu_i^{y_i} (1-\\mu_i)^{1-y_i}$. The likelihood for a single observation is identical to this pmf. The log-likelihood for a single observation, denoted $\\ell_i$, is the natural logarithm of the likelihood:\n$$\n\\ell_i(\\mu_i) = \\ln(p(y_i \\mid \\mu_i)) = y_i \\ln(\\mu_i) + (1-y_i) \\ln(1-\\mu_i)\n$$\nWe need to express $\\ell_i$ as a function of the parameter vector $\\beta$. The model links the mean $\\mu_i$ to the linear predictor $\\eta_i = x_i^\\top\\beta$ via the logit link function:\n$$\n\\eta_i = \\ln\\left(\\frac{\\mu_i}{1-\\mu_i}\\right)\n$$\nTo express $\\mu_i$ in terms of $\\eta_i$, we invert the link function:\n$$\n\\exp(\\eta_i) = \\frac{\\mu_i}{1-\\mu_i} \\implies \\exp(\\eta_i)(1-\\mu_i) = \\mu_i \\implies \\exp(\\eta_i) - \\mu_i\\exp(\\eta_i) = \\mu_i\n$$\n$$\n\\exp(\\eta_i) = \\mu_i(1 + \\exp(\\eta_i)) \\implies \\mu_i = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = \\frac{1}{1+\\exp(-\\eta_i)}\n$$\nThis is the logistic sigmoid function. We also find an expression for $1-\\mu_i$:\n$$\n1-\\mu_i = 1 - \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = \\frac{1+\\exp(\\eta_i)-\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = \\frac{1}{1+\\exp(\\eta_i)}\n$$\nSubstituting these into the single-observation log-likelihood $\\ell_i$:\n$$\n\\ell_i(\\eta_i) = y_i \\ln\\left(\\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}\\right) + (1-y_i) \\ln\\left(\\frac{1}{1+\\exp(\\eta_i)}\\right)\n$$\nUsing the properties of logarithms, $\\ln(a/b) = \\ln(a) - \\ln(b)$ and $\\ln(1/b) = -\\ln(b)$:\n$$\n\\ell_i(\\eta_i) = y_i (\\ln(\\exp(\\eta_i)) - \\ln(1+\\exp(\\eta_i))) + (1-y_i) (-\\ln(1+\\exp(\\eta_i)))\n$$\n$$\n\\ell_i(\\eta_i) = y_i \\eta_i - y_i \\ln(1+\\exp(\\eta_i)) - \\ln(1+\\exp(\\eta_i)) + y_i \\ln(1+\\exp(\\eta_i))\n$$\n$$\n\\ell_i(\\eta_i) = y_i \\eta_i - \\ln(1+\\exp(\\eta_i))\n$$\nSubstituting $\\eta_i = x_i^\\top\\beta$:\n$$\n\\ell_i(\\beta) = y_i x_i^\\top\\beta - \\ln(1+\\exp(x_i^\\top\\beta))\n$$\nAssuming the $n$ observations are conditionally independent, the total log-likelihood $\\ell(\\beta)$ is the sum of the individual log-likelihoods:\n$$\n\\ell(\\beta) = \\sum_{i=1}^{n} \\ell_i(\\beta) = \\sum_{i=1}^{n} \\left( y_i x_i^\\top\\beta - \\ln(1+\\exp(x_i^\\top\\beta)) \\right)\n$$\n\nSecond, we compute the gradient (score) vector, $\\nabla_{\\beta} \\ell(\\beta)$.\nThe gradient of the total log-likelihood is the sum of the gradients of the individual log-likelihoods: $\\nabla_{\\beta} \\ell(\\beta) = \\sum_{i=1}^{n} \\nabla_{\\beta} \\ell_i(\\beta)$. We use the chain rule for $\\nabla_{\\beta} \\ell_i(\\beta)$:\n$$\n\\nabla_{\\beta} \\ell_i(\\beta) = \\frac{\\partial \\ell_i}{\\partial \\eta_i} \\nabla_{\\beta} \\eta_i\n$$\nThe derivative of the linear predictor $\\eta_i = x_i^\\top\\beta$ with respect to the vector $\\beta$ is $\\nabla_{\\beta} \\eta_i = x_i$.\nThe derivative of $\\ell_i$ with respect to the scalar $\\eta_i$ is:\n$$\n\\frac{\\partial \\ell_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i} \\left( y_i \\eta_i - \\ln(1+\\exp(\\eta_i)) \\right) = y_i - \\frac{1}{1+\\exp(\\eta_i)} \\cdot \\exp(\\eta_i) = y_i - \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}\n$$\nRecognizing that $\\mu_i = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}$, we have $\\frac{\\partial \\ell_i}{\\partial \\eta_i} = y_i - \\mu_i$.\nCombining these results:\n$$\n\\nabla_{\\beta} \\ell_i(\\beta) = (y_i - \\mu_i)x_i\n$$\nThe total gradient is the sum over all observations:\n$$\n\\nabla_{\\beta} \\ell(\\beta) = \\sum_{i=1}^{n} (y_i - \\mu_i)x_i = \\sum_{i=1}^{n} \\left( y_i - \\frac{\\exp(x_i^\\top\\beta)}{1+\\exp(x_i^\\top\\beta)} \\right) x_i\n$$\n\nThird, we compute the Hessian (observed information) matrix, $\\nabla_{\\beta}^2 \\ell(\\beta)$.\nThe Hessian is the matrix of second partial derivatives, $\\nabla_{\\beta}^2 \\ell(\\beta) = \\nabla_{\\beta} (\\nabla_{\\beta} \\ell(\\beta))^\\top$. It is also the sum of the individual Hessians: $\\nabla_{\\beta}^2 \\ell(\\beta) = \\sum_{i=1}^{n} \\nabla_{\\beta}^2 \\ell_i(\\beta)$.\nWe compute $\\nabla_{\\beta}^2 \\ell_i(\\beta)$ by differentiating $\\nabla_{\\beta} \\ell_i(\\beta) = (y_i - \\mu_i)x_i$ with respect to $\\beta^\\top$:\n$$\n\\nabla_{\\beta}^2 \\ell_i(\\beta) = \\nabla_{\\beta} \\left[ (y_i - \\mu_i(\\beta))x_i \\right]^\\top = \\nabla_{\\beta} (y_i - \\mu_i) x_i^\\top\n$$\nSince $y_i$ and $x_i$ are constant with respect to $\\beta$:\n$$\n\\nabla_{\\beta}^2 \\ell_i(\\beta) = - (\\nabla_{\\beta} \\mu_i) x_i^\\top\n$$\nAgain, we apply the chain rule to find $\\nabla_{\\beta} \\mu_i$:\n$$\n\\nabla_{\\beta} \\mu_i = \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\nabla_{\\beta} \\eta_i = \\frac{\\partial \\mu_i}{\\partial \\eta_i} x_i\n$$\nWe need the derivative of $\\mu_i$ with respect to $\\eta_i$:\n$$\n\\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i} \\left( \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} \\right)\n$$\nUsing the quotient rule:\n$$\n\\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\frac{\\exp(\\eta_i)(1+\\exp(\\eta_i)) - \\exp(\\eta_i)\\exp(\\eta_i)}{(1+\\exp(\\eta_i))^2} = \\frac{\\exp(\\eta_i)}{(1+\\exp(\\eta_i))^2}\n$$\nThis can be written as:\n$$\n\\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} \\cdot \\frac{1}{1+\\exp(\\eta_i)} = \\mu_i(1-\\mu_i)\n$$\nThis quantity, $\\mu_i(1-\\mu_i)$, is the variance of a Bernoulli trial, $V(\\mu_i)$. For a canonical link function, it is generally true that $\\frac{\\partial\\mu_i}{\\partial\\eta_i} = V(\\mu_i)$.\nSubstituting back:\n$$\n\\nabla_{\\beta} \\mu_i = \\mu_i(1-\\mu_i)x_i\n$$\nNow we can write the Hessian for a single observation:\n$$\n\\nabla_{\\beta}^2 \\ell_i(\\beta) = - (\\mu_i(1-\\mu_i)x_i)x_i^\\top = -\\mu_i(1-\\mu_i)x_i x_i^\\top\n$$\nThe total Hessian is the sum over all observations:\n$$\n\\nabla_{\\beta}^2 \\ell(\\beta) = \\sum_{i=1}^{n} \\left( -\\mu_i(1-\\mu_i)x_i x_i^\\top \\right) = - \\sum_{i=1}^{n} \\mu_i(1-\\mu_i)x_i x_i^\\top\n$$\nSubstituting the expressions for $\\mu_i$ and $1-\\mu_i$:\n$$\n\\nabla_{\\beta}^2 \\ell(\\beta) = - \\sum_{i=1}^{n} \\frac{\\exp(x_i^\\top\\beta)}{(1+\\exp(x_i^\\top\\beta))^2} x_i x_i^\\top\n$$\nThe three required expressions have now been derived.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sum_{i=1}^{n} \\left( y_i x_i^\\top \\beta - \\ln(1 + \\exp(x_i^\\top \\beta)) \\right)\n&\n\\sum_{i=1}^{n} \\left( y_i - \\frac{\\exp(x_i^\\top \\beta)}{1 + \\exp(x_i^\\top \\beta)} \\right) x_i\n&\n- \\sum_{i=1}^{n} \\frac{\\exp(x_i^\\top \\beta)}{(1 + \\exp(x_i^\\top \\beta))^2} x_i x_i^\\top\n\\end{pmatrix}\n}\n$$", "id": "4988404"}, {"introduction": "The power of the Generalized Linear Model (GLM) framework lies in its unifying structure for diverse data types. Having explored binary outcomes, we now turn our attention to count data, which is common in medical research for variables like the number of patient visits or infection occurrences. This practice challenges you to apply the same maximum likelihood principles to a Poisson regression model [@problem_id:4988428]. Deriving the log-likelihood, score vector, and Fisher information matrix for this new context will reinforce your understanding of the GLM framework's elegant consistency and highlight how the choice of distribution and link function shapes the specific calculations.", "problem": "A hospital system is modeling counts of postoperative infections for patients indexed by $i=1,\\dots,n$. Let $y_i \\in \\{0,1,2,\\dots\\}$ denote the observed count for patient $i$ in a fixed monitoring window, and let $x_i \\in \\mathbb{R}^{p}$ be the vector of patient-level covariates (such as age, immunosuppression status, operative time, and prophylactic antibiotic indicator). Assume a Generalized Linear Model (GLM) with a Poisson conditional distribution and the canonical log link, that is, conditional on $x_i$ and the regression coefficient vector $\\beta \\in \\mathbb{R}^{p}$, the $y_i$ are independent with mean $\\mu_i$ satisfying $\\log(\\mu_i)=x_i^{\\top}\\beta$.\n\nStarting from the definition of the Poisson probability mass function and the independence of observations given $\\beta$, derive the log-likelihood for $\\beta$. Then, from first principles, compute the score vector and the expected Fisher information matrix for $\\beta$.\n\nYour final answer must be a single closed-form analytical expression containing three components, arranged as a row matrix in the order: the log-likelihood, the score vector, and the expected Fisher information matrix. No numerical approximation is required.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n-   The data consists of $n$ observations, indexed by $i=1, \\dots, n$.\n-   For each observation $i$, there is an observed count $y_i \\in \\{0,1,2,\\dots\\}$ and a vector of covariates $x_i \\in \\mathbb{R}^{p}$.\n-   The model is a Generalized Linear Model (GLM).\n-   The conditional distribution of $y_i$ given $x_i$ and a regression coefficient vector $\\beta \\in \\mathbb{R}^{p}$ is Poisson with mean $\\mu_i$. This is denoted $y_i | x_i, \\beta \\sim \\text{Poisson}(\\mu_i)$.\n-   The observations $y_i$ are conditionally independent.\n-   The link function is the canonical log link, defined by the relation $\\log(\\mu_i) = x_i^{\\top}\\beta$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n-   **Scientifically Grounded**: The problem describes a Poisson regression model, which is a standard and fundamental component of the Generalized Linear Model framework. This model is extensively used in many scientific fields, including medicine, epidemiology, and ecology, for analyzing count data. The setup is theoretically sound and firmly rooted in established statistical principles.\n-   **Well-Posed**: The problem is well-posed. It asks for the derivation of three key statistical quantities: the log-likelihood, the score vector, and the expected Fisher information matrix. Given the model specification (Poisson distribution, log link), these quantities are uniquely defined and derivable from first principles.\n-   **Objective**: The language is precise, formal, and free of any subjective or ambiguous terminology.\n-   **Complete and Consistent**: The problem provides all necessary information—the probability distribution, the link function, and the structure of the linear predictor—to perform the required derivations. There are no missing or contradictory elements.\n-   **Realistic and Feasible**: The scenario of modeling postoperative infection counts using patient-level covariates is a realistic and common application of Poisson regression in medical statistics.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is a standard and well-defined problem in statistical theory. The solution will be derived as requested.\n\n### Derivation\n\nThe solution requires the derivation of the log-likelihood function $l(\\beta)$, the score vector $U(\\beta)$, and the expected Fisher information matrix $I(\\beta)$.\n\n**1. Log-Likelihood Function $l(\\beta)$**\n\nThe probability mass function (PMF) for a single observation $y_i$ from a Poisson distribution with mean $\\mu_i$ is given by:\n$$ P(Y_i = y_i | \\mu_i) = \\frac{\\exp(-\\mu_i)\\mu_i^{y_i}}{y_i!} $$\nDue to the conditional independence of the observations, the likelihood function $L(\\beta)$ for the entire dataset $\\{y_1, \\dots, y_n\\}$ is the product of the individual probabilities:\n$$ L(\\beta) = \\prod_{i=1}^{n} P(Y_i = y_i | x_i, \\beta) = \\prod_{i=1}^{n} \\frac{\\exp(-\\mu_i)\\mu_i^{y_i}}{y_i!} $$\nThe log-likelihood function, $l(\\beta)$, is the natural logarithm of the likelihood function:\n$$ l(\\beta) = \\ln(L(\\beta)) = \\ln\\left(\\prod_{i=1}^{n} \\frac{\\exp(-\\mu_i)\\mu_i^{y_i}}{y_i!}\\right) $$\nUsing the properties of logarithms, the product becomes a sum:\n$$ l(\\beta) = \\sum_{i=1}^{n} \\ln\\left(\\frac{\\exp(-\\mu_i)\\mu_i^{y_i}}{y_i!}\\right) = \\sum_{i=1}^{n} \\left( \\ln(\\exp(-\\mu_i)) + \\ln(\\mu_i^{y_i}) - \\ln(y_i!) \\right) $$\n$$ l(\\beta) = \\sum_{i=1}^{n} \\left( -\\mu_i + y_i\\ln(\\mu_i) - \\ln(y_i!) \\right) $$\nThe model specifies the link function $\\ln(\\mu_i) = x_i^{\\top}\\beta$, which implies $\\mu_i = \\exp(x_i^{\\top}\\beta)$. Substituting these expressions for $\\mu_i$ and $\\ln(\\mu_i)$ into the log-likelihood equation yields:\n$$ l(\\beta) = \\sum_{i=1}^{n} \\left( -\\exp(x_i^{\\top}\\beta) + y_i(x_i^{\\top}\\beta) - \\ln(y_i!) \\right) $$\nThis expression can be reordered to group terms involving $\\beta$:\n$$ l(\\beta) = \\sum_{i=1}^{n} \\left( y_i x_i^{\\top}\\beta - \\exp(x_i^{\\top}\\beta) \\right) - \\sum_{i=1}^{n} \\ln(y_i!) $$\nThis is the full log-likelihood for $\\beta$.\n\n**2. Score Vector $U(\\beta)$**\n\nThe score vector is the gradient of the log-likelihood function with respect to the parameter vector $\\beta$. It is a $p \\times 1$ vector whose elements are the partial derivatives $\\frac{\\partial l(\\beta)}{\\partial \\beta_j}$ for $j=1, \\dots, p$.\n$$ U(\\beta) = \\frac{\\partial l(\\beta)}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} \\left( \\sum_{i=1}^{n} \\left( y_i x_i^{\\top}\\beta - \\exp(x_i^{\\top}\\beta) - \\ln(y_i!) \\right) \\right) $$\nWe can differentiate term-by-term. The term $\\ln(y_i!)$ does not depend on $\\beta$, so its derivative is zero.\n$$ U(\\beta) = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta} \\left( y_i x_i^{\\top}\\beta - \\exp(x_i^{\\top}\\beta) \\right) $$\nUsing standard rules of vector calculus:\n- $\\frac{\\partial}{\\partial \\beta} (x_i^{\\top}\\beta) = x_i$\n- $\\frac{\\partial}{\\partial \\beta} \\exp(x_i^{\\top}\\beta) = \\exp(x_i^{\\top}\\beta) \\cdot \\frac{\\partial}{\\partial \\beta} (x_i^{\\top}\\beta) = \\exp(x_i^{\\top}\\beta) x_i$\nSubstituting these derivatives back into the expression for $U(\\beta)$:\n$$ U(\\beta) = \\sum_{i=1}^{n} \\left( y_i x_i - \\exp(x_i^{\\top}\\beta)x_i \\right) $$\nFactoring out the common term $x_i$:\n$$ U(\\beta) = \\sum_{i=1}^{n} \\left( y_i - \\exp(x_i^{\\top}\\beta) \\right) x_i $$\nThis is the score vector. It can also be written as $\\sum_{i=1}^{n} (y_i - \\mu_i)x_i$.\n\n**3. Expected Fisher Information Matrix $I(\\beta)$**\n\nThe expected Fisher information matrix is defined as $I(\\beta) = E\\left[-\\frac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^{\\top}}\\right]$. The matrix of second derivatives, $\\frac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^{\\top}}$, is known as the Hessian matrix, denoted $H(\\beta)$. We first compute the Hessian by differentiating the score vector $U(\\beta)$ with respect to $\\beta^{\\top}$.\n$$ H(\\beta) = \\frac{\\partial U(\\beta)}{\\partial \\beta^{\\top}} = \\frac{\\partial}{\\partial \\beta^{\\top}} \\left( \\sum_{i=1}^{n} \\left( y_i x_i - \\exp(x_i^{\\top}\\beta)x_i \\right) \\right) $$\nThe term $y_i x_i$ does not depend on $\\beta$, so its derivative is zero. We are left with:\n$$ H(\\beta) = -\\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta^{\\top}} \\left( \\exp(x_i^{\\top}\\beta)x_i \\right) $$\nUsing the product rule for vector calculus, $\\frac{\\partial(f A)}{\\partial v^{\\top}} = A \\frac{\\partial f}{\\partial v^{\\top}}$, where $A$ is a vector not dependent on $v$. Here, $A = x_i$ and $f = \\exp(x_i^{\\top}\\beta)$. We need $\\frac{\\partial \\exp(x_i^{\\top}\\beta)}{\\partial \\beta^{\\top}}$. The gradient is $\\nabla_{\\beta} \\exp(x_i^{\\top}\\beta) = \\exp(x_i^{\\top}\\beta) x_i$. The derivative with respect to $\\beta^{\\top}$ is the transpose of the gradient, so $\\frac{\\partial \\exp(x_i^{\\top}\\beta)}{\\partial \\beta^{\\top}} = (\\exp(x_i^{\\top}\\beta) x_i)^{\\top} = \\exp(x_i^{\\top}\\beta) x_i^{\\top}$.\nTherefore, the derivative of the term is:\n$$ \\frac{\\partial}{\\partial \\beta^{\\top}} \\left( \\exp(x_i^{\\top}\\beta)x_i \\right) = x_i \\left( \\exp(x_i^{\\top}\\beta) x_i^{\\top} \\right) = \\exp(x_i^{\\top}\\beta) x_i x_i^{\\top} $$\nwhere $x_i x_i^{\\top}$ is a $p \\times p$ outer product matrix.\nThe Hessian matrix is:\n$$ H(\\beta) = - \\sum_{i=1}^{n} \\exp(x_i^{\\top}\\beta) x_i x_i^{\\top} $$\nThe Fisher information is $I(\\beta) = E[-H(\\beta)]$.\n$$ I(\\beta) = E\\left[ - \\left( - \\sum_{i=1}^{n} \\exp(x_i^{\\top}\\beta) x_i x_i^{\\top} \\right) \\right] = E\\left[ \\sum_{i=1}^{n} \\exp(x_i^{\\top}\\beta) x_i x_i^{\\top} \\right] $$\nThe expectation is taken with respect to the distribution of $Y_i$. The expression for the Hessian does not depend on $y_i$; it is a function of the parameters $\\beta$ and the fixed covariates $x_i$. Therefore, the expectation of the expression is the expression itself.\n$$ I(\\beta) = \\sum_{i=1}^{n} \\exp(x_i^{\\top}\\beta) x_i x_i^{\\top} $$\nThis is the expected Fisher information matrix. It can also be written as $\\sum_{i=1}^{n} \\mu_i x_i x_i^{\\top}$.\n\nThe three requested components have been derived from first principles. We can now assemble the final answer.", "answer": "$$ \\boxed{ \\begin{pmatrix} \\sum_{i=1}^{n} \\left( y_i x_i^{\\top}\\beta - \\exp(x_i^{\\top}\\beta) - \\ln(y_i!) \\right) & \\sum_{i=1}^{n} (y_i - \\exp(x_i^{\\top}\\beta))x_i & \\sum_{i=1}^{n} \\exp(x_i^{\\top}\\beta) x_i x_i^{\\top} \\end{pmatrix} } $$", "id": "4988428"}, {"introduction": "A skilled modeler not only knows how to build models but also how to diagnose them when they fail. The previous exercises explored the mechanics of GLMs under ideal conditions, but real-world data often presents challenges. This practice explores one such critical issue: perfect multicollinearity, where one predictor variable can be perfectly determined from others. Using a simple and intuitive medical example, you will see how this structural dependency in the data renders the model's parameters non-identifiable and leads to a singular Fisher information matrix, corresponding to infinite variance for the parameter estimates [@problem_id:4988423]. This exercise connects the abstract linear algebra of the design matrix to the concrete and practical failure of a statistical model.", "problem": "Consider a medical study modeling the probability of in-hospital acute kidney injury using the Generalized Linear Model (GLM) framework with a binomial distribution and the canonical logit link. For each patient $i \\in \\{1,\\dots,n\\}$, let $Y_{i} \\in \\{0,1\\}$ denote the binary outcome and let the linear predictor be\n$$\n\\eta_{i} \\equiv \\beta_{0} + \\beta_{\\mathrm{M}} M_{i} + \\beta_{\\mathrm{F}} F_{i},\n$$\nwhere $M_{i} \\in \\{0,1\\}$ is an indicator for male sex and $F_{i} \\in \\{0,1\\}$ is an indicator for female sex. The covariates satisfy the medically correct coding rule $M_{i} + F_{i} = 1$ for all patients (every patient is either male or female, and no patient is both). The mean response is $\\mu_{i} \\equiv \\mathbb{E}[Y_{i} \\mid M_{i},F_{i}]$ and is related to the linear predictor through the logit link $\\mathrm{logit}(\\mu_{i}) = \\eta_{i}$. Assume independent observations.\n\nYou are given the following $n=6$-patient design:\n$$\n(M_{1}, F_{1}) = (1,0),\\quad (M_{2}, F_{2}) = (1,0),\\quad (M_{3}, F_{3}) = (0,1),\\quad (M_{4}, F_{4}) = (0,1),\\quad (M_{5}, F_{5}) = (1,0),\\quad (M_{6}, F_{6}) = (0,1).\n$$\nStarting from first principles for a binomial GLM with canonical link, derive the observed Fisher information matrix for the parameter vector $(\\beta_{0}, \\beta_{\\mathrm{M}}, \\beta_{\\mathrm{F}})$ in terms of the design matrix and the mean-variance relationship of the binomial family, and use it to explain the effect of perfect multicollinearity among the covariates on coefficient identifiability and variance estimates. Then, compute the determinant of the observed Fisher information matrix for this design, expressed exactly as a function of the canonical weights $w_{i} \\equiv \\mu_{i}(1-\\mu_{i})$.\n\nState your final answer as the exact value of the determinant. No rounding is required.", "solution": "The problem is valid. It presents a well-defined scenario in statistical modeling, specifically within the Generalized Linear Model (GLM) framework. The setup is designed to illustrate the fundamental statistical concepts of perfect multicollinearity, parameter identifiability, and singularity of the Fisher information matrix. The problem is scientifically grounded, objective, and contains all necessary information to proceed to a full solution.\n\nThe problem asks for three main tasks: First, to derive the observed Fisher information matrix for a binomial GLM with a canonical link. Second, to use this framework to explain the consequences of perfect multicollinearity. Third, to compute the determinant of this matrix for the given design.\n\nThe general log-likelihood for a single observation $Y_i$ in a GLM from the exponential family with canonical link function is given by\n$$l_{i}(\\theta_i, \\phi; y_i) = \\frac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c(y_i, \\phi)$$\nwhere $\\theta_i$ is the canonical parameter. For the binomial distribution, the canonical parameter is the log-odds, $\\theta_i = \\mathrm{logit}(\\mu_i) = \\ln(\\frac{\\mu_i}{1-\\mu_i})$. The problem specifies a canonical logit link, which means the linear predictor $\\eta_i$ is equal to the canonical parameter, $\\eta_i = \\theta_i$. The function $b(\\theta_i)$ is given by $b(\\theta_i) = \\ln(1 + \\exp(\\theta_i))$. The mean is $\\mathbb{E}[Y_i] = \\mu_i = b'(\\theta_i)$, and the variance is $\\mathrm{Var}(Y_i) = a_i(\\phi)b''(\\theta_i)$. For the binomial distribution with $1$ trial (Bernoulli), the dispersion parameter is $\\phi=1$, a common prior weight is $a_i(\\phi)=1$, and the variance is $\\mathrm{Var}(Y_i) = b''(\\theta_i) = \\mu_i(1-\\mu_i)$.\n\nThe log-likelihood for the entire sample of $n$ independent observations is $L(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} l_i$, whereboldsymbol{\\beta} = (\\beta_0, \\beta_M, \\beta_F)^T$ is the vector of regression coefficients. The score vector is the vector of first derivatives of $L(\\boldsymbol{\\beta})$ with respect to $\\boldsymbol{\\beta}$:\n$$\n\\frac{\\partial L}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\nFrom the canonical form of the log-likelihood, $\\frac{\\partial l_i}{\\partial \\theta_i} = y_i - b'(\\theta_i) = y_i - \\mu_i$. With the canonical link, $\\theta_i = \\eta_i$, so $\\frac{\\partial \\theta_i}{\\partial \\eta_i} = 1$. The linear predictor is $\\eta_i = \\beta_0 + \\beta_M M_i + \\beta_F F_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}$, where $\\mathbf{x}_i^T = (1, M_i, F_i)$. Thus, $\\frac{\\partial \\eta_i}{\\partial \\beta_j} = x_{ij}$. The score for the $j$-th parameter is:\n$$\nU_j = \\frac{\\partial L}{\\partial \\beta_j} = \\sum_{i=1}^{n} (y_i - \\mu_i) x_{ij} \\implies \\mathbf{U}(\\boldsymbol{\\beta}) = \\mathbf{X}^T (\\mathbf{y} - \\boldsymbol{\\mu})\n$$\nThe Hessian matrix $\\mathbf{H}(\\boldsymbol{\\beta})$ consists of the second partial derivatives, $H_{jk} = \\frac{\\partial^2 L}{\\partial \\beta_j \\partial \\beta_k}$.\n$$\n\\frac{\\partial^2 L}{\\partial \\beta_j \\partial \\beta_k} = \\frac{\\partial}{\\partial \\beta_k} \\sum_{i=1}^{n} (y_i - \\mu_i) x_{ij} = \\sum_{i=1}^{n} -x_{ij} \\frac{\\partial \\mu_i}{\\partial \\beta_k}\n$$\nUsing the chain rule again, $\\frac{\\partial \\mu_i}{\\partial \\beta_k} = \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_k}$. For a GLM, $\\frac{\\partial \\mu_i}{\\partial \\eta_i}$ is the derivative of the inverse link function. For the logit link, $\\mu_i = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}$, and $\\frac{d\\mu_i}{d\\eta_i} = \\mu_i(1-\\mu_i)$. This is also the variance function $V(\\mu_i)$.\nSo, $\\frac{\\partial \\mu_i}{\\partial \\beta_k} = \\mu_i(1-\\mu_i) x_{ik}$.\nSubstituting this back, we get:\n$$\nH_{jk} = \\sum_{i=1}^{n} -x_{ij} (\\mu_i(1-\\mu_i)) x_{ik}\n$$\nIn matrix form, $\\mathbf{H}(\\boldsymbol{\\beta}) = -\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$, where $\\mathbf{W}$ is a diagonal matrix with diagonal elements $w_i = \\mu_i(1-\\mu_i)$. The observed Fisher information matrix $\\mathbf{I}(\\boldsymbol{\\beta})$ is defined as $\\mathbf{I}(\\boldsymbol{\\beta}) = -\\mathbb{E}[\\mathbf{H}(\\boldsymbol{\\beta})]$. For a canonical link, the Hessian does not depend on the data $\\mathbf{y}$, so the observed information equals the expected Fisher information:\n$$\n\\mathbf{I}(\\boldsymbol{\\beta}) = -\\mathbf{H}(\\boldsymbol{\\beta}) = \\mathbf{X}^T \\mathbf{W} \\mathbf{X}\n$$\nThis completes the first part of the problem.\n\nFor the second part, we analyze the effect of multicollinearity. The design matrix $\\mathbf{X}$ for the $n=6$ patients is constructed from the given covariates:\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1 & M_1 & F_1 \\\\\n1 & M_2 & F_2 \\\\\n1 & M_3 & F_3 \\\\\n1 & M_4 & F_4 \\\\\n1 & M_5 & F_5 \\\\\n1 & M_6 & F_6\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 1\n\\end{pmatrix}\n$$\nThe columns of this matrix are $\\mathbf{c}_0 = (1,1,1,1,1,1)^T$, $\\mathbf{c}_M = (1,1,0,0,1,0)^T$, and $\\mathbf{c}_F = (0,0,1,1,0,1)^T$. The problem states the constraint $M_i + F_i = 1$ for all patients. In terms of the columns of the design matrix, this means $\\mathbf{c}_M + \\mathbf{c}_F = \\mathbf{c}_0$, or $\\mathbf{c}_0 - \\mathbf{c}_M - \\mathbf{c}_F = \\mathbf{0}$. This is a perfect linear dependency among the columns of $\\mathbf{X}$, which is known as perfect multicollinearity.\n\nThis dependency has a profound effect on parameter identifiability. Let's consider an alternative set of parameters $\\boldsymbol{\\beta}^* = (\\beta_0^*, \\beta_M^*, \\beta_F^*)^T$ defined by $\\beta_0^* = \\beta_0 + c$, $\\beta_M^* = \\beta_M - c$, and $\\beta_F^* = \\beta_F - c$ for any arbitrary constant $c \\in \\mathbb{R}$. The linear predictor for this new set of parameters is:\n$$\n\\eta_i^* = \\beta_0^* + \\beta_M^* M_i + \\beta_F^* F_i = (\\beta_0 + c) + (\\beta_M - c)M_i + (\\beta_F - c)F_i = (\\beta_0 + \\beta_M M_i + \\beta_F F_i) + c(1 - M_i - F_i)\n$$\nGiven the constraint $M_i + F_i = 1$, the term $1 - M_i - F_i = 0$ for all $i$. Therefore, $\\eta_i^* = \\eta_i$. This implies that an infinite number of parameter vectors $\\boldsymbol{\\beta}^*$ produce the exact same predicted probabilities $\\mu_i$ and thus the same value of the likelihood function. The parameters are not identifiable, and a unique maximum likelihood estimate for $\\boldsymbol{\\beta}$ does not exist.\n\nThe effect on variance estimates is also critical. The asymptotic covariance matrix of the MLE $\\hat{\\boldsymbol{\\beta}}$ is given by the inverse of the Fisher information matrix, $\\mathrm{Cov}(\\hat{\\boldsymbol{\\beta}}) \\approx \\mathbf{I}(\\hat{\\boldsymbol{\\beta}})^{-1} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1}$. The linear dependency in the columns of $\\mathbf{X}$ implies that the matrix $\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$ is singular. To see this, let $\\mathbf{v} = (1, -1, -1)^T$. The linear dependency is $\\mathbf{Xv} = \\mathbf{0}$. Then,\n$$\n(\\mathbf{X}^T \\mathbf{W} \\mathbf{X})\\mathbf{v} = \\mathbf{X}^T \\mathbf{W} (\\mathbf{Xv}) = \\mathbf{X}^T \\mathbf{W} \\mathbf{0} = \\mathbf{0}\n$$\nSince there exists a non-zero vector $\\mathbf{v}$ for which $(\\mathbf{X}^T \\mathbf{W} \\mathbf{X})\\mathbf{v} = \\mathbf{0}$, the matrix $\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$ is singular by definition. A singular matrix is not invertible, which means the covariance matrix of the estimators is undefined. This corresponds to infinite variances for the parameter estimates, rendering them useless.\n\nFor the third part, we compute the determinant of $\\mathbf{I}(\\boldsymbol{\\beta})$. The weights $w_i = \\mu_i(1-\\mu_i)$ are constant for all patients with the same covariate values. There are $n_M=3$ male patients ($i=1,2,5$) and $n_F=3$ female patients ($i=3,4,6$).\nFor male patients, $\\eta_i = \\beta_0 + \\beta_M$. Let $w_M = \\mu_M(1-\\mu_M)$ where $\\mu_M = (1+\\exp(-(\\beta_0+\\beta_M)))^{-1}$.\nFor female patients, $\\eta_i = \\beta_0 + \\beta_F$. Let $w_F = \\mu_F(1-\\mu_F)$ where $\\mu_F = (1+\\exp(-(\\beta_0+\\beta_F)))^{-1}$.\nThe diagonal matrix of weights is $\\mathbf{W} = \\mathrm{diag}(w_M, w_M, w_F, w_F, w_M, w_F)$.\nThe Fisher information matrix is $\\mathbf{I} = \\mathbf{X}^T \\mathbf{W} \\mathbf{X}$. Its elements $I_{jk} = \\sum_{i=1}^6 x_{ij} w_i x_{ik}$ are:\n$I_{00} = \\sum w_i = 3w_M + 3w_F$\n$I_{0M} = \\sum w_i M_i = 3w_M$\n$I_{0F} = \\sum w_i F_i = 3w_F$\n$I_{MM} = \\sum w_i M_i^2 = 3w_M$\n$I_{FF} = \\sum w_i F_i^2 = 3w_F$\n$I_{MF} = \\sum w_i M_i F_i = 0$\nThe full matrix is:\n$$\n\\mathbf{I} = \\begin{pmatrix}\n3w_M + 3w_F & 3w_M & 3w_F \\\\\n3w_M & 3w_M & 0 \\\\\n3w_F & 0 & 3w_F\n\\end{pmatrix}\n$$\nTo find the determinant, we can use the cofactor expansion along the first row:\n\\begin{align*}\n\\det(\\mathbf{I}) &= (3w_M + 3w_F) \\begin{vmatrix} 3w_M & 0 \\\\ 0 & 3w_F \\end{vmatrix} - (3w_M) \\begin{vmatrix} 3w_M & 0 \\\\ 3w_F & 3w_F \\end{vmatrix} + (3w_F) \\begin{vmatrix} 3w_M & 3w_M \\\\ 3w_F & 0 \\end{vmatrix} \\\\\n&= (3w_M + 3w_F)(9w_M w_F) - (3w_M)(9w_M w_F - 0) + (3w_F)(0 - 9w_M w_F) \\\\\n&= (27w_M^2 w_F + 27w_M w_F^2) - 27w_M^2 w_F - 27w_M w_F^2 \\\\\n&= 0\n\\end{align*}\nAlternatively, we observe that the first column of $\\mathbf{I}$ is the sum of the second and third columns. A matrix with linearly dependent columns (or rows) has a determinant of zero. This confirms our theoretical expectation. The determinant is $0$, regardless of the values of the weights $w_i$.", "answer": "$$\n\\boxed{0}\n$$", "id": "4988423"}]}