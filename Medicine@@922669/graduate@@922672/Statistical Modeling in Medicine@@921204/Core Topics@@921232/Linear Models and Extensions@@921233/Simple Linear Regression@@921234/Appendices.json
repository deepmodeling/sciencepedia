{"hands_on_practices": [{"introduction": "To begin our hands-on exploration of simple linear regression, we will start with the fundamental mechanics of the model. This first practice focuses on applying the ordinary least squares (OLS) principle to derive and calculate the regression coefficients from summary statistics, a common scenario in collaborative research. By working through this dose-response study, you will solidify your understanding of how to compute the slope and intercept and, more importantly, how to translate these numerical estimates into a clinically meaningful interpretation. [@problem_id:4840047]", "problem": "A clinical pharmacology team is conducting an early-phase dose-finding investigation of a novel antihypertensive agent. For each participant, the team records the administered dose in milligrams and the change in Systolic Blood Pressure (SBP) in millimeters of mercury at four weeks. Assume the relationship between dose and SBP change for participant $i$ follows the simple linear regression model\n$$\nY_i \\;=\\; \\beta_0 \\;+\\; \\beta_1 X_i \\;+\\; \\varepsilon_i,\n$$\nwhere $Y_i$ is the SBP change, $X_i$ is the dose, $\\beta_0$ and $\\beta_1$ are unknown regression parameters, and the errors $\\varepsilon_i$ are independent with mean $0$ and constant variance, reflecting the standard conditions under which ordinary least squares estimation is justified in medical evidence generation.\n\nA dataset of $n=40$ participants yields the following summary statistics: sample means $\\bar{X}=5$ and $\\bar{Y}=12$, and centered sums $S_{xx}=160$ and $S_{xy}=320$. Starting from the least squares principle, derive the estimators $(\\hat{\\beta}_0,\\hat{\\beta}_1)$ and compute their numerical values using the provided summaries. Then, provide a clinically meaningful interpretation of the slope $\\hat{\\beta}_1$ in the context of dose-response for antihypertensive efficacy, explicitly identifying the change in SBP per unit change in dose. Report $(\\hat{\\beta}_0,\\hat{\\beta}_1)$ as exact values with no rounding. The interpretation should use milligrams for dose and millimeters of mercury for SBP change, but the numerical estimators themselves should be reported without units.", "solution": "The simple linear regression model is given by\n$$\nY_i \\;=\\; \\beta_0 \\;+\\; \\beta_1 X_i \\;+\\; \\varepsilon_i,\n$$\nwhere $Y_i$ is the change in Systolic Blood Pressure (SBP), $X_i$ is the administered dose, $\\beta_0$ and $\\beta_1$ are the unknown regression parameters, and $\\varepsilon_i$ are independent, identically distributed random errors with mean $E[\\varepsilon_i] = 0$ and constant variance $\\text{Var}(\\varepsilon_i) = \\sigma^2$.\n\nThe principle of ordinary least squares (OLS) seeks to find the estimators $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ that minimize the sum of squared errors (SSE). The sum of squared errors is defined as the function of $\\beta_0$ and $\\beta_1$:\n$$\n\\text{SSE}(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^{n} (Y_i - (\\beta_0 + \\beta_1 X_i))^2.\n$$\nTo find the values of $\\beta_0$ and $\\beta_1$ that minimize this function, we take the partial derivatives of $\\text{SSE}(\\beta_0, \\beta_1)$ with respect to each parameter and set them equal to zero. These are the normal equations.\n\nThe partial derivative with respect to $\\beta_0$ is:\n$$\n\\frac{\\partial \\text{SSE}}{\\partial \\beta_0} = \\sum_{i=1}^{n} 2(Y_i - \\beta_0 - \\beta_1 X_i)(-1) = -2 \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1 X_i).\n$$\nSetting this to zero yields the first normal equation for the estimators $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$:\n$$\n\\sum_{i=1}^{n} (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) = 0 \\\\\n\\sum Y_i - n\\hat{\\beta}_0 - \\hat{\\beta}_1 \\sum X_i = 0.\n$$\nDividing by the sample size $n$ gives:\n$$\n\\bar{Y} - \\hat{\\beta}_0 - \\hat{\\beta}_1 \\bar{X} = 0,\n$$\nwhich can be rearranged to express the intercept estimator $\\hat{\\beta}_0$ in terms of the slope estimator $\\hat{\\beta}_1$:\n$$\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X}.\n$$\nThis derivation shows that the least squares regression line must pass through the point of sample means, $(\\bar{X}, \\bar{Y})$.\n\nThe partial derivative with respect to $\\beta_1$ is:\n$$\n\\frac{\\partial \\text{SSE}}{\\partial \\beta_1} = \\sum_{i=1}^{n} 2(Y_i - \\beta_0 - \\beta_1 X_i)(-X_i) = -2 \\sum_{i=1}^{n} X_i(Y_i - \\beta_0 - \\beta_1 X_i).\n$$\nSetting this to zero yields the second normal equation:\n$$\n\\sum_{i=1}^{n} X_i(Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) = 0 \\\\\n\\sum X_i Y_i - \\hat{\\beta}_0 \\sum X_i - \\hat{\\beta}_1 \\sum X_i^2 = 0.\n$$\nNow, substitute the expression for $\\hat{\\beta}_0$ from the first normal equation into the second:\n$$\n\\sum X_i Y_i - (\\bar{Y} - \\hat{\\beta}_1 \\bar{X}) \\sum X_i - \\hat{\\beta}_1 \\sum X_i^2 = 0 \\\\\n\\sum X_i Y_i - \\bar{Y}\\sum X_i + \\hat{\\beta}_1 \\bar{X}\\sum X_i - \\hat{\\beta}_1 \\sum X_i^2 = 0.\n$$\nRearranging to solve for $\\hat{\\beta}_1$:\n$$\n\\hat{\\beta}_1 \\left( \\bar{X}\\sum X_i - \\sum X_i^2 \\right) = \\bar{Y}\\sum X_i - \\sum X_i Y_i.\n$$\nMultiplying by $-1$ and substituting $\\sum X_i = n\\bar{X}$:\n$$\n\\hat{\\beta}_1 \\left( \\sum X_i^2 - n\\bar{X}^2 \\right) = \\sum X_i Y_i - n\\bar{X}\\bar{Y}.\n$$\nThe terms in parentheses are the definitions of the centered sums of squares and products:\n$S_{xx} = \\sum_{i=1}^{n} (X_i - \\bar{X})^2 = \\sum X_i^2 - n\\bar{X}^2$.\n$S_{xy} = \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y}) = \\sum X_i Y_i - n\\bar{X}\\bar{Y}$.\n\nThus, the derived estimator for the slope parameter $\\beta_1$ is:\n$$\n\\hat{\\beta}_1 = \\frac{\\sum X_i Y_i - n\\bar{X}\\bar{Y}}{\\sum X_i^2 - n\\bar{X}^2} = \\frac{S_{xy}}{S_{xx}}.\n$$\nThe derived estimators are therefore $\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}}$ and $\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X}$.\n\nNow, we compute the numerical values using the provided summary statistics for the dataset of $n=40$ participants: $\\bar{X}=5$, $\\bar{Y}=12$, $S_{xx}=160$, and $S_{xy}=320$.\n\nFirst, we compute the slope estimate $\\hat{\\beta}_1$:\n$$\n\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}} = \\frac{320}{160} = 2.\n$$\nNext, we compute the intercept estimate $\\hat{\\beta}_0$:\n$$\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X} = 12 - (2)(5) = 12 - 10 = 2.\n$$\nThe estimated regression equation is $\\hat{Y}_i = 2 + 2 X_i$. The estimated parameters are $(\\hat{\\beta}_0, \\hat{\\beta}_1) = (2, 2)$.\n\nFinally, we provide a clinically meaningful interpretation of the slope estimator, $\\hat{\\beta}_1$. The slope of a regression line represents the estimated change in the response variable for a one-unit increase in the predictor variable. Here, the response $Y$ is the change in SBP (in millimeters of mercury, mmHg) and the predictor $X$ is the dose (in milligrams, mg).\n\nThe value $\\hat{\\beta}_1 = 2$ indicates that for every $1$ milligram increase in the dose of the novel agent, the change in Systolic Blood Pressure is estimated to increase by $2$ millimeters of mercury. A positive value for the SBP change indicates an increase in blood pressure. Therefore, this result suggests that, within the observed dose range, the drug has a hypertensive effect, which is contrary to the intended purpose of an antihypertensive agent. This is a critical finding in an early-phase trial, suggesting the agent may not be effective and could be harmful.", "answer": "$$\n\\boxed{\\begin{pmatrix} 2  2 \\end{pmatrix}}\n$$", "id": "4840047"}, {"introduction": "A crucial skill in statistical modeling is recognizing the limitations of your chosen tools. The Pearson correlation coefficient, $r$, and the simple linear regression slope are powerful measures of *linear* association, but they can be misleading in the presence of non-linear relationships. This conceptual exercise presents a classic scenario where a perfect, deterministic relationship between two variables yields a correlation coefficient of zero, challenging you to look beyond the numbers and think critically about the underlying data structure. [@problem_id:2429453]", "problem": "A research group is studying the stress response of a bacterium by varying the environmental temperature relative to its growth optimum and measuring the expression of a heat-shock gene. Let $X$ denote the deviation in degrees Celsius from the optimal temperature (negative for cooler, positive for warmer), and let $Y$ denote normalized expression measured in relative fluorescence units (RFU), where $Y$ has been baseline-corrected so that expression at the optimum is near $0$ RFU. The investigators collect data at $7$ evenly spaced deviations and obtain the following paired observations $(X_i, Y_i)$:\n$$(-3, 9),\\, (-2, 4),\\, (-1, 1),\\, (0, 0),\\, (1, 1),\\, (2, 4),\\, (3, 9).$$\nThey consider fitting the simple linear regression model $Y = \\beta_0 + \\beta_1 X + \\varepsilon$ and compute the sample Pearson correlation coefficient $r$ between $X$ and $Y$.\n\nBased only on these data and the definition of the Pearson correlation coefficient, which of the following statements is most appropriate?\n\nA. The near-zero value of $r$ proves that $X$ and $Y$ are independent; the intercept-only linear model $Y = \\beta_0 + \\varepsilon$ is appropriate and will capture the structure in the data.\n\nB. The near-zero value of $r$ arises because the relationship between $X$ and $Y$ is symmetric and nonlinear (approximately quadratic); simple linear regression will yield $\\hat{\\beta}_1 \\approx 0$ with systematic curved residuals, and a model including a quadratic term in $X$ or a transformation such as $X \\mapsto X^2$ is more appropriate.\n\nC. The near-zero value of $r$ is caused by outliers at large $\\lvert X \\rvert$; removing those points would increase $r$ substantially and validate a simple linear regression fit.\n\nD. Because $r \\approx 0$, the coefficient of determination $R^2$ of a quadratic regression of $Y$ on $X^2$ must also be near $0$, so adding polynomial terms cannot improve fit.", "solution": "The task is to assess the relationship between the temperature deviation $X$ and gene expression $Y$ using the provided data points:\n$$(-3, 9),\\, (-2, 4),\\, (-1, 1),\\, (0, 0),\\, (1, 1),\\, (2, 4),\\, (3, 9)$$\n\nFirst, we must compute the sample Pearson correlation coefficient, $r$, defined as:\n$$ r = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^n (X_i - \\bar{X})^2} \\sqrt{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}} $$\nwhere $n=7$ is the number of data points.\n\nLet's calculate the sample means $\\bar{X}$ and $\\bar{Y}$:\n$$ \\bar{X} = \\frac{1}{7} \\sum_{i=1}^7 X_i = \\frac{-3 - 2 - 1 + 0 + 1 + 2 + 3}{7} = \\frac{0}{7} = 0 $$\n$$ \\bar{Y} = \\frac{1}{7} \\sum_{i=1}^7 Y_i = \\frac{9 + 4 + 1 + 0 + 1 + 4 + 9}{7} = \\frac{28}{7} = 4 $$\n\nNow, let us compute the numerator of $r$, which is proportional to the sample covariance:\n$$ \\sum_{i=1}^7 (X_i - \\bar{X})(Y_i - \\bar{Y}) = \\sum_{i=1}^7 X_i (Y_i - 4) $$\nThe individual terms are:\n\\begin{itemize}\n    \\item For $(X_1, Y_1) = (-3, 9)$: $(-3)(9 - 4) = (-3)(5) = -15$\n    \\item For $(X_2, Y_2) = (-2, 4)$: $(-2)(4 - 4) = (-2)(0) = 0$\n    \\item For $(X_3, Y_3) = (-1, 1)$: $(-1)(1 - 4) = (-1)(-3) = 3$\n    \\item For $(X_4, Y_4) = (0, 0)$: $(0)(0 - 4) = (0)(-4) = 0$\n    \\item For $(X_5, Y_5) = (1, 1)$: $(1)(1 - 4) = (1)(-3) = -3$\n    \\item For $(X_6, Y_6) = (2, 4)$: $(2)(4 - 4) = (2)(0) = 0$\n    \\item For $(X_7, Y_7) = (3, 9)$: $(3)(9 - 4) = (3)(5) = 15$\n\\end{itemize}\nThe sum of these terms is:\n$$ -15 + 0 + 3 + 0 - 3 + 0 + 15 = 0 $$\nSince the numerator is exactly $0$, the Pearson correlation coefficient $r$ is also exactly $0$, provided the denominator is non-zero. The denominator involves sums of squares of deviations from the mean, which are non-zero as long as not all $X_i$ or $Y_i$ are identical. Here, $\\sum (X_i - \\bar{X})^2 = 28$ and $\\sum (Y_i - \\bar{Y})^2 = 84$.\nThus, for this dataset, $r = 0$.\n\nNow we evaluate each statement.\n\n**A. The near-zero value of $r$ proves that $X$ and $Y$ are independent; the intercept-only linear model $Y = \\beta_0 + \\varepsilon$ is appropriate and will capture the structure in the data.**\nThis statement is fundamentally flawed. A zero Pearson correlation coefficient indicates the absence of a *linear* relationship, but it does not prove statistical independence. In this case, the variables are perfectly dependent through the deterministic, non-linear function $Y = X^2$. Independence is a much stronger condition than zero correlation. The intercept-only model is $Y = \\beta_0 + \\varepsilon$, where the least-squares estimate for $\\beta_0$ is $\\hat{\\beta}_0 = \\bar{Y} = 4$. This model, $Y = 4$, completely ignores the pronounced U-shaped pattern in the data and thus fails to \"capture the structure.\"\n**Verdict: Incorrect.**\n\n**B. The near-zero value of $r$ arises because the relationship between $X$ and $Y$ is symmetric and nonlinear (approximately quadratic); simple linear regression will yield $\\hat{\\beta}_1 \\approx 0$ with systematic curved residuals, and a model including a quadratic term in $X$ or a transformation such as $X \\mapsto X^2$ is more appropriate.**\nThis statement accurately diagnoses the situation. The relationship is indeed perfectly quadratic ($Y = X^2$) and symmetric about the $Y$-axis. This symmetry causes the positive and negative contributions to the covariance term $\\sum X_i (Y_i - \\bar{Y})$ to cancel out exactly, leading to $r = 0$. The slope estimate in simple linear regression is $\\hat{\\beta}_1 = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2} = \\frac{0}{28} = 0$. The fitted SLR model is thus $\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X = \\bar{Y} + 0 \\cdot X = 4$. The residuals, $e_i = Y_i - \\hat{Y}_i = Y_i - 4$, are $(5, 0, -3, -4, -3, 0, 5)$. When plotted against $X$, these residuals form a clear parabola, which signifies \"systematic curved residuals\". A model of the form $Y = \\beta'_0 + \\beta'_1 X^2 + \\varepsilon'$ would fit the data perfectly, with $\\beta'_0=0$ and $\\beta'_1=1$, and is therefore a much more appropriate choice.\n**Verdict: Correct.**\n\n**C. The near-zero value of $r$ is caused by outliers at large $\\lvert X \\rvert$; removing those points would increase $r$ substantially and validate a simple linear regression fit.**\nThe points at large $|X|$, which are $(-3, 9)$ and $(3, 9)$, are not outliers. They lie perfectly on the true underlying curve $Y=X^2$. Calling them \"outliers\" is a mischaracterization resulting from incorrectly assuming a linear model. To test the claim, let's remove these two points and recalculate $r$ for the remaining data: $(-2, 4), (-1, 1), (0, 0), (1, 1), (2, 4)$.\nThe new means are $\\bar{X}_{\\text{new}} = 0$ and $\\bar{Y}_{\\text{new}} = (4+1+0+1+4)/5 = 2$.\nThe new covariance term is:\n$$ \\sum (X_i - 0)(Y_i - 2) = (-2)(4-2) + (-1)(1-2) + (0)(0-2) + (1)(1-2) + (2)(4-2) $$\n$$ = (-2)(2) + (-1)(-1) + 0 + (1)(-1) + (2)(2) = -4 + 1 - 1 + 4 = 0 $$\nThe correlation is still exactly $0$. The statement that removing these points would increase $r$ is false.\n**Verdict: Incorrect.**\n\n**D. Because $r \\approx 0$, the coefficient of determination $R^2$ of a quadratic regression of $Y$ on $X^2$ must also be near $0$, so adding polynomial terms cannot improve fit.**\nThis statement demonstrates a grave misunderstanding of regression models. The correlation coefficient $r$ (between $X$ and $Y$) is specific to the simple linear regression of $Y$ on $X$. The coefficient of determination for this model is $R^2 = r^2 = 0^2 = 0$. A \"quadratic regression of $Y$ on $X^2$\" implies a new model, $Y = \\beta'_0 + \\beta'_1 Z + \\varepsilon'$, where $Z = X^2$. Let's examine the data for $(Z, Y)$: $(9, 9), (4, 4), (1, 1), (0, 0), (1, 1), (4, 4), (9, 9)$. This is a perfect linear relationship $Y=Z$. The correlation between $Z$ and $Y$ is $r_{ZY} = 1$. The coefficient of determination for this quadratic model is $R^2_{Y \\sim Z} = r_{ZY}^2 = 1^2 = 1$. This is the maximum possible value, indicating a perfect fit, not a value \"near $0$\". Thus, adding the quadratic term $X^2$ dramatically improves the fit from $R^2=0$ to $R^2=1$.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{B}$$", "id": "2429453"}, {"introduction": "Moving beyond point estimates, a cornerstone of rigorous scientific inference is quantifying the uncertainty in our findings. This computational practice introduces the nonparametric bootstrap, a powerful and widely used resampling method for constructing confidence intervals without making strong assumptions about the underlying distribution of the data. By applying this technique to analyze the association between GC content and gene expression, you will gain hands-on experience with a modern statistical tool that is essential for robust data analysis in bioinformatics and medical research. [@problem_id:2429424]", "problem": "You are given multiple independent collections of paired observations representing guanineâ€“cytosine content (GC content) and normalized gene expression level for distinct protein-coding genes from a single organism. For each collection, consider the simple linear regression model $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$, where $x_i$ denotes the GC content (expressed as a fraction in the interval $[0,1]$), $y_i$ denotes the normalized expression level on a log base $2$ Transcripts Per Million (TPM) scale, $\\beta_0$ is the intercept, $\\beta_1$ is the slope, and $\\varepsilon_i$ is an unobserved residual for observation $i$. The slope $\\beta_1$ is defined as the ordinary least squares coefficient that minimizes the sum of squared residuals under this model.\n\nYour task is to use nonparametric bootstrap resampling of paired observations to construct a two-sided confidence interval for the slope $\\beta_1$ at confidence level $0.95$ for each collection. For each collection, treat the observed pairs $\\{(x_i,y_i)\\}_{i=1}^n$ as defining the empirical distribution that places probability mass $1/n$ on each observed pair, draw $B$ bootstrap samples of size $n$ with replacement from these pairs, compute the slope estimate for each bootstrap sample, and then report the percentile interval with lower endpoint at probability $0.025$ and upper endpoint at probability $0.975$ of the bootstrap slope distribution. Use the provided pseudorandom seed for reproducibility for each collection.\n\nTest suite (four distinct collections). In each case, $x$ denotes GC content (fraction) and $y$ denotes log base $2$ TPM. For every case, report the two endpoints of the $0.95$ confidence interval for $\\beta_1$.\n\n- Case $1$ (general increasing association):\n  - $x$: $0.34, 0.36, 0.40, 0.41, 0.43, 0.45, 0.48, 0.50, 0.52, 0.55, 0.57, 0.60$\n  - $y$: $4.10, 4.00, 4.30, 4.50, 4.60, 4.90, 5.00, 5.30, 5.40, 5.80, 6.00, 6.20$\n  - Confidence level: $0.95$; bootstrap replicates: $B=8000$; seed: $13579$.\n\n- Case $2$ (near-zero association):\n  - $x$: $0.31, 0.33, 0.35, 0.38, 0.40, 0.42, 0.44, 0.47, 0.49, 0.51, 0.53, 0.56$\n  - $y$: $5.10, 4.95, 5.05, 5.00, 5.02, 4.98, 5.01, 5.04, 4.96, 5.03, 4.97, 5.00$\n  - Confidence level: $0.95$; bootstrap replicates: $B=8000$; seed: $24680$.\n\n- Case $3$ (general decreasing association):\n  - $x$: $0.30, 0.34, 0.37, 0.39, 0.41, 0.44, 0.46, 0.49, 0.51, 0.54, 0.58, 0.61$\n  - $y$: $7.00, 6.80, 6.60, 6.50, 6.30, 6.10, 6.00, 5.90, 5.70, 5.50, 5.30, 5.20$\n  - Confidence level: $0.95$; bootstrap replicates: $B=8000$; seed: $11235$.\n\n- Case $4$ (smaller sample, increasing association):\n  - $x$: $0.32, 0.36, 0.40, 0.45, 0.50, 0.54, 0.58, 0.62$\n  - $y$: $3.80, 3.95, 4.10, 4.30, 4.45, 4.60, 4.80, 4.95$\n  - Confidence level: $0.95$; bootstrap replicates: $B=8000$; seed: $98765$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case in the order above and is itself a two-element list holding the lower and upper endpoints of the confidence interval. Each numeric endpoint must be rounded to six decimal places using standard rounding. For example, a valid output shape is $[[\\ell_1,u_1],[\\ell_2,u_2],[\\ell_3,u_3],[\\ell_4,u_4]]$, where $\\ell_k$ and $u_k$ denote the lower and upper endpoints for case $k$, respectively.", "solution": "**Solution Methodology**\n\nThe objective is to construct a $95\\%$ confidence interval for the slope parameter $\\beta_1$ of a simple linear regression model. The method specified is the nonparametric bootstrap, which makes no strong assumptions about the distribution of the error term $\\varepsilon_i$.\n\nThe ordinary least squares (OLS) estimate for the slope, $\\hat{\\beta}_1$, is the statistic of interest. For a sample of $n$ paired observations $\\{(x_i, y_i)\\}_{i=1}^n$, the OLS estimator $\\hat{\\beta}_1$ that minimizes the sum of squared residuals $\\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i))^2$ is given by the formula:\n$$\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)}\n$$\nwhere $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$ and $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$ are the sample means of the independent and dependent variables, respectively.\n\nThe core of the solution is the nonparametric bootstrap algorithm, which we apply to each of the four cases:\n$1$. The original dataset of $n$ pairs, $D = \\{(x_1, y_1), \\dots, (x_n, y_n)\\}$, is treated as the empirical distribution function, where each pair has a probability of $1/n$ of being selected.\n$2$. A pseudorandom number generator is initialized with the specified seed for reproducibility.\n$3$. A total of $B=8000$ bootstrap samples are generated. Each bootstrap sample, $D^*_j$ for $j \\in \\{1, \\dots, B\\}$, is created by drawing $n$ pairs with replacement from the original dataset $D$.\n$4$. For each bootstrap sample $D^*_j$, the OLS slope estimate, denoted $\\hat{\\beta}_{1,j}^*$, is calculated using the formula above with the data from $D^*_j$.\n$5$. This process yields a collection of $B$ bootstrap slope estimates, $\\{\\hat{\\beta}_{1,1}^*, \\hat{\\beta}_{1,2}^*, \\dots, \\hat{\\beta}_{1,B}^*\\}$. This collection serves as an empirical approximation of the sampling distribution of the estimator $\\hat{\\beta}_1$.\n\nTo construct the confidence interval, we use the percentile method. For a confidence level of $1-\\alpha = 0.95$, we have $\\alpha=0.05$. The lower and upper bounds of the confidence interval are determined by the $\\alpha/2 = 0.025$ and $1-\\alpha/2 = 0.975$ quantiles of the sorted bootstrap distribution of slopes.\n- Let the sorted list of bootstrap slopes be $\\hat{\\beta}_{1,(1)}^* \\leq \\hat{\\beta}_{1,(2)}^* \\leq \\dots \\leq \\hat{\\beta}_{1,(B)}^*$.\n- The lower endpoint $\\ell$ is the value at the $B \\times (\\alpha/2)$ position of this sorted list.\n- The upper endpoint $u$ is the value at the $B \\times (1-\\alpha/2)$ position.\n\nThe implementation will be carried out in Python using the `numpy` library. For each case, we will:\n- Define the data arrays $x$ and $y$.\n- Instantiate a random number generator with the given seed.\n- Loop $B=8000$ times. In each iteration, generate bootstrap indices, select the corresponding pairs to form a bootstrap sample, and compute the slope $\\hat{\\beta}_1^*$.\n- After the loop, `numpy.quantile` will be used to efficiently compute the $0.025$ and $0.975$ quantiles of the collection of $8000$ slope estimates.\n- The resulting endpoints will be rounded to six decimal places as required. The final output will be formatted into a single string representing a list of lists.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes 95% bootstrap confidence intervals for the slope of a simple linear regression model\n    for four different bioinformatics datasets.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"x\": np.array([0.34, 0.36, 0.40, 0.41, 0.43, 0.45, 0.48, 0.50, 0.52, 0.55, 0.57, 0.60]),\n            \"y\": np.array([4.10, 4.00, 4.30, 4.50, 4.60, 4.90, 5.00, 5.30, 5.40, 5.80, 6.00, 6.20]),\n            \"B\": 8000,\n            \"seed\": 13579\n        },\n        {\n            \"x\": np.array([0.31, 0.33, 0.35, 0.38, 0.40, 0.42, 0.44, 0.47, 0.49, 0.51, 0.53, 0.56]),\n            \"y\": np.array([5.10, 4.95, 5.05, 5.00, 5.02, 4.98, 5.01, 5.04, 4.96, 5.03, 4.97, 5.00]),\n            \"B\": 8000,\n            \"seed\": 24680\n        },\n        {\n            \"x\": np.array([0.30, 0.34, 0.37, 0.39, 0.41, 0.44, 0.46, 0.49, 0.51, 0.54, 0.58, 0.61]),\n            \"y\": np.array([7.00, 6.80, 6.60, 6.50, 6.30, 6.10, 6.00, 5.90, 5.70, 5.50, 5.30, 5.20]),\n            \"B\": 8000,\n            \"seed\": 11235\n        },\n        {\n            \"x\": np.array([0.32, 0.36, 0.40, 0.45, 0.50, 0.54, 0.58, 0.62]),\n            \"y\": np.array([3.80, 3.95, 4.10, 4.30, 4.45, 4.60, 4.80, 4.95]),\n            \"B\": 8000,\n            \"seed\": 98765\n        }\n    ]\n\n    all_results = []\n    confidence_level = 0.95\n    alpha = 1.0 - confidence_level\n    lower_quantile = alpha / 2.0\n    upper_quantile = 1.0 - lower_quantile\n\n    def calculate_slope(x, y):\n        \"\"\"Calculates the OLS slope coefficient.\"\"\"\n        x_mean = np.mean(x)\n        y_mean = np.mean(y)\n        numerator = np.sum((x - x_mean) * (y - y_mean))\n        denominator = np.sum((x - x_mean)**2)\n        # Denominator is non-zero for the given datasets and their bootstrap samples\n        # as x values are not all identical.\n        return numerator / denominator\n\n    for case in test_cases:\n        x_data = case[\"x\"]\n        y_data = case[\"y\"]\n        n = len(x_data)\n        B = case[\"B\"]\n        seed = case[\"seed\"]\n\n        rng = np.random.default_rng(seed)\n        \n        bootstrap_slopes = np.empty(B)\n\n        for i in range(B):\n            indices = rng.choice(n, size=n, replace=True)\n            x_boot = x_data[indices]\n            y_boot = y_data[indices]\n            \n            # The case where all x_boot values are identical is highly improbable\n            # and does not occur with the given seeds and data. We proceed without\n            # an explicit check for denominator == 0 for efficiency.\n            bootstrap_slopes[i] = calculate_slope(x_boot, y_boot)\n        \n        ci_lower = np.quantile(bootstrap_slopes, lower_quantile)\n        ci_upper = np.quantile(bootstrap_slopes, upper_quantile)\n\n        rounded_result = [round(ci_lower, 6), round(ci_upper, 6)]\n        all_results.append(rounded_result)\n\n    # Format the output string exactly as specified.\n    # `map(str, all_results)` produces strings like '[1.23, 4.56]',\n    # which are then joined by commas.\n    print(f\"[{','.join(map(str, all_results))}]\".replace(\" \", \"\"))\n\nsolve()\n```", "id": "2429424"}]}