{"hands_on_practices": [{"introduction": "The most fundamental task after fitting a statistical model is to interpret its coefficients. In models with categorical predictors, this means understanding how the chosen coding scheme relates the estimated parameters to the group means. This first exercise provides practice in the most common scenario: translating the coefficients from a model using treatment (or \"dummy\") coding back into the clinically meaningful predicted outcomes for each group [@problem_id:4955281]. Mastering this translation is essential for correctly interpreting and communicating model results.", "problem": "A clinical trial investigates reductions in systolic blood pressure in millimeters of mercury (mmHg) under a categorical drug regimen with $k=4$ levels: placebo, $A$, $B$, and $C$. Consider the continuous outcome for patient $i$, denoted $Y_i$, modeled using the general linear model with identity link as\n$$\nY_i \\;=\\; \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta} \\;+\\; \\varepsilon_i,\n$$\nwhere $\\mathbf{x}_i$ is the row vector of covariates coded from the categorical drug factor, $\\boldsymbol{\\beta}$ is the coefficient vector, and $\\varepsilon_i$ is a mean-zero error term with constant variance. Under treatment coding for a factor with $k$ levels, choose the placebo group as the baseline and include an intercept column. Let the next $k-1$ columns be indicator variables for levels $A$, $B$, and $C$, respectively, ensuring full column rank by omitting the placebo indicator.\n\nYou observe a sample of $n=5$ patients with drug levels $[\\text{placebo}, A, B, A, C]$ in this order. Construct the $n \\times p$ design matrix $\\mathbf{X}$ (with $p=4$ columns: intercept, indicator for $A$, indicator for $B$, indicator for $C$) for this sample under the above treatment coding. A previously fitted ordinary least squares model provides the estimated coefficients (expressed in millimeters of mercury),\n$$\n\\hat{\\boldsymbol{\\beta}} \\;=\\; \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_A \\\\ \\hat{\\beta}_B \\\\ \\hat{\\beta}_C \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n1.9735 \\\\\n4.2561 \\\\\n3.0018 \\\\\n6.4472\n\\end{pmatrix},\n$$\nwhere $\\hat{\\beta}_0$ is the fitted mean for the placebo baseline and $\\hat{\\beta}_A$, $\\hat{\\beta}_B$, and $\\hat{\\beta}_C$ are fitted deviations from baseline for $A$, $B$, and $C$ under treatment coding.\n\nUsing the fundamental properties of linear models and the definition of indicator variables, compute the fitted group means (predicted mean outcomes) for the four drug levels $\\{\\text{placebo}, A, B, C\\}$ implied by $\\hat{\\boldsymbol{\\beta}}$. Express the fitted group means in millimeters of mercury and round your answers to four significant figures. Report only the vector of the four fitted group means in the order $\\{\\text{placebo}, A, B, C\\}$ as your final answer.", "solution": "The user wants me to solve the problem.\n\n### Step 1: Extract Givens\n- **Model:** General linear model $Y_i = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta} + \\varepsilon_i$.\n- **Categorical Predictor:** Drug regimen with $k=4$ levels: $\\{\\text{placebo}, A, B, C\\}$.\n- **Coding Scheme:** Treatment coding (also known as dummy coding). The placebo group is the baseline. The model includes an intercept column. The remaining $k-1=3$ columns are indicator variables for levels $A$, $B$, and $C$.\n- **Sample Data:** $n=5$ patients with drug levels $[\\text{placebo}, A, B, A, C]$.\n- **Dimensions:** The design matrix $\\mathbf{X}$ is $n \\times p$, where $n=5$ and $p=4$.\n- **Estimated Coefficient Vector:**\n$$\n\\hat{\\boldsymbol{\\beta}} \\;=\\; \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_A \\\\ \\hat{\\beta}_B \\\\ \\hat{\\beta}_C \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n1.9735 \\\\\n4.2561 \\\\\n3.0018 \\\\\n6.4472\n\\end{pmatrix}\n$$\n- **Interpretation of Coefficients:** $\\hat{\\beta}_0$ is the fitted mean for the placebo group. $\\hat{\\beta}_A$, $\\hat{\\beta}_B$, $\\hat{\\beta}_C$ are the fitted deviations from the baseline for groups $A$, $B$, and $C$, respectively.\n- **Task 1:** Construct the $n \\times p$ design matrix $\\mathbf{X}$.\n- **Task 2:** Compute the fitted group means for all four drug levels.\n- **Reporting Requirement:** Report the vector of four fitted group means in the order $\\{\\text{placebo}, A, B, C\\}$, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on the standard statistical theory of general linear models and the coding of categorical variables, a fundamental concept in biostatistics and many other scientific fields. It is scientifically sound.\n- **Well-Posed:** The problem is well-posed. It provides a clear model specification, a defined coding scheme, and the necessary numerical values ($\\hat{\\boldsymbol{\\beta}}$) to compute the required quantities (fitted group means). The tasks are specific and lead to a unique solution.\n- **Objective:** The problem is stated in precise, objective, and unambiguous mathematical and statistical language.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, missing information, or ambiguity. It is a standard exercise in interpreting the output of a fitted linear model.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete solution will be provided.\n\nThe problem requires two tasks: constructing the design matrix $\\mathbf{X}$ for the given sample and then calculating the fitted group means for all levels of the drug factor.\n\nFirst, we construct the $n \\times p$ design matrix $\\mathbf{X}$, where $n=5$ and $p=4$. The columns of $\\mathbf{X}$ correspond to the intercept, the indicator for drug $A$ ($x_A$), the indicator for drug $B$ ($x_B$), and the indicator for drug $C$ ($x_C$). The rows correspond to the $n=5$ patients with drug levels $[\\text{placebo}, A, B, A, C]$.\n\n- Patient 1 (placebo): The indicator variables for $A$, $B$, and $C$ are all $0$. The row vector is $(1, 0, 0, 0)$.\n- Patient 2 ($A$): The indicator variable for $A$ is $1$, others are $0$. The row vector is $(1, 1, 0, 0)$.\n- Patient 3 ($B$): The indicator variable for $B$ is $1$, others are $0$. The row vector is $(1, 0, 1, 0)$.\n- Patient 4 ($A$): The indicator variable for $A$ is $1$, others are $0$. The row vector is $(1, 1, 0, 0)$.\n- Patient 5 ($C$): The indicator variable for $C$ is $1$, others are $0$. The row vector is $(1, 0, 0, 1)$.\n\nAssembling these row vectors gives the $5 \\times 4$ design matrix $\\mathbf{X}$:\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 1\n\\end{pmatrix}\n$$\n\nNext, we compute the fitted group means. The fitted or predicted value for an observation with covariate vector $\\mathbf{x}_i$ is given by $\\hat{Y}_i = \\mathbf{x}_i^{\\top}\\hat{\\boldsymbol{\\beta}}$. The fitted mean for a particular group is the predicted value for any individual in that group. The model equation is:\n$$\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_A x_A + \\hat{\\beta}_B x_B + \\hat{\\beta}_C x_C\n$$\nWe find the fitted mean for each of the $k=4$ drug levels by substituting the appropriate values for the indicator variables $x_A$, $x_B$, and $x_C$.\n\n1.  **Placebo Group:**\n    For an individual in the placebo group (the baseline), all indicator variables are zero: $x_A=0$, $x_B=0$, $x_C=0$. The covariate vector is $\\mathbf{x}_{\\text{placebo}}^{\\top} = (1, 0, 0, 0)$.\n    The fitted mean, $\\hat{\\mu}_{\\text{placebo}}$, is:\n    $$\n    \\hat{\\mu}_{\\text{placebo}} = \\mathbf{x}_{\\text{placebo}}^{\\top}\\hat{\\boldsymbol{\\beta}} = 1 \\cdot \\hat{\\beta}_0 + 0 \\cdot \\hat{\\beta}_A + 0 \\cdot \\hat{\\beta}_B + 0 \\cdot \\hat{\\beta}_C = \\hat{\\beta}_0\n    $$\n    Substituting the given value:\n    $$\n    \\hat{\\mu}_{\\text{placebo}} = 1.9735\n    $$\n\n2.  **Group A:**\n    For an individual in group $A$, the indicator variable $x_A=1$, while $x_B=0$ and $x_C=0$. The covariate vector is $\\mathbf{x}_{A}^{\\top} = (1, 1, 0, 0)$.\n    The fitted mean, $\\hat{\\mu}_{A}$, is:\n    $$\n    \\hat{\\mu}_{A} = \\mathbf{x}_{A}^{\\top}\\hat{\\boldsymbol{\\beta}} = 1 \\cdot \\hat{\\beta}_0 + 1 \\cdot \\hat{\\beta}_A + 0 \\cdot \\hat{\\beta}_B + 0 \\cdot \\hat{\\beta}_C = \\hat{\\beta}_0 + \\hat{\\beta}_A\n    $$\n    Substituting the given values:\n    $$\n    \\hat{\\mu}_{A} = 1.9735 + 4.2561 = 6.2296\n    $$\n\n3.  **Group B:**\n    For an individual in group $B$, the indicator variable $x_B=1$, while $x_A=0$ and $x_C=0$. The covariate vector is $\\mathbf{x}_{B}^{\\top} = (1, 0, 1, 0)$.\n    The fitted mean, $\\hat{\\mu}_{B}$, is:\n    $$\n    \\hat{\\mu}_{B} = \\mathbf{x}_{B}^{\\top}\\hat{\\boldsymbol{\\beta}} = 1 \\cdot \\hat{\\beta}_0 + 0 \\cdot \\hat{\\beta}_A + 1 \\cdot \\hat{\\beta}_B + 0 \\cdot \\hat{\\beta}_C = \\hat{\\beta}_0 + \\hat{\\beta}_B\n    $$\n    Substituting the given values:\n    $$\n    \\hat{\\mu}_{B} = 1.9735 + 3.0018 = 4.9753\n    $$\n\n4.  **Group C:**\n    For an individual in group $C$, the indicator variable $x_C=1$, while $x_A=0$ and $x_B=0$. The covariate vector is $\\mathbf{x}_{C}^{\\top} = (1, 0, 0, 1)$.\n    The fitted mean, $\\hat{\\mu}_{C}$, is:\n    $$\n    \\hat{\\mu}_{C} = \\mathbf{x}_{C}^{\\top}\\hat{\\boldsymbol{\\beta}} = 1 \\cdot \\hat{\\beta}_0 + 0 \\cdot \\hat{\\beta}_A + 0 \\cdot \\hat{\\beta}_B + 1 \\cdot \\hat{\\beta}_C = \\hat{\\beta}_0 + \\hat{\\beta}_C\n    $$\n    Substituting the given values:\n    $$\n    \\hat{\\mu}_{C} = 1.9735 + 6.4472 = 8.4207\n    $$\n\nFinally, we round these results to four significant figures as requested.\n- $\\hat{\\mu}_{\\text{placebo}} = 1.9735 \\approx 1.974$\n- $\\hat{\\mu}_{A} = 6.2296 \\approx 6.230$\n- $\\hat{\\mu}_{B} = 4.9753 \\approx 4.975$\n- $\\hat{\\mu}_{C} = 8.4207 \\approx 8.421$\n\nThe vector of the four fitted group means in the order $\\{\\text{placebo}, A, B, C\\}$ is $(1.974, 6.230, 4.975, 8.421)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.974 & 6.230 & 4.975 & 8.421\n\\end{pmatrix}\n}\n$$", "id": "4955281"}, {"introduction": "While treatment coding is convenient for comparing each experimental group to a baseline, our research questions often involve comparing two active treatments against each other. This exercise demonstrates how to frame such a comparison as a linear \"contrast\" of the model coefficients. You will derive the general form for this contrast and, critically, learn how to calculate its standard error from the model's coefficient covariance matrix, a necessary step for hypothesis testing and constructing confidence intervals [@problem_id:4955292].", "problem": "A clinical research team models systolic blood pressure $Y$ (in millimeters of mercury, $\\mathrm{mmHg}$) at $6$ months after initiating antihypertensive therapy using a linear regression with Ordinary Least Squares (OLS). The primary categorical predictor is the treatment regimen with $4$ levels: regimen $A$ (usual care, baseline), regimen $B$, regimen $C$, and regimen $D$. The team uses treatment coding (also known as dummy coding with a baseline), so the design matrix contains an intercept and three indicator variables for regimens $B$, $C$, and $D$. Let the coefficient vector be $\\beta = (\\beta_{0}, \\beta_{B}, \\beta_{C}, \\beta_{D})^{\\top}$.\n\nStarting from the structure of the linear model $Y = X \\beta + \\varepsilon$ and the definition of treatment coding, derive an explicit general expression for the pairwise contrast between any two non-baseline levels (say levels $i$ and $j$, both in $\\{B, C, D\\}$) as a linear functional of $\\beta$ represented by a contrast vector $c$, and justify how to obtain its standard error from the estimated covariance matrix of $\\widehat{\\beta}$. Your derivation must use only fundamental properties of linear estimators in the linear model and the definition of treatment coding, and must not assume any specific ready-made formula for contrast variances.\n\nThen, apply your derivation to compute the standard error of the estimated contrast between regimens $C$ and $D$ in the following fitted model. The estimated covariance matrix of $(\\widehat{\\beta}_{0}, \\widehat{\\beta}_{B}, \\widehat{\\beta}_{C}, \\widehat{\\beta}_{D})^{\\top}$ (entries in $\\mathrm{mmHg}^{2}$) is\n$$\n\\widehat{\\Sigma} \\;=\\;\n\\begin{pmatrix}\n0.950 & 0.020 & 0.015 & 0.018 \\\\\n0.020 & 0.120 & 0.030 & 0.025 \\\\\n0.015 & 0.030 & 0.110 & 0.028 \\\\\n0.018 & 0.025 & 0.028 & 0.115\n\\end{pmatrix}.\n$$\nExpress the final standard error in $\\mathrm{mmHg}$ and round your answer to four significant figures. Do not report any intermediate results; only report the standard error.", "solution": "The problem asks for a derivation of the expression for a pairwise contrast between two non-baseline treatment levels and its standard error, followed by a specific calculation.\n\nFirst, let us establish the theoretical framework. The model is a general linear model $Y = X \\beta + \\varepsilon$, where $Y$ is the response vector (systolic blood pressure), $X$ is the design matrix, $\\beta$ is the vector of coefficients, and $\\varepsilon$ is the vector of errors with $E[\\varepsilon] = 0$ and $\\text{Var}(\\varepsilon) = \\sigma^2 I$. The primary predictor is a categorical variable with $4$ levels: A (baseline), B, C, and D.\n\nWith treatment (dummy) coding and regimen A as the baseline, the coefficient vector is $\\beta = (\\beta_0, \\beta_B, \\beta_C, \\beta_D)^\\top$. For an individual observation, the model for the expected response $E[Y]$ depends on the treatment regimen as follows:\n- For regimen A (baseline): The row of the design matrix is $(1, 0, 0, 0)$. The expected response is $E[Y | \\text{Regimen A}] = 1 \\cdot \\beta_0 + 0 \\cdot \\beta_B + 0 \\cdot \\beta_C + 0 \\cdot \\beta_D = \\beta_0$.\n- For regimen B: The row of the design matrix is $(1, 1, 0, 0)$. The expected response is $E[Y | \\text{Regimen B}] = 1 \\cdot \\beta_0 + 1 \\cdot \\beta_B + 0 \\cdot \\beta_C + 0 \\cdot \\beta_D = \\beta_0 + \\beta_B$.\n- For regimen C: The row of the design matrix is $(1, 0, 1, 0)$. The expected response is $E[Y | \\text{Regimen C}] = 1 \\cdot \\beta_0 + 0 \\cdot \\beta_B + 1 \\cdot \\beta_C + 0 \\cdot \\beta_D = \\beta_0 + \\beta_C$.\n- For regimen D: The row of the design matrix is $(1, 0, 0, 1)$. The expected response is $E[Y | \\text{Regimen D}] = 1 \\cdot \\beta_0 + 0 \\cdot \\beta_B + 0 \\cdot \\beta_C + 1 \\cdot \\beta_D = \\beta_0 + \\beta_D$.\n\nFrom these expressions, the coefficients are interpreted as:\n- $\\beta_0$: The mean systolic blood pressure for the baseline group (Regimen A).\n- $\\beta_B, \\beta_C, \\beta_D$: The differences in mean systolic blood pressure between regimens B, C, and D, respectively, and the baseline regimen A. For example, $\\beta_B = E[Y | \\text{Regimen B}] - E[Y | \\text{Regimen A}]$.\n\nThe problem requires the derivation of an expression for the pairwise contrast between any two non-baseline levels, say level $i$ and level $j$, where $i, j \\in \\{B, C, D\\}$ and $i \\neq j$. This contrast, let's call it $\\theta_{ij}$, is the difference in their expected responses:\n$$\n\\theta_{ij} = E[Y | \\text{Regimen } i] - E[Y | \\text{Regimen } j]\n$$\nSubstituting the model-based expressions for the expected values:\n$$\n\\theta_{ij} = (\\beta_0 + \\beta_i) - (\\beta_0 + \\beta_j) = \\beta_i - \\beta_j\n$$\nThis contrast is a linear combination of the elements of the coefficient vector $\\beta$. We can express it as a linear functional $\\theta_{ij} = c^\\top \\beta$, where $c$ is a contrast vector. For the contrast $\\beta_i - \\beta_j$, the vector $c$ has a $1$ in the position corresponding to $\\beta_i$, a $-1$ in the position corresponding to $\\beta_j$, and $0$s in all other positions. For our $\\beta = (\\beta_0, \\beta_B, \\beta_C, \\beta_D)^\\top$, the contrast $\\beta_C - \\beta_B$ would correspond to $c = (0, -1, 1, 0)^\\top$.\n\nThe Ordinary Least Squares (OLS) estimator for $\\beta$ is $\\widehat{\\beta} = (X^\\top X)^{-1} X^\\top Y$. The estimator for the contrast is $\\widehat{\\theta}_{ij} = c^\\top \\widehat{\\beta}$. Since $\\widehat{\\beta}$ is a random vector, $\\widehat{\\theta}_{ij}$ is a scalar random variable. The variance of the estimated contrast is derived from the fundamental properties of the variance of a linear combination of random variables. If a random vector $\\mathbf{Z}$ has a covariance matrix $\\Sigma_{\\mathbf{Z}}$, the variance of a linear combination $\\mathbf{a}^\\top \\mathbf{Z}$ is given by $\\text{Var}(\\mathbf{a}^\\top \\mathbf{Z}) = \\mathbf{a}^\\top \\Sigma_{\\mathbf{Z}} \\mathbf{a}$.\n\nLet $\\Sigma_{\\widehat{\\beta}}$ be the covariance matrix of the estimator $\\widehat{\\beta}$. Applying the property above, the variance of the estimated contrast is:\n$$\n\\text{Var}(\\widehat{\\theta}_{ij}) = \\text{Var}(c^\\top \\widehat{\\beta}) = c^\\top \\Sigma_{\\widehat{\\beta}} c\n$$\nThe standard error (SE) is the square root of the variance:\n$$\n\\text{SE}(\\widehat{\\theta}_{ij}) = \\sqrt{c^\\top \\Sigma_{\\widehat{\\beta}} c}\n$$\nIn a practical application, we use the estimated covariance matrix of $\\widehat{\\beta}$, denoted $\\widehat{\\Sigma}$, which is provided in the problem. The estimated standard error is therefore:\n$$\n\\widehat{\\text{SE}}(\\widehat{\\theta}_{ij}) = \\sqrt{c^\\top \\widehat{\\Sigma} c}\n$$\nFor our specific contrast $\\widehat{\\theta}_{ij} = \\widehat{\\beta}_i - \\widehat{\\beta}_j$, the general formula $c^\\top \\widehat{\\Sigma} c$ with the corresponding contrast vector $c$ simplifies to:\n$$\n\\text{Var}(\\widehat{\\beta}_i - \\widehat{\\beta}_j) = \\text{Var}(\\widehat{\\beta}_i) + \\text{Var}(\\widehat{\\beta}_j) - 2 \\text{Cov}(\\widehat{\\beta}_i, \\widehat{\\beta}_j)\n$$\nThis completes the required derivation.\n\nNow, we apply this derivation to compute the standard error of the estimated contrast between regimens C and D. The contrast is $\\theta = \\beta_C - \\beta_D$. The estimator is $\\widehat{\\theta} = \\widehat{\\beta}_C - \\widehat{\\beta}_D$.\nThe corresponding contrast vector for $\\beta = (\\beta_0, \\beta_B, \\beta_C, \\beta_D)^\\top$ is $c = (0, 0, 1, -1)^\\top$.\nThe estimated variance of this contrast is:\n$$\n\\widehat{\\text{Var}}(\\widehat{\\theta}) = \\widehat{\\text{Var}}(\\widehat{\\beta}_C) + \\widehat{\\text{Var}}(\\widehat{\\beta}_D) - 2 \\widehat{\\text{Cov}}(\\widehat{\\beta}_C, \\widehat{\\beta}_D)\n$$\nThe problem provides the estimated covariance matrix of $(\\widehat{\\beta}_{0}, \\widehat{\\beta}_{B}, \\widehat{\\beta}_{C}, \\widehat{\\beta}_{D})^{\\top}$:\n$$\n\\widehat{\\Sigma} \\;=\\;\n\\begin{pmatrix}\n0.950 & 0.020 & 0.015 & 0.018 \\\\\n0.020 & 0.120 & 0.030 & 0.025 \\\\\n0.015 & 0.030 & 0.110 & 0.028 \\\\\n0.018 & 0.025 & 0.028 & 0.115\n\\end{pmatrix}\n$$\nFrom this matrix, we identify the necessary components. The variances are on the diagonal and the covariances are off-diagonal. The indices correspond to the order $(0, B, C, D)$, which can be mapped to $(1, 2, 3, 4)$.\n- The variance of $\\widehat{\\beta}_C$ is $\\widehat{\\text{Var}}(\\widehat{\\beta}_C) = \\widehat{\\Sigma}_{3,3} = 0.110 \\, \\mathrm{mmHg}^2$.\n- The variance of $\\widehat{\\beta}_D$ is $\\widehat{\\text{Var}}(\\widehat{\\beta}_D) = \\widehat{\\Sigma}_{4,4} = 0.115 \\, \\mathrm{mmHg}^2$.\n- The covariance of $\\widehat{\\beta}_C$ and $\\widehat{\\beta}_D$ is $\\widehat{\\text{Cov}}(\\widehat{\\beta}_C, \\widehat{\\beta}_D) = \\widehat{\\Sigma}_{3,4} = 0.028 \\, \\mathrm{mmHg}^2$.\n\nSubstituting these values into the variance formula:\n$$\n\\widehat{\\text{Var}}(\\widehat{\\theta}) = 0.110 + 0.115 - 2(0.028)\n$$\n$$\n\\widehat{\\text{Var}}(\\widehat{\\theta}) = 0.225 - 0.056 = 0.169 \\, \\mathrm{mmHg}^2\n$$\nThe standard error is the square root of the variance:\n$$\n\\widehat{\\text{SE}}(\\widehat{\\theta}) = \\sqrt{0.169} \\approx 0.411096096 \\, \\mathrm{mmHg}\n$$\nRounding the result to four significant figures, we get $0.4111 \\, \\mathrm{mmHg}$.", "answer": "$$\\boxed{0.4111}$$", "id": "4955292"}, {"introduction": "The properties of our parameter estimates are influenced by both the coding scheme and the study design. In balanced designs, effect coding offers an elegant parameterization where the intercept is the grand mean and effects are orthogonal to it. This exercise explores what happens when this balance is lost, a common situation in real-world medical studies [@problem_id:4955282]. By deriving the covariance between the intercept and an effect coefficient, you will gain a deeper appreciation for how unbalanced sample sizes introduce correlations between estimators and complicate interpretations.", "problem": "A three-arm randomized clinical trial evaluates a continuous biomarker response $Y$ under three treatment regimens $A$, $B$, and $C$ with unequal sample sizes $n_{1}$, $n_{2}$, and $n_{3}$, respectively. A linear regression model is fit using Ordinary Least Squares (OLS) with an intercept and an effect-coded representation of the treatment factor. Specifically, the design matrix columns are: an intercept column of ones; a first effect-coded column $x_{1}$ that takes value $1$ for regimen $A$, $0$ for regimen $B$, and $-1$ for regimen $C$; and a second effect-coded column $x_{2}$ that takes value $0$ for regimen $A$, $1$ for regimen $B$, and $-1$ for regimen $C$. Assume homoscedastic errors $\\varepsilon$ with $\\operatorname{Var}(\\varepsilon) = \\sigma^{2} I$ and no other predictors.\n\nStarting from the general linear model $Y = X \\beta + \\varepsilon$ and the well-tested OLS fact $\\operatorname{Var}(\\hat{\\beta}) = \\sigma^{2} (X^{\\top} X)^{-1}$, derive the design cross-product $X^{\\top} X$ under this effect coding and unequal cell sizes, and use it to obtain the relevant block of $(X^{\\top} X)^{-1}$.\n\nExplain, based on the structure of $X^{\\top} X$, why the intuitive “orthogonality” between the intercept and the effect-coded treatment columns that holds under balanced designs fails when $n_{1}$, $n_{2}$, and $n_{3}$ are unequal, and demonstrate how this failure manifests in the covariance of the estimated coefficients.\n\nYour task is to provide a single closed-form analytic expression for the covariance between the estimated intercept and the estimated coefficient associated with $x_{1}$, namely $\\operatorname{Cov}(\\hat{\\beta}_{0}, \\hat{\\beta}_{1})$, as a function of $n_{1}$, $n_{2}$, $n_{3}$, and $\\sigma^{2}$. Express your final answer as a simplified analytic expression. No rounding is required.", "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n-   **Model**: A three-arm randomized clinical trial with a continuous biomarker response $Y$.\n-   **Treatments**: Three regimens denoted as $A$, $B$, and $C$.\n-   **Sample Sizes**: Unequal sample sizes $n_{1}$, $n_{2}$, and $n_{3}$ for treatments $A$, $B$, and $C$ respectively.\n-   **Statistical Model**: A general linear model $Y = X \\beta + \\varepsilon$ is fit using Ordinary Least Squares (OLS).\n-   **Error Structure**: Homoscedastic errors $\\varepsilon$ with $\\operatorname{Var}(\\varepsilon) = \\sigma^{2} I$, where $I$ is the identity matrix.\n-   **Design Matrix ($X$)**: The design matrix includes:\n    1.  An intercept column ($x_{0}$) of ones.\n    2.  A first effect-coded column ($x_{1}$) taking values $\\{1 \\text{ for } A, 0 \\text{ for } B, -1 \\text{ for } C\\}$.\n    3.  A second effect-coded column ($x_{2}$) taking values $\\{0 \\text{ for } A, 1 \\text{ for } B, -1 \\text{ for } C\\}$.\n-   **Variance of Coefficients**: The variance-covariance matrix of the OLS estimator $\\hat{\\beta}$ is given by $\\operatorname{Var}(\\hat{\\beta}) = \\sigma^{2} (X^{\\top} X)^{-1}$.\n-   **Objective**: Derive a closed-form analytic expression for the covariance between the estimated intercept and the estimated coefficient for $x_{1}$, i.e., $\\operatorname{Cov}(\\hat{\\beta}_{0}, \\hat{\\beta}_{1})$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is firmly rooted in the theory of linear models, a fundamental area of statistics. Effect coding, OLS estimation, and the calculation of the variance-covariance matrix of estimators are standard, well-established concepts. The setup is scientifically and mathematically sound.\n2.  **Well-Posed**: The problem is clearly defined. It provides all necessary information (the model structure, coding scheme, and sample sizes) to construct the design matrix and derive the required covariance. A unique, meaningful solution exists.\n3.  **Objective**: The problem is stated in precise, technical language, free from any subjectivity, ambiguity, or opinion.\n\nThe problem is self-contained, consistent, and does not violate any of the specified invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete solution will be provided.\n\n### Derivation of the Covariance\nThe linear model is specified as $Y = X \\beta + \\varepsilon$, where $\\beta = (\\beta_0, \\beta_1, \\beta_2)^\\top$ is the vector of coefficients corresponding to the intercept ($x_0$), the first effect-coded predictor ($x_1$), and the second effect-coded predictor ($x_2$). The total number of observations is $N = n_1 + n_2 + n_3$.\n\nThe design matrix $X$ is an $N \\times 3$ matrix. Its columns, denoted $\\mathbf{x}_0$, $\\mathbf{x}_1$, and $\\mathbf{x}_2$, are defined by the problem statement. We first construct the cross-product matrix $X^\\top X$. The elements of this matrix are the inner products of the columns of $X$.\nThe entries are calculated as follows:\n$\\mathbf{x}_0^\\top \\mathbf{x}_0 = \\sum_{i=1}^{N} 1^2 = n_1+n_2+n_3 = N$.\n$\\mathbf{x}_0^\\top \\mathbf{x}_1 = \\sum_{i=1}^{N} x_{i1} = n_1(1) + n_2(0) + n_3(-1) = n_1 - n_3$.\n$\\mathbf{x}_0^\\top \\mathbf{x}_2 = \\sum_{i=1}^{N} x_{i2} = n_1(0) + n_2(1) + n_3(-1) = n_2 - n_3$.\n$\\mathbf{x}_1^\\top \\mathbf{x}_1 = \\sum_{i=1}^{N} x_{i1}^2 = n_1(1^2) + n_2(0^2) + n_3((-1)^2) = n_1 + n_3$.\n$\\mathbf{x}_2^\\top \\mathbf{x}_2 = \\sum_{i=1}^{N} x_{i2}^2 = n_1(0^2) + n_2(1^2) + n_3((-1)^2) = n_2 + n_3$.\n$\\mathbf{x}_1^\\top \\mathbf{x}_2 = \\sum_{i=1}^{N} x_{i1}x_{i2} = n_1(1)(0) + n_2(0)(1) + n_3(-1)(-1) = n_3$.\n\nThus, the $X^\\top X$ matrix is:\n$$\nX^\\top X = \\begin{pmatrix}\nn_1+n_2+n_3 & n_1-n_3 & n_2-n_3 \\\\\nn_1-n_3 & n_1+n_3 & n_3 \\\\\nn_2-n_3 & n_3 & n_2+n_3\n\\end{pmatrix}\n$$\n\nThe \"orthogonality\" between predictors in a design matrix refers to the condition where the inner product of their corresponding column vectors is zero. For the intercept column $\\mathbf{x}_0$ and the effect-coded column $\\mathbf{x}_1$, their inner product is $\\mathbf{x}_0^\\top \\mathbf{x}_1 = n_1 - n_3$. This term is zero only if $n_1 = n_3$. For a fully balanced design, where $n_1=n_2=n_3$, all off-diagonal elements of $X^\\top X$ involving the intercept become zero, signifying orthogonality. When the sample sizes are unequal, these off-diagonal terms are generally non-zero. This non-orthogonality (or collinearity) means that the estimation of one coefficient is not independent of the others. This dependency is captured by the off-diagonal elements of the covariance matrix $\\operatorname{Var}(\\hat{\\beta}) = \\sigma^2 (X^\\top X)^{-1}$. Non-zero off-diagonal elements in $X^\\top X$ lead to non-zero off-diagonal elements in its inverse, $(X^\\top X)^{-1}$, which implies that the covariances between the estimated coefficients, $\\operatorname{Cov}(\\hat{\\beta}_i, \\hat{\\beta}_j)$ for $i \\neq j$, are non-zero.\n\nThe task is to find $\\operatorname{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1)$, which is given by $\\sigma^2$ times the element in the first row and second column of $(X^\\top X)^{-1}$. Direct inversion of this $3 \\times 3$ matrix is algebraically cumbersome. A more insightful approach is to interpret the model parameters in terms of the group means.\n\nLet $\\mu_A$, $\\mu_B$, and $\\mu_C$ be the true mean responses for the three treatment groups. The model equation $E[Y|X] = X\\beta$ implies the following relationships based on the coding:\nFor group A ($x_1=1, x_2=0$): $\\mu_A = \\beta_0 + \\beta_1(1) + \\beta_2(0) = \\beta_0 + \\beta_1$.\nFor group B ($x_1=0, x_2=1$): $\\mu_B = \\beta_0 + \\beta_1(0) + \\beta_2(1) = \\beta_0 + \\beta_2$.\nFor group C ($x_1=-1, x_2=-1$): $\\mu_C = \\beta_0 + \\beta_1(-1) + \\beta_2(-1) = \\beta_0 - \\beta_1 - \\beta_2$.\n\nThis system of linear equations can be solved for the $\\beta$ parameters in terms of the group means:\nSumming the three equations gives $\\mu_A + \\mu_B + \\mu_C = 3\\beta_0$, which implies:\n$$ \\beta_0 = \\frac{\\mu_A + \\mu_B + \\mu_C}{3} $$\nFrom the equation for $\\mu_A$, we have $\\beta_1 = \\mu_A - \\beta_0$. Substituting the expression for $\\beta_0$:\n$$ \\beta_1 = \\mu_A - \\frac{\\mu_A + \\mu_B + \\mu_C}{3} = \\frac{3\\mu_A - \\mu_A - \\mu_B - \\mu_C}{3} = \\frac{2\\mu_A - \\mu_B - \\mu_C}{3} $$\nThe OLS estimators for the group means are the sample means: $\\hat{\\mu}_A = \\bar{Y}_A$, $\\hat{\\mu}_B = \\bar{Y}_B$, and $\\hat{\\mu}_C = \\bar{Y}_C$. Due to the properties of OLS in this reparameterization (equivalent to the invariance property of Maximum Likelihood Estimators under normality), the OLS estimators for $\\beta_0$ and $\\beta_1$ are the same linear combinations of the sample means:\n$$ \\hat{\\beta}_0 = \\frac{\\bar{Y}_A + \\bar{Y}_B + \\bar{Y}_C}{3} $$\n$$ \\hat{\\beta}_1 = \\frac{2\\bar{Y}_A - \\bar{Y}_B - \\bar{Y}_C}{3} $$\nWe can now calculate the covariance using these expressions. The sample means from the three independent treatment groups are uncorrelated random variables. Their variances are given by $\\operatorname{Var}(\\bar{Y}_A) = \\sigma^2/n_1$, $\\operatorname{Var}(\\bar{Y}_B) = \\sigma^2/n_2$, and $\\operatorname{Var}(\\bar{Y}_C) = \\sigma^2/n_3$.\nUsing the formula for the covariance of two linear combinations of random variables, $\\operatorname{Cov}(\\sum a_i Z_i, \\sum b_i Z_i) = \\sum a_i b_i \\operatorname{Var}(Z_i)$ for independent $Z_i$:\n$$ \\operatorname{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\operatorname{Cov}\\left( \\frac{1}{3}\\bar{Y}_A + \\frac{1}{3}\\bar{Y}_B + \\frac{1}{3}\\bar{Y}_C, \\quad \\frac{2}{3}\\bar{Y}_A - \\frac{1}{3}\\bar{Y}_B - \\frac{1}{3}\\bar{Y}_C \\right) $$\n$$ = \\left(\\frac{1}{3}\\right)\\left(\\frac{2}{3}\\right)\\operatorname{Var}(\\bar{Y}_A) + \\left(\\frac{1}{3}\\right)\\left(-\\frac{1}{3}\\right)\\operatorname{Var}(\\bar{Y}_B) + \\left(\\frac{1}{3}\\right)\\left(-\\frac{1}{3}\\right)\\operatorname{Var}(\\bar{Y}_C) $$\n$$ = \\frac{2}{9}\\left(\\frac{\\sigma^2}{n_1}\\right) - \\frac{1}{9}\\left(\\frac{\\sigma^2}{n_2}\\right) - \\frac{1}{9}\\left(\\frac{\\sigma^2}{n_3}\\right) $$\nFactoring out the common terms yields the final expression:\n$$ \\operatorname{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\frac{\\sigma^2}{9} \\left( \\frac{2}{n_1} - \\frac{1}{n_2} - \\frac{1}{n_3} \\right) $$\nThis expression clearly shows that the covariance is zero in the balanced case where $n_1 = n_2 = n_3$, confirming the link between unequal sample sizes (lack of balance) and the correlation of estimators for the intercept and treatment effects.", "answer": "$$ \\boxed{\\frac{\\sigma^{2}}{9} \\left( \\frac{2}{n_{1}} - \\frac{1}{n_{2}} - \\frac{1}{n_{3}} \\right)} $$", "id": "4955282"}]}