{"hands_on_practices": [{"introduction": "Modeling non-linear relationships is a common task, but naively including raw polynomial terms like $x^2$ and $x^3$ can introduce high correlation between predictors, leading to numerical instability. This practice demonstrates how to build a more robust polynomial regression model by first creating a set of orthogonal predictors using the Gram-Schmidt process. This approach not only enhances numerical stability but also provides deeper insight into the geometry of least squares regression [@problem_id:4977056].", "problem": "A small observational study investigates the relationship between age and high-sensitivity C-reactive protein (hsCRP) in adults without acute infection. Let the response variable be hsCRP concentration in milligrams per liter, denoted by $y$ (mg/L), and the explanatory variable be age in years, denoted by $x$. The multiple linear regression model permits a polynomial dependence on $x$ and an intercept, with additive noise $\\varepsilon$, assumed to have mean zero and finite variance. To improve numerical stability relative to raw power terms, the polynomial model is constructed using orthogonal polynomials obtained via Gram–Schmidt on the centered age $x_{c} = x - \\bar{x}$. The inner product between two functions $f$ and $g$ evaluated at the observed sample is defined by $\\langle f, g \\rangle = \\sum_{i=1}^{n} f(x_{c,i}) g(x_{c,i})$, where $n$ is the sample size and $x_{c,i}$ are the centered ages.\n\nData for $n = 5$ subjects:\n- Ages in years: $\\{36, 38, 40, 42, 44\\}$.\n- hsCRP values (mg/L): $\\{3.7, 1.9, 1.3, 1.9, 3.7\\}$.\n\nConsider the model\n$$\ny_i = \\beta_0 p_0(x_{c,i}) + \\beta_1 p_1(x_{c,i}) + \\beta_2 p_2(x_{c,i}) + \\beta_3 p_3(x_{c,i}) + \\varepsilon_i,\n$$\nwhere $\\{p_0, p_1, p_2, p_3\\}$ are orthogonal polynomials obtained by applying the Gram–Schmidt procedure to $\\{1, x_c, x_c^2, x_c^3\\}$ under the given inner product. Fit this model by ordinary least squares using the orthogonality implied by the Gram–Schmidt construction, and then express the fitted model in the raw monomial basis $\\{1, x_c, x_c^2, x_c^3\\}$:\n$$\ny_i = \\alpha_0 + \\alpha_1 x_{c,i} + \\alpha_2 x_{c,i}^2 + \\alpha_3 x_{c,i}^3 + \\varepsilon_i.\n$$\n\nStarting from the foundational definitions of least squares estimation and the Gram–Schmidt process, derive the orthogonal polynomials and the corresponding least squares coefficients, convert the fitted model to the raw monomial basis, and determine the numerical value of the raw quadratic coefficient $\\alpha_2$. Express the final coefficient numerically in $\\text{mg/L per year}^2$ and round your answer to four significant figures.", "solution": "The user wants to find the coefficient $\\alpha_2$ of the quadratic term in a polynomial regression model expressed in the raw monomial basis. The problem specifies that the model should first be fit using an orthogonal polynomial basis derived via the Gram–Schmidt process.\n\nThe model in the raw monomial basis centered at the mean age is given by\n$$\n\\hat{y}_i = \\alpha_0 + \\alpha_1 x_{c,i} + \\alpha_2 x_{c,i}^2 + \\alpha_3 x_{c,i}^3\n$$\nwhere $x_{c,i} = x_i - \\bar{x}$. The model is to be estimated by first fitting an equivalent model in an orthogonal basis $\\{p_0, p_1, p_2, p_3\\}$:\n$$\n\\hat{y}_i = \\hat{\\beta}_0 p_0(x_{c,i}) + \\hat{\\beta}_1 p_1(x_{c,i}) + \\hat{\\beta}_2 p_2(x_{c,i}) + \\hat{\\beta}_3 p_3(x_{c,i})\n$$\nThe problem requires us to find $\\alpha_2$.\n\n**Step 1: Data Preparation and Preliminary Calculations**\nFirst, we prepare the data. The given ages are $x = \\{36, 38, 40, 42, 44\\}$. The sample size is $n=5$. The mean age is\n$$\n\\bar{x} = \\frac{36 + 38 + 40 + 42 + 44}{5} = \\frac{200}{5} = 40 \\text{ years}.\n$$\nThe centered age values are $x_{c,i} = x_i - \\bar{x}$:\n$$\n\\{x_{c,i}\\} = \\{36-40, 38-40, 40-40, 42-40, 44-40\\} = \\{-4, -2, 0, 2, 4\\}.\n$$\nThe response variable values are $y = \\{3.7, 1.9, 1.3, 1.9, 3.7\\}$.\nThe inner product for the Gram–Schmidt procedure requires sums of powers of $x_{c,i}$. Due to the symmetry of the $x_{c,i}$ values about $0$, all sums of odd powers are zero: $\\sum x_c^k = 0$ for odd $k$.\n$$\n\\sum_{i=1}^{5} x_{c,i}^0 = \\sum_{i=1}^{5} 1 = 5 \\\\\n\\sum_{i=1}^{5} x_{c,i}^1 = -4 - 2 + 0 + 2 + 4 = 0 \\\\\n\\sum_{i=1}^{5} x_{c,i}^2 = (-4)^2 + (-2)^2 + 0^2 + 2^2 + 4^2 = 16 + 4 + 0 + 4 + 16 = 40 \\\\\n\\sum_{i=1}^{5} x_{c,i}^3 = (-4)^3 + (-2)^3 + 0^3 + 2^3 + 4^3 = -64 - 8 + 0 + 8 + 64 = 0 \\\\\n\\sum_{i=1}^{5} x_{c,i}^4 = (-4)^4 + (-2)^4 + 0^4 + 2^4 + 4^4 = 256 + 16 + 0 + 16 + 256 = 544\n$$\n\n**Step 2: Gram–Schmidt Orthogonalization**\nWe apply the Gram–Schmidt process to the monomial basis $\\{v_0, v_1, v_2, v_3\\} = \\{1, x_c, x_c^2, x_c^3\\}$ using the inner product $\\langle f, g \\rangle = \\sum_{i=1}^{n} f(x_{c,i})g(x_{c,i})$.\n\nThe first orthogonal polynomial, $p_0$:\n$$\np_0(x_c) = v_0(x_c) = 1\n$$\nThe squared norm is $\\langle p_0, p_0 \\rangle = \\sum_{i=1}^{5} 1^2 = 5$.\n\nThe second orthogonal polynomial, $p_1$:\n$$\np_1(x_c) = v_1(x_c) - \\frac{\\langle v_1, p_0 \\rangle}{\\langle p_0, p_0 \\rangle} p_0(x_c) = x_c - \\frac{\\sum x_c \\cdot 1}{\\sum 1^2} \\cdot 1 = x_c - \\frac{0}{5} \\cdot 1 = x_c\n$$\nThe squared norm is $\\langle p_1, p_1 \\rangle = \\sum x_c^2 = 40$.\n\nThe third orthogonal polynomial, $p_2$:\n$$\np_2(x_c) = v_2(x_c) - \\frac{\\langle v_2, p_0 \\rangle}{\\langle p_0, p_0 \\rangle} p_0(x_c) - \\frac{\\langle v_2, p_1 \\rangle}{\\langle p_1, p_1 \\rangle} p_1(x_c)\n$$\nThe inner products are:\n$\\langle v_2, p_0 \\rangle = \\langle x_c^2, 1 \\rangle = \\sum x_c^2 = 40$.\n$\\langle v_2, p_1 \\rangle = \\langle x_c^2, x_c \\rangle = \\sum x_c^3 = 0$.\nSo,\n$$\np_2(x_c) = x_c^2 - \\frac{40}{5} \\cdot 1 - \\frac{0}{40} \\cdot x_c = x_c^2 - 8\n$$\nThe squared norm is $\\langle p_2, p_2 \\rangle = \\sum (x_c^2 - 8)^2 = \\sum (x_c^4 - 16x_c^2 + 64) = \\sum x_c^4 - 16\\sum x_c^2 + 64\\sum 1 = 544 - 16(40) + 64(5) = 544 - 640 + 320 = 224$.\n\nWe could continue to find $p_3(x_c)$, but we will show this is not necessary to find $\\alpha_2$.\n\n**Step 3: Relationship between Coefficients $\\alpha_k$ and $\\beta_k$**\nThe fitted model can be expressed in either basis. We equate the two forms:\n$$\n\\alpha_0 + \\alpha_1 x_c + \\alpha_2 x_c^2 + \\alpha_3 x_c^3 = \\hat{\\beta}_0 p_0(x_c) + \\hat{\\beta}_1 p_1(x_c) + \\hat{\\beta}_2 p_2(x_c) + \\hat{\\beta}_3 p_3(x_c)\n$$\nSubstitute the expressions for the orthogonal polynomials:\n$$\n\\alpha_0 + \\alpha_1 x_c + \\alpha_2 x_c^2 + \\alpha_3 x_c^3 = \\hat{\\beta}_0 (1) + \\hat{\\beta}_1 (x_c) + \\hat{\\beta}_2 (x_c^2 - 8) + \\hat{\\beta}_3 p_3(x_c)\n$$\nNote that $p_3(x_c)$ will be of the form $x_c^3 + c_{31}x_c + c_{30}$ because $\\langle x_c^3, x_c^2 \\rangle = \\sum x_c^5 = 0$. The $x_c^2$ term will be absent. Collecting terms by powers of $x_c$:\n$$\n\\alpha_0 + \\alpha_1 x_c + \\alpha_2 x_c^2 + \\alpha_3 x_c^3 = (\\hat{\\beta}_0 - 8 \\hat{\\beta}_2 + \\dots) + (\\hat{\\beta}_1 + \\dots)x_c + (\\hat{\\beta}_2)x_c^2 + (\\hat{\\beta}_3)x_c^3\n$$\nBy comparing the coefficients of the powers of $x_c$ on both sides, we find the relationship for the quadratic term coefficient:\n$$\n\\alpha_2 = \\hat{\\beta}_2\n$$\nThus, to find $\\alpha_2$, we only need to compute $\\hat{\\beta}_2$.\n\n**Step 4: Least Squares Estimation of $\\hat{\\beta}_2$**\nThe ordinary least squares estimate for the coefficients in an orthogonal basis is given by the simple formula:\n$$\n\\hat{\\beta}_k = \\frac{\\langle y, p_k \\rangle}{\\langle p_k, p_k \\rangle} = \\frac{\\sum_{i=1}^n y_i p_k(x_{c,i})}{\\sum_{i=1}^n p_k(x_{c,i})^2}\n$$\nFor $k=2$, we have:\n$$\n\\hat{\\beta}_2 = \\frac{\\langle y, p_2 \\rangle}{\\langle p_2, p_2 \\rangle} = \\frac{\\sum_{i=1}^5 y_i (x_{c,i}^2 - 8)}{\\sum_{i=1}^5 (x_{c,i}^2 - 8)^2}\n$$\nThe denominator was already calculated as $\\langle p_2, p_2 \\rangle = 224$.\nWe now calculate the numerator, $\\langle y, p_2 \\rangle$. The values for $p_2(x_{c,i}) = x_{c,i}^2 - 8$ at the sample points are:\n$\\{16-8, 4-8, 0-8, 4-8, 16-8\\} = \\{8, -4, -8, -4, 8\\}$.\nThe hsCRP values are $y = \\{3.7, 1.9, 1.3, 1.9, 3.7\\}$.\n$$\n\\langle y, p_2 \\rangle = \\sum_{i=1}^5 y_i p_2(x_{c,i}) = (3.7)(8) + (1.9)(-4) + (1.3)(-8) + (1.9)(-4) + (3.7)(8)\n$$\n$$\n\\langle y, p_2 \\rangle = 29.6 - 7.6 - 10.4 - 7.6 + 29.6 = 2 \\times (29.6 - 7.6) - 10.4 = 2 \\times (22.0) - 10.4 = 44.0 - 10.4 = 33.6\n$$\nNow we can compute $\\hat{\\beta}_2$:\n$$\n\\hat{\\beta}_2 = \\frac{33.6}{224}\n$$\nTo simplify the fraction:\n$$\n\\hat{\\beta}_2 = \\frac{336}{2240} = \\frac{168}{1120} = \\frac{84}{560} = \\frac{42}{280} = \\frac{21}{140} = \\frac{3}{20} = 0.15\n$$\nSince $\\alpha_2 = \\hat{\\beta}_2$, the value of the raw quadratic coefficient is $\\alpha_2 = 0.15$. The units are those of $y$ divided by the square of the units of $x_c$, which is $\\text{mg/L}/\\text{year}^2$.\n\n**Step 5: Final Answer Formatting**\nThe problem requests the numerical value of $\\alpha_2$ rounded to four significant figures.\n$$\n\\alpha_2 = 0.1500\n$$", "answer": "$$\n\\boxed{0.1500}\n$$", "id": "4977056"}, {"introduction": "In medical studies, the effect of a treatment often varies with patient characteristics like age, a phenomenon captured by interaction terms in a regression model. This exercise challenges you to move beyond interpreting simple coefficients and compute the age-specific marginal treatment effect. You will learn to calculate this conditional effect and, critically, its standard error, by applying the formula for the variance of a linear combination of estimators [@problem_id:4977020].", "problem": "A randomized controlled trial (RCT) evaluates a new antihypertensive therapy on the short-term change in systolic blood pressure (SBP) measured in millimeters of mercury (mmHg) after $3$ months. Let $y$ denote the observed SBP change, with negative values indicating reductions. Patients are characterized by a binary treatment assignment $x_{\\text{treat}} \\in \\{0,1\\}$ and age in years $x_{\\text{age}} \\in \\mathbb{R}$. Consider the multiple linear regression model for the conditional expectation,\n$$\nE[y \\mid x] \\;=\\; \\beta_0 \\;+\\; \\beta_{\\text{treat}}\\, x_{\\text{treat}} \\;+\\; \\beta_{\\text{age}}\\, x_{\\text{age}} \\;+\\; \\beta_{\\text{treat}\\times \\text{age}}\\, \\big(x_{\\text{treat}} \\cdot x_{\\text{age}}\\big),\n$$\nfit by Ordinary Least Squares (OLS). Define the marginal treatment effect at a specified age $a_0$ as the rate of change in the conditional mean with respect to treatment at that age. You are given the OLS coefficient estimates\n$$\n\\hat{\\boldsymbol{\\beta}} \\;=\\; \\big(\\hat{\\beta}_0,\\hat{\\beta}_{\\text{treat}},\\hat{\\beta}_{\\text{age}},\\hat{\\beta}_{\\text{treat}\\times \\text{age}}\\big)^{\\top}\n\\;=\\;\n\\big(1.8,\\,-4.6,\\,0.12,\\,-0.045\\big)^{\\top},\n$$\nand their estimated variance–covariance matrix (in $\\text{mmHg}^2$),\n$$\n\\hat{\\Sigma} \\;=\\;\n\\begin{pmatrix}\n4.0  -0.12  -0.02  0.001 \\\\\n-0.12  1.44  0.03  0.006 \\\\\n-0.02  0.03  0.0025  -0.0002 \\\\\n0.001  0.006  -0.0002  0.0009\n\\end{pmatrix}.\n$$\nTake the specified age as $a_0 = 65$. Using only the definitions of the linear predictor for $E[y \\mid x]$, the notion of a marginal effect as a partial derivative, and the property that the variance of any linear combination of OLS coefficient estimates is determined by the variance–covariance matrix, compute the standard error of the estimated marginal treatment effect at age $a_0$. Round your answer to four significant figures and express it in $\\text{mmHg}$.", "solution": "The multiple linear regression model specifies a linear conditional mean,\n$$\nE[y \\mid x] \\;=\\; \\beta_0 \\;+\\; \\beta_{\\text{treat}}\\, x_{\\text{treat}} \\;+\\; \\beta_{\\text{age}}\\, x_{\\text{age}} \\;+\\; \\beta_{\\text{treat}\\times \\text{age}}\\, \\big(x_{\\text{treat}} \\cdot x_{\\text{age}}\\big).\n$$\nBy definition, the marginal treatment effect at age $x_{\\text{age}} = a_0$ is the partial derivative of the conditional mean with respect to $x_{\\text{treat}}$ evaluated at $x_{\\text{age}} = a_0$:\n$$\n\\frac{\\partial}{\\partial x_{\\text{treat}}} E[y \\mid x] \\bigg|_{x_{\\text{age}}=a_0}\n\\;=\\;\n\\beta_{\\text{treat}} \\;+\\; \\beta_{\\text{treat}\\times \\text{age}}\\, a_0.\n$$\nThis quantity measures the instantaneous change in $E[y \\mid x]$ per unit change in $x_{\\text{treat}}$ at age $a_0$. Because $x_{\\text{treat}}$ is an indicator, this is equivalently the difference in conditional means between treated and untreated at age $a_0$ under the linear model.\n\nThe corresponding estimator of the marginal treatment effect is\n$$\n\\hat{L}(a_0) \\;=\\; \\hat{\\beta}_{\\text{treat}} \\;+\\; \\hat{\\beta}_{\\text{treat}\\times \\text{age}}\\, a_0.\n$$\nTo obtain its standard error, we use the well-tested property for OLS that the variance of any linear combination of the estimated coefficients is determined by the variance–covariance matrix. Let the coefficient vector be ordered as\n$$\n\\hat{\\boldsymbol{\\beta}} \\;=\\; \\big(\\hat{\\beta}_0,\\hat{\\beta}_{\\text{treat}},\\hat{\\beta}_{\\text{age}},\\hat{\\beta}_{\\text{treat}\\times \\text{age}}\\big)^{\\top}.\n$$\nThen $\\hat{L}(a_0)$ is a linear combination with weight vector\n$$\n\\boldsymbol{g}(a_0) \\;=\\; \\big(0,\\,1,\\,0,\\,a_0\\big)^{\\top}.\n$$\nTherefore,\n$$\n\\operatorname{Var}\\big(\\hat{L}(a_0)\\big) \\;=\\; \\boldsymbol{g}(a_0)^{\\top}\\, \\hat{\\Sigma}\\, \\boldsymbol{g}(a_0),\n\\quad\n\\text{and}\n\\quad\n\\operatorname{SE}\\big(\\hat{L}(a_0)\\big) \\;=\\; \\sqrt{\\,\\boldsymbol{g}(a_0)^{\\top}\\, \\hat{\\Sigma}\\, \\boldsymbol{g}(a_0)\\,}.\n$$\nSubstituting $a_0 = 65$ and the provided $\\hat{\\Sigma}$, only the entries involving $\\hat{\\beta}_{\\text{treat}}$ and $\\hat{\\beta}_{\\text{treat}\\times \\text{age}}$ contribute:\n$$\n\\boldsymbol{g}(65)^{\\top}\\, \\hat{\\Sigma}\\, \\boldsymbol{g}(65)\n\\;=\\;\n\\hat{\\Sigma}_{22}\n\\;+\\;\n2 \\cdot 65 \\cdot \\hat{\\Sigma}_{24}\n\\;+\\;\n65^2 \\cdot \\hat{\\Sigma}_{44}.\n$$\nWith $\\hat{\\Sigma}_{22} = 1.44$, $\\hat{\\Sigma}_{24} = 0.006$, and $\\hat{\\Sigma}_{44} = 0.0009$, we obtain\n$$\n\\operatorname{Var}\\big(\\hat{L}(65)\\big)\n\\;=\\;\n1.44 \\;+\\; 2 \\cdot 65 \\cdot 0.006 \\;+\\; 65^2 \\cdot 0.0009\n\\;=\\;\n1.44 \\;+\\; 0.78 \\;+\\; 3.8025\n\\;=\\;\n6.0225.\n$$\nHence the standard error is\n$$\n\\operatorname{SE}\\big(\\hat{L}(65)\\big)\n\\;=\\;\n\\sqrt{6.0225}\n\\;\\approx\\;\n2.454077\\ldots\n$$\nRounded to four significant figures (and expressed in $\\text{mmHg}$ as requested), the standard error is $2.454$.\n\nFor interpretation, the estimated marginal treatment effect at age $65$ is\n$$\n\\hat{L}(65) \\;=\\; \\hat{\\beta}_{\\text{treat}} \\;+\\; \\hat{\\beta}_{\\text{treat}\\times \\text{age}} \\cdot 65\n\\;=\\; -4.6 \\;+\\; (-0.045)\\cdot 65\n\\;=\\; -7.525,\n$$\nwhich means that, at age $65$, treatment is associated with a reduction of the expected SBP change by approximately $7.525$ $\\text{mmHg}$ relative to control, with a standard error of about $2.454$ $\\text{mmHg}$. The computation of the standard error follows from the variance of a linear combination of OLS coefficient estimates via the variance–covariance matrix.", "answer": "$$\\boxed{2.454}$$", "id": "4977020"}, {"introduction": "After fitting a regression model, a crucial next step is to assess its stability and determine if the conclusions are overly dependent on a few specific data points. This practice introduces regression diagnostics by asking you to quantify the influence of each observation on the estimated treatment effect using a leave-one-out approach. By deriving and applying the formula for the change in a coefficient upon an observation's deletion, you will gain a practical understanding of how leverage and residuals determine an observation's influence [@problem_id:4977029].", "problem": "A clinical trial evaluates a new antihypertensive drug. The outcome is systolic blood pressure reduction after twelve weeks, measured in millimeters of mercury (mmHg). Let $y_i$ denote the reduction for patient $i$. A multiple linear regression model is fit with an intercept, a treatment indicator, and a centered baseline covariate:\n$$\ny_i = \\beta_0 + \\beta_T T_i + \\beta_B b_i + \\varepsilon_i,\n$$\nwhere $T_i$ is coded as $-1$ for standard care and $+1$ for the new drug, and $b_i$ is the baseline systolic blood pressure deviation from the cohort mean (in mmHg). The dataset for $i = 1,\\dots,6$ is:\n- $i=1$: $T_1=-1$, $b_1=-2$, $y_1=4$\n- $i=2$: $T_2=-1$, $b_2=-1$, $y_2=2$\n- $i=3$: $T_3=-1$, $b_3=3$, $y_3=11$\n- $i=4$: $T_4=+1$, $b_4=-3$, $y_4=11$\n- $i=5$: $T_5=+1$, $b_5=+1$, $y_5=16$\n- $i=6$: $T_6=+1$, $b_6=+2$, $y_6=16$\n\nStarting from the ordinary least squares normal equations and appropriate matrix identities, derive the leave-one-out expression for the change in the estimated treatment coefficient $\\hat{\\beta}_T$ when observation $i$ is deleted, and compute this change for each $i$. Quantify the sensitivity of $\\hat{\\beta}_T$ to the deletion of each observation by reporting the largest absolute leave-one-out change in $\\hat{\\beta}_T$ across $i=1,\\dots,6$.\n\nRound your final numeric answer to four significant figures. Express the magnitude in mmHg per unit of the treatment indicator.", "solution": "First, we derive the general expression for the change in an ordinary least squares (OLS) coefficient estimate upon deletion of a single observation. The multiple linear regression model is given in matrix form as:\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n$$\nwhere $\\mathbf{y}$ is the $n \\times 1$ vector of outcomes, $\\mathbf{X}$ is the $n \\times p$ design matrix, $\\boldsymbol{\\beta}$ is the $p \\times 1$ vector of coefficients, and $\\boldsymbol{\\varepsilon}$ is the $n \\times 1$ vector of errors. The OLS estimator for $\\boldsymbol{\\beta}$ using all $n$ observations is given by the normal equations:\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\nLet $\\hat{\\boldsymbol{\\beta}}_{(i)}$ be the OLS estimator obtained after deleting the $i$-th observation. Let $\\mathbf{x}_i^T$ be the $i$-th row of $\\mathbf{X}$, and $y_i$ be the $i$-th element of $\\mathbf{y}$. The design matrix and response vector for the leave-one-out (LOO) regression are $\\mathbf{X}_{(i)}$ and $\\mathbf{y}_{(i)}$, respectively. We can write $\\mathbf{X}^T\\mathbf{X} = \\mathbf{X}_{(i)}^T\\mathbf{X}_{(i)} + \\mathbf{x}_i\\mathbf{x}_i^T$ and $\\mathbf{X}^T\\mathbf{y} = \\mathbf{X}_{(i)}^T\\mathbf{y}_{(i)} + \\mathbf{x}_i y_i$.\n\nThe LOO estimator is $\\hat{\\boldsymbol{\\beta}}_{(i)} = (\\mathbf{X}_{(i)}^T\\mathbf{X}_{(i)})^{-1}\\mathbf{X}_{(i)}^T\\mathbf{y}_{(i)}$. Substituting the above relations:\n$$\n\\hat{\\boldsymbol{\\beta}}_{(i)} = (\\mathbf{X}^T\\mathbf{X} - \\mathbf{x}_i\\mathbf{x}_i^T)^{-1}(\\mathbf{X}^T\\mathbf{y} - \\mathbf{x}_i y_i)\n$$\nUsing the Sherman-Morrison-Woodbury formula, $(A-\\mathbf{uv}^T)^{-1} = A^{-1} + \\frac{A^{-1}\\mathbf{uv}^T A^{-1}}{1-\\mathbf{v}^T A^{-1}\\mathbf{u}}$, with $A=\\mathbf{X}^T\\mathbf{X}$ and $\\mathbf{u}=\\mathbf{v}=\\mathbf{x}_i$, we get:\n$$\n(\\mathbf{X}^T\\mathbf{X} - \\mathbf{x}_i\\mathbf{x}_i^T)^{-1} = (\\mathbf{X}^T\\mathbf{X})^{-1} + \\frac{(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_i\\mathbf{x}_i^T(\\mathbf{X}^T\\mathbf{X})^{-1}}{1 - \\mathbf{x}_i^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_i}\n$$\nLet $h_{ii} = \\mathbf{x}_i^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_i$ be the $i$-th leverage value (the $i$-th diagonal element of the hat matrix $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$). The change in the coefficient vector, $\\Delta\\hat{\\boldsymbol{\\beta}}^{(i)} = \\hat{\\boldsymbol{\\beta}}_{(i)} - \\hat{\\boldsymbol{\\beta}}$, can be derived to be:\n$$\n\\hat{\\boldsymbol{\\beta}}_{(i)} - \\hat{\\boldsymbol{\\beta}} = - \\frac{(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_i(y_i - \\mathbf{x}_i^T\\hat{\\boldsymbol{\\beta}})}{1-h_{ii}} = - \\frac{(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_i e_i}{1-h_{ii}}\n$$\nwhere $e_i = y_i - \\hat{y}_i$ is the $i$-th OLS residual. The change in the specific coefficient $\\hat{\\beta}_T$, which we denote $\\Delta\\hat{\\beta}_T^{(i)}$, is the component of this vector corresponding to $\\beta_T$.\n\nNow, we apply this to the given data. The model is $y_i = \\beta_0 + \\beta_T T_i + \\beta_B b_i + \\varepsilon_i$. The coefficient vector is $\\boldsymbol{\\beta} = (\\beta_0, \\beta_T, \\beta_B)^T$. The sample size is $n=6$. The design matrix $\\mathbf{X}$ and response vector $\\mathbf{y}$ are:\n$$\n\\mathbf{X} = \\begin{pmatrix} 1  -1  -2 \\\\ 1  -1  -1 \\\\ 1  -1  3 \\\\ 1  1  -3 \\\\ 1  1  1 \\\\ 1  1  2 \\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} 4 \\\\ 2 \\\\ 11 \\\\ 11 \\\\ 16 \\\\ 16 \\end{pmatrix}\n$$\nWe compute $\\mathbf{X}^T\\mathbf{X}$. The column vectors are $\\mathbf{c}_0 = \\mathbf{1}$, $\\mathbf{c}_T = \\mathbf{T}$, and $\\mathbf{c}_B = \\mathbf{b}$. We check for orthogonality:\n$\\sum T_i = -1-1-1+1+1+1 = 0$.\n$\\sum b_i = -2-1+3-3+1+2 = 0$.\n$\\sum T_i b_i = (-1)(-2)+(-1)(-1)+(-1)(3)+(1)(-3)+(1)(1)+(1)(2) = 2+1-3-3+1+2 = 0$.\nThe columns are mutually orthogonal. This greatly simplifies the problem, as $\\mathbf{X}^T\\mathbf{X}$ is a diagonal matrix.\n$$\n\\mathbf{X}^T\\mathbf{X} = \\begin{pmatrix} \\sum 1  \\sum T_i  \\sum b_i \\\\ \\sum T_i  \\sum T_i^2  \\sum T_i b_i \\\\ \\sum b_i  \\sum T_i b_i  \\sum b_i^2 \\end{pmatrix} = \\begin{pmatrix} 6  0  0 \\\\ 0  6  0 \\\\ 0  0  28 \\end{pmatrix}\n$$\nThe inverse is $C = (\\mathbf{X}^T\\mathbf{X})^{-1} = \\begin{pmatrix} 1/6  0  0 \\\\ 0  1/6  0 \\\\ 0  0  1/28 \\end{pmatrix}$.\nNext, we compute $\\mathbf{X}^T\\mathbf{y}$:\n$\\sum y_i = 4+2+11+11+16+16=60$.\n$\\sum T_i y_i = -4-2-11+11+16+16=26$.\n$\\sum b_i y_i = -8-2+33-33+16+32=38$.\nSo, $\\mathbf{X}^T\\mathbf{y} = (60, 26, 38)^T$.\nThe OLS estimates are:\n$$\n\\hat{\\boldsymbol{\\beta}} = C \\mathbf{X}^T\\mathbf{y} = \\begin{pmatrix} 1/6  0  0 \\\\ 0  1/6  0 \\\\ 0  0  1/28 \\end{pmatrix} \\begin{pmatrix} 60 \\\\ 26 \\\\ 38 \\end{pmatrix} = \\begin{pmatrix} 10 \\\\ 26/6 \\\\ 38/28 \\end{pmatrix} = \\begin{pmatrix} 10 \\\\ 13/3 \\\\ 19/14 \\end{pmatrix}\n$$\nThus, $\\hat{\\beta}_0=10$, $\\hat{\\beta}_T=13/3$, and $\\hat{\\beta}_B=19/14$. The fitted model is $\\hat{y}_i = 10 + \\frac{13}{3}T_i + \\frac{19}{14}b_i$.\n\nWe need the leverage $h_{ii}$ and residual $e_i$ for each observation.\n$h_{ii} = \\mathbf{x}_i^T C \\mathbf{x}_i = (1, T_i, b_i) \\begin{pmatrix} 1/6  0  0 \\\\ 0  1/6  0 \\\\ 0  0  1/28 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ T_i \\\\ b_i \\end{pmatrix} = \\frac{1}{6} + \\frac{T_i^2}{6} + \\frac{b_i^2}{28}$. Since $T_i^2=1$ for all $i$, $h_{ii} = \\frac{1}{3} + \\frac{b_i^2}{28}$.\n\nThe change in $\\hat{\\beta}_T$ is the second component of the vector $\\hat{\\boldsymbol{\\beta}}_{(i)} - \\hat{\\boldsymbol{\\beta}}$. Let $C_{T,T}$ be the second diagonal element of $C$.\n$\\Delta\\hat{\\beta}_T^{(i)} = \\hat{\\beta}_{T,(i)} - \\hat{\\beta}_T = - \\frac{(C\\mathbf{x}_i)_T e_i}{1-h_{ii}} = - \\frac{C_{T,T} T_i e_i}{1-h_{ii}} = - \\frac{(1/6) T_i e_i}{1-h_{ii}}$.\n\nWe compute these quantities for each $i$:\n$i=1$: $T_1=-1, b_1=-2, y_1=4$.\n$h_{11} = 1/3 + (-2)^2/28 = 1/3+4/28 = 1/3+1/7 = 10/21$.\n$\\hat{y}_1 = 10 + \\frac{13}{3}(-1) + \\frac{19}{14}(-2) = 10 - 13/3 - 19/7 = (210-91-57)/21 = 62/21$.\n$e_1 = 4 - 62/21 = (84-62)/21 = 22/21$.\n$\\Delta\\hat{\\beta}_T^{(1)} = -\\frac{(1/6)(-1)(22/21)}{1-10/21} = \\frac{22/126}{11/21} = \\frac{22}{126}\\frac{21}{11} = \\frac{2}{6} = 1/3$.\n\n$i=2$: $T_2=-1, b_2=-1, y_2=2$.\n$h_{22} = 1/3 + (-1)^2/28 = 1/3+1/28 = (28+3)/84 = 31/84$.\n$\\hat{y}_2 = 10 + \\frac{13}{3}(-1) + \\frac{19}{14}(-1) = (420-182-57)/42 = 181/42$.\n$e_2 = 2 - 181/42 = (84-181)/42 = -97/42$.\n$\\Delta\\hat{\\beta}_T^{(2)} = -\\frac{(1/6)(-1)(-97/42)}{1-31/84} = -\\frac{97/252}{53/84} = -\\frac{97}{252}\\frac{84}{53} = -97/(3 \\times 53) = -97/159$.\n\n$i=3$: $T_3=-1, b_3=3, y_3=11$.\n$h_{33} = 1/3 + 3^2/28 = 1/3+9/28 = (28+27)/84 = 55/84$.\n$\\hat{y}_3 = 10 + \\frac{13}{3}(-1) + \\frac{19}{14}(3) = (420-182+171)/42 = 409/42$.\n$e_3 = 11 - 409/42 = (462-409)/42 = 53/42$.\n$\\Delta\\hat{\\beta}_T^{(3)} = -\\frac{(1/6)(-1)(53/42)}{1-55/84} = \\frac{53/252}{29/84} = \\frac{53}{252}\\frac{84}{29} = 53/(3 \\times 29) = 53/87$.\n\n$i=4$: $T_4=1, b_4=-3, y_4=11$.\n$h_{44} = 1/3 + (-3)^2/28 = 55/84$.\n$\\hat{y}_4 = 10 + \\frac{13}{3}(1) + \\frac{19}{14}(-3) = (420+182-171)/42 = 431/42$.\n$e_4 = 11 - 431/42 = (462-431)/42 = 31/42$.\n$\\Delta\\hat{\\beta}_T^{(4)} = -\\frac{(1/6)(1)(31/42)}{1-55/84} = -\\frac{31/252}{29/84} = -\\frac{31}{252}\\frac{84}{29} = -31/87$.\n\n$i=5$: $T_5=1, b_5=1, y_5=16$.\n$h_{55} = 1/3 + 1^2/28 = 31/84$.\n$\\hat{y}_5 = 10 + \\frac{13}{3}(1) + \\frac{19}{14}(1) = (420+182+57)/42 = 659/42$.\n$e_5 = 16 - 659/42 = (672-659)/42 = 13/42$.\n$\\Delta\\hat{\\beta}_T^{(5)} = -\\frac{(1/6)(1)(13/42)}{1-31/84} = -\\frac{13/252}{53/84} = -\\frac{13}{252}\\frac{84}{53} = -13/159$.\n\n$i=6$: $T_6=1, b_6=2, y_6=16$.\n$h_{66} = 1/3 + 2^2/28 = 10/21$.\n$\\hat{y}_6 = 10 + \\frac{13}{3}(1) + \\frac{19}{14}(2) = (210+91+57)/21 = 358/21$.\n$e_6 = 16 - 358/21 = (336-358)/21 = -22/21$.\n$\\Delta\\hat{\\beta}_T^{(6)} = -\\frac{(1/6)(1)(-22/21)}{1-10/21} = \\frac{22/126}{11/21} = \\frac{22}{126}\\frac{21}{11} = 1/3$.\n\nThe calculated changes are:\n$\\Delta\\hat{\\beta}_T^{(1)} = 1/3 \\approx 0.3333$\n$\\Delta\\hat{\\beta}_T^{(2)} = -97/159 \\approx -0.6101$\n$\\Delta\\hat{\\beta}_T^{(3)} = 53/87 \\approx 0.6092$\n$\\Delta\\hat{\\beta}_T^{(4)} = -31/87 \\approx -0.3563$\n$\\Delta\\hat{\\beta}_T^{(5)} = -13/159 \\approx -0.0818$\n$\\Delta\\hat{\\beta}_T^{(6)} = 1/3 \\approx 0.3333$\n\nWe now find the largest absolute change, $\\max_i |\\Delta\\hat{\\beta}_T^{(i)}|$.\n$|\\Delta\\hat{\\beta}_T^{(1)}|=1/3 \\approx 0.3333$\n$|\\Delta\\hat{\\beta}_T^{(2)}|=97/159 \\approx 0.61006$\n$|\\Delta\\hat{\\beta}_T^{(3)}|=53/87 \\approx 0.60919$\n$|\\Delta\\hat{\\beta}_T^{(4)}|=31/87 \\approx 0.35632$\n$|\\Delta\\hat{\\beta}_T^{(5)}|=13/159 \\approx 0.08176$\n$|\\Delta\\hat{\\beta}_T^{(6)}|=1/3 \\approx 0.3333$\n\nBy comparing the values, the maximum is clearly $|-97/159| = 97/159$.\nThe numerical value is $97 \\div 159 \\approx 0.61006289...$.\nRounding to four significant figures, we get $0.6101$.", "answer": "$$\n\\boxed{0.6101}\n$$", "id": "4977029"}]}