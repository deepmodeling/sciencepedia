{"hands_on_practices": [{"introduction": "This first practice grounds your understanding in the fundamental mechanics of information criteria. You will calculate the Akaike Information Criterion (AIC) and its small-sample correction (AICc) for two competing linear models, moving from a model's residual sum of squares to its information criterion value. This exercise [@problem_id:4966101] highlights the critical importance of the AICc correction when the sample size $n$ is not large relative to the number of parameters $k$, a common scenario in clinical studies.", "problem": "A clinical epidemiology team models the logarithm of a circulating inflammatory biomarker in $n=40$ adults using two candidate Gaussian linear models with independent, homoscedastic errors. Model $\\mathcal{M}_{1}$ includes $k_{1}=10$ freely estimated parameters in the likelihood (regression coefficients, intercept, and the unknown error variance), while model $\\mathcal{M}_{2}$ includes $k_{2}=7$ such parameters. The observed residual sum of squares under $\\mathcal{M}_{1}$ is $\\mathrm{RSS}_{1}=270$, and under $\\mathcal{M}_{2}$ is $\\mathrm{RSS}_{2}=320$.\n\nStarting from the fundamental base that model selection by information criteria seeks to minimize the expected out-of-sample discrepancy measured by Kullback–Leibler divergence, and that under a Gaussian linear model the maximized log-likelihood arises from the normal density with variance estimated by maximum likelihood, derive an expression for the information criterion that estimates the expected out-of-sample deviance and its small-sample bias correction. Using these derivations, compute the numerical values for the Akaike Information Criterion (AIC) and the small-sample corrected AIC (AICc) for both $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$, and determine which model each criterion prefers (lower value indicates preference). \n\nReport, as a single number, the quantity \n$$\\Delta_{\\mathrm{AICc}} \\equiv \\mathrm{AICc}(\\mathcal{M}_{1}) - \\mathrm{AICc}(\\mathcal{M}_{2}),$$\nrounded to four significant figures. No units are required.", "solution": "The user's request is to solve a statistical modeling problem. I will first validate the problem statement.\n\n### Step 1: Extract Givens\n- **Sample size**: $n = 40$ adults\n- **Model type**: Gaussian linear models with independent, homoscedastic errors.\n- **Model $\\mathcal{M}_{1}$**:\n    - Number of freely estimated parameters: $k_{1}=10$\n    - Residual sum of squares: $\\mathrm{RSS}_{1}=270$\n- **Model $\\mathcal{M}_{2}$**:\n    - Number of freely estimated parameters: $k_{2}=7$\n    - Residual sum of squares: $\\mathrm{RSS}_{2}=320$\n- **Objective**:\n    1. Derive an expression for the information criterion that estimates the expected out-of-sample deviance and its small-sample bias correction. This corresponds to deriving the forms of AIC and AICc for a Gaussian model.\n    2. Compute AIC and AICc for both models, $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$.\n    3. Determine which model is preferred by each criterion.\n    4. Compute the final quantity $\\Delta_{\\mathrm{AICc}} \\equiv \\mathrm{AICc}(\\mathcal{M}_{1}) - \\mathrm{AICc}(\\mathcal{M}_{2})$, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is rooted in fundamental statistical theory, specifically information-theoretic model selection (Kullback-Leibler divergence, AIC, AICc) as applied to linear regression. This is a standard and well-established methodology in biostatistics and epidemiology. The premise is scientifically sound.\n- **Well-Posed**: The problem is clearly defined and provides all necessary numerical values ($n$, $k_1$, $k_2$, $\\mathrm{RSS}_1$, $\\mathrm{RSS}_2$) and theoretical context to perform the required derivations and calculations. A unique solution exists.\n- **Objective**: The problem is stated in precise, quantitative, and unbiased language.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity. It is a valid, well-posed problem in applied statistics.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with a full derivation and solution.\n\n***\n\n### Derivation and Solution\n\nThe goal of information-theoretic model selection is to identify a model that provides the best approximation to the unknown, true data-generating process. The a priori-specified \"distance\" or \"discrepancy\" from the true process to the model is commonly measured by the Kullback-Leibler (KL) divergence. For a true probability density $f(y)$ and a model approximation $g(y|\\theta)$, the KL divergence is given by:\n$$D_{\\mathrm{KL}}(f || g) = \\int f(y) \\ln\\left(\\frac{f(y)}{g(y|\\theta)}\\right) dy$$\nMinimizing $D_{\\mathrm{KL}}(f || g)$ with respect to the model is equivalent to maximizing the term $\\int f(y) \\ln(g(y|\\theta)) dy$, as $\\int f(y) \\ln(f(y)) dy$ is a constant that depends only on the true (but unknown) process.\n\nWe use a dataset of $n$ observations to estimate the model parameters, yielding the maximum likelihood estimates (MLE), $\\hat{\\theta}$. The quality of the fitted model is judged by its expected performance on new, independent data from the same process. This is the expected out-of-sample KL divergence, or equivalently, the expected out-of-sample log-likelihood. A fundamental result by Akaike (1973) showed that the maximized log-likelihood of the current data, $\\ln(L(\\hat{\\theta}|y))$, is a biased estimator of this target quantity. Akaike demonstrated that a low-bias estimator of the expected, relative KL divergence is obtained by correcting for this bias. This leads to the Akaike Information Criterion (AIC).\n\nThe AIC is an estimator of $-2$ times the expected out-of-sample log-likelihood. It is defined as:\n$$\\mathrm{AIC} = -2 \\ln(L(\\hat{\\theta})) + 2k$$\nwhere $L(\\hat{\\theta})$ is the maximized likelihood and $k$ is the number of freely estimated parameters in the model. The term $2k$ is the asymptotic bias correction.\n\nFor the specified Gaussian linear model, the likelihood function for $n$ observations is:\n$$L(\\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_i^T\\beta)^2\\right)$$\nThe log-likelihood is:\n$$\\ln(L(\\beta, \\sigma^2)) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\mathrm{RSS}(\\beta)$$\nwhere $\\mathrm{RSS}(\\beta) = \\sum_{i=1}^n (y_i - x_i^T\\beta)^2$. The MLE for the regression coefficients, $\\hat{\\beta}$, minimizes the residual sum of squares, yielding the observed $\\mathrm{RSS}$. The MLE for the variance is $\\hat{\\sigma}^2 = \\frac{\\mathrm{RSS}}{n}$. Substituting these into the log-likelihood gives the maximized log-likelihood:\n$$\\ln(L(\\hat{\\theta})) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{\\mathrm{RSS}}{n}\\right) - \\frac{n}{2}$$\nThe AIC is then:\n$$\\mathrm{AIC} = -2\\left(-\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{\\mathrm{RSS}}{n}\\right) - \\frac{n}{2}\\right) + 2k$$\n$$\\mathrm{AIC} = n\\ln(2\\pi) + n\\ln\\left(\\frac{\\mathrm{RSS}}{n}\\right) + n + 2k$$\nFor model comparison, terms constant across all models (like $n\\ln(2\\pi)+n$) can be dropped. The conventional form of AIC for a Gaussian model is:\n$$\\mathrm{AIC} = n\\ln\\left(\\frac{\\mathrm{RSS}}{n}\\right) + 2k$$\n\nThe derivation of AIC relies on large-sample theory. When the sample size $n$ is not large relative to the number of parameters $k$, the penalty $2k$ is insufficient. Sugiura (1978) and Hurvich  Tsai (1989) derived a more accurate, second-order bias correction for linear models, leading to the small-sample AIC, or AICc. This criterion is derived by finding a more exact estimator of the expected KL divergence. The result is that the bias correction term $2k$ in AIC should be replaced by a more refined term. The AICc is given by:\n$$\\mathrm{AICc} = -2\\ln(L(\\hat{\\theta})) + 2k \\frac{n}{n-k-1}$$\nThis can be expressed in relation to AIC:\n$$\\mathrm{AICc} = \\left( -2\\ln(L(\\hat{\\theta})) + 2k \\right) - 2k + 2k \\frac{n}{n-k-1} = \\mathrm{AIC} + 2k\\left(\\frac{n}{n-k-1} - 1\\right)$$\n$$\\mathrm{AICc} = \\mathrm{AIC} + 2k\\left(\\frac{n - (n-k-1)}{n-k-1}\\right) = \\mathrm{AIC} + \\frac{2k(k+1)}{n-k-1}$$\nUsing the conventional form for AIC, we have:\n$$\\mathrm{AICc} = n\\ln\\left(\\frac{\\mathrm{RSS}}{n}\\right) + 2k + \\frac{2k(k+1)}{n-k-1}$$\nThis correction is recommended when $n/k  40$.\n\nWe now apply these formulas to the given models. We are given $n=40$.\n\n**For Model $\\mathcal{M}_{1}$**:\n$k_{1}=10$, $\\mathrm{RSS}_{1}=270$.\nFirst, we compute the AIC for $\\mathcal{M}_{1}$:\n$$\\mathrm{AIC}(\\mathcal{M}_{1}) = n\\ln\\left(\\frac{\\mathrm{RSS}_{1}}{n}\\right) + 2k_1 = 40\\ln\\left(\\frac{270}{40}\\right) + 2(10)$$\n$$\\mathrm{AIC}(\\mathcal{M}_{1}) = 40\\ln(6.75) + 20 \\approx 40(1.90954) + 20 = 76.3817 + 20 = 96.3817$$\nSince $n/k_1 = 40/10 = 4  40$, the small-sample correction is necessary.\n$$\\mathrm{AICc}(\\mathcal{M}_{1}) = \\mathrm{AIC}(\\mathcal{M}_{1}) + \\frac{2k_1(k_1+1)}{n-k_1-1} = 96.3817 + \\frac{2(10)(10+1)}{40-10-1}$$\n$$\\mathrm{AICc}(\\mathcal{M}_{1}) = 96.3817 + \\frac{2(10)(11)}{29} = 96.3817 + \\frac{220}{29} \\approx 96.3817 + 7.5862 = 103.9679$$\n\n**For Model $\\mathcal{M}_{2}$**:\n$k_{2}=7$, $\\mathrm{RSS}_{2}=320$.\nFirst, we compute the AIC for $\\mathcal{M}_{2}$:\n$$\\mathrm{AIC}(\\mathcal{M}_{2}) = n\\ln\\left(\\frac{\\mathrm{RSS}_{2}}{n}\\right) + 2k_2 = 40\\ln\\left(\\frac{320}{40}\\right) + 2(7)$$\n$$\\mathrm{AIC}(\\mathcal{M}_{2}) = 40\\ln(8) + 14 \\approx 40(2.07944) + 14 = 83.1777 + 14 = 97.1777$$\nSince $n/k_2 = 40/7 \\approx 5.7  40$, the small-sample correction is also necessary.\n$$\\mathrm{AICc}(\\mathcal{M}_{2}) = \\mathrm{AIC}(\\mathcal{M}_{2}) + \\frac{2k_2(k_2+1)}{n-k_2-1} = 97.1777 + \\frac{2(7)(7+1)}{40-7-1}$$\n$$\\mathrm{AICc}(\\mathcal{M}_{2}) = 97.1777 + \\frac{2(7)(8)}{32} = 97.1777 + \\frac{112}{32} = 97.1777 + 3.5 = 100.6777$$\n\n### Model Preference\nThe preferred model is the one with the lower information criterion value.\n- **Using AIC**: $\\mathrm{AIC}(\\mathcal{M}_{1}) \\approx 96.38$ and $\\mathrm{AIC}(\\mathcal{M}_{2}) \\approx 97.18$. Since $\\mathrm{AIC}(\\mathcal{M}_{1})  \\mathrm{AIC}(\\mathcal{M}_{2})$, the AIC criterion prefers the more complex model, $\\mathcal{M}_{1}$.\n- **Using AICc**: $\\mathrm{AICc}(\\mathcal{M}_{1}) \\approx 103.97$ and $\\mathrm{AICc}(\\mathcal{M}_{2}) \\approx 100.68$. Since $\\mathrm{AICc}(\\mathcal{M}_{2})  \\mathrm{AICc}(\\mathcal{M}_{1})$, the AICc criterion prefers the simpler model, $\\mathcal{M}_{2}$. The larger penalty for complexity in the small-sample context has reversed the model preference.\n\n### Final Calculation\nThe problem asks for the value of $\\Delta_{\\mathrm{AICc}} \\equiv \\mathrm{AICc}(\\mathcal{M}_{1}) - \\mathrm{AICc}(\\mathcal{M}_{2})$.\n$$\\Delta_{\\mathrm{AICc}} = 103.9679... - 100.6777... = 3.2902...$$\nRounding to four significant figures, we get $3.290$.", "answer": "$$\\boxed{3.290}$$", "id": "4966101"}, {"introduction": "Moving from calculation to practical modeling challenges, this exercise explores how information criteria behave in the presence of highly correlated predictors. Collinearity can complicate model selection, and this practice [@problem_id:4966069] provides a concrete example of how AIC and BIC may lead to different conclusions due to their distinct penalty structures. You will see firsthand how BIC's stronger penalty for complexity often promotes a more parsimonious model compared to AIC, especially with a large sample size.", "problem": "A hospital-based cohort study models a continuous biomarker outcome with Gaussian residuals under a linear regression framework. Investigators consider the following candidate models fitted by maximum likelihood to the same dataset of size $n=600$:\n- $M_0$: intercept, age, sex, and a comorbidity index ($p_0=4$ parameters including the intercept).\n- $M_1$: $M_0$ plus body mass index (BMI) ($p_1=5$).\n- $M_2$: $M_0$ plus waist circumference (WC) ($p_2=5$).\n- $M_3$: $M_0$ plus BMI and WC ($p_3=6$).\n\nIn the study population, BMI and WC are highly correlated with Pearson correlation $\\rho=0.96$, reflecting collinearity. Let the residual sum of squares for each fitted model be:\n- $RSS_0=120000$,\n- $RSS_1=119400$,\n- $RSS_2=119380$,\n- $RSS_3=119360$.\n\nAssume standard regularity conditions for maximum likelihood estimation and that all models are identifiable and correctly specified with homoscedastic normal errors. Using only the foundational idea that information criteria trade off data fit (via the maximized likelihood) against model complexity (number of free parameters), answer the following. Which statements are correct about the impact of collinearity on model choice under Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC), and about which models would be selected in this scenario?\n\nA. Under AIC, $M_2$ is selected; under BIC, $M_0$ is selected.\n\nB. Under AIC, $M_3$ is preferred to $M_2$ because the maximized log-likelihood cannot decrease when adding parameters and the penalty term is the same for $M_2$ and $M_3$.\n\nC. The small incremental improvement from $M_2$ to $M_3$ is a direct manifestation of collinearity between BMI and WC; with large $n$, BIC’s stronger penalty per parameter makes it less likely than AIC to include such redundant predictors.\n\nD. If the sample size were reduced to $n=60$ while the $RSS$ values remained the same, BIC would penalize additional parameters less than AIC, making it more willing than AIC to include the redundant predictor.", "solution": "The problem statement is critically evaluated for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n-   **Study Design**: Hospital-based cohort study.\n-   **Statistical Framework**: Linear regression for a continuous biomarker with Gaussian residuals.\n-   **Estimation**: Maximum likelihood estimation (MLE).\n-   **Sample Size**: $n=600$.\n-   **Candidate Models**:\n    -   $M_0$: intercept, age, sex, comorbidity index. Number of regression parameters $p_0=4$.\n    -   $M_1$: $M_0$ + Body Mass Index (BMI). Number of regression parameters $p_1=5$.\n    -   $M_2$: $M_0$ + Waist Circumference (WC). Number of regression parameters $p_2=5$.\n    -   $M_3$: $M_0$ + BMI + WC. Number of regression parameters $p_3=6$.\n-   **Predictor Correlation**: Pearson correlation between BMI and WC, $\\rho=0.96$.\n-   **Residual Sum of Squares (RSS)**:\n    -   $RSS_0=120000$\n    -   $RSS_1=119400$\n    -   $RSS_2=119380$\n    -   $RSS_3=119360$\n-   **Assumptions**: Standard regularity conditions for MLE, identifiable models, correct specification, homoscedastic normal errors.\n-   **Task**: Evaluate statements about model choice using Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC), focusing on the impact of collinearity.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific Grounding**: The problem is well-grounded in standard statistical theory. Linear regression, MLE, AIC, BIC, and collinearity are core concepts in statistical modeling. The scenario is a realistic application in medical research.\n2.  **Well-Posed**: The problem provides all necessary data ($n$, $p_i$, $RSS_i$ for each model) to calculate AIC and BIC values and compare the models. The question is unambiguous.\n3.  **Objective**: The problem is stated using precise, objective language and quantitative data.\n4.  **Consistency**: The number of parameters increases logically with model complexity: $p_0=4$, $p_1=5$, $p_2=5$, $p_3=6$. The RSS values decrease as parameters are added ($RSS_0 > RSS_1, RSS_2 > RSS_3$), which is a mathematical necessity for nested models. The small decrease in RSS when adding a highly correlated predictor is consistent with the phenomenon of collinearity. The setup is internally consistent.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically sound, well-posed, objective, and internally consistent. A solution can be derived.\n\n### Derivation and Option Analysis\n\nThe foundational principle of information criteria is to balance goodness of fit with model complexity. The general forms are:\n-   Akaike Information Criterion (AIC): $AIC = -2 \\ln(\\mathcal{L}_{max}) + 2k$\n-   Bayesian Information Criterion (BIC): $BIC = -2 \\ln(\\mathcal{L}_{max}) + k \\ln(n)$\n\nwhere $\\mathcal{L}_{max}$ is the maximized likelihood, $k$ is the total number of estimated parameters, and $n$ is the sample size. The model with the lowest AIC or BIC is preferred.\n\nFor a linear regression model with $p$ regression coefficients (including the intercept) and assuming homoscedastic Gaussian errors, the variance $\\sigma^2$ is also estimated. Thus, the total number of estimated parameters is $k = p+1$. The maximized log-likelihood, using the MLE for the variance $\\hat{\\sigma}^2 = RSS/n$, is given by:\n$$ \\ln(\\mathcal{L}_{max}) = -\\frac{n}{2}\\left(\\ln(2\\pi) + 1 + \\ln\\left(\\frac{RSS}{n}\\right)\\right) $$\nSubstituting this into the AIC and BIC formulas gives:\n$$ AIC = n(\\ln(2\\pi) + 1) + n\\ln\\left(\\frac{RSS}{n}\\right) + 2(p+1) $$\n$$ BIC = n(\\ln(2\\pi) + 1) + n\\ln\\left(\\frac{RSS}{n}\\right) + (p+1)\\ln(n) $$\nFor comparing models fitted to the same data, the constant term $n(\\ln(2\\pi) + 1)$ can be ignored. We can therefore compare models using the following proportional quantities:\n$AIC' \\propto n\\ln(RSS) + 2p$\n$BIC' \\propto n\\ln(RSS) + p\\ln(n)$\nNote that using $p$ instead of $k=p+1$ in the penalty leads to the same model ranking, since the difference in penalties between any two models remains the same. Here, $p$ is the number of regression coefficients. We will use the given $p_i$ values.\n\nGiven $n=600$, the BIC penalty per parameter is $\\ln(600) \\approx 6.397$.\n\nLet's calculate the values for $n\\ln(RSS)$, the AIC penalty ($2p$), and the BIC penalty ($p\\ln(n)$) for each model. We will compare the sums (proportional to AIC and BIC).\n\n**Model $M_0$**: $p_0=4$, $RSS_0=120000$\n-   $n\\ln(RSS_0) = 600 \\times \\ln(120000) \\approx 7017.15$\n-   $AIC_0 \\propto 7017.15 + 2(4) = 7025.15$\n-   $BIC_0 \\propto 7017.15 + 4 \\times \\ln(600) \\approx 7017.15 + 25.59 = 7042.74$\n\n**Model $M_1$**: $p_1=5$, $RSS_1=119400$\n-   $n\\ln(RSS_1) = 600 \\times \\ln(119400) \\approx 7014.08$\n-   $AIC_1 \\propto 7014.08 + 2(5) = 7024.08$\n-   $BIC_1 \\propto 7014.08 + 5 \\times \\ln(600) \\approx 7014.08 + 31.98 = 7046.06$\n\n**Model $M_2$**: $p_2=5$, $RSS_2=119380$\n-   $n\\ln(RSS_2) = 600 \\times \\ln(119380) \\approx 7013.98$\n-   $AIC_2 \\propto 7013.98 + 2(5) = 7023.98$\n-   $BIC_2 \\propto 7013.98 + 5 \\times \\ln(600) \\approx 7013.98 + 31.98 = 7045.96$\n\n**Model $M_3$**: $p_3=6$, $RSS_3=119360$\n-   $n\\ln(RSS_3) = 600 \\times \\ln(119360) \\approx 7013.88$\n-   $AIC_3 \\propto 7013.88 + 2(6) = 7025.88$\n-   $BIC_3 \\propto 7013.88 + 6 \\times \\ln(600) \\approx 7013.88 + 38.38 = 7052.26$\n\n**Summary of Model Selection**:\n-   **AIC ranking (lower is better)**: $AIC_2 (7023.98)  AIC_1 (7024.08)  AIC_0 (7025.15)  AIC_3 (7025.88)$. AIC selects model $M_2$.\n-   **BIC ranking (lower is better)**: $BIC_0 (7042.74)  BIC_2 (7045.96)  BIC_1 (7046.06)  BIC_3 (7052.26)$. BIC selects model $M_0$.\n\nWith these results, we evaluate each option.\n\n**A. Under AIC, $M_2$ is selected; under BIC, $M_0$ is selected.**\nOur calculations show that the minimum AIC value belongs to model $M_2$ and the minimum BIC value belongs to model $M_0$. This statement is a direct and accurate report of the computational results.\n**Verdict: Correct.**\n\n**B. Under AIC, $M_3$ is preferred to $M_2$ because the maximized log-likelihood cannot decrease when adding parameters and the penalty term is the same for $M_2$ and $M_3$.**\nThis statement contains a critical flaw. The maximized log-likelihood for $M_3$ is indeed higher than for $M_2$ (since $RSS_3  RSS_2$). However, the claim that \"the penalty term is the same for $M_2$ and $M_3$\" is false.\n-   For $M_2$, the number of parameters is $p_2=5$, so the AIC penalty is $2p_2=10$.\n-   For $M_3$, the number of parameters is $p_3=6$, so the AIC penalty is $2p_3=12$.\nThe penalties are different. Furthermore, our calculation shows $AIC_3 \\approx 7025.88 > AIC_2 \\approx 7023.98$, so $M_2$ is preferred to $M_3$ under AIC, which contradicts the statement's primary claim.\n**Verdict: Incorrect.**\n\n**C. The small incremental improvement from $M_2$ to $M_3$ is a direct manifestation of collinearity between BMI and WC; with large $n$, BIC’s stronger penalty per parameter makes it less likely than AIC to include such redundant predictors.**\nThis statement consists of two parts.\n1.  \"The small incremental improvement from $M_2$ to $M_3$ is a direct manifestation of collinearity between BMI and WC.\" Model $M_3$ adds BMI to model $M_2$. The reduction in RSS is $RSS_2 - RSS_3 = 119380 - 119360 = 20$. This is a very small improvement in fit, especially compared to the improvement from adding WC to $M_0$ ($RSS_0 - RSS_2 = 120000 - 119380 = 620$). This small incremental gain is precisely what one expects when adding a predictor (BMI) that is highly correlated with one already in the model (WC). This part of the statement is a correct interpretation of the data.\n2.  \"with large $n$, BIC’s stronger penalty per parameter makes it less likely than AIC to include such redundant predictors.\" The penalty for each additional parameter is $2$ for AIC and $\\ln(n)$ for BIC. Given $n=600$, $\\ln(600) \\approx 6.397 > 2$. Thus, BIC's penalty is substantially stronger. This larger penalty means BIC requires a larger improvement in likelihood to justify adding a parameter, making it more 'parsimonious' or conservative. Consequently, it is less likely than AIC to select models with extra, and especially redundant, parameters. This is a fundamental and correct conceptual point about the behavior of AIC vs. BIC. This general principle is demonstrated in this problem: AIC adds a predictor (selects $M_2$ over $M_0$), whereas BIC does not (selects $M_0$ over $M_2$).\nBoth parts of the statement are conceptually and factually correct.\n**Verdict: Correct.**\n\n**D. If the sample size were reduced to $n=60$ while the $RSS$ values remained the same, BIC would penalize additional parameters less than AIC, making it more willing than AIC to include the redundant predictor.**\nThis statement proposes a hypothetical scenario. The key claim is that \"BIC would penalize additional parameters less than AIC\". Let's check this.\n-   The AIC penalty per parameter is a constant, $2$.\n-   The BIC penalty per parameter is $\\ln(n)$. For the hypothetical $n=60$, this would be $\\ln(60) \\approx 4.094$.\n-   Since $4.094 > 2$, the BIC penalty is still larger than the AIC penalty. The BIC penalty per parameter is less than the AIC penalty only when $\\ln(n)2$, which requires $n  e^2 \\approx 7.39$.\n-   The premise of the statement is false. Therefore, the conclusion that BIC would be \"more willing\" to add predictors is also false.\n**Verdict: Incorrect.**\n\nSince both statements A and C are correct, and the instructions allow for multiple correct options, they both must be included in the final answer.", "answer": "$$\\boxed{AC}$$", "id": "4966069"}, {"introduction": "A key advantage of information criteria over classical hypothesis tests is their ability to compare non-nested models. This final practice [@problem_id:4966121] puts that principle to work, asking you to evaluate two structurally different logistic regression models. You will move beyond simply selecting the 'best' model to quantifying the strength of evidence using AIC differences ($\\\\Delta \\\\mathrm{AIC}$) and Akaike weights, providing a more nuanced view of relative model performance.", "problem": "A clinical team is modeling in-hospital mortality for patients with suspected sepsis using two non-nested logistic regression models, both fitted by maximum likelihood to the same dataset of size $n=800$. Model $\\mathrm{M}_1$ uses $12$ parameters (including intercept, demographics, and $8$ biomarkers) and achieves a maximized log-likelihood of $\\ell_1=-520.3$. Model $\\mathrm{M}_2$ uses $10$ parameters (including intercept, demographics, and $6$ biomarkers) and achieves $\\ell_2=-521.8$. Both models use the same likelihood for binary outcomes (logistic regression with a Bernoulli sampling model).\n\nSelect all statements that correctly explain how the Akaike Information Criterion (AIC) provides a principled comparison between $\\mathrm{M}_1$ and $\\mathrm{M}_2$, and assess when differences $\\Delta \\mathrm{AIC}$ are meaningful in this setting.\n\nA. Under Akaike’s framework, the model with smaller deviance $-2\\ell$ is preferred; since $-2\\ell_1 -2\\ell_2$, $\\mathrm{M}_1$ should be selected.\n\nB. The difference in AIC between the two models is approximately $1.0$ in favor of $\\mathrm{M}_2$, indicating only weak evidence; this comparison is valid because both models were fit by maximum likelihood to the same likelihood and the same data.\n\nC. The Akaike Information Criterion (AIC) is principled because it targets minimal expected out-of-sample Kullback–Leibler divergence; conventional interpretation treats $\\Delta \\mathrm{AIC}$ in ranges $0$–$2$ as negligible, $4$–$7$ as considerable, and $10$ as strong. In this case, the observed difference indicates the models are essentially equivalent in expected predictive performance.\n\nD. With $n=800$, the Bayesian Information Criterion (BIC) necessarily agrees with AIC on which model to prefer; in practice, differences in penalty do not change the preference.\n\nE. Differences in AIC are meaningful only for nested models; for non-nested models, AIC cannot be used to compare.\n\nF. The implied Akaike weight for the model with lower AIC is approximately $0.62$, quantifying modest support for that model relative to the other.", "solution": "The problem statement is evaluated to be valid as it is scientifically grounded in standard statistical modeling principles, well-posed, objective, and provides a complete set of data for the required calculations.\n\nThis problem requires the comparison of two logistic regression models, $\\mathrm{M}_1$ and $\\mathrm{M}_2$, using the Akaike Information Criterion (AIC). The models are fitted to the same dataset of size $n=800$.\n\nThe given data are:\n-   Model $\\mathrm{M}_1$: Number of parameters $k_1 = 12$, maximized log-likelihood $\\ell_1 = -520.3$.\n-   Model $\\mathrm{M}_2$: Number of parameters $k_2 = 10$, maximized log-likelihood $\\ell_2 = -521.8$.\n\nThe Akaike Information Criterion is defined as:\n$$ \\mathrm{AIC} = -2\\ell + 2k $$\nwhere $\\ell$ is the maximized log-likelihood and $k$ is the number of estimated parameters in the model. The model with the lower AIC is considered to provide a better balance between goodness of fit and parsimony, and is thus preferred.\n\nFirst, we calculate the AIC for each model.\nFor model $\\mathrm{M}_1$:\n$$ \\mathrm{AIC}_1 = -2\\ell_1 + 2k_1 = -2(-520.3) + 2(12) = 1040.6 + 24 = 1064.6 $$\nFor model $\\mathrm{M}_2$:\n$$ \\mathrm{AIC}_2 = -2\\ell_2 + 2k_2 = -2(-521.8) + 2(10) = 1043.6 + 20 = 1063.6 $$\n\nComparing the two values, $\\mathrm{AIC}_2  \\mathrm{AIC}_1$. Therefore, model $\\mathrm{M}_2$ is preferred according to the AIC. The difference in AIC values is:\n$$ \\Delta \\mathrm{AIC} = \\mathrm{AIC}_1 - \\mathrm{AIC}_2 = 1064.6 - 1063.6 = 1.0 $$\n\nNow we evaluate each option.\n\nA. Under Akaike’s framework, the model with smaller deviance $-2\\ell$ is preferred; since $-2\\ell_1 -2\\ell_2$, $\\mathrm{M}_1$ should be selected.\nThe deviance for model $\\mathrm{M}_1$ is $-2\\ell_1 = -2(-520.3) = 1040.6$. The deviance for model $\\mathrm{M}_2$ is $-2\\ell_2 = -2(-521.8) = 1043.6$. Indeed, $-2\\ell_1  -2\\ell_2$. However, the premise that Akaike's framework prefers the model with smaller deviance is incorrect. AIC is defined as $\\mathrm{AIC} = -2\\ell + 2k$. It is a trade-off between model fit (represented by the deviance $-2\\ell$) and model complexity (represented by the penalty term $2k$). Simply choosing the model with the minimum deviance would always favor the most complex model, which is prone to overfitting. AIC's purpose is precisely to penalize this complexity. In this case, AIC prefers $\\mathrm{M}_2$ despite its higher deviance because of its greater parsimony.\n**Verdict: Incorrect.**\n\nB. The difference in AIC between the two models is approximately $1.0$ in favor of $\\mathrm{M}_2$, indicating only weak evidence; this comparison is valid because both models were fit by maximum likelihood to the same likelihood and the same data.\nAs calculated above, the difference $\\mathrm{AIC}_1 - \\mathrm{AIC}_2 = 1.0$. Since $\\mathrm{AIC}_2$ is lower, the difference is in favor of $\\mathrm{M}_2$. A $\\Delta \\mathrm{AIC}$ value of $1.0$ is generally considered to represent weak or negligible evidence against the model with the higher AIC ($\\mathrm{M}_1$). The statement also correctly specifies the critical condition for using AIC to compare models: they must be fitted to the same data using the same response variable and likelihood function family. The problem statement confirms this condition is met.\n**Verdict: Correct.**\n\nC. The Akaike Information Criterion (AIC) is principled because it targets minimal expected out-of-sample Kullback–Leibler divergence; conventional interpretation treats $\\Delta \\mathrm{AIC}$ in ranges $0$–$2$ as negligible, $4$–$7$ as considerable, and $10$ as strong. In this case, the observed difference indicates the models are essentially equivalent in expected predictive performance.\nThis statement accurately describes the theoretical foundation of AIC. AIC is an estimate of the relative expected KL divergence, which measures the information lost when a model is used to approximate the true data-generating process. It is a measure of expected out-of-sample predictive accuracy. The provided rules of thumb for interpreting $\\Delta \\mathrm{AIC}$ (the difference between a model's AIC and the minimum AIC in the set) are the standard guidelines proposed by Burnham and Anderson. Our calculated difference is $\\Delta \\mathrm{AIC} = 1.0$, which falls in the $0$–$2$ range. The conclusion that this indicates the models are \"essentially equivalent in expected predictive performance\" is a correct application of these rules.\n**Verdict: Correct.**\n\nD. With $n=800$, the Bayesian Information Criterion (BIC) necessarily agrees with AIC on which model to prefer; in practice, differences in penalty do not change the preference.\nThe BIC is defined as $\\mathrm{BIC} = -2\\ell + k\\ln(n)$. The penalty for each parameter is $\\ln(n)$, whereas for AIC it is $2$. Given $n=800$, $\\ln(800) \\approx 6.68$. Since $\\ln(800) > 2$, BIC penalizes complexity more heavily than AIC. It is not necessary for them to agree. For example, if a more complex model has a sufficiently better fit (lower $-2\\ell$), AIC might prefer it while BIC prefers the simpler model. Let's check for this problem:\n$\\mathrm{BIC}_1 = -2(-520.3) + 12\\ln(800) \\approx 1040.6 + 12(6.685) = 1040.6 + 80.22 = 1120.82$.\n$\\mathrm{BIC}_2 = -2(-521.8) + 10\\ln(800) \\approx 1043.6 + 10(6.685) = 1043.6 + 66.85 = 1110.45$.\nHere, $\\mathrm{BIC}_2  \\mathrm{BIC}_1$, so BIC also prefers $\\mathrm{M}_2$. However, the statement that they *necessarily* agree is false. The difference in penalties is precisely what can lead to different model preferences, especially with large sample sizes where BIC's penalty is much larger.\n**Verdict: Incorrect.**\n\nE. Differences in AIC are meaningful only for nested models; for non-nested models, AIC cannot be used to compare.\nThis is false. A major advantage of information criteria like AIC and BIC over methods like the likelihood-ratio test is that they can be used to compare non-nested models. The primary requirement is that the models are fit to the same data and use the same likelihood function, not that one model is a special case of the other. The problem explicitly states that the models are non-nested, which is a perfectly valid scenario for AIC comparison.\n**Verdict: Incorrect.**\n\nF. The implied Akaike weight for the model with lower AIC is approximately $0.62$, quantifying modest support for that model relative to the other.\nThe model with the lower AIC is $\\mathrm{M}_2$. To calculate its Akaike weight, $w_2$, we use the differences from the minimum AIC. Here, $\\mathrm{AIC}_{\\min} = \\mathrm{AIC}_2 = 1063.6$. So, $\\Delta_2 = \\mathrm{AIC}_2 - \\mathrm{AIC}_{\\min} = 0$, and $\\Delta_1 = \\mathrm{AIC}_1 - \\mathrm{AIC}_{\\min} = 1.0$.\nThe Akaike weight for model $\\mathrm{M}_2$ is:\n$$ w_2 = \\frac{\\exp(-\\frac{1}{2}\\Delta_2)}{\\exp(-\\frac{1}{2}\\Delta_1) + \\exp(-\\frac{1}{2}\\Delta_2)} = \\frac{\\exp(-\\frac{1}{2}(0))}{\\exp(-\\frac{1}{2}(1.0)) + \\exp(-\\frac{1}{2}(0))} = \\frac{1}{\\exp(-0.5) + 1} $$\nUsing the value $\\exp(-0.5) \\approx 0.6065$, we get:\n$$ w_2 \\approx \\frac{1}{0.6065 + 1} = \\frac{1}{1.6065} \\approx 0.6225 $$\nThis value is approximately $0.62$. The Akaike weight can be interpreted as the probability that model $\\mathrm{M}_2$ is the best model in the candidate set. A weight of $0.62$ (versus $w_1 = 1 - w_2 \\approx 0.38$) suggests that $\\mathrm{M}_2$ is more likely to be the better model, but the evidence is not overwhelming. The term \"modest support\" is an appropriate description.\n**Verdict: Correct.**", "answer": "$$\\boxed{BCF}$$", "id": "4966121"}]}