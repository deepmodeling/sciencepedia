{"hands_on_practices": [{"introduction": "To truly understand how the LASSO achieves its characteristic sparsity, it is essential to look beyond pre-packaged software and examine the optimization algorithm at its core. This first practice invites you to perform one cycle of coordinate descent by hand, the workhorse algorithm for fitting the LASSO. By deriving the soft-thresholding update rule and applying it to a small, tangible dataset, you will gain a concrete intuition for how the $\\ell_1$ penalty systematically shrinks coefficients and sets many of them exactly to zero [@problem_id:4990069].", "problem": "A clinical researcher is studying a continuous outcome representing standardized systolic blood pressure residuals in a cohort of $n=6$ patients. Three candidate predictors $p=3$ (standardized laboratory biomarkers) have been collected. Let the design matrix $X \\in \\mathbb{R}^{6 \\times 3}$ have columns standardized so that each column has mean $0$ and satisfies $(1/n)\\sum_{i=1}^{n} x_{ij}^{2} = 1$ for $j \\in \\{1,2,3\\}$, and let the response vector $y \\in \\mathbb{R}^{6}$ be centered to mean $0$. Consider the Least Absolute Shrinkage and Selection Operator (LASSO) regression, which estimates coefficients $\\beta \\in \\mathbb{R}^{3}$ by minimizing the penalized squared-error objective with squared-loss and an $\\ell_{1}$ penalty.\n\nYou are given\n$$\nX = \n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & -1 & 1 \\\\\n1 & 1 & -1 \\\\\n-1 & -1 & -1 \\\\\n-1 & 1 & 1 \\\\\n-1 & -1 & -1\n\\end{pmatrix}, \n\\qquad\ny = \n\\begin{pmatrix}\n4 \\\\ 2 \\\\ 4 \\\\ -2 \\\\ -4 \\\\ -4\n\\end{pmatrix},\n\\qquad\n\\lambda = \\frac{3}{2}.\n$$\nVerify that each column of $X$ has mean $0$ and $(1/n)\\sum_{i=1}^{n} x_{ij}^{2} = 1$, and that $y$ has mean $0$. Starting from the core definition of the LASSO objective for linear regression with centered response and standardized predictors, perform one full cyclic coordinate descent pass over $j=1,2,3$ beginning at the initial coefficient vector $\\beta^{(0)} = (0,0,0)$. At each coordinate, derive the coordinate-wise update rule from first principles and then compute the updated coefficient exactly by hand.\n\nExpress the final updated coefficient vector after this single pass as exact rational numbers, with no rounding, in the form of a $1 \\times 3$ row vector. No units are required.", "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information to proceed with a solution. First, we verify the preliminary conditions stated in the problem.\n\nThe number of patients is $n=6$. The number of predictors is $p=3$.\nThe design matrix $X$ and response vector $y$ are given as:\n$$\nX = \n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & -1 & 1 \\\\\n1 & 1 & -1 \\\\\n-1 & -1 & -1 \\\\\n-1 & 1 & 1 \\\\\n-1 & -1 & -1\n\\end{pmatrix}, \n\\qquad\ny = \n\\begin{pmatrix}\n4 \\\\ 2 \\\\ 4 \\\\ -2 \\\\ -4 \\\\ -4\n\\end{pmatrix}\n$$\n\nVerification of column means of $X$:\nFor $j=1$: $\\sum_{i=1}^{6} x_{i1} = 1 + 1 + 1 - 1 - 1 - 1 = 0$. Mean is $\\frac{0}{6} = 0$.\nFor $j=2$: $\\sum_{i=1}^{6} x_{i2} = 1 - 1 + 1 - 1 + 1 - 1 = 0$. Mean is $\\frac{0}{6} = 0$.\nFor $j=3$: $\\sum_{i=1}^{6} x_{i3} = 1 + 1 - 1 - 1 + 1 - 1 = 0$. Mean is $\\frac{0}{6} = 0$.\n\nVerification of column standardization of $X$:\nFor $j=1$: $\\frac{1}{n}\\sum_{i=1}^{6} x_{i1}^{2} = \\frac{1}{6}(1^2 + 1^2 + 1^2 + (-1)^2 + (-1)^2 + (-1)^2) = \\frac{6}{6} = 1$.\nFor $j=2$: $\\frac{1}{n}\\sum_{i=1}^{6} x_{i2}^{2} = \\frac{1}{6}(1^2 + (-1)^2 + 1^2 + (-1)^2 + 1^2 + (-1)^2) = \\frac{6}{6} = 1$.\nFor $j=3$: $\\frac{1}{n}\\sum_{i=1}^{6} x_{i3}^{2} = \\frac{1}{6}(1^2 + 1^2 + (-1)^2 + (-1)^2 + 1^2 + (-1)^2) = \\frac{6}{6} = 1$.\n\nVerification of mean of $y$:\n$\\sum_{i=1}^{6} y_i = 4 + 2 + 4 - 2 - 4 - 4 = 0$. Mean is $\\frac{0}{6} = 0$.\nAll initial conditions are verified.\n\nThe LASSO objective function for standardized predictors and a centered response is:\n$$\nL(\\beta) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left(y_i - \\sum_{k=1}^{p} x_{ik}\\beta_k\\right)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n$$\nCoordinate descent optimizes the objective function with respect to a single coefficient $\\beta_j$ at a time, holding all other coefficients $\\beta_k$ (for $k \\neq j$) fixed.\nLet's isolate the terms in $L(\\beta)$ that depend on $\\beta_j$:\n$$\nL(\\beta_j) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left(y_i - \\sum_{k \\neq j} x_{ik}\\beta_k - x_{ij}\\beta_j\\right)^2 + \\lambda |\\beta_j| + \\text{const}\n$$\nDefine the partial residual $r_{i,(-j)} = y_i - \\sum_{k \\neq j} x_{ik}\\beta_k$. The expression to minimize with respect to $\\beta_j$ becomes:\n$$\nf(\\beta_j) = \\frac{1}{2n} \\sum_{i=1}^{n} (r_{i,(-j)} - x_{ij}\\beta_j)^2 + \\lambda |\\beta_j|\n$$\nExpanding the squared term:\n$$\nf(\\beta_j) = \\frac{1}{2n} \\left( \\sum_{i=1}^{n} r_{i,(-j)}^2 - 2\\beta_j \\sum_{i=1}^{n} r_{i,(-j)}x_{ij} + \\beta_j^2 \\sum_{i=1}^{n} x_{ij}^2 \\right) + \\lambda |\\beta_j|\n$$\nUsing the standardization property $\\sum_{i=1}^{n} x_{ij}^2 = n$:\n$$\nf(\\beta_j) = \\frac{1}{2n} \\left( C - 2\\beta_j \\sum_{i=1}^{n} r_{i,(-j)}x_{ij} + n\\beta_j^2 \\right) + \\lambda |\\beta_j|\n$$\nwhere $C$ is a constant with respect to $\\beta_j$. Ignoring constant terms, we want to minimize:\n$$\ng(\\beta_j) = \\frac{1}{2}\\beta_j^2 - \\left(\\frac{1}{n} \\sum_{i=1}^{n} r_{i,(-j)}x_{ij}\\right) \\beta_j + \\lambda |\\beta_j|\n$$\nLet $S_j = \\frac{1}{n} \\sum_{i=1}^{n} r_{i,(-j)}x_{ij} = \\frac{1}{n}X_j^T r_{(-j)}$, where $X_j$ is the $j$-th column of $X$. We minimize $g(\\beta_j) = \\frac{1}{2}\\beta_j^2 - S_j \\beta_j + \\lambda |\\beta_j|$.\nThis is a convex function. We find the minimum by setting its subgradient to zero. The subgradient $\\partial g(\\beta_j)$ is:\n$$\n\\partial g(\\beta_j) = \\beta_j - S_j + \\lambda \\cdot \\partial|\\beta_j|\n$$\nwhere $\\partial|\\beta_j|$ is $\\text{sgn}(\\beta_j)$ if $\\beta_j \\neq 0$ and the interval $[-1, 1]$ if $\\beta_j = 0$.\nSetting the subgradient to $0$:\n1.  If $\\beta_j > 0$: $\\beta_j - S_j + \\lambda = 0 \\implies \\hat{\\beta}_j = S_j - \\lambda$. This is valid only if $S_j - \\lambda > 0$, i.e., $S_j > \\lambda$.\n2.  If $\\beta_j < 0$: $\\beta_j - S_j - \\lambda = 0 \\implies \\hat{\\beta}_j = S_j + \\lambda$. This is valid only if $S_j + \\lambda < 0$, i.e., $S_j < -\\lambda$.\n3.  If $\\hat{\\beta}_j = 0$: $0 - S_j + \\lambda \\cdot [-1, 1] = 0 \\implies S_j \\in [-\\lambda, \\lambda]$, i.e., $|S_j| \\le \\lambda$.\n\nThese three cases define the soft-thresholding function:\n$$\n\\hat{\\beta}_j = \\text{ST}(S_j, \\lambda) = \\text{sgn}(S_j) \\max(0, |S_j| - \\lambda)\n$$\nWe are given $\\lambda = \\frac{3}{2}$ and we start with $\\beta^{(0)} = (0, 0, 0)^T$.\n\n**Coordinate 1: Update $\\beta_1$**\nWe update $\\beta_1$ with $\\beta_2=0$ and $\\beta_3=0$. The partial residual is $r_{(-1)} = y - X_2\\beta_2^{(0)} - X_3\\beta_3^{(0)} = y$.\nWe compute $S_1 = \\frac{1}{n} \\sum_{i=1}^n x_{i1}y_i = \\frac{1}{n}X_1^T y$.\n$$\nX_1^T y = 1(4) + 1(2) + 1(4) + (-1)(-2) + (-1)(-4) + (-1)(-4) = 4 + 2 + 4 + 2 + 4 + 4 = 20\n$$\n$$\nS_1 = \\frac{20}{6} = \\frac{10}{3}\n$$\nWe have $\\lambda = \\frac{3}{2}$. Since $S_1 = \\frac{10}{3} > \\frac{3}{2}$ (as $\\frac{20}{6} > \\frac{9}{6}$), we use the update rule $\\hat{\\beta}_1 = S_1 - \\lambda$.\n$$\n\\beta_1^{(1)} = \\frac{10}{3} - \\frac{3}{2} = \\frac{20 - 9}{6} = \\frac{11}{6}\n$$\nThe current coefficient vector is $\\beta = (\\frac{11}{6}, 0, 0)^T$.\n\n**Coordinate 2: Update $\\beta_2$**\nWe update $\\beta_2$ with $\\beta_1 = \\beta_1^{(1)} = \\frac{11}{6}$ and $\\beta_3 = \\beta_3^{(0)} = 0$.\nThe partial residual is $r_{(-2)} = y - X_1\\beta_1^{(1)} - X_3\\beta_3^{(0)} = y - X_1\\frac{11}{6}$.\nWe compute $S_2 = \\frac{1}{n}X_2^T r_{(-2)} = \\frac{1}{n}X_2^T(y - X_1\\frac{11}{6}) = \\frac{1}{n}X_2^T y - \\frac{1}{n}(X_2^T X_1)\\beta_1^{(1)}$.\nFirst, we compute the necessary inner products:\n$$\nX_2^T y = 1(4) + (-1)(2) + 1(4) + (-1)(-2) + 1(-4) + (-1)(-4) = 4 - 2 + 4 + 2 - 4 + 4 = 8\n$$\n$$\nX_2^T X_1 = 1(1) + (-1)(1) + 1(1) + (-1)(-1) + 1(-1) + (-1)(-1) = 1 - 1 + 1 + 1 - 1 + 1 = 2\n$$\nNow, we compute $S_2$:\n$$\nS_2 = \\frac{8}{6} - \\frac{2}{6} \\cdot \\frac{11}{6} = \\frac{4}{3} - \\frac{1}{3} \\cdot \\frac{11}{6} = \\frac{4}{3} - \\frac{11}{18} = \\frac{24 - 11}{18} = \\frac{13}{18}\n$$\nWe compare $|S_2| = \\frac{13}{18}$ with $\\lambda = \\frac{3}{2} = \\frac{27}{18}$. Since $|S_2| \\le \\lambda$, the update is $\\hat{\\beta}_2 = 0$.\n$$\n\\beta_2^{(1)} = 0\n$$\nThe current coefficient vector is $\\beta = (\\frac{11}{6}, 0, 0)^T$.\n\n**Coordinate 3: Update $\\beta_3$**\nWe update $\\beta_3$ with $\\beta_1 = \\beta_1^{(1)} = \\frac{11}{6}$ and $\\beta_2 = \\beta_2^{(1)} = 0$.\nThe partial residual is $r_{(-3)} = y - X_1\\beta_1^{(1)} - X_2\\beta_2^{(1)} = y - X_1\\frac{11}{6}$.\nWe compute $S_3 = \\frac{1}{n}X_3^T r_{(-3)} = \\frac{1}{n}X_3^T y - \\frac{1}{n}(X_3^T X_1)\\beta_1^{(1)} - \\frac{1}{n}(X_3^T X_2)\\beta_2^{(1)}$.\nSince $\\beta_2^{(1)}=0$, this simplifies to $S_3 = \\frac{1}{n}X_3^T y - \\frac{1}{n}(X_3^T X_1)\\beta_1^{(1)}$.\nFirst, we compute the necessary inner products:\n$$\nX_3^T y = 1(4) + 1(2) + (-1)(4) + (-1)(-2) + 1(-4) + (-1)(-4) = 4 + 2 - 4 + 2 - 4 + 4 = 4\n$$\n$$\nX_3^T X_1 = 1(1) + 1(1) + (-1)(1) + (-1)(-1) + 1(-1) + (-1)(-1) = 1 + 1 - 1 + 1 - 1 + 1 = 2\n$$\nNow, we compute $S_3$:\n$$\nS_3 = \\frac{4}{6} - \\frac{2}{6} \\cdot \\frac{11}{6} = \\frac{2}{3} - \\frac{1}{3} \\cdot \\frac{11}{6} = \\frac{2}{3} - \\frac{11}{18} = \\frac{12 - 11}{18} = \\frac{1}{18}\n$$\nWe compare $|S_3| = \\frac{1}{18}$ with $\\lambda = \\frac{3}{2} = \\frac{27}{18}$. Since $|S_3| \\le \\lambda$, the update is $\\hat{\\beta}_3 = 0$.\n$$\n\\beta_3^{(1)} = 0\n$$\nAfter one full cyclic pass, the updated coefficient vector is $\\beta^{(1)} = (\\beta_1^{(1)}, \\beta_2^{(1)}, \\beta_3^{(1)}) = (\\frac{11}{6}, 0, 0)$.\n\nAs a $1 \\times 3$ row vector, the final answer is $(\\frac{11}{6} \\quad 0 \\quad 0)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{11}{6} & 0 & 0\n\\end{pmatrix}\n}\n$$", "id": "4990069"}, {"introduction": "After seeing how a LASSO solution can be computed, a critical question arises: is this solution the only one? In scientific applications, particularly in medicine, the uniqueness of a model is paramount for reproducibility and confidence in our conclusions. This exercise moves from the mechanics of the algorithm to the mathematical properties of its solution, asking you to investigate the conditions that guarantee a unique LASSO estimate, even in high-dimensional settings where predictors outnumber samples ($p \\gt n$) [@problem_id:4989972]. You will engage with the 'general position' condition, a key concept from convex geometry that ensures the LASSO problem is well-behaved.", "problem": "In a study of predicting a continuous clinical outcome from biomarker-derived features, consider the Least Absolute Shrinkage and Selection Operator (LASSO) estimator, which for design matrix $X \\in \\mathbb{R}^{n \\times p}$, response vector $y \\in \\mathbb{R}^{n}$, and penalty parameter $\\lambda \\geq 0$, solves\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2} \\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}.\n$$\nYou are given the design matrix\n$$\nX = \\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 4 & 8 & 16 \\\\\n1 & 3 & 9 & 27 & 81\n\\end{pmatrix},\n$$\nconstructed from polynomial expansions of a single standardized biomarker evaluated at three distinct subjects with values $t \\in \\{1,2,3\\}$, and the clinical outcomes\n$$\ny = \\begin{pmatrix} 4 \\\\ 20 \\\\ 66 \\end{pmatrix},\n$$\nwith fixed penalty level $\\lambda = 3$.\n\nUsing only foundational principles of convex optimization and linear algebra, do the following:\n- Verify whether the general position condition holds for the matrix $X$ in the sense that every subset of at most $n$ columns is linearly independent.\n- Based on that verification and the structure of the LASSO objective, determine whether the LASSO solution is unique for the given $\\lambda$.\n\nDefine the indicator\n$$\nI = \\begin{cases}\n1, & \\text{if the general position condition holds and the LASSO solution is unique for the given }\\lambda, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nProvide $I$ as your final answer. No computation of the LASSO coefficients is required. Express your final numerical answer as a single real number without units. No rounding is needed, and no percentage representation is permitted.", "solution": "The problem asks for the value of an indicator $I$, which depends on two conditions regarding a specific LASSO problem: the validity of the general position condition for the design matrix $X$, and the uniqueness of the LASSO solution for the given penalty parameter $\\lambda$.\n\nFirst, we extract the given information.\nThe LASSO objective function to be minimized is:\n$$F(\\beta) = \\frac{1}{2} \\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$$\nThe design matrix $X \\in \\mathbb{R}^{n \\times p}$ is given by:\n$$\nX = \\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 4 & 8 & 16 \\\\\n1 & 3 & 9 & 27 & 81\n\\end{pmatrix}\n$$\nThe response vector $y \\in \\mathbb{R}^{n}$ is:\n$$\ny = \\begin{pmatrix} 4 \\\\ 20 \\\\ 66 \\end{pmatrix}\n$$\nThe penalty parameter is $\\lambda = 3$.\nFrom the dimensions of $X$, we have $n=3$ samples and $p=5$ features. The matrix $X$ is constructed from polynomial expansions of a single variable, with entries $X_{ij} = t_i^{j-1}$ for $i \\in \\{1, 2, 3\\}$ and $j \\in \\{1, 2, 3, 4, 5\\}$, where the evaluation points are $t_1=1$, $t_2=2$, and $t_3=3$.\n\nThe problem has two parts to address:\n1. Verify the general position condition for $X$.\n2. Determine the uniqueness of the LASSO solution.\n\n**Part 1: Verification of the General Position Condition**\n\nThe general position condition is stated as: \"every subset of at most $n$ columns is linearly independent\". Here, $n=3$. We need to check if every subset of $X$'s columns of size $1$, $2$, or $3$ is linearly independent.\n-   **Subsets of size 1:** A single column vector is linearly independent if it is not the zero vector. All columns of $X$ are non-zero.\n-   **Subsets of size 2:** Two columns are linearly independent if one is not a scalar multiple of the other. By inspection, no column in $X$ is a scalar multiple of another. For instance, the first column is $(1, 1, 1)^T$ and the second is $(1, 2, 3)^T$. They are not multiples. This pattern holds for all pairs.\n-   **Subsets of size 3:** We need to show that any three distinct columns of $X$ are linearly independent. Let us choose three arbitrary columns with indices $j_1, j_2, j_3$ such that $1 \\le j_1 < j_2 < j_3 \\le 5$. The corresponding $3 \\times 3$ submatrix is:\n    $$\n    M = \\begin{pmatrix}\n    t_1^{j_1-1} & t_1^{j_2-1} & t_1^{j_3-1} \\\\\n    t_2^{j_1-1} & t_2^{j_2-1} & t_2^{j_3-1} \\\\\n    t_3^{j_1-1} & t_3^{j_2-1} & t_3^{j_3-1}\n    \\end{pmatrix} = \\begin{pmatrix}\n    1^{j_1-1} & 1^{j_2-1} & 1^{j_3-1} \\\\\n    2^{j_1-1} & 2^{j_2-1} & 2^{j_3-1} \\\\\n    3^{j_1-1} & 3^{j_2-1} & 3^{j_3-1}\n    \\end{pmatrix}\n    $$\n    This is a generalized Vandermonde matrix. A fundamental theorem of linear algebra states that for distinct points $t_1, \\dots, t_n$ and distinct non-negative integer exponents $\\alpha_1, \\dots, \\alpha_n$, the matrix with entries $(t_i^{\\alpha_k})$ is non-singular. This is because a non-trivial generalized polynomial $P(t) = \\sum_{k=1}^n c_k t^{\\alpha_k}$ can have at most $n-1$ distinct roots. A linear dependency among the columns of $M$ would imply that such a polynomial with $n=3$ terms has $3$ roots ($t_1=1, t_2=2, t_3=3$), which is a contradiction.\n    Since our points $\\{1, 2, 3\\}$ are distinct and any choice of three columns gives distinct exponents $\\{j_1-1, j_2-1, j_3-1\\}$, any such submatrix $M$ is non-singular. Therefore, any subset of $3$ columns of $X$ is linearly independent.\n\nWe conclude that the general position condition holds for the matrix $X$.\n\n**Part 2: Uniqueness of the LASSO Solution**\n\nThe LASSO objective function $F(\\beta)$ is convex. A sufficient condition for a unique minimizer is strict convexity. The term $\\frac{1}{2}\\|y-X\\beta\\|_2^2$ is strictly convex if and only if $X$ has full column rank. Here, $p=5 > n=3$, so $X$ does not have full column rank, and this term is not strictly convex. The term $\\lambda\\|\\beta\\|_1$ is also not strictly convex. Thus, we cannot conclude uniqueness from the strict convexity of the objective function.\n\nWe must rely on more specific properties of the LASSO problem and the structure of $X$. We proceed by contradiction. Assume the LASSO solution is not unique. This implies the existence of at least two distinct solutions, $\\beta_1$ and $\\beta_2$.\nLet $C$ be the set of all solutions. If $|C| > 1$, because $F(\\beta)$ is convex, $C$ is a convex set. Thus, for any $\\beta_1, \\beta_2 \\in C$, the entire segment $\\beta(t) = (1-t)\\beta_1 + t\\beta_2$ for $t \\in [0,1]$ must also be in $C$.\nLet $\\delta = \\beta_2 - \\beta_1 \\neq 0$. For all points on the segment to be solutions, the quadratic term $\\|y - X\\beta(t)\\|_2^2$ must be linear in $t$, which implies that its quadratic part in $t$ must be zero: $\\|X\\delta\\|_2^2 = 0$. This means $X\\delta=0$, so $\\delta$ must be a non-zero vector in the null space of $X$, $\\ker(X)$.\n\nWe now invoke two established results from the theory of the LASSO estimator for problems where $p>n$ and the design matrix $X$ is in general position.\n\n1.  A foundational result states that if the LASSO solution is not unique, there must exist a non-zero vector $\\delta \\in \\ker(X)$ and a LASSO solution $\\beta^*$ whose support is disjoint from the support of $\\delta$. Let $S = \\text{supp}(\\delta) = \\{j | \\delta_j \\neq 0\\}$. This result implies the existence of a solution $\\beta^*$ such that $\\beta^*_j = 0$ for all $j \\in S$. The support of this solution, $A = \\text{supp}(\\beta^*)$, must therefore be a subset of the complement of $S$, i.e., $A \\subseteq S^c$.\n\n2.  From linear algebra, since $\\delta \\in \\ker(X)$ and $\\delta \\neq 0$, we have $\\sum_{j=1}^p \\delta_j X_j = \\sum_{j \\in S} \\delta_j X_j = 0$. This means the set of columns $\\{X_j\\}_{j \\in S}$ is linearly dependent. The minimal number of columns that can be linearly dependent is the spark of the matrix, denoted $\\text{spark}(X)$. Therefore, $|S| \\ge \\text{spark}(X)$.\n    For our matrix $X$ with $n=3$, we know from Part 1 that any $3$ columns are linearly independent. Since the columns are vectors in $\\mathbb{R}^3$, any set of $4$ columns must be linearly dependent. Thus, $\\text{spark}(X) = n+1 = 4$.\n    Combining these facts, if the solution is non-unique, there exists a vector $\\delta \\in \\ker(X)$ with support size $|S| \\ge 4$. The corresponding solution $\\beta^*$ must have support $A \\subseteq S^c$, which implies that its number of non-zero elements is $\\|\\beta^*\\|_0 = |A| \\le p - |S| \\le p - \\text{spark}(X) = 5 - 4 = 1$.\n    So, the assumption of non-uniqueness leads to the conclusion that there must exist a LASSO solution with at most one non-zero coefficient.\n\n3.  A second key theorem regarding LASSO uniqueness (e.g., Theorem 3 in Tibshirani, 2013, \"The lasso problem and uniqueness\") states that if $p>n$ and $X$ is in general position, then any non-unique solution must have at least $n$ non-zero components. In our case, this means any non-unique solution must have $\\|\\beta\\|_0 \\ge n = 3$.\n\nWe have reached a contradiction. If the solution were non-unique, there would have to exist a solution $\\beta^*$ with $\\|\\beta^*\\|_0 \\leq 1$. However, the second theorem requires that this very solution $\\beta^*$ must satisfy $\\|\\beta^*\\|_0 \\ge 3$. Since $1 < 3$, these two necessary conditions for non-uniqueness are mutually exclusive.\nThe contradiction implies that our initial assumption—that the solution is not unique—must be false. Therefore, the LASSO solution must be unique. This holds for any $\\lambda > 0$, including the given $\\lambda=3$.\n\n**Conclusion**\n\nThe general position condition holds for the matrix $X$. Based on this property and a rigorous analysis of the conditions for non-uniqueness of the LASSO solution, we have demonstrated that the solution must be unique.\nTherefore, both conditions for the indicator $I$ are met.\n$$\nI = 1\n$$", "answer": "$$\\boxed{1}$$", "id": "4989972"}, {"introduction": "The primary allure of the LASSO in fields like clinical biomarker discovery is its ability to perform automatic variable selection. However, this powerful feature is not infallible; its success depends on the underlying correlation structure of the predictors. This final practice delves into the theoretical guarantees of LASSO's selection consistency by introducing the celebrated 'Irrepresentable Condition' [@problem_id:4990086]. By calculating a single, decisive quantity, you will assess whether the LASSO can, in principle, distinguish the true predictors from the irrelevant ones for a given problem, bridging the gap between applying the method and understanding its fundamental statistical properties.", "problem": "A clinical research team is constructing a linear risk score for a continuous cardiovascular outcome using $p=4$ standardized biomarker predictors across $n=6$ patients. The predictors have been centered to zero mean and scaled to unit variance so that the empirical Gram matrix equals the sample covariance matrix $\\Sigma = \\frac{1}{n} X^{\\top} X$. The design matrix $X \\in \\mathbb{R}^{6 \\times 4}$ is given by\n$$\nX \\;=\\;\n\\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & 1 & -1 & -1 \\\\\n1 & -1 & 1 & -1 \\\\\n-1 & -1 & -1 & 1 \\\\\n-1 & 1 & 1 & 1 \\\\\n-1 & -1 & -1 & -1\n\\end{bmatrix}.\n$$\nAssume the data follow the fixed-design linear model $y = X \\beta^{0} + \\varepsilon$, where $y \\in \\mathbb{R}^{n}$ is the outcome, $\\beta^{0} \\in \\mathbb{R}^{p}$ is the true coefficient vector, and $\\varepsilon \\in \\mathbb{R}^{n}$ is mean-zero noise. The scientific prior indicates the true support (set of nonzero coefficients) is $S = \\{1, 3\\}$, with the signs $\\operatorname{sign}(\\beta^{0}_{1}) = +1$ and $\\operatorname{sign}(\\beta^{0}_{3}) = -1$. The team intends to use the Least Absolute Shrinkage and Selection Operator (LASSO) to perform variable selection by solving\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; \\frac{1}{2n} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\nfor a positive tuning parameter $\\lambda$. Using first principles of convex optimization and the Karush–Kuhn–Tucker (KKT) conditions, derive from the fixed-design model and the covariance structure $\\Sigma$ the inactive-set feasibility quantity that governs exact selection recovery in the asymptotic, fixed-design regime, and compute its value for the given $X$ and the stated $S$ and sign pattern. Based on this value, assess whether exact selection consistency is achievable under the usual regularity assumptions (e.g., signals bounded away from zero and vanishing noise correlations). Express the computed quantity as a single exact real number. No rounding is required.", "solution": "The problem requires the derivation and computation of the inactive-set feasibility quantity that governs exact support recovery for the Least Absolute Shrinkage and Selection Operator (LASSO). This quantity arises from the Karush–Kuhn–Tucker (KKT) conditions for the LASSO optimization problem.\n\nThe LASSO estimator $\\hat{\\beta}$ is the solution to the convex optimization problem:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; L(\\beta) = \\frac{1}{2n} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}\n$$\nThe optimality condition is that the zero vector must be in the subgradient of the objective function $L(\\beta)$ at the solution $\\hat{\\beta}$, i.e., $0 \\in \\partial L(\\hat{\\beta})$. The subgradient is given by:\n$$\n\\partial L(\\beta) = -\\frac{1}{n} X^{\\top}(y - X\\beta) + \\lambda \\partial \\|\\beta\\|_{1}\n$$\nwhere $\\partial \\|\\beta\\|_{1}$ is the subgradient of the $\\ell_1$-norm. Its $j$-th component, denoted $(\\partial \\|\\beta\\|_{1})_j$, is:\n$$\n(\\partial \\|\\beta\\|_{1})_j = \\begin{cases} \\operatorname{sign}(\\beta_j) & \\text{if } \\beta_j \\neq 0 \\\\ [-1, 1] & \\text{if } \\beta_j = 0 \\end{cases}\n$$\nThe KKT conditions at the solution $\\hat{\\beta}$ can be stated as:\n1. For any active index $j \\in \\hat{S} = \\{k : \\hat{\\beta}_k \\neq 0\\}$, the subgradient is uniquely defined, and we have the stationarity condition:\n   $$ \\frac{1}{n} X_j^{\\top}(y - X\\hat{\\beta}) = \\lambda \\operatorname{sign}(\\hat{\\beta}_j) $$\n2. For any inactive index $j \\in \\hat{S}^c = \\{k : \\hat{\\beta}_k = 0\\}$, the subgradient must contain zero, which implies the feasibility condition:\n   $$ \\left| \\frac{1}{n} X_j^{\\top}(y - X\\hat{\\beta}) \\right| \\leq \\lambda $$\n\nWe are interested in the conditions for exact support recovery, meaning the estimated support $\\hat{S}$ is identical to the true support $S = \\{1, 3\\}$. Let's assume $\\hat{S} = S$ and investigate the implications. In this case, $\\hat{\\beta}_j \\neq 0$ for $j \\in S$ and $\\hat{\\beta}_j = 0$ for $j \\in S^c = \\{2, 4\\}$.\nWe partition the problem according to $S$ and $S^c$: $X = [X_S, X_{S^c}]$, $\\beta = (\\beta_S, \\beta_{S^c})$.\nSince $\\hat{\\beta}_{S^c} = 0$, the KKT conditions become:\n1. For the active set $S$: $\\frac{1}{n} X_S^{\\top}(y - X_S \\hat{\\beta}_S) = \\lambda z_S$, where $z_S = \\operatorname{sign}(\\hat{\\beta}_S)$.\n2. For the inactive set $S^c$: $|\\frac{1}{n} X_{S^c}^{\\top}(y - X_S \\hat{\\beta}_S)| \\leq \\lambda$ (element-wise).\n\nFrom the first condition, we can solve for $\\hat{\\beta}_S$:\n$$\n\\frac{1}{n} X_S^{\\top}y - \\left(\\frac{1}{n} X_S^{\\top}X_S\\right) \\hat{\\beta}_S = \\lambda z_S\n$$\n$$\n\\Sigma_{SS} \\hat{\\beta}_S = \\frac{1}{n} X_S^{\\top}y - \\lambda z_S \\implies \\hat{\\beta}_S = \\Sigma_{SS}^{-1}\\left(\\frac{1}{n} X_S^{\\top}y - \\lambda z_S\\right)\n$$\nwhere $\\Sigma = \\frac{1}{n}X^{\\top}X$ is the sample covariance matrix.\nSubstituting the true model $y = X\\beta^0 + \\varepsilon = X_S \\beta^0_S + \\varepsilon$ (since $\\beta^0_{S^c}=0$):\n$$\n\\hat{\\beta}_S = \\Sigma_{SS}^{-1}\\left(\\frac{1}{n} X_S^{\\top}(X_S \\beta^0_S + \\varepsilon) - \\lambda z_S\\right) = \\beta^0_S + \\Sigma_{SS}^{-1}\\left(\\frac{1}{n} X_S^{\\top}\\varepsilon - \\lambda z_S\\right)\n$$\nFor exact recovery to hold for a range of $\\lambda$ and under typical noise assumptions, we require $\\operatorname{sign}(\\hat{\\beta}_S) = \\operatorname{sign}(\\beta^0_S)$. Let $z_S^0 = \\operatorname{sign}(\\beta^0_S)$. Assuming this consistency holds, we have $z_S = z_S^0$.\n\nNow we analyze the second (feasibility) condition. The argument of the absolute value is:\n$$\n\\frac{1}{n} X_{S^c}^{\\top}(y - X_S \\hat{\\beta}_S) = \\frac{1}{n} X_{S^c}^{\\top}(X_S \\beta^0_S + \\varepsilon - X_S \\hat{\\beta}_S)\n$$\nSubstituting $\\hat{\\beta}_S = \\beta^0_S + \\Sigma_{SS}^{-1}(\\frac{1}{n} X_S^{\\top}\\varepsilon - \\lambda z_S^0)$:\n$$\ny - X_S \\hat{\\beta}_S = X_S \\beta^0_S + \\varepsilon - X_S\\left(\\beta^0_S + \\Sigma_{SS}^{-1}\\left(\\frac{1}{n} X_S^{\\top}\\varepsilon - \\lambda z_S^0\\right)\\right) = \\varepsilon - X_S\\Sigma_{SS}^{-1}\\frac{1}{n} X_S^{\\top}\\varepsilon + \\lambda X_S\\Sigma_{SS}^{-1}z_S^0\n$$\nPremultiplying by $\\frac{1}{n}X_{S^c}^{\\top}$:\n$$\n\\frac{1}{n} X_{S^c}^{\\top}(y - X_S \\hat{\\beta}_S) = \\frac{1}{n}X_{S^c}^{\\top}\\left(I - X_S\\Sigma_{SS}^{-1}\\frac{1}{n} X_S^{\\top}\\right)\\varepsilon + \\lambda \\left(\\frac{1}{n}X_{S^c}^{\\top}X_S\\right)\\Sigma_{SS}^{-1}z_S^0\n$$\n$$\n\\frac{1}{n} X_{S^c}^{\\top}(y - X_S \\hat{\\beta}_S) = \\text{noise term} + \\lambda \\Sigma_{S^c S} \\Sigma_{SS}^{-1}z_S^0\n$$\nThe feasibility condition $|\\frac{1}{n} X_{S^c}^{\\top}(y - X_S \\hat{\\beta}_S)| \\leq \\lambda$ becomes:\n$$\n|\\Sigma_{S^c S} \\Sigma_{SS}^{-1}z_S^0 + \\frac{1}{\\lambda}(\\text{noise term})| \\leq 1\n$$\nIn the asymptotic fixed-design regime, the influence of the noise term is assumed to vanish. For the condition to hold robustly, the deterministic part must satisfy a strict inequality. This leads to the \"Irrepresentable Condition\":\n$$\n\\| \\Sigma_{S^c S} \\Sigma_{SS}^{-1}z_S^0 \\|_{\\infty} < 1\n$$\nThe quantity to be computed is this $\\ell_{\\infty}$-norm, which we denote by $\\eta$.\n$\\eta = \\| \\Sigma_{S^c S} \\Sigma_{SS}^{-1}z_S^0 \\|_{\\infty}$.\n\nWe are given $n=6$, $S=\\{1, 3\\}$, and $S^c=\\{2, 4\\}$. The sign vector for $S$ is $z_S^0 = \\begin{pmatrix} \\operatorname{sign}(\\beta_1^0) \\\\ \\operatorname{sign}(\\beta_3^0) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\nThe design matrix is $X = \\begin{bmatrix} X_1 & X_2 & X_3 & X_4 \\end{bmatrix}$. The columns of $X$ corresponding to $S$ and $S^c$ are:\n$$\nX_S = \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\\\ 1 & 1 \\\\ -1 & -1 \\\\ -1 & 1 \\\\ -1 & -1 \\end{bmatrix}, \\quad X_{S^c} = \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\\\ -1 & -1 \\\\ -1 & 1 \\\\ 1 & 1 \\\\ -1 & -1 \\end{bmatrix}\n$$\nFirst, we compute $\\Sigma_{SS} = \\frac{1}{n}X_S^{\\top}X_S$. Since the columns of $X$ are standardized, the diagonal entries of $\\Sigma$ are $1$.\n$$\nX_1^{\\top}X_3 = 1(1) + 1(-1) + 1(1) + (-1)(-1) + (-1)(1) + (-1)(-1) = 1-1+1+1-1+1 = 2\n$$\n$$\n\\Sigma_{SS} = \\frac{1}{6} \\begin{bmatrix} 6 & 2 \\\\ 2 & 6 \\end{bmatrix} = \\begin{bmatrix} 1 & 1/3 \\\\ 1/3 & 1 \\end{bmatrix}\n$$\nNext, we compute the inverse $\\Sigma_{SS}^{-1}$:\n$$\n\\det(\\Sigma_{SS}) = 1 \\cdot 1 - (\\frac{1}{3})^2 = 1 - \\frac{1}{9} = \\frac{8}{9}\n$$\n$$\n\\Sigma_{SS}^{-1} = \\frac{1}{8/9} \\begin{bmatrix} 1 & -1/3 \\\\ -1/3 & 1 \\end{bmatrix} = \\frac{9}{8} \\begin{bmatrix} 1 & -1/3 \\\\ -1/3 & 1 \\end{bmatrix} = \\begin{bmatrix} 9/8 & -3/8 \\\\ -3/8 & 9/8 \\end{bmatrix}\n$$\nNow, we compute $\\Sigma_{S^c S} = \\frac{1}{n}X_{S^c}^{\\top}X_S$:\n$$\nX_2^{\\top}X_1 = 1(1) + 1(1) + (-1)(1) + (-1)(-1) + 1(-1) + (-1)(-1) = 1+1-1+1-1+1 = 2\n$$\n$$\nX_2^{\\top}X_3 = 1(1) + 1(-1) + (-1)(1) + (-1)(-1) + 1(1) + (-1)(-1) = 1-1-1+1+1+1 = 2\n$$\n$$\nX_4^{\\top}X_1 = 1(1) + (-1)(1) + (-1)(1) + 1(-1) + 1(-1) + (-1)(-1) = 1-1-1-1-1+1 = -2\n$$\n$$\nX_4^{\\top}X_3 = 1(1) + (-1)(-1) + (-1)(1) + 1(-1) + 1(1) + (-1)(-1) = 1+1-1-1+1+1 = 2\n$$\n$$\n\\Sigma_{S^c S} = \\frac{1}{6} \\begin{bmatrix} 2 & 2 \\\\ -2 & 2 \\end{bmatrix} = \\begin{bmatrix} 1/3 & 1/3 \\\\ -1/3 & 1/3 \\end{bmatrix}\n$$\nNow we assemble the terms to find $\\eta$:\n$$\n\\Sigma_{SS}^{-1}z_S^0 = \\begin{bmatrix} 9/8 & -3/8 \\\\ -3/8 & 9/8 \\end{bmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 9/8 - (-3/8) \\\\ -3/8 - 9/8 \\end{pmatrix} = \\begin{pmatrix} 12/8 \\\\ -12/8 \\end{pmatrix} = \\begin{pmatrix} 3/2 \\\\ -3/2 \\end{pmatrix}\n$$\n$$\n\\Sigma_{S^c S} (\\Sigma_{SS}^{-1}z_S^0) = \\begin{bmatrix} 1/3 & 1/3 \\\\ -1/3 & 1/3 \\end{bmatrix} \\begin{pmatrix} 3/2 \\\\ -3/2 \\end{pmatrix} = \\begin{pmatrix} (1/3)(3/2) + (1/3)(-3/2) \\\\ (-1/3)(3/2) + (1/3)(-3/2) \\end{pmatrix} = \\begin{pmatrix} 1/2 - 1/2 \\\\ -1/2 - 1/2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}\n$$\nThe inactive-set feasibility quantity $\\eta$ is the $\\ell_{\\infty}$-norm of this vector:\n$$\n\\eta = \\left\\| \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} \\right\\|_{\\infty} = \\max(|0|, |-1|) = 1\n$$\nThe computed value is exactly $1$. Since this value does not satisfy the strict inequality $\\eta < 1$, the irrepresentable condition is violated. This implies that exact selection consistency is not guaranteed for this problem. The system is on the boundary of recoverability, and any perturbation from noise is likely to cause LASSO to fail to select the correct set of predictors. Specifically, the value for predictor $j=4$ is $|-1|=1$, indicating it is as correlated with the model residual (in the noise-free limit) as the active predictors, and is thus prone to be incorrectly included in the model.\nThe problem asks for the value of this quantity.\nThe value is $1$.", "answer": "$$\n\\boxed{1}\n$$", "id": "4990086"}]}