{"hands_on_practices": [{"introduction": "This first practice grounds our understanding in the fundamental principle of ridge regression. We will start directly from the objective function, which balances model fit with coefficient size. By minimizing this function for a simple, single-predictor model, you will derive the ridge estimator from first principles and see how the regularization parameter $\\lambda$ directly influences the solution [@problem_id:1951876].", "problem": "In a machine learning context, we are tasked with fitting a simple linear model without an intercept, $y = \\beta x$, to a set of $n$ data points $(x_i, y_i)$. To prevent overfitting on a small dataset, we employ ridge regression. The ridge estimate for the coefficient $\\beta$ is the value that minimizes the penalized sum of squared errors, also known as the objective function $L(\\beta)$:\n$$L(\\beta) = \\sum_{i=1}^{n} (y_i - \\beta x_i)^2 + \\lambda \\beta^2$$\nwhere $\\lambda > 0$ is the regularization parameter that controls the amount of shrinkage.\n\nYour task is two-fold. First, by minimizing the objective function $L(\\beta)$, derive the general closed-form expression for the ridge estimate $\\hat{\\beta}_{\\text{ridge}}$ in terms of the data points $(x_i, y_i)$ and the parameter $\\lambda$.\n\nSecond, apply this derived expression to a specific dataset consisting of two points: $(x_1, y_1) = (1, 3)$ and $(x_2, y_2) = (2, 5)$. Calculate the numerical value of the ridge estimate $\\hat{\\beta}_{\\text{ridge}}$ using a regularization parameter of $\\lambda = 1$.\n\nProvide the final numerical value as an exact fraction.", "solution": "We minimize the penalized sum of squared errors for the no-intercept linear model $y=\\beta x$ with objective\n$$\nL(\\beta)=\\sum_{i=1}^{n}(y_{i}-\\beta x_{i})^{2}+\\lambda \\beta^{2}, \\quad \\lambda>0.\n$$\nExpand the squared term and collect like powers of $\\beta$:\n$$\nL(\\beta)=\\sum_{i=1}^{n}\\left(y_{i}^{2}-2\\beta x_{i}y_{i}+\\beta^{2}x_{i}^{2}\\right)+\\lambda \\beta^{2}\n= \\sum_{i=1}^{n}y_{i}^{2}-2\\beta \\sum_{i=1}^{n}x_{i}y_{i}+\\beta^{2}\\sum_{i=1}^{n}x_{i}^{2}+\\lambda \\beta^{2}.\n$$\nDifferentiate with respect to $\\beta$ and set the derivative to zero (first-order optimality condition):\n$$\n\\frac{dL}{d\\beta}=-2\\sum_{i=1}^{n}x_{i}y_{i}+2\\beta \\sum_{i=1}^{n}x_{i}^{2}+2\\lambda \\beta=0.\n$$\nSolve for $\\beta$:\n$$\n2\\beta\\left(\\sum_{i=1}^{n}x_{i}^{2}+\\lambda\\right)=2\\sum_{i=1}^{n}x_{i}y_{i}\n\\quad \\Longrightarrow \\quad\n\\hat{\\beta}_{\\text{ridge}}=\\frac{\\sum_{i=1}^{n}x_{i}y_{i}}{\\sum_{i=1}^{n}x_{i}^{2}+\\lambda}.\n$$\nThe second derivative is\n$$\n\\frac{d^{2}L}{d\\beta^{2}}=2\\sum_{i=1}^{n}x_{i}^{2}+2\\lambda>0,\n$$\nso the solution is the unique minimizer.\n\nApply this to $(x_{1},y_{1})=(1,3)$, $(x_{2},y_{2})=(2,5)$ with $\\lambda=1$:\n$$\n\\sum_{i=1}^{2}x_{i}y_{i}=1\\cdot 3+2\\cdot 5=13,\\qquad\n\\sum_{i=1}^{2}x_{i}^{2}=1^{2}+2^{2}=5.\n$$\nTherefore,\n$$\n\\hat{\\beta}_{\\text{ridge}}=\\frac{13}{5+1}=\\frac{13}{6}.\n$$", "answer": "$$\\boxed{\\frac{13}{6}}$$", "id": "1951876"}, {"introduction": "Moving from a single predictor to a multi-predictor model requires the concise and powerful language of linear algebra. This exercise transitions our focus to the standard matrix form of the ridge regression solution. By working with pre-computed summary statistics like $X^T X$, you can focus on the core mechanics of the matrix calculation without getting bogged down in data preprocessing, providing a clear illustration of how the ridge solution is applied in a more general setting [@problem_id:1951893].", "problem": "In the field of machine learning, ridge regression is a common technique used to regularize linear regression models. This is particularly useful for preventing overfitting and handling multicollinearity among predictor variables. The ridge regression estimator for the coefficient vector, $\\hat{\\boldsymbol{\\beta}}_{\\lambda}$, is given by the formula:\n$$ \\hat{\\boldsymbol{\\beta}}_{\\lambda} = (\\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^T \\mathbf{y} $$\nHere, $\\mathbf{X}$ is the design matrix, $\\mathbf{y}$ is the vector of observed outcomes, $\\mathbf{I}$ is the identity matrix of appropriate dimensions, and $\\lambda$ is a non-negative regularization parameter.\n\nSuppose that for a particular dataset with two predictor variables, the following quantities have been pre-computed:\n$$ \\mathbf{X}^T \\mathbf{X} = \\begin{pmatrix} 10 & 5 \\\\ 5 & 10 \\end{pmatrix} \\quad \\text{and} \\quad \\mathbf{X}^T \\mathbf{y} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} $$\nUsing a regularization parameter of $\\lambda = 5$, determine the ridge regression coefficient vector $\\hat{\\boldsymbol{\\beta}}_5$.", "solution": "The ridge regression estimator is defined by\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\lambda} = (\\mathbf{X}^{T}\\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^{T} \\mathbf{y}.\n$$\nWith the given data,\n$$\n\\mathbf{X}^{T}\\mathbf{X} = \\begin{pmatrix} 10 & 5 \\\\ 5 & 10 \\end{pmatrix}, \\quad \\mathbf{X}^{T} \\mathbf{y} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}, \\quad \\lambda = 5.\n$$\nCompute the regularized matrix:\n$$\n\\mathbf{X}^{T}\\mathbf{X} + \\lambda \\mathbf{I} = \\begin{pmatrix} 10 & 5 \\\\ 5 & 10 \\end{pmatrix} + 5 \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 15 & 5 \\\\ 5 & 15 \\end{pmatrix}.\n$$\nFor a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, the inverse is given by\n$$\n\\left(\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\right)^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}.\n$$\nApplying this,\n$$\n\\det(\\mathbf{X}^{T}\\mathbf{X} + \\lambda \\mathbf{I}) = 15 \\cdot 15 - 5 \\cdot 5 = 225 - 25 = 200,\n$$\nso\n$$\n(\\mathbf{X}^{T}\\mathbf{X} + \\lambda \\mathbf{I})^{-1} = \\frac{1}{200} \\begin{pmatrix} 15 & -5 \\\\ -5 & 15 \\end{pmatrix}.\n$$\nThen\n$$\n\\hat{\\boldsymbol{\\beta}}_{5} = \\frac{1}{200} \\begin{pmatrix} 15 & -5 \\\\ -5 & 15 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 15 \\cdot 3 - 5 \\cdot 1 \\\\ -5 \\cdot 3 + 15 \\cdot 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 40 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{5} \\\\ 0 \\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{5}\\\\0\\end{pmatrix}}$$", "id": "1951893"}, {"introduction": "Ridge regression is primarily known for shrinking coefficients, but its effects can be more complex in the presence of multicollinearity. This advanced practice explores a subtle, counter-intuitive phenomenon: a coefficient for an irrelevant predictor can initially move *away* from zero as the penalty $\\lambda$ increases, before eventually being suppressed. By analyzing the entire regularization path, this problem provides a deeper insight into the dynamic interplay between correlated features and the ridge penalty [@problem_id:1951905].", "problem": "Consider a noiseless linear model based on a dataset of size $n$. The model includes two predictors, $\\mathbf{x}_1$ and $\\mathbf{x}_2$, which have been centered and scaled such that for each predictor $j \\in \\{1, 2\\}$, we have $\\sum_{i=1}^n x_{ij} = 0$ and $\\sum_{i=1}^n x_{ij}^2 = n$. The sample correlation between the two predictors is given as $\\rho = \\frac{4}{5}$. The true relationship for the response variable $\\mathbf{y}$ is given by the equation $\\mathbf{y} = \\beta_1 \\mathbf{x}_1$, where $\\beta_1$ is a positive constant. Note that the true coefficient for the second predictor, $\\beta_2$, is zero.\n\nAn analyst fits a ridge regression model of the form $\\mathbf{y} = \\hat{\\beta}_{1, \\lambda} \\mathbf{x}_1 + \\hat{\\beta}_{2, \\lambda} \\mathbf{x}_2$. The ridge regression coefficients $\\hat{\\boldsymbol{\\beta}}_{\\lambda} = (\\hat{\\beta}_{1, \\lambda}, \\hat{\\beta}_{2, \\lambda})^T$ are the solution to the minimization of the penalized sum of squares for a given penalty parameter $\\lambda > 0$:\n$$ L(\\beta_1, \\beta_2; \\lambda) = \\sum_{i=1}^n (y_i - \\beta_1 x_{i1} - \\beta_2 x_{i2})^2 + \\lambda (\\beta_1^2 + \\beta_2^2) $$\nDue to the correlation between the predictors, the estimate $\\hat{\\beta}_{2, \\lambda}$ will be non-zero for $\\lambda > 0$. As $\\lambda$ increases from 0, the value of $\\hat{\\beta}_{2, \\lambda}$ will first increase, reach a peak, and then decay back towards its true value of 0.\n\nDetermine the maximum value that the coefficient estimate $\\hat{\\beta}_{2, \\lambda}$ attains over all possible values of the penalty parameter $\\lambda > 0$. Express your answer as a function of $\\beta_1$.", "solution": "Let $\\mathbf{X}$ be the $n \\times 2$ design matrix with columns $\\mathbf{x}_{1}$ and $\\mathbf{x}_{2}$. The standardization implies\n$$\n\\mathbf{x}_{1}^{T}\\mathbf{x}_{1}=n,\\quad \\mathbf{x}_{2}^{T}\\mathbf{x}_{2}=n,\\quad \\mathbf{x}_{1}^{T}\\mathbf{x}_{2}=\\mathbf{x}_{2}^{T}\\mathbf{x}_{1}=n\\rho,\n$$\nand the true response satisfies $\\mathbf{y}=\\beta_{1}\\mathbf{x}_{1}$, hence\n$$\n\\mathbf{X}^{T}\\mathbf{y}=\\begin{pmatrix}\\mathbf{x}_{1}^{T}\\mathbf{y}\\\\ \\mathbf{x}_{2}^{T}\\mathbf{y}\\end{pmatrix}\n=\\begin{pmatrix}\\beta_{1}\\mathbf{x}_{1}^{T}\\mathbf{x}_{1}\\\\ \\beta_{1}\\mathbf{x}_{2}^{T}\\mathbf{x}_{1}\\end{pmatrix}\n=\\begin{pmatrix}n\\beta_{1}\\\\ n\\rho\\,\\beta_{1}\\end{pmatrix}.\n$$\nThe ridge estimator solves $(\\mathbf{X}^{T}\\mathbf{X}+\\lambda \\mathbf{I})\\hat{\\boldsymbol{\\beta}}_{\\lambda}=\\mathbf{X}^{T}\\mathbf{y}$. Using\n$$\n\\mathbf{X}^{T}\\mathbf{X}=\\begin{pmatrix}n & n\\rho\\\\ n\\rho & n\\end{pmatrix},\\quad\n\\mathbf{X}^{T}\\mathbf{X}+\\lambda \\mathbf{I}=\\begin{pmatrix}n+\\lambda & n\\rho\\\\ n\\rho & n+\\lambda\\end{pmatrix},\n$$\nits inverse is\n$$\n\\left(\\mathbf{X}^{T}\\mathbf{X}+\\lambda \\mathbf{I}\\right)^{-1}\n=\\frac{1}{(n+\\lambda)^{2}-n^{2}\\rho^{2}}\n\\begin{pmatrix}n+\\lambda & -n\\rho\\\\ -n\\rho & n+\\lambda\\end{pmatrix}.\n$$\nTherefore,\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\lambda}\n=\\left(\\mathbf{X}^{T}\\mathbf{X}+\\lambda \\mathbf{I}\\right)^{-1}\\mathbf{X}^{T}\\mathbf{y}\n=\\frac{1}{(n+\\lambda)^{2}-n^{2}\\rho^{2}}\n\\begin{pmatrix}n+\\lambda & -n\\rho\\\\ -n\\rho & n+\\lambda\\end{pmatrix}\n\\begin{pmatrix}n\\beta_{1}\\\\ n\\rho\\,\\beta_{1}\\end{pmatrix}.\n$$\nExtracting the second component yields\n$$\n\\hat{\\beta}_{2,\\lambda}\n=\\frac{-n\\rho\\cdot n\\beta_{1}+(n+\\lambda)\\cdot n\\rho\\,\\beta_{1}}\n{(n+\\lambda)^{2}-n^{2}\\rho^{2}}\n=\\frac{n\\rho\\,\\beta_{1}\\lambda}{(n+\\lambda)^{2}-n^{2}\\rho^{2}}.\n$$\nLet $t=\\lambda/n>0$. Then\n$$\n\\hat{\\beta}_{2,\\lambda}\n=\\beta_{1}\\,\\rho\\,\\frac{t}{(1+t)^{2}-\\rho^{2}}.\n$$\nDefine $h(t)=\\dfrac{t}{(1+t)^{2}-\\rho^{2}}$ for $t>0$. Differentiate using the quotient rule. With $d(t)=(1+t)^{2}-\\rho^{2}$ and $d'(t)=2(1+t)$,\n$$\nh'(t)=\\frac{d(t)-t\\,d'(t)}{d(t)^{2}}\n=\\frac{(1+t)^{2}-\\rho^{2}-t\\cdot 2(1+t)}{d(t)^{2}}\n=\\frac{1-\\rho^{2}-t^{2}}{d(t)^{2}}.\n$$\nSetting $h'(t)=0$ gives $t^{2}=1-\\rho^{2}$, and since $t>0$ we get the maximizer\n$$\nt^{*}=\\sqrt{1-\\rho^{2}}.\n$$\nEvaluate $h$ at $t^{*}$. Write $s=\\sqrt{1-\\rho^{2}}$, so\n$$\nh(t^{*})=\\frac{s}{(1+s)^{2}-\\rho^{2}}\n=\\frac{s}{1+2s+s^{2}-\\rho^{2}}\n=\\frac{s}{2s+2s^{2}}\n=\\frac{1}{2(1+s)}.\n$$\nThus the maximum of $\\hat{\\beta}_{2,\\lambda}$ over $\\lambda>0$ is\n$$\n\\max_{\\lambda>0}\\hat{\\beta}_{2,\\lambda}\n=\\beta_{1}\\,\\rho\\,\\frac{1}{2\\left(1+\\sqrt{1-\\rho^{2}}\\right)}.\n$$\nWith the given $\\rho=\\frac{4}{5}$, we have $\\sqrt{1-\\rho^{2}}=\\sqrt{\\frac{9}{25}}=\\frac{3}{5}$, hence\n$$\n\\max_{\\lambda>0}\\hat{\\beta}_{2,\\lambda}\n=\\beta_{1}\\cdot\\frac{4}{5}\\cdot\\frac{1}{2\\left(1+\\frac{3}{5}\\right)}\n=\\beta_{1}\\cdot\\frac{4}{5}\\cdot\\frac{1}{2\\cdot\\frac{8}{5}}\n=\\beta_{1}\\cdot\\frac{4}{16}\n=\\frac{\\beta_{1}}{4}.\n$$", "answer": "$$\\boxed{\\frac{\\beta_{1}}{4}}$$", "id": "1951905"}]}