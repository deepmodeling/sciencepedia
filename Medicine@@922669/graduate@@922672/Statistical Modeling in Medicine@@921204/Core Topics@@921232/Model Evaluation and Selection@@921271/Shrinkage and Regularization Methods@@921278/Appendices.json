{"hands_on_practices": [{"introduction": "This first practice is a foundational exercise designed to build a solid understanding of how ridge regression works from first principles. By deriving the closed-form solution for the ridge estimator in the simplified case of orthogonal predictors, you will see precisely how the penalty term, controlled by $\\lambda$, introduces a multiplicative shrinkage factor that pulls the coefficient estimates towards zero [@problem_id:4983813]. This analytical approach demystifies the \"black box\" and provides a clear intuition for the core mechanism of all shrinkage methods.", "problem": "A biostatistician is constructing a linear prognostic model for a continuous outcome $y$ (for example, log-transformed length of stay) using $p$ centered and standardized biomarker predictors collected on $n$ patients. The design matrix is $X \\in \\mathbb{R}^{n \\times p}$, the response vector is $y \\in \\mathbb{R}^{n}$, and there is no intercept because all variables are centered. After an orthogonalization step, the investigator arranges the predictors so that the empirical cross-product matrix satisfies $X^{\\top} X = \\mathrm{diag}(d_{1},\\dots,d_{p})$ with $d_{j} \\in \\mathbb{R}_{>0}$ for all $j \\in \\{1,\\dots,p\\}$. Define $z \\equiv X^{\\top} y \\in \\mathbb{R}^{p}$ with components $z_{j}$.\n\nConsider ridge regression with tuning parameter $\\lambda \\in \\mathbb{R}_{>0}$ defined by the penalized least squares criterion\n$$\nJ(\\beta) \\equiv \\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2}, \\quad \\beta \\in \\mathbb{R}^{p}.\n$$\nStarting only from this definition and the stated structure $X^{\\top} X = \\mathrm{diag}(d_{1},\\dots,d_{p})$, compute the closed-form ridge solution $\\hat{\\beta}_{\\mathrm{ridge}}$ and derive the component-wise representation that reveals how multiplicative shrinkage factors arise relative to the ordinary least squares (OLS) estimator. Express your final answer as a single closed-form analytic expression in terms of $D \\equiv \\mathrm{diag}(d_{1},\\dots,d_{p})$, $\\lambda$, and $z \\equiv X^{\\top} y$. No numerical evaluation is required.", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of statistical learning theory, well-posed with a unique and stable solution, and expressed in objective mathematical language. We may therefore proceed with the derivation.\n\nThe ridge regression estimator, $\\hat{\\beta}_{\\mathrm{ridge}}$, is defined as the vector $\\beta \\in \\mathbb{R}^{p}$ that minimizes the penalized least squares objective function $J(\\beta)$:\n$$\nJ(\\beta) = \\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2}\n$$\nwhere $y \\in \\mathbb{R}^{n}$ is the response vector, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^{p}$ is the coefficient vector, and $\\lambda \\in \\mathbb{R}_{>0}$ is the tuning parameter.\n\nTo find the minimum, we first expand the objective function using matrix algebra. The squared Euclidean norms can be written as dot products:\n$$\n\\|v\\|_{2}^{2} = v^{\\top}v\n$$\nApplying this, we get:\n$$\nJ(\\beta) = (y - X \\beta)^{\\top}(y - X \\beta) + \\lambda \\beta^{\\top}\\beta\n$$\nExpanding the first term:\n$$\nJ(\\beta) = (y^{\\top} - \\beta^{\\top}X^{\\top})(y - X \\beta) + \\lambda \\beta^{\\top}\\beta\n$$\n$$\nJ(\\beta) = y^{\\top}y - y^{\\top}X\\beta - \\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta + \\lambda \\beta^{\\top}\\beta\n$$\nSince $\\beta^{\\top}X^{\\top}y$ is a scalar ($1 \\times 1$ matrix), it is equal to its transpose, $(\\beta^{\\top}X^{\\top}y)^{\\top} = y^{\\top}X\\beta$. Thus, we can combine the two cross-product terms:\n$$\nJ(\\beta) = y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta + \\lambda \\beta^{\\top}\\beta\n$$\nThe function $J(\\beta)$ is convex in $\\beta$. For $\\lambda > 0$, the term $\\lambda \\beta^{\\top}\\beta$ is strictly convex, making $J(\\beta)$ strictly convex. Therefore, a unique minimum exists and can be found by taking the gradient with respect to $\\beta$ and setting it to zero. Using standard rules of matrix calculus ($\\nabla_{\\beta}(\\beta^{\\top}a) = a$ and $\\nabla_{\\beta}(\\beta^{\\top}A\\beta) = 2A\\beta$ for symmetric $A$), we compute the gradient:\n$$\n\\nabla_{\\beta} J(\\beta) = \\nabla_{\\beta} (y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}(X^{\\top}X + \\lambda I)\\beta)\n$$\n$$\n\\nabla_{\\beta} J(\\beta) = 0 - 2X^{\\top}y + 2(X^{\\top}X + \\lambda I)\\beta\n$$\nwhere $I$ is the $p \\times p$ identity matrix. Setting the gradient to the zero vector to find the optimal $\\hat{\\beta}_{\\mathrm{ridge}}$:\n$$\n-2X^{\\top}y + 2(X^{\\top}X + \\lambda I)\\hat{\\beta}_{\\mathrm{ridge}} = 0\n$$\n$$\n(X^{\\top}X + \\lambda I)\\hat{\\beta}_{\\mathrm{ridge}} = X^{\\top}y\n$$\nThis gives the general closed-form solution for the ridge estimator:\n$$\n\\hat{\\beta}_{\\mathrm{ridge}} = (X^{\\top}X + \\lambda I)^{-1} X^{\\top}y\n$$\nThe matrix $(X^{\\top}X + \\lambda I)$ is guaranteed to be invertible for $\\lambda > 0$ because $X^{\\top}X$ is positive semi-definite and $\\lambda I$ is positive definite, making their sum positive definite.\n\nNow, we incorporate the specific structure provided in the problem statement. We are given that the predictors are arranged such that $X^{\\top}X = D$, where $D = \\mathrm{diag}(d_{1}, \\dots, d_{p})$ with $d_{j} > 0$ for all $j \\in \\{1,\\dots,p\\}$. We are also given the definition $z = X^{\\top}y$. Substituting these into the general solution yields:\n$$\n\\hat{\\beta}_{\\mathrm{ridge}} = (D + \\lambda I)^{-1} z\n$$\nThis is the closed-form solution in terms of the specified quantities.\n\nTo reveal the component-wise shrinkage, we first analyze the term $(D + \\lambda I)^{-1}$. The sum of two diagonal matrices is a diagonal matrix:\n$$\nD + \\lambda I = \\mathrm{diag}(d_{1}, \\dots, d_{p}) + \\mathrm{diag}(\\lambda, \\dots, \\lambda) = \\mathrm{diag}(d_{1}+\\lambda, \\dots, d_{p}+\\lambda)\n$$\nThe inverse of a diagonal matrix is a diagonal matrix whose diagonal elements are the reciprocals of the original diagonal elements:\n$$\n(D + \\lambda I)^{-1} = \\mathrm{diag}\\left(\\frac{1}{d_{1}+\\lambda}, \\dots, \\frac{1}{d_{p}+\\lambda}\\right)\n$$\nTherefore, the ridge solution vector is:\n$$\n\\hat{\\beta}_{\\mathrm{ridge}} = \\mathrm{diag}\\left(\\frac{1}{d_{1}+\\lambda}, \\dots, \\frac{1}{d_{p}+\\lambda}\\right) z\n$$\nThe $j$-th component of $\\hat{\\beta}_{\\mathrm{ridge}}$, denoted $\\hat{\\beta}_{\\mathrm{ridge}, j}$, is then:\n$$\n\\hat{\\beta}_{\\mathrm{ridge}, j} = \\frac{z_{j}}{d_{j}+\\lambda}\n$$\nTo understand this as a shrinkage mechanism, we compare it to the ordinary least squares (OLS) estimator, $\\hat{\\beta}_{\\mathrm{ols}}$. The OLS estimator is found by solving the normal equations $X^{\\top}X\\beta = X^{\\top}y$. Using our specific structure, this becomes $D\\hat{\\beta}_{\\mathrm{ols}} = z$. The solution is $\\hat{\\beta}_{\\mathrm{ols}} = D^{-1}z$, and its $j$-th component is:\n$$\n\\hat{\\beta}_{\\mathrm{ols}, j} = \\frac{z_{j}}{d_{j}}\n$$\nNow, we can express the ridge component in terms of the OLS component:\n$$\n\\hat{\\beta}_{\\mathrm{ridge}, j} = \\frac{z_{j}}{d_{j}+\\lambda} = \\left(\\frac{d_{j}}{d_{j}+\\lambda}\\right) \\frac{z_{j}}{d_{j}} = \\left(\\frac{d_{j}}{d_{j}+\\lambda}\\right)\\hat{\\beta}_{\\mathrm{ols}, j}\n$$\nThe term $s_j = \\frac{d_{j}}{d_{j}+\\lambda}$ is the multiplicative shrinkage factor for the $j$-th component. Since $d_{j} > 0$ and $\\lambda > 0$, we have $d_{j} < d_{j}+\\lambda$, which implies $0 < s_j < 1$. This shows that each component of the ridge estimator is a scaled-down version of the corresponding OLS estimator component, with the ridge estimate being \"shrunk\" towards zero. The degree of shrinkage depends on both the predictor's corresponding diagonal element $d_j$ and the global tuning parameter $\\lambda$.\n\nThe final closed-form expression for the ridge solution vector in terms of $D$, $\\lambda$, and $z$ is the matrix expression derived prior to the component-wise analysis.", "answer": "$$\\boxed{(D + \\lambda I)^{-1} z}$$", "id": "4983813"}, {"introduction": "Building on the mechanics of ridge regression, this exercise explores one of its most important practical applications: stabilizing coefficient estimates when predictors are highly correlated. You will investigate a hypothetical scenario involving collinear laboratory measures and analyze how the ridge penalty directly improves the numerical stability of the solution by altering the eigenvalues of the $X^{\\top}X$ matrix [@problem_id:4983782]. This practice connects the abstract mathematical concept of a matrix's condition number to the tangible benefit of obtaining more reliable and interpretable models from real-world medical data.", "problem": "A biostatistics team is fitting a multivariable linear model to predict a continuous outcome related to kidney function using $p=4$ standardized laboratory measures (columns of the design matrix $X \\in \\mathbb{R}^{n \\times p}$), known to be highly collinear due to shared physiological pathways. The team considers replacing ordinary least squares with ridge regression to stabilize coefficient estimates. Assume the columns of $X$ are centered and scaled to unit variance, and suppose the symmetric matrix $X^{\\top}X$ has eigenvalues $\\{2000, 300, 1.5, 0.02\\}$, reflecting the presence of near-degeneracy from collinearity. Let $I_p$ denote the $p \\times p$ identity matrix.\n\nStarting from the penalized least squares objective for ridge regression,\n$$\nL(\\beta) \\;=\\; \\|y - X\\beta\\|_2^2 \\;+\\; \\lambda \\|\\beta\\|_2^2,\n$$\nand fundamental facts about spectral decomposition of symmetric matrices and the matrix $2$-norm condition number, carry out the following:\n\n1. Derive the normal equations associated with minimizing $L(\\beta)$ and, using the spectral decomposition $X^{\\top}X = Q \\Lambda Q^{\\top}$ with $Q$ orthogonal and $\\Lambda$ diagonal, determine how the eigenvalues of the linear system matrix are altered by the ridge penalty $\\lambda$.\n\n2. Using the definition of the matrix $2$-norm condition number for a symmetric positive semidefinite matrix $M$ as $\\kappa_2(M) = \\sigma_{\\max}(M) / \\sigma_{\\min}(M)$, express the ridge-regularized condition number $\\kappa_2\\!\\left(X^{\\top}X + \\lambda I_p\\right)$ in terms of the largest and smallest eigenvalues of $X^{\\top}X$ and $\\lambda$.\n\n3. Compute the smallest nonnegative $\\lambda$ such that the condition number of $X^{\\top}X + \\lambda I_p$ is at most $50$.\n\nRound your final numeric answer for $\\lambda$ to four significant figures. Report only the value of $\\lambda$.", "solution": "The problem is valid as it is scientifically grounded in statistical learning theory, well-posed, objective, and contains all necessary information for a unique solution.\n\nThe solution is derived in three parts as requested by the problem statement.\n\n### Part 1: Derivation of Normal Equations and Eigenvalue Analysis\n\nThe objective function for ridge regression is given as:\n$$\nL(\\beta) \\;=\\; \\|y - X\\beta\\|_2^2 \\;+\\; \\lambda \\|\\beta\\|_2^2\n$$\nTo find the coefficient vector $\\beta$ that minimizes this function, we must find the gradient of $L(\\beta)$ with respect to $\\beta$ and set it to zero. First, we expand the squared Euclidean norms:\n$$\nL(\\beta) \\;=\\; (y - X\\beta)^{\\top}(y - X\\beta) \\;+\\; \\lambda \\beta^{\\top}\\beta\n$$\n$$\nL(\\beta) \\;=\\; (y^{\\top} - \\beta^{\\top}X^{\\top})(y - X\\beta) \\;+\\; \\lambda \\beta^{\\top}\\beta\n$$\n$$\nL(\\beta) \\;=\\; y^{\\top}y - y^{\\top}X\\beta - \\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta \\;+\\; \\lambda \\beta^{\\top}\\beta\n$$\nSince $\\beta^{\\top}X^{\\top}y$ is a scalar, it is equal to its transpose, $(\\beta^{\\top}X^{\\top}y)^{\\top} = y^{\\top}X\\beta$. Thus, we can combine the two middle terms:\n$$\nL(\\beta) \\;=\\; y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}(X^{\\top}X + \\lambda I_p)\\beta\n$$\nNow, we compute the gradient of $L(\\beta)$ with respect to the vector $\\beta$:\n$$\n\\frac{\\partial L(\\beta)}{\\partial \\beta} \\;=\\; -2X^{\\top}y + 2(X^{\\top}X + \\lambda I_p)\\beta\n$$\nSetting the gradient to zero to find the minimum:\n$$\n-2X^{\\top}y + 2(X^{\\top}X + \\lambda I_p)\\beta \\;=\\; 0\n$$\n$$\n(X^{\\top}X + \\lambda I_p)\\beta \\;=\\; X^{\\top}y\n$$\nThese are the normal equations for ridge regression. The linear system matrix is $M_{\\lambda} = X^{\\top}X + \\lambda I_p$.\n\nTo determine how the eigenvalues are altered, we use the given spectral decomposition $X^{\\top}X = Q \\Lambda Q^{\\top}$, where $Q$ is an orthogonal matrix ($Q Q^{\\top} = Q^{\\top}Q = I_p$) and $\\Lambda$ is a diagonal matrix whose entries are the eigenvalues of $X^{\\top}X$. Let these eigenvalues be $\\sigma_i$ for $i=1, \\dots, p$.\nWe can rewrite the system matrix $M_{\\lambda}$ as:\n$$\nM_{\\lambda} \\;=\\; X^{\\top}X + \\lambda I_p \\;=\\; Q \\Lambda Q^{\\top} + \\lambda Q Q^{\\top} \\;=\\; Q(\\Lambda + \\lambda I_p)Q^{\\top}\n$$\nThis is the spectral decomposition of $M_{\\lambda}$. The eigenvalues of $M_{\\lambda}$ are the diagonal entries of the matrix $\\Lambda + \\lambda I_p$. If the eigenvalues of $X^{\\top}X$ are $\\{\\sigma_1, \\sigma_2, \\dots, \\sigma_p\\}$, then the eigenvalues of $M_{\\lambda} = X^{\\top}X + \\lambda I_p$ are $\\{\\sigma_1 + \\lambda, \\sigma_2 + \\lambda, \\dots, \\sigma_p + \\lambda\\}$. The ridge penalty $\\lambda$ thus adds a constant shift to every eigenvalue of the matrix $X^{\\top}X$.\n\n### Part 2: Ridge-Regularized Condition Number\n\nThe problem defines the matrix $2$-norm condition number for a symmetric positive semidefinite matrix $M$ as $\\kappa_2(M) = \\sigma_{\\max}(M) / \\sigma_{\\min}(M)$, where $\\sigma_{\\max}(M)$ and $\\sigma_{\\min}(M)$ are the largest and smallest singular values of $M$, respectively. For a symmetric positive semidefinite matrix, the singular values are identical to the eigenvalues.\nThe matrix in question is $M_{\\lambda} = X^{\\top}X + \\lambda I_p$. From Part 1, its eigenvalues are $\\sigma_i + \\lambda$, where $\\sigma_i$ are the eigenvalues of $X^{\\top}X$.\nLet $\\sigma_{\\max}(X^{\\top}X)$ and $\\sigma_{\\min}(X^{\\top}X)$ be the largest and smallest eigenvalues of $X^{\\top}X$. Since $\\lambda \\ge 0$, the largest and smallest eigenvalues of $X^{\\top}X + \\lambda I_p$ are:\n$$\n\\sigma_{\\max}(X^{\\top}X + \\lambda I_p) \\;=\\; \\max_{i}(\\sigma_i + \\lambda) \\;=\\; \\sigma_{\\max}(X^{\\top}X) + \\lambda\n$$\n$$\n\\sigma_{\\min}(X^{\\top}X + \\lambda I_p) \\;=\\; \\min_{i}(\\sigma_i + \\lambda) \\;=\\; \\sigma_{\\min}(X^{\\top}X) + \\lambda\n$$\nTherefore, the ridge-regularized condition number is:\n$$\n\\kappa_2(X^{\\top}X + \\lambda I_p) \\;=\\; \\frac{\\sigma_{\\max}(X^{\\top}X + \\lambda I_p)}{\\sigma_{\\min}(X^{\\top}X + \\lambda I_p)} \\;=\\; \\frac{\\sigma_{\\max}(X^{\\top}X) + \\lambda}{\\sigma_{\\min}(X^{\\top}X) + \\lambda}\n$$\n\n### Part 3: Computation of $\\lambda$\n\nWe are given that the eigenvalues of the $p=4$ matrix $X^{\\top}X$ are $\\{2000, 300, 1.5, 0.02\\}$. The largest and smallest of these are:\n$$\n\\sigma_{\\max}(X^{\\top}X) = 2000\n$$\n$$\n\\sigma_{\\min}(X^{\\top}X) = 0.02\n$$\nThe problem requires finding the smallest non-negative $\\lambda$ such that the condition number of $X^{\\top}X + \\lambda I_p$ is at most $50$. Using the formula from Part 2:\n$$\n\\kappa_2(X^{\\top}X + \\lambda I_p) \\;=\\; \\frac{2000 + \\lambda}{0.02 + \\lambda} \\le 50\n$$\nTo solve for $\\lambda$, we rearrange the inequality. Since $\\lambda \\ge 0$, the denominator $(0.02 + \\lambda)$ is strictly positive, so we can multiply both sides by it without changing the direction of the inequality:\n$$\n2000 + \\lambda \\le 50(0.02 + \\lambda)\n$$\n$$\n2000 + \\lambda \\le 1 + 50\\lambda\n$$\nNow, we isolate $\\lambda$:\n$$\n2000 - 1 \\le 50\\lambda - \\lambda\n$$\n$$\n1999 \\le 49\\lambda\n$$\n$$\n\\lambda \\ge \\frac{1999}{49}\n$$\nThe smallest value of $\\lambda$ that satisfies this condition is $\\lambda = \\frac{1999}{49}$. To provide a numerical answer, we compute this value:\n$$\n\\lambda = \\frac{1999}{49} \\approx 40.795918367...\n$$\nThe problem requires rounding the result to four significant figures. The first four significant figures are $40.79$. The fifth significant digit is $5$, which requires rounding up the last digit. Therefore, $40.79$ is rounded to $40.80$.", "answer": "$$\\boxed{40.80}$$", "id": "4983782"}, {"introduction": "Moving beyond ridge regression, this final practice compares the behavior of the Least Absolute Shrinkage and Selection Operator (LASSO) and the Elastic Net in a critical scenario: perfect collinearity between predictors. This thought experiment reveals the distinct philosophies of these methods; while LASSO arbitrarily selects one predictor and discards the other, the Elastic Net groups them together by assigning them similar coefficients [@problem_id:4983817]. Understanding this \"grouping effect\" is crucial for model selection, especially in medical applications where groups of biomarkers may measure the same underlying biological pathway.", "problem": "A clinical risk model for hospital readmission uses two high-throughput measurements that are, due to a preprocessing artifact, perfectly collinear: both predictors are identical centered and standardized columns. Let $n \\in \\mathbb{N}$ observations be collected, and let the design matrix have two columns $x_{1} \\in \\mathbb{R}^{n}$ and $x_{2} \\in \\mathbb{R}^{n}$ satisfying $x_{1} = x_{2} = x$, with $(1/n)\\,x^{\\top} x = 1$. The outcome is a centered continuous response $y \\in \\mathbb{R}^{n}$ that is proportional to the duplicated signal, $y = \\theta\\,x$, with an unknown signal level $\\theta \\in \\mathbb{R}$, and assume $\\theta > 0$. Consider penalized least squares estimators for the coefficient vector $\\beta = (\\beta_{1},\\beta_{2})$.\n\nDefine the Least Absolute Shrinkage and Selection Operator (LASSO) estimator as the minimizer of\n$$\n\\frac{1}{2n}\\,\\|y - X\\beta\\|^{2} + \\lambda \\left(|\\beta_{1}| + |\\beta_{2}|\\right),\n$$\nwith penalty level $\\lambda > 0$, where $X = [x_{1}\\;x_{2}]$.\n\nDefine the Elastic Net (EN) estimator with mixing parameter $\\alpha \\in (0,1)$ as the minimizer of\n$$\n\\frac{1}{2n}\\,\\|y - X\\beta\\|^{2} + \\lambda \\alpha \\left(|\\beta_{1}| + |\\beta_{2}|\\right) + \\frac{\\lambda(1-\\alpha)}{2}\\left(\\beta_{1}^{2} + \\beta_{2}^{2}\\right).\n$$\n\nFor a coefficient vector $\\beta$, define the grouping effect measure\n$$\nG(\\beta) \\equiv \\frac{|\\beta_{1} - \\beta_{2}|}{|\\beta_{1} + \\beta_{2}|},\n$$\ninterpreting only the case where $|\\beta_{1} + \\beta_{2}| > 0$.\n\nUnder the conditions $\\theta > \\lambda$ and $\\theta > \\lambda \\alpha$, perform the following:\n\n- Starting from the definitions of the two penalized estimators, reduce each optimization to a one-dimensional problem in the sum $s \\equiv \\beta_{1} + \\beta_{2}$ by exploiting $x_{1} = x_{2}$ and $(1/n)\\,x^{\\top}x = 1$, and determine the corresponding optimal sum $s_{\\text{LASSO}}^{\\star}$ and $s_{\\text{EN}}^{\\star}$.\n\n- For LASSO, argue why the minimizers are not unique and that there exist minimizing solutions with one coefficient set to zero, for example $(\\hat{\\beta}_{1},\\hat{\\beta}_{2}) = (s_{\\text{LASSO}}^{\\star},0)$. For EN, show that among all splits of a fixed sum $s$, the penalty is minimized by equal sharing, i.e., $(\\hat{\\beta}_{1},\\hat{\\beta}_{2}) = \\left(s_{\\text{EN}}^{\\star}/2,\\,s_{\\text{EN}}^{\\star}/2\\right)$.\n\n- Compute $G_{\\text{LASSO}} \\equiv G(\\hat{\\beta}_{\\text{LASSO}})$ for the extreme LASSO minimizer with one zero coefficient and $G_{\\text{EN}} \\equiv G(\\hat{\\beta}_{\\text{EN}})$ for the EN minimizer with equal coefficients.\n\nReport the single scalar difference\n$$\n\\Delta \\equiv G_{\\text{LASSO}} - G_{\\text{EN}}.\n$$\nGive your final answer as an exact value with no rounding. No units are required.", "solution": "The problem is first validated and found to be well-posed, scientifically grounded, and internally consistent. We can proceed with the derivation.\n\nThe core of the problem lies in minimizing two different penalized least squares objective functions. A common term in both is the scaled residual sum of squares (RSS), which we simplify first.\nThe design matrix is $X = [x_{1}\\;x_{2}]$. Given that $x_{1} = x_{2} = x$, the prediction is $X\\beta = x_{1}\\beta_{1} + x_{2}\\beta_{2} = x\\beta_{1} + x\\beta_{2} = x(\\beta_{1} + \\beta_{2})$.\nLet $s \\equiv \\beta_{1} + \\beta_{2}$. The prediction is then $x s$.\nThe response is given as $y = \\theta x$.\nThe RSS term is $\\|y - X\\beta\\|^{2} = \\|\\theta x - xs\\|^{2} = \\|(\\theta-s)x\\|^{2} = (\\theta-s)^{2}\\|x\\|^{2}$.\nThe problem provides the normalization condition $(1/n)\\,x^{\\top} x = 1$. As $x^{\\top} x = \\|x\\|^{2}$, this implies $\\|x\\|^{2} = n$.\nSubstituting this into the scaled RSS term gives:\n$$\n\\frac{1}{2n}\\,\\|y - X\\beta\\|^{2} = \\frac{1}{2n}(\\theta-s)^{2}n = \\frac{1}{2}(\\theta-s)^{2}\n$$\nThis simplified loss term depends on $\\beta_{1}$ and $\\beta_{2}$ only through their sum $s$.\n\nFirst, we analyze the Least Absolute Shrinkage and Selection Operator (LASSO) estimator. The objective function to minimize is:\n$$\nL_{\\text{LASSO}}(\\beta_{1}, \\beta_{2}) = \\frac{1}{2}(\\theta-s)^{2} + \\lambda(|\\beta_{1}| + |\\beta_{2}|)\n$$\nTo minimize this function, we can adopt a two-stage approach. For a fixed sum $s = \\beta_{1} + \\beta_{2}$, we first minimize the penalty term $\\lambda (|\\beta_{1}| + |\\beta_{2}|)$ with respect to all possible splits of $s$. The RSS term $\\frac{1}{2}(\\theta-s)^{2}$ is constant for a fixed $s$.\nThe minimum value of the $\\ell_{1}$-norm $|\\beta_{1}| + |\\beta_{2}|$ subject to the constraint $\\beta_{1} + \\beta_{2} = s$ is $|s|$. This minimum is achieved if and only if $\\beta_{1}$ and $\\beta_{2}$ have the same sign or one of them is zero. This means that for a given optimal sum $s_{\\text{LASSO}}^{\\star}$, the solution for $(\\beta_{1}, \\beta_{2})$ is not unique.\nThe optimization problem for LASSO thus reduces to a one-dimensional problem in $s$:\n$$\n\\min_{s \\in \\mathbb{R}} \\left\\{ \\frac{1}{2}(\\theta-s)^{2} + \\lambda|s| \\right\\}\n$$\nThis is a standard problem whose solution is given by the soft-thresholding operator. The optimal sum $s_{\\text{LASSO}}^{\\star}$ is:\n$$\ns_{\\text{LASSO}}^{\\star} = \\text{sign}(\\theta)(|\\theta|-\\lambda)_{+}\n$$\nGiven the problem conditions $\\theta > 0$ and $\\theta > \\lambda$, this simplifies to:\n$$\ns_{\\text{LASSO}}^{\\star} = (\\theta-\\lambda)_{+} = \\theta - \\lambda\n$$\nAs established, any pair $(\\hat{\\beta}_{1}, \\hat{\\beta}_{2})$ with $\\hat{\\beta}_{1} \\ge 0$, $\\hat{\\beta}_{2} \\ge 0$ and $\\hat{\\beta}_{1} + \\hat{\\beta}_{2} = s_{\\text{LASSO}}^{\\star} = \\theta - \\lambda$ is a valid LASSO minimizer. The problem asks us to consider the specific \"extreme\" minimizer $\\hat{\\beta}_{\\text{LASSO}}$ where one coefficient is zero, say $\\hat{\\beta}_{\\text{LASSO}} = (s_{\\text{LASSO}}^{\\star}, 0) = (\\theta - \\lambda, 0)$.\nWe compute the grouping effect measure $G_{\\text{LASSO}}$ for this solution. Since $\\theta>\\lambda$, we have $|\\hat{\\beta}_{1} + \\hat{\\beta}_{2}| = |\\theta-\\lambda| > 0$, so $G$ is well-defined.\n$$\nG_{\\text{LASSO}} = G(\\hat{\\beta}_{\\text{LASSO}}) = \\frac{|\\hat{\\beta}_{1} - \\hat{\\beta}_{2}|}{|\\hat{\\beta}_{1} + \\hat{\\beta}_{2}|} = \\frac{|(\\theta-\\lambda) - 0|}{|(\\theta-\\lambda)+0|} = \\frac{|\\theta-\\lambda|}{|\\theta-\\lambda|} = 1\n$$\n\nNext, we analyze the Elastic Net (EN) estimator. The objective function is:\n$$\nL_{\\text{EN}}(\\beta_{1}, \\beta_{2}) = \\frac{1}{2}(\\theta-s)^{2} + \\lambda \\alpha (|\\beta_{1}| + |\\beta_{2}|) + \\frac{\\lambda(1-\\alpha)}{2}(\\beta_{1}^{2} + \\beta_{2}^{2})\n$$\nAgain, for a fixed sum $s = \\beta_{1}+\\beta_{2}$, we minimize the penalty part. Assuming the optimal $s > 0$ (which we will verify later), any optimal solution will have $\\beta_{1}, \\beta_{2} \\ge 0$. In this case, $|\\beta_{1}| + |\\beta_{2}| = \\beta_{1} + \\beta_{2} = s$. The $\\ell_1$ part of the penalty, $\\lambda \\alpha s$, becomes constant for a fixed $s$. The task reduces to minimizing the $\\ell_2$ part of the penalty, $\\frac{\\lambda(1-\\alpha)}{2}(\\beta_{1}^{2} + \\beta_{2}^{2})$, subject to $\\beta_{1}+\\beta_{2}=s$ and $\\beta_{1}, \\beta_{2} \\ge 0$.\nWe need to minimize $\\beta_{1}^{2} + \\beta_{2}^{2} = \\beta_{1}^{2} + (s-\\beta_{1})^{2}$. Let $f(\\beta_{1}) = 2\\beta_{1}^{2} - 2s\\beta_{1} + s^{2}$. This is a convex parabola with a minimum where its derivative is zero: $f'(\\beta_{1}) = 4\\beta_{1} - 2s = 0$, which yields $\\beta_{1} = s/2$. Consequently, $\\beta_{2}=s/2$. The strict convexity of the squared $\\ell_{2}$-norm forces the coefficients to be equal, demonstrating the EN grouping effect.\n\nWith the optimal split $(\\beta_{1}, \\beta_{2}) = (s/2, s/2)$, the EN objective becomes a function of $s$ alone:\n$$\nL_{\\text{EN}}(s) = \\frac{1}{2}(\\theta-s)^{2} + \\lambda \\alpha |s| + \\frac{\\lambda(1-\\alpha)}{2}\\left(\\left(\\frac{s}{2}\\right)^{2} + \\left(\\frac{s}{2}\\right)^{2}\\right) = \\frac{1}{2}(\\theta-s)^{2} + \\lambda \\alpha |s| + \\frac{\\lambda(1-\\alpha)}{4}s^{2}\n$$\nWe find the optimal sum $s_{\\text{EN}}^{\\star}$ by minimizing this function. Given $\\theta > \\lambda\\alpha > 0$, we can assume $s > 0$. Differentiating with respect to $s$ and setting to zero:\n$$\n\\frac{d}{ds}L_{\\text{EN}}(s) = -(\\theta-s) + \\lambda\\alpha + \\frac{\\lambda(1-\\alpha)}{2}s = 0\n$$\n$$\ns\\left(1 + \\frac{\\lambda(1-\\alpha)}{2}\\right) = \\theta - \\lambda\\alpha \\implies s_{\\text{EN}}^{\\star} = \\frac{\\theta - \\lambda\\alpha}{1 + \\frac{\\lambda(1-\\alpha)}{2}}\n$$\nThe condition $\\theta > \\lambda\\alpha$ ensures $s_{\\text{EN}}^{\\star} > 0$, consistent with our assumption.\nThe EN estimated coefficients are $\\hat{\\beta}_{\\text{EN}} = (\\hat{\\beta}_{1}, \\hat{\\beta}_{2}) = (s_{\\text{EN}}^{\\star}/2, s_{\\text{EN}}^{\\star}/2)$.\nWe compute the grouping effect $G_{\\text{EN}}$ for this solution. The denominator $|\\hat{\\beta}_{1} + \\hat{\\beta}_{2}| = |s_{\\text{EN}}^{\\star}|$ is non-zero.\n$$\nG_{\\text{EN}} = G(\\hat{\\beta}_{\\text{EN}}) = \\frac{|\\hat{\\beta}_{1} - \\hat{\\beta}_{2}|}{|\\hat{\\beta}_{1} + \\hat{\\beta}_{2}|} = \\frac{|s_{\\text{EN}}^{\\star}/2 - s_{\\text{EN}}^{\\star}/2|}{|s_{\\text{EN}}^{\\star}/2 + s_{\\text{EN}}^{\\star}/2|} = \\frac{0}{|s_{\\text{EN}}^{\\star}|} = 0\n$$\n\nFinally, we compute the required difference $\\Delta = G_{\\text{LASSO}} - G_{\\text{EN}}$.\n$$\n\\Delta = 1 - 0 = 1\n$$\nThis result quantifies the paradigmatic difference between LASSO and Elastic Net in the presence of perfectly correlated predictors: LASSO selects one and discards the other ($G=1$), while Elastic Net groups them by assigning them equal coefficients ($G=0$).", "answer": "$$\n\\boxed{1}\n$$", "id": "4983817"}]}