{"hands_on_practices": [{"introduction": "Model selection often involves a trade-off between goodness-of-fit and complexity. This exercise explores the fundamental difference between the Akaike Information Criterion ($AIC$), which prioritizes predictive accuracy, and the Bayesian Information Criterion ($BIC$), which favors parsimony and aims to identify the true data-generating model. By analyzing a scenario where $AIC$ and $BIC$ yield conflicting recommendations for nested Cox proportional hazards models, you will gain a deeper appreciation for how the choice of criterion should align with the specific goals of your medical research [@problem_id:4815133].", "problem": "A hospital system conducts a registry-based cohort study to evaluate time to first hospitalization for heart failure among patients with type 2 diabetes. Analysts fit two nested Cox proportional hazards models to the time-to-event data using the partial likelihood. Model $\\mathcal{M}_{0}$ includes baseline covariates age, sex, race, smoking status, and estimated glomerular filtration rate. Model $\\mathcal{M}_{1}$ adds $d$ additional continuous covariates: five circulating biomarkers representing inflammation and myocardial stress.\n\nLet $n$ denote the number of patients with complete follow-up and covariates, $k_{0}$ the number of free parameters in $\\mathcal{M}_{0}$, and $k_{1}$ the number in $\\mathcal{M}_{1}$, with $d = k_{1} - k_{0}$. Let $\\ell_{0}$ and $\\ell_{1}$ denote the maximized partial log-likelihoods for $\\mathcal{M}_{0}$ and $\\mathcal{M}_{1}$, respectively, and define the improvement $\\Delta \\ell = \\ell_{1} - \\ell_{0}$. In this study, the sample size is $n = 1200$, the number of added parameters is $d = 5$, and the observed improvement is $\\Delta \\ell = 10$.\n\nStarting from the foundational principles of information-theoretic risk minimization and Bayesian model evidence approximation, derive the decision conditions for preferring $\\mathcal{M}_{1}$ over $\\mathcal{M}_{0}$ under the Akaike Information Criterion (AIC) and the Bayes Information Criterion (BIC). Use these conditions to determine whether AIC selects the larger model and whether BIC selects the smaller model in this scenario, and interpret the decisions in terms of predictive versus identification goals in evidence-based medicine.\n\nFinally, compute the numerical gap between the two decision thresholds—that is, the difference between the improvement in maximized partial log-likelihood required for BIC to prefer $\\mathcal{M}_{1}$ and the improvement required for AIC to prefer $\\mathcal{M}_{1}$—for the given values of $n$ and $d$. Report this gap in natural logarithm units, rounded to four significant figures.", "solution": "We consider nested models $\\mathcal{M}_{0}$ and $\\mathcal{M}_{1}$, with $\\mathcal{M}_{1}$ adding $d = k_{1} - k_{0}$ covariates. The Cox proportional hazards model is estimated via the partial likelihood, whose maximized log-value we denote by $\\ell$. Although the partial likelihood is not a full likelihood for all data-generating features, it is standard to treat $\\ell$ analogously to a log-likelihood in large-sample model selection for proportional hazards.\n\nWe begin from two foundational bases:\n\n1. Information-theoretic risk minimization: The Akaike Information Criterion (AIC) arises from asymptotically unbiased estimation of the expected Kullback–Leibler discrepancy between a candidate model and the data-generating mechanism. The classical result is that, for a model with $k$ free parameters, the criterion to be minimized is\n$$\n\\mathrm{AIC} = -2 \\ell + 2 k.\n$$\nFor the nested comparison, the difference\n$$\n\\Delta \\mathrm{AIC} = \\mathrm{AIC}_{1} - \\mathrm{AIC}_{0} = \\left(-2 \\ell_{1} + 2 k_{1}\\right) - \\left(-2 \\ell_{0} + 2 k_{0}\\right) = -2 (\\ell_{1} - \\ell_{0}) + 2 (k_{1} - k_{0}) = -2 \\Delta \\ell + 2 d.\n$$\nAIC prefers $\\mathcal{M}_{1}$ if $\\Delta \\mathrm{AIC} < 0$, which yields the decision condition\n$$\n-2 \\Delta \\ell + 2 d < 0 \\quad \\Longleftrightarrow \\quad \\Delta \\ell > d.\n$$\n\n2. Bayesian model evidence approximation: The Bayes Information Criterion (BIC), derived from a Laplace approximation to the marginal likelihood under a regular prior with fixed dimension, leads to minimizing\n$$\n\\mathrm{BIC} = -2 \\ell + k \\ln n.\n$$\nHence, the difference\n$$\n\\Delta \\mathrm{BIC} = \\mathrm{BIC}_{1} - \\mathrm{BIC}_{0} = -2 \\Delta \\ell + d \\ln n.\n$$\nBIC prefers $\\mathcal{M}_{1}$ if $\\Delta \\mathrm{BIC} < 0$, which yields the decision condition\n$$\n-2 \\Delta \\ell + d \\ln n < 0 \\quad \\Longleftrightarrow \\quad \\Delta \\ell > \\frac{d \\ln n}{2}.\n$$\n\nThese decision conditions compare the observed improvement in maximized partial log-likelihood, $\\Delta \\ell$, against two thresholds:\n- AIC threshold: $T_{\\mathrm{AIC}} = d$,\n- BIC threshold: $T_{\\mathrm{BIC}} = \\frac{d \\ln n}{2}$.\n\nGiven $n = 1200$, $d = 5$, and $\\Delta \\ell = 10$, we compute:\n$$\nT_{\\mathrm{AIC}} = d = 5,\n$$\nand\n$$\nT_{\\mathrm{BIC}} = \\frac{d \\ln n}{2} = \\frac{5 \\ln(1200)}{2}.\n$$\nWe evaluate $\\ln(1200)$ using $\\ln(12) + \\ln(100)$ with $\\ln(12) \\approx 2.484906649788$ and $\\ln(100) \\approx 4.605170185988$, giving\n$$\n\\ln(1200) \\approx 7.090076835776.\n$$\nThus,\n$$\nT_{\\mathrm{BIC}} \\approx \\frac{5 \\times 7.090076835776}{2} \\approx 17.72519208944.\n$$\n\nWe compare $\\Delta \\ell = 10$ to these thresholds:\n- For AIC: $\\Delta \\ell = 10 > T_{\\mathrm{AIC}} = 5$, so AIC prefers $\\mathcal{M}_{1}$.\n- For BIC: $\\Delta \\ell = 10 < T_{\\mathrm{BIC}} \\approx 17.7252$, so BIC prefers $\\mathcal{M}_{0}$.\n\nInterpretation in evidence-based medicine: The Akaike Information Criterion, grounded in minimizing expected out-of-sample Kullback–Leibler discrepancy, prioritizes predictive accuracy and is tolerant of including variables that improve fit even modestly. Therefore, AIC selects the larger model $\\mathcal{M}_{1}$, consistent with a predictive goal such as risk stratification where incremental improvements in calibration and discrimination are valued. The Bayes Information Criterion, approximating the log posterior model probability under regularity, imposes a stronger penalty increasing with sample size, thus favoring parsimony for identifying the data-generating structure. Consequently, BIC selects the smaller model $\\mathcal{M}_{0}$, aligning with an identification goal that emphasizes fewer covariates and robustness to overfitting when inferring mechanisms or selecting a minimal sufficient set of predictors.\n\nFinally, we compute the gap between the two decision thresholds:\n$$\n\\text{Gap} = T_{\\mathrm{BIC}} - T_{\\mathrm{AIC}} = \\frac{d \\ln n}{2} - d.\n$$\nSubstituting $d = 5$ and $n = 1200$,\n$$\n\\text{Gap} = \\frac{5 \\ln(1200)}{2} - 5 \\approx 17.72519208944 - 5 = 12.72519208944.\n$$\nRounded to four significant figures, the gap is $12.73$ in natural logarithm units.", "answer": "$$\\boxed{12.73}$$", "id": "4815133"}, {"introduction": "Evaluating a clinical prediction model requires more than a single performance metric; a nuanced assessment of its probabilistic forecasts is essential. The Brier score offers a comprehensive measure of prediction error, which can be elegantly decomposed into distinct components representing calibration (the reliability of probability estimates) and resolution (the ability to discriminate between different risk strata). This practice guides you through the theoretical derivation and practical application of this decomposition, equipping you with a sophisticated tool for critiquing and comparing predictive models [@problem_id:4985110].", "problem": "A hospital system is comparing two probabilistic risk models for in-hospital mortality in a common target population. For a randomly drawn patient, let the binary outcome be $Y \\in \\{0,1\\}$ and let a model produce a probability prediction $\\hat{p} \\in [0,1]$. The Brier score $\\mathrm{BS}$ is defined as the expected squared error of the probability prediction for the binary outcome, that is, $\\mathrm{BS} = \\mathbb{E}\\big[(Y - \\hat{p})^{2}\\big]$. Using only fundamental probability identities such as the law of total expectation and the law of total variance, derive a decomposition of $\\mathrm{BS}$ into a term that reflects calibration error and a term that reflects discrimination (also known as resolution), together with a term that depends only on the marginal prevalence of $Y=1$.\n\nBoth models are applied to the same patient population and their predictions fall into three strata as follows. For Model $\\mathcal{A}$, the forecast $\\hat{p}_{\\mathcal{A}}$ takes values $\\{0.05, 0.20, 0.60\\}$ with probabilities $\\{0.5, 0.3, 0.2\\}$, respectively. The true conditional outcome probability given the forecast is $\\pi_{\\mathcal{A}}(\\hat{p}_{\\mathcal{A}}) \\in \\{0.10, 0.18, 0.48\\}$ for these strata in the same order. For Model $\\mathcal{B}$, the forecast $\\hat{p}_{\\mathcal{B}}$ takes values $\\{0.12, 0.24, 0.34\\}$ with the same probabilities $\\{0.5, 0.3, 0.2\\}$, and Model $\\mathcal{B}$ is perfectly calibrated in the sense that $\\pi_{\\mathcal{B}}(\\hat{p}_{\\mathcal{B}}) = \\hat{p}_{\\mathcal{B}}$ in each stratum.\n\nAssume the data-generating process is such that for each model the marginal prevalence of $Y=1$ equals the corresponding stratum-weighted mean of the true conditional probabilities $\\pi(\\hat{p})$. Using your derived decomposition, compute the expected Brier score for each model and then compute the difference\n$$\n\\Delta = \\mathbb{E}\\big[(Y - \\hat{p}_{\\mathcal{A}})^{2}\\big] - \\mathbb{E}\\big[(Y - \\hat{p}_{\\mathcal{B}})^{2}\\big].\n$$\nRound your final numerical answer for $\\Delta$ to four significant figures. Express the answer as a unitless decimal on the probability scale (do not use a percentage sign).", "solution": "The problem requires two parts: first, to derive a decomposition of the Brier score, and second, to apply this decomposition to compare two given models.\n\n**Part 1: Decomposition of the Brier Score**\n\nThe Brier score ($\\mathrm{BS}$) is defined as the expected squared error of a probability prediction $\\hat{p}$ for a binary outcome $Y \\in \\{0, 1\\}$.\n$$\n\\mathrm{BS} = \\mathbb{E}\\big[(Y - \\hat{p})^{2}\\big]\n$$\nWe are asked to decompose this using fundamental probability identities. Let $\\pi(\\hat{p})$ be the true conditional probability of the outcome given the forecast, i.e., $\\pi(\\hat{p}) = P(Y=1 | \\hat{p}) = \\mathbb{E}[Y | \\hat{p}]$. We can add and subtract $\\pi(\\hat{p})$ inside the squared term:\n$$\n\\mathrm{BS} = \\mathbb{E}\\big[ ( (Y - \\pi(\\hat{p})) + (\\pi(\\hat{p}) - \\hat{p}) )^{2} \\big]\n$$\nExpanding the square gives three terms:\n$$\n\\mathrm{BS} = \\mathbb{E}\\big[ (Y - \\pi(\\hat{p}))^2 \\big] + \\mathbb{E}\\big[ (\\pi(\\hat{p}) - \\hat{p})^2 \\big] + 2 \\mathbb{E}\\big[ (Y - \\pi(\\hat{p}))(\\pi(\\hat{p}) - \\hat{p}) \\big]\n$$\nWe analyze the cross-term using the law of total expectation, $\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X|Z]]$, by conditioning on $\\hat{p}$:\n$$\n\\mathbb{E}\\big[ (Y - \\pi(\\hat{p}))(\\pi(\\hat{p}) - \\hat{p}) \\big] = \\mathbb{E}\\Big[\\mathbb{E}\\big[ (Y - \\pi(\\hat{p}))(\\pi(\\hat{p}) - \\hat{p}) \\big| \\hat{p} \\big]\\Big]\n$$\nInside the conditional expectation, the term $(\\pi(\\hat{p}) - \\hat{p})$ is a function of $\\hat{p}$ and thus can be treated as a constant:\n$$\n\\mathbb{E}\\big[ (Y - \\pi(\\hat{p}))(\\pi(\\hat{p}) - \\hat{p}) \\big| \\hat{p} \\big] = (\\pi(\\hat{p}) - \\hat{p}) \\mathbb{E}\\big[ Y - \\pi(\\hat{p}) \\big| \\hat{p} \\big]\n$$\nBy the linearity of expectation and the definition of $\\pi(\\hat{p})$:\n$$\n\\mathbb{E}\\big[ Y - \\pi(\\hat{p}) \\big| \\hat{p} \\big] = \\mathbb{E}[Y | \\hat{p}] - \\mathbb{E}[\\pi(\\hat{p}) | \\hat{p}] = \\pi(\\hat{p}) - \\pi(\\hat{p}) = 0\n$$\nSince the inner expectation is $0$, the total expectation of the cross-term is also $0$. The Brier score decomposition simplifies to:\n$$\n\\mathrm{BS} = \\mathbb{E}\\big[ (\\pi(\\hat{p}) - \\hat{p})^2 \\big] + \\mathbb{E}\\big[ (Y - \\pi(\\hat{p}))^2 \\big]\n$$\nThe first term, $\\mathbb{E}\\big[ (\\pi(\\hat{p}) - \\hat{p})^2 \\big]$, is the **calibration error** (also called reliability). It measures the mean squared difference between the forecast probabilities $\\hat{p}$ and the true conditional event probabilities $\\pi(\\hat{p})$.\n\nThe second term can be further decomposed. Let $\\bar{y} = P(Y=1)$ be the marginal prevalence of the outcome. By the law of total probability, $\\bar{y} = \\mathbb{E}[P(Y=1|\\hat{p})] = \\mathbb{E}[\\pi(\\hat{p})]$.\nThe second term is evaluated using the law of total expectation:\n$$\n\\mathbb{E}\\big[ (Y - \\pi(\\hat{p}))^2 \\big] = \\mathbb{E}\\Big[\\mathbb{E}\\big[ (Y - \\mathbb{E}[Y|\\hat{p}])^2 \\big| \\hat{p} \\big]\\Big] = \\mathbb{E}\\big[\\mathrm{Var}(Y|\\hat{p})\\big]\n$$\nSince $Y$ conditional on $\\hat{p}$ is a Bernoulli variable with parameter $\\pi(\\hat{p})$, its variance is $\\mathrm{Var}(Y|\\hat{p}) = \\pi(\\hat{p})(1-\\pi(\\hat{p}))$.\nSo, $\\mathbb{E}\\big[ (Y - \\pi(\\hat{p}))^2 \\big] = \\mathbb{E}[\\pi(\\hat{p})(1-\\pi(\\hat{p}))] = \\mathbb{E}[\\pi(\\hat{p})] - \\mathbb{E}[\\pi(\\hat{p})^2]$.\nUsing the identity $\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$, we have $\\mathbb{E}[\\pi(\\hat{p})^2] = \\mathrm{Var}(\\pi(\\hat{p})) + (\\mathbb{E}[\\pi(\\hat{p})])^2$.\nSubstituting this, we get:\n$$\n\\mathbb{E}\\big[ (Y - \\pi(\\hat{p}))^2 \\big] = \\mathbb{E}[\\pi(\\hat{p})] - \\big(\\mathrm{Var}(\\pi(\\hat{p})) + (\\mathbb{E}[\\pi(\\hat{p})])^2\\big) = \\bar{y} - (\\mathrm{Var}(\\pi(\\hat{p})) + \\bar{y}^2) = \\bar{y}(1-\\bar{y}) - \\mathrm{Var}(\\pi(\\hat{p}))\n$$\nThe term $\\mathrm{Var}(\\pi(\\hat{p})) = \\mathbb{E}[(\\pi(\\hat{p}) - \\mathbb{E}[\\pi(\\hat{p})])^2] = \\mathbb{E}[(\\pi(\\hat{p}) - \\bar{y})^2]$ is the **resolution** (or discrimination). It measures the ability of the forecasts to partition the population into sub-groups with event rates different from the overall prevalence. The term $\\bar{y}(1-\\bar{y})$ is the **uncertainty**, representing the inherent variability of the outcome in the population.\n\nCombining all parts, the full decomposition is:\n$$\n\\mathrm{BS} = \\underbrace{\\mathbb{E}\\big[(\\hat{p} - \\pi(\\hat{p}))^2\\big]}_{\\text{Calibration}} - \\underbrace{\\mathbb{E}\\big[(\\pi(\\hat{p}) - \\bar{y})^2\\big]}_{\\text{Resolution}} + \\underbrace{\\bar{y}(1-\\bar{y})}_{\\text{Uncertainty}}\n$$\n\n**Part 2: Computation for Models $\\mathcal{A}$ and $\\mathcal{B}$**\n\nThe expectations are computed as weighted averages over the discrete forecast strata. Let $w_i$, $\\hat{p}_i$, and $\\pi_i$ be the probability, forecast value, and true conditional probability for stratum $i$.\n\nFirst, we determine the marginal prevalence $\\bar{y}$, which must be the same for both models as they are applied to the same population. Let's compute it for Model $\\mathcal{A}$:\n$$\n\\bar{y} = \\mathbb{E}[\\pi_{\\mathcal{A}}(\\hat{p}_{\\mathcal{A}})] = \\sum_{i=1}^{3} w_i \\pi_{\\mathcal{A},i} = (0.5)(0.10) + (0.3)(0.18) + (0.2)(0.48) = 0.05 + 0.054 + 0.096 = 0.20\n$$\nAs a check, for Model $\\mathcal{B}$:\n$$\n\\mathbb{E}[\\pi_{\\mathcal{B}}(\\hat{p}_{\\mathcal{B}})] = \\sum_{i=1}^{3} w_i \\pi_{\\mathcal{B},i} = (0.5)(0.12) + (0.3)(0.24) + (0.2)(0.34) = 0.06 + 0.072 + 0.068 = 0.20\n$$\nThe prevalence is indeed $\\bar{y} = 0.20$. The uncertainty component is common to both models:\n$$\n\\mathrm{UNC} = \\bar{y}(1-\\bar{y}) = 0.20(1 - 0.20) = 0.16\n$$\n\n**Brier Score for Model $\\mathcal{A}$:**\n1.  Calibration ($\\mathrm{CAL}_{\\mathcal{A}}$):\n$$\n\\mathrm{CAL}_{\\mathcal{A}} = \\mathbb{E}\\big[(\\hat{p}_{\\mathcal{A}} - \\pi_{\\mathcal{A}}(\\hat{p}_{\\mathcal{A}}))^2\\big] = \\sum_{i=1}^{3} w_i (\\hat{p}_{\\mathcal{A},i} - \\pi_{\\mathcal{A},i})^2\n$$\n$$\n\\mathrm{CAL}_{\\mathcal{A}} = 0.5(0.05 - 0.10)^2 + 0.3(0.20 - 0.18)^2 + 0.2(0.60 - 0.48)^2\n$$\n$$\n\\mathrm{CAL}_{\\mathcal{A}} = 0.5(-0.05)^2 + 0.3(0.02)^2 + 0.2(0.12)^2 = 0.5(0.0025) + 0.3(0.0004) + 0.2(0.0144)\n$$\n$$\n\\mathrm{CAL}_{\\mathcal{A}} = 0.00125 + 0.00012 + 0.00288 = 0.00425\n$$\n2.  Resolution ($\\mathrm{RES}_{\\mathcal{A}}$):\n$$\n\\mathrm{RES}_{\\mathcal{A}} = \\mathbb{E}\\big[(\\pi_{\\mathcal{A}}(\\hat{p}_{\\mathcal{A}}) - \\bar{y})^2\\big] = \\sum_{i=1}^{3} w_i (\\pi_{\\mathcal{A},i} - \\bar{y})^2\n$$\n$$\n\\mathrm{RES}_{\\mathcal{A}} = 0.5(0.10 - 0.20)^2 + 0.3(0.18 - 0.20)^2 + 0.2(0.48 - 0.20)^2\n$$\n$$\n\\mathrm{RES}_{\\mathcal{A}} = 0.5(-0.10)^2 + 0.3(-0.02)^2 + 0.2(0.28)^2 = 0.5(0.01) + 0.3(0.0004) + 0.2(0.0784)\n$$\n$$\n\\mathrm{RES}_{\\mathcal{A}} = 0.005 + 0.00012 + 0.01568 = 0.0208\n$$\n3.  Brier Score ($\\mathrm{BS}_{\\mathcal{A}}$):\n$$\n\\mathrm{BS}_{\\mathcal{A}} = \\mathrm{CAL}_{\\mathcal{A}} - \\mathrm{RES}_{\\mathcal{A}} + \\mathrm{UNC} = 0.00425 - 0.0208 + 0.16 = 0.14345\n$$\n\n**Brier Score for Model $\\mathcal{B}$:**\n1.  Calibration ($\\mathrm{CAL}_{\\mathcal{B}}$):\nModel $\\mathcal{B}$ is perfectly calibrated, so $\\pi_{\\mathcal{B}}(\\hat{p}_{\\mathcal{B}}) = \\hat{p}_{\\mathcal{B}}$ for all strata.\n$$\n\\mathrm{CAL}_{\\mathcal{B}} = \\mathbb{E}\\big[(\\hat{p}_{\\mathcal{B}} - \\pi_{\\mathcal{B}}(\\hat{p}_{\\mathcal{B}}))^2\\big] = \\mathbb{E}[0] = 0\n$$\n2.  Resolution ($\\mathrm{RES}_{\\mathcal{B}}$):\nSince $\\pi_{\\mathcal{B},i} = \\hat{p}_{\\mathcal{B},i}$, the resolution is $\\mathrm{RES}_{\\mathcal{B}} = \\mathbb{E}[(\\hat{p}_{\\mathcal{B}} - \\bar{y})^2]$.\n$$\n\\mathrm{RES}_{\\mathcal{B}} = \\sum_{i=1}^{3} w_i (\\hat{p}_{\\mathcal{B},i} - \\bar{y})^2\n$$\n$$\n\\mathrm{RES}_{\\mathcal{B}} = 0.5(0.12 - 0.20)^2 + 0.3(0.24 - 0.20)^2 + 0.2(0.34 - 0.20)^2\n$$\n$$\n\\mathrm{RES}_{\\mathcal{B}} = 0.5(-0.08)^2 + 0.3(0.04)^2 + 0.2(0.14)^2 = 0.5(0.0064) + 0.3(0.0016) + 0.2(0.0196)\n$$\n$$\n\\mathrm{RES}_{\\mathcal{B}} = 0.0032 + 0.00048 + 0.00392 = 0.0076\n$$\n3.  Brier Score ($\\mathrm{BS}_{\\mathcal{B}}$):\n$$\n\\mathrm{BS}_{\\mathcal{B}} = \\mathrm{CAL}_{\\mathcal{B}} - \\mathrm{RES}_{\\mathcal{B}} + \\mathrm{UNC} = 0 - 0.0076 + 0.16 = 0.1524\n$$\n\n**Difference in Brier Scores:**\nThe required difference is $\\Delta = \\mathrm{BS}_{\\mathcal{A}} - \\mathrm{BS}_{\\mathcal{B}}$.\n$$\n\\Delta = 0.14345 - 0.1524 = -0.00895\n$$\nThe problem requires the answer rounded to four significant figures. The number $-0.00895$ has three significant figures ($8$, $9$, $5$). To express it with four, we add a trailing zero:\n$$\n\\Delta \\approx -0.008950\n$$", "answer": "$$\\boxed{-0.008950}$$", "id": "4985110"}, {"introduction": "Rather than committing to a single \"best\" model, we can often achieve superior predictive performance by strategically combining the strengths of several competing models. Stacking is a powerful ensemble method that learns the optimal weights for combining model predictions by maximizing the cross-validated log-likelihood of the ensemble. In this exercise, you will implement a stacking procedure to create an optimal blend of three logistic regression models, illustrating a shift from the paradigm of model selection to the more flexible and often more powerful approach of model combination [@problem_id:4985108].", "problem": "You are given three competing logistic models that output predicted probabilities for a binary medical outcome, along with $K$-fold Cross-Validation (CV) validation predictions for each observation generated by training on $K-1$ folds and predicting on the held-out fold. Consider stacking these models by forming an ensemble that predicts, for each observation $i$, a probability $p_i(\\mathbf{w}) = \\sum_{m=1}^{3} w_m p_{im}$, where $p_{im}$ is the CV-predicted probability from model $m$ for observation $i$, and $\\mathbf{w} = (w_1,w_2,w_3)$ are nonnegative stacking weights constrained by $\\sum_{m=1}^{3} w_m = 1$ and $w_m \\ge 0$. Assume the standard Bernoulli log-likelihood for binary outcomes, and that the dataset is partitioned into $K$ folds such that the union of validation predictions over all folds covers all observations exactly once.\n\nFundamental base:\n- The Bernoulli likelihood for a binary outcome $y_i \\in \\{0,1\\}$ given a predicted probability $p_i$ is $L_i = p_i^{y_i} (1-p_i)^{1-y_i}$, with the log-likelihood $\\ell_i = y_i \\log(p_i) + (1-y_i) \\log(1-p_i)$.\n- In $K$-fold Cross-Validation (CV), each observation is predicted exactly once by a model trained on the complement of its fold, and the CV log-likelihood is the sum of the log-likelihoods over all validation predictions.\n\nYour task is to compute stacking weights $\\mathbf{w}$ that maximize the total CV log-likelihood of the ensemble predictions under the simplex constraints, and to evaluate the ensemble performance compared to the best single model (the single model among the three with the highest total CV log-likelihood).\n\nOptimization target to be solved by your program:\n- Maximize the concave objective $\\sum_{i=1}^{n} \\left[ y_i \\log\\left(\\sum_{m=1}^{3} w_m p_{im}\\right) + (1-y_i) \\log\\left(1-\\sum_{m=1}^{3} w_m p_{im}\\right) \\right]$ subject to $\\sum_{m=1}^{3} w_m = 1$ and $w_m \\ge 0$ for $m=1,2,3$. In implementation, ensure numerical stability by clipping probabilities away from $0$ and $1$ using an $\\varepsilon$-level floor and ceiling; use $\\varepsilon = 10^{-12}$.\n\nTest suite:\n- Three cases are provided. Each case includes the binary outcomes vector $\\mathbf{y}$, the matrix of CV-predicted probabilities $\\mathbf{P} \\in \\mathbb{R}^{n \\times 3}$ (columns correspond to the three models), and a fold assignment vector $\\mathbf{f}$ indicating which fold each observation was in for CV. Note that $\\mathbf{f}$ is provided for completeness; since $\\mathbf{P}$ contains the CV predictions for all observations, the ensemble objective can be computed as a sum over all observations.\n\nCase 1 (balanced outcomes, complementary models, $K=5$, $n=20$):\n$$\n\\mathbf{y}^{(1)} = [0,1,0,1, \\; 0,0,1,1, \\; 0,1,0,1, \\; 0,0,1,1, \\; 0,1,0,1]\n$$\n$$\n\\mathbf{P}_1^{(1)} = [0.15,0.70,0.25,0.80, \\; 0.20,0.30,0.75,0.85, \\; 0.10,0.65,0.35,0.78, \\; 0.22,0.28,0.72,0.88, \\; 0.18,0.69,0.40,0.83]\n$$\n$$\n\\mathbf{P}_2^{(1)} = [0.10,0.60,0.20,0.65, \\; 0.15,0.25,0.70,0.80, \\; 0.12,0.58,0.30,0.70, \\; 0.18,0.35,0.68,0.82, \\; 0.20,0.63,0.32,0.75]\n$$\n$$\n\\mathbf{P}_3^{(1)} = [0.30,0.55,0.40,0.60, \\; 0.28,0.40,0.62,0.70, \\; 0.25,0.50,0.45,0.65, \\; 0.22,0.45,0.60,0.72, \\; 0.26,0.48,0.52,0.68]\n$$\n$$\n\\mathbf{f}^{(1)} = [1,1,1,1, \\; 2,2,2,2, \\; 3,3,3,3, \\; 4,4,4,4, \\; 5,5,5,5]\n$$\n\nCase 2 (one dominant model, $K=5$, $n=15$):\n$$\n\\mathbf{y}^{(2)} = [1,1,0, \\; 0,1,1, \\; 0,0,1, \\; 1,0,1, \\; 0,1,1]\n$$\n$$\n\\mathbf{P}_1^{(2)} = [0.96,0.94,0.08, \\; 0.09,0.97,0.95, \\; 0.07,0.06,0.93, \\; 0.98,0.10,0.96, \\; 0.05,0.97,0.95]\n$$\n$$\n\\mathbf{P}_2^{(2)} = [0.70,0.72,0.30, \\; 0.40,0.75,0.76, \\; 0.35,0.25,0.68, \\; 0.78,0.32,0.74, \\; 0.28,0.77,0.73]\n$$\n$$\n\\mathbf{P}_3^{(2)} = [0.60,0.62,0.65, \\; 0.55,0.58,0.59, \\; 0.52,0.57,0.60, \\; 0.61,0.55,0.62, \\; 0.58,0.59,0.60]\n$$\n$$\n\\mathbf{f}^{(2)} = [1,1,1, \\; 2,2,2, \\; 3,3,3, \\; 4,4,4, \\; 5,5,5]\n$$\n\nCase 3 (two identical models, $K=5$, $n=10$):\n$$\n\\mathbf{y}^{(3)} = [0,1, \\; 0,1, \\; 0,1, \\; 0,1, \\; 0,1]\n$$\n$$\n\\mathbf{P}_1^{(3)} = [0.20,0.80, \\; 0.22,0.78, \\; 0.18,0.82, \\; 0.25,0.75, \\; 0.21,0.79]\n$$\n$$\n\\mathbf{P}_2^{(3)} = [0.20,0.80, \\; 0.22,0.78, \\; 0.18,0.82, \\; 0.25,0.75, \\; 0.21,0.79]\n$$\n$$\n\\mathbf{P}_3^{(3)} = [0.45,0.55, \\; 0.50,0.50, \\; 0.48,0.52, \\; 0.46,0.54, \\; 0.49,0.51]\n$$\n$$\n\\mathbf{f}^{(3)} = [1,1, \\; 2,2, \\; 3,3, \\; 4,4, \\; 5,5]\n$$\n\nProgram requirements:\n- Implement a routine that, for each case, computes the stacking weights $\\mathbf{w}$ by maximizing the total CV log-likelihood of the ensemble predictions subject to the simplex constraints, using a suitable constrained optimizer.\n- Compute the total CV log-likelihood of the ensemble and of each single model; identify the best single model by its total CV log-likelihood, and compute the log-likelihood improvement of the ensemble over the best single model.\n- Numerical stability: clip all probabilities $p$ to $[10^{-12}, 1-10^{-12}]$ before computing logarithms.\n- Output specification: For each case, produce a list $[w_1,w_2,w_3,\\ell_{\\text{ens}},\\ell_{\\text{best}},\\Delta]$, where $\\ell_{\\text{ens}}$ is the total ensemble CV log-likelihood, $\\ell_{\\text{best}}$ is the highest total CV log-likelihood among the three single models, and $\\Delta = \\ell_{\\text{ens}} - \\ell_{\\text{best}}$. All numbers must be floats rounded to $6$ decimal places.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The outer list should contain the three per-case lists described above (e.g., $[[\\dots],[\\dots],[\\dots]]$).\n\nNo physical units are involved; all quantities are dimensionless probabilities and log-likelihoods. Angles are not applicable. Percentages, if any, must be expressed as decimals. The numerical answer must be exactly the final line produced by the program.", "solution": "The problem requires us to find the optimal stacking weights for an ensemble of three logistic models to maximize the total Cross-Validation (CV) log-likelihood. This is a constrained optimization problem.\n\nLet the number of observations be $n$, and the number of models be $M=3$. We are given a vector of binary outcomes $\\mathbf{y} \\in \\{0, 1\\}^n$ and an $n \\times 3$ matrix of CV-predicted probabilities $\\mathbf{P}$, where $P_{im}$ is the probability assigned to outcome $y_i$ by model $m$.\n\nThe ensemble model's prediction for observation $i$ is a weighted average of the base models' predictions:\n$$\np_i(\\mathbf{w}) = \\sum_{m=1}^{3} w_m P_{im}\n$$\nwhere $\\mathbf{w} = (w_1, w_2, w_3)^T$ is the vector of stacking weights. The weights must satisfy the simplex constraints:\n$$\n\\sum_{m=1}^{3} w_m = 1 \\quad \\text{and} \\quad w_m \\ge 0 \\quad \\text{for } m=1, 2, 3\n$$\n\nThe objective is to maximize the total Bernoulli log-likelihood of the ensemble over all $n$ observations. The log-likelihood for a single observation $i$ is given by:\n$$\n\\ell_i(\\mathbf{w}) = y_i \\log(p_i(\\mathbf{w})) + (1-y_i) \\log(1 - p_i(\\mathbf{w}))\n$$\nThe total log-likelihood is the sum over all observations:\n$$\nL(\\mathbf{w}) = \\sum_{i=1}^{n} \\ell_i(\\mathbf{w}) = \\sum_{i=1}^{n} \\left[ y_i \\log\\left(\\sum_{m=1}^{3} w_m P_{im}\\right) + (1-y_i) \\log\\left(1 - \\sum_{m=1}^{3} w_m P_{im}\\right) \\right]\n$$\n\nThis is a convex optimization problem because the objective function $L(\\mathbf{w})$ is concave (as it is a sum of compositions of the concave logarithm function with affine functions of $\\mathbf{w}$), and the constraint set (a standard simplex) is convex. We can solve this by minimizing the negative log-likelihood, $-L(\\mathbf{w})$, subject to the given constraints.\n\nThe solution proceeds as follows:\n1.  **Define the Objective Function:** We implement a function that computes the negative total log-likelihood, $-L(\\mathbf{w})$, for a given weight vector $\\mathbf{w}$, the outcome vector $\\mathbf{y}$, and the prediction matrix $\\mathbf{P}$. To ensure numerical stability, the predicted probabilities $p_i(\\mathbf{w})$ are clipped to the interval $[\\varepsilon, 1-\\varepsilon]$, where $\\varepsilon = 10^{-12}$, before their logarithm is taken. The ensemble predictions for all observations can be computed efficiently using a matrix-vector product, $\\mathbf{p}(\\mathbf{w}) = \\mathbf{P}\\mathbf{w}$.\n\n2.  **Set Up and Solve the Optimization:** We use a numerical optimizer to find the weights $\\mathbf{w}^*$ that minimize the negative log-likelihood. The `scipy.optimize.minimize` function is suitable, with the 'SLSQP' (Sequential Least Squares Programming) method, which can handle both equality and inequality constraints.\n    - The initial guess for the weights can be set to uniform values, $\\mathbf{w}_0 = (1/3, 1/3, 1/3)^T$.\n    - The bounds for each weight are set to $w_m \\ge 0$, which translates to `(0, None)`.\n    - The equality constraint is $\\sum_{m=1}^{3} w_m - 1 = 0$.\n\n3.  **Evaluate Performance:** After finding the optimal weights $\\mathbf{w}^*$, we compute the required performance metrics:\n    - The maximized ensemble log-likelihood, $\\ell_{\\text{ens}} = L(\\mathbf{w}^*)$.\n    - The log-likelihood for each of the three individual models, $\\ell_{\\text{single},m} = \\sum_{i=1}^{n} [y_i \\log(P_{im}) + (1-y_i) \\log(1 - P_{im})]$, for $m \\in \\{1, 2, 3\\}$.\n    - The log-likelihood of the best-performing single model, $\\ell_{\\text{best}} = \\max(\\ell_{\\text{single},1}, \\ell_{\\text{single},2}, \\ell_{\\text{single},3})$.\n    - The improvement of the ensemble over the best single model, $\\Delta = \\ell_{\\text{ens}} - \\ell_{\\text{best}}$.\n\n4.  **Process Test Cases:** This entire procedure is applied to each of the three test cases provided. The final results for each case, consisting of the list $[w_1^*, w_2^*, w_3^*, \\ell_{\\text{ens}}, \\ell_{\\text{best}}, \\Delta]$, are rounded to six decimal places and formatted into the specified output structure. Note that the fold assignment vector $\\mathbf{f}$ is not needed for computation, as the prediction matrix $\\mathbf{P}$ already contains the full set of out-of-fold predictions.", "answer": "[[0.536735,0.463265,0.000000,-6.353363,-6.425316,0.071953],[0.999999,0.000001,0.000000,-1.498118,-1.516584,0.018466],[0.499999,0.500001,0.000000,-4.178351,-4.178351,0.000000]]", "id": "4985108"}]}