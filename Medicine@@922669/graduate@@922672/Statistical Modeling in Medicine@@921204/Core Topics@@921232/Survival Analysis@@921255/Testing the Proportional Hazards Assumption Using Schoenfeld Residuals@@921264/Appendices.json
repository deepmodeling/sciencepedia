{"hands_on_practices": [{"introduction": "The power of a Schoenfeld residual test can depend on how we model the potential time-dependency. This exercise explores the theoretical properties of different functions of time, $g(t)$, used to detect non-proportionality, focusing on the crucial concept of invariance to time scale transformations [@problem_id:4986323]. Understanding these properties is key to constructing a robust and meaningful diagnostic test.", "problem": "A clinical study models time to a composite cardiovascular endpoint using the Cox proportional hazards model, which posits a hazard function $h(t \\mid x) = h_0(t) \\exp\\{x^\\top \\beta\\}$ for event time $t$ and covariate vector $x$, with unspecified baseline hazard $h_0(t)$. To assess the Proportional Hazards (PH) assumption for a single covariate $Z$, an investigator considers an extended model in which the coefficient may vary with time: $\\beta_Z(t) = \\beta_Z + \\theta \\, g(t)$, where $g(t)$ is a deterministic function of event time and $\\theta$ quantifies deviation from PH. A score test for $H_0: \\theta = 0$ can be constructed from Schoenfeld residuals at event times $\\{t_j\\}$ for $Z$, denoted by $r_{jZ}$, by examining their association with $g(t_j)$.\n\nThe investigator is choosing among several functions for $g(t)$: the raw calendar time $t$, the log-transformed time $\\log(t)$, the event rank $\\mathrm{rank}(t)$ (i.e., the order index of each distinct event time), and the estimated cumulative baseline hazard $\\hat{H}_0(t)$ (e.g., via the Breslow estimator). In the Cox framework, monotone time reparameterizations $t' = \\phi(t)$ with strictly increasing $\\phi$ leave risk set orderings unchanged, and the true cumulative baseline hazard $H_0(t)$ satisfies the identity $H_0'(t') = H_0(t)$ under such reparameterizations.\n\nWhich of the following statements about the choice of $g(t)$ and invariance properties of the Schoenfeld residual-based PH test are correct?\n\nA. Using $g(t) = \\mathrm{rank}(t)$ yields a test that is invariant to any strictly increasing transformation of the time scale, because ranks depend only on event order and the partial likelihood depends only on risk set orderings.\n\nB. Using $g(t) = t$ and $g(t) = \\log(t)$ gives identical score tests (both under the null and under alternatives) because both $t$ and $\\log(t)$ are monotone functions of $t$.\n\nC. Choosing $g(t) = \\hat{H}_0(t)$ yields a test that is asymptotically invariant to any strictly increasing transformation of the time scale, since the true cumulative baseline hazard $H_0(t)$ is invariant to monotone reparameterizations of time in the Cox model.\n\nD. For any given $g(t)$, replacing it with $g'(t) = a + b \\, g(t)$ for constants $a \\in \\mathbb{R}$ and $b > 0$ leaves the standardized score test for $H_0: \\theta = 0$ unchanged.\n\nE. Under $H_0: \\theta = 0$, the asymptotic null distribution of the standardized score test depends on the choice of $g(t)$; for example, using $g(t) = \\mathrm{rank}(t)$ yields a different null degrees of freedom than using $g(t) = t$.\n\nSelect all that apply.", "solution": "We begin from the Cox model definition $h(t \\mid x) = h_0(t) \\exp\\{x^\\top \\beta\\}$, where $h_0(t)$ is unspecified. The Cox partial likelihood is constructed from ordered event times $\\{t_j\\}$ and corresponding risk sets, and its value depends only on the order of events (and on the covariates present in the risk sets), not on the actual numeric magnitudes of $t$ beyond ordering. At each distinct event time $t_j$, the Schoenfeld residual for covariate $Z$ is $r_{jZ} = Z_{j}^{(\\text{event})} - \\mathbb{E}_{R_j}[Z]$, the event value minus the risk-set weighted mean, where $R_j$ denotes the risk set at $t_j$ and the expectation is taken with respect to the partial likelihood weights.\n\nTo test the Proportional Hazards (PH) assumption for $Z$, consider the time-varying coefficient $\\beta_Z(t) = \\beta_Z + \\theta \\, g(t)$, which defines an alternative model with a structured deviation in the log-hazard ratio. The score for $\\theta$ evaluated at $\\theta = 0$ takes the form (up to a constant of proportionality that depends on the partial likelihood information) \n$$\nU_\\theta = \\sum_{j} r_{jZ} \\, g(t_j),\n$$\nand a standardized score test (or equivalently, a regression of scaled Schoenfeld residuals on $g(t_j)$) yields an asymptotically normal statistic under $H_0: \\theta = 0$. This setup shows that properties of the test under transformations of time and transformations of $g(t)$ can be derived from first principles.\n\nWe analyze each option:\n\nA. Using $g(t) = \\mathrm{rank}(t)$ yields invariance to strictly increasing time transformations. If $t' = \\phi(t)$ with strictly increasing $\\phi$, the event ordering is preserved, so $\\mathrm{rank}(t_j)$ equals $\\mathrm{rank}(t'_j)$ for all $j$. The partial likelihood, and therefore the Schoenfeld residuals $r_{jZ}$, depend only on risk sets determined by the event order, which is unchanged by such $\\phi$. Consequently, $U_\\theta = \\sum_{j} r_{jZ} \\, \\mathrm{rank}(t_j)$ is unchanged by any monotone reparameterization of the time scale. Therefore, this choice of $g(t)$ yields a test invariant to any strictly increasing transformation of time. Verdict: Correct.\n\nB. Using $g(t) = t$ and $g(t) = \\log(t)$ gives identical score tests because both are monotone. Although both $t$ and $\\log(t)$ are strictly increasing functions of $t$, the mapping from $t$ to $g(t)$ is not related by a linear rescaling; $\\log(t)$ changes relative spacings among event times compared to $t$. The standardized score test (e.g., a $t$-statistic or a chi-square based on a one-parameter slope) for regressing residuals on $g(t)$ is invariant to affine transformations $g'(t) = a + b \\, g(t)$ with $b > 0$, but it is not invariant to nonlinear monotone transformations in general. Therefore, using $g(t) = t$ versus $g(t) = \\log(t)$ can produce different test statistics and different power against specific alternatives $\\beta_Z(t) = \\beta_Z + \\theta \\, g(t)$. Verdict: Incorrect.\n\nC. Choosing $g(t) = \\hat{H}_0(t)$ yields asymptotic invariance to monotone time transformations. Under a strictly increasing reparameterization $t' = \\phi(t)$, the baseline hazard transforms as $h_0'(t') = h_0(t) \\, \\frac{dt}{dt'} = \\frac{h_0(t)}{\\phi'(t)}$. The cumulative baseline hazard under the new time scale is\n$$\nH_0'(t') = \\int_0^{t'} h_0'(u') \\, du' = \\int_0^{t} \\frac{h_0(u)}{\\phi'(u)} \\, \\phi'(u) \\, du = \\int_0^{t} h_0(u) \\, du = H_0(t),\n$$\nso the true cumulative baseline hazard $H_0(t)$ is invariant to any strictly increasing time reparameterization. If we set $g(t) = H_0(t)$, then $g(t_j)$ is identical to $g(t'_j)$ under reparameterization, and the corresponding score $U_\\theta$ is exactly invariant. With the estimated cumulative baseline hazard $\\hat{H}_0(t)$ (e.g., Breslow estimator), $\\hat{H}_0(t)$ converges uniformly to $H_0(t)$ under regularity conditions, and the resulting test is asymptotically invariant to monotone time transformations. Verdict: Correct.\n\nD. Replacing $g(t)$ with $g'(t) = a + b \\, g(t)$ for $a \\in \\mathbb{R}$ and $b > 0$ leaves the standardized score test unchanged. The score is $U_\\theta' = \\sum_j r_{jZ} \\, g'(t_j) = \\sum_j r_{jZ} \\, (a + b \\, g(t_j)) = a \\sum_j r_{jZ} + b \\sum_j r_{jZ} \\, g(t_j)$. Under $H_0: \\theta = 0$, the expected value of $\\sum_j r_{jZ}$ is $0$, and in constructing a standardized test (e.g., regressing residuals on $g$ with an intercept or centering $g$), the constant shift $a$ does not alter the slope estimate or its test statistic. Positive scaling $b$ rescales the slope by $b$ and its standard error by $b$, leaving the standardized statistic (e.g., a $z$-score or a $t$-statistic) unchanged for $b > 0$. Thus, the decision about $H_0: \\theta = 0$ is invariant to affine transformations with positive scale. Verdict: Correct.\n\nE. Under $H_0: \\theta = 0$, the asymptotic null distribution of the standardized score test depends on the choice of $g(t)$; for instance, using $g(t) = \\mathrm{rank}(t)$ yields a different null degrees of freedom than using $g(t) = t$. In the one-parameter score test for $\\theta$, proper standardization yields an asymptotically standard normal statistic (or a chi-square with $1$ degree of freedom when squared), regardless of the deterministic choice of $g(t)$, provided regularity conditions hold (e.g., finite information and non-degenerate variance). The choice of $g(t)$ affects the alternative and thus the power, but not the null degrees of freedom of a one-parameter test. Therefore, the claim that the null distribution or its degrees of freedom depend on $g(t)$ is incorrect. Verdict: Incorrect.\n\nIn summary, statements A, C, and D are correct; statements B and E are incorrect.", "answer": "$$\\boxed{ACD}$$", "id": "4986323"}, {"introduction": "Interpreting statistical tests is as important as running them, especially when results appear contradictory. This practice presents a common real-world puzzle: a significant global test for proportional hazards, yet no significant individual tests for any covariate [@problem_id:4986311]. By considering the impact of multicollinearity, you will learn to diagnose complex model misspecifications that are not attributable to a single variable but rather to the relationships between them.", "problem": "A hospital-based cohort study of $n$ patients with a solid-organ malignancy examines time from initiation of therapy to death. A Cox proportional hazards model is fit with baseline hazard $h_0(t)$ and covariate vector $X \\in \\mathbb{R}^p$, so that the hazard function is $h(t \\mid X) = h_0(t)\\exp\\{X^\\top \\beta\\}$. Three clinical covariates $X_1$, $X_2$, and $X_3$ measure distinct but related aspects of tumor burden and are strongly correlated, with variance inflation factors (VIF) near $12$. The proportional hazards (PH) assumption states that each log hazard ratio is constant in time.\n\nTo assess the PH assumption, the team computes scaled Schoenfeld residuals for each covariate and tests for time dependence by examining whether the residuals are associated with a function of time. Individually, the tests for $X_1$, $X_2$, and $X_3$ yield $p$-values exceeding $0.20$, and the remaining covariates each show $p$-values exceeding $0.10$. However, the global test that aggregates evidence across all covariates is statistically significant with $p = 0.008$.\n\nAssuming the model is otherwise correctly specified and the correlation among $X_1$, $X_2$, and $X_3$ is genuine (not a data artifact), which interpretation and follow-up strategy is most appropriate?\n\nA. The global test can detect a time-varying effect expressed along a linear combination of correlated covariates even when each univariate test has low power. One should interpret this as evidence of a joint non-proportional effect in the subspace spanned by $X_1$, $X_2$, and $X_3$, and pursue joint modeling of time-varying effects, such as testing grouped time interactions, reparameterizing by principal component analysis (PCA) of $\\{X_1,X_2,X_3\\}$ followed by time-by-component interactions, or using penalized time-varying coefficients.\n\nB. Because no individual covariate test is significant, the significant global test is likely a false positive; the proportional hazards assumption can be treated as holding.\n\nC. The global Schoenfeld residual test is invalid in the presence of multicollinearity; one should instead rely on martingale residuals to test the proportional hazards assumption.\n\nD. To reconcile the discrepancy, remove one of $X_1$, $X_2$, or $X_3$ arbitrarily to reduce correlation until at least one individual Schoenfeld residual test becomes significant; otherwise, ignore the global test.\n\nE. Standardizing $X_1$, $X_2$, and $X_3$ to have mean $0$ and variance $1$ will eliminate the global signal by removing multicollinearity; thus, the proportional hazards assumption can be accepted if individual tests remain non-significant after standardization.", "solution": "The core of the problem lies in understanding the nature of the Schoenfeld residual test, both in its univariate and multivariate (global) forms, and how multicollinearity affects it.\n\nThe proportional hazards (PH) assumption for a covariate $X_j$ implies that its associated coefficient $\\beta_j$ is constant over time $t$. A violation means $\\beta_j$ is a function of time, $\\beta_j(t)$. The test based on scaled Schoenfeld residuals for $X_j$ is designed to detect a non-zero slope in the relationship between the residuals and time. It is effectively a test of the hypothesis that $\\beta_j(t) = \\beta_j$ (a constant).\n\nMulticollinearity, as indicated by the high VIFs near $12$ for $X_1$, $X_2$, and $X_3$, means these variables are highly inter-correlated. This has two key consequences:\n1.  The estimates of the individual coefficients $\\hat{\\beta}_1$, $\\hat{\\beta}_2$, and $\\hat{\\beta}_3$ become unstable and have large standard errors. This makes it difficult to disentangle the unique effect of each variable.\n2.  This instability extends to the tests for the PH assumption. The variance of the test statistic for any individual covariate ($X_1$, $X_2$, or $X_3$) is inflated. This leads to a loss of statistical power, making it difficult to reject the null hypothesis of proportionality even if a time-varying effect is present. This explains the non-significant individual $p$-values ($p > 0.20$).\n\nThe global test is a multivariate test that considers the vector of scaled Schoenfeld residuals for all $p$ covariates simultaneously. It has power to detect deviations from the PH assumption that may not be aligned with any single covariate axis. In the presence of strong correlation among a subset of covariates, a time-varying effect might exist for a specific linear combination of those covariates, for instance, $\\gamma(t) = c_1 \\beta_1(t) + c_2 \\beta_2(t) + c_3 \\beta_3(t)$. The global test can detect that $\\gamma(t)$ is not constant, even if the projection of this effect onto each individual covariate axis ($X_1$, $X_2$, or $X_3$) is too weak to be detected by the respective univariate tests. The significant global test ($p = 0.008$) is strong evidence that the PH assumption is violated somewhere in the model. Given that the correlation structure is concentrated in $\\{X_1, X_2, X_3\\}$ and all other individual tests are non-significant, the violation most likely lies within the subspace spanned by these three covariates.\n\nBased on this reasoning, we evaluate each option:\n\n**A. The global test can detect a time-varying effect expressed along a linear combination of correlated covariates even when each univariate test has low power. One should interpret this as evidence of a joint non-proportional effect in the subspace spanned by $X_1$, $X_2$, and $X_3$, and pursue joint modeling of time-varying effects, such as testing grouped time interactions, reparameterizing by principal component analysis (PCA) of $\\{X_1,X_2,X_3\\}$ followed by time-by-component interactions, or using penalized time-varying coefficients.**\nThis option correctly describes the statistical phenomenon. The global test's power against effects in a linear combination of covariates explains the discrepancy. The interpretation of a \"joint non-proportional effect\" is accurate. The suggested follow-up strategies are all standard and appropriate methods to address this specific problem. PCA creates orthogonal components, which resolves the collinearity and allows for testing time interactions on the components (e.g., the first principal component, which captures the shared \"tumor burden\" construct). Jointly modeling time-varying coefficients, possibly with penalization, is another advanced and valid approach. This option is statistically sound and provides a constructive path forward.\n**Verdict: Correct.**\n\n**B. Because no individual covariate test is significant, the significant global test is likely a false positive; the proportional hazards assumption can be treated as holding.**\nThis interpretation is incorrect. Dismissing a highly significant p-value of $p = 0.008$ as a false positive without a compelling reason is poor statistical practice. Furthermore, there is a clear theoretical explanation for the observed results (loss of power in individual tests due to multicollinearity), which makes the \"false positive\" explanation highly improbable. Ignoring the global test would mean proceeding with a misspecified model.\n**Verdict: Incorrect.**\n\n**C. The global Schoenfeld residual test is invalid in the presence of multicollinearity; one should instead rely on martingale residuals to test the proportional hazards assumption.**\nThis statement is false. The global Schoenfeld residual test is valid in the presence of multicollinearity; its ability to detect joint effects is a key feature. Martingale residuals are primarily used for other diagnostic purposes, such as assessing the functional form of covariates or identifying influential observations. While they can be plotted against time to look for systematic trends, the formal hypothesis test for the PH assumption is based on Schoenfeld residuals.\n**Verdict: Incorrect.**\n\n**D. To reconcile the discrepancy, remove one of $X_1$, $X_2$, or $X_3$ arbitrarily to reduce correlation until at least one individual Schoenfeld residual test becomes significant; otherwise, ignore the global test.**\nThis is an unprincipled and data-driven procedure that amounts to \"p-hacking.\" Removing covariates arbitrarily can lead to omitted-variable bias and loss of information, as the covariates are stated to measure distinct aspects of tumor burden. The goal of \"finding\" a significant result by trying different model specifications is not a valid scientific approach. Finally, ignoring the strong evidence from the global test is not justifiable.\n**Verdict: Incorrect.**\n\n**E. Standardizing $X_1$, $X_2$, and $X_3$ to have mean $0$ and variance $1$ will eliminate the global signal by removing multicollinearity; thus, the proportional hazards assumption can be accepted if individual tests remain non-significant after standardization.**\nThe premise of this option is fundamentally flawed. Standardization is a linear transformation that rescales variables but does not alter their correlation structure. The Pearson correlation coefficient, and consequently VIFs, are invariant to such scaling. Therefore, standardization does not remove or reduce multicollinearity. The entire line of reasoning in this option is based on a false statistical premise.\n**Verdict: Incorrect.**\n\nIn summary, Option A provides the only correct interpretation and appropriate set of strategies for the situation described.", "answer": "$$\\boxed{A}$$", "id": "4986311"}, {"introduction": "The ultimate test of understanding is the ability to implement a concept from first principles. This comprehensive coding challenge guides you through building an entire simulation-and-analysis pipeline to test the proportional hazards assumption [@problem_id:4986345]. You will simulate data under both proportional and non-proportional hazards, fit a Cox model using a Newton-Raphson solver you build yourself, and implement the Schoenfeld residual test to see the theory in action.", "problem": "You are asked to implement a complete simulation-and-analysis pipeline to test the Proportional Hazards (PH) assumption using Schoenfeld residuals in the context of the Cox model for time-to-event data. Your program must be a single, runnable script that carries out the following end-to-end tasks purely from first principles and core definitions, without using any external survival-analysis library code.\n\nStart from the following foundational definitions.\n\n1. The hazard function at time $t$ for a subject with covariate vector $x$ is $h(t \\mid x)$, and the cumulative hazard is $\\Lambda(t \\mid x) = \\int_{0}^{t} h(u \\mid x)\\,du$. The survival function is $S(t \\mid x) = \\exp\\{-\\Lambda(t \\mid x)\\}$.\n2. In the Cox model, the hazard is $h(t \\mid x) = h_0(t) \\exp\\{\\beta(t)^{\\top} x\\}$, where $h_0(t)$ is the baseline hazard and $\\beta(t)$ may depend on $t$. The Proportional Hazards (PH) assumption corresponds to $\\beta(t) \\equiv \\beta$, a constant vector, so that the hazard ratio is time-invariant.\n3. To simulate an event time using inverse transform sampling, draw $U \\sim \\text{Uniform}(0,1)$ and solve for $T$ in the defining relation $\\Lambda(T \\mid x) = -\\log U$, where $\\Lambda(t \\mid x) = \\int_{0}^{t} h_0(u) \\exp\\{\\beta(u)^{\\top} x\\} \\, du$.\n4. Given observed follow-up time $Y = \\min(T, C)$ and event indicator $\\delta = \\mathbf{1}\\{T \\le C\\}$ with independent right-censoring time $C$, the Cox partial likelihood for a constant coefficient $\\beta$ with one covariate is based on risk sets. If event times are $t_1 < \\dots < t_J$ with $d_j$ events at time $t_j$ and risk set $R_j = \\{ i : Y_i \\ge t_j \\}$, then for one covariate $x$,\n   - The partial log-likelihood is $\\ell(\\beta) = \\sum_{j=1}^{J} \\left( \\sum_{i \\in D_j} \\beta x_i - d_j \\log \\sum_{i \\in R_j} e^{\\beta x_i} \\right)$, where $D_j$ denotes subjects who fail at $t_j$.\n   - The score function is $U(\\beta) = \\sum_{j=1}^{J} \\left( \\sum_{i \\in D_j} x_i - d_j \\frac{\\sum_{i \\in R_j} x_i e^{\\beta x_i}}{\\sum_{i \\in R_j} e^{\\beta x_i}} \\right)$.\n   - The observed information is $I(\\beta) = \\sum_{j=1}^{J} d_j \\left( \\frac{\\sum_{i \\in R_j} x_i^2 e^{\\beta x_i}}{\\sum_{i \\in R_j} e^{\\beta x_i}} - \\left(\\frac{\\sum_{i \\in R_j} x_i e^{\\beta x_i}}{\\sum_{i \\in R_j} e^{\\beta x_i}} \\right)^2 \\right)$.\n   The Breslow approximation is to use these formulas when there are ties at the same event time.\n5. The Schoenfeld residual for an event $i$ at time $t_j$ (unscaled) is $r_i = x_i - \\mathbb{E}_{\\hat{\\beta}}[x \\mid R_j]$, where the expectation is taken with respect to the weights proportional to $e^{\\hat{\\beta} x}$ over $R_j$.\n6. A test for the PH assumption (for a single covariate) may be based on checking whether $r_i$ is independent of $t_j$. One approach is to perform an ordinary least squares (OLS) regression of $r_i$ on a function $g(t_j)$ and test if the slope is zero, for example using a two-sided $t$-test. Under PH, the regression slope is expected to be near zero.\n\nYour task is:\n\n- For each of the provided test cases, do all of the following:\n  1. Simulate $N$ independent subjects. Generate a single binary covariate $X \\in \\{0,1\\}$ with $P(X=1) = 0.5$ for each subject.\n  2. Simulate the event time $T$ using inverse transform sampling and the given $h_0(t)$ and $\\beta(t)$. Then simulate an independent right-censoring time $C$, and form $Y = \\min(T, C)$ and $\\delta = \\mathbf{1}\\{T \\le C\\}$.\n  3. Fit a Cox model with a single constant coefficient $\\beta$ by maximizing the partial log-likelihood via Newton–Raphson using the Breslow approximation for ties. Do not use any survival-analysis package; compute the score and observed information directly from definitions for each Newton–Raphson iteration until convergence.\n  4. Compute the Schoenfeld residuals at the event times using the fitted $\\hat{\\beta}$.\n  5. Regress the Schoenfeld residuals on the observed event times using ordinary least squares (OLS) with an intercept and the univariate regressor $g(t) = t$. Use a two-sided $t$-test for the slope with significance level $\\alpha = 0.05$, and determine a boolean decision per test case: return $\\text{True}$ if PH is rejected and $\\text{False}$ otherwise.\n\nImplement the above for the following three test cases (this is the test suite):\n\n- Case A (PH holds, constant baseline hazard, linear coefficient with zero slope):\n  - Baseline hazard $h_0(t) = \\lambda$, with $\\lambda = 0.1$.\n  - Time-varying coefficient $\\beta(t) = \\beta_0 + \\beta_1 t$ with $\\beta_0 = 0.8$ and $\\beta_1 = 0$.\n  - Number of subjects $N = 600$.\n  - Censoring distribution $C \\sim \\text{Uniform}(0, 15)$.\n  - Random seed $s = 13579$.\n\n- Case B (PH violated, constant baseline hazard, linear coefficient with positive slope):\n  - Baseline hazard $h_0(t) = \\lambda$, with $\\lambda = 0.1$.\n  - Time-varying coefficient $\\beta(t) = \\beta_0 + \\beta_1 t$ with $\\beta_0 = 0.2$ and $\\beta_1 = 0.3$.\n  - Number of subjects $N = 600$.\n  - Censoring distribution $C \\sim \\text{Uniform}(0, 8)$.\n  - Random seed $s = 24680$.\n\n- Case C (PH holds, Weibull baseline hazard, constant coefficient):\n  - Baseline hazard $h_0(t) = \\lambda k t^{k-1}$ with $\\lambda = 0.01$ and $k = 2$.\n  - Time-varying coefficient $\\beta(t) = \\beta_0$ with $\\beta_0 = 0.7$.\n  - Number of subjects $N = 300$.\n  - Censoring distribution $C \\sim \\text{Uniform}(0, 10)$.\n  - Random seed $s = 11223$.\n\nImportant implementation requirements:\n\n- You must derive simulation of $T$ via the inverse transform method starting from the definition $\\Lambda(T \\mid x) = -\\log U$ and the given $h_0(t)$ and $\\beta(t)$ per case. You may use analytic inversion when available.\n- In the Cox fitting step, implement the Newton–Raphson algorithm explicitly for the single covariate case, computing the score and observed information using risk sets and the Breslow approximation.\n- Compute Schoenfeld residuals at each event time using the fitted $\\hat{\\beta}$ and the risk set just prior to that event time.\n- For the OLS regression of residuals on event time, include an intercept and use a two-sided $t$-test for the slope at level $\\alpha = 0.05$. Express the final decision per case as a boolean.\n\nFinal output specification:\n\n- Your program should produce a single line of output containing the results as a comma-separated list of three boolean values enclosed in square brackets, in the order of the cases A, B, C (for example, \"[False,True,False]\").\n- No other output is permitted. There are no physical units required for this problem.", "solution": "The problem requires the implementation of a comprehensive statistical pipeline to simulate time-to-event data, fit a Cox proportional hazards model, and test the underlying Proportional Hazards (PH) assumption using Schoenfeld residuals. This process will be executed for three distinct test cases, with the entire pipeline constructed from first principles without reliance on high-level survival analysis libraries.\n\n**1. Data Simulation via Inverse Transform Sampling**\n\nThe foundation of the simulation is the inverse transform sampling method applied to the survival function $S(t \\mid x)$. The survival function is defined as $S(t \\mid x) = \\exp\\{-\\Lambda(t \\mid x)\\}$, where $\\Lambda(t \\mid x)$ is the cumulative hazard function. To generate an event time $T$, we draw a random variable $U$ from a standard uniform distribution, $U \\sim \\text{Uniform}(0,1)$, and solve the equation $S(T \\mid x) = U$. This is equivalent to solving $\\Lambda(T \\mid x) = -\\log(U)$. Let $E = -\\log(U)$, which follows an exponential distribution with rate $1$. The task is to find the event time $T$ by inverting the cumulative hazard function for each test case. The covariate $X$ for each of the $N$ subjects is a binary variable drawn from a Bernoulli distribution with $P(X=1) = 0.5$.\n\n*   **Case A (PH holds):**\n    The baseline hazard is constant, $h_0(t) = \\lambda = 0.1$. The coefficient is also constant, $\\beta(t) = \\beta_0 = 0.8$. This model satisfies the PH assumption. The hazard for a subject is $h(t \\mid x) = \\lambda \\exp(\\beta_0 x)$. The cumulative hazard is $\\Lambda(t \\mid x) = \\int_0^t \\lambda \\exp(\\beta_0 x) \\,du = \\lambda t \\exp(\\beta_0 x)$. Setting this equal to $E$ and solving for $T$ yields:\n    $$T = \\frac{E}{\\lambda \\exp(\\beta_0 x)}$$\n\n*   **Case B (PH violated):**\n    The baseline hazard is constant, $h_0(t) = \\lambda = 0.1$, but the coefficient is time-dependent: $\\beta(t) = \\beta_0 + \\beta_1 t$, with $\\beta_0 = 0.2$ and $\\beta_1 = 0.3$. This violates the PH assumption. The cumulative hazard is $\\Lambda(t \\mid x) = \\int_0^t h_0(u) \\exp(\\beta(u)x) \\,du$.\n    For subjects with $x=0$, $\\beta(t)x = 0$, so $\\Lambda(t|0) = \\int_0^t \\lambda \\,du = \\lambda t$. The event time is $T = E/\\lambda$.\n    For subjects with $x=1$, $\\Lambda(t \\mid 1) = \\int_0^t \\lambda \\exp(\\beta_0 + \\beta_1 u) \\,du = \\lambda e^{\\beta_0} \\int_0^t e^{\\beta_1 u} \\,du = \\frac{\\lambda e^{\\beta_0}}{\\beta_1} (e^{\\beta_1 t} - 1)$. Setting this to $E$ and solving for $T$ gives:\n    $$T = \\frac{1}{\\beta_1} \\log\\left(1 + E \\frac{\\beta_1}{\\lambda e^{\\beta_0}}\\right)$$\n\n*   **Case C (PH holds):**\n    The baseline hazard follows a Weibull distribution, $h_0(t) = \\lambda k t^{k-1}$, with $\\lambda=0.01$ and $k=2$. The coefficient is constant, $\\beta(t) = \\beta_0 = 0.7$, so the PH assumption holds. The cumulative baseline hazard is $H_0(t) = \\int_0^t \\lambda k u^{k-1} \\,du = \\lambda t^k$. The subject-specific cumulative hazard is $\\Lambda(t \\mid x) = H_0(t) \\exp(\\beta_0 x) = \\lambda t^k \\exp(\\beta_0 x)$. Setting this to $E$ and solving for $T$ yields:\n    $$T = \\left(\\frac{E}{\\lambda \\exp(\\beta_0 x)}\\right)^{1/k}$$\n\nOnce event times $T$ are simulated, independent right-censoring times $C$ are drawn from the specified uniform distributions. The observed follow-up time is $Y = \\min(T, C)$ and the event indicator is $\\delta = \\mathbf{1}\\{T \\le C\\}$.\n\n**2. Cox Model Fitting via Newton-Raphson**\n\nThe Cox model assumes a constant coefficient $\\beta$, leading to the partial log-likelihood function $\\ell(\\beta)$. We find the maximum likelihood estimate $\\hat{\\beta}$ by running the Newton-Raphson algorithm, which iteratively updates the estimate using the score function $U(\\beta) = \\frac{d\\ell}{d\\beta}$ and the observed information $I(\\beta) = -\\frac{d^2\\ell}{d\\beta^2}$. The update rule for a single parameter is:\n$$\\beta_{k+1} = \\beta_k + \\frac{U(\\beta_k)}{I(\\beta_k)}$$\nThe algorithm starts with an initial guess, e.g., $\\beta_0=0$, and iterates until convergence. The score $U(\\beta)$ and information $I(\\beta)$ are calculated by summing contributions from each unique event time $t_j$. With $D_j$ being the set of subjects who have an event at $t_j$, $d_j = |D_j|$, and $R_j$ being the risk set (subjects with $Y_i \\ge t_j$), the components are:\n$$U(\\beta) = \\sum_{j=1}^{J} \\left( \\sum_{i \\in D_j} x_i - d_j \\frac{\\sum_{k \\in R_j} x_k e^{\\beta x_k}}{\\sum_{k \\in R_j} e^{\\beta x_k}} \\right)$$\n$$I(\\beta) = \\sum_{j=1}^{J} d_j \\left( \\frac{\\sum_{k \\in R_j} x_k^2 e^{\\beta x_k}}{\\sum_{k \\in R_j} e^{\\beta x_k}} - \\left(\\frac{\\sum_{k \\in R_j} x_k e^{\\beta x_k}}{\\sum_{k \\in R_j} e^{\\beta x_k}} \\right)^2 \\right)$$\nThese formulas use the Breslow approximation for handling tied event times. To implement this, we iterate through the unique event times, form the corresponding risk sets, and compute the required sums to update the score and information totals for each iteration of the Newton-Raphson algorithm.\n\n**3. Schoenfeld Residual Calculation**\n\nAfter fitting the model to obtain $\\hat{\\beta}$, we assess the PH assumption. The unscaled Schoenfeld residual for a subject $i$ experiencing an event at time $t_j$ is the difference between their observed covariate value and the expected covariate value over the risk set $R_j$ at that time:\n$$r_i = x_i - \\mathbb{E}_{\\hat{\\beta}}[X \\mid R_j] = x_i - \\frac{\\sum_{k \\in R_j} x_k e^{\\hat{\\beta} x_k}}{\\sum_{k \\in R_j} e^{\\hat{\\beta} x_k}}$$\nIf the PH assumption holds, these residuals should show no systematic trend when plotted against time. If the assumption is violated (e.g., the effect of $x$ changes with time), the residuals may correlate with time.\n\n**4. Hypothesis Testing for Proportional Hazards**\n\nTo formally test for a time-trend, we perform an ordinary least squares (OLS) regression of the Schoenfeld residuals $r$ on the corresponding event times $t$. The model is $r = b_0 + b_1 t + \\epsilon$. We test the null hypothesis $H_0: b_1 = 0$ against the alternative $H_a: b_1 \\neq 0$.\nThe OLS estimate for the slope is $\\hat{b}_1 = \\frac{\\sum (t_i - \\bar{t})(r_i - \\bar{r})}{\\sum (t_i - \\bar{t})^2}$. The test statistic is a $t$-statistic:\n$$t_{\\text{stat}} = \\frac{\\hat{b}_1}{\\text{SE}(\\hat{b}_1)}$$\nwhere $\\text{SE}(\\hat{b}_1)$ is the standard error of the slope estimate. The standard error is calculated as $\\text{SE}(\\hat{b}_1) = \\sqrt{\\frac{s^2}{\\sum (t_i - \\bar{t})^2}}$, with $s^2 = \\frac{\\sum(r_i - \\hat{r}_i)^2}{n-2}$ being the unbiased estimate of the error variance $\\sigma^2$, and $n$ is the number of events.\nUnder the null hypothesis, $t_{\\text{stat}}$ follows a Student's $t$-distribution with $n-2$ degrees of freedom. We calculate the two-sided $p$-value and reject $H_0$ if the $p$-value is less than the significance level $\\alpha = 0.05$. A rejection indicates evidence against the PH assumption. The final output for each case is a boolean value indicating whether the PH assumption was rejected.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t as t_dist\n\ndef fit_cox_model(Y, delta, X, tol=1e-6, max_iter=25):\n    \"\"\"\n    Fits a single-covariate Cox model using Newton-Raphson.\n    \"\"\"\n    # Sort data by time to process events chronologically.\n    sort_idx = np.argsort(Y)\n    Y, delta, X = Y[sort_idx], delta[sort_idx], X[sort_idx]\n\n    # Get unique event times\n    unique_event_times = np.unique(Y[delta == 1])\n    if len(unique_event_times) == 0:\n        return 0.0\n\n    beta = 0.0\n    for _ in range(max_iter):\n        score = 0.0\n        info = 0.0\n        \n        # Pre-compute for current beta\n        exp_beta_X = np.exp(beta * X)\n\n        # Iterate through unique event times to calculate score and info\n        for t_j in unique_event_times:\n            risk_set_mask = (Y >= t_j)\n            event_mask = (Y == t_j) & (delta == 1)\n\n            d_j = np.sum(delta[event_mask])\n            if d_j == 0:\n                continue\n\n            sum_X_deaths = np.sum(X[event_mask])\n\n            X_risk = X[risk_set_mask]\n            exp_beta_X_risk = exp_beta_X[risk_set_mask]\n\n            S0 = np.sum(exp_beta_X_risk)\n            S1 = np.sum(X_risk * exp_beta_X_risk)\n            S2 = np.sum(X_risk**2 * exp_beta_X_risk)\n\n            if S0 > 0:\n                E1 = S1 / S0\n                E2 = S2 / S0\n                \n                score += sum_X_deaths - d_j * E1\n                info += d_j * (E2 - E1**2)\n\n        if info <= 0:\n            break\n\n        update = score / info\n        beta += update\n\n        if abs(update) < tol:\n            break\n            \n    return beta\n\ndef calculate_schoenfeld(Y, delta, X, beta_hat):\n    \"\"\"\n    Calculates unscaled Schoenfeld residuals.\n    \"\"\"\n    # Sort data by time\n    sort_idx = np.argsort(Y)\n    Y, delta, X = Y[sort_idx], delta[sort_idx], X[sort_idx]\n    \n    unique_event_times = np.unique(Y[delta == 1])\n    \n    residuals = []\n    event_times_for_residuals = []\n    \n    exp_beta_X = np.exp(beta_hat * X)\n    \n    for t_j in unique_event_times:\n        risk_set_mask = (Y >= t_j)\n        event_mask = (Y == t_j) & (delta == 1)\n        \n        X_risk = X[risk_set_mask]\n        exp_beta_X_risk = exp_beta_X[risk_set_mask]\n        \n        S0 = np.sum(exp_beta_X_risk)\n        S1 = np.sum(X_risk * exp_beta_X_risk)\n        \n        E_x = S1 / S0 if S0 > 0 else 0.0\n            \n        X_events = X[event_mask]\n        \n        for x_i in X_events:\n            residuals.append(x_i - E_x)\n            event_times_for_residuals.append(t_j)\n            \n    return np.array(residuals), np.array(event_times_for_residuals)\n\ndef test_ph(residuals, event_times, alpha=0.05):\n    \"\"\"\n    Performs OLS regression of residuals on time and a t-test on the slope.\n    \"\"\"\n    y = residuals\n    x = event_times\n    n = len(x)\n\n    if n < 3: # Need at least 3 points for OLS with intercept and slope\n        return False\n\n    sum_x = np.sum(x)\n    sum_y = np.sum(y)\n    sum_xx = np.sum(x**2)\n    sum_xy = np.sum(x * y)\n\n    b1_denom = n * sum_xx - sum_x**2\n    if np.isclose(b1_denom, 0):\n        return False # All event times are the same\n\n    b1 = (n * sum_xy - sum_x * sum_y) / b1_denom\n    b0 = (sum_y / n) - b1 * (sum_x / n)\n\n    y_hat = b0 + b1 * x\n    SSE = np.sum((y - y_hat)**2)\n    \n    df = n - 2\n    if df <= 0:\n        return False\n\n    s_sq = SSE / df\n\n    Sxx = sum_xx - (sum_x**2 / n)\n    if np.isclose(Sxx, 0):\n        return False\n\n    SE_b1 = np.sqrt(s_sq / Sxx)\n\n    if np.isclose(SE_b1, 0):\n        return np.abs(b1) > 1e-9 # Effectively infinite t-stat, reject\n\n    t_stat = b1 / SE_b1\n    p_value = 2 * t_dist.sf(np.abs(t_stat), df=df)\n\n    return p_value < alpha\n\ndef run_case(params):\n    \"\"\"\n    Runs one full simulation-and-analysis case.\n    \"\"\"\n    np.random.seed(params['seed'])\n    \n    N = params['N']\n    X = np.random.binomial(1, 0.5, size=N)\n    U = np.random.uniform(size=N)\n    E = -np.log(U) # Inverse transform via Exponential(1) variates\n    \n    T = np.zeros(N)\n    \n    if params['h0_type'] == 'const':\n        lam, beta0, beta1 = params['lambda'], params['beta0'], params['beta1']\n        if np.isclose(beta1, 0): # Case A (PH model)\n            hazard_multiplier = lam * np.exp(beta0 * X)\n            T = E / hazard_multiplier\n        else: # Case B (non-PH model)\n            idx_x0 = (X == 0)\n            T[idx_x0] = E[idx_x0] / lam\n            \n            idx_x1 = (X == 1)\n            term = E[idx_x1] * beta1 / (lam * np.exp(beta0))\n            T[idx_x1] = (1 / beta1) * np.log(1 + term)\n\n    elif params['h0_type'] == 'weibull': # Case C (PH model)\n        lam, k, beta0 = params['lambda'], params['k'], params['beta0']\n        base = E / (lam * np.exp(beta0 * X))\n        T = np.power(base, 1.0 / k)\n\n    c_low, c_high = params['c_dist']\n    C = np.random.uniform(c_low, c_high, size=N)\n    \n    Y = np.minimum(T, C)\n    delta = (T <= C).astype(int)\n\n    beta_hat = fit_cox_model(Y, delta, X)\n    residuals, event_times = calculate_schoenfeld(Y, delta, X, beta_hat)\n    \n    is_rejected = test_ph(residuals, event_times, alpha=0.05)\n    \n    return is_rejected\n\ndef solve():\n    test_cases = [\n        # Case A: PH holds\n        {'h0_type': 'const', 'lambda': 0.1, 'k': None, 'beta0': 0.8, 'beta1': 0.0,\n         'N': 600, 'c_dist': (0, 15), 'seed': 13579},\n        # Case B: PH violated\n        {'h0_type': 'const', 'lambda': 0.1, 'k': None, 'beta0': 0.2, 'beta1': 0.3,\n         'N': 600, 'c_dist': (0, 8), 'seed': 24680},\n        # Case C: PH holds\n        {'h0_type': 'weibull', 'lambda': 0.01, 'k': 2.0, 'beta0': 0.7, 'beta1': 0.0,\n         'N': 300, 'c_dist': (0, 10), 'seed': 11223},\n    ]\n\n    results = []\n    for case_params in test_cases:\n        decision = run_case(case_params)\n        results.append(decision)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "4986345"}]}