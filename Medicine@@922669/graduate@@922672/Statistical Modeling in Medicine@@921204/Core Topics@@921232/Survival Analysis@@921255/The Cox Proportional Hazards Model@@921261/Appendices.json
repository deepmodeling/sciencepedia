{"hands_on_practices": [{"introduction": "A primary goal in medical research is not only to estimate the effect of a treatment or risk factor but also to quantify the uncertainty of that estimate. This practice focuses on constructing a confidence interval for the hazard ratio, the most common measure of effect size in a Cox model. Mastering this skill allows you to move beyond a simple point estimate and provide a range of plausible values for the true effect, which is essential for robust scientific interpretation. [@problem_id:1911713]", "problem": "In a clinical trial for a new therapeutic agent designed to treat a specific life-threatening condition, researchers use a Cox proportional hazards model to assess the drug's effectiveness compared to a placebo. The model's primary goal is to analyze the time until a critical adverse event occurs. The model includes a single covariate, $X$, where $X=1$ for patients in the treatment group and $X=0$ for patients in the placebo group. The hazard function for a patient is given by $h(t|X) = h_0(t)\\exp(\\beta X)$, where $h_0(t)$ is the baseline hazard function and $\\beta$ is the log-hazard ratio coefficient.\n\nFrom the study data, the maximum likelihood estimate for the coefficient is calculated as $\\hat{\\beta} = -0.500$. The standard error of this estimate is found to be $SE(\\hat{\\beta}) = 0.250$.\n\nThe hazard ratio, defined as $\\exp(\\beta)$, represents the ratio of the hazard rate for the treatment group to that of the placebo group. Construct a 95% confidence interval for this hazard ratio. For your calculations, use the standard normal distribution critical value $z_{0.025} = 1.96$. Report your answer as a pair of numbers representing the lower and upper bounds of the confidence interval, respectively. Round both bounds to three significant figures.", "solution": "We are given a Cox proportional hazards model with hazard function $h(t|X) = h_{0}(t)\\exp(\\beta X)$ and the hazard ratio defined as $\\exp(\\beta)$. The maximum likelihood estimate is $\\hat{\\beta} = -0.500$ with standard error $SE(\\hat{\\beta}) = 0.250$. To construct a $95$ percent confidence interval for the hazard ratio, we first construct the Wald confidence interval for $\\beta$:\n$$\n\\hat{\\beta} \\pm z_{0.025}\\,SE(\\hat{\\beta}) \\quad \\text{with} \\quad z_{0.025} = 1.96.\n$$\nSubstituting the given values,\n$$\n-0.500 \\pm 1.96 \\times 0.250 = -0.500 \\pm 0.490,\n$$\nwhich yields\n$$\n\\text{CI for } \\beta: \\; \\left[-0.990,\\,-0.010\\right].\n$$\nSince the hazard ratio is a monotone transformation $\\exp(\\beta)$, the $95$ percent confidence interval for the hazard ratio is obtained by exponentiating the endpoints:\n$$\n\\left[\\exp(-0.990),\\,\\exp(-0.010)\\right].\n$$\nEvaluating and rounding to three significant figures,\n$$\n\\exp(-0.990) \\approx 0.371576691 \\approx 0.372,\\quad \\exp(-0.010) \\approx 0.990049834 \\approx 0.990.\n$$\nTherefore, the $95$ percent confidence interval for the hazard ratio is $\\left[0.372,\\,0.990\\right]$.", "answer": "$$\\boxed{\\begin{pmatrix}0.372  0.990\\end{pmatrix}}$$", "id": "1911713"}, {"introduction": "To truly understand the Cox model, one must look 'under the hood' at the engine that drives its parameter estimation: the partial likelihood function. This exercise guides you through the fundamental calculus of deriving the score function (the gradient) and the observed information matrix (the negative Hessian) of the partial log-likelihood. By working through this derivation and applying it to a small dataset, you will gain a deep, mechanical understanding of how model coefficients and their standard errors are computed. [@problem_id:3181431]", "problem": "Consider the Cox proportional hazards (CPH) model with covariate vectors $x_{i} \\in \\mathbb{R}^{p}$ and regression parameter $\\beta \\in \\mathbb{R}^{p}$. Let $D$ denote the set of indices of observed events (non-tied), and let $R(t_{i})$ denote the risk set just prior to time $t_{i}$ for $i \\in D$. The partial log-likelihood is\n$$\n\\ell(\\beta) \\;=\\; \\sum_{i \\in D} \\Big\\{ x_{i}^{\\top}\\beta \\;-\\; \\ln\\!\\Big( \\sum_{j \\in R(t_{i})} \\exp(x_{j}^{\\top}\\beta) \\Big) \\Big\\}.\n$$\nStarting only from this definition, and using standard rules of vector calculus, do the following:\n\n1) Derive the score function $U(\\beta) \\equiv \\nabla_{\\beta}\\ell(\\beta)$ as a function of $\\beta$, $x_{i}$, and the risk sets $R(t_{i})$.\n\n2) Derive the observed information matrix $J(\\beta) \\equiv -\\nabla_{\\beta}^{2}\\ell(\\beta)$ as a function of $\\beta$, $x_{i}$, and the risk sets $R(t_{i})$.\n\nThen, consider the following dataset with $p=2$ covariates:\n- Subject $1$: event time $t_{1}=2$, event indicator $\\delta_{1}=1$, covariate $x_{1}=\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$.\n- Subject $2$: event time $t_{2}=3$, event indicator $\\delta_{2}=1$, covariate $x_{2}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$.\n- Subject $3$: censoring time $t_{3}=4$, event indicator $\\delta_{3}=0$, covariate $x_{3}=\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$.\n- Subject $4$: event time $t_{4}=5$, event indicator $\\delta_{4}=1$, covariate $x_{4}=\\begin{pmatrix}2 \\\\ -1\\end{pmatrix}$.\n\nAssume no tied event times. Use your general formulas to evaluate the observed information at $\\beta=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ and compute the determinant of $J(\\beta)$ at this value. Round your final numerical answer to four significant figures. Express the final answer as a pure number without units.", "solution": "The problem asks for the derivation of the score function and observed information matrix for the Cox proportional hazards model, followed by a calculation for a specific dataset.\n\nThe partial log-likelihood for the Cox model with no tied event times is given by\n$$\n\\ell(\\beta) = \\sum_{i \\in D} \\left\\{ x_{i}^{\\top}\\beta - \\ln\\left( \\sum_{j \\in R(t_{i})} \\exp(x_{j}^{\\top}\\beta) \\right) \\right\\}\n$$\nwhere $D$ is the set of indices for subjects with observed events, and $R(t_i)$ is the risk set at event time $t_i$.\n\n**1) Derivation of the Score Function $U(\\beta)$**\n\nThe score function is the gradient of the log-likelihood with respect to the parameter vector $\\beta$:\n$$\nU(\\beta) \\equiv \\nabla_{\\beta}\\ell(\\beta) = \\nabla_{\\beta} \\sum_{i \\in D} \\left\\{ x_{i}^{\\top}\\beta - \\ln\\left( \\sum_{j \\in R(t_{i})} \\exp(x_{j}^{\\top}\\beta) \\right) \\right\\}\n$$\nWe can interchange differentiation and summation:\n$$\nU(\\beta) = \\sum_{i \\in D} \\nabla_{\\beta} \\left\\{ x_{i}^{\\top}\\beta - \\ln\\left( \\sum_{j \\in R(t_{i})} \\exp(x_{j}^{\\top}\\beta) \\right) \\right\\}\n$$\nLet's differentiate a single term in the sum. The gradient of the first part is $\\nabla_{\\beta}(x_{i}^{\\top}\\beta) = x_i$.\n\nFor the second part, we use the chain rule for vector calculus, $\\nabla_{\\beta} \\ln(f(\\beta)) = \\frac{1}{f(\\beta)}\\nabla_{\\beta}f(\\beta)$. Let $S_i(\\beta) = \\sum_{j \\in R(t_{i})} \\exp(x_{j}^{\\top}\\beta)$.\nThe gradient of $S_i(\\beta)$ is:\n$$\n\\nabla_{\\beta} S_i(\\beta) = \\nabla_{\\beta} \\left(\\sum_{j \\in R(t_{i})} \\exp(x_{j}^{\\top}\\beta)\\right) = \\sum_{j \\in R(t_{i})} \\nabla_{\\beta}\\exp(x_{j}^{\\top}\\beta)\n$$\nUsing the chain rule again, $\\nabla_{\\beta}\\exp(u) = \\exp(u)\\nabla_{\\beta}u$. Here $u=x_j^\\top\\beta$, so $\\nabla_\\beta(x_j^\\top\\beta) = x_j$.\n$$\n\\nabla_{\\beta} S_i(\\beta) = \\sum_{j \\in R(t_{i})} \\exp(x_{j}^{\\top}\\beta) x_j\n$$\nThus, the gradient of the logarithmic term is:\n$$\n\\nabla_{\\beta} \\ln(S_i(\\beta)) = \\frac{\\sum_{j \\in R(t_{i})} x_j \\exp(x_{j}^{\\top}\\beta)}{\\sum_{j \\in R(t_{i})} \\exp(x_{j}^{\\top}\\beta)}\n$$\nLet us denote this term as $E(\\beta, t_i)$, which represents the expected value of the covariate vector over the risk set $R(t_i)$ at time $t_i$, weighted by the hazard terms $\\exp(x_j^\\top\\beta)$.\n\nCombining the parts, the gradient for the $i$-th term is $x_i - E(\\beta, t_i)$. The total score function is the sum over all events:\n$$\nU(\\beta) = \\sum_{i \\in D} \\left( x_i - \\frac{\\sum_{j \\in R(t_{i})} x_j \\exp(x_{j}^{\\top}\\beta)}{\\sum_{j \\in R(t_{i})} \\exp(x_{j}^{\\top}\\beta)} \\right) = \\sum_{i \\in D} (x_i - E(\\beta, t_i))\n$$\n\n**2) Derivation of the Observed Information Matrix $J(\\beta)$**\n\nThe observed information matrix is the negative of the Hessian of the log-likelihood: $J(\\beta) \\equiv -\\nabla_{\\beta}^{2}\\ell(\\beta) = -\\nabla_{\\beta} (U(\\beta)^{\\top})$. We differentiate the score function $U(\\beta)$ with respect to $\\beta^{\\top}$:\n$$\n\\nabla_{\\beta}^{2}\\ell(\\beta) = \\nabla_{\\beta}^\\top U(\\beta) = \\sum_{i \\in D} \\nabla_{\\beta}^\\top \\left( x_i - E(\\beta, t_i) \\right) = -\\sum_{i \\in D} \\nabla_{\\beta}^\\top E(\\beta, t_i)\n$$\nWe apply the quotient rule for vector differentiation to $E(\\beta, t_i) = \\frac{N_i(\\beta)}{D_i(\\beta)}$, where $N_i(\\beta) = \\sum_{j \\in R(t_i)} x_j \\exp(x_j^\\top \\beta)$ and $D_i(\\beta) = \\sum_{j \\in R(t_i)} \\exp(x_j^\\top \\beta)$.\n$$\n\\nabla_{\\beta}^\\top E(\\beta, t_i) = \\frac{D_i(\\beta) (\\nabla_{\\beta}^\\top N_i(\\beta)) - N_i(\\beta) (\\nabla_{\\beta}^\\top D_i(\\beta))}{D_i(\\beta)^2}\n$$\nThe necessary derivatives are:\n$$\n\\nabla_{\\beta}^\\top N_i(\\beta) = \\nabla_{\\beta}^\\top \\left( \\sum_{j \\in R(t_i)} x_j \\exp(x_j^\\top \\beta) \\right) = \\sum_{j \\in R(t_i)} x_j \\left( \\nabla_{\\beta}^\\top \\exp(x_j^\\top \\beta) \\right) = \\sum_{j \\in R(t_i)} x_j \\left( \\exp(x_j^\\top \\beta) x_j^\\top \\right) = \\sum_{j \\in R(t_i)} x_j x_j^\\top \\exp(x_j^\\top \\beta)\n$$\n$$\n\\nabla_{\\beta}^\\top D_i(\\beta) = \\nabla_{\\beta}^\\top \\left( \\sum_{j \\in R(t_i)} \\exp(x_j^\\top \\beta) \\right) = \\sum_{j \\in R(t_i)} \\exp(x_j^\\top \\beta) x_j^\\top = N_i(\\beta)^\\top\n$$\nSubstituting these back into the quotient rule expression:\n$$\n\\nabla_{\\beta}^\\top E(\\beta, t_i) = \\frac{\\left(\\sum_k \\exp(x_k^\\top\\beta)\\right) \\left(\\sum_j x_j x_j^\\top \\exp(x_j^\\top\\beta)\\right) - \\left(\\sum_j x_j \\exp(x_j^\\top\\beta)\\right) \\left(\\sum_k x_k \\exp(x_k^\\top\\beta)\\right)^\\top}{\\left(\\sum_k \\exp(x_k^\\top\\beta)\\right)^2}\n$$\nwhere all sums are over indices in $R(t_i)$. This simplifies to:\n$$\n\\nabla_{\\beta}^\\top E(\\beta, t_i) = \\frac{\\sum_j x_j x_j^\\top \\exp(x_j^\\top\\beta)}{\\sum_k \\exp(x_k^\\top\\beta)} - \\left(\\frac{\\sum_j x_j \\exp(x_j^\\top\\beta)}{\\sum_k \\exp(x_k^\\top\\beta)}\\right) \\left(\\frac{\\sum_k x_k \\exp(x_k^\\top\\beta)}{\\sum_l \\exp(x_l^\\top\\beta)}\\right)^\\top\n$$\nThis is $E[XX^\\top] - E[X]E[X]^\\top$, the conditional covariance matrix of the covariates in the risk set, which we denote $V(\\beta, t_i)$.\nThe Hessian is $\\nabla_{\\beta}^{2}\\ell(\\beta) = -\\sum_{i \\in D} V(\\beta, t_i)$. The observed information matrix is therefore:\n$$\nJ(\\beta) = \\sum_{i \\in D} V(\\beta, t_i) = \\sum_{i \\in D} \\left\\{ \\frac{\\sum_{j \\in R(t_i)} x_j x_j^\\top \\exp(x_j^\\top \\beta)}{\\sum_{k \\in R(t_i)} \\exp(x_k^\\top \\beta)} - E(\\beta, t_i)E(\\beta, t_i)^\\top \\right\\}\n$$\n\n**3) Calculation for the Dataset at $\\beta=0$**\n\nFirst, we identify the set of event indices $D$ and the risk sets $R(t_i)$ for each $i \\in D$. The event indicators are $\\delta_1=1$, $\\delta_2=1$, $\\delta_3=0$, $\\delta_4=1$. Thus, the set of event indices is $D = \\{1, 2, 4\\}$. The event times are $t_1=2$, $t_2=3$, $t_4=5$.\n- Risk set at $t_1=2$: $R(t_1) = \\{j : t_j \\ge 2\\} = \\{1, 2, 3, 4\\}$.\n- Risk set at $t_2=3$: $R(t_2) = \\{j : t_j \\ge 3\\} = \\{2, 3, 4\\}$.\n- Risk set at $t_4=5$: $R(t_4) = \\{j : t_j \\ge 5\\} = \\{4\\}$.\n\nWe need to evaluate $J(\\beta)$ at $\\beta=0$. At $\\beta=0$, $\\exp(x_j^\\top \\beta) = \\exp(0) = 1$ for all $j$. Let $|R(t_i)|$ be the number of subjects in risk set $R(t_i)$.\nThe expected value $E(0, t_i)$ simplifies to the sample mean of covariates in the risk set:\n$$\nE(0, t_i) = \\frac{\\sum_{j \\in R(t_i)} x_j}{|R(t_i)|} \\equiv \\bar{x}_{R(t_i)}\n$$\nThe first term in $V(0, t_i)$ becomes the mean of the outer products:\n$$\n\\frac{\\sum_{j \\in R(t_i)} x_j x_j^\\top}{|R(t_i)|}\n$$\nSo, $V(0, t_i)$ is the sample covariance matrix of covariates in $R(t_i)$ (using $1/N$ normalization):\n$$\nV(0, t_i) = \\frac{1}{|R(t_i)|} \\sum_{j \\in R(t_i)} x_j x_j^\\top - \\bar{x}_{R(t_i)}\\bar{x}_{R(t_i)}^\\top\n$$\nThen $J(0) = \\sum_{i \\in D} V(0, t_i) = V(0, t_1) + V(0, t_2) + V(0, t_4)$.\n\n- **Contribution from $t_1=2$**: $R(t_1)=\\{1, 2, 3, 4\\}$, so $|R(t_1)|=4$.\nThe covariates are $x_1=\\begin{pmatrix}0\\\\1\\end{pmatrix}$, $x_2=\\begin{pmatrix}1\\\\0\\end{pmatrix}$, $x_3=\\begin{pmatrix}1\\\\1\\end{pmatrix}$, $x_4=\\begin{pmatrix}2\\\\-1\\end{pmatrix}$.\n$\\bar{x}_{R(t_1)} = \\frac{1}{4} \\left( \\begin{pmatrix}0\\\\1\\end{pmatrix} + \\begin{pmatrix}1\\\\0\\end{pmatrix} + \\begin{pmatrix}1\\\\1\\end{pmatrix} + \\begin{pmatrix}2\\\\-1\\end{pmatrix} \\right) = \\frac{1}{4}\\begin{pmatrix}4\\\\1\\end{pmatrix} = \\begin{pmatrix}1\\\\1/4\\end{pmatrix}$.\n$\\sum_{j \\in R(t_1)} x_j x_j^\\top = \\begin{pmatrix}00\\\\01\\end{pmatrix} + \\begin{pmatrix}10\\\\00\\end{pmatrix} + \\begin{pmatrix}11\\\\11\\end{pmatrix} + \\begin{pmatrix}4-2\\\\-21\\end{pmatrix} = \\begin{pmatrix}6-1\\\\-13\\end{pmatrix}$.\n$V(0, t_1) = \\frac{1}{4} \\begin{pmatrix}6-1\\\\-13\\end{pmatrix} - \\begin{pmatrix}1\\\\1/4\\end{pmatrix}\\begin{pmatrix}11/4\\end{pmatrix} = \\begin{pmatrix}3/2  -1/4 \\\\ -1/4  3/4\\end{pmatrix} - \\begin{pmatrix}1  1/4 \\\\ 1/4  1/16\\end{pmatrix} = \\begin{pmatrix}1/2  -1/2 \\\\ -1/2  11/16\\end{pmatrix}$.\n\n- **Contribution from $t_2=3$**: $R(t_2)=\\{2, 3, 4\\}$, so $|R(t_2)|=3$.\n$\\bar{x}_{R(t_2)} = \\frac{1}{3} \\left( \\begin{pmatrix}1\\\\0\\end{pmatrix} + \\begin{pmatrix}1\\\\1\\end{pmatrix} + \\begin{pmatrix}2\\\\-1\\end{pmatrix} \\right) = \\frac{1}{3}\\begin{pmatrix}4\\\\0\\end{pmatrix} = \\begin{pmatrix}4/3\\\\0\\end{pmatrix}$.\n$\\sum_{j \\in R(t_2)} x_j x_j^\\top = \\begin{pmatrix}10\\\\00\\end{pmatrix} + \\begin{pmatrix}11\\\\11\\end{pmatrix} + \\begin{pmatrix}4-2\\\\-21\\end{pmatrix} = \\begin{pmatrix}6-1\\\\-12\\end{pmatrix}$.\n$V(0, t_2) = \\frac{1}{3} \\begin{pmatrix}6-1\\\\-12\\end{pmatrix} - \\begin{pmatrix}4/3\\\\0\\end{pmatrix}\\begin{pmatrix}4/30\\end{pmatrix} = \\begin{pmatrix}2  -1/3 \\\\ -1/3  2/3\\end{pmatrix} - \\begin{pmatrix}16/9  0 \\\\ 0  0\\end{pmatrix} = \\begin{pmatrix}2/9  -1/3 \\\\ -1/3  2/3\\end{pmatrix}$.\n\n- **Contribution from $t_4=5$**: $R(t_4)=\\{4\\}$, so $|R(t_4)|=1$.\n$\\bar{x}_{R(t_4)} = x_4$. The covariance of a single data point is zero. So, $V(0, t_4) = \\begin{pmatrix}00\\\\00\\end{pmatrix}$.\n\nNow, we sum the contributions to get $J(0)$:\n$$\nJ(0) = \\begin{pmatrix}1/2  -1/2 \\\\ -1/2  11/16\\end{pmatrix} + \\begin{pmatrix}2/9  -1/3 \\\\ -1/3  2/3\\end{pmatrix} = \\begin{pmatrix} 1/2+2/9  -1/2-1/3 \\\\ -1/2-1/3  11/16+2/3 \\end{pmatrix}\n$$\n$$\nJ(0) = \\begin{pmatrix} 9/18+4/18  -3/6-2/6 \\\\ -3/6-2/6  33/48+32/48 \\end{pmatrix} = \\begin{pmatrix} 13/18  -5/6 \\\\ -5/6  65/48 \\end{pmatrix}\n$$\nFinally, we compute the determinant of $J(0)$:\n$$\n\\det(J(0)) = \\left(\\frac{13}{18}\\right)\\left(\\frac{65}{48}\\right) - \\left(-\\frac{5}{6}\\right)^2 = \\frac{845}{864} - \\frac{25}{36}\n$$\nThe common denominator is $864 = 36 \\times 24$.\n$$\n\\det(J(0)) = \\frac{845}{864} - \\frac{25 \\times 24}{36 \\times 24} = \\frac{845 - 600}{864} = \\frac{245}{864}\n$$\nAs a decimal, this is $245 \\div 864 \\approx 0.2835648148...$. Rounding to four significant figures gives $0.2836$.", "answer": "$$\\boxed{0.2836}$$", "id": "3181431"}, {"introduction": "A model is only as good as its assumptions, and for the Cox model, the proportional hazards (PH) assumption is foundational. This practice introduces Schoenfeld residuals, a powerful diagnostic tool used to assess whether the effect of a covariate is constant over time, as the PH assumption requires. Through this exercise, you will learn to interpret these residuals to diagnose potential model misspecification, a critical skill for any serious practitioner of survival analysis. [@problem_id:4987356]", "problem": "Consider a survival study in nephrology modeling time-to-access failure for hemodialysis patients. Let $x$ denote a single continuous covariate (e.g., baseline serum albumin in $\\text{g/dL}$), and suppose failures occur at distinct event times $\\{t_i\\}$ with corresponding risk sets $R(t_i)$. A Cox proportional hazards (PH) model specifies the hazard as $h(t \\mid x) = h_0(t)\\exp(x \\beta)$, where $h_0(t)$ is an unspecified baseline hazard and $\\beta$ is a constant regression coefficient that does not vary with $t$. For each event time $t_i$, define the Schoenfeld residual for covariate $x$ as\n$$\nr_i(\\hat\\beta) = x_i - \\bar x(t_i; \\hat\\beta), \\quad \\text{where} \\quad \\bar x(t; \\beta) = \\frac{\\sum_{j \\in R(t)} x_j \\exp(x_j \\beta)}{\\sum_{j \\in R(t)} \\exp(x_j \\beta)}.\n$$\nSelect all statements that correctly explain how Schoenfeld residuals diagnose the proportional hazards assumption for $x$.\n\nA. Under proportional hazards for $x$, the conditional expectation of $r_i(\\hat\\beta)$ at each event time, given the risk set $R(t_i)$, is $0$, so regressing $r_i(\\hat\\beta)$ on a suitable function of event time should yield slope $0$; a nonzero slope indicates a time-varying effect for $x$.\n\nB. Because the baseline hazard $h_0(t)$ is unspecified, Schoenfeld residuals primarily assess misspecification of $h_0(t)$; thus, any non-flat trend of $r_i(\\hat\\beta)$ versus $t_i$ signals an incorrect baseline hazard rather than a violation of proportional hazards.\n\nC. Validity of the Cox model requires that $r_i(\\hat\\beta)$ be approximately standard normal; if the residuals are not normal, the model fails the proportional hazards assumption.\n\nD. Scaling $r_i(\\hat\\beta)$ by the estimated variance of $x$ within the risk set at $t_i$ produces residuals that, under proportional hazards, are uncorrelated with event time; this leads to a formal test in which the estimated slope of residuals versus time is $0$ under the null of proportional hazards.\n\nE. Schoenfeld residuals are defined and computed for all subjects at all observed times; their cumulative sum process approximates the martingale residual process, so trends in $r_i(\\hat\\beta)$ primarily diagnose the shape of $h_0(t)$ rather than time-varying $\\beta$.", "solution": "The Cox proportional hazards (PH) model posits $h(t \\mid x) = h_0(t)\\exp(x\\beta)$, where $h_0(t)$ is an arbitrary nonnegative function and $\\beta$ is time-invariant. Inference for $\\beta$ uses the partial likelihood, which treats the conditional probability that, at each distinct event time $t_i$, the observed case fails among those at risk. The partial likelihood score for $\\beta$ with a single covariate $x$ at $t_i$ has the form\n$$\nU_i(\\beta) = x_i - \\bar x(t_i; \\beta),\n$$\nwhere $\\bar x(t_i; \\beta)$ is the risk-set weighted mean. The full score is $U(\\beta) = \\sum_i U_i(\\beta)$ across all event times. The estimator $\\hat\\beta$ solves $U(\\hat\\beta) = 0$.\n\nPrinciple-based derivation of the residual’s diagnostic role proceeds as follows:\n\n- Fundamental definition (Cox model): $h(t \\mid x) = h_0(t)\\exp(x\\beta)$ implies that, at any event time $t_i$, the conditional probability that subject $i$ fails, given that someone fails at $t_i$ and given covariates in $R(t_i)$, is\n$$\n\\Pr(\\text{subject } i \\text{ fails at } t_i \\mid R(t_i)) = \\frac{\\exp(x_i \\beta)}{\\sum_{j \\in R(t_i)} \\exp(x_j \\beta)}.\n$$\n\n- Under proportional hazards with the true coefficient $\\beta_0$, the conditional expectation of $x$ among those who fail at $t_i$ equals the risk-set weighted mean $\\bar x(t_i; \\beta_0)$. Therefore, the conditional expectation of $U_i(\\beta_0) = x_i - \\bar x(t_i; \\beta_0)$ is $0$:\n$$\n\\mathbb{E}\\!\\left[x_i - \\bar x(t_i; \\beta_0) \\,\\middle|\\, R(t_i)\\right] = 0.\n$$\n\n- Evaluated at $\\hat\\beta$, the Schoenfeld residual $r_i(\\hat\\beta)$ estimates $U_i(\\beta_0)$, and under the null of proportional hazards (i.e., $\\beta(t) \\equiv \\beta_0$), its conditional expectation remains near $0$ for each $t_i$, with no systematic dependence on $t_i$ beyond random variation.\n\n- If the proportional hazards assumption is violated because the effect of $x$ varies over time, say $\\beta(t) = \\beta_0 + g(t)$ for some nonconstant function $g(t)$, then the conditional mean of $x_i - \\bar x(t_i; \\beta_0)$ changes systematically with $t_i$, yielding a nonzero trend when plotting $r_i(\\hat\\beta)$ against $t_i$ or regressing residuals on functions of time.\n\nFormalizing the diagnostic further uses a scaling argument: the variance of $U_i(\\beta)$, conditional on $R(t_i)$, is the risk-set variance of $x$ under the weighted distribution proportional to $\\exp(x \\beta)$. Define a scaled Schoenfeld residual\n$$\nr_i^*(\\hat\\beta) = \\hat V_i^{-1} r_i(\\hat\\beta),\n$$\nwhere $\\hat V_i$ estimates the risk-set variance of $x$ at $t_i$. Under proportional hazards, $\\mathbb{E}[r_i^*(\\hat\\beta) \\mid R(t_i)] = 0$, and because $\\beta$ is constant in $t$, these scaled residuals should exhibit no systematic association with $t_i$. Regressing $r_i^*(\\hat\\beta)$ on a function of time (e.g., $t_i$ or a transformation like the rank or $\\log(t_i)$) yields an estimated slope of $0$ under the null, and a nonzero slope provides evidence that $\\beta$ varies with time. This is the basis of the widely used Grambsch–Therneau test for time-varying coefficients.\n\nWe now evaluate each option:\n\nA. The statement correctly captures the core property: under proportional hazards, the conditional expectation of the Schoenfeld residual at each event time is $0$. Because $\\beta$ is time-invariant under PH, there should be no systematic trend of residuals with $t$. Regressing residuals on time should yield slope $0$ under the null, and deviation indicates time-varying effect. Verdict: Correct.\n\nB. Schoenfeld residuals, by construction, contrast observed covariate values at failure with their risk-set expected values under $\\exp(x \\beta)$ weights. They are diagnostic of time dependence in $\\beta$, not of the unspecified baseline hazard $h_0(t)$, whose form does not enter the partial likelihood. A non-flat pattern implicates nonproportional hazards (time-varying coefficients), not misspecification of $h_0(t)$. Verdict: Incorrect.\n\nC. There is no requirement that Schoenfeld residuals be approximately standard normal. They are defined only at event times, are heteroscedastic across $t_i$, and their distribution need not be normal. Diagnostics use their mean-zero property and independence from time under PH, not normality. Verdict: Incorrect.\n\nD. This restates the scaled residual approach: standardizing by the estimated risk-set variance yields residuals that, under PH, should have no association with event time; testing whether the slope is $0$ in a regression on time is a formal diagnostic for time-varying $\\beta$. Verdict: Correct.\n\nE. Schoenfeld residuals are defined at event times, not for all subjects or all times. Their trends diagnose departures from proportional hazards (time-varying coefficients), not the shape of the baseline hazard. The claim conflates Schoenfeld residuals with martingale residuals, which are different constructs used for other diagnostics. Verdict: Incorrect.", "answer": "$$\\boxed{AD}$$", "id": "4987356"}]}