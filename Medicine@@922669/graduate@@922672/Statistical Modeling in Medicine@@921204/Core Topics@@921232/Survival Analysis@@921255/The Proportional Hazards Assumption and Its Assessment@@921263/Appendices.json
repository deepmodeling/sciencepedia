{"hands_on_practices": [{"introduction": "A cornerstone of assessing the proportional hazards (PH) assumption is through graphical diagnostics. This exercise provides foundational practice by guiding you through the manual calculation of group-specific Nelson-Aalen estimators for the cumulative hazard function. By plotting a function of these estimators, such as the difference of their logarithms, you can visually inspect for deviations from the constancy predicted by the PH model, a fundamental skill in applied survival analysis [@problem_id:4991166].", "problem": "A randomized clinical study follows two groups, indexed by $g \\in \\{1,2\\}$, to compare time to a cardiovascular event. For subject $i$ in group $g$, let $T_{gi}$ be the time to event or censoring and $\\Delta_{gi} \\in \\{0,1\\}$ the event indicator. The hazard function is $h_g(t)$, and the cumulative hazard is $H_g(t) = \\int_0^t h_g(u)\\,du$. Survival is $S_g(t) = \\exp\\{-H_g(t)\\}$. Under the proportional hazards assumption (PH), there exists a constant hazard ratio $\\theta  0$ such that $h_2(t) = \\theta \\, h_1(t)$ for all $t \\ge 0$. Your task is to use group-specific Nelson–Aalen estimators of $H_g(t)$ to assess PH by inspecting whether the difference of the logarithms of the group-specific cumulative hazards is approximately constant over $t$.\nData: group $1$ has $n_1 = 4$ subjects with observed $(T_{1i}, \\Delta_{1i})$ equal to $(1,1)$, $(3,1)$, $(5,0)$, $(7,1)$. Group $2$ has $n_2 = 4$ subjects with $(T_{2i}, \\Delta_{2i})$ equal to $(2,1)$, $(4,1)$, $(6,1)$, $(8,0)$. Assume no left truncation and independent, non-informative right-censoring. Consider the pooled set of distinct event times across both groups.\nStarting from the core definitions of $h_g(t)$ and $H_g(t)$ and using the well-tested Nelson–Aalen estimator for $H_g(t)$ computed within each group, carry out the following tasks conceptually and computationally:\n- For each group $g \\in \\{1,2\\}$, define the group-specific risk set size $r_g(t_j)$ just prior to each distinct event time $t_j$ in that group and the number of events $d_g(t_j)$ at $t_j$. Use these to compute the group-specific Nelson–Aalen estimator $\\widehat H_g(t)$ as a right-continuous step function over the pooled event times up to the largest event time observed.\n- Using $\\log \\widehat H_g(t)$ when defined, inspect whether $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$ is approximately constant in $t$ and state whether the PH assumption appears supported in these data.\n- Be explicit about any constraints needed to compute the logarithms and any tie handling.\nWhich of the following statements are correct? Select all that apply.\n\nA. For each group $g$, the Nelson–Aalen estimator is computed using only within-group risk sets: $\\widehat H_g(t) = \\sum_{t_{gj} \\le t} d_g(t_{gj}) / r_g(t_{gj})$, where $r_g(t_{gj})$ counts subjects at risk in group $g$ strictly before $t_{gj}$ and $d_g(t_{gj})$ counts events in group $g$ at $t_{gj}$. To assess proportional hazards, evaluate $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$ over pooled event times $t$ where both $\\widehat H_g(t)  0$ and check for approximate constancy in $t$.\n\nB. A simpler and equivalent PH diagnostic is to compute Kaplan–Meier estimators $\\widehat S_g(t)$ for each group and check whether $\\widehat S_1(t) - \\widehat S_2(t)$ is approximately constant in $t$; if so, PH holds.\n\nC. To stabilize estimation, the denominators in the Nelson–Aalen increments for each group should be the pooled-at-risk counts across both groups, i.e., $r(t_j)$ from all subjects, rather than the within-group $r_g(t_j)$.\n\nD. Because under proportional hazards $H_2(t) = \\theta H_1(t)$ for some constant $\\theta$, an equivalent diagnostic to the log-difference check is to inspect whether $\\widehat H_2(t) / \\widehat H_1(t)$ is approximately constant in $t$ over times where both denominators are positive.\n\nE. If at a given time $t$ there are $d_g(t) \\ge 2$ tied events in group $g$, the Nelson–Aalen increment at $t$ is $d_g(t) / r_g(t)$ using the group-specific risk set size just prior to $t$; this tie handling is standard for Nelson–Aalen.\n\nF. Using the provided data, the computed values of $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$ over times where both are defined vary substantially with $t$, so the PH assumption is not supported in this sample.\n\nG. When $\\widehat H_g(t) = 0$ at early times, it is acceptable to define $\\log \\widehat H_g(t) = 0$ so that all pooled event times can be used in the log-difference plot without restriction.", "solution": "### Problem Validation\n\n#### Step 1: Extract Givens\n- **Study setup**: A randomized clinical study with two groups, indexed by $g \\in \\{1,2\\}$.\n- **Data per subject**: For subject $i$ in group $g$, the observed data is $(T_{gi}, \\Delta_{gi})$, where $T_{gi}$ is the time to event or censoring and $\\Delta_{gi} \\in \\{0,1\\}$ is the event indicator ($\\Delta_{gi}=1$ for event, $\\Delta_{gi}=0$ for censoring).\n- **Hazard functions**: $h_g(t)$ is the hazard function for group $g$.\n- **Cumulative hazard functions**: $H_g(t) = \\int_0^t h_g(u)\\,du$.\n- **Survival functions**: $S_g(t) = \\exp\\{-H_g(t)\\}$.\n- **Proportional Hazards (PH) Assumption**: There exists a constant hazard ratio $\\theta  0$ such that $h_2(t) = \\theta \\, h_1(t)$ for all $t \\ge 0$.\n- **Task**: Assess the PH assumption by inspecting whether the difference of the logarithms of the group-specific Nelson-Aalen estimators, $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$, is approximately constant over time $t$.\n- **Data for Group 1**: $n_1 = 4$ subjects with $(T_{1i}, \\Delta_{1i})$ pairs: $(1,1)$, $(3,1)$, $(5,0)$, $(7,1)$.\n- **Data for Group 2**: $n_2 = 4$ subjects with $(T_{2i}, \\Delta_{2i})$ pairs: $(2,1)$, $(4,1)$, $(6,1)$, $(8,0)$.\n- **Assumptions**: No left truncation, independent and non-informative right-censoring.\n- **Computational instructions**: Use group-specific Nelson–Aalen estimator $\\widehat H_g(t) = \\sum_{t_{gj} \\le t} d_g(t_{gj}) / r_g(t_{gj})$, where $r_g(t_j)$ is the number of subjects at risk in group $g$ just prior to time $t_j$, and $d_g(t_j)$ is the number of events in group $g$ at time $t_j$. Evaluation is to be done over the pooled set of distinct event times.\n\n#### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria:\n- **Scientifically Grounded**: The problem is set within the standard framework of survival analysis, a core topic in biostatistics and medical statistics. All definitions—hazard function, cumulative hazard function, survival function, proportional hazards assumption, and the Nelson-Aalen estimator—are standard and textbook-correct. The proposed method for assessing the PH assumption is a valid and widely used graphical technique. There are no scientific or factual errors.\n- **Well-Posed**: The problem is clearly structured. It provides specific data, a well-defined statistical model (PH model), a standard non-parametric estimator (Nelson-Aalen), and a precise task (compute the estimators and a diagnostic quantity, then evaluate specific statements). The data are complete and sufficient to perform the required calculations. A unique solution exists for the computations.\n- **Objective**: The language is precise, quantitative, and free of subjectivity or ambiguity. Terms like \"risk set,\" \"event indicator,\" and \"Nelson-Aalen estimator\" have unambiguous definitions in the field.\n\nThe problem does not violate any of the invalidity criteria. It is a standard, formal exercise in applied statistics.\n\n#### Step 3: Verdict and Action\nThe problem is valid. The solution process will proceed.\n\n### Solution Derivation\n\nThe proportional hazards (PH) assumption states $h_2(t) = \\theta h_1(t)$ for a constant $\\theta  0$. Integrating both sides from $0$ to $t$ yields the relationship for the cumulative hazard functions:\n$$ H_2(t) = \\int_0^t h_2(u)\\,du = \\int_0^t \\theta h_1(u)\\,du = \\theta \\int_0^t h_1(u)\\,du = \\theta H_1(t) $$\nTaking the natural logarithm of both sides (for $t$ such that $H_1(t)  0$ and $H_2(t)  0$), we get:\n$$ \\log H_2(t) = \\log(\\theta H_1(t)) = \\log \\theta + \\log H_1(t) $$\nRearranging gives:\n$$ \\log H_2(t) - \\log H_1(t) = \\log \\theta $$\nThis shows that under the PH assumption, the difference between the log-cumulative hazards of the two groups is a constant, $\\log \\theta$. The problem asks to check if $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$ is constant, which is equivalent to checking if $\\log \\widehat H_2(t) - \\log \\widehat H_1(t)$ is constant (the constant would just be the negative of the other). We can assess this by replacing the theoretical functions $H_g(t)$ with their Nelson-Aalen estimates $\\widehat H_g(t)$.\n\nThe Nelson-Aalen estimator for the cumulative hazard in group $g$ is given by:\n$$ \\widehat H_g(t) = \\sum_{t_{gj} \\le t} \\frac{d_g(t_{gj})}{r_g(t_{gj})} $$\nwhere $t_{gj}$ are the distinct event times in group $g$, $d_g(t_{gj})$ is the number of events in group $g$ at time $t_{gj}$, and $r_g(t_{gj})$ is the number of subjects at risk in group $g$ just prior to time $t_{gj}$.\n\n**1. Calculation for Group 1:**\nData: $(1,1), (3,1), (5,0), (7,1)$. Initial risk set size $n_1=4$.\nDistinct event times are $t=1, 3, 7$.\n- At $t=1$: $d_1(1)=1$, $r_1(1)=4$. Increment is $1/4$. $\\widehat H_1(t) = 1/4$ for $1 \\le t  3$.\n- At $t=3$: One subject has had an event. $r_1(3)=3$. $d_1(3)=1$. Increment is $1/3$. $\\widehat H_1(t) = 1/4 + 1/3 = 7/12$ for $3 \\le t  5$.\n- At $t=5$: One subject is censored. The risk set size decreases from $2$ to $1$ for subsequent times. There is no increment to $\\widehat H_1(t)$. For $5 \\le t  7$, $\\widehat H_1(t)$ remains $7/12$.\n- At $t=7$: One subject has had an event at $t=1$, one at $t=3$, and one was censored at $t=5$. The risk set just prior to $t=7$ is $r_1(7)=1$. $d_1(7)=1$. Increment is $1/1 = 1$. $\\widehat H_1(t) = 7/12 + 1 = 19/12$ for $t \\ge 7$.\n\n**2. Calculation for Group 2:**\nData: $(2,1), (4,1), (6,1), (8,0)$. Initial risk set size $n_2=4$.\nDistinct event times are $t=2, 4, 6$.\n- At $t=2$: $d_2(2)=1$, $r_2(2)=4$. Increment is $1/4$. $\\widehat H_2(t) = 1/4$ for $2 \\le t  4$.\n- At $t=4$: One subject has had an event. $r_2(4)=3$. $d_2(4)=1$. Increment is $1/3$. $\\widehat H_2(t) = 1/4 + 1/3 = 7/12$ for $4 \\le t  6$.\n- At $t=6$: Two subjects have had events. $r_2(6)=2$. $d_2(6)=1$. Increment is $1/2$. $\\widehat H_2(t) = 7/12 + 1/2 = 13/12$ for $t \\ge 6$.\n- At $t=8$: One subject is censored.\n\n**3. Assessing the PH assumption:**\nWe evaluate $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$ at the pooled distinct event times $t \\in \\{1, 2, 3, 4, 6, 7\\}$, restricted to times where both $\\widehat H_1(t)0$ and $\\widehat H_2(t)0$.\n\n| Time $t$ | $\\widehat H_1(t)$ | $\\widehat H_2(t)$ | $\\log \\widehat H_1(t)$ | $\\log \\widehat H_2(t)$ | $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$ |\n| :------: | :--------------: | :--------------: | :-------------------: | :-------------------: | :---------------------------------------------: |\n| 1 | $1/4$ | $0$ | $\\approx -1.386$ | Undefined | Undefined |\n| 2 | $1/4$ | $1/4$ | $\\approx -1.386$ | $\\approx -1.386$ | $0$ |\n| 3 | $7/12$ | $1/4$ | $\\approx -0.539$ | $\\approx -1.386$ | $\\approx 0.847$ |\n| 4 | $7/12$ | $7/12$ | $\\approx -0.539$ | $\\approx -0.539$ | $0$ |\n| 6 | $7/12$ | $13/12$ | $\\approx -0.539$ | $\\approx 0.080$ | $\\approx -0.619$ |\n| 7 | $19/12$ | $13/12$ | $\\approx 0.459$ | $\\approx 0.080$ | $\\approx 0.379$ |\n\nThe values of the difference, $\\{0, 0.847, 0, -0.619, 0.379\\}$, fluctuate significantly and do not appear to be constant. This suggests that the proportional hazards assumption is not supported by these data.\n\n### Option-by-Option Analysis\n\n**A. For each group $g$, the Nelson–Aalen estimator is computed using only within-group risk sets: $\\widehat H_g(t) = \\sum_{t_{gj} \\le t} d_g(t_{gj}) / r_g(t_{gj})$, where $r_g(t_{gj})$ counts subjects at risk in group $g$ strictly before $t_{gj}$ and $d_g(t_{gj})$ counts events in group $g$ at $t_{gj}$. To assess proportional hazards, evaluate $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$ over pooled event times $t$ where both $\\widehat H_g(t)  0$ and check for approximate constancy in $t$.**\nThis statement accurately describes the standard procedure. To estimate group-specific cumulative hazards, one must use group-specific risk sets and event counts. The diagnostic test involves comparing these estimated functions, specifically looking for a constant difference on the logarithmic scale, which corresponds to a constant ratio on the original scale. The restriction to times where the estimators are positive is mathematically necessary for the logarithm.\n**Verdict: Correct**\n\n**B. A simpler and equivalent PH diagnostic is to compute Kaplan–Meier estimators $\\widehat S_g(t)$ for each group and check whether $\\widehat S_1(t) - \\widehat S_2(t)$ is approximately constant in $t$; if so, PH holds.**\nThis is incorrect. Under PH, $S_2(t) = [S_1(t)]^\\theta$. This is a power relationship, not an additive one. The difference $S_1(t) - S_2(t)$ will not be constant. A correct diagnostic using survival functions is to plot $\\log(-\\log \\widehat S_2(t))$ versus $\\log(-\\log \\widehat S_1(t))$, which should yield a straight line with slope $\\theta$. Alternatively, a plot of $\\log(-\\log \\widehat S_2(t)) - \\log(-\\log \\widehat S_1(t))$ versus $\\log(t)$ should be a horizontal line at height $\\log \\theta$.\n**Verdict: Incorrect**\n\n**C. To stabilize estimation, the denominators in the Nelson–Aalen increments for each group should be the pooled-at-risk counts across both groups, i.e., $r(t_j)$ from all subjects, rather than the within-group $r_g(t_j)$.**\nThis is incorrect for the purpose of creating a PH diagnostic plot. Using pooled risk sets is the method for estimating a *common* cumulative hazard function, which is a key component in constructing the log-rank test statistic for the null hypothesis $H_0: \\theta=1$. The goal here is different: to estimate $\\widehat H_1(t)$ and $\\widehat H_2(t)$ separately to see if their relationship is consistent with proportionality for any $\\theta$. This requires group-specific estimators.\n**Verdict: Incorrect**\n\n**D. Because under proportional hazards $H_2(t) = \\theta H_1(t)$ for some constant $\\theta$, an equivalent diagnostic to the log-difference check is to inspect whether $\\widehat H_2(t) / \\widehat H_1(t)$ is approximately constant in $t$ over times where both denominators are positive.**\nThis is correct. The relationship $H_2(t) = \\theta H_1(t)$ directly implies that the ratio $H_2(t) / H_1(t) = \\theta$ for all $t$ where $H_1(t)  0$. Checking for constancy of the ratio of the estimators $\\widehat H_2(t) / \\widehat H_1(t)$ is therefore a direct and mathematically equivalent way to assess the PH assumption. Taking the logarithm linearizes this relationship, which is often preferred for visual inspection as it can make deviations from constancy easier to detect and stabilize variance, but the ratio plot is also a valid diagnostic.\n**Verdict: Correct**\n\n**E. If at a given time $t$ there are $d_g(t) \\ge 2$ tied events in group $g$, the Nelson–Aalen increment at $t$ is $d_g(t) / r_g(t)$ using the group-specific risk set size just prior to $t$; this tie handling is standard for Nelson–Aalen.**\nThis statement is correct. The Nelson-Aalen framework naturally accommodates tied event times. The hazard increment at time $t$ is estimated as the total number of events at that time divided by the number at risk just prior. This is the standard definition and handling of ties for the Nelson-Aalen estimator. While the provided data has no ties, the principle described is accurate.\n**Verdict: Correct**\n\n**F. Using the provided data, the computed values of $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$ over times where both are defined vary substantially with $t$, so the PH assumption is not supported in this sample.**\nAs shown in the calculations above, the computed difference $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$ takes on the values $\\{0, \\approx 0.847, 0, \\approx-0.619, \\approx 0.379\\}$ at the relevant time points. This sequence is clearly not constant. The variation is substantial relative to the values themselves. Therefore, a visual inspection of these data would lead one to question the validity of the PH assumption.\n**Verdict: Correct**\n\n**G. When $\\widehat H_g(t) = 0$ at early times, it is acceptable to define $\\log \\widehat H_g(t) = 0$ so that all pooled event times can be used in the log-difference plot without restriction.**\nThis is mathematically incorrect. The natural logarithm of $0$ is undefined (it tends to $-\\infty$). Assigning an arbitrary finite value like $0$ is a falsification of the mathematical properties of the logarithm function and would severely distort the diagnostic plot. The correct procedure is to restrict the domain of the plot to times $t$ where both $\\widehat H_1(t)$ and $\\widehat H_2(t)$ are strictly positive.\n**Verdict: Incorrect**", "answer": "$$\\boxed{ADEF}$$", "id": "4991166"}, {"introduction": "When diagnostic tests reveal a clear violation of the proportional hazards assumption, simply reporting a single, averaged hazard ratio can be misleading. This problem presents a realistic scenario where the effect of a treatment varies over time, prompting the need for an alternative model. You will practice a principled, evidence-based workflow for moving from the Cox PH model to a more appropriate Accelerated Failure Time (AFT) model, using residual diagnostics and information criteria to justify your choice [@problem_id:4991128].", "problem": "Consider a randomized clinical trial with right-censored time-to-event outcomes, where the event is time to disease progression. Let $T$ denote the nonnegative survival time and $\\delta \\in \\{0,1\\}$ denote the event indicator ($\\delta=1$ if progression is observed, $0$ if censored). Let $\\mathbf{x}$ denote a vector of covariates including a binary treatment indicator $Z \\in \\{0,1\\}$, baseline age $A$ in years, and disease stage $S$ as an ordinal score. The hazard function is $h(t \\mid \\mathbf{x})$, the survival function is $S(t \\mid \\mathbf{x})$, and the density is $f(t \\mid \\mathbf{x})$. You are asked to assess the Proportional Hazards (PH) assumption and, if it is violated, to justify a scientifically sound non-PH alternative using the Accelerated Failure Time (AFT) framework, and to demonstrate a principled procedure to choose between PH and AFT using residual diagnostics and information criteria.\n\nFundamental base definitions to use are:\n- Cox Proportional Hazards (PH) model: $h(t \\mid \\mathbf{x}) = h_0(t) \\exp(\\mathbf{x}^\\top \\boldsymbol{\\beta})$, with $h_0(t)$ an unspecified baseline hazard and $\\boldsymbol{\\beta}$ regression parameters. Under PH, the hazard ratio $\\mathrm{HR}(t; \\mathbf{x}_1,\\mathbf{x}_2) = h(t \\mid \\mathbf{x}_1)/h(t \\mid \\mathbf{x}_2)$ is constant in $t$ for fixed $\\mathbf{x}_1$ and $\\mathbf{x}_2$.\n- Accelerated Failure Time (AFT) model: $\\log T = \\mathbf{x}^\\top \\boldsymbol{\\beta} + \\sigma W$, where $W$ follows a specified baseline error distribution with cumulative distribution function $F_W(\\cdot)$ and density $f_W(\\cdot)$, and $\\sigma0$ is a scale parameter. Equivalently, $S(t \\mid \\mathbf{x}) = S_0\\!\\left(t \\exp(-\\mathbf{x}^\\top \\boldsymbol{\\beta})\\right)$, where $S_0(\\cdot)$ is the baseline survival function induced by $W$.\n\nSuppose you fit a Cox PH model with covariates $(Z,A,S)$ on a dataset of size $n=800$ with $30\\%$ right-censoring. You compute scaled Schoenfeld residuals for each covariate and apply the Grambsch and Therneau (GT) test of zero slope versus $\\log(t)$:\n- For $Z$, the estimated slope versus $\\log(t)$ is $0.35$ with two-sided $p$-value $p=0.0003$; for $A$ and $S$, slopes are close to $0$ with $p0.20$.\nA log-minus-log plot, i.e., curves of $\\log\\{-\\log \\widehat{S}(t \\mid Z)\\}$ against $\\log(t)$ stratified by $Z$, shows clear non-parallelism and crossings.\nYou then fit three parametric AFT candidates using $(Z,A,S)$: log-normal, log-logistic, and Weibull. For the log-normal AFT fit, standardized error residuals $\\widehat{\\epsilon}_i = \\{\\log(t_i) - \\mathbf{x}_i^\\top \\widehat{\\boldsymbol{\\beta}}\\}/\\widehat{\\sigma}$ for uncensored cases align closely on a normal quantile-quantile plot (correlation $0.995$), whereas the log-logistic analog yields a weaker alignment on a logistic quantile-quantile plot (correlation $0.972$). The Weibull AFT shows pronounced tail deviations on corresponding quantile plots.\nYou also compute information criteria for the fitted models:\n- Cox PH using partial likelihood: $\\mathrm{AIC}_{\\mathrm{Cox}} = 1420$.\n- Log-normal AFT (full likelihood): $\\mathrm{AIC}_{\\mathrm{LN}} = 1388$.\n- Log-logistic AFT (full likelihood): $\\mathrm{AIC}_{\\mathrm{LL}} = 1401$.\n- Weibull PH parameterization (full likelihood with PH link): $\\mathrm{AIC}_{\\mathrm{WPH}} = 1416$.\n\nUsing only the fundamental definitions above and the reported diagnostics, select the option that most correctly explains how the Accelerated Failure Time model serves as a non-PH alternative, and that demonstrates a valid, stepwise procedure to choose between PH and AFT for this dataset via residual diagnostics and information criteria, culminating in a justified model choice.\n\nA. The AFT model rescales time via $T^* = T \\exp(\\mathbf{x}^\\top \\boldsymbol{\\beta})$, which generally induces a time-dependent hazard ratio unless the baseline hazard $h_0(t)$ has a special power-law form (e.g., Weibull). A principled selection procedure is: first, assess the PH assumption by regressing scaled Schoenfeld residuals on functions of time and performing the Grambsch and Therneau test; nonzero slope for $Z$ with $p=0.0003$ implies PH violation for $Z$, corroborated by non-parallel log-minus-log curves. Next, fit candidate AFT families and evaluate model-specific residuals (e.g., standardized AFT errors against the assumed $W$ via quantile-quantile plots or Cox–Snell residuals against an exponential reference); the log-normal AFT residuals’ near-normal alignment (correlation $0.995$) suggests adequacy. Finally, compare Akaike Information Criteria across the Cox PH (partial likelihood) and parametric AFT fits; the lowest $\\mathrm{AIC}$, here $\\mathrm{AIC}_{\\mathrm{LN}}=1388$ versus $\\mathrm{AIC}_{\\mathrm{Cox}}=1420$ and $\\mathrm{AIC}_{\\mathrm{LL}}=1401$, supports the log-normal AFT as the preferred non-PH alternative for $Z$ while retaining $A$ and $S$ as time-invariant effects.\n\nB. The AFT model accelerates hazards, so if the log-rank test is significant, one should accept PH and choose the model with the highest concordance index; residuals and information criteria are secondary. Because the log-rank test is typically sensitive, proceed with the Cox PH model and ignore Schoenfeld residual trends; information criteria computed on partial versus full likelihoods are not comparable.\n\nC. The AFT model is equivalent to PH if the baseline distribution is log-normal. To choose, plot martingale residuals versus time to assess PH, and then select the model with the lowest Bayesian Information Criterion calculated directly from the Cox partial likelihood and AFT full likelihoods. Given the reported $p=0.0003$ for $Z$, pick the Weibull PH model because its hazards are constant over time.\n\nD. The AFT model changes the baseline hazard by an additive shift, so if the Grambsch and Therneau test shows any significance, refit a time-dependent Cox PH model with $Z \\times \\log(t)$ interactions and select the model with the smallest sum of deviance residuals; ignore information criteria comparisons across semiparametric and parametric models because they use different likelihoods. Given the residual patterns, prefer the Cox PH because it is more robust than parametric AFT models.", "solution": "The user has requested a rigorous validation of a problem statement concerning the comparison of Proportional Hazards (PH) and Accelerated Failure Time (AFT) models in survival analysis, followed by a detailed solution and evaluation of options.\n\n### Step 1: Extract Givens\n- **Problem Domain**: Randomized clinical trial, right-censored time-to-event data (time to disease progression).\n- **Variables**: $T$ (survival time), $\\delta$ (event indicator), $\\mathbf{x}$ (covariate vector), $Z$ (binary treatment indicator), $A$ (baseline age), $S$ (ordinal disease stage).\n- **Model Definitions**:\n    - **Cox PH Model**: $h(t \\mid \\mathbf{x}) = h_0(t) \\exp(\\mathbf{x}^\\top \\boldsymbol{\\beta})$, where the hazard ratio is constant over time.\n    - **AFT Model**: $\\log T = \\mathbf{x}^\\top \\boldsymbol{\\beta} + \\sigma W$, or equivalently $S(t \\mid \\mathbf{x}) = S_0\\!\\left(t \\exp(-\\mathbf{x}^\\top \\boldsymbol{\\beta})\\right)$.\n- **Dataset and Cox Model Diagnostics**:\n    - Sample size $n=800$, censoring rate $30\\%$. Covariates are $(Z,A,S)$.\n    - Grambsch and Therneau (GT) test on scaled Schoenfeld residuals versus $\\log(t)$:\n        - For $Z$: slope estimate $= 0.35$, $p$-value $= 0.0003$.\n        - For $A, S$: slopes are close to $0$, $p$-values $ 0.20$.\n    - A plot of $\\log\\{-\\log \\widehat{S}(t \\mid Z)\\}$ versus $\\log(t)$ shows \"clear non-parallelism and crossings\".\n- **AFT Model Diagnostics**:\n    - Three AFT models fitted: log-normal, log-logistic, Weibull.\n    - Standardized error residuals $\\widehat{\\epsilon}_i = \\{\\log(t_i) - \\mathbf{x}_i^\\top \\widehat{\\boldsymbol{\\beta}}\\}/\\widehat{\\sigma}$ checked via Q-Q plots:\n        - Log-normal AFT: Normal Q-Q plot correlation is $0.995$.\n        - Log-logistic AFT: Logistic Q-Q plot correlation is $0.972$.\n        - Weibull AFT: \"pronounced tail deviations\" on its corresponding Q-Q plot.\n- **Information Criteria**:\n    - $\\mathrm{AIC}_{\\mathrm{Cox}} = 1420$ (partial likelihood).\n    - $\\mathrm{AIC}_{\\mathrm{LN}} = 1388$ (log-normal AFT, full likelihood).\n    - $\\mathrm{AIC}_{\\mathrm{LL}} = 1401$ (log-logistic AFT, full likelihood).\n    - $\\mathrm{AIC}_{\\mathrm{WPH}} = 1416$ (Weibull PH, full likelihood).\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is based on standard, accepted principles of survival analysis in biostatistics. The Cox PH model, AFT models, Schoenfeld residuals, Grambsch-Therneau test, log-minus-log plots, model-specific residuals for parametric models, and Akaike Information Criterion (AIC) are all fundamental concepts and tools in this field. The definitions are correct. The problem is scientifically and mathematically sound.\n2.  **Well-Posed**: The problem provides a clear scenario, specific numerical results from diagnostic tests and model fits, and asks for a reasoned selection based on this evidence. The information is sufficient to perform the required reasoning and arrive at a single best answer among the options.\n3.  **Objective**: The problem is stated using precise, technical language. The data and diagnostic results are quantitative and objective. The task is to apply established statistical principles to this evidence, which is an objective process.\n\nThe problem statement does not exhibit any of the flaws listed in the instructions (e.g., scientific unsoundness, incompleteness, ambiguity). The comparison of AIC from a partial likelihood (Cox model) with AICs from full likelihoods (parametric models) is a recognized practice, although it has nuances. Presenting these values as givens is a valid simplification for the purposes of the problem.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A full solution will be derived.\n\n### Derivation and Option Analysis\n\nThe task is to identify the most correct explanation and procedure for choosing between a PH and an AFT model given the provided evidence. A principled approach involves three main stages: (1) assessing the assumptions of the initial model (Cox PH), (2) fitting plausible alternatives if assumptions are violated, and (3) comparing the competing models to select the best one.\n\n**1. Assessment of the Proportional Hazards Assumption**\n\nThe PH assumption states that the ratio of hazards for any two individuals is constant over time. The problem provides two key pieces of evidence regarding this assumption for the fitted Cox model.\n\n-   **Grambsch and Therneau (GT) Test**: This is a formal test for the PH assumption. It checks for a non-zero slope in a regression of the scaled Schoenfeld residuals against a function of time, commonly $t$ or $\\log(t)$. A significant p-value suggests that the corresponding covariate's effect changes over time, violating the PH assumption.\n    -   For the treatment indicator $Z$, the test yields a highly significant p-value of $p=0.0003$. This is strong statistical evidence against the PH assumption for the treatment effect. The positive slope ($0.35$) indicates that the hazard ratio for treatment increases as time progresses.\n    -   For age ($A$) and stage ($S$), the p-values are large ($p0.20$), suggesting that the PH assumption is appropriate for these covariates.\n\n-   **Log-minus-log Plot**: A plot of $\\log(-\\log(\\widehat{S}(t)))$ versus $\\log(t)$ for different strata of a covariate should yield parallel curves if the PH assumption holds for that covariate.\n    -   The problem states these curves, stratified by $Z$, show \"clear non-parallelism and crossings\". This is a classic visual representation of non-proportional hazards. Crossing curves imply that the hazard ratio is not only time-dependent but reverses direction (e.g., from $1$ to $1$) at some point in time.\n\n**Conclusion on PH assumption**: There is compelling evidence from both a formal statistical test and a graphical diagnostic that the proportional hazards assumption is violated for the treatment covariate $Z$. Therefore, the standard Cox PH model is misspecified for this dataset, and alternative models should be considered.\n\n**2. The AFT Model as a Non-PH Alternative**\n\nThe AFT model provides an alternative physical interpretation of covariate effects. Instead of multiplying the hazard, covariates act to accelerate or decelerate the time to event. The model is $\\log T = \\mathbf{x}^\\top \\boldsymbol{\\beta} + \\sigma W$, which is equivalent to $T = T_0 \\exp(\\mathbf{x}^\\top \\boldsymbol{\\beta})$, where $T_0 = \\exp(\\sigma W)$ is a baseline random variable for time.\n\nThe hazard function for an AFT model is $h(t \\mid \\mathbf{x}) = h_0(t \\exp(-\\mathbf{x}^\\top \\boldsymbol{\\beta})) \\exp(-\\mathbf{x}^\\top \\boldsymbol{\\beta})$. The hazard ratio for two individuals with covariates $\\mathbf{x}_1$ and $\\mathbf{x}_2$ is:\n$$\n\\mathrm{HR}(t) = \\frac{h(t \\mid \\mathbf{x}_1)}{h(t \\mid \\mathbf{x}_2)} = \\frac{h_0(t \\exp(-\\mathbf{x}_1^\\top \\boldsymbol{\\beta}))}{h_0(t \\exp(-\\mathbf{x}_2^\\top \\boldsymbol{\\beta}))} \\exp((\\mathbf{x}_2 - \\mathbf{x}_1)^\\top \\boldsymbol{\\beta})\n$$\nThis ratio is generally a function of time $t$, meaning the AFT model is inherently a non-PH model. The only exception is when the baseline hazard $h_0(t)$ follows a power law of time, $h_0(t) \\propto t^{\\alpha-1}$, which corresponds to the Weibull distribution. In this special case, the AFT model is also a PH model. For other common AFT distributions like log-normal or log-logistic, the PH property does not hold. Thus, AFT models represent a large, mechanistically interpretable class of non-PH models.\n\n**3. Selection Among Candidate Models**\n\nGiven the PH violation, the next step is to find a better-fitting model. The problem explores three parametric AFT models. The choice among them depends on two criteria: adequacy of the distributional assumption and overall model fit compared to other candidates.\n\n-   **Residual Diagnostics for AFT Models**: For a parametric AFT model, the assumed distribution of the error term $W$ in $\\log T = \\mathbf{x}^\\top \\boldsymbol{\\beta} + \\sigma W$ must be checked. Standardized error residuals, which are estimates of $W$, are compared to the theoretical quantiles of the assumed distribution via a Q-Q plot.\n    -   The **Log-normal AFT** shows a near-perfect fit on the normal Q-Q plot (correlation $0.995$), strongly supporting its distributional assumption.\n    -   The **Log-logistic AFT** shows a weaker fit (correlation $0.972$).\n    -   The **Weibull AFT** shows \"pronounced tail deviations,\" indicating a poor fit.\n    -   Based on residuals, the log-normal AFT is the clear winner.\n\n-   **Information Criteria for Model Comparison**: The Akaike Information Criterion (AIC) provides a relative measure of model quality, penalizing for the number of parameters to prevent overfitting. A lower AIC indicates a better model.\n    -   $\\mathrm{AIC}_{\\mathrm{Cox}} = 1420$\n    -   $\\mathrm{AIC}_{\\mathrm{LN}} = 1388$\n    -   $\\mathrm{AIC}_{\\mathrm{LL}} = 1401$\n    -   $\\mathrm{AIC}_{\\mathrm{WPH}} = 1416$\n    -   Comparing these values, the log-normal AFT model has the lowest AIC ($1388$) by a substantial margin. The difference $\\Delta \\mathrm{AIC} = \\mathrm{AIC}_{\\mathrm{Cox}} - \\mathrm{AIC}_{\\mathrm{LN}} = 32$ provides very strong support for the log-normal AFT model over the Cox PH model.\n\n**Overall Conclusion**: A systematic analysis leads to the following procedure and conclusion:\n1.  Check the PH assumption using the GT test and graphical methods. Here, it is clearly violated for $Z$.\n2.  Propose and fit scientifically sound alternatives, such as parametric AFT models.\n3.  Assess the fit of each AFT candidate using appropriate residual diagnostics (e.g., Q-Q plots of standardized error residuals). The log-normal AFT is found to be the best fit.\n4.  Compare all candidate models (including the initial Cox model) using a global fit criterion like AIC. The log-normal AFT is strongly preferred.\nThe final choice is the log-normal AFT model.\n\n### Option-by-Option Analysis\n\n**A. The AFT model rescales time via $T^* = T \\exp(\\mathbf{x}^\\top \\boldsymbol{\\beta})$, which generally induces a time-dependent hazard ratio unless the baseline hazard $h_0(t)$ has a special power-law form (e.g., Weibull). A principled selection procedure is: first, assess the PH assumption by regressing scaled Schoenfeld residuals on functions of time and performing the Grambsch and Therneau test; nonzero slope for $Z$ with $p=0.0003$ implies PH violation for $Z$, corroborated by non-parallel log-minus-log curves. Next, fit candidate AFT families and evaluate model-specific residuals (e.g., standardized AFT errors against the assumed $W$ via quantile-quantile plots or Cox–Snell residuals against an exponential reference); the log-normal AFT residuals’ near-normal alignment (correlation $0.995$) suggests adequacy. Finally, compare Akaike Information Criteria across the Cox PH (partial likelihood) and parametric AFT fits; the lowest $\\mathrm{AIC}$, here $\\mathrm{AIC}_{\\mathrm{LN}}=1388$ versus $\\mathrm{AIC}_{\\mathrm{Cox}}=1420$ and $\\mathrm{AIC}_{\\mathrm{LL}}=1401$, supports the log-normal AFT as the preferred non-PH alternative for $Z$ while retaining $A$ and $S$ as time-invariant effects.**\n\nThis option accurately describes the AFT model's relationship to the PH model, lays out a logically sound and standard stepwise procedure for model selection, correctly interprets all the diagnostic and comparative evidence provided in the problem statement, and reaches the correct conclusion. The statement about \"retaining $A$ and $S$ as time-invariant effects\" is acceptable shorthand for including them in the chosen model where their coefficients are constant. **Correct**.\n\n**B. The AFT model accelerates hazards, so if the log-rank test is significant, one should accept PH and choose the model with the highest concordance index; residuals and information criteria are secondary. Because the log-rank test is typically sensitive, proceed with the Cox PH model and ignore Schoenfeld residual trends; information criteria computed on partial versus full likelihoods are not comparable.**\n\nThis option is fundamentally flawed. (1) AFT models accelerate *time*, not hazards. (2) A significant log-rank test does not validate the PH assumption. (3) Ignoring formal tests of model assumptions (Schoenfeld residuals) is poor statistical practice. (4) Concordance is a measure of discrimination, not calibration or assumption validity. (5) Information criteria are essential for model comparison. **Incorrect**.\n\n**C. The AFT model is equivalent to PH if the baseline distribution is log-normal. To choose, plot martingale residuals versus time to assess PH, and then select the model with the lowest Bayesian Information Criterion calculated directly from the Cox partial likelihood and AFT full likelihoods. Given the reported $p=0.0003$ for $Z$, pick the Weibull PH model because its hazards are constant over time.**\n\nThis option contains multiple errors. (1) The AFT model is equivalent to PH for the **Weibull** distribution, not the log-normal. (2) While martingale residuals are useful, scaled Schoenfeld residuals are the most direct tool for assessing the PH assumption for specific covariates. (3) The problem gives AIC, not BIC. (4) The hazard function of a Weibull model is generally not constant over time, only its special case, the exponential distribution. Picking a PH model in the face of strong evidence against PH is illogical. **Incorrect**.\n\n**D. The AFT model changes the baseline hazard by an additive shift, so if the Grambsch and Therneau test shows any significance, refit a time-dependent Cox PH model with $Z \\times \\log(t)$ interactions and select the model with the smallest sum of deviance residuals; ignore information criteria comparisons across semiparametric and parametric models because they use different likelihoods. Given the residual patterns, prefer the Cox PH because it is more robust than parametric AFT models.**\n\nThis option is incorrect. (1) The AFT model is multiplicative on the time scale, not additive on the hazard scale (that's an additive hazards model). (2) Sum of deviance residuals is not a standard model selection criterion. (3) Ignoring AIC is unwarranted. (4) The final recommendation to prefer the Cox PH model directly contradicts the diagnostic evidence (\"residual patterns\") showing its primary assumption is violated. A model is not \"robust\" to violations of its own core assumptions. **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "4991128"}, {"introduction": "Perhaps the most challenging violation of proportional hazards occurs when hazard curves cross, indicating a treatment effect that changes direction over time. In such cases, standard summary measures fail, and a more sophisticated analysis strategy is required. This practice problem challenges you to select a robust framework that balances statistical efficiency with clinical interpretability, introducing the difference in Restricted Mean Survival Time (RMST) as a powerful and assumption-free alternative for summarizing treatment effects [@problem_id:4991174].", "problem": "A randomized, multicenter oncology trial compares an investigational therapy versus control on overall survival. Patients were randomized $1:1$ within centers, yielding $n=800$ participants across $J=20$ centers. Administrative censoring occurs at $T_{\\max}=24$ months; the observed number of events is $E \\approx 480$. Centers differ substantially in baseline mortality, with empirically observed center-specific baseline hazards differing by up to a factor of $3$ relative to one another, but there is no clinical rationale for a treatment-by-center interaction.\n\nLet $T$ denote survival time, $X$ the vector of baseline covariates including treatment $A \\in \\{0,1\\}$, and $Z$ a categorical indicator of center. The hazard function is $h(t \\mid X)$. The proportional hazards assumption posits $h(t \\mid X)=h_{0}(t)\\exp\\{\\beta^{\\top}X\\}$ for some baseline hazard $h_{0}(t)$ and constant regression coefficients $\\beta$. In a stratified Cox model by $Z$, the baseline hazard is allowed to vary by stratum while the regression coefficients are constrained to be common across strata.\n\nExploratory diagnostics show strong evidence against proportional hazards for treatment: scaled Schoenfeld residuals exhibit a monotone trend with time for $A$, and a global test for proportional hazards for $A$ yields $p0.001$. Visual inspection of estimated hazard ratio functions (using flexible, nonparametric smoothing of time-varying effects) suggests an early hazard ratio above $1$ that crosses below $1$ around $t \\approx 6$ months, with increasing separation thereafter, consistent with a delayed treatment effect. Weighted log-rank tests with late-event weights show more sensitivity than standard log-rank, corroborating delayed benefit. The clinical audience requests a single summary measure that is robust and easy to communicate for guideline development, but they also value a faithful depiction of time-varying effects. The investigators wish to adjust for center to address baseline hazard heterogeneity without estimating a high-dimensional set of nuisance parameters and to maintain efficiency.\n\nYou must select a decision framework that addresses the detected non-proportional hazards, balancing clinical interpretability, bias control under model misspecification, and statistical efficiency. Which option below best implements such a framework in this setting?\n\nA. Declare a stratified Cox model by center with a flexible time-varying coefficient for treatment using a penalized spline in $\\log t$ as the sole analysis. Report the time-varying hazard ratio curve with pointwise confidence intervals and summarize with a weighted average hazard ratio over time.\n\nB. Pre-specify the restricted mean survival time at $\\tau=24$ months as the primary estimand and report the adjusted difference in restricted mean survival time between treatment arms, using a covariate-adjusted method (for example, regression on pseudo-observations) that includes center to improve efficiency. Supplement this with a stratified Cox model by center with a flexible time-varying treatment coefficient to display and test time-varying effects. Commit to both the time horizon $\\tau$ and smoothing degrees of freedom before unblinding.\n\nC. Fit an unstratified Cox proportional hazards model with treatment as the only covariate and use a robust (sandwich) variance estimator to protect inferences; interpret the coefficient as an average causal hazard ratio. Do not adjust for center to avoid loss of efficiency from over-parameterization.\n\nD. To avoid bias from non-proportional hazards, stratify by treatment and present Kaplan–Meier curves for each arm, reporting the difference in medians if both are estimable. Rely on visual comparison for clinical interpretation.\n\nE. Replace the Cox model with a log-normal accelerated failure time model including a random intercept for center to capture heterogeneity. Report the acceleration factor as the primary estimand, which is a single summary number, and use likelihood-based inference for efficiency even if hazards cross.", "solution": "The user wants a meticulous and exacting professor in the STEM fields (mathematics, physics, chemistry, biology, engineering, and related disciplines) to critically validate the problem statement—excluding the options—before attempting a solution. Based on a valid statement, I will then derive the correct answer and evaluate each provided option.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Trial Design:** Randomized, multicenter oncology trial.\n*   **Comparison:** Investigational therapy versus control.\n*   **Endpoint:** Overall survival.\n*   **Randomization:** $1:1$ within centers.\n*   **Sample Size:** $n=800$ participants.\n*   **Number of Centers:** $J=20$.\n*   **Censoring:** Administrative censoring at $T_{\\max}=24$ months.\n*   **Events:** Observed number of events $E \\approx 480$.\n*   **Center Heterogeneity:** Center-specific baseline hazards differ by up to a factor of $3$.\n*   **Interaction Assumption:** No clinical rationale for a treatment-by-center interaction.\n*   **Variable Definitions:**\n    *   $T$: Survival time.\n    *   $X$: Vector of baseline covariates, including treatment $A \\in \\{0,1\\}$.\n    *   $Z$: Categorical indicator of center.\n*   **Model Definitions:**\n    *   Hazard function: $h(t \\mid X)$.\n    *   Proportional Hazards (PH) Model: $h(t \\mid X)=h_{0}(t)\\exp\\{\\beta^{\\top}X\\}$.\n    *   Stratified Cox Model by $Z$: Baseline hazard $h_{0j}(t)$ varies by stratum $j$, while coefficients $\\beta$ are common.\n*   **Diagnostic Findings (Violation of PH for Treatment A):**\n    *   Scaled Schoenfeld residuals show a monotone trend with time.\n    *   Global test for PH for $A$ gives $p0.001$.\n    *   Estimated hazard ratio (HR) function is time-varying: HR $1$ for early $t$, crosses below $1$ at $t \\approx 6$ months, and then decreases, indicating a delayed treatment effect (crossing hazards).\n    *   Weighted log-rank tests with late-event weights are more sensitive than standard log-rank.\n*   **Analysis Objectives:**\n    1.  Provide a single, robust, and easily communicable summary measure for clinical guidelines.\n    2.  Provide a faithful depiction of the time-varying effects.\n    3.  Adjust for center to address baseline hazard heterogeneity.\n    4.  Avoid estimating a high-dimensional set of nuisance parameters for centers.\n    5.  Maintain statistical efficiency.\n\n**Step 2: Validate Using Extracted Givens**\n\n*   **Scientifically Grounded:** The problem describes a standard clinical trial scenario in oncology. All statistical concepts (randomization, censoring, Cox model, proportional hazards, Schoenfeld residuals, stratified analysis, log-rank tests, restricted mean survival time, accelerated failure time models) are established principles in biostatistics. The phenomenon of non-proportional, crossing hazards is a well-documented and critical issue in modern oncology trials, particularly with immunotherapies. The problem is firmly grounded in scientific and statistical practice.\n*   **Well-Posed:** The task is to select the best statistical analysis framework from a set of options that balances multiple, clearly stated, and realistic objectives (robust summary, accurate depiction of effects, efficiency, etc.) in the face of a specific data challenge (crossing hazards). This is a well-posed decision-theory problem within the domain of applied statistics.\n*   **Objective:** The problem statement is presented in precise, objective, and technical language. The requirements of the stakeholders are listed as constraints on the solution, not as subjective opinions.\n*   **Completeness and Consistency:** The problem is self-contained and provides sufficient detail to evaluate the proposed statistical strategies. The various requirements, while representing competing priorities (e.g., simplicity vs. completeness), are not logically contradictory but reflect the real-world trade-offs inherent in statistical analysis.\n*   **Realism:** The parameters ($n=800$, $J=20$, $E \\approx 480$, $T_{\\max}=24$ months) are entirely realistic for a Phase III oncology trial. The described findings are plausible and present a common, non-trivial challenge.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is a well-formulated, realistic, and scientifically sound problem in applied biostatistics. I will proceed with the solution and evaluation of the options.\n\n### Solution Derivation\n\nThe central challenge is the definitive violation of the proportional hazards (PH) assumption for the treatment effect, characterized by crossing hazard curves. This implies that the hazard ratio, $\\text{HR}(t)$, is not constant. Specifically, the therapy appears initially harmful (or less effective than control, $\\text{HR}(t)1$) and becomes beneficial later ($\\text{HR}(t)1$). In such cases, a single hazard ratio from a standard Cox PH model is a poor summary; it represents a weighted average of $\\log(\\text{HR}(t))$ that can be close to $0$ (implying an HR near $1$), masking the true dynamics of early harm and late benefit.\n\nThe analysis framework must therefore satisfy several key criteria derived from the problem statement:\n1.  **Robustness to NPH:** The primary summary measure must be interpretable and not biased by the non-proportionality.\n2.  **Interpretability:** The summary measure must be a single number that is easy for clinicians to understand and use for guidelines (e.g., \"how much survival time is gained?\").\n3.  **Completeness:** The framework should also visualize or describe the time-varying nature of the effect, as requested.\n4.  **Center Adjustment:** It must account for the substantial heterogeneity in baseline hazards across the $J=20$ centers efficiently. A stratified analysis is an ideal approach for this, as it allows each center to have its own baseline hazard function, $h_{0j}(t)$, without estimating $J-1$ extra parameters, thus preserving power.\n\nWe will now evaluate each option against these criteria.\n\n**Option A: Declare a stratified Cox model by center with a flexible time-varying coefficient for treatment using a penalized spline in $\\log t$ as the sole analysis. Report the time-varying hazard ratio curve with pointwise confidence intervals and summarize with a weighted average hazard ratio over time.**\n\n*   This approach correctly uses stratification by center to handle baseline hazard heterogeneity efficiently. It also directly models the non-proportional hazards using a time-varying coefficient, $\\beta(t)$, which addresses the need to depict the effect over time.\n*   However, the proposed summary measure—a weighted average hazard ratio—is problematic. When $\\text{HR}(t)$ crosses $1$, any average can be highly misleading. For instance, an average HR could be $1.0$, suggesting no effect, while hiding significant early harm and late benefit. The choice of weights is also arbitrary and can influence the result. This fails the criterion for a robust and easily communicable summary measure.\n*   **Verdict: Incorrect.** While it correctly models the time-varying effect and stratifies by center, the proposed summary measure is non-robust and difficult to interpret in a crossing hazards scenario.\n\n**Option B: Pre-specify the restricted mean survival time at $\\tau=24$ months as the primary estimand and report the adjusted difference in restricted mean survival time between treatment arms, using a covariate-adjusted method (for example, regression on pseudo-observations) that includes center to improve efficiency. Supplement this with a stratified Cox model by center with a flexible time-varying treatment coefficient to display and test time-varying effects. Commit to both the time horizon $\\tau$ and smoothing degrees of freedom before unblinding.**\n\n*   This option proposes the difference in Restricted Mean Survival Time (RMST) as the primary summary measure. The RMST up to a time horizon $\\tau$ is the area under the survival curve $S(t)$ from $0$ to $\\tau$, i.e., $\\text{RMST}(\\tau) = \\int_0^\\tau S(t) dt$. The difference in RMST between arms, $\\Delta(\\tau)$, has a direct and highly intuitive clinical interpretation: the average survival time gained (or lost) on treatment up to time $\\tau$. This estimand does not rely on the PH assumption and is robust to crossing hazards.\n*   Setting the time horizon $\\tau=24$ months is appropriate as it corresponds to the maximum administrative follow-up time $T_{\\max}$, ensuring all observed data contribute to the estimate.\n*   It addresses center heterogeneity by including center as a covariate in an adjusted RMST analysis, which is known to improve statistical efficiency.\n*   Crucially, it also satisfies the second requirement of the stakeholders by supplementing the primary RMST analysis with a flexible, time-varying coefficient Cox model (stratified by center) to visualize the HR curve, thereby providing a faithful depiction of the delayed effect.\n*   This dual approach perfectly balances the need for a single robust summary measure with the need for a detailed characterization of the effect over time. The mention of pre-specification is a hallmark of a rigorous analysis plan.\n*   **Verdict: Correct.** This framework is comprehensive, robust, and directly addresses all specified requirements and constraints of the problem.\n\n**Option C: Fit an unstratified Cox proportional hazards model with treatment as the only covariate and use a robust (sandwich) variance estimator to protect inferences; interpret the coefficient as an average causal hazard ratio. Do not adjust for center to avoid loss of efficiency from over-parameterization.**\n\n*   This approach is fundamentally flawed. It knowingly applies a misspecified model (the PH model) to data with strong evidence of NPH. The resulting single HR is a misleading average, as discussed for Option A. A robust variance estimator corrects standard errors for model misspecification but does not fix the bias or poor interpretability of the coefficient estimate itself.\n*   It explicitly advises against adjusting for center. This contradicts the problem's statement of substantial center heterogeneity and the investigators' desire for adjustment. Stratified randomization was used, and failing to account for the stratification variable in the analysis can lead to a loss of efficiency and power. The justification (\"avoid loss of efficiency from over-parameterization\") is incorrect; stratification by center *avoids* over-parameterization by not estimating center-specific coefficients.\n*   **Verdict: Incorrect.** This option ignores the primary data phenomenon (NPH), fails to meet a key analysis objective (adjust for center), and provides a flawed justification for its choices.\n\n**Option D: To avoid bias from non-proportional hazards, stratify by treatment and present Kaplan–Meier curves for each arm, reporting the difference in medians if both are estimable. Rely on visual comparison for clinical interpretation.**\n\n*   Presenting Kaplan-Meier (K-M) curves is a good exploratory step, but as a final analysis, it is incomplete. \"Stratify by treatment\" is imprecise phrasing for \"generate K-M curves for each treatment group\".\n*   The proposed summary measure is the difference in median survival times. With an event rate of $E/n \\approx 480/800 = 60\\%$, the median ($50\\%$ quantile) might be estimable for the control arm but may not be reached in the treatment arm by $T_{\\max}=24$ months if there is a substantial late benefit. In such cases, the median is not estimable, and no summary measure can be reported. The RMST (Option B) avoids this problem as it is always estimable for a given $\\tau$.\n*   This option fails to adjust for center effects, thus ignoring a major source of heterogeneity and forgoing an opportunity to increase statistical power and precision, which was a stated goal.\n*   **Verdict: Incorrect.** This approach is insufficient. It fails to provide a guaranteed summary measure, and it does not perform the requested and necessary adjustment for center.\n\n**Option E: Replace the Cox model with a log-normal accelerated failure time model including a random intercept for center to capture heterogeneity. Report the acceleration factor as the primary estimand, which is a single summary number, and use likelihood-based inference for efficiency even if hazards cross.**\n\n*   An Accelerated Failure Time (AFT) model is a valid alternative to a PH model. It models the effect of covariates as a multiplicative factor on the time scale. The log-normal AFT model is one specific parametric choice. Using a random intercept for center is a valid way to handle clustering, analogous to stratification in the Cox model.\n*   The main issue is that this approach replaces one strong assumption (proportional hazards) with another (a specific parametric AFT relationship). There is no evidence in the problem statement that the AFT assumption is more appropriate than the PH assumption; in fact, the complex crossing hazard pattern might violate the AFT assumption as well. While some AFT models can produce crossing hazards, it is not their typical behavior, and the single \"acceleration factor\" summary would still fail to capture the qualitative change in the treatment effect over time.\n*   This option fails to provide a \"faithful depiction of time-varying effects\"; it instead summarizes the effect with a single number based on a strong, unverified modeling assumption. The RMST approach in Option B is superior because it is largely assumption-free.\n*   **Verdict: Incorrect.** This option is a reasonable but inferior alternative to Option B. It imposes a strong parametric assumption that may be violated and does not fulfill the requirement of depicting the time-varying nature of the effect.\n\n**Conclusion:**\n\nOption B provides the most statistically sound and complete framework. It uses the RMST difference as a primary estimand, which is robust, interpretable, and well-suited for NPH scenarios with crossing hazards. It complements this robust summary with a flexible graphical analysis to satisfy the clinical need to understand the timing of the treatment effect. Finally, it correctly and efficiently handles the multisite structure of the data.", "answer": "$$\\boxed{B}$$", "id": "4991174"}]}