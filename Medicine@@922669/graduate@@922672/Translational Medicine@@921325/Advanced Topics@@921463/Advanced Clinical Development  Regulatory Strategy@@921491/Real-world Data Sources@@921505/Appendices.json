{"hands_on_practices": [{"introduction": "Real-world data sources like EHRs and administrative claims provide a wealth of information, but this data is rarely analysis-ready. A crucial first step in any translational study is to process raw data, such as diagnostic codes, into meaningful variables for risk adjustment and analysis. This exercise [@problem_id:5054781] walks you through the fundamental process of creating a comorbidity score from scratch, mirroring how a researcher might construct the Charlson Comorbidity Index from ICD-10 codes. By completing this task, you will gain hands-on experience with feature engineering and confront the critical distinction between a variable's utility for prediction versus its role in a causal-inference framework.", "problem": "You are given a translational medicine scenario in which comorbidities are observed via International Classification of Diseases, Tenth Revision (ICD-10) codes originating from Electronic Health Records (EHRs), claims data, and disease registries. The scientific aim is to construct an interpretable risk summary using the Charlson Comorbidity Index and evaluate its predictive association with one-year mortality using fundamental statistical definitions. Start from the following fundamental bases: (i) the unadjusted Charlson Comorbidity Index (CCI) as a weighted sum of distinct comorbidity categories, (ii) the Pearson correlation between two random variables defined by $$\\rho_{XY} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y},$$ and (iii) the distinction between prediction and causation formulated using structural assumptions rather than empirical correlation. You must not assume any shortcut formulas beyond these bases.\n\nInput specification and comorbidity coding: You will use a small test suite with $n = 10$ patients. Each patient has a set of ICD-10 codes and a binary one-year mortality label $Y_i \\in \\{0,1\\}$, where $Y_i = 1$ indicates death by $1$ year. The Charlson Comorbidity Index score for patient $i$ is defined as $$S_i = \\sum_{c \\in \\mathcal{C}_i} w(c),$$ where $\\mathcal{C}_i$ is the set of unique Charlson categories present for patient $i$ after applying hierarchical rules, and $w(c)$ is the category weight. A category should contribute its weight at most once per patient regardless of the number of codes mapped to that category. Hierarchical rules enforce that when a severe form of a condition is present, less severe forms of the same condition do not contribute additional weight. Specifically:\n- If diabetes with end-organ damage is present, the uncomplicated diabetes category does not contribute.\n- If moderate or severe liver disease is present, the mild liver disease category does not contribute.\n- If metastatic solid tumor is present, the non-metastatic tumor category does not contribute.\n\nMapping from ICD-10 codes to Charlson categories and weights is restricted to the following clinically standard subset used in the test data. A patient’s code maps to a Charlson category if the code string begins with one of the listed prefixes. Weights $w(c)$ are as follows:\n- Myocardial infarction: prefixes $\\{\\text{\"I21\"}, \\text{\"I22\"}, \\text{\"I25.2\"}\\}$ with weight $w = 1$.\n- Congestive heart failure: prefixes $\\{\\text{\"I50\"}\\}$ with weight $w = 1$.\n- Peripheral vascular disease: prefixes $\\{\\text{\"I70.2\"}, \\text{\"I73.9\"}\\}$ with weight $w = 1$.\n- Cerebrovascular disease: prefixes $\\{\\text{\"I63\"}\\}$ with weight $w = 1$.\n- Dementia: prefixes $\\{\\text{\"F03\"}\\}$ with weight $w = 1$.\n- Chronic obstructive pulmonary disease: prefixes $\\{\\text{\"J44\"}\\}$ with weight $w = 1$.\n- Connective tissue disease: prefixes $\\{\\text{\"M32\"}\\}$ with weight $w = 1$.\n- Peptic ulcer disease: prefixes $\\{\\text{\"K25\"}\\}$ with weight $w = 1$.\n- Mild liver disease: prefixes $\\{\\text{\"K76.0\"}, \\text{\"K73\"}\\}$ with weight $w = 1$.\n- Diabetes without complications: prefixes $\\{\\text{\"E11\"}\\}$ with weight $w = 1$ unless diabetes with complications is present.\n- Diabetes with end-organ damage: prefixes $\\{\\text{\"E11.22\"}, \\text{\"E11.4\"}\\}$ with weight $w = 2$.\n- Hemiplegia or paraplegia: prefixes $\\{\\text{\"G81\"}\\}$ with weight $w = 2$.\n- Moderate or severe renal disease: prefixes $\\{\\text{\"N18.5\"}\\}$ with weight $w = 2$.\n- Any tumor (non-metastatic): prefixes $\\{\\text{\"C50\"}\\}$ with weight $w = 2$ unless metastatic tumor is present.\n- Leukemia: prefixes $\\{\\text{\"C91\"}\\}$ with weight $w = 2$.\n- Lymphoma: prefixes $\\{\\text{\"C85\"}\\}$ with weight $w = 2$.\n- Moderate or severe liver disease: prefixes $\\{\\text{\"K72.9\"}\\}$ with weight $w = 3$.\n- Metastatic solid tumor: prefixes $\\{\\text{\"C78\"}\\}$ with weight $w = 6$.\n- Acquired immune deficiency syndrome: prefixes $\\{\\text{\"B20\"}\\}$ with weight $w = 6$.\n\nTest suite (each element is a patient’s code list and mortality $Y$):\n- Patient $1$: codes $\\varnothing$, $Y_1 = 0$.\n- Patient $2$: codes $\\{\\text{\"I21.0\"}\\}$, $Y_2 = 0$.\n- Patient $3$: codes $\\{\\text{\"I50.9\"}, \\text{\"I50.2\"}, \\text{\"J44.9\"}\\}$, $Y_3 = 0$.\n- Patient $4$: codes $\\{\\text{\"E11.9\"}, \\text{\"I70.20\"}\\}$, $Y_4 = 0$.\n- Patient $5$: codes $\\{\\text{\"E11.22\"}, \\text{\"N18.5\"}, \\text{\"G81.9\"}\\}$, $Y_5 = 1$.\n- Patient $6$: codes $\\{\\text{\"C50.9\"}\\}$, $Y_6 = 0$.\n- Patient $7$: codes $\\{\\text{\"C78.7\"}, \\text{\"C50.9\"}\\}$, $Y_7 = 1$.\n- Patient $8$: codes $\\{\\text{\"K76.0\"}, \\text{\"K72.90\"}\\}$, $Y_8 = 1$.\n- Patient $9$: codes $\\{\\text{\"B20\"}, \\text{\"J44.9\"}, \\text{\"I63.9\"}\\}$, $Y_9 = 1$.\n- Patient $10$: codes $\\{\\text{\"F03.90\"}, \\text{\"M32.10\"}, \\text{\"K25.9\"}, \\text{\"I73.9\"}\\}$, $Y_{10} = 0$.\n\nTasks:\n1. Compute the Charlson Comorbidity Index scores $S_i$ for $i \\in \\{1,\\dots,10\\}$ by applying the mapping and hierarchical rules. Ensure that multiple codes mapping to the same category contribute weight only once.\n2. Compute the Pearson correlation $\\rho_{SY}$ between the score vector $S = (S_1,\\dots,S_{10})$ and the mortality vector $Y = (Y_1,\\dots,Y_{10})$, using the definition $$\\rho_{SY} = \\frac{\\sum_{i=1}^{n} (S_i - \\bar{S})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^{n} (S_i - \\bar{S})^2} \\, \\sqrt{\\sum_{i=1}^{n} (Y_i - \\bar{Y})^2}},$$ where $\\bar{S}$ and $\\bar{Y}$ are sample means.\n3. Interpret the predictive versus causal uses by returning two booleans computed as follows: define $\\text{predictive\\_valid} = \\text{True}$ if $\\sigma_S > 0$, $\\sigma_Y > 0$, and $\\rho_{SY} > 0$, otherwise $\\text{False}$; define $\\text{causal\\_warranted} = \\text{True}$ only if the data include an exposure variable $A$ and measured covariates $Z$ such that a valid adjustment set renders $Y \\perp\\!\\!\\!\\perp A \\mid Z$ under a justified data-generating mechanism; in this test suite there is no exposure variable, so return $\\text{False}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, formatted exactly as $$\\text{\"[[s_1,\\dots,s_{10}],\\rho,\\text{predictive\\_valid},\\text{causal\\_warranted}]\"},$$ where each $s_i$ is an integer, $\\rho$ is a float, and the last two entries are booleans.\n\nThe test suite is designed for coverage:\n- Happy path: patients with multiple distinct comorbidities and a mix of outcomes.\n- Boundary conditions: a patient with no codes ($S_i = 0$), duplicated codes that map to the same category (only counted once), and categories with hierarchical dominance (severe forms suppress less severe forms).\n- Edge cases: concurrent presence of metastatic tumor and non-metastatic tumor, diabetes with and without complications, and mild versus severe liver disease.\n\nNo physical units or angle units are involved. Express any proportion internally as decimals. Your output must follow the exact format specified above.", "solution": "The problem is deemed valid as it is scientifically grounded in established biostatistical methods, well-posed with a complete and consistent set of data and rules, and expressed in objective, formal language. The tasks are computationally feasible and lead to a unique, meaningful solution.\n\nThe solution proceeds in three steps as specified: (1) calculation of the Charlson Comorbidity Index (CCI) scores for a cohort of $10$ patients, (2) computation of the Pearson correlation coefficient between the CCI scores and $1$-year mortality, and (3) interpretation of the results from a predictive versus causal standpoint.\n\n### Step 1: Charlson Comorbidity Index (CCI) Score Calculation\n\nThe CCI score for each patient, $S_i$, is the sum of weights for their unique comorbidity categories, $S_i = \\sum_{c \\in \\mathcal{C}_i} w(c)$. The set of unique categories, $\\mathcal{C}_i$, is determined by mapping a patient's ICD-10 codes to predefined Charlson categories, ensuring each category contributes its weight at most once, and applying specified hierarchical rules where severe conditions suppress the contribution of milder forms.\n\nThe mapping from ICD-10 prefixes to category and weight $w(c)$ is provided. We apply this to each of the $n=10$ patients.\n\n- **Patient 1**: Codes: $\\varnothing$, Mortality $Y_1=0$.\n  - No codes are present.\n  - The set of categories is empty, $\\mathcal{C}_1 = \\varnothing$.\n  - The score is $S_1 = 0$.\n\n- **Patient 2**: Codes: $\\{\\text{\"I21.0\"}\\}$, Mortality $Y_2=0$.\n  - The code `\"I21.0\"` maps to the 'Myocardial infarction' category ($w=1$).\n  - $\\mathcal{C}_2 = \\{\\text{Myocardial infarction}\\}$.\n  - The score is $S_2 = 1$.\n\n- **Patient 3**: Codes: $\\{\\text{\"I50.9\"}, \\text{\"I50.2\"}, \\text{\"J44.9\"}\\}$, Mortality $Y_3=0$.\n  - Both `\"I50.9\"` and `\"I50.2\"` map to 'Congestive heart failure' ($w=1$). The code `\"J44.9\"` maps to 'Chronic obstructive pulmonary disease' ($w=1$).\n  - The set of unique categories is $\\mathcal{C}_3 = \\{\\text{Congestive heart failure}, \\text{Chronic obstructive pulmonary disease}\\}$.\n  - The score is $S_3 = 1 + 1 = 2$.\n\n- **Patient 4**: Codes: $\\{\\text{\"E11.9\"}, \\text{\"I70.20\"}\\}$, Mortality $Y_4=0$.\n  - `\"E11.9\"` maps to 'Diabetes without complications' ($w=1$). `\"I70.20\"` maps to 'Peripheral vascular disease' ($w=1$).\n  - No hierarchical rules apply. $\\mathcal{C}_4 = \\{\\text{Diabetes without complications}, \\text{Peripheral vascular disease}\\}$.\n  - The score is $S_4 = 1 + 1 = 2$.\n\n- **Patient 5**: Codes: $\\{\\text{\"E11.22\"}, \\text{\"N18.5\"}, \\text{\"G81.9\"}\\}$, Mortality $Y_5=1$.\n  - `\"E11.22\"` maps to 'Diabetes with end-organ damage' ($w=2$). `\"N18.5\"` maps to 'Moderate or severe renal disease' ($w=2$). `\"G81.9\"` maps to 'Hemiplegia or paraplegia' ($w=2$).\n  - The patient's codes might also map to 'Diabetes without complications' via the prefix `\"E11\"`. However, the hierarchical rule states that the presence of 'Diabetes with end-organ damage' suppresses the weight from the uncomplicated form.\n  - $\\mathcal{C}_5 = \\{\\text{Diabetes with end-organ damage}, \\text{Moderate or severe renal disease}, \\text{Hemiplegia or paraplegia}\\}$.\n  - The score is $S_5 = 2 + 2 + 2 = 6$.\n\n- **Patient 6**: Codes: $\\{\\text{\"C50.9\"}\\}$, Mortality $Y_6=0$.\n  - `\"C50.9\"` maps to 'Any tumor (non-metastatic)' ($w=2$).\n  - $\\mathcal{C}_6 = \\{\\text{Any tumor}\\}$.\n  - The score is $S_6 = 2$.\n\n- **Patient 7**: Codes: $\\{\\text{\"C78.7\"}, \\text{\"C50.9\"}\\}$, Mortality $Y_7=1$.\n  - `\"C78.7\"` maps to 'Metastatic solid tumor' ($w=6$). `\"C50.9\"` maps to 'Any tumor (non-metastatic)' ($w=2$).\n  - The hierarchical rule for tumors applies: the presence of 'Metastatic solid tumor' suppresses the contribution from 'Any tumor'.\n  - $\\mathcal{C}_7 = \\{\\text{Metastatic solid tumor}\\}$.\n  - The score is $S_7 = 6$.\n\n- **Patient 8**: Codes: $\\{\\text{\"K76.0\"}, \\text{\"K72.90\"}\\}$, Mortality $Y_8=1$.\n  - `\"K76.0\"` maps to 'Mild liver disease' ($w=1$). `\"K72.90\"` maps to 'Moderate or severe liver disease' ($w=3$).\n  - The hierarchical rule for liver disease applies: 'Moderate or severe liver disease' suppresses 'Mild liver disease'.\n  - $\\mathcal{C}_8 = \\{\\text{Moderate or severe liver disease}\\}$.\n  - The score is $S_8 = 3$.\n\n- **Patient 9**: Codes: $\\{\\text{\"B20\"}, \\text{\"J44.9\"}, \\text{\"I63.9\"}\\}$, Mortality $Y_9=1$.\n  - `\"B20\"` maps to 'Acquired immune deficiency syndrome' ($w=6$). `\"J44.9\"` maps to 'Chronic obstructive pulmonary disease' ($w=1$). `\"I63.9\"` maps to 'Cerebrovascular disease' ($w=1$).\n  - $\\mathcal{C}_9 = \\{\\text{AIDS}, \\text{COPD}, \\text{Cerebrovascular disease}\\}$.\n  - The score is $S_9 = 6 + 1 + 1 = 8$.\n\n- **Patient 10**: Codes: $\\{\\text{\"F03.90\"}, \\text{\"M32.10\"}, \\text{\"K25.9\"}, \\text{\"I73.9\"}\\}$, Mortality $Y_{10}=0$.\n  - `\"F03.90\"` $\\to$ 'Dementia' ($w=1$). `\"M32.10\"` $\\to$ 'Connective tissue disease' ($w=1$). `\"K25.9\"` $\\to$ 'Peptic ulcer disease' ($w=1$). `\"I73.9\"` $\\to$ 'Peripheral vascular disease' ($w=1$).\n  - $\\mathcal{C}_{10} = \\{\\text{Dementia}, \\text{Connective tissue disease}, \\text{Peptic ulcer disease}, \\text{Peripheral vascular disease}\\}$.\n  - The score is $S_{10} = 1 + 1 + 1 + 1 = 4$.\n\nThe resulting vector of CCI scores is $S = (0, 1, 2, 2, 6, 2, 6, 3, 8, 4)$.\n\n### Step 2: Pearson Correlation Calculation\n\nWe compute the Pearson correlation coefficient $\\rho_{SY}$ between the score vector $S$ and the mortality vector $Y = (0, 0, 0, 0, 1, 0, 1, 1, 1, 0)$ using the provided formula:\n$$ \\rho_{SY} = \\frac{\\sum_{i=1}^{n} (S_i - \\bar{S})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^{n} (S_i - \\bar{S})^2} \\, \\sqrt{\\sum_{i=1}^{n} (Y_i - \\bar{Y})^2}} $$\nFirst, we calculate the sample means for the $n=10$ patients:\n- $\\bar{S} = \\frac{1}{10}\\sum_{i=1}^{10} S_i = \\frac{0+1+2+2+6+2+6+3+8+4}{10} = \\frac{34}{10} = 3.4$\n- $\\bar{Y} = \\frac{1}{10}\\sum_{i=1}^{10} Y_i = \\frac{0+0+0+0+1+0+1+1+1+0}{10} = \\frac{4}{10} = 0.4$\n\nNext, we compute the terms required for the formula:\n- The sum of cross-products of deviations (numerator):\n$$ \\sum_{i=1}^{10} (S_i - \\bar{S})(Y_i - \\bar{Y}) = (0-3.4)(0-0.4) + \\dots + (4-3.4)(0-0.4) = 9.4 $$\n- The sum of squared deviations for $S$:\n$$ \\sum_{i=1}^{10} (S_i - \\bar{S})^2 = (0-3.4)^2 + (1-3.4)^2 + \\dots + (4-3.4)^2 = 58.4 $$\n- The sum of squared deviations for $Y$:\n$$ \\sum_{i=1}^{10} (Y_i - \\bar{Y})^2 = 6 \\times (0-0.4)^2 + 4 \\times (1-0.4)^2 = 6 \\times 0.16 + 4 \\times 0.36 = 2.4 $$\nSubstituting these values into the correlation formula:\n$$ \\rho_{SY} = \\frac{9.4}{\\sqrt{58.4} \\cdot \\sqrt{2.4}} = \\frac{9.4}{\\sqrt{140.16}} \\approx 0.79400003 $$\nThe positive correlation of $\\approx 0.794$ indicates a strong positive linear association between the calculated CCI score and $1$-year mortality in this sample.\n\n### Step 3: Predictive versus Causal Interpretation\n\nThe final task is to evaluate two booleans based on the results and the problem's framing.\n\n1.  **$\\text{predictive\\_valid}$**: This is defined as $\\text{True}$ if the standard deviations of both variables are positive ($\\sigma_S > 0, \\sigma_Y > 0$) and the correlation is positive ($\\rho_{SY} > 0$).\n    - The variance of $S$ is non-zero because $\\sum(S_i - \\bar{S})^2 = 58.4 > 0$, so $\\sigma_S > 0$.\n    - The variance of $Y$ is non-zero because $\\sum(Y_i - \\bar{Y})^2 = 2.4 > 0$, so $\\sigma_Y > 0$.\n    - The correlation $\\rho_{SY} \\approx 0.794 > 0$.\n    - All three conditions are met, thus $\\text{predictive\\_valid} = \\text{True}$. This signifies that the CCI score has predictive value for mortality in this dataset.\n\n2.  **$\\text{causal\\_warranted}$**: This is defined as $\\text{True}$ only under specific structural conditions, namely the availability of an exposure $A$ and a set of measured covariates $Z$ that allow for causal identification (e.g., via the back-door criterion). The problem statement explicitly notes that the test suite does not contain these elements. The correlation $\\rho_{SY}$ is a measure of association, not causation. Without a formal causal model and the necessary data to control for confounding, it is unwarranted to claim that a higher CCI score *causes* a higher risk of mortality based on this analysis alone. Therefore, as specified, $\\text{causal\\_warranted} = \\text{False}$.\n\nCombining these results yields the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes Charlson Comorbidity Index scores, their correlation with mortality,\n    and interprets the results for predictive vs. causal validity.\n    \"\"\"\n    \n    # Define the mapping from ICD-10 prefixes to Charlson categories, weights,\n    # and hierarchical suppression rules.\n    categories = {\n        'mi': {'weight': 1, 'prefixes': {\"I21\", \"I22\", \"I25.2\"}},\n        'chf': {'weight': 1, 'prefixes': {\"I50\"}},\n        'pvd': {'weight': 1, 'prefixes': {\"I70.2\", \"I73.9\"}},\n        'cvd': {'weight': 1, 'prefixes': {\"I63\"}},\n        'dementia': {'weight': 1, 'prefixes': {\"F03\"}},\n        'copd': {'weight': 1, 'prefixes': {\"J44\"}},\n        'ctd': {'weight': 1, 'prefixes': {\"M32\"}},\n        'pud': {'weight': 1, 'prefixes': {\"K25\"}},\n        'mild_liver': {'weight': 1, 'prefixes': {\"K76.0\", \"K73\"}, 'suppressed_by': 'mod_sev_liver'},\n        'dm_uncomp': {'weight': 1, 'prefixes': {\"E11\"}, 'suppressed_by': 'dm_comp'},\n        'dm_comp': {'weight': 2, 'prefixes': {\"E11.22\", \"E11.4\"}},\n        'hemiplegia': {'weight': 2, 'prefixes': {\"G81\"}},\n        'mod_sev_renal': {'weight': 2, 'prefixes': {\"N18.5\"}},\n        'tumor': {'weight': 2, 'prefixes': {\"C50\"}, 'suppressed_by': 'meta_tumor'},\n        'leukemia': {'weight': 2, 'prefixes': {\"C91\"}},\n        'lymphoma': {'weight': 2, 'prefixes': {\"C85\"}},\n        'mod_sev_liver': {'weight': 3, 'prefixes': {\"K72.9\"}},\n        'meta_tumor': {'weight': 6, 'prefixes': {\"C78\"}},\n        'aids': {'weight': 6, 'prefixes': {\"B20\"}},\n    }\n\n    # Define the test suite data from the problem statement.\n    test_suite = [\n        ({'codes': [], 'mortality': 0}),\n        ({'codes': {\"I21.0\"}, 'mortality': 0}),\n        ({'codes': {\"I50.9\", \"I50.2\", \"J44.9\"}, 'mortality': 0}),\n        ({'codes': {\"E11.9\", \"I70.20\"}, 'mortality': 0}),\n        ({'codes': {\"E11.22\", \"N18.5\", \"G81.9\"}, 'mortality': 1}),\n        ({'codes': {\"C50.9\"}, 'mortality': 0}),\n        ({'codes': {\"C78.7\", \"C50.9\"}, 'mortality': 1}),\n        ({'codes': {\"K76.0\", \"K72.90\"}, 'mortality': 1}),\n        ({'codes': {\"B20\", \"J44.9\", \"I63.9\"}, 'mortality': 1}),\n        ({'codes': {\"F03.90\", \"M32.10\", \"K25.9\", \"I73.9\"}, 'mortality': 0}),\n    ]\n    \n    # Task 1: Compute Charlson Comorbidity Index scores\n    scores = []\n    mortality_labels = []\n\n    for patient in test_suite:\n        mortality_labels.append(patient['mortality'])\n        present_categories = set()\n        for code in patient['codes']:\n            for cat_id, cat_info in categories.items():\n                for prefix in cat_info['prefixes']:\n                    if code.startswith(prefix):\n                        present_categories.add(cat_id)\n        \n        # Apply hierarchical rules\n        categories_to_remove = set()\n        for cat_id in present_categories:\n            cat_info = categories.get(cat_id, {})\n            suppressor = cat_info.get('suppressed_by')\n            if suppressor and suppressor in present_categories:\n                categories_to_remove.add(cat_id)\n        \n        final_categories = present_categories - categories_to_remove\n        \n        # Sum weights of final categories\n        current_score = sum(categories[cat_id]['weight'] for cat_id in final_categories)\n        scores.append(current_score)\n        \n    S = np.array(scores, dtype=np.float64)\n    Y = np.array(mortality_labels, dtype=np.float64)\n    \n    # Task 2: Compute Pearson correlation\n    n = len(S)\n    s_mean = np.mean(S)\n    y_mean = np.mean(Y)\n\n    # Handle the case of zero variance to avoid division by zero\n    s_ss = np.sum((S - s_mean)**2)\n    y_ss = np.sum((Y - y_mean)**2)\n    \n    if s_ss == 0 or y_ss == 0:\n        rho = 0.0\n    else:\n        numerator = np.sum((S - s_mean) * (Y - y_mean))\n        denominator = np.sqrt(s_ss * y_ss)\n        rho = numerator / denominator\n\n    # Task 3: Compute predictive vs. causal booleans\n    # The problem defines sigma  0 implicitly through the sum of squares being  0\n    sigma_S_positive = s_ss  0\n    sigma_Y_positive = y_ss  0\n    predictive_valid = sigma_S_positive and sigma_Y_positive and (rho  0)\n    \n    causal_warranted = False  # As per problem statement\n\n    # Format the final output string exactly as specified.\n    scores_str = f\"[{','.join(map(str, scores))}]\"\n    final_output_str = f\"[{scores_str},{rho},{predictive_valid},{causal_warranted}]\"\n    \n    print(final_output_str)\n\nsolve()\n```", "id": "5054781"}, {"introduction": "In non-randomized studies, a primary threat to validity is confounding, where treatment groups differ systematically on baseline characteristics. Propensity score methods are a cornerstone of modern epidemiology for addressing this challenge. This practice [@problem_id:5054629] moves from creating variables to using them to assess the suitability of potential comparator groups, tackling the core tenets of exchangeability and positivity. By evaluating propensity score overlap and calculating standardized mean differences, you will learn the essential diagnostic skills needed to select an appropriate real-world comparator and build a foundation for a credible causal analysis.", "problem": "A translational medicine comparative effectiveness study uses Electronic Health Record (EHR) data to evaluate a new therapy against two candidate real-world comparators assembled from different data sources: an insurance claims cohort (comparator $1$) and a disease registry cohort (comparator $2$). To assess exchangeability and positivity across real-world data sources, investigators estimate the propensity score (PS) for treatment using a common model and perform one-to-one nearest-neighbor matching without replacement within a fixed caliper. Balance is evaluated using the standardized mean difference (SMD) for each covariate.\n\nYou are provided summaries for the treated EHR cohort and each comparator cohort before matching, as well as summaries for matched samples. The four baseline covariates are age in years, Charlson Comorbidity Index (CCI) score, baseline estimated glomerular filtration rate (eGFR) in $\\text{mL}/\\text{min}/1.73~\\text{m}^2$, and prior hospitalization in the past year (binary). Treated cohort size is $5{,}000$. Comparator $1$ (claims) size is $12{,}000$. Comparator $2$ (registry) size is $6{,}000$.\n\nPropensity score (PS) distribution summaries (all on the treated PS model):\n- Treated cohort: mean $0.72$, standard deviation $0.10$, range $[0.40,\\,0.95]$.\n- Comparator $1$: mean $0.22$, standard deviation $0.12$, range $[0.02,\\,0.52]$.\n- Comparator $2$: mean $0.68$, standard deviation $0.15$, range $[0.20,\\,0.95]$.\n\nBefore matching, covariate summaries (mean and standard deviation for continuous; proportion for binary):\n- Treated: age mean $66$ and standard deviation $12$; CCI mean $3.2$ and standard deviation $1.6$; eGFR mean $68$ and standard deviation $18$; prior hospitalization proportion $0.42$.\n- Comparator $1$: age mean $59$ and standard deviation $13$; CCI mean $2.1$ and standard deviation $1.5$; eGFR mean $76$ and standard deviation $20$; prior hospitalization proportion $0.29$.\n- Comparator $2$: age mean $65$ and standard deviation $12$; CCI mean $3.1$ and standard deviation $1.6$; eGFR mean $69$ and standard deviation $18$; prior hospitalization proportion $0.41$.\n\nAfter one-to-one PS matching within a fixed caliper, the matched sample sizes are $1{,}150$ pairs for comparator $1$ and $3{,}900$ pairs for comparator $2$. Matched covariate summaries (treated matched and comparator matched) are:\n- With comparator $1$: treated matched age mean $66$ and standard deviation $12$; comparator matched age mean $64$ and standard deviation $12$. Treated matched CCI mean $3.2$ and standard deviation $1.6$; comparator matched CCI mean $3.0$ and standard deviation $1.6$. Treated matched eGFR mean $68$ and standard deviation $18$; comparator matched eGFR mean $70$ and standard deviation $18$. Treated matched prior hospitalization proportion $0.42$; comparator matched prior hospitalization proportion $0.39$.\n- With comparator $2$: treated matched age mean $66$ and standard deviation $12$; comparator matched age mean $66$ and standard deviation $12$. Treated matched CCI mean $3.2$ and standard deviation $1.6$; comparator matched CCI mean $3.2$ and standard deviation $1.6$. Treated matched eGFR mean $68$ and standard deviation $18$; comparator matched eGFR mean $68$ and standard deviation $18$. Treated matched prior hospitalization proportion $0.42$; comparator matched prior hospitalization proportion $0.42$.\n\nTask:\n- Using foundational definitions of standardized mean difference (SMD) for continuous and binary covariates, compute the absolute SMDs for each of the four covariates before matching and after matching for comparator $1$ and comparator $2$.\n- Define the better comparator as the one that simultaneously satisfies substantial $PS$ overlap with the treated cohort and achieves the smaller maximum absolute post-match $SMD$ across the four covariates.\n- Report your choice as a single number: $1$ if the insurance claims comparator is better, or $2$ if the disease registry comparator is better.\n\nExpress the final answer as the numeric identifier only. No rounding is required. No units should be included in the final answer.", "solution": "The problem has been validated and is deemed sound. It is scientifically grounded in the principles of pharmacoepidemiology and biostatistics, well-posed with sufficient information, and objective in its presentation. All data are consistent and plausible within the context of a real-world evidence study. We can proceed with a formal solution.\n\nThe task is to determine which of two real-world comparator cohorts is better for a comparative effectiveness study against a new therapy. The decision is based on two criteria: the degree of propensity score (PS) overlap and the post-matching covariate balance, as measured by the standardized mean difference (SMD).\n\nFirst, we must define the formula for the absolute standardized mean difference ($SMD$). For a given covariate, the $SMD$ quantifies the difference in means between two groups relative to a measure of standard deviation. A common and robust foundational definition in propensity score analysis, which will be used here, employs the standard deviation of the treated group from the original, unmatched sample as the denominator for all calculations (both pre- and post-matching). This ensures a consistent scale for comparison.\n\nFor a continuous covariate, the formula is:\n$$SMD_{\\text{continuous}} = \\frac{|\\mu_{T} - \\mu_{C}|}{\\sigma_{T, \\text{unmatched}}}$$\nwhere $\\mu_{T}$ and $\\mu_{C}$ are the means of the covariate in the groups being compared (treated and comparator, respectively), and $\\sigma_{T, \\text{unmatched}}$ is the standard deviation of the covariate in the original, unmatched treated cohort.\n\nFor a binary covariate, the formula is analogous:\n$$SMD_{\\text{binary}} = \\frac{|p_{T} - p_{C}|}{\\sqrt{p_{T, \\text{unmatched}}(1 - p_{T, \\text{unmatched}})}}$$\nwhere $p_{T}$ and $p_{C}$ are the proportions (prevalences) of the covariate in the groups being compared, and $p_{T, \\text{unmatched}}$ is the proportion in the original, unmatched treated cohort. The denominator represents the standard deviation of a Bernoulli-distributed variable.\n\nThe standard deviations from the original treated cohort ($N_T=5000$) to be used as denominators are:\n- Age: $\\sigma_{age, T} = 12$\n- CCI: $\\sigma_{CCI, T} = 1.6$\n- eGFR: $\\sigma_{eGFR, T} = 18$\n- Prior hospitalization: $\\sqrt{p_{hosp,T}(1 - p_{hosp,T})} = \\sqrt{0.42(1 - 0.42)} = \\sqrt{0.2436}$\n\nWe will now compute the absolute $SMD$s.\n\n**Part 1: Pre-matching Absolute SMDs**\n\n**Comparator 1 (Claims) vs. Treated (Unmatched)**\n- Age: $SMD = \\frac{|66 - 59|}{12} = \\frac{7}{12} \\approx 0.583$\n- CCI: $SMD = \\frac{|3.2 - 2.1|}{1.6} = \\frac{1.1}{1.6} = 0.6875$\n- eGFR: $SMD = \\frac{|68 - 76|}{18} = \\frac{8}{18} = \\frac{4}{9} \\approx 0.444$\n- Prior Hosp.: $SMD = \\frac{|0.42 - 0.29|}{\\sqrt{0.2436}} = \\frac{0.13}{\\sqrt{0.2436}} \\approx 0.263$\n\n**Comparator 2 (Registry) vs. Treated (Unmatched)**\n- Age: $SMD = \\frac{|66 - 65|}{12} = \\frac{1}{12} \\approx 0.083$\n- CCI: $SMD = \\frac{|3.2 - 3.1|}{1.6} = \\frac{0.1}{1.6} = 0.0625$\n- eGFR: $SMD = \\frac{|68 - 69|}{18} = \\frac{1}{18} \\approx 0.056$\n- Prior Hosp.: $SMD = \\frac{|0.42 - 0.41|}{\\sqrt{0.2436}} = \\frac{0.01}{\\sqrt{0.2436}} \\approx 0.020$\n\nBefore matching, Comparator $2$ is substantially more balanced with the treated cohort across all covariates, with all absolute $SMD$s being less than the common threshold of $0.1$.\n\n**Part 2: Post-matching Absolute SMDs**\n\nThe denominators remain the standard deviations from the original unmatched treated cohort.\n\n**Comparator 1 (Matched) vs. Treated (Matched)**\n- Age: $SMD = \\frac{|66 - 64|}{12} = \\frac{2}{12} = \\frac{1}{6} \\approx 0.167$\n- CCI: $SMD = \\frac{|3.2 - 3.0|}{1.6} = \\frac{0.2}{1.6} = 0.125$\n- eGFR: $SMD = \\frac{|68 - 70|}{18} = \\frac{2}{18} = \\frac{1}{9} \\approx 0.111$\n- Prior Hosp.: $SMD = \\frac{|0.42 - 0.39|}{\\sqrt{0.2436}} = \\frac{0.03}{\\sqrt{0.2436}} \\approx 0.061$\n\n**Comparator 2 (Matched) vs. Treated (Matched)**\n- Age: $SMD = \\frac{|66 - 66|}{12} = 0$\n- CCI: $SMD = \\frac{|3.2 - 3.2|}{1.6} = 0$\n- eGFR: $SMD = \\frac{|68 - 68|}{18} = 0$\n- Prior Hosp.: $SMD = \\frac{|0.42 - 0.42|}{\\sqrt{0.2436}} = 0$\n\n**Part 3: Evaluation and Decision**\n\nThe problem defines the better comparator as the one that simultaneously satisfies two conditions: (A) substantial PS overlap and (B) smaller maximum absolute post-match SMD.\n\n**(A) Propensity Score Overlap:**\n- The PS range for the treated cohort is $[0.40, 0.95]$ with a mean of $0.72$.\n- The PS range for Comparator $1$ is $[0.02, 0.52]$ with a mean of $0.22$. The region of common support is $[0.40, 0.52]$. This is a very narrow overlap. The large difference in means ($0.50$) and the small number of matched pairs ($1{,}150$ out of a possible $5{,}000$, or $23\\%$) indicate poor overlap, a violation of the positivity assumption for most of the treated cohort.\n- The PS range for Comparator $2$ is $[0.20, 0.95]$ with a mean of $0.68$. The region of common support is $[0.40, 0.95]$, which covers the entire PS range of the treated group. The small difference in means ($0.04$) and the large number of matched pairs ($3{,}900$ out of $5{,}000$, or $78\\%$) indicate substantial overlap.\n\nConclusion for (A): Comparator $2$ has substantial PS overlap, while Comparator $1$ does not.\n\n**(B) Smaller Maximum Absolute Post-match SMD:**\n- The post-match absolute $SMD$s for Comparator $1$ are $\\{\\frac{1}{6}, \\frac{1}{8}, \\frac{1}{9}, \\frac{0.03}{\\sqrt{0.2436}}\\}$. The maximum value is for age: $\\max(SMD_{C1}) = \\frac{1}{6} \\approx 0.167$. A common threshold for good balance is an $SMD  0.1$, which is not achieved for three of the four covariates.\n- The post-match absolute $SMD$s for Comparator $2$ are $\\{0, 0, 0, 0\\}$. The maximum value is $\\max(SMD_{C2}) = 0$.\n\nConclusion for (B): Comparator $2$ achieves a smaller maximum absolute post-match $SMD$ ($0  \\frac{1}{6}$).\n\n**Final Determination:**\nComparator $2$ is the only one that satisfies the condition of substantial PS overlap. It also achieves the smaller maximum absolute post-match $SMD$. Thus, Comparator $2$ simultaneously satisfies both criteria for being the better comparator. The disease registry cohort is superior to the insurance claims cohort for this analysis. The required output is the numeric identifier for this comparator.", "answer": "$$\\boxed{2}$$", "id": "5054629"}, {"introduction": "Translational research questions often involve exposures and covariates that evolve over time. When a time-varying confounder is itself affected by a past treatment, standard adjustment methods fail and can introduce bias. This exercise [@problem_id:5054426] tackles this advanced challenge by introducing marginal structural models, which use inverse probability of treatment weighting (IPTW) to estimate the causal effects of longitudinal treatment regimens. By manually calculating stabilized weights, you will demystify this powerful technique and see how it constructs a pseudo-population to properly account for time-varying confounding, a critical skill for analyzing longitudinal real-world data.", "problem": "You are given a longitudinal toy dataset consistent with data encountered in Electronic Health Record (EHR), administrative claims, and patient registry sources in translational medicine. Each individual has two binary treatment indicators at two visits, $A_1$ and $A_2$ (with $A_t \\in \\{0,1\\}$), a time-varying binary confounder $L_1 \\in \\{0,1\\}$ measured between $A_1$ and $A_2$ and affected by $A_1$, and a binary outcome $Y \\in \\{0,1\\}$ measured after $A_2$. Using the law of conditional probability and factorization of joint probabilities, construct stabilized inverse probability of treatment weights that reweight the observed data to emulate a pseudo-population in which time-varying confounding by $L_1$ is removed. Then, use these weights to estimate the marginal static-regimen causal contrast between always-treated versus never-treated, defined as the difference in weighted mean outcomes under the static regimens $(A_1,A_2)=(1,1)$ and $(A_1,A_2)=(0,0)$.\n\nFundamental base and rules for computation:\n- For any binary variable $B \\in \\{0,1\\}$ with probability $p = \\Pr(B=1 \\mid \\cdot)$, the probability mass of the observed value $b$ is $\\Pr(B=b \\mid \\cdot) = p^{b}(1-p)^{1-b}$, which reduces to $p$ if $b=1$ and $1-p$ if $b=0$.\n- By the chain rule, the joint mass of a treatment history factorizes sequentially across time. Stabilized weights are constructed as the product over treatment times of a numerator mass (treatment probabilities under a reduced model that excludes time-varying confounders) divided by a denominator mass (treatment probabilities under the observed model that conditions on history including $L_1$). In this setup with no baseline covariates before $A_1$, the time $1$ factor cancels to $1$.\n- For a static regimen $(a_1^*, a_2^*)$, the treatment-specific mean outcome is estimated by the weighted average\n$$\n\\widehat{\\mathbb{E}}[Y^{a_1^*,a_2^*}] = \\frac{\\sum_{i=1}^{n} \\text{SW}_i \\cdot \\mathbf{1}(A_{1i}=a_1^*, A_{2i}=a_2^*) \\cdot Y_i}{\\sum_{i=1}^{n} \\text{SW}_i \\cdot \\mathbf{1}(A_{1i}=a_1^*, A_{2i}=a_2^*)},\n$$\nwhere $\\text{SW}_i$ is the stabilized weight for individual $i$ and $\\mathbf{1}(\\cdot)$ is the indicator function. The target contrast is\n$$\n\\Delta = \\widehat{\\mathbb{E}}[Y^{1,1}] - \\widehat{\\mathbb{E}}[Y^{0,0}].\n$$\n\nYour program must implement this procedure using the provided datasets and treatment assignment probability models, and output the estimated $\\Delta$ for each test case. All probabilities are supplied as fixed values representing models fitted in real-world data (EHRs, claims, registries). No physical units apply. Express all final results as decimal floats rounded to six decimal places.\n\nTest suite specification:\nFor each test case, you are given:\n- A dataset of $n$ individuals with tuples $(A_1, L_1, A_2, Y)$.\n- A marginal probability for first treatment, $\\Pr(A_1=1)$ (included for completeness; there are no baseline covariates, so the time $1$ factor cancels to $1$).\n- A reduced (numerator) model for second treatment, $\\Pr(A_2=1 \\mid A_1=a_1)$ for $a_1 \\in \\{0,1\\}$.\n- A full (denominator) model for second treatment, $\\Pr(A_2=1 \\mid A_1=a_1, L_1=\\ell_1)$ for $a_1,\\ell_1 \\in \\{0,1\\}$.\n\nFor each individual $i$, compute the stabilized weight\n$$\n\\text{SW}_i \\;=\\; \\frac{\\Pr(A_{2i}=a_{2i} \\mid A_{1i}=a_{1i})}{\\Pr(A_{2i}=a_{2i} \\mid A_{1i}=a_{1i}, L_{1i}=\\ell_{1i})},\n$$\nusing the Bernoulli mass rule for the observed value $a_{2i} \\in \\{0,1\\}$ from the supplied probabilities. Then compute $\\widehat{\\mathbb{E}}[Y^{1,1}]$, $\\widehat{\\mathbb{E}}[Y^{0,0}]$, and $\\Delta$ as defined above.\n\nProvide results for the following three test cases:\n\n- Test case $1$ (general case):\n  - Data $(A_1, L_1, A_2, Y)$ for $n=8$ individuals:\n    - $(1,1,1,1)$\n    - $(1,0,1,1)$\n    - $(1,1,0,0)$\n    - $(0,0,0,0)$\n    - $(0,1,1,1)$\n    - $(0,0,1,0)$\n    - $(1,0,0,0)$\n    - $(0,1,0,0)$\n  - $\\Pr(A_1=1) = 0.6$.\n  - Numerator model for second treatment: $\\Pr(A_2=1 \\mid A_1=1)=0.5$, $\\Pr(A_2=1 \\mid A_1=0)=0.3$.\n  - Denominator model for second treatment:\n    - $\\Pr(A_2=1 \\mid A_1=1,L_1=1)=0.8$\n    - $\\Pr(A_2=1 \\mid A_1=1,L_1=0)=0.4$\n    - $\\Pr(A_2=1 \\mid A_1=0,L_1=1)=0.6$\n    - $\\Pr(A_2=1 \\mid A_1=0,L_1=0)=0.2$\n\n- Test case $2$ (near-boundary probabilities):\n  - Data $(A_1, L_1, A_2, Y)$ for $n=6$ individuals:\n    - $(1,1,1,1)$\n    - $(1,1,0,0)$\n    - $(1,0,1,1)$\n    - $(0,1,1,1)$\n    - $(0,0,0,0)$\n    - $(0,0,1,0)$\n  - $\\Pr(A_1=1) = 0.5$.\n  - Numerator model for second treatment: $\\Pr(A_2=1 \\mid A_1=1)=0.8$, $\\Pr(A_2=1 \\mid A_1=0)=0.2$.\n  - Denominator model for second treatment:\n    - $\\Pr(A_2=1 \\mid A_1=1,L_1=1)=0.95$\n    - $\\Pr(A_2=1 \\mid A_1=1,L_1=0)=0.05$\n    - $\\Pr(A_2=1 \\mid A_1=0,L_1=1)=0.9$\n    - $\\Pr(A_2=1 \\mid A_1=0,L_1=0)=0.1$\n\n- Test case $3$ (mixed outcomes and moderate probabilities):\n  - Data $(A_1, L_1, A_2, Y)$ for $n=7$ individuals:\n    - $(1,1,0,0)$\n    - $(1,0,1,0)$\n    - $(1,1,1,1)$\n    - $(0,1,0,0)$\n    - $(0,1,1,1)$\n    - $(1,0,0,0)$\n    - $(0,0,0,0)$\n  - $\\Pr(A_1=1) = 0.7$.\n  - Numerator model for second treatment: $\\Pr(A_2=1 \\mid A_1=1)=0.6$, $\\Pr(A_2=1 \\mid A_1=0)=0.25$.\n  - Denominator model for second treatment:\n    - $\\Pr(A_2=1 \\mid A_1=1,L_1=1)=0.7$\n    - $\\Pr(A_2=1 \\mid A_1=1,L_1=0)=0.3$\n    - $\\Pr(A_2=1 \\mid A_1=0,L_1=1)=0.5$\n    - $\\Pr(A_2=1 \\mid A_1=0,L_1=0)=0.2$\n\nFinal output format:\n- Your program should produce a single line of output containing the three estimated contrasts $\\Delta$ for test cases $1$, $2$, and $3$, in that order, as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places, for example $[r_1,r_2,r_3]$ where $r_1$, $r_2$, and $r_3$ are decimal floats.", "solution": "The problem is valid as it presents a well-posed, scientifically grounded, and objective task based on established principles of causal inference in biostatistics, a discipline integral to translational medicine. The problem asks for the estimation of a causal contrast using stabilized inverse probability of treatment weights (IPTW) to adjust for time-varying confounding, a standard and critical procedure when analyzing longitudinal real-world data from sources like electronic health records (EHRs). All necessary data, models, and definitions are provided, and the problem structure is free from contradiction, ambiguity, or fallacy.\n\nThe objective is to estimate the causal risk difference, $\\Delta = \\mathbb{E}[Y^{\\bar{a}=(1,1)}] - \\mathbb{E}[Y^{\\bar{a}=(0,0)}]$, comparing a static treatment regimen of \"always treated\" to \"never treated\". The analysis is complicated by the presence of a time-varying confounder, $L_1$, which is a post-baseline variable that is affected by the first treatment, $A_1$, and in turn affects the subsequent treatment, $A_2$, and the outcome, $Y$. Standard regression adjustment for $L_1$ would be inappropriate as it would block part of the effect of $A_1$ and could introduce bias. Inverse probability weighting for marginal structural models is the appropriate method to handle this causal structure.\n\nThe core of this method is the construction of stabilized weights, $\\text{SW}_i$. These weights create a pseudo-population in which the association between the time-varying confounder $L_1$ and the subsequent treatment $A_2$ is broken, while preserving the marginal association between treatments over time. For each individual $i$, the stabilized weight is the ratio of the probability of their observed treatment history under a model without the time-varying confounder to the probability of their observed treatment history under a model that includes it.\n\nGiven the sequential nature of treatment, the joint probability of the treatment history $(A_{1i}, A_{2i})$ is factorized as $\\Pr(A_{1i}=a_{1i}, A_{2i}=a_{2i}) = \\Pr(A_{2i}=a_{2i} \\mid A_{1i}=a_{1i}) \\Pr(A_{1i}=a_{1i})$. The full stabilized weight for individual $i$ with observed history $(a_{1i}, \\ell_{1i}, a_{2i})$ is given by:\n$$\n\\text{SW}_i = \\frac{\\Pr(A_{1i}=a_{1i}) \\Pr(A_{2i}=a_{2i} \\mid A_{1i}=a_{1i})}{\\Pr(A_{1i}=a_{1i}) \\Pr(A_{2i}=a_{2i} \\mid A_{1i}=a_{1i}, L_{1i}=\\ell_{1i})}\n$$\nThe problem specifies that there are no baseline covariates measured before $A_1$. Thus, the numerator and denominator models for $\\Pr(A_1=a_1)$ are identical, and this term cancels from the weight expression. The simplified stabilized weight is therefore determined solely by the time-$2$ treatment assignment:\n$$\n\\text{SW}_i \\;=\\; \\frac{\\Pr(A_{2i}=a_{2i} \\mid A_{1i}=a_{1i})}{\\Pr(A_{2i}=a_{2i} \\mid A_{1i}=a_{1i}, L_{1i}=\\ell_{1i})}\n$$\nTo operationalize this, we use the provided probability models and the general formula for the probability mass of an observed binary variable $b \\in \\{0,1\\}$ with success probability $p = \\Pr(B=1 \\mid \\cdot)$, which is $p^b(1-p)^{1-b}$. For a given individual $i$ with data $(a_{1i}, \\ell_{1i}, a_{2i}, y_i)$, we calculate the weight as follows:\n$1$. Identify the numerator probability $p_{num} = \\Pr(A_2=1 \\mid A_1=a_{1i})$. The probability mass for the observed $a_{2i}$ is $m_{num} = p_{num}^{a_{2i}}(1-p_{num})^{1-a_{2i}}$.\n$2$. Identify the denominator probability $p_{den} = \\Pr(A_2=1 \\mid A_1=a_{1i}, L_1=\\ell_{1i})$. The probability mass for the observed $a_{2i}$ is $m_{den} = p_{den}^{a_{2i}}(1-p_{den})^{1-a_{2i}}$.\n$3$. The weight is $\\text{SW}_i = m_{num} / m_{den}$.\n\nOnce the weights are computed for all $n$ individuals, we estimate the mean outcome under each static regimen $(a_1^*, a_2^*)$ using a weighted average. This estimator is a specific instance of the Horvitz-Thompson estimator. For a regimen of interest, we consider only those individuals in the dataset whose observed treatment history is consistent with that regimen, i.e., $(A_{1i}, A_{2i}) = (a_1^*, a_2^*)$. The estimator for the treatment-specific mean is:\n$$\n\\widehat{\\mathbb{E}}[Y^{a_1^*,a_2^*}] = \\frac{\\sum_{i=1}^{n} \\text{SW}_i \\cdot \\mathbf{1}(A_{1i}=a_1^*, A_{2i}=a_2^*) \\cdot Y_i}{\\sum_{i=1}^{n} \\text{SW}_i \\cdot \\mathbf{1}(A_{1i}=a_1^*, A_{2i}=a_2^*)}\n$$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function. The numerator sums the weighted outcomes of conforming individuals, and the denominator sums their weights.\n\nThe procedure is as follows:\n$1$. For each test case, we systematically calculate $\\text{SW}_i$ for every individual based on their observed data $(a_{1i}, \\ell_{1i}, a_{2i})$ and the provided numerator and denominator probability models.\n$2$. To estimate $\\widehat{\\mathbb{E}}[Y^{1,1}]$, we identify all individuals for whom $(A_{1i}, A_{2i}) = (1,1)$. We then compute the sum of their weighted outcomes, $\\sum \\text{SW}_i \\cdot Y_i$, and divide by the sum of their weights, $\\sum \\text{SW}_i$.\n$3$. Similarly, to estimate $\\widehat{\\mathbb{E}}[Y^{0,0}]$, we identify all individuals with $(A_{1i}, A_{2i}) = (0,0)$ and compute their weighted mean outcome.\n$4$. The final causal contrast is the difference: $\\Delta = \\widehat{\\mathbb{E}}[Y^{1,1}] - \\widehat{\\mathbb{E}}[Y^{0,0}]$.\n\nThis entire computational process is implemented for each of the three test cases specified in the problem statement. For example, for an individual in test case $1$ with data $(A_1=1, L_1=1, A_2=1, Y=1)$, the numerator probability is $\\Pr(A_2=1 \\mid A_1=1) = 0.5$, and the denominator probability is $\\Pr(A_2=1 \\mid A_1=1, L_1=1) = 0.8$. The weight is $\\text{SW} = 0.5/0.8 = 0.625$. For an individual with $(A_1=1, L_1=1, A_2=0, Y=0)$, the numerator mass is $\\Pr(A_2=0 \\mid A_1=1) = 1-0.5 = 0.5$, the denominator mass is $\\Pr(A_2=0 \\mid A_1=1, L_1=1) = 1-0.8 = 0.2$, and the weight is $\\text{SW} = 0.5/0.2 = 2.5$. These calculations are performed for all individuals, and the results are aggregated to produce the final estimates.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the three test cases.\n    It calculates the stabilized inverse probability of treatment weights and\n    estimates the causal contrast between always-treated and never-treated regimens.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"data\": np.array([\n                [1, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 0], [0, 0, 0, 0],\n                [0, 1, 1, 1], [0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 0, 0]\n            ]),\n            \"p_num\": {1: 0.5, 0: 0.3},\n            \"p_den\": {\n                1: {1: 0.8, 0: 0.4},\n                0: {1: 0.6, 0: 0.2}\n            }\n        },\n        {\n            \"data\": np.array([\n                [1, 1, 1, 1], [1, 1, 0, 0], [1, 0, 1, 1],\n                [0, 1, 1, 1], [0, 0, 0, 0], [0, 0, 1, 0]\n            ]),\n            \"p_num\": {1: 0.8, 0: 0.2},\n            \"p_den\": {\n                1: {1: 0.95, 0: 0.05},\n                0: {1: 0.9, 0: 0.1}\n            }\n        },\n        {\n            \"data\": np.array([\n                [1, 1, 0, 0], [1, 0, 1, 0], [1, 1, 1, 1],\n                [0, 1, 0, 0], [0, 1, 1, 1], [1, 0, 0, 0],\n                [0, 0, 0, 0]\n            ]),\n            \"p_num\": {1: 0.6, 0: 0.25},\n            \"p_den\": {\n                1: {1: 0.7, 0: 0.3},\n                0: {1: 0.5, 0: 0.2}\n            }\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        data = case[\"data\"]\n        p_num_model = case[\"p_num\"]\n        p_den_model = case[\"p_den\"]\n\n        weights = []\n        for row in data:\n            a1, l1, a2, y = int(row[0]), int(row[1]), int(row[2]), int(row[3])\n            \n            p_num = p_num_model[a1]\n            m_num = p_num if a2 == 1 else 1 - p_num\n            \n            p_den = p_den_model[a1][l1]\n            m_den = p_den if a2 == 1 else 1 - p_den\n            \n            sw = m_num / m_den\n            weights.append(sw)\n        \n        weights = np.array(weights)\n\n        # Estimate for regimen (1, 1)\n        a1_obs, a2_obs, y_obs = data[:, 0], data[:, 2], data[:, 3]\n        \n        mask_11 = (a1_obs == 1)  (a2_obs == 1)\n        sum_sw_y_11 = np.sum(weights[mask_11] * y_obs[mask_11])\n        sum_sw_11 = np.sum(weights[mask_11])\n        mean_y_11 = sum_sw_y_11 / sum_sw_11 if sum_sw_11 != 0 else 0\n\n        # Estimate for regimen (0, 0)\n        mask_00 = (a1_obs == 0)  (a2_obs == 0)\n        sum_sw_y_00 = np.sum(weights[mask_00] * y_obs[mask_00])\n        sum_sw_00 = np.sum(weights[mask_00])\n        mean_y_00 = sum_sw_y_00 / sum_sw_00 if sum_sw_00 != 0 else 0\n\n        delta = mean_y_11 - mean_y_00\n        results.append(delta)\n\n    # Format the final output string exactly as required\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "5054426"}]}