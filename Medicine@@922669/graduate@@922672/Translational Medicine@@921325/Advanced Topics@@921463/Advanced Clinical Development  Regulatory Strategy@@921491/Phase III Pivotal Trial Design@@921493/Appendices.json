{"hands_on_practices": [{"introduction": "A fundamental step in designing any Phase III pivotal trial is determining the required sample size. An underpowered study is unlikely to detect a true treatment effect, representing a waste of resources and an ethical disservice to participants, while an oversized study exposes more participants than necessary to potential risks. This practice exercise [@problem_id:5044717] provides a foundational walkthrough of sample size calculation from first principles for a common trial scenario involving a binary \"responder\" endpoint, linking the core concepts of statistical power ($1-\\beta$), significance level ($\\alpha$), and the clinically meaningful effect size you aim to detect.", "problem": "A sponsor is designing a Phase III pivotal trial in translational medicine to evaluate a new analgesic in patients with chronic osteoarthritis pain. The primary endpoint is a responder analysis: a patient is defined as a responder if, at week $12$, the Numeric Pain Rating Scale (NPRS, $10$-point scale) improves by at least the Minimal Clinically Important Difference (MCID) of $2$ points. Based on Phase II data under similar inclusion criteria, the control arm responder probability is anticipated to be $p_{C} = 0.35$, while the treatment arm responder probability is anticipated to be $p_{T} = 0.50$. The clinically meaningful between-arm improvement to be detected is the difference $\\Delta = p_{T} - p_{C}$ implied by these planning values.\n\nThe trial will use a one-sided superiority hypothesis test at significance level $\\alpha = 0.025$ with power $1 - \\beta = 0.9$, and $1{:}1$ allocation between treatment and control. Assume the following standard large-sample framework: individual responses are independent and identically distributed within arms; the arm-level responder counts are binomial; and the difference in sample proportions is approximated by a normal distribution via the Central Limit Theorem (CLT). The test is a Wald-type $z$-test using the pooled variance under the null hypothesis $H_{0}\\!:\\, p_{T} - p_{C} \\leq 0$ and the unpooled variance under the alternative hypothesis $H_{1}\\!:\\, p_{T} - p_{C}  0$. Let $\\Phi$ denote the cumulative distribution function (CDF) of the standard normal distribution, and write $z_{\\gamma} = \\Phi^{-1}(\\gamma)$ for the corresponding quantile.\n\nDerive from first principles the planning equation for the required equal per-arm sample size $n$ that achieves the specified $\\alpha$ and $1-\\beta$, starting from the binomial model, the CLT, and the definition of Type I and Type II error. Then, using the stated planning values and quantiles for $\\alpha$ and $1-\\beta$, compute the continuous-valued per-arm sample size. Express your final answer as a single real number representing the required per-arm sample size, rounded to four significant figures. No continuity correction or inflation for missing data is to be applied in this calculation.", "solution": "The user wants me to solve the following problem:\nA sponsor is designing a Phase III pivotal trial in translational medicine to evaluate a new analgesic in patients with chronic osteoarthritis pain. The primary endpoint is a responder analysis: a patient is defined as a responder if, at week $12$, the Numeric Pain Rating Scale (NPRS, $10$-point scale) improves by at least the Minimal Clinically Important Difference (MCID) of $2$ points. Based on Phase II data under similar inclusion criteria, the control arm responder probability is anticipated to be $p_{C} = 0.35$, while the treatment arm responder probability is anticipated to be $p_{T} = 0.50$. The clinically meaningful between-arm improvement to be detected is the difference $\\Delta = p_{T} - p_{C}$ implied by these planning values.\n\nThe trial will use a one-sided superiority hypothesis test at significance level $\\alpha = 0.025$ with power $1 - \\beta = 0.9$, and $1{:}1$ allocation between treatment and control. Assume the following standard large-sample framework: individual responses are independent and identically distributed within arms; the arm-level responder counts are binomial; and the difference in sample proportions is approximated by a normal distribution via the Central Limit Theorem (CLT). The test is a Wald-type $z$-test using the pooled variance under the null hypothesis $H_{0}\\!:\\, p_{T} - p_{C} \\leq 0$ and the unpooled variance under the alternative hypothesis $H_{1}\\!:\\, p_{T} - p_{C}  0$. Let $\\Phi$ denote the cumulative distribution function (CDF) of the standard normal distribution, and write $z_{\\gamma} = \\Phi^{-1}(\\gamma)$ for the corresponding quantile.\n\nDerive from first principles the planning equation for the required equal per-arm sample size $n$ that achieves the specified $\\alpha$ and $1-\\beta$, starting from the binomial model, the CLT, and the definition of Type I and Type II error. Then, using the stated planning values and quantiles for $\\alpha$ and $1-\\beta$, compute the continuous-valued per-arm sample size. Express your final answer as a single real number representing the required per-arm sample size, rounded to four significant figures. No continuity correction or inflation for missing data is to be applied in this calculation.\n\n## Problem Validation\n\n### Step 1: Extract Givens\n-   Primary endpoint: Responder analysis based on NPRS improvement $\\ge 2$ points (MCID) at week $12$.\n-   Control arm responder probability: $p_{C} = 0.35$.\n-   Treatment arm responder probability: $p_{T} = 0.50$.\n-   Effect size of interest: $\\Delta = p_{T} - p_{C}$.\n-   Hypothesis test: One-sided superiority.\n-   Significance level: $\\alpha = 0.025$.\n-   Power: $1 - \\beta = 0.9$.\n-   Allocation: $1:1$, with equal per-arm sample size $n$.\n-   Statistical framework: Large-sample, independent binomial responses per arm, normal approximation for the difference in proportions via CLT.\n-   Test statistic: Wald-type $z$-test using pooled variance under $H_0$.\n-   Power calculation: Uses unpooled variance under $H_1$.\n-   Hypotheses: $H_{0}: p_{T} - p_{C} \\leq 0$ and $H_{1}: p_{T} - p_{C}  0$.\n-   Notation: $\\Phi$ is the standard normal CDF, $z_{\\gamma} = \\Phi^{-1}(\\gamma)$ is the quantile function.\n-   Task: Derive the sample size formula for $n$ and compute its value.\n-   Final Answer: Continuous-valued $n$, rounded to four significant figures. No continuity correction.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is a standard, canonical application of statistical principles in the design of a randomized controlled trial, a cornerstone of translational medicine and drug development. All concepts (responder analysis, MCID, $\\alpha$, $\\beta$, CLT, z-test) are fundamental to biostatistics.\n2.  **Well-Posed**: The problem provides all necessary parameters ($p_C$, $p_T$, $\\alpha$, $\\beta$, allocation ratio, test type) to derive and calculate a unique sample size. The objective is unambiguous.\n3.  **Objective**: The problem is stated in precise, quantitative, and objective language, free of subjective claims.\n4.  **Incomplete or Contradictory Setup**: The setup is complete and self-consistent. The specification of using pooled variance for the test under the null and unpooled variance for power under the alternative is a standard and well-defined convention for sample size calculations.\n5.  **Unrealistic or Infeasible**: The provided probabilities ($35\\%$ and $50\\%$ response rates) and design parameters ($\\alpha=0.025$, power=$90\\%$) are highly realistic for a Phase III trial in a chronic pain indication.\n6.  **Ill-Posed or Poorly Structured**: The problem is well-structured, leading to a determinate solution.\n7.  **Pseudo-Profound, Trivial, or Tautological**: The task requires a derivation from first principles, which is a non-trivial exercise connecting fundamental theory to a practical formula. It is a substantive problem.\n8.  **Outside Scientific Verifiability**: The derivation and calculation are mathematically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n## Solution Derivation and Calculation\nWe are tasked with deriving the per-arm sample size, $n$, for a one-sided superiority test comparing two independent proportions, $p_T$ and $p_C$.\n\nLet $\\hat{p}_T$ and $\\hat{p}_C$ be the sample proportions of responders in the treatment and control arms, respectively, each with sample size $n$. The hypotheses are:\n$$H_0: p_T - p_C \\le 0 \\quad \\text{vs.} \\quad H_1: p_T - p_C  0$$\nBy the Central Limit Theorem, for a large sample size $n$, the sample proportions are approximately normally distributed. The difference in sample proportions, $\\hat{D} = \\hat{p}_T - \\hat{p}_C$, is therefore also approximately normally distributed.\n\nThe distribution of $\\hat{D}$ depends on the true underlying probabilities.\nUnder a specific alternative hypothesis $H_1$, where the true probabilities are $p_T$ and $p_C$, the distribution of $\\hat{D}$ is:\n$$ \\hat{D} | H_1 \\sim \\mathcal{N}\\left(p_T - p_C, \\frac{p_T(1-p_T)}{n} + \\frac{p_C(1-p_C)}{n}\\right) $$\nThe variance term is the unpooled variance, as specified for the power calculation. Let's denote the standard deviation of the difference under $H_1$ as $\\sigma_{H_1} = \\sqrt{\\frac{p_T(1-p_T) + p_C(1-p_C)}{n}}$.\n\nThe test is conducted at a significance level $\\alpha$. This means the probability of a Type I error (rejecting $H_0$ when it is true) must be at most $\\alpha$. We reject $H_0$ if the observed difference $\\hat{D}$ exceeds a critical value, $D_{crit}$.\n$$ \\mathbb{P}(\\hat{D}  D_{crit} | H_0 \\text{ is true}) = \\alpha $$\nUnder the boundary of the null hypothesis, $p_T = p_C = p$. The problem specifies using a pooled variance for the test statistic. This implies that for the design phase, the variance under the null hypothesis is based on a single pooled probability. A standard choice for this probability in sample size planning is the average of the anticipated probabilities under $H_1$, denoted as $\\bar{p} = \\frac{p_T + p_C}{2}$.\nUnder this null scenario ($p_T = p_C = \\bar{p}$), the mean of $\\hat{D}$ is $0$ and its variance is $\\frac{\\bar{p}(1-\\bar{p})}{n} + \\frac{\\bar{p}(1-\\bar{p})}{n} = \\frac{2\\bar{p}(1-\\bar{p})}{n}$.\nLet the standard deviation under this null be $\\sigma_{H_0} = \\sqrt{\\frac{2\\bar{p}(1-\\bar{p})}{n}}$.\nThe condition for Type I error can be expressed by standardizing $D_{crit}$:\n$$ \\mathbb{P}\\left(\\frac{\\hat{D} - 0}{\\sigma_{H_0}}  \\frac{D_{crit}}{\\sigma_{H_0}} \\bigg| H_0\\right) = \\alpha $$\nThis implies that $\\frac{D_{crit}}{\\sigma_{H_0}} = z_{1-\\alpha}$, where $z_{1-\\alpha}$ is the $(1-\\alpha)$-quantile of the standard normal distribution.\nSolving for $D_{crit}$ gives our first expression:\n$$ D_{crit} = z_{1-\\alpha} \\sigma_{H_0} = z_{1-\\alpha} \\sqrt{\\frac{2\\bar{p}(1-\\bar{p})}{n}} $$\n\nNext, we consider the power of the study, $1-\\beta$. Power is the probability of correctly rejecting $H_0$ when the specific alternative hypothesis $H_1$ is true.\n$$ \\mathbb{P}(\\hat{D}  D_{crit} | H_1 \\text{ is true}) = 1-\\beta $$\nTo evaluate this probability, we standardize $\\hat{D}$ using its distribution under $H_1$:\n$$ \\mathbb{P}\\left(\\frac{\\hat{D} - (p_T - p_C)}{\\sigma_{H_1}}  \\frac{D_{crit} - (p_T - p_C)}{\\sigma_{H_1}} \\bigg| H_1\\right) = 1-\\beta $$\nThis implies that the argument of the probability function must be $z_{1-\\beta}$. However, the standard normal CDF $\\Phi$ is typically used with probabilities of the form $\\mathbb{P}(Z \\le z)$. Rewriting the condition:\n$$ \\mathbb{P}\\left(\\frac{\\hat{D} - (p_T - p_C)}{\\sigma_{H_1}} \\le \\frac{D_{crit} - (p_T - p_C)}{\\sigma_{H_1}} \\bigg| H_1\\right) = \\beta $$\nThis requires $\\frac{D_{crit} - (p_T - p_C)}{\\sigma_{H_1}} = z_{\\beta}$. Since $z_\\beta = -z_{1-\\beta}$, we can write:\n$$ D_{crit} - (p_T - p_C) = -z_{1-\\beta} \\sigma_{H_1} $$\nSolving for $D_{crit}$ gives our second expression:\n$$ D_{crit} = (p_T - p_C) - z_{1-\\beta} \\sigma_{H_1} = (p_T - p_C) - z_{1-\\beta} \\sqrt{\\frac{p_T(1-p_T) + p_C(1-p_C)}{n}} $$\n\nNow, we equate the two expressions for $D_{crit}$ to solve for $n$:\n$$ z_{1-\\alpha} \\sqrt{\\frac{2\\bar{p}(1-\\bar{p})}{n}} = (p_T - p_C) - z_{1-\\beta} \\sqrt{\\frac{p_T(1-p_T) + p_C(1-p_C)}{n}} $$\nLet $\\Delta = p_T - p_C$. Rearranging the terms to isolate $n$:\n$$ \\Delta = z_{1-\\alpha} \\sqrt{\\frac{2\\bar{p}(1-\\bar{p})}{n}} + z_{1-\\beta} \\sqrt{\\frac{p_T(1-p_T) + p_C(1-p_C)}{n}} $$\nFactor out $\\frac{1}{\\sqrt{n}}$:\n$$ \\Delta = \\frac{1}{\\sqrt{n}} \\left( z_{1-\\alpha} \\sqrt{2\\bar{p}(1-\\bar{p})} + z_{1-\\beta} \\sqrt{p_T(1-p_T) + p_C(1-p_C)} \\right) $$\nSolving for $\\sqrt{n}$ and then squaring yields the final planning equation for the per-arm sample size $n$:\n$$ n = \\frac{\\left( z_{1-\\alpha} \\sqrt{2\\bar{p}(1-\\bar{p})} + z_{1-\\beta} \\sqrt{p_T(1-p_T) + p_C(1-p_C)} \\right)^2}{\\Delta^2} $$\nwhere $\\bar{p} = \\frac{p_C + p_T}{2}$. This completes the derivation from first principles.\n\nNow we substitute the given values to compute $n$:\n-   $p_C = 0.35$\n-   $p_T = 0.50$\n-   $\\alpha = 0.025$ (one-sided)\n-   $1 - \\beta = 0.90 \\implies \\beta = 0.10$\n\nWe first calculate the required components:\n-   Difference: $\\Delta = p_T - p_C = 0.50 - 0.35 = 0.15$.\n-   Pooled probability for null variance: $\\bar{p} = \\frac{0.35 + 0.50}{2} = 0.425$.\n-   Quantiles from the standard normal distribution:\n    -   $z_{1-\\alpha} = z_{1-0.025} = z_{0.975} \\approx 1.959964$.\n    -   $z_{1-\\beta} = z_{1-0.10} = z_{0.90} \\approx 1.281552$.\n-   Variance components (to be multiplied by $1/n$):\n    -   Pooled variance term (for null): $2\\bar{p}(1-\\bar{p}) = 2(0.425)(1 - 0.425) = 2(0.425)(0.575) = 0.48875$.\n    -   Unpooled variance term (for alternative): $p_T(1-p_T) + p_C(1-p_C) = 0.50(0.50) + 0.35(0.65) = 0.25 + 0.2275 = 0.4775$.\n\nSubstitute these values into the derived formula:\n$$ n = \\frac{\\left( z_{0.975} \\sqrt{0.48875} + z_{0.90} \\sqrt{0.4775} \\right)^2}{(0.15)^2} $$\n$$ n = \\frac{\\left( 1.959964 \\times \\sqrt{0.48875} + 1.281552 \\times \\sqrt{0.4775} \\right)^2}{0.0225} $$\nFirst, compute the square roots:\n$$ \\sqrt{0.48875} \\approx 0.69910657 $$\n$$ \\sqrt{0.4775} \\approx 0.69101375 $$\nNow compute the terms in the numerator's parenthesis:\n$$ n = \\frac{\\left( 1.959964 \\times 0.69910657 + 1.281552 \\times 0.69101375 \\right)^2}{0.0225} $$\n$$ n = \\frac{\\left( 1.370222... + 0.885541... \\right)^2}{0.0225} $$\n$$ n = \\frac{\\left( 2.255763... \\right)^2}{0.0225} $$\n$$ n = \\frac{5.08847...}{0.0225} $$\n$$ n \\approx 226.1542... $$\n\nThe problem requires the continuous-valued sample size rounded to four significant figures.\n$$ n = 226.2 $$\nThis is the required sample size for each arm of the trial.", "answer": "$$\\boxed{226.2}$$", "id": "5044717"}, {"introduction": "While many trials are designed to prove a new therapy is superior to a placebo or active control, a common objective in translational medicine is to demonstrate that a new formulation is therapeutically equivalent to an established standard. This requires a different statistical framework than a standard superiority test. This exercise [@problem_id:5044594] guides you through the application of the Two One-Sided Tests (TOST) procedure, which is the regulatory standard for concluding equivalence, challenging you to interpret whether a new drug's effect falls within a pre-specified margin of clinical acceptability.", "problem": "A Phase III pivotal, randomized, parallel-group trial in rheumatoid arthritis compared a new oral formulation to an approved reference therapy. The primary endpoint was the change in Disease Activity Score in 28 joints (DAS28) at week $24$, where more negative values indicate greater improvement. Based on historical data and clinical acceptability, an equivalence margin of $\\Delta_{0} = 0.60$ DAS28 units was pre-specified for the treatment difference parameter $\\Delta = \\mu_{T} - \\mu_{R}$, where $\\mu_{T}$ and $\\mu_{R}$ denote the population means for the treatment and reference arms, respectively. The observed sample statistics were: treatment arm mean change $-2.30$, reference arm mean change $-2.22$, treatment arm sample standard deviation $1.20$, reference arm sample standard deviation $1.20$, and sample sizes $n_{T} = 400$ and $n_{R} = 400$. Assume independent, approximately normally distributed outcomes with equal variances across arms.\n\nUsing the Two One-Sided Tests (TOST) framework at a one-sided significance level $\\alpha = 0.05$ per component test, derive from first principles the two one-sided $p$-values corresponding to the hypotheses $H_{0}^{-}: \\Delta \\leq -\\Delta_{0}$ versus $H_{1}^{-}: \\Delta  -\\Delta_{0}$ and $H_{0}^{+}: \\Delta \\geq \\Delta_{0}$ versus $H_{1}^{+}: \\Delta  \\Delta_{0}$. Treat the standardized test statistics under the equal-variance assumption as following a Student’s $t$ distribution with $n_{T} + n_{R} - 2$ degrees of freedom. Then, interpret whether equivalence can be concluded at the overall Type I error rate $0.05$ (that is, whether both one-sided tests reject their respective null hypotheses).\n\nReport the two one-sided $p$-values as a row matrix using the LaTeX $\\mathrm{pmatrix}$ environment, with the first entry for $H_{0}^{-}$ and the second entry for $H_{0}^{+}$. Round each $p$-value to four significant figures. No units are required for $p$-values.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- Trial Design: Phase III pivotal, randomized, parallel-group.\n- Indication: Rheumatoid arthritis.\n- Arms: New oral formulation (Treatment, $T$) versus approved reference therapy (Reference, $R$).\n- Primary Endpoint: Change in Disease Activity Score in 28 joints (DAS28) at week $24$.\n- Equivalence Margin: $\\Delta_{0} = 0.60$.\n- Parameter of Interest: $\\Delta = \\mu_{T} - \\mu_{R}$, where $\\mu_{T}$ and $\\mu_{R}$ are population mean changes.\n- Sample Statistics:\n  - Treatment mean change: $\\bar{x}_{T} = -2.30$.\n  - Reference mean change: $\\bar{x}_{R} = -2.22$.\n  - Treatment sample standard deviation: $s_{T} = 1.20$.\n  - Reference sample standard deviation: $s_{R} = 1.20$.\n  - Sample sizes: $n_{T} = 400$, $n_{R} = 400$.\n- Assumptions:\n  - Independent, approximately normally distributed outcomes.\n  - Equal variances across arms.\n- Statistical Framework: Two One-Sided Tests (TOST) at a one-sided significance level $\\alpha = 0.05$ per test.\n- Hypotheses:\n  - Test 1: $H_{0}^{-}: \\Delta \\leq -\\Delta_{0}$ versus $H_{1}^{-}: \\Delta  -\\Delta_{0}$.\n  - Test 2: $H_{0}^{+}: \\Delta \\geq \\Delta_{0}$ versus $H_{1}^{+}: \\Delta  \\Delta_{0}$.\n- Test Statistic Distribution: Student’s $t$ with $n_{T} + n_{R} - 2$ degrees of freedom.\n- Required Output: The two one-sided $p$-values, rounded to four significant figures, and an interpretation of the equivalence conclusion.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a standard, well-defined biostatistics problem concerning the design and analysis of a clinical equivalence trial. The TOST procedure is the U.S. FDA and European EMA accepted statistical method for demonstrating bioequivalence and therapeutic equivalence. The use of DAS28, the specified sample sizes, means, and standard deviations are all scientifically plausible for a large-scale rheumatology trial. The assumptions are explicitly stated and internally consistent (e.g., $s_{T}=s_{R}$ is consistent with the equal variance assumption). The problem is self-contained, objective, and scientifically grounded. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\nThe objective is to test for equivalence of a new treatment and a reference treatment. The null hypothesis of non-equivalence is $H_{0}: \\Delta \\leq -\\Delta_{0} \\text{ or } \\Delta \\geq \\Delta_{0}$, and the alternative hypothesis of equivalence is $H_{1}: -\\Delta_{0}  \\Delta  \\Delta_{0}$. The Two One-Sided Tests (TOST) procedure decomposes this into two separate one-sided hypothesis tests, as stated in the problem:\n$1.$ Test for superiority to the lower margin: $H_{0}^{-}: \\Delta \\leq -\\Delta_{0}$ versus $H_{1}^{-}: \\Delta  -\\Delta_{0}$.\n$2.$ Test for superiority to the upper margin (in the opposite direction): $H_{0}^{+}: \\Delta \\geq \\Delta_{0}$ versus $H_{1}^{+}: \\Delta  \\Delta_{0}$.\nEquivalence is concluded at an overall significance level of $\\alpha = 0.05$ if and only if both $H_{0}^{-}$ and $H_{0}^{+}$ are rejected, each at the $\\alpha = 0.05$ level.\n\nFirst, we calculate the point estimate of the treatment difference, $\\hat{\\Delta}$.\n$$\n\\hat{\\Delta} = \\bar{x}_{T} - \\bar{x}_{R} = -2.30 - (-2.22) = -0.08\n$$\n\nNext, we calculate the pooled standard deviation, $s_{p}$, under the assumption of equal variances. The pooled variance, $s_{p}^{2}$, is given by:\n$$\ns_{p}^{2} = \\frac{(n_{T}-1)s_{T}^{2} + (n_{R}-1)s_{R}^{2}}{n_{T} + n_{R} - 2}\n$$\nGiven $n_{T} = 400$, $n_{R} = 400$, $s_{T} = 1.20$, and $s_{R} = 1.20$:\n$$\ns_{p}^{2} = \\frac{(400-1)(1.20)^{2} + (400-1)(1.20)^{2}}{400 + 400 - 2} = \\frac{2 \\cdot (399) \\cdot (1.20)^{2}}{798} = \\frac{798 \\cdot (1.20)^{2}}{798} = (1.20)^{2} = 1.44\n$$\nTherefore, the pooled standard deviation is $s_{p} = \\sqrt{1.44} = 1.20$.\n\nThe standard error of the estimated difference, $SE(\\hat{\\Delta})$, is:\n$$\nSE(\\hat{\\Delta}) = s_{p} \\sqrt{\\frac{1}{n_{T}} + \\frac{1}{n_{R}}} = 1.20 \\sqrt{\\frac{1}{400} + \\frac{1}{400}} = 1.20 \\sqrt{\\frac{2}{400}} = 1.20 \\sqrt{\\frac{1}{200}} = \\frac{1.20}{10\\sqrt{2}} = \\frac{0.12}{\\sqrt{2}}\n$$\n\nThe test statistics for the two one-sided tests follow a Student's $t$-distribution with degrees of freedom $df = n_{T} + n_{R} - 2 = 400 + 400 - 2 = 798$.\n\nFor the first hypothesis, $H_{0}^{-}: \\Delta \\leq -\\Delta_{0}$, the test statistic $t^{-}$ is calculated at the boundary of the null hypothesis, $\\Delta = -\\Delta_{0} = -0.60$:\n$$\nt^{-} = \\frac{\\hat{\\Delta} - (-\\Delta_{0})}{SE(\\hat{\\Delta})} = \\frac{\\hat{\\Delta} + \\Delta_{0}}{SE(\\hat{\\Delta})} = \\frac{-0.08 + 0.60}{0.12/\\sqrt{2}} = \\frac{0.52 \\sqrt{2}}{0.12} \\approx 6.12826\n$$\nThe corresponding $p$-value, $p^{-}$, is the probability of observing a test statistic at least as extreme as $t^{-}$ in the direction of the alternative hypothesis $H_1^{-}: \\Delta  -\\Delta_0$. This is an upper-tail probability.\n$$\np^{-} = P(T_{798} \\geq t^{-}) = P(T_{798} \\geq 6.12826)\n$$\n\nFor the second hypothesis, $H_{0}^{+}: \\Delta \\geq \\Delta_{0}$, the test statistic $t^{+}$ is calculated at the boundary $\\Delta = \\Delta_{0} = 0.60$:\n$$\nt^{+} = \\frac{\\hat{\\Delta} - \\Delta_{0}}{SE(\\hat{\\Delta})} = \\frac{-0.08 - 0.60}{0.12/\\sqrt{2}} = \\frac{-0.68 \\sqrt{2}}{0.12} \\approx -8.01385\n$$\nThe corresponding $p$-value, $p^{+}$, is the probability of observing a test statistic at least as extreme as $t^{+}$ in the direction of the alternative hypothesis $H_1^{+}: \\Delta  \\Delta_0$. This is a lower-tail probability.\n$$\np^{+} = P(T_{798} \\leq t^{+}) = P(T_{798} \\leq -8.01385)\n$$\n\nCalculating the numerical values for the $p$-values:\nFor $p^{-}$, a $t$-value of $6.12826$ with $798$ degrees of freedom yields a very small upper-tail probability.\n$p^{-} \\approx 7.850 \\times 10^{-10}$.\nFor $p^{+}$, a $t$-value of $-8.01385$ with $798$ degrees of freedom yields a very small lower-tail probability.\n$p^{+} \\approx 1.980 \\times 10^{-15}$.\n\nWe round these values to four significant figures as requested:\n$p^{-} = 7.850 \\times 10^{-10}$\n$p^{+} = 1.980 \\times 10^{-15}$\n\nFinally, we interpret the results. The significance level for each one-sided test is $\\alpha = 0.05$.\nWe compare each $p$-value to $\\alpha$:\n$p^{-} = 7.850 \\times 10^{-10}  0.05$. Therefore, the null hypothesis $H_{0}^{-}$ is rejected.\n$p^{+} = 1.980 \\times 10^{-15}  0.05$. Therefore, the null hypothesis $H_{0}^{+}$ is rejected.\nSince both one-sided null hypotheses are rejected, the overall null hypothesis of non-equivalence is rejected. We conclude that equivalence has been demonstrated at the $\\alpha=0.05$ level. The true mean difference $\\Delta = \\mu_{T} - \\mu_{R}$ is statistically shown to lie within the pre-specified equivalence margin of $(-0.60, 0.60)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 7.850 \\times 10^{-10}  1.980 \\times 10^{-15} \\end{pmatrix}}\n$$", "id": "5044594"}, {"introduction": "Real-world clinical trials are rarely completed with perfect data; patient dropouts are an unavoidable reality that can introduce bias and uncertainty. A crucial question is whether the trial's conclusions are robust to the assumptions made about this missing data, especially the possibility that missingness is related to the outcome itself (Missing Not At Random, or MNAR). This practice [@problem_id:5044646] introduces the \"tipping point\" analysis, a powerful sensitivity analysis that quantifies exactly how extreme the MNAR deviation must be to overturn the primary conclusion of the study, providing a concrete measure of the result's robustness.", "problem": "A Phase III superiority randomized controlled trial (RCT) compares an investigational therapy to control on a continuous primary endpoint at Week $24$. The primary analysis is prespecified as an intention-to-treat comparison of marginal arm means using a likelihood-based model that is valid under the Missing At Random (MAR) assumption. The following are observed from the primary analysis: the estimated treatment effect under MAR, denoted $\\hat{\\tau}^{\\mathrm{MAR}}$, is $\\hat{\\tau}^{\\mathrm{MAR}} = 0.30$ units with a large-sample standard error $s = 0.12$, and the two-sided significance level is $\\alpha = 0.05$. The proportions of missing primary outcomes are $\\pi_T = 0.10$ in the treatment arm and $\\pi_C = 0.05$ in the control arm.\n\nTo assess robustness to violations of MAR, the team plans a pattern-mixture model sensitivity analysis under Missing Not At Random (MNAR), using a symmetric $\\delta$-adjusted imputation scheme that is explicitly unfavorable to the investigational therapy: for missing outcomes in the treatment arm, imputed values are shifted downward by $\\delta$ relative to the MAR-based imputation, and for missing outcomes in the control arm, imputed values are shifted upward by $\\delta$ relative to the MAR-based imputation. Assume, for the purpose of this sensitivity analysis and in line with large-sample practice, that the standard error $s$ is approximately unchanged by small to moderate $\\delta$ shifts, and that two-sided $100(1-\\alpha)\\%$ confidence intervals are used to judge statistical significance.\n\nBased on first principles about missing data mechanisms—Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR)—and mixture representations of marginal means within each arm, which option both correctly defines a tipping point analysis in this Phase III context and correctly identifies, under the described symmetric $\\delta$ adjustment, the minimal value of $\\delta$ (in endpoint units) that would lead to loss of statistical significance at the two-sided $5\\%$ level for the treatment effect estimate?\n\nA. A tipping point analysis is a systematic exploration over a grid of departures from MAR that maps combinations of MNAR shift parameters to qualitative conclusions (e.g., significant versus not significant), thereby identifying the boundary at which the conclusion “tips.” Under the described symmetric, unfavorable-to-treatment scheme, the smallest $\\delta$ that nullifies statistical significance is approximately $0.432$.\n\nB. A tipping point analysis determines the most likely MNAR mechanism from the data alone; here, because more data are missing in the treatment arm, only the difference in missingness proportions matters, so the minimal $\\delta$ that removes significance is approximately $1.296$.\n\nC. A tipping point analysis varies the number of imputations until results stabilize; under the symmetric $\\delta$ adjustment, using a one-sided $5\\%$ critical value is appropriate for superiority, yielding a minimal $\\delta$ of approximately $0.688$.\n\nD. A tipping point analysis assumes Missing Completely At Random, so the point estimate does not change with $\\delta$; thus the minimal $\\delta$ that would overturn statistical significance is $0$.\n\nE. A tipping point analysis is a pre-specified check that the result is robust if at least $50$ multiple imputations are used; because $50$ exceeds common practice, no $\\delta$ can change the conclusion, so the minimal $\\delta$ is undefined.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded, well-posed, and objective, describing a standard statistical sensitivity analysis in the context of a clinical trial. All necessary data and assumptions are provided to reach a unique, verifiable solution.\n\nThe primary task is twofold: first, to correctly define a tipping point analysis, and second, to calculate the minimal value of the sensitivity parameter $\\delta$ that would reverse the conclusion of statistical significance.\n\nFirst, let us establish the statistical framework. The primary analysis under the Missing At Random (MAR) assumption yielded an estimated treatment effect $\\hat{\\tau}^{\\mathrm{MAR}} = 0.30$ with a standard error of $s = 0.12$. The significance level is $\\alpha = 0.05$ for a two-sided test.\n\nTo assess statistical significance, we calculate the Z-statistic:\n$$\nZ = \\frac{\\hat{\\tau}^{\\mathrm{MAR}}}{s} = \\frac{0.30}{0.12} = 2.5\n$$\nThe critical value for a two-sided test at $\\alpha=0.05$ is $Z_{\\alpha/2} = Z_{0.025}$, which is approximately $1.96$. Since $|Z| = 2.5  1.96$, the null hypothesis of no treatment effect is rejected. The result is statistically significant. This can also be seen by constructing the $95\\%$ confidence interval (CI) for the treatment effect:\n$$\n\\text{CI} = \\hat{\\tau}^{\\mathrm{MAR}} \\pm Z_{0.025} \\cdot s = 0.30 \\pm 1.96 \\cdot 0.12 = 0.30 \\pm 0.2352 = [0.0648, 0.5352]\n$$\nSince the entire interval is above $0$, the treatment effect is statistically significant at the $5\\%$ level.\n\nNext, we formalize the sensitivity analysis under the Missing Not At Random (MNAR) assumption. A pattern-mixture model is used, where the marginal mean in each arm is a weighted average of the mean for subjects with observed outcomes and the mean for subjects with missing outcomes. Let $\\hat{\\mu}_{O}$ be the estimated mean for observed subjects and $\\hat{\\mu}_{M}$ be the estimated mean for missing subjects. The marginal mean is $\\hat{\\mu} = (1-\\pi)\\hat{\\mu}_{O} + \\pi\\hat{\\mu}_{M}$, where $\\pi$ is the proportion of missing data.\n\nThe MAR assumption implies that, conditional on covariates, the expected outcome is the same for observed and missing subjects. In this simplified context, this means the MAR-based imputation would use $\\hat{\\mu}_{M} = \\hat{\\mu}_{O}$. Thus, the MAR-based marginal mean estimate $\\hat{\\mu}^{\\mathrm{MAR}}$ is equal to $\\hat{\\mu}_{O}$.\n\nThe specified MNAR sensitivity analysis introduces a shift parameter $\\delta$. The scheme is \"explicitly unfavorable to the investigational therapy,\" meaning it assumes missing subjects in the treatment arm did worse than observed subjects, and missing subjects in the control arm did better.\n- In the treatment arm (T): The imputed mean for missing subjects is $\\hat{\\mu}_{M,T} = \\hat{\\mu}_{O,T} - \\delta = \\hat{\\mu}_T^{\\mathrm{MAR}} - \\delta$.\n- In the control arm (C): The imputed mean for missing subjects is $\\hat{\\mu}_{M,C} = \\hat{\\mu}_{O,C} + \\delta = \\hat{\\mu}_C^{\\mathrm{MAR}} + \\delta$.\n\nThe MNAR-adjusted marginal mean for each arm, $\\hat{\\mu}^{\\mathrm{MNAR}}$, is:\n$$\n\\hat{\\mu}_T^{\\mathrm{MNAR}}(\\delta) = (1-\\pi_T)\\hat{\\mu}_T^{\\mathrm{MAR}} + \\pi_T(\\hat{\\mu}_T^{\\mathrm{MAR}} - \\delta) = \\hat{\\mu}_T^{\\mathrm{MAR}} - \\pi_T \\delta\n$$\n$$\n\\hat{\\mu}_C^{\\mathrm{MNAR}}(\\delta) = (1-\\pi_C)\\hat{\\mu}_C^{\\mathrm{MAR}} + \\pi_C(\\hat{\\mu}_C^{\\mathrm{MAR}} + \\delta) = \\hat{\\mu}_C^{\\mathrm{MAR}} + \\pi_C \\delta\n$$\nThe MNAR-adjusted treatment effect, $\\hat{\\tau}^{\\mathrm{MNAR}}(\\delta)$, is the difference between these adjusted means:\n$$\n\\hat{\\tau}^{\\mathrm{MNAR}}(\\delta) = \\hat{\\mu}_T^{\\mathrm{MNAR}}(\\delta) - \\hat{\\mu}_C^{\\mathrm{MNAR}}(\\delta) = (\\hat{\\mu}_T^{\\mathrm{MAR}} - \\pi_T \\delta) - (\\hat{\\mu}_C^{\\mathrm{MAR}} + \\pi_C \\delta)\n$$\n$$\n\\hat{\\tau}^{\\mathrm{MNAR}}(\\delta) = (\\hat{\\mu}_T^{\\mathrm{MAR}} - \\hat{\\mu}_C^{\\mathrm{MAR}}) - \\pi_T\\delta - \\pi_C\\delta = \\hat{\\tau}^{\\mathrm{MAR}} - (\\pi_T + \\pi_C)\\delta\n$$\nThis equation shows how the estimated treatment effect decreases as the sensitivity parameter $\\delta$ increases.\n\nA \"tipping point\" analysis systematically explores the impact of such departures from the MAR assumption. It identifies the value of the sensitivity parameter(s) at which the qualitative conclusion of the study (e.g., statistical significance) \"tips\" or changes. In this case, we need to find the minimal $\\delta  0$ for which the result is no longer statistically significant.\n\nLoss of statistical significance occurs when the lower bound of the two-sided $95\\%$ confidence interval for the treatment effect becomes equal to $0$. Assuming the standard error $s$ is unchanged, the CI for the MNAR-adjusted effect is $\\hat{\\tau}^{\\mathrm{MNAR}}(\\delta) \\pm Z_{0.025} \\cdot s$. The lower bound is $0$ when:\n$$\n\\hat{\\tau}^{\\mathrm{MNAR}}(\\delta) - Z_{0.025} \\cdot s = 0\n$$\n$$\n\\hat{\\tau}^{\\mathrm{MNAR}}(\\delta) = Z_{0.025} \\cdot s\n$$\nSubstituting our expression for $\\hat{\\tau}^{\\mathrm{MNAR}}(\\delta)$:\n$$\n\\hat{\\tau}^{\\mathrm{MAR}} - (\\pi_T + \\pi_C)\\delta = Z_{0.025} \\cdot s\n$$\nNow, we solve for the tipping point value of $\\delta$:\n$$\n(\\pi_T + \\pi_C)\\delta = \\hat{\\tau}^{\\mathrm{MAR}} - Z_{0.025} \\cdot s\n$$\n$$\n\\delta = \\frac{\\hat{\\tau}^{\\mathrm{MAR}} - Z_{0.025} \\cdot s}{\\pi_T + \\pi_C}\n$$\nWe substitute the given values: $\\hat{\\tau}^{\\mathrm{MAR}} = 0.30$, $s = 0.12$, $\\pi_T = 0.10$, $\\pi_C = 0.05$, and $Z_{0.025} \\approx 1.96$:\n$$\n\\delta = \\frac{0.30 - 1.96 \\cdot 0.12}{0.10 + 0.05} = \\frac{0.30 - 0.2352}{0.15} = \\frac{0.0648}{0.15} = 0.432\n$$\nThus, a symmetric, unfavorable shift of $\\delta = 0.432$ units is the minimum required to make the treatment effect not statistically significant at the two-sided $5\\%$ level.\n\nNow we evaluate each option.\n\n**A. A tipping point analysis is a systematic exploration over a grid of departures from MAR that maps combinations of MNAR shift parameters to qualitative conclusions (e.g., significant versus not significant), thereby identifying the boundary at which the conclusion “tips.” Under the described symmetric, unfavorable-to-treatment scheme, the smallest $\\delta$ that nullifies statistical significance is approximately $0.432$.**\nThe definition of a tipping point analysis is accurate and well-formulated. The calculation matches our derived value of $\\delta = 0.432$.\n**Verdict: Correct.**\n\n**B. A tipping point analysis determines the most likely MNAR mechanism from the data alone; here, because more data are missing in the treatment arm, only the difference in missingness proportions matters, so the minimal $\\delta$ that removes significance is approximately $1.296$.**\nThe definition is incorrect. A tipping point analysis is a sensitivity analysis, not a method to identify the true MNAR mechanism, which is fundamentally impossible from observed data. The mathematical premise is also incorrect; the adjustment depends on the sum $(\\pi_T + \\pi_C)$, not the difference $(\\pi_T - \\pi_C)$. The calculation based on the difference gives $\\delta = \\frac{0.0648}{0.10 - 0.05} = \\frac{0.0648}{0.05} = 1.296$, which confirms the source of the error.\n**Verdict: Incorrect.**\n\n**C. A tipping point analysis varies the number of imputations until results stabilize; under the symmetric $\\delta$ adjustment, using a one-sided $5\\%$ critical value is appropriate for superiority, yielding a minimal $\\delta$ of approximately $0.688$.**\nThe definition is incorrect. It confuses a tipping point analysis (varying a model parameter like $\\delta$) with a check on the stability of a multiple imputation procedure (varying the number of imputations, $m$). Furthermore, the problem explicitly specified a two-sided significance level of $\\alpha = 0.05$, making the use of a one-sided critical value a violation of the problem's conditions.\n**Verdict: Incorrect.**\n\n**D. A tipping point analysis assumes Missing Completely At Random, so the point estimate does not change with $\\delta$; thus the minimal $\\delta$ that would overturn statistical significance is $0$.**\nThis is fundamentally incorrect. Tipping point analysis is explicitly for exploring MNAR scenarios, which are departures from MAR. It does not assume MCAR. The point estimate $\\hat{\\tau}^{\\mathrm{MNAR}}(\\delta)$ is a direct function of $\\delta$, so the claim that it does not change is false. A value of $\\delta = 0$ corresponds to the MAR analysis, which was found to be statistically significant.\n**Verdict: Incorrect.**\n\n**E. A tipping point analysis is a pre-specified check that the result is robust if at least $50$ multiple imputations are used; because $50$ exceeds common practice, no $\\delta$ can change the conclusion, so the minimal $\\delta$ is undefined.**\nThis definition is incorrect, again confusing the sensitivity parameter $\\delta$ with the number of imputations $m$. The number of imputations affects the precision of the simulation, not the underlying model of missingness. The conclusion that no $\\delta$ can change the result is false; a tipping point can always be calculated as long as the initial result is significant and there is missing data.\n**Verdict: Incorrect.**\n\nOnly option A provides both a correct definition of a tipping point analysis and the correctly calculated value for $\\delta$.", "answer": "$$\\boxed{A}$$", "id": "5044646"}]}