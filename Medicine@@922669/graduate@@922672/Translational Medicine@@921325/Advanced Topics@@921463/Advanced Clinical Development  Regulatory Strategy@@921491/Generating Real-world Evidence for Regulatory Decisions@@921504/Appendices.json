{"hands_on_practices": [{"introduction": "Real-world evidence (RWE) begins with accurately identifying patient cohorts, exposures, and outcomes from complex sources like electronic health records (EHRs). This often requires a \"phenotyping algorithm\"—a set of rules to classify patients. This first practice focuses on the crucial step of validating such an algorithm. By working through the validation data from a hypothetical chart review, you will calculate fundamental performance metrics like sensitivity, specificity, Positive Predictive Value ($PPV$), and Negative Predictive Value ($NPV$). You will then use these metrics to correct an observed outcome incidence for misclassification bias, a foundational skill for generating regulatory-grade evidence from imperfect real-world data. [@problem_id:5017986]", "problem": "A translational medicine team is generating Real-World Evidence (RWE) to support a regulatory submission to the United States Food and Drug Administration (FDA) regarding the safety profile of a newly approved therapy. The team uses an Electronic Health Records (EHR)-based phenotyping algorithm to identify incident cases of drug-induced liver injury (DILI) in a large cohort. To ensure the algorithm’s validity for regulatory-grade inference, the team performs a chart review, yielding a confusion matrix that validates the algorithm against a gold standard clinical adjudication.\n\nFrom a validation sample of $1{,}200$ patient records, the following counts are obtained:\n- True Positives ($TP$): $272$\n- False Positives ($FP$): $8$\n- True Negatives ($TN$): $904$\n- False Negatives ($FN$): $16$\n\nThe full study cohort contributes $52{,}000$ person-years of follow-up, during which the algorithm flags $1{,}040$ incident DILI events.\n\nStarting from first principles—namely, the core definitions of sensitivity, specificity, Positive Predictive Value (PPV), and Negative Predictive Value (NPV), and the law of total probability for binary misclassification—do the following:\n1. Compute the sensitivity, specificity, PPV, and NPV of the algorithm using the validation sample.\n2. Assuming nondifferential and time-invariant misclassification (constant sensitivity and specificity across all patient-time units) and that events are sufficiently rare to treat each person-year as a Bernoulli trial, derive an expression to correct the observed incident event proportion for misclassification using the validation-derived sensitivity and specificity. Apply this to the full cohort to obtain the adjusted incidence rate per $1{,}000$ person-years.\n\nExpress the final adjusted incidence rate in units of events per $1{,}000$ person-years, and round your final numeric answer to four significant figures. Only report the adjusted incidence rate as your final answer.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of epidemiology and biostatistics, well-posed with sufficient and consistent data, and objectively formulated. We may therefore proceed with a complete solution.\n\nThe problem asks for two main tasks: first, to compute the performance metrics of a phenotyping algorithm from a validation study; and second, to use these metrics to derive and apply a correction to an observed incidence rate in a larger cohort.\n\nLet $D^+$ denote the true presence of drug-induced liver injury (DILI), the \"condition positive,\" and $D^-$ denote its absence. Let $T^+$ denote a positive result from the algorithm (i.e., the algorithm flags a DILI event), and $T^-$ denote a negative result.\n\nThe givens from the validation sample of $1,200$ records are:\n- True Positives ($TP$): The number of cases where both the gold standard and the algorithm identify DILI. $TP = 272$.\n- False Positives ($FP$): The number of cases where the algorithm identifies DILI, but the gold standard does not. $FP = 8$.\n- True Negatives ($TN$): The number of cases where both the gold standard and the algorithm agree there is no DILI. $TN = 904$.\n- False Negatives ($FN$): The number of cases where the algorithm fails to identify DILI, but the gold standard confirms its presence. $FN = 16$.\n\nThe total number of records with true DILI is $P = TP + FN = 272 + 16 = 288$.\nThe total number of records without true DILI is $N = TN + FP = 904 + 8 = 912$.\nThe total sample size is $P + N = 288 + 912 = 1,200$, which is consistent with the problem statement.\n\n### Part 1: Algorithm Performance Metrics\n\nWe compute the four requested performance metrics from their fundamental definitions.\n\n1.  **Sensitivity ($Se$)**: The probability that the algorithm tests positive, given that the condition is truly present. It is also known as the true positive rate.\n    $$Se = P(T^+ | D^+) = \\frac{TP}{TP + FN} = \\frac{272}{272 + 16} = \\frac{272}{288} = \\frac{17}{18}$$\n\n2.  **Specificity ($Sp$)**: The probability that the algorithm tests negative, given that the condition is truly absent. It is also known as the true negative rate.\n    $$Sp = P(T^- | D^-) = \\frac{TN}{TN + FP} = \\frac{904}{904 + 8} = \\frac{904}{912} = \\frac{113}{114}$$\n\n3.  **Positive Predictive Value ($PPV$)**: The probability that the condition is truly present, given that the algorithm tests positive.\n    $$PPV = P(D^+ | T^+) = \\frac{TP}{TP + FP} = \\frac{272}{272 + 8} = \\frac{272}{280} = \\frac{34}{35}$$\n\n4.  **Negative Predictive Value ($NPV$)**: The probability that the condition is truly absent, given that the algorithm tests negative.\n    $$NPV = P(D^- | T^-) = \\frac{TN}{TN + FN} = \\frac{904}{904 + 16} = \\frac{904}{920} = \\frac{113}{115}$$\n\n### Part 2: Correction of Incidence Rate\n\nThe second task is to derive an expression to correct the observed incidence proportion for misclassification and apply it to the full cohort.\n\nLet $I_{true}$ be the true incidence proportion of DILI in the cohort, which is the probability of a person-year being a true case, $P(D^+)$.\nLet $I_{obs}$ be the observed incidence proportion based on the algorithm, which is the probability of a person-year being flagged by the algorithm, $P(T^+)$.\n\nWe can express $I_{obs}$ in terms of $I_{true}$ using the law of total probability:\n$$P(T^+) = P(T^+|D^+)P(D^+) + P(T^+|D^-)P(D^-)$$\nSubstituting the definitions of sensitivity, specificity, $I_{true}$, and $I_{obs}$:\n- $P(T^+) = I_{obs}$\n- $P(D^+) = I_{true}$\n- $P(T^+|D^+) = Se$\n- $P(D^-) = 1 - P(D^+) = 1 - I_{true}$\n- $P(T^+|D^-) = 1 - P(T^-|D^-) = 1 - Sp$\n\nThe equation becomes:\n$$I_{obs} = Se \\cdot I_{true} + (1 - Sp) \\cdot (1 - I_{true})$$\nOur goal is to solve for $I_{true}$. We rearrange the expression algebraically:\n$$I_{obs} = Se \\cdot I_{true} + 1 - Sp - (1 - Sp) \\cdot I_{true}$$\n$$I_{obs} = I_{true} \\cdot (Se - (1 - Sp)) + 1 - Sp$$\n$$I_{obs} = I_{true} \\cdot (Se + Sp - 1) + 1 - Sp$$\n$$I_{obs} - (1 - Sp) = I_{true} \\cdot (Se + Sp - 1)$$\n$$I_{true} = \\frac{I_{obs} + Sp - 1}{Se + Sp - 1}$$\nThis is the derived expression to correct the observed proportion for misclassification, also known as the Rogan-Gladen estimator.\n\nNow, we apply this formula to the full study cohort.\n- Total follow-up: $52,000$ person-years.\n- Algorithm-flagged events: $1,040$.\nThe observed incidence proportion is:\n$$I_{obs} = \\frac{1,040}{52,000} = \\frac{104}{5,200} = \\frac{1}{50} = 0.02$$\nUsing the values of $Se$ and $Sp$ derived from the validation study:\n- $Se = \\frac{17}{18}$\n- $Sp = \\frac{113}{114}$\n\nWe substitute these values into the correction formula:\n$$I_{true} = \\frac{0.02 + \\frac{113}{114} - 1}{\\frac{17}{18} + \\frac{113}{114} - 1}$$\nTo maintain precision, we work with fractions.\nNumerator:\n$$I_{obs} + Sp - 1 = \\frac{1}{50} + \\frac{113}{114} - 1 = \\frac{1}{50} - \\left(1 - \\frac{113}{114}\\right) = \\frac{1}{50} - \\frac{1}{114} = \\frac{114 - 50}{50 \\times 114} = \\frac{64}{5,700}$$\nDenominator:\n$$Se + Sp - 1 = \\frac{17}{18} + \\frac{113}{114} - 1$$\nThe least common multiple of $18 = 2 \\cdot 3^2$ and $114 = 2 \\cdot 3 \\cdot 19$ is $2 \\cdot 3^2 \\cdot 19 = 342$.\n$$Se + Sp - 1 = \\frac{17 \\cdot 19}{18 \\cdot 19} + \\frac{113 \\cdot 3}{114 \\cdot 3} - \\frac{342}{342} = \\frac{323}{342} + \\frac{339}{342} - \\frac{342}{342} = \\frac{323 + 339 - 342}{342} = \\frac{320}{342}$$\nNow, we compute the ratio for $I_{true}$:\n$$I_{true} = \\frac{\\frac{64}{5,700}}{\\frac{320}{342}} = \\frac{64}{5,700} \\cdot \\frac{342}{320}$$\nSince $320 = 5 \\times 64$, we can simplify:\n$$I_{true} = \\frac{1}{5,700} \\cdot \\frac{342}{5} = \\frac{342}{28,500}$$\nWe simplify this fraction by dividing the numerator and denominator by common factors.\n$$I_{true} = \\frac{342 \\div 6}{28,500 \\div 6} = \\frac{57}{4,750}$$\nSince $4,750 = 475 \\times 10 = (25 \\times 19) \\times 10 = 250 \\times 19$, and $57 = 3 \\times 19$:\n$$I_{true} = \\frac{3 \\times 19}{250 \\times 19} = \\frac{3}{250} = 0.012$$\nThis is the adjusted incidence proportion, representing $0.012$ true events per person-year.\n\nFinally, we must express this as an incidence rate per $1,000$ person-years.\nAdjusted Incidence Rate = $I_{true} \\times 1,000$\nAdjusted Incidence Rate = $0.012 \\times 1,000 = 12$ events per $1,000$ person-years.\n\nThe problem requires the final answer to be rounded to four significant figures. Therefore, $12$ must be expressed as $12.00$.", "answer": "$$\\boxed{12.00}$$", "id": "5017986"}, {"introduction": "Once outcomes are reliably identified, the central challenge in observational research is to mitigate confounding. This practice takes you deep into the mechanics of one of the most powerful tools for this purpose: propensity score weighting. You will implement a complete analytical pipeline, starting from simulating your own observational data to reflect known confounding structures. By building the propensity score model, constructing stabilized inverse probability of treatment weights ($IPTW$), and estimating the marginal risk difference from first principles, you will gain an intuitive and robust understanding of how these methods create a pseudo-population to emulate a randomized controlled trial and isolate a causal effect. [@problem_id:5017962]", "problem": "You are tasked with designing and implementing a program that estimates a marginal causal contrast from observational data for use in Real-World Evidence (RWE) in Translational Medicine (TM). Consider binary treatment, binary outcome, and measured confounders. Work from the following foundational base: (i) the Stable Unit Treatment Value Assumption (SUTVA), (ii) consistency, (iii) conditional exchangeability given observed covariates, and (iv) positivity. Define the propensity score as the conditional treatment probability $e(\\mathbf{X}) = \\mathbb{P}(A=1 \\mid \\mathbf{X})$. Construct stabilized inverse-probability weights using the sample marginal treatment prevalence in the numerator and the estimated propensity score in the denominator for the observed treatment. Use these weights to compute the marginal risk difference, defined as the difference between the weighted mean outcome under treatment and the weighted mean outcome under no treatment. All computations must be carried out numerically from first principles as described below without relying on external machine learning libraries.\n\nImplement the following algorithmic pipeline for each test case:\n1. Data generation. Generate $n$ independent observations of covariates $\\mathbf{X} = (X_1, X_2, X_3)$ with $X_1 \\sim \\mathcal{N}(0,1)$, $X_2 \\sim \\text{Bernoulli}(0.4)$, $X_3 \\sim \\text{Uniform}(-1,1)$. Then generate treatment $A \\in \\{0,1\\}$ from a logistic model with linear predictor $\\eta_A = \\alpha_0 + \\alpha_1 X_1 + \\alpha_2 X_2 + \\alpha_3 X_3$, and outcome $Y \\in \\{0,1\\}$ from a logistic model with linear predictor $\\eta_Y = \\gamma_0 + \\gamma_A A + \\gamma_1 X_1 + \\gamma_2 X_2 + \\gamma_3 X_3$. Use independent Bernoulli sampling with probabilities given by the logistic transformation of the respective linear predictors.\n2. Propensity score estimation. Estimate $e(\\mathbf{X})$ via penalized logistic regression by maximizing the penalized likelihood with an $\\ell_2$ (ridge) penalty of strength $\\lambda = 1.0$ on all coefficients except the intercept. Fit the model $A \\sim 1 + X_1 + X_2 + X_3$. Use an algorithm that is guaranteed to converge under near-separation, such as iteratively reweighted least squares with ridge stabilization. Include an intercept term. For numerical stability, clip any estimated probabilities to the closed interval $[10^{-5}, 1-10^{-5}]$ before weight construction.\n3. Weight construction. Construct stabilized inverse probability weights by scaling the inverse of the estimated propensity of the observed treatment by the sample marginal treatment probability. Use the sample prevalence $\\hat{p} = \\frac{1}{n}\\sum_{i=1}^n A_i$ in the numerator for treated and $1-\\hat{p}$ for untreated.\n4. Marginal risk difference. Compute the weighted risk difference as the difference between the weighted mean of $Y$ among treated and the weighted mean of $Y$ among untreated, each computed with the stabilized weights defined above. The final result for each test case is a single real number (a float).\n\nTest suite. Use the following four scientifically plausible and internally consistent test cases. For each case, the constants provided are to be treated as exact. All numeric constants must be implemented precisely as specified.\n\n- Case 1 (well-behaved, large $n$):\n  - $n = 5000$, seed $= 12345$\n  - Treatment coefficients $\\boldsymbol{\\alpha} = (0.1,\\, 0.6,\\, -0.8,\\, 0.5)$\n  - Outcome coefficients $\\boldsymbol{\\gamma} = (-0.4,\\, 0.8,\\, 0.5,\\, 0.4,\\, -0.3)$ where $\\gamma_0 = -0.4$, $\\gamma_A = 0.8$, and covariate coefficients correspond to $(X_1, X_2, X_3)$.\n\n- Case 2 (near-positivity challenge, moderate $n$):\n  - $n = 1500$, seed $= 202311$\n  - Treatment coefficients $\\boldsymbol{\\alpha} = (-2.0,\\, -3.5,\\, 2.0,\\, -1.0)$\n  - Outcome coefficients $\\boldsymbol{\\gamma} = (-0.2,\\, 1.2,\\, 0.7,\\, -0.6,\\, 0.2)$\n\n- Case 3 (small sample, mild effects):\n  - $n = 120$, seed $= 7$\n  - Treatment coefficients $\\boldsymbol{\\alpha} = (0.0,\\, 1.0,\\, 0.5,\\, -0.5)$\n  - Outcome coefficients $\\boldsymbol{\\gamma} = (-0.1,\\, 0.3,\\, 0.2,\\, 0.1,\\, 0.1)$\n\n- Case 4 (strong confounding via $X_2$):\n  - $n = 1000$, seed $= 424242$\n  - Treatment coefficients $\\boldsymbol{\\alpha} = (-0.3,\\, 0.2,\\, 3.0,\\, 0.2)$\n  - Outcome coefficients $\\boldsymbol{\\gamma} = (-0.5,\\, 0.6,\\, 0.3,\\, 1.0,\\, -0.2)$\n\nAngle units and physical units do not apply. All outputs are dimensionless. Express each final answer as a decimal rounded to six places.\n\nFinal output format. Your program should produce a single line of output containing the four weighted risk differences in the order of the cases above, rounded to six decimal places, as a comma-separated list enclosed in square brackets, for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$. No other output is permitted on any line.", "solution": "The problem requires the implementation of a computational pipeline to estimate a marginal causal effect, specifically the marginal risk difference (MRD), from simulated observational data. The methodology is based on the principle of inverse probability of treatment weighting (IPTW) using stabilized weights. The entire process, from data generation to final estimation, will be constructed from first principles as specified.\n\nThe causal estimand of interest is the marginal risk difference, $\\text{MRD} = \\mathbb{E}[Y^{a=1}] - \\mathbb{E}[Y^{a=0}]$, where $Y^a$ represents the potential outcome had an individual received treatment level $a$. To identify this estimand from observational data, we rely on the following standard assumptions:\n$1$. **Stable Unit Treatment Value Assumption (SUTVA)**: The potential outcomes for one individual are unaffected by the treatment assignment of other individuals, and there are no hidden variations of treatment.\n$2$. **Consistency**: For an individual who received treatment $A=a$, their observed outcome $Y$ is equal to their potential outcome $Y^a$, i.e., $Y = Y^A$.\n$3$. **Conditional Exchangeability**: Given the measured covariates $\\mathbf{X}$, treatment assignment is independent of the potential outcomes: $(Y^1, Y^0) \\perp A \\mid \\mathbf{X}$.\n$4$. **Positivity**: For all values of $\\mathbf{X}$ present in the population, the probability of receiving either treatment level is greater than zero: $0  \\mathbb{P}(A=1 \\mid \\mathbf{X})  1$.\n\nUnder these assumptions, the expected potential outcomes can be expressed as weighted averages of the observed outcomes, where the weights are derived from the propensity score, $e(\\mathbf{X}) = \\mathbb{P}(A=1 \\mid \\mathbf{X})$. The specified estimator uses stabilized weights to reduce variance. The weight for the $i$-th individual is given by:\n$$ w_i = A_i \\frac{\\mathbb{P}(A=1)}{e(\\mathbf{X}_i)} + (1-A_i) \\frac{\\mathbb{P}(A=0)}{1-e(\\mathbf{X}_i)} $$\nIn practice, the marginal treatment probabilities $\\mathbb{P}(A=1)$ and $\\mathbb{P}(A=0)$, and the propensity score $e(\\mathbf{X}_i)$ are unknown and must be estimated from the data. The former are estimated by the sample proportions, $\\hat{p} = \\frac{1}{n}\\sum_{i=1}^n A_i$ and $1-\\hat{p}$, respectively. The latter is estimated using a parametric model.\n\nThe algorithmic pipeline proceeds in four distinct steps:\n\n**Step 1: Data Generation**\nFor each of the $k=1, \\dots, 4$ test cases, we generate a dataset of $n$ independent and identically distributed observations.\nFirst, a set of three covariates, $\\mathbf{X} = (X_1, X_2, X_3)$, is generated for each observation $i=1, \\dots, n$:\n- $X_{i1} \\sim \\mathcal{N}(0, 1)$\n- $X_{i2} \\sim \\text{Bernoulli}(0.4)$\n- $X_{i3} \\sim \\text{Uniform}(-1, 1)$\n\nNext, the binary treatment assignment $A_i \\in \\{0, 1\\}$ is generated from a logistic model. The probability of receiving treatment is conditioned on the covariates via a linear predictor $\\eta_{A,i} = \\alpha_0 + \\alpha_1 X_{i1} + \\alpha_2 X_{i2} + \\alpha_3 X_{i3}$. The probability is given by the logistic function $\\sigma(\\eta) = (1+e^{-\\eta})^{-1}$:\n$$ \\mathbb{P}(A_i=1 \\mid \\mathbf{X}_i) = \\sigma(\\eta_{A,i}) $$\nThe observed treatment $A_i$ is a draw from a Bernoulli distribution with this probability.\n\nFinally, the binary outcome $Y_i \\in \\{0, 1\\}$ is generated from a logistic model conditional on both treatment $A_i$ and covariates $\\mathbf{X}_i$. The linear predictor is $\\eta_{Y,i} = \\gamma_0 + \\gamma_A A_i + \\gamma_1 X_{i1} + \\gamma_2 X_{i2} + \\gamma_3 X_{i3}$. The probability of a positive outcome is:\n$$ \\mathbb{P}(Y_i=1 \\mid A_i, \\mathbf{X}_i) = \\sigma(\\eta_{Y,i}) $$\nThe observed outcome $Y_i$ is a draw from a Bernoulli distribution with this probability.\n\n**Step 2: Propensity Score Estimation**\nThe true propensity score function $e(\\mathbf{X})$ is unknown and must be estimated from the data. We fit a logistic regression model for the treatment $A$ as a function of the covariates $\\mathbf{X}$, including an intercept: $A \\sim 1 + X_1 + X_2 + X_3$. The model parameters $\\boldsymbol{\\beta}$ are estimated by maximizing a penalized log-likelihood function with an $\\ell_2$ (ridge) penalty of strength $\\lambda=1.0$:\n$$ \\ell_p(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ A_i \\log(\\sigma(\\mathbf{x}_i^T\\boldsymbol{\\beta})) + (1-A_i) \\log(1-\\sigma(\\mathbf{x}_i^T\\boldsymbol{\\beta})) \\right] - \\frac{\\lambda}{2} \\sum_{j=1}^{p} \\beta_j^2 $$\nHere, $\\mathbf{x}_i$ is the design vector for the $i$-th subject (including a $1$ for the intercept), and the sum in the penalty term runs over all coefficients except the intercept $\\beta_0$.\n\nThis optimization is performed using the Iteratively Reweighted Least Squares (IRLS) algorithm. Starting with an initial guess for the coefficients $\\boldsymbol{\\beta}^{(0)}$, each iteration $t$ updates the estimate by solving a penalized weighted least squares problem. The update rule is:\n$$ \\boldsymbol{\\beta}^{(t+1)} = (\\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{X} + \\mathbf{\\Lambda})^{-1} \\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{z}^{(t)} $$\nwhere:\n- $\\mathbf{X}$ is the $n \\times (p+1)$ design matrix with an intercept column.\n- $\\mathbf{\\Lambda}$ is a diagonal matrix containing the penalty $\\lambda$ for diagonal entries corresponding to non-intercept coefficients, and $0$ for the intercept.\n- $\\mathbf{p}^{(t)} = \\sigma(\\mathbf{X}\\boldsymbol{\\beta}^{(t)})$ is the vector of predicted probabilities at iteration $t$.\n- $\\mathbf{W}^{(t)}$ is a diagonal matrix with elements $W_{ii}^{(t)} = p_i^{(t)}(1-p_i^{(t)})$.\n- $\\mathbf{z}^{(t)}$ is the working response vector, with elements $z_i^{(t)} = (\\mathbf{X}\\boldsymbol{\\beta}^{(t)})_i + \\frac{A_i - p_i^{(t)}}{p_i^{(t)}(1-p_i^{(t)})}$.\n\nAfter a fixed number of iterations, the final coefficients $\\hat{\\boldsymbol{\\beta}}$ are used to compute the estimated propensity scores $\\hat{e}(\\mathbf{X}_i) = \\sigma(\\mathbf{x}_i^T\\hat{\\boldsymbol{\\beta}})$. For numerical stability, these probabilities are clipped to the interval $[10^{-5}, 1-10^{-5}]$.\n\n**Step 3: Weight Construction**\nUsing the clipped estimated propensity scores $\\hat{e}_i$ and the sample marginal prevalence of treatment $\\hat{p} = \\frac{1}{n}\\sum_{i=1}^n A_i$, the stabilized inverse probability weight for each individual $i$ is calculated as:\n$$ w_i = A_i \\frac{\\hat{p}}{\\hat{e}_i} + (1-A_i) \\frac{1-\\hat{p}}{1-\\hat{e}_i} $$\n\n**Step 4: Marginal Risk Difference Estimation**\nThe marginal risk in the treated and untreated groups is estimated as the weighted mean of the outcome $Y$.\nThe weighted mean outcome for the treated group ($A=1$) is:\n$$ \\hat{\\mu}_1 = \\frac{\\sum_{i:A_i=1} w_i Y_i}{\\sum_{i:A_i=1} w_i} $$\nThe weighted mean outcome for the untreated group ($A=0$) is:\n$$ \\hat{\\mu}_0 = \\frac{\\sum_{i:A_i=0} w_i Y_i}{\\sum_{i:A_i=0} w_i} $$\nThe final estimated marginal risk difference is the difference between these two weighted means:\n$$ \\widehat{\\text{MRD}} = \\hat{\\mu}_1 - \\hat{\\mu}_0 $$\nThis procedure is applied to each of the four test cases specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit\n\ndef _estimate_propensity_scores_irls(X_design, A_observed, lambda_reg, max_iter=25):\n    \"\"\"\n    Estimates propensity scores using Iteratively Reweighted Least Squares (IRLS)\n    for ridge-penalized logistic regression.\n    \"\"\"\n    n_obs, n_features = X_design.shape\n    betas = np.zeros(n_features)\n    \n    # Penalty matrix Lambda: zero for intercept, lambda_reg for others\n    penalty_matrix = np.diag([0.0] + [lambda_reg] * (n_features - 1))\n    \n    for _ in range(max_iter):\n        # Linear predictor\n        eta = X_design @ betas\n        \n        # Predicted probabilities\n        p_hat = expit(eta)\n        \n        # Diagonal of weight matrix W\n        w_diag = p_hat * (1 - p_hat)\n        \n        # Working response z\n        # Add a small epsilon to the denominator to prevent division by zero\n        # in cases of near-perfect separation, which IRLS is sensitive to.\n        z = eta + (A_observed - p_hat) / (w_diag + 1e-8)\n        \n        # Update betas by solving the penalized weighted least squares problem\n        # (X^T W X + Lambda) beta = X^T W z\n        # We need to compute the inverse of (X^T W X + Lambda)\n        XtW = X_design.T * w_diag  # Uses broadcasting\n        XtWX = XtW @ X_design\n        \n        try:\n            hessian_inv = np.linalg.inv(XtWX + penalty_matrix)\n            XtWz = XtW @ z\n            betas = hessian_inv @ XtWz\n        except np.linalg.LinAlgError:\n            # If the matrix is singular, stop iterations and use current betas\n            break\n            \n    # Final predicted probabilities\n    final_eta = X_design @ betas\n    propensity_scores = expit(final_eta)\n    \n    return propensity_scores\n\ndef run_simulation_case(n, seed, alpha_coeffs, gamma_coeffs, lambda_reg=1.0):\n    \"\"\"\n    Executes the full pipeline for a single test case.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # 1. Data Generation\n    # Covariates\n    X1 = rng.normal(loc=0.0, scale=1.0, size=n)\n    X2 = rng.binomial(n=1, p=0.4, size=n)\n    X3 = rng.uniform(low=-1.0, high=1.0, size=n)\n    \n    # Design matrix for treatment model (with intercept)\n    X_a_design = np.c_[np.ones(n), X1, X2, X3]\n    \n    # Treatment generation\n    eta_A = X_a_design @ np.array(alpha_coeffs)\n    p_A = expit(eta_A)\n    A = rng.binomial(n=1, p=p_A)\n    \n    # Outcome generation\n    # Design matrix for outcome model (intercept, A, X1, X2, X3)\n    X_y_design = np.c_[np.ones(n), A, X1, X2, X3]\n    \n    eta_Y = X_y_design @ np.array(gamma_coeffs)\n    p_Y = expit(eta_Y)\n    Y = rng.binomial(n=1, p=p_Y)\n\n    # 2. Propensity Score Estimation\n    # The design matrix for the propensity score model is X_a_design\n    e_hat_raw = _estimate_propensity_scores_irls(X_a_design, A, lambda_reg)\n    \n    # Clip estimated probabilities for numerical stability\n    e_hat = np.clip(e_hat_raw, 1e-5, 1.0 - 1e-5)\n\n    # 3. Weight Construction\n    # Sample marginal treatment prevalence\n    p_hat_marginal = np.mean(A)\n    \n    # Stabilized weights\n    weights = np.where(A == 1, p_hat_marginal / e_hat, (1 - p_hat_marginal) / (1 - e_hat))\n\n    # 4. Marginal Risk Difference\n    treated_mask = (A == 1)\n    untreated_mask = (A == 0)\n    \n    # Handle cases where one group might be empty, though unlikely with these parameters\n    if np.sum(treated_mask) == 0 or np.sum(untreated_mask) == 0:\n        return np.nan # Or handle as an error\n        \n    Y_treated = Y[treated_mask]\n    w_treated = weights[treated_mask]\n    \n    Y_untreated = Y[untreated_mask]\n    w_untreated = weights[untreated_mask]\n    \n    # Weighted mean outcomes\n    mu_1_hat = np.sum(w_treated * Y_treated) / np.sum(w_treated)\n    mu_0_hat = np.sum(w_untreated * Y_untreated) / np.sum(w_untreated)\n    \n    # Marginal Risk Difference\n    mrd = mu_1_hat - mu_0_hat\n    \n    return mrd\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (well-behaved, large n)\n        {'n': 5000, 'seed': 12345,\n         'alpha_coeffs': (0.1, 0.6, -0.8, 0.5),\n         'gamma_coeffs': (-0.4, 0.8, 0.5, 0.4, -0.3)},\n        # Case 2 (near-positivity challenge, moderate n)\n        {'n': 1500, 'seed': 202311,\n         'alpha_coeffs': (-2.0, -3.5, 2.0, -1.0),\n         'gamma_coeffs': (-0.2, 1.2, 0.7, -0.6, 0.2)},\n        # Case 3 (small sample, mild effects)\n        {'n': 120, 'seed': 7,\n         'alpha_coeffs': (0.0, 1.0, 0.5, -0.5),\n         'gamma_coeffs': (-0.1, 0.3, 0.2, 0.1, 0.1)},\n        # Case 4 (strong confounding via X2)\n        {'n': 1000, 'seed': 424242,\n         'alpha_coeffs': (-0.3, 0.2, 3.0, 0.2),\n         'gamma_coeffs': (-0.5, 0.6, 0.3, 1.0, -0.2)}\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation_case(**params)\n        results.append(result)\n\n    # Format output as specified, rounding to six decimal places.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "5017962"}, {"introduction": "After carefully adjusting for all measured confounders and estimating a treatment effect, a critical question always remains: how robust is the finding to potential *unmeasured* confounding? This exercise introduces the E-value, a modern and essential sensitivity analysis tool for quantifying the resilience of an observed association. You will derive the formula and apply it to a reported risk ratio and its confidence interval. This practice moves beyond merely reporting an effect estimate and teaches you to critically appraise the strength of causal evidence, a vital skill when communicating findings to regulatory bodies and making high-stakes decisions. [@problem_id:5017951]", "problem": "A large integrated health system conducts an observational cohort study to generate Real-World Evidence (RWE) for a potential label expansion decision by the United States Food and Drug Administration (FDA). Among high-risk adults with symptomatic infection, initiation of an oral antiviral within $5$ days is associated, after extensive adjustment using propensity score weighting and outcome regression, with a $28$-day hospitalization adjusted risk ratio of $\\widehat{\\mathrm{RR}}=0.72$ with a $95\\%$ confidence interval of $[0.58,\\,0.89]$. Assume positivity, consistency, and no selection bias or outcome misclassification. To assess the potential impact of a single binary unmeasured confounder that is not conditioned on by the observed adjustment set, you are asked to quantify robustness using the E-value.\n\nStarting from the core definitions of risk, risk ratio, and the well-tested sharp bound on risk-ratio scale confounding by a single binary unmeasured confounder, derive a closed-form expression for the E-value corresponding to a point risk ratio $\\mathrm{RR}$ (with $\\mathrm{RR}\\geq 1$) and describe how to adapt it when $\\mathrm{RR}1$. Then apply your derivation to compute:\n- the E-value for the point estimate $\\widehat{\\mathrm{RR}}=0.72$, and\n- the E-value for the confidence limit closest to the null for the reported interval $[0.58,\\,0.89]$.\n\nRound each numerical result to four significant figures. Express your final answer as a row matrix in the order $\\big[$E-value for the point estimate, E-value for the confidence limit$\\big]$. Report the E-values as pure numbers without units.", "solution": "The user has requested a derivation of the E-value formula and its application to a specific scenario from an observational study.\n\n### Step 1: Extract Givens\n- Observational cohort study for Real-World Evidence (RWE).\n- Exposure: Initiation of an oral antiviral within $5$ days of symptomatic infection.\n- Outcome: $28$-day hospitalization.\n- Adjusted risk ratio: $\\widehat{\\mathrm{RR}} = 0.72$.\n- $95\\%$ confidence interval for the risk ratio: $[0.58, 0.89]$.\n- Assumptions: Positivity, consistency, no selection bias, no outcome misclassification.\n- Task: Quantify robustness to a single binary unmeasured confounder using the E-value.\n- Requested derivation: A closed-form expression for the E-value for an observed risk ratio $\\mathrm{RR}$ (with $\\mathrm{RR} \\ge 1$).\n- Requested adaptation: Description of how to handle the case when $\\mathrm{RR}  1$.\n- Requested computations:\n  1. The E-value for the point estimate $\\widehat{\\mathrm{RR}}=0.72$.\n  2. The E-value for the confidence limit closest to the null, which is $0.89$.\n- Required rounding: Four significant figures for numerical results.\n- Final answer format: A row matrix containing the two computed E-values in the specified order.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded. It addresses confounding bias in observational research, a central topic in epidemiology and biostatistics. The E-value is a well-established metric for sensitivity analysis, introduced by VanderWeele and Ding (2017), building on foundational work by Cornfield and others. The problem is well-posed, providing all necessary data ($\\widehat{\\mathrm{RR}}$, confidence interval) and a clear set of tasks (derivation and computation). The language is objective and precise. The provided numerical values are consistent; the point estimate of $0.72$ lies within the given confidence interval $[0.58, 0.89]$. The scenario is realistic and relevant to translational medicine and regulatory science. The problem is complete, consistent, and does not violate any scientific principles.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation and Calculation\n\nLet $E$ denote the binary exposure ($E=1$ for treated, $E=0$ for untreated), $D$ the binary outcome ($D=1$ for an event, $D=0$ for no event), and $U$ a single binary unmeasured confounder ($U=1$ for present, $U=0$ for absent). The observed risk ratio is $\\mathrm{RR}_{\\text{obs}} = \\frac{P(D=1|E=1)}{P(D=1|E=0)}$.\n\nThe goal of sensitivity analysis for unmeasured confounding is to quantify the strength of the associations between the confounder $U$ and both the exposure $E$ and the outcome $D$ that would be necessary to explain away the observed association. The strength of these associations is measured on the risk ratio scale:\n1.  The risk ratio for the association between the confounder and the exposure is $\\mathrm{RR}_{EU} = \\frac{P(U=1|E=1)}{P(U=1|E=0)}$.\n2.  The risk ratio for the association between the confounder and the outcome, conditional on the exposure, is $\\mathrm{RR}_{UD|E}$. Under the assumption of homogeneity, $\\mathrm{RR}_{UD} = \\frac{P(D=1|U=1, E=e)}{P(D=1|U=0, E=e)}$ for either $e=0$ or $e=1$.\n\nThe relationship between the observed risk ratio, $\\mathrm{RR}_{\\text{obs}}$, and the true, unconfounded risk ratio, $\\mathrm{RR}_{\\text{true}}$, is mediated by a bias factor, $B$, such that $\\mathrm{RR}_{\\text{obs}} = \\mathrm{RR}_{\\text{true}} \\times B$. An unmeasured confounder can explain away an observed association if it can produce a bias factor $B$ such that $\\mathrm{RR}_{\\text{true}} = 1$, which implies $B = \\mathrm{RR}_{\\text{obs}}$.\n\nA sharp (worst-case) upper bound for the bias factor $B$ due to a single binary confounder $U$ is given by:\n$$ B \\le \\frac{\\mathrm{RR}_{EU} \\cdot \\mathrm{RR}_{UD}}{\\mathrm{RR}_{EU} + \\mathrm{RR}_{UD} - 1} $$\nThis inequality assumes that both $\\mathrm{RR}_{EU}  1$ and $\\mathrm{RR}_{UD}  1$, meaning the confounder is a risk factor for both the exposure and the outcome.\n\nThe E-value is defined as the minimum value, $\\gamma$, that both $\\mathrm{RR}_{EU}$ and $\\mathrm{RR}_{UD}$ would have to attain to explain away the observed association. That is, we seek the minimum $\\gamma \\ge 1$ such that $\\mathrm{RR}_{EU} = \\gamma$ and $\\mathrm{RR}_{UD} = \\gamma$ could generate a bias factor of at least $\\mathrm{RR}_{\\text{obs}}$. This analysis is first performed for an observed risk ratio $\\mathrm{RR} \\ge 1$.\n\nSetting $\\mathrm{RR}_{EU} = \\mathrm{RR}_{UD} = \\gamma$ and requiring the bias to be at least the size of the observed risk ratio, $\\mathrm{RR}$, we have:\n$$ \\mathrm{RR} \\le \\frac{\\gamma \\cdot \\gamma}{\\gamma + \\gamma - 1} = \\frac{\\gamma^2}{2\\gamma - 1} $$\nTo find the minimum such $\\gamma$, we solve the equality:\n$$ \\mathrm{RR} = \\frac{\\gamma^2}{2\\gamma - 1} $$\nRearranging this gives a quadratic equation in $\\gamma$:\n$$ \\mathrm{RR}(2\\gamma - 1) = \\gamma^2 $$\n$$ 2\\mathrm{RR}\\gamma - \\mathrm{RR} = \\gamma^2 $$\n$$ \\gamma^2 - 2\\mathrm{RR}\\gamma + \\mathrm{RR} = 0 $$\nUsing the quadratic formula to solve for $\\gamma$:\n$$ \\gamma = \\frac{-(-2\\mathrm{RR}) \\pm \\sqrt{(-2\\mathrm{RR})^2 - 4(1)(\\mathrm{RR})}}{2(1)} $$\n$$ \\gamma = \\frac{2\\mathrm{RR} \\pm \\sqrt{4\\mathrm{RR}^2 - 4\\mathrm{RR}}}{2} $$\n$$ \\gamma = \\mathrm{RR} \\pm \\sqrt{\\mathrm{RR}^2 - \\mathrm{RR}} $$\nThe E-value, by definition, must be greater than or equal to $1$. For an observed $\\mathrm{RR}  1$, both roots are real. The function $f(\\gamma) = \\frac{\\gamma^2}{2\\gamma-1}$ is increasing for $\\gamma  1$. We are seeking the inverse of this function. The larger root, $\\gamma = \\mathrm{RR} + \\sqrt{\\mathrm{RR}^2 - \\mathrm{RR}}$, corresponds to the solution for $\\gamma \\ge 1$. This is the derived closed-form expression for the E-value for an observed risk ratio $\\mathrm{RR} \\ge 1$.\n\nWhen the observed risk ratio is less than $1$ (a protective effect), the framework must be adapted. A confounder that biases an observed $\\mathrm{RR}  1$ towards the null of $1$ has the same properties as one that biases an observed $\\mathrm{RR}  1$ away from the null. To use the derived formula, we invert the risk ratio to be on the scale of harm (i.e., $1$). If the observed risk ratio is $\\mathrm{RR}_{\\text{obs}}  1$, we calculate the E-value for its inverse, $1/\\mathrm{RR}_{\\text{obs}}$. The interpretation remains the same: it is the minimum strength of association the confounder must have with both exposure and outcome to explain away the observed protective effect, shifting the estimate to the null. Thus, for $\\mathrm{RR}  1$, the E-value is calculated as:\n$$ \\mathrm{E\\text{-}value} = \\left(\\frac{1}{\\mathrm{RR}}\\right) + \\sqrt{\\left(\\frac{1}{\\mathrm{RR}}\\right)^2 - \\left(\\frac{1}{\\mathrm{RR}}\\right)} $$\n\nNow, we apply these formulas to the given problem.\n\n1.  **E-value for the point estimate $\\widehat{\\mathrm{RR}} = 0.72$**\n    Since $\\widehat{\\mathrm{RR}} = 0.72  1$, we use the adapted formula with the inverted risk ratio, $1/0.72$.\n    $$ \\mathrm{E\\text{-}value}_{\\text{point}} = \\frac{1}{0.72} + \\sqrt{\\left(\\frac{1}{0.72}\\right)^2 - \\frac{1}{0.72}} $$\n    $$ \\mathrm{E\\text{-}value}_{\\text{point}} \\approx 1.388889 + \\sqrt{(1.388889)^2 - 1.388889} $$\n    $$ \\mathrm{E\\text{-}value}_{\\text{point}} \\approx 1.388889 + \\sqrt{1.929012 - 1.388889} $$\n    $$ \\mathrm{E\\text{-}value}_{\\text{point}} \\approx 1.388889 + \\sqrt{0.540123} $$\n    $$ \\mathrm{E\\text{-}value}_{\\text{point}} \\approx 1.388889 + 0.734931 $$\n    $$ \\mathrm{E\\text{-}value}_{\\text{point}} \\approx 2.123820 $$\n    Rounding to four significant figures, the E-value for the point estimate is $2.124$. This means that an unmeasured confounder associated with both antiviral treatment and hospitalization by a risk ratio of at least $2.124$ each, above and beyond the measured confounders, could explain away the observed point estimate.\n\n2.  **E-value for the confidence limit closest to the null**\n    The $95\\%$ confidence interval is $[0.58, 0.89]$. The null hypothesis is $\\mathrm{RR}=1$. The limit of the confidence interval that is closest to $1$ is $0.89$. We calculate the E-value for this limit, $\\mathrm{RR}_{\\text{CI}} = 0.89$.\n    Since $\\mathrm{RR}_{\\text{CI}} = 0.89  1$, we again use the adapted formula with the inverted risk ratio, $1/0.89$.\n    $$ \\mathrm{E\\text{-}value}_{\\text{CI}} = \\frac{1}{0.89} + \\sqrt{\\left(\\frac{1}{0.89}\\right)^2 - \\frac{1}{0.89}} $$\n    $$ \\mathrm{E\\text{-}value}_{\\text{CI}} \\approx 1.123596 + \\sqrt{(1.123596)^2 - 1.123596} $$\n    $$ \\mathrm{E\\text{-}value}_{\\text{CI}} \\approx 1.123596 + \\sqrt{1.262467 - 1.123596} $$\n    $$ \\mathrm{E\\text{-}value}_{\\text{CI}} \\approx 1.123596 + \\sqrt{0.138871} $$\n    $$ \\mathrm{E\\text{-}value}_{\\text{CI}} \\approx 1.123596 + 0.372654 $$\n    $$ \\mathrm{E\\text{-}value}_{\\text{CI}} \\approx 1.496250 $$\n    Rounding to four significant figures, the E-value for the confidence limit is $1.496$. This implies that to shift the upper bound of the confidence interval to the null value of $1$, an unmeasured confounder would need to be associated with both exposure and outcome by risk ratios of at least $1.496$ each. Since this value is relatively small, the inference of a protective effect is more sensitive to unmeasured confounding than the point estimate suggests.\n\nThe final answer is presented as a row matrix as requested.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2.124  1.496\n\\end{pmatrix}\n}\n$$", "id": "5017951"}]}