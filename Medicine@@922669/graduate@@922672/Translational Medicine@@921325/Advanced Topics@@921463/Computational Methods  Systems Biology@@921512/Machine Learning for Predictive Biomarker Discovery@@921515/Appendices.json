{"hands_on_practices": [{"introduction": "Biomarker data in translational medicine is often collected longitudinally, with measurements taken at irregular intervals. This exercise [@problem_id:5027193] challenges you to construct a sophisticated recurrent neural network from first principles, specifically designed to handle such time-stamped data. By implementing a Gated Recurrent Unit (GRU) with a time-aware exponential decay mechanism, you will gain a deeper understanding of how to build models that explicitly account for the dynamic and often sparse nature of clinical data.", "problem": "You are tasked with building a principled, time-aware recurrent model for binary risk prediction from irregularly sampled laboratory measurements, as required for predictive biomarker discovery in translational medicine. The model must be derived from core probabilistic and dynamical principles and implemented as a program that performs only forward computation (no training). Starting from the following fundamental bases: (i) a Bernoulli likelihood with a logistic link for binary risk, (ii) first-order exponential decay of latent state between observation times, and (iii) convex-combination gating to update latent state from new evidence, derive a recurrent computation that integrates time-stamped laboratory measurements into a latent summary, then derives an interpretable attention over time that aggregates this summary to produce a final risk. Your implementation must use a Gated Recurrent Unit (GRU) with time-aware decay of the latent state between events, a learned attention that is a normalized nonnegative weighting over time points, and a logistic output layer mapping the attended latent state to a probability in the interval $\\left[0,1\\right]$. Use only a forward pass with the fixed parameters provided below.\n\nModel specification to implement:\n- Input feature dimension is $D = 2$ and latent (hidden) dimension is $H = 3$.\n- Let $\\sigma(\\cdot)$ denote the logistic function and let $\\tanh(\\cdot)$ denote the hyperbolic tangent. Let $\\mathrm{softmax}$ denote the normalized exponential map that produces a nonnegative vector summing to $1$.\n- For a sequence of observations $\\left\\{\\mathbf{x}_t, t_t\\right\\}_{t=1}^{T}$ with $\\mathbf{x}_t \\in \\mathbb{R}^D$ and nondecreasing timestamps $t_t \\in \\mathbb{R}$, define elapsed times $\\Delta_t$ by setting $\\Delta_1 = 0$ and, for $t \\ge 2$, $\\Delta_t = t_t - t_{t-1}$ expressed in the same arbitrary time unit across all observations.\n- Between observations, the latent state must undergo first-order exponential decay per latent dimension given a strictly positive time-constant vector $\\boldsymbol{\\tau} \\in \\mathbb{R}^H_{>0}$, producing a decayed prior state before incorporating $\\mathbf{x}_t$.\n- Within each observation, update the latent state using convex-combination gating consistent with the Gated Recurrent Unit family, applied to the decayed prior state and the current input.\n- After processing all time steps, compute a scalar, nonnegative, normalized attention weight for each time step from the latent states and use it to form a convex combination (context vector). Finally, map this context vector through a logistic output layer to obtain the predicted risk.\n\nAll weights and biases are fixed and known:\n- Time constants (per hidden unit): $\\boldsymbol{\\tau} = \\begin{bmatrix} 6.0 \\\\ 3.0 \\\\ 12.0 \\end{bmatrix}$.\n- Update gate parameters:\n$\\mathbf{W}_z = \\begin{bmatrix} 0.2 & -0.1 \\\\ 0.0 & 0.3 \\\\ -0.2 & 0.1 \\end{bmatrix},\\quad\n\\mathbf{U}_z = \\begin{bmatrix} 0.1 & 0.0 & -0.1 \\\\ 0.05 & 0.05 & 0.0 \\\\ 0.0 & -0.1 & 0.1 \\end{bmatrix},\\quad\n\\mathbf{b}_z = \\begin{bmatrix} 0.0 \\\\ 0.1 \\\\ -0.1 \\end{bmatrix}.$\n- Reset gate parameters:\n$\\mathbf{W}_r = \\begin{bmatrix} -0.1 & 0.2 \\\\ 0.3 & -0.2 \\\\ 0.1 & 0.1 \\end{bmatrix},\\quad\n\\mathbf{U}_r = \\begin{bmatrix} 0.05 & -0.05 & 0.0 \\\\ 0.0 & 0.1 & -0.1 \\\\ -0.05 & 0.0 & 0.05 \\end{bmatrix},\\quad\n\\mathbf{b}_r = \\begin{bmatrix} 0.05 \\\\ 0.0 \\\\ 0.1 \\end{bmatrix}.$\n- Candidate state parameters:\n$\\mathbf{W}_h = \\begin{bmatrix} 0.3 & 0.1 \\\\ -0.2 & 0.2 \\\\ 0.1 & -0.3 \\end{bmatrix},\\quad\n\\mathbf{U}_h = \\begin{bmatrix} 0.1 & 0.0 & 0.0 \\\\ 0.0 & 0.1 & 0.0 \\\\ 0.0 & 0.0 & 0.1 \\end{bmatrix},\\quad\n\\mathbf{b}_h = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}.$\n- Attention parameters:\n$\\mathbf{w}_{\\mathrm{att}} = \\begin{bmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\end{bmatrix},\\quad b_{\\mathrm{att}} = 0.05.$\n- Output layer parameters:\n$\\mathbf{w}_{\\mathrm{out}} = \\begin{bmatrix} 0.4 \\\\ -0.2 \\\\ 0.1 \\end{bmatrix},\\quad b_{\\mathrm{out}} = -0.1.$\n\nInitialization: Use $\\mathbf{h}_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$.\n\nTest suite. Run your implementation on the following four sequences. For each case, the sequence provides the inputs $\\left\\{\\mathbf{x}_t\\right\\}_{t=1}^{T}$ and timestamps $\\left\\{t_t\\right\\}_{t=1}^{T}$:\n- Case $1$ (happy path, moderate gaps, $T=3$):\n$\\mathbf{x}_1 = \\begin{bmatrix} 1.0 \\\\ 0.5 \\end{bmatrix},\\ \\mathbf{x}_2 = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix},\\ \\mathbf{x}_3 = \\begin{bmatrix} 1.0 \\\\ -0.5 \\end{bmatrix};\\quad\nt = \\left[ 0.0,\\ 2.0,\\ 5.0 \\right].$\n- Case $2$ (boundary, zero gaps, $T=3$):\n$\\mathbf{x}_1 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\end{bmatrix},\\ \\mathbf{x}_2 = \\begin{bmatrix} 0.3 \\\\ 0.3 \\end{bmatrix},\\ \\mathbf{x}_3 = \\begin{bmatrix} -0.2 \\\\ 0.1 \\end{bmatrix};\\quad\nt = \\left[ 0.0,\\ 0.0,\\ 0.0 \\right].$\n- Case $3$ (edge, very long gap, $T=2$):\n$\\mathbf{x}_1 = \\begin{bmatrix} 1.5 \\\\ 0.0 \\end{bmatrix},\\ \\mathbf{x}_2 = \\begin{bmatrix} 0.0 \\\\ -1.0 \\end{bmatrix};\\quad\nt = \\left[ 0.0,\\ 48.0 \\right].$\n- Case $4$ (edge, single observation, $T=1$):\n$\\mathbf{x}_1 = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix};\\quad\nt = \\left[ 0.0 \\right].$\n\nRequired outputs:\n- For each case, compute the predicted risk $\\hat{y} \\in \\left[0,1\\right]$ and the attention weight vector $\\boldsymbol{\\alpha} \\in \\mathbb{R}^T$ with nonnegative entries summing to $1$.\n- Round all reported floating-point numbers to $6$ decimal places.\n- Final program output format: a single line containing a list of case-wise results, each as a two-element list of the form $\\left[\\hat{y},[\\alpha_1,\\ldots,\\alpha_T]\\right]$. For example, the printed output must look like a single Python-style list without spaces: `[[.,[.,...]],[.,[.,...]],[.,[.,...]],[.,[.,...]]]` but with actual numbers in place of the dots and no spaces anywhere.", "solution": "The user has provided a well-defined problem for implementing a forward pass of a time-aware Gated Recurrent Unit (GRU) model with a temporal attention mechanism. The problem is scientifically grounded in established machine learning principles, is self-contained with all necessary parameters and test data, and is computationally well-posed. Therefore, the problem is deemed valid.\n\nThe task is to compute a binary risk prediction from a sequence of irregularly sampled, time-stamped measurements. The model architecture is specified as a GRU variant that incorporates exponential decay of its latent state between observations, coupled with an attention mechanism to produce the final output. The following derivation and implementation plan outlines the computational steps based on the provided specifications.\n\n**1. Model Definition and Principles**\n\nThe model processes a sequence of $T$ observations, where each observation at step $t$ consists of a feature vector $\\mathbf{x}_t \\in \\mathbb{R}^D$ and a timestamp $t_t$. The model's objective is to map this sequence to a final risk probability $\\hat{y} \\in [0, 1]$.\n\n**Activation Functions:**\nThe model employs three standard activation functions:\n- The logistic sigmoid function, $\\sigma(z) = (1 + \\exp(-z))^{-1}$, which maps real numbers to the interval $(0, 1)$.\n- The hyperbolic tangent function, $\\tanh(z) = (\\exp(z) - \\exp(-z)) / (\\exp(z) + \\exp(-z))$, which maps real numbers to the interval $(-1, 1)$.\n- The softmax function, which transforms a vector $\\mathbf{v} \\in \\mathbb{R}^K$ into a probability distribution $\\boldsymbol{p} \\in \\mathbb{R}^K$ where $p_i = \\exp(v_i) / \\sum_{j=1}^K \\exp(v_j)$.\n\n**2. Recurrent State Update with Time-Aware Decay**\n\nThe core of the model is a recurrent neural network that maintains a latent state vector $\\mathbf{h}_t \\in \\mathbb{R}^H$. The initial state is given as $\\mathbf{h}_0 = \\mathbf{0}$. For each time step $t \\in \\{1, \\dots, T\\}$, the model performs the following sequence of operations.\n\n**2.1. Exponential State Decay:**\nThe problem mandates a first-order exponential decay of the latent state to account for the time elapsed between observations. The time difference is $\\Delta_t = t_t - t_{t-1}$, with the convention that $\\Delta_1 = 0$. The hidden state from the previous step, $\\mathbf{h}_{t-1}$, is decayed element-wise according to the time-constant vector $\\boldsymbol{\\tau} \\in \\mathbb{R}^H_{>0}$. The decayed state, denoted $\\mathbf{h}'_{t-1}$, is computed as:\n$$\n\\mathbf{h}'_{t-1} = \\mathbf{h}_{t-1} \\odot \\exp(-\\Delta_t / \\boldsymbol{\\tau})\n$$\nwhere $\\odot$ represents the element-wise (Hadamard) product. This decay mechanism ensures that the influence of past information diminishes over longer time gaps, which is a crucial aspect of modeling irregularly sampled time series. For the first step ($t=1$), $\\Delta_1=0$, so $\\mathbf{h}'_0 = \\mathbf{h}_0$.\n\n**2.2. GRU-based Update:**\nThe decayed state $\\mathbf{h}'_{t-1}$ and the current input $\\mathbf{x}_t$ are used to compute the new hidden state $\\mathbf{h}_t$ via a GRU-like gating mechanism.\n\n- **Reset Gate ($\\mathbf{r}_t$):** This gate determines how much of the previous state to forget.\n$$\n\\mathbf{r}_t = \\sigma(\\mathbf{W}_r \\mathbf{x}_t + \\mathbf{U}_r \\mathbf{h}'_{t-1} + \\mathbf{b}_r)\n$$\n\n- **Update Gate ($\\mathbf{z}_t$):** This gate determines the extent to which the new candidate state updates the existing state.\n$$\n\\mathbf{z}_t = \\sigma(\\mathbf{W}_z \\mathbf{x}_t + \\mathbf{U}_z \\mathbf{h}'_{t-1} + \\mathbf{b}_z)\n$$\n\n- **Candidate Hidden State ($\\tilde{\\mathbf{h}}_t$):** This is a proposed new state, computed using the reset gate to selectively incorporate information from the decayed past state.\n$$\n\\tilde{\\mathbf{h}}_t = \\tanh(\\mathbf{W}_h \\mathbf{x}_t + \\mathbf{U}_h (\\mathbf{r}_t \\odot \\mathbf{h}'_{t-1}) + \\mathbf{b}_h)\n$$\n\n- **New Hidden State ($\\mathbf{h}_t$):** The final state for the current time step is a convex combination of the decayed previous state and the candidate state, controlled by the update gate.\n$$\n\\mathbf{h}_t = (1 - \\mathbf{z}_t) \\odot \\mathbf{h}'_{t-1} + \\mathbf{z}_t \\odot \\tilde{\\mathbf{h}}_t\n$$\n\nThis process is repeated for all $T$ time steps, yielding a sequence of hidden states $\\{\\mathbf{h}_1, \\mathbf{h}_2, \\dots, \\mathbf{h}_T\\}$.\n\n**3. Attention Mechanism and Final Prediction**\n\nAfter processing the entire sequence, an attention mechanism is applied to the sequence of hidden states to form a context-aware summary, which is then used for the final prediction.\n\n**3.1. Attention Scores and Weights:**\nFirst, a scalar attention score $e_t$ is computed for each hidden state $\\mathbf{h}_t$. This score quantifies the relevance of the state at time $t$ for the final prediction.\n$$\ne_t = \\mathbf{w}_{\\mathrm{att}}^\\top \\mathbf{h}_t + b_{\\mathrm{att}}\n$$\nThe scores for all time steps, $\\mathbf{e} = [e_1, e_2, \\dots, e_T]^\\top$, are then normalized using the softmax function to produce a probability distribution of attention weights $\\boldsymbol{\\alpha} = [\\alpha_1, \\alpha_2, \\dots, \\alpha_T]^\\top$.\n$$\n\\boldsymbol{\\alpha} = \\mathrm{softmax}(\\mathbf{e})\n$$\nEach weight $\\alpha_t$ is non-negative, and $\\sum_{t=1}^T \\alpha_t = 1$.\n\n**3.2. Context Vector:**\nThe context vector $\\mathbf{c} \\in \\mathbb{R}^H$ is a weighted average of all hidden states, where the weights are given by the attention distribution $\\boldsymbol{\\alpha}$.\n$$\n\\mathbf{c} = \\sum_{t=1}^T \\alpha_t \\mathbf{h}_t\n$$\n\n**3.3. Output Layer:**\nFinally, the context vector $\\mathbf{c}$ is passed through a single-neuron output layer with a logistic sigmoid activation function to compute the final risk prediction $\\hat{y}$.\n$$\n\\hat{y} = \\sigma(\\mathbf{w}_{\\mathrm{out}}^\\top \\mathbf{c} + b_{\\mathrm{out}})\n$$\n\nThis completes the forward pass of the model. The implementation will involve applying these equations to the provided parameters and test cases. All vectors are treated as column vectors in matrix operations.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the forward pass of a time-aware GRU with attention for risk prediction.\n    \"\"\"\n\n    # --- Model Parameters ---\n    # Dimensions\n    # D = 2\n    # H = 3\n\n    # Time constants (as a column vector for broadcasting)\n    tau = np.array([[6.0], [3.0], [12.0]])\n\n    # Update gate parameters\n    W_z = np.array([[0.2, -0.1], [0.0, 0.3], [-0.2, 0.1]])\n    U_z = np.array([[0.1, 0.0, -0.1], [0.05, 0.05, 0.0], [0.0, -0.1, 0.1]])\n    b_z = np.array([[0.0], [0.1], [-0.1]])\n\n    # Reset gate parameters\n    W_r = np.array([[-0.1, 0.2], [0.3, -0.2], [0.1, 0.1]])\n    U_r = np.array([[0.05, -0.05, 0.0], [0.0, 0.1, -0.1], [-0.05, 0.0, 0.05]])\n    b_r = np.array([[0.05], [0.0], [0.1]])\n\n    # Candidate state parameters\n    W_h = np.array([[0.3, 0.1], [-0.2, 0.2], [0.1, -0.3]])\n    U_h = np.array([[0.1, 0.0, 0.0], [0.0, 0.1, 0.0], [0.0, 0.0, 0.1]])\n    b_h = np.array([[0.0], [0.0], [0.0]])\n\n    # Attention parameters\n    w_att = np.array([[0.2], [-0.1], [0.3]])\n    b_att = 0.05\n\n    # Output layer parameters\n    w_out = np.array([[0.4], [-0.2], [0.1]])\n    b_out = -0.1\n\n    # Initial hidden state\n    h_0 = np.array([[0.0], [0.0], [0.0]])\n\n    # --- Test Cases ---\n    test_cases = [\n        {\n            \"X\": [np.array([[1.0], [0.5]]), np.array([[0.0], [1.0]]), np.array([[1.0], [-0.5]])],\n            \"t\": [0.0, 2.0, 5.0],\n        },\n        {\n            \"X\": [np.array([[0.5], [-0.5]]), np.array([[0.3], [0.3]]), np.array([[-0.2], [0.1]])],\n            \"t\": [0.0, 0.0, 0.0],\n        },\n        {\n            \"X\": [np.array([[1.5], [0.0]]), np.array([[0.0], [-1.0]])],\n            \"t\": [0.0, 48.0],\n        },\n        {\n            \"X\": [np.array([[0.1], [0.2]])],\n            \"t\": [0.0],\n        },\n    ]\n\n    # --- Helper Functions ---\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n\n    def softmax(x):\n        # Numerically stable softmax\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum(axis=0)\n\n    # --- Main Computation Loop ---\n    all_results = []\n    \n    for case in test_cases:\n        X = case[\"X\"]\n        t_stamps = case[\"t\"]\n        T = len(t_stamps)\n\n        h_prev = h_0\n        t_prev = t_stamps[0]\n        hidden_states = []\n\n        # Recurrent updates\n        for t_idx in range(T):\n            x_t = X[t_idx]\n            current_t = t_stamps[t_idx]\n            \n            delta_t = current_t - t_prev\n            \n            # 1. State Decay\n            h_decayed = h_prev * np.exp(-delta_t / tau)\n            \n            # 2. GRU Gates\n            z_t = sigmoid(W_z @ x_t + U_z @ h_decayed + b_z)\n            r_t = sigmoid(W_r @ x_t + U_r @ h_decayed + b_r)\n            \n            # 3. Candidate State\n            h_tilde_t = np.tanh(W_h @ x_t + U_h @ (r_t * h_decayed) + b_h)\n            \n            # 4. New Hidden State\n            h_t = (1 - z_t) * h_decayed + z_t * h_tilde_t\n            \n            hidden_states.append(h_t)\n            h_prev = h_t\n            t_prev = current_t\n\n        # Stack hidden states for vectorized attention calculation\n        H_sequence = np.hstack(hidden_states) # Shape (H, T)\n        \n        # 5. Attention Mechanism\n        # Scores (e_t)\n        # w_att.T is (1, H), H_sequence is (H, T) -> result is (1, T)\n        e = w_att.T @ H_sequence + b_att \n        \n        # Weights (alpha)\n        alpha = softmax(e.T) # Pass a column vector to softmax, result is column (T, 1)\n\n        # 6. Context Vector\n        # H_sequence is (H, T), alpha is (T, 1) -> result is (H, 1)\n        c = H_sequence @ alpha\n\n        # 7. Final Prediction\n        y_hat = sigmoid(w_out.T @ c + b_out).item()\n\n        # Prepare results for formatting\n        alpha_list = alpha.flatten().tolist()\n        all_results.append([y_hat, alpha_list])\n\n    # --- Format final output string ---\n    case_strings = []\n    for y_hat, alpha_list in all_results:\n        alpha_str = f\"[{','.join(f'{val:.6f}' for val in alpha_list)}]\"\n        case_strings.append(f\"[{y_hat:.6f},{alpha_str}]\")\n    \n    final_output_str = f\"[{','.join(case_strings)}]\"\n    print(final_output_str)\n\nsolve()\n```", "id": "5027193"}, {"introduction": "A predictive model's utility in a clinical setting depends not only on its discrimination but also its calibration—the agreement between predicted probabilities and observed outcomes. This practice [@problem_id:5027251] provides a hands-on guide to assessing and correcting model calibration for time-to-event data, a common scenario in oncology. You will implement the inverse probability of censoring weighting (IPCW) method to obtain an unbiased estimate of observed events and perform a recalibration, mastering essential skills for robustly validating survival models.", "problem": "You are given predicted survival probabilities at a fixed time horizon for cohorts in translational medicine, together with right-censored time-to-event observations. Your task is to evaluate calibration-in-the-large for survival predictions at a fixed horizon by comparing expected versus observed events under right censoring and, if necessary, perform a recalibration that adjusts only an intercept on the log-odds scale so that the total expected events matches the observed events at the horizon.\n\nStart from the following fundamental base: (i) the survival function $S(t) = \\mathbb{P}(T > t)$; (ii) the cumulative incidence at a fixed horizon $t^\\star$ is $\\mathbb{P}(T \\le t^\\star)$; (iii) under independent right censoring, the Kaplan–Meier estimator for the censoring survival function $G(t) = \\mathbb{P}(C \\ge t)$ is obtained by treating censoring times as events and event times as censoring; (iv) inverse probability of censoring weighting (IPCW) yields an unbiased estimator of the cumulative incidence at $t^\\star$ via weights $1/\\widehat{G}(t^-)$ on observed events at times $t \\le t^\\star$, where $\\widehat{G}(t^-)$ is the left limit of the Kaplan–Meier estimate at $t$.\n\nDefinitions to use in your derivation and implementation:\n- For subject $i \\in \\{1,\\dots,n\\}$, denote observed time $T_i = \\min\\{T_i^{\\text{true}}, C_i\\}$ and event indicator $\\Delta_i = \\mathbb{I}\\{T_i^{\\text{true}} \\le C_i\\}$.\n- You are given model-predicted survival probabilities at the horizon $t^\\star$, denoted $p_i = \\widehat{S}_i(t^\\star)$. The corresponding predicted event probabilities at $t^\\star$ are $q_i = 1 - p_i$.\n- The expected number of events according to the model is $E = \\sum_{i=1}^n q_i$.\n- The IPCW estimate of the observed number of events by $t^\\star$ is $O = \\sum_{i=1}^n \\frac{\\mathbb{I}\\{T_i \\le t^\\star, \\Delta_i = 1\\}}{\\widehat{G}(T_i^-)}$, where $\\widehat{G}$ is the Kaplan–Meier estimator of the censoring survival function built by treating $\\Delta_i = 0$ as censoring events and $\\Delta_i = 1$ as non-events.\n- Define calibration-in-the-large per subject as $\\text{CIL} = \\frac{E - O}{n}$.\n- Recalibration-in-the-large modifies only an intercept $b \\in \\mathbb{R}$ on the log-odds scale, mapping each $q_i$ to $q_i^{\\text{recal}}(b) = \\sigma\\!\\left(\\operatorname{logit}(q_i) + b\\right)$, where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ and $\\operatorname{logit}(q) = \\log\\!\\left(\\frac{q}{1-q}\\right)$. The intercept $b$ is chosen to solve $\\sum_{i=1}^n q_i^{\\text{recal}}(b) = O$. Existence and uniqueness hold whenever $O \\in (0,n)$.\n\nAlgorithmic requirements:\n- Estimate the censoring survival $\\widehat{G}(t)$ using the Kaplan–Meier product-limit estimator where, at each observed time $u$, the decrement is determined by the number of censoring events $d_c(u) = \\sum_{i=1}^n \\mathbb{I}\\{T_i = u, \\Delta_i = 0\\}$ and the risk set size $R(u) = \\sum_{i=1}^n \\mathbb{I}\\{T_i \\ge u\\}$. Then $\\widehat{G}(u) = \\widehat{G}(u^-)\\left(1 - \\frac{d_c(u)}{R(u)}\\right)$ with $\\widehat{G}(0) = 1$, and $\\widehat{G}(t^-)$ for an event time $t$ is the product over all censoring times $u < t$ of the above multiplicative factors.\n- To avoid division by zero in IPCW, clip $\\widehat{G}(\\cdot)$ below by a small constant $\\varepsilon = 10^{-6}$.\n- Decide that recalibration is needed if $|E - O| > \\tau$, where $\\tau = 0.5$.\n- If recalibration is needed, compute $b$ by solving $\\sum_{i=1}^n \\sigma(\\operatorname{logit}(q_i) + b) = \\tilde{O}$ via a robust root-finding method. Here $\\tilde{O} = \\min\\{\\max\\{O, 10^{-6}\\}, n - 10^{-6}\\}$ ensures solvability. If recalibration is not needed, set $b = 0$ and $q_i^{\\text{recal}} = q_i$.\n\nNumerical and output requirements:\n- All times are in months; $t^\\star$ and all $T_i$ must be treated in months. The final outputs are dimensionless and must be rounded to $6$ decimals.\n- For each test case, produce a list with $5$ floats in the order $[\\text{CIL}, b, E, O, E^{\\text{recal}}]$, where $E^{\\text{recal}} = \\sum_{i=1}^n q_i^{\\text{recal}}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list corresponding to one test case, e.g., `[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\dots]`.\n\nTest suite:\nProvide code that computes the outputs for the following four parameter sets. Each test case includes a fixed horizon $t^\\star$, observed data $\\{(T_i,\\Delta_i)\\}_{i=1}^n$, and predicted survival probabilities $\\{p_i\\}_{i=1}^n$ at $t^\\star$. For each case, compute $q_i = 1 - p_i$, $E$, $O$, $\\text{CIL}$, decide on recalibration with threshold $\\tau = 0.5$, compute $b$ if needed, and return $E^{\\text{recal}}$.\n\n- Test case A (general case, moderate censoring):\n  - $t^\\star = 12$.\n  - $T = [\\,5,\\,7,\\,10,\\,15,\\,20,\\,8,\\,14,\\,9\\,]$.\n  - $\\Delta = [\\,1,\\,0,\\,1,\\,1,\\,0,\\,0,\\,1,\\,1\\,]$.\n  - $p = [\\,0.8,\\,0.9,\\,0.7,\\,0.6,\\,0.95,\\,0.85,\\,0.65,\\,0.75\\,]$.\n\n- Test case B (near-perfect calibration by construction):\n  - $t^\\star = 5$.\n  - $T = [\\,2,\\,4,\\,6,\\,7,\\,5\\,]$.\n  - $\\Delta = [\\,1,\\,1,\\,0,\\,1,\\,0\\,]$.\n  - $p = [\\,0.6,\\,0.6,\\,0.6,\\,0.6,\\,0.6\\,]$.\n\n- Test case C (extreme predicted probabilities, light censoring before $t^\\star$):\n  - $t^\\star = 10$.\n  - $T = [\\,3,\\,9,\\,12,\\,4,\\,20,\\,18\\,]$.\n  - $\\Delta = [\\,0,\\,1,\\,1,\\,0,\\,0,\\,1\\,]$.\n  - $p = [\\,0.01,\\,0.2,\\,0.2,\\,0.99,\\,0.99,\\,0.3\\,]$.\n\n- Test case D (heavy censoring before $t^\\star$):\n  - $t^\\star = 8$.\n  - $T = [\\,2,\\,3,\\,5,\\,7,\\,9,\\,11,\\,4\\,]$.\n  - $\\Delta = [\\,0,\\,0,\\,1,\\,0,\\,1,\\,1,\\,0\\,]$.\n  - $p = [\\,0.9,\\,0.85,\\,0.4,\\,0.95,\\,0.3,\\,0.25,\\,0.9\\,]$.", "solution": "The objective is to evaluate the calibration-in-the-large of a survival prediction model at a fixed time horizon $t^\\star$, given right-censored time-to-event data. If the model is miscalibrated, an intercept-only recalibration on the log-odds scale is performed. The entire procedure is grounded in established principles of survival analysis and model calibration.\n\nLet the dataset consist of $n$ subjects, with observations $(T_i, \\Delta_i, p_i)$ for $i=1,\\dots,n$, where $T_i$ is the observed time, $\\Delta_i$ is the event indicator ($1$ for an event, $0$ for censoring), and $p_i = \\widehat{S}_i(t^\\star)$ is the model-predicted survival probability at the horizon $t^\\star$.\n\n**Step 1: Expected Number of Events (Model-Based)**\n\nThe predicted probability of an event for subject $i$ by time $t^\\star$ is the complement of the survival probability, $q_i = 1 - p_i$. The total number of events expected under the model is the sum of these individual probabilities:\n$$\nE = \\sum_{i=1}^n q_i\n$$\n\n**Step 2: Observed Number of Events (IPCW-Estimated)**\n\nIn the presence of right censoring, a naive count of observed events $\\sum \\mathbb{I}\\{T_i \\le t^\\star, \\Delta_i=1\\}$ is a biased estimator of the true number of events. To obtain an unbiased estimate, we employ Inverse Probability of Censoring Weighting (IPCW). Each observed event is weighted by the inverse of the probability of remaining uncensored up to its event time.\n\nThis requires estimating the censoring survival function, $G(t) = \\mathbb{P}(C \\ge t)$, where $C$ is the censoring time. We use the Kaplan-Meier product-limit estimator, but applied to the censoring process. Here, a censoring event ($\\Delta_i=0$) is treated as the \"event\" of interest. Let $u_1 < u_2 < \\dots < u_m$ be the unique times at which censoring occurs. The Kaplan-Meier estimate for $G(t)$ is given by:\n$$\n\\widehat{G}(t) = \\prod_{u_j \\le t} \\left(1 - \\frac{d_c(u_j)}{R(u_j)}\\right)\n$$\nwhere $d_c(u_j) = \\sum_{i=1}^n \\mathbb{I}\\{T_i = u_j, \\Delta_i = 0\\}$ is the number of subjects censored at time $u_j$, and $R(u_j) = \\sum_{i=1}^n \\mathbb{I}\\{T_i \\ge u_j\\}$ is the number of subjects at risk just prior to $u_j$.\n\nThe IPCW estimator for the observed number of events, $O$, sums over all subjects who experience an event at or before the horizon $t^\\star$. The weight for a subject $i$ with an event at time $T_i$ is $1/\\widehat{G}(T_i^-)$, where $\\widehat{G}(T_i^-) = \\lim_{t \\to T_i^-} \\widehat{G}(t)$ is the estimated probability of not being censored just before time $T_i$. The estimator is:\n$$\nO = \\sum_{i=1}^n \\frac{\\mathbb{I}\\{T_i \\le t^\\star, \\Delta_i = 1\\}}{\\max(\\widehat{G}(T_i^-), \\varepsilon)}\n$$\nA small constant $\\varepsilon = 10^{-6}$ is used to clip the denominator to prevent division by zero for events that occur after the last censored observation time.\n\n**Step 3: Calibration Assessment**\n\nCalibration-in-the-large compares the total expected events $E$ to the total observed events $O$. We define the per-subject metric $\\text{CIL}$ as their scaled difference:\n$$\n\\text{CIL} = \\frac{E - O}{n}\n$$\nA value of $\\text{CIL} > 0$ indicates overprediction of events (underprediction of survival), while $\\text{CIL} < 0$ indicates underprediction of events. Recalibration is deemed necessary if the magnitude of the discrepancy exceeds a threshold $\\tau=0.5$, i.e., if $|E - O| > \\tau$.\n\n**Step 4: Recalibration-in-the-Large**\n\nIf recalibration is required, we adjust the model's predictions by modifying the intercept on the log-odds scale. The original predicted event probability $q_i$ is mapped to a recalibrated probability $q_i^{\\text{recal}}(b)$ via:\n$$\nq_i^{\\text{recal}}(b) = \\sigma(\\operatorname{logit}(q_i) + b)\n$$\nwhere $\\sigma(x) = (1 + e^{-x})^{-1}$ is the sigmoid function and $\\operatorname{logit}(q) = \\log(q/(1-q))$ is the logit function. The adjustment parameter $b$ is a single scalar applied to all subjects.\n\nThe value of $b$ is determined by solving for the root of the equation that forces the sum of recalibrated probabilities to match the observed event count:\n$$\n\\sum_{i=1}^n q_i^{\\text{recal}}(b) = \\tilde{O} \\implies f(b) = \\sum_{i=1}^n \\sigma(\\operatorname{logit}(q_i) + b) - \\tilde{O} = 0\n$$\nTo ensure the existence and uniqueness of the solution for $b$, the target value $O$ is clipped to $\\tilde{O} = \\min\\{\\max\\{O, \\varepsilon\\}, n - \\varepsilon\\}$. Since the function $f(b)$ is strictly monotonic with respect to $b$, a unique root exists and can be found using a robust numerical method like Brent's method. If no recalibration is performed, we set $b=0$.\n\n**Step 5: Final Metrics**\n\nThe final output for each test case is a set of five values:\n1.  $\\text{CIL}$: The initial calibration-in-the-large metric.\n2.  $b$: The computed recalibration intercept ($0$ if no recalibration was performed).\n3.  $E$: The initial total expected number of events predicted by the model.\n4.  $O$: The IPCW-estimated total observed number of events.\n5.  $E^{\\text{recal}}$: The total expected number of events after recalibration, calculated as $E^{\\text{recal}} = \\sum_{i=1}^n q_i^{\\text{recal}}(b)$. By construction, if recalibration is performed, $E^{\\text{recal}} \\approx \\tilde{O}$. If not, $E^{\\text{recal}} = E$.\n\nThis comprehensive procedure allows for a rigorous quantitative assessment and correction of a key aspect of survival model performance.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n\n    test_cases = [\n        # Test case A\n        {\n            \"t_star\": 12.0,\n            \"T\": np.array([5.0, 7.0, 10.0, 15.0, 20.0, 8.0, 14.0, 9.0]),\n            \"Delta\": np.array([1, 0, 1, 1, 0, 0, 1, 1]),\n            \"p\": np.array([0.8, 0.9, 0.7, 0.6, 0.95, 0.85, 0.65, 0.75]),\n        },\n        # Test case B\n        {\n            \"t_star\": 5.0,\n            \"T\": np.array([2.0, 4.0, 6.0, 7.0, 5.0]),\n            \"Delta\": np.array([1, 1, 0, 1, 0]),\n            \"p\": np.array([0.6, 0.6, 0.6, 0.6, 0.6]),\n        },\n        # Test case C\n        {\n            \"t_star\": 10.0,\n            \"T\": np.array([3.0, 9.0, 12.0, 4.0, 20.0, 18.0]),\n            \"Delta\": np.array([0, 1, 1, 0, 0, 1]),\n            \"p\": np.array([0.01, 0.2, 0.2, 0.99, 0.99, 0.3]),\n        },\n        # Test case D\n        {\n            \"t_star\": 8.0,\n            \"T\": np.array([2.0, 3.0, 5.0, 7.0, 9.0, 11.0, 4.0]),\n            \"Delta\": np.array([0, 0, 1, 0, 1, 1, 0]),\n            \"p\": np.array([0.9, 0.85, 0.4, 0.95, 0.3, 0.25, 0.9]),\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = calculate_metrics(case[\"t_star\"], case[\"T\"], case[\"Delta\"], case[\"p\"])\n        all_results.append(result)\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"[{','.join(map(str, res))}]\" for res in all_results]) + \"]\"\n    print(output_str)\n\n\ndef calculate_metrics(t_star, T, Delta, p):\n    \"\"\"\n    Calculates calibration metrics and performs recalibration for a single case.\n    \"\"\"\n    n = len(T)\n    epsilon = 1e-6\n    tau = 0.5\n    \n    # Step 1: Expected number of events\n    q = 1.0 - p\n    E = np.sum(q)\n\n    # Step 2: Kaplan-Meier estimator for censoring distribution G(t)\n    # This function provides G_hat(t^-), the survival prob of censoring just before t.\n    def get_G_minus_factory(times, deltas):\n        # Pre-compute the KM curve for censoring (event is Delta=0)\n        all_unique_times = np.sort(np.unique(times))\n        \n        km_times = [0.0]\n        km_probs = [1.0]\n        current_prob = 1.0\n\n        for u_time in all_unique_times:\n            at_risk = np.sum(times >= u_time)\n            # Censoring is the \"event\" for this KM estimate\n            censored_at_u = np.sum((times == u_time)  (deltas == 0))\n            \n            if censored_at_u > 0 and at_risk > 0:\n                current_prob *= (1.0 - censored_at_u / at_risk)\n            \n            km_times.append(u_time)\n            km_probs.append(current_prob)\n        \n        km_times = np.array(km_times)\n        km_probs = np.array(km_probs)\n        memo = {}\n        \n        def _get_G_minus(t_event):\n            if t_event in memo:\n                return memo[t_event]\n            # Find index of first time >= t_event\n            idx = np.searchsorted(km_times, t_event, side='left')\n            # G(t-) is the probability at the time point just before\n            res = km_probs[idx - 1]\n            memo[t_event] = res\n            return res\n            \n        return _get_G_minus\n\n    get_g_minus = get_G_minus_factory(T, Delta)\n\n    # Step 3: Observed number of events via IPCW\n    O = 0.0\n    for i in range(n):\n        if Delta[i] == 1 and T[i] = t_star:\n            g_ti_minus = get_g_minus(T[i])\n            weight = 1.0 / np.maximum(g_ti_minus, epsilon)\n            O += weight\n            \n    # Step 4: Calibration-in-the-large\n    CIL = (E - O) / n\n\n    # Step 5: Recalibration\n    b = 0.0\n    if abs(E - O) > tau:\n        O_tilde = np.clip(O, epsilon, n - epsilon)\n        \n        # Clip q to avoid inf/-inf in logit\n        q_clipped = np.clip(q, epsilon, 1 - epsilon)\n        logit_q = np.log(q_clipped / (1.0 - q_clipped))\n\n        def root_func(bias, lq, target):\n            recal_probs = 1.0 / (1.0 + np.exp(-(lq + bias)))\n            return np.sum(recal_probs) - target\n        \n        try:\n            # A generous but safe interval for the root finding\n            b = brentq(root_func, -50.0, 50.0, args=(logit_q, O_tilde))\n        except ValueError:\n            # Handles edge cases where O_tilde is outside the possible range\n            # of the sum of probabilities, although clipping O_tilde should prevent this.\n            b = 0.0\n\n    # Step 6: Final recalibrated expectation\n    q_clipped_for_recal = np.clip(q, epsilon, 1 - epsilon)\n    logit_q_recal = np.log(q_clipped_for_recal / (1.0 - q_clipped_for_recal)) + b\n    q_recal = 1.0 / (1.0 + np.exp(-logit_q_recal))\n    E_recal = np.sum(q_recal)\n    \n    # Round all results to 6 decimal places\n    results = [\n        np.round(CIL, 6),\n        np.round(b, 6),\n        np.round(E, 6),\n        np.round(O, 6),\n        np.round(E_recal, 6),\n    ]\n    \n    return results\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "5027251"}, {"introduction": "For a machine learning model to be adopted in clinical practice, its predictions must be interpretable. This exercise [@problem_id:5027210] demystifies one of the most powerful explainability techniques, SHapley Additive exPlanations (SHAP), by tracing it back to its roots in cooperative game theory. You will first derive the conditions under which SHAP values simplify for linear models and then apply this understanding to calculate exact feature attributions for a sparse logistic regression model, learning how to precisely quantify each biomarker's contribution to a prediction.", "problem": "A translational medicine team is developing a predictive biomarker panel to estimate the probability of clinical response in an oncology trial. They train a sparse logistic regression, obtained by a least absolute shrinkage and selection operator (LASSO) penalty, on standardized biomarker features. Let the feature vector be $X \\in \\mathbb{R}^{p}$ with coordinates $X_{j}$ representing assay-normalized biomarker measurements, and suppose features have been standardized so that for each $j$, $\\mathbb{E}[X_{j}]=0$ and $\\operatorname{Var}(X_{j})=1$. The model output is $f(x)=\\sigma(g(x))$, where $g(x)=w_{0}+\\sum_{j=1}^{p}w_{j}x_{j}$ is the log-odds and $\\sigma(t)=\\frac{1}{1+\\exp(-t)}$ is the logistic function. Consider the Shapley value framework from cooperative game theory, with the value function defined for any subset $S \\subseteq \\{1,\\dots,p\\}$ by $v(S)=\\mathbb{E}[f(X)\\mid X_{S}=x_{S}]$, where $X_{S}$ denotes the subvector indexed by $S$ and missing features are integrated out via the model-agnostic conditional expectation.\n\nFirst, starting from the axiomatic definition of Shapley values and the linearity of $g(x)$, derive general conditions on the joint distribution of $X$ and on the imputation scheme for missing features under which the Shapley values computed on $g(x)$ equal marginal contributions $w_{j}\\left(x_{j}-\\mathbb{E}[X_{j}]\\right)$ for each feature $j$. Your derivation must begin from the fundamental Shapley value definition and must not use any shortcut formula; explicitly state the minimal assumptions under which the equality holds.\n\nSecond, consider a trained sparse model with $p=3$ nonzero biomarker coefficients. Assume features are independent, standardized, and the imputation scheme for missing features in $g(x)$ uses their unconditional expectation. The learned parameters are $w_{0}=-1.2$, $w_{1}=0.8$, $w_{2}=-0.5$, $w_{3}=0.3$. For a new patient with standardized biomarker measurements $x=(1.5,-0.7,2.0)$, compute the exact Shapley feature attributions on the log-odds scale for the three biomarkers, using the cooperative game theoretic definition and the assumptions you derived. Express the final answer as a single row vector containing the three feature-level Shapley values in log-odds units. No rounding is required, and no physical units apply.", "solution": "We begin with the definition of Shapley values in cooperative game theory. Let $N=\\{1,\\dots,p\\}$ index the features. For a value function $v:\\mathcal{P}(N)\\to\\mathbb{R}$ defined on subsets $S\\subseteq N$, the Shapley value for feature $j\\in N$ is\n$$\n\\phi_{j}=\\sum_{S\\subseteq N\\setminus\\{j\\}}\\frac{|S|!\\,(p-|S|-1)!}{p!}\\left[v(S\\cup\\{j\\})-v(S)\\right].\n$$\nThe Shapley value is characterized by axioms of efficiency, symmetry, dummy, and linearity. In model explanation, we set $v(S)=\\mathbb{E}[f(X)\\mid X_{S}=x_{S}]$, which averages out missing features.\n\nWe seek conditions under which the Shapley values computed on the log-odds function $g(x)=w_{0}+\\sum_{j=1}^{p}w_{j}x_{j}$ reduce to marginal contributions $w_{j}\\left(x_{j}-\\mathbb{E}[X_{j}]\\right)$. Consider $g$ as the payoff function and define $v_{g}(S)=\\mathbb{E}[g(X)\\mid X_{S}=x_{S}]$. Because $g$ is linear in $X$, we have\n$$\nv_{g}(S)=\\mathbb{E}\\left[w_{0}+\\sum_{k=1}^{p}w_{k}X_{k}\\,\\middle|\\,X_{S}=x_{S}\\right]=w_{0}+\\sum_{k\\in S}w_{k}x_{k}+\\sum_{k\\notin S}w_{k}\\,\\mathbb{E}[X_{k}\\mid X_{S}=x_{S}].\n$$\nTherefore,\n$$\nv_{g}(S\\cup\\{j\\})-v_{g}(S)=w_{j}x_{j}-w_{j}\\,\\mathbb{E}[X_{j}\\mid X_{S}=x_{S}]=w_{j}\\left(x_{j}-\\mathbb{E}[X_{j}\\mid X_{S}=x_{S}]\\right).\n$$\nFor this difference to be independent of $S$ and equal to $w_{j}\\left(x_{j}-\\mathbb{E}[X_{j}]\\right)$, we require the following minimal conditions:\n- The model is additive and linear in the features for the payoff under consideration, i.e., the payoff function is $g(x)=w_{0}+\\sum_{j}w_{j}x_{j}$, so the contribution of each feature is separable.\n- The imputation scheme for missing features in the value function uses $\\mathbb{E}[X_{k}\\mid X_{S}=x_{S}]$, and this conditional expectation for any missing feature $k\\notin S$ equals its unconditional expectation $\\mathbb{E}[X_{k}]$. A sufficient condition is that features are jointly independent, so $\\mathbb{E}[X_{k}\\mid X_{S}=x_{S}]=\\mathbb{E}[X_{k}]$ for all $k\\notin S$ and all $S$.\n- Expectations $\\mathbb{E}[X_{j}]$ exist and are finite.\n\nUnder these conditions, we obtain\n$$\nv_{g}(S\\cup\\{j\\})-v_{g}(S)=w_{j}\\left(x_{j}-\\mathbb{E}[X_{j}]\\right)\n$$\nfor all $S\\subseteq N\\setminus\\{j\\}$. Substituting into the Shapley formula,\n$$\n\\phi_{j}=\\left[w_{j}\\left(x_{j}-\\mathbb{E}[X_{j}]\\right)\\right]\\sum_{S\\subseteq N\\setminus\\{j\\}}\\frac{|S|!\\,(p-|S|-1)!}{p!}.\n$$\nThe combinatorial sum equals $1$ because the Shapley weights form a probability distribution over all subsets $S\\subseteq N\\setminus\\{j\\}$. Hence,\n$$\n\\phi_{j}=w_{j}\\left(x_{j}-\\mathbb{E}[X_{j}]\\right).\n$$\nIf features are standardized so that $\\mathbb{E}[X_{j}]=0$ for each $j$, then $\\phi_{j}=w_{j}x_{j}$.\n\nWe now compute the Shapley feature attributions on the log-odds scale for the given sparse logistic regression. The specified model parameters are $w_{0}=-1.2$, $w_{1}=0.8$, $w_{2}=-0.5$, $w_{3}=0.3$, and the patient’s standardized biomarkers are $x=(1.5,-0.7,2.0)$. The features are independent and standardized, and the value function for $g$ imputes missing features by unconditional expectations, so the conditions derived above are satisfied. Therefore,\n$$\n\\phi_{1}=w_{1}x_{1}=0.8\\times 1.5=1.2,\n$$\n$$\n\\phi_{2}=w_{2}x_{2}=-0.5\\times(-0.7)=0.35,\n$$\n$$\n\\phi_{3}=w_{3}x_{3}=0.3\\times 2.0=0.6.\n$$\nThese are the exact Shapley feature attributions for the three biomarkers on the log-odds scale. Note that the baseline attribution on the log-odds scale is $\\phi_{0}=w_{0}=-1.2$, and the efficiency axiom holds since $\\phi_{0}+\\phi_{1}+\\phi_{2}+\\phi_{3}=w_{0}+\\sum_{j}w_{j}x_{j}=g(x)$, but the requested final answer is the feature-level vector $\\left(\\phi_{1},\\phi_{2},\\phi_{3}\\right)$.", "answer": "$$\\boxed{\\begin{pmatrix}1.2  0.35  0.6\\end{pmatrix}}$$", "id": "5027210"}]}