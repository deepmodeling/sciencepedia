{"hands_on_practices": [{"introduction": "In any multi-omics study, the integrity of the data linkage is paramount. A common but critical error is sample mislabeling, where data from different omics for a single patient ID actually originate from different individuals. This exercise [@problem_id:5034034] provides a hands-on implementation of a robust, probabilistic method to verify sample identity by assessing genotype concordance between DNA-level (WGS) and RNA-level (RNA-seq) variant data, forming an essential first step in quality control.", "problem": "You are tasked with designing and implementing a programmatic decision rule to resolve patient identifier mismatches by assessing genotype concordance between Whole Genome Sequencing (WGS) and RNA Sequencing (RNA-seq) variant calls. You must work from foundational population genetics and probabilistic inference without assuming shortcut formulas. The generative model is as follows.\n\nFundamental base:\n- Assume Hardy–Weinberg equilibrium. For a biallelic single-nucleotide variant with minor allele frequency $p$, the true diploid genotype $t \\in \\{0,1,2\\}$ (number of minor alleles) is distributed as $P(t=0)=(1-p)^2$, $P(t=1)=2p(1-p)$, $P(t=2)=p^2$.\n- For each assay, assume a symmetric genotyping error model: given the true genotype $t$, the observed genotype $g \\in \\{0,1,2\\}$ is equal to $t$ with probability $1-e$, and otherwise is one of the other two genotypes with probability $e/2$ each, where $e$ is the assay-specific error rate. Denote the error rate for WGS as $e_w$ and for RNA-seq as $e_r$.\n- Across variants, conditional on their true genotypes, observations are independent.\n\nMatching versus mismatching hypotheses:\n- Under the matching hypothesis $\\mathcal{M}$, the WGS and RNA-seq observations for a given variant arise from the same individual sharing a single underlying true genotype $t$; conditional on $t$, the two observed genotypes are independent emissions from the same $t$ with their respective error rates.\n- Under the mismatching hypothesis $\\mathcal{U}$, the WGS and RNA-seq observations arise from independent individuals sampled from the same population; equivalently, their true genotypes are independent draws from the Hardy–Weinberg distribution for the given $p$, and each observation is an emission from its own true genotype with the corresponding assay error rate.\n\nDecision framework:\n- Let the prior probability of the matching hypothesis be $\\pi_{\\mathcal{M}} \\in (0,1)$, so the prior odds are $O_0 = \\pi_{\\mathcal{M}}/(1-\\pi_{\\mathcal{M}})$.\n- For a set of variants, define the total log-likelihood ratio $\\Lambda$ as the sum over variants of the per-variant log-likelihood ratio of observing the pair of genotypes under $\\mathcal{M}$ versus $\\mathcal{U}$, using the model above.\n- The posterior odds are $O_{\\text{post}} = O_0 \\cdot \\exp(\\Lambda)$, and the posterior probability of a match is $\\pi_{\\text{post}} = O_{\\text{post}}/(1+O_{\\text{post}})$.\n- Set a concordance threshold by requiring $\\pi_{\\text{post}} \\ge \\tau$, where $\\tau \\in (0,1)$ is a specified target level.\n\nYour program must:\n1. For each test case, compute the total log-likelihood ratio $\\Lambda$ from first principles using the generative definitions above, then compute the posterior probability $\\pi_{\\text{post}}$, and finally output a boolean decision indicating whether $\\pi_{\\text{post}} \\ge \\tau$.\n2. Use the natural logarithm (base $e$) for all log computations.\n3. Round all floating-point outputs to $6$ decimal places.\n\nTest suite:\n- Case A (high concordance “happy path”): $G_w=[0,1,2,1,0,1,2,0]$, $G_r=[0,1,2,1,0,1,1,0]$, $p=[0.05,0.10,0.20,0.15,0.30,0.10,0.25,0.05]$, $e_w=0.01$, $e_r=0.05$, $\\pi_{\\mathcal{M}}=0.5$, $\\tau=0.95$.\n- Case B (clear mismatch): $G_w=[0,0,1,2,1,0,2,1,0,2]$, $G_r=[2,1,0,0,2,1,0,1,2,0]$, $p=[0.20,0.40,0.30,0.10,0.25,0.35,0.15,0.05,0.50,0.45]$, $e_w=0.01$, $e_r=0.05$, $\\pi_{\\mathcal{M}}=0.5$, $\\tau=0.95$.\n- Case C (boundary condition, few variants, moderate errors): $G_w=[0,1,1,2]$, $G_r=[0,1,0,2]$, $p=[0.30,0.10,0.10,0.40]$, $e_w=0.02$, $e_r=0.08$, $\\pi_{\\mathcal{M}}=0.5$, $\\tau=0.90$.\n\nDefinitions for computation:\n- For each variant index $i$, let $p_i$ denote the minor allele frequency, $g_{w,i}$ the WGS genotype and $g_{r,i}$ the RNA-seq genotype.\n- Define $P_{\\mathcal{M},i}$ as the probability of observing $(g_{w,i}, g_{r,i})$ under $\\mathcal{M}$ by marginalizing over the single true genotype with the Hardy–Weinberg prior and the emission model.\n- Define $P_{\\mathcal{U},i}$ as the probability of observing $(g_{w,i}, g_{r,i})$ under $\\mathcal{U}$ by taking the product of the marginal probabilities of each observed genotype under the Hardy–Weinberg prior and the emission model for each assay independently.\n\nRequired final output format:\n- Your program should produce a single line of output containing a comma-separated list enclosed in square brackets. For each test case, append the total log-likelihood ratio $\\Lambda$ (rounded to $6$ decimal places) followed by the boolean decision for the threshold (where $True$ indicates $\\pi_{\\text{post}} \\ge \\tau$). The overall output should thus be of the form $[\\Lambda_A,\\text{decision}_A,\\Lambda_B,\\text{decision}_B,\\Lambda_C,\\text{decision}_C]$ with no additional text.", "solution": "The problem requires the design and implementation of a programmatic decision rule to ascertain patient sample concordance by comparing Whole Genome Sequencing (WGS) and RNA Sequencing (RNA-seq) data. The solution rests on a Bayesian framework that evaluates the posterior probability of two samples originating from the same individual ($\\mathcal{M}$, the matching hypothesis) versus two different individuals ($\\mathcal{U}$, the mismatching hypothesis). The implementation will be based exclusively on the provided first principles of population genetics and probabilistic modeling.\n\nThe core of the solution is the calculation of the total log-likelihood ratio, $\\Lambda$, across a set of genetic variants. This quantity measures the weight of evidence provided by the observed genotype data in favor of the matching hypothesis over the mismatching one.\n\n**1. Generative Model Specification**\n\nThe probabilistic model is defined by three components:\n\n- **Hardy-Weinberg Equilibrium (HWE) Prior:** For a given biallelic variant with minor allele frequency $p$, the true diploid genotype $t \\in \\{0, 1, 2\\}$ (representing the count of minor alleles) is drawn from a trinomial distribution with probabilities:\n  $$P(t=0) = (1-p)^2$$\n  $$P(t=1) = 2p(1-p)$$\n  $$P(t=2) = p^2$$\n\n- **Symmetric Genotyping Error Model:** The probability of observing a genotype $g \\in \\{0, 1, 2\\}$ given the true genotype $t$ is defined by an assay-specific error rate $e$. The conditional probability $P(g|t, e)$ is:\n  $$P(g|t, e) = \\begin{cases} 1-e & \\text{if } g = t \\\\ e/2 & \\text{if } g \\neq t \\end{cases}$$\n  We denote the error rates for WGS and RNA-seq as $e_w$ and $e_r$, respectively.\n\n- **Hypothesis-Conditional Likelihoods:** The observed WGS genotype $g_w$ and RNA-seq genotype $g_r$ for a single variant have different joint probabilities under the two hypotheses:\n  - **Matching Hypothesis ($\\mathcal{M}$):** The genotypes $g_w$ and $g_r$ are derived from a single, shared true genotype $t$. Conditional on $t$, the observations are independent.\n  - **Mismatching Hypothesis ($\\mathcal{U}$):** The genotypes $g_w$ and $g_r$ are derived from two independent true genotypes, $t_w$ and $t_r$, each drawn from the HWE distribution.\n\n**2. Derivation of Per-Variant Likelihoods**\n\nFor each variant $i$ with minor allele frequency $p_i$ and observed genotypes $(g_{w,i}, g_{r,i})$, we derive the likelihoods under each hypothesis. For clarity, the subscript $i$ is omitted in the derivation below.\n\n- **Likelihood under $\\mathcal{M}$:** The probability $P_{\\mathcal{M}}(g_w, g_r)$ is obtained by marginalizing over the single latent true genotype $t$:\n  $$P_{\\mathcal{M}}(g_w, g_r) = \\sum_{t=0}^{2} P(g_w, g_r, t | \\mathcal{M})$$\n  By applying the chain rule and the conditional independence of $g_w$ and $g_r$ given $t$:\n  $$P_{\\mathcal{M}}(g_w, g_r) = \\sum_{t=0}^{2} P(g_w | t, e_w) P(g_r | t, e_r) P(t)$$\n  where $P(t)$ is the HWE prior probability for the given MAF $p$.\n\n- **Likelihood under $\\mathcal{U}$:** The probability $P_{\\mathcal{U}}(g_w, g_r)$ is the product of the marginal probabilities of each observation, due to the independence assumption under this hypothesis:\n  $$P_{\\mathcal{U}}(g_w, g_r) = P(g_w | \\mathcal{U}) P(g_r | \\mathcal{U})$$\n  Each marginal probability is calculated by integrating over its own latent true genotype ($t_w$ or $t_r$), which is drawn from the HWE distribution:\n  $$P(g_w | \\mathcal{U}) = \\sum_{t_w=0}^{2} P(g_w | t_w, e_w) P(t_w)$$\n  $$P(g_r | \\mathcal{U}) = \\sum_{t_r=0}^{2} P(g_r | t_r, e_r) P(t_r)$$\n  Thus,\n  $$P_{\\mathcal{U}}(g_w, g_r) = \\left( \\sum_{t_w=0}^{2} P(g_w | t_w, e_w) P(t_w) \\right) \\left( \\sum_{t_r=0}^{2} P(g_r | t_r, e_r) P(t_r) \\right)$$\n\n**3. Bayesian Inference and Decision Framework**\n\nThe decision process aggregates evidence across all variants using Bayesian updating.\n\n- **Log-Likelihood Ratio:** For each variant $i$, we compute the log-likelihood ratio (using the natural logarithm, $\\log$):\n  $$\\lambda_i = \\log \\left( \\frac{P_{\\mathcal{M},i}(g_{w,i}, g_{r,i})}{P_{\\mathcal{U},i}(g_{w,i}, g_{r,i})} \\right)$$\n  The total log-likelihood ratio, $\\Lambda$, is the sum over all variants, assuming their independence:\n  $$\\Lambda = \\sum_i \\lambda_i$$\n\n- **Posterior Probability Calculation:** The prior belief in a match is given by $\\pi_{\\mathcal{M}}$, which corresponds to prior odds $O_0 = \\pi_{\\mathcal{M}} / (1 - \\pi_{\\mathcal{M}})$. The evidence from the data, encapsulated in the likelihood ratio $\\exp(\\Lambda)$, updates the prior odds to the posterior odds:\n  $$O_{\\text{post}} = O_0 \\cdot \\exp(\\Lambda)$$\n  The posterior odds are then converted back to a posterior probability of a match, $\\pi_{\\text{post}}$:\n  $$\\pi_{\\text{post}} = \\frac{O_{\\text{post}}}{1 + O_{\\text{post}}}$$\n\n- **Decision Rule:** A match is declared if the posterior probability meets or exceeds a specified confidence threshold $\\tau$:\n  $$\\text{Decision} = (\\pi_{\\text{post}} \\ge \\tau)$$\n\nThe program will implement these calculations for each test case, yielding the total log-likelihood ratio $\\Lambda$ and the resulting boolean decision.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the genotype concordance problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"id\": \"A\",\n            \"Gw\": [0, 1, 2, 1, 0, 1, 2, 0],\n            \"Gr\": [0, 1, 2, 1, 0, 1, 1, 0],\n            \"p\": [0.05, 0.10, 0.20, 0.15, 0.30, 0.10, 0.25, 0.05],\n            \"ew\": 0.01,\n            \"er\": 0.05,\n            \"pi_M\": 0.5,\n            \"tau\": 0.95,\n        },\n        {\n            \"id\": \"B\",\n            \"Gw\": [0, 0, 1, 2, 1, 0, 2, 1, 0, 2],\n            \"Gr\": [2, 1, 0, 0, 2, 1, 0, 1, 2, 0],\n            \"p\": [0.20, 0.40, 0.30, 0.10, 0.25, 0.35, 0.15, 0.05, 0.50, 0.45],\n            \"ew\": 0.01,\n            \"er\": 0.05,\n            \"pi_M\": 0.5,\n            \"tau\": 0.95,\n        },\n        {\n            \"id\": \"C\",\n            \"Gw\": [0, 1, 1, 2],\n            \"Gr\": [0, 1, 0, 2],\n            \"p\": [0.30, 0.10, 0.10, 0.40],\n            \"ew\": 0.02,\n            \"er\": 0.08,\n            \"pi_M\": 0.5,\n            \"tau\": 0.90,\n        },\n    ]\n\n    results = []\n    \n    def get_p_g_given_t(obs_g, true_t, error_rate):\n        \"\"\"Calculates P(g|t) based on the symmetric error model.\"\"\"\n        if obs_g == true_t:\n            return 1.0 - error_rate\n        else:\n            return error_rate / 2.0\n\n    for case in test_cases:\n        Gw = case[\"Gw\"]\n        Gr = case[\"Gr\"]\n        p_list = case[\"p\"]\n        ew = case[\"ew\"]\n        er = case[\"er\"]\n        pi_M = case[\"pi_M\"]\n        tau = case[\"tau\"]\n\n        total_log_likelihood_ratio = 0.0\n\n        for gw, gr, p in zip(Gw, Gr, p_list):\n            # Hardy-Weinberg prior probabilities for t in {0, 1, 2}\n            p_t_vec = np.array([(1-p)**2, 2*p*(1-p), p**2])\n            \n            p_M = 0.0\n            p_gw_marginal = 0.0\n            p_gr_marginal = 0.0\n\n            for t_idx in range(3):\n                # P(g_w | t) and P(g_r | t)\n                p_gw_given_t = get_p_g_given_t(gw, t_idx, ew)\n                p_gr_given_t = get_p_g_given_t(gr, t_idx, er)\n                \n                # Contribution to P_M (match hypothesis)\n                p_M += p_gw_given_t * p_gr_given_t * p_t_vec[t_idx]\n                \n                # Contribution to marginals for P_U (mismatch hypothesis)\n                p_gw_marginal += p_gw_given_t * p_t_vec[t_idx]\n                p_gr_marginal += p_gr_given_t * p_t_vec[t_idx]\n\n            p_U = p_gw_marginal * p_gr_marginal\n\n            variant_llr = np.log(p_M / p_U)\n            total_log_likelihood_ratio += variant_llr\n\n        # Calculate posterior probability\n        prior_odds = pi_M / (1.0 - pi_M)\n        posterior_odds = prior_odds * np.exp(total_log_likelihood_ratio)\n        posterior_prob = posterior_odds / (1.0 + posterior_odds)\n\n        # Make decision\n        decision = posterior_prob >= tau\n\n        # Append results\n        results.append(f\"{total_log_likelihood_ratio:.6f}\")\n        results.append(str(decision))\n\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "5034034"}, {"introduction": "After confirming sample identities, the next crucial step is to assess the quality of the data within each omics layer. Technical artifacts can lead to outlier samples whose profiles deviate significantly from the rest, potentially skewing downstream analyses. This practice [@problem_id:5033985] guides you through building an automated quality control pipeline that uses robust statistical measures like PCA-based leverage and Median Absolute Deviation to flag outlier samples across multiple omics datasets.", "problem": "You are tasked with constructing a quality control pipeline for multi-omics integration in translational medicine that detects outlier samples across omics using Principal Component Analysis (PCA) leverage scores and Median Absolute Deviation (MAD) thresholds applied to key quality metrics. The pipeline must be implemented as a complete, runnable program that takes no input and produces a single line of output. The program must compute outlier indices for a provided test suite.\n\nFoundational base and definitions to use:\n- Let an omics data matrix be denoted by $X \\in \\mathbb{R}^{n \\times p}$ with $n$ samples (rows) and $p$ features (columns). Each column of $X$ must be standardized to zero mean and unit variance before subsequent computations.\n- Principal Component Analysis (PCA) can be defined via the Singular Value Decomposition (SVD), $X = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{n \\times r}$ and $V \\in \\mathbb{R}^{p \\times r}$ have orthonormal columns, $\\Sigma \\in \\mathbb{R}^{r \\times r}$ is diagonal with nonnegative singular values, and $r = \\operatorname{rank}(X)$.\n- For a chosen number of components $k$, with $k \\leq p$, define the projection onto the top-$k$ left singular subspace by $U_k \\in \\mathbb{R}^{n \\times k}$, the first $k$ columns of $U$ if $r \\geq k$, and by $U_k \\in \\mathbb{R}^{n \\times r}$ otherwise. The PCA leverage for sample $i$ is the diagonal element of the projection matrix onto this subspace, $h_i = \\sum_{j=1}^{\\min(k, r)} U_{ij}^2$.\n- The projection matrix $H_k = U_k U_k^\\top$ is symmetric and idempotent, and satisfies $\\operatorname{trace}(H_k) = \\min(k, r)$. Therefore, the mean leverage equals $\\operatorname{trace}(H_k)/n = \\min(k, r)/n$. A leverage-based outlier can be defined by comparing $h_i$ to a threshold $\\alpha \\cdot \\min(k, r)/n$, with amplification factor $\\alpha > 1$.\n- For a quality metric vector $q \\in \\mathbb{R}^n$, define $\\operatorname{median}(q)$, the median absolute deviations $d_i = |q_i - \\operatorname{median}(q)|$, $\\operatorname{MAD}(q) = \\operatorname{median}(d)$, and the robust scaled deviation $z_i = d_i / (c \\cdot \\operatorname{MAD}(q))$ with $c = 1.4826$ (the consistency constant under normality). A metric-based outlier is any $i$ for which $z_i$ exceeds a threshold $t$.\n- Integrated across omics: given $O$ omics matrices $\\{X^{(o)}\\}_{o=1}^O$, define for each sample $i$ the count of leverage exceedances across omics, $C^{\\text{lev}}_i$, and the count of metric exceedances across the provided metrics, $C^{\\text{met}}_i$. A sample $i$ is flagged as an integrated outlier if either $C^{\\text{lev}}_i \\geq \\theta$ or $C^{\\text{met}}_i \\geq \\phi$, for given integer thresholds $\\theta$ and $\\phi$.\n\nImplementation requirements:\n- Standardize each omics matrix column-wise to zero mean and unit variance; if a column has zero variance, treat its standardized values as all zeros.\n- Compute PCA leverage using the SVD-based definition above with $U_k$ and $h_i$.\n- Compute quality metric outliers using the MAD-based robust deviations. If $\\operatorname{MAD}(q) = 0$, define no outliers for that metric unless any $q_i \\neq \\operatorname{median}(q)$; under that latter condition, treat those with $q_i \\neq \\operatorname{median}(q)$ as outliers.\n- Use $0$-based indexing for reporting outlier sample indices.\n\nTest suite:\n- Test Case $1$ (general multi-omics with a clear outlier in one omic and metrics):\n    - Number of samples: $n = 6$; number of omics: $O = 3$; components $k = 2$; leverage amplification $\\alpha = 2.0$; leverage count threshold $\\theta = 1$; metric threshold $t = 3.0$; metric count threshold $\\phi = 1$.\n    - Genomics matrix $X^{(1)}$ ($6 \\times 4$):\n      $$\n      \\begin{bmatrix}\n      0.5 & 1.0 & -0.3 & 0.2 \\\\\n      0.6 & 0.8 & -0.1 & 0.0 \\\\\n      0.4 & 1.1 & -0.2 & 0.1 \\\\\n      0.5 & 0.9 & -0.3 & 0.2 \\\\\n      0.6 & 1.0 & -0.2 & 0.3 \\\\\n      0.5 & 0.95 & -0.25 & 0.15\n      \\end{bmatrix}\n      $$\n    - Transcriptomics matrix $X^{(2)}$ ($6 \\times 5$):\n      $$\n      \\begin{bmatrix}\n      10 & 12 & 9 & 11 & 10 \\\\\n      11 & 12 & 9 & 10 & 11 \\\\\n      10 & 11 & 10 & 11 & 10 \\\\\n      10 & 12 & 9 & 11 & 10 \\\\\n      11 & 11 & 9 & 10 & 12 \\\\\n      10 & 12 & 10 & 11 & 11\n      \\end{bmatrix}\n      $$\n    - Proteomics matrix $X^{(3)}$ ($6 \\times 3$):\n      $$\n      \\begin{bmatrix}\n      0 & 0 & 1 \\\\\n      0 & 0 & 1 \\\\\n      0 & 0 & 1 \\\\\n      10 & 10 & 1 \\\\\n      0 & 0 & 1 \\\\\n      0 & 0 & 1\n      \\end{bmatrix}\n      $$\n    - Quality metrics:\n      - Metric $1$ (e.g., RNA quality): $[8.0,\\, 8.2,\\, 7.9,\\, 3.0,\\, 8.1,\\, 8.0]$\n      - Metric $2$ (e.g., library size): $[5{,}000{,}000,\\, 5{,}100{,}000,\\, 4{,}900{,}000,\\, 1{,}000{,}000,\\, 5{,}200{,}000,\\, 5{,}000{,}000]$\n    - Expected behavior: a single integrated outlier corresponding to the outlying proteomics sample and metrics.\n- Test Case $2$ (boundary condition: zero-variance features and no metric deviations):\n    - $n = 4$, $O = 2$, $k = 2$, $\\alpha = 2.0$, $\\theta = 1$, $t = 3.0$, $\\phi = 1$.\n    - Omics matrices $X^{(1)} = X^{(2)} = \\begin{bmatrix}1 & 2 & 3 \\\\ 1 & 2 & 3 \\\\ 1 & 2 & 3 \\\\ 1 & 2 & 3 \\end{bmatrix}$\n    - Quality metrics:\n      - Metric $1$: $[10,\\, 10,\\, 10,\\, 10]$\n      - Metric $2$: $[100,\\, 100,\\, 100,\\, 100]$\n    - Expected behavior: no integrated outliers.\n- Test Case $3$ (edge case: outliers only via metrics):\n    - $n = 5$, $O = 2$, $k = 2$, $\\alpha = 2.0$, $\\theta = 2$, $t = 3.0$, $\\phi = 2$.\n    - Omics matrices:\n      - $X^{(1)} = \\begin{bmatrix} 1 & 2 \\\\ 1 & 2 \\\\ 1.1 & 1.9 \\\\ 0.9 & 2.1 \\\\ 1 & 2 \\end{bmatrix}$\n      - $X^{(2)} = \\begin{bmatrix} 5 & 5 \\\\ 5 & 5 \\\\ 5.1 & 4.9 \\\\ 4.9 & 5.1 \\\\ 5 & 5 \\end{bmatrix}$\n    - Quality metrics:\n      - Metric $1$: $[0.1,\\, 5.0,\\, 0.2,\\, 0.1,\\, 0.0]$\n      - Metric $2$: $[100,\\, 1000,\\, 105,\\, 98,\\, 97]$\n    - Expected behavior: a single integrated outlier via metrics (second sample).\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each result must be a list of $0$-based indices of samples flagged as integrated outliers for the corresponding test case. For example, an output of the form $[[i_1, i_2], [], [j]]$ is valid. The final output must be printed exactly as a single Python list literal of lists on one line, with no additional text.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in established statistical methods (Principal Component Analysis leverage, Median Absolute Deviation), well-posed with a deterministic algorithm, objective in its language, and complete in its specification of data and parameters. The problem is a formalizable and relevant task in the quality control of multi-omics data within translational medicine. There are no contradictions, no violations of scientific principles, and no ambiguities that would prevent a unique, verifiable solution.\n\nHere follows a complete, reasoned solution.\n\n### **Algorithmic Procedure**\n\nThe quality control pipeline identifies outlier samples by integrating two distinct types of statistical measures across multiple omics datasets and quality metric vectors: PCA-based leverage scores and MAD-based robust deviations. The procedure for a given test case is as follows.\n\n**1. Initialization**\nLet $n$ be the number of samples. Two integer vectors are initialized to store outlier counts for each sample:\n- $C^{\\text{lev}} \\in \\mathbb{Z}^n$, initialized to zeros, to count leverage-based outlier flags.\n- $C^{\\text{met}} \\in \\mathbb{Z}^n$, initialized to zeros, to count metric-based outlier flags.\n\n**2. Leverage-Based Outlier Detection (per omics dataset)**\nFor each of the $O$ omics data matrices $X^{(o)} \\in \\mathbb{R}^{n \\times p}$, where $o \\in \\{1, \\dots, O\\}$:\n\n**a. Standardization:**\nEach column $j$ of $X^{(o)}$ is standardized to have a mean of $0$ and a standard deviation of $1$. Let $X^{(o)}_{:,j}$ be the $j$-th column. Its mean $\\mu_j$ and standard deviation $\\sigma_j$ are computed. The standardized column $X'^{(o)}_{:,j}$ is given by:\n$$\nX'^{(o)}_{i,j} = \\begin{cases} (X^{(o)}_{i,j} - \\mu_j) / \\sigma_j & \\text{if } \\sigma_j > 0 \\\\ 0 & \\text{if } \\sigma_j = 0 \\end{cases}\n$$\nThis creates the standardized matrix $X'^{(o)}$.\n\n**b. Singular Value Decomposition (SVD):**\nThe SVD of the standardized matrix is computed: $X'^{(o)} = U^{(o)} \\Sigma^{(o)} (V^{(o)})^\\top$. Here, $U^{(o)} \\in \\mathbb{R}^{n \\times n}$ is the matrix of left singular vectors.\n\n**c. Rank and Component Selection:**\nThe rank of the matrix, $r^{(o)} = \\operatorname{rank}(X'^{(o)})$, is determined. This corresponds to the number of non-zero singular values. The number of principal components to consider is $k' = \\min(k, r^{(o)})$, where $k$ is the user-specified number of components. We define $U_{k'}^{(o)}$ as the matrix containing the first $k'$ columns of $U^{(o)}$.\n\n**d. Leverage Calculation:**\nThe leverage score $h_i^{(o)}$ for each sample $i \\in \\{1, \\dots, n\\}$ is the sum of the squares of the elements in the $i$-th row of $U_{k'}^{(o)}$:\n$$\nh_i^{(o)} = \\sum_{j=1}^{k'} (U^{(o)}_{ij})^2\n$$\n\n**e. Outlier Flagging:**\nA leverage threshold $\\tau_{\\text{lev}}^{(o)}$ is calculated based on the mean leverage:\n$$\n\\tau_{\\text{lev}}^{(o)} = \\frac{\\alpha \\cdot k'}{n}\n$$\nwhere $\\alpha$ is the given amplification factor. If a sample's leverage $h_i^{(o)}$ exceeds this threshold, its leverage outlier count $C^{\\text{lev}}_i$ is incremented:\nif $h_i^{(o)} > \\tau_{\\text{lev}}^{(o)}$, then $C^{\\text{lev}}_i \\leftarrow C^{\\text{lev}}_i + 1$.\n\n**3. Metric-Based Outlier Detection (per quality metric)**\nFor each provided quality metric vector $q \\in \\mathbb{R}^n$:\n\n**a. Median and Median Absolute Deviation (MAD):**\nThe median of the metric, $m = \\operatorname{median}(q)$, is calculated. The absolute deviations from the median, $d_i = |q_i - m|$, are computed for all samples. The MAD is the median of these absolute deviations: $\\operatorname{MAD}(q) = \\operatorname{median}(d)$.\n\n**b. Outlier Flagging:**\nThe method for flagging outliers depends on the value of $\\operatorname{MAD}(q)$:\n- If $\\operatorname{MAD}(q) > 0$: The robust scaled deviation $z_i$ is calculated for each sample:\n  $$\n  z_i = \\frac{d_i}{c \\cdot \\operatorname{MAD}(q)} = \\frac{|q_i - \\operatorname{median}(q)|}{1.4826 \\cdot \\operatorname{MAD}(q)}\n  $$\n  If $z_i$ exceeds the metric threshold $t$, the sample's metric outlier count $C^{\\text{met}}_i$ is incremented.\n- If $\\operatorname{MAD}(q) = 0$: Any sample $i$ for which the metric value $q_i$ is not equal to the median $m$ is considered an outlier. For each such sample, $C^{\\text{met}}_i$ is incremented. If all $q_i$ are equal to the median, no outliers are flagged for this metric.\n\n**4. Integrated Outlier Identification**\nAfter processing all omics matrices and quality metrics, the final outlier status of each sample $i$ is determined by comparing its accumulated counts $C^{\\text{lev}}_i$ and $C^{\\text{met}}_i$ with the respective integer thresholds $\\theta$ and $\\phi$:\nSample $i$ is an integrated outlier if $(C^{\\text{lev}}_i \\geq \\theta) \\lor (C^{\\text{met}}_i \\geq \\phi)$.\nThe 0-based indices of all samples satisfying this condition are collected as the final result for the test case.\n\n---\n### **Application to Test Cases**\n\n**Test Case 1**\n- Parameters: $n=6, O=3, k=2, \\alpha=2.0, \\theta=1, t=3.0, \\phi=1$.\n- **Leverage:**\n  - For $X^{(1)}$ and $X^{(2)}$, the data points are relatively homogeneous. After standardization, the SVD yields leverage scores where no single sample dominates. The ranks are $r^{(1)}=4$ and $r^{(2)}=5$. Then $k' = \\min(2,4)=2$ and $k'=\\min(2,5)=2$. The thresholds are $\\tau_{\\text{lev}}^{(1)} = 2.0 \\cdot 2 / 6 \\approx 0.667$ and $\\tau_{\\text{lev}}^{(2)} = 2.0 \\cdot 2 / 6 \\approx 0.667$. All calculated $h_i^{(1)}$ and $h_i^{(2)}$ values are found to be below this threshold.\n  - For $X^{(3)}$, the 4th sample (index $3$) is anomalous: $[10, 10, 1]$. After standardization, the matrix rank is $r^{(3)}=1$. Thus, $k'=\\min(2,1)=1$. The leverage threshold is $\\tau_{\\text{lev}}^{(3)} = 2.0 \\cdot 1 / 6 \\approx 0.333$. The leverage scores are approximately $h^{(3)} = [0.038, 0.038, 0.038, 0.769, 0.038, 0.038]$. Only $h_3^{(3)} \\approx 0.769 > 0.333$.\n  - The leverage counts are $C^{\\text{lev}} = [0, 0, 0, 1, 0, 0]$.\n- **Metrics:**\n  - Metric 1: $q = [8.0, 8.2, 7.9, 3.0, 8.1, 8.0]$. The median is $8.0$. Deviations are $[0.0, 0.2, 0.1, 5.0, 0.1, 0.0]$. The MAD is $0.1$. The z-score for sample 4 (index 3) is $z_3 = 5.0 / (1.4826 \\cdot 0.1) \\approx 33.72 > 3.0$.\n  - Metric 2: $q = [5\\text{e}6, 5.1\\text{e}6, 4.9\\text{e}6, 1\\text{e}6, 5.2\\text{e}6, 5\\text{e}6]$. The median is $5\\text{e}6$. MAD is $1\\text{e}5$. The z-score for sample 4 is $z_3 = 4\\text{e}6 / (1.4826 \\cdot 1\\text{e}5) \\approx 26.98 > 3.0$.\n  - The metric counts are $C^{\\text{met}} = [0, 0, 0, 2, 0, 0]$.\n- **Integration:**\n  - For sample 4 (index 3): $C^{\\text{lev}}_3=1 \\geq \\theta=1$ and $C^{\\text{met}}_3=2 \\geq \\phi=1$. The sample is an outlier.\n  - For all other samples $i \\neq 3$, $C^{\\text{lev}}_i=0  1$ and $C^{\\text{met}}_i=0  1$. They are not outliers.\n- **Result:** `[3]`\n\n**Test Case 2**\n- Parameters: $n=4, O=2, k=2, \\alpha=2.0, \\theta=1, t=3.0, \\phi=1$.\n- **Leverage:**\n  - Both $X^{(1)}$ and $X^{(2)}$ consist of identical rows. Each column has a standard deviation of $0$.\n  - Per the rule, both standardized matrices $X'^{(1)}$ and $X'^{(2)}$ are zero matrices.\n  - The rank of a zero matrix is $r=0$. Thus $k'=\\min(2, 0)=0$.\n  - The leverage scores $h_i$ are all $0$, and the threshold $\\tau_{\\text{lev}}$ is also $0$. The condition $h_i > \\tau_{\\text{lev}}$ is never met.\n  - The leverage counts are $C^{\\text{lev}} = [0, 0, 0, 0]$.\n- **Metrics:**\n  - Both metric vectors, $[10, 10, 10, 10]$ and $[100, 100, 100, 100]$, consist of identical values.\n  - For both, the median is the constant value, all deviations are $0$, and thus the MAD is $0$.\n  - The special case for $\\operatorname{MAD}=0$ applies. Since no $q_i$ differs from the median, no outliers are flagged for this metric.\n  - The metric counts are $C^{\\text{met}} = [0, 0, 0, 0]$.\n- **Integration:**\n  - For all samples, $C^{\\text{lev}}_i=0$ and $C^{\\text{met}}_i=0$. The conditions $C^{\\text{lev}}_i \\geq 1$ or $C^{\\text{met}}_i \\geq 1$ are never met.\n- **Result:** `[]`\n\n**Test Case 3**\n- Parameters: $n=5, O=2, k=2, \\alpha=2.0, \\theta=2, t=3.0, \\phi=2$.\n- **Leverage:**\n  - The matrices $X^{(1)}$ and $X^{(2)}$ consist of very similar samples.\n  - For $X^{(1)}$, $r^{(1)}=2, k'=2$. $\\tau_{\\text{lev}}^{(1)} = 2.0 \\cdot 2 / 5 = 0.8$. All $h_i^{(1)}$ are below this threshold.\n  - For $X^{(2)}$, $r^{(2)}=2, k'=2$. $\\tau_{\\text{lev}}^{(2)} = 2.0 \\cdot 2 / 5 = 0.8$. All $h_i^{(2)}$ are below this threshold.\n  - No sample is flagged as a leverage outlier in any omic. The leverage counts are $C^{\\text{lev}} = [0, 0, 0, 0, 0]$.\n- **Metrics:**\n  - Metric 1: $q = [0.1, 5.0, 0.2, 0.1, 0.0]$. Median is $0.1$. MAD is $0.1$. The z-score for sample 2 (index 1) is $z_1 = 4.9 / (1.4826 \\cdot 0.1) \\approx 33.05 > 3.0$. Sample 2 is an outlier.\n  - Metric 2: $q = [100, 1000, 105, 98, 97]$. Median is $100$. MAD is $3$. The z-score for sample 2 is $z_1 = 900 / (1.4826 \\cdot 3) \\approx 202.41 > 3.0$. Sample 2 is an outlier.\n  - The metric counts are $C^{\\text{met}} = [0, 2, 0, 0, 0]$.\n- **Integration:**\n  - The thresholds are high: $\\theta=2, \\phi=2$.\n  - For sample 2 (index 1): $C^{\\text{lev}}_1=0  \\theta=2$, but $C^{\\text{met}}_1=2 \\geq \\phi=2$. The sample is an outlier.\n  - For all other samples, counts are too low.\n- **Result:** `[1]`", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef solve():\n    \"\"\"\n    Main function to run the multi-omics outlier detection pipeline on a predefined test suite.\n    \"\"\"\n    \n    test_cases = [\n        # Test Case 1\n        {\n            \"params\": {\"n\": 6, \"O\": 3, \"k\": 2, \"alpha\": 2.0, \"theta\": 1, \"t\": 3.0, \"phi\": 1},\n            \"omics_data\": [\n                np.array([\n                    [0.5, 1.0, -0.3, 0.2],\n                    [0.6, 0.8, -0.1, 0.0],\n                    [0.4, 1.1, -0.2, 0.1],\n                    [0.5, 0.9, -0.3, 0.2],\n                    [0.6, 1.0, -0.2, 0.3],\n                    [0.5, 0.95, -0.25, 0.15]\n                ]),\n                np.array([\n                    [10, 12, 9, 11, 10],\n                    [11, 12, 9, 10, 11],\n                    [10, 11, 10, 11, 10],\n                    [10, 12, 9, 11, 10],\n                    [11, 11, 9, 10, 12],\n                    [10, 12, 10, 11, 11]\n                ]),\n                np.array([\n                    [0, 0, 1],\n                    [0, 0, 1],\n                    [0, 0, 1],\n                    [10, 10, 1],\n                    [0, 0, 1],\n                    [0, 0, 1]\n                ])\n            ],\n            \"quality_metrics\": [\n                np.array([8.0, 8.2, 7.9, 3.0, 8.1, 8.0]),\n                np.array([5_000_000, 5_100_000, 4_900_000, 1_000_000, 5_200_000, 5_000_000])\n            ]\n        },\n        # Test Case 2\n        {\n            \"params\": {\"n\": 4, \"O\": 2, \"k\": 2, \"alpha\": 2.0, \"theta\": 1, \"t\": 3.0, \"phi\": 1},\n            \"omics_data\": [\n                np.array([[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]]),\n                np.array([[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]])\n            ],\n            \"quality_metrics\": [\n                np.array([10, 10, 10, 10]),\n                np.array([100, 100, 100, 100])\n            ]\n        },\n        # Test Case 3\n        {\n            \"params\": {\"n\": 5, \"O\": 2, \"k\": 2, \"alpha\": 2.0, \"theta\": 2, \"t\": 3.0, \"phi\": 2},\n            \"omics_data\": [\n                np.array([[1, 2], [1, 2], [1.1, 1.9], [0.9, 2.1], [1, 2]]),\n                np.array([[5, 5], [5, 5], [5.1, 4.9], [4.9, 5.1], [5, 5]])\n            ],\n            \"quality_metrics\": [\n                np.array([0.1, 5.0, 0.2, 0.1, 0.0]),\n                np.array([100, 1000, 105, 98, 97])\n            ]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        params = case[\"params\"]\n        n = params[\"n\"]\n        k = params[\"k\"]\n        alpha = params[\"alpha\"]\n        theta = params[\"theta\"]\n        t = params[\"t\"]\n        phi = params[\"phi\"]\n        \n        c_lev = np.zeros(n, dtype=int)\n        c_met = np.zeros(n, dtype=int)\n\n        # Leverage-based outlier detection\n        for X in case[\"omics_data\"]:\n            # Standardize matrix\n            mean = np.mean(X, axis=0)\n            std = np.std(X, axis=0)\n            X_std = np.zeros_like(X, dtype=float)\n            for j in range(X.shape[1]):\n                if std[j]  1e-9: # Use tolerance for float comparison\n                    X_std[:, j] = (X[:, j] - mean[j]) / std[j]\n            \n            # SVD and rank\n            try:\n                U, s, Vh = svd(X_std, full_matrices=False)\n                # Rank is number of singular values greater than a tolerance\n                rank = np.linalg.matrix_rank(X_std)\n            except np.linalg.LinAlgError:\n                # SVD can fail on some ill-conditioned matrices, though unlikely here\n                continue\n\n            num_components = min(k, rank)\n            if num_components == 0:\n                continue\n\n            # Leverage calculation\n            U_k = U[:, :num_components]\n            h = np.sum(U_k**2, axis=1)\n            \n            # Leverage outlier flagging\n            leverage_threshold = alpha * num_components / n\n            c_lev[h  leverage_threshold] += 1\n\n        # Metric-based outlier detection\n        c_consistency = 1.4826\n        for q in case[\"quality_metrics\"]:\n            median_q = np.median(q)\n            deviations = np.abs(q - median_q)\n            mad_q = np.median(deviations)\n\n            if mad_q  1e-9: # Use tolerance for float comparison\n                z_scores = deviations / (c_consistency * mad_q)\n                c_met[z_scores  t] += 1\n            else:\n                # Special case for MAD == 0\n                c_met[q != median_q] += 1\n        \n        # Integrated outlier identification\n        integrated_outliers = np.where((c_lev = theta) | (c_met = phi))[0].tolist()\n        results.append(integrated_outliers)\n\n    # Final print statement must produce the exact single-line format.\n    # The format `f\"[{','.join(map(str, results))}]\"` is explicitly requested,\n    # resulting in a string like '[[3],[],[1]]'.\n    print(f\"{str(results).replace(' ', '')}\")\n\nsolve()\n```", "id": "5033985"}, {"introduction": "With clean and validated data, we can proceed to the core task of integration. Similarity Network Fusion (SNF) is a powerful technique that moves beyond simple feature concatenation by constructing a patient-patient similarity network for each data type and then iteratively fusing them into a single, comprehensive network. This practice [@problem_id:5034014] provides a step-by-step implementation of the SNF algorithm, allowing you to discover patient subtypes and relationships that are reinforced across multiple biological layers.", "problem": "Implement a complete, self-contained program that constructs a multi-omics Similarity Network Fusion (SNF) pipeline to integrate patient-patient similarity networks across multiple omics and returns, for each provided test case, a single fused affinity matrix. The algorithmic steps must be derived from fundamental and well-tested principles: distance-based similarity, Gaussian kernels, nearest-neighbor graphs, stochastic normalization, and iterative message passing on graphs with a convergence criterion. Your implementation must strictly follow the procedure and definitions below and produce outputs exactly in the specified format.\n\nYou are given, per test case, a set of omics-specific patient feature matrices and scalar hyperparameters. Let there be $M$ omics (data views), each represented by a real-valued matrix $X^{(m)} \\in \\mathbb{R}^{N \\times p_m}$ for $m \\in \\{1,\\dots,M\\}$, where $N$ is the number of patients and $p_m$ is the number of features in omic $m$. The goal is to produce a single fused affinity matrix $F \\in \\mathbb{R}^{N \\times N}$ that integrates similarities across all omics, using iterative message passing until convergence or a maximum number of iterations is reached.\n\nBase definitions and required steps, which must be implemented exactly as stated:\n\n1. Z-scoring per omic and per feature:\n   - For each omic $m$ and each feature (column) $j \\in \\{1,\\dots,p_m\\}$, compute the mean $\\mu_{m,j}$ and standard deviation $\\sigma_{m,j}$ across patients (rows). Transform $X^{(m)}_{:,j}$ to $\\tilde{X}^{(m)}_{:,j}$ via \n   $$ \\tilde{X}^{(m)}_{i,j} = \\frac{X^{(m)}_{i,j} - \\mu_{m,j}}{\\sigma_{m,j}} $$\n   If $\\sigma_{m,j} = 0$, set $\\tilde{X}^{(m)}_{:,j}$ to the zero vector.\n   - This is justified by the widely used standardization in statistics to remove scale differences, ensuring comparability across features and omics.\n\n2. Pairwise squared Euclidean distance within each omic:\n   - For each omic $m$, compute $D^{(m)} \\in \\mathbb{R}^{N \\times N}$ with entries $D^{(m)}_{ij} = \\sum_{l=1}^{p_m} \\left(\\tilde{X}^{(m)}_{i,l} - \\tilde{X}^{(m)}_{j,l}\\right)^2$.\n\n3. Gaussian kernel similarity:\n   - Given a positive bandwidth parameter $\\sigma > 0$ (provided per test case), define $G^{(m)} \\in \\mathbb{R}^{N \\times N}$ by\n     $$G^{(m)}_{ij} = \\exp\\left( - \\frac{D^{(m)}_{ij}}{2 \\sigma^2} \\right).$$\n   - Set $G^{(m)}_{ii} = 1$ for all $i$.\n\n4. $k$-nearest neighbor sparsification and symmetrization:\n   - Let $k$ be a positive integer with $1 \\le k \\le N-1$ (provided per test case). For each row $i$ in $G^{(m)}$, identify the indices of the $k$ largest off-diagonal entries $G^{(m)}_{ij}$ with $j \\neq i$; in case of ties, break ties by smaller index $j$ first. Create a sparse matrix $S^{(m)}_{\\text{mask}}$ that keeps only these $k$ neighbors per row (off-diagonals) and zeros out the rest of the off-diagonal entries. Then symmetrize:\n     $$W^{(m)} = \\frac{S^{(m)}_{\\text{mask}} + \\left(S^{(m)}_{\\text{mask}}\\right)^{\\top}}{2},$$\n     and finally set the diagonal by $W^{(m)}_{ii} = 1$ for all $i$.\n\n5. Row-stochastic normalization:\n   - Convert $W^{(m)}$ to a row-stochastic matrix $P^{(m,0)}$ by normalizing rows to sum to $1$:\n     $$P^{(m,0)}_{ij} = \\frac{W^{(m)}_{ij}}{\\sum_{j'=1}^{N} W^{(m)}_{ij'}}.$$\n     If a row sum is $0$, replace that row by the uniform distribution over all columns, i.e., $P^{(m,0)}_{ij} = \\frac{1}{N}$ for all $j$.\n\n6. Fixed neighbor-constrained propagation matrices:\n   - From $P^{(m,0)}$, construct fixed propagation matrices $S^{(m)}$ by zeroing all but the same $k$ off-diagonal nearest neighbors per row (based on the magnitudes in $P^{(m,0)}$, ties broken by smaller index) and then normalizing each row to sum to $1$:\n     $$S^{(m)}_{ij} = \\begin{cases}\n     \\frac{P^{(m,0)}_{ij}}{\\sum_{j' \\in \\mathcal{N}^{(m)}_k(i)} P^{(m,0)}_{ij'}}  \\text{if } j \\in \\mathcal{N}^{(m)}_k(i),\\ j \\neq i,\\\\\n     0  \\text{otherwise,}\n     \\end{cases}$$\n     where $\\mathcal{N}^{(m)}_k(i)$ is the index set of the $k$ selected off-diagonal neighbors of $i$ in $P^{(m,0)}$. If the denominator is $0$, set $S^{(m)}_{ij} = \\frac{1}{N}$ for all $j$.\n\n7. Iterative message passing (fusion):\n   - For $t = 0,1,2,\\dots$, update each view as\n     $$P^{(m,t+1)} = S^{(m)} \\left( \\frac{1}{M-1} \\sum_{\\substack{v=1 \\\\ v \\ne m}}^{M} P^{(v,t)} \\right) \\left(S^{(m)}\\right)^{\\top},$$\n     followed by row-stochastic normalization of $P^{(m,t+1)}$ as in step $5$.\n   - For $M = 1$, skip this iterative update and define the fused matrix as the single view after step $5$.\n\n8. Convergence criterion and termination:\n   - Define the change at iteration $t$ as\n     $$\\Delta^{(t)} = \\max_{m \\in \\{1,\\dots,M\\}} \\left\\| P^{(m,t+1)} - P^{(m,t)} \\right\\|_F,$$\n     where $\\|\\cdot\\|_F$ is the Frobenius norm. Given a tolerance $\\varepsilon > 0$ and a maximum number of iterations $T_{\\max} \\in \\mathbb{N}$ (both provided), stop the iteration at the first $t$ such that $\\Delta^{(t)} \\le \\varepsilon$ or when $t+1 = T_{\\max}$, whichever occurs first.\n\n9. Fused matrix:\n   - After termination, define\n     $$F = \\frac{1}{M} \\sum_{m=1}^{M} P^{(m,t_{\\text{final}})}.$$\n     Row-normalize $F$ to be row-stochastic as in step $5$.\n\n10. Required output per test case:\n    - Flatten $F$ in row-major order into a list of $N \\times N$ real numbers. Round each entry to exactly $6$ decimal places (fixed-point, no scientific notation).\n\nProgram input is fixed within your code and must consist of the following test suite. For each test case, you must implement the above steps using the given omics matrices and hyperparameters ($k, \\sigma, \\varepsilon, T_{\\max}$):\n\n- Test case $1$ (happy path):\n  - $N = 4$, $M = 2$, $k = 2$, $\\sigma = 1.0$, $\\varepsilon = 1\\times 10^{-6}$, $T_{\\max} = 50$.\n  - Omic $1$: $X^{(1)} \\in \\mathbb{R}^{4 \\times 2}$ with rows\n    $[0.0, 1.0]$, $[0.2, 0.9]$, $[3.0, 3.5]$, $[3.2, 3.6]$.\n  - Omic $2$: $X^{(2)} \\in \\mathbb{R}^{4 \\times 2}$ with rows\n    $[0.1, 1.1]$, $[0.0, 1.0]$, $[3.1, 3.4]$, $[3.3, 3.7]$.\n\n- Test case $2$ (single-omic edge case):\n  - $N = 4$, $M = 1$, $k = 1$, $\\sigma = 0.7$, $\\varepsilon = 1\\times 10^{-6}$, $T_{\\max} = 10$.\n  - Omic $1$: $X^{(1)} \\in \\mathbb{R}^{4 \\times 3}$ with rows\n    $[1.0, 0.0, 0.5]$, $[0.9, 0.1, 0.4]$, $[3.0, 3.5, 3.7]$, $[3.1, 3.6, 3.8]$.\n\n- Test case $3$ (early-stop via large tolerance):\n  - $N = 5$, $M = 2$, $k = 2$, $\\sigma = 1.5$, $\\varepsilon = 10^{9}$, $T_{\\max} = 20$.\n  - Omic $1$: $X^{(1)} \\in \\mathbb{R}^{5 \\times 2}$ with rows\n    $[0.0, 0.0]$, $[0.1, -0.1]$, $[5.0, 5.0]$, $[5.1, 4.9]$, $[2.5, 2.5]$.\n  - Omic $2$: $X^{(2)} \\in \\mathbb{R}^{5 \\times 2}$ with rows\n    $[0.2, -0.2]$, $[0.0, 0.0]$, $[4.9, 5.1]$, $[5.2, 5.0]$, $[2.4, 2.6]$.\n\n- Test case $4$ (boundary on iterations):\n  - $N = 3$, $M = 3$, $k = 2$, $\\sigma = 1.0$, $\\varepsilon = 1\\times 10^{-12}$, $T_{\\max} = 1$.\n  - Omic $1$: $X^{(1)} \\in \\mathbb{R}^{3 \\times 2}$ with rows\n    $[0.0, 0.0]$, $[1.0, 1.0]$, $[2.0, 2.0]$.\n  - Omic $2$: $X^{(2)} \\in \\mathbb{R}^{3 \\times 2}$ with rows\n    $[0.0, 0.1]$, $[1.0, 1.1]$, $[1.9, 2.1]$.\n  - Omic $3$: $X^{(3)} \\in \\mathbb{R}^{3 \\times 3}$ with rows\n    $[0.0, 1.0, 2.0]$, $[1.0, 2.0, 3.0]$, $[2.0, 3.0, 4.0]$.\n\nYour program must compute the fused matrix $F$ for each test case following steps $1$ through $9$ above, then flatten and round $F$ as specified in step $10$.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $\\big[$result\\_1,result\\_2,result\\_3,result\\_4$\\big]$). Here each result must itself be a list of $N \\times N$ floating-point numbers (flattened fused matrix) rounded to exactly $6$ decimal places, printed in fixed-point notation without scientific notation. No extra text or lines are permitted.", "solution": "The user has provided a valid problem statement. It is scientifically grounded, well-posed, objective, and complete. The problem requires the implementation of the Similarity Network Fusion (SNF) algorithm, a state-of-the-art method in bioinformatics for integrating multi-omics data. The procedure is based on established principles from statistics, numerical analysis, and graph theory. The provided test cases are well-defined and allow for a deterministic verification of the implementation. I will now proceed with a detailed solution.\n\nThe solution involves a step-by-step implementation of the specified SNF pipeline. The core idea is to convert each omics dataset into a patient-patient similarity network and then iteratively fuse these networks to produce a single, comprehensive network that captures similarities reinforced across multiple data types.\n\n### Algorithmic Design and Principles\n\nThe implementation is designed as a modular function that executes the prescribed sequence of operations.\n\n1.  **Data Standardization (Z-scoring):**\n    For each omic data type $m \\in \\{1,\\dots,M\\}$, represented by a matrix $X^{(m)} \\in \\mathbb{R}^{N \\times p_m}$, we first standardize the data. Standardization is a critical preprocessing step to ensure that features with larger numerical ranges do not disproportionately influence downstream distance calculations. For each feature (column) $j$, we compute its mean $\\mu_{m,j}$ and standard deviation $\\sigma_{m,j}$ over the $N$ patients. Each entry is then transformed according to the Z-score formula:\n    $$ \\tilde{X}^{(m)}_{i,j} = \\frac{X^{(m)}_{i,j} - \\mu_{m,j}}{\\sigma_{m,j}} $$\n    In the special case where a feature is constant across all patients, its standard deviation $\\sigma_{m,j}$ is $0$. To prevent division by zero and render the feature non-informative (as it has no variance), the corresponding transformed feature vector $\\tilde{X}^{(m)}_{:,j}$ is set to a vector of zeros, as per the problem specification.\n\n2.  **Pairwise Similarity Network Construction:**\n    Following standardization, we transform each omic's feature-based representation into a patient-patient similarity network. This is a two-stage process. First, for each omic $m$, we compute the pairwise squared Euclidean distance matrix $D^{(m)} \\in \\mathbb{R}^{N \\times N}$, where each element $D^{(m)}_{ij}$ measures the dissimilarity between patient $i$ and patient $j$ in the standardized feature space:\n    $$ D^{(m)}_{ij} = \\sum_{l=1}^{p_m} \\left(\\tilde{X}^{(m)}_{i,l} - \\tilde{X}^{(m)}_{j,l}\\right)^2 $$\n    Next, these distances are converted into affinities using a Gaussian similarity kernel, also known as a radial basis function (RBF) kernel. This creates the affinity matrix $G^{(m)} \\in \\mathbb{R}^{N \\times N}$:\n    $$ G^{(m)}_{ij} = \\exp\\left( - \\frac{D^{(m)}_{ij}}{2 \\sigma^2} \\right) $$\n    The hyperparameter $\\sigma$ is the kernel bandwidth, which controls how rapidly affinity decays with distance. This function maps distances (from $0$ to $\\infty$) to similarities (from $1$ to $0$). The diagonal elements $G^{(m)}_{ii}$ are explicitly set to $1$, representing maximal self-similarity.\n\n3.  **Network Sparsification and Symmetrization:**\n    Real-world networks are often sparse. To reflect this and to denoise the network by removing weak, likely spurious connections, we retain only the connections to the $k$-nearest neighbors for each patient. For each patient $i$, we identify the $k$ other patients $j$ corresponding to the $k$ largest affinity values $G^{(m)}_{ij}$ (where $j \\neq i$). Ties are broken by preferring the neighbor with the smaller index $j$. This results in a directed k-NN graph. Since similarity is inherently a symmetric concept, this graph is symmetrized to produce an undirected weighted adjacency matrix $W^{(m)}$:\n    $$ W^{(m)} = \\frac{S^{(m)}_{\\text{mask}} + \\left(S^{(m)}_{\\text{mask}}\\right)^{\\top}}{2} $$\n    Here, $S^{(m)}_{\\text{mask}}$ is a matrix that retains the affinity values for the $k$ nearest neighbors in each row of $G^{(m)}$ and is zero otherwise. The diagonal elements $W^{(m)}_{ii}$ are set to $1$.\n\n4.  **Stochastic Normalization and Propagation Matrix Construction:**\n    The affinity matrices $W^{(m)}$ are converted into row-stochastic matrices $P^{(m,0)}$, where each row sums to $1$. This is achieved by dividing each element by its row sum:\n    $$ P^{(m,0)}_{ij} = \\frac{W^{(m)}_{ij}}{\\sum_{j'=1}^{N} W^{(m)}_{ij'}} $$\n    If a row sum is zero (i.e., a patient has no neighbors), it is replaced by a uniform distribution, $P^{(m,0)}_{ij} = 1/N$. These matrices can be interpreted as transition probability matrices for a random walk on the graph.\n    In parallel, we construct fixed propagation matrices $S^{(m)}$. These are sparse, row-stochastic matrices that encode the local neighborhood structure of each network. $S^{(m)}$ is derived from $P^{(m,0)}$ by retaining, for each row $i$, only the entries corresponding to its $k$-nearest neighbors (as determined by the values in $P^{(m,0)}$) and re-normalizing these entries to sum to $1$. This matrix $S^{(m)}$ will act as a fixed filter to propagate information during the fusion process.\n\n5.  **Iterative Network Fusion:**\n    The fusion is achieved through an iterative cross-network diffusion process, a form of message passing. At each iteration $t$, each network $P^{(m,t)}$ is updated based on the information from all other networks. The update rule is:\n    $$ P^{(m,t+1)} = S^{(m)} \\left( \\frac{1}{M-1} \\sum_{\\substack{v=1 \\\\ v \\ne m}}^{M} P^{(v,t)} \\right) \\left(S^{(m)}\\right)^{\\top} $$\n    The term in the parenthesis is the average of all other networks at iteration $t$. This average network is then \"filtered\" through the local structure of network $m$ via pre- and post-multiplication by $S^{(m)}$ and its transpose. This step effectively reinforces similarities that are supported by multiple data sources, causing the networks to become progressively more similar. After each update, $P^{(m,t+1)}$ is re-normalized to be row-stochastic. This process is skipped for the single-omic case ($M=1$).\n\n6.  **Convergence and Final Fusion:**\n    The iterative process continues until the networks stabilize or a maximum number of iterations, $T_{\\max}$, is reached. Stabilization is assessed by measuring the change between consecutive iterations using the Frobenius norm. The process terminates at iteration $t$ if the maximum change across all networks falls below a tolerance $\\varepsilon$:\n    $$ \\Delta^{(t)} = \\max_{m \\in \\{1,\\dots,M\\}} \\left\\| P^{(m,t+1)} - P^{(m,t)} \\right\\|_F \\le \\varepsilon $$\n    Upon termination at $t_{\\text{final}}$, the final fused network $F$ is computed by averaging the converged networks:\n    $$ F = \\frac{1}{M} \\sum_{m=1}^{M} P^{(m,t_{\\text{final}})} $$\n    This final matrix is also row-normalized to yield the final patient-patient similarity network. The resulting matrix $F$ represents a consolidated view of patient similarity, integrating evidence from all $M$ omics.\n\n7.  **Output Formatting:**\n    The final fused matrix $F$ is flattened into a one-dimensional list in row-major order, and each element is rounded to $6$ decimal places as required.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the SNF pipeline, printing the final results.\n    \"\"\"\n\n    def normalize_rows(matrix):\n        \"\"\"\n        Row-normalizes a matrix to be row-stochastic.\n        If a row sum is 0, it is replaced with a uniform distribution.\n        \"\"\"\n        N = matrix.shape[0]\n        row_sums = matrix.sum(axis=1, keepdims=True)\n        normalized_matrix = matrix.copy()\n        \n        non_zero_rows = row_sums.flatten()  0\n        if np.any(non_zero_rows):\n            normalized_matrix[non_zero_rows, :] /= row_sums[non_zero_rows]\n        \n        zero_rows = ~non_zero_rows\n        if np.any(zero_rows):\n            normalized_matrix[zero_rows, :] = 1.0 / N\n            \n        return normalized_matrix\n\n    def get_k_nearest_neighbor_mask(A, k):\n        \"\"\"\n        Determines the k-nearest-neighbor mask for a given affinity matrix.\n        Tie-breaking is done by smaller column index.\n        \"\"\"\n        N = A.shape[0]\n        mask = np.zeros((N, N), dtype=bool)\n\n        for i in range(N):\n            row_values = A[i, :]\n            # Get off-diagonal elements and their original indices\n            off_diag_indices = np.arange(N) != i\n            off_diag_vals = row_values[off_diag_indices]\n            off_diag_j_indices = np.arange(N)[off_diag_indices]\n\n            # Use lexsort for stable sorting: sort by -value (desc), then index (asc)\n            sorted_indices = np.lexsort((off_diag_j_indices, -off_diag_vals))\n            \n            # Identify the top k original column indices\n            top_k_j_indices = off_diag_j_indices[sorted_indices[:k]]\n            mask[i, top_k_j_indices] = True\n            \n        return mask\n\n    def run_snf_for_case(X_list, k, sigma, eps, T_max):\n        \"\"\"\n        Executes the complete Similarity Network Fusion (SNF) pipeline for a single test case.\n        \"\"\"\n        N = X_list[0].shape[0]\n        M = len(X_list)\n\n        # Step 1: Z-scoring per omic and per feature\n        X_std_list = []\n        for X_m in X_list:\n            mean = np.mean(X_m, axis=0)\n            std = np.std(X_m, axis=0)\n            X_tilde_m = np.zeros_like(X_m, dtype=float)\n            \n            non_zero_std_mask = std  0\n            if np.any(non_zero_std_mask):\n                X_tilde_m[:, non_zero_std_mask] = (X_m[:, non_zero_std_mask] - mean[non_zero_std_mask]) / std[non_zero_std_mask]\n            \n            X_std_list.append(X_tilde_m)\n\n        # Step 2: Pairwise squared Euclidean distance\n        D_list = [squareform(pdist(X_std, metric='sqeuclidean')) for X_std in X_std_list]\n\n        # Step 3: Gaussian kernel similarity\n        G_list = []\n        for D_m in D_list:\n            G_m = np.exp(-D_m / (2 * sigma**2))\n            np.fill_diagonal(G_m, 1)\n            G_list.append(G_m)\n\n        # Step 4: k-nearest neighbor sparsification and symmetrization\n        W_list = []\n        for G_m in G_list:\n            knn_mask = get_k_nearest_neighbor_mask(G_m, k)\n            S_mask_matrix = np.zeros_like(G_m)\n            S_mask_matrix[knn_mask] = G_m[knn_mask]\n            \n            W_m = (S_mask_matrix + S_mask_matrix.T) / 2\n            np.fill_diagonal(W_m, 1)\n            W_list.append(W_m)\n\n        # Step 5: Row-stochastic normalization\n        P0_list = [normalize_rows(W_m) for W_m in W_list]\n\n        if M == 1:\n            # For M=1, result is the single normalized network\n            final_fused_matrix = normalize_rows(P0_list[0])\n            flat_rounded = [f\"{x:.6f}\" for x in final_fused_matrix.flatten()]\n            return f\"[{','.join(flat_rounded)}]\"\n\n        # Step 6: Fixed neighbor-constrained propagation matrices\n        S_list = []\n        for P0_m in P0_list:\n            S_m = np.zeros_like(P0_m)\n            knn_mask = get_k_nearest_neighbor_mask(P0_m, k)\n            for i in range(N):\n                neighbor_indices = np.where(knn_mask[i, :])[0]\n                denominator = P0_m[i, neighbor_indices].sum()\n                if denominator  0:\n                    S_m[i, neighbor_indices] = P0_m[i, neighbor_indices] / denominator\n                else:\n                    S_m[i, :] = 1.0 / N\n            S_list.append(S_m)\n            \n        # Step 7  8: Iterative message passing and convergence\n        P_current = [p.copy() for p in P0_list]\n        for t in range(T_max):\n            P_next = [np.zeros_like(p) for p in P_current]\n            sum_P_current = sum(P_current) if M  1 else P_current[0]\n            \n            for m in range(M):\n                avg_others = (sum_P_current - P_current[m]) / (M - 1)\n                P_next_m_unnorm = S_list[m] @ avg_others @ S_list[m].T\n                P_next[m] = normalize_rows(P_next_m_unnorm)\n\n            max_diff = 0\n            for m in range(M):\n                diff = np.linalg.norm(P_next[m] - P_current[m], 'fro')\n                max_diff = max(max_diff, diff)\n            \n            P_current = P_next\n            \n            if max_diff = eps:\n                break\n\n        # Step 9: Fused matrix\n        F_unnorm = sum(P_current) / M\n        final_fused_matrix = normalize_rows(F_unnorm)\n\n        # Step 10: Required output formatting\n        flat_rounded = [f\"{x:.6f}\" for x in final_fused_matrix.flatten()]\n        return f\"[{','.join(flat_rounded)}]\"\n\n    test_cases = [\n        {\n            \"X_list\": [\n                np.array([[0.0, 1.0], [0.2, 0.9], [3.0, 3.5], [3.2, 3.6]], dtype=float),\n                np.array([[0.1, 1.1], [0.0, 1.0], [3.1, 3.4], [3.3, 3.7]], dtype=float)\n            ],\n            \"k\": 2, \"sigma\": 1.0, \"eps\": 1e-6, \"T_max\": 50\n        },\n        {\n            \"X_list\": [\n                np.array([[1.0, 0.0, 0.5], [0.9, 0.1, 0.4], [3.0, 3.5, 3.7], [3.1, 3.6, 3.8]], dtype=float)\n            ],\n            \"k\": 1, \"sigma\": 0.7, \"eps\": 1e-6, \"T_max\": 10\n        },\n        {\n            \"X_list\": [\n                np.array([[0.0, 0.0], [0.1, -0.1], [5.0, 5.0], [5.1, 4.9], [2.5, 2.5]], dtype=float),\n                np.array([[0.2, -0.2], [0.0, 0.0], [4.9, 5.1], [5.2, 5.0], [2.4, 2.6]], dtype=float)\n            ],\n            \"k\": 2, \"sigma\": 1.5, \"eps\": 1e9, \"T_max\": 20\n        },\n        {\n            \"X_list\": [\n                np.array([[0.0, 0.0], [1.0, 1.0], [2.0, 2.0]], dtype=float),\n                np.array([[0.0, 0.1], [1.0, 1.1], [1.9, 2.1]], dtype=float),\n                np.array([[0.0, 1.0, 2.0], [1.0, 2.0, 3.0], [2.0, 3.0, 4.0]], dtype=float)\n            ],\n            \"k\": 2, \"sigma\": 1.0, \"eps\": 1e-12, \"T_max\": 1\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result_str = run_snf_for_case(\n            case[\"X_list\"], case[\"k\"], case[\"sigma\"], case[\"eps\"], case[\"T_max\"]\n        )\n        results.append(result_str)\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "5034014"}]}