{"hands_on_practices": [{"introduction": "A foundational task in spatial omics is the precise alignment of data from different modalities, such as sequencing-based molecular maps and high-resolution histology images. This exercise guides you through the process of computing an affine transformation to register two sets of corresponding points, a common procedure to overlay molecular data onto its morphological context. By minimizing the squared error between fiducial markers, you will apply the principle of least squares to solve for the transformation parameters, a cornerstone technique in computational image analysis [@problem_id:5062750].", "problem": "A common step in spatial transcriptomics and proteomics pipelines in translational medicine is registering complementary modalities, such as Slide-seq bead maps to hematoxylin and eosin histology images, to enable joint inference of spatial gene and protein expression. Consider an experiment in which fluorescent fiducial markers are visible in both modalities. Let the bead map coordinates be denoted by $\\mathbf{x}_i = (x_i, y_i)$ and the corresponding histology image coordinates by $\\mathbf{u}_i = (u_i, v_i)$ for $i = 1, \\dots, n$. Assume the mapping from the bead space to the histology space is well-approximated by a two-dimensional affine transformation, defined by a $2 \\times 2$ matrix $\\mathbf{A}$ and a translation vector $\\mathbf{t}$, so that $\\mathbf{u}_i \\approx \\mathbf{A}\\mathbf{x}_i + \\mathbf{t}$, where $\\mathbf{A}$ models rotation, anisotropic scaling, and shear, and $\\mathbf{t}$ models translation. Both coordinate systems are measured in micrometers.\n\nStarting from the definition of the least squares objective,\n$$\nJ(\\mathbf{A}, \\mathbf{t}) = \\sum_{i=1}^{n} \\left\\| \\mathbf{A}\\mathbf{x}_i + \\mathbf{t} - \\mathbf{u}_i \\right\\|^{2},\n$$\nderive the normal equations that minimize $J(\\mathbf{A}, \\mathbf{t})$ with respect to the six unknown scalar parameters $(\\alpha_{11}, \\alpha_{12}, \\alpha_{21}, \\alpha_{22}, \\tau_{1}, \\tau_{2})$ where $\\mathbf{A} = \\begin{pmatrix} \\alpha_{11}  \\alpha_{12} \\\\ \\alpha_{21}  \\alpha_{22} \\end{pmatrix}$ and $\\mathbf{t} = (\\tau_{1}, \\tau_{2})$. Then, use the following fiducial correspondences to compute the parameter values that minimize the squared registration error:\n\n- $i = 1$: $\\mathbf{x}_1 = (-1, -1)$, $\\mathbf{u}_1 = (6.7, -6.6)$\n- $i = 2$: $\\mathbf{x}_2 = (1, -1)$, $\\mathbf{u}_2 = (8.9, -7.2)$\n- $i = 3$: $\\mathbf{x}_3 = (-1, 1)$, $\\mathbf{u}_3 = (7.1, -4.8)$\n- $i = 4$: $\\mathbf{x}_4 = (1, 1)$, $\\mathbf{u}_4 = (9.3, -5.4)$\n- $i = 5$: $\\mathbf{x}_5 = (-2, 0)$, $\\mathbf{u}_5 = (5.8, -5.4)$\n- $i = 6$: $\\mathbf{x}_6 = (2, 0)$, $\\mathbf{u}_6 = (10.2, -6.6)$\n- $i = 7$: $\\mathbf{x}_7 = (0, -2)$, $\\mathbf{u}_7 = (7.6, -7.8)$\n- $i = 8$: $\\mathbf{x}_8 = (0, 2)$, $\\mathbf{u}_8 = (8.4, -4.2)$\n\nAll coordinates $(x_i, y_i)$ and $(u_i, v_i)$ are expressed in micrometers. State clearly any assumptions you use. Report your final answer as the ordered parameter tuple $(\\alpha_{11}, \\alpha_{12}, \\alpha_{21}, \\alpha_{22}, \\tau_{1}, \\tau_{2})$. The entries $\\alpha_{11}, \\alpha_{12}, \\alpha_{21}, \\alpha_{22}$ are unitless; express $\\tau_{1}$ and $\\tau_{2}$ in micrometers. If you perform any numerical approximation, round your values to four significant figures.", "solution": "The problem requires the determination of the parameters of a two-dimensional affine transformation that maps a set of source points $\\mathbf{x}_i$ to a corresponding set of target points $\\mathbf{u}_i$. The transformation is given by $\\mathbf{u}_i \\approx \\mathbf{A}\\mathbf{x}_i + \\mathbf{t}$, and the parameters are the elements of the matrix $\\mathbf{A}$ and the vector $\\mathbf{t}$. These parameters are to be found by minimizing the sum of squared errors, a standard least squares problem.\n\nFirst, the validation of the problem statement is performed.\n\n### Step 1: Extract Givens\n- The an affine transformation model is $\\mathbf{u}_i \\approx \\mathbf{A}\\mathbf{x}_i + \\mathbf{t}$.\n- The coordinate vectors are $\\mathbf{x}_i = (x_i, y_i)$ and $\\mathbf{u}_i = (u_i, v_i)$.\n- The transformation matrix is $\\mathbf{A} = \\begin{pmatrix} \\alpha_{11}  \\alpha_{12} \\\\ \\alpha_{21}  \\alpha_{22} \\end{pmatrix}$.\n- The translation vector is $\\mathbf{t} = (\\tau_{1}, \\tau_{2})$.\n- The objective function is the sum of squared errors: $J(\\mathbf{A}, \\mathbf{t}) = \\sum_{i=1}^{n} \\left\\| \\mathbf{A}\\mathbf{x}_i + \\mathbf{t} - \\mathbf{u}_i \\right\\|^{2}$.\n- There are $n=8$ pairs of corresponding points:\n  - $\\mathbf{x}_1 = (-1, -1), \\mathbf{u}_1 = (6.7, -6.6)$\n  - $\\mathbf{x}_2 = (1, -1), \\mathbf{u}_2 = (8.9, -7.2)$\n  - $\\mathbf{x}_3 = (-1, 1), \\mathbf{u}_3 = (7.1, -4.8)$\n  - $\\mathbf{x}_4 = (1, 1), \\mathbf{u}_4 = (9.3, -5.4)$\n  - $\\mathbf{x}_5 = (-2, 0), \\mathbf{u}_5 = (5.8, -5.4)$\n  - $\\mathbf{x}_6 = (2, 0), \\mathbf{u}_6 = (10.2, -6.6)$\n  - $\\mathbf{x}_7 = (0, -2), \\mathbf{u}_7 = (7.6, -7.8)$\n  - $\\mathbf{x}_8 = (0, 2), \\mathbf{u}_8 = (8.4, -4.2)$\n- All coordinates are in micrometers.\n- The final answer should be the ordered tuple $(\\alpha_{11}, \\alpha_{12}, \\alpha_{21}, \\alpha_{22}, \\tau_{1}, \\tau_{2})$, with numerical values rounded to four significant figures if approximation is used.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as finding an optimal affine transformation via least squares is a standard and fundamental technique in image registration and data alignment, particularly relevant in fields like spatial omics. The problem is well-posed; it is an overdetermined linear least squares problem with $8$ data points for $6$ unknown parameters. Given the spatial distribution of the source points $\\mathbf{x}_i$, a unique solution is expected. The problem statement is objective, complete, and contains no contradictions or scientifically unsound premises.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be derived.\n\n### Derivation of the Normal Equations\n\nThe objective function $J(\\mathbf{A}, \\mathbf{t})$ can be expressed in terms of the scalar parameters:\n$$\nJ(\\alpha_{11}, \\alpha_{12}, \\alpha_{21}, \\alpha_{22}, \\tau_{1}, \\tau_{2}) = \\sum_{i=1}^{n} \\left[ (\\alpha_{11}x_i + \\alpha_{12}y_i + \\tau_1 - u_i)^2 + (\\alpha_{21}x_i + \\alpha_{22}y_i + \\tau_2 - v_i)^2 \\right]\n$$\nThis function is separable into two independent parts:\n$$\nJ = J_1(\\alpha_{11}, \\alpha_{12}, \\tau_1) + J_2(\\alpha_{21}, \\alpha_{22}, \\tau_2)\n$$\nwhere\n$$\nJ_1 = \\sum_{i=1}^{n} (\\alpha_{11}x_i + \\alpha_{12}y_i + \\tau_1 - u_i)^2\n$$\n$$\nJ_2 = \\sum_{i=1}^{n} (\\alpha_{21}x_i + \\alpha_{22}y_i + \\tau_2 - v_i)^2\n$$\nTo minimize $J$, we can minimize $J_1$ and $J_2$ independently. We find the minimum by setting the partial derivatives of $J_1$ with respect to $\\alpha_{11}$, $\\alpha_{12}$, and $\\tau_1$ to zero, and similarly for $J_2$.\n\nFor $J_1$:\n$$\n\\frac{\\partial J_1}{\\partial \\alpha_{11}} = \\sum_{i=1}^{n} 2(\\alpha_{11}x_i + \\alpha_{12}y_i + \\tau_1 - u_i)x_i = 0\n$$\n$$\n\\frac{\\partial J_1}{\\partial \\alpha_{12}} = \\sum_{i=1}^{n} 2(\\alpha_{11}x_i + \\alpha_{12}y_i + \\tau_1 - u_i)y_i = 0\n$$\n$$\n\\frac{\\partial J_1}{\\partial \\tau_1} = \\sum_{i=1}^{n} 2(\\alpha_{11}x_i + \\alpha_{12}y_i + \\tau_1 - u_i) = 0\n$$\nRearranging these gives the normal equations for the first set of parameters:\n$$\n\\alpha_{11}\\left(\\sum x_i^2\\right) + \\alpha_{12}\\left(\\sum x_i y_i\\right) + \\tau_1\\left(\\sum x_i\\right) = \\sum u_i x_i\n$$\n$$\n\\alpha_{11}\\left(\\sum x_i y_i\\right) + \\alpha_{12}\\left(\\sum y_i^2\\right) + \\tau_1\\left(\\sum y_i\\right) = \\sum u_i y_i\n$$\n$$\n\\alpha_{11}\\left(\\sum x_i\\right) + \\alpha_{12}\\left(\\sum y_i\\right) + \\tau_1(n) = \\sum u_i\n$$\nThis can be written in matrix form as:\n$$\n\\begin{pmatrix} \\sum x_i^2  \\sum x_i y_i  \\sum x_i \\\\ \\sum x_i y_i  \\sum y_i^2  \\sum y_i \\\\ \\sum x_i  \\sum y_i  n \\end{pmatrix}\n\\begin{pmatrix} \\alpha_{11} \\\\ \\alpha_{12} \\\\ \\tau_1 \\end{pmatrix} =\n\\begin{pmatrix} \\sum u_i x_i \\\\ \\sum u_i y_i \\\\ \\sum u_i \\end{pmatrix}\n$$\nA similar derivation for $J_2$ yields the second set of normal equations:\n$$\n\\begin{pmatrix} \\sum x_i^2  \\sum x_i y_i  \\sum x_i \\\\ \\sum x_i y_i  \\sum y_i^2  \\sum y_i \\\\ \\sum x_i  \\sum y_i  n \\end{pmatrix}\n\\begin{pmatrix} \\alpha_{21} \\\\ \\alpha_{22} \\\\ \\tau_2 \\end{pmatrix} =\n\\begin{pmatrix} \\sum v_i x_i \\\\ \\sum v_i y_i \\\\ \\sum v_i \\end{pmatrix}\n$$\nThe primary assumption made here is that a unique solution exists, which requires the $3 \\times 3$ matrix of sums to be invertible.\n\n### Calculation of Parameters\n\nWe compute the required sums using the $n=8$ provided data points.\n- $\\sum_{i=1}^8 x_i = -1 + 1 - 1 + 1 - 2 + 2 + 0 + 0 = 0$\n- $\\sum_{i=1}^8 y_i = -1 - 1 + 1 + 1 + 0 + 0 - 2 + 2 = 0$\n- $\\sum_{i=1}^8 x_i^2 = (-1)^2 + 1^2 + (-1)^2 + 1^2 + (-2)^2 + 2^2 + 0^2 + 0^2 = 12$\n- $\\sum_{i=1}^8 y_i^2 = (-1)^2 + (-1)^2 + 1^2 + 1^2 + 0^2 + 0^2 + (-2)^2 + 2^2 = 12$\n- $\\sum_{i=1}^8 x_i y_i = (-1)(-1) + (1)(-1) + (-1)(1) + (1)(1) + 0 + 0 + 0 + 0 = 0$\n\nThe $3 \\times 3$ matrix is therefore:\n$$\n\\begin{pmatrix} 12  0  0 \\\\ 0  12  0 \\\\ 0  0  8 \\end{pmatrix}\n$$\nThis matrix is diagonal and clearly invertible, confirming a unique solution exists.\n\nNext, we compute the sums for the right-hand side vectors.\nFor the $(\\alpha_{11}, \\alpha_{12}, \\tau_1)$ system:\n- $\\sum u_i = 6.7 + 8.9 + 7.1 + 9.3 + 5.8 + 10.2 + 7.6 + 8.4 = 64.0$\n- $\\sum u_i x_i = 6.7(-1) + 8.9(1) + 7.1(-1) + 9.3(1) + 5.8(-2) + 10.2(2) = -6.7 + 8.9 - 7.1 + 9.3 - 11.6 + 20.4 = 13.2$\n- $\\sum u_i y_i = 6.7(-1) + 8.9(-1) + 7.1(1) + 9.3(1) + 7.6(-2) + 8.4(2) = -6.7 - 8.9 + 7.1 + 9.3 - 15.2 + 16.8 = 2.4$\n\nThe first linear system is:\n$$\n\\begin{pmatrix} 12  0  0 \\\\ 0  12  0 \\\\ 0  0  8 \\end{pmatrix}\n\\begin{pmatrix} \\alpha_{11} \\\\ \\alpha_{12} \\\\ \\tau_1 \\end{pmatrix} =\n\\begin{pmatrix} 13.2 \\\\ 2.4 \\\\ 64.0 \\end{pmatrix}\n$$\nSolving this diagonal system:\n- $12 \\alpha_{11} = 13.2 \\implies \\alpha_{11} = \\frac{13.2}{12} = 1.1$\n- $12 \\alpha_{12} = 2.4 \\implies \\alpha_{12} = \\frac{2.4}{12} = 0.2$\n- $8 \\tau_1 = 64.0 \\implies \\tau_1 = \\frac{64.0}{8} = 8.0$\n\nFor the $(\\alpha_{21}, \\alpha_{22}, \\tau_2)$ system:\n- $\\sum v_i = -6.6 - 7.2 - 4.8 - 5.4 - 5.4 - 6.6 - 7.8 - 4.2 = -48.0$\n- $\\sum v_i x_i = -6.6(-1) - 7.2(1) - 4.8(-1) - 5.4(1) - 5.4(-2) - 6.6(2) = 6.6 - 7.2 + 4.8 - 5.4 + 10.8 - 13.2 = -3.6$\n- $\\sum v_i y_i = -6.6(-1) - 7.2(-1) - 4.8(1) - 5.4(1) - 7.8(-2) - 4.2(2) = 6.6 + 7.2 - 4.8 - 5.4 + 15.6 - 8.4 = 10.8$\n\nThe second linear system is:\n$$\n\\begin{pmatrix} 12  0  0 \\\\ 0  12  0 \\\\ 0  0  8 \\end{pmatrix}\n\\begin{pmatrix} \\alpha_{21} \\\\ \\alpha_{22} \\\\ \\tau_2 \\end{pmatrix} =\n\\begin{pmatrix} -3.6 \\\\ 10.8 \\\\ -48.0 \\end{pmatrix}\n$$\nSolving this diagonal system:\n- $12 \\alpha_{21} = -3.6 \\implies \\alpha_{21} = \\frac{-3.6}{12} = -0.3$\n- $12 \\alpha_{22} = 10.8 \\implies \\alpha_{22} = \\frac{10.8}{12} = 0.9$\n- $8 \\tau_2 = -48.0 \\implies \\tau_2 = \\frac{-48.0}{8} = -6.0$\n\nThe calculations are exact, so no rounding is necessary. The calculated parameters are:\n- $\\alpha_{11} = 1.1$ (unitless)\n- $\\alpha_{12} = 0.2$ (unitless)\n- $\\alpha_{21} = -0.3$ (unitless)\n- $\\alpha_{22} = 0.9$ (unitless)\n- $\\tau_1 = 8.0$ (micrometers)\n- $\\tau_2 = -6.0$ (micrometers)\n\nThe final answer is the ordered tuple $(\\alpha_{11}, \\alpha_{12}, \\alpha_{21}, \\alpha_{22}, \\tau_{1}, \\tau_{2})$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1.1  0.2  -0.3  0.9  8.0  -6.0 \\end{pmatrix}}\n$$", "id": "5062750"}, {"introduction": "Once spatial data is properly aligned, a primary goal is to decipher the cellular composition within each measured spot, which is often a mixture of different cell types. This practice addresses the \"deconvolution\" problem by modeling a spot's gene expression profile as a linear combination of known cell-type-specific signatures [@problem_id:4386305]. You will implement a constrained regression solver to estimate the proportions of each cell type, a crucial step for mapping the cellular architecture of the tissue microenvironment.", "problem": "You are given a systems biomedicine scenario in which a spatial spot from the Visium Spatial Gene Expression (Visium) technology contains a mixture of cell types whose messenger ribonucleic acid (mRNA) transcript abundances are measured across multiple genes. In such a spot, the observed expression profile is a mixture of the expression profiles of constituent cell types. To quantify the composition of a spot, assume a reference signature matrix that encodes the average gene expression of each cell type. The task is to formulate and solve a constrained regression problem that estimates the cell type proportions for each spot.\n\nUse the following fundamental base and assumptions:\n- Central Dogma of Molecular Biology: deoxyribonucleic acid (DNA) is transcribed into ribonucleic acid (RNA), which is translated into protein. In sufficiently averaged and normalized settings, observed transcript counts per gene reflect the aggregate of contributions from different cell types present in a sample.\n- Linear mixture model: the observed vector of gene expression in a spot can be modeled as a linear combination of cell-type-specific signatures weighted by unknown nonnegative proportions. Let $G$ be the number of genes, $K$ be the number of cell types, $\\mathbf{S} \\in \\mathbb{R}_{\\ge 0}^{G \\times K}$ be the known signature matrix whose $k$-th column is the reference expression profile for cell type $k$, and $\\mathbf{y} \\in \\mathbb{R}_{\\ge 0}^{G}$ be the observed expression vector for a spot. The unknown vector of cell type proportions is $\\mathbf{p} \\in \\mathbb{R}_{\\ge 0}^{K}$.\n- Constraints: proportions are nonnegative and sum to one, i.e., $\\mathbf{p} \\succeq \\mathbf{0}$ and $\\mathbf{1}^{\\top} \\mathbf{p} = 1$, where $\\mathbf{1}$ is the all-ones vector in $\\mathbb{R}^{K}$.\n- Objective: the estimation criterion is the squared Euclidean distance between predicted and observed expression, optionally with nonnegative weights $w_g$ per gene to reflect reliability, and a small ridge regularization to ensure uniqueness and numerical stability.\n\nFormally, define the deconvolution as the following constrained optimization for each spot:\nMinimize the function\n$$\n\\frac{1}{2} \\sum_{g=1}^{G} w_g \\left( (\\mathbf{S}\\mathbf{p})_g - \\mathbf{y}_g \\right)^2 + \\frac{\\alpha}{2} \\lVert \\mathbf{p} \\rVert_2^2\n$$\nsubject to the constraints $\\mathbf{p} \\succeq \\mathbf{0}$ and $\\mathbf{1}^{\\top} \\mathbf{p} = 1$,\nwhere $w_g \\ge 0$ are given per-gene weights and $\\alpha \\ge 0$ is a small regularization parameter.\n\nYour program must implement a solver that, for each test case, computes the cell type proportion vector $\\widehat{\\mathbf{p}}$ that minimizes the above objective under the constraints. You may assume convexity and that a minimizer exists. The solver must be general and must not rely on closed-form inversion of the constraints. The program must round each component of $\\widehat{\\mathbf{p}}$ to $4$ decimal places.\n\nAngle units are not applicable. There are no physical units; proportions must be reported as decimal fractions.\n\nTest suite:\nFor each case, you are given $G$, $K$, the signature matrix $\\mathbf{S}$, the observed expression $\\mathbf{y}$, the weights $\\mathbf{w}$, and the ridge parameter $\\alpha$. All numerical values are given explicitly below.\n\n- Case A (general, small noise):\n  - $G = 5$, $K = 3$, $\\alpha = 10^{-6}$.\n  - $\\mathbf{S} = \\begin{bmatrix}\n  8  2  1 \\\\\n  1  7  2 \\\\\n  0  1  9 \\\\\n  5  2  1 \\\\\n  2  3  4\n  \\end{bmatrix}$.\n  - $\\mathbf{y} = \\begin{bmatrix} 4.9 \\\\ 2.95 \\\\ 2.12 \\\\ 3.27 \\\\ 2.7 \\end{bmatrix}$.\n  - $\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n\n- Case B (pure cell type, no noise):\n  - $G = 5$, $K = 3$, $\\alpha = 10^{-6}$.\n  - $\\mathbf{S}$ is identical to Case A.\n  - $\\mathbf{y} = \\begin{bmatrix} 2 \\\\ 7 \\\\ 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$.\n  - $\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n\n- Case C (collinearity, exact mixture):\n  - $G = 4$, $K = 3$, $\\alpha = 10^{-6}$.\n  - $\\mathbf{S} = \\begin{bmatrix}\n  3  3  1 \\\\\n  2  2  0.5 \\\\\n  1  1  4 \\\\\n  0.5  0.5  2\n  \\end{bmatrix}$.\n  - $\\mathbf{y} = \\begin{bmatrix} 2.6 \\\\ 1.7 \\\\ 1.6 \\\\ 0.8 \\end{bmatrix}$.\n  - $\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n\n- Case D (zeros and dropout, moderate noise):\n  - $G = 4$, $K = 2$, $\\alpha = 10^{-6}$.\n  - $\\mathbf{S} = \\begin{bmatrix}\n  0  3 \\\\\n  5  0 \\\\\n  0  2 \\\\\n  4  1\n  \\end{bmatrix}$.\n  - $\\mathbf{y} = \\begin{bmatrix} 1.15 \\\\ 3.1 \\\\ 0.4 \\\\ 2.7 \\end{bmatrix}$.\n  - $\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n\n- Case E (weighted fit, noisy second gene downweighted):\n  - $G = 3$, $K = 3$, $\\alpha = 10^{-6}$.\n  - $\\mathbf{S} = \\begin{bmatrix}\n  10  1  0.5 \\\\\n  0.5  8  1 \\\\\n  1  1  6\n  \\end{bmatrix}$.\n  - $\\mathbf{y} = \\begin{bmatrix} 2.65 \\\\ 5.9 \\\\ 2.4 \\end{bmatrix}$.\n  - $\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 0.2 \\\\ 1 \\end{bmatrix}$.\n\nImplementation requirements:\n- Implement a solver that handles the constraints $\\mathbf{p} \\succeq \\mathbf{0}$ and $\\mathbf{1}^{\\top} \\mathbf{p} = 1$ exactly, without relaxing them, and includes the ridge regularization $\\alpha$ in the objective.\n- The algorithm must be principled and general (for example, a projected gradient method with a mathematically justified step size and projection onto the probability simplex).\n- For each case, compute the estimated proportions $\\widehat{\\mathbf{p}}$ and round each component to $4$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$), where each $resulti$ is the list of rounded proportions for the corresponding case. The output must contain no spaces. For example: $[[0.5000,0.3000,0.2000],[0.0000,1.0000,0.2000],\\dots]$.", "solution": "The user has provided a problem statement that has been validated and deemed to be scientifically grounded, well-posed, and objective.\n\nThe problem requires the solution of a constrained quadratic optimization problem to perform cell-type deconvolution in spatial transcriptomics. The observed gene expression vector for a spot, $\\mathbf{y} \\in \\mathbb{R}_{\\ge 0}^{G}$, is modeled as a linear combination of reference cell-type expression signatures. These signatures are encoded in the columns of a known matrix $\\mathbf{S} \\in \\mathbb{R}_{\\ge 0}^{G \\times K}$, where $G$ is the number of genes and $K$ is the number of cell types. The unknown quantities are the proportions of each cell type, represented by the vector $\\mathbf{p} \\in \\mathbb{R}_{\\ge 0}^{K}$.\n\nThe proportions must satisfy two physical constraints: they must be non-negative, $\\mathbf{p} \\succeq \\mathbf{0}$, and they must sum to unity, $\\mathbf{1}^{\\top} \\mathbf{p} = 1$. The set of all vectors $\\mathbf{p}$ satisfying these constraints forms the standard $K-1$-dimensional simplex, denoted as $\\Delta^{K-1}$.\n\nThe goal is to find the proportion vector $\\widehat{\\mathbf{p}}$ that minimizes a regularized, weighted, squared error objective function. The objective function is given by:\n$$\nf(\\mathbf{p}) = \\frac{1}{2} \\sum_{g=1}^{G} w_g \\left( (\\mathbf{S}\\mathbf{p})_g - y_g \\right)^2 + \\frac{\\alpha}{2} \\lVert \\mathbf{p} \\rVert_2^2\n$$\nwhere $w_g \\ge 0$ are gene-specific weights and $\\alpha \\ge 0$ is a Tikhonov (ridge) regularization parameter.\n\nThis problem can be expressed in matrix form. Let $\\mathbf{W}$ be a $G \\times G$ diagonal matrix with the weights $w_g$ on its diagonal. The objective function is:\n$$\nf(\\mathbf{p}) = \\frac{1}{2} (\\mathbf{S}\\mathbf{p} - \\mathbf{y})^{\\top} \\mathbf{W} (\\mathbf{S}\\mathbf{p} - \\mathbf{y}) + \\frac{\\alpha}{2} \\mathbf{p}^{\\top}\\mathbf{p}\n$$\nThe problem is a quadratic program (QP) with a convex objective function and a compact, convex feasible set (the simplex $\\Delta^{K-1}$). The strict convexity imparted by the ridge term ($\\alpha  0$) guarantees a unique solution.\n\nA suitable and general algorithm for solving this problem is Projected Gradient Descent. This iterative method consists of taking a step in the negative gradient direction and then projecting the resulting point back onto the feasible set.\n\nThe algorithm proceeds as follows:\n1.  Initialize $\\mathbf{p}^{(0)}$ to a feasible point, typically the center of the simplex: $\\mathbf{p}^{(0)} = \\frac{1}{K}\\mathbf{1}$.\n2.  For iteration $t = 0, 1, 2, \\dots$ until convergence:\n    a.  Compute a descent direction by calculating the gradient of the objective function, $\\nabla f(\\mathbf{p})$, at the current iterate $\\mathbf{p}^{(t)}$.\n    b.  Take a step in this direction: $\\mathbf{z}^{(t+1)} = \\mathbf{p}^{(t)} - \\eta \\nabla f(\\mathbf{p}^{(t)})$, where $\\eta  0$ is the step size.\n    c.  Project the point $\\mathbf{z}^{(t+1)}$ onto the simplex $\\Delta^{K-1}$: $\\mathbf{p}^{(t+1)} = \\Pi_{\\Delta^{K-1}}(\\mathbf{z}^{(t+1)})$.\n3.  The algorithm terminates when the change in $\\mathbf{p}$ between iterations is below a small tolerance.\n\nThe key components of the algorithm are the gradient calculation, the step size determination, and the projection operator.\n\n**1. Gradient Calculation**\n\nThe objective function is:\n$$\nf(\\mathbf{p}) = \\frac{1}{2} \\left( \\mathbf{p}^{\\top}\\mathbf{S}^{\\top}\\mathbf{W}\\mathbf{S}\\mathbf{p} - 2\\mathbf{y}^{\\top}\\mathbf{W}\\mathbf{S}\\mathbf{p} + \\mathbf{y}^{\\top}\\mathbf{W}\\mathbf{y} \\right) + \\frac{\\alpha}{2} \\mathbf{p}^{\\top}\\mathbf{p}\n$$\nThe gradient with respect to $\\mathbf{p}$ is:\n$$\n\\nabla f(\\mathbf{p}) = \\mathbf{S}^{\\top}\\mathbf{W}\\mathbf{S}\\mathbf{p} - \\mathbf{S}^{\\top}\\mathbf{W}\\mathbf{y} + \\alpha\\mathbf{p} = \\mathbf{S}^{\\top}\\mathbf{W}(\\mathbf{S}\\mathbf{p} - \\mathbf{y}) + \\alpha\\mathbf{p}\n$$\n\n**2. Step Size Determination**\n\nFor gradient descent to converge, the step size $\\eta$ must be chosen appropriately. For a convex function with a Lipschitz continuous gradient, a sufficient condition for convergence is $0  \\eta  2/L$, where $L$ is the Lipschitz constant of the gradient. For our quadratic objective, $L$ is the largest eigenvalue of the Hessian matrix, $\\nabla^2 f(\\mathbf{p})$. The Hessian is:\n$$\n\\nabla^2 f(\\mathbf{p}) = \\mathbf{S}^{\\top}\\mathbf{W}\\mathbf{S} + \\alpha\\mathbf{I}\n$$\nwhere $\\mathbf{I}$ is the $K \\times K$ identity matrix. This Hessian is a constant, symmetric, positive-definite matrix (for $\\alpha  0$). We can compute its eigenvalues and set $L$ to be the maximum eigenvalue, $\\lambda_{\\text{max}}$. A safe and constant step size is then $\\eta = 1/L$.\n\n**3. Projection onto the Probability Simplex**\n\nThe projection step, $\\mathbf{p}^{(t+1)} = \\Pi_{\\Delta^{K-1}}(\\mathbf{z}^{(t+1)})$, finds the point on the simplex that is closest to $\\mathbf{z}^{(t+1)}$ in Euclidean distance. This is itself a QP:\n$$\n\\text{minimize}_{\\mathbf{p}} \\quad \\frac{1}{2} \\lVert \\mathbf{p} - \\mathbf{z} \\rVert_2^2 \\quad \\text{subject to} \\quad \\mathbf{p} \\succeq \\mathbf{0}, \\mathbf{1}^{\\top}\\mathbf{p} = 1\n$$\nAn efficient algorithm to solve this projection in $O(K \\log K)$ time exists. The steps are:\n1.  Sort the elements of the vector $\\mathbf{z}$ in descending order to get a new vector $\\mathbf{u}$.\n2.  Find an integer $\\rho$ such that $\\rho = \\max\\left\\{ j \\in \\{1, \\dots, K\\} \\mid u_j - \\frac{1}{j} \\left( \\sum_{i=1}^j u_i - 1 \\right)  0 \\right\\}$.\n3.  Define a threshold $\\theta = \\frac{1}{\\rho} \\left( \\sum_{i=1}^\\rho u_i - 1 \\right)$.\n4.  The projected vector $\\mathbf{p}$ is obtained by applying a soft-thresholding operator element-wise to the original vector $\\mathbf{z}$: $p_k = \\max(z_k - \\theta, 0)$ for $k = 1, \\dots, K$.\n\nBy combining these three components into an iterative loop, the algorithm will converge to the unique minimizer $\\widehat{\\mathbf{p}}$ for each test case.", "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Solves the constrained regression problem for cell type deconvolution\n    for a series of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case A (general, small noise)\n        {\n            \"G\": 5, \"K\": 3, \"alpha\": 1e-6,\n            \"S\": np.array([\n                [8, 2, 1],\n                [1, 7, 2],\n                [0, 1, 9],\n                [5, 2, 1],\n                [2, 3, 4]\n            ]),\n            \"y\": np.array([4.9, 2.95, 2.12, 3.27, 2.7]),\n            \"w\": np.array([1, 1, 1, 1, 1])\n        },\n        # Case B (pure cell type, no noise)\n        {\n            \"G\": 5, \"K\": 3, \"alpha\": 1e-6,\n            \"S\": np.array([\n                [8, 2, 1],\n                [1, 7, 2],\n                [0, 1, 9],\n                [5, 2, 1],\n                [2, 3, 4]\n            ]),\n            \"y\": np.array([2, 7, 1, 2, 3]),\n            \"w\": np.array([1, 1, 1, 1, 1])\n        },\n        # Case C (collinearity, exact mixture)\n        {\n            \"G\": 4, \"K\": 3, \"alpha\": 1e-6,\n            \"S\": np.array([\n                [3, 3, 1],\n                [2, 2, 0.5],\n                [1, 1, 4],\n                [0.5, 0.5, 2]\n            ]),\n            \"y\": np.array([2.6, 1.7, 1.6, 0.8]),\n            \"w\": np.array([1, 1, 1, 1])\n        },\n        # Case D (zeros and dropout, moderate noise)\n        {\n            \"G\": 4, \"K\": 2, \"alpha\": 1e-6,\n            \"S\": np.array([\n                [0, 3],\n                [5, 0],\n                [0, 2],\n                [4, 1]\n            ]),\n            \"y\": np.array([1.15, 3.1, 0.4, 2.7]),\n            \"w\": np.array([1, 1, 1, 1])\n        },\n        # Case E (weighted fit, noisy second gene downweighted)\n        {\n            \"G\": 3, \"K\": 3, \"alpha\": 1e-6,\n            \"S\": np.array([\n                [10, 1, 0.5],\n                [0.5, 8, 1],\n                [1, 1, 6]\n            ]),\n            \"y\": np.array([2.65, 5.9, 2.4]),\n            \"w\": np.array([1, 0.2, 1])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        p_hat = _projected_gradient_descent(\n            S=case[\"S\"],\n            y=case[\"y\"],\n            w=case[\"w\"],\n            alpha=case[\"alpha\"],\n            K=case[\"K\"]\n        )\n        all_results.append(p_hat)\n\n    formatted_results = []\n    for p_vec in all_results:\n        formatted_p = [f\"{x:.4f}\" for x in p_vec]\n        formatted_results.append(f\"[{','.join(formatted_p)}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef _project_simplex(z):\n    \"\"\"\n    Projects a vector z onto the probability simplex.\n\n    This implementation uses the efficient O(K log K) algorithm.\n    \"\"\"\n    K = len(z)\n    u = np.sort(z)[::-1]\n    cssv = np.cumsum(u)\n    \n    # Find rho, the number of positive components in the projection\n    i = np.arange(K) + 1\n    # Condition: u_i > (1/i) * (cssv_i - 1)\n    # Using 0-indexed numpy arrays, this becomes u[i-1] > (1/i) * (cssv[i-1] - 1)\n    # rho is the largest i that satisfies this. np.where finds all such i.\n    rho_candidates = np.where(u > (cssv - 1) / i)[0]\n    \n    if len(rho_candidates) == 0:\n        # Fallback in case of numerical issues, should not be hit\n        rho = K\n    else:\n        rho = rho_candidates[-1] + 1\n\n    # Compute threshold theta\n    theta = (cssv[rho - 1] - 1) / rho\n    \n    # Apply soft-thresholding\n    p = np.maximum(z - theta, 0)\n    return p\n\n\ndef _projected_gradient_descent(S, y, w, alpha, K, max_iter=10000, tol=1e-9):\n    \"\"\"\n    Performs projected gradient descent to solve the constrained QP.\n    \"\"\"\n    # Initialize p at the center of the simplex\n    p = np.full(K, 1.0 / K)\n\n    # Pre-compute parts of the gradient and Hessian\n    W = np.diag(w)\n    ST_W = S.T @ W\n    ST_W_S = ST_W @ S\n    ST_W_y = ST_W @ y\n\n    # Compute Hessian and step size (eta)\n    Hessian = ST_W_S + alpha * np.identity(K)\n    # Lipschitz constant L is the maximum eigenvalue of the Hessian.\n    # The Hessian is symmetric, so use eigh for efficiency.\n    eigenvalues = scipy.linalg.eigh(Hessian, eigvals_only=True)\n    L = eigenvalues[-1]\n    eta = 1.0 / L\n\n    for i in range(max_iter):\n        # Gradient calculation: grad = S'W(Sp - y) + alpha*p\n        grad = ST_W_S @ p - ST_W_y + alpha * p\n        \n        # Update step\n        p_unconstrained = p - eta * grad\n        \n        # Projection onto the simplex\n        p_new = _project_simplex(p_unconstrained)\n        \n        # Check for convergence\n        if np.linalg.norm(p_new - p)  tol:\n            break\n            \n        p = p_new\n\n    # Final rounding is handled in the main solve() function\n    return p\n\nsolve()\n```", "id": "4386305"}, {"introduction": "Beyond identifying which cells are present, spatial transcriptomics allows us to investigate how they might communicate. This exercise focuses on quantifying and assessing the statistical significance of ligand-receptor co-expression, a proxy for potential cell-cell signaling pathways [@problem_id:4386274]. You will develop a distance-weighted interaction score and use a robust permutation-based testing framework to determine whether the observed spatial patterns of a ligand and its receptor are more structured than expected by chance.", "problem": "You are given spatially resolved single-cell measurements in the form of two vectors of gene expression for a ligand and its cognate receptor and a set of two-dimensional cell coordinates. Your task is to formalize a ligandâ€“receptor (LR) spatial co-expression score that (i) respects the biological principle that potential intercellular signaling diminishes with increasing intercellular distance, and (ii) is comparable across datasets by removing dependence on marginal expression scales. Then, derive a null distribution for the score using a spatial permutation strategy that preserves cell positions and marginal distributions, and compute an empirical two-sided significance value for the observed score.\n\nStarting from fundamental bases:\n- Spatial proximity is quantified by Euclidean distance. For two cells indexed by $i$ and $j$ with positions $\\mathbf{x}_i \\in \\mathbb{R}^2$ and $\\mathbf{x}_j \\in \\mathbb{R}^2$, the Euclidean distance is $d_{ij} = \\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2$.\n- Intercellular signaling probability decreases with distance, which is captured by an isotropic, monotonically decaying kernel. Use an exponential decay kernel given by $k(d) = \\exp(-d / \\lambda)$ with a specified decay length $\\lambda  0$ measured in micrometers, and exclude self-interactions by setting the self-weight at zero.\n- To ensure comparability across datasets, standardize each expression vector to zero mean and unit variance over cells.\n\nYour program must:\n- Construct a weight matrix $W$ with entries $W_{ij} = k(d_{ij})$ for $i \\neq j$ and $W_{ii} = 0$, and then normalize so that $\\sum_{i \\neq j} W_{ij} = 1$.\n- Given ligand expression $L_i$ and receptor expression $R_i$ across $N$ cells (with $i \\in \\{1,\\dots,N\\}$), compute standardized values $\\tilde{L}_i = (L_i - \\bar{L})/\\sigma_L$ and $\\tilde{R}_i = (R_i - \\bar{R})/\\sigma_R$, where $\\bar{L}$ and $\\bar{R}$ denote means and $\\sigma_L$ and $\\sigma_R$ denote standard deviations (if a standard deviation equals zero, treat the corresponding standardized vector as identically zero).\n- Define a spatial LR score by aggregating standardized cross-products over all ordered cell pairs with the distance-based weights. You must exclude self-pairs and ensure that shorter intercellular distances contribute more strongly than longer ones. The final scalar score must be unitless.\n- Generate a null distribution that preserves cell positions by independently permuting the assignment of ligand expression values across cells while holding receptor expression and positions fixed, repeating this $B$ times. Under this null, compute the empirical two-sided significance value as $(1 + \\#\\{b \\in \\{1,\\dots,B\\}: |S^{(b)}| \\ge |S^{\\text{obs}}|\\})/(B+1)$, where $S^{\\text{obs}}$ is the observed score and $S^{(b)}$ are the permuted scores. Use a fixed random seed so that repeated runs yield identical results.\n\nUnits:\n- Coordinates are in micrometers. The decay length $\\lambda$ is in micrometers. The spatial LR score and the empirical significance values are unitless.\n\nAngle units:\n- No angles are used in this problem.\n\nTest suite:\nFor each test case below, use the specified positions, expressions, and decay length $\\lambda$. Use $B = 5000$ permutations and a fixed random seed of $12345$.\n\n- Test case $1$ (separated ligand and receptor clusters; expected negative association):\n  - Number of cells $N = 6$.\n  - Coordinates $(x_i,y_i)$ in micrometers: $\\{(0,0),(2,1),(-1,-2),(50,0),(52,2),(49,-1)\\}$.\n  - Ligand expression $L = [8,7,6,1,1,1]$.\n  - Receptor expression $R = [1,1,1,7,8,6]$.\n  - Decay length $\\lambda = 20$ micrometers.\n\n- Test case $2$ (co-localized ligand and receptor; expected positive association):\n  - Number of cells $N = 6$.\n  - Coordinates as in test case $1$.\n  - Ligand expression $L = [8,7,6,1,1,1]$.\n  - Receptor expression $R = [5,4,6,1,1,1]$.\n  - Decay length $\\lambda = 20$ micrometers.\n\n- Test case $3$ (minimal edge case):\n  - Number of cells $N = 2$.\n  - Coordinates $(x_i,y_i)$ in micrometers: $\\{(0,0),(5,0)\\}$.\n  - Ligand expression $L = [10,0]$.\n  - Receptor expression $R = [0,8]$.\n  - Decay length $\\lambda = 10$ micrometers.\n\nOutput specification:\n- For each test case, compute the empirical two-sided significance value as a float in $[0,1]$ using the null distribution defined above with $B = 5000$ and seed $12345$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of test cases $1$, $2$, $3$. Each value must be rounded to exactly $6$ decimal places. For example, an acceptable output format is $[0.123456,0.500000,1.000000]$.", "solution": "The problem requires the development of a statistical method to test for spatial co-expression of a ligand-receptor pair. The solution involves defining a score, then using a permutation test to assess its statistical significance.\n\n**1. Spatial Weight Matrix Construction**\n\nThe influence of one cell on another is assumed to decay with distance. We first formalize this into a spatial weight matrix, $W$.\n\nGiven $N$ cells with 2D coordinates $\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_N$, we compute the pairwise Euclidean distance matrix $D$, where $D_{ij} = \\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2$.\n\nThe problem specifies an exponential decay kernel $k(d) = \\exp(-d / \\lambda)$ with decay length $\\lambda  0$. We construct an unnormalized weight matrix $W'$ where each element $W'_{ij}$ represents the potential interaction strength from cell $i$ to cell $j$. Self-interactions are excluded.\n$$\nW'_{ij} = \\begin{cases} \\exp(-D_{ij} / \\lambda)  \\text{if } i \\neq j \\\\ 0  \\text{if } i = j \\end{cases}\n$$\nTo ensure the total weight is constant across analyses, we normalize this matrix such that the sum of all its elements equals $1$. Specifically, the sum is over all non-diagonal elements.\n$$\nW_{ij} = \\frac{W'_{ij}}{\\sum_{k=1}^N \\sum_{l=1, l \\neq k}^N W'_{kl}}\n$$\nThis matrix $W$ is symmetric since $D_{ij} = D_{ji}$.\n\n**2. Expression Data Standardization**\n\nTo make the score comparable across different genes and datasets, we must remove the influence of the expression scales. We standardize the ligand expression vector $L = [L_1, \\dots, L_N]^T$ and receptor expression vector $R = [R_1, \\dots, R_N]^T$ to have a mean of $0$ and a standard deviation of $1$.\n\nFor an expression vector $V$, its mean $\\bar{V}$ and population standard deviation $\\sigma_V$ are:\n$$\n\\bar{V} = \\frac{1}{N} \\sum_{i=1}^N V_i, \\quad \\sigma_V = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (V_i - \\bar{V})^2}\n$$\nThe standardized expression value $\\tilde{V}_i$ is:\n$$\n\\tilde{V}_i = \\begin{cases} (V_i - \\bar{V}) / \\sigma_V  \\text{if } \\sigma_V  0 \\\\ 0  \\text{if } \\sigma_V = 0 \\end{cases}\n$$\nThis procedure is applied to obtain the standardized vectors $\\tilde{L}$ and $\\tilde{R}$.\n\n**3. Spatial Ligand-Receptor Score**\n\nThe spatial LR score, $S$, is defined as the weighted aggregation of cross-products of standardized expression values over all ordered cell pairs. The product $\\tilde{L}_i \\tilde{R}_j$ represents the potential signaling from cell $i$ (expressing ligand) to cell $j$ (expressing receptor). The score aggregates these directed interactions, weighted by their spatial proximity.\n$$\nS = \\sum_{i=1}^N \\sum_{j=1, j \\neq i}^N W_{ij} \\tilde{L}_i \\tilde{R}_j\n$$\nGiven that $W_{ii}=0$, we can express this compactly in matrix notation:\n$$\nS = \\sum_{i=1}^N \\sum_{j=1}^N \\tilde{L}_i W_{ij} \\tilde{R}_j = \\tilde{L}^T W \\tilde{R}\n$$\nThis score is a unitless scalar. A positive score suggests that cells with high ligand expression tend to be near cells with high receptor expression (or low near low), indicating spatial co-localization. A negative score suggests that cells with high ligand expression are near cells with low receptor expression, indicating spatial segregation.\n\n**4. Permutation-based Significance Testing**\n\nTo determine if the observed score, $S^{\\text{obs}}$, is statistically significant, we compare it to a null distribution. The null hypothesis is that there is no association between the spatial patterns of ligand and receptor expression.\n\nWe generate this null distribution by a permutation test. We hold the cell positions and the receptor expression vector $R$ fixed. We then randomly permute the ligand expression values among the cells. This procedure breaks the specific assignment of ligand expression values to cell locations while preserving the overall distribution of ligand expression and the spatial arrangement of cells.\n\nThe procedure is as follows:\n1.  Calculate the observed score $S^{\\text{obs}}$ using the original data $L$ and $R$.\n2.  Repeat for $b = 1, \\dots, B$:\n    a. Generate a permuted ligand vector $L^{(b)}$ by shuffling the elements of $L$.\n    b. Standardize $L^{(b)}$ to get $\\tilde{L}^{(b)}$. Note that the set of values in $\\tilde{L}^{(b)}$ is just a permutation of the values in $\\tilde{L}$.\n    c. Calculate the score for this permutation, $S^{(b)} = (\\tilde{L}^{(b)})^T W \\tilde{R}$.\n3.  The collection $\\{S^{(b)}\\}_{b=1}^B$ forms the empirical null distribution.\n\n**5. Empirical p-value Calculation**\n\nThe problem asks for a two-sided p-value. This assesses the probability of observing a score at least as extreme (in either positive or negative direction) as the observed score under the null hypothesis. We count how many permuted scores have an absolute value greater than or equal to the absolute value of the observed score.\n\nThe empirical p-value, $p$, is given by:\n$$\np = \\frac{1 + |\\{b \\in \\{1,\\dots,B\\}: |S^{(b)}| \\ge |S^{\\text{obs}}|\\}|}{B+1}\n$$\nThe addition of $1$ to the numerator and denominator is a standard practice to prevent p-values of $0$ and to account for the observed data point itself as part of the distribution.\n\nThis completes the formalization of the method. The implementation will follow these steps precisely.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef standardize_expression(vec):\n    \"\"\"\n    Standardizes a vector to have mean 0 and variance 1.\n    If std dev is 0, returns a vector of zeros.\n    \"\"\"\n    std_dev = np.std(vec)\n    if std_dev == 0:\n        return np.zeros_like(vec, dtype=float)\n    return (vec - np.mean(vec)) / std_dev\n\ndef compute_spatial_lr_pvalue(coords, L, R, lam, B, seed):\n    \"\"\"\n    Computes the spatial LR score and its empirical p-value.\n\n    Args:\n        coords (np.ndarray): N x 2 array of cell coordinates.\n        L (np.ndarray): N-element array of ligand expression.\n        R (np.ndarray): N-element array of receptor expression.\n        lam (float): Decay length for the exponential kernel.\n        B (int): Number of permutations.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        float: The empirical two-sided p-value.\n    \"\"\"\n    N = coords.shape[0]\n\n    # 1. Construct the normalized spatial weight matrix W\n    dist_matrix = cdist(coords, coords, 'euclidean')\n    \n    # Exponential decay kernel; W' in the write-up\n    W_unnormalized = np.exp(-dist_matrix / lam)\n    np.fill_diagonal(W_unnormalized, 0)\n    \n    # Normalize by the sum of all off-diagonal elements\n    total_weight = np.sum(W_unnormalized)\n    if total_weight == 0:\n        # If all cells are infinitely far apart, weights are 0.\n        # Score will be 0, p-value undefined.\n        # Problem constraints ensure this doesn't happen, but good practice.\n        return 1.0\n        \n    W = W_unnormalized / total_weight\n\n    # 2. Standardize expression vectors\n    L_std = standardize_expression(L)\n    R_std = standardize_expression(R)\n\n    # 3. Calculate the observed score\n    # S_obs = L_std.T @ W @ R_std\n    S_obs = np.sum(L_std[:, np.newaxis] * W * R_std[np.newaxis, :])\n\n\n    # 4. Permutation testing\n    rng = np.random.default_rng(seed)\n    permuted_scores = np.zeros(B)\n    \n    # Optimization: permutation of standardized values is faster\n    # than re-standardizing in each loop iteration.\n    L_std_permute_base = np.copy(L_std)\n\n    for b in range(B):\n        permuted_L_std = rng.permutation(L_std_permute_base)\n        # S_perm = permuted_L_std.T @ W @ R_std\n        S_perm = np.sum(permuted_L_std[:, np.newaxis] * W * R_std[np.newaxis, :])\n        permuted_scores[b] = S_perm\n\n    # 5. Compute the empirical two-sided p-value\n    abs_S_obs = np.abs(S_obs)\n    num_extreme_scores = np.sum(np.abs(permuted_scores) >= abs_S_obs)\n    \n    p_value = (1.0 + num_extreme_scores) / (1.0 + B)\n    \n    return p_value\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the spatial LR analysis on them.\n    \"\"\"\n    B = 5000\n    seed = 12345\n\n    test_cases = [\n        {\n            \"coords\": np.array([[0,0],[2,1],[-1,-2],[50,0],[52,2],[49,-1]]),\n            \"L\": np.array([8,7,6,1,1,1]),\n            \"R\": np.array([1,1,1,7,8,6]),\n            \"lam\": 20.0\n        },\n        {\n            \"coords\": np.array([[0,0],[2,1],[-1,-2],[50,0],[52,2],[49,-1]]),\n            \"L\": np.array([8,7,6,1,1,1]),\n            \"R\": np.array([5,4,6,1,1,1]),\n            \"lam\": 20.0\n        },\n        {\n            \"coords\": np.array([[0,0],[5,0]]),\n            \"L\": np.array([10,0]),\n            \"R\": np.array([0,8]),\n            \"lam\": 10.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        p_val = compute_spatial_lr_pvalue(\n            case[\"coords\"], case[\"L\"], case[\"R\"], case[\"lam\"], B, seed\n        )\n        results.append(p_val)\n\n    # Format output as specified\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4386274"}]}