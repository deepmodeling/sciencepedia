{"hands_on_practices": [{"introduction": "Once a biological network is constructed, the first step is to quantify its structural properties to uncover biological meaning. Metrics like degree centrality and the local clustering coefficient allow us to move beyond a simple visual diagram to identify key players and functional modules. This practice provides a concrete exercise in calculating these fundamental topological features from an adjacency matrix and interpreting them in a translational context, distinguishing dense protein complexes from inter-module bridging proteins [@problem_id:5002394].", "problem": "A disease-specific Protein-Protein Interaction (PPI) network is constructed from a cohort in translational medicine by integrating physical binding data and condition-specific co-expression. The resulting undirected, unweighted network among $6$ gene products is represented by the symmetric adjacency matrix $A \\in \\{0,1\\}^{6 \\times 6}$ with zero diagonal:\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0  1  1  0  0  0\\\\\n1  0  1  0  0  0\\\\\n1  1  0  1  0  0\\\\\n0  0  1  0  1  1\\\\\n0  0  0  1  0  1\\\\\n0  0  0  1  1  0\n\\end{pmatrix}.\n$$\nNodes are indexed by $i \\in \\{1,2,3,4,5,6\\}$. Using core graph-theoretic definitions appropriate for undirected simple graphs, perform the following:\n- For each node $i$, compute the normalized degree centrality $C_{D}(i)$ based on $k_{i}$, the degree of node $i$, using the normalization by the maximum possible degree in a graph with $n=6$ nodes.\n- For each node $i$, compute the local clustering coefficient $C_{i}$ defined as the fraction of realized connections among the neighbors of $i$ out of all possible connections among those neighbors. Adopt the convention that $C_{i}=0$ when $k_{i}2$.\n- Based on your calculations, interpret which nodes likely lie in dense functional modules and which nodes act as inter-module connectors in the sense of translational network biology, where high local clustering suggests membership in a functional complex or pathway, and lower local clustering in high-degree nodes can suggest bridging roles.\n\nFinally, report the network average of the local clustering coefficients $\\bar{C} = \\frac{1}{n}\\sum_{i=1}^{n} C_{i}$ as a decimal rounded to $4$ significant figures. No units are required for the final reported value.", "solution": "The problem is subjected to validation and is deemed valid. It is scientifically grounded in graph theory and its application to biological network analysis, is well-posed with all necessary information provided, and is expressed in objective, formal language. We may therefore proceed with the solution.\n\nThe problem requires a multi-step analysis of a given biological network represented by an adjacency matrix $A$. The network has $n=6$ nodes.\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0  1  1  0  0  0\\\\\n1  0  1  0  0  0\\\\\n1  1  0  1  0  0\\\\\n0  0  1  0  1  1\\\\\n0  0  0  1  0  1\\\\\n0  0  0  1  1  0\n\\end{pmatrix}\n$$\nThe analysis involves computing node-specific metrics, interpreting their biological significance, and calculating a global network property.\n\nFirst, we compute the degree $k_i$ for each node $i$, which is the number of connections it has. For an undirected graph represented by a symmetric adjacency matrix $A$, the degree of node $i$ is the sum of the elements in the $i$-th row (or column).\n$k_i = \\sum_{j=1}^{n} A_{ij}$\n- $k_1 = 1+1 = 2$\n- $k_2 = 1+1 = 2$\n- $k_3 = 1+1+1 = 3$\n- $k_4 = 1+1+1 = 3$\n- $k_5 = 1+1 = 2$\n- $k_6 = 1+1 = 2$\n\nNext, we compute the normalized degree centrality $C_D(i)$ for each node. This is defined as the node's degree $k_i$ divided by the maximum possible degree in a simple graph of $n$ nodes, which is $n-1$. Here, $n=6$, so the maximum possible degree is $n-1 = 5$.\n$C_D(i) = \\frac{k_i}{n-1}$\n- $C_D(1) = \\frac{k_1}{5} = \\frac{2}{5} = 0.4$\n- $C_D(2) = \\frac{k_2}{5} = \\frac{2}{5} = 0.4$\n- $C_D(3) = \\frac{k_3}{5} = \\frac{3}{5} = 0.6$\n- $C_D(4) = \\frac{k_4}{5} = \\frac{3}{5} = 0.6$\n- $C_D(5) = \\frac{k_5}{5} = \\frac{2}{5} = 0.4$\n- $C_D(6) = \\frac{k_6}{5} = \\frac{2}{5} = 0.4$\n\nNow, we compute the local clustering coefficient $C_i$ for each node. This coefficient measures the density of connections among a node's neighbors. It is defined as the number of existing edges between the neighbors of node $i$, denoted $E_i$, divided by the total number of possible edges between them. For an undirected graph, this is given by the formula:\n$C_i = \\frac{2 E_i}{k_i (k_i - 1)}$\nThe problem specifies the convention that $C_i = 0$ if $k_i  2$. Since all nodes have a degree of at least $2$, this convention is not invoked.\n\n- **Node 1**: $k_1=2$. Neighbors are $N(1) = \\{2, 3\\}$. There is $E_1=1$ edge between them (the edge $(2,3)$ since $A_{23}=1$).\n$C_1 = \\frac{2 \\times 1}{2 \\times (2 - 1)} = \\frac{2}{2} = 1$\n\n- **Node 2**: $k_2=2$. Neighbors are $N(2) = \\{1, 3\\}$. There is $E_2=1$ edge between them (the edge $(1,3)$ since $A_{13}=1$).\n$C_2 = \\frac{2 \\times 1}{2 \\times (2 - 1)} = \\frac{2}{2} = 1$\n\n- **Node 3**: $k_3=3$. Neighbors are $N(3) = \\{1, 2, 4\\}$. The possible edges are $(1,2)$, $(1,4)$, and $(2,4)$. We check the adjacency matrix: $A_{12}=1$, $A_{14}=0$, $A_{24}=0$. Thus, there is only $E_3=1$ edge among the neighbors.\n$C_3 = \\frac{2 \\times 1}{3 \\times (3 - 1)} = \\frac{2}{6} = \\frac{1}{3}$\n\n- **Node 4**: $k_4=3$. Neighbors are $N(4) = \\{3, 5, 6\\}$. The possible edges are $(3,5)$, $(3,6)$, and $(5,6)$. We check the adjacency matrix: $A_{35}=0$, $A_{36}=0$, $A_{56}=1$. Thus, there is only $E_4=1$ edge among the neighbors.\n$C_4 = \\frac{2 \\times 1}{3 \\times (3 - 1)} = \\frac{2}{6} = \\frac{1}{3}$\n\n- **Node 5**: $k_5=2$. Neighbors are $N(5) = \\{4, 6\\}$. There is $E_5=1$ edge between them (the edge $(4,6)$ since $A_{46}=1$).\n$C_5 = \\frac{2 \\times 1}{2 \\times (2 - 1)} = \\frac{2}{2} = 1$\n\n- **Node 6**: $k_6=2$. Neighbors are $N(6) = \\{4, 5\\}$. There is $E_6=1$ edge between them (the edge $(4,5)$ since $A_{45}=1$).\n$C_6 = \\frac{2 \\times 1}{2 \\times (2 - 1)} = \\frac{2}{2} = 1$\n\nBased on these calculations, we can interpret the roles of the nodes.\n- **Nodes 1, 2, 5, 6**: These nodes have a local clustering coefficient of $C_i = 1$. This maximal value indicates that their neighbors are fully connected, forming a clique. These nodes are deeply embedded within dense functional modules. Specifically, nodes $\\{1, 2, 3\\}$ form a triangle, and nodes $\\{4, 5, 6\\}$ form another triangle. Nodes $1$ and $2$ belong to the first module, and nodes $5$ and $6$ belong to the second.\n- **Nodes 3, 4**: These nodes have the highest degree centrality in the network ($C_D(3) = C_D(4) = 0.6$). However, their local clustering coefficients are relatively low ($C_3 = C_4 = 1/3$). This pattern—high degree and low local clustering—is characteristic of nodes that act as \"bridges\" or inter-module connectors. Node $3$ is part of the first functional module but also connects to node $4$. Similarly, node $4$ is part of the second module and connects to node $3$. The edge $(3,4)$ is the sole connection linking the two dense triangular modules, confirming the bridging role of nodes $3$ and $4$.\n\nFinally, we compute the network average of the local clustering coefficients, $\\bar{C}$.\n$\\bar{C} = \\frac{1}{n}\\sum_{i=1}^{n} C_{i}$\n$\\bar{C} = \\frac{1}{6} (C_1 + C_2 + C_3 + C_4 + C_5 + C_6)$\n$\\bar{C} = \\frac{1}{6} \\left(1 + 1 + \\frac{1}{3} + \\frac{1}{3} + 1 + 1\\right)$\n$\\bar{C} = \\frac{1}{6} \\left(4 + \\frac{2}{3}\\right)$\n$\\bar{C} = \\frac{1}{6} \\left(\\frac{12}{3} + \\frac{2}{3}\\right) = \\frac{1}{6} \\left(\\frac{14}{3}\\right) = \\frac{14}{18} = \\frac{7}{9}$\n\nTo report this value as a decimal rounded to $4$ significant figures:\n$\\frac{7}{9} \\approx 0.777777...$\nRounding to $4$ significant figures gives $0.7778$.", "answer": "$$\\boxed{0.7778}$$", "id": "5002394"}, {"introduction": "Beyond describing a network's static structure, we can use it to make powerful predictions. A core principle in translational bioinformatics is 'guilt-by-association': genes functionally related to a disease are often located near known disease genes in a protein-protein interaction network. This practice guides you through implementing the Random Walk with Restart (RWR) algorithm, a sophisticated method that formalizes this principle to rank candidate genes based on their network proximity to a set of known disease 'seeds' [@problem_id:5002343].", "problem": "You are given small protein-protein interaction (PPI) networks and disease seed gene sets. Construct a principled, reproducible Random Walk with Restart (RWR) method to prioritize candidate genes by steady-state probability. Your program must implement the following, derived from the foundational theory of discrete-time Markov chains and stochastic matrices.\n\nDefinitions and foundational base:\n- A PPI network over $n$ genes is represented by a symmetric, nonnegative adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$ with entries $A_{ij} \\ge 0$ and $A_{ij} = A_{ji}$. The diagonal may be zero. The nonzero entries represent undirected interaction strengths.\n- A column-stochastic matrix $W \\in \\mathbb{R}^{n \\times n}$ satisfies $\\sum_{i=1}^{n} W_{ij} = 1$ for each column $j$ and $W_{ij} \\ge 0$. Such a matrix defines a Markov chain transition operator acting on a probability vector $p \\in \\mathbb{R}^{n}$ via the update $p^{(t+1)} = W p^{(t)}$, preserving $\\sum_{i=1}^{n} p^{(t)}_i = 1$ and $p^{(t)}_i \\ge 0$.\n- Given $A$, construct $W$ by column-normalization: for each column $j$ with column sum $\\sum_{i=1}^{n} A_{ij}  0$, set $W_{ij} = A_{ij} / \\sum_{k=1}^{n} A_{kj}$. For any column $j$ with $\\sum_{i=1}^{n} A_{ij} = 0$ (an isolated node), set $W_{jj} = 1$ and $W_{ij} = 0$ for $i \\ne j$; this adds a self-loop that preserves probability mass at isolated nodes and ensures $W$ is column-stochastic.\n- Let the seed set be $S \\subset \\{0,1,\\dots,n-1\\}$. Define the seed distribution vector $s \\in \\mathbb{R}^{n}$ by $s_i = 1/|S|$ if $i \\in S$ and $s_i = 0$ otherwise, so that $\\sum_{i=1}^{n} s_i = 1$ and $s_i \\ge 0$.\n- Consider a Markov process on the network with restart parameter $\\gamma \\in (0,1]$ such that at each time step, with probability $\\gamma$ the walk restarts to the seeds according to $s$, and with probability $(1 - \\gamma)$ it follows a single network transition according to $W$. From the foundational definition of stationary distributions of Markov chains with teleportation, the unique steady-state distribution $p^\\star \\in \\mathbb{R}^{n}$ satisfies the fixed point equation $p^\\star = (1 - \\gamma) \\, W \\, p^\\star + \\gamma \\, s$ and is guaranteed to exist uniquely because the spectral radius of $(1 - \\gamma) \\, W$ is strictly less than $1$ when $\\gamma \\in (0,1]$ and $W$ is column-stochastic.\n\nProgram requirements:\n- For each test case below, construct $W$ from $A$ as specified, form $s$ from the provided seed set, and compute the unique $p^\\star$ exactly by solving the linear system implied by the fixed point condition without using iterative heuristics. Rank all non-seed nodes in descending order by $p^\\star_i$; break ties by ascending index $i$. Return the top-$k$ ranked non-seed indices for each case using zero-based indexing.\n- If all non-seed nodes have identical scores (for example, exactly $0$), use the tie-breaking rule to select indices in ascending order.\n- Numerical stability: if numerical round-off produces tiny negative entries in $p^\\star$, treat them as $0$ by clipping, and then renormalize to ensure $\\sum_{i=1}^{n} p^\\star_i = 1$.\n\nTest suite (use exactly these cases):\n1) Case 1 (happy path, connected, unweighted):\n   - $n = 6$; adjacency matrix $A \\in \\mathbb{R}^{6 \\times 6}$ with entries\n     $$A = \\begin{bmatrix}\n     0  1  0  0  0  1 \\\\\n     1  0  1  1  0  0 \\\\\n     0  1  0  1  0  0 \\\\\n     0  1  1  0  1  0 \\\\\n     0  0  0  1  0  1 \\\\\n     1  0  0  0  1  0\n     \\end{bmatrix}.$$\n   - Seed set $S = \\{0, 3\\}$.\n   - Restart $\\gamma = 0.35$.\n   - Top-$k$: $k = 3$.\n\n2) Case 2 (edge case, isolated seed):\n   - $n = 5$; adjacency matrix $A \\in \\mathbb{R}^{5 \\times 5}$ with entries\n     $$A = \\begin{bmatrix}\n     0  1  0  0  0 \\\\\n     1  0  1  0  0 \\\\\n     0  1  0  1  0 \\\\\n     0  0  1  0  0 \\\\\n     0  0  0  0  0\n     \\end{bmatrix}.$$\n   - Seed set $S = \\{4\\}$.\n   - Restart $\\gamma = 0.7$.\n   - Top-$k$: $k = 2$.\n\n3) Case 3 (weighted network, strong restart):\n   - $n = 7$; adjacency matrix $A \\in \\mathbb{R}^{7 \\times 7}$ with entries\n     $$A = \\begin{bmatrix}\n     0  2.0  1.0  0  0  0  0 \\\\\n     2.0  0  0  1.5  0  0  0 \\\\\n     1.0  0  0  0.5  0  0  1.0 \\\\\n     0  1.5  0.5  0  2.0  0  0 \\\\\n     0  0  0  2.0  0  1.0  0 \\\\\n     0  0  0  0  1.0  0  0.5 \\\\\n     0  0  1.0  0  0  0.5  0\n     \\end{bmatrix}.$$\n   - Seed set $S = \\{1, 4\\}$.\n   - Restart $\\gamma = 0.9$.\n   - Top-$k$: $k = 3$.\n\n4) Case 4 (boundary, full restart):\n   - Use the same $A$ as in Case $1$.\n   - Seed set $S = \\{0, 3\\}$.\n   - Restart $\\gamma = 1.0$.\n   - Top-$k$: $k = 2$.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a list of top-$k$ indices for the corresponding test case. For example: \"[$[i_{1,1},i_{1,2},\\dots]$, $[i_{2,1},\\dots]$, $[i_{3,1},\\dots]$, $[i_{4,1},\\dots]$]\". The final output must contain no spaces.", "solution": "The problem requires the implementation of the Random Walk with Restart (RWR) algorithm to prioritize candidate genes in a protein-protein interaction (PPI) network. The method leverages the mathematical framework of discrete-time Markov chains to quantify the proximity of network nodes to a predefined set of \"seed\" genes associated with a disease. The core of the task is to compute the unique steady-state probability distribution of this process and use it to rank non-seed genes.\n\nThe foundation of the model is a Markov process on the graph representing the PPI network. A network of $n$ genes is described by an adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$, where $A_{ij} \\ge 0$ represents the strength of the interaction between gene $i$ and gene $j$. Since interactions are undirected, $A$ is symmetric, i.e., $A_{ij} = A_{ji}$.\n\nFirst, we construct a column-stochastic transition matrix $W \\in \\mathbb{R}^{n \\times n}$ from $A$. This matrix $W$ defines the transition probabilities of a random walk on the network. A walker at node $j$ will move to node $i$ with probability $W_{ij}$. To ensure that the total probability of moving from node $j$ to any other node is $1$, each column of $W$ must sum to $1$. This is achieved by normalizing each column of $A$ by its sum:\n$$ W_{ij} = \\frac{A_{ij}}{\\sum_{k=1}^{n} A_{kj}} $$\nThis normalization is performed for every column $j$ whose sum $\\sum_{k=1}^{n} A_{kj}$ is greater than $0$. If a column $j$ has a sum of $0$, it corresponds to an isolated node with no outgoing connections. To conserve probability, such a node is modeled as a sink with a self-loop, meaning $W_{jj} = 1$ and $W_{ij} = 0$ for $i \\neq j$.\n\nThe RWR process modifies the standard random walk by introducing a \"restart\" mechanism. At each step, the walker has two choices:\n$1$. With probability $1 - \\gamma$, it follows a transition according to the matrix $W$.\n$2$. With probability $\\gamma$, it teleports back to one of the seed nodes.\n\nThe restart probability $\\gamma \\in (0, 1]$ is a crucial parameter. The distribution of restarts is governed by the seed vector $s \\in \\mathbb{R}^n$. Given a set of seed genes $S$, $s$ is defined as a uniform distribution over these genes: $s_i = 1/|S|$ if gene $i \\in S$, and $s_i = 0$ otherwise.\n\nLet $p^{(t)} \\in \\mathbb{R}^n$ be the probability distribution vector across all $n$ genes at step $t$, where $p_i^{(t)}$ is the probability of the walker being at node $i$. The distribution at the next step, $p^{(t+1)}$, is given by the recursive formula:\n$$ p^{(t+1)} = (1 - \\gamma) W p^{(t)} + \\gamma s $$\nThe process eventually converges to a unique steady-state distribution $p^\\star$, which no longer changes with further steps. This $p^\\star$ satisfies the fixed-point equation:\n$$ p^\\star = (1 - \\gamma) W p^\\star + \\gamma s $$\nThe components $p^\\star_i$ of this vector represent the long-term probability of finding the walker at node $i$ and are used as the scores for gene prioritization. A higher score implies stronger functional relevance to the seed set.\n\nTo find $p^\\star$ exactly, as required by the problem, we do not iterate the recurrence relation. Instead, we rearrange the fixed-point equation into a system of linear equations:\n$$ p^\\star - (1 - \\gamma) W p^\\star = \\gamma s $$\n$$ (I - (1 - \\gamma) W) p^\\star = \\gamma s $$\nwhere $I$ is the $n \\times n$ identity matrix. This is a standard linear system of the form $M x = b$, where the matrix $M = I - (1 - \\gamma) W$, the unknown vector is $x = p^\\star$, and the constant vector is $b = \\gamma s$. As established in the theory of Markov chains with teleportation, for any $\\gamma \\in (0,1]$, the matrix $M$ is guaranteed to be invertible, ensuring a unique solution for $p^\\star$. This solution can be computed efficiently using standard linear algebra solvers. The special case where $\\gamma = 1$ leads to $p^\\star = s$, as the walker always restarts and never traverses the network.\n\nThe algorithm to solve the problem is as follows:\n$1$. For a given adjacency matrix $A$, construct the transition matrix $W$ by normalizing the columns of $A$. For any column that sums to $0$, set the corresponding diagonal element of $W$ to $1$.\n$2$. For a given seed set $S$, construct the seed vector $s$ with uniform probabilities on the seed nodes.\n$3$. Form the matrix $M = I - (1 - \\gamma) W$ and the vector $b = \\gamma s$. For the boundary case $\\gamma=1$, $p^{\\star}$ is simply $s$.\n$4$. Solve the linear system $M p^\\star = b$ to find the steady-state vector $p^\\star$.\n$5$. For numerical robustness, any small negative values in the computed $p^\\star$ resulting from floating-point inaccuracies are clipped to $0$. The vector is then renormalized to ensure its elements sum to $1$.\n$6$. Identify all non-seed nodes. Create a list of pairs $(p^\\star_i, i)$ for each non-seed node $i$.\n$7$. Sort this list in descending order of the score $p^\\star_i$. Ties are broken by sorting in ascending order of the node index $i$.\n$8$. The final result is the list of the first $k$ indices from the sorted list.\n\nThis principled approach provides a reproducible and mathematically rigorous method for gene prioritization, directly addressing the problem as stated. It correctly handles edge cases such as isolated nodes and the boundary value of the restart parameter $\\gamma$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the solving of all test cases.\n    \"\"\"\n\n    def _solve_rwr(A, S, gamma, k):\n        \"\"\"\n        Computes the Random Walk with Restart top-k gene ranking.\n\n        Args:\n            A (np.ndarray): The adjacency matrix of the network.\n            S (set): A set of integer indices for the seed nodes.\n            gamma (float): The restart probability.\n            k (int): The number of top non-seed nodes to return.\n\n        Returns:\n            list: A list of the top-k ranked non-seed node indices.\n        \"\"\"\n        n = A.shape[0]\n\n        # Step 1: Construct the column-stochastic transition matrix W\n        W = np.zeros_like(A, dtype=float)\n        col_sums = A.sum(axis=0)\n        \n        non_zero_sum_cols = col_sums > 1e-9 # Use a tolerance for floating point\n        zero_sum_cols = ~non_zero_sum_cols\n\n        # Normalize columns with non-zero sum\n        W[:, non_zero_sum_cols] = A[:, non_zero_sum_cols] / col_sums[non_zero_sum_cols]\n\n        # Handle isolated nodes (columns with zero sum)\n        for j in np.where(zero_sum_cols)[0]:\n            W[j, j] = 1.0\n\n        # Step 2: Construct the seed vector s\n        s = np.zeros(n)\n        if len(S) > 0:\n            s[list(S)] = 1.0 / len(S)\n\n        # Step 3  4: Solve the linear system (I - (1-gamma)W)p* = gamma*s\n        if gamma == 1.0:\n            p_star = s\n        else:\n            M = np.identity(n) - (1 - gamma) * W\n            b = gamma * s\n            p_star = np.linalg.solve(M, b)\n\n        # Step 5: Numerical stability post-processing\n        p_star = np.clip(p_star, a_min=0, a_max=None)\n        p_sum = p_star.sum()\n        if p_sum > 1e-9: # Avoid division by zero\n            p_star /= p_sum\n\n        # Step 6: Rank non-seed nodes\n        non_seed_indices = [i for i in range(n) if i not in S]\n        \n        if not non_seed_indices:\n            return []\n\n        candidates = [(p_star[i], i) for i in non_seed_indices]\n\n        # Step 7: Sort candidates: primary key score (desc), secondary key index (asc)\n        candidates.sort(key=lambda x: (-x[0], x[1]))\n        \n        # Step 8: Extract top-k indices\n        top_k_indices = [idx for score, idx in candidates[:k]]\n        \n        return top_k_indices\n\n    # Test suite definition\n    case1_A = np.array([\n        [0, 1, 0, 0, 0, 1],\n        [1, 0, 1, 1, 0, 0],\n        [0, 1, 0, 1, 0, 0],\n        [0, 1, 1, 0, 1, 0],\n        [0, 0, 0, 1, 0, 1],\n        [1, 0, 0, 0, 1, 0]\n    ], dtype=float)\n    case1_S = {0, 3}\n    case1_gamma = 0.35\n    case1_k = 3\n\n    case2_A = np.array([\n        [0, 1, 0, 0, 0],\n        [1, 0, 1, 0, 0],\n        [0, 1, 0, 1, 0],\n        [0, 0, 1, 0, 0],\n        [0, 0, 0, 0, 0]\n    ], dtype=float)\n    case2_S = {4}\n    case2_gamma = 0.7\n    case2_k = 2\n\n    case3_A = np.array([\n        [0. , 2. , 1. , 0. , 0. , 0. , 0. ],\n        [2. , 0. , 0. , 1.5, 0. , 0. , 0. ],\n        [1. , 0. , 0. , 0.5, 0. , 0. , 1. ],\n        [0. , 1.5, 0.5, 0. , 2. , 0. , 0. ],\n        [0. , 0. , 0. , 2. , 0. , 1. , 0. ],\n        [0. , 0. , 0. , 0. , 1. , 0. , 0.5],\n        [0. , 0. , 1. , 0. , 0. , 0.5, 0. ]\n    ], dtype=float)\n    case3_S = {1, 4}\n    case3_gamma = 0.9\n    case3_k = 3\n\n    case4_A = case1_A.copy()\n    case4_S = {0, 3}\n    case4_gamma = 1.0\n    case4_k = 2\n    \n    test_cases = [\n        (case1_A, case1_S, case1_gamma, case1_k),\n        (case2_A, case2_S, case2_gamma, case2_k),\n        (case3_A, case3_S, case3_gamma, case3_k),\n        (case4_A, case4_S, case4_gamma, case4_k)\n    ]\n    \n    all_results = []\n    for A, S, gamma, k in test_cases:\n        result = _solve_rwr(A, S, gamma, k)\n        all_results.append(result)\n\n    # Format the final output string according to the specification (no spaces)\n    result_strings = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "5002343"}, {"introduction": "A central goal of translational medicine is to understand how biological processes are altered in disease or in response to therapy. Differential network analysis provides a powerful framework for this by identifying changes in the 'wiring' of molecular networks between two conditions. This practice implements a rigorous statistical pipeline to compare two networks, test for significant changes in connection strength, and pinpoint the specific interactions that are gained, lost, or altered, offering deep mechanistic insights [@problem_id:5002392].", "problem": "You are given two condition-specific covariance matrices for the same set of biological variables (for example, gene expression levels) measured under two conditions that separately follow a multivariate normal model. Under the Gaussian graphical model, the nonzero entries of the precision matrix (the inverse covariance matrix) encode conditional dependencies that define the edges of the network. Your task is to construct a differential network that quantifies changes in conditional dependencies between the two conditions and to statistically identify the edges whose conditional dependence differs significantly between conditions using an asymptotic test on partial correlations with multiple-testing correction.\n\nFundamental base and definitions:\n- Assume that under each condition, the data are drawn independently from a $p$-variate normal distribution $\\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$ with covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{p \\times p}$ that is symmetric positive definite. The precision matrix is $\\boldsymbol{\\Omega} = \\boldsymbol{\\Sigma}^{-1}$.\n- The partial correlation between variables $i$ and $j$ controlling for all other variables in a Gaussian model is defined by $\\rho_{ij} = -\\dfrac{\\Omega_{ij}}{\\sqrt{\\Omega_{ii} \\, \\Omega_{jj}}}$ for $i \\neq j$, with $\\rho_{ii} = 1$.\n- For a sample of size $n$ from a $p$-variate normal distribution, the Fisher $z$-transform of an estimated partial correlation $r_{ij}$, defined as $z = \\dfrac{1}{2}\\ln\\!\\left(\\dfrac{1+r_{ij}}{1-r_{ij}}\\right)$, is approximately normal with variance $\\operatorname{Var}(z) \\approx \\dfrac{1}{n - p - 1}$ when controlling for $p-2$ variables, provided $n  p + 1$. Under two independent conditions with sample sizes $n_1$ and $n_2$, the difference of Fisher $z$-transforms $z_2 - z_1$ for the same edge $(i,j)$ is approximately normal with variance $\\dfrac{1}{n_1 - p - 1} + \\dfrac{1}{n_2 - p - 1}$ under the null hypothesis $H_0: \\rho_{ij}^{(1)} = \\rho_{ij}^{(2)}$.\n\nWhat to compute:\n- For each test case, you will be given two covariance matrices $\\boldsymbol{\\Sigma}_1$ and $\\boldsymbol{\\Sigma}_2$ of dimension $p \\times p$ (both symmetric positive definite), the sample sizes $n_1$ and $n_2$ for the two conditions, and a nominal false discovery rate level $\\alpha$ (expressed as a decimal, not a percentage).\n- Compute the precision matrices $\\boldsymbol{\\Omega}_1 = \\boldsymbol{\\Sigma}_1^{-1}$ and $\\boldsymbol{\\Omega}_2 = \\boldsymbol{\\Sigma}_2^{-1}$.\n- Compute the corresponding partial correlation matrices $\\mathbf{P}_1$ and $\\mathbf{P}_2$ with entries $[\\mathbf{P}_k]_{ij} = -\\dfrac{[\\boldsymbol{\\Omega}_k]_{ij}}{\\sqrt{[\\boldsymbol{\\Omega}_k]_{ii} \\, [\\boldsymbol{\\Omega}_k]_{jj}}}$ for $i \\neq j$ and $[\\mathbf{P}_k]_{ii} = 1$ for $k \\in \\{1,2\\}$.\n- Define the differential network as the matrix $\\mathbf{D} = \\mathbf{P}_2 - \\mathbf{P}_1$.\n- For each unordered pair $(i,j)$ with $0 \\le i  j \\le p-1$, test $H_0: [\\mathbf{P}_1]_{ij} = [\\mathbf{P}_2]_{ij}$ using the difference of Fisher $z$-transforms. Compute the two-sided $p$-value from the standard normal distribution using the statistic\n$$\nZ_{ij} = \\dfrac{z([\\mathbf{P}_2]_{ij}) - z([\\mathbf{P}_1]_{ij})}{\\sqrt{\\dfrac{1}{n_1 - p - 1} + \\dfrac{1}{n_2 - p - 1}}}\n\\quad \\text{where} \\quad\nz(r) = \\dfrac{1}{2}\\ln\\!\\left(\\dfrac{1+r}{1-r}\\right).\n$$\n- Apply the Benjamini–Hochberg procedure at level $\\alpha$ across all $\\dfrac{p(p-1)}{2}$ edges to control the false discovery rate and determine which edges show significant change.\n\nImportant implementation and output requirements:\n- Use zero-based indexing for vertices.\n- Encode an edge $(i,j)$ with $ij$ as a single integer linear index $L = i \\cdot p + j$ (row-major indexing of the full $p \\times p$ matrix).\n- For each test case, output the list of linear indices $L$ corresponding to edges declared significant by the Benjamini–Hochberg procedure, sorted in ascending order.\n- Final output format: Your program should produce a single line of output containing a list whose elements correspond to the test cases, where each element is itself a list of integers formatted without spaces. For example, a valid output could look like $[[],[1,7,13],[]]$.\n\nTest suite:\nProvide solutions for the following four test cases. In each case, the covariance matrices are defined implicitly via inverses of explicitly specified precision matrices to ensure positive definiteness. All indices are zero-based.\n\n- Test case $1$ (happy path, clear single-edge change):\n  - Dimension: $p = 4$.\n  - Precision matrices:\n    - $\\boldsymbol{\\Omega}_1$ has diagonal entries all $1$, with $[\\boldsymbol{\\Omega}_1]_{0,1} = [\\boldsymbol{\\Omega}_1]_{1,0} = 0.3$, and all other off-diagonal entries $0$.\n    - $\\boldsymbol{\\Omega}_2$ is the same as $\\boldsymbol{\\Omega}_1$ except $[\\boldsymbol{\\Omega}_2]_{0,1} = [\\boldsymbol{\\Omega}_2]_{1,0} = 0.6$.\n  - Covariances: $\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Omega}_1^{-1}$, $\\boldsymbol{\\Sigma}_2 = \\boldsymbol{\\Omega}_2^{-1}$.\n  - Sample sizes: $n_1 = 500$, $n_2 = 500$.\n  - False discovery rate: $\\alpha = 0.05$.\n\n- Test case $2$ (near-null small change, likely non-significant):\n  - Dimension: $p = 4$.\n  - Precision matrices:\n    - $\\boldsymbol{\\Omega}_1$ as in test case $1$ with $[\\boldsymbol{\\Omega}_1]_{0,1} = [\\boldsymbol{\\Omega}_1]_{1,0} = 0.3$.\n    - $\\boldsymbol{\\Omega}_2$ with $[\\boldsymbol{\\Omega}_2]_{0,1} = [\\boldsymbol{\\Omega}_2]_{1,0} = 0.32$, all other entries as in $\\boldsymbol{\\Omega}_1$.\n  - Covariances: $\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Omega}_1^{-1}$, $\\boldsymbol{\\Sigma}_2 = \\boldsymbol{\\Omega}_2^{-1}$.\n  - Sample sizes: $n_1 = 300$, $n_2 = 300$.\n  - False discovery rate: $\\alpha = 0.05$.\n\n- Test case $3$ (multiple edges change with varying directions and magnitudes):\n  - Dimension: $p = 5$.\n  - Precision matrices:\n    - $\\boldsymbol{\\Omega}_1$ has diagonal entries all $1$, with nonzero off-diagonals $[\\boldsymbol{\\Omega}_1]_{0,1} = [\\boldsymbol{\\Omega}_1]_{1,0} = 0.35$, $[\\boldsymbol{\\Omega}_1]_{2,3} = [\\boldsymbol{\\Omega}_1]_{3,2} = -0.25$, and $[\\boldsymbol{\\Omega}_1]_{3,4} = [\\boldsymbol{\\Omega}_1]_{4,3} = 0.2$; all other off-diagonals are $0$.\n    - $\\boldsymbol{\\Omega}_2$ has diagonal entries all $1$, with nonzero off-diagonals $[\\boldsymbol{\\Omega}_2]_{0,1} = [\\boldsymbol{\\Omega}_2]_{1,0} = 0.0$, $[\\boldsymbol{\\Omega}_2]_{2,3} = [\\boldsymbol{\\Omega}_2]_{3,2} = -0.6$, and $[\\boldsymbol{\\Omega}_2]_{3,4} = [\\boldsymbol{\\Omega}_2]_{4,3} = -0.2$; all other off-diagonals are $0$.\n  - Covariances: $\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Omega}_1^{-1}$, $\\boldsymbol{\\Sigma}_2 = \\boldsymbol{\\Omega}_2^{-1}$.\n  - Sample sizes: $n_1 = 1000$, $n_2 = 1000$.\n  - False discovery rate: $\\alpha = 0.05$.\n\n- Test case $4$ (no change; identical networks and covariances):\n  - Dimension: $p = 3$.\n  - Precision matrices:\n    - $\\boldsymbol{\\Omega}_1$ has diagonal entries all $1$, with nonzero off-diagonals $[\\boldsymbol{\\Omega}_1]_{0,1} = [\\boldsymbol{\\Omega}_1]_{1,0} = 0.2$ and $[\\boldsymbol{\\Omega}_1]_{1,2} = [\\boldsymbol{\\Omega}_1]_{2,1} = -0.15$; all other off-diagonals are $0$.\n    - $\\boldsymbol{\\Omega}_2 = \\boldsymbol{\\Omega}_1$.\n  - Covariances: $\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Omega}_1^{-1}$, $\\boldsymbol{\\Sigma}_2 = \\boldsymbol{\\Omega}_2^{-1}$.\n  - Sample sizes: $n_1 = 400$, $n_2 = 600$.\n  - False discovery rate: $\\alpha = 0.05$.\n\nYour program must implement the above logic exactly for these four test cases and print a single line containing the results as a list of per-test-case lists of linear indices, with each inner list formatted without spaces, for example $[[1],[13,19],[]]$.", "solution": "The user has provided a problem that is scientifically sound, well-posed, and contains all necessary information for a unique solution. The methods prescribed—Gaussian graphical models, partial correlation, Fisher's z-transform for hypothesis testing, and the Benjamini–Hochberg procedure for multiple-testing correction—are standard and rigorous techniques in computational statistics and bioinformatics. The provided test cases are well-defined, with valid positive definite precision matrices and appropriate sample sizes. Therefore, the problem is deemed valid and a full solution is provided below.\n\nThe objective is to construct a differential network by identifying statistically significant changes in conditional dependencies between two conditions. The methodology relies on the framework of Gaussian graphical models (GGMs), where the structure of a biological network is inferred from the precision matrix, which is the inverse of the covariance matrix.\n\nThe foundational principle of GGMs is that for a set of variables following a multivariate normal distribution $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, the conditional independence between any two variables $i$ and $j$, given all other variables, is equivalent to the corresponding entry in the precision matrix $\\boldsymbol{\\Omega} = \\boldsymbol{\\Sigma}^{-1}$ being zero. That is, $X_i \\perp \\! \\! \\! \\perp X_j | \\{X_k\\}_{k \\neq i,j} \\iff \\Omega_{ij} = 0$. The magnitude of the conditional dependence is quantified by the partial correlation coefficient, $\\rho_{ij}$, which is related to the precision matrix elements by the formula:\n$$\n\\rho_{ij} = -\\dfrac{\\Omega_{ij}}{\\sqrt{\\Omega_{ii} \\, \\Omega_{jj}}} \\quad \\text{for } i \\neq j\n$$\nWe are given two condition-specific precision matrices, $\\boldsymbol{\\Omega}_1$ and $\\boldsymbol{\\Omega}_2$, for a set of $p$ variables, based on samples of size $n_1$ and $n_2$ respectively. Our goal is to test the null hypothesis $H_0: \\rho_{ij}^{(1)} = \\rho_{ij}^{(2)}$ for every edge $(i,j)$ in the network.\n\nTo perform this hypothesis test, we use the Fisher $z$-transform, a variance-stabilizing transformation for correlation coefficients. The transform is defined as:\n$$\nz(r) = \\dfrac{1}{2}\\ln\\left(\\dfrac{1+r}{1-r}\\right) = \\operatorname{arctanh}(r)\n$$\nFor an estimated partial correlation $r_{ij}$ based on a sample of size $n$ from a $p$-variate normal distribution, the transformed value $z(r_{ij})$ is approximately normally distributed with mean $z(\\rho_{ij})$ and variance $\\operatorname{Var}(z) \\approx \\dfrac{1}{n - p - 1}$. Under the null hypothesis that the true partial correlations are equal ($\\rho_{ij}^{(1)} = \\rho_{ij}^{(2)}$), the difference of the transformed coefficients from two independent samples, $z_2 - z_1$, is approximately normal with a mean of $0$ and a variance equal to the sum of the individual variances. The test statistic for each edge $(i,j)$ is therefore:\n$$\nZ_{ij} = \\dfrac{z(\\rho_{ij}^{(2)}) - z(\\rho_{ij}^{(1)})}{\\sqrt{\\dfrac{1}{n_1 - p - 1} + \\dfrac{1}{n_2 - p - 1}}}\n$$\nUnder $H_0$, $Z_{ij}$ follows a standard normal distribution, $\\mathcal{N}(0,1)$. From this, we compute a two-sided $p$-value for each edge.\n\nSince we are performing $M = \\frac{p(p-1)}{2}$ simultaneous tests, one for each potential edge, we must correct for multiple comparisons to control the number of false positives. The problem specifies using the Benjamini–Hochberg (BH) procedure to control the False Discovery Rate (FDR) at a level $\\alpha$. The BH procedure works as follows:\n1.  Collect all $M$ $p$-values, one for each edge.\n2.  Sort these $p$-values in ascending order: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(M)}$.\n3.  Find the largest integer $k$ such that its corresponding sorted $p$-value, $p_{(k)}$, satisfies the condition $p_{(k)} \\le \\frac{k}{M}\\alpha$.\n4.  If such a $k$ exists, reject the null hypothesis for all tests corresponding to $p_{(1)}, \\dots, p_{(k)}$. Otherwise, reject none.\n\nThis procedure ensures that, on average, the proportion of falsely rejected null hypotheses among all rejected hypotheses is no more than $\\alpha$.\n\nThe algorithm implemented for each test case is as follows:\n1.  Iterate through all unique pairs of variables $(i,j)$ with $0 \\le i  j  p$.\n2.  For each pair, calculate the partial correlations $\\rho_{ij}^{(1)}$ and $\\rho_{ij}^{(2)}$ from the given precision matrices $\\boldsymbol{\\Omega}_1$ and $\\boldsymbol{\\Omega}_2$.\n3.  Apply the Fisher $z$-transform to both partial correlations.\n4.  Calculate the $Z_{ij}$ statistic and the corresponding two-sided $p$-value from the standard normal distribution.\n5.  Store the computed $p$-value along with its associated linear edge index, $L = i \\cdot p + j$.\n6.  After testing all $M$ edges, apply the Benjamini–Hochberg procedure to the collected $p$-values to identify the set of significant edges.\n7.  The final output for the test case is a sorted list of the linear indices of these significant edges.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and generate the final output.\n    \"\"\"\n    \n    # Test case 1\n    p1 = 4\n    omega1_1 = np.eye(p1)\n    omega1_1[0, 1] = omega1_1[1, 0] = 0.3\n    omega1_2 = np.eye(p1)\n    omega1_2[0, 1] = omega1_2[1, 0] = 0.6\n    case1 = {\n        \"p\": p1, \"omega1\": omega1_1, \"omega2\": omega1_2,\n        \"n1\": 500, \"n2\": 500, \"alpha\": 0.05\n    }\n\n    # Test case 2\n    p2 = 4\n    omega2_1 = np.eye(p2)\n    omega2_1[0, 1] = omega2_1[1, 0] = 0.3\n    omega2_2 = np.eye(p2)\n    omega2_2[0, 1] = omega2_2[1, 0] = 0.32\n    case2 = {\n        \"p\": p2, \"omega1\": omega2_1, \"omega2\": omega2_2,\n        \"n1\": 300, \"n2\": 300, \"alpha\": 0.05\n    }\n\n    # Test case 3\n    p3 = 5\n    omega3_1 = np.eye(p3)\n    omega3_1[0, 1] = omega3_1[1, 0] = 0.35\n    omega3_1[2, 3] = omega3_1[3, 2] = -0.25\n    omega3_1[3, 4] = omega3_1[4, 3] = 0.2\n    omega3_2 = np.eye(p3)\n    omega3_2[0, 1] = omega3_2[1, 0] = 0.0\n    omega3_2[2, 3] = omega3_2[3, 2] = -0.6\n    omega3_2[3, 4] = omega3_2[4, 3] = -0.2\n    case3 = {\n        \"p\": p3, \"omega1\": omega3_1, \"omega2\": omega3_2,\n        \"n1\": 1000, \"n2\": 1000, \"alpha\": 0.05\n    }\n\n    # Test case 4\n    p4 = 3\n    omega4_1 = np.eye(p4)\n    omega4_1[0, 1] = omega4_1[1, 0] = 0.2\n    omega4_1[1, 2] = omega4_1[2, 1] = -0.15\n    omega4_2 = np.copy(omega4_1)\n    case4 = {\n        \"p\": p4, \"omega1\": omega4_1, \"omega2\": omega4_2,\n        \"n1\": 400, \"n2\": 600, \"alpha\": 0.05\n    }\n\n    test_cases = [case1, case2, case3, case4]\n    \n    results = []\n    for case in test_cases:\n        significant_edges = process_case(**case)\n        results.append(significant_edges)\n    \n    # Format the final output string to match the required format e.g., '[[],[1,7,13],[]]'\n    output_str = repr(results).replace(\" \", \"\")\n    print(output_str)\n\ndef process_case(p, omega1, omega2, n1, n2, alpha):\n    \"\"\"\n    Analyzes a single test case to find significant differential edges.\n\n    Args:\n        p (int): Dimension of the matrices.\n        omega1 (np.ndarray): Precision matrix for condition 1.\n        omega2 (np.ndarray): Precision matrix for condition 2.\n        n1 (int): Sample size for condition 1.\n        n2 (int): Sample size for condition 2.\n        alpha (float): False Discovery Rate level.\n\n    Returns:\n        list[int]: A sorted list of linear indices of significant edges.\n    \"\"\"\n    \n    num_edges = p * (p - 1) // 2\n    if num_edges == 0:\n        return []\n\n    p_values_and_indices = []\n\n    # Calculate variance term for z-score denominator\n    # This must be positive as per problem constraints (n > p + 1)\n    var_diff_z = (1.0 / (n1 - p - 1)) + (1.0 / (n2 - p - 1))\n    std_err_diff_z = np.sqrt(var_diff_z)\n    \n    for i in range(p):\n        for j in range(i + 1, p):\n            # Calculate partial correlations from precision matrices\n            rho1 = -omega1[i, j] / np.sqrt(omega1[i, i] * omega1[j, j])\n            rho2 = -omega2[i, j] / np.sqrt(omega2[i, i] * omega2[j, j])\n            \n            # Fisher z-transform. np.arctanh is numerically stable for |rho|  1,\n            # which is guaranteed for PD matrices.\n            z1 = np.arctanh(rho1)\n            z2 = np.arctanh(rho2)\n            \n            # Calculate Z-statistic for the difference\n            if std_err_diff_z > 0:\n                z_stat = (z2 - z1) / std_err_diff_z\n            else: # Should not happen with problem constraints\n                z_stat = 0.0 if np.isclose(z1, z2) else np.inf * np.sign(z2 - z1)\n\n            # Two-sided p-value from standard normal distribution\n            p_val = 2 * norm.sf(np.abs(z_stat))\n            \n            linear_index = i * p + j\n            p_values_and_indices.append((p_val, linear_index))\n\n    # Apply Benjamini-Hochberg procedure\n    # 1. Sort p-values in ascending order\n    p_values_and_indices.sort()\n\n    # 2. Find the largest k such that p_(k) = (k/M) * alpha\n    last_significant_idx = -1\n    for k, (p_val, _) in enumerate(p_values_and_indices):\n        rank = k + 1\n        bh_threshold = (rank / num_edges) * alpha\n        if p_val = bh_threshold:\n            last_significant_idx = k\n    \n    # 3. All edges up to this k are significant\n    significant_indices = []\n    if last_significant_idx != -1:\n        for i in range(last_significant_idx + 1):\n            significant_indices.append(p_values_and_indices[i][1])\n            \n    # Sort the final list of indices\n    significant_indices.sort()\n    \n    return significant_indices\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "5002392"}]}