{"hands_on_practices": [{"introduction": "A cornerstone of radiomics and digital pathology is the conversion of qualitative images into quantitative, analyzable data. This exercise provides a foundational, step-by-step guide to this process by focusing on texture analysis. You will derive the Gray-Level Co-Occurrence Matrix (GLCM), a powerful tool for capturing spatial relationships between pixels, and use it to compute the 'contrast' feature, demystifying how abstract visual patterns are translated into objective numerical descriptors [@problem_id:5073256].", "problem": "A hematoxylin and eosin (H&E) stained whole-slide image tile from a colorectal carcinoma specimen has been intensity-quantized into $4$ discrete gray levels to support quantitative texture analysis in translational radiomics and digital pathology. Consider the following $5 \\times 5$ region-of-interest (ROI), where each entry denotes the gray level $i \\in \\{0,1,2,3\\}$:\n$$\n\\begin{pmatrix}\n0 & 0 & 1 & 2 & 3 \\\\\n0 & 1 & 1 & 2 & 3 \\\\\n1 & 1 & 1 & 2 & 2 \\\\\n2 & 2 & 1 & 1 & 0 \\\\\n3 & 2 & 2 & 1 & 0\n\\end{pmatrix}.\n$$\nStarting from the definition of the Gray-Level Co-Occurrence Matrix (GLCM), in which entries $p_{ij}$ are the normalized joint frequencies of observing a pixel of gray level $i$ at spatial location $(r,c)$ and a pixel of gray level $j$ at $(r+\\Delta r, c+\\Delta c)$ for a specified spatial offset, perform the following:\n\n1. Using the spatial offset that corresponds to a direction of $0^{\\circ}$ (horizontal to the right) and a distance of $d=1$ pixel, derive the GLCM for this ROI and obtain the probability-normalized entries $p_{ij}$.\n2. From first principles of probability and the interpretation of contrast as an expectation of squared intensity differences, derive the analytical form of the contrast feature of the GLCM in terms of $p_{ij}$.\n3. Compute the numerical value of the contrast for the specified offset on the given ROI. Express the final value of the contrast as a dimensionless decimal rounded to four significant figures.\n4. Briefly explain, using the probabilistic meaning of $p_{ij}$ and the spatial sampling geometry, how changing the direction (e.g., $0^{\\circ}$ versus $90^{\\circ}$) and changing the distance $d$ would be expected to affect the contrast in histological textures characterized by anisotropic stromal fibers and glandular architecture.\n\nAll mathematical steps and quantities must be explicitly shown, and all symbols and numbers must be written in LaTeX. The angle should be treated in degrees for the purpose of defining directionality. No external formulas beyond core definitions and standard probability facts may be assumed without derivation.", "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- A $5 \\times 5$ Region-of-Interest (ROI) matrix with integer gray levels:\n$$ M = \\begin{pmatrix} 0 & 0 & 1 & 2 & 3 \\\\ 0 & 1 & 1 & 2 & 3 \\\\ 1 & 1 & 1 & 2 & 2 \\\\ 2 & 2 & 1 & 1 & 0 \\\\ 3 & 2 & 2 & 1 & 0 \\end{pmatrix} $$\n- The set of gray levels is $i \\in \\{0, 1, 2, 3\\}$, so the number of gray levels is $N_g = 4$.\n- The Gray-Level Co-Occurrence Matrix (GLCM) is defined by entries $p_{ij}$, which are the normalized joint frequencies of observing a pixel of gray level $i$ and a pixel of gray level $j$ separated by a spatial offset $(\\Delta r, \\Delta c)$.\n- Task 1 requires the specific spatial offset corresponding to a direction of $0^{\\circ}$ and a distance of $d=1$ pixel.\n- Task 2 requires the derivation of the contrast feature from first principles.\n- Task 3 requires the computation of the contrast value for the given ROI and offset, rounded to four significant figures.\n- Task 4 requires a qualitative explanation of how changes in offset direction and distance affect contrast in anisotropic histological textures.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is firmly rooted in the established field of texture analysis within medical image processing, specifically digital pathology and radiomics. The GLCM and its features, such as contrast, are standard, well-defined quantitative tools used in this domain. The context of H&E stained colorectal carcinoma is a realistic application area.\n- **Well-Posed:** All necessary information to solve the problem is provided. The ROI matrix, gray levels, and spatial offset are explicitly defined. The tasks are unambiguous and lead to a unique, computable solution.\n- **Objective:** The problem is stated in precise, objective language. It relies on mathematical definitions and formal procedures, not subjective interpretation.\n- **Consistency and Completeness:** The problem is self-contained and internally consistent. There is no missing information required for the calculations, nor are there any contradictory constraints.\n- **Feasibility:** The provided data and the requested calculations are entirely realistic and computationally feasible.\n\n### Step 3: Verdict and Action\nThe problem is scientifically sound, well-posed, objective, and complete. It is therefore deemed **valid**. A full solution will be provided.\n\n### Solution\n\nThe solution is presented in four parts as requested by the problem statement.\n\n**1. Derivation of the GLCM for the specified offset**\n\nThe specified spatial offset is a direction of $0^{\\circ}$ and a distance $d=1$ pixel. This corresponds to moving one pixel to the right, so the offset vector is $(\\Delta r, \\Delta c) = (0, 1)$. We must count all pairs of adjacent pixels $(i, j)$ in the ROI where $j$ is immediately to the right of $i$. The ROI has dimensions of $5 \\times 5$. For each of the $5$ rows, there are $5-1=4$ such horizontal pairs. The total number of pairs to be counted is $N_p = 5 \\times 4 = 20$.\n\nThe gray levels are $i, j \\in \\{0, 1, 2, 3\\}$. We construct a $4 \\times 4$ co-occurrence count matrix, $C$, where the entry $C_{ij}$ is the number of times a pixel with gray level $i$ is followed on its immediate right by a pixel with gray level $j$.\n\nLet's systematically scan the ROI, row by row:\n- Row 1 ($0, 0, 1, 2, 3$): Pairs are $(0,0)$, $(0,1)$, $(1,2)$, $(2,3)$.\n- Row 2 ($0, 1, 1, 2, 3$): Pairs are $(0,1)$, $(1,1)$, $(1,2)$, $(2,3)$.\n- Row 3 ($1, 1, 1, 2, 2$): Pairs are $(1,1)$, $(1,1)$, $(1,2)$, $(2,2)$.\n- Row 4 ($2, 2, 1, 1, 0$): Pairs are $(2,2)$, $(2,1)$, $(1,1)$, $(1,0)$.\n- Row 5 ($3, 2, 2, 1, 0$): Pairs are $(3,2)$, $(2,2)$, $(2,1)$, $(1,0)$.\n\nWe tally these pairs to populate the count matrix $C$:\n- $C_{00}$: $1$ (from Row 1)\n- $C_{01}$: $2$ (from Row 1, Row 2)\n- $C_{10}$: $2$ (from Row 4, Row 5)\n- $C_{11}$: $4$ (from Row 2, Row 3(x2), Row 4)\n- $C_{12}$: $3$ (from Row 1, Row 2, Row 3)\n- $C_{21}$: $2$ (from Row 4, Row 5)\n- $C_{22}$: $3$ (from Row 3, Row 4, Row 5)\n- $C_{23}$: $2$ (from Row 1, Row 2)\n- $C_{32}$: $1$ (from Row 5)\nAll other entries $C_{ij}$ are $0$.\n\nThe unnormalized co-occurrence count matrix $C$ is:\n$$ C = \\begin{pmatrix} 1 & 2 & 0 & 0 \\\\ 2 & 4 & 3 & 0 \\\\ 0 & 2 & 3 & 2 \\\\ 0 & 0 & 1 & 0 \\end{pmatrix} $$\nThe sum of all entries in $C$ is $1+2+2+4+3+2+3+2+1 = 20$, which matches our expected total number of pairs $N_p$.\n\nThe problem asks for the probability-normalized entries $p_{ij}$. These are obtained by dividing each entry of $C$ by the total number of pairs, $N_p=20$.\n$$ p_{ij} = \\frac{C_{ij}}{N_p} $$\nThe resulting probability-normalized GLCM, $P$, is:\n$$ P = \\frac{1}{20} \\begin{pmatrix} 1 & 2 & 0 & 0 \\\\ 2 & 4 & 3 & 0 \\\\ 0 & 2 & 3 & 2 \\\\ 0 & 0 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{20} & \\frac{2}{20} & 0 & 0 \\\\ \\frac{2}{20} & \\frac{4}{20} & \\frac{3}{20} & 0 \\\\ 0 & \\frac{2}{20} & \\frac{3}{20} & \\frac{2}{20} \\\\ 0 & 0 & \\frac{1}{20} & 0 \\end{pmatrix} $$\n\n**2. Derivation of the analytical form of the contrast feature**\n\nThe problem asks for a derivation of the contrast feature from first principles, interpreting it as an expectation of squared intensity differences.\nLet $I_1$ and $I_2$ be two discrete random variables representing the gray levels of the first and second pixels in a pair defined by the spatial offset $(\\Delta r, \\Delta c)$. The GLCM entries $p_{ij}$ represent the estimated joint probability mass function $P(I_1=i, I_2=j)$ for gray levels $i, j \\in \\{0, 1, ..., N_g-1\\}$, where $N_g=4$.\n\nThe difference in intensity (gray level) for a co-occurring pair $(i, j)$ is simply $i-j$. The squared intensity difference is $(i-j)^2$.\n\nThe contrast feature, which we denote as $K$, is defined as the expected value of this squared difference. For discrete random variables, the expectation of a function $f(I_1, I_2)$ is given by the weighted sum over all possible outcomes, where the weights are the joint probabilities:\n$$ E[f(I_1, I_2)] = \\sum_{i=0}^{N_g-1} \\sum_{j=0}^{N_g-1} f(i,j) P(I_1=i, I_2=j) $$\nIn this case, the function is $f(i,j) = (i-j)^2$. Substituting this into the expectation formula, we obtain the analytical form of the contrast feature:\n$$ K = \\sum_{i=0}^{N_g-1} \\sum_{j=0}^{N_g-1} (i-j)^2 p_{ij} $$\nThis expression represents a measure of the local variations present in the image for the specified offset; it is a moment of the GLCM (specifically, the second-order moment about the main diagonal).\n\n**3. Computation of the numerical value of contrast**\n\nUsing the derived formula and the GLCM $P$, we compute the value of the contrast $K$. With $N_g=4$, the gray levels are $i,j \\in \\{0, 1, 2, 3\\}$.\n$$ K = \\sum_{i=0}^{3} \\sum_{j=0}^{3} (i-j)^2 p_{ij} = \\sum_{i=0}^{3} \\sum_{j=0}^{3} (i-j)^2 \\frac{C_{ij}}{20} $$\nTo compute the sum, we can multiply each count $C_{ij}$ by the corresponding squared difference $(i-j)^2$:\n- For $p_{00}$, $(0-0)^2 \\cdot \\frac{1}{20} = 0 \\cdot \\frac{1}{20} = 0$.\n- For $p_{01}$, $(0-1)^2 \\cdot \\frac{2}{20} = 1 \\cdot \\frac{2}{20} = \\frac{2}{20}$.\n- For $p_{10}$, $(1-0)^2 \\cdot \\frac{2}{20} = 1 \\cdot \\frac{2}{20} = \\frac{2}{20}$.\n- For $p_{11}$, $(1-1)^2 \\cdot \\frac{4}{20} = 0 \\cdot \\frac{4}{20} = 0$.\n- For $p_{12}$, $(1-2)^2 \\cdot \\frac{3}{20} = 1 \\cdot \\frac{3}{20} = \\frac{3}{20}$.\n- For $p_{21}$, $(2-1)^2 \\cdot \\frac{2}{20} = 1 \\cdot \\frac{2}{20} = \\frac{2}{20}$.\n- For $p_{22}$, $(2-2)^2 \\cdot \\frac{3}{20} = 0 \\cdot \\frac{3}{20} = 0$.\n- For $p_{23}$, $(2-3)^2 \\cdot \\frac{2}{20} = 1 \\cdot \\frac{2}{20} = \\frac{2}{20}$.\n- For $p_{32}$, $(3-2)^2 \\cdot \\frac{1}{20} = 1 \\cdot \\frac{1}{20} = \\frac{1}{20}$.\nAll other $p_{ij}$ are zero and do not contribute to the sum.\n\nSumming these contributions:\n$$ K = \\frac{2}{20} + \\frac{2}{20} + \\frac{3}{20} + \\frac{2}{20} + \\frac{2}{20} + \\frac{1}{20} = \\frac{2+2+3+2+2+1}{20} = \\frac{12}{20} $$\n$$ K = \\frac{3}{5} = 0.6 $$\nThe problem requires the final value to be expressed as a decimal rounded to four significant figures.\n$$ K = 0.6000 $$\n\n**4. Explanation of offset effects on contrast**\n\nThe contrast feature $K = \\sum \\sum (i-j)^2 p_{ij}$ weights each joint probability $p_{ij}$ by the squared difference of the gray levels, $(i-j)^2$. A high contrast value indicates a high frequency of co-occurring pixels with large gray-level differences, which visually corresponds to a \"busy\" or sharp texture.\n\nHistological textures from tissues like colorectal carcinoma often exhibit anisotropy due to organized biological structures such as stromal fibers and glandular formations.\n\n- **Effect of Changing Direction:** Consider a texture with horizontally aligned stromal fibers.\n  - If the GLCM is computed with a horizontal direction ($0^{\\circ}$), the offset samples pixel pairs *along* the fibers. Since pixels within a single fiber are likely to have similar gray levels, the joint probabilities $p_{ij}$ will be concentrated on or near the main diagonal of the GLCM (i.e., where $i \\approx j$). The weighting factors $(i-j)^2$ will be small for these high-probability entries, resulting in a **low contrast** value.\n  - If the direction is changed to vertical ($90^{\\circ}$), the offset now samples pixel pairs *across* the fibers. This path frequently traverses boundaries between fibers and surrounding tissue (e.g., cytoplasm, extracellular matrix). This leads to a higher probability of co-occurring pixels with dissimilar gray levels (e.g., a dark fiber pixel next to a light stromal pixel). Consequently, the off-diagonal entries of the GLCM (where $|i-j|$ is large) will have higher probabilities. These entries are multiplied by large $(i-j)^2$ weights, yielding a **high contrast** value.\n  - Thus, in an anisotropic texture, the contrast feature is orientation-dependent, and analyzing its value across different directions can quantify the degree and direction of anisotropy.\n\n- **Effect of Changing Distance $d$:** The distance parameter $d$ controls the scale of the texture analysis.\n  - For a small distance (e.g., $d=1$), the GLCM captures fine-scale texture details. Neighboring pixels are highly correlated, typically belonging to the same micro-structure. This generally results in lower contrast compared to measurements across structures, unless the texture is extremely fine-grained or noisy.\n  - As the distance $d$ increases, the correlation between pixel pairs typically decreases. The relationship is texture-dependent. If $d$ becomes comparable to the characteristic size of objects in the image (e.g., the width of a gland or a fiber bundle), the offset may frequently span the boundary between these objects and their background. This can lead to a peak in the contrast value at that specific distance.\n  - For very large distances ($d \\to \\infty$), the pixel gray levels become statistically independent. The joint probability $p_{ij}$ approaches the product of the marginal probabilities, $p_{i} p_{j}$. The contrast value will stabilize towards a value related to the overall variance of the image's gray-level histogram, reflecting global rather than local variation.\n  - In summary, varying $d$ allows the probing of textural properties at different spatial scales, revealing information about the size and periodicity of structural elements in the tissue.", "answer": "$$\\boxed{0.6000}$$", "id": "5073256"}, {"introduction": "Once features are extracted and a predictive model is trained, the next critical task is to rigorously evaluate its performance. This practice delves into two indispensable tools for this purpose: the Receiver Operating Characteristic (ROC) curve and the Precision-Recall (PR) curve. By working from first principles, you will learn to construct these curves and compute their respective areas, gaining a deeper intuition for how to assess a classifier's discriminative ability, particularly in the context of class imbalance that is prevalent in medical datasets [@problem_id:5073305].", "problem": "In a translational radiomics and digital pathology study for micrometastasis detection, a binary classifier assigns continuous scores to whole-slide image patches. You are asked to reason from first principles about Receiver Operating Characteristic (ROC) and Precision-Recall (PR) analyses under class imbalance typical of translational datasets. Starting from the core definitions of true positive rate, false positive rate, precision, and recall, and the interpretation of areas under curves as Riemann sums, derive how to compute the Area Under the Curve (AUC) for the ROC curve and the area under the PR curve (often called Average Precision) when thresholding at distinct score levels. Then apply your derivation to the following imbalanced validation set.\n\nThere are $P=10$ positive patches and $N=200$ negative patches. The classifier outputs the following score strata (scores decrease from top to bottom). Within each stratum, all patches share the same score:\n- Score $0.99$: $3$ positives, $1$ negative.\n- Score $0.90$: $2$ positives, $5$ negatives.\n- Score $0.80$: $2$ positives, $20$ negatives.\n- Score $0.60$: $1$ positive, $30$ negatives.\n- Score $0.40$: $1$ positive, $60$ negatives.\n- Score $0.20$: $1$ positive, $84$ negatives.\n\nAdopt the following conventions.\n1. Thresholds are set at the distinct score values in descending order; at each threshold, include all patches with scores greater than or equal to that threshold.\n2. For the ROC curve, plot true positive rate versus false positive rate at each threshold and include the trivial endpoints at $(0,0)$ and $(1,1)$. Interpret the area under the piecewise-linear ROC curve via the trapezoidal rule in the false positive rate–true positive rate plane.\n3. For the PR curve, plot precision versus recall at each threshold. Use the standard step-function convention in which precision is taken to be constant over each recall interval achieved by lowering the threshold to the next distinct score, and interpret the area as a Riemann sum of precision times the increment in recall.\n\nYour tasks are:\n- From the fundamental definitions, derive formulas to compute the ROC area under the curve and the PR area under the curve in this stepwise-threshold setting without resorting to any unproven shortcut formulas.\n- Compute the ROC area under the curve and the PR area under the curve for the given dataset using your derivations.\n- Let $r$ denote the ratio of the PR area under the curve to the ROC area under the curve. Report only the value of $r$.\n\nExpress the final answer as a pure number without units and round your answer to four significant figures.", "solution": "This problem requires the derivation and application of formulas for the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) and the Area Under the Precision-Recall Curve (AUC-PR) for a dataset with discrete classifier scores. The analysis will proceed from the fundamental definitions of the metrics involved.\n\nFirst, we define the basic quantities. Let $P$ be the total number of positive instances and $N$ be the total number of negative instances. For a given classification threshold, $\\tau$, we can define the counts of outcomes:\n- True Positives, $TP(\\tau)$: The number of positive instances with a score greater than or equal to $\\tau$.\n- False Positives, $FP(\\tau)$: The number of negative instances with a score greater than or equal to $\\tau$.\n\nFrom these counts, we define the primary performance metrics:\n- True Positive Rate (TPR), also known as Recall or Sensitivity: $TPR(\\tau) = \\text{Recall}(\\tau) = \\frac{TP(\\tau)}{P}$. This represents the fraction of actual positives that are correctly identified.\n- False Positive Rate (FPR): $FPR(\\tau) = \\frac{FP(\\tau)}{N}$. This represents the fraction of actual negatives that are incorrectly identified as positive.\n- Precision: $\\text{Precision}(\\tau) = \\frac{TP(\\tau)}{TP(\\tau) + FP(\\tau)}$. This represents the fraction of instances classified as positive that are actually positive.\n\nThe problem provides a set of $k$ distinct score strata, which define a set of thresholds, $\\tau_1 > \\tau_2 > \\dots > \\tau_k$. At each threshold $\\tau_i$, we obtain a set of values $(TP_i, FP_i, TPR_i, FPR_i, \\text{Recall}_i, \\text{Precision}_i)$.\n\n**Derivation of AUC-ROC Calculation**\n\nThe ROC curve is a plot of $TPR$ versus $FPR$ for varying thresholds. For a discrete set of thresholds, this results in a set of points $(FPR_i, TPR_i)$, for $i=1, \\dots, k$. The problem specifies including the trivial endpoints $(0,0)$ and $(1,1)$. Let us denote these points as $p_i = (FPR_i, TPR_i)$, where $p_0 = (0,0)$ corresponds to a threshold higher than any score (thus $TP=0, FP=0$), and the final point will be $(1,1)$ when the threshold is low enough to classify all instances as positive (thus $TP=P, FP=N$).\n\nThe problem mandates interpreting the area under the piecewise-linear curve connecting these ordered points using the trapezoidal rule. The area of a trapezoid formed by two consecutive points $p_{i-1} = (FPR_{i-1}, TPR_{i-1})$ and $p_i = (FPR_i, TPR_i)$ is given by the formula:\n$$A_i = \\frac{1}{2} (TPR_{i-1} + TPR_i) \\times (FPR_i - FPR_{i-1})$$\nThe total AUC-ROC is the sum of the areas of all such trapezoids defined by the sequence of thresholds. If we have $m$ points on the curve (including the origin), indexed from $i=0$ to $i=m-1$, the total area is:\n$$AUC_{ROC} = \\sum_{i=1}^{m-1} \\frac{1}{2} (TPR_{i-1} + TPR_i) (FPR_i - FPR_{i-1})$$\n\n**Derivation of AUC-PR (Average Precision) Calculation**\n\nThe PR curve is a plot of Precision versus Recall. The problem states to interpret the area as a Riemann sum based on a step-function approximation. This is a standard method for calculating Average Precision (AP). Let the points be $(\\text{Recall}_i, \\text{Precision}_i)$, ordered such that $\\text{Recall}_0=0 < \\text{Recall}_1 \\le \\text{Recall}_2 \\le \\dots \\le \\text{Recall}_k=1$. As the threshold is lowered from $\\tau_{i-1}$ to $\\tau_i$, recall increases from $\\text{Recall}_{i-1}$ to $\\text{Recall}_i$. The problem specifies that precision is constant over this interval of recall, taking the value at the new point, $\\text{Precision}_i$.\n\nThe area of the rectangle corresponding to the $i$-th step is $\\text{Precision}_i$ multiplied by the increment in recall, $\\Delta\\text{Recall}_i = \\text{Recall}_i - \\text{Recall}_{i-1}$.\nThe total area, AUC-PR, is the sum of these rectangular areas:\n$$AUC_{PR} = \\sum_{i=1}^{k} \\text{Precision}_i \\times (\\text{Recall}_i - \\text{Recall}_{i-1})$$\nwhere $\\text{Recall}_0 = 0$.\n\n**Application to the Dataset**\n\nThe dataset has $P=10$ positives and $N=200$ negatives.\nThe score strata are:\n- Score $s_1=0.99$: $3$ positives, $1$ negative.\n- Score $s_2=0.90$: $2$ positives, $5$ negatives.\n- Score $s_3=0.80$: $2$ positives, $20$ negatives.\n- Score $s_4=0.60$: $1$ positive, $30$ negatives.\n- Score $s_5=0.40$: $1$ positive, $60$ negatives.\n- Score $s_6=0.20$: $1$ positive, $84$ negatives.\n\nWe calculate the cumulative counts and metrics at each threshold $\\tau_i = s_i$:\nLet $cTP_i$ and $cFP_i$ be the cumulative true and false positives for scores $\\ge \\tau_i$.\n\n1.  **Threshold $\\tau_1=0.99$**:\n    - $cTP_1=3$, $cFP_1=1$\n    - $TPR_1 = 3/10 = 0.3$, $FPR_1 = 1/200 = 0.005$\n    - $\\text{Recall}_1 = 0.3$, $\\text{Precision}_1 = 3/(3+1) = 0.75$\n\n2.  **Threshold $\\tau_2=0.90$**:\n    - $cTP_2=3+2=5$, $cFP_2=1+5=6$\n    - $TPR_2 = 5/10 = 0.5$, $FPR_2 = 6/200 = 0.03$\n    - $\\text{Recall}_2 = 0.5$, $\\text{Precision}_2 = 5/(5+6) = 5/11$\n\n3.  **Threshold $\\tau_3=0.80$**:\n    - $cTP_3=5+2=7$, $cFP_3=6+20=26$\n    - $TPR_3 = 7/10 = 0.7$, $FPR_3 = 26/200 = 0.13$\n    - $\\text{Recall}_3 = 0.7$, $\\text{Precision}_3 = 7/(7+26) = 7/33$\n\n4.  **Threshold $\\tau_4=0.60$**:\n    - $cTP_4=7+1=8$, $cFP_4=26+30=56$\n    - $TPR_4 = 8/10 = 0.8$, $FPR_4 = 56/200 = 0.28$\n    - $\\text{Recall}_4 = 0.8$, $\\text{Precision}_4 = 8/(8+56) = 8/64 = 0.125$\n\n5.  **Threshold $\\tau_5=0.40$**:\n    - $cTP_5=8+1=9$, $cFP_5=56+60=116$\n    - $TPR_5 = 9/10 = 0.9$, $FPR_5 = 116/200 = 0.58$\n    - $\\text{Recall}_5 = 0.9$, $\\text{Precision}_5 = 9/(9+116) = 9/125 = 0.072$\n\n6.  **Threshold $\\tau_6=0.20$**:\n    - $cTP_6=9+1=10$, $cFP_6=116+84=200$\n    - $TPR_6 = 10/10 = 1.0$, $FPR_6 = 200/200 = 1.0$\n    - $\\text{Recall}_6 = 1.0$, $\\text{Precision}_6 = 10/(10+200) = 10/210 = 1/21$\n\nThe points for the ROC curve are $p_0=(0,0)$, $p_1=(0.005, 0.3)$, $p_2=(0.03, 0.5)$, $p_3=(0.13, 0.7)$, $p_4=(0.28, 0.8)$, $p_5=(0.58, 0.9)$, and $p_6=(1.0, 1.0)$.\n\n**AUC-ROC Calculation**\nUsing the trapezoidal rule:\n- Area 1: $\\frac{1}{2}(0+0.3) \\times (0.005-0) = 0.15 \\times 0.005 = 0.00075$\n- Area 2: $\\frac{1}{2}(0.3+0.5) \\times (0.03-0.005) = 0.4 \\times 0.025 = 0.01$\n- Area 3: $\\frac{1}{2}(0.5+0.7) \\times (0.13-0.03) = 0.6 \\times 0.1 = 0.06$\n- Area 4: $\\frac{1}{2}(0.7+0.8) \\times (0.28-0.13) = 0.75 \\times 0.15 = 0.1125$\n- Area 5: $\\frac{1}{2}(0.8+0.9) \\times (0.58-0.28) = 0.85 \\times 0.3 = 0.255$\n- Area 6: $\\frac{1}{2}(0.9+1.0) \\times (1.0-0.58) = 0.95 \\times 0.42 = 0.399$\n\n$AUC_{ROC} = 0.00075 + 0.01 + 0.06 + 0.1125 + 0.255 + 0.399 = 0.83725$.\n\n**AUC-PR Calculation**\nUsing the Riemann sum formula, with $\\text{Recall}_0 = 0$:\n- Area 1: $\\text{Precision}_1 \\times (\\text{Recall}_1 - \\text{Recall}_0) = 0.75 \\times (0.3 - 0) = 0.225$\n- Area 2: $\\text{Precision}_2 \\times (\\text{Recall}_2 - \\text{Recall}_1) = \\frac{5}{11} \\times (0.5 - 0.3) = \\frac{5}{11} \\times 0.2 = \\frac{1}{11}$\n- Area 3: $\\text{Precision}_3 \\times (\\text{Recall}_3 - \\text{Recall}_2) = \\frac{7}{33} \\times (0.7 - 0.5) = \\frac{7}{33} \\times 0.2 = \\frac{14}{330} = \\frac{7}{165}$\n- Area 4: $\\text{Precision}_4 \\times (\\text{Recall}_4 - \\text{Recall}_3) = 0.125 \\times (0.8 - 0.7) = 0.125 \\times 0.1 = 0.0125$\n- Area 5: $\\text{Precision}_5 \\times (\\text{Recall}_5 - \\text{Recall}_4) = 0.072 \\times (0.9 - 0.8) = 0.072 \\times 0.1 = 0.0072$\n- Area 6: $\\text{Precision}_6 \\times (\\text{Recall}_6 - \\text{Recall}_5) = \\frac{1}{21} \\times (1.0 - 0.9) = \\frac{1}{21} \\times 0.1 = \\frac{1}{210}$\n\n$AUC_{PR} = 0.225 + \\frac{1}{11} + \\frac{7}{165} + 0.0125 + 0.0072 + \\frac{1}{210}$\nIn decimal form:\n$AUC_{PR} \\approx 0.225 + 0.09090909... + 0.04242424... + 0.0125 + 0.0072 + 0.00476190...$\n$AUC_{PR} \\approx 0.38279523...$\n\n**Ratio Calculation**\nThe problem asks for the ratio $r = \\frac{AUC_{PR}}{AUC_{ROC}}$.\n$r = \\frac{0.38279523...}{0.83725} \\approx 0.4571701199...$\nRounding to four significant figures, we get $r \\approx 0.4572$.", "answer": "$$\\boxed{0.4572}$$", "id": "5073305"}, {"introduction": "A model with high statistical accuracy is not always clinically useful. The ultimate goal of translational research is to improve patient outcomes, which requires evaluating a model's value within a clinical decision-making framework. This exercise introduces Decision Curve Analysis (DCA), a powerful method for quantifying the net benefit of a predictive model by weighing the benefits of true positives against the harms of false positives across a range of clinical preferences [@problem_id:5073217]. This practice bridges the gap between abstract performance metrics and tangible clinical impact.", "problem": "In a translational medicine study integrating quantitative radiomics with computational histomorphometry from digital pathology, a fused predictive model outputs patient-level predicted probabilities of harboring a high-risk tumor phenotype that would change management. Clinical adoption is governed by a threshold-probability decision rule: treat a patient if the predicted probability $r$ exceeds a threshold $p_t \\in (0,1)$, otherwise withhold treatment. Decision Curve Analysis (DCA) evaluates the clinical utility of such a model by quantifying the net clinical value of treatment decisions made across threshold probabilities.\n\nStarting from the following fundamental base:\n- In binary decision-making under uncertainty, a threshold probability $p_t$ encodes the trade-off between the benefit of treating a true positive and the harm of treating a false positive. The implied relative harm-to-benefit weight is $w = \\frac{p_t}{1 - p_t}$.\n- Define true positives $\\mathrm{TP}$ and false positives $\\mathrm{FP}$ under the decision rule “treat if $r \\ge p_t$” in a validation sample of size $n$, and consider net clinical value in units of “true-positive equivalents per patient.”\n- Assume benefits and harms are additive across patients and time-independent for a single decision.\n\nTasks:\n1) Using only the principles above, derive an expression for the net benefit $\\mathrm{NB}(p_t)$ as a function of $\\mathrm{TP}$, $\\mathrm{FP}$, $n$, and $p_t$.\n\n2) Consider a held-out validation cohort of $n = 20$ patients with ground truth labels $y_i \\in \\{0,1\\}$ and model-predicted probabilities $r_i \\in [0,1]$. The decision rule is: treat if $r_i \\ge p_t$. Use your derived expression to compute $\\mathrm{NB}(p_t)$ at $p_t \\in \\{0.1, 0.2, 0.3, 0.4\\}$, where “true positives” are those with $y_i=1$ who are treated, and “false positives” are those with $y_i=0$ who are treated.\n\nThe data are:\n- Patient $1$: $r_1 = 0.05$, $y_1 = 0$\n- Patient $2$: $r_2 = 0.08$, $y_2 = 0$\n- Patient $3$: $r_3 = 0.09$, $y_3 = 0$\n- Patient $4$: $r_4 = 0.12$, $y_4 = 0$\n- Patient $5$: $r_5 = 0.14$, $y_5 = 0$\n- Patient $6$: $r_6 = 0.18$, $y_6 = 0$\n- Patient $7$: $r_7 = 0.19$, $y_7 = 0$\n- Patient $8$: $r_8 = 0.21$, $y_8 = 0$\n- Patient $9$: $r_9 = 0.23$, $y_9 = 1$\n- Patient $10$: $r_{10} = 0.27$, $y_{10} = 1$\n- Patient $11$: $r_{11} = 0.31$, $y_{11} = 1$\n- Patient $12$: $r_{12} = 0.33$, $y_{12} = 0$\n- Patient $13$: $r_{13} = 0.36$, $y_{13} = 1$\n- Patient $14$: $r_{14} = 0.39$, $y_{14} = 0$\n- Patient $15$: $r_{15} = 0.42$, $y_{15} = 1$\n- Patient $16$: $r_{16} = 0.48$, $y_{16} = 1$\n- Patient $17$: $r_{17} = 0.55$, $y_{17} = 1$\n- Patient $18$: $r_{18} = 0.64$, $y_{18} = 1$\n- Patient $19$: $r_{19} = 0.72$, $y_{19} = 1$\n- Patient $20$: $r_{20} = 0.85$, $y_{20} = 1$\n\n3) Let the summary performance metric be the arithmetic mean of $\\mathrm{NB}(p_t)$ across the set $\\{0.1, 0.2, 0.3, 0.4\\}$. Compute this mean net benefit and provide it as your final answer.\n\nExpress the final numerical answer as a decimal fraction (no percent sign). Round your final answer to four significant figures.", "solution": "The solution is structured to address the three tasks sequentially.\n\n**Part 1: Derivation of the Net Benefit Expression**\n\nThe net benefit ($\\mathrm{NB}$) of a decision-making strategy is defined as the total benefits minus the total harms, standardized to be expressed in a common unit. The problem specifies that the net clinical value should be in units of \"true-positive equivalents per patient.\"\n\nLet the benefit of correctly identifying and treating a patient with the high-risk phenotype (a true positive) be normalized to $1$. The total benefit for a population is the number of true positives, $\\mathrm{TP}$.\n\nThe harm of treating a patient who does not have the high-risk phenotype (a false positive) is weighted relative to the benefit of a true positive. This relative weight is given as $w = \\frac{p_t}{1 - p_t}$, where $p_t$ is the threshold probability at which a decision-maker is indifferent between treating and not treating. The total harm for the population is the number of false positives, $\\mathrm{FP}$, multiplied by this weight: $\\mathrm{FP} \\times w$.\n\nThe total net benefit for the entire sample of $n$ patients, in units of true-positive equivalents, is the sum of benefits minus the sum of harms:\n$$ \\text{Total Net Benefit} = \\mathrm{TP} - (\\mathrm{FP} \\times w) $$\nSubstituting the expression for $w$:\n$$ \\text{Total Net Benefit} = \\mathrm{TP} - \\mathrm{FP} \\left( \\frac{p_t}{1 - p_t} \\right) $$\nThe problem asks for the net benefit per patient, $\\mathrm{NB}(p_t)$. To obtain this, we divide the total net benefit by the sample size, $n$:\n$$ \\mathrm{NB}(p_t) = \\frac{\\text{Total Net Benefit}}{n} = \\frac{1}{n} \\left[ \\mathrm{TP} - \\mathrm{FP} \\left( \\frac{p_t}{1 - p_t} \\right) \\right] $$\nThis expression can also be written as the difference between the true positive rate and the weighted false positive rate:\n$$ \\mathrm{NB}(p_t) = \\frac{\\mathrm{TP}}{n} - \\frac{\\mathrm{FP}}{n} \\left( \\frac{p_t}{1 - p_t} \\right) $$\nThis is the required expression for net benefit.\n\n**Part 2: Computation of Net Benefit for the Validation Cohort**\n\nThe validation cohort consists of $n=20$ patients. The decision rule is to treat if the predicted probability $r_i$ is greater than or equal to the threshold $p_t$. We calculate the number of true positives ($\\mathrm{TP}$) and false positives ($\\mathrm{FP}$) for each specified threshold.\n\nThe data provided are:\n-   Patients with $y=0$ (no high-risk phenotype): $r \\in \\{0.05, 0.08, 0.09, 0.12, 0.14, 0.18, 0.19, 0.21, 0.33, 0.39\\}$\n-   Patients with $y=1$ (high-risk phenotype): $r \\in \\{0.23, 0.27, 0.31, 0.36, 0.42, 0.48, 0.55, 0.64, 0.72, 0.85\\}$\n\nWe evaluate $\\mathrm{NB}(p_t)$ for $p_t \\in \\{0.1, 0.2, 0.3, 0.4\\}$.\n\n**Case 1: $p_t = 0.1$**\n-   Treated patients have $r_i \\ge 0.1$.\n-   $\\mathrm{FP}$: Patients with $y_i=0$ and $r_i \\ge 0.1$. These are patients with $r \\in \\{0.12, 0.14, 0.18, 0.19, 0.21, 0.33, 0.39\\}$. Count: $\\mathrm{FP}=7$.\n-   $\\mathrm{TP}$: Patients with $y_i=1$ and $r_i \\ge 0.1$. All $10$ patients with $y_i=1$ have $r_i > 0.2$, so all are treated. Count: $\\mathrm{TP}=10$.\n-   $\\mathrm{NB}(0.1) = \\frac{10}{20} - \\frac{7}{20} \\left( \\frac{0.1}{1 - 0.1} \\right) = \\frac{1}{2} - \\frac{7}{20} \\left( \\frac{0.1}{0.9} \\right) = \\frac{1}{2} - \\frac{7}{20} \\left( \\frac{1}{9} \\right) = \\frac{1}{2} - \\frac{7}{180} = \\frac{90 - 7}{180} = \\frac{83}{180}$.\n\n**Case 2: $p_t = 0.2$**\n-   Treated patients have $r_i \\ge 0.2$.\n-   $\\mathrm{FP}$: Patients with $y_i=0$ and $r_i \\ge 0.2$. These are patients with $r \\in \\{0.21, 0.33, 0.39\\}$. Count: $\\mathrm{FP}=3$.\n-   $\\mathrm{TP}$: Patients with $y_i=1$ and $r_i \\ge 0.2$. All $10$ patients with $y_i=1$ meet this criterion. Count: $\\mathrm{TP}=10$.\n-   $\\mathrm{NB}(0.2) = \\frac{10}{20} - \\frac{3}{20} \\left( \\frac{0.2}{1 - 0.2} \\right) = \\frac{1}{2} - \\frac{3}{20} \\left( \\frac{0.2}{0.8} \\right) = \\frac{1}{2} - \\frac{3}{20} \\left( \\frac{1}{4} \\right) = \\frac{1}{2} - \\frac{3}{80} = \\frac{40 - 3}{80} = \\frac{37}{80}$.\n\n**Case 3: $p_t = 0.3$**\n-   Treated patients have $r_i \\ge 0.3$.\n-   $\\mathrm{FP}$: Patients with $y_i=0$ and $r_i \\ge 0.3$. These are patients with $r \\in \\{0.33, 0.39\\}$. Count: $\\mathrm{FP}=2$.\n-   $\\mathrm{TP}$: Patients with $y_i=1$ and $r_i \\ge 0.3$. These are patients with $r \\in \\{0.31, 0.36, 0.42, 0.48, 0.55, 0.64, 0.72, 0.85\\}$. Count: $\\mathrm{TP}=8$.\n-   $\\mathrm{NB}(0.3) = \\frac{8}{20} - \\frac{2}{20} \\left( \\frac{0.3}{1 - 0.3} \\right) = \\frac{2}{5} - \\frac{1}{10} \\left( \\frac{0.3}{0.7} \\right) = \\frac{2}{5} - \\frac{1}{10} \\left( \\frac{3}{7} \\right) = \\frac{2}{5} - \\frac{3}{70} = \\frac{28 - 3}{70} = \\frac{25}{70} = \\frac{5}{14}$.\n\n**Case 4: $p_t = 0.4$**\n-   Treated patients have $r_i \\ge 0.4$.\n-   $\\mathrm{FP}$: Patients with $y_i=0$ and $r_i \\ge 0.4$. There are no such patients. Count: $\\mathrm{FP}=0$.\n-   $\\mathrm{TP}$: Patients with $y_i=1$ and $r_i \\ge 0.4$. These are patients with $r \\in \\{0.42, 0.48, 0.55, 0.64, 0.72, 0.85\\}$. Count: $\\mathrm{TP}=6$.\n-   $\\mathrm{NB}(0.4) = \\frac{6}{20} - \\frac{0}{20} \\left( \\frac{0.4}{1 - 0.4} \\right) = \\frac{6}{20} - 0 = \\frac{3}{10}$.\n\n**Part 3: Calculation of the Mean Net Benefit**\n\nThe final task is to compute the arithmetic mean of the four net benefit values.\n$$ \\text{Mean NB} = \\frac{1}{4} \\left[ \\mathrm{NB}(0.1) + \\mathrm{NB}(0.2) + \\mathrm{NB}(0.3) + \\mathrm{NB}(0.4) \\right] $$\nSubstituting the fractional values for precision:\n$$ \\text{Mean NB} = \\frac{1}{4} \\left( \\frac{83}{180} + \\frac{37}{80} + \\frac{5}{14} + \\frac{3}{10} \\right) $$\nTo sum the fractions, we find a common denominator for $180$, $80$, $14$, and $10$.\nThe prime factorizations are $180 = 2^2 \\cdot 3^2 \\cdot 5$, $80 = 2^4 \\cdot 5$, $14 = 2 \\cdot 7$, and $10 = 2 \\cdot 5$.\nThe least common multiple is $2^4 \\cdot 3^2 \\cdot 5 \\cdot 7 = 16 \\cdot 9 \\cdot 5 \\cdot 7 = 5040$.\n$$ \\text{Mean NB} = \\frac{1}{4} \\left( \\frac{83 \\cdot 28}{5040} + \\frac{37 \\cdot 63}{5040} + \\frac{5 \\cdot 360}{5040} + \\frac{3 \\cdot 504}{5040} \\right) $$\n$$ \\text{Mean NB} = \\frac{1}{4} \\left( \\frac{2324}{5040} + \\frac{2331}{5040} + \\frac{1800}{5040} + \\frac{1512}{5040} \\right) $$\n$$ \\text{Mean NB} = \\frac{1}{4} \\left( \\frac{2324 + 2331 + 1800 + 1512}{5040} \\right) $$\n$$ \\text{Mean NB} = \\frac{1}{4} \\left( \\frac{7967}{5040} \\right) = \\frac{7967}{20160} $$\nNow, we convert this fraction to a decimal and round to four significant figures as requested.\n$$ \\text{Mean NB} = \\frac{7967}{20160} \\approx 0.39518849... $$\nRounding to four significant figures gives $0.3952$.", "answer": "$$\\boxed{0.3952}$$", "id": "5073217"}]}