{"hands_on_practices": [{"introduction": "Statistical power is a cornerstone of rigorous and reproducible research, ensuring a study is large enough to reliably detect a true effect. Underpowered studies are a primary cause of irreproducible findings, as they are prone to missing real effects or producing exaggerated estimates. This exercise [@problem_id:5057046] challenges you to derive the sample size formula for a clinical trial from first principles, solidifying the crucial relationship between the desired effect size, data variability, and the required number of participants to achieve a robust conclusion.", "problem": "A translational medicine team is planning a two-arm, parallel-group, randomized controlled trial to compare a novel anti-inflammatory small molecule against standard of care in patients with an elevated biomarker. The primary endpoint is a continuous change score in a validated inflammation biomarker measured in standardized units. The investigators wish to ensure rigor and reproducibility by prospectively determining the minimum per-arm sample size required to achieve a pre-specified power for a two-sided significance test, thereby avoiding underpowered, irreproducible results.\n\nAssume the following design features and modeling assumptions:\n- Two independent groups with equal allocation, each of size $n$.\n- Outcomes are independent and identically distributed within arms, with a common variance $\\sigma^{2}$.\n- The primary analysis will use a two-sided two-sample $t$-test at significance level $\\alpha$, testing the null hypothesis $H_{0}:\\ \\mu_{1}-\\mu_{2}=0$ versus the alternative hypothesis $H_{1}:\\ \\mu_{1}-\\mu_{2}\\neq 0$, where $\\mu_{1}-\\mu_{2}$ denotes the true mean difference between the experimental and control arms.\n- For planning purposes, the sampling distribution of the standardized difference in sample means may be approximated using the Central Limit Theorem by a standard normal distribution when plugging in a planning value for $\\sigma^{2}$.\n\nTask:\n1. Starting from core definitions of Type I error (probability $\\alpha$ of rejecting $H_{0}$ when $H_{0}$ is true), Type II error (probability $\\beta$ of failing to reject $H_{0}$ when $H_{1}$ is true), and the distribution of the difference of two independent sample means, derive an analytic expression for the minimum per-arm sample size $n$ that achieves power $1-\\beta$ to detect a clinically meaningful difference of magnitude $\\delta=|\\mu_{1}-\\mu_{2}|$ under a two-sided test at level $\\alpha$. Your derivation must begin from the distribution of the difference in sample means, the definition of the rejection region for a two-sided test at level $\\alpha$, and the definition of power under a fixed alternative $|\\mu_{1}-\\mu_{2}|=\\delta$.\n2. Then compute the resulting per-arm $n$ for the design values $\\alpha=0.05$ (two-sided), $1-\\beta=0.90$, clinically meaningful difference $\\delta=5$, and common variance $\\sigma^{2}=64$. Report the smallest integer $n$ per arm that meets or exceeds the target power under your derived criterion. Provide this integer as your final answer. Do not include any units, and do not round to a specified number of significant figures; instead, report the exact smallest integer $n$ that satisfies the requirement.", "solution": "### Step 1: Extract Givens\n- **Study Design**: Two-arm, parallel-group, randomized controlled trial.\n- **Allocation**: Two independent groups with equal allocation, each of size $n$.\n- **Outcome**: A continuous variable, assumed to be independent and identically distributed within each arm.\n- **Variance**: A common variance $\\sigma^{2}$ for both groups.\n- **Hypothesis Test**: A two-sided two-sample $t$-test for the null hypothesis $H_{0}:\\ \\mu_{1}-\\mu_{2}=0$ versus the alternative hypothesis $H_{1}:\\ \\mu_{1}-\\mu_{2}\\neq 0$.\n- **Significance Level**: $\\alpha$.\n- **Power**: $1-\\beta$.\n- **Effect Size**: A clinically meaningful difference of magnitude $\\delta=|\\mu_{1}-\\mu_{2}|$.\n- **Approximation**: For planning, the sampling distribution of the standardized difference in sample means is approximated by a standard normal distribution.\n- **Derivation Requirement**: The derivation must start from (1) the distribution of the difference in sample means, (2) the definition of the rejection region for a two-sided test, and (3) the definition of power under a fixed alternative.\n- **Numerical Values for Calculation**:\n    - Significance level $\\alpha=0.05$ (two-sided).\n    - Desired power $1-\\beta=0.90$.\n    - Clinically meaningful difference $\\delta=5$.\n    - Common variance $\\sigma^{2}=64$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It presents a standard, fundamental problem in biostatistics and clinical trial design: the a priori calculation of sample size to ensure adequate statistical power. The problem is directly relevant to the topic of research rigor and reproducibility in translational medicine. All necessary parameters and assumptions for a unique solution are provided, and there are no contradictions. The use of a normal approximation for a $t$-test during the planning phase is a standard and accepted practice, which simplifies the derivation to an algebraic one rather than an iterative one. The problem is not trivial, as it requires a derivation from first principles.\n\n### Step 3: Verdict and Action\nThe problem is valid. A rigorous solution will be provided.\n\n### Part 1: Derivation of the Sample Size Formula\n\nLet $\\bar{X}_1$ and $\\bar{X}_2$ be the sample means from the two independent groups (experimental and control), each of size $n$. The populations are assumed to have means $\\mu_1$ and $\\mu_2$ respectively, and a common variance $\\sigma^2$.\n\n1.  **Distribution of the Difference in Sample Means**:\n    The mean of $\\bar{X}_1$ is $E[\\bar{X}_1] = \\mu_1$ and its variance is $Var(\\bar{X}_1) = \\frac{\\sigma^2}{n}$. Similarly, $E[\\bar{X}_2] = \\mu_2$ and $Var(\\bar{X}_2) = \\frac{\\sigma^2}{n}$.\n    The difference in sample means is $D = \\bar{X}_1 - \\bar{X}_2$.\n    The expectation of the difference is $E[D] = E[\\bar{X}_1] - E[\\bar{X}_2] = \\mu_1 - \\mu_2$.\n    Since the groups are independent, the variance of the difference is the sum of the variances: $Var(D) = Var(\\bar{X}_1) + Var(\\bar{X}_2) = \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{n} = \\frac{2\\sigma^2}{n}$.\n    The standard error of the difference is $SE(D) = \\sqrt{\\frac{2\\sigma^2}{n}}$.\n    By the Central Limit Theorem and the problem's explicit assumption, the sampling distribution of $D$ is approximated by a normal distribution:\n    $$D \\sim N\\left(\\mu_1 - \\mu_2, \\frac{2\\sigma^2}{n}\\right)$$\n\n2.  **Rejection Region under the Null Hypothesis**:\n    The null hypothesis is $H_0: \\mu_1 - \\mu_2 = 0$. Under $H_0$, the distribution of the difference is $D \\sim N\\left(0, \\frac{2\\sigma^2}{n}\\right)$.\n    The test statistic is formed by standardizing $D$ under $H_0$:\n    $$Z = \\frac{\\bar{X}_1 - \\bar{X}_2}{SE(D)} = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sigma\\sqrt{2/n}}$$\n    Under $H_0$, $Z \\sim N(0,1)$.\n    For a two-sided test at significance level $\\alpha$, we reject $H_0$ if the observed statistic is in the tails of the null distribution. Let $z_{\\alpha/2}$ be the upper $\\alpha/2$ critical value of the standard normal distribution, defined by $P(Z > z_{\\alpha/2}) = \\alpha/2$. By symmetry, $P(Z < -z_{\\alpha/2}) = \\alpha/2$.\n    The rejection region is thus defined by $|Z| > z_{\\alpha/2}$. In terms of the difference in means $D$, we reject $H_0$ if:\n    $$|D| > z_{\\alpha/2} \\sigma\\sqrt{\\frac{2}{n}}$$\n\n3.  **Power under the Alternative Hypothesis**:\n    Power is the probability of correctly rejecting $H_0$ when the alternative hypothesis $H_1$ is true. The power is $1-\\beta$. We consider a specific alternative where the true difference has magnitude $\\delta$, so $|\\mu_1 - \\mu_2| = \\delta$. Without loss of generality, let's assume $\\mu_1 - \\mu_2 = \\delta$ where $\\delta > 0$. Under this specific alternative, the distribution of the difference in means is $D \\sim N\\left(\\delta, \\frac{2\\sigma^2}{n}\\right)$.\n    Power is the probability that $D$ falls into the rejection region, given this alternative distribution:\n    $$1-\\beta = P\\left(|D| > z_{\\alpha/2} \\sigma\\sqrt{\\frac{2}{n}} \\;\\middle|\\; \\mu_1 - \\mu_2 = \\delta\\right)$$\n    $$1-\\beta = P\\left(D > z_{\\alpha/2} \\sigma\\sqrt{\\frac{2}{n}}\\right) + P\\left(D < -z_{\\alpha/2} \\sigma\\sqrt{\\frac{2}{n}}\\right)$$\n    To evaluate these probabilities, we standardize $D$ under the alternative hypothesis. Let $Z' = \\frac{D - \\delta}{\\sigma\\sqrt{2/n}}$, where $Z' \\sim N(0,1)$.\n    $$1-\\beta = P\\left(\\frac{D - \\delta}{\\sigma\\sqrt{2/n}} > \\frac{z_{\\alpha/2} \\sigma\\sqrt{2/n} - \\delta}{\\sigma\\sqrt{2/n}}\\right) + P\\left(\\frac{D - \\delta}{\\sigma\\sqrt{2/n}} < \\frac{-z_{\\alpha/2} \\sigma\\sqrt{2/n} - \\delta}{\\sigma\\sqrt{2/n}}\\right)$$\n    $$1-\\beta = P\\left(Z' > z_{\\alpha/2} - \\frac{\\delta}{\\sigma\\sqrt{2/n}}\\right) + P\\left(Z' < -z_{\\alpha/2} - \\frac{\\delta}{\\sigma\\sqrt{2/n}}\\right)$$\n    For a well-powered study, the mean under the alternative, $\\delta$, is located far enough in the right tail of the null distribution that the probability of observing a result in the opposite (left) tail is negligible. That is, the term $P\\left(Z' < -z_{\\alpha/2} - \\frac{\\delta}{\\sigma\\sqrt{2/n}}\\right)$ is very close to $0$. We proceed with the standard approximation:\n    $$1-\\beta \\approx P\\left(Z' > z_{\\alpha/2} - \\frac{\\delta}{\\sigma\\sqrt{2/n}}\\right)$$\n    Let $z_{\\beta}$ be the upper $\\beta$ critical value of the standard normal distribution, i.e., $P(Z' > z_{\\beta}) = \\beta$. By symmetry, $P(Z' > -z_{\\beta}) = 1-\\beta$. To achieve power of $1-\\beta$, the lower bound of the probability integral must be equal to $-z_{\\beta}$:\n    $$z_{\\alpha/2} - \\frac{\\delta}{\\sigma\\sqrt{2/n}} = -z_{\\beta}$$\n    Now, we solve for the per-arm sample size $n$:\n    $$z_{\\alpha/2} + z_{\\beta} = \\frac{\\delta}{\\sigma\\sqrt{2/n}}$$\n    $$\\sqrt{n} = \\frac{\\sigma\\sqrt{2}(z_{\\alpha/2} + z_{\\beta})}{\\delta}$$\n    Squaring both sides gives the analytical expression for $n$:\n    $$n = \\frac{2\\sigma^2(z_{\\alpha/2} + z_{\\beta})^2}{\\delta^2}$$\n    This expression provides the minimum sample size per arm required to detect a true difference of magnitude $\\delta$ with power $1-\\beta$ using a two-sided test at significance level $\\alpha$.\n\n### Part 2: Calculation for the Specific Design Values\n\nWe are given:\n- $\\alpha = 0.05$, so $\\alpha/2 = 0.025$. The corresponding critical value is $z_{0.025} \\approx 1.960$.\n- $1-\\beta = 0.90$, so $\\beta = 0.10$. The corresponding critical value is $z_{0.10} \\approx 1.282$.\n- $\\delta = 5$.\n- $\\sigma^2 = 64$.\n\nSubstituting these values into the derived formula for $n$:\n$$n = \\frac{2(64)(1.960 + 1.282)^2}{5^2}$$\n$$n = \\frac{128(3.242)^2}{25}$$\n$$n = \\frac{128(10.510564)}{25}$$\n$$n = \\frac{1345.352192}{25}$$\n$$n \\approx 53.814$$\n\nSince the sample size $n$ must be an integer and the calculated value represents the minimum requirement, we must take the ceiling of this result to ensure the power is at least $0.90$. A fractional subject cannot be recruited, and rounding down to $n=53$ would result in a power slightly below the target of $0.90$.\n\nTherefore, the minimum integer sample size required per arm is $54$.", "answer": "$$\\boxed{54}$$", "id": "5057046"}, {"introduction": "Beyond sound design, rigorous study execution is critical for generating trustworthy evidence, and blinding is a key procedure to prevent bias. When outcome assessors are aware of a participant's treatment allocation, their judgments can be unconsciously swayed, leading to biased effect estimates. This practice [@problem_id:5057005] provides a powerful quantitative framework, combining analytic derivation and computational simulation, to demonstrate precisely how differential misclassification from unblinded assessment can inflate or distort treatment effects, making a non-efficacious treatment appear effective.", "problem": "In translational medicine, randomized controlled trials (RCTs) require rigorous outcome assessment to ensure research integrity, rigor, and reproducibility. Consider a two-arm RCT with a binary clinical outcome, such as success or failure of a therapeutic intervention. Let the treatment arm be denoted by $T$ and the control arm by $C$. The true probability of outcome in arm $g \\in \\{T,C\\}$ is denoted by $p_g$, where $p_g \\in [0,1]$. An outcome assessor may classify the binary outcome with sensitivity $Se_g \\in [0,1]$ and specificity $Sp_g \\in [0,1]$, which may differ between arms when blinding is absent, leading to differential misclassification. All rates and probabilities in this problem must be treated and expressed as decimals between $0$ and $1$.\n\nStarting only from the fundamental definitions of sensitivity, specificity, conditional probability, and the law of total probability, derive an analytic expression for the expected observed outcome probability in each arm under assessor misclassification. Using this, derive expressions for the expected observed risk difference and the expected observed risk ratio, and then define two bias measures: additive bias on risk difference as the difference between the expected observed risk difference and the true risk difference, and multiplicative relative bias on the risk ratio as the ratio of the expected observed risk ratio to the true risk ratio. A bias that inflates the effect estimate corresponds to an additive bias that increases the risk difference or a multiplicative relative bias greater than $1$ for the risk ratio.\n\nAdditionally, implement a Monte Carlo simulation that, for given sample sizes $n_T$ and $n_C$, and a number of replicates $R$, simulates the trial process as follows in each replicate: draw the number of true outcomes in each arm from a binomial distribution using $p_T$ and $p_C$; then apply misclassification using $Se_T$, $Sp_T$, $Se_C$, and $Sp_C$ to obtain observed outcomes; compute the observed risk difference and risk ratio; and compute the replicate-level biases relative to the true effect. Average the replicate-level biases over replicates to obtain Monte Carlo estimates of the additive bias and multiplicative relative bias. When computing ratios, ensure divisions by zero are avoided by excluding any replicate where a denominator equals $0$. All outputs must be dimensionless decimals.\n\nYour program must implement both the analytic bias calculation and the Monte Carlo bias estimation for the following test suite of parameter values. For each test case, parameters are given as $(p_T,p_C,Se_T,Sp_T,Se_C,Sp_C,n_T,n_C,R)$:\n\n- Case $1$ (general differential misclassification): $(0.30,0.20,0.92,0.92,0.85,0.95,2000,2000,4000)$.\n- Case $2$ (perfect assessment, no bias): $(0.30,0.20,1.00,1.00,1.00,1.00,2000,2000,2000)$.\n- Case $3$ (non-differential misclassification): $(0.30,0.20,0.85,0.85,0.85,0.85,3000,3000,4000)$.\n- Case $4$ (rare outcome with treatment having lower specificity): $(0.08,0.05,0.90,0.80,0.88,0.92,5000,5000,4000)$.\n- Case $5$ (extreme differential misclassification): $(0.30,0.20,0.60,0.60,0.95,0.95,3000,3000,4000)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output four floats in the following order: the analytic expected additive bias on the risk difference, the analytic expected multiplicative relative bias on the risk ratio, the Monte Carlo estimated additive bias on the risk difference, and the Monte Carlo estimated multiplicative relative bias on the risk ratio. Concatenate these four outputs across the five test cases in the given order, so the final output should contain $20$ floats formatted as a single list like $[b_{RD,1},RB_{RR,1},\\hat b_{RD,1},\\widehat{RB}_{RR,1},\\dots,b_{RD,5},RB_{RR,5},\\hat b_{RD,5},\\widehat{RB}_{RR,5}]$.", "solution": "The problem is assessed as valid. It is scientifically grounded in the principles of biostatistics and epidemiology for randomized controlled trials, is well-posed with a clear objective and sufficient information, and uses objective, formal language. It is free of any of the invalidating flaws listed in the instructions.\n\n### 1. Analytic Derivation of Bias Expressions\n\nThis section derives the analytic expressions for the expected additive bias on the risk difference and the expected multiplicative relative bias on the risk ratio, starting from fundamental definitions.\n\nLet $g \\in \\{T, C\\}$ denote the trial arm, where $T$ is the treatment arm and $C$ is the control arm. Let $O_g$ be the event of a true positive outcome (e.g., therapeutic success) in arm $g$, and $\\bar{O}_g$ be the event of a true negative outcome. The true probability of a positive outcome is given as $p_g = P(O_g)$, which implies $P(\\bar{O}_g) = 1 - p_g$.\n\nThe outcome assessment is imperfect. Let $\\hat{O}_g$ be the event of an observed positive outcome. The quality of assessment is defined by sensitivity ($Se_g$) and specificity ($Sp_g$):\n- Sensitivity: $Se_g = P(\\hat{O}_g | O_g)$, the probability of observing a positive outcome given a true positive outcome.\n- Specificity: $Sp_g = P(\\bar{\\hat{O}}_g | \\bar{O}_g)$, the probability of observing a negative outcome given a true negative outcome.\n\nFrom the definition of specificity, we can find the probability of a false positive, which is the probability of observing a positive outcome given a true negative outcome:\n$$P(\\hat{O}_g | \\bar{O}_g) = 1 - P(\\bar{\\hat{O}}_g | \\bar{O}_g) = 1 - Sp_g$$\n\nThe expected observed outcome probability in arm $g$, denoted $\\hat{p}_g = P(\\hat{O}_g)$, can be derived using the law of total probability:\n$$\\hat{p}_g = P(\\hat{O}_g) = P(\\hat{O}_g | O_g)P(O_g) + P(\\hat{O}_g | \\bar{O}_g)P(\\bar{O}_g)$$\nSubstituting the defined terms, we obtain the expression for the observed probability in arm $g$:\n$$\\hat{p}_g = (Se_g)(p_g) + (1 - Sp_g)(1 - p_g)$$\nThis expression can be rearranged to show a linear relationship: $\\hat{p}_g = (Se_g + Sp_g - 1)p_g + (1 - Sp_g)$.\n\nWith this foundation, we can define the true and expected observed effect measures and subsequently the bias terms.\n\n**Risk Difference (RD)**\n- True Risk Difference: $RD_{true} = p_T - p_C$\n- Expected Observed Risk Difference: $E[RD_{obs}] = \\hat{p}_T - \\hat{p}_C$\n- Additive Bias on Risk Difference ($b_{RD}$): This is the difference between the expected observed risk difference and the true risk difference.\n$$b_{RD} = E[RD_{obs}] - RD_{true} = (\\hat{p}_T - \\hat{p}_C) - (p_T - p_C)$$\n\n**Risk Ratio (RR)**\n- True Risk Ratio: $RR_{true} = p_T / p_C$ (assuming $p_C > 0$)\n- Expected Observed Risk Ratio: $E[RR_{obs}] = \\hat{p}_T / \\hat{p}_C$ (assuming $\\hat{p}_C > 0$)\n- Multiplicative Relative Bias on Risk Ratio ($RB_{RR}$): This is the ratio of the expected observed risk ratio to the true risk ratio.\n$$RB_{RR} = \\frac{E[RR_{obs}]}{RR_{true}} = \\frac{\\hat{p}_T / \\hat{p}_C}{p_T / p_C}$$\n\nThese analytic expressions allow for the direct calculation of the expected bias given the true probabilities and the assessor's performance characteristics.\n\n### 2. Monte Carlo Simulation Methodology\n\nThe Monte Carlo simulation estimates the biases by modeling the stochastic process of a trial over a large number of replicates, $R$. For each replicate, the simulation follows these steps:\n\n1.  **Simulate True Outcomes**: For each arm $g \\in \\{T,C\\}$ with sample size $n_g$ and true outcome probability $p_g$, the number of true positive outcomes, $k_g$, is drawn from a binomial distribution:\n    $$k_g \\sim \\text{Binomial}(n_g, p_g)$$\n    The number of true negative outcomes is then $n_g - k_g$.\n\n2.  **Simulate Outcome Misclassification**: For each arm $g$, the number of observed positive outcomes is determined by simulating the classification process for both true positives and true negatives:\n    - Number of correctly identified true positives ($TP_g$): $TP_g \\sim \\text{Binomial}(k_g, Se_g)$.\n    - Number of incorrectly identified true negatives (false positives, $FP_g$): $FP_g \\sim \\text{Binomial}(n_g - k_g, 1 - Sp_g)$.\n    The total number of observed positive outcomes in arm $g$ for the replicate is $\\hat{k}_g = TP_g + FP_g$.\n\n3.  **Compute Replicate-Level Observed Effects**: The observed outcome probabilities for the replicate are $\\hat{p}_{T} = \\hat{k}_T / n_T$ and $\\hat{p}_{C} = \\hat{k}_C / n_C$. The observed effect measures for this replicate are:\n    - Observed Risk Difference: $RD_{obs} = \\hat{p}_T - \\hat{p}_C$.\n    - Observed Risk Ratio: $RR_{obs} = \\hat{p}_T / \\hat{p}_C$. This calculation is performed only if $\\hat{k}_C > 0$ to avoid division by zero.\n\n4.  **Compute Replicate-Level Biases**: The biases for the replicate are calculated relative to the true effect measures:\n    - Additive Bias: $b_{RD,i} = RD_{obs,i} - RD_{true}$.\n    - Multiplicative Relative Bias: $RB_{RR,i} = RR_{obs,i} / RR_{true}$. This is computed only for replicates where $RR_{obs,i}$ is valid.\n\n5.  **Average Biases**: After completing all $R$ replicates, the Monte Carlo estimates of the biases are the averages of the replicate-level biases.\n    - Estimated Additive Bias: $\\hat{b}_{RD} = \\frac{1}{R} \\sum_{i=1}^{R} b_{RD,i}$.\n    - Estimated Multiplicative Relative Bias: $\\widehat{RB}_{RR} = \\frac{1}{R'} \\sum_{i \\in \\text{valid}} RB_{RR,i}$, where $R'$ is the count of replicates for which $RR_{obs,i}$ was calculable.\n\nThis simulation process provides an empirical estimate of the biases, which converges to the analytically derived expected values as $R \\to \\infty$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates analytic and Monte Carlo estimates of bias in a two-arm RCT\n    with differential outcome misclassification for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Parameters: (p_T, p_C, Se_T, Sp_T, Se_C, Sp_C, n_T, n_C, R)\n    test_cases = [\n        (0.30, 0.20, 0.92, 0.92, 0.85, 0.95, 2000, 2000, 4000), # Case 1\n        (0.30, 0.20, 1.00, 1.00, 1.00, 1.00, 2000, 2000, 2000), # Case 2\n        (0.30, 0.20, 0.85, 0.85, 0.85, 0.85, 3000, 3000, 4000), # Case 3\n        (0.08, 0.05, 0.90, 0.80, 0.88, 0.92, 5000, 5000, 4000), # Case 4\n        (0.30, 0.20, 0.60, 0.60, 0.95, 0.95, 3000, 3000, 4000), # Case 5\n    ]\n\n    all_results = []\n    # Use a seeded random number generator for reproducibility\n    rng = np.random.default_rng(seed=42)\n\n    for case in test_cases:\n        p_T, p_C, Se_T, Sp_T, Se_C, Sp_C, n_T, n_C, R = case\n\n        # --- Analytic Bias Calculation ---\n        \n        # Expected observed outcome probabilities\n        p_hat_T = Se_T * p_T + (1 - Sp_T) * (1 - p_T)\n        p_hat_C = Se_C * p_C + (1 - Sp_C) * (1 - p_C)\n\n        # True effect measures\n        rd_true = p_T - p_C\n        rr_true = p_T / p_C if p_C > 0 else np.nan\n\n        # Expected observed effect measures\n        rd_obs_exp = p_hat_T - p_hat_C\n        rr_obs_exp = p_hat_T / p_hat_C if p_hat_C > 0 else np.nan\n\n        # Analytic bias measures\n        analytic_additive_bias = rd_obs_exp - rd_true\n        analytic_mult_rel_bias = rr_obs_exp / rr_true if rr_true > 0 and not np.isnan(rr_true) else np.nan\n\n        # --- Monte Carlo Simulation ---\n        \n        # Vectorized simulation for efficiency\n        # Step 1: Simulate true outcomes for all R replicates\n        k_T_reps = rng.binomial(n_T, p_T, size=R)\n        k_C_reps = rng.binomial(n_C, p_C, size=R)\n\n        # Step 2: Simulate misclassification\n        # Treatment arm\n        tp_T_reps = rng.binomial(k_T_reps, Se_T)\n        fp_T_reps = rng.binomial(n_T - k_T_reps, 1 - Sp_T)\n        k_hat_T_reps = tp_T_reps + fp_T_reps\n        \n        # Control arm\n        tp_C_reps = rng.binomial(k_C_reps, Se_C)\n        fp_C_reps = rng.binomial(n_C - k_C_reps, 1 - Sp_C)\n        k_hat_C_reps = tp_C_reps + fp_C_reps\n\n        # Step 3 & 4: Compute replicate-level biases\n        # Observed probabilities\n        p_hat_T_reps = k_hat_T_reps / n_T\n        p_hat_C_reps = k_hat_C_reps / n_C\n        \n        # Additive bias on Risk Difference\n        rd_obs_reps = p_hat_T_reps - p_hat_C_reps\n        replicate_additive_biases = rd_obs_reps - rd_true\n        \n        # Multiplicative relative bias on Risk Ratio\n        # Filter out replicates where the denominator (observed control rate) is zero\n        valid_rr_mask = k_hat_C_reps > 0\n        \n        # Avoid division by zero for rr_true as well (though not an issue with test data)\n        if rr_true > 0 and not np.isnan(rr_true) and np.any(valid_rr_mask):\n            p_hat_T_valid = p_hat_T_reps[valid_rr_mask]\n            p_hat_C_valid = p_hat_C_reps[valid_rr_mask]\n            rr_obs_valid_reps = p_hat_T_valid / p_hat_C_valid\n            replicate_mult_rel_biases = rr_obs_valid_reps / rr_true\n            mc_mult_rel_bias = np.mean(replicate_mult_rel_biases)\n        else:\n            mc_mult_rel_bias = np.nan # Or other placeholder if all replicates invalid\n\n        # Step 5: Average biases\n        mc_additive_bias = np.mean(replicate_additive_biases)\n        \n        all_results.extend([\n            analytic_additive_bias, \n            analytic_mult_rel_bias, \n            mc_additive_bias, \n            mc_mult_rel_bias\n        ])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in all_results)}]\")\n\nsolve()\n```", "id": "5057005"}, {"introduction": "In the era of complex, high-dimensional data, the integrity of the analysis pipeline is paramount for reproducibility. A subtle but catastrophic error is 'data leakage,' where information from the test set inadvertently contaminates the training process, leading to overly optimistic and non-generalizable model performance. This exercise [@problem_id:5057006] presents a common but flawed machine learning workflow, tasking you with diagnosing the source of leakage and designing a methodologically sound cross-validation procedure that preserves the independence of the test data.", "problem": "A translational medicine team is building a classifier to predict therapeutic response ($y \\in \\{0,1\\}$) from baseline multi-omics features ($x \\in \\mathbb{R}^p$) collected on $n$ patients. They intend to estimate generalization performance using $k$-fold Cross-Validation (CV), where in each fold a model $f_i$ is trained on the training subset $S_i^{\\text{train}}$ and evaluated on the held-out subset $S_i^{\\text{test}}$. Before creating folds, the team standardizes each feature across all $n$ patients using a transformation $T_{\\hat{\\theta}}(x) = (x - \\hat{\\mu}) / \\hat{\\sigma}$, where $\\hat{\\mu}$ and $\\hat{\\sigma}$ are the sample mean and standard deviation computed over the entire dataset $D$, and then performs CV on the normalized data.\n\nAnswer the following diagnosis-and-design question from first principles, using the following base definitions and well-tested facts:\n- The CV estimator $\\hat{R}_{\\text{CV}}$ of the expected loss $R = \\mathbb{E}_{(X,Y) \\sim P}[\\ell(f(X),Y)]$ relies on independence of the test data from the training procedure: for fold $i$, $S_i^{\\text{test}}$ must be excluded from any operation that induces parameters used by $f_i$.\n- A data-dependent preprocessing transformation $T_{\\theta}$ with parameters $\\theta$ is part of the training procedure if $\\theta$ is estimated from data.\n- In translational medicine, repeated measures or shared batch effects may induce dependence across samples; preserving independence requires that any estimation step does not use information from $S_i^{\\text{test}}$ for fold $i$.\n\nWhich option correctly diagnoses the issue and specifies a corrected workflow that preserves the independence of test folds?\n\nA. There is data leakage because $\\hat{\\mu}$ and $\\hat{\\sigma}$ are estimated using all $n$ patients, which couples $S_i^{\\text{test}}$ to $S_i^{\\text{train}}$ via $T_{\\hat{\\theta}}$. The corrected workflow is: for each fold $i$, fit $\\hat{\\mu}_{\\text{train},i}$ and $\\hat{\\sigma}_{\\text{train},i}$ using only $S_i^{\\text{train}}$, transform both $S_i^{\\text{train}}$ and $S_i^{\\text{test}}$ by $T_{\\hat{\\theta}_{\\text{train},i}}$, train $f_i$ on the transformed $S_i^{\\text{train}}$, and evaluate on the transformed $S_i^{\\text{test}}$. If hyperparameters are tuned, perform nested CV within $S_i^{\\text{train}}$ with the scaler fitted anew in each inner split.\n\nB. There is no leakage because the normalization is unsupervised (it does not use $y$) and feature-wise standardization is a monotonic transform; therefore applying $T_{\\hat{\\theta}}$ globally before CV is acceptable.\n\nC. There is potential leakage, but it can be eliminated by shuffling the labels $y$ after global normalization and then computing the Area Under the Receiver Operating Characteristic Curve (AUROC); shuffling removes dependence between $T_{\\hat{\\theta}}$ and test folds.\n\nD. There is data leakage, but the corrected workflow is to compute $\\hat{\\mu}$ and $\\hat{\\sigma}$ using only $S_i^{\\text{test}}$ for each fold to avoid bias, then transform $S_i^{\\text{train}}$ and $S_i^{\\text{test}}$ using those test-derived parameters before training and evaluation.\n\nE. There is data leakage, but it can be mitigated by normalizing across the entire dataset and then performing leave-one-feature-out CV so that each model never sees all features at once; this preserves independence of test folds without changing the normalization strategy.", "solution": "The question asks to identify the methodological flaw in a proposed cross-validation (CV) procedure and to specify the correct workflow. The core of the problem lies in estimating the generalization performance of a predictive model, which requires that the test data in each fold of CV remain entirely unseen by the training procedure.\n\n**1. Problem Validation**\n\nThe problem statement is valid. It describes a common and critical error in machine learning pipelines known as data leakage or information leakage. The provided definitions concerning the CV estimator, the nature of a data-dependent transformation, and the importance of independence in translational medicine are scientifically sound and form a consistent basis for reasoning. The problem is well-posed, objective, and directly relevant to the principles of rigor and reproducibility in scientific research.\n\n**2. Analysis of the Proposed Workflow**\n\nThe stated goal is to estimate the expected loss $R = \\mathbb{E}_{(X,Y) \\sim P}[\\ell(f(X),Y)]$ using $k$-fold CV. The fundamental principle of CV is to estimate the performance of a complete learning pipeline on data that is independent of the data used to train the pipeline.\n\nThe proposed workflow is:\n1.  Compute the sample mean $\\hat{\\mu}$ and sample standard deviation $\\hat{\\sigma}$ for each feature using all $n$ patients in the dataset $D$. Let these parameters be $\\hat{\\theta} = (\\hat{\\mu}, \\hat{\\sigma})$.\n2.  Transform the entire dataset $D$ using the transformation $T_{\\hat{\\theta}}(x) = (x - \\hat{\\mu}) / \\hat{\\sigma}$.\n3.  Perform $k$-fold CV on this pre-transformed dataset.\n\nLet's analyze this procedure for a single fold $i$. The dataset is partitioned into a training set $S_i^{\\text{train}}$ and a test set $S_i^{\\text{test}}$. The transformation parameters $\\hat{\\theta}$ were calculated using data from all $n$ patients, which means $\\hat{\\theta}$ was computed using data from both $S_i^{\\text{train}}$ and $S_i^{\\text{test}}$.\n\nThe problem states that \"a data-dependent preprocessing transformation $T_{\\theta}$ with parameters $\\theta$ is part of the training procedure if $\\theta$ is estimated from data.\" In this case, the standardization is a data-dependent transformation, and its parameters $\\hat{\\theta}$ are estimated from data.\n\nThe problem also states a critical requirement: \"for fold $i$, $S_i^{\\text{test}}$ must be excluded from any operation that induces parameters used by $f_i$.\" The model $f_i$ is trained on data that has been transformed by $T_{\\hat{\\theta}}$. Because the parameters $\\hat{\\theta}$ depend on $S_i^{\\text{test}}$, the data used to train $f_i$ is no longer independent of the test set. Information from the test set has \"leaked\" into the training process. This violates the core assumption of CV, leading to an optimistically biased estimate of generalization performance. The model has, in effect, been given a \"peek\" at the test data's statistical properties.\n\n**3. The Corrected Workflow**\n\nTo ensure the independence of the test set, the entire learning pipeline, including any data-dependent preprocessing, must be fitted *only* on the training data for each fold. The test set should then be treated as new, unseen data to which the fully-fitted pipeline is applied.\n\nThe corrected workflow for each fold $i$ ($i \\in \\{1, \\dots, k\\}$) should be:\n1.  Partition the original, untransformed dataset $D$ into $S_i^{\\text{train}}$ and $S_i^{\\text{test}}$.\n2.  Estimate the preprocessing parameters $\\hat{\\theta}_{\\text{train},i} = (\\hat{\\mu}_{\\text{train},i}, \\hat{\\sigma}_{\\text{train},i})$ using *only* the data in $S_i^{\\text{train}}$.\n3.  Apply the learned transformation $T_{\\hat{\\theta}_{\\text{train},i}}$ to *both* the training data and the test data. This creates a transformed training set and a transformed test set.\n4.  Train the classifier $f_i$ on the transformed training set.\n5.  Evaluate $f_i$ on the transformed test set.\n\nThis procedure is repeated for all $k$ folds, and the performance metrics are averaged. This ensures that for every fold, the test set is completely held out from every step of the model-building process, including the estimation of scaling parameters.\n\n**4. Evaluation of Options**\n\n**A. There is data leakage because $\\hat{\\mu}$ and $\\hat{\\sigma}$ are estimated using all $n$ patients, which couples $S_i^{\\text{test}}$ to $S_i^{\\text{train}}$ via $T_{\\hat{\\theta}}$. The corrected workflow is: for each fold $i$, fit $\\hat{\\mu}_{\\text{train},i}$ and $\\hat{\\sigma}_{\\text{train},i}$ using only $S_i^{\\text{train}}$, transform both $S_i^{\\text{train}}$ and $S_i^{\\text{test}}$ by $T_{\\hat{\\theta}_{\\text{train},i}}$, train $f_i$ on the transformed $S_i^{\\text{train}}$, and evaluate on the transformed $S_i^{\\text{test}}$. If hyperparameters are tuned, perform nested CV within $S_i^{\\text{train}}$ with the scaler fitted anew in each inner split.**\nThis option correctly diagnoses the problem as data leakage resulting from global normalization. The proposed correction aligns perfectly with the principles derived above: the scaler is fit on the training data of the fold and applied to both splits. The mention of nested CV and refitting the scaler in the inner loops is also correct, as it extends the principle of preventing leakage to the hyperparameter selection stage.\n**Verdict: Correct.**\n\n**B. There is no leakage because the normalization is unsupervised (it does not use $y$) and feature-wise standardization is a monotonic transform; therefore applying $T_{\\hat{\\theta}}$ globally before CV is acceptable.**\nThis is incorrect. The distinction between supervised and unsupervised methods is not the deciding factor for data leakage. Leakage occurs when information from the test set influences the training procedure. Here, the feature statistics ($\\hat{\\mu}, \\hat{\\sigma}$) from the test set are used to define the transformation applied to the training set. The monotonic nature of the transform is also irrelevant to the violation of test-set independence.\n**Verdict: Incorrect.**\n\n**C. There is potential leakage, but it can be eliminated by shuffling the labels $y$ after global normalization and then computing the Area Under the Receiver Operating Characteristic Curve (AUROC); shuffling removes dependence between $T_{\\hat{\\theta}}$ and test folds.**\nThis is incorrect. Shuffling labels is a technique used for permutation testing to assess the statistical significance of a result, not to correct for data leakage. The leakage in this problem occurs through the features ($x$), not the labels ($y$). Shuffling labels would break the true relationship between features and outcome, and the resulting performance metric would estimate the model's performance on random data, not its true generalization performance. It does not fix the original flaw.\n**Verdict: Incorrect.**\n\n**D. There is data leakage, but the corrected workflow is to compute $\\hat{\\mu}$ and $\\hat{\\sigma}$ using only $S_i^{\\text{test}}$ for each fold to avoid bias, then transform $S_i^{\\text{train}}$ and $S_i^{\\text{test}}$ using those test-derived parameters before training and evaluation.**\nThis is incorrect. While it correctly identifies leakage, the proposed solution is fundamentally flawed. It suggests fitting the scaler on the test data. This is an extreme form of data leakage. The training process should simulate learning from available data to predict on future, unseen data. One never has access to the statistical properties of future data. The training pipeline must be derived solely from the training set.\n**Verdict: Incorrect.**\n\n**E. There is data leakage, but it can be mitigated by normalizing across the entire dataset and then performing leave-one-feature-out CV so that each model never sees all features at once; this preserves independence of test folds without changing the normalization strategy.**\nThis is incorrect. The concept of \"leave-one-feature-out CV\" is not a standard method to address sample-based data leakage. The leakage is about information from *test samples* (patients) influencing the model training, not about which *features* are used. Even if features are left out, the normalization parameters for the remaining features are still calculated using all $n$ samples, so the leakage persists. The independence of test *folds* (subsets of samples) is not preserved.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "5057006"}]}