{"hands_on_practices": [{"introduction": "This practice grounds our exploration in a fundamental task of translational science: evaluating the performance of a diagnostic or prognostic tool. Before a biomarker can be used to guide treatment or enroll patients in a trial, we must understand how well it predicts the true state of interest. This exercise [@problem_id:5069432] will guide you through calculating and interpreting key performance metrics, revealing how a test's real-world utility is critically dependent on the context of the population in which it is used.", "problem": "A translational research team is validating a blood-based biomarker assay intended for early detection of a treatment-limiting toxicity in oncology patients. In the target clinical-use population, the prevalence of true toxicity at the time of screening is estimated at $0.05$. A prior multi-center analytic validation suggests the assay’s clinical sensitivity is $0.9$ and its clinical specificity is $0.95$ in the intended-use setting. The team’s working hypothesis is that the assay will have clinically actionable rule-in and rule-out performance in this population; to interrogate this hypothesis within the scientific method, they must quantify how predictive values depend on disease frequency in the target population.\n\nUsing only the foundational rules of conditional probability, define in mathematical terms the following quantities for a binary disease state $D \\in \\{\\text{present}, \\text{absent}\\}$ and a binary test result $T \\in \\{T^{+}, T^{-}\\}$:\n- sensitivity,\n- specificity,\n- positive predictive value (PPV),\n- negative predictive value (NPV).\n\nThen, derive expressions for PPV and NPV in terms of sensitivity, specificity, and disease prevalence. Finally, compute PPV and NPV for sensitivity $0.9$, specificity $0.95$, and prevalence $0.05$. Express both PPV and NPV as decimals (not percentages), rounded to four significant figures, and report the final numerical answers in the order $\\text{PPV}, \\text{NPV}$ as a single row vector.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It presents a standard application of conditional probability and Bayes' theorem to the evaluation of a diagnostic test's performance characteristics, a fundamental task in biostatistics and translational medicine. All necessary data (prevalence, sensitivity, specificity) are provided, and the questions are unambiguous. Therefore, the problem is valid, and we may proceed with the solution.\n\nLet $D^{+}$ be the event that a patient has the true toxicity (disease is present), and $D^{-}$ be the event that the patient does not have the toxicity (disease is absent). Let $T^{+}$ be the event of a positive assay result, and $T^{-}$ be the event of a negative assay result. The prevalence of the disease, denoted by $p$, is the prior probability of a patient having the toxicity, so $p = P(D^{+})$. From the problem statement, we are given $p = 0.05$. Consequently, the probability of a patient not having the toxicity is $P(D^{-}) = 1 - p = 1 - 0.05 = 0.95$.\n\nFirst, we will formally define the required quantities in terms of conditional probabilities.\n\n1.  **Sensitivity (Sens)**: The probability of a positive test result given that the disease is present. This is the true positive rate.\n    $$ \\text{Sens} = P(T^{+} | D^{+}) $$\n    We are given that the sensitivity is $0.9$.\n\n2.  **Specificity (Spec)**: The probability of a negative test result given that the disease is absent. This is the true negative rate.\n    $$ \\text{Spec} = P(T^{-} | D^{-}) $$\n    We are given that the specificity is $0.95$.\n\n3.  **Positive Predictive Value (PPV)**: The probability that the disease is present given a positive test result. This is a post-test probability.\n    $$ \\text{PPV} = P(D^{+} | T^{+}) $$\n\n4.  **Negative Predictive Value (NPV)**: The probability that the disease is absent given a negative test result. This is also a post-test probability.\n    $$ \\text{NPV} = P(D^{-} | T^{-}) $$\n\nNext, we derive the expressions for $\\text{PPV}$ and $\\text{NPV}$ using the foundational rules of conditional probability, specifically Bayes' theorem. Bayes' theorem states that for two events $A$ and $B$, $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$.\n\nTo derive the expression for $\\text{PPV} = P(D^{+} | T^{+})$, we apply Bayes' theorem:\n$$ \\text{PPV} = P(D^{+} | T^{+}) = \\frac{P(T^{+} | D^{+}) P(D^{+})}{P(T^{+})} $$\nThe term in the numerator, $P(T^{+} | D^{+})$, is the sensitivity ($\\text{Sens}$), and $P(D^{+})$ is the prevalence ($p$).\nThe denominator, $P(T^{+})$, is the total probability of a positive test result. We can expand this using the law of total probability:\n$$ P(T^{+}) = P(T^{+} | D^{+}) P(D^{+}) + P(T^{+} | D^{-}) P(D^{-}) $$\nWe know $P(T^{+} | D^{+}) = \\text{Sens}$ and $P(D^{+}) = p$. We also know $P(D^{-}) = 1-p$.\nThe term $P(T^{+} | D^{-})$ is the probability of a positive test given the disease is absent, which is the false positive rate. It is related to specificity by $P(T^{+} | D^{-}) = 1 - P(T^{-} | D^{-}) = 1 - \\text{Spec}$.\nSubstituting these into the expression for $P(T^{+})$ gives:\n$$ P(T^{+}) = (\\text{Sens})(p) + (1 - \\text{Spec})(1 - p) $$\nTherefore, the full expression for $\\text{PPV}$ is:\n$$ \\text{PPV} = \\frac{(\\text{Sens})(p)}{(\\text{Sens})(p) + (1 - \\text{Spec})(1 - p)} $$\n\nSimilarly, to derive the expression for $\\text{NPV} = P(D^{-} | T^{-})$, we apply Bayes' theorem:\n$$ \\text{NPV} = P(D^{-} | T^{-}) = \\frac{P(T^{-} | D^{-}) P(D^{-})}{P(T^{-})} $$\nThe term in the numerator, $P(T^{-} | D^{-})$, is the specificity ($\\text{Spec}$), and $P(D^{-})$ is $1-p$.\nThe denominator, $P(T^{-})$, is the total probability of a negative test result. By the law of total probability:\n$$ P(T^{-}) = P(T^{-} | D^{-}) P(D^{-}) + P(T^{-} | D^{+}) P(D^{+}) $$\nWe know $P(T^{-} | D^{-}) = \\text{Spec}$ and $P(D^{-}) = 1-p$. The term $P(T^{-} | D^{+})$ is the probability of a negative test given the disease is present, which is the false negative rate. It is related to sensitivity by $P(T^{-} | D^{+}) = 1 - P(T^{+} | D^{+}) = 1 - \\text{Sens}$.\nSubstituting these into the expression for $P(T^{-})$ gives:\n$$ P(T^{-}) = (\\text{Spec})(1 - p) + (1 - \\text{Sens})(p) $$\nTherefore, the full expression for $\\text{NPV}$ is:\n$$ \\text{NPV} = \\frac{(\\text{Spec})(1 - p)}{(\\text{Spec})(1 - p) + (1 - \\text{Sens})(p)} $$\n\nFinally, we compute the numerical values for $\\text{PPV}$ and $\\text{NPV}$ using the given data:\n- Prevalence, $p = 0.05$\n- Sensitivity, $\\text{Sens} = 0.9$\n- Specificity, $\\text{Spec} = 0.95$\n\nFor $\\text{PPV}$:\n$$ \\text{PPV} = \\frac{(0.9)(0.05)}{(0.9)(0.05) + (1 - 0.95)(1 - 0.05)} $$\n$$ \\text{PPV} = \\frac{0.045}{0.045 + (0.05)(0.95)} $$\n$$ \\text{PPV} = \\frac{0.045}{0.045 + 0.0475} = \\frac{0.045}{0.0925} \\approx 0.486486... $$\nRounding to four significant figures, $\\text{PPV} = 0.4865$.\n\nFor $\\text{NPV}$:\n$$ \\text{NPV} = \\frac{(0.95)(1 - 0.05)}{(0.95)(1 - 0.05) + (1 - 0.9)(0.05)} $$\n$$ \\text{NPV} = \\frac{(0.95)(0.95)}{(0.95)(0.95) + (0.1)(0.05)} $$\n$$ \\text{NPV} = \\frac{0.9025}{0.9025 + 0.005} = \\frac{0.9025}{0.9075} \\approx 0.994490... $$\nRounding to four significant figures, $\\text{NPV} = 0.9945$.\n\nThe computed values indicate that while the assay has an excellent rule-out performance (high $\\text{NPV}$ of $0.9945$), its rule-in performance is mediocre ($\\text{PPV}$ of $0.4865$), as less than half of the positive results will correspond to true cases of toxicity in this low-prevalence setting.\n\nThe final answers are reported in the order ($\\text{PPV}$, $\\text{NPV}$).", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.4865 & 0.9945 \\end{pmatrix}}\n$$", "id": "5069432"}, {"introduction": "Modern clinical trials often evaluate multiple endpoints to capture a comprehensive picture of a therapy's effects, from mechanistic biomarkers to diverse clinical outcomes. However, testing multiple hypotheses simultaneously increases the risk of making a false positive discovery by chance. This problem [@problem_id:5069379] tackles the critical issue of controlling this family-wise error rate, challenging you to compare two common correction methods and understand their different implications for statistical power.", "problem": "A translational medicine Phase II study evaluates a targeted therapy with $m=8$ pre-specified co-primary endpoints spanning mechanistic biomarkers and early clinical measures. The study team requires strong control of the Family-Wise Error Rate (FWER) at level $\\alpha=0.05$ under arbitrary dependence among endpoints. Two procedures are under consideration: the Bonferroni correction and the Holm step-down method. Using only foundational principles of FWER control (specifically, the Bonferroni inequality and sequentially rejective testing logic), do the following:\n\n1. Derive the adjusted per-endpoint significance thresholds implied by the Bonferroni correction.\n2. Derive the ordered sequence of adjusted significance thresholds used by the Holm step-down method when testing the ordered $p$-values $p_{(1)},\\dots,p_{(8)}$.\n3. To summarize and compare the implications for statistical power across the full sequence of tests, define the scalar\n$$\nR \\equiv \\frac{\\text{sum of Holm step-down thresholds across all }8\\text{ steps}}{\\text{sum of Bonferroni thresholds across all }8\\text{ tests}}.\n$$\nCompute $R$ exactly. Express your final numerical answer as a simplified fraction with no units.\n\nYou may assume standard two-sided hypothesis tests for each endpoint and that both procedures are implemented in their canonical forms. Do not use or cite any shortcut formulas; begin from the core FWER definition and generally accepted properties of these procedures. The final reported answer must be the single value of $R$ as specified above. No rounding is required, and no percentage signs should be used anywhere.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and internally consistent. All necessary parameters and definitions for a unique, meaningful solution are provided. The problem asks for the derivation of statistical thresholds and the computation of a ratio based on established principles of multiple hypothesis testing, specifically controlling the Family-Wise Error Rate (FWER). The context provided, a Phase II translational medicine study, is a standard application area for these methods. Therefore, the problem is deemed valid and a full solution can be constructed.\n\nThe Family-Wise Error Rate (FWER) is defined as the probability of making at least one Type I error (a false positive) among a family of $m$ hypothesis tests. Strong control of the FWER at level $\\alpha$ requires that this probability is no greater than $\\alpha$, regardless of which subset of the null hypotheses are true.\nLet $H_1, H_2, \\dots, H_m$ be the family of $m$ null hypotheses, and let $I_0 \\subseteq \\{1, 2, \\dots, m\\}$ be the set of indices corresponding to the true null hypotheses. A Type I error for test $i$ occurs if we reject $H_i$ when it is true. Let $V$ be the number of true null hypotheses that are incorrectly rejected. Strong control of FWER means that $P(V \\ge 1) \\le \\alpha$ for any $I_0$.\n\nThe problem specifies $m=8$ co-primary endpoints and a desired FWER control level of $\\alpha=0.05$.\n\n**1. Bonferroni Correction Thresholds**\n\nThe Bonferroni correction is based on the Bonferroni inequality, also known as Boole's inequality. For a set of events $A_1, \\dots, A_m$, this inequality states that the probability of their union is no greater than the sum of their individual probabilities:\n$$P\\left(\\bigcup_{i=1}^{m} A_i\\right) \\le \\sum_{i=1}^{m} P(A_i)$$\nLet the event of a Type I error for hypothesis $H_i$ be $E_i$. The FWER is the probability of at least one such error occurring for any of the true null hypotheses.\n$$FWER = P(V \\ge 1) = P\\left(\\bigcup_{i \\in I_0} E_i\\right) \\le \\sum_{i \\in I_0} P(E_i)$$\nTo ensure $FWER \\le \\alpha$, we can set an adjusted significance threshold, $\\alpha_{adj}$, for each individual hypothesis test such that $\\sum_{i \\in I_0} P(E_i) \\le \\alpha$. Under the null hypothesis $H_i$, the probability of a Type I error, $P(E_i)$, is controlled at the per-test significance level. If we reject $H_i$ when its p-value $p_i \\le \\alpha_{adj}$, then $P(E_i) \\le \\alpha_{adj}$.\nThe most straightforward way to guarantee FWER control is to set a uniform threshold $\\alpha_{adj}$ for all $m$ tests such that $m \\cdot \\alpha_{adj} = \\alpha$. This gives:\n$$\\alpha_{adj} = \\frac{\\alpha}{m}$$\nThis choice ensures that $\\sum_{i \\in I_0} P(E_i) \\le \\sum_{i \\in I_0} \\frac{\\alpha}{m} \\le \\sum_{i=1}^{m} \\frac{\\alpha}{m} = m \\left(\\frac{\\alpha}{m}\\right) = \\alpha$.\nThis holds irrespective of the dependence structure between the tests. For this problem, with $m=8$ and $\\alpha=0.05$, the Bonferroni correction implies a single, uniform adjusted significance threshold for each of the $8$ tests:\n$$\\alpha_{Bonf} = \\frac{\\alpha}{m} = \\frac{0.05}{8}$$\n\n**2. Holm Step-Down Method Thresholds**\n\nThe Holm method is a sequentially rejective step-down procedure that provides strong FWER control and is uniformly more powerful than the Bonferroni correction. The procedure is as follows:\nLet the $m$ p-values from the individual tests be ordered from smallest to largest: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$. Let the corresponding null hypotheses be $H_{(1)}, H_{(2)}, \\dots, H_{(m)}$.\n\nThe procedure tests these ordered p-values against a sequence of decreasingly stringent significance thresholds.\n- **Step 1:** Compare the smallest p-value, $p_{(1)}$, to the threshold $\\alpha_1 = \\frac{\\alpha}{m}$. If $p_{(1)} \\le \\frac{\\alpha}{m}$, reject $H_{(1)}$ and proceed to Step 2. Otherwise, stop and fail to reject any hypotheses.\n- **Step k:** If hypotheses $H_{(1)}, \\dots, H_{(k-1)}$ have been rejected, compare the $k$-th ordered p-value, $p_{(k)}$, to the threshold $\\alpha_k = \\frac{\\alpha}{m-k+1}$. If $p_{(k)} \\le \\frac{\\alpha}{m-k+1}$, reject $H_{(k)}$ and proceed to Step $k+1$. Otherwise, stop and fail to reject $H_{(k)}$ and all subsequent hypotheses $H_{(k+1)}, \\dots, H_{(m)}$.\n\nFor the given problem with $m=8$, the ordered sequence of adjusted significance thresholds, denoted $\\alpha_{Holm,k}$ for $k=1, \\dots, 8$, are:\n- For $p_{(1)}$: $\\alpha_{Holm,1} = \\frac{\\alpha}{8-1+1} = \\frac{\\alpha}{8}$\n- For $p_{(2)}$: $\\alpha_{Holm,2} = \\frac{\\alpha}{8-2+1} = \\frac{\\alpha}{7}$\n- For $p_{(3)}$: $\\alpha_{Holm,3} = \\frac{\\alpha}{8-3+1} = \\frac{\\alpha}{6}$\n- For $p_{(4)}$: $\\alpha_{Holm,4} = \\frac{\\alpha}{8-4+1} = \\frac{\\alpha}{5}$\n- For $p_{(5)}$: $\\alpha_{Holm,5} = \\frac{\\alpha}{8-5+1} = \\frac{\\alpha}{4}$\n- For $p_{(6)}$: $\\alpha_{Holm,6} = \\frac{\\alpha}{8-6+1} = \\frac{\\alpha}{3}$\n- For $p_{(7)}$: $\\alpha_{Holm,7} = \\frac{\\alpha}{8-7+1} = \\frac{\\alpha}{2}$\n- For $p_{(8)}$: $\\alpha_{Holm,8} = \\frac{\\alpha}{8-8+1} = \\frac{\\alpha}{1} = \\alpha$\n\n**3. Computation of the Ratio R**\n\nThe ratio $R$ is defined as:\n$$R \\equiv \\frac{\\text{sum of Holm step-down thresholds across all }8\\text{ steps}}{\\text{sum of Bonferroni thresholds across all }8\\text{ tests}}$$\nFirst, we compute the denominator, which is the sum of the Bonferroni thresholds. Since the threshold is the same for all $m=8$ tests:\n$$\\text{Sum of Bonferroni thresholds} = \\sum_{k=1}^{8} \\alpha_{Bonf} = \\sum_{k=1}^{8} \\frac{\\alpha}{8} = 8 \\times \\frac{\\alpha}{8} = \\alpha$$\nNext, we compute the numerator, which is the sum of the sequence of Holm step-down thresholds:\n$$\\text{Sum of Holm thresholds} = \\sum_{k=1}^{8} \\alpha_{Holm,k} = \\sum_{k=1}^{8} \\frac{\\alpha}{8-k+1}$$\nThis sum can be written as:\n$$\\frac{\\alpha}{8} + \\frac{\\alpha}{7} + \\frac{\\alpha}{6} + \\frac{\\alpha}{5} + \\frac{\\alpha}{4} + \\frac{\\alpha}{3} + \\frac{\\alpha}{2} + \\frac{\\alpha}{1} = \\alpha \\left( \\frac{1}{8} + \\frac{1}{7} + \\frac{1}{6} + \\frac{1}{5} + \\frac{1}{4} + \\frac{1}{3} + \\frac{1}{2} + 1 \\right)$$\nThis sum is $\\alpha$ multiplied by the $8$-th harmonic number, $H_8$.\n$$H_8 = \\sum_{j=1}^{8} \\frac{1}{j}$$\nNow, we can compute the ratio $R$:\n$$R = \\frac{\\sum_{k=1}^{8} \\alpha_{Holm,k}}{\\sum_{k=1}^{8} \\alpha_{Bonf}} = \\frac{\\alpha \\sum_{j=1}^{8} \\frac{1}{j}}{\\alpha} = \\sum_{j=1}^{8} \\frac{1}{j} = H_8$$\nWe calculate the value of $H_8$ as an exact fraction.\n$$H_8 = 1 + \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{4} + \\frac{1}{5} + \\frac{1}{6} + \\frac{1}{7} + \\frac{1}{8}$$\nThe least common multiple of the denominators $(1, 2, 3, 4, 5, 6, 7, 8)$ is $LCM(1,2,3,4,5,6,7,8) = 2^3 \\cdot 3 \\cdot 5 \\cdot 7 = 840$.\nWe express each term with the common denominator:\n$$H_8 = \\frac{840}{840} + \\frac{420}{840} + \\frac{280}{840} + \\frac{210}{840} + \\frac{168}{840} + \\frac{140}{840} + \\frac{120}{840} + \\frac{105}{840}$$\nSumming the numerators:\n$$840 + 420 + 280 + 210 + 168 + 140 + 120 + 105 = 2283$$\nSo, the sum is:\n$$H_8 = \\frac{2283}{840}$$\nFinally, we simplify the fraction. The sum of the digits of the numerator is $2+2+8+3=15$, which is divisible by $3$. The sum of the digits of the denominator is $8+4+0=12$, which is also divisible by $3$.\n$$\\frac{2283 \\div 3}{840 \\div 3} = \\frac{761}{280}$$\nThe prime factors of the new denominator $280$ are $2^3$, $5$, and $7$. The numerator $761$ is not divisible by $2$ (odd), $5$ (does not end in $0$ or $5$), or $7$ ($761 = 7 \\times 108 + 5$). Thus, the fraction is in its simplest form.\nThe value of the ratio is $R = H_8 = \\frac{761}{280}$. This value being greater than $1$ illustrates that the Holm procedure allocates a larger total \"alpha budget\" across its testing sequence compared to the Bonferroni procedure, which is a quantitative reflection of its generally greater statistical power.", "answer": "$$\\boxed{\\frac{761}{280}}$$", "id": "5069379"}, {"introduction": "The ultimate goal of a clinical trial is not just to generate $p$-values, but to inform real-world decisions for patients and clinicians, which requires a careful weighing of a treatment's benefits against its potential harms. This exercise [@problem_id:5069453] will walk you through translating raw risk data from a trial into intuitive metrics like the Number Needed to Treat ($NNT$) and Number Needed to Harm ($NNH$). You will then integrate these into a patient-centered net clinical benefit framework, a core skill in translational medicine.", "problem": "A translational Phase $3$ randomized controlled trial (RCT) evaluates a novel anticoagulant, $X$, versus standard therapy in adults with atrial fibrillation at high thromboembolic risk. The trial is pre-registered with a hypothesis-driven design: the primary clinical benefit endpoint is reduction in ischemic stroke over $1$ year, and the primary safety endpoint is increase in major bleeding over $1$ year. The null hypothesis for each endpoint is that the treated and control arms have equal risks. Patient advisory board input is incorporated as preference weights into a net clinical benefit framework, recognizing that ischemic stroke prevention may carry different perceived importance than major bleeding harms.\n\nThe observed risks over $1$ year are:\n- Control arm ischemic stroke risk $R_{C,\\text{stroke}} = 0.18$, treated arm ischemic stroke risk $R_{T,\\text{stroke}} = 0.135$.\n- Control arm major bleeding risk $R_{C,\\text{bleed}} = 0.03$, treated arm major bleeding risk $R_{T,\\text{bleed}} = 0.047$.\n\nThe patient preference weights are:\n- Weight for preventing an ischemic stroke $w_{\\text{stroke}} = 1.0$.\n- Weight for experiencing a major bleeding event $w_{\\text{bleed}} = 0.5$.\n\nStarting only from fundamental definitions of risk as probability of an event, absolute risk difference as the difference in event probabilities between arms, and expected counts as the product of risk and cohort size, do the following:\n- Define the number needed to treat (NNT) to prevent one ischemic stroke and the number needed to harm (NNH) to cause one major bleeding event, explicitly connecting each definition to the absolute risk difference for its endpoint.\n- Compute NNT and NNH using the observed risks.\n- Derive an expression for the net clinical benefit per $N$ treated patients as the weighted expected number of benefit events prevented minus the weighted expected number of harm events incurred, and compute its value for $N = 1000$.\n\nExpress the final net clinical benefit in weighted events per $1000$ treated patients. If you choose to round any intermediate quantities, round to four significant figures; however, the final net clinical benefit value must be reported exactly if it can be computed without rounding.", "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and objective. It presents a standard scenario in clinical trial analysis within translational medicine, providing all necessary data and clear, formalizable objectives.\n\nThe solution proceeds in three parts as requested: first, defining the number needed to treat ($NNT$) and number needed to harm ($NNH$); second, calculating these values; and third, deriving and calculating the net clinical benefit ($NCB$).\n\n**Part 1: Definition of NNT and NNH**\n\nWe begin from the fundamental definition of risk as the probability of an event occurring in a specified group over a defined period. Let $R_{T}$ be the risk in the treated group and $R_{C}$ be the risk in the control group.\n\nThe Absolute Risk Difference ($ARD$) is the arithmetic difference in event rates between the two groups.\n$$ARD = R_{C} - R_{T}$$\nThe sign of the $ARD$ depends on whether the event is a benefit (an adverse outcome being prevented) or a harm (an adverse outcome being caused).\n\nFor a beneficial outcome, such as the prevention of an ischemic stroke, the risk in the treated arm is expected to be lower than in the control arm ($R_{T,\\text{stroke}} < R_{C,\\text{stroke}}$). The difference is termed the Absolute Risk Reduction ($ARR$).\n$$ARR = R_{C,\\text{stroke}} - R_{T,\\text{stroke}}$$\nThe $ARR$ represents the proportion of the population that is spared the adverse outcome (stroke) as a result of receiving the treatment instead of the control. For example, an $ARR$ of $0.05$ implies that for every $100$ patients treated, $5$ additional patients will be prevented from having a stroke.\n\nThe Number Needed to Treat ($NNT$) is the average number of patients who need to be treated with the intervention to prevent one additional adverse outcome. It is the multiplicative inverse of the $ARR$.\n$$NNT = \\frac{1}{ARR} = \\frac{1}{R_{C,\\text{stroke}} - R_{T,\\text{stroke}}}$$\nThe derivation is straightforward: if treating $1$ patient reduces the risk by $ARR$, then the number of patients ($NNT$) one must treat to achieve a total risk reduction of $1$ (i.e., to prevent one full event) is $NNT \\times ARR = 1$, which rearranges to the definition above.\n\nFor a harmful outcome, such as a major bleeding event, the risk in the treated arm is typically higher than in the control arm ($R_{T,\\text{bleed}} > R_{C,\\text{bleed}}$). The difference is termed the Absolute Risk Increase ($ARI$). To maintain a positive value for the metric, it is defined as:\n$$ARI = R_{T,\\text{bleed}} - R_{C,\\text{bleed}}$$\nThe $ARI$ represents the proportion of the population that experiences an additional adverse outcome (bleeding) as a result of receiving the treatment.\n\nThe Number Needed to Harm ($NNH$) is the average number of patients who need to be treated for one additional harmful outcome to occur. It is the multiplicative inverse of the $ARI$.\n$$NNH = \\frac{1}{ARI} = \\frac{1}{R_{T,\\text{bleed}} - R_{C,\\text{bleed}}}$$\nThe logic is analogous to that for $NNT$.\n\n**Part 2: Calculation of NNT and NNH**\n\nUsing the provided risk data:\n- Control arm ischemic stroke risk: $R_{C,\\text{stroke}} = 0.18$\n- Treated arm ischemic stroke risk: $R_{T,\\text{stroke}} = 0.135$\n- Control arm major bleeding risk: $R_{C,\\text{bleed}} = 0.03$\n- Treated arm major bleeding risk: $R_{T,\\text{bleed}} = 0.047$\n\nFirst, we calculate the Absolute Risk Reduction for ischemic stroke:\n$$ARR_{\\text{stroke}} = R_{C,\\text{stroke}} - R_{T,\\text{stroke}} = 0.18 - 0.135 = 0.045$$\nNow, we compute the $NNT$ for preventing one stroke:\n$$NNT = \\frac{1}{ARR_{\\text{stroke}}} = \\frac{1}{0.045} = \\frac{1}{45/1000} = \\frac{1000}{45} = \\frac{200}{9} \\approx 22.22$$\nConventionally, $NNT$ is rounded up to the nearest integer, implying one would need to treat $23$ patients to prevent one stroke. However, for calculation purposes, we retain the exact value.\n\nNext, we calculate the Absolute Risk Increase for major bleeding:\n$$ARI_{\\text{bleed}} = R_{T,\\text{bleed}} - R_{C,\\text{bleed}} = 0.047 - 0.03 = 0.017$$\nNow, we compute the $NNH$ for causing one major bleed:\n$$NNH = \\frac{1}{ARI_{\\text{bleed}}} = \\frac{1}{0.017} = \\frac{1}{17/1000} = \\frac{1000}{17} \\approx 58.82$$\nConventionally, $NNH$ is rounded down to the nearest integer, implying that for every $58$ patients treated, one additional major bleed is expected. We again retain the exact value.\n\n**Part 3: Derivation and Calculation of Net Clinical Benefit (NCB)**\n\nWe are asked to derive an expression for the net clinical benefit per $N$ treated patients. The expected count of an event in a cohort of size $N$ is the product of the risk and the cohort size, $E = N \\times R$.\n\nThe expected number of strokes prevented by treating $N$ patients is the difference in the expected number of events between a control cohort of size $N$ and a treated cohort of size $N$.\n$$\\Delta E_{\\text{prevented\\_stroke}} = E_{C,\\text{stroke}} - E_{T,\\text{stroke}} = N \\times R_{C,\\text{stroke}} - N \\times R_{T,\\text{stroke}} = N \\times (R_{C,\\text{stroke}} - R_{T,\\text{stroke}}) = N \\times ARR_{\\text{stroke}}$$\nThe expected number of additional bleeding events caused by treating $N$ patients is the difference in the expected number of events.\n$$\\Delta E_{\\text{caused\\_bleed}} = E_{T,\\text{bleed}} - E_{C,\\text{bleed}} = N \\times R_{T,\\text{bleed}} - N \\times R_{C,\\text{bleed}} = N \\times (R_{T,\\text{bleed}} - R_{C,\\text{bleed}}) = N \\times ARI_{\\text{bleed}}$$\nThe net clinical benefit, $NCB_N$, is defined as the weighted expected number of benefit events prevented minus the weighted expected number of harm events incurred. The weights are given as $w_{\\text{stroke}} = 1.0$ and $w_{\\text{bleed}} = 0.5$.\n$$NCB_N = (w_{\\text{stroke}} \\times \\Delta E_{\\text{prevented\\_stroke}}) - (w_{\\text{bleed}} \\times \\Delta E_{\\text{caused\\_bleed}})$$\nSubstituting the expressions for the change in expected events:\n$$NCB_N = (w_{\\text{stroke}} \\times N \\times ARR_{\\text{stroke}}) - (w_{\\text{bleed}} \\times N \\times ARI_{\\text{bleed}})$$\nThis can be factored as:\n$$NCB_N = N \\times (w_{\\text{stroke}} \\times ARR_{\\text{stroke}} - w_{\\text{bleed}} \\times ARI_{\\text{bleed}})$$\nThis is the general expression for the net clinical benefit for a cohort of size $N$.\n\nWe are asked to compute its value for $N = 1000$. We substitute the known values:\n- $N = 1000$\n- $w_{\\text{stroke}} = 1.0$\n- $w_{\\text{bleed}} = 0.5$\n- $ARR_{\\text{stroke}} = 0.045$\n- $ARI_{\\text{bleed}} = 0.017$\n\n$$NCB_{1000} = 1000 \\times ((1.0 \\times 0.045) - (0.5 \\times 0.017))$$\nFirst, we compute the term in the parenthesis:\n$$(1.0 \\times 0.045) = 0.045$$\n$$(0.5 \\times 0.017) = 0.0085$$\nThe difference is:\n$$0.045 - 0.0085 = 0.0365$$\nFinally, we multiply by $N=1000$:\n$$NCB_{1000} = 1000 \\times 0.0365 = 36.5$$\nThis value represents a net benefit of $36.5$ weighted events for every $1000$ patients treated with anticoagulant $X$ instead of the standard therapy. Since the value is positive, the treatment provides a net clinical benefit according to this framework. This final value is exact and does not require rounding.", "answer": "$$\\boxed{36.5}$$", "id": "5069453"}]}