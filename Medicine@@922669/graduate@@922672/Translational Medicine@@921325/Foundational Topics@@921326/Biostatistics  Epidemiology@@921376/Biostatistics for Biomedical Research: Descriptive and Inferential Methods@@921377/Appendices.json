{"hands_on_practices": [{"introduction": "In biomedical research, a common goal is to compare measurements before and after an intervention using a paired t-test. The validity of this powerful test hinges on the critical assumption that the differences between paired measurements are approximately normally distributed. This exercise guides you through the practical steps of rigorously assessing this normality assumption using a combination of formal statistical tests and graphical diagnostics, a foundational skill for any researcher [@problem_id:4936017]. Mastering this process ensures you can make an informed and defensible decision about the appropriate analytical method for your paired data.", "problem": "A researcher plans a paired analysis of pre-intervention and post-intervention systolic blood pressure in a cohort of $n=28$ participants. For each participant $i$, the researcher computes the paired differences $D_i=X_{i,\\text{post}}-X_{i,\\text{pre}}$. The inferential goal is to test whether the population mean of the paired differences is zero using a paired $t$ procedure, but the researcher first wants to assess the normality of the $D_i$ to justify using the paired $t$ procedure.\n\nThe researcher adopts the following diagnostic tools and obtains the following outputs on the sample of differences: a Shapiro–Wilk test statistic of $W=0.958$ and a corresponding $p$-value of $p=0.12$ at a planned significance level of $\\alpha=0.05$, and a quantile–quantile (QQ) plot comparing the sample quantiles of the $D_i$ to the theoretical quantiles under a normal distribution that appears approximately linear with mild tail deviations and no extreme outliers.\n\nStarting from the following fundamental bases:\n- The definition of the paired difference $D_i$ and that the paired $t$ procedure assumes the $D_i$ are independent and drawn from a population that is approximately normal.\n- The Shapiro–Wilk test for normality has null hypothesis that the sample is drawn from a normal distribution, with interpretation of the $p$-value as the probability, under the null, of observing a test statistic as extreme or more extreme than the one observed.\n- The concept of the QQ plot as a graphical tool to assess how closely sample quantiles align with theoretical normal quantiles.\n- The robustness of the paired $t$ procedure to mild deviations from normality, particularly for symmetric distributions and moderate sample sizes, and the availability of the Wilcoxon signed-rank test as a nonparametric alternative when normality is not tenable.\n\nWhich of the following protocols correctly specifies what to test for normality, how to combine Shapiro–Wilk and QQ plot interpretation, and how to state decision rules for whether to proceed with the paired $t$ procedure on the mean difference, including appropriate alternatives if assumptions are violated?\n\nA. Compute $D_i$ for all pairs; apply the Shapiro–Wilk test to $\\{D_i\\}$ at $\\alpha=0.05$; inspect the QQ plot of $\\{D_i\\}$ against theoretical normal quantiles. If $p\\ge\\alpha$ and the QQ plot is approximately linear without severe curvature or outliers, proceed with the paired $t$ procedure on the mean of $D_i$. If $p<\\alpha$ or the QQ plot shows marked S-shaped curvature, heavy tails, or outliers suggesting clear non-normality, prefer the Wilcoxon signed-rank test on $D_i$. If $p$ is borderline and $n$ is moderate (for example, $n\\approx 25$ to $n\\approx 30$) with a QQ plot showing only mild tail deviations and approximate symmetry, proceed with the paired $t$ procedure acknowledging its robustness and consider a sensitivity analysis using the Wilcoxon signed-rank test.\n\nB. Test normality separately on the pre-intervention values $\\{X_{i,\\text{pre}}\\}$ and post-intervention values $\\{X_{i,\\text{post}}\\}$ using the Shapiro–Wilk test at $\\alpha=0.05$. If both are normal, proceed with the paired $t$ procedure; if either is non-normal, apply a logarithmic transformation to both series to force normality, then proceed with the paired $t$ procedure regardless of the QQ plot of $D_i$.\n\nC. Compute $D_i$ and apply the Shapiro–Wilk test to $\\{D_i\\}$ at $\\alpha=0.05$. If $p<\\alpha$, interpret this as evidence that the $D_i$ are normal and proceed with the paired $t$ procedure; if $p\\ge\\alpha$, interpret this as non-normality and use the Mann–Whitney $U$ test on unpaired observations.\n\nD. Ignore the Shapiro–Wilk test and rely solely on the QQ plot. If the QQ plot of $\\{D_i\\}$ shows any deviation from a straight line, do not use the paired $t$ procedure and instead always use the Wilcoxon signed-rank test; if the QQ plot is perfectly straight, use the paired $t$ procedure. Do not consider $p$-values or sample size because graphical judgment suffices.\n\nE. Since $n=28$, invoke the Central Limit Theorem (CLT) to conclude that the $D_i$ themselves are normal regardless of the underlying distribution. Proceed with the paired $t$ procedure without any normality checks; only if $n<25$ should one consider the Wilcoxon signed-rank test based on a histogram of $D_i$.", "solution": "The problem asks for the correct protocol to assess the normality assumption for a paired $t$-test. A paired $t$-test is equivalent to a one-sample $t$-test on the paired differences, $D_i$. The key assumption is that these differences, $\\{D_i\\}$, are drawn from an approximately normal distribution.\n\nA sound protocol involves several steps:\n1.  **Correct Object of Testing**: The normality check must be performed on the set of differences, $\\{D_i\\}$, not on the pre- and post-intervention data separately.\n2.  **Combination of Tools**: Best practice involves using both a formal statistical test (like the Shapiro–Wilk test) and a graphical diagnostic tool (like a quantile-quantile (QQ) plot). Formal tests provide objectivity, while graphical plots reveal the nature of any deviation (e.g., skewness, outliers).\n3.  **Correct Interpretation**: The Shapiro–Wilk test has the null hypothesis that the data are normal. A small $p$-value ($p<\\alpha$) provides evidence *against* normality. A large $p$-value ($p\\ge\\alpha$) means we *fail to reject* the null hypothesis. In this case, $p=0.12 > 0.05$, so there is no significant evidence against normality from the formal test. The QQ plot is described as \"approximately linear with mild tail deviations,\" which also suggests the data are reasonably close to normal.\n4.  **Consideration of Robustness**: The $t$-test is known to be robust to mild violations of normality, especially with moderate sample sizes (like $n=28$) and when the distribution is not severely skewed.\n5.  **Correct Alternative**: If the normality assumption is clearly violated, the appropriate nonparametric alternative for paired data is the Wilcoxon signed-rank test. The Mann–Whitney $U$ test is for independent samples and would be incorrect.\n\nLet's evaluate the options based on these principles:\n\n*   **A:** This option correctly follows all steps. It tests the differences $\\{D_i\\}$, uses both the Shapiro–Wilk test and a QQ plot, interprets the results correctly, identifies the correct nonparametric alternative (Wilcoxon signed-rank test), and includes a nuanced rule for borderline cases that acknowledges the robustness of the $t$-test. This represents a complete and expert-level protocol.\n*   **B:** This is incorrect because it tests the normality of the pre- and post-intervention data separately, which is not the assumption of the paired $t$-test.\n*   **C:** This is incorrect because it fundamentally misinterprets the $p$-value from the normality test (reversing the conclusion) and suggests the wrong nonparametric alternative (Mann–Whitney $U$ test).\n*   **D:** This is incorrect because it relies on an overly strict and subjective graphical rule (\"any deviation\") while completely ignoring the objective formal test and the crucial context provided by the sample size.\n*   **E:** This is incorrect because it misinterprets the Central Limit Theorem (CLT). The CLT applies to the distribution of the *sample mean*, not the data points themselves. For a moderate sample size of $n=28$, it is still prudent to check the normality assumption.\n\nTherefore, option A provides the only correct and comprehensive protocol.", "answer": "$$\\boxed{A}$$", "id": "4936017"}, {"introduction": "Biomedical data are frequently complicated by measurements that fall below an instrument's limit of detection (LOD), a situation known as left-censoring. Handling these censored data points naively, for instance by substituting them with a constant value like half the LOD, can severely violate the core assumptions of normality and homoscedasticity required for tests like Analysis of Variance (ANOVA), leading to erroneous conclusions. This practice problem challenges you to think critically about data structure by contrasting inappropriate ad-hoc methods with statistically sound alternatives, such as Tobit regression or specialized nonparametric tests, thereby highlighting the importance of choosing a model that correctly accounts for the data-generating process [@problem_id:4777705].", "problem": "A multicenter randomized clinical trial measures a plasma biomarker to compare inflammation across $3$ treatment arms. The true latent biomarker concentration for subject $i$ in group $g$ is denoted $Y^*_{gi}$ and is assumed, for design purposes, to follow a normal distribution with common variance across groups: $Y^*_{gi} \\sim \\mathcal{N}(\\mu_g, \\sigma^2)$, with independent errors across subjects. The assay has a limit of detection (LOD), denoted $L_g$, which can differ by laboratory and thus by group because of site-specific platforms. The observed variable is left-censored: only values $Y_{gi}$ are recorded, where $Y_{gi} = Y^*_{gi}$ if $Y^*_{gi} \\ge L_g$ and otherwise the result is flagged as “$< L_g$.” In practice, some analysts replace censored observations by $L_g/2$ or by $L_g$ before analysis.\n\nIn this trial, group $1$ and group $3$ were processed at laboratories with $L_1 = L_3 = 0.5$ pg/mL, while group $2$ used a platform with $L_2 = 0.2$ pg/mL. The proportion of results reported below the LOD was approximately $30\\%$ in group $1$, $10\\%$ in group $2$, and $25\\%$ in group $3$. Investigators propose the following candidate analyses to test equality of group means: (i) one-way Analysis of Variance (ANOVA) on $\\log(Y)$ after substituting $L_g/2$ for censored values, (ii) a Tobit censored normal regression with group indicators fitted on the log scale, accounting for known $L_g$, and (iii) a rank-based nonparametric test that can accommodate censoring.\n\nGround your reasoning in the classical one-way ANOVA model $Y_{gi} = \\mu_g + \\epsilon_{gi}$ with $\\epsilon_{gi} \\sim \\mathcal{N}(0, \\sigma^2)$ independent, and in well-tested properties of censored and truncated normal distributions and rank-based inference. Analyze how left-censoring below detection limits affects the normality and homoscedasticity assumptions, and the appropriateness of the proposed methods for valid inference on differences in central tendency across groups in this medical context.\n\nWhich of the following statements are correct?\n\nA. Because log transformation often improves normality, performing Analysis of Variance (ANOVA) on $\\log(Y)$ after substituting $L_g/2$ for censored values will restore the normality and homoscedasticity assumptions regardless of the censoring proportions and $L_g$ differences.\n\nB. Under the latent model $Y^*_{gi} \\sim \\mathcal{N}(\\mu_g, \\sigma^2)$ with independent errors and known left-censoring thresholds $L_g$, a Tobit censored normal regression with group indicators fitted to $\\log(Y^*)$ yields consistent estimates and valid tests of equality of $\\mu_g$ across groups, provided the latent errors are homoscedastic and normal.\n\nC. When censoring proportions or $L_g$ differ across groups, the observed $Y$ violates normality and homoscedasticity; rank-based methods that explicitly accommodate censoring (for example, the Akritas extension of the Kruskal–Wallis test) are more appropriate if one seeks inference on differences in central tendency without parametric assumptions.\n\nD. Independence of observations is all that is required for ANOVA validity; therefore, even severe left-censoring does not threaten the validity of ANOVA F-tests so long as subjects are independent.\n\nE. If the proportion below the limit of detection is approximately equal across groups and $L_g$ is identical, then replacing censored values by $L_g/2$ ensures unbiased group mean estimates and valid ANOVA inference.", "solution": "This problem evaluates methods for analyzing left-censored data, where values below a limit of detection (LOD) are not precisely measured. The key is understanding how ad-hoc imputation of censored data violates the assumptions of standard tests like ANOVA, and what the appropriate alternatives are.\n\nThe classical ANOVA F-test relies on three assumptions about the data's residuals: independence, normality, and homoscedasticity (equal variances across groups).\n\n1.  **Violation of Normality**: Replacing all censored values with a constant (e.g., $L_g/2$) creates a large point mass in the data distribution. In group 1, $30\\%$ of the data would be artificially set to one value. A distribution with a large point mass is not continuous and therefore not normal. A log transformation does not fix this; it just moves the point mass.\n\n2.  **Violation of Homoscedasticity**: The problem states that both the LOD ($L_g$) and the censoring proportion differ across groups. This means the ad-hoc imputation procedure will distort the variance of each group differently, directly violating the assumption of equal variances.\n\nWith these violations in mind, let's analyze the statements:\n\n*   **A:** This is incorrect. As explained, substituting a constant for censored values violates both normality (by creating a point mass) and homoscedasticity (due to different LODs and censoring rates). A log transformation cannot fix these structural problems.\n*   **B:** This is correct. A Tobit model (or censored regression) is a parametric method specifically designed to handle censored data. It uses maximum likelihood estimation based on a likelihood function that properly accounts for both censored and uncensored observations. Provided its own assumptions are met (i.e., the *latent* data are normal and homoscedastic, often after a log transform), it provides consistent estimates and valid tests.\n*   **C:** This is correct. The first part of the statement accurately identifies that differing censoring rates and LODs violate the assumptions of normality and homoscedasticity. The second part correctly proposes a valid alternative. If the parametric assumptions required for a Tobit model are questionable, robust nonparametric tests that are specifically designed to handle censored data (like the Akritas test) are a superior choice. They make fewer distributional assumptions.\n*   **D:** This is incorrect. Independence is a necessary but not sufficient condition for the validity of the ANOVA F-test. The test also critically relies on the normality and homoscedasticity of residuals, both of which are violated here.\n*   **E:** This is incorrect. Even if censoring were identical across groups, imputation with a constant still introduces bias in the mean estimates (as $E[Y^* | Y^* < L_g]$ is not equal to $L_g/2$) and violates the normality assumption. Therefore, the subsequent ANOVA inference would not be valid.\n\nBoth B and C describe statistically sound approaches to dealing with censored data, representing the correct parametric and nonparametric solutions, respectively.", "answer": "$$\\boxed{BC}$$", "id": "4777705"}, {"introduction": "A deep understanding of statistical assumptions requires recognizing that they are not a universal checklist but are instead dictated by the experimental design. This is especially clear when comparing analyses for independent groups (a between-subjects design) versus repeated measurements on the same subjects (a within-subjects design). This final exercise clarifies why a critical assumption like sphericity is fundamental to repeated measures ANOVA but is entirely irrelevant to a one-way ANOVA on independent groups [@problem_id:4938840]. By contrasting these two common scenarios, you will solidify your understanding that a valid statistical model must accurately reflect the underlying structure and dependencies within the data.", "problem": "A biostatistics team is analyzing a one-way experimental design with $k=4$ independent treatment groups, each subject measured once on a continuous outcome. After finding a statistically significant omnibus analysis of variance, the team considers post-hoc procedures including Bonferroni, Tukey, Scheffé, and Dunnett. In a separate study, the same team plans a repeated measures experiment where each subject is followed at $p=4$ time points, and they wish to compare the time levels post-hoc as well.\n\nWhich statements correctly explain why the sphericity assumption is not required for one-way analysis of variance post-hoc tests and appropriately contrast this with repeated measures designs?\n\nA. In a one-way between-subjects design with independent groups, there is no within-subject covariance structure across treatment levels; the vector of group mean estimates has off-diagonal covariances equal to $0$ under independence, so procedures such as Tukey’s honestly significant difference, Scheffé’s method, Bonferroni correction, and Dunnett’s test do not require sphericity.\n\nB. Tukey’s honestly significant difference requires sphericity because all pairwise differences are correlated within subjects; without sphericity, its critical values are invalid in a one-way analysis of variance.\n\nC. In repeated measures designs, sphericity pertains to the equality of variances of all pairwise within-subject differences across levels; when sphericity is violated, the standard univariate $F$ test for the within-subject factor becomes invalid unless degrees of freedom are corrected, and post-hoc inferences must use covariance-aware standard errors for dependent comparisons.\n\nD. Bonferroni’s control of familywise error rate relies on independence of tests and therefore requires sphericity; if sphericity is violated, Bonferroni inflates type I error.\n\nE. Scheffé’s method controls the familywise error rate for all possible contrasts among group means in a one-way between-subjects analysis of variance without relying on sphericity; its validity rests on homoscedastic errors and the general linear model assumptions, not on a within-subject covariance structure.", "solution": "This problem requires distinguishing the statistical assumptions for a one-way (between-subjects) ANOVA from those for a repeated measures (within-subjects) ANOVA.\n\n**One-Way ANOVA**: This design involves independent groups. The core assumptions are independence of observations, normality of residuals, and homoscedasticity (equal variances across groups). Because the groups are independent, there are no correlations between them. The concept of a within-subject covariance structure, which is what sphericity relates to, is not applicable.\n\n**Repeated Measures ANOVA**: This design involves multiple measurements on the same subjects. These measurements are inherently correlated (dependent). The **sphericity** assumption is an assumption about this dependency structure. It states that the variances of the differences between all possible pairs of repeated measures levels are equal. If sphericity is violated, the standard F-test is biased and requires correction.\n\nLet's evaluate the statements based on these principles:\n\n*   **A. Correct.** This statement accurately explains the core difference. In a one-way ANOVA, groups are independent, so there is no within-subject covariance structure. The covariance between different group means is zero. Therefore, sphericity, an assumption about within-subject covariance, is irrelevant for post-hoc tests in this design.\n*   **B. Incorrect.** This statement falsely applies a repeated-measures concept to a one-way ANOVA. In a one-way ANOVA, the groups are independent, not \"correlated within subjects.\" Tukey's HSD for independent groups does not require sphericity.\n*   **C. Correct.** This statement provides an accurate and concise description of sphericity in the context where it *is* relevant: repeated measures designs. It correctly defines sphericity, states the consequences of its violation for the omnibus F-test (invalid unless corrected), and notes the implications for post-hoc tests (must account for the dependency). This provides the necessary contrast to the one-way ANOVA case.\n*   **D. Incorrect.** This statement misrepresents the Bonferroni correction. The Bonferroni method controls the familywise error rate even when the tests are dependent; it does not require independence. Furthermore, it is a general method for multiplicity adjustment and has no direct relationship with the sphericity assumption.\n*   **E. Correct.** This statement accurately describes Scheffé's method in the context of a one-way ANOVA. It correctly states that the method's validity depends on standard ANOVA assumptions like homoscedasticity, but not on sphericity, because the underlying model is for independent groups and lacks a \"within-subject covariance structure.\"\n\nStatements A, C, and E correctly explain why sphericity is not required for a one-way ANOVA and provide the appropriate contrast with repeated measures designs.", "answer": "$$\\boxed{ACE}$$", "id": "4938840"}]}