{"hands_on_practices": [{"introduction": "A cornerstone of evidence synthesis is the quantitative pooling of effect estimates from multiple studies. This first exercise walks you through the fundamental mechanics of a fixed-effect meta-analysis using the inverse-variance method. You will practice the critical steps of log-transforming hazard ratios, deriving study-specific variances from confidence intervals, and synthesizing the evidence to produce a single, more precise estimate of the treatment effect.", "problem": "In a translational medicine program evaluating a biomarker-guided therapy versus standard-of-care across multiple clinical centers, four independent prospective trials report adjusted hazard ratios (hazard ratio, HR) for all-cause mortality from Cox proportional hazards (Cox PH) models, each accompanied by a two-sided $95\\%$ confidence interval (confidence interval, CI). Under large-sample theory, the logarithm of the hazard ratio is approximately normally distributed. Assume the study-level log hazard ratio estimates are independent, and adopt a fixed-effect synthesis on the logarithmic scale with inverse-variance weighting. \n\nThe four studies report the following:\n\n- Study A: $HR = 0.72$ with $95\\%$ CI $[0.58, 0.89]$.\n- Study B: $HR = 0.95$ with $95\\%$ CI $[0.80, 1.13]$.\n- Study C: $HR = 0.65$ with $95\\%$ CI $[0.50, 0.84]$.\n- Study D: $HR = 0.85$ with $95\\%$ CI $[0.73, 0.99]$.\n\nStarting from first principles of large-sample normality for the log hazard ratio and the definition of a confidence interval for a normal mean, apply the appropriate logarithmic transformations and compute the inverse-variance weights on the log scale for each study. Then compute the fixed-effect pooled log hazard ratio and transform it back to the hazard ratio scale.\n\nExpress the final pooled hazard ratio as a decimal, rounding to four significant figures. No units are required.", "solution": "The problem statement is a well-posed application of standard biostatistical methods for meta-analysis. It is scientifically grounded, self-contained, and objective. We will proceed with a solution.\n\nThe task is to perform a fixed-effect meta-analysis of four studies using the inverse-variance weighting method. The synthesis is conducted on the logarithmic scale, as the log-hazard ratio is assumed to be approximately normally distributed. Let the hazard ratio for study $i$ be $HR_i$. The corresponding effect estimate on the logarithmic scale is $y_i = \\ln(HR_i)$.\n\nUnder large-sample theory, $y_i$ is assumed to follow a normal distribution, $y_i \\sim N(\\theta_i, v_i)$, where $\\theta_i$ is the true log-hazard ratio for study $i$ and $v_i$ is the sampling variance of the estimate $y_i$. A fixed-effect model assumes a common true effect across all studies, so $\\theta_i = \\theta$ for all $i$.\n\nFirst, for each study $i$, we must determine the log-hazard ratio $y_i$ and its variance $v_i$. The value of $y_i$ is obtained by taking the natural logarithm of the reported hazard ratio, $HR_i$. The variance $v_i$ is derived from the given $95\\%$ confidence interval, $CI_i = [L_i, U_i]$.\nThe confidence interval for the log-hazard ratio $y_i$ is $[\\ln(L_i), \\ln(U_i)]$. By definition, a two-sided $95\\%$ confidence interval for a normally distributed estimate is given by the point estimate plus or minus the standard error ($SE_i$) multiplied by the critical value from the standard normal distribution. The critical value for a $95\\%$ confidence level is $z_{0.025}$, which is approximately $1.96$.\nThe width of the confidence interval on the log scale is $\\ln(U_i) - \\ln(L_i)$. This width is also equal to $2 \\times z_{0.025} \\times SE_i$. Therefore, we can find the standard error for each study's log-hazard ratio:\n$$ SE_i = \\frac{\\ln(U_i) - \\ln(L_i)}{2 \\times z_{0.025}} = \\frac{\\ln(U_i/L_i)}{2 \\times 1.96} $$\nThe variance $v_i$ is the square of the standard error:\n$$ v_i = (SE_i)^2 = \\left( \\frac{\\ln(U_i/L_i)}{3.92} \\right)^2 $$\nThe weight for each study in an inverse-variance meta-analysis is the reciprocal of its variance:\n$$ w_i = \\frac{1}{v_i} $$\nWe now apply these formulas to the data from each of the four studies.\n\nStudy A: $HR_A = 0.72$, $95\\%$ CI $[0.58, 0.89]$\n- Log-hazard ratio: $y_A = \\ln(0.72) \\approx -0.32850$\n- Standard error: $SE_A = \\frac{\\ln(0.89) - \\ln(0.58)}{3.92} = \\frac{\\ln(0.89/0.58)}{3.92} \\approx 0.10923$\n- Variance: $v_A = (SE_A)^2 \\approx (0.10923)^2 \\approx 0.01193$\n- Weight: $w_A = \\frac{1}{v_A} \\approx \\frac{1}{0.01193} \\approx 83.805$\n\nStudy B: $HR_B = 0.95$, $95\\%$ CI $[0.80, 1.13]$\n- Log-hazard ratio: $y_B = \\ln(0.95) \\approx -0.05129$\n- Standard error: $SE_B = \\frac{\\ln(1.13) - \\ln(0.80)}{3.92} = \\frac{\\ln(1.13/0.80)}{3.92} \\approx 0.08810$\n- Variance: $v_B = (SE_B)^2 \\approx (0.08810)^2 \\approx 0.00776$\n- Weight: $w_B = \\frac{1}{v_B} \\approx \\frac{1}{0.00776} \\approx 128.845$\n\nStudy C: $HR_C = 0.65$, $95\\%$ CI $[0.50, 0.84]$\n- Log-hazard ratio: $y_C = \\ln(0.65) \\approx -0.43078$\n- Standard error: $SE_C = \\frac{\\ln(0.84) - \\ln(0.50)}{3.92} = \\frac{\\ln(0.84/0.50)}{3.92} \\approx 0.13235$\n- Variance: $v_C = (SE_C)^2 \\approx (0.13235)^2 \\approx 0.01752$\n- Weight: $w_C = \\frac{1}{v_C} \\approx \\frac{1}{0.01752} \\approx 57.089$\n\nStudy D: $HR_D = 0.85$, $95\\%$ CI $[0.73, 0.99]$\n- Log-hazard ratio: $y_D = \\ln(0.85) \\approx -0.16252$\n- Standard error: $SE_D = \\frac{\\ln(0.99) - \\ln(0.73)}{3.92} = \\frac{\\ln(0.99/0.73)}{3.92} \\approx 0.07772$\n- Variance: $v_D = (SE_D)^2 \\approx (0.07772)^2 \\approx 0.00604$\n- Weight: $w_D = \\frac{1}{v_D} \\approx \\frac{1}{0.00604} \\approx 165.556$\n\nThe pooled log-hazard ratio, $y_{pool}$, is the weighted average of the individual log-hazard ratios:\n$$ y_{pool} = \\frac{\\sum_{i=A,B,C,D} w_i y_i}{\\sum_{i=A,B,C,D} w_i} $$\nWe compute the numerator and denominator separately.\nSum of weights:\n$$ \\sum w_i = w_A + w_B + w_C + w_D \\approx 83.805 + 128.845 + 57.089 + 165.556 = 435.295 $$\nSum of weighted effects:\n$$ \\sum w_i y_i \\approx (83.805 \\times -0.32850) + (128.845 \\times -0.05129) + (57.089 \\times -0.43078) + (165.556 \\times -0.16252) $$\n$$ \\sum w_i y_i \\approx -27.5319 - 6.6086 - 24.5910 - 26.9064 = -85.6379 $$\nNow, we compute the pooled log-hazard ratio:\n$$ y_{pool} = \\frac{-85.6379}{435.295} \\approx -0.196735 $$\nFinally, to obtain the pooled hazard ratio, $HR_{pool}$, we transform the pooled log-hazard ratio back from the logarithmic scale to the original hazard ratio scale by taking the exponential:\n$$ HR_{pool} = \\exp(y_{pool}) \\approx \\exp(-0.196735) \\approx 0.821404 $$\nThe problem requires the final answer to be rounded to four significant figures.\n$$ HR_{pool} \\approx 0.8214 $$\nThis value represents the synthesized estimate of the effect of the biomarker-guided therapy compared to the standard-of-care, indicating an approximate $17.86\\%$ reduction in the hazard of all-cause mortality.", "answer": "$$\\boxed{0.8214}$$", "id": "5014460"}, {"introduction": "Real-world data is rarely as clean as we would like, and a common hurdle in meta-analyzing categorical outcomes is the \"zero-cell\" problem. This practice addresses the practical challenge that arises when no events are observed in a study arm, which prevents the calculation of an odds ratio. You will learn to apply a continuity correction, a standard technique to handle sparse data and enable the inclusion of all relevant evidence in the synthesis.", "problem": "In a pilot randomized controlled trial (RCT) embedded within a translational medicine program, investigators evaluated whether a novel dosing schedule of a targeted agent reduces severe cytokine release syndrome (binary outcome: present versus absent) relative to the standard schedule. The $2\\times 2$ contingency table is as follows: in the treatment arm, there were $0$ events out of $9$ participants; in the control arm, there were $5$ events out of $11$ participants. To facilitate meta-analytic compatibility in the presence of a zero cell, apply a continuity correction by adding $0.5$ to each of the four cell counts and then compute the natural logarithm of the odds ratio $\\ln(\\mathrm{OR})$. Start from first principles about probabilities, odds, and odds ratios, justify the use of the continuity correction for sparse data, and derive the requested quantity step by step without invoking any unproven shortcuts. Round your final numerical result to four significant figures and report it as a dimensionless real number.", "solution": "The problem is valid as it presents a standard, well-posed biostatistical task based on scientifically sound principles. It is self-contained, objective, and free from internal contradictions. We may therefore proceed with a full solution.\n\nThe task is to compute the natural logarithm of the odds ratio, $\\ln(\\mathrm{OR})$, for the given clinical trial data, applying a continuity correction to handle a zero-cell count. The derivation will begin from first principles.\n\nFirst, let us define the fundamental quantities. In a binary outcome setting, the probability of an event, $p$, is the proportion of individuals experiencing the event. The odds of an event are defined as the ratio of the probability that the event occurs to the probability that it does not occur:\n$$\n\\mathrm{Odds} = \\frac{p}{1-p}\n$$\nAn odds ratio ($\\mathrm{OR}$) is a measure of association between an exposure (or treatment) and an outcome. It is the ratio of the odds of the outcome in the exposed (treatment) group to the odds of the outcome in the unexposed (control) group.\n$$\n\\mathrm{OR} = \\frac{\\mathrm{Odds}_{\\text{treatment}}}{\\mathrm{Odds}_{\\text{control}}}\n$$\n\nThe data from the pilot randomized controlled trial can be organized into a $2 \\times 2$ contingency table. Let 'Event' be severe cytokine release syndrome (present) and 'No Event' be its absence.\n\n-   **Treatment Arm**: $0$ events, $9$ participants. Thus, there are $a=0$ events and $b=9-0=9$ non-events.\n-   **Control Arm**: $5$ events, $11$ participants. Thus, there are $c=5$ events and $d=11-5=6$ non-events.\n\nThe initial contingency table is:\n$$\n\\begin{array}{c|cc|c}\n& \\text{Event} & \\text{No Event} & \\text{Total} \\\\\n\\hline\n\\text{Treatment} & a=0 & b=9 & 10 \\\\\n\\text{Control} & c=5 & d=6 & 11 \\\\\n\\end{array}\n$$\nFrom this table, we can estimate the odds for each group.\nThe odds of an event in the treatment group are estimated as:\n$$\n\\mathrm{Odds}_{\\text{treatment}} = \\frac{a}{b} = \\frac{0}{9} = 0\n$$\nThe odds of an event in the control group are estimated as:\n$$\n\\mathrm{Odds}_{\\text{control}} = \\frac{c}{d} = \\frac{5}{6}\n$$\nThe odds ratio is then calculated as:\n$$\n\\mathrm{OR} = \\frac{a/b}{c/d} = \\frac{ad}{bc} = \\frac{(0)(6)}{(9)(5)} = \\frac{0}{45} = 0\n$$\nThe problem requires computing the natural logarithm of the odds ratio, $\\ln(\\mathrm{OR})$. However, $\\ln(0)$ is undefined. This is the \"zero-cell count\" problem. Furthermore, in meta-analysis, studies are often weighted by the inverse of the variance of the log-odds ratio. The standard error of the log-odds ratio is estimated by:\n$$\n\\mathrm{SE}(\\ln(\\mathrm{OR})) = \\sqrt{\\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}}\n$$\nIf any cell count ($a, b, c, \\text{ or } d$) is zero, this variance cannot be computed due to division by zero. To prevent the exclusion of such studies from a meta-analysis and to allow for the calculation of the log-odds ratio and its variance, a continuity correction is applied. This involves adding a small, positive constant, typically $0.5$, to each of the four cells in the contingency table. This specific method is often referred to as the Haldane-Anscombe correction. While this introduces a small bias, it is a standard and accepted procedure for handling sparse data.\n\nApplying the continuity correction by adding $0.5$ to each cell:\nThe adjusted cell counts ($a', b', c', d'$) are:\n-   $a' = a + 0.5 = 0 + 0.5 = 0.5$\n-   $b' = b + 0.5 = 9 + 0.5 = 9.5$\n-   $c' = c + 0.5 = 5 + 0.5 = 5.5$\n-   $d' = d + 0.5 = 6 + 0.5 = 6.5$\n\nThe adjusted contingency table is:\n$$\n\\begin{array}{c|cc}\n& \\text{Event} & \\text{No Event} \\\\\n\\hline\n\\text{Treatment} & a'=0.5 & b'=9.5 \\\\\n\\text{Control} & c'=5.5 & d'=6.5 \\\\\n\\end{array}\n$$\nNow we compute the corrected odds ratio, $\\mathrm{OR}'$:\n$$\n\\mathrm{OR}' = \\frac{a'd'}{b'c'} = \\frac{(0.5)(6.5)}{(9.5)(5.5)} = \\frac{3.25}{52.25}\n$$\nNext, we compute the natural logarithm of this corrected odds ratio, $\\ln(\\mathrm{OR}')$.\n$$\n\\ln(\\mathrm{OR}') = \\ln\\left(\\frac{3.25}{52.25}\\right)\n$$\nUsing the properties of logarithms, $\\ln(x/y) = \\ln(x) - \\ln(y)$:\n$$\n\\ln(\\mathrm{OR}') = \\ln(3.25) - \\ln(52.25)\n$$\nWe calculate the values of the natural logarithms:\n$$\n\\ln(3.25) \\approx 1.17865499\n$$\n$$\n\\ln(52.25) \\approx 3.95603998\n$$\nSubtracting these values gives:\n$$\n\\ln(\\mathrm{OR}') \\approx 1.17865499 - 3.95603998 \\approx -2.77738499\n$$\nThe problem requires the final result to be rounded to four significant figures. The number is $-2.77738499...$. The first four significant figures are $2, 7, 7, 7$. The fifth significant digit is $3$, which is less than $5$, so we round down (i.e., we do not change the fourth digit).\n$$\n\\ln(\\mathrm{OR}') \\approx -2.777\n$$\nThis dimensionless real number represents the log-odds ratio of experiencing severe cytokine release syndrome with the novel dosing schedule compared to the standard schedule, after adjustment for the zero-cell count. The negative value indicates that the odds of the event are lower in the treatment group.", "answer": "$$\\boxed{-2.777}$$", "id": "5014440"}, {"introduction": "While fixed-effect models assume a single true effect, many syntheses must account for genuine variation across studies using a random-effects model. This advanced practice explores the critical choice of statistical method, particularly in the common translational science scenario of having few studies. You will compare the classic DerSimonian-Laird method with the more robust REML and Hartung-Knapp approach, gaining insight into why modern methods provide more reliable conclusions when estimating between-study heterogeneity from limited data.", "problem": "A translational medicine team is synthesizing early-phase evidence from a small set of studies to estimate a population-average log risk ratio, using a normal–normal random-effects model as the foundational base. Let $y_i$ denote the study-level log risk ratio estimate with known within-study variance $v_i$, for $i \\in \\{1,\\dots,k\\}$. The hierarchical model is $y_i \\mid \\theta_i \\sim \\mathcal{N}(\\theta_i, v_i)$ and $\\theta_i \\sim \\mathcal{N}(\\mu, \\tau^{2})$, implying the marginal model $y_i \\sim \\mathcal{N}(\\mu, v_i + \\tau^{2})$. Consider the case where the number of studies is small, $k = 5$, and the within-study standard errors are equal and known. Specifically, the five studies report the following log risk ratio estimates with their (equal) within-study standard errors:\n- Study-level point estimates: $y = (0.10, 0.15, 0.05, 0.20, 0.00)$,\n- Within-study standard errors: $\\mathrm{se}_i = 0.07$ for all $i$, hence $v_i = 0.0049$ for all $i$.\n\nYou are asked to do two things grounded in first principles and well-tested definitions:\n1) Starting from the normal–normal random-effects model and the definition of restricted maximum likelihood (REML), and recognizing the small-$k$ setting, justify the preference for using REML in combination with the Hartung–Knapp–Sidik–Jonkman (HKSJ) adjustment over the classical DerSimonian–Laird (DL) method without small-sample adjustment for constructing a two-sided $95\\%$ confidence interval (CI) for $\\mu$. Your justification must rely on properties such as estimator bias of $\\tau^{2}$, finite-sample uncertainty in $\\hat{\\mu}$, and coverage of the CI under small $k$.\n2) Using only the model definitions and those methods’ standard constructions, compute how the two-sided $95\\%$ CI width under REML with the HKSJ adjustment changes relative to the DL method without small-sample adjustment. Concretely, compute the ratio\n$$\nR \\equiv \\frac{\\text{CI width under REML + HKSJ}}{\\text{CI width under DL without adjustment}}\n$$\nfor the data given above. Round your final numerical answer for $R$ to four significant figures. The answer is dimensionless and must be reported as a single number without a percentage sign.", "solution": "The problem is found to be valid as it is scientifically grounded in the statistical theory of meta-analysis, well-posed with sufficient data, and objective in its formulation. We can thus proceed with the solution.\n\nThe problem asks for two components: a justification for preferring the Restricted Maximum Likelihood (REML) method with the Hartung–Knapp–Sidik–Jonkman (HKSJ) adjustment over the DerSimonian–Laird (DL) method for confidence interval (CI) construction in a small-sample setting, and a calculation of the ratio of the resulting CI widths for the given data.\n\n### Part 1: Justification of REML+HKSJ over DL\n\nThe normal–normal random-effects model for a meta-analysis is specified by the two-stage hierarchy:\n$$\ny_i \\mid \\theta_i \\sim \\mathcal{N}(\\theta_i, v_i) \\\\\n\\theta_i \\sim \\mathcal{N}(\\mu, \\tau^{2})\n$$\nwhere $y_i$ is the observed effect estimate (e.g., log risk ratio) from study $i$, $v_i$ is its known within-study variance, $\\theta_i$ is the true study-specific effect, $\\mu$ is the overall mean effect, and $\\tau^2$ is the between-study variance (heterogeneity). The marginal distribution of $y_i$ is $y_i \\sim \\mathcal{N}(\\mu, v_i + \\tau^{2})$. The goal is to estimate $\\mu$ and construct a confidence interval for it. This requires estimating $\\tau^2$.\n\n**The DerSimonian–Laird (DL) Method:**\nThe DL method is a non-iterative, method-of-moments approach to estimate $\\tau^2$. The estimator, $\\hat{\\tau}_{DL}^2$, is based on Cochran's Q-statistic, $Q = \\sum_{i=1}^k w_i(y_i - \\hat{\\mu}_{FE})^2$, where $w_i = 1/v_i$ are fixed-effects weights and $\\hat{\\mu}_{FE}$ is the fixed-effect pooled estimate. The DL estimator is given by $\\hat{\\tau}_{DL}^2 = \\max\\left(0, \\frac{Q - (k-1)}{C}\\right)$, where $C = \\sum w_i - (\\sum w_i^2)/(\\sum w_i)$.\n\nThe overall mean $\\mu$ is then estimated as a weighted average $\\hat{\\mu}_{DL} = \\frac{\\sum \\hat{w}_i y_i}{\\sum \\hat{w}_i}$ with random-effects weights $\\hat{w}_i = 1/(v_i + \\hat{\\tau}_{DL}^2)$. A two-sided $(1-\\alpha) \\times 100\\%$ CI for $\\mu$ is conventionally constructed as:\n$$\n\\hat{\\mu}_{DL} \\pm z_{1-\\alpha/2} \\sqrt{\\mathrm{Var}(\\hat{\\mu}_{DL})}\n$$\nwhere $\\mathrm{Var}(\\hat{\\mu}_{DL}) = 1/\\sum \\hat{w}_i$ and $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$ quantile of the standard normal distribution.\n\nThe main statistical deficiencies of the DL method, particularly for a small number of studies ($k$), are:\n1.  **Bias in $\\hat{\\tau}^2$:** The sampling distribution of $\\hat{\\tau}_{DL}^2$ can be highly skewed and variable, and the estimator often exhibits a substantial negative bias, meaning it tends to underestimate the true heterogeneity $\\tau^2$. This underestimation can be severe when $k$ is small.\n2.  **Ignoring Uncertainty in $\\hat{\\tau}^2$:** The DL CI construction plugs the estimate $\\hat{\\tau}_{DL}^2$ into the variance formula for $\\hat{\\mu}$ and then proceeds as if this variance were known. It uses a critical value from the normal distribution ($z_{1-\\alpha/2}$), which completely ignores the additional uncertainty introduced by having to estimate $\\tau^2$.\n3.  **Poor CI Coverage:** The combination of an underestimated $\\tau^2$ and the failure to account for its estimation uncertainty leads to CIs that are systemically too narrow. Consequently, the actual coverage probability of the DL CI can be substantially lower than the nominal level (e.g., much less than $95\\%$) when $k$ is small.\n\n**The REML Method with HKSJ Adjustment:**\nRestricted Maximum Likelihood (REML) is an alternative method for estimating $\\tau^2$. Unlike standard maximum likelihood, which produces biased estimates of variance components, REML accounts for the degrees of freedom lost in estimating the fixed effects (here, $\\mu$). The REML estimator $\\hat{\\tau}_{REML}^2$ generally has lower bias than both the ML and DL estimators in small-sample settings.\n\nThe Hartung–Knapp–Sidik–Jonkman (HKSJ) method is an adjustment for constructing the CI for $\\mu$ that explicitly addresses the small-sample issues that plague the DL method. The HKSJ CI is given by:\n$$\n\\hat{\\mu} \\pm t_{k-1, 1-\\alpha/2} \\sqrt{\\mathrm{Var}_{HKSJ}(\\hat{\\mu})}\n$$\nwhere $\\hat{\\mu}$ is the pooled estimate using weights based on $\\hat{\\tau}_{REML}^2$, and $t_{k-1, 1-\\alpha/2}$ is the $(1-\\alpha/2)$ quantile of a $t$-distribution with $k-1$ degrees of freedom. The HKSJ method modifies the variance estimator to be $\\mathrm{Var}_{HKSJ}(\\hat{\\mu}) = \\frac{\\sum \\hat{w}_i (y_i - \\hat{\\mu})^2}{(k-1)\\sum \\hat{w}_i}$.\n\nThe preference for REML+HKSJ is justified by:\n1.  **Improved $\\tau^2$ Estimation:** REML provides a more reliable (less biased) estimate of the between-study variance $\\tau^2$ compared to DL, especially with small $k$.\n2.  **Accounting for Uncertainty:** The HKSJ method makes two crucial changes. First, it uses an improved variance estimator for $\\hat{\\mu}$ that better reflects data variability. Second, and more importantly, it uses a critical value from the $t$-distribution with $k-1$ degrees of freedom instead of the normal distribution. The heavier tails of the $t$-distribution relative to the normal distribution produce a wider CI, which appropriately accounts for the extra uncertainty arising from the estimation of $\\tau^2$ from a small number of studies.\n3.  **Superior CI Coverage:** Extensive simulation studies have demonstrated that the HKSJ CI maintains a coverage probability much closer to the nominal level across a wide range of conditions, even for very small $k$, whereas the DL interval frequently fails to do so. This makes the HKSJ method more robust and reliable for drawing inferences in small-sample meta-analyses.\n\nIn summary, the REML+HKSJ approach is preferred because it combines a better estimator for the heterogeneity variance with a CI construction that properly accounts for the uncertainty in that estimate, leading to more accurate and reliable statistical inference, particularly when the number of studies $k$ is small.\n\n### Part 2: Calculation of the CI Width Ratio\n\nWe need to compute the ratio $R = \\frac{\\text{CI width (REML+HKSJ)}}{\\text{CI width (DL)}}$.\nThe given data are:\n- Number of studies: $k = 5$.\n- Study-level estimates: $y = (0.10, 0.15, 0.05, 0.20, 0.00)$.\n- Within-study variances: $v_i = v = 0.07^2 = 0.0049$ for all $i \\in \\{1, \\dots, 5\\}$.\n- Significance level: $\\alpha = 0.05$ for a $95\\%$ CI.\n\nA critical simplification arises because all within-study variances $v_i$ are equal.\n\n**Shared Calculations:**\nWith equal $v_i = v$, the fixed-effects weights are equal ($w_i=1/v$), and the fixed-effect estimate $\\hat{\\mu}_{FE}$ is the arithmetic mean of the study estimates:\n$$\n\\bar{y} = \\frac{1}{5}(0.10 + 0.15 + 0.05 + 0.20 + 0.00) = \\frac{0.50}{5} = 0.10\n$$\nThe sample variance of the study estimates is:\n$$\ns_y^2 = \\frac{1}{k-1}\\sum_{i=1}^k (y_i - \\bar{y})^2 = \\frac{1}{4}\\left[(0.10-0.10)^2 + (0.15-0.10)^2 + (0.05-0.10)^2 + (0.20-0.10)^2 + (0.00-0.10)^2\\right]\n$$\n$$\ns_y^2 = \\frac{1}{4}[0 + 0.0025 + 0.0025 + 0.0100 + 0.0100] = \\frac{0.025}{4} = 0.00625\n$$\nFor the case of equal $v_i$, both the DL and REML estimators for $\\tau^2$ simplify to the same form:\n$$\n\\hat{\\tau}_{DL}^2 = \\hat{\\tau}_{REML}^2 = \\max(0, s_y^2 - v)\n$$\nUsing the given values:\n$$\n\\hat{\\tau}^2 = \\max(0, 0.00625 - 0.0049) = \\max(0, 0.00135) = 0.00135\n$$\nSince $\\hat{\\tau}^2 > 0$, the random-effects weights are $\\hat{w}_i = 1/(v+\\hat{\\tau}^2) = 1/(0.0049+0.00135) = 1/0.00625$. Since these weights are also equal for all studies, the random-effects pooled estimate of $\\mu$ for both methods is simply the arithmetic mean, $\\hat{\\mu} = \\bar{y} = 0.10$.\n\n**DL Confidence Interval Width:**\nThe width of the DL CI is $W_{DL} = 2 \\times z_{1-\\alpha/2} \\times \\mathrm{SE}(\\hat{\\mu})$. The standard error is:\n$$\n\\mathrm{SE}_{DL}(\\hat{\\mu}) = \\sqrt{\\frac{1}{\\sum \\hat{w}_i}} = \\sqrt{\\frac{1}{k \\cdot \\hat{w}}} = \\sqrt{\\frac{v+\\hat{\\tau}^2}{k}}\n$$\nSubstituting $\\hat{\\tau}^2 = s_y^2 - v$, the term inside the square root becomes:\n$$\n\\frac{v + (s_y^2 - v)}{k} = \\frac{s_y^2}{k}\n$$\nSo, $\\mathrm{SE}_{DL}(\\hat{\\mu}) = \\sqrt{s_y^2/k}$. The CI width is:\n$$\nW_{DL} = 2 \\cdot z_{0.975} \\sqrt{\\frac{s_y^2}{k}}\n$$\n\n**REML+HKSJ Confidence Interval Width:**\nThe width of the HKSJ CI is $W_{HKSJ} = 2 \\times t_{k-1, 1-\\alpha/2} \\times \\mathrm{SE}_{HKSJ}(\\hat{\\mu})$. The HKSJ standard error is derived from its variance estimator:\n$$\n\\mathrm{Var}_{HKSJ}(\\hat{\\mu}) = \\frac{\\sum \\hat{w}_i (y_i - \\hat{\\mu})^2}{(k-1)\\sum \\hat{w}_i}\n$$\nSince $\\hat{w}_i$ are equal and $\\hat{\\mu}=\\bar{y}$:\n$$\n\\mathrm{Var}_{HKSJ}(\\hat{\\mu}) = \\frac{\\hat{w} \\sum (y_i - \\bar{y})^2}{(k-1)k\\hat{w}} = \\frac{\\sum (y_i - \\bar{y})^2}{k(k-1)} = \\frac{s_y^2}{k}\n$$\nThus, $\\mathrm{SE}_{HKSJ}(\\hat{\\mu}) = \\sqrt{s_y^2/k}$. The CI width is:\n$$\nW_{HKSJ} = 2 \\cdot t_{k-1, 0.975} \\sqrt{\\frac{s_y^2}{k}}\n$$\nRemarkably, for the case of equal within-study variances, the standard error terms for both CI methods are identical.\n\n**Ratio of the Widths:**\nThe ratio $R$ is therefore:\n$$\nR = \\frac{W_{HKSJ}}{W_{DL}} = \\frac{2 \\cdot t_{k-1, 0.975} \\sqrt{s_y^2/k}}{2 \\cdot z_{0.975} \\sqrt{s_y^2/k}} = \\frac{t_{k-1, 0.975}}{z_{0.975}}\n$$\nWith $k=5$, we need the critical value from a $t$-distribution with $k-1=4$ degrees of freedom.\nThe required quantiles are:\n- $z_{0.975} \\approx 1.959964$\n- $t_{4, 0.975} = 2.776445$\n\nThe ratio is:\n$$\nR = \\frac{2.776445}{1.959964} \\approx 1.416580\n$$\nRounding to four significant figures, we get $1.417$.", "answer": "$$\n\\boxed{1.417}\n$$", "id": "5014461"}]}