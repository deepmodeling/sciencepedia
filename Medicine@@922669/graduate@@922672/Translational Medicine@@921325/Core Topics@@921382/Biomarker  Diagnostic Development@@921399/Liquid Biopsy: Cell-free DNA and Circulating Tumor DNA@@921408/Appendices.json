{"hands_on_practices": [{"introduction": "To harness cell-free DNA for clinical insights, we must first be able to accurately count specific DNA molecules, which are often incredibly rare. Digital droplet PCR (ddPCR) provides a powerful method for absolute quantification by partitioning a sample into thousands of discrete reactions and counting them digitally. This foundational exercise guides you through the Poisson statistical model that underpins ddPCR, allowing you to derive the crucial relationship between the fraction of positive droplets and the average number of DNA copies per droplet, a cornerstone of modern molecular diagnostics. [@problem_id:5026331]", "problem": "A plasma sample containing cell-free deoxyribonucleic acid (cfDNA) is analyzed to quantify a rare somatic variant present as circulating tumor deoxyribonucleic acid (ctDNA) using Digital Droplet Polymerase Chain Reaction (ddPCR). In ddPCR, the reaction mix is stochastically partitioned into many uniform droplets, and each droplet is independently interrogated for the presence of the target variant. Assume the following fundamental basis:\n- Under uniform mixing and independent partitioning, the number of target template molecules allocated to each droplet follows a Poisson distribution with mean $\\lambda$ templates per droplet.\n- A droplet is scored as “positive” if at least one target template is present and successfully amplified and detected, and “negative” otherwise. Assume unit detection conditional on presence.\n\nStarting from these assumptions, derive an expression for the mean template copies per droplet $\\lambda$ in terms of the observed fraction of positive droplets $p$.\n\nThen, consider an experiment in which $18{,}000$ droplets are generated and $3{,}600$ are observed positive for the target variant. Using your derived expression, compute the numerical value of $\\lambda$. Round your numerical answer to four significant figures. Express the final quantity as copies per droplet (dimensionless).", "solution": "The problem is valid as it is scientifically grounded in the principles of molecular biology and statistics, well-posed, objective, and contains all necessary information for a unique solution.\n\nThe problem requires the derivation of an expression for the average number of template molecules per droplet, $\\lambda$, based on the fraction of positive droplets, $p$, and then to calculate a numerical value for $\\lambda$ from experimental data.\n\nThe fundamental premise is that the number of target molecules, let's call this integer variable $k$, in any given droplet follows a Poisson distribution with mean $\\lambda$. The probability mass function for a Poisson distribution is given by:\n$$P(k; \\lambda) = \\frac{\\lambda^k \\exp(-\\lambda)}{k!}$$\nwhere $k$ is a non-negative integer ($k=0, 1, 2, \\dots$) representing the number of target molecules in a droplet.\n\nAccording to the problem statement, a droplet is scored as \"negative\" if it contains zero target molecules. Therefore, the probability of a droplet being negative, which we can denote as $p_{\\text{neg}}$, corresponds to the case where $k=0$:\n$$p_{\\text{neg}} = P(k=0; \\lambda) = \\frac{\\lambda^0 \\exp(-\\lambda)}{0!}$$\nSince $\\lambda^0 = 1$ and $0! = 1$, this simplifies to:\n$$p_{\\text{neg}} = \\exp(-\\lambda)$$\n\nA droplet is scored as \"positive\" if it contains at least one target molecule, i.e., $k \\ge 1$. The problem statement assumes unit detection efficiency, meaning a droplet with $k \\ge 1$ molecules will always be detected as positive. The probability of a droplet being positive, denoted by $p$, is the complement of the probability of it being negative:\n$$p = P(k \\ge 1) = 1 - P(k=0) = 1 - p_{\\text{neg}}$$\nSubstituting the expression for $p_{\\text{neg}}$, we get the relationship between the fraction of positive droplets $p$ and the mean template copies per droplet $\\lambda$:\n$$p = 1 - \\exp(-\\lambda)$$\n\nTo derive an expression for $\\lambda$ in terms of $p$, we rearrange the equation above:\n$$\\exp(-\\lambda) = 1 - p$$\nTaking the natural logarithm of both sides gives:\n$$\\ln(\\exp(-\\lambda)) = \\ln(1 - p)$$\n$$-\\lambda = \\ln(1 - p)$$\nMultiplying by $-1$ yields the desired expression for $\\lambda$:\n$$\\lambda = -\\ln(1 - p)$$\nThis is the required expression. It is a cornerstone of absolute quantification using digital PCR, often referred to as a \"Poisson correction\".\n\nNext, we apply this formula to the provided experimental data. The total number of droplets generated is $N_{\\text{total}} = 18,000$. The number of observed positive droplets is $N_{\\text{pos}} = 3,600$.\n\nThe observed fraction of positive droplets, $p$, is the ratio of the number of positive droplets to the total number of droplets:\n$$p = \\frac{N_{\\text{pos}}}{N_{\\text{total}}} = \\frac{3,600}{18,000} = \\frac{36}{180} = \\frac{1}{5} = 0.2$$\n\nNow, we substitute this value of $p$ into the derived expression for $\\lambda$:\n$$\\lambda = -\\ln(1 - 0.2)$$\n$$\\lambda = -\\ln(0.8)$$\n\nTo obtain the numerical value, we compute the natural logarithm of $0.8$:\n$$\\ln(0.8) \\approx -0.2231435513$$\nTherefore, $\\lambda$ is:\n$$\\lambda \\approx -(-0.2231435513) \\approx 0.2231435513$$\n\nThe problem requires the numerical answer to be rounded to four significant figures. The first four significant figures are $2$, $2$, $3$, and $1$. The fifth significant figure is $4$, which is less than $5$, so we round down (truncate).\n$$\\lambda \\approx 0.2231$$\nThis value represents the average number of template copies per droplet.", "answer": "$$\\boxed{0.2231}$$", "id": "5026331"}, {"introduction": "While next-generation sequencing (NGS) allows for a massive survey of potential mutations, the necessary step of PCR amplification can introduce significant measurement bias, skewing the apparent frequency of variants. This practice delves into the quantitative impact of this amplification bias, challenging you to derive the expected error in a naive variant allele frequency (VAF) estimate. You will then see mathematically how Unique Molecular Identifiers (UMIs) correct this distortion by enabling bioinformatic removal of PCR duplicates, restoring an unbiased view of the original molecular population. [@problem_id:5026378]", "problem": "A translational oncology team is analyzing plasma cell-free DNA (cfDNA) to detect a single-nucleotide variant in circulating tumor DNA (ctDNA) at low variant allele frequency (VAF). The team performs hybrid-capture sequencing with Unique Molecular Identifiers (UMIs) but wants to understand the quantitative impact of retaining polymerase chain reaction (PCR) duplicates on VAF estimation, and how UMI-aware collapsing changes the estimator.\n\nAssume the following scientifically grounded model.\n\n- There are $N$ original cfDNA molecules at the locus of interest, each tagged by a UMI drawn from a very large code space so that UMI collisions are negligible at low VAF. A fraction $p$ of these molecules carries the variant, and a fraction $1-p$ is wild-type.\n- Each original molecule is amplified and sequenced, producing a random number of reads (PCR duplicate family size). For variant molecules, the duplication factor is a nonnegative random variable $D_{v}$ with finite mean $\\mu_{v} = \\mathbb{E}[D_{v}]$. For wild-type molecules, the duplication factor is a nonnegative random variable $D_{r}$ with finite mean $\\mu_{r} = \\mathbb{E}[D_{r}]$. Assume $\\{D_{v}\\}$ and $\\{D_{r}\\}$ are independent across molecules and independent of UMI assignment. It is empirically observed that sequence-context differences can lead to $\\mu_{v} \\neq \\mu_{r}$.\n- The naive read-level VAF estimator that retains PCR duplicates is defined as the total number of reads carrying the variant divided by the total number of reads, evaluated at the locus.\n- The UMI-aware estimator collapses all reads sharing the same UMI into a single consensus per original molecule and defines VAF as the number of UMI families whose consensus is variant divided by the total number of UMI families.\n\nStarting from core definitions in probability and statistics (linearity of expectation and the law of large numbers for independent, identically distributed families), derive expressions for the expected naive VAF and the expected UMI-collapsed VAF in terms of $p$, $\\mu_{v}$, and $\\mu_{r}$. Then, using the scientifically plausible low-VAF parameters $p = 0.004$, $\\mu_{v} = 3.6$, and $\\mu_{r} = 3.0$, compute the absolute bias introduced by PCR duplicate retention,\n$$\nb \\equiv \\mathbb{E}[\\widehat{\\mathrm{VAF}}_{\\mathrm{naive}}] - p,\n$$\nand provide its value. Express the final answer as a decimal and round your answer to four significant figures. No physical units are required.", "solution": "The problem as stated is scientifically sound, well-posed, and objective. It presents a standard, albeit simplified, model for analyzing PCR bias in next-generation sequencing data from liquid biopsies. The model is based on established statistical principles and uses plausible parameters. The task is clearly defined and all necessary information is provided. Therefore, a solution can be derived.\n\nThe problem asks for the expected values of two different estimators for the variant allele frequency (VAF), and for the bias of the naive estimator. Let $p$ be the true VAF, which is the fraction of original cfDNA molecules carrying the variant. Let $N$ be the total number of original cfDNA molecules. The number of original variant molecules is a random variable $N_v$, which follows a binomial distribution, $N_v \\sim \\mathrm{Binomial}(N, p)$. The number of original wild-type molecules is $N_r = N - N_v$.\n\nFirst, let's analyze the UMI-aware VAF estimator, $\\widehat{\\mathrm{VAF}}_{\\mathrm{UMI}}$. By definition, this estimator collapses all reads with the same Unique Molecular Identifier (UMI) into a single consensus sequence representing one original molecule. The VAF is then the ratio of the number of unique molecules (UMI families) with the variant to the total number of unique molecules.\n$$\n\\widehat{\\mathrm{VAF}}_{\\mathrm{UMI}} = \\frac{\\text{Number of variant UMI families}}{\\text{Total number of UMI families}}\n$$\nGiven the assumption that UMI collisions are negligible, each of the $N$ original molecules is tagged with a unique UMI. Thus, the total number of UMI families is exactly $N$. The number of variant UMI families is the number of original molecules that carry the variant, which is $N_v$.\n$$\n\\widehat{\\mathrm{VAF}}_{\\mathrm{UMI}} = \\frac{N_v}{N}\n$$\nTo find the expected value of this estimator, we use the property of the binomial distribution that $\\mathbb{E}[N_v] = Np$.\n$$\n\\mathbb{E}[\\widehat{\\mathrm{VAF}}_{\\mathrm{UMI}}] = \\mathbb{E}\\left[\\frac{N_v}{N}\\right] = \\frac{1}{N}\\mathbb{E}[N_v] = \\frac{Np}{N} = p\n$$\nThe UMI-aware estimator is an unbiased estimator of the true VAF, $p$. This is the theoretical justification for using UMIs to correct for PCR amplification bias.\n\nNext, let's analyze the naive read-level VAF estimator, $\\widehat{\\mathrm{VAF}}_{\\mathrm{naive}}$. This is defined as the total number of variant reads divided by the total number of reads. Let $V$ be the total count of variant reads and $R$ be the total count of all reads at the locus.\n$$\n\\widehat{\\mathrm{VAF}}_{\\mathrm{naive}} = \\frac{V}{R}\n$$\nThe total number of variant reads $V$ is the sum of reads from each of the $N_v$ original variant molecules. Let $D_{v,i}$ be the number of duplicate reads from the $i$-th original variant molecule, for $i=1, \\dots, N_v$. The variables $D_{v,i}$ are independent and identically distributed (i.i.d.) random variables with mean $\\mathbb{E}[D_{v,i}] = \\mu_v$.\n$$\nV = \\sum_{i=1}^{N_v} D_{v,i}\n$$\nSimilarly, the total number of wild-type reads $W$ is the sum of reads from each of the $N_r = N-N_v$ original wild-type molecules. Let $D_{r,j}$ be the number of duplicate reads from the $j$-th original wild-type molecule, for $j=1, \\dots, N_r$. These are i.i.d. random variables with mean $\\mathbb{E}[D_{r,j}] = \\mu_r$.\n$$\nW = \\sum_{j=1}^{N_r} D_{r,j}\n$$\nThe total number of reads is $R = V + W$. The naive estimator is therefore:\n$$\n\\widehat{\\mathrm{VAF}}_{\\mathrm{naive}} = \\frac{\\sum_{i=1}^{N_v} D_{v,i}}{\\sum_{i=1}^{N_v} D_{v,i} + \\sum_{j=1}^{N_r} D_{r,j}}\n$$\nThis is a ratio of random variables, and in general $\\mathbb{E}[V/R] \\neq \\mathbb{E}[V]/\\mathbb{E}[R]$. However, the problem states we can use the law of large numbers, which is appropriate for a large number of initial molecules $N$. As $N \\to \\infty$, the sample averages converge in probability to their expected values.\nLet's consider the number of reads per original molecule.\nThe number of variant reads per original molecule is $\\frac{V}{N} = \\frac{N_v}{N} \\left(\\frac{1}{N_v} \\sum_{i=1}^{N_v} D_{v,i}\\right)$.\nAs $N \\to \\infty$:\n1. $\\frac{N_v}{N} \\xrightarrow{p} p$ (by the Law of Large Numbers for Bernoulli trials).\n2. For $p > 0$, $N_v \\to \\infty$, so $\\frac{1}{N_v} \\sum_{i=1}^{N_v} D_{v,i} \\xrightarrow{p} \\mu_v$ (by the Law of Large Numbers for i.i.d. random variables $D_{v,i}$).\nThus, the total number of variant reads, scaled by $N$, converges to: $\\frac{V}{N} \\xrightarrow{p} p \\mu_v$.\n\nSimilarly, the number of wild-type reads per original molecule is $\\frac{W}{N} = \\frac{N_r}{N} \\left(\\frac{1}{N_r} \\sum_{j=1}^{N_r} D_{r,j}\\right)$.\nAs $N \\to \\infty$:\n1. $\\frac{N_r}{N} = \\frac{N-N_v}{N} = 1 - \\frac{N_v}{N} \\xrightarrow{p} 1-p$.\n2. For $p < 1$, $N_r \\to \\infty$, so $\\frac{1}{N_r} \\sum_{j=1}^{N_r} D_{r,j} \\xrightarrow{p} \\mu_r$.\nThus, the total number of wild-type reads, scaled by $N$, converges to: $\\frac{W}{N} \\xrightarrow{p} (1-p)\\mu_r$.\n\nThe total number of reads scaled by $N$ is $\\frac{R}{N} = \\frac{V}{N} + \\frac{W}{N}$, which converges in probability to $p\\mu_v + (1-p)\\mu_r$.\nThe estimator can be written as $\\widehat{\\mathrm{VAF}}_{\\mathrm{naive}} = \\frac{V/N}{R/N}$. By the continuous mapping theorem, the estimator converges in probability to the ratio of the limits:\n$$\n\\widehat{\\mathrm{VAF}}_{\\mathrm{naive}} \\xrightarrow{p} \\frac{p \\mu_v}{p \\mu_v + (1-p) \\mu_r}\n$$\nSince the estimator converges in probability to this constant value and is bounded (between $0$ and $1$), its expected value converges to the same value. Therefore, for large $N$:\n$$\n\\mathbb{E}[\\widehat{\\mathrm{VAF}}_{\\mathrm{naive}}] \\approx \\frac{p \\mu_v}{p \\mu_v + (1-p) \\mu_r}\n$$\nThe problem defines the absolute bias as $b \\equiv \\mathbb{E}[\\widehat{\\mathrm{VAF}}_{\\mathrm{naive}}] - p$. Substituting our result for the expected naive VAF:\n$$\nb = \\frac{p \\mu_v}{p \\mu_v + (1-p) \\mu_r} - p\n$$\nTo simplify this expression, we find a common denominator:\n$$\nb = \\frac{p \\mu_v - p (p \\mu_v + (1-p) \\mu_r)}{p \\mu_v + (1-p) \\mu_r} = \\frac{p \\mu_v - p^2 \\mu_v - p(1-p)\\mu_r}{p \\mu_v + (1-p) \\mu_r}\n$$\nFactoring out $p(1-p)$ from the numerator terms:\n$$\np \\mu_v - p^2 \\mu_v = p(1-p)\\mu_v\n$$\nSo the expression for bias becomes:\n$$\nb = \\frac{p(1-p)\\mu_v - p(1-p)\\mu_r}{p \\mu_v + (1-p) \\mu_r} = \\frac{p(1-p)(\\mu_v - \\mu_r)}{p \\mu_v + (1-p) \\mu_r}\n$$\nThis expression demonstrates that the bias is zero if and only if $\\mu_v = \\mu_r$, i.e., there is no PCR amplification bias.\n\nNow, we substitute the given numerical values: $p = 0.004$, $\\mu_v = 3.6$, and $\\mu_r = 3.0$.\nFirst, calculate the denominator:\n$$\n\\text{Denominator} = p \\mu_v + (1-p) \\mu_r = (0.004)(3.6) + (1-0.004)(3.0)\n$$\n$$\n\\text{Denominator} = 0.0144 + (0.996)(3.0) = 0.0144 + 2.988 = 3.0024\n$$\nNext, calculate the numerator:\n$$\n\\text{Numerator} = p(1-p)(\\mu_v - \\mu_r) = (0.004)(0.996)(3.6 - 3.0)\n$$\n$$\n\\text{Numerator} = (0.004)(0.996)(0.6) = 0.0023904\n$$\nNow, compute the bias $b$:\n$$\nb = \\frac{0.0023904}{3.0024} \\approx 0.0007961630...\n$$\nThe problem requires rounding the answer to four significant figures. The first significant figure is $7$, followed by $9$, $6$, and $1$. The fifth significant figure is $6$, so we round up the fourth digit.\n$$\nb \\approx 0.0007962\n$$", "answer": "$$\\boxed{0.0007962}$$", "id": "5026378"}, {"introduction": "The clinical utility of liquid biopsy extends beyond detecting single mutations to identifying the tissue of origin for circulating tumor DNA, a critical step for localizing a previously unknown cancer. This is often achieved by analyzing genome-wide DNA methylation patterns, as each tissue carries a distinct epigenetic signature. This advanced practice challenges you to formulate a computational deconvolution model using non-negative least squares to unmix a composite cfDNA methylation signal and estimate the fractional contributions of source tissues, demonstrating a powerful data science approach in translational medicine. [@problem_id:5026362]", "problem": "Consider the translational medicine task of deconvolving tissue-of-origin contributions from a cell-free deoxyribonucleic acid (cfDNA) methylation profile using a reference methylome atlas. Assume a linear mixture model grounded in mass balance: the cfDNA methylation fraction at each locus is a convex combination of tissue-specific methylation fractions weighted by non-negative tissue contribution fractions. Specifically, let $M \\in \\mathbb{R}^{m \\times t}$ be a reference atlas whose column $j$ contains the methylation fractions for tissue $j$ across $m$ loci, and let $y \\in \\mathbb{R}^{m}$ be the observed cfDNA methylation fractions at those loci. The unknown tissue contribution vector is $x \\in \\mathbb{R}^{t}$, where each component satisfies non-negativity and the components sum to $1$ (interpreted as fractions, expressed in decimal form).\n\nStart from the following fundamental base and definitions:\n- The mixture model: for each locus $i \\in \\{1,\\dots,m\\}$, the observed fraction $y_i$ adheres to $y_i = \\sum_{j=1}^{t} M_{ij} x_j + \\varepsilon_i$, where $\\varepsilon_i$ represents measurement noise.\n- Physical constraints: tissue contributions are non-negative, so $x_j \\ge 0$ for all $j$, and normalized such that $\\sum_{j=1}^{t} x_j = 1$.\n- Weighted observations: loci have reliability weights $w_i > 0$ that scale the residuals, reflecting locus-specific confidence.\n\nFormulate a non-negative least squares deconvolution that estimates $x$ from $(M, y)$ by minimizing a weighted residual with an explicit penalty promoting the sum-to-one constraint, without violating non-negativity. Derive the augmented system that is solvable by Non-Negative Least Squares (NNLS) and explain the role of a scalar penalty parameter $\\lambda > 0$ in balancing data fidelity and normalization.\n\nYour program must implement the following optimization problem in a way that is testable:\n- Minimize $\\left\\lVert W(Mx - y) \\right\\rVert_2^2 + \\lambda \\left( \\mathbf{1}^\\top x - 1 \\right)^2$ subject to $x \\ge 0$, where $W = \\mathrm{diag}(w_1,\\dots,w_m)$, $\\mathbf{1}$ denotes a $t$-dimensional vector of ones, and all arithmetic is in real numbers.\n- Use the standard Non-Negative Least Squares solver by constructing an augmented design matrix and observation vector that encode the penalty term. After solving, return $x$ normalized to sum exactly to $1$ when its sum is positive (to reflect fractional contributions), keeping entries as decimals (not percentages).\n\nTest suite. Use the parameter sets below. All arrays are given explicitly as lists; entries are dimensionless methylation fractions, and all answers must be decimals. For each case, report the estimated tissue contribution vector $x$ as a list of floats. Your final output must aggregate the results from all cases as a single line in the format specified at the end of this problem.\n\nCase A (general, $m = 8$, $t = 4$):\n- Atlas $M$ has columns corresponding to four tissues; its entries by row are:\n  Row $1$: $[0.80, 0.20, 0.05, 0.10]$\n  Row $2$: $[0.10, 0.85, 0.20, 0.10]$\n  Row $3$: $[0.50, 0.45, 0.10, 0.05]$\n  Row $4$: $[0.20, 0.70, 0.30, 0.15]$\n  Row $5$: $[0.65, 0.25, 0.15, 0.10]$\n  Row $6$: $[0.30, 0.60, 0.40, 0.05]$\n  Row $7$: $[0.15, 0.40, 0.80, 0.05]$\n  Row $8$: $[0.90, 0.10, 0.05, 0.10]$\n- Observed cfDNA methylation $y$: $[0.47, 0.345, 0.405, 0.37, 0.43, 0.41, 0.355, 0.49]$\n- Reliability weights $w$: $[1, 1, 1, 1, 1, 1, 1, 1]$\n- Penalty parameter $\\lambda$: $10$\n\nCase B (boundary dominance, $m = 6$, $t = 3$):\n- Atlas $M$ rows:\n  Row $1$: $[0.90, 0.10, 0.05]$\n  Row $2$: $[0.20, 0.70, 0.10]$\n  Row $3$: $[0.70, 0.20, 0.10]$\n  Row $4$: $[0.40, 0.50, 0.10]$\n  Row $5$: $[0.60, 0.30, 0.05]$\n  Row $6$: $[0.80, 0.10, 0.05]$\n- Observed cfDNA methylation $y$: $[0.82, 0.25, 0.65, 0.41, 0.57, 0.73]$\n- Reliability weights $w$: $[1, 1, 1, 1, 1, 1]$\n- Penalty parameter $\\lambda$: $1$\n\nCase C (collinearity stress test, $m = 7$, $t = 3$):\n- Atlas $M$ rows:\n  Row $1$: $[0.60, 0.50, 0.55]$\n  Row $2$: $[0.20, 0.30, 0.25]$\n  Row $3$: $[0.40, 0.30, 0.35]$\n  Row $4$: $[0.30, 0.40, 0.35]$\n  Row $5$: $[0.50, 0.60, 0.55]$\n  Row $6$: $[0.70, 0.60, 0.65]$\n  Row $7$: $[0.10, 0.20, 0.15]$\n- Observed cfDNA methylation $y$: $[0.55, 0.25, 0.35, 0.35, 0.55, 0.65, 0.15]$\n- Reliability weights $w$: $[1, 1, 1, 1, 1, 1, 1]$\n- Penalty parameter $\\lambda$: $5$\n\nCase D (underdetermined system, $m = 4$, $t = 5$):\n- Atlas $M$ rows:\n  Row $1$: $[0.70, 0.20, 0.10, 0.50, 0.10]$\n  Row $2$: $[0.30, 0.80, 0.20, 0.40, 0.10]$\n  Row $3$: $[0.50, 0.40, 0.20, 0.60, 0.10]$\n  Row $4$: $[0.20, 0.60, 0.10, 0.70, 0.10]$\n- Observed cfDNA methylation $y$: $[0.45, 0.50, 0.52, 0.57]$\n- Reliability weights $w$: $[1.0, 0.8, 1.2, 1.0]$\n- Penalty parameter $\\lambda$: $100$\n\nInterpretation requirements:\n- Explain why each constraint arises from physical principles of cfDNA mixture and why the penalty formulation is appropriate for encouraging normalization without violating non-negativity.\n- Clarify the difference between exact equality constraints and penalized enforcement in the context of Non-Negative Least Squares.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the estimated contribution list for the corresponding case formatted as a bracketed comma-separated list with fixed-point decimals (for example, $[ [0.500000,0.300000,0.200000,0.000000], [\\dots], \\dots ]$ but without spaces). The contributions must be expressed as decimals, not percentages.", "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n\nThe problem provides the following data, definitions, and conditions:\n\n-   **Model**: A linear mixture model for cell-free DNA (cfDNA) methylation, given by $y_i = \\sum_{j=1}^{t} M_{ij} x_j + \\varepsilon_i$ for each locus $i$.\n-   **Variables**:\n    -   $M \\in \\mathbb{R}^{m \\times t}$: A reference methylome atlas, where $M_{ij}$ is the methylation fraction of locus $i$ in tissue $j$.\n    -   $y \\in \\mathbb{R}^{m}$: The observed cfDNA methylation fractions for $m$ loci.\n    -   $x \\in \\mathbb{R}^{t}$: The unknown tissue contribution vector.\n    -   $w \\in \\mathbb{R}^{m}$: A vector of positive reliability weights $w_i > 0$ for each locus.\n    -   $\\lambda \\in \\mathbb{R}$: A positive scalar penalty parameter, $\\lambda > 0$.\n-   **Constraints**:\n    -   Non-negativity: $x_j \\ge 0$ for all $j \\in \\{1,\\dots,t\\}$.\n    -   Sum-to-one: $\\sum_{j=1}^{t} x_j = 1$.\n-   **Optimization Problem**: To estimate $x$, we must minimize the objective function:\n    $$ \\min_{x \\ge 0} \\left\\lVert W(Mx - y) \\right\\rVert_2^2 + \\lambda \\left( \\mathbf{1}^\\top x - 1 \\right)^2 $$\n    where $W = \\mathrm{diag}(w_1,\\dots,w_m)$ and $\\mathbf{1}$ is a $t$-dimensional vector of ones.\n-   **Solution Method**: The problem must be solved by constructing an augmented system solvable by a standard Non-Negative Least Squares (NNLS) algorithm. After finding the initial solution, it must be normalized to sum exactly to $1$.\n-   **Test Suite**: Four specific test cases (A, B, C, D) are provided with explicit values for $M$, $y$, $w$, and $\\lambda$.\n\n### Step 2: Validate Using Extracted Givens\n\n1.  **Scientifically Grounded (Critical)**: The problem is firmly grounded in computational biology and translational medicine. The linear mixture model is a standard and physically motivated approach for deconvolution problems, based on the principle of mass balance. It assumes the observed signal (cfDNA methylation profile) is a linear superposition of signals from source tissues, weighted by their fractional contributions. This is a widely used model in genomics.\n2.  **Well-Posed**: The problem is formulated as a regularized least squares problem. The base problem of inverting $y=Mx$ can be ill-conditioned or ill-posed (especially if $m < t$ or if columns of $M$ are collinear). The introduction of the non-negativity constraint and the quadratic penalty term $\\lambda (\\mathbf{1}^\\top x - 1)^2$ (a form of Tikhonov regularization) serves to regularize the problem, promoting a unique, stable, and physically meaningful solution.\n3.  **Objective (Critical)**: The problem is stated using precise and unambiguous mathematical notation. All terms are clearly defined.\n4.  **Completeness and Consistency**: The problem is self-contained. All necessary components ($M, y, w, \\lambda$) are provided for each test case. The dimensions of the matrices and vectors within each case are consistent.\n5.  **Realism and Feasibility**: The values for methylation fractions (between $0$ and $1$) are physically realistic. The formulation of using a penalized sum-to-one constraint for an NNLS solver is a standard and practical technique in scientific computing.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. It is scientifically sound, mathematically well-posed, objective, and provides all necessary information to proceed with a solution.\n\n## Solution Derivation and Explanation\n\nThe task is to estimate the tissue contribution vector $x$ from an observed cfDNA methylation profile $y$ and a reference atlas $M$. The relationship is modeled as a linear system with noise, $y \\approx Mx$. This estimation is constrained by known physical principles.\n\n### Principle-Based Constraints and Objective Function\n\n1.  **Data Fidelity**: The primary goal is to find an $x$ that best explains the observed data $y$. This is captured by minimizing the difference between the model's prediction, $Mx$, and the observation, $y$. We use the squared Euclidean norm of the residual, $\\left\\lVert Mx - y \\right\\rVert_2^2$. To account for varying confidence in methylation measurements across different genomic loci, we introduce a diagonal weight matrix $W = \\mathrm{diag}(w_1, \\dots, w_m)$, leading to the weighted least squares term:\n    $$ \\text{Data Fidelity Term} = \\left\\lVert W(Mx - y) \\right\\rVert_2^2 = \\sum_{i=1}^{m} w_i^2 ( (Mx)_i - y_i )^2 $$\n    (Note: Conventionally, weights are applied to the squared residuals, so the objective is often written $\\sum w_i ((Mx)_i - y_i)^2$. If we define the input weights as the square root of the desired weights on the squared residuals, the formulation $\\left\\lVert W(Mx - y) \\right\\rVert_2^2$ with $W=\\mathrm{diag}(\\sqrt{w_1}, ...)$ is equivalent. The problem statement gives $W=\\mathrm{diag}(w_1, ...)$ and uses $\\left\\lVert W(Mx-y) \\right\\rVert_2^2$, implying our programmatic weights are the $w_i$ themselves, and they are squared in the objective function. We will follow the problem's formulation precisely.)\n\n2.  **Non-Negativity Constraint ($x \\ge 0$)**: The components $x_j$ represent fractional contributions of tissues. A negative contribution is physically meaningless. Therefore, we must enforce $x_j \\ge 0$ for all $j$. This is a \"hard\" constraint that cannot be violated.\n\n3.  **Sum-to-One Constraint ($\\mathbf{1}^\\top x = 1$)**: The vector $x$ represents the fractions of all tissues contributing to the cfDNA pool. Ideally, these fractions should sum to $1$ (or $100\\%$). However, enforcing this as a hard equality constraint ($\\mathbf{1}^\\top x = 1$) complicates the use of standard NNLS solvers, which are specifically designed for the problem form $\\min_z \\left\\lVert Az-b \\right\\rVert_2^2$ subject to $z \\ge 0$. Instead, we incorporate this condition as a \"soft\" constraint or penalty. We add a penalty term to the objective function that grows as the sum $\\mathbf{1}^\\top x$ deviates from $1$:\n    $$ \\text{Penalty Term} = \\lambda \\left( \\mathbf{1}^\\top x - 1 \\right)^2 $$\n    The parameter $\\lambda > 0$ is a regularization parameter that controls the trade-off: a larger $\\lambda$ more strictly enforces that the sum of contributions approaches $1$, potentially at the expense of a poorer fit to the data (a larger data fidelity term).\n\nCombining these terms gives the full objective function to be minimized under the non-negativity constraint:\n$$ \\mathcal{L}(x) = \\left\\lVert W(Mx - y) \\right\\rVert_2^2 + \\lambda \\left( \\mathbf{1}^\\top x - 1 \\right)^2 \\quad \\text{subject to} \\quad x \\ge 0 $$\n\n### Reformulation for Non-Negative Least Squares (NNLS)\n\nA standard NNLS solver finds the solution to $\\min_{z \\ge 0} \\left\\lVert Az - b \\right\\rVert_2^2$. To use such a solver, we must reformulate our objective function into this canonical form. The sum of squared norms can be expressed as the squared norm of a stacked vector.\n\nLet's rewrite the objective function:\n$$ \\mathcal{L}(x) = \\left\\lVert W(Mx - y) \\right\\rVert_2^2 + \\left\\lVert \\sqrt{\\lambda} (\\mathbf{1}^\\top x - 1) \\right\\rVert_2^2 $$\nThis is the sum of squared residuals for two separate linear systems. We can combine them into a single, larger system. Let's define an augmented matrix $A_{aug}$ and an augmented vector $b_{aug}$:\n$$\nA_{aug} =\n\\begin{pmatrix}\nWM \\\\\n\\sqrt{\\lambda} \\mathbf{1}^\\top\n\\end{pmatrix}\n\\quad \\text{and} \\quad\nb_{aug} =\n\\begin{pmatrix}\nWy \\\\\n\\sqrt{\\lambda} \\cdot 1\n\\end{pmatrix}\n$$\nHere, $WM$ is an $m \\times t$ matrix, and $\\sqrt{\\lambda} \\mathbf{1}^\\top$ is a $1 \\times t$ row vector. Thus, $A_{aug}$ is an $(m+1) \\times t$ matrix. Similarly, $Wy$ is an $m \\times 1$ vector, and $\\sqrt{\\lambda}$ is a scalar, so $b_{aug}$ is an $(m+1) \\times 1$ vector.\n\nWith these definitions, the augmented least squares problem becomes:\n$$\n\\left\\lVert A_{aug}x - b_{aug} \\right\\rVert_2^2 = \\left\\lVert \\begin{pmatrix} WM \\\\ \\sqrt{\\lambda} \\mathbf{1}^\\top \\end{pmatrix} x - \\begin{pmatrix} Wy \\\\ \\sqrt{\\lambda} \\end{pmatrix} \\right\\rVert_2^2 = \\left\\lVert \\begin{pmatrix} WMx - Wy \\\\ \\sqrt{\\lambda}\\mathbf{1}^\\top x - \\sqrt{\\lambda} \\end{pmatrix} \\right\\rVert_2^2\n$$\nThe squared norm of this stacked vector is the sum of the squared norms of its components:\n$$ \\left\\lVert WMx - Wy \\right\\rVert_2^2 + \\left\\lVert \\sqrt{\\lambda}(\\mathbf{1}^\\top x - 1) \\right\\rVert_2^2 = \\left\\lVert W(Mx - y) \\right\\rVert_2^2 + \\lambda \\left( \\mathbf{1}^\\top x - 1 \\right)^2 = \\mathcal{L}(x) $$\nThis is exactly our original objective function. Therefore, minimizing $\\mathcal{L}(x)$ subject to $x \\ge 0$ is equivalent to solving the following standard NNLS problem:\n$$ \\min_{x \\ge 0} \\left\\lVert A_{aug}x - b_{aug} \\right\\rVert_2^2 $$\n\n### Algorithm and Final Normalization\n\nThe complete algorithm is as follows:\n1.  Given $M, y, w, \\lambda$.\n2.  Construct the diagonal weight matrix $W = \\mathrm{diag}(w)$.\n3.  Construct the augmented matrix $A_{aug} = \\begin{pmatrix} W M \\\\ \\sqrt{\\lambda} \\mathbf{1}^\\top \\end{pmatrix}$.\n4.  Construct the augmented observation vector $b_{aug} = \\begin{pmatrix} W y \\\\ \\sqrt{\\lambda} \\end{pmatrix}$.\n5.  Use an NNLS solver to find $x_{unnorm} = \\arg\\min_{x \\ge 0} \\left\\lVert A_{aug}x - b_{aug} \\right\\rVert_2^2$.\n6.  The resulting vector $x_{unnorm}$ will satisfy non-negativity, and its components will sum to a value close to $1$, but not exactly $1$. To enforce the sum-to-one constraint as a hard condition post-optimization, we normalize the solution. Let $S = \\sum_j (x_{unnorm})_j$. If $S > 0$, the final solution is $x = x_{unnorm} / S$. If $S=0$, the solution is the zero vector. This final projection onto the simplex is a common and effective heuristic that yields a proper fractional contribution vector.\n\nThis approach elegantly combines the physical constraints and statistical model into a framework solvable by standard, efficient numerical methods.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import nnls\n\ndef solve():\n    \"\"\"\n    Solves the tissue deconvolution problem for a set of test cases\n    using a penalized Non-Negative Least Squares formulation.\n    \"\"\"\n\n    test_cases = [\n        # Case A (general, m = 8, t = 4)\n        {\n            \"M\": np.array([\n                [0.80, 0.20, 0.05, 0.10],\n                [0.10, 0.85, 0.20, 0.10],\n                [0.50, 0.45, 0.10, 0.05],\n                [0.20, 0.70, 0.30, 0.15],\n                [0.65, 0.25, 0.15, 0.10],\n                [0.30, 0.60, 0.40, 0.05],\n                [0.15, 0.40, 0.80, 0.05],\n                [0.90, 0.10, 0.05, 0.10]\n            ]),\n            \"y\": np.array([0.47, 0.345, 0.405, 0.37, 0.43, 0.41, 0.355, 0.49]),\n            \"w\": np.array([1, 1, 1, 1, 1, 1, 1, 1]),\n            \"lambda_val\": 10\n        },\n        # Case B (boundary dominance, m = 6, t = 3)\n        {\n            \"M\": np.array([\n                [0.90, 0.10, 0.05],\n                [0.20, 0.70, 0.10],\n                [0.70, 0.20, 0.10],\n                [0.40, 0.50, 0.10],\n                [0.60, 0.30, 0.05],\n                [0.80, 0.10, 0.05]\n            ]),\n            \"y\": np.array([0.82, 0.25, 0.65, 0.41, 0.57, 0.73]),\n            \"w\": np.array([1, 1, 1, 1, 1, 1]),\n            \"lambda_val\": 1\n        },\n        # Case C (collinearity stress test, m = 7, t = 3)\n        {\n            \"M\": np.array([\n                [0.60, 0.50, 0.55],\n                [0.20, 0.30, 0.25],\n                [0.40, 0.30, 0.35],\n                [0.30, 0.40, 0.35],\n                [0.50, 0.60, 0.55],\n                [0.70, 0.60, 0.65],\n                [0.10, 0.20, 0.15]\n            ]),\n            \"y\": np.array([0.55, 0.25, 0.35, 0.35, 0.55, 0.65, 0.15]),\n            \"w\": np.array([1, 1, 1, 1, 1, 1, 1]),\n            \"lambda_val\": 5\n        },\n        # Case D (underdetermined system, m = 4, t = 5)\n        {\n            \"M\": np.array([\n                [0.70, 0.20, 0.10, 0.50, 0.10],\n                [0.30, 0.80, 0.20, 0.40, 0.10],\n                [0.50, 0.40, 0.20, 0.60, 0.10],\n                [0.20, 0.60, 0.10, 0.70, 0.10]\n            ]),\n            \"y\": np.array([0.45, 0.50, 0.52, 0.57]),\n            \"w\": np.array([1.0, 0.8, 1.2, 1.0]),\n            \"lambda_val\": 100\n        }\n    ]\n\n    results_as_strings = []\n    \n    for case in test_cases:\n        M = case[\"M\"]\n        y = case[\"y\"]\n        w = case[\"w\"]\n        lambda_val = case[\"lambda_val\"]\n\n        m, t = M.shape\n\n        # Construct the diagonal weight matrix W\n        W = np.diag(w)\n\n        # Construct the augmented matrix A_aug\n        # First m rows: W * M\n        A_aug_top = W @ M\n        # Last row: sqrt(lambda) * 1^T\n        sqrt_lambda = np.sqrt(lambda_val)\n        A_aug_bottom = sqrt_lambda * np.ones((1, t))\n        \n        A_aug = np.vstack([A_aug_top, A_aug_bottom])\n\n        # Construct the augmented vector b_aug\n        # First m elements: W * y\n        b_aug_top = W @ y\n        # Last element: sqrt(lambda)\n        b_aug_bottom = np.array([sqrt_lambda])\n        \n        b_aug = np.hstack([b_aug_top, b_aug_bottom])\n        \n        # Solve the NNLS problem: min ||A_aug * x - b_aug||^2_2 s.t. x >= 0\n        x_unnormalized, _ = nnls(A_aug, b_aug)\n        \n        # Normalize the solution to sum to 1\n        x_sum = np.sum(x_unnormalized)\n        if x_sum > 0:\n            x_normalized = x_unnormalized / x_sum\n        else:\n            # If sum is 0, the solution is the zero vector, nothing to normalize\n            x_normalized = x_unnormalized\n\n        # Format the result vector into a string '[d.dddddd,...]'\n        vec_str = f\"[{','.join([f'{val:.6f}' for val in x_normalized])}]\"\n        results_as_strings.append(vec_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```", "id": "5026362"}]}