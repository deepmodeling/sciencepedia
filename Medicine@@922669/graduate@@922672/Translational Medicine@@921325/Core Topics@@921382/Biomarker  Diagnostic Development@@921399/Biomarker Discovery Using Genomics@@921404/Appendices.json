{"hands_on_practices": [{"introduction": "A central challenge in cancer genomics is to distinguish tumor-specific somatic mutations from the background of inherited germline variants. This practice [@problem_id:4994316] guides you through this critical filtering step by analyzing variant allele fractions ($VAF$) from matched tumor and normal samples. By synthesizing this information with population-level data, you will learn to apply the foundational logic used in every somatic biomarker discovery pipeline to correctly classify a variant and determine its relevance.", "problem": "In a translational oncology study aimed at discovering tumor-specific genomic biomarkers using Next-Generation Sequencing (NGS), a variant is observed in both a resected tumor sample and a matched peripheral blood normal from the same patient. The variant allele fraction (VAF) is $0.5$ in the tumor and $0.5$ in the matched normal. The variant is cataloged in the Genome Aggregation Database (gnomAD) with a population minor allele frequency of $5\\%$. Assume a diploid locus, adequate coverage, and no copy number alteration detected at this site by independent copy number analysis. Derive, from first principles, the expected VAF behavior under germline versus somatic scenarios and the implications of population allele frequency for pathogenicity. Then classify the variant and justify the decision in the context of tumor biomarker discovery.\n\nFoundational base to use:\n- Central distinction between germline and somatic variation: germline variants are present in all cells of the individual, while somatic variants arise post-zygotically and are restricted to a subset of cells.\n- Definition of variant allele fraction (VAF) as $V = \\frac{n_{\\text{alt}}}{n_{\\text{alt}} + n_{\\text{ref}}}$, where $n_{\\text{alt}}$ and $n_{\\text{ref}}$ are the read counts for the alternate and reference alleles, respectively.\n- Diploid expectation for a heterozygous genotype: each cell contributes one reference and one alternate allele, so the expected fraction of alternate alleles is $0.5$, ignoring sampling noise.\n- For a clonal somatic heterozygous variant in a diploid locus, with tumor purity $\\pi \\in [0,1]$, the expected VAF in the tumor is $V_{\\text{somatic,tumor}} = \\frac{\\pi}{2}$ (since tumor cells contribute one alternate of two total alleles, and normal cells contribute zero alternate of two total alleles), whereas in matched normal it is $V_{\\text{somatic,normal}} \\approx 0$.\n- Hardy–Weinberg equilibrium for population allele frequency $p$: expected heterozygote frequency $2p(1-p)$, used to reason about the commonness of genotypes in the population and the likelihood of high-penetrance pathogenicity when $p$ is large.\n\nWhich of the following is the most appropriate classification and action for this variant in a somatic biomarker discovery pipeline?\n\nA. Somatic clonal driver mutation at a diploid locus; retain as a tumor-specific actionable biomarker.\n\nB. Germline heterozygous common single nucleotide polymorphism; filter from the somatic call set and do not consider as a tumor-specific biomarker.\n\nC. Sequencing artifact due to systematic mapping error; discard the variant as a false positive.\n\nD. Somatic subclonal variant present at $50\\%$ cancer cell fraction driven by tumor purity $\\pi = 0.5$; retain as a tumor-specific biomarker.\n\nE. Tumor loss of heterozygosity (LOH) at this locus; treat as a copy number–driven biomarker of allelic imbalance.", "solution": "The problem statement is subjected to validation before proceeding to a solution.\n\n### Step 1: Extract Givens\n- **Study Context:** Translational oncology study for discovering tumor-specific genomic biomarkers.\n- **Technology:** Next-Generation Sequencing (NGS).\n- **Samples:** A matched pair of a resected tumor sample and a peripheral blood normal sample from the same patient.\n- **Variant Data:**\n    - A variant is observed in both the tumor and the matched normal sample.\n    - Variant Allele Fraction (VAF) in tumor: $VAF_{tumor} = 0.5$.\n    - Variant Allele Fraction (VAF) in matched normal: $VAF_{normal} = 0.5$.\n- **Population Data:** The variant is listed in the Genome Aggregation Database (gnomAD) with a population minor allele frequency (MAF) of $5\\%$, which is equivalent to $0.05$.\n- **Assumptions:**\n    - The genomic locus is diploid.\n    - Sequencing coverage is adequate.\n    - No copy number alteration (CNA) is detected at this locus.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for scientific validity and self-consistency.\n\n1.  **Scientifically Grounded:** The problem describes a standard and canonical scenario in clinical cancer genomics. The use of matched tumor-normal NGS data, VAF analysis, and population frequency databases (like gnomAD) to classify variants as somatic or germline is fundamental to the field. All concepts are based on established principles of genetics and bioinformatics. The foundational base provided is accurate.\n2.  **Well-Posed:** The problem is well-posed. The provided data ($VAF_{tumor}$, $VAF_{normal}$, pop-MAF) and explicit assumptions (diploid, no CNA) are sufficient and consistent to allow for a unique and unambiguous classification of the variant within the given context.\n3.  **Objective:** The problem statement is objective, using precise, quantitative data and standard terminology without any subjective or ambiguous language.\n\nThe problem statement is free of scientific unsoundness, incompleteness, contradiction, or any other listed flaw. It presents a realistic and formalizable challenge in genomic data interpretation.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A full derivation and analysis will now be performed.\n\n### Derivation from First Principles\n\nThe primary task is to classify the observed variant. This requires distinguishing between a germline and a somatic origin by analyzing the provided allele fraction data in conjunction with population frequency information.\n\n1.  **Distinction between Germline and Somatic Variants:**\n    - A **germline variant** is inherited and present in the constitutional DNA of the individual. Consequently, it should be present in virtually all cells of the body, including both tumor cells and non-cancerous cells (represented by the matched normal sample).\n    - A **somatic variant** is acquired post-zygotically and is restricted to the clonal population of cells in which it arose. In the context of cancer, a somatic mutation is expected to be present in the tumor cells but absent from the germline DNA, which is assayed from the matched normal sample.\n\n2.  **Expected Variant Allele Fraction (VAF) under Different Scenarios:**\n    The VAF is defined as the fraction of sequencing reads supporting the alternate allele: $V = \\frac{n_{\\text{alt}}}{n_{\\text{alt}} + n_{\\text{ref}}}$. We assume a diploid locus with no copy number alterations.\n\n    - **Scenario I: Germline Heterozygous Variant**\n        - In this case, every diploid cell in the body contains one reference allele and one alternate allele.\n        - In the **matched normal sample**, which is composed of constitutional cells, the expected proportion of alternate alleles is $1$ out of $2$. Therefore, the expected VAF is $VAF_{normal} \\approx \\frac{1}{2} = 0.5$.\n        - In the **tumor sample**, assuming no loss of heterozygosity (LOH), the tumor cells also carry the heterozygous germline variant. Therefore, the expected VAF is also $VAF_{tumor} \\approx \\frac{1}{2} = 0.5$.\n\n    - **Scenario II: Somatic Heterozygous Variant**\n        - This variant is absent in the constitutional DNA.\n        - In the **matched normal sample**, no alternate allele is present. Thus, the expected VAF is $VAF_{normal} \\approx 0$.\n        - In the **tumor sample**, the VAF depends on the tumor purity ($\\pi$) and the cancer cell fraction (CCF) of the variant. For a clonal variant (present in all tumor cells, $CCF=1$), the tumor cells are heterozygous. The VAF is a mixture of the contribution from tumor cells and contaminating normal cells: $VAF_{tumor} = \\frac{\\pi \\times (1/2)}{\\pi \\times 1 + (1-\\pi) \\times 1} = \\frac{\\pi}{2}$. The VAF is thus directly proportional to tumor purity and has a theoretical maximum of $0.5$ when $\\pi = 1$.\n\n3.  **Interpretation of the Observed Data:**\n    - The given data are $VAF_{tumor} = 0.5$ and $VAF_{normal} = 0.5$.\n    - The observation of $VAF_{normal} = 0.5$ is critically important. It decisively rules out a somatic origin (Scenario II) and perfectly matches the expectation for a **germline heterozygous variant** (Scenario I).\n\n4.  **Implication of Population Frequency:**\n    - The variant has a minor allele frequency (MAF) of $5\\%$ ($0.05$) in the gnomAD database. Variants with a population frequency greater than $1\\%$ are generally classified as **common polymorphisms**.\n    - High-penetrance driver mutations responsible for cancer are subjected to negative selective pressure and are therefore typically rare in the population (e.g., MAF $ 0.001$). A variant present in $5\\%$ of the general population is extremely unlikely to be a pathogenic driver mutation for cancer. If it were, the incidence of cancer would be far higher. Such common variants are considered part of the normal genetic variation within a species.\n\n5.  **Final Classification and Action:**\n    - **Classification:** The variant is a germline heterozygous common single nucleotide polymorphism (SNP).\n    - **Action in a Somatic Biomarker Pipeline:** The goal of the study is to find \"tumor-specific genomic biomarkers\". Since this variant is germline (not tumor-specific) and common (unlikely to be a pathogenic driver), it is considered a non-somatic, background variant. Standard bioinformatic pipelines for somatic variant discovery are designed to identify and **filter out** such germline variants by comparing the tumor to the matched normal.\n\n### Option-by-Option Analysis\n\n**A. Somatic clonal driver mutation at a diploid locus; retain as a tumor-specific actionable biomarker.**\nThis option claims the variant is somatic. This is definitively contradicted by the data showing $VAF_{normal} = 0.5$. A somatic variant should have a $VAF_{normal} \\approx 0$. Therefore, it is not tumor-specific.\n**Verdict: Incorrect.**\n\n**B. Germline heterozygous common single nucleotide polymorphism; filter from the somatic call set and do not consider as a tumor-specific biomarker.**\nThis option aligns perfectly with the derivation. \"Germline heterozygous\" is supported by $VAF_{normal} \\approx 0.5$ and $VAF_{tumor} \\approx 0.5$. \"Common single nucleotide polymorphism\" is supported by the gnomAD MAF of $5\\%$. The prescribed action to \"filter from the somatic call set\" is the correct procedure in a search for tumor-specific somatic biomarkers.\n**Verdict: Correct.**\n\n**C. Sequencing artifact due to systematic mapping error; discard the variant as a false positive.**\nThis is improbable. A true heterozygous state characteristically produces a stable VAF signal around $0.5$. Observing this clean signal in two independent samples (tumor and normal) makes an artifact highly unlikely. Artifacts are typically associated with lower, more variable VAFs and often show strand bias or other quality issues not mentioned here.\n**Verdict: Incorrect.**\n\n**D. Somatic subclonal variant present at $50\\%$ cancer cell fraction driven by tumor purity $\\pi = 0.5$; retain as a tumor-specific biomarker.**\nThis option is incorrect on multiple grounds. First, it claims a somatic origin, which is falsified by the $VAF_{normal}=0.5$. Second, the mathematics is flawed. For a somatic variant, $VAF_{tumor} = (\\pi \\times CCF) / 2$. If $\\pi=0.5$ and $CCF=0.5$, then $VAF_{tumor} = (0.5 \\times 0.5) / 2 = 0.125$, not $0.5$. To achieve a VAF of $0.5$, we would need $\\pi \\times CCF = 1$, which means the variant is clonal ($CCF=1$) and the tumor is $100\\%$ pure ($\\pi=1$), contradicting the premises of this option.\n**Verdict: Incorrect.**\n\n**E. Tumor loss of heterozygosity (LOH) at this locus; treat as a copy number–driven biomarker of allelic imbalance.**\nThis is contradicted by two key pieces of information. First, the problem explicitly states \"no copy number alteration detected\". LOH is a type of copy number alteration. Second, LOH at a heterozygous germline locus would lead to an allelic imbalance in the tumor, shifting the $VAF_{tumor}$ away from $0.5$ (either towards $1.0$ or towards $0.0$, depending on which allele is lost). The observed $VAF_{tumor}=0.5$ indicates a balanced heterozygous state, arguing against LOH.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{B}$$", "id": "4994316"}, {"introduction": "Genomic studies test thousands of hypotheses simultaneously, creating a statistical minefield where spurious associations are common. Simply using a traditional p-value threshold of $0.05$ would lead to an unacceptably high number of false positives. This exercise [@problem_id:4994350] tasks you with implementing the Benjamini-Hochberg procedure from first principles to control the False Discovery Rate ($FDR$), a cornerstone of modern statistical genomics. By calculating q-values, you will gain a practical understanding of how to maintain statistical rigor when navigating large-scale datasets.", "problem": "You are building a multiple-hypothesis testing module for a Next-Generation Sequencing (NGS) biomarker discovery pipeline in translational medicine. Each genomic feature (for example, a gene, transcript, or variant) is tested for association with a clinical phenotype, producing a list of $m$ independent p-values. Your task is to compute q-values that control the False Discovery Rate (FDR), and then identify which features are significant at a specified FDR level.\n\nFoundational base to use:\n- The definition of a p-value: under the null hypothesis, the p-value is the probability of observing a test statistic at least as extreme as the observed one.\n- The definition of the False Discovery Rate (FDR): the expected proportion of false positives among all declared discoveries.\n- The definition of a q-value: the minimum FDR threshold at which a particular feature would be called significant.\n- Independence of tests: assume the p-values arise from independent tests.\n\nRequirements:\n- Implement the computation of q-values from first principles, without calling any specialized multiple-testing correction routines from external libraries.\n- Use a mathematically valid procedure that provably controls FDR under independence.\n- Identify features significant at the FDR threshold $q  \\alpha$, where $\\alpha$ is given.\n- Use $0$-based indexing for feature indices in outputs.\n\nTest suite:\nCompute q-values and determine indices of significant features for each test case below. For each test case, set the FDR threshold $\\alpha = 0.05$ (that is, $0.05$).\n\n1. Case A (mixture of small and moderate p-values, $m = 20$):\n   p-values $p = [\\,0.0005,\\,0.001,\\,0.04,\\,0.2,\\,0.15,\\,0.5,\\,0.8,\\,0.03,\\,0.07,\\,0.06,\\,0.9,\\,0.4,\\,0.002,\\,0.049,\\,0.051,\\,0.12,\\,0.33,\\,0.005,\\,0.0015,\\,0.75\\,]$.\n   Output for this case must be the list of $0$-based indices of features with $q  0.05$, in ascending order.\n\n2. Case B (duplicates and extremes, $m = 10$):\n   p-values $p = [\\,0.0,\\,1.0,\\,1.0,\\,0.05,\\,0.05,\\,0.05,\\,0.001,\\,0.999,\\,0.2,\\,0.8\\,]$.\n   Output for this case must be the list of $0$-based indices of features with $q  0.05$, in ascending order.\n\n3. Case C (ascending p-values, $m = 11$):\n   p-values $p = [\\,0.001,\\,0.005,\\,0.01,\\,0.02,\\,0.03,\\,0.04,\\,0.06,\\,0.08,\\,0.1,\\,0.5,\\,1.0\\,]$.\n   Output for this case must be the list of $0$-based indices of features with $q  0.05$, in ascending order.\n\n4. Case D (large-scale independent tests typical of NGS, $m = 10000$):\n   Construct the p-values deterministically as follows. Let $\\pi_0 = 0.9$ and $\\pi_1 = 0.1$. Generate $9000$ null p-values independently from a Uniform distribution on $[0,1]$, and $1000$ alternative p-values independently from a Beta distribution $\\text{Beta}(a,b)$ with parameters $a = 0.3$ and $b = 1.0$, which concentrates mass near $0$. Use a fixed random seed of $12345$ to ensure reproducibility. Concatenate the two sets of p-values and uniformly shuffle them using the same seed to avoid any ordering bias. Set $\\alpha = 0.05$. For this case, output only the integer count of features with $q  0.05$ (do not list indices due to size).\n\nFinal output format:\nYour program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets. The first three entries must be lists of indices, and the fourth entry must be an integer count, for example, $[\\,[i_1,i_2,\\dots],\\,[j_1,j_2,\\dots],\\,[k_1,k_2,\\dots],\\,n\\,]$. All indices must be $0$-based integers. No additional text should be printed.", "solution": "The problem requires the implementation of a procedure to compute q-values for a given set of p-values and to identify which statistical tests are significant at a specified False Discovery Rate (FDR) threshold, $\\alpha$. This is a classic multiple hypothesis testing problem, central to biomarker discovery in genomics where thousands of features are tested simultaneously. The solution will be based on the widely used Benjamini-Hochberg (BH) procedure, which controls the FDR under the assumption of independent tests.\n\n### Principle: False Discovery Rate (FDR) Control\n\nWhen performing $m$ hypothesis tests, a certain number of null hypotheses may be incorrectly rejected (Type I errors, or false positives). The FDR is defined as the expected value of the ratio of false discoveries to total discoveries:\n$$\n\\text{FDR} = E\\left[ \\frac{V}{R} \\right]\n$$\nwhere $V$ is the number of true null hypotheses that are incorrectly rejected (false positives), and $R$ is the total number of hypotheses rejected (total discoveries). If $R=0$, the ratio is taken to be $0$.\n\nThe q-value of an individual test is defined as the minimum FDR at which that test would be declared significant. A feature with a q-value of $q_i$ means that if we set the FDR threshold to $q_i$, this feature would be on the borderline of significance. Therefore, to control the FDR at a level $\\alpha$, we reject all hypotheses for which the q-value is less than $\\alpha$.\n\n### Algorithmic Design: Benjamini-Hochberg Q-Value Calculation\n\nThe most direct \"first principles\" method to compute q-values that control the FDR under independence is derived from the Benjamini-Hochberg (1995) procedure. The algorithm is as follows:\n\n1.  **Input**: A list of $m$ p-values, $\\{p_1, p_2, \\dots, p_m\\}$, and an FDR threshold $\\alpha$.\n\n2.  **Sorting**: Let the original indices of the p-values be $\\{0, 1, \\dots, m-1\\}$. Sort the p-values in ascending order: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$. It is crucial to retain the original index corresponding to each sorted p-value. Let the original index of $p_{(i)}$ be $idx_{(i)}$.\n\n3.  **Rank-Based Adjustment**: The core of the BH procedure is to compare each sorted p-value $p_{(i)}$ to a critical value that depends on its rank, $i$. The BH-adjusted p-value, which we will use as the q-value, for the $i$-th sorted p-value is initially calculated as:\n    $$\n    q'_{\\text{raw},(i)} = \\frac{p_{(i)} \\cdot m}{i}\n    $$\n    This value represents the FDR we would incur if we rejected all hypotheses with p-values up to and including $p_{(i)}$.\n\n4.  **Enforcing Monotonicity**: By definition, if a test with p-value $p_{(i)}$ is significant, any test with a smaller p-value $p_{(j)}$ (where $j  i$) must also be significant. This implies that the corresponding q-values must be non-decreasing with respect to the sorted p-values, i.e., $q_{(1)} \\le q_{(2)} \\le \\dots \\le q_{(m)}$. To enforce this property, the final q-value for the $i$-th sorted p-value is calculated as the cumulative minimum of the raw adjusted values from rank $i$ up to rank $m$:\n    $$\n    q_{(i)} = \\min_{k=i}^{m} \\left( \\frac{p_{(k)} \\cdot m}{k} \\right)\n    $$\n    This can be computed efficiently by starting from the largest p-value and iterating backwards:\n    *   $q_{(m)} = \\frac{p_{(m)} \\cdot m}{m} = p_{(m)}$\n    *   For $i = m-1, m-2, \\dots, 1$: $q_{(i)} = \\min\\left(\\frac{p_{(i)} \\cdot m}{i}, q_{(i+1)}\\right)$\n    Conventionally, q-values, like p-values, are capped at $1.0$.\n\n5.  **Remapping**: The computed q-values $\\{q_{(1)}, q_{(2)}, \\dots, q_{(m)}\\}$ correspond to the sorted p-values. These must be re-ordered to match the original input order of the p-values. If $q_j$ is the q-value for the original p-value $p_j$, and $p_j = p_{(i)}$ with original index $idx_{(i)}=j$, then $q_j = q_{(i)}$.\n\n6.  **Significance Testing**: A feature $j$ is declared significant at the FDR level $\\alpha$ if its corresponding q-value $q_j$ is less than $\\alpha$. The final output for each case is the sorted list of $0$-based indices of these significant features.\n\n### Implementation for Test Cases\n\nThis algorithm will be implemented in a reusable function.\n\n*   For **Cases A, B, and C**, this function will be called with the provided p-value lists. The indices of features where $q  0.05$ will be identified and returned as a sorted list.\n*   For **Case D**, the p-values are first generated programmatically. A `numpy.random.default_rng` instance is initialized with the seed $12345$. This generator is used to produce $9000$ null p-values from a $\\text{Uniform}(0, 1)$ distribution and $1000$ alternative p-values from a $\\text{Beta}(0.3, 1.0)$ distribution. The two arrays are concatenated, and the same random number generator is used to shuffle the combined list, ensuring reproducibility. The q-value algorithm is then applied to these $10000$ p-values, and the total count of features with $q  0.05$ is determined.\n\nThe entire process is encapsulated within a Python script adhering to the specified environment and output format.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    alpha = 0.05\n\n    def calculate_q_values(p_values: np.ndarray) - np.ndarray:\n        \"\"\"\n        Computes q-values from a list of p-values using the Benjamini-Hochberg method.\n\n        Args:\n            p_values: A numpy array of p-values.\n\n        Returns:\n            A numpy array of q-values in the same order as the input p_values.\n        \"\"\"\n        m = len(p_values)\n        if m == 0:\n            return np.array([])\n        \n        # Sort p-values and store original indices\n        sorted_indices = np.argsort(p_values)\n        sorted_p_values = p_values[sorted_indices]\n        \n        # Calculate ranks for the sorted p-values (1-based)\n        ranks = np.arange(1, m + 1)\n        \n        # Calculate the raw Benjamini-Hochberg adjusted p-values\n        raw_q_values = sorted_p_values * m / ranks\n        \n        # Enforce monotonicity by taking the cumulative minimum from the end\n        q_values_sorted = np.minimum.accumulate(raw_q_values[::-1])[::-1]\n        \n        # Ensure q-values are capped at 1.0\n        q_values_sorted = np.minimum(q_values_sorted, 1.0)\n        \n        # Reorder q-values to match the original p-value order\n        q_values = np.empty(m)\n        q_values[sorted_indices] = q_values_sorted\n        \n        return q_values\n\n    def get_significant_indices(p_values: list, alpha_level: float) - list:\n        \"\"\"\n        Calculates q-values and returns the sorted indices of significant features.\n        \"\"\"\n        p_values_np = np.array(p_values)\n        q_values = calculate_q_values(p_values_np)\n        significant_indices = np.where(q_values  alpha_level)[0]\n        significant_indices.sort()\n        return significant_indices.tolist()\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'type': 'indices', 'p_values': [0.0005, 0.001, 0.04, 0.2, 0.15, 0.5, 0.8, 0.03, 0.07, 0.06, 0.9, 0.4, 0.002, 0.049, 0.051, 0.12, 0.33, 0.005, 0.0015, 0.75]},\n        # Case B\n        {'type': 'indices', 'p_values': [0.0, 1.0, 1.0, 0.05, 0.05, 0.05, 0.001, 0.999, 0.2, 0.8]},\n        # Case C\n        {'type': 'indices', 'p_values': [0.001, 0.005, 0.01, 0.02, 0.03, 0.04, 0.06, 0.08, 0.1, 0.5, 1.0]},\n    ]\n\n    results = []\n    # Process cases A, B, C\n    for case in test_cases:\n        result = get_significant_indices(case['p_values'], alpha)\n        results.append(result)\n\n    # Process Case D\n    m_d = 10000\n    n_null = 9000\n    n_alt = 1000\n    seed = 12345\n    rng = np.random.default_rng(seed)\n    \n    null_p = rng.uniform(0, 1, size=n_null)\n    alt_p = rng.beta(a=0.3, b=1.0, size=n_alt)\n    \n    p_values_d = np.concatenate((null_p, alt_p))\n    rng.shuffle(p_values_d)\n    \n    q_values_d = calculate_q_values(p_values_d)\n    count_d = np.sum(q_values_d  alpha)\n    results.append(int(count_d))\n\n    # Format and print the final output string exactly as required.\n    result_str_parts = []\n    for res in results[:-1]: # The list results\n        result_str_parts.append(f\"[{','.join(map(str, res))}]\")\n    result_str_parts.append(str(results[-1])) # The integer count\n\n    final_output = f\"[[{','.join(map(str, results[0]))}],[{','.join(map(str, results[1]))}],[{','.join(map(str, results[2]))}],{results[3]}]\"\n    print(final_output)\n\nsolve()\n```", "id": "4994350"}, {"introduction": "A biomarker's journey from discovery to clinical utility is fraught with challenges, one of the most significant being its performance in a real-world population. Even a classifier with high sensitivity and specificity can have limited predictive power if the disease is rare. This practice [@problem_id:4994331] demonstrates this critical principle by asking you to derive and calculate the Positive Predictive Value ($PPV$) of a genomic classifier. This exercise will solidify your understanding of how disease prevalence fundamentally impacts a biomarker's clinical value, a key consideration in translational medicine.", "problem": "A translational medicine team develops an RNA sequencing (RNA-seq) gene expression classifier, built on Next-Generation Sequencing (NGS), intended to detect subclinical inflammatory cardiomyopathy among patients referred to a specialty clinic for unexplained dyspnea. The classifier outputs a binary call based on a learned multi-gene signature. On an independent locked validation set reflecting the intended-use population, the classifier achieves sensitivity $0.90$ and specificity $0.90$. Epidemiologic data for this clinic population indicate that the true disease prevalence is $0.05$.\n\nStarting only from the definitions of sensitivity, specificity, prevalence, and Bayes’ theorem, derive an expression for the positive predictive value (PPV) in terms of sensitivity, specificity, and prevalence, and compute the PPV under the stated operating characteristics and prevalence. You may, if helpful, instantiate a hypothetical screening cohort of size $N = 10{,}000$ to illustrate counts, but your final result must be the PPV as a single number.\n\nExpress your final answer as an exact fraction or a decimal number (proportion). Do not use a percentage sign. No rounding is required.", "solution": "The problem requires the derivation and calculation of the Positive Predictive Value (PPV) of a diagnostic test, given its sensitivity, specificity, and the prevalence of the disease in the target population. I will first derive the general formula for PPV from fundamental principles and then compute its value for the specific parameters provided.\n\nLet us define the following events:\n-   $D$: The event that a patient has the disease (inflammatory cardiomyopathy).\n-   $D^c$: The event that a patient does not have the disease.\n-   $T^+$: The event that the classifier test result is positive.\n-   $T^-$: The event that the classifier test result is negative.\n\nThe problem provides the following parameters, which are defined as conditional probabilities:\n-   Prevalence ($p$): The prior probability of having the disease, $P(D)$.\n-   Sensitivity ($Se$): The probability of a positive test given the patient has the disease, $P(T^+ | D)$.\n-   Specificity ($Sp$): The probability of a negative test given the patient does not have the disease, $P(T^- | D^c)$.\n\nThe values provided are:\n-   $p = P(D) = 0.05$\n-   $Se = P(T^+ | D) = 0.90$\n-   $Sp = P(T^- | D^c) = 0.90$\n\nWe are tasked with finding the Positive Predictive Value (PPV), which is defined as the probability that a patient has the disease given that the test result is positive. In probabilistic terms, this is $P(D | T^+)$.\n\nAccording to Bayes’ theorem, the posterior probability $P(D | T^+)$ can be expressed as:\n$$\nP(D | T^+) = \\frac{P(T^+ | D) P(D)}{P(T^+)}\n$$\n\nThe denominator, $P(T^+)$, is the total probability of obtaining a positive test result. It can be calculated using the law of total probability by marginalizing over the disease status (present, $D$, or not present, $D^c$):\n$$\nP(T^+) = P(T^+ \\cap D) + P(T^+ \\cap D^c)\n$$\nUsing the definition of conditional probability, $P(A \\cap B) = P(A|B)P(B)$, this becomes:\n$$\nP(T^+) = P(T^+ | D) P(D) + P(T^+ | D^c) P(D^c)\n$$\n\nWe can express the terms in this equation using the given parameters:\n-   $P(T^+ | D) = Se$\n-   $P(D) = p$\n-   $P(D^c) = 1 - P(D) = 1 - p$\n-   $P(T^+ | D^c)$ is the probability of a positive test in a non-diseased individual, which is the false positive rate. It is related to specificity by $P(T^+ | D^c) = 1 - P(T^- | D^c) = 1 - Sp$.\n\nSubstituting these into the expression for $P(T^+)$:\n$$\nP(T^+) = (Se)(p) + (1 - Sp)(1 - p)\n$$\n\nNow, substituting this denominator back into the Bayes' theorem expression for PPV, we obtain the general formula for PPV in terms of sensitivity, specificity, and prevalence:\n$$\n\\text{PPV} = P(D | T^+) = \\frac{P(T^+ | D) P(D)}{P(T^+ | D) P(D) + P(T^+ | D^c) P(D^c)} = \\frac{Se \\cdot p}{Se \\cdot p + (1 - Sp) \\cdot (1 - p)}\n$$\nThis is the required derived expression.\n\nTo illustrate these abstract probabilities, we can instantiate a hypothetical cohort of $N = 10,000$ individuals from the specified clinic population.\n-   Number of individuals with the disease: $N \\times p = 10,000 \\times 0.05 = 500$.\n-   Number of individuals without the disease: $N \\times (1 - p) = 10,000 \\times 0.95 = 9,500$.\n\nAmong the $500$ diseased individuals:\n-   True Positives (TP): Number who test positive. This is $500 \\times Se = 500 \\times 0.90 = 450$.\n-   False Negatives (FN): Number who test negative. This is $500 \\times (1 - Se) = 500 \\times 0.10 = 50$.\n\nAmong the $9,500$ non-diseased individuals:\n-   False Positives (FP): Number who test positive. This is $9,500 \\times (1 - Sp) = 9,500 \\times 0.10 = 950$.\n-   True Negatives (TN): Number who test negative. This is $9,500 \\times Sp = 9,500 \\times 0.90 = 8,550$.\n\nThe total number of individuals who test positive is the sum of True Positives and False Positives:\nTotal Positives = $TP + FP = 450 + 950 = 1,400$.\n\nThe PPV is the fraction of individuals who test positive that truly have the disease:\n$$\n\\text{PPV} = \\frac{TP}{TP + FP} = \\frac{450}{1,400}\n$$\n\nNow, we compute the final value using the derived formula and the given parameters: $Se = 0.90$, $Sp = 0.90$, and $p = 0.05$.\n$$\n\\text{PPV} = \\frac{(0.90)(0.05)}{(0.90)(0.05) + (1 - 0.90)(1 - 0.05)}\n$$\n$$\n\\text{PPV} = \\frac{0.045}{0.045 + (0.10)(0.95)}\n$$\n$$\n\\text{PPV} = \\frac{0.045}{0.045 + 0.095}\n$$\n$$\n\\text{PPV} = \\frac{0.045}{0.140}\n$$\nTo express this as an exact fraction, we can write:\n$$\n\\text{PPV} = \\frac{45}{140} = \\frac{5 \\times 9}{5 \\times 28} = \\frac{9}{28}\n$$\nThis result is identical to the one obtained using the cohort counts ($\\frac{450}{1,400} = \\frac{45}{140} = \\frac{9}{28}$), verifying the calculation. The low PPV, despite high sensitivity and specificity, underscores the profound impact of disease prevalence on a test's predictive value in a clinical setting.", "answer": "$$\n\\boxed{\\frac{9}{28}}\n$$", "id": "4994331"}]}