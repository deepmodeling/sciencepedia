{"hands_on_practices": [{"introduction": "A crucial first step in designing any clinical trial is determining the necessary number of participants to reliably answer the research question. This exercise ([@problem_id:5060696]) grounds you in the fundamental statistical principles that connect sample size, expected effect, data variability, and statistical power. By working through the calculation from first principles, you will gain a practical understanding of how trial design parameters are determined and appreciate the critical sensitivity of these plans to the assumptions made at the design stage.", "problem": "A translational Phase IIb randomized controlled trial (RCT) is being designed to evaluate a novel therapy intended to reduce a continuous, mechanistically justified biomarker that has been selected as the primary endpoint based on its biological plausibility and anticipated linkage to clinical benefit. The trial will compare the change from baseline to week $12$ in the biomarker between the investigational therapy and control, with equal allocation and independent observations across participants. The endpoint is measured on a continuous scale, and historical pilot data suggest a standard deviation of $20$ units for change scores. The minimal clinically important difference (MCID) is hypothesized to be $5$ units.\n\nThe study will use a two-sided significance level $\\alpha=0.05$ and target power $0.80$ to detect the planned mean difference. Assume the usual large-sample normal approximation for the two-sample mean comparison holds and that the per-arm sample sizes are equal.\n\nStarting from fundamental definitions of Type I error ($\\alpha$), Type II error ($\\beta$), and the sampling distribution of the difference of two independent sample means under the null and alternative hypotheses, derive the per-arm sample size required for the primary endpoint. Then, explain qualitatively and quantitatively how misspecification of the standard deviation influences the required sample size and the achieved power, explicitly characterizing the dependence of sample size on the standard deviation.\n\nReport the per-arm sample size as the smallest integer greater than or equal to the calculated value. No units are required for the final numerical answer.", "solution": "The problem requires the derivation of the per-arm sample size for a two-sample comparison of means, followed by an analysis of the impact of misspecifying the standard deviation. We shall proceed by first establishing the statistical framework, then deriving the sample size formula from fundamental principles, calculating the required value, and finally discussing the consequences of misspecification.\n\nLet $\\mu_1$ and $\\mu_2$ be the true mean change from baseline in the biomarker for the investigational therapy and control arms, respectively. The trial aims to detect a difference between these two means. The hypotheses for a two-sided test are:\nNull hypothesis, $H_0: \\mu_1 - \\mu_2 = 0$.\nAlternative hypothesis, $H_1: |\\mu_1 - \\mu_2| = \\delta$, where $\\delta$ is the minimal clinically important difference (MCID).\n\nThe givens from the problem statement are:\n- The effect size to detect, $\\delta = 5$.\n- The common standard deviation of the change scores, $\\sigma = 20$.\n- The Type I error rate, $\\alpha = 0.05$ (two-sided).\n- The target power, $1-\\beta = 0.80$, which implies a Type II error rate of $\\beta = 0.20$.\n- The sample sizes per arm are equal, denoted by $n$, i.e., $n_1 = n_2 = n$.\n\nLet $\\bar{X}_1$ and $\\bar{X}_2$ be the sample means of the change scores from the two arms. The test statistic is the difference in sample means, $D = \\bar{X}_1 - \\bar{X}_2$. Assuming independent observations, the variance of $D$ is $\\text{Var}(D) = \\text{Var}(\\bar{X}_1) + \\text{Var}(\\bar{X}_2) = \\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_2}$. Since $n_1=n_2=n$, the variance is $\\frac{2\\sigma^2}{n}$, and the standard error of the difference is $SE(D) = \\sigma\\sqrt{\\frac{2}{n}}$.\n\nBased on the large-sample normal approximation, the sampling distribution of $D$ is:\n- Under $H_0$, $D \\sim \\mathcal{N}(0, \\frac{2\\sigma^2}{n})$.\n- Under $H_1$, $D \\sim \\mathcal{N}(\\delta, \\frac{2\\sigma^2}{n})$ (without loss of generality, we assume $\\mu_1 - \\mu_2 = \\delta > 0$).\n\nThe Type I error, $\\alpha$, is the probability of rejecting $H_0$ when it is true. For a two-sided test at level $\\alpha$, we reject $H_0$ if the absolute value of the standardized test statistic, $|Z|$, exceeds the critical value $z_{1-\\alpha/2}$. The standardized statistic under $H_0$ is $Z = \\frac{D-0}{SE(D)}$.\nThe rejection region is thus defined by $|D| > c$, where the critical value $c$ is given by $c = z_{1-\\alpha/2} \\cdot SE(D) = z_{1-\\alpha/2} \\sigma \\sqrt{\\frac{2}{n}}$.\n\nThe power, $1-\\beta$, is the probability of correctly rejecting $H_0$ when $H_1$ is true. Power is the probability that the observed difference $D$ falls into the rejection region, given that the true difference is $\\delta$.\n$$1-\\beta = P(|D| > c | \\mu_1 - \\mu_2 = \\delta)$$\nThis can be written as $P(D > c | \\mu_1 - \\mu_2 = \\delta) + P(D < -c | \\mu_1 - \\mu_2 = \\delta)$.\nThe second term, $P(D < -c)$, corresponds to rejecting in the opposite direction of the true effect. For a reasonably powered study, this probability is negligible. Thus, we approximate the power by considering only the primary rejection tail:\n$$1-\\beta \\approx P(D > c | \\mu_1 - \\mu_2 = \\delta)$$\nTo evaluate this probability, we standardize $D$ under $H_1$:\n$$1-\\beta \\approx P\\left(\\frac{D-\\delta}{SE(D)} > \\frac{c-\\delta}{SE(D)}\\right)$$\nThe term $\\frac{D-\\delta}{SE(D)}$ is a standard normal variable, let's call it $Z'$. So, $1-\\beta \\approx P(Z' > \\frac{c-\\delta}{SE(D)})$.\nFor this probability to equal $1-\\beta$, its argument must be the $\\beta$-quantile of the standard normal distribution, which is $z_{\\beta} = -z_{1-\\beta}$.\n$$\\frac{c-\\delta}{SE(D)} = -z_{1-\\beta}$$\nSubstituting $c = z_{1-\\alpha/2} \\cdot SE(D)$:\n$$\\frac{z_{1-\\alpha/2} \\cdot SE(D) - \\delta}{SE(D)} = -z_{1-\\beta}$$\n$$z_{1-\\alpha/2} - \\frac{\\delta}{SE(D)} = -z_{1-\\beta}$$\nRearranging the terms gives the fundamental relationship for sample size calculation:\n$$z_{1-\\alpha/2} + z_{1-\\beta} = \\frac{\\delta}{SE(D)} = \\frac{\\delta}{\\sigma\\sqrt{2/n}}$$\nWe now solve for the per-arm sample size, $n$:\n$$\\sqrt{n} = \\frac{\\sigma\\sqrt{2} (z_{1-\\alpha/2} + z_{1-\\beta})}{\\delta}$$\n$$n = \\frac{2\\sigma^2 (z_{1-\\alpha/2} + z_{1-\\beta})^2}{\\delta^2}$$\nThis is the general formula for the per-arm sample size.\n\nNow, we substitute the given values:\n- $\\sigma = 20$\n- $\\delta = 5$\n- For $\\alpha = 0.05$, $z_{1-\\alpha/2} = z_{0.975} \\approx 1.95996$.\n- For $1-\\beta = 0.80$, $z_{1-\\beta} = z_{0.80} \\approx 0.84162$.\n\n$$n = \\frac{2(20)^2 (1.95996 + 0.84162)^2}{(5)^2} = \\frac{2 \\cdot 400 \\cdot (2.80158)^2}{25} = 32 \\cdot (2.80158)^2 \\approx 32 \\cdot 7.84885 \\approx 251.163$$\nThe problem requires the smallest integer sample size greater than or equal to the calculated value.\n$$n = \\lceil 251.163 \\rceil = 252$$\nThus, $252$ participants are required in each arm.\n\nNext, we address the influence of misspecifying the standard deviation, $\\sigma$.\n\n**Qualitative Explanation:**\nThe formula $n = \\frac{2\\sigma^2 (z_{1-\\alpha/2} + z_{1-\\beta})^2}{\\delta^2}$ shows that the required sample size $n$ is directly proportional to the variance $\\sigma^2$ (and thus quadratically dependent on the standard deviation $\\sigma$). All other factors ($\\alpha, \\beta, \\delta$) being equal, a higher population variability requires a larger sample size to achieve the same statistical power for a given effect size.\n- If the standard deviation is **underestimated** (i.e., the value used for calculation, $\\sigma_{assumed}$, is smaller than the true value, $\\sigma_{true}$), the calculated sample size will be smaller than what is truly required. A study conducted with this insufficient sample size will be **underpowered**, meaning its actual power will be less than the target $1-\\beta$. The trial will have a higher than planned risk of failing to detect a true effect (a false negative).\n- If the standard deviation is **overestimated** ($\\sigma_{assumed} > \\sigma_{true}$), the calculated sample size will be larger than necessary. The study will be **overpowered**, achieving a power greater than the target $1-\\beta$. While statistically robust, this leads to an inefficient use of resources and unnecessarily exposes more participants to the investigational therapy than required.\n\n**Quantitative Characterization:**\n\n**Dependence of Sample Size on $\\sigma$:**\nThe relationship is $n \\propto \\sigma^2$. If the standard deviation is misspecified by a factor $k$, such that $\\sigma_{assumed} = k \\cdot \\sigma_{true}$, the estimated sample size $n_{assumed}$ will be related to the truly required sample size $n_{true}$ as follows:\n$$n_{assumed} = \\frac{2(k\\sigma_{true})^2 (\\dots)^2}{\\delta^2} = k^2 \\left( \\frac{2\\sigma_{true}^2 (\\dots)^2}{\\delta^2} \\right) = k^2 n_{true}$$\nThis quadratic relationship means that even small errors in estimating $\\sigma$ can lead to substantial misestimations of the required sample size. For instance, a $20\\%$ underestimation of $\\sigma$ ($k=0.8$) leads to a $36\\%$ underestimation of the required sample size ($n_{assumed}=0.8^2 n_{true} = 0.64 n_{true}$).\n\n**Dependence of Achieved Power on $\\sigma$:**\nSuppose a sample size $n$ is fixed based on an assumed standard deviation $\\sigma_{assumed}$ and a target power $1-\\beta_{target}$. If the true standard deviation is $\\sigma_{true}$, the achieved power, $1-\\beta_{achieved}$, can be calculated. From the fundamental relationship derived earlier:\n$$z_{1-\\alpha/2} + z_{1-\\beta_{achieved}} = \\frac{\\delta}{\\sigma_{true}\\sqrt{2/n}}$$\nWe can substitute the expression for $n$ based on $\\sigma_{assumed}$:\n$$n = \\frac{2\\sigma_{assumed}^2 (z_{1-\\alpha/2} + z_{1-\\beta_{target}})^2}{\\delta^2}$$\nSubstituting this into the power equation:\n$$z_{1-\\alpha/2} + z_{1-\\beta_{achieved}} = \\frac{\\delta}{\\sigma_{true}\\sqrt{2}} \\sqrt{\\frac{2\\sigma_{assumed}^2 (z_{1-\\alpha/2} + z_{1-\\beta_{target}})^2}{\\delta^2}} = \\frac{\\delta}{\\sigma_{true}\\sqrt{2}} \\frac{\\sigma_{assumed}\\sqrt{2}(z_{1-\\alpha/2} + z_{1-\\beta_{target}})}{\\delta}$$\nThis simplifies to:\n$$z_{1-\\alpha/2} + z_{1-\\beta_{achieved}} = \\frac{\\sigma_{assumed}}{\\sigma_{true}} (z_{1-\\alpha/2} + z_{1-\\beta_{target}})$$\nSolving for the z-score of the achieved power:\n$$z_{1-\\beta_{achieved}} = \\left(\\frac{\\sigma_{assumed}}{\\sigma_{true}}\\right) (z_{1-\\alpha/2} + z_{1-\\beta_{target}}) - z_{1-\\alpha/2}$$\nThis equation quantitatively describes how the achieved power depends on the ratio of the assumed to the true standard deviation. If $\\sigma_{assumed} < \\sigma_{true}$, the ratio is less than $1$, leading to $z_{1-\\beta_{achieved}} < z_{1-\\beta_{target}}$, and thus an achieved power lower than the target power. For example, if the true standard deviation were $\\sigma_{true}=25$ instead of the assumed $\\sigma_{assumed}=20$, the achieved power with $n=252$ would be:\n$$z_{1-\\beta_{achieved}} = \\left(\\frac{20}{25}\\right) (1.95996 + 0.84162) - 1.95996 = 0.8 \\cdot (2.80158) - 1.95996 = 2.24126 - 1.95996 = 0.2813$$\nThe achieved power is $1-\\beta_{achieved} = \\Phi(0.2813) \\approx 0.61$, a significant drop from the target of $0.80$. This demonstrates the critical importance of accurate variance estimation in clinical trial design.", "answer": "$$\\boxed{252}$$", "id": "5060696"}, {"introduction": "Choosing how to measure an outcome is as important as the outcome itself, as a statistically significant result is not always a clinically meaningful one. This practice ([@problem_id:5060747]) delves into the strategic choice between analyzing a continuous endpoint as a mean difference versus a dichotomized \"responder\" analysis based on a validated clinically meaningful threshold. It highlights the critical role of patient-centered interpretability in endpoint selection and demonstrates how a single dataset can yield different, complementary insights into a therapy's benefit.", "problem": "A translational medicine team is designing a Phase II randomized study in chronic neuropathic pain to inform the pivotal trial endpoints. The candidate primary endpoint is a continuous change score on a $0$ to $10$ Numeric Rating Scale (NRS) of pain intensity, defined as $Y = \\text{baseline NRS} - \\text{week }12 \\text{ NRS}$ so that larger positive $Y$ indicates improvement. A well-validated Minimal Clinically Important Difference (MCID) for this NRS is a reduction of at least $2$ points, that is, a responder is any participant with $Y \\ge 2$. The team has pilot data suggesting that $Y$ is approximately normally distributed within arms. In the pilot, the investigational treatment arm had mean change $\\mu_T = 1.6$ with standard deviation $\\sigma_T = 1.8$, and the placebo arm had mean change $\\mu_P = 0.9$ with standard deviation $\\sigma_P = 1.6$. Each arm had $n = 200$ participants.\n\nUsing only fundamental definitions and well-tested facts suitable for endpoint selection, including: endpoints represent clinical benefit; patient-centered interpretability relies on linking outcomes to meaningful changes such as the MCID; and normal distribution properties for continuous measures, answer the following. Assume that $Y$ in each arm is well approximated by a normal distribution with the given parameters. Do not assume any unvalidated transformations or arbitrary thresholds other than the MCID. Under these assumptions:\n\n- Estimate the proportion of responders in each arm.\n- Compare the mean difference in change scores to the difference in responder proportions.\n- Determine which endpoint specification most appropriately prioritizes patient-centered interpretability for a potential primary endpoint, and how to position the alternative as a secondary endpoint to complement inference.\n\nWhich option most closely reflects the scientifically justified selection and rationale?\n\nA. Select the responder analysis with threshold $Y \\ge 2$ as the primary endpoint because the threshold is patient-anchored and validated; use the continuous mean change as a key secondary endpoint to quantify magnitude among all patients, including near-threshold improvements. Justify this by estimating normal-theory responder proportions and showing that a clinically interpretable increase in the proportion achieving MCID accompanies the observed mean difference.\n\nB. Select the continuous mean change as the primary endpoint because it is statistically more powerful, and designate responder analysis only as exploratory; patient-centered interpretability can be deferred to labeling discussions irrespective of whether the MCID is validated.\n\nC. Declare both mean change and responder proportion as co-primary endpoints without adjustment for multiplicity, since they capture different constructs and thereby reduce ambiguity about benefit.\n\nD. Replace both endpoints with a composite area-under-the-curve of NRS improvement over $12$ weeks as the primary endpoint, because it integrates time and therefore is always more interpretable to patients than a single time-point responder threshold.\n\nE. Select a responder analysis with threshold $Y \\ge 1$ as the primary endpoint to maximize statistical power, and justify the threshold post hoc by convenience since any positive change is clinically relevant in principle.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of biostatistics and clinical trial design, well-posed with sufficient and consistent data, and objective in its presentation. The question concerns a standard decision-making process in translational medicine, requiring the application of statistical theory to a practical problem of endpoint selection.\n\nThe solution proceeds by first calculating the requested estimates and then evaluating the strategic options for endpoint selection.\n\n**1. Estimation of Responder Proportions**\n\nThe problem states that the change from baseline in pain score, $Y$, is approximately normally distributed in each arm. A responder is defined as a participant with a pain reduction of at least $2$ points, i.e., $Y \\ge 2$. Let $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution $N(0, 1)$. The proportion of responders in a given arm with mean $\\mu$ and standard deviation $\\sigma$ is $P(Y \\ge 2)$. This probability is calculated by standardizing the variable $Y$:\n$$P(Y \\ge 2) = P\\left(\\frac{Y - \\mu}{\\sigma} \\ge \\frac{2 - \\mu}{\\sigma}\\right) = P\\left(Z \\ge \\frac{2 - \\mu}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{2 - \\mu}{\\sigma}\\right)$$\nwhere $Z$ is a standard normal random variable.\n\n**Treatment Arm (T):**\nThe parameters are $\\mu_T = 1.6$ and $\\sigma_T = 1.8$.\nThe standardized score for the responder threshold is:\n$$z_T = \\frac{2 - \\mu_T}{\\sigma_T} = \\frac{2 - 1.6}{1.8} = \\frac{0.4}{1.8} = \\frac{2}{9} \\approx 0.2222$$\nThe proportion of responders in the treatment arm, $P_T$, is:\n$$P_T = P(Y_T \\ge 2) = 1 - \\Phi(0.2222) \\approx 1 - 0.5879 = 0.4121$$\nThus, approximately $41.2\\%$ of participants in the treatment arm are estimated to be responders.\n\n**Placebo Arm (P):**\nThe parameters are $\\mu_P = 0.9$ and $\\sigma_P = 1.6$.\nThe standardized score for the responder threshold is:\n$$z_P = \\frac{2 - \\mu_P}{\\sigma_P} = \\frac{2 - 0.9}{1.6} = \\frac{1.1}{1.6} = \\frac{11}{16} = 0.6875$$\nThe proportion of responders in the placebo arm, $P_P$, is:\n$$P_P = P(Y_P \\ge 2) = 1 - \\Phi(0.6875) \\approx 1 - 0.7541 = 0.2459$$\nThus, approximately $24.6\\%$ of participants in the placebo arm are estimated to be responders.\n\n**2. Comparison of Endpoint Effect Measures**\n\n**Difference in Mean Change Scores:**\nThe difference in the mean change from baseline is:\n$$\\Delta_{\\mu} = \\mu_T - \\mu_P = 1.6 - 0.9 = 0.7$$\nThis indicates that, on average, the treatment reduces pain by $0.7$ NRS points more than placebo. While this is the true mean difference, its clinical significance is not immediately obvious, as $0.7$ is substantially less than the MCID of $2.0$.\n\n**Difference in Responder Proportions:**\nThe difference in the proportion of responders is:\n$$\\Delta_{P} = P_T - P_P \\approx 0.4121 - 0.2459 = 0.1662$$\nThis indicates that the treatment increases the likelihood of achieving a clinically important improvement by approximately $16.6$ percentage points. This is a highly interpretable, patient-centered statement of benefit: for every $100$ patients treated, about $17$ additional patients will experience a meaningful reduction in pain compared to placebo.\n\n**3. Endpoint Selection Rationale**\n\nThe primary goal for a primary endpoint is to provide a clear and persuasive demonstration of a clinically meaningful treatment benefit.\n- The **responder analysis** directly quantifies the proportion of patients who achieve a predefined, validated level of clinically important improvement (the MCID). This provides excellent patient-centered interpretability. Its main drawback is a potential loss of statistical power compared to the continuous analysis, because it dichotomizes a continuous outcome.\n- The **continuous mean change** analysis uses all available data, which typically gives it greater statistical power. However, as seen here, a mean difference ($\\Delta_{\\mu} = 0.7$) that is below the MCID threshold can be difficult to interpret in terms of clinical relevance.\n\nA robust strategy is to select the most interpretable, patient-relevant endpoint as primary, and use the more statistically powerful endpoint as a key secondary to provide complementary evidence. The secondary endpoint can bolster the primary findings by showing a consistent effect and characterizing the benefit across the full spectrum of patient responses. The calculations above show that a modest-appearing mean difference of $0.7$ points corresponds to a substantial and clinically relevant difference of $16.6\\%$ in responder rates.\n\n**Evaluation of Options**\n\n**A. Select the responder analysis with threshold $Y \\ge 2$ as the primary endpoint because the threshold is patient-anchored and validated; use the continuous mean change as a key secondary endpoint to quantify magnitude among all patients, including near-threshold improvements. Justify this by estimating normal-theory responder proportions and showing that a clinically interpretable increase in the proportion achieving MCID accompanies the observed mean difference.**\nThis option correctly identifies the strengths of each endpoint and proposes a scientifically sound and conventional strategy. It prioritizes the patient-centered interpretability of the responder analysis (anchored to the validated MCID of $Y \\ge 2$) for the primary endpoint. It correctly positions the continuous variable as a key secondary endpoint to provide complementary information and statistical support. The justification it proposes is precisely the analysis performed above, linking the mean difference to a meaningful difference in responder proportions.\n**Verdict: Correct.**\n\n**B. Select the continuous mean change as the primary endpoint because it is statistically more powerful, and designate responder analysis only as exploratory; patient-centered interpretability can be deferred to labeling discussions irrespective of whether the MCID is validated.**\nThis option incorrectly deprioritizes clinical interpretability. While the continuous endpoint is often more powerful, a primary endpoint should be clinically meaningful. The problem explicitly states the MCID is \"well-validated,\" making the dismissal of its importance a serious flaw. Deferring interpretability is poor practice and undermines the goal of designing a trial to demonstrate clear clinical benefit.\n**Verdict: Incorrect.**\n\n**C. Declare both mean change and responder proportion as co-primary endpoints without adjustment for multiplicity, since they capture different constructs and thereby reduce ambiguity about benefit.**\nThis option proposes a statistically invalid approach. Using co-primary endpoints requires demonstrating success on both (or a pre-specified one of several), and nearly always mandates a statistical adjustment for multiplicity to control the family-wise Type I error rate. Claiming victory if either one is positive without such an adjustment inflates the probability of a false-positive result and is unacceptable to regulatory agencies.\n**Verdict: Incorrect.**\n\n**D. Replace both endpoints with a composite area-under-the-curve of NRS improvement over $12$ weeks as the primary endpoint, because it integrates time and therefore is always more interpretable to patients than a single time-point responder threshold.**\nThis option makes an unsubstantiated claim. While longitudinal endpoints like AUC can be valuable, it is not true that they are \"always more interpretable to patients.\" A statement like \"your pain was reduced by at least $2$ points\" is often more direct and understandable than an integrated point-weeks value. Furthermore, this approach discards the two given endpoints and the validated MCID, which is a core piece of information.\n**Verdict: Incorrect.**\n\n**E. Select a responder analysis with threshold $Y \\ge 1$ as the primary endpoint to maximize statistical power, and justify the threshold post hoc by convenience since any positive change is clinically relevant in principle.**\nThis option describes scientific misconduct. Changing an endpoint threshold from a validated MCID ($Y \\ge 2$) to an arbitrary one ($Y \\ge 1$) simply to increase statistical power is a form of data-dredging or p-hacking. The justification of \"post hoc by convenience\" is the antithesis of the scientific method, which relies on pre-specification. It discards the validated, patient-centered meaning of the original threshold.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "5060747"}, {"introduction": "Clinical trials often evaluate a therapy's effect on several outcomes beyond the primary one, creating a risk of finding false-positive results due to multiple testing. This exercise ([@problem_id:5060754]) introduces a powerful statistical method, hierarchical gatekeeping, for testing a family of secondary endpoints in a structured sequence. Mastering this technique is essential for making multiple valid claims from a single study while rigorously controlling the overall Type I error rate, a cornerstone of robust scientific and regulatory decision-making.", "problem": "A late-phase translational medicine trial evaluates a novel therapeutic with one prespecified primary endpoint and five prespecified secondary endpoints. The scientific objective is to ensure robust clinical interpretation while maintaining strong control of the Familywise Error Rate (FWER) at the overall Type I error level $ \\alpha = 0.05 $. Let the primary null hypothesis be $ H_{P} $ and the five secondary null hypotheses be $ H_{S1}, H_{S2}, H_{S3}, H_{S4}, H_{S5} $. The five secondary endpoints are prioritized by translational relevance with prespecified weights $ w_{1} = 0.27 $, $ w_{2} = 0.23 $, $ w_{3} = 0.20 $, $ w_{4} = 0.17 $, and $ w_{5} = 0.13 $, satisfying $ \\sum_{i=1}^{5} w_{i} = 1 $.\n\nDesign a hierarchical gatekeeping procedure that first tests $ H_{P} $ and, conditional on rejecting $ H_{P} $, opens the gate to the family of five secondary endpoints. Within the secondary family, allocate the available Type I error using a principled approach grounded in first principles to ensure strong FWER control.\n\nUsing only foundational probability reasoning and well-tested multiplicity control facts, derive and then calculate the adjusted local significance thresholds for $ H_{S1} $ through $ H_{S5} $ so that the overall FWER across the hierarchy is controlled at $ \\alpha = 0.05 $. Express each threshold as an exact decimal. Provide your final numerical thresholds in the order $ (H_{S1}, H_{S2}, H_{S3}, H_{S4}, H_{S5}) $. No rounding is required, and no physical units apply. The final answer must be a single row vector of the five thresholds.", "solution": "The problem as stated is valid. It is scientifically grounded in the established statistical principles of clinical trial design, specifically multiple hypothesis testing and Familywise Error Rate (FWER) control. The problem is well-posed, providing all necessary data and constraints to derive a unique solution. The language is objective and precise.\n\nThe objective is to design a multiple testing procedure for one primary null hypothesis, $H_{P}$, and five secondary null hypotheses, $H_{S1}, H_{S2}, H_{S3}, H_{S4}, H_{S5}$, that strongly controls the FWER at an overall significance level of $\\alpha = 0.05$. The problem specifies a hierarchical gatekeeping structure.\n\nThe chosen procedure is a serial gatekeeping test. In this framework, the total Type I error rate, $\\alpha$, is first allocated entirely to the primary hypothesis, $H_{P}$, which serves as the \"gatekeeper\" for the family of secondary hypotheses.\n\n**Stage 1: Testing the Primary Hypothesis**\nThe primary null hypothesis $H_{P}$ is tested at a local significance level of $\\alpha_{P} = \\alpha = 0.05$.\n- If the p-value for the primary endpoint, $p_{P}$, is greater than or equal to $\\alpha_{P}$ (i.e., $p_{P} \\ge 0.05$), $H_{P}$ is not rejected. The testing procedure terminates, and no claims of statistical significance can be made for any of the secondary endpoints.\n- If $p_{P} < 0.05$, $H_{P}$ is rejected. This successful test \"opens the gate,\" allowing the Type I error rate to be passed to the family of secondary hypotheses. The amount of error passed is the full $\\alpha$ that was allocated to $H_{P}$.\n\n**Stage 2: Testing the Secondary Hypotheses**\nConditional on the rejection of $H_{P}$, the family of five secondary hypotheses can be tested. The full Type I error rate of $\\alpha' = \\alpha = 0.05$ is now available for this family. The problem requires this error rate to be allocated based on the prespecified weights $w_{1} = 0.27$, $w_{2} = 0.23$, $w_{3} = 0.20$, $w_{4} = 0.17$, and $w_{5} = 0.13$.\n\nA principled method that is grounded in first principles and provides strong FWER control is the weighted Bonferroni procedure. In this procedure, the total available alpha, $\\alpha'$, is partitioned among the secondary hypotheses according to their weights. The local significance level, $\\alpha_{Si}$, for testing hypothesis $H_{Si}$ is given by:\n$$ \\alpha_{Si} = w_i \\times \\alpha' $$\nSince $\\alpha' = \\alpha = 0.05$, the adjusted local significance thresholds are:\n$$ \\alpha_{Si} = w_i \\times \\alpha $$\n\n**Derivation of Strong FWER Control**\nWe must demonstrate that this two-stage procedure strongly controls the overall FWER at $\\alpha = 0.05$. Strong control means the FWER is controlled for any configuration of true and false null hypotheses. Let $I_{T}$ be the index set of all true null hypotheses. The FWER is the probability of rejecting at least one hypothesis $H_j$ for $j \\in I_{T}$.\n\nWe analyze the FWER under two mutually exclusive and exhaustive scenarios regarding the truth of the primary hypothesis $H_{P}$.\n\nCase 1: $H_{P}$ is a true null hypothesis.\nIn this case, any rejection of $H_{P}$ is a Type I error. The probability of this is $P(\\text{reject } H_{P} | H_{P} \\text{ is true}) \\le \\alpha_{P} = \\alpha$. Since the secondary hypotheses can only be tested if $H_{P}$ is rejected, the event of any Type I error (either on $H_P$ or any subsequent $H_{Si}$) is a subset of the event of rejecting $H_{P}$. Therefore, the total probability of making at least one Type I error is bounded by the probability of the initial error:\n$$ \\text{FWER} = P(\\text{at least one false rejection} | H_{P} \\text{ is true}) \\le P(\\text{reject } H_{P} | H_{P} \\text{ is true}) \\le \\alpha $$\n\nCase 2: $H_{P}$ is a false null hypothesis.\nIn this case, rejecting $H_{P}$ is a correct decision, not a Type I error. The FWER is then determined exclusively by the potential for Type I errors within the secondary family. Let $I_{S0}$ be the index set of true null hypotheses among the secondary family. Conditional on rejecting the false $H_P$, the FWER is the probability of rejecting at least one $H_{Si}$ for $i \\in I_{S0}$.\n$$ \\text{FWER} = P\\left(\\bigcup_{i \\in I_{S0}} \\{\\text{reject } H_{Si}\\} \\right) $$\nUsing Boole's inequality (the union bound), this probability is bounded by the sum of the individual probabilities:\n$$ \\text{FWER} \\le \\sum_{i \\in I_{S0}} P(\\text{reject } H_{Si}) $$\nFor a true null hypothesis $H_{Si}$, the p-value $p_{Si}$ follows a uniform distribution on $[0, 1]$. The probability of rejecting $H_{Si}$ is the probability that $p_{Si}$ is less than its threshold $\\alpha_{Si}$, so $P(\\text{reject } H_{Si}) = \\alpha_{Si}$.\n$$ \\text{FWER} \\le \\sum_{i \\in I_{S0}} \\alpha_{Si} = \\sum_{i \\in I_{S0}} (w_i \\alpha) = \\alpha \\sum_{i \\in I_{S0}} w_i $$\nSince $I_{S0}$ is a subset of all secondary hypotheses, and the weights are non-negative, we have $\\sum_{i \\in I_{S0}} w_i \\le \\sum_{i=1}^{5} w_i = 1$. Therefore:\n$$ \\text{FWER} \\le \\alpha \\times 1 = \\alpha $$\n\nSince the FWER is controlled at or below $\\alpha$ in all possible scenarios (whether $H_{P}$ is true or false), the proposed hierarchical procedure provides strong control of the FWER at the overall $\\alpha = 0.05$ level.\n\n**Calculation of Significance Thresholds**\nThe adjusted local significance thresholds for the five secondary hypotheses are calculated as follows:\nGiven $\\alpha = 0.05$ and the weights $w_{1} = 0.27$, $w_{2} = 0.23$, $w_{3} = 0.20$, $w_{4} = 0.17$, and $w_{5} = 0.13$.\n\nFor $H_{S1}$:\n$$ \\alpha_{S1} = w_1 \\times \\alpha = 0.27 \\times 0.05 = 0.0135 $$\nFor $H_{S2}$:\n$$ \\alpha_{S2} = w_2 \\times \\alpha = 0.23 \\times 0.05 = 0.0115 $$\nFor $H_{S3}$:\n$$ \\alpha_{S3} = w_3 \\times \\alpha = 0.20 \\times 0.05 = 0.0100 $$\nFor $H_{S4}$:\n$$ \\alpha_{S4} = w_4 \\times \\alpha = 0.17 \\times 0.05 = 0.0085 $$\nFor $H_{S5}$:\n$$ \\alpha_{S5} = w_5 \\times \\alpha = 0.13 \\times 0.05 = 0.0065 $$\n\nThese are the required significance thresholds for each secondary hypothesis, to be used only if the primary hypothesis is rejected at the $\\alpha = 0.05$ level. The final answer is the vector of these five thresholds.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.0135 & 0.0115 & 0.0100 & 0.0085 & 0.0065\n\\end{pmatrix}\n}\n$$", "id": "5060754"}]}