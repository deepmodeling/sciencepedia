{"hands_on_practices": [{"introduction": "The foundation of clinical trial transparency is the timely public disclosure of results. In the United States, this is not merely a scientific ideal but a legal requirement under the FDA Amendments Act (FDAAA). This first exercise provides practice in applying the core FDAAA deadline calculation, a critical skill for any researcher or sponsor responsible for trial compliance. By working through this scenario [@problem_id:4999215], you will learn to precisely determine the latest permissible reporting date based on a trial's Primary Completion Date, reinforcing the strict, calendar-based nature of these regulatory obligations.", "problem": "A sponsor of an Applicable Clinical Trial (ACT) has registered the study on ClinicalTrials.gov and is subject to the results reporting requirements of the Food and Drug Administration Amendments Act of 2007 (FDAAA). The trial’s Primary Completion Date (as defined in the Code of Federal Regulations (CFR) Title 42, Part 11, as the date of final collection for the primary outcome measure) is $2024$-$03$-$15$. The sponsor has not obtained a Certificate of Delay and has not requested or been granted any extension for good cause.\n\nUsing only the following foundational bases:\n- The statutory and regulatory rule that results information for an ACT must be submitted no later than one calendar year after the Primary Completion Date.\n- Standard Gregorian calendar arithmetic and the leap-year rule (years divisible by $4$ are leap years, except those divisible by $100$ unless also divisible by $400$).\n- Electronic registries do not adjust submission deadlines for weekends or federal holidays.\n\nDerive the latest permissible results reporting date under FDAAA for this trial. Explicitly state assumptions about holidays and grace periods in your derivation. Express your final answer as a single integer in the format YYYYMMDD with no separators. No rounding is required. Provide no time-of-day or time zone; only the calendar date is required.", "solution": "The problem will first be validated to ensure it is scientifically grounded, well-posed, objective, and complete.\n\n### Step 1: Extract Givens\nThe following information is provided verbatim in the problem statement:\n- Trial Type: Applicable Clinical Trial (ACT)\n- Governing Regulation: Food and Drug Administration Amendments Act of 2007 (FDAAA)\n- Registration System: ClinicalTrials.gov\n- Primary Completion Date (PCD): $2024$-$03$-$15$\n- Definition of PCD: The date of final collection for the primary outcome measure, as defined in the Code of Federal Regulations (CFR) Title 42, Part 11.\n- Certificate of Delay: Not obtained.\n- Good-cause extension: Not requested or granted.\n- Rule 1: Results information for an ACT must be submitted no later than one calendar year after the Primary Completion Date.\n- Rule 2: Standard Gregorian calendar arithmetic and the leap-year rule (years divisible by $4$ are leap years, except those divisible by $100$ unless also divisible by $400$) must be used.\n- Rule 3: Electronic registries do not adjust submission deadlines for weekends or federal holidays.\n- Required Output Format: A single integer in the format YYYYMMDD.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the required criteria.\n\n- **Scientifically Grounded**: The problem is grounded in the legal and regulatory framework of translational medicine and clinical research, specifically 42 CFR Part 11 and FDAAA. These are established, real-world regulations. The calculation itself is based on standard, universally accepted Gregorian calendar arithmetic. The problem is factually sound and scientifically realistic.\n- **Well-Posed**: The problem provides a clear starting date, a precisely defined time interval (\"one calendar year\"), and explicit rules for calculation. The constraints (no extensions, no adjustments for weekends) eliminate ambiguity, ensuring a unique solution exists.\n- **Objective**: The language is precise and unbiased. All terms are either explicitly defined (PCD) or have standard, unambiguous meanings (Gregorian calendar, leap year).\n\nThe problem does not exhibit any invalidating flaws. It is a well-defined logical problem based on established rules.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A reasoned solution will be provided.\n\n### Derivation of the Submission Deadline\n\nThe task is to determine the latest permissible results reporting date for the specified Applicable Clinical Trial (ACT). The calculation is based on the rules and data provided.\n\n1.  **Identify the Starting Point**: The starting point for the calculation is the Primary Completion Date (PCD), which is given as $2024$-$03$-$15$.\n\n2.  **Apply the Time Interval**: The governing regulation stipulates that results must be submitted no later than one calendar year after the PCD. The term \"one calendar year\" signifies advancing the calendar to the same month and day in the subsequent year.\n    - Starting Year: 2024\n    - Starting Month: 03 (March)\n    - Starting Day: 15\n\n    Adding one calendar year to the PCD of $2024$-$03$-$15$ leads to the date $2025$-$03$-$15$.\n\n3.  **Consider the Leap Year Rule**: The problem requires the use of the standard leap-year rule. A leap year occurs in a year divisible by $4$, except for end-of-century years, which must be divisible by $400$.\n    - The year 2024 is divisible by $4$ and is not divisible by $100$, so it is a leap year. February 2024 contains 29 days.\n    - The PCD, $2024$-$03$-$15$, occurs *after* the leap day of 2024, which was $2024$-$02$-$29$.\n    - The year 2025 is not divisible by $4$, so it is not a leap year.\n    - The one-year interval from $2024$-$03$-$15$ to $2025$-$03$-$15$ does not include a leap day that would cause ambiguity in the end date. For instance, if the PCD had been $2024$-$02$-$29$, the deadline would be $2025$-$02$-$28$, as 2025 has no February 29. However, for a PCD of $2024$-$03$-$15$, the end date is unambiguously $2025$-$03$-$15$.\n\n4.  **Apply Constraints on Delays and Adjustments**:\n    - **Grace Periods**: The problem explicitly states the sponsor has not obtained a Certificate of Delay and has not been granted any extension for good cause. Therefore, no grace period beyond the standard one-year deadline applies.\n    - **Holidays and Weekends**: The problem provides a critical rule: \"Electronic registries do not adjust submission deadlines for weekends or federal holidays.\" The calculated deadline of $2025$-$03$-$15$ falls on a Saturday. According to the stated rule, the deadline is not moved to the next business day. The submission must be completed on or before this date, regardless of the day of the week.\n\n5.  **Determine the Final Date**: Based on the preceding steps, the latest permissible date for the submission of results is March $15$, $2025$.\n\n6.  **Format the Final Answer**: The problem requires the final answer to be a single integer in the format YYYYMMDD.\n    - Year (YYYY): 2025\n    - Month (MM): 03\n    - Day (DD): 15\n\n    Concatenating these values yields the integer $20250315$.", "answer": "$$\\boxed{20250315}$$", "id": "4999215"}, {"introduction": "Beyond simply reporting on time, scientific integrity demands that trial results correspond to the outcomes that were pre-specified before the study's conclusion. The practice of \"outcome switching\"—whereby outcomes are changed, added, or omitted based on the findings—is a critical source of reporting bias. This exercise [@problem_id:4999140] challenges you to move from theory to practice by designing an algorithm to detect potential outcome switching. By operationalizing concepts like text similarity and version control, you will gain hands-on experience in the computational methods used to enforce transparency and hold research to its pre-stated goals.", "problem": "You are to design and implement an algorithmic detector for potential primary outcome switching in clinical trials by comparing versioned registry records to published manuscripts. The detector must use principled, transparent matching criteria for outcome text and time frames, with explicit thresholds. The context is translational medicine, where pre-specification of primary outcomes in trial registries is a cornerstone of data transparency advocated by the International Committee of Medical Journal Editors (ICMJE) and the Consolidated Standards of Reporting Trials (CONSORT).\n\nFundamental base:\n- Clinical trial registries (e.g., ClinicalTrials.gov) store versioned records that include pre-specified primary outcomes and timestamps. A published manuscript reports one or more primary outcomes and a publication date. A primary completion date delineates when the primary outcome data collection is scheduled to finish.\n- Potential outcome switching is suspected when the published primary outcome does not match the pre-specified primary outcome recorded before the primary completion date, especially if registry changes after this date cause a match.\n- Matching outcomes requires comparing outcome texts and time frames. For text, we use set-theoretic similarity; for time frames, we use bounded differences.\n\nDefinitions and criteria:\n1. Let a registry have versions indexed by $i$, each with a timestamp $t_i$ (in days), and a set $R_i$ of primary outcomes. Each registry outcome $r \\in R_i$ has fields $(\\text{text}_r, \\text{time}_r)$, where $\\text{time}_r$ is in days.\n2. Let the published manuscript have publication date $T_{\\mathrm{pub}}$ (in days) and a set $P$ of reported primary outcomes. Each published outcome $p \\in P$ has fields $(\\text{text}_p, \\text{time}_p)$, where $\\text{time}_p$ is in days.\n3. Let $T_{\\mathrm{ref}}$ (in days) denote the primary completion date, which serves as the reference cutoff for pre-specification.\n4. Text normalization: For any outcome text string $s$, define a normalization function that:\n   - Converts to lowercase.\n   - Applies synonym expansion on token boundaries for the following mappings: \"bp\" $\\rightarrow$ \"blood pressure\", \"sbp\" $\\rightarrow$ \"systolic blood pressure\", \"dbp\" $\\rightarrow$ \"diastolic blood pressure\", \"vas\" $\\rightarrow$ \"visual analogue scale\", \"ldl\" $\\rightarrow$ \"low density lipoprotein\", \"hdl\" $\\rightarrow$ \"high density lipoprotein\", \"death\" $\\rightarrow$ \"mortality\".\n   - Removes punctuation.\n   - Splits into tokens by whitespace.\n   - Removes stopwords from the set $\\{\\text{the}, \\text{of}, \\text{and}, \\text{to}, \\text{in}, \\text{for}, \\text{with}, \\text{by}, \\text{on}, \\text{at}, \\text{from}, \\text{a}, \\text{an}, \\text{or}, \\text{as}\\}$.\n   - Applies simple stemming: remove trailing \"ing\", \"ed\", or \"s\" from tokens of length greater than $3$ where applicable.\n   The result is a set of normalized tokens $T(s)$.\n5. Text similarity: For two texts $s_1$ and $s_2$, define the Jaccard similarity\n   $$ J(s_1, s_2) = \\frac{|T(s_1) \\cap T(s_2)|}{|T(s_1) \\cup T(s_2)|} $$\n   with the convention that if the union is empty, $J = 1$.\n6. Time-frame match: Given outcome times in days $\\text{time}_p$ and $\\text{time}_r$, define a match if\n   $$ |\\text{time}_p - \\text{time}_r| \\le \\tau, $$\n   where $\\tau$ is a nonnegative tolerance parameter (in days).\n7. Outcome-level match: A published outcome $p$ matches a registry outcome $r$ if both $J(\\text{text}_p, \\text{text}_r) \\ge \\alpha$ and $|\\text{time}_p - \\text{time}_r| \\le \\tau$, where $\\alpha \\in [0,1]$ is a text similarity threshold and $\\tau \\ge 0$ is a time-frame tolerance in days.\n8. Version-level match: For a set of published outcomes $P$ and a registry version $R_i$, define that $P$ matches $R_i$ if for every $p \\in P$ there exists some $r \\in R_i$ such that $p$ matches $r$ by the criterion above.\n9. Detection decision rule:\n   - Let $i^{\\mathrm{ref}}$ be the largest index $i$ such that $t_i \\le T_{\\mathrm{ref}}$ (the last registry version not later than the primary completion date). If no such index exists, treat as no pre-reference version.\n   - Let $i^{\\mathrm{pub}}$ be the largest index $i$ such that $t_i \\le T_{\\mathrm{pub}}$ (the last registry version not later than publication).\n   - Potential outcome switching is flagged as true if either:\n     (a) $P$ does not match $R_{i^{\\mathrm{ref}}}$ (or no $i^{\\mathrm{ref}}$ exists) and $P$ matches $R_{i^{\\mathrm{pub}}}$, or\n     (b) $P$ does not match $R_{i^{\\mathrm{ref}}}$ and $P$ does not match $R_{i^{\\mathrm{pub}}}$.\n   Otherwise, it is flagged as false.\nAll time quantities must be handled in units of days.\n\nProgram requirements:\n- Implement the detector strictly according to the definitions above.\n- Use the specified text normalization, Jaccard similarity, and time-frame match.\n- Use the detection decision rule to produce a boolean output per test case.\n\nTest suite:\nProvide five test cases as follows. Each case consists of registry versions (each with a timestamp and a list of primary outcomes, each with text and time in days), a published set of primary outcomes (each with text and time in days), the primary completion date $T_{\\mathrm{ref}}$, the publication date $T_{\\mathrm{pub}}$, and thresholds $(\\alpha, \\tau)$.\n\n- Case $1$ (happy path):\n  - Registry versions:\n    - $(t_v = 0, \\text{\"Change in systolic blood pressure at 12 weeks\"}, 84)$\n    - $(t_v = 50, \\text{\"Change in systolic blood pressure at twelve weeks\"}, 84)$\n  - $T_{\\mathrm{ref}} = 100$, $T_{\\mathrm{pub}} = 180$\n  - Published outcomes: $[(\\text{\"Change in systolic blood pressure at 12 weeks\"}, 84)]$\n  - Thresholds: $\\alpha = 0.6$, $\\tau = 30$.\n- Case $2$ (clear switch after primary completion):\n  - Registry versions:\n    - $(t_v = 0, \\text{\"HbA1c at 24 weeks\"}, 168)$\n    - $(t_v = 190, \\text{\"Fasting plasma glucose at 12 weeks\"}, 84)$\n  - $T_{\\mathrm{ref}} = 170$, $T_{\\mathrm{pub}} = 200$\n  - Published outcomes: $[(\\text{\"Fasting plasma glucose at 12 weeks\"}, 84)]$\n  - Thresholds: $\\alpha = 0.6$, $\\tau = 30$.\n- Case $3$ (boundary, time-frame within tolerance, minor text variation):\n  - Registry versions:\n    - $(t_v = 0, \\text{\"Change in LDL cholesterol at 3 months\"}, 90)$\n    - $(t_v = 70, \\text{\"Change in LDL cholesterol at 95 days\"}, 95)$\n  - $T_{\\mathrm{ref}} = 80$, $T_{\\mathrm{pub}} = 160$\n  - Published outcomes: $[(\\text{\"Change in LDL cholesterol at 3 months\"}, 92)]$\n  - Thresholds: $\\alpha = 0.5$, $\\tau = 10$.\n- Case $4$ (edge, published selects different construct):\n  - Registry versions:\n    - $(t_v = 0, \\text{\"Pain score on VAS at 4 weeks\"}, 28)$\n    - $(t_v = 30, \\text{\"Pain score on VAS at 4 weeks\"}, 28)$\n  - $T_{\\mathrm{ref}} = 60$, $T_{\\mathrm{pub}} = 120$\n  - Published outcomes: $[(\\text{\"All-cause mortality at 4 weeks\"}, 28)]$\n  - Thresholds: $\\alpha = 0.6$, $\\tau = 30$.\n- Case $5$ (edge, synonym handling and exact time tolerance boundary):\n  - Registry versions:\n    - $(t_v = 0, \\text{\"All-cause mortality at 30 days\"}, 30)$\n    - $(t_v = 10, \\text{\"All-cause death at 30 days\"}, 30)$\n  - $T_{\\mathrm{ref}} = 25$, $T_{\\mathrm{pub}} = 40$\n  - Published outcomes: $[(\\text{\"All-cause mortality at 31 days\"}, 31)]$\n  - Thresholds: $\\alpha = 0.6$, $\\tau = 1$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the five test cases as a comma-separated list enclosed in square brackets (e.g., $[\\text{result1},\\text{result2},\\text{result3},\\text{result4},\\text{result5}]$), where each result is a boolean value produced by the detector for the corresponding case.", "solution": "The problem requires the design and implementation of an algorithm to detect potential primary outcome switching in clinical trials. This is achieved by comparing outcome descriptions from versioned trial registry records against those from a final published manuscript. The algorithm must adhere to a set of precisely defined criteria for text similarity, time-frame concordance, and logical decision-making. The solution is predicated on the principle that primary outcomes must be pre-specified before the trial's primary completion date, $T_{\\mathrm{ref}}$, to ensure transparency and prevent reporting bias.\n\nThe algorithmic solution is structured into four main components: (1) a text normalization and similarity module, (2) an outcome-level and version-level matching engine, (3) a temporal analysis component to identify relevant registry versions, and (4) a final decision rule to flag potential switching.\n\nFirst, we must define the process for comparing outcome texts. A robust comparison requires a normalization function, $T(s)$, that transforms a raw text string $s$ into a canonical set of tokens. This function is implemented through a strict, ordered sequence of operations:\n1.  Conversion to lowercase to ensure case-insensitivity.\n2.  Synonym expansion based on a provided dictionary (e.g., \"bp\" $\\rightarrow$ \"blood pressure\"). This is performed on the full string, respecting token boundaries to ensure, for example, that the token `sbp` is expanded while a string like `sbp` within another word is not.\n3.  Removal of all punctuation characters.\n4.  Tokenization of the cleaned string by splitting on whitespace.\n5.  Filtering of common stopwords (e.g., `the`, `of`, `a`) to remove noise and focus on meaningful terms.\n6.  Application of a simple stemming algorithm to reduce tokens to their root form. Specifically, for tokens with length greater than $3$, trailing `s`, `ed`, or `ing` suffixes are removed.\nThe result of this process is a set of normalized tokens, $T(s)$.\n\nWith the normalization function in place, the similarity between two outcome texts, $s_1$ and $s_2$, is quantified using the Jaccard similarity index, $J$. This metric is defined as the ratio of the size of the intersection to the size of the union of their normalized token sets:\n$$ J(s_1, s_2) = \\frac{|T(s_1) \\cap T(s_2)|}{|T(s_1) \\cup T(s_2)|} $$\nBy convention, if the union is empty (i.e., both texts normalize to empty sets), $J(s_1, s_2) = 1$.\n\nSecond, we establish the criteria for matching. A single published outcome $p$, characterized by its text $\\text{text}_p$ and time point $\\text{time}_p$, is considered to match a single registry outcome $r$ (with $\\text{text}_r$ and $\\text{time}_r$) if and only if two conditions are met simultaneously:\n1.  The text similarity exceeds a given threshold $\\alpha$: $J(\\text{text}_p, \\text{text}_r) \\ge \\alpha$.\n2.  The absolute difference between their time points is within a given tolerance $\\tau$: $|\\text{time}_p - \\text{time}_r| \\le \\tau$. Here, $\\alpha \\in [0, 1]$ and $\\tau \\ge 0$.\n\nThis outcome-level match is extended to a version-level match. A set of published outcomes $P$ matches a set of registry outcomes from a specific version $R_i$ if, for every published outcome $p \\in P$, there exists at least one registry outcome $r \\in R_i$ that satisfies the outcome-level match criteria. This is a universal quantification over the set $P$.\n\nThird, we perform the temporal analysis. The algorithm must identify the critical registry version that represents the state of the trial's pre-specification. This is the last version recorded on or before the primary completion date, $T_{\\mathrm{ref}}$. We denote the index of this version as $i^{\\mathrm{ref}}$, defined as the largest index $i$ such that the version's timestamp $t_i \\le T_{\\mathrm{ref}}$. The set of outcomes from this version is $R_{i^{\\mathrm{ref}}}$. If no such version exists, it is considered a failure to pre-specify.\n\nFourth, we apply the final decision rule. The core of the detection logic rests on comparing the published outcomes $P$ against the pre-specified outcomes $R_{i^{\\mathrm{ref}}}$. The problem statement defines two conditions for flagging potential outcome switching:\n(a) The published set $P$ does not match the reference version $R_{i^{\\mathrm{ref}}}$ (or no $i^{\\mathrm{ref}}$ exists), but it does match the last registry version before publication, $R_{i^{\\mathrm{pub}}}$ (where $i^{\\mathrm{pub}}$ is the largest index $i$ with $t_i \\le T_{\\mathrm{pub}}$). This represents a scenario where the registry was amended post-hoc to align with the published results.\n(b) The published set $P$ does not match the reference version $R_{i^{\\mathrm{ref}}}$ and also does not match the publication-time version $R_{i^{\\mathrm{pub}}}$. This represents a discrepancy where the published outcomes match neither the pre-specified record nor any subsequent official record.\n\nLet `ref_match` be the boolean result of checking if $P$ matches $R_{i^{\\mathrm{ref}}}$. If no $i^{\\mathrm{ref}}$ exists, `ref_match` is false. The logical disjunction of conditions (a) and (b) is `(not ref_match AND pub_match) OR (not ref_match AND not pub_match)`. By the distributive law of boolean algebra, this simplifies to `not ref_match AND (pub_match OR not pub_match)`. Since `pub_match OR not pub_match` is a tautology (always true), the entire decision rule simplifies to `not ref_match`.\n\nTherefore, the detector flags potential outcome switching as `True` if the set of published outcomes $P$ does not have a version-level match with the reference registry version $R_{i^{\\mathrm{ref}}}$. Otherwise, it is flagged as `False`. This elegantly captures the fundamental principle: if the published primary outcomes do not correspond to what was declared before the primary completion date, a potential reporting issue exists. The implementation will proceed by constructing functions for each of these logical steps and applying them to the provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport string\nimport re\n\ndef solve():\n    \"\"\"\n    Main function to run the outcome switching detector on all test cases.\n    \"\"\"\n    \n    # Define synonyms and stopwords as per the problem statement.\n    SYNONYMS = {\n        \"bp\": \"blood pressure\", \"sbp\": \"systolic blood pressure\",\n        \"dbp\": \"diastolic blood pressure\", \"vas\": \"visual analogue scale\",\n        \"ldl\": \"low density lipoprotein\", \"hdl\": \"high density lipoprotein\",\n        \"death\": \"mortality\"\n    }\n    STOPWORDS = {\n        \"the\", \"of\", \"and\", \"to\", \"in\", \"for\", \"with\", \"by\", \"on\", \"at\",\n        \"from\", \"a\", \"an\", \"or\", \"as\"\n    }\n    PUNCTUATION_TABLE = str.maketrans('', '', string.punctuation)\n\n    def normalize_text(text):\n        \"\"\"\n        Normalizes a text string according to the specified rules.\n        \"\"\"\n        # 1. Convert to lowercase\n        normalized_text = text.lower()\n        \n        # 2. Apply synonym expansion on token boundaries\n        for abbr, expansion in SYNONYMS.items():\n            normalized_text = re.sub(r'\\b' + re.escape(abbr) + r'\\b', expansion, normalized_text)\n            \n        # 3. Remove punctuation\n        normalized_text = normalized_text.translate(PUNCTUATION_TABLE)\n        \n        # 4. Split into tokens by whitespace\n        tokens = normalized_text.split()\n        \n        # 5. Remove stopwords\n        tokens = [token for token in tokens if token not in STOPWORDS]\n        \n        # 6. Apply simple stemming\n        stemmed_tokens = []\n        for token in tokens:\n            if len(token) > 3:\n                if token.endswith('ing'):\n                    stemmed_tokens.append(token[:-3])\n                elif token.endswith('ed'):\n                    stemmed_tokens.append(token[:-2])\n                elif token.endswith('s'):\n                    stemmed_tokens.append(token[:-1])\n                else:\n                    stemmed_tokens.append(token)\n            else:\n                stemmed_tokens.append(token)\n                \n        # 7. Return a set of normalized tokens\n        return set(stemmed_tokens)\n\n    def jaccard_similarity(text1, text2):\n        \"\"\"\n        Calculates the Jaccard similarity between two text strings.\n        \"\"\"\n        tokens1 = normalize_text(text1)\n        tokens2 = normalize_text(text2)\n        \n        intersection_size = len(tokens1.intersection(tokens2))\n        union_size = len(tokens1.union(tokens2))\n        \n        if union_size == 0:\n            return 1.0\n        \n        return intersection_size / union_size\n\n    def check_outcome_match(published_outcome, registry_outcome, alpha, tau):\n        \"\"\"\n        Checks if a published outcome matches a registry outcome.\n        \"\"\"\n        text_p, time_p = published_outcome\n        text_r, time_r = registry_outcome\n        \n        # Text similarity check\n        j_sim = jaccard_similarity(text_p, text_r)\n        text_match = j_sim >= alpha\n        \n        # Time-frame match check\n        time_match = abs(time_p - time_r) = tau\n        \n        return text_match and time_match\n\n    def check_version_match(published_outcomes, registry_outcomes, alpha, tau):\n        \"\"\"\n        Checks if a set of published outcomes matches a registry version.\n        \"\"\"\n        if not published_outcomes:\n            return True # Vacuously true if there are no published outcomes to check\n        if not registry_outcomes:\n            return False # Cannot match if registry is empty\n\n        for p_outcome in published_outcomes:\n            found_match_for_p = False\n            for r_outcome in registry_outcomes:\n                if check_outcome_match(p_outcome, r_outcome, alpha, tau):\n                    found_match_for_p = True\n                    break # Found a match, move to next published outcome\n            if not found_match_for_p:\n                return False # One published outcome had no match, so version match fails\n        \n        return True # All published outcomes found a match\n\n    def detect_switching(registry_versions, published_outcomes, T_ref, T_pub, alpha, tau):\n        \"\"\"\n        Implements the main detection decision rule.\n        \"\"\"\n        # Find the reference registry version, R_ref\n        ref_versions = [v for v in registry_versions if v[0] = T_ref]\n        R_ref = []\n        if ref_versions:\n            latest_ref_version = max(ref_versions, key=lambda v: v[0])\n            # Handle cases where a version has multiple outcomes\n            R_ref = [outcome for outcome in latest_ref_version[1:]]\n\n        # Calculate ref_match\n        ref_match = check_version_match(published_outcomes, R_ref, alpha, tau)\n\n        # The decision rule simplifies to `not ref_match`\n        return not ref_match\n    \n    # Test suite from the problem statement.\n    test_cases = [\n        {\n            \"registry_versions\": [\n                (0, (\"Change in systolic blood pressure at 12 weeks\", 84)),\n                (50, (\"Change in systolic blood pressure at twelve weeks\", 84))\n            ],\n            \"published_outcomes\": [(\"Change in systolic blood pressure at 12 weeks\", 84)],\n            \"T_ref\": 100, \"T_pub\": 180, \"alpha\": 0.6, \"tau\": 30\n        },\n        {\n            \"registry_versions\": [\n                (0, (\"HbA1c at 24 weeks\", 168)),\n                (190, (\"Fasting plasma glucose at 12 weeks\", 84))\n            ],\n            \"published_outcomes\": [(\"Fasting plasma glucose at 12 weeks\", 84)],\n            \"T_ref\": 170, \"T_pub\": 200, \"alpha\": 0.6, \"tau\": 30\n        },\n        {\n            \"registry_versions\": [\n                (0, (\"Change in LDL cholesterol at 3 months\", 90)),\n                (70, (\"Change in LDL cholesterol at 95 days\", 95))\n            ],\n            \"published_outcomes\": [(\"Change in LDL cholesterol at 3 months\", 92)],\n            \"T_ref\": 80, \"T_pub\": 160, \"alpha\": 0.5, \"tau\": 10\n        },\n        {\n            \"registry_versions\": [\n                (0, (\"Pain score on VAS at 4 weeks\", 28)),\n                (30, (\"Pain score on VAS at 4 weeks\", 28))\n            ],\n            \"published_outcomes\": [(\"All-cause mortality at 4 weeks\", 28)],\n            \"T_ref\": 60, \"T_pub\": 120, \"alpha\": 0.6, \"tau\": 30\n        },\n        {\n            \"registry_versions\": [\n                (0, (\"All-cause mortality at 30 days\", 30)),\n                (10, (\"All-cause death at 30 days\", 30))\n            ],\n            \"published_outcomes\": [(\"All-cause mortality at 31 days\", 31)],\n            \"T_ref\": 25, \"T_pub\": 40, \"alpha\": 0.6, \"tau\": 1\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = detect_switching(\n            registry_versions=case[\"registry_versions\"],\n            published_outcomes=case[\"published_outcomes\"],\n            T_ref=case[\"T_ref\"],\n            T_pub=case[\"T_pub\"],\n            alpha=case[\"alpha\"],\n            tau=case[\"tau\"]\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results)).lower()}]\")\n\nsolve()\n```", "id": "4999140"}, {"introduction": "The ultimate form of transparency is the sharing of individual participant data (IPD), which allows for full reproducibility and novel secondary research. However, this practice carries a profound ethical responsibility to protect participant privacy. This final exercise [@problem_id:4999116] provides a hands-on introduction to the formal privacy models used to de-identify clinical data. By applying the concepts of $k$-anonymity, $l$-diversity, and $t$-closeness to a dataset, you will learn to quantify privacy risks and understand why simple anonymization is often insufficient to prevent attribute disclosure.", "problem": "A sponsor intends to share individual participant data (IPD) from a rare-disease phase II trial through a controlled-access repository linked from ClinicalTrials.gov. To balance transparency and participant privacy, the sponsor applies generalization to quasi-identifiers and plans to assess privacy using three models commonly cited in health data sharing: $k$-anonymity, $l$-diversity, and $t$-closeness. The quasi-identifiers are age group, sex, and site, and the sensitive attribute is occurrence of a Treatment-Emergent Serious Adverse Event (TESAE; Yes/No). After generalization, the $n$ released records fall into the following equivalence classes (defined by identical quasi-identifiers):\n\n- Equivalence Class $1$: size $4$; TESAE distribution: Yes $1$, No $3$.\n- Equivalence Class $2$: size $3$; TESAE distribution: Yes $3$, No $0$.\n- Equivalence Class $3$: size $5$; TESAE distribution: Yes $1$, No $4$.\n\nAssume that $k$-anonymity requires each equivalence class to contain at least $k$ records; $l$-diversity requires each equivalence class to contain at least $l$ distinct sensitive values that are well represented; and $t$-closeness requires the distribution of the sensitive attribute within each equivalence class to be within distance $t$ of the global distribution over the entire released dataset. For this problem, take the global TESAE distribution to be computed from the above classes, and take the distance between class-level and global distributions to be measured by the total variation distance $d_{\\mathrm{TV}}(P,Q) = \\frac{1}{2}\\sum_i \\lvert p_i - q_i \\rvert$. Consider thresholds $k=3$, $l=2$, and $t=0.2$.\n\nSelect all options that are correct in defining these models in the context of shared clinical trial data and in analyzing the above dataset’s compliance and implications for participant privacy protection.\n\nA. The dataset satisfies $k$-anonymity for $k=3$, satisfies $l$-diversity for $l=2$, and fails $t$-closeness for $t=0.2$; thus, failure of $t$-closeness here reflects only a technicality and does not imply any increased risk of attribute disclosure.\n\nB. The dataset satisfies $k$-anonymity for $k=3$, fails $l$-diversity for $l=2$, and fails $t$-closeness for $t=0.2$. This demonstrates that protecting against identity disclosure via $k$-anonymity does not by itself prevent attribute disclosure in shared clinical trial IPD.\n\nC. The dataset violates $k$-anonymity for $k=3$ because one class has all TESAE values identical, but it satisfies $l$-diversity for $l=2$ since the global TESAE distribution has two categories (Yes/No) overall.\n\nD. In clinical trial registries such as ClinicalTrials.gov, baseline characteristic tables inherently satisfy $k$-anonymity with $k$ equal to the arm’s sample size, so the $k$-anonymity criterion is automatically met for any posted result.\n\nE. In the context of sharing clinical trial IPD, $k$-anonymity means each set of records sharing the same quasi-identifiers has at least $k$ members, mitigating record linkage; $l$-diversity requires each such set to contain at least $l$ well-represented distinct sensitive values, mitigating homogeneity and background knowledge attacks; and $t$-closeness constrains the sensitive-attribute distribution in each set to be within distance $t$ of the global distribution, mitigating attribute disclosure in skewed data. Together, these models reduce identity and attribute disclosure risk while enabling reproducibility and secondary analyses when IPD access is linked from ClinicalTrials.gov.", "solution": "The problem statement is evaluated for validity prior to proceeding with a solution.\n\n**Step 1: Extract Givens**\n-   Data type: Individual participant data (IPD) from a rare-disease phase II trial.\n-   Data sharing mechanism: Controlled-access repository linked from ClinicalTrials.gov.\n-   Privacy enhancement technique: Generalization of quasi-identifiers.\n-   Quasi-identifiers (QIs): age group, sex, site.\n-   Sensitive attribute (SA): Treatment-Emergent Serious Adverse Event (TESAE), with values {Yes, No}.\n-   Equivalence Classes (ECs) post-generalization and their properties:\n    -   EC $1$: size $n_1=4$; TESAE counts: Yes($1$), No($3$).\n    -   EC $2$: size $n_2=3$; TESAE counts: Yes($3$), No($0$).\n    -   EC $3$: size $n_3=5$; TESAE counts: Yes($1$), No($4$).\n-   Privacy model definitions and thresholds:\n    -   $k$-anonymity: Each EC must contain at least $k$ records. Test with $k=3$.\n    -   $l$-diversity: Each EC must contain at least $l$ distinct sensitive values. Test with $l=2$.\n    -   $t$-closeness: The total variation distance between the SA distribution in each EC and the global SA distribution must not exceed $t$. Test with $t=0.2$.\n    -   Distance metric: Total Variation Distance, $d_{\\mathrm{TV}}(P, Q) = \\frac{1}{2}\\sum_i |p_i - q_i|$.\n    -   Global distribution is to be computed from the provided ECs.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is firmly grounded in the established field of statistical disclosure control and data privacy. $k$-anonymity, $l$-diversity, and $t$-closeness are canonical models. Their application to clinical trial IPD is a current and critical topic in medical informatics and translational medicine.\n-   **Well-Posed:** The problem provides all necessary data (EC sizes and distributions), clear definitions for the privacy models, and specific thresholds ($k=3$, $l=2$, $t=0.2$) for evaluation. The question is unambiguous.\n-   **Objective:** The problem is stated using precise, objective language and quantitative definitions. There are no subjective or opinion-based elements in the problem setup.\n-   **Conclusion:** The problem statement is valid. It is scientifically sound, well-posed, objective, and contains no internal contradictions or missing information.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution will now be derived.\n\n**Derivation and Analysis**\n\nFirst, we must compute the global distribution of the sensitive attribute (TESAE) over the entire released dataset.\nThe total number of participants is $n = n_1 + n_2 + n_3 = 4 + 3 + 5 = 12$.\nThe total number of participants with TESAE='Yes' is $1 + 3 + 1 = 5$.\nThe total number of participants with TESAE='No' is $3 + 0 + 4 = 7$.\nThe global distribution $Q$ for the sensitive attribute TESAE is:\n-   $q_{\\text{Yes}} = \\frac{5}{12}$\n-   $q_{\\text{No}} = \\frac{7}{12}$\n\nNow, we evaluate the dataset against each privacy criterion.\n\n**1. $k$-anonymity for $k=3$**\nThe criterion is that the size of each equivalence class must be at least $k$.\n-   EC $1$: size is $4$. $4 \\geq 3$. This class satisfies the condition.\n-   EC $2$: size is $3$. $3 \\geq 3$. This class satisfies the condition.\n-   EC $3$: size is $5$. $5 \\geq 3$. This class satisfies the condition.\nSince all equivalence classes have a size of at least $3$, the dataset satisfies $3$-anonymity.\n\n**2. $l$-diversity for $l=2$**\nThe criterion is that each equivalence class must have at least $l$ distinct sensitive attribute values. The sensitive attribute TESAE has two distinct values: {Yes, No}. Therefore, for $l=2$, each class must contain at least one record with 'Yes' and at least one record with 'No'.\n-   EC $1$: Contains 'Yes' ($1$ record) and 'No' ($3$ records). It has $2$ distinct values. This class satisfies the condition.\n-   EC $2$: Contains 'Yes' ($3$ records) and 'No' ($0$ records). It has only $1$ distinct value. This class **fails** the condition.\n-   EC $3$: Contains 'Yes' ($1$ record) and 'No' ($4$ records). It has $2$ distinct values. This class satisfies the condition.\nSince EC $2$ fails the criterion, the entire dataset **fails** to satisfy $2$-diversity. This is a homogeneity attack vulnerability: anyone identifiable as belonging to EC $2$ is known to have had a TESAE with $100\\%$ certainty.\n\n**3. $t$-closeness for $t=0.2$**\nThe criterion is that for each equivalence class, the total variation distance $d_{\\mathrm{TV}}$ between its local sensitive attribute distribution $P$ and the global distribution $Q$ must be no more than $t$.\n$d_{\\mathrm{TV}}(P,Q) = \\frac{1}{2}(|p_{\\text{Yes}} - q_{\\text{Yes}}| + |p_{\\text{No}} - q_{\\text{No}}|)$.\nSince $p_{\\text{No}} = 1 - p_{\\text{Yes}}$ and $q_{\\text{No}} = 1 - q_{\\text{Yes}}$, the distance simplifies to $d_{\\mathrm{TV}}(P,Q) = |p_{\\text{Yes}} - q_{\\text{Yes}}|$. We have $q_{\\text{Yes}} = \\frac{5}{12}$. The threshold is $t=0.2$.\n\n-   EC $1$: $p_{1, \\text{Yes}} = \\frac{1}{4}$.\n    $d_{\\mathrm{TV}}(P_1, Q) = |\\frac{1}{4} - \\frac{5}{12}| = |\\frac{3}{12} - \\frac{5}{12}| = |-\\frac{2}{12}| = \\frac{1}{6} \\approx 0.167$.\n    Since $0.167 \\leq 0.2$, this class satisfies the condition.\n\n-   EC $2$: $p_{2, \\text{Yes}} = \\frac{3}{3} = 1$.\n    $d_{\\mathrm{TV}}(P_2, Q) = |1 - \\frac{5}{12}| = |\\frac{12}{12} - \\frac{5}{12}| = \\frac{7}{12} \\approx 0.583$.\n    Since $0.583 > 0.2$, this class **fails** the condition.\n\n-   EC $3$: $p_{3, \\text{Yes}} = \\frac{1}{5}$.\n    $d_{\\mathrm{TV}}(P_3, Q) = |\\frac{1}{5} - \\frac{5}{12}| = |\\frac{12}{60} - \\frac{25}{60}| = |-\\frac{13}{60}| = \\frac{13}{60} \\approx 0.217$.\n    Since $0.217 > 0.2$, this class **fails** the condition.\n\nSince EC $2$ and EC $3$ fail the criterion, the entire dataset **fails** to satisfy $0.2$-closeness.\n\n**Summary of Compliance:**\n-   $k$-anonymity ($k=3$): **Satisfied**\n-   $l$-diversity ($l=2$): **Failed**\n-   $t$-closeness ($t=0.2$): **Failed**\n\n**Option-by-Option Analysis**\n\n**A. The dataset satisfies $k$-anonymity for $k=3$, satisfies $l$-diversity for $l=2$, and fails $t$-closeness for $t=0.2$; thus, failure of $t$-closeness here reflects only a technicality and does not imply any increased risk of attribute disclosure.**\nThis statement claims the dataset satisfies $l$-diversity for $l=2$, which is incorrect based on our analysis of EC $2$. Furthermore, the assertion that the failure of $t$-closeness is a \"technicality\" with no increased risk is scientifically unsound. The large distance for EC $2$ ($d_{\\mathrm{TV}} \\approx 0.583$) directly quantifies a severe information leak, as it corresponds to a class where the sensitive outcome is known with certainty.\n**Verdict: Incorrect.**\n\n**B. The dataset satisfies $k$-anonymity for $k=3$, fails $l$-diversity for $l=2$, and fails $t$-closeness for $t=0.2$. This demonstrates that protecting against identity disclosure via $k$-anonymity does not by itself prevent attribute disclosure in shared clinical trial IPD.**\nThe first sentence accurately summarizes our compliance analysis. The second sentence draws a valid and fundamental conclusion from this analysis. The dataset is $3$-anonymous, which provides a degree of protection against identity disclosure (record linkage). However, it fails both $l$-diversity and $t$-closeness, leading to a definitive attribute disclosure for members of EC $2$. This perfectly illustrates the principle that $k$-anonymity alone is insufficient to protect against attribute disclosure.\n**Verdict: Correct.**\n\n**C. The dataset violates $k$-anonymity for $k=3$ because one class has all TESAE values identical, but it satisfies $l$-diversity for $l=2$ since the global TESAE distribution has two categories (Yes/No) overall.**\nThis statement is incorrect on multiple counts. First, the dataset *satisfies* $3$-anonymity, as all class sizes are $\\geq 3$. The reason given for the purported violation (homogeneity of sensitive attribute) is the definition relevant to $l$-diversity, not $k$-anonymity. Second, the dataset *fails* $2$-diversity. Third, the reason given for satisfying $l$-diversity (the global distribution having two categories) is irrelevant to the definition of $l$-diversity, which is a per-class property.\n**Verdict: Incorrect.**\n\n**D. In clinical trial registries such as ClinicalTrials.gov, baseline characteristic tables inherently satisfy $k$-anonymity with $k$ equal to the arm’s sample size, so the $k$-anonymity criterion is automatically met for any posted result.**\nThis statement fundamentally misapplies the concept of $k$-anonymity. $k$-anonymity is a property of microdata sets (IPD), where individual records exist and can potentially be linked to external information. Baseline characteristic tables posted on ClinicalTrials.gov are aggregate data (e.g., means, proportions, counts for an entire study arm). They do not contain individual records, so the notion of an equivalence class of size $k$ does not apply in the standard way. The statement conflates aggregate reporting with microdata anonymization.\n**Verdict: Incorrect.**\n\n**E. In the context of sharing clinical trial IPD, $k$-anonymity means each set of records sharing the same quasi-identifiers has at least $k$ members, mitigating record linkage; $l$-diversity requires each such set to contain at least $l$ well-represented distinct sensitive values, mitigating homogeneity and background knowledge attacks; and $t$-closeness constrains the sensitive-attribute distribution in each set to be within distance $t$ of the global distribution, mitigating attribute disclosure in skewed data. Together, these models reduce identity and attribute disclosure risk while enabling reproducibility and secondary analyses when IPD access is linked from ClinicalTrials.gov.**\nThis option provides a comprehensive and accurate definition of the three privacy models and their respective goals. It correctly identifies that $k$-anonymity addresses identity disclosure via record linkage. It correctly states the purpose of $l$-diversity (mitigating homogeneity and background knowledge attacks) and $t$-closeness (mitigating skewness attacks and attribute disclosure). Finally, it correctly situates these models within the broader context of balancing privacy risks with the benefits of data sharing for scientific advancement. This statement is a correct and complete conceptual summary.\n**Verdict: Correct.**", "answer": "$$\\boxed{BE}$$", "id": "4999116"}]}