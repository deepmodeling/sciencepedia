{"hands_on_practices": [{"introduction": "A well-designed trial begins with a robust plan. This practice addresses the fundamental question of \"how many participants do we need?\" by guiding you through the derivation of the sample size formula for a main effect in a classic $2 \\times 2$ factorial design. Working from first principles [@problem_id:5014984], you will connect the target effect size $\\Delta_A$, statistical power, and data variability $\\sigma$ to determine the necessary sample size, a cornerstone skill in efficient experimental design.", "problem": "A translational medicine team is planning a balanced two-by-two factorial trial to screen two binary interventions, denoted as factor $A$ (levels $A_0$ and $A_1$) and factor $B$ (levels $B_0$ and $B_1$), on a continuous biomarker endpoint. The measurement process has been standardized so that the population standard deviation is known and equal to $\\sigma$. The design assigns $n$ independent patients to each of the four factorial cells $(A_0B_0)$, $(A_0B_1)$, $(A_1B_0)$, and $(A_1B_1)$, with independence across all patients and cells.\n\nThe scientific estimand is the main effect of factor $A$, defined as the population difference in marginal means across levels of $B$, that is,\n$$\n\\Delta_A \\equiv \\frac{\\mu_{A_1B_0} + \\mu_{A_1B_1}}{2} - \\frac{\\mu_{A_0B_0} + \\mu_{A_0B_1}}{2},\n$$\nwhere $\\mu_{ab}$ denotes the cell mean for cell $(A_a,B_b)$. The team will test the null hypothesis $H_0:\\Delta_A=0$ versus the two-sided alternative $H_1:\\Delta_A\\neq 0$ at two-sided significance level $\\alpha=0.05$, using a normal-theory test with known $\\sigma$.\n\nStarting from first principles, namely the definition of the main effect as a linear contrast of cell means, the properties of averages of independent and identically distributed normal random variables, and the distributional properties of standardized statistics under the null and alternative, derive the minimum per-cell sample size $n$ required to achieve power $1-\\beta$ to detect a true main effect of magnitude $|\\Delta_A|$.\n\nExpress your final answer as a single closed-form analytic expression in terms of $\\sigma$, $\\Delta_A$, and $\\beta$, and you may use the cumulative distribution function of the standard normal distribution and its quantiles if needed. No rounding is required, and no units are needed for $n$ because it is a count.", "solution": "The problem asks for the derivation of the minimum per-cell sample size, denoted by $n$, for a balanced two-by-two factorial trial. The objective is to achieve a specified power $1-\\beta$ for testing the main effect of factor $A$ at a significance level $\\alpha=0.05$. The derivation will proceed from first principles as stipulated.\n\nLet the four factorial cells be $(A_0B_0)$, $(A_0B_1)$, $(A_1B_0)$, and $(A_1B_1)$. The population mean response for a subject in cell $(A_a, B_b)$ is denoted by $\\mu_{ab}$, for $a,b \\in \\{0,1\\}$. We are given that there are $n$ independent patients per cell, and the biomarker measurements are independent across all patients. The observations for patients in cell $(A_a, B_b)$, denoted $Y_{ab,i}$ for $i=1, \\dots, n$, are assumed to be drawn from a normal distribution with mean $\\mu_{ab}$ and a known common variance $\\sigma^2$. That is, $Y_{ab,i} \\sim N(\\mu_{ab}, \\sigma^2)$.\n\nThe scientific estimand is the main effect of factor $A$, defined as the difference between the marginal means of the levels of $A$:\n$$\n\\Delta_A \\equiv \\frac{\\mu_{A_1B_0} + \\mu_{A_1B_1}}{2} - \\frac{\\mu_{A_0B_0} + \\mu_{A_0B_1}}{2}\n$$\n\n**Step 1: Define the Estimator and its Sampling Distribution**\n\nA natural and unbiased estimator for $\\Delta_A$ is obtained by replacing the population cell means $\\mu_{ab}$ with their respective sample means $\\bar{Y}_{ab} = \\frac{1}{n} \\sum_{i=1}^{n} Y_{ab,i}$. The estimator for the main effect of $A$ is:\n$$\n\\hat{\\Delta}_A = \\frac{\\bar{Y}_{A_1B_0} + \\bar{Y}_{A_1B_1}}{2} - \\frac{\\bar{Y}_{A_0B_0} + \\bar{Y}_{A_0B_1}}{2}\n$$\nThis can be written as a linear contrast of the four cell means:\n$$\n\\hat{\\Delta}_A = \\frac{1}{2}\\bar{Y}_{A_1B_0} + \\frac{1}{2}\\bar{Y}_{A_1B_1} - \\frac{1}{2}\\bar{Y}_{A_0B_0} - \\frac{1}{2}\\bar{Y}_{A_0B_1}\n$$\nFrom the properties of averages of independent and identically distributed normal random variables, each sample mean $\\bar{Y}_{ab}$ is also normally distributed:\n$$\n\\bar{Y}_{ab} \\sim N\\left(\\mu_{ab}, \\frac{\\sigma^2}{n}\\right)\n$$\nSince $\\hat{\\Delta}_A$ is a linear combination of independent normal random variables, it is itself normally distributed. We find its mean and variance.\n\nThe expected value of $\\hat{\\Delta}_A$ is:\n$$\nE[\\hat{\\Delta}_A] = \\frac{1}{2}E[\\bar{Y}_{A_1B_0}] + \\frac{1}{2}E[\\bar{Y}_{A_1B_1}] - \\frac{1}{2}E[\\bar{Y}_{A_0B_0}] - \\frac{1}{2}E[\\bar{Y}_{A_0B_1}] = \\frac{\\mu_{A_1B_0} + \\mu_{A_1B_1}}{2} - \\frac{\\mu_{A_0B_0} + \\mu_{A_0B_1}}{2} = \\Delta_A\n$$\nThis shows that $\\hat{\\Delta}_A$ is an unbiased estimator of $\\Delta_A$.\n\nThe variance of $\\hat{\\Delta}_A$, using the independence of the four cell means, is:\n$$\n\\text{Var}(\\hat{\\Delta}_A) = \\text{Var}\\left(\\frac{1}{2}\\bar{Y}_{A_1B_0} + \\frac{1}{2}\\bar{Y}_{A_1B_1} - \\frac{1}{2}\\bar{Y}_{A_0B_0} - \\frac{1}{2}\\bar{Y}_{A_0B_1}\\right)\n$$\n$$\n\\text{Var}(\\hat{\\Delta}_A) = \\left(\\frac{1}{2}\\right)^2\\text{Var}(\\bar{Y}_{A_1B_0}) + \\left(\\frac{1}{2}\\right)^2\\text{Var}(\\bar{Y}_{A_1B_1}) + \\left(-\\frac{1}{2}\\right)^2\\text{Var}(\\bar{Y}_{A_0B_0}) + \\left(-\\frac{1}{2}\\right)^2\\text{Var}(\\bar{Y}_{A_0B_1})\n$$\n$$\n\\text{Var}(\\hat{\\Delta}_A) = \\frac{1}{4} \\left( \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{n} \\right) = \\frac{1}{4} \\left( \\frac{4\\sigma^2}{n} \\right) = \\frac{\\sigma^2}{n}\n$$\nThus, the sampling distribution of the estimator is:\n$$\n\\hat{\\Delta}_A \\sim N\\left(\\Delta_A, \\frac{\\sigma^2}{n}\\right)\n$$\n\n**Step 2: Formulate the Hypothesis Test**\n\nThe null hypothesis is $H_0: \\Delta_A = 0$ and the alternative is $H_1: \\Delta_A \\neq 0$. The test statistic is the standardized estimator under the null hypothesis:\n$$\nZ = \\frac{\\hat{\\Delta}_A - 0}{\\sqrt{\\text{Var}(\\hat{\\Delta}_A)}} = \\frac{\\hat{\\Delta}_A}{\\sigma/\\sqrt{n}}\n$$\nUnder $H_0$, $\\Delta_A=0$, so $E[\\hat{\\Delta}_A] = 0$. The test statistic $Z$ follows the standard normal distribution, $Z \\sim N(0,1)$.\n\nFor a two-sided test with significance level $\\alpha$, we reject $H_0$ if the observed value of $|Z|$ is greater than the critical value $z_{1-\\alpha/2}$, where $z_q$ is the $q$-th quantile of the standard normal distribution. The rejection region is $|Z| > z_{1-\\alpha/2}$.\n\n**Step 3: Derive the Power Function**\n\nPower is the probability of rejecting $H_0$ given that the alternative hypothesis $H_1$ is true. Let us assume the true main effect has a specific magnitude, denoted by $\\Delta_A$. Under this alternative, the estimator $\\hat{\\Delta}_A$ is distributed as $N(\\Delta_A, \\sigma^2/n)$. The test statistic $Z = \\hat{\\Delta}_A / (\\sigma/\\sqrt{n})$ is therefore distributed as $N(\\Delta_A / (\\sigma/\\sqrt{n}), 1)$.\n\nThe power, $1-\\beta$, is given by:\n$$\n1-\\beta = P(\\text{Reject } H_0 | \\Delta_A \\text{ is true}) = P(|Z| > z_{1-\\alpha/2} | \\Delta_A)\n$$\n$$\n1-\\beta = P\\left(Z > z_{1-\\alpha/2} | \\Delta_A\\right) + P\\left(Z < -z_{1-\\alpha/2} | \\Delta_A\\right)\n$$\nTo evaluate this probability, we standardize the random variable $Z$ by subtracting its true mean under $H_1$ and dividing by its standard deviation (which is $1$). Let $Z_{std} \\sim N(0,1)$.\n$$\n\\frac{Z - \\frac{\\Delta_A}{\\sigma/\\sqrt{n}}}{1} = \\frac{\\hat{\\Delta}_A/(\\sigma/\\sqrt{n}) - \\Delta_A/(\\sigma/\\sqrt{n})}{1} = \\frac{\\hat{\\Delta}_A - \\Delta_A}{\\sigma/\\sqrt{n}} \\sim N(0,1)\n$$\nSo, the power expression becomes:\n$$\n1-\\beta = P\\left(Z_{std} > z_{1-\\alpha/2} - \\frac{\\Delta_A\\sqrt{n}}{\\sigma}\\right) + P\\left(Z_{std} < -z_{1-\\alpha/2} - \\frac{\\Delta_A\\sqrt{n}}{\\sigma}\\right)\n$$\nThe problem specifies detecting an effect of magnitude $|\\Delta_A|$. Let's consider the case where $\\Delta_A > 0$. For a reasonable power, $n$ will be large enough such that the term $\\frac{\\Delta_A\\sqrt{n}}{\\sigma}$ is positive and significant. The second term, $P(Z_{std} < -z_{1-\\alpha/2} - \\frac{\\Delta_A\\sqrt{n}}{\\sigma})$, which corresponds to rejecting in the wrong tail, becomes negligible. Power is dominated by the first term. A similar argument holds if $\\Delta_A < 0$, where the first term becomes negligible.\n\nThus, we focus on the main contribution to power. For $|\\Delta_A| > 0$:\n$$\n1-\\beta \\approx P\\left(Z_{std} > z_{1-\\alpha/2} - \\frac{|\\Delta_A|\\sqrt{n}}{\\sigma}\\right)\n$$\nThe standard normal quantile $z_{1-\\beta}$ is defined by $1-\\beta = P(Z_{std} > z_{\\beta})$. By symmetry of the normal distribution, $z_{\\beta} = -z_{1-\\beta}$. So, $1-\\beta = P(Z_{std} > -z_{1-\\beta})$.\nEquating the arguments of the probability function, we get:\n$$\n-z_{1-\\beta} = z_{1-\\alpha/2} - \\frac{|\\Delta_A|\\sqrt{n}}{\\sigma}\n$$\n\n**Step 4: Solve for the Sample Size $n$**\n\nRearranging the equation to solve for $n$:\n$$\n\\frac{|\\Delta_A|\\sqrt{n}}{\\sigma} = z_{1-\\alpha/2} + z_{1-\\beta}\n$$\n$$\n\\sqrt{n} = \\frac{\\sigma}{|\\Delta_A|} (z_{1-\\alpha/2} + z_{1-\\beta})\n$$\nSquaring both sides gives the expression for the required per-cell sample size $n$:\n$$\nn = \\frac{\\sigma^2 (z_{1-\\alpha/2} + z_{1-\\beta})^2}{\\Delta_A^2}\n$$\nThe problem specifies a two-sided significance level of $\\alpha = 0.05$. Therefore, $1-\\alpha/2 = 1-0.025 = 0.975$. Substituting this value into the general formula yields the final expression:\n$$\nn = \\frac{\\sigma^2 (z_{0.975} + z_{1-\\beta})^2}{\\Delta_A^2}\n$$\nThis is the required closed-form expression for the minimum per-cell sample size $n$ in terms of $\\sigma$, $\\Delta_A$, and $\\beta$. Note that in a practical application, the calculated value of $n$ would be rounded up to the nearest integer.", "answer": "$$\n\\boxed{\\frac{\\sigma^2 (z_{0.975} + z_{1-\\beta})^2}{\\Delta_A^2}}\n$$", "id": "5014984"}, {"introduction": "After collecting data, the focus shifts to analysis and estimation. This exercise demystifies how main effects are quantified within the rigorous framework of the General Linear Model (GLM). You will use effects coding and matrix algebra to derive the least-squares estimator for a main effect [@problem_id:5015061], providing a deeper understanding of how statistical models partition variability and formally define the contrasts that correspond to our scientific questions.", "problem": "In a translational medicine Phase II trial evaluating two interventions using a $2 \\times 2$ factorial design with equal allocation, let factor $A$ represent a novel cellular therapy ($A=-1$ denotes control and $A=+1$ denotes therapy) and let factor $B$ represent biomarker-guided dosing ($B=-1$ denotes standard dosing and $B=+1$ denotes biomarker-guided dosing). At the first interim look of a group sequential design with equal information contributed by each cell, assume the per-cell sample sizes are equal and that the observed cell means are $\\bar{Y}_{ab}$ with $a,b \\in \\{0,1\\}$, where the mapping to effects coding is $a=0 \\leftrightarrow A=-1$, $a=1 \\leftrightarrow A=+1$, $b=0 \\leftrightarrow B=-1$, and $b=1 \\leftrightarrow B=+1$. Suppose the data are modeled by the General Linear Model (GLM) $Y = \\mu + \\alpha A + \\beta B + \\gamma AB + \\varepsilon$, where $\\mu$ is the grand mean, $\\alpha$ and $\\beta$ are the main effects of $A$ and $B$, respectively, $\\gamma$ is the interaction effect, and $\\varepsilon$ is a mean-zero error term with finite variance. Using only the assumptions stated (effects coding, balanced cells at the interim look) and first principles from the GLM, derive the closed-form analytic expression for the least-squares contrast estimate of the $B$ main effect parameter $\\beta$ in terms of the four observed cell means $\\bar{Y}_{00}$, $\\bar{Y}_{01}$, $\\bar{Y}_{10}$, and $\\bar{Y}_{11}$. Provide the final formula as a single analytic expression. No rounding is required and no units are to be reported.", "solution": "The objective is to find the least-squares estimate for the parameter $\\beta$ in the model $Y = \\mu + \\alpha A + \\beta B + \\gamma AB + \\varepsilon$. Let the vector of parameters be $\\boldsymbol{\\theta} = \\begin{pmatrix} \\mu & \\alpha & \\beta & \\gamma \\end{pmatrix}^T$. The least-squares estimate $\\hat{\\boldsymbol{\\theta}}$ is given by the formula $\\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$, where $\\mathbf{X}$ is the design matrix and $\\mathbf{y}$ is the vector of observations.\n\nSince the design is balanced (equal sample sizes per cell), the least-squares estimates of the model parameters can be derived using the four cell means as the observation vector. Let us define the observation vector $\\mathbf{y}$ and the design matrix $\\mathbf{X}$ for the four treatment combinations. The four combinations correspond to $(A, B)$ pairs of $(-1, -1)$, $(-1, +1)$, $(+1, -1)$, and $(+1, +1)$. Using the provided mapping, these correspond to the cell means $\\bar{Y}_{00}$, $\\bar{Y}_{01}$, $\\bar{Y}_{10}$, and $\\bar{Y}_{11}$, respectively.\n\nThe vector of observed outcomes is:\n$$\n\\mathbf{y} = \\begin{pmatrix} \\bar{Y}_{00} \\\\ \\bar{Y}_{01} \\\\ \\bar{Y}_{10} \\\\ \\bar{Y}_{11} \\end{pmatrix}\n$$\nThe model equation $E[Y] = \\mu(1) + \\alpha(A) + \\beta(B) + \\gamma(AB)$ defines the columns of the design matrix $\\mathbf{X}$. The columns correspond to the predictors for $\\mu, \\alpha, \\beta, \\gamma$, which are $1, A, B,$ and $AB$. The rows correspond to the four experimental conditions.\n\n1.  Row 1: $(A,B) = (-1,-1)$. The predictor vector is $(1, -1, -1, 1)$.\n2.  Row 2: $(A,B) = (-1,+1)$. The predictor vector is $(1, -1, +1, (-1)(+1)) = (1, -1, 1, -1)$.\n3.  Row 3: $(A,B) = (+1,-1)$. The predictor vector is $(1, +1, -1, (+1)(-1)) = (1, 1, -1, -1)$.\n4.  Row 4: $(A,B) = (+1,+1)$. The predictor vector is $(1, +1, +1, (+1)(+1)) = (1, 1, 1, 1)$.\n\nThe design matrix $\\mathbf{X}$ is therefore:\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1 & -1 & -1 & 1 \\\\\n1 & -1 & 1 & -1 \\\\\n1 & 1 & -1 & -1 \\\\\n1 & 1 & 1 & 1\n\\end{pmatrix}\n$$\nNext, we compute $\\mathbf{X}^T \\mathbf{X}$. Due to the use of effects coding in a balanced factorial design, the columns of $\\mathbf{X}$ are orthogonal. The dot product of any two distinct columns is zero. For example, the dot product of column 2 (for $\\alpha$) and column 3 (for $\\beta$) is $(-1)(-1) + (-1)(1) + (1)(-1) + (1)(1) = 1 - 1 - 1 + 1 = 0$. The dot product of any column with itself is the sum of its squared elements, which is $1^2+(-1)^2+1^2+(-1)^2 = 4$ for columns 2, 3, and 4, and $1^2+1^2+1^2+1^2 = 4$ for column 1.\nTherefore, $\\mathbf{X}^T \\mathbf{X}$ is a diagonal matrix:\n$$\n\\mathbf{X}^T \\mathbf{X} = \\begin{pmatrix}\n4 & 0 & 0 & 0 \\\\\n0 & 4 & 0 & 0 \\\\\n0 & 0 & 4 & 0 \\\\\n0 & 0 & 0 & 4\n\\end{pmatrix} = 4\\mathbf{I}\n$$\nwhere $\\mathbf{I}$ is the $4 \\times 4$ identity matrix.\n\nThe inverse matrix is straightforward to compute:\n$$\n(\\mathbf{X}^T \\mathbf{X})^{-1} = \\frac{1}{4} \\mathbf{I} = \\begin{pmatrix}\n1/4 & 0 & 0 & 0 \\\\\n0 & 1/4 & 0 & 0 \\\\\n0 & 0 & 1/4 & 0 \\\\\n0 & 0 & 0 & 1/4\n\\end{pmatrix}\n$$\nNext, we compute $\\mathbf{X}^T \\mathbf{y}$:\n$$\n\\mathbf{X}^T \\mathbf{y} = \\begin{pmatrix}\n1 & 1 & 1 & 1 \\\\\n-1 & -1 & 1 & 1 \\\\\n-1 & 1 & -1 & 1 \\\\\n1 & -1 & -1 & 1\n\\end{pmatrix}\n\\begin{pmatrix} \\bar{Y}_{00} \\\\ \\bar{Y}_{01} \\\\ \\bar{Y}_{10} \\\\ \\bar{Y}_{11} \\end{pmatrix}\n= \\begin{pmatrix}\n\\bar{Y}_{00} + \\bar{Y}_{01} + \\bar{Y}_{10} + \\bar{Y}_{11} \\\\\n-\\bar{Y}_{00} - \\bar{Y}_{01} + \\bar{Y}_{10} + \\bar{Y}_{11} \\\\\n-\\bar{Y}_{00} + \\bar{Y}_{01} - \\bar{Y}_{10} + \\bar{Y}_{11} \\\\\n\\bar{Y}_{00} - \\bar{Y}_{01} - \\bar{Y}_{10} + \\bar{Y}_{11}\n\\end{pmatrix}\n$$\nFinally, we find the vector of parameter estimates, $\\hat{\\boldsymbol{\\theta}} = (\\hat{\\mu}, \\hat{\\alpha}, \\hat{\\beta}, \\hat{\\gamma})^T$:\n$$\n\\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y} = \\frac{1}{4}\n\\begin{pmatrix}\n\\bar{Y}_{00} + \\bar{Y}_{01} + \\bar{Y}_{10} + \\bar{Y}_{11} \\\\\n-\\bar{Y}_{00} - \\bar{Y}_{01} + \\bar{Y}_{10} + \\bar{Y}_{11} \\\\\n-\\bar{Y}_{00} + \\bar{Y}_{01} - \\bar{Y}_{10} + \\bar{Y}_{11} \\\\\n\\bar{Y}_{00} - \\bar{Y}_{01} - \\bar{Y}_{10} + \\bar{Y}_{11}\n\\end{pmatrix}\n$$\nFrom this vector, we can extract the estimate for $\\beta$, which is the third component:\n$$\n\\hat{\\beta} = \\frac{1}{4} \\left( -\\bar{Y}_{00} + \\bar{Y}_{01} - \\bar{Y}_{10} + \\bar{Y}_{11} \\right)\n$$\nThis expression is the least-squares contrast estimate for the parameter $\\beta$. It represents half of the main effect of factor $B$, where the main effect is defined as the difference between the average response at the high level of $B$ and the average response at the low level of $B$. The expression can be rearranged for clarity.\n$$\n\\hat{\\beta} = \\frac{(\\bar{Y}_{01} + \\bar{Y}_{11}) - (\\bar{Y}_{00} + \\bar{Y}_{10})}{4}\n$$\nThis is the final closed-form analytic expression for the estimator of $\\beta$.", "answer": "$$\n\\boxed{\\frac{1}{4} \\left( -\\bar{Y}_{00} + \\bar{Y}_{01} - \\bar{Y}_{10} + \\bar{Y}_{11} \\right)}\n$$", "id": "5015061"}, {"introduction": "Group sequential designs (GSDs) are monitored not by sample size alone, but by the accumulation of statistical information. This practice illuminates this core concept by asking you to derive the relationship between sample size and information for a trial with a binary endpoint [@problem_id:5015009]. Understanding \"information time\" is crucial for correctly planning interim analyses and applying spending functions to control error rates in adaptive trials.", "problem": "A two-arm, $1{:}1$ randomized group sequential design (GSD) is planned in a translational medicine trial evaluating an investigational add-on therapy versus standard of care on a binary clinical endpoint assessed at a fixed time point. The primary analysis is a Wald-type test of the difference in responder proportions between treatment and control. Assume that, at each analysis, responders in each arm are independent Bernoulli outcomes with a common response probability under the null hypothesis, and that the allocation ratio remains $1{:}1$ at each information look. The first interim analysis occurs after a total of $n_1$ randomized participants, the second interim after a total of $n_2$ randomized participants, and the final analysis after a total of $n$ randomized participants, where the totals are across both arms combined. Assume equal per-subject information over time, justified by stable response probability across looks and independence of outcomes.\n\nStarting from the large-sample variance of the difference in sample proportions and the definition of information as the inverse of the variance of an efficient estimator, derive how the cumulative information at a look relates to the cumulative total sample size under these assumptions. Using this relationship, compute the information fraction at the second interim, defined as $t_2 = I_2 / I^{*}$ where $I_2$ is the cumulative information at the second interim and $I^{*}$ is the information at the final analysis, for $n_1 = 200$, $n_2 = 400$, and $n = 800$. Provide your final answer as a single real number. No rounding is necessary.", "solution": "Let $p_T$ and $p_C$ be the true proportions of responders in the treatment and control arms, respectively. Let $\\hat{p}_T$ and $\\hat{p}_C$ be the corresponding sample proportions observed in the trial. The primary analysis concerns the difference in proportions, $\\delta = p_T - p_C$, which is estimated by $\\hat{\\delta} = \\hat{p}_T - \\hat{p}_C$.\n\nThe problem states that the analysis is a Wald-type test. The test statistic for this test is based on the estimator $\\hat{\\delta}$ and its variance. We begin by defining the variance of $\\hat{\\delta}$ at an arbitrary analysis look $k$, where the cumulative total number of participants is $N_k$.\n\nGiven the $1{:}1$ randomization, the number of participants in the treatment arm is $N_{k,T} = N_k/2$ and in the control arm is $N_{k,C} = N_k/2$. The outcomes are independent Bernoulli variables. The variance of a sample proportion from a single arm (e.g., treatment) is given by:\n$$ \\text{Var}(\\hat{p}_{k,T}) = \\frac{p_T(1-p_T)}{N_{k,T}} $$\nSince the two arms are independent, the variance of the difference in sample proportions is the sum of their individual variances:\n$$ \\text{Var}(\\hat{\\delta}_k) = \\text{Var}(\\hat{p}_{k,T}) + \\text{Var}(\\hat{p}_{k,C}) = \\frac{p_T(1-p_T)}{N_{k,T}} + \\frac{p_C(1-p_C)}{N_{k,C}} $$\nThe problem specifies that under the null hypothesis ($H_0$), there is a common response probability, so $p_T = p_C = p$. The variance under $H_0$ is then:\n$$ \\text{Var}(\\hat{\\delta}_k | H_0) = \\frac{p(1-p)}{N_{k,T}} + \\frac{p(1-p)}{N_{k,C}} $$\nSubstituting $N_{k,T} = N_{k,C} = N_k/2$:\n$$ \\text{Var}(\\hat{\\delta}_k | H_0) = \\frac{p(1-p)}{N_k/2} + \\frac{p(1-p)}{N_k/2} = 2 \\times \\frac{p(1-p)}{N_k/2} = \\frac{4p(1-p)}{N_k} $$\nThe problem defines statistical information, $I_k$, as the inverse of the variance of the efficient estimator $\\hat{\\delta}_k$. Therefore, the cumulative information at look $k$ is:\n$$ I_k = \\frac{1}{\\text{Var}(\\hat{\\delta}_k | H_0)} = \\frac{1}{\\frac{4p(1-p)}{N_k}} = \\frac{N_k}{4p(1-p)} $$\nThis equation provides the derived relationship between the cumulative information $I_k$ and the cumulative total sample size $N_k$. The term $p(1-p)$ is constant under the assumption of a stable response probability across looks. This confirms that information is directly proportional to the sample size, which is consistent with the stated assumption of \"equal per-subject information over time.\"\n\nThe problem asks for the information fraction at the second interim analysis, defined as $t_2 = I_2 / I^*$.\nHere, $I_2$ is the cumulative information at the second interim, corresponding to a total sample size of $N_2 = n_2$.\n$I^*$ is the information at the final analysis, corresponding to a total sample size of $N^* = n$.\n\nUsing the derived relationship:\n- The information at the second interim is $I_2 = \\frac{n_2}{4p(1-p)}$.\n- The information at the final analysis is $I^* = \\frac{n}{4p(1-p)}$.\n\nNow, we compute the ratio $t_2$:\n$$ t_2 = \\frac{I_2}{I^*} = \\frac{\\frac{n_2}{4p(1-p)}}{\\frac{n}{4p(1-p)}} $$\nThe term $4p(1-p)$ is a non-zero constant that cancels from the numerator and denominator, yielding:\n$$ t_2 = \\frac{n_2}{n} $$\nThis result shows that under the given assumptions, the information fraction is simply the ratio of the cumulative sample sizes.\n\nWe are given the following numerical values:\n- $n_2 = 400$\n- $n = 800$\n\nSubstituting these values into the expression for $t_2$:\n$$ t_2 = \\frac{400}{800} = \\frac{1}{2} = 0.5 $$\nThe information fraction at the second interim analysis is $0.5$. The information for the first interim analysis, $n_1=200$, is not required for this final calculation.", "answer": "$$\\boxed{0.5}$$", "id": "5015009"}]}