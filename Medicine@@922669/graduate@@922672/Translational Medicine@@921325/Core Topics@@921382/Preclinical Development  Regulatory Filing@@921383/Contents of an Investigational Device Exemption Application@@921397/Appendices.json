{"hands_on_practices": [{"introduction": "A robust risk analysis is the foundation of a successful Investigational Device Exemption (IDE) application, demonstrating a thorough understanding of potential harms and a systematic plan for their mitigation. This exercise [@problem_id:5002857] will guide you through the application of Failure Modes and Effects Analysis (FMEA), a core tool in medical device risk management. You will practice quantifying the impact of proposed risk controls on severity ($S$), occurrence ($O$), and detection ($D$) ratings to calculate a post-mitigation risk priority number (RPN), a critical skill for justifying that a device is acceptably safe to proceed to human trials.", "problem": "A sponsor is preparing the risk analysis section of an Investigational Device Exemption (IDE) application for a first-in-human study of an implantable neuromodulation system. The study team uses Failure Modes and Effects Analysis (FMEA), defining three ordinal ratings for each identified hazard: severity $S$ (harm magnitude), occurrence $O$ (frequency of the failure mode), and detection difficulty $D$ (likelihood the failure escapes detection before harm), each on the scale $1$ (best) to $10$ (worst). In the IDE risk analysis narrative, risk controls must be selected in accordance with International Organization for Standardization (ISO) 14971, in hierarchical order: inherent safety by design, protective measures in the device or process, and information for safety. The team aims to compute an exposure-weighted overall post-control risk priority number from three hazards, consistent with standard FMEA aggregation practices.\n\nHazards and pre-mitigation ratings:\n- Hazard $H_1$ (lead fracture causing intermittent stimulation loss): $S_1 = 7$, $O_1 = 4$, $D_1 = 6$.\n- Hazard $H_2$ (electrical overstimulation causing reversible dysesthesia): $S_2 = 6$, $O_2 = 3$, $D_2 = 5$.\n- Hazard $H_3$ (deep surgical site infection requiring explant): $S_3 = 9$, $O_3 = 2$, $D_3 = 7$.\n\nRisk controls proposed (aligned to the IDE narrative) and their independent, verified quantitative effects on the corresponding ratings:\n- For $H_1$: inherent design control (braided conductor with doubled fatigue life) reduces occurrence by $50\\%$; protective measure (embedded impedance surveillance) improves detectability, reducing detection difficulty by $30\\%$.\n- For $H_2$: protective measures (current-limiting hardware and auto-shutdown) reduce severity by $25\\%$ and occurrence by $30\\%$; alarm logic improves detectability, reducing detection difficulty by $40\\%$.\n- For $H_3$: inherent design control (antimicrobial coating) reduces occurrence by $20\\%$; information for safety (aseptic protocol plus perioperative antibiotics) reduces occurrence by $35\\%$; structured post-operative surveillance improves detectability, reducing detection difficulty by $20\\%$.\n\nAssume the controls act independently and multiplicatively on each affected rating (for example, two occurrence reductions of $a\\%$ and $b\\%$ yield a combined occurrence factor of $(1 - a/100)\\times(1 - b/100)$ applied to the original $O$). Unaffected ratings remain unchanged.\n\nDefine the overall post-control risk score as the exposure-weighted average of the individual post-control risk priority numbers for $H_1$, $H_2$, and $H_3$, using patient exposure weights $w_1 = 0.55$, $w_2 = 0.25$, and $w_3 = 0.20$ that sum to $1$ and represent the fraction of device-lifetime patient-time in which each hazard is the dominant risk state.\n\nCompute this overall post-control risk score. Round your final numeric answer to four significant figures. Provide your final numeric answer without units.", "solution": "The problem requires the calculation of an overall post-control risk score for a neuromodulation device. This score is defined as the exposure-weighted average of the post-control Risk Priority Numbers (RPNs) for three identified hazards: $H_1$, $H_2$, and $H_3$. The standard formula for an RPN, consistent with Failure Modes and Effects Analysis (FMEA) practice, is the product of Severity ($S$), Occurrence ($O$), and Detection Difficulty ($D$): $RPN = S \\times O \\times D$.\n\nFirst, we must calculate the post-control ratings for each hazard by applying the stated risk controls. The pre-control ratings are denoted with a 'pre' subscript, and post-control ratings with a 'post' subscript.\n\n**Hazard $H_1$: Lead fracture**\n\nThe pre-control ratings are $S_{1, \\text{pre}} = 7$, $O_{1, \\text{pre}} = 4$, and $D_{1, \\text{pre}} = 6$.\nThe risk controls affect the Occurrence and Detection Difficulty ratings. The Severity rating is unchanged.\n-   A design control reduces occurrence by $50\\%$.\n-   A protective measure reduces detection difficulty by $30\\%$.\n\nThe post-control ratings are calculated as follows:\n$S_{1, \\text{post}} = S_{1, \\text{pre}} = 7$\n$O_{1, \\text{post}} = O_{1, \\text{pre}} \\times (1 - \\frac{50}{100}) = 4 \\times (1 - 0.5) = 4 \\times 0.5 = 2$\n$D_{1, \\text{post}} = D_{1, \\text{pre}} \\times (1 - \\frac{30}{100}) = 6 \\times (1 - 0.3) = 6 \\times 0.7 = 4.2$\n\nThe post-control RPN for $H_1$ is:\n$RPN_{1, \\text{post}} = S_{1, \\text{post}} \\times O_{1, \\text{post}} \\times D_{1, \\text{post}} = 7 \\times 2 \\times 4.2 = 58.8$\n\n**Hazard $H_2$: Electrical overstimulation**\n\nThe pre-control ratings are $S_{2, \\text{pre}} = 6$, $O_{2, \\text{pre}} = 3$, and $D_{2, \\text{pre}} = 5$.\nRisk controls affect all three ratings.\n-   Protective measures reduce severity by $25\\%$ and occurrence by $30\\%$.\n-   An alarm logic improves detectability, reducing detection difficulty by $40\\%$.\n\nThe post-control ratings are:\n$S_{2, \\text{post}} = S_{2, \\text{pre}} \\times (1 - \\frac{25}{100}) = 6 \\times (1 - 0.25) = 6 \\times 0.75 = 4.5$\n$O_{2, \\text{post}} = O_{2, \\text{pre}} \\times (1 - \\frac{30}{100}) = 3 \\times (1 - 0.3) = 3 \\times 0.7 = 2.1$\n$D_{2, \\text{post}} = D_{2, \\text{pre}} \\times (1 - \\frac{40}{100}) = 5 \\times (1 - 0.4) = 5 \\times 0.6 = 3$\n\nThe post-control RPN for $H_2$ is:\n$RPN_{2, \\text{post}} = S_{2, \\text{post}} \\times O_{2, \\text{post}} \\times D_{2, \\text{post}} = 4.5 \\times 2.1 \\times 3 = 28.35$\n\n**Hazard $H_3$: Deep surgical site infection**\n\nThe pre-control ratings are $S_{3, \\text{pre}} = 9$, $O_{3, \\text{pre}} = 2$, and $D_{3, \\text{pre}} = 7$.\nThe controls affect Occurrence and Detection Difficulty. The Severity rating is unchanged.\n-   An inherent design control reduces occurrence by $20\\%$.\n-   Information for safety (protocols) reduces occurrence by an additional $35\\%$.\n-   Surveillance improves detectability, reducing detection difficulty by $20\\%$.\n\nThe problem states that controls act independently and multiplicatively. Therefore, the two reductions in occurrence are combined.\n$S_{3, \\text{post}} = S_{3, \\text{pre}} = 9$\n$O_{3, \\text{post}} = O_{3, \\text{pre}} \\times (1 - \\frac{20}{100}) \\times (1 - \\frac{35}{100}) = 2 \\times (0.8) \\times (0.65) = 2 \\times 0.52 = 1.04$\n$D_{3, \\text{post}} = D_{3, \\text{pre}} \\times (1 - \\frac{20}{100}) = 7 \\times (1 - 0.2) = 7 \\times 0.8 = 5.6$\n\nThe post-control RPN for $H_3$ is:\n$RPN_{3, \\text{post}} = S_{3, \\text{post}} \\times O_{3, \\text{post}} \\times D_{3, \\text{post}} = 9 \\times 1.04 \\times 5.6 = 52.416$\n\n**Overall Post-Control Risk Score**\n\nThe overall score, $R_{\\text{overall}}$, is the exposure-weighted average of the individual post-control RPNs. The weights are given as $w_1 = 0.55$, $w_2 = 0.25$, and $w_3 = 0.20$.\n$$R_{\\text{overall}} = w_1 \\cdot RPN_{1, \\text{post}} + w_2 \\cdot RPN_{2, \\text{post}} + w_3 \\cdot RPN_{3, \\text{post}}$$\nSubstituting the calculated values:\n$$R_{\\text{overall}} = (0.55 \\times 58.8) + (0.25 \\times 28.35) + (0.20 \\times 52.416)$$\n$$R_{\\text{overall}} = 32.34 + 7.0875 + 10.4832$$\n$$R_{\\text{overall}} = 49.9107$$\n\nThe problem requires the final answer to be rounded to four significant figures.\nThe number $49.9107$ rounded to four significant figures is $49.91$.", "answer": "$$\\boxed{49.91}$$", "id": "5002857"}, {"introduction": "For many investigational devices, particularly sterile or single-use products, demonstrating stability over time is a key safety requirement. This practice [@problem_id:5002834] explores the use of accelerated aging studies to establish an interim shelf-life, a crucial component of the preclinical data package in an IDE. By applying the temperature coefficient ($Q_{10}$) model, you will learn to translate data from high-temperature testing into an equivalent real-time duration, providing the necessary evidence to support the use of the device throughout a clinical investigation.", "problem": "A sponsor is preparing an Investigational Device Exemption (IDE) application for a sterile, single-use polymeric catheter system intended for early feasibility clinical use. To support a temporary shelf-life claim while real-time aging studies proceed, the sponsor has generated accelerated aging data at an elevated temperature of $55^{\\circ}\\mathrm{C}$ for $6$ weeks, using a temperature coefficient ($Q_{10}$) of $2$. The intended real-time storage temperature is $25^{\\circ}\\mathrm{C}$.\n\nStarting only from the core kinetic principle that the temperature coefficient $Q_{10}$ is defined as the factor by which a temperature-dependent degradation rate changes for each $10^{\\circ}\\mathrm{C}$ increment, and that equivalent aging is achieved when the product of a degradation rate and time is conserved across conditions, derive the acceleration factor between $55^{\\circ}\\mathrm{C}$ and $25^{\\circ}\\mathrm{C}$ and compute the equivalent real-time duration at $25^{\\circ}\\mathrm{C}$ corresponding to $6$ weeks at $55^{\\circ}\\mathrm{C}$. Round your final answer to $3$ significant figures. Express the final duration in weeks.\n\nThen, based on fundamental principles of kinetic equivalence, risk management, and the stated purpose of an Investigational Device Exemption (IDE), briefly justify which sections of the IDE would appropriately include these interim accelerated aging results and why they are acceptable as supportive evidence while real-time aging continues. Your justification should rely on core definitions and widely accepted regulatory expectations, not on organization-specific practices.", "solution": "The problem is evaluated in two parts: first, a quantitative derivation and calculation related to accelerated aging kinetics, and second, a qualitative justification based on regulatory science principles for an Investigational Device Exemption (IDE) application.\n\n**Part 1: Derivation and Calculation of Equivalent Real-Time Duration**\n\nThe problem requires the derivation to start from fundamental principles. Let $k$ represent the rate of the degradation reaction that determines the device's shelf-life. This rate is temperature-dependent. Let $T_{AA}$ be the accelerated aging temperature and $T_{RT}$ be the real-time storage temperature. The corresponding degradation rates are $k_{AA}$ and $k_{RT}$, respectively.\n\nThe temperature coefficient, $Q_{10}$, is defined as the factor by which the reaction rate increases for every $10^{\\circ}\\mathrm{C}$ rise in temperature. The relationship between the rates at two different temperatures, $T_{AA}$ and $T_{RT}$, can be expressed as a function of $Q_{10}$ and the temperature difference, $\\Delta T = T_{AA} - T_{RT}$. For each interval of $10^{\\circ}\\mathrm{C}$, the rate multiplies by $Q_{10}$. The number of such intervals is $\\frac{\\Delta T}{10^{\\circ}\\mathrm{C}}$. Therefore, the ratio of the rates is:\n$$ \\frac{k_{AA}}{k_{RT}} = (Q_{10})^{\\frac{T_{AA} - T_{RT}}{10^{\\circ}\\mathrm{C}}} $$\nThis ratio, $\\frac{k_{AA}}{k_{RT}}$, is defined as the Accelerated Aging Factor (AAF).\n\nThe principle of equivalent aging states that the total amount of degradation is conserved between the accelerated and real-time conditions. The total degradation can be modeled as the product of the degradation rate and the duration of exposure, assuming a zero-order or pseudo-zero-order reaction, which is a common simplification in this context. Thus, we have:\n$$ \\text{Total Degradation}_{AA} = \\text{Total Degradation}_{RT} $$\n$$ k_{AA} \\times t_{AA} = k_{RT} \\times t_{RT} $$\nwhere $t_{AA}$ is the duration of the accelerated aging study and $t_{RT}$ is the equivalent duration at the real-time storage temperature.\n\nRearranging this equation to solve for $t_{RT}$ gives:\n$$ t_{RT} = \\frac{k_{AA}}{k_{RT}} \\times t_{AA} $$\nSubstituting the expression for the rate ratio (the AAF), we get the general formula:\n$$ t_{RT} = (Q_{10})^{\\frac{T_{AA} - T_{RT}}{10^{\\circ}\\mathrm{C}}} \\times t_{AA} $$\n\nNow, we substitute the given values into this derived formula:\n- Accelerated aging temperature, $T_{AA} = 55^{\\circ}\\mathrm{C}$\n- Real-time storage temperature, $T_{RT} = 25^{\\circ}\\mathrm{C}$\n- Temperature coefficient, $Q_{10} = 2$\n- Accelerated aging duration, $t_{AA} = 6 \\text{ weeks}$\n\nFirst, we calculate the temperature difference:\n$$ \\Delta T = T_{AA} - T_{RT} = 55^{\\circ}\\mathrm{C} - 25^{\\circ}\\mathrm{C} = 30^{\\circ}\\mathrm{C} $$\n\nNext, we calculate the Accelerated Aging Factor (AAF):\n$$ \\text{AAF} = (Q_{10})^{\\frac{\\Delta T}{10^{\\circ}\\mathrm{C}}} = 2^{\\frac{30}{10}} = 2^3 = 8 $$\nThe acceleration factor is $8$. This means the degradation process is predicted to occur $8$ times faster at $55^{\\circ}\\mathrm{C}$ than at $25^{\\circ}\\mathrm{C}$.\n\nFinally, we compute the equivalent real-time duration, $t_{RT}$:\n$$ t_{RT} = \\text{AAF} \\times t_{AA} = 8 \\times 6 \\text{ weeks} = 48 \\text{ weeks} $$\nThe problem requires the answer to be rounded to $3$ significant figures. The calculated value is exactly $48$, which expressed to three significant figures is $48.0$.\n\n**Part 2: Justification for Inclusion in an IDE Application**\n\nBased on fundamental principles, the interim shelf-life data derived from this accelerated aging study would be appropriately included in several sections of the Investigational Device Exemption (IDE) application. The justification for its acceptability rests on the purpose of an IDE and a risk-based approach.\n\n1.  **IDE Sections for Inclusion:**\n    *   **Section on Preclinical Data / Bench Testing (e.g., as specified in 21 CFR 812.27):** This is the primary location for the full study report. This section must contain all laboratory data used to support the safety of the device for investigational use. The protocol, results, analysis (including the $Q_{10}$ calculation), and conclusions of the accelerated aging study provide critical evidence of material and performance stability.\n    *   **Section on Device Description:** This section describes the device's characteristics, including its materials and performance specifications. The temporary shelf-life is a key specification, and this section would state the claimed shelf-life (e.g., \"$48$ weeks\") and reference the supporting data in the preclinical section.\n    *   **Section on Risk Analysis / Risk Management:** Shelf-life failure represents a significant risk to patient safety (e.g., loss of sterility leading to infection, or material degradation leading to device failure). The risk analysis must identify these hazards. The accelerated aging study serves as a risk control measure, providing evidence that these risks are acceptably mitigated for the duration of the clinical trial.\n    *   **Section on Labeling:** The IDE submission must include proposed labeling for the investigational device, which includes the expiration date. The expiration date on the label is directly derived from and justified by the shelf-life study.\n\n2.  **Justification for Acceptability:**\n    *   **Purpose of an IDE:** The objective of an IDE is to permit a clinical investigation to proceed to collect safety and effectiveness data (21 CFR 812). It is not a marketing application (like a PMA or 510(k)), which requires definitive, long-term evidence. The standard for an IDE is to demonstrate that the device is sufficiently safe to be used in a limited, controlled human study.\n    *   **Early Feasibility Study Context:** For an early feasibility study, it is understood that the device is in an early phase of development. Expecting multi-year, real-time aging data would be prohibitive and would stifle innovation. Regulatory bodies typically accept scientifically-sound predictive data to enable these crucial early studies to begin.\n    *   **Scientifically-Based Prediction:** The $Q_{10}$ method, while a simplification of the more complex Arrhenius model, is a widely accepted and scientifically-grounded convention for estimating shelf-life for temperature-dependent degradation of polymers (per standards like ASTM F1980). It provides a rational, evidence-based prediction of stability.\n    *   **Risk Mitigation Strategy:** Accepting accelerated data is a risk-based decision. For a sterile, single-use device in a controlled clinical trial, the risk associated with an expiration date based on a well-conducted accelerated study is generally considered acceptable, with the explicit understanding that this is an interim claim. The sponsor is required to conduct a real-time aging study in parallel. The accelerated data bridges the gap, allowing the clinical trial to proceed while the definitive real-time data is being generated. The real-time results will ultimately confirm or modify the final shelf-life claim for the commercial product.\n\nIn summary, accelerated aging data is acceptable as supportive evidence in an IDE because it provides a reasonable, scientifically-based assurance of device stability and safety for the limited scope and duration of an initial clinical trial, consistent with the risk-based framework of medical device regulation.", "answer": "$$\\boxed{48.0}$$", "id": "5002834"}, {"introduction": "The scientific and ethical integrity of a clinical investigation rests on its ability to answer the research question, which requires a statistically appropriate number of participants. An inadequately sized study wastes resources and needlessly exposes subjects to risk. This problem [@problem_id:5002870] challenges you to derive the sample size for a clinical trial from first principles, connecting the concepts of statistical power ($1-\\beta$), significance level ($\\alpha$), effect size ($\\delta$), and variance ($\\sigma^2$). Mastering this fundamental calculation is essential for writing the clinical study protocol section of an IDE and justifying that the investigation is designed for success.", "problem": "A sponsor is preparing the clinical study section of an Investigational Device Exemption (IDE) application for an implantable device intended to reduce systolic blood pressure. The primary effectiveness endpoint is the between-arm difference in mean systolic pressure, specified as a reduction of $\\delta = 10$ mmHg in the investigational arm relative to the control arm at a prespecified time point. Based on prior observational data, the common Standard Deviation (SD) of systolic pressure within arms is anticipated to be $\\sigma = 15$ mmHg. The study will randomize participants in equal numbers to the investigational and control arms, yielding balanced samples with $n$ participants per arm.\n\nTo justify the sample size in the IDE application, derive the minimal continuous sample size per arm required to detect the mean difference $\\delta$ using a two-sample Studentâ€™s $t$-test under the following design parameters: two-sided significance level $\\alpha = 0.05$ and power $1 - \\beta = 0.80$. Start from first principles by using the definitions of Type I error ($\\alpha$) and Type II error ($\\beta$), the large-sample normal approximation to the sampling distribution of the difference in sample means under the null and alternative hypotheses, and construct the rejection criterion that achieves the specified $\\alpha$ and $1-\\beta$. Then, compute the numerical value of the continuous $n$ per arm using standard normal quantiles.\n\nReport the final continuous $n$ per arm to four significant figures. Express the final value as a pure number without units.", "solution": "The problem is to derive the formula for the minimal continuous sample size per arm, $n$, required for a two-arm clinical trial to detect a specified treatment effect, and then to calculate its numerical value.\n\nLet $\\mu_I$ and $\\mu_C$ represent the true mean systolic blood pressure in the investigational and control populations, respectively. The study is designed to detect a mean difference of $\\delta = 10 \\text{ mmHg}$, where the investigational arm is expected to show a reduction. We define the difference as $d = \\mu_I - \\mu_C$. The specific alternative hypothesis of interest is that $d = -\\delta = -10 \\text{ mmHg}$.\n\nThe statistical framework is a two-sided hypothesis test with the following hypotheses:\nThe null hypothesis, $H_0$, states that there is no difference between the arms:\n$$H_0: \\mu_I - \\mu_C = 0$$\nThe alternative hypothesis, $H_A$, states that there is a non-zero difference:\n$$H_A: \\mu_I - \\mu_C \\neq 0$$\n\nLet $\\bar{X}_I$ and $\\bar{X}_C$ be the sample mean systolic blood pressures for the investigational and control arms, respectively, each with sample size $n$. The point estimator for the difference in population means is $\\hat{d} = \\bar{X}_I - \\bar{X}_C$. Given that the two samples are independent and share a common population standard deviation $\\sigma$, the variance of the estimator $\\hat{d}$ is:\n$$ \\text{Var}(\\hat{d}) = \\text{Var}(\\bar{X}_I) + \\text{Var}(\\bar{X}_C) = \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{n} = \\frac{2\\sigma^2}{n} $$\nThe standard error of the difference in sample means is $\\text{SE}_{\\hat{d}} = \\sqrt{\\frac{2\\sigma^2}{n}}$.\n\nThe problem asks for the use of a large-sample normal approximation. Under the null hypothesis $H_0$, the sampling distribution of $\\hat{d}$ is approximately normal with a mean of $0$ and a variance of $\\frac{2\\sigma^2}{n}$. We can write this as:\n$$ \\hat{d} | H_0 \\sim N\\left(0, \\frac{2\\sigma^2}{n}\\right) $$\n\nThe Type I error rate, $\\alpha$, is the probability of rejecting $H_0$ when it is true. For a two-sided test with significance level $\\alpha$, we reject $H_0$ if the observed difference $\\hat{d}$ is sufficiently far from $0$. The standardized test statistic under $H_0$ is $Z = \\frac{\\hat{d} - 0}{\\text{SE}_{\\hat{d}}}$. We reject $H_0$ if $|Z| > z_{1-\\alpha/2}$, where $z_{1-\\alpha/2}$ is the upper $(1-\\alpha/2)$-quantile of the standard normal distribution. This defines the rejection region in terms of $\\hat{d}$: we reject $H_0$ if $|\\hat{d}| > C$, where the critical value $C$ is given by:\n$$ C = z_{1-\\alpha/2} \\times \\text{SE}_{\\hat{d}} = z_{1-\\alpha/2} \\sqrt{\\frac{2\\sigma^2}{n}} $$\n\nThe power of the study, $1-\\beta$, is the probability of correctly rejecting $H_0$ when a specific alternative hypothesis is true. We are interested in the case where the true difference is $d_A = -\\delta$. Under this alternative, the sampling distribution of $\\hat{d}$ is:\n$$ \\hat{d} | H_A \\sim N\\left(-\\delta, \\frac{2\\sigma^2}{n}\\right) $$\n\nPower is the probability that $\\hat{d}$ falls into the rejection region ($|\\hat{d}| > C$) given that $H_A$ is true:\n$$ \\text{Power} = 1 - \\beta = P(\\hat{d} < -C \\text{ or } \\hat{d} > C \\enskip | \\enskip d = -\\delta) $$\nSince the distribution of $\\hat{d}$ under this alternative is centered at a negative value ($-\\delta$), the probability of $\\hat{d}$ exceeding the positive critical value $C$ is negligible. Thus, we can approximate the power by considering only the lower tail of the rejection region:\n$$ 1 - \\beta \\approx P(\\hat{d} < -C \\enskip | \\enskip d = -\\delta) $$\nTo evaluate this probability, we standardize $\\hat{d}$ with respect to its distribution under $H_A$. Let $Z'$ be a standard normal random variable.\n$$ 1 - \\beta = P\\left( \\frac{\\hat{d} - (-\\delta)}{\\text{SE}_{\\hat{d}}} < \\frac{-C - (-\\delta)}{\\text{SE}_{\\hat{d}}} \\right) = P\\left( Z' < \\frac{\\delta - C}{\\text{SE}_{\\hat{d}}} \\right) $$\nFrom the definition of standard normal quantiles, this implies:\n$$ \\frac{\\delta - C}{\\text{SE}_{\\hat{d}}} = z_{1-\\beta} $$\nWe now have a system of two equations:\n1. $ C = z_{1-\\alpha/2} \\times \\text{SE}_{\\hat{d}} $\n2. $ \\delta - C = z_{1-\\beta} \\times \\text{SE}_{\\hat{d}} $\n\nSubstituting the first equation into the second gives:\n$$ \\delta - (z_{1-\\alpha/2} \\times \\text{SE}_{\\hat{d}}) = z_{1-\\beta} \\times \\text{SE}_{\\hat{d}} $$\nSolving for $\\delta$:\n$$ \\delta = (z_{1-\\alpha/2} + z_{1-\\beta}) \\times \\text{SE}_{\\hat{d}} $$\nSubstituting the expression for the standard error, $\\text{SE}_{\\hat{d}} = \\sqrt{\\frac{2\\sigma^2}{n}}$:\n$$ \\delta = (z_{1-\\alpha/2} + z_{1-\\beta}) \\sqrt{\\frac{2\\sigma^2}{n}} $$\nFinally, we solve for the sample size per arm, $n$:\n$$ \\sqrt{n} = \\frac{(z_{1-\\alpha/2} + z_{1-\\beta}) \\sqrt{2\\sigma^2}}{\\delta} $$\nSquaring both sides yields the derived formula:\n$$ n = \\frac{2\\sigma^2(z_{1-\\alpha/2} + z_{1-\\beta})^2}{\\delta^2} $$\nWe are given:\n- Effect size $\\delta = 10 \\text{ mmHg}$\n- Standard deviation $\\sigma = 15 \\text{ mmHg}$\n- Significance level $\\alpha = 0.05$ (two-sided)\n- Power $1-\\beta = 0.80$, so $\\beta = 0.20$\n\nThe required quantiles from the standard normal distribution are:\n- $z_{1-\\alpha/2} = z_{1-0.05/2} = z_{0.975} \\approx 1.959964$\n- $z_{1-\\beta} = z_{1-0.20} = z_{0.80} \\approx 0.841621$\n\nSubstituting these values into the formula for $n$:\n$$ n = \\frac{2(15)^2(1.959964 + 0.841621)^2}{(10)^2} $$\n$$ n = \\frac{2(225)(2.801585)^2}{100} $$\n$$ n = \\frac{450(7.848879...)}{100} $$\n$$ n = 4.5 \\times 7.848879... $$\n$$ n \\approx 35.319955 $$\nRounding to four significant figures, the continuous sample size per arm is $35.32$.", "answer": "$$\\boxed{35.32}$$", "id": "5002870"}]}