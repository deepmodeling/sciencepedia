{"hands_on_practices": [{"introduction": "Metagenomic sequencing of clinical samples often faces the challenge of an overwhelming host nucleic acid background. This exercise [@problem_id:4664101] explores this fundamental \"signal-to-noise\" problem. By applying basic principles of probabilistic sampling, you will derive a quantitative relationship between the host DNA fraction, $f_{h}$, and the total sequencing depth, $D$, required to detect a rare pathogen, building a critical skill in experimental design and resource planning for surveillance projects.", "problem": "A public health laboratory uses unbiased metagenomic next-generation sequencing (Next-Generation Sequencing (NGS)) to screen clinical samples for a low-abundance pathogen. Suppose each sequenced read independently originates from either host or microbial nucleic acid present in the sample. Let the host background nucleic acid fraction be $f_{h} \\in (0,1)$, so that the expected fraction of non-host (microbial) nucleic acid is $(1 - f_{h})$. Within the microbial component, let the pathogen of interest constitute a fixed fraction $q \\in (0,1)$ of microbial nucleic acid. The laboratory defines its detection limit by requiring that the probability of observing at least one read from the pathogen in a sequencing run is at least a target threshold $p^{\\ast} \\in (0,1)$; the total number of reads generated in the run is $D \\in \\mathbb{N}$.\n\nStarting from fundamental probabilistic sampling principles for independent reads, derive an expression for how the host background fraction $f_{h}$ influences the effective depth available for microbial reads and the detection limit, using only well-tested approximations valid in metagenomic sampling of rare targets. Then, under the same detection limit $p^{\\ast}$ and fixed pathogen fraction $q$, compute the fold-increase in the required total sequencing depth when the host fraction doubles from $f_{h}$ to $2 f_{h}$, assuming $2 f_{h} < 1$ so that a nonzero microbial fraction remains. Express your final answer as a single simplified analytic expression. No numerical rounding is required, and the final answer is dimensionless.", "solution": "Let $f_{h}$ be the fraction of sequencing reads originating from the host, where $f_{h} \\in (0,1)$. The fraction of reads from the microbial community is therefore $(1 - f_{h})$. Within this microbial fraction, the pathogen of interest constitutes a fraction $q \\in (0,1)$.\n\nThe probability, $p_{pathogen}$, that a single, randomly selected read originates from the pathogen is the product of the probability that the read is microbial and the conditional probability that it is from the pathogen, given it is microbial.\n$$p_{pathogen} = (1 - f_{h}) q$$\nThe total number of reads is $D$. We assume each read is an independent Bernoulli trial, where a \"success\" is observing a read from the pathogen, with probability $p_{pathogen}$. Let $K$ be the random variable representing the number of pathogen reads observed in a total of $D$ reads. $K$ follows a binomial distribution:\n$$K \\sim \\text{Binomial}(D, p_{pathogen})$$\nThe probability of observing exactly $k$ pathogen reads is given by:\n$$P(K=k) = \\binom{D}{k} (p_{pathogen})^{k} (1 - p_{pathogen})^{D-k}$$\nThe detection limit is defined by the condition that the probability of observing at least one pathogen read is at least $p^{\\ast}$.\n$$P(K \\ge 1) \\ge p^{\\ast}$$\nThis is equivalent to:\n$$1 - P(K=0) \\ge p^{\\ast}$$\nFrom the binomial probability mass function, the probability of observing zero pathogen reads is:\n$$P(K=0) = \\binom{D}{0} (p_{pathogen})^{0} (1 - p_{pathogen})^{D-0} = (1 - p_{pathogen})^{D}$$\nSubstituting this into the detection condition gives:\n$$1 - (1 - (1-f_{h})q)^{D} \\ge p^{\\ast}$$\nThe problem states that the pathogen is a \"low-abundance\" or \"rare\" target, and in next-generation sequencing, the total number of reads $D$ is typically very large. In this regime (large $D$, small $p_{pathogen}$), the binomial distribution is well approximated by a Poisson distribution with rate parameter $\\lambda$, where $\\lambda$ is the expected number of successes.\n$$\\lambda = D \\cdot p_{pathogen} = Dq(1 - f_{h})$$\nUsing the Poisson approximation, the probability of observing $k$ pathogen reads is:\n$$P(K=k) \\approx \\frac{\\lambda^{k} \\exp(-\\lambda)}{k!}$$\nThe probability of observing zero pathogen reads simplifies to:\n$$P(K=0) \\approx \\exp(-\\lambda)$$\nThe detection condition $P(K \\ge 1) \\ge p^{\\ast}$ can now be expressed using this approximation:\n$$1 - \\exp(-\\lambda) \\ge p^{\\ast}$$\nRearranging this inequality to solve for the required rate parameter $\\lambda$:\n$$\\exp(-\\lambda) \\le 1 - p^{\\ast}$$\nTaking the natural logarithm of both sides:\n$$-\\lambda \\le \\ln(1 - p^{\\ast})$$\nMultiplying by $-1$ reverses the inequality:\n$$\\lambda \\ge -\\ln(1 - p^{\\ast})$$\nThe term $-\\ln(1 - p^{\\ast})$ is a positive constant determined by the desired detection probability threshold $p^{\\ast}$. Let us define this minimum required expected number of reads as $\\lambda_{min} = -\\ln(1 - p^{\\ast})$. The condition for detection is $\\lambda \\ge \\lambda_{min}$.\nSubstituting the expression for $\\lambda$:\n$$Dq(1 - f_{h}) \\ge \\lambda_{min}$$\nThis expression shows how the host fraction $f_{h}$ influences the system. The term $D(1 - f_{h})$ can be viewed as the effective sequencing depth for the microbial fraction. To maintain the same detection sensitivity (i.e., the same $\\lambda_{min}$) with a fixed pathogen fraction $q$, an increase in the host fraction $f_{h}$ must be compensated by an increase in the total sequencing depth $D$.\n\nWe are asked to compute the fold-increase in the required total sequencing depth when the host fraction doubles from $f_{h}$ to $2f_{h}$, assuming the detection limit $p^{\\ast}$ and pathogen fraction $q$ remain fixed. This implies that the required expected number of pathogen reads, $\\lambda_{min}$, is the same in both scenarios.\n\nLet $D_1$ be the required sequencing depth for a host fraction of $f_{h}$. At the detection limit, the condition is met with equality:\n$$D_{1}q(1 - f_{h}) = \\lambda_{min}$$\nSolving for $D_1$:\n$$D_1 = \\frac{\\lambda_{min}}{q(1 - f_{h})}$$\nLet $D_2$ be the required a sequencing depth for a host fraction of $2f_{h}$. The problem states $2f_{h} < 1$. The corresponding equation is:\n$$D_{2}q(1 - 2f_{h}) = \\lambda_{min}$$\nSolving for $D_2$:\n$$D_2 = \\frac{\\lambda_{min}}{q(1 - 2f_{h})}$$\nThe fold-increase in the required total sequencing depth is the ratio $\\frac{D_2}{D_1}$.\n$$\\frac{D_2}{D_1} = \\frac{\\frac{\\lambda_{min}}{q(1 - 2f_{h})}}{\\frac{\\lambda_{min}}{q(1 - f_{h})}}$$\nThe terms $\\lambda_{min}$ and $q$ are common to both the numerator and the denominator and thus cancel out.\n$$\\frac{D_2}{D_1} = \\frac{\\frac{1}{1 - 2f_{h}}}{\\frac{1}{1 - f_{h}}} = \\frac{1 - f_{h}}{1 - 2f_{h}}$$\nThis expression represents the required fold-increase in sequencing depth. It is a dimensionless quantity as required.", "answer": "$$\\boxed{\\frac{1 - f_{h}}{1 - 2f_{h}}}$$", "id": "4664101"}, {"introduction": "While metagenomics naturally yields relative abundances, many public health applications require absolute quantification of pathogen load. This practice [@problem_id:4664147] delves into the use of internal spike-in controls, a cornerstone technique for achieving absolute measurements. You will model the entire process, from read counts to concentration, and importantly, learn to account for experimental uncertainty by modeling the relative extraction efficiency, $W$, as a lognormal random variable, a key skill for generating robust and reliable quantitative data.", "problem": "A public health laboratory processes a wastewater sample to quantify the absolute concentration of a target pathogen using internal spike-in controls and metagenomic sequencing. A volume of $50$ mL of wastewater is subjected to nucleic acid extraction. Immediately prior to extraction, a known quantity of synthetic spike-in molecules equal to $1.00 \\times 10^{8}$ genome copies is added to the extraction tube. After extraction and library preparation, high-depth metagenomic sequencing is performed. Reads are mapped with stringent criteria and normalized to effective template length so that, conditional on extraction and library preparation, the expected proportion of reads mapping to any template is proportional to the number of its molecules entering the library. The mapped read counts are $2.50 \\times 10^{5}$ reads for the spike-in and $7.50 \\times 10^{4}$ reads for the target pathogen.\n\nAssume that sequencing sampling variability is negligible at this depth relative to extraction variability. Let the extraction efficiencies for the target and the spike-in be $e_{\\mathrm{t}}$ and $e_{\\mathrm{s}}$, respectively, and suppose that the relative extraction efficiency $W = e_{\\mathrm{t}}/e_{\\mathrm{s}}$ is a lognormal random variable with median $1$ and geometric standard deviation $1.6$.\n\nStarting only from the following foundational principles: (i) read proportions after length normalization are proportional to input molecule counts into the library, (ii) absolute calibration arises by comparing the target to a spike-in of known input abundance, and (iii) the lognormal distributionâ€™s definition in terms of the normal distribution, derive an expression for the posterior mean absolute concentration of the target pathogen in the original wastewater, in genome copies per mL, as a function of the observed read counts, the known spike-in amount, the processed volume, and the parameters of the lognormal model for $W$. Then evaluate this expression numerically for the data above.\n\nExpress the final concentration in copies per mL. Round your answer to four significant figures.", "solution": "#### Derivation of the Posterior Mean Concentration\n\nLet $C_{\\mathrm{t}}$ be the absolute concentration of the target pathogen in the wastewater, in genome copies per mL. The total number of target pathogen genomes in the processed volume $V$ is $N_{\\mathrm{t}} = C_{\\mathrm{t}} V$. The number of spike-in genomes added is $N_{\\mathrm{s}}$.\n\nThe extraction process recovers a fraction of these genomes. The number of target and spike-in molecules entering the library preparation step are $N'_{\\mathrm{t}} = e_{\\mathrm{t}} N_{\\mathrm{t}}$ and $N'_{\\mathrm{s}} = e_{\\mathrm{s}} N_{\\mathrm{s}}$, respectively, where $e_{\\mathrm{t}}$ and $e_{\\mathrm{s}}$ are the extraction efficiencies.\n\nAccording to foundational principle (i), after normalization, the expected proportion of reads is proportional to the number of input molecules. This implies that the ratio of expected read counts is equal to the ratio of the number of molecules post-extraction:\n$$\n\\frac{E[R_{\\mathrm{t}}]}{E[R_{\\mathrm{s}}]} = \\frac{N'_{\\mathrm{t}}}{N'_{\\mathrm{s}}} = \\frac{e_{\\mathrm{t}} N_{\\mathrm{t}}}{e_{\\mathrm{s}} N_{\\mathrm{s}}}\n$$\nThe problem states that sequencing sampling variability is negligible. This allows us to equate the observed read ratio to the ratio of expected read counts:\n$$\n\\frac{R_{\\mathrm{t}}}{R_{\\mathrm{s}}} \\approx \\frac{E[R_{\\mathrm{t}}]}{E[R_{\\mathrm{s}}]}\n$$\nCombining these, and using foundational principle (ii) for calibration, we have:\n$$\n\\frac{R_{\\mathrm{t}}}{R_{\\mathrm{s}}} = \\frac{e_{\\mathrm{t}} N_{\\mathrm{t}}}{e_{\\mathrm{s}} N_{\\mathrm{s}}}\n$$\nWe define the relative extraction efficiency as $W = e_{\\mathrm{t}}/e_{\\mathrm{s}}$. Substituting this into the equation gives:\n$$\n\\frac{R_{\\mathrm{t}}}{R_{\\mathrm{s}}} = W \\frac{N_{\\mathrm{t}}}{N_{\\mathrm{s}}}\n$$\nWe wish to find the concentration $C_{\\mathrm{t}} = N_{\\mathrm{t}}/V$. We first solve for $N_{\\mathrm{t}}$:\n$$\nN_{\\mathrm{t}} = N_{\\mathrm{s}} \\frac{R_{\\mathrm{t}}}{R_{\\mathrm{s}}} \\frac{1}{W}\n$$\nThe concentration $C_{\\mathrm{t}}$ is therefore given by:\n$$\nC_{\\mathrm{t}} = \\frac{N_{\\mathrm{t}}}{V} = \\frac{N_{\\mathrm{s}} R_{\\mathrm{t}}}{V R_{\\mathrm{s}}} \\frac{1}{W}\n$$\nIn this expression, all terms except $W$ are known constants from the experiment. The quantity $W$ is a random variable. The problem asks for the \"posterior mean\" of $C_{\\mathrm{t}}$. The assumption of negligible sampling variability implies that the data ($R_{\\mathrm{t}}$, $R_{\\mathrm{s}}$) provide no information to update the prior distribution of $W$. Thus, the posterior distribution of $W$ is identical to its prior distribution. The \"posterior mean\" is the expectation of $C_{\\mathrm{t}}$ with respect to this distribution.\n$$\nE[C_{\\mathrm{t}}] = E\\left[ \\frac{N_{\\mathrm{s}} R_{\\mathrm{t}}}{V R_{\\mathrms}} \\frac{1}{W} \\right] = \\frac{N_{\\mathrm{s}} R_{\\mathrm{t}}}{V R_{\\mathrm{s}}} E\\left[\\frac{1}{W}\\right]\n$$\nNow, we must find the expectation of $1/W$. We are given that $W$ follows a lognormal distribution. According to foundational principle (iii), if $W \\sim \\text{Lognormal}(\\mu, \\sigma^2)$, then its natural logarithm $Y = \\ln(W)$ follows a normal distribution, $Y \\sim N(\\mu, \\sigma^2)$.\n\nWe need to relate the parameters $\\mu$ and $\\sigma$ to the given median and geometric standard deviation (GSD).\nThe median of a lognormal distribution is $\\exp(\\mu)$. We are given the median is $1$, so:\n$$\n\\exp(\\mu) = 1 \\implies \\mu = \\ln(1) = 0\n$$\nThe GSD of a lognormal distribution is $\\exp(\\sigma)$. We are given the GSD is $1.6$, so:\n$$\n\\exp(\\sigma) = 1.6 \\implies \\sigma = \\ln(1.6)\n$$\nThus, $\\ln(W) \\sim N(0, (\\ln(1.6))^2)$.\n\nThe expectation $E[1/W]$ can be calculated as follows:\n$$\nE\\left[\\frac{1}{W}\\right] = E[W^{-1}] = E[\\exp(-\\ln(W))] = E[\\exp(-Y)]\n$$\nThis is the moment-generating function (MGF) of the normal random variable $Y$, $M_Y(t) = E[\\exp(tY)]$, evaluated at $t=-1$. The MGF for $Y \\sim N(\\mu, \\sigma^2)$ is $M_Y(t) = \\exp(\\mu t + \\frac{1}{2}\\sigma^2 t^2)$.\nSubstituting $t=-1$:\n$$\nE\\left[\\frac{1}{W}\\right] = M_Y(-1) = \\exp(\\mu(-1) + \\frac{1}{2}\\sigma^2(-1)^2) = \\exp(-\\mu + \\frac{1}{2}\\sigma^2)\n$$\nSubstituting our derived values for $\\mu$ and $\\sigma$:\n$$\nE\\left[\\frac{1}{W}\\right] = \\exp(-0 + \\frac{1}{2}(\\ln(1.6))^2) = \\exp\\left(\\frac{1}{2}(\\ln(1.6))^2\\right)\n$$\nThe expression for the posterior mean absolute concentration is therefore:\n$$\nE[C_{\\mathrm{t}}] = \\frac{N_{\\mathrm{s}} R_{\\mathrm{t}}}{V R_{\\mathrm{s}}} \\exp\\left(-\\mu + \\frac{1}{2}\\sigma^2\\right)\n$$\nwhere $\\mu$ is the log of the median of $W$ and $\\sigma$ is the log of the geometric standard deviation of $W$.\n\n#### Numerical Evaluation\n\nWe now substitute the given values into the derived expression.\n-   $N_{\\mathrm{s}} = 1.00 \\times 10^{8}$\n-   $V = 50$ mL\n-   $R_{\\mathrm{s}} = 2.50 \\times 10^{5}$\n-   $R_{\\mathrm{t}} = 7.50 \\times 10^{4}$\n-   Median of $W = 1 \\implies \\mu = \\ln(1) = 0$\n-   GSD of $W = 1.6 \\implies \\sigma = \\ln(1.6)$\n\nThe mean concentration is:\n$$\nE[C_{\\mathrm{t}}] = \\frac{1.00 \\times 10^{8}}{50} \\times \\frac{7.50 \\times 10^{4}}{2.50 \\times 10^{5}} \\times \\exp\\left(\\frac{1}{2}(\\ln(1.6))^2\\right)\n$$\nFirst, let's evaluate the constant pre-factor:\n$$\n\\frac{N_{\\mathrm{s}}}{V} \\times \\frac{R_{\\mathrm{t}}}{R_{\\mathrm{s}}} = \\frac{1.00 \\times 10^{8} \\text{ copies}}{50 \\text{ mL}} \\times \\frac{7.50 \\times 10^{4}}{2.50 \\times 10^{5}} = (2.00 \\times 10^6 \\text{ mL}^{-1}) \\times 0.3 = 6.00 \\times 10^{5} \\text{ mL}^{-1}\n$$\nNext, let's evaluate the expectation term:\n$$\n\\exp\\left(\\frac{1}{2}(\\ln(1.6))^2\\right) \\approx \\exp\\left(\\frac{1}{2}(0.4700036)^2\\right) \\approx \\exp\\left(\\frac{1}{2}(0.2209034)\\right) \\approx \\exp(0.1104517) \\approx 1.1167709\n$$\nFinally, we multiply the two parts:\n$$\nE[C_{\\mathrm{t}}] \\approx (6.00 \\times 10^5) \\times 1.1167709 \\approx 670062.56 \\text{ copies/mL}\n$$\nRounding to four significant figures, we get $6.701 \\times 10^5$ copies/mL.", "answer": "$$\n\\boxed{6.701 \\times 10^5}\n$$", "id": "4664147"}, {"introduction": "A primary goal of metagenomic surveillance is to provide actionable early warnings for disease outbreaks. This practice [@problem_id:4664114] places you in the role of a data analyst tasked with quantifying the lead time between a wastewater signal, $S_t$, and reported clinical cases, $C_t$. By implementing a deconvolution model using non-negative least squares, you will estimate the lag distribution, $g_k$, and its key summary statistics, translating raw time-series data into a critical epidemiological parameter. This exercise provides direct, hands-on experience in building the analytical tools that power wastewater-based epidemiology.", "problem": "A public health agency applies Wastewater-Based Epidemiology (WBE) metagenomics to monitor pathogen dynamics. Let the wastewater metagenomic signal time series be $S_t$ and the reported clinical case time series be $C_t$, both indexed by discrete time in days $t \\in \\{0,1,2,\\dots,T-1\\}$. Assume a causal discrete-time linear model linking these two observables through a nonnegative, normalized lag distribution $g_k$ and a positive scaling parameter $\\beta$, with additive deterministic measurement error $e_t$:\n$$\nC_t = \\beta \\sum_{k=0}^{K} g_k S_{t-k} + e_t,\n$$\nwith the convention $S_{t-k} = 0$ whenever $t-k < 0$. The vector $g = (g_0,\\dots,g_K)$ represents the delay distribution from wastewater signal to reported cases and must satisfy $g_k \\ge 0$ for all $k$ and $\\sum_{k=0}^{K} g_k = 1$. The quantity of interest for actionable early warning is the lead time implied by $g$, defined by summary functionals of $g$: the expected lag $\\mathbb{E}[K] = \\sum_{k=0}^{K} k g_k$ (in days), the median lag $\\operatorname{med}(g)$ (in days) defined as the smallest integer $L$ such that $\\sum_{k=0}^{L} g_k \\ge 0.5$, and the actionable lead time at mass level $\\tau$ (in days) defined as $L_{\\tau} = \\min\\{L \\in \\{0,\\dots,K\\} : \\sum_{k=0}^{L} g_k \\ge \\tau\\}$.\n\nStarting from fundamental discrete-time signal assumptions and the standard convolutional observation model above, design a program that, for a given pair $(S_t, C_t)$ and maximum lag $K$, estimates $g$ under the constraints $g_k \\ge 0$ and $\\sum_{k=0}^{K} g_k = 1$, and then reports the three lead-time functionals $\\mathbb{E}[K]$, $\\operatorname{med}(g)$, and $L_{\\tau}$. Your estimator must be grounded in minimizing the mean squared error between modeled and observed cases over time, treating $\\beta$ as a nuisance parameter to be estimated from the data for the fitted $g$. Express all lead times in days as real-valued outputs; report the results rounded to two decimal places.\n\nUse the following test suite of parameterized, scientifically plausible scenarios. In each case, the synthetic $C_t$ is generated deterministically from the specified $S_t$, $g_k$, $\\beta$, and $e_t$ according to the model above, with $e_t$ given by a sinusoidal term of small amplitude to represent structured measurement error.\n\nFor each test case, define $S_t$ by a baseline plus one or two Gaussian pulses:\n$$\nS_t = b + \\sum_{j} A_j \\exp\\left(-\\frac{(t - t_j)^2}{2\\sigma_j^2}\\right),\n$$\nand define $e_t$ by\n$$\ne_t = \\delta \\sin\\left(\\frac{2\\pi t}{p}\\right).\n$$\nEach case specifies $T$, $K$, $\\tau$, $b$, the pulse parameters $(A_j, t_j, \\sigma_j)$, the lag distribution $g_k$, $\\beta$, and $(\\delta, p)$.\n\nTest Case $1$ (unimodal lag with mean near $3$ days):\n- $T = 60$, $K = 7$, $\\tau = 0.8$.\n- $S_t$: $b = 5$, two pulses with $(A_1,t_1,\\sigma_1) = (30,15,3)$ and $(A_2,t_2,\\sigma_2) = (25,40,3)$.\n- $g_k$ for $k \\in \\{0,\\dots,7\\}$ proportional to $k^{\\theta-1} \\exp(-k/\\lambda)$ with $\\theta = 3$ and $\\lambda = 1$, normalized to sum to $1$.\n- $\\beta = 0.05$.\n- $e_t$: $\\delta = 0.5$, $p = 12$.\n\nTest Case $2$ (bimodal lag from dual pathways):\n- $T = 70$, $K = 9$, $\\tau = 0.8$.\n- $S_t$: $b = 10$, two pulses with $(A_1,t_1,\\sigma_1) = (40,20,4)$ and $(A_2,t_2,\\sigma_2) = (35,50,4)$.\n- $g_k$ is a mixture of point masses: $g_2 = 0.6$, $g_6 = 0.4$, and $g_k = 0$ for other $k$.\n- $\\beta = 0.03$.\n- $e_t$: $\\delta = 0.3$, $p = 10$.\n\nTest Case $3$ (near-zero lag alignment):\n- $T = 55$, $K = 5$, $\\tau = 0.8$.\n- $S_t$: $b = 8$, single pulse with $(A_0,t_0,\\sigma_0) = (50,25,5)$.\n- $g_k$ concentrated at $k=0$: $g_0 = 1$, $g_k = 0$ for $k \\in \\{1,\\dots,5\\}$.\n- $\\beta = 0.02$.\n- $e_t$: $\\delta = 0.2$, $p = 11$.\n\nTest Case $4$ (short series with heavy-tailed lag):\n- $T = 25$, $K = 10$, $\\tau = 0.8$.\n- $S_t$: $b = 6$, two pulses with $(A_1,t_1,\\sigma_1) = (18,8,2)$ and $(A_2,t_2,\\sigma_2) = (10,17,2)$.\n- $g_k$ for $k \\in \\{0,\\dots,10\\}$ proportional to $k^{\\theta-1} \\exp(-k/\\lambda)$ with $\\theta = 2$ and $\\lambda = 2$, normalized to sum to $1$.\n- $\\beta = 0.04$.\n- $e_t$: $\\delta = 0.7$, $p = 8$.\n\nAll constants and indices above are in days or unitless as appropriate; when computing $C_t$ via convolution, use $S_{t-k} = 0$ for $t-k < 0$. For estimation, restrict the fitting domain to $t \\in \\{K, K+1, \\dots, T-1\\}$ to ensure full-lag support in the design matrix.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a three-element list $[\\mathbb{E}[K], \\operatorname{med}(g), L_{\\tau}]$, all expressed in days and rounded to two decimal places; for example, in the format $[[x_1,y_1,z_1],[x_2,y_2,z_2],[x_3,y_3,z_3],[x_4,y_4,z_4]]$ with no spaces.", "solution": "### Principle-Based Solution Design\n\nThe problem requires the estimation of a lag distribution, $g$, that links a wastewater metagenomic signal time series, $S_t$, to a clinical case time series, $C_t$. The relationship is described by a discrete-time linear convolutional model:\n$$\nC_t = \\beta \\sum_{k=0}^{K} g_k S_{t-k} + e_t\n$$\nwhere $\\beta$ is a positive scaling parameter, $K$ is the maximum lag, and $e_t$ is an additive error term. The lag distribution $g = (g_0, \\dots, g_K)$ is constrained to be non-negative ($g_k \\ge 0$) and normalized ($\\sum_{k=0}^{K} g_k = 1$). The goal is to estimate $g$ by minimizing the mean squared error (MSE) and then to compute three key functionals of the estimated distribution: the expected lag $\\mathbb{E}[K]$, the median lag $\\operatorname{med}(g)$, and the actionable lead time $L_{\\tau}$.\n\nThe estimation procedure can be formulated as a constrained least squares problem.\n\n#### 1. Model Reparameterization\n\nThe model is non-linear in its parameters $g$ and $\\beta$ taken together, but it can be linearized. Let us define a new set of parameters $\\gamma_k = \\beta g_k$. The model equation becomes:\n$$\nC_t = \\sum_{k=0}^{K} \\gamma_k S_{t-k} + e_t\n$$\nThis form is linear in the parameters $\\gamma = (\\gamma_0, \\dots, \\gamma_K)$. The constraints on $g_k$ and $\\beta$ translate to constraints on $\\gamma_k$:\n1.  Since $\\beta > 0$ and $g_k \\ge 0$, it follows that $\\gamma_k \\ge 0$ for all $k \\in \\{0, \\dots, K\\}$.\n2.  The normalization $\\sum_{k=0}^{K} g_k = 1$ implies $\\sum_{k=0}^{K} \\frac{\\gamma_k}{\\beta} = 1$, which simplifies to $\\sum_{k=0}^{K} \\gamma_k = \\beta$.\n\nThis reparameterization allows us to first estimate $\\gamma$ and then recover $\\beta$ and $g$.\n\n#### 2. Formulation as Non-Negative Least Squares (NNLS)\n\nThe estimation is performed by minimizing the sum of squared errors (SSE), which is equivalent to minimizing the MSE, over the specified time domain $t \\in \\{K, K+1, \\dots, T-1\\}$. This domain ensures that for every $t$, the full history of the signal $S$ up to lag $K$ is available.\n\nLet $\\vec{y}$ be the vector of observed cases over the fitting domain:\n$$\n\\vec{y} = \\begin{pmatrix} C_K \\\\ C_{K+1} \\\\ \\vdots \\\\ C_{T-1} \\end{pmatrix}\n$$\nThis vector has dimension $(T-K) \\times 1$. We construct a design matrix, $A$, of size $(T-K) \\times (K+1)$, where each row corresponds to a time point $t$ and each column corresponds to a lag $k$. The element $A_{i,j}$ (with row index $i$ corresponding to time $t=K+i$ and column index $j$ corresponding to lag $k=j$) is given by the lagged signal value $S_{t-k} = S_{(K+i)-j}$.\n$$\nA = \\begin{pmatrix}\nS_K & S_{K-1} & \\cdots & S_0 \\\\\nS_{K+1} & S_K & \\cdots & S_1 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nS_{T-1} & S_{T-2} & \\cdots & S_{T-1-K}\n\\end{pmatrix}\n$$\nThe system of equations for all time points in the fitting domain can be written in matrix form:\n$$\n\\vec{y} = A\\vec{\\gamma} + \\vec{\\epsilon}\n$$\nwhere $\\vec{\\gamma} = (\\gamma_0, \\dots, \\gamma_K)^T$ is the vector of parameters to be estimated, and $\\vec{\\epsilon}$ is the vector of errors. The objective is to find the vector $\\hat{\\vec{\\gamma}}$ that minimizes the squared Euclidean norm of the residual:\n$$\n\\min_{\\vec{\\gamma}} \\| A\\vec{\\gamma} - \\vec{y} \\|_2^2 \\quad \\text{subject to} \\quad \\vec{\\gamma} \\ge 0\n$$\nThis is a standard Non-Negative Least Squares (NNLS) problem. It is a convex optimization problem with a unique solution, which can be found efficiently using established numerical algorithms, such as the one provided in `scipy.optimize.nnls`.\n\n#### 3. Parameter Recovery and Functional Calculation\n\nOnce the NNLS problem is solved to obtain the optimal estimate $\\hat{\\vec{\\gamma}}$, the original model parameters $\\hat{\\beta}$ and $\\hat{g}$ are recovered as follows:\n$$\n\\hat{\\beta} = \\sum_{k=0}^{K} \\hat{\\gamma}_k\n$$\nIf $\\hat{\\beta} > 0$, the estimated lag distribution $\\hat{g}$ is obtained by normalization:\n$$\n\\hat{g}_k = \\frac{\\hat{\\gamma}_k}{\\hat{\\beta}}\n$$\nThis construction automatically ensures that $\\hat{g}_k \\ge 0$ and $\\sum_{k=0}^{K} \\hat{g}_k = 1$.\n\nWith the estimated distribution $\\hat{g} = (\\hat{g}_0, \\dots, \\hat{g}_K)$, the required lead-time functionals are calculated:\n\n1.  **Expected Lag:** The mean of the lag distribution.\n    $$\n    \\mathbb{E}[K] = \\sum_{k=0}^{K} k \\cdot \\hat{g}_k\n    $$\n\n2.  **Median Lag:** The smallest integer lag $L$ at which the cumulative distribution function (CDF) of $g$ reaches or exceeds $0.5$.\n    $$\n    \\operatorname{med}(g) = \\min\\{L \\in \\{0,\\dots,K\\} : \\sum_{k=0}^{L} \\hat{g}_k \\ge 0.5\\}\n    $$\n\n3.  **Actionable Lead Time at Mass Level $\\tau$:** The smallest integer lag $L$ at which the CDF of $g$ reaches or exceeds the threshold $\\tau$.\n    $$\n    L_{\\tau} = \\min\\{L \\in \\{0,\\dots,K\\} : \\sum_{k=0}^{L} \\hat{g}_k \\ge \\tau\\}\n    $$\n\nThese three values, rounded to two decimal places, provide a comprehensive summary of the lead time from the wastewater signal to clinical case reporting. The overall algorithm proceeds by generating the synthetic data for each test case as specified, applying the NNLS-based estimation procedure, and then calculating the three functionals from the estimated lag distribution.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import nnls\nfrom scipy.special import gamma as gamma_func\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It generates synthetic data, estimates the lag distribution, and computes lead time functionals.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"T\": 60, \"K\": 7, \"tau\": 0.8,\n            \"S_params\": {\"b\": 5, \"pulses\": [(30, 15, 3), (25, 40, 3)]},\n            \"g_params\": {\"type\": \"gamma\", \"theta\": 3, \"lambda\": 1},\n            \"beta\": 0.05,\n            \"e_params\": {\"delta\": 0.5, \"p\": 12},\n        },\n        {\n            \"T\": 70, \"K\": 9, \"tau\": 0.8,\n            \"S_params\": {\"b\": 10, \"pulses\": [(40, 20, 4), (35, 50, 4)]},\n            \"g_params\": {\"type\": \"mixture\", \"points\": {2: 0.6, 6: 0.4}},\n            \"beta\": 0.03,\n            \"e_params\": {\"delta\": 0.3, \"p\": 10},\n        },\n        {\n            \"T\": 55, \"K\": 5, \"tau\": 0.8,\n            \"S_params\": {\"b\": 8, \"pulses\": [(50, 25, 5)]},\n            \"g_params\": {\"type\": \"mixture\", \"points\": {0: 1.0}},\n            \"beta\": 0.02,\n            \"e_params\": {\"delta\": 0.2, \"p\": 11},\n        },\n        {\n            \"T\": 25, \"K\": 10, \"tau\": 0.8,\n            \"S_params\": {\"b\": 6, \"pulses\": [(18, 8, 2), (10, 17, 2)]},\n            \"g_params\": {\"type\": \"gamma\", \"theta\": 2, \"lambda\": 2},\n            \"beta\": 0.04,\n            \"e_params\": {\"delta\": 0.7, \"p\": 8},\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        T, K, tau = case[\"T\"], case[\"K\"], case[\"tau\"]\n        S_params, g_params, beta, e_params = case[\"S_params\"], case[\"g_params\"], case[\"beta\"], case[\"e_params\"]\n        \n        # 1. Generate synthetic data (S_t, C_t)\n        \n        # Generate true lag distribution g_k\n        g_true = np.zeros(K + 1)\n        if g_params[\"type\"] == \"gamma\":\n            theta, lam = g_params[\"theta\"], g_params[\"lambda\"]\n            for k_idx in range(K + 1):\n                # Proportional to k^(theta-1) * exp(-k/lambda)\n                if k_idx == 0 and theta < 1:\n                    # Handle k=0 case for theta < 1, though not present in test cases\n                    g_true[k_idx] = 0\n                else:\n                    g_true[k_idx] = (k_idx ** (theta - 1)) * np.exp(-k_idx / lam)\n        elif g_params[\"type\"] == \"mixture\":\n            for k_idx, val in g_params[\"points\"].items():\n                if 0 <= k_idx <= K:\n                    g_true[k_idx] = val\n        \n        g_sum = np.sum(g_true)\n        if g_sum > 0:\n            g_true /= g_sum\n\n        # Generate signal S_t\n        t_vals = np.arange(T)\n        S_signal = np.full(T, S_params[\"b\"], dtype=float)\n        for A, t_j, sigma_j in S_params[\"pulses\"]:\n            S_signal += A * np.exp(-((t_vals - t_j) ** 2) / (2 * sigma_j ** 2))\n\n        # Generate error e_t\n        e_signal = e_params[\"delta\"] * np.sin(2 * np.pi * t_vals / e_params[\"p\"])\n\n        # Generate cases C_t using convolution\n        convolved_S = np.convolve(S_signal, g_true, mode='full')[:T]\n        C_signal = beta * convolved_S + e_signal\n\n        # 2. Set up and solve the Non-Negative Least Squares (NNLS) problem\n        \n        # Fitting domain is t in {K, ..., T-1}\n        num_rows = T - K\n        y_vec = C_signal[K:]\n        \n        # Construct the design matrix A\n        A_mat = np.zeros((num_rows, K + 1))\n        for i in range(num_rows):\n            t = K + i\n            # The slice S_signal[t-K : t+1] gives S from indices t-K to t.\n            # Reversed, this gives (S_t, S_{t-1}, ..., S_{t-K})\n            A_mat[i, :] = S_signal[t-K : t+1][::-1]\n\n        # Solve for gamma_hat\n        gamma_hat, _ = nnls(A_mat, y_vec)\n\n        # 3. Recover g_hat and compute functionals\n        \n        # Recover beta_hat and g_hat\n        beta_hat = np.sum(gamma_hat)\n        g_hat = np.zeros_like(gamma_hat)\n        if beta_hat > 1e-12: # Avoid division by zero\n            g_hat = gamma_hat / beta_hat\n\n        # Compute functionals from g_hat\n        k_vals = np.arange(K + 1)\n        \n        # Expected lag\n        expected_lag = np.sum(k_vals * g_hat)\n        \n        # Median lag\n        cum_g_hat = np.cumsum(g_hat)\n        \n        # np.argmax returns the index of the first True value.\n        # This corresponds to the smallest integer L satisfying the condition.\n        median_lag_indices = np.where(cum_g_hat >= 0.5)[0]\n        median_lag = float(median_lag_indices[0]) if len(median_lag_indices) > 0 else float(K)\n\n        # Actionable lead time\n        l_tau_indices = np.where(cum_g_hat >= tau)[0]\n        actionable_lead_time = float(l_tau_indices[0]) if len(l_tau_indices) > 0 else float(K)\n        \n        case_results = [\n            round(expected_lag, 2),\n            round(median_lag, 2),\n            round(actionable_lead_time, 2)\n        ]\n        all_results.append(case_results)\n\n    # 4. Format and print the final output\n    print(f\"{all_results}\".replace(\" \", \"\"))\n\n\nsolve()\n```", "id": "4664114"}]}