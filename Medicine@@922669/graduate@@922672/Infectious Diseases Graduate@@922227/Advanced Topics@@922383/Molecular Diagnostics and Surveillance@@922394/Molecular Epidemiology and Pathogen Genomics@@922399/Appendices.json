{"hands_on_practices": [{"introduction": "Before any biological conclusions can be drawn, we must first assess the quality of our genomic data. This exercise delves into the fundamental unit of sequencing quality, the Phred score, and demonstrates how to rigorously combine evidence from multiple sequencing reads. By deriving the consensus quality from first principles, you will gain a deep appreciation for the probabilistic foundation of modern genomics and how we build confidence in the genetic data that underpins all subsequent analyses [@problem_id:4667814].", "problem": "A genomic epidemiology team uses Next-Generation Sequencing (NGS) to investigate a suspected transmission cluster of an RNA virus. At a single locus used for cluster assignment, $k$ independent NGS reads all report the nucleotide $\\texttt{G}$. Each read $i \\in \\{1,\\dots,k\\}$ has an associated base quality score $Q_i$ produced by a standard base caller. The Phred quality score is defined as a logarithmic measure of base-calling error probability such that a reduction of the error probability by a factor of $10$ increases the quality score by $10$ units, and $Q_i$ is monotone decreasing in the error probability.\n\nStarting from this definition and a probabilistic model in which:\n- the nucleotide alphabet is four states $\\{\\texttt{A},\\texttt{C},\\texttt{G},\\texttt{T}\\}$,\n- the prior over true nucleotides is uniform across the four states,\n- per-read base-calling errors are independent across reads,\n- conditional on an error when the true base is not $\\texttt{G}$, the erroneous report is uniformly distributed over the three incorrect nucleotides,\n\nderive from first principles an explicit analytic expression for the posterior error probability of the consensus call $\\texttt{G}$ given the observed data and then its Phred-scaled consensus confidence. Then evaluate this expression for $k=5$ reads with per-base quality scores $Q_1=30$, $Q_2=27$, $Q_3=35$, $Q_4=20$, and $Q_5=40$, all calling $\\texttt{G}$.\n\nReport the final consensus Phred quality score as a single real number. Round your answer to four significant figures. No physical units are required.", "solution": "The problem is to derive and evaluate the consensus Phred quality score for a nucleotide base call given a set of Next-Generation Sequencing (NGS) reads and their associated quality scores. This requires a Bayesian approach.\n\nFirst, we formalize the relationship between the Phred quality score $Q_i$ and the base-calling error probability $p_{e,i}$ for a single read $i$. The problem states that a reduction of the error probability by a factor of $10$ increases the quality score by $10$ units. This implies a logarithmic relationship. Let the error probability be $p_e$. The quality score $Q$ is a function of $p_e$, say $Q(p_e)$. The condition is $Q(p_e/10) = Q(p_e) + 10$. This functional equation is solved by the form $Q(p_e) = -10 \\log_{10}(p_e)$. We can verify this: $Q(p_e/10) = -10 \\log_{10}(p_e/10) = -10(\\log_{10}(p_e) - 1) = -10\\log_{10}(p_e) + 10 = Q(p_e) + 10$.\nFrom this definition, the error probability for read $i$ with quality score $Q_i$ is given by:\n$$p_{e,i} = 10^{-Q_i/10}$$\nThe probability of a correct call for read $i$ is $1 - p_{e,i}$.\n\nNext, we establish the Bayesian framework. Let $T$ be the random variable for the true nucleotide at the genomic locus, which can take values in the set $\\{\\texttt{A}, \\texttt{C}, \\texttt{G}, \\texttt{T}\\}$. Let $D$ represent the observed data, which consists of $k$ independent reads, all reporting the nucleotide $\\texttt{G}$. We denote the event that read $i$ reports $\\texttt{G}$ as $D_i$. Thus, $D = \\{D_1, D_2, \\dots, D_k\\}$.\n\nThe goal is to find the posterior error probability of the consensus call $\\texttt{G}$, which is the probability that the true nucleotide is not $\\texttt{G}$, given the data $D$. We denote this as $P_{\\text{err, cons}} = P(T \\neq \\texttt{G} | D)$. This is equal to $1 - P(T = \\texttt{G} | D)$.\n\nUsing Bayes' theorem, the posterior probability of the true base being $\\texttt{G}$ is:\n$$P(T = \\texttt{G} | D) = \\frac{P(D | T = \\texttt{G}) P(T = \\texttt{G})}{P(D)}$$\nThe marginal probability of the data, $P(D)$, is given by the law of total probability:\n$$P(D) = \\sum_{b \\in \\{\\texttt{A},\\texttt{C},\\texttt{G},\\texttt{T}\\}} P(D|T=b)P(T=b)$$\nThe problem specifies a uniform prior over the true nucleotides:\n$$P(T=b) = \\frac{1}{4} \\quad \\text{for any } b \\in \\{\\texttt{A}, \\texttt{C}, \\texttt{G}, \\texttt{T}\\}$$\n\nNow, we derive the likelihood terms, $P(D|T=b)$. Since the reads are independent, the likelihood is the product of the probabilities for each read: $P(D|T=b) = \\prod_{i=1}^{k} P(D_i|T=b)$.\n\nCase 1: The true nucleotide is $\\texttt{G}$ ($T = \\texttt{G}$).\nThe probability of observing read $i$ as $\\texttt{G}$ is the probability of a correct call, which is $1 - p_{e,i}$.\nThe likelihood is:\n$$L_G = P(D | T = \\texttt{G}) = \\prod_{i=1}^{k} (1 - p_{e,i})$$\n\nCase 2: The true nucleotide is not $\\texttt{G}$ (e.g., $T = \\texttt{A}$).\nThe probability of observing read $i$ as $\\texttt{G}$ is the probability of an error. The problem states that conditional on an error, the reported base is uniformly distributed over the three incorrect options. If the true base is $\\texttt{A}$, an error means reading $\\texttt{C}$, $\\texttt{G}$, or $\\texttt{T}$. The probability of reading $\\texttt{G}$ specifically is $1/3$ of the total error probability.\n$$P(D_i | T = \\texttt{A}) = p_{e,i} \\times \\frac{1}{3} = \\frac{p_{e,i}}{3}$$\nBy symmetry, the likelihood is the same for $T = \\texttt{C}$ and $T = \\texttt{T}$. Let's denote this likelihood as $L_{\\neg G}$.\n$$L_{\\neg G} = P(D | T = b \\neq \\texttt{G}) = \\prod_{i=1}^{k} \\frac{p_{e,i}}{3} = \\left(\\frac{1}{3}\\right)^k \\prod_{i=1}^{k} p_{e,i}$$\n\nNow we can write the posterior error probability:\n$$P_{\\text{err, cons}} = P(T \\neq \\texttt{G} | D) = \\frac{P(D | T \\neq \\texttt{G}) P(T \\neq \\texttt{G})}{P(D)}$$\n$$P(D | T \\neq \\texttt{G})P(T \\neq \\texttt{G}) = P(D|T=\\texttt{A})P(T=\\texttt{A}) + P(D|T=\\texttt{C})P(T=\\texttt{C}) + P(D|T=\\texttt{T})P(T=\\texttt{T})$$\n$$= L_{\\neg G} \\left(\\frac{1}{4}\\right) + L_{\\neg G} \\left(\\frac{1}{4}\\right) + L_{\\neg G} \\left(\\frac{1}{4}\\right) = 3 L_{\\neg G} \\left(\\frac{1}{4}\\right)$$\nThe total evidence is $P(D) = P(D|T=\\texttt{G})P(T=\\texttt{G}) + 3 L_{\\neg G} (1/4) = L_G(1/4) + 3L_{\\neg G}(1/4) = \\frac{1}{4}(L_G + 3L_{\\neg G})$.\nSo, the posterior error probability is:\n$$P_{\\text{err, cons}} = \\frac{3 L_{\\neg G} (1/4)}{\\frac{1}{4}(L_G + 3L_{\\neg G})} = \\frac{3 L_{\\neg G}}{L_G + 3 L_{\\neg G}}$$\nThis can be rewritten as:\n$$P_{\\text{err, cons}} = \\frac{1}{1 + \\frac{L_G}{3L_{\\neg G}}}$$\nThe consensus Phred quality score, $Q_{\\text{cons}}$, is defined in the same way as the per-read score:\n$$Q_{\\text{cons}} = -10 \\log_{10}(P_{\\text{err, cons}}) = 10 \\log_{10}\\left(1 + \\frac{L_G}{3L_{\\neg G}}\\right)$$\nLet's substitute the expressions for $L_G$ and $L_{\\neg G}$:\n$$\\frac{L_G}{3L_{\\neg G}} = \\frac{\\prod_{i=1}^{k} (1 - p_{e,i})}{3 \\left(\\frac{1}{3}\\right)^k \\prod_{i=1}^{k} p_{e,i}} = \\frac{\\prod_{i=1}^{k} (1 - p_{e,i})}{3^{1-k} \\prod_{i=1}^{k} p_{e,i}} = 3^{k-1} \\prod_{i=1}^{k} \\frac{1 - p_{e,i}}{p_{e,i}}$$\nUsing the relation $p_{e,i} = 10^{-Q_i/10}$, we have $\\frac{1-p_{e,i}}{p_{e,i}} = \\frac{1 - 10^{-Q_i/10}}{10^{-Q_i/10}} = 10^{Q_i/10} - 1$.\nThe final analytic expression for the consensus Phred score is:\n$$Q_{\\text{cons}} = 10 \\log_{10}\\left(1 + 3^{k-1} \\prod_{i=1}^{k} (10^{Q_i/10} - 1)\\right)$$\n\nNow, we evaluate this expression for the given data: $k=5$ reads with quality scores $Q_1=30$, $Q_2=27$, $Q_3=35$, $Q_4=20$, and $Q_5=40$.\nFirst, we compute the term $(10^{Q_i/10} - 1)$ for each read:\nFor $Q_1=30$: $10^{30/10} - 1 = 10^3 - 1 = 999$\nFor $Q_2=27$: $10^{27/10} - 1 = 10^{2.7} - 1$\nFor $Q_3=35$: $10^{35/10} - 1 = 10^{3.5} - 1$\nFor $Q_4=20$: $10^{20/10} - 1 = 10^2 - 1 = 99$\nFor $Q_5=40$: $10^{40/10} - 1 = 10^4 - 1 = 9999$\n\nThe product term is:\n$$\\prod_{i=1}^{5} (10^{Q_i/10} - 1) = (999) \\times (10^{2.7} - 1) \\times (10^{3.5} - 1) \\times (99) \\times (9999)$$\nWith $k=5$, the factor $3^{k-1}$ is $3^{5-1} = 3^4 = 81$.\nSubstituting these values into the expression for $Q_{\\text{cons}}$:\n$$Q_{\\text{cons}} = 10 \\log_{10}\\left(1 + 81 \\times (999) \\times (10^{2.7} - 1) \\times (10^{3.5} - 1) \\times (99) \\times (9999) \\right)$$\nLet's evaluate the argument of the logarithm.\n$10^{2.7} \\approx 501.1872$\n$10^{3.5} \\approx 3162.2777$\nThe argument is $1 + 81 \\times 999 \\times (501.1872 - 1) \\times (3162.2777 - 1) \\times 99 \\times 9999$.\nThis is approximately $1 + 81 \\times 999 \\times 500.1872 \\times 3161.2777 \\times 99 \\times 9999 \\approx 1.26663 \\times 10^{17}$.\nSince this value is very large, the $1$ is negligible.\n$Q_{\\text{cons}} \\approx 10 \\log_{10}(1.26663 \\times 10^{17})$\n$Q_{\\text{cons}} \\approx 10 \\times ( \\log_{10}(1.26663) + \\log_{10}(10^{17}) )$\n$Q_{\\text{cons}} \\approx 10 \\times (0.10264 + 17) = 10 \\times 17.10264 = 171.0264$\n\nTo be more precise and avoid intermediate rounding:\n$\\log_{10}\\left(1 + 3^{k-1} \\prod (10^{Q_i/10}-1)\\right) \\approx \\log_{10}\\left(3^{k-1} \\prod (10^{Q_i/10}-1)\\right)$\n$= \\log_{10}(3^{k-1}) + \\sum_{i=1}^{k} \\log_{10}(10^{Q_i/10}-1)$\nFor $k=5$:\n$= 4 \\log_{10}(3) + \\log_{10}(999) + \\log_{10}(10^{2.7}-1) + \\log_{10}(10^{3.5}-1) + \\log_{10}(99) + \\log_{10}(9999)$\n$\\approx 4(0.477121) + 2.999565 + 2.699140 + 3.499862 + 1.995635 + 3.999957$\n$\\approx 1.908485 + 15.194159 = 17.102644$\nThe consensus score is $10$ times this value:\n$Q_{\\text{cons}} \\approx 10 \\times 17.102644 = 171.02644$\nRounding to four significant figures gives $171.0$.", "answer": "$$\\boxed{171.0}$$", "id": "4667814"}, {"introduction": "Identifying genetic variants is a cornerstone of molecular epidemiology, but distinguishing true, low-frequency variants from sequencing errors is a major statistical challenge. This practice introduces a formal framework for this task, using a binomial model of read counts to control the False Discovery Rate (FDR). By working through the derivation and calculating an optimal calling threshold, you will learn how to balance sensitivity and specificity in variant detection, a critical skill for studying intra-host diversity and evolution [@problem_id:4667660].", "problem": "A deep sequencing experiment targeting a clonally dominant viral pathogen is used to screen for within-host minor variants across a genome. At any genomic site, each read covering that site independently reports a non-reference base due to sequencing error with probability $\\epsilon$, and reports the true base correctly with probability $1 - \\epsilon$. Assume independence of errors across reads and sites, constant site-specific error probability $\\epsilon$, and that the per-site read coverage is exactly $C$ reads.\n\nA variant-calling rule declares a site as harboring a minor variant if the observed non-reference allele count $x$ among the $C$ reads satisfies $x \\geq k$, for some integer threshold $k$. Suppose that, across sites, a fraction $r$ of sites truly harbor a minor variant at within-host allele fraction $p$ (constant across those variant sites), and a fraction $1 - r$ of sites are truly monomorphic for the reference allele.\n\nStarting from the independence assumptions and the definition of the binomial distribution, derive how the per-base error rate $\\epsilon$ and coverage $C$ jointly determine the per-site probability of a false variant call under the rule $x \\geq k$ when the site is truly monomorphic. Then, using the definition of False Discovery Rate (FDR) as the expected proportion of falsely called sites among all called sites, derive an expression for the FDR as a function of $k$, and compute the minimal integer threshold $k^{\\star}$ such that the FDR does not exceed a target level $\\alpha$ under the following scientifically realistic parameter values: $\\epsilon = 1.0 \\times 10^{-3}$, $C = 200$, $r = 1.0 \\times 10^{-4}$, $p = 0.05$, and $\\alpha = 0.05$. Express your final answer as the minimal integer $k^{\\star}$. No rounding instruction applies because the threshold is an integer.", "solution": "The problem is evaluated as valid, as it is scientifically grounded in the principles of genomics and statistics, is well-posed with all necessary information provided, and is formulated objectively.\n\nThe problem asks for two main derivations followed by a calculation. First, to derive the per-site probability of a false variant call. Second, to derive the False Discovery Rate (FDR) of the variant calling rule. Finally, to compute the minimal integer threshold $k^{\\star}$ that constrains the FDR below a specified level $\\alpha$.\n\nLet us model the number of non-reference reads observed at a genomic site. The experiment consists of $C$ independent trials (reads), where each trial can result in a reference or a non-reference base. This is a classic Bernoulli process, and the total count of non-reference reads, which we denote by a random variable, will follow a binomial distribution. The parameter of this distribution, the probability of observing a non-reference base, depends on whether the site is truly monomorphic or harbors a variant.\n\n**Part 1: Per-site Probability of a False Variant Call**\n\nA false variant call, also known as a False Positive (FP), occurs when a site that is truly monomorphic (i.e., contains only the reference allele) is declared to have a minor variant.\n\nFor a truly monomorphic site, the true base is the reference allele. A non-reference base can be observed in a read only due to a sequencing error. The problem states that this occurs with a probability $\\epsilon$. Therefore, for each of the $C$ reads covering the site, the probability of observing a non-reference base is $\\epsilon$.\n\nLet $X$ be the random variable representing the number of non-reference reads at a truly monomorphic site. Since the reads are independent, $X$ follows a binomial distribution with parameters $C$ (number of trials) and $\\epsilon$ (probability of success, i.e., observing a non-reference allele). We write this as $X \\sim \\text{Bin}(C, \\epsilon)$.\n\nThe variant-calling rule declares a site as a variant if the observed non-reference count $x$ is greater than or equal to a threshold $k$, i.e., $x \\geq k$. A false call is made if this rule is met at a monomorphic site. The per-site probability of a false variant call is therefore $P(X \\geq k)$.\n\nThis probability is the sum of the probabilities of observing exactly $i$ non-reference reads, for all $i$ from $k$ to $C$:\n$$P(\\text{False Call}) = P(X \\geq k) = \\sum_{i=k}^{C} P(X=i)$$\nUsing the probability mass function for a binomial distribution, $P(X=i) = \\binom{C}{i} \\epsilon^i (1-\\epsilon)^{C-i}$, we obtain the expression for the per-site probability of a false variant call:\n$$P(\\text{False Call}) = \\sum_{i=k}^{C} \\binom{C}{i} \\epsilon^i (1-\\epsilon)^{C-i}$$\n\n**Part 2: Derivation of the False Discovery Rate (FDR)**\n\nThe False Discovery Rate (FDR) is defined as the expected proportion of false discoveries (false positives) among all discoveries (all positive calls). Let's denote the number of sites called as variants as $D$. Let the number of false positive calls be $FP$ and the number of true positive calls be $TP$. Then $D = FP + TP$. The FDR is defined as:\n$$\\text{FDR} = E\\left[\\frac{FP}{D}\\right | D>0] P(D>0)$$\nUnder common assumptions in large-scale testing, this is approximated by:\n$$\\text{FDR} \\approx \\frac{E[FP]}{E[D]} = \\frac{E[FP]}{E[FP] + E[TP]}$$\n\nTo calculate the expected number of FPs and TPs, we consider a large number of sites, say $N_{total}$.\nA fraction $1-r$ of these sites are truly monomorphic. The expected number of FPs is the number of monomorphic sites multiplied by the probability of making a false call at each such site.\n$$E[FP] = N_{total} (1-r) P(X \\geq k) \\quad \\text{where } X \\sim \\text{Bin}(C, \\epsilon)$$\n\nA fraction $r$ of sites truly harbor a minor variant at frequency $p$. At these sites, a non-reference read can be observed in two mutually exclusive ways:\n1. The read samples the true variant allele (probability $p$) and the sequencer does not make an error (probability $1-\\epsilon$).\n2. The read samples the reference allele (probability $1-p$) and the sequencer makes an error, reporting a non-reference base (probability $\\epsilon$).\n\nSo, the total probability of observing a non-reference base at a true variant site, let's call it $p_{\\text{obs}}$, is:\n$$p_{\\text{obs}} = p(1-\\epsilon) + (1-p)\\epsilon = p - p\\epsilon + \\epsilon - p\\epsilon = p + \\epsilon(1-2p)$$\nLet $Y$ be the random variable for the number of non-reference reads at a true variant site. $Y$ follows a binomial distribution $Y \\sim \\text{Bin}(C, p_{\\text{obs}})$.\n\nA true positive (TP) occurs when a true variant site is correctly called as a variant. The probability of this is $P(Y \\geq k)$.\n$$P(\\text{True Call}) = P(Y \\geq k) = \\sum_{i=k}^{C} \\binom{C}{i} p_{\\text{obs}}^i (1-p_{\\text{obs}})^{C-i}$$\nThe expected number of TPs is the number of true variant sites multiplied by the probability of a true call.\n$$E[TP] = N_{total} \\, r \\, P(Y \\geq k) \\quad \\text{where } Y \\sim \\text{Bin}(C, p_{\\text{obs}})$$\n\nSubstituting these into the FDR formula, the $N_{total}$ term cancels out:\n$$\\text{FDR}(k) = \\frac{(1-r) P(X \\geq k)}{(1-r) P(X \\geq k) + r P(Y \\geq k)}$$\nSubstituting the binomial probability sums gives the final expression for the FDR as a function of $k$:\n$$\\text{FDR}(k) = \\frac{(1-r) \\sum_{i=k}^{C} \\binom{C}{i} \\epsilon^i (1-\\epsilon)^{C-i}}{(1-r) \\sum_{i=k}^{C} \\binom{C}{i} \\epsilon^i (1-\\epsilon)^{C-i} + r \\sum_{i=k}^{C} \\binom{C}{i} p_{\\text{obs}}^i (1-p_{\\text{obs}})^{C-i}}$$\n\n**Part 3: Computation of the Minimal Threshold $k^{\\star}$**\n\nWe are given the parameter values: $\\epsilon = 1.0 \\times 10^{-3}$, $C = 200$, $r = 1.0 \\times 10^{-4}$, $p = 0.05$, and the target FDR $\\alpha = 0.05$.\nFirst, we calculate $p_{\\text{obs}}$:\n$$p_{\\text{obs}} = 0.05 + 0.001(1 - 2 \\times 0.05) = 0.05 + 0.001(0.9) = 0.0509$$\nWe need to find the smallest integer $k$ such that $\\text{FDR}(k) \\leq 0.05$.\nLet $P_{FP}(k) = P(X \\geq k)$ with $X \\sim \\text{Bin}(200, 0.001)$, and $P_{TP}(k) = P(Y \\geq k)$ with $Y \\sim \\text{Bin}(200, 0.0509)$.\nThe expression for FDR becomes:\n$$\\text{FDR}(k) = \\frac{(1-10^{-4}) P_{FP}(k)}{(1-10^{-4}) P_{FP}(k) + 10^{-4} P_{TP}(k)}$$\nThe term $\\text{FDR}(k)$ is a monotonically non-increasing function of $k$. We can find the minimal $k^{\\star}$ by testing integer values of $k$ starting from $1$.\n\nFor $k=4$:\n$P_{FP}(4) = \\sum_{i=4}^{200} \\binom{200}{i} (0.001)^i (0.999)^{200-i} \\approx 5.8407 \\times 10^{-5}$\n$P_{TP}(4) = \\sum_{i=4}^{200} \\binom{200}{i} (0.0509)^i (0.9491)^{200-i} \\approx 0.99843$\n$$\\text{FDR}(4) = \\frac{0.9999 \\times 5.8407 \\times 10^{-5}}{0.9999 \\times 5.8407 \\times 10^{-5} + 0.0001 \\times 0.99843} \\approx \\frac{5.8401 \\times 10^{-5}}{5.8401 \\times 10^{-5} + 9.9843 \\times 10^{-5}} = \\frac{5.8401}{15.8244} \\approx 0.36906$$\nSince $0.36906 > 0.05$, the threshold $k=4$ is not stringent enough.\n\nFor $k=5$:\n$P_{FP}(5) = \\sum_{i=5}^{200} \\binom{200}{i} (0.001)^i (0.999)^{200-i} \\approx 2.3426 \\times 10^{-6}$\n$P_{TP}(5) = \\sum_{i=5}^{200} \\binom{200}{i} (0.0509)^i (0.9491)^{200-i} \\approx 0.99264$\n$$\\text{FDR}(5) = \\frac{0.9999 \\times 2.3426 \\times 10^{-6}}{0.9999 \\times 2.3426 \\times 10^{-6} + 0.0001 \\times 0.99264} \\approx \\frac{2.3424 \\times 10^{-6}}{2.3424 \\times 10^{-6} + 9.9264 \\times 10^{-5}} = \\frac{2.3424}{101.6064} \\approx 0.02305$$\nSince $0.02305 \\leq 0.05$, the threshold $k=5$ meets the requirement.\n\nAs $\\text{FDR}(4) > 0.05$ and $\\text{FDR}(5) \\leq 0.05$, the minimal integer threshold $k^{\\star}$ that ensures the FDR does not exceed $0.05$ is $5$.", "answer": "$$\\boxed{5}$$", "id": "4667660"}, {"introduction": "A collection of pathogen genomes contains a rich, albeit noisy, historical record of the epidemic from which they were sampled. This exercise introduces a powerful method for \"reading\" this history: the Bayesian skyline plot, which reconstructs changes in effective population size ($N_e$) over time. Starting from the coalescent model, you will derive the posterior distribution of $N_e$ and compute its key summary statistics, thereby translating phylogenetic patterns into a quantitative narrative of population dynamics [@problem_id:4667746].", "problem": "You are given inter-coalescent waiting times and corresponding lineage counts from a single rooted, strictly bifurcating phylogeny sampled under the standard coalescent. Assume a piecewise constant effective population size function $N_e(t)$ segmented into groups of consecutive inter-coalescent intervals. Within each group $g$, the effective population size is constant and equal to $N_{e,g}$.\n\nFundamental base and modeling assumptions:\n- Under the standard coalescent with piecewise constant population size, each inter-coalescent waiting time $t_i$ with $k_i$ contemporary lineages has an exponential distribution with hazard rate $h_i = \\binom{k_i}{2}/N_{e,g}$ when interval $i$ belongs to group $g$.\n- Let $\\lambda_g = 1/N_{e,g}$. Then the likelihood contribution of an interval $i$ in group $g$ is exponential with rate $r_i = \\lambda_g \\binom{k_i}{2}$, independent across intervals given the group-wise constants.\n- Adopt the conjugate prior for each group $g$ on $\\lambda_g$: a Gamma distribution with shape $a_g$ and rate $b_g$, denoted $\\lambda_g \\sim \\mathrm{Gamma}(a_g,b_g)$, where the density is proportional to $\\lambda_g^{a_g - 1} \\exp(-b_g \\lambda_g)$.\n- The sufficient statistics for each group $g$ are the number of inter-coalescent intervals $m_g$ and the scaled time sum $S_g = \\sum_{i \\in g} \\binom{k_i}{2} t_i$.\n\nYour task is to compute a Bayesian skyline summary for $N_{e,g}$ for each group $g$ by deriving the posterior distribution of $\\lambda_g$ and then transforming to $N_{e,g} = 1/\\lambda_g$. For each group $g$, compute:\n- The posterior mean of $N_{e,g}$.\n- The posterior median of $N_{e,g}$.\n- The lower and upper endpoints of the central $95$-level credible interval for $N_{e,g}$, that is, the $0.025$ and $0.975$ posterior quantiles.\n\nExpress all outputs for $N_{e,g}$ as real numbers (counts of individuals, a dimensionless quantity), rounded to six decimal places. Time units for $t_i$ are in years; no time unit should be printed in the output.\n\nDerive your method from the fundamental assumptions above. Do not use or assume any formula that is not a direct consequence of these assumptions.\n\nTest suite. For each test case, you are given:\n- A list of inter-coalescent times $\\{t_i\\}$ in years.\n- A list of lineage counts $\\{k_i\\}$ of the same length, where $k_i \\ge 2$ and $t_i \\ge 0$.\n- A list of positive integers specifying the group sizes that partition the sequence of intervals consecutively.\n- Prior hyperparameters shared across groups: shape $a$ and rate $b$.\n\nCompute the Bayesian skyline summary for each group in each test case. The test cases are:\n\n- Test case $1$:\n  - $t = [0.05, 0.12, 0.08, 0.20, 0.30, 0.25]$\n  - $k = [10, 9, 8, 7, 6, 5]$\n  - group sizes $= [2, 2, 2]$\n  - prior $(a, b) = (2.5, 1.0)$\n\n- Test case $2$:\n  - $t = [0.02, 0.02, 0.02, 0.02]$\n  - $k = [50, 49, 48, 47]$\n  - group sizes $= [1, 1, 2]$\n  - prior $(a, b) = (2.1, 0.5)$\n\n- Test case $3$:\n  - $t = [0.30, 0.25, 0.35, 0.40]$\n  - $k = [6, 5, 4, 3]$\n  - group sizes $= [4]$\n  - prior $(a, b) = (1.5, 1.0)$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case. Each test case element is a list of groups, and each group element is a list of four numbers in the order [posterior mean, posterior median, $0.025$ quantile, $0.975$ quantile], each rounded to six decimal places. For example, a valid structural shape is:\n  - $[\\,[\\,[x_{11}, x_{12}, x_{13}, x_{14}], [x_{21}, x_{22}, x_{23}, x_{24}], \\ldots], \\; [\\ldots], \\; [\\ldots]\\,]$\n\nIn your written solution, explain the derivation from the base assumptions and discuss identifiability of piecewise constant segments of $N_e(t)$ in terms of how $m_g$ and $S_g$ constrain inference, including edge cases where a segment has very limited or no information.", "solution": "The problem requires the computation of posterior summary statistics for a piecewise constant effective population size, $N_e(t)$, within a Bayesian framework. The methodology is derived from the fundamental assumptions of the standard coalescent model, an exponential likelihood for inter-coalescent waiting times, and a conjugate Gamma prior on the precision parameter $\\lambda_g = 1/N_{e,g}$.\n\nThe derivation proceeds in three main stages:\n1.  Derivation of the posterior distribution for the parameter $\\lambda_g$ for each population size group $g$.\n2.  Transformation of the posterior distribution from $\\lambda_g$ to the effective population size $N_{e,g}$.\n3.  Computation of the required summary statistics (mean, median, and $95\\%$ credible interval) from the posterior distribution of $N_{e,g}$.\n\n### 1. Posterior Distribution of $\\lambda_g$\n\nAccording to Bayes' theorem, the posterior probability distribution is proportional to the product of the likelihood and the prior probability distribution.\n$$P(\\lambda_g | \\text{data}) \\propto P(\\text{data} | \\lambda_g) \\times P(\\lambda_g)$$\n\n**Likelihood Function:**\nThe problem states that for an interval $i$ within a group $g$, the inter-coalescent waiting time $t_i$ for $k_i$ lineages is exponentially distributed with a rate $r_i = \\lambda_g \\binom{k_i}{2}$. The probability density function (PDF) for a single interval is:\n$$P(t_i | \\lambda_g, k_i) = \\lambda_g \\binom{k_i}{2} \\exp\\left( -\\lambda_g \\binom{k_i}{2} t_i \\right)$$\nAssuming the waiting times are conditionally independent given $\\lambda_g$, the total likelihood for the data in group $g$ (comprising $m_g$ intervals) is the product of the individual PDFs:\n$$L(\\lambda_g) = P(\\{t_i, k_i\\}_{i \\in g} | \\lambda_g) = \\prod_{i \\in g} P(t_i | \\lambda_g, k_i)$$\n$$L(\\lambda_g) = \\prod_{i \\in g} \\left[ \\lambda_g \\binom{k_i}{2} \\exp\\left( -\\lambda_g \\binom{k_i}{2} t_i \\right) \\right]$$\nWe can group the terms involving $\\lambda_g$:\n$$L(\\lambda_g) = \\left( \\prod_{i \\in g} \\binom{k_i}{2} \\right) \\left( \\prod_{i \\in g} \\lambda_g \\right) \\exp\\left( -\\sum_{i \\in g} \\lambda_g \\binom{k_i}{2} t_i \\right)$$\n$$L(\\lambda_g) = \\left( \\prod_{i \\in g} \\binom{k_i}{2} \\right) \\lambda_g^{m_g} \\exp\\left( -\\lambda_g \\sum_{i \\in g} \\binom{k_i}{2} t_i \\right)$$\nThe term $\\prod_{i \\in g} \\binom{k_i}{2}$ is a constant with respect to $\\lambda_g$. Using the provided sufficient statistic $S_g = \\sum_{i \\in g} \\binom{k_i}{2} t_i$, the likelihood is proportional to:\n$$L(\\lambda_g) \\propto \\lambda_g^{m_g} \\exp(-\\lambda_g S_g)$$\nThis is the kernel of the likelihood function.\n\n**Prior Distribution:**\nThe problem specifies a conjugate Gamma prior for $\\lambda_g$:\n$$\\lambda_g \\sim \\mathrm{Gamma}(a_g, b_g)$$\nThe PDF is $P(\\lambda_g) \\propto \\lambda_g^{a_g - 1} \\exp(-b_g \\lambda_g)$, where $a_g$ is the shape parameter and $b_g$ is the rate parameter.\n\n**Posterior Distribution:**\nMultiplying the likelihood kernel and the prior density kernel gives the posterior density kernel:\n$$P(\\lambda_g | \\text{data}) \\propto \\left( \\lambda_g^{m_g} \\exp(-\\lambda_g S_g) \\right) \\times \\left( \\lambda_g^{a_g - 1} \\exp(-b_g \\lambda_g) \\right)$$\n$$P(\\lambda_g | \\text{data}) \\propto \\lambda_g^{a_g + m_g - 1} \\exp\\left( -(b_g + S_g) \\lambda_g \\right)$$\nThis is the kernel of a Gamma distribution with updated parameters. Let the posterior parameters be $a'_g = a_g + m_g$ and $b'_g = b_g + S_g$.\nThus, the posterior distribution of $\\lambda_g$ is:\n$$\\lambda_g | \\text{data} \\sim \\mathrm{Gamma}(a_g + m_g, b_g + S_g)$$\n\n### 2. Posterior Distribution of $N_{e,g}$\n\nThe effective population size $N_{e,g}$ is the reciprocal of $\\lambda_g$, i.e., $N_{e,g} = 1/\\lambda_g$. If a random variable $X$ follows a Gamma distribution, $X \\sim \\mathrm{Gamma}(\\alpha, \\beta)$, then its reciprocal $Y=1/X$ follows an Inverse-Gamma distribution with the same shape parameter $\\alpha$ and a scale parameter $\\beta$ (the rate of the Gamma becomes the scale of the Inverse-Gamma).\nTherefore, the posterior distribution for $N_{e,g}$ is an Inverse-Gamma distribution:\n$$N_{e,g} | \\text{data} \\sim \\mathrm{InverseGamma}(a'_g, b'_g) = \\mathrm{InverseGamma}(a_g + m_g, b_g + S_g)$$\n\n### 3. Posterior Summary Statistics for $N_{e,g}$\n\nWe can now compute the required statistics from this posterior $\\mathrm{InverseGamma}(a'_g, b'_g)$ distribution.\n\n**Posterior Mean:**\nThe expected value of an Inverse-Gamma distribution, $\\mathrm{InverseGamma}(\\alpha, \\beta)$, is $E[Y] = \\frac{\\beta}{\\alpha - 1}$, defined for $\\alpha > 1$.\nIn our case, the posterior mean of $N_{e,g}$ is:\n$$E[N_{e,g} | \\text{data}] = \\frac{b'_g}{a'_g - 1} = \\frac{b_g + S_g}{a_g + m_g - 1}$$\nThe condition $a'_g > 1$ is satisfied for all test cases, since the given prior shape $a$ is greater than $1$ and the number of intervals per group $m_g$ is at least $1$.\n\n**Posterior Median and Credible Interval:**\nThese statistics are derived from the quantiles of the posterior distribution. There is a direct relationship between the quantiles of a Gamma distribution and its corresponding Inverse-Gamma distribution. Let $Q_Y(p)$ be the $p$-th quantile of $Y \\sim \\mathrm{InverseGamma}(\\alpha, \\beta)$ and $Q_X(p)$ be the $p$-th quantile of $X \\sim \\mathrm{Gamma}(\\alpha, \\beta)$. The relationship is:\n$$Q_Y(p) = \\frac{1}{Q_X(1-p)}$$\nWe apply this to find the median ($50$-th percentile) and the $95\\%$ central credible interval (endpoints at the $2.5$-th and $97.5$-th percentiles) for $N_{e,g}$. Let $Q_{\\lambda_g}(p)$ denote the $p$-th quantile of the posterior distribution $\\lambda_g | \\text{data} \\sim \\mathrm{Gamma}(a'_g, b'_g)$.\n\n-   **Posterior Median ($p=0.5$):**\n    $$\\text{Median}[N_{e,g}] = Q_{N_{e,g}}(0.5) = \\frac{1}{Q_{\\lambda_g}(1-0.5)} = \\frac{1}{Q_{\\lambda_g}(0.5)}$$\n-   **Lower Credible Bound ($p=0.025$):**\n    $$Q_{N_{e,g}}(0.025) = \\frac{1}{Q_{\\lambda_g}(1-0.025)} = \\frac{1}{Q_{\\lambda_g}(0.975)}$$\n-   **Upper Credible Bound ($p=0.975$):**\n    $$Q_{N_{e,g}}(0.975) = \\frac{1}{Q_{\\lambda_g}(1-0.975)} = \\frac{1}{Q_{\\lambda_g}(0.025)}$$\n\nThese quantiles of the Gamma distribution do not have a closed-form expression and must be computed numerically, for which scientific libraries like `scipy.stats` are used.\n\n### Discussion on Identifiability\nThe posterior parameters $a'_g = a_g + m_g$ and $b'_g = b_g + S_g$ explicitly show how the data, summarized by the sufficient statistics $m_g$ and $S_g$, update the prior beliefs encoded in $a_g$ and $b_g$. Identifiability of the population size $N_{e,g}$ for a given segment depends on the information content of the data within that segment.\n-   The statistic $m_g$, the number of coalescent events in group $g$, informs the shape of the posterior distribution. A larger $m_g$ leads to a posterior shape parameter $a'_g$ that is dominated by the data, making the inference less sensitive to the prior shape $a_g$.\n-   The statistic $S_g = \\sum_{i \\in g} \\binom{k_i}{2} t_i$, which is the sum of waiting times each scaled by the number of lineage pairs, informs the rate/scale of the posterior. A large $S_g$ value (indicating long waiting times) will dominate the prior rate $b_g$, pulling the posterior distribution of $\\lambda_g$ towards smaller values, and consequently, the posterior of $N_{e,g}$ towards larger values.\n-   **Edge Cases:** If a group contains very little information (e.g., $m_g$ is small, or all $t_i$ are very short, leading to a small $S_g$), the posterior distribution will be wide (high variance) and heavily influenced by the choice of prior ($a_g, b_g$). In such cases, $N_{e,g}$ is weakly identified. Conversely, when $m_g$ and $S_g$ are large, the likelihood dominates the prior, the posterior distribution becomes sharply peaked, and $N_{e,g}$ is strongly identified by the data. The problem setup ensures $m_g \\ge 1$ for all groups, so the parameters are always technically identifiable, but the precision of the estimate varies with the data content.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import gamma\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian skyline problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"t\": [0.05, 0.12, 0.08, 0.20, 0.30, 0.25],\n            \"k\": [10, 9, 8, 7, 6, 5],\n            \"group_sizes\": [2, 2, 2],\n            \"prior\": (2.5, 1.0)\n        },\n        {\n            \"t\": [0.02, 0.02, 0.02, 0.02],\n            \"k\": [50, 49, 48, 47],\n            \"group_sizes\": [1, 1, 2],\n            \"prior\": (2.1, 0.5)\n        },\n        {\n            \"t\": [0.30, 0.25, 0.35, 0.40],\n            \"k\": [6, 5, 4, 3],\n            \"group_sizes\": [4],\n            \"prior\": (1.5, 1.0)\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        case_results = bayesian_skyline_summary(\n            times=np.array(case[\"t\"]),\n            lineages=np.array(case[\"k\"]),\n            group_sizes=case[\"group_sizes\"],\n            prior_a=case[\"prior\"][0],\n            prior_b=case[\"prior\"][1]\n        )\n        all_results.append(case_results)\n\n    # Manual string formatting to match the required output format exactly.\n    case_strings = []\n    for case_res in all_results:\n        group_strings = []\n        for group_res in case_res:\n            num_strings = [f\"{x:.6f}\" for x in group_res]\n            group_strings.append(f\"[{','.join(num_strings)}]\")\n        case_strings.append(f\"[{','.join(group_strings)}]\")\n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    print(final_output)\n\ndef bayesian_skyline_summary(times, lineages, group_sizes, prior_a, prior_b):\n    \"\"\"\n    Computes the Bayesian skyline summary for a single test case.\n\n    For each group of coalescent intervals, it calculates the posterior mean,\n    median, and 95% central credible interval for the effective population size Ne.\n    \"\"\"\n    case_summary = []\n    \n    # Calculate start and end indices for each group\n    group_indices = np.cumsum([0] + group_sizes)\n    \n    for i in range(len(group_sizes)):\n        start_idx = group_indices[i]\n        end_idx = group_indices[i+1]\n        \n        # Slice data for the current group\n        group_t = times[start_idx:end_idx]\n        group_k = lineages[start_idx:end_idx]\n        \n        # Calculate sufficient statistics for the group\n        # m_g: number of intervals in group g\n        m_g = len(group_t)\n        \n        # S_g: sum of (k_i choose 2) * t_i\n        combs = group_k * (group_k - 1) / 2.0\n        s_g = np.sum(combs * group_t)\n        \n        # Calculate posterior Gamma parameters for lambda_g\n        # lambda_g ~ Gamma(a', b')\n        post_a = prior_a + m_g\n        post_b = prior_b + s_g\n        \n        # Calculate posterior statistics for Ne_g\n        # Ne_g = 1 / lambda_g, so Ne_g ~ InverseGamma(a', b')\n\n        # 1. Posterior Mean of Ne_g\n        # E[Ne_g] = b' / (a' - 1)\n        if post_a = 1:\n            # This case should not occur with the given problem constraints\n            # (a > 1, m_g >= 1), but is included for robustness. Mean is undefined.\n            post_mean_ne = np.nan\n        else:\n            post_mean_ne = post_b / (post_a - 1)\n            \n        # 2. Posterior Median and Credible Interval of Ne_g\n        # These are calculated using quantiles of the posterior Gamma distribution for lambda_g.\n        # Quantile of Ne_g(p) = 1 / Quantile of lambda_g(1-p)\n        \n        # scipy.stats.gamma is parameterized by shape 'a' and 'scale', where scale = 1 / rate.\n        # We want quantiles for Gamma(shape=post_a, rate=post_b).\n        gamma_quantiles = gamma.ppf([0.025, 0.5, 0.975], a=post_a, scale=1/post_b)\n        \n        q_025_lambda, q_50_lambda, q_975_lambda = gamma_quantiles\n        \n        # Median of Ne_g = 1 / median of lambda_g\n        post_median_ne = 1.0 / q_50_lambda\n        \n        # Lower bound of 95% CI for Ne_g = 1 / upper bound of 95% CI for lambda_g\n        lower_ci_ne = 1.0 / q_975_lambda\n        \n        # Upper bound of 95% CI for Ne_g = 1 / lower bound of 95% CI for lambda_g\n        upper_ci_ne = 1.0 / q_025_lambda\n        \n        group_summary = [post_mean_ne, post_median_ne, lower_ci_ne, upper_ci_ne]\n        case_summary.append(group_summary)\n        \n    return case_summary\n\nsolve()\n\n```", "id": "4667746"}]}