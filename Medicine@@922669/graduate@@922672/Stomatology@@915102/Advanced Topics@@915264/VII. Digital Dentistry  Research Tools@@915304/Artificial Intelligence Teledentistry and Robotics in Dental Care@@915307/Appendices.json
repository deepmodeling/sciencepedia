{"hands_on_practices": [{"introduction": "This first practice bridges the gap between abstract AI concepts and concrete clinical applications in orthodontics. We will explore how AI-driven systems can automate traditional cephalometric analysis by precisely calculating key diagnostic angles from 3D coordinate data [@problem_id:4694099]. This exercise reinforces fundamental vector geometry and demonstrates how computational methods provide rapid, objective insights, forming the basis for AI-assisted diagnosis and robotic guidance in teledentistry.", "problem": "An Artificial Intelligence (AI)-enabled teledentistry platform integrates robot-guided head positioning and Cone-Beam Computed Tomography (CBCT) to automatically locate cephalometric landmarks in a head-fixed, right-handed coordinate frame (units: millimeters). For a remote orthodontic assessment, the system returns the three-dimensional coordinates of Nasion, Point A, and Point B as $N = (120,\\, 90,\\, 95)$, $A = (130,\\, 90,\\, 95)$, and $B = (129.9756,\\, 89.3024,\\, 95)$. The A-point–Nasion–B-point (ANB) angle is defined as the smaller planar angle between the two rays $\\,\\overrightarrow{NA}\\,$ and $\\,\\overrightarrow{NB}\\,$ lying in the midsagittal plane.\n\nUsing first principles of vector geometry, compute the ANB angle from these coordinates, and then briefly interpret its clinical meaning for skeletal classification in your reasoning. Express the final angle in degrees and round your answer to $4$ significant figures. The final answer must be a single number with no units in the answer box.", "solution": "The problem is valid, scientifically grounded, and well-posed. It requires the computation of a planar angle between two vectors defined by three points in a three-dimensional Cartesian coordinate system. This is a standard application of vector geometry.\n\nThe A-point–Nasion–B-point (ANB) angle, denoted as $\\theta_{ANB}$, is the angle between the vector from Nasion ($N$) to Point A ($A$) and the vector from Nasion ($N$) to Point B ($B$). Let these vectors be $\\vec{u} = \\overrightarrow{NA}$ and $\\vec{v} = \\overrightarrow{NB}$.\n\nThe coordinates of the three cephalometric landmarks are given as:\n$N = (120,\\, 90,\\, 95)$\n$A = (130,\\, 90,\\, 95)$\n$B = (129.9756,\\, 89.3024,\\, 95)$\n\nFirst, we compute the components of the vectors $\\vec{u}$ and $\\vec{v}$ by subtracting the coordinates of the initial point ($N$) from the coordinates of the terminal points ($A$ and $B$).\n\nThe vector $\\vec{u} = \\overrightarrow{NA}$ is:\n$$ \\vec{u} = A - N = (130 - 120,\\, 90 - 90,\\, 95 - 95) = (10,\\, 0,\\, 0) $$\n\nThe vector $\\vec{v} = \\overrightarrow{NB}$ is:\n$$ \\vec{v} = B - N = (129.9756 - 120,\\, 89.3024 - 90,\\, 95 - 95) = (9.9756,\\, -0.6976,\\, 0) $$\n\nThe angle $\\theta$ between two non-zero vectors $\\vec{u}$ and $\\vec{v}$ can be determined using the dot product formula:\n$$ \\vec{u} \\cdot \\vec{v} = \\|\\vec{u}\\| \\|\\vec{v}\\| \\cos(\\theta) $$\nRearranging for $\\theta$, we get:\n$$ \\theta = \\arccos\\left(\\frac{\\vec{u} \\cdot \\vec{v}}{\\|\\vec{u}\\| \\|\\vec{v}\\|}\\right) $$\n\nNext, we calculate the dot product, $\\vec{u} \\cdot \\vec{v}$:\n$$ \\vec{u} \\cdot \\vec{v} = (10)(9.9756) + (0)(-0.6976) + (0)(0) = 99.756 $$\n\nNow, we calculate the magnitudes (Euclidean norms) of the vectors, $\\|\\vec{u}\\|$ and $\\|\\vec{v}\\|$.\nThe magnitude of $\\vec{u}$ is:\n$$ \\|\\vec{u}\\| = \\sqrt{10^2 + 0^2 + 0^2} = \\sqrt{100} = 10 $$\n\nThe magnitude of $\\vec{v}$ is:\n$$ \\|\\vec{v}\\| = \\sqrt{(9.9756)^2 + (-0.6976)^2 + 0^2} = \\sqrt{99.51261536 + 0.48664576} = \\sqrt{99.99926112} $$\n\nNow, we substitute these values into the arccosine formula to find the angle in radians:\n$$ \\theta = \\arccos\\left(\\frac{99.756}{10 \\sqrt{99.99926112}}\\right) $$\n$$ \\theta \\approx \\arccos\\left(\\frac{99.756}{10 \\times 9.999963056}\\right) \\approx \\arccos\\left(\\frac{9.9756}{9.999963056}\\right) $$\n$$ \\theta \\approx \\arccos(0.997563663) \\approx 0.0698188 \\text{ radians} $$\n\nThe problem requires the angle to be in degrees. We convert from radians to degrees using the conversion factor $\\frac{180}{\\pi}$:\n$$ \\theta_{\\text{deg}} = \\theta_{\\text{rad}} \\times \\frac{180}{\\pi} \\approx 0.0698188 \\times \\frac{180}{\\pi} \\approx 4.000325^{\\circ} $$\n\nFinally, we round the result to $4$ significant figures as requested. The value is $4.000325...$. The first four significant figures are $4$, $0$, $0$, and $0$. The fifth digit is $3$, which is less than $5$, so we round down.\n$$ \\theta_{ANB} \\approx 4.000^{\\circ} $$\n\nFor clinical interpretation, the ANB angle is a critical measurement in orthodontics for assessing the sagittal (anteroposterior) relationship between the maxilla (upper jaw) and mandible (lower jaw).\n- A value of approximately $2^{\\circ}$ is considered to represent a normal, or Class I, skeletal relationship.\n- An angle significantly greater than $2^{\\circ}$, such as the computed $4.000^{\\circ}$, indicates a Skeletal Class II malocclusion. This suggests that the maxilla is positioned anteriorly relative to the mandible, or the mandible is positioned posteriorly relative to the maxilla, which is often associated with a convex facial profile and an increased overjet.\n- A value significantly less than $2^{\\circ}$ (including negative values) would indicate a Skeletal Class III relationship, characteristic of a prognathic mandible.\n\nThus, the calculated ANB angle of $4.000^{\\circ}$ suggests a Skeletal Class II jaw relationship for this patient.", "answer": "$$\\boxed{4.000}$$", "id": "4694099"}, {"introduction": "The performance of any supervised AI model is fundamentally limited by the quality of its training data. This exercise delves into the critical process of creating a reliable \"gold standard\" by quantifying the agreement between expert annotators using a core statistical measure, the Cohen’s $\\kappa$ coefficient [@problem_id:4694101]. By working through this calculation, you will gain hands-on experience in assessing the consistency of diagnostic labels, a crucial step for building trustworthy and robust AI systems for dental applications.", "problem": "A teledentistry program is building an Artificial Intelligence (AI) system to triage dental radiographs for remote care in stomatology. Three board-certified experts independently annotate a shared set of $N=120$ tooth sites on intraoral images into three nominal categories: sound enamel ($S$), cavitated caries ($C$), and periapical pathology ($P$). To assess the quality of the gold-standard annotations, interrater reliability is quantified using the Cohen’s $\\kappa$ definition derived from observed agreement and chance agreement. For a given rater pair $(X,Y)$ over $K$ categories with total $N$ items, the observed agreement is the empirical proportion of identical labels across items, and the expected chance agreement is the probability of identical labels if $X$ and $Y$ label items independently following their respective marginal category frequencies.\n\nYou are given the pairwise $3 \\times 3$ contingency matrices of counts (rows are the categories chosen by the first rater in the pair, columns are the categories chosen by the second rater in the pair):\n\n$$M_{AB}=\\begin{pmatrix}\n50  6  4\\\\\n3  30  7\\\\\n2  9  9\n\\end{pmatrix},\\quad\nM_{AC}=\\begin{pmatrix}\n48  8  4\\\\\n6  28  6\\\\\n4  6  10\n\\end{pmatrix},\\quad\nM_{BC}=\\begin{pmatrix}\n47  6  2\\\\\n7  26  12\\\\\n4  10  6\n\\end{pmatrix}.$$\n\nAll matrices are computed over the same $N=120$ items and share marginals consistent with each rater’s category distribution across the three pairings. Using only the core definition of Cohen’s $\\kappa$, compute the aggregated interrater reliability by macro-averaging the pairwise $\\kappa$ values across the three rater pairs $(A,B)$, $(A,C)$, and $(B,C)$. Express your final answer as a decimal number, and round to four significant figures. Do not append a percentage sign.", "solution": "The problem requires the calculation of the aggregated interrater reliability by macro-averaging the pairwise Cohen's $\\kappa$ values for three rater pairs $(A,B)$, $(A,C)$, and $(B,C)$. The analysis is based on three given $3 \\times 3$ contingency matrices over a total of $N=120$ items and $K=3$ categories.\n\nThe core definition of Cohen's $\\kappa$ is given by the formula:\n$$ \\kappa = \\frac{p_o - p_e}{1 - p_e} $$\nwhere $p_o$ is the observed proportional agreement and $p_e$ is the expected proportional agreement by chance.\n\nFor a contingency matrix $M = (m_{ij})$ of size $K \\times K$, where $m_{ij}$ is the number of items assigned to category $i$ by the first rater and category $j$ by the second rater, the total number of items is $N = \\sum_{i=1}^K \\sum_{j=1}^K m_{ij}$.\n\nThe observed agreement, $p_o$, is the proportion of items on which the raters agree. It is calculated as the sum of the diagonal elements of the contingency matrix divided by the total number of items:\n$$ p_o = \\frac{\\sum_{i=1}^K m_{ii}}{N} = \\frac{\\text{Tr}(M)}{N} $$\n\nThe expected chance agreement, $p_e$, is the probability of agreement if the raters made their judgments independently. Let $r_i = \\sum_{j=1}^K m_{ij}$ be the total number of items the first rater assigned to category $i$ (row $i$ sum), and $c_j = \\sum_{i=1}^K m_{ij}$ be the total number of items the second rater assigned to category $j$ (column $j$ sum). The probability of both raters independently choosing category $k$ is the product of their marginal probabilities for that category. Summing over all categories:\n$$ p_e = \\sum_{k=1}^K \\frac{r_k}{N} \\cdot \\frac{c_k}{N} = \\frac{1}{N^2} \\sum_{k=1}^K r_k c_k $$\n\nWe will compute $\\kappa$ for each of the three rater pairs and then find their arithmetic mean. The total number of items is $N=120$.\n\n**1. Calculation for Rater Pair (A, B)**\nThe contingency matrix is $M_{AB} = \\begin{pmatrix} 50  6  4\\\\ 3  30  7\\\\ 2  9  9 \\end{pmatrix}$.\nThe observed agreement is:\n$$ p_{o,AB} = \\frac{50 + 30 + 9}{120} = \\frac{89}{120} $$\nThe marginal totals for rater A (rows) are $r_1 = 60$, $r_2 = 40$, $r_3 = 20$.\nThe marginal totals for rater B (columns) are $c_1 = 55$, $c_2 = 45$, $c_3 = 20$.\nThe expected chance agreement is:\n$$ p_{e,AB} = \\frac{(60 \\times 55) + (40 \\times 45) + (20 \\times 20)}{120^2} = \\frac{3300 + 1800 + 400}{14400} = \\frac{5500}{14400} = \\frac{55}{144} $$\nNow, we compute $\\kappa_{AB}$:\n$$ \\kappa_{AB} = \\frac{\\frac{89}{120} - \\frac{55}{144}}{1 - \\frac{55}{144}} = \\frac{\\frac{89 \\times 6 - 55 \\times 5}{720}}{\\frac{144 - 55}{144}} = \\frac{\\frac{534-275}{720}}{\\frac{89}{144}} = \\frac{\\frac{259}{720}}{\\frac{89}{144}} = \\frac{259}{720} \\times \\frac{144}{89} = \\frac{259}{5 \\times 89} = \\frac{259}{445} $$\n\n**2. Calculation for Rater Pair (A, C)**\nThe contingency matrix is $M_{AC} = \\begin{pmatrix} 48  8  4\\\\ 6  28  6\\\\ 4  6  10 \\end{pmatrix}$.\nThe observed agreement is:\n$$ p_{o,AC} = \\frac{48 + 28 + 10}{120} = \\frac{86}{120} = \\frac{43}{60} $$\nThe marginal totals for rater A (rows) are $r_1=60$, $r_2=40$, $r_3=20$.\nThe marginal totals for rater C (columns) are $c_1 = 58$, $c_2 = 42$, $c_3 = 20$.\nThe expected chance agreement is:\n$$ p_{e,AC} = \\frac{(60 \\times 58) + (40 \\times 42) + (20 \\times 20)}{120^2} = \\frac{3480 + 1680 + 400}{14400} = \\frac{5560}{14400} = \\frac{139}{360} $$\nNow, we compute $\\kappa_{AC}$:\n$$ \\kappa_{AC} = \\frac{\\frac{43}{60} - \\frac{139}{360}}{1 - \\frac{139}{360}} = \\frac{\\frac{43 \\times 6 - 139}{360}}{\\frac{360 - 139}{360}} = \\frac{\\frac{258 - 139}{360}}{\\frac{221}{360}} = \\frac{119}{221} = \\frac{7 \\times 17}{13 \\times 17} = \\frac{7}{13} $$\n\n**3. Calculation for Rater Pair (B, C)**\nThe contingency matrix is $M_{BC} = \\begin{pmatrix} 47  6  2\\\\ 7  26  12\\\\ 4  10  6 \\end{pmatrix}$.\nThe observed agreement is:\n$$ p_{o,BC} = \\frac{47 + 26 + 6}{120} = \\frac{79}{120} $$\nThe marginal totals for rater B (rows) are $r_1=55$, $r_2=45$, $r_3=20$.\nThe marginal totals for rater C (columns) are $c_1=58$, $c_2=42$, $c_3=20$.\nThe expected chance agreement is:\n$$ p_{e,BC} = \\frac{(55 \\times 58) + (45 \\times 42) + (20 \\times 20)}{120^2} = \\frac{3190 + 1890 + 400}{14400} = \\frac{5480}{14400} = \\frac{137}{360} $$\nNow, we compute $\\kappa_{BC}$:\n$$ \\kappa_{BC} = \\frac{\\frac{79}{120} - \\frac{137}{360}}{1 - \\frac{137}{360}} = \\frac{\\frac{79 \\times 3 - 137}{360}}{\\frac{360 - 137}{360}} = \\frac{\\frac{237 - 137}{360}}{\\frac{223}{360}} = \\frac{100}{223} $$\n\n**4. Macro-Averaging the Pairwise $\\kappa$ Values**\nThe final step is to compute the arithmetic mean (macro-average) of the three pairwise $\\kappa$ values:\n$$ \\bar{\\kappa} = \\frac{\\kappa_{AB} + \\kappa_{AC} + \\kappa_{BC}}{3} = \\frac{1}{3} \\left( \\frac{259}{445} + \\frac{7}{13} + \\frac{100}{223} \\right) $$\nTo obtain the final numerical answer, we convert the fractions to decimals:\n$$ \\kappa_{AB} = \\frac{259}{445} \\approx 0.58202247 $$\n$$ \\kappa_{AC} = \\frac{7}{13} \\approx 0.53846154 $$\n$$ \\kappa_{BC} = \\frac{100}{223} \\approx 0.44843049 $$\n$$ \\bar{\\kappa} \\approx \\frac{0.58202247 + 0.53846154 + 0.44843049}{3} = \\frac{1.5689145}{3} \\approx 0.5229715 $$\nRounding the result to four significant figures gives $0.5230$.", "answer": "$$\\boxed{0.5230}$$", "id": "4694101"}, {"introduction": "To train a neural network effectively, we must define a loss function that accurately reflects the task's goal and guides the model's learning process. This problem examines the Sørensen–Dice loss, a function widely used for medical image segmentation tasks like identifying dental lesions, where the target is often very small compared to the background [@problem_id:4694116]. Deriving its gradient will reveal the mathematical mechanism that makes this loss function particularly effective for handling the class imbalance common in medical imaging, ensuring the AI can learn to identify even subtle pathologies.", "problem": "A teledentistry platform integrates Convolutional Neural Networks (CNN) for automated segmentation of periapical lesions in cone-beam computed tomography (CBCT) images to guide a robotic endodontic access system. For a single two-dimensional slice, the segmentation network outputs per-pixel predictions $\\{p_{i}\\}_{i=1}^{N}$ with $p_{i} \\in [0,1]$, and the ground-truth labels are $\\{g_{i}\\}_{i=1}^{N}$ with $g_{i} \\in \\{0,1\\}$, where $N$ is the total number of pixels in the slice. The training objective uses the Sørensen–Dice loss\n$$\nL \\;=\\; 1 \\;-\\; \\frac{2 \\sum_{j=1}^{N} p_{j} g_{j}}{\\sum_{j=1}^{N} p_{j} \\;+\\; \\sum_{j=1}^{N} g_{j}}.\n$$\nStarting from the fundamental definitions of partial differentiation and the quotient rule from elementary calculus, derive the analytic expression for the partial derivative $\\frac{\\partial L}{\\partial p_{i}}$ with respect to the prediction $p_{i}$ for a single pixel index $i$. Then, using the derived expression, explain in scientific terms how the Dice loss inherently addresses class imbalance between lesion and background pixels in dental images (for example, when the number of lesion pixels is small relative to background). Your final answer must be the closed-form analytic expression for $\\frac{\\partial L}{\\partial p_{i}}$; no numerical approximation is required and no units apply.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It provides a complete and consistent setup for a standard mathematical derivation within the context of machine learning for medical imaging. The task is to derive the partial derivative of the Sørensen–Dice loss function and explain its utility in handling class imbalance. All necessary definitions are provided, and no scientific principles are violated. Therefore, the problem is valid, and a solution can be constructed.\n\nThe Sørensen–Dice loss function, $L$, is given by:\n$$\nL = 1 - \\frac{2 \\sum_{j=1}^{N} p_{j} g_{j}}{\\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j}}\n$$\nwhere $\\{p_j\\}_{j=1}^N$ are the predicted probabilities, $\\{g_j\\}_{j=1}^N$ are the ground-truth binary labels, and $N$ is the total number of pixels. We are tasked with finding the partial derivative of $L$ with respect to the prediction $p_{i}$ for a single arbitrary pixel $i$.\n\nLet us define the Dice coefficient, $D$, as the fraction term:\n$$\nD = \\frac{2 \\sum_{j=1}^{N} p_{j} g_{j}}{\\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j}}\n$$\nThus, the loss is $L = 1 - D$. The partial derivative of $L$ with respect to $p_i$ is:\n$$\n\\frac{\\partial L}{\\partial p_{i}} = \\frac{\\partial}{\\partial p_{i}}(1 - D) = - \\frac{\\partial D}{\\partial p_{i}}\n$$\nTo calculate $\\frac{\\partial D}{\\partial p_{i}}$, we apply the quotient rule from elementary calculus. Let $f(x) = \\frac{u(x)}{v(x)}$, then $f'(x) = \\frac{v(x)u'(x) - u(x)v'(x)}{[v(x)]^2}$.\nFor our function $D$, the variable is $p_i$. Let us define the numerator and denominator functions:\n$$\nU(p_1, \\dots, p_N) = 2 \\sum_{j=1}^{N} p_{j} g_{j}\n$$\n$$\nV(p_1, \\dots, p_N) = \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j}\n$$\nNext, we find the partial derivatives of $U$ and $V$ with respect to $p_i$.\n\nFor the numerator $U$, the summation $\\sum_{j=1}^{N} p_{j} g_{j}$ is a sum of terms $p_1 g_1 + p_2 g_2 + \\dots + p_i g_i + \\dots + p_N g_N$. When we differentiate this sum with respect to $p_i$, only the term containing $p_i$ will yield a non-zero result. The derivative of $p_i g_i$ with respect to $p_i$ is $g_i$, as $g_i$ is a constant. Therefore:\n$$\n\\frac{\\partial U}{\\partial p_{i}} = \\frac{\\partial}{\\partial p_{i}} \\left( 2 \\sum_{j=1}^{N} p_{j} g_{j} \\right) = 2 g_{i}\n$$\nFor the denominator $V$, the term $\\sum_{j=1}^{N} g_{j}$ is a constant with respect to any $p_k$. The term $\\sum_{j=1}^{N} p_{j}$ is the sum $p_1 + p_2 + \\dots + p_i + \\dots + p_N$. Its partial derivative with respect to $p_i$ is $1$. Thus:\n$$\n\\frac{\\partial V}{\\partial p_{i}} = \\frac{\\partial}{\\partial p_{i}} \\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right) = 1 + 0 = 1\n$$\nNow we can substitute these results into the quotient rule to find $\\frac{\\partial D}{\\partial p_{i}}$:\n$$\n\\frac{\\partial D}{\\partial p_{i}} = \\frac{V \\frac{\\partial U}{\\partial p_{i}} - U \\frac{\\partial V}{\\partial p_{i}}}{V^2} = \\frac{\\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right)(2 g_{i}) - \\left( 2 \\sum_{j=1}^{N} p_{j} g_{j} \\right)(1)}{\\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right)^2}\n$$\nFinally, we compute $\\frac{\\partial L}{\\partial p_{i}} = - \\frac{\\partial D}{\\partial p_{i}}$:\n$$\n\\frac{\\partial L}{\\partial p_{i}} = - \\frac{2 g_{i} \\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right) - 2 \\sum_{j=1}^{N} p_{j} g_{j}}{\\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right)^2}\n$$\nRearranging the terms in the numerator gives the final analytic expression:\n$$\n\\frac{\\partial L}{\\partial p_{i}} = \\frac{2 \\sum_{j=1}^{N} p_{j} g_{j} - 2 g_{i} \\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right)}{\\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right)^2}\n$$\nThis expression is the gradient component used in backpropagation to update the network weights based on the prediction for pixel $i$.\n\nThe scientific explanation for how the Dice loss addresses class imbalance lies in the structure of this gradient. In typical dental imaging, the number of lesion pixels (foreground, $g_j=1$) is far smaller than the number of healthy tissue pixels (background, $g_j=0$).\n\nLet's compare this to a standard per-pixel loss like binary cross-entropy (BCE), where the gradient for pixel $i$ depends only on the local prediction $p_i$ and label $g_i$. In a highly imbalanced dataset, the total loss and the overall gradient are dominated by the vast number of background pixels. Even if these pixels are classified correctly with high confidence (e.g., $p_i \\approx 0$ for $g_i=0$), their sheer quantity can overwhelm the learning signal from the few foreground pixels.\n\nThe Dice loss gradient, however, is fundamentally different. The numerator and denominator contain sums over *all* pixels in the image. This means the gradient for any single pixel $p_i$ is a function of the global state of the segmentation. It is not an independent, local quantity.\n\nLet's analyze the gradient for a background pixel ($g_i=0$):\n$$\n\\frac{\\partial L}{\\partial p_{i}} \\bigg|_{g_i=0} = \\frac{2 \\sum_{j=1}^{N} p_{j} g_{j}}{\\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right)^2}\n$$\nThis gradient is a function of the \"soft\" true positive count in the numerator, normalized by the squared sum of the \"soft\" predicted positive area and the true positive area. The contribution of each background pixel to the total gradient is modulated by the model's overall performance on the foreground class. If the foreground is small and poorly detected ($\\sum p_j g_j$ is small), the gradient updates for background pixels are also small. This prevents the easy-to-classify background pixels from generating large, distracting gradients.\n\nConversely, for a foreground pixel ($g_i=1$), the gradient has a more complex numerator: $2(\\sum p_j g_j - \\sum p_j - \\sum g_j)$. This term will be strongly negative (pushing $p_i$ towards $1$) if the overall lesion is being missed.\n\nIn essence, the Dice loss frames the optimization problem not as an aggregation of independent pixel-wise classification tasks, but as a holistic task of maximizing the overlap between the predicted and ground-truth foreground regions. The gradient for each pixel reflects how a change in its prediction would affect this global overlap score. By normalizing with respect to the sizes of the predicted and true positive sets, the loss function inherently balances the learning process, ensuring that the rare foreground class contributes a substantial training signal, thereby mitigating the class imbalance problem.", "answer": "$$\n\\boxed{\\frac{2 \\sum_{j=1}^{N} p_{j} g_{j} - 2 g_{i} \\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right)}{\\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right)^{2}}}\n$$", "id": "4694116"}]}