{"hands_on_practices": [{"introduction": "Visual acuity is the most common measure of visual function, but it is expressed in several different notations. This foundational exercise focuses on the essential skill of converting between the clinical Snellen fraction, the decimal acuity, and the psychophysically robust logMAR (logarithm of the Minimum Angle of Resolution) scale. By performing these conversions and calculating the physical size of a standard optotype, you will solidify your understanding of the geometric and mathematical principles that underpin all visual acuity testing [@problem_id:4733066].", "problem": "A patient’s distance visual acuity is recorded at a standard testing distance using the Snellen notation $20/40$. In clinical visual psychophysics, the decimal visual acuity is defined by the ratio of testing distance to the “optotype design distance” (the denominator of the Snellen fraction), the minimum angle of resolution (MAR) is defined as the smallest resolvable feature in the optotype expressed in arcminutes and is the reciprocal of the decimal acuity, and the logarithm of the minimum angle of resolution (logMAR) is defined as $\\log_{10}(\\mathrm{MAR})$. Separately, the geometric visual angle $\\theta$ subtended by an object of physical height $H$ at viewing distance $d$ follows the small-angle optical geometry of the eye, which can be derived from first principles of Euclidean geometry: the full subtended angle satisfies $\\theta = 2 \\arctan\\!\\big(H/(2d)\\big)$, and for sufficiently small angles one may approximate $\\theta \\approx H/d$ after converting $\\theta$ to radians. Assume the standard optotype design in which the overall letter height subtends $5$ arcminutes.\n\nStarting from these core definitions and geometric relations, perform the following for the given Snellen acuity $20/40$ and viewing distance $d = 4\\,\\mathrm{m}$:\n\n1. Convert the Snellen acuity $20/40$ to logMAR, where $\\log_{10}$ denotes base-$10$ logarithm.\n2. State the MAR in arcminutes.\n3. Compute the physical letter height $H$ in millimeters required at $d=4\\,\\mathrm{m}$ to subtend an overall angle of $5$ arcminutes (use the exact geometric relation $\\theta = 2 \\arctan\\!\\big(H/(2d)\\big)$ and convert the angle to radians for calculation; do not use the small-angle approximation in the final numerical computation).\n\nRound the logMAR and the letter height to four significant figures. Express the MAR as a real number in arcminutes without rounding. Report your final answer as three quantities in the order: logMAR (dimensionless), MAR (in arcminutes), and letter height (in millimeters).", "solution": "The Snellen notation $20/40$ encodes the ratio of testing distance to the “design distance” of the optotype line. The decimal visual acuity $A_{\\mathrm{dec}}$ is defined by\n$$\nA_{\\mathrm{dec}} = \\frac{20}{40} = \\frac{1}{2} = 0.5.\n$$\nBy definition in visual psychophysics, the minimum angle of resolution (MAR) is the smallest resolvable feature size in angular units (arcminutes), and is the reciprocal of decimal acuity:\n$$\n\\mathrm{MAR} = \\frac{1}{A_{\\mathrm{dec}}} = \\frac{1}{0.5} = 2 \\text{ arcminutes}.\n$$\nThe logarithm of the minimum angle of resolution (logMAR) is defined as\n$$\n\\mathrm{logMAR} = \\log_{10}(\\mathrm{MAR}) = \\log_{10}(2).\n$$\nSymbolically, $\\log_{10}(2)$ is exact; to report a numerical value to four significant figures, evaluate\n$$\n\\log_{10}(2) \\approx 0.3010 \\quad \\text{(four significant figures)}.\n$$\n\nFor the physical letter height, use the exact geometric relationship between height $H$, viewing distance $d$, and full subtended angle $\\theta$:\n$$\n\\theta = 2 \\arctan\\!\\left(\\frac{H}{2 d}\\right).\n$$\nWe are given that the standard overall letter height should subtend $\\theta = 5$ arcminutes. To use trigonometric functions consistently, convert $\\theta$ to radians. There are $60$ arcminutes in $1$ degree and $\\pi$ radians in $180$ degrees, so\n$$\n\\theta_{\\mathrm{rad}} = 5 \\times \\frac{1}{60} \\times \\frac{\\pi}{180} = \\frac{5 \\pi}{10800} \\text{ radians}.\n$$\nSolving the exact relation for $H$ gives\n$$\n\\theta = 2 \\arctan\\!\\left(\\frac{H}{2 d}\\right) \\;\\Rightarrow\\; \\arctan\\!\\left(\\frac{H}{2 d}\\right) = \\frac{\\theta}{2} \\;\\Rightarrow\\; \\frac{H}{2 d} = \\tan\\!\\left(\\frac{\\theta}{2}\\right) \\;\\Rightarrow\\; H = 2 d \\, \\tan\\!\\left(\\frac{\\theta}{2}\\right).\n$$\nSubstitute $d = 4\\,\\mathrm{m}$ and $\\theta_{\\mathrm{rad}} = \\frac{5 \\pi}{10800}$:\n$$\nH = 2 \\times 4 \\times \\tan\\!\\left(\\frac{1}{2} \\cdot \\frac{5 \\pi}{10800}\\right) \\,\\mathrm{m} = 8 \\, \\tan\\!\\left(\\frac{5 \\pi}{21600}\\right) \\,\\mathrm{m}.\n$$\nTo express the height in millimeters, multiply by $1000$:\n$$\nH_{\\mathrm{mm}} = 1000 \\times 8 \\, \\tan\\!\\left(\\frac{5 \\pi}{21600}\\right) = 8000 \\, \\tan\\!\\left(\\frac{5 \\pi}{21600}\\right) \\;\\text{mm}.\n$$\nNumerically, since $\\frac{5 \\pi}{21600}$ is a small angle, we still use the exact $\\tan$ for computation and then round:\n$$\n\\frac{5 \\pi}{21600} \\approx 7.27220521664304 \\times 10^{-4} \\text{ radians},\n$$\nso\n$$\nH_{\\mathrm{mm}} = 8000 \\, \\tan\\!\\left(7.27220521664304 \\times 10^{-4}\\right) \\,\\text{mm}.\n$$\nEvaluating $\\tan$ and multiplying:\n$$\nH_{\\mathrm{mm}} \\approx 5.81776 \\,\\text{mm}.\n$$\nRounded to four significant figures:\n$$\nH_{\\mathrm{mm}} \\approx 5.818 \\,\\text{mm}.\n$$\n\nCollecting results in the required order:\n- logMAR (dimensionless): $\\log_{10}(2) \\approx 0.3010$ (four significant figures),\n- MAR: $2$ arcminutes (exact real number, no rounding),\n- Letter height at $d=4\\,\\mathrm{m}$ to subtend $5$ arcminutes: $5.818$ millimeters (four significant figures).", "answer": "$$\\boxed{\\begin{pmatrix}0.3010  2  5.818\\end{pmatrix}}$$", "id": "4733066"}, {"introduction": "Beyond central acuity, assessing the peripheral visual field is critical for diagnosing conditions like glaucoma. This practice explores the principle of spatial summation, which governs how the retina integrates light to detect a stimulus. You will apply an established psychophysical model of partial summation to predict how the detection threshold in static perimetry changes with stimulus size, a key concept for interpreting visual field maps and understanding the design of perimetric tests [@problem_id:4733089].", "problem": "A laboratory is conducting static automated perimetry to compare detection thresholds for two standard Goldmann stimulus sizes at a peripheral retinal location. At an eccentricity of $30^\\circ$ in the horizontal meridian under a steady photopic background, both Goldmann size III and Goldmann size V stimuli are known to lie outside complete spatial summation. The experimenters wish to estimate how much the differential luminance threshold (the luminance increment above background required for detection) changes when switching from Goldmann size III to Goldmann size V, assuming spatial partial summation at this eccentricity.\n\nUse the following well-tested facts as the foundational base:\n- For spatial summation, the differential luminance threshold $T$ depends on stimulus area $A$. Under complete spatial summation (Ricco’s law), $T \\propto A^{-1}$. Beyond the critical area (partial summation), empirical data at peripheral locations are well fitted by a power law $T \\propto A^{-\\beta}$ with $0\\beta1$ (Piper-type behavior). At $30^\\circ$ eccentricity under photopic backgrounds, a widely observed slope is $\\beta = \\tfrac{1}{2}$.\n- The standard angular diameters of Goldmann size III and Goldmann size V are $0.43^\\circ$ and $1.72^\\circ$, respectively.\n- For circular stimuli, area scales with the square of angular diameter.\n- A change in threshold expressed in decibels (dB) is defined as $10 \\log_{10}\\!\\left(\\dfrac{T_{2}}{T_{1}}\\right)$, where $T_{1}$ and $T_{2}$ are the thresholds being compared.\n\nStarting from these bases, derive and compute the expected signed change in differential luminance threshold, in decibels, when moving from Goldmann size III to Goldmann size V at $30^\\circ$ eccentricity under partial summation. A negative value indicates that a lower luminance increment is required for detection with the larger stimulus. Round your final answer to four significant figures and express it in dB (decibels).", "solution": "### Solution Derivation\nLet $T_{III}$ and $T_V$ be the differential luminance thresholds for the Goldmann size III and Goldmann size V stimuli, respectively. Let their corresponding areas be $A_{III}$ and $A_V$, and their diameters be $d_{III}$ and $d_V$.\n\nThe problem states that under conditions of partial summation, the threshold $T$ follows a power law with respect to the stimulus area $A$:\n$$T \\propto A^{-\\beta}$$\nThis can be written as an equation $T = k A^{-\\beta}$, where $k$ is a constant of proportionality.\n\nThe ratio of the thresholds for the two stimuli is therefore:\n$$\\frac{T_V}{T_{III}} = \\frac{k A_V^{-\\beta}}{k A_{III}^{-\\beta}} = \\left(\\frac{A_V}{A_{III}}\\right)^{-\\beta}$$\n\nThe stimuli are circular, so their area $A$ is proportional to the square of their diameter $d$:\n$$A \\propto d^2$$\nThis implies that the ratio of the areas can be expressed in terms of the ratio of their diameters:\n$$\\frac{A_V}{A_{III}} = \\frac{c d_V^2}{c d_{III}^2} = \\left(\\frac{d_V}{d_{III}}\\right)^2$$\nwhere $c$ is a geometric constant (specifically, $c=\\frac{\\pi}{4}$ for circular area, but its value is irrelevant as it cancels out).\n\nSubstituting this area ratio back into the threshold ratio equation yields:\n$$\\frac{T_V}{T_{III}} = \\left(\\left(\\frac{d_V}{d_{III}}\\right)^2\\right)^{-\\beta} = \\left(\\frac{d_V}{d_{III}}\\right)^{-2\\beta}$$\n\nThe problem specifies that at the given retinal location and background, the exponent is $\\beta = \\frac{1}{2}$. Substituting this value:\n$$\\frac{T_V}{T_{III}} = \\left(\\frac{d_V}{d_{III}}\\right)^{-2\\left(\\frac{1}{2}\\right)} = \\left(\\frac{d_V}{d_{III}}\\right)^{-1} = \\frac{d_{III}}{d_V}$$\n\nThe given diameters are $d_{III} = 0.43^\\circ$ and $d_V = 1.72^\\circ$. We can now compute the ratio of the diameters.\n$$\\frac{d_V}{d_{III}} = \\frac{1.72}{0.43} = 4$$\nThis implies that the threshold ratio is:\n$$\\frac{T_V}{T_{III}} = \\frac{1}{4} = 0.25$$\n\nThe change in threshold in decibels, $\\Delta_{\\text{dB}}$, when moving from size III (stimulus $1$) to size V (stimulus $2$) is defined as:\n$$\\Delta_{\\text{dB}} = 10 \\log_{10}\\left(\\frac{T_V}{T_{III}}\\right)$$\nSubstituting the value of the threshold ratio:\n$$\\Delta_{\\text{dB}} = 10 \\log_{10}(0.25)$$\n\nTo compute the numerical value:\n$$\\Delta_{\\text{dB}} = 10 \\log_{10}\\left(\\frac{1}{4}\\right) = 10 \\log_{10}(4^{-1}) = -10 \\log_{10}(4)$$\nUsing the logarithm property $\\log(x^y) = y \\log(x)$:\n$$\\Delta_{\\textdB} = -10 \\log_{10}(2^2) = -20 \\log_{10}(2)$$\n\nUsing a high-precision value for $\\log_{10}(2) \\approx 0.30102999566$:\n$$\\Delta_{\\text{dB}} \\approx -20 \\times 0.30102999566 = -6.0205999132$$\nThe problem requires the answer to be rounded to four significant figures. The value is $-6.02059...$. The fourth significant digit is $0$, and the following digit is $5$, which requires rounding up. Therefore, the value rounds to $-6.021$.\n\nThe negative sign indicates that the threshold for the Goldmann size V stimulus is lower than for the size III stimulus, which is consistent with the principle that larger stimuli are easier to detect.", "answer": "$$\n\\boxed{-6.021}\n$$", "id": "4733089"}, {"introduction": "The Contrast Sensitivity Function (CSF) offers a far more complete picture of spatial vision than a single acuity measurement. This advanced, hands-on coding practice puts you in the role of a vision scientist, fitting a standard mathematical model to CSF data. By deriving and implementing a least-squares estimation procedure, you will learn a powerful technique for extracting meaningful parameters, such as peak sensitivity and peak frequency, from noisy experimental measurements, a core skill in modern psychophysical research [@problem_id:4733125].", "problem": "You are given a parametric model for the human Contrast Sensitivity Function (CSF), a standard model used in visual function testing in ophthalmology. The model describes sensitivity as a function of spatial frequency and is defined as follows: for spatial frequency $f$ (in cycles per degree), the sensitivity $S(f)$ is\n$$\nS(f) = S_0 \\exp\\!\\left(-\\alpha \\ln^2\\!\\left(\\frac{f}{f_0}\\right)\\right),\n$$\nwhere $S_0$ is the peak sensitivity (unitless), $f_0$ is the peak frequency (in cycles per degree), and $\\alpha$ is a known positive curvature parameter. All logarithms are natural logarithms.\n\nYour task is to design and implement a program that, for each dataset, estimates the peak sensitivity $S_0$ and the peak frequency $f_0$ by principled fitting from first principles. Assume that measurement noise in sensitivity is multiplicative (equivalently, additive in log-sensitivity), independent across frequencies, and approximately Gaussian in the log domain. Under this assumption, estimate $(S_0, f_0)$ by minimizing the sum of squared residuals between observed and model-predicted log-sensitivities. Constrain the estimates to $S_0  0$ and $f_0  0$. Use only the given data and fundamental principles; you must not assume or use any pre-derived closed-form \"shortcuts\" not derived from these principles.\n\nImportant requirements:\n- Use the natural logarithm throughout.\n- Report $f_0$ in cycles per degree (cpd).\n- Report $S_0$ as a unitless quantity.\n- Round $S_0$ to two decimal places and $f_0$ to three decimal places in the final output.\n\nTest suite (each test case provides $\\alpha$, spatial frequencies $f_i$ in cycles per degree, and observed sensitivities $S_i$):\n- Test Case 1 (noise-free, symmetric sampling around the peak):\n  - $\\alpha = 0.4$\n  - $f = [0.5, 1, 2, 4, 8, 16]$ (in cpd)\n  - $S = [44.33, 115.89, 206.35, 250.00, 206.35, 115.89]$\n- Test Case 2 (noisy, dense sampling around the peak and in the tails):\n  - $\\alpha = 0.5$\n  - $f = [0.5, 1, 2, 3, 3.5, 5, 7, 10, 16]$ (in cpd)\n  - $S = [34.77, 95.41, 191.91, 210.88, 220.00, 214.71, 166.13, 114.18, 58.87]$\n- Test Case 3 (sparse sampling, moderate curvature):\n  - $\\alpha = 0.6$\n  - $f = [0.75, 2, 6]$ (in cpd)\n  - $S = [84.23, 147.00, 74.15]$\n- Test Case 4 (flatter CSF, broader peak with moderate noise):\n  - $\\alpha = 0.15$\n  - $f = [2, 4, 8, 10, 12, 16, 24]$ (in cpd)\n  - $S = [131.59, 181.63, 202.48, 200.00, 195.03, 185.75, 165.78]$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a two-element list $[S_0^{\\text{hat}}, f_0^{\\text{hat}}]$ for the corresponding test case in order.\n- The required format is:\n  - $[[S_{0,1}^{\\text{hat}}, f_{0,1}^{\\text{hat}}],[S_{0,2}^{\\text{hat}}, f_{0,2}^{\\text{hat}}],[S_{0,3}^{\\text{hat}}, f_{0,3}^{\\text{hat}}],[S_{0,4}^{\\text{hat}}, f_{0,4}^{\\text{hat}}]]$\n- Round $S_0^{\\text{hat}}$ to two decimal places and $f_0^{\\text{hat}}$ to three decimal places, and report $f_0^{\\text{hat}}$ in cycles per degree (cpd).", "solution": "The problem is assessed to be valid as it is scientifically grounded in the field of ophthalmology, well-posed as a parameter estimation task, and specified with all necessary data and objective constraints.\n\nThe objective is to estimate the peak sensitivity $S_0$ and peak frequency $f_0$ for the Contrast Sensitivity Function (CSF) model:\n$$\nS(f) = S_0 \\exp\\!\\left(-\\alpha \\ln^2\\!\\left(\\frac{f}{f_0}\\right)\\right)\n$$\ngiven a set of measurements $(f_i, S_i)$ and a known curvature parameter $\\alpha$. The estimation must be based on the principle of minimizing the sum of squared residuals in the log-sensitivity domain.\n\n### Derivation from First Principles\n\nThe core principle is least-squares estimation applied to a transformed version of the model. The noise is assumed to be multiplicative in sensitivity $S$, which is equivalent to being additive and Gaussian in log-sensitivity $\\ln(S)$. This implies that the appropriate objective function to minimize is the sum of squared errors in the log domain.\n\n1.  **Log-Transformation of the Model:**\n    We begin by taking the natural logarithm of the model equation to linearize its structure. Let $y = \\ln(S)$ and $x = \\ln(f)$.\n    $$\n    \\ln(S(f)) = \\ln\\left(S_0 \\exp\\left(-\\alpha \\ln^2\\left(\\frac{f}{f_0}\\right)\\right)\\right)\n    $$\n    Using logarithm properties, $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(e^c) = c$:\n    $$\n    \\ln(S(f)) = \\ln(S_0) - \\alpha \\ln^2\\left(\\frac{f}{f_0}\\right)\n    $$\n    Further expanding the squared term using $\\ln(a/b) = \\ln(a) - \\ln(b)$:\n    $$\n    \\ln(S) = \\ln(S_0) - \\alpha (\\ln(f) - \\ln(f_0))^2\n    $$\n    This equation describes a parabola in the log-log plot of sensitivity versus frequency.\n\n2.  **Linearization through Change of Variables:**\n    To simplify the estimation, we can rearrange the equation into a form amenable to linear regression. Let $y_0 = \\ln(S_0)$ and $x_0 = \\ln(f_0)$ be the transformed parameters to estimate. The model for a single data point $(x_i, y_i)$, where $x_i = \\ln(f_i)$ and $y_i=\\ln(S_i)$, is:\n    $$\n    y_i \\approx y_0 - \\alpha(x_i - x_0)^2\n    $$\n    Expanding the quadratic term:\n    $$\n    y_i \\approx y_0 - \\alpha(x_i^2 - 2x_i x_0 + x_0^2)\n    $$\n    Rearranging the terms to group the unknowns:\n    $$\n    y_i + \\alpha x_i^2 \\approx (y_0 - \\alpha x_0^2) + (2\\alpha x_0) x_i\n    $$\n    This equation is now in the form of a linear model. Let us define a new dependent variable $z_i = y_i + \\alpha x_i^2$ and new coefficients $c_0 = y_0 - \\alpha x_0^2$ and $c_1 = 2\\alpha x_0$. The model becomes a simple linear regression:\n    $$\n    z_i \\approx c_0 + c_1 x_i\n    $$\n\n3.  **Least-Squares Minimization:**\n    We need to find the estimates $\\hat{c_0}$ and $\\hat{c_1}$ that minimize the sum of squared residuals, $Q$:\n    $$\n    Q(c_0, c_1) = \\sum_{i=1}^n (z_i - (c_0 + c_1 x_i))^2\n    $$\n    To find the minimum, we set the partial derivatives of $Q$ with respect to $c_0$ and $c_1$ to zero.\n    $$\n    \\frac{\\partial Q}{\\partial c_0} = \\sum_{i=1}^n 2(z_i - c_0 - c_1 x_i)(-1) = 0 \\implies \\sum_{i=1}^n (z_i - c_0 - c_1 x_i) = 0\n    $$\n    $$\n    \\frac{\\partial Q}{\\partial c_1} = \\sum_{i=1}^n 2(z_i - c_0 - c_1 x_i)(-x_i) = 0 \\implies \\sum_{i=1}^n (z_i x_i - c_0 x_i - c_1 x_i^2) = 0\n    $$\n    These yield the normal equations for a simple linear regression:\n    $$\n    \\begin{align*}\n    n c_0 + \\left(\\sum x_i\\right) c_1 = \\sum z_i \\\\\n    \\left(\\sum x_i\\right) c_0 + \\left(\\sum x_i^2\\right) c_1 = \\sum x_i z_i\n    \\end{align*}\n    $$\n    This is a $2 \\times 2$ system of linear equations for $(c_0, c_1)$. Solving this system gives the least-squares estimates $\\hat{c_0}$ and $\\hat{c_1}$:\n    $$\n    \\hat{c_1} = \\frac{n(\\sum x_i z_i) - (\\sum x_i)(\\sum z_i)}{n(\\sum x_i^2) - (\\sum x_i)^2}\n    $$\n    $$\n    \\hat{c_0} = \\bar{z} - \\hat{c_1} \\bar{x}\n    $$\n    where $\\bar{x} = \\frac{1}{n}\\sum x_i$ and $\\bar{z} = \\frac{1}{n}\\sum z_i$.\n\n4.  **Back-transformation to Original Parameters:**\n    Once $\\hat{c_0}$ and $\\hat{c_1}$ are computed, we can recover the estimates for the original log-domain parameters, $\\hat{x_0}$ and $\\hat{y_0}$.\n    From $c_1 = 2\\alpha x_0$, we have:\n    $$\n    \\hat{x_0} = \\frac{\\hat{c_1}}{2\\alpha}\n    $$\n    From $c_0 = y_0 - \\alpha x_0^2$, we have:\n    $$\n    \\hat{y_0} = \\hat{c_0} + \\alpha \\hat{x_0}^2\n    $$\n\n5.  **Final Parameter Estimates:**\n    Finally, we obtain the estimates for $S_0$ and $f_0$ by applying the exponential function:\n    $$\n    \\hat{f_0} = \\exp(\\hat{x_0})\n    $$\n    $$\n    \\hat{S_0} = \\exp(\\hat{y_0})\n    $$\n    The constraints $S_0  0$ and $f_0  0$ are automatically satisfied by this procedure since the exponential function's range is all positive real numbers.\n\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_csf_params(alpha, f_data, s_data):\n    \"\"\"\n    Estimates the parameters S0 and f0 of the CSF model using linear least squares\n    on a transformed version of the model.\n\n    Args:\n        alpha (float): The known curvature parameter.\n        f_data (np.ndarray): Array of spatial frequencies in cycles per degree.\n        s_data (np.ndarray): Array of observed sensitivities.\n\n    Returns:\n        tuple[float, float]: A tuple containing the estimated (S0_hat, f0_hat).\n    \"\"\"\n    # Step 1: Log-transform the data.\n    # The model is S(f) = S0 * exp(-alpha * ln(f/f0)^2).\n    # In the log-log domain, this is ln(S) = ln(S0) - alpha * (ln(f) - ln(f0))^2.\n    # Let x = ln(f) and y = ln(S). The model becomes y = y0 - alpha * (x - x0)^2.\n    x = np.log(f_data)\n    y = np.log(s_data)\n    n = len(f_data)\n\n    # Step 2: Linearize the model.\n    # y = y0 - alpha * (x^2 - 2*x*x0 + x0^2)\n    # y + alpha*x^2 = (y0 - alpha*x0^2) + (2*alpha*x0)*x\n    # This is a linear model z = c0 + c1*x, where:\n    # z = y + alpha*x^2\n    # c1 = 2*alpha*x0\n    # c0 = y0 - alpha*x0^2\n    z = y + alpha * x**2\n\n    # Step 3: Perform linear regression of z on x to find c0 and c1.\n    # We solve the normal equations for simple linear regression.\n    sum_x = np.sum(x)\n    sum_z = np.sum(z)\n    sum_x_sq = np.sum(x**2)\n    sum_xz = np.sum(x * z)\n\n    # Denominator of the ordinary least squares (OLS) estimators\n    denom = n * sum_x_sq - sum_x**2\n\n    # Estimate c1 and c0\n    c1_hat = (n * sum_xz - sum_x * sum_z) / denom\n    c0_hat = np.mean(z) - c1_hat * np.mean(x)\n\n    # Step 4: Back-transform to get estimates for x0 and y0 (log-domain parameters).\n    # x0 = ln(f0), y0 = ln(S0)\n    x0_hat = c1_hat / (2 * alpha)\n    y0_hat = c0_hat + alpha * x0_hat**2\n\n    # Step 5: Transform back to the original parameter space (S0, f0).\n    s0_hat = np.exp(y0_hat)\n    f0_hat = np.exp(x0_hat)\n\n    return s0_hat, f0_hat\n\ndef solve():\n    \"\"\"\n    Main function to run the estimation on all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.4, np.array([0.5, 1, 2, 4, 8, 16]), np.array([44.33, 115.89, 206.35, 250.00, 206.35, 115.89])),\n        (0.5, np.array([0.5, 1, 2, 3, 3.5, 5, 7, 10, 16]), np.array([34.77, 95.41, 191.91, 210.88, 220.00, 214.71, 166.13, 114.18, 58.87])),\n        (0.6, np.array([0.75, 2, 6]), np.array([84.23, 147.00, 74.15])),\n        (0.15, np.array([2, 4, 8, 10, 12, 16, 24]), np.array([131.59, 181.63, 202.48, 200.00, 195.03, 185.75, 165.78]))\n    ]\n    \n    formatted_results = []\n    for alpha, f_data, s_data in test_cases:\n        # Calculate the estimates for the current test case\n        s0_hat, f0_hat = solve_csf_params(alpha, f_data, s_data)\n        \n        # Round the estimates to the specified decimal places\n        s0_rounded = round(s0_hat, 2)\n        f0_rounded = round(f0_hat, 3)\n        \n        # Format the result for this case as a string '[S0,f0]' ensuring trailing zeros\n        result_str = f\"[{s0_rounded:.2f},{f0_rounded:.3f}]\"\n        formatted_results.append(result_str)\n\n    # Final print statement in the exact required format: [[S_hat1, f_hat1],[S_hat2, f_hat2],...]\n    final_output = f\"[{','.join(formatted_results)}]\"\n    print(final_output)\n\n# This part is for conceptual validation; the function is called from the platform.\n# solve()\n```", "answer": "[[250.00,4.000],[218.43,3.699],[147.00,2.000],[205.15,8.956]]", "id": "4733125"}]}