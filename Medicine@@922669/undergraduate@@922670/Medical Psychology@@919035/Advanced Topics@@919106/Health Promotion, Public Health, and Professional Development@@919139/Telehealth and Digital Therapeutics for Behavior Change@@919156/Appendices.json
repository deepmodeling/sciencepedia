{"hands_on_practices": [{"introduction": "To prove a new digital therapeutic is truly effective, we must compare it against more than just 'no treatment'. This exercise challenges you to design a credible digital placebo, a critical component of a rigorous Randomized Controlled Trial (RCT). By learning to isolate the 'active ingredients' of a behavioral app from nonspecific factors like user attention and expectation, you will grasp one of the most sophisticated and essential skills in clinical research [@problem_id:4749605].", "problem": "A university-affiliated clinic is preparing a Randomized Controlled Trial (RCT) of a telehealth-delivered digital therapeutic for smoking cessation. The active app delivers behavior change techniques derived from cognitive-behavioral therapy, including goal setting, action planning, tailored feedback, and self-monitoring with contingent feedback. It also uses geolocation-triggered just-in-time prompts. In a pilot, the active app generated the following nonspecific exposure metrics: average daily push notifications $n_A = 5$, average daily in-app time $m_A = 15$ minutes, average structured module sessions per week $s_A = 4$, and average asynchronous coach touchpoints per week $h_A = 1$ (brief neutral check-in lasting $10$ minutes via secure chat). The primary outcome is biochemically verified $7$-day point prevalence abstinence at week $8$.\n\nYou are asked to select the best design for a credible digital placebo control that matches nonspecific factors (time, attention, interface exposure, expectancy) without delivering active behavior change ingredients. Use the following foundational principles to reason to your choice: definitions of placebo and nonspecific effects in clinical trials, internal validity via isolation of active components, expectancy effects and their relation to credibility, and the requirement that control conditions equate attention and exposure while withholding mechanisms hypothesized to produce change.\n\nWhich option below most appropriately specifies a digital placebo control and justifies its validity in this RCT?\n\nA. Control app mirrors the look-and-feel of the active app and schedules push notifications at $n_C = 5$ per day with scripted neutral content (e.g., general health trivia unrelated to smoking), targets an average daily time-on-app $m_C \\approx 14$ minutes using short neutral tasks (e.g., simple visuospatial puzzles and non-health quizzes), and offers $s_C = 4$ brief weekly modules on general digital wellness literacy (e.g., privacy and ergonomics) with no advice about smoking or coping. It provides $h_C = 1$ weekly coach chat focused on user experience only (no behavioral coaching), includes sham personalization language that does not change content, and incorporates expectancy and credibility checks using the Credibility/Expectancy Questionnaire (CEQ) at week $1$ with monitoring of app logs to maintain exposure parity. No self-monitoring of smoking, no goals, no tailored feedback, and no contingent rewards are present.\n\nB. Control app provides psychoeducation on the health risks of smoking, coping strategies for cravings, and breathing exercises, but intentionally omits goal setting and geolocation features. It limits push notifications to $n_C = 2$ per day, targets $m_C \\approx 8$ minutes daily, and offers $s_C = 2$ sessions weekly. There is no coach contact. Credibility is assessed only at the end of the trial.\n\nC. Control app disables notifications entirely ($n_C = 0$) to avoid expectancy effects, but requires daily self-monitoring of cigarettes smoked with graphs that display weekly totals (no explicit feedback). It targets $m_C \\approx 16$ minutes daily and $s_C = 4$ sessions weekly with a user interface intentionally less polished than the active app. There is no coach contact. Credibility is assumed due to the presence of graphs.\n\nD. Control app matches notifications ($n_C = 5$), daily time ($m_C \\approx 15$ minutes), and sessions ($s_C = 4$) and removes tailored feedback, but adds badges and points for daily streaks and provides generic praise messages (e.g., “Great job!”) after any app use. It includes $h_C = 1$ weekly coach chat emphasizing motivation to keep engaging with the app. No self-monitoring of smoking is included. Credibility is not measured because engagement metrics are matched.\n\nYour answer should be based on first principles about isolating active mechanisms while equating nonspecific factors and ensuring perceived credibility and equipoise. Choose the single best option.", "solution": "The objective is to design a digital placebo control that isolates the effects of the \"active ingredients\" of the therapeutic app. According to the stated principles, a valid placebo must accomplish two primary goals:\n1.  **Match Nonspecific Factors**: It must provide an equivalent \"dose\" of time, attention, and interaction as the active intervention. This is crucial for ensuring that any observed difference in outcomes between the groups is not merely due to one group receiving more attention or spending more time engaged with an app. The target metrics are $n_A = 5$ notifications/day, $m_A = 15$ minutes/day, $s_A = 4$ sessions/week, and $h_A = 1$ coach contact/week. The placebo must also be credible and generate similar user expectancy to the active arm.\n2.  **Withhold Active Ingredients**: It must be inert with respect to the therapeutic goal (smoking cessation). This means it must systematically exclude the specified behavior change techniques: CBT content, goal setting, action planning, tailored feedback, self-monitoring with contingent feedback, and relevant just-in-time prompts.\n\nWe will now evaluate each option against these two fundamental requirements.\n\n#### Option-by-Option Analysis\n\n**Option A**\n\n-   **Matching Nonspecific Factors**:\n    -   **Look-and-feel**: \"mirrors the look-and-feel of the active app\". This is excellent for maintaining the blind and ensuring comparable credibility.\n    -   **Notifications**: $n_C = 5$ per day. This perfectly matches the target $n_A = 5$. The content is \"scripted neutral content...unrelated to smoking\", which is appropriately inert.\n    -   **Time-on-app**: $m_C \\approx 14$ minutes daily. This is a very close and appropriate match for $m_A = 15$ minutes. The tasks (\"simple visuospatial puzzles and non-health quizzes\") are inert.\n    -   **Sessions**: $s_C = 4$ weekly modules. This perfectly matches $s_A = 4$. The content (\"general digital wellness literacy\") is unrelated to smoking and thus inert.\n    -   **Coach Contact**: $h_C = 1$ weekly chat. This perfectly matches $h_A = 1$. The focus on \"user experience only (no behavioral coaching)\" expertly matches the attention component while withholding the active therapeutic interaction.\n    -   **Credibility/Expectancy**: This design specifically includes \"sham personalization language\" to bolster credibility and, crucially, directly measures it using the \"Credibility/Expectancy Questionnaire (CEQ) at week $1$\". This is considered best practice in clinical trials to verify that the placebo is functioning as intended.\n\n-   **Withholding Active Ingredients**: The description explicitly states: \"No self-monitoring of smoking, no goals, no tailored feedback, and no contingent rewards are present\". This demonstrates a systematic removal of all specified active components.\n\n-   **Conclusion**: This option describes a methodologically rigorous digital placebo. It systematically matches all key nonspecific factors (exposure, attention, credibility) while carefully excluding all active therapeutic ingredients. The inclusion of credibility measurement is a marker of a high-quality design.\n\n**Option B**\n\n-   **Matching Nonspecific Factors**:\n    -   **Notifications**: $n_C = 2$ per day. This fails to match $n_A = 5$.\n    -   **Time-on-app**: $m_C \\approx 8$ minutes daily. This fails to match $m_A = 15$ minutes.\n    -   **Sessions**: $s_C = 2$ sessions weekly. This fails to match $s_A = 4$.\n    -   **Coach Contact**: \"No coach contact\". This fails to match $h_A = 1$.\n    -   This design provides a significantly lower \"dose\" of attention and exposure. Any difference in outcome could be attributed to this disparity rather than the active ingredients of the therapeutic app.\n    -   **Credibility/Expectancy**: \"Credibility is assessed only at the end of the trial.\" This is a critical flaw. Credibility must be assessed early on to ensure the blind is effective and that expectancy effects are equivalent across groups from the outset.\n\n-   **Withholding Active Ingredients**: The design \"provides psychoeducation on the health risks of smoking, coping strategies for cravings, and breathing exercises\". These are not inert components; they are well-established, active interventions for smoking cessation. This design does not create a placebo; it creates a \"dismantling study\" comparing a full intervention to a partial one, which fails the stated goal of isolating the specified active ingredients against an inert control.\n\n-   **Conclusion**: This option fails on every key principle. It does not match exposure or attention, and it includes active therapeutic components, making it an invalid placebo.\n\n**Option C**\n\n-   **Matching Nonspecific Factors**:\n    -   **Notifications**: $n_C = 0$. This fails to match $n_A = 5$. The rationale provided (\"to avoid expectancy effects\") misunderstands the principle; the goal is to *equate* expectancy, not eliminate it in one arm while it is present in the other.\n    -   **Coach Contact**: \"No coach contact\". This fails to match $h_A = 1$.\n    -   **Credibility/Expectancy**: The design specifies a \"user interface intentionally less polished than the active app\". This is a catastrophic flaw that would systematically break the blind and introduce negative expectancy, violating the core purpose of a placebo. Furthermore, it states \"Credibility is assumed...\". In rigorous science, credibility cannot be assumed; it must be measured.\n\n-   **Withholding Active Ingredients**: The design \"requires daily self-monitoring of cigarettes smoked with graphs\". Self-monitoring is explicitly listed as an active ingredient in the main intervention. Even without explicit feedback, the act of monitoring is a reactive behavioral technique that can induce change. This design fails to withhold a key active ingredient.\n\n-   **Conclusion**: This option is poorly designed. It fails to match multiple exposure metrics, intentionally and unacceptably compromises credibility, and includes a known active behavior change technique.\n\n**Option D**\n\n-   **Matching Nonspecific Factors**: The exposure metrics are matched: $n_C = 5$, $m_C \\approx 15$ minutes, $s_C = 4$, and $h_C = 1$. Superficially, this appears correct.\n\n-   **Withholding Active Ingredients**: The design \"adds badges and points for daily streaks and provides generic praise messages...after any app use.\" This is a form of gamification that constitutes contingent positive reinforcement, which is a powerful behavior change technique. While the active app uses \"contingent feedback,\" this design substitutes it with another form of contingent reward, rather than removing it. This violates the principle of using an inert control. Additionally, the coach chat \"emphasizing motivation to keep engaging with the app\" is not a neutral interaction and can be considered a nonspecific therapeutic element (support/empathy).\n\n-   **Credibility/Expectancy**: It explicitly states, \"Credibility is not measured because engagement metrics are matched.\" This is a severe methodological error. Engagement does not equal credibility. Participants might engage with an app they find unhelpful for various reasons (e.g., trial obligation). Failure to measure credibility makes it impossible to know if expectancy effects were matched, thus threatening internal validity.\n\n-   **Conclusion**: This option fails because it introduces a different set of active ingredients (gamification) rather than being inert, and it makes the critical mistake of assuming, rather than measuring, credibility.\n\nBased on this comprehensive analysis, Option A is the only one that adheres to all the foundational principles of a rigorous placebo-controlled trial for a digital therapeutic.", "answer": "$$\\boxed{A}$$", "id": "4749605"}, {"introduction": "A digital therapeutic is only as good as the data it collects. Before we can trust an app's ability to monitor behavior like physical activity, we must validate its measurements against a reliable benchmark. This practice introduces the Bland-Altman analysis, a powerful statistical method used to assess the agreement between two different measurement tools [@problem_id:4749681]. Mastering this technique is fundamental for anyone looking to develop or evaluate digital health technologies that rely on sensor data.", "problem": "A medical psychology research team is evaluating the measurement agreement between a smartphone-based step-counting application and a validated wrist-worn wearable device as part of a telehealth and digital therapeutics program for physical activity behavior change. For each of $n$ days across a cohort of participants, they compute the paired difference $d_i$ defined as smartphone app steps minus wearable steps. Across the aggregated paired differences, they obtain a sample mean difference of $m=-320$ steps and a sample standard deviation of $s=850$ steps. Assume that the daily paired differences are independent and approximately normally distributed due to the central limit properties of aggregated day-level errors and the device behavior being stable over the study interval.\n\nUsing the conventional $95\\%$ limits of agreement framework from Bland–Altman analysis, and treating the wearable device as the comparative reference, determine the lower and upper limits of agreement for the app relative to the wearable, expressed in steps. Round each limit to four significant figures. State your final numerical results in steps.", "solution": "The problem requires the calculation of the $95\\%$ limits of agreement based on the Bland-Altman method for assessing agreement between two quantitative measurement techniques. The provided data are from a study comparing a smartphone-based step-counting application to a validated wrist-worn wearable device.\n\nLet $d_i$ represent the paired difference for a given day $i$, defined as the number of steps measured by the smartphone app minus the number of steps measured by the wearable device. The problem provides the sample mean of these differences, $m$, and the sample standard deviation of these differences, $s$.\n\nThe given values are:\n- Sample mean difference: $m = -320$ steps.\n- Sample standard deviation of differences: $s = 850$ steps.\n\nThe problem states that the differences $d_i$ are assumed to be approximately normally distributed. This assumption is crucial for the standard Bland-Altman analysis.\n\nThe $95\\%$ limits of agreement (LoA) are calculated to provide an interval within which approximately $95\\%$ of the future differences between the two methods are expected to lie. The formula for the limits of agreement is based on the mean and standard deviation of the differences:\n$$\n\\text{LoA} = m \\pm z_{\\alpha/2} \\cdot s\n$$\nFor $95\\%$ limits, the confidence level is $1 - \\alpha = 0.95$, which implies $\\alpha = 0.05$. The value $z_{\\alpha/2} = z_{0.025}$ is the critical value from the standard normal distribution that corresponds to the upper $2.5\\%$ tail. This value is approximately $1.96$. Therefore, the formula becomes:\n$$\n\\text{LoA} = m \\pm 1.96 \\cdot s\n$$\nThis interval is often approximated in practice as $m \\pm 2s$, but the use of $1.96$ is more precise.\n\nWe can now calculate the lower and upper limits of agreement.\n\nThe Lower Limit of Agreement (LLOA) is:\n$$\n\\text{LLOA} = m - 1.96 \\cdot s\n$$\nSubstituting the given values:\n$$\n\\text{LLOA} = -320 - 1.96 \\times 850\n$$\nFirst, we compute the product:\n$$\n1.96 \\times 850 = 1666\n$$\nNow, we perform the subtraction:\n$$\n\\text{LLOA} = -320 - 1666 = -1986\n$$\n\nThe Upper Limit of Agreement (ULOA) is:\n$$\n\\text{ULOA} = m + 1.96 \\cdot s\n$$\nSubstituting the given values:\n$$\n\\text{ULOA} = -320 + 1.96 \\times 850\n$$\nUsing the product calculated previously:\n$$\n\\text{ULOA} = -320 + 1666 = 1346\n$$\n\nThe problem requires rounding each limit to four significant figures.\nFor the LLOA, the value is $-1986$. The significant figures are $1$, $9$, $8$, and $6$. This number already has exactly four significant figures.\nFor the ULOA, the value is $1346$. The significant figures are $1$, $3$, $4$, and $6$. This number also has exactly four significant figures.\nTherefore, no further rounding is necessary.\n\nThe calculated $95\\%$ limits of agreement are $-1986$ steps and $1346$ steps. This means that for any given day, the step count from the smartphone app is expected to be between $1986$ steps lower and $1346$ steps higher than the step count from the wearable device for approximately $95\\%$ of the observations.", "answer": "$$\n\\boxed{\\begin{pmatrix} -1986  1346 \\end{pmatrix}}\n$$", "id": "4749681"}, {"introduction": "In healthcare, clinical effectiveness is only part of the story; economic value is equally important for real-world adoption. This final practice walks you through a cost-effectiveness analysis, a cornerstone of health economics. You will calculate the Incremental Cost-Effectiveness Ratio (ICER) to determine if a digital therapeutic provides a worthwhile health gain for its cost [@problem_id:4749632]. This skill is vital for making informed decisions about which new technologies should be integrated into health systems.", "problem": "A regional health system is evaluating a smartphone-based digital therapeutic, delivered via telehealth, that combines cognitive behavioral therapy and just-in-time adaptive prompts to support smoking cessation and medication adherence. Compared with usual care, the telehealth program adds a per-patient incremental cost of $\\$150$ over one year due to licensing, coaching time, and monitoring. Observational follow-up for one year estimates an incremental health gain of $0.02$ Quality-Adjusted Life Years (QALYs) per patient attributable to improved cessation and adherence.\n\nUsing the standard definitions from cost-effectiveness analysis in health economics and medical decision-making in medical psychology—where the Incremental Cost-Effectiveness Ratio (ICER) is the ratio of incremental cost to incremental effectiveness in QALYs—do the following:\n\n1) Compute the ICER for the telehealth digital therapeutic relative to usual care.\n2) Using a willingness-to-pay (WTP) threshold of $\\$50{,}000$ per QALY, state whether the telehealth program is cost-effective under this criterion.\n\nExpress the ICER in dollars per Quality-Adjusted Life Year (QALY). Round your numerical ICER to four significant figures. The final answer you provide must be only the numerical value of the ICER (without any units); provide your interpretation about cost-effectiveness in your working, not in the final answer box.", "solution": "### Solution\nThe problem requires the calculation of the Incremental Cost-Effectiveness Ratio (ICER) for a digital therapeutic and an assessment of its cost-effectiveness against a given threshold.\n\n1) **Compute the ICER.**\n\nThe ICER is defined as the ratio of the incremental cost of an intervention to its incremental effectiveness. Let $\\Delta C$ be the incremental cost and $\\Delta E$ be the incremental effectiveness. The formula is:\n$$\n\\text{ICER} = \\frac{\\Delta C}{\\Delta E}\n$$\nThe problem provides the following values:\n- Incremental cost, $\\Delta C = \\$150$.\n- Incremental effectiveness, $\\Delta E = 0.02$ QALYs.\n\nSubstituting these values into the formula:\n$$\n\\text{ICER} = \\frac{\\$150}{0.02 \\text{ QALYs}}\n$$\nPerforming the division:\n$$\n\\text{ICER} = \\frac{150}{0.02} \\frac{\\$}{\\text{QALY}} = \\frac{150}{\\frac{2}{100}} \\frac{\\$}{\\text{QALY}} = \\frac{150 \\times 100}{2} \\frac{\\$}{\\text{QALY}} = 75 \\times 100 \\frac{\\$}{\\text{QALY}} = 7500 \\frac{\\$}{\\text{QALY}}\n$$\nThe calculated ICER is exactly $\\$7500$ per QALY. The problem mandates that the result be expressed to four significant figures. To represent the value $7500$ unambiguously with four significant figures, scientific notation is appropriate. Therefore, the ICER is expressed as $7.500 \\times 10^3$ dollars per QALY.\n\n2) **Assess Cost-Effectiveness.**\n\nAn intervention is considered cost-effective if its ICER is less than or equal to the societal or predetermined willingness-to-pay (WTP) threshold for a unit of health gain. The given WTP threshold is:\n$$\n\\text{WTP} = \\$50,000 \\text{ per QALY}\n$$\nTo determine if the telehealth program is cost-effective, we compare the calculated ICER to the WTP threshold. The decision rule is:\n- If $\\text{ICER} \\le \\text{WTP}$, the intervention is cost-effective.\n- If $\\text{ICER}  \\text{WTP}$, the intervention is not cost-effective.\n\nComparing the values:\n$$\n\\$7500 \\le \\$50,000\n$$\nSince the ICER of $\\$7500$ per QALY is substantially less than the WTP threshold of $\\$50,000$ per QALY, the condition is met.\n\nTherefore, under the specified criterion, the telehealth digital therapeutic program is deemed to be a cost-effective intervention relative to usual care.", "answer": "$$\n\\boxed{7.500 \\times 10^{3}}\n$$", "id": "4749632"}]}