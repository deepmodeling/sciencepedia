{"hands_on_practices": [{"introduction": "Before integrating diverse datasets, we must first ensure the quality of each individual omics layer. This practice simulates a critical first step in any analysis workflow: performing Quality Control (QC) to identify and flag outlier samples that could otherwise confound our results [@problem_id:5062582]. You will calculate standard metrics for hypothetical Ribonucleic Acid sequencing (RNA-seq) and proteomics data and apply robust statistical methods to establish data-driven thresholds for what constitutes a high-quality sample.", "problem": "You are given Ribonucleic Acid sequencing (RNA-seq) and proteomics data from $n$ samples. The RNA-seq data are summarized as a meta-gene body coverage matrix $X \\in \\mathbb{N}^{n \\times p}$, where each row corresponds to a sample and each column $j \\in \\{1,\\dots,p\\}$ corresponds to the count of reads mapping to the $j$-th bin along the aggregated gene body from the $5'$ end to the $3'$ end. The proteomics data are provided as an intensity matrix $Y \\in \\mathbb{R}^{n \\times q}$, where each row corresponds to a sample and each column corresponds to a peptide intensity, with strictly positive values indicating peptide identification. Your task is to compute per-sample quality metrics and use robust statistics to establish thresholds for flagging outliers.\n\nDefinitions:\n- The per-sample RNA-seq library size for sample $i$ is $LS_i = \\sum_{j=1}^{p} X_{i,j}$.\n- The per-sample $5'$ bias for sample $i$ at proportion $\\alpha \\in (0,1)$ is $B_i = \\dfrac{\\sum_{j=1}^{k} X_{i,j}}{\\sum_{j=1}^{p} X_{i,j}}$ where $k = \\lfloor \\alpha p \\rfloor$. If $\\sum_{j=1}^{p} X_{i,j} = 0$, define $B_i = 0$ by convention. Express $B_i$ as a decimal fraction.\n- The per-sample number of identified peptides for sample $i$ is $P_i = \\#\\{j \\in \\{1,\\dots,q\\} : Y_{i,j} > 0\\}$.\n\nRobust thresholds for outlier detection:\n- For a metric vector $v \\in \\mathbb{R}^{n}$ across samples (for example, $v = (LS_1,\\dots,LS_n)$, $v = (B_1,\\dots,B_n)$, or $v = (P_1,\\dots,P_n)$), compute the median $m = \\mathrm{median}(v)$, and the Median Absolute Deviation (MAD) defined as $\\mathrm{MAD} = \\mathrm{median}(|v - m|)$. Use the normal consistency constant $c = 1.4826$ to form a robust scale $s = c \\cdot \\mathrm{MAD}$. If $s = 0$, fall back to the Interquartile Range (IQR) defined by $\\mathrm{IQR} = Q_3 - Q_1$ with quartiles $Q_1$ and $Q_3$, and set $s = \\mathrm{IQR}/1.349$. If $s = 0$ still holds, set $s = 0$.\n- With a user-specified robust multiplier $z > 0$, define lower and upper thresholds as $T_{\\mathrm{low}} = m - z s$ and $T_{\\mathrm{high}} = m + z s$.\n- Flag low outliers for library size and number of identified peptides when $LS_i  T_{\\mathrm{low}}$ and $P_i  T_{\\mathrm{low}}$, respectively.\n- Flag high outliers for $5'$ bias when $B_i > T_{\\mathrm{high}}$.\n\nFundamental base:\n- Sequencing and proteomics measurements yield count and intensity data that aggregate over independent events, where sums and ratios of counts provide meaningful sample-level metrics. Robust statistics using the median and Median Absolute Deviation (MAD) provide stable thresholding under outliers and non-Gaussian noise, and the Interquartile Range (IQR) furnishes a fallback scale when MAD vanishes.\n\nImplement a program that computes $LS_i$, $B_i$, and $P_i$ for each sample, derives $T_{\\mathrm{low}}$ and $T_{\\mathrm{high}}$ for each metric using the rules above, and returns boolean flags per sample for:\n- Low RNA-seq library size,\n- High $5'$ bias,\n- Low number of identified peptides.\n\nUse the following test suite. For each case, $n$, $p$, and $q$ are given implicitly by the shapes of $X$ and $Y$.\n\nCase $1$: parameters $\\alpha = 0.2$, $z = 2$, with\n- $X^{(1)}$ rows:\n  - sample $1$: $[100,95,90,85,80,75,70,65,60,55]$\n  - sample $2$: $[300,300,10,10,10,10,10,10,10,10]$\n  - sample $3$: $[5,5,5,5,5,5,5,5,5,5]$\n- $Y^{(1)}$ rows:\n  - sample $1$: $[12000,9000,8000,5000,3000,2000,1000,500]$\n  - sample $2$: $[100,50,25,10,5,1,0.5,0]$\n  - sample $3$: $[0,0,0,0,0,0,0,0]$\n\nCase $2$: parameters $\\alpha = 0.2$, $z = 3$, with\n- $X^{(2)}$ rows:\n  - sample $1$: $[50,50,50,50,50,50,50,50,50,50]$\n  - sample $2$: $[50,50,50,50,50,50,50,50,50,50]$\n  - sample $3$: $[50,50,50,50,50,50,50,50,50,50]$\n- $Y^{(2)}$ rows:\n  - sample $1$: $[1,1,1,1,0,0]$\n  - sample $2$: $[1,1,1,1,0,0]$\n  - sample $3$: $[1,1,1,1,0,0]$\n\nCase $3$: parameters $\\alpha = 0.2$, $z = 0.5$, with\n- $X^{(3)}$ rows:\n  - sample $1$: $[40,38,36,34,32,30,28,26,24,22]$\n  - sample $2$: $[0,0,0,0,0,0,0,0,0,0]$\n  - sample $3$: $[10,10,10,10,10,10,10,10,10,10]$\n- $Y^{(3)}$ rows:\n  - sample $1$: $[10,9,0,0,0]$\n  - sample $2$: $[0,0,0,0,0]$\n  - sample $3$: $[5,4,3,2,1]$\n\nAngle units are not applicable. There are no physical units. Proportions such as $B_i$ must be expressed as decimal fractions. Your program should produce a single line of output containing, for each case, a list of three boolean lists in the order $[$library size low flags, $5'$ bias high flags, peptide count low flags$]$, aggregated over the three cases as a comma-separated list enclosed in square brackets, for example $[[[b_{1},\\dots],[b_{1},\\dots],[b_{1},\\dots]],[[...]],[[...]]]$ where each $b_{i}$ is either $\\mathrm{True}$ or $\\mathrm{False}$.", "solution": "The problem has been validated and is deemed sound. It is scientifically grounded in standard bioinformatics quality control procedures, mathematically well-posed with clear definitions and constraints, and objective in its formulation. All necessary data and parameters are provided for the test cases.\n\nThe task is to compute three quality control metrics for each of $n$ samples from multi-omics data (RNA-seq and proteomics), establish robust statistical thresholds for these metrics, and flag samples that are outliers.\n\nThe solution proceeds in three main stages for each test case:\n1.  Calculation of per-sample metrics: Library Size ($LS_i$), $5'$ Bias ($B_i$), and Peptide Count ($P_i$).\n2.  Derivation of robust outlier thresholds ($T_{\\mathrm{low}}$, $T_{\\mathrm{high}}$) for each metric vector.\n3.  Application of thresholds to flag outlier samples.\n\nThe input data for each sample $i \\in \\{1, \\dots, n\\}$ consists of a row vector of RNA-seq read counts $X_{i,:} \\in \\mathbb{N}^{p}$ and a row vector of peptide intensities $Y_{i,:} \\in \\mathbb{R}^{q}$.\n\n**1. Per-Sample Metric Calculation**\n\nFor each sample $i$, we compute the following metrics based on the provided definitions:\n- **RNA-seq Library Size ($LS_i$)**: The total number of reads in a sample's RNA-seq profile.\n$$LS_i = \\sum_{j=1}^{p} X_{i,j}$$\n- **$5'$ Bias ($B_i$)**: The fraction of reads that fall into the initial bins of the aggregated gene body, indicating potential degradation or incomplete reverse transcription. Given a proportion $\\alpha \\in (0,1)$, we first determine the number of bins $k = \\lfloor \\alpha p \\rfloor$.\n$$B_i = \\begin{cases} \\frac{\\sum_{j=1}^{k} X_{i,j}}{\\sum_{j=1}^{p} X_{i,j}}  \\text{if } \\sum_{j=1}^{p} X_{i,j} > 0 \\\\ 0  \\text{if } \\sum_{j=1}^{p} X_{i,j} = 0 \\end{cases}$$\n- **Number of Identified Peptides ($P_i$)**: The count of peptides with a positive intensity signal, reflecting the depth of the proteomics measurement.\n$$P_i = \\#\\{j \\in \\{1, \\dots, q\\} : Y_{i,j} > 0\\}$$\n\n**2. Robust Threshold Derivation**\n\nFor each of the three metric vectors $v = (LS_1, \\dots, LS_n)$, $v = (B_1, \\dots, B_n)$, and $v = (P_1, \\dots, P_n)$, we compute outlier thresholds. This method uses robust statistics, which are less sensitive to the presence of outliers than classical methods based on mean and standard deviation.\n\n-   First, we compute the median $m$ of the metric vector $v$:\n    $$m = \\mathrm{median}(v)$$\n-   Next, we compute the Median Absolute Deviation (MAD), which is the median of the absolute deviations from the median:\n    $$\\mathrm{MAD} = \\mathrm{median}(|v - m|) = \\mathrm{median}(|v_1 - m|, |v_2 - m|, \\dots, |v_n - m|)$$\n-   A robust estimate of scale, $s$, is then calculated. The primary method uses the MAD, scaled by a constant $c = 1.4826$ to make it comparable to the standard deviation for normally distributed data.\n    $$s = c \\cdot \\mathrm{MAD}$$\n-   A fallback mechanism is defined for the case where $\\mathrm{MAD} = 0$, which occurs if at least half the data points are identical. In this situation, the Interquartile Range (IQR) is used.\n    $$Q_1 = 25\\text{th percentile of } v$$\n    $$Q_3 = 75\\text{th percentile of } v$$\n    $$\\mathrm{IQR} = Q_3 - Q_1$$\n    The scale is then set to $s = \\mathrm{IQR} / 1.349$. The divisor $1.349$ relates the IQR to the standard deviation for a normal distribution. If this calculation also yields $s=0$, the scale is set to $s=0$.\n-   Finally, with the robust scale $s$ and a user-specified multiplier $z > 0$, the lower and upper thresholds are defined as:\n    $$T_{\\mathrm{low}} = m - z s$$\n    $$T_{\\mathrm{high}} = m + z s$$\n\n**3. Outlier Flagging**\n\nSamples are flagged based on these thresholds:\n-   **Low Library Size**: Flag sample $i$ if $LS_i  T_{\\mathrm{low}, LS}$.\n-   **High $5'$ Bias**: Flag sample $i$ if $B_i > T_{\\mathrm{high}, B}$.\n-   **Low Peptide Count**: Flag sample $i$ if $P_i  T_{\\mathrm{low}, P}$.\n\n---\n\n**Execution on Test Cases**\n\n**Case 1: $\\alpha = 0.2, z = 2$**\n-   $n=3, p=10, q=8$. $k = \\lfloor 0.2 \\times 10 \\rfloor = 2$.\n-   **Metrics**:\n    -   $LS = [\\sum X_{1,j}, \\sum X_{2,j}, \\sum X_{3,j}] = [775, 680, 50]$.\n    -   $B = [195/775, 600/680, 10/50] \\approx [0.2516, 0.8824, 0.2]$.\n    -   $P = [8, 7, 0]$.\n-   **Thresholds and Flags**:\n    -   $v=LS$: $m=680$, $\\mathrm{MAD}=95$, $s = 1.4826 \\times 95 \\approx 140.85$. $T_{\\mathrm{low}} = 680 - 2 \\times 140.85 = 398.3$. $LS_3=50  398.3$. Flags: `[False, False, True]`.\n    -   $v=B$: $m \\approx 0.2516$, $\\mathrm{MAD} \\approx |0.2 - 0.2516| = 0.0516$. $s = 1.4826 \\times 0.0516 \\approx 0.0765$. $T_{\\mathrm{high}} = 0.2516 + 2 \\times 0.0765 \\approx 0.4046$. $B_2 \\approx 0.8824 > 0.4046$. Flags: `[False, True, False]`.\n    -   $v=P$: $m=7$, $\\mathrm{MAD}=1$, $s = 1.4826 \\times 1 = 1.4826$. $T_{\\mathrm{low}} = 7 - 2 \\times 1.4826 = 4.0348$. $P_3=0  4.0348$. Flags: `[False, False, True]`.\n-   **Result**: `[[False, False, True], [False, True, False], [False, False, True]]`\n\n**Case 2: $\\alpha = 0.2, z = 3$**\n-   $n=3, p=10, q=6$. $k = \\lfloor 0.2 \\times 10 \\rfloor = 2$. All samples are identical.\n-   **Metrics**:\n    -   $LS = [500, 500, 500]$.\n    -   $B = [0.2, 0.2, 0.2]$.\n    -   $P = [4, 4, 4]$.\n-   **Thresholds and Flags**:\n    -   For all three metrics, the vector of values $v$ has all identical elements. This results in $\\mathrm{MAD}=0$ and $\\mathrm{IQR}=0$. Therefore, the scale $s=0$.\n    -   $T_{\\mathrm{low}} = m - z \\cdot 0 = m$ and $T_{\\mathrm{high}} = m + z \\cdot 0 = m$.\n    -   The outlier conditions $v_i  m$ and $v_i > m$ are never met since all $v_i=m$.\n    -   All flags are `False`.\n-   **Result**: `[[False, False, False], [False, False, False], [False, False, False]]`\n\n**Case 3: $\\alpha = 0.2, z = 0.5$**\n-   $n=3, p=10, q=5$. $k = \\lfloor 0.2 \\times 10 \\rfloor = 2$.\n-   **Metrics**:\n    -   $LS = [310, 0, 100]$.\n    -   $B = [78/310, 0, 20/100] \\approx [0.2516, 0.0, 0.2]$.\n    -   $P = [2, 0, 5]$.\n-   **Thresholds and Flags**:\n    -   $v=LS$: $m=100$, $\\mathrm{MAD}=100$, $s = 1.4826 \\times 100 = 148.26$. $T_{\\mathrm{low}} = 100 - 0.5 \\times 148.26 = 25.87$. $LS_2=0  25.87$. Flags: `[False, True, False]`.\n    -   $v=B$: $m=0.2$, $\\mathrm{MAD} = |0.2516-0.2| \\approx 0.0516$. $s = 1.4826 \\times 0.0516 \\approx 0.0765$. $T_{\\mathrm{high}} = 0.2 + 0.5 \\times 0.0765 \\approx 0.2383$. $B_1 \\approx 0.2516 > 0.2383$. Flags: `[True, False, False]`.\n    -   $v=P$: $m=2$, $\\mathrm{MAD}=2$, $s = 1.4826 \\times 2 = 2.9652$. $T_{\\mathrm{low}} = 2 - 0.5 \\times 2.9652 = 0.5174$. $P_2=0  0.5174$. Flags: `[False, True, False]`.\n-   **Result**: `[[False, True, False], [True, False, False], [False, True, False]]`\n\nThe final implementation will programmatically execute these computations.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"matrices\": {\n                \"X\": np.array([\n                    [100, 95, 90, 85, 80, 75, 70, 65, 60, 55],\n                    [300, 300, 10, 10, 10, 10, 10, 10, 10, 10],\n                    [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n                ], dtype=np.int64),\n                \"Y\": np.array([\n                    [12000, 9000, 8000, 5000, 3000, 2000, 1000, 500],\n                    [100, 50, 25, 10, 5, 1, 0.5, 0],\n                    [0, 0, 0, 0, 0, 0, 0, 0]\n                ], dtype=np.float64)\n            },\n            \"params\": {\"alpha\": 0.2, \"z\": 2.0}\n        },\n        {\n            \"matrices\": {\n                \"X\": np.array([\n                    [50, 50, 50, 50, 50, 50, 50, 50, 50, 50],\n                    [50, 50, 50, 50, 50, 50, 50, 50, 50, 50],\n                    [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n                ], dtype=np.int64),\n                \"Y\": np.array([\n                    [1, 1, 1, 1, 0, 0],\n                    [1, 1, 1, 1, 0, 0],\n                    [1, 1, 1, 1, 0, 0]\n                ], dtype=np.float64)\n            },\n            \"params\": {\"alpha\": 0.2, \"z\": 3.0}\n        },\n        {\n            \"matrices\": {\n                \"X\": np.array([\n                    [40, 38, 36, 34, 32, 30, 28, 26, 24, 22],\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n                ], dtype=np.int64),\n                \"Y\": np.array([\n                    [10, 9, 0, 0, 0],\n                    [0, 0, 0, 0, 0],\n                    [5, 4, 3, 2, 1]\n                ], dtype=np.float64)\n            },\n            \"params\": {\"alpha\": 0.2, \"z\": 0.5}\n        }\n    ]\n\n    all_case_results = []\n    for case in test_cases:\n        X = case[\"matrices\"][\"X\"]\n        Y = case[\"matrices\"][\"Y\"]\n        alpha = case[\"params\"][\"alpha\"]\n        z = case[\"params\"][\"z\"]\n        \n        case_result = analyze_multi_omics(X, Y, alpha, z)\n        all_case_results.append(case_result)\n    \n    # Format the final output string as specified\n    case_strings = [f\"[{','.join(map(str, res))}]\" for res in all_case_results]\n    final_output_string = f\"[{','.join(case_strings)}]\"\n    print(final_output_string)\n\n\ndef get_robust_thresholds(v, z):\n    \"\"\"\n    Computes robust lower and upper thresholds for a given metric vector.\n    \"\"\"\n    if v.size == 0:\n        return 0.0, 0.0\n        \n    m = np.median(v)\n    \n    # Calculate MAD and the robust scale s\n    c = 1.4826\n    mad = np.median(np.abs(v - m))\n    s = c * mad\n    \n    # Fallback to IQR if MAD-based scale is zero\n    if s == 0:\n        q1 = np.quantile(v, 0.25)\n        q3 = np.quantile(v, 0.75)\n        iqr = q3 - q1\n        s = iqr / 1.349\n    \n    # Final thresholds\n    t_low = m - z * s\n    t_high = m + z * s\n    \n    return t_low, t_high\n\ndef analyze_multi_omics(X, Y, alpha, z):\n    \"\"\"\n    Performs the full quality control analysis for one case.\n    \"\"\"\n    n, p = X.shape\n    q = Y.shape[1]\n\n    # 1. Calculate per-sample metrics\n    \n    # RNA-seq Library Size (LS)\n    ls_vec = np.sum(X, axis=1)\n\n    # RNA-seq 5' Bias (B)\n    k = int(np.floor(alpha * p))\n    b_vec = np.zeros(n, dtype=np.float64)\n    for i in range(n):\n        if ls_vec[i] > 0:\n            b_vec[i] = np.sum(X[i, :k]) / ls_vec[i]\n        else:\n            b_vec[i] = 0.0\n            \n    # Number of Identified Peptides (P)\n    p_vec = np.sum(Y > 0, axis=1)\n\n    # 2. Derive thresholds and flag outliers\n    \n    # For Library Size (LS) - flag low outliers\n    t_low_ls, _ = get_robust_thresholds(ls_vec, z)\n    ls_flags = (ls_vec  t_low_ls).tolist()\n\n    # For 5' Bias (B) - flag high outliers\n    _, t_high_b = get_robust_thresholds(b_vec, z)\n    b_flags = (b_vec > t_high_b).tolist()\n    \n    # For Peptide Count (P) - flag low outliers\n    t_low_p, _ = get_robust_thresholds(p_vec, z)\n    p_flags = (p_vec  t_low_p).tolist()\n\n    return [ls_flags, b_flags, p_flags]\n\nsolve()\n```", "id": "5062582"}, {"introduction": "With quality-controlled data, we can proceed to integration to generate new biological hypotheses. This exercise introduces Transcriptome-Wide Association Studies (TWAS), a powerful method that leverages expression Quantitative Trait Loci (eQTL) data to translate variant-level associations from a Genome-Wide Association Study (GWAS) into gene-level findings [@problem_id:5062552]. You will implement the core logic of TWAS and explore the crucial concepts of replication and reproducibility, which are essential for validating scientific discoveries.", "problem": "You are given a synthetic multi-omics integration scenario suitable for an advanced undergraduate understanding of medical genetics. The objective is to implement a simplified Transcriptome-Wide Association Study (TWAS) by integrating expression Quantitative Trait Loci (eQTL) weights with Genome-Wide Association Study (GWAS) single-nucleotide polymorphism (SNP) summary statistics while accounting for Linkage Disequilibrium (LD). You will then compute reproducibility metrics across independent cohorts and across tissues.\n\nFundamental base and definitions: The Central Dogma of Molecular Biology states that deoxyribonucleic acid (DNA) is transcribed into ribonucleic acid (RNA) and translated into protein. Expression Quantitative Trait Loci (eQTL) associate genetic variants with gene expression. In a linear prediction model, genetic variation at SNPs with weights derived from eQTL can be used to predict gene expression in a tissue. GWAS summary statistics provide standardized SNP-level associations with a phenotype. Linkage Disequilibrium (LD) across SNPs introduces correlation that must be incorporated when forming linear aggregates of SNP-level statistics. Under a standard assumption of normality, standardized gene-level association statistics can be derived by appropriately normalizing a linear combination of SNP-level GWAS statistics by their variance induced by LD. Statistical significance is assessed using the standard normal distribution.\n\nYour tasks, all expressed in purely mathematical and computational terms, are:\n1. For each gene, tissue, and cohort, derive and compute the standardized TWAS test statistic using a linear predictor for gene expression built from eQTL-derived SNP weights, integrating GWAS SNP-level standardized statistics and an LD covariance matrix to normalize the statistic.\n2. Convert the resulting standardized statistics into two-sided p-values using the standard normal distribution. A gene-tissue-cohort association is considered significant if the p-value is strictly less than the significance threshold $\\alpha$.\n3. Compute cross-cohort replication rate per tissue as a decimal as follows: Use cohort $1$ as the reference cohort. For tissue $t$, among the set of genes significant in cohort $1$, count how many are also significant in cohort $2$ and have the same sign of the TWAS statistic in both cohorts. The rate is the count divided by the number of significant genes in cohort $1$. If there are zero significant genes in cohort $1$, define the replication rate to be $0.0$ for that tissue.\n4. Compute cross-tissue reproducibility within each cohort as the Pearson correlation across genes between the TWAS statistics in tissue $1$ and tissue $2$ within the same cohort. If the standard deviation of the TWAS statistics across genes is zero in either tissue for the cohort, define the correlation to be $0.0$ for that cohort to avoid undefined behavior.\n5. Perform a fixed-effect meta-analysis across cohorts per gene and tissue using Stouffer’s method weighted by the square root of cohort sample sizes. Combine cohort-level standardized TWAS statistics for the same gene and tissue into a single meta-analysis statistic, convert to a two-sided p-value, and call meta-analysis significance at threshold $\\alpha$. Compute a cross-tissue meta-analysis replication rate as a decimal by taking the set of genes significant in tissue $1$ meta-analysis as reference, counting those that are also significant in tissue $2$ meta-analysis with the same sign of the combined statistic, and dividing by the size of the reference set. If the reference set is empty, define this rate to be $0.0$.\n\nUnits: No physical units or angles are involved. All rates must be expressed as decimals. All reported values in the final output must be numeric.\n\nAlgorithmic conventions to enforce numerical robustness:\n- If the LD-normalization term equals $0$ for a gene-tissue due to a zero weight vector, define the standardized TWAS statistic to be $0.0$ to avoid division by zero.\n- Use two-sided p-values computed from the standard normal distribution, i.e., $p = 2 \\cdot \\Phi(-|Z|)$ where $Z$ is the standardized TWAS statistic and $\\Phi$ is the cumulative distribution function of the standard normal distribution.\n- For Pearson correlation, if either vector has zero standard deviation, return $0.0$.\n\nTest suite with specified parameter values:\nCase $1$:\n- Tissues: $2$ tissues labeled $T1$ and $T2$.\n- Cohorts: $2$ cohorts labeled $C1$ and $C2$ with sample sizes $N_{1} = 5000$ and $N_{2} = 7000$.\n- Significance threshold: $\\alpha = 0.05$.\n- Genes: $3$ genes labeled $g1$, $g2$, and $g3$.\n- eQTL-derived weights per tissue and gene:\n  - Tissue $T1$: $g1: [0.6, -0.2]$, $g2: [0.5, 0.0, -0.3]$, $g3: [0.0]$.\n  - Tissue $T2$: $g1: [0.3, 0.1]$, $g2: [0.2, 0.4, 0.1]$, $g3: [0.5]$.\n- LD covariance matrices per gene:\n  - $g1$: $$\\Sigma = \\begin{bmatrix} 1.0  0.2 \\\\ 0.2  1.0 \\end{bmatrix}$$\n  - $g2$: $$\\Sigma = \\operatorname{diag}(1.0, 1.0, 1.0)$$\n  - $g3$: $$\\Sigma = [1.0]$$\n- GWAS standardized SNP-level statistics $z$ per cohort:\n  - Cohort $C1$: $g1: [1.2, -0.8]$, $g2: [0.1, 3.5, -2.8]$, $g3: [-0.5]$.\n  - Cohort $C2$: $g1: [1.0, -0.6]$, $g2: [0.2, 3.0, -2.0]$, $g3: [-0.3]$.\n\nCase $2$:\n- Tissues: $2$ tissues labeled $T1$ and $T2$.\n- Cohorts: $2$ cohorts labeled $C1$ and $C2$ with sample sizes $N_{1} = 1000$ and $N_{2} = 1000$.\n- Significance threshold: $\\alpha = 0.05$.\n- Genes: $2$ genes labeled $g1$ and $g2$.\n- eQTL-derived weights per tissue and gene:\n  - Tissue $T1$: $g1: [0.0, 0.0]$, $g2: [-0.5, 0.3]$.\n  - Tissue $T2$: $g1: [0.1, 0.1]$, $g2: [0.5, -0.3]$.\n- LD covariance matrices per gene:\n  - $g1$: $$\\Sigma = \\begin{bmatrix} 1.0  0.5 \\\\ 0.5  1.0 \\end{bmatrix}$$\n  - $g2$: $$\\Sigma = \\begin{bmatrix} 1.0  0.5 \\\\ 0.5  1.0 \\end{bmatrix}$$\n- GWAS standardized SNP-level statistics $z$ per cohort:\n  - Cohort $C1$: $g1: [2.0, -2.0]$, $g2: [2.4, 0.8]$.\n  - Cohort $C2$: $g1: [1.5, -1.5]$, $g2: [-1.8, -0.4]$.\n\nFinal output format requirement:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For Case $1$, produce $5$ floating-point values in the following order: replication rate for tissue $T1$, replication rate for tissue $T2$, cross-tissue Pearson correlation within cohort $C1$, cross-tissue Pearson correlation within cohort $C2$, and the cross-tissue meta-analysis replication rate. For Case $2$, produce the same $5$ values in the same order. Concatenate the two sets for a total of $10$ values in one list. For example, the output must look like $[v_1,v_2,v_3,v_4,v_5,v_6,v_7,v_8,v_9,v_{10}]$ where each $v_i$ is a float computed by your program for the corresponding metric and case.\n\nYour implementation must not read any input and must rely solely on the parameters provided above. All rates must be decimals. No units or angles appear in the problem.", "solution": "The problem presented is a well-defined computational task in the domain of medical genetics, specifically a simplified simulation of a Transcriptome-Wide Association Study (TWAS). It is scientifically grounded in established principles, provides a complete and consistent set of data and definitions, and requests the computation of specific, objective metrics. Therefore, the problem is deemed valid and a full solution is provided below.\n\nThe solution is structured to follow the five tasks outlined in the problem statement. For each gene $g$, tissue $t$, and cohort $c$, we will compute the necessary statistics. Let $W_{g,t}$ be the vector of eQTL-derived SNP weights, $z_{g,c}$ be the vector of GWAS standardized SNP-level statistics, and $\\Sigma_g$ be the LD covariance matrix for the SNPs associated with gene $g$. The sample sizes for cohort $c \\in \\{1, 2\\}$ are denoted by $N_c$. The significance threshold is $\\alpha$.\n\n**1. Standardized TWAS Test Statistic ($Z_{g,t,c}$)**\n\nThe standardized TWAS test statistic, $Z_{g,t,c}$, is derived by taking the linear combination of GWAS SNP statistics, weighted by their eQTL effects on gene expression, and normalizing this sum by its standard deviation under the null hypothesis. The unnormalized statistic is the dot product $W_{g,t}^T z_{g,c}$. The variance of this linear combination, induced by the correlation structure (LD) between SNPs, is given by the quadratic form $W_{g,t}^T \\Sigma_g W_{g,t}$. The standardized statistic is therefore:\n\n$$ Z_{g,t,c} = \\frac{W_{g,t}^T z_{g,c}}{\\sqrt{W_{g,t}^T \\Sigma_g W_{g,t}}} $$\n\nAs per the problem's algorithmic convention, if the denominator is $0$ (which occurs if $W_{g,t}$ is a zero vector), $Z_{g,t,c}$ is defined to be $0.0$.\n\n**2. Two-Sided p-values ($p_{g,t,c}$)**\n\nUnder the null hypothesis of no association, the standardized statistic $Z_{g,t,c}$ follows a standard normal distribution, $\\mathcal{N}(0, 1)$. The two-sided p-value is the probability of observing a test statistic as or more extreme than the one calculated. This is computed as:\n\n$$ p_{g,t,c} = 2 \\cdot \\Phi(-|Z_{g,t,c}|) $$\n\nwhere $\\Phi$ is the cumulative distribution function (CDF) of the standard normal distribution. An association is deemed statistically significant if $p_{g,t,c}  \\alpha$.\n\n**3. Cross-Cohort Replication Rate ($R_t$)**\n\nThis metric assesses the consistency of significant findings between two independent cohorts. Cohort $1$ serves as the reference. For each tissue $t$, we first identify the set of genes that are significant in cohort $1$, denoted $S_{t,C1} = \\{g \\mid p_{g,t,C1}  \\alpha \\}$. The number of such genes is $N_{sig,t,C1} = |S_{t,C1}|$.\n\nWe then count the number of genes in this set that are also significant in cohort $2$ and exhibit an association in the same direction (i.e., their $Z$-scores have the same sign). This count is $N_{rep,t} = |\\{g \\in S_{t,C1} \\mid p_{g,t,C2}  \\alpha \\text{ and } \\operatorname{sign}(Z_{g,t,C1}) = \\operatorname{sign}(Z_{g,t,C2}) \\}|$.\n\nThe replication rate for tissue $t$ is the ratio:\n\n$$ R_t = \\frac{N_{rep,t}}{N_{sig,t,C1}} $$\n\nIf $N_{sig,t,C1} = 0$, the rate $R_t$ is defined as $0.0$.\n\n**4. Cross-Tissue Reproducibility ($\\rho_c$)**\n\nThis metric measures the correlation of gene-level association signals between two different tissues within the same cohort. For each cohort $c$, we form two vectors of Z-scores, one for each tissue, across all genes:\n$\\mathbf{Z}_{T1, c} = (Z_{g1,T1,c}, Z_{g2,T1,c}, \\dots, Z_{G,T1,c})$\n$\\mathbf{Z}_{T2, c} = (Z_{g1,T2,c}, Z_{g2,T2,c}, \\dots, Z_{G,T2,c})$\nwhere $G$ is the total number of genes.\n\nThe reproducibility is the Pearson correlation coefficient between these two vectors:\n\n$$ \\rho_c = \\operatorname{Corr}(\\mathbf{Z}_{T1, c}, \\mathbf{Z}_{T2, c}) $$\n\nAs stipulated, if the standard deviation of either vector is $0$, $\\rho_c$ is defined as $0.0$.\n\n**5. Meta-Analysis and Meta-Replication Rate ($R_{meta}$)**\n\nA fixed-effect meta-analysis combines the results from the two cohorts to obtain a more powerful single estimate of association for each gene-tissue pair. We use Stouffer's method, weighting each cohort's Z-score by the square root of its sample size, $w_c = \\sqrt{N_c}$. The combined Z-score is:\n\n$$ Z_{meta,g,t} = \\frac{\\sum_{c=1}^{2} w_c Z_{g,t,c}}{\\sqrt{\\sum_{c=1}^{2} w_c^2}} = \\frac{\\sqrt{N_1} Z_{g,t,C1} + \\sqrt{N_2} Z_{g,t,C2}}{\\sqrt{N_1 + N_2}} $$\n\nFrom these meta-analysis Z-scores, we compute p-values and determine significance as in step $2$.\n\nFinally, we compute a cross-tissue meta-analysis replication rate, $R_{meta}$. This is analogous to the cross-cohort replication rate, but applied to the meta-analyzed results. Tissue $1$ is the reference. We find the set of genes significant in the tissue $1$ meta-analysis, $S_{meta,T1} = \\{ g \\mid p_{meta,g,T1}  \\alpha \\}$. We then count how many of these are also significant in the tissue $2$ meta-analysis with the same sign of $Z_{meta}$. The rate is the ratio of this count to the size of the reference set, $|S_{meta,T1}|$. If the reference set is empty, the rate is $0.0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main solver function that processes the test cases and prints the final results.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"tissues\": [\"T1\", \"T2\"],\n            \"cohorts\": [\"C1\", \"C2\"],\n            \"n_samples\": {\"C1\": 5000, \"C2\": 7000},\n            \"alpha\": 0.05,\n            \"genes\": [\"g1\", \"g2\", \"g3\"],\n            \"weights\": {\n                \"g1\": {\"T1\": np.array([0.6, -0.2]), \"T2\": np.array([0.3, 0.1])},\n                \"g2\": {\"T1\": np.array([0.5, 0.0, -0.3]), \"T2\": np.array([0.2, 0.4, 0.1])},\n                \"g3\": {\"T1\": np.array([0.0]), \"T2\": np.array([0.5])},\n            },\n            \"ld_matrices\": {\n                \"g1\": np.array([[1.0, 0.2], [0.2, 1.0]]),\n                \"g2\": np.diag([1.0, 1.0, 1.0]),\n                \"g3\": np.array([[1.0]]),\n            },\n            \"gwas_stats\": {\n                \"g1\": {\"C1\": np.array([1.2, -0.8]), \"C2\": np.array([1.0, -0.6])},\n                \"g2\": {\"C1\": np.array([0.1, 3.5, -2.8]), \"C2\": np.array([0.2, 3.0, -2.0])},\n                \"g3\": {\"C1\": np.array([-0.5]), \"C2\": np.array([-0.3])},\n            },\n        },\n        {\n            \"tissues\": [\"T1\", \"T2\"],\n            \"cohorts\": [\"C1\", \"C2\"],\n            \"n_samples\": {\"C1\": 1000, \"C2\": 1000},\n            \"alpha\": 0.05,\n            \"genes\": [\"g1\", \"g2\"],\n            \"weights\": {\n                \"g1\": {\"T1\": np.array([0.0, 0.0]), \"T2\": np.array([0.1, 0.1])},\n                \"g2\": {\"T1\": np.array([-0.5, 0.3]), \"T2\": np.array([0.5, -0.3])},\n            },\n            \"ld_matrices\": {\n                \"g1\": np.array([[1.0, 0.5], [0.5, 1.0]]),\n                \"g2\": np.array([[1.0, 0.5], [0.5, 1.0]]),\n            },\n            \"gwas_stats\": {\n                \"g1\": {\"C1\": np.array([2.0, -2.0]), \"C2\": np.array([1.5, -1.5])},\n                \"g2\": {\"C1\": np.array([2.4, 0.8]), \"C2\": np.array([-1.8, -0.4])},\n            },\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = process_case(case)\n        all_results.extend(results)\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef process_case(params):\n    \"\"\"\n    Processes a single test case, computing all required metrics.\n    \"\"\"\n    alpha = params[\"alpha\"]\n    tissues = params[\"tissues\"]\n    cohorts = params[\"cohorts\"]\n    genes = params[\"genes\"]\n    n_samples = params[\"n_samples\"]\n\n    # --- Task 1  2: Compute Z-scores and p-values ---\n    twas_stats = {}  # {gene: {tissue: {cohort: {'z': ..., 'p': ...}}}}\n    for g in genes:\n        twas_stats[g] = {}\n        for t in tissues:\n            twas_stats[g][t] = {}\n            for c in cohorts:\n                W = params[\"weights\"][g][t]\n                Z_snp = params[\"gwas_stats\"][g][c]\n                Sigma = params[\"ld_matrices\"][g]\n                \n                # Calculate normalization term\n                norm_term_sq = W.T @ Sigma @ W\n\n                if norm_term_sq == 0:\n                    z_score = 0.0\n                else:\n                    numerator = W.T @ Z_snp\n                    z_score = numerator / np.sqrt(norm_term_sq)\n                \n                p_value = 2 * norm.cdf(-np.abs(z_score))\n                twas_stats[g][t][c] = {\"z\": z_score, \"p\": p_value}\n\n    # --- Task 3: Cross-cohort replication rate ---\n    # Reference cohort is C1 (the first in the list)\n    ref_cohort = cohorts[0]\n    comp_cohort = cohorts[1]\n    replication_rates = []\n    for t in tissues:\n        sig_in_ref = {g for g in genes if twas_stats[g][t][ref_cohort][\"p\"]  alpha}\n        n_sig_ref = len(sig_in_ref)\n        \n        if n_sig_ref == 0:\n            replication_rates.append(0.0)\n            continue\n            \n        n_replicated = 0\n        for g in sig_in_ref:\n            is_sig_in_comp = twas_stats[g][t][comp_cohort][\"p\"]  alpha\n            z_ref = twas_stats[g][t][ref_cohort][\"z\"]\n            z_comp = twas_stats[g][t][comp_cohort][\"z\"]\n            signs_match = np.sign(z_ref) == np.sign(z_comp)\n            \n            if is_sig_in_comp and signs_match:\n                n_replicated += 1\n        \n        replication_rates.append(n_replicated / n_sig_ref)\n\n    # --- Task 4: Cross-tissue Pearson correlation ---\n    ref_tissue = tissues[0]\n    comp_tissue = tissues[1]\n    correlations = []\n    for c in cohorts:\n        z_vec_t1 = np.array([twas_stats[g][ref_tissue][c][\"z\"] for g in genes])\n        z_vec_t2 = np.array([twas_stats[g][comp_tissue][c][\"z\"] for g in genes])\n\n        if np.std(z_vec_t1) == 0 or np.std(z_vec_t2) == 0:\n            correlations.append(0.0)\n        else:\n            corr_matrix = np.corrcoef(z_vec_t1, z_vec_t2)\n            correlations.append(corr_matrix[0, 1])\n\n    # --- Task 5: Meta-analysis and meta-replication rate ---\n    meta_stats = {} # {gene: {tissue: {'z': ..., 'p': ...}}}\n    w1 = np.sqrt(n_samples[cohorts[0]])\n    w2 = np.sqrt(n_samples[cohorts[1]])\n    meta_norm_term = np.sqrt(w1**2 + w2**2)\n    \n    for g in genes:\n        meta_stats[g] = {}\n        for t in tissues:\n            z1 = twas_stats[g][t][cohorts[0]][\"z\"]\n            z2 = twas_stats[g][t][cohorts[1]][\"z\"]\n            \n            meta_z = (w1 * z1 + w2 * z2) / meta_norm_term\n            meta_p = 2 * norm.cdf(-np.abs(meta_z))\n            meta_stats[g][t] = {\"z\": meta_z, \"p\": meta_p}\n\n    # Meta-replication\n    sig_meta_t1 = {g for g in genes if meta_stats[g][ref_tissue][\"p\"]  alpha}\n    n_sig_meta_t1 = len(sig_meta_t1)\n    \n    meta_replication_rate = 0.0\n    if n_sig_meta_t1 > 0:\n        n_meta_replicated = 0\n        for g in sig_meta_t1:\n            is_sig_meta_t2 = meta_stats[g][comp_tissue][\"p\"]  alpha\n            z_meta_t1 = meta_stats[g][ref_tissue][\"z\"]\n            z_meta_t2 = meta_stats[g][comp_tissue][\"z\"]\n            signs_match = np.sign(z_meta_t1) == np.sign(z_meta_t2)\n            \n            if is_sig_meta_t2 and signs_match:\n                n_meta_replicated += 1\n        meta_replication_rate = n_meta_replicated / n_sig_meta_t1\n        \n    case_results = [\n        replication_rates[0],  # rep rate T1\n        replication_rates[1],  # rep rate T2\n        correlations[0],       # corr C1\n        correlations[1],       # corr C2\n        meta_replication_rate\n    ]\n    return case_results\n\nsolve()\n```", "id": "5062552"}, {"introduction": "Finding a gene associated with a disease via TWAS is an exciting discovery, but it raises a deeper question: are the genetic signals for the disease and for gene expression truly driven by the same causal variant? This practice delves into statistical colocalization, a Bayesian framework designed to dissect such shared signals and distinguish true shared causality from coincidental association due to Linkage Disequilibrium (LD) [@problem_id:5062585]. By implementing a colocalization analysis, you will learn to quantify the evidence for a shared genetic basis, a key step towards causal inference.", "problem": "You are given a simplified, principled framework to perform statistical colocalization at a single genomic locus using Genome-Wide Association Study (GWAS) and expression Quantitative Trait Locus (eQTL) signals, and to quantify sensitivity to the choice of Bayesian priors and to differences in Linkage Disequilibrium (LD) reference panels. The foundational starting points are: the laws of probability including Bayes’ theorem, the Gaussian likelihood for standardized regression statistics, and the definition of linkage disequilibrium as a correlation structure among single-nucleotide polymorphisms (SNPs). You must derive an implementable algorithm from these principles and compute the posterior probability that both traits share the same causal variant.\n\nStart from these base principles and definitions:\n- Let there be $M$ SNPs indexed by $j \\in \\{1,\\dots,M\\}$ within a locus. For each trait, the vector of marginal $z$-scores, denoted by $\\mathbf{z}_{\\mathrm{marg}} \\in \\mathbb{R}^{M}$, arises from linear regression of the trait on each SNP individually with genotypes standardized to unit variance. Under a single-causal-variant approximation and standardized genotypes, the expected vector of marginal $z$-scores satisfies $\\mathbb{E}[\\mathbf{z}_{\\mathrm{marg}}] = \\mathbf{R}\\,\\mathbf{z}_{\\mathrm{causal}}$, where $\\mathbf{R} \\in \\mathbb{R}^{M \\times M}$ is the LD correlation matrix and $\\mathbf{z}_{\\mathrm{causal}} \\in \\mathbb{R}^{M}$ is a sparse vector whose non-zero entries correspond to the causal SNP(s).\n- By the properties of multivariate normal regression and linear algebra, an estimator for the putative causal $z$-score vector can be obtained by solving the linear system $\\mathbf{R}\\,\\widehat{\\mathbf{z}}_{\\mathrm{causal}} = \\mathbf{z}_{\\mathrm{marg}}$, that is, $\\widehat{\\mathbf{z}}_{\\mathrm{causal}} = \\mathbf{R}^{-1}\\,\\mathbf{z}_{\\mathrm{marg}}$ when $\\mathbf{R}$ is invertible. This step accounts for LD structure and will depend on the chosen LD reference panel.\n- For each SNP $j$, assume a Gaussian prior for the true standardized effect size $\\beta_{j} \\sim \\mathcal{N}(0, W)$ under the alternative hypothesis and a point mass at $0$ under the null hypothesis. With a Gaussian likelihood for the observed standardized effect (equivalently, $z$-score) conditional on $\\beta_{j}$, the per-SNP Bayes factor comparing the alternative to the null is the Wakefield approximate Bayes factor:\n$$\n\\mathrm{ABF}(z_{j}; W) \\;=\\; \\sqrt{\\frac{1}{1 + W}} \\;\\exp\\!\\left(\\frac{1}{2}\\,\\frac{W}{1+W}\\,z_{j}^{2}\\right),\n$$\nwhere $z_{j}$ is the standardized effect (here, we use the deconvolved estimate $z_{j} = \\widehat{z}_{\\mathrm{causal},j}$) and $W$ is the prior variance of the standardized effect size under the alternative.\n- Define the following hypotheses for two traits (trait $1$ for GWAS and trait $2$ for eQTL): $H_{0}$ (no association in either trait), $H_{1}$ (association only in trait $1$), $H_{2}$ (association only in trait $2$), $H_{3}$ (associations in both traits driven by distinct causal variants), and $H_{4}$ (associations in both traits driven by the same causal variant). Assume per-SNP prior probabilities $p_{1}$ for trait $1$-only causality, $p_{2}$ for trait $2$-only causality, and $p_{12}$ for shared causality at the same SNP. Under the rare-causality approximation, the unnormalized posterior weights for each hypothesis are:\n$$\nw_{0} \\;=\\; 1,\n$$\n$$\nw_{1} \\;=\\; \\sum_{j=1}^{M} p_{1}\\,\\mathrm{ABF}_{1,j},\n\\quad\nw_{2} \\;=\\; \\sum_{j=1}^{M} p_{2}\\,\\mathrm{ABF}_{2,j},\n$$\n$$\nw_{3} \\;=\\; \\sum_{\\substack{j=1\\\\k=1\\\\j\\neq k}}^{M} p_{1}\\,p_{2}\\,\\mathrm{ABF}_{1,j}\\,\\mathrm{ABF}_{2,k},\n\\quad\nw_{4} \\;=\\; \\sum_{j=1}^{M} p_{12}\\,\\mathrm{ABF}_{1,j}\\,\\mathrm{ABF}_{2,j},\n$$\nwhere $\\mathrm{ABF}_{1,j}$ and $\\mathrm{ABF}_{2,j}$ are the per-SNP Bayes factors for trait $1$ and trait $2$, respectively, computed from the corresponding deconvolved $z$-scores. The posterior probability for $H_{4}$ is then\n$$\n\\mathrm{PP}_{4} \\;=\\; \\frac{w_{4}}{w_{0} + w_{1} + w_{2} + w_{3} + w_{4}}.\n$$\n\nYour task is to implement the above model exactly as specified and compute $\\mathrm{PP}_{4}$ across a small test suite that varies the prior choices and LD reference panels.\n\nUse the following numerically specified and scientifically plausible test inputs.\n\n- Number of SNPs: $M = 5$.\n\n- LD reference panel A (matrix $\\mathbf{R}_{A}$):\n$$\n\\begin{bmatrix}\n1  0.2  0.1  0  0 \\\\\n0.2  1  0.3  0.1  0 \\\\\n0.1  0.3  1  0.25  0.05 \\\\\n0  0.1  0.25  1  0.2 \\\\\n0  0  0.05  0.2  1\n\\end{bmatrix}\n$$\n\n- LD reference panel B (matrix $\\mathbf{R}_{B}$):\n$$\n\\begin{bmatrix}\n1  0.15  0.05  0  0 \\\\\n0.15  1  0.25  0.05  0 \\\\\n0.05  0.25  1  0.2  0.1 \\\\\n0  0.05  0.2  1  0.25 \\\\\n0  0  0.1  0.25  1\n\\end{bmatrix}\n$$\n\n- Prior variance for standardized effects (same for both traits): $W = 0.2$.\n\n- Prior set A: $p_{1} = 1 \\times 10^{-4}$, $p_{2} = 1 \\times 10^{-4}$, $p_{12} = 1 \\times 10^{-5}$.\n\n- Prior set B: $p_{1} = 1 \\times 10^{-4}$, $p_{2} = 1 \\times 10^{-4}$, $p_{12} = 5 \\times 10^{-5}$.\n\n- Test loci and observed marginal $z$-scores (trait $1$ for GWAS, trait $2$ for eQTL). Each vector is ordered by SNP index $j \\in \\{1,2,3,4,5\\}$.\n    - Locus $1$ (strong, shared signal):\n        - $\\mathbf{z}^{(1)}_{\\mathrm{GWAS}} = [\\,0.6,\\,1.8,\\,6.0,\\,1.5,\\,0.3\\,]$\n        - $\\mathbf{z}^{(1)}_{\\mathrm{eQTL}} = [\\,0.5,\\,1.5,\\,5.0,\\,1.25,\\,0.25\\,]$\n    - Locus $2$ (distinct signals across traits):\n        - $\\mathbf{z}^{(2)}_{\\mathrm{GWAS}} = [\\,1.0,\\,5.0,\\,1.5,\\,0.5,\\,0.0\\,]$\n        - $\\mathbf{z}^{(2)}_{\\mathrm{eQTL}} = [\\,0.0,\\,0.5,\\,1.25,\\,5.0,\\,1.0\\,]$\n    - Locus $3$ (GWAS signal without eQTL support):\n        - $\\mathbf{z}^{(3)}_{\\mathrm{GWAS}} = [\\,0.6,\\,1.8,\\,6.0,\\,1.5,\\,0.3\\,]$\n        - $\\mathbf{z}^{(3)}_{\\mathrm{eQTL}} = [\\,0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0\\,]$\n\nComputation requirements:\n- For each locus, for each LD reference panel ($\\mathbf{R}_{A}$ and $\\mathbf{R}_{B}$), and for each prior set (A and B), compute $\\mathrm{PP}_{4}$ by:\n    $1)$ deconvolving both traits’ marginal $z$-scores using the chosen $\\mathbf{R}$ to obtain $\\widehat{\\mathbf{z}}_{\\mathrm{causal}}$ per trait,\n    $2)$ computing per-SNP approximate Bayes factors with the given $W$,\n    $3)$ forming the unnormalized weights $w_{0},\\dots,w_{4}$ using the specified per-SNP priors, and\n    $4)$ normalizing to obtain $\\mathrm{PP}_{4}$.\n\nTest suite and output specification:\n- Evaluate the $12$ computations in the following fixed order:\n    $1)$ Locus $1$, $\\mathbf{R}_{A}$, Prior A;\n    $2)$ Locus $1$, $\\mathbf{R}_{A}$, Prior B;\n    $3)$ Locus $1$, $\\mathbf{R}_{B}$, Prior A;\n    $4)$ Locus $1$, $\\mathbf{R}_{B}$, Prior B;\n    $5)$ Locus $2$, $\\mathbf{R}_{A}$, Prior A;\n    $6)$ Locus $2$, $\\mathbf{R}_{A}$, Prior B;\n    $7)$ Locus $2$, $\\mathbf{R}_{B}$, Prior A;\n    $8)$ Locus $2$, $\\mathbf{R}_{B}$, Prior B;\n    $9)$ Locus $3$, $\\mathbf{R}_{A}$, Prior A;\n    $10)$ Locus $3$, $\\mathbf{R}_{A}$, Prior B;\n    $11)$ Locus $3$, $\\mathbf{R}_{B}$, Prior A;\n    $12)$ Locus $3$, $\\mathbf{R}_{B}$, Prior B.\n- The program must produce a single line of output containing the $12$ $\\mathrm{PP}_{4}$ values as a comma-separated list enclosed in square brackets, for example, $[x_{1},x_{2},\\dots,x_{12}]$, where each $x_{i}$ is a floating-point number in decimal form. No percentages are allowed; all quantities must be decimals in $[0,1]$.\n\nYour implementation must be a complete runnable program that reads no input and writes exactly one line with the results in the specified order and format.", "solution": "The user has provided a problem that requires the implementation of a statistical colocalization model to compute the posterior probability of a shared causal variant ($H_4$) for a Genome-Wide Association Study (GWAS) signal and an expression Quantitative Trait Locus (eQTL) signal.\n\n### Step 1: Problem Validation\n\nThe problem is first critically assessed for validity.\n\n#### Givens Extraction\nThe givens are extracted verbatim from the problem statement:\n-   **Model Parameters**: Number of SNPs $M=5$. Prior variance for standardized effect sizes $W=0.2$.\n-   **LD Matrices**: $\\mathbf{R}_A$ and $\\mathbf{R}_B$, both $5 \\times 5$ matrices representing linkage disequilibrium correlation structures.\n-   **Prior Sets**:\n    -   Prior A: $p_1 = 1 \\times 10^{-4}$, $p_2 = 1 \\times 10^{-4}$, $p_{12} = 1 \\times 10^{-5}$.\n    -   Prior B: $p_1 = 1 \\times 10^{-4}$, $p_2 = 1 \\times 10^{-4}$, $p_{12} = 5 \\times 10^{-5}$.\n-   **Observed Data (Marginal z-scores)**:\n    -   Locus 1: $\\mathbf{z}^{(1)}_{\\mathrm{GWAS}} = [0.6, 1.8, 6.0, 1.5, 0.3]$, $\\mathbf{z}^{(1)}_{\\mathrm{eQTL}} = [0.5, 1.5, 5.0, 1.25, 0.25]$.\n    -   Locus 2: $\\mathbf{z}^{(2)}_{\\mathrm{GWAS}} = [1.0, 5.0, 1.5, 0.5, 0.0]$, $\\mathbf{z}^{(2)}_{\\mathrm{eQTL}} = [0.0, 0.5, 1.25, 5.0, 1.0]$.\n    -   Locus 3: $\\mathbf{z}^{(3)}_{\\mathrm{GWAS}} = [0.6, 1.8, 6.0, 1.5, 0.3]$, $\\mathbf{z}^{(3)}_{\\mathrm{eQTL}} = [0.0, 0.0, 0.0, 0.0, 0.0]$.\n-   **Core Equations**:\n    1.  Causal z-score estimation: $\\widehat{\\mathbf{z}}_{\\mathrm{causal}} = \\mathbf{R}^{-1}\\,\\mathbf{z}_{\\mathrm{marg}}$.\n    2.  Approximate Bayes Factor: $\\mathrm{ABF}(z_j; W) = \\sqrt{\\frac{1}{1+W}}\\,\\exp(\\frac{1}{2}\\,\\frac{W}{1+W}\\,z_j^2)$, with $z_j = \\widehat{z}_{\\mathrm{causal},j}$.\n    3.  Unnormalized posterior weights: Expressions for $w_0, w_1, w_2, w_3, w_4$.\n    4.  Posterior probability of $H_4$: $\\mathrm{PP}_4 = w_4 / \\sum_{i=0}^4 w_i$.\n-   **Computational Task**: Compute $\\mathrm{PP}_4$ for $12$ specified combinations of locus, LD panel, and prior set.\n\n#### Validation Verdict\n-   **Scientifically Grounded**: The problem describes a simplified version of the `coloc` statistical framework, a standard and widely accepted method in statistical genetics for integrating GWAS and eQTL data. The underlying principles—Bayes' theorem, multivariate normal theory for regression coefficients, and the use of Approximate Bayes Factors (ABF)—are all standard in the field.\n-   **Well-Posed**: All necessary data, equations, and constants are provided. The LD matrices $\\mathbf{R}_A$ and $\\mathbf{R}_B$ are symmetric and positive definite (their determinants are $\\approx$$0.741$ and $\\approx$$0.823$, respectively), ensuring they are invertible and that a unique solution for $\\widehat{\\mathbf{z}}_{\\mathrm{causal}}$ exists. The instructions are unambiguous, leading to a unique numerical result for each specified case.\n-   **Objective**: The problem is stated in precise, mathematical language, free from subjectivity or ambiguity.\n\nThe problem is deemed **valid** as it is self-contained, scientifically sound, and well-posed.\n\n### Step 2: Solution Derivation and Implementation\nThe solution involves systematically applying the provided formulas for each of the $12$ test cases. The algorithm proceeds as follows for each case.\n\n1.  **Select Inputs**: For each case, the specific marginal z-score vectors ($\\mathbf{z}_{\\mathrm{GWAS}}, \\mathbf{z}_{\\mathrm{eQTL}}$), LD matrix ($\\mathbf{R}$), and prior probabilities ($p_1, p_2, p_{12}$) are chosen.\n\n2.  **Deconvolve z-scores to Estimate Causal Effects**: The LD structure masks the true causal signals. To account for this, the marginal z-scores are \"deconvolved\" by inverting the LD matrix. This is a form of statistical fine-mapping.\n    $$ \\widehat{\\mathbf{z}}_{\\mathrm{causal},1} = \\mathbf{R}^{-1}\\mathbf{z}_{\\mathrm{GWAS}} $$\n    $$ \\widehat{\\mathbf{z}}_{\\mathrm{causal},2} = \\mathbf{R}^{-1}\\mathbf{z}_{\\mathrm{eQTL}} $$\n    Here, $\\widehat{\\mathbf{z}}_{\\mathrm{causal},1}$ and $\\widehat{\\mathbf{z}}_{\\mathrm{causal},2}$ are the estimated causal z-score vectors for trait $1$ (GWAS) and trait $2$ (eQTL), respectively.\n\n3.  **Calculate Approximate Bayes Factors (ABF)**: For each SNP $j \\in \\{1, \\dots, M\\}$ and for each trait, an ABF is calculated. The ABF quantifies the evidence for a non-zero effect at that SNP. The calculation uses the components of the estimated causal z-score vectors.\n    Let $c_1 = \\sqrt{\\frac{1}{1+W}}$ and $c_2 = \\frac{1}{2}\\frac{W}{1+W}$. The ABFs for each trait are computed element-wise:\n    $$ \\mathrm{ABF}_{1,j} = c_1 \\exp(c_2 (\\widehat{z}_{\\mathrm{causal},1,j})^2) $$\n    $$ \\mathrm{ABF}_{2,j} = c_1 \\exp(c_2 (\\widehat{z}_{\\mathrm{causal},2,j})^2) $$\n\n4.  **Compute Aggregate Statistics and Unnormalized Posterior Weights**: The per-SNP ABFs are aggregated to form sums needed for the hypothesis weights.\n    $$ S_1 = \\sum_{j=1}^{M} \\mathrm{ABF}_{1,j} $$\n    $$ S_2 = \\sum_{j=1}^{M} \\mathrm{ABF}_{2,j} $$\n    $$ S_{12} = \\sum_{j=1}^{M} \\mathrm{ABF}_{1,j} \\mathrm{ABF}_{2,j} $$\n    These sums are then used to calculate the unnormalized posterior weights for each of the five hypotheses, as defined in the problem statement:\n    $$ w_0 = 1 $$\n    $$ w_1 = p_1 S_1 $$\n    $$ w_2 = p_2 S_2 $$\n    $$ w_3 = p_1 p_2 (S_1 S_2 - S_{12}) $$\n    $$ w_4 = p_{12} S_{12} $$\n\n5.  **Calculate the Posterior Probability for $H_4$**: The weights are normalized by their sum to obtain posterior probabilities. The desired quantity, $\\mathrm{PP}_4$, is the normalized weight for hypothesis $H_4$.\n    $$ \\mathrm{PP}_4 = \\frac{w_4}{w_0 + w_1 + w_2 + w_3 + w_4} $$\n\nThis procedure is implemented and looped over the $12$ specified test cases, and the resulting $\\mathrm{PP}_4$ values are collected.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the specified statistical colocalization model and computes the posterior probability of a shared causal variant (PP4) for a suite of 12 test cases.\n    \"\"\"\n    \n    # ------------------ Define Givens from Problem Statement ------------------\n    \n    # Number of SNPs\n    M = 5\n    \n    # Prior variance for standardized effects\n    W = 0.2\n    \n    # LD reference panels\n    R_A = np.array([\n        [1.0, 0.2, 0.1, 0.0, 0.0],\n        [0.2, 1.0, 0.3, 0.1, 0.0],\n        [0.1, 0.3, 1.0, 0.25, 0.05],\n        [0.0, 0.1, 0.25, 1.0, 0.2],\n        [0.0, 0.0, 0.05, 0.2, 1.0]\n    ])\n    \n    R_B = np.array([\n        [1.0, 0.15, 0.05, 0.0, 0.0],\n        [0.15, 1.0, 0.25, 0.05, 0.0],\n        [0.05, 0.25, 1.0, 0.2, 0.1],\n        [0.0, 0.05, 0.2, 1.0, 0.25],\n        [0.0, 0.0, 0.1, 0.25, 1.0]\n    ])\n    \n    # Prior sets\n    priors_A = {'p1': 1e-4, 'p2': 1e-4, 'p12': 1e-5}\n    priors_B = {'p1': 1e-4, 'p2': 1e-4, 'p12': 5e-5}\n    \n    # Test loci and observed marginal z-scores\n    locus1_z_gwas = np.array([0.6, 1.8, 6.0, 1.5, 0.3])\n    locus1_z_eqtl = np.array([0.5, 1.5, 5.0, 1.25, 0.25])\n    \n    locus2_z_gwas = np.array([1.0, 5.0, 1.5, 0.5, 0.0])\n    locus2_z_eqtl = np.array([0.0, 0.5, 1.25, 5.0, 1.0])\n    \n    locus3_z_gwas = np.array([0.6, 1.8, 6.0, 1.5, 0.3])\n    locus3_z_eqtl = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n\n    # ------------------ Setup Test Suite ------------------\n    \n    test_cases = [\n        # Locus 1\n        (locus1_z_gwas, locus1_z_eqtl, R_A, priors_A),\n        (locus1_z_gwas, locus1_z_eqtl, R_A, priors_B),\n        (locus1_z_gwas, locus1_z_eqtl, R_B, priors_A),\n        (locus1_z_gwas, locus1_z_eqtl, R_B, priors_B),\n        # Locus 2\n        (locus2_z_gwas, locus2_z_eqtl, R_A, priors_A),\n        (locus2_z_gwas, locus2_z_eqtl, R_A, priors_B),\n        (locus2_z_gwas, locus2_z_eqtl, R_B, priors_A),\n        (locus2_z_gwas, locus2_z_eqtl, R_B, priors_B),\n        # Locus 3\n        (locus3_z_gwas, locus3_z_eqtl, R_A, priors_A),\n        (locus3_z_gwas, locus3_z_eqtl, R_A, priors_B),\n        (locus3_z_gwas, locus3_z_eqtl, R_B, priors_A),\n        (locus3_z_gwas, locus3_z_eqtl, R_B, priors_B),\n    ]\n\n    results = []\n    \n    # Pre-calculate ABF constants\n    c1 = np.sqrt(1 / (1 + W))\n    c2 = 0.5 * W / (1 + W)\n    \n    # Pre-calculate matrix inverses\n    inv_R_A = np.linalg.inv(R_A)\n    inv_R_B = np.linalg.inv(R_B)\n    \n    # Map LD matrices to their pre-computed inverses\n    inv_R_map = {id(R_A): inv_R_A, id(R_B): inv_R_B}\n\n    for z_gwas, z_eqtl, R, priors in test_cases:\n        \n        # Step 1: Deconvolve z-scores\n        inv_R = inv_R_map[id(R)]\n        z_causal_gwas = inv_R @ z_gwas\n        z_causal_eqtl = inv_R @ z_eqtl\n        \n        # Step 2: Compute Approximate Bayes Factors (ABF)\n        abf1 = c1 * np.exp(c2 * z_causal_gwas**2)\n        abf2 = c1 * np.exp(c2 * z_causal_eqtl**2)\n        \n        # Step 3: Compute aggregate statistics and unnormalized posterior weights\n        S1 = np.sum(abf1)\n        S2 = np.sum(abf2)\n        S12 = np.sum(abf1 * abf2)\n        \n        p1 = priors['p1']\n        p2 = priors['p2']\n        p12 = priors['p12']\n        \n        w0 = 1.0\n        w1 = p1 * S1\n        w2 = p2 * S2\n        w3 = p1 * p2 * (S1 * S2 - S12)\n        w4 = p12 * S12\n        \n        # Step 4: Calculate the Posterior Probability for H4\n        total_w = w0 + w1 + w2 + w3 + w4\n        pp4 = w4 / total_w if total_w > 0 else 0.0\n        \n        results.append(pp4)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "5062585"}]}