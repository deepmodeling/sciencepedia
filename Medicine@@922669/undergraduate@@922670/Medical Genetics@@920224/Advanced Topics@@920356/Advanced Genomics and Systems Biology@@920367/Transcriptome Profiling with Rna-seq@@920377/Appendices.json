{"hands_on_practices": [{"introduction": "A crucial first step in any RNA-seq analysis is ensuring the integrity and quality of the data. This involves verifying that the experimental procedures in the lab, such as strand-specific library preparation, produced data with the expected characteristics. This exercise [@problem_id:5088488] provides a hands-on look at this process, connecting the molecular biology of a common strand-specific protocol with the statistical reasoning used to confirm its success from sequencing reads. You will use the binomial test, a fundamental tool in bioinformatics, to assess whether the observed read orientations provide strong evidence against the null hypothesis of a non-stranded library.", "problem": "A clinical transcriptome profiling project in medical genetics uses Ribonucleic Acid sequencing (RNA-seq) to quantify messenger Ribonucleic Acid (mRNA) abundances across patient samples. The foundational biological principles are that deoxyribonucleic acid (DNA) is transcribed into RNA in a strand-specific manner, and complementary DNA (cDNA) libraries are constructed from RNA molecules for sequencing. In certain strand-specific protocols, the second-strand cDNA synthesis incorporates deoxyuridine triphosphate (dUTP), which is later selectively degraded, yielding a predictable relationship between the sequenced read orientation and the original transcript strand. Assume that reference gene annotations indicate the transcriptional strand for each gene.\n\nUsing these fundamentals, justify at a high level why strand-specific library preparation can produce a reproducible orientation of reads relative to the annotated gene strand, and why a statistical test based on independent Bernoulli trials is appropriate for assessing strandedness when counting read orientations.\n\nTo empirically infer strandedness from observed data, consider a curated set of genes annotated on the forward DNA strand with no overlapping antisense transcripts. You align read $1$ from a single strand-specific RNA-seq library to these genes and record the orientation of each read relative to the gene annotation. Suppose that of $n=20$ alignments to these genes, $k=17$ reads map in the antisense orientation relative to the gene annotation, with the remaining $n-k$ mapping in the sense orientation.\n\nUnder the null hypothesis that the library is non-stranded (read orientations are symmetric with probability $p = \\frac{1}{2}$ for antisense versus sense), treat the orientation of each read as an independent Bernoulli trial and perform a one-sided binomial test for enrichment of antisense orientations. Compute the exact one-sided $p$-value for observing $k$ or more antisense reads out of $n$ under the null, expressed as a decimal. Round your final numeric answer to four significant figures. No physical units are required for this quantity.", "solution": "The problem statement is scientifically sound and computationally well-posed. It accurately describes the principles of a dUTP-based strand-specific RNA-seq library preparation protocol and poses a standard statistical question used for quality control in bioinformatics. All necessary data for the calculation are provided.\n\nFirst, we address the qualitative justification for the strand-specific nature of the library and the appropriateness of a Bernoulli trial model.\n\nThe predictability of read orientation in a strand-specific protocol, such as the described dUTP method, arises directly from the molecular biology of complementary DNA (cDNA) synthesis and selection. The process can be summarized as follows:\n$1$. **First-Strand Synthesis**: The messenger RNA (mRNA) molecule serves as a template. A reverse transcriptase enzyme synthesizes a single strand of cDNA that is complementary to the mRNA. This is the \"first strand\". The sequence of this strand is the reverse complement of the original RNA.\n$2$. **Second-Strand Synthesis**: The first-strand cDNA then serves as a template for synthesizing a second, complementary DNA strand. In the dUTP protocol, this synthesis occurs in a reaction mixture where deoxythymidine triphosphate (dTTP) is replaced with deoxyuridine triphosphate (dUTP). Consequently, the newly synthesized \"second strand\" incorporates uracil (U) at positions where thymine (T) would normally be.\n$3$. **Strand Selection**: The resulting double-stranded cDNA-DNA hybrid contains one \"normal\" strand (the first strand) and one uracil-containing strand (the second strand). Treatment with Uracil-DNA Glycosylase (UDG) specifically excises the uracil bases from the second strand. This is followed by treatment with an endonuclease (like APE1) that cleaves the phosphodiester backbone at these abasic sites, effectively degrading the second strand.\n$4$. **Sequencing**: As a result, only the first-strand cDNA molecules (or their amplification products) are ligated to sequencing adapters and sequenced. Since all sequenced fragments originate from the first strand, which has a fixed relationship to the original RNA transcript (it is the reverse complement), the resulting sequencing reads will have a consistent, non-random orientation relative to the gene from which the RNA was transcribed. For instance, if the sequencing process reads the first strand directly, all reads will map to the strand antisense to the gene's coding sequence. This justifies why strand-specific library preparation produces a reproducible orientation of reads.\n\nNext, we justify the use of a statistical test based on independent Bernoulli trials. Under the null hypothesis ($H_0$), the library is assumed to be *non-stranded*. In a non-stranded protocol, both the first and second cDNA strands are ligated and sequenced with equal probability. This means that for any given transcript, a read derived from it has a probability $p = \\frac{1}{2}$ of originating from the first strand (mapping antisense, for example) and a probability $1-p = \\frac{1}{2}$ of originating from the second strand (mapping sense). The orientation of each sequenced read can thus be modeled as a single Bernoulli trial with two outcomes (\"antisense\" or \"sense\") and a probability of success (e.g., \"antisense\") of $p = \\frac{1}{2}$. Assuming that the fragmentation and sequencing of RNA molecules are independent events, the orientations of $n$ different reads constitute a series of $n$ independent Bernoulli trials. The total count of reads with a specific orientation (e.g., antisense) in a sample of size $n$ will therefore follow a binomial distribution. This makes the binomial test the appropriate statistical framework for assessing strandedness.\n\nNow, we perform the quantitative analysis requested. We are asked to compute the exact one-sided $p$-value for observing $k$ or more antisense reads out of $n$ total reads, under the null hypothesis of a non-stranded library.\n\nLet $X$ be the random variable representing the number of reads mapping in the antisense orientation.\nThe total number of trials (aligned reads) is $n=20$.\nThe observed number of antisense reads is $k=17$.\n\nThe null hypothesis, $H_0$, is that the library is non-stranded, which implies that the probability of a read mapping to the antisense orientation is $p = \\frac{1}{2}$.\nThe alternative hypothesis, $H_a$, is that there is an enrichment of antisense orientations, meaning $p > \\frac{1}{2}$.\n\nUnder $H_0$, the random variable $X$ follows a binomial distribution, $X \\sim B(n, p)$, with parameters $n=20$ and $p=\\frac{1}{2}$. The probability mass function is given by:\n$$P(X=j) = \\binom{n}{j} p^j (1-p)^{n-j}$$\nFor our case, this becomes:\n$$P(X=j) = \\binom{20}{j} \\left(\\frac{1}{2}\\right)^j \\left(1-\\frac{1}{2}\\right)^{20-j} = \\binom{20}{j} \\left(\\frac{1}{2}\\right)^{20}$$\nThe one-sided $p$-value is the probability of observing a result at least as extreme as the one measured, assuming $H_0$ is true. We are testing for an enrichment of antisense reads, so we calculate $P(X \\ge k)$.\n$$p\\text{-value} = P(X \\ge 17) = \\sum_{j=17}^{20} P(X=j)$$\n$$p\\text{-value} = P(X=17) + P(X=18) + P(X=19) + P(X=20)$$\n$$p\\text{-value} = \\binom{20}{17}\\left(\\frac{1}{2}\\right)^{20} + \\binom{20}{18}\\left(\\frac{1}{2}\\right)^{20} + \\binom{20}{19}\\left(\\frac{1}{2}\\right)^{20} + \\binom{20}{20}\\left(\\frac{1}{2}\\right)^{20}$$\nWe can factor out the probability term:\n$$p\\text{-value} = \\left(\\frac{1}{2}\\right)^{20} \\left[ \\binom{20}{17} + \\binom{20}{18} + \\binom{20}{19} + \\binom{20}{20} \\right]$$\nNow, we compute the binomial coefficients:\n$$\\binom{20}{17} = \\frac{20!}{17!(20-17)!} = \\frac{20!}{17!3!} = \\frac{20 \\times 19 \\times 18}{3 \\times 2 \\times 1} = 20 \\times 19 \\times 3 = 1140$$\n$$\\binom{20}{18} = \\frac{20!}{18!(20-18)!} = \\frac{20!}{18!2!} = \\frac{20 \\times 19}{2 \\times 1} = 10 \\times 19 = 190$$\n$$\\binom{20}{19} = \\frac{20!}{19!(20-19)!} = \\frac{20!}{19!1!} = 20$$\n$$\\binom{20}{20} = \\frac{20!}{20!(20-20)!} = \\frac{20!}{20!0!} = 1$$\nSumming these coefficients:\n$$1140 + 190 + 20 + 1 = 1351$$\nThe probability term is:\n$$\\left(\\frac{1}{2}\\right)^{20} = \\frac{1}{2^{20}} = \\frac{1}{1048576}$$\nNow, we compute the $p$-value:\n$$p\\text{-value} = \\frac{1351}{1048576} \\approx 0.0012883944699...$$\nThe problem requires rounding the final numeric answer to four significant figures. The first non-zero digit is $1$ in the thousandths place. The fourth significant digit is the second $8$. The next digit is $3$, which is less than $5$, so we round down (i.e., we do not change the fourth digit).\n$$p\\text{-value} \\approx 0.001288$$\nThis extremely small $p$-value provides strong evidence to reject the null hypothesis, leading to the conclusion that the library is indeed strand-specific with an enrichment for antisense reads.", "answer": "$$\\boxed{0.001288}$$", "id": "5088488"}, {"introduction": "One of the most powerful applications of RNA-seq in medical genetics is elucidating the functional consequences of genetic variants. This practice [@problem_id:5088382] simulates a realistic clinical scenario where you must determine if a heterozygous truncating variant triggers Nonsense-Mediated Decay (NMD), a cellular surveillance mechanism that degrades faulty transcripts. You will learn to synthesize multiple lines of evidence from RNA-seq data—including allele-specific expression, total gene expression levels, and transcript coverage patterns—to build a robust argument for a variant's molecular impact.", "problem": "A heterozygous patient harbors a truncating variant in gene $G$ that introduces a premature termination codon (PTC) located $80$ nucleotides upstream of the final exon–exon junction. Based on the Exon Junction Complex (EJC) rule of Nonsense-Mediated Decay (NMD), a PTC positioned more than approximately $55$ nucleotides upstream of the last exon–exon junction is predicted to trigger NMD. You have bulk ribonucleic acid sequencing (RNA-seq) from the patient’s skeletal muscle and $10$ matched controls. At a heterozygous single-nucleotide polymorphism (SNP) situated upstream of the PTC, the patient’s allele-specific counts are $n_R = 100$ reads carrying the reference allele and $n_A = 20$ reads carrying the alternate (mutant) allele, while matched controls exhibit approximately balanced allelic counts near $n_R \\approx n_A$. Gene-level normalized counts for $G$ are $c_P = 600$ for the patient and an average $c_C = 1100$ across controls. Aggregated exon-level coverage upstream of the PTC is $u_P = 400$ in the patient versus $u_C = 720$ in controls, and downstream of the PTC is $d_P = 200$ in the patient versus $d_C = 380$ in controls. Assume RNA-seq read counts sample from the underlying steady-state transcript abundance and that, in the absence of allelic bias, allele-specific reads at heterozygous sites follow a binomial sampling model with null proportion $p = 0.5$ for either allele. Also assume standard differential expression inference models total expression as proportional to true abundance.\n\nWhich option best describes how a truncating variant that triggers NMD would be expected to manifest in allele-specific expression and total expression, and outlines a principled RNA-seq analysis to corroborate NMD in this patient?\n\nA. Under NMD, transcripts from the mutant allele are selectively degraded, leading to allelic imbalance with depletion of the mutant allele across the gene and an overall reduction in total gene expression. A principled analysis includes (i) testing allele-specific imbalance at heterozygous sites using a binomial test with null $p = 0.5$ and evaluating the tail probability for the observed $n_A$ versus $n_R$, (ii) quantifying gene-level log fold-change $\\log_2(c_P / c_C)$ to assess reduced total expression, (iii) confirming position-based plausibility using the EJC rule ($80 > 55$ nucleotides), and (iv) checking that upstream and downstream exons are reduced by a similar factor, consistent with global transcript depletion rather than localized coverage drop.\n\nB. Under NMD, only exons downstream of the PTC are affected, and allele-specific expression remains balanced at $p = 0.5$. A principled analysis focuses on detecting a sharp $3'$ coverage drop immediately downstream of the PTC using a simple $t$-test, with no expected change in total gene expression.\n\nC. Under NMD, the reference allele is preferentially degraded due to feedback compensation, increasing the mutant allele’s proportion. A principled analysis uses Hardy–Weinberg equilibrium to test allelic ratios and expects increased total gene expression for $G$ because premature termination codons upregulate transcription.\n\nD. Under NMD, allelic imbalance is expected only at SNPs located downstream of the PTC; upstream SNPs remain balanced. A principled analysis uses a chi-squared goodness-of-fit test with null $p = 0.33$ to reflect three possible allelic states and focuses on untranslated region coverage comparisons, as coding-region reads are uninformative for NMD.", "solution": "The problem statement will first be validated for scientific and logical integrity.\n\n### Step 1: Extract Givens\n- **Patient Condition**: Heterozygous for a truncating variant in gene $G$.\n- **Variant Type**: Introduces a premature termination codon (PTC).\n- **PTC Location**: $80$ nucleotides upstream of the final exon–exon junction.\n- **NMD Rule**: A PTC located more than approximately $55$ nucleotides upstream of the last exon–exon junction is predicted to trigger Nonsense-Mediated Decay (NMD).\n- **Data Source**: Bulk ribonucleic acid sequencing (RNA-seq) from the patient’s skeletal muscle and $10$ matched controls.\n- **Allele-Specific Counts (Patient)**: At a heterozygous single-nucleotide polymorphism (SNP) upstream of the PTC, $n_R = 100$ reads (reference allele) and $n_A = 20$ reads (alternate/mutant allele).\n- **Allele-Specific Counts (Controls)**: Approximately balanced allelic counts, $n_R \\approx n_A$.\n- **Gene-Level Counts**: Patient normalized counts $c_P = 600$; average control normalized counts $c_C = 1100$.\n- **Exon-Level Coverage (Upstream of PTC)**: Patient $u_P = 400$; controls $u_C = 720$.\n- **Exon-Level Coverage (Downstream of PTC)**: Patient $d_P = 200$; controls $d_C = 380$.\n- **Assumption 1**: RNA-seq read counts sample from steady-state transcript abundance.\n- **Assumption 2**: For allele-specific reads at heterozygous sites, the null model (no bias) is a binomial distribution with proportion $p = 0.5$.\n- **Assumption 3**: Standard differential expression models relate total expression to true abundance.\n- **Question**: Describe the expected manifestation of NMD in RNA-seq data and outline a principled analysis to corroborate it.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated for validity.\n\n- **Scientific Grounding**: The problem is firmly grounded in established molecular biology and genetics. Nonsense-Mediated Decay (NMD) is a well-characterized cellular surveillance pathway. The \"EJC rule\" (also known as the \"50-55 nucleotide rule\") is a central and empirically validated principle of NMD, stating that PTCs located more than $50-55$ nucleotides upstream of the final exon-exon junction typically trigger mRNA degradation. The use of RNA-seq to query allele-specific expression (ASE) and total gene expression is the standard state-of-the-art methodology for functionally validating the impact of such variants. All concepts are scientifically sound.\n- **Well-Posedness**: The problem provides a clear biological scenario, quantitative data consistent with that scenario, and a specific question about the expected effects and correct analytical approach. A unique, best answer can be determined from the provided options based on first principles.\n- **Objectivity**: The problem is stated using precise, objective, and quantitative language. No subjective or opinion-based claims are made.\n- **Consistency and Completeness**: The provided data are internally consistent and support the premise. The PTC is $80$ nucleotides upstream of the final exon-exon junction, which satisfies the condition ($80 > 55$) for expecting NMD. The patient's allelic counts ($n_R=100, n_A=20$) show strong imbalance, with depletion of the mutant allele. The patient's total gene expression ($c_P=600$) is reduced compared to controls ($c_C=1100$). These data points are all concordant with the expected molecular consequence of NMD on a heterozygous variant. The problem is self-contained and provides all necessary information and assumptions.\n- **No Other Flaws**: The problem does not violate any other criteria for validity. It is not trivial, metaphorical, or unfalsifiable.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived.\n\n### Derivation of Principles and Solution\n\nNonsense-Mediated Decay (NMD) is a crucial mRNA quality-control mechanism that targets and degrades transcripts containing a premature termination codon (PTC) to prevent the synthesis of potentially deleterious truncated proteins.\n\n1.  **Mechanism and Allelic Specificity**: In a heterozygous individual, there are two alleles for gene $G$: a reference (wild-type) allele and a mutant allele carrying the PTC. The NMD machinery specifically recognizes and degrades the mRNA transcripts originating from the mutant allele. Transcripts from the reference allele are not affected.\n2.  **Effect on Allele-Specific Expression (ASE)**: Because NMD selectively reduces the abundance of mutant allele transcripts, the steady-state pool of mRNA will be depleted of these transcripts. When performing RNA-seq, the reads are sampled from this pool. At a heterozygous SNP that distinguishes the two alleles, we expect to observe significantly fewer reads corresponding to the mutant allele compared to the reference allele. This phenomenon is termed allelic imbalance. The null hypothesis of no imbalance is that reads from either allele should be observed with equal probability, i.e., $p=0.5$. The data provided, $n_R = 100$ and $n_A = 20$, clearly show an imbalance favoring the reference allele. A binomial test can formally assess the statistical significance of this deviation from the expected $1:1$ ratio.\n3.  **Effect on Total Gene Expression**: If transcripts from one of the two alleles are substantially degraded, the total steady-state abundance of mRNA for gene $G$ in the patient should be lower than in a control individual with two functional alleles. Assuming equal transcription rates from both alleles and perfectly efficient NMD, one would expect the total expression to be reduced by approximately $50\\%$. The data show patient expression $c_P = 600$ and average control expression $c_C = 1100$. The ratio is $c_P/c_C = 600/1100 \\approx 0.545$, which is a reduction of approximately $45.5\\%$, consistent with the expected effect of NMD. This change can be quantified using a log fold-change, $\\log_2(c_P/c_C)$.\n4.  **Effect on Transcript Coverage Profile**: NMD results in the degradation of the *entire* mRNA molecule containing the PTC. It does not simply truncate the transcript or degrade only the portion downstream of the PTC. Therefore, the reduction in transcript abundance should be evident across the entire length of the gene. An analysis of RNA-seq coverage should show a proportional decrease in read depth across both upstream and downstream exons in the patient compared to controls. This distinguishes NMD from a scenario where a stable, truncated transcript is produced, which might cause a sharp drop in coverage only downstream of the PTC. Let's examine the reduction factor:\n    -   Upstream: Patient/Control = $u_P/u_C = 400/720 \\approx 0.556$.\n    -   Downstream: Patient/Control = $d_P/d_C = 200/380 \\approx 0.526$.\n    The reduction factors are similar ($\\approx 53-56\\%$), supporting the model of whole-transcript degradation.\n5.  **Principled Analysis Workflow**: A rigorous analysis to corroborate NMD should integrate these distinct lines of evidence:\n    *   (i) Check positional plausibility: Does the PTC location satisfy the EJC rule? (Yes, $80 > 55$ nucleotides).\n    *   (ii) Test for significant allelic imbalance: Use a binomial test on ASE counts ($n_A, n_R$) with a null of $p=0.5$.\n    *   (iii) Test for reduced total gene expression: Compare patient expression to control expression (e.g., calculate log fold-change).\n    *   (iv) Examine the coverage profile: Ensure the reduction in expression is consistent across the transcript body, comparing patient vs. control.\n\n### Option-by-Option Analysis\n\n**A. Under NMD, transcripts from the mutant allele are selectively degraded, leading to allelic imbalance with depletion of the mutant allele across the gene and an overall reduction in total gene expression. A principled analysis includes (i) testing allele-specific imbalance at heterozygous sites using a binomial test with null $p = 0.5$ and evaluating the tail probability for the observed $n_A$ versus $n_R$, (ii) quantifying gene-level log fold-change $\\log_2(c_P / c_C)$ to assess reduced total expression, (iii) confirming position-based plausibility using the EJC rule ($80 > 55$ nucleotides), and (iv) checking that upstream and downstream exons are reduced by a similar factor, consistent with global transcript depletion rather than localized coverage drop.**\n\nThis option accurately describes the key molecular consequences of NMD: selective degradation of the mutant allele transcript, leading to allelic imbalance and reduced total gene expression. The proposed four-part analysis is comprehensive and methodologically sound. It correctly specifies the use of a binomial test for ASE, log fold-change for total expression, the EJC positional rule as prior evidence, and the inspection of the coverage profile to confirm whole-transcript degradation. This aligns perfectly with the principled derivation.\n**Verdict: Correct.**\n\n**B. Under NMD, only exons downstream of the PTC are affected, and allele-specific expression remains balanced at $p = 0.5$. A principled analysis focuses on detecting a sharp $3'$ coverage drop immediately downstream of the PTC using a simple $t$-test, with no expected change in total gene expression.**\n\nThis option is fundamentally flawed. First, NMD degrades the entire transcript, not just the downstream portion. Second, the selective degradation of the mutant allele is the very cause of allelic imbalance; expression would not remain balanced. Third, because an entire population of transcripts is removed, total gene expression is expected to decrease significantly. A sharp $3'$ drop is characteristic of a stable truncated protein, not NMD.\n**Verdict: Incorrect.**\n\n**C. Under NMD, the reference allele is preferentially degraded due to feedback compensation, increasing the mutant allele’s proportion. A principled analysis uses Hardy–Weinberg equilibrium to test allelic ratios and expects increased total gene expression for $G$ because premature termination codons upregulate transcription.**\n\nThis option contains multiple severe scientific errors. NMD targets the PTC-containing transcript, which is the *mutant* allele, not the reference allele. Hardy-Weinberg equilibrium is a principle of population genetics related to genotype frequencies at the DNA level in a population; it is completely irrelevant to RNA expression analysis within a single individual. The premise that PTCs generally upregulate transcription is false; the dominant effect on steady-state mRNA levels is a decrease due to degradation.\n**Verdict: Incorrect.**\n\n**D. Under NMD, allelic imbalance is expected only at SNPs located downstream of the PTC; upstream SNPs remain balanced. A principled analysis uses a chi-squared goodness-of-fit test with null $p = 0.33$ to reflect three possible allelic states and focuses on untranslated region coverage comparisons, as coding-region reads are uninformative for NMD.**\n\nThis option is incorrect on several counts. The degradation of the entire transcript means that any heterozygous SNP on that transcript, regardless of its position relative to the PTC, can be used to measure allelic imbalance. The statistical test proposed is nonsensical; a heterozygous SNP has two allelic states, so the null hypothesis for a binomial or chi-squared test should be based on a $1:1$ ratio ($p=0.5$), not $p=0.33$ or three states. Finally, reads from all parts of the transcript, including the coding region (CDS) and untranslated regions (UTRs), are informative, as the entire molecule is the unit of degradation.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "5088382"}, {"introduction": "Generating a list of differentially expressed genes is a primary goal of many RNA-seq studies, but the validity of that list depends critically on the statistical model used. An invalid model can produce an excess of false positives, leading to wasted resources and incorrect conclusions. This exercise [@problem_id:5088384] introduces a vital diagnostic technique for assessing the statistical soundness of an analysis pipeline: checking the p-value distribution under a permuted null. By comparing a naive t-test on transformed data with a purpose-built negative binomial model, you will learn to identify the signs of a poorly calibrated, anti-conservative test and understand why specialized count-based models are indispensable for RNA-seq analysis.", "problem": "An RNA sequencing (RNA-seq) study compares two phenotypic groups with sample sizes $n_1 = 6$ and $n_2 = 6$. Differential expression is evaluated across $m = 20000$ genes. Two analysis pipelines are considered.\n\n- Pipeline $\\mathrm{P1}$: Per-gene two-sample $t$-tests on $\\log_2(\\mathrm{TPM}+1)$ transformed expression values, without additional covariates.\n- Pipeline $\\mathrm{P2}$: Negative binomial generalized linear model (GLM) on raw counts with Trimmed Mean of M-values (TMM) normalization and a design matrix that can include covariates, fitted with empirical dispersion moderation; independent filtering removes genes with extremely low counts.\n\nTo assess calibration of $p$-values under the null hypothesis, you rerun each pipeline on a permuted dataset where group labels are randomly reassigned, breaking any association between group and expression while preserving the count structure. You summarize the empirical $p$-value distributions by the number of $p$-values less than a nominal level $0.05$:\n\n- For $\\mathrm{P1}$ on permuted labels: $1500$ of $m = 20000$ $p$-values are below $0.05$.\n- For $\\mathrm{P2}$ on permuted labels: $980$ of $m = 20000$ $p$-values are below $0.05$.\n\nOn the original (unpermuted) labels, $\\mathrm{P1}$ reports $4400$ $p$-values below $0.05$.\n\nAssume the following fundamental base facts:\n- Under a true null hypothesis with a valid test, the $p$-value $P$ is uniformly distributed on $[0,1]$, so for any $\\alpha \\in (0,1)$, $\\Pr(P \\le \\alpha) = \\alpha$.\n- RNA-seq counts exhibit a mean–variance relationship consistent with a negative binomial distribution, and valid testing typically requires normalization for library size and modeling of dispersion and relevant covariates.\n\nWhich option best diagnoses the deviation from the uniform null and proposes an appropriate corrective action for $\\mathrm{P1}$?\n\nA. Diagnosis: Many truly differentially expressed genes explain the excess of small $p$-values in $\\mathrm{P1}$. Action: Accept $\\mathrm{P1}$ $p$-values as calibrated and proceed directly to False Discovery Rate (FDR) control.\n\nB. Diagnosis: Anti-conservative $p$-values due to model mis-specification and unmodeled structure; using $t$-tests on $\\log_2(\\mathrm{TPM}+1)$ ignores count mean–variance and possible sample-level covariates, inflating small $p$-values even under permutation. Action: Refit with a count-aware model (e.g., negative binomial GLM with TMM), include known covariates or perform Surrogate Variable Analysis (SVA), apply independent filtering of low counts, then reassess calibration and apply FDR control.\n\nC. Diagnosis: Overly conservative testing in $\\mathrm{P1}$ due to aggressive outlier removal. Action: Reduce minimum count thresholds and remove normalization to increase the number of discoveries.\n\nD. Diagnosis: The deviation arises from not applying a Bonferroni correction. Action: Apply Bonferroni correction to $\\mathrm{P1}$ $p$-values; this will restore a uniform null distribution.\n\nE. Diagnosis: Misalignment to the reference genome is the primary cause of small $p$-values under permutation. Action: Remap all reads to a different reference assembly to correct the $p$-value distribution shape.\n\nSelect the single best option.", "solution": "### Step 1: Extract Givens\nThe problem statement provides the following information:\n- Study type: RNA sequencing (RNA-seq) comparing two phenotypic groups.\n- Sample sizes: $n_1 = 6$ and $n_2 = 6$.\n- Number of genes tested: $m = 20000$.\n- Pipeline $\\mathrm{P1}$: Per-gene two-sample $t$-tests on $\\log_2(\\mathrm{TPM}+1)$ transformed data, no covariates.\n- Pipeline $\\mathrm{P2}$: Negative binomial generalized linear model (GLM) on raw counts, with Trimmed Mean of M-values (TMM) normalization, empirical dispersion moderation, and independent filtering. It can model covariates.\n- Null simulation via permutation of group labels:\n    - Result for $\\mathrm{P1}$: $1500$ of $m = 20000$ $p$-values are less than $0.05$.\n    - Result for $\\mathrm{P2}$: $980$ of $m = 20000$ $p$-values are less than $0.05$.\n- Result for $\\mathrm{P1}$ on original data: $4400$ $p$-values are less than $0.05$.\n- Fundamental assumptions:\n    1. For a valid test under the true null hypothesis, the $p$-value distribution is uniform on $[0,1]$, such that $\\Pr(P \\le \\alpha) = \\alpha$ for any significance level $\\alpha \\in (0,1)$.\n    2. RNA-seq counts follow a distribution (like the negative binomial) with a strong mean–variance relationship, requiring specific statistical modeling.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded, well-posed, and objective. It presents a realistic scenario in bioinformatics concerning the statistical analysis of RNA-seq data. The described pipelines, $\\mathrm{P1}$ and $\\mathrm{P2}$, represent an older, now-suboptimal method and a modern, best-practice method, respectively. The data provided ($n_1=6$, $n_2=6$, $m=20000$) are typical. The results of the permutation analysis are plausible and serve to illustrate a key statistical concept: the calibration of $p$-values under the null hypothesis. The problem does not violate any fundamental scientific principles, is not incomplete or contradictory, and uses well-defined terminology within the field of medical genetics and bioinformatics. The problem is valid.\n\n### Step 3: Derivation and Option Evaluation\nThe core of the problem is to assess the validity of the statistical tests performed by Pipeline $\\mathrm{P1}$. This is achieved by examining the distribution of its $p$-values under the null hypothesis, which is simulated by permuting the sample group labels.\n\n**Analysis of P-value Calibration**\n\nAccording to the provided base fact, for a valid test under the null hypothesis, the distribution of $p$-values should be uniform on the interval $[0,1]$. This implies that the expected proportion of $p$-values less than or equal to a given threshold $\\alpha$ is $\\alpha$.\n\nIn this problem, the analysis is performed on $m = 20000$ genes, and the threshold is $\\alpha = 0.05$. In the permuted dataset, the null hypothesis is true for all genes by construction. Therefore, the expected number of $p$-values less than $0.05$ is:\n$$ E[\\text{count of } p \\le 0.05] = m \\times \\alpha = 20000 \\times 0.05 = 1000 $$\n\n**Evaluation of Pipeline $\\mathrm{P1}$**\n\nPipeline $\\mathrm{P1}$ produced $1500$ $p$-values less than $0.05$ on the permuted data. This is $50\\%$ more than the expected number of $1000$. The empirical proportion is $1500 / 20000 = 0.075$, which is substantially greater than the nominal level of $0.05$. This indicates that the $p$-value distribution from $\\mathrm{P1}$ is not uniform under the null; it is skewed towards small values. Such a test is termed **anti-conservative** or **liberal**. It inflates the test statistic, leading to an excess of false positives.\n\nThe cause of this anti-conservatism lies in the model misspecification of $\\mathrm{P1}$. A simple two-sample $t$-test on $\\log_2(\\mathrm{TPM}+1)$ data has several known flaws when applied to RNA-seq counts:\n1.  **Inadequate Variance Stabilization**: The $\\log_2(x+1)$ transformation does not fully stabilize the variance of count data. For genes with low counts, the variance after transformation remains higher than for genes with high counts. The $t$-test assumes homoscedasticity (or uses a Welch correction, but still relies on approximations that may not hold well), and this violation leads to incorrect variance estimates and inflated test statistics, particularly for low-count genes.\n2.  **Ignoring the Count Nature**: The data are fundamentally counts, best described by discrete distributions like the Poisson or, more accurately, the negative binomial. The $t$-test assumes normality of the underlying data, an assumption that is often poorly met by transformed count data, especially with small sample sizes ($n_1=n_2=6$).\n3.  **Unmodeled Heterogeneity**: $\\mathrm{P1}$ does not account for covariates. Biological experiments are often affected by batch effects or other confounding variables. If these are not modeled, they contribute to the residual variance, and if correlated by chance with the (permuted) group labels, they can create spurious associations, further inflating test statistics.\n\n**Evaluation of Pipeline $\\mathrm{P2}$**\n\nPipeline $\\mathrm{P2}$ produced $980$ $p$-values less than $0.05$. This is very close to the expected number of $1000$. The empirical proportion is $980 / 20000 = 0.049 \\approx 0.05$. This demonstrates that $\\mathrm{P2}$ is **well-calibrated**. Its $p$-values are approximately uniformly distributed under the null. This is because $\\mathrm{P2}$ uses methods specifically designed for RNA-seq data (negative binomial GLM, TMM normalization, empirical Bayes dispersion shrinkage), which correctly model the mean-variance relationship and account for library size differences. Independent filtering of low-count genes removes those most likely to violate model assumptions, and the ability to include covariates handles confounding factors.\n\n### Option-by-Option Analysis\n\n**A. Diagnosis: Many truly differentially expressed genes explain the excess of small $p$-values in $\\mathrm{P1}$. Action: Accept $\\mathrm{P1}$ $p$-values as calibrated and proceed directly to False Discovery Rate (FDR) control.**\n- **Analysis**: The diagnosis is fundamentally flawed. The test was run on a *permuted dataset*, where, by construction, there are no truly differentially expressed genes. The excess of small $p$-values is a statistical artifact of the testing procedure, not a biological signal. The action is therefore inappropriate; applying FDR control to anti-conservative $p$-values would lead to an excessive number of false discoveries and an unreliable final gene list.\n- **Verdict**: Incorrect.\n\n**B. Diagnosis: Anti-conservative $p$-values due to model mis-specification and unmodeled structure; using $t$-tests on $\\log_2(\\mathrm{TPM}+1)$ ignores count mean–variance and possible sample-level covariates, inflating small $p$-values even under permutation. Action: Refit with a count-aware model (e.g., negative binomial GLM with TMM), include known covariates or perform Surrogate Variable Analysis (SVA), apply independent filtering of low counts, then reassess calibration and apply FDR control.**\n- **Analysis**: This diagnosis accurately identifies the problem as anti-conservative $p$-values ($1500 > 1000$) and correctly attributes it to the model misspecification inherent in using a $t$-test on log-transformed counts, as well as potential unmodeled covariates. The proposed action—to use a pipeline essentially identical to the well-calibrated $\\mathrm{P2}$—is the standard and correct remedy in modern bioinformatics. This addresses all identified flaws of $\\mathrm{P1}$.\n- **Verdict**: Correct.\n\n**C. Diagnosis: Overly conservative testing in $\\mathrm{P1}$ due to aggressive outlier removal. Action: Reduce minimum count thresholds and remove normalization to increase the number of discoveries.**\n- **Analysis**: The diagnosis is the opposite of what the data show. The test is anti-conservative (too many small $p$-values), not conservative (too few). The proposed action is also counterproductive. Removing normalization is a critical analysis error that invalidates results, and including more low-count genes would likely worsen the statistical artifacts.\n- **Verdict**: Incorrect.\n\n**D. Diagnosis: The deviation arises from not applying a Bonferroni correction. Action: Apply Bonferroni correction to $\\mathrm{P1}$ $p$-values; this will restore a uniform null distribution.**\n- **Analysis**: This diagnosis confuses multiple testing correction with $p$-value calibration. A Bonferroni correction is an adjustment made to significance thresholds *after* valid $p$-values have been generated to control the family-wise error rate. It does not alter the underlying distribution of the raw $p$-values. The distribution must be uniform *before* correction. The action is ineffective; it would not fix the distributional problem.\n- **Verdict**: Incorrect.\n\n**E. Diagnosis: Misalignment to the reference genome is the primary cause of small $p$-values under permutation. Action: Remap all reads to a different reference assembly to correct the $p$-value distribution shape.**\n- **Analysis**: While alignment is a critical upstream step, it is not the most plausible cause here. Both pipelines, $\\mathrm{P1}$ and $\\mathrm{P2}$, would use the same count matrix derived from the same alignment. The fact that $\\mathrm{P2}$ is well-calibrated using these exact same counts strongly indicates the problem is with the statistical model of $\\mathrm{P1}$, not the counts themselves. Remapping is a drastic, computationally expensive step that fails to address the demonstrated statistical flaw.\n- **Verdict**: Incorrect.", "answer": "$$\\boxed{B}$$", "id": "5088384"}]}