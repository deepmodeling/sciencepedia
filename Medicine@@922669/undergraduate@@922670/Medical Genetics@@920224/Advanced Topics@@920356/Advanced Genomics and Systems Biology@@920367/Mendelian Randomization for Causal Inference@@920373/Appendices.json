{"hands_on_practices": [{"introduction": "Before using a genetic variant as an instrumental variable, we must ensure it has a sufficiently strong association with the exposure. A 'weak' instrument can lead to biased results and unreliable conclusions, undermining our causal inference. This practice guides you through the calculation of the F-statistic, a standard metric used to quantify instrument strength and assess whether it meets the conventional threshold for reliability. [@problem_id:4358043]", "problem": "In Mendelian Randomization (MR) for causal inference within precision medicine and genomic diagnostics, instrument strength in the first-stage regression is routinely assessed to guard against weak instrument bias. Consider a linear two-stage least squares framework where a single genetic variant (for example, a Single Nucleotide Polymorphism (SNP) acting as a cis-expression quantitative trait locus) is used as an Instrumental Variable (IV) to predict a continuous biomarker exposure. The first-stage Ordinary Least Squares (OLS) regression of the exposure on the instrument includes only an intercept and the instrument, with no additional covariates. Let the sample size be $n = 10{,}000$, the number of instruments be $k = 1$, and the first-stage coefficient of determination be $R^{2} = 0.01$.\n\nStarting from the standard OLS definitions of the explained sum of squares and residual sum of squares, and the definition of the F-statistic for testing the joint null hypothesis that the instrument coefficient(s) are zero in the first-stage model, derive the first-stage conditional F-statistic in terms of $R^{2}$, $n$, and $k$. Then evaluate it for the given numerical values. Finally, based on the computed value, state whether the instrument exceeds the common rule-of-thumb threshold of $10$ for avoiding weak instrument concerns. Provide the F-statistic as a single exact value (do not round), and note that the quantity is dimensionless.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the established statistical theory of Mendelian Randomization and two-stage least squares regression. It is well-posed, providing all necessary information ($n$, $k$, $R^2$) to derive and calculate the first-stage F-statistic. The language is objective and precise, and the posed task is a standard exercise in econometrics and statistical genetics, free of any factual unsoundness, contradiction, or ambiguity.\n\nWe begin by formally defining the components of the first-stage Ordinary Least Squares (OLS) regression. The model regresses the exposure, which we denote as $Y$, on a set of $k$ instrumental variables, denoted $X_1, \\dots, X_k$, and an intercept. The model is:\n$$\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\dots + \\beta_k X_{ik} + u_i\n$$\nwhere $i$ indexes the observations from $1$ to $n$, $\\beta_0$ is the intercept, $\\beta_1, \\dots, \\beta_k$ are the coefficients for the instruments, and $u_i$ is the error term.\n\nThe derivation proceeds from the fundamental definitions of sums of squares in OLS regression.\nLet $\\bar{Y}$ be the sample mean of the exposure, and let $\\hat{Y}_i$ be the fitted value of $Y_i$ from the OLS regression.\n\n1.  The Total Sum of Squares ($SST$) measures the total variance in the dependent variable $Y$:\n    $$\n    SST = \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2\n    $$\n2.  The Explained Sum of Squares ($ESS$), also known as the regression sum of squares, measures the variation in $Y$ that is explained by the regression model:\n    $$\n    ESS = \\sum_{i=1}^{n} (\\hat{Y}_i - \\bar{Y})^2\n    $$\n3.  The Residual Sum of Squares ($RSS$), also known as the sum of squared errors, measures the variation in $Y$ not explained by the model:\n    $$\n    RSS = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^{n} \\hat{u}_i^2\n    $$\nThese quantities are related by the identity $SST = ESS + RSS$.\n\nThe coefficient of determination, $R^2$, is defined as the fraction of the total sum of squares that is explained by the model:\n$$\nR^2 = \\frac{ESS}{SST}\n$$\nFrom this definition, we can express $ESS$ and $RSS$ in terms of $R^2$ and $SST$:\n$$\nESS = R^2 \\cdot SST\n$$\n$$\nRSS = SST - ESS = SST - R^2 \\cdot SST = (1 - R^2) \\cdot SST\n$$\n\nThe F-statistic for the joint null hypothesis that all instrument coefficients are zero ($H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_k = 0$) is defined as the ratio of the Mean Square Regression ($MSR$) to the Mean Square Error ($MSE$).\n\nThe $MSR$ is the explained sum of squares divided by its degrees of freedom, which is the number of predictors being tested, $k$:\n$$\nMSR = \\frac{ESS}{k}\n$$\nThe $MSE$ is the residual sum of squares divided by its degrees of freedom. The degrees of freedom for the residuals are the sample size, $n$, minus the total number of parameters estimated in the model. Since the model includes an intercept and $k$ instrument coefficients, the total number of parameters is $k+1$. Thus, the denominator degrees of freedom is $n - (k+1)$.\n$$\nMSE = \\frac{RSS}{n - k - 1}\n$$\nThe F-statistic is then:\n$$\nF = \\frac{MSR}{MSE} = \\frac{ESS/k}{RSS/(n - k - 1)}\n$$\n\nNow, we substitute the expressions for $ESS$ and $RSS$ in terms of $R^2$ and $SST$ into the F-statistic formula:\n$$\nF = \\frac{(R^2 \\cdot SST) / k}{((1 - R^2) \\cdot SST) / (n - k - 1)}\n$$\nThe $SST$ term appears in both the numerator and the denominator, so it cancels out. This cancellation also confirms that the F-statistic is a dimensionless quantity, as it is ultimately a ratio of two portions of variance. This leaves us with the desired expression for the F-statistic in terms of $R^2$, $n$, and $k$:\n$$\nF = \\frac{R^2 / k}{(1 - R^2) / (n - k - 1)} = \\frac{R^2}{1 - R^2} \\cdot \\frac{n - k - 1}{k}\n$$\nThis completes the derivation.\n\nNext, we evaluate this expression using the numerical values provided in the problem statement:\n- Sample size, $n = 10{,}000$\n- Number of instruments, $k = 1$\n- First-stage coefficient of determination, $R^2 = 0.01$\n\nSubstituting these values into the derived formula:\n$$\nF = \\frac{0.01}{1 - 0.01} \\cdot \\frac{10{,}000 - 1 - 1}{1}\n$$\n$$\nF = \\frac{0.01}{0.99} \\cdot \\frac{9998}{1}\n$$\n$$\nF = \\frac{1}{99} \\cdot 9998\n$$\n$$\nF = \\frac{9998}{99}\n$$\nThis is the exact value of the F-statistic. To compare this value to the rule-of-thumb threshold, we can approximate it.\n$F = \\frac{9998}{99} \\approx 100.9898...$\n\nFinally, we must state whether the instrument exceeds the common rule-of-thumb threshold of $10$ for avoiding weak instrument concerns. The calculated F-statistic is $\\frac{9998}{99}$, which is approximately $100.99$. Since $100.99 > 10$, the instrument strength substantially exceeds the common threshold. Therefore, based on this metric, the genetic variant is not considered a weak instrument.", "answer": "$$\n\\boxed{\\frac{9998}{99}}\n$$", "id": "4358043"}, {"introduction": "Once a genetic instrument is deemed strong enough, the next step is to estimate the causal effect. In the fundamental setting of a single instrument, this is accomplished using the Wald ratio, which elegantly relates the instrument-outcome association to the instrument-exposure association. This exercise demonstrates how to compute this essential causal estimate and, just as importantly, how to quantify its statistical uncertainty by calculating a standard error. [@problem_id:4357996]", "problem": "A precision medicine study in genomic diagnostics aims to estimate the causal effect of a standardized plasma biomarker $X$ on a standardized disease trait $Y$ using Mendelian Randomization (MR). A single genetic variant $G$ (a Single-Nucleotide Polymorphism (SNP)) is used as an instrument. Assume the following core instrumental variable assumptions hold: relevance ($G$ associates with $X$), independence ($G$ is independent of confounders of $X$ and $Y$), and exclusion restriction ($G$ affects $Y$ only through $X$). The associations are estimated in a two-sample setting with independent sampling errors and large-sample normality. Let $\\hat{\\beta}_{GX}$ denote the estimated association of $G$ with $X$ and $\\hat{\\beta}_{GY}$ the estimated association of $G$ with $Y$. You are given $\\hat{\\beta}_{GX} = 0.08$ (standard deviation units per allele), $\\hat{\\beta}_{GY} = -0.016$ (standard deviation units per allele), $\\mathrm{SE}(\\hat{\\beta}_{GX}) = 0.01$ and $\\mathrm{SE}(\\hat{\\beta}_{GY}) = 0.005$, where SE denotes the standard error. \n\nUnder the linear structural model consistent with the assumptions above, compute the Wald estimator of the causal effect of $X$ on $Y$ and its standard error using the first-order multivariate delta method, assuming independence of $\\hat{\\beta}_{GX}$ and $\\hat{\\beta}_{GY}$. Round both the estimator and its standard error to four significant figures. Express your final answer as a two-entry row vector $\\begin{pmatrix}\\text{estimate} & \\text{standard error}\\end{pmatrix}$ with no units.", "solution": "The goal is to estimate the causal effect of $X$ on $Y$ using Mendelian Randomization (MR) with a single instrument $G$. Under the instrumental variable assumptions—relevance, independence, and exclusion restriction—and a linear structural model, the causal effect at the population level is identified by the ratio of the instrument–outcome association to the instrument–exposure association. The corresponding sample estimator is the Wald estimator, which is the ratio of the estimated associations.\n\nLet $\\hat{\\beta}_{GX}$ denote the estimated association of $G$ with $X$ and $\\hat{\\beta}_{GY}$ the estimated association of $G$ with $Y$. The Wald estimator is\n$$\n\\hat{\\beta}_{YX}^{\\text{Wald}} = \\frac{\\hat{\\beta}_{GY}}{\\hat{\\beta}_{GX}}.\n$$\nWith $\\hat{\\beta}_{GX} = 0.08$ and $\\hat{\\beta}_{GY} = -0.016$, we compute\n$$\n\\hat{\\beta}_{YX}^{\\text{Wald}} = \\frac{-0.016}{0.08} = -0.2.\n$$\n\nNext, we compute the standard error of the ratio estimator using the first-order multivariate delta method. Define the function $g(a,b) = \\frac{a}{b}$ with $(a,b)$ corresponding to $(\\hat{\\beta}_{GY}, \\hat{\\beta}_{GX})$. The gradient of $g$ at $(a,b)$ is\n$$\n\\nabla g(a,b) = \\left( \\frac{\\partial g}{\\partial a}, \\frac{\\partial g}{\\partial b} \\right) = \\left( \\frac{1}{b}, -\\frac{a}{b^{2}} \\right).\n$$\nAssuming the two-sample setting with independent sampling errors so that $\\operatorname{Cov}(\\hat{\\beta}_{GY}, \\hat{\\beta}_{GX}) = 0$, and denoting $\\operatorname{Var}(\\hat{\\beta}_{GY}) = \\mathrm{SE}(\\hat{\\beta}_{GY})^{2}$ and $\\operatorname{Var}(\\hat{\\beta}_{GX}) = \\mathrm{SE}(\\hat{\\beta}_{GX})^{2}$, the delta-method approximation to the variance of $g(\\hat{\\beta}_{GY}, \\hat{\\beta}_{GX})$ is\n$$\n\\operatorname{Var}\\!\\left(\\frac{\\hat{\\beta}_{GY}}{\\hat{\\beta}_{GX}}\\right) \\approx \\left(\\frac{1}{\\hat{\\beta}_{GX}}\\right)^{2} \\operatorname{Var}(\\hat{\\beta}_{GY}) + \\left(\\frac{\\hat{\\beta}_{GY}}{\\hat{\\beta}_{GX}^{2}}\\right)^{2} \\operatorname{Var}(\\hat{\\beta}_{GX}).\n$$\nSubstituting the given values $\\hat{\\beta}_{GX} = 0.08$, $\\hat{\\beta}_{GY} = -0.016$, $\\mathrm{SE}(\\hat{\\beta}_{GX}) = 0.01$, and $\\mathrm{SE}(\\hat{\\beta}_{GY}) = 0.005$:\n$$\n\\operatorname{Var}\\!\\left(\\hat{\\beta}_{YX}^{\\text{Wald}}\\right) \\approx \\frac{(0.005)^{2}}{(0.08)^{2}} + \\frac{(-0.016)^{2} (0.01)^{2}}{(0.08)^{4}}.\n$$\nCompute each term symbolically, then numerically:\n- First term:\n$$\n\\frac{(0.005)^{2}}{(0.08)^{2}} = \\frac{0.000025}{0.0064} = 0.00390625.\n$$\n- Second term:\n$$\n\\frac{(-0.016)^{2} (0.01)^{2}}{(0.08)^{4}} = \\frac{0.000256 \\times 0.0001}{0.00004096} = \\frac{2.56 \\times 10^{-8}}{4.096 \\times 10^{-5}} = 6.25 \\times 10^{-4} = 0.000625.\n$$\nSumming the two terms,\n$$\n\\operatorname{Var}\\!\\left(\\hat{\\beta}_{YX}^{\\text{Wald}}\\right) \\approx 0.00390625 + 0.000625 = 0.00453125.\n$$\nTherefore, the standard error is\n$$\n\\mathrm{SE}\\!\\left(\\hat{\\beta}_{YX}^{\\text{Wald}}\\right) \\approx \\sqrt{0.00453125} \\approx 0.06731,\n$$\nrounded to four significant figures.\n\nThe Wald estimate itself is exactly $-0.2$, which rounded to four significant figures is $-0.2000$. The requested final answer is the two-entry row vector containing the estimate and its standard error, both rounded to four significant figures and expressed with no units:\n$$\n\\begin{pmatrix}\n-0.2000 & 0.06731\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}-0.2000 & 0.06731\\end{pmatrix}}$$", "id": "4357996"}, {"introduction": "While single-instrument MR is instructive, most modern studies leverage summary data from multiple genetic variants to increase statistical power and robustness. The Inverse-Variance Weighted (IVW) method is a cornerstone for meta-analyzing the causal effects estimated by each instrument. In this practice, you will learn how to synthesize data from several independent variants into a single, more precise causal effect estimate and construct its corresponding confidence interval. [@problem_id:4358027]", "problem": "A research team in precision medicine seeks to estimate the causal effect of a continuous biomarker exposure on a binary disease outcome using Mendelian Randomization (MR). The team has identified four independent Single-Nucleotide Polymorphism (SNP) instruments strongly associated with the exposure and has collected two-sample summary association estimates. For each SNP $j \\in \\{1,2,3,4\\}$, they report the genetic associations with the exposure $\\hat{\\beta}_{GX,j}$ and with the outcome $\\hat{\\beta}_{GY,j}$, along with the corresponding standard errors $\\mathrm{SE}(\\hat{\\beta}_{GX,j})$ and $\\mathrm{SE}(\\hat{\\beta}_{GY,j})$. The MR assumptions of relevance, independence, and exclusion restriction are judged to be plausible, and the samples used for exposure and outcome associations are independent.\n\nThe available summary data are:\n- SNP $1$: $\\hat{\\beta}_{GX,1} = 0.08$, $\\mathrm{SE}(\\hat{\\beta}_{GX,1}) = 0.004$; $\\hat{\\beta}_{GY,1} = 0.032$, $\\mathrm{SE}(\\hat{\\beta}_{GY,1}) = 0.015$.\n- SNP $2$: $\\hat{\\beta}_{GX,2} = 0.12$, $\\mathrm{SE}(\\hat{\\beta}_{GX,2}) = 0.005$; $\\hat{\\beta}_{GY,2} = 0.050$, $\\mathrm{SE}(\\hat{\\beta}_{GY,2}) = 0.020$.\n- SNP $3$: $\\hat{\\beta}_{GX,3} = 0.05$, $\\mathrm{SE}(\\hat{\\beta}_{GX,3}) = 0.003$; $\\hat{\\beta}_{GY,3} = 0.020$, $\\mathrm{SE}(\\hat{\\beta}_{GY,3}) = 0.012$.\n- SNP $4$: $\\hat{\\beta}_{GX,4} = 0.10$, $\\mathrm{SE}(\\hat{\\beta}_{GX,4}) = 0.004$; $\\hat{\\beta}_{GY,4} = 0.036$, $\\mathrm{SE}(\\hat{\\beta}_{GY,4}) = 0.018$.\n\nAssume a linear causal model linking the SNP-outcome association to the product of the SNP-exposure association and a single causal effect parameter, plus random error with variance known up to the reported outcome standard errors, and assume the No Measurement Error (NOME) approximation holds for the exposure associations in the sense that uncertainty in $\\hat{\\beta}_{GX,j}$ is negligible for weighting. Under these assumptions, use the Inverse-Variance Weighted (IVW) approach (weighted least squares through the origin with weights proportional to the inverse of the outcome association variances) to compute:\n- the IVW point estimate of the causal effect, and\n- the corresponding $95\\%$ confidence interval using a normal approximation.\n\nRound each of the three numbers (the point estimate, the lower confidence bound, and the upper confidence bound) to four significant figures. Express your final numerical answers as decimals with no units.", "solution": "The user-provided problem has been validated and is determined to be a valid, well-posed scientific question.\n\nThe problem requires the calculation of the Inverse-Variance Weighted (IVW) point estimate of a causal effect and its corresponding $95\\%$ confidence interval, based on summary data from a two-sample Mendelian Randomization (MR) study.\n\nThe core principle of the IVW method is to perform a weighted linear regression of the SNP-outcome associations ($\\hat{\\beta}_{GY,j}$) on the SNP-exposure associations ($\\hat{\\beta}_{GX,j}$), constrained to pass through the origin. The model for each SNP $j$ is:\n$$ \\hat{\\beta}_{GY,j} = \\beta_{IVW} \\hat{\\beta}_{GX,j} + \\epsilon_j $$\nwhere $\\beta_{IVW}$ is the causal effect estimate we seek to find.\n\nThe problem specifies that the weights are proportional to the inverse of the outcome association variances. This, combined with the No Measurement Error (NOME) assumption (i.e., treating $\\hat{\\beta}_{GX,j}$ as fixed and known), leads to the standard IVW estimator. The weight for the $j$-th instrument is $w_j = 1 / \\mathrm{SE}(\\hat{\\beta}_{GY,j})^2$.\n\nThe formula for the IVW point estimate, $\\hat{\\beta}_{IVW}$, is the solution to the weighted least squares problem:\n$$ \\hat{\\beta}_{IVW} = \\frac{\\sum_{j=1}^{J} w_j \\hat{\\beta}_{GX,j} \\hat{\\beta}_{GY,j}}{\\sum_{j=1}^{J} w_j \\hat{\\beta}_{GX,j}^2} = \\frac{\\sum_{j=1}^{J} \\frac{\\hat{\\beta}_{GX,j} \\hat{\\beta}_{GY,j}}{\\mathrm{SE}(\\hat{\\beta}_{GY,j})^2}}{\\sum_{j=1}^{J} \\frac{\\hat{\\beta}_{GX,j}^2}{\\mathrm{SE}(\\hat{\\beta}_{GY,j})^2}} $$\nwhere $J=4$ is the number of SNPs.\n\nThe standard error of the IVW estimate, assuming the instruments are independent and the NOME assumption holds, is given by:\n$$ \\mathrm{SE}(\\hat{\\beta}_{IVW}) = \\sqrt{\\frac{1}{\\sum_{j=1}^{J} \\frac{\\hat{\\beta}_{GX,j}^2}{\\mathrm{SE}(\\hat{\\beta}_{GY,j})^2}}} $$\n\nWe will calculate the numerator and denominator terms for the $\\hat{\\beta}_{IVW}$ formula for each of the $J=4$ SNPs. Let $N_j = \\frac{\\hat{\\beta}_{GX,j} \\hat{\\beta}_{GY,j}}{\\mathrm{SE}(\\hat{\\beta}_{GY,j})^2}$ and $D_j = \\frac{\\hat{\\beta}_{GX,j}^2}{\\mathrm{SE}(\\hat{\\beta}_{GY,j})^2}$.\n\nFor SNP $1$:\n$\\hat{\\beta}_{GX,1} = 0.08$, $\\hat{\\beta}_{GY,1} = 0.032$, $\\mathrm{SE}(\\hat{\\beta}_{GY,1}) = 0.015$.\n$N_1 = \\frac{0.08 \\times 0.032}{0.015^2} = \\frac{0.00256}{0.000225} = \\frac{512}{45}$\n$D_1 = \\frac{0.08^2}{0.015^2} = \\frac{0.0064}{0.000225} = \\frac{256}{9}$\n\nFor SNP $2$:\n$\\hat{\\beta}_{GX,2} = 0.12$, $\\hat{\\beta}_{GY,2} = 0.050$, $\\mathrm{SE}(\\hat{\\beta}_{GY,2}) = 0.020$.\n$N_2 = \\frac{0.12 \\times 0.050}{0.020^2} = \\frac{0.006}{0.0004} = 15$\n$D_2 = \\frac{0.12^2}{0.020^2} = \\frac{0.0144}{0.0004} = 36$\n\nFor SNP $3$:\n$\\hat{\\beta}_{GX,3} = 0.05$, $\\hat{\\beta}_{GY,3} = 0.020$, $\\mathrm{SE}(\\hat{\\beta}_{GY,3}) = 0.012$.\n$N_3 = \\frac{0.05 \\times 0.020}{0.012^2} = \\frac{0.001}{0.000144} = \\frac{125}{18}$\n$D_3 = \\frac{0.05^2}{0.012^2} = \\frac{0.0025}{0.000144} = \\frac{625}{36}$\n\nFor SNP $4$:\n$\\hat{\\beta}_{GX,4} = 0.10$, $\\hat{\\beta}_{GY,4} = 0.036$, $\\mathrm{SE}(\\hat{\\beta}_{GY,4}) = 0.018$.\n$N_4 = \\frac{0.10 \\times 0.036}{0.018^2} = \\frac{0.0036}{0.000324} = \\frac{100}{9}$\n$D_4 = \\frac{0.10^2}{0.018^2} = \\frac{0.01}{0.000324} = \\frac{2500}{81}$\n\nNow, we sum these terms:\n$$ \\sum_{j=1}^{4} N_j = \\frac{512}{45} + 15 + \\frac{125}{18} + \\frac{100}{9} = \\frac{1024 + 1350 + 625 + 1000}{90} = \\frac{3999}{90} $$\n$$ \\sum_{j=1}^{4} D_j = \\frac{256}{9} + 36 + \\frac{625}{36} + \\frac{2500}{81} = \\frac{9216 + 11664 + 5625 + 10000}{324} = \\frac{36505}{324} $$\n\nThe IVW point estimate is:\n$$ \\hat{\\beta}_{IVW} = \\frac{\\sum N_j}{\\sum D_j} = \\frac{3999/90}{36505/324} = \\frac{3999}{90} \\times \\frac{324}{36505} \\approx 0.3943673 $$\nRounding to four significant figures, the point estimate is $0.3944$.\n\nThe standard error of the estimate is:\n$$ \\mathrm{SE}(\\hat{\\beta}_{IVW}) = \\frac{1}{\\sqrt{\\sum D_j}} = \\frac{1}{\\sqrt{36505/324}} = \\sqrt{\\frac{324}{36505}} \\approx 0.0942103 $$\n\nTo compute the $95\\%$ confidence interval, we use the normal approximation:\n$$ CI_{95\\%} = \\hat{\\beta}_{IVW} \\pm z_{0.975} \\times \\mathrm{SE}(\\hat{\\beta}_{IVW}) $$\nThe critical value from the standard normal distribution for a $95\\%$ confidence level is $z_{0.975} \\approx 1.96$.\n\nThe margin of error is:\n$$ ME = 1.96 \\times 0.0942103 \\approx 0.1846522 $$\n\nThe lower and upper bounds of the confidence interval are:\nLower Bound: $LB = 0.3943673 - 0.1846522 = 0.2097151$\nUpper Bound: $UB = 0.3943673 + 0.1846522 = 0.5790195$\n\nRounding these bounds to four significant figures:\nLower Bound $\\approx 0.2097$\nUpper Bound $\\approx 0.5790$ (the trailing zero is significant)\n\nThe final results, rounded to four significant figures, are:\n-   IVW point estimate: $0.3944$\n-   $95\\%$ CI lower bound: $0.2097$\n-   $95\\%$ CI upper bound: $0.5790$", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.3944 & 0.2097 & 0.5790 \\end{pmatrix}}\n$$", "id": "4358027"}]}