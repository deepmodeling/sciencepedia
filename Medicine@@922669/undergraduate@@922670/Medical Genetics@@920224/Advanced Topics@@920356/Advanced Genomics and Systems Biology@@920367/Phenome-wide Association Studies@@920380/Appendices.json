{"hands_on_practices": [{"introduction": "The reliability of a Phenome-Wide Association Study hinges on the quality of its phenotype data. This practice problem simulates a common and critical task: designing a validation study to determine the positive predictive value (PPV) of an EHR-derived phenotype, or \"PheCode\". By calculating the necessary sample size for a chart review, you will engage with the statistical principles that ensure our computational phenotypes correspond reliably to true clinical diagnoses. [@problem_id:5071597]", "problem": "A research team conducting a Phenome-Wide Association Study (PheWAS) plans to validate the positive predictive value (PPV) of a specific PheCode derived from the Electronic Health Record (EHR). The PheCode flags individuals as potential cases for a phenotype of interest. In a validation study using chart review, each flagged individual’s chart is reviewed to confirm whether the phenotype truly exists. Let the true PPV be the probability $p=\\Pr(\\text{true phenotype} \\mid \\text{PheCode flagged})$. Assume that charts are sampled by simple random sampling from the set of flagged individuals, chart reviews are independent, and each review yields a binary outcome $\\{0,1\\}$ indicating false positive or true case, respectively, modeled as independent and identically distributed Bernoulli trials with success probability $p$.\n\nYou will design the validation component and compute the minimum required number of chart reviews to estimate $p$ with a two-sided $95\\%$ confidence interval having half-width $\\pm 0.05$. In your design, use the asymptotic normal approximation justified by the Central Limit Theorem for the sample proportion, and take the anticipated PPV to be $0.8$ as the planning value. Treat the confidence interval as symmetric about the estimator and base the precision on the planning value rather than the unknown $p$.\n\nUnder these assumptions, derive the sample size requirement from first principles and then calculate the minimum integer number of chart reviews needed so that the confidence interval half-width is at most $0.05$. Report the final numerical answer as the smallest integer that satisfies the criterion. No units are required, and no rounding to significant figures is necessary beyond selecting the minimum integer.", "solution": "The problem requires the calculation of the minimum sample size for a validation study. The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **Objective**: Validate the positive predictive value (PPV) of a PheCode.\n- **Definition of PPV**: $p=\\Pr(\\text{true phenotype} \\mid \\text{PheCode flagged})$.\n- **Sampling Model**: Simple random sampling of charts from flagged individuals.\n- **Statistical Model**: Chart reviews are independent and identically distributed Bernoulli trials.\n- **Outcome of a trial**: A binary outcome $\\{0,1\\}$ indicating false positive or true case, respectively.\n- **Parameter of interest**: The success probability $p$ of the Bernoulli trials.\n- **Estimation Goal**: Compute the minimum number of chart reviews, $n$, required to estimate $p$.\n- **Confidence Interval**: A two-sided $95\\%$ confidence interval for $p$.\n- **Required Precision**: The confidence interval half-width must be at most $0.05$.\n- **Methodology**: Use the asymptotic normal approximation for the sample proportion.\n- **Planning Value**: The anticipated PPV is to be taken as $p_{plan} = 0.8$.\n- **Calculation Constraint**: The precision is to be based on the planning value $p_{plan}$, not the unknown true $p$.\n- **Final Answer Requirement**: The smallest integer number of chart reviews that satisfies the criterion.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientific Grounding**: The problem is well-grounded in standard biostatistical theory. It describes a common scenario in medical informatics and clinical research: sample size calculation for estimating a proportion (specifically, a positive predictive value) with a specified precision. The use of the normal approximation to the binomial distribution is a standard technique justified by the Central Limit Theorem.\n- **Well-Posedness**: The problem is well-posed. It clearly defines the parameter to be estimated ($p$), the desired confidence level ($95\\%$), the maximum acceptable half-width ($0.05$), the statistical model (Bernoulli trials), the approximation method (normal approximation), and a planning value for the parameter ($0.8$). These elements are sufficient to determine a unique minimum integer sample size.\n- **Objectivity**: The problem is stated in objective, quantitative terms, free from ambiguity or subjective content.\n\nThe problem exhibits none of the invalidity flaws. It is scientifically sound, formalizable, complete, realistic, and well-posed.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be derived from first principles as requested.\n\n### Solution Derivation\nLet $n$ be the number of chart reviews to be conducted. Each review is modeled as a Bernoulli trial. Let $X_i$ be the random variable representing the outcome of the $i$-th review, for $i = 1, 2, \\ldots, n$. We have $X_i = 1$ if the chart represents a true case (success) and $X_i = 0$ if it is a false positive (failure). The trials are independent and identically distributed with $\\Pr(X_i = 1) = p$, where $p$ is the true PPV.\n\nThe estimator for $p$ is the sample proportion, $\\hat{p}$, defined as:\n$$ \\hat{p} = \\frac{1}{n} \\sum_{i=1}^{n} X_i $$\n\nAccording to the Central Limit Theorem, for a sufficiently large sample size $n$, the sampling distribution of $\\hat{p}$ is approximately normal:\n$$ \\hat{p} \\approx \\mathcal{N}\\left(\\mu_{\\hat{p}}, \\sigma_{\\hat{p}}^2\\right) $$\nwhere the mean is $\\mu_{\\hat{p}} = E[\\hat{p}] = p$ and the variance is $\\sigma_{\\hat{p}}^2 = \\text{Var}(\\hat{p}) = \\frac{p(1-p)}{n}$.\n\nA two-sided $100(1-\\alpha)\\%$ confidence interval for $p$ based on the normal approximation is given by:\n$$ \\hat{p} \\pm z_{1-\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} $$\nwhere $z_{1-\\alpha/2}$ is the quantile of the standard normal distribution that leaves an area of $1-\\alpha/2$ to its left.\n\nThe half-width of this confidence interval is $W = z_{1-\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$. For the purpose of planning (i.e., calculating the required sample size $n$ before the data are collected), the sample proportion $\\hat{p}$ is unknown. The problem specifies that we should use the planning value $p_{plan} = 0.8$ in its place. Thus, the formula for the required half-width becomes:\n$$ W = z_{1-\\alpha/2} \\sqrt{\\frac{p_{plan}(1-p_{plan})}{n}} $$\n\nWe are given the following requirements:\n- Confidence level is $95\\%$, so $1-\\alpha = 0.95$, which implies $\\alpha = 0.05$.\n- The critical value is $z_{1-\\alpha/2} = z_{1-0.025} = z_{0.975}$. The standard value for $z_{0.975}$ is approximately $1.96$.\n- The maximum acceptable half-width is $W = 0.05$.\n- The planning value for the proportion is $p_{plan} = 0.8$.\n\nWe need to find the minimum integer $n$ that satisfies the inequality $W \\le 0.05$:\n$$ z_{1-\\alpha/2} \\sqrt{\\frac{p_{plan}(1-p_{plan})}{n}} \\le 0.05 $$\n\nSubstituting the known values:\n$$ 1.96 \\sqrt{\\frac{0.8(1-0.8)}{n}} \\le 0.05 $$\n$$ 1.96 \\sqrt{\\frac{0.8(0.2)}{n}} \\le 0.05 $$\n$$ 1.96 \\sqrt{\\frac{0.16}{n}} \\le 0.05 $$\n\nTo solve for $n$, we rearrange the inequality:\n$$ \\sqrt{\\frac{0.16}{n}} \\le \\frac{0.05}{1.96} $$\nSquaring both sides of the inequality gives:\n$$ \\frac{0.16}{n} \\le \\left(\\frac{0.05}{1.96}\\right)^2 $$\nNow, we solve for $n$:\n$$ n \\ge 0.16 \\left(\\frac{1.96}{0.05}\\right)^2 $$\nFirst, calculate the term in the parentheses:\n$$ \\frac{1.96}{0.05} = 39.2 $$\nThen, square this value:\n$$ (39.2)^2 = 1536.64 $$\nFinally, multiply by $0.16$:\n$$ n \\ge 0.16 \\times 1536.64 $$\n$$ n \\ge 245.8624 $$\nSince the number of chart reviews, $n$, must be an integer, we must take the smallest integer that is greater than or equal to $245.8624$. This is obtained by taking the ceiling of the calculated value:\n$$ n_{min} = \\lceil 245.8624 \\rceil = 246 $$\nTherefore, a minimum of $246$ chart reviews are required to achieve the desired precision.", "answer": "$$\n\\boxed{246}\n$$", "id": "5071597"}, {"introduction": "When performing association tests across the phenome, we often encounter rare genetic variants that present unique statistical challenges. This exercise uses fundamental principles of population genetics to illustrate how the scarcity of certain genotypes can lead to model instability in logistic regression, a common issue known as data separation. Understanding this problem is key to appreciating why specialized methods, like Firth correction, are essential for robust PheWAS results. [@problem_id:5071604]", "problem": "A Phenome-Wide Association Study (PheWAS) systematically tests a large set of clinical phenotypes for association with genetic variants to characterize the phenotypic spectrum of genetic effects. Consider a biallelic locus in a diploid population with minor allele frequency $p=0.01$ in a sample of $N=200{,}000$ unrelated individuals from a single ancestry group. Assume random mating and the conditions of Hardy–Weinberg equilibrium (HWE), and that individuals are sampled independently from the population. Using only core population-genetic definitions (allele frequencies and genotype formation by random union of gametes) and the fact that an expected count equals sample size multiplied by the corresponding population frequency, derive the expected number of minor-allele homozygotes in the full sample, and compute its value for the given $p$ and $N$. Then, using a case prevalence denoted $\\pi$ for an arbitrary binary phenotype tested in PheWAS, explain how the scarcity of minor-allele homozygotes within phenotype strata can lead to quasi- or complete separation in logistic regression models and why bias-reducing penalized likelihood (Firth correction) is often warranted across many phenotypes. Report as your final numeric answer the expected number of minor-allele homozygotes in the entire sample. No rounding is required, and express your final answer as an exact integer.", "solution": "The problem as stated is valid. It is scientifically grounded in established principles of population genetics and statistical genetics, is well-posed with all necessary information provided, and is expressed in objective, formal language. We may proceed with the solution.\n\nThe problem asks for three components: first, a derivation of the expected number of minor-allele homozygotes; second, a calculation of this value for the given parameters; and third, an explanation of a related statistical issue in Phenome-Wide Association Studies (PheWAS).\n\nFirst, we address the derivation. The problem specifies a biallelic locus in a diploid population. Let the two alleles be denoted $A_1$ (major allele) and $A_2$ (minor allele). The frequency of the minor allele, $A_2$, is given as $p$. The frequency of the major allele, $A_1$, can be denoted as $q$. Since there are only two alleles at this locus, the sum of their frequencies must be $1$.\n$$p + q = 1$$\nThe problem states to assume random mating and the conditions of Hardy–Weinberg equilibrium (HWE). Under HWE, the frequencies of the three possible genotypes ($A_1A_1$, $A_1A_2$, and $A_2A_2$) in the population are determined by the allele frequencies according to the binomial expansion $(p+q)^2 = p^2 + 2pq + q^2 = 1$. The frequency of each genotype is derived from the random union of gametes from the population's gene pool.\nThe frequency of major-allele homozygotes ($A_1A_1$) is $f(A_1A_1) = q^2 = (1-p)^2$.\nThe frequency of heterozygotes ($A_1A_2$) is $f(A_1A_2) = 2pq = 2p(1-p)$.\nThe frequency of minor-allele homozygotes ($A_2A_2$) is $f(A_2A_2) = p^2$.\n\nThe problem requires deriving the expected number of minor-allele homozygotes in a sample of size $N$. The expected count of any genotype in a sample is the product of the sample size and the population frequency of that genotype. Let $E[n_{A_2A_2}]$ denote the expected number of minor-allele homozygotes.\n$$E[n_{A_2A_2}] = N \\times f(A_2A_2)$$\nSubstituting the HWE frequency for the minor-allele homozygote, we obtain the derived formula:\n$$E[n_{A_2A_2}] = N p^2$$\n\nSecond, we compute the numerical value using the provided data: the sample size is $N=200,000$ and the minor allele frequency is $p=0.01$.\n$$E[n_{A_2A_2}] = 200,000 \\times (0.01)^2$$\nWe evaluate the terms:\n$$(0.01)^2 = (1 \\times 10^{-2})^2 = 1 \\times 10^{-4}$$\n$$E[n_{A_2A_2}] = (2 \\times 10^5) \\times (1 \\times 10^{-4}) = 2 \\times 10^{5-4} = 2 \\times 10^1 = 20$$\nThus, the expected number of minor-allele homozygotes in the entire sample is $20$.\n\nThird, we explain the statistical implications for PheWAS. In a PheWAS, each of many phenotypes is tested for association with the genetic variant. For a binary phenotype (e.g., presence/absence of a disease), this is typically done using logistic regression. A common model is:\n$$\\text{logit}(P(\\text{phenotype})) = \\beta_0 + \\beta_1 \\cdot G + \\text{covariates}$$\nwhere $G$ is a variable representing the genotype, often coded as the number of minor alleles ($0$ for $A_1A_1$, $1$ for $A_1A_2$, $2$ for $A_2A_2$).\n\nThe total sample of size $N$ is partitioned into cases (individuals with the phenotype) and controls (individuals without it). Let the phenotype prevalence be $\\pi$. The number of cases is approximately $N\\pi$ and the number of controls is $N(1-\\pi)$.\nOur calculated expected number of minor-allele homozygotes ($A_2A_2$) is only $20$. These $20$ individuals are distributed between the case and control groups. The expected number of $A_2A_2$ individuals among cases is $E[n_{A_2A_2, \\text{case}}] = (Np^2)\\pi = 20\\pi$, and among controls is $E[n_{A_2A_2, \\text{control}}] = (Np^2)(1-\\pi) = 20(1-\\pi)$.\n\nFor many phenotypes tested in a PheWAS, the prevalence $\\pi$ is low. For example, if a disease has a prevalence of $\\pi=0.05$ ($5\\%$), the expected number of $A_2A_2$ cases is $20 \\times 0.05 = 1$. With such a low expected count, it is highly probable that the observed count in a random sample will be zero.\n\nThis leads to the problem of **quasi- or complete separation** in logistic regression. Complete separation occurs when a predictor variable (here, the genotype $G=2$) perfectly predicts the outcome. For example, if by chance all observed individuals with the $A_2A_2$ genotype are controls, then the cell count for $A_2A_2$ cases is zero. In this scenario, the maximum likelihood estimate for the corresponding regression coefficient (the log odds ratio) diverges to $-\\infty$, as the model tries to predict a zero probability of being a case for this genotype. Similarly, if all $A_2A_2$ individuals are cases, the estimate diverges to $+\\infty$. Quasi-complete separation is a related issue where a zero cell count exists, also leading to non-existence of the finite maximum likelihood estimate.\n\nGiven that the total count of minor-allele homozygotes is very small ($20$), and this small number is further stratified by phenotype status, the probability of observing a zero cell in the contingency table of genotype-by-phenotype status is very high, especially for phenotypes with low prevalence. Since a PheWAS involves testing thousands of phenotypes, many of which are rare, this issue of data sparsity and resulting separation is not an anomaly but a systematic, widespread problem.\n\n**Firth's penalized likelihood** (often called Firth correction) is a standard method to address this. It modifies the score function used in maximum likelihood estimation by adding a penalty term derived from the Jeffreys prior. This penalty effectively shrinks the coefficient estimates towards zero, preventing them from diverging to infinity. This ensures that finite, stable estimates for the odds ratios can be obtained even in the presence of complete separation. The resulting estimates are also less biased in small samples. Therefore, its application across many or all phenotypes in a PheWAS is warranted to ensure robust and reliable results in the face of inevitable data sparsity for rare variants and/or rare phenotypes.\n\nThe final numeric answer requested is the expected number of minor-allele homozygotes in the full sample.", "answer": "$$\\boxed{20}$$", "id": "5071604"}, {"introduction": "A core challenge of the \"phenome-wide\" approach is correcting for the thousands of statistical tests performed, a problem complicated by the natural correlation between clinical phenotypes. This exercise introduces a powerful, non-parametric solution: permutation testing. You will explore how this method creates an empirical null distribution that respects the phenotype correlation structure, providing a more accurate way to calculate adjusted $p$-values and control the family-wise error rate. [@problem_id:5071614]", "problem": "A researcher performs a Phenome-Wide Association Study (PheWAS) for a single genetic variant across three binary clinical phenotypes measured in the same sample of $N = 1000$ unrelated individuals. The genetic variant is coded additively as $G \\in \\{0,1,2\\}$ and has minor allele frequency approximately $0.30$. Each phenotype $Y_{j}$ for $j \\in \\{1,2,3\\}$ is modeled with a logistic regression of the form $\\ln\\left(\\frac{\\Pr(Y_{j}=1 \\mid G)}{\\Pr(Y_{j}=0 \\mid G)}\\right) = \\alpha_{j} + \\beta_{j} G$, with $\\beta_{j}$ tested under the null hypothesis $H_{0,j}: \\beta_{j} = 0$. The joint phenotype correlation matrix (estimated from the observed sample) is\n$$\n\\Sigma_{Y} = \\begin{pmatrix}\n1  0.45  0.10 \\\\\n0.45  1  0.20 \\\\\n0.10  0.20  1\n\\end{pmatrix},\n$$\nreflecting nontrivial correlations among phenotypes.\n\nFrom the original (unpermuted) data, the researcher obtains Wald $z$-statistics for the genotype coefficient: $Z_{1} = 2.50$, $Z_{2} = 1.20$, and $Z_{3} = -0.50$. To construct an empirical null that respects the observed phenotype correlation structure, the researcher performs $B = 5000$ random permutations of the genotype labels across the $N$ individuals, leaving the phenotype data matrix unchanged. For each permutation $b \\in \\{1,\\dots,B\\}$, she re-fits the three logistic models to obtain $Z_{1}^{(b)}$, $Z_{2}^{(b)}$, $Z_{3}^{(b)}$, and records the maximum absolute statistic $M^{(b)} = \\max\\left(|Z_{1}^{(b)}|, |Z_{2}^{(b)}|, |Z_{3}^{(b)}|\\right)$. In the resulting permutation sample, it is observed that in $k = 62$ of the $B = 5000$ permutations, the maximum absolute statistic satisfies $M^{(b)} \\ge |Z_{1}| = 2.50$.\n\nUsing fundamental definitions of the null hypothesis as genotype–phenotype independence and the definition of the empirical $p$-value as a tail probability under the null, explain how permuting genotype labels while keeping phenotypes intact yields a valid empirical null distribution for the maximum statistic that preserves $\\Sigma_{Y}$. Then, compute the phenotype-wise adjusted $p$-value for phenotype $1$ based on the permutation maxima, using the add-one correction to ensure a valid tail probability, and round your final numerical answer to four significant figures. Express the final value as a pure number with no units.", "solution": "The problem is deemed valid as it describes a standard and scientifically sound procedure in statistical genetics—a Phenome-Wide Association Study (PheWAS) using a permutation-based approach for multiple testing correction. The problem is self-contained, well-posed, objective, and relies on established statistical principles.\n\nThe solution consists of two parts: first, an explanation of the validity of the permutation procedure, and second, the calculation of the adjusted $p$-value.\n\n### Part 1: Validity of the Permutation Procedure\n\nThe fundamental null hypothesis ($H_0$) in this genetic association context is that the genotype $G$ is not associated with any of the phenotypes $\\{Y_1, Y_2, Y_3\\}$. Statistically, this implies that the genotype and the vector of phenotypes are independent. Under this hypothesis of independence, the observed pairing of the vector of genotype labels, $(G_1, G_2, \\dots, G_N)$, with the matrix of phenotype data is merely one of $N!$ equally likely pairings.\n\nThe permutation procedure described directly simulates this null hypothesis. For each permutation $b$, the vector of genotype labels is randomly shuffled, and these permuted genotypes are paired with the original, unshuffled phenotype data. This process explicitly breaks any true genotype-phenotype association that might exist in the data. Therefore, the test statistics ($Z_{1}^{(b)}, Z_{2}^{(b)}, Z_{3}^{(b)}$) calculated from each permuted dataset are, by construction, drawn from the null distribution where no association exists.\n\nA critical feature of this procedure is that the phenotype data for each individual remains intact. That is, for each individual $i$, their set of measurements $(Y_{i1}, Y_{i2}, Y_{i3})$ is treated as a fixed unit. Consequently, the entire $N \\times 3$ matrix of phenotype data is held constant across all permutations. Because the vectors of observations for each phenotype $Y_j$ do not change, any relationships *among* the phenotypes are preserved. This includes their pairwise correlations. The sample correlation matrix, $\\Sigma_Y$, is an estimate of the true population correlation structure and is a property of the phenotype data alone. By not permuting the phenotypes, this structure is perfectly maintained in every iteration of the permutation test.\n\nThe test statistic chosen for multiple testing correction is the maximum absolute Wald statistic, $M = \\max(|Z_1|, |Z_2|, |Z_3|)$. To assess the significance of the observed maximum, $M_{obs}$, we require its distribution under the global null hypothesis. This null distribution depends on the joint distribution of the individual test statistics $(Z_1, Z_2, Z_3)$, which is in turn influenced by the correlation among the phenotypes, $\\Sigma_Y$. An analytical derivation of this joint distribution can be complex, especially for logistic regression models.\n\nThe permutation procedure provides an elegant, non-parametric solution. By repeatedly generating the maximum statistic $M^{(b)}$ under a simulated null hypothesis that also preserves the phenotype correlation structure $\\Sigma_Y$, we construct an empirical null distribution for $M$. This empirical distribution correctly accounts for the dependencies among the test statistics that arise from the correlated phenotypes. Comparing our observed maximum statistic to this empirically generated null distribution yields a family-wise error rate (FWER) adjusted $p$-value that is valid and often more powerful than standard analytical corrections (e.g., Bonferroni) that might make overly conservative assumptions about the correlation structure.\n\n### Part 2: Calculation of the Adjusted $p$-Value\n\nThe problem asks for the phenotype-wise adjusted $p$-value for phenotype $1$, which is derived from the distribution of the maximum statistic. This adjusted $p$-value, denoted $p_{adj,1}$, is the estimated probability of observing a maximum null statistic that is at least as extreme as the observed statistic for phenotype $1$, $|Z_1|$. This value represents the significance of phenotype $1$ after correcting for the multiple tests performed across all three phenotypes.\n\nThe provided data are:\n-   Number of permutations: $B = 5000$.\n-   Observed Wald statistic for phenotype $1$: $Z_1 = 2.50$.\n-   The number of permutations where the maximum absolute statistic, $M^{(b)} = \\max(|Z_1^{(b)}|, |Z_2^{(b)}|, |Z_3^{(b)}|)$, was greater than or equal to the observed absolute statistic for phenotype $1$, $|Z_1| = |2.50| = 2.50$. This count is given as $k = 62$.\n\nThe empirical $p$-value is the proportion of null statistics that are at least as extreme as the observed statistic. To avoid a $p$-value of $0$ when $k=0$ and to provide a more stable estimate, the \"add-one\" correction is used. The formula for the adjusted $p$-value with this correction is:\n$$p_{adj,1} = \\frac{k+1}{B+1}$$\nSubstituting the given values into this formula:\n$$p_{adj,1} = \\frac{62+1}{5000+1} = \\frac{63}{5001}$$\nNow, we perform the division and round the result to four significant figures:\n$$p_{adj,1} = \\frac{63}{5001} \\approx 0.0125974805...$$\nRounding this value to four significant figures, we examine the fifth significant digit. The first four significant figures are $1$, $2$, $5$, and $9$. The fifth digit is $7$. Since $7 \\ge 5$, we round up the fourth significant digit. Rounding $9$ up results in $0$ and carrying over $1$ to the previous digit.\n$$p_{adj,1} \\approx 0.01260$$\nThe trailing zero is significant and is kept to indicate the specified precision.", "answer": "$$\\boxed{0.01260}$$", "id": "5071614"}]}