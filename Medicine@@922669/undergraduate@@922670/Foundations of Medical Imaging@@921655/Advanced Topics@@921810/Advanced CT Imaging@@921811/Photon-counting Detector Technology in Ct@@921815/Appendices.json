{"hands_on_practices": [{"introduction": "The primary function of any detector is to efficiently stop and register incoming photons. This exercise explores the concept of quantum efficiency, $\\eta$, which is the probability that a photon is absorbed within the detector material. By applying the fundamental Beer-Lambert law to a Cadmium Telluride (CdTe) sensor—a material widely used in modern PCDs—you will calculate this efficiency at different energies and analyze the critical trade-off between detector thickness and charge collection performance. [@problem_id:4911033]", "problem": "A Photon-Counting Detector (PCD) module in Computed Tomography (CT) uses a Cadmium Telluride (CdTe) sensor of thickness $d=2\\,\\mathrm{mm}$. Assume normally incident photons and neglect scattering. Start from the Beer–Lambert law for a monoenergetic beam traversing a homogeneous attenuating medium, and the definition of quantum efficiency as the fraction of incident photons that are absorbed in the sensor. Derive an expression for the quantum efficiency $\\eta(E)$ in terms of the linear attenuation coefficient $\\mu(E)$ and the thickness $d$. Then, using the following tabulated linear attenuation coefficients for CdTe,\n- $\\mu(30\\,\\mathrm{keV})=3.5\\,\\mathrm{mm}^{-1}$,\n- $\\mu(60\\,\\mathrm{keV})=1.2\\,\\mathrm{mm}^{-1}$,\n- $\\mu(100\\,\\mathrm{keV})=0.6\\,\\mathrm{mm}^{-1}$,\ncompute $\\eta(E)$ for $E=30\\,\\mathrm{keV}$, $E=60\\,\\mathrm{keV}$, and $E=100\\,\\mathrm{keV}$. Report the three values in the order $E=30\\,\\mathrm{keV}$, $E=60\\,\\mathrm{keV}$, $E=100\\,\\mathrm{keV}$. Round each value to four significant figures and express your answers as decimals (do not use a percentage symbol).\n\nFinally, briefly discuss, using first principles of charge transport (drift under an electric field and finite carrier lifetimes), the trade-off between increasing $d$ to improve $\\eta(E)$ and the risk of degraded charge transport performance in CdTe PCDs.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is based on the fundamental Beer–Lambert law and established principles of semiconductor detector physics. The provided data are realistic and sufficient to solve the problem. The tasks are clearly defined and lead to a unique, meaningful solution. Therefore, the problem is deemed valid and a full solution will be provided.\n\nThe first part of the problem requires the derivation of an expression for the quantum efficiency, $\\eta(E)$, of a photon-counting detector.\n\nWe begin with the Beer–Lambert law, which describes the attenuation of a monoenergetic beam of photons as it passes through a homogeneous material. Let $N_0(E)$ be the number of incident photons of energy $E$ per unit area per unit time. The number of photons, $N(x, E)$, that have not interacted after traversing a distance $x$ into the material is given by:\n$$N(x, E) = N_0(E) \\exp(-\\mu(E)x)$$\nwhere $\\mu(E)$ is the linear attenuation coefficient of the material at energy $E$.\n\nThe detector has a thickness $d$. The number of photons that pass through the entire detector without interacting (the transmitted photons) is found by setting $x=d$:\n$$N_{trans}(E) = N(d, E) = N_0(E) \\exp(-\\mu(E)d)$$\nThe problem states to neglect scattering. Under this assumption, any photon that does not pass through is considered to be absorbed. The number of absorbed photons, $N_{abs}(E)$, is the difference between the incident and transmitted photons:\n$$N_{abs}(E) = N_0(E) - N_{trans}(E)$$\nSubstituting the expression for $N_{trans}(E)$:\n$$N_{abs}(E) = N_0(E) - N_0(E) \\exp(-\\mu(E)d) = N_0(E) [1 - \\exp(-\\mu(E)d)]$$\nQuantum efficiency, $\\eta(E)$, is defined as the fraction of incident photons that are absorbed by the sensor. Mathematically, this is the ratio of the number of absorbed photons to the number of incident photons:\n$$\\eta(E) = \\frac{N_{abs}(E)}{N_0(E)}$$\nSubstituting the expression for $N_{abs}(E)$ into this definition, we obtain the desired expression for quantum efficiency:\n$$\\eta(E) = 1 - \\exp(-\\mu(E)d)$$\n\nThe second part of the problem is to compute the quantum efficiency for three specific photon energies using the provided data for Cadmium Telluride (CdTe). The sensor thickness is given as $d=2\\,\\mathrm{mm}$.\n\nCase 1: $E = 30\\,\\mathrm{keV}$\nThe linear attenuation coefficient is $\\mu(30\\,\\mathrm{keV}) = 3.5\\,\\mathrm{mm}^{-1}$.\nThe argument of the exponent is the product $\\mu(E)d$:\n$$\\mu(30\\,\\mathrm{keV})d = (3.5\\,\\mathrm{mm}^{-1}) \\times (2\\,\\mathrm{mm}) = 7$$\nThe quantum efficiency is:\n$$\\eta(30\\,\\mathrm{keV}) = 1 - \\exp(-7) \\approx 1 - 0.0009118819... \\approx 0.999088118...$$\nRounding to four significant figures, we get $\\eta(30\\,\\mathrm{keV}) = 0.9991$.\n\nCase 2: $E = 60\\,\\mathrm{keV}$\nThe linear attenuation coefficient is $\\mu(60\\,\\mathrm{keV}) = 1.2\\,\\mathrm{mm}^{-1}$.\nThe argument of the exponent is:\n$$\\mu(60\\,\\mathrm{keV})d = (1.2\\,\\mathrm{mm}^{-1}) \\times (2\\,\\mathrm{mm}) = 2.4$$\nThe quantum efficiency is:\n$$\\eta(60\\,\\mathrm{keV}) = 1 - \\exp(-2.4) \\approx 1 - 0.09071795... \\approx 0.90928204...$$\nRounding to four significant figures, we get $\\eta(60\\,\\mathrm{keV}) = 0.9093$.\n\nCase 3: $E = 100\\,\\mathrm{keV}$\nThe linear attenuation coefficient is $\\mu(100\\,\\mathrm{keV}) = 0.6\\,\\mathrm{mm}^{-1}$.\nThe argument of the exponent is:\n$$\\mu(100\\,\\mathrm{keV})d = (0.6\\,\\mathrm{mm}^{-1}) \\times (2\\,\\mathrm{mm}) = 1.2$$\nThe quantum efficiency is:\n$$\\eta(100\\,\\mathrm{keV}) = 1 - \\exp(-1.2) \\approx 1 - 0.30119421... \\approx 0.69880578...$$\nRounding to four significant figures, we get $\\eta(100\\,\\mathrm{keV}) = 0.6988$.\n\nThe final part of the problem asks for a discussion of the trade-off between detector thickness $d$ and charge transport performance.\n\nFrom the derived expression $\\eta(E) = 1 - \\exp(-\\mu(E)d)$, it is clear that increasing the detector thickness $d$ leads to an increase in quantum efficiency, as the negative exponential term diminishes. A higher $\\eta(E)$ is desirable because it means more incident photons are detected, improving the statistical quality of the image and/or allowing for a lower radiation dose to the patient. This improvement is particularly important for higher-energy photons, which have a lower $\\mu(E)$ and are thus more likely to penetrate a thin detector.\n\nHowever, increasing $d$ introduces significant challenges related to charge transport within the semiconductor material (CdTe). Here is the reasoning based on first principles:\n\n1.  **Charge Generation and Transport**: When a photon is absorbed in the semiconductor, it creates a number of electron-hole pairs proportional to the absorbed photon energy. An external electric field, $E_{field}$, is applied across the detector thickness to separate these pairs and drift them towards the collecting electrodes (electrons to the anode, holes to the cathode). The drift velocity $v$ of a charge carrier is given by $v = \\mu_{carrier} E_{field}$, where $\\mu_{carrier}$ is the carrier mobility.\n\n2.  **Carrier Trapping and Recombination**: Semiconductor crystals like CdTe are not perfect and contain defects (impurities, vacancies, dislocations). These defects act as trapping centers or recombination centers. A charge carrier moving through the crystal can be captured by a trapping center for a period of time or can recombine with a carrier of the opposite type, effectively being lost. The average time a carrier can drift before being trapped is its lifetime, $\\tau$.\n\n3.  **Charge Collection Efficiency (CCE)**: For a full signal to be registered, a charge carrier must travel the entire distance to its respective electrode. The maximum distance it needs to travel is the detector thickness $d$. The mean distance a carrier can travel before trapping, known as the drift length or Schubweg, is $\\lambda = v \\tau = (\\mu_{carrier} E_{field}) \\tau$. The product $\\mu_{carrier}\\tau$, the mobility-lifetime product, is a critical figure of merit for a detector material.\n\n4.  **The Trade-off**: If the detector thickness $d$ is increased, the transit distance for charges created deep within the detector also increases. If $d$ becomes comparable to or greater than the drift length $\\lambda$ of the carriers (i.e., $d \\gtrsim \\lambda$), there is a high probability that carriers will be trapped before reaching the electrodes. This phenomenon is known as incomplete charge collection. In CdTe, holes have a significantly lower mobility-lifetime product ($\\mu_h \\tau_h$) compared to electrons ($\\mu_e \\tau_e$), making hole trapping the dominant limiting factor.\n\n5.  **Consequences of Incomplete Charge Collection**:\n    *   **Signal Amplitude Reduction**: The magnitude of the measured electrical pulse is proportional to the amount of charge collected. Trapping reduces the collected charge, resulting in a lower pulse height than what corresponds to the full energy of the incident photon.\n    *   **Degradation of Energy Resolution**: The interaction depth of photons is a random variable. Photons interacting close to a collecting electrode have a short path for one carrier type and a long path for the other. Photons interacting in the middle require both carriers to travel a distance of approximately $d/2$. This depth-dependent variation in charge loss leads to a broadening of the full-energy peak in the measured spectrum. This degradation of energy resolution is highly detrimental to a photon-counting detector, as its primary function is to accurately discriminate between photons of different energies.\n\nIn summary, there is a fundamental trade-off: increasing detector thickness $d$ improves quantum efficiency $\\eta(E)$, but at the cost of degraded charge collection efficiency, which leads to poorer energy resolution and spectral distortion. The optimal design of a PCD sensor involves a careful balance to achieve sufficiently high $\\eta(E)$ across the relevant energy spectrum while keeping $d$ small enough relative to the carrier drift lengths to ensure excellent spectral performance.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.9991  0.9093  0.6988\n\\end{pmatrix}\n}\n$$", "id": "4911033"}, {"introduction": "Once a photon is absorbed, the pixel's electronics must process the resulting signal, and there is a physical limit to how fast this can occur. This practice delves into the critical performance metric of count rate, which is constrained by the processing speed of both the analog and digital components within each pixel. By modeling the system as a combination of a discriminator with a finite dead time and a counter with a finite clock speed, you will learn how to determine the maximum sustainable count rate and identify the performance bottleneck. [@problem_id:4911031]", "problem": "A single pixel in a Photon-Counting Detector (PCD) for Computed Tomography (CT) converts each absorbed X-ray photon into a shaped analog pulse that is discriminated by a fast comparator and then counted by a synchronous digital counter. Assume the following physically grounded device behaviors that are standard in pulse-counting systems:\n\n- Analog discrimination and pulse separation: After an input pulse crosses the comparator threshold, the comparator output transitions with a finite leading-edge rise time $t_{r}$ and requires a finite recovery time $t_{\\mathrm{rec}}$ to return below threshold and be ready to resolve a subsequent pulse as a distinct event. Consequently, any two detected events separated by less than $t_{r} + t_{\\mathrm{rec}}$ cannot both be registered without loss. Treat this as a nonparalyzable dead-time constraint.\n- Synchronous digital counting: The downstream edge-counting logic is clocked at frequency $f_{\\mathrm{clk}}$ and can increment by at most one count per clock period due to synchronous sampling of the comparator’s digital output.\n\nStarting from these two constraints and fundamental timing arguments (no additional formulas are provided), derive an expression for the maximum sustainable count rate $R_{\\max}$ at which the pixel can register all incident events without loss in steady operation. Then evaluate $R_{\\max}$ numerically for\n$t_{r} = 2.5\\,\\mathrm{ns}$, $t_{\\mathrm{rec}} = 12.5\\,\\mathrm{ns}$, and $f_{\\mathrm{clk}} = 100\\,\\mathrm{MHz}$.\n\nState which subsystem is the bottleneck under these numerical conditions based on your derivation. Round your final numerical result for $R_{\\max}$ to four significant figures and express it in megacounts per second (Mcps). Provide only the value of $R_{\\max}$ as your final answer.", "solution": "The problem statement has been validated and is deemed scientifically sound, well-posed, and objective. It is based on fundamental principles of pulse-counting electronics and provides a complete and consistent set of givens. We may therefore proceed with a formal derivation.\n\nThe goal is to find the maximum sustainable count rate $R_{\\max}$ at which all incident X-ray photon events are registered without loss. The system has two primary subsystems, the analog front-end and the synchronous digital counter, each imposing its own rate limitation. The overall system performance is limited by the more restrictive of these two constraints.\n\nFirst, we analyze the constraint imposed by the analog front-end, which includes the discriminator and pulse shaping circuitry. The problem states that after a pulse is detected, a total time of $t_{\\mathrm{dead}} = t_r + t_{\\mathrm{rec}}$ must elapse before a subsequent pulse can be resolved and registered. Here, $t_r$ is the comparator's leading-edge rise time and $t_{\\mathrm{rec}}$ is its recovery time. This period, $t_{\\mathrm{dead}}$, constitutes a nonparalyzable dead time. To register all incident events without loss, the minimum time interval between any two consecutive events must be at least $t_{\\mathrm{dead}}$. The maximum sustainable rate is achieved when events arrive exactly at this minimum separation. Therefore, the maximum rate supported by the analog subsystem, $R_{\\max, \\mathrm{analog}}$, is the reciprocal of the dead time:\n$$R_{\\max, \\mathrm{analog}} = \\frac{1}{t_{\\mathrm{dead}}} = \\frac{1}{t_r + t_{\\mathrm{rec}}}$$\n\nSecond, we analyze the constraint imposed by the synchronous digital counter. This counter is driven by a clock with frequency $f_{\\mathrm{clk}}$. The period of this clock is $T_{\\mathrm{clk}} = \\frac{1}{f_{\\mathrm{clk}}}$. The problem specifies that the counter can increment by at most one count per clock period. This means the counter can register, at most, one event during each clock cycle. To register all events without loss, the minimum time interval between any two consecutive events must be at least one clock period, $T_{\\mathrm{clk}}$. Consequently, the maximum rate supported by the digital counter, $R_{\\max, \\mathrm{digital}}$, is the reciprocal of the minimum time interval, which is simply the clock frequency itself:\n$$R_{\\max, \\mathrm{digital}} = \\frac{1}{T_{\\mathrm{clk}}} = f_{\\mathrm{clk}}$$\n\nFor the entire system to operate without loss, both the analog and digital constraints must be satisfied simultaneously. An incoming train of events at a rate $R$ will be fully registered only if $R \\le R_{\\max, \\mathrm{analog}}$ AND $R \\le R_{\\max, \\mathrm{digital}}$. The maximum sustainable count rate for the entire pixel, $R_{\\max}$, is therefore the minimum of the maximum rates allowed by the individual subsystems. The system's performance is dictated by its bottleneck.\n$$R_{\\max} = \\min(R_{\\max, \\mathrm{analog}}, R_{\\max, \\mathrm{digital}}) = \\min\\left(\\frac{1}{t_r + t_{\\mathrm{rec}}}, f_{\\mathrm{clk}}\\right)$$\nThis is the derived expression for the maximum sustainable count rate.\n\nNow, we evaluate this expression using the provided numerical values:\n$t_r = 2.5\\,\\mathrm{ns} = 2.5 \\times 10^{-9}\\,\\mathrm{s}$\n$t_{\\mathrm{rec}} = 12.5\\,\\mathrm{ns} = 12.5 \\times 10^{-9}\\,\\mathrm{s}$\n$f_{\\mathrm{clk}} = 100\\,\\mathrm{MHz} = 100 \\times 10^6\\,\\mathrm{Hz} = 1 \\times 10^8\\,\\mathrm{s}^{-1}$\n\nWe calculate the maximum rate for each subsystem.\nFor the analog front-end:\nThe total dead time is $t_{\\mathrm{dead}} = t_r + t_{\\mathrm{rec}} = 2.5 \\times 10^{-9}\\,\\mathrm{s} + 12.5 \\times 10^{-9}\\,\\mathrm{s} = 15.0 \\times 10^{-9}\\,\\mathrm{s}$.\nThe maximum rate is:\n$$R_{\\max, \\mathrm{analog}} = \\frac{1}{15.0 \\times 10^{-9}\\,\\mathrm{s}} = \\frac{1}{15} \\times 10^9\\,\\mathrm{s}^{-1} \\approx 6.6667 \\times 10^7\\,\\mathrm{s}^{-1}$$\nIn megacounts per second (Mcps), this is approximately $66.67\\,\\mathrm{Mcps}$.\n\nFor the digital counter:\n$$R_{\\max, \\mathrm{digital}} = f_{\\mathrm{clk}} = 1 \\times 10^8\\,\\mathrm{s}^{-1} = 100\\,\\mathrm{Mcps}$$\n\nTo find the overall maximum rate, we take the minimum of these two values:\n$$R_{\\max} = \\min(66.67\\,\\mathrm{Mcps}, 100\\,\\mathrm{Mcps})$$\nSince $66.67  100$, the analog front-end is the performance-limiting subsystem, or the bottleneck.\nThe maximum sustainable count rate for the pixel is $R_{\\max} = R_{\\max, \\mathrm{analog}} \\approx 66.666...$\\,Mcps.\n\nThe problem requires the final numerical result to be rounded to four significant figures.\n$$R_{\\max} \\approx 66.67\\,\\mathrm{Mcps}$$\nThe value to be provided as the final answer is $66.67$.", "answer": "$$\\boxed{66.67}$$", "id": "4911031"}, {"introduction": "Moving from the performance of a single pixel to the design of a complete detector array requires balancing multiple, often competing, requirements. This hands-on problem puts you in the role of a system designer, tasked with choosing an optimal pixel size to meet a clinical need for both high spatial resolution and accurate performance under high photon flux. You will apply your understanding of how pixel pitch influences both spatial sampling and per-pixel count rate to navigate the crucial trade-off between image sharpness and pulse pile-up artifacts. [@problem_id:4911053]", "problem": "A Photon-Counting Detector (PCD) module in a Computed Tomography (CT) system uses square pixels of pitch $p$ so that the active pixel area is $a = p^{2}$. A scan configuration produces a uniform photon flux density at the detector plane of $\\Phi = 5 \\times 10^{7} \\ \\mathrm{photons}/(\\mathrm{s}\\cdot\\mathrm{mm}^{2})$. Photon arrivals can be modeled as a Poisson process, and the front-end electronics for each pixel exhibit a nonparalyzable dead time of $\\tau = 50 \\ \\mathrm{ns}$. The clinical task requires in-plane resolution of $d_{\\mathrm{res}} = 0.30 \\ \\mathrm{mm}$ at isocenter. For this task, the design criterion is to keep dead-time/pile-up losses below $5\\%$ at the stated peak flux density.\n\nExplain, from first principles, the trade-off between pixel pitch $p$ and the per-pixel count rate under fixed $\\Phi$, and use that reasoning to choose the optimal pixel pitch among the options, defined here as the largest $p$ that simultaneously meets the resolution requirement and the pile-up loss constraint at the stated $\\Phi$ and $\\tau$.\n\nWhich option is optimal?\n\nA. $p = 0.50 \\ \\mathrm{mm}$\n\nB. $p = 0.25 \\ \\mathrm{mm}$\n\nC. $p = 0.125 \\ \\mathrm{mm}$\n\nD. $p = 0.0625 \\ \\mathrm{mm}$", "solution": "The user wants me to solve a problem about Photon-Counting Detectors (PCDs).\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Detector pixel geometry: Square pixels of pitch $p$.\n- Active pixel area: $a = p^2$.\n- Photon flux density: $\\Phi = 5 \\times 10^{7} \\ \\text{photons}/(\\text{s}\\cdot\\text{mm}^{2})$.\n- Photon arrival statistics: Poisson process.\n- Detector electronics model: Nonparalyzable dead time.\n- Dead time value: $\\tau = 50 \\ \\text{ns}$.\n- Required in-plane resolution at isocenter: $d_{\\mathrm{res}} = 0.30 \\ \\text{mm}$.\n- Design criterion on count loss: Dead-time/pile-up losses must be below $5\\%$.\n- Definition of \"optimal\": The largest pixel pitch $p$ that meets both the resolution requirement and the pile-up loss constraint.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is well-grounded in the principles of medical imaging physics, specifically detector technology for X-ray Computed Tomography (CT). The use of a Poisson model for photon arrivals, the concept of a nonparalyzable dead time, and the relation between pixel size, spatial resolution, and count rate performance are all standard and scientifically sound concepts.\n- **Well-Posedness**: The problem is well-posed. It requires finding an optimal value for a parameter ($p$) subject to a set of well-defined constraints (resolution and pile-up loss) and provides a clear definition for \"optimal\" (the largest value of $p$ that satisfies the constraints).\n- **Objectivity**: The problem is stated in objective, quantitative terms.\n- **Consistency and Completeness**:\n    - The given values for flux density ($\\Phi$), dead time ($\\tau$), and resolution ($d_{\\mathrm{res}}$) are physically realistic for a high-resolution CT application.\n    - There is a minor ambiguity in the problem statement regarding the resolution constraint. The relationship between the required system resolution at isocenter ($d_{\\mathrm{res}}$) and the detector pixel pitch ($p$) also involves the system's geometric magnification factor, $M$. This factor is not provided. However, a common and reasonable simplification in such problems is to establish a direct relationship between $p$ and $d_{\\mathrm{res}}$. For instance, one might assume $p \\le d_{\\mathrm{res}}$ or, based on the Nyquist sampling theorem, $p \\le d_{\\mathrm{res}}/2$. As will be shown in the derivation, the pile-up constraint is more restrictive than either of these reasonable interpretations of the resolution constraint for the larger pixel pitch options, meaning a unique solution can be found despite this minor ambiguity. The problem is therefore solvable without making arbitrary assumptions.\n\n**Step 3: Verdict and Action**\nThe problem is valid and can be solved by applying the stated principles and constraints.\n\n### Derivation and Solution\n\nThe problem asks for an explanation of the trade-off between pixel pitch $p$ and count rate, and then to find the optimal pixel pitch from the given options.\n\n**Trade-off between Pixel Pitch and Per-Pixel Count Rate**\nFrom first principles, the relationship between the detector's physical characteristics and its performance is as follows:\n1.  **Pixel Pitch and Resolution**: The spatial resolution of a digital imaging system is fundamentally limited by its pixel size. To resolve smaller features, one needs smaller detector pixels. A smaller pixel pitch $p$ allows for sampling the incoming X-ray field at a higher spatial frequency, which is a prerequisite for achieving higher spatial resolution in the reconstructed image.\n2.  **Pixel Pitch and Incident Photon Rate**: For a uniform photon flux density $\\Phi$ (photons per unit area per unit time), the incident photon rate on a single pixel, $R_{in}$, is the product of the flux density and the pixel area $a = p^2$. Thus, $R_{in} = \\Phi \\cdot p^2$. This means the incident rate on a pixel scales quadratically with the pixel pitch.\n3.  **Incident Rate and Pile-up Loss**: Photon-counting detectors have a characteristic nonparalyzable dead time $\\tau$, which is the minimum time required to process a single photon event. If a second photon arrives within this time window after a preceding one, it is not counted. This leads to count losses, also known as pile-up. For a nonparalyzable system, the measured count rate $R_{out}$ is related to the true incident rate $R_{in}$ by $R_{out} = \\frac{R_{in}}{1 + R_{in}\\tau}$. The fractional count loss, $L$, is given by $L = \\frac{R_{in} - R_{out}}{R_{in}} = \\frac{R_{in}\\tau}{1 + R_{in}\\tau}$.\n\nThe trade-off is therefore clear: a larger pixel pitch $p$ increases the incident photon rate $R_{in}$ on the pixel. While this can increase the signal collected by the pixel, it also leads to a higher fractional count loss $L$. Conversely, a smaller pixel pitch $p$ reduces the per-pixel incident rate $R_{in}$, thereby decreasing pile-up losses, and simultaneously enables higher potential spatial resolution. The task is to find the largest pixel pitch $p$ that does not violate the specified limits on resolution and pile-up loss.\n\nNext, we formalize the two constraints to find the permissible range for $p$.\n\n**Constraint 1: Resolution Requirement**\nThe system must achieve an in-plane resolution of $d_{\\mathrm{res}} = 0.30 \\ \\text{mm}$. A fundamental principle of digital sampling, the Nyquist-Shannon theorem, states that to unambiguously resolve a spatial frequency $f$, the sampling frequency must be at least $2f$. This implies the sampling interval must be no more than half the corresponding spatial wavelength. Applying this principle to imaging, to resolve a feature of size $d_{\\mathrm{res}}$, the effective pixel size at that location should be at most $d_{\\mathrm{res}}/2$. While a complete CT system model would include a magnification factor, a common and robust design rule is to require the detector pixel pitch $p$ itself to be smaller than or on the order of the target resolution. A conservative interpretation sets the limit $p \\le d_{\\mathrm{res}}/2$. A more lenient one would be $p \\le d_{\\mathrm{res}}$. Let's evaluate both.\n-   Lenient case: $p \\le d_{\\mathrm{res}} = 0.30 \\ \\text{mm}$.\n-   Conservative (Nyquist) case: $p \\le d_{\\mathrm{res}}/2 = 0.15 \\ \\text{mm}$.\nAs we will see, the pile-up constraint is more restrictive and will be the deciding factor.\n\n**Constraint 2: Pile-up Loss Constraint**\nThe pile-up loss must be below $5\\%$.\n$$L = \\frac{R_{in}\\tau}{1 + R_{in}\\tau}  0.05$$\nSolving for the product $R_{in}\\tau$:\n$$R_{in}\\tau  0.05 \\cdot (1 + R_{in}\\tau)$$\n$$R_{in}\\tau  0.05 + 0.05 R_{in}\\tau$$\n$$0.95 R_{in}\\tau  0.05$$\n$$R_{in}\\tau  \\frac{0.05}{0.95} = \\frac{5}{95} = \\frac{1}{19}$$\nNow substitute $R_{in} = \\Phi p^2$:\n$$(\\Phi p^2)\\tau  \\frac{1}{19}$$\nWe can now solve for the maximum allowed pixel pitch, $p_{max}$, based on this loss constraint.\n$$p^2  \\frac{1}{19 \\Phi \\tau}$$\n$$p  \\sqrt{\\frac{1}{19 \\Phi \\tau}}$$\nLet's substitute the given values, keeping units in mm and seconds:\n- $\\Phi = 5 \\times 10^{7} \\ \\text{photons} \\cdot \\text{s}^{-1} \\cdot \\text{mm}^{-2}$\n- $\\tau = 50 \\ \\text{ns} = 50 \\times 10^{-9} \\ \\text{s} = 5 \\times 10^{-8} \\ \\text{s}$\n\n$$p  \\sqrt{\\frac{1}{19 \\cdot (5 \\times 10^{7} \\ \\text{s}^{-1} \\text{mm}^{-2}) \\cdot (5 \\times 10^{-8} \\ \\text{s})}}$$\n$$p  \\sqrt{\\frac{1}{19 \\cdot (2.5 \\ \\text{mm}^{-2})}}$$\n$$p  \\sqrt{\\frac{1}{47.5 \\ \\text{mm}^{-2}}}$$\n$$p  \\sqrt{0.02105...} \\ \\text{mm}$$\n$$p  0.14509... \\ \\text{mm}$$\nSo, the pile-up constraint requires $p  0.145 \\ \\text{mm}$.\n\n**Combining Constraints and Evaluating Options**\nWe have two constraints:\n1.  Resolution: $p \\le 0.30 \\ \\text{mm}$ (lenient) or $p \\le 0.15 \\ \\text{mm}$ (conservative).\n2.  Pile-up Loss: $p  0.145 \\ \\text{mm}$.\n\nThe condition that must be satisfied is the intersection of these constraints. The pile-up loss constraint ($p  0.145 \\ \\text{mm}$) is stricter than the lenient resolution constraint and very similar to the conservative one. Therefore, any acceptable pixel pitch must satisfy $p  0.145 \\ \\text{mm}$.\n\nNow we evaluate each option against this governing constraint. The \"optimal\" choice is the largest $p$ that satisfies it.\n\n**A. $p = 0.50 \\ \\text{mm}$**\n- This pixel pitch does not satisfy the constraint $p  0.145 \\ \\text{mm}$ ($0.50 \\not 0.145$).\n- It also fails both interpretations of the resolution constraint ($0.50 \\not\\le 0.30$).\n- **Verdict: Incorrect.**\n\n**B. $p = 0.25 \\ \\text{mm}$**\n- This pixel pitch does not satisfy the constraint $p  0.145 \\ \\text{mm}$ ($0.25 \\not 0.145$).\n- As a check, let's calculate the loss: $R_{in} = (5 \\times 10^7) \\cdot (0.25)^2 = 3.125 \\times 10^6 \\ \\text{s}^{-1}$. $R_{in}\\tau = (3.125 \\times 10^6) \\cdot (5 \\times 10^{-8}) = 0.15625$. $L = 0.15625 / (1 + 0.15625) \\approx 0.135$, or $13.5\\%$, which is greater than the $5\\%$ limit.\n- **Verdict: Incorrect.**\n\n**C. $p = 0.125 \\ \\text{mm}$**\n- This pixel pitch satisfies the constraint $p  0.145 \\ \\text{mm}$ ($0.125  0.145$).\n- It also satisfies both interpretations of the resolution constraint ($0.125 \\le 0.30$ and $0.125 \\le 0.15$).\n- Let's verify the loss: $R_{in} = (5 \\times 10^7) \\cdot (0.125)^2 = 781250 \\ \\text{s}^{-1}$. $R_{in}\\tau = 781250 \\cdot (5 \\times 10^{-8}) = 0.0390625$. $L = 0.0390625 / (1 + 0.0390625) \\approx 0.0376$, or $3.76\\%$, which is less than the $5\\%$ limit.\n- This option is a valid solution.\n- **Verdict: Correct.**\n\n**D. $p = 0.0625 \\ \\text{mm}$**\n- This pixel pitch satisfies the constraint $p  0.145 \\ \\text{mm}$ ($0.0625  0.145$).\n- It also satisfies both interpretations of the resolution constraint.\n- The pile-up loss will be even lower than for option C. Since $p$ is halved, $p^2$ and thus $R_{in}$ are quartered. $L \\approx R_{in}\\tau \\approx 0.039/4 \\approx 1\\%$, which is less than the $5\\%$ limit.\n- This option is also a valid solution.\n- **Verdict: Correct.**\n\n**Choosing the Optimal Pixel Pitch**\nBoth option C ($p = 0.125 \\ \\text{mm}$) and option D ($p = 0.0625 \\ \\text{mm}$) satisfy all the design criteria. The problem defines the \"optimal\" pixel pitch as the **largest** $p$ that meets these requirements. Comparing the two valid options:\n$$0.125 \\ \\text{mm}  0.0625 \\ \\text{mm}$$\nTherefore, $p=0.125 \\ \\text{mm}$ is the optimal choice among the given options.", "answer": "$$\\boxed{C}$$", "id": "4911053"}]}