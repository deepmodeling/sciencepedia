{"hands_on_practices": [{"introduction": "Many model-based iterative reconstruction methods rely on minimizing a quadratic objective function, which elegantly reduces to solving a large linear system of equations. In this practice, you will implement the Conjugate Gradient (CG) algorithm, an essential tool for solving such systems efficiently without constructing the full system matrix. By exploring the role of preconditioning, you will gain a hands-on understanding of the computational engine inside many foundational CT reconstruction algorithms.", "problem": "Consider the quadratic Penalized Weighted Least Squares (PWLS) subproblem that arises in model-based and statistical iterative reconstruction for Computed Tomography (CT). The objective function is given by\n$$\n\\min_{\\mathbf{x} \\in \\mathbb{R}^n} \\ \\frac{1}{2}\\,\\lVert \\mathbf{y} - A\\mathbf{x} \\rVert_{W}^2 \\ + \\ \\frac{1}{2}\\,\\beta\\,\\lVert C\\mathbf{x} \\rVert_2^2,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ is the system matrix that maps image coefficients to line integrals, $\\mathbf{y} \\in \\mathbb{R}^m$ is the measurement vector, $W \\in \\mathbb{R}^{m \\times m}$ is a diagonal weighting matrix with nonnegative entries that reflect measurement variance, $C \\in \\mathbb{R}^{p \\times n}$ is a linear regularization operator, and $\\beta \\in \\mathbb{R}_{>0}$ is the regularization parameter. The weighted norm is defined by $\\lVert \\mathbf{v} \\rVert_W^2 = \\mathbf{v}^{\\top} W \\mathbf{v}$.\n\nStarting from fundamental definitions of convex quadratic minimization and the properties of symmetric positive definite (SPD) matrices, derive the normal equations for the minimizer and show that the solution $\\mathbf{x}^\\star$ satisfies\n$$\nH\\,\\mathbf{x}^\\star = \\mathbf{b},\n$$\nwhere\n$$\nH = A^{\\top} W A + \\beta\\, C^{\\top} C, \\qquad \\mathbf{b} = A^{\\top} W \\mathbf{y}.\n$$\nDesign a Conjugate Gradient (CG) scheme to solve the linear system $H\\,\\mathbf{x} = \\mathbf{b}$ using only matrix-vector products with $A$, $W$, $C$, and $C^{\\top}$, without explicitly forming $H$. Discuss and implement left preconditioning using the diagonal matrix\n$$\nM = \\operatorname{diag}\\!\\left(A^{\\top} W A + \\beta\\, C^{\\top} C\\right),\n$$\nand use $M^{-1}$ as the preconditioner. Explain why $M$ is a valid preconditioner and how it approximates $H$.\n\nYour program must implement both unpreconditioned CG and diagonally preconditioned CG for the system $H\\,\\mathbf{x} = \\mathbf{b}$ with the following specifications:\n- The initial iterate must be $\\mathbf{x}_0 = \\mathbf{0}$.\n- Use the Euclidean norm for residuals.\n- The stopping criterion must be the relative residual threshold\n$$\n\\frac{\\lVert \\mathbf{r}_k \\rVert_2}{\\lVert \\mathbf{r}_0 \\rVert_2} \\leq \\varepsilon,\n$$\nwith $\\varepsilon = 10^{-8}$, where $\\mathbf{r}_k = \\mathbf{b} - H\\mathbf{x}_k$.\n- The maximum number of iterations must be $5n$.\n\nImplement matrix-vector products via\n$$\nH\\mathbf{v} = A^{\\top}\\left(W(A\\mathbf{v})\\right) + \\beta\\, C^{\\top}(C\\mathbf{v}),\n$$\nand implement the diagonal preconditioner via\n$$\n\\operatorname{diag}(H)_j = \\sum_{i=1}^{m} W_{ii}\\,A_{ij}^2 + \\beta \\sum_{k=1}^{p} C_{kj}^2,\n$$\nwith the convention that any zero diagonal entry is replaced by a small positive constant to avoid division by zero in $M^{-1}$.\n\nTest Suite:\nFor reproducibility, use the following four test cases. In each case, construct $A$, $W$, $C$, $\\beta$, a ground-truth vector $\\mathbf{x}_{\\text{true}}$, and measurements $\\mathbf{y} = A\\mathbf{x}_{\\text{true}}$. Then set $\\mathbf{b} = A^{\\top} W \\mathbf{y}$.\n\n- Case 1 (general well-conditioned): $n=6$, $m=12$. Generate $A$ and $\\mathbf{x}_{\\text{true}}$ with a pseudorandom normal generator seeded by $1$. Generate $W$ as diagonal with entries drawn uniformly from $[0.5, 2.0]$. Set $C$ to be the first-order finite difference operator on $\\mathbb{R}^n$ of size $(n-1)\\times n$, and set $\\beta = 0.1$.\n\n- Case 2 (partial missing data via zero weights): $n=6$, $m=12$. Generate $A$ and $\\mathbf{x}_{\\text{true}}$ with a pseudorandom normal generator seeded by $2$. Generate initial diagonal $W$ entries uniformly on $[0.5, 2.0]$, then set four entries to zero at indices $2$, $3$, $6$, and $9$ (using zero-based indexing). Set $C = I_n$ (the $n \\times n$ identity) and $\\beta = 0.5$.\n\n- Case 3 (ill-conditioned forward model): $n=6$, $m=10$. Generate using a pseudorandom normal generator seeded by $3$ a vector $\\mathbf{u} \\in \\mathbb{R}^m$; form $A$ with its first column equal to $\\mathbf{u}$ and each subsequent column equal to $\\mathbf{u} + 10^{-4}\\,\\mathbf{v}_j$, where each $\\mathbf{v}_j$ is drawn i.i.d. from a standard normal distribution. Draw the diagonal entries of $W$ uniformly on $[0.8, 1.2]$. Set $C$ to be the first-order finite difference operator on $\\mathbb{R}^n$, and $\\beta = 10^{-4}$.\n\n- Case 4 (strong regularization): $n=8$, $m=12$. Generate $A$ and $\\mathbf{x}_{\\text{true}}$ with a pseudorandom normal generator seeded by $4$. Generate the diagonal entries of $W$ uniformly on $[0.2, 0.5]$. Set $C$ to be the first-order finite difference operator on $\\mathbb{R}^n$, and $\\beta = 100$.\n\nAngle units are not applicable. There are no physical units in the output.\n\nOutput Specification:\nFor each test case, run unpreconditioned CG and diagonally preconditioned CG and record the number of iterations required to reach the stopping criterion. The final program output must be a single line containing a comma-separated list enclosed in square brackets with eight integers in the following order:\n$$\n[\\text{iters\\_unpre\\_case1},\\text{iters\\_pre\\_case1},\\text{iters\\_unpre\\_case2},\\text{iters\\_pre\\_case2},\\text{iters\\_unpre\\_case3},\\text{iters\\_pre\\_case3},\\text{iters\\_unpre\\_case4},\\text{iters\\_pre\\_case4}].\n$$\nYour program must be self-contained, deterministic, and produce exactly this single line as its only output.", "solution": "The problem statement has been validated and is deemed sound, complete, and well-posed. We may proceed with the derivation and solution.\n\nThe problem requires the solution of a Penalized Weighted Least Squares (PWLS) minimization problem common in medical imaging reconstruction. The objective function is given by:\n$$\nf(\\mathbf{x}) = \\frac{1}{2}\\,\\lVert \\mathbf{y} - A\\mathbf{x} \\rVert_{W}^2 + \\frac{1}{2}\\,\\beta\\,\\lVert C\\mathbf{x} \\rVert_2^2\n$$\nwhere $\\mathbf{x} \\in \\mathbb{R}^n$ is the vector of image coefficients to be estimated, $\\mathbf{y} \\in \\mathbb{R}^m$ is the measurement vector, $A \\in \\mathbb{R}^{m \\times n}$ is the system matrix, $W \\in \\mathbb{R}^{m \\times m}$ is a diagonal weighting matrix with non-negative entries, $C \\in \\mathbb{R}^{p \\times n}$ is a regularization operator, and $\\beta \\in \\mathbb{R}_{>0}$ is the regularization parameter. The weighted norm is defined as $\\lVert \\mathbf{v} \\rVert_W^2 = \\mathbf{v}^{\\top} W \\mathbf{v}$.\n\n**1. Derivation of the Normal Equations**\n\nThe function $f(\\mathbf{x})$ is a quadratic function of $\\mathbf{x}$. To find the vector $\\mathbf{x}^\\star$ that minimizes $f(\\mathbf{x})$, we must find the point where the gradient of $f(\\mathbf{x})$ with respect to $\\mathbf{x}$ is zero. First, we expand the objective function:\n$$\nf(\\mathbf{x}) = \\frac{1}{2} (\\mathbf{y} - A\\mathbf{x})^{\\top} W (\\mathbf{y} - A\\mathbf{x}) + \\frac{1}{2} \\beta (C\\mathbf{x})^{\\top}(C\\mathbf{x})\n$$\nExpanding the first term:\n$$\n(\\mathbf{y} - A\\mathbf{x})^{\\top} W (\\mathbf{y} - A\\mathbf{x}) = (\\mathbf{y}^{\\top} - \\mathbf{x}^{\\top}A^{\\top})W(\\mathbf{y} - A\\mathbf{x}) = \\mathbf{y}^{\\top}W\\mathbf{y} - \\mathbf{y}^{\\top}WA\\mathbf{x} - \\mathbf{x}^{\\top}A^{\\top}W\\mathbf{y} + \\mathbf{x}^{\\top}A^{\\top}WA\\mathbf{x}\n$$\nSince $\\mathbf{x}^{\\top}A^{\\top}W\\mathbf{y}$ is a scalar, it is equal to its transpose, $(\\mathbf{x}^{\\top}A^{\\top}W\\mathbf{y})^{\\top} = \\mathbf{y}^{\\top}W^{\\top}A\\mathbf{x}$. As $W$ is diagonal, it is symmetric ($W=W^\\top$), so $\\mathbf{x}^{\\top}A^{\\top}W\\mathbf{y} = \\mathbf{y}^{\\top}WA\\mathbf{x}$. The term thus becomes:\n$$\n\\mathbf{y}^{\\top}W\\mathbf{y} - 2\\mathbf{y}^{\\top}WA\\mathbf{x} + \\mathbf{x}^{\\top}A^{\\top}WA\\mathbf{x}\n$$\nThe second term of $f(\\mathbf{x})$ is $\\frac{1}{2} \\beta \\mathbf{x}^{\\top}C^{\\top}C\\mathbf{x}$. Combining all parts, the objective function is:\n$$\nf(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^{\\top} (A^{\\top}WA + \\beta C^{\\top}C) \\mathbf{x} - (A^{\\top}W\\mathbf{y})^{\\top}\\mathbf{x} + \\frac{1}{2}\\mathbf{y}^{\\top}W\\mathbf{y}\n$$\nThis is a standard quadratic form $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^{\\top}H\\mathbf{x} - \\mathbf{b}^{\\top}\\mathbf{x} + \\text{constant}$, with:\n$$\nH = A^{\\top}WA + \\beta C^{\\top}C\n$$\n$$\n\\mathbf{b} = A^{\\top}W\\mathbf{y}\n$$\nThe gradient of $f(\\mathbf{x})$ with respect to $\\mathbf{x}$ is $\\nabla_{\\mathbf{x}}f(\\mathbf{x}) = H\\mathbf{x} - \\mathbf{b}$. The matrix $H$ is symmetric, since $W$ is symmetric and thus $(A^{\\top}WA)^{\\top} = A^{\\top}W^{\\top}A = A^{\\top}WA$, and $(C^{\\top}C)^{\\top} = C^{\\top}C$. The Hessian of $f(\\mathbf{x})$ is $\\nabla^2_{\\mathbf{x}}f(\\mathbf{x}) = H$. Since $W$ has non-negative entries, $A^{\\top}WA$ is positive semi-definite. Similarly, $C^{\\top}C$ is positive semi-definite. With $\\beta > 0$, their sum $H$ is also positive semi-definite. Under typical conditions where $\\ker(A) \\cap \\ker(C) = \\{\\mathbf{0}\\}$, $H$ is positive definite, making $f(\\mathbf{x})$ strictly convex and ensuring a unique minimizer.\n\nSetting the gradient to zero to find the minimum $\\mathbf{x}^\\star$:\n$$\n\\nabla_{\\mathbf{x}}f(\\mathbf{x}^\\star) = H \\mathbf{x}^\\star - \\mathbf{b} = \\mathbf{0}\n$$\nThis yields the system of linear equations known as the normal equations:\n$$\nH \\mathbf{x}^\\star = \\mathbf{b}\n$$\nThis confirms the structure provided in the problem statement.\n\n**2. Conjugate Gradient Method and Preconditioning**\n\nThe Conjugate Gradient (CG) algorithm is an iterative method ideally suited for solving large, sparse, symmetric positive-definite (SPD) linear systems like $H\\mathbf{x} = \\mathbf{b}$. A key advantage is its requirement of only a function to compute the matrix-vector product $H\\mathbf{v}$, avoiding the explicit formation and storage of the matrix $H$. This product is implemented as:\n$$\nH\\mathbf{v} = (A^{\\top}WA + \\beta C^{\\top}C)\\mathbf{v} = A^{\\top}(W(A\\mathbf{v})) + \\beta C^{\\top}(C\\mathbf{v})\n$$\nThe convergence rate of CG depends on the condition number of $H$. Preconditioning is a technique to transform the system to one with a more favorable condition number. We use a preconditioner matrix $M$ that approximates $H$ and is easily invertible. The Preconditioned Conjugate Gradient (PCG) method modifies the standard CG algorithm to incorporate $M^{-1}$.\n\nThe chosen preconditioner is the diagonal matrix $M = \\operatorname{diag}(H)$. This is a form of Jacobi preconditioning.\n- **Validity as a Preconditioner**: For $M$ to be a valid preconditioner, it must be symmetric and positive definite. $M$ is diagonal, hence symmetric. As $H$ is SPD (under mild conditions), all its diagonal entries $H_{jj} = \\mathbf{e}_j^\\top H \\mathbf{e}_j$ are positive, where $\\mathbf{e}_j$ is the $j$-th standard basis vector. Thus, $M$ is positive definite. Should any diagonal entry be zero, it is replaced by a small positive constant to maintain invertibility and positive definiteness.\n- **Approximation of $H$**: $M$ approximates $H$ by retaining its diagonal elements while discarding all off-diagonal information. This is a simple but often effective strategy, particularly if $H$ has some degree of diagonal dominance.\n- **Invertibility**: Being a diagonal matrix, $M^{-1}$ is trivial to compute; its diagonal entries are the reciprocals of the diagonal entries of $M$.\n\nThe $j$-th diagonal entry of $H = A^{\\top}WA + \\beta C^{\\top}C$ is computed as:\n$$\nM_{jj} = H_{jj} = (A^{\\top}WA)_{jj} + \\beta (C^{\\top}C)_{jj}\n$$\nThe term $(A^{\\top}WA)_{jj}$ is $\\sum_{i=1}^m (A^\\top)_{ji} (WA)_{ij} = \\sum_{i=1}^m A_{ij} \\sum_{k=1}^m W_{ik} A_{kj}$. Since $W$ is diagonal ($W_{ik}=0$ for $i \\neq k$), this simplifies to $\\sum_{i=1}^m A_{ij} W_{ii} A_{ij} = \\sum_{i=1}^m W_{ii} A_{ij}^2$.\nThe term $(C^{\\top}C)_{jj}$ is $\\sum_{k=1}^p (C^\\top)_{jk} C_{kj} = \\sum_{k=1}^p C_{kj}^2$.\nCombining these gives the specified formula:\n$$\nM_{jj} = \\sum_{i=1}^{m} W_{ii}\\,A_{ij}^2 + \\beta \\sum_{k=1}^{p} C_{kj}^2\n$$\n\nThe implementation will follow the standard PCG algorithm, where the step $M\\mathbf{z}_k = \\mathbf{r}_k$ is solved by element-wise division $\\mathbf{z}_k = \\mathbf{r}_k ./ \\operatorname{diag}(M)$.\n\nThe algorithms will be implemented with an initial guess $\\mathbf{x}_0 = \\mathbf{0}$, a relative residual stopping criterion $\\lVert \\mathbf{r}_k \\rVert_2 / \\lVert \\mathbf{r}_0 \\rVert_2 \\leq 10^{-8}$, and a maximum iteration count of $5n$.", "answer": "```python\nimport numpy as np\n\ndef H_matvec(v, A, W_diag, C, beta):\n    \"\"\"Computes the matrix-vector product H*v without forming H explicitly.\"\"\"\n    # H*v = (A.T @ W @ A + beta * C.T @ C) @ v\n    #     = A.T @ (W @ (A @ v)) + beta * C.T @ (C @ v)\n    # Since W is diagonal, W @ u is just W_diag * u\n    return A.T @ (W_diag * (A @ v)) + beta * (C.T @ (C @ v))\n\ndef compute_M_diag(A, W_diag, C, beta):\n    \"\"\"Computes the diagonal of H.\"\"\"\n    # diag(H)_j = sum_i(W_ii * A_ij^2) + beta * sum_k(C_kj^2)\n    # Vectorized computation:\n    # Part 1: diag(A.T @ W @ A) = einsum('i,ij->j', W_diag, A**2)\n    # Part 2: diag(C.T @ C) = sum(C**2, axis=0)\n    diag_H = np.einsum('i,ij->j', W_diag, A**2) + beta * np.sum(C**2, axis=0)\n    \n    # Replace zero entries with a small positive constant to ensure invertibility\n    diag_H[diag_H == 0] = 1e-12\n    return diag_H\n\ndef cg_unpreconditioned(A, W_diag, C, beta, b, tol=1e-8, max_iter=None):\n    \"\"\"Implements the unpreconditioned Conjugate Gradient algorithm.\"\"\"\n    n = b.shape[0]\n    if max_iter is None:\n        max_iter = 5 * n\n\n    x = np.zeros(n)\n    r = b - H_matvec(x, A, W_diag, C, beta)\n    p = r.copy()\n    \n    r0_norm = np.linalg.norm(r)\n    if r0_norm == 0:\n        return 0\n\n    rs_old = np.dot(r, r)\n\n    for k in range(max_iter):\n        Hp = H_matvec(p, A, W_diag, C, beta)\n        alpha = rs_old / np.dot(p, Hp)\n        \n        x = x + alpha * p\n        r = r - alpha * Hp\n        \n        r_norm = np.linalg.norm(r)\n        if r_norm / r0_norm <= tol:\n            return k + 1\n\n        rs_new = np.dot(r, r)\n        gamma = rs_new / rs_old\n        p = r + gamma * p\n        rs_old = rs_new\n        \n    return max_iter\n\ndef cg_preconditioned(A, W_diag, C, beta, b, tol=1e-8, max_iter=None):\n    \"\"\"Implements the diagonally preconditioned Conjugate Gradient algorithm.\"\"\"\n    n = b.shape[0]\n    if max_iter is None:\n        max_iter = 5 * n\n\n    M_diag = compute_M_diag(A, W_diag, C, beta)\n    \n    x = np.zeros(n)\n    r = b - H_matvec(x, A, W_diag, C, beta)\n    \n    r0_norm = np.linalg.norm(r)\n    if r0_norm == 0:\n        return 0\n\n    z = r / M_diag\n    p = z.copy()\n    rz_old = np.dot(r, z)\n\n    for k in range(max_iter):\n        Hp = H_matvec(p, A, W_diag, C, beta)\n        alpha = rz_old / np.dot(p, Hp)\n        \n        x = x + alpha * p\n        r = r - alpha * Hp\n        \n        r_norm = np.linalg.norm(r)\n        if r_norm / r0_norm <= tol:\n            return k + 1\n\n        z = r / M_diag\n        rz_new = np.dot(r, z)\n        gamma = rz_new / rz_old\n        p = z + gamma * p\n        rz_old = rz_new\n        \n    return max_iter\n\ndef setup_case(case_params):\n    \"\"\"Sets up the matrices and vectors for a given test case.\"\"\"\n    n, m, beta, seed, case_type = case_params\n    rng = np.random.default_rng(seed)\n\n    # Generate A and x_true\n    if case_type == 'ill-conditioned':\n        A = np.zeros((m, n))\n        u = rng.normal(size=m)\n        A[:, 0] = u\n        for j in range(1, n):\n            v_j = rng.normal(size=m)\n            A[:, j] = u + 1e-4 * v_j\n        x_true = rng.normal(size=n)\n    else:\n        A = rng.normal(size=(m, n))\n        x_true = rng.normal(size=n)\n\n    # Generate W\n    if case_type == 'general':\n        W_diag = rng.uniform(0.5, 2.0, size=m)\n    elif case_type == 'missing-data':\n        W_diag = rng.uniform(0.5, 2.0, size=m)\n        W_diag[[2, 3, 6, 9]] = 0.0\n    elif case_type == 'ill-conditioned':\n        W_diag = rng.uniform(0.8, 1.2, size=m)\n    elif case_type == 'strong-reg':\n        W_diag = rng.uniform(0.2, 0.5, size=m)\n    else:\n        raise ValueError(\"Unknown case type\")\n\n    # Generate C\n    if case_type == 'missing-data':\n        C = np.eye(n)\n    else: # first-order finite difference operator\n        C = np.eye(n - 1, n, k=1) - np.eye(n - 1, n)\n\n    y = A @ x_true\n    b = A.T @ (W_diag * y)\n    \n    return A, W_diag, C, beta, b\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        # n, m, beta, seed, case_type\n        (6, 12, 0.1, 1, 'general'),\n        (6, 12, 0.5, 2, 'missing-data'),\n        (6, 10, 1e-4, 3, 'ill-conditioned'),\n        (8, 12, 100.0, 4, 'strong-reg')\n    ]\n\n    results = []\n    \n    for params in test_cases:\n        n = params[0]\n        max_iter = 5 * n\n        A, W_diag, C, beta, b = setup_case(params)\n        \n        iters_unpre = cg_unpreconditioned(A, W_diag, C, beta, b, max_iter=max_iter)\n        iters_pre = cg_preconditioned(A, W_diag, C, beta, b, max_iter=max_iter)\n        \n        results.extend([iters_unpre, iters_pre])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "4900883"}, {"introduction": "While quadratic regularizers are computationally convenient, more advanced techniques use non-smooth regularizers like Total Variation (TV) to better preserve sharp edges in the reconstructed image. These objective functions are no longer differentiable everywhere, requiring a more sophisticated optimization toolkit. This exercise introduces the proximal gradient method, a powerful algorithm that elegantly handles such problems by separating the objective into its smooth and non-smooth parts, a key concept for developing state-of-the-art reconstruction algorithms.", "problem": "A transmission Computed Tomography (CT) system is modeled by line integrals through a linear system matrix $A \\in \\mathbb{R}^{m \\times n}$ acting on an unknown linear attenuation image $x \\in \\mathbb{R}^{n}$. For sufficiently high photon counts, the Poisson measurement statistics for transmitted photons can be approximated after a logarithmic transform by a Gaussian model $b \\approx A x + \\varepsilon$ with zero-mean noise $\\varepsilon$ having inverse covariance $W \\succ 0$. In a Maximum A Posteriori (MAP) formulation, the image estimate $x$ minimizes an objective of the form $F(x) = f(x) + g(x)$ where the data fidelity term $f$ is the negative log-likelihood under the Gaussian approximation and the regularization term $g$ is convex and possibly nonsmooth. Consider the case\n$$\nf(x) = \\frac{1}{2} \\left\\| W^{1/2} (A x - b) \\right\\|_{2}^{2}, \\qquad g(x) = \\lambda \\| D x \\|_{1},\n$$\nwhere $D$ is a discrete finite-difference operator implementing anisotropic total variation, and $\\lambda > 0$ is a regularization weight. The proximal gradient method applies when $f$ is differentiable with Lipschitz-continuous gradient and $g$ is convex (but not necessarily differentiable).\n\nWhich option correctly and precisely specifies both:\n- the definition of the proximal operator $\\operatorname{prox}_{\\tau g}(z)$ for step size $\\tau > 0$, and\n- the corresponding proximal gradient update for the CT MAP problem described above, including the correct gradient expression and a valid step size condition in terms of the Lipschitz constant of $\\nabla f$?\n\nChoose the single best option.\n\nA. $\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ g(u) + \\frac{1}{2 \\tau} \\| u - z \\|_{2}^{2} \\right\\}$, and with $\\nabla f(x) = A^{\\top} W (A x - b)$ the update is\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\big( x^{k} - \\tau \\, A^{\\top} W (A x^{k} - b) \\big),\n$$\nfor any $\\tau$ satisfying $0 < \\tau \\le \\frac{1}{ \\| A^{\\top} W A \\|_{2} }$.\n\nB. $\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ \\tau \\, g(u) + \\frac{1}{2} \\| u - z \\|_{2}^{2} \\right\\}$, and with $\\nabla f(x) = A^{\\top} W (A x - b)$ the update is\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\big( x^{k} + \\tau \\, A^{\\top} W (A x^{k} - b) \\big),\n$$\nfor any $\\tau$ satisfying $0 < \\tau < \\frac{2}{ \\| A^{\\top} W A \\|_{2} }$.\n\nC. The proximal operator is not needed because $g$ is differentiable; use\n$$\nx^{k+1} = x^{k} - \\tau \\big( A^{\\top} W (A x^{k} - b) + \\lambda \\, D^{\\top} D x^{k} \\big),\n$$\nwith any $\\tau$ satisfying $0 < \\tau \\le \\frac{1}{ \\| A^{\\top} W A + \\lambda D^{\\top} D \\|_{2} }$.\n\nD. $\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ g(u) + \\frac{1}{2 \\tau} \\| u - z \\|_{2}^{2} \\right\\}$, and with $\\nabla f(x) = A^{\\top} W^{-1} (A x - b)$ the update is\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\big( x^{k} - \\tau \\, A^{\\top} W^{-1} (A x^{k} - b) \\big),\n$$\nfor any $\\tau$ satisfying $0 < \\tau \\le \\frac{1}{ \\| A^{\\top} W^{-1} A \\|_{2} }$.\n\nE. $\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ g(u) + \\frac{1}{2 \\tau} \\| u - z \\|_{2}^{2} \\right\\}$, and with $\\nabla f(x) = A^{\\top} (A x - b)$ the update is\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\big( x^{k} - \\tau \\, A^{\\top} (A x^{k} - b) \\big),\n$$\nfor any $\\tau$ satisfying $0 < \\tau \\le \\frac{1}{ \\| A^{\\top} A \\|_{2} }$.", "solution": "The user wants me to validate and solve a problem concerning the proximal gradient method for a specific MAP objective function in CT image reconstruction.\n\n### Step 1: Extract Givens\n- **Objective function:** The goal is to find an image estimate $x$ that minimizes $F(x) = f(x) + g(x)$.\n- **Data fidelity term ($f$):** $f(x) = \\frac{1}{2} \\| W^{1/2} (A x - b) \\|_{2}^{2}$, where $A \\in \\mathbb{R}^{m \\times n}$ is the system matrix, $x \\in \\mathbb{R}^{n}$ is the image, $b \\in \\mathbb{R}^{m}$ represents the measurements, and $W \\succ 0$ is the inverse covariance matrix of the noise.\n- **Regularization term ($g$):** $g(x) = \\lambda \\| D x \\|_{1}$, where $D$ is a finite-difference operator and $\\lambda > 0$.\n- **Optimization method:** Proximal gradient method.\n- **Properties:** $f$ is differentiable with a Lipschitz-continuous gradient, and $g$ is convex but possibly nonsmooth.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific Groundedness:** The problem is firmly grounded in the field of computational medical imaging and inverse problems. The objective function is a standard formulation for model-based iterative reconstruction (MBIR) in CT, combining a weighted least-squares data fidelity term (a valid high-count approximation for the log-likelihood of transmission data) and a total variation (TV) regularizer. The proximal gradient method is a standard and appropriate algorithm for solving this class of optimization problems. The problem is scientifically and mathematically sound.\n2.  **Well-Posedness:** The problem is well-posed. The objective function is convex, being the sum of a convex quadratic function $f(x)$ and a convex function $g(x)$. The question asks for the correct definition and application of a standard optimization algorithm, which has a unique, correct formulation based on established theory.\n3.  **Objectivity and Clarity:** The problem is stated using precise mathematical notation and terminology. There are no ambiguities or subjective statements.\n\n**Verdict:** The problem is valid. I will proceed to the solution.\n\n### Derivation\n\nThe problem requires constructing the update step for the proximal gradient method to minimize $F(x) = f(x) + g(x)$.\n\n**1. The Proximal Gradient Method Update Rule**\nThe proximal gradient method is an iterative algorithm for minimizing composite functions of the form $f(x) + g(x)$. At each iteration $k$, it performs a gradient descent step on the smooth part $f(x)$ and then applies the proximal operator corresponding to the non-smooth part $g(x)$. The update rule is:\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\left(x^k - \\tau \\nabla f(x^k)\\right)\n$$\nwhere $\\tau > 0$ is the step size.\n\n**2. The Definition of the Proximal Operator**\nThe proximal operator, denoted as $\\operatorname{prox}_{\\tau g}(z)$, is defined as the solution to the following minimization problem:\n$$\n\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ g(u) + \\frac{1}{2 \\tau} \\| u - z \\|_{2}^{2} \\right\\}\n$$\nThis operator finds a point $u$ that is a trade-off between being close to the input point $z$ (the term $\\|u-z\\|_2^2$) and minimizing the function $g(u)$. The parameter $\\tau$ controls this trade-off.\n\n**3. The Gradient of the Data Fidelity Term, $\\nabla f(x)$**\nThe data fidelity term is $f(x) = \\frac{1}{2} \\| W^{1/2} (A x - b) \\|_{2}^{2}$. We can rewrite the squared Euclidean norm as an inner product:\n$$\nf(x) = \\frac{1}{2} \\left\\langle W^{1/2} (A x - b), W^{1/2} (A x - b) \\right\\rangle\n$$\nSince $W$ is a positive definite (and therefore symmetric) matrix, its square root $W^{1/2}$ is also symmetric, i.e., $(W^{1/2})^{\\top} = W^{1/2}$.\n$$\nf(x) = \\frac{1}{2} (A x - b)^{\\top} (W^{1/2})^{\\top} W^{1/2} (A x - b) = \\frac{1}{2} (A x - b)^{\\top} W (A x - b)\n$$\nTo find the gradient, we can use standard matrix calculus rules. For a function $h(x) = \\frac{1}{2} (Ax-b)^{\\top}Q(Ax-b)$, the gradient is $\\nabla h(x) = A^{\\top}Q(Ax-b)$, provided $Q$ is symmetric. In our case, $Q=W$, which is symmetric.\nTherefore, the gradient of $f(x)$ is:\n$$\n\\nabla f(x) = A^{\\top} W (A x - b)\n$$\n\n**4. The Step Size Condition**\nThe convergence of the proximal gradient method is guaranteed if the step size $\\tau$ is chosen appropriately. The condition depends on the Lipschitz constant, $L$, of the gradient $\\nabla f(x)$. The gradient $\\nabla f(x)$ is Lipschitz continuous with constant $L$ if $\\|\\nabla f(x_1) - \\nabla f(x_2)\\|_2 \\le L \\|x_1 - x_2\\|_2$ for all $x_1, x_2$.\nTo find $L$, we compute the Hessian matrix of $f(x)$, which is the Jacobian of $\\nabla f(x)$:\n$$\n\\nabla f(x) = A^{\\top} W A x - A^{\\top} W b\n$$\n$$\n\\nabla^2 f(x) = \\frac{\\partial}{\\partial x} (A^{\\top} W A x - A^{\\top} W b) = A^{\\top} W A\n$$\nThe Lipschitz constant $L$ is the largest eigenvalue (spectral norm) of the Hessian matrix. Since $A^{\\top}W A$ is a symmetric positive semi-definite matrix, its spectral norm is its largest eigenvalue.\n$$\nL = \\| A^{\\top} W A \\|_{2}\n$$\nThe proximal gradient method is guaranteed to converge for any step size $\\tau$ satisfying $0 < \\tau < \\frac{2}{L}$. A more restrictive condition, $0 < \\tau \\le \\frac{1}{L}$, ensures a monotonic decrease in the objective function value in each step and is also a valid condition for convergence.\n$$\n0 < \\tau \\le \\frac{1}{\\| A^{\\top} W A \\|_{2}}\n$$\n\n**Summary of Correct Components:**\n- **Proximal operator definition:** $\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ g(u) + \\frac{1}{2 \\tau} \\| u - z \\|_{2}^{2} \\right\\}$\n- **Gradient expression:** $\\nabla f(x) = A^{\\top} W (A x - b)$\n- **Update rule:** $x^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\big( x^{k} - \\tau \\, A^{\\top} W (A x^{k} - b) \\big)$\n- **Step size condition:** $0 < \\tau \\le \\frac{1}{ \\| A^{\\top} W A \\|_{2} }$ or $0 < \\tau < \\frac{2}{ \\| A^{\\top} W A \\|_{2} }$ are both valid.\n\n### Option-by-Option Analysis\n\n**A. $\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ g(u) + \\frac{1}{2 \\tau} \\| u - z \\|_{2}^{2} \\right\\}$, and with $\\nabla f(x) = A^{\\top} W (A x - b)$ the update is\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\big( x^{k} - \\tau \\, A^{\\top} W (A x^{k} - b) \\big),\n$$\nfor any $\\tau$ satisfying $0 < \\tau \\le \\frac{1}{ \\| A^{\\top} W A \\|_{2} }$.**\n\n- The definition of the proximal operator is correct.\n- The expression for the gradient $\\nabla f(x)$ is correct.\n- The update rule correctly combines the gradient descent step (note the minus sign) with the proximal operator.\n- The step size condition $0 < \\tau \\le \\frac{1}{L}$ is a valid and commonly used condition for convergence.\nAll components are correct and consistent.\n\nVerdict: **Correct**.\n\n**B. $\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ \\tau \\, g(u) + \\frac{1}{2} \\| u - z \\|_{2}^{2} \\right\\}$, and with $\\nabla f(x) = A^{\\top} W (A x - b)$ the update is\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\big( x^{k} + \\tau \\, A^{\\top} W (A x^{k} - b) \\big),\n$$\nfor any $\\tau$ satisfying $0 < \\tau < \\frac{2}{ \\| A^{\\top} W A \\|_{2} }$.**\n\n- The definition of the proximal operator uses an alternative (but also valid) notational convention, where the operator is applied to the function $\\tau g$.\n- The update rule uses a `+` sign for the gradient term, i.e., $x^k + \\tau \\nabla f(x^k)$. This is a gradient *ascent* step, which would seek to maximize $f(x)$, not minimize it. This is fundamentally incorrect for a minimization problem.\n\nVerdict: **Incorrect**.\n\n**C. The proximal operator is not needed because $g$ is differentiable; use\n$$\nx^{k+1} = x^{k} - \\tau \\big( A^{\\top} W (A x^{k} - b) + \\lambda \\, D^{\\top} D x^{k} \\big),\n$$\nwith any $\\tau$ satisfying $0 < \\tau \\le \\frac{1}{ \\| A^{\\top} W A + \\lambda D^{\\top} D \\|_{2} }$.**\n\n- The premise that $g(x) = \\lambda \\|Dx\\|_1$ is differentiable is false. The L1 norm is not differentiable when its argument is zero.\n- The term $\\lambda D^{\\top} D x^k$ is not the gradient of $g(x)$. It is the gradient of $\\frac{\\lambda}{2}\\|Dx\\|_2^2$, a different regularizer (Tikhonov regularization on the image gradient). The proposed update is for a different problem.\n\nVerdict: **Incorrect**.\n\n**D. $\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ g(u) + \\frac{1}{2 \\tau} \\| u - z \\|_{2}^{2} \\right\\}$, and with $\\nabla f(x) = A^{\\top} W^{-1} (A x - b)$ the update is\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\big( x^{k} - \\tau \\, A^{\\top} W^{-1} (A x^{k} - b) \\big),\n$$\nfor any $\\tau$ satisfying $0 < \\tau \\le \\frac{1}{ \\| A^{\\top} W^{-1} A \\|_{2} }$.**\n\n- The expression for the gradient, $\\nabla f(x) = A^{\\top} W^{-1} (A x - b)$, is incorrect. It should be $A^{\\top} W (A x - b)$. The gradient expression corresponds to a different data fidelity term, $\\frac{1}{2} (A x - b)^{\\top} W^{-1} (A x - b)$, which is not what was given in the problem statement.\n\nVerdict: **Incorrect**.\n\n**E. $\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ g(u) + \\frac{1}{2 \\tau} \\| u - z \\|_{2}^{2} \\right\\}$, and with $\\nabla f(x) = A^{\\top} (A x - b)$ the update is\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\big( x^{k} - \\tau \\, A^{\\top} (A x^{k} - b) \\big),\n$$\nfor any $\\tau$ satisfying $0 < \\tau \\le \\frac{1}{ \\| A^{\\top} A \\|_{2} }$.**\n\n- The expression for the gradient, $\\nabla f(x) = A^{\\top} (A x - b)$, is incorrect. This expression is only valid for unweighted least squares, which corresponds to setting $W=I$ (the identity matrix). The problem states $W \\succ 0$ is a general inverse covariance matrix.\n\nVerdict: **Incorrect**.\n\nBased on the detailed analysis, only option A provides a fully correct and consistent formulation of the proximal gradient method for the given CT reconstruction problem.", "answer": "$$\\boxed{A}$$", "id": "4900910"}, {"introduction": "The computational cost of processing millions of CT projection rays in every iteration can be a major bottleneck. The Ordered Subsets (OS) method is a universally adopted acceleration strategy that dramatically speeds up convergence by updating the image using only a small fraction of the data at a time. This practice challenges you to formalize the OS update and to investigate a crucial, counter-intuitive aspect of its behavior: the tendency to fall into limit cycles instead of converging to a single point. Understanding this phenomenon is vital for the effective practical use of nearly all modern clinical iterative reconstruction systems.", "problem": "You are designing an iterative image reconstruction algorithm for transmission Computed Tomography (CT) using a convex quadratic Penalized Weighted Least Squares (PWLS) objective. The system model is linear with forward projector matrix $A \\in \\mathbb{R}^{M \\times N}$, image vector $x \\in \\mathbb{R}^{N}$, sinogram data $y \\in \\mathbb{R}^{M}$, and statistical weights $W \\in \\mathbb{R}^{M \\times M}$ that are diagonal and positive definite. The regularizer is quadratic with positive semidefinite matrix $R \\in \\mathbb{R}^{N \\times N}$. The PWLS objective is\n$$\nf(x) \\triangleq \\tfrac{1}{2}\\lVert y - A x \\rVert_{W}^{2} + \\tfrac{1}{2} x^{\\mathsf{T}} R x,\n$$\nwhere $\\lVert v \\rVert_{W}^{2} \\triangleq v^{\\mathsf{T}} W v$. You consider accelerating a preconditioned gradient method by the Ordered Subsets (OS) approach. You partition the $M$ rays into $S$ disjoint subsets $\\{\\mathcal{I}_{s}\\}_{s=1}^{S}$ of approximately equal size, and define the per-subset quantities $A_{s} \\in \\mathbb{R}^{|\\mathcal{I}_{s}| \\times N}$, $W_{s} \\in \\mathbb{R}^{|\\mathcal{I}_{s}| \\times |\\mathcal{I}_{s}|}$, and $y_{s} \\in \\mathbb{R}^{|\\mathcal{I}_{s}|}$ by restricting $A$, $W$, and $y$ to rows in $\\mathcal{I}_{s}$. Let $D \\in \\mathbb{R}^{N \\times N}$ be any fixed diagonal positive definite surrogate (e.g., a majorizing diagonal) for the Hessian $H \\triangleq A^{\\mathsf{T}} W A + R$, and let $\\alpha \\!>\\! 0$ denote a fixed step size.\n\nFrom first principles, starting from the definition of $f(x)$ and basic rules of matrix calculus, reason about how OS is constructed by approximating the full-data gradient with a scaled subset gradient. Then, carefully formalize a single OS step that uses a cyclic subset index $s_{k} \\in \\{1,\\dots,S\\}$ at iteration $k$ (with a typical cyclic schedule $s_{k} = ((k \\bmod S) + 1)$), and explain under what mechanism OS may fail to converge exactly and instead exhibit a limit cycle, even though $f(x)$ is convex quadratic.\n\nSelect the option that correctly specifies:\n- a mathematically consistent OS update for this PWLS setting that follows from the subset-splitting construction and an appropriate scaling, and\n- a correct reasoning for why OS can introduce limit cycles for fixed $\\alpha$ and $S \\ge 2$.\n\nA. The rays are partitioned into $S$ disjoint subsets. With $D \\succ 0$ diagonal and fixed, an OS step with subset $s_{k}$ is\n$$\nx^{k+1} = x^{k} - \\alpha D^{-1} \\Big( R x^{k} + S\\, A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} \\big( A_{s_{k}} x^{k} - y_{s_{k}} \\big) \\Big).\n$$\nThis uses the subset gradient of the data term scaled by $S$ (so that the average over subsets matches the full-data gradient) and includes the full regularizer gradient at every step. OS can create limit cycles because the per-iteration gradient is a deterministic, cyclic approximation of the full gradient; the iteration applies a product of subset-dependent linear maps that need not be a contraction for a fixed $\\alpha$, so the iterates can orbit around the true minimizer without settling, unless $\\alpha$ is diminished or other damping is used.\n\nB. The rays are partitioned into $S$ subsets. With $D \\succ 0$, the OS step is\n$$\nx^{k+1} = x^{k} - \\alpha D^{-1} \\Big( S\\, R x^{k} + S\\, A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} \\big( A_{s_{k}} x^{k} - y_{s_{k}} \\big) \\Big).\n$$\nScaling both the data and regularization gradients by $S$ preserves the expected gradient. Since $f(x)$ is convex quadratic, OS with any fixed $\\alpha \\in (0,2)$ guarantees monotone decrease of $f(x)$ and global convergence without cycles.\n\nC. The rays are partitioned into $S$ subsets. With $D \\succ 0$, the OS step is\n$$\nx^{k+1} = x^{k} - \\alpha D^{-1} \\Big( R x^{k} + A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} \\big( A_{s_{k}} x^{k} - y_{s_{k}} \\big) \\Big).\n$$\nNo scaling by $S$ is needed because the subset gradient already approximates the full gradient. Since the Hessian $H$ is constant, OS cannot produce limit cycles for any fixed $\\alpha$.\n\nD. The rays are partitioned into $S$ subsets. At iteration $k$, solve exactly the normal equations for subset $s_{k}$,\n$$\n\\big( A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} A_{s_{k}} + R \\big) x^{k+1} = A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} y_{s_{k}},\n$$\nand cycle through subsets. Because each subset is solved exactly, this sequentially solves the full problem in $S$ iterations and cannot exhibit limit cycles.\n\nE. The rays are partitioned into $S$ subsets. With $D \\succ 0$, an OS step is\n$$\nx^{k+1} = x^{k} - \\alpha D^{-1} \\Big( R x^{k} + S\\, A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} \\big( A_{s_{k}} x^{k} - y_{s_{k}} \\big) \\Big).\n$$\nOS may show limit cycles only when the penalty is nonquadratic; for convex quadratic $f(x)$ the constant Hessian makes cycles impossible for any fixed $\\alpha$.", "solution": "The user has provided a problem concerning the formulation and convergence properties of the Ordered Subsets (OS) method for a Penalized Weighted Least Squares (PWLS) objective function in Computed Tomography (CT).\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Objective function**: $f(x) \\triangleq \\tfrac{1}{2}\\lVert y - A x \\rVert_{W}^{2} + \\tfrac{1}{2} x^{\\mathsf{T}} R x$.\n- **Vector/Matrix definitions**:\n    - $x \\in \\mathbb{R}^{N}$ is the image vector.\n    - $y \\in \\mathbb{R}^{M}$ is the sinogram data.\n    - $A \\in \\mathbb{R}^{M \\times N}$ is the forward projector matrix.\n    - $W \\in \\mathbb{R}^{M \\times M}$ is a diagonal, positive definite matrix of statistical weights.\n    - $R \\in \\mathbb{R}^{N \\times N}$ is a positive semidefinite regularization matrix.\n- **Norm definition**: $\\lVert v \\rVert_{W}^{2} \\triangleq v^{\\mathsf{T}} W v$.\n- **Ordered Subsets (OS) partitioning**:\n    - The $M$ rays are partitioned into $S$ disjoint subsets $\\{\\mathcal{I}_{s}\\}_{s=1}^{S}$.\n    - Per-subset quantities $A_{s}$, $W_{s}$, and $y_{s}$ are defined by restricting rows of $A$, $W$, and $y$ to the index set $\\mathcal{I}_{s}$.\n- **Iterative method parameters**:\n    - $D \\in \\mathbb{R}^{N \\times N}$ is a fixed, diagonal, positive definite surrogate for the Hessian $H \\triangleq A^{\\mathsf{T}} W A + R$.\n    - $\\alpha > 0$ is a fixed step size.\n    - The subset index $s_{k}$ is cyclic, e.g., $s_{k} = ((k \\bmod S) + 1)$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is formulated within the standard mathematical framework of statistical iterative reconstruction for CT. PWLS is a widely used objective function, and Ordered Subsets is a canonical acceleration technique. All concepts are well-established in the field of medical imaging and numerical optimization. The problem is scientifically sound.\n- **Well-Posed**: The problem asks for the derivation of a standard algorithm and an explanation of its known convergence behavior. The PWLS objective function is convex, as it is the sum of two convex functions (a quadratic data-fidelity term and a quadratic regularization term). The problem is clearly stated and has a definite, non-ambiguous solution based on established theory.\n- **Objective**: The language is technical, precise, and free of subjective content.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is a well-posed, scientifically grounded problem in medical image reconstruction. I will proceed with the derivation and solution.\n\n### Derivation and Analysis\n\n**1. Gradient of the Objective Function**\n\nThe PWLS objective function is given by:\n$$\nf(x) = \\tfrac{1}{2}\\lVert y - A x \\rVert_{W}^{2} + \\tfrac{1}{2} x^{\\mathsf{T}} R x = \\tfrac{1}{2}(y - Ax)^{\\mathsf{T}} W (y - Ax) + \\tfrac{1}{2}x^{\\mathsf{T}} R x\n$$\nTo find the gradient, we use standard rules of matrix calculus.\n$$\n\\nabla f(x) = \\frac{d}{dx} \\left( \\tfrac{1}{2}(y^{\\mathsf{T}}Wy - 2x^{\\mathsf{T}}A^{\\mathsf{T}}Wy + x^{\\mathsf{T}}A^{\\mathsf{T}}WAx) + \\tfrac{1}{2}x^{\\mathsf{T}} R x \\right)\n$$\n$$\n\\nabla f(x) = -A^{\\mathsf{T}}Wy + A^{\\mathsf{T}}WAx + Rx\n$$\n$$\n\\nabla f(x) = A^{\\mathsf{T}}W(Ax - y) + Rx\n$$\nA standard preconditioned gradient descent iteration would be:\n$$\nx^{k+1} = x^{k} - \\alpha D^{-1} \\nabla f(x^k) = x^{k} - \\alpha D^{-1} \\Big( A^{\\mathsf{T}}W(Ax^k - y) + Rx^k \\Big)\n$$\nThis update uses the entire dataset $(A, y, W)$ at each iteration, which can be computationally prohibitive.\n\n**2. Construction of the Ordered Subsets (OS) Update**\n\nThe OS method accelerates the process by using only a subset of the data at each iteration. The full data-fidelity part of the gradient, $A^{\\mathsf{T}}W(Ax - y)$, can be expressed as a sum over the disjoint subsets:\n$$\nA^{\\mathsf{T}}W(Ax - y) = \\sum_{s=1}^{S} A_{s}^{\\mathsf{T}} W_{s} (A_{s} x - y_{s})\n$$\nThe core idea of OS is to approximate this full sum by a single term from the sum, scaled up to have a similar magnitude. Assuming the subsets are chosen to be of roughly equal size and contribution, a scaling factor of $S$ is used.\n$$\n\\sum_{s=1}^{S} A_{s}^{\\mathsf{T}} W_{s} (A_{s} x - y_{s}) \\approx S \\cdot A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} (A_{s_{k}} x - y_{s_{k}})\n$$\nfor the sub-iteration $k$ that uses subset $s_k$.\n\nThe regularization term's gradient, $Rx$, does not depend on the data subsets. It is typically computed in its entirety at each sub-iteration because it is often computationally less expensive than the data term gradient. Therefore, the OS approximation of the full gradient $\\nabla f(x^k)$ is:\n$$\n\\nabla_{\\text{OS}} f(x^k) \\triangleq S\\, A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} (A_{s_{k}} x^{k} - y_{s_{k}}) + R x^{k}\n$$\nSubstituting this approximate gradient into the preconditioned gradient descent formula gives the OS update rule:\n$$\nx^{k+1} = x^{k} - \\alpha D^{-1} \\Big( S\\, A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} (A_{s_{k}} x^{k} - y_{s_{k}}) + R x^{k} \\Big)\n$$\n\n**3. Analysis of OS Convergence and Limit Cycles**\n\nAn iterative algorithm converges to a point $x^*$ if $x^*$ is a fixed point of the iteration map. The global minimizer $x^*$ of $f(x)$ is defined by the condition $\\nabla f(x^*) = 0$:\n$$\n\\nabla f(x^*) = \\sum_{s=1}^{S} A_{s}^{\\mathsf{T}} W_{s} (A_{s} x^* - y_{s}) + Rx^* = 0\n$$\nNow, let's check if this minimizer $x^*$ is a fixed point of the OS update for a given subset $s_k$. For $x^*$ to be a fixed point, the update term must be zero, i.e., $\\nabla_{\\text{OS}}f(x^*) = 0$.\n$$\n\\nabla_{\\text{OS}} f(x^*) = S\\, A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} (A_{s_{k}} x^* - y_{s_{k}}) + R x^* \\stackrel{?}{=} 0\n$$\nIn general, this condition is not met. The true minimizer $x^*$ only guarantees that the *sum* of the subset gradient components (plus the regularization gradient) is zero. It does not guarantee that each individual (scaled) subset gradient component (plus regularization) is zero. This happens only in the \"consistent data\" case where $A_s^\\mathsf{T} W_s (A_s x^* - y_s)$ is the same for all $s$, which is not true for realistic (noisy) CT data.\n\nBecause $x^*$ is not a fixed point for each sub-iteration map $x \\mapsto x - \\alpha D^{-1} \\nabla_{\\text{OS}} f(x)$, the iterate $x^k$ will be \"pulled\" away from the true solution at each step. As the algorithm cycles through the subsets $s=1, \\dots, S$, the direction of this pull also cycles. The iteration is a composition of affine maps, one for each subset. For a fixed step size $\\alpha > 0$ and $S \\ge 2$, this sequence of deterministic, biased updates prevents the iterates from settling at the true minimizer $x^*$. Instead, they often converge to a stable orbit, or \"limit cycle,\" around $x^*$.\n\nThe fact that the objective function $f(x)$ is convex and quadratic (and thus has a constant Hessian $H = A^{\\mathsf{T}} W A + R$) is irrelevant to this phenomenon. The limit cycle is a consequence of the iterative procedure itself, which is not a true gradient descent on $f(x)$, but a sequence of steps that are individually descents on different, auxiliary objective functions. Convergence to the true minimizer for OS methods typically requires a diminishing step size, $\\alpha_k \\to 0$.\n\n### Option-by-Option Analysis\n\n**A. The rays are partitioned into $S$ disjoint subsets. With $D \\succ 0$ diagonal and fixed, an OS step with subset $s_{k}$ is...**\n- **Update Rule**: $x^{k+1} = x^{k} - \\alpha D^{-1} \\Big( R x^{k} + S\\, A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} \\big( A_{s_{k}} x^{k} - y_{s_{k}} \\big) \\Big)$. This matches the correctly derived OS-PWLS update rule.\n- **Reasoning**: It correctly states that OS uses a scaled, cyclic, deterministic approximation of the gradient. It correctly identifies that the composition of subset-dependent maps may not be a contraction and can lead to orbits around the minimizer. It correctly mentions that remedies like diminishing step size are needed for exact convergence. This is a complete and accurate explanation.\n- **Verdict**: **Correct**.\n\n**B. The rays are partitioned into $S$ subsets. With $D \\succ 0$, the OS step is...**\n- **Update Rule**: $x^{k+1} = x^{k} - \\alpha D^{-1} \\Big( S\\, R x^{k} + S\\, A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} \\big( A_{s_{k}} x^{k} - y_{s_{k}} \\big) \\Big)$. This update incorrectly scales the regularization gradient $Rx^k$ by $S$. This would overtune the regularization by a factor of $S$ over one epoch.\n- **Reasoning**: It claims that for a convex quadratic function, OS guarantees global convergence without cycles for a fixed step size. This is fundamentally incorrect, as explained in the derivation above. Limit cycles are a hallmark of OS with fixed step sizes.\n- **Verdict**: **Incorrect**.\n\n**C. The rays are partitioned into $S$ subsets. With $D \\succ 0$, the OS step is...**\n- **Update Rule**: $x^{k+1} = x^{k} - \\alpha D^{-1} \\Big( R x^{k} + A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} \\big( A_{s_{k}} x^{k} - y_{s_{k}} \\big) \\Big)$. This update fails to scale the data-fidelity subset gradient by $S$. The resulting gradient approximation is too small, leading to very slow convergence. This is not the standard OS algorithm.\n- **Reasoning**: It claims that the constant Hessian prevents limit cycles. This is false. The source of the limit cycle is the inconsistent gradient approximation, not any property of the Hessian or non-linearity of the objective.\n- **Verdict**: **Incorrect**.\n\n**D. The rays are partitioned into $S$ subsets. At iteration $k$, solve exactly the normal equations for subset $s_{k}$...**\n- **Update Rule**: This describes a block-iterative method where each step completely replaces the solution, i.e., $x^{k+1}$ is the exact minimizer of $\\tfrac{1}{2}\\lVert y_{s_k} - A_{s_k} x \\rVert_{W_{s_k}}^{2} + \\tfrac{1}{2} x^{\\mathsf{T}} R x$, but this does not build upon $x^k$. This is a different class of algorithm, not a gradient-based OS method derived from $f(x)$.\n- **Reasoning**: It claims this procedure cannot exhibit limit cycles. This is incorrect. Cycling through exact solutions to different subproblems is a classic recipe for limit-cycle behavior, as the solution will jump between the different minima of the subproblems, orbiting the global minimum.\n- **Verdict**: **Incorrect**.\n\n**E. The rays are partitioned into $S$ subsets. With $D \\succ 0$, an OS step is...**\n- **Update Rule**: $x^{k+1} = x^{k} - \\alpha D^{-1} \\Big( R x^{k} + S\\, A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} \\big( A_{s_{k}} x^{k} - y_{s_{k}} \\big) \\Big)$. The update rule itself is stated correctly, identical to option A.\n- **Reasoning**: It claims limit cycles only occur for non-quadratic penalties and are impossible for convex quadratic $f(x)$ due to the constant Hessian. This reasoning is identical to the flawed logic in option C and is fundamentally incorrect. The convexity or quadratic nature of the objective does not prevent OS limit cycles.\n- **Verdict**: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "4900906"}]}