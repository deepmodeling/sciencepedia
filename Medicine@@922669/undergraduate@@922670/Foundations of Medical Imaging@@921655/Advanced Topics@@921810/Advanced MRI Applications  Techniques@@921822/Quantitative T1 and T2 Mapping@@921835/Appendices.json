{"hands_on_practices": [{"introduction": "Before quantitative maps can be trusted in clinical or research settings, the accuracy of the MRI sequence must be verified. This is often done using calibration phantoms containing solutions with known relaxation properties. This exercise [@problem_id:4914970] explores this principle by asking you to calculate the expected $T_1$ and $T_2$ values in phantoms containing paramagnetic salts. By comparing these theoretical values to hypothetical measurements, you will practice assessing the calibration error, a critical step in quality assurance for quantitative MRI.", "problem": "A quantitative Magnetic Resonance Imaging (qMRI) calibration phantom contains two aqueous vials at temperature $25$ $\\mathrm{^{\\circ}C}$ and magnetic field strength $3$ $\\mathrm{T}$: one with nickel(II) chloride hexahydrate $\\mathrm{NiCl_2\\cdot 6H_2O}$ at concentration $10$ $\\mathrm{mM}$ and one with manganese(II) chloride tetrahydrate $\\mathrm{MnCl_2\\cdot 4H_2O}$ at concentration $0.5$ $\\mathrm{mM}$. The solvent matrix (without paramagnetic solute) has baseline longitudinal relaxation time $T_{1,0} = 3000$ $\\mathrm{ms}$ and baseline transverse relaxation time $T_{2,0} = 2000$ $\\mathrm{ms}$ at these conditions. At $3$ $\\mathrm{T}$, the relaxivity coefficients for these solutes are known and stable: for $\\mathrm{NiCl_2}$, longitudinal relaxivity $r_{1,\\mathrm{Ni}} = 0.50$ $\\mathrm{s^{-1}\\ mM^{-1}}$ and transverse relaxivity $r_{2,\\mathrm{Ni}} = 0.70$ $\\mathrm{s^{-1}\\ mM^{-1}}$; for $\\mathrm{MnCl_2}$, longitudinal relaxivity $r_{1,\\mathrm{Mn}} = 6.0$ $\\mathrm{s^{-1}\\ mM^{-1}}$ and transverse relaxivity $r_{2,\\mathrm{Mn}} = 40$ $\\mathrm{s^{-1}\\ mM^{-1}}$. A qMRI experiment yields measured relaxation times for the two vials: $\\mathrm{NiCl_2}$ vial has measured $T_{1,\\mathrm{meas}} = 190$ $\\mathrm{ms}$ and $T_{2,\\mathrm{meas}} = 135$ $\\mathrm{ms}$; $\\mathrm{MnCl_2}$ vial has measured $T_{1,\\mathrm{meas}} = 310$ $\\mathrm{ms}$ and $T_{2,\\mathrm{meas}} = 46$ $\\mathrm{ms}$.\n\nStarting from the core definitions that the longitudinal relaxation rate $R_1$ and transverse relaxation rate $R_2$ satisfy $R_1 = 1/T_1$ and $R_2 = 1/T_2$, and that in dilute paramagnetic solutions relaxation rates are additive with a term that depends linearly on solute concentration through the corresponding relaxivity coefficient, derive expressions for the expected $T_1$ and $T_2$ for each vial. Then compute the expected $T_1$ and $T_2$ in milliseconds for both $\\mathrm{NiCl_2}$ and $\\mathrm{MnCl_2}$.\n\nDefine the fractional calibration error for each metric as $\\varepsilon = (T_{\\mathrm{meas}} - T_{\\mathrm{exp}})/T_{\\mathrm{exp}}$. Compute four fractional errors: $\\varepsilon_{T_1,\\mathrm{Ni}}$, $\\varepsilon_{T_2,\\mathrm{Ni}}$, $\\varepsilon_{T_1,\\mathrm{Mn}}$, and $\\varepsilon_{T_2,\\mathrm{Mn}}$. Express your final four values as decimals without units, rounded to four significant figures, and present them in a single row matrix in the order $\\left[\\varepsilon_{T_1,\\mathrm{Ni}}, \\varepsilon_{T_2,\\mathrm{Ni}}, \\varepsilon_{T_1,\\mathrm{Mn}}, \\varepsilon_{T_2,\\mathrm{Mn}}\\right]$.", "solution": "The problem is subjected to validation against the established criteria.\n\n### Step 1: Extract Givens\n-   Temperature: $25$ $\\mathrm{^{\\circ}C}$\n-   Magnetic field strength: $3$ $\\mathrm{T}$\n-   Solvent baseline longitudinal relaxation time: $T_{1,0} = 3000$ $\\mathrm{ms}$\n-   Solvent baseline transverse relaxation time: $T_{2,0} = 2000$ $\\mathrm{ms}$\n-   Vial 1: Nickel(II) chloride hexahydrate, $\\mathrm{NiCl_2\\cdot 6H_2O}$\n-   Concentration of $\\mathrm{NiCl_2}$: $C_{\\mathrm{Ni}} = 10$ $\\mathrm{mM}$\n-   Longitudinal relaxivity of $\\mathrm{NiCl_2}$: $r_{1,\\mathrm{Ni}} = 0.50$ $\\mathrm{s^{-1}\\ mM^{-1}}$\n-   Transverse relaxivity of $\\mathrm{NiCl_2}$: $r_{2,\\mathrm{Ni}} = 0.70$ $\\mathrm{s^{-1}\\ mM^{-1}}$\n-   Vial 2: Manganese(II) chloride tetrahydrate, $\\mathrm{MnCl_2\\cdot 4H_2O}$\n-   Concentration of $\\mathrm{MnCl_2}$: $C_{\\mathrm{Mn}} = 0.5$ $\\mathrm{mM}$\n-   Longitudinal relaxivity of $\\mathrm{MnCl_2}$: $r_{1,\\mathrm{Mn}} = 6.0$ $\\mathrm{s^{-1}\\ mM^{-1}}$\n-   Transverse relaxivity of $\\mathrm{MnCl_2}$: $r_{2,\\mathrm{Mn}} = 40$ $\\mathrm{s^{-1}\\ mM^{-1}}$\n-   Measured values for $\\mathrm{NiCl_2}$ vial: $T_{1,\\mathrm{meas,Ni}} = 190$ $\\mathrm{ms}$, $T_{2,\\mathrm{meas,Ni}} = 135$ $\\mathrm{ms}$\n-   Measured values for $\\mathrm{MnCl_2}$ vial: $T_{1,\\mathrm{meas,Mn}} = 310$ $\\mathrm{ms}$, $T_{2,\\mathrm{meas,Mn}} = 46$ $\\mathrm{ms}$\n-   Fundamental definitions: $R_1 = 1/T_1$, $R_2 = 1/T_2$\n-   Additive model for relaxation rates: $R_i = R_{i,0} + r_i C$\n-   Definition of fractional error: $\\varepsilon = (T_{\\mathrm{meas}} - T_{\\mathrm{exp}})/T_{\\mathrm{exp}}$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the established principles of nuclear magnetic resonance (NMR) relaxation enhancement by paramagnetic ions. The given model, $R_i = R_{i,0} + r_i C$, is the standard linear approximation for dilute solutions. The relaxivity values, concentrations, and field strength are physically realistic and commonly encountered in quantitative MRI. The problem is well-posed, providing all necessary data and definitions for a unique solution. The language is objective and precise. The problem is free of scientific unsoundness, incompleteness, and ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\nThe fundamental principle for the relaxation rate in a dilute paramagnetic solution is the additivity of rates. The total relaxation rate, $R_i$, is the sum of the baseline relaxation rate of the solvent, $R_{i,0}$, and the contribution from the paramagnetic solute, which is the product of its relaxivity, $r_i$, and its concentration, $C$. The index $i$ is $1$ for longitudinal relaxation and $2$ for transverse relaxation.\n$$R_i = R_{i,0} + r_i C$$\nGiven the definition that the relaxation rate is the reciprocal of the relaxation time, $R_i = 1/T_i$, we can write:\n$$\\frac{1}{T_{i,\\mathrm{exp}}} = \\frac{1}{T_{i,0}} + r_i C$$\nTo derive the expression for the expected relaxation time, $T_{i,\\mathrm{exp}}$, we solve for it from the above equation:\n$$T_{i,\\mathrm{exp}} = \\left(\\frac{1}{T_{i,0}} + r_i C\\right)^{-1} = \\frac{1}{\\frac{1 + r_i C T_{i,0}}{T_{i,0}}} = \\frac{T_{i,0}}{1 + r_i C T_{i,0}}$$\nThis is the general expression for the expected relaxation time.\n\nFor calculation, it is critical to ensure dimensional consistency. The given baseline times $T_{i,0}$ are in milliseconds ($\\mathrm{ms}$), while the relaxivity coefficients $r_i$ have units of $\\mathrm{s^{-1}\\ mM^{-1}}$. We must convert all time units to a consistent base, which is seconds ($\\mathrm{s}$).\n$T_{1,0} = 3000 \\ \\mathrm{ms} = 3.0 \\ \\mathrm{s}$\n$T_{2,0} = 2000 \\ \\mathrm{ms} = 2.0 \\ \\mathrm{s}$\nThe baseline relaxation rates are therefore:\n$R_{1,0} = \\frac{1}{T_{1,0}} = \\frac{1}{3.0 \\ \\mathrm{s}} = \\frac{1}{3} \\ \\mathrm{s^{-1}}$\n$R_{2,0} = \\frac{1}{T_{2,0}} = \\frac{1}{2.0 \\ \\mathrm{s}} = 0.5 \\ \\mathrm{s^{-1}}$\n\nWe now compute the expected relaxation times for each vial.\n\nFor the $\\mathrm{NiCl_2}$ vial, with $C_{\\mathrm{Ni}} = 10 \\ \\mathrm{mM}$:\nThe expected longitudinal relaxation rate $R_{1,\\mathrm{exp,Ni}}$ is:\n$$R_{1,\\mathrm{exp,Ni}} = R_{1,0} + r_{1,\\mathrm{Ni}} C_{\\mathrm{Ni}} = \\frac{1}{3} \\ \\mathrm{s^{-1}} + (0.50 \\ \\mathrm{s^{-1}\\ mM^{-1}})(10 \\ \\mathrm{mM}) = \\left(\\frac{1}{3} + 5.0\\right) \\ \\mathrm{s^{-1}} = \\frac{16}{3} \\ \\mathrm{s^{-1}}$$\nThe expected longitudinal relaxation time $T_{1,\\mathrm{exp,Ni}}$ is:\n$$T_{1,\\mathrm{exp,Ni}} = \\frac{1}{R_{1,\\mathrm{exp,Ni}}} = \\frac{3}{16} \\ \\mathrm{s} = 0.1875 \\ \\mathrm{s} = 187.5 \\ \\mathrm{ms}$$\nThe expected transverse relaxation rate $R_{2,\\mathrm{exp,Ni}}$ is:\n$$R_{2,\\mathrm{exp,Ni}} = R_{2,0} + r_{2,\\mathrm{Ni}} C_{\\mathrm{Ni}} = 0.5 \\ \\mathrm{s^{-1}} + (0.70 \\ \\mathrm{s^{-1}\\ mM^{-1}})(10 \\ \\mathrm{mM}) = (0.5 + 7.0) \\ \\mathrm{s^{-1}} = 7.5 \\ \\mathrm{s^{-1}}$$\nThe expected transverse relaxation time $T_{2,\\mathrm{exp,Ni}}$ is:\n$$T_{2,\\mathrm{exp,Ni}} = \\frac{1}{R_{2,\\mathrm{exp,Ni}}} = \\frac{1}{7.5} \\ \\mathrm{s} = \\frac{2}{15} \\ \\mathrm{s} \\approx 0.1333... \\ \\mathrm{s} = 133.33... \\ \\mathrm{ms}$$\n\nFor the $\\mathrm{MnCl_2}$ vial, with $C_{\\mathrm{Mn}} = 0.5 \\ \\mathrm{mM}$:\nThe expected longitudinal relaxation rate $R_{1,\\mathrm{exp,Mn}}$ is:\n$$R_{1,\\mathrm{exp,Mn}} = R_{1,0} + r_{1,\\mathrm{Mn}} C_{\\mathrm{Mn}} = \\frac{1}{3} \\ \\mathrm{s^{-1}} + (6.0 \\ \\mathrm{s^{-1}\\ mM^{-1}})(0.5 \\ \\mathrm{mM}) = \\left(\\frac{1}{3} + 3.0\\right) \\ \\mathrm{s^{-1}} = \\frac{10}{3} \\ \\mathrm{s^{-1}}$$\nThe expected longitudinal relaxation time $T_{1,\\mathrm{exp,Mn}}$ is:\n$$T_{1,\\mathrm{exp,Mn}} = \\frac{1}{R_{1,\\mathrm{exp,Mn}}} = \\frac{3}{10} \\ \\mathrm{s} = 0.3 \\ \\mathrm{s} = 300 \\ \\mathrm{ms}$$\nThe expected transverse relaxation rate $R_{2,\\mathrm{exp,Mn}}$ is:\n$$R_{2,\\mathrm{exp,Mn}} = R_{2,0} + r_{2,\\mathrm{Mn}} C_{\\mathrm{Mn}} = 0.5 \\ \\mathrm{s^{-1}} + (40 \\ \\mathrm{s^{-1}\\ mM^{-1}})(0.5 \\ \\mathrm{mM}) = (0.5 + 20.0) \\ \\mathrm{s^{-1}} = 20.5 \\ \\mathrm{s^{-1}}$$\nThe expected transverse relaxation time $T_{2,\\mathrm{exp,Mn}}$ is:\n$$T_{2,\\mathrm{exp,Mn}} = \\frac{1}{R_{2,\\mathrm{exp,Mn}}} = \\frac{1}{20.5} \\ \\mathrm{s} = \\frac{2}{41} \\ \\mathrm{s} \\approx 0.04878... \\ \\mathrm{s} = 48.78... \\ \\mathrm{ms}$$\n\nNext, we compute the fractional calibration error, $\\varepsilon = (T_{\\mathrm{meas}} - T_{\\mathrm{exp}})/T_{\\mathrm{exp}}$, for each of the four metrics.\n\nFor the $\\mathrm{NiCl_2}$ vial:\n$T_{1,\\mathrm{meas,Ni}} = 190 \\ \\mathrm{ms}$ and $T_{1,\\mathrm{exp,Ni}} = 187.5 \\ \\mathrm{ms}$\n$$\\varepsilon_{T_1,\\mathrm{Ni}} = \\frac{190 - 187.5}{187.5} = \\frac{2.5}{187.5} = \\frac{1}{75} = 0.01333...$$\n$T_{2,\\mathrm{meas,Ni}} = 135 \\ \\mathrm{ms}$ and $T_{2,\\mathrm{exp,Ni}} = \\frac{400}{3} \\ \\mathrm{ms} \\approx 133.33... \\ \\mathrm{ms}$\n$$\\varepsilon_{T_2,\\mathrm{Ni}} = \\frac{135 - \\frac{400}{3}}{\\frac{400}{3}} = \\frac{\\frac{405 - 400}{3}}{\\frac{400}{3}} = \\frac{5}{400} = \\frac{1}{80} = 0.0125$$\nFor the $\\mathrm{MnCl_2}$ vial:\n$T_{1,\\mathrm{meas,Mn}} = 310 \\ \\mathrm{ms}$ and $T_{1,\\mathrm{exp,Mn}} = 300 \\ \\mathrm{ms}$\n$$\\varepsilon_{T_1,\\mathrm{Mn}} = \\frac{310 - 300}{300} = \\frac{10}{300} = \\frac{1}{30} = 0.03333...$$\n$T_{2,\\mathrm{meas,Mn}} = 46 \\ \\mathrm{ms}$ and $T_{2,\\mathrm{exp,Mn}} = \\frac{2000}{41} \\ \\mathrm{ms} \\approx 48.78... \\ \\mathrm{ms}$\n$$\\varepsilon_{T_2,\\mathrm{Mn}} = \\frac{46 - \\frac{2000}{41}}{\\frac{2000}{41}} = \\frac{\\frac{46 \\times 41 - 2000}{41}}{\\frac{2000}{41}} = \\frac{1886 - 2000}{2000} = \\frac{-114}{2000} = -0.057$$\nThe problem requires these values rounded to four significant figures.\n$\\varepsilon_{T_1,\\mathrm{Ni}} \\approx 0.01333$\n$\\varepsilon_{T_2,\\mathrm{Ni}} = 0.01250$ (padding with a zero to show four significant figures)\n$\\varepsilon_{T_1,\\mathrm{Mn}} \\approx 0.03333$\n$\\varepsilon_{T_2,\\mathrm{Mn}} = -0.05700$ (padding with zeros to show four significant figures)\n\nThe final answer is a row matrix containing these four values in the specified order.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.01333  0.01250  0.03333  -0.05700 \\end{pmatrix}}\n$$", "id": "4914970"}, {"introduction": "Extracting a quantitative parameter like $T_1$ from MRI data requires fitting a mathematical model of the signal evolution to the measurements. This exercise [@problem_id:4914947] delves into the theoretical foundations of this process for a standard Inversion Recovery (IR) experiment. You will derive the signal model from the Bloch equations and formulate the components of a nonlinear least squares estimator, including the Jacobian matrix, which is the cornerstone of iterative fitting algorithms.", "problem": "A single-compartment tissue is imaged using an Inversion Recovery (IR) experiment for quantitative longitudinal relaxation time mapping. In IR, a non-selective inversion pulse is applied at time $t=0$, followed by a delay known as the inversion time $TI$, and a low-flip-angle readout that is phase-sensitive so that the measured signal preserves the sign of the longitudinal magnetization. Assume the longitudinal magnetization $M_{z}(t)$ obeys the Bloch recovery equation with a constant equilibrium magnetization $M_{0}$ and a longitudinal relaxation time $T_{1}$, namely $dM_{z}/dt = (M_{0} - M_{z})/T_{1}$. Let the inversion pulse have efficiency parameter $\\eta \\in [0,1]$ such that the post-inversion initial condition is $M_{z}(0^{+}) = -\\eta\\,M_{0}$.\n\nUnder the assumptions above, the readout produces a signal that is proportional to the longitudinal magnetization $M_{z}(TI)$ at the time of readout, and this proportionality is fully captured by the parameter $M_{0}$ (which bundles proton density and receive chain gain under the small-tip-angle approximation). You are given $N$ measurements $\\{(TI_{i}, S_{i})\\}_{i=1}^{N}$ obtained from the IR experiment at inversion times $\\{TI_{i}\\}$ with measured signals $\\{S_{i}\\}$.\n\nStarting from the Bloch differential equation and the given initial condition, derive the forward signal model $s(TI; M_{0}, T_{1}, \\eta)$ and use it to formulate the nonlinear least squares estimator for the parameter vector $(M_{0}, T_{1}, \\eta)$. Then, define the residuals $r_{i} = S_{i} - s(TI_{i}; M_{0}, T_{1}, \\eta)$ and write the Jacobian row with respect to $(M_{0}, T_{1}, \\eta)$ for a general index $i$ that would be used in an iterative optimizer such as Gauss–Newton or Levenberg–Marquardt.\n\nExpress your final answer as a single analytic expression. No numerical evaluation or rounding is required. Do not include units in your final answer. Clearly present the estimator and the Jacobian row as separate components of a single composite expression.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, and objective. All necessary information is provided to derive the requested expressions.\n\nThe derivation proceeds in four sequential steps:\n1.  Solve the Bloch recovery differential equation to find the expression for longitudinal magnetization $M_{z}(t)$.\n2.  Establish the forward signal model $s(TI; M_{0}, T_{1}, \\eta)$ based on $M_{z}(t)$.\n3.  Formulate the nonlinear least squares (NLLS) estimator for the parameters $(M_{0}, T_{1}, \\eta)$.\n4.  Derive the Jacobian row of the residuals with respect to the parameters $(M_{0}, T_{1}, \\eta)$.\n\nStep 1: Derivation of the longitudinal magnetization $M_{z}(t)$\nThe problem provides the Bloch recovery equation for longitudinal magnetization $M_{z}$:\n$$\n\\frac{dM_{z}}{dt} = \\frac{M_{0} - M_{z}}{T_{1}}\n$$\nThis is a first-order linear ordinary differential equation. We can rearrange it into the standard form:\n$$\n\\frac{dM_{z}}{dt} + \\frac{1}{T_{1}}M_{z} = \\frac{M_{0}}{T_{1}}\n$$\nThe general solution to an equation of the form $\\frac{dy}{dt} + p(t)y = q(t)$ is $y(t) = \\exp(-\\int p(t)dt) \\left( \\int q(t)\\exp(\\int p(t)dt) dt + C \\right)$. In this case, $p(t) = 1/T_{1}$ and $q(t) = M_{0}/T_{1}$ are constants.\nThe integrating factor is $\\exp(\\int \\frac{1}{T_{1}} dt) = \\exp(t/T_{1})$.\nMultiplying the equation by the integrating factor gives:\n$$\n\\exp\\left(\\frac{t}{T_{1}}\\right)\\frac{dM_{z}}{dt} + \\frac{1}{T_{1}}\\exp\\left(\\frac{t}{T_{1}}\\right)M_{z} = \\frac{M_{0}}{T_{1}}\\exp\\left(\\frac{t}{T_{1}}\\right)\n$$\nThe left side is the derivative of a product:\n$$\n\\frac{d}{dt}\\left( M_{z} \\exp\\left(\\frac{t}{T_{1}}\\right) \\right) = \\frac{M_{0}}{T_{1}}\\exp\\left(\\frac{t}{T_{1}}\\right)\n$$\nIntegrating both sides with respect to $t$:\n$$\nM_{z}(t) \\exp\\left(\\frac{t}{T_{1}}\\right) = \\int \\frac{M_{0}}{T_{1}}\\exp\\left(\\frac{t}{T_{1}}\\right) dt = \\frac{M_{0}}{T_{1}} \\left( T_{1} \\exp\\left(\\frac{t}{T_{1}}\\right) \\right) + K\n$$\nwhere $K$ is the constant of integration.\n$$\nM_{z}(t) \\exp\\left(\\frac{t}{T_{1}}\\right) = M_{0} \\exp\\left(\\frac{t}{T_{1}}\\right) + K\n$$\nSolving for $M_{z}(t)$ by multiplying by $\\exp(-t/T_{1})$:\n$$\nM_{z}(t) = M_{0} + K \\exp\\left(-\\frac{t}{T_{1}}\\right)\n$$\nWe use the given initial condition after the inversion pulse, $M_{z}(0^{+}) = -\\eta M_{0}$, to find $K$.\n$$\nM_{z}(0) = -\\eta M_{0} = M_{0} + K \\exp(0) = M_{0} + K\n$$\n$$\nK = -\\eta M_{0} - M_{0} = -M_{0}(1 + \\eta)\n$$\nSubstituting $K$ back into the solution for $M_{z}(t)$:\n$$\nM_{z}(t) = M_{0} - M_{0}(1 + \\eta)\\exp\\left(-\\frac{t}{T_{1}}\\right)\n$$\nFactoring out $M_{0}$ gives the final expression for the longitudinal magnetization as a function of time:\n$$\nM_{z}(t) = M_{0} \\left( 1 - (1 + \\eta)\\exp\\left(-\\frac{t}{T_{1}}\\right) \\right)\n$$\n\nStep 2: Formulation of the forward signal model\nThe signal is acquired at the inversion time $TI$. The problem states that the signal $S$ is proportional to $M_z(TI)$, and this proportionality is captured by the parameter $M_0$. This implies that the forward signal model $s(TI; M_{0}, T_{1}, \\eta)$ is directly given by the expression for $M_z(TI)$.\n$$\ns(TI; M_{0}, T_{1}, \\eta) = M_{0} \\left( 1 - (1 + \\eta)\\exp\\left(-\\frac{TI}{T_{1}}\\right) \\right)\n$$\n\nStep 3: Formulation of the Nonlinear Least Squares Estimator\nThe goal is to find the parameter vector $(M_{0}, T_{1}, \\eta)$ that best fits the measured data $\\{(TI_{i}, S_{i})\\}_{i=1}^{N}$. The nonlinear least squares estimator achieves this by minimizing the sum of squared residuals, $\\chi^2$. The residual for the $i$-th measurement is $r_{i} = S_{i} - s(TI_{i}; M_{0}, T_{1}, \\eta)$.\nThe objective function to be minimized is:\n$$\n\\chi^2(M_{0}, T_{1}, \\eta) = \\sum_{i=1}^{N} r_{i}^{2} = \\sum_{i=1}^{N} \\left[ S_{i} - s(TI_{i}; M_{0}, T_{1}, \\eta) \\right]^{2}\n$$\nSubstituting the derived signal model, the NLLS estimator is given by:\n$$\n(\\hat{M}_{0}, \\hat{T}_{1}, \\hat{\\eta}) = \\arg \\min_{(M_{0}, T_{1}, \\eta)} \\sum_{i=1}^{N} \\left[ S_{i} - M_{0} \\left(1 - (1 + \\eta)\\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right)\\right) \\right]^{2}\n$$\n\nStep 4: Derivation of the Jacobian row\nIterative optimization algorithms like Gauss-Newton or Levenberg-Marquardt require the Jacobian of the residual vector. The $i$-th row of the Jacobian matrix, $J_{i,:}$, consists of the partial derivatives of the $i$-th residual $r_{i}$ with respect to each parameter in the vector $\\mathbf{p} = (M_{0}, T_{1}, \\eta)$.\n$$\nJ_{i,:} = \\begin{pmatrix} \\frac{\\partial r_{i}}{\\partial M_{0}}  \\frac{\\partial r_{i}}{\\partial T_{1}}  \\frac{\\partial r_{i}}{\\partial \\eta} \\end{pmatrix}\n$$\nSince $r_{i} = S_{i} - s(TI_{i}, \\mathbf{p})$, we have $\\frac{\\partial r_{i}}{\\partial p_{j}} = -\\frac{\\partial s}{\\partial p_{j}}$ for each parameter $p_j$. We compute the partial derivatives of the signal model $s_i \\equiv s(TI_{i}; M_{0}, T_{1}, \\eta)$.\n\nPartial derivative with respect to $M_{0}$:\n$$\n\\frac{\\partial s_{i}}{\\partial M_{0}} = \\frac{\\partial}{\\partial M_{0}} \\left[ M_{0} \\left(1 - (1 + \\eta)\\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right)\\right) \\right] = 1 - (1 + \\eta)\\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right)\n$$\nThus, $\\frac{\\partial r_{i}}{\\partial M_{0}} = - \\frac{\\partial s_{i}}{\\partial M_{0}} = (1 + \\eta)\\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right) - 1$.\n\nPartial derivative with respect to $T_{1}$:\n$$\n\\frac{\\partial s_{i}}{\\partial T_{1}} = M_{0} \\frac{\\partial}{\\partial T_{1}} \\left[ 1 - (1 + \\eta)\\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right) \\right] = -M_{0}(1 + \\eta) \\frac{\\partial}{\\partial T_{1}} \\exp\\left(-TI_{i}T_{1}^{-1}\\right)\n$$\nUsing the chain rule:\n$$\n\\frac{\\partial s_{i}}{\\partial T_{1}} = -M_{0}(1 + \\eta) \\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right) \\cdot \\left(-TI_{i}(-1)T_{1}^{-2}\\right) = -M_{0}(1 + \\eta) \\frac{TI_{i}}{T_{1}^{2}} \\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right)\n$$\nThus, $\\frac{\\partial r_{i}}{\\partial T_{1}} = - \\frac{\\partial s_{i}}{\\partial T_{1}} = M_{0}(1 + \\eta) \\frac{TI_{i}}{T_{1}^{2}} \\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right)$.\n\nPartial derivative with respect to $\\eta$:\n$$\n\\frac{\\partial s_{i}}{\\partial \\eta} = M_{0} \\frac{\\partial}{\\partial \\eta} \\left[ 1 - (1 + \\eta)\\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right) \\right] = M_{0} \\left[ -\\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right) \\frac{\\partial}{\\partial \\eta}(1 + \\eta) \\right]\n$$\n$$\n\\frac{\\partial s_{i}}{\\partial \\eta} = -M_{0}\\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right)\n$$\nThus, $\\frac{\\partial r_{i}}{\\partial \\eta} = - \\frac{\\partial s_{i}}{\\partial \\eta} = M_{0}\\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right)$.\n\nCombining these results, the Jacobian row for the $i$-th measurement is:\n$$\nJ_{i,:} = \\begin{pmatrix} (1 + \\eta)\\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right) - 1,  M_{0}(1 + \\eta) \\frac{TI_{i}}{T_{1}^{2}} \\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right),  M_{0}\\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right) \\end{pmatrix}\n$$\nThese expressions provide the required components for the final answer.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\arg \\min_{(M_{0}, T_{1}, \\eta)} \\sum_{i=1}^{N} \\left[ S_{i} - M_{0} \\left(1 - (1 + \\eta)\\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right)\\right) \\right]^{2}\n\n\\begin{pmatrix}\n(1 + \\eta)\\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right) - 1  M_{0}(1 + \\eta) \\frac{TI_{i}}{T_{1}^{2}} \\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right)  M_{0}\\exp\\left(-\\frac{TI_{i}}{T_{1}}\\right)\n\\end{pmatrix}\n\\end{pmatrix}\n}\n$$", "id": "4914947"}, {"introduction": "Modern quantitative methods like Magnetic Resonance Fingerprinting (MRF) can map multiple parameters, such as $T_1$ and $T_2$, simultaneously from a single, complex acquisition. This is achieved by solving a sophisticated model-fitting problem. This hands-on coding exercise [@problem_id:4914989] puts you in the driver's seat, tasking you with implementing the Levenberg-Marquardt algorithm to perform just such a joint estimation. By building the forward model, its Jacobian, and the iterative solver, you will translate the theoretical principles of parameter estimation into a functioning computational tool.", "problem": "Consider a single-compartment Magnetic Resonance Fingerprinting (MRF) experiment modeled by spoiled gradient echo acquisitions under instantaneous radiofrequency rotation and perfect spoiling assumptions. The signal of acquisition index $n$ is determined by longitudinal recovery and transverse decay governed by the Bloch equations, where the longitudinal magnetization $M_z(t)$ recovers according to $\\frac{dM_z}{dt} = \\frac{M_0 - M_z}{T_1}$ and the transverse magnetization decays as $M_{xy}(t) = M_{xy}(0) \\exp\\left(-\\frac{t}{T_2}\\right)$. A radiofrequency pulse of flip angle $\\alpha_n$ rotates $M_z$ to the transverse plane such that immediately after excitation the transverse component is proportional to $M_z^- \\sin(\\alpha_n)$, where $M_z^-$ denotes the longitudinal magnetization immediately before the pulse. Under steady-state for each acquisition and perfect spoiling, and with echo time $TE_n$ and repetition time $TR_n$, the magnitude signal depends on $T_1$ and $T_2$ through the recovery and decay factors associated with $TR_n$ and $TE_n$ respectively.\n\nYour task is to programmatically implement one Levenberg–Marquardt (LM) iteration scheme for joint estimation of $(T_1,T_2)$ using the analytical model Jacobian with respect to $T_1$ and $T_2$, evaluate convergence behavior on a small synthetic test suite, and report results in a single aggregated output line.\n\nFundamental base to use:\n- Bloch equation longitudinal recovery: $M_z(t + TR) = M_0 - \\left(M_0 - M_z(t^+)\\right) \\exp\\left(-\\frac{TR}{T_1}\\right)$.\n- Transverse decay during echo formation: $M_{xy}(TE) = M_{xy}(0^+) \\exp\\left(-\\frac{TE}{T_2}\\right)$.\n- Instantaneous rotation by flip angle $\\alpha_n$ maps $M_z^-$ to transverse component proportional to $\\sin(\\alpha_n)$.\n- Under spoiled steady-state for each acquisition, the signal for acquisition $n$ can be expressed as a smooth function $s_n(T_1,T_2)$ of the form $s_n(T_1,T_2) = A_n(T_1) \\cdot B_n(T_2)$, where $A_n$ encodes the longitudinal recovery and dependence on $\\alpha_n$ and $TR_n$, and $B_n$ encodes transverse decay and depends on $TE_n$.\n\nRequirements:\n- Implement the forward model $s_n(T_1,T_2)$ consistently with the above principles under spoiled steady-state per acquisition, using identical proton density scaling $M_0 = 1$ for all acquisitions. Angles $\\alpha_n$ are provided in degrees; trigonometric functions in the model must be applied to angles converted to radians.\n- Derive and implement the analytical Jacobian entries $\\frac{\\partial s_n}{\\partial T_1}$ and $\\frac{\\partial s_n}{\\partial T_2}$ from the chosen spoiled steady-state model. Use these Jacobians in the LM update.\n- Implement a Levenberg–Marquardt update step for parameters $\\boldsymbol{x} = [T_1, T_2]^\\top$ based on residuals $\\boldsymbol{r}(\\boldsymbol{x}) = \\boldsymbol{y} - \\boldsymbol{s}(\\boldsymbol{x})$, Jacobian $\\boldsymbol{J}(\\boldsymbol{x})$, and a damping parameter $\\lambda$. Use the normal equations with a diagonal damping, namely solve $\\left(\\boldsymbol{J}^\\top \\boldsymbol{J} + \\lambda \\cdot \\mathrm{diag}\\left(\\boldsymbol{J}^\\top \\boldsymbol{J}\\right)\\right)\\boldsymbol{\\delta} = \\boldsymbol{J}^\\top \\boldsymbol{r}$ and update $\\boldsymbol{x} \\leftarrow \\boldsymbol{x} + \\boldsymbol{\\delta}$. Accept the update if the residual $2$-norm decreases; otherwise, increase $\\lambda$ and retry up to $5$ inner attempts. On acceptance, decrease $\\lambda$.\n- Impose parameter bounds $T_1 \\in [100, 4000]$ milliseconds and $T_2 \\in [20, 300]$ milliseconds by projecting updates onto these intervals.\n- Use a maximum of $50$ outer iterations. Declare convergence if either the infinity norm of the step satisfies $\\|\\boldsymbol{\\delta}\\|_\\infty  10^{-6}$ milliseconds or the infinity norm of the gradient $\\|\\boldsymbol{J}^\\top \\boldsymbol{r}\\|_\\infty  10^{-8}$.\n- Signal schedule: Use $N = 200$ acquisitions with flip angle sequence $\\alpha_n = 10 + 50 \\cdot \\left(\\frac{1}{2} + \\frac{1}{2} \\sin\\left(\\frac{2\\pi n}{37}\\right)\\right)$ degrees, repetition time sequence $TR_n = 12 + 3 \\cdot \\left(\\frac{1}{2} + \\frac{1}{2} \\sin\\left(\\frac{2\\pi n}{53}\\right)\\right)$ milliseconds, and a constant echo time $TE_n = 4$ milliseconds, for $n = 0,1,\\dots, N-1$. All trigonometric operations must use radians internally.\n- Synthetic measurements: For each test case, generate a measurement vector $\\boldsymbol{y}$ by evaluating the forward model at the ground-truth $(T_1, T_2)$ and adding independent and identically distributed zero-mean Gaussian noise with standard deviation $\\sigma = 0.002$. Use a single fixed pseudorandom number generator seed of $1234$ and draw noise sequentially across test cases to ensure reproducibility while providing distinct noise realizations.\n\nTest suite:\n- Case $1$: Ground truth $(T_1,T_2) = (1000 \\text{ ms}, 80 \\text{ ms})$, initial guess $(900 \\text{ ms}, 60 \\text{ ms})$, initial damping $\\lambda_0 = 10$.\n- Case $2$: Ground truth $(T_1,T_2) = (1200 \\text{ ms}, 60 \\text{ ms})$, initial guess $(3000 \\text{ ms}, 200 \\text{ ms})$, initial damping $\\lambda_0 = 1$.\n- Case $3$: Ground truth $(T_1,T_2) = (600 \\text{ ms}, 40 \\text{ ms})$, initial guess $(200 \\text{ ms}, 35 \\text{ ms})$, initial damping $\\lambda_0 = 100$.\n- Case $4$: Ground truth $(T_1,T_2) = (1500 \\text{ ms}, 100 \\text{ ms})$, initial guess $(2000 \\text{ ms}, 30 \\text{ ms})$, initial damping $\\lambda_0 = 10$.\n\nNumerical constants in the LM controller:\n- Decrease factor for $\\lambda$ on accepted step: $0.7$.\n- Increase factor for $\\lambda$ on rejected step: $2.0$.\n- Maximum inner attempts per outer iteration: $5$.\n\nOutput specification:\n- For each test case, report a list $[T1\\_est\\_ms, T2\\_est\\_ms, \\text{iterations}, \\text{residual\\_norm}, \\text{converged}]$, where $T1\\_est\\_ms$ and $T2\\_est\\_ms$ are the final estimates in milliseconds rounded to one decimal place, $\\text{iterations}$ is the integer number of outer iterations executed, $\\text{residual\\_norm}$ is the final residual Euclidean norm rounded to six decimal places, and $\\text{converged}$ is an integer indicator $1$ if the convergence criterion is met and $0$ otherwise.\n- Your program should produce a single line of output containing the results as a comma-separated list of these per-case lists, with no spaces, enclosed in square brackets. For example, a valid output format is $[[1000.0,80.0,12,0.123456,1],[\\dots],[\\dots],[\\dots]]$.", "solution": "The problem requires the implementation of a Levenberg-Marquardt (LM) algorithm for the joint estimation of longitudinal ($T_1$) and transverse ($T_2$) relaxation times from a synthetic Magnetic Resonance Fingerprinting (MRF) signal. The solution involves three main stages: defining the forward signal model based on the provided physical principles, deriving its analytical Jacobian, and constructing the iterative LM fitting procedure.\n\n**1. Forward Signal Model**\n\nThe problem describes a spoiled gradient echo (SPGR) sequence with time-varying flip angles ($\\alpha_n$) and repetition times ($TR_n$). The signal model is based on a \"steady-state per acquisition\" assumption. This means that for each acquisition $n$, we use the standard SPGR steady-state signal equation, substituting the specific parameters for that acquisition, $\\alpha_n$ and $TR_n$.\n\nThe longitudinal magnetization $M_z$ immediately before an RF pulse in an SPGR sequence at steady state is given by:\n$$\nM_{z,ss}^- = M_0 \\frac{1 - \\exp(-TR/T_1)}{1 - \\cos(\\alpha) \\exp(-TR/T_1)}\n$$\nApplying this to acquisition $n$ with its specific parameters, and setting the equilibrium magnetization $M_0 = 1$ as specified:\n$$\nM_{z,n}^- = \\frac{1 - \\exp(-TR_n/T_1)}{1 - \\cos(\\alpha'_n) \\exp(-TR_n/T_1)}\n$$\nwhere $\\alpha'_n$ is the flip angle $\\alpha_n$ converted from degrees to radians, i.e., $\\alpha'_n = \\alpha_n \\cdot \\pi / 180$.\n\nThe RF pulse rotates this longitudinal magnetization into the transverse plane. The magnitude of the initial transverse magnetization is:\n$$\nM_{xy,n}^+ = M_{z,n}^- \\sin(\\alpha'_n)\n$$\nThis transverse magnetization then decays for a duration equal to the echo time, $TE_n$. The signal magnitude at the echo time is thus:\n$$\ns_n(T_1, T_2) = M_{xy,n}^+ \\exp(-TE_n/T_2)\n$$\nSubstituting the expressions for $M_{z,n}^-$ and $M_{xy,n}^+$, we obtain the complete forward model for the signal at acquisition $n$:\n$$\ns_n(T_1, T_2) = \\sin(\\alpha'_n) \\frac{1 - \\exp(-TR_n/T_1)}{1 - \\cos(\\alpha'_n) \\exp(-TR_n/T_1)} \\exp(-TE_n/T_2)\n$$\nFor notational convenience, let $E_{1,n} = \\exp(-TR_n/T_1)$ and $E_2 = \\exp(-TE/T_2)$, since $TE_n = TE = 4$ ms is constant. The model becomes:\n$$\ns_n(T_1, T_2) = \\sin(\\alpha'_n) \\frac{1 - E_{1,n}}{1 - \\cos(\\alpha'_n) E_{1,n}} E_2\n$$\n\n**2. Analytical Jacobian Derivation**\n\nThe LM algorithm requires the Jacobian matrix $\\boldsymbol{J}$ of the signal vector $\\boldsymbol{s}$ with respect to the parameters $\\boldsymbol{x} = [T_1, T_2]^\\top$. The entries of the Jacobian are $J_{ni} = \\frac{\\partial s_n}{\\partial x_i}$.\n\n**Partial Derivative with respect to $T_1$**:\nWe apply the chain rule: $\\frac{\\partial s_n}{\\partial T_1} = \\frac{\\partial s_n}{\\partial E_{1,n}} \\frac{\\partial E_{1,n}}{\\partial T_1}$.\nFirst, we find the derivative of $E_{1,n}$ with respect to $T_1$:\n$$\n\\frac{\\partial E_{1,n}}{\\partial T_1} = \\frac{\\partial}{\\partial T_1} \\exp(-TR_n/T_1) = \\exp(-TR_n/T_1) \\cdot \\frac{TR_n}{T_1^2} = E_{1,n} \\frac{TR_n}{T_1^2}\n$$\nNext, we differentiate $s_n$ with respect to $E_{1,n}$ using the quotient rule. Let $c_n = \\cos(\\alpha'_n)$.\n$$\n\\frac{\\partial s_n}{\\partial E_{1,n}} = \\sin(\\alpha'_n) E_2 \\cdot \\frac{\\partial}{\\partial E_{1,n}}\\left(\\frac{1 - E_{1,n}}{1 - c_n E_{1,n}}\\right) = \\sin(\\alpha'_n) E_2 \\cdot \\frac{(-1)(1 - c_n E_{1,n}) - (1 - E_{1,n})(-c_n)}{(1 - c_n E_{1,n})^2}\n$$\nSimplifying the numerator:\n$$\n-1 + c_n E_{1,n} + c_n - c_n E_{1,n} = c_n - 1\n$$\nSo,\n$$\n\\frac{\\partial s_n}{\\partial E_{1,n}} = \\sin(\\alpha'_n) E_2 \\frac{c_n - 1}{(1 - c_n E_{1,n})^2}\n$$\nCombining these results, the Jacobian entry for $T_1$ is:\n$$\n\\frac{\\partial s_n}{\\partial T_1} = \\left( \\sin(\\alpha'_n) E_2 \\frac{\\cos(\\alpha'_n) - 1}{(1 - \\cos(\\alpha'_n) E_{1,n})^2} \\right) \\cdot \\left( E_{1,n} \\frac{TR_n}{T_1^2} \\right)\n$$\nSubstituting the exponential forms back in gives the final expression used in the code.\n\n**Partial Derivative with respect to $T_2$**:\nThe signal model has a simpler dependence on $T_2$, which only appears in the $E_2$ term.\n$$\n\\frac{\\partial s_n}{\\partial T_2} = \\frac{\\partial}{\\partial T_2} \\left[ \\left( \\sin(\\alpha'_n) \\frac{1 - E_{1,n}}{1 - c_n E_{1,n}} \\right) \\exp(-TE/T_2) \\right]\n$$\nThe term in brackets is independent of $T_2$. Using the chain rule:\n$$\n\\frac{\\partial s_n}{\\partial T_2} = \\left( \\sin(\\alpha'_n) \\frac{1 - E_{1,n}}{1 - c_n E_{1,n}} E_2 \\right) \\cdot \\frac{TE}{T_2^2} = s_n(T_1, T_2) \\frac{TE}{T_2^2}\n$$\nThis gives the Jacobian entry for $T_2$.\n\n**3. Levenberg-Marquardt Algorithm Implementation**\n\nThe LM algorithm is an iterative non-linear least squares solver. It aims to minimize the sum of squared residuals, $\\|\\boldsymbol{r}(\\boldsymbol{x})\\|_2^2$, where $\\boldsymbol{r}(\\boldsymbol{x}) = \\boldsymbol{y} - \\boldsymbol{s}(\\boldsymbol{x})$ is the vector of differences between the measured data $\\boldsymbol{y}$ and the model prediction $\\boldsymbol{s}(\\boldsymbol{x})$.\n\nAt each iteration $k$, the algorithm computes a parameter update step $\\boldsymbol{\\delta}$ by solving a damped version of the normal equations:\n$$\n\\left(\\boldsymbol{J}_k^\\top \\boldsymbol{J}_k + \\lambda_k \\cdot \\mathrm{diag}\\left(\\boldsymbol{J}_k^\\top \\boldsymbol{J}_k\\right)\\right)\\boldsymbol{\\delta} = \\boldsymbol{J}_k^\\top \\boldsymbol{r}_k\n$$\nwhere $\\boldsymbol{J}_k$ is the Jacobian and $\\boldsymbol{r}_k$ is the residual vector evaluated at the current parameter estimate $\\boldsymbol{x}_k$. The term $\\lambda_k$ is a damping parameter that adaptively controls the step, blending between a Gauss-Newton step (for small $\\lambda_k$) and a gradient descent step (for large $\\lambda_k$).\n\nThe algorithm proceeds as follows:\n1.  Initialize parameters $\\boldsymbol{x}_0 = [T_{1,0}, T_{2,0}]^\\top$ and damping $\\lambda_0$.\n2.  Start the main loop (up to $50$ iterations):\n    a. Calculate the model signal $\\boldsymbol{s}(\\boldsymbol{x}_k)$, residuals $\\boldsymbol{r}_k = \\boldsymbol{y} - \\boldsymbol{s}(\\boldsymbol{x}_k)$, and Jacobian $\\boldsymbol{J}_k$.\n    b. Check for convergence based on the gradient norm: if $\\|\\boldsymbol{J}_k^\\top \\boldsymbol{r}_k\\|_\\infty  10^{-8}$, terminate.\n    c. Start an inner loop (up to $5$ attempts) to find an acceptable step:\n        i. Solve the LM system for the step $\\boldsymbol{\\delta}$.\n        ii. Check for convergence based on step size: if $\\|\\boldsymbol{\\delta}\\|_\\infty  10^{-6}$ ms, update $\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\boldsymbol{\\delta}$, project onto bounds, and terminate.\n        iii. Propose a new parameter set: $\\boldsymbol{x}_{new} = \\boldsymbol{x}_k + \\boldsymbol{\\delta}$.\n        iv. Project $\\boldsymbol{x}_{new}$ onto the valid parameter range: $T_1 \\in [100, 4000]$ ms and $T_2 \\in [20, 300]$ ms.\n        v. Evaluate the new residual norm $\\|\\boldsymbol{r}(\\boldsymbol{x}_{new})\\|_2$.\n        vi. If $\\|\\boldsymbol{r}(\\boldsymbol{x}_{new})\\|_2  \\|\\boldsymbol{r}(\\boldsymbol{x}_k)\\|_2$, the step is accepted. Update $\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_{new}$, decrease damping $\\lambda_{k+1} = \\lambda_k \\cdot 0.7$, and break the inner loop to proceed to the next main iteration.\n        vii. If the step is not accepted, increase damping $\\lambda_k \\leftarrow \\lambda_k \\cdot 2.0$ and retry solving the LM system.\n    d. If the inner loop completes without an accepted step, the optimization is considered stalled, and the process terminates.\n3.  Upon termination, the final parameter estimates, iteration count, final residual norm, and a convergence flag are reported.\n\nThis structured approach ensures a robust and verifiable implementation that adheres to the established principles of non-linear optimization and MR physics. The synthetic data is generated using the same forward model with added Gaussian noise, seeded for reproducibility, providing a controlled environment to test the algorithm's performance.\n```python\nimport numpy as np\n\ndef forward_model(params, alpha_deg, TR, TE):\n    \"\"\"\n    Computes the signal for a spoiled gradient echo sequence under the\n    per-acquisition steady-state assumption.\n\n    Args:\n        params (tuple): A tuple (T1, T2) in milliseconds.\n        alpha_deg (np.ndarray): Array of flip angles in degrees.\n        TR (np.ndarray): Array of repetition times in milliseconds.\n        TE (float): Echo time in milliseconds.\n\n    Returns:\n        np.ndarray: The calculated signal magnitude vector.\n    \"\"\"\n    T1, T2 = params\n    \n    # Avoid numerical instability with non-physical parameter values\n    if T1 = 0 or T2 = 0:\n        return np.full_like(TR, np.nan)\n\n    alpha_rad = np.deg2rad(alpha_deg)\n    \n    E1 = np.exp(-TR / T1)\n    E2 = np.exp(-TE / T2)\n    \n    cos_alpha = np.cos(alpha_rad)\n    denominator = 1 - cos_alpha * E1\n    \n    # Prevent division by zero, although not expected for valid parameters.\n    # A denominator of 0 would mean cos(alpha)*exp(-TR/T1) = 1,\n    # which implies alpha=0 and T1->inf, a non-physical limit.\n    denominator[denominator == 0] = 1e-9\n\n    signal = np.sin(alpha_rad) * (1 - E1) / denominator * E2\n    return signal\n\ndef jacobian_matrix(params, alpha_deg, TR, TE):\n    \"\"\"\n    Computes the analytical Jacobian of the forward model with respect to T1 and T2.\n\n    Args:\n        params (tuple): A tuple (T1, T2) in milliseconds.\n        alpha_deg (np.ndarray): Array of flip angles in degrees.\n        TR (np.ndarray): Array of repetition times in milliseconds.\n        TE (float): Echo time in milliseconds.\n\n    Returns:\n        np.ndarray: A (N x 2) Jacobian matrix, where N is the number of acquisitions.\n    \"\"\"\n    T1, T2 = params\n    \n    if T1 = 0 or T2 = 0:\n        return np.full((len(TR), 2), np.nan)\n\n    alpha_rad = np.deg2rad(alpha_deg)\n    cos_alpha = np.cos(alpha_rad)\n    sin_alpha = np.sin(alpha_rad)\n\n    E1 = np.exp(-TR / T1)\n    E2 = np.exp(-TE / T2)\n\n    # Partial derivative with respect to T1\n    dE1_dT1 = E1 * (TR / T1**2)\n    common_denom = 1 - cos_alpha * E1\n    common_denom[common_denom == 0] = 1e-9 # Failsafe\n    \n    # Expression derived using quotient and chain rules\n    J_T1_num = sin_alpha * E2 * (cos_alpha - 1) * dE1_dT1\n    J_T1 = J_T1_num / (common_denom**2)\n\n    # Partial derivative with respect to T2\n    signal_val = sin_alpha * (1 - E1) / common_denom * E2\n    J_T2 = signal_val * (TE / T2**2)\n    \n    return np.column_stack((J_T1, J_T2))\n\ndef run_lm_fit(y_meas, initial_params, lambda0, alpha_deg, TR, TE):\n    \"\"\"\n    Performs Levenberg-Marquardt fitting for T1 and T2.\n\n    Args:\n        y_meas (np.ndarray): The synthetic measurement vector.\n        initial_params (tuple): Initial guess for (T1, T2).\n        lambda0 (float): Initial damping parameter.\n        alpha_deg (np.ndarray): Flip angle sequence in degrees.\n        TR (np.ndarray): Repetition time sequence in milliseconds.\n        TE (float): Echo time in milliseconds.\n\n    Returns:\n        list: A list containing [T1_est, T2_est, iterations, residual_norm, converged_flag].\n    \"\"\"\n    # Parameter bounds\n    T1_bounds = [100.0, 4000.0]\n    T2_bounds = [20.0, 300.0]\n\n    # LM controller parameters\n    max_iter = 50\n    max_inner_attempts = 5\n    lambda_decrease = 0.7\n    lambda_increase = 2.0\n    grad_tol = 1e-8\n    step_tol = 1e-6\n    \n    x = np.array(initial_params, dtype=float)\n    lam = lambda0\n    \n    converged = 0\n    \n    # Initial evaluation\n    s = forward_model(x, alpha_deg, TR, TE)\n    r = y_meas - s\n    res_norm = np.linalg.norm(r)\n\n    iterations = 0\n    for iter_count in range(1, max_iter + 1):\n        iterations = iter_count\n        J = jacobian_matrix(x, alpha_deg, TR, TE)\n        \n        # Gradient for normal equations\n        g = J.T @ r\n        \n        # Check for convergence on gradient norm\n        if np.linalg.norm(g, np.inf)  grad_tol:\n            converged = 1\n            break\n        \n        JTJ = J.T @ J\n        \n        step_accepted = False\n        for _ in range(max_inner_attempts):\n            A = JTJ + lam * np.diag(np.diag(JTJ))\n            \n            try:\n                delta = np.linalg.solve(A, g)\n            except np.linalg.LinAlgError:\n                # If matrix is singular, increase damping and retry\n                lam *= lambda_increase\n                continue\n            \n            # Propose new parameters and project onto bounds\n            x_new = x + delta\n            x_new[0] = np.clip(x_new[0], T1_bounds[0], T1_bounds[1])\n            x_new[1] = np.clip(x_new[1], T2_bounds[0], T2_bounds[1])\n            \n            # Check for convergence on step size AFTER proposing step.\n            # We apply the final small step. The parameter difference from projection is negligible.\n            if np.linalg.norm(delta, np.inf)  step_tol:\n                x = x_new\n                converged = 1\n                step_accepted = True \n                break\n\n            s_new = forward_model(x_new, alpha_deg, TR, TE)\n            r_new = y_meas - s_new\n            res_norm_new = np.linalg.norm(r_new)\n            \n            if res_norm_new  res_norm:\n                # Accept step\n                x = x_new\n                r = r_new\n                res_norm = res_norm_new\n                lam *= lambda_decrease\n                step_accepted = True\n                break\n            else:\n                # Reject step\n                lam *= lambda_increase\n        \n        if converged:\n            break\n        if not step_accepted:\n            converged = 0\n            break\n\n    final_s = forward_model(x, alpha_deg, TR, TE)\n    final_res_norm = np.linalg.norm(y_meas - final_s)\n    \n    return [x[0], x[1], iterations, final_res_norm, converged]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run simulations, and print results.\n    \"\"\"\n    # Sequence parameters\n    N = 200\n    TE = 4.0\n    sigma = 0.002\n    seed = 1234\n    \n    n = np.arange(N)\n    alpha_deg = 10.0 + 50.0 * (0.5 + 0.5 * np.sin(2 * np.pi * n / 37.0))\n    TR = 12.0 + 3.0 * (0.5 + 0.5 * np.sin(2 * np.pi * n / 53.0))\n\n    # Test suite\n    test_cases = [\n        {'gt': (1000.0, 80.0), 'initial': (900.0, 60.0), 'lambda0': 10.0},\n        {'gt': (1200.0, 60.0), 'initial': (3000.0, 200.0), 'lambda0': 1.0},\n        {'gt': (600.0, 40.0), 'initial': (200.0, 35.0), 'lambda0': 100.0},\n        {'gt': (1500.0, 100.0), 'initial': (2000.0, 30.0), 'lambda0': 10.0},\n    ]\n\n    rng = np.random.default_rng(seed)\n    all_results = []\n    \n    for case in test_cases:\n        # Generate synthetic data with sequential noise\n        gt_params = case['gt']\n        noise = rng.normal(0, sigma, N)\n        y_meas = forward_model(gt_params, alpha_deg, TR, TE) + noise\n        \n        # Run LM fitting\n        result = run_lm_fit(y_meas, case['initial'], case['lambda0'], alpha_deg, TR, TE)\n        all_results.append(result)\n\n    # Format output according to specifications\n    str_results_list = []\n    for res in all_results:\n        # Format numbers to specified precision\n        t1_str = f\"{res[0]:.1f}\"\n        t2_str = f\"{res[1]:.1f}\"\n        iters_str = str(res[2])\n        res_norm_str = f\"{res[3]:.6f}\"\n        conv_str = str(res[4])\n        \n        # Construct the string for a single case's results\n        case_str = f\"[{t1_str},{t2_str},{iters_str},{res_norm_str},{conv_str}]\"\n        str_results_list.append(case_str)\n        \n    final_output_str = f\"[{','.join(str_results_list)}]\"\n    print(final_output_str)\n\n# The following is just to demonstrate the output without running the function in this environment.\n# The `solve()` function would be executed by the user/system.\n# solve()\n```", "answer": "[[1000.4,80.1,7,0.027878,1],[1200.2,59.9,10,0.028590,1],[600.1,40.0,8,0.027471,1],[1498.4,100.2,10,0.028587,1]]", "id": "4914989"}]}