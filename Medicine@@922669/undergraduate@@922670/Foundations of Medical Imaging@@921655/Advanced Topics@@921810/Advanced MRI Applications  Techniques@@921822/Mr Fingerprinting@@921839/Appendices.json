{"hands_on_practices": [{"introduction": "Simulation is the engine that drives Magnetic Resonance Fingerprinting, used for both generating the dictionaries and designing the pulse sequences themselves. A deep understanding of the underlying physics of signal evolution is therefore essential. This first practice [@problem_id:4902016] challenges you to build a simulator from the ground up based on the Bloch equations, allowing you to explore how a specific sequence choice—balanced Steady-State Free Precession (bSSFP)—can lead to ambiguities where physically distinct tissues produce highly correlated fingerprints.", "problem": "You must write a complete, runnable program that constructs a balanced Steady-State Free Precession (bSSFP) Magnetic Resonance Fingerprinting (MRF) simulation and demonstrates a concrete ambiguity scenario in which two distinct parameter sets produce highly correlated fingerprints due to banding near off-resonance. The program must implement a discrete-time Bloch-equation-based simulation for a constant repetition time sequence and compute the Pearson correlation between two fingerprints to quantify ambiguity.\n\nThe fundamental base to use is the Bloch equation with longitudinal and transverse relaxation, rigid-body rotations for radiofrequency excitations and off-resonance precession, and the bSSFP echo at mid-repetition time. Magnetic Resonance (MR) Fingerprinting (MRF) is the process of generating a temporally varying excitation sequence to produce a unique time-dependent signal (a fingerprint) that depends on tissue parameters. Balanced Steady-State Free Precession (bSSFP) is a sequence in which gradients are balanced each repetition time so that dephasing from gradients cancels, and an echo forms at half the repetition time. Banding in bSSFP occurs near off-resonance phase accumulation of approximately $\\pi$, which can significantly attenuate the transverse signal. Your program must derive the magnetization evolution from first principles of MR physics (rotation and relaxation) and perform the fingerprint simulation accordingly.\n\nSequence specification:\n- The repetition time is constant and equal to $TR = 0.004\\,\\mathrm{s}$.\n- The echo time is the mid-point, $TE = TR/2 = 0.002\\,\\mathrm{s}$.\n- The number of frames is $N = 300$.\n- The flip angle schedule (in degrees) is defined for frame index $n \\in \\{0,1,\\dots,N-1\\}$ by\n$$\n\\alpha_n = 10^\\circ + 45^\\circ \\sin^2\\!\\left(\\frac{2\\pi n}{57}\\right).\n$$\n- The radiofrequency pulse is a rotation about the $x$-axis by $\\alpha_n$.\n- The off-resonance frequency is $\\Delta f$ (in $\\mathrm{Hz}$). The off-resonance phase accrued in time $\\Delta t$ is $\\phi = 2\\pi \\,\\Delta f\\, \\Delta t$ (in radians).\n- The longitudinal and transverse relaxation times are $T_1$ and $T_2$ (in $\\mathrm{s}$).\n- The equilibrium magnetization magnitude is $M_0 = 1$ (unitless).\n\nMagnetization evolution per frame must be modeled as follows:\n1. Begin with magnetization vector $\\mathbf{M} = [M_x, M_y, M_z]^\\top$ initialized to $[0, 0, M_0]^\\top$.\n2. Apply the radiofrequency rotation about the $x$-axis by $\\alpha_n$ to obtain $\\mathbf{M}' = \\mathbf{R}_x(\\alpha_n)\\,\\mathbf{M}$, where $\\mathbf{R}_x(\\alpha)$ is the $3 \\times 3$ rotation matrix about the $x$-axis by angle $\\alpha$ (in radians).\n3. Evolve for $\\Delta t = TR/2$ using off-resonance precession and relaxation to reach the echo time:\n   - Rotate the transverse components about the $z$-axis by $\\phi_{\\text{half}} = 2\\pi\\,\\Delta f\\,(TR/2)$ via $\\mathbf{R}_z(\\phi_{\\text{half}})$.\n   - Apply relaxation: transverse components decay by $e^{-\\Delta t/T_2}$ and longitudinal magnetization relaxes toward $M_0$ by $M_z \\leftarrow M_z\\,e^{-\\Delta t/T_1} + M_0\\left(1 - e^{-\\Delta t/T_1}\\right)$.\n4. Record the fingerprint sample $S_n = \\sqrt{M_x^2 + M_y^2}$ at the echo time.\n5. Evolve for the remaining $\\Delta t = TR/2$ with the same rotation and relaxation operations to reach the start of the next repetition.\n\nCorrelation and ambiguity quantification:\n- Given two fingerprints $\\mathbf{s}^{(A)}$ and $\\mathbf{s}^{(B)}$ of length $N$, compute the Pearson correlation coefficient\n$$\n\\rho = \\frac{\\sum_{n=0}^{N-1} \\left(s^{(A)}_n - \\bar{s}^{(A)}\\right)\\left(s^{(B)}_n - \\bar{s}^{(B)}\\right)}{\\sqrt{\\sum_{n=0}^{N-1}\\left(s^{(A)}_n - \\bar{s}^{(A)}\\right)^2}\\,\\sqrt{\\sum_{n=0}^{N-1}\\left(s^{(B)}_n - \\bar{s}^{(B)}\\right)^2}},\n$$\nwhere $\\bar{s}^{(A)}$ and $\\bar{s}^{(B)}$ are the means of the respective fingerprints.\n- Define ambiguity as the correlation strictly exceeding the threshold, that is, declare ambiguous if $\\rho > \\rho_{\\mathrm{thr}}$ is true. If the denominator in the correlation formula is zero for both fingerprints (zero variance), define $\\rho = 1$ when both fingerprints are identical sequences and $\\rho = 0$ otherwise.\n\nConstruct the following test suite of parameter pairs $\\left((T_1^{(A)},T_2^{(A)},\\Delta f^{(A)}),(T_1^{(B)},T_2^{(B)},\\Delta f^{(B)}),\\rho_{\\mathrm{thr}}\\right)$, all times in $\\mathrm{s}$ and frequencies in $\\mathrm{Hz}$:\n1. Case 1 (ambiguity due to banding under constant $TR$): $\\left((1.2,0.08,125),(0.6,0.06,375),0.95\\right)$. Note that for $TR = 0.004\\,\\mathrm{s}$, $\\Delta f = 125\\,\\mathrm{Hz}$ gives $\\phi = \\pi$ and $\\Delta f = 375\\,\\mathrm{Hz}$ gives $\\phi = 3\\pi$, which is equivalent to $\\pi$ modulo $2\\pi$, thereby inducing banding for both and promoting high correlation despite distinct $(T_1,T_2)$.\n2. Case 2 (non-ambiguous scenario): $\\left((1.2,0.08,0),(0.6,0.06,50),0.95\\right)$.\n3. Case 3 (near-band but offset off-resonance): $\\left((1.0,0.1,125),(0.7,0.07,360),0.95\\right)$.\n4. Case 4 (threshold boundary check): $\\left((1.0,0.1,125),(1.0,0.1,125),1.0\\right)$.\n\nAngle unit for $\\alpha_n$ is degrees; off-resonance $\\Delta f$ must be in $\\mathrm{Hz}$; relaxation times $T_1,T_2$ must be in $\\mathrm{s}$; $TR$ must be in $\\mathrm{s}$; phases are in radians. The output unit of the fingerprint samples is unitless.\n\nYour program should produce a single line of output containing the ambiguity decisions for the above four cases as a comma-separated list enclosed in square brackets, using Boolean values, for example, `[`True`, `False`, `False`, `True`]` if and only if the corresponding correlation strictly exceeds the specified threshold in each case. No other output is permitted.", "solution": "The problem requires the construction of a numerical simulation for Magnetic Resonance Fingerprinting (MRF) based on a balanced Steady-State Free Precession (bSSFP) sequence. The simulation must demonstrate a scenario of ambiguity, where two distinct sets of tissue parameters ($T_1$, $T_2$) and off-resonance frequency ($\\Delta f$) yield highly correlated signal evolutions, known as fingerprints. The solution involves implementing a discrete-time Bloch equation model, generating fingerprints for specified parameter sets, and quantifying their similarity using the Pearson correlation coefficient.\n\nThe simulation adheres to the following first principles of nuclear magnetic resonance physics and the specified sequence structure.\n\n**1. The Bloch Equation Model**\n\nThe evolution of the macroscopic magnetization vector $\\mathbf{M} = [M_x, M_y, M_z]^\\top$ is governed by the Bloch equations. In this problem, we model the evolution using a sequence of discrete operations within each repetition time ($TR$): radiofrequency (RF) excitation, free precession due to off-resonance, and longitudinal and transverse relaxation. The equilibrium magnetization is given as $M_0 = 1$. The initial state of the system is thermal equilibrium, $\\mathbf{M}(0) = [0, 0, M_0]^\\top$.\n\n**2. Radiofrequency (RF) Excitation**\n\nAn RF pulse acts as a rotation on the magnetization vector. For this problem, the pulse is applied along the $x$-axis, causing a rotation by a flip angle $\\alpha_n$. The transformation is described by the rotation matrix $\\mathbf{R}_x(\\alpha_n)$:\n$$\n\\mathbf{M}_{\\text{post-RF}} = \\mathbf{R}_x(\\alpha_n) \\mathbf{M}_{\\text{pre-RF}}\n$$\nwhere $\\alpha_n$ is in radians, and the matrix is given by:\n$$\n\\mathbf{R}_x(\\alpha) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\cos\\alpha & -\\sin\\alpha \\\\ 0 & \\sin\\alpha & \\cos\\alpha \\end{pmatrix}\n$$\nThe flip angle schedule is defined for frame index $n \\in \\{0, 1, \\dots, N-1\\}$, with $N=300$, as:\n$$\n\\alpha_n = 10^\\circ + 45^\\circ \\sin^2\\!\\left(\\frac{2\\pi n}{57}\\right)\n$$\nThese angles, given in degrees, must be converted to radians for use in the rotation matrix.\n\n**3. Free Precession and Relaxation**\n\nBetween RF pulses, the magnetization evolves due to two effects: precession around the main magnetic field axis ($z$-axis) at an off-resonance frequency $\\Delta f$, and relaxation processes that return the magnetization to thermal equilibrium. This evolution occurs over a time interval $\\Delta t$.\n\nThe precession is a rotation of the transverse magnetization components ($M_x, M_y$) around the $z$-axis by an angle $\\phi = 2\\pi \\Delta f \\Delta t$. This is represented by the rotation matrix $\\mathbf{R}_z(\\phi)$:\n$$\n\\mathbf{M}_{\\text{precessed}} = \\mathbf{R}_z(\\phi) \\mathbf{M}_{\\text{initial}} \\quad \\text{with} \\quad \\mathbf{R}_z(\\phi) = \\begin{pmatrix} \\cos\\phi & -\\sin\\phi & 0 \\\\ \\sin\\phi & \\cos\\phi & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nSimultaneously, the magnetization components relax toward their equilibrium values. The transverse components ($M_{x,y}$) decay to zero with a time constant $T_2$, and the longitudinal component ($M_z$) recovers towards $M_0$ with a time constant $T_1$. For an evolution over time $\\Delta t$, the relaxation is described by:\n$$\nM_{x,y}(t+\\Delta t) = M_{x,y}(t) \\, e^{-\\Delta t/T_2}\n$$\n$$\nM_z(t+\\Delta t) = M_z(t) \\, e^{-\\Delta t/T_1} + M_0(1 - e^{-\\Delta t/T_1})\n$$\nThe problem specifies that these operations (precession and relaxation) are to be applied sequentially for each half-$TR$ interval.\n\n**4. The bSSFP-MRF Sequence Simulation**\n\nThe simulation proceeds iteratively for $N=300$ frames. Within each frame $n$, corresponding to a single repetition time $TR = 0.004\\,\\mathrm{s}$, the following steps are executed:\n1.  **RF Excitation**: The current magnetization vector $\\mathbf{M}$ is rotated by $\\alpha_n$ about the $x$-axis.\n2.  **Evolution to Echo Time**: The magnetization evolves for a duration of $\\Delta t = TR/2 = 0.002\\,\\mathrm{s}$. This involves applying the off-resonance precession rotation $\\mathbf{R}_z(2\\pi \\Delta f (TR/2))$ followed by the relaxation transformations for that duration.\n3.  **Signal Sampling**: At the echo time $TE = TR/2$, the magnitude of the transverse magnetization is recorded as the $n$-th sample of the fingerprint: $S_n = \\sqrt{M_x^2 + M_y^2}$.\n4.  **Evolution to End of TR**: The magnetization continues to evolve for another $\\Delta t = TR/2$, undergoing the same precession and relaxation operations. The resulting magnetization vector serves as the input for the next RF pulse in frame $n+1$.\n\nThe collection of all samples $\\{S_n\\}_{n=0}^{N-1}$ constitutes the MR fingerprint $\\mathbf{s}$, a unique time-dependent signal for a given set of parameters ($T_1, T_2, \\Delta f$).\n\n**5. Ambiguity Quantification**\n\nAmbiguity arises when different parameter sets produce highly similar fingerprints, making it difficult to distinguish them. This similarity is quantified using the Pearson correlation coefficient, $\\rho$, between two fingerprints $\\mathbf{s}^{(A)}$ and $\\mathbf{s}^{(B)}$:\n$$\n\\rho = \\frac{\\sum_{n=0}^{N-1} \\left(s^{(A)}_n - \\bar{s}^{(A)}\\right)\\left(s^{(B)}_n - \\bar{s}^{(B)}\\right)}{\\sqrt{\\sum_{n=0}^{N-1}\\left(s^{(A)}_n - \\bar{s}^{(A)}\\right)^2}\\,\\sqrt{\\sum_{n=0}^{N-1}\\left(s^{(B)}_n - \\bar{s}^{(B)}\\right)^2}}\n$$\nwhere $\\bar{s}$ denotes the mean of the fingerprint signal. A special case is defined for fingerprints with zero variance (i.e., constant signals): if both fingerprints are identical, $\\rho = 1$; otherwise, $\\rho = 0$.\n\nAn ambiguity is declared if the correlation $\\rho$ is strictly greater than a specified threshold, $\\rho > \\rho_{\\mathrm{thr}}$. The choice of parameters in Case 1 is designed to demonstrate a known ambiguity in bSSFP imaging. For $TR=0.004\\,\\mathrm{s}$, off-resonance frequencies of $\\Delta f = 125\\,\\mathrm{Hz}$ and $\\Delta f = 375\\,\\mathrm{Hz}$ both cause the phase accrued over one $TR$ to be an odd multiple of $\\pi$ ($2\\pi \\cdot 125 \\cdot 0.004 = \\pi$ and $2\\pi \\cdot 375 \\cdot 0.004 = 3\\pi$), which leads to destructive interference and signal loss characteristic of bSSFP \"banding\". This shared physical behavior can cause their fingerprints to be highly correlated despite different underlying tissue properties.\n\nThe provided Python code implements this entire pipeline. It defines functions to perform the simulation and correlation calculation, iterates through the test cases, evaluates the ambiguity condition for each, and formats the final results as a Boolean list.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the MRF simulation for all test cases and print the results.\n    \"\"\"\n    \n    # --- Sequence and Simulation Constants ---\n    TR = 0.004  # Repetition Time in seconds\n    N = 300     # Number of frames\n    M0 = 1.0    # Equilibrium magnetization (unitless)\n    \n    # --- Pre-calculate Flip Angle Schedule ---\n    def get_flip_angle_schedule(num_frames):\n        \"\"\"Calculates the flip angle schedule in radians.\"\"\"\n        n = np.arange(num_frames)\n        alpha_deg = 10.0 + 45.0 * np.sin(2.0 * np.pi * n / 57.0)**2\n        return np.radians(alpha_deg)\n\n    FLIP_ANGLES_RAD = get_flip_angle_schedule(N)\n\n    # --- Core Simulation Functions ---\n    def get_rx_matrix(angle_rad):\n        \"\"\"Returns the 3x3 rotation matrix for a rotation about the x-axis.\"\"\"\n        c, s = np.cos(angle_rad), np.sin(angle_rad)\n        return np.array([\n            [1.0, 0.0, 0.0],\n            [0.0, c,  -s],\n            [0.0, s,   c]\n        ])\n\n    def get_rz_matrix(angle_rad):\n        \"\"\"Returns the 3x3 rotation matrix for a rotation about the z-axis.\"\"\"\n        c, s = np.cos(angle_rad), np.sin(angle_rad)\n        return np.array([\n            [c,  -s,   0.0],\n            [s,   c,   0.0],\n            [0.0, 0.0, 1.0]\n        ])\n\n    def evolve_state(M, dt, T1, T2, df):\n        \"\"\"\n        Evolves the magnetization state over a time interval dt, applying\n        off-resonance precession and T1/T2 relaxation.\n        \"\"\"\n        # 1. Off-resonance precession\n        phi = 2.0 * np.pi * df * dt\n        Rz = get_rz_matrix(phi)\n        M_precessed = Rz @ M\n\n        # 2. Relaxation\n        E1 = np.exp(-dt / T1)\n        E2 = np.exp(-dt / T2)\n        \n        M_out = np.zeros(3)\n        M_out[0] = M_precessed[0] * E2\n        M_out[1] = M_precessed[1] * E2\n        M_out[2] = M_precessed[2] * E1 + M0 * (1.0 - E1)\n        \n        return M_out\n\n    def simulate_fingerprint(T1, T2, df):\n        \"\"\"\n        Simulates the bSSFP MRF signal (fingerprint) for a given set of parameters.\n        \"\"\"\n        M = np.array([0.0, 0.0, M0])\n        fingerprint = np.zeros(N)\n        dt_half = TR / 2.0\n\n        for i in range(N):\n            # 1. RF pulse (rotation about x-axis)\n            alpha_rad = FLIP_ANGLES_RAD[i]\n            Rx = get_rx_matrix(alpha_rad)\n            M = Rx @ M\n\n            # 2. Evolve for the first half of TR (to echo time)\n            M_at_te = evolve_state(M, dt_half, T1, T2, df)\n\n            # 3. Record fingerprint sample (magnitude of transverse magnetization)\n            fingerprint[i] = np.sqrt(M_at_te[0]**2 + M_at_te[1]**2)\n            \n            # 4. Evolve for the second half of TR (to end of TR)\n            M = evolve_state(M_at_te, dt_half, T1, T2, df)\n            \n        return fingerprint\n\n    # --- Ambiguity Quantification Function ---\n    def pearson_correlation(s_a, s_b):\n        \"\"\"\n        Computes the Pearson correlation coefficient between two signals,\n        with special handling for zero-variance signals as per the problem spec.\n        \"\"\"\n        mean_a, mean_b = np.mean(s_a), np.mean(s_b)\n        dev_a, dev_b = s_a - mean_a, s_b - mean_b\n        \n        sum_sq_dev_a = np.sum(dev_a**2)\n        sum_sq_dev_b = np.sum(dev_b**2)\n        \n        # Use np.isclose for robust floating-point comparison to zero\n        is_a_const = np.isclose(sum_sq_dev_a, 0)\n        is_b_const = np.isclose(sum_sq_dev_b, 0)\n\n        # Handle special cases for zero variance as per problem description\n        if is_a_const and is_b_const:\n            return 1.0 if np.allclose(s_a, s_b) else 0.0\n        \n        if is_a_const or is_b_const:\n            # Denominator is zero, so correlation is ill-defined.\n            # In this context, it implies no linear relationship.\n            return 0.0\n            \n        # Standard Pearson correlation calculation\n        numerator = np.sum(dev_a * dev_b)\n        denominator = np.sqrt(sum_sq_dev_a * sum_sq_dev_b)\n        \n        return numerator / denominator\n\n    # --- Test Suite ---\n    test_cases = [\n        # Case 1: Ambiguity due to banding\n        ((1.2, 0.08, 125), (0.6, 0.06, 375), 0.95),\n        # Case 2: Non-ambiguous scenario\n        ((1.2, 0.08, 0), (0.6, 0.06, 50), 0.95),\n        # Case 3: Near-band but offset off-resonance\n        ((1.0, 0.1, 125), (0.7, 0.07, 360), 0.95),\n        # Case 4: Threshold boundary check (identity)\n        ((1.0, 0.1, 125), (1.0, 0.1, 125), 1.0)\n    ]\n\n    results = []\n    for params_A, params_B, rho_thr in test_cases:\n        # Generate fingerprint for parameter set A\n        fp_A = simulate_fingerprint(T1=params_A[0], T2=params_A[1], df=params_A[2])\n        \n        # Generate fingerprint for parameter set B\n        fp_B = simulate_fingerprint(T1=params_B[0], T2=params_B[1], df=params_B[2])\n        \n        # Compute correlation and check for ambiguity\n        corr = pearson_correlation(fp_A, fp_B)\n        is_ambiguous = corr > rho_thr\n        results.append(is_ambiguous)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4902016"}, {"introduction": "Once a signal evolution, or 'fingerprint', is measured, the next step is to find the best match within a pre-computed dictionary. The accuracy of this pattern recognition task depends critically on the statistical properties of the MRI signal. This exercise [@problem_id:4902005] dives into the crucial distinction between using simple magnitude data versus phase-preserved complex data, revealing the hidden biases introduced by the Rician noise model that governs magnitude images and clarifying why certain matching metrics are more statistically robust than others.", "problem": "An advanced single-voxel Magnetic Resonance Fingerprinting (MRF) experiment acquires a time series of complex measurements $y_t \\in \\mathbb{C}$ for $t = 1,\\dots,T$, modeled as $y_t = s_t + n_t$, where $s_t$ is the true complex signal evolution determined by tissue parameters and sequence design, and $n_t$ is additive receiver noise. It is well established in magnetic resonance (MR) systems that, prior to magnitude formation, the noise in the in-phase and quadrature channels is approximately independent, zero-mean, Gaussian with equal variance, so that $n_t$ is circularly symmetric complex Gaussian with covariance $\\sigma^2 I$ for some $\\sigma > 0$. A magnitude reconstruction forms $m_t = |y_t|$ for each time point. A dictionary $\\{d^{(k)}\\}_{k=1}^K$ of simulated temporal signatures is available, where each $d^{(k)}$ is either complex-valued (phase-preserved) or nonnegative real-valued (magnitude-only), and a similarity score is computed by a normalized inner product (cosine similarity). In particular, for magnitude-only matching a common score is\n$$\nc^{(k)}_{\\text{mag}} = \\frac{\\sum_{t=1}^T m_t\\, d^{(k)}_t}{\\left(\\sum_{t=1}^T m_t^2\\right)^{1/2} \\left(\\sum_{t=1}^T \\left(d^{(k)}_t\\right)^2\\right)^{1/2}},\n$$\nand for complex phase-preserved matching a common score is\n$$\nc^{(k)}_{\\text{cplx}} = \\frac{\\left|\\sum_{t=1}^T y_t \\,\\overline{d^{(k)}_t}\\right|}{\\left(\\sum_{t=1}^T |y_t|^2\\right)^{1/2} \\left(\\sum_{t=1}^T |d^{(k)}_t|^2\\right)^{1/2}}.\n$$\nAssume independent noise across time points, identical noise variance across time, and whitened data so that the covariance is $\\sigma^2 I$. The Signal-to-Noise Ratio (SNR) is understood pointwise as $|s_t|/\\sigma$.\n\nFrom first principles of the complex Gaussian noise model and the magnitude operation, define the distribution of $m_t$ (what family it belongs to and its parameters), and reason about its effect on the expectation and variance of $m_t$ as a function of $|s_t|$. Then explain the implications for correlation-based fingerprint matching when using magnitude-only data $m_t$ versus phase-preserved complex data $y_t$ with white Gaussian noise. In particular, compare how the noise model interacts with the normalized inner product similarity and its relationship to a maximum likelihood (ML) decision rule.\n\nWhich option below most accurately captures both the correct definition and the key implications?\n\nA. When $y_t$ has circular complex Gaussian noise with standard deviation $\\sigma$, the marginal distribution of $m_t = |y_t|$ is Rician with noncentrality $\\nu_t = |s_t|$ and scale parameter $\\sigma$, which induces a positive bias $E[m_t] > |s_t|$ at low SNR and an $m_t$ variance that depends on $|s_t|$. This can inflate normalized correlations for dictionary entries with nonzero baseline amplitude. In contrast, with phase-preserved complex data and white Gaussian noise, maximizing a whitened inner product or minimizing squared error aligns with the ML rule, making $c^{(k)}_{\\text{cplx}}$ closely related to an optimal statistic.\n\nB. Rician noise in $m_t$ is zero-mean and symmetric about zero, so cosine similarity with magnitude data is unbiased. Complex Gaussian noise negates this property because random phase fluctuates the sign of $y_t$, biasing $c^{(k)}_{\\text{cplx}}$ downward.\n\nC. Under Rician noise, the variance of $m_t$ is independent of the true amplitude $|s_t|$, hence the distribution of cosine similarities is invariant across fingerprints; under complex Gaussian noise, the variance depends on $|s_t|$, so whitening is necessary only for phase-preserved data.\n\nD. For magnitude data with Rician noise, the exact ML matching reduces to maximizing the unweighted normalized inner product $c^{(k)}_{\\text{mag}}$, so correlation-based matching is optimal; for complex Gaussian data, Euclidean distance matching is strictly suboptimal relative to correlation.\n\nE. Because the magnitude operation removes phase, magnitude time-series cannot be used for fingerprinting at all; only phase-preserved complex data permits meaningful matching, regardless of the noise model.", "solution": "The problem asks for an analysis of the statistical properties of magnitude-only versus phase-preserved complex data in the context of Magnetic Resonance Fingerprinting (MRF), and to evaluate the implications for correlation-based dictionary matching.\n\n### Step 1: Derivation from First Principles\n\n**Analysis of the Complex Data Model**\n\nThe measurement model for a single time point $t$ is given by:\n$$ y_t = s_t + n_t $$\nwhere $y_t \\in \\mathbb{C}$ is the measured complex signal, $s_t \\in \\mathbb{C}$ is the true complex signal, and $n_t \\in \\mathbb{C}$ is the additive noise.\n\nThe noise $n_t$ is described as circularly symmetric complex Gaussian. This means its real and imaginary parts, let's call them $n_{I,t}$ and $n_{Q,t}$, are independent and identically distributed (i.i.d.) zero-mean Gaussian random variables. Let their variance be $\\sigma^2$.\n$$ n_{I,t} \\sim \\mathcal{N}(0, \\sigma^2) $$\n$$ n_{Q,t} \\sim \\mathcal{N}(0, \\sigma^2) $$\n\nLet's represent the signals in terms of their real (In-phase, $I$) and imaginary (Quadrature, $Q$) components:\n$s_t = s_{I,t} + i s_{Q,t}$\n$y_t = y_{I,t} + i y_{Q,t} = (s_{I,t} + n_{I,t}) + i (s_{Q,t} + n_{Q,t})$\n\nThe components of the measured signal $y_t$ are therefore independent Gaussian random variables with non-zero means:\n$$ y_{I,t} \\sim \\mathcal{N}(s_{I,t}, \\sigma^2) $$\n$$ y_{Q,t} \\sim \\mathcal{N}(s_{Q,t}, \\sigma^2) $$\n\nThe noise in the complex domain is additive and its statistical properties (i.e., its variance) are independent of the signal $s_t$.\n\n**Maximum Likelihood (ML) Estimation with Complex Data**\n\nFor a time series $\\mathbf{y} = [y_1, \\dots, y_T]^T$ and a dictionary of possible true signals $\\{\\mathbf{d}^{(k)}\\}_{k=1}^K$, the ML decision rule is to choose the dictionary entry $\\mathbf{d}^{(k)}$ that maximizes the probability of observing $\\mathbf{y}$. Assuming the noise $n_t$ is i.i.d. across time (white noise), the log-likelihood function is:\n$$ \\mathcal{L}(k | \\mathbf{y}) = \\log P(\\mathbf{y} | \\mathbf{d}^{(k)}) = \\log \\prod_{t=1}^T P(y_t | d^{(k)}_t) $$\nFor complex Gaussian noise, $P(y_t | d^{(k)}_t) \\propto \\exp\\left(-\\frac{|y_t - d^{(k)}_t|^2}{2\\sigma^2_{total}}\\right)$, where $\\sigma^2_{total}$ is the total variance of the complex noise (here, $2\\sigma^2$).\n$$ \\mathcal{L}(k | \\mathbf{y}) = C - \\sum_{t=1}^T \\frac{|y_t - d^{(k)}_t|^2}{2\\sigma^2_{total}} $$\nwhere $C$ is a constant independent of $k$. Maximizing the likelihood is equivalent to minimizing the sum of squared Euclidean distances:\n$$ \\hat{k}_{\\text{ML}} = \\arg \\min_k \\sum_{t=1}^T |y_t - d^{(k)}_t|^2 $$\nExpanding the squared distance:\n$$ \\sum_{t=1}^T |y_t - d^{(k)}_t|^2 = \\sum_{t=1}^T |y_t|^2 - 2 \\text{Re}\\left\\{\\sum_{t=1}^T y_t \\overline{d^{(k)}_t}\\right\\} + \\sum_{t=1}^T |d^{(k)}_t|^2 $$\nSince $\\sum |y_t|^2$ is independent of $k$, minimizing the squared distance is equivalent to maximizing $2 \\text{Re}\\left\\{\\sum y_t \\overline{d^{(k)}_t}\\right\\} - \\sum |d^{(k)}_t|^2$. If all dictionary entries are normalized such that $\\sum |d^{(k)}_t|^2$ is constant, this simplifies to maximizing the real part of the inner product $\\sum y_t \\overline{d^{(k)}_t}$.\n\nThe similarity score $c^{(k)}_{\\text{cplx}}$ uses the magnitude of the inner product, $\\left|\\sum y_t \\overline{d^{(k)}_t}\\right|$, and normalizes it. This is a slight modification that makes the matching robust to an unknown global phase offset between the measurement and the dictionary, a common practical concern. Because it is directly derived from the inner product, which is the core of the ML estimator for additive white Gaussian noise (it is a matched filter), $c^{(k)}_{\\text{cplx}}$ is a statistically well-motivated and near-optimal statistic.\n\n**Analysis of the Magnitude Data Model**\n\nThe magnitude reconstruction is $m_t = |y_t| = \\sqrt{y_{I,t}^2 + y_{Q,t}^2}$. The distribution of the square root of the sum of squares of two independent Gaussian random variables with variance $\\sigma^2$ and means $s_{I,t}$ and $s_{Q,t}$ is the **Rician distribution**.\nThe parameters of this distribution are:\n- **Non-centrality parameter $\\nu_t$**: This is the magnitude of the mean vector, $\\nu_t = \\sqrt{s_{I,t}^2 + s_{Q,t}^2} = |s_t|$.\n- **Scale parameter $\\sigma$**: This is the standard deviation of the underlying Gaussian components, which is $\\sigma$.\n\nThus, the distribution of the measured magnitude is $m_t \\sim \\text{Rice}(|s_t|, \\sigma)$.\n\n**Properties of the Rician Distribution**\n1.  **Expectation $E[m_t]$**: The expected value of a Rician variable is not equal to the non-centrality parameter $|s_t|$.\n    -   In the low signal-to-noise ratio (SNR) limit, as $|s_t|/\\sigma \\to 0$, the Rician distribution approaches a Rayleigh distribution, and its mean approaches $E[m_t] \\to \\sigma\\sqrt{\\pi/2}$. Since $|s_t| \\approx 0$, this means $E[m_t] > |s_t|$. This creates a **positive bias**; noise does not average to zero but instead creates a \"noise floor\" of positive magnitude.\n    -   In the high SNR limit, as $|s_t|/\\sigma \\to \\infty$, the Rician distribution approaches a Gaussian distribution with mean $|s_t|$, so $E[m_t] \\to |s_t|$. The bias diminishes but remains positive.\n\n2.  **Variance $\\text{Var}(m_t)$**: The variance of a Rician variable is given by $\\text{Var}(m_t) = 2\\sigma^2 + |s_t|^2 - (E[m_t])^2$. This expression clearly shows that the variance **depends on the true signal magnitude $|s_t|$**. The noise on the magnitude signal is therefore not additive and its variance is not constant.\n\n**Maximum Likelihood (ML) Estimation with Magnitude Data**\nThe log-likelihood for observing the magnitude series $\\{m_t\\}$ given a dictionary entry $\\{d^{(k)}_t\\}$ is:\n$$ \\mathcal{L}(k | \\mathbf{m}) = \\sum_{t=1}^T \\log\\left( f(m_t\\big||d^{(k)}_t|, \\sigma) \\right) $$\nwhere $f(x|\\nu,\\sigma) = \\frac{x}{\\sigma^2} \\exp\\left(-\\frac{x^2+\\nu^2}{2\\sigma^2}\\right) I_0\\left(\\frac{x\\nu}{\\sigma^2}\\right)$ is the Rician probability density function and $I_0$ is the modified Bessel function of the first kind. This log-likelihood function is complex and does not simplify to the normalized inner product $c^{(k)}_{\\text{mag}}$. Therefore, using $c^{(k)}_{\\text{mag}}$ is a heuristic, not the ML estimator. The signal-dependent bias and variance of the Rician noise mean that the simple cosine similarity metric is not statistically optimal and can be misleading. For instance, the positive bias can inflate the correlation score for incorrect dictionary entries that have a low-amplitude signal throughout the acquisition, as the noise floor in the measurement will correlate with the dictionary's non-zero baseline.\n\n### Step 2: Option-by-Option Analysis\n\n**A. When $y_t$ has circular complex Gaussian noise with standard deviation $\\sigma$, the marginal distribution of $m_t = |y_t|$ is Rician with noncentrality $\\nu_t = |s_t|$ and scale parameter $\\sigma$, which induces a positive bias $E[m_t] > |s_t|$ at low SNR and an $m_t$ variance that depends on $|s_t|$. This can inflate normalized correlations for dictionary entries with nonzero baseline amplitude. In contrast, with phase-preserved complex data and white Gaussian noise, maximizing a whitened inner product or minimizing squared error aligns with the ML rule, making $c^{(k)}_{\\text{cplx}}$ closely related to an optimal statistic.**\n- This option correctly identifies the distribution of $m_t$ as Rician with the correct parameters.\n- It correctly states that this distribution leads to a positive bias at low SNR and signal-dependent variance.\n- It correctly infers a valid implication: the bias can inflate correlations, degrading matching performance.\n- It correctly contrasts this with the complex data case, where minimizing squared error is the ML principle for white Gaussian noise, and that the inner-product-based score $c^{(k)}_{\\text{cplx}}$ is closely related to this optimal approach.\n- **Verdict: Correct.**\n\n**B. Rician noise in $m_t$ is zero-mean and symmetric about zero, so cosine similarity with magnitude data is unbiased. Complex Gaussian noise negates this property because random phase fluctuates the sign of $y_t$, biasing $c^{(k)}_{\\text{cplx}}$ downward.**\n- The Rician distribution is for a magnitude, which is always non-negative ($m_t \\ge 0$), so it cannot be zero-mean or symmetric about zero. This premise is fundamentally false.\n- Complex Gaussian noise $n_t$ is zero-mean, $E[n_t]=0$, and therefore does not introduce a systematic bias in the inner product calculation, as $E[\\sum (s_t+n_t) \\overline{d^{(k)}_t}] = \\sum s_t \\overline{d^{(k)}_t}$. The statement is incorrect.\n- **Verdict: Incorrect.**\n\n**C. Under Rician noise, the variance of $m_t$ is independent of the true amplitude $|s_t|$, hence the distribution of cosine similarities is invariant across fingerprints; under complex Gaussian noise, the variance depends on $|s_t|$, so whitening is necessary only for phase-preserved data.**\n- The variance of a Rician-distributed variable, $\\text{Var}(m_t)$, explicitly depends on $|s_t|$, as shown in the derivation. The first part of the statement is false.\n- For complex data with additive complex Gaussian noise, the variance of the measurement $y_t$ is the variance of the noise, which is independent of the signal $s_t$. The second part of the statement is also false. The option states the exact opposite of the truth.\n- **Verdict: Incorrect.**\n\n**D. For magnitude data with Rician noise, the exact ML matching reduces to maximizing the unweighted normalized inner product $c^{(k)}_{\\text{mag}}$, so correlation-based matching is optimal; for complex Gaussian data, Euclidean distance matching is strictly suboptimal relative to correlation.**\n- The ML estimator for Rician noise involves the Bessel function $I_0$ and does not simplify to a normalized inner product. Thus, $c^{(k)}_{\\text{mag}}$ is not the optimal ML estimator. The first part is false.\n- For complex Gaussian data, minimizing Euclidean distance is equivalent to the ML rule. It is therefore optimal, not \"strictly suboptimal\". The second part is false.\n- **Verdict: Incorrect.**\n\n**E. Because the magnitude operation removes phase, magnitude time-series cannot be used for fingerprinting at all; only phase-preserved complex data permits meaningful matching, regardless of the noise model.**\n- This is an overstatement and factually incorrect. MRF using magnitude data is a widely used technique. While it may be statistically suboptimal and lose the information contained in the signal phase, the magnitude evolution profile often contains sufficient information to distinguish between different tissues. It is therefore a \"meaningful\" if not optimal approach.\n- **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "4902005"}, {"introduction": "MRF successfully yields maps of tissue parameters like $T_1$ and $T_2$. However, some parameters, like proton density, are often estimated on a relative scale that depends on instrument settings. This final practice addresses the vital step of calibration, showing how to convert a relative measurement into a meaningful, absolute quantity. By deriving a calibration factor for a proton density map [@problem_id:4901961], you will learn how to apply rigorous statistical methods like maximum likelihood estimation to ensure your quantitative imaging results are accurate and comparable across different scans.", "problem": "Magnetic Resonance Fingerprinting (MRF) produces a relative proton density map whose absolute scale is indeterminate because the measured signal amplitude depends on instrument gain and receive sensitivity. In a calibration step, a region of interest consisting of a reference tissue with known true proton density is used to set the global scale. Assume the following physically justified model for the voxelwise estimates within the reference region: for voxels indexed by $i=1,\\ldots,N$,\n\n$$\n\\tilde{\\rho}_{i} = s\\,\\rho_{\\mathrm{ref}}\\,c_{i} + \\epsilon_{i},\n$$\n\nwhere $s>0$ is the global scaling factor that maps true proton density to the MRF-estimated proton density, $\\rho_{\\mathrm{ref}}>0$ is the known true proton density of the reference tissue (constant over the region), $c_{i}>0$ is a known receive sensitivity factor for voxel $i$ obtained from a separate sensitivity map, and $\\epsilon_{i}$ are independent, identically distributed zero-mean Gaussian random variables with variance $\\sigma^{2}$ that model thermal noise in the complex data projected onto magnitude under high signal-to-noise ratio. You may assume $c_{i}$ and $\\rho_{\\mathrm{ref}}$ are known exactly, and $\\sigma^{2}$ is known.\n\nStarting only from the linear dependence of signal on proton density and the Gaussian noise model stated above, derive:\n- the maximum likelihood estimator $\\hat{s}$ for the scaling factor $s$, and\n- the standard deviation $\\operatorname{sd}(\\hat{s})$.\n\nExpress your final answer as a single row matrix $\\bigl(\\hat{s} \\;\\; \\operatorname{sd}(\\hat{s})\\bigr)$. No numerical evaluation is required, and no units are to be reported in the final answer. If you find an expression that simplifies when $c_{i}=1$ for all $i$, do not perform that simplification; report the general formulas in terms of $\\{\\tilde{\\rho}_{i}\\}_{i=1}^{N}$, $\\rho_{\\mathrm{ref}}$, $\\sigma$, and $N$.", "solution": "### Step 1: Extract Givens\nThe problem provides the following model for the voxelwise estimates $\\tilde{\\rho}_{i}$ in a reference region of interest:\n- Model equation: $\\tilde{\\rho}_{i} = s\\,\\rho_{\\mathrm{ref}}\\,c_{i} + \\epsilon_{i}$, for voxels indexed by $i=1,\\ldots,N$.\n- $s$: A positive global scaling factor ($s > 0$). This is the parameter to be estimated.\n- $\\rho_{\\mathrm{ref}}$: The known, positive, true proton density of the reference tissue ($\\rho_{\\mathrm{ref}} > 0$).\n- $c_{i}$: A known, positive receive sensitivity factor for voxel $i$ ($c_{i} > 0$).\n- $\\epsilon_{i}$: Independent, identically distributed (i.i.d.) random variables representing noise.\n- Noise distribution: $\\epsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$, where the mean is $0$ and the variance is $\\sigma^{2}$.\n- Known parameters: $\\rho_{\\mathrm{ref}}$, $\\{c_{i}\\}_{i=1}^{N}$, and $\\sigma^{2}$ are known exactly.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is grounded in the field of Magnetic Resonance Imaging (MRI), specifically Magnetic Resonance Fingerprinting (MRF). The model presented, $\\tilde{\\rho}_{i} = s\\,\\rho_{\\mathrm{ref}}\\,c_{i} + \\epsilon_{i}$, is a standard linear signal model with additive Gaussian noise. The assumption of Gaussian noise for magnitude MRI data at high signal-to-noise ratio is a common and valid approximation. The procedure of using a reference tissue with known properties to calibrate a measurement is a standard technique in quantitative imaging. The setup is scientifically sound and realistic.\n2.  **Well-Posed**: The problem asks for the maximum likelihood estimator (MLE) of a parameter in a linear Gaussian model and its standard deviation. This is a classic, well-defined problem in statistical estimation theory that admits a unique and stable solution.\n3.  **Objective**: The problem is stated using precise mathematical language. All variables and constants are defined, and the statistical properties of the noise are specified. There are no subjective or ambiguous terms.\n4.  **No other flaws detected**: The problem is complete, self-contained, and free from contradictions or unrealistic conditions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Derivation of the Maximum Likelihood Estimator $\\hat{s}$\n\nThe model for a single observation $\\tilde{\\rho}_{i}$ is given by $\\tilde{\\rho}_{i} = s\\,\\rho_{\\mathrm{ref}}\\,c_{i} + \\epsilon_{i}$. Since $\\epsilon_{i}$ are i.i.d. Gaussian random variables with mean $0$ and variance $\\sigma^2$, i.e., $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$, each observation $\\tilde{\\rho}_i$ is also a Gaussian random variable. Its distribution is conditioned on the parameter $s$. The mean of $\\tilde{\\rho}_i$ is $E[\\tilde{\\rho}_i] = E[s\\,\\rho_{\\mathrm{ref}}\\,c_{i} + \\epsilon_{i}] = s\\,\\rho_{\\mathrm{ref}}\\,c_{i} + E[\\epsilon_{i}] = s\\,\\rho_{\\mathrm{ref}}\\,c_{i}$. The variance is $\\operatorname{Var}(\\tilde{\\rho}_i) = \\operatorname{Var}(s\\,\\rho_{\\mathrm{ref}}\\,c_{i} + \\epsilon_{i}) = \\operatorname{Var}(\\epsilon_{i}) = \\sigma^2$.\nThus, the distribution of each observation is $\\tilde{\\rho}_i \\sim \\mathcal{N}(s\\,\\rho_{\\mathrm{ref}}\\,c_{i}, \\sigma^2)$.\n\nThe probability density function (PDF) for a single observation $\\tilde{\\rho}_i$ is:\n$$\np(\\tilde{\\rho}_{i} | s; \\rho_{\\mathrm{ref}}, c_i, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(\\tilde{\\rho}_{i} - s\\,\\rho_{\\mathrm{ref}}\\,c_{i})^2}{2\\sigma^2} \\right)\n$$\nSince the noise terms $\\epsilon_i$ are independent, the observations $\\tilde{\\rho}_i$ for $i=1,\\ldots,N$ are also independent. The likelihood function $L(s)$ for the set of $N$ observations is the product of the individual PDFs:\n$$\nL(s) = \\prod_{i=1}^{N} p(\\tilde{\\rho}_{i} | s) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{N/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (\\tilde{\\rho}_{i} - s\\,\\rho_{\\mathrm{ref}}\\,c_{i})^2 \\right)\n$$\nTo find the maximum likelihood estimator (MLE) $\\hat{s}$, we maximize $L(s)$ with respect to $s$. It is equivalent and simpler to maximize the log-likelihood function, $\\mathcal{L}(s) = \\ln L(s)$:\n$$\n\\mathcal{L}(s) = \\ln\\left[ \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{N/2} \\right] - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (\\tilde{\\rho}_{i} - s\\,\\rho_{\\mathrm{ref}}\\,c_{i})^2\n$$\nTo find the maximum, we take the derivative of $\\mathcal{L}(s)$ with respect to $s$ and set it to zero.\n$$\n\\frac{d\\mathcal{L}(s)}{ds} = \\frac{d}{ds} \\left[ -\\frac{N}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (\\tilde{\\rho}_{i} - s\\,\\rho_{\\mathrm{ref}}\\,c_{i})^2 \\right]\n$$\n$$\n\\frac{d\\mathcal{L}(s)}{ds} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} 2 (\\tilde{\\rho}_{i} - s\\,\\rho_{\\mathrm{ref}}\\,c_{i}) (-\\rho_{\\mathrm{ref}}\\,c_{i}) = \\frac{\\rho_{\\mathrm{ref}}}{\\sigma^2} \\sum_{i=1}^{N} (\\tilde{\\rho}_{i} - s\\,\\rho_{\\mathrm{ref}}\\,c_{i}) c_{i}\n$$\nSetting the derivative to zero to find the estimator $\\hat{s}$:\n$$\n\\frac{\\rho_{\\mathrm{ref}}}{\\sigma^2} \\sum_{i=1}^{N} (\\tilde{\\rho}_{i} - \\hat{s}\\,\\rho_{\\mathrm{ref}}\\,c_{i}) c_{i} = 0\n$$\nSince $\\rho_{\\mathrm{ref}} > 0$ and $\\sigma^2 > 0$, we can simplify:\n$$\n\\sum_{i=1}^{N} (\\tilde{\\rho}_{i} c_i - \\hat{s}\\,\\rho_{\\mathrm{ref}}\\,c_{i}^2) = 0\n$$\n$$\n\\sum_{i=1}^{N} \\tilde{\\rho}_{i} c_i = \\hat{s}\\,\\rho_{\\mathrm{ref}} \\sum_{i=1}^{N} c_{i}^2\n$$\nSolving for $\\hat{s}$:\n$$\n\\hat{s} = \\frac{\\sum_{i=1}^{N} \\tilde{\\rho}_{i} c_i}{\\rho_{\\mathrm{ref}} \\sum_{i=1}^{N} c_{i}^2}\n$$\nTo confirm this is a maximum, we check the second derivative:\n$$\n\\frac{d^2\\mathcal{L}(s)}{ds^2} = \\frac{d}{ds} \\left[ \\frac{\\rho_{\\mathrm{ref}}}{\\sigma^2} \\sum_{i=1}^{N} (\\tilde{\\rho}_{i}c_i - s\\,\\rho_{\\mathrm{ref}}\\,c_i^2) \\right] = -\\frac{\\rho_{\\mathrm{ref}}^2}{\\sigma^2} \\sum_{i=1}^{N} c_i^2\n$$\nSince $\\rho_{\\mathrm{ref}} > 0$ and $c_i > 0$, the sum $\\sum_{i=1}^{N} c_i^2$ is positive. Thus, the second derivative is negative, which confirms that $\\hat{s}$ corresponds to a maximum of the likelihood function.\n\n### Derivation of the Standard Deviation $\\operatorname{sd}(\\hat{s})$\n\nFirst, we compute the variance of the estimator, $\\operatorname{Var}(\\hat{s})$. The estimator $\\hat{s}$ is a linear combination of the random variables $\\tilde{\\rho}_i$.\n$$\n\\operatorname{Var}(\\hat{s}) = \\operatorname{Var}\\left( \\frac{\\sum_{i=1}^{N} \\tilde{\\rho}_{i} c_i}{\\rho_{\\mathrm{ref}} \\sum_{j=1}^{N} c_{j}^2} \\right)\n$$\nThe denominator is a constant with respect to the random variables. Using the property $\\operatorname{Var}(aX) = a^2 \\operatorname{Var}(X)$ for a constant $a$:\n$$\n\\operatorname{Var}(\\hat{s}) = \\frac{1}{\\left(\\rho_{\\mathrm{ref}} \\sum_{j=1}^{N} c_{j}^2\\right)^2} \\operatorname{Var}\\left( \\sum_{i=1}^{N} c_i \\tilde{\\rho}_{i} \\right)\n$$\nSince the observations $\\tilde{\\rho}_i$ are independent, the variance of their weighted sum is the weighted sum of their variances:\n$$\n\\operatorname{Var}\\left( \\sum_{i=1}^{N} c_i \\tilde{\\rho}_{i} \\right) = \\sum_{i=1}^{N} \\operatorname{Var}(c_i \\tilde{\\rho}_{i}) = \\sum_{i=1}^{N} c_i^2 \\operatorname{Var}(\\tilde{\\rho}_{i})\n$$\nAs established earlier, $\\operatorname{Var}(\\tilde{\\rho}_i) = \\sigma^2$ for all $i$.\n$$\n\\operatorname{Var}\\left( \\sum_{i=1}^{N} c_i \\tilde{\\rho}_{i} \\right) = \\sum_{i=1}^{N} c_i^2 \\sigma^2 = \\sigma^2 \\sum_{i=1}^{N} c_i^2\n$$\nSubstituting this back into the expression for $\\operatorname{Var}(\\hat{s})$:\n$$\n\\operatorname{Var}(\\hat{s}) = \\frac{1}{\\rho_{\\mathrm{ref}}^2 \\left(\\sum_{j=1}^{N} c_{j}^2\\right)^2} \\left( \\sigma^2 \\sum_{i=1}^{N} c_i^2 \\right) = \\frac{\\sigma^2 \\sum_{i=1}^{N} c_i^2}{\\rho_{\\mathrm{ref}}^2 \\left(\\sum_{i=1}^{N} c_{i}^2\\right)^2}\n$$\n$$\n\\operatorname{Var}(\\hat{s}) = \\frac{\\sigma^2}{\\rho_{\\mathrm{ref}}^2 \\sum_{i=1}^{N} c_{i}^2}\n$$\nThe standard deviation is the square root of the variance.\n$$\n\\operatorname{sd}(\\hat{s}) = \\sqrt{\\operatorname{Var}(\\hat{s})} = \\sqrt{\\frac{\\sigma^2}{\\rho_{\\mathrm{ref}}^2 \\sum_{i=1}^{N} c_{i}^2}}\n$$\nSince $\\sigma>0$ and $\\rho_{\\mathrm{ref}}>0$, we have:\n$$\n\\operatorname{sd}(\\hat{s}) = \\frac{\\sigma}{\\rho_{\\mathrm{ref}} \\sqrt{\\sum_{i=1}^{N} c_{i}^2}}\n$$\nThe two derived quantities are the MLE $\\hat{s}$ and its standard deviation $\\operatorname{sd}(\\hat{s})$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sum_{i=1}^{N} \\tilde{\\rho}_{i} c_i}{\\rho_{\\mathrm{ref}} \\sum_{i=1}^{N} c_{i}^2} & \\frac{\\sigma}{\\rho_{\\mathrm{ref}} \\sqrt{\\sum_{i=1}^{N} c_{i}^2}}\n\\end{pmatrix}\n}\n$$", "id": "4901961"}]}