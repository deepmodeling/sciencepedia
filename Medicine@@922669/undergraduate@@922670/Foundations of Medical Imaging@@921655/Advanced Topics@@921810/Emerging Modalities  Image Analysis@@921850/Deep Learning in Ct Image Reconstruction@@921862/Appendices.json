{"hands_on_practices": [{"introduction": "To effectively train a neural network for CT reconstruction, the loss function must reflect the underlying physics of the imaging process. In low-dose CT, photon counting is well-described by Poisson statistics. This exercise [@problem_id:4875558] provides essential practice in connecting this physical model to the optimization algorithm by having you derive the gradient of the Poisson negative log-likelihood loss, a cornerstone of many modern reconstruction methods.", "problem": "Consider a transmission Computed Tomography (CT) system in which the detector photon counts for each ray bin are modeled as independent Poisson random variables. For a given ray bin indexed by $i$, the observed raw count is $y_i \\in \\mathbb{N}$ and the corresponding expected photon count is $\\lambda_i  0$. A deep neural network is trained directly on the raw counts $y_i$ to predict the expected count for each bin. To guarantee positivity, the network outputs an unconstrained real number $s_i \\in \\mathbb{R}$ and the predicted expected count is defined by $\\hat{\\lambda}_i = \\exp(s_i)$. Assume the measurements are independent across bins and the training objective is the negative log-likelihood (NLL) under the Poisson model, summed over bins, ignoring additive constants that do not depend on the network outputs.\n\nStarting from the Poisson probability mass function $p(y_i \\mid \\lambda_i) = \\exp(-\\lambda_i)\\,\\lambda_i^{y_i}/y_i!$ and the definition of the negative log-likelihood, derive the gradient of the total NLL with respect to the network output $s_i$ for an arbitrary bin $i$, expressed only in terms of $s_i$ and $y_i$. Provide your final gradient as a single closed-form analytic expression. No numerical approximation is required, and no units are needed.", "solution": "The objective is to derive the gradient of the total negative log-likelihood (NLL) with respect to the unconstrained network output $s_i$ for a single ray bin $i$. The final expression should be in terms of $s_i$ and the observed count $y_i$.\n\nLet the total NLL be denoted by $L$. The problem states that this is the sum of the NLLs for each bin, indexed by $j$.\n$$\nL = \\sum_j L_j\n$$\nwhere $L_j$ is the NLL for bin $j$.\n\nThe gradient of the total NLL with respect to the network output $s_i$ for a specific bin $i$ is given by the partial derivative $\\frac{\\partial L}{\\partial s_i}$. Since the terms $L_j$ for $j \\neq i$ do not depend on $s_i$ (due to the independence of measurements across bins), their derivatives with respect to $s_i$ are zero.\n$$\n\\frac{\\partial L}{\\partial s_i} = \\frac{\\partial}{\\partial s_i} \\left( \\sum_j L_j \\right) = \\sum_j \\frac{\\partial L_j}{\\partial s_i} = \\frac{\\partial L_i}{\\partial s_i}\n$$\nThus, we only need to compute the derivative of the NLL for bin $i$.\n\nThe likelihood of observing a count $y_i$ for a given predicted mean $\\hat{\\lambda}_i$ is governed by the Poisson probability mass function (PMF):\n$$\nP(y_i \\mid \\hat{\\lambda}_i) = \\frac{\\exp(-\\hat{\\lambda}_i) \\hat{\\lambda}_i^{y_i}}{y_i!}\n$$\nThe log-likelihood, $\\ell_i$, is the natural logarithm of the PMF:\n$$\n\\ell_i(\\hat{\\lambda}_i) = \\ln \\left( \\frac{\\exp(-\\hat{\\lambda}_i) \\hat{\\lambda}_i^{y_i}}{y_i!} \\right) = \\ln(\\exp(-\\hat{\\lambda}_i)) + \\ln(\\hat{\\lambda}_i^{y_i}) - \\ln(y_i!)\n$$\n$$\n\\ell_i(\\hat{\\lambda}_i) = -\\hat{\\lambda}_i + y_i \\ln(\\hat{\\lambda}_i) - \\ln(y_i!)\n$$\nThe negative log-likelihood, $L_i$, is simply $-\\ell_i$:\n$$\nL_i(\\hat{\\lambda}_i) = \\hat{\\lambda}_i - y_i \\ln(\\hat{\\lambda}_i) + \\ln(y_i!)\n$$\nThe term $\\ln(y_i!)$ is a constant with respect to the network's parameters, as $y_i$ is a fixed observation. We can disregard it for optimization, but we will retain it here for completeness; it will vanish upon differentiation with respect to any network parameter.\n\nTo find the gradient of $L_i$ with respect to $s_i$, we must apply the chain rule, as $L_i$ depends on $\\hat{\\lambda}_i$, which in turn depends on $s_i$:\n$$\n\\frac{\\partial L_i}{\\partial s_i} = \\frac{\\partial L_i}{\\partial \\hat{\\lambda}_i} \\frac{\\partial \\hat{\\lambda}_i}{\\partial s_i}\n$$\nFirst, we compute the derivative of $L_i$ with respect to $\\hat{\\lambda}_i$:\n$$\n\\frac{\\partial L_i}{\\partial \\hat{\\lambda}_i} = \\frac{\\partial}{\\partial \\hat{\\lambda}_i} \\left( \\hat{\\lambda}_i - y_i \\ln(\\hat{\\lambda}_i) + \\ln(y_i!) \\right) = 1 - y_i \\frac{1}{\\hat{\\lambda}_i} + 0 = 1 - \\frac{y_i}{\\hat{\\lambda}_i}\n$$\nNext, we compute the derivative of $\\hat{\\lambda}_i$ with respect to $s_i$. The problem defines the relationship as $\\hat{\\lambda}_i = \\exp(s_i)$.\n$$\n\\frac{\\partial \\hat{\\lambda}_i}{\\partial s_i} = \\frac{\\partial}{\\partial s_i} (\\exp(s_i)) = \\exp(s_i)\n$$\nBy the definition provided, we also have $\\exp(s_i) = \\hat{\\lambda}_i$. Thus:\n$$\n\\frac{\\partial \\hat{\\lambda}_i}{\\partial s_i} = \\hat{\\lambda}_i\n$$\nNow, we substitute these two components back into the chain rule expression:\n$$\n\\frac{\\partial L_i}{\\partial s_i} = \\left( 1 - \\frac{y_i}{\\hat{\\lambda}_i} \\right) \\cdot \\hat{\\lambda}_i = \\hat{\\lambda}_i - y_i\n$$\nThe final step is to express this gradient solely in terms of the network output $s_i$ and the observed data $y_i$. We substitute $\\hat{\\lambda}_i = \\exp(s_i)$ into the expression for the gradient.\n$$\n\\frac{\\partial L}{\\partial s_i} = \\frac{\\partial L_i}{\\partial s_i} = \\exp(s_i) - y_i\n$$\nThis is the final closed-form analytic expression for the gradient of the total NLL with respect to the network output $s_i$ for an arbitrary bin $i$.", "answer": "$$\n\\boxed{\\exp(s_i) - y_i}\n$$", "id": "4875558"}, {"introduction": "Deep learning models for CT must produce images where pixel values correspond to accurate physical densities (Hounsfield Units). However, standard architectural components like Batch Normalization can introduce instabilities that compromise this quantitative accuracy, particularly when memory constraints necessitate small batch sizes. This practice [@problem_id:4875577] challenges you to analyze this common problem and identify more robust normalization strategies suited for quantitative medical imaging.", "problem": "A convolutional unrolled reconstruction network is trained to recover an attenuation map $x$ from low-dose Computed Tomography (CT) sinograms $y$. The forward model for a single ray follows the Beer–Lambert law $I = I_0 \\exp\\left(-\\int \\mu(\\ell) \\, d\\ell\\right)$, where $I_0$ is the incident photon count and $\\mu$ is the linear attenuation coefficient; measurements are subject to Poisson shot noise. After log-transformation, the sinogram $s = -\\ln(y/I_0)$ is approximately a noisy linear projection $s \\approx A x + \\eta$ with heteroscedastic noise $\\eta$ whose variance decreases with increasing $I_0$. The network employs Batch Normalization (BN) in several convolutional blocks, defined for a feature $z$ by\n$$\nz' = \\gamma \\frac{z - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} + \\beta,\n$$\nwhere $\\mu_B$ and $\\sigma_B^2$ are the per-batch mean and variance, and $(\\gamma,\\beta)$ are learned affine parameters. Due to memory constraints, training uses very small mini-batches, $B = 2$, with slices spanning different anatomical regions and dose levels. Empirically, the reconstructions exhibit fluctuating contrast in the liver parenchyma across iterations.\n\nWhich statements best capture the effect of BN on the scale of intermediate features in this low-dose CT reconstruction setting, and identify suitable alternatives when the batch size is small? Select all that apply.\n\nA. In low-dose CT, per-batch estimates $\\mu_B$ and $\\sigma_B^2$ can be noisy when $B$ is small and the data are heterogeneous, so BN can introduce random rescaling of features across iterations and samples; Group Normalization (GN) mitigates this by computing statistics per sample over channels and spatial positions, making it less sensitive to $B$.\n\nB. BN preserves absolute Hounsfield Unit (HU) scale in the reconstructed image regardless of batch size, because $(\\gamma,\\beta)$ exactly undo the normalization and make the output invariant to $\\mu_B$ and $\\sigma_B^2$.\n\nC. Instance Normalization (IN) is a safe alternative that preserves contrast and absolute scale, since it normalizes within each instance and therefore retains the global mean intensity of each image.\n\nD. Layer Normalization (LN) and Group Normalization (GN) are preferable at small $B$ because they compute statistics within each sample and avoid cross-sample leakage; they stabilize feature scaling but may still alter absolute intensity, so including explicit scale-preserving objectives on Hounsfield Units (HU) or avoiding normalization in the final reconstruction blocks is prudent.\n\nE. The variance of BN’s sample variance estimator decreases as $\\mathcal{O}\\!\\left((B-1)^{-1}\\right)$, so with $B = 2$ it is large; this instability is fully eliminated by using running statistics only at inference during training.\n\nF. Weight Standardization, applied to convolutional kernels, in combination with Group Normalization (GN), can stabilize training for small batches by decoupling kernel scale from activation statistics and reducing sensitivity to $B$.", "solution": "The core issue is the instability of Batch Normalization (BN) when training with very small mini-batches ($B=2$) containing heterogeneous data (e.g., different anatomical regions and dose levels). BN computes feature statistics (mean $\\mu_B$, variance $\\sigma_B^2$) across all samples in a batch. When $B=2$, these statistics are extremely noisy estimates of the true population statistics. The normalization applied to one sample becomes heavily dependent on the features of the other, randomly paired sample. This introduces a random, iteration-dependent scaling and shifting of features. In a quantitative imaging task like CT reconstruction, where pixel values must correspond to stable physical densities (Hounsfield Units), this random rescaling leads to the observed \"fluctuating contrast\" and degrades performance.\n\nThis analysis leads to the evaluation of each statement:\n\n**A. Correct.** This statement accurately diagnoses the problem: with a small, heterogeneous batch, the batch statistics $\\mu_B$ and $\\sigma_B^2$ are noisy and introduce random feature rescaling. It correctly identifies Group Normalization (GN) as a solution. Since GN computes statistics within a single sample (across groups of channels), its calculations are independent of the batch size $B$ and the content of other samples, thus eliminating the source of the instability.\n\n**B. Incorrect.** The learned affine parameters $(\\gamma,\\beta)$ in BN do not make the layer's output invariant to the batch statistics $\\mu_B$ and $\\sigma_B^2$. Instead, they learn an optimal scaling and shifting *after* normalization. The entire training process, including the gradients, is affected by the noisy batch statistics. Therefore, BN does not preserve the absolute HU scale, especially when trained with small batches.\n\n**C. Incorrect.** Instance Normalization (IN) normalizes features for each channel independently within each sample. This process removes instance-specific mean and variance, a form of contrast normalization. This is detrimental for quantitative CT, as it discards the absolute intensity information (HU value) which is physically meaningful. While useful in style transfer applications, it is unsuitable here.\n\n**D. Correct.** Layer Normalization (LN) and Group Normalization (GN) are preferable for small batch sizes because they compute statistics within each sample, avoiding the cross-sample dependency that destabilizes BN. The statement astutely notes that any normalization layer can still alter the absolute intensity of features. Therefore, it is a prudent and common practice in quantitative imaging to add an explicit scale-preserving loss term or to avoid normalization in the final layers of the network to allow a direct mapping to absolute HU values.\n\n**E. Incorrect.** While the statistical claim about the variance of the variance estimator is correct, the proposed solution is flawed. The training instability arises from using noisy batch statistics to compute gradients *during training*. Using running statistics is the standard procedure for the *inference* phase, not the training phase. Applying them during training would change the algorithm entirely and remove the adaptive nature of BN, failing to address the fundamental problem of unstable gradient calculations with small batches.\n\n**F. Correct.** Weight Standardization (WS), which normalizes the weights of convolutional layers, is a technique known to stabilize training. When combined with a batch-independent activation normalization like Group Normalization (GN), it is particularly effective for small batch sizes. WS helps control the gradients by standardizing the weights, while GN stably normalizes the activations. This combination decouples kernel scale from activation statistics and effectively reduces sensitivity to batch size.", "answer": "$$\\boxed{ADF}$$", "id": "4875577"}, {"introduction": "Does a higher Peak Signal-to-Noise Ratio (PSNR) always signify a better medical image? The goal of reconstruction is ultimately to enhance clinical diagnosis, which may not always align with simple pixel-wise fidelity metrics. This exercise [@problem_id:4875595] guides you through a concrete calculation of PSNR and the Structural Similarity Index Measure (SSIM) to reveal their potential shortcomings, fostering a deeper understanding of task-based image quality assessment.", "problem": "A researcher evaluates two deep learning reconstructions of a small computed tomography (CT) image patch. Pixel intensities are normalized to the range $[0,1]$. The ground-truth patch $G \\in \\mathbb{R}^{4 \\times 4}$ and two reconstructions $R_1, R_2 \\in \\mathbb{R}^{4 \\times 4}$ are given by\n$$\nG \\;=\\;\n\\begin{bmatrix}\n0  0  1  1 \\\\\n0  0  1  1 \\\\\n0  0  1  1 \\\\\n0  0  1  1\n\\end{bmatrix},\\quad\nR_1 \\;=\\;\n\\begin{bmatrix}\n0.2  0.2  0.8  0.8 \\\\\n0.2  0.2  0.8  0.8 \\\\\n0.2  0.2  0.8  0.8 \\\\\n0.2  0.2  0.8  0.8\n\\end{bmatrix},\\quad\nR_2 \\;=\\;\n\\begin{bmatrix}\n0    0.2  1    0.8 \\\\\n0    0.2  1    0.8 \\\\\n0    0.2  1    0.8 \\\\\n0    0.2  1    0.8\n\\end{bmatrix}.\n$$\nAssume the peak signal value is $L=1$. Use the standard definition of mean squared error (MSE) over all $N=16$ pixels and the standard definition of Peak Signal-to-Noise Ratio (PSNR). For the Structural Similarity Index Measure (SSIM), use a single-window computation over the full $4 \\times 4$ patch with population statistics (divide by $N$ for variance and covariance), and constants $C_1=(0.01)^2$ and $C_2=(0.03)^2$.\n\nBased on correct computations of PSNR and SSIM for $R_1$ and $R_2$, and on reasoning grounded in task-based image quality for clinical decision making (e.g., detection of small, high-contrast structures), which of the following statements are true?\n\nA. The Peak Signal-to-Noise Ratio (PSNR) of $R_2$ is approximately $17\\,\\mathrm{dB}$ and exceeds that of $R_1$ (approximately $14\\,\\mathrm{dB}$).\n\nB. The Structural Similarity Index Measure (SSIM) of $R_1$ exceeds that of $R_2$ because $R_1$ better matches global brightness and contrast.\n\nC. Despite differences in PSNR and SSIM, either PSNR or SSIM alone can guarantee better clinical lesion-detection performance for any deep learning reconstruction.\n\nD. For clinical tasks sensitive to high-frequency features, a reconstruction like $R_1$ can underperform $R_2$ even if their PSNR were matched, because smoothing degrades task-relevant structure.\n\nE. With the stated constants, SSIM is approximately $0.88$ for $R_1$ and approximately $0.95$ for $R_2$.", "solution": "We proceed from standard definitions. The mean squared error (MSE) between an image $X$ and the ground truth $G$ over $N$ pixels is\n$$\n\\mathrm{MSE}(X,G) \\;=\\; \\frac{1}{N} \\sum_{i=1}^{N} \\left(X_i - G_i\\right)^2.\n$$\nThe Peak Signal-to-Noise Ratio (PSNR) in decibels for peak value $L$ is\n$$\n\\mathrm{PSNR}(X,G) \\;=\\; 10 \\log_{10}\\!\\left(\\frac{L^2}{\\mathrm{MSE}(X,G)}\\right).\n$$\nThe Structural Similarity Index Measure (SSIM) between $X$ and $G$, computed over the full patch with population statistics, is\n$$\n\\mathrm{SSIM}(X,G) \\;=\\; \\frac{\\left(2 \\mu_X \\mu_G + C_1\\right)\\left(2 \\sigma_{XG} + C_2\\right)}{\\left(\\mu_X^2 + \\mu_G^2 + C_1\\right)\\left(\\sigma_X^2 + \\sigma_G^2 + C_2\\right)},\n$$\nwhere $\\mu_X$ and $\\mu_G$ are the means, $\\sigma_X^2$ and $\\sigma_G^2$ are the variances (dividing by $N$), and $\\sigma_{XG}$ is the covariance (dividing by $N$). We take $C_1=(0.01)^2=0.0001$ and $C_2=(0.03)^2=0.0009$, and $L=1$.\n\nStep 1: Compute MSE and PSNR.\n\n- For $R_1$ versus $G$: Each $0$ in $G$ is approximated by $0.2$ in $R_1$ (error $0.2$), and each $1$ is approximated by $0.8$ (error $-0.2$). Squared error per pixel is $0.04$ for all $N=16$ pixels. Hence\n$$\n\\mathrm{MSE}(R_1,G) \\;=\\; \\frac{16 \\cdot 0.04}{16} \\;=\\; 0.04.\n$$\nThen\n$$\n\\mathrm{PSNR}(R_1,G) \\;=\\; 10 \\log_{10}\\!\\left(\\frac{1}{0.04}\\right) \\;=\\; 10 \\log_{10}(25) \\;\\approx\\; 13.98\\,\\mathrm{dB}.\n$$\n\n- For $R_2$ versus $G$: Eight pixels match exactly ($0 \\rightarrow 0$ and $1 \\rightarrow 1$, squared error $0$), and eight are off by $\\pm 0.2$ ($0 \\rightarrow 0.2$ and $1 \\rightarrow 0.8$, squared error $0.04$). Thus\n$$\n\\mathrm{MSE}(R_2,G) \\;=\\; \\frac{8 \\cdot 0 + 8 \\cdot 0.04}{16} \\;=\\; 0.02,\n$$\nand\n$$\n\\mathrm{PSNR}(R_2,G) \\;=\\; 10 \\log_{10}\\!\\left(\\frac{1}{0.02}\\right) \\;=\\; 10 \\log_{10}(50) \\;\\approx\\; 16.99\\,\\mathrm{dB}.\n$$\n\nTherefore, $\\mathrm{PSNR}(R_2,G) \\approx 16.99\\,\\mathrm{dB}$ exceeds $\\mathrm{PSNR}(R_1,G) \\approx 13.98\\,\\mathrm{dB}$.\n\nStep 2: Compute SSIM components.\n\nFirst, compute ground-truth statistics. For $G$, there are eight zeros and eight ones:\n- Mean:\n$$\n\\mu_G \\;=\\; \\frac{8 \\cdot 0 + 8 \\cdot 1}{16} \\;=\\; 0.5.\n$$\n- Variance (population):\n$$\n\\sigma_G^2 \\;=\\; \\frac{8 \\cdot (0-0.5)^2 + 8 \\cdot (1-0.5)^2}{16} \\;=\\; \\frac{8 \\cdot 0.25 + 8 \\cdot 0.25}{16} \\;=\\; 0.25.\n$$\n\nNow for $R_1$: eight values are $0.2$, eight are $0.8$.\n- Mean:\n$$\n\\mu_{R_1} \\;=\\; \\frac{8 \\cdot 0.2 + 8 \\cdot 0.8}{16} \\;=\\; 0.5.\n$$\n- Variance:\n$$\n\\sigma_{R_1}^2 \\;=\\; \\frac{8 \\cdot (0.2-0.5)^2 + 8 \\cdot (0.8-0.5)^2}{16} \\;=\\; \\frac{8 \\cdot 0.09 + 8 \\cdot 0.09}{16} \\;=\\; 0.09.\n$$\n- Covariance with $G$:\nFor each zero location, $(G-\\mu_G) = -0.5$ and $(R_1-\\mu_{R_1}) = -0.3$ gives product $0.15$ (eight times); for each one location, $(G-\\mu_G) = +0.5$ and $(R_1-\\mu_{R_1}) = +0.3$ gives product $0.15$ (eight times). Hence\n$$\n\\sigma_{R_1,G} \\;=\\; \\frac{8 \\cdot 0.15 + 8 \\cdot 0.15}{16} \\;=\\; 0.15.\n$$\nNow compute $\\mathrm{SSIM}(R_1,G)$:\n- Luminance term:\n$$\n\\frac{2 \\mu_{R_1} \\mu_G + C_1}{\\mu_{R_1}^2 + \\mu_G^2 + C_1} \\;=\\; \\frac{2 \\cdot 0.5 \\cdot 0.5 + 0.0001}{0.5^2 + 0.5^2 + 0.0001} \\;=\\; \\frac{0.5001}{0.5001} \\;=\\; 1.\n$$\n- Contrast-structure term:\n$$\n\\frac{2 \\sigma_{R_1,G} + C_2}{\\sigma_{R_1}^2 + \\sigma_G^2 + C_2} \\;=\\; \\frac{2 \\cdot 0.15 + 0.0009}{0.09 + 0.25 + 0.0009} \\;=\\; \\frac{0.3009}{0.3409} \\;\\approx\\; 0.883.\n$$\nThus\n$$\n\\mathrm{SSIM}(R_1,G) \\;\\approx\\; 0.883.\n$$\n\nFor $R_2$: the values are four $0$, four $0.2$, four $0.8$, four $1.0$.\n- Mean:\n$$\n\\mu_{R_2} \\;=\\; \\frac{4 \\cdot 0 + 4 \\cdot 0.2 + 4 \\cdot 0.8 + 4 \\cdot 1}{16} \\;=\\; 0.5.\n$$\n- Variance:\n$$\n\\sigma_{R_2}^2 \\;=\\; \\frac{4 \\cdot (0-0.5)^2 + 4 \\cdot (0.2-0.5)^2 + 4 \\cdot (0.8-0.5)^2 + 4 \\cdot (1-0.5)^2}{16}\n$$\n$$\n\\;=\\; \\frac{4 \\cdot 0.25 + 4 \\cdot 0.09 + 4 \\cdot 0.09 + 4 \\cdot 0.25}{16} \\;=\\; \\frac{2.72}{16} \\;=\\; 0.17.\n$$\n- Covariance with $G$:\nProducts $(G-\\mu_G)(R_2-\\mu_{R_2})$ are $0.25$ for exact matches ($0$ with $0$, $1$ with $1$) and $0.15$ for the perturbed cases ($0$ with $0.2$, $1$ with $0.8$), each occurring four times per half. Thus\n$$\n\\sigma_{R_2,G} \\;=\\; \\frac{4 \\cdot 0.25 + 4 \\cdot 0.15 + 4 \\cdot 0.25 + 4 \\cdot 0.15}{16} \\;=\\; \\frac{3.2}{16} \\;=\\; 0.2.\n$$\nNow compute $\\mathrm{SSIM}(R_2,G)$:\n- Luminance term is again\n$$\n\\frac{2 \\mu_{R_2} \\mu_G + C_1}{\\mu_{R_2}^2 + \\mu_G^2 + C_1} \\;=\\; 1.\n$$\n- Contrast-structure term:\n$$\n\\frac{2 \\sigma_{R_2,G} + C_2}{\\sigma_{R_2}^2 + \\sigma_G^2 + C_2} \\;=\\; \\frac{2 \\cdot 0.2 + 0.0009}{0.17 + 0.25 + 0.0009} \\;=\\; \\frac{0.4009}{0.4209} \\;\\approx\\; 0.952.\n$$\nThus\n$$\n\\mathrm{SSIM}(R_2,G) \\;\\approx\\; 0.952.\n$$\n\nStep 3: Relate metrics to clinical task performance.\n\nTask-based image quality in medical imaging emphasizes performance on a specified observer task (for example, detection of a small, high-contrast lesion). Fundamental considerations from detection theory indicate that detectability depends on the preservation of task-relevant signal components and the noise statistics across spatial frequencies. Smoothing operations reduce high-frequency content, potentially diminishing the energy of small structures critical for detection, even if pixel-wise error metrics are maintained or improved. Consequently, higher PSNR or SSIM does not, by itself, guarantee superior clinical task performance.\n\nOption-by-option analysis:\n\n- Option A: Using the computed values, $\\mathrm{PSNR}(R_1,G) \\approx 13.98\\,\\mathrm{dB}$ and $\\mathrm{PSNR}(R_2,G) \\approx 16.99\\,\\mathrm{dB}$. Hence $R_2$ exceeds $R_1$ by approximately $3\\,\\mathrm{dB}$. This statement is Correct.\n\n- Option B: We found $\\mathrm{SSIM}(R_1,G) \\approx 0.883$ and $\\mathrm{SSIM}(R_2,G) \\approx 0.952$. Despite equal means and similar contrast alignment, $R_2$ has higher SSIM due to stronger covariance and adequate variance relative to $G$. The claim that $R_1$ exceeds $R_2$ in SSIM is incorrect. Verdict: Incorrect.\n\n- Option C: No single fidelity metric (PSNR or SSIM) can guarantee better clinical lesion-detection performance across all reconstructions and tasks; task-based assessment is required. This claim of a guarantee is false. Verdict: Incorrect.\n\n- Option D: Smoothing degrades high-frequency content relevant to detecting small structures; even at matched PSNR, a smoothed image can underperform a less-smoothed image on high-frequency detection tasks. This aligns with task-based principles. Verdict: Correct.\n\n- Option E: The computed SSIM values are approximately $0.88$ for $R_1$ and $0.95$ for $R_2$, consistent with our calculations. Verdict: Correct.\n\nTherefore, the correct statements are A, D, and E.", "answer": "$$\\boxed{ADE}$$", "id": "4875595"}]}