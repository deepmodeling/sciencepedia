{"hands_on_practices": [{"introduction": "A primary function of any Computer-Aided Diagnosis (CADx) system is to assign a confidence score that effectively separates positive cases (disease present) from negative ones (disease absent). The Area Under the ROC Curve (AUC) provides a comprehensive, threshold-independent measure of this separability. This practice will solidify your understanding of AUC by guiding you through the derivation of its common non-parametric estimator and the computation of its variance, which are essential skills for rigorously comparing different diagnostic models [@problem_id:4871482].", "problem": "A Computer-Aided Diagnosis (CADx) system produces continuous confidence scores intended to separate patients with disease from those without disease. Consider a binary classification task in which the system is evaluated on a test set comprising $m$ truly positive cases and $n$ truly negative cases. The Receiver Operating Characteristic (ROC) curve is defined as the parametric locus of true positive rate versus false positive rate as the decision threshold varies over the real line. The Area Under the ROC Curve (AUC) is the expected performance measure of interest.\n\nStarting strictly from these foundational definitions, derive the estimator of AUC that coincides with the Mann–Whitney formulation based on pairwise comparisons between scores from the positive and negative classes. Your derivation must begin from the interpretation of the ROC curve for thresholded decisions and conclude with a closed-form empirical estimator in terms of pairwise comparisons, making clear how comparisons contribute when two scores are equal.\n\nThen, using your derived estimator, evaluate the AUC for the following CADx scores. The $m=4$ positive-class scores are $s=(0.92,\\,0.87,\\,0.55,\\,0.60)$ and the $n=5$ negative-class scores are $t=(0.80,\\,0.58,\\,0.40,\\,0.62,\\,0.55)$. Finally, compute the estimated variance of the AUC using DeLong’s method, which models the AUC estimator as a U-statistic and accounts for the correlation induced by reusing the same subjects across all pairwise comparisons. Round your final variance estimate to four significant figures. Express the answer as a decimal without a percentage sign.", "solution": "The problem requires a three-part response: first, a derivation of the Mann-Whitney U-statistic estimator for the Area Under the Receiver Operating Characteristic Curve (AUC) starting from the definition of the ROC curve; second, the calculation of this AUC estimator for a given set of scores; and third, the estimation of the variance of this AUC estimator using DeLong's method.\n\n### 1. Derivation of the AUC Estimator\n\nLet $S$ be the continuous-valued score from a truly positive case, and $T$ be the score from a truly negative case. Let $F_S(s) = P(S \\le s)$ and $F_T(t) = P(T \\le t)$ be their respective cumulative distribution functions (CDFs), and $f_S(s)$ and $f_T(t)$ be their probability density functions (PDFs).\n\nA decision threshold $\\tau$ is used to classify a case as positive if its score exceeds $\\tau$. The True Positive Rate (TPR) and False Positive Rate (FPR) are functions of this threshold:\n$$ TPR(\\tau) = P(S > \\tau) = 1 - F_S(\\tau) $$\n$$ FPR(\\tau) = P(T > \\tau) = 1 - F_T(\\tau) $$\nThe ROC curve is the locus of points $(FPR(\\tau), TPR(\\tau))$ for $\\tau \\in (-\\infty, \\infty)$. It is a plot of $TPR$ as a function of $FPR$. Let $x = FPR(\\tau)$. The ROC curve is the function $y = R(x)$, where $x \\in [0, 1]$.\n\nThe Area Under the ROC Curve (AUC) is defined by the integral of the ROC curve from $FPR=0$ to $FPR=1$:\n$$ AUC = \\int_{0}^{1} R(x) \\, dx $$\nTo evaluate this integral, we change the variable of integration from $x$ to $\\tau$. From $x = FPR(\\tau) = 1 - F_T(\\tau)$, we have $dx = -f_T(\\tau) d\\tau$. The limits of integration also change: as $\\tau$ goes from $-\\infty$ to $+\\infty$, $x$ goes from $1$ to $0$.\nThe function $R(x)$ is $TPR$ expressed in terms of $x$. We have $R(x) = TPR(\\tau) = 1 - F_S(\\tau)$.\nSubstituting these into the integral:\n$$ AUC = \\int_{x=0}^{x=1} R(x) \\, dx = \\int_{\\tau=+\\infty}^{\\tau=-\\infty} (1 - F_S(\\tau)) (-f_T(\\tau)) \\, d\\tau $$\n$$ AUC = \\int_{-\\infty}^{+\\infty} (1 - F_S(\\tau)) f_T(\\tau) \\, d\\tau $$\nThis integral can be interpreted probabilistically. The expression $(1-F_S(\\tau))$ is $P(S>\\tau)$. So, the integral is the expectation of $P(S>\\tau)$ where $\\tau$ is a random variable with PDF $f_T(\\tau)$, which is the distribution of scores for negative cases. This is equivalent to $P(S > T)$.\nLet's confirm this through integration by parts. Let $u = 1 - F_S(\\tau)$ and $dv = f_T(\\tau)d\\tau$. Then $du = -f_S(\\tau)d\\tau$ and $v = F_T(\\tau)$.\n$$ AUC = \\left[ (1 - F_S(\\tau)) F_T(\\tau) \\right]_{-\\infty}^{+\\infty} - \\int_{-\\infty}^{+\\infty} F_T(\\tau) (-f_S(\\tau)) \\, d\\tau $$\nThe boundary term evaluates to $0$ since $F_S(\\infty)=1$, $F_T(-\\infty)=0$.\n$$ AUC = \\int_{-\\infty}^{+\\infty} F_T(\\tau) f_S(\\tau) \\, d\\tau $$\nThis is the expectation of $F_T(S)$, which is $P(T  S)$ or $P(S > T)$ for continuous variables. Thus, the AUC is precisely the probability that a randomly selected positive case will have a higher score than a randomly selected negative case.\n$$ AUC = P(S > T) $$\nGiven a sample of $m$ scores $\\{s_i\\}_{i=1}^m$ from positive cases and $n$ scores $\\{t_j\\}_{j=1}^n$ from negative cases, a non-parametric, unbiased estimator for $AUC = P(S > T)$ is the empirical proportion of pairs $(s_i, t_j)$ for which $s_i > t_j$.\nWe define a comparison function $\\Psi(x, y)$:\n$$ \\Psi(x, y) = \\begin{cases} 1  \\text{if } x > y \\\\ \\frac{1}{2}  \\text{if } x = y \\\\ 0  \\text{if } x  y \\end{cases} $$\nThe inclusion of the $x=y$ case handles ties, which can occur with discrete or rounded scores. Assigning a value of $\\frac{1}{2}$ corresponds to the convention of breaking ties randomly, and ensures the estimator remains unbiased for $P(S > T) + \\frac{1}{2}P(S = T)$.\n\nThe empirical estimator for the AUC, denoted $\\hat{\\theta}_{AUC}$, is the average of $\\Psi(s_i, t_j)$ over all $m \\times n$ pairs:\n$$ \\hat{\\theta}_{AUC} = \\frac{1}{mn} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\Psi(s_i, t_j) $$\nThis formulation is directly related to the Mann-Whitney U-statistic. The statistic $U$ is defined as $U = \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\Psi(s_i, t_j)$. Therefore, the AUC estimator is given by:\n$$ \\hat{\\theta}_{AUC} = \\frac{U}{mn} $$\nThis completes the derivation.\n\n### 2. Calculation of AUC\n\nThe given scores are:\nPositive-class ($m=4$): $s = (0.92, 0.87, 0.55, 0.60)$\nNegative-class ($n=5$): $t = (0.80, 0.58, 0.40, 0.62, 0.55)$\n\nWe compute the sum of pairwise comparisons, $U = \\sum_{i=1}^{4} \\sum_{j=1}^{5} \\Psi(s_i, t_j)$. Let's evaluate the contribution for each positive score $s_i$:\n- For $s_1=0.92$: It is greater than all $5$ negative scores $(0.80, 0.58, 0.40, 0.62, 0.55)$. Contribution is $5 \\times 1 = 5$.\n- For $s_2=0.87$: It is greater than all $5$ negative scores. Contribution is $5 \\times 1 = 5$.\n- For $s_3=0.60$: It is greater than $0.58, 0.40, 0.55$ (3 scores). It is less than $0.80, 0.62$ (2 scores). Contribution is $3 \\times 1 + 2 \\times 0 = 3$.\n- For $s_4=0.55$: It is greater than $0.40$ (1 score). It is less than $0.80, 0.58, 0.62$ (3 scores). It is equal to $0.55$ (1 score). Contribution is $1 \\times 1 + 3 \\times 0 + 1 \\times \\frac{1}{2} = 1.5$.\n\nThe total sum is $U = 5 + 5 + 3 + 1.5 = 14.5$.\nThe AUC estimate is:\n$$ \\hat{\\theta}_{AUC} = \\frac{U}{mn} = \\frac{14.5}{4 \\times 5} = \\frac{14.5}{20} = 0.725 $$\n\n### 3. Variance Estimation using DeLong's Method\n\nDeLong's method treats $\\hat{\\theta}_{AUC}$ as a generalized U-statistic and provides a non-parametric estimate of its variance. The variance is decomposed into contributions from the positive and negative samples.\n\nFirst, we define components for each positive and negative case.\nFor each positive case $i$, its component $V_{10}(s_i)$ is:\n$$ V_{10}(s_i) = \\frac{1}{n} \\sum_{j=1}^{n} \\Psi(s_i, t_j) $$\nFor each negative case $j$, its component $V_{01}(t_j)$ is:\n$$ V_{01}(t_j) = \\frac{1}{m} \\sum_{i=1}^{m} \\Psi(s_i, t_j) $$\n\nLet's compute these components.\nFor the positive-class scores ($m=4$, $n=5$):\n- $V_{10}(0.92) = \\frac{5}{5} = 1$\n- $V_{10}(0.87) = \\frac{5}{5} = 1$\n- $V_{10}(0.60) = \\frac{3}{5} = 0.6$\n- $V_{10}(0.55) = \\frac{1.5}{5} = 0.3$\n\nFor the negative-class scores ($m=4$, $n=5$):\n- For $t_j = 0.80$: $s_i>0.80$ for $s_i \\in \\{0.92, 0.87\\}$. Thus, $\\sum_i \\Psi(s_i, 0.80) = 2$. $V_{01}(0.80) = \\frac{2}{4} = 0.5$.\n- For $t_j = 0.58$: $s_i>0.58$ for $s_i \\in \\{0.92, 0.87, 0.60\\}$. Thus, $\\sum_i \\Psi(s_i, 0.58) = 3$. $V_{01}(0.58) = \\frac{3}{4} = 0.75$.\n- For $t_j = 0.40$: $s_i>0.40$ for all $4$ positive scores. Thus, $\\sum_i \\Psi(s_i, 0.40) = 4$. $V_{01}(0.40) = \\frac{4}{4} = 1$.\n- For $t_j = 0.62$: $s_i>0.62$ for $s_i \\in \\{0.92, 0.87\\}$. Thus, $\\sum_i \\Psi(s_i, 0.62) = 2$. $V_{01}(0.62) = \\frac{2}{4} = 0.5$.\n- For $t_j = 0.55$: $s_i>0.55$ for $s_i \\in \\{0.92, 0.87, 0.60\\}$. $s_i=0.55$ for one case. Thus, $\\sum_i \\Psi(s_i, 0.55) = 3 \\times 1 + 1 \\times 0.5 = 3.5$. $V_{01}(0.55) = \\frac{3.5}{4} = 0.875$.\n\nThe estimated variance of $\\hat{\\theta}_{AUC}$ is given by:\n$$ \\widehat{Var}(\\hat{\\theta}_{AUC}) = \\frac{1}{m} S_{10} + \\frac{1}{n} S_{01} $$\nwhere $S_{10}$ and $S_{01}$ are the sample variances of the components:\n$$ S_{10} = \\frac{1}{m-1} \\sum_{i=1}^{m} (V_{10}(s_i) - \\hat{\\theta}_{AUC})^2 $$\n$$ S_{01} = \\frac{1}{n-1} \\sum_{j=1}^{n} (V_{01}(t_j) - \\hat{\\theta}_{AUC})^2 $$\n\nNow, we compute $S_{10}$ and $S_{01}$. We have $\\hat{\\theta}_{AUC}=0.725$.\nFor $S_{10}$ ($m=4$):\nThe $V_{10}$ values are $(1, 1, 0.6, 0.3)$.\n$$ \\sum_{i=1}^{4} (V_{10}(s_i) - 0.725)^2 = (1-0.725)^2 + (1-0.725)^2 + (0.6-0.725)^2 + (0.3-0.725)^2 $$\n$$ = (0.275)^2 + (0.275)^2 + (-0.125)^2 + (-0.425)^2 $$\n$$ = 0.075625 + 0.075625 + 0.015625 + 0.180625 = 0.3475 $$\n$$ S_{10} = \\frac{0.3475}{4-1} = \\frac{0.3475}{3} $$\n\nFor $S_{01}$ ($n=5$):\nThe $V_{01}$ values are $(0.5, 0.75, 1, 0.5, 0.875)$.\n$$ \\sum_{j=1}^{5} (V_{01}(t_j) - 0.725)^2 = (0.5-0.725)^2 + (0.75-0.725)^2 + (1-0.725)^2 + (0.5-0.725)^2 + (0.875-0.725)^2 $$\n$$ = (-0.225)^2 + (0.025)^2 + (0.275)^2 + (-0.225)^2 + (0.15)^2 $$\n$$ = 0.050625 + 0.000625 + 0.075625 + 0.050625 + 0.0225 = 0.2 $$\n$$ S_{01} = \\frac{0.2}{5-1} = \\frac{0.2}{4} = 0.05 $$\n\nFinally, we compute the total variance:\n$$ \\widehat{Var}(\\hat{\\theta}_{AUC}) = \\frac{1}{4} S_{10} + \\frac{1}{5} S_{01} = \\frac{1}{4} \\left( \\frac{0.3475}{3} \\right) + \\frac{1}{5} (0.05) $$\n$$ = \\frac{0.3475}{12} + 0.01 $$\n$$ \\approx 0.02895833... + 0.01 = 0.03895833... $$\nRounding the final variance estimate to four significant figures, we get $0.03896$.", "answer": "$$\n\\boxed{0.03896}\n$$", "id": "4871482"}, {"introduction": "While ROC analysis provides a view of a classifier's intrinsic discriminative ability, its interpretation can be misleading in real-world clinical settings where diseases are often rare. This exercise demonstrates how class prevalence, denoted by $\\pi$, connects the abstract ROC space to the more intuitive Precision-Recall (PR) space. By deriving the mapping between these two evaluation frameworks, you will gain a deeper insight into why PR curves are often more informative for evaluating models in such low-prevalence scenarios [@problem_id:4871532].", "problem": "A Computer-Aided Detection (CAD) system is evaluated on a large medical imaging dataset where the positive class (pathology present) is rare. Let $Y \\in \\{0,1\\}$ denote the ground-truth class, with $Y=1$ indicating pathology and $Y=0$ indicating normal. The classifier outputs a binary decision $\\hat{Y} \\in \\{0,1\\}$ for a fixed threshold. Define the Receiver Operating Characteristic (ROC) operating point by the true positive rate $TPR = P(\\hat{Y}=1 \\mid Y=1)$ and the false positive rate $FPR = P(\\hat{Y}=1 \\mid Y=0)$. Let the prevalence be $\\pi = P(Y=1)$, which is fixed and known. Precision-Recall (PR) coordinates are given by precision $PPV = P(Y=1 \\mid \\hat{Y}=1)$ and recall $R = P(\\hat{Y}=1 \\mid Y=1)$.\n\nStarting from the fundamental definitions of conditional probability and Bayes’ rule, and without invoking any pre-derived mapping formula, derive the functional relationship that maps a ROC operating point $(TPR, FPR)$ to the corresponding PR coordinates $(PPV, R)$ when the prevalence $\\pi$ is fixed. In your derivation, explicitly express $PPV$ solely in terms of $TPR$, $FPR$, and $\\pi$, and identify $R$ in terms of $TPR$.\n\nAdditionally, explain, using these probability definitions, how severe class imbalance (small $\\pi$) affects the interpretation of ROC curves relative to precision-recall curves, and why PR curves are more sensitive to $\\pi$ than ROC curves.\n\nExpress your final mapping as a single row matrix of the form $\\begin{pmatrix} PPV  R \\end{pmatrix}$ in terms of $TPR$, $FPR$, and $\\pi$. No numerical evaluation is required.", "solution": "The problem asks for two main components: first, to derive the functional relationship that maps Receiver Operating Characteristic (ROC) coordinates to Precision-Recall (PR) coordinates, and second, to explain the impact of class imbalance on these two types of curves.\n\nLet's begin by stating the given definitions:\n-   Prevalence: $\\pi = P(Y=1)$. It follows that $P(Y=0) = 1 - \\pi$.\n-   True Positive Rate (TPR): $TPR = P(\\hat{Y}=1 \\mid Y=1)$. This is also known as sensitivity or recall.\n-   False Positive Rate (FPR): $FPR = P(\\hat{Y}=1 \\mid Y=0)$.\n-   Precision (PPV): $PPV = P(Y=1 \\mid \\hat{Y}=1)$, also known as Positive Predictive Value.\n-   Recall (R): $R = P(\\hat{Y}=1 \\mid Y=1)$.\n\nThe mapping is from the ROC point $(TPR, FPR)$ to the PR point $(PPV, R)$, given a fixed prevalence $\\pi$.\n\nFirst, let us establish the relationship for Recall, $R$. By its definition, $R = P(\\hat{Y}=1 \\mid Y=1)$. This is identical to the definition of the True Positive Rate, $TPR$.\nTherefore, the recall component of the PR point is simply equal to the true positive rate component of the ROC point:\n$$R = TPR$$\n\nNext, we derive the expression for Precision, $PPV$. The definition is $PPV = P(Y=1 \\mid \\hat{Y}=1)$. This is a posterior probability. To express it in terms of the given quantities, which include likelihoods ($TPR$, $FPR$) and a prior probability ($\\pi$), we must use Bayes' rule.\nBayes' rule states that for two events $A$ and $B$:\n$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}$$\nWe set $A$ to be the event $Y=1$ (pathology is present) and $B$ to be the event $\\hat{Y}=1$ (the classifier predicts pathology). Substituting these into Bayes' rule gives:\n$$PPV = P(Y=1 \\mid \\hat{Y}=1) = \\frac{P(\\hat{Y}=1 \\mid Y=1) P(Y=1)}{P(\\hat{Y}=1)}$$\nWe can substitute the known definitions into the numerator:\n-   $P(\\hat{Y}=1 \\mid Y=1) = TPR$\n-   $P(Y=1) = \\pi$\nSo the expression becomes:\n$$PPV = \\frac{TPR \\cdot \\pi}{P(\\hat{Y}=1)}$$\nNow, we must find an expression for the denominator, $P(\\hat{Y}=1)$, which is the marginal probability of a positive prediction. We can expand this term using the law of total probability, conditioning on the true class $Y$:\n$$P(\\hat{Y}=1) = P(\\hat{Y}=1 \\mid Y=1)P(Y=1) + P(\\hat{Y}=1 \\mid Y=0)P(Y=0)$$\nAgain, we substitute the known quantities into this expansion:\n-   $P(\\hat{Y}=1 \\mid Y=1) = TPR$\n-   $P(Y=1) = \\pi$\n-   $P(\\hat{Y}=1 \\mid Y=0) = FPR$\n-   $P(Y=0) = 1 - P(Y=1) = 1 - \\pi$\nSubstituting these gives the full expression for the denominator:\n$$P(\\hat{Y}=1) = (TPR \\cdot \\pi) + (FPR \\cdot (1 - \\pi))$$\nFinally, by substituting this expression for $P(\\hat{Y}=1)$ back into our equation for $PPV$, we obtain the complete relationship for precision:\n$$PPV = \\frac{TPR \\cdot \\pi}{TPR \\cdot \\pi + FPR \\cdot (1 - \\pi)}$$\nThis equation expresses $PPV$ solely in terms of $TPR$, $FPR$, and $\\pi$, as required.\nCombining the results, the mapping from a ROC point $(TPR, FPR)$ to a PR point $(PPV, R)$ is:\n$$(PPV, R) = \\left( \\frac{TPR \\cdot \\pi}{TPR \\cdot \\pi + FPR \\cdot (1 - \\pi)}, TPR \\right)$$\n\nNow, for the second part of the problem, we explain the effect of severe class imbalance (small $\\pi$) on the interpretation of ROC versus PR curves.\n\nThe coordinates of the ROC curve, $(FPR, TPR)$, are defined as $P(\\hat{Y}=1 \\mid Y=0)$ and $P(\\hat{Y}=1 \\mid Y=1)$, respectively. These probabilities are conditioned on the true class. They measure the classifier's performance independently within the negative class population and the positive class population. Consequently, the prevalence $\\pi = P(Y=1)$ does not appear in their definitions. An ROC curve is therefore invariant to the class prevalence. It represents a fundamental property of the classifier's ability to discriminate between the two classes, regardless of how rare one class is.\n\nIn contrast, the PR curve has coordinates $(R, PPV)$. While $R = TPR$ is independent of prevalence, the precision, $PPV$, is highly dependent on $\\pi$, as shown by the derived formula:\n$$PPV = \\frac{TPR \\cdot \\pi}{TPR \\cdot \\pi + FPR \\cdot (1 - \\pi)}$$\nLet's analyze this relationship under severe class imbalance, where $\\pi$ is very small ($\\pi \\to 0$). In this case, $(1 - \\pi) \\to 1$. The term $TPR \\cdot \\pi$ in the denominator becomes negligible compared to $FPR \\cdot (1 - \\pi)$, especially for a reasonably good classifier where $FPR$ is not zero. The expression simplifies to:\n$PPV \\approx \\frac{TPR \\cdot \\pi}{FPR}$\nThis approximation demonstrates that even for a classifier with excellent ROC characteristics (high $TPR$, very low $FPR$), the precision will be very low if the prevalence $\\pi$ is also very low. For example, if $TPR = 0.95$, $FPR = 0.01$, and $\\pi = 0.001$, the $PPV$ would be approximately $PPV \\approx (0.95 \\cdot 0.001) / 0.01 = 0.095$. This means that despite the classifier's high true positive rate and low false positive rate, over $90\\%$ of its positive predictions would be incorrect.\n\nThis sensitivity makes PR curves more informative than ROC curves in settings with significant class imbalance, such as medical screening for rare diseases. An ROC curve might look impressively close to the top-left corner, suggesting excellent performance. However, the corresponding PR curve for a low-prevalence scenario would reveal the practical reality that the positive predictive value is poor, which is a critical piece of information for clinical application. The PR curve directly visualizes the impact of prevalence on performance, whereas the ROC curve obfuscates it.\n\nThe final mapping from $(TPR, FPR)$ to $(PPV, R)$ is thus fully derived and its implications for imbalanced data are explained.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{TPR \\cdot \\pi}{TPR \\cdot \\pi + FPR \\cdot (1-\\pi)}  TPR\n\\end{pmatrix}\n}\n$$", "id": "4871532"}, {"introduction": "A well-calibrated CADx system provides a probability of disease, but the ultimate goal is to make a clinical decision, such as whether to alert a radiologist. This practice bridges the gap by introducing decision theory, showing how to select an optimal decision threshold $t^{\\ast}$ that maximizes expected utility. By explicitly balancing the costs of false positives ($c_{\\mathrm{FP}}$) and false negatives ($c_{\\mathrm{FN}}$), you will learn to translate a model's probabilistic output into a decision rule aligned with specific clinical and economic consequences [@problem_id:4871526].", "problem": "A hospital deploys a Computer-Aided Detection and Diagnosis system (CADx) for triaging pulmonary nodules on low-dose computed tomography. For each candidate nodule, the CADx model outputs a calibrated posterior probability $p$ of malignancy, where calibration means $p$ equals the true conditional probability $\\mathbb{P}(Y=1 \\mid \\text{features})$. The triage policy is a threshold rule: raise an alert to the radiologist if $p \\geq t$, otherwise do not alert.\n\nTo formalize decision quality, define a single-case utility function where correct decisions have baseline utility $0$, a false positive incurs utility $-c_{\\mathrm{FP}}$, and a false negative incurs utility $-c_{\\mathrm{FN}}$. Suppose the hospital’s health economics analysis yields $c_{\\mathrm{FP}}=3$ and $c_{\\mathrm{FN}}=57$ in comparable utility-loss units.\n\nUsing the principles of expected utility maximization for a single case under a calibrated probability model, derive the threshold $t^{\\ast}$ that maximizes expected utility and provide its value. Express your final threshold $t^{\\ast}$ as a single exact number (a decimal or a fraction). If you choose a decimal representation, round your answer to four significant figures.", "solution": "The problem asks for the optimal decision threshold $t^{\\ast}$ for a Computer-Aided Detection and Diagnosis (CADx) system that maximizes expected utility. The system provides a calibrated posterior probability $p$ of malignancy for a given case. The decision is to either raise an alert or not, based on whether $p$ is greater than or equal to a threshold $t$.\n\nLet us formalize the decision problem. For any given case, there are two possible true states of nature, denoted by the random variable $Y$:\n1.  The nodule is malignant ($Y=1$).\n2.  The nodule is benign ($Y=0$).\n\nThe CADx system provides the calibrated posterior probability $p = \\mathbb{P}(Y=1 \\mid \\text{features})$. Consequently, the probability of the nodule being benign is $\\mathbb{P}(Y=0 \\mid \\text{features}) = 1-p$.\n\nThere are two possible actions, $A$, that can be taken based on the value of $p$:\n1.  $A_{\\text{alert}}$: Raise an alert to the radiologist. This corresponds to a positive diagnosis.\n2.  $A_{\\text{no alert}}$: Do not raise an alert. This corresponds to a negative diagnosis.\n\nThe decision rule is defined by a threshold $t$:\n- If $p \\geq t$, take action $A_{\\text{alert}}$.\n- If $p  t$, take action $A_{\\text{no alert}}$.\n\nThe utility of a decision depends on the action taken and the true state of nature. The problem defines a utility function where correct decisions have a baseline utility of $0$, and incorrect decisions incur a utility loss (a negative utility). Let's define the utility $U(A, Y)$ for each of the four possible outcomes:\n\n1.  **True Positive (TP)**: $A=A_{\\text{alert}}$ and $Y=1$. Correct decision. $U(A_{\\text{alert}}, Y=1) = 0$.\n2.  **True Negative (TN)**: $A=A_{\\text{no alert}}$ and $Y=0$. Correct decision. $U(A_{\\text{no alert}}, Y=0) = 0$.\n3.  **False Positive (FP)**: $A=A_{\\text{alert}}$ and $Y=0$. Incorrect decision. $U(A_{\\text{alert}}, Y=0) = -c_{\\mathrm{FP}}$.\n4.  **False Negative (FN)**: $A=A_{\\text{no alert}}$ and $Y=1$. Incorrect decision. $U(A_{\\text{no alert}}, Y=1) = -c_{\\mathrm{FN}}$.\n\nThe principle of maximizing expected utility dictates that for a given case with probability $p$, we should choose the action that has the higher expected utility. The expected utility of each action, $\\mathbb{E}[U(A)]$, is calculated by averaging the utilities of its possible outcomes, weighted by their probabilities.\n\nThe expected utility of taking action $A_{\\text{alert}}$ is:\n$$\n\\mathbb{E}[U(A_{\\text{alert}})] = U(A_{\\text{alert}}, Y=1) \\cdot \\mathbb{P}(Y=1) + U(A_{\\text{alert}}, Y=0) \\cdot \\mathbb{P}(Y=0)\n$$\nSubstituting the utilities and probabilities:\n$$\n\\mathbb{E}[U(A_{\\text{alert}})] = (0) \\cdot p + (-c_{\\mathrm{FP}}) \\cdot (1-p) = -c_{\\mathrm{FP}}(1-p)\n$$\n\nThe expected utility of taking action $A_{\\text{no alert}}$ is:\n$$\n\\mathbb{E}[U(A_{\\text{no alert}})] = U(A_{\\text{no alert}}, Y=1) \\cdot \\mathbb{P}(Y=1) + U(A_{\\text{no alert}}, Y=0) \\cdot \\mathbb{P}(Y=0)\n$$\nSubstituting the utilities and probabilities:\n$$\n\\mathbb{E}[U(A_{\\text{no alert}})] = (-c_{\\mathrm{FN}}) \\cdot p + (0) \\cdot (1-p) = -c_{\\mathrm{FN}}p\n$$\n\nTo maximize expected utility, we should choose to alert if $\\mathbb{E}[U(A_{\\text{alert}})] \\geq \\mathbb{E}[U(A_{\\text{no alert}})]$. This sets the condition on $p$ for triggering an alert:\n$$\n-c_{\\mathrm{FP}}(1-p) \\geq -c_{\\mathrm{FN}}p\n$$\nMultiplying both sides by $-1$ reverses the inequality sign:\n$$\nc_{\\mathrm{FP}}(1-p) \\leq c_{\\mathrm{FN}}p\n$$\nExpanding the left side:\n$$\nc_{\\mathrm{FP}} - c_{\\mathrm{FP}}p \\leq c_{\\mathrm{FN}}p\n$$\nRearranging the terms to solve for $p$:\n$$\nc_{\\mathrm{FP}} \\leq c_{\\mathrm{FN}}p + c_{\\mathrm{FP}}p\n$$\n$$\nc_{\\mathrm{FP}} \\leq (c_{\\mathrm{FP}} + c_{\\mathrm{FN}})p\n$$\n$$\np \\geq \\frac{c_{\\mathrm{FP}}}{c_{\\mathrm{FP}} + c_{\\mathrm{FN}}}\n$$\nThis inequality defines the optimal decision rule. An alert should be raised if the probability of malignancy $p$ is greater than or equal to the ratio $\\frac{c_{\\mathrm{FP}}}{c_{\\mathrm{FP}} + c_{\\mathrm{FN}}}$. The optimal threshold $t^{\\ast}$ is the value at which the decision changes, which is the boundary of this condition.\n$$\nt^{\\ast} = \\frac{c_{\\mathrm{FP}}}{c_{\\mathrm{FP}} + c_{\\mathrm{FN}}}\n$$\nThe problem provides the numerical values for the utility-loss constants: $c_{\\mathrm{FP}}=3$ and $c_{\\mathrm{FN}}=57$. Substituting these values into the expression for $t^{\\ast}$:\n$$\nt^{\\ast} = \\frac{3}{3 + 57} = \\frac{3}{60}\n$$\nSimplifying the fraction gives:\n$$\nt^{\\ast} = \\frac{1}{20}\n$$\nExpressed as a decimal, this is:\n$$\nt^{\\ast} = 0.05\n$$\nThis value represents the optimal threshold for the triage policy. If the CADx model outputs a probability $p \\geq 0.05$, the expected utility is maximized by raising an alert. If $p  0.05$, the expected utility is maximized by not raising an alert. The value $0.05$ is an exact numerical answer.", "answer": "$$\\boxed{0.05}$$", "id": "4871526"}]}