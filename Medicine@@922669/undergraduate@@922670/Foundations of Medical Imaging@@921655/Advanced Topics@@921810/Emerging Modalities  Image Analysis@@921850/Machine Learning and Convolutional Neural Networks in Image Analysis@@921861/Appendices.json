{"hands_on_practices": [{"introduction": "To truly understand Convolutional Neural Networks, we must first deconstruct their fundamental building block: the convolutional layer. This exercise will guide you through deriving the computational and memory costs of a single layer from first principles. By calculating the number of learnable parameters and the multiply-accumulate (MAC) operations, you will gain a core intuition for how network design choices impact model size and efficiency [@problem_id:4897442].", "problem": "Consider a two-dimensional convolutional layer within a Convolutional Neural Network (CNN) applied to a Magnetic Resonance Imaging (MRI) slice for tissue segmentation. The layer receives an input feature map with $C_{\\text{in}}$ channels and produces $C_{\\text{out}}$ output channels by convolving with $C_{\\text{out}}$ learnable filters, each of spatial size $k \\times k$. Assume the following:\n- The operation is a standard dense convolution (no grouping, no depthwise separability, no dilation).\n- The stride is $1$ and padding does not affect the count of operations per output pixel.\n- Each filter has a single scalar bias that is added after the convolutional sum for its corresponding output channel.\n- A Multiply-Accumulate operation (MAC) is defined as one multiplication followed immediately by an accumulation into a running sum.\nStarting from the discrete convolution definition and the parameterization of filters as weights connecting input channels to output channels over the $k \\times k$ spatial support, derive two quantities:\n- The total number of learnable parameters in the layer.\n- The number of Multiply-Accumulate operations (MACs) required to compute all $C_{\\text{out}}$ outputs at a single spatial location (that is, per output pixel across all output channels). Count only MACs associated with the weight applications; do not include bias additions in the MAC count.\nExpress your final result as closed-form symbolic expressions in terms of $C_{\\text{in}}$, $C_{\\text{out}}$, and $k$. These are dimensionless counts; report your answer as analytic expressions without units. No numerical evaluation or rounding is required.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- A two-dimensional convolutional layer in a CNN is considered.\n- Input feature map has $C_{\\text{in}}$ channels.\n- Output feature map has $C_{\\text{out}}$ channels.\n- The layer uses $C_{\\text{out}}$ learnable filters.\n- Each filter has a spatial size of $k \\times k$.\n- The operation is a standard dense convolution.\n- The stride is $1$.\n- Padding does not affect the count of operations per output pixel.\n- Each filter has a single scalar bias for its corresponding output channel.\n- A Multiply-Accumulate (MAC) operation is one multiplication followed by one accumulation.\n- The objective is to derive two quantities: the total number of learnable parameters and the number of MACs per output pixel.\n- Bias additions are not included in the MAC count.\n- The final expressions should be in terms of $C_{\\text{in}}$, $C_{\\text{out}}$, and $k$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, describing a standard 2D convolution, a fundamental operation in machine learning and computational neuroscience. It is well-posed, providing all necessary variables and constraints ($C_{\\text{in}}$, $C_{\\text{out}}$, $k$, standard convolution, MAC definition) to derive unique, meaningful symbolic expressions. The language is objective and precise. The problem is self-contained and free of contradictions. The assumptions, such as stride of $1$ and the clarification on padding, serve to simplify the problem to its core calculation without loss of generality for the computation at a single point, which is a standard pedagogical approach. The problem does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. A rigorous derivation of the requested quantities will now be provided.\n\nThe derivation proceeds in two parts, addressing each of the required quantities.\n\n#### Part 1: Total Number of Learnable Parameters\nThe learnable parameters in a standard convolutional layer consist of two components: the filter weights and the bias terms.\n\n1.  **Filter Weights**: The layer is designed to transform an input with $C_{\\text{in}}$ channels into an output with $C_{\\text{out}}$ channels. This requires $C_{\\text{out}}$ distinct filters. Each filter must process the entire depth of the input feature map. Therefore, a single filter is not merely a $k \\times k$ matrix, but a three-dimensional tensor with dimensions $k \\times k \\times C_{\\text{in}}$. This tensor contains the weights that connect a $k \\times k$ spatial patch across all $C_{\\text{in}}$ input channels to a single value in one of the output channels.\n    The number of weight parameters in one such filter is the product of its dimensions:\n    $$\n    \\text{Parameters per filter} = k \\times k \\times C_{\\text{in}} = k^2 C_{\\text{in}}\n    $$\n    Since there are $C_{\\text{out}}$ independent filters to produce the $C_{\\text{out}}$ output channels, the total number of weight parameters is the product of the number of filters and the parameters per filter:\n    $$\n    \\text{Total weight parameters} = C_{\\text{out}} \\times (\\text{Parameters per filter}) = C_{\\text{out}} \\times (k^2 C_{\\text{in}}) = k^2 C_{\\text{in}} C_{\\text{out}}\n    $$\n\n2.  **Bias Terms**: The problem states that each filter has a single scalar bias that is added to its corresponding output channel. Since there are $C_{\\text{out}}$ output channels, each produced by a different filter, there must be $C_{\\text{out}}$ bias terms in total.\n    $$\n    \\text{Total bias parameters} = C_{\\text{out}}\n    $$\n\n3.  **Total Parameters**: The total number of learnable parameters is the sum of the total weight parameters and the total bias parameters.\n    $$\n    \\text{Total Parameters} = (\\text{Total weight parameters}) + (\\text{Total bias parameters}) = k^2 C_{\\text{in}} C_{\\text{out}} + C_{\\text{out}}\n    $$\n    This expression can be factored to a more compact form:\n    $$\n    \\text{Total Parameters} = C_{\\text{out}} (k^2 C_{\\text{in}} + 1)\n    $$\n\n#### Part 2: Number of Multiply-Accumulate (MAC) Operations per Output Pixel\nWe are asked to find the number of MACs required to compute the values for all $C_{\\text{out}}$ channels at a single spatial location in the output feature map. The problem specifies that bias additions are not to be counted as MACs.\n\n1.  **MACs for a Single Output Channel**: Let us first consider the computation for a single pixel in a single output channel, say channel $j$. The value of this pixel is the result of the convolution operation between the $j$-th filter and a corresponding $k \\times k$ spatial patch of the input feature map.\n    The input patch has dimensions $k \\times k \\times C_{\\text{in}}$. The $j$-th filter also has dimensions $k \\times k \\times C_{\\text{in}}$. The discrete convolution at this location is a dot product between the flattened filter tensor and the flattened input patch tensor. This involves an element-wise multiplication of the $k^2 C_{\\text{in}}$ filter weights with the corresponding $k^2 C_{\\text{in}}$ input values, followed by the summation of all these products.\n    The total number of multiplications is therefore $k^2 C_{\\text{in}}$. These products are then accumulated into a single sum. The definition of a MAC operation as one multiplication and one accumulation fits this process perfectly. Thus, computing the pre-activation value for one pixel in one output channel requires $k^2 C_{\\text{in}}$ MAC operations.\n\n2.  **MACs for All Output Channels**: The computation for each of the $C_{\\text{out}}$ output channels is independent. To find the total number of MACs for a single output spatial location, we must calculate the contributions for all $C_{\\text{out}}$ channels. Since each output channel requires $k^2 C_{\\text{in}}$ MACs, the total number of MACs is the product of the MACs per channel and the number of channels.\n    $$\n    \\text{Total MACs per output pixel} = (\\text{MACs per channel}) \\times (\\text{Number of output channels})\n    $$\n    $$\n    \\text{Total MACs per output pixel} = (k^2 C_{\\text{in}}) \\times C_{\\text{out}} = k^2 C_{\\text{in}} C_{\\text{out}}\n    $$\nThis concludes the derivation. The two requested quantities are the total number of learnable parameters, $C_{\\text{out}} (k^2 C_{\\text{in}} + 1)$, and the number of MACs per output pixel, $k^2 C_{\\text{in}} C_{\\text{out}}$.", "answer": "$$\\boxed{\\begin{pmatrix} C_{\\text{out}}(k^2 C_{\\text{in}} + 1) & k^2 C_{\\text{in}} C_{\\text{out}} \\end{pmatrix}}$$", "id": "4897442"}, {"introduction": "A neural network learns by minimizing a loss function, which quantifies the error between its predictions and the ground truth. This process, called backpropagation, relies on calculating the gradient of the loss with respect to the network's outputs. In this practice problem, you will compute this crucial gradient for the soft Dice loss, a function widely used in medical image segmentation, to see exactly how a model is guided to make better predictions [@problem_id:4897433].", "problem": "In a binary segmentation task in foundations of medical imaging, a Convolutional Neural Network (CNN) predicts class probabilities for each pixel of a small patch extracted from a Magnetic Resonance Imaging (MRI) slice. Consider a $2 \\times 2$ patch whose ground truth labels are $y_1 = 1$, $y_2 = 1$, $y_3 = 0$, $y_4 = 0$, and whose predicted foreground probabilities are $p_1 = \\frac{1}{2}$, $p_2 = \\frac{3}{4}$, $p_3 = \\frac{1}{4}$, $p_4 = \\frac{1}{8}$. The Dice similarity coefficient between two binary sets $A$ and $B$ is defined as $D_{\\text{set}} = \\frac{2|A \\cap B|}{|A| + |B|}$. For differentiable training with probabilities, a common relaxation replaces the set cardinalities with sums over pixels, defining the soft Dice coefficient on this patch as $D = \\frac{2 \\sum_{i=1}^{4} p_i y_i}{\\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i}$ and the soft Dice loss as $L = 1 - D$. Using standard rules of calculus and treating $y_i$ as constants, compute the gradient vector $\\nabla_{\\mathbf{p}} L = \\left( \\frac{\\partial L}{\\partial p_1}, \\frac{\\partial L}{\\partial p_2}, \\frac{\\partial L}{\\partial p_3}, \\frac{\\partial L}{\\partial p_4} \\right)$ for this patch. Express your final answer as exact rational numbers, and present the components as a single row vector.", "solution": "The problem is well-defined, scientifically sound, and provides all necessary information to compute the requested gradient. It is a standard application of calculus to a common loss function used in machine learning for medical image segmentation. Therefore, the problem is valid and a solution will be provided.\n\nThe soft Dice loss, $L$, is defined as $L = 1 - D$, where $D$ is the soft Dice coefficient. Given the definitions for a patch of $4$ pixels, we have:\n$$\nD = \\frac{2 \\sum_{i=1}^{4} p_i y_i}{\\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i}\n$$\nThe loss function is therefore:\n$$\nL(\\mathbf{p}) = 1 - \\frac{2 \\sum_{i=1}^{4} p_i y_i}{\\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i}\n$$\nWe need to compute the gradient vector $\\nabla_{\\mathbf{p}} L$, whose components are the partial derivatives $\\frac{\\partial L}{\\partial p_j}$ for $j \\in \\{1, 2, 3, 4\\}$.\n\nUsing the linearity of differentiation, the partial derivative of the loss with respect to a single predicted probability $p_j$ is:\n$$\n\\frac{\\partial L}{\\partial p_j} = \\frac{\\partial}{\\partial p_j} \\left( 1 - D \\right) = - \\frac{\\partial D}{\\partial p_j}\n$$\nTo simplify the differentiation of $D$, we define the numerator and denominator as separate functions:\nLet $N = 2 \\sum_{i=1}^{4} p_i y_i$.\nLet $M = \\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i$.\nSo, $D = \\frac{N}{M}$.\n\nWe apply the quotient rule for differentiation:\n$$\n\\frac{\\partial D}{\\partial p_j} = \\frac{\\frac{\\partial N}{\\partial p_j} M - N \\frac{\\partial M}{\\partial p_j}}{M^2}\n$$\nNow, we compute the partial derivatives of $N$ and $M$ with respect to $p_j$. The ground truth labels $y_i$ are treated as constants.\n$$\n\\frac{\\partial N}{\\partial p_j} = \\frac{\\partial}{\\partial p_j} \\left( 2 \\sum_{i=1}^{4} p_i y_i \\right) = 2 y_j\n$$\n$$\n\\frac{\\partial M}{\\partial p_j} = \\frac{\\partial}{\\partial p_j} \\left( \\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i \\right) = 1\n$$\nSubstituting these into the quotient rule formula:\n$$\n\\frac{\\partial D}{\\partial p_j} = \\frac{(2y_j)M - N(1)}{M^2} = \\frac{2y_j \\left(\\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i\\right) - 2\\sum_{i=1}^{4} p_i y_i}{\\left(\\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i\\right)^2}\n$$\nTherefore, the partial derivative of the loss $L$ is:\n$$\n\\frac{\\partial L}{\\partial p_j} = - \\frac{\\partial D}{\\partial p_j} = \\frac{2\\sum_{i=1}^{4} p_i y_i - 2y_j \\left(\\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i\\right)}{\\left(\\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i\\right)^2}\n$$\nNext, we substitute the given numerical values into the terms of this expression.\nThe ground truth labels are $y_1 = 1$, $y_2 = 1$, $y_3 = 0$, $y_4 = 0$.\nThe predicted probabilities are $p_1 = \\frac{1}{2}$, $p_2 = \\frac{3}{4}$, $p_3 = \\frac{1}{4}$, $p_4 = \\frac{1}{8}$.\n\nWe calculate the necessary sums:\n$$\n\\sum_{i=1}^{4} y_i = y_1 + y_2 + y_3 + y_4 = 1 + 1 + 0 + 0 = 2\n$$\n$$\n\\sum_{i=1}^{4} p_i = p_1 + p_2 + p_3 + p_4 = \\frac{1}{2} + \\frac{3}{4} + \\frac{1}{4} + \\frac{1}{8} = \\frac{4}{8} + \\frac{6}{8} + \\frac{2}{8} + \\frac{1}{8} = \\frac{13}{8}\n$$\n$$\n\\sum_{i=1}^{4} p_i y_i = p_1 y_1 + p_2 y_2 + p_3 y_3 + p_4 y_4 = \\left(\\frac{1}{2}\\right)(1) + \\left(\\frac{3}{4}\\right)(1) + \\left(\\frac{1}{4}\\right)(0) + \\left(\\frac{1}{8}\\right)(0) = \\frac{1}{2} + \\frac{3}{4} = \\frac{2}{4} + \\frac{3}{4} = \\frac{5}{4}\n$$\nNow, we compute the constant terms in our derivative formula:\n$2 \\sum_{i=1}^{4} p_i y_i = 2 \\times \\frac{5}{4} = \\frac{5}{2}$\nThe denominator term is $\\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i = \\frac{13}{8} + 2 = \\frac{13}{8} + \\frac{16}{8} = \\frac{29}{8}$.\nThe squared denominator is $\\left(\\frac{29}{8}\\right)^2 = \\frac{841}{64}$.\n\nNow we can compute each component of the gradient:\n\nFor $j=1$, $y_1=1$:\n$$\n\\frac{\\partial L}{\\partial p_1} = \\frac{\\frac{5}{2} - 2(1) \\left(\\frac{29}{8}\\right)}{\\frac{841}{64}} = \\frac{\\frac{5}{2} - \\frac{29}{4}}{\\frac{841}{64}} = \\frac{\\frac{10}{4} - \\frac{29}{4}}{\\frac{841}{64}} = \\frac{-\\frac{19}{4}}{\\frac{841}{64}} = -\\frac{19}{4} \\times \\frac{64}{841} = -\\frac{19 \\times 16}{841} = -\\frac{304}{841}\n$$\n\nFor $j=2$, $y_2=1$:\nThe calculation is identical to that for $j=1$ since $y_2=1$.\n$$\n\\frac{\\partial L}{\\partial p_2} = -\\frac{304}{841}\n$$\n\nFor $j=3$, $y_3=0$:\n$$\n\\frac{\\partial L}{\\partial p_3} = \\frac{\\frac{5}{2} - 2(0) \\left(\\frac{29}{8}\\right)}{\\frac{841}{64}} = \\frac{\\frac{5}{2} - 0}{\\frac{841}{64}} = \\frac{\\frac{5}{2}}{\\frac{841}{64}} = \\frac{5}{2} \\times \\frac{64}{841} = \\frac{5 \\times 32}{841} = \\frac{160}{841}\n$$\n\nFor $j=4$, $y_4=0$:\nThe calculation is identical to that for $j=3$ since $y_4=0$.\n$$\n\\frac{\\partial L}{\\partial p_4} = \\frac{160}{841}\n$$\n\nThe gradient vector $\\nabla_{\\mathbf{p}} L$ is therefore:\n$$\n\\nabla_{\\mathbf{p}} L = \\left( -\\frac{304}{841}, -\\frac{304}{841}, \\frac{160}{841}, \\frac{160}{841} \\right)\n$$\nThis is the final answer, expressed as exact rational numbers in a row vector.", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{304}{841} & -\\frac{304}{841} & \\frac{160}{841} & \\frac{160}{841} \\end{pmatrix}}\n$$", "id": "4897433"}, {"introduction": "While understanding individual layers is essential, a real-world application involves assembling them into a deep architecture with significant computational resource requirements. This final exercise bridges theory and practice by asking you to estimate the total GPU memory needed to run a complete 3D U-Net, a state-of-the-art model for volumetric medical image segmentation. By accounting for both model parameters and the activations stored during a forward pass, you will develop a practical skill for planning and executing deep learning experiments within hardware constraints [@problem_id:4897443].", "problem": "A hospital research team is training a Three-Dimensional Convolutional Neural Network (3D CNN) in a U-shaped architecture (3D U-Net) for volumetric Magnetic Resonance Imaging segmentation, on cubic patches of size $128 \\times 128 \\times 128$. The encoder has four levels with feature widths $\\{32, 64, 128, 256\\}$ and the decoder mirrors these widths. At each encoder and decoder level (except upsampling and the final classifier), there are two convolutional layers with kernel size $3 \\times 3 \\times 3$ and a bias term. Downsampling in the encoder is performed by max pooling with stride $2$ (which has no parameters), and upsampling in the decoder is performed by a transposed convolution with kernel size $2 \\times 2 \\times 2$, stride $2$, that halves channels at every step: $256 \\to 128$, $128 \\to 64$, $64 \\to 32$. After each upsampling, the upsampled feature map is concatenated with the corresponding encoder skip feature map. The final classifier is a $1 \\times 1 \\times 1$ convolution mapping $32$ channels to $2$ output classes. Assume a single input channel, Rectified Linear Unit (ReLU) nonlinearities are in-place and do not add to memory, and there is no batch normalization.\n\nTraining uses automatic mixed precision on the Graphics Processing Unit (GPU): treat both parameters and activations as stored in $16$-bit floating point ($2$ bytes per value). Assume that during the forward pass, the outputs of all convolutional and transposed convolutional layers in the entire network are retained until backpropagation starts. Ignore optimizer states, gradient buffers, and any non-convolutional intermediates.\n\nUsing the core definitions that (i) the number of parameters of a convolution with kernel size $k_d \\times k_h \\times k_w$, input channels $C_{\\text{in}}$, output channels $C_{\\text{out}}$, and bias is $k_d k_h k_w \\, C_{\\text{in}} C_{\\text{out}} + C_{\\text{out}}$, and (ii) the activation tensor size equals $C_{\\text{out}} \\times D \\times H \\times W$ elements, with each element occupying $2$ bytes under mixed precision, estimate the total GPU memory required to store all retained activations simultaneously plus all parameters for one forward pass on a single $128^3$ patch.\n\nExpress your final answer in gibibytes (GiB), using $1\\,\\mathrm{GiB} = 2^{30}$ bytes, and round your answer to four significant figures.", "solution": "The total GPU memory required is the sum of the memory for the model's parameters and the memory for the retained activation maps. We will calculate these two components separately and then sum them. All values are stored using $16$-bit precision, which is $2$ bytes per value.\n\n**1. Parameter Memory Calculation**\n\nThe number of parameters for a convolutional layer is given by $N_{\\text{params}} = k^3 C_{\\text{in}} C_{\\text{out}} + C_{\\text{out}}$, where $k$ is the kernel size, $C_{\\text{in}}$ is the number of input channels, and $C_{\\text{out}}$ is the number of output channels.\n\nThe network architecture is traced level by level:\n\n**Encoder Path**\n- **Level 1 (features: $32$, resolution: $128^3$)**\n  - Conv 1.1: $k=3, C_{\\text{in}}=1, C_{\\text{out}}=32$. $P_1 = 3^3 \\cdot 1 \\cdot 32 + 32 = 896$.\n  - Conv 1.2: $k=3, C_{\\text{in}}=32, C_{\\text{out}}=32$. $P_2 = 3^3 \\cdot 32 \\cdot 32 + 32 = 27,680$.\n- **Level 2 (features: $64$, resolution: $64^3$)**\n  - Conv 2.1: $k=3, C_{\\text{in}}=32, C_{\\text{out}}=64$. $P_3 = 3^3 \\cdot 32 \\cdot 64 + 64 = 55,360$.\n  - Conv 2.2: $k=3, C_{\\text{in}}=64, C_{\\text{out}}=64$. $P_4 = 3^3 \\cdot 64 \\cdot 64 + 64 = 110,656$.\n- **Level 3 (features: $128$, resolution: $32^3$)**\n  - Conv 3.1: $k=3, C_{\\text{in}}=64, C_{\\text{out}}=128$. $P_5 = 3^3 \\cdot 64 \\cdot 128 + 128 = 221,312$.\n  - Conv 3.2: $k=3, C_{\\text{in}}=128, C_{\\text{out}}=128$. $P_6 = 3^3 \\cdot 128 \\cdot 128 + 128 = 442,496$.\n- **Level 4 (features: $256$, resolution: $16^3$, bottleneck)**\n  - Conv 4.1: $k=3, C_{\\text{in}}=128, C_{\\text{out}}=256$. $P_7 = 3^3 \\cdot 128 \\cdot 256 + 256 = 884,992$.\n  - Conv 4.2: $k=3, C_{\\text{in}}=256, C_{\\text{out}}=256$. $P_8 = 3^3 \\cdot 256 \\cdot 256 + 256 = 1,769,728$.\n\n**Decoder Path**\n- **Upsample from $16^3$ to $32^3$ and convolutions (produces $128$ channels)**\n  - Transposed Conv 1: $k=2, C_{\\text{in}}=256, C_{\\text{out}}=128$. $P_9 = 2^3 \\cdot 256 \\cdot 128 + 128 = 262,272$.\n  - After concatenation with skip from Level 3 ($128$ channels), $C_{\\text{in}}$ becomes $128+128=256$.\n  - Conv 5.1: $k=3, C_{\\text{in}}=256, C_{\\text{out}}=128$. $P_{10} = 3^3 \\cdot 256 \\cdot 128 + 128 = 884,864$.\n  - Conv 5.2: $k=3, C_{\\text{in}}=128, C_{\\text{out}}=128$. $P_{11} = 3^3 \\cdot 128 \\cdot 128 + 128 = 442,496$.\n- **Upsample from $32^3$ to $64^3$ and convolutions (produces $64$ channels)**\n  - Transposed Conv 2: $k=2, C_{\\text{in}}=128, C_{\\text{out}}=64$. $P_{12} = 2^3 \\cdot 128 \\cdot 64 + 64 = 65,600$.\n  - After concatenation with skip from Level 2 ($64$ channels), $C_{\\text{in}}$ becomes $64+64=128$.\n  - Conv 6.1: $k=3, C_{\\text{in}}=128, C_{\\text{out}}=64$. $P_{13} = 3^3 \\cdot 128 \\cdot 64 + 64 = 221,248$.\n  - Conv 6.2: $k=3, C_{\\text{in}}=64, C_{\\text{out}}=64$. $P_{14} = 3^3 \\cdot 64 \\cdot 64 + 64 = 110,656$.\n- **Upsample from $64^3$ to $128^3$ and convolutions (produces $32$ channels)**\n  - Transposed Conv 3: $k=2, C_{\\text{in}}=64, C_{\\text{out}}=32$. $P_{15} = 2^3 \\cdot 64 \\cdot 32 + 32 = 16,416$.\n  - After concatenation with skip from Level 1 ($32$ channels), $C_{\\text{in}}$ becomes $32+32=64$.\n  - Conv 7.1: $k=3, C_{\\text{in}}=64, C_{\\text{out}}=32$. $P_{16} = 3^3 \\cdot 64 \\cdot 32 + 32 = 55,328$.\n  - Conv 7.2: $k=3, C_{\\text{in}}=32, C_{\\text{out}}=32$. $P_{17} = 3^3 \\cdot 32 \\cdot 32 + 32 = 27,680$.\n\n**Final Classifier**\n- Final Conv: $k=1, C_{\\text{in}}=32, C_{\\text{out}}=2$. $P_{18} = 1^3 \\cdot 32 \\cdot 2 + 2 = 66$.\n\nTotal Parameters:\n$P_{\\text{total}} = \\sum_{i=1}^{18} P_i = 896 + 27680 + 55360 + 110656 + 221312 + 442496 + 884992 + 1769728 + 262272 + 884864 + 442496 + 65600 + 221248 + 110656 + 16416 + 55328 + 27680 + 66 = 5,599,746$.\n\nParameter Memory:\n$M_{\\text{params}} = P_{\\text{total}} \\times 2\\,\\text{bytes/param} = 5,599,746 \\times 2 = 11,199,492$ bytes.\n\n**2. Activation Memory Calculation**\n\nThe memory for activations is the sum of the sizes of the output tensors of all convolutional and transposed convolutional layers. The size of one activation tensor is $A = C_{\\text{out}} \\times D \\times H \\times W$ elements.\n\nWe group the activations by their spatial resolution:\n- **Resolution $128 \\times 128 \\times 128$ ($V_{128} = 128^3 = 2,097,152$ voxels)**\n  - Encoder Conv 1.1 ($C_{\\text{out}}=32$), Conv 1.2 ($C_{\\text{out}}=32$).\n  - Decoder Transposed Conv 3 ($C_{\\text{out}}=32$), Conv 7.1 ($C_{\\text{out}}=32$), Conv 7.2 ($C_{\\text{out}}=32$).\n  - Final Classifier Conv ($C_{\\text{out}}=2$).\n  - Total channels: $32 + 32 + 32 + 32 + 32 + 2 = 162$.\n  - Elements: $A_{128} = 162 \\times V_{128} = 162 \\times 2,097,152 = 339,738,624$.\n- **Resolution $64 \\times 64 \\times 64$ ($V_{64} = 64^3 = 262,144$ voxels)**\n  - Encoder Conv 2.1 ($C_{\\text{out}}=64$), Conv 2.2 ($C_{\\text{out}}=64$).\n  - Decoder Transposed Conv 2 ($C_{\\text{out}}=64$), Conv 6.1 ($C_{\\text{out}}=64$), Conv 6.2 ($C_{\\text{out}}=64$).\n  - Total channels: $64+64+64+64+64 = 320$.\n  - Elements: $A_{64} = 320 \\times V_{64} = 320 \\times 262,144 = 83,886,080$.\n- **Resolution $32 \\times 32 \\times 32$ ($V_{32} = 32^3 = 32,768$ voxels)**\n  - Encoder Conv 3.1 ($C_{\\text{out}}=128$), Conv 3.2 ($C_{\\text{out}}=128$).\n  - Decoder Transposed Conv 1 ($C_{\\text{out}}=128$), Conv 5.1 ($C_{\\text{out}}=128$), Conv 5.2 ($C_{\\text{out}}=128$).\n  - Total channels: $128+128+128+128+128 = 640$.\n  - Elements: $A_{32} = 640 \\times V_{32} = 640 \\times 32,768 = 20,971,520$.\n- **Resolution $16 \\times 16 \\times 16$ ($V_{16} = 16^3 = 4,096$ voxels)**\n  - Encoder Conv 4.1 ($C_{\\text{out}}=256$), Conv 4.2 ($C_{\\text{out}}=256$).\n  - Total channels: $256 + 256 = 512$.\n  - Elements: $A_{16} = 512 \\times V_{16} = 512 \\times 4,096 = 2,097,152$.\n\nTotal Activation Elements:\n$A_{\\text{total}} = A_{128} + A_{64} + A_{32} + A_{16} = 339,738,624 + 83,886,080 + 20,971,520 + 2,097,152 = 446,693,376$.\n\nActivation Memory:\n$M_{\\text{act}} = A_{\\text{total}} \\times 2\\,\\text{bytes/element} = 446,693,376 \\times 2 = 893,386,752$ bytes.\n\n**3. Total Memory and Conversion to GiB**\n\nTotal Memory:\n$M_{\\text{total}} = M_{\\text{params}} + M_{\\text{act}} = 11,199,492 + 893,386,752 = 904,586,244$ bytes.\n\nTo convert from bytes to gibibytes (GiB), we use the conversion factor $1\\,\\text{GiB} = 2^{30}\\,\\text{bytes} = 1,073,741,824$ bytes.\n\n$M_{\\text{GiB}} = \\frac{M_{\\text{total}}}{2^{30}} = \\frac{904,586,244}{1,073,741,824} \\approx 0.842465753$ GiB.\n\nRounding to four significant figures, we get $0.8425$ GiB.", "answer": "$$\\boxed{0.8425}$$", "id": "4897443"}]}