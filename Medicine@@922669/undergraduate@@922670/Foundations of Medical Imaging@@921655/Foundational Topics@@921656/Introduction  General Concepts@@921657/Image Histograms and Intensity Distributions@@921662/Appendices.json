{"hands_on_practices": [{"introduction": "When comparing images from different scans or patients, raw intensity values can be misleading due to variations in acquisition settings. This practice explores z-score normalization, a fundamental technique for standardizing an intensity distribution to a common scale. By deriving the transformation that sets the mean to $0$ and variance to $1$, you will gain insight into how this process facilitates comparison while preserving the intrinsic shape of the histogram, a critical concept for robust image analysis [@problem_id:4891654].", "problem": "In a radiological image, a Region of Interest (ROI) is selected to study its intensity distribution via an empirical histogram that approximates the underlying probability density of pixel intensities. Let pixel intensity within the ROI be modeled as a real-valued random variable $X$ with finite mean $ \\mu $ and positive standard deviation $ \\sigma $, where $ \\sigma  0 $. Consider constructing a standardized intensity $Z$ by an affine transformation of $X$ so that the standardized histogram is centered and scaled in a way that facilitates cross-subject or cross-scan comparison without distorting the intrinsic shape characteristics of the distribution.\n\nStarting from the definitions of expectation $E[\\cdot]$ and variance $\\mathrm{Var}(\\cdot)$, and using only the properties of affine transformations of random variables and how they act on probability densities and histograms, derive the conditions on an affine map $Z = a X + b$ that produce a standardized intensity with zero mean and unit variance for the ROI. Then, reason about how this rescaling affects the histogram’s center, spread, and shape descriptors such as skewness and kurtosis, and whether the relative ordering of pixel intensities in the ROI is preserved. Assume that the histogram is a faithful empirical representation of the distribution and that the rescaling is applied uniformly to all ROI pixels. Finally, select the option that correctly states the derivation and the resulting effects on the histogram.\n\nWhich option is correct?\n\nA. Choose $a$ and $b$ in $Z = a X + b$ such that $E[Z] = 0$ and $\\mathrm{Var}(Z) = 1$, yielding $a = 1/\\sigma$ and $b = -\\mu/\\sigma$. The transformation is strictly increasing and thus preserves the relative ordering of ROI intensities. The standardized histogram is translated to center at $0$ and horizontally scaled by a factor $1/\\sigma$. Shape descriptors such as skewness and kurtosis are unchanged by this affine standardization.\n\nB. Choose $a$ and $b$ in $Z = a X + b$ such that $E[Z] = 0$ and $\\mathrm{Var}(Z) = 1$, yielding $a = \\sigma$ and $b = -\\mu$. The transformation compresses the intensity spread by a factor $\\sigma$ and makes the standardized histogram uniform on an interval when $X$ is Gaussian.\n\nC. The map $Z = (X - \\mu)/\\sigma$ ensures that the histogram’s bin edges and counts are identical to those of $X$, except for relabeling the $x$-axis. The mean and variance of the ROI are preserved under this normalization.\n\nD. The map $Z = (X - \\mu)/\\sigma$ centers the histogram at $0$ and sets its variance to $1$, but it alters shape by changing skewness and kurtosis unless $X$ is Gaussian.\n\nE. When applied to a Region of Interest (ROI) in a Computed Tomography (CT) image, the z-score normalization depends on the spatial arrangement of pixels and may break intensity ordering because the transformation is nonlinear in $X$.", "solution": "The goal is to find the parameters $a$ and $b$ for the affine transformation $Z = aX + b$ that result in a standardized variable $Z$ with a mean of 0 and a variance of 1. We are given that the original variable $X$ has mean $E[X] = \\mu$ and variance $\\mathrm{Var}(X) = \\sigma^2$.\n\n1.  **Derive the mean of Z:**\n    Using the linearity of expectation, we have:\n    $E[Z] = E[aX + b] = aE[X] + b = a\\mu + b$.\n    For the mean of $Z$ to be zero, we must have:\n    $a\\mu + b = 0 \\implies b = -a\\mu$.\n\n2.  **Derive the variance of Z:**\n    Using the properties of variance for an affine transformation, we have:\n    $\\mathrm{Var}(Z) = \\mathrm{Var}(aX + b) = a^2 \\mathrm{Var}(X) = a^2 \\sigma^2$.\n    For the variance of $Z$ to be one, we must have:\n    $a^2 \\sigma^2 = 1 \\implies a^2 = 1/\\sigma^2$.\n    Since $\\sigma > 0$, we can take the square root. By convention, the positive root is chosen to preserve the orientation of the data: $a = 1/\\sigma$.\n\n3.  **Determine the parameters:**\n    Substituting $a = 1/\\sigma$ back into the equation for $b$:\n    $b = - (1/\\sigma)\\mu = -\\mu/\\sigma$.\n    Thus, the affine transformation is $Z = (1/\\sigma)X - \\mu/\\sigma = (X - \\mu)/\\sigma$. This is the z-score transformation.\n\n4.  **Analyze the effects:**\n    *   **Center and Spread:** By construction, the transformation shifts the mean of the histogram to 0 and scales its width such that the variance becomes 1. The horizontal axis is scaled by the factor $1/\\sigma$.\n    *   **Ordering:** The transformation is a linear function with a positive slope ($a = 1/\\sigma > 0$). Therefore, it is strictly increasing and preserves the relative ordering of all pixel intensities.\n    *   **Shape:** Shape descriptors like skewness and kurtosis are defined based on central moments standardized by the standard deviation. An affine transformation does not change these standardized moments. For example, the skewness of $Z$ is $E[(Z-0)^3/1^3] = E[((X-\\mu)/\\sigma)^3] = (1/\\sigma^3)E[(X-\\mu)^3]$, which is the definition of the skewness of $X$. Thus, skewness and kurtosis are invariant under this transformation.\n\nBased on this derivation, option A correctly identifies the parameters $a$ and $b$, and accurately describes the effects of the transformation on ordering, center, spread, and shape. The other options contain factual errors regarding the parameters, the effect on shape, or the nature of the transformation.", "answer": "$$\\boxed{A}$$", "id": "4891654"}, {"introduction": "Real-world imaging systems are not perfect, and artifacts like detector saturation can corrupt the resulting data. This exercise models such a scenario by treating the observed pixel intensities as a mixture of an ideal distribution and a point mass representing saturated pixels. By deriving the effect of this saturation on the empirical mean and variance, you will learn to quantify how acquisition limitations can bias image statistics and skew the results of subsequent automated analyses like thresholding [@problem_id:4891605].", "problem": "A digital medical imaging system records pixel intensities on a normalized scale in the closed interval $\\left[0,1\\right]$, where the maximum code $1$ corresponds to saturation. Consider an acquisition in which a proportion $p$ of pixels are saturated at the maximum code due to detector clipping, while the remaining proportion $1-p$ of pixels would, in the absence of saturation, follow a baseline intensity distribution that is uniform on $\\left[0,1\\right]$. The image histogram is used to compute an empirical mean and variance over all pixels in the field of view. Many simple global thresholding heuristics choose a threshold $T$ of the form $T=\\mu+k\\sigma$, where $\\mu$ is the empirical mean, $\\sigma$ is the empirical standard deviation, and $k0$ is a user-selected constant.\n\nUsing only the foundational definitions of expectation, variance, and mixture distributions:\n- Derive closed-form expressions for the empirical mean $\\mu(p)$ and empirical variance $\\sigma^{2}(p)$ as functions of the saturation proportion $p$ under the mixture model consisting of a point mass at $1$ with probability $p$ and a uniform distribution on $\\left[0,1\\right]$ with probability $1-p$.\n- Let $T_{\\text{true}}(k)$ denote the threshold computed in the ideal case where $p=0$, and let $T_{\\text{obs}}(p,k)$ denote the threshold computed under the saturated mixture model. Derive a closed-form expression for the threshold bias $\\Delta T(p,k)=T_{\\text{obs}}(p,k)-T_{\\text{true}}(k)$.\n\nFinally, quantify the threshold bias for a given saturation proportion by evaluating $\\Delta T(p,k)$ at $p=0.18$ and $k=1.2$. Round your final numerical answer to four significant figures. Express the answer as a unitless number.", "solution": "Let $I$ be the random variable for pixel intensity. The distribution of $I$ is a mixture of a uniform distribution $I_U \\sim U(0,1)$ with weight $(1-p)$ and a point mass at $I_S=1$ with weight $p$.\n\n**1. Derive Mean $\\mu(p)$ and Variance $\\sigma^2(p)$**\n\nFirst, we find the first and second moments for the component distributions:\n- For $I_U \\sim U(0,1)$: $\\mathbb{E}[I_U] = \\int_0^1 x \\, dx = 1/2$; $\\mathbb{E}[I_U^2] = \\int_0^1 x^2 \\, dx = 1/3$.\n- For the point mass $I_S=1$: $\\mathbb{E}[I_S] = 1$; $\\mathbb{E}[I_S^2] = 1^2 = 1$.\n\nUsing the law of total expectation for the mixture distribution:\n- **Mean $\\mu(p)$**:\n  $\\mu(p) = \\mathbb{E}[I] = (1-p)\\mathbb{E}[I_U] + p\\mathbb{E}[I_S] = (1-p) \\cdot \\frac{1}{2} + p \\cdot 1 = \\frac{1-p+2p}{2} = \\frac{1+p}{2}$.\n- **Second Moment $\\mathbb{E}[I^2]$**:\n  $\\mathbb{E}[I^2] = (1-p)\\mathbb{E}[I_U^2] + p\\mathbb{E}[I_S^2] = (1-p) \\cdot \\frac{1}{3} + p \\cdot 1 = \\frac{1-p+3p}{3} = \\frac{1+2p}{3}$.\n- **Variance $\\sigma^2(p)$**:\n  $\\sigma^2(p) = \\mathbb{E}[I^2] - (\\mu(p))^2 = \\frac{1+2p}{3} - \\left(\\frac{1+p}{2}\\right)^2 = \\frac{1+2p}{3} - \\frac{1+2p+p^2}{4}$.\n  Using a common denominator of 12:\n  $\\sigma^2(p) = \\frac{4(1+2p) - 3(1+2p+p^2)}{12} = \\frac{4+8p - 3-6p-3p^2}{12} = \\frac{1+2p-3p^2}{12}$.\n\n**2. Derive Threshold Bias $\\Delta T(p,k)$**\n\nThe threshold is $T = \\mu + k\\sigma$.\n- **True Threshold ($p=0$)**:\n  $\\mu(0) = \\frac{1}{2}$; $\\sigma^2(0) = \\frac{1}{12} \\implies \\sigma(0) = \\frac{1}{\\sqrt{12}} = \\frac{1}{2\\sqrt{3}}$.\n  $T_{\\text{true}}(k) = \\frac{1}{2} + k \\frac{1}{2\\sqrt{3}}$.\n- **Observed Threshold ($p>0$)**:\n  $\\mu(p) = \\frac{1+p}{2}$; $\\sigma(p) = \\sqrt{\\frac{1+2p-3p^2}{12}} = \\frac{\\sqrt{(1-p)(1+3p)}}{2\\sqrt{3}}$.\n  $T_{\\text{obs}}(p,k) = \\frac{1+p}{2} + k \\frac{\\sqrt{(1-p)(1+3p)}}{2\\sqrt{3}}$.\n- **Threshold Bias $\\Delta T(p,k)$**:\n  $\\Delta T(p,k) = T_{\\text{obs}}(p,k) - T_{\\text{true}}(k)$\n  $\\Delta T(p,k) = \\left(\\frac{1+p}{2} - \\frac{1}{2}\\right) + \\left(k \\frac{\\sqrt{(1-p)(1+3p)}}{2\\sqrt{3}} - k \\frac{1}{2\\sqrt{3}}\\right)$\n  $\\Delta T(p,k) = \\frac{p}{2} + \\frac{k}{2\\sqrt{3}} \\left( \\sqrt{(1-p)(1+3p)} - 1 \\right)$.\n\n**3. Evaluate for $p=0.18$ and $k=1.2$**\n\nSubstitute the given values into the bias expression:\n$\\Delta T(0.18, 1.2) = \\frac{0.18}{2} + \\frac{1.2}{2\\sqrt{3}} \\left( \\sqrt{(1-0.18)(1+3 \\cdot 0.18)} - 1 \\right)$\n$\\Delta T(0.18, 1.2) = 0.09 + \\frac{0.6}{\\sqrt{3}} \\left( \\sqrt{(0.82)(1.54)} - 1 \\right)$\n$\\Delta T(0.18, 1.2) = 0.09 + \\frac{0.6}{\\sqrt{3}} \\left( \\sqrt{1.2628} - 1 \\right)$\n$\\Delta T(0.18, 1.2) \\approx 0.09 + 0.34641 \\left( 1.12374 - 1 \\right)$\n$\\Delta T(0.18, 1.2) \\approx 0.09 + 0.34641 \\left( 0.12374 \\right)$\n$\\Delta T(0.18, 1.2) \\approx 0.09 + 0.042861$\n$\\Delta T(0.18, 1.2) \\approx 0.132861$\n\nRounding to four significant figures, the result is $0.1329$.", "answer": "$$\\boxed{0.1329}$$", "id": "4891605"}, {"introduction": "Beyond simple visualization, histograms are powerful tools for automated quality control. This practice demonstrates how to use a formal statistical method—the Pearson $\\chi^2$ goodness-of-fit test—to detect the presence of impulsive (\"salt-and-pepper\") noise in an image. By modeling the corrupted data and analyzing the behavior of the test statistic, you will see how deviations from an expected histogram shape can be rigorously identified, bridging the gap between visual inspection and quantitative diagnostics [@problem_id:4891613].", "problem": "An imaging scientist is analyzing a single grayscale slice from Magnetic Resonance Imaging (MRI). The image has $N$ pixels, and each pixel intensity takes a discrete value in $\\{0,1,\\dots,L-1\\}$. In an ideal noise-free acquisition, pixel intensities are independent and identically distributed (i.i.d.) from a smooth baseline probability mass function (PMF) $q(i)$ on $\\{0,1,\\dots,L-1\\}$, where $q(i)$ varies slowly with $i$ and $q(0)$ and $q(L-1)$ are small but nonzero. Due to a malfunction, a fraction $p$ of pixels are corrupted by impulsive noise: each corrupted pixel is set to intensity $0$ with probability $1/2$ and to intensity $L-1$ with probability $1/2$, independently of the baseline process. Let $O_i$ denote the observed count in bin $i$ and $H_i = O_i/N$ the normalized histogram. A noise-free template region provides a known baseline PMF $q(i)$ for the scene, or alternatively $q(i)$ is fit from the observed image within a two-parameter family before testing. To decide whether impulsive noise is present, the scientist plans to use a Pearson $\\chi^2$ goodness-of-fit (GOF) test that compares observed bin counts to the expected counts under the smooth baseline model.\n\nBased only on fundamental definitions of histograms as empirical PMFs, mixture models for contamination, the multinomial sampling model for binned counts, and large-sample properties of the Pearson $\\chi^2$ GOF test, which of the following statements are correct?\n\nA. As $N \\to \\infty$, $H_i$ converges in probability to $(1-p)\\,q(i) + \\dfrac{p}{2}\\,\\big(\\delta_{i,0} + \\delta_{i,L-1}\\big)$ for each bin $i \\in \\{0,1,\\dots,L-1\\}$, where $\\delta_{i,j}$ is the Kronecker delta. Consequently, the limiting histogram exhibits distinct spikes at bins $0$ and $L-1$ whenever $p0$.\n\nB. Under the null hypothesis of no impulsive noise (that is, $p=0$) with known $q(i)$, the Pearson $\\chi^2$ GOF statistic for $L$ bins has an asymptotic $\\chi^2$ distribution with $L-1$ degrees of freedom as $N \\to \\infty$.\n\nC. For any fixed $p0$, the probability that the Pearson $\\chi^2$ GOF test (at any fixed significance level) rejects the null hypothesis approaches $1$ as $N \\to \\infty$.\n\nD. If $q(0)$ and $q(L-1)$ are nonzero, impulsive noise at $0$ and $L-1$ cannot be detected by a Pearson $\\chi^2$ GOF test, because those endpoint bins are already populated under the baseline.\n\nE. To keep the $\\chi^2$ approximation valid when isolating endpoint contamination by defining narrow endpoint bins, one must always merge bins until all expected counts exceed $5$; otherwise, the test is invalid regardless of $N$.\n\nF. If, instead of using a known $q(i)$, the baseline PMF is fit from the same image data by estimating $2$ free parameters before computing the Pearson statistic, then the asymptotic degrees of freedom remain $L-1$.\n\nSelect all that apply.", "solution": "Let's analyze each statement based on statistical theory. The observed intensity is a mixture model with PMF $p_{obs}(i) = (1-p)q(i) + p \\cdot r(i)$, where $q(i)$ is the baseline PMF and $r(i) = \\frac{1}{2}(\\delta_{i,0} + \\delta_{i,L-1})$ is the noise PMF.\n\n*   **Statement A: Correct.** The Weak Law of Large Numbers states that the empirical PMF (the normalized histogram, $H_i$) converges in probability to the true underlying PMF. Here, the true PMF is $p_{obs}(i)$. The formula given is the correct expression for this mixture PMF. For any $p>0$, this adds a constant probability mass of $p/2$ to bins 0 and $L-1$, creating abrupt increases or \"spikes\" relative to the smoothly varying baseline $q(i)$.\n\n*   **Statement B: Correct.** This is the fundamental theorem for the Pearson $\\chi^2$ goodness-of-fit test. When the null hypothesis ($p=0$, so the PMF is $q(i)$) is true and the probabilities $q(i)$ are known (not estimated from the data), the test statistic asymptotically follows a $\\chi^2$ distribution. The degrees of freedom are the number of bins ($L$) minus 1 (for the constraint that the probabilities sum to 1). So, the degrees of freedom are $L-1$.\n\n*   **Statement C: Correct.** This describes the consistency of the $\\chi^2$ test. When the alternative hypothesis is true (i.e., $p>0$ is fixed), the observed counts $O_i$ will center around $N \\cdot p_{obs}(i)$, which is different from the expected counts under the null hypothesis, $E_i = N \\cdot q(i)$. The test statistic, $X^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}$, will have terms where $(O_i/N - q(i))^2$ converges to a non-zero constant. Thus, the overall statistic $X^2$ grows proportionally to $N$. Since the test's critical value for any fixed significance level is a constant, the statistic will eventually exceed it as $N \\to \\infty$. Therefore, the power of the test approaches 1.\n\n*   **Statement D: Incorrect.** The $\\chi^2$ test doesn't check if a bin is empty or not; it checks if the observed count in a bin significantly deviates from the expected count. The impulsive noise adds a predictable number of counts ($N \\cdot p/2$) to bins 0 and $L-1$. The test is designed to detect exactly this kind of statistically significant deviation from the baseline expectation $N \\cdot q(0)$ and $N \\cdot q(L-1)$. The fact that the bins are already populated is irrelevant to the test's ability to detect a further significant increase in counts.\n\n*   **Statement E: Incorrect.** The rule that expected counts should exceed 5 is a practical rule of thumb for finite sample sizes to ensure the $\\chi^2$ distribution is a good approximation. It is not a condition for the asymptotic validity of the test. The asymptotic theory requires that the expected counts $Nq(i)$ go to infinity as $N \\to \\infty$, which holds as long as $q(i) > 0$. The statement that the test is \"invalid regardless of $N$\" is false.\n\n*   **Statement F: Incorrect.** According to Fisher's theorem on goodness-of-fit tests, if parameters of the null distribution are estimated from the data being tested, the degrees of freedom of the asymptotic $\\chi^2$ distribution must be reduced. Specifically, one degree of freedom is lost for each independent parameter estimated. If 2 parameters are estimated to define $q(i)$, the degrees of freedom become $(L-1) - 2 = L-3$.\n\nTherefore, the correct statements are A, B, and C.", "answer": "$$\\boxed{ABC}$$", "id": "4891613"}]}