{"hands_on_practices": [{"introduction": "In Computed Tomography (CT), the final image quality is fundamentally limited by noise. This practice delves into the statistical nature of CT measurements by modeling the two primary noise sources: the inherent randomness of photon detection (quantum noise) and the thermal noise from the detector electronics. By deriving the Signal-to-Noise Ratio (SNR) as a function of acquisition parameters, you will gain a quantitative understanding of how dose, represented by the $mAs$ product, governs the transition between a quantum-limited and an electronic-noise-limited imaging regime [@problem_id:4545337].", "problem": "A Computed Tomography (CT) projection measurement through a uniform slab of thickness $L$ and linear attenuation coefficient $\\mu$ is modeled as a photon counting process with additive electronic noise. Let the tube current–time product be $m$ measured in milliampere–seconds ($\\mathrm{mAs}$). The mean incident photon count per detector channel before attenuation is $N_{0}(m) = \\alpha\\, m$, where $\\alpha$ is a system-dependent constant. The mean transmitted photon count is $N(m) = N_{0}(m)\\,\\exp(-\\mu L)$. The measured raw signal is $M = I + E$, where $I$ is a Poisson random variable with mean $N(m)$ (independent of $E$), and $E$ is an additive electronic noise modeled as a zero-mean Gaussian random variable with variance $\\sigma_{e}^{2}$ (independent of $I$). The reconstruction computes the line integral by a logarithmic transform $s = -\\ln\\!\\big(M/N_{0}(m)\\big)$.\n\nStarting from the definitions of Poisson statistics and the properties of independent additive Gaussian noise, use first-order error propagation (the delta method) for the logarithmic transform to derive an explicit analytical expression for the Signal-to-Noise Ratio (SNR) of $s$ as a function of $m$, $\\alpha$, $\\mu$, $L$, and $\\sigma_{e}$. Then determine the asymptotic dependence of this SNR on $m$ in the regimes $m \\to \\infty$ and $m \\to 0$.\n\nFinally, define $m_{\\star}$ as the value of $m$ for which the photon counting variance equals the electronic noise variance in the raw measurement, that is, $N(m_{\\star}) = \\sigma_{e}^{2}$. For the numerical evaluation, use $\\alpha = 5.0 \\times 10^{5}\\,$ photons per $\\mathrm{mAs}$ per detector channel, $L = 1.0\\,\\mathrm{cm}$, $\\mu = 0.5\\,\\mathrm{cm}^{-1}$, and $\\sigma_{e} = 400\\,$ equivalent photon counts. Compute the SNR of $s$ at $m = m_{\\star}$ and report it as a dimensionless number. Round your answer to $4$ significant figures.", "solution": "The problem asks for the derivation of the Signal-to-Noise Ratio (SNR) for the reconstructed line integral $s$, its asymptotic behavior, and a specific numerical evaluation. The reconstructed signal is given by the logarithmic transform $s = -\\ln(M/N_0(m))$, where $M$ is the measured raw signal and $N_0(m)$ is the mean incident photon count.\n\nFirst, we characterize the measured raw signal $M = I + E$.\nThe mean of $M$ is $E[M]$:\n$$E[M] = E[I + E] = E[I] + E[E]$$\nWe are given that $I$ is a Poisson random variable with mean $N(m)$, so $E[I] = N(m)$. We are also given that $E$ is a Gaussian random variable with mean $0$, so $E[E]=0$. Therefore, the mean of the measured signal is:\n$$E[M] = N(m) = \\alpha m \\exp(-\\mu L)$$\n\nThe variance of $M$ is $\\text{Var}(M) = \\sigma_M^2$:\n$$\\sigma_M^2 = \\text{Var}(I + E) = \\text{Var}(I) + \\text{Var}(E)$$\nFor a Poisson distribution, the variance is equal to the mean, so $\\text{Var}(I) = E[I] = N(m)$. The variance of the electronic noise $E$ is given as $\\sigma_e^2$. Thus, the variance of the measured signal is:\n$$\\sigma_M^2 = N(m) + \\sigma_e^2$$\n\nThe SNR of $s$, denoted $\\mathrm{SNR}_s$, is defined as the ratio of the mean of $s$ to the standard deviation of $s$:\n$$\\mathrm{SNR}_s = \\frac{|E[s]|}{\\sqrt{\\text{Var}(s)}} = \\frac{|S|}{\\sigma_s}$$\nWe will use a first-order Taylor expansion (the delta method) to approximate the mean and variance of $s = f(M) = -\\ln(M/N_0(m))$.\n\nThe signal component, $S$, is the expected value of $s$. To first order, $E[f(X)] \\approx f(E[X])$.\n$$S = E[s] \\approx s(E[M]) = -\\ln\\left(\\frac{E[M]}{N_0(m)}\\right) = -\\ln\\left(\\frac{N(m)}{N_0(m)}\\right)$$\nSubstituting $N(m) = N_0(m)\\exp(-\\mu L)$:\n$$S \\approx -\\ln\\left(\\frac{N_0(m)\\exp(-\\mu L)}{N_0(m)}\\right) = -\\ln(\\exp(-\\mu L)) = \\mu L$$\nThis is the true line integral, which is the desired signal.\n\nThe noise component is the standard deviation of $s$, $\\sigma_s$. We first find the variance, $\\sigma_s^2$. The delta method states that for a function $f(X)$, $\\text{Var}(f(X)) \\approx (f'(E[X]))^2 \\text{Var}(X)$.\nHere, $s(M) = -\\ln(M/N_0(m)) = -\\ln(M) + \\ln(N_0(m))$. The derivative of $s$ with respect to $M$ is:\n$$\\frac{ds}{dM} = \\frac{d}{dM} (-\\ln(M) + \\ln(N_0(m))) = -\\frac{1}{M}$$\nEvaluating the derivative at the mean of $M$, $E[M]=N(m)$:\n$$\\left.\\frac{ds}{dM}\\right|_{M=N(m)} = -\\frac{1}{N(m)}$$\nThe variance of $s$ is then:\n$$\\sigma_s^2 = \\text{Var}(s) \\approx \\left(-\\frac{1}{N(m)}\\right)^2 \\sigma_M^2 = \\frac{\\sigma_M^2}{N(m)^2}$$\nSubstituting the expression for $\\sigma_M^2$:\n$$\\sigma_s^2 \\approx \\frac{N(m) + \\sigma_e^2}{N(m)^2}$$\n\nNow, we can write the expression for the SNR of $s$:\n$$\\mathrm{SNR}_s = \\frac{|S|}{\\sigma_s} \\approx \\frac{\\mu L}{\\sqrt{\\frac{N(m) + \\sigma_e^2}{N(m)^2}}} = \\frac{\\mu L N(m)}{\\sqrt{N(m) + \\sigma_e^2}}$$\nSince $\\mu$ and $L$ are non-negative physical quantities, $|\\mu L| = \\mu L$.\nSubstituting $N(m) = \\alpha m \\exp(-\\mu L)$, we get the explicit analytical expression for $\\mathrm{SNR}_s$ as a function of the given parameters:\n$$\\mathrm{SNR}_s(m) = \\frac{\\mu L \\alpha m \\exp(-\\mu L)}{\\sqrt{\\alpha m \\exp(-\\mu L) + \\sigma_e^2}}$$\n\nNext, we determine the asymptotic dependence of $\\mathrm{SNR}_s$ on $m$.\n\nFor $m \\to \\infty$ (high-flux limit): The term $\\alpha m \\exp(-\\mu L)$ becomes much larger than $\\sigma_e^2$. The denominator is approximately $\\sqrt{\\alpha m \\exp(-\\mu L)}$.\n$$\\mathrm{SNR}_s(m) \\approx \\frac{\\mu L \\alpha m \\exp(-\\mu L)}{\\sqrt{\\alpha m \\exp(-\\mu L)}} = \\mu L \\sqrt{\\alpha m \\exp(-\\mu L)}$$\nIn this regime, the SNR is proportional to $\\sqrt{m}$, which is characteristic of a system limited by Poisson quantum noise. So, $\\mathrm{SNR}_s(m) \\propto \\sqrt{m}$.\n\nFor $m \\to 0$ (low-flux limit): The term $\\alpha m \\exp(-\\mu L)$ becomes much smaller than $\\sigma_e^2$. The denominator is approximately $\\sqrt{\\sigma_e^2} = \\sigma_e$.\n$$\\mathrm{SNR}_s(m) \\approx \\frac{\\mu L \\alpha m \\exp(-\\mu L)}{\\sigma_e}$$\nIn this regime, the SNR is proportional to $m$, which is characteristic of a system limited by additive electronic noise. So, $\\mathrm{SNR}_s(m) \\propto m$.\n\nFinally, we perform the numerical evaluation. We are given $m_{\\star}$ as the value of $m$ where the photon counting variance equals the electronic noise variance, i.e., $N(m_{\\star}) = \\sigma_e^2$.\nWe evaluate the SNR at this point, $\\mathrm{SNR}_s(m_{\\star})$:\n$$\\mathrm{SNR}_s(m_{\\star}) = \\frac{\\mu L N(m_{\\star})}{\\sqrt{N(m_{\\star}) + \\sigma_e^2}}$$\nSubstituting the condition $N(m_{\\star}) = \\sigma_e^2$:\n$$\\mathrm{SNR}_s(m_{\\star}) = \\frac{\\mu L \\sigma_e^2}{\\sqrt{\\sigma_e^2 + \\sigma_e^2}} = \\frac{\\mu L \\sigma_e^2}{\\sqrt{2 \\sigma_e^2}} = \\frac{\\mu L \\sigma_e^2}{\\sigma_e \\sqrt{2}} = \\frac{\\mu L \\sigma_e}{\\sqrt{2}}$$\nThe given numerical values are:\n$\\mu = 0.5\\,\\mathrm{cm}^{-1}$\n$L = 1.0\\,\\mathrm{cm}$\n$\\sigma_e = 400$\nPlugging these values into the expression for $\\mathrm{SNR}_s(m_{\\star})$:\n$$\\mathrm{SNR}_s(m_{\\star}) = \\frac{(0.5) \\cdot (1.0) \\cdot (400)}{\\sqrt{2}} = \\frac{200}{\\sqrt{2}} = 100\\sqrt{2}$$\nThe numerical value is:\n$$100\\sqrt{2} \\approx 100 \\times 1.41421356... = 141.421356...$$\nRounding to $4$ significant figures, we get $141.4$.", "answer": "$$\\boxed{141.4}$$", "id": "4545337"}, {"introduction": "Zero-filling is a ubiquitous technique in Magnetic Resonance Imaging (MRI) that produces visually smoother, less \"blocky\" images by interpolating the raw data. This visual improvement often leads to the misconception that the image quality and Signal-to-Noise Ratio (SNR) have fundamentally increased. This exercise challenges you to move beyond visual intuition by rigorously deriving the effect of zero-filling on the true pixel-wise SNR, revealing a crucial distinction between image appearance and the actual information content [@problem_id:4923452].", "problem": "A one-dimensional Magnetic Resonance Imaging (MRI) experiment acquires a uniformly sampled set of $k$-space data $\\{K[m]\\}_{m=0}^{N-1}$, where each acquired sample obeys $K[m] = S[m] + \\eta[m]$. The deterministic signal component $S[m]$ is bandlimited to the acquired support, and the noise $\\eta[m]$ is zero-mean complex Gaussian, independent across $m$, with variance $\\operatorname{Var}(\\eta[m]) = \\sigma_{k}^{2}$ for every $m$. The image is reconstructed on an $N$-point grid by the unitary inverse Discrete Fourier Transform (DFT),\n$$\nx_{N}[n] = \\frac{1}{\\sqrt{N}}\\sum_{m=0}^{N-1} K[m] \\exp\\!\\left(i\\,\\frac{2\\pi m n}{N}\\right), \\quad n=0,1,\\dots,N-1.\n$$\nConsider now a zero-filling interpolation in $k$-space to size $M=\\alpha N$ for an integer $\\alpha>1$, defined by\n$$\nK_{\\mathrm{ZP}}[m] =\n\\begin{cases}\nK[m], & m=0,1,\\dots,N-1,\\\\[4pt]\n0, & m=N,\\dots,M-1,\n\\end{cases}\n$$\nfollowed by the $M$-point unitary inverse DFT to produce an oversampled image\n$$\nx_{M}[n] = \\frac{1}{\\sqrt{M}}\\sum_{m=0}^{M-1} K_{\\mathrm{ZP}}[m] \\exp\\!\\left(i\\,\\frac{2\\pi m n}{M}\\right), \\quad n=0,1,\\dots,M-1.\n$$\nDefine the signal-to-noise ratio (SNR) at a pixel as the ratio of the magnitude of the deterministic image component to the standard deviation of the noise in the complex image at that pixel. Using only the linearity of the inverse DFT, the independence and variance of the $k$-space noise, and the fact that the inverse DFT specified above is unitary, derive the closed-form expression for the ratio\n$$\nr(N,M) \\equiv \\frac{\\text{SNR of }x_{M}\\text{ at any pixel}}{\\text{SNR of }x_{N}\\text{ at any pixel}}\n$$\nas a function of $N$ and $M$ under zero-filling interpolation. Your derivation should begin from the stated definitions and properties without invoking any shortcut formulas. The scenario is scientifically realistic: $S[m]$ can be arbitrary within the acquired support so long as it is deterministic and bandlimited, and there is no additional apodization or filtering beyond zero-filling. The final answer must be a single analytic expression in terms of $N$ and $M$. No rounding is required. Explain, within your derivation, why zero-filling can make the image look smoother even though the true SNR does not increase.", "solution": "The signal-to-noise ratio (SNR) at an image pixel is defined as the ratio of the magnitude of the deterministic signal component to the standard deviation of the noise at that pixel. We will derive this quantity for both the $N$-point image, $x_N[n]$, and the $M$-point zero-filled image, $x_M[n]$, and then compute their ratio.\n\nDue to the linearity of the inverse Discrete Fourier Transform (DFT), we can decompose the reconstructed images into their signal and noise components:\n$$x_N[n] = x_{N,S}[n] + x_{N,\\eta}[n]$$\n$$x_M[n] = x_{M,S}[n] + x_{M,\\eta}[n]$$\nwhere the signal components are $x_{N,S}[n] = \\mathcal{F}_N^{-1}\\{S\\}[n]$ and $x_{M,S}[n] = \\mathcal{F}_M^{-1}\\{S_{\\mathrm{ZP}}\\}[n]$, and the noise components are $x_{N,\\eta}[n] = \\mathcal{F}_N^{-1}\\{\\eta\\}[n]$ and $x_{M,\\eta}[n] = \\mathcal{F}_M^{-1}\\{\\eta_{\\mathrm{ZP}}\\}[n]$. The operators $\\mathcal{F}_N^{-1}$ and $\\mathcal{F}_M^{-1}$ denote the $N$-point and $M$-point unitary inverse DFTs, respectively.\n\nFirst, we analyze the noise properties in the image domain. For the $N$-point image, the noise component at pixel $n$ is:\n$$x_{N,\\eta}[n] = \\frac{1}{\\sqrt{N}}\\sum_{m=0}^{N-1} \\eta[m] \\exp\\left(i\\frac{2\\pi mn}{N}\\right)$$\nSince the $k$-space noise $\\eta[m]$ has zero mean, $E[\\eta[m]]=0$, the image-space noise also has zero mean:\n$$E[x_{N,\\eta}[n]] = \\frac{1}{\\sqrt{N}}\\sum_{m=0}^{N-1} E[\\eta[m]] \\exp\\left(i\\frac{2\\pi mn}{N}\\right) = 0$$\nThe variance of the complex noise at pixel $n$ is $\\sigma_{x_N,n}^2 = E[|x_{N,\\eta}[n]|^2]$.\n\\begin{align*} \\sigma_{x_N,n}^2 &= E\\left[ \\left(\\frac{1}{\\sqrt{N}}\\sum_{m=0}^{N-1} \\eta[m] e^{i\\frac{2\\pi mn}{N}}\\right) \\left(\\frac{1}{\\sqrt{N}}\\sum_{k=0}^{N-1} \\eta^*[k] e^{-i\\frac{2\\pi kn}{N}}\\right) \\right] \\\\ &= \\frac{1}{N} \\sum_{m=0}^{N-1} \\sum_{k=0}^{N-1} E[\\eta[m]\\eta^*[k]] e^{i\\frac{2\\pi(m-k)n}{N}}\\end{align*}\nGiven that the noise samples are independent with variance $\\sigma_k^2$, we have $E[\\eta[m]\\eta^*[k]] = E[|\\eta[m]|^2]\\delta_{mk} = \\sigma_k^2 \\delta_{mk}$, where $\\delta_{mk}$ is the Kronecker delta.\n$$\\sigma_{x_N,n}^2 = \\frac{1}{N} \\sum_{m=0}^{N-1} \\sum_{k=0}^{N-1} \\sigma_k^2 \\delta_{mk} e^{i\\frac{2\\pi(m-k)n}{N}} = \\frac{1}{N} \\sum_{m=0}^{N-1} \\sigma_k^2 = \\frac{N \\sigma_k^2}{N} = \\sigma_k^2$$\nThis result also follows directly from the fact that a unitary transform preserves the total variance (Parseval's theorem). The noise variance is uniform across all pixels, $\\sigma_{x_N}^2 = \\sigma_k^2$. The standard deviation of the noise in the $N$-point image is $\\sigma_{x_N} = \\sigma_k$.\n\nNext, we analyze the noise in the $M$-point zero-filled image. The zero-filled noise is $\\eta_{\\mathrm{ZP}}[m] = \\eta[m]$ for $m \\in \\{0, \\dots, N-1\\}$ and $0$ otherwise.\n$$x_{M,\\eta}[n] = \\frac{1}{\\sqrt{M}}\\sum_{m=0}^{M-1} \\eta_{\\mathrm{ZP}}[m] \\exp\\left(i\\frac{2\\pi mn}{M}\\right) = \\frac{1}{\\sqrt{M}}\\sum_{m=0}^{N-1} \\eta[m] \\exp\\left(i\\frac{2\\pi mn}{M}\\right)$$\nThe mean is again zero. The variance is:\n\\begin{align*} \\sigma_{x_M,n}^2 &= E\\left[ \\left(\\frac{1}{\\sqrt{M}}\\sum_{m=0}^{N-1} \\eta[m] e^{i\\frac{2\\pi mn}{M}}\\right) \\left(\\frac{1}{\\sqrt{M}}\\sum_{k=0}^{N-1} \\eta^*[k] e^{-i\\frac{2\\pi kn}{M}}\\right) \\right] \\\\ &= \\frac{1}{M} \\sum_{m=0}^{N-1} \\sum_{k=0}^{N-1} E[\\eta[m]\\eta^*[k]] e^{i\\frac{2\\pi(m-k)n}{M}} \\\\ &= \\frac{1}{M} \\sum_{m=0}^{N-1} \\sigma_k^2 = \\frac{N\\sigma_k^2}{M} \\end{align*}\nThe noise variance is also uniform across pixels in the $M$-point image, $\\sigma_{x_M}^2 = \\frac{N}{M}\\sigma_k^2$. The standard deviation of the noise in the $M$-point image is $\\sigma_{x_M} = \\sigma_k \\sqrt{\\frac{N}{M}}$.\n\nNow, we analyze the deterministic signal components. The problem asks for the ratio of SNRs \"at any pixel,\" which implies the ratio is independent of pixel location. This requires comparing pixels that correspond to the same physical location in the object. An $N$-point grid over a field-of-view (FOV) has pixel spacing $\\Delta x_N=\\text{FOV}/N$. An $M$-point grid has spacing $\\Delta x_M=\\text{FOV}/M$. A point at physical location $p = n_N \\Delta x_N$ in the first image corresponds to the point $p = n_M \\Delta x_M$ in the second. This leads to $n_M = n_N \\frac{\\Delta x_N}{\\Delta x_M} = n_N \\frac{M}{N}$. Since $M = \\alpha N$, the pixel $n$ in the $N$-point image corresponds to pixel $\\alpha n$ in the $M$-point image.\n\nThe signal at pixel $n$ of the $N$-point image is:\n$$x_{N,S}[n] = \\frac{1}{\\sqrt{N}}\\sum_{m=0}^{N-1} S[m] \\exp\\left(i\\frac{2\\pi mn}{N}\\right)$$\nThe signal at the corresponding pixel $\\alpha n$ of the $M$-point image is:\n$$x_{M,S}[\\alpha n] = \\frac{1}{\\sqrt{M}}\\sum_{m=0}^{M-1} S_{\\mathrm{ZP}}[m] \\exp\\left(i\\frac{2\\pi m(\\alpha n)}{M}\\right) = \\frac{1}{\\sqrt{M}}\\sum_{m=0}^{N-1} S[m] \\exp\\left(i\\frac{2\\pi m(\\alpha n)}{\\alpha N}\\right)$$\n$$x_{M,S}[\\alpha n] = \\frac{1}{\\sqrt{M}}\\sum_{m=0}^{N-1} S[m] \\exp\\left(i\\frac{2\\pi mn}{N}\\right)$$\nComparing the magnitudes of the signal components at corresponding locations:\n$$|x_{M,S}[\\alpha n]| = \\frac{1}{\\sqrt{M}}\\left|\\sum_{m=0}^{N-1} S[m] \\exp\\left(i\\frac{2\\pi mn}{N}\\right)\\right|$$\n$$|x_{N,S}[n]| = \\frac{1}{\\sqrt{N}}\\left|\\sum_{m=0}^{N-1} S[m] \\exp\\left(i\\frac{2\\pi mn}{N}\\right)\\right|$$\nThe ratio of the signal magnitudes is therefore:\n$$\\frac{|x_{M,S}[\\alpha n]|}{|x_{N,S}[n]|} = \\frac{1/\\sqrt{M}}{1/\\sqrt{N}} = \\sqrt{\\frac{N}{M}}$$\nThis ratio is independent of the pixel $n$ and the specific signal $S[m]$, as long as the signal is not zero at that location.\n\nWe can now formulate the SNRs. At pixel $n$ of the $N$-point image:\n$$\\mathrm{SNR}_N[n] = \\frac{|x_{N,S}[n]|}{\\sigma_{x_N}} = \\frac{|x_{N,S}[n]|}{\\sigma_k}$$\nAt the corresponding pixel $\\alpha n$ of the $M$-point image:\n$$\\mathrm{SNR}_M[\\alpha n] = \\frac{|x_{M,S}[\\alpha n]|}{\\sigma_{x_M}} = \\frac{|x_{N,S}[n]| \\sqrt{N/M}}{\\sigma_k \\sqrt{N/M}} = \\frac{|x_{N,S}[n]|}{\\sigma_k}$$\nThus, $\\mathrm{SNR}_M[\\alpha n] = \\mathrm{SNR}_N[n]$. The SNR at corresponding spatial locations is identical.\n\nThe ratio is then:\n$$r(N,M) = \\frac{\\mathrm{SNR}_M[\\alpha n]}{\\mathrm{SNR}_N[n]} = 1$$\n\nThe reason zero-filling can make an image look smoother, despite the SNR not increasing, lies in the nature of the interpolation. Zero-filling the $k$-space data from size $N$ to $M$ is mathematically equivalent to sinc interpolation of the image-domain data $x_N[n]$. The resulting image $x_M[n]$ contains points that lie on a smooth, bandlimited curve that passes through the original signal points $x_{N,S}[n]$. Displaying these finely-sampled points creates a visually smoother representation of the underlying continuous object, reducing the \"pixelated\" appearance. This visual enhancement is often misconstrued as an increase in resolution or SNR. However, no new information has been added to the $k$-space data. The noise is also interpolated. While the noise variance per pixel is reduced by a factor of $N/M$, the signal magnitude is also reduced by the same factor $\\sqrt{N/M}$ (due to the change in DFT normalization from $1/\\sqrt{N}$ to $1/\\sqrt{M}$), resulting in an unchanged SNR. This interpolation introduces correlations in the image noise, which means the noise itself also appears smoother (less \"white\"), contributing to the overall smoother aesthetic of the image. The fundamental ability to distinguish signal from noise at any given point, as quantified by the SNR, remains unchanged.", "answer": "$$\\boxed{1}$$", "id": "4923452"}, {"introduction": "After understanding how to model and analyze SNR, the next logical step is to optimize it. When searching for a specific, known signal, such as a particular type of lesion, can we design a process to make it maximally detectable against the background noise? This practice introduces the powerful concept of the matched filter, a cornerstone of signal detection theory, guiding you through the derivation of the theoretically optimal filter and the highest possible SNR it can achieve for a given task [@problem_id:4545398].", "problem": "A two-dimensional radiological image is modeled as a linear superposition of a deterministic lesion template and random noise, written in spatial coordinates as $g(\\mathbf{x}) = s(\\mathbf{x}) + n(\\mathbf{x})$, where $\\mathbf{x} \\in \\mathbb{R}^{2}$. A linear detector forms a scalar decision variable by correlating the image with a filter $h(\\mathbf{x})$, defined as $y = \\int_{\\mathbb{R}^{2}} h(\\mathbf{x})\\, g(\\mathbf{x})\\, d^{2}\\mathbf{x}$. Assume the noise $n(\\mathbf{x})$ is zero-mean, wide-sense stationary (WSS), and characterized by its Noise Power Spectrum (NPS), denoted $\\mathrm{NPS}(\\mathbf{f})$, where $\\mathbf{f}$ is spatial frequency in cycles per millimeter. The Fourier transform pair is defined by\n$$\nS(\\mathbf{f}) = \\int_{\\mathbb{R}^{2}} s(\\mathbf{x})\\, \\exp\\!\\big(-2\\pi i\\, \\mathbf{f}\\cdot\\mathbf{x}\\big)\\, d^{2}\\mathbf{x}, \n\\quad\ns(\\mathbf{x}) = \\int_{\\mathbb{R}^{2}} S(\\mathbf{f})\\, \\exp\\!\\big(2\\pi i\\, \\mathbf{f}\\cdot\\mathbf{x}\\big)\\, d^{2}\\mathbf{f}.\n$$\nSimilarly, $H(\\mathbf{f})$ is the Fourier transform of $h(\\mathbf{x})$. The Signal-to-Noise Ratio (SNR) of the decision variable is defined as the mean response to the template divided by the standard deviation of the response due to noise. Starting only from these definitions, the properties of WSS noise, and standard results such as Parseval’s theorem and the definition of the NPS as the Fourier transform of the noise autocovariance, derive the frequency-domain form of the filter $H(\\mathbf{f})$ that maximizes the SNR of $y$ and the corresponding expression for the maximum squared SNR.\n\nThen, specialize to a radiomics lesion modeled by a circularly symmetric Gaussian template in spatial coordinates,\n$$\ns(\\mathbf{x}) = A \\exp\\!\\Big(-\\frac{\\|\\mathbf{x}\\|^{2}}{2\\sigma^{2}}\\Big),\n$$\nwith amplitude $A = 2.5$ in arbitrary intensity units and spatial scale $\\sigma = 1.2\\,\\mathrm{mm}$. Assume white noise with a constant Noise Power Spectrum $\\mathrm{NPS}(\\mathbf{f}) = N_{0}$, where $N_{0} = 0.3$ in intensity-squared times square-millimeter units per frequency-area. Using your derived result for the maximum squared SNR, compute the numerical value of the squared SNR for this template. Express the final SNR in dimensionless form. Round your final numeric value of the squared SNR to four significant figures.", "solution": "The decision variable $y$ can be separated into a signal component $y_s$ and a noise component $y_n$:\n$$\ny = \\int_{\\mathbb{R}^{2}} h(\\mathbf{x}) [s(\\mathbf{x}) + n(\\mathbf{x})] d^{2}\\mathbf{x} = \\int_{\\mathbb{R}^{2}} h(\\mathbf{x}) s(\\mathbf{x}) d^{2}\\mathbf{x} + \\int_{\\mathbb{R}^{2}} h(\\mathbf{x}) n(\\mathbf{x}) d^{2}\\mathbf{x} = y_s + y_n.\n$$\nThe SNR is defined as the mean response to the template divided by the standard deviation of the response due to noise. The mean response is the expected value of $y$, denoted $\\langle y \\rangle$. Since the signal $s(\\mathbf{x})$ and filter $h(\\mathbf{x})$ are deterministic and the noise is zero-mean ($\\langle n(\\mathbf{x}) \\rangle = 0$), the mean is:\n$$\n\\langle y \\rangle = \\langle y_s + y_n \\rangle = y_s + \\langle y_n \\rangle = \\int_{\\mathbb{R}^{2}} h(\\mathbf{x}) s(\\mathbf{x}) d^{2}\\mathbf{x} + \\int_{\\mathbb{R}^{2}} h(\\mathbf{x}) \\langle n(\\mathbf{x}) \\rangle d^{2}\\mathbf{x} = \\int_{\\mathbb{R}^{2}} h(\\mathbf{x}) s(\\mathbf{x}) d^{2}\\mathbf{x}.\n$$\nThe standard deviation of the response due to noise is the square root of the variance of $y$, $\\sigma_y^2$. The variance is given by $\\sigma_y^2 = \\langle (y - \\langle y \\rangle)^2 \\rangle = \\langle (y_s + y_n - y_s)^2 \\rangle = \\langle y_n^2 \\rangle$. This is the noise power at the output of the filter.\n\nTo find the optimal filter, it is most convenient to work in the frequency domain. Using properties of the Fourier transform, the mean signal response $\\langle y \\rangle$ can be written as:\n$$\n\\langle y \\rangle = \\int_{\\mathbb{R}^{2}} S(-\\mathbf{f}) H(\\mathbf{f}) d^{2}\\mathbf{f} = \\int_{\\mathbb{R}^{2}} S^*(\\mathbf{f}) H(\\mathbf{f}) d^{2}\\mathbf{f},\n$$\nwhere the second equality holds for real-valued signals $s(\\mathbf{x})$. The variance of the output noise, $\\sigma_y^2$, is found by integrating the output Noise Power Spectrum over all frequencies:\n$$\n\\sigma_y^2 = \\int_{\\mathbb{R}^{2}} |H(\\mathbf{f})|^2 \\mathrm{NPS}(\\mathbf{f}) d^{2}\\mathbf{f}.\n$$\nThe squared SNR is therefore:\n$$\n\\mathrm{SNR}^2 = \\frac{(\\langle y \\rangle)^2}{\\sigma_y^2} = \\frac{\\left| \\int_{\\mathbb{R}^{2}} S^*(\\mathbf{f}) H(\\mathbf{f}) d^{2}\\mathbf{f} \\right|^2}{\\int_{\\mathbb{R}^{2}} |H(\\mathbf{f})|^2 \\mathrm{NPS}(\\mathbf{f}) d^{2}\\mathbf{f}}.\n$$\nTo maximize this expression, we apply the Cauchy-Schwarz inequality, which states $\\left| \\int A^*(\\mathbf{f}) B(\\mathbf{f}) d^{2}\\mathbf{f} \\right|^2 \\le \\left( \\int |A(\\mathbf{f})|^2 d^{2}\\mathbf{f} \\right) \\left( \\int |B(\\mathbf{f})|^2 d^{2}\\mathbf{f} \\right)$. Letting $A(\\mathbf{f}) = S(\\mathbf{f})/\\sqrt{\\mathrm{NPS}(\\mathbf{f})}$ and $B(\\mathbf{f}) = H(\\mathbf{f})\\sqrt{\\mathrm{NPS}(\\mathbf{f})}$, we find the upper bound for the SNR:\n$$\n\\mathrm{SNR}^2 \\le \\int_{\\mathbb{R}^{2}} |A(\\mathbf{f})|^2 d^{2}\\mathbf{f} = \\int_{\\mathbb{R}^{2}} \\frac{|S(\\mathbf{f})|^2}{\\mathrm{NPS}(\\mathbf{f})} d^{2}\\mathbf{f}.\n$$\nThis maximum squared SNR is achieved when $B(\\mathbf{f}) = k A(\\mathbf{f})$ for some non-zero complex constant $k$. This yields the optimal filter, known as the pre-whitening matched filter:\n$$\nH(\\mathbf{f}) = k \\frac{S(\\mathbf{f})}{\\mathrm{NPS}(\\mathbf{f})}.\n$$\nNow, we specialize to the given Gaussian template and white noise. The signal is $s(\\mathbf{x}) = A \\exp(-\\frac{\\|\\mathbf{x}\\|^{2}}{2\\sigma^{2}})$ and the noise is white, $\\mathrm{NPS}(\\mathbf{f}) = N_{0}$. The maximum squared SNR is:\n$$\n\\mathrm{SNR}^2_{\\max} = \\int_{\\mathbb{R}^{2}} \\frac{|S(\\mathbf{f})|^2}{N_0} d^{2}\\mathbf{f} = \\frac{1}{N_0} \\int_{\\mathbb{R}^{2}} |S(\\mathbf{f})|^2 d^{2}\\mathbf{f}.\n$$\nBy Parseval's theorem, $\\int |S(\\mathbf{f})|^2 d^2\\mathbf{f} = \\int |s(\\mathbf{x})|^2 d^2\\mathbf{x}$. Since $s(\\mathbf{x})$ is real, $|s(\\mathbf{x})|^2 = s(\\mathbf{x})^2$. The calculation is simplified by working in the spatial domain:\n$$\n\\mathrm{SNR}^2_{\\max} = \\frac{1}{N_0} \\int_{\\mathbb{R}^{2}} s(\\mathbf{x})^2 d^{2}\\mathbf{x} = \\frac{A^2}{N_0} \\int_{\\mathbb{R}^{2}} \\exp\\left(-\\frac{\\|\\mathbf{x}\\|^{2}}{\\sigma^{2}}\\right) d^{2}\\mathbf{x}.\n$$\nThis is a 2D Gaussian integral which evaluates to $\\pi\\sigma^2$. The expression for the maximum squared SNR is:\n$$\n\\mathrm{SNR}^2_{\\max} = \\frac{A^2 \\pi \\sigma^2}{N_0}.\n$$\nNow, we substitute the given numerical values: $A = 2.5$, $\\sigma = 1.2\\,\\mathrm{mm}$, and $N_0 = 0.3$.\n$$\n\\mathrm{SNR}^2_{\\max} = \\frac{(2.5)^2 \\pi (1.2)^2}{0.3} = \\frac{6.25 \\times \\pi \\times 1.44}{0.3} = \\frac{9 \\pi}{0.3} = 30\\pi.\n$$\nCalculating the numerical value:\n$$\n\\mathrm{SNR}^2_{\\max} = 30 \\times 3.14159265...\\approx 94.2477796...\n$$\nRounding to four significant figures, we get $94.25$.", "answer": "$$\\boxed{94.25}$$", "id": "4545398"}]}