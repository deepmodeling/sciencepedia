{"hands_on_practices": [{"introduction": "In medical imaging modalities like MRI, measurements are acquired in the frequency domain (k-space), which is related to the final image by the Fourier transform. A cornerstone of this relationship is Parseval's theorem, which states that for a unitary Fourier transform, the signal's total energy is conserved between the image and frequency domains. This hands-on exercise [@problem_id:4870067] allows you to computationally verify this fundamental principle, strengthening your intuition for the equivalence of information represented in these two critical spaces.", "problem": "You are to write a complete, runnable program that tests the preservation of energy between the image domain and the frequency domain (commonly called k-space in Magnetic Resonance Imaging (MRI)) using the Discrete Fourier Transform (DFT). This task verifies the equality predicted by the foundational principle known as Parseval's theorem under a unitary transform.\n\nBegin from the following fundamental base that is standard in the foundations of medical imaging:\n\n1. The complex-valued signal or image is modeled as a finite array of samples, either one-dimensional or two-dimensional. For a one-dimensional signal of length $N$, write $s[n] \\in \\mathbb{C}$ for $n \\in \\{0,1,\\dots,N-1\\}$; for a two-dimensional array of size $M \\times N$, write $s[m,n] \\in \\mathbb{C}$ for $m \\in \\{0,1,\\dots,M-1\\}$ and $n \\in \\{0,1,\\dots,N-1\\}$.\n\n2. The unitary Discrete Fourier Transform (DFT) is defined by\n$$\n\\hat{s}[k] = \\frac{1}{\\sqrt{N}} \\sum_{n=0}^{N-1} s[n] \\, e^{-i 2\\pi \\frac{n k}{N}}, \\quad k \\in \\{0,1,\\dots,N-1\\},\n$$\nfor the one-dimensional case, and\n$$\n\\hat{s}[u,v] = \\frac{1}{\\sqrt{MN}} \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} s[m,n] \\, e^{-i 2\\pi \\left( \\frac{m u}{M} + \\frac{n v}{N} \\right)}, \\quad (u,v) \\in \\{0,1,\\dots,M-1\\} \\times \\{0,1,\\dots,N-1\\},\n$$\nfor the two-dimensional case, where $i$ is the imaginary unit and all angles are in radians.\n\n3. The energy of a complex array in the image domain is defined by\n$$\nE_{\\text{image}} = \\sum_{n} |s[n]|^2 \\quad \\text{(one-dimensional)}, \\qquad E_{\\text{image}} = \\sum_{m} \\sum_{n} |s[m,n]|^2 \\quad \\text{(two-dimensional)},\n$$\nand the energy in k-space is defined by\n$$\nE_{\\text{k}} = \\sum_{k} |\\hat{s}[k]|^2 \\quad \\text{(one-dimensional)}, \\qquad E_{\\text{k}} = \\sum_{u} \\sum_{v} |\\hat{s}[u,v]|^2 \\quad \\text{(two-dimensional)}.\n$$\n\nYour program must implement the following:\n\n- Use a unitary Fast Fourier Transform (FFT) implementation that corresponds to the above DFT definitions (that is, orthonormal normalization).\n- For each provided test case, compute $E_{\\text{image}}$ and $E_{\\text{k}}$ and verify if $|E_{\\text{image}} - E_{\\text{k}}| \\le \\varepsilon$, with tolerance $\\varepsilon = 10^{-12}$.\n- Produce as output a single line containing a comma-separated list of boolean values enclosed in square brackets, where each boolean corresponds to whether the energy equality holds within tolerance for the respective test case. For example, the output must be of the form $[b_1,b_2,b_3,b_4]$, where each $b_j$ is either $\\text{True}$ or $\\text{False}$ without quotes.\n\nNo physical units are involved in this problem; treat all quantities as dimensionless. All angles in any trigonometric functions must be in radians.\n\nTest Suite:\n\nCompute the above for the following signals/images:\n\n1. One-dimensional complex signal of length $7$:\n   - $s[0] = 1 + 2i$\n   - $s[1] = -0.5 + 0.25i$\n   - $s[2] = 0 + 0i$\n   - $s[3] = 3 - 4i$\n   - $s[4] = -2 + 0i$\n   - $s[5] = 0.1 + 0.2i$\n   - $s[6] = -1.5 - 0.5i$\n\n2. Two-dimensional complex image of size $8 \\times 8$ defined by the analytic formula\n   $$\n   s[m,n] = \\exp\\!\\big(-\\alpha \\big((m - m_0)^2 + (n - n_0)^2\\big)\\big) + i \\, \\beta \\, \\sin\\!\\Big(2\\pi \\Big(\\frac{u_0 m}{M} + \\frac{v_0 n}{N}\\Big)\\Big),\n   $$\n   with parameters $M = 8$, $N = 8$, $\\alpha = 0.3$, $\\beta = 0.2$, $m_0 = 2.3$, $n_0 = 5.5$, $u_0 = 1.5$, $v_0 = 2.0$. All trigonometric angles are in radians.\n\n3. One-dimensional complex signal of length $16$ that is identically zero:\n   - $s[n] = 0 + 0i$ for all $n \\in \\{0,1,\\dots,15\\}$.\n\n4. Two-dimensional complex image of size $10 \\times 12$ containing a single complex impulse:\n   - $s[p,q] = 3 - 4i$ at $(p,q) = (2,7)$, and $s[m,n] = 0 + 0i$ for all other $(m,n)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[b_1,b_2,b_3,b_4]$). Each $b_j$ must be a boolean value indicating whether $|E_{\\text{image}} - E_{\\text{k}}| \\le 10^{-12}$ for the corresponding test case.", "solution": "The problem requires the verification of Parseval's theorem for the unitary Discrete Fourier Transform (DFT) across several test cases. This theorem is a fundamental principle in Fourier analysis and signal processing, establishing that the energy of a signal is conserved under the unitary DFT. In medical imaging, particularly Magnetic Resonance Imaging (MRI), this corresponds to the equality of energy between the image domain and the frequency domain, or k-space.\n\nThe energy of a discrete, complex-valued signal $s$ is defined as the sum of the squared magnitudes of its samples. This is equivalent to the squared $L_2$-norm of the signal. For a one-dimensional signal $s[n]$ of length $N$, the image-domain energy is:\n$$\nE_{\\text{image}} = \\sum_{n=0}^{N-1} |s[n]|^2\n$$\nFor a two-dimensional signal $s[m,n]$ of size $M \\times N$, the energy is:\n$$\nE_{\\text{image}} = \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} |s[m,n]|^2\n$$\n\nThe problem defines a unitary DFT. The transform of a signal $s$ results in a k-space signal $\\hat{s}$. For the one-dimensional case, the transform is:\n$$\n\\hat{s}[k] = \\frac{1}{\\sqrt{N}} \\sum_{n=0}^{N-1} s[n] \\, e^{-i 2\\pi \\frac{n k}{N}}\n$$\nFor the two-dimensional case, it is:\n$$\n\\hat{s}[u,v] = \\frac{1}{\\sqrt{MN}} \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} s[m,n] \\, e^{-i 2\\pi \\left( \\frac{m u}{M} + \\frac{n v}{N} \\right)}\n$$\nThe normalization factors $\\frac{1}{\\sqrt{N}}$ and $\\frac{1}{\\sqrt{MN}}$ ensure the transform is unitary.\n\nThe corresponding energy in k-space is calculated analogously to the image-domain energy:\n$$\nE_{\\text{k}} = \\sum_{k=0}^{N-1} |\\hat{s}[k]|^2 \\quad \\text{(one-dimensional)}\n$$\n$$\nE_{\\text{k}} = \\sum_{u=0}^{M-1} \\sum_{v=0}^{N-1} |\\hat{s}[u,v]|^2 \\quad \\text{(two-dimensional)}\n$$\n\nParseval's theorem for the unitary DFT states that the energy is preserved, i.e., $E_{\\text{image}} = E_{\\text{k}}$. Our task is to numerically verify this equality for four distinct signals. Due to the nature of floating-point arithmetic, we will not expect exact equality but will test if the absolute difference is within a small tolerance $\\varepsilon = 10^{-12}$:\n$$\n|E_{\\text{image}} - E_{\\text{k}}| \\le \\varepsilon\n$$\n\nThe computational procedure for each test case is as follows:\n1.  Construct the input signal $s$ as a NumPy array of complex numbers.\n2.  Calculate the image-domain energy $E_{\\text{image}}$ by summing the squared absolute values of the elements of $s$. This can be computed via `numpy.sum(numpy.abs(s)**2)`.\n3.  Compute the k-space representation $\\hat{s}$ using the Fast Fourier Transform (FFT) algorithm. To match the problem's unitary definition, the `norm=\"ortho\"` option in NumPy's FFT functions (`numpy.fft.fft` for 1D, `numpy.fft.fft2` for 2D) must be used. This applies the correct orthonormal scaling.\n4.  Calculate the k-space energy $E_{\\text{k}}$ by summing the squared absolute values of the elements of $\\hat{s}$.\n5.  Compare $E_{\\text{image}}$ and $E_{\\text{k}}$ using the specified tolerance $\\varepsilon$. The result is a boolean value indicating whether the energy conservation principle holds.\n\nThe four test cases are implemented as follows:\n-   **Case 1:** A one-dimensional complex signal of length $7$ is created directly from the provided list of values.\n-   **Case 2:** A two-dimensional complex image of size $8 \\times 8$ is generated programmatically. We create two grids of indices, $m \\in \\{0, \\dots, 7\\}$ and $n \\in \\{0, \\dots, 7\\}$, and apply the given analytic formula to each coordinate pair $(m,n)$.\n-   **Case 3:** A one-dimensional complex signal of length $16$ is created as an array of zeros. This serves as a trivial base case where both energies are expected to be exactly $0$.\n-   **Case 4:** A two-dimensional complex image of size $10 \\times 12$ is created as an array of zeros, with a single non-zero value (a complex impulse) at coordinates $(2,7)$.\n\nThe program will execute this procedure for all four cases and report the boolean results in the specified format.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Verifies Parseval's theorem for the unitary DFT on four test cases.\n    \"\"\"\n    \n    tolerance = 1e-12\n    results = []\n\n    # --- Test Case 1: 1D complex signal of length 7 ---\n    s1 = np.array([\n        1 + 2j,\n        -0.5 + 0.25j,\n        0 + 0j,\n        3 - 4j,\n        -2 + 0j,\n        0.1 + 0.2j,\n        -1.5 - 0.5j\n    ], dtype=np.complex128)\n    \n    # --- Test Case 2: 2D complex image of size 8x8 from formula ---\n    M2, N2 = 8, 8\n    alpha = 0.3\n    beta = 0.2\n    m0, n0 = 2.3, 5.5\n    u0, v0 = 1.5, 2.0\n    \n    m_indices, n_indices = np.meshgrid(np.arange(M2), np.arange(N2), indexing='ij')\n    \n    term1 = np.exp(-alpha * ((m_indices - m0)**2 + (n_indices - n0)**2))\n    term2 = 1j * beta * np.sin(2 * np.pi * (u0 * m_indices / M2 + v0 * n_indices / N2))\n    s2 = term1 + term2\n\n    # --- Test Case 3: 1D complex signal of length 16 (all zeros) ---\n    s3 = np.zeros(16, dtype=np.complex128)\n\n    # --- Test Case 4: 2D complex image 10x12 with a single impulse ---\n    M4, N4 = 10, 12\n    s4 = np.zeros((M4, N4), dtype=np.complex128)\n    s4[2, 7] = 3 - 4j\n    \n    signals = [s1, s2, s3, s4]\n\n    for s in signals:\n        # Calculate image domain energy\n        e_image = np.sum(np.abs(s)**2)\n\n        # Compute k-space representation using unitary FFT\n        if s.ndim == 1:\n            s_hat = np.fft.fft(s, norm=\"ortho\")\n        else: # s.ndim == 2\n            s_hat = np.fft.fft2(s, norm=\"ortho\")\n        \n        # Calculate k-space energy\n        e_k = np.sum(np.abs(s_hat)**2)\n        \n        # Verify if energy is preserved within tolerance\n        is_preserved = np.abs(e_image - e_k) <= tolerance\n        results.append(is_preserved)\n\n    # Format the final output string\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "4870067"}, {"introduction": "Image reconstruction is fundamentally an inverse problem: estimating an image $x$ from measurements $y = Ax + e$. A naive inversion of the system matrix $A$ can be disastrously sensitive to noise $e$, especially when $A$ is ill-conditioned. This exercise [@problem_id:4870009] introduces regularization through Truncated Singular Value Decomposition (TSVD), a classic method to stabilize the solution. You will derive and then numerically explore the essential bias-variance tradeoff, seeing firsthand how truncating small singular values reduces noise amplification (variance) at the cost of losing some image detail (bias).", "problem": "Consider a linear inverse problem in a complex-valued imaging system where measurements are modeled as $y \\in \\mathbb{C}^m$, an unknown image parameter vector $x \\in \\mathbb{C}^n$, a known system matrix $A \\in \\mathbb{C}^{m \\times n}$, and additive zero-mean complex noise $e \\in \\mathbb{C}^m$ with covariance $\\sigma^2 I_m$. The forward model is $y = A x + e$. Let the Singular Value Decomposition (SVD) of $A$ be $A = U \\Sigma V^{H}$, where $U \\in \\mathbb{C}^{m \\times m}$ and $V \\in \\mathbb{C}^{n \\times n}$ are unitary, $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with nonnegative singular values $\\{\\sigma_i\\}$, and $H$ denotes the conjugate transpose. For a truncation level $k$ with $0 \\leq k \\leq r$ where $r$ is the numerical rank of $A$, the truncated SVD estimator is defined as $x_k = V_k \\Sigma_k^{-1} U_k^{H} y$, where $U_k \\in \\mathbb{C}^{m \\times k}$ contains the first $k$ left singular vectors, $V_k \\in \\mathbb{C}^{n \\times k}$ contains the first $k$ right singular vectors, and $\\Sigma_k \\in \\mathbb{R}^{k \\times k}$ contains the first $k$ singular values on its diagonal.\n\nStarting from the definitions above and the linear estimator framework, derive expressions for the bias, variance, and the expected squared reconstruction error $E[\\|x_k - x\\|_2^2]$ of the truncated SVD estimator as functions of the truncation level $k$, the singular values $\\{\\sigma_i\\}$, the right singular vectors $V_k$, the unknown vector $x$, and the noise variance $\\sigma^2$. Your derivation must be based on the properties of unitary matrices, projections, and linear estimators for complex-valued random vectors with covariance $\\sigma^2 I_m$.\n\nThen, implement a complete program that, for each test case listed below, computes:\n- For every truncation level $k$ from $0$ to $r$:\n    - The squared bias term $\\| (I_n - V_k V_k^{H}) x \\|_2^2$.\n    - The variance term $\\sigma^2 \\sum_{i=1}^{k} \\sigma_i^{-2}$.\n    - The expected squared error $E[\\|x_k - x\\|_2^2]$ as the sum of the squared bias and the variance term.\n- The truncation level $k^{\\star}$ (choose the smallest $k$ in case of a tie) that minimizes the expected squared error.\n- The expected squared error at $k^{\\star}$, along with the squared bias and variance terms at $k^{\\star}$.\n\nAll vector norms must be the Euclidean $2$-norm, and all matrix transposes involving complex quantities must use the conjugate transpose operation. There are no physical units required in this problem. Angles are not used in this problem.\n\nTest suite (each case provides $A$, $x$, and $\\sigma^2$):\n\n- Case $1$ (diagonal, well-conditioned):\n    - $m = n = 5$,\n    - $A = \\mathrm{diag}([1.0, 0.9, 0.8, 0.7, 0.6])$,\n    - $x = [1.0, -0.5, 0.7, -0.3, 0.2]^T$,\n    - $\\sigma^2 = 0.0001$.\n\n- Case $2$ (diagonal, ill-conditioned):\n    - $m = n = 6$,\n    - $A = \\mathrm{diag}([5.0, 3.0, 1.5, 0.6, 0.3, 0.1])$,\n    - $x = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]^T$,\n    - $\\sigma^2 = 0.02$.\n\n- Case $3$ (partial discrete Fourier transform, complex-valued, underdetermined):\n    - $m = 6$, $n = 8$,\n    - Let $F \\in \\mathbb{C}^{8 \\times 8}$ be the unitary discrete Fourier transform matrix with entries $F_{j\\ell} = \\frac{1}{\\sqrt{8}} \\exp\\left(-\\mathrm{i} \\frac{2\\pi}{8} j \\ell\\right)$ for $j,\\ell \\in \\{0,1,\\dots,7\\}$.\n    - $A$ is formed by taking the first $6$ rows of $F$ (i.e., $A \\in \\mathbb{C}^{6 \\times 8}$ is the row-restricted unitary matrix).\n    - $x = [1.0 + 0.0\\mathrm{i}, 0.0 + 1.0\\mathrm{i}, -1.0 + 0.0\\mathrm{i}, 0.0 - 1.0\\mathrm{i}, 0.5 + 0.0\\mathrm{i}, -0.5 + 0.0\\mathrm{i}, 0.0 + 0.25\\mathrm{i}, -0.25 + 0.0\\mathrm{i}]^T$,\n    - $\\sigma^2 = 0.1$.\n\nProgram requirements:\n- For each test case, compute the truncation-level-dependent quantities described above, determine $k^{\\star}$, and report, in the specified output format, the tuple $(k^{\\star}, E[\\|x_{k^{\\star}} - x\\|_2^2], \\| (I_n - V_{k^{\\star}} V_{k^{\\star}}^{H}) x \\|_2^2, \\sigma^2 \\sum_{i=1}^{k^{\\star}} \\sigma_i^{-2})$.\n- Floating-point numbers in the final output must be rounded to $6$ decimal places.\n- The final output format must be a single line containing a comma-separated list enclosed in square brackets, concatenating the four values for each test case in order. Concretely, the output must be of the form $[k_1^{\\star},\\ \\text{mse}_1,\\ \\text{bias}_1,\\ \\text{var}_1,\\ k_2^{\\star},\\ \\text{mse}_2,\\ \\text{bias}_2,\\ \\text{var}_2,\\ k_3^{\\star},\\ \\text{mse}_3,\\ \\text{bias}_3,\\ \\text{var}_3]$, where $k_j^{\\star}$ is an integer and each of the other values is a float rounded to $6$ decimal places.\n\nYour derivation must avoid shortcut formulas and proceed from the definitions of SVD, linear estimators, unitary projections, and covariance in complex-valued spaces. Ensure scientific realism and internal consistency. The program must be self-contained and reproducible without any external input.", "solution": "The problem requires the derivation of the bias, variance, and expected squared error for the truncated Singular Value Decomposition (TSVD) estimator in the context of a linear inverse problem, followed by a numerical implementation.\n\nThe linear model is given by $y = Ax + e$, where $y \\in \\mathbb{C}^m$ are the measurements, $x \\in \\mathbb{C}^n$ is the unknown vector, $A \\in \\mathbb{C}^{m \\times n}$ is the system matrix, and $e \\in \\mathbb{C}^m$ is zero-mean complex additive noise with covariance $E[ee^H] = \\sigma^2 I_m$.\n\nThe Singular Value Decomposition (SVD) of $A$ is $A = U \\Sigma V^H$, where $U$ and $V$ are unitary matrices and $\\Sigma$ is a real-valued $m \\times n$ diagonal matrix with non-negative singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$, where $r = \\text{rank}(A)$. We can express $A$ as a sum of outer products: $A = \\sum_{i=1}^r \\sigma_i u_i v_i^H$, where $u_i$ and $v_i$ are the $i$-th columns of $U$ and $V$, respectively.\n\nThe TSVD estimator for a truncation level $k$ ($0 \\le k \\le r$) is defined as $x_k = V_k \\Sigma_k^{-1} U_k^H y$. Here, $V_k$ consists of the first $k$ columns of $V$, $U_k$ consists of the first $k$ columns of $U$, and $\\Sigma_k$ is a $k \\times k$ diagonal matrix with the first $k$ singular values $\\{\\sigma_1, \\dots, \\sigma_k\\}$. The estimator can also be written using the pseudoinverse of the rank-$k$ approximation of $A$, $A_k = U_k \\Sigma_k V_k^H$, as $x_k = A_k^\\dagger y$.\n\n### Derivation of the Bias\n\nThe bias of an estimator $\\hat{\\theta}$ is defined as $B(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$. For the TSVD estimator $x_k$, the bias is $B(x_k) = E[x_k] - x$.\n\nFirst, we compute the expected value of the estimator, $E[x_k]$:\n$$E[x_k] = E[V_k \\Sigma_k^{-1} U_k^H y]$$\nSince $V_k$, $\\Sigma_k$, and $U_k$ are deterministic, we can move the expectation operator inside:\n$$E[x_k] = V_k \\Sigma_k^{-1} U_k^H E[y]$$\nThe expected value of the measurements $y$ is:\n$$E[y] = E[Ax + e] = A E[x] + E[e]$$\nSince $x$ is a deterministic (but unknown) vector and the noise $e$ is zero-mean ($E[e]=0$), we have $E[y] = Ax$.\nSubstituting this back into the expression for $E[x_k]$:\n$$E[x_k] = V_k \\Sigma_k^{-1} U_k^H (Ax)$$\nNow, we substitute the SVD of $A$:\n$$E[x_k] = (V_k \\Sigma_k^{-1} U_k^H) (U \\Sigma V^H) x$$\nThe term $U_k^H U$ represents the product of the first $k$ columns of $U$ (transposed and conjugated) with the full matrix $U$. Since the columns of $U$ are orthonormal, $u_i^H u_j = \\delta_{ij}$, the resulting $k \\times m$ matrix is $[I_k, 0]$. So, $U_k^H A = U_k^H (U \\Sigma V^H) = [I_k, 0] \\Sigma V^H$. The product $[I_k, 0] \\Sigma$ selects the top $k$ rows of $\\Sigma$, which we can write as $[\\Sigma_k, 0] V^H$.\nLet's use a more direct approach. The product $A_k^\\dagger A = (V_k \\Sigma_k^{-1} U_k^H)(U \\Sigma V^H)$ simplifies nicely.\n$$A_k^\\dagger A = V_k \\Sigma_k^{-1} (U_k^H U) \\Sigma V^H$$\nSince $U_k^H U = [I_k, 0]$, $U_k^H U \\Sigma$ is a $k \\times n$ matrix whose first $k$ columns form $\\Sigma_k$ and the rest are zero. That is, $U_k^H U \\Sigma V^H = (\\sum_{i=1}^k e_i \\sigma_i v_i^H)$, where $e_i$ are standard basis vectors. A simpler way is to note that $A_k^\\dagger A = V_k \\Sigma_k^{-1} U_k^H \\sum_{i=1}^r \\sigma_i u_i v_i^H = V_k \\Sigma_k^{-1} \\sum_{i=1}^k \\sigma_i (U_k^H u_i) v_i^H = V_k \\Sigma_k^{-1} \\sum_{i=1}^k \\sigma_i e_i v_i^H = V_k \\Sigma_k^{-1} \\Sigma_k V_k^H = V_k V_k^H$.\nThus, the expected value of the estimator is:\n$$E[x_k] = (V_k V_k^H) x$$\nThe matrix $P_k = V_k V_k^H$ is the orthogonal projector onto the subspace spanned by the first $k$ right singular vectors.\nThe bias vector is therefore:\n$$B(x_k) = E[x_k] - x = V_k V_k^H x - x = (V_k V_k^H - I_n) x$$\nThe squared bias is the squared Euclidean norm of this vector:\n$$\\|B(x_k)\\|_2^2 = \\|(V_k V_k^H - I_n) x\\|_2^2$$\nSince $(I_n - V_k V_k^H)$ is an orthogonal projector, this is equivalent to $\\|(I_n - V_k V_k^H)x\\|_2^2$. The vector $x$ can be expanded in the basis of right singular vectors $\\{v_i\\}_{i=1}^n$ as $x = \\sum_{i=1}^n (v_i^H x) v_i$. The projection $V_k V_k^H x = \\sum_{i=1}^k v_i v_i^H x = \\sum_{i=1}^k (v_i^H x) v_i$.\nThe bias vector is $(V_k V_k^H - I_n)x = -\\sum_{i=k+1}^n (v_i^H x) v_i$. Due to the orthonormality of the $\\{v_i\\}$, the squared norm is:\n$$\\|B(x_k)\\|_2^2 = \\sum_{i=k+1}^n |v_i^H x|^2$$\n\n### Derivation of the Variance\n\nThe variance of a vector estimator is the trace of its covariance matrix, $\\text{Var}(x_k) = \\text{Tr}(\\text{Cov}(x_k))$. The covariance matrix is $\\text{Cov}(x_k) = E[(x_k - E[x_k])(x_k - E[x_k])^H]$.\nFirst, we find the random component of the estimator:\n$$x_k - E[x_k] = V_k \\Sigma_k^{-1} U_k^H y - V_k \\Sigma_k^{-1} U_k^H (Ax) = V_k \\Sigma_k^{-1} U_k^H (y - Ax) = V_k \\Sigma_k^{-1} U_k^H e$$\nNow, we compute the covariance matrix:\n$$\\text{Cov}(x_k) = E[(V_k \\Sigma_k^{-1} U_k^H e) (V_k \\Sigma_k^{-1} U_k^H e)^H] = E[V_k \\Sigma_k^{-1} U_k^H e e^H U_k \\Sigma_k^{-1} V_k^H]$$\nMoving the deterministic matrices outside the expectation:\n$$\\text{Cov}(x_k) = V_k \\Sigma_k^{-1} U_k^H E[e e^H] U_k \\Sigma_k^{-1} V_k^H$$\nSubstituting $E[e e^H] = \\sigma^2 I_m$:\n$$\\text{Cov}(x_k) = V_k \\Sigma_k^{-1} U_k^H (\\sigma^2 I_m) U_k \\Sigma_k^{-1} V_k^H = \\sigma^2 V_k \\Sigma_k^{-1} (U_k^H U_k) \\Sigma_k^{-1} V_k^H$$\nSince $U_k$ has orthonormal columns, $U_k^H U_k = I_k$. The singular values are real, so $\\Sigma_k^{-1}$ is self-adjoint.\n$$\\text{Cov}(x_k) = \\sigma^2 V_k \\Sigma_k^{-1} I_k \\Sigma_k^{-1} V_k^H = \\sigma^2 V_k \\Sigma_k^{-2} V_k^H$$\nThe variance is the trace of this matrix. Using the cyclic property of the trace, $\\text{Tr}(ABC) = \\text{Tr}(CAB)$:\n$$\\text{Var}(x_k) = \\text{Tr}(\\sigma^2 V_k \\Sigma_k^{-2} V_k^H) = \\sigma^2 \\text{Tr}(\\Sigma_k^{-2} V_k^H V_k)$$\nSince $V_k$ has orthonormal columns, $V_k^H V_k = I_k$.\n$$\\text{Var}(x_k) = \\sigma^2 \\text{Tr}(\\Sigma_k^{-2})$$\nAs $\\Sigma_k^{-2}$ is a diagonal matrix with entries $\\sigma_i^{-2}$ for $i=1, \\dots, k$, the trace is the sum of its diagonal elements:\n$$\\text{Var}(x_k) = \\sigma^2 \\sum_{i=1}^k \\frac{1}{\\sigma_i^2}$$\n\n### Derivation of the Expected Squared Error (MSE)\n\nThe expected squared error, or Mean Squared Error (MSE), is $E[\\|x_k - x\\|_2^2]$. It can be decomposed into bias and variance components:\n$$E[\\|x_k - x\\|_2^2] = E[\\|(x_k - E[x_k]) + (E[x_k] - x)\\|_2^2]$$\nLet the zero-mean random component be $\\tilde{x}_k = x_k - E[x_k]$ and the deterministic bias vector be $B_k = E[x_k] - x$.\n$$E[\\|\\tilde{x}_k + B_k\\|_2^2] = E[(\\tilde{x}_k + B_k)^H (\\tilde{x}_k + B_k)] = E[\\tilde{x}_k^H \\tilde{x}_k + \\tilde{x}_k^H B_k + B_k^H \\tilde{x}_k + B_k^H B_k]$$\nBy linearity of expectation, and since $E[\\tilde{x}_k] = 0$:\n$$E[\\|\\tilde{x}_k + B_k\\|_2^2] = E[\\tilde{x}_k^H \\tilde{x}_k] + E[\\tilde{x}_k^H]B_k + B_k^H E[\\tilde{x}_k] + B_k^H B_k = E[\\|\\tilde{x}_k\\|_2^2] + \\|B_k\\|_2^2$$\nThe first term is the expected norm of the zero-mean component, which is the variance:\n$$E[\\|\\tilde{x}_k\\|_2^2] = E[\\|x_k - E[x_k]\\|_2^2] = E[\\text{Tr}((x_k-E[x_k])(x_k-E[x_k])^H)] = \\text{Tr}(\\text{Cov}(x_k)) = \\text{Var}(x_k)$$\nThe second term is the squared norm of the bias vector.\nTherefore, the MSE is the sum of the variance and the squared bias:\n$$E[\\|x_k - x\\|_2^2] = \\text{Var}(x_k) + \\|B(x_k)\\|_2^2$$\nSubstituting the derived expressions:\n$$E[\\|x_k - x\\|_2^2] = \\sigma^2 \\sum_{i=1}^k \\frac{1}{\\sigma_i^2} + \\sum_{i=k+1}^n |v_i^H x|^2$$\nThis final expression reveals the fundamental bias-variance tradeoff. As the truncation level $k$ increases, the bias term (error from truncating the solution space) decreases, while the variance term (error from noise amplification) increases, particularly when small singular values $\\sigma_i$ are included in the inversion. The optimal $k^{\\star}$ balances these two opposing effects to minimize the total expected error.\nFor the special case $k=0$, the estimator is $x_0 = 0$. The variance sum is empty and thus $0$. The bias sum runs from $i=1$ to $n$, giving $\\sum_{i=1}^n |v_i^H x|^2 = \\|x\\|_2^2$. The MSE is $\\|x\\|_2^2$, which is correct since $E[\\|0-x\\|_2^2] = \\|x\\|_2^2$.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import dft\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    def _compute_metrics_for_case(A, x, sigma_sq):\n        \"\"\"\n        Computes the optimal truncation k and associated metrics for a single case.\n        \"\"\"\n        m, n = A.shape\n        \n        # Perform SVD\n        # full_matrices=True is important to get the full V matrix for n-dim space\n        U, s, Vh = np.linalg.svd(A, full_matrices=True)\n        \n        # Determine numerical rank r\n        # The problem defines k up to r, so we consider only non-zero singular values.\n        # A small tolerance is used for numerical stability.\n        # Although the problems are designed to have clear ranks, this is robust.\n        r = np.sum(s > 1e-12)\n\n        k_values = range(r + 1)\n        mses = []\n        biases = []\n        variances = []\n        \n        # Pre-compute coefficients |v_i^H x|^2\n        # Vh contains rows v_i^H.\n        v_coeffs_sq = np.abs(Vh @ x)**2\n\n        for k in k_values:\n            if k == 0:\n                # For k=0, estimator is x0=0. Variance is 0. Bias^2 is ||x||^2.\n                # bias_sq = np.sum_{i=1..n} |v_i^H x|^2\n                bias_sq = np.sum(v_coeffs_sq)\n                var = 0.0\n            else:\n                # Bias term: sum_{i=k+1..n} |v_i^H x|^2\n                # In 0-based indexing, this is sum over elements from index k onwards.\n                bias_sq = np.sum(v_coeffs_sq[k:])\n                \n                # Variance term: sigma^2 * sum_{i=1..k} 1/sigma_i^2\n                # s contains singular values, s[:k] are the first k values\n                var = sigma_sq * np.sum(1.0 / s[:k]**2)\n\n            mse = bias_sq + var\n            mses.append(mse)\n            biases.append(bias_sq)\n            variances.append(var)\n\n        # Find the optimal truncation level k_star (smallest k in case of a tie)\n        k_star = np.argmin(mses)\n        \n        # Get metrics for k_star\n        min_mse = mses[k_star]\n        bias_at_k_star = biases[k_star]\n        var_at_k_star = variances[k_star]\n\n        return k_star, min_mse, bias_at_k_star, var_at_k_star\n\n    # --- Test Cases ---\n    \n    # Case 1: diagonal, well-conditioned\n    A1 = np.diag([1.0, 0.9, 0.8, 0.7, 0.6])\n    x1 = np.array([1.0, -0.5, 0.7, -0.3, 0.2])\n    sigma_sq1 = 0.0001\n\n    # Case 2: diagonal, ill-conditioned\n    A2 = np.diag([5.0, 3.0, 1.5, 0.6, 0.3, 0.1])\n    x2 = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n    sigma_sq2 = 0.02\n\n    # Case 3: partial discrete Fourier transform, complex\n    F = dft(8, scale='sqrtn')\n    A3 = F[:6, :]\n    x3 = np.array([1.0 + 0.0j, 0.0 + 1.0j, -1.0 + 0.0j, 0.0 - 1.0j, \n                   0.5 + 0.0j, -0.5 + 0.0j, 0.0 + 0.25j, -0.25 + 0.0j])\n    sigma_sq3 = 0.1\n\n    test_cases = [\n        (A1, x1, sigma_sq1),\n        (A2, x2, sigma_sq2),\n        (A3, x3, sigma_sq3),\n    ]\n\n    all_results = []\n    for A, x, sigma_sq in test_cases:\n        k_star, mse, bias_sq, var = _compute_metrics_for_case(A, x, sigma_sq)\n        all_results.append(str(k_star))\n        all_results.append(f\"{mse:.6f}\")\n        all_results.append(f\"{bias_sq:.6f}\")\n        all_results.append(f\"{var:.6f}\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "4870009"}, {"introduction": "Modern imaging leverages prior knowledge about image structure to reconstruct high-quality images from limited data. This is often formulated as an optimization problem that balances data consistency with a regularization term, such as the $\\ell_1$-norm penalty $\\lambda \\|x\\|_1$ which promotes sparsity. This practice [@problem_id:4870062] provides a look inside state-of-the-art algorithms by having you implement a single update step of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA), a powerful method for solving these composite, non-smooth optimization problems.", "problem": "Consider a discrete linear measurement model used in complex-valued medical imaging, where a complex image vector $x \\in \\mathbb{C}^n$ is related to a measurement vector $y \\in \\mathbb{C}^m$ by a known linear operator (matrix) $A \\in \\mathbb{C}^{m \\times n}$. A standard reconstruction approach is to minimize the composite objective\n$$\nf(x) = \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|x\\|_1,\n$$\nwhere $\\lambda \\in \\mathbb{R}_{\\ge 0}$ is a regularization parameter and $\\|x\\|_1$ for complex $x$ is defined as $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$ with $|x_i|$ denoting the complex magnitude of the $i$-th entry. The Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) performs an accelerated proximal gradient step for such composite objectives. Your task is to implement a single FISTA update for a complex-valued image vector and to track whether the objective value decreases after this update.\n\nThe implementation must adhere to the following mathematical constraints:\n\n- Use the standard complex-valued inner product and associated conjugate transpose. The gradient of the smooth data fidelity term $g(x)=\\frac{1}{2}\\|A x - y\\|_2^2$ is taken with respect to the complex vector $x$ using the conjugate transpose of $A$.\n- Use a step size equal to the inverse of the Lipschitz constant $L$ of the gradient of the smooth term $g(x)$, where $L$ equals the square of the spectral norm of $A$, that is $L = \\|A\\|_2^2$. This value should be computed from $A$ (do not hard-code $L$).\n- Apply an entry-wise complex soft-thresholding proximal operator that preserves phase: for each entry $v_i$ in a complex vector $v$, the proximal mapping with threshold $\\tau$ is defined as\n$$\n\\operatorname{soft}_\\tau(v_i) = \\begin{cases}\n0, & |v_i| \\le \\tau,\\\\\n\\left(1 - \\frac{\\tau}{|v_i|}\\right) v_i, & |v_i| > \\tau.\n\\end{cases}\n$$\n- Perform exactly one accelerated proximal gradient (FISTA) update starting from given $x_k$, $x_{k-1}$, and $t_{k-1}$, producing $x_{k+1}$ and $t_k$, and compute $f(x_k)$ and $f(x_{k+1})$.\n\nYour program must implement the above and produce results for the following test suite of parameter values:\n\n- Test case $1$ (happy path, unitary sampling):\n    - $n = 4$, $m = 4$.\n    - $A$ is the $4 \\times 4$ unitary discrete Fourier transform (DFT) matrix with entries\n      $A_{p,q} = \\frac{1}{\\sqrt{4}} \\exp\\left(-\\mathrm{i} \\frac{2\\pi p q}{4}\\right)$ for $p,q \\in \\{0,1,2,3\\}$.\n    - $y = [1 + 2\\mathrm{i},\\ -0.5 + 0.3\\mathrm{i},\\ 0.2 - 1.1\\mathrm{i},\\ 0 + 0\\mathrm{i}]$.\n    - $\\lambda = 0.3$.\n    - $x_k = [-0.2 + 0.1\\mathrm{i},\\ 0.7 - 0.4\\mathrm{i},\\ 0 + 0\\mathrm{i},\\ -0.3 + 0.9\\mathrm{i}]$.\n    - $x_{k-1} = [-0.1 + 0.0\\mathrm{i},\\ 0.5 - 0.5\\mathrm{i},\\ 0.1 - 0.2\\mathrm{i},\\ -0.4 + 0.8\\mathrm{i}]$.\n    - $t_{k-1} = 1.0$.\n- Test case $2$ (boundary, zero measurements, identity forward model):\n    - $n = 4$, $m = 4$.\n    - $A$ is the $4 \\times 4$ identity matrix.\n    - $y = [0 + 0\\mathrm{i},\\ 0 + 0\\mathrm{i},\\ 0 + 0\\mathrm{i},\\ 0 + 0\\mathrm{i}]$.\n    - $\\lambda = 1.0$.\n    - $x_k = [0.3 - 0.2\\mathrm{i},\\ -0.1 + 0.1\\mathrm{i},\\ 0.05 + 0.0\\mathrm{i},\\ -0.5 + 0.0\\mathrm{i}]$.\n    - $x_{k-1} = [0.3 - 0.2\\mathrm{i},\\ -0.1 + 0.1\\mathrm{i},\\ 0.05 + 0.0\\mathrm{i},\\ -0.5 + 0.0\\mathrm{i}]$.\n    - $t_{k-1} = 1.0$.\n- Test case $3$ (edge, ill-conditioned scaling):\n    - $n = 3$, $m = 3$.\n    - $A$ is the $3 \\times 3$ diagonal matrix with diagonal $[2.0,\\ 1.0,\\ 0.5]$.\n    - $y = [0.5 + 0.0\\mathrm{i},\\ -1.0 + 0.5\\mathrm{i},\\ 0.2 - 0.3\\mathrm{i}]$.\n    - $\\lambda = 0.05$.\n    - $x_k = [0.1 + 0.2\\mathrm{i},\\ -0.3 + 0.4\\mathrm{i},\\ 0.5 - 0.6\\mathrm{i}]$.\n    - $x_{k-1} = [0.05 + 0.1\\mathrm{i},\\ -0.25 + 0.35\\mathrm{i},\\ 0.45 - 0.55\\mathrm{i}]$.\n    - $t_{k-1} = 1.0$.\n\nFor each test case, compute the objective values $f(x_k)$ and $f(x_{k+1})$ and a boolean indicating whether the objective decreased, i.e., whether $f(x_{k+1})  f(x_k)$.\n\nFinal output format specification:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of the form $[f(x_k), f(x_{k+1}), \\text{decreased}]$. For example, the output should look like $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$, where $a_i$ and $b_i$ are real numbers (floats) and $c_i$ is a boolean. No angles or physical units are involved; all outputs are dimensionless quantities.", "solution": "The problem requires the implementation of a single iteration of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) for a complex-valued inverse problem common in medical imaging. We must also evaluate the objective function before and after the update to check for a decrease.\n\nThe objective function to be minimized is of a composite form $f(x) = g(x) + h(x)$, where:\n1.  $g(x) = \\frac{1}{2}\\|A x - y\\|_2^2$ is the data fidelity term. This function is convex and has a Lipschitz continuous gradient. Here, $x \\in \\mathbb{C}^n$ is the image vector, $y \\in \\mathbb{C}^m$ is the measurement vector, and $A \\in \\mathbb{C}^{m \\times n}$ is the forward operator. The norm $\\| \\cdot \\|_2$ is the standard Euclidean norm.\n2.  $h(x) = \\lambda \\|x\\|_1$ is the regularization term, promoting sparsity in the solution. The parameter $\\lambda \\in \\mathbb{R}_{\\ge 0}$ controls the strength of this regularization. The complex $\\ell_1$-norm is defined as $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$, where $|x_i|$ is the magnitude of the complex number $x_i$. This function is convex but not differentiable everywhere, which precludes the use of simple gradient descent.\n\nFISTA is an accelerated proximal gradient method well-suited for such composite objective functions. A single iteration, which computes the $(k+1)$-th iterate $x_{k+1}$ from the previous two iterates $x_k$ and $x_{k-1}$, consists of the following steps.\n\n**1. Lipschitz Constant and Step Size Calculation**\n\nThe gradient of the smooth term $g(x)$ with respect to the complex vector $x$ is given by $\\nabla g(x) = A^H(Ax - y)$, where $A^H$ is the conjugate transpose of $A$. The gradient descent part of the algorithm requires a step size $\\alpha$. For guaranteed convergence, $\\alpha$ should be less than or equal to the inverse of the Lipschitz constant $L$ of $\\nabla g(x)$. The constant $L$ is given by the squared spectral norm of $A$:\n$$\nL = \\|A^H A\\|_2 = \\|A\\|_2^2\n$$\nwhere $\\|A\\|_2$ is the largest singular value of $A$. We will use the prescribed step size $\\alpha = 1/L$.\n\n**2. FISTA Update Sequence**\n\nGiven the iterates $x_k$ and $x_{k-1}$, and a momentum parameter $t_{k-1}$, the update proceeds as follows:\n\n-   **Momentum Parameter Update**: First, the momentum parameter $t_k$ is updated:\n    $$\n    t_k = \\frac{1 + \\sqrt{1 + 4 t_{k-1}^2}}{2}\n    $$\n    We are given $t_{k-1}$ as part of the initial state.\n\n-   **Extrapolation Step**: An intermediate extrapolated point $z_k$ is computed using the previous two iterates, introducing momentum into the descent:\n    $$\n    z_k = x_k + \\frac{t_{k-1} - 1}{t_k}(x_k - x_{k-1})\n    $$\n\n-   **Gradient Descent and Proximal Mapping**: A standard proximal gradient step is then performed starting from the extrapolated point $z_k$. This involves a gradient descent step on the smooth part $g(x)$ followed by the application of the proximal operator for the non-smooth part $h(x)$.\n    $$\n    x_{k+1} = \\operatorname{prox}_{\\alpha h}\\left(z_k - \\alpha \\nabla g(z_k)\\right)\n    $$\n    The proximal operator for $h(x) = \\lambda \\|x\\|_1$ is the entry-wise complex soft-thresholding operator. Let $\\tau = \\alpha \\lambda$ be the threshold. The operation is:\n    $$\n    x_{k+1} = \\operatorname{soft}_{\\alpha \\lambda}\\left(z_k - \\alpha A^H(A z_k - y)\\right)\n    $$\n    The complex soft-thresholding function $\\operatorname{soft}_\\tau(\\cdot)$ applied to a complex vector $v$ operates on each entry $v_i$ as defined:\n    $$\n    \\operatorname{soft}_\\tau(v_i) = \\begin{cases}\n    0,  |v_i| \\le \\tau, \\\\\n    \\left(1 - \\frac{\\tau}{|v_i|}\\right) v_i,  |v_i|  \\tau.\n    \\end{cases}\n    $$\n    This operation shrinks the magnitude of entries greater than $\\tau$ towards zero and sets entries with magnitude less than or equal to $\\tau$ to exactly zero, while preserving the phase of the non-zero entries.\n\n**3. Objective Function Evaluation**\n\nTo fulfill the problem's requirements, we must compute the objective function value at the current iterate $x_k$ and the new iterate $x_{k+1}$:\n$$f(x_k) = \\frac{1}{2}\\|A x_k - y\\|_2^2 + \\lambda \\sum_{i=1}^n |x_{k,i}|$$\n$$f(x_{k+1}) = \\frac{1}{2}\\|A x_{k+1} - y\\|_2^2 + \\lambda \\sum_{i=1}^n |x_{k+1,i}|$$\nFinally, we determine if the update resulted in a decrease of the objective function value by checking the boolean condition $f(x_{k+1})  f(x_k)$.\n\nThe implementation will follow these steps for each of the three provided test cases. The spectral norm $\\|A\\|_2$ will be computed numerically using standard linear algebra routines. All vector and matrix operations will use complex arithmetic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by performing a single FISTA update for three test cases\n    and evaluating the objective function.\n    \"\"\"\n\n    # Test case 1: Unitary DFT matrix\n    n1, m1 = 4, 4\n    p, q = np.meshgrid(np.arange(n1), np.arange(m1), indexing='ij')\n    A1 = (1 / np.sqrt(n1)) * np.exp(-1j * 2 * np.pi * p * q / n1)\n    y1 = np.array([1 + 2j, -0.5 + 0.3j, 0.2 - 1.1j, 0 + 0j], dtype=np.complex128)\n    lambda1 = 0.3\n    xk1 = np.array([-0.2 + 0.1j, 0.7 - 0.4j, 0 + 0j, -0.3 + 0.9j], dtype=np.complex128)\n    xk_minus_1_1 = np.array([-0.1 + 0.0j, 0.5 - 0.5j, 0.1 - 0.2j, -0.4 + 0.8j], dtype=np.complex128)\n    tk_minus_1_1 = 1.0\n\n    # Test case 2: Identity matrix, zero measurements\n    n2, m2 = 4, 4\n    A2 = np.eye(n2, dtype=np.complex128)\n    y2 = np.zeros(m2, dtype=np.complex128)\n    lambda2 = 1.0\n    xk2 = np.array([0.3 - 0.2j, -0.1 + 0.1j, 0.05 + 0.0j, -0.5 + 0.0j], dtype=np.complex128)\n    xk_minus_1_2 = xk2.copy()\n    tk_minus_1_2 = 1.0\n\n    # Test case 3: Diagonal scaling matrix\n    n3, m3 = 3, 3\n    A3 = np.diag(np.array([2.0, 1.0, 0.5], dtype=np.complex128))\n    y3 = np.array([0.5 + 0.0j, -1.0 + 0.5j, 0.2 - 0.3j], dtype=np.complex128)\n    lambda3 = 0.05\n    xk3 = np.array([0.1 + 0.2j, -0.3 + 0.4j, 0.5 - 0.6j], dtype=np.complex128)\n    xk_minus_1_3 = np.array([0.05 + 0.1j, -0.25 + 0.35j, 0.45 - 0.55j], dtype=np.complex128)\n    tk_minus_1_3 = 1.0\n    \n    test_cases = [\n        (A1, y1, lambda1, xk1, xk_minus_1_1, tk_minus_1_1),\n        (A2, y2, lambda2, xk2, xk_minus_1_2, tk_minus_1_2),\n        (A3, y3, lambda3, xk3, xk_minus_1_3, tk_minus_1_3),\n    ]\n\n    def objective_function(x, A, y, lambda_reg):\n        \"\"\"Computes the value of the composite objective function.\"\"\"\n        residual = A @ x - y\n        data_fidelity = 0.5 * np.linalg.norm(residual)**2\n        l1_norm = np.sum(np.abs(x))\n        return data_fidelity + lambda_reg * l1_norm\n\n    def complex_soft_threshold(v, tau):\n        \"\"\"Applies the complex soft-thresholding operator entry-wise.\"\"\"\n        v_abs = np.abs(v)\n        result = np.zeros_like(v, dtype=v.dtype)\n        \n        # Indices where magnitude is greater than the threshold\n        idx = v_abs  tau\n        \n        # Apply thresholding only to these elements to avoid division by zero\n        factor = 1 - tau / v_abs[idx]\n        result[idx] = factor * v[idx]\n        \n        return result\n\n    def perform_fista_update(A, y, lambda_reg, xk, xk_minus_1, tk_minus_1):\n        \"\"\"Performs a single FISTA update and returns objective values.\"\"\"\n        \n        # 1. Compute Lipschitz constant and step size\n        L = np.linalg.norm(A, 2)**2\n        alpha = 1.0 / L\n\n        # Evaluate objective at current point xk\n        f_xk = objective_function(xk, A, y, lambda_reg)\n\n        # 2. FISTA update sequence\n        # Update momentum parameter\n        tk = (1.0 + np.sqrt(1.0 + 4.0 * tk_minus_1**2)) / 2.0\n        \n        # Extrapolation step\n        zk = xk + ((tk_minus_1 - 1.0) / tk) * (xk - xk_minus_1)\n\n        # Gradient of smooth term at zk\n        grad_g_zk = A.conj().T @ (A @ zk - y)\n        \n        # Argument for the proximal operator\n        prox_arg = zk - alpha * grad_g_zk\n        \n        # Proximal mapping (soft-thresholding)\n        threshold = alpha * lambda_reg\n        xk_plus_1 = complex_soft_threshold(prox_arg, threshold)\n\n        # Evaluate objective at new point xk+1\n        f_xk_plus_1 = objective_function(xk_plus_1, A, y, lambda_reg)\n        \n        # Check if objective decreased\n        decreased = f_xk_plus_1  f_xk\n        \n        return f_xk, f_xk_plus_1, decreased\n\n    results = []\n    for case in test_cases:\n        A, y, lambda_reg, xk, xk_minus_1, tk_minus_1 = case\n        f_xk, f_xk_plus_1, decreased = perform_fista_update(A, y, lambda_reg, xk, xk_minus_1, tk_minus_1)\n        results.append([f_xk, f_xk_plus_1, decreased])\n\n    # Format the final output string\n    formatted_results = []\n    for res in results:\n        f_xk, f_xk_plus_1, dec = res\n        # Convert boolean to lowercase 'true'/'false' as per convention\n        dec_str = str(dec).lower()\n        formatted_results.append(f\"[{f_xk},{f_xk_plus_1},{dec_str}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\n\nsolve()\n```", "id": "4870062"}]}