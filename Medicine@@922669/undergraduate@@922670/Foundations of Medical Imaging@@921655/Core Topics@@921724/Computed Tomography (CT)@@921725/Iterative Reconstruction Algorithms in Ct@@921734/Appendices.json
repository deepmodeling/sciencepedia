{"hands_on_practices": [{"introduction": "This exercise takes you to the heart of iterative reconstruction by asking you to implement two foundational algorithms: the Gauss-Seidel method and the Algebraic Reconstruction Technique (ART). By deriving these methods from first principles and applying them to a simplified Computed Tomography (CT) problem, you will directly compare how they handle ideal data versus noisy measurements [@problem_id:3135124]. This practice provides crucial insight into the trade-offs between convergence speed and stability, setting the stage for understanding the role of regularization.", "problem": "You are modeling iterative image reconstruction in Computed Tomography (CT)—an imaging modality that reconstructs cross-sectional images from X-ray projections. The problem is a linear inverse problem described by the linear system $A x = b$, where $A \\in \\mathbb{R}^{m \\times n}$ is the projection (system) matrix, $x \\in \\mathbb{R}^{n}$ is the unknown image vector (for example, pixel values), and $b \\in \\mathbb{R}^{m}$ are measured line integrals. In the presence of measurement noise, the system is typically inconsistent, and one seeks a least-squares solution that balances convergence and noise amplification. The task is to design and implement two iterative solvers from fundamental principles: Gauss-Seidel iteration applied to the normal equations and the Algebraic Reconstruction Technique (ART)—an iterative method that updates the image by sequentially projecting onto the hyperplanes defined by the linear equations.\n\nBegin from the fundamental base that the least-squares solution minimizes the squared error objective $$\\min_{x \\in \\mathbb{R}^{n}} \\ \\lVert A x - b \\rVert_2^2,$$ and that the stationary point satisfies the normal equations $$A^{\\mathsf{T}} A x = A^{\\mathsf{T}} b.$$ For ART, use the geometric fact that each linear equation $a_i^{\\mathsf{T}} x = b_i$ defines a hyperplane in $\\mathbb{R}^{n}$, and sequential orthogonal projections onto these hyperplanes drive the iterate toward feasibility. Derive the Gauss-Seidel iteration for the normal equations by splitting the coefficient matrix into lower and upper triangular parts and imposing that each component of the new iterate exactly satisfies the corresponding scalar equation using the latest available components. Derive the ART update rule by expressing an orthogonal projection onto one hyperplane using the inner product and the Euclidean norm. Do not use shortcut formulas not derived from these principles.\n\nImplement both methods to reconstruct $x$ from $A$ and $b$ for the following synthetic CT test suite. The system models a $2 \\times 2$ pixel image ($n = 4$) with $m = 6$ ray sums: two rows, two columns, and two diagonals. Pixels are ordered as $[x_1, x_2, x_3, x_4]^{\\mathsf{T}}$ corresponding to positions $[(1,1), (1,2), (2,1), (2,2)]$. The projection matrices and data for each test case are:\n\n- Test Case $1$ (happy path, consistent and well-conditioned):\n  $$A_1 = \\begin{bmatrix}\n  1 & 1 & 0 & 0 \\\\\n  0 & 0 & 1 & 1 \\\\\n  1 & 0 & 1 & 0 \\\\\n  0 & 1 & 0 & 1 \\\\\n  1 & 0 & 0 & 1 \\\\\n  0 & 1 & 1 & 0\n  \\end{bmatrix}, \\quad x_{\\text{true},1} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}, \\quad \\eta_1 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad b_1 = A_1 x_{\\text{true},1} + \\eta_1.$$\n  Use $N_{\\mathrm{GS},1} = 20$ Gauss-Seidel iterations on the normal equations and $N_{\\mathrm{ART},1} = 20$ full sweeps of ART across all rows with relaxation parameter $\\lambda_1 = 1.0$.\n\n- Test Case $2$ (moderate noise, same geometry):\n  $$A_2 = A_1, \\quad x_{\\text{true},2} = \\begin{bmatrix} 1 \\\\ 0.5 \\\\ 1.5 \\\\ 1 \\end{bmatrix}, \\quad \\eta_2 = \\begin{bmatrix} 0.02 \\\\ -0.015 \\\\ 0.01 \\\\ -0.005 \\\\ 0.0 \\\\ 0.025 \\end{bmatrix}, \\quad b_2 = A_2 x_{\\text{true},2} + \\eta_2.$$\n  Use $N_{\\mathrm{GS},2} = 40$ Gauss-Seidel iterations and $N_{\\mathrm{ART},2} = 40$ full sweeps with relaxation parameter $\\lambda_2 = 1.0$.\n\n- Test Case $3$ (near-redundant rays, higher noise, tests stability):\n  $$A_3 = \\begin{bmatrix}\n  1 & 1 & 0 & 0 \\\\\n  0 & 0 & 1 & 1 \\\\\n  1 & 0 & 1 & 0 \\\\\n  0 & 1 & 0 & 1 \\\\\n  0.99 & 0.99 & 0 & 0 \\\\\n  0 & 0 & 1.01 & 1.01\n  \\end{bmatrix}, \\quad x_{\\text{true},3} = \\begin{bmatrix} 1.2 \\\\ 0.8 \\\\ 1.0 \\\\ 1.5 \\end{bmatrix}, \\quad \\eta_3 = \\begin{bmatrix} 0.2 \\\\ -0.15 \\\\ 0.1 \\\\ -0.05 \\\\ 0.0 \\\\ 0.25 \\end{bmatrix}, \\quad b_3 = A_3 x_{\\text{true},3} + \\eta_3.$$\n  Use $N_{\\mathrm{GS},3} = 80$ Gauss-Seidel iterations and $N_{\\mathrm{ART},3} = 80$ full sweeps with relaxation parameter $\\lambda_3 = 1.2$.\n\nFor all cases, use the zero vector $x^{(0)} = \\mathbf{0}$ as the initial guess. Compute and report, for each test case, the following quantifiable metrics for both methods:\n- The final residual norm in the data space, defined as $$\\rho = \\lVert A x_{\\text{final}} - b \\rVert_2,$$ reported as a float.\n- The noise amplification factor, defined as $$\\alpha = \\frac{\\lVert x_{\\text{final}} - x_{\\text{true}} \\rVert_2}{\\max\\big(\\lVert \\eta \\rVert_2, \\varepsilon\\big)},$$ where $\\varepsilon = 10^{-12}$ is a small stabilizer. For noise-free data (i.e., $\\lVert \\eta \\rVert_2 = 0$), set $\\alpha = 0$ by definition. Report $\\alpha$ as a float.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be ordered as\n$$[\\rho_{\\mathrm{GS},1}, \\ \\rho_{\\mathrm{ART},1}, \\ \\alpha_{\\mathrm{GS},1}, \\ \\alpha_{\\mathrm{ART},1}, \\ \\rho_{\\mathrm{GS},2}, \\ \\rho_{\\mathrm{ART},2}, \\ \\alpha_{\\mathrm{GS},2}, \\ \\alpha_{\\mathrm{ART},2}, \\ \\rho_{\\mathrm{GS},3}, \\ \\rho_{\\mathrm{ART},3}, \\ \\alpha_{\\mathrm{GS},3}, \\ \\alpha_{\\mathrm{ART},3}],$$\nwith each float rounded to $6$ decimal places. No physical units are involved in this problem. Angles are not used. Percentages must not be used; only the specified floats should be printed.", "solution": "The problem requires the derivation and implementation of two iterative algorithms, Gauss-Seidel applied to the normal equations and the Algebraic Reconstruction Technique (ART), for solving a linear system $A x = b$ that models a simplified Computed Tomography (CT) reconstruction problem. The vector $x$ represents the unknown pixel values, $A$ is the system matrix mapping pixel values to line integrals, and $b$ is the vector of measured line integrals, potentially corrupted by noise $\\eta$.\n\nThe solution approach is to first derive the iterative update rules for both methods from fundamental principles as specified. Then, these methods will be implemented and applied to three distinct test cases to evaluate their performance based on the final residual norm $\\rho$ and a noise amplification factor $\\alpha$.\n\nFirst, we establish the mathematical foundation. For an inconsistent system $A x = b$, we seek a least-squares solution that minimizes the squared Euclidean norm of the residual, defined by the objective function:\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\ \\lVert A x - b \\rVert_2^2 $$\nThe minimum of this objective function is found at a stationary point where the gradient with respect to $x$ is zero. The objective function can be written as $f(x) = (Ax-b)^{\\mathsf{T}}(Ax-b) = x^{\\mathsf{T}}A^{\\mathsf{T}}Ax - 2b^{\\mathsf{T}}Ax + b^{\\mathsf{T}}b$. The gradient is $\\nabla_x f(x) = 2A^{\\mathsf{T}}Ax - 2A^{\\mathsf{T}}b$. Setting the gradient to zero yields the normal equations:\n$$ A^{\\mathsf{T}} A x = A^{\\mathsf{T}} b $$\nThis forms a symmetric positive semi-definite linear system, which we will denote as $Cx = d$, where $C = A^{\\mathsf{T}}A$ and $d = A^{\\mathsf{T}}b$. If $A$ has full column rank, $C$ is positive definite.\n\n**Derivation of the Gauss-Seidel Method for the Normal Equations**\n\nThe Gauss-Seidel method is an iterative technique for solving a square linear system $Cx=d$. It is derived by first decomposing the matrix $C$ into its diagonal ($D$), strictly lower triangular ($L$), and strictly upper triangular ($U$) parts, such that $C = L + D + U$. The system $Cx = d$ can be written as:\n$$ (L + D + U)x = d $$\nThe iterative method generates a sequence of vectors $x^{(k)}$ that ideally converges to the true solution $x$. The core idea of Gauss-Seidel is to rearrange the equation to solve for the next iterate $x^{(k+1)}$ using the most recently computed values:\n$$ (L + D) x^{(k+1)} = d - U x^{(k)} $$\nThis can be expressed component-wise. For the $j$-th equation in the system, we have:\n$$ \\sum_{i=1}^{n} C_{ji} x_i = d_j $$\nExpanding this sum gives:\n$$ \\sum_{i=1}^{j-1} C_{ji} x_i + C_{jj} x_j + \\sum_{i=j+1}^{n} C_{ji} x_i = d_j $$\nThe Gauss-Seidel method updates the $j$-th component of the solution vector, $x_j$, at iteration $k+1$ by using the already updated components $x_1^{(k+1)}, \\dots, x_{j-1}^{(k+1)}$ from the current iteration and the old components $x_{j+1}^{(k)}, \\dots, x_n^{(k)}$ from the previous iteration. We solve for $x_j^{(k+1)}$:\n$$ C_{jj} x_j^{(k+1)} = d_j - \\sum_{i=1}^{j-1} C_{ji} x_i^{(k+1)} - \\sum_{i=j+1}^{n} C_{ji} x_i^{(k)} $$\nThe final update rule for the $j$-th component at iteration $k+1$ is:\n$$ x_j^{(k+1)} = \\frac{1}{C_{jj}} \\left( d_j - \\sum_{i=1}^{j-1} C_{ji} x_i^{(k+1)} - \\sum_{i=j+1}^{n} C_{ji} x_i^{(k)} \\right) $$\nThis iteration is performed for $j = 1, \\dots, n$ to complete one full update from $x^{(k)}$ to $x^{(k+1)}$. The process starts with an initial guess $x^{(0)}$ (in this problem, the zero vector) and is repeated for a specified number of iterations. For this method to converge, the matrix $C$ must be symmetric positive-definite or strictly diagonally dominant. Since $C=A^{\\mathsf{T}}A$ and the provided matrices $A$ have full column rank for the first two cases and nearly so for the third, $C$ is symmetric positive-definite, guaranteeing convergence.\n\n**Derivation of the Algebraic Reconstruction Technique (ART)**\n\nThe Algebraic Reconstruction Technique (ART), a specific application of Kaczmarz's method, operates on the original system $Ax=b$ row by row. Each row of the system, $a_i^{\\mathsf{T}} x = b_i$ for $i=1, \\dots, m$, defines a hyperplane $H_i$ in $\\mathbb{R}^n$. The goal is to iteratively project the current solution estimate onto these hyperplanes.\n\nLet $x^{(k)}$ be the current estimate. We wish to find the next estimate, $x^{(k+1)}$, by projecting $x^{(k)}$ orthogonally onto the hyperplane $H_i$. The vector connecting $x^{(k)}$ to its projection on $H_i$ must be parallel to the hyperplane's normal vector, $a_i$. Thus, the update has the form:\n$$ x^{(k+1)} = x^{(k)} + c \\cdot a_i $$\nfor some scalar $c$. Since $x^{(k+1)}$ must lie on the hyperplane $H_i$, it must satisfy the equation $a_i^{\\mathsf{T}} x^{(k+1)} = b_i$. Substituting the update form into this equation gives:\n$$ a_i^{\\mathsf{T}} (x^{(k)} + c \\cdot a_i) = b_i $$\n$$ a_i^{\\mathsf{T}} x^{(k)} + c \\cdot (a_i^{\\mathsf{T}} a_i) = b_i $$\nThe term $a_i^{\\mathsf{T}} a_i$ is the squared Euclidean norm of the vector $a_i$, denoted $\\lVert a_i \\rVert_2^2$. Solving for $c$:\n$$ c \\cdot \\lVert a_i \\rVert_2^2 = b_i - a_i^{\\mathsf{T}} x^{(k)} $$\n$$ c = \\frac{b_i - a_i^{\\mathsf{T}} x^{(k)}}{\\lVert a_i \\rVert_2^2} $$\nThis assumes $\\lVert a_i \\rVert_2 \\neq 0$, which is true for the given system matrices. Substituting $c$ back into the update form yields the orthogonal projection update:\n$$ x^{(k+1)} = x^{(k)} + \\frac{b_i - a_i^{\\mathsf{T}} x^{(k)}}{\\lVert a_i \\rVert_2^2} a_i $$\nThe problem asks for a version with a relaxation parameter $\\lambda$. This parameter controls the step size of the projection, where $\\lambda=1$ corresponds to a full orthogonal projection. The generalized update rule is:\n$$ x^{(k+1)} = x^{(k)} + \\lambda \\frac{b_i - a_i^{\\mathsf{T}} x^{(k)}}{\\lVert a_i \\rVert_2^2} a_i $$\nOne full sweep of ART consists of applying this update sequentially for each row $i=1, \\dots, m$. That is, the result from projecting onto $H_i$ becomes the starting point for projecting onto $H_{i+1}$.\n\n**Implementation and Metrics**\n\nThe two derived methods are implemented and applied to the three test cases. For each method and case, the final solution vector $x_{\\text{final}}$ is used to calculate two metrics:\n1. The final residual norm: $\\rho = \\lVert A x_{\\text{final}} - b \\rVert_2$.\n2. The noise amplification factor: $\\alpha = \\frac{\\lVert x_{\\text{final}} - x_{\\text{true}} \\rVert_2}{\\max\\big(\\lVert \\eta \\rVert_2, \\varepsilon\\big)}$, with $\\varepsilon = 10^{-12}$. By definition given, if $\\lVert \\eta \\rVert_2=0$, then $\\alpha = 0$.\n\nThe implementation will follow the derived formulas, starting from an initial guess of $x^{(0)} = \\mathbf{0}$, for the specified number of iterations or sweeps. The final results are then collected and formatted as required.", "answer": "```python\nimport numpy as np\n\ndef run_gauss_seidel(A, b, n_iters):\n    \"\"\"\n    Solves the normal equations Ax=b using the Gauss-Seidel method.\n    The system being solved is (A^T A)x = (A^T b).\n    \"\"\"\n    m, n = A.shape\n    x_gs = np.zeros(n)\n    \n    C = A.T @ A\n    d = A.T @ b\n    \n    for _ in range(n_iters):\n        for j in range(n):\n            # Sum for already updated components in this iteration\n            sum_new = C[j, :j] @ x_gs[:j]\n            # Sum for components from the previous iteration\n            sum_old = C[j, j+1:] @ x_gs[j+1:]\n            \n            # Avoid division by zero, though C[j, j] should be positive\n            diag_element = C[j, j]\n            if abs(diag_element)  1e-12:\n                diag_element = 1e-12\n\n            x_gs[j] = (d[j] - sum_new - sum_old) / diag_element\n            \n    return x_gs\n\ndef run_art(A, b, n_sweeps, lam):\n    \"\"\"\n    Solves Ax=b using the Algebraic Reconstruction Technique (ART).\n    \"\"\"\n    m, n = A.shape\n    x_art = np.zeros(n)\n    \n    # Pre-calculate the squared L2 norm of each row of A\n    a_row_norm_sq = np.sum(A**2, axis=1)\n    \n    for _ in range(n_sweeps):\n        for i in range(m):\n            a_i = A[i, :]\n            b_i = b[i]\n            \n            norm_sq = a_row_norm_sq[i]\n            if norm_sq  1e-12:\n                # This row is all zeros, no information, skip update\n                continue\n            \n            # ART update rule\n            current_projection = a_i @ x_art\n            update_factor = lam * (b_i - current_projection) / norm_sq\n            x_art = x_art + update_factor * a_i\n            \n    return x_art\n\ndef calculate_alpha(x_final, x_true, eta):\n    \"\"\"\n    Calculates the noise amplification factor alpha.\n    \"\"\"\n    eps = 1e-12\n    norm_eta = np.linalg.norm(eta)\n    \n    if norm_eta == 0:\n        return 0.0\n    \n    error_norm = np.linalg.norm(x_final - x_true)\n    alpha = error_norm / max(norm_eta, eps)\n    return alpha\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Test Case 1\n    A1 = np.array([\n        [1, 1, 0, 0],\n        [0, 0, 1, 1],\n        [1, 0, 1, 0],\n        [0, 1, 0, 1],\n        [1, 0, 0, 1],\n        [0, 1, 1, 0]\n    ], dtype=float)\n    x_true1 = np.array([1, 2, 3, 4], dtype=float)\n    eta1 = np.zeros(6, dtype=float)\n    N_GS1, N_ART1, lambda1 = 20, 20, 1.0\n\n    # Test Case 2\n    A2 = A1\n    x_true2 = np.array([1, 0.5, 1.5, 1], dtype=float)\n    eta2 = np.array([0.02, -0.015, 0.01, -0.005, 0.0, 0.025], dtype=float)\n    N_GS2, N_ART2, lambda2 = 40, 40, 1.0\n\n    # Test Case 3\n    A3 = np.array([\n        [1.00, 1.00, 0.00, 0.00],\n        [0.00, 0.00, 1.00, 1.00],\n        [1.00, 0.00, 1.00, 0.00],\n        [0.00, 1.00, 0.00, 1.00],\n        [0.99, 0.99, 0.00, 0.00],\n        [0.00, 0.00, 1.01, 1.01]\n    ], dtype=float)\n    x_true3 = np.array([1.2, 0.8, 1.0, 1.5], dtype=float)\n    eta3 = np.array([0.2, -0.15, 0.1, -0.05, 0.0, 0.25], dtype=float)\n    N_GS3, N_ART3, lambda3 = 80, 80, 1.2\n    \n    test_cases = [\n        (A1, x_true1, eta1, N_GS1, N_ART1, lambda1),\n        (A2, x_true2, eta2, N_GS2, N_ART2, lambda2),\n        (A3, x_true3, eta3, N_GS3, N_ART3, lambda3),\n    ]\n\n    results = []\n    for A, x_true, eta, N_GS, N_ART, lam in test_cases:\n        b = A @ x_true + eta\n\n        # Run Gauss-Seidel for Normal Equations\n        x_final_gs = run_gauss_seidel(A, b, N_GS)\n        rho_gs = np.linalg.norm(A @ x_final_gs - b)\n        alpha_gs = calculate_alpha(x_final_gs, x_true, eta)\n\n        # Run ART\n        x_final_art = run_art(A, b, N_ART, lam)\n        rho_art = np.linalg.norm(A @ x_final_art - b)\n        alpha_art = calculate_alpha(x_final_art, x_true, eta)\n\n        results.extend([rho_gs, rho_art, alpha_gs, alpha_art])\n\n    # Format and print the final output\n    print(f\"[{','.join(f'{x:.6f}' for x in results)}]\")\n\nsolve()\n```", "id": "3135124"}, {"introduction": "After observing how noise can destabilize reconstruction, this practice introduces a powerful solution: Tikhonov regularization. You will work through a concrete example to solve a regularized least-squares problem, adding a penalty term that favors smoother images [@problem_id:4895893]. This exercise provides direct, hands-on experience with how regularization modifies the optimization problem to produce a more stable and physically plausible solution.", "problem": "Consider a simplified parallel-beam Computed Tomography (CT) forward model with two unknown, spatially uniform pixel attenuation coefficients $x \\in \\mathbb{R}^{2}$ representing a $2 \\times 1$ object with column pixels. Three rays are measured: ray $1$ traverses only pixel $1$ for a path length of $1\\,\\mathrm{cm}$, ray $2$ traverses only pixel $2$ for a path length of $1\\,\\mathrm{cm}$, and ray $3$ traverses both pixels, each for a path length of $1\\,\\mathrm{cm}$. After logarithmic conversion of detected photon counts to line integrals, the measured data vector is $b \\in \\mathbb{R}^{3}$ with components $b_{1} = 0.10\\,\\mathrm{cm}^{-1}$, $b_{2} = 0.20\\,\\mathrm{cm}^{-1}$, and $b_{3} = 0.25\\,\\mathrm{cm}^{-1}$. The system matrix $A \\in \\mathbb{R}^{3 \\times 2}$ corresponds to these path lengths and is given implicitly by the geometry described above. Assume independent and identically distributed Gaussian noise across rays so the weighting matrix for Weighted Least Squares (WLS) is $W = I_{3}$. To stabilize the reconstruction, apply quadratic Tikhonov regularization with a first-order finite-difference operator $L \\in \\mathbb{R}^{1 \\times 2}$ defined by $L = \\begin{bmatrix}1  -1\\end{bmatrix}$ and regularization parameter $\\lambda = 0.50$.\n\nStarting from the linear forward model $b = A x + n$ and the WLS Tikhonov objective\n$$\nJ(x) = \\frac{1}{2}\\,\\|W^{1/2}(A x - b)\\|_{2}^{2} + \\frac{\\lambda}{2}\\,\\|L x\\|_{2}^{2},\n$$\nderive the condition that the minimizer must satisfy, then compute the minimizer $x^{\\star}$ using the provided $A$, $b$, $W$, $L$, and $\\lambda$. Express the final reconstructed attenuation coefficients in reciprocal centimeters ($\\mathrm{cm}^{-1}$). Provide your final answer as a single row vector in the $\\begin{pmatrix}\\cdot  \\cdot\\end{pmatrix}$ format. No rounding is required; report the exact values.", "solution": "The objective is to find the vector $x \\in \\mathbb{R}^2$ that minimizes the Tikhonov-regularized weighted least squares objective function:\n$$\nJ(x) = \\frac{1}{2}\\,\\|W^{1/2}(A x - b)\\|_{2}^{2} + \\frac{\\lambda}{2}\\,\\|L x\\|_{2}^{2}\n$$\nThe squared $L_2$-norm can be expressed using the transpose: $\\|v\\|_2^2 = v^T v$.\n$$\nJ(x) = \\frac{1}{2}\\,(A x - b)^T W (A x - b) + \\frac{\\lambda}{2}\\,(L x)^T (L x)\n$$\nExpanding the terms, we get:\n$$\nJ(x) = \\frac{1}{2}\\,(x^T A^T W A x - x^T A^T W b - b^T W A x + b^T W b) + \\frac{\\lambda}{2}\\, x^T L^T L x\n$$\nSince $x^T A^T W b$ is a scalar, it equals its transpose, $b^T W^T A x$. As the weighting matrix $W$ is symmetric ($W^T = W$), this becomes $b^T W A x$. Combining terms gives:\n$$\nJ(x) = \\frac{1}{2}\\,x^T (A^T W A + \\lambda L^T L) x - x^T (A^T W b) + \\frac{1}{2}\\,b^T W b\n$$\nThis is a quadratic function of $x$. To find the minimum, we compute the gradient of $J(x)$ with respect to $x$ and set it to the zero vector.\n$$\n\\nabla_{x} J(x) = (A^T W A + \\lambda L^T L) x - A^T W b\n$$\nSetting the gradient to zero gives the condition for the minimizer $x^{\\star}$:\n$$\n(A^T W A + \\lambda L^T L) x^{\\star} = A^T W b\n$$\nThis system is known as the regularized normal equations. The solution is thus:\n$$\nx^{\\star} = (A^T W A + \\lambda L^T L)^{-1} A^T W b\n$$\nNow, we substitute the given values. From the problem description, we construct the system matrix $A$ for $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$:\n$$\nA = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 0.10 \\\\ 0.20 \\\\ 0.25 \\end{pmatrix} = \\begin{pmatrix} 1/10 \\\\ 1/5 \\\\ 1/4 \\end{pmatrix}\n$$\nThe weighting matrix is $W = I_3$, the regularization operator is $L = \\begin{bmatrix} 1  -1 \\end{bmatrix}$, and the regularization parameter is $\\lambda = 0.50 = 1/2$. The formula simplifies to $x^{\\star} = (A^T A + \\lambda L^T L)^{-1} A^T b$.\n\nWe compute the required matrices:\n$A^T A = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$\n$L^T L = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\begin{bmatrix} 1  -1 \\end{bmatrix} = \\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix}$\n\nThe matrix to be inverted is:\n$$\nA^T A + \\lambda L^T L = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix} = \\begin{pmatrix} 2.5  0.5 \\\\ 0.5  2.5 \\end{pmatrix} = \\begin{pmatrix} 5/2  1/2 \\\\ 1/2  5/2 \\end{pmatrix}\n$$\nThe determinant is $(5/2)(5/2) - (1/2)(1/2) = 25/4 - 1/4 = 24/4 = 6$. The inverse is:\n$$\n(A^T A + \\lambda L^T L)^{-1} = \\frac{1}{6} \\begin{pmatrix} 5/2  -1/2 \\\\ -1/2  5/2 \\end{pmatrix}\n$$\nNext, we compute the term $A^T b$:\n$$\nA^T b = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 1/10 \\\\ 1/5 \\\\ 1/4 \\end{pmatrix} = \\begin{pmatrix} 1/10 + 1/4 \\\\ 1/5 + 1/4 \\end{pmatrix} = \\begin{pmatrix} 7/20 \\\\ 9/20 \\end{pmatrix}\n$$\nFinally, we compute the solution $x^{\\star}$:\n$$\nx^{\\star} = \\frac{1}{6} \\begin{pmatrix} 5/2  -1/2 \\\\ -1/2  5/2 \\end{pmatrix} \\begin{pmatrix} 7/20 \\\\ 9/20 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} (5/2)(7/20) - (1/2)(9/20) \\\\ -(1/2)(7/20) + (5/2)(9/20) \\end{pmatrix}\n$$\n$$\nx^{\\star} = \\frac{1}{6} \\begin{pmatrix} \\frac{35 - 9}{40} \\\\ \\frac{-7 + 45}{40} \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} \\frac{26}{40} \\\\ \\frac{38}{40} \\end{pmatrix} = \\begin{pmatrix} \\frac{26}{240} \\\\ \\frac{38}{240} \\end{pmatrix} = \\begin{pmatrix} \\frac{13}{120} \\\\ \\frac{19}{120} \\end{pmatrix}\n$$\nThe reconstructed attenuation coefficients are $x_1^{\\star} = \\frac{13}{120}\\,\\mathrm{cm}^{-1}$ and $x_2^{\\star} = \\frac{19}{120}\\,\\mathrm{cm}^{-1}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{13}{120}  \\frac{19}{120} \\end{pmatrix}}\n$$", "id": "4895893"}, {"introduction": "Moving beyond simple smoothness, this advanced practice explores the state-of-the-art concept of sparsity-promoting reconstruction. You will learn how the $\\ell_1$-norm can be used to preserve sharp features while removing noise, a key idea from compressed sensing [@problem_id:4913497]. By deriving and applying modern proximal gradient algorithms like ISTA and FISTA, you will gain insight into the computational tools that enable high-quality imaging with reduced radiation dose.", "problem": "In a parallel-beam computed tomography (CT) system, the continuous Radon transform is discretized to give a linear forward model. Let the unknown image be represented by a vector $\\boldsymbol{x} \\in \\mathbb{R}^{n}$, and let the system matrix $\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$ encode line integrals along rays. Measured data are $\\boldsymbol{b} \\in \\mathbb{R}^{m}$. To incorporate the prior knowledge that the image is sparse in an orthonormal wavelet basis, consider an orthonormal wavelet transform $\\boldsymbol{W} \\in \\mathbb{R}^{n \\times n}$ (so that $\\boldsymbol{W}^{\\top}\\boldsymbol{W}=\\boldsymbol{I}$) and the convex optimization problem\n$$\n\\min_{\\boldsymbol{x}\\in\\mathbb{R}^{n}} \\ \\frac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b}\\|_{2}^{2} \\ + \\ \\lambda \\|\\boldsymbol{W}\\boldsymbol{x}\\|_{1},\n$$\nwhere $\\lambda0$ is a regularization parameter. \n\nStarting only from the following fundamental bases:\n(i) the linear forward model of tomography,\n(ii) the definition of the gradient of a smooth function, and\n(iii) the definition of the proximal operator of a closed, proper, convex function,\nperform the following:\n\n1) Explain why the term $\\lambda\\|\\boldsymbol{W}\\boldsymbol{x}\\|_{1}$ promotes sparsity of wavelet coefficients and how this relates to suppressing noise-like structures in CT reconstruction.\n\n2) Derive the Iterative Shrinkage-Thresholding Algorithm (ISTA) updates for the above problem by applying proximal gradient descent to the smooth data-fidelity term and the non-smooth wavelet $\\ell_{1}$ term. Assume that the step size is chosen as $t \\in (0, 2/L)$, where $L$ is a Lipschitz constant of the gradient of the data-fidelity term.\n\n3) Derive the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) acceleration for this problem, clearly specifying the momentum sequence and the extrapolated iterates.\n\n4) Evaluate the derived updates for the concrete $2$-dimensional test case with\n$$\n\\boldsymbol{A}=\\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix}, \\quad\n\\boldsymbol{W}=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  1 \\\\ 1  -1\\end{pmatrix}, \\quad\n\\lambda=\\frac{3}{5}, \\quad \\boldsymbol{x}^{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix},\n$$\nand\n$$\n\\boldsymbol{b}=\\boldsymbol{W}\\begin{pmatrix}1 \\\\ \\frac{1}{5}\\end{pmatrix}.\n$$\nUse the canonical choice $t=1/L$, where $L$ is the Lipschitz constant of the gradient of the data-fidelity term for this case. Compute the first ISTA iterate $\\boldsymbol{x}^{1}_{\\text{ISTA}}$ and the second FISTA iterate $\\boldsymbol{x}^{2}_{\\text{FISTA}}$ starting from $\\boldsymbol{x}^{0}$. Provide your final numerical result as a single row vector containing the four entries $\\big(x^{1}_{\\text{ISTA},1}, x^{1}_{\\text{ISTA},2}, x^{2}_{\\text{FISTA},1}, x^{2}_{\\text{FISTA},2}\\big)$. No rounding is required; report exact values.", "solution": "The optimization problem is a composite convex problem of the form $\\min_{\\boldsymbol{x}} F(\\boldsymbol{x}) = f(\\boldsymbol{x}) + g(\\boldsymbol{x})$, where $f(\\boldsymbol{x}) = \\frac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b}\\|_{2}^{2}$ is a smooth, convex data-fidelity term, and $g(\\boldsymbol{x}) = \\lambda \\|\\boldsymbol{W}\\boldsymbol{x}\\|_{1}$ is a non-smooth, convex regularization term.\n\n**1) Sparsity Promotion and Noise Suppression**\n\nThe term $\\lambda\\|\\boldsymbol{W}\\boldsymbol{x}\\|_{1}$ serves as a regularization penalty. The vector $\\boldsymbol{\\alpha} = \\boldsymbol{W}\\boldsymbol{x}$ represents the coefficients of the image $\\boldsymbol{x}$ in the orthonormal wavelet basis $\\boldsymbol{W}$. The $\\ell_1$-norm, $\\|\\boldsymbol{\\alpha}\\|_1 = \\sum_{i} |\\alpha_i|$, is being minimized, scaled by the regularization parameter $\\lambda  0$.\n\nThe $\\ell_1$-norm is a convex proxy for the non-convex $\\ell_0$ pseudo-norm, which counts the number of non-zero elements in a vector. Minimizing the $\\ell_1$-norm is known to promote sparsity, meaning it encourages solutions where many of the coefficients $\\alpha_i$ are exactly zero. Geometrically, the level sets of the $\\ell_1$-norm are hyper-diamonds (e.g., a square rotated by $45^\\circ$ in $\\mathbb{R}^2$), which have sharp \"corners\" on the coordinate axes. The optimal solution to the combined objective function is often found at these corners, where one or more coefficients are zero.\n\nIn the context of CT imaging, this is highly effective because:\ni. Natural images, while not sparse in the pixel domain, are often sparse or \"compressible\" in a wavelet basis. Their structure can be accurately represented by a few large-magnitude wavelet coefficients, while the rest are near zero.\nii. Measurement noise in CT, when transformed into the wavelet domain, tends to distribute its energy across many small-magnitude coefficients.\n\nBy minimizing the $\\ell_1$-norm of the wavelet coefficients, the optimization process preferentially shrinks small coefficients to zero while preserving the large coefficients. This effectively eliminates the contributions from noise (which manifest as small coefficients) while retaining the essential structural information of the image (captured by large coefficients).\n\n**2) Derivation of the Iterative Shrinkage-Thresholding Algorithm (ISTA)**\n\nISTA is an instance of the proximal gradient descent algorithm. The iterative update is:\n$$\n\\boldsymbol{x}^{k+1} = \\text{prox}_{t g}(\\boldsymbol{x}^k - t \\nabla f(\\boldsymbol{x}^k)),\n$$\nwhere $t  0$ is a step size.\n\nFirst, the gradient of the smooth term $f(\\boldsymbol{x}) = \\frac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b}\\|_{2}^{2}$ is:\n$$\n\\nabla f(\\boldsymbol{x}) = \\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b}).\n$$\nNext, we derive the proximal operator of $g(\\boldsymbol{x}) = \\lambda\\|\\boldsymbol{W}\\boldsymbol{x}\\|_1$. The proximal operator is defined as:\n$$\n\\text{prox}_{t g}(\\boldsymbol{y}) = \\arg\\min_{\\boldsymbol{z} \\in \\mathbb{R}^n} \\left( \\frac{1}{2}\\|\\boldsymbol{z}-\\boldsymbol{y}\\|_{2}^{2} + t\\lambda\\|\\boldsymbol{W}\\boldsymbol{z}\\|_1 \\right).\n$$\nSince $\\boldsymbol{W}$ is an orthonormal matrix, it preserves the Euclidean norm. Let $\\boldsymbol{\\alpha} = \\boldsymbol{W}\\boldsymbol{z}$ and $\\boldsymbol{\\beta} = \\boldsymbol{W}\\boldsymbol{y}$. Then $\\boldsymbol{z} = \\boldsymbol{W}^{\\top}\\boldsymbol{\\alpha}$, and $\\|\\boldsymbol{z}-\\boldsymbol{y}\\|_2^2 = \\|\\boldsymbol{W}^{\\top}(\\boldsymbol{\\alpha}-\\boldsymbol{\\beta})\\|_2^2 = \\|\\boldsymbol{\\alpha}-\\boldsymbol{\\beta}\\|_2^2$. The minimization problem can be rewritten in terms of $\\boldsymbol{\\alpha}$:\n$$\n\\arg\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^n} \\left( \\frac{1}{2}\\|\\boldsymbol{\\alpha}-\\boldsymbol{\\beta}\\|_2}^{2} + t\\lambda\\|\\boldsymbol{\\alpha}\\|_1 \\right).\n$$\nThis is the definition of the proximal operator of the $\\ell_1$-norm, which is the soft-thresholding operator $S_{t\\lambda}(\\cdot)$, applied element-wise: $(S_{\\tau}(u))_i = \\text{sign}(u_i)\\max(|u_i|-\\tau, 0)$. The solution is $\\boldsymbol{\\alpha}^{*} = S_{t\\lambda}(\\boldsymbol{\\beta})$.\n\nTransforming back to the original variable $\\boldsymbol{z}$, we get $\\boldsymbol{z}^{*} = \\boldsymbol{W}^{\\top}\\boldsymbol{\\alpha}^{*} = \\boldsymbol{W}^{\\top}S_{t\\lambda}(\\boldsymbol{\\beta}) = \\boldsymbol{W}^{\\top}S_{t\\lambda}(\\boldsymbol{W}\\boldsymbol{y})$.\nCombining these parts, the ISTA update is a two-step process:\n1. Gradient step: $\\boldsymbol{y}^k = \\boldsymbol{x}^k - t \\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{x}^k-\\boldsymbol{b})$\n2. Proximal step: $\\boldsymbol{x}^{k+1} = \\boldsymbol{W}^{\\top}S_{t\\lambda}(\\boldsymbol{W}\\boldsymbol{y}^k)$\n\n**3) Derivation of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)**\n\nFISTA accelerates ISTA by incorporating a momentum term. It maintains an auxiliary sequence of extrapolated points $\\boldsymbol{z}^k$.\nInitialize: Choose $\\boldsymbol{x}^0 \\in \\mathbb{R}^n$, set $\\boldsymbol{z}^0 = \\boldsymbol{x}^0$, and $\\theta_0=1$.\nFor $k=0, 1, 2, \\dots$:\n1. Perform the gradient and proximal steps at the extrapolated point $\\boldsymbol{z}^k$:\n   a. Gradient step: $\\boldsymbol{y}^k = \\boldsymbol{z}^k - t_k \\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{z}^k - \\boldsymbol{b})$\n   b. Proximal step: $\\boldsymbol{x}^{k+1} = \\boldsymbol{W}^{\\top}S_{t_k\\lambda}(\\boldsymbol{W}\\boldsymbol{y}^k)$\n2. Update the momentum parameter $\\theta_k$:\n   $$\n   \\theta_{k+1} = \\frac{1 + \\sqrt{1 + 4\\theta_k^2}}{2}\n   $$\n3. Compute the next extrapolated point $\\boldsymbol{z}^{k+1}$ via a linear combination of the current and previous iterates:\n   $$\n   \\boldsymbol{z}^{k+1} = \\boldsymbol{x}^{k+1} + \\frac{\\theta_k - 1}{\\theta_{k+1}}(\\boldsymbol{x}^{k+1} - \\boldsymbol{x}^k)\n   $$\nThe step size $t_k$ is typically constant, $t_k=t \\in (0, 2/L)$, where $L$ is the Lipschitz constant of $\\nabla f$.\n\n**4) Evaluation for the Concrete Test Case**\n\nGiven: $\\boldsymbol{A}=\\boldsymbol{I}$, $\\boldsymbol{W}=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  1 \\\\ 1  -1\\end{pmatrix}$, $\\lambda=\\frac{3}{5}$, $\\boldsymbol{x}^{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$, and $\\boldsymbol{b}=\\boldsymbol{W}\\begin{pmatrix}1 \\\\ \\frac{1}{5}\\end{pmatrix}$. Note that $\\boldsymbol{W}$ is symmetric and orthonormal ($\\boldsymbol{W}^{\\top}=\\boldsymbol{W}$ and $\\boldsymbol{W}^2=\\boldsymbol{I}$).\n$\\boldsymbol{b} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  1 \\\\ 1  -1\\end{pmatrix}\\begin{pmatrix}1 \\\\ 1/5\\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}6/5 \\\\ 4/5\\end{pmatrix}$.\nThe Lipschitz constant of $\\nabla f(\\boldsymbol{x})=\\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b})$ is $L = \\|\\boldsymbol{A}^{\\top}\\boldsymbol{A}\\|_2 = \\|\\boldsymbol{I}\\|_2=1$.\nThe step size is $t=1/L = 1$. The soft-thresholding parameter is $\\tau = t\\lambda = 3/5$.\n\n**Computation of $\\boldsymbol{x}^{1}_{\\text{ISTA}}$:**\nStarting with $\\boldsymbol{x}^0 = \\mathbf{0}$.\n1. Gradient step: $\\boldsymbol{y}^0 = \\boldsymbol{x}^0 - t (\\boldsymbol{A}\\boldsymbol{x}^0 - \\boldsymbol{b}) = \\mathbf{0} - 1 (\\mathbf{0} - \\boldsymbol{b}) = \\boldsymbol{b}$.\n2. Proximal step: $\\boldsymbol{x}^1_{\\text{ISTA}} = \\boldsymbol{W}^{\\top}S_{\\tau}(\\boldsymbol{W}\\boldsymbol{y}^0) = \\boldsymbol{W}S_{3/5}(\\boldsymbol{W}\\boldsymbol{b})$.\n   The argument of the thresholding function is $\\boldsymbol{W}\\boldsymbol{b} = \\boldsymbol{W}\\left(\\boldsymbol{W}\\begin{pmatrix}1 \\\\ 1/5\\end{pmatrix}\\right) = \\boldsymbol{W}^2 \\begin{pmatrix}1 \\\\ 1/5\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 1/5\\end{pmatrix}$.\n   Applying soft-thresholding: $S_{3/5}\\begin{pmatrix}1 \\\\ 1/5\\end{pmatrix} = \\begin{pmatrix} \\max(1-3/5, 0) \\\\ \\max(1/5-3/5, 0) \\end{pmatrix} = \\begin{pmatrix}2/5 \\\\ 0\\end{pmatrix}$.\n   Transforming back: $\\boldsymbol{x}^1_{\\text{ISTA}} = \\boldsymbol{W}\\begin{pmatrix}2/5 \\\\ 0\\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  1 \\\\ 1  -1\\end{pmatrix}\\begin{pmatrix}2/5 \\\\ 0\\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}2/5 \\\\ 2/5\\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{2}}{5} \\\\ \\frac{\\sqrt{2}}{5} \\end{pmatrix}$.\n\n**Computation of $\\boldsymbol{x}^{2}_{\\text{FISTA}}$:**\nInitialize: $\\boldsymbol{x}^0 = \\mathbf{0}$, $\\boldsymbol{z}^0 = \\boldsymbol{x}^0$, $\\theta_0=1$.\n*Iteration 1 (k=0):*\n1. Gradient/Proximal step at $\\boldsymbol{z}^0$: $\\boldsymbol{y}^0 = \\boldsymbol{z}^0 - t(\\boldsymbol{A}\\boldsymbol{z}^0-\\boldsymbol{b}) = \\boldsymbol{b}$.\n   $\\boldsymbol{x}^1 = \\boldsymbol{W}^{\\top}S_{\\tau}(\\boldsymbol{W}\\boldsymbol{y}^0) = \\boldsymbol{x}^1_{\\text{ISTA}} = \\begin{pmatrix} \\frac{\\sqrt{2}}{5} \\\\ \\frac{\\sqrt{2}}{5} \\end{pmatrix}$.\n2. Update momentum: $\\theta_1 = \\frac{1+\\sqrt{1+4\\theta_0^2}}{2} = \\frac{1+\\sqrt{5}}{2}$.\n3. Update extrapolation point: $\\boldsymbol{z}^1 = \\boldsymbol{x}^1 + \\frac{\\theta_0-1}{\\theta_1}(\\boldsymbol{x}^1-\\boldsymbol{x}^0) = \\boldsymbol{x}^1$.\n\n*Iteration 2 (k=1):*\n1. Gradient/Proximal step at $\\boldsymbol{z}^1$: $\\boldsymbol{y}^1 = \\boldsymbol{z}^1 - t(\\boldsymbol{A}\\boldsymbol{z}^1-\\boldsymbol{b}) = \\boldsymbol{x}^1 - (\\boldsymbol{x}^1-\\boldsymbol{b}) = \\boldsymbol{b}$.\n   The argument for the proximal operator is again $\\boldsymbol{b}$.\n   $\\boldsymbol{x}^2_{\\text{FISTA}} = \\boldsymbol{W}^{\\top}S_{\\tau}(\\boldsymbol{W}\\boldsymbol{y}^1) = \\boldsymbol{W}S_{3/5}(\\boldsymbol{W}\\boldsymbol{b}) = \\boldsymbol{x}^1_{\\text{ISTA}} = \\begin{pmatrix} \\frac{\\sqrt{2}}{5} \\\\ \\frac{\\sqrt{2}}{5} \\end{pmatrix}$.\n\nThe algorithm converges in a single step for this special case.\nThe final result is the row vector $\\big(x^{1}_{\\text{ISTA},1}, x^{1}_{\\text{ISTA},2}, x^{2}_{\\text{FISTA},1}, x^{2}_{\\text{FISTA},2}\\big)$.\n$x^{1}_{\\text{ISTA},1} = \\frac{\\sqrt{2}}{5}$, $x^{1}_{\\text{ISTA},2} = \\frac{\\sqrt{2}}{5}$.\n$x^{2}_{\\text{FISTA},1} = \\frac{\\sqrt{2}}{5}$, $x^{2}_{\\text{FISTA},2} = \\frac{\\sqrt{2}}{5}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sqrt{2}}{5}  \\frac{\\sqrt{2}}{5}  \\frac{\\sqrt{2}}{5}  \\frac{\\sqrt{2}}{5}\n\\end{pmatrix}\n}\n$$", "id": "4913497"}]}