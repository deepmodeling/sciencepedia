{"hands_on_practices": [{"introduction": "Before we can reconstruct a SPECT image, we must first build a mathematical model of the imaging process itself. This is known as the forward problem. The exercise [@problem_id:4927199] below introduces the fundamental linear model, $\\bar{y} = Ax + r$, which describes how the underlying tracer activity ($x$) is transformed by the imaging system ($A$) and corrupted by background events ($r$) to produce the mean projection data ($\\bar{y}$) that our scanner measures. Working through this calculation will help you understand how each physical component contributes to the final raw data we use for reconstruction.", "problem": "Consider Single Photon Emission Computed Tomography (SPECT), where the measured projection data are modeled as random counts whose expectation is determined by the system response to the underlying activity distribution. In a discrete two-voxel, two-bin setting, let the system matrix $A$ describe the contribution of each voxel to each projection bin, the activity vector $x$ encode the voxel activities, and the additive term $r$ represent mean background counts (including scatter and electronic noise). Adopt the standard forward expectation model for SPECT that combines linear system response and additive background at the level of expected counts. With \n$A=\\begin{bmatrix}0.5  0.2 \\\\ 0.3  0.4\\end{bmatrix}$, \n$x=\\begin{bmatrix}10 \\\\ 4\\end{bmatrix}$ (becquerel, Bq), and \n$r=\\begin{bmatrix}1 \\\\ 2\\end{bmatrix}$ (counts), compute the mean projection vector $\\bar{y}$ in counts. Then briefly interpret the contribution of each term to each entry of $\\bar{y}$ in terms of voxel contributions and background. Express your final numeric answer in counts and do not round.", "solution": "The user wants me to solve the problem by following the instructions.\n1. ### **Problem Validation**\nThe problem must be validated before a solution is attempted.\n\n#### **Step 1: Extract Givens**\n- **Topic**: Single Photon Emission Computed Tomography (SPECT) image reconstruction.\n- **Model**: A discrete two-voxel, two-bin setting. The forward expectation model for mean projection counts $\\bar{y}$ is given as a combination of linear system response and additive background.\n- **System Matrix**: $A = \\begin{bmatrix} 0.5  0.2 \\\\ 0.3  0.4 \\end{bmatrix}$. The elements $A_{ij}$ represent the contribution of voxel $j$ to projection bin $i$.\n- **Activity Vector**: $x = \\begin{bmatrix} 10 \\\\ 4 \\end{bmatrix}$ (in units of becquerel, Bq).\n- **Mean Background Vector**: $r = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$ (in units of counts).\n- **Task**: Compute the mean projection vector $\\bar{y}$ (in counts) and interpret the contribution of each term.\n\n#### **Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is scientifically grounded. The model for the mean projection data, $\\bar{y} = Ax + r$, is a standard and fundamental linear approximation used in the field of medical imaging, particularly for SPECT and PET reconstruction. It correctly represents the physical process where photons from an activity distribution ($x$) are detected by a system (with response $A$) and are corrupted by additive background signals like scatter and random coincidences ($r$).\n- **Well-Posedness**: The problem is well-posed. It provides all necessary numerical data ($A$, $x$, $r$) and a clear definition of the model to be used. The calculation is a straightforward matrix-vector multiplication and addition, which yields a unique and meaningful solution.\n- **Objectivity**: The problem is stated objectively, using standard terminology from the field of medical imaging. All quantities are defined precisely.\n- **Completeness and Consistency**: The problem is self-contained. While there is a slight ambiguity in units (Bq for activity vs. counts for projections and background), it is standard in such simplified problems to assume that the system matrix $A$ incorporates any necessary conversion factors (e.g., acquisition time, detector efficiency, geometric factors) to ensure dimensional consistency. The model is therefore applied as given, with the result being in counts.\n- **Other Flaws**: The problem does not violate any other criteria. It is not unrealistic (for a didactic example), ill-posed, trivial, or unverifiable.\n\n#### **Step 3: Verdict and Action**\nThe problem is **valid**. A full solution will be provided.\n\n2. ### **Solution**\nThe problem describes the standard linear forward model for the expectation of SPECT projection data. The mean projection vector, $\\bar{y}$, is the sum of the expected counts originating from the tracer distribution within the patient and the expected counts from background sources. This relationship is expressed as:\n$$ \\bar{y} = Ax + r $$\nwhere $A$ is the system matrix, $x$ is the activity vector, and $r$ is the mean background vector.\n\nWe are given the following values:\nThe system matrix $A$:\n$$ A = \\begin{bmatrix} 0.5  0.2 \\\\ 0.3  0.4 \\end{bmatrix} $$\nThe activity vector $x$:\n$$ x = \\begin{bmatrix} 10 \\\\ 4 \\end{bmatrix} $$\nThe mean background vector $r$:\n$$ r = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} $$\n\nFirst, we compute the contribution from the activity distribution, which is the product $Ax$:\n$$ Ax = \\begin{bmatrix} 0.5  0.2 \\\\ 0.3  0.4 \\end{bmatrix} \\begin{bmatrix} 10 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} (0.5)(10) + (0.2)(4) \\\\ (0.3)(10) + (0.4)(4) \\end{bmatrix} $$\n$$ Ax = \\begin{bmatrix} 5 + 0.8 \\\\ 3 + 1.6 \\end{bmatrix} = \\begin{bmatrix} 5.8 \\\\ 4.6 \\end{bmatrix} $$\nThis vector represents the mean counts detected in each bin originating from the two voxels, before considering background.\n\nNext, we add the mean background counts $r$ to find the total mean projection vector $\\bar{y}$:\n$$ \\bar{y} = Ax + r = \\begin{bmatrix} 5.8 \\\\ 4.6 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 5.8 + 1 \\\\ 4.6 + 2 \\end{bmatrix} = \\begin{bmatrix} 6.8 \\\\ 6.6 \\end{bmatrix} $$\nThus, the mean projection vector is $\\bar{y} = \\begin{bmatrix} 6.8 \\\\ 6.6 \\end{bmatrix}$ counts.\n\nThe interpretation of the contributions to each entry of $\\bar{y}$ is as follows:\nLet the components of the mean projection vector be $\\bar{y} = \\begin{bmatrix} \\bar{y}_1 \\\\ \\bar{y}_2 \\end{bmatrix}$.\n\nFor the first projection bin, $\\bar{y}_1 = 6.8$ counts:\nThe total mean count is a sum of three terms:\n1.  Contribution from the first voxel (activity $x_1 = 10$) to the first bin: $A_{11}x_1 = 0.5 \\times 10 = 5.0$ counts.\n2.  Contribution from the second voxel (activity $x_2 = 4$) to the first bin: $A_{12}x_2 = 0.2 \\times 4 = 0.8$ counts.\n3.  Contribution from background to the first bin: $r_1 = 1$ count.\nThe sum is $\\bar{y}_1 = 5.0 + 0.8 + 1 = 6.8$ counts.\n\nFor the second projection bin, $\\bar{y}_2 = 6.6$ counts:\nThe total mean count is a sum of three terms:\n1.  Contribution from the first voxel (activity $x_1 = 10$) to the second bin: $A_{21}x_1 = 0.3 \\times 10 = 3.0$ counts.\n2.  Contribution from the second voxel (activity $x_2 = 4$) to the second bin: $A_{22}x_2 = 0.4 \\times 4 = 1.6$ counts.\n3.  Contribution from background to the second bin: $r_2 = 2$ counts.\nThe sum is $\\bar{y}_2 = 3.0 + 1.6 + 2 = 6.6$ counts.\nThe system matrix $A$ shows that the first voxel contributes more strongly to the first bin, while the second voxel contributes more strongly to the second bin, which is typical for a detector geometry where each bin is positioned closer to its corresponding voxel.", "answer": "$$ \\boxed{ \\begin{bmatrix} 6.8 \\\\ 6.6 \\end{bmatrix} } $$", "id": "4927199"}, {"introduction": "With the forward model established, we can now tackle the main challenge: the inverse problem of image reconstruction. This means starting with the measured data ($y$) and figuring out the original tracer distribution ($x$). The Maximum Likelihood Expectation-Maximization (ML-EM) algorithm is a cornerstone iterative method for this task, designed specifically for the Poisson statistics of photon counting. This practice [@problem_id:4927224] demystifies the algorithm by guiding you through the essential calculations of a single update step, showing how an initial guess of the image is refined to better explain the measurements.", "problem": "In Single Photon Emission Computed Tomography (SPECT), a standard binned projection model assumes that each detector bin count $y_i$ is a realization of a Poisson random variable with mean $\\lambda_i = \\sum_{j} A_{ij} x_j + r_i$, where $A_{ij}$ is a known system matrix encoding the probability that an emission from voxel $j$ is detected in bin $i$, $x_j \\ge 0$ is the unknown activity in voxel $j$, and $r_i \\ge 0$ is the known mean background count in bin $i$. Under this model, the maximum likelihood estimate can be sought with the Expectation-Maximization framework, yielding a multiplicative fixed-point iteration that preserves nonnegativity.\n\nStarting from the Poisson data model, the definition of the log-likelihood for independent Poisson measurements, and the Expectation-Maximization construction with appropriate latent variables, derive the fixed-point multiplicative update appropriate for Maximum-Likelihood Expectation-Maximization (ML-EM) under known $A$ and $r$. Then, using your derived update, perform exactly one iteration starting from the given initialization to compute $x^{1}$ for the following numerically specified SPECT system:\n$$\nA=\\begin{bmatrix}0.4  0.2\\\\ 0.1  0.5\\end{bmatrix},\\quad\ny=\\begin{bmatrix}20\\\\ 10\\end{bmatrix},\\quad\nr=\\begin{bmatrix}0\\\\ 0\\end{bmatrix},\\quad\nx^{0}=\\begin{bmatrix}5\\\\ 5\\end{bmatrix}.\n$$\nReport the two components of $x^{1}$ as exact values in counts (no rounding). Express the final numerical result for $x^{1}$ in counts.", "solution": "The problem is valid. It is a standard question in medical image reconstruction that is scientifically grounded, well-posed, and contains all necessary information for a unique solution.\n\nThe problem is divided into two parts: first, to derive the Maximum-Likelihood Expectation-Maximization (ML-EM) update rule for SPECT reconstruction, and second, to apply this rule for one iteration with given numerical data.\n\n### Part 1: Derivation of the ML-EM Update Rule\n\nThe problem states the statistical model for the measured projection data. For each detector bin $i$, the count $y_i$ is a realization of an independent Poisson random variable with mean $\\lambda_i$. The mean is given by the linear model:\n$$\n\\lambda_i = \\sum_{j=1}^{N} A_{ij} x_j + r_i\n$$\nwhere $x_j$ is the unknown activity in voxel $j$ ($j=1, \\dots, N$), $A_{ij}$ is the system matrix element representing the probability that an emission from voxel $j$ is detected in bin $i$, and $r_i$ is the known mean of background events (e.g., scatter) in bin $i$. The goal is to find the activity vector $x = (x_1, \\dots, x_N)^T$ that maximizes the likelihood of observing the measurement vector $y = (y_1, \\dots, y_M)^T$.\n\nThe log-likelihood function $L(x)$ for independent Poisson measurements is:\n$$\nL(x) = \\ln P(y|x) = \\sum_{i=1}^{M} \\ln \\left( \\frac{\\exp(-\\lambda_i) \\lambda_i^{y_i}}{y_i!} \\right) = \\sum_{i=1}^{M} \\left( -\\lambda_i + y_i \\ln(\\lambda_i) - \\ln(y_i!) \\right)\n$$\nSubstituting the expression for $\\lambda_i$ and dropping the term $\\ln(y_i!)$ which is constant with respect to $x$:\n$$\nL(x) \\propto \\sum_{i=1}^{M} \\left( - \\left(\\sum_{j=1}^{N} A_{ij} x_j + r_i\\right) + y_i \\ln\\left(\\sum_{j=1}^{N} A_{ij} x_j + r_i\\right) \\right)\n$$\nDirect maximization of this function is difficult due to the logarithm of a sum. The Expectation-Maximization (EM) algorithm circumvents this by introducing latent variables, which constitute the \"complete data\".\n\nLet the complete data be the set of variables $\\{z_{ij}\\}$, where $z_{ij}$ is the number of photons originating from voxel $j$ and detected in bin $i$. For the background, we can think of it as originating from a separate source, contributing $w_i$ counts to bin $i$.\nThe observed, or \"incomplete\", data $y_i$ is the sum of these contributions:\n$$\ny_i = \\sum_{j=1}^{N} z_{ij} + w_i\n$$\nAccording to the Poisson model, $z_{ij}$ and $w_i$ are independent Poisson random variables with means $E[z_{ij}] = A_{ij} x_j$ and $E[w_i] = r_i$. The sum of independent Poisson variables is also Poisson, so $y_i$ is Poisson with mean $\\sum_j E[z_{ij}] + E[w_i] = \\sum_j A_{ij} x_j + r_i$, which is consistent with the problem statement.\n\nThe log-likelihood of the complete data $\\{z_{ij}, w_i\\}$, given $x$, is:\n$$\nL_c(x) = \\ln P(\\{z_{ij}, w_i\\}|x) = \\sum_{i=1}^{M} \\sum_{j=1}^{N} \\ln P(z_{ij}|x) + \\sum_{i=1}^{M} \\ln P(w_i)\n$$\nAs the term for $w_i$ does not depend on $x$, it can be ignored during maximization.\n$$\nL_c(x) \\propto \\sum_{i=1}^{M} \\sum_{j=1}^{N} \\left( -A_{ij}x_j + z_{ij}\\ln(A_{ij}x_j) - \\ln(z_{ij}!) \\right)\n$$\nThe EM algorithm proceeds in two steps:\n\n**E-Step (Expectation):**\nWe compute the expectation of the complete-data log-likelihood, conditioned on the observed data $y$ and the current estimate of the parameters $x^{(k)}$. This is the function $Q(x | x^{(k)})$:\n$$\nQ(x | x^{(k)}) = E[L_c(x) | y, x^{(k)}] = \\sum_{i=1}^{M} \\sum_{j=1}^{N} \\left( -A_{ij}x_j + E[z_{ij}|y, x^{(k)}]\\ln(A_{ij}x_j) \\right) + \\text{const.}\n$$\nThe key is to compute the conditional expectation $E[z_{ij}|y, x^{(k)}]$. Given that the sum of independent Poisson variables $\\{z_{i1}, \\dots, z_{iN}, w_i\\}$ is $y_i$, their joint distribution conditioned on the sum is multinomial. The expectation of any one component, say $z_{ij}$, is the total count $y_i$ multiplied by the ratio of its mean to the sum of all means:\n$$\nE[z_{ij}|y_i, x^{(k)}] = y_i \\frac{E[z_{ij}|x^{(k)}]}{\\sum_{l=1}^{N}E[z_{il}|x^{(k)}] + E[w_i]} = y_i \\frac{A_{ij} x_j^{(k)}}{\\sum_{l=1}^{N} A_{il} x_l^{(k)} + r_i}\n$$\nLet's denote $\\hat{z}_{ij}^{(k)} = E[z_{ij}|y, x^{(k)}]$.\n\n**M-Step (Maximization):**\nWe find the next estimate $x^{(k+1)}$ by maximizing $Q(x | x^{(k)})$ with respect to $x$.\n$$\nx^{(k+1)} = \\arg\\max_{x \\ge 0} Q(x | x^{(k)})\n$$\nWe take the partial derivative of $Q(x|x^{(k)})$ with respect to each $x_j$ and set it to zero.\n$$\n\\frac{\\partial Q(x|x^{(k)})}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\sum_{i=1}^{M} \\sum_{l=1}^{N} \\left( -A_{il}x_l + \\hat{z}_{il}^{(k)}\\ln(A_{il}x_l) \\right)\n$$\nThe derivative is non-zero only for terms where $l=j$:\n$$\n\\frac{\\partial Q}{\\partial x_j} = \\sum_{i=1}^{M} \\left( -A_{ij} + \\frac{\\hat{z}_{ij}^{(k)}}{x_j} \\right)\n$$\nSetting this to $0$:\n$$\n\\sum_{i=1}^{M} \\frac{\\hat{z}_{ij}^{(k)}}{x_j} = \\sum_{i=1}^{M} A_{ij} \\implies x_j = \\frac{\\sum_{i=1}^{M} \\hat{z}_{ij}^{(k)}}{\\sum_{i=1}^{M} A_{ij}}\n$$\nThis gives the update rule for $x_j^{(k+1)}$:\n$$\nx_j^{(k+1)} = \\frac{1}{\\sum_{i=1}^{M} A_{ij}} \\sum_{i=1}^{M} \\left( y_i \\frac{A_{ij} x_j^{(k)}}{\\sum_{l=1}^{N} A_{il} x_l^{(k)} + r_i} \\right)\n$$\nThis can be rearranged into the standard multiplicative form:\n$$\nx_j^{(k+1)} = x_j^{(k)} \\frac{\\sum_{i=1}^{M} A_{ij} \\frac{y_i}{\\sum_{l=1}^{N} A_{il} x_l^{(k)} + r_i}}{\\sum_{i=1}^{M} A_{ij}}\n$$\nThis is the fixed-point multiplicative update rule for ML-EM. The term $\\sum_{i=1}^{M} A_{ij}$ is often denoted as the sensitivity $s_j$ of voxel $j$.\n\n### Part 2: Numerical Calculation for One Iteration\n\nWe are given the following numerical values:\n$$\nA=\\begin{bmatrix}0.4  0.2\\\\ 0.1  0.5\\end{bmatrix},\\quad\ny=\\begin{bmatrix}20\\\\ 10\\end{bmatrix},\\quad\nr=\\begin{bmatrix}0\\\\ 0\\end{bmatrix},\\quad\nx^{(0)}=\\begin{bmatrix}5\\\\ 5\\end{bmatrix}\n$$\nWe want to compute $x^{(1)} = \\begin{bmatrix} x_1^{(1)} \\\\ x_2^{(1)} \\end{bmatrix}$.\n\nThe number of voxels is $N=2$ and the number of detector bins is $M=2$. The update rule for $j=1, 2$ is:\n$$\nx_j^{(1)} = x_j^{(0)} \\frac{\\sum_{i=1}^{2} A_{ij} \\frac{y_i}{\\sum_{l=1}^{2} A_{il} x_l^{(0)} + r_i}}{\\sum_{i=1}^{2} A_{ij}}\n$$\nWith $r=0$, this simplifies to:\n$$\nx_j^{(1)} = x_j^{(0)} \\frac{\\sum_{i=1}^{2} A_{ij} \\frac{y_i}{(Ax^{(0)})_i}}{\\sum_{i=1}^{2} A_{ij}}\n$$\n\n**1. Calculate the forward projection $\\lambda^{(0)} = Ax^{(0)}$:**\n$$\n\\lambda^{(0)} = \\begin{bmatrix}0.4  0.2\\\\ 0.1  0.5\\end{bmatrix} \\begin{bmatrix}5\\\\ 5\\end{bmatrix} = \\begin{bmatrix}0.4(5) + 0.2(5) \\\\ 0.1(5) + 0.5(5)\\end{bmatrix} = \\begin{bmatrix}2 + 1 \\\\ 0.5 + 2.5\\end{bmatrix} = \\begin{bmatrix}3\\\\ 3\\end{bmatrix}\n$$\n\n**2. Calculate the correction ratios $y_i / \\lambda_i^{(0)}$:**\n$$\n\\frac{y_1}{\\lambda_1^{(0)}} = \\frac{20}{3}\n$$\n$$\n\\frac{y_2}{\\lambda_2^{(0)}} = \\frac{10}{3}\n$$\n\n**3. Calculate the back-projection term in the numerator of the update factor:**\nThis is the term $b_j = \\sum_{i=1}^{2} A_{ij} \\frac{y_i}{\\lambda_i^{(0)}}$. In matrix form, this is $A^T (y./\\lambda^{(0)})$.\nFor $j=1$:\n$$\nb_1 = A_{11}\\frac{y_1}{\\lambda_1^{(0)}} + A_{21}\\frac{y_2}{\\lambda_2^{(0)}} = (0.4)\\left(\\frac{20}{3}\\right) + (0.1)\\left(\\frac{10}{3}\\right) = \\frac{8}{3} + \\frac{1}{3} = \\frac{9}{3} = 3\n$$\nFor $j=2$:\n$$\nb_2 = A_{12}\\frac{y_1}{\\lambda_1^{(0)}} + A_{22}\\frac{y_2}{\\lambda_2^{(0)}} = (0.2)\\left(\\frac{20}{3}\\right) + (0.5)\\left(\\frac{10}{3}\\right) = \\frac{4}{3} + \\frac{5}{3} = \\frac{9}{3} = 3\n$$\n\n**4. Calculate the sensitivities $s_j = \\sum_{i=1}^{2} A_{ij}$:**\nFor $j=1$:\n$$\ns_1 = A_{11} + A_{21} = 0.4 + 0.1 = 0.5\n$$\nFor $j=2$:\n$$\ns_2 = A_{12} + A_{22} = 0.2 + 0.5 = 0.7\n$$\n\n**5. Apply the multiplicative update to find $x^{(1)}$:**\n$$\nx_j^{(1)} = x_j^{(0)} \\frac{b_j}{s_j}\n$$\nFor $j=1$:\n$$\nx_1^{(1)} = x_1^{(0)} \\frac{b_1}{s_1} = 5 \\cdot \\frac{3}{0.5} = 5 \\cdot 6 = 30\n$$\nFor $j=2$:\n$$\nx_2^{(1)} = x_2^{(0)} \\frac{b_2}{s_2} = 5 \\cdot \\frac{3}{0.7} = \\frac{15}{7/10} = \\frac{150}{7}\n$$\n\nThus, the updated activity vector after one iteration is $x^{(1)} = \\begin{bmatrix} 30 \\\\ 150/7 \\end{bmatrix}$.", "answer": "$$\n\\boxed{ \\begin{bmatrix} 30 \\\\ \\frac{150}{7} \\end{bmatrix} }\n$$", "id": "4927224"}, {"introduction": "While ML-EM is powerful, its reconstructions can sometimes be noisy, especially with low-count data typical in clinical SPECT. To address this, we can enhance the algorithm by adding a regularization term that incorporates prior knowledge about what a \"good\" image should look like. A very common approach is to enforce smoothness, penalizing solutions with noisy, unnatural variations between adjacent voxels. This practice [@problem_id:4927241] will show you how to mathematically formulate and compute this smoothness penalty and its gradient, which are the core components needed to upgrade an ML-EM algorithm to a more robust, penalized-likelihood method.", "problem": "Consider Single Photon Emission Computed Tomography (SPECT) reconstruction, where a penalized-likelihood objective augments a data fidelity term with a spatial smoothness regularization that encodes a prior belief about the image. Let the prior be modeled as a Gaussian Markov Random Field (GMRF) using a Gibbs distribution over undirected neighbor pairs. Specifically, for each undirected neighboring pair of voxel indices $(j,k)$, the clique potential is defined by $v_{jk}(x_j,x_k)=\\frac{\\beta\\,w_{jk}}{2}\\,(x_j-x_k)^{2}$, where $\\beta0$ is the regularization strength and $w_{jk}\\ge 0$ are symmetric edge weights. The negative log-prior (up to an additive constant) is the sum of these clique potentials over all undirected neighbor pairs, and this sum is taken as the regularization term $R(x)$ in the penalized objective.\n\nStarting from these definitions, derive an explicit expression for the regularization $R(x)$ and its gradient $\\nabla R(x)$ in terms of the neighbor sets and the image $x$. Then, for the toy one-dimensional image $x=\\begin{bmatrix}2\\\\4\\\\3\\end{bmatrix}$ with undirected neighbor sets given by $\\mathcal{N}(1)=\\{2\\}$, $\\mathcal{N}(2)=\\{1,3\\}$, $\\mathcal{N}(3)=\\{2\\}$, uniform weights $w_{jk}=1$ for all neighbor pairs, and regularization strength $\\beta=0.2$, compute the numerical values of $R(x)$ and the components of $\\nabla R(x)$. Provide the final answer as a single row vector containing $R(x)$ followed by the three components of $\\nabla R(x)$. No rounding is required, and no physical units are associated with these quantities.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   **Model**: Penalized-likelihood for SPECT image reconstruction.\n-   **Prior**: Gaussian Markov Random Field (GMRF) modeled as a Gibbs distribution.\n-   **Clique Potential**: For an undirected neighbor pair of voxels $(j,k)$, the potential is $v_{jk}(x_j,x_k)=\\frac{\\beta\\,w_{jk}}{2}\\,(x_j-x_k)^{2}$.\n-   **Regularization Strength**: $\\beta0$.\n-   **Edge Weights**: $w_{jk}\\ge 0$, symmetric ($w_{jk}=w_{kj}$).\n-   **Regularization Term $R(x)$**: The negative log-prior, defined as the sum of clique potentials over all undirected neighbor pairs.\n-   **Task 1**: Derive explicit expressions for $R(x)$ and its gradient $\\nabla R(x)$.\n-   **Task 2**: Compute numerical values for a specific case.\n-   **Image**: $x=\\begin{bmatrix}2\\\\4\\\\3\\end{bmatrix}$, so $x_1=2$, $x_2=4$, $x_3=3$.\n-   **Neighbor Sets**: $\\mathcal{N}(1)=\\{2\\}$, $\\mathcal{N}(2)=\\{1,3\\}$, $\\mathcal{N}(3)=\\{2\\}$.\n-   **Weights**: $w_{jk}=1$ for all neighbor pairs.\n-   **Regularization Strength**: $\\beta=0.2$.\n-   **Output Format**: A single row vector $[R(x), (\\nabla R(x))_1, (\\nabla R(x))_2, (\\nabla R(x))_3]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is analyzed against the validation criteria.\n-   **Scientifically Grounded**: The problem describes a quadratic smoothness prior (a form of GMRF), a cornerstone of modern statistical image reconstruction in medical imaging (SPECT, PET, CT). The formulation is standard and mathematically sound.\n-   **Well-Posed**: The problem is clearly stated with all necessary variables, definitions, and data for both the derivation and the numerical calculation. A unique solution exists.\n-   **Objective**: The problem is stated in precise, unbiased mathematical language.\n\nThe problem does not exhibit any of the invalidity flaws. It is a standard, well-defined problem in the field of medical image processing.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Derivation of $R(x)$ and $\\nabla R(x)$\n\nThe regularization term $R(x)$ is the sum of clique potentials over all undirected neighbor pairs. An undirected pair $(j,k)$ means that voxel $j$ is a neighbor of voxel $k$, and vice-versa. We can represent the set of all such unique pairs as $\\mathcal{E}$, where if $(j,k) \\in \\mathcal{E}$, we conventionally take $jk$ to avoid double-counting.\n\nThe definition of $R(x)$ is thus:\n$$R(x) = \\sum_{(j,k) \\in \\mathcal{E}} v_{jk}(x_j, x_k)$$\nSubstituting the given clique potential expression:\n$$R(x) = \\sum_{(j,k) \\in \\mathcal{E}} \\frac{\\beta\\,w_{jk}}{2}\\,(x_j-x_k)^{2}$$\nThis can be written using the neighbor sets $\\mathcal{N}(j)$ by summing over all voxels $j$ and their neighbors $k \\in \\mathcal{N}(j)$ with the condition $kj$ to ensure each pair is counted only once:\n$$R(x) = \\frac{\\beta}{2} \\sum_{j} \\sum_{k \\in \\mathcal{N}(j), kj} w_{jk}\\,(x_j-x_k)^{2}$$\nThis is the explicit expression for $R(x)$.\n\nNext, we derive the gradient of $R(x)$, denoted $\\nabla R(x)$. The $i$-th component of the gradient is the partial derivative of $R(x)$ with respect to the $i$-th voxel value, $x_i$.\n$$(\\nabla R(x))_i = \\frac{\\partial R(x)}{\\partial x_i}$$\nFor differentiation, it is often more convenient to use a symmetric form of the sum that explicitly double-counts and then corrects with a factor of $\\frac{1}{2}$.\n$$R(x) = \\frac{1}{2} \\sum_{j} \\sum_{k \\in \\mathcal{N}(j)} \\frac{\\beta\\,w_{jk}}{2}\\,(x_j-x_k)^{2} = \\frac{\\beta}{4} \\sum_{j} \\sum_{k \\in \\mathcal{N}(j)} w_{jk}\\,(x_j-x_k)^{2}$$\nNow, we differentiate with respect to $x_i$:\n$$(\\nabla R(x))_i = \\frac{\\partial}{\\partial x_i} \\left[ \\frac{\\beta}{4} \\sum_{j} \\sum_{k \\in \\mathcal{N}(j)} w_{jk}\\,(x_j-x_k)^{2} \\right]$$\nThe derivative is non-zero only for terms where $j=i$ or $k=i$. We can split the sum into these two cases.\n$$(\\nabla R(x))_i = \\frac{\\beta}{4} \\left[ \\frac{\\partial}{\\partial x_i} \\sum_{k \\in \\mathcal{N}(i)} w_{ik}\\,(x_i-x_k)^{2} + \\frac{\\partial}{\\partial x_i} \\sum_{j \\text{ s.t. } i \\in \\mathcal{N}(j)} w_{ji}\\,(x_j-x_i)^{2} \\right]$$\nFor the first term:\n$$\\frac{\\partial}{\\partial x_i} \\sum_{k \\in \\mathcal{N}(i)} w_{ik}\\,(x_i-x_k)^{2} = \\sum_{k \\in \\mathcal{N}(i)} w_{ik} \\cdot 2(x_i-x_k) \\cdot (1) = 2 \\sum_{k \\in \\mathcal{N}(i)} w_{ik}(x_i-x_k)$$\nFor the second term:\n$$\\frac{\\partial}{\\partial x_i} \\sum_{j \\in \\mathcal{N}(i)} w_{ji}\\,(x_j-x_i)^{2} = \\sum_{j \\in \\mathcal{N}(i)} w_{ji} \\cdot 2(x_j-x_i) \\cdot (-1) = -2 \\sum_{j \\in \\mathcal{N}(i)} w_{ji}(x_j-x_i) = 2 \\sum_{j \\in \\mathcal{N}(i)} w_{ji}(x_i-x_j)$$\n(Note that the condition $i \\in \\mathcal{N}(j)$ is equivalent to $j \\in \\mathcal{N}(i)$ due to the undirected nature of the neighborhood graph).\nSince weights are symmetric ($w_{ji}=w_{ij}$), the second term is identical to the first (by relabeling the summation index $j$ to $k$).\n$$(\\nabla R(x))_i = \\frac{\\beta}{4} \\left[ 2 \\sum_{k \\in \\mathcal{N}(i)} w_{ik}(x_i-x_k) + 2 \\sum_{k \\in \\mathcal{N}(i)} w_{ik}(x_i-x_k) \\right] = \\frac{\\beta}{4} \\left[ 4 \\sum_{k \\in \\mathcal{N}(i)} w_{ik}(x_i-x_k) \\right]$$\nThis simplifies to the final expression for the gradient component:\n$$(\\nabla R(x))_i = \\beta \\sum_{k \\in \\mathcal{N}(i)} w_{ik}(x_i-x_k)$$\n\n### Numerical Computation\n\nWe apply these derived formulas to the given numerical example.\nThe image is $x = \\begin{bmatrix} 2 \\\\ 4 \\\\ 3 \\end{bmatrix}$, so $x_1=2$, $x_2=4$, $x_3=3$.\nThe neighbor sets are $\\mathcal{N}(1)=\\{2\\}$, $\\mathcal{N}(2)=\\{1,3\\}$, $\\mathcal{N}(3)=\\{2\\}$. From this, the set of unique undirected neighbor pairs is $\\mathcal{E}=\\{(1,2), (2,3)\\}$.\nParameters are $\\beta=0.2$ and $w_{jk}=1$ for all $(j,k) \\in \\mathcal{E}$.\n\n**1. Compute $R(x)$:**\nWe sum the potentials over the unique neighbor pairs $(1,2)$ and $(2,3)$.\n$$R(x) = v_{12}(x_1, x_2) + v_{23}(x_2, x_3)$$\n$$R(x) = \\frac{\\beta w_{12}}{2}(x_1 - x_2)^2 + \\frac{\\beta w_{23}}{2}(x_2 - x_3)^2$$\n$$R(x) = \\frac{0.2 \\cdot 1}{2}(2-4)^2 + \\frac{0.2 \\cdot 1}{2}(4-3)^2$$\n$$R(x) = 0.1 \\cdot (-2)^2 + 0.1 \\cdot (1)^2$$\n$$R(x) = 0.1 \\cdot 4 + 0.1 \\cdot 1 = 0.4 + 0.1 = 0.5$$\n\n**2. Compute $\\nabla R(x)$:**\nWe compute each component of the gradient using the formula $(\\nabla R(x))_i = \\beta \\sum_{k \\in \\mathcal{N}(i)} w_{ik}(x_i-x_k)$.\n\n-   **Component 1 ($i=1$):** The neighbor set is $\\mathcal{N}(1)=\\{2\\}$.\n    $$(\\nabla R(x))_1 = \\beta \\cdot w_{12}(x_1 - x_2) = 0.2 \\cdot 1 \\cdot (2 - 4) = 0.2 \\cdot (-2) = -0.4$$\n\n-   **Component 2 ($i=2$):** The neighbor set is $\\mathcal{N}(2)=\\{1,3\\}$.\n    $$(\\nabla R(x))_2 = \\beta \\cdot [w_{21}(x_2 - x_1) + w_{23}(x_2 - x_3)]$$\n    $$(\\nabla R(x))_2 = 0.2 \\cdot [1 \\cdot (4-2) + 1 \\cdot (4-3)] = 0.2 \\cdot [2 + 1] = 0.2 \\cdot 3 = 0.6$$\n\n-   **Component 3 ($i=3$):** The neighbor set is $\\mathcal{N}(3)=\\{2\\}$.\n    $$(\\nabla R(x))_3 = \\beta \\cdot w_{32}(x_3 - x_2) = 0.2 \\cdot 1 \\cdot (3-4) = 0.2 \\cdot (-1) = -0.2$$\n\nThe gradient vector is $\\nabla R(x) = \\begin{bmatrix} -0.4 \\\\ 0.6 \\\\ -0.2 \\end{bmatrix}$.\n\nThe final answer is the row vector containing $R(x)$ followed by the components of $\\nabla R(x)$.\n$$[R(x), (\\nabla R(x))_1, (\\nabla R(x))_2, (\\nabla R(x))_3] = [0.5, -0.4, 0.6, -0.2]$$", "answer": "$$\\boxed{\\begin{pmatrix} 0.5  -0.4  0.6  -0.2 \\end{pmatrix}}$$", "id": "4927241"}]}