{"hands_on_practices": [{"introduction": "In any laboratory measurement, a final calculated result is often derived from several independent measurements, each with its own uncertainty. This practice explores how to combine these individual uncertainties to determine the uncertainty of the final result. By deriving and applying the fundamental law of propagation of uncertainty, you will see how errors in mass and volume measurements combine to affect the final calculated concentration of a solution ([@problem_id:5230808]). This skill is essential for reporting results with a valid statement of measurement quality.", "problem": "In a clinical chemistry laboratory, analytical results are reported with quantified uncertainty to support metrological traceability and medical decision-making. Consider two measured quantities, $x_{1}$ and $x_{2}$, that represent independent realizations of random variables with small, zero-mean errors about their true values, and finite variances. Let a reported measurand be a differentiable function $y = f(x_{1}, x_{2})$. Starting from a first-order Taylor expansion of $f$ about the best estimates of $x_{1}$ and $x_{2}$ and from the definitions of variance, covariance, and independence of random variables, derive the linear (first-order) propagation-of-uncertainty expressions for the combined standard uncertainty of the following two cases:\n1) $y = x_{1} + x_{2}$,\n2) $y = x_{1} x_{2}$.\nClearly state any assumptions used in the derivations.\n\nNext, apply these results to a routine preparation in laboratory diagnostics. A stock solution of an analyte is prepared by quantitatively transferring a mass $m$ of the pure analyte into a volumetric flask and diluting to the calibration mark to obtain a final solution volume $V$. The reported concentration is $c = m / V$ in units of $\\mathrm{g}\\,\\mathrm{L}^{-1}$. The measured quantities and their uncertainties are:\n- Mass: $m = 0.40123\\,\\mathrm{g}$ with a standard uncertainty $u_{m} = 0.00012\\,\\mathrm{g}$.\n- Volume: $V = 100.00\\,\\mathrm{mL}$ determined with a calibrated Class A volumetric flask whose calibration certificate states an expanded uncertainty $U_{V} = 0.08\\,\\mathrm{mL}$ at coverage factor $k = 2$. Assume a normal distribution for the stated expanded uncertainty and convert it to a standard uncertainty before use. Neglect thermal expansion effects and meniscus reading bias. Assume $m$ and $V$ are independent.\n\nUsing your derived linear propagation-of-uncertainty framework, compute the combined standard uncertainty $u_{c}$ of the concentration $c$ in $\\mathrm{g}\\,\\mathrm{L}^{-1}$. Round your final numerical answer to three significant figures. Express the final combined standard uncertainty in $\\mathrm{g}\\,\\mathrm{L}^{-1}$.", "solution": "The problem requires the derivation of the linear propagation-of-uncertainty formulas for the sum and product of two independent variables, followed by an application to a concentration calculation in a clinical laboratory context. The solution is structured in three parts: first, the general derivation; second, the specialization to the two required cases; and third, the numerical application.\n\n**Part 1: General Derivation of the Law of Propagation of Uncertainty**\n\nLet $x_1$ and $x_2$ be two independently measured quantities with best estimates (mean values) $\\bar{x}_1$ and $\\bar{x}_2$, and standard uncertainties (standard deviations) $u_{x_1}$ and $u_{x_2}$ respectively. The variances are $u_{x_1}^2$ and $u_{x_2}^2$. The measurand $y$ is a differentiable function of these quantities, $y = f(x_1, x_2)$.\n\nAn assumption stated in the problem is that the errors in $x_1$ and $x_2$ are small. This justifies approximating the function $f(x_1, x_2)$ using a first-order Taylor series expansion about the point $(\\bar{x}_1, \\bar{x}_2)$:\n$$y = f(x_1, x_2) \\approx f(\\bar{x}_1, \\bar{x}_2) + \\left(\\frac{\\partial f}{\\partial x_1}\\right)_{(\\bar{x}_1, \\bar{x}_2)}(x_1 - \\bar{x}_1) + \\left(\\frac{\\partial f}{\\partial x_2}\\right)_{(\\bar{x}_1, \\bar{x}_2)}(x_2 - \\bar{x}_2)$$\nThe best estimate for $y$ is $\\bar{y} = f(\\bar{x}_1, \\bar{x}_2)$. The deviation of $y$ from its best estimate is therefore:\n$$y - \\bar{y} \\approx \\left(\\frac{\\partial f}{\\partial x_1}\\right)(x_1 - \\bar{x}_1) + \\left(\\frac{\\partial f}{\\partial x_2}\\right)(x_2 - \\bar{x}_2)$$\nFor brevity, the partial derivatives are understood to be evaluated at $(\\bar{x}_1, \\bar{x}_2)$.\n\nThe combined variance of $y$, denoted $u_c(y)^2$ or simply $u_y^2$, is defined as the expectation of the squared deviation, $E[(y - \\bar{y})^2]$. Substituting the linear approximation for the deviation gives:\n$$u_y^2 \\approx E\\left[ \\left( \\left(\\frac{\\partial f}{\\partial x_1}\\right)(x_1 - \\bar{x}_1) + \\left(\\frac{\\partial f}{\\partial x_2}\\right)(x_2 - \\bar{x}_2) \\right)^2 \\right]$$\nExpanding the square within the expectation operator:\n$$u_y^2 \\approx E\\left[ \\left(\\frac{\\partial f}{\\partial x_1}\\right)^2(x_1 - \\bar{x}_1)^2 + \\left(\\frac{\\partial f}{\\partial x_2}\\right)^2(x_2 - \\bar{x}_2)^2 + 2\\left(\\frac{\\partial f}{\\partial x_1}\\right)\\left(\\frac{\\partial f}{\\partial x_2}\\right)(x_1 - \\bar{x}_1)(x_2 - \\bar{x}_2) \\right]$$\nUsing the linearity of the expectation operator, we can write:\n$$u_y^2 \\approx \\left(\\frac{\\partial f}{\\partial x_1}\\right)^2 E[(x_1 - \\bar{x}_1)^2] + \\left(\\frac{\\partial f}{\\partial x_2}\\right)^2 E[(x_2 - \\bar{x}_2)^2] + 2\\left(\\frac{\\partial f}{\\partial x_1}\\right)\\left(\\frac{\\partial f}{\\partial x_2}\\right)E[(x_1 - \\bar{x}_1)(x_2 - \\bar{x}_2)]$$\nBy definition, the variance of $x_i$ is $u_{x_i}^2 = E[(x_i - \\bar{x}_i)^2]$, and the covariance of $x_1$ and $x_2$ is $u(x_1, x_2) = \\mathrm{Cov}(x_1, x_2) = E[(x_1 - \\bar{x}_1)(x_2 - \\bar{x}_2)]$. The expression becomes:\n$$u_y^2 \\approx \\left(\\frac{\\partial f}{\\partial x_1}\\right)^2 u_{x_1}^2 + \\left(\\frac{\\partial f}{\\partial x_2}\\right)^2 u_{x_2}^2 + 2\\left(\\frac{\\partial f}{\\partial x_1}\\right)\\left(\\frac{\\partial f}{\\partial x_2}\\right)u(x_1, x_2)$$\nA key assumption provided is that $x_1$ and $x_2$ are independent random variables. For independent variables, their covariance is zero, $u(x_1, x_2) = 0$. This simplifies the expression to the general law of propagation of uncertainty for independent variables:\n$$u_y^2 = \\left(\\frac{\\partial f}{\\partial x_1}\\right)^2 u_{x_1}^2 + \\left(\\frac{\\partial f}{\\partial x_2}\\right)^2 u_{x_2}^2$$\nThe combined standard uncertainty, $u_y$, is the square root of this variance.\n\n**Part 2: Derivations for Specific Cases**\n\nWe apply the general formula derived above to the two specified functions.\n\n**Case 1: $y = x_1 + x_2$**\nThe partial derivatives of the function $f(x_1, x_2) = x_1 + x_2$ are:\n$$\\frac{\\partial y}{\\partial x_1} = 1, \\quad \\frac{\\partial y}{\\partial x_2} = 1$$\nSubstituting these into the general uncertainty propagation formula gives the combined variance:\n$$u_y^2 = (1)^2 u_{x_1}^2 + (1)^2 u_{x_2}^2 = u_{x_1}^2 + u_{x_2}^2$$\nThis shows that for the sum of two independent variables, the variances add. The combined standard uncertainty is $u_y = \\sqrt{u_{x_1}^2 + u_{x_2}^2}$.\n\n**Case 2: $y = x_1 x_2$**\nThe partial derivatives of the function $f(x_1, x_2) = x_1 x_2$ are:\n$$\\frac{\\partial y}{\\partial x_1} = x_2, \\quad \\frac{\\partial y}{\\partial x_2} = x_1$$\nSubstituting these into the general formula (and evaluating at the best estimates $\\bar{x}_1, \\bar{x}_2$):\n$$u_y^2 = (\\bar{x}_2)^2 u_{x_1}^2 + (\\bar{x}_1)^2 u_{x_2}^2$$\nIt is often more convenient to express this in terms of relative uncertainties. Dividing both sides by $y^2 = (x_1 x_2)^2$ (evaluated at the best estimates $\\bar{y}^2 = (\\bar{x}_1 \\bar{x}_2)^2$):\n$$\\frac{u_y^2}{\\bar{y}^2} = \\frac{(\\bar{x}_2)^2 u_{x_1}^2}{(\\bar{x}_1 \\bar{x}_2)^2} + \\frac{(\\bar{x}_1)^2 u_{x_2}^2}{(\\bar{x}_1 \\bar{x}_2)^2}$$\nSimplifying the expression yields:\n$$\\left(\\frac{u_y}{\\bar{y}}\\right)^2 = \\left(\\frac{u_{x_1}}{\\bar{x}_1}\\right)^2 + \\left(\\frac{u_{x_2}}{\\bar{x}_2}\\right)^2$$\nThis shows that for the product of two independent variables, the squares of the relative standard uncertainties add.\n\n**Part 3: Application to Concentration Uncertainty Calculation**\n\nThe concentration $c$ is given by the model equation $c = m/V$, where $m$ is the mass and $V$ is the volume. We are given that $m$ and $V$ are independent. This model is a division, which is analogous to a multiplication ($c=mV^{-1}$). The rule for relative uncertainties applies. To be rigorous, we will derive it from the general formula.\n\nThe partial derivatives of $c(m, V) = m/V$ are:\n$$\\frac{\\partial c}{\\partial m} = \\frac{1}{V}, \\quad \\frac{\\partial c}{\\partial V} = -\\frac{m}{V^2}$$\nApplying the general uncertainty propagation formula:\n$$u_c^2 = \\left(\\frac{\\partial c}{\\partial m}\\right)^2 u_m^2 + \\left(\\frac{\\partial c}{\\partial V}\\right)^2 u_V^2 = \\left(\\frac{1}{V}\\right)^2 u_m^2 + \\left(-\\frac{m}{V^2}\\right)^2 u_V^2$$\n$$u_c^2 = \\frac{u_m^2}{V^2} + \\frac{m^2 u_V^2}{V^4}$$\nTo obtain the relative uncertainty, we divide by $c^2 = (m/V)^2 = m^2/V^2$:\n$$\\frac{u_c^2}{c^2} = \\frac{u_m^2/V^2}{m^2/V^2} + \\frac{m^2 u_V^2/V^4}{m^2/V^2} = \\frac{u_m^2}{m^2} + \\frac{u_V^2}{V^2}$$\n$$\\left(\\frac{u_c}{c}\\right)^2 = \\left(\\frac{u_m}{m}\\right)^2 + \\left(\\frac{u_V}{V}\\right)^2$$\nThis confirms that the squares of the relative standard uncertainties add for division as well.\n\nNow, we substitute the numerical values.\nThe given data are:\n- Mass: $m = 0.40123\\,\\mathrm{g}$\n- Standard uncertainty of mass: $u_m = 0.00012\\,\\mathrm{g}$\n- Volume: $V = 100.00\\,\\mathrm{mL}$\n- Expanded uncertainty of volume: $U_V = 0.08\\,\\mathrm{mL}$ with a coverage factor $k=2$.\n\nFirst, we must ensure all units are consistent. The final result is requested in $\\mathrm{g}\\,\\mathrm{L}^{-1}$, so we convert the volume quantities from $\\mathrm{mL}$ to $\\mathrm{L}$.\n$$V = 100.00\\,\\mathrm{mL} = 0.10000\\,\\mathrm{L}$$\n$$U_V = 0.08\\,\\mathrm{mL} = 0.00008\\,\\mathrm{L}$$\nNext, we convert the expanded uncertainty $U_V$ to a standard uncertainty $u_V$ by dividing by the coverage factor $k$:\n$$u_V = \\frac{U_V}{k} = \\frac{0.00008\\,\\mathrm{L}}{2} = 0.00004\\,\\mathrm{L}$$\n\nNow we calculate the relative standard uncertainties for mass and volume:\nRelative uncertainty of mass:\n$$\\frac{u_m}{m} = \\frac{0.00012\\,\\mathrm{g}}{0.40123\\,\\mathrm{g}} \\approx 2.990803 \\times 10^{-4}$$\nRelative uncertainty of volume:\n$$\\frac{u_V}{V} = \\frac{0.00004\\,\\mathrm{L}}{0.10000\\,\\mathrm{L}} = 4.0000 \\times 10^{-4}$$\n\nThe square of the relative combined uncertainty is the sum of the squares of these individual relative uncertainties:\n$$\\left(\\frac{u_c}{c}\\right)^2 = \\left(\\frac{u_m}{m}\\right)^2 + \\left(\\frac{u_V}{V}\\right)^2 \\approx (2.990803 \\times 10^{-4})^2 + (4.0000 \\times 10^{-4})^2$$\n$$\\left(\\frac{u_c}{c}\\right)^2 \\approx 8.94490 \\times 10^{-8} + 1.60000 \\times 10^{-7} = 8.94490 \\times 10^{-8} + 16.0000 \\times 10^{-8} = 2.49449 \\times 10^{-7}$$\n\nThe relative combined standard uncertainty is the square root of this value:\n$$\\frac{u_c}{c} = \\sqrt{2.49449 \\times 10^{-7}} \\approx 4.994487 \\times 10^{-4}$$\n\nTo find the absolute combined standard uncertainty $u_c$, we multiply this relative uncertainty by the value of the concentration $c$. First, calculate $c$:\n$$c = \\frac{m}{V} = \\frac{0.40123\\,\\mathrm{g}}{0.10000\\,\\mathrm{L}} = 4.0123\\,\\mathrm{g}\\,\\mathrm{L}^{-1}$$\nNow, calculate $u_c$:\n$$u_c = c \\times \\left(\\frac{u_c}{c}\\right) = 4.0123\\,\\mathrm{g}\\,\\mathrm{L}^{-1} \\times 4.994487 \\times 10^{-4} \\approx 0.00200397\\,\\mathrm{g}\\,\\mathrm{L}^{-1}$$\n\nThe problem requires rounding the final numerical answer to three significant figures.\n$$u_c \\approx 0.00200\\,\\mathrm{g}\\,\\mathrm{L}^{-1}$$\nThis is the combined standard uncertainty of the concentration.", "answer": "$$\\boxed{0.00200}$$", "id": "5230808"}, {"introduction": "Measurement error is not a single entity; it consists of both random and systematic components. This exercise demonstrates mathematically why averaging multiple measurements is a cornerstone of quality control, as it effectively reduces the impact of random error while leaving systematic bias unchanged ([@problem_id:5230829]). Understanding this distinction is crucial for designing experiments and interpreting the reliability of a reported average value.", "problem": "A clinical chemistry laboratory measures serum glucose concentration using an enzymatic spectrophotometric assay. The measurement model for a single replicate is that the reported value $X$ equals the true concentration $T$ plus a constant calibration bias $b$ and a random error $\\varepsilon$, where $\\varepsilon$ is independent across replicates and follows a Gaussian (normal) distribution with mean $0$ and variance $\\sigma^{2}$. The laboratory has characterized this method and found a constant bias of $b=+1.5$ mg/dL and a single-replicate random standard deviation of $\\sigma=2.0$ mg/dL under stable operating conditions.\n\nAn analyst considers measuring $n$ independent replicate aliquots from the same patient sample and reporting the arithmetic mean $\\bar{X}$ to reduce the impact of random error on a clinical decision. Using only fundamental definitions of expectation and variance, and the assumption of independence of replicate errors, derive how the expectation and variance of $\\bar{X}$ depend on $n$ and explain, in terms of these definitions, why the contribution of random error to the uncertainty of $\\bar{X}$ decreases with increasing $n$ while the systematic bias does not.\n\nThe laboratory’s clinical decision rule for this assay requires that the magnitude of the random component of the error in the reported mean be less than $2.0$ mg/dL with probability at least $0.95$. Assume the Gaussian model above holds for the random error and that the patient’s analyte concentration is stable during the measurement series. Determine the smallest integer number of replicates $n$ that satisfies this requirement.\n\nExpress your final answer as the minimal integer $n$. Do not include units. If any rounding is needed during intermediate steps, carry it through to a final integer $n$ as required by the decision rule. The bias value and all other quantities in the prompt are given in mg/dL.", "solution": "The problem is divided into two parts: a derivation and explanation of the effect of replicate measurements on systematic and random errors, and a calculation of the minimum number of replicates to meet a specific clinical requirement.\n\n**Part 1: Expectation, Variance, and Error Analysis**\n\nLet $T$ be the true glucose concentration, $b$ the constant systematic bias, and $\\varepsilon_i$ the random error for the $i$-th replicate measurement. The model for a single measurement $X_i$ is given by:\n$$X_i = T + b + \\varepsilon_i$$\nWe are given that the random errors $\\varepsilon_i$ are independent and identically distributed (i.i.d.) from a Gaussian distribution with mean $E[\\varepsilon_i] = 0$ and variance $Var(\\varepsilon_i) = \\sigma^2$. The given values are $b = 1.5$ mg/dL and $\\sigma = 2.0$ mg/dL.\n\nThe analyst reports the arithmetic mean $\\bar{X}$ of $n$ independent replicates:\n$$\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$$\n\nFirst, we derive the expectation of $\\bar{X}$. Using the linearity property of the expectation operator, and noting that $T$ and $b$ are constants:\n$$E[\\bar{X}] = E\\left[\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right] = \\frac{1}{n} \\sum_{i=1}^{n} E[X_i]$$\nThe expectation of a single measurement $X_i$ is:\n$$E[X_i] = E[T + b + \\varepsilon_i] = E[T] + E[b] + E[\\varepsilon_i] = T + b + 0 = T + b$$\nSubstituting this back into the expression for $E[\\bar{X}]$:\n$$E[\\bar{X}] = \\frac{1}{n} \\sum_{i=1}^{n} (T + b) = \\frac{1}{n} (n(T+b)) = T + b$$\nThe expectation of the mean of $n$ replicates is $T+b$.\n\nNext, we derive the variance of $\\bar{X}$. Using the properties of variance for a sum of independent random variables and for scaling by a constant:\n$$Var(\\bar{X}) = Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right) = \\left(\\frac{1}{n}\\right)^2 Var\\left(\\sum_{i=1}^{n} X_i\\right)$$\nSince the replicate measurements are independent (due to the independence of the random errors $\\varepsilon_i$), the variance of the sum is the sum of the variances:\n$$Var\\left(\\sum_{i=1}^{n} X_i\\right) = \\sum_{i=1}^{n} Var(X_i)$$\nThe variance of a single measurement $X_i$ is:\n$$Var(X_i) = Var(T + b + \\varepsilon_i)$$\nSince $T$ and $b$ are constants, they do not contribute to the variance. Thus:\n$$Var(X_i) = Var(\\varepsilon_i) = \\sigma^2$$\nSubstituting this back into the expression for $Var(\\bar{X})$:\n$$Var(\\bar{X}) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\sigma^2 = \\frac{1}{n^2} (n\\sigma^2) = \\frac{\\sigma^2}{n}$$\nThe variance of the mean of $n$ replicates is $\\frac{\\sigma^2}{n}$.\n\nNow, we explain the effect of $n$ on systematic and random errors.\nThe total error in the reported mean is $\\bar{X} - T$. The expected value of this total error is the systematic error, or bias:\n$$E[\\bar{X} - T] = E[\\bar{X}] - T = (T+b) - T = b$$\nThis result shows that the systematic bias $b$ is not affected by the number of replicates $n$. Averaging does not reduce systematic error.\n\nThe random part of the error in the mean can be defined as the deviation of $\\bar{X}$ from its own expected value, which is $\\bar{\\varepsilon} = \\bar{X} - E[\\bar{X}] = \\bar{X} - (T+b)$. The uncertainty due to this random component is quantified by its standard deviation, which is the square root of the variance of $\\bar{X}$:\n$$\\sigma_{\\bar{X}} = \\sqrt{Var(\\bar{X})} = \\sqrt{\\frac{\\sigma^2}{n}} = \\frac{\\sigma}{\\sqrt{n}}$$\nThis quantity is also known as the standard error of the mean. As the number of replicates $n$ increases, the denominator $\\sqrt{n}$ increases, and therefore the standard deviation of the mean, $\\sigma_{\\bar{X}}$, decreases. This demonstrates that the contribution of random error to the total uncertainty of the measurement is reduced by averaging multiple replicates, and this reduction scales with $1/\\sqrt{n}$.\n\n**Part 2: Calculation of the Minimum Number of Replicates**\n\nThe clinical decision rule requires that the magnitude of the random component of the error in the reported mean be less than $2.0$ mg/dL with a probability of at least $0.95$.\nThe random component of the error in the mean is $\\bar{\\varepsilon} = \\bar{X} - (T+b)$. We have shown that this is equivalent to the mean of the individual random errors:\n$$\\bar{\\varepsilon} = \\frac{1}{n} \\sum_{i=1}^{n} \\varepsilon_i$$\nSince each $\\varepsilon_i$ is independently drawn from a $N(0, \\sigma^2)$ distribution, their sum $\\sum \\varepsilon_i$ is distributed as $N(0, n\\sigma^2)$. Consequently, their mean $\\bar{\\varepsilon}$ is distributed as:\n$$\\bar{\\varepsilon} \\sim N\\left(0, \\frac{\\sigma^2}{n}\\right)$$\nThe decision rule can be stated mathematically as:\n$$P(|\\bar{\\varepsilon}|  2.0) \\ge 0.95$$\nTo evaluate this probability, we standardize the random variable $\\bar{\\varepsilon}$ to a standard normal variable $Z \\sim N(0, 1)$, where $Z = \\frac{\\bar{\\varepsilon} - 0}{\\sigma/\\sqrt{n}}$.\nThe inequality becomes:\n$$P\\left( \\left|\\frac{\\bar{\\varepsilon}}{\\sigma/\\sqrt{n}}\\right|  \\frac{2.0}{\\sigma/\\sqrt{n}} \\right) \\ge 0.95$$\n$$P\\left( |Z|  \\frac{2.0\\sqrt{n}}{\\sigma} \\right) \\ge 0.95$$\nFor the standard normal distribution, the interval that contains $95\\%$ of the probability is defined by the critical value $z_{crit}$ such that $P(|Z|  z_{crit}) = 0.95$. This corresponds to a cumulative probability of $0.975$ at the upper limit. This critical value is $z_{0.975} \\approx 1.96$.\n\nTherefore, we must satisfy the condition:\n$$\\frac{2.0\\sqrt{n}}{\\sigma} \\ge z_{0.975}$$\nWe are given $\\sigma = 2.0$ mg/dL. Substituting this value:\n$$\\frac{2.0\\sqrt{n}}{2.0} \\ge 1.96$$\n$$\\sqrt{n} \\ge 1.96$$\nSquaring both sides gives the condition for $n$:\n$$n \\ge (1.96)^2$$\n$$n \\ge 3.8416$$\nSince the number of replicates $n$ must be an integer, the smallest integer value of $n$ that satisfies this condition is $4$.\nTherefore, a minimum of $4$ replicates are required.", "answer": "$$ \\boxed{4} $$", "id": "5230829"}, {"introduction": "Knowing that both systematic and random errors exist, a critical question for any analyst is: which one is the dominant source of uncertainty in my measurement? This practice provides a framework for quantitatively comparing the effects of a known systematic bias ($b$) versus random imprecision ($c$) from a single instrument ([@problem_id:5230774]). By deriving the ratio of their impacts on a final measurement, you will develop a key skill for troubleshooting assays and prioritizing efforts for method improvement.", "problem": "A clinical biochemistry laboratory prepares a substrate solution for an enzymatic initial-rate assay by weighing a solute mass $m$ on an analytical balance and diluting it into a solvent volume $V_{0}$ using a calibrated micropipette. The intended concentration is $C_{0} = m / V_{0}$. The solute mass $m$ is measured with negligible uncertainty relative to volume errors. The micropipette that delivers the solvent exhibits a fractional pipetting bias $b$ (systematic over-delivery if $b  0$) and a random volume error characterized by a Coefficient of Variation (CV), denoted $c$, defined as $c = s_{V} / \\mu_{V}$ where $s_{V}$ is the standard deviation of delivered volumes and $\\mu_{V}$ is the mean delivered volume. Assume small errors so that first-order linearization of transformations is valid.\n\nIn the enzymatic assay, the initial rate $v$ is measured under substrate concentration conditions where $[S] \\ll K_{m}$, so that $v$ is proportional to $C$ with proportionality constant $k$ (that is, $v = k\\,C$ where $k = V_{\\max} / K_{m}$ is constant across replicates). Starting from the fundamental definitions of bias, variance, and the Coefficient of Variation, and using scientifically justified approximations for small errors, derive the ratio $R$ of the magnitude of the systematic error in the initial rate $v$ (arising from the pipetting bias in $V_{s}$) to the standard deviation in $v$ (arising from random volume error across replicate pipettings). Evaluate $R$ for a micropipette set to $V_{0} = 200~\\mu\\mathrm{L}$ with fractional bias $b = 0.015$ and Coefficient of Variation $c = 0.007$.\n\nExpress the final numerical ratio $R$ as a dimensionless number and round your answer to four significant figures.", "solution": "The problem asks for the ratio $R$ of the magnitude of the systematic error in the measured initial rate $v$ to the standard deviation of $v$. The systematic error in $v$ is specified to arise from the pipetting bias, and the standard deviation in $v$ arises from the random volume error.\n\nFirst, we must formalize the statistical properties of the pipetted volume, denoted by the random variable $V$. The intended volume is $V_0$. The micropipette has a fractional bias $b$ and a coefficient of variation $c$.\n\nThe fractional bias $b$ is defined relative to the intended volume $V_0$ and the mean delivered volume $\\mu_V$:\n$$b = \\frac{\\mu_V - V_0}{V_0}$$\nFrom this, the mean delivered volume is $\\mu_V = V_0(1+b)$.\n\nThe coefficient of variation $c$ is defined as the ratio of the standard deviation of the delivered volume, $s_V$, to the mean delivered volume, $\\mu_V$:\n$$c = \\frac{s_V}{\\mu_V}$$\nFrom this, the standard deviation of the volume is $s_V = c \\mu_V = c V_0(1+b)$.\n\nThe concentration of the substrate solution, $C$, is given by $C = m/V$, where the mass $m$ is a constant with negligible uncertainty. The intended concentration is $C_0 = m/V_0$.\n\nThe initial rate of the enzymatic reaction, $v$, is proportional to the concentration, $v = kC$, where $k$ is a constant. Thus, $v = k(m/V)$. The intended rate is $v_0 = kC_0 = k(m/V_0)$.\n\nNext, we derive the systematic error in the initial rate, $\\Delta v_{sys}$. This error is due to the systematic bias in the volume delivery. It is the difference between the rate evaluated at the mean (biased) volume, $\\mu_V$, and the intended rate evaluated at the intended volume, $V_0$.\nThe rate at the mean volume is $v(\\mu_V) = k \\frac{m}{\\mu_V}$.\nThe intended rate is $v_0 = v(V_0) = k \\frac{m}{V_0}$.\nThe systematic error is therefore:\n$$\\Delta v_{sys} = v(\\mu_V) - v(V_0) = k \\frac{m}{\\mu_V} - k \\frac{m}{V_0}$$\nSubstituting $\\mu_V = V_0(1+b)$ and recognizing that $k \\frac{m}{V_0} = v_0$:\n$$\\Delta v_{sys} = k \\frac{m}{V_0(1+b)} - v_0 = \\frac{1}{1+b} \\left(k \\frac{m}{V_0}\\right) - v_0 = \\frac{v_0}{1+b} - v_0$$\n$$\\Delta v_{sys} = v_0 \\left(\\frac{1}{1+b} - 1\\right) = v_0 \\left(\\frac{1 - (1+b)}{1+b}\\right) = v_0 \\left(\\frac{-b}{1+b}\\right)$$\nThe magnitude of this systematic error is:\n$$|\\Delta v_{sys}| = \\left| v_0 \\left(\\frac{-b}{1+b}\\right) \\right| = v_0 \\frac{|b|}{1+b}$$\nSince the problem states $b  0$ for positive bias, $|b|=b$ and $1+b$ is positive.\n\nNow, we derive the standard deviation of the initial rate, $s_v$. This arises from the random error in the pipetted volume $V$. We use the formula for propagation of uncertainty, which is a first-order linearization, as sanctioned by the problem statement. For a function $f(X)$, the variance is $\\text{Var}(f(X)) \\approx (f'(\\mu_X))^2 \\text{Var}(X)$. Here, our function is $v(V) = km/V$ and the variable is $V$.\nThe derivative of $v$ with respect to $V$ is:\n$$\\frac{dv}{dV} = \\frac{d}{dV}\\left(\\frac{km}{V}\\right) = -\\frac{km}{V^2}$$\nWe evaluate this derivative at the mean volume, $\\mu_V$:\n$$\\left. \\frac{dv}{dV} \\right|_{V=\\mu_V} = -\\frac{km}{\\mu_V^2}$$\nThe variance of the rate, $s_v^2$, is then:\n$$s_v^2 = \\text{Var}(v) \\approx \\left(-\\frac{km}{\\mu_V^2}\\right)^2 s_V^2 = \\frac{k^2 m^2}{\\mu_V^4} s_V^2$$\nThe standard deviation, $s_v$, is the square root of the variance:\n$$s_v \\approx \\sqrt{\\frac{k^2 m^2}{\\mu_V^4} s_V^2} = \\frac{km}{\\mu_V^2} s_V$$\nWe can substitute the definition of the coefficient of variation, $s_V = c \\mu_V$:\n$$s_v \\approx \\frac{km}{\\mu_V^2} (c \\mu_V) = \\frac{kmc}{\\mu_V}$$\nNow, we express this in terms of the intended rate $v_0$. Substitute $\\mu_V = V_0(1+b)$:\n$$s_v \\approx \\frac{kmc}{V_0(1+b)} = \\frac{c}{1+b} \\left(\\frac{km}{V_0}\\right) = v_0 \\frac{c}{1+b}$$\n\nFinally, we compute the ratio $R$:\n$$R = \\frac{|\\Delta v_{sys}|}{s_v} = \\frac{v_0 \\frac{|b|}{1+b}}{v_0 \\frac{c}{1+b}}$$\nThe terms $v_0$ and $(1+b)$ cancel out, yielding a simple expression for the ratio:\n$$R = \\frac{|b|}{c}$$\nThis result shows that the ratio of the relative systematic error to the relative random error in the measurement is equal to the ratio of the relative systematic error source ($b$) to the relative random error source ($c$).\n\nWe are given the numerical values $b = 0.015$ and $c = 0.007$. The value of $V_0 = 200~\\mu\\mathrm{L}$ is not needed for the final ratio calculation.\n$$R = \\frac{|0.015|}{0.007} = \\frac{0.015}{0.007} = \\frac{15}{7}$$\nCalculating the numerical value:\n$$R \\approx 2.142857...$$\nRounding to four significant figures, as requested:\n$$R \\approx 2.143$$", "answer": "$$\\boxed{2.143}$$", "id": "5230774"}]}