{"hands_on_practices": [{"introduction": "Before we can rely on a Minimal Residual Disease (MRD) assay, we must first understand its performance. This exercise grounds you in the fundamental metrics of diagnostic testing: sensitivity, specificity, and predictive values. By applying Bayes' theorem to a hypothetical MRD assay, you will explore the critical relationship between a test's intrinsic characteristics and the prevalence of the disease in a population to determine the true meaning of a positive result [@problem_id:5231437].", "problem": "A laboratory develops an assay for Minimal Residual Disease (MRD) detection using circulating tumor DNA (ctDNA) quantified by digital polymerase chain reaction (PCR). In a given clinical cohort, disease status is unknown, and a single MRD test result is observed for each individual. Let $D$ denote the event \"true residual disease present\" and $\\neg D$ denote \"true residual disease absent.\" Let $T$ denote the event \"MRD test positive\" and $\\neg T$ denote \"MRD test negative.\" The cohort has disease prevalence $\\pi$, understood as the probability $P(D)$ and expressed as a decimal fraction. The assay has sensitivity $s$ and specificity $c$, defined in terms of conditional probabilities on $D$ and $\\neg D$.\n\nUsing only the fundamental definitions of conditional probability, the law of total probability, and Bayes' theorem, do the following:\n- Define sensitivity, specificity, Positive Predictive Value (PPV), and Negative Predictive Value (NPV) for this MRD assay in terms of $P(\\cdot)$ with the events $D$, $\\neg D$, $T$, and $\\neg T$.\n- Derive an analytic expression for the PPV, $P(D \\mid T)$, as a function of $\\pi$, $s$, and $c$.\n\nExpress the final PPV strictly as a closed-form analytic expression in terms of $\\pi$, $s$, and $c$. Do not use a percentage sign anywhere; treat all probabilities, including $\\pi$, as decimal fractions. No rounding is required.", "solution": "The problem is valid as it is scientifically grounded in the principles of biostatistics and medical diagnostics, well-posed with a clear objective and sufficient information, and objective in its formulation. We can proceed with a formal derivation.\n\nThe problem requires us to first define several key metrics for a diagnostic test and then derive an expression for the Positive Predictive Value (PPV).\n\nLet $D$ be the event that true residual disease is present, and $\\neg D$ be the event that it is absent. Let $T$ be the event of a positive test result, and $\\neg T$ be the event of a negative test result. Let the disease prevalence be $\\pi = P(D)$.\n\nThe definitions of sensitivity, specificity, Positive Predictive Value (PPV), and Negative Predictive Value (NPV) are based on conditional probabilities involving these events.\n\n1.  **Sensitivity ($s$)**: Sensitivity is the probability that the test correctly identifies individuals who have the disease. It is the probability of a positive test result ($T$) given that the disease is present ($D$).\n    $$s = P(T \\mid D)$$\n\n2.  **Specificity ($c$)**: Specificity is the probability that the test correctly identifies individuals who do not have the disease. It is the probability of a negative test result ($\\neg T$) given that the disease is absent ($\\neg D$).\n    $$c = P(\\neg T \\mid \\neg D)$$\n\n3.  **Positive Predictive Value (PPV)**: PPV is the probability that an individual with a positive test result actually has the disease. It is the probability of the disease being present ($D$) given a positive test result ($T$).\n    $$\\text{PPV} = P(D \\mid T)$$\n\n4.  **Negative Predictive Value (NPV)**: NPV is the probability that an individual with a negative test result is truly free of the disease. It is the probability of the disease being absent ($\\neg D$) given a negative test result ($\\neg T$).\n    $$\\text{NPV} = P(\\neg D \\mid \\neg T)$$\n\nNext, we derive an analytic expression for the PPV, $P(D \\mid T)$, as a function of prevalence $\\pi$, sensitivity $s$, and specificity $c$.\n\nThe derivation starts with the definition of conditional probability, which can be rearranged into the form known as Bayes' theorem. The PPV is $P(D \\mid T)$, and by Bayes' theorem, it is expressed as:\n$$P(D \\mid T) = \\frac{P(T \\mid D) P(D)}{P(T)}$$\n\nWe can identify the terms in the numerator from the problem's givens and our definitions:\n-   $P(T \\mid D)$ is the sensitivity, $s$.\n-   $P(D)$ is the prevalence, $\\pi$.\n\nSo, the numerator is $s \\pi$.\n\nThe denominator, $P(T)$, is the overall probability of a positive test result in the cohort. To express this in terms of the given parameters, we use the Law of Total Probability. The sample space is partitioned by the events $D$ and $\\neg D$. Therefore, the probability of event $T$ can be written as:\n$$P(T) = P(T \\cap D) + P(T \\cap \\neg D)$$\n\nUsing the definition of conditional probability, $P(A \\cap B) = P(A \\mid B) P(B)$, we can rewrite this as:\n$$P(T) = P(T \\mid D) P(D) + P(T \\mid \\neg D) P(\\neg D)$$\n\nLet's evaluate each term in this expression for $P(T)$:\n-   $P(T \\mid D) = s$, the sensitivity.\n-   $P(D) = \\pi$, the prevalence.\n-   $P(\\neg D)$ is the probability of not having the disease, which is $1 - P(D) = 1 - \\pi$.\n-   $P(T \\mid \\neg D)$ is the probability of a positive test given the absence of disease. This is the false positive rate. It can be derived from the specificity, $c$. Since for a disease-absent individual the test can only be positive or negative, we have $P(T \\mid \\neg D) + P(\\neg T \\mid \\neg D) = 1$. The specificity is $c = P(\\neg T \\mid \\neg D)$. Therefore, $P(T \\mid \\neg D) = 1 - c$.\n\nSubstituting these components back into the expression for $P(T)$:\n$$P(T) = (s)(\\pi) + (1 - c)(1 - \\pi)$$\n$$P(T) = s\\pi + (1 - c)(1 - \\pi)$$\n\nNow we have expressions for both the numerator and the denominator of the Bayes' theorem formula for PPV. Substituting them back, we obtain the final expression for PPV, $P(D \\mid T)$:\n$$P(D \\mid T) = \\frac{s \\pi}{s\\pi + (1 - c)(1 - \\pi)}$$\n\nThis is the analytic expression for the Positive Predictive Value as a function of prevalence ($\\pi$), sensitivity ($s$), and specificity ($c$).", "answer": "$$\\boxed{\\frac{s \\pi}{s \\pi + (1 - c)(1 - \\pi)}}$$", "id": "5231437"}, {"introduction": "Moving from test theory to laboratory practice, this problem focuses on a cornerstone technology for MRD detection: real-time quantitative PCR (qPCR). Accurate quantification depends on a properly constructed standard curve, which translates raw instrument readings ($C_t$ values) into initial target quantities. This exercise will guide you through analyzing a set of calibration data to determine the reaction's efficiency, a critical parameter for ensuring the reliability and accuracy of MRD measurements [@problem_id:5231553].", "problem": "In laboratory diagnostics of Minimal Residual Disease (MRD), real-time quantitative polymerase chain reaction (qPCR) is used to infer the initial target copy number in a patient sample by comparing its cycle threshold to a calibration curve constructed from known standards. Assume the following foundational facts: in qPCR, the target amplicon population increases multiplicatively by a constant factor per cycle, the fluorescence threshold is fixed across runs, and the cycle threshold $C_t$ is the cycle at which the accumulated amplicon first exceeds this fixed threshold. A ten-fold serial dilution of a standard with known initial target copies per reaction $N_0$ was run in triplicate and produced the following averaged calibration data (one value per dilution for this exercise):\n\n- $N_0$: $1.00 \\times 10^5$, $1.00 \\times 10^4$, $1.00 \\times 10^3$, $1.00 \\times 10^2$, $1.00 \\times 10^1$\n- $C_t$: $16.50$, $19.95$, $23.40$, $26.85$, $30.30$\n\nStarting from the exponential amplification model and the definition of $C_t$ with a fixed fluorescence threshold, derive the linear relationship between $C_t$ and $\\log_{10}(N_0)$ and identify the slope $m$ in terms of the per-cycle efficiency $E$. Then, using the given calibration data, compute the best-fit slope $m$ of $C_t$ versus $\\log_{10}(N_0)$ by least-squares linear regression, use your derived relation to compute the qPCR efficiency $E$, and round your final $E$ to four significant figures. Express the final efficiency as a decimal fraction (do not use a percentage sign).", "solution": "The problem is valid as it is scientifically grounded in the principles of real-time quantitative PCR (qPCR), is well-posed with sufficient data for a unique solution, and is expressed in objective, formal language. We can proceed with the solution.\n\nThe first step is to derive the theoretical relationship between the cycle threshold $C_t$ and the initial number of target molecules $N_0$. The amplification of the target DNA in qPCR follows an exponential model. Let $N_c$ be the number of target molecules after $c$ cycles. If $N_0$ is the initial number of molecules and $E$ is the per-cycle amplification efficiency (where $0 \\le E \\le 1$), the number of molecules after $c$ cycles is given by:\n$$N_c = N_0 (1+E)^c$$\nIn this equation, an efficiency $E=1$ corresponds to a perfect doubling of the product in each cycle.\n\nThe cycle threshold, $C_t$, is defined as the fractional cycle number at which the fluorescence signal, which is proportional to the amount of amplified DNA, crosses a fixed threshold. Let $N_T$ be the number of DNA molecules required to cross this fluorescence threshold. At the cycle threshold, $c=C_t$, we have $N_{C_t} = N_T$. Substituting this into the amplification equation:\n$$N_T = N_0 (1+E)^{C_t}$$\nTo establish the relationship between $C_t$ and $N_0$, we rearrange this equation. First, we isolate the term containing $C_t$:\n$$\\frac{N_T}{N_0} = (1+E)^{C_t}$$\nNext, we take the base-10 logarithm of both sides, as the problem requires a relationship with $\\log_{10}(N_0)$:\n$$\\log_{10}\\left(\\frac{N_T}{N_0}\\right) = \\log_{10}\\left((1+E)^{C_t}\\right)$$\nUsing the properties of logarithms, $\\log(a/b) = \\log(a) - \\log(b)$ and $\\log(a^b) = b\\log(a)$, we get:\n$$\\log_{10}(N_T) - \\log_{10}(N_0) = C_t \\log_{10}(1+E)$$\nNow, we solve for $C_t$:\n$$C_t = \\frac{\\log_{10}(N_T) - \\log_{10}(N_0)}{\\log_{10}(1+E)}$$\nThis can be written in the form of a linear equation, $y = mx + b$, where $y = C_t$ and $x = \\log_{10}(N_0)$:\n$$C_t = \\left(-\\frac{1}{\\log_{10}(1+E)}\\right) \\log_{10}(N_0) + \\left(\\frac{\\log_{10}(N_T)}{\\log_{10}(1+E)}\\right)$$\nFrom this derived form, we can identify the slope $m$ of the line $C_t$ versus $\\log_{10}(N_0)$:\n$$m = -\\frac{1}{\\log_{10}(1+E)}$$\nThis completes the first part of the problem.\n\nThe second part is to compute the best-fit slope $m$ using the provided experimental data. We will use the formula for the slope of a least-squares linear regression line. Let our data points be $(x_i, y_i)$, where $x_i = \\log_{10}(N_{0,i})$ and $y_i = C_{t,i}$. The number of data points is $n=5$.\n\nThe data are:\n- $N_0$: $1.00 \\times 10^5, 1.00 \\times 10^4, 1.00 \\times 10^3, 1.00 \\times 10^2, 1.00 \\times 10^1$\n- $x = \\log_{10}(N_0)$: $5, 4, 3, 2, 1$\n- $y = C_t$: $16.50, 19.95, 23.40, 26.85, 30.30$\n\nThe formula for the slope $m$ is:\n$$m = \\frac{n(\\sum_{i=1}^{n} x_i y_i) - (\\sum_{i=1}^{n} x_i)(\\sum_{i=1}^{n} y_i)}{n(\\sum_{i=1}^{n} x_i^2) - (\\sum_{i=1}^{n} x_i)^2}$$\nWe compute the necessary sums:\n$$ \\sum_{i=1}^{5} x_i = 5 + 4 + 3 + 2 + 1 = 15 $$\n$$ \\sum_{i=1}^{5} y_i = 16.50 + 19.95 + 23.40 + 26.85 + 30.30 = 117.00 $$\n$$ \\sum_{i=1}^{5} x_i^2 = 5^2 + 4^2 + 3^2 + 2^2 + 1^2 = 25 + 16 + 9 + 4 + 1 = 55 $$\n$$ \\sum_{i=1}^{5} x_i y_i = (5)(16.50) + (4)(19.95) + (3)(23.40) + (2)(26.85) + (1)(30.30) $$\n$$ \\sum_{i=1}^{5} x_i y_i = 82.50 + 79.80 + 70.20 + 53.70 + 30.30 = 316.50 $$\nNow, we substitute these sums into the formula for $m$:\n$$ m = \\frac{5(316.50) - (15)(117.00)}{5(55) - (15)^2} = \\frac{1582.5 - 1755}{275 - 225} = \\frac{-172.5}{50} = -3.45 $$\nThe best-fit slope of the calibration curve is $m = -3.45$.\n\nFinally, we use the derived relationship between $m$ and $E$ to calculate the qPCR efficiency.\n$$ m = -3.45 = -\\frac{1}{\\log_{10}(1+E)} $$\nSolving for $\\log_{10}(1+E)$:\n$$ \\log_{10}(1+E) = \\frac{-1}{m} = \\frac{-1}{-3.45} = \\frac{1}{3.45} $$\nTo find $E$, we first take the antilogarithm (power of $10$):\n$$ 1+E = 10^{(1/3.45)} $$\nThen, we solve for $E$:\n$$ E = 10^{(1/3.45)} - 1 $$\nNow we compute the numerical value:\n$$ \\frac{1}{3.45} \\approx 0.28985507 $$\n$$ 1+E \\approx 10^{0.28985507} \\approx 1.94916005 $$\n$$ E \\approx 1.94916005 - 1 = 0.94916005 $$\nThe problem requires the final answer for $E$ to be rounded to four significant figures.\n$$ E \\approx 0.9492 $$\nThe qPCR efficiency is approximately $0.9492$, or $94.92\\%$.", "answer": "$$\\boxed{0.9492}$$", "id": "5231553"}, {"introduction": "The ultimate power of MRD testing lies in monitoring a patient's response to therapy over time. A single measurement is a snapshot, but a series of measurements reveals a trajectory that can inform clinical decisions. In this final practice, you will model longitudinal MRD data using an exponential decay model, estimate the key parameters of the patient's response, and use this model to forecast the future risk of relapse [@problem_id:5231541]. This exercise demonstrates how quantitative lab data can be transformed into a powerful, personalized predictive tool.", "problem": "You are given a measurement model for Minimal Residual Disease (MRD) in laboratory diagnostics. Minimal Residual Disease (MRD) is the fraction of malignant cells remaining after treatment and is often modeled to decay exponentially over time under effective therapy. Assume that the true underlying MRD fraction follows exponential decay, and measurements are affected by independent, additive Gaussian noise on the natural logarithm scale. Formally, for observation times $t_i$ in days and measured MRD fractions $y_i$ (dimensionless decimals), the model is\n$$\n\\ln(y_i) = a - k t_i + \\varepsilon_i,\n$$\nwhere $a = \\ln(y_0)$ is the logarithm of the initial MRD fraction, $k$ is the decay rate in day$^{-1}$ (which may be negative if MRD increases), and $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ are independent and identically distributed Gaussian errors.\n\nStarting from the core definitions of the Gaussian likelihood and the exponential decay model, derive the maximum likelihood estimators for $a$, $k$, and $\\sigma^2$ under the assumption of independent Gaussian noise on $\\ln(y_i)$. Then, use these estimators to compute the relapse risk at a future time $t_{\\mathrm{future}}$ defined as the probability that the next observed MRD measurement exceeds a clinical threshold $y_{\\mathrm{thr}}$, meaning\n$$\n\\text{risk} = \\mathbb{P} \\left( \\ln(Y_{\\mathrm{future}}) > \\ln(y_{\\mathrm{thr}}) \\right),\n$$\nwhere $Y_{\\mathrm{future}}$ denotes the future observed MRD at time $t_{\\mathrm{future}}$. Under the Gaussian noise assumption, the predictive distribution for $\\ln(Y_{\\mathrm{future}})$ is normal with mean $\\hat{\\mu} = \\hat{a} - \\hat{k} \\, t_{\\mathrm{future}}$ and variance $\\hat{\\sigma}^2$, so express the risk in terms of the cumulative distribution function of the standard normal distribution. Assume independence of errors and ignore parameter estimation uncertainty when forming the predictive distribution.\n\nYour task is to write a complete, runnable program that:\n- Implements the maximum likelihood estimation of $a$, $k$, and $\\sigma^2$ from the data $\\{(t_i,y_i)\\}_{i=1}^n$.\n- Computes the relapse risk at $t_{\\mathrm{future}}$ for a given threshold $y_{\\mathrm{thr}}$ using the predictive normal model on $\\ln(Y_{\\mathrm{future}})$.\n- Handles the edge case $\\hat{\\sigma} = 0$ by returning a deterministic risk: $1$ if $\\hat{\\mu} > \\ln(y_{\\mathrm{thr}})$ and $0$ otherwise.\n\nPhysical units and numerical requirements:\n- Time $t_i$ and $t_{\\mathrm{future}}$ must be treated in days.\n- MRD fractions $y_i$ and $y_{\\mathrm{thr}}$ must be treated as dimensionless decimals (not percentages).\n- The program must output the relapse risk values as decimal floats rounded to six decimal places.\n\nTest Suite:\nUse the following four test cases, each a tuple $(\\text{times}, \\text{observed MRD fractions}, t_{\\mathrm{future}}, y_{\\mathrm{thr}})$:\n1. Case A (general decay, moderate noise): times $[0,30,60,90]$, MRD fractions $[0.0105,0.0060,0.0028,0.0017]$, $t_{\\mathrm{future}}=120$, $y_{\\mathrm{thr}}=0.0010$.\n2. Case B (boundary with two points yielding near-deterministic fit): times $[0,60]$, MRD fractions $[0.0010,0.0009]$, $t_{\\mathrm{future}}=30$, $y_{\\mathrm{thr}}=0.0008$.\n3. Case C (rising MRD indicating potential relapse): times $[0,30,60,90]$, MRD fractions $[0.0008,0.0010,0.0014,0.0022]$, $t_{\\mathrm{future}}=120$, $y_{\\mathrm{thr}}=0.0015$.\n4. Case D (near detection limit, high noise): times $[0,90,180]$, MRD fractions $[0.000010,0.000012,0.000009]$, $t_{\\mathrm{future}}=270$, $y_{\\mathrm{thr}}=0.000010$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). Each result must be the relapse risk for the corresponding test case, rounded to six decimal places.", "solution": "The problem requires the derivation of maximum likelihood estimators (MLEs) for the parameters of an exponential decay model with additive Gaussian noise on the logarithm of the measured quantity, and the subsequent application of these estimators to calculate a relapse risk probability.\n\nThe model for the measured Minimal Residual Disease (MRD) fraction, $y_i$, at time $t_i$ is given by\n$$\n\\ln(y_i) = a - k t_i + \\varepsilon_i\n$$\nwhere $a = \\ln(y_0)$ is the logarithm of the initial MRD fraction, $k$ is the decay rate, and $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ are independent and identically distributed (i.i.d.) Gaussian errors.\n\nTo simplify the notation, let $z_i = \\ln(y_i)$. The model can be expressed as a simple linear regression of $z_i$ on $t_i$:\n$$\nz_i = a - k t_i + \\varepsilon_i\n$$\nThe parameters to be estimated are the intercept $a$, the rate $k$, and the error variance $\\sigma^2$.\n\n**1. Maximum Likelihood Estimation of $a$ and $k$**\n\nThe probability density function (PDF) for a single observation $z_i$, given the parameters, is that of a normal distribution with mean $\\mu_i = a - k t_i$ and variance $\\sigma^2$:\n$$\nf(z_i | a, k, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(z_i - (a - k t_i))^2}{2\\sigma^2} \\right)\n$$\nAssuming the $n$ observations are independent, the likelihood function $L$ for the entire dataset $\\{ (t_i, z_i) \\}_{i=1}^n$ is the product of the individual PDFs:\n$$\nL(a, k, \\sigma^2) = \\prod_{i=1}^{n} f(z_i | a, k, \\sigma^2) = \\left( \\frac{1}{2\\pi\\sigma^2} \\right)^{n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (z_i - a + k t_i)^2 \\right)\n$$\nMaximizing the likelihood is equivalent to maximizing its natural logarithm, the log-likelihood function $\\ell = \\ln(L)$:\n$$\n\\ell(a, k, \\sigma^2) = -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (z_i - a + k t_i)^2\n$$\nTo find the MLEs for $a$ and $k$, we take the partial derivatives of $\\ell$ with respect to $a$ and $k$ and set them to zero. This is equivalent to minimizing the sum of squared errors (or residuals), $S(a, k) = \\sum_{i=1}^{n} (z_i - a + k t_i)^2$.\n$$\n\\frac{\\partial S}{\\partial a} = \\sum_{i=1}^{n} 2(z_i - a + k t_i)(-1) = -2\\left(\\sum_{i=1}^{n} z_i - na + k\\sum_{i=1}^{n} t_i\\right) = 0\n$$\n$$\n\\frac{\\partial S}{\\partial k} = \\sum_{i=1}^{n} 2(z_i - a + k t_i)(t_i) = 2\\left(\\sum_{i=1}^{n} z_i t_i - a\\sum_{i=1}^{n} t_i + k\\sum_{i=1}^{n} t_i^2\\right) = 0\n$$\nThese yield the normal equations for the estimators $\\hat{a}$ and $\\hat{k}$:\n$$\n\\sum z_i - n\\hat{a} + \\hat{k}\\sum t_i = 0\n$$\n$$\n\\sum z_i t_i - \\hat{a}\\sum t_i + \\hat{k}\\sum t_i^2 = 0\n$$\nFrom the first equation, defining the sample means $\\bar{t} = \\frac{1}{n}\\sum t_i$ and $\\bar{z} = \\frac{1}{n}\\sum z_i$:\n$$\nn\\bar{z} - n\\hat{a} + n\\hat{k}\\bar{t} = 0 \\implies \\hat{a} = \\bar{z} + \\hat{k}\\bar{t}\n$$\nSubstituting this expression for $\\hat{a}$ into the second normal equation gives the solution for $\\hat{k}$:\n$$\n\\sum z_i t_i - (\\bar{z} + \\hat{k}\\bar{t})\\sum t_i + \\hat{k}\\sum t_i^2 = 0\n$$\n$$\n\\hat{k}\\left(\\sum t_i^2 - \\bar{t}\\sum t_i\\right) = \\sum z_i t_i - \\bar{z}\\sum t_i\n$$\nRecognizing that $\\sum t_i^2 - \\bar{t}\\sum t_i = \\sum (t_i - \\bar{t})^2$ and $\\sum z_i t_i - \\bar{z}\\sum t_i = \\sum (z_i - \\bar{z})(t_i-\\bar{t})$, we get:\n$$\n\\hat{k} = \\frac{\\sum (z_i - \\bar{z})(t_i - \\bar{t})}{\\sum (t_i - \\bar{t})^2}\n$$\nNote that in the conventional linear model $z_i = \\beta_0 + \\beta_1 t_i$, the slope is $\\beta_1$. Here, the coefficient of $t_i$ is $-k$, so $\\beta_1 = -k$. The standard OLS estimator for the slope is $\\hat{\\beta}_1 = \\frac{\\sum (t_i - \\bar{t})(z_i - \\bar{z})}{\\sum (t_i - \\bar{t})^2}$. Therefore, the MLE for $k$ is:\n$$\n\\hat{k} = - \\frac{\\sum_{i=1}^{n} (t_i - \\bar{t})(z_i - \\bar{z})}{\\sum_{i=1}^{n} (t_i - \\bar{t})^2}\n$$\nAnd the MLE for $a$ is:\n$$\n\\hat{a} = \\bar{z} - (-\\hat{k})\\bar{t} = \\bar{z} + \\hat{k}\\bar{t}\n$$\n\n**2. Maximum Likelihood Estimation of $\\sigma^2$**\n\nTo find the MLE for $\\sigma^2$, we differentiate the log-likelihood function $\\ell$ with respect to $\\sigma^2$ and set the derivative to zero, substituting the MLEs $\\hat{a}$ and $\\hat{k}$:\n$$\n\\frac{\\partial \\ell}{\\partial(\\sigma^2)} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (z_i - a + k t_i)^2 = 0\n$$\n$$\n-\\frac{n}{2\\hat{\\sigma}^2} + \\frac{1}{2(\\hat{\\sigma}^2)^2} \\sum_{i=1}^{n} (z_i - \\hat{a} + \\hat{k} t_i)^2 = 0\n$$\nSolving for $\\hat{\\sigma}^2$:\n$$\n\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (z_i - (\\hat{a} - \\hat{k} t_i))^2\n$$\nThis is the mean of the squared residuals.\n\n**3. Relapse Risk Calculation**\n\nThe relapse risk at a future time $t_{\\mathrm{future}}$ is defined as the probability that a future measurement $Y_{\\mathrm{future}}$ exceeds a threshold $y_{\\mathrm{thr}}$:\n$$\n\\text{risk} = \\mathbb{P} \\left( \\ln(Y_{\\mathrm{future}}) > \\ln(y_{\\mathrm{thr}}) \\right)\n$$\nThe problem states that the predictive distribution for $\\ln(Y_{\\mathrm{future}})$ is normal, with mean given by the regression line and variance given by the estimated error variance (ignoring parameter uncertainty):\n$$\n\\ln(Y_{\\mathrm{future}}) \\sim \\mathcal{N}(\\hat{\\mu}, \\hat{\\sigma}^2)\n$$\nwhere $\\hat{\\mu} = \\hat{a} - \\hat{k} t_{\\mathrm{future}}$ and $\\hat{\\sigma}^2$ is the MLE derived above. Let $Z_{\\mathrm{future}} = \\ln(Y_{\\mathrm{future}})$ and $z_{\\mathrm{thr}} = \\ln(y_{\\mathrm{thr}})$. We need to calculate $\\mathbb{P}(Z_{\\mathrm{future}} > z_{\\mathrm{thr}})$.\n\nTo evaluate this probability, we standardize the random variable $Z_{\\mathrm{future}}$. Let $W = (Z_{\\mathrm{future}} - \\hat{\\mu})/\\hat{\\sigma}$, which follows a standard normal distribution $\\mathcal{N}(0,1)$. The probability becomes:\n$$\n\\text{risk} = \\mathbb{P}\\left( \\frac{Z_{\\mathrm{future}} - \\hat{\\mu}}{\\hat{\\sigma}} > \\frac{z_{\\mathrm{thr}} - \\hat{\\mu}}{\\hat{\\sigma}} \\right) = \\mathbb{P}\\left( W > \\frac{z_{\\mathrm{thr}} - \\hat{\\mu}}{\\hat{\\sigma}} \\right)\n$$\nLet $\\Phi(\\cdot)$ be the cumulative distribution function (CDF) of the standard normal distribution. Then $\\mathbb{P}(W > x) = 1 - \\mathbb{P}(W \\le x) = 1 - \\Phi(x)$. Thus, the risk is:\n$$\n\\text{risk} = 1 - \\Phi\\left( \\frac{\\ln(y_{\\mathrm{thr}}) - (\\hat{a} - \\hat{k} t_{\\mathrm{future}})}{\\hat{\\sigma}} \\right)\n$$\nIn the special case where $\\hat{\\sigma} = 0$, the predictive distribution is a point mass at $\\hat{\\mu}$. The probability is deterministic: if $\\hat{\\mu} > \\ln(y_{\\mathrm{thr}})$, the risk is $1$; otherwise, it is $0$. This occurs when the data points fall perfectly on a line, for instance, with $n=2$ observations.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Implements the full solution pipeline for MRD relapse risk calculation.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (times, observed MRD fractions, t_future, y_thr)\n    test_cases = [\n        # Case A (general decay, moderate noise)\n        (\n            [0, 30, 60, 90],\n            [0.0105, 0.0060, 0.0028, 0.0017],\n            120,\n            0.0010,\n        ),\n        # Case B (boundary with two points yielding near-deterministic fit)\n        (\n            [0, 60],\n            [0.0010, 0.0009],\n            30,\n            0.0008,\n        ),\n        # Case C (rising MRD indicating potential relapse)\n        (\n            [0, 30, 60, 90],\n            [0.0008, 0.0010, 0.0014, 0.0022],\n            120,\n            0.0015,\n        ),\n        # Case D (near detection limit, high noise)\n        (\n            [0, 90, 180],\n            [0.000010, 0.000012, 0.000009],\n            270,\n            0.000010,\n        ),\n    ]\n\n    results = []\n    for case in test_cases:\n        times, mrd_fractions, t_future, y_thr = case\n\n        # Convert to numpy arrays for vectorized calculations\n        t = np.array(times, dtype=np.float64)\n        y = np.array(mrd_fractions, dtype=np.float64)\n        \n        # Transform the model to a linear form: z = a - k*t\n        # where z = ln(y)\n        z = np.log(y)\n        n = len(t)\n        \n        # Calculate maximum likelihood estimators for a, k, and sigma^2\n        # These are equivalent to ordinary least squares (OLS) estimators\n        \n        # Calculate sample means\n        t_mean = np.mean(t)\n        z_mean = np.mean(z)\n        \n        # Calculate k_hat\n        # Using the covariance/variance formula for the slope of the regression of z on t\n        # The slope is -k.\n        # Handle the case where var_t is zero to avoid division by zero,\n        # although this won't happen for the given test cases.\n        var_t = np.sum((t - t_mean)**2)\n        if np.isclose(var_t, 0):\n             # When all time points are the same, k is undefined.\n             # This scenario is not physically meaningful for this problem.\n             # We can set k to 0 and proceed, as there's no trend.\n            k_hat = 0.0\n        else:\n            cov_tz = np.sum((t - t_mean) * (z - z_mean))\n            k_hat = -cov_tz / var_t\n\n        # Calculate a_hat\n        a_hat = z_mean + k_hat * t_mean\n        \n        # Calculate the predicted z values and residuals\n        z_predicted = a_hat - k_hat * t\n        residuals = z - z_predicted\n        \n        # Calculate sigma_hat (MLE for standard deviation)\n        # The MLE for variance is the mean of squared residuals.\n        sigma_sq_hat = np.mean(residuals**2)\n        sigma_hat = np.sqrt(sigma_sq_hat)\n        \n        # Calculate the relapse risk\n        \n        # Predicted mean for ln(Y_future)\n        mu_hat_future = a_hat - k_hat * t_future\n        \n        # Log of the clinical threshold\n        z_thr = np.log(y_thr)\n        \n        # Handle edge case where sigma_hat is zero (perfect fit)\n        if np.isclose(sigma_hat, 0):\n            risk = 1.0 if mu_hat_future > z_thr else 0.0\n        else:\n            # Standardize the variable to use the standard normal CDF\n            arg = (z_thr - mu_hat_future) / sigma_hat\n            # The risk is P(Z > z_thr) = 1 - P(Z <= z_thr)\n            # which is the survival function (sf) of the normal distribution.\n            risk = norm.sf(arg)\n\n        results.append(f\"{risk:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "5231541"}]}