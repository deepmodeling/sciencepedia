{"hands_on_practices": [{"introduction": "Many machine learning models provide a continuous risk score rather than a simple 'positive' or 'negative' result. To translate this score into a clinically actionable decision, a specific cutoff threshold must be chosen. This exercise [@problem_id:5207994] guides you through the fundamental process of evaluating a classifier at different thresholds by calculating its sensitivity and specificity. You will then use Youden's $J$ index, a common metric for summarizing diagnostic performance, to identify an 'optimal' threshold and critically examine the assumptions and limitations of this approach.", "problem": "A hospital laboratory is piloting an Artificial Intelligence (AI) and Machine Learning (ML) classifier to assist clinical interpretation of a continuous laboratory risk score for acute kidney injury. In a prospective validation cohort, there are $N^{+} = 120$ patients with confirmed disease and $N^{-} = 180$ patients without disease. The classifier outputs a calibrated risk score in $[0,1]$, and a decision threshold $t$ converts scores to a binary positive/negative result.\n\nFor each candidate threshold $t \\in \\{0.30, 0.50, 0.70, 0.85\\}$, the number of patients predicted positive (i.e., classifier output $\\geq t$) among the diseased ($N^{+}$) and non-diseased ($N^{-}$) groups is recorded as follows:\n- At $t = 0.30$: predicted positive among diseased $= 110$; predicted positive among non-diseased $= 70$.\n- At $t = 0.50$: predicted positive among diseased $= 96$; predicted positive among non-diseased $= 36$.\n- At $t = 0.70$: predicted positive among diseased $= 80$; predicted positive among non-diseased $= 20$.\n- At $t = 0.85$: predicted positive among diseased $= 60$; predicted positive among non-diseased $= 10$.\n\nUsing only foundational definitions of sensitivity and specificity from the confusion matrix, compute, for each threshold $t$, the sensitivity $Se(t)$ and specificity $Sp(t)$, then compute Youden’s index $J(t)$ from these quantities. Identify the single threshold $t$ that maximizes $J(t)$. In your reasoning, articulate the assumptions under which maximizing $J(t)$ is appropriate for clinical decision-making in laboratory diagnostics, and explain at least two limitations of this criterion in practice.\n\nReport the final answer as the value of the threshold $t$ that maximizes $J(t)$. Do not include any units. If your computations produce exact decimal values, do not round them.", "solution": "The problem requires the evaluation of a machine learning classifier for acute kidney injury at four different decision thresholds. We must first validate the problem statement.\n\nThe givens are:\n- The number of patients with confirmed disease (positives): $N^{+} = 120$.\n- The number of patients without disease (negatives): $N^{-} = 180$.\n- The set of candidate decision thresholds: $t \\in \\{0.30, 0.50, 0.70, 0.85\\}$.\n- The number of true positives ($TP$) and false positives ($FP$) at each threshold:\n  - For $t = 0.30$: $TP(0.30) = 110$, $FP(0.30) = 70$.\n  - For $t = 0.50$: $TP(0.50) = 96$, $FP(0.50) = 36$.\n  - For $t = 0.70$: $TP(0.70) = 80$, $FP(0.70) = 20$.\n  - For $t = 0.85$: $TP(0.85) = 60$, $FP(0.85) = 10$.\n\nThe problem is scientifically grounded, well-posed, objective, and contains complete and consistent data. It uses standard metrics from diagnostic medicine and statistics. The number of predicted positives for each class does not exceed the total number in that class. Furthermore, as the threshold $t$ increases, the number of samples classified as positive (both $TP$ and $FP$) non-increases, which is a necessary property of a threshold-based classifier. The problem is therefore deemed valid.\n\nWe proceed with the solution by first defining the necessary performance metrics based on the confusion matrix. The elements of the confusion matrix for a given threshold $t$ are:\n- True Positives ($TP(t)$): Patients with the disease correctly classified as positive.\n- False Positives ($FP(t)$): Patients without the disease incorrectly classified as positive.\n- False Negatives ($FN(t)$): Patients with the disease incorrectly classified as negative.\n- True Negatives ($TN(t)$): Patients without the disease correctly classified as negative.\n\nThe total number of diseased and non-diseased patients are $N^{+} = TP(t) + FN(t)$ and $N^{-} = FP(t) + TN(t)$, respectively. From these, we can calculate $FN(t) = N^{+} - TP(t)$ and $TN(t) = N^{-} - FP(t)$.\n\nThe primary metrics are defined as:\n- Sensitivity ($Se(t)$), or True Positive Rate: The proportion of diseased patients correctly identified.\n$$Se(t) = \\frac{TP(t)}{N^{+}} = \\frac{TP(t)}{TP(t) + FN(t)}$$\n- Specificity ($Sp(t)$), or True Negative Rate: The proportion of non-diseased patients correctly identified.\n$$Sp(t) = \\frac{TN(t)}{N^{-}} = \\frac{TN(t)}{TN(t) + FP(t)}$$\n- Youden's Index ($J(t)$), or the Youden's J statistic: A metric that summarizes the diagnostic ability of a test. It is the sum of sensitivity and specificity minus one.\n$$J(t) = Se(t) + Sp(t) - 1$$\nWe will now compute these values for each given threshold.\n\nFor $t = 0.30$:\n- Given: $TP(0.30) = 110$, $FP(0.30) = 70$.\n- $FN(0.30) = N^{+} - TP(0.30) = 120 - 110 = 10$.\n- $TN(0.30) = N^{-} - FP(0.30) = 180 - 70 = 110$.\n- $Se(0.30) = \\frac{110}{120} = \\frac{11}{12}$.\n- $Sp(0.30) = \\frac{110}{180} = \\frac{11}{18}$.\n- $J(0.30) = \\frac{11}{12} + \\frac{11}{18} - 1 = \\frac{33 + 22}{36} - 1 = \\frac{55}{36} - \\frac{36}{36} = \\frac{19}{36}$.\n\nFor $t = 0.50$:\n- Given: $TP(0.50) = 96$, $FP(0.50) = 36$.\n- $FN(0.50) = N^{+} - TP(0.50) = 120 - 96 = 24$.\n- $TN(0.50) = N^{-} - FP(0.50) = 180 - 36 = 144$.\n- $Se(0.50) = \\frac{96}{120} = \\frac{4 \\times 24}{5 \\times 24} = \\frac{4}{5}$.\n- $Sp(0.50) = \\frac{144}{180} = \\frac{4 \\times 36}{5 \\times 36} = \\frac{4}{5}$.\n- $J(0.50) = \\frac{4}{5} + \\frac{4}{5} - 1 = \\frac{8}{5} - 1 = \\frac{3}{5}$.\n\nFor $t = 0.70$:\n- Given: $TP(0.70) = 80$, $FP(0.70) = 20$.\n- $FN(0.70) = N^{+} - TP(0.70) = 120 - 80 = 40$.\n- $TN(0.70) = N^{-} - FP(0.70) = 180 - 20 = 160$.\n- $Se(0.70) = \\frac{80}{120} = \\frac{2}{3}$.\n- $Sp(0.70) = \\frac{160}{180} = \\frac{8}{9}$.\n- $J(0.70) = \\frac{2}{3} + \\frac{8}{9} - 1 = \\frac{6 + 8}{9} - 1 = \\frac{14}{9} - \\frac{9}{9} = \\frac{5}{9}$.\n\nFor $t = 0.85$:\n- Given: $TP(0.85) = 60$, $FP(0.85) = 10$.\n- $FN(0.85) = N^{+} - TP(0.85) = 120 - 60 = 60$.\n- $TN(0.85) = N^{-} - FP(0.85) = 180 - 10 = 170$.\n- $Se(0.85) = \\frac{60}{120} = \\frac{1}{2}$.\n- $Sp(0.85) = \\frac{170}{180} = \\frac{17}{18}$.\n- $J(0.85) = \\frac{1}{2} + \\frac{17}{18} - 1 = \\frac{9 + 17}{18} - 1 = \\frac{26}{18} - \\frac{18}{18} = \\frac{8}{18} = \\frac{4}{9}$.\n\nNow, we compare the values of $J(t)$ to find the maximum. To facilitate comparison, we convert the fractions to a common denominator of $180$:\n- $J(0.30) = \\frac{19}{36} = \\frac{19 \\times 5}{36 \\times 5} = \\frac{95}{180}$.\n- $J(0.50) = \\frac{3}{5} = \\frac{3 \\times 36}{5 \\times 36} = \\frac{108}{180}$.\n- $J(0.70) = \\frac{5}{9} = \\frac{5 \\times 20}{9 \\times 20} = \\frac{100}{180}$.\n- $J(0.85) = \\frac{4}{9} = \\frac{4 \\times 20}{9 \\times 20} = \\frac{80}{180}$.\n\nComparing the numerators ($80  95  100  108$), the maximum value of Youden's Index is $J(0.50) = \\frac{108}{180} = \\frac{3}{5}$. Therefore, the threshold that maximizes $J(t)$ is $t=0.50$.\n\nDiscussion of Assumptions and Limitations:\n\nMaximizing Youden's Index $J(t) = Se(t) + Sp(t) - 1$ is appropriate for clinical decision-making under a key assumption: that the utility of the test is maximized when sensitivity and specificity are given equal weight. This implies that the cost or harm associated with a false negative (missing a true case of disease) is considered equal to the cost or harm of a false positive (incorrectly labeling a healthy individual as diseased). This criterion is thus optimal when the objective is to maximize the overall number of correctly classified patients (both diseased and non-diseased), without prioritizing one type of error over the other.\n\nHowever, this approach has at least two significant limitations in clinical practice:\n1.  **Assumption of Equal Misclassification Costs:** The core assumption that false negatives and false positives have equal consequences is rarely true in medicine. For a serious condition like acute kidney injury, a false negative ($FN$) could lead to delayed or missed treatment, resulting in severe morbidity (e.g., progression to chronic kidney disease, need for dialysis) or mortality. A false positive ($FP$), while causing patient anxiety and potentially leading to unnecessary follow-up tests and costs, generally has a much lower associated harm. A threshold chosen by maximizing Youden's index may be suboptimal and even dangerous if the cost of an $FN$ is substantially higher than the cost of an $FP$. Decision-analytic methods that explicitly model these differing costs are often more appropriate.\n2.  **Independence from Disease Prevalence:** Youden's index is calculated from sensitivity and specificity, which are intrinsic properties of the test and are independent of the prevalence of the disease in the population being tested. However, the optimal decision threshold for clinical application *should* be influenced by prevalence. For instance, in a low-prevalence setting, even a test with high specificity will generate a large number of false positives, leading to a low Positive Predictive Value ($PPV$). A clinician might therefore prefer a higher threshold to increase specificity and improve the $PPV$. Conversely, in a high-prevalence setting, a lower threshold might be acceptable to maximize sensitivity. Youden's index does not account for this context, and the threshold it identifies may not optimize the test's utility in a specific target population.\n\nIn summary, despite its mathematical simplicity, maximizing Youden's index is a-contextual and often fails to reflect the complex trade-offs inherent in clinical decision-making.", "answer": "$$\\boxed{0.50}$$", "id": "5207994"}, {"introduction": "While analyzing performance at specific thresholds is crucial, it is often desirable to have a single metric that summarizes a model's overall discriminative ability across all possible thresholds. This is the role of the Area Under the Curve (AUC). However, the choice of which curve to use—the Receiver Operating Characteristic (ROC) or the Precision-Recall (PR)—has profound implications, especially in clinical settings where diseases are often rare. This practice [@problem_id:5207923] challenges you to compute the PR AUC and articulate why it provides a more realistic performance assessment than ROC AUC on imbalanced datasets.", "problem": "A clinical laboratory is validating a machine learning classifier to screen for a rare autoantibody in a referral population with disease prevalence $\\pi = 0.02$. The model outputs continuous scores that are thresholded to produce binary calls. From an internal validation set, the following precision–recall operating points were obtained by sweeping the decision threshold (reported as ordered pairs of recall $R$ and precision $P$ in order of increasing recall):\n- $(R,P) = (0, 1.0)$\n- $(R,P) = (0.10, 0.60)$\n- $(R,P) = (0.30, 0.50)$\n- $(R,P) = (0.55, 0.40)$\n- $(R,P) = (0.80, 0.30)$\n- $(R,P) = (1.00, 0.20)$\n\nAssume that between each consecutive pair of points, the precision–recall curve is linearly interpolated as a function $P(R)$ over $R \\in [0,1]$, and adopt the standard definition of the area under the precision–recall curve as the integral of $P(R)$ with respect to $R$ over $[0,1]$.\n\nTasks:\n1. Using only the information above, compute the Area Under the Precision–Recall Curve (PR AUC). Give your answer as a decimal number rounded to four significant figures.\n2. Using first principles of diagnostic test metrics, briefly justify why the Precision–Recall Area Under the Curve (PR AUC) is more informative than the Receiver Operating Characteristic Area Under the Curve (ROC AUC) for rare disease screening when $\\pi$ is small. No calculation is required for this justification; base your reasoning on the definitions of precision, recall, false positive rate, and the role of prevalence in posterior probabilities.\n\nOnly the numeric value for part 1 will be graded; express it as a pure number without a percentage sign or units.", "solution": "The problem has been validated and is deemed sound, well-posed, and objective. It is based on established principles of machine learning model evaluation and provides all necessary information for a complete solution.\n\nThe problem asks for two tasks: first, to compute the Area Under the Precision-Recall Curve (PR AUC) from a given set of operating points, and second, to provide a conceptual justification for the utility of PR AUC over ROC AUC in the context of rare disease screening.\n\n**Part 1: Computation of the Area Under the Precision–Recall Curve (PR AUC)**\n\nThe problem specifies that the precision-recall curve, denoted as a function $P(R)$, is formed by linear interpolation between a set of given operating points $(R_i, P_i)$. The Area Under the Curve (AUC) is defined by the integral $\\int_0^1 P(R) dR$. With piecewise linear interpolation, this integral can be calculated by summing the areas of the trapezoids formed between each consecutive pair of points.\n\nThe given operating points are:\n- $(R_0, P_0) = (0, 1.0)$\n- $(R_1, P_1) = (0.10, 0.60)$\n- $(R_2, P_2) = (0.30, 0.50)$\n- $(R_3, P_3) = (0.55, 0.40)$\n- $(R_4, P_4) = (0.80, 0.30)$\n- $(R_5, P_5) = (1.00, 0.20)$\n\nThe area of a trapezoid between points $(R_i, P_i)$ and $(R_{i+1}, P_{i+1})$ is given by the formula:\n$$ A_i = \\frac{1}{2} (P_i + P_{i+1}) (R_{i+1} - R_i) $$\n\nWe compute the area for each of the five segments defined by the six points:\n\nSegment 1: from $(0, 1.0)$ to $(0.10, 0.60)$\n$$ A_1 = \\frac{1}{2} (1.0 + 0.60) (0.10 - 0) = \\frac{1}{2} (1.60)(0.10) = 0.08 $$\n\nSegment 2: from $(0.10, 0.60)$ to $(0.30, 0.50)$\n$$ A_2 = \\frac{1}{2} (0.60 + 0.50) (0.30 - 0.10) = \\frac{1}{2} (1.10)(0.20) = 0.11 $$\n\nSegment 3: from $(0.30, 0.50)$ to $(0.55, 0.40)$\n$$ A_3 = \\frac{1}{2} (0.50 + 0.40) (0.55 - 0.30) = \\frac{1}{2} (0.90)(0.25) = 0.1125 $$\n\nSegment 4: from $(0.55, 0.40)$ to $(0.80, 0.30)$\n$$ A_4 = \\frac{1}{2} (0.40 + 0.30) (0.80 - 0.55) = \\frac{1}{2} (0.70)(0.25) = 0.0875 $$\n\nSegment 5: from $(0.80, 0.30)$ to $(1.00, 0.20)$\n$$ A_5 = \\frac{1}{2} (0.30 + 0.20) (1.00 - 0.80) = \\frac{1}{2} (0.50)(0.20) = 0.05 $$\n\nThe total PR AUC is the sum of these individual areas:\n$$ \\text{PR AUC} = A_1 + A_2 + A_3 + A_4 + A_5 $$\n$$ \\text{PR AUC} = 0.08 + 0.11 + 0.1125 + 0.0875 + 0.05 $$\n$$ \\text{PR AUC} = 0.19 + 0.2000 + 0.05 = 0.44 $$\nThe problem requires the answer to be rounded to four significant figures. The calculated value is $0.44$, which can be expressed as $0.4400$.\n\n**Part 2: Justification for PR AUC over ROC AUC in Rare Disease Screening**\n\nTo justify why the PR AUC is more informative than the ROC AUC for rare diseases (small prevalence $\\pi$), we must examine the definitions of the metrics involved. Let $D^+$ denote the presence of disease and $T^+$ denote a positive test result.\n\nThe metrics for a Receiver Operating Characteristic (ROC) curve are:\n- **True Positive Rate (TPR)**, also known as Recall ($R$) or Sensitivity: $R = \\text{TPR} = P(T^+|D^+)$. This is the fraction of actual positives correctly identified.\n- **False Positive Rate (FPR)**: $\\text{FPR} = P(T^+|\\neg D^+)$. This is the fraction of actual negatives incorrectly identified as positive.\n\nAn ROC curve plots TPR versus FPR. Both metrics are conditioned on the true disease status. Consequently, the ROC curve and its area (ROC AUC) are invariant to the disease prevalence, $\\pi = P(D^+)$.\n\nThe metrics for a Precision-Recall (PR) curve are:\n- **Recall (R)**, as defined above.\n- **Precision (P)**, also known as Positive Predictive Value (PPV): $P = P(D^+|T^+)$. This is the fraction of positive test results that are correct.\n\nPrecision is a posterior probability, and its relationship with prevalence can be made explicit using Bayes' theorem:\n$$ P(D^+|T^+) = \\frac{P(T^+|D^+)P(D^+)}{P(T^+)} $$\nThe denominator, $P(T^+)$, can be expanded using the law of total probability:\n$$ P(T^+) = P(T^+|D^+)P(D^+) + P(T^+|\\neg D^+)P(\\neg D^+) $$\nSubstituting the definitions of TPR, FPR, and prevalence $\\pi$:\n$$ P = \\frac{\\text{TPR} \\cdot \\pi}{\\text{TPR} \\cdot \\pi + \\text{FPR} \\cdot (1 - \\pi)} $$\nThis equation demonstrates that Precision is fundamentally dependent on prevalence $\\pi$.\n\nIn the context of a rare disease, $\\pi$ is very small, and thus $(1 - \\pi)$ is close to $1$. The population of non-diseased individuals is vastly larger than the population of diseased individuals. Consequently, even a very small FPR can result in a large absolute number of false positives ($FP$) that can easily outnumber the true positives ($TP$). This has a profound impact on Precision. For example, if $\\pi = 0.02$, a classifier with a high TPR of $0.9$ and a low FPR of $0.05$ would have a Precision of:\n$$ P = \\frac{0.9 \\cdot 0.02}{0.9 \\cdot 0.02 + 0.05 \\cdot 0.98} = \\frac{0.018}{0.018 + 0.049} \\approx 0.2687 $$\nDespite yielding an excellent operating point on an ROC curve, fewer than $27\\%$ of the positive classifications would be correct.\n\nThe ROC AUC, being insensitive to prevalence, can provide a misleadingly optimistic assessment of a classifier's performance in a rare disease setting. It measures the ability to distinguish between positive and negative classes without regard to their proportions in the population. The PR AUC, in contrast, directly incorporates the effect of prevalence through the Precision metric. It effectively evaluates the performance on the minority (positive) class, which is often the primary interest in screening applications. A sharp drop in the PR curve immediately signals that the classifier generates an excess of false positives relative to true positives at a given recall level, a critical piece of information that is obscured in ROC space. Therefore, for assessing classifiers on imbalanced datasets, such as in rare disease screening, the PR AUC is a more informative and relevant metric than the ROC AUC.", "answer": "$$\\boxed{0.4400}$$", "id": "5207923"}, {"introduction": "The performance metrics you calculate, such as AUC or Brier score, are point estimates derived from a finite sample of data. To build robust and trustworthy diagnostic tools, we must also quantify the uncertainty around these estimates. This advanced, hands-on coding practice [@problem_id:5208017] introduces the nonparametric bootstrap, a powerful computational statistics technique. You will implement a complete workflow to resample a training dataset, refit a model multiple times, and generate confidence intervals for its performance, giving you a truer picture of the model's reliability.", "problem": "You are tasked with implementing a complete, runnable program that quantifies uncertainty in binary diagnostic model performance via nonparametric bootstrap resampling of the training data and constructs percentile intervals for two metrics: the Area Under the Receiver Operating Characteristic Curve (AUC) and the Brier score. The intended application is laboratory diagnostics, where a probabilistic classifier estimates the probability that a specimen has a target condition, and you must quantify uncertainty due to finite training samples. The classifier to be used is logistic regression, trained by maximum likelihood with an intercept and an $\\ell_{2}$ penalty on the coefficients excluding the intercept. The evaluation is performed on a fixed validation set for each case. All randomness must be controlled by a fixed seed.\n\nStart from the following foundational bases: (i) the definition of maximum likelihood estimation for logistic regression, where the conditional probability of class membership is $p(y=1 \\mid \\mathbf{x}; \\boldsymbol{\\beta}) = \\sigma(\\boldsymbol{\\beta}^{\\top}\\mathbf{\\tilde{x}})$ with $\\sigma(z) = \\frac{1}{1+e^{-z}}$ and $\\mathbf{\\tilde{x}}$ is the feature vector augmented with an intercept term; (ii) the bootstrap principle that approximates the sampling distribution of a statistic by resampling with replacement from the empirical distribution; (iii) the definition of the Receiver Operating Characteristic (ROC) curve and that the AUC equals the probability that a randomly chosen positive case is ranked higher than a randomly chosen negative case by the classifier score; and (iv) the Brier score defined as the mean squared error between predicted probabilities and binary outcomes.\n\nImplement the following without using any external data or user input. For each test case, do the following:\n- Train a logistic regression model on the training set by minimizing the negative penalized log-likelihood with an $\\ell_{2}$ penalty of magnitude $\\lambda$, penalizing only the non-intercept coefficients. Use a Newton–Raphson method with a fixed iteration cap and a convergence tolerance for the parameter update norm.\n- Using nonparametric bootstrap, draw $B$ bootstrap samples of size $n_{\\text{train}}$ by sampling indices with replacement from the training set. For each bootstrap sample, refit the model and compute on the fixed validation set: (a) the Area Under the Receiver Operating Characteristic Curve (AUC), and (b) the Brier score $=\\frac{1}{n_{\\text{val}}}\\sum_{i=1}^{n_{\\text{val}}}\\left(\\hat{p}_{i}-y_{i}\\right)^{2}$.\n- Construct percentile intervals at confidence level $1-\\alpha$ for both metrics using the empirical quantiles at probabilities $\\alpha/2$ and $1-\\alpha/2$ computed with linear interpolation of the cumulative distribution.\n\nUse the following definitions and constraints:\n- Binary labels are encoded as $0$ and $1$.\n- The logistic regression model uses an intercept term. Denote the augmented design matrix as $\\mathbf{X}\\in\\mathbb{R}^{n\\times(d+1)}$ with a leading column of ones. The penalty matrix places a zero on the intercept coefficient and ones on the remaining diagonal entries, so the intercept is not penalized.\n- When computing AUC on the validation set, handle ties via average ranks. If $n_{+}$ and $n_{-}$ denote the counts of positive and negative validation labels respectively, and $R_{+}$ the sum of ranks of the positive scores, then the AUC is $\\frac{R_{+}-\\frac{n_{+}(n_{+}+1)}{2}}{n_{+}n_{-}}$.\n- Clip predicted probabilities to the closed interval $[\\varepsilon, 1-\\varepsilon]$ with $\\varepsilon=10^{-12}$ for numerical stability.\n- Use a fixed random seed $s$ for all bootstrap resampling within each test case to ensure reproducibility, and do not reseed between bootstrap draws of the same test case.\n- Express the final intervals as floating-point numbers rounded to $4$ decimal places.\n\nTest suite. Implement exactly the following three test cases. For each case, you are given the training set, the validation set, the bootstrap size $B$, the penalty magnitude $\\lambda$, the confidence level via $\\alpha$, and the seed $s$.\n\nCase $\\#1$ (balanced, moderately separated):\n- Training features $\\mathbf{X}_{\\text{train}}\\in\\mathbb{R}^{12\\times 2}$ and labels $\\mathbf{y}_{\\text{train}}\\in\\{0,1\\}^{12}$:\n  - Class $0$: $(-0.5,\\,0.1)$, $(0.3,\\,-0.2)$, $(-0.7,\\,-0.8)$, $(0.2,\\,0.0)$, $(-0.3,\\,0.4)$, $(0.0,\\,-0.5)$.\n  - Class $1$: $(1.8,\\,2.2)$, $(2.5,\\,1.7)$, $(1.9,\\,1.8)$, $(2.2,\\,2.4)$, $(1.5,\\,2.0)$, $(2.0,\\,1.6)$.\n- Validation features $\\mathbf{X}_{\\text{val}}\\in\\mathbb{R}^{8\\times 2}$ and labels $\\mathbf{y}_{\\text{val}}\\in\\{0,1\\}^{8}$:\n  - Class $0$: $(-0.6,\\,0.2)$, $(0.1,\\,-0.1)$, $(-0.2,\\,-0.6)$, $(0.4,\\,0.3)$.\n  - Class $1$: $(2.1,\\,1.9)$, $(1.7,\\,2.3)$, $(2.3,\\,2.1)$, $(1.6,\\,1.8)$.\n- Parameters: $B=200$, $\\lambda=1.0$, $\\alpha=0.05$, $s=20231111$.\n\nCase $\\#2$ (imbalanced, weak separation):\n- Training features $\\mathbf{X}_{\\text{train}}\\in\\mathbb{R}^{10\\times 2}$ and labels $\\mathbf{y}_{\\text{train}}\\in\\{0,1\\}^{10}$:\n  - Class $0$: $(0.1,\\,0.2)$, $(-0.2,\\,0.0)$, $(0.0,\\,-0.1)$, $(0.3,\\,-0.2)$, $(-0.4,\\,0.1)$, $(0.2,\\,0.1)$, $(-0.1,\\,-0.3)$, $(0.1,\\,-0.2)$.\n  - Class $1$: $(0.6,\\,0.7)$, $(0.7,\\,0.5)$.\n- Validation features $\\mathbf{X}_{\\text{val}}\\in\\mathbb{R}^{6\\times 2}$ and labels $\\mathbf{y}_{\\text{val}}\\in\\{0,1\\}^{6}$:\n  - Class $0$: $(0.0,\\,0.1)$, $(0.2,\\,-0.2)$, $(-0.2,\\,0.0)$, $(0.1,\\,-0.1)$.\n  - Class $1$: $(0.6,\\,0.6)$, $(0.8,\\,0.7)$.\n- Parameters: $B=200$, $\\lambda=1.0$, $\\alpha=0.05$, $s=20231111$.\n\nCase $\\#3$ (nearly separable, strong separation):\n- Training features $\\mathbf{X}_{\\text{train}}\\in\\mathbb{R}^{10\\times 2}$ and labels $\\mathbf{y}_{\\text{train}}\\in\\{0,1\\}^{10}$:\n  - Class $0$: $(-1.2,\\,-0.8)$, $(-0.8,\\,-1.1)$, $(-1.0,\\,-1.3)$, $(-0.7,\\,-0.9)$, $(-1.1,\\,-1.0)$.\n  - Class $1$: $(0.9,\\,1.2)$, $(1.1,\\,0.8)$, $(1.3,\\,1.0)$, $(0.8,\\,0.9)$, $(1.2,\\,1.1)$.\n- Validation features $\\mathbf{X}_{\\text{val}}\\in\\mathbb{R}^{6\\times 2}$ and labels $\\mathbf{y}_{\\text{val}}\\in\\{0,1\\}^{6}$:\n  - Class $0$: $(-1.1,\\,-1.1)$, $(-0.9,\\,-1.0)$, $(-1.2,\\,-0.9)$.\n  - Class $1$: $(1.0,\\,1.0)$, $(1.2,\\,0.9)$, $(0.9,\\,1.1)$.\n- Parameters: $B=200$, $\\lambda=1.0$, $\\alpha=0.05$, $s=20231111$.\n\nFor each case, return a list of four values $[\\text{AUC}_{\\text{lo}},\\,\\text{AUC}_{\\text{hi}},\\,\\text{Brier}_{\\text{lo}},\\,\\text{Brier}_{\\text{hi}}]$, where the subscripts $\\text{lo}$ and $\\text{hi}$ denote the empirical $\\alpha/2$ and $1-\\alpha/2$ quantiles computed using linear interpolation. Round each value to $4$ decimal places.\n\nFinal output format. Your program should produce a single line of output containing the results for the three cases as a comma-separated list of lists enclosed in square brackets, in the order of cases $\\#1$, $\\#2$, $\\#3$. For example, print\n$[\\,[a_{1},b_{1},c_{1},d_{1}],\\,[a_{2},b_{2},c_{2},d_{2}],\\,[a_{3},b_{3},c_{3},d_{3}]\\,]$\nwhere each placeholder is replaced by the corresponding rounded floating-point value. No other text should be printed.", "solution": "The problem requires the implementation of a statistical procedure to quantify the uncertainty of a binary diagnostic model's performance. The model is a logistic regression classifier, and the uncertainty, arising from finite training data, is assessed using nonparametric bootstrap resampling. Confidence intervals for two performance metrics, the Area Under the Receiver Operating Characteristic Curve (AUC) and the Brier score, are to be constructed. The entire process must be deterministic, governed by a fixed random seed for each case.\n\nThe solution is designed around three core components: (1) a robust solver for $\\ell_2$-penalized logistic regression using the Newton-Raphson method; (2) functions to compute the specified performance metrics; and (3) a main procedure to execute the bootstrap resampling and compute the percentile confidence intervals.\n\nFirst, we define the logistic regression model. The conditional probability of a positive outcome ($y=1$) given a feature vector $\\mathbf{x} \\in \\mathbb{R}^d$ is modeled as:\n$$ p(y=1 \\mid \\mathbf{x}; \\boldsymbol{\\beta}) = \\sigma(\\boldsymbol{\\beta}^{\\top}\\mathbf{\\tilde{x}}) = \\frac{1}{1 + \\exp(-\\boldsymbol{\\beta}^{\\top}\\mathbf{\\tilde{x}})} $$\nwhere $\\mathbf{\\tilde{x}} = [1, x_1, \\dots, x_d]^\\top$ is the feature vector augmented with a leading $1$ to accommodate the intercept term, and $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\dots, \\beta_d]^\\top$ is the vector of model coefficients.\n\nThe coefficients $\\boldsymbol{\\beta}$ are determined by minimizing the negative log-likelihood of the training data, with an added $\\ell_2$ penalty on the non-intercept coefficients. For a training set of $n$ samples $(\\mathbf{x}_i, y_i)$, the objective function to minimize is:\n$$ J(\\boldsymbol{\\beta}) = -\\sum_{i=1}^{n} \\left[ y_i \\log p_i + (1-y_i) \\log(1-p_i) \\right] + \\frac{\\lambda}{2} \\sum_{j=1}^{d} \\beta_j^2 $$\nwhere $p_i = \\sigma(\\boldsymbol{\\beta}^\\top \\mathbf{\\tilde{x}}_i)$ and $\\lambda$ is the regularization parameter. In matrix notation, this is:\n$$ J(\\boldsymbol{\\beta}) = - \\left( \\mathbf{y}^\\top \\log(\\mathbf{p}) + (\\mathbf{1}-\\mathbf{y})^\\top \\log(\\mathbf{1}-\\mathbf{p}) \\right) + \\frac{\\lambda}{2} \\boldsymbol{\\beta}^\\top \\mathbf{P} \\boldsymbol{\\beta} $$\nwhere $\\mathbf{\\tilde{X}}$ is the $n \\times (d+1)$ design matrix, $\\mathbf{y}$ is the vector of labels, $\\mathbf{p}$ is the vector of probabilities, and $\\mathbf{P}$ is a $(d+1) \\times (d+1)$ diagonal matrix with $P_{00}=0$ and $P_{jj}=1$ for $j \\in \\{1, \\dots, d\\}$. The penalty matrix $\\mathbf{P}$ ensures that the intercept $\\beta_0$ is not penalized.\n\nThis objective function is convex and can be efficiently minimized using the Newton-Raphson method. The iterative update rule is:\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - (\\mathbf{H}^{(t)})^{-1} \\mathbf{g}^{(t)} $$\nwhere $\\mathbf{g}^{(t)}$ is the gradient and $\\mathbf{H}^{(t)}$ is the Hessian of $J(\\boldsymbol{\\beta})$ evaluated at the current estimate $\\boldsymbol{\\beta}^{(t)}$. The gradient is given by:\n$$ \\mathbf{g}(\\boldsymbol{\\beta}) = \\nabla_{\\boldsymbol{\\beta}} J(\\boldsymbol{\\beta}) = \\mathbf{\\tilde{X}}^\\top (\\mathbf{p} - \\mathbf{y}) + \\lambda \\mathbf{P} \\boldsymbol{\\beta} $$\nAnd the Hessian is:\n$$ \\mathbf{H}(\\boldsymbol{\\beta}) = \\nabla^2_{\\boldsymbol{\\beta}} J(\\boldsymbol{\\beta}) = \\mathbf{\\tilde{X}}^\\top \\mathbf{W} \\mathbf{\\tilde{X}} + \\lambda \\mathbf{P} $$\nHere, $\\mathbf{W}$ is an $n \\times n$ diagonal matrix of weights with $W_{ii} = p_i(1-p_i)$. The update step $\\Delta\\boldsymbol{\\beta} = (\\mathbf{H}^{(t)})^{-1} \\mathbf{g}^{(t)}$ is computed by solving the linear system $\\mathbf{H}^{(t)}\\Delta\\boldsymbol{\\beta} = \\mathbf{g}^{(t)}$, which is more numerically stable than matrix inversion. The iteration starts with $\\boldsymbol{\\beta}^{(0)}=\\mathbf{0}$ and continues until the Euclidean norm of the update, $\\|\\Delta\\boldsymbol{\\beta}\\|_2$, falls below a tolerance, or a maximum number of iterations is reached. For numerical stability, predicted probabilities $p_i$ are clipped to a small interval $[\\varepsilon, 1-\\varepsilon]$ where $\\varepsilon=10^{-12}$.\n\nSecond, we define the performance metrics.\nThe AUC is calculated on the validation set using the Wilcoxon-Mann-Whitney U statistic formulation, which correctly handles ties in the predicted probabilities. Given $n_+$ positive and $n_-$ negative samples in the validation set, and the sum of ranks of positive samples' scores, $R_+$, the AUC is:\n$$ \\text{AUC} = \\frac{R_+ - \\frac{n_+(n_+ + 1)}{2}}{n_+ n_-} $$\nThe Brier score is the mean squared error between the predicted probabilities $\\hat{p}_i$ and the true binary outcomes $y_i$ on the validation set:\n$$ \\text{Brier score} = \\frac{1}{n_{\\text{val}}} \\sum_{i=1}^{n_{\\text{val}}} (\\hat{p}_i - y_i)^2 $$\n\nThird, we implement the nonparametric bootstrap procedure. For each of the $B$ bootstrap replications:\n1.  A bootstrap sample of size $n_{\\text{train}}$ is created by drawing indices with replacement from the original training set. A seeded pseudo-random number generator ensures reproducibility.\n2.  The penalized logistic regression model is trained on this bootstrap sample to obtain a coefficient vector $\\boldsymbol{\\beta}_{\\text{boot}}$.\n3.  This fitted model is then used to predict probabilities on the fixed, original validation set.\n4.  The AUC and Brier score are computed for these predictions and stored.\n\nAfter $B$ iterations, we obtain two empirical distributions of $B$ scores each, one for AUC and one for the Brier score. These distributions approximate the sampling distributions of the statistics. From these, we construct the $(1-\\alpha)$ percentile confidence intervals. The lower and upper bounds of the intervals are the empirical quantiles at probabilities $\\alpha/2$ and $1-\\alpha/2$, respectively. As specified, these quantiles are computed using linear interpolation between adjacent ordered values in the empirical distributions. For a sorted list of scores $v$ of size $B$, the quantile at probability $q$ is found at index $(B-1)q$. If this index is not an integer, linear interpolation is performed between the values at the floor and ceiling of the index.\n\nThe overall algorithm for each test case is:\n1. Initialize a random number generator with the specified seed $s$.\n2. Create two empty lists, `auc_scores` and `brier_scores`.\n3. Loop $B$ times:\n    a. Draw a bootstrap sample from the training data.\n    b. Fit the penalized logistic regression model on the sample. If the fit fails (e.g., due to a singular Hessian), record `NaN` for the metrics.\n    c. Predict probabilities on the validation set.\n    d. Compute and append AUC and Brier score to their respective lists.\n4.  Use `numpy.nanquantile` with the option `method='linear'` to compute the $[\\alpha/2, 1-\\alpha/2]$ quantiles for both lists, which robustly handles any `NaN` values from failed fits.\n5.  Round the four resulting interval bounds to $4$ decimal places and return them.\nThis procedure is applied to each of the three test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit\nfrom scipy.stats import rankdata\n\nclass PenalizedLogisticRegression:\n    \"\"\"\n    Implements L2-penalized logistic regression trained with Newton-Raphson.\n\n    The intercept term is not penalized.\n    \"\"\"\n    def __init__(self, lambda_=1.0, max_iter=100, tol=1e-6, epsilon=1e-12):\n        self.lambda_ = lambda_\n        self.max_iter = max_iter\n        self.tol = tol\n        self.epsilon = epsilon\n        self.beta_ = None\n\n    def _add_intercept(self, X):\n        return np.c_[np.ones(X.shape[0]), X]\n\n    def fit(self, X, y):\n        \"\"\"Fits the model to the training data.\"\"\"\n        X_aug = self._add_intercept(X)\n        n_samples, n_features_plus_one = X_aug.shape\n        \n        self.beta_ = np.zeros(n_features_plus_one)\n        \n        # Penalty matrix P: diag(0, 1, 1, ...)\n        P = np.diag([0] + [1] * (n_features_plus_one - 1))\n        \n        for _ in range(self.max_iter):\n            z = X_aug @ self.beta_\n            p = expit(z)\n            p = np.clip(p, self.epsilon, 1 - self.epsilon)\n            \n            grad = X_aug.T @ (p - y) + self.lambda_ * (P @ self.beta_)\n            \n            W = np.diag(p * (1 - p))\n            H = X_aug.T @ W @ X_aug + self.lambda_ * P\n            \n            try:\n                # Solve H * delta_beta = grad for delta_beta\n                delta_beta = np.linalg.solve(H, grad)\n            except np.linalg.LinAlgError:\n                # Hessian is singular; fit fails for this bootstrap sample\n                self.beta_ = None\n                return self\n            \n            self.beta_ -= delta_beta\n            \n            if np.linalg.norm(delta_beta)  self.tol:\n                break\n        \n        return self\n\n    def predict_proba(self, X):\n        \"\"\"Predicts class probabilities for X.\"\"\"\n        if self.beta_ is None:\n            return np.full(X.shape[0], np.nan)\n            \n        X_aug = self._add_intercept(X)\n        z = X_aug @ self.beta_\n        p = expit(z)\n        return np.clip(p, self.epsilon, 1 - self.epsilon)\n\ndef calculate_auc(y_true, y_pred):\n    \"\"\"Calculates AUC with tie-handling via average ranks.\"\"\"\n    n_pos = np.sum(y_true == 1)\n    n_neg = np.sum(y_true == 0)\n    \n    if n_pos == 0 or n_neg == 0:\n        return np.nan\n        \n    ranks = rankdata(y_pred, method='average')\n    r_pos_sum = np.sum(ranks[y_true == 1])\n    \n    auc = (r_pos_sum - n_pos * (n_pos + 1) / 2) / (n_pos * n_neg)\n    return auc\n\ndef calculate_brier(y_true, y_pred):\n    \"\"\"Calculates the Brier score.\"\"\"\n    return np.mean((y_pred - y_true)**2)\n\ndef run_bootstrap_analysis(X_train, y_train, X_val, y_val, B, lambda_, alpha, seed):\n    \"\"\"Performs the bootstrap analysis for a single test case.\"\"\"\n    n_train = X_train.shape[0]\n    rng = np.random.default_rng(seed)\n    \n    auc_scores = []\n    brier_scores = []\n    \n    for _ in range(B):\n        boot_indices = rng.choice(n_train, size=n_train, replace=True)\n        X_boot, y_boot = X_train[boot_indices], y_train[boot_indices]\n        \n        model = PenalizedLogisticRegression(lambda_=lambda_)\n        model.fit(X_boot, y_boot)\n        \n        if model.beta_ is not None:\n            p_val = model.predict_proba(X_val)\n            \n            auc = calculate_auc(y_val, p_val)\n            brier = calculate_brier(y_val, p_val)\n            \n            auc_scores.append(auc)\n            brier_scores.append(brier)\n        else:\n            auc_scores.append(np.nan)\n            brier_scores.append(np.nan)\n\n    q = [alpha / 2, 1 - alpha / 2]\n    auc_interval = np.nanquantile(auc_scores, q, method='linear')\n    brier_interval = np.nanquantile(brier_scores, q, method='linear')\n    \n    result = [\n        round(auc_interval[0], 4),\n        round(auc_interval[1], 4),\n        round(brier_interval[0], 4),\n        round(brier_interval[1], 4),\n    ]\n    return result\n\ndef solve():\n    test_cases = [\n        # Case 1\n        (\n            np.array([[-0.5, 0.1], [0.3, -0.2], [-0.7, -0.8], [0.2, 0.0], [-0.3, 0.4], [0.0, -0.5], [1.8, 2.2], [2.5, 1.7], [1.9, 1.8], [2.2, 2.4], [1.5, 2.0], [2.0, 1.6]]),\n            np.array([0]*6 + [1]*6),\n            np.array([[-0.6, 0.2], [0.1, -0.1], [-0.2, -0.6], [0.4, 0.3], [2.1, 1.9], [1.7, 2.3], [2.3, 2.1], [1.6, 1.8]]),\n            np.array([0]*4 + [1]*4),\n            200, 1.0, 0.05, 20231111\n        ),\n        # Case 2\n        (\n            np.array([[0.1, 0.2], [-0.2, 0.0], [0.0, -0.1], [0.3, -0.2], [-0.4, 0.1], [0.2, 0.1], [-0.1, -0.3], [0.1, -0.2], [0.6, 0.7], [0.7, 0.5]]),\n            np.array([0]*8 + [1]*2),\n            np.array([[0.0, 0.1], [0.2, -0.2], [-0.2, 0.0], [0.1, -0.1], [0.6, 0.6], [0.8, 0.7]]),\n            np.array([0]*4 + [1]*2),\n            200, 1.0, 0.05, 20231111\n        ),\n        # Case 3\n        (\n            np.array([[-1.2, -0.8], [-0.8, -1.1], [-1.0, -1.3], [-0.7, -0.9], [-1.1, -1.0], [0.9, 1.2], [1.1, 0.8], [1.3, 1.0], [0.8, 0.9], [1.2, 1.1]]),\n            np.array([0]*5 + [1]*5),\n            np.array([[-1.1, -1.1], [-0.9, -1.0], [-1.2, -0.9], [1.0, 1.0], [1.2, 0.9], [0.9, 1.1]]),\n            np.array([0]*3 + [1]*3),\n            200, 1.0, 0.05, 20231111\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        X_train, y_train, X_val, y_val, B, lambda_, alpha, seed = case\n        result = run_bootstrap_analysis(X_train, y_train, X_val, y_val, B, lambda_, alpha, seed)\n        results.append(result)\n\n    print(str(results).replace(\" \", \"\"))\n\n\nsolve()\n```", "id": "5208017"}]}