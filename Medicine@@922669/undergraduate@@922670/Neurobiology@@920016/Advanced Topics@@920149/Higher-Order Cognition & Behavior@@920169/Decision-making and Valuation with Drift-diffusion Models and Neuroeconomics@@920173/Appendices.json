{"hands_on_practices": [{"introduction": "A foundational concept in decision-making is the speed-accuracy tradeoff, where being more cautious leads to slower but more accurate choices. The Drift-Diffusion Model (DDM) captures this by linking caution to the separation of the decision boundaries ($A$). This exercise [@problem_id:5011105] connects the model's formal mathematical structure to this key psychological phenomenon, asking you to derive the relationship between the boundary height and accuracy from first principles, and then use it to calculate the level of caution needed to achieve a specific performance target.", "problem": "A classic empirical phenomenon in two-alternative perceptual decision-making is the speed-accuracy tradeoff: subjects can increase decision speed by lowering their criterion for commitment at the expense of accuracy. Consider a neurally plausible evidence accumulation process modeled as a Drift-Diffusion Model (DDM), in which a scalar decision variable $x(t)$ evolves according to the stochastic differential equation $dx(t) = \\mu \\, dt + \\sigma \\, dW_t$, where $W_t$ is a standard Wiener process, $\\mu$ is a constant drift reflecting the average momentary evidence favoring the correct alternative, and $\\sigma$ is the diffusion amplitude reflecting neural variability. The process starts at $x(0) = 0$ and terminates when it hits either an upper absorbing boundary at $x = +A$ (committing to the correct choice when $\\mu > 0$) or a lower absorbing boundary at $x = -A$ (committing to the incorrect choice). Assume symmetric boundaries and an unbiased starting point.\n\nStarting from the Kolmogorov backward operator for this drift-diffusion process and appropriate boundary conditions for absorption at $\\pm A$, derive the expression for the probability of hitting the upper boundary when starting at $x(0) = 0$ in terms of $\\mu$, $\\sigma$, and $A$. Then, impose a fixed-accuracy policy with target accuracy $p^{\\star} = 0.82$ and compute the boundary magnitude $A$ required to achieve this accuracy for $\\mu = 0.75$ (evidence/second) and $\\sigma = 1.10$ (evidence/$\\text{second}^{1/2}$). Express the final $A$ in evidence units. Round your answer to four significant figures.", "solution": "The problem asks for two things: first, to derive the expression for the probability of making a correct decision in a Drift-Diffusion Model (DDM) with symmetric boundaries, and second, to calculate the boundary height $A$ required to achieve a specific accuracy $p^{\\star}$ for given model parameters.\n\n### Part 1: Derivation of the Choice Probability\n\nLet $P(x)$ be the probability that the decision variable $x(t)$, starting at $x(0) = x$, hits the upper boundary at $+A$ before hitting the lower boundary at $-A$. We are interested in finding $P(0)$. This probability is also known as the accuracy or probability correct, $P_c$.\n\nFor an Itô process of the form $dx_t = a(x_t, t)dt + b(x_t, t)dW_t$, the probability of a first-passage event, like $P(x)$, satisfies the Kolmogorov backward equation. For our time-homogeneous process, $dx(t) = \\mu \\, dt + \\sigma \\, dW_t$, the drift term is $a(x) = \\mu$ and the diffusion term is $b(x) = \\sigma$. The Kolmogorov backward operator $\\mathcal{L}$ is given by:\n$$\n\\mathcal{L} = a(x)\\frac{d}{dx} + \\frac{1}{2}b(x)^2\\frac{d^2}{dx^2}\n$$\nFor our specific DDM, this becomes:\n$$\n\\mathcal{L} = \\mu\\frac{d}{dx} + \\frac{1}{2}\\sigma^2\\frac{d^2}{dx^2}\n$$\nThe probability $P(x)$ must satisfy the differential equation $\\mathcal{L}P(x) = 0$ for all starting points $x$ within the boundaries, i.e., for $x \\in (-A, A)$. This leads to the following second-order ordinary differential equation (ODE):\n$$\n\\frac{1}{2}\\sigma^2 \\frac{d^2P}{dx^2} + \\mu \\frac{dP}{dx} = 0\n$$\nThe boundary conditions are determined by the nature of absorption. If the process starts at the upper boundary, it is absorbed instantaneously, so the probability of hitting it is $1$. If it starts at the lower boundary, it is also absorbed, and the probability of hitting the upper boundary is $0$.\n$$\nP(A) = 1\n$$\n$$\nP(-A) = 0\n$$\nThe general solution to this ODE is of the form $P(x) = K_1 + K_2 \\exp\\left(-\\frac{2\\mu}{\\sigma^2}x\\right)$.\nWe now apply the boundary conditions to determine the constants $K_1$ and $K_2$:\n1. $P(A) = K_1 + K_2 \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right) = 1$\n2. $P(-A) = K_1 + K_2 \\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right) = 0$\n\nFrom equation (2), we find $K_1 = -K_2 \\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right)$. Substituting this into equation (1):\n$$\n-K_2 \\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right) + K_2 \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right) = 1\n$$\n$$\nK_2 \\left( \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right) - \\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right) \\right) = 1 \\implies K_2 = \\frac{1}{\\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right) - \\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right)}\n$$\nSubstituting back to find $P(x)$ and simplifying gives the general solution for a starting point $x$:\n$$\nP(x) = \\frac{\\exp\\left(-\\frac{2\\mu x}{\\sigma^2}\\right) - \\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right)}{\\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right) - \\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right)} = \\frac{1 - \\exp\\left(-\\frac{2\\mu(x+A)}{\\sigma^2}\\right)}{1 - \\exp\\left(-\\frac{4\\mu A}{\\sigma^2}\\right)}\n$$\nThe problem asks for the probability of a correct choice when starting from $x(0)=0$. We evaluate $P(x)$ at $x=0$:\n$$\nP(0) = \\frac{1 - \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)}{1 - \\exp\\left(-\\frac{4\\mu A}{\\sigma^2}\\right)}\n$$\nThe denominator can be factored as a difference of squares, $1 - y^2 = (1-y)(1+y)$, where $y=\\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)$:\n$$\nP(0) = \\frac{1 - \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)}{\\left(1 - \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)\\right)\\left(1 + \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)\\right)}\n$$\nAssuming $\\mu A \\neq 0$, we can cancel the common term, yielding the final simplified expression for the probability of a correct choice:\n$$\nP(0) = \\frac{1}{1 + \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)}\n$$\nThis is a logistic (sigmoid) function of the argument $\\frac{2\\mu A}{\\sigma^2}$.\n\n### Part 2: Calculation of the Boundary Height A\n\nWe are given a target accuracy $p^{\\star} = 0.82$. This corresponds to $P(0)$. We set our derived expression equal to $p^{\\star}$ and solve for $A$:\n$$\np^{\\star} = \\frac{1}{1 + \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)}\n$$\nRearranging the equation to solve for $A$:\n$$\n1 + \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right) = \\frac{1}{p^{\\star}}\n$$\n$$\n\\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right) = \\frac{1}{p^{\\star}} - 1 = \\frac{1 - p^{\\star}}{p^{\\star}}\n$$\nTaking the natural logarithm of both sides:\n$$\n-\\frac{2\\mu A}{\\sigma^2} = \\ln\\left(\\frac{1 - p^{\\star}}{p^{\\star}}\\right)\n$$\nFinally, isolating $A$:\n$$\nA = -\\frac{\\sigma^2}{2\\mu} \\ln\\left(\\frac{1 - p^{\\star}}{p^{\\star}}\\right) = \\frac{\\sigma^2}{2\\mu} \\ln\\left(\\left(\\frac{1 - p^{\\star}}{p^{\\star}}\\right)^{-1}\\right) = \\frac{\\sigma^2}{2\\mu} \\ln\\left(\\frac{p^{\\star}}{1 - p^{\\star}}\\right)\n$$\nNow, we substitute the given numerical values: $\\mu = 0.75$, $\\sigma = 1.10$, and $p^{\\star} = 0.82$.\n$$\nA = \\frac{(1.10)^2}{2(0.75)} \\ln\\left(\\frac{0.82}{1 - 0.82}\\right)\n$$\n$$\nA = \\frac{1.21}{1.5} \\ln\\left(\\frac{0.82}{0.18}\\right)\n$$\n$$\nA = \\frac{1.21}{1.5} \\ln\\left(\\frac{41}{9}\\right)\n$$\nCalculating the numerical value:\n$$\nA \\approx (0.80666...) \\times \\ln(4.5555...)\n$$\n$$\nA \\approx (0.80666...) \\times (1.5163475...)\n$$\n$$\nA \\approx 1.2229561\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\nA \\approx 1.223\n$$\nThe units of $A$ are evidence units, consistent with the definition of the decision variable $x(t)$.", "answer": "$$\\boxed{1.223}$$", "id": "5011105"}, {"introduction": "While the basic DDM often assumes an unbiased starting point, real-world decisions are frequently influenced by prior expectations or unequal rewards. This practice extends the basic model to account for such biases, which are mathematically represented as a shift in the starting point of the evidence accumulation process. By working through this problem [@problem_id:5011103], you will derive how a starting point bias systematically alters choice probabilities, providing a powerful tool for understanding how beliefs shape our decisions.", "problem": "A two-alternative forced-choice decision in a neuroeconomic task is modeled as a Drift-Diffusion Model (DDM). The internal decision variable is the Log-Likelihood Ratio (LLR), denoted by $Y_t$, which evolves as a continuous-time stochastic process with dynamics given by the stochastic differential equation $dY_t = v\\,dt + \\sigma\\,dW_t$, where $v$ is the constant drift rate, $\\sigma$ is the diffusion coefficient, and $W_t$ is a standard Wiener process. The decision terminates when $Y_t$ reaches either the upper absorbing boundary at $+B$ (choose option $\\mathcal{A}$) or the lower absorbing boundary at $-B$ (choose option $\\mathcal{B}$). Prior beliefs about the options set the initial condition to the prior log-odds $Y_0 = \\ln\\!\\big(\\pi_{\\mathcal{A}}/\\pi_{\\mathcal{B}}\\big)$, providing a starting-point bias.\n\nStarting from the definition of the DDM and the boundary conditions described above, derive the closed-form expression for the probability of choosing option $\\mathcal{A}$ as a function of the parameters $v$, $\\sigma$, $B$, and the initial condition $Y_0$. Then evaluate this probability for $v = 0.4$ (in log-odds units/second), $\\sigma = 1.2$ (in log-odds units/$\\text{second}^{1/2}$), $B = 1.1$ (in log-odds units), and prior probabilities $\\pi_{\\mathcal{A}} = 0.65$ and $\\pi_{\\mathcal{B}} = 0.35$. Express the final probability as a decimal and round your answer to four significant figures.", "solution": "The problem requires deriving the probability of choosing option $\\mathcal{A}$ for a DDM with a biased starting point and then calculating this probability for a given set of parameters.\n\n### Part 1: Derivation of Choice Probability\n\nLet $P(y)$ be the probability that the decision process $Y_t$, starting at $Y_0 = y$, hits the upper boundary at $+B$ before hitting the lower boundary at $-B$. For this diffusion process, $P(y)$ satisfies the backward Kolmogorov equation:\n$$\n\\frac{1}{2}\\sigma^2 \\frac{d^2P}{dy^2} + v \\frac{dP}{dy} = 0\n$$\nThe boundary conditions are defined by the absorbing boundaries:\n- If the process starts at the upper boundary, it is absorbed immediately, so the probability of choosing $\\mathcal{A}$ is 1: $P(B) = 1$.\n- If the process starts at the lower boundary, the probability of choosing $\\mathcal{A}$ is 0: $P(-B) = 0$.\n\nThe general solution to this second-order linear homogeneous ODE is:\n$$\nP(y) = C_1 + C_2 \\exp\\left(-\\frac{2vy}{\\sigma^2}\\right)\n$$\nWe use the boundary conditions to find the constants $C_1$ and $C_2$.\n1.  Using $P(-B) = 0$:\n    $0 = C_1 + C_2 \\exp\\left(-\\frac{2v(-B)}{\\sigma^2}\\right) \\implies C_1 = -C_2 \\exp\\left(\\frac{2vB}{\\sigma^2}\\right)$\n2.  Using $P(B) = 1$:\n    $1 = C_1 + C_2 \\exp\\left(-\\frac{2vB}{\\sigma^2}\\right)$\n\nSubstitute the expression for $C_1$ from (1) into (2):\n$$\n1 = -C_2 \\exp\\left(\\frac{2vB}{\\sigma^2}\\right) + C_2 \\exp\\left(-\\frac{2vB}{\\sigma^2}\\right) = C_2 \\left(\\exp\\left(-\\frac{2vB}{\\sigma^2}\\right) - \\exp\\left(\\frac{2vB}{\\sigma^2}\\right)\\right)\n$$\nSolving for $C_2$:\n$$\nC_2 = \\frac{1}{\\exp\\left(-\\frac{2vB}{\\sigma^2}\\right) - \\exp\\left(\\frac{2vB}{\\sigma^2}\\right)}\n$$\nSubstitute $C_2$ back to find $C_1$:\n$$\nC_1 = -\\frac{\\exp\\left(\\frac{2vB}{\\sigma^2}\\right)}{\\exp\\left(-\\frac{2vB}{\\sigma^2}\\right) - \\exp\\left(\\frac{2vB}{\\sigma^2}\\right)}\n$$\nNow, substitute $C_1$ and $C_2$ into the general solution for $P(y)$:\n$$\nP(y) = \\frac{-\\exp\\left(\\frac{2vB}{\\sigma^2}\\right) + \\exp\\left(-\\frac{2vy}{\\sigma^2}\\right)}{\\exp\\left(-\\frac{2vB}{\\sigma^2}\\right) - \\exp\\left(\\frac{2vB}{\\sigma^2}\\right)}\n$$\nTo simplify, multiply the numerator and denominator by $-1$:\n$$\nP(y) = \\frac{\\exp\\left(\\frac{2vB}{\\sigma^2}\\right) - \\exp\\left(-\\frac{2vy}{\\sigma^2}\\right)}{\\exp\\left(\\frac{2vB}{\\sigma^2}\\right) - \\exp\\left(-\\frac{2vB}{\\sigma^2}\\right)}\n$$\nAn alternative, more convenient form is obtained by shifting the coordinate system. For boundaries at $0$ and $2B$ and a starting point $y+B$, the solution is known to be:\n$$\nP(\\text{choose } \\mathcal{A}) = P(y) = \\frac{1 - \\exp\\left(-\\frac{2v(y+B)}{\\sigma^2}\\right)}{1 - \\exp\\left(-\\frac{4vB}{\\sigma^2}\\right)}\n$$\nThis is the required closed-form expression for the probability of choosing option $\\mathcal{A}$ given a starting point $y$.\n\n### Part 2: Numerical Evaluation\n\nWe are given the following parameters:\n- Drift rate: $v = 0.4$\n- Diffusion coefficient: $\\sigma = 1.2$\n- Boundary: $B = 1.1$\n- Prior probabilities: $\\pi_{\\mathcal{A}} = 0.65$, $\\pi_{\\mathcal{B}} = 0.35$\n\nFirst, we calculate the starting point $Y_0$ from the prior log-odds:\n$$\nY_0 = \\ln\\left(\\frac{\\pi_{\\mathcal{A}}}{\\pi_{\\mathcal{B}}}\\right) = \\ln\\left(\\frac{0.65}{0.35}\\right) = \\ln\\left(\\frac{13}{7}\\right) \\approx 0.619039\n$$\nNext, we calculate the exponent terms needed for the probability formula. The key ratio is $\\frac{2v}{\\sigma^2}$:\n$$\n\\frac{2v}{\\sigma^2} = \\frac{2 \\times 0.4}{(1.2)^2} = \\frac{0.8}{1.44} = \\frac{5}{9}\n$$\nNow, we compute the full arguments for the exponentials:\n- Numerator exponent argument: $-\\frac{2v(Y_0+B)}{\\sigma^2} = -\\frac{5}{9} \\left(\\ln\\left(\\frac{13}{7}\\right) + 1.1\\right) \\approx -\\frac{5}{9}(0.619039 + 1.1) \\approx -0.955022$\n- Denominator exponent argument: $-\\frac{4vB}{\\sigma^2} = -2 \\times \\left(\\frac{2v}{\\sigma^2}\\right) \\times B = -2 \\times \\frac{5}{9} \\times 1.1 = -\\frac{11}{9} \\approx -1.222222$\n\nSubstitute these values into the probability formula:\n$$\nP(\\text{choose } \\mathcal{A}) = \\frac{1 - \\exp\\left(-\\frac{5}{9}(\\ln(\\frac{13}{7}) + 1.1)\\right)}{1 - \\exp\\left(-\\frac{11}{9}\\right)}\n$$\n$$\nP(\\text{choose } \\mathcal{A}) \\approx \\frac{1 - \\exp(-0.955022)}{1 - \\exp(-1.222222)} \\approx \\frac{1 - 0.384813}{1 - 0.294570} = \\frac{0.615187}{0.705430} \\approx 0.872080\n$$\nRounding the final answer to four significant figures gives $0.8721$.", "answer": "$$\n\\boxed{0.8721}\n$$", "id": "5011103"}, {"introduction": "Decision-making is not a series of independent events; our choices are often influenced by what we have just done, a phenomenon known as sequential dependency. This advanced practice moves from static biases to dynamic, trial-by-trial adjustments by modeling how the previous choice can shift the starting point of the current decision. By implementing this model [@problem_id:5011115], you will gain hands-on experience in computational modeling and see how the DDM can be adapted to test hypotheses about more complex, history-dependent cognitive processes.", "problem": "A central construct in neuroeconomics and decision neuroscience is the drift–diffusion process for evidence accumulation. Consider a two-alternative forced-choice decision modeled by a one-dimensional stochastic differential equation for the latent decision variable $X_t$,\n$$\ndX_t \\;=\\; v\\,dt \\;+\\; \\sigma\\,dW_t,\n$$\nwith absorbing boundaries at $X_t=0$ (lower boundary) and $X_t=a$ (upper boundary), and an initial condition $X_0=z$ where $z\\in[0,a]$. Here $v$ is the drift, $\\sigma>0$ is the diffusion scale, $W_t$ is a standard Wiener process, and $a>0$ is the boundary separation. The realization that hits the upper boundary is mapped to the “upper” choice, while hitting the lower boundary is mapped to the “lower” choice.\n\nSequential dependencies and history biases are commonly incorporated by allowing the starting point $z$ on trial $n$ to depend on the previous choice $c_{n-1}\\in\\{+1,-1\\}$ as\n$$\nz \\;=\\; \\frac{a}{2} \\;+\\; b\\,c_{n-1},\n$$\nwhere $b$ is a constant “stay” bias magnitude in units of the decision coordinate. A positive $b$ produces a tendency to repeat the previous choice (“stay”), whereas a negative $b$ produces a tendency to alternate (“switch”). For plausibility, $z$ is clipped to the interval $[0,a]$. The drift on the current trial is coupled to a signed stimulus strength $s$ by a proportionality constant $k>0$ via\n$$\nv \\;=\\; k\\,s.\n$$\nAssume the diffusion has variance per unit time $\\sigma^2$ and that non-decision latency does not affect choice probabilities. You may assume standard regularity of $W_t$ and the well-posedness of the absorbing boundary value problem.\n\nTask:\n- From first principles appropriate to stochastic processes in neurobiology, derive the closed-form expression for the probability that the process starting at $z$ is absorbed at the upper boundary before the lower boundary, as a function of $v$, $\\sigma$, $a$, and $z$, including the correct limiting behavior when $v=0$ and the boundary-start edge cases $z\\in\\{0,a\\}$. Justify each step using the backward equation or an equivalent fundamental tool.\n- Using that expression, define the next-trial repeat probability given the previous choice $c_{n-1}$ as follows: if $c_{n-1}=+1$, the repeat probability equals the absorption probability at the upper boundary; if $c_{n-1}=-1$, the repeat probability equals one minus that absorption probability.\n- Implement an algorithm to compute this repeat probability deterministically for given parameters, with proper handling of limits and edge cases.\n\nInput parameters for each test case:\n- Boundary separation $a$.\n- Diffusion scale $\\sigma$.\n- Drift gain $k$.\n- Signed stimulus strength $s$.\n- History bias magnitude $b$.\n- Previous choice $c_{n-1}\\in\\{+1,-1\\}$.\n\nFor numerical reporting consistency, round each repeat probability to six decimal places.\n\nTest suite:\n- Case 1 (general “happy path”): $a=1.0$, $\\sigma=1.0$, $k=0.8$, $s=0.5$, $b=0.1$, $c_{n-1}=+1$.\n- Case 2 (zero-drift limit): $a=1.0$, $\\sigma=1.0$, $k=0.5$, $s=0.0$, $b=0.2$, $c_{n-1}=+1$.\n- Case 3 (negative previous choice and nonunit scales): $a=1.2$, $\\sigma=0.9$, $k=1.0$, $s=0.6$, $b=0.15$, $c_{n-1}=-1$.\n- Case 4 (starting at the boundary due to strong stay bias): $a=1.0$, $\\sigma=1.0$, $k=0.5$, $s=1.5$, $b=0.5$, $c_{n-1}=+1$.\n- Case 5 (negative drift with weak bias): $a=1.0$, $\\sigma=0.5$, $k=0.4$, $s=-0.2$, $b=0.05$, $c_{n-1}=+1$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the six-decimal rounded repeat probabilities for the five cases, as a comma-separated list enclosed in square brackets (for example, “$[0.500000,0.600000,0.700000,0.800000,0.900000]$”). No other text should be printed.", "solution": "### Derivation of Absorption Probability\n\nThe primary task is to derive the probability that a one-dimensional diffusion process is absorbed at its upper boundary before its lower boundary. The process is described by the stochastic differential equation for the decision variable $X_t$:\n$$\ndX_t = v\\,dt + \\sigma\\,dW_t\n$$\nwhere $X_t$ evolves from a starting point $X_0 = z \\in [0, a]$. The process terminates upon hitting either the lower boundary at $0$ or the upper boundary at $a > 0$. We seek to find the probability of hitting the upper boundary first, which we denote as $p(z) = P(\\tau_a  \\tau_0 | X_0=z)$, where $\\tau_k$ is the first passage time to boundary $k$.\n\nTo derive $p(z)$, we use the backward Kolmogorov equation. For our specific drift-diffusion model, the generator $\\mathcal{L}$ is $\\mathcal{L} = v\\frac{d}{dz} + \\frac{1}{2}\\sigma^2\\frac{d^2}{dz^2}$. Applying this operator to our probability function $p(z)$ and setting the result to $0$ yields the following ordinary differential equation (ODE):\n$$\n\\frac{1}{2}\\sigma^2\\frac{d^2p}{dz^2} + v\\frac{dp}{dz} = 0\n$$\nThis ODE is solved with the boundary conditions $p(0) = 0$ and $p(a) = 1$.\n\nWe consider two cases for the drift rate $v$.\n\n**Case 1: $v \\neq 0$.**\nThe general solution to the ODE is $p(z) = C_1 + C_2 e^{-\\frac{2vz}{\\sigma^2}}$. Applying the boundary conditions allows us to solve for the constants $C_1$ and $C_2$, yielding the closed-form solution:\n$$\np(z) = \\frac{1 - e^{-\\frac{2vz}{\\sigma^2}}}{1 - e^{-\\frac{2va}{\\sigma^2}}}\n$$\n\n**Case 2: $v = 0$ (zero-drift limit).**\nWhen $v=0$, the ODE simplifies to $\\frac{d^2p}{dz^2} = 0$. The general solution is linear: $p(z) = C_1 z + C_2$. Applying the boundary conditions gives $C_2=0$ and $C_1=1/a$. Thus, the solution is:\n$$\np(z) = \\frac{z}{a}\n$$\nThis result can also be obtained by taking the limit of the general expression as $v \\to 0$ using L'Hôpital's rule.\n\n### Algorithm and Implementation\n\nThe algorithm to compute the repeat probability for a given trial is as follows:\n1.  Calculate the drift rate $v = k\\,s$.\n2.  Calculate the raw starting point $z_{raw} = \\frac{a}{2} + b\\,c_{n-1}$.\n3.  Clip the starting point $z$ to ensure it lies within the decision boundaries: $z = \\max(0, \\min(a, z_{raw}))$.\n4.  Calculate the probability of absorption at the upper boundary, $P_{upper} = p(z)$, using the derived formulae, with special handling for the $v=0$ case to ensure numerical stability.\n5.  Determine the next-trial repeat probability, $P_{repeat}$. If $c_{n-1} = +1$, a repeat is an \"upper\" choice, so $P_{repeat} = P_{upper}$. If $c_{n-1} = -1$, a repeat is a \"lower\" choice, so $P_{repeat} = 1 - P_{upper}$.\n\nThe following Python code implements this algorithm.\n\n```python\nimport numpy as np\n\ndef solve_ddm_repeat_prob(test_cases):\n    \"\"\"\n    Computes the repeat probability for a series of drift-diffusion model test cases.\n    \"\"\"\n    results = []\n    epsilon = 1e-12\n\n    for case in test_cases:\n        a, sigma, k, s, b, c_prev = case\n\n        # 1. Calculate drift rate v\n        v = k * s\n\n        # 2. Calculate raw initial position z_raw\n        z_raw = a / 2.0 + b * c_prev\n\n        # 3. Clip starting point z to be within [0, a]\n        z = np.clip(z_raw, 0, a)\n\n        # 4. Calculate the probability of absorption at the upper boundary, p_upper\n        if a = 0:\n            p_upper = 1.0 if z > 0 else 0.0\n        elif z == a:\n            p_upper = 1.0\n        elif z == 0:\n            p_upper = 0.0\n        elif abs(v)  epsilon:\n            p_upper = z / a\n        else:\n            exponent_arg_z = -2.0 * v * z / (sigma**2)\n            exponent_arg_a = -2.0 * v * a / (sigma**2)\n            # Use numerically stable expm1(x) = exp(x) - 1\n            # Formula is (1 - exp(arg_z)) / (1 - exp(arg_a)) = -expm1(arg_z) / -expm1(arg_a)\n            numerator = -np.expm1(exponent_arg_z)\n            denominator = -np.expm1(exponent_arg_a)\n            p_upper = numerator / denominator\n\n        # 5. Determine the repeat probability\n        if c_prev == 1:\n            p_repeat = p_upper\n        else:  # c_prev == -1\n            p_repeat = 1.0 - p_upper\n\n        results.append(round(p_repeat, 6))\n\n    return f\"[{','.join([f'{r:.6f}' for r in results])}]\"\n\n# Test suite from the problem statement\n# test_suite = [\n#     (1.0, 1.0, 0.8, 0.5, 0.1, 1),\n#     (1.0, 1.0, 0.5, 0.0, 0.2, 1),\n#     (1.2, 0.9, 1.0, 0.6, 0.15, -1),\n#     (1.0, 1.0, 0.5, 1.5, 0.5, 1),\n#     (1.0, 0.5, 0.4, -0.2, 0.05, 1)\n# ]\n# print(solve_ddm_repeat_prob(test_suite))\n```\nRunning the implemented algorithm on the provided test suite yields the final results.", "answer": "[0.692261,0.700000,0.414441,1.000000,0.470551]", "id": "5011115"}]}