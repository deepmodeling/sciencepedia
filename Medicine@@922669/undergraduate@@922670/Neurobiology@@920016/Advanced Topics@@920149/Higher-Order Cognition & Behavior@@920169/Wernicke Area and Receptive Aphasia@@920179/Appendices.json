{"hands_on_practices": [{"introduction": "Clinical diagnosis of aphasia relies on observing patterns in behavioral test scores. This practice moves beyond qualitative description by showing how these scores can be integrated into a formal statistical model to calculate the probability of a specific diagnosis, like receptive aphasia. By working through this hypothetical multinomial logistic regression model, you will gain hands-on experience with the quantitative tools that are transforming clinical neuroscience and providing more objective diagnostic frameworks. [@problem_id:5079533]", "problem": "Language comprehension, spontaneous fluency, repetition, and confrontation naming are standardized behavioral measures of language function closely linked to the integrity of Wernicke’s area in the posterior superior temporal cortex and its distributed network connections. Receptive aphasia (also known as Wernicke’s aphasia) is characterized by impaired comprehension with relatively fluent speech and variable deficits in repetition and naming. Consider a discriminative classifier based on multinomial logistic modeling that assigns posterior probabilities to four diagnostic categories from observed test scores: receptive aphasia (Wernicke’s aphasia), Broca’s aphasia (expressive aphasia), anomic aphasia, and neurotypical language function. The patient’s scores are $(\\text{comprehension}=30,\\ \\text{fluency}=85,\\ \\text{repetition}=40,\\ \\text{naming}=50)$ on $0$–$100$ scales. The model uses a feature vector $x$ obtained by rescaling each score to the unit interval via $x=\\left(\\frac{c}{100},\\frac{f}{100},\\frac{r}{100},\\frac{n}{100}\\right)$, where $c$, $f$, $r$, and $n$ denote comprehension, fluency, repetition, and naming respectively. The multinomial logistic model specifies a linear predictor $z_{k}=\\beta_{k,0}+\\beta_{k}^{\\top}x$ for each class $k\\in\\{\\text{Wernicke},\\ \\text{Broca},\\ \\text{Anomic},\\ \\text{Healthy}\\}$ with the following parameters:\n- Wernicke: $\\beta_{\\text{W},0}=1.0$, $\\beta_{\\text{W}}=\\left(-2.5,\\ 1.8,\\ -1.2,\\ -0.8\\right)$.\n- Broca: $\\beta_{\\text{B},0}=0.2$, $\\beta_{\\text{B}}=\\left(1.4,\\ -2.0,\\ -0.8,\\ -0.3\\right)$.\n- Anomic: $\\beta_{\\text{A},0}=0.0$, $\\beta_{\\text{A}}=\\left(0.1,\\ 0.4,\\ -0.4,\\ -2.2\\right)$.\n- Healthy: $\\beta_{\\text{H},0}=-3.5$, $\\beta_{\\text{H}}=\\left(3.0,\\ 1.0,\\ 2.0,\\ 1.0\\right)$.\n\nUsing the standard probabilistic interpretation for multinomial logistic models consistent with Bayes’ theorem and maximum-entropy assumptions, compute the posterior probability of receptive aphasia given the patient’s scores. Round your final answer to four significant figures and express it as a decimal fraction (no percent sign).", "solution": "The problem statement is assessed to be valid. It is scientifically grounded within the fields of clinical neuroscience and statistical modeling, well-posed with a complete and consistent set of data and parameters, and objectively stated. The task is to compute a posterior probability using a standard multinomial logistic regression model, for which all necessary inputs are provided.\n\nThe first step is to construct the feature vector $x$ from the patient's test scores. The scores are given as comprehension $c=30$, fluency $f=85$, repetition $r=40$, and naming $n=50$. These scores are on $0$–$100$ scales and are rescaled to the unit interval $[0, 1]$ according to the formula $x=\\left(\\frac{c}{100},\\frac{f}{100},\\frac{r}{100},\\frac{n}{100}\\right)$.\nSubstituting the given scores:\n$$\nx = \\left(\\frac{30}{100}, \\frac{85}{100}, \\frac{40}{100}, \\frac{50}{100}\\right) = (0.3, 0.85, 0.4, 0.5)\n$$\n\nThe multinomial logistic regression model defines a linear predictor, $z_k$, for each diagnostic class $k$. The formula for the linear predictor is $z_k = \\beta_{k,0} + \\beta_k^\\top x$, where $\\beta_{k,0}$ is the intercept and $\\beta_k$ is the vector of weights for class $k$. We calculate $z_k$ for each of the four classes: Wernicke (W), Broca (B), Anomic (A), and Healthy (H).\n\nFor Wernicke's aphasia ($k=\\text{W}$):\n$\\beta_{\\text{W},0} = 1.0$ and $\\beta_{\\text{W}} = (-2.5, 1.8, -1.2, -0.8)$.\n$$\nz_{\\text{W}} = 1.0 + (-2.5)(0.3) + (1.8)(0.85) + (-1.2)(0.4) + (-0.8)(0.5)\n$$\n$$\nz_{\\text{W}} = 1.0 - 0.75 + 1.53 - 0.48 - 0.40 = 0.90\n$$\n\nFor Broca's aphasia ($k=\\text{B}$):\n$\\beta_{\\text{B},0} = 0.2$ and $\\beta_{\\text{B}} = (1.4, -2.0, -0.8, -0.3)$.\n$$\nz_{\\text{B}} = 0.2 + (1.4)(0.3) + (-2.0)(0.85) + (-0.8)(0.4) + (-0.3)(0.5)\n$$\n$$\nz_{\\text{B}} = 0.2 + 0.42 - 1.70 - 0.32 - 0.15 = -1.55\n$$\n\nFor anomic aphasia ($k=\\text{A}$):\n$\\beta_{\\text{A},0} = 0.0$ and $\\beta_{\\text{A}} = (0.1, 0.4, -0.4, -2.2)$.\n$$\nz_{\\text{A}} = 0.0 + (0.1)(0.3) + (0.4)(0.85) + (-0.4)(0.4) + (-2.2)(0.5)\n$$\n$$\nz_{\\text{A}} = 0.03 + 0.34 - 0.16 - 1.10 = -0.89\n$$\n\nFor neurotypical language function (Healthy, $k=\\text{H}$):\n$\\beta_{\\text{H},0} = -3.5$ and $\\beta_{\\text{H}} = (3.0, 1.0, 2.0, 1.0)$.\n$$\nz_{\\text{H}} = -3.5 + (3.0)(0.3) + (1.0)(0.85) + (2.0)(0.4) + (1.0)(0.5)\n$$\n$$\nz_{\\text{H}} = -3.5 + 0.90 + 0.85 + 0.80 + 0.50 = -0.45\n$$\n\nThe posterior probability for a given class $k$, $P(y=k|x)$, is calculated using the softmax function, which normalizes the exponentiated linear predictors:\n$$\nP(y=k|x) = \\frac{\\exp(z_k)}{\\sum_{j \\in \\{\\text{W, B, A, H}\\}} \\exp(z_j)}\n$$\nWe need to compute the posterior probability of receptive aphasia, which corresponds to the Wernicke class ($k=\\text{W}$). Let's denote this probability as $P_{\\text{W}}$.\n$$\nP_{\\text{W}} = \\frac{\\exp(z_{\\text{W}})}{\\exp(z_{\\text{W}}) + \\exp(z_{\\text{B}}) + \\exp(z_{\\text{A}}) + \\exp(z_{\\text{H}})}\n$$\nSubstituting the calculated values of $z_k$:\n$$\nP_{\\text{W}} = \\frac{\\exp(0.90)}{\\exp(0.90) + \\exp(-1.55) + \\exp(-0.89) + \\exp(-0.45)}\n$$\nNow, we evaluate the exponential terms:\n$\\exp(0.90) \\approx 2.459603$\n$\\exp(-1.55) \\approx 0.212248$\n$\\exp(-0.89) \\approx 0.410656$\n$\\exp(-0.45) \\approx 0.637628$\n\nThe sum in the denominator is:\n$$\n\\sum_{j} \\exp(z_j) \\approx 2.459603 + 0.212248 + 0.410656 + 0.637628 = 3.720135\n$$\nFinally, the posterior probability for Wernicke's aphasia is:\n$$\nP_{\\text{W}} \\approx \\frac{2.459603}{3.720135} \\approx 0.66115904\n$$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $6, 6, 1, 1$. The fifth significant digit is $5$, so we round up the fourth digit.\n$$\nP_{\\text{W}} \\approx 0.6612\n$$", "answer": "$$\\boxed{0.6612}$$", "id": "5079533"}, {"introduction": "Once a language function is impaired, we want to understand the underlying neural computations. This practice delves into the analytical heart of functional MRI (fMRI), the General Linear Model (GLM), to demonstrate how researchers can pinpoint brain activity related to a specific cognitive process like lexical processing. You will define a statistical contrast to isolate the brain's response to real words versus pseudowords, a fundamental technique for mapping the functional architecture of the language network, including Wernicke's area. [@problem_id:5079579]", "problem": "In a rapid event-related functional Magnetic Resonance Imaging (fMRI) experiment aimed at probing lexical processing in the posterior Superior Temporal Gyrus (STG; Wernicke’s area), participants view visually presented words and pronounceable pseudowords. Assume the Blood Oxygenation Level Dependent (BOLD) system is approximately Linear Time-Invariant (LTI) over the task timescale, with a hemodynamic response function (HRF) that acts as the impulse response. Starting from the LTI assumption and the definition of convolution, construct a signal model in which the predicted BOLD response to words and pseudowords is formed by convolving their respective stimulus time series with a canonical HRF. Show how this leads to a General Linear Model (GLM) of the form $y = X \\beta + \\varepsilon$ for a single left posterior STG voxel, where $y$ is the sampled BOLD time series, $X$ contains at least two columns corresponding to the convolved word and pseudoword regressors, and additional nuisance columns (such as motion and low-frequency drift) may be included. Clearly state any assumptions needed to justify ordinary least squares estimation at the voxel level, and specify a contrast vector that isolates lexical processing in STG in the sense of greater response to words than pseudowords.\n\nFor a particular left posterior STG voxel, suppose ordinary least squares yields the coefficient estimates for the first two regressors (word and pseudoword) as $\\hat{\\beta}_{\\text{word}} = 0.92$ and $\\hat{\\beta}_{\\text{pseudo}} = 0.50$. Assume the sampling covariance matrix for these two coefficients (i.e., the corresponding $2 \\times 2$ principal submatrix of $\\operatorname{Var}(\\hat{\\beta})$) is\n$$\n\\begin{pmatrix}\n0.0100 & 0.0025\\\\\n0.0025 & 0.0100\n\\end{pmatrix}.\n$$\nUse your derived contrast to compute the corresponding $t$-statistic for the lexical effect in this voxel. Round your final numerical answer to four significant figures. The final answer must be a single real number with no units.", "solution": "The problem is valid as it is scientifically grounded in the principles of fMRI data analysis, is well-posed with sufficient and consistent information, and is stated objectively. We can proceed with a solution.\n\nThe solution is structured in two main parts. First, we will derive the signal model based on the Linear Time-Invariant (LTI) system assumption and construct the General Linear Model (GLM). Second, we will use the provided numerical data to compute the requested $t$-statistic.\n\n**Part 1: The General Linear Model for fMRI**\n\nThe core assumption is that the system mapping neural activity to the observed Blood Oxygenation Level Dependent (BOLD) signal is approximately Linear and Time-Invariant (LTI). In an LTI system, the output signal is the convolution of the input signal with the system's impulse response.\n\nLet $s(t)$ represent the neural activity over time, which acts as the input to the hemodynamic system. In event-related fMRI, this input is modeled as a series of impulses (Dirac delta functions) at the onset times of the stimuli. Let $h(t)$ be the hemodynamic response function (HRF), which is the BOLD response to a single, infinitesimally brief impulse of neural activity. The predicted BOLD signal, $y_{\\text{predicted}}(t)$, is then the convolution of $s(t)$ and $h(t)$, denoted by $(s * h)(t)$:\n$$y_{\\text{predicted}}(t) = (s * h)(t) = \\int_{-\\infty}^{\\infty} s(\\tau) h(t - \\tau) d\\tau$$\n\nIn this experiment, there are two distinct stimulus types: words and pseudowords. Let their respective stimulus time series be $s_{\\text{word}}(t)$ and $s_{\\text{pseudo}}(t)$. Each function is a train of delta functions representing the onsets of word or pseudoword stimuli. Due to the linearity property of the system, the total response is the sum of the responses to each stimulus type, scaled by their respective response amplitudes, $\\beta_{\\text{word}}$ and $\\beta_{\\text{pseudo}}$:\n$$y_{\\text{predicted}}(t) = \\beta_{\\text{word}} (s_{\\text{word}} * h)(t) + \\beta_{\\text{pseudo}} (s_{\\text{pseudo}} * h)(t)$$\n\nfMRI measures the BOLD signal at discrete time points, $t_i$, for $i = 1, 2, \\dots, N$, where $N$ is the total number of scans. The measured BOLD signal in a voxel, $y(t_i)$, is modeled as the sum of the predicted signal, contributions from nuisance sources (e.g., head motion, low-frequency drift), and random error, $\\varepsilon(t_i)$. The complete model is:\n$$y(t_i) = \\beta_{\\text{word}} (s_{\\text{word}} * h)(t_i) + \\beta_{\\text{pseudo}} (s_{\\text{pseudo}} * h)(t_i) + \\sum_{j=1}^{M} \\beta_j g_j(t_i) + \\varepsilon(t_i)$$\nwhere $g_j(t_i)$ are the values of the $M$ nuisance regressors at time $t_i$, with corresponding coefficients $\\beta_j$.\n\nThis equation can be expressed in matrix form, which defines the General Linear Model (GLM). Let $\\mathbf{y}$ be an $N \\times 1$ column vector of the observed BOLD time series, where $y_i = y(t_i)$. Let $\\mathbf{x}_{\\text{word}}$ be an $N \\times 1$ column vector where the $i$-th element is $(s_{\\text{word}} * h)(t_i)$. This is the \"word regressor\". Similarly, let $\\mathbf{x}_{\\text{pseudo}}$ be the \"pseudoword regressor\". Let $\\mathbf{x}_j$ be the column vectors for the nuisance regressors. The design matrix, $\\mathbf{X}$, is formed by concatenating these column vectors:\n$$\\mathbf{X} = \\begin{pmatrix} \\mathbf{x}_{\\text{word}} & \\mathbf{x}_{\\text{pseudo}} & \\mathbf{x}_1 & \\cdots & \\mathbf{x}_M \\end{pmatrix}$$\nLet $\\boldsymbol{\\beta}$ be the column vector of parameters to be estimated:\n$$\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_{\\text{word}} & \\beta_{\\text{pseudo}} & \\beta_1 & \\cdots & \\beta_M \\end{pmatrix}^T$$\nLet $\\boldsymbol{\\varepsilon}$ be the $N \\times 1$ column vector of errors. The GLM is then expressed as:\n$$\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$$\n\nFor ordinary least squares (OLS) estimation of $\\boldsymbol{\\beta}$ to provide the Best Linear Unbiased Estimator (BLUE), the Gauss-Markov assumptions regarding the error term $\\boldsymbol{\\varepsilon}$ must hold:\n1.  **Zero Mean**: The expectation of the errors is zero, $E[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$.\n2.  **Homoscedasticity and No Autocorrelation**: The errors are uncorrelated and have constant variance. This is expressed as the covariance matrix of the errors being a scalar multiple of the identity matrix: $\\operatorname{Var}(\\boldsymbol{\\varepsilon}) = E[\\boldsymbol{\\varepsilon} \\boldsymbol{\\varepsilon}^T] = \\sigma^2 \\mathbf{I}$, where $\\mathbf{I}$ is the $N \\times N$ identity matrix and $\\sigma^2$ is the error variance.\nFor statistical inference (such as the $t$-test below), an additional assumption is that the errors are normally distributed, i.e., $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$.\n\nThe research question is to isolate lexical processing, specified as a greater response to words than to pseudowords. This translates to the statistical hypothesis $H_1: \\beta_{\\text{word}} > \\beta_{\\text{pseudo}}$, or equivalently, $\\beta_{\\text{word}} - \\beta_{\\text{pseudo}} > 0$. This is a linear contrast on the model parameters. This contrast can be written as $\\mathbf{c}^T \\boldsymbol{\\beta}$, where $\\mathbf{c}$ is a contrast vector. Given the ordering of parameters in $\\boldsymbol{\\beta}$ as $(\\beta_{\\text{word}}, \\beta_{\\text{pseudo}}, \\dots)$, the contrast vector to test this hypothesis is:\n$$\\mathbf{c} = \\begin{pmatrix} 1 & -1 & 0 & \\cdots & 0 \\end{pmatrix}^T$$\n\n**Part 2: Calculation of the $t$-statistic**\n\nThe $t$-statistic for a general linear contrast $\\mathbf{c}^T \\boldsymbol{\\beta}$ is given by:\n$$t = \\frac{\\mathbf{c}^T \\hat{\\boldsymbol{\\beta}}}{\\text{SE}(\\mathbf{c}^T \\hat{\\boldsymbol{\\beta}})} = \\frac{\\mathbf{c}^T \\hat{\\boldsymbol{\\beta}}}{\\sqrt{\\mathbf{c}^T \\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}) \\mathbf{c}}}$$\nwhere $\\hat{\\boldsymbol{\\beta}}$ is the OLS estimate of $\\boldsymbol{\\beta}$, and $\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}})$ is its sampling covariance matrix.\n\nWe are given the following information for a specific voxel:\nThe coefficient estimates for the first two regressors:\n$$\\hat{\\beta}_{\\text{word}} = 0.92$$\n$$\\hat{\\beta}_{\\text{pseudo}} = 0.50$$\nLet $\\hat{\\boldsymbol{\\beta}}_{\\text{sub}} = \\begin{pmatrix} 0.92 \\\\ 0.50 \\end{pmatrix}$.\n\nThe corresponding $2 \\times 2$ sampling covariance submatrix is:\n$$\\mathbf{C}_{\\text{sub}} = \\operatorname{Var} \\left( \\begin{pmatrix} \\hat{\\beta}_{\\text{word}} \\\\ \\hat{\\beta}_{\\text{pseudo}} \\end{pmatrix} \\right) = \\begin{pmatrix} 0.0100 & 0.0025 \\\\ 0.0025 & 0.0100 \\end{pmatrix}$$\nThe relevant part of the contrast vector is $\\mathbf{c}_{\\text{sub}} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\nFirst, we compute the value of the contrast using the estimated coefficients (the numerator of the $t$-statistic):\n$$\\mathbf{c}_{\\text{sub}}^T \\hat{\\boldsymbol{\\beta}}_{\\text{sub}} = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 0.92 \\\\ 0.50 \\end{pmatrix} = (1)(0.92) + (-1)(0.50) = 0.92 - 0.50 = 0.42$$\n\nNext, we compute the variance of the contrast estimate (the term under the square root in the denominator):\n$$\\text{Var}(\\mathbf{c}_{\\text{sub}}^T \\hat{\\boldsymbol{\\beta}}_{\\text{sub}}) = \\mathbf{c}_{\\text{sub}}^T \\mathbf{C}_{\\text{sub}} \\mathbf{c}_{\\text{sub}}$$\n$$\\text{Var}(\\mathbf{c}_{\\text{sub}}^T \\hat{\\boldsymbol{\\beta}}_{\\text{sub}}) = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 0.0100 & 0.0025 \\\\ 0.0025 & 0.0100 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$$\n$$\\text{Var}(\\mathbf{c}_{\\text{sub}}^T \\hat{\\boldsymbol{\\beta}}_{\\text{sub}}) = \\begin{pmatrix} 1 \\times 0.0100 - 1 \\times 0.0025 & 1 \\times 0.0025 - 1 \\times 0.0100 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$$\n$$\\text{Var}(\\mathbf{c}_{\\text{sub}}^T \\hat{\\boldsymbol{\\beta}}_{\\text{sub}}) = \\begin{pmatrix} 0.0075 & -0.0075 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$$\n$$\\text{Var}(\\mathbf{c}_{\\text{sub}}^T \\hat{\\boldsymbol{\\beta}}_{\\text{sub}}) = (0.0075)(1) + (-0.0075)(-1) = 0.0075 + 0.0075 = 0.0150$$\n\nAlternatively, the variance can be computed as:\n$$\\text{Var}(\\hat{\\beta}_{\\text{word}} - \\hat{\\beta}_{\\text{pseudo}}) = \\text{Var}(\\hat{\\beta}_{\\text{word}}) + \\text{Var}(\\hat{\\beta}_{\\text{pseudo}}) - 2 \\text{Cov}(\\hat{\\beta}_{\\text{word}}, \\hat{\\beta}_{\\text{pseudo}})$$\n$$= 0.0100 + 0.0100 - 2(0.0025) = 0.0200 - 0.0050 = 0.0150$$\n\nThe standard error (SE) of the contrast is the square root of this variance:\n$$\\text{SE}(\\mathbf{c}_{\\text{sub}}^T \\hat{\\boldsymbol{\\beta}}_{\\text{sub}}) = \\sqrt{0.0150}$$\n\nFinally, we compute the $t$-statistic:\n$$t = \\frac{0.42}{\\sqrt{0.0150}} \\approx \\frac{0.42}{0.122474487} \\approx 3.429281$$\n\nRounding the result to four significant figures, we get $3.429$.", "answer": "$$\\boxed{3.429}$$", "id": "5079579"}, {"introduction": "The classic view of Wernicke's area comes from lesion studies, and modern neuroscience has supercharged this approach. This advanced practice simulates a Voxel-Based Lesion-Symptom Mapping (VLSM) analysis, a powerful computational method that examines thousands of brain voxels across many patients to create a statistical map linking lesion location to behavioral deficits. By implementing this analysis, you will see how neuroscientists can move beyond a single \"area\" to precisely identify the critical neural tissue underlying functions like auditory comprehension, while also tackling the statistical challenge of multiple comparisons. [@problem_id:5079543]", "problem": "You are tasked with implementing a voxel-based lesion–symptom mapping analysis grounded in neurobiology of language. The conceptual base is that lesions affecting posterior superior temporal cortex (Wernicke’s area) are associated with impaired auditory–verbal comprehension in receptive aphasia. You will simulate lesion masks and Boston Naming Test comprehension subscores for a cohort and then perform a voxel-wise statistical test and multiple comparisons correction.\n\nFundamental base and definitions:\n- Lesion–symptom mapping assumes that if a voxel in Wernicke’s area is lesioned, patients will tend to have lower comprehension subscores. Let $N$ be the number of patients and $V$ be the number of voxels. Let $s_i$ be the comprehension subscore for patient $i$, and let $X_{ij} \\in \\{0,1\\}$ be the binary lesion indicator at voxel $j$ for patient $i$.\n- The two-sample Welch’s $t$-statistic compares the scores of patients with a lesion at voxel $j$ and those without, without assuming equal variances. For a voxel $j$, define group $\\mathcal{L}_j = \\{ i \\mid X_{ij} = 1 \\}$ and group $\\mathcal{U}_j = \\{ i \\mid X_{ij} = 0 \\}$, with sizes $n_1 = |\\mathcal{L}_j|$ and $n_2 = |\\mathcal{U}_j|$. Define sample means $\\bar{s}_1$ and $\\bar{s}_2$, and sample variances $v_1$ and $v_2$ for the two groups. The Welch’s $t$-statistic is\n$$\nt_j = \\frac{\\bar{s}_1 - \\bar{s}_2}{\\sqrt{\\frac{v_1}{n_1} + \\frac{v_2}{n_2}}} \\, ,\n$$\nwith the Welch–Satterthwaite degrees of freedom\n$$\n\\nu_j = \\frac{\\left(\\frac{v_1}{n_1} + \\frac{v_2}{n_2}\\right)^2}{\\frac{\\left(\\frac{v_1}{n_1}\\right)^2}{n_1 - 1} + \\frac{\\left(\\frac{v_2}{n_2}\\right)^2}{n_2 - 1}} \\, .\n$$\nThe two-sided $p$-value for voxel $j$ is\n$$\np_j = 2 \\cdot \\left(1 - F_{t,\\nu_j}\\left(|t_j|\\right)\\right) \\, ,\n$$\nwhere $F_{t,\\nu}$ is the cumulative distribution function of the Student’s $t$ distribution with $\\nu$ degrees of freedom. Only voxels with both groups meeting a minimum sample size are testable; denote the minimum per-group size by $n_{\\min}$.\n\n- False Discovery Rate (FDR) correction controls the expected proportion of false positives among declared discoveries. Use the Benjamini–Hochberg procedure at level $q$, which for $m$ tested voxels with $p$-values $\\{p_1,\\dots,p_m\\}$ sorts them ascending $p_{(1)} \\le \\dots \\le p_{(\\!m)}$, and finds the largest $k$ such that\n$$\np_{(k)} \\le \\frac{k}{m} q \\, .\n$$\nReject (declare significant) all tested voxels with $p \\le p_{(k)}$. Voxels not tested (due to insufficient group sizes) must not contribute to $m$ and must not be declared significant.\n\nData generation model for simulation:\n- Fix $N = 50$ and $V = 200$. Define a Wernicke’s area mask $\\mathcal{W} = \\{80,81,\\dots,119\\}$ (a contiguous block of $40$ voxels) that serves as the ground-truth region associated with comprehension deficits.\n- For each patient $i$, draw a latent Wernicke-lesion indicator $L_i \\sim \\mathrm{Bernoulli}(\\pi)$, where $\\pi$ is the prevalence of Wernicke-centric lesions. Generate a comprehension score\n$$\ns_i = \\mu - \\Delta \\, L_i + \\epsilon_i \\, ,\n$$\nwhere $\\mu$ is the baseline score, $\\Delta$ is the effect size (decrement due to Wernicke-centric lesion), and $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ is noise.\n- For each voxel $j \\in \\mathcal{W}$, draw $X_{ij} \\sim \\mathrm{Bernoulli}(p_{\\mathrm{W} \\mid L_i})$ with $p_{\\mathrm{W} \\mid 1} = p_{\\mathrm{W}|\\mathrm{L}}$ if $L_i = 1$ and $p_{\\mathrm{W} \\mid 0} = p_{\\mathrm{W}|\\neg \\mathrm{L}}$ otherwise. For voxels $j \\notin \\mathcal{W}$, draw $X_{ij} \\sim \\mathrm{Bernoulli}(p_{\\mathrm{NW}})$ independently. This induces a plausible association between Wernicke-region lesions and comprehension impairment while allowing background lesions elsewhere.\n\nImplementation requirements:\n- Use a pseudorandom number generator with a specified integer seed for each test case to ensure reproducibility.\n- Use Welch’s $t$-statistic and the Welch–Satterthwaite degrees of freedom to compute two-sided $p$-values voxel-wise. Exclude voxels from testing if either group size is less than $n_{\\min}$ or if the denominator of $t_j$ is zero.\n- Apply the Benjamini–Hochberg FDR correction at level $q = 0.05$ over the set of tested voxels only.\n- For each test case, output the integer count of significant voxels that lie within the Wernicke mask $\\mathcal{W}$.\n\nTest suite:\nFor all cases, take $N = 50$, $V = 200$, $\\mathcal{W} = \\{80,81,\\dots,119\\}$, $n_{\\min} = 5$, and $q = 0.05$. Each test case is specified by the tuple $(\\mu, \\Delta, \\sigma, \\pi, p_{\\mathrm{W}|\\mathrm{L}}, p_{\\mathrm{W}|\\neg \\mathrm{L}}, p_{\\mathrm{NW}}, \\text{seed})$:\n\n1. Case A (happy path, moderate effect and adequate group sizes): $(10, 3.0, 2.0, 0.5, 0.8, 0.05, 0.1, 12345)$.\n2. Case B (no effect, should yield no significant voxels after FDR): $(10, 0.0, 2.0, 0.5, 0.8, 0.05, 0.1, 12346)$.\n3. Case C (boundary: nearly universal Wernicke lesions make the non-lesioned group too small, leading to zero tested voxels): $(10, 3.0, 2.0, 1.0, 0.98, 0.0, 0.1, 12347)$.\n4. Case D (edge: small effect size likely insufficient after FDR): $(10, 0.5, 2.0, 0.5, 0.7, 0.05, 0.1, 12348)$.\n\nFinal output specification:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). Each result is the integer count of significant voxels within $\\mathcal{W}$ for the corresponding test case, in the same order as listed above. No physical units or angle units apply. Express the FDR level as a decimal $q = 0.05$ as given.", "solution": "The problem presents a valid and well-defined task in computational neurobiology, specifically a simulation of voxel-based lesion–symptom mapping (VLSM). The premises are scientifically grounded in the principles of aphasiology and brain mapping, the statistical methodology is appropriate, and the simulation parameters are complete and unambiguous. I will therefore proceed with a complete solution.\n\nThe core of the task is to implement a computational pipeline that first generates a synthetic dataset of patient lesion masks and behavioral scores, and then applies a voxel-wise statistical analysis to identify brain regions associated with the behavioral deficit. The design of this solution is guided by principles of reproducibility, statistical rigor, and computational efficiency.\n\nThe overall algorithm proceeds sequentially through the test cases provided. For each case, the following steps are executed:\n\n**1. Data Generation**\n\nThis phase simulates the biological and behavioral data for a cohort of $N$ patients across $V$ voxels, based on a set of generative parameters. Reproducibility is ensured by seeding a pseudorandom number generator (PRNG) with the specified integer seed for each test case.\n\n- **Patient-Level Latent State**: For each of the $N=50$ patients, a latent binary variable $L_i$ is drawn from a Bernoulli distribution, $L_i \\sim \\mathrm{Bernoulli}(\\pi)$. This variable represents whether the patient has a lesion primarily centered in Wernicke's area ($L_i=1$) or not ($L_i=0$). The parameter $\\pi$ dictates the prevalence of such lesions in the simulated cohort.\n\n- **Behavioral Scores**: A comprehension score $s_i$ is generated for each patient. The score is modeled as a linear function of the latent state $L_i$ with additive Gaussian noise: $s_i = \\mu - \\Delta \\cdot L_i + \\epsilon_i$. Here, $\\mu$ is the baseline score for individuals without a Wernicke-centric lesion, $\\Delta$ is the magnitude of the comprehension deficit associated with such a lesion, and $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ is a normally distributed noise term. This model establishes the ground-truth relationship between a specific lesion type and the behavioral outcome.\n\n- **Voxel-Level Lesion Masks**: A binary lesion matrix $X$ of size $N \\times V$ (where $V=200$) is generated. The probability of a lesion at a given voxel $j$ for patient $i$, $X_{ij}=1$, depends on whether the voxel is part of the designated Wernicke's area mask, $\\mathcal{W} = \\{80, 81, \\dots, 119\\}$, and on the patient's latent state $L_i$.\n    - If voxel $j$ is within Wernicke's area ($j \\in \\mathcal{W}$), the lesion probability is conditioned on $L_i$: $P(X_{ij}=1) = p_{\\mathrm{W}|\\mathrm{L}}$ if $L_i=1$, and $P(X_{ij}=1) = p_{\\mathrm{W}|\\neg\\mathrm{L}}$ if $L_i=0$.\n    - If voxel $j$ is outside Wernicke's area ($j \\notin \\mathcal{W}$), the lesion probability is a constant background rate, $P(X_{ij}=1) = p_{\\mathrm{NW}}$, independent of $L_i$.\nThis probabilistic model creates the desired correlation structure: lesions within $\\mathcal{W}$ are statistically associated with lower scores $s_i$ (via the shared dependence on $L_i$), while lesions outside $\\mathcal{W}$ are not.\n\n**2. Voxel-Wise Statistical Analysis**\n\nThis phase aims to recover the ground-truth association by testing each voxel independently.\n\n- **Group Comparison**: For each voxel $j=0, \\dots, V-1$, the cohort of $N$ patients is partitioned into two groups: those with a lesion at voxel $j$ (group $\\mathcal{L}_j$, where $X_{ij}=1$) and those without (group $\\mathcal{U}_j$, where $X_{ij}=0$).\n\n- **Testability Criteria**: A voxel is considered testable only if it meets minimum sample size requirements for both groups. Specifically, the number of patients in the lesioned group, $n_1 = |\\mathcal{L}_j|$, and the number in the unlesioned group, $n_2 = |\\mathcal{U}_j|$, must both be greater than or equal to a minimum threshold, $n_{\\min}=5$. Furthermore, voxels leading to a degenerate statistical test (e.g., zero variance in both groups, resulting in an undefined t-statistic) are excluded.\n\n- **Welch's t-test**: For each testable voxel, a two-sample Welch's $t$-test is performed on the comprehension scores $s_i$ of the two groups. This test is chosen because it does not assume equal variances between the lesioned and unlesioned groups, a prudent assumption in lesion studies. The test yields a $t$-statistic $t_j$ and a corresponding two-sided $p$-value $p_j$. The $p$-value is calculated using the Student's $t$-distribution with degrees of freedom $\\nu_j$ estimated by the Welch-Satterthwaite formula. The `scipy.stats.ttest_ind` function with the parameter `equal_var=False` provides a robust and numerically stable implementation of this entire procedure.\n\n**3. Multiple Comparisons Correction**\n\nPerforming a statistical test at every voxel incurs a multiple comparisons problem, inflating the rate of false positive findings. To address this, the False Discovery Rate (FDR) is controlled using the Benjamini-Hochberg (B-H) procedure.\n\n- **Procedure**: The B-H procedure is applied only to the set of $m$ $p$-values from the testable voxels.\n    1. The $m$ $p$-values are sorted in ascending order: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$.\n    2. The largest index $k$ (where $k \\in \\{1, \\dots, m\\}$) is found such that the $k$-th sorted $p$-value satisfies the condition $p_{(k)} \\le \\frac{k}{m} q$, where $q=0.05$ is the desired FDR level.\n    3. If such a $k$ is found, all tested voxels whose original (unsorted) $p$-value is less than or equal to $p_{(k)}$ are declared statistically significant. If no such $k$ exists, no voxels are declared significant.\n\n**4. Final Metric Calculation**\n\nThe final step is to quantify the analysis outcome for each test case. The problem requires reporting the number of significant voxels that are located within the true Wernicke's area mask, $\\mathcal{W}$. This count serves as a measure of true positives, reflecting the power of the VLSM analysis to correctly identify the region associated with the simulated deficit under different conditions (e.g., varying effect size, lesion prevalence). The final output is a list of these integer counts, one for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import ttest_ind\n\ndef run_analysis(N, V, wernicke_mask_indices, n_min, q_level, params):\n    \"\"\"\n    Runs the full lesion-symptom mapping simulation and analysis for one test case.\n    \"\"\"\n    mu, delta, sigma, pi, p_W_L, p_W_notL, p_NW, seed = params\n    \n    rng = np.random.default_rng(seed)\n\n    # 1. Data Generation\n    # Generate latent Wernicke-lesion indicators\n    L = rng.binomial(1, pi, size=N)\n    \n    # Generate comprehension scores based on latent state and noise\n    epsilon = rng.normal(loc=0, scale=sigma, size=N)\n    s = mu - delta * L + epsilon\n    \n    # Generate lesion mask matrix X (N x V)\n    X = np.zeros((N, V), dtype=np.int8)\n    num_wernicke_voxels = len(wernicke_mask_indices)\n    \n    for i in range(N):\n        if L[i] == 1:\n            # Lesion probability for Wernicke's area voxels\n            p_W = p_W_L\n        else:\n            p_W = p_W_notL\n        \n        # Generate lesions in Wernicke's area\n        if num_wernicke_voxels > 0:\n            X[i, wernicke_mask_indices] = rng.binomial(1, p_W, size=num_wernicke_voxels)\n        \n        # Generate lesions in non-Wernicke's area\n        # This can be done more efficiently outside the loop\n    \n    # Vectorized generation of non-Wernicke lesions\n    non_wernicke_mask = np.ones(V, dtype=bool)\n    non_wernicke_mask[wernicke_mask_indices] = False\n    num_non_wernicke_voxels = V - num_wernicke_voxels\n    if num_non_wernicke_voxels > 0:\n        X[:, non_wernicke_mask] = rng.binomial(1, p_NW, size=(N, num_non_wernicke_voxels))\n\n    # 2. Voxel-wise Statistical Analysis\n    tested_voxels_pvals = []\n    tested_voxels_indices = []\n\n    for j in range(V):\n        lesion_mask = X[:, j] == 1\n        scores_lesioned = s[lesion_mask]\n        scores_unlesioned = s[~lesion_mask]\n        \n        n1 = len(scores_lesioned)\n        n2 = len(scores_unlesioned)\n        \n        # Check testability criteria\n        if n1  n_min or n2  n_min:\n            continue\n            \n        # Perform Welch's t-test\n        # equal_var=False ensures Welch's t-test is used.\n        # nan_policy='omit' can also be used, but let's be explicit\n        # scipy's ttest_ind handles zero variance cases by returning inf or nan.\n        # We need to filter out nans, which correspond to the 0/0 case.\n        t_stat, p_val = ttest_ind(scores_lesioned, scores_unlesioned, equal_var=False, nan_policy='propagate')\n        \n        if np.isnan(p_val):\n            continue\n            \n        tested_voxels_pvals.append(p_val)\n        tested_voxels_indices.append(j)\n\n    # 3. False Discovery Rate (FDR) Correction\n    significant_voxel_indices = []\n    m = len(tested_voxels_pvals)\n\n    if m > 0:\n        # Sort p-values and retain original indices\n        pvals_array = np.array(tested_voxels_pvals)\n        sort_indices = np.argsort(pvals_array)\n        sorted_pvals = pvals_array[sort_indices]\n        \n        # Calculate Benjamini-Hochberg thresholds\n        k_values = np.arange(1, m + 1)\n        bh_thresholds = (k_values / m) * q_level\n        \n        # Find p-values below the BH threshold\n        pvals_below_threshold = sorted_pvals[sorted_pvals = bh_thresholds]\n        \n        if len(pvals_below_threshold) > 0:\n            # The significance threshold is the maximum p-value that met the criterion\n            p_threshold = np.max(pvals_below_threshold)\n            \n            # Identify all original voxels with p-values = this threshold\n            significant_mask = pvals_array = p_threshold\n            original_indices = np.array(tested_voxels_indices)\n            significant_voxel_indices = list(original_indices[significant_mask])\n\n    # 4. Final Metric Calculation\n    # Count how many significant voxels are in the Wernicke's area mask\n    count = 0\n    w_set = set(wernicke_mask_indices)\n    for idx in significant_voxel_indices:\n        if idx in w_set:\n            count += 1\n            \n    return count\n\ndef solve():\n    # Define global parameters and test cases from the problem statement.\n    N = 50\n    V = 200\n    wernicke_mask_indices = list(range(80, 120))\n    n_min = 5\n    q_level = 0.05\n\n    test_cases = [\n        # Case A: (mu, delta, sigma, pi, p_W|L, p_W|~L, p_NW, seed)\n        (10, 3.0, 2.0, 0.5, 0.8, 0.05, 0.1, 12345),\n        # Case B:\n        (10, 0.0, 2.0, 0.5, 0.8, 0.05, 0.1, 12346),\n        # Case C:\n        (10, 3.0, 2.0, 1.0, 0.98, 0.0, 0.1, 12347),\n        # Case D:\n        (10, 0.5, 2.0, 0.5, 0.7, 0.05, 0.1, 12348),\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = run_analysis(N, V, wernicke_mask_indices, n_min, q_level, case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "5079543"}]}