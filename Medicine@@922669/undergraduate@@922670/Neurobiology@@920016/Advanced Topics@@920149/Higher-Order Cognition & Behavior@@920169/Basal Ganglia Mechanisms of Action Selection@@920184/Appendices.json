{"hands_on_practices": [{"introduction": "To understand how the brain selects an action, we can start with a simplified model of the basal ganglia's core circuitry. This practice [@problem_id:5001161] treats the main pathways as inputs that compete to control the output of the Globus Pallidus internal segment ($GPi$), a key gateway for motor commands. By manipulating the strengths of the \"Go\" and \"No-Go\" pathways in a simple equation, you'll develop a quantitative feel for how the brain can either release or suppress an action.", "problem": "Action selection in the basal ganglia depends on the balance of pathway activities that converge on the Globus Pallidus internal segment (GPi). In a standard rate-based model, the GPi output is modeled as a weighted sum of pathway activities, where the direct pathway ($D$) reduces GPi output, and the indirect pathway ($I$) and hyperdirect pathway ($H$) increase GPi output. The GPi output is given by the linear form $$g = g_{0} - w_{d} D + w_{i} I + w_{h} H,$$ where $g_{0}$ is a baseline offset and $w_{d}$, $w_{i}$, and $w_{h}$ are positive weights reflecting pathway efficacy. The hyperdirect pathway is the cortico–Subthalamic Nucleus (STN)–GPi route, and the indirect pathway is the striatum–external Globus Pallidus (GPe)–STN–GPi route. The direct pathway is the striatum–GPi route. Assume a transient neuromodulatory event causes a fractional increase in pathway activities: the direct pathway increases by $20\\%$ and the hyperdirect pathway increases by $10\\%$, while the indirect pathway remains unchanged. Using the above model and the values $$w_{d} = 2,\\quad w_{i} = 1.5,\\quad w_{h} = 1,$$ compute the change in GPi output $$\\Delta g = g' - g$$ induced by these changes, where $g'$ is the GPi output after the changes and $g$ is the baseline GPi output before the changes. Express your final answer as a single simplified analytic expression in terms of the baseline activities $D$ and $H$. No numerical rounding is required, and no units are required in the final expression.", "solution": "The problem is valid. It is a well-posed problem in computational neuroscience, grounded in a standard simplified model of basal ganglia function. It is self-contained, objective, and scientifically sound.\n\nThe baseline output of the Globus Pallidus internal segment (GPi), denoted as $g$, is given by the linear model:\n$$g = g_{0} - w_{d} D + w_{i} I + w_{h} H$$\nwhere $g_{0}$ is a baseline offset, $D$, $I$, and $H$ represent the activities of the direct, indirect, and hyperdirect pathways respectively, and $w_{d}$, $w_{i}$, and $w_{h}$ are the positive weights associated with these pathways.\n\nThe problem specifies the following weight values:\n$$w_{d} = 2$$\n$$w_{i} = 1.5$$\n$$w_{h} = 1$$\n\nA neuromodulatory event induces a transient change in the pathway activities. Let the new activities be denoted by $D'$, $I'$, and $H'$.\nThe direct pathway activity, $D$, increases by $20\\%$. The new activity $D'$ is therefore:\n$$D' = D + 0.20 D = 1.2 D$$\nThe hyperdirect pathway activity, $H$, increases by $10\\%$. The new activity $H'$ is:\n$$H' = H + 0.10 H = 1.1 H$$\nThe indirect pathway activity, $I$, remains unchanged, so:\n$$I' = I$$\n\nThe GPi output after these changes, denoted as $g'$, is calculated using the same model but with the new activity levels:\n$$g' = g_{0} - w_{d} D' + w_{i} I' + w_{h} H'$$\nSubstituting the expressions for $D'$, $I'$, and $H'$ into this equation, we get:\n$$g' = g_{0} - w_{d}(1.2 D) + w_{i}(I) + w_{h}(1.1 H)$$\n\nThe objective is to compute the change in GPi output, $\\Delta g$, which is defined as the difference between the new output and the baseline output:\n$$\\Delta g = g' - g$$\nWe substitute the full expressions for $g'$ and $g$:\n$$\\Delta g = (g_{0} - 1.2 w_{d} D + w_{i} I + 1.1 w_{h} H) - (g_{0} - w_{d} D + w_{i} I + w_{h} H)$$\n\nTo simplify, we distribute the negative sign and group like terms:\n$$\\Delta g = g_{0} - 1.2 w_{d} D + w_{i} I + 1.1 w_{h} H - g_{0} + w_{d} D - w_{i} I - w_{h} H$$\nThe baseline offset term $g_{0}$ cancels out. The term for the indirect pathway, $w_{i} I$, also cancels out as its activity was unchanged.\n$$\\Delta g = (-1.2 w_{d} D + w_{d} D) + (1.1 w_{h} H - w_{h} H)$$\nFactoring out $w_{d} D$ and $w_{h} H$ from their respective groups:\n$$\\Delta g = (1 - 1.2) w_{d} D + (1.1 - 1) w_{h} H$$\n$$\\Delta g = -0.2 w_{d} D + 0.1 w_{h} H$$\n\nThis general expression for $\\Delta g$ depends on the baseline activities $D$ and $H$ and their weights. Now, we substitute the given numerical values for the weights $w_{d} = 2$ and $w_{h} = 1$:\n$$\\Delta g = -0.2(2)D + 0.1(1)H$$\nPerforming the scalar multiplication gives the final simplified expression:\n$$\\Delta g = -0.4 D + 0.1 H$$\nThis is the required analytic expression for the change in GPi output in terms of the baseline activities $D$ and $H$.", "answer": "$$\\boxed{-0.4 D + 0.1 H}$$", "id": "5001161"}, {"introduction": "Action selection is not static; it is refined by learning from the consequences of our choices. A key theory suggests that dopamine neurons signal a \"reward prediction error,\" a teaching signal that updates the value of actions. This practice [@problem_id:5001000] has you calculate this error, known as the temporal-difference ($TD$) error, demonstrating how an outcome that is \"better than expected\" can drive learning even without an immediate reward.", "problem": "In the basal ganglia, phasic activity of dopaminergic neurons in the Substantia nigra pars compacta (SNc) is widely modeled as reporting a reward prediction error that trains striatal circuits for action selection. Consider an actor-critic formulation in which the critic maintains a state-value function $V(s)$ and learning is driven by the temporal-difference error $\\delta_t$. Starting from the definition of the state-value function as the expected discounted return, $V(s_t) = \\mathbb{E}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k} \\mid s_t\\right]$, and the Bellman consistency condition $V(s_t) = \\mathbb{E}\\left[r_t + \\gamma V(s_{t+1}) \\mid s_t\\right]$, derive the expression for the sample temporal-difference error $\\delta_t$ in terms of $r_t$, $\\gamma$, $V(s_t)$, and $V(s_{t+1})$. Then, for a single trial in which $r_t = 0$, $\\gamma = 0.9$, $V(s_t) = 0.5$, and $V(s_{t+1}) = 0.6$, compute the numerical value of $\\delta_t$. Briefly explain whether, under the standard interpretation linking $\\delta_t$ to phasic dopaminergic signaling in the striatum, dopamine should increase or decrease relative to baseline for this transition.\n\nReport only the numerical value of $\\delta_t$ as a pure number (arbitrary units). Do not include units and do not round.", "solution": "The problem is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- Definition of the state-value function: $V(s_t) = \\mathbb{E}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k} \\mid s_t\\right]$\n- Bellman consistency condition: $V(s_t) = \\mathbb{E}\\left[r_t + \\gamma V(s_{t+1}) \\mid s_t\\right]$\n- The temporal-difference error is denoted by $\\delta_t$.\n- Task 1: Derive the expression for the sample temporal-difference error $\\delta_t$ in terms of $r_t$, $\\gamma$, $V(s_t)$, and $V(s_{t+1})$.\n- Task 2: For a single trial, compute the numerical value of $\\delta_t$ given the values:\n  - $r_t = 0$\n  - $\\gamma = 0.9$\n  - $V(s_t) = 0.5$\n  - $V(s_{t+1}) = 0.6$\n- Task 3: Briefly explain the implication for phasic dopaminergic signaling.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n- **Scientifically Grounded**: The problem is based on the standard temporal-difference (TD) learning framework, a cornerstone of modern reinforcement learning and computational neuroscience. The definitions of the state-value function and the Bellman equation are correct. The premise that phasic dopamine activity reports a reward prediction error ($\\delta_t$) is a central, well-established, and empirically supported hypothesis in neurobiology. The problem is scientifically sound.\n- **Well-Posed**: The problem is clearly stated. It asks for a standard derivation followed by a numerical calculation using provided, complete data. The existence of a unique, stable, and meaningful solution is guaranteed.\n- **Objective**: The problem uses precise, standard terminology from the field and is free of subjective or ambiguous language.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is scientifically grounded, well-posed, and objective. There are no contradictions, missing information, or violations of fundamental principles. A complete solution will be provided.\n\n### Derivation and Calculation\nThe starting point for deriving the temporal-difference (TD) error is the Bellman consistency condition for the state-value function, $V(s_t)$:\n$$V(s_t) = \\mathbb{E}\\left[r_t + \\gamma V(s_{t+1}) \\mid s_t\\right]$$\nThis equation asserts that the value of the current state, $s_t$, is the expectation of the sum of the immediate reward, $r_t$, and the discounted value of the subsequent state, $s_{t+1}$. The expectation $\\mathbb{E}[\\cdot]$ is taken over the possible rewards and next states, given the current state $s_t$.\n\nIn a real-world scenario or a simulation, an agent experiences a single trajectory, not an expectation over all possible trajectories. For a specific transition at time $t$, the agent moves from state $s_t$ to $s_{t+1}$ and receives a reward $r_t$. From this single sample, we can construct an improved, or updated, estimate of $V(s_t)$. This estimate is the \"TD target\", and it is formed by taking the quantity inside the expectation from the Bellman equation for this specific instance:\n$$ \\text{TD Target} = r_t + \\gamma V(s_{t+1}) $$\nThe term $V(s_{t+1})$ is the current estimate of the value of the next state. The TD target combines the actually received reward $r_t$ with this existing estimate, providing a more informed valuation of $s_t$ than the old value $V(s_t)$.\n\nThe temporal-difference error, $\\delta_t$, is defined as the difference between this new, sample-based estimate (the TD target) and the old estimate, $V(s_t)$.\n$$ \\delta_t = (\\text{TD Target}) - V(s_t) $$\nSubstituting the expression for the TD target, we arrive at the standard formula for the sample temporal-difference error:\n$$ \\delta_t = \\left( r_t + \\gamma V(s_{t+1}) \\right) - V(s_t) $$\nThis expression represents the error in the prediction $V(s_t)$ when compared to the outcome realized one time-step later.\n\nThe problem provides the following values for a single trial:\n- Reward: $r_t = 0$\n- Discount factor: $\\gamma = 0.9$\n- Value of current state: $V(s_t) = 0.5$\n- Value of next state: $V(s_{t+1}) = 0.6$\n\nWe substitute these numerical values into the derived expression for $\\delta_t$:\n$$ \\delta_t = (0 + 0.9 \\times 0.6) - 0.5 $$\nFirst, we compute the product inside the parentheses:\n$$ 0.9 \\times 0.6 = 0.54 $$\nNow, we substitute this result back into the equation for $\\delta_t$:\n$$ \\delta_t = 0.54 - 0.5 $$\n$$ \\delta_t = 0.04 $$\nThe numerical value of the temporal-difference error for this transition is $0.04$.\n\nFinally, we address the link to dopaminergic signaling. The reward prediction error hypothesis posits that phasic changes in dopamine neuron firing encode $\\delta_t$.\n- A positive prediction error ($\\delta_t > 0$) signifies that the outcome was better than expected. This is associated with a phasic increase (a burst) of dopamine neuron firing.\n- A negative prediction error ($\\delta_t  0$) signifies that the outcome was worse than expected. This is associated with a phasic decrease (a pause) in dopamine neuron firing below the baseline tonic rate.\n- A zero prediction error ($\\delta_t = 0$) signifies that the outcome was exactly as expected, resulting in no change to the tonic firing rate.\n\nIn this case, $\\delta_t = 0.04$, which is a positive value. This indicates a positive reward prediction error. Despite the immediate reward being zero ($r_t=0$), the agent transitioned to a state of higher value ($V(s_{t+1}) = 0.6$) than was fully anticipated by its initial state value ($V(s_t) = 0.5$). The discounted value of the next state, $0.9 \\times 0.6 = 0.54$, is greater than the initial value of $0.5$. The outcome was \"better than expected\". Therefore, this event should trigger a phasic increase in dopamine release in the striatum.", "answer": "$$\n\\boxed{0.04}\n$$", "id": "5001000"}, {"introduction": "Once the brain has learned the values of different actions, how does it decide which one to perform? This decision process must balance choosing the known best option (exploitation) with trying other options that might be better (exploration). This exercise [@problem_id:5001106] walks you through the derivation of the softmax function, a cornerstone model in decision neuroscience that translates action values into choice probabilities and formalizes the trade-off between exploitation and exploration.", "problem": "In vertebrate basal ganglia, cortico-striatal representations of expected action values can be modeled as subjective action values $Q_{i}$, and stochastic action selection emerges from competition between direct and indirect pathways modulated by dopamine. A widely used normative assumption in neurobiology and reinforcement learning posits that, at a decision time, the system chooses a probability distribution $p_{i}$ over actions that trades off maximizing expected subjective value $\\sum_{i} p_{i} Q_{i}$ against maintaining behavioral variability, quantified by Shannon entropy $H(p) = -\\sum_{i} p_{i} \\ln p_{i}$. Let the strength of value maximization relative to entropy be controlled by the inverse temperature parameter $\\beta$, so that higher $\\beta$ biases selection toward higher-valued actions and lower $\\beta$ increases exploration.\n\nStarting from this trade-off principle, derive the choice rule that gives $p_{i}$ for a set of actions with values $Q_{i}$ and inverse temperature $\\beta$, and then apply it to a case with $2$ actions and $Q$-values $[1.0, 0.9]$ under $\\beta = 3$. Compute the resulting action probabilities and express your numerical results rounded to $4$ significant figures. No physical units are required. Finally, explain, in neurobiological terms grounded in this optimization principle, how lowering $\\beta$ changes exploration in basal ganglia-driven action selection.", "solution": "The problem requires deriving the probabilistic choice rule for action selection based on a trade-off between maximizing expected value and maximizing entropy, applying this rule to a specific case, and explaining the neurobiological interpretation of the inverse temperature parameter $\\beta$.\n\nFirst, we validate the problem statement.\n1.  **Extracted Givens**:\n    -   Action selection is a competition between direct and indirect pathways in the basal ganglia.\n    -   Action values are denoted by $Q_i$ for a set of actions $i$.\n    -   The choice is a probability distribution $p_i$ over actions.\n    -   The objective is to choose $p_i$ that trades off maximizing expected value, $\\sum_{i} p_{i} Q_{i}$, against maintaining behavioral variability, quantified by Shannon entropy $H(p) = -\\sum_{i} p_{i} \\ln p_{i}$.\n    -   The inverse temperature parameter $\\beta$ controls the strength of value maximization relative to entropy.\n    -   Task 1: Derive the choice rule for $p_i(Q_i, \\beta)$.\n    -   Task 2: Apply the rule for a case with $2$ actions, $Q$-values $[1.0, 0.9]$, and $\\beta = 3$.\n    -   Task 3: Compute probabilities rounded to $4$ significant figures.\n    -   Task 4: Explain neurobiologically how lowering $\\beta$ changes exploration.\n\n2.  **Validation**:\n    -   **Scientific Grounding**: The problem is grounded in computational neuroscience and reinforcement learning principles commonly used to model decision-making and basal ganglia function. The trade-off between value maximization (exploitation) and entropy maximization (exploration) is a fundamental concept, and its formalization via the softmax function is a standard model.\n    -   **Well-Posedness**: The problem is well-posed. It describes a constrained optimization problem for which a unique solution exists. The provided data is sufficient to perform the derivation and the calculation.\n    -   **Objectivity**: The problem is stated using precise, objective, and standard terminology from the relevant scientific fields.\n\n3.  **Verdict**: The problem is valid.\n\nWe now proceed with the solution.\n\n**Part 1: Derivation of the Choice Rule**\n\nThe principle is to find the probability distribution $\\{p_i\\}$ that maximizes a function representing the trade-off between expected value and entropy. The problem states that $\\beta$ controls the strength of value maximization relative to entropy. This trade-off can be formalized by maximizing the following functional, often called the \"free energy\" in this context:\n$$F(p) = \\beta \\sum_{i} p_i Q_i + H(p) = \\beta \\sum_{i} p_i Q_i - \\sum_{i} p_i \\ln p_i$$\nThis maximization must be performed under the constraint that $\\{p_i\\}$ is a probability distribution, which means $\\sum_i p_i = 1$ and $p_i \\ge 0$ for all $i$. The non-negativity is typically satisfied by the form of the solution. We enforce the normalization constraint using the method of Lagrange multipliers.\n\nThe Lagrangian $\\mathcal{L}$ is constructed as:\n$$\\mathcal{L}(\\{p_i\\}, \\lambda) = \\left( \\beta \\sum_{i} p_i Q_i - \\sum_{i} p_i \\ln p_i \\right) - \\lambda \\left( \\sum_{i} p_i - 1 \\right)$$\nwhere $\\lambda$ is the Lagrange multiplier. To find the optimal distribution, we take the partial derivative of $\\mathcal{L}$ with respect to an arbitrary component $p_k$ and set it to zero.\n$$\\frac{\\partial \\mathcal{L}}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left( \\beta p_k Q_k - p_k \\ln p_k - \\lambda p_k \\right) = 0$$\nUsing the product rule for the derivative of $p_k \\ln p_k$, which is $\\frac{d}{dx}(x \\ln x) = \\ln x + 1$, we get:\n$$\\frac{\\partial \\mathcal{L}}{\\partial p_k} = \\beta Q_k - (\\ln p_k + 1) - \\lambda = 0$$\nNow, we solve for $p_k$:\n$$\\ln p_k = \\beta Q_k - 1 - \\lambda$$\n$$p_k = \\exp(\\beta Q_k - 1 - \\lambda)$$\nWe can separate the exponential term:\n$$p_k = \\exp(\\beta Q_k) \\exp(-1 - \\lambda)$$\nThe term $\\exp(-1 - \\lambda)$ is a constant that does not depend on the action index $k$. Let's denote it as $1/Z$.\n$$p_k = \\frac{\\exp(\\beta Q_k)}{Z}$$\nTo determine the constant $Z$, we apply the normalization constraint $\\sum_k p_k = 1$:\n$$\\sum_k p_k = \\sum_k \\frac{\\exp(\\beta Q_k)}{Z} = \\frac{1}{Z} \\sum_k \\exp(\\beta Q_k) = 1$$\nSolving for $Z$, we find that it is the sum over all actions, often called the partition function:\n$$Z = \\sum_i \\exp(\\beta Q_i)$$\nSubstituting this back into the expression for $p_k$, we arrive at the choice rule:\n$$p_k = \\frac{\\exp(\\beta Q_k)}{\\sum_i \\exp(\\beta Q_i)}$$\nThis is the Boltzmann distribution, commonly known as the softmax function in the context of machine learning and computational neuroscience.\n\n**Part 2: Application to the Specific Case**\n\nWe are given a scenario with $2$ actions, let's call them action $1$ and action $2$.\nThe Q-values are $Q_1 = 1.0$ and $Q_2 = 0.9$.\nThe inverse temperature is $\\beta = 3$.\n\nFirst, we calculate the normalization factor $Z$:\n$$Z = \\sum_{i=1}^{2} \\exp(\\beta Q_i) = \\exp(\\beta Q_1) + \\exp(\\beta Q_2)$$\n$$Z = \\exp(3 \\times 1.0) + \\exp(3 \\times 0.9) = \\exp(3) + \\exp(2.7)$$\n\nNow we compute the probability for each action.\nFor action $1$:\n$$p_1 = \\frac{\\exp(\\beta Q_1)}{Z} = \\frac{\\exp(3)}{\\exp(3) + \\exp(2.7)}$$\nFor action $2$:\n$$p_2 = \\frac{\\exp(\\beta Q_2)}{Z} = \\frac{\\exp(2.7)}{\\exp(3) + \\exp(2.7)}$$\nTo compute the numerical values:\n$$p_1 = \\frac{1}{1 + \\exp(2.7 - 3)} = \\frac{1}{1 + \\exp(-0.3)}$$\n$$p_2 = \\frac{1}{\\exp(3 - 2.7) + 1} = \\frac{1}{\\exp(0.3) + 1}$$\nUsing the values $\\exp(-0.3) \\approx 0.740818$ and $\\exp(0.3) \\approx 1.349859$:\n$$p_1 \\approx \\frac{1}{1 + 0.740818} = \\frac{1}{1.740818} \\approx 0.574442$$\n$$p_2 \\approx \\frac{1}{1.349859 + 1} = \\frac{1}{2.349859} \\approx 0.425557$$\nRounding these results to $4$ significant figures, we get:\n$p_1 \\approx 0.5744$\n$p_2 \\approx 0.4256$\nAs a check, the sum is $0.5744 + 0.4256 = 1.0000$.\n\n**Part 3: Neurobiological Explanation**\n\nThe inverse temperature parameter $\\beta$ governs the stochasticity of action selection. Lowering $\\beta$ increases exploration in basal ganglia-driven action selection. This can be understood by examining the behavior of the derived choice rule, $p_i \\propto \\exp(\\beta Q_i)$.\n\nIn neurobiological terms, the action values $Q_i$ are encoded by the strength of cortico-striatal synapses. The action selection process involves competition between the \"Go\" (direct) and \"NoGo\" (indirect) pathways of the basal ganglia, which is heavily modulated by dopamine.\n\n-   A **high $\\beta$** value corresponds to a low-temperature, deterministic system. In this regime, even small differences in $Q$-values are amplified by the exponential function. The action with the highest $Q$-value will have a disproportionately high probability of being selected. This corresponds to a state of **exploitation**, where the system reliably chooses the action it has learned to be the most rewarding. This is thought to be associated with high tonic dopamine levels, which enhance the signal of the most valuable action and effectively sharpen the competition in its favor.\n\n-   A **low $\\beta$** value (approaching $0$) corresponds to a high-temperature, highly stochastic system. As $\\beta \\rightarrow 0$, the term $\\beta Q_i \\rightarrow 0$ for all $i$. Consequently, $\\exp(\\beta Q_i) \\rightarrow 1$ for all actions. The probabilities $p_i$ approach a uniform distribution, $p_i \\approx 1/N$ where $N$ is the number of available actions. This means action selection becomes nearly random and insensitive to the learned values $Q_i$. This corresponds to a state of **exploration**, where the system tries out different actions, including those that are currently deemed suboptimal. This is crucial for discovering new sources of reward or adapting to a changing environment. This state might be associated with lower tonic dopamine levels or the influence of other neuromodulators (like norepinephrine or acetylcholine) that increase behavioral variability and reduce the system's reliance on pre-existing value estimates.\n\nTherefore, lowering $\\beta$ flattens the probability distribution over actions, making selection less dependent on learned values and increasing the propensity to choose actions other than the one with the highest expected value. This mechanism allows the organism to flexibly switch between exploiting known resources and exploring for new ones.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.5744  0.4256\n\\end{pmatrix}\n}\n$$", "id": "5001106"}]}