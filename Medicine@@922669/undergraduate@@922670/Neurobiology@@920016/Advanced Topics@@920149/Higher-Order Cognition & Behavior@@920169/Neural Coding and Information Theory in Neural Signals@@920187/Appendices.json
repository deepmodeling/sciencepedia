{"hands_on_practices": [{"introduction": "A fundamental step in understanding neural codes is to characterize the variability of a neuron's response. While the Poisson process provides a simple and useful baseline model for spike generation, real neurons often show different statistical properties. This practice will guide you through calculating the Fano factor, a key metric for spike count variability, and using it to statistically test whether observed neural data deviates from the Poisson model [@problem_id:5037305].", "problem": "You are given finite sequences of nonnegative integer spike counts observed from a single neuron across equal-duration, non-overlapping time windows. Assume stationarity so that all windows share a common underlying mean rate and independence across windows. Starting from first principles in neural coding and statistical inference, design a program that, for each sequence, estimates the dispersion of spike counts and tests whether the data deviate from a Poisson process. The design must adhere to the following requirements.\n\nFundamental base:\n- The spike count in a window is modeled as a random variable $X$ taking values in $\\{0,1,2,\\dots\\}$.\n- The sample mean is defined as $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$ for $n$ windows, and the unbiased sample variance is defined as $s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_i - \\bar{x})^2$.\n- The Fano factor used in neural coding is defined from the underlying distribution as $F = \\frac{\\mathrm{Var}[X]}{\\mathbb{E}[X]}$. For a Poisson process with rate parameter that is constant across windows, the equidispersion property gives $\\mathrm{Var}[X] = \\mathbb{E}[X]$, implying $F = 1$.\n- The index of dispersion, defined as a scaled variance-to-mean ratio, has an approximate chi-square distribution with $n-1$ degrees of freedom under the equidispersion property when the common mean is estimated from the data.\n\nTask:\n- For each test case sequence $\\{x_i\\}_{i=1}^{n}$, compute the estimate of the Fano factor $\\hat{F}$ using only $\\bar{x}$ and $s^2$.\n- Construct a two-sided confidence interval for the dispersion parameter $\\phi$ in the model $\\mathrm{Var}[X] = \\phi \\,\\mathbb{E}[X]$ using the chi-square approximation to the index of dispersion. Use confidence level $0.95$, which corresponds to significance level $\\alpha = 0.05$.\n- Decide deviation from Poisson by checking whether the value $1$ lies inside the constructed confidence interval for $\\phi$. Return a boolean that is true if the data deviate from Poisson and false otherwise.\n\nImportant details:\n- If $\\bar{x} = 0$, do not attempt to compute $\\hat{F}$ or the confidence interval; instead return a non-number for the undefined quantities that cannot be computed from the data.\n- If $n = 1$, treat the sample variance as undefined and follow the same practice of returning a non-number for quantities that require variance.\n- The program must not assume any angles or physical units in its input or output.\n- The program must output, for each test case, a quadruple $(\\hat{F}, L, U, B)$ where $L$ and $U$ are the lower and upper bounds of the confidence interval for $\\phi$ at confidence level $0.95$, and $B$ is the boolean decision for deviation from Poisson based on whether $1$ lies outside $[L, U]$.\n\nTest suite:\nUse the following five sequences of spike counts as the input test suite. Each sequence is across equal-duration time windows and is listed in order.\n1. A typical case with moderate dispersion: $[2,3,1,2,0,4,2,1,3,2,2,1,3,2,1,2,3,1,2,2]$.\n2. An overdispersed case: $[0,7,2,10,3,8,1,9,4,6,0,11]$.\n3. An underdispersed constant case: $[3,3,3,3,3,3,3,3,3,3]$.\n4. A small-sample boundary case: $[0,1,0]$.\n5. A low-mean sparse case: $[0,0,1,0,1,0,1,0,0,1]$.\n\nFinal output format:\nYour program should produce a single line of output containing the results for the five test cases as a comma-separated list enclosed in square brackets. Each element of the list must itself be a tuple of the form $(\\hat{F}, L, U, B)$, where the first three elements are floating-point numbers and the last element is a boolean. For example, the output should look like $[(\\hat{F}_1, L_1, U_1, B_1), (\\hat{F}_2, L_2, U_2, B_2), \\dots]$ with no additional text.", "solution": "The problem requires the design of a statistical procedure to test whether a series of neural spike counts deviates from a Poisson process. This is accomplished by estimating the dispersion of the counts and constructing a confidence interval for a dispersion parameter. The entire analysis is predicated on foundational principles of statistical inference as applied to neuroscience.\n\nThe core of the problem lies in quantifying the variability of spike counts relative to their mean. For a Poisson process, a key characteristic is equidispersion, where the variance equals the mean. Deviations from this property, manifesting as overdispersion (variance greater than the mean) or underdispersion (variance less than the mean), suggest that the underlying spike generation mechanism is not a simple Poisson process.\n\nLet the sequence of observed non-negative integer spike counts from a single neuron across $n$ equal, non-overlapping time windows be $\\{x_1, x_2, \\dots, x_n\\}$. We model these counts as independent and identically distributed samples from a random variable $X$.\n\nThe sample mean, $\\bar{x}$, is an estimator for the true mean $\\mu = \\mathbb{E}[X]$:\n$$ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i $$\nThe unbiased sample variance, $s^2$, is an estimator for the true variance $\\sigma^2 = \\mathrm{Var}[X]$:\n$$ s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 $$\nA central quantity in neural spike train analysis is the Fano factor, defined for the underlying distribution as $F = \\frac{\\mathrm{Var}[X]}{\\mathbb{E}[X]}$. For a Poisson process, $F=1$. A natural estimator for the Fano factor, which we denote $\\hat{F}$, is the ratio of the sample variance to the sample mean:\n$$ \\hat{F} = \\frac{s^2}{\\bar{x}} $$\nTo formalize the test for deviation from a Poisson process, we adopt the generalized model $\\mathrm{Var}[X] = \\phi \\mathbb{E}[X]$, where $\\phi$ is a dimensionless dispersion parameter. In this model, the Fano factor $F$ is identical to $\\phi$. The null hypothesis, $H_0$, that the process is Poisson, is equivalent to testing $H_0: \\phi = 1$. The alternative hypothesis, $H_1$, is that the process is not Poisson, which corresponds to $\\phi \\neq 1$.\n\nTo test this hypothesis, we construct a confidence interval for $\\phi$. This is achieved using a pivotal quantity, whose sampling distribution is (at least approximately) known and independent of the parameter $\\phi$. Based on the problem statement, we use the fact that the index of dispersion statistic, under certain assumptions (such as normality of data, which is approximated by a Poisson distribution with a sufficiently large mean), follows a chi-square distribution. The pivotal quantity, $Q$, is defined as:\n$$ Q = \\frac{(n-1)s^2}{\\phi \\bar{x}} $$\nThis quantity is approximately distributed as a chi-square random variable with $n-1$ degrees of freedom, denoted $\\chi^2_{n-1}$.\n$$ Q \\sim \\chi^2_{n-1} $$\nFor a two-sided confidence interval with confidence level $1-\\alpha$, we find the lower and upper critical values from the $\\chi^2_{n-1}$ distribution. Let $\\chi^2_{\\alpha/2, n-1}$ be the value such that $P(\\chi^2_{n-1} \\le \\chi^2_{\\alpha/2, n-1}) = \\alpha/2$, and let $\\chi^2_{1-\\alpha/2, n-1}$ be the value such that $P(\\chi^2_{n-1} \\le \\chi^2_{1-\\alpha/2, n-1}) = 1-\\alpha/2$. The confidence interval is derived from the probability statement:\n$$ P\\left( \\chi^2_{\\alpha/2, n-1} \\le Q \\le \\chi^2_{1-\\alpha/2, n-1} \\right) \\approx 1-\\alpha $$\nSubstituting the expression for $Q$ and rearranging the inequalities to isolate $\\phi$, we obtain:\n$$ P\\left( \\frac{(n-1)s^2}{\\bar{x} \\cdot \\chi^2_{1-\\alpha/2, n-1}} \\le \\phi \\le \\frac{(n-1)s^2}{\\bar{x} \\cdot \\chi^2_{\\alpha/2, n-1}} \\right) \\approx 1-\\alpha $$\nThe lower bound $L$ and upper bound $U$ of the confidence interval for $\\phi$ are therefore:\n$$ L = \\frac{(n-1)\\hat{F}}{\\chi^2_{1-\\alpha/2, n-1}}, \\quad U = \\frac{(n-1)\\hat{F}}{\\chi^2_{\\alpha/2, n-1}} $$\nThe problem specifies a confidence level of $0.95$, so $\\alpha = 0.05$. The required probabilities for the critical values are $\\alpha/2 = 0.025$ and $1-\\alpha/2 = 0.975$.\n\nThe decision rule for deviation from a Poisson process is based on whether the hypothesized value $\\phi=1$ falls within this confidence interval. If $1 \\in [L, U]$, we do not have sufficient evidence to reject the null hypothesis, and we conclude the data are consistent with a Poisson process. If $1 \\notin [L, U]$, we reject the null hypothesis and conclude that the data deviate from a Poisson process. The boolean decision variable $B$ is thus:\n$$ B = (1  L) \\lor (1 > U) $$\nSpecial cases must be handled appropriately:\n- If the sample size is $n \\le 1$, the sample variance $s^2$ is undefined. Consequently, $\\hat{F}$, $L$, and $U$ cannot be computed. In this scenario, we cannot make a meaningful statistical decision, so we report non-number values for these quantities and conclude no deviation (i.e., $B = \\text{False}$).\n- If the sample mean $\\bar{x} = 0$, which implies all $x_i = 0$, the estimator $\\hat{F}$ is undefined due to division by zero. A sequence of all zeros is perfectly consistent with a Poisson process having a rate of $0$. Therefore, we again report non-number values for the computed quantities and conclude no deviation ($B = \\text{False}$).", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Main function to process the test suite and print results.\n    \"\"\"\n    \n    # Test suite provided in the problem statement.\n    test_cases = [\n        [2, 3, 1, 2, 0, 4, 2, 1, 3, 2, 2, 1, 3, 2, 1, 2, 3, 1, 2, 2],\n        [0, 7, 2, 10, 3, 8, 1, 9, 4, 6, 0, 11],\n        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n        [0, 1, 0],\n        [0, 0, 1, 0, 1, 0, 1, 0, 0, 1]\n    ]\n\n    results = []\n    for case_data in test_cases:\n        result_tuple = analyze_spike_counts(case_data)\n        results.append(result_tuple)\n\n    # Format the output as a list of tuples, without extra spaces.\n    formatted_results = []\n    for res in results:\n        # Handle nan for formatting\n        F_hat_str = f\"{res[0]:.7f}\" if not np.isnan(res[0]) else \"nan\"\n        L_str = f\"{res[1]:.7f}\" if not np.isnan(res[1]) else \"nan\"\n        U_str = f\"{res[2]:.7f}\" if not np.isnan(res[2]) else \"nan\"\n        B_str = str(res[3])\n        formatted_results.append(f\"({F_hat_str},{L_str},{U_str},{B_str})\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef analyze_spike_counts(x):\n    \"\"\"\n    Analyzes a sequence of spike counts to test for deviation from a Poisson process.\n\n    Args:\n        x (list): A list of non-negative integer spike counts.\n\n    Returns:\n        tuple: A quadruple (F_hat, L, U, B), where:\n               - F_hat is the estimated Fano factor.\n               - L and U are the lower and upper bounds of the 95% CI for the dispersion parameter phi.\n               - B is a boolean indicating deviation from a Poisson process.\n    \"\"\"\n    n = len(x)\n    alpha = 0.05\n\n    # Edge case: If n = 1, sample variance is undefined.\n    # No basis for statistical inference, so return non-numbers and False for deviation.\n    if n = 1:\n        return (np.nan, np.nan, np.nan, False)\n\n    x_arr = np.array(x, dtype=float)\n    x_bar = np.mean(x_arr)\n\n    # Edge case: If mean is 0, all counts are 0.\n    # This is consistent with a Poisson(0) process. F_hat is undefined (0/0).\n    # Return non-numbers for computed quantities and False for deviation.\n    if x_bar == 0:\n        return (np.nan, np.nan, np.nan, False)\n        \n    # Unbiased sample variance (ddof=1 for n-1 denominator).\n    s2 = np.var(x_arr, ddof=1)\n    \n    # Estimated Fano factor (sample variance / sample mean).\n    F_hat = s2 / x_bar\n    \n    df = n - 1\n    \n    # Critical values from the chi-square distribution for the 95% confidence interval.\n    # ppf is the percent point function (inverse of CDF).\n    chi2_lower_crit = chi2.ppf(alpha / 2, df)\n    chi2_upper_crit = chi2.ppf(1 - alpha / 2, df)\n    \n    # The denominator chi2_lower_crit is guaranteed to be  0 since df=n-1 = 1 and alpha  0.\n\n    # Lower bound of the confidence interval for the dispersion parameter phi.\n    L = (df * F_hat) / chi2_upper_crit\n    \n    # Upper bound of the confidence interval.\n    U = (df * F_hat) / chi2_lower_crit\n    \n    # Decision: The data deviates from Poisson if the value 1 is outside the CI [L, U].\n    is_deviated = not (L = 1 and 1 = U)\n    \n    return (F_hat, L, U, is_deviated)\n\nif __name__ == '__main__':\n    solve()\n```", "id": "5037305"}, {"introduction": "Beyond characterizing variability, a central goal of information theory in neuroscience is to quantify how much a neural response tells us about a stimulus. This exercise provides a concrete, hands-on calculation of the mutual information between a set of discrete stimuli and the activity of a small neural population. By working through the steps from stimulus-dependent firing rates to a final value in nats, you will gain a practical understanding of how information is formally measured in a neural code [@problem_id:5037371].", "problem": "A single sensory feature takes one of two discrete stimuli, denoted by $s \\in \\{s_{1}, s_{2}\\}$, with equal prior probability $p(s_{1}) = p(s_{2}) = \\frac{1}{2}$. You record a population of $N=2$ neurons over a time window of duration $\\Delta t = 0.1\\,\\text{s}$. Conditional on the stimulus $s$, neuron $i \\in \\{1,2\\}$ emits spikes as a Poisson process with stimulus-dependent rate $\\lambda_{i}(s)$ (in $\\text{Hz}$), and neurons are conditionally independent given $s$.\n\nThe recording system reports, for each neuron $i$, a spike-count $K_{i}$ within the window, but it saturates within the window: any count greater than or equal to $1$ is recorded as $1$. Thus the observed population response is the binary vector $X = (X_{1}, X_{2})$ with $X_{i} = \\mathbf{1}[K_{i} \\ge 1] \\in \\{0,1\\}$.\n\nThe stimulus-conditioned Poisson rates (in $\\text{Hz}$) are:\n- For $s_{1}$: $\\lambda_{1}(s_{1}) = 10 \\ln 2$, $\\lambda_{2}(s_{1}) = 10 \\ln 4$.\n- For $s_{2}$: $\\lambda_{1}(s_{2}) = 10 \\ln 3$, $\\lambda_{2}(s_{2}) = 10 \\ln 2$.\n\nUsing only first principles (the definition of mutual information, properties of Poisson processes over finite windows, and conditional independence), compute the mutual information between the stimulus $S$ and the observed population response $X$, denoted $I(S;X)$, in natural units (nats). Round your final numerical answer to four significant figures. Express the final answer in nats.", "solution": "The problem is valid. It is a well-posed, scientifically grounded problem in computational neuroscience that can be solved using first principles of information theory and probability theory.\n\nThe mutual information between the stimulus $S$ and the observed population response $X$ is given by the formula $I(S;X) = H(X) - H(X|S)$, where all entropies are calculated using the natural logarithm, yielding units of nats.\n\nFirst, we determine the probability of the discrete, binarized neural response $X_{i} \\in \\{0, 1\\}$ for each neuron $i \\in \\{1, 2\\}$, conditional on the stimulus $s \\in \\{s_1, s_2\\}$. The number of spikes $K_i$ from neuron $i$ in a time window of duration $\\Delta t$ follows a Poisson distribution with mean $\\mu_{i}(s) = \\lambda_{i}(s) \\Delta t$.\nThe observed response is $X_i = \\mathbf{1}[K_i \\ge 1]$. Thus, $X_i=0$ corresponds to $K_i=0$, and $X_i=1$ corresponds to $K_i \\ge 1$.\n\nThe probability of observing zero spikes, $p(K_i=0)$, for a Poisson distribution with mean $\\mu_i(s)$ is given by:\n$$p(K_i=0|s) = \\frac{(\\mu_i(s))^0 \\exp(-\\mu_i(s))}{0!} = \\exp(-\\mu_i(s))$$\nTherefore, the probability of the binarized observation $X_i=0$ is:\n$$p(X_i=0|s) = \\exp(-\\lambda_i(s)\\Delta t)$$\nThe probability of observing one or more spikes is:\n$$p(X_i=1|s) = 1 - p(X_i=0|s) = 1 - \\exp(-\\lambda_i(s)\\Delta t)$$\nThe given values are $\\Delta t = 0.1\\,\\text{s}$ and the firing rates $\\lambda_i(s)$. Let's calculate the Poisson means $\\mu_i(s)$:\n- For $s_1$:\n  - $\\mu_1(s_1) = \\lambda_1(s_1) \\Delta t = (10 \\ln 2 \\text{ Hz}) \\times (0.1 \\text{ s}) = \\ln 2$.\n  - $\\mu_2(s_1) = \\lambda_2(s_1) \\Delta t = (10 \\ln 4 \\text{ Hz}) \\times (0.1 \\text{ s}) = \\ln 4$.\n- For $s_2$:\n  - $\\mu_1(s_2) = \\lambda_1(s_2) \\Delta t = (10 \\ln 3 \\text{ Hz}) \\times (0.1 \\text{ s}) = \\ln 3$.\n  - $\\mu_2(s_2) = \\lambda_2(s_2) \\Delta t = (10 \\ln 2 \\text{ Hz}) \\times (0.1 \\text{ s}) = \\ln 2$.\n\nNow we can compute the conditional probabilities for the binarized responses:\n- For $s_1$:\n  - $p(X_1=0|s_1) = \\exp(-\\ln 2) = \\frac{1}{2}$. Thus, $p(X_1=1|s_1) = 1 - \\frac{1}{2} = \\frac{1}{2}$.\n  - $p(X_2=0|s_1) = \\exp(-\\ln 4) = \\frac{1}{4}$. Thus, $p(X_2=1|s_1) = 1 - \\frac{1}{4} = \\frac{3}{4}$.\n- For $s_2$:\n  - $p(X_1=0|s_2) = \\exp(-\\ln 3) = \\frac{1}{3}$. Thus, $p(X_1=1|s_2) = 1 - \\frac{1}{3} = \\frac{2}{3}$.\n  - $p(X_2=0|s_2) = \\exp(-\\ln 2) = \\frac{1}{2}$. Thus, $p(X_2=1|s_2) = 1 - \\frac{1}{2} = \\frac{1}{2}$.\n\nThe neurons are conditionally independent given the stimulus $s$. Thus, the probability of the population response $X = (X_1, X_2)$ is $p(X|s) = p(X_1|s) p(X_2|s)$. The possible responses are $X \\in \\{(0,0), (0,1), (1,0), (1,1)\\}$.\n- Conditional on $s_1$:\n  - $p(X=(0,0)|s_1) = (\\frac{1}{2})(\\frac{1}{4}) = \\frac{1}{8}$\n  - $p(X=(0,1)|s_1) = (\\frac{1}{2})(\\frac{3}{4}) = \\frac{3}{8}$\n  - $p(X=(1,0)|s_1) = (\\frac{1}{2})(\\frac{1}{4}) = \\frac{1}{8}$\n  - $p(X=(1,1)|s_1) = (\\frac{1}{2})(\\frac{3}{4}) = \\frac{3}{8}$\n- Conditional on $s_2$:\n  - $p(X=(0,0)|s_2) = (\\frac{1}{3})(\\frac{1}{2}) = \\frac{1}{6}$\n  - $p(X=(0,1)|s_2) = (\\frac{1}{3})(\\frac{1}{2}) = \\frac{1}{6}$\n  - $p(X=(1,0)|s_2) = (\\frac{2}{3})(\\frac{1}{2}) = \\frac{1}{3}$\n  - $p(X=(1,1)|s_2) = (\\frac{2}{3})(\\frac{1}{2}) = \\frac{1}{3}$\n\nNext, we calculate the conditional entropy $H(X|S) = \\sum_s p(s) H(X|S=s)$.\n$$H(X|S=s) = -\\sum_{x \\in X} p(x|s) \\ln p(x|s)$$\n- For $s_1$:\n$$H(X|s_1) = -\\left[2 \\times \\frac{1}{8}\\ln\\left(\\frac{1}{8}\\right) + 2 \\times \\frac{3}{8}\\ln\\left(\\frac{3}{8}\\right)\\right] = -\\frac{1}{4}(-\\ln 8) - \\frac{3}{4}(\\ln 3 - \\ln 8) = \\ln 8 - \\frac{3}{4}\\ln 3 = 3\\ln 2 - \\frac{3}{4}\\ln 3$$\n- For $s_2$:\n$$H(X|s_2) = -\\left[2 \\times \\frac{1}{6}\\ln\\left(\\frac{1}{6}\\right) + 2 \\times \\frac{1}{3}\\ln\\left(\\frac{1}{3}\\right)\\right] = -\\frac{1}{3}(-\\ln 6) - \\frac{2}{3}(-\\ln 3) = \\frac{1}{3}\\ln 6 + \\frac{2}{3}\\ln 3 = \\frac{1}{3}(\\ln 2 + \\ln 3) + \\frac{2}{3}\\ln 3 = \\frac{1}{3}\\ln 2 + \\ln 3$$\nGiven $p(s_1) = p(s_2) = \\frac{1}{2}$, the average conditional entropy is:\n$$H(X|S) = \\frac{1}{2}H(X|s_1) + \\frac{1}{2}H(X|s_2) = \\frac{1}{2}\\left[\\left(3\\ln 2 - \\frac{3}{4}\\ln 3\\right) + \\left(\\frac{1}{3}\\ln 2 + \\ln 3\\right)\\right]$$\n$$H(X|S) = \\frac{1}{2}\\left[\\left(3+\\frac{1}{3}\\right)\\ln 2 + \\left(1-\\frac{3}{4}\\right)\\ln 3\\right] = \\frac{1}{2}\\left[\\frac{10}{3}\\ln 2 + \\frac{1}{4}\\ln 3\\right] = \\frac{5}{3}\\ln 2 + \\frac{1}{8}\\ln 3$$\nNow, we calculate the entropy of the response, $H(X) = -\\sum_x p(x) \\ln p(x)$. First, we find the marginal probabilities $p(x)$ using the law of total probability: $p(x) = p(x|s_1)p(s_1) + p(x|s_2)p(s_2)$.\n- $p(X=(0,0)) = \\frac{1}{2}\\left(\\frac{1}{8} + \\frac{1}{6}\\right) = \\frac{1}{2}\\left(\\frac{3+4}{24}\\right) = \\frac{7}{48}$\n- $p(X=(0,1)) = \\frac{1}{2}\\left(\\frac{3}{8} + \\frac{1}{6}\\right) = \\frac{1}{2}\\left(\\frac{9+4}{24}\\right) = \\frac{13}{48}$\n- $p(X=(1,0)) = \\frac{1}{2}\\left(\\frac{1}{8} + \\frac{1}{3}\\right) = \\frac{1}{2}\\left(\\frac{3+8}{24}\\right) = \\frac{11}{48}$\n- $p(X=(1,1)) = \\frac{1}{2}\\left(\\frac{3}{8} + \\frac{1}{3}\\right) = \\frac{1}{2}\\left(\\frac{9+8}{24}\\right) = \\frac{17}{48}$\nThe entropy of the response is:\n$$H(X) = -\\left[\\frac{7}{48}\\ln\\left(\\frac{7}{48}\\right) + \\frac{13}{48}\\ln\\left(\\frac{13}{48}\\right) + \\frac{11}{48}\\ln\\left(\\frac{11}{48}\\right) + \\frac{17}{48}\\ln\\left(\\frac{17}{48}\\right)\\right]$$\n$$H(X) = -\\frac{1}{48}\\left[7(\\ln 7 - \\ln 48) + 13(\\ln 13 - \\ln 48) + 11(\\ln 11 - \\ln 48) + 17(\\ln 17 - \\ln 48)\\right]$$\n$$H(X) = -\\frac{1}{48}\\left[7\\ln 7 + 13\\ln 13 + 11\\ln 11 + 17\\ln 17 - (7+13+11+17)\\ln 48\\right]$$\n$$H(X) = \\ln 48 - \\frac{1}{48}\\left[7\\ln 7 + 13\\ln 13 + 11\\ln 11 + 17\\ln 17\\right]$$\nUsing $\\ln 48 = \\ln(16 \\times 3) = \\ln(2^4 \\times 3) = 4\\ln 2 + \\ln 3$.\n$$H(X) = 4\\ln 2 + \\ln 3 - \\frac{1}{48}\\left[7\\ln 7 + 13\\ln 13 + 11\\ln 11 + 17\\ln 17\\right]$$\nFinally, we compute the mutual information:\n$$I(S;X) = H(X) - H(X|S)$$\n$$I(S;X) = \\left(4\\ln 2 + \\ln 3 - \\frac{1}{48}\\left[7\\ln 7 + 13\\ln 13 + 11\\ln 11 + 17\\ln 17\\right]\\right) - \\left(\\frac{5}{3}\\ln 2 + \\frac{1}{8}\\ln 3\\right)$$\n$$I(S;X) = \\left(4 - \\frac{5}{3}\\right)\\ln 2 + \\left(1 - \\frac{1}{8}\\right)\\ln 3 - \\frac{1}{48}\\left[7\\ln 7 + 13\\ln 13 + 11\\ln 11 + 17\\ln 17\\right]$$\n$$I(S;X) = \\frac{7}{3}\\ln 2 + \\frac{7}{8}\\ln 3 - \\frac{1}{48}\\left[7\\ln 7 + 13\\ln 13 + 11\\ln 11 + 17\\ln 17\\right]$$\nNow, we substitute numerical values: $\\ln 2 \\approx 0.693147$, $\\ln 3 \\approx 1.098612$, $\\ln 7 \\approx 1.945910$, $\\ln 11 \\approx 2.397895$, $\\ln 13 \\approx 2.564949$, $\\ln 17 \\approx 2.833213$.\n$$I(S;X) \\approx \\frac{7}{3}(0.693147) + \\frac{7}{8}(1.098612) - \\frac{1}{48}[7(1.945910) + 13(2.564949) + 11(2.397895) + 17(2.833213)]$$\n$$I(S;X) \\approx 1.617343 + 0.961286 - \\frac{1}{48}[13.62137 + 33.34434 + 26.37685 + 48.16462]$$\n$$I(S;X) \\approx 2.578629 - \\frac{1}{48}[121.50718]$$\n$$I(S;X) \\approx 2.578629 - 2.5313996$$\n$$I(S;X) \\approx 0.0472294 \\text{ nats}$$\nRounding to four significant figures, the result is $0.04723$.", "answer": "$$\\boxed{0.04723}$$", "id": "5037371"}, {"introduction": "The ultimate application of understanding neural codes is often to \"read the brain's mind\"—that is, to decode a sensory stimulus from observed neural activity. This practice introduces population decoding through the powerful framework of Bayesian inference, a cornerstone of modern computational neuroscience. You will implement a decoder that combines prior knowledge about a stimulus with a likelihood model based on neural tuning curves to compute the most probable stimulus that caused the observed spikes [@problem_id:5037476].", "problem": "You are tasked with implementing a principled Bayesian population decoder for a one-dimensional stimulus variable using a discretized grid. The goal is to compute the posterior distribution over the stimulus given observed spike counts from a population of neurons with known tuning curves, and then to report the posterior mean estimate and uncertainty for each test case.\n\nFundamental base:\n- Assume a time window of duration $T$ seconds.\n- Each neuron $i$ produces spike counts $k_i$ that are independent across neurons given the stimulus $s$, and each count is modeled as a Poisson random variable with expected count $\\lambda_i(s) = T \\, r_i(s)$, where $r_i(s)$ is the firing rate (in spikes per second).\n- The tuning curve for neuron $i$ is Gaussian: \n$$r_i(s) = r_{0,i} + r_{\\max,i} \\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{s - \\mu_i}{\\sigma_i}\\right)^{2}\\right),$$ \nwhere $r_{0,i} \\ge 0$ is a baseline rate, $r_{\\max,i} \\ge 0$ is a peak amplitude, $\\mu_i$ is the preferred stimulus (in degrees), and $\\sigma_i  0$ is the tuning width (in degrees).\n- Use Bayes’ theorem with either a uniform prior over the stimulus grid or a Gaussian prior \n$$p(s) \\propto \\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{s-\\mu_0}{\\tau}\\right)^2\\right),$$ \nwith prior mean $\\mu_0$ and prior standard deviation $\\tau$.\n\nYour implementation must:\n1. Discretize the stimulus domain into a grid $s \\in \\{-90, -89, \\dots, 89, 90\\}$ (in degrees).\n2. For each test case, compute the unnormalized log-posterior on the grid,\n   $$\\log p(s \\mid \\mathbf{k}) = \\log p(s) + \\sum_{i} \\left[k_i \\log \\lambda_i(s) - \\lambda_i(s)\\right],$$\n   where $\\lambda_i(s) = T \\, r_i(s)$ and $\\mathbf{k} = (k_1, \\dots, k_N)$. The factor $\\log(k_i!)$ may be omitted since it does not depend on $s$.\n3. Numerically normalize the posterior over the grid to obtain a valid probability mass function,\n   $$p(s \\mid \\mathbf{k}) = \\frac{\\exp(\\log p(s \\mid \\mathbf{k}) - c)}{\\sum_{s'} \\exp(\\log p(s' \\mid \\mathbf{k}) - c)},$$\n   where $c = \\max_s \\log p(s \\mid \\mathbf{k})$ is a stabilizing constant.\n4. Compute the posterior mean,\n   $$\\mathbb{E}[s \\mid \\mathbf{k}] = \\sum_s s \\, p(s \\mid \\mathbf{k}),$$\n   and the posterior standard deviation,\n   $$\\sqrt{\\mathbb{V}[s \\mid \\mathbf{k}]} = \\sqrt{\\sum_s (s - \\mathbb{E}[s \\mid \\mathbf{k}])^2 \\, p(s \\mid \\mathbf{k})}.$$\n5. Express the posterior mean in degrees and the posterior standard deviation in degrees. All angles in this problem are in degrees.\n6. Round both the posterior mean and the posterior standard deviation to $4$ decimal places.\n\nTest suite:\nFor each test case below, the stimulus grid is $s \\in \\{-90,-89,\\dots,90\\}$ in degrees. The required parameters are the time window $T$ (in seconds), the prior, the observed spike counts $\\mathbf{k}$, and the set of neuron parameters $(\\mu_i, \\sigma_i, r_{0,i}, r_{\\max,i})$ for each neuron $i$.\n\n- Test case $1$ (general case):\n  - $T = 0.2$.\n  - Prior: Gaussian with $\\mu_0 = 10$ and $\\tau = 30$.\n  - Observed counts $\\mathbf{k} = [1, 2, 1, 3, 0]$.\n  - Neurons:\n    - $i=1$: $(\\mu_1, \\sigma_1, r_{0,1}, r_{\\max,1}) = (-40, 20, 2, 18)$.\n    - $i=2$: $(\\mu_2, \\sigma_2, r_{0,2}, r_{\\max,2}) = (-10, 20, 2, 18)$.\n    - $i=3$: $(\\mu_3, \\sigma_3, r_{0,3}, r_{\\max,3}) = (0, 20, 2, 18)$.\n    - $i=4$: $(\\mu_4, \\sigma_4, r_{0,4}, r_{\\max,4}) = (15, 20, 2, 18)$.\n    - $i=5$: $(\\mu_5, \\sigma_5, r_{0,5}, r_{\\max,5}) = (40, 20, 2, 18)$.\n\n- Test case $2$ (edge case with zero spikes and uniform prior):\n  - $T = 0.1$.\n  - Prior: uniform over the grid.\n  - Observed counts $\\mathbf{k} = [0, 0, 0, 0]$.\n  - Neurons:\n    - $i=1$: $(\\mu_1, \\sigma_1, r_{0,1}, r_{\\max,1}) = (-30, 15, 1, 9)$.\n    - $i=2$: $(\\mu_2, \\sigma_2, r_{0,2}, r_{\\max,2}) = (0, 15, 1, 9)$.\n    - $i=3$: $(\\mu_3, \\sigma_3, r_{0,3}, r_{\\max,3}) = (30, 15, 1, 9)$.\n    - $i=4$: $(\\mu_4, \\sigma_4, r_{0,4}, r_{\\max,4}) = (60, 15, 1, 9)$.\n\n- Test case $3$ (highly informative spiking around a preferred direction):\n  - $T = 0.2$.\n  - Prior: Gaussian with $\\mu_0 = 25$ and $\\tau = 20$.\n  - Observed counts $\\mathbf{k} = [0, 1, 8, 12, 2, 1]$.\n  - Neurons:\n    - $i=1$: $(\\mu_1, \\sigma_1, r_{0,1}, r_{\\max,1}) = (-60, 12, 3, 27)$.\n    - $i=2$: $(\\mu_2, \\sigma_2, r_{0,2}, r_{\\max,2}) = (-30, 12, 3, 27)$.\n    - $i=3$: $(\\mu_3, \\sigma_3, r_{0,3}, r_{\\max,3}) = (0, 12, 3, 27)$.\n    - $i=4$: $(\\mu_4, \\sigma_4, r_{0,4}, r_{\\max,4}) = (30, 12, 3, 27)$.\n    - $i=5$: $(\\mu_5, \\sigma_5, r_{0,5}, r_{\\max,5}) = (60, 12, 3, 27)$.\n    - $i=6$: $(\\mu_6, \\sigma_6, r_{0,6}, r_{\\max,6}) = (75, 12, 3, 27)$.\n\n- Test case $4$ (posterior concentrated near the boundary of the grid):\n  - $T = 0.15$.\n  - Prior: Gaussian with $\\mu_0 = 85$ and $\\tau = 5$.\n  - Observed counts $\\mathbf{k} = [0, 0, 5]$.\n  - Neurons:\n    - $i=1$: $(\\mu_1, \\sigma_1, r_{0,1}, r_{\\max,1}) = (80, 10, 2, 20)$.\n    - $i=2$: $(\\mu_2, \\sigma_2, r_{0,2}, r_{\\max,2}) = (85, 10, 2, 20)$.\n    - $i=3$: $(\\mu_3, \\sigma_3, r_{0,3}, r_{\\max,3}) = (90, 10, 2, 20)$.\n\nYour program must compute, for each test case, the posterior mean $\\mathbb{E}[s \\mid \\mathbf{k}]$ in degrees and the posterior standard deviation $\\sqrt{\\mathbb{V}[s \\mid \\mathbf{k}]}$ in degrees, both rounded to $4$ decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list in the form $[\\text{mean}, \\text{std}]$. For example: $[[m_1, s_1],[m_2, s_2],[m_3, s_3],[m_4, s_4]]$, where each $m_j$ and $s_j$ are floats rounded to $4$ decimal places.", "solution": "The problem requires the implementation of a Bayesian population decoder to estimate a one-dimensional stimulus, $s$, from the spike counts, $\\mathbf{k} = (k_1, \\dots, k_N)$, of a population of $N$ neurons. The solution is formulated within a probabilistic framework, utilizing Bayes' theorem to compute the posterior distribution of the stimulus given the observed neural activity.\n\nThe core of the Bayesian approach is the posterior probability distribution, given by Bayes' theorem:\n$$\np(s \\mid \\mathbf{k}) \\propto p(\\mathbf{k} \\mid s) \\, p(s)\n$$\nwhere $p(\\mathbf{k} \\mid s)$ is the likelihood of observing the spike counts $\\mathbf{k}$ given the stimulus $s$, and $p(s)$ is the prior distribution over the stimulus. The problem is solved by discretizing the stimulus space and computing this posterior distribution on the grid.\n\nThe problem states that the stimulus variable $s$ is discretized over the integer-valued grid from $-90$ to $90$ degrees. All calculations are performed on this grid.\n\n**1. Likelihood Model: $p(\\mathbf{k} \\mid s)$**\n\nThe response of each neuron is modeled as an independent Poisson process. The spike count $k_i$ of neuron $i$ in a time window of duration $T$ is a random variable drawn from a Poisson distribution with a mean $\\lambda_i(s)$ that depends on the stimulus $s$:\n$$\nk_i \\sim \\text{Poisson}(\\lambda_i(s))\n$$\nThe probability of observing a specific count $k_i$ is therefore:\n$$\np(k_i \\mid s) = \\frac{\\lambda_i(s)^{k_i} e^{-\\lambda_i(s)}}{k_i!}\n$$\nThe mean spike count $\\lambda_i(s)$ is the product of the time duration $T$ and the neuron's firing rate $r_i(s)$:\n$$\n\\lambda_i(s) = T \\cdot r_i(s)\n$$\nThe firing rate $r_i(s)$ is described by a Gaussian tuning curve, which models the neuron's response specificity to the stimulus:\n$$\nr_i(s) = r_{0,i} + r_{\\max,i} \\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{s - \\mu_i}{\\sigma_i}\\right)^{2}\\right)\n$$\nHere, $r_{0,i}$ is the baseline firing rate, $r_{\\max,i}$ is the maximum firing rate above baseline, $\\mu_i$ is the neuron's preferred stimulus, and $\\sigma_i$ is the tuning width.\n\nSince the neurons are assumed to be independent given the stimulus, the total likelihood for the population is the product of the individual likelihoods:\n$$\np(\\mathbf{k} \\mid s) = \\prod_{i=1}^{N} p(k_i \\mid s) = \\prod_{i=1}^{N} \\frac{\\lambda_i(s)^{k_i} e^{-\\lambda_i(s)}}{k_i!}\n$$\nFor computational stability and convenience, we work with the log-likelihood:\n$$\n\\log p(\\mathbf{k} \\mid s) = \\sum_{i=1}^{N} \\left[ k_i \\log \\lambda_i(s) - \\lambda_i(s) - \\log(k_i!) \\right]\n$$\nThe term $\\log(k_i!)$ is a constant with respect to the stimulus $s$ and can be dropped when calculating the posterior, as it will be absorbed into the normalization constant.\n\n**2. Prior Model: $p(s)$**\n\nThe prior distribution $p(s)$ represents our initial belief about the stimulus value before observing any data. Two types of priors are considered:\n- **Uniform Prior**: This prior assumes all stimulus values on the grid are equally likely. The log-prior, $\\log p(s)$, is a constant, which can be set to $0$ for simplicity, as any additive constant will be handled by the final normalization.\n- **Gaussian Prior**: This prior assumes the stimulus is likely to be near a value $\\mu_0$ with a standard deviation of $\\tau$. The unnormalized prior is $p(s) \\propto \\exp\\left(-\\frac{1}{2}\\left(\\frac{s-\\mu_0}{\\tau}\\right)^2\\right)$. The log-prior is thus:\n$$\n\\log p(s) = -\\frac{1}{2}\\left(\\frac{s-\\mu_0}{\\tau}\\right)^2 + C\n$$\nwhere the constant $C$ can be ignored.\n\n**3. Posterior Distribution: $p(s \\mid \\mathbf{k})$**\n\nThe log-posterior is the sum of the log-likelihood and the log-prior, up to a normalization constant:\n$$\n\\log p(s \\mid \\mathbf{k}) \\approx \\log p(s) + \\log p(\\mathbf{k} \\mid s) = \\log p(s) + \\sum_{i=1}^{N} \\left[ k_i \\log \\lambda_i(s) - \\lambda_i(s) \\right]\n$$\nThis unnormalized log-posterior is computed for every stimulus value $s_j$ on the discrete grid. To obtain a valid probability mass function, we must normalize it. A numerically stable way to do this is to use the log-sum-exp trick. First, we find the maximum value of the log-posterior, $c = \\max_{s_j} \\log p(s_j \\mid \\mathbf{k})$. Then, the posterior probability for each grid point $s_j$ is:\n$$\np(s_j \\mid \\mathbf{k}) = \\frac{\\exp(\\log p(s_j \\mid \\mathbf{k}) - c)}{\\sum_{s'_l} \\exp(\\log p(s'_l \\mid \\mathbf{k}) - c)}\n$$\n\n**4. Estimation and Uncertainty**\n\nFrom the normalized posterior distribution, we can compute an estimate of the stimulus and its associated uncertainty.\n- **Posterior Mean**: The optimal estimate of the stimulus under a squared error loss function is the posterior mean. It is calculated as the expected value of $s$ under the posterior distribution:\n$$\n\\hat{s} = \\mathbb{E}[s \\mid \\mathbf{k}] = \\sum_{j} s_j \\, p(s_j \\mid \\mathbf{k})\n$$\n- **Posterior Standard Deviation**: The uncertainty of the estimate is quantified by the posterior standard deviation, which is the square root of the posterior variance:\n$$\n\\sqrt{\\mathbb{V}[s \\mid \\mathbf{k}]} = \\sqrt{\\sum_{j} (s_j - \\hat{s})^2 \\, p(s_j \\mid \\mathbf{k})}\n$$\n\nThe implementation will follow these steps for each test case, calculating the posterior mean and standard deviation and rounding them to the specified precision.", "answer": "```python\nimport numpy as np\n\ndef run_bayesian_decoder(T, prior_params, k, neurons):\n    \"\"\"\n    Computes the posterior mean and standard deviation for a stimulus s.\n\n    Args:\n        T (float): Time window duration in seconds.\n        prior_params (dict): Dictionary specifying the prior ('uniform' or 'gaussian' with mu0, tau).\n        k (np.ndarray): Array of observed spike counts for each neuron.\n        neurons (np.ndarray): Array of neuron parameters (mu, sigma, r0, r_max).\n\n    Returns:\n        tuple[float, float]: The posterior mean and a posteriori standard deviation.\n    \"\"\"\n    # 1. Discretize the stimulus domain\n    s_grid = np.arange(-90, 91, 1).astype(float)\n\n    # 2. Compute the log-prior probability log p(s)\n    if prior_params[\"type\"] == \"uniform\":\n        log_prior = np.zeros_like(s_grid)\n    elif prior_params[\"type\"] == \"gaussian\":\n        mu0 = prior_params[\"mu0\"]\n        tau = prior_params[\"tau\"]\n        log_prior = -0.5 * ((s_grid - mu0) / tau)**2\n    else:\n        raise ValueError(\"Invalid prior type specified\")\n\n    # 3. Compute the log-likelihood log p(k|s)\n    # Unpack neuron parameters\n    mu = neurons[:, 0]\n    sigma = neurons[:, 1]\n    r0 = neurons[:, 2]\n    r_max = neurons[:, 3]\n\n    # Use broadcasting for efficient computation across the stimulus grid\n    # s_grid shape: (181,) - s_grid_col shape: (181, 1)\n    # neuron params mu, sigma, etc. shape: (N,) - (1, N)\n    s_grid_col = s_grid[:, np.newaxis]\n    \n    # Firing rates r_i(s) for all neurons i and stimuli s\n    # rates shape: (181, N)\n    exponent = -0.5 * ((s_grid_col - mu) / sigma)**2\n    rates = r0 + r_max * np.exp(exponent)\n\n    # Expected spike counts lambda_i(s)\n    # lambdas shape: (181, N)\n    lambdas = T * rates\n    \n    # Total log-likelihood for each stimulus s, summed over neurons\n    # The term k_i * log(lambda_i(s)) is calculated safely.\n    # If lambda_i(s) is close to 0, log(lambda) is a large negative number.\n    # The problem parameters ensure r_i(s)  0, so lambda_i(s)  0, avoiding log(0).\n    log_likelihood_matrix = k * np.log(lambdas) - lambdas\n    total_log_likelihood = np.sum(log_likelihood_matrix, axis=1)\n\n    # 4. Compute the unnormalized log-posterior\n    log_posterior = log_prior + total_log_likelihood\n\n    # 5. Numerically stabilize and normalize the posterior\n    # Use the log-sum-exp trick for numerical stability\n    c = np.max(log_posterior)\n    log_posterior_stable = log_posterior - c\n    unnormalized_posterior = np.exp(log_posterior_stable)\n    normalization_constant = np.sum(unnormalized_posterior)\n    posterior = unnormalized_posterior / normalization_constant\n\n    # 6. Compute posterior mean and standard deviation\n    posterior_mean = np.sum(s_grid * posterior)\n    posterior_variance = np.sum(((s_grid - posterior_mean)**2) * posterior)\n    posterior_std_dev = np.sqrt(posterior_variance)\n\n    return posterior_mean, posterior_std_dev\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the Bayesian population decoder.\n    \"\"\"\n    test_cases = [\n        {\n            \"T\": 0.2,\n            \"prior\": {\"type\": \"gaussian\", \"mu0\": 10, \"tau\": 30},\n            \"k\": np.array([1, 2, 1, 3, 0]),\n            \"neurons\": np.array([\n                [-40, 20, 2, 18], [-10, 20, 2, 18], [0, 20, 2, 18],\n                [15, 20, 2, 18], [40, 20, 2, 18]\n            ])\n        },\n        {\n            \"T\": 0.1,\n            \"prior\": {\"type\": \"uniform\"},\n            \"k\": np.array([0, 0, 0, 0]),\n            \"neurons\": np.array([\n                [-30, 15, 1, 9], [0, 15, 1, 9], [30, 15, 1, 9], [60, 15, 1, 9]\n            ])\n        },\n        {\n            \"T\": 0.2,\n            \"prior\": {\"type\": \"gaussian\", \"mu0\": 25, \"tau\": 20},\n            \"k\": np.array([0, 1, 8, 12, 2, 1]),\n            \"neurons\": np.array([\n                [-60, 12, 3, 27], [-30, 12, 3, 27], [0, 12, 3, 27],\n                [30, 12, 3, 27], [60, 12, 3, 27], [75, 12, 3, 27]\n            ])\n        },\n        {\n            \"T\": 0.15,\n            \"prior\": {\"type\": \"gaussian\", \"mu0\": 85, \"tau\": 5},\n            \"k\": np.array([0, 0, 5]),\n            \"neurons\": np.array([\n                [80, 10, 2, 20], [85, 10, 2, 20], [90, 10, 2, 20]\n            ])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mean, std = run_bayesian_decoder(case[\"T\"], case[\"prior\"], case[\"k\"], case[\"neurons\"])\n        # Round to 4 decimal places as required\n        rounded_mean = round(mean, 4)\n        rounded_std = round(std, 4)\n        results.append((rounded_mean, rounded_std))\n\n    # Format the output string exactly as specified: [[m1,s1],[m2,s2],...]\n    formatted_results = [f\"[{m},{s}]\" for m, s in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "5037476"}]}