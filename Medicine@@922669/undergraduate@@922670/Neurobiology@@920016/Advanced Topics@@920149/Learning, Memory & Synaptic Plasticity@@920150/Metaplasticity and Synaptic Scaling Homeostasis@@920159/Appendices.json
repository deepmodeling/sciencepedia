{"hands_on_practices": [{"introduction": "A key feature of homeostatic synaptic scaling is its ability to adjust the overall input strength to a neuron without erasing the learned relative differences between individual synapses. This practice delves into the mathematical foundation of this property by asking you to analyze how a dataset of synaptic strengths responds to a uniform multiplicative change. By demonstrating that the coefficient of variation (CV) remains constant, you will gain a concrete understanding of how scaling preserves the essential pattern of synaptic weights while globally adjusting their magnitude [@problem_id:5032163].", "problem": "A postsynaptic neuron exhibits metaplasticity and homeostatic synaptic scaling: when its long-term average firing deviates from a target, all excitatory synaptic strengths are multiplicatively adjusted by the same scale factor to restore activity while preserving relative differences across synapses. Consider a sample of baseline Excitatory Postsynaptic Current (EPSC) amplitudes (in picoamperes, $\\mathrm{pA}$) recorded from $6$ synapses: $\\{10, 12, 14, 16, 18, 20\\}\\,\\mathrm{pA}$. Under homeostatic synaptic scaling, each amplitude $a_{i}$ is transformed to $a_{i}'$ by a common multiplicative factor $s = 1.3$, such that $a_{i}' = s\\,a_{i}$. Using the core definitions that the sample mean is $\\bar{a} = \\frac{1}{n}\\sum_{i=1}^{n} a_{i}$, the sample standard deviation is $\\sigma = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (a_{i}-\\bar{a})^{2}}$, and the coefficient of variation is $\\mathrm{CV} = \\frac{\\sigma}{\\bar{a}}$, derive from first principles why a uniform multiplicative scaling by $s$ leaves $\\mathrm{CV}$ unchanged. Then, compute the scaled amplitudes $a_{i}'$ for the given dataset and verify numerically that the coefficient of variation is unchanged. Report the common coefficient of variation value as a unitless decimal. Round your final reported value to four significant figures. Express amplitudes in $\\mathrm{pA}$.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the neurobiological principle of homeostatic synaptic scaling, mathematically well-posed, and presented with objective and unambiguous language. All necessary data, definitions, and constraints are provided for a unique and meaningful solution. The problem requires a formal derivation from first principles and a subsequent numerical verification, which will be performed in sequence.\n\nThe problem asks to demonstrate, first through formal derivation and then through numerical calculation, that the coefficient of variation ($\\mathrm{CV}$) of a dataset is invariant under a uniform multiplicative scaling.\n\nLet the initial set of $n$ Excitatory Postsynaptic Current (EPSC) amplitudes be denoted by $\\{a_1, a_2, \\ldots, a_n\\}$. The problem provides the following definitions:\nThe sample mean:\n$$ \\bar{a} = \\frac{1}{n}\\sum_{i=1}^{n} a_{i} $$\nThe sample standard deviation:\n$$ \\sigma = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (a_{i}-\\bar{a})^{2}} $$\nThe coefficient of variation:\n$$ \\mathrm{CV} = \\frac{\\sigma}{\\bar{a}} $$\n\n**Part 1: Formal Derivation**\n\nWe begin by establishing the aforestated properties of the original dataset. The homeostatic scaling process transforms each amplitude $a_i$ into a new amplitude $a_i'$ using a common multiplicative factor $s$:\n$$ a_{i}' = s \\cdot a_{i} $$\n\nWe now derive the statistical properties of the new, scaled dataset $\\{a_1', a_2', \\ldots, a_n'\\}$.\n\nFirst, we compute the new sample mean, $\\bar{a}'$:\n$$ \\bar{a}' = \\frac{1}{n}\\sum_{i=1}^{n} a_{i}' = \\frac{1}{n}\\sum_{i=1}^{n} (s \\cdot a_{i}) $$\nSince $s$ is a constant factor, it can be moved outside the summation:\n$$ \\bar{a}' = s \\left( \\frac{1}{n}\\sum_{i=1}^{n} a_{i} \\right) = s \\cdot \\bar{a} $$\nThis shows that the mean of the scaled data is simply the original mean multiplied by the scaling factor $s$.\n\nNext, we compute the new sample standard deviation, $\\sigma'$. Using its definition and substituting the expressions for $a_i'$ and $\\bar{a}'$:\n$$ \\sigma' = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (a_{i}'-\\bar{a}')^{2}} = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (s \\cdot a_{i} - s \\cdot \\bar{a})^{2}} $$\nWe factor out the common term $s$ inside the parentheses:\n$$ \\sigma' = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} [s(a_{i} - \\bar{a})]^{2}} = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} s^{2}(a_{i} - \\bar{a})^{2}} $$\nThe constant factor $s^2$ can be moved outside the summation:\n$$ \\sigma' = \\sqrt{s^{2} \\left[ \\frac{1}{n-1}\\sum_{i=1}^{n} (a_{i} - \\bar{a})^{2} \\right]} $$\nRecognizing the term in the square brackets as the original sample variance, $\\sigma^2$, we have:\n$$ \\sigma' = \\sqrt{s^{2} \\sigma^{2}} = |s| \\sqrt{\\sigma^{2}} = |s| \\sigma $$\nThe problem describes homeostatic scaling, where the factor $s$ is a positive multiplier ($s=1.3$), so $|s|=s$. Thus:\n$$ \\sigma' = s \\cdot \\sigma $$\nThe standard deviation of the scaled data is the original standard deviation multiplied by the scaling factor $s$.\n\nFinally, we compute the new coefficient of variation, $\\mathrm{CV}'$:\n$$ \\mathrm{CV}' = \\frac{\\sigma'}{\\bar{a}'} $$\nSubstituting the derived expressions for $\\bar{a}'$ and $\\sigma'$:\n$$ \\mathrm{CV}' = \\frac{s \\cdot \\sigma}{s \\cdot \\bar{a}} = \\frac{\\sigma}{\\bar{a}} = \\mathrm{CV} $$\nThis derivation from first principles proves that the coefficient of variation is invariant under a uniform multiplicative scaling of the data. This is because the $\\mathrm{CV}$ is a measure of relative variability, and scaling the data affects both the measure of central tendency (mean) and the measure of dispersion (standard deviation) by the exact same factor, which cancels out in their ratio.\n\n**Part 2: Numerical Verification and Calculation**\n\nWe are given the baseline EPSC amplitudes: $\\{10, 12, 14, 16, 18, 20\\}\\,\\mathrm{pA}$. Here, $n=6$.\n\nFirst, we calculate the statistics for the original dataset.\nThe sum of the amplitudes is $\\sum_{i=1}^{6} a_{i} = 10 + 12 + 14 + 16 + 18 + 20 = 90$.\nThe sample mean is:\n$$ \\bar{a} = \\frac{90}{6} = 15\\,\\mathrm{pA} $$\nThe sum of squared deviations from the mean is:\n$$ \\sum_{i=1}^{6} (a_{i}-\\bar{a})^{2} = (10-15)^{2} + (12-15)^{2} + (14-15)^{2} + (16-15)^{2} + (18-15)^{2} + (20-15)^{2} \\\\ = (-5)^{2} + (-3)^{2} + (-1)^{2} + (1)^{2} + (3)^{2} + (5)^{2} \\\\ = 25 + 9 + 1 + 1 + 9 + 25 = 70 $$\nThe sample standard deviation is:\n$$ \\sigma = \\sqrt{\\frac{70}{6-1}} = \\sqrt{\\frac{70}{5}} = \\sqrt{14}\\,\\mathrm{pA} $$\nThe coefficient of variation for the original data is:\n$$ \\mathrm{CV} = \\frac{\\sigma}{\\bar{a}} = \\frac{\\sqrt{14}}{15} $$\n\nNow, we apply the scaling factor $s=1.3$ to compute the new amplitudes $a_i' = 1.3 \\cdot a_i$:\n$a_{1}' = 1.3 \\times 10 = 13\\,\\mathrm{pA}$\n$a_{2}' = 1.3 \\times 12 = 15.6\\,\\mathrm{pA}$\n$a_{3}' = 1.3 \\times 14 = 18.2\\,\\mathrm{pA}$\n$a_{4}' = 1.3 \\times 16 = 20.8\\,\\mathrm{pA}$\n$a_{5}' = 1.3 \\times 18 = 23.4\\,\\mathrm{pA}$\n$a_{6}' = 1.3 \\times 20 = 26\\,\\mathrm{pA}$\nThe scaled amplitudes are $\\{13, 15.6, 18.2, 20.8, 23.4, 26\\}\\,\\mathrm{pA}$.\n\nWe now compute the statistics for this scaled dataset to verify the derivation.\nThe new mean is $\\bar{a}' = s \\cdot \\bar{a} = 1.3 \\times 15 = 19.5\\,\\mathrm{pA}$.\nThe new standard deviation is $\\sigma' = s \\cdot \\sigma = 1.3 \\sqrt{14}\\,\\mathrm{pA}$.\nThe new coefficient of variation is:\n$$ \\mathrm{CV}' = \\frac{\\sigma'}{\\bar{a}'} = \\frac{1.3 \\sqrt{14}}{19.5} = \\frac{1.3 \\sqrt{14}}{1.3 \\times 15} = \\frac{\\sqrt{14}}{15} $$\nAs predicted by the derivation, $\\mathrm{CV}' = \\mathrm{CV}$. The numerical verification is successful.\n\nFinally, we compute the numerical value of the common coefficient of variation and round it to four significant figures.\n$$ \\mathrm{CV} = \\frac{\\sqrt{14}}{15} \\approx \\frac{3.74165738677}{15} \\approx 0.24944382578 $$\nRounding to four significant figures gives $0.2494$.\nThe biological significance is that synaptic scaling adjusts the overall strength of a neuron's inputs to maintain a target activity level, but it does so in a way that preserves the relative weight distribution across its synapses. The invariance of the $\\mathrm{CV}$ is the mathematical signature of this preservation of relative synaptic strength.", "answer": "$$\\boxed{0.2494}$$", "id": "5032163"}, {"introduction": "Now that we've seen the mathematical signature of synaptic scaling, we can ask how such a process might be implemented by a neuron. This exercise guides you to derive the fundamental update rule for homeostatic scaling from a set of basic biophysical constraints, such as the dependence on a target firing rate and the need for a multiplicative update. This \"first-principles\" approach illuminates how complex biological regulation can emerge from simple, logical rules and provides the engine for the homeostatic control system [@problem_id:5032175].", "problem": "A single-compartment excitatory neuron maintains an average firing rate via homeostatic synaptic scaling, a form of negative feedback that preserves relative synaptic strengths while adjusting absolute synaptic efficacy. Let the target (set-point) firing rate be $r^{*}$ and the measured average rate over a sufficiently long integration window be $r$. At each discrete homeostatic step, all excitatory synaptic weights $w_{i}$ are updated multiplicatively by the same factor so that the relative ratios $w_{i}/w_{j}$ are preserved.\n\nUsing only foundational constraints of homeostatic synaptic scaling:\n- The weight update is multiplicative and homogeneous across synapses, so the fractional change $\\Delta w_{i}/w_{i}$ is the same for all $i$.\n- The update vanishes when the neuron is at the set-point, so it is zero when $r=r^{*}$.\n- The update must be dimensionless and invariant to uniform rescaling of the firing-rate measurement, so it depends only on the ratio $r^{*}/r$.\n- For sufficiently small steps, the update is first-order in a small, dimensionless learning rate $\\eta$ and varies smoothly with $r^{*}/r$.\n\nFrom these constraints, derive the minimal first-order expression for the fractional change $\\Delta w_{i}/w_{i}$ in terms of $r^{*}$, $r$, and $\\eta$. Then, for a neuron with $r^{*}=5$ Hz, current rate $r=3$ Hz, and learning rate $\\eta=0.01$, compute the numerical value of the proportional change $\\Delta w_{i}/w_{i}$ applied to each $w_{i}$ after one scaling step. Report your answer as a single pure number (dimensionless). No rounding is required; if you choose to approximate, ensure at least four significant figures.", "solution": "The problem will be validated by first extracting the given information and then assessing its scientific and logical integrity.\n\n### Step 1: Extract Givens\n- The target (set-point) firing rate is $r^{*}$.\n- The measured average firing rate is $r$.\n- Individual excitatory synaptic weights are denoted by $w_{i}$.\n- The update to the weights is multiplicative, preserving the relative ratios $w_{i}/w_{j}$.\n- **Constraint 1:** The fractional change $\\frac{\\Delta w_{i}}{w_{i}}$ is the same for all synapses $i$. Let this common fractional change be denoted by $F$.\n- **Constraint 2:** The update vanishes when the neuron is at its set-point, i.e., $F=0$ when $r=r^{*}$.\n- **Constraint 3:** The update is dimensionless and depends only on the ratio $\\frac{r^{*}}{r}$. This implies $F$ is a function of this ratio, so we may write $F = f\\left(\\frac{r^{*}}{r}\\right)$.\n- **Constraint 4:** For small steps, the update is first-order in a small, dimensionless learning rate $\\eta$ and varies smoothly with $\\frac{r^{*}}{r}$.\n- The task is to derive the minimal first-order expression for $\\frac{\\Delta w_{i}}{w_{i}}$ and then compute its value for $r^{*}=5$ Hz, $r=3$ Hz, and $\\eta=0.01$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding:** The problem describes synaptic scaling, a well-documented form of homeostatic plasticity in neurobiology. The representation of the process as a multiplicative update rule that depends on the deviation from a target firing rate is a standard and scientifically accepted model.\n- **Well-Posedness:** The problem provides a set of clear, mathematical constraints and asks for the derivation of a functional form that satisfies them. The term \"minimal first-order expression\" is sufficiently precise to guide the derivation toward a unique solution (up to a scaling constant absorbed into the learning rate).\n- **Objectivity:** The problem is stated in objective, formal language without subjective or ambiguous terminology.\n- **Consistency and Completeness:** The constraints are self-consistent and sufficient to derive the requested expression. For example, Constraint 3 ($F$ depends on $r^{*}/r$) is consistent with Constraint 2 (the update is zero when $r^{*}/r = 1$). All necessary numerical values for the final calculation are provided.\n\n### Step 3: Verdict and Action\nThe problem is scientifically grounded, well-posed, objective, and contains no discernible flaws. It is therefore deemed **valid**. The solution will now be derived.\n\n### Derivation of the Update Rule\nLet the fractional change in synaptic weight be $F = \\frac{\\Delta w_{i}}{w_{i}}$. This change is posited to be identical for all synapses $i$, ensuring that relative synaptic strengths are preserved.\n\nFrom a combination of constraints 3 and 4, we can express $F$ as a function of the ratio of the target rate to the measured rate, scaled by a small learning rate $\\eta$:\n$$F = \\eta \\cdot g\\left(\\frac{r^{*}}{r}\\right)$$\nwhere $g$ is a smooth function. The problem asks for a minimal first-order expression, which guides us to seek the simplest non-trivial form for $g$.\n\nConstraint 2 states that the update is zero when the system is at its homeostatic set-point, i.e., when $r = r^{*}$. In terms of the argument of the function $g$, this equilibrium condition corresponds to $\\frac{r^{*}}{r} = 1$. Therefore, we must have:\n$$g(1) = 0$$\n\nTo find the first-order behavior of the update rule for small deviations from the set-point, we perform a Taylor series expansion of the function $g(x)$ around the point $x=1$, where $x = \\frac{r^{*}}{r}$.\n$$g(x) = g(1) + g'(1)(x-1) + \\frac{1}{2}g''(1)(x-1)^{2} + \\dots$$\nSubstituting $g(1)=0$, we have:\n$$g(x) = g'(1)(x-1) + \\mathcal{O}((x-1)^{2})$$\nThe first-order approximation for $g(x)$ for values of $x$ close to $1$ is the linear term:\n$$g(x) \\approx g'(1)(x-1)$$\nThe problem asks for the *minimal* expression. The constant factor $g'(1)$ represents a scaling of the response to the error signal $(x-1)$. This constant can be absorbed into the definition of the dimensionless learning rate $\\eta$. To obtain the most fundamental and minimal form, we can set $g'(1) = 1$ without loss of generality (by appropriately defining $\\eta$). This simplification yields the minimal form for the function $g$:\n$$g(x) = x-1$$\nSubstituting this back into our expression for $F$, we obtain the minimal first-order expression for the fractional change in synaptic weight:\n$$\\frac{\\Delta w_{i}}{w_{i}} = \\eta \\left( \\frac{r^{*}}{r} - 1 \\right)$$\nThis expression correctly captures the homeostatic nature of the process. If the firing rate $r$ is below the target $r^{*}$, then $\\frac{r^{*}}{r}  1$, leading to a positive $\\frac{\\Delta w_{i}}{w_{i}}$, which potentiates the synapses to increase the firing rate. Conversely, if $r  r^{*}$, then $\\frac{r^{*}}{r}  1$, leading to a negative $\\frac{\\Delta w_{i}}{w_{i}}$, which depresses the synapses to decrease the firing rate.\n\n### Numerical Calculation\nThe problem provides the following numerical values:\n- Target firing rate: $r^{*} = 5$ Hz\n- Current firing rate: $r = 3$ Hz\n- Learning rate: $\\eta = 0.01$\n\nWe substitute these values into the derived expression for the fractional change $\\frac{\\Delta w_{i}}{w_{i}}$:\n$$\\frac{\\Delta w_{i}}{w_{i}} = 0.01 \\left( \\frac{5}{3} - 1 \\right)$$\nFirst, we evaluate the term in the parentheses:\n$$\\frac{5}{3} - 1 = \\frac{5}{3} - \\frac{3}{3} = \\frac{2}{3}$$\nNow, we multiply by the learning rate $\\eta$:\n$$\\frac{\\Delta w_{i}}{w_{i}} = 0.01 \\times \\frac{2}{3} = \\frac{1}{100} \\times \\frac{2}{3} = \\frac{2}{300} = \\frac{1}{150}$$\nThis is the exact, dimensionless proportional change applied to each synaptic weight $w_{i}$ in a single homeostatic update step. The value is a pure number as required.", "answer": "$$\\boxed{\\frac{1}{150}}$$", "id": "5032175"}, {"introduction": "This final practice synthesizes the concepts of Hebbian plasticity and homeostatic scaling in a dynamic simulation, which is a cornerstone of computational neuroscience research. You will model a realistic scenario where rapid, synapse-specific strengthening (LTP) drives the neuron's activity away from its stable set-point, triggering a slower, global downscaling to restore balance. By implementing and quantifying the outcome of this interaction, you will directly observe how metaplasticity helps a neuron to both learn new information and maintain overall stability, a fundamental challenge for any adaptive system [@problem_id:5032199].", "problem": "You are asked to formalize and simulate a neurobiologically realistic scenario of metaplasticity expressing homeostatic synaptic scaling following a Hebbian potentiation event. The goal is to quantify the net change in absolute synaptic weights and assess whether relative synaptic differences are preserved under slow multiplicative scaling. Begin from core definitions: Hebbian plasticity states that synaptic weight change is proportional to the correlation between presynaptic and postsynaptic activity, and homeostatic synaptic scaling is a slow, cell-wide process that adjusts all synaptic weights multiplicatively to stabilize the postsynaptic firing rate around a target rate. In this problem, Long-Term Potentiation (LTP) is applied to a predefined subset of synapses, followed by slow multiplicative scaling that reduces weights to restore the postsynaptic rate to a set point. All quantities must be handled in purely mathematical terms, with synaptic weights in arbitrary units (a.u.) and firing rates in hertz (Hz).\n\nThe system consists of $N$ synapses with weights $w_i(t)$ and constant presynaptic input rates $x_i$ for $i \\in \\{1,\\dots,N\\}$. The postsynaptic firing rate is modeled as a linear sum $r(t) = \\sum_{i=1}^{N} w_i(t) x_i$. At time $t = 0$, apply a Hebbian Long-Term Potentiation (LTP) event to a predefined subset $S \\subset \\{1,\\dots,N\\}$ by increasing the weight $w_i(0)$ on each $i \\in S$ by a fixed fractional amount $\\gamma$; weights on $i \\notin S$ remain unchanged. Then, simulate slow synaptic scaling over a duration $T$ using a small time step $\\Delta t$ and a rate constant $\\beta$, where the scaling acts multiplicatively and uniformly across all synapses to move $r(t)$ toward a target rate $r^*$ chosen as the pre-LTP baseline $r(0)$. Enforce a lower bound $w_{\\min}$ on weights to reflect non-negativity and minimal synaptic efficacy. The scaling process should be implemented algorithmically from the definitions, ensuring scientific realism and avoiding any ad hoc shortcuts. Angle units are not applicable. Express any change in synaptic weights in arbitrary units (a.u.), and rates in hertz (Hz).\n\nYour program must, for each test case, compute the following metrics:\n- $M_1$: the mean absolute change in synaptic weights between the final state and the initial state, defined by\n$$\nM_1 = \\frac{1}{N} \\sum_{i=1}^{N} \\left| w_i(T) - w_i(0) \\right| \\quad \\text{(in a.u.)}.\n$$\n- $M_2$: the maximum normalized ratio error quantifying preservation of relative synaptic differences, using the weights immediately after LTP, denoted $w_i^{\\mathrm{LTP}}$, and the final weights $w_i(T)$. Define\n$$\nE_{ij} = \\left| \\frac{ \\left( w_i(T) / w_j(T) \\right) }{ \\left( w_i^{\\mathrm{LTP}} / w_j^{\\mathrm{LTP}} \\right) } - 1 \\right|, \\quad i \\neq j,\n$$\nand let\n$$\nM_2 = \\max_{i \\neq j} E_{ij} \\quad \\text{(dimensionless)}.\n$$\n- $M_3$: the Pearson correlation coefficient between the final weights $\\mathbf{w}(T)$ and the post-LTP weights $\\mathbf{w}^{\\mathrm{LTP}}$, defined by\n$$\nM_3 = \\frac{\\sum_{i=1}^{N} \\left( w_i(T) - \\overline{w(T)} \\right) \\left( w_i^{\\mathrm{LTP}} - \\overline{w^{\\mathrm{LTP}}} \\right)}{\\sqrt{\\sum_{i=1}^{N} \\left( w_i(T) - \\overline{w(T)} \\right)^2} \\sqrt{\\sum_{i=1}^{N} \\left( w_i^{\\mathrm{LTP}} - \\overline{w^{\\mathrm{LTP}}} \\right)^2}},\n$$\nwhere $\\overline{w(T)}$ and $\\overline{w^{\\mathrm{LTP}}}$ are the means of the final and post-LTP weight vectors, respectively.\n- $M_4$: a boolean indicating whether relative differences are preserved within a tight tolerance, defined as $M_2 \\le \\epsilon$ with $\\epsilon = 10^{-9}$.\n\nImplement the simulation as follows from first principles: compute the pre-LTP baseline $r(0)$, apply the LTP to obtain $\\mathbf{w}^{\\mathrm{LTP}}$, set $r^* = r(0)$, and iterate the slow scaling dynamics using a small time step $\\Delta t$ and rate constant $\\beta$. At each step, adjust all weights multiplicatively in a manner consistent with homeostatic scaling that moves $r(t)$ toward $r^*$, enforce the lower bound $w_{\\min}$, and proceed until $t = T$. After the simulation, compute the metrics $M_1$, $M_2$, $M_3$, and $M_4$ for each test case.\n\nTest suite:\n- Case $1$ (happy path): $N = 10$, initial weights $\\mathbf{w}(0) = [0.4, 0.5, 0.3, 0.6, 0.45, 0.55, 0.35, 0.25, 0.5, 0.4]$ a.u., presynaptic rates $\\mathbf{x} = [5, 7, 6, 3, 4, 8, 2, 1, 5, 4]$ Hz, LTP subset $S = \\{0,1,2,3,4\\}$ (zero-based indexing), fractional LTP $\\gamma = 0.4$, time step $\\Delta t = 0.01$ s, duration $T = 5$ s, rate constant $\\beta = 0.05$, lower bound $w_{\\min} = 0.05$ a.u.\n- Case $2$ (boundary with floor clipping): $N = 10$, initial weights $\\mathbf{w}(0) = [0.12, 0.15, 0.1, 0.08, 0.2, 0.18, 0.11, 0.09, 0.14, 0.13]$ a.u., presynaptic rates $\\mathbf{x} = [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]$ Hz, LTP subset $S = \\{0,2,4,6,8\\}$, fractional LTP $\\gamma = 0.8$, time step $\\Delta t = 0.02$ s, duration $T = 20$ s, rate constant $\\beta = 0.1$, lower bound $w_{\\min} = 0.1$ a.u.\n- Case $3$ (small LTP on single synapse): $N = 6$, initial weights $\\mathbf{w}(0) = [0.5, 0.6, 0.4, 0.3, 0.7, 0.2]$ a.u., presynaptic rates $\\mathbf{x} = [2, 2, 2, 2, 2, 2]$ Hz, LTP subset $S = \\{4\\}$, fractional LTP $\\gamma = 0.1$, time step $\\Delta t = 0.01$ s, duration $T = 10$ s, rate constant $\\beta = 0.05$, lower bound $w_{\\min} = 0.001$ a.u.\n\nYour program should produce a single line of output containing the results as a comma-separated list of per-case vectors, with no spaces, where each per-case vector is itself a comma-separated list enclosed in square brackets in the order $[M_1,M_2,M_3,M_4]$. Each float must be rounded to $6$ decimal places, and the boolean must be printed as either True or False. For example, the format is $[[m_{11},m_{12},m_{13},m_{14}],[m_{21},m_{22},m_{23},m_{24}],[m_{31},m_{32},m_{33},m_{34}]]$ with no whitespace.", "solution": "The user-provided problem is assessed as valid. It is a well-posed, scientifically grounded problem in computational neurobiology that requires the simulation of standard models of synaptic plasticity. All necessary parameters and definitions are provided, and the problem setup is internally consistent.\n\nThe solution proceeds by first principles, as requested. We formalize the concepts of Hebbian potentiation and homeostatic scaling, then implement a numerical simulation to track synaptic weight dynamics, and finally compute the specified metrics.\n\n### 1. Principles and Formalization\n\nThe model is built upon three core components: a linear model for neuronal firing rate, a rule for Hebbian-style Long-Term Potentiation (LTP), and a dynamic rule for homeostatic synaptic scaling.\n\n**a. Firing Rate Model**\nThe postsynaptic neuron's firing rate, $r(t)$, is modeled as a linear weighted sum of its $N$ presynaptic inputs, which have constant firing rates $x_i$:\n$$\nr(t) = \\sum_{i=1}^{N} w_i(t) x_i\n$$\nwhere $w_i(t)$ is the synaptic weight of the $i$-th synapse at time $t$.\n\n**b. Long-Term Potentiation (LTP) Event**\nAt time $t=0$, an LTP event is induced. This is modeled as a one-time, instantaneous, fractional increase of weights for a specified subset of synapses $S \\subset \\{1, \\dots, N\\}$. The weights immediately after LTP, denoted $\\mathbf{w}^{\\mathrm{LTP}}$, are given by:\n$$\nw_i^{\\mathrm{LTP}} =\n\\begin{cases}\nw_i(0) (1 + \\gamma)  \\text{if } i \\in S \\\\\nw_i(0)  \\text{if } i \\notin S\n\\end{cases}\n$$\nwhere $\\gamma$ is the fractional potentiation strength. These post-LTP weights serve as the initial condition for the subsequent homeostatic scaling process.\n\n**c. Homeostatic Synaptic Scaling Dynamics**\nHomeostatic scaling is a slower process that multiplicatively adjusts all synapses to restore the neuron's average firing rate to a target set-point, $r^*$. The problem specifies that this target rate is the baseline firing rate before the LTP event:\n$$\nr^* = r(0) = \\sum_{i=1}^{N} w_i(0) x_i\n$$\nThe dynamics of each weight $w_i$ are governed by a canonical model for multiplicative scaling, which can be expressed as a differential equation:\n$$\n\\frac{dw_i(t)}{dt} = - \\beta w_i(t) (r(t) - r^*)\n$$\nHere, $\\beta$ is a rate constant. The term $(r(t) - r^*)$ is the error signal; if the current rate $r(t)$ is above the target $r^*$, the change $\\frac{dw_i}{dt}$ is negative, causing weights to decrease (down-scaling). The change is proportional to the current weight $w_i(t)$, ensuring the scaling is multiplicative.\n\nFor numerical simulation, we discretize this equation using the first-order Euler method with a time step $\\Delta t$:\n$$\nw_i(t + \\Delta t) = w_i(t) + \\Delta t \\left( - \\beta w_i(t) (r(t) - r^*) \\right)\n$$\n$$\nw_i(t + \\Delta t) = w_i(t) \\left[ 1 - \\beta (r(t) - r^*) \\Delta t \\right]\n$$\nAfter each update step, the biological constraint that synaptic weights cannot be negative or fall below a minimal efficacy is enforced by applying a floor at $w_{\\min}$:\n$$\nw_i(t + \\Delta t) \\leftarrow \\max(w_i(t + \\Delta t), w_{\\min})\n$$\n\n### 2. Algorithmic Implementation\n\nThe simulation proceeds algorithmically as follows for each test case:\n\n1.  **Initialization**: Given initial weights $\\mathbf{w}(0)$, presynaptic rates $\\mathbf{x}$, and parameters $N, S, \\gamma, \\Delta t, T, \\beta, w_{\\min}$. Store the vector $\\mathbf{w}(0)$ as $\\mathbf{w}_{\\text{initial}}$.\n2.  **Set Target Rate**: Calculate the baseline firing rate $r^* = \\sum_{i=1}^{N} w_i(0) x_i$.\n3.  **Apply LTP**: Calculate the post-LTP weights $\\mathbf{w}^{\\mathrm{LTP}}$ using the rule defined above. This vector is stored for later use in metric calculations. The simulation starts with $\\mathbf{w}_{\\text{current}} = \\mathbf{w}^{\\mathrm{LTP}}$.\n4.  **Iterative Scaling**: A loop runs for a total of $k = T / \\Delta t$ steps. In each step:\n    a. Calculate the current firing rate: $r_{\\text{current}} = \\sum_{i=1}^{N} w_{i, \\text{current}} x_i$.\n    b. Calculate the multiplicative scaling factor: $f = 1 - \\beta (r_{\\text{current}} - r^*) \\Delta t$.\n    c. Update all weights: $w_{i, \\text{current}} \\leftarrow w_{i, \\text{current}} \\cdot f$.\n    d. Enforce the weight floor: $w_{i, \\text{current}} \\leftarrow \\max(w_{i, \\text{current}}, w_{\\min})$ for all $i$.\n5.  **Compute Metrics**: After the simulation loop completes, the final weight vector is $\\mathbf{w}(T) = \\mathbf{w}_{\\text{current}}$. The following metrics are then computed:\n    -  $M_1 = \\frac{1}{N} \\sum_{i=1}^{N} \\left| w_i(T) - w_i(0) \\right|$: The mean absolute change relative to the pre-LTP initial state.\n    -  $M_2 = \\max_{i \\neq j} \\left| \\frac{ w_i(T) / w_j(T) }{ w_i^{\\mathrm{LTP}} / w_j^{\\mathrm{LTP}} } - 1 \\right|$: The maximum deviation from perfect preservation of weight ratios. Purely multiplicative scaling (without clipping) would yield $w_i(T) = c \\cdot w_i^{\\mathrm{LTP}}$ for some constant $c$, resulting in $M_2 = 0$. The floor constraint $w_{\\min}$ introduces a non-linearity that can cause $M_2  0$.\n    -  $M_3 = \\text{PearsonCorr}(\\mathbf{w}(T), \\mathbf{w}^{\\mathrm{LTP}})$: The Pearson correlation coefficient measures the linear relationship between the final and post-LTP weight vectors. It should be very close to $1$ if relative differences are preserved.\n    -  $M_4 = (M_2 \\le 10^{-9})$: A boolean flag indicating high-fidelity preservation of relative weight differences.\n\nThis procedure is implemented for each test case provided.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Formalizes and simulates metaplasticity expressing homeostatic synaptic scaling.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 10,\n            \"w0\": np.array([0.4, 0.5, 0.3, 0.6, 0.45, 0.55, 0.35, 0.25, 0.5, 0.4], dtype=np.float64),\n            \"x\": np.array([5, 7, 6, 3, 4, 8, 2, 1, 5, 4], dtype=np.float64),\n            \"S\": {0, 1, 2, 3, 4},\n            \"gamma\": 0.4,\n            \"dt\": 0.01,\n            \"T\": 5,\n            \"beta\": 0.05,\n            \"w_min\": 0.05,\n        },\n        {\n            \"N\": 10,\n            \"w0\": np.array([0.12, 0.15, 0.1, 0.08, 0.2, 0.18, 0.11, 0.09, 0.14, 0.13], dtype=np.float64),\n            \"x\": np.array([10, 9, 8, 7, 6, 5, 4, 3, 2, 1], dtype=np.float64),\n            \"S\": {0, 2, 4, 6, 8},\n            \"gamma\": 0.8,\n            \"dt\": 0.02,\n            \"T\": 20,\n            \"beta\": 0.1,\n            \"w_min\": 0.1,\n        },\n        {\n            \"N\": 6,\n            \"w0\": np.array([0.5, 0.6, 0.4, 0.3, 0.7, 0.2], dtype=np.float64),\n            \"x\": np.array([2, 2, 2, 2, 2, 2], dtype=np.float64),\n            \"S\": {4},\n            \"gamma\": 0.1,\n            \"dt\": 0.01,\n            \"T\": 10,\n            \"beta\": 0.05,\n            \"w_min\": 0.001,\n        },\n    ]\n\n    all_results = []\n    epsilon = 1e-9\n\n    for case in test_cases:\n        w_initial = case[\"w0\"]\n        x = case[\"x\"]\n        S = case[\"S\"]\n        gamma = case[\"gamma\"]\n        dt = case[\"dt\"]\n        T = case[\"T\"]\n        beta = case[\"beta\"]\n        w_min = case[\"w_min\"]\n        N = case[\"N\"]\n\n        # 1. Calculate baseline and target rate r*\n        r_star = np.dot(w_initial, x)\n\n        # 2. Apply LTP to get w_LTP\n        w_ltp = w_initial.copy()\n        for i in S:\n            w_ltp[i] *= (1 + gamma)\n\n        # 3. Simulate slow synaptic scaling\n        w_current = w_ltp.copy()\n        num_steps = int(T / dt)\n\n        for _ in range(num_steps):\n            # a. Calculate current firing rate\n            r_current = np.dot(w_current, x)\n            \n            # b. Calculate multiplicative factor\n            factor = 1 - beta * (r_current - r_star) * dt\n            \n            # c. Update weights\n            w_current *= factor\n            \n            # d. Enforce lower bound\n            w_current = np.maximum(w_current, w_min)\n            \n        w_final = w_current\n\n        # 4. Compute metrics\n        # M1: Mean absolute change from initial state\n        m1 = np.mean(np.abs(w_final - w_initial))\n\n        # M2: Max normalized ratio error\n        # E_ij = | (w_i(T)/w_j(T)) / (w_i_LTP/w_j_LTP) - 1 |\n        #      = | (w_i(T) * w_j_LTP) / (w_j(T) * w_i_LTP) - 1 |\n        w_final_col = w_final[:, np.newaxis]\n        w_ltp_col = w_ltp[:, np.newaxis]\n        \n        # Denominator matrix contains terms w_j(T) * w_i_LTP\n        # All weights are strictly positive, so no division by zero\n        denom_matrix = w_final * w_ltp_col\n        # Numerator matrix contains terms w_i(T) * w_j_LTP\n        numer_matrix = w_final_col * w_ltp\n\n        # Suppress division by zero warnings, though not expected here\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio_matrix = numer_matrix / denom_matrix\n        \n        E_matrix = np.abs(ratio_matrix - 1)\n        np.fill_diagonal(E_matrix, 0) # Exclude i=j cases\n        \n        m2 = np.max(E_matrix)\n        if np.isnan(m2):\n            m2 = 0.0\n\n        # M3: Pearson correlation between w_final and w_ltp\n        # np.corrcoef returns a 2x2 matrix, we need the off-diagonal element\n        corr_matrix = np.corrcoef(w_final, w_ltp)\n        m3 = corr_matrix[0, 1]\n\n        # M4: Boolean for relative difference preservation\n        m4 = m2 = epsilon\n\n        all_results.append((m1, m2, m3, m4))\n\n    # Format the final output string\n    output_str = \",\".join(\n        f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f},{res[3]}]\" for res in all_results\n    )\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "5032199"}]}