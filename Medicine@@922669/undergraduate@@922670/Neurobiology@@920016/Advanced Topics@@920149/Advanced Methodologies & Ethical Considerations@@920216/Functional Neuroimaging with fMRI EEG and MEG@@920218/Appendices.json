{"hands_on_practices": [{"introduction": "In electroencephalography (EEG) and magnetoencephalography (MEG), the brain signals evoked by a specific event are often minuscule compared to ongoing background noise. To reveal these event-related potentials (ERPs) or fields (ERFs), we rely on the power of signal averaging across many trials. This exercise [@problem_id:5018722] provides a hands-on calculation to demonstrate the fundamental relationship between the number of trials and the resulting signal-to-noise ratio, a critical consideration for designing any ERP/ERF experiment.", "problem": "In functional neuroimaging, Electroencephalography (EEG) is used to measure event-related potentials (ERP) by averaging time-locked responses across repeated trials to improve the Signal-to-Noise Ratio (SNR). Consider an ERP component whose single-trial signal amplitude is $5\\,\\mu\\mathrm{V}$ and whose additive noise per trial is zero-mean, stationary, and independent across trials with a standard deviation of $20\\,\\mu\\mathrm{V}$. Use the following foundational facts: (i) SNR is defined as the ratio of signal amplitude to noise standard deviation, and (ii) for independent and identically distributed noise across trials, the variance of the arithmetic mean of $N$ trials is reduced by a factor of $N$ relative to the variance of a single trial. Assume the signal is perfectly phase-locked and identical across trials, so averaging preserves its amplitude. Derive from these principles an expression for the SNR of the averaged ERP as a function of $N$, and determine the number of trials $N$ required to achieve $\\text{SNR} = 5$ in the averaged ERP. Express your final answer as an exact integer without units.", "solution": "Let $S_1$ denote the signal amplitude of a single trial and $\\sigma_1$ denote the standard deviation of the noise in a single trial. The problem provides these values as:\n$$ S_1 = 5\\,\\mu\\mathrm{V} $$\n$$ \\sigma_1 = 20\\,\\mu\\mathrm{V} $$\nThe Signal-to-Noise Ratio, or SNR, is defined as the ratio of signal amplitude to noise standard deviation. Therefore, the SNR for a single trial, denoted $\\text{SNR}_1$, is:\n$$ \\text{SNR}_1 = \\frac{S_1}{\\sigma_1} = \\frac{5\\,\\mu\\mathrm{V}}{20\\,\\mu\\mathrm{V}} = \\frac{1}{4} $$\nThe process of creating an event-related potential (ERP) involves averaging the signals over $N$ repeated trials. Let $S_N$ be the signal amplitude of the averaged ERP and $\\sigma_N$ be the standard deviation of the noise of the averaged ERP.\n\nAccording to the problem statement, the signal component is perfectly phase-locked and identical across all trials. When we compute the arithmetic mean of $N$ identical signals, the resulting amplitude is unchanged.\n$$ S_N = \\frac{1}{N} \\sum_{i=1}^{N} S_1 = \\frac{N \\cdot S_1}{N} = S_1 = 5\\,\\mu\\mathrm{V} $$\nThe noise component is stated to be zero-mean, stationary, and independent across trials. The problem specifies that for such noise, the variance of the arithmetic mean of $N$ trials is reduced by a factor of $N$ relative to the variance of a single trial. Let $\\text{Var}_1 = \\sigma_1^2$ be the variance of the noise in a single trial, and let $\\text{Var}_N$ be the variance of the noise in the averaged ERP. The relationship is:\n$$ \\text{Var}_N = \\frac{\\text{Var}_1}{N} = \\frac{\\sigma_1^2}{N} $$\nThe standard deviation is the square root of the variance. Thus, the standard deviation of the noise in the averaged ERP, $\\sigma_N$, is:\n$$ \\sigma_N = \\sqrt{\\text{Var}_N} = \\sqrt{\\frac{\\sigma_1^2}{N}} = \\frac{\\sigma_1}{\\sqrt{N}} $$\nWe can now derive the expression for the SNR of the averaged ERP, $\\text{SNR}_N$, as a function of the number of trials $N$.\n$$ \\text{SNR}_N = \\frac{S_N}{\\sigma_N} = \\frac{S_1}{\\left(\\frac{\\sigma_1}{\\sqrt{N}}\\right)} = \\left(\\frac{S_1}{\\sigma_1}\\right) \\sqrt{N} $$\nThis can be expressed in terms of the single-trial SNR:\n$$ \\text{SNR}_N = \\text{SNR}_1 \\sqrt{N} $$\nSubstituting the value of $\\text{SNR}_1 = \\frac{1}{4}$, we obtain the specific expression for this problem:\n$$ \\text{SNR}_N = \\frac{1}{4} \\sqrt{N} $$\nThis equation is the first required part of the solution, representing the SNR of the averaged ERP as a function of $N$.\n\nThe second part of the task is to determine the number of trials $N$ required to achieve an SNR of $5$. We set $\\text{SNR}_N = 5$ in the derived equation and solve for $N$:\n$$ 5 = \\frac{1}{4} \\sqrt{N} $$\nTo isolate $\\sqrt{N}$, we multiply both sides of the equation by $4$:\n$$ 20 = \\sqrt{N} $$\nFinally, to solve for $N$, we square both sides of the equation:\n$$ N = 20^2 = 400 $$\nTherefore, a total of $400$ trials are required to achieve a Signal-to-Noise Ratio of $5$ in the averaged ERP.", "answer": "$$\\boxed{400}$$", "id": "5018722"}, {"introduction": "Once fMRI data is acquired, the General Linear Model (GLM) is the standard framework used to estimate brain activity related to a task. However, fMRI noise is not random over time; it exhibits temporal autocorrelation that violates the assumptions of ordinary least squares. This problem [@problem_id:5018724] takes you under the hood of fMRI analysis to apply the more appropriate Generalized Least Squares (GLS) method, demonstrating how to obtain accurate estimates and statistics in the presence of correlated noise.", "problem": "A single-voxel time series from functional Magnetic Resonance Imaging (fMRI) is modeled by a General Linear Model with correlated errors. The data consist of $T=4$ time points acquired at a repetition time of $2$ seconds during a simple block design with one task regressor convolved with a canonical Hemodynamic Response Function. The design matrix is\n$$\nX=\\begin{pmatrix}\n1  0 \\\\\n1  1 \\\\\n1  1 \\\\\n1  0\n\\end{pmatrix},\n$$\nwhere the first column is an intercept and the second column is the task regressor. The observed Blood Oxygenation Level Dependent (BOLD) signal is\n$$\ny=\\begin{pmatrix}\n0.1 \\\\\n0.9 \\\\\n0.8 \\\\\n0.2\n\\end{pmatrix}.\n$$\nAssume the noise $\\epsilon$ is zero-mean Gaussian with known first-order autoregressive (AR(1), autoregressive of order $1$) autocorrelation parameter $\\rho=0.5$ and covariance $\\operatorname{Var}(\\epsilon)=\\sigma^{2}V$, where $V$ is the AR(1) correlation matrix with entries $V_{ij}=\\rho^{|i-j|}$. Let the contrast of interest isolate the task effect, with contrast vector\n$$\nc=\\begin{pmatrix}\n0 \\\\\n1\n\\end{pmatrix}.\n$$\nStarting from the General Linear Model assumptions and the properties of Gaussian noise with known autocorrelation, derive and compute:\n- the contrast estimate $c^{\\top}\\hat{\\beta}$,\n- its sampling variance under generalized least squares,\n- and the corresponding $t$-statistic using the residual variance estimate with $T-p$ degrees of freedom, where $p$ is the number of regressors.\n\nRound your final numerical values to four significant figures. Express your final answer as a single row matrix containing, in order, the three quantities listed above.", "solution": "The problem is described by the General Linear Model (GLM):\n$$y = X\\beta + \\epsilon$$\nwhere $y$ is the $T \\times 1$ vector of observed data, $X$ is the $T \\times p$ design matrix, $\\beta$ is the $p \\times 1$ vector of parameters to be estimated, and $\\epsilon$ is a $T \\times 1$ vector of noise terms. The noise is assumed to be Gaussian with a mean of zero and a covariance matrix $\\operatorname{Var}(\\epsilon) = \\sigma^2 V$, where $\\sigma^2$ is the noise variance and $V$ is the $T \\times T$ correlation matrix. Since $V$ is not the identity matrix ($V \\neq I$), the errors are correlated, and the appropriate estimation method is Generalized Least Squares (GLS).\n\nThe GLS estimator for $\\beta$, which is the Best Linear Unbiased Estimator (BLUE), is given by:\n$$\\hat{\\beta} = (X^\\top V^{-1} X)^{-1} X^\\top V^{-1} y$$\nThe quantities to be computed are:\n1. Contrast estimate: $c^\\top \\hat{\\beta}$\n2. Sampling variance of the contrast: This is interpreted as the estimated variance, $\\widehat{\\operatorname{Var}}(c^\\top \\hat{\\beta}) = \\hat{\\sigma}^2 c^\\top (X^\\top V^{-1} X)^{-1} c$.\n3. The $t$-statistic: $t = \\frac{c^\\top \\hat{\\beta}}{\\sqrt{\\widehat{\\operatorname{Var}}(c^\\top \\hat{\\beta})}}$.\n\nFirst, we must construct the matrix $V$ and its inverse $V^{-1}$. Given $T=4$ and $\\rho=0.5$, the AR(1) correlation matrix $V$ is:\n$$V = \\begin{pmatrix}\n1  \\rho  \\rho^2  \\rho^3 \\\\\n\\rho  1  \\rho  \\rho^2 \\\\\n\\rho^2  \\rho  1  \\rho \\\\\n\\rho^3  \\rho^2  \\rho  1\n\\end{pmatrix} = \\begin{pmatrix}\n1  0.5  0.25  0.125 \\\\\n0.5  1  0.5  0.25 \\\\\n0.25  0.5  1  0.5 \\\\\n0.125  0.25  0.5  1\n\\end{pmatrix}$$\nThe inverse of an AR(1) correlation matrix is a sparse tridiagonal matrix:\n$$V^{-1} = \\frac{1}{1-\\rho^2} \\begin{pmatrix}\n1  -\\rho  0  0 \\\\\n-\\rho  1+\\rho^2  -\\rho  0 \\\\\n0  -\\rho  1+\\rho^2  -\\rho \\\\\n0  0  -\\rho  1\n\\end{pmatrix}$$\nSubstituting $\\rho=0.5$:\n$1-\\rho^2 = 1 - (0.5)^2 = 0.75$.\n$1+\\rho^2 = 1 + (0.5)^2 = 1.25$.\n$$V^{-1} = \\frac{1}{0.75} \\begin{pmatrix}\n1  -0.5  0  0 \\\\\n-0.5  1.25  -0.5  0 \\\\\n0  -0.5  1.25  -0.5 \\\\\n0  0  -0.5  1\n\\end{pmatrix} = \\frac{4}{3} \\begin{pmatrix}\n1  -0.5  0  0 \\\\\n-0.5  1.25  -0.5  0 \\\\\n0  -0.5  1.25  -0.5 \\\\\n0  0  -0.5  1\n\\end{pmatrix} = \\begin{pmatrix}\n4/3  -2/3  0  0 \\\\\n-2/3  5/3  -2/3  0 \\\\\n0  -2/3  5/3  -2/3 \\\\\n0  0  -2/3  4/3\n\\end{pmatrix}$$\n\nNext, we compute the term $X^\\top V^{-1} X$:\n$$X^\\top V^{-1} X = \\begin{pmatrix} 2  2/3 \\\\ 2/3  2 \\end{pmatrix}$$\nThe inverse is:\n$$\\det(X^\\top V^{-1} X) = 2 \\cdot 2 - (2/3)^2 = 4 - 4/9 = 32/9$$\n$$(X^\\top V^{-1} X)^{-1} = \\frac{1}{32/9} \\begin{pmatrix} 2  -2/3 \\\\ -2/3  2 \\end{pmatrix} = \\frac{9}{32} \\begin{pmatrix} 2  -2/3 \\\\ -2/3  2 \\end{pmatrix} = \\begin{pmatrix} 9/16  -3/16 \\\\ -3/16  9/16 \\end{pmatrix}$$\nNext, we compute $X^\\top V^{-1} y$:\n$$X^\\top V^{-1} y = \\begin{pmatrix} 23/30 \\\\ 1.5 \\end{pmatrix}$$\nNow, we can compute $\\hat{\\beta}$:\n$$\\hat{\\beta} = (X^\\top V^{-1} X)^{-1} (X^\\top V^{-1} y) = \\begin{pmatrix} 9/16  -3/16 \\\\ -3/16  9/16 \\end{pmatrix} \\begin{pmatrix} 23/30 \\\\ 3/2 \\end{pmatrix} = \\begin{pmatrix} 0.15 \\\\ 0.7 \\end{pmatrix}$$\nSo, $\\hat{\\beta_0}=0.15$ and $\\hat{\\beta_1}=0.7$.\n\n1. Compute the contrast estimate $c^\\top\\hat{\\beta}$:\nWith $c = (0, 1)^\\top$, the contrast estimate is:\n$$c^\\top \\hat{\\beta} = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 0.15 \\\\ 0.7 \\end{pmatrix} = 0.7$$\n\n2. Compute the estimated sampling variance of the contrast.\nFirst, we need to estimate the residual variance $\\sigma^2$. The unbiased GLS estimator is:\n$$\\hat{\\sigma}^2 = \\frac{e^\\top V^{-1} e}{T-p}$$\nwhere $e = y - X\\hat{\\beta}$ are the residuals and $T-p=4-2=2$ are the degrees of freedom.\n$$\\hat{y} = X\\hat{\\beta} = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 1  1 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 0.15 \\\\ 0.7 \\end{pmatrix} = \\begin{pmatrix} 0.15 \\\\ 0.85 \\\\ 0.85 \\\\ 0.15 \\end{pmatrix}$$\n$$e = y - \\hat{y} = \\begin{pmatrix} 0.1 \\\\ 0.9 \\\\ 0.8 \\\\ 0.2 \\end{pmatrix} - \\begin{pmatrix} 0.15 \\\\ 0.85 \\\\ 0.85 \\\\ 0.15 \\end{pmatrix} = \\begin{pmatrix} -0.05 \\\\ 0.05 \\\\ -0.05 \\\\ 0.05 \\end{pmatrix}$$\nThe whitened sum of squared errors is $e^\\top V^{-1} e$. A known identity is $e^\\top V^{-1} e = y^\\top V^{-1} y - \\hat{\\beta}^\\top X^\\top V^{-1} y$.\nWe have $X^\\top V^{-1} y = (23/30, 1.5)^\\top$ and $\\hat{\\beta} = (0.15, 0.7)^\\top$. So $\\hat{\\beta}^\\top X^\\top V^{-1} y = 0.15(23/30) + 0.7(1.5) = 0.115 + 1.05 = 1.165$.\nWe calculate $y^\\top V^{-1} y = 1.19$.\nSo, $e^\\top V^{-1} e = 1.19 - 1.165 = 0.025$.\n$$\\hat{\\sigma}^2 = \\frac{0.025}{2} = 0.0125$$\nThe estimated sampling variance of the contrast is:\n$$\\widehat{\\operatorname{Var}}(c^\\top \\hat{\\beta}) = \\hat{\\sigma}^2 c^\\top (X^\\top V^{-1} X)^{-1} c$$\n$$c^\\top (X^\\top V^{-1} X)^{-1} c = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 9/16  -3/16 \\\\ -3/16  9/16 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 9/16$$\n$$\\widehat{\\operatorname{Var}}(c^\\top \\hat{\\beta}) = 0.0125 \\times \\frac{9}{16} = 0.00703125$$\n\n3. Compute the $t$-statistic:\n$$t = \\frac{c^\\top \\hat{\\beta}}{\\sqrt{\\widehat{\\operatorname{Var}}(c^\\top \\hat{\\beta})}} = \\frac{0.7}{\\sqrt{0.00703125}} = \\frac{0.7}{0.0838525...} \\approx 8.347987...$$\n\nFinally, we round the three quantities to four significant figures:\n- Contrast estimate: $0.7000$\n- Sampling variance: $0.007031$\n- $t$-statistic: $8.348$\nThese three values form the final answer.", "answer": "$$ \\boxed{ \\begin{pmatrix} 0.7000  0.007031  8.348 \\end{pmatrix} } $$", "id": "5018724"}, {"introduction": "A typical fMRI study involves testing for activation in tens or even hundreds of thousands of voxels, creating a massive multiple comparisons problem where the chance of finding false positives is extremely high. Simply using a conventional p-value threshold is statistically invalid and would lead to spurious results. This practice problem [@problem_id:5018733] lets you calculate and compare two essential correction strategies—the stringent Bonferroni correction and the more powerful Benjamini-Hochberg False Discovery Rate (FDR)—to understand how we can draw valid conclusions from whole-brain analyses.", "problem": "In a task-based functional Magnetic Resonance Imaging (fMRI) study, a mass-univariate general linear model is fit at each voxel, producing a vector of voxel-wise p-values across the brain. Assume there are $50{,}000$ voxels, and that the ordered p-values $\\{p_{(k)}\\}_{k=1}^{50{,}000}$ (sorted from smallest to largest) follow the empirically observed monotone relationship\n$$\np_{(k)} \\;=\\; 0.12 \\left(\\frac{k}{50{,}000}\\right)^{1.5} \\quad \\text{for} \\quad k=1,2,\\dots,50{,}000.\n$$\nUsing only fundamental definitions, compute:\n1) The family-wise error rate (FWER) controlling Bonferroni threshold at overall level $\\alpha=0.05$ across $50{,}000$ voxel-wise tests.\n2) The Benjamini–Hochberg false discovery rate (FDR) threshold at target level $q=0.05$ applied to the ordered p-values above, defined as the largest rank-dependent threshold at which the expected proportion of false rejections among all rejections does not exceed $q$.\n\nReport the two thresholds as a two-entry row vector in the order $[\\text{Bonferroni threshold}, \\text{Benjamini–Hochberg threshold}]$. Round both thresholds to four significant figures. No units are required. Finally, briefly explain, based on first principles about error-rate control, why the Benjamini–Hochberg procedure is typically more powerful than Bonferroni in high-dimensional neuroimaging.\n\nExpress the final numerical thresholds in your answer as requested above, rounded to four significant figures.", "solution": "Let $m$ be the total number of voxel-wise tests, where $m = 50,000$. The ordered p-values are given by the model $p_{(k)} = 0.12 \\left(\\frac{k}{m}\\right)^{1.5}$ for $k=1, 2, \\dots, m$.\n\n**1) Bonferroni Threshold**\n\nThe Bonferroni correction controls the family-wise error rate (FWER), which is the probability of making at least one Type I error (a false positive) across all $m$ tests. To control the FWER at a specified level $\\alpha$, the Bonferroni method applies a uniform significance threshold, $\\alpha_{bonf}$, to each individual test. This threshold is derived from the Bonferroni inequality, which states that $FWER \\le m \\cdot \\alpha_{test}$. To ensure $FWER \\le \\alpha$, we set the threshold for each test to be:\n$$\n\\alpha_{bonf} = \\frac{\\alpha}{m}\n$$\nGiven the overall significance level $\\alpha = 0.05$ and the number of tests $m = 50,000$, the Bonferroni threshold is:\n$$\n\\alpha_{bonf} = \\frac{0.05}{50,000} = \\frac{5 \\times 10^{-2}}{5 \\times 10^4} = 1 \\times 10^{-6}\n$$\nRounding this to four significant figures, the Bonferroni threshold is $1.000 \\times 10^{-6}$. Under this correction, any voxel with a p-value $p \\le 1.000 \\times 10^{-6}$ would be declared statistically significant.\n\n**2) Benjamini–Hochberg False Discovery Rate (FDR) Threshold**\n\nThe Benjamini–Hochberg (BH) procedure controls the False Discovery Rate (FDR), which is the expected proportion of Type I errors among all rejected null hypotheses (i.e., all \"discoveries\"). The procedure is as follows:\n1.  Order the $m$ p-values from smallest to largest: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$.\n2.  For a target FDR level $q$, find the largest integer index $k$, denoted $k_{FDR}$, such that the p-value at that rank satisfies the condition:\n    $$\n    p_{(k)} \\le \\frac{k}{m} q\n    $$\n3.  Reject all null hypotheses $H_{(i)}$ for $i=1, \\dots, k_{FDR}$. This is equivalent to setting a significance threshold equal to $p_{(k_{FDR})}$.\n\nWe are given the model for the ordered p-values: $p_{(k)} = 0.12 \\left(\\frac{k}{m}\\right)^{1.5}$. We substitute this into the BH inequality with $q = 0.05$ and $m = 50,000$:\n$$\n0.12 \\left(\\frac{k}{m}\\right)^{1.5} \\le \\frac{k}{m} \\cdot 0.05\n$$\nFor $k > 0$, we can divide both sides by $\\frac{k}{m}$, which is a positive quantity:\n$$\n0.12 \\left(\\frac{k}{m}\\right)^{0.5} \\le 0.05\n$$\nSolving for the ratio $\\frac{k}{m}$:\n$$\n\\left(\\frac{k}{m}\\right)^{0.5} \\le \\frac{0.05}{0.12}\n$$\n$$\n\\frac{k}{m} \\le \\left(\\frac{0.05}{0.12}\\right)^2 = \\left(\\frac{5}{12}\\right)^2 = \\frac{25}{144}\n$$\nNow, we find the maximum integer rank $k$ that satisfies this condition:\n$$\nk \\le m \\cdot \\frac{25}{144} = 50,000 \\cdot \\frac{25}{144} = \\frac{1,250,000}{144} \\approx 8680.555...\n$$\nSince $k$ must be an integer, the largest rank satisfying the inequality is $k_{FDR} = 8680$.\n\nThe BH threshold is the p-value corresponding to this rank, $p_{(k_{FDR})}$. We compute this using the given model for $p_{(k)}$ with $k = 8680$:\n$$\np_{FDR} = p_{(8680)} = 0.12 \\left(\\frac{8680}{50,000}\\right)^{1.5}\n$$\nFirst, calculate the ratio:\n$$\n\\frac{8680}{50,000} = 0.1736\n$$\nNow substitute this into the expression for the threshold:\n$$\np_{FDR} = 0.12 \\times (0.1736)^{1.5} \\approx 0.008680\n$$\nRounding this result to four significant figures, the Benjamini–Hochberg threshold is $0.008680$.\n\n**Explanation of Power Difference**\n\nThe Benjamini–Hochberg (BH) procedure is typically more powerful than the Bonferroni correction in high-dimensional settings like neuroimaging because it is based on a less stringent error control criterion.\n\n1.  **Error Criterion**: The Bonferroni method controls the Family-Wise Error Rate (FWER), the probability of making even a single false discovery ($P(\\text{at least one false positive}) \\le \\alpha$). This is an extremely strict criterion when the number of tests ($m$) is large, as it aims to prevent any false positives across the entire \"family\" of tests. This stringency results in a very low significance threshold ($\\alpha_{bonf} = \\alpha/m = 1 \\times 10^{-6}$), which reduces the ability to detect true effects (i.e., it has low statistical power).\n\n2.  **Adaptive Thresholding**: In contrast, the BH procedure controls the False Discovery Rate (FDR), the expected *proportion* of false discoveries among all significant findings ($E[\\frac{\\text{False Positives}}{\\text{Total Discoveries}}] \\le q$). This criterion acknowledges that in a large-scale analysis, a small number of false positives may be tolerable as long as they constitute a small fraction of the total findings. The BH threshold is not fixed but is determined adaptively from the data themselves. It is the largest p-value, $p_{(k)}$, that remains below the line $\\frac{k}{m}q$.\n\n3.  **Resulting Power**: Because the FDR criterion is less conservative than the FWER criterion, the resulting significance threshold for the BH procedure ($p_{FDR} \\approx 0.008680$) is typically much larger than the Bonferroni threshold ($\\alpha_{bonf} = 0.000001$). A larger threshold means that more null hypotheses will be rejected. While this increases the chance of making some false discoveries, it substantially increases the probability of detecting true effects, thereby yielding greater statistical power. In fMRI, where thousands of voxels might be truly activated, controlling the proportion of errors (FDR) is a more practical and powerful approach than attempting to eliminate all errors (FWER).", "answer": "$$\n\\boxed{\\begin{pmatrix} 1.000 \\times 10^{-6}  0.008680 \\end{pmatrix}}\n$$", "id": "5018733"}]}