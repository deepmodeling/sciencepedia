{"hands_on_practices": [{"introduction": "To understand how the primary motor cortex (M1) orchestrates movement, we must first examine its building blocks: the individual neurons. This exercise models a large M1 corticospinal neuron, a Betz cell, as a simple electrical circuit to explore how it integrates inputs over time to generate an action potential. By calculating the time-to-threshold based on the membrane's resistance and capacitance, you will gain a hands-on understanding of how a neuron's biophysical properties fundamentally determine the temporal precision of its output signals ([@problem_id:5049102]).", "problem": "A Betz cell is a large layer V corticospinal neuron in the primary motor cortex (M1) whose somatic membrane can be approximated by a resistor-capacitor element. Consider a Betz cell whose membrane potential relative to rest, $V(t)$, evolves under a constant intracellular step current $I$ beginning at $t=0$, according to conservation of charge on a capacitor and Ohm's law: $C \\frac{dV}{dt} + \\frac{V}{R} = I$ for $t \\ge 0$, with initial condition $V(0)=0$. Here $C$ is the membrane capacitance, $R$ is the input resistance, and $I$ is the injected current. The spike threshold above rest is $V_{\\text{th}}$.\n\nUsing only these fundamental relations, derive an expression for the time $t_{\\text{th}}$ at which the membrane potential first reaches threshold, defined by $V(t_{\\text{th}})=V_{\\text{th}}$. Then, for a Betz cell with $I = 1.0 \\times 10^{-9}\\,\\text{A}$, $R = 50 \\times 10^{6}\\,\\Omega$, $C = 5.0 \\times 10^{-10}\\,\\text{F}$, and $V_{\\text{th}} = 15 \\times 10^{-3}\\,\\text{V}$, compute the numerical value of $t_{\\text{th}}$. Assume $I R > V_{\\text{th}}$ so that threshold is achievable. Express your final time in milliseconds and round your numerical answer to four significant figures. Finally, explain qualitatively, using your derived expression, how increasing $C$ influences the temporal precision of spike initiation in such neurons.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of circuit theory as applied to a standard, simplified model of a neuron (the leaky integrate-and-fire model). The problem is well-posed, containing a first-order linear ordinary differential equation with a given initial condition, which guarantees a unique solution. All parameters are defined, and the provided numerical values are physically realistic for a large neuron. The condition $I R > V_{\\text{th}}$ ensures that the threshold is reachable, making the problem solvable.\n\nThe task is to derive the time to threshold, $t_{\\text{th}}$, calculate its numerical value for a specific case, and explain the role of capacitance $C$ in spike timing precision.\n\nFirst, we solve for the membrane potential $V(t)$. The governing equation is a first-order linear ordinary differential equation:\n$$\nC \\frac{dV}{dt} + \\frac{V}{R} = I\n$$\nWe can rearrange this into the standard form $\\frac{dy}{dx} + P(x)y = Q(x)$:\n$$\n\\frac{dV}{dt} + \\frac{1}{RC}V = \\frac{I}{C}\n$$\nLet the membrane time constant be $\\tau_m = RC$. The equation becomes:\n$$\n\\frac{dV}{dt} + \\frac{1}{\\tau_m}V = \\frac{I}{C}\n$$\nThe general solution is the sum of the homogeneous solution, $V_h(t)$, and a particular solution, $V_p(t)$.\nThe homogeneous equation is $\\frac{dV_h}{dt} + \\frac{1}{\\tau_m}V_h = 0$, which has the solution $V_h(t) = A \\exp(-t/\\tau_m)$, where $A$ is a constant of integration.\nFor the particular solution, since the forcing term $I/C$ is a constant, we assume a constant solution $V_p(t) = K$. Substituting this into the full ODE gives:\n$$\n0 + \\frac{K}{\\tau_m} = \\frac{I}{C} \\implies K = \\frac{I\\tau_m}{C} = \\frac{I(RC)}{C} = IR\n$$\nSo, the particular solution is $V_p(t) = IR$.\nThe general solution for the membrane potential is the sum of the homogeneous and particular solutions:\n$$\nV(t) = V_h(t) + V_p(t) = A \\exp(-t/\\tau_m) + IR\n$$\nWe use the initial condition $V(0)=0$ to determine the constant $A$:\n$$\nV(0) = A \\exp(0) + IR = A + IR = 0 \\implies A = -IR\n$$\nSubstituting $A$ back into the general solution gives the specific solution for the membrane potential as a function of time:\n$$\nV(t) = IR - IR \\exp(-t/\\tau_m) = IR \\left(1 - \\exp\\left(-\\frac{t}{RC}\\right)\\right)\n$$\nThis equation describes the charging of the membrane capacitor towards a steady-state voltage of $IR$.\n\nNext, we derive the expression for the time to threshold, $t_{\\text{th}}$. This is the time at which $V(t)$ first reaches the threshold potential $V_{\\text{th}}$. We set $V(t_{\\text{th}}) = V_{\\text{th}}$ and solve for $t_{\\text{th}}$:\n$$\nV_{\\text{th}} = IR \\left(1 - \\exp\\left(-\\frac{t_{\\text{th}}}{RC}\\right)\\right)\n$$\nThe problem states that $IR > V_{\\text{th}}$, which ensures that $0 < V_{\\text{th}}/(IR) < 1$, guaranteeing a real, positive solution for $t_{\\text{th}}$.\nWe rearrange the equation to solve for $t_{\\text{th}}$:\n$$\n\\frac{V_{\\text{th}}}{IR} = 1 - \\exp\\left(-\\frac{t_{\\text{th}}}{RC}\\right)\n$$\n$$\n\\exp\\left(-\\frac{t_{\\text{th}}}{RC}\\right) = 1 - \\frac{V_{\\text{th}}}{IR}\n$$\nTaking the natural logarithm of both sides:\n$$\n-\\frac{t_{\\text{th}}}{RC} = \\ln\\left(1 - \\frac{V_{\\text{th}}}{IR}\\right)\n$$\nFinally, solving for $t_{\\text{th}}$ gives the symbolic expression:\n$$\nt_{\\text{th}} = -RC \\ln\\left(1 - \\frac{V_{\\text{th}}}{IR}\\right)\n$$\nNow, we compute the numerical value of $t_{\\text{th}}$ using the provided parameters:\n$I = 1.0 \\times 10^{-9}\\,\\text{A}$\n$R = 50 \\times 10^{6}\\,\\Omega$\n$C = 5.0 \\times 10^{-10}\\,\\text{F}$\n$V_{\\text{th}} = 15 \\times 10^{-3}\\,\\text{V}$\n\nFirst, we calculate the product terms in SI units:\nThe time constant $\\tau_m = RC$:\n$$\n\\tau_m = (50 \\times 10^{6}\\,\\Omega) \\times (5.0 \\times 10^{-10}\\,\\text{F}) = 250 \\times 10^{-4}\\,\\text{s} = 0.025\\,\\text{s}\n$$\nThe steady-state voltage $V_{\\infty} = IR$:\n$$\nV_{\\infty} = (1.0 \\times 10^{-9}\\,\\text{A}) \\times (50 \\times 10^{6}\\,\\Omega) = 50 \\times 10^{-3}\\,\\text{V} = 0.050\\,\\text{V}\n$$\nThe dimensionless ratio $V_{\\text{th}}/(IR)$:\n$$\n\\frac{V_{\\text{th}}}{IR} = \\frac{15 \\times 10^{-3}\\,\\text{V}}{50 \\times 10^{-3}\\,\\text{V}} = 0.3\n$$\nNow, we substitute these values into the expression for $t_{\\text{th}}$:\n$$\nt_{\\text{th}} = -(0.025\\,\\text{s}) \\ln(1 - 0.3) = -(0.025\\,\\text{s}) \\ln(0.7)\n$$\nUsing the value $\\ln(0.7) \\approx -0.3566749$:\n$$\nt_{\\text{th}} \\approx -(0.025\\,\\text{s}) \\times (-0.3566749) \\approx 0.00891687\\,\\text{s}\n$$\nThe problem requires the answer in milliseconds, rounded to four significant figures:\n$$\nt_{\\text{th}} \\approx 8.917 \\times 10^{-3}\\,\\text{s} = 8.917\\,\\text{ms}\n$$\n\nFinally, we explain qualitatively how increasing $C$ influences the temporal precision of spike initiation. The temporal precision refers to the reliability of the spike-timing in the face of noise. A lower precision means a larger variance in spike time for a given level of noise.\nOur derived expression $t_{\\text{th}} = -RC \\ln(1 - V_{\\text{th}}/IR)$ shows that $t_{\\text{th}}$ is directly proportional to $C$. Thus, increasing the capacitance increases the time required to reach threshold. The neuron becomes a \"slower\" integrator of its input current.\n\nTo understand temporal precision, we must consider the rate of change of the membrane potential, $\\frac{dV}{dt}$, as it approaches the threshold $V_{\\text{th}}$. A steeper slope ($\\frac{dV}{dt}$ is large) means that small fluctuations in voltage (due to current noise) result in only small variations in the time of threshold crossing. Conversely, a shallower slope ($\\frac{dV}{dt}$ is small) means that the same small voltage fluctuations cause larger variations in spike time, thus reducing temporal precision.\nThe rate of change of voltage is given by rearranging the original ODE:\n$$\n\\frac{dV}{dt} = \\frac{1}{C}\\left(I - \\frac{V}{R}\\right) = \\frac{IR - V}{RC}\n$$\nWhen the potential reaches threshold, $V=V_{\\text{th}}$, the slope of the trajectory at that point is:\n$$\n\\frac{dV}{dt}\\Bigg|_{t=t_{\\text{th}}} = \\frac{IR - V_{\\text{th}}}{RC}\n$$\nFrom this expression, we can see that the slope of the voltage trajectory at threshold is inversely proportional to the capacitance $C$. Therefore, increasing $C$ decreases the slope, making the voltage rise to threshold shallower. This shallower trajectory means that any noise in the membrane potential will cause a larger jitter or uncertainty in the exact time $t_{\\text{th}}$ at which the threshold is crossed.\nIn conclusion, increasing the membrane capacitance $C$ makes the neuron a better integrator by smoothing out high-frequency input fluctuations over a longer time window, but this comes at the cost of reduced temporal precision in spike initiation. The neuron's function shifts from being a \"coincidence detector\" (low $C$, high precision) to being an \"integrator\" (high $C$, low precision).", "answer": "$$\n\\boxed{\\begin{pmatrix} -RC \\ln\\left(1 - \\frac{V_{\\text{th}}}{IR}\\right) & 8.917 \\end{pmatrix}}\n$$", "id": "5049102"}, {"introduction": "Moving from single cells to populations, a central question is what information M1 neurons collectively represent. This computational practice challenges you to adjudicate the classic debate between kinematic (e.g., velocity) and kinetic (e.g., force) encoding of motor commands ([@problem_id:5049049]). You will implement a powerful model comparison technique based on cross-condition generalization, learning how to test which variable provides a more invariant and robust description of neural activity across different physical loads.", "problem": "You are given a computational task grounded in neurobiology to adjudicate whether observed tuning shifts in neurons of the Primary Motor Cortex (M1) are better explained by kinematic or kinetic encoding under two external load conditions. The Primary Motor Cortex (M1) is widely observed to exhibit directionally tuned firing that can be approximated locally by linear relations to either hand kinematics (for example, velocity) or kinetics (for example, force or torque). For linear encoding models over small ranges, it is reasonable to approximate firing rate as a linear function of movement-related variables. Your program must implement model comparison via cross-condition regression generalization.\n\nMathematical setup:\n- Let $r^{(c)}(t)$ denote the firing rate (in spikes/s) of a single neuron at time index $t$ during condition $c \\in \\{1,2\\}$.\n- Let $\\mathbf{v}^{(c)}(t) \\in \\mathbb{R}^2$ denote the planar hand velocity vector (in cm/s).\n- Let $\\mathbf{F}^{(c)}(t) \\in \\mathbb{R}^2$ denote the planar hand force vector (in Newtons), modeled as $\\mathbf{F}^{(c)}(t) = \\alpha \\mathbf{v}^{(c)}(t) + \\mathbf{L}^{(c)}$, where $\\alpha$ is a constant with units N/(cm/s), and $\\mathbf{L}^{(c)}$ is a constant external load vector (in Newtons) for condition $c$.\n- Kinematic model (velocity encoding): \n$$\nr^{(c)}(t) = \\beta_0^{(c)} + \\boldsymbol{\\beta}^{(c)} \\cdot \\mathbf{v}^{(c)}(t) + \\epsilon^{(c)}(t),\n$$\nwhere $\\beta_0^{(c)} \\in \\mathbb{R}$, $\\boldsymbol{\\beta}^{(c)} \\in \\mathbb{R}^2$, and $\\epsilon^{(c)}(t)$ is zero-mean noise.\n- Kinetic model (force encoding): \n$$\nr^{(c)}(t) = \\gamma_0^{(c)} + \\boldsymbol{\\gamma}^{(c)} \\cdot \\mathbf{F}^{(c)}(t) + \\epsilon^{(c)}(t),\n$$\nwhere $\\gamma_0^{(c)} \\in \\mathbb{R}$, $\\boldsymbol{\\gamma}^{(c)} \\in \\mathbb{R}^2$, and $\\epsilon^{(c)}(t)$ is zero-mean noise.\n\nRegression-based comparison:\n- For a given model $m \\in \\{\\text{kinematic}, \\text{kinetic}\\}$, construct the design matrices $\\mathbf{X}_1^{(m)}$ and $\\mathbf{X}_2^{(m)}$ for conditions $1$ and $2$ respectively, each with three columns: an intercept (all ones), and the two components of the relevant regressors ($\\mathbf{v}$ for kinematic and $\\mathbf{F}$ for kinetic).\n- For each model $m$, compute cross-condition generalization error by fitting ordinary least squares on one condition and evaluating mean squared error on the other, averaged symmetrically:\n$$\n\\hat{\\mathbf{w}}_1^{(m)} = \\arg\\min_{\\mathbf{w}} \\lVert \\mathbf{X}_1^{(m)} \\mathbf{w} - \\mathbf{y}_1 \\rVert_2^2, \\quad \n\\hat{\\mathbf{w}}_2^{(m)} = \\arg\\min_{\\mathbf{w}} \\lVert \\mathbf{X}_2^{(m)} \\mathbf{w} - \\mathbf{y}_2 \\rVert_2^2,\n$$\n$$\n\\mathrm{MSE}_{1\\rightarrow 2}^{(m)} = \\frac{1}{n} \\lVert \\mathbf{y}_2 - \\mathbf{X}_2^{(m)} \\hat{\\mathbf{w}}_1^{(m)} \\rVert_2^2, \\quad\n\\mathrm{MSE}_{2\\rightarrow 1}^{(m)} = \\frac{1}{n} \\lVert \\mathbf{y}_1 - \\mathbf{X}_1^{(m)} \\hat{\\mathbf{w}}_2^{(m)} \\rVert_2^2,\n$$\n$$\nE_m = \\frac{1}{2} \\left( \\mathrm{MSE}_{1\\rightarrow 2}^{(m)} + \\mathrm{MSE}_{2\\rightarrow 1}^{(m)} \\right),\n$$\nwhere $\\mathbf{y}_c$ stacks $r^{(c)}(t)$ over $t = 1,\\dots,n$ and $n$ is the number of samples per condition.\n\nDecision rule:\n- Declare the kinematic model the better explanation if $E_{\\text{kinematic}} \\leq E_{\\text{kinetic}}$; otherwise declare the kinetic model the better explanation. Output a boolean where $\\text{True}$ means kinematic is better and $\\text{False}$ means kinetic is better. In case of exact equality, choose kinematic (that is, $\\text{True}$).\n\nUnits and angle conventions:\n- Velocity must be expressed in cm/s.\n- Force must be expressed in Newtons.\n- Firing rate must be expressed in spikes/s.\n- Any internal angles used for tuning computations should be in radians. No angles are part of the final output.\n\nTest suite specification:\nYou must implement the following three cases. In all cases, there are $n=8$ movement directions with direction angles \n$$\n\\theta_i \\in \\{0, \\frac{\\pi}{4}, \\frac{\\pi}{2}, \\frac{3\\pi}{4}, \\pi, \\frac{5\\pi}{4}, \\frac{3\\pi}{2}, \\frac{7\\pi}{4}\\},\n$$\nconstant speed $s = 10$ cm/s, and $\\mathbf{v}(t) = s[\\cos\\theta_t, \\sin\\theta_t]$. Use the deterministic noise vectors \n$$\n\\boldsymbol{\\epsilon}^{(1)} = [0.05, -0.03, 0.02, -0.01, 0.04, -0.02, 0.03, -0.04],\n$$\n$$\n\\boldsymbol{\\epsilon}^{(2)} = [0.01, -0.02, 0.03, -0.01, 0.02, -0.03, 0.01, -0.02],\n$$\napplied sample-wise to conditions $1$ and $2$ respectively.\n\n- Case $1$ (happy path: predominantly kinematic encoding):\n    - $\\alpha = 0.5$ N/(cm/s).\n    - $\\mathbf{L}^{(1)} = [0, 0]$ N, $\\mathbf{L}^{(2)} = [5, 0]$ N.\n    - Kinematic coefficients: $\\beta_0 = 5$ spikes/s, $\\boldsymbol{\\beta} = [0.8, 0.2]$ spikes/(cm/s).\n    - Generate firing as $r^{(c)}(t) = \\beta_0 + \\boldsymbol{\\beta} \\cdot \\mathbf{v}^{(c)}(t) + \\epsilon^{(c)}(t)$.\n\n- Case $2$ (opposite path: predominantly kinetic encoding):\n    - $\\alpha = 0.5$ N/(cm/s).\n    - $\\mathbf{L}^{(1)} = [0, 0]$ N, $\\mathbf{L}^{(2)} = [5, 0]$ N.\n    - Kinetic coefficients: $\\gamma_0 = 6$ spikes/s, $\\boldsymbol{\\gamma} = [0.5, -0.3]$ spikes/N.\n    - Generate firing as $r^{(c)}(t) = \\gamma_0 + \\boldsymbol{\\gamma} \\cdot \\mathbf{F}^{(c)}(t) + \\epsilon^{(c)}(t)$ with $\\mathbf{F}^{(c)}(t) = \\alpha \\mathbf{v}^{(c)}(t) + \\mathbf{L}^{(c)}$.\n\n- Case $3$ (edge case: mixed encoding, potential tie):\n    - $\\alpha = 0.5$ N/(cm/s).\n    - $\\mathbf{L}^{(1)} = [0, 0]$ N, $\\mathbf{L}^{(2)} = [2, -2]$ N.\n    - Kinematic coefficients: $\\beta_0 = 4$ spikes/s, $\\boldsymbol{\\beta} = [0.6, 0.6]$ spikes/(cm/s).\n    - Kinetic coefficients: $\\gamma_0 = 4$ spikes/s, $\\boldsymbol{\\gamma} = [0.4, 0.1]$ spikes/N.\n    - Mixing weights: $w_k = 0.5$, $w_f = 0.5$.\n    - Generate firing as $r^{(c)}(t) = w_k(\\beta_0 + \\boldsymbol{\\beta} \\cdot \\mathbf{v}^{(c)}(t)) + w_f(\\gamma_0 + \\boldsymbol{\\gamma} \\cdot \\mathbf{F}^{(c)}(t)) + \\epsilon^{(c)}(t)$ with $\\mathbf{F}^{(c)}(t) = \\alpha \\mathbf{v}^{(c)}(t) + \\mathbf{L}^{(c)}$.\n\nYour program must:\n- Construct the datasets as specified above for each case.\n- For each case, compute cross-condition generalization errors $E_{\\text{kinematic}}$ and $E_{\\text{kinetic}}$ using the regression procedure described.\n- Produce a boolean result per case indicating whether kinematic encoding better explains the observed tuning shift via regression comparison according to the decision rule above.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\text{result1},\\text{result2},\\text{result3}]$), where each result is a boolean ($\\text{True}$ for kinematic better; $\\text{False}$ for kinetic better). No other text should be printed.", "solution": "The problem requires us to implement a model comparison procedure to determine whether a kinematic or kinetic encoding model better explains simulated neural firing data from the primary motor cortex (M1) under two different external load conditions. The method of comparison is cross-condition regression generalization, a standard technique in computational neuroscience for arbitrating between competing models of neural function.\n\nThe core principle is that a good model of neural encoding should have parameters that are invariant to changes in external conditions that are explicitly accounted for by the model. For instance, if a neuron truly encodes force, its force-tuning parameters ($\\gamma_0$, $\\boldsymbol{\\gamma}$) should remain constant even when the external load changes. A model with such invariant parameters will generalize well when trained on data from one condition and tested on data from another. Conversely, a model that fails to capture the true underlying variable will exhibit changing parameters across conditions and will thus generalize poorly.\n\nWe will analyze three distinct cases, each representing a different ground truth for the neural encoding. For each case, we will:\n1.  Generate the kinematic variables (velocity $\\mathbf{v}^{(c)}(t)$), kinetic variables (force $\\mathbf{F}^{(c)}(t)$), and the neural response (firing rate $r^{(c)}(t)$) for two conditions, $c \\in \\{1, 2\\}$, across $n=8$ time points (corresponding to $8$ movement directions).\n2.  Construct the design matrices for the kinematic and kinetic models.\n3.  Fit each model's parameters using ordinary least squares (OLS) regression on data from each condition separately.\n4.  Evaluate the generalization performance of each model by calculating the Mean Squared Error (MSE) when predicting data of one condition using parameters fit from the other.\n5.  Average the two cross-condition MSEs to get a symmetric error score, $E_m$, for each model $m$.\n6.  Apply the decision rule: the kinematic model is deemed better if $E_{\\text{kinematic}} \\leq E_{\\text{kinetic}}$.\n\nLet us formalize the steps for any given case. First, we generate the kinematic data. The speed is a constant $s=10$ cm/s, and the directions are given by $\\theta_t \\in \\{0, \\frac{\\pi}{4}, \\frac{\\pi}{2}, \\frac{3\\pi}{4}, \\pi, \\frac{5\\pi}{4}, \\frac{3\\pi}{2}, \\frac{7\\pi}{4}\\}$ for $t=1, \\dots, 8$. The velocity vector at time $t$ is $\\mathbf{v}(t) = [v_x(t), v_y(t)]^T = s[\\cos\\theta_t, \\sin\\theta_t]^T$. The velocity is independent of the load condition, so $\\mathbf{v}^{(1)}(t) = \\mathbf{v}^{(2)}(t) = \\mathbf{v}(t)$.\n\nThe force vector is modeled as $\\mathbf{F}^{(c)}(t) = \\alpha \\mathbf{v}(t) + \\mathbf{L}^{(c)}$, where $\\alpha$ is a viscosity-like coefficient and $\\mathbf{L}^{(c)}$ is the external load vector for condition $c$.\n\nThe firing rates $y_c = [r^{(c)}(1), \\dots, r^{(c)}(8)]^T$ are generated according to the specific rules of each case.\n\nFor each model $m \\in \\{\\text{kinematic}, \\text{kinetic}\\}$, we set up a linear regression problem. The general form is $\\mathbf{y}_c = \\mathbf{X}_c^{(m)} \\mathbf{w}_c^{(m)} + \\boldsymbol{\\epsilon}_c$.\n- For the kinematic model, the regressors are velocity components. The design matrix $\\mathbf{X}_{\\text{kin}}$ is the same for both conditions, as velocity is the same. It is an $n \\times 3$ matrix where the $t$-th row is $[1, v_x(t), v_y(t)]$.\n- For the kinetic model, the regressors are force components. The design matrices $\\mathbf{X}_{\\text{kinet}}^{(c)}$ are different for each condition, as forces depend on the load $\\mathbf{L}^{(c)}$. The $t$-th row of $\\mathbf{X}_{\\text{kinet}}^{(c)}$ is $[1, F_x^{(c)}(t), F_y^{(c)}(t)]$.\n\nThe OLS solution for the weight vector $\\mathbf{w}$ is given by $\\hat{\\mathbf{w}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$. We compute four such weight vectors: $\\hat{\\mathbf{w}}_1^{(\\text{kin})}$, $\\hat{\\mathbf{w}}_2^{(\\text{kin})}$, $\\hat{\\mathbf{w}}_1^{(\\text{kinet})}$, and $\\hat{\\mathbf{w}}_2^{(\\text{kinet})}$.\n\nThe cross-condition MSEs are calculated as:\n$\\mathrm{MSE}_{1\\rightarrow 2}^{(m)} = \\frac{1}{n} \\lVert \\mathbf{y}_2 - \\mathbf{X}_2^{(m)} \\hat{\\mathbf{w}}_1^{(m)} \\rVert_2^2$\n$\\mathrm{MSE}_{2\\rightarrow 1}^{(m)} = \\frac{1}{n} \\lVert \\mathbf{y}_1 - \\mathbf{X}_1^{(m)} \\hat{\\mathbf{w}}_2^{(m)} \\rVert_2^2$\nThe total error for model $m$ is $E_m = \\frac{1}{2} (\\mathrm{MSE}_{1\\rightarrow 2}^{(m)} + \\mathrm{MSE}_{2\\rightarrow 1}^{(m)})$.\n\nWe now apply this procedure to each of the three cases.\n\n**Case 1: Predominantly Kinematic Encoding**\n- Parameters: $\\alpha = 0.5$ N/(cm/s), $\\mathbf{L}^{(1)} = [0, 0]$ N, $\\mathbf{L}^{(2)} = [5, 0]$ N.\n- Firing rate generation: $r^{(c)}(t) = \\beta_0 + \\boldsymbol{\\beta} \\cdot \\mathbf{v}(t) + \\epsilon^{(c)}(t)$, with $\\beta_0 = 5$ and $\\boldsymbol{\\beta} = [0.8, 0.2]$.\n- The firing rate is generated by a process whose parameters ($\\beta_0, \\boldsymbol{\\beta}$) are stable across conditions with respect to velocity. The kinematic model should therefore fit the data well and, more importantly, generalize well. The coefficients $\\hat{\\mathbf{w}}_1^{(\\text{kin})}$ and $\\hat{\\mathbf{w}}_2^{(\\text{kin})}$ should be very similar to each other and to the true parameters $[\\beta_0, \\beta_x, \\beta_y]^T = [5, 0.8, 0.2]^T$. Consequently, $E_{\\text{kinematic}}$ is expected to be small, dominated only by the noise terms $\\boldsymbol{\\epsilon}^{(c)}$.\n- In contrast, the relationship between firing rate and force is not stable. $r^{(c)}(t) = \\beta_0 + \\boldsymbol{\\beta} \\cdot \\frac{1}{\\alpha}(\\mathbf{F}^{(c)}(t) - \\mathbf{L}^{(c)}) + \\epsilon^{(c)}(t) = (\\beta_0 - \\frac{1}{\\alpha}\\boldsymbol{\\beta}\\cdot\\mathbf{L}^{(c)}) + (\\frac{1}{\\alpha}\\boldsymbol{\\beta}) \\cdot \\mathbf{F}^{(c)}(t) + \\epsilon^{(c)}(t)$. The intercept term $(\\beta_0 - \\frac{1}{\\alpha}\\boldsymbol{\\beta}\\cdot\\mathbf{L}^{(c)})$ changes with the load $\\mathbf{L}^{(c)}$. A kinetic model fit on condition $1$ will have the wrong intercept for condition $2$, leading to poor generalization and a large $E_{\\text{kinetic}}$.\n- Expectation: $E_{\\text{kinematic}} < E_{\\text{kinetic}}$, resulting in a `True` output.\n\n**Case 2: Predominantly Kinetic Encoding**\n- Parameters: $\\alpha = 0.5$ N/(cm/s), $\\mathbf{L}^{(1)} = [0, 0]$ N, $\\mathbf{L}^{(2)} = [5, 0]$ N.\n- Firing rate generation: $r^{(c)}(t) = \\gamma_0 + \\boldsymbol{\\gamma} \\cdot \\mathbf{F}^{(c)}(t) + \\epsilon^{(c)}(t)$, with $\\gamma_0 = 6$ and $\\boldsymbol{\\gamma} = [0.5, -0.3]$.\n- This is the converse of Case 1. The data is generated from a stable kinetic model. The kinetic model's parameters are invariant across conditions. Therefore, we expect $\\hat{\\mathbf{w}}_1^{(\\text{kinet})}$ and $\\hat{\\mathbf{w}}_2^{(\\text{kinet})}$ to be similar to $[\\gamma_0, \\gamma_x, \\gamma_y]^T = [6, 0.5, -0.3]^T$, and $E_{\\text{kinetic}}$ should be small.\n- The relationship with velocity, $r^{(c)}(t) = \\gamma_0 + \\boldsymbol{\\gamma} \\cdot (\\alpha \\mathbf{v}(t) + \\mathbf{L}^{(c)}) + \\epsilon^{(c)}(t) = (\\gamma_0 + \\boldsymbol{\\gamma}\\cdot\\mathbf{L}^{(c)}) + (\\alpha\\boldsymbol{\\gamma}) \\cdot \\mathbf{v}(t) + \\epsilon^{(c)}(t)$, has a condition-dependent intercept $(\\gamma_0 + \\boldsymbol{\\gamma}\\cdot\\mathbf{L}^{(c)})$. The kinematic model will fail to generalize.\n- Expectation: $E_{\\text{kinetic}} < E_{\\text{kinematic}}$, resulting in a `False` output.\n\n**Case 3: Mixed Encoding**\n- Parameters: $\\alpha = 0.5$ N/(cm/s), $\\mathbf{L}^{(1)} = [0, 0]$ N, $\\mathbf{L}^{(2)} = [2, -2]$ N.\n- Firing rate generation: $r^{(c)}(t) = w_k(\\beta_0 + \\boldsymbol{\\beta} \\cdot \\mathbf{v}(t)) + w_f(\\gamma_0 + \\boldsymbol{\\gamma} \\cdot \\mathbf{F}^{(c)}(t)) + \\epsilon^{(c)}(t)$, with $w_k=w_f=0.5$, $\\beta_0=4, \\boldsymbol{\\beta}=[0.6, 0.6]$, $\\gamma_0=4, \\boldsymbol{\\gamma}=[0.4, 0.1]$.\n- The firing rate is a linear combination of kinematic and kinetic terms. We can analyze its structure by expressing it purely in terms of one variable.\n- Kinematic representation: $r^{(c)}(t) = (\\dots) + (\\dots)\\cdot\\mathbf{v}(t)$. As shown in Case 2, substituting $\\mathbf{F}^{(c)}(t) = \\alpha \\mathbf{v}(t) + \\mathbf{L}^{(c)}$ results in an effective intercept that is dependent on $\\mathbf{L}^{(c)}$.\n- Kinetic representation: We substitute $\\mathbf{v}(t) = \\frac{1}{\\alpha}(\\mathbf{F}^{(c)}(t) - \\mathbf{L}^{(c)})$.\n$r^{(c)} = w_k(\\beta_0 + \\frac{1}{\\alpha}\\boldsymbol{\\beta}\\cdot(\\mathbf{F}^{(c)} - \\mathbf{L}^{(c)})) + w_f(\\gamma_0 + \\boldsymbol{\\gamma}\\cdot\\mathbf{F}^{(c)}) + \\epsilon^{(c)}$\n$r^{(c)} = (w_k\\beta_0 + w_f\\gamma_0 - \\frac{w_k}{\\alpha}\\boldsymbol{\\beta}\\cdot\\mathbf{L}^{(c)}) + (\\frac{w_k}{\\alpha}\\boldsymbol{\\beta} + w_f\\boldsymbol{\\gamma})\\cdot\\mathbf{F}^{(c)} + \\epsilon^{(c)}$\n- Let's examine the effective intercept: $\\gamma_{0, \\text{eff}}^{(c)} = w_k\\beta_0 + w_f\\gamma_0 - \\frac{w_k}{\\alpha}\\boldsymbol{\\beta}\\cdot\\mathbf{L}^{(c)}$.\nPlugging in the values: $\\alpha=0.5, w_k=0.5, \\beta_0=4, \\gamma_0=4, \\boldsymbol{\\beta}=[0.6, 0.6]$.\n$\\gamma_{0, \\text{eff}}^{(c)} = 0.5(4) + 0.5(4) - \\frac{0.5}{0.5}([0.6, 0.6]\\cdot\\mathbf{L}^{(c)}) = 4 - [0.6, 0.6]\\cdot\\mathbf{L}^{(c)}$.\nFor condition 1: $\\mathbf{L}^{(1)} = [0, 0]$, so $\\gamma_{0, \\text{eff}}^{(1)} = 4$.\nFor condition 2: $\\mathbf{L}^{(2)} = [2, -2]$, so $[0.6, 0.6]\\cdot[2, -2] = 0.6(2) + 0.6(-2) = 1.2 - 1.2 = 0$. Thus, $\\gamma_{0, \\text{eff}}^{(2)} = 4$.\n- Remarkably, the effective kinetic intercept is stable across conditions for this specific choice of parameters. The effective kinetic slope, $(\\frac{w_k}{\\alpha}\\boldsymbol{\\beta} + w_f\\boldsymbol{\\gamma})$, is also stable. This means the mixed model can be perfectly represented by a single, invariant kinetic model.\n- Because the kinetic representation is stable and the kinematic one is not, the kinetic model will generalize far better.\n- Expectation: $E_{\\text{kinetic}} < E_{\\text{kinematic}}$, resulting in a `False` output.\n\nThe computational implementation will follow this logic precisely to arrive at the final boolean results for each case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the three test cases for M1 encoding model comparison.\n    \"\"\"\n\n    def run_case(case_params: dict) -> bool:\n        \"\"\"\n        Runs a single case for model comparison.\n\n        Args:\n            case_params: A dictionary containing parameters for the case.\n\n        Returns:\n            A boolean, True if kinematic model is better or equal, False otherwise.\n        \"\"\"\n        # Common parameters\n        n_samples = 8\n        speed = 10.0\n        thetas = np.linspace(0, 2 * np.pi, n_samples, endpoint=False)\n        eps1 = np.array([0.05, -0.03, 0.02, -0.01, 0.04, -0.02, 0.03, -0.04])\n        eps2 = np.array([0.01, -0.02, 0.03, -0.01, 0.02, -0.03, 0.01, -0.02])\n\n        # Generate velocity data (same for both conditions)\n        v_data = speed * np.array([np.cos(thetas), np.sin(thetas)]).T\n\n        # Unpack case-specific parameters\n        alpha = case_params['alpha']\n        L1 = np.array(case_params['L1'])\n        L2 = np.array(case_params['L2'])\n\n        # Generate force data\n        F1_data = alpha * v_data + L1\n        F2_data = alpha * v_data + L2\n\n        # Generate firing rate data (response vectors y1, y2)\n        if case_params['type'] == 'kinematic':\n            beta0 = case_params['beta0']\n            beta_vec = np.array(case_params['beta'])\n            y1 = beta0 + v_data @ beta_vec + eps1\n            y2 = beta0 + v_data @ beta_vec + eps2\n        elif case_params['type'] == 'kinetic':\n            gamma0 = case_params['gamma0']\n            gamma_vec = np.array(case_params['gamma'])\n            y1 = gamma0 + F1_data @ gamma_vec + eps1\n            y2 = gamma0 + F2_data @ gamma_vec + eps2\n        elif case_params['type'] == 'mixed':\n            wk, wf = case_params['weights']\n            beta0, beta_vec = case_params['beta0'], np.array(case_params['beta'])\n            gamma0, gamma_vec = case_params['gamma0'], np.array(case_params['gamma'])\n            kin_term1 = wk * (beta0 + v_data @ beta_vec)\n            kin_term2 = wk * (beta0 + v_data @ beta_vec) \n            kinet_term1 = wf * (gamma0 + F1_data @ gamma_vec)\n            kinet_term2 = wf * (gamma0 + F2_data @ gamma_vec)\n            y1 = kin_term1 + kinet_term1 + eps1\n            y2 = kin_term2 + kinet_term2 + eps2\n        else:\n            raise ValueError(\"Unknown case type\")\n\n        # --- Model Comparison ---\n        \n        # 1. Kinematic Model\n        X_kin = np.c_[np.ones(n_samples), v_data]\n        \n        # Fit on condition 1, test on 2\n        w1_kin = np.linalg.lstsq(X_kin, y1, rcond=None)[0]\n        y2_pred_kin_from_1 = X_kin @ w1_kin\n        mse_1_2_kin = np.mean((y2 - y2_pred_kin_from_1)**2)\n        \n        # Fit on condition 2, test on 1\n        w2_kin = np.linalg.lstsq(X_kin, y2, rcond=None)[0]\n        y1_pred_kin_from_2 = X_kin @ w2_kin\n        mse_2_1_kin = np.mean((y1 - y1_pred_kin_from_2)**2)\n        \n        E_kinematic = 0.5 * (mse_1_2_kin + mse_2_1_kin)\n\n        # 2. Kinetic Model\n        X_kinet_1 = np.c_[np.ones(n_samples), F1_data]\n        X_kinet_2 = np.c_[np.ones(n_samples), F2_data]\n\n        # Fit on condition 1, test on 2\n        w1_kinet = np.linalg.lstsq(X_kinet_1, y1, rcond=None)[0]\n        y2_pred_kinet_from_1 = X_kinet_2 @ w1_kinet\n        mse_1_2_kinet = np.mean((y2 - y2_pred_kinet_from_1)**2)\n\n        # Fit on condition 2, test on 1\n        w2_kinet = np.linalg.lstsq(X_kinet_2, y2, rcond=None)[0]\n        y1_pred_kinet_from_2 = X_kinet_1 @ w2_kinet\n        mse_2_1_kinet = np.mean((y1 - y1_pred_kinet_from_2)**2)\n\n        E_kinetic = 0.5 * (mse_1_2_kinet + mse_2_1_kinet)\n\n        # Decision rule\n        return E_kinematic <= E_kinetic\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        {\n            'type': 'kinematic',\n            'alpha': 0.5,\n            'L1': [0.0, 0.0], 'L2': [5.0, 0.0],\n            'beta0': 5.0, 'beta': [0.8, 0.2]\n        },\n        {\n            'type': 'kinetic',\n            'alpha': 0.5,\n            'L1': [0.0, 0.0], 'L2': [5.0, 0.0],\n            'gamma0': 6.0, 'gamma': [0.5, -0.3]\n        },\n        {\n            'type': 'mixed',\n            'alpha': 0.5,\n            'L1': [0.0, 0.0], 'L2': [2.0, -2.0],\n            'beta0': 4.0, 'beta': [0.6, 0.6],\n            'gamma0': 4.0, 'gamma': [0.4, 0.1],\n            'weights': (0.5, 0.5)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_case(case)\n        results.append(str(result))\n\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "5049049"}, {"introduction": "The final step in generating movement is transforming M1's neural commands into coordinated muscle contractions. This problem explores the concept of muscle synergies, the hypothesis that the brain simplifies control by activating muscles in functional groups rather than individually ([@problem_id:5049067]). By analyzing simulated neural and electromyography (EMG) data, you will use linear algebra to estimate the mapping from cortex to muscles and determine its 'effective dimensionality,' providing insight into the structure of the brain's motor output.", "problem": "You are provided with ensemble neural firing rates and Electromyography (EMG) data recorded during a planar reaching task. Assume that the Primary Motor Cortex (M1) contributes to muscle activation through a linear population-code model in which EMG activity is approximated by a linear transformation of ensemble firing rates. Formally, for each trial, let the neural firing matrix be $F \\in \\mathbb{R}^{n \\times T}$, where $n$ is the number of neurons and $T$ is the number of time samples, and let the EMG matrix be $E \\in \\mathbb{R}^{m \\times T}$, where $m$ is the number of muscles. The linear model posits\n$$\nE \\approx W F,\n$$\nwhere $W \\in \\mathbb{R}^{m \\times n}$ is a time-invariant mapping that captures how ensemble firing controls muscle activations. Your task is to estimate $W$ via least squares using the Moore–Penrose pseudoinverse, compute the numerical rank of $W$, and infer the effective dimensionality of muscle control from the singular value spectrum of $W$.\n\nUse the following fundamental base:\n- The population code assumption that averaged EMG activity can be locally approximated as a linear combination of ensemble firing rates in M1.\n- The definition of the Moore–Penrose pseudoinverse $(\\cdot)^{+}$ and the least squares solution $W = E F^{+}$.\n- The Singular Value Decomposition (SVD) of $W$, $W = U \\Sigma V^{\\top}$, where $\\Sigma$ contains singular values $\\{\\sigma_i\\}$.\n- The numerical rank defined by the count of singular values $\\sigma_i$ strictly greater than a tolerance $t = \\max(m,n) \\cdot \\sigma_{\\max} \\cdot \\epsilon$, where $\\sigma_{\\max} = \\max_i \\sigma_i$ and $\\epsilon$ is machine epsilon for double-precision arithmetic.\n- The effective dimensionality defined as the count of singular values whose normalized magnitude is at least a task-specific threshold $\\tau$, i.e., the number of $i$ such that $\\sigma_i / \\sigma_{\\max} \\ge \\tau$.\n\nGiven the matrices and thresholds below, compute for each test case both the numerical rank of $W$ and the effective dimensionality using the provided $\\tau$. All computations should be carried out in pure mathematical terms; no physical units are required for the output. Angles are not used, and percentages should not be used.\n\nTest suite:\n- Case $1$ (happy path, full row rank for $F$):\n  - $F^{(1)} = \\begin{bmatrix}\n  1 & 2 & 3 & 4 & 5 & 6 \\\\\n  0 & 1 & 0 & 1 & 0 & 1 \\\\\n  2 & 0 & 1 & 2 & 3 & 4 \\\\\n  1 & 1 & 1 & 1 & 1 & 1\n  \\end{bmatrix}$,\n  - $E^{(1)} = \\begin{bmatrix}\n  1 & 2 & 3 & 4 & 5 & 6 \\\\\n  2 & 1 & 1 & 3 & 3 & 5 \\\\\n  3 & 1 & 2 & 3 & 4 & 5\n  \\end{bmatrix}$,\n  - $\\tau^{(1)} = 10^{-10}$.\n- Case $2$ (low-dimensional synergy in $W$ with rank $\\leq 2$):\n  - $F^{(2)} = \\begin{bmatrix}\n  1 & 2 & 3 & 4 & 5 & 6 \\\\\n  0 & 1 & 0 & 1 & 0 & 1 \\\\\n  2 & 0 & 1 & 2 & 3 & 4 \\\\\n  1 & 1 & 1 & 1 & 1 & 1\n  \\end{bmatrix}$,\n  - $E^{(2)} = \\begin{bmatrix}\n  5 & 4 & 6 & 8 & 10 & 12 \\\\\n  3 & 2 & 2 & 4 & 4 & 6 \\\\\n  8 & 6 & 8 & 12 & 14 & 18\n  \\end{bmatrix}$,\n  - $\\tau^{(2)} = 0.3$.\n- Case $3$ (boundary case, rank $1$ mapping):\n  - $F^{(3)} = \\begin{bmatrix}\n  1 & 2 & 3 & 4 & 5 & 6 \\\\\n  0 & 1 & 0 & 1 & 0 & 1 \\\\\n  2 & 0 & 1 & 2 & 3 & 4 \\\\\n  1 & 1 & 1 & 1 & 1 & 1\n  \\end{bmatrix}$,\n  - $E^{(3)} = \\begin{bmatrix}\n  11 & 8 & 10 & 16 & 18 & 24 \\\\\n  5.5 & 4 & 5 & 8 & 9 & 12 \\\\\n  22 & 16 & 20 & 32 & 36 & 48\n  \\end{bmatrix}$,\n  - $\\tau^{(3)} = 0.5$.\n- Case $4$ (edge case, ill-conditioned $F$ with dependent rows):\n  - $F^{(4)} = \\begin{bmatrix}\n  1 & 0 & 1 & 0 & 1 & 0 \\\\\n  2 & 0 & 2 & 0 & 2 & 0 \\\\\n  0 & 1 & 0 & 1 & 0 & 1 \\\\\n  1 & 1 & 1 & 1 & 1 & 1\n  \\end{bmatrix}$,\n  - $E^{(4)} = \\begin{bmatrix}\n  1 & 0 & 1 & 0 & 1 & 0 \\\\\n  2 & 1 & 2 & 1 & 2 & 1 \\\\\n  1 & 2 & 1 & 2 & 1 & 2\n  \\end{bmatrix}$,\n  - $\\tau^{(4)} = 10^{-10}$.\n\nYour program should, for each case $k \\in \\{1,2,3,4\\}$:\n1. Compute $W^{(k)} = E^{(k)} \\left(F^{(k)}\\right)^{+}$ using the Moore–Penrose pseudoinverse.\n2. Compute the numerical rank of $W^{(k)}$ using SVD and tolerance $t^{(k)} = \\max(m,n) \\cdot \\sigma_{\\max}^{(k)} \\cdot \\epsilon$, with $\\epsilon$ equal to double-precision machine epsilon.\n3. Compute the effective dimensionality $D_{\\mathrm{eff}}^{(k)}$ as the count of singular values with $\\sigma_i^{(k)} / \\sigma_{\\max}^{(k)} \\ge \\tau^{(k)}$.\n\nFinal output format:\n- Produce a single line containing a comma-separated list of pairs, each pair formatted as $[r_k,d_k]$ where $r_k$ is the numerical rank and $d_k$ is the effective dimensionality for case $k$. The entire list must be enclosed in square brackets and contain no spaces. For example, $\\left[[r_1,d_1],[r_2,d_2],[r_3,d_3],[r_4,d_4]\\right]$.", "solution": "We begin from a population-code model of the Primary Motor Cortex (M1), in which ensemble neural firing rates contribute to muscle activation. In a first-order approximation over short time scales and fixed posture, EMG can be modeled as a linear transformation of ensemble firing, yielding the relationship $E \\approx W F$, where $E \\in \\mathbb{R}^{m \\times T}$, $F \\in \\mathbb{R}^{n \\times T}$, and $W \\in \\mathbb{R}^{m \\times n}$ is a time-invariant mapping capturing the organization of outputs from M1 to muscles.\n\nTo estimate $W$ from observations $(E,F)$, we use the least squares solution derived from the Moore–Penrose pseudoinverse. Given the residual minimization problem\n$$\n\\min_{W} \\|E - W F\\|_F^2,\n$$\nthe solution is\n$$\nW^{\\star} = E F^{+},\n$$\nwhere $F^{+}$ is the Moore–Penrose pseudoinverse of $F$. This follows from the normal equations in matrix least squares and the definition of the pseudoinverse, which yields the minimum-norm solution even when $F$ is rank-deficient.\n\nTo quantify the organization and function of the mapping in terms of control dimensionality, we compute the Singular Value Decomposition (SVD) of $W$:\n$$\nW = U \\Sigma V^{\\top},\n$$\nwhere $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix containing singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$.\n\nThe numerical rank $r$ is defined as the number of singular values strictly greater than a tolerance $t$, where\n$$\nt = \\max(m,n) \\cdot \\sigma_{\\max} \\cdot \\epsilon,\n$$\nwith $\\sigma_{\\max} = \\max_i \\sigma_i$ and $\\epsilon$ the machine epsilon for double-precision floating point arithmetic. This tolerance is motivated by backward stability considerations of SVD and relative scaling of singular values.\n\nThe effective dimensionality $D_{\\mathrm{eff}}$ is defined as the count of singular values whose normalized magnitude exceeds a task-specific threshold $\\tau$, namely\n$$\nD_{\\mathrm{eff}} = \\left|\\left\\{ i \\,:\\, \\frac{\\sigma_i}{\\sigma_{\\max}} \\ge \\tau \\right\\}\\right|.\n$$\nThis thresholding captures the number of strong modes through which the neural ensemble effectively controls muscle activations, consistent with the concept of muscle synergies and dimensionality reduction in motor control.\n\nAlgorithmic steps for each test case $k$:\n1. Take $F^{(k)} \\in \\mathbb{R}^{n \\times T}$ and $E^{(k)} \\in \\mathbb{R}^{m \\times T}$.\n2. Compute $F^{(k)+}$ via a numerically stable pseudoinverse (e.g., SVD-based).\n3. Form $W^{(k)} = E^{(k)} F^{(k)+}$.\n4. Compute singular values $\\{\\sigma_i^{(k)}\\}$ of $W^{(k)}$ via SVD.\n5. Compute $t^{(k)} = \\max(m,n) \\cdot \\sigma_{\\max}^{(k)} \\cdot \\epsilon$, and set\n$$\nr_k = \\sum_{i} \\mathbf{1}\\left[\\sigma_i^{(k)} > t^{(k)}\\right].\n$$\n6. Compute\n$$\nd_k = \\sum_{i} \\mathbf{1}\\left[\\frac{\\sigma_i^{(k)}}{\\sigma_{\\max}^{(k)}} \\ge \\tau^{(k)}\\right].\n$$\n\nInterpretation:\n- A higher $r_k$ indicates more independent control channels encoded by $W^{(k)}$, subject to numerical precision.\n- A higher $d_k$ indicates more effective control modes above the task-dependent resolution threshold, relating to strong synergies.\n\nApplying this to the provided cases:\n- Case $1$: $F^{(1)}$ has full row rank and $E^{(1)}$ is generated by a mapping with three independent rows, yielding $r_1 = 3$. With $\\tau^{(1)} = 10^{-10}$, all nonzero singular values count, so $d_1 = 3$.\n- Case $2$: $E^{(2)}$ embodies a mapping of rank at most $2$; the SVD gives two dominant singular values. With $\\tau^{(2)} = 0.3$, the two strong singular values exceed the threshold, so $r_2 = 2$ and $d_2 = 2$.\n- Case $3$: The mapping is rank $1$. With $\\tau^{(3)} = 0.5$, the single nonzero singular value exceeds the threshold, yielding $r_3 = 1$ and $d_3 = 1$.\n- Case $4$: $F^{(4)}$ is rank-deficient with dependent rows, so the least-squares estimate $W^{(4)}$ collapses to the identifiable subspace, whose rank is $2$. With $\\tau^{(4)} = 10^{-10}$, the effective dimensionality equals the numerical rank, $d_4 = 2$.\n\nFinal output must be a single line containing the list of pairs $\\left[[r_1,d_1],[r_2,d_2],[r_3,d_3],[r_4,d_4]\\right]$ with no spaces.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_w(E, F):\n    # Moore-Penrose pseudoinverse-based least squares estimate of W\n    F_pinv = np.linalg.pinv(F)\n    W = E @ F_pinv\n    return W\n\ndef numerical_rank_and_effective_dim(W, tau):\n    # Singular values\n    s = np.linalg.svd(W, compute_uv=False)\n    smax = np.max(s) if s.size > 0 else 0.0\n    # Machine epsilon for double precision\n    eps = np.finfo(float).eps\n    # Numerical rank tolerance\n    tol = max(W.shape) * smax * eps\n    rank = int(np.sum(s > tol))\n    # Effective dimensionality: count singular values with normalized magnitude >= tau\n    if smax == 0.0:\n        eff_dim = 0\n    else:\n        eff_dim = int(np.sum((s / smax) >= tau))\n    return rank, eff_dim\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (E, F, tau)\n    F1 = np.array([\n        [1, 2, 3, 4, 5, 6],\n        [0, 1, 0, 1, 0, 1],\n        [2, 0, 1, 2, 3, 4],\n        [1, 1, 1, 1, 1, 1]\n    ], dtype=float)\n    E1 = np.array([\n        [1, 2, 3, 4, 5, 6],\n        [2, 1, 1, 3, 3, 5],\n        [3, 1, 2, 3, 4, 5]\n    ], dtype=float)\n    tau1 = 1e-10\n\n    F2 = F1.copy()\n    E2 = np.array([\n        [5, 4, 6, 8, 10, 12],\n        [3, 2, 2, 4, 4, 6],\n        [8, 6, 8, 12, 14, 18]\n    ], dtype=float)\n    tau2 = 0.3\n\n    F3 = F1.copy()\n    E3 = np.array([\n        [11, 8, 10, 16, 18, 24],\n        [5.5, 4, 5, 8, 9, 12],\n        [22, 16, 20, 32, 36, 48]\n    ], dtype=float)\n    tau3 = 0.5\n\n    F4 = np.array([\n        [1, 0, 1, 0, 1, 0],\n        [2, 0, 2, 0, 2, 0],\n        [0, 1, 0, 1, 0, 1],\n        [1, 1, 1, 1, 1, 1]\n    ], dtype=float)\n    E4 = np.array([\n        [1, 0, 1, 0, 1, 0],\n        [2, 1, 2, 1, 2, 1],\n        [1, 2, 1, 2, 1, 2]\n    ], dtype=float)\n    tau4 = 1e-10\n\n    test_cases = [\n        (E1, F1, tau1),\n        (E2, F2, tau2),\n        (E3, F3, tau3),\n        (E4, F4, tau4),\n    ]\n\n    results = []\n    for E, F, tau in test_cases:\n        W = compute_w(E, F)\n        r, d = numerical_rank_and_effective_dim(W, tau)\n        results.append((r, d))\n\n    # Format output exactly as required: [[r1,d1],[r2,d2],[r3,d3],[r4,d4]]\n    output = \"[\" + \",\".join(f\"[{r},{d}]\" for (r, d) in results) + \"]\"\n    print(output)\n\nsolve()\n```", "id": "5049067"}]}