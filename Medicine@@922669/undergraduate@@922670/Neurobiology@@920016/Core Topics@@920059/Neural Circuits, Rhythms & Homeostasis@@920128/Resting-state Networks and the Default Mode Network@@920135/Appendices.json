{"hands_on_practices": [{"introduction": "The raw signal we measure with fMRI is a mixture of true neural activity and various sources of noise. Before we can study resting-state networks, we must first \"clean\" the data by identifying and removing these non-neural fluctuations. This foundational practice will guide you through building a linear model to regress out common confounds like head motion, allowing you to isolate the underlying signal of interest [@problem_id:5056395].", "problem": "You are given a time series from a region of interest (ROI) that is part of the Default Mode Network (DMN) in functional Magnetic Resonance Imaging (fMRI) data and a set of rigid-body motion parameters with their temporal derivatives. In resting-state analysis, motion can confound the estimation of intrinsic neural fluctuations relevant to resting-state networks. Your task is to construct a regression design matrix that captures motion-related confounds and then compute the residual time series that approximates the intrinsic neural signal for the ROI. Use a principled linear modeling approach from first principles.\n\nBase the derivation and implementation on the following fundamental facts:\n- The general linear model for observations is $y = X\\beta + \\varepsilon$, where $y \\in \\mathbb{R}^T$ is the observed time series, $X \\in \\mathbb{R}^{T \\times p}$ is the design matrix of regressors, $\\beta \\in \\mathbb{R}^p$ are coefficients, and $\\varepsilon$ is the error term.\n- Ordinary Least Squares (OLS) estimates coefficients by minimizing the sum of squared errors and yields $\\hat{\\beta} = X^{+} y$, where $X^{+}$ denotes the Moore–Penrose pseudo-inverse, which is consistent in the presence of rank deficiency.\n- The residuals are given by $r = y - X\\hat{\\beta}$, which are orthogonal to the column space of $X$ under OLS.\n\nConstruct the design matrix $X$ by concatenating the following columns:\n1. The rigid-body motion parameters $M \\in \\mathbb{R}^{T \\times 6}$ (three translations and three rotations), column-wise.\n2. The first temporal derivatives $D \\in \\mathbb{R}^{T \\times 6}$, where for $t \\geq 1$, $D_t = M_t - M_{t-1}$ and $D_0$ is the zero vector.\n3. An intercept column of ones.\n4. A linear trend column equal to $t - \\bar{t}$ for $t = 0, 1, \\dots, T-1$, where $\\bar{t}$ is the mean of the index vector.\n\nYou will compute the residual $r$ from the ROI time series $y$ given the above $X$, and then evaluate how well $r$ approximates the intrinsic neural component $n$ (a provided synthetic ground truth) using the Pearson correlation. Use the definition of Pearson correlation:\n$$\n\\rho(r,n) = \\frac{\\sum_{t=0}^{T-1} (r_t - \\bar{r})(n_t - \\bar{n})}{\\sqrt{\\sum_{t=0}^{T-1} (r_t - \\bar{r})^2} \\sqrt{\\sum_{t=0}^{T-1} (n_t - \\bar{n})^2}},\n$$\nwhere $\\bar{r}$ and $\\bar{n}$ are the sample means of $r$ and $n$, respectively.\n\nUnits: use time in seconds for constructing sinusoids and specify frequency in Hertz. The repetition time (TR) is given in seconds. Angle unit for sinusoidal phases is radians.\n\nImplement a program that carries out the above steps for the following test suite, which includes a general case, a boundary condition, and a rank-deficient edge case. All signals are deterministic and defined point-wise to ensure reproducibility.\n\nLet $t = 0, 1, \\dots, T-1$ and define $u_t = t/T$ for convenience. For all cases, set the derivative $D_0$ to the zero vector.\n\nTest Case 1 (general case, TR $= 2\\,\\mathrm{s}$, $T = 200$):\n- Neural ground truth:\n$$\nn_t = \\sin(2\\pi \\cdot 0.03 \\cdot \\mathrm{TR} \\cdot t) + 0.5 \\sin(2\\pi \\cdot 0.05 \\cdot \\mathrm{TR} \\cdot t + \\pi/4).\n$$\n- Motion parameters $M$ columns $(m_1,\\dots,m_6)$:\n$$\n\\begin{aligned}\nm_{1,t} &= 0.5 \\sin(2\\pi \\cdot 0.2 \\cdot \\mathrm{TR} \\cdot t) + 0.1 u_t, \\\\\nm_{2,t} &= 0.4 \\cos(2\\pi \\cdot 0.25 \\cdot \\mathrm{TR} \\cdot t), \\\\\nm_{3,t} &= 0.3 \\sin(2\\pi \\cdot 0.15 \\cdot \\mathrm{TR} \\cdot t), \\\\\nm_{4,t} &= 0.5 u_t, \\\\\nm_{5,t} &= 0.2 \\sin(2\\pi \\cdot 0.05 \\cdot \\mathrm{TR} \\cdot t), \\\\\nm_{6,t} &= 0.25 \\cos(2\\pi \\cdot 0.07 \\cdot \\mathrm{TR} \\cdot t).\n\\end{aligned}\n$$\n- Coefficients for confounds: $b = [0.9, -0.5, 0.3, 0.6, -0.4, 0.2]$, $c = [0.5, 0.1, -0.2, 0.3, 0.0, -0.1]$.\n- ROI time series:\n$$\ny_t = n_t + \\sum_{j=1}^{6} b_j m_{j,t} + \\sum_{j=1}^{6} c_j d_{j,t} + 0.1 \\sin(2\\pi \\cdot 0.4 \\cdot \\mathrm{TR} \\cdot t),\n$$\nwhere $d_{j,t}$ is the $j$-th derivative column at time $t$.\n\nTest Case 2 (boundary condition with zero motion, TR $= 0.8\\,\\mathrm{s}$, $T = 60$):\n- Neural ground truth:\n$$\nn_t = \\sin(2\\pi \\cdot 0.04 \\cdot \\mathrm{TR} \\cdot t) + 0.6 \\cos(2\\pi \\cdot 0.02 \\cdot \\mathrm{TR} \\cdot t).\n$$\n- Motion parameters: $m_{j,t} = 0$ for all $j$ and $t$.\n- ROI time series (includes a small high-frequency component and a linear drift):\n$$\ny_t = n_t + 0.05 \\sin(2\\pi \\cdot 0.3 \\cdot \\mathrm{TR} \\cdot t) + 0.2 u_t.\n$$\n\nTest Case 3 (edge case with rank deficiency, TR $= 1.5\\,\\mathrm{s}$, $T = 120$):\n- Neural ground truth:\n$$\nn_t = \\sin(2\\pi \\cdot 0.025 \\cdot \\mathrm{TR} \\cdot t).\n$$\n- Motion parameters $M$ columns $(m_1,\\dots,m_6)$ introducing collinearity:\n$$\n\\begin{aligned}\nm_{1,t} &= \\sin(2\\pi \\cdot 0.1 \\cdot \\mathrm{TR} \\cdot t), \\\\\nm_{2,t} &= m_{1,t}, \\\\\nm_{3,t} &= 0.3 \\sin(2\\pi \\cdot 0.1 \\cdot \\mathrm{TR} \\cdot t + \\pi/3), \\\\\nm_{4,t} &= 0.5 u_t, \\\\\nm_{5,t} &= 0.5 u_t, \\\\\nm_{6,t} &= \\cos(2\\pi \\cdot 0.2 \\cdot \\mathrm{TR} \\cdot t).\n\\end{aligned}\n$$\n- Coefficients for confounds: $b = [0.5, -0.5, 0.4, 0.3, -0.3, 0.2]$, $c = [0.2, -0.2, 0.1, 0.1, -0.1, 0.0]$.\n- ROI time series:\n$$\ny_t = n_t + \\sum_{j=1}^{6} b_j m_{j,t} + \\sum_{j=1}^{6} c_j d_{j,t}.\n$$\n\nImplementation requirements:\n- Compute $D$ by backward differences with $D_0$ set to zero.\n- Build $X$ by concatenating $M$, $D$, an intercept of ones, and the centered linear trend $(t - \\bar{t})$.\n- Estimate $\\hat{\\beta}$ using the Moore–Penrose pseudo-inverse of $X$ and compute residuals $r = y - X\\hat{\\beta}$.\n- Compute the Pearson correlation $\\rho(r,n)$ for each test case.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each correlation rounded to six decimal places (e.g., \"[0.987654,0.912345,0.998765]\"). No additional text should be printed.", "solution": "The problem requires the implementation of a linear regression-based signal processing technique to remove motion-related and other nuisance artifacts from a synthetic functional Magnetic Resonance Imaging (fMRI) time series. The validity of this approach is first established, followed by a detailed, step-by-step solution based on fundamental principles of linear algebra and statistics.\n\n### Problem Validation\n\n**Step 1: Givens Extraction**\nThe problem provides the following definitions and data:\n- **General Linear Model (GLM):** $y = X\\beta + \\varepsilon$\n- **Ordinary Least Squares (OLS) Solution:** $\\hat{\\beta} = X^{+} y$, where $X^{+}$ is the Moore–Penrose pseudo-inverse.\n- **Residuals:** $r = y - X\\hat{\\beta}$\n- **Design Matrix $X$:** A concatenation of columns representing:\n    1. Rigid-body motion parameters $M \\in \\mathbb{R}^{T \\times 6}$.\n    2. Temporal derivatives $D \\in \\mathbb{R}^{T \\times 6}$, where $D_t = M_t - M_{t-1}$ for $t \\geq 1$ and $D_0 = \\mathbf{0}$.\n    3. An intercept column (vector of ones).\n    4. A demeaned linear trend column, $(t - \\bar{t})$.\n- **Evaluation Metric:** Pearson correlation coefficient, $\\rho(r,n)$.\n- **Three Test Cases:** Each case specifies the number of time points $T$, the repetition time TR, and deterministic formulas for a ground-truth neural signal $n_t$, motion parameters $M$, and the observed ROI time series $y_t$. The cases are designed to test a general scenario, a boundary condition (zero motion), and an edge case (rank-deficient design matrix).\n\n**Step 2: Validation Using Extracted Givens**\nA critical assessment of the problem statement against the required criteria yields the following:\n1.  **Scientific Grounding:** The problem is firmly rooted in the standard methodology for fMRI data analysis. The use of a GLM to model and regress out nuisance signals—including the \"Friston 24-parameter model\" extended family (motion, derivatives, and sometimes their squares) and low-frequency drifts (intercept and linear trend)—is a cornerstone of resting-state fMRI preprocessing. The mathematical tools specified, namely OLS and the Moore–Penrose pseudo-inverse, are the correct and standard methods for solving such a model, especially in the presence of collinear regressors. The problem is scientifically and mathematically sound.\n2.  **Well-Posedness:** The problem is unambiguously well-posed. All signals and parameters are defined by explicit mathematical equations, and the computational procedure is algorithmically specified. This ensures that a unique and stable numerical solution exists for each test case. The inclusion of a rank-deficient case, coupled with the requirement to use the pseudo-inverse, demonstrates a sophisticated understanding of potential numerical issues and their proper resolution.\n3.  **Objectivity:** The problem statement is written in precise, objective language, free of any subjective or speculative claims. All components are quantitatively defined.\n4.  **Completeness and Consistency:** All necessary information, including parameters ($T$, TR), signal definitions, and procedural steps, is provided. The test cases are internally consistent and logically follow from the given framework.\n5.  **Feasibility:** The parameter values (TR, T, signal frequencies) are realistic for fMRI studies. The computational task is feasible and directly implementable using standard numerical libraries.\n\n**Step 3: Verdict**\nThe problem is **valid**. It represents a well-formulated, scientifically relevant, and verifiable exercise in computational neuroscience and signal processing.\n\n### Principled Solution\n\nThe core principle is to isolate a signal of interest by modeling and removing known sources of nuisance variance. This is accomplished using a linear model framework.\n\n**1. Modeling Nuisance Signals with a Design Matrix**\nThe observed fMRI time series, $y \\in \\mathbb{R}^T$, is assumed to be a linear mixture of the true neural signal, various noise sources (confounds), and measurement error. The GLM, $y = X\\beta + \\varepsilon$, provides a mathematical structure for this assumption. The key is to construct a design matrix, $X \\in \\mathbb{R}^{T \\times p}$, whose columns (regressors) represent the time courses of the suspected nuisance signals.\nIn this problem, the design matrix $X$ is constructed to model four types of confounds:\n- **Rigid-body motion ($M$):** Head motion is a primary source of non-neural variance in fMRI. The $6$ columns of $M$ model the translational and rotational shifts at each time point.\n- **Temporal derivatives of motion ($D$):** The backward differences of the motion parameters, $D_t = M_t - M_{t-1}$, are included to capture more complex, temporally-shifted effects of motion.\n- **DC offset (Intercept):** A column of ones is included to model the mean signal level, which has no physiological interpretation.\n- **Linear drift (Trend):** A column representing a linear trend, $t-\\bar{t}$, is included to account for slow scanner-induced drifts over time.\nThe complete design matrix is formed by concatenating these components: $X = [M, D, \\mathbf{1}, (t-\\bar{t})]$. For this problem, the dimensions are $T \\times 14$ ($6$ for $M$, $6$ for $D$, $1$ for intercept, $1$ for trend).\n\n**2. Estimating and Removing Nuisance Signals**\nThe goal is to find the contribution of these nuisance regressors to the observed signal $y$. The Ordinary Least Squares (OLS) method finds the set of coefficients, $\\hat{\\beta} \\in \\mathbb{R}^p$, that minimizes the sum of squared differences between the observed signal $y$ and the modeled signal $X\\beta$. The solution is given by:\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n$$\nIn practice, the matrix $X^T X$ may be ill-conditioned or singular (not invertible), especially if some regressors are highly correlated (collinear). The problem explicitly creates this scenario in Test Case 3. The Moore-Penrose pseudo-inverse, $X^{+}$, provides a stable and general solution that is valid even for rank-deficient matrices:\n$$\n\\hat{\\beta} = X^{+} y\n$$\nOnce $\\hat{\\beta}$ is estimated, we can compute the signal component that is explained by our nuisance regressors: $\\hat{y} = X\\hat{\\beta}$.\n\n**3. Computing the Residuals**\nThe residual time series, $r$, is what remains after subtracting the modeled nuisance component from the original observation:\n$$\nr = y - \\hat{y} = y - X\\hat{\\beta}\n$$\nBy construction, the residual vector $r$ is orthogonal to the column space of $X$ ($X^T r = \\mathbf{0}$). This means we have projected $y$ onto the subspace orthogonal to the space of confounds, effectively \"cleaning\" the signal of any variance that can be linearly explained by the columns of $X$. This residual series $r$ is our estimate of the true underlying neural signal, up to unmodeled noise and measurement error $\\varepsilon$.\n\n**4. Evaluating the Denoising Performance**\nTo quantify how well the residual series $r$ recovers the known ground-truth neural signal $n$, we compute the Pearson correlation coefficient, $\\rho(r, n)$.\n$$\n\\rho(r,n) = \\frac{\\text{cov}(r, n)}{\\sigma_r \\sigma_n} = \\frac{\\sum_{t=0}^{T-1} (r_t - \\bar{r})(n_t - \\bar{n})}{\\sqrt{\\sum_{t=0}^{T-1} (r_t - \\bar{r})^2} \\sqrt{\\sum_{t=0}^{T-1} (n_t - \\bar{n})^2}}\n$$\nA correlation close to $1$ indicates that the denoising procedure was successful in removing the confounds, leaving a residual signal that closely matches the temporal dynamics of the true neural activity.\n\nThe implementation will proceed by executing these four steps for each of the three test cases specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not used as per the instructions.\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for fMRI signal denoising.\n    \"\"\"\n\n    def process_case(TR, T, n_func, m_func, y_func_extras):\n        \"\"\"\n        Processes a single test case for fMRI signal denoising.\n\n        Args:\n            TR (float): Repetition time in seconds.\n            T (int): Number of time points.\n            n_func (callable): Function to generate the neural ground truth signal n(t).\n            m_func (callable): Function to generate the motion parameters matrix M(t).\n            y_func_extras (callable): Function to generate extra components of the ROI signal y(t).\n\n        Returns:\n            float: The Pearson correlation between the residual signal r and the neural signal n.\n        \"\"\"\n        # 1. Generate time vectors and base signals\n        t = np.arange(T)\n        u = t / T\n\n        # Generate neural ground truth\n        n = n_func(t, TR, u)\n\n        # Generate motion parameters M\n        M = m_func(t, TR, u)\n        \n        # Generate temporal derivatives D\n        D = np.zeros_like(M)\n        if T > 1:\n            D[1:] = M[1:] - M[:-1]\n\n        # Generate the observed ROI time series y\n        y_confound_M = np.sum(y_func_extras[\"b\"] * M, axis=1) if \"b\" in y_func_extras else 0\n        y_confound_D = np.sum(y_func_extras[\"c\"] * D, axis=1) if \"c\" in y_func_extras else 0\n        y_extra_terms = y_func_extras[\"extra_func\"](t, TR, u) if \"extra_func\" in y_func_extras else 0\n        \n        y = n + y_confound_M + y_confound_D + y_extra_terms\n\n        # 2. Construct the design matrix X\n        # Intercept column\n        intercept = np.ones((T, 1))\n        # Linear trend column (demeaned)\n        t_bar = np.mean(t)\n        linear_trend = (t - t_bar).reshape(-1, 1)\n\n        # Concatenate all regressors\n        # X will have T rows and 6 (M) + 6 (D) + 1 (intercept) + 1 (trend) = 14 columns\n        X = np.c_[M, D, intercept, linear_trend]\n\n        # 3. Perform OLS regression and compute residuals\n        # Use Moore-Penrose pseudo-inverse for stability and to handle rank deficiency\n        X_pinv = np.linalg.pinv(X)\n        beta_hat = X_pinv @ y\n        \n        # Residuals are the \"cleaned\" signal\n        r = y - X @ beta_hat\n        \n        # 4. Evaluate performance using Pearson correlation\n        # np.corrcoef returns a 2x2 matrix, the off-diagonal element is the correlation\n        correlation = np.corrcoef(r, n)[0, 1]\n        \n        return correlation\n\n    # Test Case 1: General case\n    case1_params = {\n        \"TR\": 2.0, \"T\": 200,\n        \"n_func\": lambda t, TR, u: np.sin(2 * np.pi * 0.03 * TR * t) + 0.5 * np.sin(2 * np.pi * 0.05 * TR * t + np.pi / 4),\n        \"m_func\": lambda t, TR, u: np.c_[\n            0.5 * np.sin(2 * np.pi * 0.2 * TR * t) + 0.1 * u,\n            0.4 * np.cos(2 * np.pi * 0.25 * TR * t),\n            0.3 * np.sin(2 * np.pi * 0.15 * TR * t),\n            0.5 * u,\n            0.2 * np.sin(2 * np.pi * 0.05 * TR * t),\n            0.25 * np.cos(2 * np.pi * 0.07 * TR * t)\n        ],\n        \"y_func_extras\": {\n            \"b\": np.array([0.9, -0.5, 0.3, 0.6, -0.4, 0.2]),\n            \"c\": np.array([0.5, 0.1, -0.2, 0.3, 0.0, -0.1]),\n            \"extra_func\": lambda t, TR, u: 0.1 * np.sin(2 * np.pi * 0.4 * TR * t)\n        }\n    }\n\n    # Test Case 2: Boundary condition (zero motion)\n    case2_params = {\n        \"TR\": 0.8, \"T\": 60,\n        \"n_func\": lambda t, TR, u: np.sin(2 * np.pi * 0.04 * TR * t) + 0.6 * np.cos(2 * np.pi * 0.02 * TR * t),\n        \"m_func\": lambda t, TR, u: np.zeros((len(t), 6)),\n        \"y_func_extras\": {\n            \"extra_func\": lambda t, TR, u: 0.05 * np.sin(2 * np.pi * 0.3 * TR * t) + 0.2 * u\n        }\n    }\n\n    # Test Case 3: Edge case (rank deficiency)\n    case3_params = {\n        \"TR\": 1.5, \"T\": 120,\n        \"n_func\": lambda t, TR, u: np.sin(2 * np.pi * 0.025 * TR * t),\n        \"m_func\": lambda t, TR, u: np.c_[\n            np.sin(2 * np.pi * 0.1 * TR * t),\n            np.sin(2 * np.pi * 0.1 * TR * t), # Collinear with m1\n            0.3 * np.sin(2 * np.pi * 0.1 * TR * t + np.pi/3),\n            0.5 * u,\n            0.5 * u, # Collinear with m4\n            np.cos(2 * np.pi * 0.2 * TR * t)\n        ],\n        \"y_func_extras\": {\n            \"b\": np.array([0.5, -0.5, 0.4, 0.3, -0.3, 0.2]),\n            \"c\": np.array([0.2, -0.2, 0.1, 0.1, -0.1, 0.0])\n        }\n    }\n\n    test_cases = [case1_params, case2_params, case3_params]\n    results = []\n    for case in test_cases:\n        result = process_case(case[\"TR\"], case[\"T\"], case[\"n_func\"], case[\"m_func\"], case[\"y_func_extras\"])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{res:.6f}' for res in results])}]\")\n\nsolve()\n```", "id": "5056395"}, {"introduction": "One of the most debated findings in resting-state research is the strong anticorrelation observed between the Default Mode Network (DMN) and attention networks. However, this observation is heavily influenced by a data processing step called Global Signal Regression (GSR). This exercise challenges you to investigate this issue directly by calculating the predicted mathematical effect of GSR and comparing it to empirical data, teaching you to critically evaluate how analysis choices can shape scientific conclusions [@problem_id:5056380].", "problem": "You are given symmetric region-by-region Pearson correlation matrices representing resting-state functional magnetic resonance imaging time series for a set of brain regions, with the diagonal entries equal to $1.00$. The context is the study of resting-state networks, focusing on the Default Mode Network (DMN) and the Dorsal Attention Network (DAN). The primary aim is to assess the effect of removing shared variance via global signal regression (GSR) on observed anticorrelations between the DMN and the DAN. Specifically, you must compute whether DMN–DAN anticorrelations observed after GSR exceed what is expected purely from removing the global shared component.\n\nFundamental base and definitions:\n- Pearson correlation quantifies linear association between standardized time series. Let $X_i$ denote the standardized time series for region $i$. The correlation matrix without GSR is $R^{(0)}$, with entries $r^{(0)}_{ij} = \\mathrm{corr}(X_i, X_j)$.\n- The global signal $G$ is defined as the temporal mean across all $N$ regional time series: $G = \\frac{1}{N}\\sum_{k=1}^{N} X_k$. Under linear regression, removing the global signal is equivalent to computing the partial correlation that controls for $G$.\n- For any region $i$, the covariance with the global signal is $\\mathrm{cov}(X_i, G) = \\frac{1}{N}\\sum_{k=1}^{N} r^{(0)}_{ik}$, and the variance of the global signal is $\\mathrm{var}(G) = \\frac{1}{N^2}\\sum_{a=1}^{N}\\sum_{b=1}^{N} r^{(0)}_{ab}$. Therefore, the correlation between region $i$ and the global signal is\n$$\nr_{iG} = \\frac{\\frac{1}{N}\\sum_{k=1}^{N} r^{(0)}_{ik}}{\\sqrt{\\mathrm{var}(G)}}.\n$$\n- The expected pairwise partial correlation between regions $i$ and $j$ after removing the global signal is\n$$\nr^{\\mathrm{pred}}_{ij} = \\frac{r^{(0)}_{ij} - r_{iG}\\, r_{jG}}{\\sqrt{\\left(1 - r_{iG}^2\\right)\\left(1 - r_{jG}^2\\right)}}.\n$$\nThis quantity represents the correlation that is expected solely from removing shared variance attributable to the global signal.\n\nGiven:\n- $R^{(0)}$: correlation matrix without GSR.\n- $R^{(1)}$: empirical correlation matrix with GSR.\n- Network index sets: the Default Mode Network (DMN) indices and the Dorsal Attention Network (DAN) indices, both as disjoint subsets of $\\{0,1,\\dots,N-1\\}$.\n\nYour tasks:\n1. Compute the difference matrix $\\Delta = R^{(1)} - R^{(0)}$.\n2. Using $R^{(0)}$ only, compute $r_{iG}$ for all $i$, then compute the predicted partial correlation matrix $R^{\\mathrm{pred}}$ with entries $r^{\\mathrm{pred}}_{ij}$ according to the formula above.\n3. For all DMN–DAN pairs $(i,j)$ with $i$ in DMN and $j$ in DAN, assess whether the observed correlations $r^{(1)}_{ij}$ are more negative than the prediction $r^{\\mathrm{pred}}_{ij}$ based only on shared variance removal. Quantify this assessment as the fraction, in decimal form, of DMN–DAN pairs satisfying $r^{(1)}_{ij} < r^{\\mathrm{pred}}_{ij}$.\n\nYour program must process the following test suite. Each test case gives $N$, DMN indices, DAN indices, $R^{(0)}$, and $R^{(1)}$. All numeric entries are in unitless correlation values.\n\nTest case $1$:\n- $N = 6$.\n- DMN indices: $[0,1,2]$.\n- DAN indices: $[3,4,5]$.\n- $R^{(0)}$:\n  $$\n  \\begin{bmatrix}\n  1.00 & 0.45 & 0.40 & -0.12 & -0.15 & -0.10 \\\\\n  0.45 & 1.00 & 0.42 & -0.18 & -0.16 & -0.14 \\\\\n  0.40 & 0.42 & 1.00 & -0.11 & -0.13 & -0.12 \\\\\n  -0.12 & -0.18 & -0.11 & 1.00 & 0.48 & 0.44 \\\\\n  -0.15 & -0.16 & -0.13 & 0.48 & 1.00 & 0.46 \\\\\n  -0.10 & -0.14 & -0.12 & 0.44 & 0.46 & 1.00\n  \\end{bmatrix}\n  $$\n- $R^{(1)}$:\n  $$\n  \\begin{bmatrix}\n  1.00 & 0.47 & 0.45 & -0.25 & -0.28 & -0.22 \\\\\n  0.47 & 1.00 & 0.46 & -0.30 & -0.29 & -0.27 \\\\\n  0.45 & 0.46 & 1.00 & -0.24 & -0.26 & -0.25 \\\\\n  -0.25 & -0.30 & -0.24 & 1.00 & 0.50 & 0.47 \\\\\n  -0.28 & -0.29 & -0.26 & 0.50 & 1.00 & 0.49 \\\\\n  -0.22 & -0.27 & -0.25 & 0.47 & 0.49 & 1.00\n  \\end{bmatrix}\n  $$\n\nTest case $2$:\n- $N = 6$.\n- DMN indices: $[0,1,2]$.\n- DAN indices: $[3,4,5]$.\n- $R^{(0)}$:\n  $$\n  \\begin{bmatrix}\n  1.00 & 0.30 & 0.28 & -0.02 & 0.00 & 0.01 \\\\\n  0.30 & 1.00 & 0.32 & -0.01 & 0.02 & 0.00 \\\\\n  0.28 & 0.32 & 1.00 & 0.00 & -0.01 & 0.02 \\\\\n  -0.02 & -0.01 & 0.00 & 1.00 & 0.31 & 0.29 \\\\\n  0.00 & 0.02 & -0.01 & 0.31 & 1.00 & 0.33 \\\\\n  0.01 & 0.00 & 0.02 & 0.29 & 0.33 & 1.00\n  \\end{bmatrix}\n  $$\n- $R^{(1)}$:\n  $$\n  \\begin{bmatrix}\n  1.00 & 0.29 & 0.27 & -0.02 & -0.01 & 0.00 \\\\\n  0.29 & 1.00 & 0.31 & -0.02 & 0.01 & -0.01 \\\\\n  0.27 & 0.31 & 1.00 & 0.00 & -0.02 & 0.01 \\\\\n  -0.02 & -0.02 & 0.00 & 1.00 & 0.30 & 0.28 \\\\\n  -0.01 & 0.01 & -0.02 & 0.30 & 1.00 & 0.32 \\\\\n  0.00 & -0.01 & 0.01 & 0.28 & 0.32 & 1.00\n  \\end{bmatrix}\n  $$\n\nTest case $3$:\n- $N = 8$.\n- DMN indices: $[0,1,2,3]$.\n- DAN indices: $[4,5,6,7]$.\n- $R^{(0)}$:\n  $$\n  \\begin{bmatrix}\n  1.00 & 0.55 & 0.50 & 0.52 & -0.05 & -0.04 & -0.06 & -0.05 \\\\\n  0.55 & 1.00 & 0.53 & 0.54 & -0.06 & -0.05 & -0.07 & -0.06 \\\\\n  0.50 & 0.53 & 1.00 & 0.51 & -0.04 & -0.03 & -0.05 & -0.04 \\\\\n  0.52 & 0.54 & 0.51 & 1.00 & -0.05 & -0.04 & -0.06 & -0.05 \\\\\n  -0.05 & -0.06 & -0.04 & -0.05 & 1.00 & 0.56 & 0.54 & 0.52 \\\\\n  -0.04 & -0.05 & -0.03 & -0.04 & 0.56 & 1.00 & 0.55 & 0.53 \\\\\n  -0.06 & -0.07 & -0.05 & -0.06 & 0.54 & 0.55 & 1.00 & 0.57 \\\\\n  -0.05 & -0.06 & -0.04 & -0.05 & 0.52 & 0.53 & 0.57 & 1.00\n  \\end{bmatrix}\n  $$\n- $R^{(1)}$:\n  $$\n  \\begin{bmatrix}\n  1.00 & 0.58 & 0.54 & 0.56 & -0.22 & -0.21 & -0.23 & -0.22 \\\\\n  0.58 & 1.00 & 0.57 & 0.58 & -0.24 & -0.23 & -0.25 & -0.24 \\\\\n  0.54 & 0.57 & 1.00 & 0.55 & -0.20 & -0.19 & -0.21 & -0.20 \\\\\n  0.56 & 0.58 & 0.55 & 1.00 & -0.23 & -0.22 & -0.24 & -0.23 \\\\\n  -0.22 & -0.24 & -0.20 & -0.23 & 1.00 & 0.60 & 0.58 & 0.57 \\\\\n  -0.21 & -0.23 & -0.19 & -0.22 & 0.60 & 1.00 & 0.59 & 0.58 \\\\\n  -0.23 & -0.25 & -0.21 & -0.24 & 0.58 & 0.59 & 1.00 & 0.61 \\\\\n  -0.22 & -0.24 & -0.20 & -0.23 & 0.57 & 0.58 & 0.61 & 1.00\n  \\end{bmatrix}\n  $$\n\nOutput specification:\n- For each test case, compute the fraction of DMN–DAN pairs with $r^{(1)}_{ij} < r^{\\mathrm{pred}}_{ij}$ and round the fraction to three decimal places as a decimal number.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[0.125,0.000,1.000]$ for three test cases.\n\nNo physical units or angles are involved. Express any fractional results as decimals, not with a percentage sign. The input is embedded in your program; no user input or external files will be used.", "solution": "The problem is valid as it is scientifically grounded in the principles of neuroimaging data analysis, mathematically well-posed, and provides a complete and consistent set of data and definitions for its resolution. The analysis directly addresses a known methodological question in functional connectivity research concerning the effects of Global Signal Regression (GSR).\n\nThe solution proceeds by implementing the computations as described in the problem statement for each test case. The overall goal is to determine the fraction of region pairs between the Default Mode Network (DMN) and the Dorsal Attention Network (DAN) for which the empirically observed correlation after GSR, $r^{(1)}_{ij}$, is more negative than the correlation predicted purely by the statistical removal of a common global signal, $r^{\\mathrm{pred}}_{ij}$.\n\nThe logical steps for each test case are as follows:\n\n1.  **Parse Inputs**: For each test case, we are given the number of regions $N$, the index sets for DMN and DAN, the correlation matrix before GSR, $R^{(0)}$, and the empirical correlation matrix after GSR, $R^{(1)}$.\n\n2.  **Compute Predicted Correlations $R^{\\mathrm{pred}}$**: The core of the task is to compute the matrix $R^{\\mathrm{pred}}$ using only the pre-GSR matrix $R^{(0)}$. This involves three sub-steps based on the provided formulas.\n\n    a.  **Compute Variance of the Global Signal $\\mathrm{var}(G)$**: The global signal $G$ is the average of all regional time series. As the time series are standardized, the variance of $G$ can be calculated from the correlation matrix $R^{(0)}$:\n        $$\n        \\mathrm{var}(G) = \\frac{1}{N^2}\\sum_{a=1}^{N}\\sum_{b=1}^{N} r^{(0)}_{ab}\n        $$\n        This is implemented by summing all entries of the matrix $R^{(0)}$ and dividing by $N^2$.\n\n    b.  **Compute Region-to-Global-Signal Correlations $r_{iG}$**: For each region $i$, its correlation with the global signal, $r_{iG}$, is calculated. This requires the covariance between the region's time series $X_i$ and the global signal $G$, which is given by $\\mathrm{cov}(X_i, G) = \\frac{1}{N}\\sum_{k=1}^{N} r^{(0)}_{ik}$. This is the mean of the $i$-th row (or column) of $R^{(0)}$. The correlation is then:\n        $$\n        r_{iG} = \\frac{\\mathrm{cov}(X_i, G)}{\\sqrt{\\mathrm{var}(G)}} = \\frac{\\frac{1}{N}\\sum_{k=1}^{N} r^{(0)}_{ik}}{\\sqrt{\\mathrm{var}(G)}}\n        $$\n        This calculation is performed for each region $i$ from $0$ to $N-1$, yielding a vector of $r_{iG}$ values.\n\n    c.  **Compute Predicted Partial Correlation Matrix $R^{\\mathrm{pred}}$**: Using the $r_{iG}$ values, the predicted partial correlation $r^{\\mathrm{pred}}_{ij}$ for each pair of regions $(i,j)$ is computed using the standard formula for partial correlation, which models the effect of regressing out the global signal $G$:\n        $$\n        r^{\\mathrm{pred}}_{ij} = \\frac{r^{(0)}_{ij} - r_{iG}\\, r_{jG}}{\\sqrt{\\left(1 - r_{iG}^2\\right)\\left(1 - r_{jG}^2\\right)}}\n        $$\n        This formula is applied to all pairs $(i,j)$ to construct the full predicted matrix $R^{\\mathrm{pred}}$. The diagonal elements are $1$.\n\n3.  **Quantify DMN-DAN Anticorrelation Differences**: The final step is to compare the empirical post-GSR correlations with the predicted ones for the specific network pairs of interest.\n    \n    a.  **Identify DMN-DAN Pairs**: We iterate through all pairs of regions $(i, j)$ such that region $i$ is in the DMN index set and region $j$ is in the DAN index set.\n\n    b.  **Apply the Condition**: For each DMN-DAN pair $(i, j)$, we test the condition $r^{(1)}_{ij} < r^{\\mathrm{pred}}_{ij}$. This condition is true if the empirically observed correlation is more negative (i.e., a smaller number) than the one predicted by the statistical artifact of GSR.\n\n    c.  **Calculate the Fraction**: A counter is maintained for the number of pairs satisfying the condition. This count is then divided by the total number of DMN-DAN pairs, which is the product of the number of regions in the DMN and the DAN.\n        $$\n        \\text{fraction} = \\frac{\\left| \\left\\{ (i,j) \\mid i \\in \\text{DMN}, j \\in \\text{DAN}, \\text{ and } r^{(1)}_{ij} < r^{\\mathrm{pred}}_{ij} \\right\\} \\right|}{\\left| \\text{DMN} \\right| \\times \\left| \\text{DAN} \\right|}\n        $$\n\n4.  **Format Output**: The resulting fraction for each test case is rounded to three decimal places. The final output is a comma-separated list of these fractions, enclosed in square brackets.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Processes a suite of test cases to analyze the effect of Global Signal Regression (GSR)\n    on DMN-DAN anticorrelations in fMRI data.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"N\": 6,\n            \"dmn_indices\": [0, 1, 2],\n            \"dan_indices\": [3, 4, 5],\n            \"R0\": [\n                [1.00, 0.45, 0.40, -0.12, -0.15, -0.10],\n                [0.45, 1.00, 0.42, -0.18, -0.16, -0.14],\n                [0.40, 0.42, 1.00, -0.11, -0.13, -0.12],\n                [-0.12, -0.18, -0.11, 1.00, 0.48, 0.44],\n                [-0.15, -0.16, -0.13, 0.48, 1.00, 0.46],\n                [-0.10, -0.14, -0.12, 0.44, 0.46, 1.00]\n            ],\n            \"R1\": [\n                [1.00, 0.47, 0.45, -0.25, -0.28, -0.22],\n                [0.47, 1.00, 0.46, -0.30, -0.29, -0.27],\n                [0.45, 0.46, 1.00, -0.24, -0.26, -0.25],\n                [-0.25, -0.30, -0.24, 1.00, 0.50, 0.47],\n                [-0.28, -0.29, -0.26, 0.50, 1.00, 0.49],\n                [-0.22, -0.27, -0.25, 0.47, 0.49, 1.00]\n            ]\n        },\n        {\n            \"N\": 6,\n            \"dmn_indices\": [0, 1, 2],\n            \"dan_indices\": [3, 4, 5],\n            \"R0\": [\n                [1.00, 0.30, 0.28, -0.02, 0.00, 0.01],\n                [0.30, 1.00, 0.32, -0.01, 0.02, 0.00],\n                [0.28, 0.32, 1.00, 0.00, -0.01, 0.02],\n                [-0.02, -0.01, 0.00, 1.00, 0.31, 0.29],\n                [0.00, 0.02, -0.01, 0.31, 1.00, 0.33],\n                [0.01, 0.00, 0.02, 0.29, 0.33, 1.00]\n            ],\n            \"R1\": [\n                [1.00, 0.29, 0.27, -0.02, -0.01, 0.00],\n                [0.29, 1.00, 0.31, -0.02, 0.01, -0.01],\n                [0.27, 0.31, 1.00, 0.00, -0.02, 0.01],\n                [-0.02, -0.02, 0.00, 1.00, 0.30, 0.28],\n                [-0.01, 0.01, -0.02, 0.30, 1.00, 0.32],\n                [0.00, -0.01, 0.01, 0.28, 0.32, 1.00]\n            ]\n        },\n        {\n            \"N\": 8,\n            \"dmn_indices\": [0, 1, 2, 3],\n            \"dan_indices\": [4, 5, 6, 7],\n            \"R0\": [\n                [1.00, 0.55, 0.50, 0.52, -0.05, -0.04, -0.06, -0.05],\n                [0.55, 1.00, 0.53, 0.54, -0.06, -0.05, -0.07, -0.06],\n                [0.50, 0.53, 1.00, 0.51, -0.04, -0.03, -0.05, -0.04],\n                [0.52, 0.54, 0.51, 1.00, -0.05, -0.04, -0.06, -0.05],\n                [-0.05, -0.06, -0.04, -0.05, 1.00, 0.56, 0.54, 0.52],\n                [-0.04, -0.05, -0.03, -0.04, 0.56, 1.00, 0.55, 0.53],\n                [-0.06, -0.07, -0.05, -0.06, 0.54, 0.55, 1.00, 0.57],\n                [-0.05, -0.06, -0.04, -0.05, 0.52, 0.53, 0.57, 1.00]\n            ],\n            \"R1\": [\n                [1.00, 0.58, 0.54, 0.56, -0.22, -0.21, -0.23, -0.22],\n                [0.58, 1.00, 0.57, 0.58, -0.24, -0.23, -0.25, -0.24],\n                [0.54, 0.57, 1.00, 0.55, -0.20, -0.19, -0.21, -0.20],\n                [0.56, 0.58, 0.55, 1.00, -0.23, -0.22, -0.24, -0.23],\n                [-0.22, -0.24, -0.20, -0.23, 1.00, 0.60, 0.58, 0.57],\n                [-0.21, -0.23, -0.19, -0.22, 0.60, 1.00, 0.59, 0.58],\n                [-0.23, -0.25, -0.21, -0.24, 0.58, 0.59, 1.00, 0.61],\n                [-0.22, -0.24, -0.20, -0.23, 0.57, 0.58, 0.61, 1.00]\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        dmn_indices = case[\"dmn_indices\"]\n        dan_indices = case[\"dan_indices\"]\n        R0 = np.array(case[\"R0\"])\n        R1 = np.array(case[\"R1\"])\n\n        # Calculate variance of the global signal G\n        var_G = np.sum(R0) / (N**2)\n        sqrt_var_G = np.sqrt(var_G)\n        \n        # Calculate correlation of each region i with the global signal G\n        r_iG = np.zeros(N)\n        if sqrt_var_G > 0:\n            cov_Xi_G = np.sum(R0, axis=1) / N\n            r_iG = cov_Xi_G / sqrt_var_G\n\n        # Calculate the predicted partial correlation matrix R_pred\n        R_pred = np.zeros_like(R0)\n        for i in range(N):\n            for j in range(N):\n                if i == j:\n                    R_pred[i, j] = 1.0\n                    continue\n                \n                # Denominator for the partial correlation formula\n                den_sqrt_term = (1 - r_iG[i]**2) * (1 - r_iG[j]**2)\n                \n                # Handle potential numerical instability if correlations are perfect\n                if den_sqrt_term = 0:\n                    # This case implies r_iG or r_jG is 1, partial correlation is ill-defined.\n                    # With valid correlation matrices, this should not happen. We set to NaN.\n                    R_pred[i, j] = np.nan\n                    continue\n\n                denominator = np.sqrt(den_sqrt_term)\n                numerator = R0[i, j] - r_iG[i] * r_iG[j]\n                R_pred[i, j] = numerator / denominator\n\n        # Count DMN-DAN pairs where observed correlation is more negative than predicted\n        count_exceeding = 0\n        total_pairs = len(dmn_indices) * len(dan_indices)\n\n        for i in dmn_indices:\n            for j in dan_indices:\n                if R1[i, j]  R_pred[i, j]:\n                    count_exceeding += 1\n\n        # Calculate the final fraction\n        if total_pairs > 0:\n            fraction = count_exceeding / total_pairs\n        else:\n            fraction = 0.0\n\n        results.append(f\"{fraction:.3f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "5056380"}, {"introduction": "A key goal in neuroscience is to understand how intrinsic brain states influence our behavior. The strength of connectivity within the Default Mode Network (DMN) during rest has been linked to performance on subsequent tasks. In this capstone practice, you will build a predictive model from the ground up, using DMN connectivity from a group of individuals to forecast their reaction time variability, demonstrating a powerful link between brain and behavior [@problem_id:5056143].", "problem": "You are to derive and implement a predictive model that uses pre-task Default Mode Network (DMN) connectivity to forecast performance on externally oriented tasks, operationalized as reaction time (RT) variability. The Default Mode Network (DMN) is a set of brain regions that exhibit coherent activity during rest; stronger DMN connectivity prior to a task is often associated with increased variability in externally oriented performance due to internally focused processing. Starting from fundamental definitions in neurobiology and statistics, construct a model in which a scalar DMN connectivity index predicts RT variability via ordinary least squares linear regression, and apply it to a specified test suite.\n\nFundamental base and definitions:\n- Resting-state functional connectivity between two regions is defined by the Pearson correlation coefficient $r$ computed from their blood oxygen level dependent time series during rest.\n- To obtain a normally distributed summary index from correlation coefficients, use the Fisher $z$-transform $z = \\operatorname{atanh}(r)$.\n- Given $N$ DMN regions, with $M = N(N-1)/2$ unique pairwise correlations $\\{r_{ij}\\}$ (upper triangle), define the DMN connectivity index for participant $i$ as the arithmetic mean of the Fisher $z$-transformed pairwise correlations:\n$$\nC_i = \\frac{1}{M} \\sum_{(j,k)\\in \\mathcal{U}} \\operatorname{atanh}(r_{jk}^{(i)}),\n$$\nwhere $\\mathcal{U}$ denotes the set of upper-triangular index pairs and $r_{jk}^{(i)} \\in (-1,1)$.\n- Reaction time variability for participant $i$ is defined as the population standard deviation (denominator $n$) of their trial-wise reaction times $\\{t_{i\\ell}\\}_{\\ell=1}^n$ recorded during an externally oriented task:\n$$\nV_i = \\sqrt{\\frac{1}{n}\\sum_{\\ell=1}^n \\left(t_{i\\ell} - \\bar{t}_i\\right)^2},\n\\quad \\text{with} \\quad\n\\bar{t}_i = \\frac{1}{n} \\sum_{\\ell=1}^n t_{i\\ell}.\n$$\n- The predictive model is a linear regression linking connectivity to variability:\n$$\n\\hat{V} = a + b C,\n$$\nwith parameters $(a,b)$ chosen to minimize the sum of squared residuals on the training set. The ordinary least squares solution is:\n$$\nb = \\frac{\\sum_{i=1}^n (C_i - \\bar{C})(V_i - \\bar{V})}{\\sum_{i=1}^n (C_i - \\bar{C})^2},\n\\quad\na = \\bar{V} - b\\,\\bar{C},\n$$\nwhere $\\bar{C}$ and $\\bar{V}$ are the sample means of $\\{C_i\\}$ and $\\{V_i\\}$.\n\nTraining data specification:\n- Consider $N=4$ DMN regions, so $M=6$ unique pairwise correlations per participant. For each training participant $i\\in\\{1,\\dots,8\\}$, the six off-diagonal correlations $\\{r^{(i)}\\}$ are provided as follows (each list is ordered but order does not matter for the mean):\n1. Participant $1$: $[\\,0.15,\\,0.20,\\,0.10,\\,0.12,\\,0.18,\\,0.16\\,]$\n2. Participant $2$: $[\\,0.25,\\,0.28,\\,0.22,\\,0.24,\\,0.26,\\,0.27\\,]$\n3. Participant $3$: $[\\,0.35,\\,0.33,\\,0.32,\\,0.36,\\,0.37,\\,0.34\\,]$\n4. Participant $4$: $[\\,0.45,\\,0.48,\\,0.42,\\,0.44,\\,0.46,\\,0.47\\,]$\n5. Participant $5$: $[\\,0.55,\\,0.52,\\,0.56,\\,0.58,\\,0.54,\\,0.57\\,]$\n6. Participant $6$: $[\\,0.65,\\,0.62,\\,0.63,\\,0.66,\\,0.67,\\,0.64\\,]$\n7. Participant $7$: $[\\,0.70,\\,0.72,\\,0.68,\\,0.69,\\,0.71,\\,0.73\\,]$\n8. Participant $8$: $[\\,0.75,\\,0.78,\\,0.74,\\,0.76,\\,0.77,\\,0.79\\,]$\n- For each training participant $i$, the trial-wise reaction times $\\{t_{i\\ell}\\}$ are constructed deterministically from a base pattern vector $p$ and an amplitude $A_i$ (in seconds). Let the base pattern be\n$$\np = [\\,-0.3,\\,-0.2,\\,-0.1,\\,0.0,\\,0.1,\\,0.2,\\,0.3,\\,-0.1,\\,0.1,\\,0.0,\\,-0.2,\\,0.2\\,],\n$$\nand define\n$$\nt_{i\\ell} = 0.6 + A_i\\,p_\\ell \\quad \\text{seconds},\n$$\nfor $\\ell=1,\\dots,12$. Use the following amplitudes (seconds):\n$$\nA_1=0.16,\\; A_2=0.20,\\; A_3=0.24,\\; A_4=0.28,\\; A_5=0.32,\\; A_6=0.36,\\; A_7=0.40,\\; A_8=0.44.\n$$\nThese amplitudes produce increasing variability magnitudes in a scientifically plausible range for human reaction times.\n\nModel fitting task:\n- Compute $C_i$ for each training participant by averaging the Fisher $z$-transformed correlations.\n- Compute $V_i$ for each training participant as the population standard deviation of $t_{i\\ell}$.\n- Fit the linear regression to obtain $a$ and $b$ using the ordinary least squares formulas above.\n\nPrediction and test suite:\n- Using the fitted $(a,b)$, predict RT variability $\\hat{V}$ (in seconds) for three new participants (test cases) specified only by their six off-diagonal DMN correlation coefficients:\n1. Test case $1$ (moderate connectivity): $[\\,0.60,\\,0.58,\\,0.62,\\,0.59,\\,0.61,\\,0.60\\,]$\n2. Test case $2$ (low connectivity boundary): $[\\,0.05,\\,0.08,\\,0.02,\\,0.04,\\,0.06,\\,0.03\\,]$\n3. Test case $3$ (high connectivity edge): $[\\,0.82,\\,0.80,\\,0.83,\\,0.81,\\,0.84,\\,0.82\\,]$\n\nRequired output:\n- Your program should produce a single line of output containing the predicted RT variability values for the three test cases, in seconds, each rounded to six decimal places, as a comma-separated list enclosed in square brackets, for example: $[0.052345,0.031000,0.078999]$.", "solution": "The problem is to construct a predictive model for reaction time (RT) variability based on pre-task Default Mode Network (DMN) connectivity. The model will be a simple linear regression, trained on data from a cohort of participants and then used to make predictions for a new set of test cases. The procedure involves several steps: calculating a DMN connectivity index for each participant, determining their RT variability, fitting the regression model, and finally, applying the model for prediction.\n\nThe analysis proceeds in four main stages:\n1.  Computation of the independent variable (DMN connectivity index, $C_i$) for each of the $8$ training participants.\n2.  Computation of the dependent variable (RT variability, $V_i$) for each training participant.\n3.  Estimation of the parameters for the ordinary least squares (OLS) linear regression model.\n4.  Prediction of RT variability for $3$ test cases using the fitted model.\n\n**Step 1: Computation of the DMN Connectivity Index ($C_i$)**\n\nFor each participant $i$, the DMN connectivity index $C_i$ is defined as the arithmetic mean of the Fisher $z$-transformed Pearson correlation coefficients from $M$ unique pairwise connections within the DMN. With $N=4$ regions, there are $M = N(N-1)/2 = 6$ unique pairs. The formula is:\n$$\nC_i = \\frac{1}{6} \\sum_{(j,k)\\in \\mathcal{U}} \\operatorname{atanh}(r_{jk}^{(i)})\n$$\nwhere $\\{r_{jk}^{(i)}\\}$ are the $6$ correlation coefficients for participant $i$. The function $\\operatorname{atanh}(r)$ is the Fisher $z$-transform.\n\nUsing the provided correlation data for the $8$ training participants, we calculate the following connectivity indices:\n- Participant $1$: $r^{(1)} = [\\,0.15,\\,0.20,\\,0.10,\\,0.12,\\,0.18,\\,0.16\\,] \\implies C_1 \\approx 0.153018$\n- Participant $2$: $r^{(2)} = [\\,0.25,\\,0.28,\\,0.22,\\,0.24,\\,0.26,\\,0.27\\,] \\implies C_2 \\approx 0.259096$\n- Participant $3$: $r^{(3)} = [\\,0.35,\\,0.33,\\,0.32,\\,0.36,\\,0.37,\\,0.34\\,] \\implies C_3 \\approx 0.359877$\n- Participant $4$: $r^{(4)} = [\\,0.45,\\,0.48,\\,0.42,\\,0.44,\\,0.46,\\,0.47\\,] \\implies C_4 \\approx 0.489166$\n- Participant $5$: $r^{(5)} = [\\,0.55,\\,0.52,\\,0.56,\\,0.58,\\,0.54,\\,0.57\\,] \\implies C_5 \\approx 0.623617$\n- Participant $6$: $r^{(6)} = [\\,0.65,\\,0.62,\\,0.63,\\,0.66,\\,0.67,\\,0.64\\,] \\implies C_6 \\approx 0.767243$\n- Participant $7$: $r^{(7)} = [\\,0.70,\\,0.72,\\,0.68,\\,0.69,\\,0.71,\\,0.73\\,] \\implies C_7 \\approx 0.877983$\n- Participant $8$: $r^{(8)} = [\\,0.75,\\,0.78,\\,0.74,\\,0.76,\\,0.77,\\,0.79\\,] \\implies C_8 \\approx 1.009470$\n\nThese $8$ values constitute the set of independent variables $\\{C_i\\}_{i=1}^8$ for the regression.\n\n**Step 2: Computation of Reaction Time Variability ($V_i$)**\n\nThe RT variability $V_i$ is the population standard deviation of a participant's $n=12$ reaction times $\\{t_{i\\ell}\\}$. The times are generated by the formula $t_{i\\ell} = 0.6 + A_i p_\\ell$, where $A_i$ is a participant-specific amplitude and $p$ is a base pattern vector.\n\nThe formula for $V_i$ is:\n$$\nV_i = \\sqrt{\\frac{1}{n}\\sum_{\\ell=1}^n \\left(t_{i\\ell} - \\bar{t}_i\\right)^2}\n$$\nWe can simplify this calculation. First, find the mean reaction time $\\bar{t}_i$:\n$$\n\\bar{t}_i = \\frac{1}{n} \\sum_{\\ell=1}^n t_{i\\ell} = \\frac{1}{n} \\sum_{\\ell=1}^n (0.6 + A_i p_\\ell) = 0.6 + A_i \\left(\\frac{1}{n} \\sum_{\\ell=1}^n p_\\ell\\right) = 0.6 + A_i \\bar{p}\n$$\nThe deviation from the mean is then $t_{i\\ell} - \\bar{t}_i = (0.6 + A_i p_\\ell) - (0.6 + A_i \\bar{p}) = A_i(p_\\ell - \\bar{p})$.\nSubstituting this into the variance formula:\n$$\nV_i^2 = \\frac{1}{n}\\sum_{\\ell=1}^n \\left(A_i(p_\\ell - \\bar{p})\\right)^2 = A_i^2 \\left(\\frac{1}{n}\\sum_{\\ell=1}^n (p_\\ell - \\bar{p})^2\\right) = A_i^2 \\sigma_p^2\n$$\nwhere $\\sigma_p$ is the population standard deviation of the base pattern $p$. Since the amplitudes $A_i$ are all positive, $V_i = A_i \\sigma_p$.\n\nLet's compute $\\sigma_p$ for the given pattern $p = [\\,-0.3,\\,-0.2,\\,-0.1,\\,0.0,\\,0.1,\\,0.2,\\,0.3,\\,-0.1,\\,0.1,\\,0.0,\\,-0.2,\\,0.2\\,]$. The sum of its elements is $\\sum p_\\ell = 0$, so the mean $\\bar{p}=0$. The sum of squares is $\\sum p_\\ell^2 = (-0.3)^2 + (-0.2)^2 + \\dots + (0.2)^2 = 0.38$. Thus, the population variance is $\\sigma_p^2 = \\frac{1}{n} \\sum p_\\ell^2 = 0.38 / 12$. The standard deviation is $\\sigma_p = \\sqrt{0.38 / 12} \\approx 0.177951$.\n\nNow, we compute $V_i = A_i \\sigma_p$ for each training participant using their specified amplitude $A_i$:\n- $A_1=0.16 \\implies V_1 \\approx 0.028472$\n- $A_2=0.20 \\implies V_2 \\approx 0.035590$\n- $A_3=0.24 \\implies V_3 \\approx 0.042708$\n- $A_4=0.28 \\implies V_4 \\approx 0.049826$\n- $A_5=0.32 \\implies V_5 \\approx 0.056944$\n- $A_6=0.36 \\implies V_6 \\approx 0.064062$\n- $A_7=0.40 \\implies V_7 \\approx 0.071181$\n- $A_8=0.44 \\implies V_8 \\approx 0.078299$\n\nThese $8$ values constitute the set of dependent variables $\\{V_i\\}_{i=1}^8$.\n\n**Step 3: Fitting the Linear Regression Model**\n\nThe model is $\\hat{V} = a + b C$. The OLS parameters $(a,b)$ are found using the formulas:\n$$\nb = \\frac{\\sum_{i=1}^8 (C_i - \\bar{C})(V_i - \\bar{V})}{\\sum_{i=1}^8 (C_i - \\bar{C})^2}, \\quad a = \\bar{V} - b\\,\\bar{C}\n$$\nFirst, we compute the means of our training data vectors $\\{C_i\\}$ and $\\{V_i\\}$:\n- $\\bar{C} = \\frac{1}{8} \\sum C_i \\approx 0.567434$\n- $\\bar{V} = \\frac{1}{8} \\sum V_i \\approx 0.053385$\n\nNext, we compute the numerator (covariance term) and denominator (variance term) for the slope $b$:\n- $\\sum (C_i - \\bar{C})(V_i - \\bar{V}) \\approx 0.005117$\n- $\\sum (C_i - \\bar{C})^2 \\approx 0.088001$\n\nThe slope is $b = 0.005117 / 0.088001 \\approx 0.058146$.\nThe intercept is $a = \\bar{V} - b\\bar{C} \\approx 0.053385 - (0.058146 \\times 0.567434) \\approx 0.020412$.\n\nThe final predictive model is approximately $\\hat{V} = 0.020412 + 0.058146 C$.\n\n**Step 4: Prediction for Test Cases**\n\nWe now use this model to predict RT variability for the $3$ test cases. For each case, we first compute its connectivity index $C_{test}$ from the provided correlations, then substitute it into the regression equation.\n\n- **Test Case 1**: $r_{test,1} = [\\,0.60,\\,0.58,\\,0.62,\\,0.59,\\,0.61,\\,0.60\\,]$\n  - $C_{test,1} = \\frac{1}{6} \\sum \\operatorname{atanh}(r_{test,1}) \\approx 0.693383$\n  - $\\hat{V}_{test,1} = a + b C_{test,1} \\approx 0.020412 + (0.058146 \\times 0.693383) \\approx 0.060714$\n\n- **Test Case 2**: $r_{test,2} = [\\,0.05,\\,0.08,\\,0.02,\\,0.04,\\,0.06,\\,0.03\\,]$\n  - $C_{test,2} = \\frac{1}{6} \\sum \\operatorname{atanh}(r_{test,2}) \\approx 0.046726$\n  - $\\hat{V}_{test,2} = a + b C_{test,2} \\approx 0.020412 + (0.058146 \\times 0.046726) \\approx 0.023129$\n\n- **Test Case 3**: $r_{test,3} = [\\,0.82,\\,0.80,\\,0.83,\\,0.81,\\,0.84,\\,0.82\\,]$\n  - $C_{test,3} = \\frac{1}{6} \\sum \\operatorname{atanh}(r_{test,3}) \\approx 1.158098$\n  - $\\hat{V}_{test,3} = a + b C_{test,3} \\approx 0.020412 + (0.058146 \\times 1.158098) \\approx 0.087799$\n\nThe final predicted values for RT variability, rounded to six decimal places, are approximately $0.060714$, $0.023129$, and $0.087799$ seconds for test cases $1$, $2$, and $3$, respectively.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements a predictive model for RT variability from DMN connectivity.\n    \"\"\"\n\n    # --- Training Data Specification ---\n    # Correlation coefficients for N=8 training participants (M=6 pairs each)\n    training_correlations = [\n        [0.15, 0.20, 0.10, 0.12, 0.18, 0.16],  # P1\n        [0.25, 0.28, 0.22, 0.24, 0.26, 0.27],  # P2\n        [0.35, 0.33, 0.32, 0.36, 0.37, 0.34],  # P3\n        [0.45, 0.48, 0.42, 0.44, 0.46, 0.47],  # P4\n        [0.55, 0.52, 0.56, 0.58, 0.54, 0.57],  # P5\n        [0.65, 0.62, 0.63, 0.66, 0.67, 0.64],  # P6\n        [0.70, 0.72, 0.68, 0.69, 0.71, 0.73],  # P7\n        [0.75, 0.78, 0.74, 0.76, 0.77, 0.79],  # P8\n    ]\n\n    # Amplitudes for generating reaction times\n    training_amplitudes = [0.16, 0.20, 0.24, 0.28, 0.32, 0.36, 0.40, 0.44]\n\n    # Base pattern for reaction time trials\n    base_pattern = np.array([-0.3, -0.2, -0.1, 0.0, 0.1, 0.2, 0.3, -0.1, 0.1, 0.0, -0.2, 0.2])\n\n    # --- Test Suite Data ---\n    test_cases_correlations = [\n        [0.60, 0.58, 0.62, 0.59, 0.61, 0.60],  # Test case 1\n        [0.05, 0.08, 0.02, 0.04, 0.06, 0.03],  # Test case 2\n        [0.82, 0.80, 0.83, 0.81, 0.84, 0.82],  # Test case 3\n    ]\n\n    # === Step 1: Compute training variables C_i and V_i ===\n\n    # Compute DMN connectivity index (C_i) for each training participant\n    # C_i is the mean of the Fisher z-transformed correlations.\n    C_train = np.array([\n        np.mean(np.arctanh(r_set)) for r_set in training_correlations\n    ])\n\n    # Compute RT variability (V_i) for each training participant\n    # V_i is the population standard deviation of reaction times.\n    # As derived, V_i = A_i * population_std(base_pattern).\n    # numpy.std calculates population std by default (ddof=0).\n    sigma_p = np.std(base_pattern)\n    V_train = np.array(training_amplitudes) * sigma_p\n\n    # === Step 2: Fit the linear regression model ===\n    # Model: V_hat = a + b*C\n    # Using ordinary least squares (OLS) formulas.\n\n    # Calculate means of the training data vectors\n    C_mean = np.mean(C_train)\n    V_mean = np.mean(V_train)\n\n    # Calculate the slope (b)\n    numerator = np.sum((C_train - C_mean) * (V_train - V_mean))\n    denominator = np.sum((C_train - C_mean)**2)\n    b = numerator / denominator\n\n    # Calculate the intercept (a)\n    a = V_mean - b * C_mean\n\n    # === Step 3: Make predictions for test cases ===\n    \n    # Compute DMN connectivity index (C_test) for each test case\n    C_test = np.array([\n        np.mean(np.arctanh(r_set)) for r_set in test_cases_correlations\n    ])\n    \n    # Predict RT variability (V_hat) using the fitted model\n    V_predicted = a + b * C_test\n\n    # --- Format and print the final output ---\n    results = [f\"{val:.6f}\" for val in V_predicted]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "5056143"}]}