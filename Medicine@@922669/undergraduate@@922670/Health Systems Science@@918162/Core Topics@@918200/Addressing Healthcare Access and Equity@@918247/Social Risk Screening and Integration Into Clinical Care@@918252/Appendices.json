{"hands_on_practices": [{"introduction": "Once a screening tool is chosen, it is crucial to understand how it will perform in a specific clinical population. This exercise demonstrates how to use fundamental metrics like sensitivity ($Se$), specificity ($Sp$), and prevalence ($\\pi$) to calculate the Positive and Negative Predictive Values ($PPV$ and $NPV$). Understanding these values is essential for correctly interpreting a screening result and for practical resource planning, such as estimating the weekly referral load a clinic might expect. [@problem_id:4396192]", "problem": "A community health center integrates a standardized social risk screening for food insecurity into routine clinical care using a brief, validated tool administered through the Electronic Health Record (EHR). The tool’s performance in the clinic population is characterized by sensitivity $Se = 0.80$, specificity $Sp = 0.90$, and the condition’s prevalence $\\pi = 0.20$ estimated from a recent community health needs assessment. The clinic policy is to automatically generate a referral to food assistance resources for every positive screen, and the clinic sees $500$ unique adult patients per week.\n\nStarting from the definitions of sensitivity $Se = P(T^{+} \\mid D)$ and specificity $Sp = P(T^{-} \\mid \\bar{D})$, where $D$ denotes the presence of food insecurity and $T^{+}$ denotes a positive test (screen), use Bayes’ theorem and the law of total probability to derive expressions for the Positive Predictive Value (PPV) $P(D \\mid T^{+})$ and the Negative Predictive Value (NPV) $P(\\bar{D} \\mid T^{-})$ in terms of $Se$, $Sp$, and $\\pi$. Then, compute the numerical values of $PPV$ and $NPV$ for the given $Se$, $Sp$, and $\\pi$. Express $PPV$ and $NPV$ as decimals and round each to four significant figures.\n\nFinally, compute the expected weekly referral load, defined as the expected number of patients per week who will receive a referral under the “refer all positive screens” policy. Report this referral load as a count of patients per week. Provide your final answers in the order $PPV$, $NPV$, expected referrals.", "solution": "The problem statement is evaluated for validity prior to attempting a solution.\n\n### Step 1: Extract Givens\n- Sensitivity: $Se = P(T^{+} \\mid D) = 0.80$\n- Specificity: $Sp = P(T^{-} \\mid \\bar{D}) = 0.90$\n- Prevalence of food insecurity: $\\pi = P(D) = 0.20$\n- $D$: The event that a patient has food insecurity.\n- $\\bar{D}$: The event that a patient does not have food insecurity.\n- $T^{+}$: The event of a positive test result.\n- $T^{-}$: The event of a negative test result.\n- Total number of unique adult patients per week: $N = 500$\n- Policy: A referral is generated for every positive screen ($T^{+}$).\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It is based on fundamental principles of probability theory (Bayes' theorem, law of total probability) and standard epidemiological metrics (sensitivity, specificity, prevalence, predictive values). The problem is self-contained, with all necessary data and definitions provided. The numerical values ($Se=0.80$, $Sp=0.90$, $\\pi=0.20$) are realistic for a public health screening scenario. The problem asks for specific, formalizable derivations and calculations, and poses no logical contradictions or ambiguities.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Derivation of Predictive Values\n\nThe Positive Predictive Value ($PPV$) is the probability that a patient with a positive test result truly has the condition, defined as $P(D \\mid T^{+})$. Using Bayes' theorem:\n$$PPV = P(D \\mid T^{+}) = \\frac{P(T^{+} \\mid D) P(D)}{P(T^{+})}$$\nThe numerator contains the given terms: $P(T^{+} \\mid D) = Se$ and $P(D) = \\pi$.\nThe denominator, $P(T^{+})$, is the overall probability of a positive test. We expand this using the law of total probability:\n$$P(T^{+}) = P(T^{+} \\mid D) P(D) + P(T^{+} \\mid \\bar{D}) P(\\bar{D})$$\nWe can identify the components of this expansion:\n- $P(D) = \\pi$, so $P(\\bar{D}) = 1 - P(D) = 1 - \\pi$.\n- $P(T^{+} \\mid D) = Se$.\n- $P(T^{+} \\mid \\bar{D})$ is the false positive rate. It is the complement of the specificity, $Sp = P(T^{-} \\mid \\bar{D})$. Thus, $P(T^{+} \\mid \\bar{D}) = 1 - P(T^{-} \\mid \\bar{D}) = 1 - Sp$.\n\nSubstituting these into the expression for $P(T^{+})$:\n$$P(T^{+}) = (Se \\cdot \\pi) + ((1 - Sp) \\cdot (1 - \\pi))$$\nFinally, substituting this back into the Bayes' theorem expression for $PPV$ gives the general formula:\n$$PPV = \\frac{Se \\cdot \\pi}{Se \\cdot \\pi + (1 - Sp)(1 - \\pi)}$$\n\nThe Negative Predictive Value ($NPV$) is the probability that a patient with a negative test result is truly free of the condition, defined as $P(\\bar{D} \\mid T^{-})$. Using Bayes' theorem:\n$$NPV = P(\\bar{D} \\mid T^{-}) = \\frac{P(T^{-} \\mid \\bar{D}) P(\\bar{D})}{P(T^{-})}$$\nThe numerator contains the given terms: $P(T^{-} \\mid \\bar{D}) = Sp$ and $P(\\bar{D}) = 1 - \\pi$.\nThe denominator, $P(T^{-})$, is the overall probability of a negative test. We expand this using the law of total probability:\n$$P(T^{-}) = P(T^{-} \\mid D) P(D) + P(T^{-} \\mid \\bar{D}) P(\\bar{D})$$\nWe can identify the components of this expansion:\n- $P(D) = \\pi$ and $P(\\bar{D}) = 1 - \\pi$.\n- $P(T^{-} \\mid \\bar{D}) = Sp$.\n- $P(T^{-} \\mid D)$ is the false negative rate. It is the complement of the sensitivity, $Se = P(T^{+} \\mid D)$. Thus, $P(T^{-} \\mid D) = 1 - P(T^{+} \\mid D) = 1 - Se$.\n\nSubstituting these into the expression for $P(T^{-})$:\n$$P(T^{-}) = ((1 - Se) \\cdot \\pi) + (Sp \\cdot (1 - \\pi))$$\nFinally, substituting this back into the Bayes' theorem expression for $NPV$ gives the general formula:\n$$NPV = \\frac{Sp(1 - \\pi)}{(1 - Se)\\pi + Sp(1 - \\pi)}$$\n\n### Numerical Computation of PPV and NPV\nUsing the given values $Se = 0.80$, $Sp = 0.90$, and $\\pi = 0.20$:\n\nFor $PPV$:\n$$PPV = \\frac{0.80 \\cdot 0.20}{(0.80 \\cdot 0.20) + (1 - 0.90)(1 - 0.20)} = \\frac{0.16}{0.16 + (0.10)(0.80)} = \\frac{0.16}{0.16 + 0.08} = \\frac{0.16}{0.24} = \\frac{2}{3}$$\nAs a decimal rounded to four significant figures, $PPV \\approx 0.6667$.\n\nFor $NPV$:\n$$NPV = \\frac{0.90(1 - 0.20)}{(1 - 0.80)(0.20) + 0.90(1 - 0.20)} = \\frac{0.90 \\cdot 0.80}{(0.20)(0.20) + 0.90(0.80)} = \\frac{0.72}{0.04 + 0.72} = \\frac{0.72}{0.76} = \\frac{18}{19}$$\nAs a decimal rounded to four significant figures, $NPV \\approx 0.9474$.\n\n### Computation of Expected Weekly Referral Load\nThe clinic policy is to refer all patients who screen positive. The expected weekly referral load is the total number of patients per week, $N$, multiplied by the probability of a positive test, $P(T^{+})$.\nExpected Referrals $= N \\cdot P(T^{+})$\nWe have already derived and calculated $P(T^{+})$:\n$$P(T^{+}) = (Se \\cdot \\pi) + ((1 - Sp) \\cdot (1 - \\pi)) = 0.24$$\nWith $N = 500$ patients per week:\nExpected Referrals $= 500 \\cdot 0.24 = 120$\nThe expected weekly referral load is $120$ patients.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.6667 & 0.9474 & 120\n\\end{pmatrix}\n}\n$$", "id": "4396192"}, {"introduction": "Screening data is often used not just for individual patient care but also to estimate the overall prevalence of a social need within a larger population. However, the way a screening program is rolled out can introduce systematic errors that skew these estimates. This practice reveals how selection bias—for example, by screening frequent clinic attenders more often—can distort our view of population health and introduces a powerful statistical method, Inverse Probability Weighting (IPW), to generate a more accurate, corrected estimate. [@problem_id:4396213]", "problem": "A health system integrates social risk screening into routine care to identify patients experiencing food insecurity. Let $A \\in \\{L,M,H\\}$ denote clinic attendance category over the past year: low ($L$), medium ($M$), and high ($H$). Let $S \\in \\{0,1\\}$ indicate whether a patient was screened, and let $R \\in \\{0,1\\}$ indicate a positive screen for food insecurity. The patient panel distribution by attendance is $\\Pr(A=L)=0.5$, $\\Pr(A=M)=0.3$, and $\\Pr(A=H)=0.2$. The screening workflow is more likely to reach frequent attenders: $\\Pr(S=1\\mid A=L)=0.2$, $\\Pr(S=1\\mid A=M)=0.4$, and $\\Pr(S=1\\mid A=H)=0.8$. Among those screened, the observed stratum-specific risks are $\\Pr(R=1\\mid A=L, S=1)=0.15$, $\\Pr(R=1\\mid A=M, S=1)=0.25$, and $\\Pr(R=1\\mid A=H, S=1)=0.40$. Assume that within each attendance stratum the screening process is non-differential with respect to the outcome, that is, $\\Pr(R=1\\mid A=a, S=1)=\\Pr(R=1\\mid A=a)$ for each $a \\in \\{L,M,H\\}$, and that positivity holds (each stratum has a nonzero probability of screening).\n\nUsing only fundamental probability definitions and the law of total probability, do the following:\n\n1. Define the specific mechanism of selection bias that arises when estimating the overall prevalence $\\Pr(R=1)$ using only the screened sample (that is, using $\\Pr(R=1\\mid S=1)$) under the given workflow.\n\n2. Derive a strategy based on Inverse Probability Weighting (IPW) that corrects the estimate of $\\Pr(R=1)$ for this selection mechanism, and compute the corrected prevalence value.\n\nChoose the single option that both correctly defines the selection bias mechanism in this scenario and reports the correct IPW-adjusted prevalence estimate.\n\nA. Selection bias occurs because $\\Pr(S=1\\mid A)$ is higher among $H$ than $L$ and $M$, and clinic attendance $A$ is associated with food insecurity $R$; thus $\\Pr(R=1\\mid S=1)$ upweights high-attenders. Correct by weighting each screened individual by $1/\\Pr(S=1\\mid A)$ (Inverse Probability Weighting) to recover the population distribution of $A$. The corrected prevalence is $0.23$.\n\nB. Selection bias is random sampling error due to fewer low attenders being screened; the best correction is to use the unweighted screened sample proportion. The corrected prevalence is $0.2868$.\n\nC. Selection bias is due to differential misclassification among high attenders; correct by averaging the stratum-specific risks equally across $L$, $M$, and $H$. The corrected prevalence is $0.2667$.\n\nD. Selection bias arises from overrepresentation of high attenders; correct by weighting by $1/\\Pr(A)$ instead of $1/\\Pr(S=1\\mid A)$ to down-weight $H$. The corrected prevalence is $0.32$.", "solution": "This problem requires identifying the mechanism of selection bias and using Inverse Probability Weighting (IPW) to calculate a corrected population prevalence.\n\n### 1. Understanding Selection Bias\nThe goal is to estimate the true prevalence of food insecurity in the total population, $\\Pr(R=1)$. However, we only observe the outcome $R$ for the screened sub-population ($S=1$). A naive estimate using only the screened sample would be $\\Pr(R=1|S=1)$, the prevalence among those screened.\n\nSelection bias occurs if $\\Pr(R=1|S=1) \\neq \\Pr(R=1)$. This happens when the probability of being selected into the sample ($S=1$) is related to the outcome ($R=1$). In this scenario, clinic attendance $A$ is a common cause of both screening and food insecurity risk.\n-   **Association of $A$ with $S$**: Patients with high attendance are much more likely to be screened ($\\Pr(S=1|A=H) = 0.8$) than patients with low attendance ($\\Pr(S=1|A=L) = 0.2$).\n-   **Association of $A$ with $R$**: Patients with high attendance also have a higher risk of food insecurity ($\\Pr(R=1|A=H) = 0.40$) than patients with low attendance ($\\Pr(R=1|A=L) = 0.15$).\n\nBecause high-risk, high-attending patients are more likely to be screened, they will be over-represented in the screened sample. This will make the naive prevalence estimate, $\\Pr(R=1|S=1)$, an overestimate of the true population prevalence.\n\n### 2. Correction using IPW / Standardization\nTo find the true prevalence, we must calculate the weighted average of the stratum-specific risks, where the weights are the proportions of each stratum in the *total* population. This process is known as standardization and is equivalent to the Inverse Probability Weighting (IPW) approach.\n\nThe Law of Total Probability gives the true prevalence:\n$$ \\Pr(R=1) = \\sum_{a \\in \\{L,M,H\\}} \\Pr(R=1 | A=a) \\Pr(A=a) $$\nThe problem states we can assume $\\Pr(R=1|A=a) = \\Pr(R=1|A=a, S=1)$, so we can use the observed risks.\n$$ \\Pr(R=1) = \\Pr(R=1|A=L)\\Pr(A=L) + \\Pr(R=1|A=M)\\Pr(A=M) + \\Pr(R=1|A=H)\\Pr(A=H) $$\nPlugging in the values:\n$$ \\Pr(R=1) = (0.15)(0.5) + (0.25)(0.3) + (0.40)(0.2) $$\n$$ \\Pr(R=1) = 0.075 + 0.075 + 0.080 = 0.23 $$\nThe IPW-corrected prevalence is $0.23$.\n\n### 3. Evaluating the Options\n-   **Option A:** Correctly describes the selection bias mechanism (attendance $A$ is a common cause of selection $S$ and outcome $R$) and the result of upweighting high-attenders. It correctly identifies the IPW method with weights $1/\\Pr(S=1|A)$ as the solution. The calculated prevalence of $0.23$ is correct.\n-   **Option B:** Incorrectly calls selection bias \"random sampling error\" and proposes using the biased estimate as the correction. The value $0.2868$ is the biased, uncorrected prevalence in the screened sample.\n-   **Option C:** Incorrectly identifies the bias as \"differential misclassification\" and proposes an incorrect correction (unweighted average of risks).\n-   **Option D:** Correctly notes overrepresentation but proposes the wrong IPW weights ($1/\\Pr(A)$ instead of $1/\\Pr(S=1|A)$).\n\nTherefore, Option A is the only choice that is correct in its entirety.", "answer": "$$\\boxed{A}$$", "id": "4396213"}, {"introduction": "As health systems increasingly turn to automated, algorithm-based models for social risk screening, a new set of challenges arises. This practice introduces the critical concept of algorithmic bias, which refers to systematic patterns of error that can lead to inequitable performance or decisions across different patient groups. By analyzing realistic clinical scenarios, you will learn to identify key sources of bias—such as label bias, representation bias, and deployment bias—a vital skill for anyone involved in developing or implementing health technologies. [@problem_id:4396139]", "problem": "A health system plans to integrate a social risk screening model into routine clinical care to identify patients at risk of food insecurity and unstable housing. The model takes routinely collected data (demographics, prior visits, language preference, and previous social needs assessments) as input and outputs a predicted probability $\\hat{p}$ of social risk. Leadership asks the data science team to explain what counts as algorithmic bias and to classify the source of bias in several realistic situations.\n\nConsider the following situations in clinical implementation of social risk prediction:\n\nScenario One: To supervise the model during development, the team uses International Classification of Diseases codes and structured fields in the Electronic Health Record (EHR) as the target for the presence of social needs, even though many social needs are only documented in free-text notes or not recorded at all. Under-documentation is known to be more common for patients with limited access to care and for those speaking non-English languages.\n\nScenario Two: The training set comes entirely from one large urban academic medical center. In the region where the model will be used, half of patients receive care in community clinics that serve a higher proportion of uninsured and non-English-speaking patients than the academic center. The training cohort underrepresents non-English speakers and uninsured individuals relative to the deployment population.\n\nScenario Three: The model was tuned in the inpatient setting to maximize sensitivity at a fixed threshold and then deployed with the same decision threshold in primary care clinics. The prevalence of documented social needs is lower in primary care than inpatient care, and care teams in primary care have different workflows and resource constraints.\n\nWhich option both correctly defines algorithmic bias in social risk prediction and correctly classifies Scenario One, Scenario Two, and Scenario Three as label bias, representation bias, and deployment bias, respectively?\n\nA. Algorithmic bias is any random error in prediction performance. Scenario One is representation bias, Scenario Two is label bias, and Scenario Three is deployment bias.\n\nB. Algorithmic bias is a systematic pattern of error that yields inequitable model performance or decisions across groups. Scenario One is label bias, Scenario Two is representation bias, and Scenario Three is deployment bias.\n\nC. Algorithmic bias is intentional discrimination by developers baked into the code. Scenario One is deployment bias, Scenario Two is representation bias, and Scenario Three is label bias.\n\nD. Algorithmic bias is solely differences in model calibration across groups. Scenario One is label bias, Scenario Two is selection bias, and Scenario Three is deployment bias.\n\nE. Algorithmic bias is eliminated by retraining on more data. Scenario One is measurement bias, Scenario Two is representation bias, and Scenario Three is data leakage.", "solution": "The problem requires a critical evaluation of several scenarios involving a social risk prediction model in a clinical setting. The task is to identify the option that correctly defines algorithmic bias and correctly classifies the source of bias in each of the three given scenarios.\n\n### Derivation and Option Analysis\n\nFirst, let us establish the formal definitions of the key concepts from the field of machine learning fairness.\n\n-   **Algorithmic Bias:** This refers to systematic and repeatable errors in a computer system that create unfair outcomes, such as privileging one arbitrary group of users over others. It is not a random error but a persistent pattern of skewed results that can lead to inequitable or discriminatory decisions.\n-   **Label Bias (or Measurement Bias):** This type of bias occurs when the data used to label the ground truth is a flawed or biased proxy for the actual outcome of interest. If the quality or accuracy of the labels systematically differs across groups, the model will learn these systematic inaccuracies. In Scenario One, the \"label\" for social need is based on structured EHR fields. The problem states that this method of labeling systematically under-documents social needs for non-English speakers and those with limited care access. Therefore, the label itself is biased against these groups. This is a clear case of **label bias**.\n-   **Representation Bias (or Sampling Bias):** This bias arises when the training dataset does not accurately reflect the target population where the model will be deployed. If certain subgroups are under- or over-represented in the training data, the model may not generalize well to these subgroups, leading to poorer performance for them. In Scenario Two, the training data comes from an academic medical center, while the deployment population also includes community clinics with a different patient demographic (more uninsured, more non-English speaking). The training data \"underrepresents non-English speakers and uninsured individuals relative to the deployment population.\" This is the definition of **representation bias**.\n-   **Deployment Bias (or Implementation Bias):** This bias occurs when there is a mismatch between the context in which the model was developed and the context in which it is deployed. This can involve changes in population characteristics (population drift), differences in how the model's predictions are used, or shifts in the underlying data generating process. In Scenario Three, the model is developed and tuned for an inpatient setting (high prevalence of social need) but deployed in a primary care setting (low prevalence). The prevalence of the outcome is a critical parameter. For a given sensitivity and specificity, a change in prevalence dramatically alters the Positive Predictive Value ($PPV = \\frac{\\text{sensitivity} \\times \\text{prevalence}}{\\text{sensitivity} \\times \\text{prevalence} + (1 - \\text{specificity}) \\times (1 - \\text{prevalence})}$). Using a threshold optimized for a high-prevalence setting in a low-prevalence setting will likely lead to a very low PPV and a high number of false positives, overburdening the different workflows and resource constraints of the primary care teams. This mismatch between the development and deployment context is a textbook example of **deployment bias**.\n\nBased on this analysis, the correct classifications are:\n-   Scenario One: Label Bias\n-   Scenario Two: Representation Bias\n-   Scenario Three: Deployment Bias\n\nNow, we evaluate each option.\n\n**A. Algorithmic bias is any random error in prediction performance. Scenario One is representation bias, Scenario Two is label bias, and Scenario Three is deployment bias.**\n-   **Definition of Bias:** The statement that algorithmic bias is \"any random error\" is fundamentally **Incorrect**. Bias is systematic, not random. Random, irreducible error is a component of nearly all statistical models and is distinct from bias.\n-   **Classification:** The classification swaps the labels for Scenario One and Scenario Two. Scenario One is label bias, not representation bias. Scenario Two is representation bias, not label bias. This is **Incorrect**.\n-   **Verdict:** **Incorrect**.\n\n**B. Algorithmic bias is a systematic pattern of error that yields inequitable model performance or decisions across groups. Scenario One is label bias, Scenario Two is representation bias, and Scenario Three is deployment bias.**\n-   **Definition of Bias:** This definition is **Correct**. It accurately captures the essence of algorithmic bias as a systematic, not random, phenomenon that results in inequitable outcomes.\n-   **Classification:** The classifications for all three scenarios—label bias for One, representation bias for Two, and deployment bias for Three—are **Correct**, as determined by the principled analysis above.\n-   **Verdict:** **Correct**.\n\n**C. Algorithmic bias is intentional discrimination by developers baked into the code. Scenario One is deployment bias, Scenario Two is representation bias, and Scenario Three is label bias.**\n-   **Definition of Bias:** This definition is **Incorrect**. While intentional discrimination is a possible, and egregious, source of bias, the vast majority of algorithmic bias is unintentional, arising from flawed data, inappropriate model choices, or mismatches in deployment. Defining it as solely intentional is a severe mischaracterization.\n-   **Classification:** The classification misidentifies Scenario One as deployment bias and Scenario Three as label bias. This is **Incorrect**.\n-   **Verdict:** **Incorrect**.\n\n**D. Algorithmic bias is solely differences in model calibration across groups. Scenario One is label bias, Scenario Two is selection bias, and Scenario Three is deployment bias.**\n-   **Definition of Bias:** The definition that bias is \"solely differences in model calibration\" is **Incorrect**. Poor calibration is one important manifestation of bias, but not the only one. Bias can also appear as group-wise differences in accuracy, false positive rates, false negative rates, and other performance metrics, even in a well-calibrated model. The definition is too narrow.\n-   **Classification:** The classification is mostly correct. \"Selection bias\" is a term often used synonymously with representation bias. Scenario One and Three are classified correctly. However, the flawed definition of bias makes the entire option incorrect.\n-   **Verdict:** **Incorrect**.\n\n**E. Algorithmic bias is eliminated by retraining on more data. Scenario One is measurement bias, Scenario Two is representation bias, and Scenario Three is data leakage.**\n-   **Definition of Bias:** This is not a definition of bias but an overly simplistic and often incorrect statement about its mitigation. Simply adding \"more data\" of the same kind can amplify existing biases. While more *representative* data can address representation bias, it does not fix label bias or deployment bias. This statement is **Incorrect**.\n-   **Classification:** The classification for Scenario One as \"measurement bias\" (a synonym for label bias) and Scenario Two as \"representation bias\" is correct. However, the classification of Scenario Three as \"data leakage\" is **Incorrect**. Data leakage refers to the inappropriate use of information from outside the training data during model development, leading to inflated performance metrics. Scenario Three describes a mismatch between development and deployment environments, which is deployment bias, not data leakage.\n-   **Verdict:** **Incorrect**.\n\nOnly Option B provides both an accurate definition of algorithmic bias and the correct classification for all three scenarios.", "answer": "$$\\boxed{B}$$", "id": "4396139"}]}