{"hands_on_practices": [{"introduction": "When comparing health outcomes across different communities, raw rates can be misleading. A city with a large elderly population may naturally have higher rates of chronic disease than a younger city, even if its underlying health risks are lower. This exercise introduces direct age standardization, a fundamental epidemiological tool that adjusts for these demographic differences, allowing for a fair and accurate comparison of health metrics in a Community Health Needs Assessment (CHNA). [@problem_id:4364090]", "problem": "A Community Health Needs Assessment (CHNA) by a city health department seeks to compare the burden of asthma-related Emergency Department (ED) visits across communities while accounting for differences in age structure. To isolate underlying risk independent of age distribution, compute a directly age-standardized asthma ED visit rate for the city using the direct method with the 2000 United States standard population. Use the following information:\n\n- Age groups, city population denominators, and annual asthma ED visit counts:\n  - Ages $0$–$17$: population $= 52{,}000$, ED visits $= 780$.\n  - Ages $18$–$44$: population $= 95{,}000$, ED visits $= 600$.\n  - Ages $45$–$64$: population $= 61{,}000$, ED visits $= 500$.\n  - Ages $\\ge 65$: population $= 28{,}000$, ED visits $= 420$.\n\n- Use the 2000 United States standard population weights aggregated to the above age groups:\n  - Ages $0$–$17$: weight $w_1 = 0.26$.\n  - Ages $18$–$44$: weight $w_2 = 0.39$.\n  - Ages $45$–$64$: weight $w_3 = 0.23$.\n  - Ages $\\ge 65$: weight $w_4 = 0.12$.\n\nStarting from core definitions, compute the directly age-standardized annual rate of asthma ED visits per $10{,}000$ population. Then, under a large-sample Poisson assumption for ED visit counts and independence across age strata, construct a $95\\%$ confidence interval (CI) for the directly standardized rate.\n\nInstructions:\n- Express the final age-standardized rate and the lower and upper bounds of the $95\\%$ CI per $10{,}000$ population.\n- Round all three reported numbers to four significant figures.\n- Report your final result as three numbers in the order: standardized rate, lower bound, upper bound.", "solution": "The problem is assessed as valid for solution. It is scientifically grounded in standard epidemiological methods (direct age standardization), well-posed with all necessary information provided, and objective in its formulation. The provided standard population weights sum to $1$ ($0.26 + 0.39 + 0.23 + 0.12 = 1.00$), and all data are consistent and realistic. The requested calculations are standard procedures in biostatistics.\n\nLet the index $i$ represent the age stratum, where $i \\in \\{1, 2, 3, 4\\}$.\nThe given data are:\n- For stratum $i=1$ (Ages $0$–$17$): Count $C_1 = 780$, Population $N_1 = 52,000$, Standard Weight $w_1 = 0.26$.\n- For stratum $i=2$ (Ages $18$–$44$): Count $C_2 = 600$, Population $N_2 = 95,000$, Standard Weight $w_2 = 0.39$.\n- For stratum $i=3$ (Ages $45$–$64$): Count $C_3 = 500$, Population $N_3 = 61,000$, Standard Weight $w_3 = 0.23$.\n- For stratum $i=4$ (Ages $\\ge 65$): Count $C_4 = 420$, Population $N_4 = 28,000$, Standard Weight $w_4 = 0.12$.\n\nThe task is to compute the directly age-standardized rate (DASR) and its $95\\%$ confidence interval (CI).\n\nFirst, we calculate the age-specific rate, $r_i$, for each stratum $i$. The age-specific rate is the number of events (ED visits) divided by the population size in that stratum.\n$$r_i = \\frac{C_i}{N_i}$$\nThe rates for the four strata are:\n$$r_1 = \\frac{780}{52,000} = 0.015$$\n$$r_2 = \\frac{600}{95,000} \\approx 0.00631579$$\n$$r_3 = \\frac{500}{61,000} \\approx 0.00819672$$\n$$r_4 = \\frac{420}{28,000} = 0.015$$\n\nThe directly age-standardized rate is the weighted average of the age-specific rates, where the weights are from the standard population. The formula for the DASR is:\n$$\\text{DASR} = \\sum_{i=1}^{4} w_i r_i$$\nSubstituting the values:\n$$\\text{DASR} = (0.26)(0.015) + (0.39)\\left(\\frac{600}{95,000}\\right) + (0.23)\\left(\\frac{500}{61,000}\\right) + (0.12)(0.015)$$\n$$\\text{DASR} \\approx (0.26)(0.015) + (0.39)(0.00631579) + (0.23)(0.00819672) + (0.12)(0.015)$$\n$$\\text{DASR} \\approx 0.0039 + 0.00246316 + 0.00188525 + 0.0018$$\n$$\\text{DASR} \\approx 0.01004841$$\nTo express this rate per $10,000$ population, we multiply by $10,000$:\n$$\\text{DASR}_{\\text{per } 10,000} = 0.01004841 \\times 10,000 = 100.4841$$\n\nNext, we construct the $95\\%$ confidence interval for the DASR. Under the assumption that the event counts $C_i$ follow a Poisson distribution, the variance of the age-specific rate $r_i$ is estimated as:\n$$\\text{Var}(r_i) \\approx \\frac{C_i}{N_i^2}$$\nSince the age strata are independent, the variance of the DASR, which is a linear combination of the $r_i$, is the sum of the weighted variances:\n$$\\text{Var}(\\text{DASR}) = \\sum_{i=1}^{4} w_i^2 \\text{Var}(r_i) \\approx \\sum_{i=1}^{4} \\frac{w_i^2 C_i}{N_i^2}$$\nWe calculate this sum term by term:\n$$\\text{Var}(\\text{DASR}) \\approx \\frac{0.26^2 \\times 780}{52,000^2} + \\frac{0.39^2 \\times 600}{95,000^2} + \\frac{0.23^2 \\times 500}{61,000^2} + \\frac{0.12^2 \\times 420}{28,000^2}$$\n$$\\text{Var}(\\text{DASR}) \\approx \\frac{52.728}{2,704,000,000} + \\frac{91.26}{9,025,000,000} + \\frac{26.45}{3,721,000,000} + \\frac{6.048}{784,000,000}$$\n$$\\text{Var}(\\text{DASR}) \\approx 1.9485 \\times 10^{-8} + 1.0112 \\times 10^{-8} + 0.7108 \\times 10^{-8} + 0.7714 \\times 10^{-8}$$\n$$\\text{Var}(\\text{DASR}) \\approx 4.4419 \\times 10^{-8}$$\nThe standard error (SE) of the DASR is the square root of its variance:\n$$\\text{SE}(\\text{DASR}) = \\sqrt{\\text{Var}(\\text{DASR})} \\approx \\sqrt{4.4419 \\times 10^{-8}} \\approx 2.1076 \\times 10^{-4}$$\n\nFor a large sample, the $95\\%$ CI is constructed using the normal approximation:\n$$\\text{CI} = \\text{DASR} \\pm Z_{1-\\alpha/2} \\times \\text{SE}(\\text{DASR})$$\nFor a $95\\%$ CI, $\\alpha=0.05$, and the critical value is $Z_{0.975} = 1.96$.\nThe margin of error (ME) is:\n$$\\text{ME} = 1.96 \\times \\text{SE}(\\text{DASR}) \\approx 1.96 \\times 2.1076 \\times 10^{-4} \\approx 4.1309 \\times 10^{-4}$$\nThe lower and upper bounds of the CI for the DASR are:\n$$\\text{Lower Bound} = \\text{DASR} - \\text{ME} \\approx 0.01004841 - 0.00041309 = 0.00963532$$\n$$\\text{Upper Bound} = \\text{DASR} + \\text{ME} \\approx 0.01004841 + 0.00041309 = 0.01046150$$\n\nTo express these bounds per $10,000$ population, we multiply by $10,000$:\n$$\\text{Lower Bound}_{\\text{per } 10,000} = 0.00963532 \\times 10,000 = 96.3532$$\n$$\\text{Upper Bound}_{\\text{per } 10,000} = 0.01046150 \\times 10,000 = 104.6150$$\n\nFinally, we round the standardized rate and the CI bounds to four significant figures as instructed:\n- Standardized Rate: $100.4841 \\to 100.5$\n- Lower Bound: $96.3532 \\to 96.35$\n- Upper Bound: $104.6150 \\to 104.6$\n\nThe three requested numbers are the standardized rate, the lower bound, and the upper bound, all per $10,000$ population.", "answer": "$$\\boxed{\\begin{pmatrix} 100.5  96.35  104.6 \\end{pmatrix}}$$", "id": "4364090"}, {"introduction": "Even after standardizing rates, a critical analyst must question the data's integrity. A common pitfall in health analysis is the numerator-denominator mismatch, where the group counted in the numerator (e.g., patients visiting a hospital) does not perfectly align with the population in the denominator (e.g., residents of the hospital's neighborhood). This thought experiment demonstrates how this misalignment can lead to a confounding effect known as Simpson's paradox, where an observed trend is reversed when data is properly stratified, highlighting the importance of using carefully defined populations in a CHNA. [@problem_id:4364116]", "problem": "A county health department is conducting a Community Health Needs Assessment focused on Emergency Department (ED) use. Two neighborhoods, Area $A$ and Area $B$, each have two age strata: young ($Y$) and older ($O$). The fundamental definitions to be used are: (i) an incidence rate equals events divided by the appropriate population at risk, and (ii) when aggregating across strata, the overall rate equals a weighted average of stratum-specific rates, with weights proportional to stratum populations if numerators and denominators are aligned. \n\nAssume the following scientifically plausible resident populations and true stratum-specific ED visit rates per resident-year by place-of-residence:\n- Area $A$ resident populations: $N_{A,Y} = 9000$, $N_{A,O} = 1000$.\n- Area $B$ resident populations: $N_{B,Y} = 1000$, $N_{B,O} = 9000$.\n- True stratum-specific rates (per resident-year): $r_{A,Y} = 0.01$, $r_{A,O} = 0.08$, $r_{B,Y} = 0.02$, $r_{B,O} = 0.09$.\n\nSuppose Area $A$ hosts the regional ED. Because of commuting and service-seeking patterns, a fraction $k$ of all ED visits generated by Area $B$ residents occur in Area $A$’s ED (place-of-service), while Area $A$ residents’ ED visits all occur in Area $A$’s ED. The health department mistakenly computes each area’s “community ED visit rate” by dividing the total number of ED visits occurring in EDs located in that area (place-of-service) by the resident population of that area (place-of-residence). This misaligns numerators and denominators.\n\nUse only the core definitions stated above to:\n- Verify that within each age stratum, Area $A$ has a lower true resident rate than Area $B$.\n- Derive the misaligned, commuting-affected overall observed rates for Area $A$ and Area $B$ as functions of $k$, then solve for the smallest $k \\in [0,1]$ at which the misaligned overall rate in Area $A$ equals that in Area $B$. Provide your answer as a simplified fraction (no units). This value is the threshold at which a reversal of the disparity occurs if $k$ increases beyond it, illustrating Simpson’s paradox driven by misaligned denominators.\n\nExpress your final answer as a single simplified fraction. No rounding is required.", "solution": "Start from the core definition that an incidence rate equals events divided by the appropriate population at risk, and that when aggregating across strata with aligned numerators and denominators, the overall rate is a weighted average of stratum-specific rates.\n\nStep $1$: Verify stratum-specific disparities by place-of-residence. The given stratum-specific rates per resident-year are:\n- Young: $r_{A,Y} = 0.01$ and $r_{B,Y} = 0.02$, so $r_{A,Y}  r_{B,Y}$.\n- Older: $r_{A,O} = 0.08$ and $r_{B,O} = 0.09$, so $r_{A,O}  r_{B,O}$.\nThus, within each stratum, Area $A$ has a lower true resident rate than Area $B$.\n\nStep $2$: Compute the expected number of ED visits per year by place-of-residence in each area and stratum, using $E_{j,s}^{\\text{res}} = N_{j,s} \\times r_{j,s}$ for area $j \\in \\{A,B\\}$ and stratum $s \\in \\{Y,O\\}$.\n- Area $A$ residents:\n  - Young: $E_{A,Y}^{\\text{res}} = 9000 \\times 0.01 = 90$.\n  - Older: $E_{A,O}^{\\text{res}} = 1000 \\times 0.08 = 80$.\n  - Total: $E_{A,\\cdot}^{\\text{res}} = 90 + 80 = 170$.\n- Area $B$ residents:\n  - Young: $E_{B,Y}^{\\text{res}} = 1000 \\times 0.02 = 20$.\n  - Older: $E_{B,O}^{\\text{res}} = 9000 \\times 0.09 = 810$.\n  - Total: $E_{B,\\cdot}^{\\text{res}} = 20 + 810 = 830$.\n\nTotal resident populations are $N_{A,\\cdot} = N_{A,Y} + N_{A,O} = 9000 + 1000 = 10000$ and $N_{B,\\cdot} = N_{B,Y} + N_{B,O} = 1000 + 9000 = 10000$.\n\nIf one were to compute aligned overall resident rates, they would be $170/10000 = 0.017$ for Area $A$ and $830/10000 = 0.083$ for Area $B$, confirming Area $A$ is lower overall when denominators match the numerators’ population of origin.\n\nStep $3$: Incorporate commuting and place-of-service misalignment. By the scenario’s assumption, a fraction $k$ of all ED visits generated by Area $B$ residents occur in Area $A$’s ED. All Area $A$ residents’ visits occur in Area $A$’s ED. Therefore, the place-of-service annual visit counts are:\n- Area $A$ EDs: $E_{A,\\cdot}^{\\text{svc}}(k) = E_{A,\\cdot}^{\\text{res}} + k \\, E_{B,\\cdot}^{\\text{res}} = 170 + 830\\,k$.\n- Area $B$ EDs: $E_{B,\\cdot}^{\\text{svc}}(k) = (1-k)\\, E_{B,\\cdot}^{\\text{res}} = (1-k)\\,830$.\n\nStep $4$: Apply the health department’s misaligned rate calculation, which divides place-of-service events by place-of-residence population. The misaligned overall observed rates are:\n- Area $A$: $R_{A}^{\\text{obs}}(k) = \\dfrac{E_{A,\\cdot}^{\\text{svc}}(k)}{N_{A,\\cdot}} = \\dfrac{170 + 830\\,k}{10000}$.\n- Area $B$: $R_{B}^{\\text{obs}}(k) = \\dfrac{E_{B,\\cdot}^{\\text{svc}}(k)}{N_{B,\\cdot}} = \\dfrac{830\\,(1-k)}{10000}$.\n\nStep $5$: Solve for the threshold $k$ at which $R_{A}^{\\text{obs}}(k) = R_{B}^{\\text{obs}}(k)$. Since $N_{A,\\cdot} = N_{B,\\cdot} = 10000$, this reduces to equality of numerators:\n$$170 + 830\\,k = 830\\,(1-k).$$\nRearrange and solve:\n$$170 + 830\\,k = 830 - 830\\,k,$$\n$$830\\,k + 830\\,k = 830 - 170,$$\n$$1660\\,k = 660,$$\n$$k = \\dfrac{660}{1660} = \\dfrac{33}{83}.$$\n\nThis $k$ is the smallest value in $[0,1]$ at which the misaligned overall rates are equal; for any $k  \\dfrac{33}{83}$, one has $R_{A}^{\\text{obs}}(k)  R_{B}^{\\text{obs}}(k)$ even though $r_{A,Y}  r_{B,Y}$ and $r_{A,O}  r_{B,O}$. This demonstrates a reversal of the disparity (Simpson’s paradox) induced solely by using misaligned denominators under commuting.", "answer": "$$\\boxed{\\frac{33}{83}}$$", "id": "4364116"}, {"introduction": "The ultimate goal of a CHNA is to guide action, but with multiple identified needs and limited resources, prioritization is essential. This practice introduces Multi-Criteria Decision Analysis (MCDA), a structured and transparent method for making these tough choices. By implementing an MCDA model, you will learn to synthesize diverse criteria—such as a problem's magnitude, its severity, and community preferences—into a single, defensible priority score, translating complex data into actionable strategy. [@problem_id:4364052]", "problem": "A health system must prioritize community health needs using a multi-criteria decision analysis framework. Each health need is characterized by five criteria: magnitude, severity, disparity, feasibility, and community preference. The fundamental base for decision-making is a value function model rooted in utility theory: when attributes are mutually preferentially independent, an additive multi-attribute value function exists that rationally aggregates criterion-specific values.\n\nDefinitions and assumptions for the model are as follows:\n- Magnitude ($x_{\\text{mag}}$): the number of persons affected, measured in persons.\n- Severity ($x_{\\text{sev}}$): an expert rating on a bounded scale from $0$ to $10$.\n- Disparity ($x_{\\text{disp}}$): a dimensionless ratio of the rate in a target subpopulation to the rate in a reference population, with $x_{\\text{disp}} \\ge 1$.\n- Feasibility ($x_{\\text{feas}}$): an estimated probability of successful implementation in $[0,1]$, expressed as a decimal.\n- Community preference ($x_{\\text{pref}}$): the fraction of surveyed respondents who favor prioritization, in $[0,1]$, expressed as a decimal.\n\nLet $v_j(\\cdot)$ denote the criterion-specific value transformation that maps raw measurements to a commensurate scale in $[0,1]$ with monotone increasing preference (larger is better). The overall value (priority score) for need $i$ is modeled as\n$$\nU_i = \\sum_{j=1}^{5} w_j \\, v_j\\!\\left(x_{ij}\\right),\n$$\nwhere $w_j \\ge 0$ are criterion weights satisfying $\\sum_{j=1}^{5} w_j = 1$.\n\nYou must construct the following normalization scheme for commensurability:\n- For magnitude, use a logarithmically tempered min-max transform to address heavy-tailed counts:\n$$\nv_{\\text{mag}}(x) = \n\\begin{cases}\n\\dfrac{\\log(1+x) - \\log(1+x_{\\min})}{\\log(1+x_{\\max}) - \\log(1+x_{\\min})},  \\text{if } \\log(1+x_{\\max}) \\neq \\log(1+x_{\\min}),\\\\[8pt]\n0.5,  \\text{otherwise,}\n\\end{cases}\n$$\nwhere $x_{\\min}$ and $x_{\\max}$ are the minimum and maximum of the observed magnitude values in a given test case.\n- For severity, use linear scaling on its known bounds:\n$$\nv_{\\text{sev}}(s) = \\dfrac{s}{10}.\n$$\n- For disparity, use a logarithmically tempered min-max transform to account for multiplicative differences:\n$$\nv_{\\text{disp}}(r) = \n\\begin{cases}\n\\dfrac{\\log(r) - \\log(r_{\\min})}{\\log(r_{\\max}) - \\log(r_{\\min})},  \\text{if } \\log(r_{\\max}) \\neq \\log(r_{\\min}),\\\\[8pt]\n0.5,  \\text{otherwise,}\n\\end{cases}\n$$\nwhere $r_{\\min}$ and $r_{\\max}$ are the minimum and maximum of the observed disparity ratios in a given test case.\n- For feasibility, use identity on $[0,1]$:\n$$\nv_{\\text{feas}}(f) = f.\n$$\n- For community preference, use identity on $[0,1]$:\n$$\nv_{\\text{pref}}(p) = p.\n$$\n\nWhen any min-max denominator is zero (all observed values are equal for that criterion within a test case), define the normalized value as $0.5$ for all alternatives on that criterion. Use $0$-based indexing for needs, and break ties by choosing the smallest index.\n\nYour task is to implement this normalization scheme and compute $U_i$ for each need across the following test suite. For each test case, output the index of the single highest-priority need.\n\nTest suite:\n- Test case $1$ (weights $w = [0.3, 0.25, 0.2, 0.15, 0.1]$):\n  - Magnitude (persons): $[8500, 12000, 3000, 9500]$\n  - Severity ($0$–$10$): $[7.5, 6.0, 5.5, 8.0]$\n  - Disparity ratio ($\\ge 1$): $[1.8, 1.2, 2.5, 1.4]$\n  - Feasibility (decimal): $[0.7, 0.9, 0.6, 0.5]$\n  - Preference (decimal): $[0.65, 0.55, 0.80, 0.60]$\n- Test case $2$ (weights $w = [0.25, 0.20, 0.30, 0.15, 0.10]$):\n  - Magnitude (persons): $[50000, 900, 2500, 4000]$\n  - Severity ($0$–$10$): $[9.0, 4.0, 6.5, 7.0]$\n  - Disparity ratio ($\\ge 1$): $[1.1, 2.0, 3.2, 1.5]$\n  - Feasibility (decimal): $[0.4, 0.8, 0.6, 0.7]$\n  - Preference (decimal): $[0.50, 0.70, 0.60, 0.55]$\n- Test case $3$ (weights $w = [0.2, 0.2, 0.2, 0.2, 0.2]$):\n  - Magnitude (persons): $[2000, 2000, 2000, 2000]$\n  - Severity ($0$–$10$): $[5.0, 5.0, 5.0, 5.0]$\n  - Disparity ratio ($\\ge 1$): $[1.0, 1.0, 1.0, 1.0]$\n  - Feasibility (decimal): $[0.2, 0.4, 0.6, 0.8]$\n  - Preference (decimal): $[0.1, 0.2, 0.3, 0.4]$\n- Test case $4$ (weights $w = [0.35, 0.25, 0.20, 0.10, 0.10]$):\n  - Magnitude (persons): $[0, 750, 1500]$\n  - Severity ($0$–$10$): $[8.5, 7.0, 3.0]$\n  - Disparity ratio ($\\ge 1$): $[1.7, 1.3, 2.1]$\n  - Feasibility (decimal): $[0.3, 0.9, 0.2]$\n  - Preference (decimal): $[0.0, 0.6, 0.4]$\n\nYour program must compute the normalized criterion values, aggregate them via the weighted sum to obtain $U_i$ for each need in each test case, and then report the index of the highest $U_i$ per test case. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[0,2,1,3]$). No user input is required.", "solution": "The problem presents a multi-criteria decision analysis (MCDA) framework for prioritizing community health needs. The method is based on an additive multi-attribute value function, a standard technique in decision theory when the assumption of mutual preferential independence among criteria holds. Our task is to implement this model, calculate the priority score for each health need within several test cases, and identify the need with the highest score in each case.\n\nThe overall value, or priority score $U_i$, for a given health need $i$ is calculated as a weighted sum of its performance on five criteria:\n$$\nU_i = \\sum_{j=1}^{5} w_j \\, v_j\\!\\left(x_{ij}\\right)\n$$\nHere, $x_{ij}$ is the raw score of need $i$ on criterion $j$, $v_j(\\cdot)$ is a criterion-specific value function that normalizes the raw score to a common scale of $[0, 1]$, and $w_j$ is the weight of criterion $j$, with $w_j \\ge 0$ and $\\sum_j w_j = 1$. The criteria are magnitude ($x_{\\text{mag}}$), severity ($x_{\\text{sev}}$), disparity ($x_{\\text{disp}}$), feasibility ($x_{\\text{feas}}$), and community preference ($x_{\\text{pref}}$).\n\nThe process involves three main steps for each test case:\n1.  Apply the specified value functions $v_j(\\cdot)$ to normalize the raw data for each criterion across all competing health needs.\n2.  Aggregate the normalized values for each need using the provided weights to compute the total priority score $U_i$.\n3.  Identify the index of the health need with the maximum score $U_i$, using the smallest index as a tie-breaker.\n\nThe value functions $v_j(\\cdot)$ are defined as follows:\n\n**1. Magnitude ($v_{\\text{mag}}$):**\nFor the number of persons affected, $x$, a logarithmically tempered min-max normalization is employed. The transformation $x \\to \\log(1+x)$ is used to handle potentially heavy-tailed distributions of counts and to ensure the logarithm is defined for $x=0$.\nThe value function is:\n$$\nv_{\\text{mag}}(x) = \\frac{\\log(1+x) - \\log(1+x_{\\min})}{\\log(1+x_{\\max}) - \\log(1+x_{\\min})}\n$$\nwhere $x_{\\min}$ and $x_{\\max}$ are the minimum and maximum observed magnitude values within the set of needs being compared. If all needs have the same magnitude ($x_{\\min} = x_{\\max}$), the denominator becomes zero. In this case, the normalized value is defined as $0.5$ for all needs, representing a neutral or average contribution from this criterion.\n\n**2. Severity ($v_{\\text{sev}}$):**\nSeverity, $s$, is an expert rating on a fixed scale from $0$ to $10$. It is normalized using linear scaling with respect to its theoretical bounds:\n$$\nv_{\\text{sev}}(s) = \\frac{s}{10}\n$$\nThis transforms the score to the required $[0, 1]$ interval.\n\n**3. Disparity ($v_{\\text{disp}}$):**\nDisparity, $r$, is a ratio where multiplicative differences are more meaningful than additive ones. Thus, a logarithmic min-max normalization is used:\n$$\nv_{\\text{disp}}(r) = \\frac{\\log(r) - \\log(r_{\\min})}{\\log(r_{\\max}) - \\log(r_{\\min})}\n$$\nwhere $r_{\\min}$ and $r_{\\max}$ are the minimum and maximum observed disparity ratios. Since $r \\ge 1$, $\\log(r)$ is well-defined and non-negative. Similar to magnitude, if all disparity values are equal ($r_{\\min} = r_{\\max}$), the normalized value is set to $0.5$.\n\n**4. Feasibility ($v_{\\text{feas}}$) and Community Preference ($v_{\\text{pref}}$):**\nBoth feasibility, $f$, and community preference, $p$, are given as values in the interval $[0,1]$ (a probability and a fraction, respectively). As they are already on the desired common scale, the identity function is used for their value transformation:\n$$\nv_{\\text{feas}}(f) = f \\\\\nv_{\\text{pref}}(p) = p\n$$\n\n**Computational Procedure:**\nFor each test case, we will perform the following computations using vectorized operations for efficiency.\nLet the raw data for $N$ needs and $5$ criteria be represented by an $N \\times 5$ matrix of raw scores. We will generate an $N \\times 5$ matrix of normalized values, $V$, where each element $V_{ij} = v_j(x_{ij})$.\n1.  For the `magnitude` column, determine $x_{\\min}$ and $x_{\\max}$. Apply the $v_{\\text{mag}}$ formula to obtain the first column of $V$.\n2.  For the `severity` column, apply the $v_{\\text{sev}}$ formula to obtain the second column of $V$.\n3.  For the `disparity` column, determine $r_{\\min}$ and $r_{\\max}$. Apply the $v_{\\text{disp}}$ formula to obtain the third column of $V$.\n4.  The `feasibility` and `preference` columns directly form the fourth and fifth columns of $V$.\n5.  The priority scores for all needs are then computed in a single operation by taking the matrix-vector product of the normalized value matrix $V$ and the weight vector $w$:\n    $$\n    \\mathbf{U} = V \\mathbf{w}\n    $$\n    where $\\mathbf{U}$ is an $N \\times 1$ vector of priority scores.\n6.  Finally, we find the $0$-based index of the maximum value in the vector $\\mathbf{U}$. The `argmax` function, which returns the index of the first occurrence of the maximum value, naturally handles the specified tie-breaking rule. This process is repeated for each test case provided in the problem statement.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the multi-criteria decision analysis model to prioritize\n    community health needs based on the specified value functions and weights.\n    \"\"\"\n    test_cases = [\n        {\n            \"weights\": [0.3, 0.25, 0.2, 0.15, 0.1],\n            \"data\": {\n                \"mag\": [8500, 12000, 3000, 9500],\n                \"sev\": [7.5, 6.0, 5.5, 8.0],\n                \"disp\": [1.8, 1.2, 2.5, 1.4],\n                \"feas\": [0.7, 0.9, 0.6, 0.5],\n                \"pref\": [0.65, 0.55, 0.80, 0.60],\n            }\n        },\n        {\n            \"weights\": [0.25, 0.20, 0.30, 0.15, 0.10],\n            \"data\": {\n                \"mag\": [50000, 900, 2500, 4000],\n                \"sev\": [9.0, 4.0, 6.5, 7.0],\n                \"disp\": [1.1, 2.0, 3.2, 1.5],\n                \"feas\": [0.4, 0.8, 0.6, 0.7],\n                \"pref\": [0.50, 0.70, 0.60, 0.55],\n            }\n        },\n        {\n            \"weights\": [0.2, 0.2, 0.2, 0.2, 0.2],\n            \"data\": {\n                \"mag\": [2000, 2000, 2000, 2000],\n                \"sev\": [5.0, 5.0, 5.0, 5.0],\n                \"disp\": [1.0, 1.0, 1.0, 1.0],\n                \"feas\": [0.2, 0.4, 0.6, 0.8],\n                \"pref\": [0.1, 0.2, 0.3, 0.4],\n            }\n        },\n        {\n            \"weights\": [0.35, 0.25, 0.20, 0.10, 0.10],\n            \"data\": {\n                \"mag\": [0, 750, 1500],\n                \"sev\": [8.5, 7.0, 3.0],\n                \"disp\": [1.7, 1.3, 2.1],\n                \"feas\": [0.3, 0.9, 0.2],\n                \"pref\": [0.0, 0.6, 0.4],\n            }\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        weights = np.array(case[\"weights\"])\n        data = case[\"data\"]\n        \n        mag = np.array(data[\"mag\"], dtype=float)\n        sev = np.array(data[\"sev\"], dtype=float)\n        disp = np.array(data[\"disp\"], dtype=float)\n        feas = np.array(data[\"feas\"], dtype=float)\n        pref = np.array(data[\"pref\"], dtype=float)\n\n        num_needs = len(mag)\n        \n        # 1. Normalize Magnitude\n        mag_min, mag_max = np.min(mag), np.max(mag)\n        if np.isclose(mag_min, mag_max):\n            v_mag = np.full(num_needs, 0.5)\n        else:\n            log_mag = np.log(1 + mag)\n            log_min = np.log(1 + mag_min)\n            log_max = np.log(1 + mag_max)\n            v_mag = (log_mag - log_min) / (log_max - log_min)\n\n        # 2. Normalize Severity\n        v_sev = sev / 10.0\n        \n        # 3. Normalize Disparity\n        disp_min, disp_max = np.min(disp), np.max(disp)\n        if np.isclose(disp_min, disp_max):\n            v_disp = np.full(num_needs, 0.5)\n        else:\n            log_disp = np.log(disp)\n            log_min = np.log(disp_min)\n            log_max = np.log(disp_max)\n            v_disp = (log_disp - log_min) / (log_max - log_min)\n\n        # 4. Feasibility (Identity)\n        v_feas = feas\n\n        # 5. Preference (Identity)\n        v_pref = pref\n\n        # Assemble the normalized value matrix V\n        V = np.stack([v_mag, v_sev, v_disp, v_feas, v_pref], axis=1)\n\n        # Calculate priority scores U\n        U = V @ weights\n\n        # Find the index of the highest score\n        highest_priority_idx = np.argmax(U)\n        results.append(highest_priority_idx)\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4364052"}]}