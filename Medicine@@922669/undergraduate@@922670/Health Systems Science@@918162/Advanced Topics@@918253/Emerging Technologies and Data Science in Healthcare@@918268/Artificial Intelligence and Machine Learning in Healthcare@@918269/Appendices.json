{"hands_on_practices": [{"introduction": "An AI model's performance is often described by its sensitivity and specificity, but these metrics don't tell the whole story. This exercise guides you through the fundamental application of Bayes' theorem to derive the Positive Predictive Value ($PPV$) and Negative Predictive Value ($NPV$), which are crucial for understanding the real-world utility of a diagnostic test. By exploring how $PPV$ changes with disease prevalence, you will gain a deeper appreciation for why a test's effectiveness can vary dramatically across different patient populations. [@problem_id:4360373]", "problem": "A hospital system deploys a binary Artificial Intelligence (AI) screening tool built using Machine Learning (ML) to flag a condition in primary care. Let $D$ denote the event that a randomly selected patient truly has the condition and $\\bar{D}$ the event that the patient does not have the condition. The tool outputs a binary label, with $+$ for a positive screen and $-$ for a negative screen. The population disease prevalence is $\\pi = P(D)$, the tool's sensitivity is $t = P(+ \\mid D)$, and its specificity is $c = P(- \\mid \\bar{D})$. Assume $t, c \\in (0,1)$ and $\\pi \\in (0,1)$.\n\nUsing only core definitions from diagnostic testing and the probability axioms (in particular the Law of Total Probability and Bayes' theorem), derive closed-form expressions for the Positive Predictive Value (PPV), $PPV = P(D \\mid +)$, and the Negative Predictive Value (NPV), $NPV = P(\\bar{D} \\mid -)$, in terms of $\\pi$, $t$, and $c$. Then, for fixed $t$ and $c$, compute the derivative $\\frac{d}{d\\pi}PPV(\\pi)$ with respect to $\\pi$ and simplify it to a single closed-form expression that depends only on $\\pi$, $t$, and $c$.\n\nExpress your final results for $PPV$, $NPV$, and $\\frac{d}{d\\pi}PPV(\\pi)$ as closed-form analytic expressions. No rounding is required.", "solution": "The problem is well-posed, scientifically grounded, objective, and contains all necessary information for a unique solution. The concepts of prevalence, sensitivity, and specificity are cornerstone principles in biostatistics and epidemiology for evaluating diagnostic tests. The derivation of Positive Predictive Value (PPV) and Negative Predictive Value (NPV) is a standard application of Bayes' theorem and the Law of Total Probability. I will proceed with the derivation.\n\nThe problem provides the following definitions:\nThe event that a patient has the condition is denoted by $D$.\nThe event that a patient does not have the condition is denoted by $\\bar{D}$.\nThe event of a positive test result is denoted by $+$.\nThe event of a negative test result is denoted by $-$.\n\nThe given probabilities are:\nPrevalence: $\\pi = P(D) \\in (0,1)$.\nSensitivity: $t = P(+ \\mid D) \\in (0,1)$.\nSpecificity: $c = P(- \\mid \\bar{D}) \\in (0,1)$.\n\nFrom the axioms of probability, we can derive the probabilities of complementary events:\nThe probability of a patient not having the condition is $P(\\bar{D}) = 1 - P(D) = 1 - \\pi$.\nThe false positive rate is the probability of a positive test given the patient does not have the condition: $P(+ \\mid \\bar{D}) = 1 - P(- \\mid \\bar{D}) = 1 - c$.\nThe false negative rate is the probability of a negative test given the patient does have the condition: $P(- \\mid D) = 1 - P(+ \\mid D) = 1 - t$.\n\n**Part 1: Derivation of the Positive Predictive Value (PPV)**\nThe PPV is defined as the probability that a patient has the condition given a positive test result, $PPV = P(D \\mid +)$.\nUsing Bayes' theorem, we have:\n$$PPV = P(D \\mid +) = \\frac{P(+ \\mid D) P(D)}{P(+)}$$\nThe numerator is given by the product of the sensitivity and the prevalence:\n$$P(+ \\mid D) P(D) = t \\pi$$\nThe denominator, $P(+)$, is the total probability of a positive test. We find this using the Law of Total Probability, summing over the mutually exclusive events $D$ and $\\bar{D}$:\n$$P(+) = P(+ \\mid D) P(D) + P(+ \\mid \\bar{D}) P(\\bar{D})$$\nSubstituting the known quantities:\n$$P(+) = t \\pi + (1 - c) (1 - \\pi)$$\nNow, we substitute the expressions for the numerator and denominator back into the Bayes' theorem formula for PPV:\n$$PPV = \\frac{t \\pi}{t \\pi + (1 - c) (1 - \\pi)}$$\nThis is the closed-form expression for PPV in terms of $\\pi$, $t$, and $c$.\n\n**Part 2: Derivation of the Negative Predictive Value (NPV)**\nThe NPV is defined as the probability that a patient does not have the condition given a negative test result, $NPV = P(\\bar{D} \\mid -)$.\nUsing Bayes' theorem:\n$$NPV = P(\\bar{D} \\mid -) = \\frac{P(- \\mid \\bar{D}) P(\\bar{D})}{P(-)}$$\nThe numerator is given by the product of the specificity and the probability of not having the condition:\n$$P(- \\mid \\bar{D}) P(\\bar{D}) = c (1 - \\pi)$$\nThe denominator, $P(-)$, is the total probability of a negative test. Using the Law of Total Probability:\n$$P(-) = P(- \\mid D) P(D) + P(- \\mid \\bar{D}) P(\\bar{D})$$\nSubstituting the known quantities:\n$$P(-) = (1 - t) \\pi + c (1 - \\pi)$$\nSubstituting the expressions for the numerator and denominator back into the Bayes' theorem formula for NPV:\n$$NPV = \\frac{c (1 - \\pi)}{(1 - t) \\pi + c (1 - \\pi)}$$\nThis is the closed-form expression for NPV in terms of $\\pi$, $t$, and $c$. For clarity, we can write the denominator as $c(1-\\pi) + (1-t)\\pi$.\n\n**Part 3: Derivative of PPV with respect to Prevalence ($\\pi$)**\nWe are asked to compute $\\frac{d}{d\\pi} PPV(\\pi)$ for fixed $t$ and $c$. The expression for PPV is:\n$$PPV(\\pi) = \\frac{t \\pi}{t \\pi + (1 - c) (1 - \\pi)}$$\nLet's first simplify the denominator:\n$$t \\pi + (1 - c) (1 - \\pi) = t \\pi + 1 - \\pi - c + c \\pi = \\pi(t + c - 1) + (1 - c)$$\nSo, the function to differentiate is:\n$$PPV(\\pi) = \\frac{t \\pi}{\\pi(t + c - 1) + (1 - c)}$$\nWe will use the quotient rule for differentiation, $\\frac{d}{dx} \\left( \\frac{u(x)}{v(x)} \\right) = \\frac{u'(x)v(x) - u(x)v'(x)}{[v(x)]^2}$.\nLet $u(\\pi) = t \\pi$ and $v(\\pi) = \\pi(t + c - 1) + (1 - c)$.\nThe derivatives with respect to $\\pi$ are:\n$u'(\\pi) = t$\n$v'(\\pi) = t + c - 1$\nApplying the quotient rule:\n$$\\frac{d}{d\\pi} PPV(\\pi) = \\frac{(t)[\\pi(t + c - 1) + (1 - c)] - (t \\pi)(t + c - 1)}{[\\pi(t + c - 1) + (1 - c)]^2}$$\nNow, we simplify the numerator:\n$$\\text{Numerator} = t\\pi(t + c - 1) + t(1 - c) - t\\pi(t + c - 1)$$\nThe first and third terms cancel each other out, leaving:\n$$\\text{Numerator} = t(1 - c)$$\nThe denominator is the square of the denominator of $PPV(\\pi)$. So, we can write the final derivative as:\n$$\\frac{d}{d\\pi} PPV(\\pi) = \\frac{t(1-c)}{[\\pi(t + c - 1) + (1 - c)]^2} = \\frac{t(1-c)}{\\left[ t\\pi + (1-c)(1-\\pi) \\right]^2}$$\nThis is the closed-form expression for the derivative of PPV with respect to $\\pi$. Since $t \\in (0,1)$ and $c \\in (0,1)$, the numerator $t(1-c)$ is always positive. The denominator is a squared term and thus is also positive. Therefore, the derivative is always positive, which confirms the known relationship that PPV increases with prevalence.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{t\\pi}{t\\pi + (1-c)(1-\\pi)}  \\frac{c(1-\\pi)}{c(1-\\pi) + (1-t)\\pi}  \\frac{t(1-c)}{\\left(t\\pi + (1-c)(1-\\pi)\\right)^2} \\end{pmatrix}}$$", "id": "4360373"}, {"introduction": "A key challenge in deploying clinical AI is translating a model's probabilistic output, like a risk score $p$, into a concrete action. This exercise introduces the powerful framework of cost-sensitive decision-making, where the optimal choice is found by minimizing the expected cost of errors. You will derive the Bayes-optimal decision threshold, demonstrating how to systematically balance the costs of false positives ($C_{FP}$) and false negatives ($C_{FN}$) to align an AI tool's behavior with clinical priorities. [@problem_id:4360388]", "problem": "A hospital deploys a binary classifier to triage suspected sepsis using Electronic Health Record (EHR) data. For each patient encounter with features $x$, the model outputs a calibrated probability $p \\in [0,1]$ of true sepsis within $6$ hours, meaning $p = \\mathbb{P}(Y=1 \\mid x)$ where $Y \\in \\{0,1\\}$ denotes the true sepsis status. The clinical action set is $a \\in \\{1,0\\}$, where $a=1$ triggers an early sepsis protocol and $a=0$ does not trigger it.\n\nFrom a health systems science perspective, the hospital assigns a cost $C_{FP}  0$ to a false positive (unnecessary protocol) and a cost $C_{FN}  0$ to a false negative (missed sepsis). Assume baseline costs for true positives and true negatives are $0$. Using the principle of expected risk minimization under calibrated probabilities, do the following:\n\n1. Construct the conditional expected cost (risk) for each action $a \\in \\{1,0\\}$ as a function of $p$, $C_{FP}$, and $C_{FN}$.\n2. From first principles of Bayesian decision theory, derive the decision rule that minimizes conditional expected cost and show that it reduces to a threshold test $p \\ge \\tau$ for some threshold $\\tau$ expressed in terms of $C_{FP}$ and $C_{FN}$.\n3. For $C_{FP} = 200$ and $C_{FN} = 3000$, compute the numerical value of the optimal threshold $\\tau$.\n\nProvide your final answer as the single numeric value of $\\tau$ with no units. No rounding is required.", "solution": "The problem is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n-   A binary classifier for sepsis provides a calibrated probability $p = \\mathbb{P}(Y=1 \\mid x)$, where $Y \\in \\{0,1\\}$ is the true sepsis status and $x$ are the patient features.\n-   The clinical action set is $a \\in \\{1,0\\}$, where $a=1$ is triggering an early sepsis protocol and $a=0$ is not.\n-   Cost of a false positive: $C_{FP}  0$.\n-   Cost of a false negative: $C_{FN}  0$.\n-   Cost of a true positive: $0$.\n-   Cost of a true negative: $0$.\n-   Specific cost values for part 3: $C_{FP} = 200$ and $C_{FN} = 3000$.\n-   The task is to:\n    1.  Construct the conditional expected cost (risk) for each action.\n    2.  Derive the optimal decision rule based on minimizing this risk, showing it leads to a threshold test $p \\ge \\tau$.\n    3.  Compute the numerical value of $\\tau$ for the given costs.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n-   **Scientifically Grounded**: The problem is a classic application of Bayesian decision theory to medical diagnosis, a well-established and fundamental topic in both machine learning and health systems science. The concepts of calibrated probabilities, cost-sensitive classification, and risk minimization are sound.\n-   **Well-Posed**: The problem is well-posed. It provides all necessary information—the actions, states of nature, conditional probabilities, and a complete cost structure—to derive a unique decision rule. The objective is clearly stated.\n-   **Objective**: The language is precise, quantitative, and free of ambiguity or subjective claims.\n-   **Conclusion**: There are no identifiable flaws. The problem does not violate any criteria for invalidity (e.g., it is not scientifically unsound, incomplete, or ambiguous).\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution is provided below.\n\nThe solution proceeds in three parts as requested by the problem statement.\n\n### 1. Construction of Conditional Expected Cost (Risk)\n\nLet $Y \\in \\{0, 1\\}$ be the random variable for the true patient state, where $Y=1$ indicates sepsis and $Y=0$ indicates no sepsis. Let $a \\in \\{0, 1\\}$ be the chosen action, where $a=1$ means triggering the protocol and $a=0$ means not triggering it. The model provides the calibrated probability $p = \\mathbb{P}(Y=1 \\mid x)$ for a given patient with features $x$. Consequently, $\\mathbb{P}(Y=0 \\mid x) = 1-p$.\n\nThe cost function, $C(a, Y)$, is defined based on the problem description:\n-   True Positive (TP): Action $a=1$, State $Y=1$. Cost $C(1,1) = 0$.\n-   False Positive (FP): Action $a=1$, State $Y=0$. Cost $C(1,0) = C_{FP}$.\n-   False Negative (FN): Action $a=0$, State $Y=1$. Cost $C(0,1) = C_{FN}$.\n-   True Negative (TN): Action $a=0$, State $Y=0$. Cost $C(0,0) = 0$.\n\nThe conditional expected cost, or risk, $R(a \\mid x)$, of taking an action $a$ given the information $x$ (and thus probability $p$) is the expectation of the cost function with respect to the conditional probability distribution of $Y$.\n$$R(a \\mid x) = \\mathbb{E}[C(a, Y) \\mid x] = \\sum_{y \\in \\{0,1\\}} C(a, y) \\mathbb{P}(Y=y \\mid x)$$\n\nWe compute the risk for each of the two possible actions:\n-   **Risk of action $a=1$ (trigger protocol):**\n    $$R(a=1 \\mid x) = C(1,1) \\mathbb{P}(Y=1 \\mid x) + C(1,0) \\mathbb{P}(Y=0 \\mid x)$$\n    $$R(a=1 \\mid x) = (0)(p) + (C_{FP})(1-p) = C_{FP}(1-p)$$\n\n-   **Risk of action $a=0$ (do not trigger protocol):**\n    $$R(a=0 \\mid x) = C(0,1) \\mathbb{P}(Y=1 \\mid x) + C(0,0) \\mathbb{P}(Y=0 \\mid x)$$\n    $$R(a=0 \\mid x) = (C_{FN})(p) + (0)(1-p) = C_{FN}p$$\n\n### 2. Derivation of the Optimal Decision Rule\n\nAccording to Bayesian decision theory, the optimal decision rule is to choose the action $a$ that minimizes the conditional expected cost $R(a \\mid x)$. We should therefore choose to trigger the protocol (action $a=1$) if and only if the risk of doing so is less than or equal to the risk of not doing so. The tie-breaking rule (choosing $a=1$ when risks are equal) is a common convention.\n\nThe decision rule is: choose $a=1$ if $R(a=1 \\mid x) \\le R(a=0 \\mid x)$.\nSubstituting the expressions for the risks derived in the previous step:\n$$C_{FP}(1-p) \\le C_{FN}p$$\n\nWe now solve this inequality for $p$:\n$$C_{FP} - C_{FP}p \\le C_{FN}p$$\n$$C_{FP} \\le C_{FN}p + C_{FP}p$$\n$$C_{FP} \\le p(C_{FN} + C_{FP})$$\n\nSince $C_{FP}  0$ and $C_{FN}  0$, the term $(C_{FN} + C_{FP})$ is strictly positive. We can divide by it without changing the direction of the inequality:\n$$p \\ge \\frac{C_{FP}}{C_{FN} + C_{FP}}$$\n\nThis demonstrates that the optimal decision rule is a threshold test. We should take action $a=1$ if the probability $p$ meets or exceeds a certain threshold $\\tau$. The threshold $\\tau$ is given by:\n$$\\tau = \\frac{C_{FP}}{C_{FN} + C_{FP}}$$\n\n### 3. Computation of the Numerical Threshold\n\nThe problem provides the specific cost values $C_{FP} = 200$ and $C_{FN} = 3000$. We can substitute these values into the expression for $\\tau$:\n$$\\tau = \\frac{200}{3000 + 200}$$\n$$\\tau = \\frac{200}{3200}$$\n\nSimplifying the fraction:\n$$\\tau = \\frac{2}{32} = \\frac{1}{16}$$\nTherefore, the optimal decision threshold is $\\frac{1}{16}$. The clinical protocol should be triggered if the model's predicted probability of sepsis is $p \\ge \\frac{1}{16}$.", "answer": "$$\\boxed{\\frac{1}{16}}$$", "id": "4360388"}, {"introduction": "This hands-on computational problem immerses you in the cutting-edge field of explainable AI (XAI). By designing a generator for counterfactual explanations, you will learn how to model real-world clinical constraints and causality to provide clinicians with actionable \"what-if\" scenarios. This practice illustrates how to move an AI model from being a \"black box\" predictor to a collaborative diagnostic partner that can explain what needs to change to achieve a desired outcome. [@problem_id:4360360]", "problem": "You are designing a programmatic counterfactual explanation generator for an imaging-based diagnostic decision in healthcare, grounded in health systems science. The diagnostic decision is produced by a linear risk score model widely used in healthcare analytics. A counterfactual explanation is defined as the smallest set of plausible changes to controllable acquisition parameters that flips a denied diagnosis to an approved diagnosis while respecting causal, clinical, and feasibility constraints.\n\nStart from the following fundamental bases, which are well-tested definitions and facts in health systems science and machine learning:\n- A linear risk score model maps feature inputs to a scalar diagnostic score, where approval is determined by thresholding this score. Given a feature vector, the model outputs a score that is the weighted sum of features plus an intercept.\n- A Structural Causal Model (SCM) encodes how variables are generated from others via structural equations. Counterfactual interventions change controllable variables and propagate their effects through these structural equations.\n- A clinically plausible counterfactual only suggests changes to controllable acquisition parameters (for example, using contrast or adjusting slice thickness), never changing immutable patient state (for example, age).\n\nVariables and definitions:\n- Let $I$ denote the observed lesion intensity from imaging (unitless).\n- Let $a \\in \\{0,1\\}$ denote whether contrast was used ($a = 1$) or not ($a = 0$).\n- Let $s$ denote the slice thickness in millimeters (mm).\n- Let $n$ denote the normalized noise level (unitless, bounded between quantitative limits).\n- Let $c$ denote the calcification score (unitless, fixed).\n- Let $\\text{age}$ denote patient age in years (years, fixed).\n- Let $g$ denote estimated Glomerular Filtration Rate (eGFR) in $\\text{mL}/\\text{min}/(1.73\\,\\text{m}^2)$ (fixed).\n- The linear diagnostic risk score is\n$$\nz = w_I I + w_c c + w_{\\text{age}}\\, \\text{age} + w_g g + b,\n$$\nand the decision is approved if and only if $z \\ge 0$ and denied if $z  0$.\n- The SCM for intensity $I$ under acquisition parameters is\n$$\nI' = I + \\alpha (a' - a_0) - \\beta (s' - s_0) - \\gamma (n' - n_0),\n$$\nwhere $a_0, s_0, n_0$ are the observed settings and $a', s', n'$ are the counterfactual settings. This expresses that contrast increases observed intensity by $\\alpha$ when turned on, and increased slice thickness and noise reduce intensity proportionally via $\\beta$ and $\\gamma$.\n- Clinical feasibility constraints:\n    - Contrast usage requires adequate renal function: if $g  g_{\\min}$, then $a' = 0$; otherwise $a' \\in \\{0,1\\}$.\n    - Slice thickness must satisfy $s_{\\min} \\le s' \\le s_{\\max}$.\n    - Noise must satisfy $n_{\\min} \\le n' \\le n_{\\max}$.\n    - A coupling feasibility relation captures that thinner slices increase minimum achievable noise: \n    $$\n    n' + \\lambda s' \\ge \\theta,\n    $$\n    which defines a convex feasible region for acquisition settings.\n    - Immutable variables ($c$, $\\text{age}$, $g$) cannot be changed.\n- Action cost for a counterfactual intervention is measured via an $\\ell_1$ cost on changes:\n$$\nJ = c_a |a' - a_0| + c_s |s' - s_0| + c_n |n' - n_0|.\n$$\nOnly turning contrast on is allowed if it was off and renal function is adequate; turning contrast off is not considered a clinically plausible way to flip a diagnosis from denied to approved.\n\nGoal:\n- For each test case, compute the minimal-cost counterfactual $(a', s', n')$ that flips the decision (i.e., yields $z' \\ge 0$) while satisfying all feasibility and causal constraints. If no feasible counterfactual exists, report infeasibility.\n\nModel parameters (use exactly these values):\n- Weights: $w_I = 0.7$, $w_c = 0.15$, $w_{\\text{age}} = -0.01$, $w_g = 0.004$, $b = -0.65$.\n- SCM intensity effects: $\\alpha = 0.8$, $\\beta = 0.3\\, \\text{per mm}$, $\\gamma = 0.5$.\n- Feasibility bounds: $s_{\\min} = 1.0\\, \\text{mm}$, $s_{\\max} = 5.0\\, \\text{mm}$, $n_{\\min} = 0.1$, $n_{\\max} = 1.0$, $g_{\\min} = 45.0\\, \\text{mL}/\\text{min}/(1.73\\,\\text{m}^2)$.\n- Coupling parameters: $\\lambda = 0.1$, $\\theta = 0.4$.\n- Action costs: $c_a = 0.05$, $c_s = 0.2$, $c_n = 0.3$.\n\nOptimization formulation guidance:\n- Enumerate feasible $a' \\in \\{0,1\\}$ consistent with the renal constraint and the rule that $a' \\ge a_0$.\n- For each feasible $a'$, minimize $c_s |s' - s_0| + c_n |n' - n_0|$ subject to linear inequality constraints and $z' \\ge 0$. The absolute values can be represented via nonnegative variables that decompose differences into positive and negative parts. The overall cost adds $c_a |a' - a_0|$ to the minimum.\n- Select the minimal-cost feasible counterfactual across the enumerated $a'$ values.\n\nTest suite:\n- Case $1$ (adequate renal function; contrast allowed; turning on contrast should be sufficient): \n    - $(I, c, \\text{age}, g, a_0, s_0, n_0) = (1.0, 0.1, 55, 90.0, 0, 3.0\\, \\text{mm}, 0.4)$.\n- Case $2$ (inadequate renal function; must adjust slice thickness and noise): \n    - $(I, c, \\text{age}, g, a_0, s_0, n_0) = (0.8, 0.05, 70, 35.0, 0, 4.0\\, \\text{mm}, 0.6)$.\n- Case $3$ (adequate renal function; turning on contrast alone is insufficient, but further acquisition adjustments can flip the decision): \n    - $(I, c, \\text{age}, g, a_0, s_0, n_0) = (0.4, 0.0, 90, 80.0, 0, 3.5\\, \\text{mm}, 0.5)$.\n- Case $4$ (inadequate renal function; even extreme feasible acquisition changes cannot flip the decision): \n    - $(I, c, \\text{age}, g, a_0, s_0, n_0) = (0.1, 0.0, 85, 30.0, 0, 5.0\\, \\text{mm}, 1.0)$.", "solution": "The user requires a solution to a constrained optimization problem formulated to find minimal-cost counterfactual explanations for a linear diagnostic model in healthcare. The problem is well-posed and grounded in established principles of machine learning and health systems science.\n\n### **Problem Formulation**\n\nThe goal is to find a counterfactual set of acquisition parameters $(a', s', n')$ that flips a \"denied\" decision ($z  0$) to \"approved\" ($z' \\ge 0$) at minimum cost. The variables are:\n- $a' \\in \\{0, 1\\}$: Counterfactual contrast usage.\n- $s'$: Counterfactual slice thickness (mm).\n- $n'$: Counterfactual noise level.\n\nThe cost function to minimize is the weighted $\\ell_1$ distance between the original parameters $(a_0, s_0, n_0)$ and the counterfactual ones:\n$$J = c_a |a' - a_0| + c_s |s' - s_0| + c_n |n' - n_0|$$\n\nThe optimization is subject to several constraints:\n\n1.  **Decision Flip**: The new risk score $z'$ must be non-negative.\n    $$z' = w_I I' + w_c c + w_{\\text{age}}\\, \\text{age} + w_g g + b \\ge 0$$\n    Using the Structural Causal Model (SCM) for intensity $I'$, we can express $z'$ in terms of the original score $z_{obs}$ and the changes in parameters:\n    $$I' = I + \\alpha (a' - a_0) - \\beta (s' - s_0) - \\gamma (n' - n_0)$$\n    $$z' = z_{obs} + w_I \\alpha (a' - a_0) - w_I \\beta (s' - s_0) - w_I \\gamma (n' - n_0)$$\n    Since $w_I, \\beta, \\gamma$ are positive, this leads to a linear inequality constraint on $s'$ and $n'$ for a fixed $a'$:\n    $$w_I \\beta s' + w_I \\gamma n' \\le z_{obs} + w_I \\alpha (a' - a_0) + w_I \\beta s_0 + w_I \\gamma n_0$$\n\n2.  **Feasibility Constraints**:\n    - Contrast: $a'$ is constrained by renal function ($g$) and the principle that we only intervene to turn contrast on ($a' \\ge a_0$). If $g  g_{\\min}$, then $a'$ must be $0$.\n    - Slice thickness: $s_{\\min} \\le s' \\le s_{\\max}$.\n    - Noise: $n_{\\min} \\le n' \\le n_{\\max}$.\n    - Coupling: $n' + \\lambda s' \\ge \\theta$.\n\n### **Optimization Strategy**\n\nThe problem is a Mixed-Integer Linear Program (MILP). Since the integer variable $a'$ is binary and highly constrained, we can solve the problem by enumerating its feasible values. For each test case, the initial state has $a_0 = 0$.\n- If renal function is inadequate ($g  g_{\\min}$), the only possibility is $a'=0$.\n- If renal function is adequate ($g \\ge g_{\\min}$), we consider two subproblems: $a'=0$ and $a'=1$.\n\nFor each fixed value of $a'$, the problem becomes a Linear Program (LP). The objective is to minimize $c_s |s' - s_0| + c_n |n' - n_0|$ subject to the linear constraints detailed above. This non-linear objective can be linearized by introducing non-negative slack variables:\n- Let $s' - s_0 = \\Delta_s^+ - \\Delta_s^-$ with $\\Delta_s^+, \\Delta_s^- \\ge 0$. Then $|s' - s_0| = \\Delta_s^+ + \\Delta_s^-$.\n- Let $n' - n_0 = \\Delta_n^+ - \\Delta_n^-$ with $\\Delta_n^+, \\Delta_n^- \\ge 0$. Then $|n' - n_0| = \\Delta_n^+ + \\Delta_n^-$.\n\nThe LP for a fixed $a'$ is thus:\n**Minimize:** $c_s (\\Delta_s^+ + \\Delta_s^-) + c_n (\\Delta_n^+ + \\Delta_n^-)$\n\n**Subject to:**\n1.  $w_I \\beta (s_0 + \\Delta_s^+ - \\Delta_s^-) + w_I \\gamma (n_0 + \\Delta_n^+ - \\Delta_n^-) \\le z_{obs} + w_I \\alpha (a' - a_0) + w_I \\beta s_0 + w_I \\gamma n_0$\n2.  $s_{\\min} \\le s_0 + \\Delta_s^+ - \\Delta_s^- \\le s_{\\max}$\n3.  $n_{\\min} \\le n_0 + \\Delta_n^+ - \\Delta_n^- \\le n_{\\max}$\n4.  $(n_0 + \\Delta_n^+ - \\Delta_n^-) + \\lambda (s_0 + \\Delta_s^+ - \\Delta_s^-) \\ge \\theta$\n5.  $\\Delta_s^+, \\Delta_s^-, \\Delta_n^+, \\Delta_n^- \\ge 0$\n\nThis standard LP formulation can be solved using scientific computing libraries like `scipy.optimize.linprog`.\n\nThe overall algorithm for each test case is:\n1.  Calculate the initial risk score $z_{obs}$.\n2.  Determine the set of feasible $a'$ values.\n3.  Initialize a placeholder for the best solution found so far, marked as infeasible with infinite cost.\n4.  For each feasible $a'$:\n    a. Formulate and solve the corresponding LP for the optimal changes $(\\Delta_s^+, \\Delta_s^-, \\Delta_n^+, \\Delta_n^-)$.\n    b. If the LP is feasible (a solution exists), calculate the counterfactual $(s', n')$ and the total cost $J = c_a|a'-a_0| + \\text{LP cost}$.\n    c. If this solution's cost $J$ is lower than the best cost found so far, update the best solution.\n5.  If a best feasible solution was found, report its parameters $(a', s', n')$, the resulting intensity $I'$ and score $z'$, and the total cost $J$.\n6.  If no feasible solution was found across all possible $a'$ values, report the original state and mark the case as infeasible.\n\nThis step-by-step process guarantees finding the globally optimal counterfactual explanation according to the specified model and constraints. If the feasible set for the LP is empty for all valid choices of $a'$, no such counterfactual exists.", "answer": "[[1, 1, 3.0, 0.4, 1.8, 0.435, 0.05], [1, 0, 1.0, 0.3, 1.85, 0.0725, 0.69], [1, 1, 1.0, 0.3, 2.3, 0.005, 0.61], [0, 0, 5.0, 1.0, 0.1, -0.84, -1.0]]", "id": "4360360"}]}