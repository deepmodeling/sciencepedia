{"hands_on_practices": [{"introduction": "The foundation of genomic medicine rests on the accuracy of the raw sequence data generated by sequencing instruments. To quantify this accuracy, the field uses the Phred quality score, $Q$, a logarithmic measure of a base call's error probability. This first practice [@problem_id:4361966] provides a direct application of this concept, guiding you to translate quality scores from a sequencing run into a tangible estimate of the total number of errors across an entire human genome. This skill is vital for health system analysts to plan for downstream data validation and understand the inherent quality of their genomic data assets.", "problem": "A national precision health initiative is using Next-Generation Sequencing (NGS) to generate whole-genome data for clinical decision support in a population cohort. The genome size under analysis is $3 \\times 10^{9}$ bases per individual. Health systems analysts need to estimate the expected burden of erroneous base calls to plan downstream validation and quality control workflows. The Phred quality score definition is $Q = -10 \\log_{10}(p_{\\text{err}})$, where $p_{\\text{err}}$ is the probability that a base call is incorrect. In a typical sequencing run, $0.85$ of all base calls have quality $Q=30$ and the remaining $0.15$ have quality $Q=20$. Starting from the core definition of expectation for Bernoulli trials and the Phred score definition above, derive a formula that connects the distribution of Phred scores to the expected number of errors across the $3 \\times 10^{9}$ bases, and compute the expected number of erroneous base calls for this run. Express your final answer as a count in scientific notation rounded to three significant figures.", "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. All data required for a solution are provided, the definitions (e.g., Phred score) are standard in the field of genomics, and the premises are consistent and physically realistic. The problem asks for a derivation and a calculation, which is a standard format for a quantitative scientific question. Therefore, I will proceed with a full solution.\n\nThe primary goal is to find the expected number of erroneous base calls, denoted as $E[X]$, in a genome sequence of total size $N$. Let us model the correctness of each base call as an independent Bernoulli trial. For each base $i$ from $1$ to $N$, we can define a random variable $X_i$ such that:\n$X_i = 1$ if the base call is an error (with probability $p_{\\text{err},i}$).\n$X_i = 0$ if the base call is correct (with probability $1 - p_{\\text{err},i}$).\n\nThe expected value of this random variable is $E[X_i] = 1 \\cdot p_{\\text{err},i} + 0 \\cdot (1 - p_{\\text{err},i}) = p_{\\text{err},i}$.\n\nThe total number of errors, $X$, across the entire sequence of $N$ bases is the sum of these individual random variables:\n$$X = \\sum_{i=1}^{N} X_i$$\n\nBy the linearity of expectation, the expected total number of errors is the sum of the individual expectations:\n$$E[X] = E\\left[\\sum_{i=1}^{N} X_i\\right] = \\sum_{i=1}^{N} E[X_i] = \\sum_{i=1}^{N} p_{\\text{err},i}$$\n\nThe problem states that the base calls are partitioned into groups based on their Phred quality score, $Q$. Let there be $k$ such groups, where each group $k$ is characterized by a quality score $Q_k$ and comprises a fraction $f_k$ of the total bases. Within each group, all bases have the same probability of error, $p_{\\text{err},k}$. The total number of bases in group $k$ is $N_k = f_k N$.\n\nThe total sum of error probabilities can be restructured as a sum over these groups:\n$$E[X] = \\sum_{k} \\sum_{i \\in \\text{group } k} p_{\\text{err},i} = \\sum_{k} (N_k \\cdot p_{\\text{err},k})$$\nSubstituting $N_k = f_k N$:\n$$E[X] = \\sum_{k} (f_k N \\cdot p_{\\text{err},k}) = N \\sum_{k} (f_k \\cdot p_{\\text{err},k})$$\n\nThe connection between the Phred score $Q_k$ and the error probability $p_{\\text{err},k}$ is given by the definition:\n$$Q_k = -10 \\log_{10}(p_{\\text{err},k})$$\nTo find $p_{\\text{err},k}$ as a function of $Q_k$, we rearrange this equation:\n$$-\\frac{Q_k}{10} = \\log_{10}(p_{\\text{err},k})$$\n$$p_{\\text{err},k} = 10^{-Q_k/10}$$\n\nSubstituting this expression for $p_{\\text{err},k}$ into our equation for $E[X]$ gives the general formula that connects the distribution of Phred scores to the expected number of errors:\n$$E[X] = N \\sum_{k} f_k 10^{-Q_k/10}$$\n\nNow, we apply this derived formula to the specific data provided in the problem. The total number of bases is $N = 3 \\times 10^9$. The bases are partitioned into two groups:\nGroup 1: fraction $f_1 = 0.85$ with quality score $Q_1 = 30$.\nGroup 2: fraction $f_2 = 0.15$ with quality score $Q_2 = 20$.\n\nFirst, we calculate the error probabilities for each group:\nFor Group 1:\n$$p_{\\text{err},1} = 10^{-Q_1/10} = 10^{-30/10} = 10^{-3} = 0.001$$\nFor Group 2:\n$$p_{\\text{err},2} = 10^{-Q_2/10} = 10^{-20/10} = 10^{-2} = 0.01$$\n\nNow we use the formula for the expected number of errors with $k=2$:\n$$E[X] = N (f_1 p_{\\text{err},1} + f_2 p_{\\text{err},2})$$\nSubstituting the numerical values:\n$$E[X] = (3 \\times 10^9) \\left( (0.85)(10^{-3}) + (0.15)(10^{-2}) \\right)$$\n$$E[X] = (3 \\times 10^9) \\left( 0.00085 + 0.0015 \\right)$$\n$$E[X] = (3 \\times 10^9) \\left( 0.00235 \\right)$$\nTo perform the final multiplication, we express $0.00235$ in scientific notation as $2.35 \\times 10^{-3}$:\n$$E[X] = (3 \\times 10^9) \\times (2.35 \\times 10^{-3})$$\n$$E[X] = (3 \\times 2.35) \\times 10^{(9-3)}$$\n$$E[X] = 7.05 \\times 10^6$$\n\nThe expected number of erroneous base calls for this run is $7.05 \\times 10^6$. The result is already expressed in scientific notation with three significant figures as requested.", "answer": "$$\\boxed{7.05 \\times 10^6}$$", "id": "4361966"}, {"introduction": "Beyond assessing individual data points, a key challenge in large-scale genomics is ensuring consistency across different batches of samples, which may be processed on different instruments or with different chemical reagents. These non-biological variations, known as \"batch effects,\" can obscure true biological signals. This exercise [@problem_id:4361937] introduces Principal Component Analysis (PCA), a powerful technique for identifying such hidden structures in complex quality control data. By working through this problem, you will learn a standard, robust method to diagnose and quantify batch effects, ensuring the integrity of large genomic datasets within a health system.", "problem": "A health system sequencing laboratory running Whole-Genome Sequencing (WGS) on patient samples wants to diagnose and quantify batch effects arising from instruments and reagent lots using quality control features. For each of $n=120$ samples, the lab records four per-sample quality metrics: average coverage depth $x_{1}$, median insert size $x_{2}$, a GC-content bias metric $x_{3}$, and an estimated contamination fraction $x_{4}$. Let $X \\in \\mathbb{R}^{n \\times 4}$ be the data matrix whose columns are the four metrics; each column is standardized to zero mean and unit variance across samples (z-scored). The lab will use Principal Component Analysis (PCA) to detect instrument- or reagent-specific clusters and quantify how much of the total variance in these four metrics is explained by batch. Assume there are two instruments (Instrument A and Instrument B), with $60$ samples on each, and three reagent lots used uniformly across instruments.\n\nFrom PCA on the standardized data (equivalently, the sample correlation matrix $R = \\frac{1}{n-1} X^\\top X$), the eigenvalues are observed to be $\\lambda_{1} = 1.80$, $\\lambda_{2} = 1.20$, $\\lambda_{3} = 0.70$, and $\\lambda_{4} = 0.30$. Because columns are standardized, the total variance equals $\\sum_{k=1}^{4} \\lambda_k = 4$.\n\nWhen PC1 scores (the projections of samples onto the first principal component) are grouped by instrument, their group means are $\\bar{y}_{\\text{A}} = 0.6$ for Instrument A and $\\bar{y}_{\\text{B}} = -0.6$ for Instrument B; the grand mean of PC1 scores is $\\bar{y} = 0$ by construction. Assume reagent lot has negligible association with PC1 and PC2 in this data.\n\nWhich option most correctly specifies a scientifically sound batch-effect diagnosis plan using PCA on these four metrics to detect instrument-specific clustering and to quantify the proportion of total variance in the four metrics explained by instrument batch under the given data?\n\nA. Standardize each metric (columns of $X$), compute PCA on the sample correlation matrix, visualize PC scores colored by instrument and reagent lot, then formally test association of PC scores with instrument using one-way Analysis of Variance (ANOVA). Quantify the proportion of total variance in the four metrics explained by instrument as the fraction of PC variance explained by instrument, weighted by each PC's share of total variance; with only PC1 showing association, this equals $\\left(\\frac{\\lambda_{1}}{\\sum_{k} \\lambda_{k}}\\right) \\cdot \\frac{\\text{SSR}_{\\text{PC1}}}{\\text{SST}_{\\text{PC1}}}$, yielding approximately $0.091$ or $9.1\\%$.\n\nB. Compute PCA on the covariance of the raw (unstandardized) metrics to preserve natural scales, test instrument association using $t$-tests on PC loadings, and report the proportion of variance explained by instrument as the $R^{2}$ from PC1 alone, approximately $0.202$ or $20.2\\%$.\n\nC. Normalize each sample (row of $X$) to unit norm, compute PCA on the resulting covariance matrix, assess batch association using correlation of PC loadings with instrument, and quantify variance explained by instrument as the proportion of variance captured by PC1, $\\lambda_{1}/\\sum_{k} \\lambda_{k} = 0.45$ or $45\\%$.\n\nD. Standardize metrics, compute PCA on the correlation matrix, cluster samples in PC space with $k$-means ($k=2$) to recover instrument groups, then quantify variance explained by instrument as the squared difference in PC1 group means divided by the total number of metrics, $\\left(\\bar{y}_{\\text{A}} - \\bar{y}_{\\text{B}}\\right)^{2}/4 = 0.36$ or $36\\%$.", "solution": "### Step 1: Extract Givens\nThe problem provides the following information for a Whole-Genome Sequencing (WGS) quality control analysis:\n- Number of samples, $n = 120$.\n- Number of quality metrics (features), $p = 4$. The metrics are average coverage depth ($x_1$), median insert size ($x_2$), a GC-content bias metric ($x_3$), and an estimated contamination fraction ($x_4$).\n- The data matrix is $X \\in \\mathbb{R}^{n \\times 4}$, where $n=120$.\n- Each of the $4$ columns of $X$ is standardized to have zero mean and unit variance (z-scored).\n- The analysis method is Principal Component Analysis (PCA).\n- There are two instrument batch groups: Instrument A ($n_{\\text{A}} = 60$ samples) and Instrument B ($n_{\\text{B}} = 60$ samples).\n- PCA is performed on the standardized data, which is equivalent to performing PCA on the sample correlation matrix $R = \\frac{1}{n-1} X^\\top X$.\n- The eigenvalues of the correlation matrix are given as $\\lambda_{1} = 1.80$, $\\lambda_{2} = 1.20$, $\\lambda_{3} = 0.70$, and $\\lambda_{4} = 0.30$.\n- The total variance is the trace of the correlation matrix, $\\sum_{k=1}^{4} \\lambda_k = 1.80 + 1.20 + 0.70 + 0.30 = 4$, which is equal to the number of variables, as expected for standardized data.\n- The scores for the first principal component (PC1) have group means $\\bar{y}_{\\text{A}} = 0.6$ for Instrument A and $\\bar{y}_{\\text{B}} = -0.6$ for Instrument B.\n- The grand mean of PC1 scores is $\\bar{y} = 0$.\n- It is assumed that reagent lot has a negligible association with PC1 and PC2.\n- The question is to identify the most scientifically sound plan to diagnose the instrument-specific batch effect and quantify the proportion of total variance in the four metrics that it explains.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically sound and well-posed.\n- **Scientifically Grounded**: The use of PCA on quality control metrics to identify batch effects is a standard and widely accepted practice in bioinformatics and genomics. The metrics listed are common for sequencing data. The statistical approach is based on established principles of linear algebra and analysis of variance.\n- **Well-Posed & Complete**: The problem provides all necessary data and definitions to perform the required calculations and evaluate the proposed methodologies. The numerical values are consistent (e.g., $\\sum \\lambda_k = p$, and the symmetric group means for PC1 are consistent with equal group sizes).\n- **Objective**: The problem is stated using precise, objective, and technical language. It is free from ambiguity or subjective claims.\n- **Realism**: The scenario and data are realistic for a genomics core facility.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with a full derivation and evaluation of the options.\n\n### Principle-Based Derivation\n\nThe goal is to quantify the proportion of the *total variance* in the original four metrics that can be attributed to the instrument batch effect. The total variance in the standardized data is the sum of the variances of the four z-scored metrics, which is $1+1+1+1=4$. In PCA, this total variance is equal to the sum of the eigenvalues of the correlation matrix, $\\sum_{k=1}^{4} \\lambda_k = 4$.\n\nPCA decomposes the total variance into orthogonal principal components (PCs). The variance captured by the $k$-th principal component, PC$k$, is given by its eigenvalue, $\\lambda_k$. The proportion of total variance captured by PC$k$ is $\\frac{\\lambda_k}{\\sum_{j=1}^{4} \\lambda_j}$.\n\nThe problem states that the instrument batch effect is observable in the PC scores, particularly PC1. To quantify the amount of variance *within* PC1 that is explained by the instrument, we can use an Analysis of Variance (ANOVA) framework. The proportion of variance in PC1 scores ($y_1$) explained by the instrument factor is given by the coefficient of determination, $R^2_{\\text{PC1}}$, which is the ratio of the between-group sum of squares ($\\text{SSR}_{\\text{PC1}}$) to the total sum of squares ($\\text{SST}_{\\text{PC1}}$).\n\n1.  **Calculate the Total Sum of Squares for PC1 ($\\text{SST}_{\\text{PC1}}$)**:\n    The variance of the scores of the first principal component is, by definition, the first eigenvalue, $\\lambda_1$.\n    $$ \\text{Var}(y_1) = \\lambda_1 = 1.80 $$\n    The total sum of squares is related to the sample variance by $\\text{SST}_{\\text{PC1}} = (n-1)\\text{Var}(y_1)$.\n    $$ \\text{SST}_{\\text{PC1}} = (120-1) \\times 1.80 = 119 \\times 1.80 = 214.2 $$\n\n2.  **Calculate the Between-Group Sum of Squares for PC1 ($\\text{SSR}_{\\text{PC1}}$)**:\n    This measures the variance between the group means.\n    $$ \\text{SSR}_{\\text{PC1}} = n_{\\text{A}}(\\bar{y}_{\\text{A}} - \\bar{y})^2 + n_{\\text{B}}(\\bar{y}_{\\text{B}} - \\bar{y})^2 $$\n    Using the given values $n_{\\text{A}}=60$, $n_{\\text{B}}=60$, $\\bar{y}_{\\text{A}}=0.6$, $\\bar{y}_{\\text{B}}=-0.6$, and $\\bar{y}=0$:\n    $$ \\text{SSR}_{\\text{PC1}} = 60(0.6 - 0)^2 + 60(-0.6 - 0)^2 = 60(0.36) + 60(0.36) = 2 \\times 21.6 = 43.2 $$\n\n3.  **Calculate the Proportion of PC1 Variance Explained by Instrument**:\n    $$ R^2_{\\text{PC1}} = \\frac{\\text{SSR}_{\\text{PC1}}}{\\text{SST}_{\\text{PC1}}} = \\frac{43.2}{214.2} \\approx 0.20168 $$\n\n4.  **Calculate the Proportion of Total Variance Explained by Instrument**:\n    This is the final quantity of interest. It is the proportion of variance in PC1 explained by the instrument, weighted by the proportion of total variance that PC1 itself represents. The problem implies we should only consider PC1, as it is the component given to be associated with the instrument.\n    $$ \\text{Proportion} = \\left( \\frac{\\text{Variance of PC1}}{\\text{Total Variance}} \\right) \\times \\left( \\frac{\\text{Variance in PC1 due to Instrument}}{\\text{Variance of PC1}} \\right) $$\n    $$ \\text{Proportion} = \\left( \\frac{\\lambda_1}{\\sum_{k=1}^{4} \\lambda_k} \\right) \\times R^2_{\\text{PC1}} = \\left( \\frac{1.80}{4.00} \\right) \\times \\frac{\\text{SSR}_{\\text{PC1}}}{\\text{SST}_{\\text{PC1}}} $$\n    $$ \\text{Proportion} = 0.45 \\times \\frac{43.2}{214.2} \\approx 0.45 \\times 0.20168 \\approx 0.090756 $$\n    This value is approximately $0.091$, or $9.1\\%$.\n\n### Option-by-Option Analysis\n\n**A. Standardize each metric (columns of $X$), compute PCA on the sample correlation matrix, visualize PC scores colored by instrument and reagent lot, then formally test association of PC scores with instrument using one-way Analysis of Variance (ANOVA). Quantify the proportion of total variance in the four metrics explained by instrument as the fraction of PC variance explained by instrument, weighted by each PC's share of total variance; with only PC1 showing association, this equals $\\left(\\frac{\\lambda_{1}}{\\sum_{k} \\lambda_{k}}\\right) \\cdot \\frac{\\text{SSR}_{\\text{PC1}}}{\\text{SST}_{\\text{PC1}}}$, yielding approximately $0.091$ or $9.1\\%$.**\n\n- **Methodology**: This option outlines the standard, correct procedure. Standardizing variables (or using the correlation matrix) is essential when metrics have different scales. Visualizing PC scores is the correct exploratory step. Using ANOVA on PC scores is the correct formal test for association with a categorical factor. The formula for quantifying the proportion of total variance is also correct, as it properly weights the within-PC variance explained by the batch factor ($R^2_{\\text{PC1}}$) by the proportion of total variance the PC accounts for ($\\lambda_1/\\sum\\lambda_k$).\n- **Calculation**: The calculation derived above ($0.090756$) matches the value provided in the option ($0.091$ or $9.1\\%$).\n- **Verdict**: **Correct**.\n\n**B. Compute PCA on the covariance of the raw (unstandardized) metrics to preserve natural scales, test instrument association using $t$-tests on PC loadings, and report the proportion of variance explained by instrument as the $R^{2}$ from PC1 alone, approximately $0.202$ or $20.2\\%$.**\n\n- **Methodology**: This option contains multiple fundamental errors.\n    1.  PCA on the raw covariance matrix is inappropriate here. The four metrics (coverage, insert size, bias, contamination) have different units and scales of variance. The analysis would be dominated by the metric with the highest variance, ignoring meaningful variation in the others.\n    2.  Testing PC *loadings* for association with a sample-level factor (instrument) is incorrect. Loadings describe the contribution of variables to a PC. PC *scores* are the projections of samples and are the correct object to test.\n    3.  Reporting $R^2_{\\text{PC1}} \\approx 0.202$ as the final answer is incorrect. This value represents the proportion of variance *of PC1* explained by the instrument, not the proportion of the *total variance of all four metrics*. It fails to weight by PC1's contribution to total variance.\n- **Verdict**: **Incorrect**.\n\n**C. Normalize each sample (row of $X$) to unit norm, compute PCA on the resulting covariance matrix, assess batch association using correlation of PC loadings with instrument, and quantify variance explained by instrument as the proportion of variance captured by PC1, $\\lambda_{1}/\\sum_{k} \\lambda_{k} = 0.45$ or $45\\%$.**\n\n- **Methodology**: This option also contains multiple fundamental errors.\n    1.  Normalizing each sample (row-wise normalization) is not the standard procedure for this type of PCA. It conflates the magnitude of measurements for a sample across different metrics, which is not the goal. We want to compare samples based on their feature values, so we standardize features (columns).\n    2.  As with option B, assessing correlation of PC *loadings* with a sample-level factor is conceptually wrong.\n    3.  Quantifying the effect as $\\lambda_1 / \\sum\\lambda_k = 0.45$ is incorrect. This is merely the proportion of total variance captured by PC1; it does not measure how much of that variance is attributable to the instrument batch effect. It confuses the existence of a dominant PC with an explanation for it.\n- **Verdict**: **Incorrect**.\n\n**D. Standardize metrics, compute PCA on the correlation matrix, cluster samples in PC space with $k$-means ($k=2$) to recover instrument groups, then quantify variance explained by instrument as the squared difference in PC1 group means divided by the total number of metrics, $\\left(\\bar{y}_{\\text{A}} - \\bar{y}_{\\text{B}}\\right)^{2}/4 = 0.36$ or $36\\%$.**\n\n- **Methodology**: The initial steps are correct, but the later steps are flawed.\n    1.  Using $k$-means clustering is an unsupervised method. Since we already have the instrument labels for each sample, the problem is supervised. We should use these labels directly (as in ANOVA) rather than trying to \"recover\" them with clustering.\n    2.  The quantification formula, $(\\bar{y}_{\\text{A}} - \\bar{y}_{\\text{B}})^{2}/4$, is ad hoc and not based on any principle of variance decomposition. It correctly computes to $(0.6 - (-0.6))^2 / 4 = 1.2^2 / 4 = 1.44 / 4 = 0.36$, but the formula itself is statistically meaningless for quantifying the proportion of variance explained. It does not account for the within-group variance or the total sample size.\n- **Verdict**: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "4361937"}, {"introduction": "After generating high-quality genomic data, the final step is to integrate it into a cohesive health system for clinical use. This requires standardized data representation, but the two dominant standards—the descriptive Human Genome Variation Society (HGVS) nomenclature and the technical Variant Call Format (VCF)—are not perfectly interchangeable. This practice [@problem_id:4361989] explores the critical informatics challenge of \"lossy\" data conversion, where essential biological context can be lost when translating between formats. Understanding these interoperability gaps is fundamental to designing and managing a functional clinical genomics data warehouse.", "problem": "A health system is integrating laboratory reports written in Human Genome Variation Society (HGVS) nomenclature (HGVS) with an enterprise Variant Call Format (VCF) warehouse that enforces a single canonical allele representation. Two independently developed normalization tools are in use in different parts of the pipeline. Both tools normalize VCF alleles with left alignment and minimal trimming but differ in secondary choices about representation when mapping back to HGVS. The enterprise goal is that a variant described in HGVS, converted to normalized VCF, and then converted back to HGVS should remain semantically equivalent to the original HGVS description at the level needed for clinical decision support.\n\nStarting from first principles:\n\n- The Central Dogma of molecular biology states that DNA is transcribed to RNA and translated to protein; transcripts are spliced products whose coordinates are mapped to genomic coordinates through a transcript-to-genome alignment. Let $f_{\\text{tx}\\to\\text{genome}}$ denote the mapping from a transcript coordinate system to the reference genome coordinate system. This mapping depends on the exact transcript accession and version, strand, and exon structure.\n- In VCF canonicalization of insertions and deletions, minimal representation removes matching prefix and suffix bases and left alignment places the variant at the leftmost equivalent coordinate on the reference genome. If a deletion of a base $b$ occurs within a homopolymer run of $m \\ge 2$ copies of $b$, there are $m$ equivalent ways to write the deletion prior to normalization but only one leftmost normalized coordinate.\n- HGVS supports representation choices that encode biological context, such as duplication notation (\"dup\") when the inserted sequence equals adjacent reference sequence, multi-nucleotide changes as a single \"delins\", and multiple equivalent ways of anchoring an indel within a repeat. These choices are part of the HGVS semantic layer and may not be preserved by VCF normalization.\n\nConsider the following scenarios in which an HGVS description is round-tripped through normalized VCF:\n\nA. An HGVS coding-level description $c.n\\text{del}A$ deletes a single $A$ within a homopolymer of $m \\ge 3$ $A$ bases on the reference genome, and no transcript version is provided. Both VCF tools apply minimal representation and left alignment.\n\nB. An HGVS coding-level description $c.kG>A$ describes a single-nucleotide variant from $G$ to $A$ in a unique, non-repetitive context, with a specified transcript accession and version that matches the reference genome build used in VCF. Both tools apply minimal representation and left alignment.\n\nC. An HGVS coding-level description $c.100\\text{dup}A$ uses duplication notation to indicate insertion of a single $A$ that duplicates the adjacent reference base. Both tools convert to VCF insertions by trimming and left aligning. On conversion back to HGVS, neither tool preserves explicit duplication semantics beyond the inserted sequence.\n\nD. An HGVS coding-level description $c.(200\\_201)\\text{delins}TT$ captures a multi-nucleotide variant affecting two adjacent coding bases on the same haplotype (a phased change). During normalization, one tool emits two separate biallelic single-nucleotide VCF records without preserving haplotype phase, and the other emits a single multi-nucleotide variant record but downstream systems split multi-nucleotide records for storage. Phasing markers are not retained in the round-trip.\n\nE. An HGVS coding-level description $c.n\\text{del}A$ within a tandem repeat is converted to normalized VCF, but both tools carry forward a metadata field that records the original HGVS anchor position and transcript version. Downstream HGVS regeneration explicitly uses that metadata to select the same anchor and transcript.\n\nWhich scenarios are expected to lead to a lossy round-trip conversion across the different normalization tools, meaning that the regenerated HGVS cannot be guaranteed to be semantically equivalent to the original description without additional context?\n\nChoose all that apply.\n\nA. Scenario A\n\nB. Scenario B\n\nC. Scenario C\n\nD. Scenario D\n\nE. Scenario E", "solution": "The validity of the problem statement is hereby confirmed. The problem is scientifically grounded in the principles of bioinformatics and genomics, specifically concerning the standards of HGVS and VCF nomenclature. It is well-posed, objective, and describes a realistic and non-trivial informatics challenge in clinical genomics.\n\nThe core of the problem lies in the conversion between two different variant representation standards: the Human Genome Variation Society (HGVS) nomenclature and the Variant Call Format (VCF). HGVS is transcript-centric and semantically rich, capable of encoding biological context. VCF is genome-coordinate-centric and represents the raw sequence change, with a canonicalization process (minimal representation and left-alignment) that ensures a single representation for a given variant on a reference genome. A \"lossy\" round-trip conversion (HGVS $\\rightarrow$ VCF $\\rightarrow$ HGVS) occurs when information present in the original HGVS string is destroyed during the conversion to the canonical VCF format and cannot be recovered upon conversion back to HGVS. This means the final HGVS string is not semantically equivalent to the original.\n\nLet's analyze each scenario based on these first principles.\n\n**A. An HGVS coding-level description $c.n\\text{del}A$ deletes a single $A$ within a homopolymer of $m \\ge 3$ $A$ bases on the reference genome, and no transcript version is provided. Both VCF tools apply minimal representation and left alignment.**\n\n1.  **HGVS to VCF Conversion:**\n    -   **Transcript Ambiguity:** The description \"no transcript version is provided\" is a critical flaw. The mapping from a coding coordinate ($c.n$) to a genomic coordinate ($f_{\\text{tx}\\to\\text{genome}}$) is dependent on the specific transcript. Different versions of a transcript for a gene can have different exon boundaries or lengths, meaning $c.n$ could map to completely different genomic locations depending on the assumed transcript. This ambiguity alone makes a reliable round-trip impossible.\n    -   **Normalization in a Repetitive Region:** Even assuming a transcript could be unambiguously inferred, the deletion of a single base $A$ within a homopolymer of $m$ $A$s has $m$ equivalent representations at the sequence level. For example, deleting the second $A$ in `GAAAACT` is equivalent to deleting the first, third, or fourth. HGVS nomenclature might specify any of these positions, e.g., using the 3' rule. However, VCF normalization, specifically left-alignment, will force this variant to a single, canonical, leftmost position. This is a many-to-one mapping.\n\n2.  **VCF to HGVS Conversion:**\n    -   When converting the single, canonical VCF record back to HGVS, the tool has no information about the original anchor position ($n$) chosen in the source HGVS. It will map the canonical genomic coordinate back to the transcript, which will yield only one of the possible HGVS descriptions, not necessarily the original one. The information about the original choice of $n$ is irreversibly lost.\n\n3.  **Verdict:** This scenario will lead to a lossy conversion. The combination of transcript ambiguity and the many-to-one nature of VCF normalization in repetitive regions makes it impossible to guarantee the recovery of the original HGVS description. **Correct.**\n\n**B. An HGVS coding-level description $c.kG>A$ describes a single-nucleotide variant from $G$ to $A$ in a unique, non-repetitive context, with a specified transcript accession and version that matches the reference genome build used in VCF. Both tools apply minimal representation and left alignment.**\n\n1.  **HGVS to VCF Conversion:**\n    -   The variant is a single-nucleotide variant (SNV). A transcript is specified, so the mapping $f_{\\text{tx}\\to\\text{genome}}$ from coding coordinate $c.k$ to a genomic coordinate is unambiguous.\n    -   The context is \"unique, non-repetitive\". VCF normalization (left-alignment and trimming) has no effect on a simple SNV, as there are no adjacent identical bases to shift or trim. The VCF representation is inherently canonical.\n\n2.  **VCF to HGVS Conversion:**\n    -   The conversion from the unique genomic coordinate back to the specified transcript is a one-to-one mapping. The result will be the same coding position $k$ and the same change $G>A$.\n\n3.  **Verdict:** The conversion is unambiguous in both directions. The round-trip is lossless. **Incorrect.**\n\n**C. An HGVS coding-level description $c.100\\text{dup}A$ uses duplication notation to indicate insertion of a single $A$ that duplicates the adjacent reference base. Both tools convert to VCF insertions by trimming and left aligning. On conversion back to HGVS, neither tool preserves explicit duplication semantics beyond the inserted sequence.**\n\n1.  **HGVS to VCF Conversion:**\n    -   HGVS `dup` notation is semantically specific, implying a particular mutational mechanism. However, at the sequence level, `c.100dupA` is identical to an insertion, `c.100_101insA`.\n    -   Standard VCF represents this as an insertion. For example, if the reference base at the corresponding genomic position is $A$, the VCF record would show an insertion of an $A$ (e.g., `REF=G`, `ALT=GA`, where the $G$ is the preceding base). The VCF format itself has no standard field to encode \"duplication\" as a mechanism. This semantic information is lost at this step.\n\n2.  **VCF to HGVS Conversion:**\n    -   The problem explicitly states that the tools' reverse converters do not preserve/re-infer the `dup` semantic. They see an insertion of an $A` next to a reference $A$. They will likely generate an `ins` description, e.g., `c.100_101insA`.\n    -   While `c.100dupA` and `c.100_101insA` describe the same final sequence, they are not semantically equivalent according to HGVS standards. The choice of `dup` conveys extra biological information that has been lost.\n\n3.  **Verdict:** The special semantic `dup` is lost during the conversion to the more general VCF format. The regenerated HGvs string is not guaranteed to be semantically equivalent to the original. **Correct.**\n\n**D. An HGVS coding-level description $c.(200\\_201)\\text{delins}TT$ captures a multi-nucleotide variant affecting two adjacent coding bases on the same haplotype (a phased change). During normalization, one tool emits two separate biallelic single-nucleotide VCF records without preserving haplotype phase, and the other emits a single multi-nucleotide variant record but downstream systems split multi-nucleotide records for storage. Phasing markers are not retained in the round-trip.**\n\n1.  **HGVS to VCF Conversion:**\n    -   The HGVS description `c.(200_201)delinsTT` unequivocally states that two adjacent bases are replaced by `TT` *on the same allele*. This is a phased, multi-nucleotide variant (MNV).\n    -   The problem describes two pathways, both of which result in loss of phase information. Tool 1 actively splits the MNV into two separate VCF records. Tool 2 creates a proper MNV record (e.g., `REF=XY`, `ALT=TT`), but the downstream system architecture splits it. In either case, the final state in the VCF warehouse is two separate, unphased records.\n\n2.  **VCF to HGVS Conversion:**\n    -   Starting with two separate, unphased VCF records, it is impossible to determine that they originated from a single, phased event. The problem confirms this by stating, \"Phasing markers are not retained in the round-trip.\"\n    -   The regenerated HGVS will describe two independent variants (e.g., `[c.200X>T;c.201Y>T]`), not a single phased `delins`. This loss of phase information is a critical loss of semantic equivalence, as it has implications for protein translation and clinical interpretation.\n\n3.  **Verdict:** The phase information, which is explicit in the original HGVS description, is lost due to tool behavior and/or system architecture. The round-trip is definitively lossy. **Correct.**\n\n**E. An HGVS coding-level description $c.n\\textdelA$ within a tandem repeat is converted to normalized VCF, but both tools carry forward a metadata field that records the original HGVS anchor position and transcript version. Downstream HGVS regeneration explicitly uses that metadata to select the same anchor and transcript.**\n\n1.  **HGVS to VCF Conversion:**\n    -   As in Scenario A, the conversion of an indel in a repeat to a canonical VCF record is a many-to-one mapping, which would normally lose information about the original anchor point.\n    -   However, this scenario introduces a specific engineering solution: the \"lost\" information (original HGVS anchor position and transcript) is explicitly captured and stored in a metadata field.\n\n2.  **VCF to HGVS Conversion:**\n    -   The HGVS regeneration tool is designed to \"explicitly use that metadata\". This side-channel information allows the tool to bypass the ambiguity of the canonical VCF record and perfectly reconstruct the original HGVS string, using the original anchor and transcript.\n\n3.  **Verdict:** This scenario describes a system specifically designed to prevent information loss. By carrying the necessary semantic information in a metadata side-channel, the round-trip conversion is rendered lossless. **Incorrect.**\n\nBased on the analysis, the scenarios that are expected to lead to a lossy round-trip conversion are A, C, and D.", "answer": "$$\\boxed{ACD}$$", "id": "4361989"}]}