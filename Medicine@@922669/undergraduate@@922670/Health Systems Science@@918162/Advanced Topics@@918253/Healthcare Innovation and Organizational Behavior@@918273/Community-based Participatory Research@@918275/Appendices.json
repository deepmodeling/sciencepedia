{"hands_on_practices": [{"introduction": "Community-Based Participatory Research (CBPR) often involves interventions delivered to entire groups, such as neighborhoods or clinics, rather than to individuals one by one. This approach requires a specific study design known as a Cluster Randomized Trial (CRT). This practice will guide you through the essential calculation of the sample size needed for a CRT, accounting for the fact that individuals within a cluster are often more similar to each other than to individuals in other clusters. Mastering this skill is critical for designing studies that are statistically robust and make efficient use of community and research resources [@problem_id:4364528].", "problem": "A community-based participatory research (CBPR) partnership plans a two-arm cluster randomized trial to evaluate a neighborhood-led hypertension management program. Each neighborhood is a cluster. Assume equal cluster sizes, approximate normality of an individual-level outcome, and independence across clusters. Let $m$ be the number of clusters per arm, $n$ the number of individuals per cluster, $\\rho$ the intraclass correlation coefficient, and $\\delta$ a standardized effect size that equals the arm mean difference divided by the common individual-level standard deviation. Using only the following foundational elements:\n- The definition of the intraclass correlation coefficient $\\rho$ as the ratio of between-cluster variance to total variance, and the implied variance of a cluster mean under equal cluster sizes.\n- The properties of variances of averages and differences of independent random variables.\n- The normal approximation to the two-sample test for a difference in means at two-sided significance level $\\alpha$ and power $1-\\beta$.\nDerive, from first principles, an analytic expression for the required $m$ that achieves two-sided type I error $\\alpha$ and power $1-\\beta$ to detect a standardized effect size $\\delta$, given $\\rho$ and $n$. Then compute $m$ for the following planning values: $\\alpha = 0.05$, $1-\\beta = 0.9$, $n = 25$, $\\rho = 0.05$, and $\\delta = 0.3$. Report $m$ as the smallest integer satisfying the criterion, using the ceiling function, and present the final integer. No percentages are to be used anywhere in the answer.", "solution": "The problem as stated is a valid and well-posed problem in biostatistics, specifically in the design of cluster randomized trials. It requires the derivation of a sample size formula from first principles and subsequent numerical calculation. All necessary parameters and assumptions are provided, and the problem is scientifically grounded and objective.\n\nThe objective is to derive an expression for $m$, the number of clusters per arm, required to detect a standardized effect size $\\delta$ with a $2$-sided type I error rate of $\\alpha$ and power of $1-\\beta$.\n\nLet $\\sigma^2$ be the total variance of the individual-level outcome. The model of variance in a cluster design partitions this total variance into two components: the between-cluster variance, $\\sigma_b^2$, and the within-cluster variance, $\\sigma_w^2$. The relationship is $\\sigma^2 = \\sigma_b^2 + \\sigma_w^2$.\n\nThe intraclass correlation coefficient, $\\rho$, is defined as the ratio of the between-cluster variance to the total variance:\n$$\n\\rho = \\frac{\\sigma_b^2}{\\sigma^2}\n$$\nFrom this definition, we can express the variance components in terms of $\\sigma^2$ and $\\rho$:\n$$\n\\sigma_b^2 = \\rho \\sigma^2\n$$\n$$\n\\sigma_w^2 = \\sigma^2 - \\sigma_b^2 = \\sigma^2 - \\rho \\sigma^2 = (1-\\rho)\\sigma^2\n$$\n\nNow, consider the mean of a single cluster of size $n$, denoted $\\bar{Y}_{\\text{cluster}}$. The variance of this cluster mean is given by:\n$$\n\\text{Var}(\\bar{Y}_{\\text{cluster}}) = \\sigma_b^2 + \\frac{\\sigma_w^2}{n}\n$$\nSubstituting the expressions for $\\sigma_b^2$ and $\\sigma_w^2$:\n$$\n\\text{Var}(\\bar{Y}_{\\text{cluster}}) = \\rho \\sigma^2 + \\frac{(1-\\rho)\\sigma^2}{n} = \\sigma^2 \\left( \\rho + \\frac{1-\\rho}{n} \\right) = \\frac{\\sigma^2}{n} (n\\rho + 1 - \\rho)\n$$\nThis simplifies to:\n$$\n\\text{Var}(\\bar{Y}_{\\text{cluster}}) = \\frac{\\sigma^2}{n} [1 + (n-1)\\rho]\n$$\nThe term $[1 + (n-1)\\rho]$ is the variance inflation factor (VIF) or design effect, which quantifies the inflation in variance due to clustering.\n\nThe trial has $2$ arms, each with $m$ independent clusters. Let $\\bar{Y}_1$ and $\\bar{Y}_2$ be the overall mean outcomes for the intervention and control arms, respectively. Each arm mean is the average of its $m$ cluster means. The variance of an arm mean is:\n$$\n\\text{Var}(\\bar{Y}_1) = \\text{Var}\\left(\\frac{1}{m} \\sum_{j=1}^{m} \\bar{Y}_{1j}\\right) = \\frac{1}{m^2} \\sum_{j=1}^{m} \\text{Var}(\\bar{Y}_{1j}) = \\frac{1}{m} \\text{Var}(\\bar{Y}_{\\text{cluster}})\n$$\nSo, for both arms:\n$$\n\\text{Var}(\\bar{Y}_1) = \\text{Var}(\\bar{Y}_2) = \\frac{\\sigma^2}{mn} [1 + (n-1)\\rho]\n$$\nThe comparison of interest is the difference in arm means, $\\Delta = \\bar{Y}_1 - \\bar{Y}_2$. Since the arms are independent, the variance of the difference is the sum of the variances:\n$$\n\\text{Var}(\\Delta) = \\text{Var}(\\bar{Y}_1) + \\text{Var}(\\bar{Y}_2) = \\frac{2\\sigma^2}{mn} [1 + (n-1)\\rho]\n$$\nThe standard error of the difference is $SE(\\Delta) = \\sqrt{\\text{Var}(\\Delta)}$.\n\nThe hypothesis test is based on a normal approximation. The null hypothesis is $H_0: \\mu_1 - \\mu_2 = 0$. The alternative hypothesis is $H_A: \\mu_1 - \\mu_2 = D \\neq 0$, where $D$ is the true mean difference. The standardized effect size is given as $\\delta = D/\\sigma$, so $D = \\delta\\sigma$.\n\nThe general formula for the sample size in a $2$-sample test, relating power to the quantiles of the standard normal distribution, is:\n$$\n(z_{1-\\alpha/2} + z_{1-\\beta})^2 = \\frac{(\\text{Effect})^2}{\\text{Variance of Effect Estimate}}\n$$\nHere, the effect is the true mean difference $D$, and the variance of the effect estimate is $\\text{Var}(\\Delta)$.\n$$\n(z_{1-\\alpha/2} + z_{1-\\beta})^2 = \\frac{D^2}{\\text{Var}(\\Delta)}\n$$\nSubstituting the expressions for $D^2 = (\\delta\\sigma)^2$ and for $\\text{Var}(\\Delta)$:\n$$\n(z_{1-\\alpha/2} + z_{1-\\beta})^2 = \\frac{(\\delta\\sigma)^2}{\\frac{2\\sigma^2}{mn} [1 + (n-1)\\rho]}\n$$\nThe $\\sigma^2$ term cancels out:\n$$\n(z_{1-\\alpha/2} + z_{1-\\beta})^2 = \\frac{\\delta^2}{\\frac{2}{mn} [1 + (n-1)\\rho]} = \\frac{mn\\delta^2}{2[1 + (n-1)\\rho]}\n$$\nNow, we solve for $m$:\n$$\nm = \\frac{2(z_{1-\\alpha/2} + z_{1-\\beta})^2 [1 + (n-1)\\rho]}{n\\delta^2}\n$$\nThis is the derived analytic expression for the required number of clusters per arm, $m$.\n\nWe are given the following planning values: $\\alpha = 0.05$, $1-\\beta = 0.9$, $n = 25$, $\\rho = 0.05$, and $\\delta = 0.3$.\nFrom these values, we determine the corresponding standard normal quantiles:\nFor a $2$-sided $\\alpha = 0.05$, we need $z_{1-\\alpha/2} = z_{1-0.025} = z_{0.975} \\approx 1.95996$.\nFor power $1-\\beta = 0.9$, we need $z_{1-\\beta} = z_{0.9} \\approx 1.28155$.\n\nNow we substitute the numerical values into the formula for $m$:\n$$\nm = \\frac{2(z_{0.975} + z_{0.9})^2 [1 + (25-1)(0.05)]}{(25)(0.3)^2}\n$$\nFirst, calculate the components:\nThe variance inflation factor is $[1 + (n-1)\\rho] = [1 + (24)(0.05)] = 1 + 1.2 = 2.2$.\nThe denominator term is $n\\delta^2 = 25 \\times (0.3)^2 = 25 \\times 0.09 = 2.25$.\nThe sum of the quantiles is $z_{0.975} + z_{0.9} \\approx 1.95996 + 1.28155 = 3.24151$.\nThe squared sum is $(3.24151)^2 \\approx 10.50735$.\n\nPlugging these into the equation for $m$:\n$$\nm \\approx \\frac{2 (10.50735) (2.2)}{2.25} = \\frac{46.23234}{2.25} \\approx 20.5477\n$$\nThe problem requires $m$ to be an integer, representing a number of clusters. Since we must achieve at least the specified power, we need to take the ceiling of this value.\n$$\nm = \\lceil 20.5477 \\rceil = 21\n$$\nThus, $21$ clusters are required in each arm of the trial.", "answer": "$$\\boxed{21}$$", "id": "4364528"}, {"introduction": "A core tenet of CBPR is the meaningful involvement of community members in all phases of the research, including the collection of data. To ensure that the study's findings are trustworthy and valid, it is vital to assess the consistency of data gathered by different individuals. This exercise provides hands-on practice in calculating a fundamental metric of data quality: the chance-corrected inter-rater reliability, or Cohen's Kappa (${\\kappa}$). Performing this calculation will help you understand how to quantify the level of agreement between raters, a crucial step in ensuring the integrity of data in any community-partnered project [@problem_id:4578943].", "problem": "In a Community-Based Participatory Research (CBPR) environmental health project, pairs of trained community data collectors independently classified each household’s dust hazard severity into one of $3$ ordered categories: low risk $(L)$, moderate risk $(M)$, and high risk $(H)$. For a single instrument reliability audit, one representative pair (Rater A and Rater B) independently assessed the same set of households. Their joint classifications are summarized by the following $3 \\times 3$ contingency counts, where rows correspond to Rater A’s category and columns correspond to Rater B’s category:\n- $c_{LL} = 48$, $c_{LM} = 6$, $c_{LH} = 3$\n- $c_{ML} = 8$, $c_{MM} = 32$, $c_{MH} = 5$\n- $c_{HL} = 2$, $c_{HM} = 9$, $c_{HH} = 10$\n\nUsing first principles from probability and measurement in epidemiology, derive the chance-corrected inter-rater agreement coefficient for these categorical assessments by:\n- Defining and computing the observed agreement probability from the contingency data.\n- Constructing the expected agreement probability under independence from the marginal category distributions.\n- Normalizing the observed agreement beyond chance by the maximum possible agreement beyond chance to obtain the coefficient.\n\nExpress the final coefficient as a decimal number rounded to four significant figures.", "solution": "The problem is deemed valid as it is scientifically grounded, well-posed, objective, and contains all necessary information to derive the requested statistical coefficient. The procedure of calculating a chance-corrected agreement coefficient (Cohen's Kappa) is a standard and well-defined method in epidemiology and related fields.\n\nThe problem requires the derivation of a chance-corrected inter-rater agreement coefficient. This coefficient is commonly known as Cohen's Kappa ($\\kappa$). It measures the agreement between two raters, accounting for the agreement that would be expected purely by chance. The derivation will follow the three steps outlined in the problem statement.\n\nFirst, we organize the provided data into a $3 \\times 3$ contingency table. Let the rows represent the ratings of Rater A and the columns represent the ratings of Rater B. The categories are Low risk ($L$), Moderate risk ($M$), and High risk ($H$). The cell counts, $c_{ij}$, represent the number of households classified in category $i$ by Rater A and category $j$ by Rater B.\n\nThe contingency table is:\n$$\n\\begin{array}{c|ccc|c}\n\\text{Rater A} & \\text{Rater B: } L & \\text{Rater B: } M & \\text{Rater B: } H & \\text{Row Total} \\\\\n\\hline\nL & 48 & 6 & 3 & r_L = 57 \\\\\nM & 8 & 32 & 5 & r_M = 45 \\\\\nH & 2 & 9 & 10 & r_H = 21 \\\\\n\\hline\n\\text{Col Total} & c_L = 58 & c_M = 47 & c_H = 18 & N = 123\n\\end{array}\n$$\n\nThe total number of households assessed, $N$, is the sum of all cell counts:\n$$N = c_{LL} + c_{LM} + c_{LH} + c_{ML} + c_{MM} + c_{MH} + c_{HL} + c_{HM} + c_{HH}$$\n$$N = 48 + 6 + 3 + 8 + 32 + 5 + 2 + 9 + 10 = 123$$\nWe also calculate the row totals ($r_i$) and column totals ($c_j$), which represent the marginal frequencies for each rater.\nRow totals:\n$r_L = 48+6+3 = 57$\n$r_M = 8+32+5 = 45$\n$r_H = 2+9+10 = 21$\nSum of row totals: $57+45+21 = 123 = N$.\n\nColumn totals:\n$c_L = 48+8+2 = 58$\n$c_M = 6+32+9 = 47$\n$c_H = 3+5+10 = 18$\nSum of column totals: $58+47+18 = 123 = N$.\n\nStep 1: Define and compute the observed agreement probability ($p_o$).\nThe observed agreement is the proportion of households for which both raters assigned the same category. These are the counts along the main diagonal of the contingency table.\nThe number of agreements is the sum of diagonal elements:\n$$N_{agree} = c_{LL} + c_{MM} + c_{HH} = 48 + 32 + 10 = 90$$\nThe observed probability of agreement, $p_o$, is the ratio of the number of agreements to the total number of observations:\n$$p_o = \\frac{N_{agree}}{N} = \\frac{90}{123}$$\n\nStep 2: Construct the expected agreement probability ($p_e$) under independence.\nThe expected probability of agreement by chance is calculated based on the marginal distributions of the raters' classifications. Under the assumption of independence, the probability that both raters classify a household into a specific category $k$ is the product of their individual probabilities of choosing that category.\nThe probability for Rater A to choose category $k$ is $p_{A,k} = \\frac{r_k}{N}$.\nThe probability for Rater B to choose category $k$ is $p_{B,k} = \\frac{c_k}{N}$.\nThe total expected probability of agreement, $p_e$, is the sum of the probabilities of chance agreement for each category:\n$$p_e = \\sum_{k \\in \\{L, M, H\\}} p_{A,k} \\cdot p_{B,k} = p_{A,L}p_{B,L} + p_{A,M}p_{B,M} + p_{A,H}p_{B,H}$$\n$$p_e = \\left(\\frac{r_L}{N}\\right)\\left(\\frac{c_L}{N}\\right) + \\left(\\frac{r_M}{N}\\right)\\left(\\frac{c_M}{N}\\right) + \\left(\\frac{r_H}{N}\\right)\\left(\\frac{c_H}{N}\\right)$$\nSubstituting the numerical values:\n$$p_e = \\frac{1}{N^2} (r_L c_L + r_M c_M + r_H c_H)$$\n$$p_e = \\frac{1}{123^2} ((57)(58) + (45)(47) + (21)(18))$$\n$$p_e = \\frac{1}{15129} (3306 + 2115 + 378)$$\n$$p_e = \\frac{5799}{15129}$$\n\nStep 3: Normalize to obtain the coefficient.\nThe problem defines the coefficient as the normalization of \"observed agreement beyond chance\" by the \"maximum possible agreement beyond chance\".\nThe observed agreement beyond chance is the difference between the observed agreement and the expected agreement: $p_o - p_e$.\nThe maximum possible agreement is $1$. Therefore, the maximum possible agreement beyond chance is $1 - p_e$.\nThe coefficient, $\\kappa$, is the ratio of these two quantities:\n$$\\kappa = \\frac{p_o - p_e}{1 - p_e}$$\nNow, we substitute the calculated values of $p_o$ and $p_e$:\n$$p_o = \\frac{90}{123} = \\frac{90 \\times 123}{123 \\times 123} = \\frac{11070}{15129}$$\n$$p_e = \\frac{5799}{15129}$$\nNow substitute these into the kappa formula:\n$$\\kappa = \\frac{\\frac{11070}{15129} - \\frac{5799}{15129}}{1 - \\frac{5799}{15129}} = \\frac{\\frac{11070 - 5799}{15129}}{\\frac{15129 - 5799}{15129}}$$\n$$\\kappa = \\frac{5271}{9330}$$\nTo obtain the final numerical answer, we perform the division and round to four significant figures:\n$$\\kappa = 0.564951768...$$\nRounding to four significant figures, we get:\n$$\\kappa \\approx 0.5650$$\nThe trailing zero is significant.", "answer": "$$\\boxed{0.5650}$$", "id": "4578943"}, {"introduction": "The CBPR principles of co-ownership of data and equitable partnership are foundational, but they can also create complex ethical and governance challenges. Tensions may arise when academic requirements, such as publication deadlines, conflict with community priorities, like preventing potential harm from the dissemination of findings. This case study presents a realistic ethical dilemma involving the risk of community stigmatization from research maps, forcing a choice between honoring a partnership agreement and meeting a funder's timeline. Working through this scenario will strengthen your ability to apply core ethical principles and regulatory frameworks to navigate the real-world complexities of community-partnered research [@problem_id:4578990].", "problem": "A university-based epidemiology team partners with a neighborhood coalition using Community-Based Participatory Research (CBPR) principles to study substance use and overdose patterns. The partnership agreement, formalized in a Memorandum of Understanding (MOU), states that the community coalition and the university will co-own study data and that any public dissemination (presentations, reports, journal submissions) will be drafted collaboratively and reviewed by the coalition prior to release. The university’s Institutional Review Board (IRB) has approved the protocol under the United States Common Rule (Title 45 Code of Federal Regulations section 46), including consent language describing how results will be shared with both academic and community audiences.\n\nDuring analysis, the team identifies that maps showing overdose rates by small areas could stigmatize a particular neighborhood if published as-is. The principal investigator intends to submit a manuscript with these maps to a journal to meet a funder’s dissemination timeline, while the community coalition requests a pause to co-develop mitigation strategies (for example, contextualizing findings with asset framing, aggregating to larger geographies, and coordinating with local leaders). There is disagreement about whether the IRB or the community coalition has authority over when and how dissemination occurs, and about who “owns” the analyzed data.\n\nUsing core ethical principles from the Belmont Report (Respect for Persons, Beneficence, Justice) and the regulatory scope of the Common Rule, which option best characterizes the roles and potential tensions between academic IRBs and community review boards in CBPR, and identifies the action that is most aligned with those principles and regulations?\n\nA. The academic IRB has sole jurisdiction over data ownership and dissemination; once IRB approval is granted, the principal investigator may disseminate without regard to the community coalition’s position.\n\nB. The community review board holds regulatory authority equivalent to an IRB under the Common Rule and can unilaterally block dissemination and override the consent language approved by the IRB.\n\nC. The IRB’s jurisdiction is limited to human subjects protections (for example, consent validity, risk minimization, privacy and confidentiality), not contractual data ownership; the community coalition’s authority arises from governance agreements (for example, an MOU) that define co-ownership and co-dissemination. The ethically appropriate action is to honor the agreed terms, collaboratively negotiate a dissemination plan that minimizes community harms consistent with Beneficence and Justice, and submit an IRB amendment if the dissemination plan deviates from what was approved.\n\nD. Neither the IRB nor the community coalition has authority once data are de-identified; the principal investigator can proceed because de-identification eliminates all ethical concerns.\n\nE. The IRB must consider stigmatization risk only if it could lead to physical harm; reputational or social harms are outside IRB scope, so the coalition’s concern is not an IRB matter.", "solution": "This problem statement is valid. It presents a realistic and well-defined ethical and regulatory conflict in the context of Community-Based Participatory Research (CBPR), grounded in established principles and regulations.\n\nThe problem requires an analysis of the respective authorities of an Institutional Review Board (IRB) and a community partner, as well as the application of core ethical principles to a dissemination conflict. The core of the problem lies in the intersection of three frameworks: the Belmont Report's ethical principles, the U.S. Common Rule's regulatory scope (Title 45 Code of Federal Regulations section 46), and the specific governance agreement (Memorandum of Understanding, MOU) established in a CBPR project.\n\nFirst, let us define the roles and purviews of the key entities.\n\n1.  **The Belmont Report**: This report established three fundamental ethical principles for research involving human subjects.\n    *   **Respect for Persons**: This principle requires treating individuals as autonomous agents and protecting those with diminished autonomy. In the context of CBPR, this principle is extended to the community partner, recognizing their autonomy and role as an equal collaborator, which is formalized in the MOU. The original consent process respects the autonomy of individual participants. The PI's intent to publish without the coalition's agreement disrespects the autonomy of the community partner.\n    *   **Beneficence**: This principle obligates researchers to do no harm and to maximize benefits while minimizing potential harms. The potential for stigmatization of a neighborhood is a significant social harm. The community coalition's request to pause and develop mitigation strategies is a direct application of this principle—to minimize harm. The PI's focus on a dissemination timeline at the risk of causing community harm contravenes this principle.\n    *   **Justice**: This principle concerns the fair distribution of the burdens and benefits of research. If the research results in the stigmatization of a specific neighborhood, that community bears a significant burden. The benefits (a publication, fulfillment of funder requirements) would accrue primarily to the academic researcher. This imbalance raises a clear issue of justice. A just approach would ensure that the community is not disproportionately harmed for the benefit of the academic partner.\n\n2.  **The Common Rule (45 CFR 46) and the IRB**: The Common Rule provides federal regulations for the protection of human subjects in research. The Institutional Review Board (IRB) is the body charged with implementing these regulations.\n    *   The IRB's jurisdiction is focused on the rights and welfare of **human subjects**. Its responsibilities include, but are not limited to, ensuring that risks to subjects are minimized, that risks are reasonable in relation to anticipated benefits, that informed consent is sought and properly documented, and that provisions are made to protect the privacy of subjects and maintain the confidentiality of data.\n    *   The definition of \"risk\" that an IRB must consider is broad and includes physical, psychological, social, economic, and legal harms. Stigmatization is a well-recognized social harm and is therefore squarely within the IRB's purview of risk assessment.\n    *   Crucially, the IRB's authority does not typically extend to matters of contractual agreements, intellectual property, or data ownership. These are legal and institutional matters often handled through separate agreements, such as the MOU in this case.\n\n3.  **The Memorandum of Understanding (MOU)**: This is a formal governance agreement that defines the terms of the partnership between the university team and the community coalition. Based on the problem description, the MOU explicitly states that data are co-owned and that dissemination must be collaborative and subject to coalition review. This agreement is contractually and ethically binding on the partners. It is the practical embodiment of the CBPR principle of equitable partnership.\n\nWith these frameworks established, we can analyze the conflict. The PI's desire to publish is in direct conflict with the MOU, which grants the community coalition co-ownership and review authority over dissemination. The coalition's concern about stigmatization is directly related to the ethical principles of Beneficence and Justice, and is a valid risk for IRB consideration. The disagreement is over which authority—the IRB's initial approval or the MOU's governance terms—takes precedence. The answer is that they are separate but complementary authorities. IRB approval does not nullify other binding legal or ethical agreements.\n\nNow we evaluate each option:\n\n**A. The academic IRB has sole jurisdiction over data ownership and dissemination; once IRB approval is granted, the principal investigator may disseminate without regard to the community coalition’s position.**\nThis statement is incorrect. The IRB's jurisdiction is not \"sole\" and does not typically cover data ownership, which is a contractual/legal matter. The MOU explicitly defines data co-ownership. Ignoring the MOU is a breach of the research agreement and is inconsistent with the ethical principles of CBPR and Respect for Persons (as applied to the community partner). IRB approval of a protocol does not grant the PI license to violate other binding agreements.\n\n**B. The community review board holds regulatory authority equivalent to an IRB under the Common Rule and can unilaterally block dissemination and override the consent language approved by the IRB.**\nThis statement is incorrect. A community review board does not have regulatory authority *under the Common Rule*; that power is vested in the IRB. The community board's authority stems from the specific, mutually-agreed-upon terms of the MOU, which is a governance/contractual instrument, not a federal regulation. It cannot \"override\" IRB-approved language, though in a true partnership, it would have been involved in drafting that language. Its power to influence or \"block\" dissemination comes from the MOU's terms of co-ownership and co-dissemination, not from an equivalent regulatory status.\n\n**C. The IRB’s jurisdiction is limited to human subjects protections (for example, consent validity, risk minimization, privacy and confidentiality), not contractual data ownership; the community coalition’s authority arises from governance agreements (for example, an MOU) that define co-ownership and co-dissemination. The ethically appropriate action is to honor the agreed terms, collaboratively negotiate a dissemination plan that minimizes community harms consistent with Beneficence and Justice, and submit an IRB amendment if the dissemination plan deviates from what was approved.**\nThis statement is correct. It accurately distinguishes between the IRB's regulatory role (human subjects protection) and the community coalition's governance role derived from the MOU (co-ownership, co-dissemination). The proposed action is fully aligned with ethical principles: honoring the MOU shows Respect for Persons (the partner); negotiating to mitigate harm upholds Beneficence; and preventing undue burden on the community reflects Justice. The final point about submitting an IRB amendment for a modified dissemination plan demonstrates a correct understanding of the regulatory process, ensuring that the formal human subjects protection plan on record remains accurate.\n\n**D. Neither the IRB nor the community coalition has authority once data are de-identified; the principal investigator can proceed because de-identification eliminates all ethical concerns.**\nThis statement is incorrect. First, de-identification does not eliminate all ethical concerns. Geographically specific data, even if anonymized at the individual level, can lead to group harms like community stigmatization. This is a central issue in this scenario. Second, de-identification does not nullify the MOU. The agreement on co-ownership and co-dissemination applies to the data generated by the partnership, regardless of its identification status. The community coalition's authority, derived from the MOU, remains fully in effect.\n\n**E. The IRB must consider stigmatization risk only if it could lead to physical harm; reputational or social harms are outside IRB scope, so the coalition’s concern is not an IRB matter.**\nThis statement is incorrect. The scope of risk that IRBs must assess under the Common Rule is broad and explicitly includes social and psychological harms. Stigmatization is a classic example of a social harm that can have profound negative consequences for a community and its members. Therefore, this concern is definitively an IRB matter related to the principle of Beneficence and the requirement to minimize risk.\n\nBased on this analysis, option C provides the most accurate and ethically sound characterization of the situation and the appropriate path forward.", "answer": "$$\\boxed{C}$$", "id": "4578990"}]}