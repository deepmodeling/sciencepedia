{"hands_on_practices": [{"introduction": "The Radiomics Hypothesis suggests that image features can track biological processes as they evolve. This exercise challenges you to translate this abstract idea into a concrete algorithm. You will develop a method to test whether a feature's change over time is consistent with a genuine biological signal, focusing on three key properties: a clear directional trend, temporal smoothness, and a strong signal relative to measurement noise [@problem_id:4567515].", "problem": "Consider the Radiomics Hypothesis, which posits that quantitative image-derived features are measurable functions that encode underlying biological states and that their temporal evolution under a controlled intervention is coherent, smooth, and detectable beyond measurement noise. In mathematical terms, let a radiomics feature be a real-valued time series $x(t)$ observed at discrete time points $t_0, t_1, \\dots, t_{N-1}$ in days. The measured feature values are modeled as $x_i = s_i + n_i$ for $i \\in \\{0,1,\\dots,N-1\\}$, where $s_i$ denotes the latent biological signal and $n_i$ denotes measurement noise. Assume $n_i$ are independent and identically distributed random variables drawn from a Normal distribution with zero mean and known variance $\\sigma_n^2$.\n\nA temporal radiomics test should accept a series as consistent with the Radiomics Hypothesis when three criteria hold simultaneously:\n\n1. There is a strong monotonic association between the feature values and time in an expected direction $d \\in \\{-1, +1\\}$, where $d = +1$ encodes expected increase and $d = -1$ encodes expected decrease.\n2. The series is temporally smooth in the sense of having low discrete curvature relative to its overall variability.\n3. The signal-to-noise ratio is sufficiently high to indicate that the observed variability is dominated by the latent biological signal rather than measurement noise.\n\nStarting from these fundamental bases:\n- The definition of monotonic association based on pairwise order relationships between $(t_i, x_i)$.\n- The discrete second difference as a finite-difference proxy for curvature, defined on uniformly or nonuniformly sampled series using index-local differences.\n- Variance decomposition for additive noise $x = s + n$ with $n$ independent of $s$, implying $\\operatorname{Var}(x) = \\operatorname{Var}(s) + \\sigma_n^2$.\n\nDesign an algorithm that, for each test case, decides acceptance or rejection according to a decision rule that operationalizes the three criteria above. Use the following fixed thresholds for all cases: minimum monotonic association threshold $\\tau_{\\min} = 0.8$, maximum normalized curvature energy $E_{\\max} = 0.25$, and minimum Signal-to-Noise Ratio (SNR) threshold $\\mathrm{SNR}_{\\min} = 1.0$.\n\nYou are given five test cases. For each case, the inputs are the time points $t$, feature values $x$, the expected direction $d$, and the noise standard deviation $\\sigma_n$. All time values are in days; angles are not involved. Implement the decision rule and produce a boolean for each case: $\\text{True}$ if the series is accepted and $\\text{False}$ otherwise. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\text{True},\\text{False},\\dots]$).\n\nTest Suite:\n- Case $1$ (smooth decreasing series with weekly sampling):\n  - $t = [0, 7, 14, 21, 28, 35]$\n  - $x = [1.00, 0.86, 0.73, 0.62, 0.52, 0.45]$\n  - $d = -1$\n  - $\\sigma_n = 0.02$\n- Case $2$ (near-threshold non-monotonicity):\n  - $t = [0, 7, 14, 21, 28, 35]$\n  - $x = [1.00, 0.90, 0.88, 0.87, 0.89, 0.88]$\n  - $d = -1$\n  - $\\sigma_n = 0.02$\n- Case $3$ (constant series):\n  - $t = [0, 5, 10, 15, 20]$\n  - $x = [0.70, 0.70, 0.70, 0.70, 0.70]$\n  - $d = -1$\n  - $\\sigma_n = 0.02$\n- Case $4$ (oscillatory series with roughly triweekly sampling):\n  - $t = [0, 4, 8, 12, 16, 20, 24]$\n  - $x = [0.90, 1.00, 0.85, 1.05, 0.80, 1.10, 0.75]$\n  - $d = -1$\n  - $\\sigma_n = 0.03$\n- Case $5$ (smooth increasing series with approximately biweekly sampling):\n  - $t = [0, 6, 12, 18, 24, 30]$\n  - $x = [0.50, 0.58, 0.66, 0.75, 0.83, 0.90]$\n  - $d = +1$\n  - $\\sigma_n = 0.02$\n\nYour program must implement the decision rule based on first principles as described above and compute the boolean outcomes for the five cases. The final output must be a single line containing the list of booleans in the exact format $[\\text{result1},\\text{result2},\\text{result3},\\text{result4},\\text{result5}]$.", "solution": "The problem statement has been analyzed and is deemed valid. It is scientifically grounded in the principles of radiomics and signal processing, is well-posed with clearly defined inputs and objectives, and is expressed in objective, formalizable language. The task is to operationalize three criteria for evaluating a temporal radiomics series and apply them to a set of test cases.\n\nThe algorithm will assess each time series $x(t)$ observed at points $t_0, t_1, \\dots, t_{N-1}$ against three criteria: monotonic association, temporal smoothness, and signal-to-noise ratio. A series is accepted as consistent with the Radiomics Hypothesis if and only if it satisfies all three criteria simultaneously, using the provided thresholds.\n\nLet the given data for a series be the time points $t$, the feature values $x$, the expected direction of change $d$, and the noise standard deviation $\\sigma_n$. The number of observations is $N$. The thresholds are $\\tau_{\\min} = 0.8$, $E_{\\max} = 0.25$, and $\\mathrm{SNR}_{\\min} = 1.0$.\n\n### Criterion 1: Monotonic Association\n\nThis criterion evaluates if the series shows a strong monotonic trend in the expected direction $d \\in \\{-1, +1\\}$. It is operationalized by calculating a concordance score, $M$, based on pairwise order relationships. A pair of observations $(x_i, x_j)$ with $i  j$ (and thus $t_i  t_j$) is considered concordant with the expected direction $d$ if the sign of the change in $x$ matches the direction. Mathematically, a pair is concordant if $(x_j - x_i)d  0$.\n\nThe metric $M$ is defined as the fraction of concordant pairs out of all possible unique pairs of time points.\nLet $N$ be the number of observations. The total number of pairs $(i,j)$ with $i  j$ is $N_p = \\frac{N(N-1)}{2}$.\nLet $N_c$ be the number of concordant pairs, given by:\n$$\nN_c = \\sum_{i=0}^{N-2} \\sum_{j=i+1}^{N-1} \\mathbf{1}[(x_j - x_i)d  0]\n$$\nwhere $\\mathbf{1}[\\cdot]$ is the indicator function. The monotonic association metric is:\n$$\nM = \\frac{N_c}{N_p}\n$$\nThe series passes this criterion if $M \\ge \\tau_{\\min}$.\n\n### Criterion 2: Temporal Smoothness\n\nThis criterion assesses whether the series is smooth, meaning it has low curvature. Curvature is approximated by the discrete second difference, calculated using index-local differences as specified. The metric, Normalized Curvature Energy $E$, relates the energy of the curvature to the overall variability of the series.\n\nThe discrete second difference at index $i$ is $\\Delta^2 x_i = x_{i+2} - 2x_{i+1} + x_i$. This is computed for $i \\in \\{0, 1, \\dots, N-3\\}$.\nThe curvature energy, $N_E$, is the sum of the squares of these second differences:\n$$\nN_E = \\sum_{i=0}^{N-3} (\\Delta^2 x_i)^2 = \\sum_{i=0}^{N-3} (x_{i+2} - 2x_{i+1} + x_i)^2\n$$\nThis energy is normalized by the total energy of the series' variability, $D_E$, which is the sum of squared deviations from the mean $\\bar{x}$:\n$$\nD_E = \\sum_{i=0}^{N-1} (x_i - \\bar{x})^2, \\quad \\text{where } \\bar{x} = \\frac{1}{N}\\sum_{k=0}^{N-1} x_k\n$$\nThe Normalized Curvature Energy is their ratio:\n$$\nE = \\frac{N_E}{D_E}\n$$\nIn the special case where the series is constant, $D_E=0$. In this case, the numerator $N_E$ is also $0$, as a constant series has zero curvature. We define $E=0$ for a constant series, reflecting perfect smoothness.\nThe series passes this criterion if $E \\le E_{\\max}$.\n\n### Criterion 3: Signal-to-Noise Ratio (SNR)\n\nThis criterion ensures that the variability in the latent biological signal $s$ is substantially larger than the measurement noise $n$. The model is $x_i = s_i + n_i$. Given the independence of signal and noise, their variances add: $\\operatorname{Var}(x) = \\operatorname{Var}(s) + \\operatorname{Var}(n)$. The noise variance $\\sigma_n^2$ is known.\n\nWe first estimate the variance of the observed series $x$, using the unbiased sample variance:\n$$\n\\hat{\\sigma}_x^2 = \\frac{1}{N-1} \\sum_{i=0}^{N-1} (x_i - \\bar{x})^2\n$$\nFrom this, we estimate the variance of the latent signal $s$:\n$$\n\\hat{\\sigma}_s^2 = \\hat{\\sigma}_x^2 - \\sigma_n^2\n$$\nSince variance cannot be negative, we enforce this by taking $\\hat{\\sigma}_s^2 = \\max(0, \\hat{\\sigma}_x^2 - \\sigma_n^2)$.\nThe Signal-to-Noise Ratio (SNR) is the ratio of the estimated signal variance to the known noise variance:\n$$\n\\mathrm{SNR} = \\frac{\\hat{\\sigma}_s^2}{\\sigma_n^2} = \\frac{\\max(0, \\hat{\\sigma}_x^2 - \\sigma_n^2)}{\\sigma_n^2}\n$$\nThis assumes $\\sigma_n  0$, which holds for all test cases.\nThe series passes this criterion if $\\mathrm{SNR} \\ge \\mathrm{SNR}_{\\min}$.\n\n### Final Decision Rule\n\nA test case is accepted, and its result is a boolean `True`, if and only if all three conditions are satisfied:\n$$\n(M \\ge \\tau_{\\min}) \\land (E \\le E_{\\max}) \\land (\\mathrm{SNR} \\ge \\mathrm{SNR}_{\\min})\n$$\nOtherwise, the result is `False`. The algorithm applies this logic to each of the provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the radiomics hypothesis test on the given suite of cases.\n    \"\"\"\n    \n    # Define the fixed thresholds from the problem statement.\n    TAU_MIN = 0.8\n    E_MAX = 0.25\n    SNR_MIN = 1.0\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: smooth decreasing series with weekly sampling\n        {\n            \"t\": [0, 7, 14, 21, 28, 35],\n            \"x\": [1.00, 0.86, 0.73, 0.62, 0.52, 0.45],\n            \"d\": -1,\n            \"sigma_n\": 0.02\n        },\n        # Case 2: near-threshold non-monotonicity\n        {\n            \"t\": [0, 7, 14, 21, 28, 35],\n            \"x\": [1.00, 0.90, 0.88, 0.87, 0.89, 0.88],\n            \"d\": -1,\n            \"sigma_n\": 0.02\n        },\n        # Case 3: constant series\n        {\n            \"t\": [0, 5, 10, 15, 20],\n            \"x\": [0.70, 0.70, 0.70, 0.70, 0.70],\n            \"d\": -1,\n            \"sigma_n\": 0.02\n        },\n        # Case 4: oscillatory series with roughly triweekly sampling\n        {\n            \"t\": [0, 4, 8, 12, 16, 20, 24],\n            \"x\": [0.90, 1.00, 0.85, 1.05, 0.80, 1.10, 0.75],\n            \"d\": -1,\n            \"sigma_n\": 0.03\n        },\n        # Case 5: smooth increasing series with approximately biweekly sampling\n        {\n            \"t\": [0, 6, 12, 18, 24, 30],\n            \"x\": [0.50, 0.58, 0.66, 0.75, 0.83, 0.90],\n            \"d\": +1,\n            \"sigma_n\": 0.02\n        }\n    ]\n\n    def evaluate_series(x_vals, d, sigma_n):\n        \"\"\"\n        Evaluates a single time series against the three criteria.\n        \n        Args:\n            x_vals (list): The feature values.\n            d (int): The expected direction of change (-1 or +1).\n            sigma_n (float): The noise standard deviation.\n\n        Returns:\n            bool: True if the series is accepted, False otherwise.\n        \"\"\"\n        x = np.array(x_vals, dtype=float)\n        n = len(x)\n\n        if n  2:\n            return False\n\n        # Criterion 1: Monotonic Association\n        num_pairs = n * (n - 1) / 2\n        if num_pairs == 0:\n            monotonicity_score = 0.0\n        else:\n            concordant_pairs = 0\n            for i in range(n):\n                for j in range(i + 1, n):\n                    if (x[j] - x[i]) * d  0:\n                        concordant_pairs += 1\n            monotonicity_score = concordant_pairs / num_pairs\n\n        c1_pass = (monotonicity_score = TAU_MIN)\n\n        # Criterion 2: Temporal Smoothness (Normalized Curvature Energy)\n        if n  3:\n            # Curvature is not defined for less than 3 points\n            # A series of 2 points is perfectly smooth (a line).\n            norm_curvature_energy = 0.0\n        else:\n            x_mean = np.mean(x)\n            denominator_energy = np.sum((x - x_mean)**2)\n            \n            if denominator_energy == 0:\n                # Perfectly constant series has zero curvature.\n                norm_curvature_energy = 0.0\n            else:\n                second_diffs = x[2:] - 2 * x[1:-1] + x[:-2]\n                numerator_energy = np.sum(second_diffs**2)\n                norm_curvature_energy = numerator_energy / denominator_energy\n\n        c2_pass = (norm_curvature_energy = E_MAX)\n\n        # Criterion 3: Signal-to-Noise Ratio (SNR)\n        noise_var = sigma_n**2\n        if noise_var == 0:\n             # If no noise is expected, any variance is signal\n             # This case is not in the test suite but is handled for robustness.\n             snr = float('inf')\n        else:\n            # Use ddof=1 for unbiased sample variance\n            obs_var = np.var(x, ddof=1) if n  1 else 0.0\n            signal_var = max(0, obs_var - noise_var)\n            snr = signal_var / noise_var\n            \n        c3_pass = (snr = SNR_MIN)\n        \n        return c1_pass and c2_pass and c3_pass\n\n    results = []\n    for case in test_cases:\n        # Time 't' is not explicitly used in the metric calculations as per\n        # problem interpretation (e.g., \"index-local differences\").\n        result = evaluate_series(case[\"x\"], case[\"d\"], case[\"sigma_n\"])\n        results.append(result)\n    \n    # Format the final output as a string list of booleans.\n    # JSON-style boolean format in Python is \"True\" / \"False\".\n    # The problem asks for this format, e.g. `[True,False,...]`.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "4567515"}, {"introduction": "Beyond finding correlations, a key goal in science is to understand causal relationships. In medical studies, the effect of a treatment can be confounded by a patient's underlying biology, and a radiomic feature might serve as a measurable proxy for this hidden factor. This exercise delves into the theory of causal inference, asking you to derive and compute the bias that arises when using such a proxy for adjustment, revealing the conditions under which this practice helps or hinders the estimation of a true causal effect [@problem_id:4567522].", "problem": "Consider the Radiomics Hypothesis, which posits that quantitative image-derived features capture information about underlying biological states relevant to disease processes. Formalize this idea using a linear Gaussian Structural Causal Model (SCM), and analyze confounding and adjustment using a noisy radiomic proxy. Let the latent biological state be a scalar random variable $B$, let treatment be a scalar random variable $T$, let the radiomic feature be a scalar random variable $R$, and let the outcome be a scalar random variable $Y$. Assume zero-mean variables and mutually independent noise terms. The SCM is:\n$$\nB \\sim \\mathcal{N}(0,\\sigma_B^2), \\quad U_T \\sim \\mathcal{N}(0,\\sigma_T^2), \\quad U_Y \\sim \\mathcal{N}(0,\\sigma_Y^2), \\quad \\varepsilon_R \\sim \\mathcal{N}(0,\\sigma_R^2),\n$$\nwith structural equations\n$$\nT = \\alpha B + U_T, \\quad R = B + \\varepsilon_R, \\quad Y = \\tau T + \\gamma B + U_Y.\n$$\nHere, $R$ is a noisy proxy of the latent biology $B$, consistent with the Radiomics Hypothesis that radiomic features encode latent biological information. The parameter $\\tau$ is the Average Treatment Effect (ATE), and we seek to estimate $\\tau$ from observational data. You will derive the bias of two estimators:\n1. The naive Ordinary Least Squares (OLS) estimator of regressing $Y$ on $T$ only.\n2. The adjusted OLS estimator of regressing $Y$ on both $T$ and $R$.\n\nBegin from the following fundamental base:\n- Definitions of expectation, variance, and covariance, and the linearity of expectation.\n- For any zero-mean random variables $X$ and $Z$, $\\operatorname{Var}(X) = \\mathbb{E}[X^2]$ and $\\operatorname{Cov}(X,Z) = \\mathbb{E}[XZ]$.\n- For linear Gaussian models with independent noises, covariances are obtained by linear propagation and independence assumptions.\n- The well-tested OLS identities: for a simple regression of $Y$ on $T$, the coefficient is $\\beta_T = \\operatorname{Cov}(Y,T)/\\operatorname{Var}(T)$. For the multiple regression of $Y$ on $(T,R)$, the coefficient on $T$ is\n$$\n\\beta_T^{\\text{adj}} = \\frac{\\operatorname{Cov}(Y,T)\\operatorname{Var}(R) - \\operatorname{Cov}(Y,R)\\operatorname{Cov}(T,R)}{\\operatorname{Var}(T)\\operatorname{Var}(R) - \\operatorname{Cov}(T,R)^2}.\n$$\n\nYour tasks:\n- Derive, from first principles using the base above, closed-form expressions for $\\operatorname{Var}(T)$, $\\operatorname{Var}(R)$, $\\operatorname{Cov}(B,T)$, $\\operatorname{Cov}(T,R)$, $\\operatorname{Cov}(Y,T)$, and $\\operatorname{Cov}(Y,R)$ in terms of $(\\alpha, \\tau, \\gamma, \\sigma_B^2, \\sigma_T^2, \\sigma_R^2)$.\n- Using these, derive the naive estimator $\\beta_T$ and compute its bias $b_{\\text{naive}} = \\beta_T - \\tau$.\n- Using the multiple regression identity above, derive the adjusted estimator $\\beta_T^{\\text{adj}}$ and compute its bias $b_{\\text{adj}} = \\beta_T^{\\text{adj}} - \\tau$.\n- Implement a program that computes, for each provided test case parameter set, the pair of biases $(b_{\\text{naive}}, b_{\\text{adj}})$ and a boolean indicating whether adjustment using the radiomic proxy reduces the absolute bias, i.e., whether $|b_{\\text{adj}}|  |b_{\\text{naive}}|$.\n\nNumerical test suite:\nUse the following parameter sets, each specified as $(\\alpha, \\tau, \\gamma, \\sigma_B^2, \\sigma_T^2, \\sigma_R^2)$:\n- Case $1$ (no confounding): $(0.0, 1.2, 0.7, 1.0, 1.0, 0.5)$.\n- Case $2$ (confounding with strong proxy): $(1.0, 1.0, 1.0, 1.0, 0.1, 0.01)$.\n- Case $3$ (confounding with weak proxy and measurement error amplification): $(1.0, 1.0, 1.0, 1.0, 0.1, 10.0)$.\n- Case $4$ (confounding with perfect proxy): $(1.0, 0.5, 2.0, 2.0, 0.2, 0.0)$.\n- Case $5$ (randomized treatment): $(0.0, 0.8, 1.5, 5.0, 2.0, 1.0)$.\n- Case $6$ (no biological variability): $(2.0, 1.0, 3.0, 0.0, 1.0, 0.5)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, append three values in order: $b_{\\text{naive}}$, $b_{\\text{adj}}$, and the boolean $|b_{\\text{adj}}|  |b_{\\text{naive}}|$. The final output therefore contains $18$ entries corresponding to the $6$ cases.", "solution": "We model confounding and adjustment under the Radiomics Hypothesis using a linear Gaussian Structural Causal Model (SCM). The latent biology $B$ is assumed to causally affect both treatment $T$ and outcome $Y$, and the radiomic feature $R$ is a noisy proxy for $B$. The goal is to estimate the Average Treatment Effect (ATE) $\\tau$ and to quantify confounding bias under naive estimation and after adjustment using the proxy $R$.\n\nWe begin from the fundamental definitions. All variables have zero mean, so for any random variables $X$ and $Z$,\n$$\n\\operatorname{Var}(X) = \\mathbb{E}[X^2], \\quad \\operatorname{Cov}(X,Z) = \\mathbb{E}[XZ].\n$$\nIndependence of noise terms implies cross-covariances of independent components are zero. The structural equations are:\n$$\nT = \\alpha B + U_T, \\quad R = B + \\varepsilon_R, \\quad Y = \\tau T + \\gamma B + U_Y.\n$$\nUsing linearity and independence, we derive the second-order moments needed for Ordinary Least Squares (OLS).\n\nFirst, compute $\\operatorname{Var}(T)$ and $\\operatorname{Cov}(B,T)$:\n$$\n\\operatorname{Var}(T) = \\operatorname{Var}(\\alpha B + U_T) = \\alpha^2 \\operatorname{Var}(B) + \\operatorname{Var}(U_T) = \\alpha^2 \\sigma_B^2 + \\sigma_T^2,\n$$\n$$\n\\operatorname{Cov}(B,T) = \\operatorname{Cov}(B, \\alpha B + U_T) = \\alpha \\operatorname{Var}(B) + 0 = \\alpha \\sigma_B^2.\n$$\nNext, compute $\\operatorname{Var}(R)$ and $\\operatorname{Cov}(T,R)$:\n$$\n\\operatorname{Var}(R) = \\operatorname{Var}(B + \\varepsilon_R) = \\operatorname{Var}(B) + \\operatorname{Var}(\\varepsilon_R) = \\sigma_B^2 + \\sigma_R^2,\n$$\n$$\n\\operatorname{Cov}(T,R) = \\operatorname{Cov}(\\alpha B + U_T, B + \\varepsilon_R) = \\alpha \\operatorname{Var}(B) + 0 + 0 + 0 = \\alpha \\sigma_B^2.\n$$\nNow compute covariances of $Y$ with $T$ and $R$. Using $Y = \\tau T + \\gamma B + U_Y$,\n$$\n\\operatorname{Cov}(Y,T) = \\operatorname{Cov}(\\tau T + \\gamma B + U_Y, T) = \\tau \\operatorname{Var}(T) + \\gamma \\operatorname{Cov}(B,T) + 0 = \\tau(\\alpha^2 \\sigma_B^2 + \\sigma_T^2) + \\gamma \\alpha \\sigma_B^2,\n$$\n$$\n\\operatorname{Cov}(Y,R) = \\operatorname{Cov}(\\tau T + \\gamma B + U_Y, R) = \\tau \\operatorname{Cov}(T,R) + \\gamma \\operatorname{Cov}(B,R) + 0 = \\tau \\alpha \\sigma_B^2 + \\gamma \\sigma_B^2.\n$$\n\nWe analyze two estimators for the effect of $T$ on $Y$.\n\nNaive OLS estimator (regress $Y$ on $T$ only). The coefficient on $T$ is\n$$\n\\beta_T = \\frac{\\operatorname{Cov}(Y,T)}{\\operatorname{Var}(T)} = \\frac{\\tau(\\alpha^2 \\sigma_B^2 + \\sigma_T^2) + \\gamma \\alpha \\sigma_B^2}{\\alpha^2 \\sigma_B^2 + \\sigma_T^2} = \\tau + \\frac{\\gamma \\alpha \\sigma_B^2}{\\alpha^2 \\sigma_B^2 + \\sigma_T^2}.\n$$\nTherefore, the naive bias is\n$$\nb_{\\text{naive}} = \\beta_T - \\tau = \\frac{\\gamma \\alpha \\sigma_B^2}{\\alpha^2 \\sigma_B^2 + \\sigma_T^2}.\n$$\nThis term is nonzero when there is confounding through $B$ ($\\alpha \\neq 0$ and $\\gamma \\neq 0$), and vanishes if $T$ is randomized ($\\alpha = 0$) or if biology has no variation ($\\sigma_B^2 = 0$).\n\nAdjusted OLS estimator (regress $Y$ on both $T$ and $R$). The coefficient on $T$ in a multiple regression of $Y$ on $(T,R)$ is the well-tested identity\n$$\n\\beta_T^{\\text{adj}} = \\frac{\\operatorname{Cov}(Y,T)\\operatorname{Var}(R) - \\operatorname{Cov}(Y,R)\\operatorname{Cov}(T,R)}{\\operatorname{Var}(T)\\operatorname{Var}(R) - \\operatorname{Cov}(T,R)^2}.\n$$\nDefine the denominator\n$$\nD = \\operatorname{Var}(T)\\operatorname{Var}(R) - \\operatorname{Cov}(T,R)^2 = (\\alpha^2 \\sigma_B^2 + \\sigma_T^2)(\\sigma_B^2 + \\sigma_R^2) - (\\alpha \\sigma_B^2)^2.\n$$\nExpanding and simplifying,\n$$\nD = \\alpha^2 \\sigma_B^2 \\sigma_R^2 + \\sigma_T^2(\\sigma_B^2 + \\sigma_R^2).\n$$\nThe numerator is\n$$\nN_T = \\operatorname{Cov}(Y,T)\\operatorname{Var}(R) - \\operatorname{Cov}(Y,R)\\operatorname{Cov}(T,R).\n$$\nSubstitute the previously derived covariances and variances:\n$$\nN_T = \\big[\\tau(\\alpha^2 \\sigma_B^2 + \\sigma_T^2) + \\gamma \\alpha \\sigma_B^2\\big](\\sigma_B^2 + \\sigma_R^2) - \\big[\\tau \\alpha \\sigma_B^2 + \\gamma \\sigma_B^2\\big]\\big[\\alpha \\sigma_B^2\\big].\n$$\nGrouping terms,\n$$\nN_T = \\tau\\big[\\alpha^2 \\sigma_B^2 \\sigma_R^2 + \\sigma_T^2(\\sigma_B^2 + \\sigma_R^2)\\big] + \\gamma \\alpha \\sigma_B^2 \\sigma_R^2 = \\tau D + \\gamma \\alpha \\sigma_B^2 \\sigma_R^2.\n$$\nTherefore,\n$$\n\\beta_T^{\\text{adj}} = \\frac{\\tau D + \\gamma \\alpha \\sigma_B^2 \\sigma_R^2}{D} = \\tau + \\frac{\\gamma \\alpha \\sigma_B^2 \\sigma_R^2}{D}.\n$$\nThe adjusted bias is\n$$\nb_{\\text{adj}} = \\beta_T^{\\text{adj}} - \\tau = \\frac{\\gamma \\alpha \\sigma_B^2 \\sigma_R^2}{\\alpha^2 \\sigma_B^2 \\sigma_R^2 + \\sigma_T^2(\\sigma_B^2 + \\sigma_R^2)}.\n$$\nKey consequences:\n- If the radiomic proxy is perfect ($\\sigma_R^2 = 0$), then $b_{\\text{adj}} = 0$; adjustment removes confounding via the back-door criterion because $R = B$ blocks the path $T \\leftarrow B \\rightarrow Y$.\n- If $T$ is randomized ($\\alpha = 0$), then both $b_{\\text{naive}} = 0$ and $b_{\\text{adj}} = 0$.\n- If the proxy is very noisy ($\\sigma_R^2$ large), then $D \\approx \\sigma_T^2 \\sigma_R^2$ and $b_{\\text{adj}} \\approx \\gamma \\alpha \\sigma_B^2 / \\sigma_T^2$, which can exceed the naive bias $b_{\\text{naive}} = \\gamma \\alpha \\sigma_B^2 / (\\alpha^2 \\sigma_B^2 + \\sigma_T^2)$, demonstrating bias amplification due to measurement error in the proxy.\n\nAlgorithmic design:\n- For each test case $(\\alpha, \\tau, \\gamma, \\sigma_B^2, \\sigma_T^2, \\sigma_R^2)$, compute $b_{\\text{naive}}$ using $b_{\\text{naive}} = \\dfrac{\\gamma \\alpha \\sigma_B^2}{\\alpha^2 \\sigma_B^2 + \\sigma_T^2}$.\n- Compute $D = \\alpha^2 \\sigma_B^2 \\sigma_R^2 + \\sigma_T^2(\\sigma_B^2 + \\sigma_R^2)$ and then $b_{\\text{adj}} = \\dfrac{\\gamma \\alpha \\sigma_B^2 \\sigma_R^2}{D}$.\n- Report, for each case, the tuple $(b_{\\text{naive}}, b_{\\text{adj}}, |b_{\\text{adj}}|  |b_{\\text{naive}}|)$.\n- Aggregate all case results into one flat list in the required output order.\n\nThe program uses only deterministic closed-form computations and does not require simulation. No physical units or angle units apply; outputs are real numbers and booleans as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_biases(alpha, tau, gamma, sigma_B2, sigma_T2, sigma_R2):\n    \"\"\"\n    Compute naive and adjusted biases for the linear Gaussian SCM:\n      T = alpha * B + U_T\n      R = B + eps_R\n      Y = tau * T + gamma * B + U_Y\n    All variables zero-mean; B, U_T, U_Y, eps_R mutually independent.\n    Parameters represent variances and coefficients.\n    Returns:\n      b_naive, b_adj\n    \"\"\"\n    # Naive bias: b_naive = (gamma * alpha * sigma_B^2) / (alpha^2 * sigma_B^2 + sigma_T^2)\n    denom_naive = alpha**2 * sigma_B2 + sigma_T2\n    # To be robust in degenerate cases, handle denominator zero (should not occur with nonnegative variances unless all zero).\n    if denom_naive == 0.0:\n        # If both alpha and sigma_T2 are 0, T is identically zero; regression undefined.\n        # Define bias as NaN to indicate non-identifiable; but per problem, we avoid such test cases.\n        b_naive = float('nan')\n    else:\n        b_naive = (gamma * alpha * sigma_B2) / denom_naive\n\n    # Adjusted bias: b_adj = (gamma * alpha * sigma_B^2 * sigma_R^2) / D\n    D = (alpha**2 * sigma_B2 * sigma_R2) + (sigma_T2 * (sigma_B2 + sigma_R2))\n    if D == 0.0:\n        # Degenerate multicollinearity or zero-variance conditions; avoid in test suite.\n        b_adj = float('nan')\n    else:\n        b_adj = (gamma * alpha * sigma_B2 * sigma_R2) / D\n\n    return b_naive, b_adj\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (alpha, tau, gamma, sigma_B^2, sigma_T^2, sigma_R^2)\n    test_cases = [\n        (0.0, 1.2, 0.7, 1.0, 1.0, 0.5),   # Case 1: no confounding\n        (1.0, 1.0, 1.0, 1.0, 0.1, 0.01),  # Case 2: confounding with strong proxy\n        (1.0, 1.0, 1.0, 1.0, 0.1, 10.0),  # Case 3: confounding with weak proxy (bias amplification)\n        (1.0, 0.5, 2.0, 2.0, 0.2, 0.0),   # Case 4: confounding with perfect proxy\n        (0.0, 0.8, 1.5, 5.0, 2.0, 1.0),   # Case 5: randomized treatment\n        (2.0, 1.0, 3.0, 0.0, 1.0, 0.5),   # Case 6: no biological variability\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, tau, gamma, sigma_B2, sigma_T2, sigma_R2 = case\n        b_naive, b_adj = compute_biases(alpha, tau, gamma, sigma_B2, sigma_T2, sigma_R2)\n        # Determine whether adjustment reduces absolute bias\n        reduces_bias = (abs(b_adj)  abs(b_naive)) if (np.isfinite(b_naive) and np.isfinite(b_adj)) else False\n        # Append outputs in required order: b_naive, b_adj, boolean\n        results.append(b_naive)\n        results.append(b_adj)\n        results.append(reduces_bias)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4567514"}, {"introduction": "A typical radiomics analysis generates a vast number of features, creating a \"high-dimensional\" problem where the number of variables can dwarf the number of patients. This practice guides you through the essential statistical pipeline for navigating this complex landscape. You will implement methods to control for false discoveries when testing many features simultaneously and use Principal Component Analysis ($PCA$) to distill the most important patterns from the data [@problem_id:4567514].", "problem": "You are given a formalization of the radiomics hypothesis and are asked to build a program that operationalizes feature selection under multiple testing and dimensionality reduction using Principal Component Analysis, starting from first principles in statistical inference. The radiomics hypothesis asserts that quantitative image-derived features encode latent biological properties; hence, if a clinical endpoint is causally tied to underlying tissue states, then appropriate image features should exhibit statistically detectable associations with the endpoint. In a high-dimensional setting the number of features is large relative to the number of samples, which requires principled control of false positives and careful dimensionality reduction.\n\nFundamental base to use:\n- Statistical hypothesis testing for linear association: for each feature $j$ with samples $\\{x_{ij}\\}_{i=1}^n$ and a clinical endpoint $\\{y_i\\}_{i=1}^n$, consider the null hypothesis $H_{0j}: \\rho_j = 0$, where $\\rho_j$ is the population Pearson correlation between feature $j$ and the endpoint. The sample Pearson correlation $r_j$ is derived from centered and scaled variables and, under the null hypothesis for independent, identically distributed samples with finite second moments, yields a $t$-statistic with Student’s $t$ distribution with $n-2$ degrees of freedom.\n- Family-Wise Error Rate (FWER) control by the Bonferroni method: given a significance level $\\alpha$ and $m$ simultaneous tests, reject $H_{0j}$ if the two-sided $p$-value $p_j \\le \\alpha/m$.\n- False Discovery Rate (FDR) control by the Benjamini–Hochberg (BH) procedure: given a target level $q$ and $m$ $p$-values, sort them ascending as $p_{(1)} \\le \\cdots \\le p_{(m)}$, and find the largest index $k$ such that $p_{(k)} \\le (k/m)q$. Reject the $k$ hypotheses with the smallest $p$-values.\n- Principal Component Analysis (PCA): for a mean-centered data matrix $Z \\in \\mathbb{R}^{n \\times p}$, the singular values from a singular value decomposition quantify the eigenvalues of the sample covariance matrix via $Z = U S V^\\top$, where the eigenvalues are $S^2/(n-1)$ and the explained variance ratio of the $i$-th principal component equals its eigenvalue divided by the sum of all eigenvalues. The minimal number of components $K$ needed to explain at least a target fraction $\\tau$ of total variance satisfies $\\sum_{i=1}^K \\lambda_i / \\sum_{i=1}^{r} \\lambda_i \\ge \\tau$, where $\\lambda_i$ are sorted eigenvalues and $r$ is the rank.\n\nYour program must perform the following, strictly following these steps for each test case:\n1. Standardize each feature column by subtracting its sample mean and dividing by its sample standard deviation with degrees of freedom $n-1$. Denote the resulting standardized feature matrix by $Z$. Any feature whose sample standard deviation is zero must be ignored in all subsequent computations. Standardize the endpoint vector $y$ similarly; you may assume the endpoint has nonzero sample variance in all test cases.\n2. Compute the sample Pearson correlation $r_j$ between each standardized feature column $Z_{\\cdot j}$ and standardized endpoint $z_y$. Use the identity $r_j = \\frac{1}{n-1}\\sum_{i=1}^n Z_{ij} z_{y,i}$, which follows from defining correlation as covariance over product of standard deviations after standardization.\n3. For each $r_j$, compute the two-sided $p$-value under the null hypothesis using the Student’s $t$ distribution: form the statistic $t_j = r_j \\sqrt{\\frac{n-2}{1-r_j^2}}$ for $|r_j|  1$, and set the corresponding two-sided $p$-value to $p_j = 2\\left(1 - F_{t_{n-2}}(|t_j|)\\right)$, where $F_{t_{n-2}}$ is the cumulative distribution function of the Student’s $t$ distribution with $n-2$ degrees of freedom. If $|r_j| = 1$, set $p_j = 0$ by continuity.\n4. Apply Bonferroni correction at significance level $\\alpha$ to obtain the number of rejected hypotheses $B$, where rejection occurs when $p_j \\le \\alpha/m$ and $m$ is the count of non-ignored features.\n5. Apply the Benjamini–Hochberg procedure at target level $q$ to obtain the number of rejected hypotheses $H$ as the largest $k$ satisfying $p_{(k)} \\le (k/m)q$.\n6. Perform Principal Component Analysis on $Z$ via singular value decomposition, compute eigenvalues $\\lambda_i = S_i^2/(n-1)$, sort them descending, and determine the minimal number $K$ such that the cumulative explained variance fraction reaches or exceeds the target $\\tau$, that is, the smallest $K$ with $\\sum_{i=1}^K \\lambda_i / \\sum_{i=1}^{r} \\lambda_i \\ge \\tau$. If there are no non-ignored features, define $K = 0$.\n\nYour program must implement the above and, for each test case, produce a three-integer result $[B, H, K]$.\n\nTest Suite:\nUse the following four deterministic test cases. Each case provides a feature matrix $X$, an endpoint vector $y$, and levels $(\\alpha, q, \\tau)$. All entries are integers, and sizes are small to support manual verification.\n\n- Case $1$ (happy path, orthogonal features with a perfectly associated endpoint):\n    - $n = 8$, $p = 4$.\n    - Columns of $X$ are\n      $v_1 = [1, 1, 1, 1, -1, -1, -1, -1]$,\n      $v_2 = [1, 1, -1, -1, 1, 1, -1, -1]$,\n      $v_3 = [1, -1, 1, -1, 1, -1, 1, -1]$,\n      $v_4 = [1, -1, -1, 1, -1, 1, 1, -1]$.\n    - Endpoint $y = v_3$.\n    - Levels: $\\alpha = 0.05$, $q = 0.10$, $\\tau = 0.95$.\n\n- Case $2$ (boundary with no signal; endpoint orthogonal to all features):\n    - $n = 8$, $p = 4$, same $X$ as Case $1$.\n    - Endpoint $y = [1, -1, 1, -1, -1, 1, -1, 1]$.\n    - Levels: $\\alpha = 0.05$, $q = 0.10$, $\\tau = 0.50$.\n\n- Case $3$ (highly collinear features, $p  n$, multiple perfect correlates and low effective rank):\n    - $n = 6$, $p = 8$.\n    - Define base sequences:\n      $u_1 = [1, 2, 3, -3, -2, -1]$,\n      $u_2 = [2, 0, -2, 2, 0, -2]$,\n      $u_3 = [1, -1, 1, -1, 1, -1]$.\n    - Columns of $X$ are\n      $f_1 = u_1$,\n      $f_2 = u_2$,\n      $f_3 = u_3$,\n      $f_4 = f_1 + f_2$,\n      $f_5 = f_1 - f_2$,\n      $f_6 = 2 f_1$,\n      $f_7 = f_2 - f_3$,\n      $f_8 = f_3$.\n    - Endpoint $y = f_6$.\n    - Levels: $\\alpha = 0.01$, $q = 0.05$, $\\tau = 0.90$.\n\n- Case $4$ (constant feature to be ignored; duplicated signal at different scale):\n    - $n = 6$, $p = 5$.\n    - Columns of $X$ are\n      $g_1 = [0, 0, 0, 0, 0, 0]$,\n      $g_2 = [1, 0, -1, 1, 0, -1]$,\n      $g_3 = [-2, -1, 0, 0, 1, 2]$,\n      $g_4 = [1, 2, 1, -1, -2, -1]$,\n      $g_5 = [3, 0, -3, 3, 0, -3]$.\n    - Endpoint $y = g_5$.\n    - Levels: $\\alpha = 0.05$, $q = 0.10$, $\\tau = 0.80$.\n\nOutput specification:\n- For each case, compute the triple $[B, H, K]$ as defined above.\n- Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, where each case’s result is itself a list. For example, an output with generic placeholders would look like $[[B_1, H_1, K_1],[B_2, H_2, K_2],[B_3, H_3, K_3],[B_4, H_4, K_4]]$.", "solution": "The problem requires the implementation of a statistical pipeline to test the radiomics hypothesis. This involves feature selection via hypothesis testing with multiple comparison corrections, followed by dimensionality reduction using Principal Component Analysis (PCA). The process is executed for several test cases, each defined by a feature matrix $X$, an endpoint vector $y$, and a set of parameters $(\\alpha, q, \\tau)$. The required output for each case is a triplet $[B, H, K]$, representing the number of significant features found by the Bonferroni and Benjamini-Hochberg methods, and the number of principal components needed to explain a target variance, respectively.\n\nThe programmatic solution is structured as a series of well-defined steps, adhering strictly to the provided statistical and mathematical formalisms.\n\n**Step 1: Data Standardization**\nFor each test case, we are given a feature matrix $X \\in \\mathbb{R}^{n \\times p}$ and an endpoint vector $y \\in \\mathbb{R}^{n}$, where $n$ is the number of samples and $p$ is the number of features. The first step is to standardize the data.\n\nFor each feature column $X_{\\cdot j}$ for $j=1, \\dots, p$, we compute its sample mean $\\bar{x}_j$ and sample standard deviation $s_j$. The sample standard deviation is calculated with $n-1$ degrees of freedom (Bessel's correction):\n$$\ns_j = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2}\n$$\nFeatures for which $s_j = 0$ are identified and excluded from all subsequent analyses. Let $m$ be the number of features with $s_j  0$. We construct a new matrix $Z \\in \\mathbb{R}^{n \\times m}$ containing only the standardized columns of the valid features. Each column $Z_{\\cdot j}$ is computed as:\n$$\nZ_{ij} = \\frac{x_{ij} - \\bar{x}_j}{s_j}\n$$\nSimilarly, the endpoint vector $y$ is standardized to produce $z_y$, which has a sample mean of $0$ and a sample standard deviation of $1$. The problem statement guarantees that the sample variance of $y$ is non-zero.\n\nIf no features have a non-zero standard deviation ($m=0$), the analysis for the case terminates, and the result is $[0, 0, 0]$.\n\n**Step 2: Pearson Correlation**\nWith standardized features $Z$ and a standardized endpoint $z_y$, the sample Pearson correlation coefficient $r_j$ for each valid feature $j$ simplifies. Since the sample mean of any standardized variable is $0$ and its sample standard deviation is $1$, the correlation is equivalent to the sample covariance:\n$$\nr_j = \\frac{\\text{cov}(Z_{\\cdot j}, z_y)}{s_{Z_{\\cdot j}} s_{z_y}} = \\frac{\\frac{1}{n-1}\\sum_{i=1}^n (Z_{ij} - \\bar{Z}_j)(z_{y,i} - \\bar{z}_y)}{1 \\cdot 1} = \\frac{1}{n-1}\\sum_{i=1}^n Z_{ij} z_{y,i}\n$$\nThis calculation is performed for all $m$ valid features, yielding a vector of correlation coefficients $\\{r_j\\}_{j=1}^m$.\n\n**Step 3: P-Value Calculation**\nFor each correlation coefficient $r_j$, we test the null hypothesis $H_{0j}: \\rho_j = 0$. Under the null hypothesis, the statistic\n$$\nt_j = r_j \\sqrt{\\frac{n-2}{1-r_j^2}}\n$$\nfollows a Student's $t$-distribution with $\\nu = n-2$ degrees of freedom. This holds for $|r_j|  1$. If $|r_j|=1$, indicating perfect correlation, the denominator becomes zero. By continuity, we set the p-value to $0$ in this case. For $|r_j|1$, the two-sided p-value $p_j$ is calculated as the probability of observing a test statistic at least as extreme as $|t_j|$:\n$$\np_j = 2 \\cdot P(T_\\nu \\ge |t_j|) = 2 \\left(1 - F_{t_{n-2}}(|t_j|)\\right)\n$$\nwhere $F_{t_{n-2}}$ is the cumulative distribution function (CDF) of the Student's $t$-distribution with $n-2$ degrees of freedom.\n\n**Step 4: Bonferroni Correction (FWER Control)**\nTo control the Family-Wise Error Rate (FWER) across the $m$ simultaneous tests, the Bonferroni correction is applied. Given a significance level $\\alpha$, the corrected significance threshold for each test is $\\alpha_{bonf} = \\alpha/m$. A null hypothesis $H_{0j}$ is rejected if its corresponding p-value $p_j \\le \\alpha_{bonf}$. The total number of such rejections is counted to determine the value of $B$.\n\n**Step 5: Benjamini-Hochberg Procedure (FDR Control)**\nTo control the False Discovery Rate (FDR), the Benjamini-Hochberg (BH) procedure is applied at a target level $q$. The $m$ p-values are sorted in ascending order: $p_{(1)} \\le p_{(2)} \\le \\cdots \\le p_{(m)}$. The largest index $k$ is found such that the $k$-th ordered p-value satisfies the condition:\n$$\np_{(k)} \\le \\frac{k}{m}q\n$$\nThe BH procedure dictates rejecting all null hypotheses corresponding to the p-values $p_{(1)}, \\dots, p_{(k)}$. The total number of rejections is $H=k$. If no such $k$ exists, $H=0$.\n\n**Step 6: Principal Component Analysis (PCA)**\nPCA is performed on the standardized valid feature matrix $Z \\in \\mathbb{R}^{n \\times m}$. The principal components are derived from the eigendecomposition of the sample covariance matrix $\\text{Cov}(Z) = \\frac{1}{n-1} Z^\\top Z$. An efficient way to compute the eigenvalues of this matrix is via Singular Value Decomposition (SVD) of $Z$.\nLet $Z = U S V^\\top$ be the SVD of $Z$, where $S$ is a diagonal matrix of singular values $S_i$. The eigenvalues $\\lambda_i$ of the covariance matrix are related to the singular values by:\n$$\n\\lambda_i = \\frac{S_i^2}{n-1}\n$$\nThese eigenvalues are sorted in descending order. The fraction of total variance explained by the $i$-th principal component is $\\lambda_i / \\sum_{j=1}^r \\lambda_j$, where $r$ is the rank of $Z$ (the number of non-zero eigenvalues). The objective is to find the minimum number of components $K$ such that the cumulative explained variance fraction reaches or exceeds a target threshold $\\tau$:\n$$\n\\text{Find minimal } K \\text{ such that } \\frac{\\sum_{i=1}^K \\lambda_i}{\\sum_{j=1}^r \\lambda_j} \\ge \\tau\n$$\nThis value of $K$ represents the intrinsic dimensionality of the feature set required to capture at least a fraction $\\tau$ of its total variance.\n\nBy executing these six steps for each test case, we derive the required integer triplet $[B, H, K]$.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and print results in the specified format.\n    \"\"\"\n    # Case 1: Happy path, orthogonal features with a perfectly associated endpoint\n    v1 = np.array([1, 1, 1, 1, -1, -1, -1, -1], dtype=float)\n    v2 = np.array([1, 1, -1, -1, 1, 1, -1, -1], dtype=float)\n    v3 = np.array([1, -1, 1, -1, 1, -1, 1, -1], dtype=float)\n    v4 = np.array([1, -1, -1, 1, -1, 1, 1, -1], dtype=float)\n    case1_X = np.array([v1, v2, v3, v4]).T\n    case1_y = v3\n    case1_params = (0.05, 0.10, 0.95)\n\n    # Case 2: Boundary with no signal; endpoint orthogonal to all features\n    case2_X = np.array([v1, v2, v3, v4]).T\n    case2_y = np.array([1, -1, 1, -1, -1, 1, -1, 1], dtype=float)\n    case2_params = (0.05, 0.10, 0.50)\n\n    # Case 3: Highly collinear features, p  n\n    u1 = np.array([1, 2, 3, -3, -2, -1], dtype=float)\n    u2 = np.array([2, 0, -2, 2, 0, -2], dtype=float)\n    u3 = np.array([1, -1, 1, -1, 1, -1], dtype=float)\n    f1, f2, f3 = u1, u2, u3\n    f4, f5, f6 = f1 + f2, f1 - f2, 2 * f1\n    f7, f8 = f2 - f3, f3\n    case3_X = np.array([f1, f2, f3, f4, f5, f6, f7, f8]).T\n    case3_y = f6\n    case3_params = (0.01, 0.05, 0.90)\n\n    # Case 4: Constant feature to be ignored; duplicated signal\n    g1 = np.array([0, 0, 0, 0, 0, 0], dtype=float)\n    g2 = np.array([1, 0, -1, 1, 0, -1], dtype=float)\n    g3 = np.array([-2, -1, 0, 0, 1, 2], dtype=float)\n    g4 = np.array([1, 2, 1, -1, -2, -1], dtype=float)\n    g5 = np.array([3, 0, -3, 3, 0, -3], dtype=float)\n    case4_X = np.array([g1, g2, g3, g4, g5]).T\n    case4_y = g5\n    case4_params = (0.05, 0.10, 0.80)\n\n    test_cases = [\n        (case1_X, case1_y, *case1_params),\n        (case2_X, case2_y, *case2_params),\n        (case3_X, case3_y, *case3_params),\n        (case4_X, case4_y, *case4_params),\n    ]\n\n    results = []\n    for X, y, alpha, q, tau in test_cases:\n        result = process_case(X, y, alpha, q, tau)\n        results.append(result)\n\n    # Format output string to be exactly [[B1,H1,K1],[B2,H2,K2],...]\n    results_str = [f\"[{b},{h},{k}]\" for b, h, k in results]\n    print(f\"[{','.join(results_str)}]\")\n\ndef process_case(X, y, alpha, q, tau):\n    \"\"\"\n    Executes the full statistical pipeline for a single test case.\n    \"\"\"\n    n, p = X.shape\n    \n    # --- Step 1: Standardization ---\n    std_devs = np.std(X, axis=0, ddof=1)\n    valid_feature_indices = np.where(std_devs  1e-9)[0] # Use tolerance for float\n    \n    m = len(valid_feature_indices)\n    if m == 0:\n        return [0, 0, 0]\n\n    X_valid = X[:, valid_feature_indices]\n    \n    mean_X = np.mean(X_valid, axis=0)\n    std_X = np.std(X_valid, axis=0, ddof=1)\n    Z = (X_valid - mean_X) / std_X\n    \n    mean_y = np.mean(y)\n    std_y = np.std(y, ddof=1)\n    z_y = (y - mean_y) / std_y\n    \n    # --- Step 2: Pearson Correlation ---\n    r_values = (1 / (n - 1)) * (z_y @ Z)\n    \n    # --- Step 3: P-Value Calculation ---\n    p_values = np.zeros(m)\n    df = n - 2\n    if df = 0: # Cannot form t-statistic\n        return [0, 0, 0] # Or handle as error\n        \n    for j in range(m):\n        r = r_values[j]\n        if abs(r) = 1.0:\n            p_values[j] = 0.0\n        else:\n            t_stat = r * np.sqrt(df / (1 - r**2))\n            p_values[j] = 2 * (1 - t.cdf(np.abs(t_stat), df))\n\n    # --- Step 4: Bonferroni Correction ---\n    alpha_bonf = alpha / m\n    B = np.sum(p_values = alpha_bonf)\n\n    # --- Step 5: Benjamini-Hochberg Procedure ---\n    sorted_indices = np.argsort(p_values)\n    sorted_p_values = p_values[sorted_indices]\n    \n    k = np.arange(1, m + 1)\n    bh_thresholds = (k / m) * q\n    \n    rejections = sorted_p_values = bh_thresholds\n    \n    H = 0\n    if np.any(rejections):\n        H = np.max(np.where(rejections)[0]) + 1\n        \n    # --- Step 6: Principal Component Analysis (PCA) ---\n    if m == 0:\n        K = 0\n    else:\n        # SVD on the standardized matrix Z\n        # full_matrices=False for thin SVD, efficient for nm or nm\n        _, S, _ = np.linalg.svd(Z, full_matrices=False)\n        \n        # Eigenvalues of the covariance matrix\n        eigenvalues = (S**2) / (n - 1)\n        \n        total_variance = np.sum(eigenvalues)\n        \n        if total_variance  1e-9:\n             K = 0\n        else:\n            cumulative_variance_ratio = np.cumsum(eigenvalues) / total_variance\n            \n            # Find the minimal number of components K\n            # np.searchsorted can also be used here\n            found_indices = np.where(cumulative_variance_ratio = tau)[0]\n            if len(found_indices) == 0:\n                # This could happen if tau  1, or due to float precision\n                # if tau is very close to 1. Assume all components are needed.\n                K = m \n            else:\n                K = found_indices[0] + 1\n                \n    return [int(B), int(H), int(K)]\n\nsolve()\n```", "id": "4567522"}]}