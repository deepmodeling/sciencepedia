{"hands_on_practices": [{"introduction": "In Positron Emission Tomography, the useful signal comes from detecting two photons from a single annihilation event in \"coincidence.\" However, detectors are constantly being hit by photons from various unrelated events, creating a background of \"random\" coincidences that add noise to the image. This exercise guides you through a foundational derivation, using the principles of Poisson processes, to model the rate of these random events, a critical step in PET image correction and quantification [@problem_id:4556056].", "problem": "In Positron Emission Tomography (PET), consider two opposing detectors labeled $i$ and $j$. Let the measured singles rates (counts per unit time) be $S_i$ and $S_j$, respectively, and assume these singles streams represent independent realizations of stationary Poisson processes. The system declares a coincidence whenever the absolute time difference between a detection in detector $i$ and a detection in detector $j$ is less than or equal to a fixed coincidence window $\\tau$, that is, whenever $|t_i - t_j| \\le \\tau$. A random coincidence is defined as any such declared coincidence arising from detections that do not originate from the same positron annihilation event.\n\nStarting only from the defining property of a stationary Poisson process—that the expected number of events in any interval of length $L$ equals the process rate multiplied by $L$—derive an analytic expression for the expected random coincidence rate $R_{ij}$ between detectors $i$ and $j$ in terms of $S_i$, $S_j$, and $\\tau$. Clearly state the assumptions required for the validity of your expression, and explain the origin of any multiplicative factors in your result. Express your final answer as a closed-form algebraic expression in $S_i$, $S_j$, and $\\tau$. Do not include units in the final answer.", "solution": "We model the singles streams in detectors $i$ and $j$ as independent stationary Poisson processes with rates $S_i$ and $S_j$, respectively. By the defining property of a stationary Poisson process, the expected number of events from detector $j$ in any time interval of length $L$ is $S_j L$, and similarly for detector $i$ with rate $S_i$.\n\nA random coincidence is declared whenever there exists a pair of events, one from detector $i$ and one from detector $j$, whose timestamps satisfy $|t_i - t_j| \\le \\tau$. To compute the expected rate $R_{ij}$ of such pairs, we proceed by conditioning on events in detector $i$ and counting expected partner events in detector $j$.\n\nConsider a single event in detector $i$ occurring at time $t$. The set of times in detector $j$ that produce a declared coincidence with this event is the interval $[t - \\tau, t + \\tau]$, which has length $2\\tau$. For a stationary Poisson process in detector $j$ with rate $S_j$, the expected number of events in this interval is\n$$\n\\text{Expected number from detector } j \\text{ in } [t - \\tau, t + \\tau] = S_j \\cdot (2\\tau) = 2\\tau S_j.\n$$\nThis quantity includes the possibility of more than one event occurring in the interval; however, for a Poisson process the expected number in any interval is additive, and this expectation is exact.\n\nNow, the expected number of such $i$-events per unit time is $S_i$. Therefore, the expected total number of $i$–$j$ pairs per unit time that satisfy the coincidence criterion is the product of the expected number of $i$-events per unit time and the expected number of $j$-events in the coincidence interval around each $i$-event:\n$$\nR_{ij} = S_i \\cdot (2\\tau S_j) = 2\\tau S_i S_j.\n$$\nThis derivation counts each $i$–$j$ pair exactly once by anchoring the counting to events in detector $i$; no double counting occurs because each pair is associated with one $i$-event and one $j$-event. The factor $2$ arises from the symmetric coincidence condition $|t_i - t_j| \\le \\tau$, which corresponds to an interval of length $2\\tau$ around each anchor event.\n\nAssumptions and conditions for validity:\n- Independence: The singles in detectors $i$ and $j$ are independent Poisson processes, implying no temporal correlation between the streams aside from true coincidences, which are excluded by the definition of random coincidences.\n- Stationarity: The rates $S_i$ and $S_j$ are constant over the time scale of interest, so the expected counts scale linearly with interval length.\n- Well-defined coincidence model: The system declares a coincidence for any pair with $|t_i - t_j| \\le \\tau$, implying an effective acceptance interval of length $2\\tau$ around each anchor event.\n- Throughput and dead time: The measured singles rates $S_i$ and $S_j$ already reflect any dead-time and pile-up effects; the derivation assumes that the system can register all potential random coincidences implied by these measured rates (no additional saturation or veto logic that would suppress pairs).\n- Timing resolution: Jitter or finite timing resolution that broadens the effective coincidence criterion can be absorbed into an effective window $\\tau$; the result depends on the window used operationally for declaring coincidences.\n\nUnder these standard Positron Emission Tomography principles, the expected random coincidence rate is given by\n$$\nR_{ij} = 2\\tau S_i S_j.\n$$", "answer": "$$\\boxed{2\\tau S_i S_j}$$", "id": "4556056"}, {"introduction": "Time-of-Flight (TOF) technology represents a major advance in PET, improving image quality by using the minute time difference between the arrival of the two annihilation photons to better localize the event along the line of response. This practice connects the abstract concept of a scanner's timing resolution, typically measured in picoseconds, to its direct impact on spatial precision in the final image. By deriving this relationship from first principles, you will gain a tangible understanding of how improvements in detector and electronic performance translate into clinically meaningful benefits [@problem_id:4907332].", "problem": "A time-of-flight Positron Emission Tomography (PET) system measures the difference in arrival times of the two $511\\ \\text{keV}$ annihilation photons at opposite detectors along a single line of response (LOR). Assume the coincidence timing uncertainty can be modeled as a Gaussian random variable with full width at half maximum (FWHM) in time equal to $FWHM_t = 300\\ \\text{ps}$. Use the following foundational facts: (i) both annihilation photons propagate at the speed of light $c$, and (ii) the difference in arrival times between the two detectors is determined solely by the difference in path lengths traveled along the LOR. The Full Width at Half Maximum (FWHM) and the standard deviation $\\sigma$ of a Gaussian are related, but do not assume any specific conversion until you derive it.\n\nStarting from these principles, derive the mapping from timing uncertainty to spatial uncertainty along the LOR. Then compute:\n- the spatial full width at half maximum along the LOR, $FWHM_x$, in millimeters, and\n- the corresponding spatial standard deviation, $\\sigma_x$, in millimeters,\n\nfor the given $FWHM_t$. Use $c = 2.99792458 \\times 10^8\\ \\text{m/s}$. Round both $FWHM_x$ and $\\sigma_x$ to $4$ significant figures. Express both results in millimeters. Provide the final numerical values only (without units) in the answer.", "solution": "The problem is valid as it is scientifically grounded in the principles of Positron Emission Tomography (PET), is well-posed with all necessary information, and is expressed in objective, formal language. We may proceed with the solution.\n\nThe core principle of time-of-flight (TOF) PET is to use the difference in arrival times of the two annihilation photons to better estimate the position of the annihilation event along the line of response (LOR). Let the LOR be a one-dimensional coordinate axis. Let the midpoint between the two detectors be the origin, $x=0$. An annihilation event occurs at an unknown position $x$ along this axis.\n\nWhen the event occurs at $x$, one photon travels a distance $d_1$ to the first detector, and the other photon travels a distance $d_2$ to the second detector. The position $x$ represents the displacement from the center of the LOR. This displacement means one photon's path is shortened by a length $|x|$ and the other's is lengthened by the same amount, relative to an event at the center. Therefore, the difference in the path lengths traveled by the two photons is $\\Delta l = 2|x|$. For simplicity, we define the sign of $x$ such that the path difference is $\\Delta l = 2x$.\n\nThe two photons are created simultaneously and travel at the speed of light, $c$. The difference in their arrival times at the detectors, $\\Delta t$, is directly proportional to the path length difference:\n$$ \\Delta t = \\frac{\\Delta l}{c} $$\nSubstituting the expression for $\\Delta l$, we get:\n$$ \\Delta t = \\frac{2x}{c} $$\nThis equation can be rearranged to find the position of the event, $x$, based on the measured time difference, $\\Delta t$:\n$$ x = \\frac{c}{2} \\Delta t $$\nThis equation represents the mapping from the measured timing difference to the event's spatial position along the LOR.\n\nThe problem states that the measurement of $\\Delta t$ is subject to a timing uncertainty, which is modeled as a Gaussian random variable with a full width at half maximum (FWHM) of $FWHM_t$. Since the relationship between $x$ and $\\Delta t$ is linear, the resulting uncertainty in the position, $x$, will also follow a Gaussian distribution. The uncertainty in position, characterized by its FWHM, $FWHM_x$, is related to the timing uncertainty $FWHM_t$ by the same linear factor:\n$$ FWHM_x = \\frac{c}{2} FWHM_t $$\nThis is the derived mapping from timing uncertainty to spatial uncertainty along the LOR.\n\nNext, we must derive the relationship between the FWHM and the standard deviation, $\\sigma$, for a Gaussian distribution. A Gaussian probability density function is given by:\n$$ f(y) = A \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right) $$\nwhere $\\mu$ is the mean, $\\sigma$ is the standard deviation, and $A$ is a normalization constant. The maximum value of the function is $f(\\mu) = A$. The FWHM is the width of the distribution at half of its maximum value, i.e., at a height of $A/2$. We find the points $y$ where $f(y) = A/2$:\n$$ \\frac{A}{2} = A \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right) $$\n$$ \\frac{1}{2} = \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right) $$\nTaking the natural logarithm of both sides:\n$$ \\ln\\left(\\frac{1}{2}\\right) = -\\frac{(y-\\mu)^2}{2\\sigma^2} $$\n$$ -\\ln(2) = -\\frac{(y-\\mu)^2}{2\\sigma^2} $$\n$$ (y-\\mu)^2 = 2\\sigma^2\\ln(2) $$\n$$ |y-\\mu| = \\sigma\\sqrt{2\\ln(2)} $$\nThe two points are $y_1 = \\mu - \\sigma\\sqrt{2\\ln(2)}$ and $y_2 = \\mu + \\sigma\\sqrt{2\\ln(2)}$. The FWHM is the distance between these points:\n$$ FWHM = y_2 - y_1 = (\\mu + \\sigma\\sqrt{2\\ln(2)}) - (\\mu - \\sigma\\sqrt{2\\ln(2)}) = 2\\sigma\\sqrt{2\\ln(2)} $$\nThis relationship, $FWHM = \\sigma \\cdot 2\\sqrt{2\\ln(2)}$, holds for any Gaussian distribution, including both the timing and spatial uncertainties.\n\nWe can now compute the required values. The given data are:\nTiming FWHM: $FWHM_t = 300\\ \\text{ps} = 300 \\times 10^{-12}\\ \\text{s} = 3 \\times 10^{-10}\\ \\text{s}$.\nSpeed of light: $c = 2.99792458 \\times 10^8\\ \\text{m/s}$.\n\nFirst, we compute the spatial FWHM, $FWHM_x$:\n$$ FWHM_x = \\frac{c}{2} FWHM_t = \\frac{2.99792458 \\times 10^8\\ \\text{m/s}}{2} \\times (3 \\times 10^{-10}\\ \\text{s}) $$\n$$ FWHM_x = (1.49896229 \\times 10^8\\ \\text{m/s}) \\times (3 \\times 10^{-10}\\ \\text{s}) = 0.0449688687\\ \\text{m} $$\nTo express the result in millimeters, we multiply by $1000$:\n$$ FWHM_x = 0.0449688687\\ \\text{m} \\times 1000\\ \\text{mm/m} = 44.9688687\\ \\text{mm} $$\nRounding to $4$ significant figures, we get:\n$$ FWHM_x \\approx 44.97\\ \\text{mm} $$\n\nNext, we compute the corresponding spatial standard deviation, $\\sigma_x$, using the derived relationship:\n$$ \\sigma_x = \\frac{FWHM_x}{2\\sqrt{2\\ln(2)}} $$\nUsing the unrounded value of $FWHM_x$ for precision:\n$$ \\sigma_x = \\frac{44.9688687\\ \\text{mm}}{2\\sqrt{2\\ln(2)}} \\approx \\frac{44.9688687\\ \\text{mm}}{2.354820045} \\approx 19.096530\\ \\text{mm} $$\nRounding to $4$ significant figures, we get:\n$$ \\sigma_x \\approx 19.10\\ \\text{mm} $$", "answer": "$$\\boxed{\\begin{pmatrix} 44.97 & 19.10 \\end{pmatrix}}$$", "id": "4907332"}, {"introduction": "The finite spatial resolution of any imaging system causes a phenomenon known as the Partial Volume Effect, where the signal from a small, active region \"spills over\" into adjacent regions, corrupting quantitative measurements. This computational practice introduces the Geometric Transfer Matrix (GTM) method, a powerful technique to model and correct for this effect by treating it as a linear mixing problem. By implementing a solution to this inverse problem, you will develop practical skills in a key image analysis task essential for accurate radiomics and clinical research [@problem_id:4556059].", "problem": "You are given a mathematical model of cross-contamination among multiple regions in Positron Emission Tomography (PET) arising from the imaging system’s finite resolution. Let the true region-wise activity concentrations be represented by the vector $\\mathbf{a} \\in \\mathbb{R}^{N}$, measured in kilobecquerels per milliliter $\\mathrm{(kBq/mL)}$. Because of the system Point Spread Function (PSF) blur, the measured region-wise mean activities $\\mathbf{m} \\in \\mathbb{R}^{N}$ are linear mixtures of the true activities. This can be modeled by a Geometric Transfer Matrix (GTM), $\\mathbf{G} \\in \\mathbb{R}^{N \\times N}$, where the element $G_{ij}$ denotes the fraction of activity originating from region $j$ that contributes to the measured mean in region $i$. The fundamental base is the linear mixing and mass conservation under blurring: the expected regional means satisfy $\\mathbf{m} = \\mathbf{G}\\mathbf{a} + \\boldsymbol{\\varepsilon}$, where $\\boldsymbol{\\varepsilon}$ is a stochastic error term with zero mean. This follows from the linearity of the convolution operator that models blur, and the linearity of regional averaging.\n\nYour task is to implement a program that, for each provided test case, estimates the true activities $\\hat{\\mathbf{a}}$ from the observed $\\mathbf{m}$ and a known GTM $\\mathbf{G}$ by solving a regularized least-squares problem that enforces physical plausibility. Starting from the principle of minimum squared discrepancy between predicted and observed regional means under a Tikhonov penalty on the activity magnitude, determine $\\hat{\\mathbf{a}}$ as the minimizer of a quadratic objective with non-negativity imposed on each component after solving. You must not assume any special structure beyond the given numeric $\\mathbf{G}$ and $\\mathbf{m}$. Express all estimated activities in $\\mathrm{kBq/mL}$ and round each component to exactly three decimal places.\n\nImplement the following test suite of cases. For each case, $\\mathbf{G}$ is given along with the observed $\\mathbf{m}$ and a regularization parameter $\\lambda \\ge 0$.\n\n- Case $1$ (no cross-contamination, baseline consistency):\n  - $N = 3$,\n  - $\\mathbf{G}_1 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$,\n  - $\\mathbf{m}_1 = \\begin{bmatrix} 5.0 \\\\ 12.0 \\\\ 0.0 \\end{bmatrix}$ $\\mathrm{(kBq/mL)}$,\n  - $\\lambda_1 = 0.0$.\n\n- Case $2$ (moderate symmetric spill-over with exact data):\n  - $N = 3$,\n  - $\\mathbf{G}_2 = \\begin{bmatrix} 0.8 & 0.1 & 0.1 \\\\ 0.15 & 0.7 & 0.15 \\\\ 0.1 & 0.2 & 0.7 \\end{bmatrix}$,\n  - $\\mathbf{m}_2 = \\begin{bmatrix} 8.3 \\\\ 3.05 \\\\ 2.1 \\end{bmatrix}$ $\\mathrm{(kBq/mL)}$,\n  - $\\lambda_2 = 0.0$.\n\n- Case $3$ (near-collinearity requiring regularization):\n  - $N = 3$,\n  - $\\mathbf{G}_3 = \\begin{bmatrix} 0.6 & 0.35 & 0.05 \\\\ 0.34 & 0.6 & 0.06 \\\\ 0.05 & 0.35 & 0.6 \\end{bmatrix}$,\n  - $\\mathbf{m}_3 = \\begin{bmatrix} 5.5 \\\\ 6.52 \\\\ 6.6 \\end{bmatrix}$ $\\mathrm{(kBq/mL)}$,\n  - $\\lambda_3 = 0.000001$.\n\n- Case $4$ (four-region coupling with a zero-activity region):\n  - $N = 4$,\n  - $\\mathbf{G}_4 = \\begin{bmatrix} 0.7 & 0.2 & 0.1 & 0.0 \\\\ 0.15 & 0.7 & 0.1 & 0.05 \\\\ 0.1 & 0.2 & 0.6 & 0.1 \\\\ 0.0 & 0.1 & 0.2 & 0.7 \\end{bmatrix}$,\n  - $\\mathbf{m}_4 = \\begin{bmatrix} 2.6 \\\\ 1.075 \\\\ 3.55 \\\\ 2.75 \\end{bmatrix}$ $\\mathrm{(kBq/mL)}$,\n  - $\\lambda_4 = 0.0$.\n\nAlgorithmic requirements:\n- For each case, compute an estimator $\\hat{\\mathbf{a}}$ by minimizing a Tikhonov-regularized sum of squared residuals, followed by elementwise truncation to enforce non-negativity on $\\hat{\\mathbf{a}}$.\n- Use $\\lambda$ exactly as specified for each case.\n- If a direct solve fails due to numerical issues for $\\lambda = 0$, use a least-squares fallback.\n\nOutput requirements:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- The overall output must be a list of lists: each inner list corresponds to one test case, in order, containing the components of $\\hat{\\mathbf{a}}$ expressed in $\\mathrm{kBq/mL}$, each rounded to exactly three decimal places (do not print the unit string in the output).\n- For example, a syntactically valid output format looks like `texttt{[[x_11,x_12,...],[x_21,x_22,...],...]}` where every $x_{ij}$ is a decimal string with exactly three digits after the decimal point.\n\nAnswer in $\\mathrm{kBq/mL}$ and ensure the final printed line strictly follows the specified format.", "solution": "The problem of estimating true regional activity concentrations in Positron Emission Tomography (PET) from measured data confounded by system blur is a well-defined inverse problem. The provided model, data, and algorithmic requirements are scientifically sound, mathematically consistent, and complete. Therefore, the problem is deemed valid and a solution can be formulated.\n\nThe core of the problem is to estimate the true activity vector $\\mathbf{a} \\in \\mathbb{R}^{N}$ from a measured activity vector $\\mathbf{m} \\in \\mathbb{R}^{N}$, given the linear model $\\mathbf{m} = \\mathbf{G}\\mathbf{a} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{G} \\in \\mathbb{R}^{N \\times N}$ is the Geometric Transfer Matrix (GTM) and $\\boldsymbol{\\varepsilon}$ is a zero-mean noise term. The task is to find an estimate $\\hat{\\mathbf{a}}$ that is physically plausible (non-negative) and robust to potential ill-conditioning of the matrix $\\mathbf{G}$.\n\nThe specified method is Tikhonov regularization, a standard technique for solving ill-posed inverse problems. We seek to find the vector $\\mathbf{a}$ that minimizes the following objective function $J(\\mathbf{a})$:\n$$\nJ(\\mathbf{a}) = \\|\\mathbf{G}\\mathbf{a} - \\mathbf{m}\\|_2^2 + \\lambda \\|\\mathbf{a}\\|_2^2\n$$\nHere, $\\|\\cdot\\|_2$ denotes the Euclidean norm. The first term, $\\|\\mathbf{G}\\mathbf{a} - \\mathbf{m}\\|_2^2$, is the sum of squared residuals, which enforces fidelity to the measured data. The second term, $\\lambda \\|\\mathbf{a}\\|_2^2$, is the regularization term, which penalizes solutions with large magnitudes. The regularization parameter $\\lambda \\ge 0$ controls the trade-off between data fidelity and solution stability.\n\nTo find the minimizer of $J(\\mathbf{a})$, we compute its gradient with respect to $\\mathbf{a}$ and set it to zero. The objective function can be written in matrix form as:\n$$\nJ(\\mathbf{a}) = (\\mathbf{G}\\mathbf{a} - \\mathbf{m})^T (\\mathbf{G}\\mathbf{a} - \\mathbf{m}) + \\lambda \\mathbf{a}^T \\mathbf{I} \\mathbf{a}\n$$\nExpanding this expression gives:\n$$\nJ(\\mathbf{a}) = \\mathbf{a}^T\\mathbf{G}^T\\mathbf{G}\\mathbf{a} - 2\\mathbf{m}^T\\mathbf{G}\\mathbf{a} + \\mathbf{m}^T\\mathbf{m} + \\lambda \\mathbf{a}^T\\mathbf{I}\\mathbf{a}\n$$\n$$\nJ(\\mathbf{a}) = \\mathbf{a}^T(\\mathbf{G}^T\\mathbf{G} + \\lambda\\mathbf{I})\\mathbf{a} - 2(\\mathbf{G}^T\\mathbf{m})^T\\mathbf{a} + \\mathbf{m}^T\\mathbf{m}\n$$\nThe gradient with respect to $\\mathbf{a}$ is:\n$$\n\\nabla_{\\mathbf{a}} J(\\mathbf{a}) = 2(\\mathbf{G}^T\\mathbf{G} + \\lambda\\mathbf{I})\\mathbf{a} - 2\\mathbf{G}^T\\mathbf{m}\n$$\nSetting the gradient to $\\mathbf{0}$ to find the minimum yields the normal equations for Tikhonov regularization:\n$$\n(\\mathbf{G}^T\\mathbf{G} + \\lambda\\mathbf{I})\\mathbf{a} = \\mathbf{G}^T\\mathbf{m}\n$$\nThe solution to this system of linear equations, which we denote $\\mathbf{a}^*$, is the unconstrained regularized least-squares estimate:\n$$\n\\mathbf{a}^* = (\\mathbf{G}^T\\mathbf{G} + \\lambda\\mathbf{I})^{-1} \\mathbf{G}^T\\mathbf{m}\n$$\nThe matrix $(\\mathbf{G}^T\\mathbf{G} + \\lambda\\mathbf{I})$ is invertible for any $\\lambda > 0$, ensuring a unique solution.\n\nThe problem specifies two algorithmic paths based on the value of $\\lambda$:\n1.  For $\\lambda > 0$: The system is solved as formulated above. The matrix $A = \\mathbf{G}^T\\mathbf{G} + \\lambda\\mathbf{I}$ and the vector $b = \\mathbf{G}^T\\mathbf{m}$ are constructed, and the linear system $A\\mathbf{a}^* = b$ is solved for $\\mathbf{a}^*$.\n2.  For $\\lambda = 0$: The problem simplifies to the standard linear least-squares problem, $\\min_{\\mathbf{a}} \\|\\mathbf{G}\\mathbf{a} - \\mathbf{m}\\|_2^2$. In this case, it is numerically preferable to use a dedicated least-squares solver (e.g., based on QR decomposition or SVD) rather than forming the normal equations $\\mathbf{G}^T\\mathbf{G}\\mathbf{a} = \\mathbf{G}^T\\mathbf{m}$, as forming $\\mathbf{G}^T\\mathbf{G}$ can square the condition number of the matrix and amplify numerical errors. This directly addresses the \"least-squares fallback\" requirement.\n\nAfter obtaining the solution $\\mathbf{a}^*$, the physical constraint of non-negative activity concentration must be enforced. As per the problem statement, this is done by element-wise truncation of any negative values to zero:\n$$\n\\hat{a}_i = \\max(0, a_i^*)\n$$\nwhere $\\hat{a}_i$ and $a_i^*$ are the $i$-th components of the final estimate $\\hat{\\mathbf{a}}$ and the intermediate solution $\\mathbf{a}^*$, respectively.\n\nThe final estimated activities in $\\hat{\\mathbf{a}}$ are then rounded to three decimal places for each test case as required.\n\nThis procedure will be applied to each of the four test cases.\n- Case 1 ($\\lambda_1 = 0.0$): A simple identity matrix, solved with least squares. We expect $\\hat{\\mathbf{a}}_1 = \\mathbf{m}_1$.\n- Case 2 ($\\lambda_2 = 0.0$): A symmetric contamination matrix, solved with least squares. The data is constructed to have an exact, positive solution.\n- Case 3 ($\\lambda_3 > 0$): A near-collinear matrix requiring regularization. The Tikhonov-regularized normal equations will be solved. The solution may contain negative components before the final truncation step.\n- Case 4 ($\\lambda_4 = 0.0$): A four-region system, solved with least squares. The data is constructed to correspond to an exact solution where one region has zero activity.", "answer": "```\n[[5.000,12.000,0.000],[10.000,2.000,1.000],[5.000,7.000,8.000],[2.000,0.000,5.000,3.000]]\n```", "id": "4556059"}]}