{"hands_on_practices": [{"introduction": "Before any radiomic features can be calculated, the raw pixel values stored in an image file must be converted into physically meaningful units. This first exercise focuses on a fundamental step for Computed Tomography (CT) data: transforming a stored value into the standardized Hounsfield Unit (HU) scale using metadata from the image header. This practice [@problem_id:4546167] not only builds your skill in applying the standard DICOM linear transformation but also encourages critical thinking about whether the resulting values are anatomically plausible.", "problem": "A radiomics pipeline ingests an axial slice from Computed Tomography (CT) stored in Digital Imaging and Communications in Medicine (DICOM) format. In CT, the patient-relevant intensity scale is the Hounsfield Unit (HU), which is defined from linear attenuation coefficients by the relation $$\\mathrm{HU} = 1000 \\cdot \\frac{\\mu - \\mu_{\\mathrm{water}}}{\\mu_{\\mathrm{water}}},$$ and, operationally for clinical images, is provided by a linear rescaling from the stored quantized pixel value. The DICOM header specifies a linear mapping from stored values to real-world values of the form $$x_{\\mathrm{real}} = m \\cdot x_{\\mathrm{stored}} + b,$$ where $m$ is the rescale slope and $b$ is the rescale intercept. Assume this real-world value is the Hounsfield Unit (HU). A voxel has stored value $x_{\\mathrm{stored}} = 1240$, with $\\mathrm{RescaleSlope} = m = 1.2$ and $\\mathrm{RescaleIntercept} = b = -1024$.\n\nUsing only the stated definitions and the linear rescaling convention above, compute the Hounsfield Unit (HU) for this voxel. Then, based on typical CT tissue ranges, assess whether this mapped HU is physically plausible given human anatomy and clinical CT practice. Provide the computed HU as your final numerical answer (no units). If any rounding is required, round to four significant figures; otherwise, give the exact value.", "solution": "The problem requires the computation of a Hounsfield Unit (HU) value from a stored pixel value using a linear rescaling transformation defined in the DICOM standard, and then an assessment of its physical plausibility.\n\nFirst, the problem validation is conducted.\n- **Givens Extraction**:\n  - Hounsfield Unit definition: $\\mathrm{HU} = 1000 \\cdot \\frac{\\mu - \\mu_{\\mathrm{water}}}{\\mu_{\\mathrm{water}}}$\n  - DICOM linear rescaling equation: $x_{\\mathrm{real}} = m \\cdot x_{\\mathrm{stored}} + b$\n  - Assumption: $x_{\\mathrm{real}}$ is the Hounsfield Unit, so $\\mathrm{HU} = x_{\\mathrm{real}}$.\n  - Stored voxel value: $x_{\\mathrm{stored}} = 1240$\n  - Rescale slope: $m = 1.2$\n  - Rescale intercept: $b = -1024$\n- **Validation Verdict**: The problem is scientifically grounded, well-posed, objective, and self-contained. The definitions and values provided (e.g., the formula for HU, the linear rescaling model, and the specific intercept $b = -1024$) are standard and correct within the field of medical imaging and the DICOM specification for Computed Tomography (CT). The problem is therefore deemed **valid**.\n\nThe solution process proceeds as follows.\n\nThe relationship between the stored value ($x_{\\mathrm{stored}}$) and the real-world physical value ($x_{\\mathrm{real}}$) is given by the linear equation:\n$$x_{\\mathrm{real}} = m \\cdot x_{\\mathrm{stored}} + b$$\nThe problem states that for this CT image, the real-world value is the Hounsfield Unit (HU). Therefore, we can write:\n$$\\mathrm{HU} = m \\cdot x_{\\mathrm{stored}} + b$$\nThe problem provides the following values:\n- The stored value, $x_{\\mathrm{stored}} = 1240$.\n- The rescale slope, $m = 1.2$.\n- The rescale intercept, $b = -1024$.\n\nSubstituting these values into the equation yields the HU value for the voxel:\n$$\\mathrm{HU} = (1.2) \\cdot (1240) + (-1024)$$\nPerforming the multiplication first:\n$$1.2 \\times 1240 = 1488$$\nNow, substitute this result back into the equation and perform the addition:\n$$\\mathrm{HU} = 1488 - 1024$$\n$$\\mathrm{HU} = 464$$\nThe calculation yields an exact integer value. The problem requires rounding to four significant figures only if rounding is necessary. Since $464$ is an exact result with three significant figures, no rounding is performed.\n\nThe second part of the task is to assess the physical plausibility of this HU value. The Hounsfield scale is a quantitative scale for describing radiodensity. It is defined by reference values for distilled water and air:\n- The radiodensity of distilled water at standard temperature and pressure is defined as $0$ HU.\n- The radiodensity of air is, by convention, $-1000$ HU.\n\nTypical HU values for various human tissues and substances in clinical CT images are well-established:\n- Fat: Approximately $-100$ HU to $-50$ HU.\n- Soft tissues (e.g., muscle, liver, spleen, brain): Approximately $+10$ HU to $+100$ HU.\n- Blood (uncoagulated): Approximately $+30$ HU to $+45$ HU.\n- Cancellous (spongy) bone: Approximately $+200$ HU to $+700$ HU.\n- Cortical (dense) bone: Greater than $+1000$ HU, often up to $+3000$ HU.\n- Pathological calcifications: Typically in the range of $+150$ HU to $+1000$ HU or more.\n\nThe computed value is $\\mathrm{HU} = 464$. Comparing this value to the typical ranges, we observe that it falls squarely within the range for cancellous bone. It is also a plausible value for a dense calcification, such as might be found in an artery or a kidney stone. The value is significantly higher than that of soft tissue or fat, and lower than that of dense cortical bone. Therefore, finding a voxel with an intensity of $464$ HU within a human CT scan is entirely physically plausible and would likely correspond to a bony structure.", "answer": "$$\\boxed{464}$$", "id": "4546167"}, {"introduction": "Once image intensities are in a physical scale like HU, many radiomic features, especially those describing texture, require a process called quantization or discretization. This practice explores two common methods, Fixed Bin Number (FBN) and Fixed Bin Width (FBW), for grouping a wide range of continuous intensities into a smaller, fixed number of discrete gray levels. Mastering these techniques [@problem_id:4546204] is essential for understanding how feature stability and reproducibility are achieved in a radiomics workflow.", "problem": "A radiomics preprocessing pipeline requires discretization of image intensities before computing texture features. In this context, discretization replaces a continuous or high-resolution intensity value by a gray-level bin index to achieve stability and reproducibility across scans. Two widely used schemes are Fixed Bin Number (FBN) and Fixed Bin Width (FBW), both of which assume one-based bin indexing. Under FBN, the range from the minimum intensity $I_{\\min}$ to the maximum intensity $I_{\\max}$ is linearly partitioned into $N_{g}$ bins and intensities are mapped to these bins by proportional scaling of their position within the range. Under FBW, the intensity axis is partitioned into bins of constant width $w$, and the bin index is determined by the count of whole widths between the intensity and $I_{\\min}$; for values exactly equal to $I_{\\max}$, the index is assigned to the highest bin by convention. In this problem, choose $w$ such that exactly $N_{g}$ bins cover the half-open interval $\\left[I_{\\min}, I_{\\max}\\right)$, i.e., $w = \\left(I_{\\max} - I_{\\min}\\right)/N_{g}$.\n\nGiven a voxel intensity $I = 75$ with $I_{\\min} = 0$, $I_{\\max} = 255$, and $N_{g} = 32$, compute the discretized gray-level under both the Fixed Bin Number (FBN) and Fixed Bin Width (FBW) schemes. Express your final answer as a row matrix containing two entries in the order $\\left(\\text{FBN}, \\text{FBW}\\right)$. No rounding is required and no physical units apply to bin indices.", "solution": "The problem statement has been validated and is deemed self-contained, scientifically grounded, and well-posed. All necessary data and definitions for computing the discretized gray-levels under the Fixed Bin Number (FBN) and Fixed Bin Width (FBW) schemes are provided.\n\nLet the given parameters be:\n- Voxel intensity: $I = 75$\n- Minimum intensity: $I_{\\min} = 0$\n- Maximum intensity: $I_{\\max} = 255$\n- Number of gray-level bins: $N_{g} = 32$\n\nThe bin indices are one-based, meaning they range from $1$ to $N_{g}$.\n\nFirst, we compute the discretized gray-level for the Fixed Bin Number (FBN) scheme.\nThe FBN scheme partitions the intensity range $[I_{\\min}, I_{\\max}]$ into $N_{g}$ bins. The description \"proportional scaling of their position within the range\" implies a linear mapping. The position of an intensity $I$ relative to the start of the range is given by the difference $I - I_{\\min}$. The total span of the range is $I_{\\max} - I_{\\min}$. The fractional position of $I$ within the range is $\\frac{I - I_{\\min}}{I_{\\max} - I_{\\min}}$.\n\nTo map this fractional position, which lies in $[0, 1]$, to one of $N_{g}$ bins, we multiply by $N_{g}$. This gives a value in the range $[0, N_{g}]$. Since binning requires assigning an intensity to a single discrete bin, we use the floor function to find which bin interval the value falls into. Specifically, the number of full bin widths that fit between $I_{\\min}$ and $I$ determines the zero-based index. Adding $1$ converts this to the required one-based index.\nThe formula for the FBN bin index, $k_{\\text{FBN}}$, is therefore:\n$$k_{\\text{FBN}} = \\left\\lfloor N_{g} \\frac{I - I_{\\min}}{I_{\\max} - I_{\\min}} \\right\\rfloor + 1$$\nThis formula correctly maps $I = I_{\\min}$ to bin $1$. For an intensity $I$ approaching $I_{\\max}$, the argument of the floor function approaches $N_{g}$, resulting in a bin index of $N_{g}$. For $I=I_{\\max}$, the formula yields $N_g+1$, which would require a special handling rule (e.g., assigning to bin $N_g$). However, for the given intensity $I=75$, we are not at the endpoint, so the general formula is sufficient.\n\nSubstituting the given values:\n$$k_{\\text{FBN}} = \\left\\lfloor 32 \\times \\frac{75 - 0}{255 - 0} \\right\\rfloor + 1$$\nWe simplify the fraction:\n$$\\frac{75}{255} = \\frac{15 \\times 5}{15 \\times 17} = \\frac{5}{17}$$\nThe expression becomes:\n$$k_{\\text{FBN}} = \\left\\lfloor 32 \\times \\frac{5}{17} \\right\\rfloor + 1 = \\left\\lfloor \\frac{160}{17} \\right\\rfloor + 1$$\nSince $17 \\times 9 = 153$ and $17 \\times 10 = 170$, the value of the fraction $\\frac{160}{17}$ is between $9$ and $10$. Specifically, $\\frac{160}{17} \\approx 9.41$.\n$$k_{\\text{FBN}} = \\lfloor 9.41... \\rfloor + 1 = 9 + 1 = 10$$\nThus, the FBN gray-level is $10$.\n\nNext, we compute the discretized gray-level for the Fixed Bin Width (FBW) scheme.\nThe problem specifies that the bin width, $w$, is chosen such that $N_{g}$ bins cover the half-open interval $[I_{\\min}, I_{\\max})$. This leads to the definition:\n$$w = \\frac{I_{\\max} - I_{\\min}}{N_{g}}$$\nThe bin index, $k_{\\text{FBW}}$, is determined by \"the count of whole widths between the intensity and $I_{\\min}$\". This translates directly to using the floor function on the ratio of the intensity's position to the bin width. With one-based indexing, the formula is:\n$$k_{\\text{FBW}} = \\left\\lfloor \\frac{I - I_{\\min}}{w} \\right\\rfloor + 1$$\nSubstituting the expression for $w$ into this formula gives:\n$$k_{\\text{FBW}} = \\left\\lfloor \\frac{I - I_{\\min}}{(I_{\\max} - I_{\\min}) / N_{g}} \\right\\rfloor + 1 = \\left\\lfloor N_{g} \\frac{I - I_{\\min}}{I_{\\max} - I_{\\min}} \\right\\rfloor + 1$$\nThis analytical expression is identical to the one derived for the FBN scheme. This is a direct consequence of the problem's specific constraint on $w$ for the FBW method, which forces the parameters of both methods to be equivalent.\n\nTherefore, the numerical calculation for $k_{\\text{FBW}}$ is identical to that for $k_{\\text{FBN}}$.\n$$k_{\\text{FBW}} = \\left\\lfloor \\frac{160}{17} \\right\\rfloor + 1 = 9 + 1 = 10$$\nThe FBW gray-level is also $10$.\n\nThe final answer is a row matrix containing the computed gray-levels in the order (FBN, FBW). Both values are $10$.", "answer": "$$\\boxed{\\begin{pmatrix} 10 & 10 \\end{pmatrix}}$$", "id": "4546204"}, {"introduction": "This final practice is a comprehensive challenge that integrates the concepts of spatial sampling and intensity quantization into a complete radiomics task: the calculation of texture features from a Gray Level Co-occurrence Matrix (GLCM). You will develop a program that accounts for voxel spacing to measure relationships at a specific physical distance and uses quantized gray levels to build the GLCM. This capstone exercise [@problem_id:4546164] provides invaluable hands-on experience, bridging the gap between theoretical definitions and the practical implementation of a feature extraction pipeline.", "problem": "You are given three three-dimensional image datasets and asked to compute Gray Level Co-occurrence Matrix (GLCM) features at a specified physical separation using a principled approach grounded in radiomics. Begin from core definitions of image sampling on a voxel grid, intensity quantization, and co-occurrence statistics. Implement a complete program that, for each dataset, selects appropriate integer voxel offsets to approximate a fixed physical separation in millimeters, constructs an aggregated symmetric and normalized GLCM across a set of orientations, and computes a set of standard co-occurrence texture features.\n\nThe fundamental base includes the following definitions. A three-dimensional image is defined on a regular sampling grid with voxel indices $\\mathbf{r} = (x,y,z) \\in \\mathbb{Z}^3$ and voxel spacing $\\mathbf{s} = (s_x, s_y, s_z)$ in $\\mathrm{mm}$ along each axis. The physical displacement corresponding to an index offset $\\mathbf{o} = (o_x, o_y, o_z) \\in \\mathbb{Z}^3$ is given by the Euclidean length\n$$\nd_{\\mathrm{phys}}(\\mathbf{o}; \\mathbf{s}) = \\sqrt{ (o_x s_x)^2 + (o_y s_y)^2 + (o_z s_z)^2 } \\, .\n$$\nIntensity quantization maps raw intensities $I(\\mathbf{r}) \\in \\mathbb{R}$ to discrete gray levels $g(\\mathbf{r}) \\in \\{0,1,\\dots,G-1\\}$ using a fixed bin width $w > 0$ as\n$$\ng(\\mathbf{r}) = \\left\\lfloor \\frac{I(\\mathbf{r}) - I_{\\min}}{w} \\right\\rfloor \\, ,\n$$\nwhere $I_{\\min}$ is the minimum intensity in the dataset, and the number of bins is\n$$\nG = \\left\\lfloor \\frac{I_{\\max} - I_{\\min}}{w} \\right\\rfloor + 1 \\, ,\n$$\nwith $I_{\\max}$ the maximum intensity. The bin index is clipped to the range $[0, G-1]$ if necessary.\n\nThe Gray Level Co-occurrence Matrix (GLCM) for a given offset $\\mathbf{o}$ counts occurrences of gray-level pairs separated by $\\mathbf{o}$. For all valid voxel positions $\\mathbf{r}$ such that $\\mathbf{r} + \\mathbf{o}$ lies within the image bounds, define the count matrix\n$$\nC_{i,j}(\\mathbf{o}) = \\#\\{\\mathbf{r} \\mid g(\\mathbf{r}) = i,\\; g(\\mathbf{r}+\\mathbf{o}) = j\\} \\, .\n$$\nAggregate counts across a set of orientation offsets $\\mathcal{O}$ to obtain\n$$\nC_{i,j}^{\\mathrm{agg}} = \\sum_{\\mathbf{o} \\in \\mathcal{O}} C_{i,j}(\\mathbf{o}) \\, .\n$$\nForm the symmetric co-occurrence matrix\n$$\n\\tilde{C} = C^{\\mathrm{agg}} + (C^{\\mathrm{agg}})^{\\top} \\, ,\n$$\nand normalize to a probability matrix\n$$\nP = \\frac{\\tilde{C}}{\\sum_{i=0}^{G-1}\\sum_{j=0}^{G-1} \\tilde{C}_{i,j}} \\, .\n$$\nUsing $P$, compute the following features:\n- Contrast:\n$$\n\\mathrm{contrast} = \\sum_{i=0}^{G-1}\\sum_{j=0}^{G-1} P_{i,j} (i - j)^2 \\, .\n$$\n- Energy (Angular Second Moment):\n$$\n\\mathrm{energy} = \\sum_{i=0}^{G-1}\\sum_{j=0}^{G-1} P_{i,j}^2 \\, .\n$$\n- Homogeneity:\n$$\n\\mathrm{homogeneity} = \\sum_{i=0}^{G-1}\\sum_{j=0}^{G-1} \\frac{P_{i,j}}{1 + |i - j|} \\, .\n$$\n- Entropy (base-$2$ logarithm; zero probability contributes zero):\n$$\n\\mathrm{entropy} = - \\sum_{i=0}^{G-1}\\sum_{j=0}^{G-1} P_{i,j} \\log_2(P_{i,j}) \\, .\n$$\n\nTo select orientation offsets $\\mathcal{O}$ that approximate a target physical distance, begin from the set of all unit directions with components in $\\{-1,0,1\\}$ excluding the zero vector, and reduce by the equivalence relation $\\mathbf{v} \\sim -\\mathbf{v}$ to obtain $13$ unique orientations (this is the canonical set combining the three axial directions, the six face diagonals, and the four body diagonals). For each canonical unit orientation $\\mathbf{c}$, compute the base physical step length\n$$\n\\ell(\\mathbf{c}; \\mathbf{s}) = \\sqrt{ (c_x s_x)^2 + (c_y s_y)^2 + (c_z s_z)^2 } \\, ,\n$$\nand choose an integer multiplier\n$$\nk(\\mathbf{c}) = \\max\\!\\left(1, \\left\\lfloor \\frac{d}{\\ell(\\mathbf{c}; \\mathbf{s})} + \\tfrac{1}{2} \\right\\rfloor \\right) \\, ,\n$$\nto form the integer offset $\\mathbf{o} = k(\\mathbf{c}) \\mathbf{c}$. This choice minimizes the absolute error $|d_{\\mathrm{phys}}(\\mathbf{o}; \\mathbf{s}) - d|$ among integer multiples of $\\mathbf{c}$. Use a tolerance of $0.5\\,\\mathrm{mm}$ to report whether the achieved physical distance is close to the target, but always use the nearest integer multiplier $\\ge 1$ for constructing $\\mathcal{O}$.\n\nImplement the above for the following test suite, with target physical distance $d = 5\\,\\mathrm{mm}$.\n\n- Dataset $\\mathsf{A}$ (happy path, isotropic spacing):\n  - Size: $4 \\times 4 \\times 4$ voxels.\n  - Spacing: $\\mathbf{s} = (1, 1, 1)\\,\\mathrm{mm}$.\n  - Quantization bin width: $w = 10$ (intensity units).\n  - Intensities: for indices $(x,y,z)$ with $x,y,z \\in \\{0,1,2,3\\}$,\n    $$\n    I(x,y,z) = 10 x + 5 y + 2 z \\, .\n    $$\n\n- Dataset $\\mathsf{B}$ (anisotropic spacing, exact axial match):\n  - Size: $4 \\times 4 \\times 3$ voxels.\n  - Spacing: $\\mathbf{s} = (2, 2, 5)\\,\\mathrm{mm}$.\n  - Quantization bin width: $w = 8$ (intensity units).\n  - Intensities: for indices $(x,y,z)$ with $x,y \\in \\{0,1,2,3\\}$ and $z \\in \\{0,1,2\\}$,\n    $$\n    I(x,y,z) = 4 x + 12 y + 1 z \\, .\n    $$\n\n- Dataset $\\mathsf{C}$ (edge case, uniform image):\n  - Size: $3 \\times 3 \\times 3$ voxels.\n  - Spacing: $\\mathbf{s} = (1.25, 1.25, 1.25)\\,\\mathrm{mm}$.\n  - Quantization bin width: $w = 100$ (intensity units).\n  - Intensities: for indices $(x,y,z)$ with $x,y,z \\in \\{0,1,2\\}$,\n    $$\n    I(x,y,z) = 50 \\, .\n    $$\n\nFor each dataset, construct the set of $13$ canonical unit orientations as described, compute the integer voxel offsets that approximate the target distance $d = 5\\,\\mathrm{mm}$, build the aggregated symmetric normalized GLCM, and compute the four features defined above. Features are unitless; express each as a decimal rounded to six places.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each dataset contributes a sublist of four numbers in the order $\\big[\\mathrm{contrast}, \\mathrm{energy}, \\mathrm{homogeneity}, \\mathrm{entropy}\\big]$. For example, the format must be\n$$\n\\big[ [a_1,a_2,a_3,a_4], [b_1,b_2,b_3,b_4], [c_1,c_2,c_3,c_4] \\big]\n$$\nwith each $a_i$, $b_i$, $c_i$ a decimal rounded to six places. No units appear in the output because the features are dimensionless.", "solution": "The user has provided a well-defined problem in the domain of radiomics. The problem is to compute a set of Gray Level Co-occurrence Matrix (GLCM) features for three distinct three-dimensional image datasets. The validation process confirms that the problem statement is scientifically sound, self-contained, and algorithmically specified, thus it is deemed valid. A step-by-step solution will be developed and implemented based on the provided definitions and methodologies.\n\nThe solution process for each dataset involves the following stages:\n1.  **Image Generation**: A three-dimensional NumPy array is created representing the image voxels. The intensity $I(\\mathbf{r})$ at each voxel location $\\mathbf{r}=(x,y,z)$ is computed according to the function provided for the dataset.\n2.  **Intensity Quantization**: The raw intensity values are mapped to a discrete set of gray levels. First, the minimum ($I_{\\min}$) and maximum ($I_{\\max}$) intensities in the image are determined. The number of gray levels, $G$, is calculated as $G = \\lfloor (I_{\\max} - I_{\\min}) / w \\rfloor + 1$, where $w$ is the specified bin width. Each intensity $I(\\mathbf{r})$ is then quantized to a gray level $g(\\mathbf{r}) = \\lfloor (I(\\mathbf{r}) - I_{\\min}) / w \\rfloor$. The resulting gray levels are integers in the range $\\{0, 1, \\dots, G-1\\}$.\n3.  **Offset Vector Calculation**: A set of $13$ offset vectors, $\\mathcal{O}$, is determined to approximate a target physical distance $d=5\\,\\mathrm{mm}$. This begins by defining the $13$ canonical orientations, which are unit vectors $\\mathbf{c}$ with components in $\\{-1, 0, 1\\}$, where antipodal vectors ($\\mathbf{c}$ and $-\\mathbf{c}$) are considered equivalent. For each canonical orientation $\\mathbf{c}$ and the given voxel spacing $\\mathbf{s}=(s_x, s_y, s_z)$, the base physical step length $\\ell(\\mathbf{c}; \\mathbf{s}) = \\sqrt{(c_x s_x)^2 + (c_y s_y)^2 + (c_z s_z)^2}$ is computed. An integer multiplier $k(\\mathbf{c}) = \\max(1, \\lfloor d/\\ell(\\mathbf{c};\\mathbf{s}) + 0.5 \\rfloor)$ is chosen to scale the orientation vector, yielding the final integer voxel offset $\\mathbf{o} = k(\\mathbf{c})\\mathbf{c}$.\n4.  **Aggregated GLCM Construction**: An empty $G \\times G$ matrix, $C^{\\text{agg}}$, is initialized. For each offset vector $\\mathbf{o} \\in \\mathcal{O}$, all valid voxel pairs $(\\mathbf{r}, \\mathbf{r}+\\mathbf{o})$ that lie within the image boundaries are identified. For each such pair, the count $C^{\\text{agg}}_{i,j}$ is incremented, where $i=g(\\mathbf{r})$ and $j=g(\\mathbf{r}+\\mathbf{o})$. This process is performed efficiently using vectorized NumPy operations.\n5.  **Symmetrization and Normalization**: The aggregated matrix is made symmetric by adding its transpose: $\\tilde{C} = C^{\\mathrm{agg}} + (C^{\\mathrm{agg}})^{\\top}$. The resulting symmetric matrix is then normalized by dividing each element by the total sum of all elements, producing the probability matrix $P_{i,j}$. If the total sum is zero (i.e., no co-occurring pairs were found), the probability matrix is a zero matrix.\n6.  **Feature Computation**: Four standard GLCM features are computed from the probability matrix $P$. For a $G \\times G$ matrix $P$:\n    - Contrast: $\\mathrm{contrast} = \\sum_{i,j=0}^{G-1} P_{i,j} (i - j)^2$\n    - Energy: $\\mathrm{energy} = \\sum_{i,j=0}^{G-1} P_{i,j}^2$\n    - Homogeneity: $\\mathrm{homogeneity} = \\sum_{i,j=0}^{G-1} \\frac{P_{i,j}}{1 + |i - j|}$\n    - Entropy: $\\mathrm{entropy} = - \\sum_{i,j=0}^{G-1} P_{i,j} \\log_2(P_{i,j})$, where the term is taken to be zero if $P_{i,j}=0$.\n\nThis entire procedure is applied to each of the three datasets ($\\mathsf{A}$, $\\mathsf{B}$, and $\\mathsf{C}$), and the final computed features are rounded to six decimal places and formatted into the specified output string.\n\nThe implementation will use `numpy` for efficient numerical computation and array manipulation, and `itertools` from the standard library to generate the canonical orientations. The code is modularized, with a primary function to process each dataset and a main `solve` function to orchestrate the tests and format the final output.", "answer": "```python\nimport numpy as np\nimport itertools\n\ndef calculate_glcm_features(size, spacing, w, I_func, d):\n    \"\"\"\n    Computes GLCM features for a given 3D image dataset.\n\n    Args:\n        size (tuple): The dimensions of the image (sz_x, sz_y, sz_z).\n        spacing (tuple): The voxel spacing in mm (s_x, s_y, s_z).\n        w (float): The quantization bin width.\n        I_func (callable): A function I(x,y,z) that gives the intensity at a voxel index.\n        d (float): The target physical distance in mm for co-occurrence.\n\n    Returns:\n        list: A list of four computed features [contrast, energy, homogeneity, entropy],\n              each rounded to six decimal places.\n    \"\"\"\n    # Step 1: Generate Image Data\n    sz_x, sz_y, sz_z = size\n    coords = np.mgrid[0:sz_x, 0:sz_y, 0:sz_z]\n    image = I_func(coords[0], coords[1], coords[2])\n\n    # Step 2: Quantize Image\n    I_min, I_max = np.min(image), np.max(image)\n    if I_min == I_max:\n        G = 1\n        quantized_image = np.zeros(size, dtype=np.int32)\n    else:\n        G = int(np.floor((I_max - I_min) / w)) + 1\n        quantized_image = np.floor((image - I_min) / w).astype(np.int32)\n        quantized_image = np.clip(quantized_image, 0, G - 1)\n\n    # Step 3: Determine Offsets\n    canonical_dirs = []\n    seen_negations = set()\n    for direction in itertools.product([-1, 0, 1], repeat=3):\n        if direction == (0, 0, 0):\n            continue\n        if direction not in seen_negations:\n            canonical_dirs.append(np.array(direction))\n            neg_dir = tuple(-x for x in direction)\n            seen_negations.add(neg_dir)\n\n    offsets = []\n    s_arr = np.array(spacing)\n    for c in canonical_dirs:\n        ell_c = np.sqrt(np.sum((c * s_arr)**2))\n        k = max(1, int(np.floor(d / ell_c + 0.5))) if ell_c > 0 else 1\n        o = k * c\n        offsets.append(o.astype(int))\n\n    # Step 4: Compute Aggregated GLCM\n    C_agg = np.zeros((G, G), dtype=np.int64)\n    for o in offsets:\n        ox, oy, oz = o\n        \n        # Calculate slices for the valid region for this offset\n        from_slices = (\n            slice(-min(0, ox), sz_x - max(0, ox)),\n            slice(-min(0, oy), sz_y - max(0, oy)),\n            slice(-min(0, oz), sz_z - max(0, oz))\n        )\n        to_slices = (\n            slice(max(0, ox), sz_x + min(0, ox)),\n            slice(max(0, oy), sz_y + min(0, oy)),\n            slice(max(0, oz), sz_z + min(0, oz))\n        )\n        \n        i_levels = quantized_image[from_slices].flatten()\n        j_levels = quantized_image[to_slices].flatten()\n        \n        if i_levels.size > 0:\n            np.add.at(C_agg, (i_levels, j_levels), 1)\n\n    # Step 5: Symmetrize and Normalize\n    C_sym = C_agg + C_agg.T\n    total_sum = np.sum(C_sym)\n    \n    if total_sum == 0:\n        P = np.zeros((G, G), dtype=np.float64)\n    else:\n        P = C_sym / total_sum\n\n    # Step 6: Compute Features\n    i_indices, j_indices = np.ogrid[0:G, 0:G]\n    \n    contrast = np.sum(P * (i_indices - j_indices)**2)\n    energy = np.sum(P**2)\n    homogeneity = np.sum(P / (1 + np.abs(i_indices - j_indices)))\n    \n    non_zero_P = P[P > 0]\n    entropy = -np.sum(non_zero_P * np.log2(non_zero_P))\n\n    features = [contrast, energy, homogeneity, entropy]\n    return [round(f, 6) for f in features]\n\ndef solve():\n    \"\"\"\n    Main solver function to process all datasets and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"size\": (4, 4, 4), \"spacing\": (1.0, 1.0, 1.0), \"w\": 10.0,\n            \"I_func\": lambda x, y, z: 10 * x + 5 * y + 2 * z\n        },\n        {\n            \"size\": (4, 4, 3), \"spacing\": (2.0, 2.0, 5.0), \"w\": 8.0,\n            \"I_func\": lambda x, y, z: 4 * x + 12 * y + 1 * z\n        },\n        {\n            \"size\": (3, 3, 3), \"spacing\": (1.25, 1.25, 1.25), \"w\": 100.0,\n            \"I_func\": lambda x, y, z: 50.0\n        }\n    ]\n    target_d = 5.0\n\n    all_results = []\n    for case in test_cases:\n        features = calculate_glcm_features(\n            case[\"size\"], case[\"spacing\"], case[\"w\"], case[\"I_func\"], target_d\n        )\n        all_results.append(features)\n\n    # Format the output string as required\n    sublist_strings = []\n    for res in all_results:\n        formatted_res = [f\"{val:.6f}\" for val in res]\n        sublist_strings.append(f\"[{','.join(formatted_res)}]\")\n    final_string = f\"[{','.join(sublist_strings)}]\"\n    \n    print(final_string)\n\nsolve()\n```", "id": "4546164"}]}