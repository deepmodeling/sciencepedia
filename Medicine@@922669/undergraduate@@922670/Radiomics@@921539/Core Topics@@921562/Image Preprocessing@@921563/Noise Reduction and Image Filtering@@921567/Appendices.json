{"hands_on_practices": [{"introduction": "Gaussian filtering is a fundamental tool for noise reduction in radiomics. For reproducible research, the degree of smoothing is best specified in physical units, like the Full Width at Half Maximum (FWHM) in millimeters. This practice guides you through the crucial process of translating this physical specification into the correct digital filter parameters ($\\sigma$ in voxels) for an image with non-uniform voxel sizes, a common scenario in clinical CT scans. [@problem_id:4553384]", "problem": "A three-dimensional Computed Tomography (CT) image in a radiomics pipeline is to be smoothed by a separable Gaussian filter to reduce high-frequency noise while preserving lesion shape characteristics. The filter acts along each Cartesian axis with identical physical blurring, and its smoothing profile per axis in one dimension is modeled by the Gaussian function $g(x) = A \\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)$, where $A$ is the peak amplitude at $x=0$ and $\\sigma$ is the standard deviation. The desired blurring is specified in physical space by the Full Width at Half Maximum (FWHM), defined as the width between the two points where $g(x)$ equals half of its maximum value. \n\nStarting from the definition of the Gaussian function and the definition of Full Width at Half Maximum (FWHM), derive the analytic relationship between FWHM and $\\sigma$ without using any pre-stated shortcut formulas. Then, for a target FWHM of $W = 6\\,\\mathrm{mm}$ applied isotropically in physical space, compute the standard deviation in millimeters, and convert it to per-axis standard deviations in voxels for a CT image with voxel sizes $s_{x} = 0.8\\,\\mathrm{mm}$, $s_{y} = 0.8\\,\\mathrm{mm}$, and $s_{z} = 1.5\\,\\mathrm{mm}$. Express the final answer as a row matrix containing $(\\sigma_{x}, \\sigma_{y}, \\sigma_{z})$ in voxels, where $\\sigma_{i} = \\sigma_{\\mathrm{mm}}/s_{i}$ for $i \\in \\{x,y,z\\}$. Round each component to four significant figures. The final answer must be given in voxels.", "solution": "The problem statement is evaluated for validity prior to attempting a solution.\n\n### Step 1: Extract Givens\n- Gaussian function: $g(x) = A \\exp(-\\frac{x^{2}}{2\\sigma^{2}})$\n- $A$: peak amplitude at $x=0$.\n- $\\sigma$: standard deviation.\n- FWHM (Full Width at Half Maximum) definition: The width between the two points where $g(x)$ equals half of its maximum value.\n- Target FWHM: $W = 6\\,\\mathrm{mm}$.\n- Smoothing is applied isotropically in physical space.\n- Voxel sizes: $s_{x} = 0.8\\,\\mathrm{mm}$, $s_{y} = 0.8\\,\\mathrm{mm}$, and $s_{z} = 1.5\\,\\mathrm{mm}$.\n- Conversion formula for standard deviation in voxels: $\\sigma_{i} = \\sigma_{\\mathrm{mm}}/s_{i}$ for $i \\in \\{x,y,z\\}$.\n- Required output: A row matrix $(\\sigma_{x}, \\sigma_{y}, \\sigma_{z})$ in voxels, with each component rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as it deals with standard definitions and applications of Gaussian functions and FWHM in the context of image processing, specifically within the field of radiomics. The values provided are realistic for clinical CT imaging. The problem is well-posed, providing all necessary information and definitions to arrive at a unique solution. It is objective and uses precise, unambiguous language. The problem does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A full, reasoned solution will be provided.\n\n### Solution Derivation\n\nThe solution proceeds in three parts: first, deriving the analytical relationship between the Full Width at Half Maximum ($W$, denoted as FWHM in the problem) and the standard deviation ($\\sigma$); second, calculating the standard deviation in physical units (millimeters) for the given $W$; and third, converting this physical standard deviation into per-axis standard deviations in voxel units.\n\n**Part 1: Derivation of the relationship between FWHM and $\\sigma$**\n\nThe one-dimensional Gaussian function is given by:\n$$g(x) = A \\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)$$\nThe maximum value of the function, $g_{\\mathrm{max}}$, occurs at the peak of the Gaussian, which is at $x=0$. Substituting $x=0$ into the equation gives:\n$$g_{\\mathrm{max}} = g(0) = A \\exp\\left(-\\frac{0^{2}}{2\\sigma^{2}}\\right) = A \\exp(0) = A$$\nThe FWHM is defined as the width between the two points $x$ where the function's value is half of its maximum. We set $g(x) = \\frac{1}{2} g_{\\mathrm{max}}$:\n$$g(x) = \\frac{A}{2}$$\nSubstituting the expression for $g(x)$, we get:\n$$A \\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right) = \\frac{A}{2}$$\nAssuming $A \\neq 0$, we can divide both sides by $A$:\n$$\\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right) = \\frac{1}{2}$$\nTo solve for $x$, we take the natural logarithm ($\\ln$) of both sides:\n$$\\ln\\left[\\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)\\right] = \\ln\\left(\\frac{1}{2}\\right)$$\n$$-\\frac{x^{2}}{2\\sigma^{2}} = -\\ln(2)$$\nMultiplying by $-1$ on both sides yields:\n$$\\frac{x^{2}}{2\\sigma^{2}} = \\ln(2)$$\nNow, we solve for $x^2$:\n$$x^{2} = 2\\sigma^{2}\\ln(2)$$\nAnd finally for $x$:\n$$x = \\pm \\sqrt{2\\sigma^{2}\\ln(2)} = \\pm \\sigma\\sqrt{2\\ln(2)}$$\nLet the two points be $x_1 = -\\sigma\\sqrt{2\\ln(2)}$ and $x_2 = +\\sigma\\sqrt{2\\ln(2)}$. The FWHM, denoted by $W$, is the distance between these two points:\n$$W = x_2 - x_1 = \\left(\\sigma\\sqrt{2\\ln(2)}\\right) - \\left(-\\sigma\\sqrt{2\\ln(2)}\\right)$$\n$$W = 2\\sigma\\sqrt{2\\ln(2)}$$\nThis is the analytic relationship between the FWHM ($W$) and the standard deviation ($\\sigma$).\n\n**Part 2: Calculation of $\\sigma$ in physical units (mm)**\n\nWe are given a target FWHM of $W = 6\\,\\mathrm{mm}$. We can rearrange the derived relationship to solve for $\\sigma$:\n$$\\sigma = \\frac{W}{2\\sqrt{2\\ln(2)}}$$\nSince the blurring is isotropic in physical space, this standard deviation, let's call it $\\sigma_{\\mathrm{mm}}$, is the same along all axes. Substituting $W = 6\\,\\mathrm{mm}$:\n$$\\sigma_{\\mathrm{mm}} = \\frac{6}{2\\sqrt{2\\ln(2)}} = \\frac{3}{\\sqrt{2\\ln(2)}}\\,\\mathrm{mm}$$\nTo proceed with numerical calculations, we use the value of $\\ln(2) \\approx 0.69314718$:\n$$\\sigma_{\\mathrm{mm}} = \\frac{3}{\\sqrt{2 \\times 0.69314718}} \\approx \\frac{3}{\\sqrt{1.38629436}} \\approx \\frac{3}{1.17741002}\\,\\mathrm{mm}$$\n$$\\sigma_{\\mathrm{mm}} \\approx 2.5481755\\,\\mathrm{mm}$$\n\n**Part 3: Conversion of $\\sigma_{\\mathrm{mm}}$ to voxel units**\n\nThe problem requires the standard deviation to be expressed in voxels for each axis, using the formula $\\sigma_{i} = \\sigma_{\\mathrm{mm}}/s_{i}$ where $s_i$ is the voxel size along axis $i$.\nThe given voxel sizes are $s_{x} = 0.8\\,\\mathrm{mm}$, $s_{y} = 0.8\\,\\mathrm{mm}$, and $s_{z} = 1.5\\,\\mathrm{mm}$.\n\nFor the x-axis:\n$$\\sigma_{x} = \\frac{\\sigma_{\\mathrm{mm}}}{s_{x}} = \\frac{2.5481755\\,\\mathrm{mm}}{0.8\\,\\mathrm{mm}} \\approx 3.185219\\,\\text{voxels}$$\nRounding to four significant figures, $\\sigma_{x} \\approx 3.185$.\n\nFor the y-axis:\n$$\\sigma_{y} = \\frac{\\sigma_{\\mathrm{mm}}}{s_{y}} = \\frac{2.5481755\\,\\mathrm{mm}}{0.8\\,\\mathrm{mm}} \\approx 3.185219\\,\\text{voxels}$$\nRounding to four significant figures, $\\sigma_{y} \\approx 3.185$.\n\nFor the z-axis:\n$$\\sigma_{z} = \\frac{\\sigma_{\\mathrm{mm}}}{s_{z}} = \\frac{2.5481755\\,\\mathrm{mm}}{1.5\\,\\mathrm{mm}} \\approx 1.698784\\,\\text{voxels}$$\nRounding to four significant figures, $\\sigma_{z} \\approx 1.699$.\n\nThe final answer is the row matrix $(\\sigma_x, \\sigma_y, \\sigma_z)$ with the computed values.", "answer": "$$\\boxed{\\begin{pmatrix} 3.185 & 3.185 & 1.699 \\end{pmatrix}}$$", "id": "4553384"}, {"introduction": "Applying filters to large 3D medical images can be computationally intensive, potentially creating bottlenecks in analysis pipelines. Fortunately, many common filters like the Gaussian are \"separable,\" a property that allows for a massive reduction in computational cost. This exercise will have you derive and quantify this efficiency gain, demonstrating why understanding algorithmic complexity is critical for developing practical and scalable radiomics software. [@problem_id:4553315]", "problem": "A three-dimensional image in radiomics, such as a Computed Tomography (CT) volume, can be denoised by discrete convolution with a finite-support kernel. Let $f[x,y,z]$ be a volumetric image on a regular grid and let $h[i,j,k]$ be a real-valued kernel with cubic support of odd length $K$ along each axis. Consider the operation count for computing each output voxel via direct three-dimensional discrete convolution, assuming a straightforward implementation that, for each kernel tap, performs one multiply and one add, and count each multiply and each add as one floating-point operation (FLOP), for a total of $2$ FLOPs per tap. Next, suppose the kernel is separable, meaning $h[i,j,k] = h_{x}[i]\\,h_{y}[j]\\,h_{z}[k]$, so the three-dimensional convolution can be implemented by three successive one-dimensional convolutions along the $x$, $y$, and $z$ axes, each with the same odd length $K$.\n  \nStarting from the definitions of discrete convolution and separability, derive the per-voxel FLOP counts for both the direct three-dimensional convolution and the three-pass separable implementation, and state the corresponding big-$O$ complexities with respect to $K$. Then, for a typical CT volume of size $512 \\times 512 \\times 128$ voxels and a kernel of odd length $K=7$, assume a sustained processing rate of $150 \\times 10^{9}$ FLOP/s on the processor. Under these assumptions and ignoring boundary effects, compute the total time saved, in seconds, when using the separable implementation instead of the direct three-dimensional convolution for the whole volume. Round your final answer to four significant figures and express the time in seconds.", "solution": "The discrete three-dimensional convolution of an image $f[x,y,z]$ with a kernel $h[i,j,k]$ at a voxel location $(x,y,z)$ is defined by\n$$\n(f * h)[x,y,z] \\;=\\; \\sum_{i=-a}^{a} \\sum_{j=-a}^{a} \\sum_{k=-a}^{a} h[i,j,k]\\, f[x-i, y-j, z-k],\n$$\nwhere $K=2a+1$ is the odd kernel length along each axis and the kernel support has $K$ samples per dimension. For each output voxel, the direct three-dimensional convolution evaluates one product $h[i,j,k]\\,f[\\cdot]$ and then adds it to the running sum for every triplet $(i,j,k)$ in the kernel support. Therefore, the number of multiply-add pairs per voxel is $K^{3}$, and if we count each multiply and each add as one floating-point operation (FLOP), this yields\n$$\n\\text{FLOPs per voxel (direct 3D)} \\;=\\; 2\\,K^{3}.\n$$\nSince the number of operations scales as the cube of $K$, the per-voxel computational complexity is $\\mathcal{O}(K^{3})$.\n\nIf the kernel is separable, so $h[i,j,k] = h_{x}[i]\\,h_{y}[j]\\,h_{z}[k]$, the three-dimensional convolution can be computed as three successive one-dimensional convolutions:\n$$\ng_{x}[x,y,z] \\;=\\; \\sum_{i=-a}^{a} h_{x}[i]\\, f[x-i, y, z], \\quad\ng_{y}[x,y,z] \\;=\\; \\sum_{j=-a}^{a} h_{y}[j]\\, g_{x}[x, y-j, z], \\quad\n(f*h)[x,y,z] \\;=\\; \\sum_{k=-a}^{a} h_{z}[k]\\, g_{y}[x, y, z-k].\n$$\nEach one-dimensional convolution has $K$ taps, so it performs $K$ multiply-add pairs per voxel. There are three passes, one along each axis, which leads to $3K$ multiply-add pairs per voxel, and thus\n$$\n\\text{FLOPs per voxel (separable 3-pass)} \\;=\\; 2 \\times (3K) \\;=\\; 6K.\n$$\nThis scales linearly with $K$, so the per-voxel computational complexity is $\\mathcal{O}(K)$ for each pass and $\\mathcal{O}(3K)$ overall, which is $\\mathcal{O}(K)$.\n\nTo quantify runtime impact, let the volume size be $N_{x} \\times N_{y} \\times N_{z} = 512 \\times 512 \\times 128$, so the number of voxels is\n$$\nN \\;=\\; N_{x} N_{y} N_{z} \\;=\\; 512 \\times 512 \\times 128 \\;=\\; 33{,}554{,}432.\n$$\nWith $K=7$ (so $a=3$), the total FLOPs for the direct three-dimensional convolution over the whole volume are\n$$\n\\text{FLOPs}_{\\text{direct}} \\;=\\; N \\times 2 K^{3} \\;=\\; 33{,}554{,}432 \\times 2 \\times 7^{3}\n\\;=\\; 33{,}554{,}432 \\times 686 \\;=\\; 23{,}018{,}340{,}352.\n$$\nThe total FLOPs for the separable implementation are\n$$\n\\text{FLOPs}_{\\text{sep}} \\;=\\; N \\times 6K \\;=\\; 33{,}554{,}432 \\times 42 \\;=\\; 1{,}409{,}286{,}144.\n$$\nHence, the FLOPs saved by using separability are\n$$\n\\Delta \\text{FLOPs} \\;=\\; \\text{FLOPs}_{\\text{direct}} - \\text{FLOPs}_{\\text{sep}} \\;=\\; 23{,}018{,}340{,}352 \\;-\\; 1{,}409{,}286{,}144 \\;=\\; 21{,}609{,}054{,}208.\n$$\nAssuming a sustained processing rate $R = 150 \\times 10^{9}$ FLOP/s, the time saved is\n$$\n\\Delta t \\;=\\; \\frac{\\Delta \\text{FLOPs}}{R} \\;=\\; \\frac{21{,}609{,}054{,}208}{150 \\times 10^{9}} \\;\\text{s} \\;\\approx\\; 0.144060361386\\ldots \\;\\text{s}.\n$$\nRounded to four significant figures, the time saved is\n$$\n0.1441 \\;\\text{s}.\n$$\nThis result illustrates the reduction in computational complexity from cubic to linear in $K$ and quantifies the runtime impact for a typical volumetric dataset.", "answer": "$$\\boxed{0.1441}$$", "id": "4553315"}, {"introduction": "While standard filters like the Gaussian are widely used, a more powerful approach is to design a filter that is optimally tailored to the specific signal and noise characteristics of your image. The Wiener filter is a classic example of such an adaptive filter, designed to minimize mean squared error by considering the power spectra of the signal and noise. This comprehensive coding exercise will guide you from the theoretical derivation of the Wiener filter to its full implementation and evaluation, providing a capstone experience in statistical image processing. [@problem_id:4553371]", "problem": "You are given a linear imaging model for a two-dimensional computed tomography (CT) slice under additive noise. Let the true image be $X[\\mathbf{r}]$, the noise be $N[\\mathbf{r}]$, and the observed image be $Y[\\mathbf{r}] = X[\\mathbf{r}] + N[\\mathbf{r}]$, where $\\mathbf{r} = (u,v)$ indexes pixel coordinates on a finite grid. Assume $X[\\mathbf{r}]$ and $N[\\mathbf{r}]$ are zero-mean, second-order stationary random fields and are uncorrelated. Let $\\mathcal{F}\\{\\cdot\\}$ denote the two-dimensional Discrete Fourier Transform (DFT), and define the two-dimensional angular frequency as $\\boldsymbol{\\omega} = (\\omega_x,\\omega_y)$. The Power Spectral Density (PSD) $S_{AA}(\\boldsymbol{\\omega})$ of a field $A[\\mathbf{r}]$ is the Fourier transform of its autocorrelation function. Under the independence and stationarity assumptions, the observation PSD satisfies $S_{YY}(\\boldsymbol{\\omega}) = S_{XX}(\\boldsymbol{\\omega}) + S_{NN}(\\boldsymbol{\\omega})$. The goal is to recover a linear, shift-invariant estimator $\\widehat{X}[\\mathbf{r}]$ obtained by filtering $Y[\\mathbf{r}]$ with a filter whose frequency response is $H(\\boldsymbol{\\omega})$, that is, $\\mathcal{F}\\{\\widehat{X}\\}(\\boldsymbol{\\omega}) = H(\\boldsymbol{\\omega}) \\,\\mathcal{F}\\{Y\\}(\\boldsymbol{\\omega})$. You must derive from first principles the optimal $H(\\boldsymbol{\\omega})$ in the sense of minimizing the mean squared error $\\mathbb{E}\\{ \\lvert X[\\mathbf{r}] - \\widehat{X}[\\mathbf{r}] \\rvert^2 \\}$ over all linear shift-invariant estimators.\n\nYou will implement a complete numerical pipeline to:\n- Construct a synthetic CT-like phantom image $X[\\mathbf{r}]$ of size $N \\times N$ with $N = 64$ containing simple geometric structures with piecewise-constant intensities.\n- Generate noisy observations $Y[\\mathbf{r}]$ under three test cases (test suite), with known but unobserved noise realizations $N[\\mathbf{r}]$.\n- Estimate $S_{NN}(\\boldsymbol{\\omega})$ from a background-only patch assumed to contain only noise, and estimate $S_{YY}(\\boldsymbol{\\omega})$ from the full observed image.\n- Infer $S_{XX}(\\boldsymbol{\\omega})$ from $S_{YY}(\\boldsymbol{\\omega})$ and $S_{NN}(\\boldsymbol{\\omega})$ using only the given assumptions.\n- Compute the optimal $H(\\boldsymbol{\\omega})$ you derived, apply it in the frequency domain to obtain $\\widehat{X}[\\mathbf{r}]$, and quantify performance.\n\nFundamental base you may use:\n- Linearity of the Discrete Fourier Transform (DFT) and convolution, and that linear shift-invariant filtering corresponds to multiplication in the frequency domain.\n- The definition of Power Spectral Density (PSD) as the Fourier transform of the autocorrelation function for wide-sense stationary processes.\n- For uncorrelated signal and noise, $S_{YY}(\\boldsymbol{\\omega}) = S_{XX}(\\boldsymbol{\\omega}) + S_{NN}(\\boldsymbol{\\omega})$.\n- Mean squared error optimality can be analyzed per frequency for linear shift-invariant estimators.\n\nNumerical details to implement:\n- Image size is $N \\times N$ with $N = 64$; use a fixed random seed $1337$ for reproducibility.\n- Construct $X[\\mathbf{r}]$ as follows on a zero background:\n  - A disk centered at $(32,32)$ with radius $12$ and intensity $1.00$.\n  - A vertical bar covering all rows and columns $28$ to $31$ inclusive with intensity $0.70$.\n  - A square from rows $12$ to $19$ and columns $44$ to $51$ inclusive with intensity $1.50$.\n- Define a background-only patch as the top-left $16 \\times 16$ block, that is, indices $u \\in [0,15]$, $v \\in [0,15]$, which contains only background in the clean phantom by construction.\n- Use the following test suite of three noise conditions to create $Y[\\mathbf{r}] = X[\\mathbf{r}] + N[\\mathbf{r}]$:\n  1. White Gaussian noise with standard deviation $0.10$.\n  2. Colored Gaussian noise formed by convolving white Gaussian noise (standard deviation $0.20$) with a Gaussian kernel of standard deviation $2.0$ pixels.\n  3. White Gaussian noise with standard deviation $0.02$.\n- Estimate $S_{YY}(\\boldsymbol{\\omega})$ by the periodogram of $Y[\\mathbf{r}]$ over the full image: if $F_Y = \\mathcal{F}\\{Y\\}$ on the $N \\times N$ grid, use $\\widehat{S}_{YY}(\\boldsymbol{\\omega}) = \\lvert F_Y \\rvert^2 / (N^2)$.\n- Estimate $S_{NN}(\\boldsymbol{\\omega})$ by the periodogram of the noise-only patch, zero-padded to size $N \\times N$ before applying the DFT. If the patch has size $M \\times M$ with $M = 16$, and $F_P$ is the $N \\times N$ DFT of the zero-padded patch, use $\\widehat{S}_{NN}(\\boldsymbol{\\omega}) = \\lvert F_P \\rvert^2 / (M^2)$.\n- Infer $\\widehat{S}_{XX}(\\boldsymbol{\\omega})$ from the stationarity and uncorrelatedness assumptions using only the observable quantities, and enforce nonnegativity by clamping any negative values to $0$.\n- Use a small positive regularizer $\\varepsilon = 10^{-8}$ wherever needed to avoid division by zero during implementation.\n- Apply your derived optimal $H(\\boldsymbol{\\omega})$ to $Y[\\mathbf{r}]$ in the frequency domain and invert the DFT to obtain the real-valued estimate $\\widehat{X}[\\mathbf{r}]$.\n- Quantify performance using Peak Signal-to-Noise Ratio (PSNR), defined as $\\mathrm{PSNR} = 10 \\log_{10}\\!\\left(\\dfrac{A^2}{\\mathrm{MSE}}\\right)$ where $A$ is the known peak intensity of $X[\\mathbf{r}]$ and the compared image. Use $A = 1.50$. Report the improvement in PSNR as a float equal to $\\mathrm{PSNR}(\\widehat{X}, X) - \\mathrm{PSNR}(Y, X)$ for each test case.\n\nAngle units do not apply. There are no physical units required. All outputs must be real numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$), where each result is the PSNR improvement for the corresponding test case, rounded to $4$ decimal places.\n\nTest suite summary to implement:\n- Case $1$: white noise, standard deviation $0.10$.\n- Case $2$: colored noise via Gaussian convolution, base white noise standard deviation $0.20$, Gaussian kernel standard deviation $2.0$ pixels.\n- Case $3$: white noise, standard deviation $0.02$.\n\nYour code must be a complete, runnable program that constructs $X[\\mathbf{r}]$, generates $Y[\\mathbf{r}]$ per case, estimates the required spectra from the specified patches, derives and applies the optimal $H(\\boldsymbol{\\omega})$, and outputs the PSNR improvements in the specified single-line format.", "solution": "The problem requires the derivation of an optimal linear shift-invariant filter for image denoising under an additive noise model, followed by a numerical implementation and evaluation on synthetic data. This is a standard problem in signal and image processing, the solution to which is the Wiener filter.\n\n### 1. Theoretical Derivation of the Optimal Filter\n\nWe are given a linear imaging model where the observed image $Y[\\mathbf{r}]$ is the sum of the true image $X[\\mathbf{r}]$ and uncorrelated, zero-mean, second-order stationary additive noise $N[\\mathbf{r}]$:\n$$ Y[\\mathbf{r}] = X[\\mathbf{r}] + N[\\mathbf{r}] $$\nWe seek a linear shift-invariant (LSI) estimator $\\widehat{X}[\\mathbf{r}]$ of $X[\\mathbf{r}]$. An LSI filter is defined by its convolution kernel $h[\\mathbf{r}]$ or, equivalently, its frequency response $H(\\boldsymbol{\\omega})$. The restored image in the frequency domain is given by:\n$$ \\mathcal{F}\\{\\widehat{X}\\}(\\boldsymbol{\\omega}) = H(\\boldsymbol{\\omega}) \\mathcal{F}\\{Y\\}(\\boldsymbol{\\omega}) $$\nThe objective is to find the filter $H(\\boldsymbol{\\omega})$ that minimizes the mean squared error (MSE) between the true and estimated images, defined as $\\mathrm{MSE} = \\mathbb{E}\\{ |X[\\mathbf{r}] - \\widehat{X}[\\mathbf{r}]|^2 \\}$. Due to the stationarity assumption, the MSE is independent of the spatial coordinate $\\mathbf{r}$.\n\nLet $F_X(\\boldsymbol{\\omega})$, $F_Y(\\boldsymbol{\\omega})$, and $F_{\\widehat{X}}(\\boldsymbol{\\omega})$ be the Discrete Fourier Transforms (DFTs) of the respective signals. The error signal in the frequency domain is $F_E(\\boldsymbol{\\omega}) = F_X(\\boldsymbol{\\omega}) - F_{\\widehat{X}}(\\boldsymbol{\\omega})$. Substituting the filter definition, we get:\n$$ F_E(\\boldsymbol{\\omega}) = F_X(\\boldsymbol{\\omega}) - H(\\boldsymbol{\\omega}) F_Y(\\boldsymbol{\\omega}) $$\nThe MSE can be calculated as the sum (integral in the continuous case) of the Power Spectral Density (PSD) of the error signal, $S_{EE}(\\boldsymbol{\\omega})$, over all frequencies. The PSD is proportional to the expected squared magnitude of the DFT coefficients, $S_{EE}(\\boldsymbol{\\omega}) \\propto \\mathbb{E}\\{ |F_E(\\boldsymbol{\\omega})|^2 \\}$. We can minimize the total MSE by minimizing $S_{EE}(\\boldsymbol{\\omega})$ for each frequency $\\boldsymbol{\\omega}$ independently.\n$$ \\mathbb{E}\\{ |F_E(\\boldsymbol{\\omega})|^2 \\} = \\mathbb{E}\\{ |F_X(\\boldsymbol{\\omega}) - H(\\boldsymbol{\\omega}) F_Y(\\boldsymbol{\\omega})|^2 \\} $$\nExpanding the magnitude squared:\n$$ \\mathbb{E}\\{ |F_E|^2 \\} = \\mathbb{E}\\{ (F_X - H F_Y)(F_X^* - H^* F_Y^*) \\} = \\mathbb{E}\\{ |F_X|^2 \\} - H \\mathbb{E}\\{ F_Y F_X^* \\} - H^* \\mathbb{E}\\{ F_Y^* F_X \\} + |H|^2 \\mathbb{E}\\{ |F_Y|^2 \\} $$\nHere, $H$ is a function of $\\boldsymbol{\\omega}$, and $*$ denotes the complex conjugate. The expectation terms relate to the PSDs ($S_{AA}(\\boldsymbol{\\omega}) \\propto \\mathbb{E}\\{|F_A|^2\\}$) and cross-PSDs ($S_{AB}(\\boldsymbol{\\omega}) \\propto \\mathbb{E}\\{F_A F_B^*\\}$). Since $X$ and $N$ are uncorrelated and zero-mean, their cross-PSD $S_{XN}(\\boldsymbol{\\omega})$ is zero. This implies $\\mathbb{E}\\{F_X F_N^*\\} = 0$.\nThe cross-term $\\mathbb{E}\\{ F_Y F_X^* \\}$ becomes:\n$$ \\mathbb{E}\\{ F_Y F_X^* \\} = \\mathbb{E}\\{ (F_X + F_N) F_X^* \\} = \\mathbb{E}\\{ |F_X|^2 \\} + \\mathbb{E}\\{ F_N F_X^* \\} = \\mathbb{E}\\{ |F_X|^2 \\} $$\nThus, the expression for the error power at frequency $\\boldsymbol{\\omega}$ becomes proportional to:\n$$ S_{EE}(\\boldsymbol{\\omega}) = S_{XX}(\\boldsymbol{\\omega}) - H(\\boldsymbol{\\omega}) S_{XX}(\\boldsymbol{\\omega}) - H^*(\\boldsymbol{\\omega}) S_{XX}(\\boldsymbol{\\omega}) + |H(\\boldsymbol{\\omega})|^2 S_{YY}(\\boldsymbol{\\omega}) $$\nTo find the optimal $H(\\boldsymbol{\\omega}) = H_R + iH_I$ that minimizes this real-valued quantity ($S_{XX}$ and $S_{YY}$ are real), we can take partial derivatives with respect to its real and imaginary parts, $H_R$ and $H_I$. Setting $\\frac{\\partial S_{EE}}{\\partial H_I} = 0$ reveals that $H_I=0$, so the optimal filter is purely real. The minimization problem reduces to differentiating with respect to $H(\\boldsymbol{\\omega})$:\n$$ \\frac{\\partial S_{EE}(\\boldsymbol{\\omega})}{\\partial H(\\boldsymbol{\\omega})} = -2 S_{XX}(\\boldsymbol{\\omega}) + 2 H(\\boldsymbol{\\omega}) S_{YY}(\\boldsymbol{\\omega}) = 0 $$\nSolving for $H(\\boldsymbol{\\omega})$ yields the optimal filter transfer function, known as the Wiener filter:\n$$ H_{optimal}(\\boldsymbol{\\omega}) = \\frac{S_{XX}(\\boldsymbol{\\omega})}{S_{YY}(\\boldsymbol{\\omega})} $$\nUsing the given additive property of PSDs for uncorrelated signals, $S_{YY}(\\boldsymbol{\\omega}) = S_{XX}(\\boldsymbol{\\omega}) + S_{NN}(\\boldsymbol{\\omega})$, we can also write this as:\n$$ H_{optimal}(\\boldsymbol{\\omega}) = \\frac{S_{XX}(\\boldsymbol{\\omega})}{S_{XX}(\\boldsymbol{\\omega}) + S_{NN}(\\boldsymbol{\\omega})} $$\nThis filter intelligently attenuates frequencies where the noise power $S_{NN}$ is high relative to the signal power $S_{XX}$.\n\n### 2. Numerical Implementation\nThe implementation follows the problem's detailed instructions. A fixed random seed of $1337$ is used for reproducibility.\n\n**Phantom Construction:** A true image $X[\\mathbf{r}]$ of size $N \\times N$ with $N=64$ is created. It consists of a zero-intensity background upon which three objects are placed in order: a disk (center $(32,32)$, radius $12$, intensity $1.00$), a vertical bar (columns $28$ to $31$, intensity $0.70$), and a square (rows $12-19$, columns $44-51$, intensity $1.50$).\n\n**Noise Generation and Observation:** For each of the three test cases, a noise field $N[\\mathbf{r}]$ is generated and added to $X[\\mathbf{r}]$ to form the observed image $Y[\\mathbf{r}]$. The noise models are:\n1.  White Gaussian noise with standard deviation $\\sigma=0.10$.\n2.  Colored Gaussian noise, created by convolving white Gaussian noise ($\\sigma=0.20$) with a Gaussian kernel of standard deviation $2.0$ pixels.\n3.  White Gaussian noise with standard deviation $\\sigma=0.02$.\n\n**PSD Estimation:** The required PSDs are estimated from the observed image $Y[\\mathbf{r}]$ using periodograms.\n-   The observed image PSD, $S_{YY}(\\boldsymbol{\\omega})$, is estimated using the full $N \\times N$ image $Y[\\mathbf{r}]$:\n    $$ \\widehat{S}_{YY}(\\boldsymbol{\\omega}) = \\frac{|\\mathcal{F}\\{Y[\\mathbf{r}]\\}|^2}{N^2} $$\n-   The noise PSD, $S_{NN}(\\boldsymbol{\\omega})$, is estimated from a $M \\times M$ noise-only patch ($M=16$) from the top-left corner of $Y[\\mathbf{r}]$, where $X[\\mathbf{r}]=0$. This patch is zero-padded to $N \\times N$ before the DFT. The prescribed formula is:\n    $$ \\widehat{S}_{NN}(\\boldsymbol{\\omega}) = \\frac{|\\mathcal{F}\\{\\text{zero-padded patch}\\}|^2}{M^2} $$\n-   The signal PSD, $S_{XX}(\\boldsymbol{\\omega})$, is then inferred from the other two estimates, with a non-negativity constraint:\n    $$ \\widehat{S}_{XX}(\\boldsymbol{\\omega}) = \\max(0, \\widehat{S}_{YY}(\\boldsymbol{\\omega}) - \\widehat{S}_{NN}(\\boldsymbol{\\omega})) $$\n\n**Filtering and Restoration:** The numerical Wiener filter is constructed using the estimated PSDs and a small regularizer $\\varepsilon=10^{-8}$ to prevent division by zero:\n$$ \\widehat{H}(\\boldsymbol{\\omega}) = \\frac{\\widehat{S}_{XX}(\\boldsymbol{\\omega})}{\\widehat{S}_{YY}(\\boldsymbol{\\omega}) + \\varepsilon} $$\nThe filter is applied by multiplying it with the DFT of the observed image, $F_Y(\\boldsymbol{\\omega})$. The estimated image $\\widehat{X}[\\mathbf{r}]$ is then obtained by taking the real part of the inverse DFT of the product:\n$$ \\widehat{X}[\\mathbf{r}] = \\text{real}(\\mathcal{F}^{-1}\\{ \\widehat{H}(\\boldsymbol{\\omega}) F_Y(\\boldsymbol{\\omega}) \\}) $$\n\n**Performance Evaluation:** The performance is measured by the improvement in Peak Signal-to-Noise Ratio (PSNR), where $\\mathrm{PSNR} = 10 \\log_{10}(A^2 / \\mathrm{MSE})$ and the peak signal amplitude is given as $A=1.50$. The final reported value for each case is the difference: $\\mathrm{PSNR}(\\widehat{X}, X) - \\mathrm{PSNR}(Y, X)$. The results are rounded to $4$ decimal places.", "answer": "[7.0279,7.6074,13.8821]", "id": "4553371"}]}