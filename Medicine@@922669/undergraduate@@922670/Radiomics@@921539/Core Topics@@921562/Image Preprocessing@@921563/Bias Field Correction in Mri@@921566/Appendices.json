{"hands_on_practices": [{"introduction": "Many state-of-the-art bias correction algorithms, such as the popular N4ITK method, are based on modeling the smooth, low-frequency bias field using a flexible basis of functions like B-splines. This exercise provides a hands-on look into the core mathematics of this approach [@problem_id:4531113]. By working through a simplified one-dimensional example, you will see how the multiplicative bias model is transformed into an additive one and how linear least-squares is used to estimate the B-spline coefficients that define the bias field.", "problem": "In radiomics, intensity nonuniformity in Magnetic Resonance Imaging (MRI) is commonly modeled as a multiplicative, slowly varying bias field. Consider the multiplicative model $I_{\\text{obs}}(x) = b(x)\\, I_{\\text{true}}(x) + n(x)$, where $I_{\\text{obs}}(x)$ is the observed intensity at spatial location $x$, $I_{\\text{true}}(x)$ is the true underlying tissue intensity, $b(x)$ is a smooth multiplicative bias field, and $n(x)$ is noise that we will neglect in this problem. Within a homogeneous white matter region of interest used for radiomics intensity standardization, it is commonly assumed that $I_{\\text{true}}(x) \\approx I_{\\text{ref}}$ is spatially constant. Taking the natural logarithm yields an additive model for the log-bias: $y(x) \\equiv \\ln I_{\\text{obs}}(x) - \\ln I_{\\text{ref}} \\approx \\ln b(x)$. The Nonparametric Nonuniform intensity Normalization with the Insight Toolkit (N4ITK) approach represents $\\ln b(x)$ with a low-frequency B-spline basis and estimates its coefficients by minimizing a suitable data fidelity objective over a mask.\n\nConsider a one-dimensional profile along a single imaging axis with positions $x \\in \\{0, 1, 2, 3\\}$ (in arbitrary spatial units) within a white matter region where $I_{\\text{ref}} = 100$. Assume the observed intensities are $I_{\\text{obs}}(0) = 100$, $I_{\\text{obs}}(1) = 100\\,\\exp(0.02)$, $I_{\\text{obs}}(2) = 100\\,\\exp(0.04)$, and $I_{\\text{obs}}(3) = 100\\,\\exp(0.06)$. In a single coarse-grid step of N4ITK, approximate $\\ln b(x)$ over $[0,3]$ by a linear B-spline expansion with two control points at $x=0$ and $x=3$, with basis functions\n- $B_{1}(x) = 1 - \\frac{x}{3}$ on $[0,3]$ and $B_{1}(x) = 0$ otherwise,\n- $B_{2}(x) = \\frac{x}{3}$ on $[0,3]$ and $B_{2}(x) = 0$ otherwise.\nAssume unit weights for all four samples in the fidelity term.\n\nStarting from the multiplicative model and its log transformation, set up the least-squares estimation of the B-spline coefficients and carry the derivation through to compute the predicted log-bias at $x_{0} = 1.5$. Then, return to the multiplicative scale to obtain the estimated bias value $\\hat{b}(x_{0})$ that this coarse N4ITK step would produce.\n\nWhat is the value of $\\hat{b}(1.5)$? Give your final answer as an exact expression in terms of the exponential function. Report the result as a dimensionless quantity; do not round.", "solution": "The user wants to determine the estimated bias field value, $\\hat{b}(x_0)$, at a specific point $x_0 = 1.5$ using a simplified one-dimensional model of the N4ITK bias field correction algorithm.\n\nThe problem starts with the multiplicative model for observed MRI intensity:\n$$I_{\\text{obs}}(x) = b(x)\\, I_{\\text{true}}(x) + n(x)$$\nNeglecting the noise term $n(x)$ and assuming a constant true intensity $I_{\\text{true}}(x) \\approx I_{\\text{ref}}$ within a homogeneous region, the model simplifies to:\n$$I_{\\text{obs}}(x) \\approx b(x)\\, I_{\\text{ref}}$$\nTaking the natural logarithm of this equation gives an additive model for the log-bias, $\\ln b(x)$:\n$$\\ln(I_{\\text{obs}}(x)) \\approx \\ln(b(x)) + \\ln(I_{\\text{ref}})$$\nWe define the quantity to be fitted, $y(x)$, as the observed log-bias:\n$$y(x) \\equiv \\ln(I_{\\text{obs}}(x)) - \\ln(I_{\\text{ref}}) \\approx \\ln(b(x))$$\nThe problem provides the following data:\n- Sample points: $x \\in \\{0, 1, 2, 3\\}$.\n- Reference intensity: $I_{\\text{ref}} = 100$.\n- Observed intensities:\n  - $I_{\\text{obs}}(0) = 100$\n  - $I_{\\text{obs}}(1) = 100\\,\\exp(0.02)$\n  - $I_{\\text{obs}}(2) = 100\\,\\exp(0.04)$\n  - $I_{\\text{obs}}(3) = 100\\,\\exp(0.06)$\n\nFirst, we calculate the observed log-bias values, $y(x)$, at each sample point:\n- $y(0) = \\ln(I_{\\text{obs}}(0)) - \\ln(100) = \\ln(100) - \\ln(100) = 0$.\n- $y(1) = \\ln(I_{\\text{obs}}(1)) - \\ln(100) = \\ln(100\\,\\exp(0.02)) - \\ln(100) = \\ln(100) + 0.02 - \\ln(100) = 0.02$.\n- $y(2) = \\ln(I_{\\text{obs}}(2)) - \\ln(100) = \\ln(100\\,\\exp(0.04)) - \\ln(100) = \\ln(100) + 0.04 - \\ln(100) = 0.04$.\n- $y(3) = \\ln(I_{\\text{obs}}(3)) - \\ln(100) = \\ln(100\\,\\exp(0.06)) - \\ln(100) = \\ln(100) + 0.06 - \\ln(100) = 0.06$.\n\nThe log-bias field, $\\ln b(x)$, is approximated by a linear combination of two basis functions, $B_1(x) = 1 - \\frac{x}{3}$ and $B_2(x) = \\frac{x}{3}$:\n$$\\ln \\hat{b}(x) = c_1 B_1(x) + c_2 B_2(x)$$\nWe need to find the coefficients $c_1$ and $c_2$ that minimize the sum of squared differences between the model $\\ln \\hat{b}(x_i)$ and the observed data $y(x_i)$. The objective function is:\n$$J(c_1, c_2) = \\sum_{i=0}^{3} (y(x_i) - (c_1 B_1(x_i) + c_2 B_2(x_i)))^2$$\nThis is a linear least-squares problem that can be written in matrix form as $\\mathbf{A}\\mathbf{c} \\approx \\mathbf{y}$. The solution $\\hat{\\mathbf{c}}$ is found by solving the normal equations: $(\\mathbf{A}^T \\mathbf{A}) \\hat{\\mathbf{c}} = \\mathbf{A}^T \\mathbf{y}$.\n\nThe vector of observations $\\mathbf{y}$ is:\n$$\\mathbf{y} = \\begin{pmatrix} 0 \\\\ 0.02 \\\\ 0.04 \\\\ 0.06 \\end{pmatrix}$$\nThe design matrix $\\mathbf{A}$ is constructed by evaluating the basis functions at each sample point $x_i$:\n- $B_1(0) = 1$, $B_2(0) = 0$\n- $B_1(1) = 1 - 1/3 = 2/3$, $B_2(1) = 1/3$\n- $B_1(2) = 1 - 2/3 = 1/3$, $B_2(2) = 2/3$\n- $B_1(3) = 1 - 3/3 = 0$, $B_2(3) = 3/3 = 1$\n$$\\mathbf{A} = \\begin{pmatrix} B_1(0) & B_2(0) \\\\ B_1(1) & B_2(1) \\\\ B_1(2) & B_2(2) \\\\ B_1(3) & B_2(3) \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 2/3 & 1/3 \\\\ 1/3 & 2/3 \\\\ 0 & 1 \\end{pmatrix}$$\nThe coefficient vector to be determined is $\\mathbf{c} = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$.\n\nNext, we compute $\\mathbf{A}^T \\mathbf{A}$ and $\\mathbf{A}^T \\mathbf{y}$:\n$$\\mathbf{A}^T \\mathbf{A} = \\begin{pmatrix} 1 & 2/3 & 1/3 & 0 \\\\ 0 & 1/3 & 2/3 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 2/3 & 1/3 \\\\ 1/3 & 2/3 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1+\\frac{4}{9}+\\frac{1}{9} & \\frac{2}{9}+\\frac{2}{9} \\\\ \\frac{2}{9}+\\frac{2}{9} & \\frac{1}{9}+\\frac{4}{9}+1 \\end{pmatrix} = \\begin{pmatrix} \\frac{14}{9} & \\frac{4}{9} \\\\ \\frac{4}{9} & \\frac{14}{9} \\end{pmatrix}$$\n$$\\mathbf{A}^T \\mathbf{y} = \\begin{pmatrix} 1 & 2/3 & 1/3 & 0 \\\\ 0 & 1/3 & 2/3 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0.02 \\\\ 0.04 \\\\ 0.06 \\end{pmatrix} = \\begin{pmatrix} 0 \\cdot 1 + (2/3)(0.02) + (1/3)(0.04) + 0 \\cdot 0.06 \\\\ 0 \\cdot 0 + (1/3)(0.02) + (2/3)(0.04) + 1 \\cdot 0.06 \\end{pmatrix} = \\begin{pmatrix} \\frac{0.04}{3} + \\frac{0.04}{3} \\\\ \\frac{0.02}{3} + \\frac{0.08}{3} + \\frac{0.18}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{0.08}{3} \\\\ \\frac{0.28}{3} \\end{pmatrix}$$\nNow we solve the system $(\\mathbf{A}^T \\mathbf{A}) \\mathbf{c} = \\mathbf{A}^T \\mathbf{y}$. We find the inverse of $\\mathbf{A}^T \\mathbf{A}$:\n$$\\det(\\mathbf{A}^T \\mathbf{A}) = \\left(\\frac{14}{9}\\right)^2 - \\left(\\frac{4}{9}\\right)^2 = \\frac{196 - 16}{81} = \\frac{180}{81}$$\n$$(\\mathbf{A}^T \\mathbf{A})^{-1} = \\frac{81}{180} \\begin{pmatrix} \\frac{14}{9} & -\\frac{4}{9} \\\\ -\\frac{4}{9} & \\frac{14}{9} \\end{pmatrix} = \\frac{9}{20} \\begin{pmatrix} \\frac{14}{9} & -\\frac{4}{9} \\\\ -\\frac{4}{9} & \\frac{14}{9} \\end{pmatrix} = \\frac{1}{20} \\begin{pmatrix} 14 & -4 \\\\ -4 & 14 \\end{pmatrix}$$\nThe coefficient vector is $\\mathbf{c} = (\\mathbf{A}^T \\mathbf{A})^{-1} (\\mathbf{A}^T \\mathbf{y})$:\n$$\\mathbf{c} = \\frac{1}{20} \\begin{pmatrix} 14 & -4 \\\\ -4 & 14 \\end{pmatrix} \\begin{pmatrix} \\frac{0.08}{3} \\\\ \\frac{0.28}{3} \\end{pmatrix} = \\frac{1}{60} \\begin{pmatrix} 14(0.08) - 4(0.28) \\\\ -4(0.08) + 14(0.28) \\end{pmatrix} = \\frac{1}{60} \\begin{pmatrix} 1.12 - 1.12 \\\\ -0.32 + 3.92 \\end{pmatrix} = \\frac{1}{60} \\begin{pmatrix} 0 \\\\ 3.6 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0.06 \\end{pmatrix}$$\nThus, we find the coefficients to be $c_1 = 0$ and $c_2 = 0.06$.\n\nThe estimated log-bias field is:\n$$\\ln \\hat{b}(x) = 0 \\cdot \\left(1 - \\frac{x}{3}\\right) + 0.06 \\cdot \\left(\\frac{x}{3}\\right) = 0.02x$$\nNote that the observed log-bias data $y(x_i)$ perfectly lie on the line $y=0.02x$. Since this function is in the space spanned by the basis functions, the least-squares fit is exact, and the sum of squared errors is zero.\n\nThe problem asks for the estimated bias value $\\hat{b}(x_0)$ at $x_0 = 1.5$. First, we evaluate the estimated log-bias at this point:\n$$\\ln \\hat{b}(1.5) = 0.02 \\times 1.5 = 0.03$$\nFinally, we exponentiate this result to find the bias field value on the multiplicative scale:\n$$\\hat{b}(1.5) = \\exp(\\ln \\hat{b}(1.5)) = \\exp(0.03)$$\nThis is a dimensionless quantity, as required.", "answer": "$$\\boxed{\\exp(0.03)}$$", "id": "4531113"}, {"introduction": "Bias field correction is not always a standalone preprocessing step; it is often tightly coupled with other image analysis tasks like tissue segmentation. This practice explores this advanced concept through the powerful framework of the Expectation-Maximization (EM) algorithm [@problem_id:4531103]. You will derive the update rule for a bias field parameter within a generative model of different tissue classes, gaining a deeper appreciation for how statistical models can jointly infer anatomical labels and correct for image artifacts.", "problem": "In Magnetic Resonance Imaging (MRI) radiomics, low-frequency intensity nonuniformity (the bias field) distorts voxel intensities and can confound downstream feature extraction. In a small homogeneous region, it is often reasonable to approximate a smooth multiplicative bias field by a single constant factor. Consider a simplified generative model for a small two-tissue patch, where the observed intensity at voxel $i$, denoted $y_{i}$, arises from a latent tissue class $z_{i} \\in \\{1,2\\}$ with class-specific mean $\\mu_{k}$ and a global, unknown multiplicative bias parameter $b>0$, corrupted by zero-mean Gaussian noise of known variance $\\sigma^{2}$. The conditional distribution is\n$$\np(y_{i} \\mid z_{i}=k, b) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{(y_{i} - b\\,\\mu_{k})^{2}}{2\\sigma^{2}}\\right).\n$$\nAssume a mixture prior $p(z_{i}=k) = \\pi_{k}$ with $\\sum_{k=1}^{2}\\pi_{k}=1$. You will perform one iteration of the Expectation–Maximization (EM) algorithm to jointly infer soft segmentation (posterior class memberships) and update the bias parameter $b$.\n\nTasks:\n1) Starting from the maximum likelihood principle for latent-variable models and the definition of the complete-data log-likelihood, derive from first principles the Maximization-step update for the global multiplicative bias parameter $b$ in closed form, given fixed class means $\\mu_{1}, \\mu_{2}$, known variance $\\sigma^{2}$, and mixture weights $\\pi_{1}, \\pi_{2}$. Your derivation must proceed by constructing the expected complete-data log-likelihood under the current posterior responsibilities and then optimizing with respect to $b$.\n\n2) Consider $N=4$ voxels with observed intensities $y_{1}=82$, $y_{2}=120$, $y_{3}=78$, $y_{4}=130$. Let the class means be $\\mu_{1}=110$ and $\\mu_{2}=80$, the standard deviation be $\\sigma=10$, and the mixture weights be $\\pi_{1}=\\pi_{2}=\\tfrac{1}{2}$. Initialize the EM algorithm with $b^{(0)}=1$. Perform the Expectation-step to compute the posterior responsibilities\n$$\n\\gamma_{ik} \\equiv p(z_{i}=k \\mid y_{i}, b^{(0)}, \\mu_{1}, \\mu_{2}, \\sigma^{2}, \\pi_{1}, \\pi_{2}),\n$$\nand then perform the Maximization-step using your derived expression to compute the updated bias estimate $b^{(1)}$. Report the numerical value of $b^{(1)}$ rounded to five significant figures. Express the final answer as a pure number with no units.", "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n- **Generative Model**: The conditional distribution of an observed voxel intensity $y_i$ given latent class $z_i=k$ and bias parameter $b$ is a Gaussian distribution:\n$$\np(y_{i} \\mid z_{i}=k, b) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{(y_{i} - b\\,\\mu_{k})^{2}}{2\\sigma^{2}}\\right)\n$$\n- **Latent Variable**: $z_i \\in \\{1,2\\}$ is the tissue class for voxel $i$.\n- **Model Parameters**:\n    - $\\mu_k$: Class-specific mean intensity for class $k$.\n    - $b$: A global, unknown multiplicative bias parameter, $b>0$.\n    - $\\sigma^2$: Known variance of zero-mean Gaussian noise.\n- **Prior Distribution**: A mixture prior $p(z_i = k) = \\pi_k$, with $\\sum_{k=1}^{2} \\pi_k = 1$.\n- **Task 1**: Derive the Maximization-step update for $b$ from first principles.\n- **Task 2**: Perform one iteration of the Expectation-Maximization (EM) algorithm.\n- **Numerical Data for Task 2**:\n    - Number of voxels, $N=4$.\n    - Observed intensities: $y_1=82$, $y_2=120$, $y_3=78$, $y_4=130$.\n    - Class means: $\\mu_1=110$, $\\mu_2=80$.\n    - Standard deviation: $\\sigma=10$, which implies variance $\\sigma^2=100$.\n    - Mixture weights: $\\pi_1 = \\pi_2 = \\frac{1}{2}$.\n    - Initial bias estimate: $b^{(0)}=1$.\n- **Definition of Responsibilities**:\n$$\n\\gamma_{ik} \\equiv p(z_{i}=k \\mid y_{i}, b^{(0)}, \\mu_{1}, \\mu_{2}, \\sigma^{2}, \\pi_{1}, \\pi_{2})\n$$\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem describes a simplified but standard generative model for MRI intensities, incorporating a multiplicative bias field and tissue-specific Gaussian statistics. This is a common approach in medical image analysis. The application of the EM algorithm to find the maximum likelihood estimate of parameters in this latent variable model is a canonical and well-established statistical method. The problem is firmly grounded in statistical modeling and machine learning theory as applied to medical imaging.\n2.  **Well-Posed**: The problem is well-posed. Task 1 asks for a standard derivation which leads to a unique closed-form update equation. Task 2 provides a complete set of parameters and an initial condition, ensuring that the first iteration of the EM algorithm can be executed to yield a unique numerical result for the updated bias parameter $b^{(1)}$.\n3.  **Objective**: The problem is stated in precise mathematical language, free of ambiguity or subjective claims.\n4.  **Completeness and Consistency**: The problem provides all necessary definitions, data, and initial conditions required to perform both the derivation and the numerical calculation. There are no contradictions in the provided information.\n5.  **No other flaws**: The problem is a standard, non-trivial exercise in applying the EM algorithm. It is not ill-posed, unrealistic for a theoretical problem, or tautological.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n---\n\n## Solution\n\n### Task 1: Derivation of the M-step Update for the Bias Parameter $b$\n\nThe Expectation-Maximization (EM) algorithm is an iterative method for finding maximum likelihood estimates of parameters in statistical models with latent variables. The algorithm alternates between an Expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a Maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E-step.\n\nLet the observed data be $Y = \\{y_1, ..., y_N\\}$ and the latent data be $Z = \\{z_1, ..., z_N\\}$. We introduce indicator variables $z_{ik}$ such that $z_{ik}=1$ if voxel $i$ belongs to class $k$, and $z_{ik}=0$ otherwise. The complete-data set is $(Y, Z)$. The parameters to be estimated are denoted by $\\theta$. In this problem, the only parameter we are updating is $b$, while $\\mu_k$, $\\sigma^2$, and $\\pi_k$ are considered fixed.\n\nThe complete-data likelihood is given by:\n$$L_c(\\theta; Y, Z) = p(Y, Z | \\theta) = \\prod_{i=1}^{N} p(y_i, z_i | \\theta) = \\prod_{i=1}^{N} \\prod_{k=1}^{2} [p(y_i|z_i=k, \\theta) p(z_i=k | \\theta)]^{z_{ik}}$$\nThe complete-data log-likelihood is:\n$$\\mathcal{L}_c(\\theta; Y, Z) = \\log L_c(\\theta; Y, Z) = \\sum_{i=1}^{N} \\sum_{k=1}^{2} z_{ik} \\left[ \\log p(y_i|z_i=k, \\theta) + \\log p(z_i=k | \\theta) \\right]$$\nSubstituting the given distributions, with $\\theta$ implicitly containing $b$:\n$$p(y_i|z_i=k, b) = \\mathcal{N}(y_i; b\\mu_k, \\sigma^2)$$\n$$p(z_i=k) = \\pi_k$$\nThe complete-data log-likelihood becomes:\n$$\\mathcal{L}_c(b; Y, Z) = \\sum_{i=1}^{N} \\sum_{k=1}^{2} z_{ik} \\left[ \\log\\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y_i - b\\mu_k)^2}{2\\sigma^2}\\right)\\right) + \\log \\pi_k \\right]$$\n$$\\mathcal{L}_c(b; Y, Z) = \\sum_{i=1}^{N} \\sum_{k=1}^{2} z_{ik} \\left[ -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(y_i - b\\mu_k)^2}{2\\sigma^2} + \\log \\pi_k \\right]$$\n\n**E-step**: We compute the expectation of the complete-data log-likelihood with respect to the posterior distribution of the latent variables $Z$, given the observed data $Y$ and the current parameter estimate $b^{(t)}$. This function is denoted $Q(b | b^{(t)})$.\n$$Q(b | b^{(t)}) = E_{Z|Y, b^{(t)}}[\\mathcal{L}_c(b; Y, Z)]$$\nThe expectation of the indicator variable $z_{ik}$ is the posterior probability, or responsibility, $\\gamma_{ik}^{(t)}$:\n$$E[z_{ik}] = p(z_i=k | y_i, b^{(t)}) = \\gamma_{ik}^{(t)}$$\nSo the $Q$ function is:\n$$Q(b | b^{(t)}) = \\sum_{i=1}^{N} \\sum_{k=1}^{2} \\gamma_{ik}^{(t)} \\left[ -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(y_i - b\\mu_k)^2}{2\\sigma^2} + \\log \\pi_k \\right]$$\n\n**M-step**: We find the parameter $b$ that maximizes the $Q$ function. This gives the updated estimate $b^{(t+1)}$.\n$$b^{(t+1)} = \\arg\\max_{b} Q(b | b^{(t)})$$\nTo maximize $Q(b | b^{(t)})$ with respect to $b$, we only need to consider the terms that depend on $b$. This is equivalent to minimizing the following expression:\n$$J(b) = \\sum_{i=1}^{N} \\sum_{k=1}^{2} \\gamma_{ik}^{(t)} (y_i - b\\mu_k)^2$$\nWe take the derivative of $J(b)$ with respect to $b$ and set it to zero:\n$$\\frac{\\partial J(b)}{\\partial b} = \\frac{\\partial}{\\partial b} \\sum_{i=1}^{N} \\sum_{k=1}^{2} \\gamma_{ik}^{(t)} (y_i^2 - 2by_i\\mu_k + b^2\\mu_k^2) = 0$$\n$$\\sum_{i=1}^{N} \\sum_{k=1}^{2} \\gamma_{ik}^{(t)} (-2y_i\\mu_k + 2b\\mu_k^2) = 0$$\n$$-2\\sum_{i=1}^{N} \\sum_{k=1}^{2} \\gamma_{ik}^{(t)} y_i\\mu_k + 2b\\sum_{i=1}^{N} \\sum_{k=1}^{2} \\gamma_{ik}^{(t)} \\mu_k^2 = 0$$\nSolving for $b$:\n$$b \\sum_{i=1}^{N} \\sum_{k=1}^{2} \\gamma_{ik}^{(t)} \\mu_k^2 = \\sum_{i=1}^{N} \\sum_{k=1}^{2} \\gamma_{ik}^{(t)} y_i\\mu_k$$\nThis yields the M-step update equation for $b^{(t+1)}$:\n$$b^{(t+1)} = \\frac{\\sum_{i=1}^{N} \\sum_{k=1}^{2} \\gamma_{ik}^{(t)} y_i \\mu_k}{\\sum_{i=1}^{N} \\sum_{k=1}^{2} \\gamma_{ik}^{(t)} \\mu_k^2}$$\n\n### Task 2: Numerical Calculation\n\nWe are given $N=4$ voxels with intensities $y = \\{82, 120, 78, 130\\}$, class means $\\mu_1=110$ and $\\mu_2=80$, standard deviation $\\sigma=10$ ($\\sigma^2=100$), and mixture weights $\\pi_1=\\pi_2=\\frac{1}{2}$. The initial bias estimate is $b^{(0)}=1$.\n\n**E-step at iteration $t=0$**: We compute the responsibilities $\\gamma_{ik}^{(0)}$ using $b^{(0)}=1$.\nThe responsibility $\\gamma_{ik}^{(0)}$ is given by Bayes' theorem:\n$$\\gamma_{ik}^{(0)} = p(z_i=k | y_i, b^{(0)}) = \\frac{p(y_i | z_i=k, b^{(0)}) p(z_i=k)}{\\sum_{j=1}^{2} p(y_i | z_i=j, b^{(0)}) p(z_i=j)}$$\nSince $\\pi_1 = \\pi_2$, the priors cancel out. Let $\\mathcal{L}_{ik} = p(y_i|z_i=k, b^{(0)})$.\nThe term $\\frac{1}{\\sqrt{2\\pi}\\sigma}$ is common to all $\\mathcal{L}_{ik}$ for a given $i$ and also cancels. We only need the exponential part for the responsibilities:\n$$\\gamma_{i1}^{(0)} = \\frac{\\exp\\left(-\\frac{(y_i - b^{(0)}\\mu_1)^2}{2\\sigma^2}\\right)}{\\exp\\left(-\\frac{(y_i - b^{(0)}\\mu_1)^2}{2\\sigma^2}\\right) + \\exp\\left(-\\frac{(y_i - b^{(0)}\\mu_2)^2}{2\\sigma^2}\\right)}$$\nand $\\gamma_{i2}^{(0)} = 1 - \\gamma_{i1}^{(0)}$. With $b^{(0)}=1$, $\\mu_1=110$, $\\mu_2=80$, $\\sigma^2=100$:\n\n- For $i=1$, $y_1=82$:\n  - $(y_1 - b^{(0)}\\mu_1)^2 = (82 - 110)^2 = (-28)^2 = 784$.\n  - $(y_1 - b^{(0)}\\mu_2)^2 = (82 - 80)^2 = (2)^2 = 4$.\n  - $\\gamma_{11}^{(0)} = \\frac{\\exp(-784/200)}{\\exp(-784/200) + \\exp(-4/200)} = \\frac{\\exp(-3.92)}{\\exp(-3.92) + \\exp(-0.02)} \\approx 0.019839$.\n  - $\\gamma_{12}^{(0)} = 1 - \\gamma_{11}^{(0)} \\approx 0.980161$.\n\n- For $i=2$, $y_2=120$:\n  - $(y_2 - b^{(0)}\\mu_1)^2 = (120 - 110)^2 = (10)^2 = 100$.\n  - $(y_2 - b^{(0)}\\mu_2)^2 = (120 - 80)^2 = (40)^2 = 1600$.\n  - $\\gamma_{21}^{(0)} = \\frac{\\exp(-100/200)}{\\exp(-100/200) + \\exp(-1600/200)} = \\frac{\\exp(-0.5)}{\\exp(-0.5) + \\exp(-8)} \\approx 0.999448$.\n  - $\\gamma_{22}^{(0)} = 1 - \\gamma_{21}^{(0)} \\approx 0.000552$.\n\n- For $i=3$, $y_3=78$:\n  - $(y_3 - b^{(0)}\\mu_1)^2 = (78 - 110)^2 = (-32)^2 = 1024$.\n  - $(y_3 - b^{(0)}\\mu_2)^2 = (78 - 80)^2 = (-2)^2 = 4$.\n  - $\\gamma_{31}^{(0)} = \\frac{\\exp(-1024/200)}{\\exp(-1024/200) + \\exp(-4/200)} = \\frac{\\exp(-5.12)}{\\exp(-5.12) + \\exp(-0.02)} \\approx 0.006059$.\n  - $\\gamma_{32}^{(0)} = 1 - \\gamma_{31}^{(0)} \\approx 0.993941$.\n\n- For $i=4$, $y_4=130$:\n  - $(y_4 - b^{(0)}\\mu_1)^2 = (130 - 110)^2 = (20)^2 = 400$.\n  - $(y_4 - b^{(0)}\\mu_2)^2 = (130 - 80)^2 = (50)^2 = 2500$.\n  - $\\gamma_{41}^{(0)} = \\frac{\\exp(-400/200)}{\\exp(-400/200) + \\exp(-2500/200)} = \\frac{\\exp(-2)}{\\exp(-2) + \\exp(-12.5)} \\approx 0.999973$.\n  - $\\gamma_{42}^{(0)} = 1 - \\gamma_{41}^{(0)} \\approx 0.000027$.\n\n**M-step**: We compute the updated bias $b^{(1)}$ using the derived formula and the calculated responsibilities.\n$$b^{(1)} = \\frac{\\sum_{i=1}^{4} (\\gamma_{i1}^{(0)} y_i \\mu_1 + \\gamma_{i2}^{(0)} y_i \\mu_2)}{\\sum_{i=1}^{4} (\\gamma_{i1}^{(0)} \\mu_1^2 + \\gamma_{i2}^{(0)} \\mu_2^2)}$$\nFirst, calculate the numerator:\nNumerator = $(\\gamma_{11}^{(0)} y_1 \\mu_1 + \\gamma_{12}^{(0)} y_1 \\mu_2) + (\\gamma_{21}^{(0)} y_2 \\mu_1 + \\gamma_{22}^{(0)} y_2 \\mu_2) + \\dots$\n- $i=1$: $(0.019839 \\times 82 \\times 110) + (0.980161 \\times 82 \\times 80) \\approx 179.0 + 6429.9 \\approx 6608.9$\n- $i=2$: $(0.999448 \\times 120 \\times 110) + (0.000552 \\times 120 \\times 80) \\approx 13192.7 + 5.3 \\approx 13198.0$\n- $i=3$: $(0.006059 \\times 78 \\times 110) + (0.993941 \\times 78 \\times 80) \\approx 52.0 + 6202.4 \\approx 6254.4$\n- $i=4$: $(0.999973 \\times 130 \\times 110) + (0.000027 \\times 130 \\times 80) \\approx 14299.6 + 0.3 \\approx 14299.9$\nSum of Numerator $\\approx 6608.9 + 13198.0 + 6254.4 + 14299.9 = 40361.2$\n\nNext, calculate the denominator with $\\mu_1^2=12100$ and $\\mu_2^2=6400$:\nDenominator = $(\\gamma_{11}^{(0)} \\mu_1^2 + \\gamma_{12}^{(0)} \\mu_2^2) + (\\gamma_{21}^{(0)} \\mu_1^2 + \\gamma_{22}^{(0)} \\mu_2^2) + \\dots$\n- $i=1$: $(0.019839 \\times 12100) + (0.980161 \\times 6400) \\approx 240.1 + 6273.0 \\approx 6513.1$\n- $i=2$: $(0.999448 \\times 12100) + (0.000552 \\times 6400) \\approx 12093.3 + 3.5 \\approx 12096.8$\n- $i=3$: $(0.006059 \\times 12100) + (0.993941 \\times 6400) \\approx 73.3 + 6361.2 \\approx 6434.5$\n- $i=4$: $(0.999973 \\times 12100) + (0.000027 \\times 6400) \\approx 12099.7 + 0.2 \\approx 12099.9$\nSum of Denominator $\\approx 6513.1 + 12096.8 + 6434.5 + 12099.9 = 37144.3$\n\nFinally, we compute $b^{(1)}$:\n$$b^{(1)} = \\frac{40361.2}{37144.3} \\approx 1.086598$$\nRounding to five significant figures, the updated bias estimate is $1.0866$.", "answer": "$$\\boxed{1.0866}$$", "id": "4531103"}, {"introduction": "After applying a bias field correction algorithm, how can we be sure it was beneficial for our ultimate goal in radiomics—to extract stable and reproducible features? The answer lies in rigorous statistical testing. This final practice guides you through the process of quantifying the impact of correction on feature reproducibility using a test-retest dataset [@problem_id:4531093]. By implementing a paired t-test, you will learn the formal statistical procedure for validating whether an image processing step leads to a significant improvement in downstream quantitative analysis.", "problem": "Consider Magnetic Resonance Imaging (MRI), where the observed intensity at spatial location $\\mathbf{x}$ in scan $j$ can be expressed as $I_{j}(\\mathbf{x}) = B_{j}(\\mathbf{x})\\,S(\\mathbf{x}) + \\varepsilon_{j}(\\mathbf{x})$, with $B_{j}(\\mathbf{x})$ representing a smooth multiplicative bias field, $S(\\mathbf{x})$ the true underlying signal, and $\\varepsilon_{j}(\\mathbf{x})$ stochastic noise. In radiomics, a scalar feature $F$ is a deterministic functional of the image intensity distribution; for a fixed subject $i$ and scan $r \\in \\{1,2\\}$, denote the feature by $F_{i,r}^{\\text{pre}}$ before bias field correction and $F_{i,r}^{\\text{post}}$ after bias field correction. A correction method aims to reduce spurious variability introduced by $B_{j}(\\mathbf{x})$ so that $F$ is more reproducible across repeated scans.\n\nAssume a test–retest acquisition for each subject $i$, yielding two feature measurements pre-correction, $F_{i,1}^{\\text{pre}}$ and $F_{i,2}^{\\text{pre}}$, and two measurements post-correction, $F_{i,1}^{\\text{post}}$ and $F_{i,2}^{\\text{post}}$. Define the absolute test–retest error per subject by $e_{i}^{\\text{pre}} = \\lvert F_{i,1}^{\\text{pre}} - F_{i,2}^{\\text{pre}} \\rvert$ and $e_{i}^{\\text{post}} = \\lvert F_{i,1}^{\\text{post}} - F_{i,2}^{\\text{post}} \\rvert$. Let the paired difference be $d_{i} = e_{i}^{\\text{pre}} - e_{i}^{\\text{post}}$. The hypothesis of interest is the one-sided test $H_{0}: \\mu_{d} = 0$ versus $H_{1}: \\mu_{d} > 0$, where $\\mu_{d}$ is the population mean of $d_{i}$. Under the assumptions that subjects are independent and the sampling distribution of the standardized mean difference is approximately described by the Student’s $t$-distribution, compute the one-sided p-value for the observed paired differences and decide whether the improvement in reproducibility after bias field correction is statistically significant at significance level $\\alpha = 0.05$.\n\nYour program must:\n- For each test case, compute $e_{i}^{\\text{pre}}$, $e_{i}^{\\text{post}}$, the paired differences $d_{i}$, the one-sided p-value for $H_{1}: \\mu_{d} > 0$, and output a decision integer where $1$ indicates rejecting $H_{0}$ at $\\alpha = 0.05$ and $0$ indicates failing to reject $H_{0}$.\n- If the sample standard deviation of $d_{i}$ is zero or the sample size is less than $2$, define the p-value to be $1.0$ and the decision to be $0$.\n- Round each p-value to six decimal places.\n\nTest suite (each case lists pairs per subject; pairs are the two measurements for a single radiomics feature, pre-correction and post-correction):\n\n- Case $1$ (general improvement, $10$ subjects):\n  Subject $1$: pre $(100, 110)$, post $(100, 102)$.\n  Subject $2$: pre $(95, 85)$, post $(95, 96)$.\n  Subject $3$: pre $(120, 130)$, post $(120, 119)$.\n  Subject $4$: pre $(80, 92)$, post $(80, 81)$.\n  Subject $5$: pre $(60, 72)$, post $(60, 63)$.\n  Subject $6$: pre $(105, 99)$, post $(105, 104)$.\n  Subject $7$: pre $(140, 150)$, post $(140, 142)$.\n  Subject $8$: pre $(77, 88)$, post $(77, 78)$.\n  Subject $9$: pre $(90, 104)$, post $(90, 92)$.\n  Subject $10$: pre $(112, 100)$, post $(112, 111)$.\n\n- Case $2$ (no improvement, identical errors, $4$ subjects):\n  Subject $1$: pre $(100, 105)$, post $(100, 105)$.\n  Subject $2$: pre $(95, 100)$, post $(95, 100)$.\n  Subject $3$: pre $(120, 115)$, post $(120, 115)$.\n  Subject $4$: pre $(80, 85)$, post $(80, 85)$.\n\n- Case $3$ (small sample, mixed effects, $3$ subjects):\n  Subject $1$: pre $(90, 98)$, post $(90, 95)$.\n  Subject $2$: pre $(110, 117)$, post $(110, 117)$.\n  Subject $3$: pre $(70, 76)$, post $(70, 74)$.\n\n- Case $4$ (worse after correction, $5$ subjects):\n  Subject $1$: pre $(100, 102)$, post $(100, 104)$.\n  Subject $2$: pre $(95, 96)$, post $(95, 98)$.\n  Subject $3$: pre $(120, 123)$, post $(120, 126)$.\n  Subject $4$: pre $(80, 82)$, post $(80, 82)$.\n  Subject $5$: pre $(60, 61)$, post $(60, 65)$.\n\nFinal output format:\nYour program should produce a single line of output containing a list of results for the four cases, where each case’s result is a two-element list $[p, d]$ with $p$ the one-sided p-value rounded to six decimal places and $d$ the decision integer ($1$ or $0$). For example, the output must have the form $[[p_{1},d_{1}],[p_{2},d_{2}],[p_{3},d_{3}],[p_{4},d_{4}]]$ with no additional text.", "solution": "The problem requires a statistical assessment of the efficacy of a bias field correction algorithm in Magnetic Resonance Imaging (MRI) radiomics. Specifically, we are to determine if the correction method leads to a statistically significant improvement in the reproducibility of a radiomics feature, $F$. The improvement is quantified by a reduction in the absolute test-retest error. This will be accomplished by performing a one-sided paired Student's $t$-test on the test-retest errors measured before and after correction for a cohort of subjects.\n\nThe core of the problem lies in a hypothesis test. We are given paired data for each subject $i$: a test-retest measurement pair before correction, $(F_{i,1}^{\\text{pre}}, F_{i,2}^{\\text{pre}})$, and a pair after correction, $(F_{i,1}^{\\text{post}}, F_{i,2}^{\\text{post}})$. The absolute test-retest error for subject $i$ before correction is defined as $e_{i}^{\\text{pre}} = \\lvert F_{i,1}^{\\text{pre}} - F_{i,2}^{\\text{pre}} \\rvert$, and after correction as $e_{i}^{\\text{post}} = \\lvert F_{i,1}^{\\text{post}} - F_{i,2}^{\\text{post}} \\rvert$.\n\nTo assess the improvement, we consider the paired difference for each subject, $d_{i} = e_{i}^{\\text{pre}} - e_{i}^{\\text{post}}$. A positive value of $d_{i}$ signifies that the test-retest error was reduced for subject $i$ after correction, indicating an improvement in reproducibility. The objective is to test if this improvement is systematic across the population, not just a result of random chance.\n\nThe statistical hypotheses are formulated with respect to the population mean of these differences, $\\mu_{d}$:\n- Null Hypothesis ($H_{0}$): $\\mu_{d} = 0$. This hypothesis states that, on average, there is no difference between the pre-correction and post-correction errors. The observed differences are due to random sampling variability.\n- Alternative Hypothesis ($H_{1}$): $\\mu_{d} > 0$. This hypothesis posits that the bias field correction systematically reduces the test-retest error, leading to a positive mean difference.\n\nThis is a one-sided (right-tailed) test because we are specifically interested in whether the correction *improves* reproducibility (i.e., reduces error). The problem states that the standardized mean difference follows a Student's $t$-distribution. This is the foundation for the paired $t$-test.\n\nThe procedure for each test case is as follows:\n\n1.  **Calculate Paired Differences**: For a sample of $n$ subjects, compute the vector of differences $\\mathbf{d} = [d_1, d_2, \\ldots, d_n]$, where $d_i = \\lvert F_{i,1}^{\\text{pre}} - F_{i,2}^{\\text{pre}} \\rvert - \\lvert F_{i,1}^{\\text{post}} - F_{i,2}^{\\text{post}} \\rvert$.\n\n2.  **Handle Edge Cases**: The problem specifies that if the sample size $n < 2$ or if the sample standard deviation of the differences is $0$, the test is inconclusive. In these scenarios, the p-value is defined as $1.0$, and we fail to reject $H_0$. A sample size less than $2$ does not permit the calculation of a sample standard deviation. A standard deviation of $0$ implies all $d_i$ values are identical; if their mean is not greater than $0$, there is no evidence against $H_0$. If their mean is greater than $0$ but $s_d = 0$, the $t$-statistic would be infinite, which is a problematic singularity. The rule provides a practical way to handle this.\n\n3.  **Compute Test Statistic**: If $n \\ge 2$ and the sample standard deviation $s_d > 0$, we proceed.\n    -   Calculate the sample mean of the differences:\n        $$ \\bar{d} = \\frac{1}{n} \\sum_{i=1}^{n} d_i $$\n    -   Calculate the sample standard deviation of the differences (with Bessel's correction):\n        $$ s_d = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (d_i - \\bar{d})^2} $$\n    -   Calculate the standard error of the mean difference:\n        $$ SE(\\bar{d}) = \\frac{s_d}{\\sqrt{n}} $$\n    -   Compute the $t$-statistic, which measures how many standard errors the sample mean $\\bar{d}$ is from the hypothesized mean of $0$:\n        $$ t = \\frac{\\bar{d} - 0}{SE(\\bar{d})} = \\frac{\\bar{d}}{s_d / \\sqrt{n}} $$\n\n4.  **Calculate p-value**: The p-value is the probability of observing a $t$-statistic at least as extreme as the one computed, assuming the null hypothesis is true. For our right-tailed test, this corresponds to the area under the probability density function of a Student's $t$-distribution with $\\nu = n-1$ degrees of freedom to the right of our calculated $t$-statistic.\n    $$ p = P(T_{\\nu} \\ge t) $$\n    This is calculated using the survival function (1 minus the cumulative distribution function) of the $t$-distribution. The p-value is then rounded to six decimal places.\n\n5.  **Make a Decision**: Compare the p-value to the given significance level, $\\alpha = 0.05$.\n    -   If $p \\le \\alpha$, we reject the null hypothesis $H_0$. This suggests that the observed reduction in error is statistically significant. The decision is denoted by the integer $1$.\n    -   If $p > \\alpha$, we fail to reject the null hypothesis $H_0$. There is insufficient evidence to conclude that the correction method improves reproducibility. The decision is denoted by the integer $0$.\n\nThe final output for each case is a two-element list containing the rounded p-value and the integer decision.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Computes one-sided p-values and decisions for a paired t-test\n    on radiomics feature reproducibility improvement.\n    \"\"\"\n    # Test suite data as provided in the problem statement.\n    test_cases = [\n        # Case 1\n        [\n            {'pre': (100, 110), 'post': (100, 102)},\n            {'pre': (95, 85), 'post': (95, 96)},\n            {'pre': (120, 130), 'post': (120, 119)},\n            {'pre': (80, 92), 'post': (80, 81)},\n            {'pre': (60, 72), 'post': (60, 63)},\n            {'pre': (105, 99), 'post': (105, 104)},\n            {'pre': (140, 150), 'post': (140, 142)},\n            {'pre': (77, 88), 'post': (77, 78)},\n            {'pre': (90, 104), 'post': (90, 92)},\n            {'pre': (112, 100), 'post': (112, 111)},\n        ],\n        # Case 2\n        [\n            {'pre': (100, 105), 'post': (100, 105)},\n            {'pre': (95, 100), 'post': (95, 100)},\n            {'pre': (120, 115), 'post': (120, 115)},\n            {'pre': (80, 85), 'post': (80, 85)},\n        ],\n        # Case 3\n        [\n            {'pre': (90, 98), 'post': (90, 95)},\n            {'pre': (110, 117), 'post': (110, 117)},\n            {'pre': (70, 76), 'post': (70, 74)},\n        ],\n        # Case 4\n        [\n            {'pre': (100, 102), 'post': (100, 104)},\n            {'pre': (95, 96), 'post': (95, 98)},\n            {'pre': (120, 123), 'post': (120, 126)},\n            {'pre': (80, 82), 'post': (80, 82)},\n            {'pre': (60, 61), 'post': (60, 65)},\n        ]\n    ]\n\n    results = []\n    alpha = 0.05\n    rounding_digits = 6\n\n    for case_data in test_cases:\n        # Calculate paired differences d_i for each subject\n        differences = []\n        for subject_data in case_data:\n            e_pre = abs(subject_data['pre'][0] - subject_data['pre'][1])\n            e_post = abs(subject_data['post'][0] - subject_data['post'][1])\n            d_i = e_pre - e_post\n            differences.append(d_i)\n\n        d = np.array(differences)\n        n = len(d)\n\n        # Handle special conditions as per problem statement\n        if n < 2:\n            p_value = 1.0\n            decision = 0\n            results.append([round(p_value, rounding_digits), decision])\n            continue\n        \n        s_d = np.std(d, ddof=1) # Sample standard deviation\n\n        if s_d == 0:\n            p_value = 1.0\n            decision = 0\n            results.append([round(p_value, rounding_digits), decision])\n            continue\n\n        # Perform the paired t-test\n        d_bar = np.mean(d)\n        t_statistic = d_bar / (s_d / np.sqrt(n))\n        df = n - 1\n\n        # Calculate one-sided (right-tailed) p-value\n        # p = P(T > t_statistic)\n        p_value = t.sf(t_statistic, df)\n        \n        p_value_rounded = round(p_value, rounding_digits)\n\n        # Make decision based on significance level alpha\n        decision = 1 if p_value_rounded <= alpha else 0\n        \n        results.append([p_value_rounded, decision])\n\n    # Final print statement in the exact required format.\n    print(f\"[[{results[0][0]:.6f},{results[0][1]}],[{results[1][0]:.6f},{results[1][1]}],[{results[2][0]:.6f},{results[2][1]}],[{results[3][0]:.6f},{results[3][1]}]]\")\n\nsolve()\n```", "id": "4531093"}]}