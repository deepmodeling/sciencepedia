{"hands_on_practices": [{"introduction": "A decision tree builds its structure by recursively asking the best possible question at each node to split the data. But what makes a question \"best\"? This is often determined by a principle from information theory called information gain, which measures how much a particular split reduces the uncertainty about the outcome. This exercise provides a hands-on calculation to demystify this core concept, allowing you to find the optimal split for a radiomics feature by maximizing the information gain [@problem_id:4535354].", "problem": "A radiomics pipeline for classifying lung nodules as benign or malignant uses a single univariate decision tree split on a texture feature $x_{j}$ derived from the Gray-Level Co-occurrence Matrix (GLCM). You are given the sorted feature values $x_{j} \\in \\{0.12, 0.14, 0.18, 0.30, 0.31\\}$ with corresponding binary labels $y \\in \\{0,0,1,1,1\\}$, where $0$ denotes benign and $1$ denotes malignant. Consider a decision tree split of the form $x_{j} \\le \\tau$ versus $x_{j} > \\tau$, where $\\tau$ is a threshold chosen from midpoints between consecutive, distinct feature values.\n\nStarting from the definitions below, determine the threshold $\\tau$ that maximizes the information gain and justify the choice by evaluating the information gain for all admissible midpoints.\n\nUse the following fundamental base:\n- The entropy of a set $S$ with class probabilities $\\{p_{c}\\}$ is defined as $H(S) = -\\sum_{c} p_{c} \\ln p_{c}$, using the natural logarithm.\n- For a split of $S$ into left and right subsets $S_{L}$ and $S_{R}$, the weighted post-split entropy is $\\frac{|S_{L}|}{|S|} H(S_{L}) + \\frac{|S_{R}|}{|S|} H(S_{R})$.\n- The information gain is $IG = H(S) - \\left(\\frac{|S_{L}|}{|S|} H(S_{L}) + \\frac{|S_{R}|}{|S|} H(S_{R})\\right)$.\n\nProvide the final threshold $\\tau$ as a single real number. No rounding is required.", "solution": "The problem is valid as it is scientifically grounded in the principles of information theory and machine learning, well-posed with all necessary information, and stated objectively. We proceed to find the optimal decision tree split threshold, $\\tau$, that maximizes the information gain.\n\nThe dataset, a set of radiomics feature values and corresponding class labels, is denoted by $S$. The feature values $x_j$ are sorted, and the pairs $(x_j, y)$ are:\n$S = \\{(0.12, 0), (0.14, 0), (0.18, 1), (0.30, 1), (0.31, 1)\\}$.\nThe total number of samples is $|S| = 5$. The classes are benign (label $0$) and malignant (label $1$). The number of benign samples is $n_0 = 2$, and the number of malignant samples is $n_1 = 3$.\n\nThe first step is to calculate the entropy of the entire dataset $S$, denoted as $H(S)$. The probabilities of the classes are $p_0 = \\frac{n_0}{|S|} = \\frac{2}{5}$ and $p_1 = \\frac{n_1}{|S|} = \\frac{3}{5}$.\nUsing the given formula for entropy, $H(S) = -\\sum_{c} p_{c} \\ln p_{c}$:\n$$H(S) = -\\left( \\frac{2}{5} \\ln\\left(\\frac{2}{5}\\right) + \\frac{3}{5} \\ln\\left(\\frac{3}{5}\\right) \\right)$$\nThis value represents the total uncertainty in the dataset before any split.\n\nNext, we identify the candidate thresholds, $\\tau$. These are defined as the midpoints between consecutive, distinct feature values. The sorted distinct feature values are $0.12$, $0.14$, $0.18$, $0.30$, and $0.31$.\nThe candidate thresholds are:\n1. $\\tau_1 = \\frac{0.12 + 0.14}{2} = 0.13$\n2. $\\tau_2 = \\frac{0.14 + 0.18}{2} = 0.16$\n3. $\\tau_3 = \\frac{0.18 + 0.30}{2} = 0.24$\n4. $\\tau_4 = \\frac{0.30 + 0.31}{2} = 0.305$\n\nWe now evaluate the information gain for each candidate threshold. The information gain, $IG(\\tau)$, for a split at threshold $\\tau$ is given by:\n$$IG(\\tau) = H(S) -_H(S|\\tau)$$\nwhere $H(S|\\tau)$ is the weighted post-split entropy:\n$$H(S|\\tau) = \\frac{|S_L|}{|S|} H(S_L) + \\frac{|S_R|}{|S|} H(S_R)$$\nHere, $S_L = \\{(x_j, y) \\in S | x_j \\le \\tau \\}$ and $S_R = \\{(x_j, y) \\in S | x_j > \\tau \\}$. Maximizing $IG(\\tau)$ is equivalent to minimizing the weighted post-split entropy $H(S|\\tau)$.\n\nWe will calculate $H(S|\\tau)$ for each of the four candidate thresholds.\n\nCase 1: $\\tau = \\tau_1 = 0.13$\nThe split is $x_j \\le 0.13$ versus $x_j > 0.13$.\n$S_L = \\{(0.12, 0)\\}$. Thus, $|S_L| = 1$. The class distribution is $\\{n_0=1, n_1=0\\}$. This is a pure subset.\nThe entropy is $H(S_L) = -(1 \\ln(1) + 0) = 0$. Note that we use the convention $0 \\ln(0)=0$.\n$S_R = \\{(0.14, 0), (0.18, 1), (0.30, 1), (0.31, 1)\\}$. Thus, $|S_R| = 4$. The class distribution is $\\{n_0=1, n_1=3\\}$.\nThe entropy is $H(S_R) = -\\left(\\frac{1}{4} \\ln\\left(\\frac{1}{4}\\right) + \\frac{3}{4} \\ln\\left(\\frac{3}{4}\\right)\\right)$.\nThe weighted post-split entropy is:\n$$H(S|\\tau_1) = \\frac{1}{5} H(S_L) + \\frac{4}{5} H(S_R) = \\frac{1}{5}(0) + \\frac{4}{5}\\left(-\\frac{1}{4} \\ln\\left(\\frac{1}{4}\\right) - \\frac{3}{4} \\ln\\left(\\frac{3}{4}\\right)\\right) = -\\frac{1}{5} \\ln\\left(\\frac{1}{4}\\right) - \\frac{3}{5} \\ln\\left(\\frac{3}{4}\\right)$$\n$$H(S|\\tau_1) = \\frac{1}{5} \\ln(4) - \\frac{3}{5}(\\ln(3) - \\ln(4)) = \\frac{4}{5} \\ln(4) - \\frac{3}{5} \\ln(3) = \\frac{8}{5} \\ln(2) - \\frac{3}{5} \\ln(3)$$\n\nCase 2: $\\tau = \\tau_2 = 0.16$\nThe split is $x_j \\le 0.16$ versus $x_j > 0.16$.\n$S_L = \\{(0.12, 0), (0.14, 0)\\}$. Thus, $|S_L| = 2$. The class distribution is $\\{n_0=2, n_1=0\\}$. This is a pure subset.\nThe entropy is $H(S_L) = -(1 \\ln(1) + 0) = 0$.\n$S_R = \\{(0.18, 1), (0.30, 1), (0.31, 1)\\}$. Thus, $|S_R| = 3$. The class distribution is $\\{n_0=0, n_1=3\\}$. This is also a pure subset.\nThe entropy is $H(S_R) = -(0 + 1 \\ln(1)) = 0$.\nThe weighted post-split entropy is:\n$$H(S|\\tau_2) = \\frac{2}{5} H(S_L) + \\frac{3}{5} H(S_R) = \\frac{2}{5}(0) + \\frac{3}{5}(0) = 0$$\n\nCase 3: $\\tau = \\tau_3 = 0.24$\nThe split is $x_j \\le 0.24$ versus $x_j > 0.24$.\n$S_L = \\{(0.12, 0), (0.14, 0), (0.18, 1)\\}$. Thus, $|S_L| = 3$. The class distribution is $\\{n_0=2, n_1=1\\}$.\nThe entropy is $H(S_L) = -\\left(\\frac{2}{3} \\ln\\left(\\frac{2}{3}\\right) + \\frac{1}{3} \\ln\\left(\\frac{1}{3}\\right)\\right)$.\n$S_R = \\{(0.30, 1), (0.31, 1)\\}$. Thus, $|S_R| = 2$. The class distribution is $\\{n_0=0, n_1=2\\}$. This is a pure subset.\nThe entropy is $H(S_R) = -(0 + 1 \\ln(1)) = 0$.\nThe weighted post-split entropy is:\n$$H(S|\\tau_3) = \\frac{3}{5} H(S_L) + \\frac{2}{5} H(S_R) = \\frac{3}{5}\\left(-\\frac{2}{3} \\ln\\left(\\frac{2}{3}\\right) - \\frac{1}{3} \\ln\\left(\\frac{1}{3}\\right)\\right) + \\frac{2}{5}(0) = -\\frac{2}{5} \\ln\\left(\\frac{2}{3}\\right) - \\frac{1}{5} \\ln\\left(\\frac{1}{3}\\right)$$\n$$H(S|\\tau_3) = -\\frac{2}{5}(\\ln(2) - \\ln(3)) - \\frac{1}{5}(-\\ln(3)) = \\frac{3}{5} \\ln(3) - \\frac{2}{5} \\ln(2)$$\n\nCase 4: $\\tau = \\tau_4 = 0.305$\nThe split is $x_j \\le 0.305$ versus $x_j > 0.305$.\n$S_L = \\{(0.12, 0), (0.14, 0), (0.18, 1), (0.30, 1)\\}$. Thus, $|S_L| = 4$. The class distribution is $\\{n_0=2, n_1=2\\}$.\nThe entropy is $H(S_L) = -\\left(\\frac{2}{4} \\ln\\left(\\frac{2}{4}\\right) + \\frac{2}{4} \\ln\\left(\\frac{2}{4}\\right)\\right) = -\\ln\\left(\\frac{1}{2}\\right) = \\ln(2)$.\n$S_R = \\{(0.31, 1)\\}$. Thus, $|S_R| = 1$. The class distribution is $\\{n_0=0, n_1=1\\}$. This is a pure subset.\nThe entropy is $H(S_R) = -(0 + 1 \\ln(1)) = 0$.\nThe weighted post-split entropy is:\n$$H(S|\\tau_4) = \\frac{4}{5} H(S_L) + \\frac{1}{5} H(S_R) = \\frac{4}{5} \\ln(2) + \\frac{1}{5}(0) = \\frac{4}{5} \\ln(2)$$\n\nTo find the optimal threshold, we compare the weighted post-split entropies. The lowest entropy corresponds to the highest information gain.\n- $H(S|\\tau_1) = \\frac{8}{5} \\ln(2) - \\frac{3}{5} \\ln(3) \\approx \\frac{8}{5}(0.693) - \\frac{3}{5}(1.098) = 1.109 - 0.659 = 0.450$\n- $H(S|\\tau_2) = 0$\n- $H(S|\\tau_3) = \\frac{3}{5} \\ln(3) - \\frac{2}{5} \\ln(2) \\approx \\frac{3}{5}(1.098) - \\frac{2}{5}(0.693) = 0.659 - 0.277 = 0.382$\n- $H(S|\\tau_4) = \\frac{4}{5} \\ln(2) \\approx \\frac{4}{5}(0.693) = 0.554$\n\nComparing the values: $0 < 0.382 < 0.450 < 0.554$.\nThe minimum weighted post-split entropy is $0$, which occurs for $\\tau_2 = 0.16$. This split results in two pure subsets, perfectly separating the classes. Therefore, the information gain is maximized at this threshold. The maximum information gain is $IG(\\tau_2) = H(S) - 0 = H(S)$.\n\nThe threshold that maximizes the information gain is $\\tau_2$.", "answer": "$$\n\\boxed{0.16}\n$$", "id": "4535354"}, {"introduction": "While single decision trees are intuitive, they can be unstable and prone to overfitting, a condition of high variance. Random Forests overcome this by aggregating the predictions of many individual trees, a technique known as bootstrap aggregation or \"bagging.\" This exercise delves into the bias-variance tradeoff, a fundamental concept in machine learning, to provide a quantitative estimate of how much bagging reduces the model's variance and improves its ability to generalize to new, unseen data [@problem_id:4535439].", "problem": "In a radiomics regression study using computed tomography (CT) images, a team predicts a continuous imaging-derived risk score from a high-dimensional set of radiomic features using squared-error loss. For a single decision tree trained on the radiomic features, the observed training mean squared error (MSE) is $0.10$, and the observed test MSE is $0.18$. For a Random Forest (RF) model obtained by bootstrap aggregation (bagging) of decision trees built on the same features, the observed test MSE is $0.12$. Under the bias–variance–noise decomposition of squared-error predictive risk, and assuming that bagging changes the variance component while leaving the bias and the irreducible noise with negligible change across these two models, estimate the absolute reduction in the variance component of the generalization error attributable to bagging. Provide your answer as a single decimal number and round your result to three significant figures.", "solution": "The problem asks for the absolute reduction in the variance component of the generalization error when moving from a single decision tree model to a Random Forest model. The solution is based on the bias–variance–noise decomposition for squared-error loss.\n\nThe expected generalization error, which is estimated by the test mean squared error ($MSE_{test}$), can be decomposed into three components: squared bias, variance, and irreducible noise. For a given predictive model, the relationship is expressed as:\n$$\nMSE_{test} = (\\text{Bias})^2 + \\text{Variance} + \\text{Noise}\n$$\nwhere:\n- $(\\text{Bias})^2$ is the squared bias, representing the error from erroneous assumptions in the learning algorithm.\n- $\\text{Variance}$ is the variance of the model's prediction, representing the model's sensitivity to small fluctuations in the training set.\n- $\\text{Noise}$ is the irreducible error, which is a property of the data generating process and represents a lower bound on the expected error of any model.\n\nLet's denote the components for the single decision tree (DT) and the Random Forest (RF) models with respective subscripts.\n\nFor the single decision tree model, the test MSE is given as $MSE_{test, DT} = 0.18$. Its decomposition is:\n$$\nMSE_{test, DT} = (\\text{Bias}_{DT})^2 + \\text{Variance}_{DT} + \\text{Noise}\n$$\nSubstituting the given value, we have:\n$$\n0.18 = (\\text{Bias}_{DT})^2 + \\text{Variance}_{DT} + \\text{Noise} \\quad (1)\n$$\nThe training MSE for the decision tree, given as $0.10$, is noted but not directly required for the calculation under the problem's assumptions. The discrepancy between the training MSE ($0.10$) and test MSE ($0.18$) is indicative of overfitting, a high-variance condition that bagging is designed to mitigate.\n\nFor the Random Forest model, which uses bootstrap aggregation (bagging), the test MSE is given as $MSE_{test, RF} = 0.12$. Its decomposition is:\n$$\nMSE_{test, RF} = (\\text{Bias}_{RF})^2 + \\text{Variance}_{RF} + \\text{Noise}\n$$\nSubstituting the given value, we have:\n$$\n0.12 = (\\text{Bias}_{RF})^2 + \\text{Variance}_{RF} + \\text{Noise} \\quad (2)\n$$\nThe problem states a critical assumption: \"bagging changes the variance component while leaving the bias and the irreducible noise with negligible change\". This implies two conditions:\n1. The irreducible noise term, $\\text{Noise}$, is inherent to the data and is therefore identical for both models.\n2. The squared bias of the Random Forest model is approximately equal to the squared bias of the single decision tree model. That is, $(\\text{Bias}_{RF})^2 \\approx (\\text{Bias}_{DT})^2$. We can denote this common squared bias term as $(\\text{Bias})^2$.\n\nApplying this assumption, equations $(1)$ and $(2)$ can be rewritten as:\n$$\n0.18 = (\\text{Bias})^2 + \\text{Variance}_{DT} + \\text{Noise} \\quad (1')\n$$\n$$\n0.12 = (\\text{Bias})^2 + \\text{Variance}_{RF} + \\text{Noise} \\quad (2')\n$$\nWe are asked to find the absolute reduction in the variance component, which is the difference $\\text{Variance}_{DT} - \\text{Variance}_{RF}$. To find this quantity, we can subtract equation $(2')$ from equation $(1')$:\n$$\n0.18 - 0.12 = ((\\text{Bias})^2 + \\text{Variance}_{DT} + \\text{Noise}) - ((\\text{Bias})^2 + \\text{Variance}_{RF} + \\text{Noise})\n$$\nThe common bias and noise terms cancel out:\n$$\n0.06 = (\\text{Bias})^2 - (\\text{Bias})^2 + \\text{Variance}_{DT} - \\text{Variance}_{RF} + \\text{Noise} - \\text{Noise}\n$$\n$$\n0.06 = \\text{Variance}_{DT} - \\text{Variance}_{RF}\n$$\nThus, the absolute reduction in the variance component attributable to bagging is $0.06$.\n\nThe problem requires the answer to be rounded to three significant figures. The calculated value is exactly $0.06$. To express this decimal with three significant figures, we include trailing zeros. The first significant figure is the $6$. To show three significant figures, we write the number as $0.0600$.", "answer": "$$\n\\boxed{0.0600}\n$$", "id": "4535439"}, {"introduction": "Applying machine learning in a clinical setting like radiomics requires rigorous methodology. A common challenge arises when a dataset contains multiple observations from the same patient, such as features from several lesions. Treating these correlated data points as independent can lead to \"data leakage\" and dangerously misleading results. This practice addresses this critical issue by outlining a principled pipeline for model validation with patient-level data splitting and introduces the concept of effective sample size, $n_{\\text{eff}}$, to correctly plan studies with such clustered data structures [@problem_id:4535444].", "problem": "Consider a radiomics classification task where the binary outcome is $Y \\in \\{0,1\\}$ and the radiomic feature matrix is $X \\in \\mathbb{R}^{n \\times p}$, with $n$ observations derived from $N$ patients. Each patient contributes exactly $m$ lesions, so $n = N m$. In this context, train decision tree and random forest classifiers in a principled manner that avoids data leakage when multiple lesions per patient are present. Your pipeline should begin from core definitions about independence, correlation within clusters (patients), and the need for patient-level splitting, and should articulate how to perform preprocessing, model selection, validation, and testing without contaminating the evaluation by cross-patient information flow. Then, under a scientifically realistic assumption that lesions within the same patient are exchangeable with a common intra-patient correlation coefficient $\\rho \\in [0,1)$ and lesions from different patients are uncorrelated, derive from first principles an analytic expression for the minimal integer number of unique patients $N_{\\min}$ required so that the effective sample size $n_{\\text{eff}}$ is achieved or exceeded. Express your final answer solely in terms of $n_{\\text{eff}}$, $m$, and $\\rho$. No numerical substitution is required, and no rounding beyond the minimal-integer requirement is needed. Provide only the final expression for $N_{\\min}$.", "solution": "The problem presented is a valid and highly relevant one in the field of medical image analysis, specifically in radiomics, where machine learning models are developed using data that possesses a hierarchical or clustered structure. The core challenge stems from the fact that multiple observations (lesions) are derived from the same unit of analysis (patient), which violates the assumption of independence that underlies many standard statistical and machine learning procedures.\n\nFirst, we will articulate a principled pipeline for training decision tree and random forest classifiers while rigorously avoiding data leakage. The fundamental principle is that the observations are not independent and identically distributed (IID). While lesions from different patients can be assumed to be independent, multiple lesions from the same patient are typically correlated. This correlation arises from shared genetics, physiology, environmental exposures, and other patient-specific factors. The set of observations for a patient $i$ constitutes a cluster. Ignoring this structure and performing data splitting at the observation (lesion) level leads to data leakage. For instance, if one lesion from patient $A$ is in the training set and another lesion from the same patient $A$ is in the test set, the model may learn patient-specific idiosyncrasies rather than generalizable biomarkers of the outcome $Y$. This contaminates the evaluation, leading to an overly optimistic estimate of model performance that will not generalize to new, unseen patients.\n\nTo prevent such information contamination, all data partitioning must be performed at the patient level, as the patient is the unit of independence. The correct pipeline is as follows:\n1.  **Data Splitting**: The entire cohort of $N$ patients is partitioned into a training set, a validation set, and a test set. For instance, one might allocate a percentage of patients, say $70\\%$, to training, $15\\%$ to validation, and $15\\%$ to testing. All $m$ lesions from any given patient must reside entirely within one of these sets. This ensures strict independence between the data used for training, hyperparameter tuning, and final evaluation.\n2.  **Preprocessing**: Any data preprocessing steps, such as feature standardization (scaling to zero mean and unit variance), must be learned exclusively from the training set. The parameters of the transformation (e.g., mean and standard deviation) are computed using only the data from the training patients. These same fixed parameters are then applied to transform the validation and test sets. This prevents information from the validation or test sets from \"leaking\" into the training process.\n3.  **Model Selection and Hyperparameter Tuning**: This stage uses the training and validation sets to find the optimal hyperparameters for the classifier (e.g., maximum depth of a decision tree, or number of trees in a random forest). The standard method is $k$-fold cross-validation, which must also be adapted for clustered data. The set of training patients is partitioned into $k$ folds. In each iteration, models are trained on $k-1$ folds of patients and validated on the remaining fold of patients. This process is repeated $k$ times, and the performance is averaged to select the best hyperparameter set. This patient-level folding ensures that the validation fold in each iteration is always independent of the training folds for that iteration.\n4.  **Final Training and Testing**: After the optimal hyperparameters are identified, the final model is trained on the entire training set (all lesions from all patients in the training partition). Its generalization performance is then evaluated a single time on the held-out test set. The performance on this previously untouched set of patients provides an unbiased estimate of how the model will perform on new patients in a clinical setting.\n\nNext, we derive the expression for the minimal number of patients, $N_{\\min}$. The concept of effective sample size, $n_{\\text{eff}}$, is central here. It is the size of a hypothetical IID sample that would yield the same statistical precision (i.e., the same variance of an estimator) as our actual, correlated sample of size $n=Nm$.\n\nLet $Y_{ij}$ be the outcome for the $j$-th lesion of the $i$-th patient, for $i \\in \\{1, \\dots, N\\}$ and $j \\in \\{1, \\dots, m\\}$. Let's assume, without loss of generality, that each $Y_{ij}$ has a common variance $\\text{Var}(Y_{ij}) = \\sigma^2$. The problem states that lesions from different patients are uncorrelated, while lesions from the same patient are exchangeable with a common intra-patient correlation coefficient $\\rho$.\n$$ \\text{Corr}(Y_{ij}, Y_{ik}) = \\rho \\quad \\text{for } j \\neq k $$\n$$ \\text{Corr}(Y_{ij}, Y_{i'k'}) = 0 \\quad \\text{for } i \\neq i' $$\nThis implies the covariance structure:\n$$ \\text{Cov}(Y_{ij}, Y_{ik}) = \\rho \\sigma^2 \\quad \\text{for } j \\neq k $$\nThe variance of the sample mean $\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^N \\sum_{j=1}^m Y_{ij}$ for our clustered sample is:\n$$ \\text{Var}(\\bar{Y}) = \\text{Var}\\left(\\frac{1}{Nm} \\sum_{i=1}^N \\sum_{j=1}^m Y_{ij}\\right) = \\frac{1}{(Nm)^2} \\text{Var}\\left(\\sum_{i=1}^N \\sum_{j=1}^m Y_{ij}\\right) $$\nSince data from different patients are independent, the variance of the sum is the sum of the variances of patient-level totals. Let $S_i = \\sum_{j=1}^m Y_{ij}$ be the sum of outcomes for patient $i$.\n$$ \\text{Var}\\left(\\sum_{i=1}^N S_i\\right) = \\sum_{i=1}^N \\text{Var}(S_i) $$\nAssuming patients are statistically homogeneous, $\\text{Var}(S_i)$ is constant across patients, so $\\sum_{i=1}^N \\text{Var}(S_i) = N \\cdot \\text{Var}(S_1)$. Let's compute $\\text{Var}(S_1)$:\n$$ \\text{Var}(S_1) = \\text{Var}\\left(\\sum_{j=1}^m Y_{1j}\\right) = \\sum_{j=1}^m \\text{Var}(Y_{1j}) + \\sum_{j \\neq k} \\text{Cov}(Y_{1j}, Y_{1k}) $$\nThere are $m$ variance terms and $m(m-1)$ covariance terms.\n$$ \\text{Var}(S_1) = m \\sigma^2 + m(m-1) \\rho \\sigma^2 = m \\sigma^2 [1 + (m-1)\\rho] $$\nSubstituting this back into the expression for $\\text{Var}(\\bar{Y})$:\n$$ \\text{Var}(\\bar{Y}) = \\frac{1}{(Nm)^2} \\cdot N \\cdot \\left( m \\sigma^2 [1 + (m-1)\\rho] \\right) = \\frac{Nm\\sigma^2 [1+(m-1)\\rho]}{N^2 m^2} $$\n$$ \\text{Var}(\\bar{Y}) = \\frac{\\sigma^2}{Nm} [1 + (m-1)\\rho] = \\frac{\\sigma^2}{n} [1 + (m-1)\\rho] $$\nThe term $[1 + (m-1)\\rho]$ is the variance inflation factor due to clustering. For an IID sample of size $n_{\\text{eff}}$, the variance of the mean would be $\\frac{\\sigma^2}{n_{\\text{eff}}}$. By definition of effective sample size, we equate the two variances:\n$$ \\frac{\\sigma^2}{n_{\\text{eff}}} = \\frac{\\sigma^2}{n} [1 + (m-1)\\rho] $$\nThis gives the relationship between the total number of observations $n$ and the effective sample size $n_{\\text{eff}}$:\n$$ n_{\\text{eff}} = \\frac{n}{1 + (m-1)\\rho} $$\nSubstituting $n = Nm$:\n$$ n_{\\text{eff}} = \\frac{Nm}{1 + (m-1)\\rho} $$\nWe are asked for the minimal integer number of patients $N_{\\min}$ required for the effective sample size to be at least a target value $n_{\\text{eff}}$. This is expressed by the inequality:\n$$ \\frac{N m}{1 + (m-1)\\rho} \\ge n_{\\text{eff}} $$\nWe now solve for $N$. The denominator $[1+(m-1)\\rho]$ is positive since $m \\ge 1$ and $\\rho \\in [0,1)$.\n$$ Nm \\ge n_{\\text{eff}} [1 + (m-1)\\rho] $$\n$$ N \\ge \\frac{n_{\\text{eff}} [1 + (m-1)\\rho]}{m} $$\nSince $N$ must be an integer, the minimal number of patients $N_{\\min}$ is the smallest integer satisfying this inequality, which is obtained by taking the ceiling of the right-hand side.\n$$ N_{\\min} = \\left\\lceil \\frac{n_{\\text{eff}} (1 + (m-1)\\rho)}{m} \\right\\rceil $$\nThis expression provides the required minimum number of unique patients as a function of the desired effective sample size $n_{\\text{eff}}$, the number of lesions per patient $m$, and the intra-patient correlation $\\rho$.", "answer": "$$\\boxed{\\left\\lceil \\frac{n_{\\text{eff}} (1 + (m-1)\\rho)}{m} \\right\\rceil}$$", "id": "4535444"}]}