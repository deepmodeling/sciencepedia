{"hands_on_practices": [{"introduction": "At the heart of logistic regression is the transformation of a linear combination of predictors into a probability. This exercise is essential for building intuition about how model coefficients directly influence predictions. By working through the core formulas that connect the linear predictor, $\\eta$, to the odds, $\\exp(\\eta)$, probability, $\\frac{\\exp(\\eta)}{1 + \\exp(\\eta)}$, and odds ratio, $\\exp(\\beta_j)$, you will demystify the inner workings of the model and gain a concrete understanding of its outputs [@problem_id:4974080].", "problem": "You are given a binary outcome model appropriate for medical event data, where a patient’s event indicator $Y \\in \\{0,1\\}$ has conditional probability $p(\\mathbf{x}) = \\mathbb{P}(Y=1 \\mid \\mathbf{x})$ given a vector of predictors $\\mathbf{x} = (x_1,\\dots,x_k)$. Assume the logistic regression model, defined by the fundamental relation that the log-odds (logit) of the event is linear in the predictors: $$\\log\\left(\\frac{p(\\mathbf{x})}{1 - p(\\mathbf{x})}\\right) = \\beta_0 + \\sum_{j=1}^{k} \\beta_j x_j,$$ where $\\beta_0$ is an intercept and $(\\beta_1,\\dots,\\beta_k)$ are regression coefficients. Starting only from this definition and the definition of odds as the ratio of probability to its complement, derive how to compute both the fitted odds and the predicted probability for a given patient’s predictors and a specified coefficient vector. Then, consider a one-unit increase in a single predictor $x_j$ while holding all other predictors fixed and derive the implied multiplicative change in the odds (that is, the odds ratio), as a function of $\\beta_j$.\n\nYour task is to implement a program that, for each test case below, computes:\n- the fitted odds $O(\\mathbf{x})$ at the given predictor vector $\\mathbf{x}$ and coefficients,\n- the predicted probability $p(\\mathbf{x})$,\n- the odds ratio for a one-unit increase in the specified predictor index $j$ while holding other predictors fixed.\n\nAll computed outputs must be real numbers. There are no physical units in this problem. Your program must round each floating-point output to exactly six decimal places.\n\nTest suite:\n- Case 1: $\\beta = (-2.0, 0.6, 0.4)$, $\\mathbf{x} = (2.0, 1.5)$, $j = 1$.\n- Case 2: $\\beta = (1.0, 2.0, -0.5, 0.3)$, $\\mathbf{x} = (3.0, -2.0, 5.0)$, $j = 2$.\n- Case 3: $\\beta = (0.0, -3.0)$, $\\mathbf{x} = (2.0)$, $j = 1$.\n- Case 4: $\\beta = (0.0, 0.0, 0.0)$, $\\mathbf{x} = (10.0, -5.0)$, $j = 2$.\n\nInterpretation and indexing details:\n- For each case, $\\beta_0$ is the intercept, followed by $(\\beta_1,\\dots,\\beta_k)$ corresponding to the components of $\\mathbf{x}$ in order.\n- The index $j$ refers to the predictor coefficient $\\beta_j$ associated with $x_j$, using $1$-based indexing for predictors (not counting the intercept).\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list of lists, enclosed in square brackets.\n- For each case, output a list of the form $[O(\\mathbf{x}), p(\\mathbf{x}), \\text{OR}_j]$, where $\\text{OR}_j$ is the odds ratio for a one-unit increase in $x_j$.\n- Each number must be rounded to exactly six decimal places.\n- Example format (illustrative only): $[[a_{11},a_{12},a_{13}],[a_{21},a_{22},a_{23}],\\dots]$ where each $a_{mn}$ is a float with six decimal places.\n\nYour program must run without any user input or external files and must compute and print the results for the four specified cases only, in the exact format described above.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the established theory of generalized linear models, specifically logistic regression. The problem is well-posed, with all necessary data and definitions provided to derive a unique, meaningful solution for each test case. The language is objective and precise.\n\nThe task requires the derivation and computation of three key quantities in a logistic regression model: the fitted odds, the predicted probability, and the odds ratio. We begin from the fundamental definition of the model.\n\nThe logistic regression model posits a linear relationship between the predictors $\\mathbf{x} = (x_1, \\dots, x_k)$ and the log-odds of the outcome probability $p(\\mathbf{x}) = \\mathbb{P}(Y=1 \\mid \\mathbf{x})$. This relationship is expressed as:\n$$\n\\log\\left(\\frac{p(\\mathbf{x})}{1 - p(\\mathbf{x})}\\right) = \\beta_0 + \\sum_{j=1}^{k} \\beta_j x_j\n$$\nwhere $\\beta_0$ is the intercept and $\\boldsymbol{\\beta} = (\\beta_1, \\dots, \\beta_k)$ is the vector of predictor coefficients. For convenience, we define the linear predictor, often denoted by $\\eta(\\mathbf{x})$, as the right-hand side of this equation:\n$$\n\\eta(\\mathbf{x}) = \\beta_0 + \\sum_{j=1}^{k} \\beta_j x_j = \\beta_0 + \\boldsymbol{\\beta}^T \\mathbf{x}\n$$\n\n**1. Derivation of Fitted Odds, $O(\\mathbf{x})$**\n\nThe odds of an event are defined as the ratio of the probability of the event occurring to the probability of it not occurring.\n$$\nO(\\mathbf{x}) = \\frac{p(\\mathbf{x})}{1 - p(\\mathbf{x})}\n$$\nBy substituting this definition into the fundamental model equation, we have:\n$$\n\\log(O(\\mathbf{x})) = \\eta(\\mathbf{x})\n$$\nTo solve for the odds, $O(\\mathbf{x})$, we exponentiate both sides of this equation, using the fact that the exponential function is the inverse of the natural logarithm:\n$$\nO(\\mathbf{x}) = \\exp(\\log(O(\\mathbf{x}))) = \\exp(\\eta(\\mathbf{x}))\n$$\nThus, the fitted odds are calculated by exponentiating the linear predictor.\n$$\nO(\\mathbf{x}) = \\exp\\left(\\beta_0 + \\sum_{j=1}^{k} \\beta_j x_j\\right)\n$$\n\n**2. Derivation of Predicted Probability, $p(\\mathbf{x})$**\n\nTo find the predicted probability $p(\\mathbf{x})$, we start with the definition of odds, $O(\\mathbf{x}) = \\frac{p(\\mathbf{x})}{1 - p(\\mathbf{x})}$, and solve for $p(\\mathbf{x})$ algebraically.\n$$\n\\begin{aligned}\nO(\\mathbf{x}) (1 - p(\\mathbf{x})) &= p(\\mathbf{x}) \\\\\nO(\\mathbf{x}) - O(\\mathbf{x}) p(\\mathbf{x}) &= p(\\mathbf{x}) \\\\\nO(\\mathbf{x}) &= p(\\mathbf{x}) + O(\\mathbf{x}) p(\\mathbf{x}) \\\\\nO(\\mathbf{x}) &= p(\\mathbf{x}) (1 + O(\\mathbf{x})) \\\\\np(\\mathbf{x}) &= \\frac{O(\\mathbf{x})}{1 + O(\\mathbf{x})}\n\\end{aligned}\n$$\nSubstituting our derived expression for $O(\\mathbf{x}) = \\exp(\\eta(\\mathbf{x}))$, we obtain the predicted probability in terms of the linear predictor:\n$$\np(\\mathbf{x}) = \\frac{\\exp(\\eta(\\mathbf{x}))}{1 + \\exp(\\eta(\\mathbf{x}))}\n$$\nThis function is commonly known as the logistic function or sigmoid function. An equivalent and often more numerically stable form is obtained by dividing the numerator and denominator by $\\exp(\\eta(\\mathbf{x}))$:\n$$\np(\\mathbf{x}) = \\frac{1}{\\exp(-\\eta(\\mathbf{x})) + 1}\n$$\n\n**3. Derivation of the Odds Ratio, $\\text{OR}_j$**\n\nThe odds ratio ($OR$) for a predictor $x_j$ is the factor by which the odds change for a one-unit increase in $x_j$, holding all other predictors constant. Let $\\mathbf{x}$ be the original predictor vector $(x_1, \\dots, x_j, \\dots, x_k)$, and let $\\mathbf{x}'$ be the vector with $x_j$ incremented by one: $(x_1, \\dots, x_j + 1, \\dots, x_k)$.\n\nThe odds at $\\mathbf{x}$ are:\n$$\nO(\\mathbf{x}) = \\exp\\left(\\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i\\right)\n$$\nThe odds at $\\mathbf{x}'$ are:\n$$\nO(\\mathbf{x}') = \\exp\\left(\\beta_0 + \\sum_{i \\neq j} \\beta_i x_i + \\beta_j(x_j + 1)\\right) = \\exp\\left(\\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i + \\beta_j\\right)\n$$\nThe odds ratio, $\\text{OR}_j$, is the ratio of these two odds:\n$$\n\\text{OR}_j = \\frac{O(\\mathbf{x}')}{O(\\mathbf{x})} = \\frac{\\exp\\left(\\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i + \\beta_j\\right)}{\\exp\\left(\\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i\\right)}\n$$\nUsing the property of exponents $\\frac{e^{a+b}}{e^a} = e^b$, we can simplify the expression:\n$$\n\\text{OR}_j = \\exp\\left(\\left(\\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i + \\beta_j\\right) - \\left(\\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i\\right)\\right) = \\exp(\\beta_j)\n$$\nThis is a key result: the odds ratio for a one-unit increase in predictor $x_j$ is simply the exponential of its corresponding coefficient, $\\beta_j$. It is constant and does not depend on the values of any of the predictors $\\mathbf{x}$.\n\n**Computational Algorithm**\n\nFor each test case, specified by a coefficient vector $\\boldsymbol{\\beta}_{\\text{full}} = (\\beta_0, \\beta_1, \\dots, \\beta_k)$, a predictor vector $\\mathbf{x} = (x_1, \\dots, x_k)$, and a predictor index $j$, the algorithm is as follows:\n1.  Calculate the linear predictor: $\\eta(\\mathbf{x}) = \\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i$. This can be computed efficiently using a dot product of the predictor coefficients $(\\beta_1, \\dots, \\beta_k)$ with $\\mathbf{x}$, and adding the intercept $\\beta_0$.\n2.  Calculate the fitted odds: $O(\\mathbf{x}) = \\exp(\\eta(\\mathbf{x}))$.\n3.  Calculate the predicted probability: $p(\\mathbf{x}) = \\frac{O(\\mathbf{x})}{1 + O(\\mathbf{x})}$.\n4.  Calculate the odds ratio: $\\text{OR}_j = \\exp(\\beta_j)$. The problem uses $1$-based indexing for $j$, so $\\beta_j$ refers to the coefficient of $x_j$. When using $0$-indexed arrays, this would be the element at index $j$ of the full coefficient vector.\n5.  Round each of the three results, $O(\\mathbf{x})$, $p(\\mathbf{x})$, and $\\text{OR}_j$, to exactly six decimal places for the final output.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes fitted odds, predicted probability, and an odds ratio for a logistic\n    regression model across several test cases.\n    \"\"\"\n\n    # Test cases defined as tuples of (beta_vector, x_vector, j_index).\n    # beta_vector is (beta_0, beta_1, ..., beta_k).\n    # x_vector is (x_1, ..., x_k).\n    # j_index is 1-based for the predictor of interest.\n    test_cases = [\n        # Case 1: beta = (-2.0, 0.6, 0.4), x = (2.0, 1.5), j = 1\n        ((-2.0, 0.6, 0.4), (2.0, 1.5), 1),\n        # Case 2: beta = (1.0, 2.0, -0.5, 0.3), x = (3.0, -2.0, 5.0), j = 2\n        ((1.0, 2.0, -0.5, 0.3), (3.0, -2.0, 5.0), 2),\n        # Case 3: beta = (0.0, -3.0), x = (2.0), j = 1\n        ((0.0, -3.0), (2.0,), 1),\n        # Case 4: beta = (0.0, 0.0, 0.0), x = (10.0, -5.0), j = 2\n        ((0.0, 0.0, 0.0), (10.0, -5.0), 2)\n    ]\n\n    all_results = []\n    for case in test_cases:\n        beta_tuple, x_tuple, j = case\n\n        # Convert tuples to numpy arrays for vectorized operations\n        beta = np.array(beta_tuple)\n        x = np.array(x_tuple)\n\n        # Decompose beta into intercept and predictor coefficients\n        beta_0 = beta[0]\n        beta_predictors = beta[1:]\n\n        # 1. Calculate the linear predictor (eta)\n        # eta = beta_0 + sum(beta_i * x_i)\n        eta = beta_0 + np.dot(beta_predictors, x)\n\n        # 2. Calculate the fitted odds\n        # O(x) = exp(eta)\n        odds = np.exp(eta)\n\n        # 3. Calculate the predicted probability\n        # p(x) = O(x) / (1 + O(x))\n        prob = odds / (1 + odds)\n\n        # 4. Calculate the odds ratio for a one-unit increase in x_j\n        # OR_j = exp(beta_j)\n        # The problem uses 1-based indexing for j, which corresponds to\n        # the coefficient beta[j] in our 0-indexed beta array.\n        odds_ratio = np.exp(beta[j])\n\n        # Store the results for this case. The problem requires rounding to\n        # exactly six decimal places, which we achieve using f-string formatting.\n        all_results.append([odds, prob, odds_ratio])\n\n    # Format the final output string as a list of lists of numbers\n    # with each number formatted to exactly six decimal places.\n    formatted_cases = []\n    for result_set in all_results:\n        o, p, or_j = result_set\n        formatted_cases.append(f\"[{o:.6f},{p:.6f},{or_j:.6f}]\")\n\n    print(f\"[{','.join(formatted_cases)}]\")\n\nsolve()\n```", "id": "4974080"}, {"introduction": "In radiomics, many features can be highly correlated, which can destabilize your model and make its coefficients difficult to interpret. This practice introduces a standard diagnostic technique, calculating the Variance Inflation Factor (VIF), to detect and quantify such multicollinearity. Mastering this calculation is an essential step toward building robust and reliable predictive models [@problem_id:4549628].", "problem": "A radiomics pipeline extracts standardized quantitative features from computed tomography images of pulmonary nodules to train a binary classifier using a logistic regression model with a logit link. Let the three standardized predictors be $x_1$ (Gray-Level Co-occurrence Matrix (GLCM) contrast), $x_2$ (Gray-Level Run-Length Matrix (GLRLM) long-run emphasis), and $x_3$ (Neighborhood Gray Tone Difference Matrix (NGTDM) coarseness). The features have been standardized to zero mean and unit variance across $n=120$ training cases, and their empirical correlation matrix is\n$$\nR \\;=\\;\n\\begin{pmatrix}\n1 & 0.85 & 0.3 \\\\\n0.85 & 1 & 0.25 \\\\\n0.3 & 0.25 & 1\n\\end{pmatrix}.\n$$\nTo assess multicollinearity before model fitting, compute the Variance Inflation Factor (VIF) for each predictor using only the feature correlation structure. Round each VIF to four significant figures. Based on the conventional instability threshold of $5$, interpret whether any features would be candidates for removal to improve model stability. For the final answer, report only the VIFs for $(x_1, x_2, x_3)$ as a single row matrix, rounded to four significant figures; any interpretive remarks must appear only in your solution.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It provides a complete and consistent set of data to perform a standard statistical calculation relevant to the field of radiomics.\n\nThe Variance Inflation Factor ($VIF$) for a predictor in a multiple regression model quantifies the extent to which the variance of its estimated coefficient is inflated due to multicollinearity with other predictors. For the $j$-th predictor, $x_j$, the $VIF_j$ is defined as:\n$$\nVIF_j = \\frac{1}{1 - R_j^2}\n$$\nwhere $R_j^2$ is the coefficient of determination from a linear regression of $x_j$ against all other predictors in the model.\n\nWhen predictors are standardized (mean of $0$ and unit variance), the $VIF$s can be computed directly from the predictor correlation matrix, $R$. Specifically, the $VIF$ for the $j$-th predictor, $VIF_j$, is the $j$-th diagonal element of the inverse of the correlation matrix, $R^{-1}$.\n$$\nVIF_j = (R^{-1})_{jj}\n$$\nThe problem provides the correlation matrix for the three standardized predictors $x_1$, $x_2$, and $x_3$:\n$$\nR =\n\\begin{pmatrix}\n1 & 0.85 & 0.3 \\\\\n0.85 & 1 & 0.25 \\\\\n0.3 & 0.25 & 1\n\\end{pmatrix}\n$$\nTo find the diagonal elements of $R^{-1}$, we use the formula for matrix inversion involving the adjugate matrix and the determinant:\n$$\nR^{-1} = \\frac{1}{\\det(R)} \\text{adj}(R)\n$$\nwhere $\\text{adj}(R)$ is the transpose of the cofactor matrix of $R$. The diagonal elements of $R^{-1}$ are given by:\n$$\n(R^{-1})_{jj} = \\frac{C_{jj}}{\\det(R)}\n$$\nwhere $C_{jj}$ is the cofactor of the diagonal element $R_{jj}$.\n\nFirst, we calculate the determinant of $R$:\n$$\n\\det(R) = 1 \\begin{vmatrix} 1 & 0.25 \\\\ 0.25 & 1 \\end{vmatrix} - 0.85 \\begin{vmatrix} 0.85 & 0.25 \\\\ 0.3 & 1 \\end{vmatrix} + 0.3 \\begin{vmatrix} 0.85 & 1 \\\\ 0.3 & 0.25 \\end{vmatrix}\n$$\n$$\n\\det(R) = 1(1 \\cdot 1 - 0.25 \\cdot 0.25) - 0.85(0.85 \\cdot 1 - 0.25 \\cdot 0.3) + 0.3(0.85 \\cdot 0.25 - 1 \\cdot 0.3)\n$$\n$$\n\\det(R) = 1(1 - 0.0625) - 0.85(0.85 - 0.075) + 0.3(0.2125 - 0.3)\n$$\n$$\n\\det(R) = 0.9375 - 0.85(0.775) + 0.3(-0.0875)\n$$\n$$\n\\det(R) = 0.9375 - 0.65875 - 0.02625 = 0.2525\n$$\nNow we compute the cofactors for the diagonal elements.\nFor $x_1$, the required cofactor is $C_{11}$:\n$$\nC_{11} = (-1)^{1+1} \\begin{vmatrix} 1 & 0.25 \\\\ 0.25 & 1 \\end{vmatrix} = 1 - (0.25)^2 = 1 - 0.0625 = 0.9375\n$$\nThe $VIF$ for $x_1$ is:\n$$\nVIF_1 = \\frac{C_{11}}{\\det(R)} = \\frac{0.9375}{0.2525} \\approx 3.712871287...\n$$\nFor $x_2$, the required cofactor is $C_{22}$:\n$$\nC_{22} = (-1)^{2+2} \\begin{vmatrix} 1 & 0.3 \\\\ 0.3 & 1 \\end{vmatrix} = 1 - (0.3)^2 = 1 - 0.09 = 0.91\n$$\nThe $VIF$ for $x_2$ is:\n$$\nVIF_2 = \\frac{C_{22}}{\\det(R)} = \\frac{0.91}{0.2525} \\approx 3.603960396...\n$$\nFor $x_3$, the required cofactor is $C_{33}$:\n$$\nC_{33} = (-1)^{3+3} \\begin{vmatrix} 1 & 0.85 \\\\ 0.85 & 1 \\end{vmatrix} = 1 - (0.85)^2 = 1 - 0.7225 = 0.2775\n$$\nThe $VIF$ for $x_3$ is:\n$$\nVIF_3 = \\frac{C_{33}}{\\det(R)} = \\frac{0.2775}{0.2525} \\approx 1.099009901...\n$$\nRounding these values to four significant figures as requested:\n$$\nVIF_1 \\approx 3.713\n$$\n$$\nVIF_2 \\approx 3.604\n$$\n$$\nVIF_3 \\approx 1.099\n$$\nThe problem specifies a conventional instability threshold of $VIF > 5$. Comparing our results to this threshold, we find that all three predictors have $VIF$ values below $5$.\n$$\nVIF_1 = 3.713 < 5\n$$\n$$\nVIF_2 = 3.604 < 5\n$$\n$$\nVIF_3 = 1.099 < 5\n$$\nTherefore, based on this specific criterion, none of the features would be considered candidates for removal due to excessive multicollinearity. However, it is noteworthy that predictors $x_1$ and $x_2$ exhibit moderately high $VIF$s, which is a direct consequence of their strong correlation ($r=0.85$). While not exceeding the threshold of $5$, this level of collinearity could still have implications for the stability and interpretability of the model coefficients. The predictor $x_3$ shows very low collinearity with the other two predictors, as reflected by its $VIF$ being very close to $1$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3.713 & 3.604 & 1.099\n\\end{pmatrix}\n}\n$$", "id": "4549628"}, {"introduction": "To build a model that generalizes well to new, unseen data, one must not only train it but also tune its hyperparameters and honestly assess its performance. This practice guides you through designing a nested cross-validation protocol, the gold-standard methodology for obtaining an unbiased estimate of your model's predictive power. Properly implementing this procedure is crucial for preventing the common pitfall of overly optimistic results and for producing credible scientific findings [@problem_id:4549456].", "problem": "A radiomics study aims to classify lung nodules on computed tomography using an $\\ell_2$-regularized logistic regression model. Let the dataset consist of $N=240$ patients, with $N_+=96$ malignant and $N_-=144$ benign nodules (approximately $40\\%$ positive). Each patient has $d=120$ quantitative radiomic features. Logistic regression models the conditional probability $P(y=1\\mid x)$ via the logistic (sigmoid) function $\\sigma(z)$, where $\\sigma(z)=\\frac{1}{1+e^{-z}}$, and $z=\\beta_0+x^\\top\\beta$. The $\\ell_2$-regularized fit penalizes large coefficients by adding a term $\\lambda\\lVert\\beta\\rVert_2^2$, where $\\lambda>0$ is the regularization strength to be selected.\n\nYou are tasked with designing a scientifically sound nested cross-validation (CV) protocol to select the hyperparameter $\\lambda$ from a candidate grid $\\Lambda=\\{10^{-3},10^{-2},10^{-1},1,10\\}$ (thus $M=5$ candidates) and to estimate generalization performance using Area Under the Receiver Operating Characteristic (AUROC). The design must satisfy all of the following:\n\n- Use an outer stratified $K$-fold CV for unbiased performance estimation, with $K=5$.\n- Use an inner stratified $L$-fold CV for hyperparameter selection within each outer training split, with $L=3$.\n- In every training/validation split, perform z-score standardization of features using statistics $\\mu_j$ and $\\sigma_j$ estimated on the split’s training portion only, i.e., for feature $j$, transform as $\\tilde{x}_j=(x_j-\\mu_j)/\\sigma_j$, and apply the same transformation to the corresponding validation or test portion; do not compute $\\mu_j$ or $\\sigma_j$ on any data outside the current training partition.\n- Select $\\lambda\\in\\Lambda$ based on inner-fold validation AUROC averaged across $L$ inner folds, and estimate performance only on the held-out outer test fold with the model refit on the full outer training data using the selected $\\lambda$.\n- Maintain approximately the $40\\%$ positive rate via stratification in both inner and outer folds.\n\nWhich option correctly specifies a valid nested CV scheme meeting all criteria and states the total number of model fits required under this scheme?\n\nA. Use outer stratified $K=5$ folds. For each outer split, run inner stratified $L=3$-fold CV on the outer training data only. In each inner training fold, fit the z-score standardization using that inner training data and apply it to the inner validation data; train the logistic regression with each $\\lambda\\in\\Lambda$, compute validation AUROC, and average across $L$ folds to select the best $\\lambda$. Then refit the z-score standardization on the entire outer training data, retrain the logistic regression with the selected $\\lambda$ on the outer training data, and evaluate AUROC on the held-out outer test fold. Aggregate AUROC across the $K$ outer folds. This requires a total of $K\\cdot(L\\cdot M+1)=5\\cdot(3\\cdot 5+1)=5\\cdot 16=80$ model fits.\n\nB. Compute z-score standardization using all $N=240$ patients once. Use outer $K=5$ folds, but for each outer split choose $\\lambda$ by evaluating AUROC on the outer test fold (no inner CV). Retrain the model on the combined outer training and test data and report AUROC on the same test fold. This requires $K\\cdot M=5\\cdot 5=25$ model fits.\n\nC. Use a single stratified $K=5$-fold CV. For each $\\lambda\\in\\Lambda$, train on each of the $K$ training folds and evaluate AUROC on each corresponding test fold, then choose the $\\lambda$ maximizing the mean AUROC across the $K$ test folds. Report that same mean AUROC as the final performance. This requires $K\\cdot M=5\\cdot 5=25$ model fits.\n\nD. Use outer stratified $K=5$ folds and inner non-stratified $L=3$ folds. In the inner loop, for each $\\lambda\\in\\Lambda$, select the $\\lambda$ minimizing the inner training loss (not validation AUROC). Report performance as the mean AUROC across the inner validation folds only; do not refit on the full outer training fold or evaluate on the held-out outer test fold. This requires $K\\cdot L\\cdot M=5\\cdot 3\\cdot 5=75$ model fits.\n\nSelect the single best option that correctly constructs the nested CV scheme and states the correct total number of model fits under that scheme.", "solution": "The problem statement is valid. It is scientifically grounded in best-practice machine learning methodology for model evaluation and hyperparameter tuning. The problem is well-posed, providing all necessary details to identify the correct protocol among the given options.\n\nThe task is to design a nested cross-validation protocol to achieve two distinct goals: 1) unbiased estimation of the final model's generalization performance, and 2) selection of the optimal hyperparameter $\\lambda$ from a grid $\\Lambda$. A nested design is essential to prevent \"information leakage\" from the test set into the hyperparameter tuning process, which would result in an overly optimistic and biased performance estimate.\n\nThe correct protocol proceeds as follows:\n\n1.  **Outer Loop (Performance Estimation):** The full dataset ($N=240$) is split into $K=5$ stratified folds. This loop runs $K$ times. In each iteration $k \\in \\{1, ..., K\\}$, one fold serves as the outer test set (e.g., `Test_k`), and the remaining $K-1$ folds form the outer training set (`Train_k`). The sole purpose of `Test_k` is to evaluate the single best model produced by the inner loop for this outer fold.\n\n2.  **Inner Loop (Hyperparameter Selection):** For each outer training set `Train_k`, a separate, independent $L=3$-fold stratified cross-validation is performed.\n    *   `Train_k` is split into $L=3$ inner folds.\n    *   This inner loop runs $L$ times. In each inner iteration $l \\in \\{1, ..., L\\}$, one fold is the inner validation set (`Val_l`), and the remaining $L-1$ folds form the inner training set (`Train_l`).\n    *   **Crucially, feature standardization (z-scoring) must be done here:** The mean and standard deviation are calculated **only** on `Train_l` and then applied to both `Train_l` and `Val_l`.\n    *   For each candidate hyperparameter $\\lambda \\in \\Lambda$ (where $M=5$), a model is trained on the standardized `Train_l` and its AUROC is evaluated on the standardized `Val_l`.\n    *   After the inner loop completes, for each $\\lambda$, we average its validation AUROC across the $L=3$ folds. The hyperparameter $\\lambda^*$ that yields the highest average inner-loop AUROC is selected as the best for the outer fold $k$.\n\n3.  **Final Evaluation for Outer Fold:**\n    *   The model is retrained on the **entire** outer training set `Train_k` using the selected hyperparameter $\\lambda^*$.\n    *   **Crucially again, feature standardization is re-calculated:** The mean and standard deviation are computed on the full `Train_k` and applied to both `Train_k` and the outer test set `Test_k`.\n    *   The performance (AUROC) of this final model for fold $k$ is measured on the unseen outer test set `Test_k`.\n\n4.  **Overall Performance:** The final generalization performance is the average of the AUROCs obtained from the $K=5$ outer test sets.\n\n**Calculation of Total Model Fits:**\n*   In the inner loop for a single outer fold, we have $L=3$ splits. For each split, we train a model for each of the $M=5$ values of $\\lambda$. This gives $L \\times M = 3 \\times 5 = 15$ model fits.\n*   After selecting the best $\\lambda^*$, we retrain one final model on the full outer training set. This is 1 additional fit.\n*   Total fits per outer fold = $(L \\times M) + 1 = 15 + 1 = 16$.\n*   Since there are $K=5$ outer folds, the total number of model fits is $K \\times ((L \\times M) + 1) = 5 \\times 16 = 80$.\n\n**Analysis of Options:**\n*   **A:** This option correctly describes the entire nested CV procedure, including the critical points of stratified folds, isolating the outer test set, performing hyperparameter tuning only on the outer training data, and correctly handling data standardization at each step to prevent information leakage. It also correctly calculates the total number of model fits as 80.\n*   **B:** This option is fundamentally flawed. It performs standardization on the entire dataset before splitting, which is a major form of data leakage. It also uses the outer test fold for hyperparameter selection, which leads to a biased performance estimate.\n*   **C:** This option describes a simple (non-nested) cross-validation used for both hyperparameter tuning and performance estimation. It reports the performance on the same data used to select the best model, which is a methodological error that results in an optimistically biased performance metric.\n*   **D:** This option is incorrect on multiple counts. It suggests a non-stratified inner loop, which is suboptimal for an imbalanced dataset. It uses the incorrect criterion (training loss) for hyperparameter selection, which would favor overfitting. It incorrectly states that the final performance is reported from the inner validation folds, bypassing the outer loop's purpose entirely. The fit count is also incorrect as it omits the final refitting step.\n\nTherefore, option A is the only one that correctly outlines a valid nested cross-validation scheme and computes the required number of model fits accurately.", "answer": "$$\\boxed{A}$$", "id": "4549456"}]}