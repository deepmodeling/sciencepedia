{"hands_on_practices": [{"introduction": "The best way to understand how a machine learning model works is to build it from scratch. This first exercise guides you through the implementation of a linear Support Vector Machine (SVM) directly from its foundational principles [@problem_id:2435452]. You will translate the theoretical concept of regularized empirical risk minimization into a working classifier using subgradient descent, gaining a deep, practical understanding of how an SVM learns to separate data by optimizing a convex objective function.", "problem": "You are asked to build, from first principles of empirical risk minimization, a linear binary classifier that predicts whether a household will default on its student loans using a Support Vector Machine (SVM). The desired classifier must be a linear soft-margin Support Vector Machine (SVM) with an intercept, trained by subgradient descent on a convex margin-based loss with Tikhonov regularization, and must operate on standardized features. The goal is to translate the economic-financial classification task into a well-posed optimization problem and then into an algorithm that can be implemented as a runnable program.\n\nStart from the following fundamental base:\n- Empirical risk minimization over a linear decision function with an intercept, mapping a feature vector $x \\in \\mathbb{R}^d$ to a signed score.\n- A convex margin-based loss that penalizes violations of a desired margin, together with Tikhonov (ridge) regularization on the weight vector to control complexity and improve generalization.\n- Subgradient descent as a principled algorithm for minimizing a convex, possibly non-differentiable objective.\n\nImplement a program that:\n- Trains a linear soft-margin Support Vector Machine (SVM) classifier on each provided training set.\n- Standardizes features by a per-feature $z$-score, using only the training data: for each feature, subtract the training mean and divide by the training standard deviation (use $1$ for any zero standard deviation to avoid division by zero).\n- Uses subgradient descent to minimize the regularized empirical risk under a hinge-type margin-based loss, with user-specified learning-rate schedule and iteration count.\n- After training on each case, predicts the class label for a specified test household by the sign of the learned linear score. Use labels in $\\{-1,+1\\}$, where $+1$ denotes default and $-1$ denotes non-default.\n\nTest suite. There are three cases. In each case, the input consists of a training matrix $X^{(k)}$ with rows as households and columns as features, a label vector $y^{(k)} \\in \\{-1,+1\\}^{n_k}$, a test vector $x_\\star^{(k)}$, and hyperparameters regularization $C$, iteration count $T$, base step size $\\eta_0$, and step-decay $\\lambda$ for the learning rate schedule $\\eta_t = \\eta_0/(1+\\lambda t)$.\n\nCase $1$ (separable, mixed educational/financial/demographic features):\n- Features (columns) are: years of education, household income in thousands of dollars, student loan balance in thousands of dollars, credit utilization ratio (unitless fraction), age in years.\n- Training matrix $X^{(1)}$ (shape $6 \\times 5$):\n  [\n  [$16$, $120$, $10$, $0.1$, $30$],\n  [$16$, $40$, $50$, $0.8$, $24$],\n  [$14$, $60$, $20$, $0.3$, $28$],\n  [$12$, $35$, $45$, $0.9$, $22$],\n  [$18$, $150$, $20$, $0.1$, $33$],\n  [$12$, $25$, $60$, $0.95$, $21$]\n  ]\n- Labels $y^{(1)}$: [$-1$, $+1$, $-1$, $+1$, $-1$, $+1$]\n- Test vector $x_\\star^{(1)}$: [$15$, $55$, $35$, $0.6$, $25$]\n- Hyperparameters: $C = 10.0$, $T = 2000$, $\\eta_0 = 0.5$, $\\lambda = 10^{-3}$.\n\nCase $2$ (non-separable, overlapping financial/educational indicators):\n- Features (columns) are: household income in thousands of dollars, student loan balance in thousands of dollars, grade point average (unitless), graduate degree indicator (binary $0$ or $1$).\n- Training matrix $X^{(2)}$ (shape $8 \\times 4$):\n  [\n  [$70$, $30$, $3.0$, $0$],\n  [$50$, $45$, $2.6$, $0$],\n  [$65$, $40$, $3.2$, $1$],\n  [$55$, $50$, $3.0$, $0$],\n  [$80$, $20$, $3.8$, $1$],\n  [$60$, $35$, $2.8$, $0$],\n  [$75$, $25$, $3.1$, $1$],\n  [$58$, $42$, $3.4$, $0$]\n  ]\n- Labels $y^{(2)}$: [$-1$, $+1$, $-1$, $+1$, $-1$, $+1$, $-1$, $+1$]\n- Test vector $x_\\star^{(2)}$: [$62$, $38$, $3.0$, $0$]\n- Hyperparameters: $C = 5.0$, $T = 2500$, $\\eta_0 = 0.4$, $\\lambda = 5 \\times 10^{-4}$.\n\nCase $3$ (feature scaling edge case, heterogeneous financial/demographic scales):\n- Features (columns) are: household income in dollars, age in years, number of dependents, FICO credit score, months since graduation.\n- Training matrix $X^{(3)}$ (shape $6 \\times 5$):\n  [\n  [$120000$, $34$, $0$, $780$, $12$],\n  [$45000$, $23$, $1$, $620$, $6$],\n  [$80000$, $29$, $2$, $700$, $24$],\n  [$35000$, $25$, $0$, $600$, $3$],\n  [$150000$, $40$, $3$, $800$, $60$],\n  [$40000$, $27$, $2$, $580$, $8$]\n  ]\n- Labels $y^{(3)}$: [$-1$, $+1$, $-1$, $+1$, $-1$, $+1$]\n- Test vector $x_\\star^{(3)}$: [$65000$, $26$, $1$, $680$, $10$]\n- Hyperparameters: $C = 8.0$, $T = 3000$, $\\eta_0 = 0.3$, $\\lambda = 10^{-4}$.\n\nAlgorithmic requirements:\n- Implement subgradient descent on the regularized empirical risk with hinge-type margin-based loss, updating both the weight vector and the intercept.\n- Use the specified learning rate schedule $\\eta_t = \\eta_0/(1+\\lambda t)$ for iteration $t \\in \\{0,1,\\dots,T-1\\}$.\n- Initialize the weight vector and intercept to zero.\n- Standardize features using only the training data, and apply the same parameters to the test vector.\n\nFinal output format:\n- For each case $k \\in \\{1,2,3\\}$, output the predicted label $\\hat{y}_\\star^{(k)} \\in \\{-1,+1\\}$ for the corresponding test household.\n- Your program should produce a single line of output containing these three predictions as a comma-separated list enclosed in square brackets, e.g., \"[$\\hat{y}_\\star^{(1)}$,$\\hat{y}_\\star^{(2)}$,$\\hat{y}_\\star^{(3)}$]\". The printed output must be exactly one line, without additional text.", "solution": "The user requests the design and implementation of a linear soft-margin Support Vector Machine (SVM) classifier from first principles. The problem is framed in the context of computational economics and finance, specifically for predicting student loan defaults. My analysis proceeds in two stages: first, a rigorous validation of the problem statement, and second, the development of a solution founded on established scientific principles.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Model Type**: Linear soft-margin Support Vector Machine with an intercept.\n- **Task**: Binary classification of household student loan default ($y \\in \\{-1, +1\\}$).\n- **Theoretical Basis**: Empirical Risk Minimization (ERM) on a linear decision function.\n- **Decision Function**: $f(x) = w^\\top x + b$, mapping features $x \\in \\mathbb{R}^d$ to a score.\n- **Objective Function Components**: Tikhonov (ridge) regularization on the weight vector $w$, and a convex margin-based loss (hinge loss).\n- **Optimization Algorithm**: Subgradient descent.\n- **Preprocessing**: Per-feature $z$-score standardization based on training data. Standard deviation of zero is handled by dividing by $1$.\n- **Initialization**: Weight vector $w$ and intercept $b$ are initialized to zero.\n- **Learning Rate Schedule**: $\\eta_t = \\eta_0 / (1 + \\lambda t)$ for iteration $t \\in \\{0, 1, \\dots, T-1\\}$.\n- **Prediction Rule**: $\\hat{y}_\\star = \\text{sign}(w^\\top x_\\star + b)$. The label is $+1$ for default, $-1$ for non-default.\n- **Test Cases**:\n    - **Case 1**:\n        - $X^{(1)}$ (training data, $6 \\times 5$ matrix), $y^{(1)}$ (training labels, length $6$), $x_\\star^{(1)}$ (test vector, length $5$).\n        - Hyperparameters: $C = 10.0$, $T = 2000$, $\\eta_0 = 0.5$, $\\lambda = 10^{-3}$.\n    - **Case 2**:\n        - $X^{(2)}$ (training data, $8 \\times 4$ matrix), $y^{(2)}$ (training labels, length $8$), $x_\\star^{(2)}$ (test vector, length $4$).\n        - Hyperparameters: $C = 5.0$, $T = 2500$, $\\eta_0 = 0.4$, $\\lambda = 5 \\times 10^{-4}$.\n    - **Case 3**:\n        - $X^{(3)}$ (training data, $6 \\times 5$ matrix), $y^{(3)}$ (training labels, length $6$), $x_\\star^{(3)}$ (test vector, length $5$).\n        - Hyperparameters: $C = 8.0$, $T = 3000$, $\\eta_0 = 0.3$, $\\lambda = 10^{-4}$.\n- **Output Format**: A single line printing a comma-separated list of the three predicted labels, enclosed in square brackets.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is subjected to rigorous scientific and structural validation.\n\n- **Scientifically Grounded**: The problem is firmly rooted in the established principles of statistical learning theory and convex optimization. Empirical risk minimization, Tikhonov regularization, hinge loss, and subgradient descent are canonical components of modern machine learning. Their application to credit risk modeling is a standard and well-understood task in computational finance. The problem adheres to fundamental mathematical and statistical logic.\n- **Well-Posed**: The problem is well-posed. The objective function, being a sum of a strictly convex function ($\\frac{1}{2}\\|w\\|^2$) and a convex function (sum of hinge losses), is strictly convex. This guarantees a unique minimizer $(w^*, b^*)$. The subgradient descent algorithm, with a learning rate schedule that satisfies the conditions $\\sum_{t=0}^{\\infty} \\eta_t = \\infty$ and $\\sum_{t=0}^{\\infty} \\eta_t^2 < \\infty$ (which the specified schedule $\\eta_t \\propto 1/t$ does), is guaranteed to converge to this unique minimum.\n- **Objective**: The problem is stated in precise, unbiased, and formal mathematical language, devoid of any subjective or speculative content.\n- **Completeness and Consistency**: All necessary components are provided: datasets, labels, hyperparameters, algorithmic specifications (initialization, learning rate), and preprocessing steps. There are no internal contradictions. The use of the hyperparameter $C$ is consistent with the standard primal formulation of the soft-margin SVM.\n- **Realism**: The provided datasets, while small, represent plausible economic and demographic features of households. The heterogeneity in scales (e.g., Case 3: income in dollars vs. number of dependents) correctly motivates the requirement for feature standardization.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. It is scientifically sound, well-posed, objective, complete, and internally consistent. It represents a standard, non-trivial problem in machine learning. I will now proceed with a principled solution.\n\n### Solution Derivation\n\nThe goal is to derive and implement a linear soft-margin SVM classifier from first principles.\n\n**1. Model and Objective Function**\n\nWe begin with the principle of regularized empirical risk minimization. The classifier is a linear function of the input features $x \\in \\mathbb{R}^d$:\n$$\nf(x; w, b) = w^\\top x + b\n$$\nwhere $w \\in \\mathbb{R}^d$ is the weight vector and $b \\in \\mathbb{R}$ is the intercept. The predicted class is given by $\\hat{y} = \\text{sign}(f(x; w, b))$. We adopt the convention that $\\text{sign}(z) = +1$ for $z \\ge 0$ and $-1$ for $z < 0$.\n\nThe objective is to find the parameters $(w, b)$ that minimize a combination of empirical loss on the training data $\\{(x_i, y_i)\\}_{i=1}^n$ and a regularization term that controls model complexity. For SVMs, the standard choice is the hinge loss, which penalizes predictions that fail to achieve a margin of at least $1$:\n$$\nL_{\\text{hinge}}(y_i, f(x_i)) = \\max(0, 1 - y_i f(x_i)) = \\max(0, 1 - y_i(w^\\top x_i + b))\n$$\nThis loss is a convex upper bound on the non-convex $0$-$1$ classification error.\n\nTo prevent overfitting and improve generalization, we add a Tikhonov ($\\ell_2$) regularization penalty on the weight vector, $\\frac{1}{2}\\|w\\|^2 = \\frac{1}{2} w^\\top w$. The intercept $b$ is typically left unregularized.\n\nCombining these components, we arrive at the primal objective function for the soft-margin SVM:\n$$\nJ(w, b) = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\max(0, 1 - y_i(w^\\top x_i + b))\n$$\nThe hyperparameter $C > 0$ controls the trade-off: a small $C$ favors a larger margin at the cost of some margin violations, while a large $C$ penalizes violations more heavily, leading to a smaller margin. Minimizing this objective function is a convex optimization problem.\n\n**2. Optimization by Subgradient Descent**\n\nThe hinge loss term makes the objective function $J(w, b)$ non-differentiable wherever $y_i(w^\\top x_i + b) = 1$. Therefore, we use subgradient descent. A subgradient is a generalization of the gradient for non-differentiable convex functions.\n\nThe subgradient of $J(w, b)$ with respect to $w$ and $b$ is the sum of the gradient of the differentiable regularizer and the subgradients of the non-differentiable hinge loss terms. For a single data point $(x_i, y_i)$, the subgradient of the hinge loss is:\n$$\n\\partial \\max(0, 1 - y_i(w^\\top x_i + b)) =\n\\begin{cases}\n    \\{ \\mathbf{0} \\} & \\text{if } y_i(w^\\top x_i + b) > 1 \\\\\n    \\{ -y_i \\begin{pmatrix} x_i \\\\ 1 \\end{pmatrix} \\} & \\text{if } y_i(w^\\top x_i + b) < 1 \\\\\n    \\{ -\\alpha y_i \\begin{pmatrix} x_i \\\\ 1 \\end{pmatrix} \\mid \\alpha \\in [0, 1] \\} & \\text{if } y_i(w^\\top x_i + b) = 1\n\\end{cases}\n$$\nFor algorithmic implementation, we can select any element from the subgradient set. A conventional choice is to take the subgradient as $-y_i \\begin{pmatrix} x_i \\\\ 1 \\end{pmatrix}$ if $y_i(w^\\top x_i + b) \\le 1$ and $\\mathbf{0}$ otherwise. However, the problem statement only provides strict inequality in the condition, which we will follow. Let $\\mathcal{S}_t = \\{i \\mid y_i(w^{(t)\\top} x_i + b^{(t)}) < 1\\}$ be the set of indices of training samples that violate the margin condition at iteration $t$.\n\nThe subgradients of the full objective function $J(w, b)$ are:\n$$\ng_w = \\frac{\\partial J}{\\partial w} = w - C \\sum_{i \\in \\mathcal{S}_t} y_i x_i\n$$\n$$\ng_b = \\frac{\\partial J}{\\partial b} = -C \\sum_{i \\in \\mathcal{S}_t} y_i\n$$\nThe subgradient descent update rules for iteration $t$ are:\n$$\nw^{(t+1)} \\leftarrow w^{(t)} - \\eta_t g_w\n$$\n$$\nb^{(t+1)} \\leftarrow b^{(t)} - \\eta_t g_b\n$$\nThe learning rate $\\eta_t$ is given by the schedule $\\eta_t = \\eta_0 / (1 + \\lambda t)$.\n\n**3. Feature Standardization**\n\nThe features provided have vastly different scales (e.g., income in thousands of dollars versus a unitless ratio). SVMs are sensitive to feature scales because the objective function involves dot products and norms. To ensure that all features contribute appropriately, we standardize them using a $z$-score transformation. For each feature $j \\in \\{1, \\dots, d\\}$, we compute its mean $\\mu_j$ and standard deviation $\\sigma_j$ from the training data.\n$$\n\\mu_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij} \\quad , \\quad \\sigma_j^2 = \\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\mu_j)^2\n$$\nThe standardized feature $x'_{ij}$ is then:\n$$\nx'_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j'}\n$$\nwhere $\\sigma_j' = \\sigma_j$ if $\\sigma_j > 0$, and $\\sigma_j' = 1$ if $\\sigma_j = 0$. The same transformation, using the parameters $(\\mu_j, \\sigma_j')$ derived from the training set, must be applied to the test vector $x_\\star$ before prediction.\n\nThis procedure yields a principled and complete algorithm for training the SVM classifier and making predictions, which we now implement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the SVM classification problem for all test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"X_train\": np.array([\n                [16.0, 120.0, 10.0, 0.1, 30.0],\n                [16.0, 40.0, 50.0, 0.8, 24.0],\n                [14.0, 60.0, 20.0, 0.3, 28.0],\n                [12.0, 35.0, 45.0, 0.9, 22.0],\n                [18.0, 150.0, 20.0, 0.1, 33.0],\n                [12.0, 25.0, 60.0, 0.95, 21.0]\n            ]),\n            \"y_train\": np.array([-1, 1, -1, 1, -1, 1]),\n            \"x_test\": np.array([15.0, 55.0, 35.0, 0.6, 25.0]),\n            \"C\": 10.0,\n            \"T\": 2000,\n            \"eta0\": 0.5,\n            \"lmbda\": 1e-3\n        },\n        {\n            \"X_train\": np.array([\n                [70.0, 30.0, 3.0, 0.0],\n                [50.0, 45.0, 2.6, 0.0],\n                [65.0, 40.0, 3.2, 1.0],\n                [55.0, 50.0, 3.0, 0.0],\n                [80.0, 20.0, 3.8, 1.0],\n                [60.0, 35.0, 2.8, 0.0],\n                [75.0, 25.0, 3.1, 1.0],\n                [58.0, 42.0, 3.4, 0.0]\n            ]),\n            \"y_train\": np.array([-1, 1, -1, 1, -1, 1, -1, 1]),\n            \"x_test\": np.array([62.0, 38.0, 3.0, 0.0]),\n            \"C\": 5.0,\n            \"T\": 2500,\n            \"eta0\": 0.4,\n            \"lmbda\": 5e-4\n        },\n        {\n            \"X_train\": np.array([\n                [120000.0, 34.0, 0.0, 780.0, 12.0],\n                [45000.0, 23.0, 1.0, 620.0, 6.0],\n                [80000.0, 29.0, 2.0, 700.0, 24.0],\n                [35000.0, 25.0, 0.0, 600.0, 3.0],\n                [150000.0, 40.0, 3.0, 800.0, 60.0],\n                [40000.0, 27.0, 2.0, 580.0, 8.0]\n            ]),\n            \"y_train\": np.array([-1, 1, -1, 1, -1, 1]),\n            \"x_test\": np.array([65000.0, 26.0, 1.0, 680.0, 10.0]),\n            \"C\": 8.0,\n            \"T\": 3000,\n            \"eta0\": 0.3,\n            \"lmbda\": 1e-4\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X_train, y_train, x_test = case[\"X_train\"], case[\"y_train\"], case[\"x_test\"]\n        C, T, eta0, lmbda = case[\"C\"], case[\"T\"], case[\"eta0\"], case[\"lmbda\"]\n\n        # 1. Feature Standardization\n        # Using ddof=0 for population standard deviation, consistent with the formula.\n        train_mean = np.mean(X_train, axis=0)\n        train_std = np.std(X_train, axis=0, ddof=0)\n        train_std[train_std == 0] = 1.0  # Avoid division by zero\n\n        X_train_std = (X_train - train_mean) / train_std\n        x_test_std = (x_test - train_mean) / train_std\n\n        n_samples, n_features = X_train.shape\n\n        # 2. Subgradient Descent for SVM\n        # Initialize parameters\n        w = np.zeros(n_features)\n        b = 0.0\n\n        for t in range(T):\n            # Calculate learning rate\n            eta_t = eta0 / (1.0 + lmbda * t)\n\n            # Calculate scores and identify margin violators\n            scores = y_train * (X_train_std @ w + b)\n            violators_mask = scores < 1.0\n            \n            # Compute subgradients\n            # Subgradient for w\n            # The sum is over samples that violate the margin condition\n            if np.any(violators_mask):\n                dw = w - C * np.sum(y_train[violators_mask, np.newaxis] * X_train_std[violators_mask], axis=0)\n                db = -C * np.sum(y_train[violators_mask])\n            else:\n                dw = w\n                db = 0.0\n\n            # Update parameters\n            w -= eta_t * dw\n            b -= eta_t * db\n\n        # 3. Prediction\n        test_score = x_test_std @ w + b\n        prediction = 1 if test_score >= 0 else -1\n        results.append(prediction)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2435452"}, {"introduction": "While subgradient descent on the primal problem is intuitive, many powerful SVM solvers tackle the dual formulation, a path that directly enables the famous \"kernel trick.\" This next practice challenges you to implement a more advanced solver—an Interior Point Method (IPM)—to solve the SVM dual, which is a type of Quadratic Program (QP) [@problem_id:3242644]. This exercise will deepen your understanding of convex optimization and reveal the elegant mathematical structure that makes SVMs so versatile.", "problem": "You are asked to implement a complete, runnable program that constructs and solves a family of Quadratic Programming (QP) problems using an Interior Point Method (IPM) and applies it to the dual of a classic Support Vector Machine (SVM) binary classification problem with a linear kernel. Your implementation must be self-contained and produce a single line of output as specified below.\n\nThe core optimization problem to solve is the dual of a soft-margin linear-kernel SVM. Given training data matrix $X \\in \\mathbb{R}^{n \\times d}$ and labels $y \\in \\{-1,+1\\}^n$, the dual problem is to find $ \\alpha \\in \\mathbb{R}^n $ that minimizes the strictly convex quadratic objective subject to one equality constraint and box constraints. Define the Gram matrix $K \\in \\mathbb{R}^{n \\times n}$ by $K_{ij} = x_i^\\top x_j$, the label matrix $Y = \\operatorname{diag}(y)$, and the quadratic matrix $Q = Y K Y$. The dual QP is\n$$\n\\begin{aligned}\n\\min_{\\alpha \\in \\mathbb{R}^n}\\quad & \\tfrac{1}{2}\\,\\alpha^\\top Q\\,\\alpha - \\mathbf{1}^\\top \\alpha \\\\\n\\text{subject to}\\quad & y^\\top \\alpha = 0, \\\\\n& 0 \\le \\alpha \\le C,\n\\end{aligned}\n$$\nwhere $C \\in \\mathbb{R}$ is a given penalty parameter with $C > 0$, and $\\mathbf{1} \\in \\mathbb{R}^n$ is the vector of ones. Your solver must implement a barrier-based interior point method for equality-constrained QPs with simple bound constraints of the form $l \\le x \\le u$.\n\nStarting from fundamental definitions of convex optimization, Lagrangian optimality conditions, and the log-barrier approach for inequality constraints, implement a feasible-start barrier method that, for a barrier parameter $\\mu > 0$, minimizes the barrier-augmented objective\n$$\n\\phi_\\mu(\\alpha) = \\tfrac{1}{2}\\,\\alpha^\\top Q\\,\\alpha - \\mathbf{1}^\\top \\alpha - \\mu \\sum_{i=1}^n \\log(\\alpha_i - 0) - \\mu \\sum_{i=1}^n \\log(C - \\alpha_i)\n$$\nsubject to the equality constraint $y^\\top \\alpha = 0$. Use Newton’s method with equality constraints to compute search directions by solving the Karush–Kuhn–Tucker (KKT) system for the Newton step at each barrier parameter value, and a backtracking line search that preserves strict feasibility $0 < \\alpha_i < C$ for all $i \\in \\{1,\\dots,n\\}$ while ensuring sufficient decrease of the barrier objective. Decrease $\\mu$ geometrically until convergence, and regularize as needed to maintain numerical stability. Your algorithm must be justified from first principles in your solution.\n\nAfter solving for $\\alpha$, recover the primal separating hyperplane parameters for the linear-kernel SVM as follows. Compute\n$$\nw = \\sum_{i=1}^n \\alpha_i y_i x_i \\in \\mathbb{R}^d,\n$$\nand estimate the bias $b \\in \\mathbb{R}$ using the standard complementary slackness relationship. If there exist indices $i$ such that $0 < \\alpha_i < C$ (strictly within bounds up to a small numerical tolerance), compute\n$$\nb = \\operatorname{mean}_{i \\,:\\, 0 < \\alpha_i < C} \\left( y_i - w^\\top x_i \\right).\n$$\nIf no such $i$ exists, use a robust fallback based on class margins:\n$$\nb = -\\tfrac{1}{2}\\left( \\max_{i \\,:\\, y_i=-1} w^\\top x_i + \\min_{i \\,:\\, y_i=+1} w^\\top x_i \\right).\n$$\n\nFor each test case below, compute the in-sample classification accuracy defined as the fraction of correctly classified training examples using the decision rule $\\hat{y} = \\operatorname{sign}(w^\\top x + b)$ with labels in $\\{-1,+1\\}$. Express each accuracy as a real number in $[0,1]$ rounded to six decimal places.\n\nTest suite. Use exactly the following three test cases:\n\n- Case A (linearly separable, balanced):\n  - $X = \\begin{bmatrix}\n  2 & 2 \\\\\n  2 & 3 \\\\\n  3 & 2 \\\\\n  -2 & -2 \\\\\n  -2 & -3 \\\\\n  -3 & -2\n  \\end{bmatrix}$,\n  $y = \\begin{bmatrix} +1 \\\\ +1 \\\\ +1 \\\\ -1 \\\\ -1 \\\\ -1 \\end{bmatrix}$,\n  $C = 100.0$.\n- Case B (partially overlapping, balanced):\n  - $X = \\begin{bmatrix}\n  2 & 2 \\\\\n  2 & 0 \\\\\n  3 & 1 \\\\\n  3 & 2 \\\\\n  0 & 0 \\\\\n  1 & 0 \\\\\n  0 & 1 \\\\\n  1 & 1.5\n  \\end{bmatrix}$,\n  $y = \\begin{bmatrix} +1 \\\\ +1 \\\\ +1 \\\\ +1 \\\\ -1 \\\\ -1 \\\\ -1 \\\\ -1 \\end{bmatrix}$,\n  $C = 1.0$.\n- Case C (linearly separable, imbalanced):\n  - $X = \\begin{bmatrix}\n  2 & 2 \\\\\n  2 & 3 \\\\\n  3 & 2 \\\\\n  -2 & -2\n  \\end{bmatrix}$,\n  $y = \\begin{bmatrix} +1 \\\\ +1 \\\\ +1 \\\\ -1 \\end{bmatrix}$,\n  $C = 10.0$.\n\nFinal output format. Your program should produce a single line of output containing the results for the three cases as a comma-separated list of real numbers in a single pair of square brackets, in the order A, B, C. Each number must be rounded to six decimal places. For example, the output must look like\n$$\n[\\text{acc}_A,\\text{acc}_B,\\text{acc}_C],\n$$\nwhere each of $\\text{acc}_A$, $\\text{acc}_B$, and $\\text{acc}_C$ is a real number formatted to six decimal places. No additional text or lines should be printed.\n\nUnits and angle units are not applicable to this problem.\n\nYour implementation must be complete and runnable as-is, without any user input, external files, or network access. The only allowed external library is Numerical Python (NumPy).", "solution": "We begin from fundamental definitions in convex optimization and the construction of Support Vector Machine (SVM) classifiers as solutions to convex programs. For binary classification with a linear kernel, the primal soft-margin SVM seeks a separating hyperplane defined by $w \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$ that balances margin maximization against hinge losses. The corresponding dual problem is a strictly convex Quadratic Program (QP) with one linear equality constraint and simple bound constraints. This dual is particularly convenient for kernel methods, but here we focus on the linear kernel.\n\nGiven $X \\in \\mathbb{R}^{n \\times d}$ and labels $y \\in \\{-1,+1\\}^n$, define the Gram matrix $K \\in \\mathbb{R}^{n \\times n}$ by $K_{ij} = x_i^\\top x_j$ and $Q = Y K Y$, where $Y = \\operatorname{diag}(y)$. The soft-margin dual with penalty parameter $C \\in \\mathbb{R}$, $C > 0$, is\n$$\n\\begin{aligned}\n\\min_{\\alpha \\in \\mathbb{R}^n} \\quad & f(\\alpha) = \\tfrac{1}{2} \\alpha^\\top Q \\alpha - \\mathbf{1}^\\top \\alpha \\\\\n\\text{subject to} \\quad & y^\\top \\alpha = 0, \\\\\n& 0 \\le \\alpha \\le C,\n\\end{aligned}\n$$\nwith $\\mathbf{1} \\in \\mathbb{R}^n$ the all-ones vector. The decision function in the primal variables is recovered as $w = \\sum_{i=1}^n \\alpha_i y_i x_i$ and a bias $b$ consistent with the Karush–Kuhn–Tucker (KKT) conditions.\n\nTo solve the QP, we use an interior point method (IPM) based on the logarithmic barrier. The inequality constraints $0 < \\alpha_i < C$ are enforced by augmenting the objective with a barrier term parameterized by $\\mu > 0$:\n$$\n\\phi_\\mu(\\alpha) = f(\\alpha) - \\mu \\sum_{i=1}^n \\log(\\alpha_i) - \\mu \\sum_{i=1}^n \\log(C - \\alpha_i),\n$$\nsubject to the equality constraint $y^\\top \\alpha = 0$. For each fixed $\\mu$, we solve the equality-constrained minimization of $\\phi_\\mu(\\alpha)$ using Newton’s method on the manifold $y^\\top \\alpha = 0$. As $\\mu \\to 0^+$, solutions trace the central path and converge to the constrained minimizer of the original QP.\n\nWe derive the Newton step using first principles. The gradient of the barrier-augmented objective is\n$$\n\\nabla \\phi_\\mu(\\alpha) = Q \\alpha - \\mathbf{1} - \\mu \\left[ \\frac{1}{\\alpha_1}, \\dots, \\frac{1}{\\alpha_n} \\right]^\\top + \\mu \\left[ \\frac{1}{C - \\alpha_1}, \\dots, \\frac{1}{C - \\alpha_n} \\right]^\\top.\n$$\nThe Hessian is\n$$\n\\nabla^2 \\phi_\\mu(\\alpha) = Q + \\operatorname{diag}\\left( \\frac{\\mu}{\\alpha_1^2} + \\frac{\\mu}{(C - \\alpha_1)^2}, \\dots, \\frac{\\mu}{\\alpha_n^2} + \\frac{\\mu}{(C - \\alpha_n)^2} \\right).\n$$\nWe enforce the equality constraint via Lagrange multipliers. For the current iterate $\\alpha$ and Lagrange multiplier $\\lambda \\in \\mathbb{R}$, the Newton step $(\\Delta \\alpha, \\Delta \\lambda)$ solves the linearized KKT system\n$$\n\\begin{bmatrix}\nW & y \\\\\ny^\\top & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta \\alpha \\\\\n\\Delta \\lambda\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\ng \\\\\nr_p\n\\end{bmatrix},\n$$\nwhere $g = \\nabla \\phi_\\mu(\\alpha)$, $W = \\nabla^2 \\phi_\\mu(\\alpha)$, and $r_p = y^\\top \\alpha - 0$ is the primal equality residual. This system emerges from second-order Taylor expansion of $\\phi_\\mu(\\alpha)$ and linearization of the equality constraint. It can be solved efficiently by the Schur complement. Solving $W \\Delta \\alpha + y \\Delta \\lambda = -g$ and $y^\\top \\Delta \\alpha = -r_p$, we obtain the Schur complement equation\n$$\n(y^\\top W^{-1} y)\\, \\Delta \\lambda = -\\left( r_p + y^\\top W^{-1} g \\right),\n$$\nand then\n$$\n\\Delta \\alpha = - W^{-1} (g + y \\Delta \\lambda).\n$$\nBecause $W$ is strictly positive definite for $\\mu > 0$ due to the barrier curvature (even if $Q$ is only positive semidefinite), the linear solves exist and are numerically stable with mild regularization.\n\nThe step length is chosen to maintain strict feasibility of the inequality constraints and to ensure sufficient decrease in the barrier objective. Given a candidate direction $\\Delta \\alpha$, the maximal step preserving strict interior feasibility is computed componentwise as\n$$\n\\alpha_{\\max} = \\min \\left\\{ 1,\\, 0.99 \\cdot \\min_{i:\\,\\Delta \\alpha_i < 0} \\frac{\\alpha_i}{-\\Delta \\alpha_i},\\, 0.99 \\cdot \\min_{i:\\,\\Delta \\alpha_i > 0} \\frac{C - \\alpha_i}{\\Delta \\alpha_i} \\right\\}.\n$$\nWe then apply backtracking with factor $0 < \\beta < 1$ and Armijo condition using the directional derivative $g^\\top \\Delta \\alpha$ to guarantee decrease of $\\phi_\\mu(\\alpha)$ while preserving feasibility.\n\nInitialization must satisfy strict feasibility and the equality constraint. Let $P = \\{ i \\,:\\, y_i = +1 \\}$ and $N = \\{ i \\,:\\, y_i = -1 \\}$. Choose scalars $s, t \\in (0,1)$ such that $s\\,|P| = t\\,|N|$. One concrete choice that guarantees strict interiority regardless of class imbalance is\n$$\ns = \\min\\!\\left(0.5,\\, 0.5 \\cdot \\frac{|N|}{|P|}\\right), \\quad t = s \\cdot \\frac{|P|}{|N|}.\n$$\nThen set $\\alpha_i = C s$ for $i \\in P$ and $\\alpha_j = C t$ for $j \\in N$. This choice satisfies $0 < \\alpha_i < C$ for all $i$ and $y^\\top \\alpha = 0$.\n\nThe outer barrier loop decreases $\\mu$ geometrically, e.g., $\\mu \\leftarrow \\sigma \\mu$ with $\\sigma \\in (0,1)$, until $\\mu$ is sufficiently small; for each $\\mu$, the inner equality-constrained Newton method is iterated until the norm of the KKT residual $(g, r_p)$ is below a tolerance. Mild diagonal regularization of $Q$ can be used to improve numerical stability in small problems.\n\nAfter convergence, recover the primal parameters $w = \\sum_{i=1}^n \\alpha_i y_i x_i$. For $b$, use the KKT complementary slackness condition. If there exist indices $i$ strictly within the bounds $0 < \\alpha_i < C$ (up to a small tolerance), then for such $i$ we have $y_i (w^\\top x_i + b) \\approx 1$, implying $b \\approx y_i - w^\\top x_i$. Averaging over available such indices yields a robust estimate:\n$$\nb = \\operatorname{mean}_{i \\,:\\, \\varepsilon < \\alpha_i < C - \\varepsilon} \\left( y_i - w^\\top x_i \\right).\n$$\nIf no such indices exist, a robust fallback is to choose $b$ halfway between the closest projected margins of the two classes:\n$$\nb = -\\tfrac{1}{2}\\left( \\max_{i \\,:\\, y_i=-1} w^\\top x_i + \\min_{i \\,:\\, y_i=+1} w^\\top x_i \\right).\n$$\n\nFinally, compute the in-sample accuracy $\\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{ \\operatorname{sign}(w^\\top x_i + b) = y_i \\}$ for each test case. The program must output a single line: a list of three real numbers in $[0,1]$, each rounded to six decimal places, corresponding to the three specified test cases in order A, B, C.\n\nThis method integrates the core definitions of convexity, the KKT conditions, and the log-barrier principle into an implementable algorithm: equality-constrained Newton steps with a barrier-augmented Hessian ensure descent within the feasible interior, while Schur complement reduction leverages the structure of a small number of equality constraints. The chosen initialization guarantees strict feasibility and equality satisfaction from first principles, and the recovery of $(w,b)$ uses the complementary slackness relationship inherent in the SVM formulation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef interior_point_qp(H, c, A, b, l, u, *,\n                      mu_start=1.0, mu_tol=1e-8, mu_decay=0.2,\n                      newton_tol=1e-8, max_newton_iter=60,\n                      backtrack_beta=0.5, armijo_c1=1e-4, ridge=1e-10):\n    \"\"\"\n    Solve: minimize 0.5 x^T H x + c^T x\n           subject to A x = b, l < x < u\n    using a log-barrier interior point method with equality-constrained Newton steps.\n\n    Parameters:\n        H: (n,n) symmetric positive semidefinite\n        c: (n,)\n        A: (m,n) equality constraints\n        b: (m,)\n        l, u: (n,) lower and upper bounds with l < u elementwise\n    Returns:\n        x: (n,) approximate solution\n    \"\"\"\n    n = H.shape[0]\n    # Symmetrize H and add small ridge for numerical stability\n    H = 0.5 * (H + H.T) + ridge * np.eye(n)\n\n    # Build strictly feasible starting point satisfying A x = b and l < x < u\n    # Here A has shape (m, n). We'll construct x0 using a problem-specific strategy when m=1,\n    # otherwise project a central point to the equality manifold with line search for feasibility.\n    m = A.shape[0]\n    # Start from midpoint\n    x = 0.5 * (l + u)\n\n    # Try to adjust to equality feasibility while staying interior\n    # Solve min ||x - x_mid|| s.t. A x = b => x = x_mid + A^T (A A^T)^{-1} (b - A x_mid)\n    # Then shrink towards midpoint if needed to maintain strict interior.\n    try:\n        At = A\n        M = At @ At.T\n        # Regularize M if near singular\n        if M.ndim == 0:\n            M = np.array([[M + 1e-12]])\n        else:\n            M = M + 1e-12 * np.eye(M.shape[0])\n        adjustment = At.T @ np.linalg.solve(M, (b - A @ x))\n        x_candidate = x + adjustment\n        # Ensure strict interior via convex combination with midpoint\n        t = 0.0\n        # If x_candidate violates bounds, pull it towards midpoint\n        while True:\n            xc = (1 - t) * x_candidate + t * x  # t increases towards midpoint\n            if np.all(xc > l) and np.all(xc < u) and np.allclose(A @ xc, b, atol=1e-10):\n                x = xc\n                break\n            t = min(1.0, t + 0.05)\n            if t >= 1.0:\n                # Fallback: stay at midpoint and enforce equality numerically later\n                x = (x + 0.0)\n                break\n    except np.linalg.LinAlgError:\n        pass\n\n    # Ensure strict interiority\n    eps = 1e-6\n    x = np.minimum(np.maximum(x, l + eps * (u - l)), u - eps * (u - l))\n\n    mu = mu_start\n\n    def barrier_grad_hess(x, mu):\n        # gradient g = H x + c - mu/(x-l) + mu/(u-x)\n        inv_xl = 1.0 / (x - l)\n        inv_ux = 1.0 / (u - x)\n        g = H @ x + c - mu * inv_xl + mu * inv_ux\n        # Hessian W = H + diag(mu/(x-l)^2 + mu/(u-x)^2)\n        diag_terms = mu * (inv_xl**2 + inv_ux**2)\n        W = H + np.diag(diag_terms)\n        return g, W\n\n    def phi(x, mu):\n        # barrier objective\n        if np.any(x <= l) or np.any(x >= u):\n            return np.inf\n        return 0.5 * float(x.T @ H @ x) + float(c.T @ x) - mu * np.sum(np.log(x - l)) - mu * np.sum(np.log(u - x))\n\n    # Newton solve for each mu\n    while mu > mu_tol:\n        # Newton iterations\n        for _ in range(max_newton_iter):\n            g, W = barrier_grad_hess(x, mu)\n            r_p = A @ x - b  # primal residual\n            # Solve KKT system via Schur complement:\n            # W dx + A^T lam = -g\n            # A dx = -r_p\n            # Schur: (A W^{-1} A^T) lam = -(r_p + A W^{-1} g)\n            try:\n                Winv_A_T = np.linalg.solve(W, A.T)\n                S = A @ Winv_A_T  # (m,m)\n                rhs = -(r_p + A @ np.linalg.solve(W, g))\n                # Solve for lambda\n                lam = np.linalg.solve(S, rhs)\n                # Then dx\n                dx = -np.linalg.solve(W, g + A.T @ lam)\n            except np.linalg.LinAlgError:\n                # Add additional ridge and retry\n                W = W + 1e-8 * np.eye(n)\n                Winv_A_T = np.linalg.solve(W, A.T)\n                S = A @ Winv_A_T\n                rhs = -(r_p + A @ np.linalg.solve(W, g))\n                lam = np.linalg.solve(S, rhs)\n                dx = -np.linalg.solve(W, g + A.T @ lam)\n\n            # Check convergence of KKT residual\n            res_norm = np.sqrt(np.linalg.norm(g, 2)**2 + np.linalg.norm(r_p, 2)**2)\n            if res_norm < newton_tol:\n                break\n\n            # Compute maximum feasible step to remain strictly inside bounds\n            alpha = 1.0\n            with np.errstate(divide='ignore', invalid='ignore'):\n                idx_pos = dx > 0\n                if np.any(idx_pos):\n                    alpha = min(alpha, 0.99 * np.min((u[idx_pos] - x[idx_pos]) / dx[idx_pos]))\n                idx_neg = dx < 0\n                if np.any(idx_neg):\n                    alpha = min(alpha, 0.99 * np.min((x[idx_neg] - l[idx_neg]) / (-dx[idx_neg])))\n            if not np.isfinite(alpha) or alpha <= 0:\n                alpha = 0.5\n\n            # Backtracking line search for sufficient decrease\n            phi_x = phi(x, mu)\n            slope = g @ dx\n            t = alpha\n            while True:\n                x_new = x + t * dx\n                if np.any(x_new <= l) or np.any(x_new >= u):\n                    t *= backtrack_beta\n                else:\n                    phi_new = phi(x_new, mu)\n                    if phi_new <= phi_x + armijo_c1 * t * slope:\n                        break\n                    t *= backtrack_beta\n                if t < 1e-16:\n                    break\n            x = x + t * dx\n\n        # Reduce barrier parameter\n        mu *= mu_decay\n\n    return x\n\ndef svm_linear_dual_ipm(X, y, C):\n    \"\"\"\n    Train a soft-margin linear SVM via its dual QP using an interior point method.\n    Returns: (w, b, alpha)\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float)\n    n, d = X.shape\n\n    # Build Q = Y K Y with K = X X^T\n    K = X @ X.T\n    Y = np.diag(y)\n    Q = Y @ K @ Y\n\n    # c = -1, A = y^T, b = 0, l = 0, u = C\n    c = -np.ones(n)\n    A = y.reshape(1, -1)\n    b = np.array([0.0])\n    l = np.zeros(n)\n    u = C * np.ones(n)\n\n    # Strictly feasible initializer satisfying A x = b and l < x < u\n    # Construct x0 with class-balanced interior alphas\n    pos_idx = np.where(y > 0)[0]\n    neg_idx = np.where(y < 0)[0]\n    if len(pos_idx) == 0 or len(neg_idx) == 0:\n        raise ValueError(\"Both classes must be present.\")\n    s = min(0.5, 0.5 * len(neg_idx) / max(1, len(pos_idx)))\n    t = s * len(pos_idx) / len(neg_idx)\n    alpha0 = np.zeros(n)\n    alpha0[pos_idx] = C * s\n    alpha0[neg_idx] = C * t\n\n    # Solve QP via interior point\n    alpha = interior_point_qp(Q, c, A, b, l, u,\n                              mu_start=1.0, mu_tol=1e-9, mu_decay=0.2,\n                              newton_tol=1e-9, max_newton_iter=80,\n                              backtrack_beta=0.5, armijo_c1=1e-4, ridge=1e-10)\n\n    # Recover w\n    w = (alpha * y) @ X\n\n    # Recover b\n    tol = 1e-6\n    support_mask = (alpha > tol) & (alpha < C - tol)\n    scores = X @ w\n    if np.any(support_mask):\n        b_vals = y[support_mask] - scores[support_mask]\n        b = float(np.mean(b_vals))\n    else:\n        # Fallback: midpoint between class projections\n        scores_pos = scores[y > 0]\n        scores_neg = scores[y < 0]\n        if len(scores_pos) == 0 or len(scores_neg) == 0:\n            b = 0.0\n        else:\n            b = -0.5 * (np.max(scores_neg) + np.min(scores_pos))\n\n    return w, b, alpha\n\ndef accuracy_score(y_true, y_pred):\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    return float(np.mean(y_true == y_pred))\n\ndef run_case(X, y, C):\n    w, b, alpha = svm_linear_dual_ipm(X, y, C)\n    y_pred = np.sign(X @ w + b)\n    # Map zeros to +1 to avoid ambiguity\n    y_pred[y_pred == 0] = 1.0\n    return accuracy_score(y, y_pred)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    case_A_X = np.array([\n        [2.0, 2.0],\n        [2.0, 3.0],\n        [3.0, 2.0],\n        [-2.0, -2.0],\n        [-2.0, -3.0],\n        [-3.0, -2.0],\n    ], dtype=float)\n    case_A_y = np.array([+1.0, +1.0, +1.0, -1.0, -1.0, -1.0], dtype=float)\n    case_A_C = 100.0\n\n    case_B_X = np.array([\n        [2.0, 2.0],\n        [2.0, 0.0],\n        [3.0, 1.0],\n        [3.0, 2.0],\n        [0.0, 0.0],\n        [1.0, 0.0],\n        [0.0, 1.0],\n        [1.0, 1.5],\n    ], dtype=float)\n    case_B_y = np.array([+1.0, +1.0, +1.0, +1.0, -1.0, -1.0, -1.0, -1.0], dtype=float)\n    case_B_C = 1.0\n\n    case_C_X = np.array([\n        [2.0, 2.0],\n        [2.0, 3.0],\n        [3.0, 2.0],\n        [-2.0, -2.0],\n    ], dtype=float)\n    case_C_y = np.array([+1.0, +1.0, +1.0, -1.0], dtype=float)\n    case_C_C = 10.0\n\n    test_cases = [\n        (case_A_X, case_A_y, case_A_C),\n        (case_B_X, case_B_y, case_B_C),\n        (case_C_X, case_C_y, case_C_C),\n    ]\n\n    results = []\n    for X, y, C in test_cases:\n        acc = run_case(X, y, C)\n        results.append(f\"{acc:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3242644"}, {"introduction": "A trained model is useless without a reliable measure of its performance. This final practice moves beyond simple accuracy to a more nuanced and robust evaluation metric: the Area Under the Receiver Operating Characteristic Curve (AUC) [@problem_id:4562067]. You will implement the calculation of the AUC and, critically, its confidence interval using DeLong's method, providing a statistically grounded assessment of your classifier's predictive power. This exercise emphasizes that reporting a single performance number is insufficient; understanding and quantifying the uncertainty of that estimate is key to responsible machine learning practice.", "problem": "You are given predicted decision-function scores from Support Vector Machine (SVM) classifiers trained on radiomic features, along with the true binary outcome labels. Your task is to compute the Area Under the Receiver Operating Characteristic Curve (AUC) and a two-sided confidence interval using DeLong’s method, starting from the probability interpretation of AUC and the theory of unbiased estimators for pairwise comparisons. You must then interpret whether the classifier’s performance is clinically significant given a specified discrimination threshold. Use only foundational definitions of AUC as a probability, unbiased sample variance, and normal quantiles. Do not use any shortcuts or prepackaged routines beyond these foundations.\n\nDefinitions and requirements:\n\n- Let the set of positive-class scores be $\\mathbf{s}^+ = \\{s^+_1, s^+_2, \\dots, s^+_m\\}$ and the set of negative-class scores be $\\mathbf{s}^- = \\{s^-_1, s^-_2, \\dots, s^-_n\\}$. The AUC is defined as the probability that a randomly chosen positive score exceeds a randomly chosen negative score. Explicitly, let\n$$\n\\psi(a,b) = \n\\begin{cases}\n1, & \\text{if } a > b,\\\\\n0, & \\text{if } a < b,\\\\\n\\frac{1}{2}, & \\text{if } a = b,\n\\end{cases}\n$$\nthen the empirical AUC is\n$$\n\\widehat{\\theta} = \\frac{1}{mn} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\psi\\!\\left(s^+_i, s^-_j\\right).\n$$\n- Use DeLong’s method grounded in the theory of $U$-statistics to estimate the variance of $\\widehat{\\theta}$ without parametric assumptions. From this, construct a two-sided $(1-\\alpha)$ confidence interval via a normal approximation. Explicitly, use the critical value $z_{1-\\alpha/2}$ from the standard normal distribution and return bounds clipped to the interval $[0,1]$.\n- For clinical interpretability in radiomics, define a discrimination threshold $\\tau$ (a float in $[0,1]$). Declare the classifier “clinically significant” if the lower confidence bound is greater than or equal to $\\tau$. Express this decision as a boolean value.\n\nImplementation constraints:\n\n- Implement all computations directly from the above definitions, including the pairwise comparison function $\\psi$, unbiased sample variances, and standard normal quantiles.\n- Round all reported floating-point results (AUC and confidence bounds) to six decimal places.\n- No physical units are involved. Report any fractional quantities in decimal form.\n\nTest suite:\n\nCompute and report results for each of the following four test cases. In each case, you are given $\\mathbf{s}^+$, $\\mathbf{s}^-$, a confidence level $\\gamma$ (so $\\alpha = 1-\\gamma$), and a clinical threshold $\\tau$.\n\n- Case A (typical radiomics SVM with moderate discrimination):\n  - $\\mathbf{s}^+ = [1.8, 1.2, 0.9, 1.5, 1.1, 1.7, 0.8, 1.3, 0.95, 1.6]$\n  - $\\mathbf{s}^- = [-0.4, -0.1, 0.2, -0.6, 0.0, -0.2, 0.1, -0.3, 0.05, -0.5]$\n  - $\\gamma = 0.95$\n  - $\\tau = 0.75$\n- Case B (near-perfect separation):\n  - $\\mathbf{s}^+ = [2.5, 2.2, 1.9, 2.1, 2.3]$\n  - $\\mathbf{s}^- = [-1.0, -0.8, -0.9, -1.2, -0.7]$\n  - $\\gamma = 0.95$\n  - $\\tau = 0.75$\n- Case C (many ties, equal distributions):\n  - $\\mathbf{s}^+ = [0.5, 0.5, 0.5, 0.6, 0.4]$\n  - $\\mathbf{s}^- = [0.5, 0.5, 0.5, 0.6, 0.4]$\n  - $\\gamma = 0.95$\n  - $\\tau = 0.75$\n- Case D (small sample size, borderline discrimination):\n  - $\\mathbf{s}^+ = [0.7, 0.2, -0.1]$\n  - $\\mathbf{s}^- = [0.6, 0.3, 0.0, -0.2]$\n  - $\\gamma = 0.90$\n  - $\\tau = 0.70$\n\nFinal output format:\n\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list of lists, with no spaces, enclosed in square brackets. For each test case, output the list $[\\widehat{\\theta}, \\text{lower}, \\text{upper}, \\text{clinically\\_significant}]$, where the first three entries are floats rounded to six decimal places and the last entry is a boolean. For example, the final output should look like\n$[[\\theta_A,\\ell_A,u_A,B_A],[\\theta_B,\\ell_B,u_B,B_B],[\\theta_C,\\ell_C,u_C,B_C],[\\theta_D,\\ell_D,u_D,B_D]]$\nwith each symbol replaced by the computed values.", "solution": "The problem requires the computation of the Area Under the Receiver Operating Characteristic Curve (AUC), its confidence interval using DeLong's non-parametric method, and an assessment of clinical significance for several datasets of Support Vector Machine (SVM) classifier scores.\n\n**Problem Validation**\n\nFirst, I will validate the problem statement according to the specified criteria.\n\n**Step 1: Extracted Givens**\n-   Positive-class scores: $\\mathbf{s}^+ = \\{s^+_1, s^+_2, \\dots, s^+_m\\}$, with $m$ samples.\n-   Negative-class scores: $\\mathbf{s}^- = \\{s^-_1, s^-_2, \\dots, s^-_n\\}$, with $n$ samples.\n-   Pairwise comparison function: $\\psi(a,b) = 1$ if $a > b$, $0$ if $a < b$, and $\\frac{1}{2}$ if $a = b$.\n-   Empirical AUC estimator: $\\widehat{\\theta} = \\frac{1}{mn} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\psi(s^+_i, s^-_j)$.\n-   Confidence interval method: DeLong's method based on $U$-statistics, with a normal approximation using critical value $z_{1-\\alpha/2}$ for a confidence level of $\\gamma = 1-\\alpha$. Bounds must be clipped to $[0,1]$.\n-   Clinical significance threshold: $\\tau$. A classifier is \"clinically significant\" if its lower confidence bound is $\\ge \\tau$.\n-   Rounding: All floating-point outputs (AUC, bounds) to six decimal places.\n-   Test Cases: Four specific cases (A, B, C, D) are provided with score vectors $\\mathbf{s}^+$, $\\mathbf{s}^-$, confidence level $\\gamma$, and threshold $\\tau$.\n\n**Step 2: Validation Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is fundamentally sound. The definition of AUC as the probability $P(S^+ > S^-)$ is standard. The empirical estimator $\\widehat{\\theta}$ is the corresponding Wilcoxon-Mann-Whitney statistic. DeLong's method is a well-established, non-parametric technique for estimating the variance of this statistic, grounded in the theory of generalized $U$-statistics. This is a common and rigorous approach in biostatistics and machine learning evaluation.\n-   **Well-Posed**: The problem is well-posed. The inputs are clearly defined, the mathematical formulas are explicit, and the desired output format is unambiguous. For each test case, a unique, stable, and meaningful solution can be computed by following the defined procedures. The sample sizes in the test cases ($m \\ge 3, n \\ge 4$) are sufficient to compute the unbiased sample variances, which require at least two samples in each group.\n-   **Objective**: The problem is stated in precise, objective, mathematical language, free from ambiguity or subjective claims. While Case A is described as having \"moderate discrimination,\" the provided data actually exhibit perfect separation. This is a minor inconsistency in the descriptive text, but it does not invalidate the problem itself. The calculation must proceed based on the provided numerical data, which is unambiguous.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid as it is scientifically grounded, well-posed, objective, and contains all necessary information for a rigorous solution. I will proceed with the detailed solution.\n\n**Methodology and Solution**\n\nThe solution proceeds in four stages: (1) calculation of the empirical AUC, (2) estimation of the variance of the AUC estimator using DeLong’s method, (3) construction of the confidence interval, and (4) assessment of clinical significance.\n\n**1. Empirical AUC Estimation**\nThe AUC, $\\theta$, represents the probability that a randomly selected subject from the positive class has a higher score than a randomly selected subject from the negative class, i.e., $\\theta = P(S^+ > S^-)$. The quantity $\\widehat{\\theta}$ is an unbiased estimator of $\\theta$. It is calculated by averaging the outcomes of all $m \\times n$ pairwise comparisons between scores from the positive and negative classes.\nThe comparison is performed by the function $\\psi(a,b)$:\n$$\n\\psi(a,b) = \n\\begin{cases}\n1, & \\text{if } a > b,\\\\\n0, & \\text{if } a < b,\\\\\n\\frac{1}{2}, & \\text{if } a = b.\n\\end{cases}\n$$\nThe empirical AUC is then:\n$$\n\\widehat{\\theta} = \\frac{1}{mn} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\psi(s^+_i, s^-_j)\n$$\n\n**2. Variance Estimation via DeLong's Method**\nDeLong's method provides a non-parametric estimate of the variance of $\\widehat{\\theta}$. It is based on the theory of $U$-statistics. The first step is to compute structural components of the estimator.\nLet's define two sets of components. For each positive-class observation $s^+_i$, we calculate its average comparison score against all negative-class observations:\n$$\nV_{10}(s^+_i) = \\frac{1}{n} \\sum_{j=1}^{n} \\psi(s^+_i, s^-_j)\n$$\nThere are $m$ such components, one for each $s^+_i$. Intuitively, $V_{10}(s^+_i)$ is the empirical AUC calculated using only the $i$-th positive sample and all negative samples.\n\nSimilarly, for each negative-class observation $s^-_j$, we calculate its average comparison score against all positive-class observations:\n$$\nV_{01}(s^-_j) = \\frac{1}{m} \\sum_{i=1}^{m} \\psi(s^+_i, s^-_j)\n$$\nThere are $n$ such components. Notice that the average of all $V_{10}$ components and the average of all $V_{01}$ components are both equal to $\\widehat{\\theta}$.\n\nThe variance of $\\widehat{\\theta}$ is then estimated by combining the sample variances of these structural components. The unbiased sample variance of the $V_{10}$ components is:\n$$\nS_{10} = \\frac{1}{m-1} \\sum_{i=1}^{m} \\left( V_{10}(s^+_i) - \\widehat{\\theta} \\right)^2\n$$\nAnd the unbiased sample variance of the $V_{01}$ components is:\n$$\nS_{01} = \\frac{1}{n-1} \\sum_{j=1}^{n} \\left( V_{01}(s^-_j) - \\widehat{\\theta} \\right)^2\n$$\nThe estimated variance of the AUC estimator, $\\widehat{\\mathrm{Var}}(\\widehat{\\theta})$, is given by:\n$$\n\\widehat{\\mathrm{Var}}(\\widehat{\\theta}) = \\frac{S_{10}}{m} + \\frac{S_{01}}{n}\n$$\nThe standard error (SE) of $\\widehat{\\theta}$ is the square root of this variance:\n$$\n\\mathrm{SE}(\\widehat{\\theta}) = \\sqrt{\\widehat{\\mathrm{Var}}(\\widehat{\\theta})}\n$$\n\n**3. Confidence Interval Construction**\nThe distribution of $\\widehat{\\theta}$ can be approximated by a normal distribution, $\\mathcal{N}(\\theta, \\mathrm{SE}(\\widehat{\\theta})^2)$, especially for reasonably large $m$ and $n$. A two-sided $(1-\\alpha)$ confidence interval for $\\theta$ is constructed as:\n$$\n\\left[ \\widehat{\\theta} - z_{1-\\alpha/2} \\cdot \\mathrm{SE}(\\widehat{\\theta}), \\quad \\widehat{\\theta} + z_{1-\\alpha/2} \\cdot \\mathrm{SE}(\\widehat{\\theta}) \\right]\n$$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution. For a $\\gamma=0.95$ confidence level, $\\alpha=0.05$, and $z_{0.975} \\approx 1.96$. For $\\gamma=0.90$, $\\alpha=0.10$, and $z_{0.95} \\approx 1.645$. Since AUC is a probability, the calculated bounds must be clipped to the valid range $[0, 1]$.\n\n**4. Clinical Significance**\nThe clinical utility of a classifier is often judged by its ability to discriminate between classes with a certain level of confidence. The problem defines a classifier as \"clinically significant\" if the lower bound of its AUC confidence interval is greater than or equal to a pre-specified discrimination threshold $\\tau$. This provides a conservative estimate of performance, ensuring with $(1-\\alpha/2)$ confidence that the true AUC is at least $\\tau$. The decision is a boolean value based on the condition:\n$\\text{clinically\\_significant} = (\\text{lower\\_bound} \\ge \\tau)$\n\nAll calculations will be performed for each test case, and the final results (AUC, lower bound, upper bound, clinical significance) will be rounded and formatted as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating AUC, its CI via DeLong's method,\n    and clinical significance for given test cases.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {\n            \"s_plus\": [1.8, 1.2, 0.9, 1.5, 1.1, 1.7, 0.8, 1.3, 0.95, 1.6],\n            \"s_minus\": [-0.4, -0.1, 0.2, -0.6, 0.0, -0.2, 0.1, -0.3, 0.05, -0.5],\n            \"gamma\": 0.95,\n            \"tau\": 0.75\n        },\n        # Case B\n        {\n            \"s_plus\": [2.5, 2.2, 1.9, 2.1, 2.3],\n            \"s_minus\": [-1.0, -0.8, -0.9, -1.2, -0.7],\n            \"gamma\": 0.95,\n            \"tau\": 0.75\n        },\n        # Case C\n        {\n            \"s_plus\": [0.5, 0.5, 0.5, 0.6, 0.4],\n            \"s_minus\": [0.5, 0.5, 0.5, 0.6, 0.4],\n            \"gamma\": 0.95,\n            \"tau\": 0.75\n        },\n        # Case D\n        {\n            \"s_plus\": [0.7, 0.2, -0.1],\n            \"s_minus\": [0.6, 0.3, 0.0, -0.2],\n            \"gamma\": 0.90,\n            \"tau\": 0.70\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        s_plus = np.array(case[\"s_plus\"])\n        s_minus = np.array(case[\"s_minus\"])\n        gamma = case[\"gamma\"]\n        tau = case[\"tau\"]\n\n        m = len(s_plus)\n        n = len(s_minus)\n\n        # 1. Calculate matrix of psi(s_plus_i, s_minus_j) values\n        # This uses broadcasting to efficiently compute pairwise differences.\n        # The formula (sign(diff) + 1) / 2 is equivalent to the psi function.\n        diff_matrix = s_plus[:, np.newaxis] - s_minus\n        psi_matrix = (np.sign(diff_matrix) + 1) / 2\n\n        # 2. Calculate empirical AUC (theta_hat)\n        theta_hat = np.mean(psi_matrix)\n\n        # 3. DeLong's method for variance\n        v_10 = np.mean(psi_matrix, axis=1) # Structural components for positive class\n        v_01 = np.mean(psi_matrix, axis=0) # Structural components for negative class\n\n        # Unbiased sample variances of the components\n        s_10 = np.var(v_10, ddof=1)\n        s_01 = np.var(v_01, ddof=1)\n        \n        # Handle cases with perfect separation where variance is 0, or single-sample cases\n        if m = 1: s_10 = 0\n        if n = 1: s_01 = 0\n        if np.isnan(s_10): s_10 = 0\n        if np.isnan(s_01): s_01 = 0\n        \n        # Total variance of theta_hat\n        var_theta = (s_10 / m) + (s_01 / n)\n        se_theta = np.sqrt(var_theta)\n\n        # 4. Construct Confidence Interval\n        alpha = 1 - gamma\n        z_quantile = norm.ppf(1 - alpha / 2)\n        margin_of_error = z_quantile * se_theta\n        \n        lower_bound = theta_hat - margin_of_error\n        upper_bound = theta_hat + margin_of_error\n\n        # Clip bounds to [0, 1]\n        lower_bound = np.clip(lower_bound, 0, 1)\n        upper_bound = np.clip(upper_bound, 0, 1)\n\n        # 5. Determine Clinical Significance\n        is_significant = lower_bound = tau\n\n        # 6. Format results\n        result = [\n            round(theta_hat, 6),\n            round(lower_bound, 6),\n            round(upper_bound, 6),\n            is_significant\n        ]\n        results.append(str(result).replace(\" \", \"\"))\n\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4562067"}]}