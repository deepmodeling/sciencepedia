{"hands_on_practices": [{"introduction": "The Area Under the Curve ($AUC$) can be viewed from two different angles: geometrically, as the physical area beneath the ROC curve, and probabilistically, as the chance that a classifier will assign a higher score to a randomly chosen positive instance than to a randomly chosen negative one. This exercise is a fundamental proof that demonstrates these two perspectives are mathematically identical. Working through this \"hands-on\" proof solidifies the theoretical underpinnings of the $AUC$, ensuring you understand what the metric represents from first principles [@problem_id:3167034].", "problem": "A binary classifier assigns a real-valued score to each instance. There are $n$ positive instances and $m$ negative instances, with $n \\geq 1$ and $m \\geq 1$. Consider the following two empirical estimators of the area under the receiver operating characteristic (ROC) curve.\n\n1. Define the empirical Area Under the Curve (AUC) via the Mann–Whitney statistic as\n$$\nA_{\\mathrm{MW}} \\;=\\; \\frac{1}{nm} \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\left[ \\mathbf{1}\\big(s_{i}^{+}  s_{j}^{-}\\big) \\;+\\; \\frac{1}{2}\\,\\mathbf{1}\\big(s_{i}^{+} = s_{j}^{-}\\big) \\right],\n$$\nwhere $s_{i}^{+}$ denotes the score of the $i$-th positive instance, $s_{j}^{-}$ denotes the score of the $j$-th negative instance, and $\\mathbf{1}(\\cdot)$ is the indicator function.\n\n2. Construct the empirical ROC curve by sorting all instances in nonincreasing order of their score and grouping them into blocks of identical score. Suppose there are $B$ distinct score levels; at level $b \\in \\{1,\\dots,B\\}$ there are $t_{b}$ positives and $u_{b}$ negatives, with $\\sum_{b=1}^{B} t_{b} = n$ and $\\sum_{b=1}^{B} u_{b} = m$. Let the cumulative false positive rate and true positive rate just before block $b$ be\n$$\nx_{b-1} \\;=\\; \\frac{1}{m} \\sum_{a=1}^{b-1} u_{a}, \\qquad\ny_{b-1} \\;=\\; \\frac{1}{n} \\sum_{a=1}^{b-1} t_{a},\n$$\nand after block $b$ be\n$$\nx_{b} \\;=\\; x_{b-1} + \\frac{u_{b}}{m}, \\qquad\ny_{b} \\;=\\; y_{b-1} + \\frac{t_{b}}{n}.\n$$\nDefine the trapezoidal rule AUC on these $B+1$ points $\\{(x_{b},y_{b})\\}_{b=0}^{B}$ as\n$$\nA_{\\mathrm{trap}} \\;=\\; \\sum_{b=1}^{B} \\frac{y_{b-1} + y_{b}}{2} \\,\\big(x_{b} - x_{b-1}\\big).\n$$\n\nTreat the scores $\\{s_{i}^{+}\\}$ and $\\{s_{j}^{-}\\}$ as arbitrary real numbers, which determine the block sizes $\\{t_{b}\\}$ and $\\{u_{b}\\}$ and their order. Over all possible assignments of scores (equivalently, over all possible $\\{t_{b},u_{b}\\}_{b=1}^{B}$ consistent with $n$ and $m$ and all possible block orders), compute the supremum of the absolute discrepancy between these two AUC estimators,\n$$\n\\sup \\left| A_{\\mathrm{MW}} \\,-\\, A_{\\mathrm{trap}} \\right|.\n$$\n\nYour final answer must be a single real-valued number. No rounding is required.", "solution": "The problem asks for the supremum of the absolute difference between two estimators for the Area Under the ROC Curve (AUC): one based on the Mann-Whitney statistic, $A_{\\mathrm{MW}}$, and another based on the trapezoidal rule, $A_{\\mathrm{trap}}$. The supremum is taken over all possible assignments of scores to the $n$ positive and $m$ negative instances.\n\nOur strategy will be to express both $A_{\\mathrm{MW}}$ and $A_{\\mathrm{trap}}$ in a common algebraic form dependent on the structure of the sorted scores. Any assignment of scores $\\{s_i^+\\}$ and $\\{s_j^-\\}$ results in a specific sequence of blocks of tied scores when all $n+m$ scores are sorted in non-increasing order. Let there be $B$ such blocks, indexed $b=1, \\dots, B$, corresponding to distinct score values $S_1 > S_2 > \\dots > S_B$. Let block $b$ contain $t_b$ positive instances and $u_b$ negative instances. The total number of positive and negative instances are conserved, so we have the constraints $\\sum_{b=1}^{B} t_b = n$ and $\\sum_{b=1}^{B} u_b = m$. The problem is equivalent to finding the supremum of $|A_{\\mathrm{MW}} - A_{\\mathrm{trap}}|$ over all possible partitions $\\{t_b, u_b\\}_{b=1}^B$ and their order.\n\nFirst, let us analyze the trapezoidal rule estimator, $A_{\\mathrm{trap}}$.\nIt is defined as the sum of the areas of trapezoids formed by the points on the empirical ROC curve:\n$$\nA_{\\mathrm{trap}} \\;=\\; \\sum_{b=1}^{B} \\frac{y_{b-1} + y_{b}}{2} \\,\\big(x_{b} - x_{b-1}\\big).\n$$\nThe vertices of the ROC curve are $\\{(x_b, y_b)\\}_{b=0}^B$, where $(x_0, y_0) = (0,0)$. The point $(x_b, y_b)$ represents the cumulative false positive rate (FPR) and true positive rate (TPR) after considering all instances with scores greater than or equal to $S_b$.\nThe width of the $b$-th trapezoid is the change in FPR due to the instances in block $b$:\n$$\nx_b - x_{b-1} = \\frac{u_b}{m}.\n$$\nThe height of the $b$-th trapezoid changes from $y_{b-1}$ to $y_b$. The average height is $\\frac{y_{b-1} + y_b}{2}$. We can express $y_b$ in terms of $y_{b-1}$ and the number of positive instances $t_b$ in block $b$:\n$$\ny_b = y_{b-1} + \\frac{t_b}{n}.\n$$\nSubstituting this into the average height expression gives:\n$$\n\\frac{y_{b-1} + y_b}{2} = \\frac{y_{b-1} + (y_{b-1} + t_b/n)}{2} = y_{b-1} + \\frac{t_b}{2n}.\n$$\nThe TPR at point $b-1$, $y_{b-1}$, is the cumulative sum of positive instances in blocks with higher scores (i.e., blocks $a=1, \\ldots, b-1$):\n$$\ny_{b-1} = \\frac{1}{n} \\sum_{a=1}^{b-1} t_a.\n$$\nNow, we substitute these back into the formula for $A_{\\mathrm{trap}}$:\n$$\nA_{\\mathrm{trap}} = \\sum_{b=1}^{B} \\left( \\frac{1}{n} \\sum_{a=1}^{b-1} t_a + \\frac{t_b}{2n} \\right) \\left( \\frac{u_b}{m} \\right).\n$$\nFactoring out the constants $n$ and $m$, we arrive at a simplified expression for $A_{\\mathrm{trap}}$:\n$$\nA_{\\mathrm{trap}} = \\frac{1}{nm} \\sum_{b=1}^{B} u_b \\left( \\sum_{a=1}^{b-1} t_a + \\frac{t_b}{2} \\right).\n$$\n\nNext, we analyze the Mann-Whitney estimator, $A_{\\mathrm{MW}}$.\nIt is defined by averaging over all possible pairs of one positive and one negative instance:\n$$\nA_{\\mathrm{MW}} \\;=\\; \\frac{1}{nm} \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\left[ \\mathbf{1}\\big(s_{i}^{+}  s_{j}^{-}\\big) \\;+\\; \\frac{1}{2}\\,\\mathbf{1}\\big(s_{i}^{+} = s_{j}^{-}\\big) \\right].\n$$\nThe term $\\mathbf{1}(s_i^+  s_j^-) + \\frac{1}{2}\\mathbf{1}(s_i^+ = s_j^-)$ can be interpreted as the outcome of a comparison between the score of positive instance $i$ and negative instance $j$, where a win for the positive instance scores $1$, a tie scores $\\frac{1}{2}$, and a loss scores $0$.\nTo simplify this expression, we can change the order of summation and group the terms by the score blocks. Let us first sum over the negative instances $j$:\n$$\nA_{\\mathrm{MW}} = \\frac{1}{m} \\sum_{j=1}^{m} \\left[ \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\mathbf{1}(s_{i}^{+}  s_{j}^{-}) + \\frac{1}{2}\\mathbf{1}(s_{i}^{+} = s_{j}^{-}) \\right) \\right].\n$$\nConsider a negative instance $j$ belonging to block $b$. Its score is $s_j^- = S_b$. The inner sum over $i$ counts the number of positive instances $i$ for which $s_i^+  S_b$, plus half the number of positive instances for which $s_i^+ = S_b$.\nThe positive instances with scores $s_i^+  S_b$ are those in blocks $a=1, \\dots, b-1$. Their total count is $\\sum_{a=1}^{b-1} t_a$.\nThe positive instances with scores $s_i^+ = S_b$ are those in block $b$. Their count is $t_b$.\nThus, for any negative instance $j$ in block $b$, the value of the inner sum over $i$ is:\n$$\n\\sum_{i=1}^{n} \\left( \\mathbf{1}(s_{i}^{+}  S_b) + \\frac{1}{2}\\mathbf{1}(s_{i}^{+} = S_b) \\right) = \\left(\\sum_{a=1}^{b-1} t_a\\right) \\cdot 1 + t_b \\cdot \\frac{1}{2}.\n$$\nSince there are $u_b$ negative instances in block $b$, all of which have the same score $S_b$, we can rewrite the sum over $j$ as a sum over blocks $b$:\n$$\n\\sum_{j=1}^{m} \\left[ \\sum_{i=1}^{n} \\left(\\dots\\right) \\right] = \\sum_{b=1}^{B} u_b \\left( \\sum_{a=1}^{b-1} t_a + \\frac{t_b}{2} \\right).\n$$\nSubstituting this back into the formula for $A_{\\mathrm{MW}}$:\n$$\nA_{\\mathrm{MW}} = \\frac{1}{nm} \\sum_{b=1}^{B} u_b \\left( \\sum_{a=1}^{b-1} t_a + \\frac{t_b}{2} \\right).\n$$\nBy comparing the derived expressions, we find that\n$$\nA_{\\mathrm{trap}} = A_{\\mathrm{MW}} = \\frac{1}{nm} \\sum_{b=1}^{B} u_b \\left( \\sum_{a=1}^{b-1} t_a + \\frac{t_b}{2} \\right).\n$$\nThe two estimators are algebraically identical. Their equality holds for any choice of scores, as it only depends on the block counts $\\{t_b, u_b\\}$ and their rank ordering, which are determined by the scores.\n\nTherefore, for any possible assignment of scores, the difference between the two estimators is always zero:\n$$\nA_{\\mathrm{MW}} - A_{\\mathrm{trap}} = 0.\n$$\nThe absolute discrepancy is consequently also always zero:\n$$\n| A_{\\mathrm{MW}} - A_{\\mathrm{trap}} | = 0.\n$$\nThe problem asks for the supremum of this quantity over all possible score assignments. Since the quantity is constant and equal to $0$, its supremum is also $0$.\n$$\n\\sup \\left| A_{\\mathrm{MW}} \\,-\\, A_{\\mathrm{trap}} \\right| = \\sup \\{0\\} = 0.\n$$", "answer": "$$\n\\boxed{0}\n$$", "id": "3167034"}, {"introduction": "A high $AUC$ value confirms that a model is good at *discriminating* between classes, but it does not tell the whole story about model performance. This exercise explores the crucial difference between discrimination and *calibration*—the degree to which a model's predicted probabilities align with the true frequencies of events. By analyzing two simple but illustrative hypothetical models, you will see how a model can have excellent discrimination ($AUC \\approx 1$) but poor calibration, or perfect calibration but no discrimination ($AUC=0.5$), a vital distinction for using model outputs to make real-world decisions [@problem_id:4946999].", "problem": "A cohort of individuals is partitioned into two equally frequent covariate strata, denoted by $G \\in \\{L,H\\}$. Outcomes $Y \\in \\{0,1\\}$ are generated independently for each individual according to the following two data-generating mechanisms, and two corresponding predictors are defined.\n\nModel A (calibrated, non-discriminatory):\n- Data-generating mechanism: $Y \\sim \\text{Bernoulli}(p)$ with $p=0.3$, independent of $G$.\n- Predictor: For all individuals, the predicted probability is $\\hat{p}=0.3$.\n\nModel B (highly discriminatory, miscalibrated):\n- Data-generating mechanism: $Y \\mid (G=L) \\sim \\text{Bernoulli}(0.1)$ and $Y \\mid (G=H) \\sim \\text{Bernoulli}(0.9)$, with $\\mathbb{P}(G=L)=\\mathbb{P}(G=H)=\\frac{1}{2}$.\n- Predictor: For all individuals in $G=L$, the predicted probability is $\\hat{p}=0.01$, and for all individuals in $G=H$, the predicted probability is $\\hat{p}=0.99$.\n\nUsing only fundamental definitions, compute:\n1. The Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve for Model A.\n2. The AUC of the ROC curve for Model B.\n3. The calibration slope for Model B defined on the logit scale as the slope $m$ of the straight line through the two calibration points $\\big(\\text{logit}(\\hat{p}),\\text{logit}(\\text{observed event rate})\\big)$, namely the two points $\\big(\\text{logit}(0.01),\\text{logit}(0.1)\\big)$ and $\\big(\\text{logit}(0.99),\\text{logit}(0.9)\\big)$.\n\nReport your final answer as the ordered triple $\\big(\\text{AUC}_{A},\\text{AUC}_{B},m\\big)$. No rounding is required. Express logarithms using the natural logarithm $\\ln(\\cdot)$.", "solution": "The problem statement is first validated for scientific soundness, consistency, and completeness.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n- Two equally frequent covariate strata: $G \\in \\{L,H\\}$ with $\\mathbb{P}(G=L) = \\mathbb{P}(G=H) = \\frac{1}{2}$.\n- Outcome variable: $Y \\in \\{0,1\\}$, generated independently for each individual.\n\n**Model A:**\n- Data-generating mechanism: $Y \\sim \\text{Bernoulli}(p)$ with $p=0.3$, independent of $G$.\n- Predictor: For all individuals, the predicted probability is $\\hat{p}=0.3$.\n\n**Model B:**\n- Data-generating mechanism: $Y \\mid (G=L) \\sim \\text{Bernoulli}(0.1)$ and $Y \\mid (G=H) \\sim \\text{Bernoulli}(0.9)$.\n- Predictor: For individuals in stratum $G=L$, $\\hat{p}=0.01$. For individuals in stratum $G=H$, $\\hat{p}=0.99$.\n\n**Tasks:**\n1. Compute the Area Under the Curve (AUC) for Model A, denoted $\\text{AUC}_A$.\n2. Compute the AUC for Model B, denoted $\\text{AUC}_B$.\n3. Compute the calibration slope for Model B, $m$, defined as the slope of the line passing through the points $\\big(\\text{logit}(0.01),\\text{logit}(0.1)\\big)$ and $\\big(\\text{logit}(0.99),\\text{logit}(0.9)\\big)$. The logit function is defined on the natural logarithm scale, $\\ln(\\cdot)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria.\n- **Scientifically Grounded**: The problem utilizes standard, well-defined concepts from biostatistics, including Bernoulli distributions, Receiver Operating Characteristic (ROC) curves, Area Under the Curve (AUC), and model calibration. The models presented are simplified but coherent statistical constructs used to illustrate these concepts. The problem is scientifically sound.\n- **Well-Posed**: All necessary data and definitions are provided. The tasks are specific and mathematical, leading to a unique and meaningful solution. The problem is self-contained and unambiguous.\n- **Objective**: The problem is stated using formal, precise language, free of any subjective or opinion-based content.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A complete, reasoned solution will be provided.\n\n### Solution\n\n**1. Calculation of $\\text{AUC}_A$ for Model A**\n\nThe Area Under the Curve (AUC) of a Receiver Operating Characteristic (ROC) curve has a standard probabilistic interpretation: it is the probability that a randomly chosen subject with the event ($Y=1$) has a higher predicted risk score than a randomly chosen subject without the event ($Y=0$). If ties in scores are possible, the AUC is given by:\n$$ \\text{AUC} = \\mathbb{P}(S_1  S_0) + \\frac{1}{2}\\mathbb{P}(S_1 = S_0) $$\nwhere $S_1$ is the score for a random subject with $Y=1$ and $S_0$ is the score for a random subject with $Y=0$.\n\nIn Model A, the predictor assigns the same score, $\\hat{p}=0.3$, to every individual, regardless of their covariate stratum or true outcome. Therefore, for any pair of subjects, one with $Y=1$ and one with $Y=0$, their scores will be $S_1 = 0.3$ and $S_0 = 0.3$.\n\nConsequently:\n- The probability of the score for the positive case being strictly greater than the score for the negative case is zero: $\\mathbb{P}(S_1  S_0) = 0$.\n- The probability of their scores being equal is one: $\\mathbb{P}(S_1 = S_0) = 1$.\n\nSubstituting these values into the formula for AUC:\n$$ \\text{AUC}_A = 0 + \\frac{1}{2}(1) = \\frac{1}{2} = 0.5 $$\nAn AUC of $0.5$ indicates that the model has no discriminatory ability, which is expected since the predictor is a constant value. The ROC curve for this model is the diagonal line from $(0,0)$ to $(1,1)$.\n\n**2. Calculation of $\\text{AUC}_B$ for Model B**\n\nWe again use the probabilistic definition of AUC. In Model B, the score $\\hat{p}$ can take two values: $0.01$ for subjects in stratum $G=L$ and $0.99$ for subjects in stratum $G=H$. We need the distribution of scores conditional on the outcome $Y$.\n\nFirst, we calculate the overall prevalence of the event $Y=1$:\n$$ \\mathbb{P}(Y=1) = \\mathbb{P}(Y=1|G=L)\\mathbb{P}(G=L) + \\mathbb{P}(Y=1|G=H)\\mathbb{P}(G=H) $$\n$$ \\mathbb{P}(Y=1) = (0.1)\\left(\\frac{1}{2}\\right) + (0.9)\\left(\\frac{1}{2}\\right) = 0.05 + 0.45 = 0.5 $$\nThe prevalence of non-events is $\\mathbb{P}(Y=0) = 1 - \\mathbb{P}(Y=1) = 0.5$.\n\nNext, we find the distribution of scores for subjects with the event ($Y=1$). The score is $\\hat{p}=S_1$.\n$$ \\mathbb{P}(S_1=0.99) = \\mathbb{P}(G=H | Y=1) = \\frac{\\mathbb{P}(Y=1|G=H)\\mathbb{P}(G=H)}{\\mathbb{P}(Y=1)} = \\frac{(0.9)(\\frac{1}{2})}{0.5} = 0.9 $$\n$$ \\mathbb{P}(S_1=0.01) = \\mathbb{P}(G=L | Y=1) = 1 - \\mathbb{P}(S_1=0.99) = 1 - 0.9 = 0.1 $$\n\nThen, we find the distribution of scores for subjects without the event ($Y=0$). The score is $\\hat{p}=S_0$.\n$$ \\mathbb{P}(S_0=0.01) = \\mathbb{P}(G=L | Y=0) = \\frac{\\mathbb{P}(Y=0|G=L)\\mathbb{P}(G=L)}{\\mathbb{P}(Y=0)} = \\frac{(1-0.1)(\\frac{1}{2})}{0.5} = \\frac{(0.9)(\\frac{1}{2})}{0.5} = 0.9 $$\n$$ \\mathbb{P}(S_0=0.99) = \\mathbb{P}(G=H | Y=0) = 1 - \\mathbb{P}(S_0=0.01) = 1 - 0.9 = 0.1 $$\n\nNow we compute the terms for the AUC formula. Subjects are chosen independently.\n$$ \\mathbb{P}(S_1  S_0) = \\mathbb{P}(S_1=0.99 \\text{ and } S_0=0.01) = \\mathbb{P}(S_1=0.99)\\mathbb{P}(S_0=0.01) = (0.9)(0.9) = 0.81 $$\n$$ \\mathbb{P}(S_1 = S_0) = \\mathbb{P}(S_1=0.99, S_0=0.99) + \\mathbb{P}(S_1=0.01, S_0=0.01) $$\n$$ \\mathbb{P}(S_1 = S_0) = \\mathbb{P}(S_1=0.99)\\mathbb{P}(S_0=0.99) + \\mathbb{P}(S_1=0.01)\\mathbb{P}(S_0=0.01) = (0.9)(0.1) + (0.1)(0.9) = 0.09 + 0.09 = 0.18 $$\n\nFinally, we calculate $\\text{AUC}_B$:\n$$ \\text{AUC}_B = \\mathbb{P}(S_1  S_0) + \\frac{1}{2}\\mathbb{P}(S_1 = S_0) = 0.81 + \\frac{1}{2}(0.18) = 0.81 + 0.09 = 0.9 $$\nThis high AUC reflects the model's strong ability to discriminate between subjects who will and will not have the event.\n\n**3. Calculation of the calibration slope $m$ for Model B**\n\nThe calibration slope $m$ is defined as the slope of the line passing through two specified points on the logit-transformed scale: $(x_1, y_1)$ and $(x_2, y_2)$.\nThe logit function is $\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right)$.\n\nThe two points are given as:\n- Point 1: $(x_1, y_1) = \\big(\\text{logit}(0.01), \\text{logit}(0.1)\\big)$\n- Point 2: $(x_2, y_2) = \\big(\\text{logit}(0.99), \\text{logit}(0.9)\\big)$\n\nWe calculate the coordinates:\n$x_1 = \\text{logit}(0.01) = \\ln\\left(\\frac{0.01}{1-0.01}\\right) = \\ln\\left(\\frac{0.01}{0.99}\\right) = \\ln\\left(\\frac{1}{99}\\right) = -\\ln(99)$\n$y_1 = \\text{logit}(0.1) = \\ln\\left(\\frac{0.1}{1-0.1}\\right) = \\ln\\left(\\frac{0.1}{0.9}\\right) = \\ln\\left(\\frac{1}{9}\\right) = -\\ln(9)$\n$x_2 = \\text{logit}(0.99) = \\ln\\left(\\frac{0.99}{1-0.99}\\right) = \\ln\\left(\\frac{0.99}{0.01}\\right) = \\ln(99)$\n$y_2 = \\text{logit}(0.9) = \\ln\\left(\\frac{0.9}{1-0.9}\\right) = \\ln\\left(\\frac{0.9}{0.1}\\right) = \\ln(9)$\n\nThe slope $m$ is calculated as $m = \\frac{y_2 - y_1}{x_2 - x_1}$:\n$$ m = \\frac{\\ln(9) - (-\\ln(9))}{\\ln(99) - (-\\ln(99))} = \\frac{2\\ln(9)}{2\\ln(99)} = \\frac{\\ln(9)}{\\ln(99)} $$\nThe result is left in its exact symbolic form as requested.\n\nThe final ordered triple is $(\\text{AUC}_{A}, \\text{AUC}_{B}, m) = \\left(0.5, 0.9, \\frac{\\ln(9)}{\\ln(99)}\\right)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.5  0.9  \\frac{\\ln(9)}{\\ln(99)}\n\\end{pmatrix}\n}\n$$", "id": "4946999"}, {"introduction": "In modern machine learning, we train models by minimizing a \"loss function.\" If $AUC$ is our primary metric for success, a natural question is whether we can use it directly in the training process. This advanced practice delves into that question, showing how the standard, non-differentiable $AUC$ formula can be transformed into a smooth, differentiable surrogate function using the logistic sigmoid. This bridge between evaluation and optimization is a core concept in learning-to-rank and is essential for developing models that are explicitly trained to maximize their discriminative power [@problem_id:3167109].", "problem": "Consider a binary classification and learning-to-rank scenario with $n$ instances indexed by $i \\in \\{1,\\dots,n\\}$, true labels $y_i \\in \\{0,1\\}$, and real-valued scores $s_i \\in \\mathbb{R}$. Let $\\mathcal{P} = \\{i: y_i = 1\\}$ denote the set of positives with $n_{+} = |\\mathcal{P}|$, and $\\mathcal{N} = \\{j: y_j = 0\\}$ denote the set of negatives with $n_{-} = |\\mathcal{N}|$. The empirical Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve is defined as the average over all positive-negative pairs of the indicator that the positive score exceeds the negative score.\n\nStarting from fundamental definitions of the Heaviside indicator and differentiability, do the following:\n\n1) Derive, from first principles, the gradient (or subgradient where appropriate) with respect to the score vector $s = (s_1,\\dots,s_n)$ of the empirical Area Under the Curve (AUC), defined by\n$$\n\\mathrm{AUC}(s) \\equiv \\frac{1}{n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{N}} \\mathbf{1}\\{ s_i - s_j  0 \\}.\n$$\nClearly state where the function is differentiable, and provide the resulting expression for $\\nabla_{s} \\mathrm{AUC}(s)$ or a subgradient when differentiability fails.\n\n2) To obtain a differentiable surrogate suitable for optimization, replace the indicator $\\mathbf{1}\\{x  0\\}$ with the logistic sigmoid $\\sigma_{\\tau}(x) \\equiv \\frac{1}{1 + \\exp(-x/\\tau)}$ with temperature parameter $\\tau  0$, and define the smooth surrogate\n$$\nA_{\\tau}(s) \\equiv \\frac{1}{n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{N}} \\sigma_{\\tau}(s_i - s_j).\n$$\nDerive the exact analytic expression for the gradient $\\nabla_{s} A_{\\tau}(s)$.\n\n3) Consider the concrete case with $n = 4$, labels $y = (1, 0, 1, 0)$ so that $\\mathcal{P} = \\{1, 3\\}$ and $\\mathcal{N} = \\{2, 4\\}$, score vector $s = (1.2, 0.8, 0.5, 0.3)$, temperature $\\tau = 1$, and direction vector $v = (1, -2, 0.5, 3)$. Compute the directional derivative of the smooth surrogate at $s$ along $v$, that is,\n$$\nD A_{\\tau}(s)[v] \\equiv \\nabla_{s} A_{\\tau}(s)^{\\top} v,\n$$\nas a real number. Round your final numerical result to four significant figures. Express your answer as a unitless real number.", "solution": "The problem is evaluated to be scientifically grounded, well-posed, and contains all necessary information for a unique solution. The steps are logically consistent and pertain to standard concepts in statistical learning. Thus, the problem is valid.\n\nThe solution is presented in three parts, as requested.\n\n### Part 1: Gradient of the Empirical AUC\n\nThe empirical Area Under the Curve (AUC) is given by:\n$$\n\\mathrm{AUC}(s) = \\frac{1}{n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{N}} \\mathbf{1}\\{ s_i - s_j  0 \\}\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function, which can be defined using the Heaviside step function $H(x)$ as $\\mathbf{1}\\{x  0\\} \\equiv H(x)$. The function $H(x)$ is $1$ for $x  0$ and $0$ for $x \\leq 0$. The function $\\mathrm{AUC}(s)$ is a sum of these step functions and is thus piecewise constant.\n\nThe function is differentiable at any point $s$ for which $s_i \\neq s_j$ for all pairs $(i, j)$ with $i \\in \\mathcal{P}$ and $j \\in \\mathcal{N}$. At such points, a small perturbation in any score $s_k$ does not change the value of any indicator function, so the function is locally constant. The gradient is therefore the zero vector:\n$$\n\\nabla_{s} \\mathrm{AUC}(s) = \\mathbf{0} \\quad \\text{if } s_i \\neq s_j \\text{ for all } i \\in \\mathcal{P}, j \\in \\mathcal{N}.\n$$\nThe function is not differentiable at points where one or more scores of positive instances are tied with scores of negative instances, i.e., $s_i = s_j$ for some $i \\in \\mathcal{P}, j \\in \\mathcal{N}$. At these points, the concept of a subgradient or a generalized gradient is required.\n\nIn a distributional sense, the derivative of the Heaviside step function $H(x)$ is the Dirac delta function $\\delta(x)$. We use this to define a generalized gradient. The gradient of $\\mathrm{AUC}(s)$ with respect to the score vector $s$ is a vector where the $k$-th component is the partial derivative $\\frac{\\partial}{\\partial s_k} \\mathrm{AUC}(s)$.\n\nTo find the $k$-th component of the gradient, we differentiate the sum term by term:\n$$\n\\frac{\\partial}{\\partial s_k} \\mathrm{AUC}(s) = \\frac{1}{n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{N}} \\frac{\\partial}{\\partial s_k} H(s_i - s_j)\n$$\nUsing the chain rule, $\\frac{\\partial}{\\partial s_k} H(s_i - s_j) = \\delta(s_i - s_j) \\frac{\\partial}{\\partial s_k}(s_i - s_j)$. The term $\\frac{\\partial}{\\partial s_k}(s_i - s_j)$ evaluates to $1$ if $k=i$, $-1$ if $k=j$, and $0$ otherwise.\n\nWe consider two cases for the index $k$:\n\nCase 1: $k \\in \\mathcal{P}$ (the $k$-th instance is a positive).\nThe derivative $\\frac{\\partial}{\\partial s_k}(s_i - s_j)$ is non-zero only if $i=k$.\n$$\n\\frac{\\partial \\mathrm{AUC}(s)}{\\partial s_k} = \\frac{1}{n_{+} n_{-}} \\sum_{j \\in \\mathcal{N}} \\delta(s_k - s_j) \\cdot (1) = \\frac{1}{n_{+} n_{-}} \\sum_{j \\in \\mathcal{N}} \\delta(s_k - s_j)\n$$\n\nCase 2: $k \\in \\mathcal{N}$ (the $k$-th instance is a negative).\nThe derivative $\\frac{\\partial}{\\partial s_k}(s_i - s_j)$ is non-zero only if $j=k$.\n$$\n\\frac{\\partial \\mathrm{AUC}(s)}{\\partial s_k} = \\frac{1}{n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\delta(s_i - s_k) \\cdot (-1) = -\\frac{1}{n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\delta(s_i - s_k)\n$$\nThese expressions define the components of the generalized gradient $\\nabla_{s} \\mathrm{AUC}(s)$. Where differentiable (i.e., no ties $s_i=s_j$), the arguments to the delta functions are all non-zero, making the gradient the zero vector, consistent with our earlier observation.\n\n### Part 2: Gradient of the Smooth Surrogate\n\nThe smooth surrogate for AUC is defined as:\n$$\nA_{\\tau}(s) = \\frac{1}{n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{N}} \\sigma_{\\tau}(s_i - s_j)\n$$\nwith the logistic sigmoid function $\\sigma_{\\tau}(x) = \\frac{1}{1 + \\exp(-x/\\tau)}$. Since $\\sigma_{\\tau}(x)$ is a smooth function for $\\tau > 0$, $A_{\\tau}(s)$ is also smooth and its gradient is well-defined everywhere.\n\nFirst, we derive the derivative of the sigmoid function with respect to its argument $x$:\n\\begin{align*}\n\\frac{d}{dx} \\sigma_{\\tau}(x) = \\frac{d}{dx} \\left(1 + \\exp(-x/\\tau)\\right)^{-1} \\\\\n= -1 \\cdot \\left(1 + \\exp(-x/\\tau)\\right)^{-2} \\cdot \\left(\\exp(-x/\\tau) \\cdot \\left(-\\frac{1}{\\tau}\\right)\\right) \\\\\n= \\frac{1}{\\tau} \\frac{\\exp(-x/\\tau)}{\\left(1 + \\exp(-x/\\tau)\\right)^2} \\\\\n= \\frac{1}{\\tau} \\left(\\frac{1}{1 + \\exp(-x/\\tau)}\\right) \\left(\\frac{\\exp(-x/\\tau)}{1 + \\exp(-x/\\tau)}\\right) \\\\\n= \\frac{1}{\\tau} \\sigma_{\\tau}(x) \\left(\\frac{1 + \\exp(-x/\\tau) - 1}{1 + \\exp(-x/\\tau)}\\right) \\\\\n= \\frac{1}{\\tau} \\sigma_{\\tau}(x) \\left(1 - \\frac{1}{1 + \\exp(-x/\\tau)}\\right) \\\\\n= \\frac{1}{\\tau} \\sigma_{\\tau}(x) \\left(1 - \\sigma_{\\tau}(x)\\right)\n\\end{align*}\nLet's denote this derivative as $\\sigma'_{\\tau}(x)$. To find the $k$-th component of the gradient $\\nabla_{s} A_{\\tau}(s)$, we differentiate:\n$$\n\\frac{\\partial A_{\\tau}(s)}{\\partial s_k} = \\frac{1}{n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{N}} \\frac{\\partial}{\\partial s_k} \\sigma_{\\tau}(s_i - s_j)\n$$\nApplying the chain rule:\n$$\n\\frac{\\partial}{\\partial s_k} \\sigma_{\\tau}(s_i - s_j) = \\sigma'_{\\tau}(s_i - s_j) \\cdot \\frac{\\partial}{\\partial s_k}(s_i - s_j)\n$$\nAs before, we consider two cases for the index $k$:\n\nCase 1: $k \\in \\mathcal{P}$\n$$\n\\frac{\\partial A_{\\tau}(s)}{\\partial s_k} = \\frac{1}{n_{+} n_{-}} \\sum_{j \\in \\mathcal{N}} \\sigma'_{\\tau}(s_k - s_j) = \\frac{1}{\\tau n_{+} n_{-}} \\sum_{j \\in \\mathcal{N}} \\sigma_{\\tau}(s_k - s_j) (1 - \\sigma_{\\tau}(s_k - s_j))\n$$\n\nCase 2: $k \\in \\mathcal{N}$\n$$\n\\frac{\\partial A_{\\tau}(s)}{\\partial s_k} = \\frac{1}{n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\sigma'_{\\tau}(s_i - s_k) \\cdot (-1) = -\\frac{1}{\\tau n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\sigma_{\\tau}(s_i - s_k) (1 - \\sigma_{\\tau}(s_i - s_k))\n$$\nThese expressions give the exact analytic formula for each component of the gradient vector $\\nabla_{s} A_{\\tau}(s)$.\n\n### Part 3: Directional Derivative Calculation\n\nWe need to compute the directional derivative $D A_{\\tau}(s)[v] = \\nabla_{s} A_{\\tau}(s)^{\\top} v$.\nThe given parameters are:\n- Labels $y = (1, 0, 1, 0)$, so $\\mathcal{P} = \\{1, 3\\}$ and $\\mathcal{N} = \\{2, 4\\}$.\n- $n_{+} = 2$, $n_{-} = 2$.\n- Scores $s = (s_1, s_2, s_3, s_4) = (1.2, 0.8, 0.5, 0.3)$.\n- Temperature $\\tau = 1$. The sigmoid is $\\sigma_{1}(x) = (1 + \\exp(-x))^{-1}$.\n- Direction vector $v = (v_1, v_2, v_3, v_4) = (1, -2, 0.5, 3)$.\n\nThe gradient components are given by the formulas from Part 2, with the pre-factor being $\\frac{1}{\\tau n_{+} n_{-}} = \\frac{1}{1 \\cdot 2 \\cdot 2} = \\frac{1}{4}$. Let $\\sigma'(x) = \\sigma_1(x)(1-\\sigma_1(x))$.\n\nThe gradient components are:\n- For $k=1 \\in \\mathcal{P}$: $\\frac{\\partial A_1}{\\partial s_1} = \\frac{1}{4} \\left( \\sigma'(s_1-s_2) + \\sigma'(s_1-s_4) \\right)$\n- For $k=3 \\in \\mathcal{P}$: $\\frac{\\partial A_1}{\\partial s_3} = \\frac{1}{4} \\left( \\sigma'(s_3-s_2) + \\sigma'(s_3-s_4) \\right)$\n- For $k=2 \\in \\mathcal{N}$: $\\frac{\\partial A_1}{\\partial s_2} = -\\frac{1}{4} \\left( \\sigma'(s_1-s_2) + \\sigma'(s_3-s_2) \\right)$\n- For $k=4 \\in \\mathcal{N}$: $\\frac{\\partial A_1}{\\partial s_4} = -\\frac{1}{4} \\left( \\sigma'(s_1-s_4) + \\sigma'(s_3-s_4) \\right)$\n\nThe score differences are:\n- $s_1 - s_2 = 1.2 - 0.8 = 0.4$\n- $s_1 - s_4 = 1.2 - 0.3 = 0.9$\n- $s_3 - s_2 = 0.5 - 0.8 = -0.3$\n- $s_3 - s_4 = 0.5 - 0.3 = 0.2$\n\nThe values of $\\sigma'(x)$:\n- $\\sigma'(0.4) = \\sigma_1(0.4)(1-\\sigma_1(0.4)) \\approx 0.598688(1 - 0.598688) \\approx 0.240260$\n- $\\sigma'(0.9) = \\sigma_1(0.9)(1-\\sigma_1(0.9)) \\approx 0.710950(1 - 0.710950) \\approx 0.205504$\n- $\\sigma'(-0.3) = \\sigma_1(-0.3)(1-\\sigma_1(-0.3)) \\approx 0.425557(1 - 0.425557) \\approx 0.244460$\n- $\\sigma'(0.2) = \\sigma_1(0.2)(1-\\sigma_1(0.2)) \\approx 0.549834(1 - 0.549834) \\approx 0.247525$\n\nThe directional derivative is the dot product:\n$D A_1(s)[v] = \\sum_{k=1}^{4} \\frac{\\partial A_1}{\\partial s_k} v_k$.\n$$\nD A_1(s)[v] = \\frac{\\partial A_1}{\\partial s_1} v_1 + \\frac{\\partial A_1}{\\partial s_2} v_2 + \\frac{\\partial A_1}{\\partial s_3} v_3 + \\frac{\\partial A_1}{\\partial s_4} v_4\n$$\n$$\n= \\frac{1}{4} \\left( \\sigma'(0.4) + \\sigma'(0.9) \\right) \\cdot (1) \\\\\n- \\frac{1}{4} \\left( \\sigma'(0.4) + \\sigma'(-0.3) \\right) \\cdot (-2) \\\\\n+ \\frac{1}{4} \\left( \\sigma'(-0.3) + \\sigma'(0.2) \\right) \\cdot (0.5) \\\\\n- \\frac{1}{4} \\left( \\sigma'(0.9) + \\sigma'(0.2) \\right) \\cdot (3)\n$$\nWe can factor out $\\frac{1}{4}$ and group terms by $\\sigma'$ values:\n$$\n= \\frac{1}{4} \\left[ \\sigma'(0.4)(1+2) + \\sigma'(0.9)(1-3) + \\sigma'(-0.3)(2+0.5) + \\sigma'(0.2)(0.5-3) \\right]\n$$\n$$\n= \\frac{1}{4} \\left[ 3 \\sigma'(0.4) - 2 \\sigma'(0.9) + 2.5 \\sigma'(-0.3) - 2.5 \\sigma'(0.2) \\right]\n$$\nSubstituting the numerical values:\n$$\n\\approx \\frac{1}{4} \\left[ 3(0.240260) - 2(0.205504) + 2.5(0.244460) - 2.5(0.247525) \\right]\n$$\n$$\n= \\frac{1}{4} \\left[ 0.72078 - 0.411008 + 0.61115 - 0.6188125 \\right]\n$$\n$$\n= \\frac{1}{4} \\left[ 0.3021095 \\right] \\approx 0.075527375\n$$\nRounding to four significant figures, we get $0.07553$.", "answer": "$$\n\\boxed{0.07553}\n$$", "id": "3167109"}]}