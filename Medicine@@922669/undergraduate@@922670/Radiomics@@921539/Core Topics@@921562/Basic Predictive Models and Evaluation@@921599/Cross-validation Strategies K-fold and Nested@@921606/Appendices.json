{"hands_on_practices": [{"introduction": "In radiomics, datasets are often imbalanced, meaning one class is much rarer than another. Stratified k-fold cross-validation is a vital technique to ensure each fold reflects this overall class distribution as closely as possible. This practice moves beyond the abstract concept by using foundational probability to derive the exact expected number of positive and negative samples per fold, giving you a rigorous understanding of how stratification works [@problem_id:4535090].", "problem": "In a radiomics classification study, each patient is labeled as either positive (e.g., responders) or negative (e.g., non-responders). To estimate generalization performance while mitigating class imbalance, the team uses stratified $k$-fold cross-validation, and in deployment they plan to use nested cross-validation, where the outer loop is itself stratified. Assume that within each class, assignment to folds is performed so that each individual has the same probability of being placed in any given fold of the outer loop, and that assignments are exchangeable within a class. Using the foundational rules of probability, specifically the expectation of sums of indicator random variables and the equal-probability assignment principle for stratified folds, derive the expected number of positives per outer fold and the expected number of negatives per outer fold in stratified $k$-fold cross-validation in terms of $N_1$, $N_0$, and $k$. Then compute these expected values explicitly for $N_1=36$, $N_0=84$, and $k=5$. Express your final expected counts as exact rational numbers (do not round).", "solution": "The problem asks for the derivation of the expected number of positive and negative samples per fold in a stratified $k$-fold cross-validation setup, followed by a specific numerical calculation. The derivation will be based on the principles of probability, specifically using indicator random variables and the linearity of expectation.\n\nLet $N_1$ be the total number of positive samples and $N_0$ be the total number of negative samples in the dataset. Let $k$ be the number of folds. The total number of samples is $N = N_1 + N_0$. The process is stratified, which means the partitioning into folds is done separately for each class (positives and negatives).\n\nFirst, let us derive the expected number of positive samples per fold.\nLet the set of positive samples be $\\mathcal{S}_1 = \\{P_1, P_2, \\dots, P_{N_1}\\}$.\nLet the set of folds be $\\mathcal{F} = \\{F_1, F_2, \\dots, F_k\\}$.\nWe are interested in the number of positive samples in an arbitrary fold, let's say $F_j$ for some $j \\in \\{1, \\dots, k\\}$. Let this number be denoted by the random variable $X_j$.\n\nTo find the expected value of $X_j$, we can express $X_j$ as a sum of indicator random variables. For each positive sample $P_i \\in \\mathcal{S}_1$, let $I_{i,j}$ be an indicator variable such that:\n$$\nI_{i,j} = \\begin{cases} 1 & \\text{if sample } P_i \\text{ is in fold } F_j \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nThe total number of positive samples in fold $F_j$ is the sum of these indicators over all positive samples:\n$$\nX_j = \\sum_{i=1}^{N_1} I_{i,j}\n$$\nBy the linearity of expectation, the expected value of $X_j$ is the sum of the expected values of the individual indicator variables:\n$$\nE[X_j] = E\\left[\\sum_{i=1}^{N_1} I_{i,j}\\right] = \\sum_{i=1}^{N_1} E[I_{i,j}]\n$$\nThe expectation of an indicator variable is the probability of the event it indicates. Thus,\n$$\nE[I_{i,j}] = P(I_{i,j} = 1)\n$$\nwhere $P(I_{i,j} = 1)$ is the probability that sample $P_i$ is assigned to fold $F_j$. The problem states that \"within each class, assignment to folds is performed so that each individual has the same probability of being placed in any given fold.\" Since there are $k$ folds and each sample must be assigned to exactly one fold, the probability of a specific sample $P_i$ being assigned to a specific fold $F_j$ is $\\frac{1}{k}$.\n$$\nP(I_{i,j} = 1) = \\frac{1}{k}\n$$\nThis holds for any positive sample $i \\in \\{1, \\dots, N_1\\}$ and any fold $j \\in \\{1, \\dots, k\\}$.\nSubstituting this probability back into the expression for the expectation of the indicator variable:\n$$\nE[I_{i,j}] = \\frac{1}{k}\n$$\nNow, we can compute the sum:\n$$\nE[X_j] = \\sum_{i=1}^{N_1} \\frac{1}{k} = N_1 \\cdot \\frac{1}{k} = \\frac{N_1}{k}\n$$\nSince our choice of fold $F_j$ was arbitrary, this result holds for any fold. Let $E[N_{1,\\text{fold}}]$ denote the expected number of positive samples per fold.\n$$\nE[N_{1,\\text{fold}}] = \\frac{N_1}{k}\n$$\nThe derivation for the expected number of negative samples per fold follows the exact same logic.\nLet the set of negative samples be $\\mathcal{S}_0 = \\{N_1, N_2, \\dots, N_{N_0}\\}$.\nLet $Y_j$ be the random variable for the number of negative samples in fold $F_j$.\nLet $J_{i,j}$ be the indicator variable for the event that negative sample $N_i$ is in fold $F_j$.\n$$\nY_j = \\sum_{i=1}^{N_0} J_{i,j}\n$$\nBy linearity of expectation:\n$$\nE[Y_j] = \\sum_{i=1}^{N_0} E[J_{i,j}] = \\sum_{i=1}^{N_0} P(J_{i,j}=1)\n$$\nBased on the same assignment principle, the probability for any negative sample $N_i$ to be in any fold $F_j$ is also $\\frac{1}{k}$.\n$$\nP(J_{i,j}=1) = \\frac{1}{k}\n$$\nTherefore, the expected number of negative samples in fold $F_j$, denoted $E[N_{0,\\text{fold}}]$, is:\n$$\nE[Y_j] = \\sum_{i=1}^{N_0} \\frac{1}{k} = N_0 \\cdot \\frac{1}{k} = \\frac{N_0}{k}\n$$\nSo, the general expressions are $E[N_{1,\\text{fold}}] = \\frac{N_1}{k}$ and $E[N_{0,\\text{fold}}] = \\frac{N_0}{k}$.\n\nNow we apply these formulas to the specific values given in the problem:\n$N_1 = 36$ (positives)\n$N_0 = 84$ (negatives)\n$k = 5$ (folds)\n\nThe expected number of positives per fold is:\n$$\nE[N_{1,\\text{fold}}] = \\frac{N_1}{k} = \\frac{36}{5}\n$$\nThe expected number of negatives per fold is:\n$$\nE[N_{0,\\text{fold}}] = \\frac{N_0}{k} = \\frac{84}{5}\n$$\nThe problem requires these values to be expressed as exact rational numbers, which they are. The values represent the average number of samples of each class that would be found in a fold if the random assignment process were repeated many times. In any single partition, the actual counts must be integers, distributed as evenly as possible (e.g., for positives: one fold with $8$ and four folds with $7$, since $36 = 1 \\times 8 + 4 \\times 7$). The average over these folds is indeed $\\frac{36}{5} = 7.2$. Similarly for negatives, $84 = 4 \\times 17 + 1 \\times 16$, and the average is $\\frac{84}{5} = 16.8$. The derived expected values are thus confirmed to be correct.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{36}{5} & \\frac{84}{5}\n\\end{pmatrix}\n}\n$$", "id": "4535090"}, {"introduction": "Nested cross-validation is a powerful method for obtaining unbiased model performance estimates while simultaneously tuning hyperparameters. However, its multi-layered structure can be confusing and computationally expensive. This practice provides clarity by guiding you through a step-by-step calculation of the total number of model fits required, revealing the true computational cost and solidifying your understanding of the inner and outer loop mechanics [@problem_id:4535122].", "problem": "A radiomics research team is building a binary classifier to distinguish high-grade from low-grade tumors using quantitative features extracted from computed tomography scans. They choose a Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel. To obtain an unbiased estimate of generalization performance and to tune hyperparameters, they employ Nested Cross-Validation (CV), where an outer $k$-fold CV provides performance estimation and an inner $k$-fold CV performs hyperparameter selection by grid search.\n\nBy definition, in $k$-fold CV, the dataset is partitioned into $k$ disjoint folds; for each fold $i$, the model is trained on the $k-1$ training folds and evaluated on the held-out fold $i$. In nested CV, for each outer fold, an inner CV loop is run on the outer training data to select hyperparameters. A hyperparameter grid search evaluates each candidate hyperparameter setting via inner CV by training a model on each inner training fold. After inner CV selects the best hyperparameters for that outer split, the model is refit once on the full outer training data before evaluating on the outer test fold. Every distinct training of the model on a data split counts as one model fit, and there is no reuse or warm-start of fits across folds or hyperparameter settings.\n\nThe team specifies the hyperparameter grid as $C \\in \\{0.1, 1, 10\\}$ and $\\gamma \\in \\{10^{-3}, 10^{-2}, 10^{-1}\\}$. They choose $k_{\\text{outer}} = 5$ and $k_{\\text{inner}} = 3$.\n\nAssuming the procedure adheres strictly to the definitions above, compute the total number of model fits required to complete the entire nested CV process, including all inner CV fits for hyperparameter evaluation and the single refit on the full outer training set for each outer fold prior to evaluation. Express your final answer as a single integer. No rounding is necessary.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in standard machine learning methodology (nested cross-validation), is well-posed with all necessary parameters provided, and is expressed in objective, unambiguous language.\n\nThe objective is to compute the total number of model fits required for a complete nested cross-validation procedure. Let us systematically dissect the process as described.\n\nThe procedure involves an outer loop for performance estimation and an inner loop for hyperparameter tuning. We are given the following parameters:\n- The number of folds for the outer cross-validation, $k_{\\text{outer}} = 5$.\n- The number of folds for the inner cross-validation, $k_{\\text{inner}} = 3$.\n- The set of hyperparameter values for $C$ is $\\{0.1, 1, 10\\}$. The number of these values is $N_C = 3$.\n- The set of hyperparameter values for $\\gamma$ is $\\{10^{-3}, 10^{-2}, 10^{-1}\\}$. The number of these values is $N_\\gamma = 3$.\n\nThe total number of hyperparameter combinations to be evaluated in the grid search is the product of the number of values for each hyperparameter. Let $N_{\\text{params}}$ be the number of combinations in the grid.\n$$N_{\\text{params}} = N_C \\times N_\\gamma = 3 \\times 3 = 9$$\nThese $9$ distinct hyperparameter pairs must be evaluated within each outer fold's training process.\n\nThe entire procedure consists of $k_{\\text{outer}}$ iterations of an outer loop. In each iteration, one fold is held out as the outer test set, and the remaining $k_{\\text{outer}}-1$ folds constitute the outer training set. Let's analyze the number of model fits for a single iteration of this outer loop.\n\n1.  **Inner Cross-Validation for Hyperparameter Tuning:**\n    For one specific outer loop iteration (e.g., for outer fold $i$), the corresponding outer training data is used to find the optimal hyperparameters from the grid. This is done using an inner $k_{\\text{inner}}$-fold cross-validation.\n    - For *each* of the $N_{\\text{params}} = 9$ hyperparameter settings, a full $k_{\\text{inner}}$-fold CV is performed on the outer training data.\n    - By definition, a $k_{\\text{inner}}$-fold CV involves training the model $k_{\\text{inner}}$ times, each time on a different combination of $k_{\\text{inner}}-1$ inner folds and evaluating on the remaining inner fold.\n    - Therefore, the total number of model fits for the inner CV loop, for a single outer fold, is the number of hyperparameter settings multiplied by the number of inner folds.\n    - Let $N_{\\text{fits, inner-loop}}$ be this number.\n    $$N_{\\text{fits, inner-loop}} = N_{\\text{params}} \\times k_{\\text{inner}} = 9 \\times 3 = 27$$\n    These $27$ fits are performed to evaluate all hyperparameter candidates and select the best one for the current outer training set.\n\n2.  **Model Refitting for Outer Fold Evaluation:**\n    After the inner CV loop identifies the best hyperparameter pair (e.g., $(C^*, \\gamma^*)$), the problem states: \"the model is refit once on the full outer training data before evaluating on the outer test fold.\"\n    - This step involves a single model training. The model is trained using the best hyperparameters $(C^*, \\gamma^*)$ on the *entire* outer training set (all $k_{\\text{outer}}-1$ folds combined).\n    - This adds exactly $1$ model fit to our count for the current outer loop iteration.\n\n3.  **Total Fits per Outer Loop Iteration:**\n    The total number of fits for one complete iteration of the outer loop is the sum of the fits from the inner CV and the final refitting step.\n    - Let $N_{\\text{fits, one-outer-iter}}$ be this number.\n    $$N_{\\text{fits, one-outer-iter}} = N_{\\text{fits, inner-loop}} + 1 = 27 + 1 = 28$$\n\n4.  **Total Number of Model Fits:**\n    The process described in steps $1$-$3$ is repeated for each of the $k_{\\text{outer}}$ folds of the outer cross-validation. Since there is no reuse of fits across folds, we simply multiply the number of fits per outer iteration by the number of outer iterations.\n    - Let $N_{\\text{fits, total}}$ be the total number of model fits for the entire nested CV procedure.\n    $$N_{\\text{fits, total}} = k_{\\text{outer}} \\times N_{\\text{fits, one-outer-iter}}$$\n    Substituting the given and calculated values:\n    $$N_{\\text{fits, total}} = 5 \\times 28 = 140$$\n\nIn summary, the calculation is as follows: The outer loop runs $k_{\\text{outer}}=5$ times. Inside each run, a grid search is performed over $N_{\\text{params}}=9$ hyperparameter settings. Each setting is evaluated with a $k_{\\text{inner}}=3$-fold CV, resulting in $9 \\times 3 = 27$ trainings. After this, one final model is trained on the full outer training data, adding $1$ more training. Thus, each of the $5$ outer runs requires $27 + 1 = 28$ model trainings. The total number is $5 \\times 28 = 140$.", "answer": "$$\\boxed{140}$$", "id": "4535122"}, {"introduction": "Theory comes to life when put into practice. This exercise simulates a full, realistic workflow for model evaluation using nested cross-validation, tackling common challenges like class imbalance and threshold optimization. By working through the provided hypothetical data, you will see firsthand how to use inner validation sets for model tuning and then generate an unbiased performance estimate on a completely separate outer test set, cementing the critical principle of preventing data leakage [@problem_id:4535117].", "problem": "A binary radiomics classification task aims to distinguish malignant lesions ($y=1$) from benign lesions ($y=0$) using quantitative features extracted from medical images. Assume a nested cross-validation design with $K=3$ outer folds and $k=2$ inner folds. In each inner loop, Synthetic Minority Over-sampling Technique (SMOTE) is applied strictly to the inner training partition to address class imbalance, and a probabilistic classifier is trained to produce calibrated scores. The decision threshold $\\tau$ is selected within each outer fold by maximizing Youden’s $J$ statistic on inner validation predictions, where Youden’s $J$ is defined as $J=\\mathrm{TPR}+\\mathrm{TNR}-1$, with $\\mathrm{TPR}$ the true positive rate and $\\mathrm{TNR}$ the true negative rate. The Area Under the Receiver Operating Characteristic (AUC) is defined as the probability that a randomly chosen positive has a higher score than a randomly chosen negative, which for finite samples equals the fraction of positive-negative pairs correctly ordered, counting ties as one-half.\n\nFor each outer fold $j\\in\\{1,2,3\\}$, you are given the inner validation predictions (obtained after training with SMOTE applied only to the inner training partitions) and the outer test predictions (no SMOTE is ever applied to test data). Use only the inner validation predictions to choose $\\tau_{j}$, then evaluate sensitivity (true positive rate) on the corresponding outer test set at $\\tau_{j}$, and compute the AUC on the outer test set using the pairwise ranking definition.\n\nOuter fold $1$:\n- Inner validation set (scores, $y$): $(0.72,1)$, $(0.56,1)$, $(0.48,0)$, $(0.20,0)$.\n- Outer test set (scores, $y$): $(0.80,1)$, $(0.60,1)$, $(0.40,0)$, $(0.30,0)$.\n\nOuter fold $2$:\n- Inner validation set (scores, $y$): $(0.62,1)$, $(0.52,1)$, $(0.50,0)$, $(0.18,0)$.\n- Outer test set (scores, $y$): $(0.60,1)$, $(0.35,1)$, $(0.50,0)$, $(0.20,0)$.\n\nOuter fold $3$:\n- Inner validation set (scores, $y$): $(0.40,1)$, $(0.32,1)$, $(0.38,0)$, $(0.12,0)$.\n- Outer test set (scores, $y$): $(0.45,1)$, $(0.25,1)$, $(0.40,0)$, $(0.10,0)$.\n\nTasks:\n1. For each outer fold $j$, determine the threshold $\\tau_{j}$ that maximizes Youden’s $J$ on the inner validation set by evaluating $J$ at the set of candidate thresholds formed by the unique inner validation scores.\n2. For each outer fold $j$, compute the outer test AUC using pairwise ranking of scores between positives and negatives.\n3. For each outer fold $j$, compute the outer test sensitivity at the selected $\\tau_{j}$.\n4. Report the macro-averaged outer AUC across the three outer folds (the arithmetic mean of the three fold-specific AUC values) and the micro-averaged outer sensitivity (the total true positives across all outer test sets divided by the total number of positives across all outer test sets). Round your final two values to four significant figures. Express your final answer as a two-entry row matrix $\\begin{pmatrix}\\text{AUC} & \\text{sensitivity}\\end{pmatrix}$ without units.\n\nDemonstrate, through your calculations, that SMOTE and threshold tuning are confined to training partitions in the inner loop and do not use any outer test data.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It describes a standard and realistic procedure for model evaluation in machine learning, specifically within the context of medical image analysis (radiomics). The data provided are complete and consistent, and the tasks are well-defined with standard metrics. The problem is solvable and requires a careful application of the specified methodology.\n\nThe problem asks for an analysis based on a nested cross-validation (CV) procedure. The core principle of nested CV is to provide an unbiased estimate of a model's performance. This is achieved by separating the data used for model tuning (hyperparameter optimization, like selecting a decision threshold $\\tau$) from the data used for final performance evaluation.\n\nIn this problem, for each outer fold $j \\in \\{1,2,3\\}$ of a $K=3$ fold CV, there is an associated outer test set. The remaining data is used for an inner CV loop (with $k=2$ folds). This inner loop is used to train and tune the model. The problem states that the decision threshold $\\tau_j$ is selected on the inner validation set predictions. This selection process simulates the model tuning that would occur in a real-world application without \"peeking\" at the final test data. Once the optimal threshold $\\tau_j$ is determined for fold $j$, it is fixed. This fixed threshold is then used to evaluate the model's performance (sensitivity) on the completely separate outer test set for fold $j$. The AUC, being a threshold-independent metric, is also calculated on this outer test set. This strict separation ensures that the performance metrics (sensitivity at $\\tau_j$ and AUC) for each outer fold are not optimistically biased.\n\nThe use of SMOTE is confined to the inner training partitions, which are not even provided to us, correctly preventing any data leakage from validation or test sets into the oversampling process. Our calculations will strictly adhere to this separation of data, thereby demonstrating the correct implementation of the nested CV protocol.\n\nWe proceed with the calculations for each outer fold.\n\n### Outer Fold $j=1$\n\n#### 1. Threshold Selection ($\\tau_1$)\nThe inner validation set for fold $1$ is given by the score-label pairs: $\\{(0.72,1), (0.56,1), (0.48,0), (0.20,0)\\}$. There are $P_{val,1}=2$ positive samples (scores $\\{0.72, 0.56\\}$) and $N_{val,1}=2$ negative samples (scores $\\{0.48, 0.20\\}$).\nThe candidate thresholds are the unique scores: $\\{0.20, 0.48, 0.56, 0.72\\}$. We calculate Youden's $J = \\mathrm{TPR} + \\mathrm{TNR} - 1$ for each candidate threshold $\\tau$. A sample is predicted as positive if its score is $\\ge \\tau$.\n\n- For $\\tau = 0.20$: predictions are $\\{1,1,1,1\\}$. TP$=2$, FP$=2$, TN$=0$, FN$=0$.\n$\\mathrm{TPR} = \\frac{2}{2} = 1$. $\\mathrm{TNR} = \\frac{0}{2} = 0$. $J = 1 + 0 - 1 = 0$.\n- For $\\tau = 0.48$: predictions are $\\{1,1,1,0\\}$. TP$=2$, FP$=1$, TN$=1$, FN$=0$.\n$\\mathrm{TPR} = \\frac{2}{2} = 1$. $\\mathrm{TNR} = \\frac{1}{2} = 0.5$. $J = 1 + 0.5 - 1 = 0.5$.\n- For $\\tau = 0.56$: predictions are $\\{1,1,0,0\\}$. TP$=2$, FP$=0$, TN$=2$, FN$=0$.\n$\\mathrm{TPR} = \\frac{2}{2} = 1$. $\\mathrm{TNR} = \\frac{2}{2} = 1$. $J = 1 + 1 - 1 = 1$.\n- For $\\tau = 0.72$: predictions are $\\{1,0,0,0\\}$. TP$=1$, FP$=0$, TN$=2$, FN$=1$.\n$\\mathrm{TPR} = \\frac{1}{2} = 0.5$. $\\mathrm{TNR} = \\frac{2}{2} = 1$. $J = 0.5 + 1 - 1 = 0.5$.\n\nThe maximum Youden's $J$ is $1$, which occurs at $\\tau_1 = 0.56$.\n\n#### 2. Outer Test Set Evaluation for Fold $1$\nThe outer test set is $\\{(0.80,1), (0.60,1), (0.40,0), (0.30,0)\\}$. There are $P_{test,1}=2$ positives (scores $\\{0.80, 0.60\\}$) and $N_{test,1}=2$ negatives (scores $\\{0.40, 0.30\\}$).\n\n- **AUC Calculation**: The number of positive-negative pairs is $P_{test,1} \\times N_{test,1} = 2 \\times 2 = 4$.\nThe pairs are $(0.80, 0.40)$, $(0.80, 0.30)$, $(0.60, 0.40)$, and $(0.60, 0.30)$.\nIn all $4$ pairs, the score of the positive sample is greater than the score of the negative sample. There are no ties.\n$\\mathrm{AUC}_1 = \\frac{4}{4} = 1$.\n\n- **Sensitivity Calculation**: Using the selected threshold $\\tau_1 = 0.56$, we classify the outer test positives:\n- Score $0.80 \\ge 0.56 \\implies$ True Positive (TP).\n- Score $0.60 \\ge 0.56 \\implies$ True Positive (TP).\nThe number of true positives is $\\mathrm{TP}_1 = 2$.\n$\\mathrm{Sensitivity}_1 = \\frac{\\mathrm{TP}_1}{P_{test,1}} = \\frac{2}{2} = 1$.\n\n### Outer Fold $j=2$\n\n#### 1. Threshold Selection ($\\tau_2$)\nThe inner validation set for fold $2$ is $\\{(0.62,1), (0.52,1), (0.50,0), (0.18,0)\\}$. There are $P_{val,2}=2$ positives (scores $\\{0.62, 0.52\\}$) and $N_{val,2}=2$ negatives (scores $\\{0.50, 0.18\\}$).\nThe candidate thresholds are $\\{0.18, 0.50, 0.52, 0.62\\}$.\n\n- For $\\tau = 0.18$: TP$=2$, FP$=2$, TN$=0$, FN$=0$. $\\mathrm{TPR} = 1$, $\\mathrm{TNR} = 0$. $J=0$.\n- For $\\tau = 0.50$: TP$=2$, FP$=1$, TN$=1$, FN$=0$. $\\mathrm{TPR} = 1$, $\\mathrm{TNR} = 0.5$. $J=0.5$.\n- For $\\tau = 0.52$: TP$=2$, FP$=0$, TN$=2$, FN$=0$. $\\mathrm{TPR} = 1$, $\\mathrm{TNR} = 1$. $J=1$.\n- For $\\tau = 0.62$: TP$=1$, FP$=0$, TN$=2$, FN$=1$. $\\mathrm{TPR} = 0.5$, $\\mathrm{TNR} = 1$. $J=0.5$.\n\nThe maximum Youden's $J$ is $1$, which occurs at $\\tau_2 = 0.52$.\n\n#### 2. Outer Test Set Evaluation for Fold $2$\nThe outer test set is $\\{(0.60,1), (0.35,1), (0.50,0), (0.20,0)\\}$. There are $P_{test,2}=2$ positives (scores $\\{0.60, 0.35\\}$) and $N_{test,2}=2$ negatives (scores $\\{0.50, 0.20\\}$).\n\n- **AUC Calculation**: The number of pairs is $2 \\times 2 = 4$.\n- $(0.60, 0.50)$: Positive score $>$ Negative score (correctly ranked, $+1$).\n- $(0.60, 0.20)$: Positive score $>$ Negative score (correctly ranked, $+1$).\n- $(0.35, 0.50)$: Positive score $<$ Negative score (incorrectly ranked, $+0$).\n- $(0.35, 0.20)$: Positive score $>$ Negative score (correctly ranked, $+1$).\nThe number of correctly ranked pairs is $3$.\n$\\mathrm{AUC}_2 = \\frac{3}{4} = 0.75$.\n\n- **Sensitivity Calculation**: Using $\\tau_2 = 0.52$, we classify the outer test positives:\n- Score $0.60 \\ge 0.52 \\implies$ True Positive (TP).\n- Score $0.35 < 0.52 \\implies$ False Negative (FN).\nThe number of true positives is $\\mathrm{TP}_2 = 1$.\n$\\mathrm{Sensitivity}_2 = \\frac{\\mathrm{TP}_2}{P_{test,2}} = \\frac{1}{2} = 0.5$.\n\n### Outer Fold $j=3$\n\n#### 1. Threshold Selection ($\\tau_3$)\nThe inner validation set for fold $3$ is $\\{(0.40,1), (0.32,1), (0.38,0), (0.12,0)\\}$. There are $P_{val,3}=2$ positives (scores $\\{0.40, 0.32\\}$) and $N_{val,3}=2$ negatives (scores $\\{0.38, 0.12\\}$).\nThe candidate thresholds are $\\{0.12, 0.32, 0.38, 0.40\\}$.\n\n- For $\\tau = 0.12$: TP$=2$, FP$=2$, TN$=0$. $\\mathrm{TPR} = 1$, $\\mathrm{TNR} = 0$. $J=0$.\n- For $\\tau = 0.32$: TP$=2$, FP$=1$, TN$=1$. $\\mathrm{TPR} = 1$, $\\mathrm{TNR} = 0.5$. $J=0.5$.\n- For $\\tau = 0.38$: TP$=1$, FP$=1$, TN$=1$. $\\mathrm{TPR} = 0.5$, $\\mathrm{TNR} = 0.5$. $J=0$.\n- For $\\tau = 0.40$: TP$=1$, FP$=0$, TN$=2$. $\\mathrm{TPR} = 0.5$, $\\mathrm{TNR} = 1$. $J=0.5$.\n\nThe maximum Youden's $J$ is $0.5$, which is achieved at both $\\tau = 0.32$ and $\\tau = 0.40$. The problem does not specify a tie-breaking rule. A common convention is to select the lower threshold to prioritize sensitivity. We will adopt this convention and choose $\\tau_3 = 0.32$. (Note: Choosing $\\tau=0.40$ yields the same test sensitivity in this specific case).\n\n#### 2. Outer Test Set Evaluation for Fold $3$\nThe outer test set is $\\{(0.45,1), (0.25,1), (0.40,0), (0.10,0)\\}$. There are $P_{test,3}=2$ positives (scores $\\{0.45, 0.25\\}$) and $N_{test,3}=2$ negatives (scores $\\{0.40, 0.10\\}$).\n\n- **AUC Calculation**: The number of pairs is $2 \\times 2 = 4$.\n- $(0.45, 0.40)$: Positive score $>$ Negative score (correctly ranked, $+1$).\n- $(0.45, 0.10)$: Positive score $>$ Negative score (correctly ranked, $+1$).\n- $(0.25, 0.40)$: Positive score $<$ Negative score (incorrectly ranked, $+0$).\n- $(0.25, 0.10)$: Positive score $>$ Negative score (correctly ranked, $+1$).\nThe number of correctly ranked pairs is $3$.\n$\\mathrm{AUC}_3 = \\frac{3}{4} = 0.75$.\n\n- **Sensitivity Calculation**: Using $\\tau_3 = 0.32$, we classify the outer test positives:\n- Score $0.45 \\ge 0.32 \\implies$ True Positive (TP).\n- Score $0.25 < 0.32 \\implies$ False Negative (FN).\nThe number of true positives is $\\mathrm{TP}_3 = 1$.\n$\\mathrm{Sensitivity}_3 = \\frac{\\mathrm{TP}_3}{P_{test,3}} = \\frac{1}{2} = 0.5$.\n\n### Final Performance Aggregation\n\n#### Macro-Averaged AUC\nThe macro-averaged AUC is the arithmetic mean of the AUCs from each outer fold.\n$$ \\mathrm{AUC}_{\\text{macro}} = \\frac{\\mathrm{AUC}_1 + \\mathrm{AUC}_2 + \\mathrm{AUC}_3}{3} = \\frac{1 + 0.75 + 0.75}{3} = \\frac{2.5}{3} \\approx 0.833333... $$\nRounding to four significant figures, $\\mathrm{AUC}_{\\text{macro}} = 0.8333$.\n\n#### Micro-Averaged Sensitivity\nMicro-averaged sensitivity is the total number of true positives across all folds divided by the total number of positive samples across all folds.\n$$ \\mathrm{Sensitivity}_{\\text{micro}} = \\frac{\\sum_{j=1}^3 \\mathrm{TP}_j}{\\sum_{j=1}^3 P_{test,j}} = \\frac{\\mathrm{TP}_1 + \\mathrm{TP}_2 + \\mathrm{TP}_3}{P_{test,1} + P_{test,2} + P_{test,3}} = \\frac{2 + 1 + 1}{2 + 2 + 2} = \\frac{4}{6} = \\frac{2}{3} \\approx 0.666666... $$\nRounding to four significant figures, $\\mathrm{Sensitivity}_{\\text{micro}} = 0.6667$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix} 0.8333 & 0.6667 \\end{pmatrix}\n}\n$$", "id": "4535117"}]}