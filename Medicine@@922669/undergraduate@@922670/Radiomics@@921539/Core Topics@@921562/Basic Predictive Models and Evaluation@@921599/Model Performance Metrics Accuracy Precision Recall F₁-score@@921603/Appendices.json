{"hands_on_practices": [{"introduction": "This exercise grounds your understanding in the fundamental outputs of a binary classifier. You will start with the raw counts of classification events—true positives, false positives, and false negatives—to calculate the essential performance metrics of precision, recall, and the F₁-score. More importantly, this practice will challenge you to explain why, in the context of imbalanced medical data common to radiomics, the F₁-score provides a more meaningful assessment of model performance than simple accuracy [@problem_id:4551730].", "problem": "A radiomics system is developed to classify lesions as malignant ($+$) or benign ($-$) based on features extracted from Magnetic Resonance Imaging (MRI). On an independent, multi-institution hold-out cohort, the system produces the following confusion events: true positives (TP) $TP=40$, false positives (FP) $FP=20$, and false negatives (FN) $FN=10$. The number of true negatives (TN) is not reported because the institutions differ in benign case sampling strategies. Starting from the core definitions of event rates in binary classification, derive the expressions for precision, recall, and the F₁-score, and compute their values for this cohort. Then, using first principles, explain how the F₁-score should be interpreted relative to accuracy in a class-imbalanced screening setting typical of radiomics, where benign cases dominate and $TN$ is much larger than $TP+FN$. Express the three computed metric values as exact fractions. No rounding is required. Provide the three numbers in the order: precision, recall, F₁-score.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- Classification task: Lesions as malignant ($+$) or benign ($-$).\n- True Positives ($TP$): $TP=40$.\n- False Positives ($FP$): $FP=20$.\n- False Negatives ($FN$): $FN=10$.\n- True Negatives ($TN$): Not reported.\n- Task a: Derive expressions for precision, recall, and F₁-score.\n- Task b: Compute the values for these metrics.\n- Task c: Explain the interpretation of the F₁-score relative to accuracy in a class-imbalanced setting where $TN$ is much larger than $TP+FN$.\n- Task d: Express computed metrics as exact fractions.\n- Task e: Provide the final values in the order: precision, recall, F₁-score.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is valid. It presents a standard binary classification scenario from the field of radiomics, a subfield of medical imaging analysis. The provided data ($TP$, $FP$, $FN$) are sufficient to calculate the requested metrics (precision, recall, F₁-score), as these metrics are independent of the true negative count ($TN$). The explicit mention that $TN$ is unreported is a deliberate and realistic constraint, not a flaw. The conceptual question regarding the F₁-score versus accuracy in an imbalanced setting is a fundamental and important topic in machine learning evaluation. The problem is self-contained, scientifically sound, and well-posed.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Derivation and Computation of Metrics**\n\nWe begin by defining the fundamental classification events:\n- **True Positive ($TP$)**: A malignant lesion correctly classified as malignant.\n- **False Positive ($FP$)**: A benign lesion incorrectly classified as malignant (Type I error).\n- **True Negative ($TN$)**: A benign lesion correctly classified as benign.\n- **False Negative ($FN$)**: A malignant lesion incorrectly classified as benign (Type II error).\n\nThe total number of actual positive (malignant) instances is $P_{actual} = TP + FN$.\nThe total number of actual negative (benign) instances is $N_{actual} = TN + FP$.\nThe total number of predicted positive instances is $P_{predicted} = TP + FP$.\nThe total number of predicted negative instances is $N_{predicted} = TN + FN$.\n\n**1. Precision ($P$)**\nPrecision, or Positive Predictive Value (PPV), measures the proportion of predicted positive cases that are actually positive. It answers the question: \"Of all lesions the system flagged as malignant, what fraction were actually malignant?\"\nStarting from its definition:\n$$P = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}}$$\nIn terms of the confusion events, this is:\n$$P = \\frac{TP}{TP + FP}$$\nUsing the given values, $TP=40$ and $FP=20$:\n$$P = \\frac{40}{40 + 20} = \\frac{40}{60} = \\frac{2}{3}$$\n\n**2. Recall ($R$)**\nRecall, also known as Sensitivity or the True Positive Rate (TPR), measures the proportion of actual positive cases that were correctly identified. It answers the question: \"Of all the truly malignant lesions present, what fraction did the system correctly identify?\"\nStarting from its definition:\n$$R = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Positives}}$$\nIn terms of the confusion events, this is:\n$$R = \\frac{TP}{TP + FN}$$\nUsing the given values, $TP=40$ and $FN=10$:\n$$R = \\frac{40}{40 + 10} = \\frac{40}{50} = \\frac{4}{5}$$\n\n**3. F₁-score**\nThe F₁-score is the harmonic mean of precision and recall. It is used as a single metric that balances the concerns of both precision and recall. The harmonic mean is chosen because it penalizes extreme values more than the arithmetic mean. A high F₁-score requires both high precision and high recall.\nThe general formula for the harmonic mean of two numbers $a$ and $b$ is $2 \\frac{ab}{a+b}$. Thus, for precision $P$ and recall $R$:\n$$F_1 = 2 \\frac{P \\cdot R}{P + R}$$\nWe can derive an expression for the F₁-score directly in terms of $TP$, $FP$, and $FN$ by substituting the expressions for $P$ and $R$:\n$$F_1 = 2 \\frac{\\left(\\frac{TP}{TP + FP}\\right) \\cdot \\left(\\frac{TP}{TP + FN}\\right)}{\\left(\\frac{TP}{TP + FP}\\right) + \\left(\\frac{TP}{TP + FN}\\right)}$$\n$$F_1 = 2 \\frac{\\frac{TP^2}{(TP + FP)(TP + FN)}}{\\frac{TP(TP + FN) + TP(TP + FP)}{(TP + FP)(TP + FN)}}$$\n$$F_1 = \\frac{2 \\cdot TP^2}{TP(TP + FN) + TP(TP + FP)}$$\nFactoring out $TP$ from the denominator:\n$$F_1 = \\frac{2 \\cdot TP}{(TP + FN) + (TP + FP)} = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}$$\nThis final expression is often more direct for computation. Using the given values, $TP=40$, $FP=20$, and $FN=10$:\n$$F_1 = \\frac{2 \\cdot 40}{2 \\cdot 40 + 20 + 10} = \\frac{80}{80 + 20 + 10} = \\frac{80}{110} = \\frac{8}{11}$$\n\nThe computed values are: Precision $P = \\frac{2}{3}$, Recall $R = \\frac{4}{5}$, and F₁-score $F_1 = \\frac{8}{11}$.\n\n**Interpretation of F₁-score vs. Accuracy in an Imbalanced Setting**\n\nFirst, we must define accuracy ($ACC$). Accuracy is the proportion of all predictions that are correct.\n$$ACC = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} = \\frac{TP + TN}{TP + TN + FP + FN}$$\nThe problem states that in a typical radiomics screening setting, benign cases dominate, meaning the number of true negatives ($TN$) is much larger than the number of cases related to the positive (malignant) class. Mathematically, $TN \\gg (TP + FN)$.\n\nLet's analyze the behavior of accuracy under this condition. As $TN$ becomes very large relative to the other terms ($TP$, $FP$, $FN$), both the numerator and denominator of the accuracy formula are dominated by $TN$:\n$$ACC = \\frac{TP + TN}{TP + TN + FP + FN} \\approx \\frac{TN}{TN + FP}$$\nFurthermore, since the model is not completely random, $FP$ is typically much smaller than $TN$. Thus, as $TN \\to \\infty$, the ratio approaches $1$:\n$$\\lim_{TN \\to \\infty} \\left( \\frac{TP + TN}{TP + TN + FP + FN} \\right) = 1$$\nThis reveals the critical flaw of accuracy in class-imbalanced scenarios. A trivial classifier that always predicts the majority class (benign) would have $TP=0$ and $FP=0$. Its accuracy would be $ACC = \\frac{TN}{TN + FN}$. If there is $1$ malignant case for every $999$ benign cases ($FN=1, TN=999$), this trivial classifier would achieve an accuracy of $\\frac{999}{1000} = 99.9\\%$, giving a dangerously misleading impression of high performance while it fails to identify any of the malignant cases it is designed to find.\n\nIn contrast, the F₁-score, along with its components precision and recall, are independent of $TN$:\n$$P = \\frac{TP}{TP+FP}, \\quad R = \\frac{TP}{TP+FN}, \\quad F_1 = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}$$\nThese metrics exclusively evaluate the model's performance on the positive class.\n- **Precision** focuses on the reliability of positive predictions, directly addressing the cost of false alarms ($FP$).\n- **Recall** focuses on the model's ability to find all positive cases, directly addressing the cost of missed cases ($FN$).\nA trivial \"always benign\" classifier would have $TP=0$, resulting in $P=0$, $R=0$, and $F_1=0$, correctly reflecting its complete lack of utility for detecting the malignant class.\n\nThe F₁-score synthesizes the trade-off between precision and recall into a single number. In medical screening, both are critical: low recall ($high FN$) means missing cancers, which is a severe failure. Low precision ($high FP$) means subjecting many healthy individuals to unnecessary, costly, and stressful follow-up procedures. The F₁-score provides a more robust and relevant measure of a model's practical utility in such imbalanced settings than accuracy, because it is not inflated by the large number of correctly identified majority-class cases ($TN$). It properly focuses the evaluation on the performance related to the minority, high-stakes class of interest.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2}{3}  \\frac{4}{5}  \\frac{8}{11}\n\\end{pmatrix}\n}\n$$", "id": "4551730"}, {"introduction": "To truly master performance metrics, we must understand not only how they work but also how they can fail. This problem guides you through a powerful thought experiment involving a \"pathological\" classifier that always predicts the positive class. By deriving metrics like precision and the F₁-score as a function of disease prevalence, you will gain a profound intuition for their behavior at the extremes and see firsthand why a single metric, viewed in isolation, can be dangerously misleading [@problem_id:4551735].", "problem": "A radiomics team trains a binary classifier on texture and shape features from Computed Tomography (CT) images to detect malignancy in lung nodules. To probe the limits of model performance metrics in a scientifically controlled way, consider an extreme, pathological decision rule that predicts the positive class (\"malignant\") for every case, regardless of features. Let the cohort size be $N$, and let the disease prevalence be $p \\in (0,1)$, defined as the proportion of truly positive cases in the cohort. Use the standard confusion-matrix counts: true positives ($TP$), false positives ($FP$), true negatives ($TN$), and false negatives ($FN$).\n\nStarting solely from core definitions—accuracy as the fraction of correct predictions, precision as the fraction of predicted positives that are truly positive, recall (sensitivity) as the fraction of true positives that are identified by the classifier, and the F₁-score as the harmonic mean of precision and recall—derive expressions for accuracy, precision, recall, and the F₁-score in terms of $p$ and $N$ for this always-positive classifier. Analyze the behavior of these expressions as $p \\to 0$ and as $p \\to 1$ to illustrate the extremes under varying prevalence.\n\nReport only the closed-form expression for the F₁-score as a function of $p$ as your final answer. No rounding is required.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- Classifier type: Binary, for malignancy in lung nodules.\n- Decision rule: Predicts the positive class (\"malignant\") for every case.\n- Cohort size: $N$.\n- Disease prevalence (proportion of truly positive cases): $p \\in (0,1)$.\n- Confusion matrix counts: True positives ($TP$), false positives ($FP$), true negatives ($TN$), and false negatives ($FN$).\n- Metric definitions:\n  - Accuracy: $\\frac{TP+TN}{TP+TN+FP+FN}$\n  - Precision: $\\frac{TP}{TP+FP}$\n  - Recall (Sensitivity): $\\frac{TP}{TP+FN}$\n  - F₁-score: Harmonic mean of precision and recall.\n- Task: Derive expressions for accuracy, precision, recall, and the F₁-score in terms of $p$ and $N$. Analyze their behavior as $p \\to 0$ and $p \\to 1$.\n- Final answer requirement: The closed-form expression for the F₁-score as a function of $p$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a well-defined theoretical exercise in evaluating performance metrics under specific, albeit extreme, conditions.\n- **Scientifically Grounded:** The problem uses standard, well-established definitions for performance metrics (accuracy, precision, recall, F₁-score) from the field of machine learning and diagnostics. The concept of prevalence is central to epidemiology and medical test evaluation. The \"always-positive\" classifier is a valid pathological case for demonstrating the behavior and limitations of these metrics.\n- **Well-Posed:** All necessary information is provided. The variables $N$ and $p$ are sufficient to define the state of the cohort, and the classifier's behavior is unambiguously specified. A unique solution for each metric can be derived.\n- **Objective:** The problem statement is free of subjective language and relies on formal definitions.\n\nThe problem does not violate any of the invalidity criteria. It is a valid, formalizable problem that tests the understanding of core statistical concepts.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n**Derivation of Performance Metrics**\n\nLet us begin by formalizing the composition of the cohort based on the givens. The total number of cases is $N$. The prevalence of the positive class (malignancy) is $p$.\n\nThe number of truly positive (malignant) cases in the cohort is the product of the prevalence and the total cohort size:\n$$ \\text{Number of positives} = P = pN $$\nThe number of truly negative (benign) cases is therefore:\n$$ \\text{Number of negatives} = Neg = (1-p)N $$\n\nNext, we evaluate the confusion matrix counts for the pathological classifier that \"predicts the positive class for every case\".\n\n1.  **True Positives ($TP$)**: The classifier predicts \"positive\" for all cases, including all $pN$ truly positive cases. Therefore, all truly positive cases are correctly identified as positive.\n    $$ TP = pN $$\n2.  **False Negatives ($FN$)**: The classifier never predicts \"negative\". Consequently, it is impossible for it to misclassify a truly positive case as negative.\n    $$ FN = 0 $$\n3.  **False Positives ($FP$)**: The classifier predicts \"positive\" for all cases, including all $(1-p)N$ truly negative cases. Therefore, all truly negative cases are incorrectly identified as positive.\n    $$ FP = (1-p)N $$\n4.  **True Negatives ($TN$)**: The classifier never predicts \"negative\". Consequently, it is impossible for it to correctly identify a truly negative case.\n    $$ TN = 0 $$\n\nWe can verify that the total number of cases is conserved: $TP + FN + FP + TN = pN + 0 + (1-p)N + 0 = N$.\n\nNow, we derive the expressions for the performance metrics using their fundamental definitions.\n\n**Accuracy**\nAccuracy is the fraction of all predictions that are correct. Correct predictions are $TP$ and $TN$.\n$$ \\text{Accuracy} = \\frac{TP + TN}{N} = \\frac{pN + 0}{N} = p $$\n\n**Precision**\nPrecision is the fraction of positive predictions that are correct. The total number of positive predictions is $TP + FP$.\n$$ \\text{Precision} = \\frac{TP}{TP + FP} $$\nThe denominator is $TP + FP = pN + (1-p)N = N$. Since the cohort size $N$ must be a positive integer for the problem to be meaningful, the denominator is non-zero.\n$$ \\text{Precision} = \\frac{pN}{N} = p $$\n\n**Recall (Sensitivity)**\nRecall is the fraction of actual positives that are correctly identified. The total number of actual positives is $TP + FN$.\n$$ \\text{Recall} = \\frac{TP}{TP + FN} $$\nThe denominator is $TP+FN = pN+0 = pN$. Since the problem specifies $p \\in (0,1)$, the prevalence $p$ is strictly positive, ensuring a non-zero denominator.\n$$ \\text{Recall} = \\frac{pN}{pN} = 1 $$\nA recall of $1$ is expected, as this classifier, by design, never misses a positive case.\n\n**F₁-Score**\nThe F₁-score is the harmonic mean of precision and recall.\n$$ F_{1} = 2 \\cdot \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\nSubstituting the derived expressions for precision and recall:\n$$ F_{1} = 2 \\cdot \\frac{p \\times 1}{p + 1} = \\frac{2p}{p+1} $$\n\n**Analysis of Limiting Behavior**\n\nThis analysis illuminates the pitfalls of relying on single metrics, especially with imbalanced data.\n\n- **As prevalence approaches zero ($p \\to 0$):**\n  - $\\text{Accuracy} = p \\to 0$. The model is almost always wrong because it predicts \"positive\" for a vast majority of cases that are truly \"negative\".\n  - $\\text{Precision} = p \\to 0$. An overwhelming majority of the positive predictions are false positives. The model's positive predictions are unreliable.\n  - $\\text{Recall} = 1$. The model perfectly identifies all the rare positive cases.\n  - $F_{1} = \\frac{2p}{p+1} \\to \\frac{2(0)}{0+1} = 0$. The $F_{1}$-score correctly reflects the model's poor overall performance, driven to zero by the abysmal precision, despite the perfect recall.\n\n- **As prevalence approaches one ($p \\to 1$):**\n  - $\\text{Accuracy} = p \\to 1$. The model is almost always correct because it predicts \"positive\" for a vast majority of cases that are truly \"positive\".\n  - $\\text{Precision} = p \\to 1$. Almost all positive predictions are correct since there are very few negative cases to misclassify.\n  - $\\text{Recall} = 1$. As always for this classifier.\n  - $F_{1} = \\frac{2p}{p+1} \\to \\frac{2(1)}{1+1} = 1$. All metrics approach $1$, suggesting a perfect classifier. This demonstrates how high prevalence can mask the fundamentally flawed nature of a simplistic decision rule. The classifier succeeds not by discerning features, but by guessing the overwhelmingly common class.\n\nThis thought experiment establishes that accuracy and precision are highly sensitive to prevalence, while recall for an always-positive classifier is fixed at $1$. The $F_{1}$-score provides a more balanced assessment than any single metric, as its value is governed by the more pessimistic of its components (in this case, precision). The final requested expression is the formula for the $F_{1}$-score.", "answer": "$$ \\boxed{\\frac{2p}{p+1}} $$", "id": "4551735"}, {"introduction": "This final practice bridges the gap between theoretical metrics and clinical reality, where decisions are constrained by practical limitations. You will tackle a realistic optimization problem: selecting a classifier's decision threshold to maximize performance while respecting real-world constraints like limited follow-up capacity and minimum sensitivity requirements. This hands-on coding exercise demonstrates how model evaluation directly informs actionable clinical strategy and involves navigating the crucial trade-offs inherent in deploying a diagnostic model [@problem_id:4551721].", "problem": "You are given a binary classification scenario from radiomics in which a probabilistic classifier assigns to each case a malignancy score in the closed interval $[0,1]$. Each week, the clinic can perform at most $C$ follow-up actions, and a follow-up is triggered for every case predicted as positive. The weekly radiologist team requires a minimum sensitivity level, expressed as a minimum recall requirement $r_{\\min} \\in [0,1]$, to ensure that a sufficient fraction of actual malignant cases are flagged for follow-up. Your task is to choose a single decision threshold $\\tau \\in [0,1]$ such that a case is predicted positive if and only if its score is at least $\\tau$.\n\nYour program must determine $\\tau$ that maximizes precision while satisfying both the weekly follow-up capacity and the minimum recall requirement. Formally, for each candidate threshold $\\tau$, compute the confusion matrix entries based on standard definitions: True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN). Let $N$ be the total number of cases and $P$ be the total number of actual positives. The performance metrics are defined as follows, all expressed as decimals:\n- Accuracy: $\\mathrm{Acc} = \\dfrac{\\mathrm{TP} + \\mathrm{TN}}{N}$.\n- Precision: $\\mathrm{Prec} = \\dfrac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}$ with the convention that $\\mathrm{Prec} = 0$ when $\\mathrm{TP} + \\mathrm{FP} = 0$.\n- Recall: $\\mathrm{Rec} = \\dfrac{\\mathrm{TP}}{P}$ with the convention that $\\mathrm{Rec} = 0$ when $P = 0$.\n- F₁-score: $\\mathrm{F}_1 = \\dfrac{2 \\cdot \\mathrm{Prec} \\cdot \\mathrm{Rec}}{\\mathrm{Prec} + \\mathrm{Rec}}$ with the convention that $\\mathrm{F}_1 = 0$ when $\\mathrm{Prec} + \\mathrm{Rec} = 0$.\n\nA threshold $\\tau$ is feasible if and only if both:\n- The number of predicted positives is at most the weekly capacity $C$, i.e., $\\mathrm{TP} + \\mathrm{FP} \\le C$.\n- The recall meets the minimum requirement, i.e., $\\mathrm{Rec} \\ge r_{\\min}$.\n\nAmong all feasible thresholds, select the one that maximizes $\\mathrm{Prec}$. If there is a tie in precision, break ties by choosing the threshold with the highest F₁-score. If there is still a tie, choose the largest threshold $\\tau$. If no threshold satisfies both feasibility conditions, return $\\tau = -1$ and set all four metrics to $0$.\n\nUse the following test suite. In all cases, a case is predicted positive if and only if its score is at least $\\tau$. All numeric answers must be expressed as decimals (not percentages). Your program must evaluate the threshold only at the set of unique score values observed in each dataset.\n\nTest suite:\n- Case $1$: Weekly capacity $C = 20$, minimum recall $r_{\\min} = 0.7$. Scores and labels (each pair is $(\\text{score}, \\text{label})$) for $N = 30$ radiomics cases:\n  $\\{(0.95,1),(0.92,1),(0.90,1),(0.88,0),(0.86,1),(0.84,1),(0.82,0),(0.80,1),(0.78,0),(0.76,1),(0.74,1),(0.72,0),(0.70,1),(0.68,0),(0.66,1),(0.64,0),(0.62,0),(0.60,1),(0.58,0),(0.56,0),(0.54,1),(0.52,0),(0.50,0),(0.48,1),(0.46,0),(0.44,0),(0.42,0),(0.40,1),(0.38,0),(0.36,0)\\}$.\n- Case $2$: Weekly capacity $C = 5$, minimum recall $r_{\\min} = 0.9$. Scores and labels for $N = 20$ radiomics cases:\n  $\\{(0.99,1),(0.95,1),(0.93,1),(0.85,1),(0.80,1),(0.70,1),(0.60,0),(0.58,0),(0.55,0),(0.52,0),(0.50,0),(0.48,0),(0.46,0),(0.44,0),(0.42,0),(0.40,0),(0.38,0),(0.36,0),(0.34,0),(0.30,0)\\}$.\n- Case $3$: Weekly capacity $C = 3$, minimum recall $r_{\\min} = 0.5$. Scores and labels for $N = 12$ radiomics cases:\n  $\\{(0.96,1),(0.90,0),(0.88,1),(0.85,0),(0.80,1),(0.78,0),(0.60,1),(0.55,0),(0.52,0),(0.50,0),(0.40,0),(0.30,0)\\}$.\n\nFinal output format:\n- For each case, output a list $[\\tau, \\mathrm{Acc}, \\mathrm{Prec}, \\mathrm{Rec}, \\mathrm{F}_1]$ with each value rounded to exactly $6$ decimal places.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list of these lists, enclosed in square brackets, for example $[[\\cdot],[\\cdot],[\\cdot]]$.", "solution": "The user-provided problem is assessed to be valid. It is a well-posed optimization problem grounded in the standard principles of binary classification model evaluation. The problem statement is self-contained, objective, and scientifically sound. All necessary data, constraints, and definitions are provided, and there are no internal contradictions.\n\nThe problem asks for the determination of an optimal decision threshold, $\\tau$, for a probabilistic binary classifier used in a radiomics context. The goal is to maximize the classifier's precision ($\\mathrm{Prec}$) subject to two operational constraints: a weekly follow-up capacity, $C$, and a minimum required sensitivity level, $r_{\\min}$, expressed as a recall ($\\mathrm{Rec}$) value.\n\nThe solution methodology proceeds in a structured, stepwise manner.\n\n1.  **Problem Formalization**\n    The task is a constrained optimization problem. We must find a threshold $\\tau \\in [0,1]$ that solves:\n    $$\n    \\underset{\\tau}{\\text{maximize}} \\quad \\mathrm{Prec}(\\tau)\n    $$\n    Subject to:\n    1.  Capacity Constraint: $\\mathrm{TP}(\\tau) + \\mathrm{FP}(\\tau) \\le C$\n    2.  Recall Constraint: $\\mathrm{Rec}(\\tau) \\ge r_{\\min}$\n\n    In cases where multiple thresholds yield the same maximum precision, a tie-breaking hierarchy is enforced:\n    1.  Select the threshold with the highest $\\mathrm{F}_1$-score.\n    2.  If a tie persists, select the largest threshold value, $\\tau$.\n\n    The search space for $\\tau$ is explicitly limited to the set of unique radiomics scores present in the dataset.\n\n2.  **Systematic Evaluation of Candidate Thresholds**\n    The core of the methodology is to perform an exhaustive search over the allowed set of candidate thresholds. For each unique score value observed in the data, we treat it as a potential threshold $\\tau$.\n\n    For each candidate $\\tau$, we first classify all $N$ cases. A case is predicted as positive if its score is greater than or equal to $\\tau$, and negative otherwise. This allows us to construct a confusion matrix by comparing the predictions to the true labels. The elements of this matrix are:\n    -   $\\mathrm{TP}(\\tau)$: True Positives (score $\\ge \\tau$ and true label is $1$)\n    -   $\\mathrm{FP}(\\tau)$: False Positives (score $\\ge \\tau$ and true label is $0$)\n    -   $\\mathrm{TN}(\\tau)$: True Negatives (score $ \\tau$ and true label is $0$)\n    -   $\\mathrm{FN}(\\tau)$: False Negatives (score $ \\tau$ and true label is $1$)\n\n3.  **Performance Metric Calculation**\n    Using the confusion matrix values, we calculate the required performance metrics for each $\\tau$. Let $P = \\mathrm{TP} + \\mathrm{FN}$ be the total number of actual positive cases in the dataset.\n    -   **Accuracy**: $\\mathrm{Acc} = \\dfrac{\\mathrm{TP} + \\mathrm{TN}}{N}$\n    -   **Precision**: $\\mathrm{Prec} = \\dfrac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}$. By convention, if the denominator $\\mathrm{TP} + \\mathrm{FP} = 0$, then $\\mathrm{Prec} = 0$.\n    -   **Recall**: $\\mathrm{Rec} = \\dfrac{\\mathrm{TP}}{P}$. By convention, if $P=0$, then $\\mathrm{Rec}=0$.\n    -   **F₁-score**: $\\mathrm{F}_1 = \\dfrac{2 \\cdot \\mathrm{Prec} \\cdot \\mathrm{Rec}}{\\mathrm{Prec} + \\mathrm{Rec}}$. By convention, if the denominator $\\mathrm{Prec} + \\mathrm{Rec} = 0$, then $\\mathrm{F}_1 = 0$.\n\n4.  **Feasibility Analysis**\n    Each candidate threshold $\\tau$ and its corresponding metrics are tested for feasibility against the two given constraints:\n    -   $\\mathrm{TP}(\\tau) + \\mathrm{FP}(\\tau) \\le C$\n    -   $\\mathrm{Rec}(\\tau) \\ge r_{\\min}$\n\n    A threshold is considered a valid solution only if it satisfies both conditions simultaneously. All such feasible solutions are collected for the final selection step.\n\n5.  **Optimal Solution Selection**\n    If the set of feasible solutions is empty after evaluating all candidate thresholds, it is concluded that no solution exists that satisfies the clinical requirements. In this scenario, the output is $\\tau = -1$ and all performance metrics are set to $0$.\n\n    If one or more feasible solutions exist, the optimal solution is identified by applying the specified hierarchical criteria. An efficient way to implement this is to sort the set of feasible solutions. Each solution is represented by a tuple or structure containing its metrics, for instance $(\\mathrm{Prec}, \\mathrm{F}_1, \\tau)$. The sorting is performed in descending order of $\\mathrm{Prec}$, then descending order of $\\mathrm{F}_1$, and finally descending order of $\\tau$. The first element in the sorted list is the unique optimal solution according to the problem's rules.\n\n    The final output for each test case is a list containing the optimal threshold and its associated metrics: $[\\tau, \\mathrm{Acc}, \\mathrm{Prec}, \\mathrm{Rec}, \\mathrm{F}_1]$, with each value numerically formatted as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the threshold optimization problem for a series of test cases.\n    \"\"\"\n    test_cases = [\n        (\n            20, 0.7, \n            [\n                (0.95, 1), (0.92, 1), (0.90, 1), (0.88, 0), (0.86, 1), (0.84, 1), \n                (0.82, 0), (0.80, 1), (0.78, 0), (0.76, 1), (0.74, 1), (0.72, 0), \n                (0.70, 1), (0.68, 0), (0.66, 1), (0.64, 0), (0.62, 0), (0.60, 1), \n                (0.58, 0), (0.56, 0), (0.54, 1), (0.52, 0), (0.50, 0), (0.48, 1), \n                (0.46, 0), (0.44, 0), (0.42, 0), (0.40, 1), (0.38, 0), (0.36, 0)\n            ]\n        ),\n        (\n            5, 0.9, \n            [\n                (0.99, 1), (0.95, 1), (0.93, 1), (0.85, 1), (0.80, 1), (0.70, 1), \n                (0.60, 0), (0.58, 0), (0.55, 0), (0.52, 0), (0.50, 0), (0.48, 0), \n                (0.46, 0), (0.44, 0), (0.42, 0), (0.40, 0), (0.38, 0), (0.36, 0), \n                (0.34, 0), (0.30, 0)\n            ]\n        ),\n        (\n            3, 0.5, \n            [\n                (0.96, 1), (0.90, 0), (0.88, 1), (0.85, 0), (0.80, 1), (0.78, 0), \n                (0.60, 1), (0.55, 0), (0.52, 0), (0.50, 0), (0.40, 0), (0.30, 0)\n            ]\n        )\n    ]\n\n    all_results = []\n    for C, r_min, data in test_cases:\n        scores = np.array([d[0] for d in data])\n        labels = np.array([d[1] for d in data])\n        \n        N = len(labels)\n        P = np.sum(labels)\n        \n        # Per problem statement, evaluate only at unique score values.\n        unique_scores = np.unique(scores)\n        \n        feasible_solutions = []\n        \n        for tau in unique_scores:\n            predictions = (scores >= tau).astype(int)\n            TP = np.sum((predictions == 1)  (labels == 1))\n            FP = np.sum((predictions == 1)  (labels == 0))\n            \n            predicted_positives = TP + FP\n\n            if P == 0:\n                rec = 0.0\n            else:\n                rec = TP / P\n\n            # Check feasibility first.\n            is_feasible = (predicted_positives = C) and (rec >= r_min)\n\n            if is_feasible:\n                # Calculate remaining metrics only for feasible thresholds.\n                TN = np.sum((predictions == 0)  (labels == 0))\n                \n                if predicted_positives == 0:\n                    prec = 0.0\n                else:\n                    prec = TP / predicted_positives\n                \n                if (prec + rec) == 0:\n                    f1 = 0.0\n                else:\n                    f1 = 2 * prec * rec / (prec + rec)\n\n                acc = (TP + TN) / N\n                \n                # Store all metrics for this feasible solution.\n                feasible_solutions.append({\n                    'prec': prec, \n                    'f1': f1, \n                    'tau': tau, \n                    'acc': acc, \n                    'rec': rec\n                })\n        \n        if not feasible_solutions:\n            result = [-1.0, 0.0, 0.0, 0.0, 0.0]\n        else:\n            # Sort using a key that implements the hierarchical tie-breaking rules:\n            # 1. Maximize precision (sort by -prec)\n            # 2. Maximize F1-score (sort by -f1)\n            # 3. Maximize threshold tau (sort by -tau)\n            best_solution = sorted(feasible_solutions, key=lambda x: (-x['prec'], -x['f1'], -x['tau']))[0]\n            \n            result = [\n                best_solution['tau'],\n                best_solution['acc'],\n                best_solution['prec'],\n                best_solution['rec'],\n                best_solution['f1']\n            ]\n        \n        # Format the result list into the required string format with 6 decimal places.\n        formatted_result = f\"[{','.join([f'{v:.6f}' for v in result])}]\"\n        all_results.append(formatted_result)\n\n    # Print a single line with all results, as specified.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "4551721"}]}