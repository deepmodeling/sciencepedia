{"hands_on_practices": [{"introduction": "Principal Component Analysis is fundamentally a tool for discovering the most important patterns of variation in a dataset. This practice delves into the core mechanism of how PCA handles redundant information. By working through a scenario with perfectly collinear features, you will see how concepts from linear algebra—such as matrix rank and the null space—directly translate into identifying and discarding redundant data, thereby revealing the true \"intrinsic dimension\" of the dataset without any loss of information [@problem_id:3191985].", "problem": "Consider a zero-mean random vector $\\mathbf{X} = (X_{1}, X_{2}, X_{3})^{\\top}$ constructed as follows. Let $(X_{1}, X_{2})^{\\top}$ be a jointly distributed, zero-mean pair with covariance matrix $\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$, and define $X_{3} = X_{1} + X_{2}$ almost surely. You perform Principal Components Analysis (PCA) on the feature triple $(X_{1}, X_{2}, X_{3})$ using the population covariance matrix.\n\nUsing only the definitions of covariance, matrix rank, eigenvalues and eigenvectors, and the PCA interpretation of variance explained by principal components, complete the following tasks:\n\n1) Construct the covariance matrix $\\Sigma$ of $(X_{1}, X_{2}, X_{3})^{\\top}$.\n\n2) From first principles, establish whether $\\Sigma$ is rank-deficient and identify any eigenvalue $\\lambda_{i}$ that equals $0$ by exhibiting a nontrivial vector in the null space.\n\n3) Argue what the exact intrinsic dimension is, that is, the smallest target dimension $k$ to which PCA can reduce $(X_{1}, X_{2}, X_{3})$ without any loss of variance, and justify your answer from the structure of $\\Sigma$.\n\n4) Compute the exact proportion of total variance explained by the first principal component (that is, the largest eigenvalue divided by the trace of $\\Sigma$). Express your final answer for this proportion as a reduced fraction. Do not use a percentage sign. No rounding is required.\n\nProvide your final numeric answer only for item $4$.", "solution": "The problem statement is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n- A zero-mean random vector $\\mathbf{X} = (X_{1}, X_{2}, X_{3})^{\\top}$, meaning $\\mathbb{E}[X_i] = 0$ for $i \\in \\{1, 2, 3\\}$.\n- The covariance matrix for the sub-vector $(X_{1}, X_{2})^{\\top}$ is $\\text{Cov}((X_1, X_2)^\\top) = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$. This implies $\\text{Var}(X_1) = 2$, $\\text{Var}(X_2) = 2$, and $\\text{Cov}(X_1, X_2) = 1$.\n- A linear dependency is defined: $X_{3} = X_{1} + X_{2}$ almost surely.\n- The analysis to be performed is Principal Components Analysis (PCA) on the population covariance matrix of $\\mathbf{X}$.\n- The tasks are: 1) Construct the covariance matrix $\\Sigma$ of $\\mathbf{X}$. 2) Establish rank-deficiency and find a zero eigenvalue. 3) Determine the intrinsic dimension for lossless reduction. 4) Compute the proportion of variance explained by the first principal component.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is set within the standard framework of multivariate statistics and linear algebra. The definitions of mean, covariance, PCA, eigenvalues, and rank are standard. The given $2 \\times 2$ covariance matrix is symmetric and positive definite (eigenvalues are $2+1=3$ and $2-1=1$), so it is a valid covariance matrix. The construction of a new random variable as a linear combination of others is a standard procedure. The problem is scientifically and mathematically sound.\n- **Well-Posed:** The problem is self-contained and provides all necessary information to construct the covariance matrix and perform the requested analysis. The questions are unambiguous and lead to a unique, derivable solution.\n- **Objective:** The problem is stated in precise mathematical language, free from subjective or opinion-based content.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid** as it is scientifically grounded, well-posed, and objective. A full solution will be provided.\n\n### Solution\n\nThe solution proceeds by addressing each of the four tasks described in the problem statement.\n\n**1) Construction of the covariance matrix $\\Sigma$**\n\nThe covariance matrix $\\Sigma$ of the random vector $\\mathbf{X} = (X_{1}, X_{2}, X_{3})^{\\top}$ is a $3 \\times 3$ matrix where the element $\\Sigma_{ij}$ is given by $\\text{Cov}(X_i, X_j)$. Since the random variables are zero-mean, $\\text{Cov}(X_i, X_j) = \\mathbb{E}[X_i X_j]$.\n\nThe given information directly provides the upper-left $2 \\times 2$ block of $\\Sigma$:\n- $\\Sigma_{11} = \\text{Var}(X_1) = 2$\n- $\\Sigma_{22} = \\text{Var}(X_2) = 2$\n- $\\Sigma_{12} = \\Sigma_{21} = \\text{Cov}(X_1, X_2) = 1$\n\nThe remaining elements involve $X_3$. Using the definition $X_3 = X_1 + X_2$ and the linearity of the covariance operator:\n- $\\Sigma_{13} = \\text{Cov}(X_1, X_3) = \\text{Cov}(X_1, X_1 + X_2) = \\text{Cov}(X_1, X_1) + \\text{Cov}(X_1, X_2) = \\text{Var}(X_1) + \\text{Cov}(X_1, X_2) = 2 + 1 = 3$. By symmetry, $\\Sigma_{31} = 3$.\n- $\\Sigma_{23} = \\text{Cov}(X_2, X_3) = \\text{Cov}(X_2, X_1 + X_2) = \\text{Cov}(X_2, X_1) + \\text{Cov}(X_2, X_2) = \\text{Cov}(X_1, X_2) + \\text{Var}(X_2) = 1 + 2 = 3$. By symmetry, $\\Sigma_{32} = 3$.\n- $\\Sigma_{33} = \\text{Var}(X_3) = \\text{Var}(X_1 + X_2) = \\text{Var}(X_1) + \\text{Var}(X_2) + 2\\text{Cov}(X_1, X_2) = 2 + 2 + 2(1) = 6$.\n\nAssembling these elements gives the full covariance matrix:\n$$ \\Sigma = \\begin{pmatrix} 2 & 1 & 3 \\\\ 1 & 2 & 3 \\\\ 3 & 3 & 6 \\end{pmatrix} $$\n\n**2) Rank-deficiency and zero eigenvalue**\n\nA matrix is rank-deficient if its columns (or rows) are not linearly independent. The linear dependency among the random variables, $X_3 = X_1 + X_2$, implies a linear dependency in the columns of the data matrix, which in turn leads to a rank-deficient covariance matrix. This dependency can be written as $1 \\cdot X_1 + 1 \\cdot X_2 - 1 \\cdot X_3 = 0$. The vector of coefficients, $\\mathbf{v} = (1, 1, -1)^{\\top}$, defines the specific linear combination that is zero. This vector $\\mathbf{v}$ will lie in the null space of $\\Sigma$.\n\nTo verify this from first principles, we show that $\\mathbf{v}$ is an eigenvector of $\\Sigma$ with a corresponding eigenvalue of $0$. We compute the product $\\Sigma\\mathbf{v}$:\n$$ \\Sigma\\mathbf{v} = \\begin{pmatrix} 2 & 1 & 3 \\\\ 1 & 2 & 3 \\\\ 3 & 3 & 6 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2(1) + 1(1) + 3(-1) \\\\ 1(1) + 2(1) + 3(-1) \\\\ 3(1) + 3(1) + 6(-1) \\end{pmatrix} = \\begin{pmatrix} 2 + 1 - 3 \\\\ 1 + 2 - 3 \\\\ 3 + 3 - 6 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0 \\cdot \\mathbf{v} $$\nSince $\\Sigma\\mathbf{v} = \\mathbf{0}$ for the non-trivial vector $\\mathbf{v} = (1, 1, -1)^{\\top}$, it is established that $\\lambda = 0$ is an eigenvalue of $\\Sigma$. The existence of a zero eigenvalue implies that the determinant of $\\Sigma$ is $0$, and thus the matrix is not full rank. It is rank-deficient.\n\n**3) Intrinsic dimension for lossless reduction**\n\nThe goal of PCA is to find an orthogonal basis (the principal components) for the data space. The eigenvalues of the covariance matrix $\\Sigma$ represent the variance of the data along these principal component axes. The total variance in the data is the sum of the eigenvalues, which is equal to the trace of $\\Sigma$.\n\nA dimensionality reduction to a target dimension $k$ is lossless if the first $k$ principal components capture $100\\%$ of the total variance. This occurs if and only if the sum of the first $k$ largest eigenvalues equals the total variance. Let the eigenvalues be ordered $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_p$. Lossless reduction to dimension $k$ requires $\\sum_{i=1}^k \\lambda_i = \\sum_{i=1}^p \\lambda_i$, which implies that the remaining eigenvalues $\\lambda_{k+1}, \\dots, \\lambda_p$ must be zero.\n\nThe intrinsic dimension is therefore the number of non-zero eigenvalues, which is equivalent to the rank of the covariance matrix $\\Sigma$.\n\nFrom part 2), we know that $\\Sigma$ has at least one zero eigenvalue, so its rank is less than $3$. To determine the exact rank, we can examine a submatrix. The upper-left $2 \\times 2$ submatrix is $\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$. Its determinant is $2(2) - 1(1) = 3 \\neq 0$. Since there exists a $2 \\times 2$ submatrix with a non-zero determinant, the rank of $\\Sigma$ is at least $2$.\n\nCombined, we have $\\text{rank}(\\Sigma) < 3$ and $\\text{rank}(\\Sigma) \\ge 2$. Thus, the rank of $\\Sigma$ is exactly $2$. This means there are exactly two non-zero eigenvalues. To capture all the variance, we must retain the principal components corresponding to these two non-zero eigenvalues. Therefore, the smallest target dimension $k$ for lossless reduction is $2$. This is the intrinsic dimension of the data, which lies on a 2D plane defined by $x_1 + x_2 - x_3 = 0$ within the 3D feature space.\n\n**4) Proportion of variance explained by the first principal component**\n\nThis proportion is given by the ratio of the largest eigenvalue, $\\lambda_1$, to the total variance, which is $\\text{Tr}(\\Sigma)$.\nThe total variance is:\n$$ \\text{Tr}(\\Sigma) = \\Sigma_{11} + \\Sigma_{22} + \\Sigma_{33} = 2 + 2 + 6 = 10 $$\nTo find the eigenvalues, we solve the characteristic equation $\\det(\\Sigma - \\lambda I) = 0$:\n$$ \\det \\begin{pmatrix} 2-\\lambda & 1 & 3 \\\\ 1 & 2-\\lambda & 3 \\\\ 3 & 3 & 6-\\lambda \\end{pmatrix} = 0 $$\nExpanding the determinant:\n$$ (2-\\lambda)((2-\\lambda)(6-\\lambda) - 9) - 1(1(6-\\lambda) - 9) + 3(3 - 3(2-\\lambda)) = 0 $$\n$$ (2-\\lambda)(\\lambda^2 - 8\\lambda + 12 - 9) - (6-\\lambda-9) + 3(3 - 6 + 3\\lambda) = 0 $$\n$$ (2-\\lambda)(\\lambda^2 - 8\\lambda + 3) - (-\\lambda - 3) + 3(3\\lambda - 3) = 0 $$\n$$ 2\\lambda^2 - 16\\lambda + 6 - \\lambda^3 + 8\\lambda^2 - 3\\lambda + \\lambda + 3 + 9\\lambda - 9 = 0 $$\nCombining terms:\n$$ -\\lambda^3 + (2+8)\\lambda^2 + (-16-3+1+9)\\lambda + (6+3-9) = 0 $$\n$$ -\\lambda^3 + 10\\lambda^2 - 9\\lambda = 0 $$\n$$ \\lambda^3 - 10\\lambda^2 + 9\\lambda = 0 $$\nFactorizing the polynomial:\n$$ \\lambda(\\lambda^2 - 10\\lambda + 9) = 0 $$\n$$ \\lambda(\\lambda-1)(\\lambda-9) = 0 $$\nThe eigenvalues are $\\lambda = 0$, $\\lambda = 1$, and $\\lambda = 9$.\nThe ordered eigenvalues are $\\lambda_1 = 9$, $\\lambda_2 = 1$, and $\\lambda_3 = 0$.\nThe largest eigenvalue is $\\lambda_1 = 9$.\nThe proportion of total variance explained by the first principal component is:\n$$ \\frac{\\lambda_1}{\\text{Tr}(\\Sigma)} = \\frac{9}{10} $$\nThis result is a reduced fraction as required.", "answer": "$$\\boxed{\\frac{9}{10}}$$", "id": "3191985"}, {"introduction": "In practical applications like radiomics, features are often measured in different units and on vastly different scales, from micrometers to unitless texture ratios. This exercise demonstrates a critical and non-negotiable preprocessing step: feature standardization. Through this coding practice, you will quantify how PCA's results can be skewed by features with large variance and understand why placing all features on a common scale is essential for a meaningful analysis of the underlying data structure [@problem_id:2430028].", "problem": "You are given a family of data matrices with columns on vastly different numerical scales. For each case, consider a real-valued data matrix $X \\in \\mathbb{R}^{n \\times d}$ with $d=3$ features and $n$ samples. Define $X$ deterministically by explicit formulas for each test case below. The task is to compare Principal Component Analysis (PCA) performed on the mean-centered data and on the standardized data, and to quantify how the dominant principal component direction and its explained variance fraction change due to standardization. Principal Component Analysis (PCA) is defined here as the eigen-decomposition of the sample covariance matrix of the transformed data. Standardization of features is defined here as centering each column (subtracting its sample mean) and dividing by its sample standard deviation; if a column has zero standard deviation, leave that standardized column identically zero after centering.\n\nDefinitions to use:\n- Given $X \\in \\mathbb{R}^{n \\times d}$, let $\\bar{\\mathbf{x}} \\in \\mathbb{R}^{d}$ be the column-wise sample mean and let $X_c = X - \\mathbf{1}\\bar{\\mathbf{x}}^\\top$ denote the centered data, where $\\mathbf{1} \\in \\mathbb{R}^{n}$ is the vector of ones. The sample covariance matrix is $S = \\frac{1}{n-1} X_c^\\top X_c$.\n- The PCA eigenvalues and eigenvectors are the eigenpairs $\\{(\\lambda_k,\\mathbf{v}_k)\\}_{k=1}^d$ of $S$, with $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\ge 0$ and orthonormal eigenvectors $\\{\\mathbf{v}_k\\}$. The explained variance ratio of the first principal component is $r_1 = \\frac{\\lambda_1}{\\sum_{j=1}^d \\lambda_j}$.\n- For standardized data, compute the column-wise sample standard deviations $\\sigma_j$ of $X_c$. Form $Z$ by $Z_{:,j} = X_{c,:,j}/\\sigma_j$ for all $j$ with $\\sigma_j \\neq 0$, and $Z_{:,j} = \\mathbf{0}$ for any $j$ with $\\sigma_j = 0$. Then define $S^{(z)} = \\frac{1}{n-1} Z^\\top Z$ and its eigenpairs $\\{(\\mu_k,\\mathbf{u}_k)\\}_{k=1}^d$ sorted $\\mu_1 \\ge \\mu_2 \\ge \\mu_3 \\ge 0$, with explained variance ratio $r_1^{(z)} = \\frac{\\mu_1}{\\sum_{j=1}^d \\mu_j}$.\n- The alignment between two unit principal directions $\\mathbf{v}_1$ and $\\mathbf{u}_1$ is measured by $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1| \\in [0,1]$. The sign indeterminacy of eigenvectors is handled by the absolute value.\n- To quantify component dominance by original axes, define $i_{\\mathrm{before}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{v}_1)_j|$ and $i_{\\mathrm{after}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{u}_1)_j|$, using zero-based indexing for feature indices.\n\nFor each test case, you must compute the following ordered list of quantities:\n- $r_1$ computed from $S$,\n- $r_1^{(z)}$ computed from $S^{(z)}$,\n- $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1|$,\n- $i_{\\mathrm{before}}$,\n- $i_{\\mathrm{after}}$.\n\nTest suite (each case defines $n$, then $t_i$ for $i \\in \\{0,\\dots,n-1\\}$, and the three features as functions of $t_i$):\n- Case $1$: $n=200$. For each $i \\in \\{0,\\dots,199\\}$, let $t_i = \\frac{i}{199}$, and define\n  - $x_{i1} = 1000 \\cos(2\\pi t_i)$,\n  - $x_{i2} = \\sin(2\\pi t_i) + 0.1 \\cos(4\\pi t_i)$,\n  - $x_{i3} = 0.001\\, t_i$.\n  Assemble $X$ by stacking these three features as columns.\n- Case $2$: $n=100$. For each $i \\in \\{0,\\dots,99\\}$, let $t_i = \\frac{i}{99}$, and define\n  - $x_{i1} = 10^6\\, t_i$,\n  - $x_{i2} = 0$,\n  - $x_{i3} = 10\\,(t_i - 0.5)$.\n  Assemble $X$ by stacking these three features as columns.\n- Case $3$: $n=150$. For each $i \\in \\{0,\\dots,149\\}$, let $t_i = \\frac{i}{149}$, and define\n  - $x_{i1} = 1000\\,(2 t_i - 1)$,\n  - $x_{i2} = (2 t_i - 1) + 0.1 \\sin(3\\pi t_i)$,\n  - $x_{i3} = 0.01 \\cos(5\\pi t_i)$.\n  Assemble $X$ by stacking these three features as columns.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output the ordered list $[r_1, r_1^{(z)}, a, i_{\\mathrm{before}}, i_{\\mathrm{after}}]$. Aggregate the three cases into a single list of three lists, in the order of cases $1$, $2$, $3$. For example, the overall printed structure must be of the form $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]]$.\n\nAll answers are dimensionless real numbers or integers as specified. No physical units or angle units are required because all requested quantities are pure numbers.", "solution": "The user has provided a problem that is valid and requires a solution. The problem statement is scientifically grounded in the fields of linear algebra and statistics, specifically Principal Component Analysis (PCA). It is well-posed, providing deterministic instructions for constructing the data, defining all necessary mathematical objects and procedures, and requesting a set of specific, computable quantities. The language is objective and free of ambiguity. Therefore, a reasoned, step-by-step solution can be constructed.\n\nThe problem requires a comparison of PCA performed on mean-centered data versus standardized data for three distinct cases. The core of the problem lies in observing how scaling affects the outcome of PCA. PCA identifies the directions of maximum variance in a dataset. When features (columns of the data matrix) have vastly different scales, the feature with the largest variance will dominate the first principal component, regardless of the underlying data structure. Standardization, which rescales each feature to have a mean of $0$ and a standard deviation of $1$, prevents this by placing all features on an equal footing.\n\nThe overall procedure is as follows:\n$1$. For each test case, construct the $n \\times d$ data matrix $X$, where $d=3$.\n$2$. Perform PCA on the mean-centered data $X_c$.\n    a. Compute the column-wise sample mean vector $\\bar{\\mathbf{x}}$.\n    b. Center the data: $X_c = X - \\mathbf{1}\\bar{\\mathbf{x}}^\\top$.\n    c. Compute the sample covariance matrix $S = \\frac{1}{n-1} X_c^\\top X_c$.\n    d. Find the eigenvalues $\\lambda_k$ and eigenvectors $\\mathbf{v}_k$ of $S$ by solving the eigenproblem $S\\mathbf{v}_k = \\lambda_k\\mathbf{v}_k$. The eigenvalues are sorted, $\\lambda_1 \\ge \\lambda_2 \\ge \\lambda_3$, and the eigenvectors $\\{\\mathbf{v}_k\\}$ are orthonormal. The first principal component direction is $\\mathbf{v}_1$.\n    e. Compute the explained variance ratio for the first component: $r_1 = \\lambda_1 / (\\sum_{j=1}^d \\lambda_j)$.\n    f. Identify the original feature that dominates $\\mathbf{v}_1$: $i_{\\mathrm{before}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{v}_1)_j|$.\n\n$3$. Perform PCA on the standardized data $Z$.\n    a. Compute the column-wise sample standard deviations, $\\sigma_j$, of $X$ using a divisor of $n-1$.\n    b. Construct the standardized data matrix $Z$. Each column $Z_{:,j}$ is obtained by scaling the corresponding centered column $X_{c,:,j}$ by $1/\\sigma_j$. If $\\sigma_j=0$, the column $Z_{:,j}$ is set to a zero vector.\n    c. Compute the sample covariance matrix of $Z$: $S^{(z)} = \\frac{1}{n-1} Z^\\top Z$. This matrix is equivalent to the sample correlation matrix of $X$. Its diagonal entries are $1$ for any non-constant feature.\n    d. Find the eigenvalues $\\mu_k$ and eigenvectors $\\mathbf{u}_k$ of $S^{(z)}$, sorted such that $\\mu_1 \\ge \\mu_2 \\ge \\mu_3$. The first principal component direction of the standardized data is $\\mathbf{u}_1$.\n    e. Compute the corresponding explained variance ratio: $r_1^{(z)} = \\mu_1 / (\\sum_{j=1}^d \\mu_j)$. The sum in the denominator, $\\text{Tr}(S^{(z)})$, equals the number of non-constant features.\n    f. Identify the original feature that dominates $\\mathbf{u}_1$: $i_{\\mathrm{after}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{u}_1)_j|$.\n\n$4$. Compare the results from the two analyses by computing the alignment metric $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1|$, which measures the cosine of the angle between the two principal directions.\n\n$5$. For each case, the final output is the ordered list $[r_1, r_1^{(z)}, a, i_{\\mathrm{before}}, i_{\\mathrm{after}}]$.\n\nCase-specific analysis:\n- **Case 1**: The data consists of three features with scales $O(10^3)$, $O(1)$, and $O(10^{-3})$. The variance of the first feature, $x_1 = 1000 \\cos(2\\pi t_i)$, will be overwhelmingly larger than the others. Thus, the first principal component $\\mathbf{v}_1$ of the unstandardized data is expected to align almost perfectly with the first feature axis. This will yield $r_1 \\approx 1$ and $i_{\\mathrm{before}} = 0$. After standardization, all features have unit variance, and the structural relationship (an elliptical trajectory in the $(x_1, x_2)$ plane) will become apparent. The variance will be distributed more equitably, leading to a smaller $r_1^{(z)}$, and $\\mathbf{u}_1$ will be a combination of features $1$ and $2$.\n- **Case 2**: The first feature, $x_1 = 10^6 t_i$, has a massive scale. The second feature, $x_2=0$, is constant and has zero variance. The third feature, $x_3 = 10(t_i-0.5)$, has a much smaller scale than the first. For the unstandardized data, PCA will be dominated by feature $1$, giving $r_1 \\approx 1$ and $i_{\\mathrm{before}}=0$. After standardization, the constant feature $x_2$ remains a zero vector. Features $1$ and $3$ are both linear functions of $t_i$ and will become perfectly correlated after centering and scaling. Their standardized versions will be identical, $Z_{:,1} = Z_{:,3}$. The data will collapse onto a single direction in the $(Z_1, Z_3)$ plane. This will result in $r_1^{(z)}=1$ (as the effective rank is $1$ among the non-constant features) and an eigenvector $\\mathbf{u}_1$ of the form $[1/\\sqrt{2}, 0, 1/\\sqrt{2}]^\\top$.\n- **Case 3**: The first feature, $x_1 = 1000(2t_i-1)$, has a large scale. The second feature, $x_2 = (2t_i-1) + 0.1 \\sin(3\\pi t_i)$, is highly correlated with the first but has a much smaller scale. The third feature is of negligible scale. As with the other cases, unstandardized PCA will be dictated by the first feature's scale, so $r_1 \\approx 1$ and $i_{\\mathrm{before}}=0$. After standardization, the strong linear relationship between features $1$ and $2$ will be the most prominent characteristic. The first principal component $\\mathbf{u}_1$ will capture this shared variance, representing a direction along the major axis of the correlated cloud, roughly at a $45$-degree angle between the standardized axes of features $1$ and $2$.\n\nThe implementation will utilize `numpy` for all numerical computations, particularly `numpy.linalg.eigh` for the eigendecomposition of the symmetric covariance matrices. Care will be taken to handle the sorting of eigenvalues in descending order and the case of zero standard deviation.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA comparison problem for three given test cases.\n    \"\"\"\n\n    def generate_case_1_data():\n        n = 200\n        t = np.linspace(0, 1, n)\n        x1 = 1000 * np.cos(2 * np.pi * t)\n        x2 = np.sin(2 * np.pi * t) + 0.1 * np.cos(4 * np.pi * t)\n        x3 = 0.001 * t\n        return np.stack([x1, x2, x3], axis=1)\n\n    def generate_case_2_data():\n        n = 100\n        t = np.linspace(0, 1, n)\n        x1 = 1e6 * t\n        x2 = np.zeros(n)\n        x3 = 10 * (t - 0.5)\n        return np.stack([x1, x2, x3], axis=1)\n\n    def generate_case_3_data():\n        n = 150\n        t = np.linspace(0, 1, n)\n        x1 = 1000 * (2 * t - 1)\n        x2 = (2 * t - 1) + 0.1 * np.sin(3 * np.pi * t)\n        x3 = 0.01 * np.cos(5 * np.pi * t)\n        return np.stack([x1, x2, x3], axis=1)\n\n    def perform_pca_analysis(X):\n        \"\"\"\n        Performs PCA on both mean-centered and standardized data, returning the required metrics.\n        \"\"\"\n        n, d = X.shape\n        \n        # --- PCA on Mean-Centered Data ---\n        X_c = X - X.mean(axis=0)\n        S = (X_c.T @ X_c) / (n - 1)\n        \n        # Eigendecomposition. eigh returns sorted eigenvalues (ascending).\n        # We reverse them to get principal components in descending order of variance.\n        evals, evecs = np.linalg.eigh(S)\n        evals = evals[::-1]\n        evecs = evecs[:, ::-1]\n        \n        lambda_1 = evals[0]\n        v1 = evecs[:, 0]\n        \n        # Explained variance ratio\n        total_variance = np.sum(evals)\n        r1 = lambda_1 / total_variance if total_variance > 0 else 0\n        \n        # Dominant feature index\n        i_before = np.argmax(np.abs(v1))\n\n        # --- PCA on Standardized Data ---\n        stds = np.std(X_c, axis=0, ddof=1)\n        \n        # Handle features with zero standard deviation\n        Z = np.zeros_like(X_c)\n        non_zero_std_mask = stds > 0\n        if np.any(non_zero_std_mask):\n            Z[:, non_zero_std_mask] = X_c[:, non_zero_std_mask] / stds[non_zero_std_mask]\n\n        S_z = (Z.T @ Z) / (n - 1)\n        \n        # Eigendecomposition of the correlation matrix\n        mu, u = np.linalg.eigh(S_z)\n        mu = mu[::-1]\n        u = u[:, ::-1]\n        \n        mu_1 = mu[0]\n        u1 = u[:, 0]\n        \n        # Explained variance ratio for standardized data\n        total_variance_z = np.sum(mu)\n        r1_z = mu_1 / total_variance_z if total_variance_z > 0 else 0\n        \n        # Dominant feature index after standardization\n        i_after = np.argmax(np.abs(u1))\n        \n        # Alignment between the first principal components\n        alignment = np.abs(np.dot(v1, u1))\n        \n        return [r1, r1_z, alignment, int(i_before), int(i_after)]\n\n    test_cases = [\n        generate_case_1_data,\n        generate_case_2_data,\n        generate_case_3_data\n    ]\n    \n    all_results = []\n    for case_generator in test_cases:\n        X = case_generator()\n        result = perform_pca_analysis(X)\n        all_results.append(result)\n\n    # Format the output string without spaces as [[...],[...],[...]]\n    inner_strings = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "2430028"}, {"introduction": "Real-world data is often imperfect and may contain measurement errors or anomalous samples known as outliers. Because PCA seeks to maximize variance, it can be highly sensitive to these extreme values. This hands-on problem allows you to explore PCA's robustness by quantifying how a single, distant outlier can pull the principal components towards it, potentially misrepresenting the structure of the rest of the data and highlighting the importance of data quality checks in any analysis [@problem_id:2430058].", "problem": "You are to quantify how a single, distant outlier affects the direction of the first principal component in a $2$-dimensional dataset. Consider a deterministic base cloud of $M$ points lying on an ellipse centered at the origin, defined by the parametric set\n$$\n\\mathbf{x}_k = \\begin{bmatrix} r_x \\cos\\left(\\tfrac{2\\pi k}{M}\\right) \\\\ r_y \\sin\\left(\\tfrac{2\\pi k}{M}\\right) \\end{bmatrix}, \\quad k = 0,1,2,\\dots,M-1,\n$$\nwith $r_x \\gt 0$ and $r_y \\gt 0$. Form an augmented dataset by adding one additional point (the outlier) at coordinates\n$$\n\\mathbf{z} = \\begin{bmatrix} o_x \\\\ o_y \\end{bmatrix}.\n$$\nLet $\\mathbf{S}_0$ be the sample covariance matrix of the base cloud and $\\mathbf{S}_1$ be the sample covariance matrix of the augmented dataset that contains the $M$ ellipse points and the outlier. The first principal component is defined as the unit eigenvector of the respective sample covariance matrix associated with its largest eigenvalue. Denote by $\\mathbf{u}_0 \\in \\mathbb{R}^2$ the unit eigenvector corresponding to the largest eigenvalue of $\\mathbf{S}_0$, and by $\\mathbf{u}_1 \\in \\mathbb{R}^2$ the analogous eigenvector for $\\mathbf{S}_1$. Because eigenvectors are defined up to a sign, define the acute angular difference $\\Delta$ between the two principal directions as\n$$\n\\Delta = \\arccos\\!\\left(\\left|\\mathbf{u}_0^\\top \\mathbf{u}_1\\right|\\right).\n$$\nYou must compute $\\Delta$ in radians. All final numerical answers must be expressed in radians and rounded to $6$ decimal places.\n\nTest suite. For each parameter tuple $(r_x, r_y, M, o_x, o_y)$ below, construct the base cloud and augmented dataset as specified, compute $\\Delta$, and report the result:\n- Case $1$ (general case): $(r_x, r_y, M, o_x, o_y) = (3.0, 1.0, 60, 10.0, 10.0)$.\n- Case $2$ (boundary, no effect): $(r_x, r_y, M, o_x, o_y) = (3.0, 1.0, 60, 0.0, 0.0)$.\n- Case $3$ (edge, outlier dominates orthogonal direction): $(r_x, r_y, M, o_x, o_y) = (2.0, 1.0, 40, 0.0, 1000.0)$.\n- Case $4$ (near-isotropic base, moderate outlier): $(r_x, r_y, M, o_x, o_y) = (1.05, 1.0, 50, 5.0, 0.2)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the cases above, with each value rounded to $6$ decimal places. For example, a valid output format is\n\"[x_1,x_2,x_3,x_4]\"\nwhere each $x_i$ is a float in radians with exactly $6$ digits after the decimal point.", "solution": "The problem is subjected to validation and is deemed valid. It is a well-posed problem in computational physics and linear algebra, resting on established scientific principles. All terms are defined with sufficient rigor, and the provided data are consistent and complete.\n\nThe task is to compute the angular deviation, $\\Delta$, of the first principal component of a $2$-dimensional dataset when a single outlier is introduced. The base dataset is a cloud of $M$ points uniformly distributed on an ellipse, and the augmented dataset includes one additional outlier point.\n\nThe first principal component of a dataset is the direction of maximum variance, which corresponds to the unit eigenvector associated with the largest eigenvalue of the sample covariance matrix. Let the set of $N$ data points be $\\{\\mathbf{p}_i\\}_{i=1}^N$, where each $\\mathbf{p}_i \\in \\mathbb{R}^2$. The sample mean is $\\bar{\\mathbf{p}} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{p}_i$. The sample covariance matrix $\\mathbf{S}$ is given by:\n$$\n\\mathbf{S} = \\frac{1}{N-1}\\sum_{i=1}^N (\\mathbf{p}_i - \\bar{\\mathbf{p}})(\\mathbf{p}_i - \\bar{\\mathbf{p}})^\\top\n$$\nNote that the specific choice of denominator, whether $N$ or $N-1$, is inconsequential as it only scales the covariance matrix and does not alter its eigenvectors. The numerical implementation will use the conventional unbiased estimator with a denominator of $N-1$.\n\nFirst, we analyze the base cloud of $M$ points, $\\{\\mathbf{x}_k\\}_{k=0}^{M-1}$, defined by:\n$$\n\\mathbf{x}_k = \\begin{bmatrix} r_x \\cos\\left(\\frac{2\\pi k}{M}\\right) \\\\ r_y \\sin\\left(\\frac{2\\pi k}{M}\\right) \\end{bmatrix}, \\quad k = 0, 1, \\dots, M-1\n$$\nDue to the symmetry of the cosine and sine functions over a full cycle, the sample mean $\\bar{\\mathbf{x}}_0$ of the base cloud is the zero vector for $M > 1$:\n$$\n\\bar{\\mathbf{x}}_0 = \\frac{1}{M}\\sum_{k=0}^{M-1} \\mathbf{x}_k = \\begin{bmatrix} \\frac{r_x}{M}\\sum_{k=0}^{M-1}\\cos(\\frac{2\\pi k}{M}) \\\\ \\frac{r_y}{M}\\sum_{k=0}^{M-1}\\sin(\\frac{2\\pi k}{M}) \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n$$\nThe sample covariance matrix of the base cloud, $\\mathbf{S}_0$, is then:\n$$\n\\mathbf{S}_0 = \\frac{1}{M-1}\\sum_{k=0}^{M-1} \\mathbf{x}_k \\mathbf{x}_k^\\top = \\frac{1}{M-1} \\sum_{k=0}^{M-1} \\begin{bmatrix} r_x^2 \\cos^2(\\theta_k) & r_x r_y \\cos(\\theta_k)\\sin(\\theta_k) \\\\ r_x r_y \\cos(\\theta_k)\\sin(\\theta_k) & r_y^2 \\sin^2(\\theta_k) \\end{bmatrix}\n$$\nwhere $\\theta_k = \\frac{2\\pi k}{M}$. For $M > 2$, the sums of the off-diagonal terms evaluate to zero. The sums of the diagonal terms are $\\sum \\cos^2(\\theta_k) = M/2$ and $\\sum \\sin^2(\\theta_k) = M/2$. Thus, $\\mathbf{S}_0$ is a diagonal matrix:\n$$\n\\mathbf{S}_0 = \\frac{M}{2(M-1)} \\begin{bmatrix} r_x^2 & 0 \\\\ 0 & r_y^2 \\end{bmatrix}\n$$\nThe eigenvectors of a diagonal matrix are the standard basis vectors. The largest eigenvalue corresponds to the larger of $r_x^2$ and $r_y^2$. If $r_x > r_y$, the first principal component is $\\mathbf{u}_0 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$. If $r_y > r_x$, it is $\\mathbf{u}_0 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$. This aligns with the semi-major axis of the ellipse.\n\nNext, we analyze the augmented dataset, which comprises the $M$ points of the base cloud plus the outlier $\\mathbf{z} = \\begin{bmatrix} o_x \\\\ o_y \\end{bmatrix}$. The total number of points is $N = M+1$. The mean of the augmented dataset, $\\bar{\\mathbf{x}}_1$, is:\n$$\n\\bar{\\mathbf{x}}_1 = \\frac{1}{M+1}\\left(\\sum_{k=0}^{M-1}\\mathbf{x}_k + \\mathbf{z}\\right) = \\frac{1}{M+1}\\mathbf{z}\n$$\nThe covariance matrix $\\mathbf{S}_1$ is calculated over this set of $M+1$ points.\n$$\n\\mathbf{S}_1 = \\frac{1}{M} \\left( \\sum_{k=0}^{M-1}(\\mathbf{x}_k - \\bar{\\mathbf{x}}_1)(\\mathbf{x}_k - \\bar{\\mathbf{x}}_1)^\\top + (\\mathbf{z} - \\bar{\\mathbf{x}}_1)(\\mathbf{z} - \\bar{\\mathbf{x}}_1)^\\top \\right)\n$$\nIn general, $\\mathbf{S}_1$ is not a diagonal matrix. The outlier, especially if its coordinates $(o_x, o_y)$ are large, will significantly shift the mean and contribute a large term to the covariance sum, thereby rotating the principal axes of the data distribution. The first principal component $\\mathbf{u}_1$ is the unit eigenvector corresponding to the largest eigenvalue of $\\mathbf{S}_1$. This is found by performing an eigendecomposition of the numerically computed $\\mathbf{S}_1$.\n\nThe angular difference $\\Delta$ is computed as the acute angle between the two principal component vectors $\\mathbf{u}_0$ and $\\mathbf{u}_1$:\n$$\n\\Delta = \\arccos(|\\mathbf{u}_0^\\top \\mathbf{u}_1|)\n$$\nThe absolute value of the dot product ensures that the angle is acute, accounting for the fact that an eigenvector and its negative are equivalent.\n\nThe algorithmic procedure to solve for each test case is as follows:\n$1$. Construct the base dataset of $M$ points on the ellipse defined by $r_x$ and $r_y$.\n$2$. Compute the sample covariance matrix $\\mathbf{S}_0$ for the base dataset.\n$3$. Find the eigenvectors and eigenvalues of $\\mathbf{S}_0$. The eigenvector $\\mathbf{u}_0$ corresponding to the largest eigenvalue is the first principal component.\n$4$. Construct the augmented dataset by adding the outlier point $\\mathbf{z}=(o_x, o_y)$ to the base dataset.\n$5$. Compute the sample covariance matrix $\\mathbf{S}_1$ for the augmented dataset.\n$6$. Find the eigenvectors and eigenvalues of $\\mathbf{S}_1$. The eigenvector $\\mathbf{u}_1$ corresponding to the largest eigenvalue is the new first principal component.\n$7$. Calculate the angular difference $\\Delta = \\arccos(|\\mathbf{u}_0 \\cdot \\mathbf{u}_1|)$ in radians. The dot product argument should be clipped to the range $[-1, 1]$ to avoid numerical errors.\n$8$. Round the result to $6$ decimal places.\nThis procedure is implemented for each provided set of parameters.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the angular difference in the first principal component of a 2D dataset\n    due to the addition of an outlier.\n    \"\"\"\n    test_cases = [\n        # (r_x, r_y, M, o_x, o_y)\n        (3.0, 1.0, 60, 10.0, 10.0),\n        (3.0, 1.0, 60, 0.0, 0.0),\n        (2.0, 1.0, 40, 0.0, 1000.0),\n        (1.05, 1.0, 50, 5.0, 0.2),\n    ]\n\n    results = []\n    \n    for r_x, r_y, M, o_x, o_y in test_cases:\n        # Step 1: Construct the base dataset\n        theta = (2 * np.pi / M) * np.arange(M)\n        x_coords = r_x * np.cos(theta)\n        y_coords = r_y * np.sin(theta)\n        base_cloud = np.stack((x_coords, y_coords), axis=1)\n\n        # Step 2 & 3: PCA on the base cloud\n        # Covariance matrix for a centered ellipse is diagonal, so we can determine u0 analytically.\n        # This is more robust and faster than numerical computation for this specific geometry.\n        # Although numerical computation would yield the same result.\n        # Let's use numerical computation for generality and to match the full algorithm described.\n        # The choice of divisor (N or N-1) does not affect the eigenvectors.\n        # np.cov uses N-1 by default.\n        cov_0 = np.cov(base_cloud, rowvar=False)\n        eigvals_0, eigvecs_0 = np.linalg.eigh(cov_0)\n        # eigh sorts eigenvalues in ascending order, so the last eigenvector is the principal one.\n        u_0 = eigvecs_0[:, -1]\n\n        # Step 4: Construct the augmented dataset\n        outlier = np.array([[o_x, o_y]])\n        augmented_cloud = np.vstack((base_cloud, outlier))\n\n        # Step 5 & 6: PCA on the augmented dataset\n        cov_1 = np.cov(augmented_cloud, rowvar=False)\n        eigvals_1, eigvecs_1 = np.linalg.eigh(cov_1)\n        u_1 = eigvecs_1[:, -1]\n\n        # Step 7: Compute the angular difference\n        # Dot product between the two unit eigenvectors\n        dot_product = np.dot(u_0, u_1)\n        \n        # Take the absolute value to find the acute angle\n        abs_dot_product = np.abs(dot_product)\n        \n        # Clip to handle potential floating point inaccuracies > 1.0\n        clipped_dot = np.clip(abs_dot_product, -1.0, 1.0)\n        \n        delta = np.arccos(clipped_dot)\n\n        # Step 8: Append rounded result\n        results.append(round(delta, 6))\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"{res:.6f}\" for res in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "2430058"}]}