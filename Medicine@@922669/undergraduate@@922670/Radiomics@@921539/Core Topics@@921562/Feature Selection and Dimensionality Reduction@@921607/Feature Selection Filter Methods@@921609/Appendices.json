{"hands_on_practices": [{"introduction": "A common task in radiomics is to identify features that differ significantly across multiple clinical groups, such as different tumor grades or treatment responses. This practice moves beyond simple two-group comparisons by exploring the Analysis of Variance (ANOVA), a powerful filter method that partitions the total data variability into between-group and within-group components. By deriving the ANOVA $F$-statistic from the fundamental principle of orthogonal variance decomposition, you will gain a deeper, geometric understanding of how this cornerstone statistical test operates. [@problem_id:4539195]", "problem": "In radiomics feature selection using filter methods, a common approach for ranking a continuous feature across discrete outcome groups is Analysis of Variance (ANOVA). Consider a single radiomic texture feature (for example, gray-level co-occurrence matrix contrast) measured on tumors assigned to three clinical response groups. Let the groups be indexed by $i \\in \\{1,2,3\\}$, with group sizes $n_{1}$, $n_{2}$, and $n_{3}$, and observations within group $i$ denoted by $\\{y_{ij}\\}_{j=1}^{n_{i}}$. Let the total sample size be $N = n_{1} + n_{2} + n_{3}$. Define the group means $\\bar{y}_{i\\cdot} = \\frac{1}{n_{i}}\\sum_{j=1}^{n_{i}} y_{ij}$ and the overall (grand) mean $\\bar{y}_{\\cdot\\cdot} = \\frac{1}{N}\\sum_{i=1}^{3}\\sum_{j=1}^{n_{i}} y_{ij}$.\n\nStarting only from the definitions of these means and the Euclidean inner product on $\\mathbb{R}^{N}$, derive the orthogonal decomposition of the total centered sum of squares $\\sum_{i=1}^{3}\\sum_{j=1}^{n_{i}}(y_{ij}-\\bar{y}_{\\cdot\\cdot})^{2}$ into a between-group component and a within-group component by showing that the cross-term vanishes due to orthogonality. Use this decomposition and dimension (degrees-of-freedom) arguments to obtain the one-way ANOVA $F$-statistic as a ratio of a between-group mean square to a within-group mean square.\n\nThen, apply your derived expression to the following dataset of a single radiomic feature across three groups. The groups correspond to complete responders ($i=1$), partial responders ($i=2$), and non-responders ($i=3$). The observed values are:\n- Group $1$ ($n_{1}=4$): $1.8$, $2.1$, $1.9$, $2.2$.\n- Group $2$ ($n_{2}=5$): $2.4$, $2.5$, $2.7$, $2.6$, $2.8$.\n- Group $3$ ($n_{3}=4$): $3.1$, $3.0$, $3.2$, $2.9$.\n\nCompute the numerical value of the one-way ANOVA $F$-statistic for this feature using your derivation. Round your final numeric answer to $4$ significant figures.", "solution": "The problem requires the derivation of the one-way Analysis of Variance (ANOVA) $F$-statistic and its application to a specific dataset. The solution is structured into two main parts: first, the theoretical derivation, and second, the numerical computation.\n\n### Part 1: Derivation of the ANOVA $F$-statistic\n\nWe begin by establishing the framework. We are given $k$ groups of observations, where for this problem $k=3$. Let $y_{ij}$ be the $j$-th observation in the $i$-th group, for $i \\in \\{1, 2, ..., k\\}$ and $j \\in \\{1, 2, ..., n_i\\}$. The total number of observations is $N = \\sum_{i=1}^{k} n_i$.\n\nThe group mean for group $i$ is $\\bar{y}_{i\\cdot} = \\frac{1}{n_{i}}\\sum_{j=1}^{n_{i}} y_{ij}$.\nThe overall or grand mean is $\\bar{y}_{\\cdot\\cdot} = \\frac{1}{N}\\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}} y_{ij} = \\frac{1}{N}\\sum_{i=1}^{k} n_i \\bar{y}_{i\\cdot}$.\n\nThe total variance in the data is captured by the Total Sum of Squares (SST), which measures the squared deviation of each observation from the grand mean:\n$$SST = \\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}}(y_{ij}-\\bar{y}_{\\cdot\\cdot})^{2}$$\n\nThe core idea of ANOVA is to decompose this total variation into components attributable to different sources. We decompose the deviation of a single data point from the grand mean by introducing the group mean:\n$$(y_{ij} - \\bar{y}_{\\cdot\\cdot}) = (y_{ij} - \\bar{y}_{i\\cdot}) + (\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot})$$\nThe first term, $(y_{ij} - \\bar{y}_{i\\cdot})$, represents the deviation of an observation from its own group mean (within-group deviation). The second term, $(\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot})$, represents the deviation of the group mean from the grand mean (between-group deviation).\n\nTo formalize this using the language of linear algebra, we can represent the $N$ observations as a vector $\\mathbf{y} \\in \\mathbb{R}^N$. Let's define three vectors in $\\mathbb{R}^N$ whose components (indexed by pairs $(i,j)$) are:\n1. Total deviation vector $\\mathbf{v}_{T}$: components are $(y_{ij} - \\bar{y}_{\\cdot\\cdot})$.\n2. Within-group deviation vector $\\mathbf{v}_{W}$: components are $(y_{ij} - \\bar{y}_{i\\cdot})$.\n3. Between-group deviation vector $\\mathbf{v}_{B}$: components are $(\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot})$.\n\nFrom the algebraic identity above, we have the vector decomposition $\\mathbf{v}_{T} = \\mathbf{v}_{W} + \\mathbf{v}_{B}$.\nThe Total Sum of Squares is the squared Euclidean norm of $\\mathbf{v}_{T}$:\n$$SST = ||\\mathbf{v}_{T}||^2 = ||\\mathbf{v}_{W} + \\mathbf{v}_{B}||^2$$\nUsing the property of the inner product $\\langle \\cdot, \\cdot \\rangle$, where $||\\mathbf{a}||^2 = \\langle\\mathbf{a}, \\mathbf{a}\\rangle$:\n$$||\\mathbf{v}_{W} + \\mathbf{v}_{B}||^2 = \\langle \\mathbf{v}_{W} + \\mathbf{v}_{B}, \\mathbf{v}_{W} + \\mathbf{v}_{B} \\rangle = ||\\mathbf{v}_{W}||^2 + ||\\mathbf{v}_{B}||^2 + 2\\langle \\mathbf{v}_{W}, \\mathbf{v}_{B} \\rangle$$\nThe problem requires showing that the cross-term, $2\\langle \\mathbf{v}_{W}, \\mathbf{v}_{B} \\rangle$, vanishes due to orthogonality. The inner product is:\n$$\\langle \\mathbf{v}_{W}, \\mathbf{v}_{B} \\rangle = \\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}} (y_{ij} - \\bar{y}_{i\\cdot})(\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot})$$\nWe can rearrange the summation:\n$$\\langle \\mathbf{v}_{W}, \\mathbf{v}_{B} \\rangle = \\sum_{i=1}^{k} \\left( (\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot}) \\sum_{j=1}^{n_{i}} (y_{ij} - \\bar{y}_{i\\cdot}) \\right)$$\nNow we evaluate the inner sum for a given group $i$:\n$$\\sum_{j=1}^{n_{i}} (y_{ij} - \\bar{y}_{i\\cdot}) = \\left(\\sum_{j=1}^{n_{i}} y_{ij}\\right) - \\left(\\sum_{j=1}^{n_{i}} \\bar{y}_{i\\cdot}\\right)$$\nBy definition of the group mean, $\\sum_{j=1}^{n_{i}} y_{ij} = n_i \\bar{y}_{i\\cdot}$. The second term is a sum of a constant, so $\\sum_{j=1}^{n_{i}} \\bar{y}_{i\\cdot} = n_i \\bar{y}_{i\\cdot}$. Thus, the inner sum is:\n$$n_i \\bar{y}_{i\\cdot} - n_i \\bar{y}_{i\\cdot} = 0$$\nSince this holds for every group $i$, the entire inner product is zero:\n$$\\langle \\mathbf{v}_{W}, \\mathbf{v}_{B} \\rangle = \\sum_{i=1}^{k} (\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot}) \\cdot 0 = 0$$\nThis proves that the vectors $\\mathbf{v}_{W}$ and $\\mathbf{v}_{B}$ are orthogonal. The decomposition of the sum of squares is therefore:\n$$SST = ||\\mathbf{v}_{W}||^2 + ||\\mathbf{v}_{B}||^2$$\nWe define the Within-group Sum of Squares (SSW) and the Between-group Sum of Squares (SSB):\n$$SSW = ||\\mathbf{v}_{W}||^2 = \\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}}(y_{ij} - \\bar{y}_{i\\cdot})^{2}$$\n$$SSB = ||\\mathbf{v}_{B}||^2 = \\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}}(\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot})^{2} = \\sum_{i=1}^{k}n_{i}(\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot})^{2}$$\nThus, we have derived the fundamental decomposition of ANOVA: $SST = SSW + SSB$.\n\nNext, we introduce the concept of degrees of freedom (df).\n- The $df$ for SST is $df_T = N-1$, as there are $N$ data points but one parameter (the grand mean $\\bar{y}_{\\cdot\\cdot}$) is used.\n- The $df$ for SSB is $df_B = k-1$, as it is based on the variance of the $k$ group means around the grand mean.\n- The $df$ for SSW is $df_W = N-k$, as for each of the $k$ groups, one degree of freedom is used to calculate its own mean $\\bar{y}_{i\\cdot}$. The total is $\\sum_{i=1}^k (n_i - 1) = N-k$.\nNotice that $df_T = df_B + df_W$, paralleling the sum of squares decomposition.\n\nA sum of squares divided by its degrees of freedom is called a Mean Square (MS).\n- Between-group Mean Square: $MSB = \\frac{SSB}{df_B} = \\frac{\\sum_{i=1}^{k}n_{i}(\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot})^{2}}{k-1}$\n- Within-group Mean Square: $MSW = \\frac{SSW}{df_W} = \\frac{\\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}}(y_{ij} - \\bar{y}_{i\\cdot})^{2}}{N-k}$\n\nThe one-way ANOVA $F$-statistic is the ratio of the between-group variance estimate to the within-group variance estimate:\n$$F = \\frac{MSB}{MSW}$$\nUnder the null hypothesis that all group means are equal, this statistic follows an $F$-distribution with $(k-1, N-k)$ degrees of freedom. A large $F$-value suggests that the variation between groups is large relative to the variation within groups, providing evidence to reject the null hypothesis.\n\n### Part 2: Numerical Computation\n\nWe apply the derived formulas to the given dataset.\nThe number of groups is $k=3$.\n- Group $1$: $\\{1.8, 2.1, 1.9, 2.2\\}$; $n_{1}=4$.\n- Group $2$: $\\{2.4, 2.5, 2.7, 2.6, 2.8\\}$; $n_{2}=5$.\n- Group $3$: $\\{3.1, 3.0, 3.2, 2.9\\}$; $n_{3}=4$.\n\nTotal sample size: $N = n_1 + n_2 + n_3 = 4 + 5 + 4 = 13$.\n\n**Step 1: Calculate the means.**\n- Group 1 mean: $\\bar{y}_{1\\cdot} = \\frac{1.8 + 2.1 + 1.9 + 2.2}{4} = \\frac{8.0}{4} = 2.0$.\n- Group 2 mean: $\\bar{y}_{2\\cdot} = \\frac{2.4 + 2.5 + 2.7 + 2.6 + 2.8}{5} = \\frac{13.0}{5} = 2.6$.\n- Group 3 mean: $\\bar{y}_{3\\cdot} = \\frac{3.1 + 3.0 + 3.2 + 2.9}{4} = \\frac{12.2}{4} = 3.05$.\n- Grand mean: $\\bar{y}_{\\cdot\\cdot} = \\frac{n_1\\bar{y}_{1\\cdot} + n_2\\bar{y}_{2\\cdot} + n_3\\bar{y}_{3\\cdot}}{N} = \\frac{4(2.0) + 5(2.6) + 4(3.05)}{13} = \\frac{8.0 + 13.0 + 12.2}{13} = \\frac{33.2}{13}$.\n\n**Step 2: Calculate the Sum of Squares.**\n- **Between-group Sum of Squares (SSB):**\n$$SSB = n_{1}(\\bar{y}_{1\\cdot} - \\bar{y}_{\\cdot\\cdot})^{2} + n_{2}(\\bar{y}_{2\\cdot} - \\bar{y}_{\\cdot\\cdot})^{2} + n_{3}(\\bar{y}_{3\\cdot} - \\bar{y}_{\\cdot\\cdot})^{2}$$\n$$SSB = 4\\left(2.0 - \\frac{33.2}{13}\\right)^{2} + 5\\left(2.6 - \\frac{33.2}{13}\\right)^{2} + 4\\left(3.05 - \\frac{33.2}{13}\\right)^{2}$$\n$$SSB = 4\\left(\\frac{26.0 - 33.2}{13}\\right)^{2} + 5\\left(\\frac{33.8 - 33.2}{13}\\right)^{2} + 4\\left(\\frac{39.65 - 33.2}{13}\\right)^{2}$$\n$$SSB = 4\\left(\\frac{-7.2}{13}\\right)^{2} + 5\\left(\\frac{0.6}{13}\\right)^{2} + 4\\left(\\frac{6.45}{13}\\right)^{2}$$\n$$SSB = \\frac{4(51.84) + 5(0.36) + 4(41.6025)}{169} = \\frac{207.36 + 1.8 + 166.41}{169} = \\frac{375.57}{169}$$\n- **Within-group Sum of Squares (SSW):**\nFor Group 1: $\\sum_{j=1}^{4}(y_{1j} - 2.0)^2 = (1.8-2.0)^2+(2.1-2.0)^2+(1.9-2.0)^2+(2.2-2.0)^2 = 0.04+0.01+0.01+0.04=0.10$.\nFor Group 2: $\\sum_{j=1}^{5}(y_{2j} - 2.6)^2 = (2.4-2.6)^2+(2.5-2.6)^2+(2.7-2.6)^2+(2.6-2.6)^2+(2.8-2.6)^2 = 0.04+0.01+0.01+0.0+0.04 = 0.10$.\nFor Group 3: $\\sum_{j=1}^{4}(y_{3j} - 3.05)^2 = (3.1-3.05)^2+(3.0-3.05)^2+(3.2-3.05)^2+(2.9-3.05)^2 = 0.0025+0.0025+0.0225+0.0225=0.05$.\n$$SSW = 0.10 + 0.10 + 0.05 = 0.25$$\n\n**Step 3: Calculate the degrees of freedom.**\n- $df_B = k - 1 = 3 - 1 = 2$.\n- $df_W = N - k = 13 - 3 = 10$.\n\n**Step 4: Calculate the Mean Squares.**\n- $MSB = \\frac{SSB}{df_B} = \\frac{375.57/169}{2} = \\frac{375.57}{338}$.\n- $MSW = \\frac{SSW}{df_W} = \\frac{0.25}{10} = 0.025$.\n\n**Step 5: Calculate the $F$-statistic.**\n$$F = \\frac{MSB}{MSW} = \\frac{375.57/338}{0.025} = \\frac{375.57}{338 \\times 0.025} = \\frac{375.57}{8.45}$$\n$$F \\approx 44.4461538...$$\nRounding to $4$ significant figures, we get $44.45$.", "answer": "$$\\boxed{44.45}$$", "id": "4539195"}, {"introduction": "While parametric tests like the t-test or ANOVA are powerful, their validity rests on certain assumptions about the data. Real-world datasets, however, are rarely perfect and may contain extreme outliers from sources like imaging artifacts or rare biological events. This exercise provides a crucial lesson in statistical robustness by demonstrating how a single outlier can disproportionately influence the t-statistic, potentially leading to false conclusions about a feature's importance. Through direct calculation, you will see why it is essential to critically assess your data and consider robust statistical alternatives. [@problem_id:4539220]", "problem": "In a radiomics study, you consider a single quantitative feature extracted from regions of interest in medical images for two classes: benign and malignant. You aim to use a two-sample Welch t-test as a filter method to rank features by class-separating power. Suppose the benign class has $n_B = 8$ samples with values\n$0.92, 1.05, 1.03, 0.98, 1.10, 0.87, 1.01, 0.95$,\nand the malignant class has $n_M = 8$ samples with values\n$1.02, 1.06, 0.97, 1.01, 0.94, 1.00, 1.03, 2.50$,\nwhere the final value $2.50$ is a suspected extreme outlier.\n\nUsing only foundational statistical definitions (sample mean, sample variance) and the standard two-sample Welch t-statistic from statistical inference, do the following:\n\n- Compute the Welch t-statistic for testing the difference in class means with the outlier included in the malignant class.\n- Compute the Welch t-statistic again after removing the single suspected outlier from the malignant class.\n- Compare the magnitudes you obtain and reason from first principles why the difference arises.\n\nThen select the option that both correctly describes the numerical effect and recommends an appropriate robust filter-method alternative or preprocessing step that mitigates the impact of such outliers without leaking information across training and test data.\n\nA. With the outlier, the t-statistic is approximately $1.07$; without it, it is approximately $0.51$, indicating inflation due to the single extreme value. A suitable remedy is to use a rank-based filter such as the Mann–Whitney Wilcoxon rank-sum test, or apply symmetric capping via winsorization or trimming (e.g., Yuen’s trimmed t-test) and robust scaling by median and median absolute deviation, all performed strictly within each training fold during cross-validation to prevent information leakage.\n\nB. With the outlier, the t-statistic is approximately $0.35$; without it, it is approximately $1.20$, so the outlier deflates the statistic. The best fix is to z-score all features using the mean and standard deviation computed on the entire dataset before any splitting, which prevents leakage while reducing outlier effects.\n\nC. With the outlier, the t-statistic is approximately $3.1$; without it, it is approximately $0.6$, showing large inflation due to the outlier. The appropriate robust choice is to switch to a one-way Analysis of Variance (ANOVA), which is inherently robust to outliers and therefore preferred over t-tests in filter selection.\n\nD. With the outlier, the t-statistic is approximately $1.07$; without it, it is approximately $0.51$, indicating inflation. The recommended approach is to replace t-tests with Mutual Information (MI) filtering, which is immune to outliers and may be safely computed once on the full dataset prior to model evaluation to stabilize selection.", "solution": "The problem statement is scientifically grounded, well-posed, objective, and complete. It describes a standard statistical scenario in the field of radiomics, providing all necessary data for the requested calculations. Therefore, the problem is valid, and we may proceed with the solution.\n\nThe two-sample Welch t-test is used to test the hypothesis that two populations have equal means, without assuming that they have equal variances. The test statistic, $t$, is defined as:\n$$ t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} $$\nwhere $\\bar{x}_i$, $s_i^2$, and $n_i$ are the sample mean, sample variance, and sample size for group $i$, respectively.\n\nThe sample mean is calculated as:\n$$ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i $$\nThe unbiased sample variance is calculated as:\n$$ s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2 $$\n\nLet us denote the benign class as group B and the malignant class as group M.\n\nFirst, we calculate the sample mean and variance for the benign class, which remain constant for both scenarios.\nGiven data for benign class: $X_B = \\{0.92, 1.05, 1.03, 0.98, 1.10, 0.87, 1.01, 0.95\\}$\nSample size: $n_B = 8$\nSample mean:\n$$ \\bar{x}_B = \\frac{1}{8}(0.92+1.05+1.03+0.98+1.10+0.87+1.01+0.95) = \\frac{7.91}{8} = 0.98875 $$\nSum of squared differences: $\\sum(x_i - \\bar{x}_B)^2 \\approx 0.0386875$\nSample variance:\n$$ s_B^2 = \\frac{0.0386875}{8-1} = \\frac{0.0386875}{7} \\approx 0.005526786 $$\n\n**Case 1: With the outlier included in the malignant class**\n\nGiven data for malignant class: $X_M = \\{1.02, 1.06, 0.97, 1.01, 0.94, 1.00, 1.03, 2.50\\}$\nSample size: $n_M = 8$\nSample mean:\n$$ \\bar{x}_{M, \\text{with}} = \\frac{1}{8}(1.02+1.06+0.97+1.01+0.94+1.00+1.03+2.50) = \\frac{9.53}{8} = 1.19125 $$\nSum of squared differences: $\\sum(x_i - \\bar{x}_{M, \\text{with}})^2 \\approx 1.9668875$\nSample variance:\n$$ s_{M, \\text{with}}^2 = \\frac{1.9668875}{8-1} = \\frac{1.9668875}{7} \\approx 0.2809839 $$\nNow we compute the Welch t-statistic:\n$$ t_{\\text{with}} = \\frac{\\bar{x}_{M, \\text{with}} - \\bar{x}_B}{\\sqrt{\\frac{s_B^2}{n_B} + \\frac{s_{M, \\text{with}}^2}{n_M}}} = \\frac{1.19125 - 0.98875}{\\sqrt{\\frac{0.005526786}{8} + \\frac{0.2809839}{8}}} $$\n$$ t_{\\text{with}} = \\frac{0.2025}{\\sqrt{0.00069085 + 0.03512299}} = \\frac{0.2025}{\\sqrt{0.03581384}} \\approx \\frac{0.2025}{0.1892454} \\approx 1.07004 $$\n\n**Case 2: Without the suspected outlier in the malignant class**\n\nData for malignant class (outlier removed): $X_{M'} = \\{1.02, 1.06, 0.97, 1.01, 0.94, 1.00, 1.03\\}$\nSample size: $n_{M'} = 7$\nSample mean:\n$$ \\bar{x}_{M, \\text{without}} = \\frac{1}{7}(1.02+1.06+0.97+1.01+0.94+1.00+1.03) = \\frac{7.03}{7} \\approx 1.004286 $$\nSum of squared differences: $\\sum(x_i - \\bar{x}_{M, \\text{without}})^2 \\approx 0.00937143$\nSample variance:\n$$ s_{M, \\text{without}}^2 = \\frac{0.00937143}{7-1} = \\frac{0.00937143}{6} \\approx 0.00156190 $$\nNow we compute the Welch t-statistic:\n$$ t_{\\text{without}} = \\frac{\\bar{x}_{M, \\text{without}} - \\bar{x}_B}{\\sqrt{\\frac{s_B^2}{n_B} + \\frac{s_{M, \\text{without}}^2}{n_{M'}}}} = \\frac{1.004286 - 0.98875}{\\sqrt{\\frac{0.005526786}{8} + \\frac{0.00156190}{7}}} $$\n$$ t_{\\text{without}} = \\frac{0.015536}{\\sqrt{0.00069085 + 0.00022313}} = \\frac{0.015536}{\\sqrt{0.00091398}} \\approx \\frac{0.015536}{0.030232} \\approx 0.51389 $$\n\n**Comparison and Reasoning**\nWith the outlier, the t-statistic is approximately $1.07$. Without the outlier, it is approximately $0.51$. The presence of the single extreme value $2.50$ significantly inflates the t-statistic. This occurs because the t-statistic is a ratio of the difference in means to the standard error of the difference. The outlier ($2.50$) has two competing effects:\n1.  It pulls the mean of the malignant group $\\bar{x}_M$ upwards, from $\\approx 1.00$ to $\\approx 1.19$. This increases the numerator $(\\bar{x}_M - \\bar{x}_B)$ from $\\approx 0.016$ to $\\approx 0.203$, a more than $12$-fold increase.\n2.  It dramatically increases the sample variance of the malignant group $s_M^2$ (from $\\approx 0.0016$ to $\\approx 0.281$), as variance is sensitive to the square of deviations from the mean. This increases the denominator (the standard error of the difference) from $\\approx 0.030$ to $\\approx 0.189$, a more than $6$-fold increase.\nIn this case, the numerator's increase is proportionally larger than the denominator's increase, resulting in a net inflation of the t-statistic. This illustrates the lack of robustness of the t-test to outliers. A feature with this outlier would be ranked as more discriminative than it would be based on the bulk of its data.\n\nNow, we evaluate the given options:\n\n**A. With the outlier, the t-statistic is approximately $1.07$; without it, it is approximately $0.51$, indicating inflation due to the single extreme value. A suitable remedy is to use a rank-based filter such as the Mann–Whitney Wilcoxon rank-sum test, or apply symmetric capping via winsorization or trimming (e.g., Yuen’s trimmed t-test) and robust scaling by median and median absolute deviation, all performed strictly within each training fold during cross-validation to prevent information leakage.**\nThe calculated t-statistics ($1.07$ and $0.51$) are correct. The interpretation of \"inflation\" is correct. The suggested remedies are all standard, appropriate methods for achieving robustness against outliers: the Mann-Whitney test is a non-parametric, rank-based alternative, and trimming/winsorization are direct methods to reduce outlier influence on parametric tests. The final point about performing these steps strictly within training folds to prevent data leakage is a critical and correct principle of machine learning methodology.\n**Verdict: Correct**\n\n**B. With the outlier, the t-statistic is approximately $0.35$; without it, it is approximately $1.20$, so the outlier deflates the statistic. The best fix is to z-score all features using the mean and standard deviation computed on the entire dataset before any splitting, which prevents leakage while reducing outlier effects.**\nThe calculated t-statistics are incorrect. The conclusion that the outlier deflates the statistic is incorrect. Z-scoring does not mitigate the influence of outliers on the mean and standard deviation; it simply re-scales them. Most importantly, computing scaling parameters on the entire dataset before splitting is a textbook example of data leakage, and the claim that this \"prevents leakage\" is definitively false.\n**Verdict: Incorrect**\n\n**C. With the outlier, the t-statistic is approximately $3.1$; without it, it is approximately $0.6$, showing large inflation due to the outlier. The appropriate robust choice is to switch to a one-way Analysis of Variance (ANOVA), which is inherently robust to outliers and therefore preferred over t-tests in filter selection.**\nThe calculated t-statistics are incorrect. The primary flaw is the statistical reasoning: for two groups, one-way ANOVA is mathematically equivalent to a pooled-variance t-test ($F = t^2$) and is based on the same principles of means and variances. Therefore, ANOVA is equally sensitive (i.e., not robust) to outliers. The claim that it is \"inherently robust\" is false.\n**Verdict: Incorrect**\n\n**D. With the outlier, the t-statistic is approximately $1.07$; without it, it is approximately $0.51$, indicating inflation. The recommended approach is to replace t-tests with Mutual Information (MI) filtering, which is immune to outliers and may be safely computed once on the full dataset prior to model evaluation to stabilize selection.**\nThe calculated t-statistics are correct. Mutual Information is a valid and often more-robust alternative. However, the claim that it is \"immune\" to outliers is an overstatement; its robustness depends on the estimation method (e.g., binning). The critical flaw is the procedural recommendation: computing MI (or any feature selection metric) on the full dataset before cross-validation introduces optimistic bias from data leakage. This is a severe methodological error.\n**Verdict: Incorrect**", "answer": "$$\\boxed{A}$$", "id": "4539220"}, {"introduction": "The most intuitive relationships are often linear or monotonic, where a feature's value consistently increases or decreases with an outcome. However, biological relationships can be far more complex; for example, both very low and very high levels of a biomarker could indicate disease. This thought experiment challenges you to think beyond simple correlations by exploring a scenario where a feature is clearly predictive, yet standard linear methods like the Pearson correlation and t-test fail completely. By comparing these tools to more general measures like Mutual Information, you will learn to identify and select features with powerful non-linear and non-monotonic dependencies. [@problem_id:4539252]", "problem": "A radiomics study considers a single quantitative feature $R$ extracted from pre-treatment computed tomography images of lung tumors, standardized to have distribution approximately $\\mathcal{N}(0,1)$ across a cohort of $n=200$ patients. The binary clinical endpoint $Y \\in \\{0,1\\}$ indicates local control at $2$ years, constructed as follows for this thought experiment: there exists a threshold $\\tau  0$ such that $Y=\\mathbf{1}(|R|\\tau)$. Assume that the class prevalences are non-degenerate (that is, $\\mathbb{P}(Y=1)\\in(0,1)$), and ignore measurement noise for the purpose of this question.\n\nYou must evaluate univariate filter methods for feature selection on $R$ with respect to $Y$. Using only fundamental definitions of expectation, covariance, independence, and mutual information, and the symmetry properties of the standard normal distribution, reason about whether the following filters, when applied directly to the raw feature $R$, would be expected to flag $R$ as informative for $Y$ under the data-generating process described:\n\nA. Pearson correlation filter that ranks features by the absolute value of the sample correlation between $R$ and the numeric label $Y$ coded as $0$ or $1$.\n\nB. $2$-sample Student’s $t$-test comparing the mean of $R$ across the two classes defined by $Y$.\n\nC. Mutual information (MI) filter that estimates $I(R;Y)$ using a consistent estimator for mixed continuous-discrete variables and ranks features by estimated MI.\n\nD. Spearman rank correlation filter between $R$ and the numeric label $Y$.\n\nE. Distance correlation filter, which uses the population distance correlation that equals zero if and only if independence.\n\nWhich of the filters in options A–E would be expected, in principle, to correctly identify $R$ as informative for $Y$ without any prior nonlinear feature engineering such as transforming $R$ to $R^2$?\n\nSelect all that apply.", "solution": "The validity of the problem statement must first be assessed.\n\n### Step 1: Extract Givens\n- A quantitative radiomic feature $R$ is standardized, with distribution approximately $\\mathcal{N}(0,1)$.\n- The sample size is $n=200$.\n- A binary clinical endpoint $Y \\in \\{0,1\\}$ is defined.\n- The data-generating process for the endpoint is $Y = \\mathbf{1}(|R|  \\tau)$ for some constant threshold $\\tau  0$, where $\\mathbf{1}(\\cdot)$ is the indicator function.\n- Class prevalences are non-degenerate, i.e., $\\mathbb{P}(Y=1) \\in (0,1)$.\n- Measurement noise is to be ignored.\n- The task is to evaluate five univariate filter methods for feature selection on the raw feature $R$ with respect to $Y$.\n- The evaluation should use fundamental definitions and symmetry properties.\n- The question is which filters would, in principle, identify $R$ as informative for $Y$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem describes a simplified but plausible scenario in radiomics, where a feature's magnitude (irrespective of sign) might correlate with a clinical outcome. The statistical methods listed are standard in the field. The use of a standard normal distribution is a common and valid simplification for modeling standardized data. The problem is grounded in established statistical and machine learning principles.\n- **Well-Posed:** The problem is mathematically well-defined. The distributions of $R$ and the functional relationship between $R$ and $Y$ are clearly specified. The condition $\\mathbb{P}(Y=1) \\in (0,1)$ implies that for $R \\sim \\mathcal{N}(0,1)$, the threshold $\\tau$ must be such that $\\mathbb{P}(|R|  \\tau)$ is not $0$ or $1$, which is true for any finite $\\tau0$. The question is specific and answerable.\n- **Objective:** The language is formal, precise, and free of subjective content.\n- **Completeness and Consistency:** The problem is self-contained and provides all necessary information to reason about the population-level behavior of the statistical filters. There are no contradictions.\n- **Other criteria:** The problem is not trivial, as it requires understanding the limitations of different statistical dependency measures. It is not ill-posed, unrealistic (within the context of a thought experiment), or otherwise flawed.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived.\n\n### Solution Derivation\n\nThe core of the problem is to determine which statistical measures can detect the specific dependency $Y = \\mathbf{1}(|R|  \\tau)$, where $R \\sim \\mathcal{N}(0,1)$. This relationship is symmetric: both large positive and large negative values of $R$ map to $Y=1$, while values of $R$ close to zero map to $Y=0$. We analyze the expected behavior of each filter by examining the population-level properties they measure. Let $p(r)$ be the probability density function of the standard normal distribution, which is an even function: $p(-r) = p(r)$.\n\n**A. Pearson correlation filter**\n\nThe Pearson correlation coefficient is $\\rho(R,Y) = \\frac{\\text{Cov}(R,Y)}{\\sigma_R \\sigma_Y}$. A filter would flag $R$ if $|\\rho(R,Y)|$ is significantly non-zero. The covariance is $\\text{Cov}(R,Y) = \\mathbb{E}[RY] - \\mathbb{E}[R]\\mathbb{E}[Y]$.\nGiven $R \\sim \\mathcal{N}(0,1)$, we have $\\mathbb{E}[R]=0$. Therefore, $\\text{Cov}(R,Y) = \\mathbb{E}[RY]$.\nWe calculate the expectation $\\mathbb{E}[RY]$:\n$$ \\mathbb{E}[RY] = \\mathbb{E}[R \\cdot \\mathbf{1}(|R|  \\tau)] = \\int_{-\\infty}^{\\infty} r \\cdot \\mathbf{1}(|r|  \\tau) \\cdot p(r) \\, dr $$\nThis integral can be split into two parts:\n$$ \\mathbb{E}[RY] = \\int_{-\\infty}^{-\\tau} r \\cdot p(r) \\, dr + \\int_{\\tau}^{\\infty} r \\cdot p(r) \\, dr $$\nThe integrand $f(r) = r \\cdot p(r)$ is an odd function because $r$ is an odd function and $p(r)$ is an even function. Let's show this explicitly. Let $u = -r$ in the first integral:\n$$ \\int_{-\\infty}^{-\\tau} r \\cdot p(r) \\, dr = \\int_{\\infty}^{\\tau} (-u) \\cdot p(-u) \\cdot (-du) = \\int_{\\infty}^{\\tau} u \\cdot p(u) \\, du = - \\int_{\\tau}^{\\infty} u \\cdot p(u) \\, du $$\nSubstituting this back, we get:\n$$ \\mathbb{E}[RY] = - \\int_{\\tau}^{\\infty} r \\cdot p(r) \\, dr + \\int_{\\tau}^{\\infty} r \\cdot p(r) \\, dr = 0 $$\nSince the covariance is $0$, the population Pearson correlation $\\rho(R,Y)$ is $0$. A filter based on Pearson correlation is designed to detect linear relationships and would, in principle, fail to identify the non-linear, symmetric dependency between $R$ and $Y$.\n\nVerdict on A: **Incorrect**.\n\n**B. 2-sample Student’s t-test**\n\nThe t-test compares the means of $R$ in the two groups defined by $Y$. It tests the null hypothesis $H_0: \\mathbb{E}[R|Y=0] = \\mathbb{E}[R|Y=1]$. The feature $R$ would be flagged as informative if this null hypothesis were rejected.\nLet's compute the conditional expectations:\n$$ \\mathbb{E}[R|Y=1] = \\mathbb{E}[R \\,|\\, |R|  \\tau] = \\frac{\\mathbb{E}[R \\cdot \\mathbf{1}(|R|  \\tau)]}{\\mathbb{P}(|R|  \\tau)} $$\nThe numerator is $\\mathbb{E}[RY]$, which we have already shown to be $0$. Since $\\mathbb{P}(Y=1) = \\mathbb{P}(|R|  \\tau) \\in (0,1)$, the denominator is non-zero. Thus, $\\mathbb{E}[R|Y=1] = 0$.\nNext, we compute the other conditional expectation:\n$$ \\mathbb{E}[R|Y=0] = \\mathbb{E}[R \\,|\\, |R| \\le \\tau] = \\frac{\\mathbb{E}[R \\cdot \\mathbf{1}(|R| \\le \\tau)]}{\\mathbb{P}(|R| \\le \\tau)} $$\nThe numerator is an integral over a symmetric interval:\n$$ \\mathbb{E}[R \\cdot \\mathbf{1}(|R| \\le \\tau)] = \\int_{-\\tau}^{\\tau} r \\cdot p(r) \\, dr $$\nAs the integrand $r \\cdot p(r)$ is an odd function and the interval of integration $[-\\tau, \\tau]$ is symmetric about $0$, this integral is $0$. The denominator $\\mathbb{P}(|R| \\le \\tau)$ is non-zero. Thus, $\\mathbb{E}[R|Y=0] = 0$.\nSince $\\mathbb{E}[R|Y=0] = \\mathbb{E}[R|Y=1] = 0$, the null hypothesis of the t-test is true at the population level. The t-test is therefore not expected to identify $R$ as an informative feature.\n\nVerdict on B: **Incorrect**.\n\n**C. Mutual information (MI) filter**\n\nMutual information $I(R;Y)$ is a measure of statistical dependence between two random variables. A fundamental property of mutual information is that $I(R;Y) \\ge 0$, with $I(R;Y) = 0$ if and only if $R$ and $Y$ are statistically independent.\nIn this problem, $Y$ is a deterministic function of $R$, namely $Y = \\mathbf{1}(|R|  \\tau)$. Since the problem states that the class prevalences are non-degenerate ($\\mathbb{P}(Y=1) \\in (0,1)$), $Y$ is not a constant random variable. If $Y$ is a non-constant function of $R$, then $R$ and $Y$ cannot be independent. Knowledge of $R$ provides information about $Y$. For example, if we observe $R=0$, we know with certainty that $Y=0$. If we observe $R = \\tau+1$, we know with certainty that $Y=1$. Since $R$ and $Y$ are not independent, it must be that $I(R;Y)  0$.\nA filter based on mutual information, using a consistent estimator, will estimate a value greater than zero and thus correctly flag $R$ as informative for predicting $Y$.\n\nVerdict on C: **Correct**.\n\n**D. Spearman rank correlation filter**\n\nSpearman correlation measures the strength of a monotonic relationship between two variables by computing the Pearson correlation on their ranks. Let's analyze the relationship between the rank of $R$ and the value of $Y$. Low values of $R$ (low ranks) and high values of $R$ (high ranks) both lead to $Y=1$. Intermediate values of $R$ (middle ranks) lead to $Y=0$. This relationship is \"U-shaped\" and is fundamentally non-monotonic.\nWe can formalize this by calculating the population Spearman correlation, which is the Pearson correlation of the probability integral transforms, $\\rho(F_R(R), F_Y(Y))$. For a mixed continuous-discrete case, a more direct approach is to compute $\\text{Cov}(\\text{rank}(R), Y)$, where the rank is represented by the CDF value $F_R(R)$.\n$\\text{Cov}(F_R(R), Y) = \\mathbb{E}[F_R(R) \\cdot Y] - \\mathbb{E}[F_R(R)]\\mathbb{E}[Y]$.\nFor any continuous variable $R$, $F_R(R) \\sim \\text{Uniform}(0,1)$, so $\\mathbb{E}[F_R(R)] = 1/2$.\nWe need $\\mathbb{E}[F_R(R) \\cdot Y] = \\mathbb{E}[F_R(R) \\cdot \\mathbf{1}(|R|  \\tau)]$:\n$$ \\mathbb{E}[F_R(R) \\cdot Y] = \\int_{|r|\\tau} F_R(r)p(r)dr = \\int_{-\\infty}^{-\\tau} F_R(r)p(r)dr + \\int_{\\tau}^{\\infty} F_R(r)p(r)dr $$\nUsing the property that for a symmetric distribution around $0$, $F_R(-r) = 1 - F_R(r)$, and symmetry of $p(r)$, the first integral becomes (with $u=-r$):\n$$ \\int_{\\infty}^{\\tau} F_R(-u)p(-u)(-du) = \\int_{\\tau}^{\\infty} (1-F_R(u))p(u)du $$\nSo, $\\mathbb{E}[F_R(R) \\cdot Y] = \\int_{\\tau}^{\\infty} (1-F_R(r))p(r)dr + \\int_{\\tau}^{\\infty} F_R(r)p(r)dr = \\int_{\\tau}^{\\infty} p(r)dr = \\mathbb{P}(R\\tau)$.\nAlso, $\\mathbb{E}[Y] = \\mathbb{P}(|R|\\tau) = \\mathbb{P}(R-\\tau) + \\mathbb{P}(R\\tau) = 2\\mathbb{P}(R\\tau)$ by symmetry.\nPlugging these into the covariance formula:\n$$ \\text{Cov}(F_R(R), Y) = \\mathbb{P}(R\\tau) - \\left(\\frac{1}{2}\\right) \\cdot (2\\mathbb{P}(R\\tau)) = \\mathbb{P}(R\\tau) - \\mathbb{P}(R\\tau) = 0 $$\nThe population Spearman correlation is zero. This filter would fail to identify the symmetric, non-monotonic relationship.\n\nVerdict on D: **Incorrect**.\n\n**E. Distance correlation filter**\n\nThe problem statement provides the key property of distance correlation: the population distance correlation is zero if and only if the variables are statistically independent. This is a crucial advantage over Pearson correlation, which can be zero even for strongly dependent variables (as seen in option A).\nWe have already established in the analysis for option C that $R$ and $Y$ are not independent because $Y$ is a non-constant, deterministic function of $R$.\nSince $R$ and $Y$ are not independent, their population distance correlation must be strictly greater than zero. A filter based on distance correlation would therefore compute a non-zero value and correctly identify $R$ as an informative feature.\n\nVerdict on E: **Correct**.", "answer": "$$\\boxed{CE}$$", "id": "4539252"}]}