{"hands_on_practices": [{"introduction": "An essential first step in understanding fractals is to connect their abstract dimension to tangible, measurable properties. This exercise [@problem_id:4541431] builds foundational intuition by having you derive the classic perimeter-area scaling law, which shows how a fractal boundary dimension $D_b$ governs the relationship between two of the most basic geometric measurements. Completing this practice will clarify how a non-integer dimension can be calculated from straightforward measurements of a shape, a key technique in characterizing the complexity of tumor contours.", "problem": "In radiomics, quantifying the geometric complexity of a tumor contour extracted from Computed Tomography (CT) images hinges on scale-invariant properties. Consider a two-dimensional tumor segmentation whose boundary is a statistically self-similar irregular curve. Under isotropic dilation by a linear factor $s$, a $k$-dimensional measure scales as $s^{k}$. In particular, the enclosed area scales as $s^{2}$ and the boundary measure scales as $s^{D_b}$, where $D_b$ is the boundary fractal dimension. \n\n(a) Starting strictly from these scaling properties, derive a scaling relation that connects the measured perimeter $P$ and area $A$ of such a self-similar tumor segmentation in terms of the boundary fractal dimension $D_b$, eliminating the dilation factor $s$ and any shape-dependent prefactors. Explain briefly why this relation is useful for characterizing irregular tumor contours in radiomics.\n\n(b) A lesion is segmented at two self-similar scales. At the native scale, its measured area and perimeter are $A_1 = 10~\\text{mm}^{2}$ and $P_1 = 20~\\text{mm}$. After isotropic rescaling of the segmentation, its measured area and perimeter are $A_2 = 270~\\text{mm}^{2}$ and $P_2 = 180~\\text{mm}$. Using only the relation you derived in part (a), compute the boundary fractal dimension $D_b$. Express your final estimate of $D_b$ as a dimensionless quantity rounded to four significant figures.", "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- A $2$-dimensional tumor segmentation with a statistically self-similar irregular boundary.\n- Under isotropic dilation by a linear factor $s$, a $k$-dimensional measure scales as $s^k$.\n- Enclosed area, $A$, scales as $s^2$.\n- Boundary measure (perimeter), $P$, scales as $s^{D_b}$, where $D_b$ is the boundary fractal dimension.\n- Part (a) requires deriving a scaling relation connecting $P$ and $A$ in terms of $D_b$, eliminating $s$ and shape-dependent prefactors.\n- Part (b) provides data for a lesion at two self-similar scales:\n    - Scale 1: $A_1 = 10~\\text{mm}^2$, $P_1 = 20~\\text{mm}$.\n    - Scale 2: $A_2 = 270~\\text{mm}^2$, $P_2 = 180~\\text{mm}$.\n- Part (b) requires computing $D_b$ using the relation from part (a) and expressing the result rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on the well-established principles of fractal geometry and its application in medical imaging analysis (radiomics). The scaling laws for area and perimeter are fundamental to the definition of fractal dimension. The problem is scientifically sound.\n- **Well-Posed**: The problem is clearly stated. Part (a) requires a standard derivation from the given principles. Part (b) provides sufficient and consistent data ($A_1, P_1, A_2, P_2$) to determine the single unknown variable, $D_b$. A unique and stable solution exists.\n- **Objective**: The language is technical, precise, and free of any subjective or ambiguous terminology.\n- **Completeness and Consistency**: The problem is self-contained and the data provided is internally consistent. The ratio of areas gives a scaling factor, which is consistent with the ratio of perimeters for a solvable fractal dimension.\n- **Other Flaws**: The problem does not exhibit any other flaws such as being unrealistic, ill-posed, trivial, or unverifiable.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A complete solution will be provided.\n\n***\n\n### Part (a): Derivation of the Scaling Relation\n\nLet us consider a reference shape with area $A_0$ and perimeter $P_0$. When this shape is isotropically dilated by a linear factor $s$, the new area $A$ and perimeter $P$ are given by the scaling laws provided in the problem statement:\n$$A = A_0 s^2$$\n$$P = P_0 s^{D_b}$$\nwhere $D_b$ is the boundary fractal dimension.\n\nOur goal is to find a relation between $P$ and $A$ that eliminates the scaling factor $s$ and the shape-dependent prefactors $A_0$ and $P_0$. We can achieve this by first expressing $s$ in terms of $A$ and $A_0$ from the first equation:\n$$s^2 = \\frac{A}{A_0} \\implies s = \\left(\\frac{A}{A_0}\\right)^{1/2}$$\nNext, we substitute this expression for $s$ into the equation for the perimeter $P$:\n$$P = P_0 \\left[ \\left(\\frac{A}{A_0}\\right)^{1/2} \\right]^{D_b}$$\nUsing the property of exponents $(x^a)^b = x^{ab}$, we get:\n$$P = P_0 \\left(\\frac{A}{A_0}\\right)^{D_b/2}$$\nWe can separate the terms that depend on the measurement ($A$) from the terms that depend on the reference shape ($A_0, P_0$):\n$$P = P_0 \\frac{A^{D_b/2}}{A_0^{D_b/2}} = \\left(\\frac{P_0}{A_0^{D_b/2}}\\right) A^{D_b/2}$$\nThe term in the parenthesis, $C = \\frac{P_0}{A_0^{D_b/2}}$, is a constant for a given shape, as it depends only on the reference geometry. Thus, we have established a power-law relationship between perimeter and area:\n$$P = C A^{D_b/2}$$\nThis is often expressed as a proportionality, which is independent of the specific prefactor:\n$$P \\propto A^{D_b/2}$$\nThis is the required scaling relation.\n\nThis relation is useful in radiomics because it connects two simple, measurable geometric quantities ($P$ and $A$) to a more profound, scale-invariant descriptor of morphological complexity, the fractal dimension $D_b$. For a smooth boundary, $D_b=1$, and the relation becomes $P \\propto A^{1/2}$, which is characteristic of standard Euclidean shapes (e.g., for a circle, $P = 2\\sqrt{\\pi}\\sqrt{A}$). As the tumor boundary becomes more irregular and tortuous, $D_b$ increases towards $2$. A higher $D_b$ signifies greater complexity, which has been shown in numerous studies to correlate with tumor aggressiveness, treatment response, and patient prognosis. Therefore, this relation provides the basis for a quantitative biomarker of tumor phenotype.\n\n### Part (b): Computation of the Boundary Fractal Dimension\n\nTo compute $D_b$ from the measurements at two different scales, we use the derived relation $P = C A^{D_b/2}$. Let the measurements at the two scales be $(P_1, A_1)$ and $(P_2, A_2)$.\nFor the first scale:\n$$P_1 = C A_1^{D_b/2}$$\nFor the second scale:\n$$P_2 = C A_2^{D_b/2}$$\nThe shape factor $C$ is the same for both since they are self-similar representations of the same lesion. By taking the ratio of these two equations, we can eliminate $C$:\n$$\\frac{P_2}{P_1} = \\frac{C A_2^{D_b/2}}{C A_1^{D_b/2}} = \\left(\\frac{A_2}{A_1}\\right)^{D_b/2}$$\nWe are given the following values: $A_1 = 10$, $P_1 = 20$, $A_2 = 270$, and $P_2 = 180$. We substitute these into the equation:\n$$\\frac{180}{20} = \\left(\\frac{270}{10}\\right)^{D_b/2}$$\nSimplifying the ratios on both sides gives:\n$$9 = (27)^{D_b/2}$$\nTo solve for $D_b$, we take the natural logarithm of both sides:\n$$\\ln(9) = \\ln\\left((27)^{D_b/2}\\right)$$\nUsing the logarithm property $\\ln(x^y) = y\\ln(x)$, we have:\n$$\\ln(9) = \\frac{D_b}{2} \\ln(27)$$\nNow, we can isolate $D_b$:\n$$D_b = 2 \\frac{\\ln(9)}{\\ln(27)}$$\nTo simplify this expression, we can express $9$ and $27$ as powers of the same base, $3$: $9 = 3^2$ and $27 = 3^3$.\n$$D_b = 2 \\frac{\\ln(3^2)}{\\ln(3^3)} = 2 \\frac{2\\ln(3)}{3\\ln(3)}$$\nThe term $\\ln(3)$ cancels out from the numerator and the denominator:\n$$D_b = 2 \\left(\\frac{2}{3}\\right) = \\frac{4}{3}$$\nThe problem asks for the result to be rounded to four significant figures.\n$$D_b = \\frac{4}{3} = 1.3333...$$\nRounding to four significant figures, we get:\n$$D_b \\approx 1.333$$\nThis value is dimensionless, as is required for a dimension, and falls within the expected range of $1 \\le D_b  2$ for a fractal boundary curve in a $2$-dimensional plane.", "answer": "$$\\boxed{1.333}$$", "id": "4541431"}, {"introduction": "While boundary analysis is useful, many radiomic features are derived from the texture within a region of interest. This problem [@problem_id:4541410] transitions from boundary dimension to the fractal dimension of a surface, as represented by a grayscale image. You will explore the Differential Box-Counting (DBC) method and derive the statistical estimator $\\hat{D}$ that is fundamental to calculating dimension from experimental or image data, bridging the gap between fractal theory and practical application.", "problem": "In radiomics, the heterogeneity of tumor texture in a grayscale image can be characterized by a fractal geometry model in which the image intensities define a surface over the spatial domain. Consider a grayscale image represented by an array $I(x,y)$ of size $M \\times M$ with intensity levels in $\\{0,1,\\dots,G-1\\}$. You will define the Differential Box-Counting (DBC) method for estimating the box-counting fractal dimension of this surface by partitioning the intensity axis into stacks and then derive a closed-form least-squares estimator $\\hat{D}$ based on the summed stack counts across multiple scales.\n\nAt a spatial scale indexed by $k \\in \\{1,2,\\dots,K\\}$, partition the image plane into a grid of $b_{k} \\times b_{k}$ non-overlapping spatial boxes of side length $s_{k} = M/b_{k}$ pixels. Independently, partition the intensity axis into $c_{k}$ equal-height stacks of height $h_{k} = G/c_{k}$. For each spatial box $B$ at scale $s_{k}$, let $g_{\\min}(B)$ and $g_{\\max}(B)$ denote the minimum and maximum intensity values observed in that spatial box. Define the number of intersected intensity stacks for $B$ by\n$$\nn_{B}(s_{k}) \\;=\\; \\left\\lfloor \\frac{g_{\\max}(B)}{h_{k}} \\right\\rfloor \\;-\\; \\left\\lfloor \\frac{g_{\\min}(B)}{h_{k}} \\right\\rfloor \\;+\\; 1,\n$$\nand define the total (summed) stack count at scale $s_{k}$ by\n$$\nN(s_{k}) \\;=\\; \\sum_{B} n_{B}(s_{k}),\n$$\nwhere the sum is over all $b_{k}^{2}$ spatial boxes $B$ at that scale.\n\nStarting only from the fundamental definition of the box-counting dimension for a set with covering number $N(s)$,\n$$\nD \\;=\\; \\lim_{s \\to 0} \\frac{\\ln N(s)}{\\ln \\!\\left(\\frac{1}{s}\\right)},\n$$\nand the well-tested assumption that, over a finite range of scales relevant to medical images, $\\ln N(s)$ depends approximately linearly on $\\ln(1/s)$, derive the closed-form ordinary least squares estimator $\\hat{D}$ for $D$ using the $K$ observed pairs $\\{(s_{k}, N(s_{k}))\\}_{k=1}^{K}$. Express your final answer for $\\hat{D}$ as a single analytic expression in terms of $\\{\\ln(1/s_{k})\\}_{k=1}^{K}$ and $\\{\\ln N(s_{k})\\}_{k=1}^{K}$. Do not introduce any additional symbols beyond those defined here. Your final answer must be a single closed-form expression. No numerical evaluation or rounding is required.", "solution": "The problem statement is evaluated based on the established criteria for validity before proceeding to a solution.\n\n**Step 1: Extract Givens**\n-   Image representation: An array $I(x,y)$ of size $M \\times M$.\n-   Intensity levels: $\\{0, 1, \\dots, G-1\\}$.\n-   Spatial scales: $s_{k}$ where $k \\in \\{1, 2, \\dots, K\\}$, with $s_{k} = M/b_{k}$.\n-   Intensity stack heights: $h_{k} = G/c_{k}$.\n-   Number of intersected stacks for a spatial box $B$: $n_{B}(s_{k}) = \\lfloor g_{\\max}(B)/h_{k} \\rfloor - \\lfloor g_{\\min}(B)/h_{k} \\rfloor + 1$.\n-   Total stack count at scale $s_{k}$: $N(s_{k}) = \\sum_{B} n_{B}(s_{k})$.\n-   Definition of box-counting dimension: $D = \\lim_{s \\to 0} \\frac{\\ln N(s)}{\\ln(1/s)}$.\n-   Linearity assumption: $\\ln N(s)$ is approximately linear with $\\ln(1/s)$ over the observed scales.\n-   Data: $K$ pairs of observations $\\{(s_{k}, N(s_{k}))\\}_{k=1}^{K}$.\n-   Objective: Derive the closed-form ordinary least squares (OLS) estimator $\\hat{D}$ for $D$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, describing the Differential Box-Counting (DBC) method, a valid technique in fractal analysis applied to digital images. The mathematical foundation, based on the power-law relationship $N(s) \\propto (1/s)^D$, is sound. The task—deriving an OLS estimator for the exponent $D$ from a log-log linear model—is a standard, well-posed problem in statistical inference. The problem statement is self-contained, objective, and provides all necessary information for the derivation. It does not violate any scientific principles, contain ambiguities, or present unrealistic conditions.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A solution will be derived.\n\n**Derivation of the Estimator**\nThe definition of the box-counting dimension, $D = \\lim_{s \\to 0} \\frac{\\ln N(s)}{\\ln(1/s)}$, suggests a power-law relationship between the total count $N(s)$ and the scale $s$. For a finite range of scales, this relationship is expressed as $N(s) \\propto (1/s)^D$. Taking the natural logarithm of this proportionality yields a linear equation:\n$$\n\\ln N(s) = D \\ln\\left(\\frac{1}{s}\\right) + A\n$$\nwhere $A$ is a constant equal to the logarithm of the proportionality constant. This equation forms the basis of a simple linear regression model.\n\nLet us define the variables for our regression analysis based on the $K$ observed data points $\\{(s_{k}, N(s_{k}))\\}_{k=1}^{K}$:\n-   Let the dependent variable be $y_{k} = \\ln N(s_{k})$.\n-   Let the independent variable be $x_{k} = \\ln(1/s_{k})$.\n\nThe linear model for the $k$-th observation is:\n$$\ny_{k} = \\hat{D} x_{k} + \\hat{A} + \\epsilon_{k}\n$$\nwhere $\\hat{D}$ and $\\hat{A}$ are the estimates for the slope $D$ and the intercept $A$, respectively, and $\\epsilon_{k}$ is the error term for the $k$-th observation.\n\nThe method of ordinary least squares (OLS) seeks to find the parameters $\\hat{D}$ and $\\hat{A}$ that minimize the sum of the squared residuals (SSR). The residual for the $k$-th observation is $e_k = y_{k} - (\\hat{D}x_{k} + \\hat{A})$. The SSR is given by:\n$$\nSSR(\\hat{A}, \\hat{D}) = \\sum_{k=1}^{K} e_k^{2} = \\sum_{k=1}^{K} (y_{k} - \\hat{D}x_{k} - \\hat{A})^{2}\n$$\n\nTo minimize $SSR$, we take the partial derivatives with respect to $\\hat{A}$ and $\\hat{D}$ and set them to zero. This gives the following system of normal equations:\n\n1.  Partial derivative with respect to $\\hat{A}$:\n$$\n\\frac{\\partial SSR}{\\partial \\hat{A}} = \\sum_{k=1}^{K} 2(y_{k} - \\hat{D}x_{k} - \\hat{A})(-1) = 0\n$$\n$$\n\\sum_{k=1}^{K} (y_{k} - \\hat{D}x_{k} - \\hat{A}) = 0 \\quad \\implies \\quad \\sum_{k=1}^{K} y_{k} - \\hat{D}\\sum_{k=1}^{K} x_{k} - K\\hat{A} = 0\n$$\nSolving for $\\hat{A}$ gives:\n$$\n\\hat{A} = \\frac{1}{K}\\sum_{k=1}^{K} y_{k} - \\hat{D}\\frac{1}{K}\\sum_{k=1}^{K} x_{k}\n$$\n\n2.  Partial derivative with respect to $\\hat{D}$:\n$$\n\\frac{\\partial SSR}{\\partial \\hat{D}} = \\sum_{k=1}^{K} 2(y_{k} - \\hat{D}x_{k} - \\hat{A})(-x_{k}) = 0\n$$\n$$\n\\sum_{k=1}^{K} (x_{k}y_{k} - \\hat{D}x_{k}^{2} - \\hat{A}x_{k}) = 0 \\quad \\implies \\quad \\sum_{k=1}^{K} x_{k}y_{k} - \\hat{D}\\sum_{k=1}^{K} x_{k}^{2} - \\hat{A}\\sum_{k=1}^{K} x_{k} = 0\n$$\n\nNow, we substitute the expression for $\\hat{A}$ from the first normal equation into the second one:\n$$\n\\sum_{k=1}^{K} x_{k}y_{k} - \\hat{D}\\sum_{k=1}^{K} x_{k}^{2} - \\left(\\frac{1}{K}\\sum_{j=1}^{K} y_{j} - \\hat{D}\\frac{1}{K}\\sum_{j=1}^{K} x_{j}\\right)\\sum_{k=1}^{K} x_{k} = 0\n$$\nLet $\\bar{x} = \\frac{1}{K}\\sum x_k$ and $\\bar{y} = \\frac{1}{K}\\sum y_k$. The equation is $\\sum x_k y_k - \\hat{D}\\sum x_k^2 - (\\bar{y} - \\hat{D}\\bar{x})(K\\bar{x}) = 0$.\n$$\n\\sum_{k=1}^{K} x_{k}y_{k} - \\hat{D}\\sum_{k=1}^{K} x_{k}^{2} - K\\bar{x}\\bar{y} + \\hat{D}K\\bar{x}^2 = 0\n$$\nRearranging to solve for $\\hat{D}$:\n$$\n\\hat{D}\\left(K\\bar{x}^2 - \\sum_{k=1}^{K} x_{k}^{2}\\right) = K\\bar{x}\\bar{y} - \\sum_{k=1}^{K} x_{k}y_{k}\n$$\n$$\n\\hat{D} = \\frac{K\\bar{x}\\bar{y} - \\sum_{k=1}^{K} x_{k}y_{k}}{K\\bar{x}^{2} - \\sum_{k=1}^{K} x_{k}^{2}} = \\frac{\\sum_{k=1}^{K} x_{k}y_{k} - K\\bar{x}\\bar{y}}{\\sum_{k=1}^{K} x_{k}^{2} - K\\bar{x}^{2}}\n$$\nSubstituting the full expressions for the means $\\bar{x}$ and $\\bar{y}$:\n$$\n\\hat{D} = \\frac{\\sum_{k=1}^{K} x_{k}y_{k} - K\\left(\\frac{1}{K}\\sum_{j=1}^{K} x_{j}\\right)\\left(\\frac{1}{K}\\sum_{j=1}^{K} y_{j}\\right)}{\\sum_{k=1}^{K} x_{k}^{2} - K\\left(\\frac{1}{K}\\sum_{j=1}^{K} x_{j}\\right)^{2}} = \\frac{\\sum_{k=1}^{K} x_{k}y_{k} - \\frac{1}{K}\\left(\\sum_{k=1}^{K} x_{k}\\right)\\left(\\sum_{k=1}^{K} y_{k}\\right)}{\\sum_{k=1}^{K} x_{k}^{2} - \\frac{1}{K}\\left(\\sum_{k=1}^{K} x_{k}\\right)^{2}}\n$$\nMultiplying the numerator and denominator by $K$ to clear the fractions gives the standard closed-form expression for the OLS slope estimator:\n$$\n\\hat{D} = \\frac{K\\sum_{k=1}^{K} x_{k}y_{k} - \\left(\\sum_{k=1}^{K} x_{k}\\right)\\left(\\sum_{k=1}^{K} y_{k}\\right)}{K\\sum_{k=1}^{K} x_{k}^{2} - \\left(\\sum_{k=1}^{K} x_{k}\\right)^{2}}\n$$\nFinally, we substitute back the original variable definitions from the problem, $x_k = \\ln(1/s_k)$ and $y_k = \\ln N(s_k)$:\n$$\n\\hat{D} = \\frac{K\\sum_{k=1}^{K} \\left(\\ln\\left(\\frac{1}{s_{k}}\\right)\\ln N(s_{k})\\right) - \\left(\\sum_{k=1}^{K} \\ln\\left(\\frac{1}{s_{k}}\\right)\\right)\\left(\\sum_{k=1}^{K} \\ln N(s_{k})\\right)}{K\\sum_{k=1}^{K} \\left(\\ln\\left(\\frac{1}{s_{k}}\\right)\\right)^{2} - \\left(\\sum_{k=1}^{K} \\ln\\left(\\frac{1}{s_{k}}\\right)\\right)^{2}}\n$$\nThis is the closed-form ordinary least squares estimator for the box-counting dimension $D$.", "answer": "$$\n\\boxed{\\frac{K\\sum_{k=1}^{K} \\left(\\ln\\left(\\frac{1}{s_{k}}\\right)\\ln N(s_{k})\\right) - \\left(\\sum_{k=1}^{K} \\ln\\left(\\frac{1}{s_{k}}\\right)\\right)\\left(\\sum_{k=1}^{K} \\ln N(s_{k})\\right)}{K\\sum_{k=1}^{K} \\left(\\ln\\left(\\frac{1}{s_{k}}\\right)\\right)^{2} - \\left(\\sum_{k=1}^{K} \\ln\\left(\\frac{1}{s_{k}}\\right)\\right)^{2}}}\n$$", "id": "4541410"}, {"introduction": "Many complex natural patterns, including tumor textures, exhibit a range of different scaling behaviors and cannot be fully described by a single fractal dimension. This advanced hands-on coding practice [@problem_id:4541500] introduces you to multifractal analysis, which uses a continuous spectrum of dimensions, $f(\\alpha)$, to provide a much richer characterization of heterogeneity. By implementing the complete computational pipeline, you will gain a deep, practical understanding of how to quantify and interpret complex, multi-scale systems.", "problem": "You are given a two-dimensional radiomic region of interest represented as a nonnegative intensity image on a square grid. Let the grid side length be $L$, and let the image define a finite measure whose total mass is the sum of pixel intensities. Consider a scale parameter $\\epsilon = s/L$ with $s$ measured in pixels, and partition the image into non-overlapping boxes of side $s$ pixels. Let $\\mu_i(\\epsilon)$ denote the normalized mass in box $i$ at scale $\\epsilon$, so that $\\sum_i \\mu_i(\\epsilon) = 1$. The local singularity strength at box $i$ and scale $\\epsilon$ is defined by the relation $\\mu_i(\\epsilon) \\asymp \\epsilon^{\\alpha_i(\\epsilon)}$, which yields the estimator $\\alpha_i(\\epsilon) = \\log \\mu_i(\\epsilon) / \\log \\epsilon$ for boxes with $\\mu_i(\\epsilon)  0$. The multifractal singularity spectrum $f(\\alpha)$ is the box-counting dimension of the set of points with singularity strength $\\alpha$, which implies the scaling law $N_{\\epsilon}(\\alpha) \\asymp \\epsilon^{-f(\\alpha)}$, where $N_{\\epsilon}(\\alpha)$ is the number of boxes at scale $\\epsilon$ whose $\\alpha_i(\\epsilon)$ values fall within a small neighborhood of $\\alpha$.\n\nStarting from these core definitions, derive a principled computational procedure to estimate $f(\\alpha)$ directly by:\n- Computing $\\alpha_i(\\epsilon)$ from $\\mu_i(\\epsilon)$ across multiple scales $\\epsilon$.\n- Binning the $\\alpha_i(\\epsilon)$ values to approximate $N_{\\epsilon}(\\alpha)$ for each bin.\n- Performing a regression across scales to estimate $f(\\alpha)$ from the scaling of $N_{\\epsilon}(\\alpha)$.\n\nYou must implement a complete program that:\n- Generates the specified test images.\n- Computes $\\alpha_i(\\epsilon)$ at each prescribed scale using natural logarithms.\n- Excludes boxes with $\\mu_i(\\epsilon) = 0$ from $\\alpha_i(\\epsilon)$ computation.\n- Forms a histogram of $\\alpha_i(\\epsilon)$ values over a prescribed set of $\\alpha$ bins to obtain $N_{\\epsilon}(\\alpha)$.\n- For each $\\alpha$ bin, fits a line to $\\log N_{\\epsilon}(\\alpha)$ versus $\\log(1/\\epsilon)$ over scales where $N_{\\epsilon}(\\alpha)  0$, yielding an estimate of $f(\\alpha)$ as the slope. If fewer than $2$ scales have $N_{\\epsilon}(\\alpha)  0$ for a given bin, declare $f(\\alpha)$ undefined for that bin and exclude it from summary statistics.\n- From the estimated $f(\\alpha)$ sampled at bin centers, computes three summary metrics: the location of the maximum $\\alpha_{\\text{peak}}$, the maximum value $f_{\\max}$, and the full width at half maximum (FWHM) defined as the difference between the largest and smallest $\\alpha$ bin centers where $f(\\alpha) \\ge \\tfrac{1}{2} f_{\\max}$. If FWHM is undefined due to insufficient valid bins, set $\\text{FWHM} = 0$.\n\nUse the following test suite of parameter values:\n- Case $1$ (happy path): a two-dimensional dyadic multiplicative cascade on a grid of side $L=64$ pixels, constructed by $n=6$ levels of $2 \\times 2$ splits so that $2^n=L$. At each split, assign quadrant weights $w_{00} = 0.4$, $w_{01} = 0.3$, $w_{10} = 0.2$, $w_{11} = 0.1$, normalized to sum to $1$, and multiply weights down the cascade to obtain the final measure. Normalize the resulting image so that the total mass is $1$. Use scales $s \\in \\{1,2,4,8,16\\}$, so that $\\epsilon = s/L$. Use $\\alpha$-bin edges uniformly over $[0,3]$ with $B=41$ bins.\n- Case $2$ (boundary condition): a uniform image of side $L=64$ pixels with all intensities equal and normalized to total mass $1$. Use the same scales $s \\in \\{1,2,4,8,16\\}$ and the same $\\alpha$ bins over $[0,3]$ with $B=41$.\n- Case $3$ (edge case): a sparse image of side $L=64$ pixels with a single nonzero pixel of intensity $1$ at the center and zeros elsewhere, normalized to total mass $1$. Use the same scales $s \\in \\{1,2,4,8,16\\}$ and the same $\\alpha$ bins over $[0,3]$ with $B=41$.\n\nYour program should produce a single line of output containing the results as a nested list of lists of three decimal floats per case, in the order $\\big[\\alpha_{\\text{peak}}, f_{\\max}, \\text{FWHM}\\big]$ for Case $1$, Case $2$, and Case $3$, respectively. Each float must be rounded to exactly $3$ decimal places. The exact final output format must be:\n$$[[\\alpha_{\\text{peak},1},f_{\\max,1},\\text{FWHM}_1],[\\alpha_{\\text{peak},2},f_{\\max,2},\\text{FWHM}_2],[\\alpha_{\\text{peak},3},f_{\\max,3},\\text{FWHM}_3]]$$\nNo physical units are involved. Angles are not used. Express all numeric outputs as decimal floats.", "solution": "The problem requires the implementation of a direct method for estimating the multifractal singularity spectrum, $f(\\alpha)$, of a two-dimensional measure defined by a non-negative intensity image. This is a standard technique in a variety of scientific fields, including the analysis of radiomic textures. The solution proceeds by first generating the specified test images and then applying a multi-step analysis algorithm derived from the foundational principles of multifractal theory.\n\n**1. Theoretical Foundation**\n\nThe core of multifractal analysis lies in characterizing how the measure, or mass, within a region scales with the size of that region. We are given an image on a square grid of side length $L$. The analysis is performed at multiple scales $\\epsilon = s/L$, where $s$ is the side length of non-overlapping square boxes used to partition the image.\n\nThe normalized mass $\\mu_i(\\epsilon)$ in the $i$-th box is the fraction of the total image intensity contained within that box. The local singularity strength, $\\alpha_i$, quantifies the local scaling behavior of this mass, defined by the power-law relationship:\n$$ \\mu_i(\\epsilon) \\asymp \\epsilon^{\\alpha_i(\\epsilon)} $$\nwhere $\\asymp$ denotes scaling proportionality. For computational purposes, this relationship is used to define an estimator for $\\alpha_i$ at a given scale $\\epsilon$. By taking the natural logarithm of both sides, we get:\n$$ \\alpha_i(\\epsilon) = \\frac{\\ln \\mu_i(\\epsilon)}{\\ln \\epsilon} $$\nThis estimator is computed for all boxes where the mass $\\mu_i(\\epsilon)  0$.\n\nThe multifractal spectrum, $f(\\alpha)$, describes the geometric structure of the measure. It is defined as the Hausdorff (or, in this context, box-counting) dimension of the set of all points that share the same singularity strength $\\alpha$. This definition leads to another key scaling law concerning $N_{\\epsilon}(\\alpha)$, the number of boxes at scale $\\epsilon$ for which the estimated singularity strength $\\alpha_i(\\epsilon)$ falls within a small neighborhood of a value $\\alpha$:\n$$ N_{\\epsilon}(\\alpha) \\asymp \\epsilon^{-f(\\alpha)} $$\nThis law states that the number of boxes exhibiting a certain singularity strength $\\alpha$ grows as the scale $\\epsilon$ decreases, with an exponent given by $f(\\alpha)$.\n\n**2. Computational Procedure**\n\nTo estimate $f(\\alpha)$ from the scaling law for $N_{\\epsilon}(\\alpha)$, we can linearize the relationship by taking the logarithm. It is conventional and computationally convenient to use $\\log(1/\\epsilon) = -\\log\\epsilon$ as the independent variable, as it increases with finer resolution (smaller $\\epsilon$):\n$$ \\ln N_{\\epsilon}(\\alpha) \\approx C - f(\\alpha) \\ln \\epsilon = C + f(\\alpha) \\ln\\left(\\frac{1}{\\epsilon}\\right) $$\nwhere $C$ is a constant. This equation has the form of a line, $y = m x + c$, where $y = \\ln N_{\\epsilon}(\\alpha)$, $x = \\ln(1/\\epsilon)$, and the slope $m$ is the desired value $f(\\alpha)$.\n\nThe overall algorithm is as follows:\n1.  **Image Generation**: For each test case, generate the specified $L \\times L$ image.\n    -   **Case 1 (Cascade)**: A $64 \\times 64$ dyadic multiplicative cascade is constructed using $n=6$ levels. Starting with a single cell of value $1.0$, each cell is iteratively replaced by a $2 \\times 2$ grid of new cells, with values multiplied by the weights $w_{00}=0.4, w_{01}=0.3, w_{10}=0.2, w_{11}=0.1$. This is efficiently implemented using the Kronecker product. The final image is normalized to have a total mass of $1$.\n    -   **Case 2 (Uniform)**: A $64 \\times 64$ image where all pixel intensities are equal. The image is normalized so the total mass is $1$, meaning each pixel has intensity $1/64^2 = 1/4096$.\n    -   **Case 3 (Sparse)**: A $64 \\times 64$ image of zeros, with a single pixel of intensity $1$ at the center, for instance at index $(31, 31)$. The total mass is already $1$.\n\n2.  **Multi-scale Analysis**: For each scale $s \\in \\{1, 2, 4, 8, 16\\}$:\n    a.  **Box Mass Calculation**: The image is partitioned into non-overlapping boxes of size $s \\times s$. The total mass within each box is calculated, yielding the set of $\\mu_i(\\epsilon)$ values, where $\\epsilon = s/L$.\n    b.  **Singularity Calculation**: For each box with $\\mu_i(\\epsilon)  0$, the local singularity strength $\\alpha_i(\\epsilon)$ is computed using the logarithmic formula.\n    c.  **Histogramming**: The computed $\\alpha_i(\\epsilon)$ values are binned into a histogram defined over the range $[0, 3]$ with $B=41$ bins. The count in each bin gives the value $N_{\\epsilon}(\\alpha)$ for that bin and scale.\n\n3.  **Spectrum Estimation**: For each of the $41$ alpha bins (indexed by $j$):\n    a.  **Data Collection**: The counts $N_{\\epsilon}(\\alpha_j)$ and corresponding scales $\\ln(1/\\epsilon)$ are collected.\n    b.  **Linear Regression**: A straight line is fitted to the points $(\\ln(1/\\epsilon), \\ln N_{\\epsilon}(\\alpha_j))$ for all scales where $N_{\\epsilon}(\\alpha_j)  0$. If there are fewer than two such data points, $f(\\alpha_j)$ is considered undefined.\n    c.  **Slope as $f(\\alpha)$**: The slope of the fitted line provides the estimate for $f(\\alpha_j)$.\n\n4.  **Summary Statistics**: From the calculated discrete spectrum $f(\\alpha_j)$, where $\\alpha_j$ are the bin centers:\n    -   $f_{\\max}$: The maximum value among all valid (non-undefined) $f(\\alpha_j)$ estimates.\n    -   $\\alpha_{\\text{peak}}$: The alpha bin center $\\alpha_j$ corresponding to $f_{\\max}$.\n    -   FWHM: The full width at half maximum. This is computed as the difference between the largest and smallest alpha bin centers for which $f(\\alpha_j) \\ge \\frac{1}{2} f_{\\max}$. If fewer than two such points exist, FWHM is set to $0$, as specified.\n\nThis rigorous, principle-based procedure allows for a direct and robust estimation of the multifractal spectrum and its key descriptive statistics from the provided image data.", "answer": "```python\nimport numpy as np\n\ndef generate_cascade_image(L, weights):\n    \"\"\"Generates a multiplicative cascade image.\"\"\"\n    n = int(np.log2(L))\n    image = np.array([[1.0]])\n    for _ in range(n):\n        image = np.kron(image, weights)\n    image /= np.sum(image)\n    return image\n\ndef generate_uniform_image(L):\n    \"\"\"Generates a uniform intensity image.\"\"\"\n    image = np.ones((L, L))\n    image /= np.sum(image)\n    return image\n\ndef generate_sparse_image(L):\n    \"\"\"Generates an image with a single non-zero pixel at the center.\"\"\"\n    image = np.zeros((L, L))\n    # Using one of the four center pixels in a 0-indexed grid\n    center_idx = L // 2 - 1\n    image[center_idx, center_idx] = 1.0\n    return image\n\ndef multifractal_analysis(image, scales_s, alpha_bin_edges):\n    \"\"\"Performs multifractal analysis on a given image.\"\"\"\n    L = image.shape[0]\n    num_bins = len(alpha_bin_edges) - 1\n    log_inv_epsilons = [np.log(L / s) for s in scales_s]\n    \n    all_Ns = []\n    for s in scales_s:\n        epsilon = s / L\n        \n        # Calculate box masses mu_i\n        num_boxes_axis = L // s\n        mass_grid = np.zeros((num_boxes_axis, num_boxes_axis))\n        for i in range(num_boxes_axis):\n            for j in range(num_boxes_axis):\n                box = image[i*s:(i+1)*s, j*s:(j+1)*s]\n                mass_grid[i, j] = np.sum(box)\n\n        # Since the image is normalized, mass_grid contains mu_i values\n        mu_values = mass_grid.flatten()\n        mu_values = mu_values[mu_values  0]\n        \n        if len(mu_values) == 0:\n            all_Ns.append(np.zeros(num_bins))\n            continue\n            \n        # Calculate alpha_i values\n        alpha_values = np.log(mu_values) / np.log(epsilon)\n        \n        # Bin alpha_i to get N_epsilon(alpha)\n        N_s, _ = np.histogram(alpha_values, bins=alpha_bin_edges)\n        all_Ns.append(N_s)\n        \n    all_Ns = np.array(all_Ns)\n    \n    # Perform regression for each alpha bin to find f(alpha)\n    f_alpha_values = []\n    alpha_bin_centers = (alpha_bin_edges[:-1] + alpha_bin_edges[1:]) / 2.0\n    \n    for j in range(num_bins):\n        Ns_for_bin_j = all_Ns[:, j]\n        valid_indices = np.where(Ns_for_bin_j  0)[0]\n        \n        if len(valid_indices)  2:\n            f_alpha_values.append(np.nan)\n        else:\n            y = np.log(Ns_for_bin_j[valid_indices])\n            x = np.array(log_inv_epsilons)[valid_indices]\n            slope, _ = np.polyfit(x, y, 1)\n            f_alpha_values.append(slope)\n            \n    return np.array(f_alpha_values), alpha_bin_centers\n\ndef calculate_summary_stats(f_alpha, alpha_centers):\n    \"\"\"Calculates summary statistics for a given f(alpha) spectrum.\"\"\"\n    valid_indices = ~np.isnan(f_alpha)\n    \n    if not np.any(valid_indices):\n        return [0.0, 0.0, 0.0]\n        \n    valid_f_alpha = f_alpha[valid_indices]\n    valid_alpha_centers = alpha_centers[valid_indices]\n    \n    if len(valid_f_alpha) == 0:\n        return [0.0, 0.0, 0.0]\n\n    # f_max and alpha_peak\n    f_max_idx = np.argmax(valid_f_alpha)\n    f_max = valid_f_alpha[f_max_idx]\n    alpha_peak = valid_alpha_centers[f_max_idx]\n    \n    # FWHM\n    half_max = f_max / 2.0\n    above_half_max_indices = np.where(valid_f_alpha = half_max)[0]\n    \n    if len(above_half_max_indices)  2:\n        fwhm = 0.0\n    else:\n        alpha_at_half_max = valid_alpha_centers[above_half_max_indices]\n        fwhm = np.max(alpha_at_half_max) - np.min(alpha_at_half_max)\n        \n    return [alpha_peak, f_max, fwhm]\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    # Define common parameters\n    L = 64\n    scales_s = [1, 2, 4, 8, 16]\n    alpha_bin_edges = np.linspace(0, 3, 41 + 1)\n    \n    # Case 1 parameters\n    cascade_weights = np.array([[0.4, 0.3], [0.2, 0.1]])\n\n    # Define test cases\n    test_cases_params = [\n        {'type': 'cascade', 'L': L, 'weights': cascade_weights},\n        {'type': 'uniform', 'L': L},\n        {'type': 'sparse', 'L': L}\n    ]\n\n    all_results = []\n    \n    for params in test_cases_params:\n        if params['type'] == 'cascade':\n            image = generate_cascade_image(params['L'], params['weights'])\n        elif params['type'] == 'uniform':\n            image = generate_uniform_image(params['L'])\n        else: # sparse\n            image = generate_sparse_image(params['L'])\n            \n        f_alpha, alpha_centers = multifractal_analysis(image, scales_s, alpha_bin_edges)\n        stats = calculate_summary_stats(f_alpha, alpha_centers)\n        all_results.append(stats)\n\n    # Format output\n    formatted_results = []\n    for res in all_results:\n        formatted_results.append(f\"[{res[0]:.3f},{res[1]:.3f},{res[2]:.3f}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4541500"}]}