{"hands_on_practices": [{"introduction": "When we extract multiple features from a set of images, we create a \"cloud\" of points in a high-dimensional feature space. The first step to understanding this dataset is to quantify the shape and orientation of this cloud. The sample covariance matrix, $\\Sigma$, is the fundamental tool for this task, as it measures both the spread of each feature (variance) and the linear relationships between them (covariance). This practice [@problem_id:4540293] provides a concrete exercise in computing the covariance matrix from feature vectors and using its properties to diagnose multicollinearity, a common issue where features are highly correlated.", "problem": "In a radiomics study, two quantitative texture descriptors are extracted from computed tomography images for each subject: histogram skewness and Gray-Level Co-occurrence Matrix (GLCM) correlation (GLCM stands for Gray-Level Co-occurrence Matrix). Let the feature vector for subject $i$ be $x_i \\in \\mathbb{R}^2$, where the first entry is the skewness and the second entry is the GLCM correlation. The features have been mean-centered across the cohort, so $\\sum_{i=1}^{n} x_i = 0$. For $n=4$ subjects, the centered feature vectors are\n$$\nx_1 = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix},\\quad\nx_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix},\\quad\nx_3 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix},\\quad\nx_4 = \\begin{pmatrix} -3 \\\\ -3 \\end{pmatrix}.\n$$\nStarting from the core definition of the sample covariance matrix,\n$$\n\\Sigma = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})(x_i - \\bar{x})^{\\top},\n$$\nwhere $\\bar{x}$ is the sample mean vector, compute $\\Sigma$ for this dataset. Use the eigenvalues of $\\Sigma$ to assess multicollinearity by evaluating the spectral condition number,\n$$\n\\kappa(\\Sigma) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}.\n$$\nExplain briefly how the magnitudes of the eigenvalues relate to multicollinearity among the two features. Report the value of $\\kappa(\\Sigma)$ as a single real number. Round your final answer to four significant figures.", "solution": "The first step is to validate the problem statement.\nThe problem provides a set of $n=4$ mean-centered feature vectors $x_i \\in \\mathbb{R}^2$. The condition of being mean-centered, $\\sum_{i=1}^{n} x_i = 0$, is confirmed by summing the given vectors:\n$$\nx_1 + x_2 + x_3 + x_4 = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} -3 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 3+1-1-3 \\\\ 3-1+1-3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis implies that the sample mean vector is $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = 0$. The problem is well-defined, scientifically grounded in statistics, and internally consistent. Thus, it is valid.\n\nThe problem asks for the computation of the sample covariance matrix $\\Sigma$ using its definition:\n$$\n\\Sigma = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})(x_i - \\bar{x})^{\\top}\n$$\nSince the data is mean-centered, $\\bar{x} = 0$, and the formula simplifies to:\n$$\n\\Sigma = \\frac{1}{n-1} \\sum_{i=1}^{n} x_i x_i^{\\top}\n$$\nWith $n=4$, the pre-factor is $\\frac{1}{3}$. We need to compute the sum of the outer products $x_i x_i^{\\top}$ for $i=1, 2, 3, 4$.\nFor $x_1 = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}$:\n$$\nx_1 x_1^{\\top} = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix} \\begin{pmatrix} 3  3 \\end{pmatrix} = \\begin{pmatrix} 9  9 \\\\ 9  9 \\end{pmatrix}\n$$\nFor $x_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$:\n$$\nx_2 x_2^{\\top} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\begin{pmatrix} 1  -1 \\end{pmatrix} = \\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix}\n$$\nFor $x_3 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$:\n$$\nx_3 x_3^{\\top} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} -1  1 \\end{pmatrix} = \\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix}\n$$\nFor $x_4 = \\begin{pmatrix} -3 \\\\ -3 \\end{pmatrix}$:\n$$\nx_4 x_4^{\\top} = \\begin{pmatrix} -3 \\\\ -3 \\end{pmatrix} \\begin{pmatrix} -3  -3 \\end{pmatrix} = \\begin{pmatrix} 9  9 \\\\ 9  9 \\end{pmatrix}\n$$\nThe sum of these matrices is:\n$$\n\\sum_{i=1}^{4} x_i x_i^{\\top} = \\begin{pmatrix} 9  9 \\\\ 9  9 \\end{pmatrix} + \\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix} + \\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix} + \\begin{pmatrix} 9  9 \\\\ 9  9 \\end{pmatrix} = \\begin{pmatrix} 20  16 \\\\ 16  20 \\end{pmatrix}\n$$\nNow, we compute $\\Sigma$:\n$$\n\\Sigma = \\frac{1}{3} \\begin{pmatrix} 20  16 \\\\ 16  20 \\end{pmatrix} = \\begin{pmatrix} \\frac{20}{3}  \\frac{16}{3} \\\\ \\frac{16}{3}  \\frac{20}{3} \\end{pmatrix}\n$$\nNext, we find the eigenvalues of $\\Sigma$ by solving the characteristic equation $\\det(\\Sigma - \\lambda I) = 0$, where $I$ is the identity matrix.\n$$\n\\det\\left( \\begin{pmatrix} \\frac{20}{3} - \\lambda  \\frac{16}{3} \\\\ \\frac{16}{3}  \\frac{20}{3} - \\lambda \\end{pmatrix} \\right) = 0\n$$\n$$\n\\left( \\frac{20}{3} - \\lambda \\right)^2 - \\left( \\frac{16}{3} \\right)^2 = 0\n$$\n$$\n\\left( \\frac{20}{3} - \\lambda \\right)^2 = \\left( \\frac{16}{3} \\right)^2\n$$\nTaking the square root of both sides gives two possibilities:\n1. $\\frac{20}{3} - \\lambda = \\frac{16}{3} \\implies \\lambda = \\frac{20}{3} - \\frac{16}{3} = \\frac{4}{3}$\n2. $\\frac{20}{3} - \\lambda = -\\frac{16}{3} \\implies \\lambda = \\frac{20}{3} + \\frac{16}{3} = \\frac{36}{3} = 12$\nThe eigenvalues are $\\lambda_{\\max} = 12$ and $\\lambda_{\\min} = \\frac{4}{3}$.\n\nThe eigenvalues of the covariance matrix represent the variance of the data projected onto the principal component axes. The largest eigenvalue, $\\lambda_{\\max}$, corresponds to the variance along the first principal component (the direction of maximal variance in the data), while the smallest eigenvalue, $\\lambda_{\\min}$, corresponds to the variance along the last principal component (the direction of minimal variance). Multicollinearity signifies a strong linear relationship between features. Geometrically, this means the data points in the feature space lie close to a lower-dimensional subspace (a line in this $2$D case). Consequently, the variance along this line (approximated by the first principal component) is large, while the variance in a direction orthogonal to it (approximated by the last principal component) is very small. A very small $\\lambda_{\\min}$ (relative to $\\lambda_{\\max}$) indicates strong multicollinearity. The spectral condition number, $\\kappa(\\Sigma)$, is the ratio of these variances and serves as a quantitative measure of this phenomenon. A large $\\kappa(\\Sigma)$ implies that the matrix is \"ill-conditioned,\" which is the numerical manifestation of multicollinearity.\n\nFinally, we compute the spectral condition number, $\\kappa(\\Sigma)$:\n$$\n\\kappa(\\Sigma) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{12}{\\frac{4}{3}} = 12 \\times \\frac{3}{4} = 9\n$$\nThe problem requires the answer to be rounded to four significant figures. The exact value is $9$. Expressed with four significant figures, this is $9.000$.", "answer": "$$\n\\boxed{9.000}\n$$", "id": "4540293"}, {"introduction": "Many machine learning algorithms are sensitive to the scale of input features, making preprocessing a critical step. A straightforward approach is to rescale each feature to a common interval, such as $[0, 1]$. This method, known as min-max scaling, ensures that no single feature dominates the analysis simply due to its large numerical range. In this problem [@problem_id:4540269], you will derive this transformation from first principles and investigate a crucial side effect: the compression of data when extreme outliers are present, which can dramatically alter the geometry of the feature space.", "problem": "In a radiomics pipeline for Computed Tomography (CT), each subject is represented by a feature vector $\\mathbf{x} \\in \\mathbb{R}^d$. Consider a single feature coordinate $x_j$ collected over a training set of $N$ subjects. You wish to perform a linear rescaling of $x_j$ to obtain a transformed feature $\\tilde{x}_j$ that maps the smallest observed training value to $0$ and the largest observed training value to $1$. Assume the rescaling must be an affine function of $x_j$.\n\nUsing only the definition of an affine map and the boundary mapping requirements, derive the expression for $\\tilde{x}_j$ in terms of $x_j$, $\\min_j$, and $\\max_j$, where $\\min_j$ and $\\max_j$ are the minimum and maximum of $x_j$ in the training set. Then, consider the following training values for feature $x_j$ across $N=8$ subjects:\n$\\{12,14,13,15,16,14,15,200\\}.$\nCompute the approximate value of $\\tilde{x}_j$ for the subject with $x_j=16$ under your derived rescaling. Finally, reason from first principles about how an extreme outlier in $x_j$ affects the geometry of the transformed feature space under this rescaling, especially with respect to dynamic range and Euclidean distances among non-outlier subjects.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. The unique affine rescaling that sends $x_j=\\min_j$ to $0$ and $x_j=\\max_j$ to $1$ is $\\tilde{x}_j=\\dfrac{x_j-\\min_j}{\\max_j-\\min_j}$.\n\nB. For the dataset $\\{12,14,13,15,16,14,15,200\\}$, the rescaled value for $x_j=16$ is approximately $0.021$.\n\nC. In this dataset, because $\\max_j=200$ is an outlier far above the bulk, the rescaled values of the non-outlier observations occupy a narrow interval near $0$, compressing their dynamic range and diminishing the contribution of coordinate $j$ to Euclidean distances among those observations.\n\nD. With the outlier present, the majority of rescaled values cluster near $1$ because the maximum is far from the bulk.\n\nE. The min–max rescaling is invariant to strictly monotone affine reparameterizations $x_j' = a\\,x_j + b$ with $a0$, in the sense that $\\tilde{x}_j$ computed from $x_j$ equals the $\\tilde{x}_j$ computed from $x_j'$.\n\nF. After min–max rescaling, the Pearson product–moment correlation between $x_j$ and another feature becomes invariant to the presence of outliers in $x_j$.", "solution": "Begin with the definition of an affine map. An affine transformation of a scalar $x_j$ has the form $\\tilde{x}_j = a\\,x_j + b$ for constants $a$ and $b$. The boundary mapping requirements are:\n$$(1)\\quad a\\,\\min_j + b = 0,\\qquad (2)\\quad a\\,\\max_j + b = 1.$$\nSubtracting $(1)$ from $(2)$ gives\n$$a\\,(\\max_j - \\min_j) = 1 \\;\\;\\Rightarrow\\;\\; a = \\frac{1}{\\max_j - \\min_j}.$$\nSubstituting into $(1)$ yields\n$$\\frac{1}{\\max_j - \\min_j}\\,\\min_j + b = 0 \\;\\;\\Rightarrow\\;\\; b = -\\frac{\\min_j}{\\max_j - \\min_j}.$$\nTherefore,\n$$\\tilde{x}_j = a\\,x_j + b = \\frac{x_j - \\min_j}{\\max_j - \\min_j}.$$\nThis map is unique among affine functions because two distinct pointwise constraints determine a unique line.\n\nNow evaluate the numeric example. For the dataset $\\{12,14,13,15,16,14,15,200\\}$, we have $\\min_j = 12$ and $\\max_j = 200$, so the denominator is $\\max_j - \\min_j = 200 - 12 = 188$. For $x_j = 16$,\n$$\\tilde{x}_j = \\frac{16 - 12}{200 - 12} = \\frac{4}{188} \\approx 0.0212766\\ldots \\approx 0.021.$$\nSimilarly, non-outlier values $\\{12,13,14,15,16\\}$ map to $\\{0,\\; \\frac{1}{188}\\approx 0.0053,\\; \\frac{2}{188}\\approx 0.0106,\\; \\frac{3}{188}\\approx 0.0160,\\; \\frac{4}{188}\\approx 0.0213\\}$, a narrow band near $0$. This illustrates dynamic range compression for the bulk of the data when an extreme maximum outlier is present. Geometrically, if feature vectors are compared using Euclidean distance $\\|\\cdot\\|_2$, the contribution along coordinate $j$ between two non-outlier subjects with raw difference $\\Delta x_j$ becomes\n$$|\\tilde{\\Delta x}_j| = \\left|\\frac{\\Delta x_j}{\\max_j - \\min_j}\\right|,$$\nwhich is small when $\\max_j - \\min_j$ is inflated by an outlier. Hence, along coordinate $j$, pairwise distances among non-outliers shrink.\n\nFinally, consider invariance to strictly monotone affine reparameterizations $x_j' = a\\,x_j + b$ with $a0$. Under such a change, $\\min_j' = a\\,\\min_j + b$ and $\\max_j' = a\\,\\max_j + b$. The min–max rescaling computed from $x_j'$ is\n$$\\tilde{x}_j' = \\frac{x_j' - \\min_j'}{\\max_j' - \\min_j'} = \\frac{a x_j + b - (a \\min_j + b)}{a \\max_j + b - (a \\min_j + b)} = \\frac{a(x_j - \\min_j)}{a(\\max_j - \\min_j)} = \\frac{x_j - \\min_j}{\\max_j - \\min_j} = \\tilde{x}_j.$$\nThus, min–max rescaling is invariant to such reparameterizations, provided $a0$.\n\nOption-by-option analysis:\n\nA. This follows directly from the affine constraints and the derivation above. Verdict: Correct.\n\nB. With $\\min_j=12$ and $\\max_j=200$, $\\tilde{x}_j=(16-12)/188=4/188\\approx 0.0213$, so approximately $0.021$ is accurate. Verdict: Correct.\n\nC. As computed, non-outlier values map to roughly $\\{0, 0.005, 0.011, 0.016, 0.021\\}$, clustering near $0$. This compresses dynamic range for the bulk and reduces their contribution along coordinate $j$ to Euclidean distances. Verdict: Correct.\n\nD. The majority of rescaled values cluster near $0$, not near $1$, because the extreme maximum stretches the denominator. Verdict: Incorrect.\n\nE. Proven above: for $x_j' = a x_j + b$ with $a0$, min–max rescaling yields the same $\\tilde{x}_j$. Verdict: Correct.\n\nF. Pearson product–moment correlation is invariant to affine reparameterization of a variable, but it is not invariant to the presence of outliers in the data-generating values themselves. Adding an outlier can substantially change the correlation, and min–max rescaling does not eliminate this effect because it is itself an affine transformation. Therefore, the statement that correlation becomes invariant to the presence of outliers after min–max rescaling is false. Verdict: Incorrect.", "answer": "$$\\boxed{ABCE}$$", "id": "4540269"}, {"introduction": "Beyond simple scaling, we can perform more powerful transformations that reshape the geometry of our feature space in desirable ways. An ideal feature space is one where features are uncorrelated and have equal variance, forming a spherical data cloud. This process, known as \"whitening\" or \"sphering\" the data, simplifies the data structure and can improve the performance of downstream models. This exercise [@problem_id:4540232] walks through the derivation of the whitening transform, connecting the eigendecomposition of the covariance matrix to the geometric goal of turning a data ellipsoid into a perfect hypersphere.", "problem": "In radiomics, a subject is represented by a feature vector $x \\in \\mathbb{R}^{d}$ aggregating quantitative descriptors such as first-order intensity statistics and texture metrics derived from the Grey-Level Co-occurrence Matrix (GLCM) and the Grey-Level Run Length Matrix (GLRLM) computed within a Region of Interest (ROI). Consider a dataset of $N$ subjects with sample mean $\\mu \\in \\mathbb{R}^{d}$ and a symmetric positive definite sample covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$. A linear whitening transform is defined by $y = W(x - \\mu)$ for some matrix $W \\in \\mathbb{R}^{d \\times d}$.\n\nStarting only from the definitions of mean and covariance, and the spectral theorem for symmetric positive definite matrices, do the following:\n\n- Derive the condition on $W$ that guarantees the transformed feature vectors $y$ have identity covariance. Then, using the eigendecomposition of $\\Sigma$, solve for such a $W$ in closed form and express your answer using the eigenvalues and eigenvectors of $\\Sigma$.\n- Explain, using geometric reasoning tied to the Mahalanobis geometry of $\\Sigma$, why this transform maps the covariance ellipsoid of the original feature space to a hypersphere in the whitened feature space. Your explanation should articulate how axes are reoriented and rescaled.\n- Discuss why multiplying the derived whitening matrix on the left by any orthogonal matrix yields another valid whitening transform, and interpret this additional degree of freedom geometrically.\n- Provide, as your final numeric answer, the determinant of the covariance matrix of $y$ after whitening. No rounding is required. Express your final answer as a pure number without units.\n\nAssume all expectations are taken with respect to the empirical distribution defined by the dataset and that $\\Sigma$ is strictly positive definite so that all required matrix powers are well-defined. If you refer to Principal Component Analysis (PCA), spell it out as Principal Component Analysis (PCA) on its first use.", "solution": "The problem asks for a comprehensive analysis of the linear whitening transform in the context of radiomics feature vectors. The analysis will proceed in four parts as requested: deriving the whitening matrix $W$, providing a geometric interpretation of the transformation, discussing the role of orthogonal matrices, and finally computing the determinant of the resulting covariance matrix.\n\nFirst, we derive the condition on the matrix $W$ and solve for it.\nLet the original feature vectors be denoted by $x \\in \\mathbb{R}^{d}$, with a sample mean $\\mu = E[x]$ and a sample covariance matrix $\\Sigma = E[(x - \\mu)(x - \\mu)^T]$, where $E[\\cdot]$ denotes the expectation over the empirical distribution of the $N$ subjects. The transformed feature vectors are given by $y = W(x - \\mu)$.\n\nThe mean of the transformed vectors $y$ is:\n$$ \\mu_y = E[y] = E[W(x - \\mu)] = W(E[x] - \\mu) = W(\\mu - \\mu) = 0 $$\nThe transformed data is thus centered at the origin.\n\nThe covariance matrix of the transformed vectors $y$ is:\n$$ \\Sigma_y = E[(y - \\mu_y)(y - \\mu_y)^T] = E[y y^T] $$\nSubstituting the expression for $y$:\n$$ \\Sigma_y = E[ (W(x - \\mu)) (W(x - \\mu))^T ] = E[ W(x - \\mu)(x - \\mu)^T W^T ] $$\nSince $W$ is a constant matrix with respect to the expectation over the data samples, we can factor it out:\n$$ \\Sigma_y = W E[(x - \\mu)(x - \\mu)^T] W^T = W \\Sigma W^T $$\nThe condition that the transformed vectors $y$ have an identity covariance matrix is $\\Sigma_y = I$, where $I$ is the $d \\times d$ identity matrix. Therefore, the condition on $W$ is:\n$$ W \\Sigma W^T = I $$\nTo solve for $W$, we use the spectral theorem. Since $\\Sigma$ is a symmetric positive definite matrix, it admits an eigendecomposition $\\Sigma = U \\Lambda U^T$, where $U$ is an orthogonal matrix ($U^T U = U U^T = I$) whose columns are the eigenvectors of $\\Sigma$, and $\\Lambda$ is a diagonal matrix whose diagonal entries are the corresponding strictly positive eigenvalues $\\lambda_1, \\lambda_2, \\dots, \\lambda_d$.\n\nWe seek a matrix $W$ that satisfies $W (U \\Lambda U^T) W^T = I$. A common choice for $W$, which facilitates a clear geometric interpretation related to Principal Component Analysis (PCA), is $W = \\Lambda^{-1/2} U^T$. Here, $\\Lambda^{-1/2}$ is the diagonal matrix with entries $1 / \\sqrt{\\lambda_i}$. Let's verify this choice:\n$$ \\Sigma_y = (\\Lambda^{-1/2} U^T) \\Sigma (\\Lambda^{-1/2} U^T)^T = (\\Lambda^{-1/2} U^T) (U \\Lambda U^T) (U (\\Lambda^{-1/2})^T) $$\nSince $\\Lambda^{-1/2}$ is diagonal, $(\\Lambda^{-1/2})^T = \\Lambda^{-1/2}$. Using the orthogonality of $U$, $U^T U = I$:\n$$ \\Sigma_y = \\Lambda^{-1/2} (U^T U) \\Lambda (U^T U) \\Lambda^{-1/2} = \\Lambda^{-1/2} I \\Lambda I \\Lambda^{-1/2} = \\Lambda^{-1/2} \\Lambda \\Lambda^{-1/2} = \\Lambda^{(-1/2 + 1 - 1/2)} = \\Lambda^0 = I $$\nThus, a valid whitening matrix is $W = \\Lambda^{-1/2} U^T$, where $U$ and $\\Lambda$ are derived from the eigendecomposition of the original covariance matrix $\\Sigma$.\n\nSecond, we provide a geometric explanation for the transformation.\nA locus of points at a constant Mahalanobis distance $c$ from the mean $\\mu$ in the original feature space is described by the equation $(x - \\mu)^T \\Sigma^{-1} (x - \\mu) = c^2$. This equation defines a hyperellipsoid, which can be thought of as the covariance ellipsoid of the data. The principal axes of this ellipsoid are oriented along the eigenvectors of $\\Sigma$ (the columns of $U$), and the lengths of the semi-axes are proportional to the square roots of the eigenvalues, $\\sqrt{\\lambda_i}$.\n\nThe whitening transform $y = W(x - \\mu) = \\Lambda^{-1/2} U^T (x - \\mu)$ can be understood as a sequence of three operations:\n1.  **Centering:** The term $x' = x - \\mu$ translates the data so that its mean is at the origin.\n2.  **Rotation:** The operation $z = U^T x'$ rotates the centered data. Since the columns of $U$ are the principal axes of the covariance ellipsoid, this rotation aligns these axes with the standard coordinate axes. The features in the $z$ space are now uncorrelated, as their covariance matrix is $E[zz^T] = U^T \\Sigma U = \\Lambda$.\n3.  **Scaling:** The final operation $y = \\Lambda^{-1/2} z$ scales each new coordinate $z_i$ by a factor of $1/\\sqrt{\\lambda_i}$. Since the variance of $z_i$ was $\\lambda_i$, the variance of the final coordinate $y_i$ is $(1/\\sqrt{\\lambda_i})^2 \\text{Var}(z_i) = (1/\\lambda_i)\\lambda_i = 1$.\n\nGeometrically, the rotation $U^T$ aligns the covariance ellipsoid with the coordinate axes. The subsequent scaling by $\\Lambda^{-1/2}$ shrinks or stretches the ellipsoid along each of these axes, normalizing the semi-axis lengths. An ellipsoid with semi-axes of equal length is a hypersphere. To confirm this, we can substitute $x - \\mu = W^{-1} y = (U \\Lambda^{1/2}) y$ into the ellipsoid equation:\n$$ ( (U \\Lambda^{1/2}) y )^T \\Sigma^{-1} ( (U \\Lambda^{1/2}) y ) = c^2 $$\n$$ y^T (\\Lambda^{1/2})^T U^T \\Sigma^{-1} U \\Lambda^{1/2} y = c^2 $$\nUsing $\\Sigma^{-1} = U \\Lambda^{-1} U^T$, we find $U^T \\Sigma^{-1} U = \\Lambda^{-1}$.\n$$ y^T \\Lambda^{1/2} \\Lambda^{-1} \\Lambda^{1/2} y = c^2 \\implies y^T I y = c^2 \\implies \\|y\\|^2 = c^2 $$\nThis is the equation of a hypersphere of radius $c$. Thus, the whitening transform maps the covariance ellipsoid of the original data to a hypersphere in the whitened space by reorienting and rescaling its axes.\n\nThird, we discuss the role of orthogonal matrices.\nLet $W$ be any valid whitening matrix, such that $W \\Sigma W^T = I$. Let $R$ be any orthogonal matrix, meaning $R R^T = I$. Consider a new transformation matrix $W' = RW$. The covariance matrix resulting from this new transformation is:\n$$ \\Sigma'_y = W' \\Sigma (W')^T = (RW) \\Sigma (RW)^T = R (W \\Sigma W^T) R^T $$\nSince $W \\Sigma W^T = I$, this simplifies to:\n$$ \\Sigma'_y = R I R^T = R R^T = I $$\nThis demonstrates that $W' = RW$ is also a valid whitening transform. Geometrically, the initial transformation $y = W(x - \\mu)$ produces a data cloud whose covariance is a hypersphere (described by $I$). The subsequent multiplication by $R$ in $y' = Ry$ corresponds to applying a rotation and/or reflection to this already whitened, hyperspherical data cloud. A rotation or reflection of a hypersphere results in the same hypersphere. The statistical properties of being uncorrelated and having unit variance in all directions are preserved under such rigid transformations. This additional degree of freedom means that whitening is unique only up to an arbitrary rotation/reflection; it fixes the \"shape\" of the final covariance (a hypersphere) but not its \"orientation\".\n\nFinally, we provide the determinant of the covariance matrix of $y$ after whitening.\nBy definition, the whitening process transforms the data such that the resulting feature vectors $y$ have a covariance matrix equal to the identity matrix. So, the covariance matrix of $y$ is $\\Sigma_y = I$. We are asked to find the determinant of this matrix, $\\det(\\Sigma_y)$.\n$$ \\det(\\Sigma_y) = \\det(I) $$\nThe determinant of the $d \\times d$ identity matrix is always $1$.", "answer": "$$\\boxed{1}$$", "id": "4540232"}]}