{"hands_on_practices": [{"introduction": "In medical imaging, especially with CT or MRI, the spacing between pixels is often not uniform in all three dimensions, a property known as anisotropy. Applying filters designed to be isotropic, such as the Laplacian of Gaussian, directly to such anisotropic data can lead to a distortion of the filter's shape and an incorrect analysis of the underlying anatomy. This first practice guides you through quantifying the physical scale errors that arise from this mismatch, providing a clear, quantitative justification for the critical preprocessing step of resampling images to an isotropic grid before feature extraction [@problem_id:4543619].", "problem": "You will implement a physically grounded quantification of interpolation-induced blurring during resampling to isotropic voxels, and a measure of the scale mismatch that occurs if isotropic filters are applied directly on anisotropic grids. The context is radiomics, where higher-order features are computed after applying isotropic Laplacian of Gaussian (LoG) and three-dimensional wavelet filters, both of which assume physical isotropy.\n\nFoundational basis to be used:\n- The continuous Laplacian operator is rotationally invariant, and the isotropic Laplacian of Gaussian filter preserves this invariance only when the sampling grid is physically isotropic. Applying an isotropic kernel in voxel units on an anisotropic grid leads to direction-dependent effective scales.\n- The acquisition process can be approximated as a separable convolution by a rectangular voxel aperture along each axis. A one-dimensional rectangular function of width $w$ (unit area) has second central moment (variance) $w^{2}/12$.\n- Linear interpolation on a regular grid is equivalent to reconstruction with a separable triangular basis (\"hat\" function). A one-dimensional triangular function with base width $2w$ (unit area) has variance $w^{2}/6$.\n- When two independent blurs are approximated as Gaussian kernels, their convolution is approximated by another Gaussian whose variance is the sum of the component variances. This Gaussian-moment equivalence is widely used to summarize blur in a single scale parameter.\n- For a three-dimensional separable point spread function, an isotropic equivalent blur radius can be summarized by the root-mean-square of per-axis standard deviations.\n\nYour program must do the following for each test case:\n1) Given original voxel spacings $s_{x}$, $s_{y}$, $s_{z}$ in millimeters, and a target isotropic spacing $t$ in millimeters (which may be smaller or larger than each $s_{i}$), model the effective interpolation-induced blur along each axis as the convolution of the rectangular voxel aperture (width $s_{i}$) and the linear interpolation triangular basis (base width $2 s_{i}$). Use the Gaussian-moment equivalence to compute an equivalent Gaussian standard deviation along each axis, denoted $\\sigma_{x}$, $\\sigma_{y}$, $\\sigma_{z}$ in millimeters. You must start from the variances $s_{i}^{2}/12$ and $s_{i}^{2}/6$ for the rectangular and triangular kernels respectively, add them, and then take the square root to obtain $\\sigma_{i}$ for $i \\in \\{x,y,z\\}$.\n2) Compute an isotropic equivalent blur radius $\\sigma_{\\mathrm{rms}}$ in millimeters as\n$$\n\\sigma_{\\mathrm{rms}} \\;=\\; \\sqrt{\\dfrac{\\sigma_{x}^{2} + \\sigma_{y}^{2} + \\sigma_{z}^{2}}{3}} \\, .\n$$\n3) Suppose an isotropic Laplacian of Gaussian filter with intended physical scale $\\sigma_{\\mathrm{phys}}$ in millimeters is applied directly on the anisotropic grid using a single voxel-scale parameter that is held constant across axes (that is, the same per-axis Gaussian standard deviation in voxel units). If this voxel-scale parameter were chosen using the smallest spacing $s_{\\min} = \\min\\{s_{x}, s_{y}, s_{z}\\}$, the intended physical scale is matched along the axis with spacing $s_{\\min}$, but is mismatched along other axes. Quantify the worst-case relative physical scale error (dimensionless, expressed as a decimal) over the three axes under this naive practice:\n$$\n\\varepsilon \\;=\\; \\max_{i \\in \\{x,y,z\\}} \\left| \\dfrac{s_{i} - s_{\\min}}{s_{\\min}} \\right| \\, .\n$$\nThis quantifies why resampling to isotropic voxels is recommended prior to applying isotropic LoG and three-dimensional wavelet filters: without resampling, the effective physical scale depends on $s_{i}$.\n\nYour program should implement the above computations exactly, for the following test suite. All spacings and scales are in millimeters:\n- Test case $1$: $s_{x} = 0.7$, $s_{y} = 0.7$, $s_{z} = 5.0$, $t = 1.0$, $\\sigma_{\\mathrm{phys}} = 1.5$.\n- Test case $2$: $s_{x} = 1.0$, $s_{y} = 1.0$, $s_{z} = 1.0$, $t = 1.0$, $\\sigma_{\\mathrm{phys}} = 1.5$.\n- Test case $3$: $s_{x} = 1.2$, $s_{y} = 1.2$, $s_{z} = 3.0$, $t = 0.8$, $\\sigma_{\\mathrm{phys}} = 1.5$.\n- Test case $4$: $s_{x} = 0.8$, $s_{y} = 0.8$, $s_{z} = 0.8$, $t = 2.0$, $\\sigma_{\\mathrm{phys}} = 1.5$.\n\nFor each test case, your program must output a list with five floats in the following order, all expressed in millimeters except the last one which is dimensionless:\n- $\\sigma_{x}$,\n- $\\sigma_{y}$,\n- $\\sigma_{z}$,\n- $\\sigma_{\\mathrm{rms}}$,\n- $\\varepsilon$.\n\nFinal output format:\n- Produce a single line of output containing a comma-separated list of the per-test-case results, where each per-test-case result is itself a comma-separated list enclosed in square brackets. For example, an output with two test cases would look like $[ [a,b,c,d,e], [f,g,h,i,j] ]$ with all numbers in decimal notation.\n- Round every numeric entry you print to exactly $6$ digits after the decimal point.", "solution": "The problem statement is critically validated as scientifically grounded, well-posed, objective, and complete for the tasks required. It is based on established principles of digital image processing and their application in the field of radiomics. The provided data and formulas are consistent and sufficient to derive a unique, meaningful solution. The presence of the variables $t$ and $\\sigma_{\\mathrm{phys}}$, while not used in the final calculations, provides essential context for the problem's premise and does not constitute a contradiction or an instance of being overconstrained. The problem is therefore deemed **valid**.\n\nThe solution proceeds by implementing the specified computations in a step-by-step manner, grounded in the provided physical and mathematical principles.\n\n### Step 1: Per-Axis Effective Blur Standard Deviation ($\\sigma_{i}$)\n\nThe problem models the effective blur on the original, potentially anisotropic, imaging grid. This blur is a composite of two distinct processes: the data acquisition, which samples a continuous signal into discrete voxels, and the subsequent linear interpolation, which is a necessary step for resampling. These two blurring processes are modeled as convolutions. A fundamental principle of signal processing, under the Central Limit Theorem's influence, allows us to approximate these blur kernels as Gaussian functions and add their variances to find the variance of the combined blur.\n\n1.  **Acquisition Blur**: The acquisition process integrates the signal over a rectangular region, which is modeled as a convolution with a separable rectangular function. For a given axis $i \\in \\{x, y, z\\}$, the width of this rectangular function is the voxel spacing $s_i$. The problem states that a one-dimensional rectangular function of width $w$ and unit area has a second central moment (variance) of $w^2/12$. Therefore, the variance contribution from acquisition along axis $i$ is $\\text{var}_{\\text{acq}, i} = s_i^2/12$.\n\n2.  **Interpolation Blur**: Linear interpolation is mathematically equivalent to convolution with a triangular basis function (a \"hat\" function). For a grid with spacing $s_i$, the corresponding triangular basis function has a base width of $2s_i$. The problem specifies that a one-dimensional triangular function of base width $2w$ and unit area has a variance of $w^2/6$. Thus, the variance contribution from linear interpolation along axis $i$ is $\\text{var}_{\\text{interp}, i} = s_i^2/6$.\n\n3.  **Total Variance and Standard Deviation**: By the principle of variance addition for convolved independent sources, the total effective variance along axis $i$, $\\sigma_i^2$, is the sum of the acquisition and interpolation variances:\n    $$\n    \\sigma_i^2 = \\text{var}_{\\text{acq}, i} + \\text{var}_{\\text{interp}, i} = \\frac{s_i^2}{12} + \\frac{s_i^2}{6} = \\frac{s_i^2}{12} + \\frac{2s_i^2}{12} = \\frac{3s_i^2}{12} = \\frac{s_i^2}{4}\n    $$\n    The effective standard deviation, $\\sigma_i$, is the square root of the total variance:\n    $$\n    \\sigma_i = \\sqrt{\\frac{s_i^2}{4}} = \\frac{s_i}{2}\n    $$\n    This calculation is performed for each axis $i \\in \\{x, y, z\\}$, yielding $\\sigma_x$, $\\sigma_y$, and $\\sigma_z$.\n\n### Step 2: Isotropic Equivalent Blur Radius ($\\sigma_{\\mathrm{rms}}$)\n\nWhen dealing with an anisotropic point spread function (PSF) that is separable, it is common to summarize its effective scale with a single isotropic value. The problem specifies using the root-mean-square (RMS) of the per-axis standard deviations. This provides a rotationally-averaged measure of the blur radius.\n\nThe formula is given as:\n$$\n\\sigma_{\\mathrm{rms}} = \\sqrt{\\frac{\\sigma_{x}^{2} + \\sigma_{y}^{2} + \\sigma_{z}^{2}}{3}}\n$$\nSubstituting the expression for $\\sigma_i^2$ from Step 1, we get:\n$$\n\\sigma_{\\mathrm{rms}} = \\sqrt{\\frac{(s_x^2/4) + (s_y^2/4) + (s_z^2/4)}{3}} = \\sqrt{\\frac{s_x^2 + s_y^2 + s_z^2}{12}}\n$$\nThis formula is used to compute the single isotropic equivalent blur radius.\n\n### Step 3: Worst-Case Relative Physical Scale Error ($\\varepsilon$)\n\nThis metric quantifies a critical issue in radiomics: applying filters designed for isotropic grids directly onto anisotropic data. An isotropic filter, such as a 3D Laplacian of Gaussian, is typically defined by a single scale parameter, $\\sigma_{\\text{vox}}$, in voxel units. When this filter is applied to an anisotropic grid with physical spacings $s_x, s_y, s_z$, the effective physical scale of the filter becomes direction-dependent: $\\sigma_{\\text{phys}, i} = \\sigma_{\\text{vox}} \\cdot s_i$.\n\nThe problem describes a naive but common practice where the filter's voxel-based scale, $\\sigma_{\\text{vox}}$, is chosen to match a desired physical scale, $\\sigma_{\\mathrm{phys}}$, along the axis with the highest resolution (i.e., minimum spacing, $s_{\\min} = \\min\\{s_x, s_y, s_z\\}$). This implies $\\sigma_{\\mathrm{phys}} = \\sigma_{\\text{vox}} \\cdot s_{\\min}$, or $\\sigma_{\\text{vox}} = \\sigma_{\\mathrm{phys}} / s_{\\min}$.\n\nThe effective physical scale along any other axis $i$ is then $\\sigma_{\\text{phys}, i} = \\sigma_{\\text{vox}} \\cdot s_i = (\\sigma_{\\mathrm{phys}} / s_{\\min}) \\cdot s_i$. The relative error between this effective scale and the intended physical scale $\\sigma_{\\mathrm{phys}}$ is:\n$$\n\\text{error}_i = \\frac{|\\sigma_{\\text{phys}, i} - \\sigma_{\\mathrm{phys}}|}{\\sigma_{\\mathrm{phys}}} = \\frac{|(\\sigma_{\\mathrm{phys}} / s_{\\min}) \\cdot s_i - \\sigma_{\\mathrm{phys}}|}{\\sigma_{\\mathrm{phys}}} = \\left| \\frac{s_i}{s_{\\min}} - 1 \\right| = \\left| \\frac{s_i - s_{\\min}}{s_{\\min}} \\right|\n$$\nThe quantity $\\varepsilon$ is defined as the worst-case (maximum) of this error over all three axes:\n$$\n\\varepsilon = \\max_{i \\in \\{x,y,z\\}} \\left| \\frac{s_{i} - s_{\\min}}{s_{\\min}} \\right|\n$$\nThis calculation requires finding the minimum spacing $s_{\\min}$ and then evaluating the expression for each $s_i$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes radiomic metrics related to grid anisotropy for a suite of test cases.\n    \"\"\"\n    # Test cases: (s_x, s_y, s_z, t, sigma_phys) all in millimeters.\n    # The variables t and sigma_phys are provided for context as per the problem\n    # statement, but are not used in the specified calculations.\n    test_cases = [\n        (0.7, 0.7, 5.0, 1.0, 1.5),\n        (1.0, 1.0, 1.0, 1.0, 1.5),\n        (1.2, 1.2, 3.0, 0.8, 1.5),\n        (0.8, 0.8, 0.8, 2.0, 1.5),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        s_x, s_y, s_z, t, sigma_phys = case\n        spacings = np.array([s_x, s_y, s_z])\n\n        # Step 1: Calculate per-axis effective blur standard deviations (sigma_i).\n        # The total variance is the sum of acquisition (s_i^2/12) and linear\n        # interpolation (s_i^2/6) variances. Total variance = s_i^2/4.\n        # The standard deviation sigma_i is the square root, so sigma_i = s_i/2.\n        sigmas = spacings / 2.0\n        sigma_x, sigma_y, sigma_z = sigmas[0], sigmas[1], sigmas[2]\n\n        # Step 2: Calculate the isotropic equivalent blur radius (sigma_rms).\n        # sigma_rms = sqrt((sigma_x^2 + sigma_y^2 + sigma_z^2) / 3).\n        # This simplifies to sqrt((s_x^2 + s_y^2 + s_z^2) / 12).\n        sigma_rms = np.sqrt(np.sum(spacings**2) / 12.0)\n\n        # Step 3: Calculate the worst-case relative physical scale error (epsilon).\n        s_min = np.min(spacings)\n        if s_min == 0:\n            # Avoid division by zero, though not expected with problem data.\n            epsilon = 0.0\n        else:\n            relative_errors = np.abs((spacings - s_min) / s_min)\n            epsilon = np.max(relative_errors)\n\n        # Store the 5 required float values for the current test case.\n        case_results = [sigma_x, sigma_y, sigma_z, sigma_rms, epsilon]\n        all_results.append(case_results)\n\n    # Format the final output string according to the problem specification.\n    # Each sub-list is formatted as \"[n1,n2,n3,n4,n5]\" with numbers\n    # rounded to 6 decimal places. These are then joined into a main list.\n    # e.g., \"[[a,b,c,d,e],[f,g,h,i,j]]\"\n    formatted_rows = []\n    for row in all_results:\n        formatted_numbers = [f\"{num:.6f}\" for num in row]\n        formatted_rows.append(f\"[{','.join(formatted_numbers)}]\")\n    \n    final_output = f\"[{','.join(formatted_rows)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "4543619"}, {"introduction": "The Laplacian of Gaussian (LoG) filter is a cornerstone of feature extraction, renowned for its ability to detect blob-like structures at specific scales. Building a high-quality discrete LoG filter from its continuous mathematical form, however, involves important practical decisions. This exercise [@problem_id:4543676] takes you through the principled construction of a 3D LoG kernel, from determining the optimal kernel size based on an energy-truncation criterion to evaluating the rotational symmetry of the final discrete filter.", "problem": "You are tasked with constructing a three-dimensional ($3$D) discrete Laplacian of Gaussian (LoG) kernel suitable for radiomics feature computation from isotropic volumetric images. Your program must implement a principled method to select the truncation radius that preserves rotational symmetry and controls energy loss in the truncated tails.\n\nStart from the following foundational definitions and facts:\n\n- The three-dimensional isotropic Gaussian with standard deviation $\\sigma$ is the radially symmetric function $G_{\\sigma}(\\mathbf{x})$ on $\\mathbb{R}^{3}$, where $\\mathbf{x} \\in \\mathbb{R}^{3}$ and $r = \\|\\mathbf{x}\\|$, such that $G_{\\sigma}(\\mathbf{x}) = a_{\\sigma} \\exp\\!\\left(-\\dfrac{r^{2}}{2\\sigma^{2}}\\right)$ with $a_{\\sigma} > 0$ chosen so that $\\int_{\\mathbb{R}^{3}} G_{\\sigma}(\\mathbf{x}) \\, d\\mathbf{x} = 1$.\n- The Laplacian of Gaussian (LoG) is defined by the Laplacian operator $\\nabla^{2}$ acting on $G_{\\sigma}$, that is, $\\mathrm{LoG}_{\\sigma}(\\mathbf{x}) = \\nabla^{2} G_{\\sigma}(\\mathbf{x})$. In an isotropic medium, $\\mathrm{LoG}_{\\sigma}(\\mathbf{x})$ is a radial function of $r$.\n- The squared energy (squared $L^{2}$ norm) of a function $f$ over $\\mathbb{R}^{3}$ is $\\|f\\|_{2}^{2} = \\int_{\\mathbb{R}^{3}} |f(\\mathbf{x})|^{2} \\, d\\mathbf{x}$. For a radial function $f(r)$, the energy reduces to a one-dimensional integral with measure $4\\pi r^{2} \\, dr$.\n- In isotropic sampling with unit voxel spacing, a radially symmetric continuous kernel $k(r)$ can be discretized onto the integer lattice $\\mathbb{Z}^{3}$ by sampling at integer offsets $(i,j,k)$ with $r = \\sqrt{i^{2} + j^{2} + k^{2}}$.\n\nYour tasks are:\n\n- For each given pair $(\\sigma,\\varepsilon)$ with $\\sigma > 0$ and $\\varepsilon \\in (0,1)$, determine the smallest nonnegative integer radius $R$ (in voxels) such that the fraction of squared LoG energy outside the ball of radius $R$ is at most $\\varepsilon$. Formally, let $L_{\\sigma}(r)$ denote the continuous radial LoG. Define the total energy $E_{\\mathrm{tot}} = 4\\pi \\int_{0}^{\\infty} L_{\\sigma}(r)^{2} r^{2} \\, dr$ and the tail energy $E_{\\mathrm{tail}}(R) = 4\\pi \\int_{R}^{\\infty} L_{\\sigma}(r)^{2} r^{2} \\, dr$. Choose the minimal integer $R \\ge 0$ such that $E_{\\mathrm{tail}}(R)/E_{\\mathrm{tot}} \\le \\varepsilon$. The justification of truncation must follow from this energy ratio criterion.\n- Construct the discrete kernel by sampling the continuous $\\mathrm{LoG}_{\\sigma}$ on the integer lattice within the cube $[-R,R]^{3} \\cap \\mathbb{Z}^{3}$. Enforce the zero-sum property discretely by subtracting the mean of the sampled kernel values so that the discrete sum equals $0$ (up to numerical precision), which preserves the band-pass behavior of the LoG in discrete convolution while maintaining rotational symmetry because the subtraction is spatially uniform.\n- Quantify the rotational symmetry of the constructed discrete kernel as follows. Partition the sampled points into spherical shells by radial bins of fixed width $\\Delta r = 0.5$ (in voxel units). For the set of absolute kernel values in each shell, compute the coefficient of variation defined as the standard deviation divided by the mean of absolute values in that shell (use a small positive constant only to prevent division by zero). Report the maximum of these shell-wise coefficients of variation over all shells that contain at least $6$ points. A small maximum value implies that within shells (points at similar radius but different angles), the kernel is nearly constant in magnitude, indicating good rotational symmetry upon discretization.\n\nImplementation requirements:\n\n- The program must determine $R$ using the continuous energy ratio criterion above (not by ad hoc heuristics), then sample the kernel and compute the rotational symmetry metric defined above.\n- Your program must not read any input. Use the following test suite of $(\\sigma,\\varepsilon)$ pairs:\n  - $(\\sigma,\\varepsilon) = (1.0, 10^{-3})$\n  - $(\\sigma,\\varepsilon) = (1.5, 10^{-4})$\n  - $(\\sigma,\\varepsilon) = (2.0, 10^{-6})$\n  - $(\\sigma,\\varepsilon) = (0.6, 10^{-3})$\n- For each test case, output a list containing:\n  - the kernel edge length $N = 2R + 1$ as an integer,\n  - the achieved continuous tail energy ratio $E_{\\mathrm{tail}}(R)/E_{\\mathrm{tot}}$ as a float,\n  - the maximum shell-wise coefficient of variation (as defined above) as a float.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list in the order specified above. For example, the final output should look like a single line in the form $[[N_{1},e_{1},v_{1}],[N_{2},e_{2},v_{2}],\\dots]$.\n\nNotes:\n\n- Angle units are not used in this problem.\n- No physical units are involved beyond the voxel unit implied by the discrete grid.\n- All numerical values must be in standard floating-point format in the program; the mathematical descriptions above use exact real-valued integrals.", "solution": "The problem is well-posed, scientifically grounded, and provides sufficient information for a unique, verifiable solution. We proceed with a step-by-step derivation and algorithmic design.\n\n### Part 1: Continuous Laplacian of Gaussian (LoG) and Energy Criterion\n\nThe first step is to derive the analytical expression for the three-dimensional Laplacian of Gaussian (LoG) and formulate the energy-based truncation criterion.\n\n**1.1. The 3D Isotropic Gaussian and LoG Functions**\n\nThe problem defines the 3D isotropic Gaussian as $G_{\\sigma}(\\mathbf{x}) = a_{\\sigma} \\exp(-\\frac{r^2}{2\\sigma^2})$, where $r = \\|\\mathbf{x}\\| = \\sqrt{x^2+y^2+z^2}$. The normalization constant $a_{\\sigma}$ is determined by the condition $\\int_{\\mathbb{R}^3} G_{\\sigma}(\\mathbf{x}) \\, d\\mathbf{x} = 1$. Evaluating this integral in spherical coordinates gives:\n$$ \\int_{0}^{\\infty} a_{\\sigma} e^{-\\frac{r^2}{2\\sigma^2}} 4\\pi r^2 \\, dr = 1 $$\nThe standard Gaussian integral in 3D is $\\int_{\\mathbb{R}^3} e^{-\\|\\mathbf{x}\\|^2/(2\\sigma^2)} \\, d\\mathbf{x} = (2\\pi\\sigma^2)^{3/2}$. Thus, $a_{\\sigma} (2\\pi\\sigma^2)^{3/2} = 1$, which implies $a_{\\sigma} = (2\\pi\\sigma^2)^{-3/2}$. The normalized Gaussian is:\n$$ G_{\\sigma}(r) = (2\\pi\\sigma^2)^{-3/2} \\exp\\left(-\\frac{r^2}{2\\sigma^2}\\right) $$\nThe LoG is defined as $L_{\\sigma}(r) = \\nabla^2 G_{\\sigma}(r)$. For a radial function $f(r)$, the Laplacian is $\\nabla^2 f(r) = f''(r) + \\frac{2}{r}f'(r)$. We compute the derivatives of $G_{\\sigma}(r)$:\n$$ G'_{\\sigma}(r) = G_{\\sigma}(r) \\cdot \\left(-\\frac{r}{\\sigma^2}\\right) $$\n$$ G''_{\\sigma}(r) = G'_{\\sigma}(r) \\cdot \\left(-\\frac{r}{\\sigma^2}\\right) + G_{\\sigma}(r) \\cdot \\left(-\\frac{1}{\\sigma^2}\\right) = G_{\\sigma}(r)\\left(\\frac{r^2}{\\sigma^4} - \\frac{1}{\\sigma^2}\\right) $$\nCombining these, the LoG is:\n$$ L_{\\sigma}(r) = \\nabla^2 G_{\\sigma}(r) = \\left(\\frac{r^2}{\\sigma^4} - \\frac{1}{\\sigma^2}\\right)G_{\\sigma}(r) + \\frac{2}{r}\\left(-\\frac{r}{\\sigma^2}\\right)G_{\\sigma}(r) = \\left(\\frac{r^2}{\\sigma^4} - \\frac{3}{\\sigma^2}\\right)G_{\\sigma}(r) $$\nSubstituting the expression for $G_{\\sigma}(r)$:\n$$ L_{\\sigma}(r) = \\frac{1}{\\sigma^2}\\left(\\frac{r^2}{\\sigma^2} - 3\\right) (2\\pi\\sigma^2)^{-3/2} \\exp\\left(-\\frac{r^2}{2\\sigma^2}\\right) = \\frac{1}{(2\\pi)^{3/2}\\sigma^5}\\left(\\frac{r^2}{\\sigma^2} - 3\\right) \\exp\\left(-\\frac{r^2}{2\\sigma^2}\\right) $$\n\n**1.2. Energy Truncation Criterion**\n\nThe truncation radius $R$ is determined by the fraction of the LoG's squared energy contained in the tails. The total energy is $E_{\\mathrm{tot}} = \\int_{\\mathbb{R}^3} |L_{\\sigma}(\\mathbf{x})|^2 \\, d\\mathbf{x}$, which for a radial function becomes:\n$$ E_{\\mathrm{tot}} = 4\\pi \\int_{0}^{\\infty} L_{\\sigma}(r)^2 r^2 \\, dr $$\nThe tail energy outside a ball of radius $R$ is:\n$$ E_{\\mathrm{tail}}(R) = 4\\pi \\int_{R}^{\\infty} L_{\\sigma}(r)^2 r^2 \\, dr $$\nThe condition is to find the minimal integer $R \\ge 0$ such that the ratio $\\frac{E_{\\mathrm{tail}}(R)}{E_{\\mathrm{tot}}} \\le \\varepsilon$. The integrand for these energy calculations is $L_{\\sigma}(r)^2 r^2$:\n$$ L_{\\sigma}(r)^2 r^2 = \\frac{1}{(2\\pi)^3 \\sigma^{10}} \\left(\\frac{r^2}{\\sigma^2} - 3\\right)^2 \\exp\\left(-\\frac{r^2}{\\sigma^2}\\right) r^2 $$\nThe constant pre-factor $\\frac{1}{(2\\pi)^3 \\sigma^{10}}$ cancels out in the energy ratio. Therefore, we only need to consider the integrals of the function $f(r, \\sigma) = \\left(\\frac{r^2}{\\sigma^2} - 3\\right)^2 \\exp\\left(-\\frac{r^2}{\\sigma^2}\\right) r^2$:\n$$ \\frac{E_{\\mathrm{tail}}(R)}{E_{\\mathrm{tot}}} = \\frac{\\int_{R}^{\\infty} f(r, \\sigma) \\, dr}{\\int_{0}^{\\infty} f(r, \\sigma) \\, dr} \\le \\varepsilon $$\nThese integrals do not have a simple closed-form anti-derivative and are best computed using numerical quadrature.\n\n### Part 2: Algorithmic Procedure\n\n**2.1. Finding the Truncation Radius $R$**\n\nFor each pair $(\\sigma, \\varepsilon)$, we find the minimal integer $R$ by the following iterative search:\n1. Compute the total energy integral $I_{\\mathrm{tot}} = \\int_{0}^{\\infty} f(r, \\sigma) \\, dr$ once using a numerical quadrature method.\n2. Iterate through integer radii $R = 0, 1, 2, \\ldots$.\n3. In each iteration, compute the tail energy integral $I_{\\mathrm{tail}}(R) = \\int_{R}^{\\infty} f(r, \\sigma) \\, dr$.\n4. Calculate the energy ratio $e_R = I_{\\mathrm{tail}}(R) / I_{\\mathrm{tot}}$.\n5. The first integer $R$ for which $e_R \\le \\varepsilon$ is the desired truncation radius. The kernel edge length is then $N = 2R + 1$. The achieved energy ratio is $e_R$.\n\n**2.2. Constructing the Discrete Kernel**\n\nWith the determined radius $R$, the discrete kernel is constructed by sampling the continuous LoG function on a 3D integer grid.\n1. Define the grid of points $(i, j, k)$ where $i, j, k \\in \\{-R, -R+1, \\ldots, R\\}$.\n2. For each point, calculate its Euclidean distance from the origin: $r_{ijk} = \\sqrt{i^2 + j^2 + k^2}$.\n3. The initial discrete kernel values are $K_{ijk} = L_{\\sigma}(r_{ijk})$. Any overall scaling constant in $L_{\\sigma}$ can be omitted at this stage as it does not affect the subsequent symmetry calculation. For numerical stability, we can sample the function $L'_{\\sigma}(r) = (\\frac{r^2}{\\sigma^2}-3)\\exp(-\\frac{r^2}{2\\sigma^2})$.\n4. To enforce the zero-sum property, a crucial aspect of discrete filters that correspond to derivatives, we subtract the mean of all sampled values from each element. Let the set of all sampled values be $\\{K_{ijk}\\}$. The mean is $\\bar{K} = \\frac{1}{(2R+1)^3} \\sum_{i,j,k=-R}^R K_{ijk}$.\n5. The final, mean-corrected kernel is $K'_{ijk} = K_{ijk} - \\bar{K}$. The sum $\\sum_{i,j,k} K'_{ijk}$ is now zero (up to machine precision).\n\n**2.3. Quantifying Rotational Symmetry**\n\nThe rotational symmetry of the final discrete kernel $K'$ is assessed by analyzing the variation of its values within spherical shells.\n1. The kernel points $(i,j,k)$ are partitioned into bins based on their radius $r_{ijk}$. Bins have a fixed width of $\\Delta r = 0.5$, so bin $m$ contains points with radii $r \\in [m \\cdot \\Delta r, (m+1) \\cdot \\Delta r)$.\n2. For each bin that contains at least $6$ points, we compute the coefficient of variation (CV) for the set of absolute kernel values $\\{|K'_{ijk}|\\}$ within that bin.\n3. The CV for a bin is defined as the standard deviation of its values divided by their mean. If the mean is zero (which only happens if all absolute values are zero), the CV is taken to be $0$.\n4. The final symmetry metric is the maximum CV found among all qualifying bins. A lower value indicates better rotational symmetry, as the kernel magnitudes are more constant at similar radial distances.\n\nThis comprehensive procedure allows us to construct a principled LoG kernel and objectively evaluate its discrete rotational symmetry.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import integrate\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        (1.0, 1e-3),\n        (1.5, 1e-4),\n        (2.0, 1e-6),\n        (0.6, 1e-3),\n    ]\n\n    all_results = []\n    for sigma, epsilon in test_cases:\n        # Part 1: Determine the truncation radius R\n        R, achieved_energy_ratio = find_truncation_radius(sigma, epsilon)\n        \n        # Part 2: Construct the discrete kernel\n        N = 2 * R + 1\n        coords = np.mgrid[-R:R+1, -R:R+1, -R:R+1].astype(np.float64)\n        radii = np.sqrt(coords[0]**2 + coords[1]**2 + coords[2]**2)\n\n        # We use the unnormalized LoG for sampling as constants do not affect CV.\n        # This is proportional to the full L_sigma(r) function.\n        # The form is (r^2/sigma^2 - 3) * exp(-r^2/(2*sigma^2))\n        r_sq_over_sigma_sq = (radii**2) / (sigma**2)\n        kernel = (r_sq_over_sigma_sq - 3.0) * np.exp(-0.5 * r_sq_over_sigma_sq)\n        \n        # Enforce zero-sum property\n        mean_corrected_kernel = kernel - np.mean(kernel)\n        \n        # Part 3: Quantify rotational symmetry\n        max_cv = calculate_rotational_symmetry(mean_corrected_kernel, radii)\n        \n        result_tuple = [N, achieved_energy_ratio, max_cv]\n        all_results.append(result_tuple)\n\n    # Final print statement in the exact required format.\n    # Produces a single line, e.g., [[N1,e1,v1],[N2,e2,v2],...]\n    result_str = \",\".join([f\"[{N},{e:.8f},{v:.8f}]\" for N, e, v in all_results])\n    print(f\"[{result_str}]\")\n\n\ndef find_truncation_radius(sigma, epsilon):\n    \"\"\"\n    Finds the smallest integer radius R satisfying the energy criterion.\n    \"\"\"\n    # Integrand for energy calculation, omitting constant factors that cancel.\n    def energy_integrand(r, s):\n        r_sq_over_s_sq = (r**2) / (s**2)\n        # The exponential term comes from squaring L_sigma, which has exp(-r^2/(2*sigma^2)).\n        # So L_sigma^2 has exp(-r^2/sigma^2).\n        return (r_sq_over_s_sq - 3.0)**2 * np.exp(-r_sq_over_s_sq) * r**2\n\n    # Calculate total energy once\n    total_energy, _ = integrate.quad(energy_integrand, 0, np.inf, args=(sigma,))\n\n    if total_energy == 0:\n        return 0, 1.0\n\n    R = 0\n    while True:\n        tail_energy, _ = integrate.quad(energy_integrand, R, np.inf, args=(sigma,))\n        ratio = tail_energy / total_energy if total_energy > 0 else 1.0\n        \n        if ratio = epsilon:\n            return R, ratio\n        R += 1\n        # Safety break for very large R\n        if R > 100:\n            # This should not be reached with typical inputs\n            raise RuntimeError(\"Exceeded maximum search radius for R.\")\n\n\ndef calculate_rotational_symmetry(kernel, radii):\n    \"\"\"\n    Calculates the maximum shell-wise coefficient of variation.\n    \"\"\"\n    delta_r = 0.5\n    \n    flat_radii = radii.flatten()\n    flat_abs_values = np.abs(kernel.flatten())\n    \n    # Determine the number of bins needed\n    max_radius = np.max(flat_radii) if flat_radii.size > 0 else 0\n    # Add one bin for safety, handles max_radius falling exactly on a bin edge.\n    num_bins = int(np.ceil(max_radius / delta_r)) + 1 \n    \n    bins = [[] for _ in range(num_bins)]\n    \n    for r, val in zip(flat_radii, flat_abs_values):\n        bin_idx = int(r / delta_r)\n        if bin_idx  num_bins:\n            bins[bin_idx].append(val)\n    \n    max_cv = 0.0\n    for shell_values in bins:\n        if len(shell_values) >= 6:\n            mean_val = np.mean(shell_values)\n            std_dev = np.std(shell_values)\n            \n            # Prevent division by zero, as specified.\n            if mean_val > 1e-12: # A small positive constant to check against\n                cv = std_dev / mean_val\n                if cv > max_cv:\n                    max_cv = cv\n    \n    return max_cv\n\nif __name__ == '__main__':\n    solve()\n```", "id": "4543676"}, {"introduction": "Beyond single filters, filter banks provide a richer, multiscale representation of image texture. The Discrete Wavelet Transform (DWT) is a powerful filter bank that decomposes an image into different frequency subbands corresponding to different scales and orientations. In this final practice [@problem_id:4543681], you will implement the Haar wavelet transform and then compute two fundamental radiomic features—energy and entropy—from the resulting subbands to create a quantitative multiscale signature of image texture.", "problem": "You are given a square two-dimensional Region of Interest (ROI) image as a real-valued matrix and asked to compute multiscale radiomic texture features derived from a discrete wavelet transform using an orthonormal, separable Haar basis. The goal is to define and compute, at each decomposition level, subband-wise energy and entropy features, and then aggregate them across levels to capture a multiscale texture signature.\n\nStart from the following fundamental base:\n- A two-dimensional orthonormal transform preserves energy, i.e., if the transform is orthonormal then the sum of squared coefficients equals the sum of squared input pixels. This holds for the separable Haar wavelet transform.\n- The separable Haar wavelet transform applies one-dimensional low-pass and high-pass orthonormal filters along rows and columns with downsampling by a factor of $2$ per dimension at each level. The resulting subbands at each level $l$ are the low-low approximation $LL_l$ (used as input to the next level) and the detail subbands $LH_l$, $HL_l$, and $HH_l$.\n- For a finite collection of nonnegative values $\\{p_k\\}$ summing to $1$, the Shannon entropy in bits is $H = - \\sum_k p_k \\log_2 p_k$, with the convention that $0 \\log_2 0 = 0$.\n\nDefinitions to be used in this problem:\n- For any subband matrix $S$, define its energy as $E(S) = \\sum_{i,j} S_{i,j}^2$.\n- For any subband matrix $S$, define its entropy $H(S)$ as the Shannon entropy of the normalized coefficient energy distribution, i.e., let $p_{i,j} = S_{i,j}^2 / \\sum_{u,v} S_{u,v}^2$, and define $H(S) = - \\sum_{i,j} p_{i,j} \\log_2 p_{i,j}$ with $0 \\log_2 0$ interpreted as $0$. If $\\sum_{i,j} S_{i,j}^2 = 0$, define $H(S) = 0$.\n\nAggregation across levels:\n- Let there be $L$ levels of decomposition. For each level $l \\in \\{1,\\dots,L\\}$, compute the per-level detail energy $E_l = E(LH_l) + E(HL_l) + E(HH_l)$.\n- Define the per-level normalized detail energy as $e_l = E_l \\big/ \\left(\\sum_{k=1}^{L} E_k\\right)$, with the convention that if $\\sum_{k=1}^{L} E_k = 0$ then $e_l = 0$.\n- Define the per-level mean entropy as $h_l = \\left(H(LH_l) + H(HL_l) + H(HH_l)\\right) / 3$.\n- Let the total input energy be $E_{\\mathrm{tot}} = \\sum_{i,j} X_{i,j}^2$. Define the global detail energy fraction as $f = \\left(\\sum_{l=1}^{L} E_l\\right) / E_{\\mathrm{tot}}$.\n\nYour task:\n- Implement a function that performs a $L$-level two-dimensional Haar wavelet transform on the ROI and computes $\\{e_l\\}_{l=1}^{L}$, $\\{h_l\\}_{l=1}^{L}$, and $f$ as defined above.\n- For each ROI, output a vector that concatenates per-level normalized detail energies and per-level mean entropies across levels, followed by the global detail energy fraction:\n  $$\\left[e_1, h_1, e_2, h_2, \\dots, e_L, h_L, f\\right].$$\n- Round each real number in the output vector to six decimal places for standardized reporting.\n\nAssumptions and constraints:\n- All ROIs are square with side length divisible by $2^L$ to permit $L$ levels of decimation.\n- ROI intensities are unitless real numbers.\n\nTest suite to implement inside your program (no user input):\n- Case $1$: An $8 \\times 8$ uniform ROI with all entries equal to $5.0$ and $L = 3$.\n- Case $2$: An $8 \\times 8$ checkerboard ROI with entries $X_{i,j} = (-1)^{i+j}$ for $i,j \\in \\{0,\\dots,7\\}$ and $L = 3$.\n- Case $3$: An $8 \\times 8$ horizontal gradient ROI with entries $X_{i,j} = j$ for $i,j \\in \\{0,\\dots,7\\}$ and $L = 3$.\n- Case $4$: An $8 \\times 8$ random ROI drawn from a standard normal distribution with a fixed seed $0$, i.e., $X_{i,j} \\sim \\mathcal{N}(0,1)$ using a pseudorandom number generator initialized with seed $0$, and $L = 3$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of per-case results, where each result is the vector $\\left[e_1, h_1, e_2, h_2, e_3, h_3, f\\right]$ for $L = 3$. The numbers must be rounded to six decimal places. The final line must be a single string in the exact format:\n  \"[[v11,v12,...,v1,7],[v21,v22,...,v2,7],[v31,v32,...,v3,7],[v41,v42,...,v4,7]]\"\nwhere $v_{k,m}$ denotes the $m$-th element of the vector for case $k$. No additional text should be printed.", "solution": "The problem requires the computation of a set of multiscale texture features from a two-dimensional Region of Interest (ROI), represented as a matrix of real numbers. These features are derived from the coefficients of a two-dimensional discrete Haar wavelet transform. The process involves signal decomposition, feature extraction from subbands at each decomposition level, and aggregation of these features into a final signature vector. The methodology is grounded in the principles of wavelet analysis and information theory.\n\nFirst, we address the core transformation: the $L$-level two-dimensional separable orthonormal Haar wavelet transform. The Haar basis is the simplest wavelet basis. Its one-dimensional (1D) low-pass filter is defined by the coefficients $[1/\\sqrt{2}, 1/\\sqrt{2}]$ and the high-pass filter by $[1/\\sqrt{2}, -1/\\sqrt{2}]$. The factor $1/\\sqrt{2}$ ensures that the transform is orthonormal, a property which guarantees energy preservation. A single level of the separable two-dimensional (2D) transform on an $N \\times N$ input matrix $X$ proceeds in two steps:\n$1$. The 1D Haar transform is applied to each row of $X$. This yields two matrices of size $N \\times (N/2)$: one containing the low-pass (approximation) coefficients and another containing the high-pass (detail) coefficients.\n$2$. The 1D Haar transform is then applied to each column of these two intermediate matrices. This results in four $(N/2) \\times (N/2)$ subband matrices:\n- $LL_1$: Approximation subband (low-pass in both row and column directions).\n- $LH_1$: Detail subband (low-pass rows, high-pass columns), capturing horizontal features.\n- $HL_1$: Detail subband (high-pass rows, low-pass columns), capturing vertical features.\n- $HH_1$: Detail subband (high-pass in both directions), capturing diagonal features.\n\nFor an $L$-level decomposition, this process is applied recursively. The approximation subband $LL_l$ from level $l$ serves as the input for the decomposition at level $l+1$. This recursive decomposition separates the image into components at different scales (frequencies), with level $l=1$ corresponding to the finest details (highest frequencies) and level $L$ to coarser features (lower frequencies). The problem specifies that the input ROI size is a multiple of $2^L$, ensuring that this 'decimation' or downsampling by a factor of $2$ in each dimension is always possible.\n\nNext, we define the features to be extracted from the wavelet coefficients. Two fundamental quantities are defined for any subband matrix $S$:\n- **Energy**: The energy of a subband is defined as the sum of the squares of its coefficients: $E(S) = \\sum_{i,j} S_{i,j}^2$. This measures the signal power contained within the frequency band corresponding to $S$.\n- **Entropy**: The entropy of a subband, $H(S)$, is defined as the Shannon entropy of the normalized energy distribution of its coefficients. We construct a probability distribution $\\{p_{i,j}\\}$ where each $p_{i,j} = S_{i,j}^2 / E(S)$. The entropy is then $H(S) = - \\sum_{i,j} p_{i,j} \\log_2 p_{i,j}$. This feature quantifies the complexity or randomness of the texture within the subband. A low entropy suggests a regular pattern where energy is concentrated in a few coefficients, while a high entropy indicates a more complex or irregular texture with energy spread across many coefficients. By convention, if $E(S)=0$, then $H(S)=0$.\n\nWith the subbands $\\{LH_l, HL_l, HH_l\\}_{l=1}^L$ from the $L$-level decomposition, we compute the aggregated features as specified:\n$1$. **Per-level detail energy**, $E_l = E(LH_l) + E(HL_l) + E(HH_l)$: The total energy of all detail components at level $l$.\n$2$. **Per-level normalized detail energy**, $e_l = E_l / \\sum_{k=1}^{L} E_k$: The fraction of the total detail energy present at level $l$. This creates a signature of how texture energy is distributed across scales. If $\\sum_{k=1}^{L} E_k = 0$, then $e_l=0$.\n$3$. **Per-level mean entropy**, $h_l = (H(LH_l) + H(HL_l) + H(HH_l)) / 3$: The average entropy of the detail subbands at level $l$, capturing the mean texture complexity at that scale.\n$4$. **Global detail energy fraction**, $f = (\\sum_{l=1}^{L} E_l) / E_{\\mathrm{tot}}$: The ratio of the total energy in all detail subbands to the total energy of the original ROI, $E_{\\mathrm{tot}} = \\sum_{i,j} X_{i,j}^2$. This value, $f$, indicates the overall \"textureness\" of the image. A value near $1$ implies a highly textured or noisy image, while a value near $0$ implies a smooth, uniform image.\n\nThe final algorithm for processing a given ROI matrix $X$ with $L$ levels of decomposition is as follows:\n$1$. Compute the total energy of the input ROI, $E_{\\mathrm{tot}}$.\n$2$. Perform the $L$-level 2D Haar wavelet transform on $X$ to obtain the set of detail subbands $\\{LH_l, HL_l, HH_l\\}$ for each level $l$ from $1$ to $L$.\n$3$. For each level $l \\in \\{1, \\dots, L\\}$, calculate the per-level detail energy $E_l$ and the per-level mean entropy $h_l$.\n$4$. Sum the per-level detail energies to get the total detail energy, $\\sum_k E_k$.\n$5$. Calculate the per-level normalized detail energies $e_l$ for $l \\in \\{1, \\dots, L\\}$.\n$6$. Calculate the global detail energy fraction $f$.\n$7$. Concatenate these computed values into a single feature vector: $[e_1, h_1, e_2, h_2, \\dots, e_L, h_L, f]$.\n$8$. Each element of this vector is then rounded to six decimal places for reporting. This procedure is applied to each of the four test cases specified in the problem statement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef dwt2d_haar_single_level(matrix):\n    \"\"\"\n    Performs a single level of 2D Haar wavelet decomposition.\n    \"\"\"\n    # Row-wise transform\n    mat_r_a = (matrix[:, 0::2] + matrix[:, 1::2]) / np.sqrt(2)\n    mat_r_d = (matrix[:, 0::2] - matrix[:, 1::2]) / np.sqrt(2)\n    \n    # Column-wise transform on the results of the row-wise transform\n    ll = (mat_r_a[0::2, :] + mat_r_a[1::2, :]) / np.sqrt(2)\n    hl = (mat_r_a[0::2, :] - mat_r_a[1::2, :]) / np.sqrt(2)\n    lh = (mat_r_d[0::2, :] + mat_r_d[1::2, :]) / np.sqrt(2)\n    hh = (mat_r_d[0::2, :] - mat_r_d[1::2, :]) / np.sqrt(2)\n    \n    return ll, lh, hl, hh\n\ndef dwt2d_haar_L_level(matrix, L):\n    \"\"\"\n    Performs an L-level 2D Haar wavelet decomposition.\n    \"\"\"\n    approx = matrix.astype(float)\n    details_per_level = []\n    for _ in range(L):\n        approx, lh, hl, hh = dwt2d_haar_single_level(approx)\n        details_per_level.append({'lh': lh, 'hl': hl, 'hh': hh})\n    return details_per_level\n\ndef subband_energy(S):\n    \"\"\"\n    Computes the energy of a subband matrix S.\n    \"\"\"\n    return np.sum(S**2)\n\ndef subband_entropy(S):\n    \"\"\"\n    Computes the Shannon entropy of the normalized energy distribution of S.\n    \"\"\"\n    E_S = subband_energy(S)\n    if E_S == 0:\n        return 0.0\n    \n    p = (S**2) / E_S\n    \n    # Filter out p_ij = 0 terms to avoid log2(0) situations.\n    non_zero_p = p[p > 0]\n    \n    # an empty array will sum to 0.0, handling the case where S is non-zero\n    # but contains only one non-zero element (p=1, H=0)\n    if non_zero_p.size == 0:\n        return 0.0\n\n    H = -np.sum(non_zero_p * np.log2(non_zero_p))\n    return H\n\ndef calculate_features(roi, L):\n    \"\"\"\n    Calculates the full feature vector for a given ROI and decomposition level L.\n    \"\"\"\n    # Total input energy\n    E_tot = subband_energy(roi.astype(float))\n\n    if E_tot == 0:  # Handle all-zero image\n        return [0.0] * (2 * L + 1)\n\n    # Perform L-level DWT\n    details_all_levels = dwt2d_haar_L_level(roi, L)\n    \n    # Per-level feature calculation\n    E_l_list = []\n    h_l_list = []\n    for l_idx in range(L):\n        details = details_all_levels[l_idx]\n        lh_l, hl_l, hh_l = details['lh'], details['hl'], details['hh']\n        \n        # Per-level detail energy\n        E_l = subband_energy(lh_l) + subband_energy(hl_l) + subband_energy(hh_l)\n        E_l_list.append(E_l)\n        \n        # Per-level mean entropy\n        H_lh = subband_entropy(lh_l)\n        H_hl = subband_entropy(hl_l)\n        H_hh = subband_entropy(hh_l)\n        h_l = (H_lh + H_hl + H_hh) / 3.0\n        h_l_list.append(h_l)\n        \n    # Total detail energy across all levels\n    E_detail = np.sum(E_l_list)\n    \n    # Per-level normalized detail energy\n    if E_detail == 0:\n        e_l_list = [0.0] * L\n    else:\n        e_l_list = [E_l / E_detail for E_l in E_l_list]\n        \n    # Global detail energy fraction\n    f = E_detail / E_tot\n    \n    # Assemble the final feature vector\n    feature_vector = []\n    for l in range(L):\n        feature_vector.append(e_l_list[l])\n        feature_vector.append(h_l_list[l])\n    feature_vector.append(f)\n    \n    return feature_vector\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, compute features, and print results.\n    \"\"\"\n    L = 3\n    test_cases = [\n        # Case 1: Uniform ROI\n        np.full((8, 8), 5.0),\n        # Case 2: Checkerboard ROI\n        np.array([[(-1)**(i+j) for j in range(8)] for i in range(8)]),\n        # Case 3: Horizontal gradient ROI\n        np.array([[j for j in range(8)] for i in range(8)]),\n        # Case 4: Random ROI from N(0,1) with fixed seed\n        np.random.default_rng(seed=0).normal(size=(8, 8))\n    ]\n\n    all_results = []\n    for case_roi in test_cases:\n        features = calculate_features(case_roi, L)\n        all_results.append(features)\n\n    # Format the final output string as specified\n    formatted_results = []\n    for vec in all_results:\n        # Round each number and format to 6 decimal places\n        inner_list_str = '[' + ','.join([f'{x:.6f}' for x in vec]) + ']'\n        formatted_results.append(inner_list_str)\n    \n    final_output_string = '[' + ','.join(formatted_results) + ']'\n    \n    # Final print statement in the exact required format.\n    print(final_output_string)\n\nsolve()\n```", "id": "4543681"}]}