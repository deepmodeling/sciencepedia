{"hands_on_practices": [{"introduction": "Unsupervised learning excels at discovering inherent patterns in data without relying on pre-existing labels. This exercise guides you through the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm, a powerful method for identifying clusters of arbitrary shapes based on data density. By working through a toy radiomics dataset, you will gain hands-on experience in defining clusters, identifying noise, and understanding the core principles of density-based discovery [@problem_id:4561470].", "problem": "Consider unlabeled two-dimensional standardized radiomics feature vectors representing Gray-Level Co-occurrence Matrix (GLCM) contrast and entropy. The dataset consists of the following $16$ points:\n$$A_1=(0.10, 0.10),\\ A_2=(0.18, 0.12),\\ A_3=(0.12, 0.20),\\ A_4=(0.08, 0.15),\\ A_5=(0.16, 0.18),\\ A_6=(0.14, 0.08),\\ A_7=(0.20, 0.14),$$\n$$B_1=(0.80, 0.80),\\ B_2=(0.86, 0.78),\\ B_3=(0.78, 0.86),\\ B_4=(0.84, 0.88),\\ B_5=(0.76, 0.76),\\ B_6=(0.88, 0.82),\\ B_7=(0.82, 0.74),$$\n$$N_1=(0.02, 0.98),\\ N_2=(0.12, 0.88).$$\nApply Density-Based Spatial Clustering of Applications with Noise (DBSCAN) with neighborhood radius $\\epsilon=0.5$ and minimum points $\\mathrm{minPts}=5$, using the Euclidean norm $\\|\\cdot\\|_2$ and the standard DBSCAN neighborhood definition $N_{\\epsilon}(p)=\\{q:\\|q-p\\|_2\\leq \\epsilon\\}$ that includes the point $p$ itself. Starting from the foundational definitions of DBSCAN (core points have $|N_{\\epsilon}(p)|\\geq \\mathrm{minPts}$, border points have $|N_{\\epsilon}(p)|<\\mathrm{minPts}$ but lie within $\\epsilon$ of at least one core point, and noise points lie outside the $\\epsilon$-neighborhood of all core points), determine which points are core, border, or noise. Then, reason from these definitions to derive the cluster assignments by DBSCAN, explaining how density reachability and density connectivity induce clusters in this dataset.\n\nReport as your single final quantity the total number of clusters discovered by DBSCAN on this dataset. Express the final answer as an integer with no units. No rounding is required.", "solution": "The problem is first validated to ensure it is scientifically sound, self-contained, and well-posed.\n\n### Step 1: Extract Givens\n- **Dataset**: A set of $16$ two-dimensional points, denoted $A_1, \\dots, A_7$, $B_1, \\dots, B_7$, and $N_1, N_2$.\n  - $A_1=(0.10, 0.10)$, $A_2=(0.18, 0.12)$, $A_3=(0.12, 0.20)$, $A_4=(0.08, 0.15)$, $A_5=(0.16, 0.18)$, $A_6=(0.14, 0.08)$, $A_7=(0.20, 0.14)$\n  - $B_1=(0.80, 0.80)$, $B_2=(0.86, 0.78)$, $B_3=(0.78, 0.86)$, $B_4=(0.84, 0.88)$, $B_5=(0.76, 0.76)$, $B_6=(0.88, 0.82)$, $B_7=(0.82, 0.74)$\n  - $N_1=(0.02, 0.98)$, $N_2=(0.12, 0.88)$\n- **DBSCAN Parameters**:\n  - Neighborhood radius $\\epsilon=0.5$\n  - Minimum points $\\mathrm{minPts}=5$\n- **Distance Metric**: Euclidean norm, $\\|\\cdot\\|_2$.\n- **Neighborhood Definition**: $N_{\\epsilon}(p)=\\{q:\\|q-p\\|_2\\leq \\epsilon\\}$, which includes point $p$.\n- **Point Definitions**:\n  - **Core point**: A point $p$ is a core point if $|N_{\\epsilon}(p)|\\geq \\mathrm{minPts}$.\n  - **Border point**: A point $p$ is a border point if $|N_{\\epsilon}(p)|<\\mathrm{minPts}$ but $p$ is in the $\\epsilon$-neighborhood of at least one core point.\n  - **Noise point**: A point that is neither a core nor a border point.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is a standard exercise in applying the DBSCAN clustering algorithm. All data points, parameters ($\\epsilon$, $\\mathrm{minPts}$), and definitions are provided and are unambiguous. The problem is scientifically grounded in the fields of data mining and machine learning, and specifically in unsupervised learning. It is well-posed, as the DBSCAN algorithm is deterministic and will produce a single, unique result for the given inputs. There are no contradictions, missing data, or scientifically implausible conditions.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivation\nThe solution proceeds by first identifying the type of each point (core, border, or noise) and then forming clusters based on the principles of density-reachability and density-connectivity. The squared Euclidean distance will be compared against $\\epsilon^2 = 0.5^2 = 0.25$ to simplify calculations.\n\n**Part 1: Neighborhood Analysis and Point Classification**\n\nWe analyze the points by visually apparent groupings: the 'A' points, 'B' points, and 'N' points.\n\n1.  **Analysis of the 'A' points**: $\\{A_1, \\dots, A_7\\}$\n    These points are tightly grouped in the low-value region of the feature space. Let's find the maximum squared distance between any two points in this group. The coordinates range from $x=0.08$ to $x=0.20$ and $y=0.08$ to $y=0.20$. The largest separation might be between $A_4=(0.08, 0.15)$ and $A_7=(0.20, 0.14)$.\n    $$ \\|A_7-A_4\\|_2^2 = (0.20-0.08)^2 + (0.14-0.15)^2 = 0.12^2 + (-0.01)^2 = 0.0144 + 0.0001 = 0.0145 $$\n    Since the maximum squared distance $0.0145 < \\epsilon^2 = 0.25$, all $7$ points in this group lie within the $\\epsilon$-neighborhood of each other.\n    For any point $p \\in \\{A_1, \\dots, A_7\\}$, its neighborhood $N_{\\epsilon}(p)$ will contain all $7$ points of this group. We must also check if points from other groups are nearby. Let's check the distance from a central 'A' point, e.g., $A_1=(0.10, 0.10)$, to the nearest 'B' point, $B_5=(0.76, 0.76)$, and nearest 'N' point, $N_2=(0.12, 0.88)$.\n    $$ \\|A_1-B_5\\|_2^2 = (0.10-0.76)^2 + (0.10-0.76)^2 = (-0.66)^2 + (-0.66)^2 = 0.4356 + 0.4356 = 0.8712 > 0.25 $$\n    $$ \\|A_1-N_2\\|_2^2 = (0.10-0.12)^2 + (0.10-0.88)^2 = (-0.02)^2 + (-0.78)^2 = 0.0004 + 0.6084 = 0.6088 > 0.25 $$\n    The other groups are far away. Therefore, for any point $p_A \\in \\{A_1, \\dots, A_7\\}$, its neighborhood is precisely $N_{\\epsilon}(p_A) = \\{A_1, \\dots, A_7\\}$, and its size is $|N_{\\epsilon}(p_A)| = 7$.\n    Since $7 \\ge \\mathrm{minPts}=5$, all $7$ points $\\{A_1, \\dots, A_7\\}$ are **core points**.\n\n2.  **Analysis of the 'B' points**: $\\{B_1, \\dots, B_7\\}$\n    These points are grouped in the high-value region. The coordinates range from $x=0.76$ to $x=0.88$ and $y=0.74$ to $y=0.88$. Let's check the maximum squared distance, perhaps between $B_5=(0.76, 0.76)$ and $B_4=(0.84, 0.88)$.\n    $$ \\|B_4-B_5\\|_2^2 = (0.84-0.76)^2 + (0.88-0.76)^2 = 0.08^2 + 0.12^2 = 0.0064 + 0.0144 = 0.0208 $$\n    As $0.0208 < \\epsilon^2 = 0.25$, all points in this group are within each other's $\\epsilon$-neighborhood.\n    Similar to the 'A' points, the 'B' points are far from all other points. Thus, for any point $p_B \\in \\{B_1, \\dots, B_7\\}$, its neighborhood is $N_{\\epsilon}(p_B) = \\{B_1, \\dots, B_7\\}$, and $|N_{\\epsilon}(p_B)| = 7$.\n    Since $7 \\ge \\mathrm{minPts}=5$, all $7$ points $\\{B_1, \\dots, B_7\\}$ are also **core points**.\n\n3.  **Analysis of the 'N' points**: $\\{N_1, N_2\\}$\n    Let's check the distance between $N_1$ and $N_2$.\n    $$ \\|N_1 - N_2\\|_2^2 = (0.02-0.12)^2 + (0.98-0.88)^2 = (-0.1)^2 + 0.1^2 = 0.01 + 0.01 = 0.02 < 0.25 $$\n    So, $N_1$ and $N_2$ are in each other's neighborhood. The neighborhood of $N_1$ is $N_{\\epsilon}(N_1) = \\{N_1, N_2\\}$, and the neighborhood of $N_2$ is $N_{\\epsilon}(N_2) = \\{N_1, N_2\\}$. The size of each neighborhood is $2$.\n    Since $|N_{\\epsilon}(N_1)|=2 < \\mathrm{minPts}=5$ and $|N_{\\epsilon}(N_2)|=2 < \\mathrm{minPts}=5$, neither $N_1$ nor $N_2$ is a core point.\n    To be border points, they must lie in the $\\epsilon$-neighborhood of a core point. The core points are the 'A' and 'B' sets. Let's check the distance from $N_2=(0.12, 0.88)$ to its closest 'A' point, $A_3=(0.12, 0.20)$.\n    $$ \\|N_2 - A_3\\|_2^2 = (0.12-0.12)^2 + (0.88-0.20)^2 = 0^2 + 0.68^2 = 0.4624 > 0.25 $$\n    Let's check the distance from $N_2=(0.12, 0.88)$ to its closest 'B' point, $B_3=(0.78, 0.86)$.\n    $$ \\|N_2 - B_3\\|_2^2 = (0.12-0.78)^2 + (0.88-0.86)^2 = (-0.66)^2 + 0.02^2 = 0.4356 + 0.0004 = 0.436 > 0.25 $$\n    Since the closest core points are outside the $\\epsilon$-radius of $N_1$ and $N_2$, these points are not in the neighborhood of any core point. Hence, they are not border points.\n    By definition, points that are neither core nor border are **noise points**. Thus, $N_1$ and $N_2$ are noise.\n\n**Part 2: Cluster Formation**\n\nClusters are formed from sets of density-connected points.\n- A point $q$ is **density-reachable** from a point $p$ if there is a path $p_1, \\dots, p_k$ with $p_1 = p$ and $p_k = q$, where each $p_{i+1}$ is directly density-reachable from $p_i$ (i.e., $p_{i+1} \\in N_{\\epsilon}(p_i)$ and $p_i$ is a core point).\n- Two points $p, q$ are **density-connected** if there is a core point $o$ from which both $p$ and $q$ are density-reachable.\n\nDBSCAN finds clusters by picking an arbitrary point and expanding from it if it's a core point.\n\n- **Cluster 1**: Let's start with $A_1$. We established it is a core point. The algorithm creates a new cluster. All points in its neighborhood, $\\{A_1, \\dots, A_7\\}$, are added to this cluster. The algorithm then expands from these newly added points. For any other point $A_i$ in this set, it is also a core point, and its neighborhood is the same set $\\{A_1, \\dots, A_7\\}$. No new points can be added. All points in $\\{A_1, \\dots, A_7\\}$ are density-connected to each other (e.g., via the core point $A_1$). This forms the first cluster.\n\n- **Cluster 2**: Let's pick an unvisited point, say $B_1$. It is a core point, so a second cluster is created. Its neighborhood $\\{B_1, \\dots, B_7\\}$ is added to this new cluster. Just as with the 'A' points, all points in this set are core points, and their neighborhoods are confined to this set. They are all density-connected to one another, forming the second cluster. The neighborhoods of the 'A' points and 'B' points are disjoint, so there is no density-connectivity path between the two groups.\n\n- **Noise**: The remaining unvisited points are $N_1$ and $N_2$. When the algorithm selects $N_1$, it finds it is not a core point and is not part of any existing cluster, so it is labeled as noise. The same occurs for $N_2$.\n\nThe algorithm terminates having identified two distinct clusters and two noise points. The total number of clusters discovered is $2$.\n\nFinal Point Status:\n- **Core points**: $\\{A_1, ..., A_7\\}$ and $\\{B_1, ..., B_7\\}$\n- **Border points**: None\n- **Noise points**: $\\{N_1, N_2\\}$\n- **Clusters**:\n  - Cluster $1$: $\\{A_1, A_2, A_3, A_4, A_5, A_6, A_7\\}$\n  - Cluster $2$: $\\{B_1, B_2, B_3, B_4, B_5, B_6, B_7\\}$\n\nThe total number of clusters discovered by DBSCAN is $2$.", "answer": "$$\\boxed{2}$$", "id": "4561470"}, {"introduction": "In contrast to unsupervised discovery, supervised learning uses labeled data to train predictive models. This practice problem focuses on the Support Vector Machine (SVM), a cornerstone of modern classification, which seeks to find the optimal separating boundary between classes. You will apply the hard-margin SVM to a linearly separable radiomics dataset to determine the decision boundary, calculate its margin, and identify the crucial 'support vectors' that define it [@problem_id:4561510].", "problem": "A radiomics researcher extracts two standardized features, denoted by $f_1$ and $f_2$, from computed tomography scans of lesions. After $z$-score normalization, a toy dataset of $4$ samples is obtained, located at the corners $\\left(\\pm 1, \\pm 1\\right)$ in the feature plane. Consider a supervised classification task to distinguish lesions into two classes using a linear classifier trained by the hard-margin Support Vector Machine (SVM), where Support Vector Machine (SVM) refers to maximizing the margin subject to zero empirical hinge loss and linear separability.\n\nIndex the samples as follows:\n- Sample $1$: $x_1 = \\left(1, 1\\right)$, $y_1 = +1$,\n- Sample $2$: $x_2 = \\left(1, -1\\right)$, $y_2 = +1$,\n- Sample $3$: $x_3 = \\left(-1, 1\\right)$, $y_3 = -1$,\n- Sample $4$: $x_4 = \\left(-1, -1\\right)$, $y_4 = -1$,\n\nwhich corresponds to labeling by the sign of the first feature, $y_i = \\operatorname{sign}\\!\\left(x_{i1}\\right)$, making the dataset linearly separable. Let the separating hyperplane be parameterized by a weight vector $w \\in \\mathbb{R}^2$ and bias $b \\in \\mathbb{R}$, with classification rule $\\operatorname{sign}\\!\\left(w^{\\top} x + b\\right)$. The hard-margin SVM primal problem imposes the constraints $y_i\\left(w^{\\top} x_i + b\\right) \\ge 1$ and minimizes the squared Euclidean norm $\\|w\\|_2^2$.\n\nStarting from these definitions and the constraint set, derive the maximum-margin separator and compute:\n- the geometric margin, defined as the minimal distance from the decision boundary to either class under the hard-margin constraints, and\n- the indices of the support vectors.\n\nExpress the final answer as a single row matrix using the geometric margin (first entry) followed by the support vector indices in ascending order. No rounding is required, and no units are involved.", "solution": "The user wants to solve a hard-margin Support Vector Machine (SVM) problem.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n-   **Dataset:**\n    -   $x_1 = (1, 1)$, $y_1 = +1$\n    -   $x_2 = (1, -1)$, $y_2 = +1$\n    -   $x_3 = (-1, 1)$, $y_3 = -1$\n    -   $x_4 = (-1, -1)$, $y_4 = -1$\n-   **Classifier:** $\\operatorname{sign}(w^{\\top} x + b)$ for $w \\in \\mathbb{R}^2$ and $b \\in \\mathbb{R}$.\n-   **SVM Primal Problem:** Minimize $\\|w\\|_2^2$ subject to the constraints $y_i(w^{\\top} x_i + b) \\ge 1$ for $i \\in \\{1, 2, 3, 4\\}$.\n-   **Task:** Derive the maximum-margin separator and compute:\n    1.  The geometric margin.\n    2.  The indices of the support vectors.\n-   **Answer Format:** A single row matrix with the geometric margin as the first entry, followed by the support vector indices in ascending order.\n\n**1.2. Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is a standard application of the hard-margin SVM algorithm, a fundamental concept in machine learning and statistical learning theory. The mathematical formulation is correct.\n-   **Well-Posed:** The provided dataset is linearly separable. The hard-margin SVM problem for a linearly separable dataset is a convex optimization problem, which has a unique solution for the weight vector $w$ and bias $b$. The quantities to be derived (geometric margin, support vectors) are well-defined based on this solution.\n-   **Objective:** The problem is stated using precise mathematical language and definitions. There are no subjective or ambiguous terms.\n\n**1.3. Verdict**\nThe problem is valid. It is scientifically sound, self-contained, and well-posed.\n\n### Step 2: Solution Derivation\n\nThe objective of the hard-margin SVM is to find the hyperplane that maximizes the distance to the nearest data point from either class. This is equivalent to minimizing $\\frac{1}{2}\\|w\\|_2^2$ subject to the constraints that all data points are classified correctly and are at a functional margin of at least $1$ from the decision boundary.\n\nThe optimization problem is:\n$$\n\\min_{w, b} \\frac{1}{2} \\|w\\|_2^2 \\quad \\text{subject to} \\quad y_i(w^{\\top} x_i + b) \\ge 1, \\quad i=1, 2, 3, 4\n$$\nLet $w = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$. The constraints for the four data points are:\n1.  For $x_1 = (1, 1), y_1 = +1$:\n    $$+1(w^{\\top}x_1 + b) \\ge 1 \\implies w_1 \\cdot 1 + w_2 \\cdot 1 + b \\ge 1 \\implies w_1 + w_2 + b \\ge 1$$\n2.  For $x_2 = (1, -1), y_2 = +1$:\n    $$+1(w^{\\top}x_2 + b) \\ge 1 \\implies w_1 \\cdot 1 + w_2 \\cdot (-1) + b \\ge 1 \\implies w_1 - w_2 + b \\ge 1$$\n3.  For $x_3 = (-1, 1), y_3 = -1$:\n    $$-1(w^{\\top}x_3 + b) \\ge 1 \\implies -1(w_1 \\cdot (-1) + w_2 \\cdot 1 + b) \\ge 1 \\implies w_1 - w_2 - b \\ge 1$$\n4.  For $x_4 = (-1, -1), y_4 = -1$:\n    $$-1(w^{\\top}x_4 + b) \\ge 1 \\implies -1(w_1 \\cdot (-1) + w_2 \\cdot (-1) + b) \\ge 1 \\implies w_1 + w_2 - b \\ge 1$$\n\nThe data points for class $+1$ are $\\{ (1, 1), (1, -1) \\}$, and for class $-1$ are $\\{ (-1, 1), (-1, -1) \\}$. The classes are separated by a vertical line. By symmetry, the optimal separating hyperplane must be a vertical line of the form $x_1 = c$, for some constant $c$.\nA hyperplane $w^{\\top}x + b = 0$ is vertical if and only if $w_2 = 0$. So we can set $w_2 = 0$.\n\nThe problem simplifies to minimizing $\\frac{1}{2}w_1^2$ subject to the constraints with $w_2=0$:\n1.  $w_1 + b \\ge 1$\n2.  $w_1 + b \\ge 1$\n3.  $w_1 - b \\ge 1$\n4.  $w_1 - b \\ge 1$\n\nThese reduce to two distinct inequalities:\n(A) $w_1 + b \\ge 1$\n(B) $w_1 - b \\ge 1$\n\nTo ensure correct classification, we need $\\operatorname{sign}(w_1 x_{i1} + b) = y_i$. For $x_1=(1,1), y_1=1$, we need $w_1+b > 0$, which is satisfied by (A). For $x_3=(-1,1), y_3=-1$, we need $-w_1+b  0 \\implies w_1 > b$, which is satisfied by (B) since $w_1 \\ge 1+b$. From (A) and (B), adding the two inequalities yields $2w_1 \\ge 2$, so $w_1 \\ge 1$.\nThis implies $w_1$ must be positive.\n\nTo minimize $w_1^2$ (or $\\frac{1}{2}w_1^2$), we need to find the smallest possible value for $|w_1|$. Since $w_1 \\ge 1$, the minimum value for $w_1^2$ is achieved when $w_1$ is as close to $0$ as possible, which is $w_1 = 1$.\n\nNow we find the value of $b$. Substituting $w_1 = 1$ back into the inequalities:\n(A) $1 + b \\ge 1 \\implies b \\ge 0$\n(B) $1 - b \\ge 1 \\implies -b \\ge 0 \\implies b \\le 0$\nThe only value of $b$ that satisfies both $b \\ge 0$ and $b \\le 0$ is $b=0$.\n\nThus, the optimal parameters for the separating hyperplane are $w = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $b = 0$.\nThe decision boundary is $w^{\\top}x + b = 0$, which is $1 \\cdot x_1 + 0 \\cdot x_2 + 0 = 0$, or $x_1 = 0$.\n\n**Computation of Geometric Margin**\nThe geometric margin $\\gamma$ is defined as the distance from the decision boundary to the closest training examples. For a hard-margin SVM, this distance is given by $\\gamma = \\frac{1}{\\|w\\|_2}$.\nWith $w = (1, 0)^{\\top}$, the Euclidean norm is:\n$$\n\\|w\\|_2 = \\sqrt{w_1^2 + w_2^2} = \\sqrt{1^2 + 0^2} = 1\n$$\nTherefore, the geometric margin is:\n$$\n\\gamma = \\frac{1}{1} = 1\n$$\n\n**Identification of Support Vectors**\nSupport vectors are the data points $x_i$ that lie exactly on the margin hyperplanes, i.e., the points for which the inequality constraint is active (an equality):\n$$\ny_i (w^{\\top}x_i + b) = 1\n$$\nLet's test each of the four points with our solution $w=(1, 0)^{\\top}$ and $b=0$:\n1.  Sample $1$: $y_1(w^{\\top}x_1 + b) = +1(1 \\cdot 1 + 0 \\cdot 1 + 0) = 1$. The constraint is active.\n2.  Sample $2$: $y_2(w^{\\top}x_2 + b) = +1(1 \\cdot 1 + 0 \\cdot (-1) + 0) = 1$. The constraint is active.\n3.  Sample $3$: $y_3(w^{\\top}x_3 + b) = -1(1 \\cdot (-1) + 0 \\cdot 1 + 0) = -1(-1) = 1$. The constraint is active.\n4.  Sample $4$: $y_4(w^{\\top}x_4 + b) = -1(1 \\cdot (-1) + 0 \\cdot (-1) + 0) = -1(-1) = 1$. The constraint is active.\n\nSince the constraint is active for all four data points, all four are support vectors. Their indices are $1, 2, 3, 4$.\n\n**Final Answer Formulation**\nThe required output is a row matrix containing the geometric margin followed by the indices of the support vectors in ascending order.\n-   Geometric margin: $1$\n-   Support vector indices: $1, 2, 3, 4$\nThe resulting row matrix is $\\begin{pmatrix} 1  1  2  3  4 \\end{pmatrix}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  1  2  3  4 \\end{pmatrix}}\n$$", "id": "4561510"}, {"introduction": "The distinction between supervised and unsupervised approaches is not always limited to the final modeling stage; it is also critical during data preprocessing. This advanced problem explores the challenge of 'batch effects' from different medical scanners, a common issue in radiomics. You will derive and quantify how an unsupervised (pooled) normalization approach can introduce bias, and how a supervised method that incorporates scanner information can correct for it, ensuring data harmonization [@problem_id:4561535].", "problem": "In a radiomics study of Computed Tomography (CT), voxel intensities are measured in Hounsfield Units (HU). Consider two batches of patients: Scanner A with $n_A=70$ and Scanner B with $n_B=50$. Due to a known hardware calibration offset, Scanner B exhibits a global intensity shift of $\\delta=15$ HU relative to Scanner A for healthy tissue. Assume that, within each scanner, a given radiomic intensity feature $X$ follows a distribution with the same within-scanner standard deviation $\\sigma$ and scanner-specific means $\\mu_A$ and $\\mu_B=\\mu_A+\\delta$. Let pooled $z$-score normalization be defined by $z=(x-\\mu_{\\text{pool}})/s_{\\text{pool}}$, where $\\mu_{\\text{pool}}$ and $s_{\\text{pool}}$ are the sample mean and sample standard deviation computed from the combined $n_A+n_B$ observations, ignoring scanner identity (unsupervised).\n\nStarting from the fundamental definitions of $z$-score normalization, the law of total expectation, and the law of total variance for mixtures, derive the expected mean and variance of the pooled $z$-scores within each scanner cohort, thereby quantifying the bias introduced by pooled normalization when there is a known inter-scanner shift $\\delta$. Then, compute scanner-specific corrected standardization parameters that use scanner labels (supervised) to remove the inter-scanner shift, expressing these parameters purely in terms of the pooled statistics $\\mu_{\\text{pool}}$, $s_{\\text{pool}}$, the sample sizes $n_A$, $n_B$, and the known shift $\\delta$.\n\nYour final expressions must eliminate the unknown $\\mu_A$ and $\\sigma$ and depend only on $\\mu_{\\text{pool}}$, $s_{\\text{pool}}$, $n_A$, $n_B$, and $\\delta$. Provide the final answer as a single row matrix containing, in order:\n- The expected pooled-normalized mean in Scanner A, $\\mathbb{E}[z \\mid A]$.\n- The pooled-normalized variance in Scanner A, $\\operatorname{Var}(z \\mid A)$.\n- The expected pooled-normalized mean in Scanner B, $\\mathbb{E}[z \\mid B]$.\n- The pooled-normalized variance in Scanner B, $\\operatorname{Var}(z \\mid B)$.\n- The corrected within-scanner mean for Scanner A, $\\mu_A^{\\text{corr}}$.\n- The corrected within-scanner standard deviation for Scanner A, $\\sigma_A^{\\text{corr}}$.\n- The corrected within-scanner mean for Scanner B, $\\mu_B^{\\text{corr}}$.\n- The corrected within-scanner standard deviation for Scanner B, $\\sigma_B^{\\text{corr}}$.\n\nExpress your final answer symbolically with the given numeric values $n_A=70$, $n_B=50$, and $\\delta=15$ embedded where appropriate. No rounding is required. Do not include units in the final boxed expression.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   Scanner A patient count: $n_A = 70$.\n-   Scanner B patient count: $n_B = 50$.\n-   Total patient count: $N = n_A + n_B = 120$.\n-   Intensity shift in Scanner B relative to A: $\\delta = 15$ HU.\n-   A radiomic feature $X$ from Scanner A is a random variable with mean $\\mu_A$ and standard deviation $\\sigma$.\n-   The same feature $X$ from Scanner B is a random variable with mean $\\mu_B = \\mu_A + \\delta$ and standard deviation $\\sigma$.\n-   Pooled $z$-score normalization is defined as $z = (x - \\mu_{\\text{pool}}) / s_{\\text{pool}}$, where $\\mu_{\\text{pool}}$ and $s_{\\text{pool}}$ are the sample mean and sample standard deviation from the combined $n_A+n_B$ observations.\n-   The task is to derive expressions for $\\mathbb{E}[z \\mid A]$, $\\operatorname{Var}(z \\mid A)$, $\\mathbb{E}[z \\mid B]$, $\\operatorname{Var}(z \\mid B)$, and the corrected standardization parameters $\\mu_A^{\\text{corr}}$, $\\sigma_A^{\\text{corr}}$, $\\mu_B^{\\text{corr}}$, $\\sigma_B^{\\text{corr}}$.\n-   The final expressions must be in terms of $\\mu_{\\text{pool}}$, $s_{\\text{pool}}$, $n_A$, $n_B$, and $\\delta$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a realistic scenario in medical imaging data analysis, known as correcting for batch effects or scanner effects. The statistical model is a standard mixture model. The concepts of mean, variance, $z$-score normalization, the law of total expectation, and the law of total variance are fundamental statistical principles. The problem is well-defined, objective, and contains sufficient information to derive a unique solution under the reasonable assumption that the sample statistics ($\\mu_{\\text{pool}}, s_{\\text{pool}}$) are good estimates of the true population moments of the mixture distribution. Therefore, the problem is deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Derivation\nLet $K$ be a random variable indicating the scanner, with outcomes $A$ and $B$. The probabilities of an observation coming from each scanner are the sample proportions:\n$$p_A = P(K=A) = \\frac{n_A}{n_A + n_B} = \\frac{70}{120} = \\frac{7}{12}$$\n$$p_B = P(K=B) = \\frac{n_B}{n_A + n_B} = \\frac{50}{120} = \\frac{5}{12}$$\nThe given conditional moments are:\n$\\mathbb{E}[X \\mid K=A] = \\mu_A$\n$\\mathbb{E}[X \\mid K=B] = \\mu_B = \\mu_A + \\delta$\n$\\operatorname{Var}(X \\mid K=A) = \\operatorname{Var}(X \\mid K=B) = \\sigma^2$\n\nWe relate the unknown parameters $\\mu_A$ and $\\sigma$ to the given pooled sample statistics $\\mu_{\\text{pool}}$ and $s_{\\text{pool}}$. We use the principle that for a large sample, the sample statistics are reliable estimates of the true population moments of the mixture distribution.\n\nFirst, we apply the Law of Total Expectation to the pooled data:\n$$\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X \\mid K]] = \\mathbb{E}[X \\mid A] p_A + \\mathbb{E}[X \\mid B] p_B$$\nApproximating $\\mathbb{E}[X]$ with $\\mu_{\\text{pool}}$:\n$$\\mu_{\\text{pool}} \\approx \\mu_A p_A + (\\mu_A + \\delta) p_B = \\mu_A (p_A + p_B) + \\delta p_B = \\mu_A + \\delta p_B$$\nFrom this, we can express the unknown $\\mu_A$ in terms of known quantities:\n$$\\mu_A \\approx \\mu_{\\text{pool}} - \\delta p_B = \\mu_{\\text{pool}} - \\delta \\frac{n_B}{n_A + n_B}$$\n\nNext, we apply the Law of Total Variance:\n$$\\operatorname{Var}(X) = \\mathbb{E}[\\operatorname{Var}(X \\mid K)] + \\operatorname{Var}(\\mathbb{E}[X \\mid K])$$\nApproximating $\\operatorname{Var}(X)$ with $s_{\\text{pool}}^2$:\nThe first term is the average of the within-scanner variances:\n$$\\mathbb{E}[\\operatorname{Var}(X \\mid K)] = \\operatorname{Var}(X \\mid A) p_A + \\operatorname{Var}(X \\mid B) p_B = \\sigma^2 p_A + \\sigma^2 p_B = \\sigma^2$$\nThe second term is the variance of the conditional expectations (between-scanner variance):\n$$\\operatorname{Var}(\\mathbb{E}[X \\mid K]) = \\mathbb{E}[(\\mathbb{E}[X \\mid K])^2] - (\\mathbb{E}[\\mathbb{E}[X \\mid K]])^2$$\n$$= (\\mu_A^2 p_A + \\mu_B^2 p_B) - (\\mu_{\\text{pool}})^2$$\nA more direct way is:\n$$\\operatorname{Var}(\\mathbb{E}[X \\mid K]) = (\\mathbb{E}[X \\mid A] - \\mathbb{E}[X])^2 p_A + (\\mathbb{E}[X \\mid B] - \\mathbb{E}[X])^2 p_B$$\n$$ \\approx (\\mu_A - \\mu_{\\text{pool}})^2 p_A + (\\mu_B - \\mu_{\\text{pool}})^2 p_B$$\nSubstituting $\\mu_A \\approx \\mu_{\\text{pool}} - \\delta p_B$ and $\\mu_B = \\mu_A + \\delta \\approx \\mu_{\\text{pool}} + \\delta(1-p_B) = \\mu_{\\text{pool}} + \\delta p_A$:\n$$\\operatorname{Var}(\\mathbb{E}[X \\mid K]) \\approx (-\\delta p_B)^2 p_A + (\\delta p_A)^2 p_B = \\delta^2 p_B^2 p_A + \\delta^2 p_A^2 p_B = \\delta^2 p_A p_B (p_A + p_B) = \\delta^2 p_A p_B$$\n$$\\operatorname{Var}(\\mathbb{E}[X \\mid K]) \\approx \\delta^2 \\frac{n_A n_B}{(n_A + n_B)^2}$$\nCombining the terms for the total variance:\n$$s_{\\text{pool}}^2 \\approx \\sigma^2 + \\delta^2 \\frac{n_A n_B}{(n_A + n_B)^2}$$\nFrom this, we express the unknown $\\sigma^2$ in terms of known quantities:\n$$\\sigma^2 \\approx s_{\\text{pool}}^2 - \\delta^2 \\frac{n_A n_B}{(n_A + n_B)^2}$$\n\nNow we can derive the first four required quantities. The pooled $z$-score transformation is $z(x) = (x - \\mu_{\\text{pool}}) / s_{\\text{pool}}$.\n\n1.  **Expected pooled-normalized mean in Scanner A, $\\mathbb{E}[z \\mid A]$**:\n    $$\\mathbb{E}[z \\mid A] = \\mathbb{E}\\left[\\frac{X - \\mu_{\\text{pool}}}{s_{\\text{pool}}} \\mid A \\right] = \\frac{\\mathbb{E}[X \\mid A] - \\mu_{\\text{pool}}}{s_{\\text{pool}}} = \\frac{\\mu_A - \\mu_{\\text{pool}}}{s_{\\text{pool}}}$$\n    Substituting our expression for $\\mu_A$:\n    $$\\mathbb{E}[z \\mid A] \\approx \\frac{(\\mu_{\\text{pool}} - \\delta \\frac{n_B}{n_A+n_B}) - \\mu_{\\text{pool}}}{s_{\\text{pool}}} = -\\frac{\\delta}{s_{\\text{pool}}} \\frac{n_B}{n_A+n_B}$$\n\n2.  **Pooled-normalized variance in Scanner A, $\\operatorname{Var}(z \\mid A)$**:\n    $$\\operatorname{Var}(z \\mid A) = \\operatorname{Var}\\left(\\frac{X - \\mu_{\\text{pool}}}{s_{\\text{pool}}} \\mid A \\right) = \\frac{\\operatorname{Var}(X \\mid A)}{s_{\\text{pool}}^2} = \\frac{\\sigma^2}{s_{\\text{pool}}^2}$$\n    Substituting our expression for $\\sigma^2$:\n    $$\\operatorname{Var}(z \\mid A) \\approx \\frac{s_{\\text{pool}}^2 - \\delta^2 \\frac{n_A n_B}{(n_A+n_B)^2}}{s_{\\text{pool}}^2} = 1 - \\frac{\\delta^2}{s_{\\text{pool}}^2} \\frac{n_A n_B}{(n_A+n_B)^2}$$\n\n3.  **Expected pooled-normalized mean in Scanner B, $\\mathbb{E}[z \\mid B]$**:\n    $$\\mathbb{E}[z \\mid B] = \\frac{\\mathbb{E}[X \\mid B] - \\mu_{\\text{pool}}}{s_{\\text{pool}}} = \\frac{\\mu_B - \\mu_{\\text{pool}}}{s_{\\text{pool}}} = \\frac{(\\mu_A + \\delta) - \\mu_{\\text{pool}}}{s_{\\text{pool}}}$$\n    Substituting our expression for $\\mu_A$:\n    $$\\mathbb{E}[z \\mid B] \\approx \\frac{(\\mu_{\\text{pool}} - \\delta \\frac{n_B}{n_A+n_B} + \\delta) - \\mu_{\\text{pool}}}{s_{\\text{pool}}} = \\frac{\\delta(1 - \\frac{n_B}{n_A+n_B})}{s_{\\text{pool}}} = \\frac{\\delta}{s_{\\text{pool}}} \\frac{n_A}{n_A+n_B}$$\n\n4.  **Pooled-normalized variance in Scanner B, $\\operatorname{Var}(z \\mid B)$**:\n    $$\\operatorname{Var}(z \\mid B) = \\frac{\\operatorname{Var}(X \\mid B)}{s_{\\text{pool}}^2} = \\frac{\\sigma^2}{s_{\\text{pool}}^2}$$\n    This is identical to the variance for Scanner A:\n    $$\\operatorname{Var}(z \\mid B) \\approx 1 - \\frac{\\delta^2}{s_{\\text{pool}}^2} \\frac{n_A n_B}{(n_A+n_B)^2}$$\n\nThe results show that pooled normalization introduces a bias in the mean for each subgroup ($\\mathbb{E}[z \\mid A] \\neq 0$ and $\\mathbb{E}[z \\mid B] \\neq 0$) and reduces the within-group variance ($\\operatorname{Var}(z \\mid K)  1$) because the pooled variance $s_{\\text{pool}}^2$ is inflated by the between-scanner variance component.\n\nFinally, we derive the corrected (supervised) standardization parameters. These parameters should correspond to the true mean and standard deviation within each scanner cohort.\n5.  **Corrected mean for Scanner A, $\\mu_A^{\\text{corr}}$**: This is simply $\\mu_A$.\n    $$\\mu_A^{\\text{corr}} = \\mu_A \\approx \\mu_{\\text{pool}} - \\delta \\frac{n_B}{n_A + n_B}$$\n\n6.  **Corrected standard deviation for Scanner A, $\\sigma_A^{\\text{corr}}$**: This is simply $\\sigma$.\n    $$\\sigma_A^{\\text{corr}} = \\sigma \\approx \\sqrt{s_{\\text{pool}}^2 - \\delta^2 \\frac{n_A n_B}{(n_A + n_B)^2}}$$\n\n7.  **Corrected mean for Scanner B, $\\mu_B^{\\text{corr}}$**: This is $\\mu_B = \\mu_A + \\delta$.\n    $$\\mu_B^{\\text{corr}} = \\mu_A + \\delta \\approx \\left(\\mu_{\\text{pool}} - \\delta \\frac{n_B}{n_A + n_B}\\right) + \\delta = \\mu_{\\text{pool}} + \\delta \\frac{n_A}{n_A + n_B}$$\n\n8.  **Corrected standard deviation for Scanner B, $\\sigma_B^{\\text{corr}}$**: This is also $\\sigma$, as the within-scanner standard deviation is assumed to be the same.\n    $$\\sigma_B^{\\text{corr}} = \\sigma \\approx \\sqrt{s_{\\text{pool}}^2 - \\delta^2 \\frac{n_A n_B}{(n_A + n_B)^2}}$$\n\nNow we substitute the given numerical values: $n_A = 70$, $n_B = 50$, $N = n_A+n_B = 120$, and $\\delta = 15$.\nThe constant factors are:\n$\\frac{n_B}{N} = \\frac{50}{120} = \\frac{5}{12}$\n$\\frac{n_A}{N} = \\frac{70}{120} = \\frac{7}{12}$\n$\\frac{n_A n_B}{N^2} = \\frac{70 \\times 50}{120^2} = \\frac{3500}{14400} = \\frac{35}{144}$\n\n-   $\\mathbb{E}[z \\mid A] \\approx -\\frac{15}{s_{\\text{pool}}} \\left(\\frac{5}{12}\\right) = -\\frac{75}{12 s_{\\text{pool}}} = -\\frac{25}{4 s_{\\text{pool}}}$.\n-   $\\operatorname{Var}(z \\mid A) \\approx 1 - \\frac{15^2}{s_{\\text{pool}}^2} \\left(\\frac{35}{144}\\right) = 1 - \\frac{225 \\times 35}{144 s_{\\text{pool}}^2} = 1 - \\frac{7875}{144 s_{\\text{pool}}^2} = 1 - \\frac{875}{16 s_{\\text{pool}}^2}$.\n-   $\\mathbb{E}[z \\mid B] \\approx \\frac{15}{s_{\\text{pool}}} \\left(\\frac{7}{12}\\right) = \\frac{105}{12 s_{\\text{pool}}} = \\frac{35}{4 s_{\\text{pool}}}$.\n-   $\\operatorname{Var}(z \\mid B) \\approx 1 - \\frac{875}{16 s_{\\text{pool}}^2}$.\n-   $\\mu_A^{\\text{corr}} \\approx \\mu_{\\text{pool}} - 15 \\left(\\frac{5}{12}\\right) = \\mu_{\\text{pool}} - \\frac{75}{12} = \\mu_{\\text{pool}} - \\frac{25}{4}$.\n-   $\\sigma_A^{\\text{corr}} \\approx \\sqrt{s_{\\text{pool}}^2 - 15^2 \\left(\\frac{35}{144}\\right)} = \\sqrt{s_{\\text{pool}}^2 - \\frac{7875}{144}} = \\sqrt{s_{\\text{pool}}^2 - \\frac{875}{16}}$.\n-   $\\mu_B^{\\text{corr}} \\approx \\mu_{\\text{pool}} + 15 \\left(\\frac{7}{12}\\right) = \\mu_{\\text{pool}} + \\frac{105}{12} = \\mu_{\\text{pool}} + \\frac{35}{4}$.\n-   $\\sigma_B^{\\text{corr}} \\approx \\sqrt{s_{\\text{pool}}^2 - \\frac{875}{16}}$.\n\nThese eight expressions constitute the final answer.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{25}{4 s_{\\text{pool}}}  1 - \\frac{875}{16 s_{\\text{pool}}^2}  \\frac{35}{4 s_{\\text{pool}}}  1 - \\frac{875}{16 s_{\\text{pool}}^2}  \\mu_{\\text{pool}} - \\frac{25}{4}  \\sqrt{s_{\\text{pool}}^2 - \\frac{875}{16}}  \\mu_{\\text{pool}} + \\frac{35}{4}  \\sqrt{s_{\\text{pool}}^2 - \\frac{875}{16}}\n\\end{pmatrix}\n}\n$$", "id": "4561535"}]}