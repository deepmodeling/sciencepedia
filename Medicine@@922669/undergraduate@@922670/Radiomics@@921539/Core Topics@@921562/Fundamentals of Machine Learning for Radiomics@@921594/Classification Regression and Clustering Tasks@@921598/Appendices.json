{"hands_on_practices": [{"introduction": "Tree-based classifiers like Random Forests are powerful tools in radiomics, but how do they actually learn to make decisions? The process hinges on finding the best way to split the data at each step, creating child nodes that are more \"pure\" than the parent. This exercise invites you to look under the hood and calculate the Gini impurity decrease for a proposed split, a core metric that guides how a decision tree is constructed [@problem_id:4532531]. By doing so, you'll gain a tangible understanding of how these algorithms quantify information and learn from features.", "problem": "A radiomics study aims to classify brain tumor samples into low-grade glioma and high-grade glioma using textural features extracted from the Gray-Level Co-occurrence Matrix (GLCM). A Random Forest (RF) classifier is trained on these features. At a particular decision node, there are $N = 120$ samples: $80$ are high-grade and $40$ are low-grade. Consider a candidate split on a single texture feature (for example, GLCM entropy) at threshold $t$, which partitions the node into a left child with $70$ samples ($50$ high-grade, $20$ low-grade) and a right child with $50$ samples ($30$ high-grade, $20$ low-grade).\n\nStarting from the definition that the Gini impurity at a node is the probability that two independent labels drawn at random from the node’s class distribution are different, and that the Random Forest splitting criterion selects the split that maximizes the expected reduction in impurity (parent impurity minus the child impurities weighted by their sample proportions), derive the exact expected impurity decrease for this threshold $t$.\n\nExpress your final result as a single exact rational number in simplest form. Do not include units. No rounding is required.", "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. The data is internally consistent. I will proceed with the calculation.\n\nThe objective is to compute the expected impurity decrease, also known as the information gain, for a given split in a decision tree node. This is calculated as the impurity of the parent node minus the weighted average of the impurities of the child nodes. The impurity measure to be used is the Gini impurity.\n\nFirst, let's formalize the Gini impurity. As defined in the problem, the Gini impurity, $G$, at a node is the probability that two independent samples drawn from that node's class distribution have different labels. For a node with $K$ classes, where the proportion of class $i$ is $p_i$, the probability of drawing a sample of class $i$ is $p_i$. The probability of independently drawing two samples of the same class $i$ is $p_i^2$. The probability of drawing two samples of the same class (any class) is $\\sum_{i=1}^{K} p_i^2$. Therefore, the Gini impurity is given by:\n$$G = 1 - \\sum_{i=1}^{K} p_i^2$$\nIn this problem, there are $K=2$ classes: high-grade glioma (HG) and low-grade glioma (LG).\n\nLet's calculate the Gini impurity for the parent node, $G_{parent}$.\nThe parent node has $N_{parent} = 120$ samples.\nThe number of high-grade samples is $N_{HG, parent} = 80$.\nThe number of low-grade samples is $N_{LG, parent} = 40$.\nThe proportions of the classes in the parent node are:\n$p_{HG, parent} = \\frac{N_{HG, parent}}{N_{parent}} = \\frac{80}{120} = \\frac{2}{3}$\n$p_{LG, parent} = \\frac{N_{LG, parent}}{N_{parent}} = \\frac{40}{120} = \\frac{1}{3}$\nThe Gini impurity of the parent node is:\n$$G_{parent} = 1 - \\left(p_{HG, parent}^2 + p_{LG, parent}^2\\right) = 1 - \\left(\\left(\\frac{2}{3}\\right)^2 + \\left(\\frac{1}{3}\\right)^2\\right) = 1 - \\left(\\frac{4}{9} + \\frac{1}{9}\\right) = 1 - \\frac{5}{9} = \\frac{4}{9}$$\n\nNext, we calculate the Gini impurity for the left child node, $G_{left}$.\nThe left child node has $N_{left} = 70$ samples.\nThe number of high-grade samples is $N_{HG, left} = 50$.\nThe number of low-grade samples is $N_{LG, left} = 20$.\nThe proportions of the classes in the left child node are:\n$p_{HG, left} = \\frac{N_{HG, left}}{N_{left}} = \\frac{50}{70} = \\frac{5}{7}$\n$p_{LG, left} = \\frac{N_{LG, left}}{N_{left}} = \\frac{20}{70} = \\frac{2}{7}$\nThe Gini impurity of the left child node is:\n$$G_{left} = 1 - \\left(p_{HG, left}^2 + p_{LG, left}^2\\right) = 1 - \\left(\\left(\\frac{5}{7}\\right)^2 + \\left(\\frac{2}{7}\\right)^2\\right) = 1 - \\left(\\frac{25}{49} + \\frac{4}{49}\\right) = 1 - \\frac{29}{49} = \\frac{20}{49}$$\n\nThen, we calculate the Gini impurity for the right child node, $G_{right}$.\nThe right child node has $N_{right} = 50$ samples.\nThe number of high-grade samples is $N_{HG, right} = 30$.\nThe number of low-grade samples is $N_{LG, right} = 20$.\nThe proportions of the classes in the right child node are:\n$p_{HG, right} = \\frac{N_{HG, right}}{N_{right}} = \\frac{30}{50} = \\frac{3}{5}$\n$p_{LG, right} = \\frac{N_{LG, right}}{N_{right}} = \\frac{20}{50} = \\frac{2}{5}$\nThe Gini impurity of the right child node is:\n$$G_{right} = 1 - \\left(p_{HG, right}^2 + p_{LG, right}^2\\right) = 1 - \\left(\\left(\\frac{3}{5}\\right)^2 + \\left(\\frac{2}{5}\\right)^2\\right) = 1 - \\left(\\frac{9}{25} + \\frac{4}{25}\\right) = 1 - \\frac{13}{25} = \\frac{12}{25}$$\n\nFinally, we calculate the expected impurity decrease, or Information Gain, $\\Delta G$. This is the parent impurity minus the weighted average of the child impurities. The weights are the proportion of samples from the parent that go into each child.\n$$\\Delta G = G_{parent} - \\left(\\frac{N_{left}}{N_{parent}} G_{left} + \\frac{N_{right}}{N_{parent}} G_{right}\\right)$$\nSubstituting the given values and our calculated impurities:\n$$\\Delta G = \\frac{4}{9} - \\left(\\frac{70}{120} \\cdot \\frac{20}{49} + \\frac{50}{120} \\cdot \\frac{12}{25}\\right)$$\nFirst, we simplify the fractions for the weights: $\\frac{70}{120} = \\frac{7}{12}$ and $\\frac{50}{120} = \\frac{5}{12}$.\n$$\\Delta G = \\frac{4}{9} - \\left(\\frac{7}{12} \\cdot \\frac{20}{49} + \\frac{5}{12} \\cdot \\frac{12}{25}\\right)$$\nNow, we compute the terms in the parenthesis:\n$$\\frac{7}{12} \\cdot \\frac{20}{49} = \\frac{1}{12} \\cdot \\frac{20}{7} = \\frac{5}{3 \\cdot 7} = \\frac{5}{21}$$\n$$\\frac{5}{12} \\cdot \\frac{12}{25} = \\frac{5}{25} = \\frac{1}{5}$$\nSo the weighted average of child impurities is:\n$$\\frac{5}{21} + \\frac{1}{5} = \\frac{5 \\cdot 5}{21 \\cdot 5} + \\frac{1 \\cdot 21}{5 \\cdot 21} = \\frac{25}{105} + \\frac{21}{105} = \\frac{46}{105}$$\nNow we subtract this from the parent impurity:\n$$\\Delta G = \\frac{4}{9} - \\frac{46}{105}$$\nTo subtract these fractions, we find a common denominator. The least common multiple of $9 = 3^2$ and $105 = 3 \\cdot 5 \\cdot 7$ is $3^2 \\cdot 5 \\cdot 7 = 9 \\cdot 35 = 315$.\n$$\\Delta G = \\frac{4 \\cdot 35}{9 \\cdot 35} - \\frac{46 \\cdot 3}{105 \\cdot 3} = \\frac{140}{315} - \\frac{138}{315} = \\frac{140 - 138}{315} = \\frac{2}{315}$$\nThe numerator $2$ and the denominator $315$ are coprime, so the fraction is in its simplest form. The exact expected impurity decrease is $\\frac{2}{315}$.", "answer": "$$\\boxed{\\frac{2}{315}}$$", "id": "4532531"}, {"introduction": "Unsupervised clustering helps uncover natural groupings in radiomic data, but how can we know if the resulting clusters are meaningful? Without ground-truth labels, we rely on internal evaluation metrics. The Silhouette score is a popular and intuitive metric that assesses cluster quality by comparing an object's similarity to its own cluster (cohesion) against its similarity to other clusters (separation) [@problem_id:4532507]. This practice will guide you through the calculation of the Silhouette score, providing a quantitative tool to appraise the performance of your clustering algorithm.", "problem": "A research team extracts high-dimensional radiomic features from Computed Tomography (CT) scans of lung nodules, standardizes each feature to zero mean and unit variance, and computes pairwise dissimilarities using standardized Euclidean distance. They perform unsupervised clustering and obtain a clustering solution with $2$ clusters. For each lesion $i$, let $a(i)$ denote the mean distance from $i$ to all other lesions in its own cluster (the intra-cluster mean distance), and let $b(i)$ denote the minimum, over all other clusters, of the mean distance from $i$ to all lesions in that other cluster (the nearest-cluster mean distance).\n\nTask:\n1. Using standard clustering definitions as the foundational base, precisely define the Silhouette score $s(i)$ for a single lesion $i$ in terms of $a(i)$ and $b(i)$, and the overall average Silhouette score $\\bar{s}$ across all lesions.\n2. Compute the overall average Silhouette score $\\bar{s}$ for the following clustering solution. There are $6$ lesions, indexed $1$ through $6$. The mean intra-cluster and nearest-cluster mean distances $(a(i), b(i))$ for each lesion are:\n- Lesion $1$: $a(1) = 0.45$, $b(1) = 0.80$\n- Lesion $2$: $a(2) = 0.50$, $b(2) = 0.95$\n- Lesion $3$: $a(3) = 0.40$, $b(3) = 0.70$\n- Lesion $4$: $a(4) = 0.55$, $b(4) = 0.85$\n- Lesion $5$: $a(5) = 0.60$, $b(5) = 1.20$\n- Lesion $6$: $a(6) = 0.50$, $b(6) = 0.90$\n\nExpress your final answer for $\\bar{s}$ as a decimal rounded to four significant figures.", "solution": "The problem statement has been validated and is deemed sound, well-posed, and scientifically grounded. It presents a standard computational task in the field of unsupervised machine learning and its application in radiomics. All necessary data and definitions are provided.\n\nThe first task is to define the Silhouette score for a single data point and the overall average Silhouette score for a dataset. Let $i$ be a single lesion (data point) in a dataset that has been partitioned into several clusters. The quantities $a(i)$ and $b(i)$ are defined as:\n- $a(i)$: The mean dissimilarity (in this case, standardized Euclidean distance) of lesion $i$ to all other lesions within the same cluster. This measures how well $i$ is assigned to its cluster (cohesion).\n- $b(i)$: The smallest mean dissimilarity of $i$ to all lesions in any other cluster, of which $i$ is not a member. This measures how well $i$ has been separated from other clusters (separation).\n\nThe Silhouette score $s(i)$ for the single lesion $i$ is a measure of how similar that lesion is to its own cluster compared to other clusters. It is defined as:\n$$s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}$$\nThe value of $s(i)$ ranges from $-1$ to $1$. A value near $1$ indicates that the lesion is well-matched to its own cluster and poorly-matched to neighboring clusters. A value near $0$ indicates that the lesion is on or very close to the decision boundary between two neighboring clusters. A negative value suggests that the lesion may have been assigned to the wrong cluster.\n\nThe overall average Silhouette score, denoted as $\\bar{s}$, is the mean of the individual Silhouette scores $s(i)$ for all lesions in the dataset. If there are $N$ lesions in total, the average score is:\n$$\\bar{s} = \\frac{1}{N} \\sum_{i=1}^{N} s(i)$$\n\nThe second task is to compute the overall average Silhouette score $\\bar{s}$ for the given clustering solution with $N=6$ lesions. The provided values for $(a(i), b(i))$ are:\n- Lesion $1$: $a(1) = 0.45$, $b(1) = 0.80$\n- Lesion $2$: $a(2) = 0.50$, $b(2) = 0.95$\n- Lesion $3$: $a(3) = 0.40$, $b(3) = 0.70$\n- Lesion $4$: $a(4) = 0.55$, $b(4) = 0.85$\n- Lesion $5$: $a(5) = 0.60$, $b(5) = 1.20$\n- Lesion $6$: $a(6) = 0.50$, $b(6) = 0.90$\n\nWe first compute the individual Silhouette score $s(i)$ for each lesion $i$ from $1$ to $6$.\n\nFor lesion $1$:\n$$s(1) = \\frac{b(1) - a(1)}{\\max\\{a(1), b(1)\\}} = \\frac{0.80 - 0.45}{\\max\\{0.45, 0.80\\}} = \\frac{0.35}{0.80} = 0.4375$$\n\nFor lesion $2$:\n$$s(2) = \\frac{b(2) - a(2)}{\\max\\{a(2), b(2)\\}} = \\frac{0.95 - 0.50}{\\max\\{0.50, 0.95\\}} = \\frac{0.45}{0.95} \\approx 0.473684$$\n\nFor lesion $3$:\n$$s(3) = \\frac{b(3) - a(3)}{\\max\\{a(3), b(3)\\}} = \\frac{0.70 - 0.40}{\\max\\{0.40, 0.70\\}} = \\frac{0.30}{0.70} \\approx 0.428571$$\n\nFor lesion $4$:\n$$s(4) = \\frac{b(4) - a(4)}{\\max\\{a(4), b(4)\\}} = \\frac{0.85 - 0.55}{\\max\\{0.55, 0.85\\}} = \\frac{0.30}{0.85} \\approx 0.352941$$\n\nFor lesion $5$:\n$$s(5) = \\frac{b(5) - a(5)}{\\max\\{a(5), b(5)\\}} = \\frac{1.20 - 0.60}{\\max\\{0.60, 1.20\\}} = \\frac{0.60}{1.20} = 0.5$$\n\nFor lesion $6$:\n$$s(6) = \\frac{b(6) - a(6)}{\\max\\{a(6), b(6)\\}} = \\frac{0.90 - 0.50}{\\max\\{0.50, 0.90\\}} = \\frac{0.40}{0.90} \\approx 0.444444$$\n\nNext, we compute the sum of these individual scores:\n$$\\sum_{i=1}^{6} s(i) = s(1) + s(2) + s(3) + s(4) + s(5) + s(6)$$\n$$\\sum_{i=1}^{6} s(i) \\approx 0.4375 + 0.473684 + 0.428571 + 0.352941 + 0.5 + 0.444444 \\approx 2.63714$$\n\nFinally, we compute the average Silhouette score $\\bar{s}$ by dividing the sum by the number of lesions, $N=6$:\n$$\\bar{s} = \\frac{1}{6} \\sum_{i=1}^{6} s(i) \\approx \\frac{2.63714}{6} \\approx 0.439523...$$\n\nThe problem requires the final answer to be rounded to four significant figures. The fifth significant figure is $2$, which is less than $5$, so we round down.\n$$\\bar{s} \\approx 0.4395$$", "answer": "$$\\boxed{0.4395}$$", "id": "4532507"}, {"introduction": "A classifier's output is often a continuous score, not a final decision. Translating this score into a practical diagnosis requires choosing a threshold, a decision with profound clinical implications. This exercise explores how to select a threshold to meet a specific clinical requirement, such as achieving a high Negative Predictive Value ($NPV$) in a screening scenario [@problem_id:4532566]. By working through the connection between sensitivity, specificity, prevalence, and predictive values, you will learn to optimize a classifier for its intended real-world use.", "problem": "A radiomics-based screening classifier outputs a continuous score $S$ for each subject, where higher scores indicate higher likelihood of disease. In a target screening population with disease prevalence $p = 0.05$, suppose that the score distributions are approximately Gaussian: conditioned on disease ($D$) the score is $S \\mid D \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$ with $\\mu_1 = 2$ and $\\sigma = 1$, and conditioned on no disease ($\\neg D$) the score is $S \\mid \\neg D \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$ with $\\mu_0 = 0$ and $\\sigma = 1$. A subject is classified as screen-positive if $S \\ge t$ for a threshold $t \\in \\mathbb{R}$. Denote the sensitivity by $\\mathrm{Se}(t) = \\mathbb{P}(S \\ge t \\mid D)$ and the specificity by $\\mathrm{Sp}(t) = \\mathbb{P}(S < t \\mid \\neg D)$. Let $\\Phi(\\cdot)$ be the standard normal cumulative distribution function.\n\nStarting from the definitions of sensitivity and specificity and Bayes’ theorem for conditional probabilities, express the Positive Predictive Value (PPV) and Negative Predictive Value (NPV) as functions $\\mathrm{PPV}(t)$ and $\\mathrm{NPV}(t)$ in terms of $\\mathrm{Se}(t)$, $\\mathrm{Sp}(t)$, and $p$. Then, using the specified Gaussian score model, determine the unique threshold $t^{\\star}$ that achieves the screening design requirement $\\mathrm{NPV}(t^{\\star}) = 0.995$.\n\nRound your final threshold to three significant figures. Express the final threshold as a pure number with no units.", "solution": "We begin from core definitions used in binary classification and Bayes’ theorem. For a threshold $t$, define\n$$\n\\mathrm{Se}(t) = \\mathbb{P}(S \\ge t \\mid D), \\quad \\mathrm{Sp}(t) = \\mathbb{P}(S < t \\mid \\neg D).\n$$\nLet $p = \\mathbb{P}(D)$ denote the disease prevalence and $1-p = \\mathbb{P}(\\neg D)$ the non-diseased prevalence. By Bayes’ theorem, the Positive Predictive Value (PPV) and Negative Predictive Value (NPV) are\n$$\n\\mathrm{PPV}(t) = \\mathbb{P}(D \\mid S \\ge t) = \\frac{\\mathbb{P}(S \\ge t \\mid D)\\,\\mathbb{P}(D)}{\\mathbb{P}(S \\ge t \\mid D)\\,\\mathbb{P}(D) + \\mathbb{P}(S \\ge t \\mid \\neg D)\\,\\mathbb{P}(\\neg D)},\n$$\n$$\n\\mathrm{NPV}(t) = \\mathbb{P}(\\neg D \\mid S < t) = \\frac{\\mathbb{P}(S < t \\mid \\neg D)\\,\\mathbb{P}(\\neg D)}{\\mathbb{P}(S < t \\mid \\neg D)\\,\\mathbb{P}(\\neg D) + \\mathbb{P}(S < t \\mid D)\\,\\mathbb{P}(D)}.\n$$\nRecognizing that $\\mathbb{P}(S \\ge t \\mid \\neg D) = 1 - \\mathrm{Sp}(t)$ and $\\mathbb{P}(S < t \\mid D) = 1 - \\mathrm{Se}(t)$, we rewrite these as\n$$\n\\mathrm{PPV}(t) = \\frac{\\mathrm{Se}(t)\\,p}{\\mathrm{Se}(t)\\,p + \\left(1 - \\mathrm{Sp}(t)\\right)\\,(1-p)},\n$$\n$$\n\\mathrm{NPV}(t) = \\frac{\\mathrm{Sp}(t)\\,(1-p)}{\\mathrm{Sp}(t)\\,(1-p) + \\left(1 - \\mathrm{Se}(t)\\right)\\,p}.\n$$\n\nNext, we connect $\\mathrm{Se}(t)$ and $\\mathrm{Sp}(t)$ to the Gaussian score model. With $S \\mid D \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$ and $S \\mid \\neg D \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$, and a “positive if $S \\ge t$” rule, we have\n$$\n\\mathrm{Se}(t) = \\mathbb{P}(S \\ge t \\mid D) = 1 - \\Phi\\!\\left(\\frac{t - \\mu_1}{\\sigma}\\right) = \\Phi\\!\\left(\\frac{\\mu_1 - t}{\\sigma}\\right),\n$$\n$$\n\\mathrm{Sp}(t) = \\mathbb{P}(S < t \\mid \\neg D) = \\Phi\\!\\left(\\frac{t - \\mu_0}{\\sigma}\\right).\n$$\nFor the specified parameters $\\mu_1 = 2$, $\\mu_0 = 0$, and $\\sigma = 1$, these specialize to\n$$\n\\mathrm{Se}(t) = \\Phi(2 - t), \\qquad \\mathrm{Sp}(t) = \\Phi(t).\n$$\n\nThe screening design requirement is $\\mathrm{NPV}(t^{\\star}) = 0.995$ at prevalence $p = 0.05$. Let $\\alpha = 0.995$. Substituting the NPV formula,\n$$\n\\alpha = \\frac{\\mathrm{Sp}(t)\\,(1-p)}{\\mathrm{Sp}(t)\\,(1-p) + \\left(1 - \\mathrm{Se}(t)\\right)\\,p}.\n$$\nRearrange to isolate a linear relation between $\\left(1 - \\mathrm{Se}(t)\\right)$ and $\\mathrm{Sp}(t)$:\n\\begin{align*}\n\\alpha\\left[\\mathrm{Sp}(t)\\,(1-p) + \\left(1 - \\mathrm{Se}(t)\\right)\\,p\\right] &= \\mathrm{Sp}(t)\\,(1-p), \\\\\n\\alpha\\,p\\left(1 - \\mathrm{Se}(t)\\right) &= \\mathrm{Sp}(t)\\,(1-p)\\,(1 - \\alpha), \\\\\n1 - \\mathrm{Se}(t) &= \\frac{(1-\\alpha)(1-p)}{\\alpha\\,p}\\,\\mathrm{Sp}(t).\n\\end{align*}\nWith $p = 0.05$ and $\\alpha = 0.995$,\n$$\n\\frac{(1-\\alpha)(1-p)}{\\alpha\\,p} = \\frac{0.005 \\times 0.95}{0.995 \\times 0.05} = \\frac{0.00475}{0.04975} \\approx 0.0954773869.\n$$\nThus,\n$$\n1 - \\mathrm{Se}(t) \\approx 0.0954773869 \\,\\mathrm{Sp}(t).\n$$\nUsing the Gaussian expressions,\n$$\n1 - \\mathrm{Se}(t) = 1 - \\Phi(2 - t) = \\Phi(t - 2),\n$$\nso the threshold $t^{\\star}$ is the unique solution to\n$$\n\\Phi(t^{\\star} - 2) = 0.0954773869 \\,\\Phi(t^{\\star}).\n$$\n\nWe now solve this equation numerically. Define $f(t) = \\Phi(t - 2) - 0.0954773869\\,\\Phi(t)$. We locate a root by bracketing:\n\n- At $t = 0.49$, using standard normal values $\\Phi(0.49) \\approx 0.6879$ and $\\Phi(-1.51) \\approx 0.0655$, we have\n$$\nf(0.49) \\approx 0.0655 - 0.0954773869 \\times 0.6879 \\approx 0.0655 - 0.06568 \\approx -1.8 \\times 10^{-4}.\n$$\n- At $t = 0.50$, with $\\Phi(0.50) \\approx 0.691462$ and $\\Phi(-1.50) \\approx 0.066807$, we have\n$$\nf(0.50) \\approx 0.066807 - 0.0954773869 \\times 0.691462 \\approx 0.066807 - 0.066019 \\approx 7.9 \\times 10^{-4}.\n$$\nA root lies between $t = 0.49$ and $t = 0.50$. Refining at $t = 0.492$:\n- $\\Phi(0.492) \\approx 0.688612$, $\\Phi(-1.508) \\approx 0.06574$,\n$$\nf(0.492) \\approx 0.06574 - 0.0954773869 \\times 0.688612 \\approx 0.06574 - 0.065746 \\approx -6 \\times 10^{-6}.\n$$\nThis is extremely close to zero and indicates the solution is approximately $t^{\\star} \\approx 0.492$. This value satisfies $\\mathrm{NPV}(t^{\\star}) \\approx 0.995$ by construction.\n\nFor completeness, we can compute the corresponding sensitivity and specificity at $t^{\\star} \\approx 0.492$:\n$$\n\\mathrm{Se}(t^{\\star}) = \\Phi(2 - 0.492) = \\Phi(1.508) \\approx 1 - \\Phi(-1.508) \\approx 1 - 0.06574 \\approx 0.93426,\n$$\n$$\n\\mathrm{Sp}(t^{\\star}) = \\Phi(0.492) \\approx 0.68861.\n$$\nThen\n$$\n\\mathrm{PPV}(t^{\\star}) = \\frac{0.93426 \\times 0.05}{0.93426 \\times 0.05 + (1 - 0.68861)\\times 0.95} \\approx \\frac{0.046713}{0.046713 + 0.31139 \\times 0.95} \\approx \\frac{0.046713}{0.34253} \\approx 0.136,\n$$\nand\n$$\n\\mathrm{NPV}(t^{\\star}) = \\frac{0.68861 \\times 0.95}{0.68861 \\times 0.95 + (1 - 0.93426)\\times 0.05} \\approx \\frac{0.65418}{0.65418 + 0.003287} \\approx 0.995,\n$$\nconfirming the design target.\n\nRounding the threshold to three significant figures yields $t^{\\star} = 0.492$.", "answer": "$$\\boxed{0.492}$$", "id": "4532566"}]}