{"hands_on_practices": [{"introduction": "Building a robust radiomics model begins with a solid foundation in data partitioning. Before we can train and evaluate a model, we must split our dataset in a way that ensures our test results are meaningful. This exercise introduces the fundamental technique of stratified cross-validation, a method designed to handle class imbalance—a common challenge in medical datasets—by ensuring each fold is a representative microcosm of the whole dataset. Mastering this calculation is the first step toward conducting a methodologically sound validation study [@problem_id:4568156].", "problem": "A binary radiomics study seeks to predict whether a lesion is clinically significant based on engineered image features. Let the dataset contain $N = 1500$ patients, with a positive class prevalence of $p = 0.10$ (that is, $10\\%$ of patients are positive and $90\\%$ are negative). You will evaluate a classifier using stratified $k$-fold cross-validation (CV), where $k = 5$. In stratified $k$-fold CV, each fold is a disjoint subset containing exactly $N/k$ patients and preserves the class prevalence of the full dataset in expectation, with no patient appearing in more than one fold.\n\nFrom first principles, and without invoking any shortcut formulas beyond core definitions of prevalence and stratified sampling, design the fold-wise split under these constraints and compute the expected number of positive and negative patients in each validation fold. Provide the counts as exact integers; no rounding is required. Your final answer must be a single ordered pair representing the counts in one validation fold, in the form $\\big(\\text{positives}, \\text{negatives}\\big)$.", "solution": "The problem statement has been validated and is determined to be self-contained, scientifically sound, and well-posed. The task is to determine the composition of each validation fold in a stratified $k$-fold cross-validation setup. We shall proceed from first principles.\n\nLet $N$ be the total number of patients in the dataset, $p$ be the prevalence of the positive class, and $k$ be the number of folds for cross-validation. The given values are:\n- Total patients: $N = 1500$\n- Positive class prevalence: $p = 0.10$\n- Number of folds: $k = 5$\n\nFirst, we determine the total number of patients in the positive and negative classes for the entire dataset.\nThe number of positive patients, denoted as $N_{pos}$, is the product of the total number of patients and the class prevalence:\n$$N_{pos} = N \\times p$$\nSubstituting the given values:\n$$N_{pos} = 1500 \\times 0.10 = 150$$\n\nThe number of negative patients, denoted as $N_{neg}$, is the remainder of the dataset:\n$$N_{neg} = N - N_{pos} = N \\times (1 - p)$$\nSubstituting the given values:\n$$N_{neg} = 1500 \\times (1 - 0.10) = 1500 \\times 0.90 = 1350$$\nAs a verification, the sum of patients in both classes is $N_{pos} + N_{neg} = 150 + 1350 = 1500$, which\nis equal to the total number of patients $N$.\n\nNext, we consider the structure of the $k$-fold cross-validation. The dataset is partitioned into $k$ disjoint folds of equal size. The size of each fold, $N_{fold}$, is:\n$$N_{fold} = \\frac{N}{k}$$\nSubstituting the given values:\n$$N_{fold} = \\frac{1500}{5} = 300$$\n\nThe problem specifies *stratified* $k$-fold cross-validation. The principle of stratification is to ensure that the class distribution within each fold mirrors the class distribution of the overall dataset. This is achieved by separately partitioning the set of positive patients and the set of negative patients into $k$ equal-sized subsets. Each final validation fold is then constructed by combining one subset of positive patients with one subset of negative patients.\n\nTherefore, the expected number of positive patients in each fold, denoted $E[N_{pos, fold}]$, is the total number of positive patients distributed evenly across the $k$ folds:\n$$E[N_{pos, fold}] = \\frac{N_{pos}}{k}$$\nSubstituting the calculated value for $N_{pos}$:\n$$E[N_{pos, fold}] = \\frac{150}{5} = 30$$\n\nSimilarly, the expected number of negative patients in each fold, denoted $E[N_{neg, fold}]$, is the total number of negative patients distributed evenly across the $k$ folds:\n$$E[N_{neg, fold}] = \\frac{N_{neg}}{k}$$\nSubstituting the calculated value for $N_{neg}$:\n$$E[N_{neg, fold}] = \\frac{1350}{5} = 270$$\n\nSince both $N_{pos}$ and $N_{neg}$ are integer multiples of $k$, the stratification can be performed perfectly without any remainder. In this case, the expected number of patients in each class per fold is an exact integer, and every fold will contain precisely this number of positive and negative patients.\n\nWe can verify the composition of each fold. The total number of patients in one fold is the sum of its positive and negative constituents:\n$$N_{pos, fold} + N_{neg, fold} = 30 + 270 = 300$$\nThis matches the calculated fold size $N_{fold}$.\n\nFurthermore, the prevalence within each fold is:\n$$p_{fold} = \\frac{N_{pos, fold}}{N_{fold}} = \\frac{30}{300} = 0.10$$\nThis confirms that the class prevalence $p$ is preserved in each fold, consistent with the definition of stratified sampling.\n\nThe problem asks for the counts as an ordered pair $(\\text{positives}, \\text{negatives})$. The resulting counts for each validation fold are $(30, 270)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n30 & 270\n\\end{pmatrix}\n}\n$$", "id": "4568156"}, {"introduction": "While stratification ensures class balance, radiomics data presents another critical challenge: non-independence. A single patient may contribute multiple images, lesions, or slices, creating hidden correlations within the dataset. If we are not careful, data from the same patient can \"leak\" from the training set into the validation or test set, leading to falsely optimistic performance estimates. This exercise focuses on the vital practice of data auditing, challenging you to devise a procedure to detect this patient-level leakage and ensure your evaluation is truly independent and unbiased [@problem_id:4568150].", "problem": "A radiomics pipeline is being developed to predict treatment response from Magnetic Resonance Imaging (MRI) volumes, with feature extraction performed on segmented regions. Each patient contributes multiple slices and possibly multiple acquisitions, and images are stored with Digital Imaging and Communications in Medicine (DICOM) metadata, including a patient identifier field. The dataset is organized for $k$-fold cross-validation, and a final hold-out test set is also reserved. To ensure rigorous evaluation, the training, validation, and test sets must be independent at the patient level, meaning that no single patient’s data should appear in more than one cross-validation fold or simultaneously in training and validation/test splits.\n\nFundamental base: In statistical learning, evaluation unbiasedness requires that the training sample and the evaluation sample be drawn from disjoint sets of underlying units to avoid dependence (data leakage). In set-theoretic terms, if $\\mathcal{P}$ denotes the set of unique patient identifiers and $\\{F_1, F_2, \\dots, F_k\\}$ denotes the $k$ disjoint fold subsets of $\\mathcal{P}$, then a proper partition satisfies $F_i \\cap F_j = \\varnothing$ for all $i \\neq j$. Similarly, for a hold-out split with training set $\\mathcal{T} \\subset \\mathcal{P}$ and validation/test sets $\\mathcal{V}, \\mathcal{S} \\subset \\mathcal{P}$, independence requires $\\mathcal{T} \\cap \\mathcal{V} = \\varnothing$, $\\mathcal{T} \\cap \\mathcal{S} = \\varnothing$, and $\\mathcal{V} \\cap \\mathcal{S} = \\varnothing$. A detection procedure should determine whether these disjointness conditions are violated by using only the available patient identifier mapping and split assignments.\n\nWhich of the following procedures correctly detect inadvertent leakage at the patient level in the described setting?\n\nA. Extract the patient identifier for each image from the Digital Imaging and Communications in Medicine (DICOM) metadata, construct the set of unique patient identifiers per fold $\\{F_1, \\dots, F_k\\}$ and per split $\\mathcal{T}, \\mathcal{V}, \\mathcal{S}$, then compute all pairwise intersections $F_i \\cap F_j$ for $i \\neq j$ and $\\mathcal{T} \\cap \\mathcal{V}$, $\\mathcal{T} \\cap \\mathcal{S}$, $\\mathcal{V} \\cap \\mathcal{S}$; flag leakage if any intersection has cardinality greater than $0$.\n\nB. Verify that the number of images per fold is similar, that the class distribution is approximately balanced, and that the total count of images across folds sums to the dataset size; conclude no leakage if these checks pass.\n\nC. Compute feature vectors for all images and check whether any feature vector in one fold is exactly identical to a feature vector in another fold; if such equality is found, flag leakage.\n\nD. Hash each patient identifier to a short fixed-length code using a non-cryptographic hash and compare hashed codes across folds; if no hash collisions are observed across folds, conclude no leakage.\n\nE. For the cross-validation folds, concatenate all patient identifiers and let $U = \\bigcup_{i=1}^k F_i$. Compute $\\lvert U \\rvert$ and $\\sum_{i=1}^k \\lvert F_i \\rvert$; if $\\lvert U \\rvert = \\sum_{i=1}^k \\lvert F_i \\rvert$ and similarly $\\lvert \\mathcal{T} \\cup \\mathcal{V} \\cup \\mathcal{S} \\rvert = \\lvert \\mathcal{T} \\rvert + \\lvert \\mathcal{V} \\rvert + \\lvert \\mathcal{S} \\rvert$, conclude no leakage; otherwise flag leakage.", "solution": "The problem statement is found to be valid. It is scientifically grounded in the principles of statistical learning and machine learning validation, specifically the critical need to prevent data leakage in a radiomics context. It is well-posed, objective, and provides a clear, formal definition of the condition to be verified. The terminology is precise and consistent with standard practices in the field.\n\nThe core of the problem is to devise a procedure to verify the disjointness of sets of patient identifiers. The problem defines two scenarios:\n1.  For $k$-fold cross-validation, the sets of unique patient identifiers for each fold, denoted $\\{F_1, F_2, \\dots, F_k\\}$, must be pairwise disjoint. That is, for any two distinct folds $F_i$ and $F_j$ (where $i \\neq j$), their intersection must be the empty set: $F_i \\cap F_j = \\varnothing$.\n2.  For a hold-out split, the sets of unique patient identifiers for the training ($\\mathcal{T}$), validation ($\\mathcal{V}$), and test ($\\mathcal{S}$) sets must be pairwise disjoint: $\\mathcal{T} \\cap \\mathcal{V} = \\varnothing$, $\\mathcal{T} \\cap \\mathcal{S} = \\varnothing$, and $\\mathcal{V} \\cap \\mathcal{S} = \\varnothing$.\n\nA correct detection procedure must be a logically sound method to verify these conditions. There are two primary mathematical approaches to confirm that a collection of sets is pairwise disjoint.\n\nFirst Principle 1: Direct Intersection. A collection of sets is pairwise disjoint if and only if the intersection of every distinct pair of sets is the empty set. The cardinality of the empty set is $0$. Therefore, leakage exists if and only if there exists a pair of distinct sets (e.g., $F_i, F_j$ with $i \\neq j$) such that $\\lvert F_i \\cap F_j \\rvert > 0$.\n\nFirst Principle 2: Cardinality of the Union. Based on the principle of inclusion-exclusion, the cardinality of the union of a collection of finite sets $\\{A_1, A_2, \\dots, A_n\\}$ is given by:\n$$ \\left| \\bigcup_{i=1}^n A_i \\right| = \\sum_{i=1}^n |A_i| - \\sum_{1 \\le i < j \\le n} |A_i \\cap A_j| + \\dots $$\nThis equality simplifies to $\\left| \\bigcup_{i=1}^n A_i \\right| = \\sum_{i=1}^n |A_i|$ if and only if all pairwise intersections are empty, i.e., the sets are pairwise disjoint. If there is any overlap, then $\\left| \\bigcup_{i=1}^n A_i \\right| < \\sum_{i=1}^n |A_i|$. Therefore, leakage exists if and only if $\\left| \\bigcup_{i=1}^k F_i \\right| \\neq \\sum_{i=1}^k |F_i|$.\n\nA correct procedure must be a valid implementation of one of these two principles.\n\n### Option-by-Option Analysis\n\n**Option A:** This procedure proposes to:\n1.  Construct the sets of unique patient identifiers for each data split: $\\{F_1, \\dots, F_k\\}$ and $\\{\\mathcal{T}, \\mathcal{V}, \\mathcal{S}\\}$.\n2.  Compute all pairwise intersections: $F_i \\cap F_j$ for $i \\neq j$, and $\\mathcal{T} \\cap \\mathcal{V}$, $\\mathcal{T} \\cap \\mathcal{S}$, $\\mathcal{V} \\cap \\mathcal{S}$.\n3.  Flag leakage if the cardinality of any of these intersections is greater than $0$.\n\nThis is a direct and exact implementation of the definition of pairwise disjoint sets (First Principle 1). It is exhaustive, covering both cross-validation folds and the hold-out splits. It is both a necessary and sufficient condition for detecting patient-level leakage.\n\n**Verdict: Correct.**\n\n**Option B:** This procedure proposes checking for similar numbers of images per fold, balanced class distribution, and that the total image count is conserved. These are properties related to creating balanced and stratified splits, which are desirable for model training and evaluation, but they are entirely orthogonal to the issue of patient-level independence. For instance, a patient could have $10$ images in fold $F_1$ and $12$ images in fold $F_2$. The total image count would be correct, and the folds might even be balanced, but there is a clear violation of patient-level disjointness. This procedure is insufficient and would fail to detect leakage.\n\n**Verdict: Incorrect.**\n\n**Option C:** This procedure suggests checking for identical feature vectors across different folds. This is flawed for two main reasons. First, the unit of leakage is the patient, not the feature vector. A single patient contributes multiple images (slices, acquisitions), which will almost always produce distinct feature vectors. This method would therefore fail to detect the most common form of patient leakage. Second, it is theoretically possible (though unlikely) for images from two *different* patients to produce identical feature vectors, which would lead this method to flag a false positive for leakage. This procedure fundamentally misidentifies the object of interest.\n\n**Verdict: Incorrect.**\n\n**Option D:** This procedure suggests hashing patient identifiers and then comparing the hashes across folds. While hashing is a common technique for comparing data, introducing it here is unnecessary and potentially problematic. The problem specifies a \"non-cryptographic\" and \"short\" hash. Such hash functions have a non-negligible probability of collisions, where two different patient identifiers map to the same hash value. This could lead to false positives (flagging leakage where none exists) if two different patients in different folds happen to have colliding hashes. The direct comparison of patient identifiers, as described in Option A, is simple, computationally inexpensive for any realistic dataset size, and error-free. The procedure in D is less rigorous and introduces a potential source of error.\n\n**Verdict: Incorrect.**\n\n**Option E:** This procedure proposes to check if the cardinality of the union of patient ID sets equals the sum of the cardinalities of the individual sets. That is, for cross-validation folds, check if $\\lvert \\bigcup_{i=1}^k F_i \\rvert = \\sum_{i=1}^k \\lvert F_i \\rvert$. A similar check is proposed for the hold-out splits. As derived from the principle of inclusion-exclusion (First Principle 2), this equality holds if and only if the sets $\\{F_i\\}$ are pairwise disjoint. Therefore, this is a mathematically sound and complete method for detecting the presence of leakage. Leakage is correctly flagged if the equality does not hold.\n\n**Verdict: Correct.**\n\nBoth options A and E describe correct and sufficient procedures for detecting patient-level data leakage based on fundamental set-theoretic principles. Option A directly checks the pairwise disjointness condition, while Option E checks an equivalent global property. Both are valid.", "answer": "$$\\boxed{AE}$$", "id": "4568150"}, {"introduction": "Now we combine the principles of stratification and data integrity into a single, powerful validation scheme. Grouped stratified cross-validation is a standard for many radiomics studies because it simultaneously prevents data leakage by keeping all data from a single patient in one fold, while also maintaining class balance across folds. This practice problem asks you to design and quantify such a scheme, applying your understanding from the previous exercises to a realistic, multi-lesion patient cohort, a key skill for designing and reporting credible radiomics research [@problem_id:4568097].", "problem": "A radiomics study investigates whether texture features extracted from cancer lesions on Computed Tomography (CT) predict a binary clinical outcome, denoted by $y \\in \\{0,1\\}$, at the patient level. For each patient, features are extracted from $5$ lesions, and all lesions from a given patient must be kept together to avoid information leakage. There are $200$ patients in total, with $80$ patients having $y=1$ and $120$ patients having $y=0$. The model development plan uses $k$-fold Cross-Validation (CV), where Cross-Validation (CV) refers to partitioning the dataset into $k$ disjoint subsets (folds); in each CV iteration, one fold serves as the validation set and the remaining $k-1$ folds form the training set. The CV must be grouped by patient (so that all lesion-level feature vectors from any single patient are assigned to the same fold) and stratified by outcome (so that each fold contains an equal number of $y=1$ and $y=0$ patients, up to exact divisibility).\n\nStarting only from the core definitions of grouped partitioning, stratification by class labels, and $k$-fold CV, design a grouped stratified $5$-fold CV scheme that preserves patient grouping and outcome balance at the patient level. Then compute, for each CV iteration, the exact sizes of:\n- the validation set in patients and lesions,\n- the training set in patients and lesions,\n- the count of $y=1$ patients in the validation set and in the training set,\n- the count of $y=0$ patients in the validation set and in the training set.\n\nReport your final numerical answer as a single row matrix in the order\n$$\\text{(validation patients)} \\;\\&\\; \\text{(training patients)} \\;\\&\\; \\text{(validation } y=1\\text{)} \\;\\&\\; \\text{(validation } y=0\\text{)} \\;\\&\\; \\text{(training } y=1\\text{)} \\;\\&\\; \\text{(training } y=0\\text{)} \\;\\&\\; \\text{(validation lesions)} \\;\\&\\; \\text{(training lesions)}.$$\nNo rounding is required. Express the answer without units.", "solution": "The problem requires the design and quantification of a grouped, stratified $k$-fold Cross-Validation (CV) scheme for a given dataset. I will first validate the problem statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Total number of patients, $N_{total}$: $200$.\n- Number of patients with outcome $y=1$, $N_{y=1}$: $80$.\n- Number of patients with outcome $y=0$, $N_{y=0}$: $120$.\n- Number of lesions per patient, $L$: $5$.\n- Number of folds for CV, $k$: $5$.\n- CV constraints: The CV must be grouped by patient and stratified by outcome $y$.\n- Stratification goal: Each fold should contain an equal number of $y=1$ and $y=0$ patients, as far as possible.\n- Grouping constraint: All lesions from a single patient must reside in the same fold.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, employing standard methodologies from machine learning and medical data analysis (radiomics, grouped CV, stratified CV). It is well-posed, providing all necessary information to determine a unique solution. The provided numbers are consistent: $N_{y=1} + N_{y=0} = 80 + 120 = 200 = N_{total}$. The stratification is perfectly achievable since both $N_{y=1}$ and $N_{y=0}$ are divisible by $k=5$. The problem is objective and free of ambiguities. It does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\n\nThe analysis is based on a dataset of $N_{total} = 200$ patients. The patient population is divided into two classes based on a binary outcome $y \\in \\{0, 1\\}$. There are $N_{y=1} = 80$ patients in class $y=1$ and $N_{y=0} = 120$ patients in class $y=0$. Each patient has data from $L=5$ distinct lesions. The model development plan calls for a $k$-fold CV with $k=5$. The division of data must adhere to two principles: grouping and stratification.\n\n1.  **Grouping**: The partitioning into folds must occur at the patient level. This means that all $5$ lesions from any given patient must belong to the same fold, preventing information about a single patient from being present in both the training and validation sets within the same CV iteration. This is a critical step to avoid data leakage and obtain an unbiased estimate of model performance.\n\n2.  **Stratification**: The partitioning must be stratified by the patient outcome $y$. This ensures that the proportion of patients from each class ($y=1$ and $y=0$) is maintained across all folds, making each fold a representative microcosm of the entire dataset.\n\nFirst, we determine the composition of each of the $k=5$ folds. Since the partitioning is done at the patient level, we calculate the number of patients per fold.\nThe total number of patients per fold, $N_{fold}$, is:\n$$N_{fold} = \\frac{N_{total}}{k} = \\frac{200}{5} = 40 \\text{ patients}$$\n\nTo satisfy the stratification requirement, we distribute the patients from each class evenly across the $5$ folds.\nThe number of patients with outcome $y=1$ per fold, $N_{fold, y=1}$, is:\n$$N_{fold, y=1} = \\frac{N_{y=1}}{k} = \\frac{80}{5} = 16 \\text{ patients}$$\n\nThe number of patients with outcome $y=0$ per fold, $N_{fold, y=0}$, is:\n$$N_{fold, y=0} = \\frac{N_{y=0}}{k} = \\frac{120}{5} = 24 \\text{ patients}$$\n\nAs a check, the total number of patients in each fold is $N_{fold, y=1} + N_{fold, y=0} = 16 + 24 = 40$, which is consistent with our calculation for $N_{fold}$. Since all divisions result in integers, the stratification is perfect. Each of the $5$ folds will contain exactly $16$ patients with $y=1$ and $24$ patients with $y=0$.\n\nNext, we analyze a single iteration of the $5$-fold CV process. In any given iteration, one of the $5$ folds serves as the **validation set**, and the remaining $k-1 = 4$ folds are combined to form the **training set**. Since all folds are identical in size and composition, the calculated sizes will be the same for every CV iteration.\n\n**Validation Set Sizes:**\nThe validation set consists of a single fold.\n- Number of patients in the validation set: $N_{val\\_patients} = N_{fold} = 40$.\n- Number of $y=1$ patients in the validation set: $N_{val, y=1} = N_{fold, y=1} = 16$.\n- Number of $y=0$ patients in the validation set: $N_{val, y=0} = N_{fold, y=0} = 24$.\n- Total number of lesions in the validation set: $N_{val\\_lesions} = N_{val\\_patients} \\times L = 40 \\times 5 = 200$.\n\n**Training Set Sizes:**\nThe training set consists of $k-1 = 4$ combined folds.\n- Number of patients in the training set: $N_{train\\_patients} = (k-1) \\times N_{fold} = 4 \\times 40 = 160$. This can also be calculated as $N_{total} - N_{val\\_patients} = 200 - 40 = 160$.\n- Number of $y=1$ patients in the training set: $N_{train, y=1} = (k-1) \\times N_{fold, y=1} = 4 \\times 16 = 64$. This is also $N_{y=1} - N_{val, y=1} = 80 - 16 = 64$.\n- Number of $y=0$ patients in the training set: $N_{train, y=0} = (k-1) \\times N_{fold, y=0} = 4 \\times 24 = 96$. This is also $N_{y=0} - N_{val, y=0} = 120 - 24 = 96$.\n- Total number of lesions in the training set: $N_{train\\_lesions} = N_{train\\_patients} \\times L = 160 \\times 5 = 800$.\n\nThe problem asks for these $8$ values. The specified order for the final answer is: (validation patients), (training patients), (validation $y=1$), (validation $y=0$), (training $y=1$), (training $y=0$), (validation lesions), (training lesions).\n\nThe calculated values are:\n- Validation patients: $40$\n- Training patients: $160$\n- Validation $y=1$: $16$\n- Validation $y=0$: $24$\n- Training $y=1$: $64$\n- Training $y=0$: $96$\n- Validation lesions: $200$\n- Training lesions: $800$\n\nThese values will be presented as a single row matrix in the final answer.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n40 & 160 & 16 & 24 & 64 & 96 & 200 & 800\n\\end{pmatrix}\n}\n$$", "id": "4568097"}]}